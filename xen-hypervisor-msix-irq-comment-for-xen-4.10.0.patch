From cf1e852aa9a10ed524a32e1de97d4532d8d7cabd Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Fri, 30 Aug 2019 15:59:29 +0800
Subject: [PATCH 1/1] xen hypervisor msix irq comment for xen-4.10.0

xen-4.10.0

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 tools/libxl/libxl_pci.c                      |   4 +
 tools/qemu-xen-traditional/hw/pass-through.c |   8 ++
 tools/qemu-xen-traditional/hw/piix4acpi.c    |   4 +
 tools/qemu-xen-traditional/i386-dm/helper2.c |  29 +++++
 tools/qemu-xen-traditional/xen-vl-extra.c    |   5 +
 tools/qemu-xen-traditional/xenstore.c        |   9 ++
 tools/qemu-xen/hw/i386/xen/xen-hvm.c         |  49 ++++++++
 tools/qemu-xen/hw/xen/xen_pt_msi.c           |  29 +++++
 xen/arch/x86/hvm/emulate.c                   |  45 +++++++
 xen/arch/x86/hvm/intercept.c                 |  34 ++++++
 xen/arch/x86/hvm/ioreq.c                     |  19 +++
 xen/arch/x86/hvm/irq.c                       |   9 ++
 xen/arch/x86/hvm/vlapic.c                    |  57 +++++++++
 xen/arch/x86/hvm/vmsi.c                      | 125 ++++++++++++++++++++
 xen/arch/x86/hvm/vmx/vmcs.c                  |  10 ++
 xen/arch/x86/hvm/vmx/vmx.c                   |  33 ++++++
 xen/arch/x86/io_apic.c                       |  20 ++++
 xen/arch/x86/irq.c                           | 171 +++++++++++++++++++++++++++
 xen/arch/x86/msi.c                           |  29 +++++
 xen/arch/x86/pci.c                           |  21 ++++
 xen/arch/x86/physdev.c                       |  17 +++
 xen/arch/x86/pv/emul-priv-op.c               |   5 +
 xen/common/irq.c                             |   1 +
 xen/drivers/passthrough/io.c                 |  53 +++++++++
 xen/include/asm-x86/domain.h                 |   8 ++
 xen/include/asm-x86/hvm/domain.h             |  10 ++
 xen/include/asm-x86/hvm/hvm.h                |  11 ++
 xen/include/asm-x86/hvm/irq.h                |  21 ++++
 xen/include/asm-x86/hvm/vlapic.h             |  10 ++
 xen/include/asm-x86/hvm/vmx/vmcs.h           |  23 ++++
 xen/include/asm-x86/mach-generic/mach_apic.h |   5 +
 xen/include/public/arch-x86/hvm/save.h       |  14 +++
 xen/include/public/arch-x86/xen.h            |   8 ++
 xen/include/xen/event.h                      |   8 ++
 xen/include/xen/pci.h                        |   5 +
 35 files changed, 909 insertions(+)

diff --git a/tools/libxl/libxl_pci.c b/tools/libxl/libxl_pci.c
index 88a55ce..c259730 100644
--- a/tools/libxl/libxl_pci.c
+++ b/tools/libxl/libxl_pci.c
@@ -938,6 +938,10 @@ static int pci_ins_check(libxl__gc *gc, uint32_t domid, const char *state, void
     return 1;
 }
 
+/*
+ * called by:
+ *   - libxl/libxl_pci.c|1005| <<do_pci_add>> rc = qemu_pci_add_xenstore(gc, domid, pcidev);
+ */
 static int qemu_pci_add_xenstore(libxl__gc *gc, uint32_t domid,
                                  libxl_device_pci *pcidev)
 {
diff --git a/tools/qemu-xen-traditional/hw/pass-through.c b/tools/qemu-xen-traditional/hw/pass-through.c
index 0b76585..0371795 100644
--- a/tools/qemu-xen-traditional/hw/pass-through.c
+++ b/tools/qemu-xen-traditional/hw/pass-through.c
@@ -4339,6 +4339,10 @@ static int pt_intel_opregion_write(struct pt_dev *ptdev,
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/pass-through.c|4613| <<power_on_php_devfn>> register_real_device(dpci_infos.e_bus,
+ */
 static struct pt_dev * register_real_device(PCIBus *e_bus,
         const char *e_dev_name, int e_devfn, uint8_t r_bus, uint8_t r_dev,
         uint8_t r_func, uint32_t machine_irq, struct pci_access *pci_access,
@@ -4602,6 +4606,10 @@ static int unregister_real_device(int devfn)
     return 0;
 }
 
+/*
+ * called by only:
+ *   - hw/piix4acpi.c|714| <<acpi_php_add>> power_on_php_devfn(devfn);
+ */
 int power_on_php_devfn(int devfn)
 {
     struct php_dev *php_dev = &dpci_infos.php_devs[devfn];
diff --git a/tools/qemu-xen-traditional/hw/piix4acpi.c b/tools/qemu-xen-traditional/hw/piix4acpi.c
index ddbe8e0..0d8cb47 100644
--- a/tools/qemu-xen-traditional/hw/piix4acpi.c
+++ b/tools/qemu-xen-traditional/hw/piix4acpi.c
@@ -668,6 +668,10 @@ void acpi_php_del(int devfn)
     acpi_sci_intr(s);
 }
 
+/*
+ * called by:
+ *   - xen-vl-extra.c|129| <<do_pci_add>> acpi_php_add(devfn);
+ */
 void acpi_php_add(int devfn)
 {
     GPEState *s = &gpe_state;
diff --git a/tools/qemu-xen-traditional/i386-dm/helper2.c b/tools/qemu-xen-traditional/i386-dm/helper2.c
index 78093fe..c3ce163 100644
--- a/tools/qemu-xen-traditional/i386-dm/helper2.c
+++ b/tools/qemu-xen-traditional/i386-dm/helper2.c
@@ -105,6 +105,18 @@ buffered_iopage_t *buffered_io_page = NULL;
 QEMUTimer *buffered_io_timer;
 
 /* the evtchn fd for polling */
+/*
+ * used by:
+ *   - i386-dm/helper2.c|145| <<cpu_x86_init>> xce_handle = xenevtchn_open(NULL, 0);
+ *   - i386-dm/helper2.c|146| <<cpu_x86_init>> if (xce_handle == NULL) {
+ *   - i386-dm/helper2.c|154| <<cpu_x86_init>> xce_handle, domid, shared_page->vcpu_ioreq[i].vp_eport);
+ *   - i386-dm/helper2.c|168| <<cpu_x86_init>> rc = xenevtchn_bind_interdomain(xce_handle, domid, (uint32_t)bufioreq_evtchn);
+ *   - i386-dm/helper2.c|282| <<cpu_get_ioreq>> port = xenevtchn_pending(xce_handle);
+ *   - i386-dm/helper2.c|300| <<cpu_get_ioreq>> xenevtchn_unmask(xce_handle, port);
+ *   - i386-dm/helper2.c|556| <<handle_buffered_io>> xenevtchn_unmask(xce_handle, bufioreq_local_port);
+ *   - i386-dm/helper2.c|604| <<cpu_handle_ioreq>> xenevtchn_notify(xce_handle, ioreq_local_port[send_vcpu]);
+ *   - i386-dm/helper2.c|613| <<main_loop>> int evtchn_fd = xce_handle == NULL ? -1 : xenevtchn_fd(xce_handle);
+ */
 xenevtchn_handle *xce_handle = NULL;
 
 /* which vcpu we are serving */
@@ -274,6 +286,10 @@ static ioreq_t *__cpu_get_ioreq(int vcpu)
 //use poll to get the port notification
 //ioreq_vec--out,the
 //retval--the number of ioreq packet
+/*
+ * called by:
+ *   - i386-dm/helper2.c|564| <<cpu_handle_ioreq>> ioreq_t *req = cpu_get_ioreq();
+ */
 static ioreq_t *cpu_get_ioreq(void)
 {
     int i;
@@ -544,6 +560,11 @@ static int __handle_buffered_iopage(CPUState *env)
     return req.count;
 }
 
+/*
+ * called by:
+ *   - i386-dm/helper2.c|619| <<main_loop>> buffered_io_timer = qemu_new_timer(rt_clock, handle_buffered_io,
+ *   - i386-dm/helper2.c|643| <<main_loop>> handle_buffered_io(env);
+ */
 static void handle_buffered_io(void *opaque)
 {
     CPUState *env = opaque;
@@ -557,6 +578,10 @@ static void handle_buffered_io(void *opaque)
     }
 }
 
+/*
+ * called by:
+ *   - i386-dm/helper2.c|623| <<main_loop>> qemu_set_fd_handler(evtchn_fd, cpu_handle_ioreq, NULL, env);
+ */
 static void cpu_handle_ioreq(void *opaque)
 {
     extern int shutdown_requested;
@@ -607,6 +632,10 @@ static void cpu_handle_ioreq(void *opaque)
 
 int xen_pause_requested;
 
+/*
+ * called by:
+ *   - vl.c|6221| <<main>> main_loop();
+ */
 int main_loop(void)
 {
     CPUState *env = cpu_single_env;
diff --git a/tools/qemu-xen-traditional/xen-vl-extra.c b/tools/qemu-xen-traditional/xen-vl-extra.c
index 206ac65..63e8e90 100644
--- a/tools/qemu-xen-traditional/xen-vl-extra.c
+++ b/tools/qemu-xen-traditional/xen-vl-extra.c
@@ -120,6 +120,11 @@ void do_pci_del(char *devname)
     free(devname_cpy);
 }
 
+/*
+ * used by:
+ *   - monitor.c|1548| <<global>> { "pci_add", "s", do_pci_add,
+ *   - xenstore.c|919| <<xenstore_process_dm_command_event>> do_pci_add(par);
+ */
 void do_pci_add(char *devname)
 {
     int devfn;
diff --git a/tools/qemu-xen-traditional/xenstore.c b/tools/qemu-xen-traditional/xenstore.c
index 8af2715..2c050af 100644
--- a/tools/qemu-xen-traditional/xenstore.c
+++ b/tools/qemu-xen-traditional/xenstore.c
@@ -843,6 +843,10 @@ out:
 
 
 /* Accept state change commands from the control tools */
+/*
+ * called by:
+ *   - xenstore.c|1088| <<xenstore_process_event>> xenstore_process_dm_command_event();
+ */
 static void xenstore_process_dm_command_event(void)
 {
     char *path = NULL, *command = NULL, *par = NULL;
@@ -1074,6 +1078,11 @@ static void xenstore_process_vcpu_set_event(char **vec)
     return;
 }
 
+/*
+ * 在以下使用:
+ *   - i386-dm/helper2.c|627| <<main_loop>> qemu_set_fd_handler(xenstore_fd(), xenstore_process_event, NULL, NULL);
+ *   - i386-dm/helper2.c|658| <<main_loop>> xenstore_process_event(NULL);
+ */
 void xenstore_process_event(void *opaque)
 {
     char **vec, *offset, *bpath = NULL, *buf = NULL, *drv = NULL, *image = NULL;
diff --git a/tools/qemu-xen/hw/i386/xen/xen-hvm.c b/tools/qemu-xen/hw/i386/xen/xen-hvm.c
index d9ccd5d..8973d31 100644
--- a/tools/qemu-xen/hw/i386/xen/xen-hvm.c
+++ b/tools/qemu-xen/hw/i386/xen/xen-hvm.c
@@ -94,8 +94,21 @@ typedef struct XenIOState {
     QEMUTimer *buffered_io_timer;
     CPUState **cpu_by_vcpu_id;
     /* the evtchn port for polling the notification, */
+    /*
+     * used by:
+     *   - hw/i386/xen/xen-hvm.c|738| <<cpu_get_ioreq>> if (state->ioreq_local_port[i] == port) {
+     *   - hw/i386/xen/xen-hvm.c|1156| <<cpu_handle_ioreq>> state->ioreq_local_port[state->send_vcpu]);
+     *   - hw/i386/xen/xen-hvm.c|1364| <<xen_hvm_init>> state->ioreq_local_port = g_malloc0(max_cpus * sizeof (evtchn_port_t));
+     *   - hw/i386/xen/xen-hvm.c|1374| <<xen_hvm_init>> state->ioreq_local_port[i] = rc;
+     */
     evtchn_port_t *ioreq_local_port;
     /* evtchn local port for buffered io */
+    /*
+     * used by:
+     *   - hw/i386/xen/xen-hvm.c|730| <<cpu_get_ioreq>> if (port == state->bufioreq_local_port) {
+     *   - hw/i386/xen/xen-hvm.c|1089| <<handle_buffered_io>> xenevtchn_unmask(state->xce_handle, state->bufioreq_local_port);
+     *   - hw/i386/xen/xen-hvm.c|1383| <<xen_hvm_init>> state->bufioreq_local_port = rc;
+     */
     evtchn_port_t bufioreq_local_port;
     /* the evtchn fd for polling */
     xenevtchn_handle *xce_handle;
@@ -942,6 +955,20 @@ static void handle_vmport_ioreq(XenIOState *state, ioreq_t *req)
     current_cpu = NULL;
 }
 
+/*
+ * (gdb) bt
+ * #0  handle_ioreq (state=state@entry=0x55e009e82fd0, req=req@entry=0x7ffdfcd1f820) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/hw/i386/xen/xen-hvm.c:946
+ * #1  0x000055e008fd7079 in cpu_handle_ioreq (opaque=0x55e009e82fd0) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/hw/i386/xen/xen-hvm.c:1089
+ * #2  0x000055e0092a65bc in aio_dispatch_handlers (ctx=ctx@entry=0x55e009e51a90) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/aio-posix.c:399
+ * #3  0x000055e0092a6e48 in aio_dispatch (ctx=0x55e009e51a90) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/aio-posix.c:430
+ * #4  0x000055e0092a3d9e in aio_ctx_dispatch (source=<optimized out>, callback=<optimized out>, user_data=<optimized out>) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/async.c:261
+ * #5  0x00007f8e2c992197 in g_main_context_dispatch () from /lib/x86_64-linux-gnu/libglib-2.0.so.0
+ * #6  0x000055e0092a5fb3 in glib_pollfds_poll () at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/main-loop.c:257
+ * #7  os_host_main_loop_wait (timeout=<optimized out>) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/main-loop.c:307
+ * #8  main_loop_wait (nonblocking=<optimized out>) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/main-loop.c:563
+ * #9  0x000055e008f14a47 in main_loop () at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/vl.c:1917
+ * #10 main (argc=<optimized out>, argv=<optimized out>, envp=<optimized out>) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/vl.c:4795
+ */
 static void handle_ioreq(XenIOState *state, ioreq_t *req)
 {
     trace_handle_ioreq(req, req->type, req->dir, req->df, req->data_is_ptr,
@@ -1076,6 +1103,23 @@ static void handle_buffered_io(void *opaque)
     }
 }
 
+/*
+ * (gdb) bt
+ * #0  handle_ioreq (state=state@entry=0x55e009e82fd0, req=req@entry=0x7ffdfcd1f820) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/hw/i386/xen/xen-hvm.c:946
+ * #1  0x000055e008fd7079 in cpu_handle_ioreq (opaque=0x55e009e82fd0) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/hw/i386/xen/xen-hvm.c:1089
+ * #2  0x000055e0092a65bc in aio_dispatch_handlers (ctx=ctx@entry=0x55e009e51a90) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/aio-posix.c:399
+ * #3  0x000055e0092a6e48 in aio_dispatch (ctx=0x55e009e51a90) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/aio-posix.c:430
+ * #4  0x000055e0092a3d9e in aio_ctx_dispatch (source=<optimized out>, callback=<optimized out>, user_data=<optimized out>) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/async.c:261
+ * #5  0x00007f8e2c992197 in g_main_context_dispatch () from /lib/x86_64-linux-gnu/libglib-2.0.so.0
+ * #6  0x000055e0092a5fb3 in glib_pollfds_poll () at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/main-loop.c:257
+ * #7  os_host_main_loop_wait (timeout=<optimized out>) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/main-loop.c:307
+ * #8  main_loop_wait (nonblocking=<optimized out>) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/util/main-loop.c:563
+ * #9  0x000055e008f14a47 in main_loop () at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/vl.c:1917
+ * #10 main (argc=<optimized out>, argv=<optimized out>, envp=<optimized out>) at /home/zhang/xen/xen-4.10.0-build/tools/qemu-xen/vl.c:4795
+ *
+ * used by:
+ *   - hw/i386/xen/xen-hvm.c|1149| <<xen_main_loop_prepare>> qemu_set_fd_handler(evtchn_fd, cpu_handle_ioreq, NULL, state);
+ */
 static void cpu_handle_ioreq(void *opaque)
 {
     XenIOState *state = opaque;
@@ -1232,6 +1276,11 @@ static void xen_wakeup_notifier(Notifier *notifier, void *data)
     xc_set_hvm_param(xen_xc, xen_domid, HVM_PARAM_ACPI_S_STATE, 0);
 }
 
+/*
+ * called by:
+ *   - hw/i386/pc_piix.c|122| <<pc_init1>> xen_hvm_init(pcms, &ram_memory);
+ *   - hw/i386/pc_q35.c|119| <<pc_q35_init>> xen_hvm_init(pcms, &ram_memory)
+ */
 void xen_hvm_init(PCMachineState *pcms, MemoryRegion **ram_memory)
 {
     int i, rc;
diff --git a/tools/qemu-xen/hw/xen/xen_pt_msi.c b/tools/qemu-xen/hw/xen/xen_pt_msi.c
index 6d1e3bd..ed7ab12 100644
--- a/tools/qemu-xen/hw/xen/xen_pt_msi.c
+++ b/tools/qemu-xen/hw/xen/xen_pt_msi.c
@@ -436,6 +436,35 @@ static void set_entry_value(XenPTMSIXEntry *e, int offset, uint32_t val)
     e->latch[offset / sizeof(*e->latch)] = val;
 }
 
+/*
+ * (gdb) bt
+ * #0  pci_msix_write (opaque=0x556af1c22310, addr=0, val=0, size=4) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/hw/xen/xen_pt_msi.c:441
+ * #1  0x0000556aee6b2d78 in memory_region_write_accessor (mr=0x556af1a4ed50, addr=0, value=<optimized out>, size=4, shift=<optimized out>, mask=<optimized out>, attrs=...)
+ *     at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/memory.c:529
+ * #2  0x0000556aee6b093d in access_with_adjusted_size (addr=addr@entry=0, value=value@entry=0x7ffd64ef9608, size=size@entry=4, access_size_min=<optimized out>, access_size_max=<optimized out>, 
+ *     access=0x556aee6b2d00 <memory_region_write_accessor>, mr=0x556af1a4ed50, attrs=...) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/memory.c:595
+ * #3  0x0000556aee6b4ba6 in memory_region_dispatch_write (mr=mr@entry=0x556af1a4ed50, addr=0, data=0, size=size@entry=4, attrs=..., attrs@entry=...) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/memory.c:1350
+ * #4  0x0000556aee66e441 in address_space_write_continue (mr=0x556af1a4ed50, l=4, addr1=0, len=4, buf=0x7ffd64ef9788 "", attrs=..., addr=4077977600, as=0x556aeeffacc0 <address_space_memory>)
+ *     at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/exec.c:2930
+ * #5  address_space_write (as=<optimized out>, addr=<optimized out>, attrs=..., buf=<optimized out>, len=<optimized out>) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/exec.c:2987
+ * #6  0x0000556aee66e99d in address_space_rw (as=as@entry=0x556aeeffacc0 <address_space_memory>, addr=<optimized out>, attrs=..., attrs@entry=..., buf=buf@entry=0x7ffd64ef9788 "", len=<optimized out>, 
+ *     is_write=<optimized out>) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/exec.c:3089
+ * #7  0x0000556aee66e9c7 in cpu_physical_memory_rw (addr=<optimized out>, buf=buf@entry=0x7ffd64ef9788 "", len=<optimized out>, is_write=is_write@entry=1) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/exec.c:3098
+ * #8  0x0000556aee72578c in rw_phys_req_item (rw=1, val=0x7ffd64ef9788, i=0, req=0x7ffd64ef9780, addr=<optimized out>) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/hw/i386/xen/xen-hvm.c:809
+ * #9  write_phys_req_item (val=0x7ffd64ef9788, i=0, req=0x7ffd64ef9780, addr=<optimized out>) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/hw/i386/xen/xen-hvm.c:820
+ * #10 cpu_ioreq_move (req=<optimized out>) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/hw/i386/xen/xen-hvm.c:882
+ * #11 handle_ioreq (state=state@entry=0x556af0549000, req=req@entry=0x7ffd64ef9780) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/hw/i386/xen/xen-hvm.c:964
+ * #12 0x0000556aee727679 in cpu_handle_ioreq (opaque=0x556af0549000) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/hw/i386/xen/xen-hvm.c:1089
+ * #13 0x0000556aee9e05dc in aio_dispatch_handlers (ctx=ctx@entry=0x556af0524ec0) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/util/aio-posix.c:399
+ * #14 0x0000556aee9e0e68 in aio_dispatch (ctx=0x556af0524ec0) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/util/aio-posix.c:430
+ * #15 0x0000556aee9dddbe in aio_ctx_dispatch (source=<optimized out>, callback=<optimized out>, user_data=<optimized out>) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/util/async.c:261
+ * #16 0x00007fc0d9d6a197 in g_main_context_dispatch () from /lib/x86_64-linux-gnu/libglib-2.0.so.0
+ * #17 0x0000556aee9dffd3 in glib_pollfds_poll () at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/util/main-loop.c:257
+ * #18 os_host_main_loop_wait (timeout=<optimized out>) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/util/main-loop.c:307
+ * #19 main_loop_wait (nonblocking=<optimized out>) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/util/main-loop.c:563
+ * #20 0x0000556aee665177 in main_loop () at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/vl.c:1917
+ * #21 main (argc=<optimized out>, argv=<optimized out>, envp=<optimized out>) at /home/zhang/xen/xen-4.10.0/tools/qemu-xen/vl.c:4795
+ */
 static void pci_msix_write(void *opaque, hwaddr addr,
                            uint64_t val, unsigned size)
 {
diff --git a/xen/arch/x86/hvm/emulate.c b/xen/arch/x86/hvm/emulate.c
index f88a011..873858a 100644
--- a/xen/arch/x86/hvm/emulate.c
+++ b/xen/arch/x86/hvm/emulate.c
@@ -117,6 +117,11 @@ static const struct hvm_io_handler ioreq_server_handler = {
     .ops = &ioreq_server_ops
 };
 
+/*
+ * called by:
+ *   - arch/x86/hvm/emulate.c|321| <<hvmemul_do_io_buffer>> rc = hvmemul_do_io(is_mmio, addr, reps, size, dir, df, 0,
+ *   - arch/x86/hvm/emulate.c|415| <<hvmemul_do_io_addr>> rc = hvmemul_do_io(is_mmio, addr, &count, size, dir, df, 1,
+ */
 static int hvmemul_do_io(
     bool_t is_mmio, paddr_t addr, unsigned long *reps, unsigned int size,
     uint8_t dir, bool_t df, bool_t data_is_addr, uintptr_t data)
@@ -310,6 +315,11 @@ static int hvmemul_do_io(
     return X86EMUL_OKAY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/emulate.c|450| <<hvmemul_do_pio_buffer>> return hvmemul_do_io_buffer(0, port, &one_rep, size, dir, 0, buffer);
+ *   - arch/x86/hvm/emulate.c|494| <<hvmemul_do_mmio_buffer>> return hvmemul_do_io_buffer(1, mmio_gpa, reps, size, dir, df, buffer);
+ */
 static int hvmemul_do_io_buffer(
     bool_t is_mmio, paddr_t addr, unsigned long *reps, unsigned int size,
     uint8_t dir, bool_t df, void *buffer)
@@ -479,6 +489,11 @@ static int hvmemul_do_pio_addr(uint16_t port,
  *       <buffer> pointer; there is no implicit interation over a
  *       block of memory starting at <buffer>.
  */
+/*
+ * called by:
+ *   - arch/x86/hvm/emulate.c|880| <<hvmemul_phys_mmio_access>> rc = hvmemul_do_mmio_buffer(gpa, &one_rep, chunk, dir, 0,
+ *   - arch/x86/hvm/emulate.c|1695| <<hvmemul_rep_stos>> return hvmemul_do_mmio_buffer(gpa, reps, bytes_per_rep, IOREQ_WRITE, df,
+ */
 static int hvmemul_do_mmio_buffer(paddr_t mmio_gpa,
                                   unsigned long *reps,
                                   unsigned int size,
@@ -835,6 +850,10 @@ static int hvmemul_virtual_to_linear(
     return X86EMUL_EXCEPTION;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/emulate.c|1001| <<hvmemul_linear_mmio_access>> rc = hvmemul_phys_mmio_access(cache, gpa, chunk, dir, buffer, buffer_offset);
+ */
 static int hvmemul_phys_mmio_access(
     struct hvm_mmio_cache *cache, paddr_t gpa, unsigned int size, uint8_t dir,
     uint8_t *buffer, unsigned int offset)
@@ -952,6 +971,11 @@ static void latch_linear_to_phys(struct hvm_vcpu_io *vio, unsigned long gla,
                                        .write_access = write };
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/emulate.c|1027| <<hvmemul_linear_mmio_read>> return hvmemul_linear_mmio_access(gla, size, IOREQ_READ, buffer,
+ *   - arch/x86/hvm/emulate.c|1036| <<hvmemul_linear_mmio_write>> return hvmemul_linear_mmio_access(gla, size, IOREQ_WRITE, buffer,
+ */
 static int hvmemul_linear_mmio_access(
     unsigned long gla, unsigned int size, uint8_t dir, void *buffer,
     uint32_t pfec, struct hvm_emulate_ctxt *hvmemul_ctxt, bool_t known_gpfn)
@@ -1013,6 +1037,11 @@ static inline int hvmemul_linear_mmio_read(
                                       pfec, hvmemul_ctxt, translate);
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/emulate.c|1189| <<hvmemul_write>> return hvmemul_linear_mmio_write(addr, bytes, p_data, pfec, hvmemul_ctxt, 1);
+ *   - arch/x86/hvm/emulate.c|1196| <<hvmemul_write>> return hvmemul_linear_mmio_write(addr, bytes, p_data, pfec, hvmemul_ctxt, 0);
+ */
 static inline int hvmemul_linear_mmio_write(
     unsigned long gla, unsigned int size, void *buffer,
     uint32_t pfec, struct hvm_emulate_ctxt *hvmemul_ctxt,
@@ -1143,6 +1172,11 @@ int hvmemul_insn_fetch(
     return X86EMUL_OKAY;
 }
 
+/*
+ * used by:
+ *   - struct x86_emulate_ops hvm_emulate_ops.write = hvmemu_write()
+ *   - arch/x86/hvm/emulate.c|1313| <<hvmemul_cmpxchg>> return hvmemul_write(seg, offset, p_new, bytes, ctxt);
+ */
 static int hvmemul_write(
     enum x86_segment seg,
     unsigned long offset,
@@ -2044,6 +2078,10 @@ static int hvmemul_vmfunc(
     return rc;
 }
 
+/*
+ * used and called by:
+ *   - arch/x86/hvm/emulate.c|2203| <<hvm_emulate_one>> return _hvm_emulate_one(hvmemul_ctxt, &hvm_emulate_ops);
+ */
 static const struct x86_emulate_ops hvm_emulate_ops = {
     .read          = hvmemul_read,
     .insn_fetch    = hvmemul_insn_fetch,
@@ -2163,6 +2201,13 @@ static int _hvm_emulate_one(struct hvm_emulate_ctxt *hvmemul_ctxt,
     return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/emulate.c|2291| <<hvm_emulate_one_vm_event>> rc = hvm_emulate_one(&ctx);
+ *   - arch/x86/hvm/hvm.c|3780| <<hvm_ud_intercept>> switch ( hvm_emulate_one(&ctxt) )
+ *   - arch/x86/hvm/io.c|89| <<hvm_emulate_one_insn>> rc = hvm_emulate_one(&ctxt);
+ *   - arch/x86/hvm/vmx/realmode.c|104| <<vmx_realmode_emulate_one>> rc = hvm_emulate_one(hvmemul_ctxt);
+ */
 int hvm_emulate_one(
     struct hvm_emulate_ctxt *hvmemul_ctxt)
 {
diff --git a/xen/arch/x86/hvm/intercept.c b/xen/arch/x86/hvm/intercept.c
index 2bc156d..1a32a3c 100644
--- a/xen/arch/x86/hvm/intercept.c
+++ b/xen/arch/x86/hvm/intercept.c
@@ -112,6 +112,12 @@ static const struct hvm_io_ops portio_ops = {
     .write = hvm_portio_write
 };
 
+/*
+ * called by:
+ *   - arch/x86/hvm/emulate.c|263| <<hvmemul_do_io>> rc = hvm_process_io_intercept(&ioreq_server_handler, &p);
+ *   - arch/x86/hvm/emulate.c|276| <<hvmemul_do_io>> rc = hvm_process_io_intercept(&null_handler, &p);
+ *   - arch/x86/hvm/intercept.c|249| <<hvm_io_intercept>> rc = hvm_process_io_intercept(handler, p);
+ */
 int hvm_process_io_intercept(const struct hvm_io_handler *handler,
                              ioreq_t *p)
 {
@@ -235,6 +241,10 @@ static const struct hvm_io_handler *hvm_find_io_handler(const ioreq_t *p)
     return NULL;
 }
 
+/*
+ * called by onlu:
+ *   - arch/x86/hvm/emulate.c|188| <<hvmemul_do_io>> rc = hvm_io_intercept(&p);
+ */
 int hvm_io_intercept(ioreq_t *p)
 {
     const struct hvm_io_handler *handler;
@@ -255,6 +265,14 @@ int hvm_io_intercept(ioreq_t *p)
     return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/intercept.c|286| <<register_mmio_handler>> struct hvm_io_handler *handler = hvm_next_io_handler(d);
+ *   - arch/x86/hvm/intercept.c|299| <<register_portio_handler>> struct hvm_io_handler *handler = hvm_next_io_handler(d);
+ *   - arch/x86/hvm/io.c|255| <<register_g2m_portio_handler>> struct hvm_io_handler *handler = hvm_next_io_handler(d);
+ *   - arch/x86/hvm/stdvga.c|606| <<stdvga_init>> handler = hvm_next_io_handler(d);
+ *   - arch/x86/hvm/vmsi.c|688| <<msixtbl_init>> handler = hvm_next_io_handler(d);
+ */
 struct hvm_io_handler *hvm_next_io_handler(struct domain *d)
 {
     unsigned int i = d->arch.hvm_domain.io_handler_count++;
@@ -283,6 +301,22 @@ void register_mmio_handler(struct domain *d,
     handler->mmio.ops = ops;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/hvm.c|649| <<hvm_domain_initialise>> register_portio_handler(d, 0xe9, 1, hvm_print_line);
+ *   - arch/x86/hvm/i8254.c|479| <<pit_init>> register_portio_handler(d, PIT_BASE, 4, handle_pit_io);
+ *   - arch/x86/hvm/i8254.c|480| <<pit_init>> register_portio_handler(d, 0x61, 1, handle_speaker_io);
+ *   - arch/x86/hvm/ioreq.c|1462| <<hvm_ioreq_init>> register_portio_handler(d, 0xcf8, 4, hvm_access_cf8);
+ *   - arch/x86/hvm/pmtimer.c|362| <<pmtimer_init>> register_portio_handler(v->domain, TMR_VAL_ADDR_V0, 4, handle_pmt_io);
+ *   - arch/x86/hvm/pmtimer.c|363| <<pmtimer_init>> register_portio_handler(v->domain, PM1a_STS_ADDR_V0, 4, handle_evt_io);
+ *   - arch/x86/hvm/rtc.c|814| <<rtc_init>> register_portio_handler(d, RTC_PORT(0), 2, handle_rtc_io);
+ *   - arch/x86/hvm/stdvga.c|601| <<stdvga_init>> register_portio_handler(d, 0x3c4, 2, stdvga_intercept_pio);
+ *   - arch/x86/hvm/stdvga.c|603| <<stdvga_init>> register_portio_handler(d, 0x3ce, 2, stdvga_intercept_pio);
+ *   - arch/x86/hvm/vpic.c|441| <<vpic_init>> register_portio_handler(d, 0x20, 2, vpic_intercept_pic_io);
+ *   - arch/x86/hvm/vpic.c|442| <<vpic_init>> register_portio_handler(d, 0xa0, 2, vpic_intercept_pic_io);
+ *   - arch/x86/hvm/vpic.c|444| <<vpic_init>> register_portio_handler(d, 0x4d0, 1, vpic_intercept_elcr_io);
+ *   - arch/x86/hvm/vpic.c|445| <<vpic_init>> register_portio_handler(d, 0x4d1, 1, vpic_intercept_elcr_io);
+ */
 void register_portio_handler(struct domain *d, unsigned int port,
                              unsigned int size, portio_action_t action)
 {
diff --git a/xen/arch/x86/hvm/ioreq.c b/xen/arch/x86/hvm/ioreq.c
index d5afe20..1886e6a 100644
--- a/xen/arch/x86/hvm/ioreq.c
+++ b/xen/arch/x86/hvm/ioreq.c
@@ -66,6 +66,11 @@ bool hvm_io_pending(struct vcpu *v)
     return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/ioreq.c|103| <<hvm_wait_for_io>> hvm_io_assist(sv, ~0ul);
+ *   - arch/x86/hvm/ioreq.c|107| <<hvm_wait_for_io>> hvm_io_assist(sv, p->data);
+ */
 static void hvm_io_assist(struct hvm_ioreq_vcpu *sv, uint64_t data)
 {
     struct vcpu *v = sv->vcpu;
@@ -85,6 +90,10 @@ static void hvm_io_assist(struct hvm_ioreq_vcpu *sv, uint64_t data)
     sv->pending = false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/ioreq.c|143| <<handle_hvm_io_completion>> if ( !hvm_wait_for_io(sv, get_ioreq(s, v)) )
+ */
 static bool hvm_wait_for_io(struct hvm_ioreq_vcpu *sv, ioreq_t *p)
 {
     while ( sv->pending )
@@ -1352,6 +1361,12 @@ static int hvm_send_buffered_ioreq(struct hvm_ioreq_server *s, ioreq_t *p)
     return X86EMUL_OKAY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/emulate.c|286| <<hvmemul_do_io>> rc = hvm_send_ioreq(s, &p, 0);
+ *   - arch/x86/hvm/ioreq.c|1433| <<hvm_broadcast_ioreq>> if ( hvm_send_ioreq(s, p, buffered) == X86EMUL_UNHANDLEABLE )
+ *   - arch/x86/hvm/stdvga.c|514| <<stdvga_mem_write>> return hvm_send_ioreq(srv, &p, 1);
+ */
 int hvm_send_ioreq(struct hvm_ioreq_server *s, ioreq_t *proto_p,
                    bool buffered)
 {
@@ -1439,6 +1454,10 @@ static int hvm_access_cf8(
     return X86EMUL_UNHANDLEABLE;
 }
 
+/*
+ * called by only:
+ *   - arch/x86/hvm/hvm.c|633| <<hvm_domain_initialise>> hvm_ioreq_init(d);
+ */
 void hvm_ioreq_init(struct domain *d)
 {
     spin_lock_init(&d->arch.hvm_domain.ioreq_server.lock);
diff --git a/xen/arch/x86/hvm/irq.c b/xen/arch/x86/hvm/irq.c
index 0077f68..4699f68 100644
--- a/xen/arch/x86/hvm/irq.c
+++ b/xen/arch/x86/hvm/irq.c
@@ -467,6 +467,15 @@ void hvm_set_callback_via(struct domain *d, uint64_t via)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/irq.c|532| <<hvm_local_events_need_delivery>> struct hvm_intack intack = hvm_vcpu_has_pending_irq(v);
+ *   - arch/x86/hvm/svm/intr.c|144| <<svm_intr_assist>> intack = hvm_vcpu_has_pending_irq(v);
+ *   - arch/x86/hvm/svm/intr.c|216| <<svm_intr_assist>> intack = hvm_vcpu_has_pending_irq(v);
+ *   - arch/x86/hvm/vmx/intr.c|207| <<nvmx_intr_intercept>> intack = hvm_vcpu_has_pending_irq(v);
+ *   - arch/x86/hvm/vmx/intr.c|249| <<vmx_intr_assist>> intack = hvm_vcpu_has_pending_irq(v);
+ *   - arch/x86/hvm/vmx/intr.c|396| <<vmx_intr_assist>> intack = hvm_vcpu_has_pending_irq(v);
+ */
 struct hvm_intack hvm_vcpu_has_pending_irq(struct vcpu *v)
 {
     struct hvm_domain *plat = &v->domain->arch.hvm_domain;
diff --git a/xen/arch/x86/hvm/vlapic.c b/xen/arch/x86/hvm/vlapic.c
index 50f53bd..dcbde24 100644
--- a/xen/arch/x86/hvm/vlapic.c
+++ b/xen/arch/x86/hvm/vlapic.c
@@ -101,6 +101,10 @@ static int vlapic_find_highest_vector(const void *bitmap)
  * IRR-specific bitmap update & search routines.
  */
 
+/*
+ * called by:
+ *   - arch/x86/hvm/vlapic.c|170| <<vlapic_set_irq>> else if ( !vlapic_test_and_set_irr(vec, vlapic) )
+ */
 static int vlapic_test_and_set_irr(int vector, struct vlapic *vlapic)
 {
     return vlapic_test_and_set_vector(vector, &vlapic->regs->data[APIC_IRR]);
@@ -111,6 +115,10 @@ static void vlapic_clear_irr(int vector, struct vlapic *vlapic)
     vlapic_clear_vector(vector, &vlapic->regs->data[APIC_IRR]);
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/vlapic.c|1255| <<vlapic_has_pending_irq>> irr = vlapic_find_highest_irr(vlapic);
+ */
 static int vlapic_find_highest_irr(struct vlapic *vlapic)
 {
     if ( hvm_funcs.sync_pir_to_irr )
@@ -1045,6 +1053,10 @@ static int vlapic_range(struct vcpu *v, unsigned long addr)
            (offset < PAGE_SIZE);
 }
 
+/*
+ * used by:
+ *   - arch/x86/hvm/vlapic.c|1593| <<vlapic_init>> register_mmio_handler(v->domain, &vlapic_mmio_ops);
+ */
 static const struct hvm_mmio_ops vlapic_mmio_ops = {
     .check = vlapic_range,
     .read = vlapic_read,
@@ -1236,6 +1248,11 @@ int vlapic_virtual_intr_delivery_enabled(void)
         return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/irq.c|488| <<hvm_vcpu_has_pending_irq>> vector = vlapic_has_pending_irq(v);
+ *   - arch/x86/hvm/vmx/vvmx.c|1349| <<nvmx_update_apicv>> rvi = vlapic_has_pending_irq(v);
+ */
 int vlapic_has_pending_irq(struct vcpu *v)
 {
     struct vlapic *vlapic = vcpu_vlapic(v);
@@ -1312,6 +1329,11 @@ bool_t is_vlapic_lvtpc_enabled(struct vlapic *vlapic)
 }
 
 /* Reset the VLAPIC back to its init state. */
+/*
+ * called by:
+ *   - arch/x86/hvm/vlapic.c|306| <<vlapic_init_sipi_one>> vlapic_do_init(vcpu_vlapic(target));
+ *   - arch/x86/hvm/vlapic.c|1392| <<vlapic_reset>> vlapic_do_init(vlapic);
+ */
 static void vlapic_do_init(struct vlapic *vlapic)
 {
     int i;
@@ -1353,6 +1375,12 @@ static void vlapic_do_init(struct vlapic *vlapic)
 }
 
 /* Reset the VLAPIC back to its power-on/reset state. */
+/*
+ * called by:
+ *   - arch/x86/hvm/hvm.c|3980| <<hvm_s3_suspend>> vlapic_reset(vcpu_vlapic(v));
+ *   - arch/x86/hvm/vlapic.c|1074| <<vlapic_msr_set>> vlapic_reset(vlapic);
+ *   - arch/x86/hvm/vlapic.c|1584| <<vlapic_init>> vlapic_reset(vlapic);
+ */
 void vlapic_reset(struct vlapic *vlapic)
 {
     const struct vcpu *v = vlapic_vcpu(vlapic);
@@ -1360,6 +1388,20 @@ void vlapic_reset(struct vlapic *vlapic)
     if ( !has_vlapic(v->domain) )
         return;
 
+    /*
+     * 在以下使用apic_base_msr:
+     *   - arch/x86/hvm/hvm.c|3441| <<hvm_msr_read_intercept>> *msr_content = vcpu_vlapic(v)->hw.apic_base_msr;
+     *   - arch/x86/hvm/vlapic.c|1080| <<vlapic_msr_set>> if ( (vlapic->hw.apic_base_msr ^ value) & MSR_IA32_APICBASE_ENABLE )
+     *   - arch/x86/hvm/vlapic.c|1096| <<vlapic_msr_set>> else if ( ((vlapic->hw.apic_base_msr ^ value) & MSR_IA32_APICBASE_EXTD) &&
+     *   - arch/x86/hvm/vlapic.c|1100| <<vlapic_msr_set>> vlapic->hw.apic_base_msr = value;
+     *   - arch/x86/hvm/vlapic.c|1109| <<vlapic_msr_set>> "apic base msr is 0x%016"PRIx64, vlapic->hw.apic_base_msr);
+     *   - arch/x86/hvm/vlapic.c|1386| <<vlapic_reset>> vlapic->hw.apic_base_msr = (MSR_IA32_APICBASE_ENABLE |
+     *   - arch/x86/hvm/vlapic.c|1389| <<vlapic_reset>> vlapic->hw.apic_base_msr |= MSR_IA32_APICBASE_BSP;
+     *   - arch/x86/hvm/vlapic.c|1521| <<lapic_load_hidden>> if ( !(s->hw.apic_base_msr & MSR_IA32_APICBASE_ENABLE) &&
+     *   - include/asm-x86/hvm/vlapic.h|53| <<vlapic_base_address>> ((vlapic)->hw.apic_base_msr & MSR_IA32_APICBASE_BASE)
+     *   - include/asm-x86/hvm/vlapic.h|56| <<vlapic_x2apic_mode>> ((vlapic)->hw.apic_base_msr & MSR_IA32_APICBASE_EXTD)
+     *   - include/asm-x86/hvm/vlapic.h|59| <<vlapic_xapic_mode>> !((vlapic)->hw.apic_base_msr & MSR_IA32_APICBASE_EXTD))
+     */
     vlapic->hw.apic_base_msr = (MSR_IA32_APICBASE_ENABLE |
                                 APIC_DEFAULT_PHYS_BASE);
     if ( v->vcpu_id == 0 )
@@ -1557,10 +1599,24 @@ int vlapic_init(struct vcpu *v)
         return 0;
     }
 
+    /*
+     * struct vlapic包含struct periodic_time pt;
+     */
     vlapic->pt.source = PTSRC_lapic;
 
+    /* regs_page类型是struct page_info* */
     if (vlapic->regs_page == NULL)
     {
+        /*
+	 * regs_page使用的地方:
+	 *   - arch/x86/hvm/vlapic.c|1586| <<vlapic_init>> if (vlapic->regs_page == NULL)
+	 *   - arch/x86/hvm/vlapic.c|1588| <<vlapic_init>> vlapic->regs_page = alloc_domheap_page(v->domain, MEMF_no_owner);
+	 *   - arch/x86/hvm/vlapic.c|1589| <<vlapic_init>> if ( vlapic->regs_page == NULL )
+	 *   - arch/x86/hvm/vlapic.c|1599| <<vlapic_init>> vlapic->regs = __map_domain_page_global(vlapic->regs_page);
+	 *   - arch/x86/hvm/vlapic.c|1634| <<vlapic_destroy>> free_domheap_page(vlapic->regs_page);
+	 *   - arch/x86/hvm/vmx/vmcs.c|1241| <<construct_vmcs>> page_to_maddr(vcpu_vlapic(v)->regs_page));
+	 *   - arch/x86/hvm/vmx/vmx.c|3029| <<vmx_install_vlapic_mapping>> virt_page_ma = page_to_maddr(vcpu_vlapic(v)->regs_page);
+	 */
         vlapic->regs_page = alloc_domheap_page(v->domain, MEMF_no_owner);
         if ( vlapic->regs_page == NULL )
         {
@@ -1569,6 +1625,7 @@ int vlapic_init(struct vcpu *v)
             return -ENOMEM;
         }
     }
+    /* reges类型是struct hvm_hw_lapic_regs *regs; */
     if (vlapic->regs == NULL) 
     {
         vlapic->regs = __map_domain_page_global(vlapic->regs_page);
diff --git a/xen/arch/x86/hvm/vmsi.c b/xen/arch/x86/hvm/vmsi.c
index 7126de7..bb29ec4 100644
--- a/xen/arch/x86/hvm/vmsi.c
+++ b/xen/arch/x86/hvm/vmsi.c
@@ -40,6 +40,11 @@
 #include <asm/event.h>
 #include <asm/io_apic.h>
 
+/*
+ * called by:
+ *   - arch/x86/hvm/vmsi.c|77| <<vmsi_deliver>> vmsi_inj_irq(target, vector, trig_mode, delivery_mode);
+ *   - arch/x86/hvm/vmsi.c|88| <<vmsi_deliver>> vmsi_inj_irq(vcpu_vlapic(v), vector,
+ */
 static void vmsi_inj_irq(
     struct vlapic *target,
     uint8_t vector,
@@ -60,6 +65,12 @@ static void vmsi_inj_irq(
     }
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/irq.c|380| <<hvm_inject_msi>> return vmsi_deliver(d, vector, dest, dest_mode, delivery_mode, trig_mode);
+ *   - arch/x86/hvm/vmsi.c|118| <<vmsi_deliver_pirq>> vmsi_deliver(d, vector, dest, dest_mode, delivery_mode, trig_mode);
+ *   - drivers/passthrough/amd/iommu_guest.c|138| <<guest_iommu_deliver_msi>> vmsi_deliver(d, vector, dest, dest_mode, delivery_mode, trig_mode);
+ */
 int vmsi_deliver(
     struct domain *d, int vector,
     uint8_t dest, uint8_t dest_mode,
@@ -99,9 +110,27 @@ int vmsi_deliver(
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/io.c|918| <<hvm_dirq_assist>> vmsi_deliver_pirq(d, pirq_dpci);
+ */
 void vmsi_deliver_pirq(struct domain *d, const struct hvm_pirq_dpci *pirq_dpci)
 {
     uint32_t flags = pirq_dpci->gmsi.gflags;
+    /*
+     * 更新msi.gvec的地方:
+     *   - drivers/passthrough/io.c|376| <<pt_irq_create_bind>> pirq_dpci->gmsi.gvec = pt_irq_bind->u.msi.gvec;
+     *   - drivers/passthrough/io.c|410| <<pt_irq_create_bind>> pirq_dpci->gmsi.gvec = 0;
+     *   - drivers/passthrough/io.c|435| <<pt_irq_create_bind>> pirq_dpci->gmsi.gvec = pt_irq_bind->u.msi.gvec;
+     *
+     * 使用XEN_DOMCTL_bind_pt_irq的例子:
+     *   - libxc/xc_domain.c|1747| <<xc_domain_update_msi_irq>> domctl.cmd = XEN_DOMCTL_bind_pt_irq;
+     *   - libxc/xc_domain.c|1801| <<xc_domain_bind_pt_irq_int>> domctl.cmd = XEN_DOMCTL_bind_pt_irq;
+     *
+     * qemu-xen-traditional中调用xc_domain_update_msi_irq()的例子:
+     *   - hw/pt-msi.c|144| <<pt_msi_update>> ret = xc_domain_update_msi_irq(xc_handle, domid, gvec,
+     *   - hw/pt-msi.c|322| <<pt_msix_update_one>> ret = xc_domain_update_msi_irq(xc_handle, domid, gvec, pirq, gflags,
+     */
     int vector = pirq_dpci->gmsi.gvec;
     uint8_t dest = (uint8_t)flags;
     bool dest_mode = flags & XEN_DOMCTL_VMSI_X86_DM_MASK;
@@ -119,6 +148,10 @@ void vmsi_deliver_pirq(struct domain *d, const struct hvm_pirq_dpci *pirq_dpci)
 }
 
 /* Return value, -1 : multi-dests, non-negative value: dest_vcpu_id */
+/*
+ * called by:
+ *   - drivers/passthrough/io.c|428| <<pt_irq_create_bind>> dest_vcpu_id = hvm_girq_dest_2_vcpu_id(d, dest, dest_mode);
+ */
 int hvm_girq_dest_2_vcpu_id(struct domain *d, uint8_t dest, uint8_t dest_mode)
 {
     int dest_vcpu_id = -1, w = 0;
@@ -180,6 +213,16 @@ static struct msixtbl_entry *msixtbl_find_entry(
     struct msixtbl_entry *entry;
     struct domain *d = v->domain;
 
+    /*
+     * msixtbl_list在以下使用:
+     *   - arch/x86/hvm/vmsi.c|174| <<msixtbl_initialised>> return !!d->arch.hvm_domain.msixtbl_list.next;
+     *   - arch/x86/hvm/vmsi.c|183| <<msixtbl_find_entry>> list_for_each_entry( entry, &d->arch.hvm_domain.msixtbl_list, list )
+     *   - arch/x86/hvm/vmsi.c|445| <<add_msixtbl_entry>> list_add_rcu(&entry->list, &d->arch.hvm_domain.msixtbl_list);
+     *   - arch/x86/hvm/vmsi.c|498| <<msixtbl_pt_register>> list_for_each_entry( entry, &d->arch.hvm_domain.msixtbl_list, list )
+     *   - arch/x86/hvm/vmsi.c|557| <<msixtbl_pt_unregister>> list_for_each_entry( entry, &d->arch.hvm_domain.msixtbl_list, list )
+     *   - arch/x86/hvm/vmsi.c|583| <<msixtbl_init>> INIT_LIST_HEAD(&d->arch.hvm_domain.msixtbl_list);
+     *   - arch/x86/hvm/vmsi.c|603| <<msixtbl_pt_cleanup>> &d->arch.hvm_domain.msixtbl_list, list )
+     */
     list_for_each_entry( entry, &d->arch.hvm_domain.msixtbl_list, list )
         if ( addr >= entry->gtable &&
              addr < entry->gtable + entry->table_len )
@@ -207,6 +250,9 @@ static struct msi_desc *msixtbl_addr_to_desc(
     return NULL;
 }
 
+/*
+ * struct hvm_io_ops msixtbl_mmio_ops.read = msixtbl_read()
+ */
 static int msixtbl_read(const struct hvm_io_handler *handler,
                         uint64_t address, uint32_t len, uint64_t *pval)
 {
@@ -263,6 +309,11 @@ out:
     return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/vmsi.c|346| <<_msixtbl_write>> return msixtbl_write(current, address, len, val);
+ *   - arch/x86/hvm/vmsi.c|621| <<msix_write_completion>> if ( msixtbl_write(v, ctrl_address, 4, 0) != X86EMUL_OKAY )
+ */
 static int msixtbl_write(struct vcpu *v, unsigned long address,
                          unsigned int len, unsigned long val)
 {
@@ -285,11 +336,26 @@ static int msixtbl_write(struct vcpu *v, unsigned long address,
     nr_entry = (address - entry->gtable) / PCI_MSIX_ENTRY_SIZE;
 
     offset = address & (PCI_MSIX_ENTRY_SIZE - 1);
+    /*
+     * vector control | msg data | msg upperaddr | msg addr
+     *
+     * PCI_MSIX_ENTRY_VECTOR_CTRL_OFFSET是12, 指向vector control的地址
+     *
+     * 这里的if语句如果满足, 说明下面index是0, 1或者2
+     */
     if ( offset != PCI_MSIX_ENTRY_VECTOR_CTRL_OFFSET )
     {
         index = offset / sizeof(uint32_t);
         if ( nr_entry < MAX_MSIX_ACC_ENTRIES ) 
         {
+            /*
+	     * entry->gentries的定义:
+	     * #define MAX_MSIX_ACC_ENTRIES 3
+	     * unsigned int table_len;
+	     * struct {
+	     *     uint32_t msi_ad[3]; //Shadow of address low, high and data
+             * } gentries[MAX_MSIX_ACC_ENTRIES];
+	     */
             entry->gentries[nr_entry].msi_ad[index] = val;
             acc_bit(set, entry, nr_entry, index);
             if ( len == 8 && !index )
@@ -340,12 +406,45 @@ out:
     return r;
 }
 
+/*
+ * [<ffff82d0802f2ea0>] vmsi.c#msixtbl_write+0x160/0x240
+ * [<ffff82d0802e587a>] hvm_process_io_intercept+0x6a/0x270
+ * [<ffff82d0802e55c7>] intercept.c#hvm_mmio_accept+0x37/0xe0
+ * [<ffff82d0802f2c5f>] vmsi.c#msixtbl_range+0x6f/0x150
+ * [<ffff82d0802e5aa4>] hvm_io_intercept+0x24/0x40
+ * [<ffff82d0802d7407>] emulate.c#hvmemul_do_io+0x1e7/0x5b0
+ * [<ffff82d0802f2c5f>] vmsi.c#msixtbl_range+0x6f/0x150
+ * [<ffff82d0802d7f4e>] emulate.c#hvmemul_do_io_buffer+0x2e/0x70
+ * [<ffff82d0802d8c5b>] emulate.c#hvmemul_linear_mmio_access+0x23b/0x530
+ * [<ffff82d0802d96cc>] emulate.c#hvmemul_write+0x46c/0x500
+ * [<ffff82d0802d91cf>] hvmemul_insn_fetch+0x3f/0xa0
+ * [<ffff82d0802a16c1>] x86_emulate.c#x86_decode+0x11b1/0x1e40
+ * [<ffff82d0802a4427>] x86_emulate+0x20d7/0x1d610
+ * [<ffff82d080311073>] __get_gfn_type_access+0xf3/0x280
+ * [<ffff82d0802da26a>] emulate.c#_hvm_emulate_one+0x4a/0x1c0
+ * [<ffff82d0802da088>] hvm_emulate_init_once+0x78/0xb0
+ * [<ffff82d0802e5efb>] hvm_emulate_one_insn+0x3b/0x120
+ * [<ffff82d0802e0177>] hvm.c#__hvm_copy+0xd7/0x350
+ * [<ffff82d0802bfa60>] x86_insn_is_mem_access+0/0xc0
+ * [<ffff82d0802de977>] hvm_hap_nested_page_fault+0x117/0x6b0
+ * [<ffff82d080234635>] schedule.c#schedule+0x245/0x620
+ * [<ffff82d0803076e9>] vmx_vmexit_handler+0x969/0x1b20
+ * [<ffff82d08030a161>] nvmx_switch_guest+0x81/0x19c0
+ * [<ffff82d080308989>] vmx_vmenter_helper+0xe9/0x3a0
+ * [<ffff82d08023a6bd>] tasklet.c#tasklet_softirq_action+0x3d/0x70
+ * [<ffff82d08030db4c>] vmx_asm_vmexit_handler+0x3c/0x110
+ *
+ * struct hvm_io_ops msixtbl_mmio_ops.write = _msixtlb_write()
+ */
 static int _msixtbl_write(const struct hvm_io_handler *handler,
                           uint64_t address, uint32_t len, uint64_t val)
 {
     return msixtbl_write(current, address, len, val);
 }
 
+/*
+ * struct hvm_io_ops msixtbl_mmio_ops.accept = msixtbl_range()
+ */
 static bool_t msixtbl_range(const struct hvm_io_handler *handler,
                             const ioreq_t *r)
 {
@@ -415,6 +514,10 @@ static const struct hvm_io_ops msixtbl_mmio_ops = {
     .write = _msixtbl_write,
 };
 
+/*
+ * called by:
+ *   - arch/x86/hvm/vmsi.c|504| <<msixtbl_pt_register>> add_msixtbl_entry(d, pdev, gtable, entry);
+ */
 static void add_msixtbl_entry(struct domain *d,
                               struct pci_dev *pdev,
                               uint64_t gtable,
@@ -446,6 +549,10 @@ static void del_msixtbl_entry(struct msixtbl_entry *entry)
     call_rcu(&entry->rcu, free_msixtbl_entry);
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/io.c|376| <<pt_irq_create_bind>> rc = msixtbl_pt_register(d, info, pt_irq_bind->u.msi.gtable);
+ */
 int msixtbl_pt_register(struct domain *d, struct pirq *pirq, uint64_t gtable)
 {
     struct irq_desc *irq_desc;
@@ -555,6 +662,10 @@ found:
     spin_unlock_irq(&irq_desc->lock);
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/pci.c|1412| <<assign_device>> msixtbl_init(d);
+ */
 void msixtbl_init(struct domain *d)
 {
     struct hvm_io_handler *handler;
@@ -562,6 +673,16 @@ void msixtbl_init(struct domain *d)
     if ( !is_hvm_domain(d) || !has_vlapic(d) || msixtbl_initialised(d) )
         return;
 
+    /*
+     * used by:
+     *   - arch/x86/hvm/vmsi.c|174| <<msixtbl_initialised>> return !!d->arch.hvm_domain.msixtbl_list.next;
+     *   - arch/x86/hvm/vmsi.c|183| <<msixtbl_find_entry>> list_for_each_entry( entry, &d->arch.hvm_domain.msixtbl_list, list )
+     *   - arch/x86/hvm/vmsi.c|445| <<add_msixtbl_entry>> list_add_rcu(&entry->list, &d->arch.hvm_domain.msixtbl_list);
+     *   - arch/x86/hvm/vmsi.c|498| <<msixtbl_pt_register>> list_for_each_entry( entry, &d->arch.hvm_domain.msixtbl_list, list )
+     *   - arch/x86/hvm/vmsi.c|557| <<msixtbl_pt_unregister>> list_for_each_entry( entry, &d->arch.hvm_domain.msixtbl_list, list )
+     *   - arch/x86/hvm/vmsi.c|583| <<msixtbl_init>> INIT_LIST_HEAD(&d->arch.hvm_domain.msixtbl_list);
+     *   - arch/x86/hvm/vmsi.c|603| <<msixtbl_pt_cleanup>> &d->arch.hvm_domain.msixtbl_list, list )
+     */
     INIT_LIST_HEAD(&d->arch.hvm_domain.msixtbl_list);
 
     handler = hvm_next_io_handler(d);
@@ -588,6 +709,10 @@ void msixtbl_pt_cleanup(struct domain *d)
     spin_unlock(&d->event_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/ioreq.c|82| <<hvm_io_assist>> msix_write_completion(v);
+ */
 void msix_write_completion(struct vcpu *v)
 {
     unsigned long ctrl_address = v->arch.hvm_vcpu.hvm_io.msix_unmask_address;
diff --git a/xen/arch/x86/hvm/vmx/vmcs.c b/xen/arch/x86/hvm/vmx/vmcs.c
index b5100b5..286f524 100644
--- a/xen/arch/x86/hvm/vmx/vmcs.c
+++ b/xen/arch/x86/hvm/vmx/vmcs.c
@@ -1237,6 +1237,16 @@ static int construct_vmcs(struct vcpu *v)
 
     if ( cpu_has_vmx_tpr_shadow )
     {
+        /*
+	 * regs_page使用的地方:
+	 *   - arch/x86/hvm/vlapic.c|1586| <<vlapic_init>> if (vlapic->regs_page == NULL)
+	 *   - arch/x86/hvm/vlapic.c|1588| <<vlapic_init>> vlapic->regs_page = alloc_domheap_page(v->domain, MEMF_no_owner);
+	 *   - arch/x86/hvm/vlapic.c|1589| <<vlapic_init>> if ( vlapic->regs_page == NULL )
+	 *   - arch/x86/hvm/vlapic.c|1599| <<vlapic_init>> vlapic->regs = __map_domain_page_global(vlapic->regs_page);
+	 *   - arch/x86/hvm/vlapic.c|1634| <<vlapic_destroy>> free_domheap_page(vlapic->regs_page);
+	 *   - arch/x86/hvm/vmx/vmcs.c|1241| <<construct_vmcs>> page_to_maddr(vcpu_vlapic(v)->regs_page));
+	 *   - arch/x86/hvm/vmx/vmx.c|3029| <<vmx_install_vlapic_mapping>> virt_page_ma = page_to_maddr(vcpu_vlapic(v)->regs_page);
+	 */
         __vmwrite(VIRTUAL_APIC_PAGE_ADDR,
                   page_to_maddr(vcpu_vlapic(v)->regs_page));
         __vmwrite(TPR_THRESHOLD, 0);
diff --git a/xen/arch/x86/hvm/vmx/vmx.c b/xen/arch/x86/hvm/vmx/vmx.c
index b18ccea..def01ec 100644
--- a/xen/arch/x86/hvm/vmx/vmx.c
+++ b/xen/arch/x86/hvm/vmx/vmx.c
@@ -2059,6 +2059,12 @@ static void __vmx_deliver_posted_interrupt(struct vcpu *v)
     }
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/vlapic.c|173| <<vlapic_set_irq>> hvm_funcs.deliver_posted_intr(target, vec);
+ *
+ * struct hvm_function_table vmx_function_table.deliver_posted_intr = vmx_deliver_posted_intr()
+ */
 static void vmx_deliver_posted_intr(struct vcpu *v, u8 vector)
 {
     if ( pi_test_and_set_pir(vector, &v->arch.hvm_vmx.pi_desc) )
@@ -2109,6 +2115,13 @@ static void vmx_deliver_posted_intr(struct vcpu *v, u8 vector)
     vcpu_kick(v);
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/vlapic.c|121| <<vlapic_find_highest_irr>> hvm_funcs.sync_pir_to_irr(vlapic_vcpu(vlapic));
+ *   - arch/x86/hvm/vlapic.c|1447| <<lapic_save_regs>> hvm_funcs.sync_pir_to_irr(v);
+ *
+ * struct hvm_function_table vmx_function_table.sync_pir_to_irr = vmx_sync_pir_to_irr()
+ */
 static void vmx_sync_pir_to_irr(struct vcpu *v)
 {
     struct vlapic *vlapic = vcpu_vlapic(v);
@@ -2989,6 +3002,14 @@ static int vmx_alloc_vlapic_mapping(struct domain *d)
     mfn = page_to_mfn(pg);
     clear_domain_page(_mfn(mfn));
     share_xen_page_with_guest(pg, d, XENSHARE_writable);
+    /*
+     *  apic_access_mfn在以下使用:
+     *   - arch/x86/hvm/mtrr.c|787| <<epte_get_entry_emt>> if ( (mfn_x(mfn) ^ d->arch.hvm_domain.vmx.apic_access_mfn) >> order )
+     *   - arch/x86/hvm/vmx/vmx.c|3005| <<vmx_alloc_vlapic_mapping>> d->arch.hvm_domain.vmx.apic_access_mfn = mfn;
+     *   - arch/x86/hvm/vmx/vmx.c|3014| <<vmx_free_vlapic_mapping>> unsigned long mfn = d->arch.hvm_domain.vmx.apic_access_mfn;
+     *   - arch/x86/hvm/vmx/vmx.c|3024| <<vmx_install_vlapic_mapping>> if ( v->domain->arch.hvm_domain.vmx.apic_access_mfn == 0 )
+     *   - arch/x86/hvm/vmx/vmx.c|3030| <<vmx_install_vlapic_mapping>> apic_page_ma = v->domain->arch.hvm_domain.vmx.apic_access_mfn;
+     */
     d->arch.hvm_domain.vmx.apic_access_mfn = mfn;
     set_mmio_p2m_entry(d, paddr_to_pfn(APIC_DEFAULT_PHYS_BASE), _mfn(mfn),
                        PAGE_ORDER_4K, p2m_get_hostp2m(d)->default_access);
@@ -3004,6 +3025,10 @@ static void vmx_free_vlapic_mapping(struct domain *d)
         free_shared_domheap_page(mfn_to_page(mfn));
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/vmx/vmx.c|465| <<vmx_vcpu_initialise>> vmx_install_vlapic_mapping(v);
+ */
 static void vmx_install_vlapic_mapping(struct vcpu *v)
 {
     paddr_t virt_page_ma, apic_page_ma;
@@ -3014,6 +3039,14 @@ static void vmx_install_vlapic_mapping(struct vcpu *v)
     ASSERT(cpu_has_vmx_virtualize_apic_accesses);
 
     virt_page_ma = page_to_maddr(vcpu_vlapic(v)->regs_page);
+    /*
+     * apic_access_mfn在以下使用:
+     *   - arch/x86/hvm/mtrr.c|787| <<epte_get_entry_emt>> if ( (mfn_x(mfn) ^ d->arch.hvm_domain.vmx.apic_access_mfn) >> order )
+     *   - arch/x86/hvm/vmx/vmx.c|3005| <<vmx_alloc_vlapic_mapping>> d->arch.hvm_domain.vmx.apic_access_mfn = mfn;
+     *   - arch/x86/hvm/vmx/vmx.c|3014| <<vmx_free_vlapic_mapping>> unsigned long mfn = d->arch.hvm_domain.vmx.apic_access_mfn;
+     *   - arch/x86/hvm/vmx/vmx.c|3024| <<vmx_install_vlapic_mapping>> if ( v->domain->arch.hvm_domain.vmx.apic_access_mfn == 0 )
+     *   - arch/x86/hvm/vmx/vmx.c|3030| <<vmx_install_vlapic_mapping>> apic_page_ma = v->domain->arch.hvm_domain.vmx.apic_access_mfn;
+     */
     apic_page_ma = v->domain->arch.hvm_domain.vmx.apic_access_mfn;
     apic_page_ma <<= PAGE_SHIFT;
 
diff --git a/xen/arch/x86/io_apic.c b/xen/arch/x86/io_apic.c
index f959090..71864a8 100644
--- a/xen/arch/x86/io_apic.c
+++ b/xen/arch/x86/io_apic.c
@@ -2522,6 +2522,12 @@ void __init init_ioapic_mappings(void)
     unsigned int i, idx = FIX_IO_APIC_BASE_0;
     union IO_APIC_reg_01 reg_01;
 
+    /*
+     * 在desktop上:
+     *   smp_found_config = true
+     *   nr_ioapics = 1
+     */
+
     if ( smp_found_config )
         nr_irqs_gsi = 0;
     for ( i = 0; i < nr_ioapics; i++ )
@@ -2560,6 +2566,9 @@ void __init init_ioapic_mappings(void)
             /* The number of IO-APIC IRQ registers (== #pins): */
             reg_01.raw = io_apic_read(i, 1);
             nr_ioapic_entries[i] = reg_01.bits.entries + 1;
+	    /*
+	     * nr_irqs_gsi上面设置的开始是0, nr_ioapic_entries[i]是120
+	     */
             nr_irqs_gsi += nr_ioapic_entries[i];
 
             if ( rangeset_add_singleton(mmio_ro_ranges,
@@ -2569,8 +2578,16 @@ void __init init_ioapic_mappings(void)
         }
     }
 
+    /*
+     * highest_gsi()是119
+     */
     nr_irqs_gsi = max(nr_irqs_gsi, highest_gsi() + 1);
 
+    /*
+     * max_gsi_irqs是0
+     *
+     * 因为nr_irqs默认是0, max_gsi_irqs=4096
+     */
     if ( max_gsi_irqs == 0 )
         max_gsi_irqs = nr_irqs ? nr_irqs / 8 : PAGE_SIZE;
     else if ( nr_irqs != 0 && max_gsi_irqs > nr_irqs )
@@ -2595,6 +2612,9 @@ void __init init_ioapic_mappings(void)
         nr_irqs_gsi = max_gsi_irqs;
     }
 
+    /*
+     * 在desktop上默认nr_irqs在这里是0
+     */
     if ( nr_irqs == 0 )
         nr_irqs = cpu_has_apic ?
                   max(16U + num_present_cpus() * NR_DYNAMIC_VECTORS,
diff --git a/xen/arch/x86/irq.c b/xen/arch/x86/irq.c
index c0ab299..64066c6 100644
--- a/xen/arch/x86/irq.c
+++ b/xen/arch/x86/irq.c
@@ -26,6 +26,53 @@
 #include <asm/mach-generic/mach_apic.h>
 #include <public/physdev.h>
 
+/*
+ * 在arch/x86/x86_64/entry.S,
+ * autogen_entrypoints生成vector table, 每一个vector指向一个处理函数
+ * 函数都差不多 (每个cpu支持256个vector):
+ *
+ * 先"movb  $vec,4(%rsp)", 再"jmp   common_interrupt"
+ *
+ * 在arch/x86/x86_64/entry.S:
+ * 392 ENTRY(common_interrupt)
+ * 393         SAVE_ALL CLAC
+ * 394         CR4_PV32_RESTORE
+ * 395         movq %rsp,%rdi
+ * 396         callq do_IRQ
+ * 397         jmp ret_from_intr
+ *
+ * do_IRQ()如何把vector发送到guest?
+ *
+ * 1. 先通过vector查找percpu的vector_irq[vector]找到irq:
+ * int irq = __get_cpu_var(vector_irq[vector]);
+ *
+ * 2. 再通过irq找到对应的struct irq_desc, x86下就是&irq_desc[irq]:
+ * desc = irq_to_desc(irq);
+ *
+ * 3. 很可能desc->status & IRQ_GUEST是true, 就要调用__do_IRQ_guest(irq)了
+ *
+ * 4. 如果是dom0, 调用send_guest_pirq()往dom0插入event
+ *
+ *
+ * 重要的函数或者数据结构:
+ *
+ * - vector_irq[vector]: 把cpu的vector转换成irq, 索引&irq_desc[irq]
+ * - domain_irq_to_pirq(d, irq)负责将xen的irq转换成某个domain d的pirq
+ * - pirq_info(d, pirq)负责将某个domain d的pirq转换成'struct pirq'
+ *
+ * struct pirq {
+ *   int pirq;
+ *   u16 evtchn;
+ *   bool_t masked;
+ *   struct rcu_head rcu_head;
+ *   struct arch_pirq arch;
+ * };
+ *
+ * dom0 linux的核心函数感觉是__startup_pirq()
+ *
+ * xen有自己的irq, linux有自己的irq, pirq是domain相关的, linux和xen都认识
+ */
+
 static int parse_irq_vector_map_param(const char *s);
 
 /* opt_noirqbalance: If true, software IRQ balancing/affinity is disabled. */
@@ -48,6 +95,27 @@ static DECLARE_BITMAP(used_vectors, NR_VECTORS);
 
 static DEFINE_SPINLOCK(vector_lock);
 
+/*
+ * 使用的地方:
+ *   - xen/arch/x86/i8259.c|106| <<_disable_8259A_irq>> per_cpu(vector_irq, 0)[LEGACY_VECTOR(irq)] = ~irq;
+ *   - xen/arch/x86/i8259.c|122| <<enable_8259A_irq>> per_cpu(vector_irq, 0)[LEGACY_VECTOR(desc->irq)] = desc->irq;
+ *   - xen/arch/x86/i8259.c|352| <<init_IRQ>> per_cpu(vector_irq, cpu)[FIRST_LEGACY_VECTOR + irq] = irq;
+ *   - xen/arch/x86/i8259.c|357| <<init_IRQ>> per_cpu(vector_irq, cpu)[IRQ0_VECTOR] = 0;
+ *   - xen/arch/x86/irq.c|134| <<__bind_irq_vector>> per_cpu(vector_irq, cpu)[vector] = irq;
+ *   - xen/arch/x86/irq.c|284| <<__clear_irq_vector>> ASSERT( per_cpu(vector_irq, cpu)[vector] == irq );
+ *   - xen/arch/x86/irq.c|285| <<__clear_irq_vector>> per_cpu(vector_irq, cpu)[vector] = ~irq;
+ *   - xen/arch/x86/irq.c|309| <<__clear_irq_vector>> ASSERT( per_cpu(vector_irq, cpu)[old_vector] == irq );
+ *   - xen/arch/x86/irq.c|311| <<__clear_irq_vector>> per_cpu(vector_irq, cpu)[old_vector] = ~irq;
+ *   - xen/arch/x86/irq.c|385| <<init_irq_data>> this_cpu(vector_irq)[vector] = INT_MIN;
+ *   - xen/arch/x86/irq.c|548| <<__assign_irq_vector>> if (per_cpu(vector_irq, new_cpu)[vector] >= 0)
+ *   - xen/arch/x86/irq.c|560| <<__assign_irq_vector>> per_cpu(vector_irq, new_cpu)[vector] = irq;
+ *   - xen/arch/x86/irq.c|617| <<setup_vector_irq>> per_cpu(vector_irq, cpu)[vector] = INT_MIN;
+ *   - xen/arch/x86/irq.c|631| <<setup_vector_irq>> per_cpu(vector_irq, cpu)[vector] = irq;
+ *   - xen/arch/x86/irq.c|691| <<irq_move_cleanup_interrupt>> irq = __get_cpu_var(vector_irq)[vector];
+ *   - xen/arch/x86/irq.c|729| <<irq_move_cleanup_interrupt>> __get_cpu_var(vector_irq)[vector] = ~irq;
+ *   - xen/arch/x86/irq.c|861| <<do_IRQ>> int irq = __get_cpu_var(vector_irq[vector]);
+ *   - xen/arch/x86/smpboot.c|1060| <<smp_intr_init>> per_cpu(vector_irq, cpu)[vector] = irq;
+ */
 DEFINE_PER_CPU(vector_irq_t, vector_irq);
 
 DEFINE_PER_CPU(struct cpu_user_regs *, __irq_regs);
@@ -112,6 +180,10 @@ static void trace_irq_mask(u32 event, int irq, int vector, cpumask_t *mask)
     trace_var(event, 1, sizeof(d), &d);
 }
 
+/*
+ * called by:
+ *   - arch/x86/irq.c|220| <<bind_irq_vector>> ret = __bind_irq_vector(irq, vector, cpu_mask);
+ */
 static int __init __bind_irq_vector(int irq, int vector, const cpumask_t *cpu_mask)
 {
     cpumask_t online_mask;
@@ -143,6 +215,10 @@ static int __init __bind_irq_vector(int irq, int vector, const cpumask_t *cpu_ma
     return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/io_apic.c|1896| <<check_timer>> if ((ret = bind_irq_vector(0, vector, &mask_all)))
+ */
 int __init bind_irq_vector(int irq, int vector, const cpumask_t *cpu_mask)
 {
     unsigned long flags;
@@ -157,18 +233,43 @@ int __init bind_irq_vector(int irq, int vector, const cpumask_t *cpu_mask)
 /*
  * Dynamic irq allocate and deallocation for MSI
  */
+/*
+ * called by:
+ *   - arch/x86/hpet.c|376| <<hpet_assign_irq>> if ( (irq = create_irq(NUMA_NO_NODE)) < 0 )
+ *   - arch/x86/irq.c|2036| <<map_domain_pirq>> irq = create_irq(NUMA_NO_NODE);
+ *   - arch/x86/irq.c|2673| <<allocate_and_map_msi_pirq>> irq = create_irq(NUMA_NO_NODE);
+ *   - drivers/passthrough/amd/iommu_init.c|783| <<set_iommu_interrupt_handler>> irq = create_irq(NUMA_NO_NODE);
+ *   - drivers/passthrough/vtd/iommu.c|1130| <<iommu_set_interrupt>> irq = create_irq(rhsa ? pxm_to_node(rhsa->proximity_domain)
+ */
 int create_irq(nodeid_t node)
 {
     int irq, ret;
     struct irq_desc *desc;
 
+    /*
+     * 在desktop默认:
+     * (XEN) IRQ limits: 120 GSI, 1432 MSI/MSI-X
+     * nr_irqs_gsi = 120, nr_irqs = 1552
+     *
+     * 设置nr_irqs=4096后:
+     * (XEN) IRQ limits: 120 GSI, 3976 MSI/MSI-X
+     */
+
     for (irq = nr_irqs_gsi; irq < nr_irqs; irq++)
     {
+        /* 返回&irq_desc[irq] */
         desc = irq_to_desc(irq);
+        /*
+	 * For use with irq_desc.arch.used
+	 *   #define IRQ_UNUSED      (0)
+	 *   #define IRQ_USED        (1)
+	 *   #define IRQ_RESERVED    (-1)
+	 */
         if (cmpxchg(&desc->arch.used, IRQ_UNUSED, IRQ_RESERVED) == IRQ_UNUSED)
            break;
     }
 
+    /* 上面也间接设置了返回值irq */
     if (irq >= nr_irqs)
          return -ENOSPC;
 
@@ -188,6 +289,7 @@ int create_irq(nodeid_t node)
     if (ret < 0)
     {
         desc->arch.used = IRQ_UNUSED;
+	/* 设置返回值irq的地方 */
         irq = ret;
     }
     else if ( hardware_domain )
@@ -438,6 +540,28 @@ static vmask_t *irq_get_used_vector_mask(int irq)
     return ret;
 }
 
+/*
+ * called by:
+ *   - xen/arch/x86/irq.c|591| <<assign_irq_vector>> ret = __assign_irq_vector(irq, desc, mask ?: TARGET_CPUS);
+ *   - xen/arch/x86/irq.c|780| <<set_desc_affinity>> ret = __assign_irq_vector(irq, desc, mask);
+ *
+ * 如果是assign_irq_vector()进来的, 如果之前mask是NULL, 则参数mask是genapic->target_cpus()
+ *
+ * 调用以下的代码可以获得allocation mask:
+ *
+ * 2274 static void dump_cpu_allocation(void)
+ * 2275 {
+ * 2276     int cpu, new_cpu;
+ * 2277 
+ * 2278     for_each_cpu(cpu, TARGET_CPUS) {
+ * 2279         const cpumask_t *mask = vector_allocation_cpumask(cpu);
+ * 2280 
+ * 2281         for_each_cpu(new_cpu, mask) {
+ * 2282             printk("allocation mask: %d: %d\n", cpu, new_cpu);
+ * 2283         }
+ * 2284     }
+ * 2285 }
+ */
 static int __assign_irq_vector(
     int irq, struct irq_desc *desc, const cpumask_t *mask)
 {
@@ -452,6 +576,11 @@ static int __assign_irq_vector(
      * Also, we've got to be careful not to trash gate
      * 0x80, because int 0x80 is hm, kind of importantish. ;)
      */
+    /*
+     * #define FIRST_DYNAMIC_VECTOR    0x20
+     * #define LAST_DYNAMIC_VECTOR     0xdf 
+     * #define NR_DYNAMIC_VECTORS      (LAST_DYNAMIC_VECTOR - FIRST_DYNAMIC_VECTOR + 1)
+     */
     static int current_vector = FIRST_DYNAMIC_VECTOR, current_offset = 0;
     int cpu, err, old_vector;
     cpumask_t tmp_mask;
@@ -489,6 +618,13 @@ static int __assign_irq_vector(
         if (!cpu_online(cpu))
             continue;
 
+	/*
+	 * 设置genapic的地方:
+	 *   - xen/arch/x86/apic.c|945| <<x2apic_bsp_setup>> genapic = apic_x2apic_probe();
+	 *      xen/arch/x86/apic.c|946| <<x2apic_bsp_setup>> printk("Switched to APIC driver %s.\n", genapic->name);
+	 *
+	 * "x2apic_cluster"是vector_allocation_cpumask_x2apic_cluster()
+	 */
         cpumask_and(&tmp_mask, vector_allocation_cpumask(cpu),
                     &cpu_online_map);
 
@@ -546,10 +682,18 @@ next:
     return err;
 }
 
+/*
+ * called by:
+ *   - xen/arch/x86/io_apic.c|1032| <<setup_IO_APIC_irqs>> vector = assign_irq_vector(irq, NULL);
+ *   - xen/arch/x86/io_apic.c|2232| <<io_apic_set_pci_routing>> vector = assign_irq_vector(irq, NULL);
+ *   - xen/arch/x86/io_apic.c|2396| <<ioapic_guest_write>> ret = assign_irq_vector(irq, NULL);
+ *   - xen/arch/x86/irq.c|204| <<create_irq>> ret = assign_irq_vector(irq, mask);
+ */
 int assign_irq_vector(int irq, const cpumask_t *mask)
 {
     int ret;
     unsigned long flags;
+    /* 相当于&irq_desc[irq] */
     struct irq_desc *desc = irq_to_desc(irq);
     
     BUG_ON(irq >= nr_irqs || irq <0);
@@ -568,6 +712,10 @@ int assign_irq_vector(int irq, const cpumask_t *mask)
  * Initialize vector_irq on a new cpu. This function must be called
  * with vector_lock held.
  */
+/*
+ * called by:
+ *   - arch/x86/smpboot.c|372| <<start_secondary>> setup_vector_irq(cpu);
+ */
 void setup_vector_irq(unsigned int cpu)
 {
     unsigned int irq, vector;
@@ -859,6 +1007,7 @@ void do_IRQ(struct cpu_user_regs *regs)
         goto out_no_unlock;
     }
 
+    /* 就是&irq_desc[irq] */
     desc = irq_to_desc(irq);
 
     spin_lock(&desc->lock);
@@ -1863,12 +2012,16 @@ static inline bool is_free_pirq(const struct domain *d,
         pirq->arch.hvm.emuirq == IRQ_UNBOUND));
 }
 
+/*
+ * 搜索返回一个没人用的pirq
+ */
 int get_free_pirq(struct domain *d, int type)
 {
     int i;
 
     ASSERT(spin_is_locked(&d->event_lock));
 
+    /* gsi用的是nr_irqs_gsi之前的 */
     if ( type == MAP_PIRQ_TYPE_GSI )
     {
         for ( i = 16; i < nr_irqs_gsi; i++ )
@@ -1878,6 +2031,7 @@ int get_free_pirq(struct domain *d, int type)
                 return i;
             }
     }
+    /* msi用的是nr_irqs_gsi之后的?? */
     for ( i = d->nr_pirqs - 1; i >= nr_irqs_gsi; i-- )
         if ( is_free_pirq(d, pirq_info(d, i)) )
         {
@@ -1909,6 +2063,12 @@ int get_free_pirqs(struct domain *d, unsigned int nr)
 
 #define MAX_MSI_IRQS 32 /* limited by MSI capability struct properties */
 
+/*
+ * called and used by:
+ *   - arch/x86/io_apic.c|2411| <<ioapic_guest_write>> ret = map_domain_pirq(hardware_domain, pirq, irq,
+ *   - arch/x86/irq.c|2807| <<allocate_and_map_gsi_pirq>> ret = map_domain_pirq(d, pirq, irq, MAP_PIRQ_TYPE_GSI, NULL);
+ *   - arch/x86/irq.c|2870| <<allocate_and_map_msi_pirq>> ret = map_domain_pirq(d, pirq, irq, type, msi);
+ */
 int map_domain_pirq(
     struct domain *d, int pirq, int irq, int type, void *data)
 {
@@ -2561,6 +2721,9 @@ bool hvm_domain_use_pirq(const struct domain *d, const struct pirq *pirq)
     return is_hvm_domain(d) && pirq && pirq->arch.hvm.emuirq != IRQ_UNBOUND;
 }
 
+/*
+ * 核心思想是搜索返回一个没人用的pirq
+ */
 static int allocate_pirq(struct domain *d, int index, int pirq, int irq,
                          int type, int *nr)
 {
@@ -2598,6 +2761,7 @@ static int allocate_pirq(struct domain *d, int index, int pirq, int irq,
         }
         else
         {
+            /* 搜索返回一个没人用的pirq */
             pirq = get_free_pirq(d, type);
             if ( pirq < 0 )
                 dprintk(XENLOG_G_ERR, "dom%d: no free pirq\n", d->domain_id);
@@ -2656,6 +2820,10 @@ int allocate_and_map_gsi_pirq(struct domain *d, int index, int *pirq_p)
     return ret;
 }
 
+/*
+ * called by only:
+ *   - arch/x86/physdev.c|129| <<physdev_map_pirq>> ret = allocate_and_map_msi_pirq(d, *index, pirq_p, type, msi);
+ */
 int allocate_and_map_msi_pirq(struct domain *d, int index, int *pirq_p,
                               int type, struct msi_info *msi)
 {
@@ -2695,6 +2863,9 @@ int allocate_and_map_msi_pirq(struct domain *d, int index, int *pirq_p,
     pcidevs_lock();
     /* Verify or get pirq. */
     spin_lock(&d->event_lock);
+    /*
+     * 核心思想是搜索返回一个没人用的pirq
+     */
     pirq = allocate_pirq(d, index, *pirq_p, irq, type, &msi->entry_nr);
     if ( pirq < 0 )
     {
diff --git a/xen/arch/x86/msi.c b/xen/arch/x86/msi.c
index 4652b98..d8be6b7 100644
--- a/xen/arch/x86/msi.c
+++ b/xen/arch/x86/msi.c
@@ -585,6 +585,10 @@ static struct msi_desc *alloc_msi_entry(unsigned int nr)
     return entry;
 }
 
+/*
+ * called by:
+ *   - arch/x86/irq.c|2163| <<map_domain_pirq>> while ( !(ret = setup_msi_irq(desc, msi_desc + nr)) )
+ */
 int setup_msi_irq(struct irq_desc *desc, struct msi_desc *msidesc)
 {
     const struct pci_dev *pdev = msidesc->dev;
@@ -614,6 +618,11 @@ int setup_msi_irq(struct irq_desc *desc, struct msi_desc *msidesc)
     return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/msi.c|606| <<setup_msi_irq>> rc = __setup_msi_irq(desc, msidesc,
+ *   - drivers/passthrough/amd/iommu_init.c|814| <<set_iommu_interrupt_handler>> ret = __setup_msi_irq(irq_to_desc(irq), &iommu->msi, handler);
+ */
 int __setup_msi_irq(struct irq_desc *desc, struct msi_desc *msidesc,
                     hw_irq_controller *handler)
 {
@@ -681,6 +690,10 @@ static struct msi_desc *find_msi_entry(struct pci_dev *dev,
  * multiple messages. A return of zero indicates the successful setup
  * of an entry zero with the new MSI irq or non-zero for otherwise.
  **/
+/*
+ * called by:
+ *   - arch/x86/msi.c|1077| <<__pci_enable_msi>> return msi_capability_init(pdev, msi->irq, desc, msi->entry_nr);
+ */
 static int msi_capability_init(struct pci_dev *dev,
                                int irq,
                                struct msi_desc **desc,
@@ -1037,6 +1050,10 @@ static int msix_capability_init(struct pci_dev *dev,
  * irq or non-zero for otherwise.
  **/
 
+/*
+ * called by:
+ *   - arch/x86/msi.c|1255| <<pci_enable_msi>> __pci_enable_msi(msi, desc);
+ */
 static int __pci_enable_msi(struct msi_info *msi, struct msi_desc **desc)
 {
     struct pci_dev *pdev;
@@ -1093,6 +1110,10 @@ static void __pci_disable_msi(struct msi_desc *entry)
  * of irqs available. Driver should use the returned value to re-send
  * its request.
  **/
+/*
+ * called by:
+ *   - arch/x86/msi.c|1262| <<pci_enable_msi>> return msi->table_base ? __pci_enable_msix(msi, desc) :
+ */
 static int __pci_enable_msix(struct msi_info *msi, struct msi_desc **desc)
 {
     int pos, nr_entries;
@@ -1231,6 +1252,10 @@ int pci_prepare_msix(u16 seg, u8 bus, u8 devfn, bool off)
  * Notice: only construct the msi_desc
  * no change to irq_desc here, and the interrupt is masked
  */
+/*
+ * called by:
+ *   - arch/x86/irq.c|2142| <<map_domain_pirq>> ret = pci_enable_msi(msi, &msi_desc);
+ */
 int pci_enable_msi(struct msi_info *msi, struct msi_desc **desc)
 {
     ASSERT(pcidevs_locked());
@@ -1272,6 +1297,10 @@ void pci_cleanup_msi(struct pci_dev *pdev)
     msi_free_irqs(pdev);
 }
 
+/*
+ * called by:
+ *   - arch/x86/pci.c|95| <<pci_conf_write_intercept>> rc = pci_msi_conf_write_intercept(pdev, reg, size, data);
+ */
 int pci_msi_conf_write_intercept(struct pci_dev *pdev, unsigned int reg,
                                  unsigned int size, uint32_t *data)
 {
diff --git a/xen/arch/x86/pci.c b/xen/arch/x86/pci.c
index a9decd4..db0f2d9 100644
--- a/xen/arch/x86/pci.c
+++ b/xen/arch/x86/pci.c
@@ -11,6 +11,14 @@
 
 static DEFINE_SPINLOCK(pci_config_lock);
 
+/*
+ * called by:
+ *   - arch/x86/pv/emul-priv-op.c|274| <<guest_io_read>> sub_data = pci_conf_read(currd->arch.pci_cf8, port & 3, size);
+ *   - arch/x86/x86_64/pci.c|28| <<pci_conf_read8>> return pci_conf_read(PCI_CONF_ADDRESS(bus, dev, func, reg), reg & 3, 1);
+ *   - arch/x86/x86_64/pci.c|46| <<pci_conf_read16>> return pci_conf_read(PCI_CONF_ADDRESS(bus, dev, func, reg), reg & 2, 2);
+ *   - arch/x86/x86_64/pci.c|64| <<pci_conf_read32>> return pci_conf_read(PCI_CONF_ADDRESS(bus, dev, func, reg), 0, 4);
+ *   - include/xen/pci.h|159| <<pci_conf_read32>> uint32_t pci_conf_read(uint32_t cf8, uint8_t offset, uint8_t bytes);
+ */
 uint32_t pci_conf_read(uint32_t cf8, uint8_t offset, uint8_t bytes)
 {
     unsigned long flags;
@@ -43,6 +51,14 @@ uint32_t pci_conf_read(uint32_t cf8, uint8_t offset, uint8_t bytes)
     return value;
 }
 
+/*
+ * called by:
+ *   - arch/x86/pv/emul-priv-op.c|412| <<guest_io_write>> pci_conf_write(currd->arch.pci_cf8, port & 3, size, data);
+ *   - arch/x86/x86_64/pci.c|77| <<pci_conf_write8>> pci_conf_write(PCI_CONF_ADDRESS(bus, dev, func, reg), reg & 3, 1, data);
+ *   - arch/x86/x86_64/pci.c|90| <<pci_conf_write16>> pci_conf_write(PCI_CONF_ADDRESS(bus, dev, func, reg), reg & 2, 2, data);
+ *   - arch/x86/x86_64/pci.c|103| <<pci_conf_write32>> pci_conf_write(PCI_CONF_ADDRESS(bus, dev, func, reg), 0, 4, data);
+ *   - include/xen/pci.h|160| <<pci_conf_write32>> void pci_conf_write(uint32_t cf8, uint8_t offset, uint8_t bytes, uint32_t data);
+ */
 void pci_conf_write(uint32_t cf8, uint8_t offset, uint8_t bytes, uint32_t data)
 {
     unsigned long flags;
@@ -69,6 +85,11 @@ void pci_conf_write(uint32_t cf8, uint8_t offset, uint8_t bytes, uint32_t data)
     spin_unlock_irqrestore(&pci_config_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm.c|4444| <<mmcfg_intercept_write>> if ( pci_conf_write_intercept(mmio_ctxt->seg, mmio_ctxt->bdf,
+ *   - arch/x86/pv/emul-priv-op.c|216| <<pci_cfg_ok>> pci_conf_write_intercept(0, machine_bdf, start, size, write) >= 0;
+ */
 int pci_conf_write_intercept(unsigned int seg, unsigned int bdf,
                              unsigned int reg, unsigned int size,
                              uint32_t *data)
diff --git a/xen/arch/x86/physdev.c b/xen/arch/x86/physdev.c
index a5fedca..3ab6973 100644
--- a/xen/arch/x86/physdev.c
+++ b/xen/arch/x86/physdev.c
@@ -88,6 +88,12 @@ static int physdev_hvm_map_pirq(
     return ret;
 }
 
+/*
+ * called only by:
+ *   - arch/x86/physdev.c|335| <<XEN_GUEST_HANDLE_PARAM>> ret = physdev_map_pirq(map.domid, map.type, &map.index, &map.pirq,
+ *
+ * 从desktop nvme进来的时候猜测, type=MAP_PIRQ_TYPE_MSI, *index=-1
+ */
 int physdev_map_pirq(domid_t domid, int type, int *index, int *pirq_p,
                      struct msi_info *msi)
 {
@@ -126,6 +132,9 @@ int physdev_map_pirq(domid_t domid, int type, int *index, int *pirq_p,
             msi->entry_nr = 1;
         /* fallthrough */
     case MAP_PIRQ_TYPE_MULTI_MSI:
+        /*
+	 * 上面desktop的nvme进来, 猜测msi->entry_nr每次分别是0, 1, 2, 3, 4, 5, 6, 7
+         */
         ret = allocate_and_map_msi_pirq(d, *index, pirq_p, type, msi);
         break;
 
@@ -313,6 +322,9 @@ ret_t do_physdev_op(int cmd, XEN_GUEST_HANDLE_PARAM(void) arg)
 
         switch ( map.type )
         {
+        /*
+	 * 在desktop nvme上猜测这里应该是MAP_PIRQ_TYPE_MSI_SEG
+	 */
         case MAP_PIRQ_TYPE_MSI_SEG:
             map.type = MAP_PIRQ_TYPE_MSI;
             msi.seg = map.bus >> 16;
@@ -332,6 +344,11 @@ ret_t do_physdev_op(int cmd, XEN_GUEST_HANDLE_PARAM(void) arg)
         msi.devfn = map.devfn;
         msi.entry_nr = map.entry_nr;
         msi.table_base = map.table_base;
+        /*
+	 * 因为上面在desktop nvme上猜测应该是MAP_PIRQ_TYPE_MSI_SEG
+	 *
+	 * 所以上面把map.type换成了MAP_PIRQ_TYPE_MSI, msi.seg包含了seg的信息
+         */
         ret = physdev_map_pirq(map.domid, map.type, &map.index, &map.pirq,
                                &msi);
 
diff --git a/xen/arch/x86/pv/emul-priv-op.c b/xen/arch/x86/pv/emul-priv-op.c
index 2f92645..52b58a1 100644
--- a/xen/arch/x86/pv/emul-priv-op.c
+++ b/xen/arch/x86/pv/emul-priv-op.c
@@ -177,6 +177,11 @@ static bool admin_io_okay(unsigned int port, unsigned int bytes,
     return ioports_access_permitted(d, port, port + bytes - 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/pv/emul-priv-op.c|268| <<guest_io_read>> if ( pci_cfg_ok(currd, port & 3, size, NULL) )
+ *   - arch/x86/pv/emul-priv-op.c|406| <<guest_io_write>> if ( pci_cfg_ok(currd, port & 3, size, &data) )
+ */
 static bool pci_cfg_ok(struct domain *currd, unsigned int start,
                        unsigned int size, uint32_t *write)
 {
diff --git a/xen/common/irq.c b/xen/common/irq.c
index f42512d..1eadfc7 100644
--- a/xen/common/irq.c
+++ b/xen/common/irq.c
@@ -5,6 +5,7 @@ int init_one_irq_desc(struct irq_desc *desc)
 {
     int err;
 
+    /* 如果(desc)->handler != NULL */
     if (irq_desc_initialized(desc))
         return 0;
 
diff --git a/xen/drivers/passthrough/io.c b/xen/drivers/passthrough/io.c
index 8f16e6c..979fc97 100644
--- a/xen/drivers/passthrough/io.c
+++ b/xen/drivers/passthrough/io.c
@@ -25,6 +25,15 @@
 #include <asm/hvm/support.h>
 #include <asm/io_apic.h>
 
+/*
+ * 在以下使用dpci_list:
+ *   - drivers/passthrough/io.c|71| <<raise_softirq_for>> list_add_tail(&pirq_dpci->softirq_list, &this_cpu(dpci_list));
+ *   - drivers/passthrough/io.c|1053| <<dpci_softirq>> list_splice_init(&per_cpu(dpci_list, cpu), &our_list);
+ *   - drivers/passthrough/io.c|1072| <<dpci_softirq>> list_add_tail(&pirq_dpci->softirq_list, &this_cpu(dpci_list));
+ *   - drivers/passthrough/io.c|1098| <<cpu_callback>> INIT_LIST_HEAD(&per_cpu(dpci_list, cpu));
+ *   - drivers/passthrough/io.c|1109| <<cpu_callback>> ASSERT(list_empty(&per_cpu(dpci_list, cpu)));
+ *   - drivers/passthrough/io.c|1125| <<setup_dpci_softirq>> INIT_LIST_HEAD(&per_cpu(dpci_list, cpu));
+ */
 static DEFINE_PER_CPU(struct list_head, dpci_list);
 
 /*
@@ -68,6 +77,15 @@ static void raise_softirq_for(struct hvm_pirq_dpci *pirq_dpci)
     get_knownalive_domain(pirq_dpci->dom);
 
     local_irq_save(flags);
+    /*
+     * 在以下使用dpci_list:
+     *   - drivers/passthrough/io.c|71| <<raise_softirq_for>> list_add_tail(&pirq_dpci->softirq_list, &this_cpu(dpci_list));
+     *   - drivers/passthrough/io.c|1053| <<dpci_softirq>> list_splice_init(&per_cpu(dpci_list, cpu), &our_list);
+     *   - drivers/passthrough/io.c|1072| <<dpci_softirq>> list_add_tail(&pirq_dpci->softirq_list, &this_cpu(dpci_list));
+     *   - drivers/passthrough/io.c|1098| <<cpu_callback>> INIT_LIST_HEAD(&per_cpu(dpci_list, cpu));
+     *   - drivers/passthrough/io.c|1109| <<cpu_callback>> ASSERT(list_empty(&per_cpu(dpci_list, cpu)));
+     *   - drivers/passthrough/io.c|1125| <<setup_dpci_softirq>> INIT_LIST_HEAD(&per_cpu(dpci_list, cpu));
+     */
     list_add_tail(&pirq_dpci->softirq_list, &this_cpu(dpci_list));
     local_irq_restore(flags);
 
@@ -275,6 +293,11 @@ static struct vcpu *vector_hashing_dest(const struct domain *d,
     return dest;
 }
 
+/*
+ * called by:
+ *   - arch/x86/domctl.c|729| <<arch_do_domctl()::XEN_DOMCTL_bind_pt_irq>> ret = pt_irq_create_bind(d, bind);
+ *   - arch/x86/hvm/vioapic.c|193| <<vioapic_hwdom_map_gsi>> ret = pt_irq_create_bind(currd, &pt_irq_bind);
+ */
 int pt_irq_create_bind(
     struct domain *d, const struct xen_domctl_bind_pt_irq *pt_irq_bind)
 {
@@ -795,6 +818,10 @@ int pt_pirq_iterate(struct domain *d,
     return rc;
 }
 
+/*
+ * called by only:
+ *   - arch/x86/irq.c|1340| <<__do_IRQ_guest>> if ( !is_hvm_domain(d) || !hvm_do_IRQ_dpci(d, pirq) )
+ */
 int hvm_do_IRQ_dpci(struct domain *d, struct pirq *pirq)
 {
     struct hvm_irq_dpci *dpci = domain_get_irq_dpci(d);
@@ -812,6 +839,11 @@ int hvm_do_IRQ_dpci(struct domain *d, struct pirq *pirq)
 }
 
 /* called with d->event_lock held */
+/*
+ * called by:
+ *   - drivers/passthrough/io.c|847| <<_hvm_dpci_msi_eoi>> __msi_pirq_eoi(pirq_dpci);
+ *   - drivers/passthrough/io.c|913| <<hvm_dirq_assist>> __msi_pirq_eoi(pirq_dpci);
+ */
 static void __msi_pirq_eoi(struct hvm_pirq_dpci *pirq_dpci)
 {
     irq_desc_t *desc;
@@ -829,6 +861,10 @@ static void __msi_pirq_eoi(struct hvm_pirq_dpci *pirq_dpci)
     }
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/io.c|861| <<hvm_dpci_msi_eoi>> pt_pirq_iterate(d, _hvm_dpci_msi_eoi, (void *)(long )vector);
+ */
 static int _hvm_dpci_msi_eoi(struct domain *d,
                              struct hvm_pirq_dpci *pirq_dpci, void *arg)
 {
@@ -852,6 +888,10 @@ static int _hvm_dpci_msi_eoi(struct domain *d,
     return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/vlapic.c|440| <<vlapic_handle_EOI>> hvm_dpci_msi_eoi(d, vector);
+ */
 void hvm_dpci_msi_eoi(struct domain *d, int vector)
 {
     if ( !iommu_enabled || !hvm_domain_irq(d)->dpci )
@@ -862,6 +902,10 @@ void hvm_dpci_msi_eoi(struct domain *d, int vector)
     spin_unlock(&d->event_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/io.c|1074| <<dpci_softirq>> hvm_dirq_assist(d, pirq_dpci);
+ */
 static void hvm_dirq_assist(struct domain *d, struct hvm_pirq_dpci *pirq_dpci)
 {
     if ( unlikely(!hvm_domain_irq(d)->dpci) && !is_hardware_domain(d) )
@@ -1024,6 +1068,15 @@ static void dpci_softirq(void)
     LIST_HEAD(our_list);
 
     local_irq_disable();
+    /*
+     * 在以下使用dpci_list:
+     *   - drivers/passthrough/io.c|71| <<raise_softirq_for>> list_add_tail(&pirq_dpci->softirq_list, &this_cpu(dpci_list));
+     *   - drivers/passthrough/io.c|1053| <<dpci_softirq>> list_splice_init(&per_cpu(dpci_list, cpu), &our_list);
+     *   - drivers/passthrough/io.c|1072| <<dpci_softirq>> list_add_tail(&pirq_dpci->softirq_list, &this_cpu(dpci_list));
+     *   - drivers/passthrough/io.c|1098| <<cpu_callback>> INIT_LIST_HEAD(&per_cpu(dpci_list, cpu));
+     *   - drivers/passthrough/io.c|1109| <<cpu_callback>> ASSERT(list_empty(&per_cpu(dpci_list, cpu)));
+     *   - drivers/passthrough/io.c|1125| <<setup_dpci_softirq>> INIT_LIST_HEAD(&per_cpu(dpci_list, cpu));
+     */
     list_splice_init(&per_cpu(dpci_list, cpu), &our_list);
     local_irq_enable();
 
diff --git a/xen/include/asm-x86/domain.h b/xen/include/asm-x86/domain.h
index f699119..3b2f153 100644
--- a/xen/include/asm-x86/domain.h
+++ b/xen/include/asm-x86/domain.h
@@ -421,6 +421,14 @@ struct arch_domain
     uint32_t emulation_flags;
 } __cacheline_aligned;
 
+/*
+ * XEN_X86_EMU_LAPIC在以下使用:
+ *   - arch/x86/domain.c|400| <<emulation_flags_ok>> emflags != (XEN_X86_EMU_LAPIC|XEN_X86_EMU_IOAPIC) )
+ *   - arch/x86/domain.c|403| <<emulation_flags_ok>> emflags != XEN_X86_EMU_ALL && emflags != XEN_X86_EMU_LAPIC )
+ *   - arch/x86/setup.c|1585| <<__start_xen>> config.emulation_flags = XEN_X86_EMU_LAPIC|XEN_X86_EMU_IOAPIC;
+ *   - include/asm-x86/domain.h|424| <<has_vlapic>> #define has_vlapic(d) (!!((d)->arch.emulation_flags & XEN_X86_EMU_LAPIC))
+ *   - include/public/arch-x86/xen.h|298| <<XEN_X86_EMU_ALL>> #define XEN_X86_EMU_ALL (XEN_X86_EMU_LAPIC | XEN_X86_EMU_HPET | \
+ */
 #define has_vlapic(d)      (!!((d)->arch.emulation_flags & XEN_X86_EMU_LAPIC))
 #define has_vhpet(d)       (!!((d)->arch.emulation_flags & XEN_X86_EMU_HPET))
 #define has_vpm(d)         (!!((d)->arch.emulation_flags & XEN_X86_EMU_PM))
diff --git a/xen/include/asm-x86/hvm/domain.h b/xen/include/asm-x86/hvm/domain.h
index 7f128c0..dbbcd02 100644
--- a/xen/include/asm-x86/hvm/domain.h
+++ b/xen/include/asm-x86/hvm/domain.h
@@ -162,6 +162,16 @@ struct hvm_domain {
     bool_t                 is_in_uc_mode;
 
     /* hypervisor intercepted msix table */
+    /*
+     * used by:
+     *   - arch/x86/hvm/vmsi.c|174| <<msixtbl_initialised>> return !!d->arch.hvm_domain.msixtbl_list.next;
+     *   - arch/x86/hvm/vmsi.c|183| <<msixtbl_find_entry>> list_for_each_entry( entry, &d->arch.hvm_domain.msixtbl_list, list )
+     *   - arch/x86/hvm/vmsi.c|445| <<add_msixtbl_entry>> list_add_rcu(&entry->list, &d->arch.hvm_domain.msixtbl_list);
+     *   - arch/x86/hvm/vmsi.c|498| <<msixtbl_pt_register>> list_for_each_entry( entry, &d->arch.hvm_domain.msixtbl_list, list )
+     *   - arch/x86/hvm/vmsi.c|557| <<msixtbl_pt_unregister>> list_for_each_entry( entry, &d->arch.hvm_domain.msixtbl_list, list )
+     *   - arch/x86/hvm/vmsi.c|583| <<msixtbl_init>> INIT_LIST_HEAD(&d->arch.hvm_domain.msixtbl_list);
+     *   - arch/x86/hvm/vmsi.c|603| <<msixtbl_pt_cleanup>> &d->arch.hvm_domain.msixtbl_list, list )
+     */
     struct list_head       msixtbl_list;
 
     struct viridian_domain viridian;
diff --git a/xen/include/asm-x86/hvm/hvm.h b/xen/include/asm-x86/hvm/hvm.h
index 6ecad33..6088b9c 100644
--- a/xen/include/asm-x86/hvm/hvm.h
+++ b/xen/include/asm-x86/hvm/hvm.h
@@ -38,6 +38,17 @@ extern bool_t opt_hvm_fep;
 enum hvm_intsrc {
     hvm_intsrc_none,
     hvm_intsrc_pic,
+    /*
+     * used by:
+     *   - arch/x86/hvm/hvm.c|3822| <<hvm_interrupt_blocked>> if ( intack.source == hvm_intsrc_lapic )
+     *   - arch/x86/hvm/irq.c|516| <<hvm_vcpu_ack_pending_irq>> case hvm_intsrc_lapic:
+     *   - arch/x86/hvm/svm/intr.c|127| <<svm_enable_intr_window>> intr.fields.ign_tpr = (intack.source != hvm_intsrc_lapic);
+     *   - arch/x86/hvm/svm/nestedsvm.c|1567| <<nestedsvm_vcpu_interrupt>> case hvm_intsrc_lapic:
+     *   - arch/x86/hvm/vmx/intr.c|196| <<nvmx_intr_intercept>> intack.source == hvm_intsrc_lapic )
+     *   - arch/x86/hvm/vmx/intr.c|282| <<vmx_intr_assist>> ASSERT(intack.source == hvm_intsrc_lapic);
+     *   - arch/x86/hvm/vmx/vvmx.c|1335| <<nvmx_update_apicv>> nvmx->intr.source == hvm_intsrc_lapic &&
+     *   - arch/x86/hvm/vpt.c|94| <<pt_irq_vector>> ASSERT(src == hvm_intsrc_lapic);
+     */
     hvm_intsrc_lapic,
     hvm_intsrc_nmi,
     hvm_intsrc_mce,
diff --git a/xen/include/asm-x86/hvm/irq.h b/xen/include/asm-x86/hvm/irq.h
index f756cb5..5ceed6f 100644
--- a/xen/include/asm-x86/hvm/irq.h
+++ b/xen/include/asm-x86/hvm/irq.h
@@ -132,9 +132,30 @@ struct dev_intx_gsi_link {
 #define HVM_IRQ_DPCI_TRANSLATE       (1u << _HVM_IRQ_DPCI_TRANSLATE_SHIFT)
 
 struct hvm_gmsi_info {
+    /*
+     * 更新msi.gvec的地方:
+     *   - drivers/passthrough/io.c|376| <<pt_irq_create_bind>> pirq_dpci->gmsi.gvec = pt_irq_bind->u.msi.gvec;
+     *   - drivers/passthrough/io.c|410| <<pt_irq_create_bind>> pirq_dpci->gmsi.gvec = 0;
+     *   - drivers/passthrough/io.c|435| <<pt_irq_create_bind>> pirq_dpci->gmsi.gvec = pt_irq_bind->u.msi.gvec;
+     *
+     * 使用XEN_DOMCTL_bind_pt_irq的例子:
+     *   - libxc/xc_domain.c|1747| <<xc_domain_update_msi_irq>> domctl.cmd = XEN_DOMCTL_bind_pt_irq;
+     *   - libxc/xc_domain.c|1801| <<xc_domain_bind_pt_irq_int>> domctl.cmd = XEN_DOMCTL_bind_pt_irq;
+     *
+     * qemu-xen-traditional中调用xc_domain_update_msi_irq()的例子:
+     *   - hw/pt-msi.c|144| <<pt_msi_update>> ret = xc_domain_update_msi_irq(xc_handle, domid, gvec,
+     *   - hw/pt-msi.c|322| <<pt_msix_update_one>> ret = xc_domain_update_msi_irq(xc_handle, domid, gvec, pirq, gflags,
+     */
     uint32_t gvec;
     uint32_t gflags;
     int dest_vcpu_id; /* -1 :multi-dest, non-negative: dest_vcpu_id */
+    /*
+     * 使用posted的地方:
+     *   - arch/x86/hvm/hvm.c|471| <<hvm_migrate_pirq>> !pirq_dpci->gmsi.posted &&
+     *   - drivers/passthrough/io.c|450| <<pt_irq_create_bind>> pirq_dpci->gmsi.posted = false;
+     *   - drivers/passthrough/io.c|458| <<pt_irq_create_bind>> pirq_dpci->gmsi.posted = true;
+     *   - drivers/passthrough/io.c|739| <<pt_irq_destroy_bind>> else if ( pirq_dpci && pirq_dpci->gmsi.posted )
+     */
     bool posted; /* directly deliver to guest via VT-d PI? */
 };
 
diff --git a/xen/include/asm-x86/hvm/vlapic.h b/xen/include/asm-x86/hvm/vlapic.h
index 212c36b..405c7c8 100644
--- a/xen/include/asm-x86/hvm/vlapic.h
+++ b/xen/include/asm-x86/hvm/vlapic.h
@@ -85,6 +85,16 @@ struct vlapic {
     spinlock_t               esr_lock;
     struct periodic_time     pt;
     s_time_t                 timer_last_update;
+    /*
+     * regs_page使用的地方:
+     *   - arch/x86/hvm/vlapic.c|1586| <<vlapic_init>> if (vlapic->regs_page == NULL)
+     *   - arch/x86/hvm/vlapic.c|1588| <<vlapic_init>> vlapic->regs_page = alloc_domheap_page(v->domain, MEMF_no_owner);
+     *   - arch/x86/hvm/vlapic.c|1589| <<vlapic_init>> if ( vlapic->regs_page == NULL )
+     *   - arch/x86/hvm/vlapic.c|1599| <<vlapic_init>> vlapic->regs = __map_domain_page_global(vlapic->regs_page);
+     *   - arch/x86/hvm/vlapic.c|1634| <<vlapic_destroy>> free_domheap_page(vlapic->regs_page);
+     *   - arch/x86/hvm/vmx/vmcs.c|1241| <<construct_vmcs>> page_to_maddr(vcpu_vlapic(v)->regs_page));
+     *   - arch/x86/hvm/vmx/vmx.c|3029| <<vmx_install_vlapic_mapping>> virt_page_ma = page_to_maddr(vcpu_vlapic(v)->regs_page);
+     */
     struct page_info         *regs_page;
     /* INIT-SIPI-SIPI work gets deferred to a tasklet. */
     struct {
diff --git a/xen/include/asm-x86/hvm/vmx/vmcs.h b/xen/include/asm-x86/hvm/vmx/vmcs.h
index 8fb9e3c..8b010c1 100644
--- a/xen/include/asm-x86/hvm/vmx/vmcs.h
+++ b/xen/include/asm-x86/hvm/vmx/vmcs.h
@@ -60,6 +60,14 @@ struct ept_data {
 #define _VMX_DOMAIN_PML_ENABLED    0
 #define VMX_DOMAIN_PML_ENABLED     (1ul << _VMX_DOMAIN_PML_ENABLED)
 struct vmx_domain {
+    /*
+     * apic_access_mfn在以下使用:
+     *   - arch/x86/hvm/mtrr.c|787| <<epte_get_entry_emt>> if ( (mfn_x(mfn) ^ d->arch.hvm_domain.vmx.apic_access_mfn) >> order )
+     *   - arch/x86/hvm/vmx/vmx.c|3005| <<vmx_alloc_vlapic_mapping>> d->arch.hvm_domain.vmx.apic_access_mfn = mfn;
+     *   - arch/x86/hvm/vmx/vmx.c|3014| <<vmx_free_vlapic_mapping>> unsigned long mfn = d->arch.hvm_domain.vmx.apic_access_mfn;
+     *   - arch/x86/hvm/vmx/vmx.c|3024| <<vmx_install_vlapic_mapping>> if ( v->domain->arch.hvm_domain.vmx.apic_access_mfn == 0 )
+     *   - arch/x86/hvm/vmx/vmx.c|3030| <<vmx_install_vlapic_mapping>> apic_page_ma = v->domain->arch.hvm_domain.vmx.apic_access_mfn;
+     */
     unsigned long apic_access_mfn;
     /* VMX_DOMAIN_* */
     unsigned int status;
@@ -389,7 +397,22 @@ enum vmcs_field {
     VM_ENTRY_MSR_LOAD_ADDR          = 0x0000200a,
     PML_ADDRESS                     = 0x0000200e,
     TSC_OFFSET                      = 0x00002010,
+    /*
+     * VIRTUAL_APIC_PAGE_ADDR在以下使用:
+     *   - arch/x86/hvm/vmx/vmcs.c|1250| <<construct_vmcs>> __vmwrite(VIRTUAL_APIC_PAGE_ADDR,
+     *   - arch/x86/hvm/vmx/vmx.c|3034| <<vmx_install_vlapic_mapping>> __vmwrite(VIRTUAL_APIC_PAGE_ADDR, virt_page_ma);
+     *   - arch/x86/hvm/vmx/vvmx.c|721| <<nvmx_update_virtual_apic_address>> vapic_gpfn = get_vvmcs(v, VIRTUAL_APIC_PAGE_ADDR) >> PAGE_SHIFT;
+     *   - arch/x86/hvm/vmx/vvmx.c|724| <<nvmx_update_virtual_apic_address>> __vmwrite(VIRTUAL_APIC_PAGE_ADDR, page_to_maddr(vapic_pg));
+     *   - arch/x86/hvm/vmx/vvmx.c|728| <<nvmx_update_virtual_apic_address>> __vmwrite(VIRTUAL_APIC_PAGE_ADDR, 0);
+     */
     VIRTUAL_APIC_PAGE_ADDR          = 0x00002012,
+    /*
+     * APIC_ACCESS_ADDR在以下使用:
+     *   - arch/x86/hvm/vmx/vmx.c|3035| <<vmx_install_vlapic_mapping>> __vmwrite(APIC_ACCESS_ADDR, apic_page_ma);
+     *   - arch/x86/hvm/vmx/vvmx.c|700| <<nvmx_update_apic_access_address>> apic_gpfn = get_vvmcs(v, APIC_ACCESS_ADDR) >> PAGE_SHIFT;
+     *   - arch/x86/hvm/vmx/vvmx.c|703| <<nvmx_update_apic_access_address>> __vmwrite(APIC_ACCESS_ADDR, page_to_maddr(apic_pg));
+     *   - arch/x86/hvm/vmx/vvmx.c|707| <<nvmx_update_apic_access_address>> __vmwrite(APIC_ACCESS_ADDR, 0);
+     */
     APIC_ACCESS_ADDR                = 0x00002014,
     PI_DESC_ADDR                    = 0x00002016,
     VM_FUNCTION_CONTROL             = 0x00002018,
diff --git a/xen/include/asm-x86/mach-generic/mach_apic.h b/xen/include/asm-x86/mach-generic/mach_apic.h
index 03e9e8a..c7ba8f5 100644
--- a/xen/include/asm-x86/mach-generic/mach_apic.h
+++ b/xen/include/asm-x86/mach-generic/mach_apic.h
@@ -16,6 +16,11 @@
 #define init_apic_ldr (genapic->init_apic_ldr)
 #define clustered_apic_check (genapic->clustered_apic_check) 
 #define cpu_mask_to_apicid (genapic->cpu_mask_to_apicid)
+/*
+ * 设置genapic的地方:
+ *   - xen/arch/x86/apic.c|945| <<x2apic_bsp_setup>> genapic = apic_x2apic_probe();
+ *   - xen/arch/x86/apic.c|946| <<x2apic_bsp_setup>> printk("Switched to APIC driver %s.\n", genapic->name);
+ */
 #define vector_allocation_cpumask(cpu) (genapic->vector_allocation_cpumask(cpu))
 
 static inline void enable_apic_mode(void)
diff --git a/xen/include/public/arch-x86/hvm/save.h b/xen/include/public/arch-x86/hvm/save.h
index fd7bf3f..44b0739 100644
--- a/xen/include/public/arch-x86/hvm/save.h
+++ b/xen/include/public/arch-x86/hvm/save.h
@@ -405,6 +405,20 @@ DECLARE_HVM_SAVE_TYPE(IOAPIC, 4, struct hvm_hw_vioapic);
  */
 
 struct hvm_hw_lapic {
+    /*
+     * 在以下使用apic_base_msr:
+     *   - arch/x86/hvm/hvm.c|3441| <<hvm_msr_read_intercept>> *msr_content = vcpu_vlapic(v)->hw.apic_base_msr;
+     *   - arch/x86/hvm/vlapic.c|1080| <<vlapic_msr_set>> if ( (vlapic->hw.apic_base_msr ^ value) & MSR_IA32_APICBASE_ENABLE )
+     *   - arch/x86/hvm/vlapic.c|1096| <<vlapic_msr_set>> else if ( ((vlapic->hw.apic_base_msr ^ value) & MSR_IA32_APICBASE_EXTD) &&
+     *   - arch/x86/hvm/vlapic.c|1100| <<vlapic_msr_set>> vlapic->hw.apic_base_msr = value;
+     *   - arch/x86/hvm/vlapic.c|1109| <<vlapic_msr_set>> "apic base msr is 0x%016"PRIx64, vlapic->hw.apic_base_msr);
+     *   - arch/x86/hvm/vlapic.c|1386| <<vlapic_reset>> vlapic->hw.apic_base_msr = (MSR_IA32_APICBASE_ENABLE |
+     *   - arch/x86/hvm/vlapic.c|1389| <<vlapic_reset>> vlapic->hw.apic_base_msr |= MSR_IA32_APICBASE_BSP;
+     *   - arch/x86/hvm/vlapic.c|1521| <<lapic_load_hidden>> if ( !(s->hw.apic_base_msr & MSR_IA32_APICBASE_ENABLE) &&
+     *   - include/asm-x86/hvm/vlapic.h|53| <<vlapic_base_address>> ((vlapic)->hw.apic_base_msr & MSR_IA32_APICBASE_BASE)
+     *   - include/asm-x86/hvm/vlapic.h|56| <<vlapic_x2apic_mode>> ((vlapic)->hw.apic_base_msr & MSR_IA32_APICBASE_EXTD)
+     *   - include/asm-x86/hvm/vlapic.h|59| <<vlapic_xapic_mode>> !((vlapic)->hw.apic_base_msr & MSR_IA32_APICBASE_EXTD))
+     */
     uint64_t             apic_base_msr;
     uint32_t             disabled; /* VLAPIC_xx_DISABLED */
     uint32_t             timer_divisor;
diff --git a/xen/include/public/arch-x86/xen.h b/xen/include/public/arch-x86/xen.h
index 3b0b1d6..53fbe1e 100644
--- a/xen/include/public/arch-x86/xen.h
+++ b/xen/include/public/arch-x86/xen.h
@@ -275,6 +275,14 @@ typedef struct arch_shared_info arch_shared_info_t;
  */
 struct xen_arch_domainconfig {
 #define _XEN_X86_EMU_LAPIC          0
+/*
+ * used by:
+ *   - arch/x86/domain.c|400| <<emulation_flags_ok>> emflags != (XEN_X86_EMU_LAPIC|XEN_X86_EMU_IOAPIC) )
+ *   - arch/x86/domain.c|403| <<emulation_flags_ok>> emflags != XEN_X86_EMU_ALL && emflags != XEN_X86_EMU_LAPIC )
+ *   - arch/x86/setup.c|1585| <<__start_xen>> config.emulation_flags = XEN_X86_EMU_LAPIC|XEN_X86_EMU_IOAPIC;
+ *   - include/asm-x86/domain.h|424| <<has_vlapic>> #define has_vlapic(d) (!!((d)->arch.emulation_flags & XEN_X86_EMU_LAPIC))
+ *   - include/public/arch-x86/xen.h|298| <<XEN_X86_EMU_ALL>> #define XEN_X86_EMU_ALL (XEN_X86_EMU_LAPIC | XEN_X86_EMU_HPET | \
+ */
 #define XEN_X86_EMU_LAPIC           (1U<<_XEN_X86_EMU_LAPIC)
 #define _XEN_X86_EMU_HPET           1
 #define XEN_X86_EMU_HPET            (1U<<_XEN_X86_EMU_HPET)
diff --git a/xen/include/xen/event.h b/xen/include/xen/event.h
index 87915ea..6e0926e 100644
--- a/xen/include/xen/event.h
+++ b/xen/include/xen/event.h
@@ -100,6 +100,10 @@ static inline struct evtchn *evtchn_from_port(struct domain *d, unsigned int p)
 }
 
 /* Wait on a Xen-attached event channel. */
+/*
+ * called by:
+ *   - arch/x86/hvm/ioreq.c|120| <<hvm_wait_for_io>> wait_on_xen_event_channel(sv->ioreq_evtchn, p->state != state);
+ */
 #define wait_on_xen_event_channel(port, condition)                      \
     do {                                                                \
         if ( condition )                                                \
@@ -115,6 +119,10 @@ static inline struct evtchn *evtchn_from_port(struct domain *d, unsigned int p)
         do_softirq();                                                   \
     } while ( 0 )
 
+/*
+ * called by:
+ *   - arch/x86/hvm/ioreq.c|1412| <<hvm_send_ioreq>> prepare_wait_on_xen_event_channel(port);
+ */
 #define prepare_wait_on_xen_event_channel(port)                         \
     do {                                                                \
         set_bit(_VPF_blocked_in_xen, &current->pause_flags);            \
diff --git a/xen/include/xen/pci.h b/xen/include/xen/pci.h
index 43f2125..b4f27b9 100644
--- a/xen/include/xen/pci.h
+++ b/xen/include/xen/pci.h
@@ -55,6 +55,11 @@ struct pci_dev {
     struct list_head alldevs_list;
     struct list_head domain_list;
 
+    /*
+     * 在以下添加:
+     *   - arch/x86/msi.c|750| <<msi_capability_init>> list_add_tail(&entry->list, &dev->msi_list);
+     *   - arch/x86/msi.c|990| <<msix_capability_init>> list_add_tail(&entry->list, &dev->msi_list);
+     */
     struct list_head msi_list;
 
     struct arch_msix *msix;
-- 
2.7.4

