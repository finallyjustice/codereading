From a3d73097a2a5bb63f8febc40fb3100a173db72f2 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sat, 22 Jun 2024 09:39:59 -0700
Subject: [PATCH 1/1] linux-v6.9

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/esr.h                  |   10 +
 arch/arm64/include/asm/kvm_emulate.h          |    9 +
 arch/arm64/include/asm/kvm_pgtable.h          |   73 +
 arch/arm64/include/asm/pgtable-hwdef.h        |    7 +
 arch/arm64/kvm/handle_exit.c                  |   16 +
 arch/arm64/kvm/hyp/nvhe/mem_protect.c         |    3 +
 arch/arm64/kvm/hyp/nvhe/mm.c                  |    3 +
 arch/arm64/kvm/hyp/nvhe/setup.c               |    6 +
 arch/arm64/kvm/hyp/pgtable.c                  |  626 ++++++++
 arch/arm64/kvm/mmio.c                         |    4 +
 arch/arm64/kvm/mmu.c                          |  125 ++
 arch/arm64/kvm/nested.c                       |   18 +
 arch/x86/include/asm/kvm_host.h               |   30 +
 arch/x86/kernel/apic/vector.c                 |    4 +
 arch/x86/kvm/hyperv.c                         |   20 +
 arch/x86/kvm/irq_comm.c                       |   67 +
 arch/x86/kvm/pmu.c                            |    4 +
 arch/x86/kvm/vmx/posted_intr.c                |  120 ++
 arch/x86/kvm/vmx/posted_intr.h                |   48 +
 arch/x86/kvm/vmx/vmx.c                        |   22 +
 arch/x86/kvm/vmx/vmx.h                        |    7 +
 arch/x86/kvm/x86.c                            |  128 ++
 arch/x86/kvm/xen.c                            |    6 +
 .../net/ethernet/mellanox/mlx5/core/en_tx.c   |   56 +
 drivers/net/tap.c                             |  117 ++
 drivers/net/tun.c                             |   21 +
 drivers/net/virtio_net.c                      |  102 ++
 drivers/pci/hotplug/pciehp_ctrl.c             |    4 +
 drivers/pci/hotplug/pciehp_hpc.c              |    5 +
 drivers/scsi/scsi_ioctl.c                     |   28 +
 drivers/vfio/pci/vfio_pci_core.c              |    4 +
 drivers/vfio/pci/vfio_pci_intrs.c             |   20 +
 drivers/vhost/iotlb.c                         |   32 +
 drivers/vhost/net.c                           |  557 ++++++++
 drivers/vhost/vhost.c                         |  216 +++
 drivers/vhost/vhost.h                         |  110 ++
 drivers/virtio/virtio_ring.c                  |   26 +
 fs/open.c                                     |   23 +
 include/linux/gfp.h                           |    5 +
 include/linux/if_tap.h                        |   20 +
 include/linux/irqbypass.h                     |   24 +
 include/linux/kvm_host.h                      |   36 +
 include/linux/kvm_irqfd.h                     |    3 +
 include/linux/perf_event.h                    |   58 +
 include/linux/sched.h                         |   53 +
 include/linux/skbuff.h                        |    5 +
 include/linux/uio.h                           |   50 +
 include/linux/virtio_net.h                    |    9 +
 include/uapi/linux/if_tun.h                   |   18 +
 include/uapi/linux/pci_regs.h                 |    5 +
 kernel/events/core.c                          |  494 +++++++
 net/core/datagram.c                           |   18 +
 tools/include/uapi/linux/kvm.h                |   13 +
 tools/perf/util/cpumap.c                      |    8 +
 .../selftests/kvm/aarch64/page_fault_test.c   | 1269 +++++++++++++++++
 .../selftests/kvm/include/kvm_util_base.h     |   60 +
 .../selftests/kvm/lib/aarch64/processor.c     |   82 ++
 tools/testing/selftests/kvm/lib/elf.c         |    6 +
 tools/testing/selftests/kvm/lib/guest_modes.c |   11 +
 tools/testing/selftests/kvm/lib/kvm_util.c    |   45 +
 .../testing/selftests/kvm/lib/ucall_common.c  |    6 +
 .../selftests/kvm/lib/userfaultfd_util.c      |    7 +
 virt/kvm/eventfd.c                            |   95 ++
 virt/kvm/irqchip.c                            |  111 ++
 virt/lib/irqbypass.c                          |   52 +
 65 files changed, 5240 insertions(+)

diff --git a/arch/arm64/include/asm/esr.h b/arch/arm64/include/asm/esr.h
index 81606bf7d..44cb87875 100644
--- a/arch/arm64/include/asm/esr.h
+++ b/arch/arm64/include/asm/esr.h
@@ -386,6 +386,16 @@ static inline bool esr_is_data_abort(unsigned long esr)
 	return ec == ESR_ELx_EC_DABT_LOW || ec == ESR_ELx_EC_DABT_CUR;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/include/asm/kvm_emulate.h|413| <<kvm_vcpu_trap_is_translation_fault>> return esr_fsc_is_translation_fault(kvm_vcpu_get_esr(vcpu));
+ *   - arch/arm64/kvm/mmu.c|1698| <<kvm_handle_guest_abort>> if (esr_fsc_is_translation_fault(esr)) {
+ *   - arch/arm64/kvm/mmu.c|1733| <<kvm_handle_guest_abort>> if (!esr_fsc_is_translation_fault(esr) &&
+ *   - arch/arm64/mm/fault.c|267| <<is_el1_permission_fault>> return esr_fsc_is_translation_fault(esr) &&
+ *   - arch/arm64/mm/fault.c|280| <<is_spurious_el1_translation_fault>> if (!is_el1_data_abort(esr) || !esr_fsc_is_translation_fault(esr))
+ *   - arch/arm64/mm/fault.c|301| <<is_spurious_el1_translation_fault>> return !esr_fsc_is_translation_fault(dfsc);
+ *   - arch/arm64/mm/fault.c|400| <<__do_kernel_fault>> if (esr_fsc_is_translation_fault(esr) &&
+ */
 static inline bool esr_fsc_is_translation_fault(unsigned long esr)
 {
 	/* Translation fault, level -1 */
diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h
index 975af30af..020548e76 100644
--- a/arch/arm64/include/asm/kvm_emulate.h
+++ b/arch/arm64/include/asm/kvm_emulate.h
@@ -296,6 +296,15 @@ static inline bool vcpu_mode_priv(const struct kvm_vcpu *vcpu)
 
 static __always_inline u64 kvm_vcpu_get_esr(const struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_vcpu_fault_info fault;
+	 *       -> u64 esr_el2;            // Hyp Syndrom Register
+	 *       -> u64 far_el2;            // Hyp Fault Address Register
+	 *       -> u64 hpfar_el2;          // Hyp IPA Fault Address Register
+	 *       -> u64 disr_el1;           // Deferred [SError] Status Register
+	 */
 	return vcpu->arch.fault.esr_el2;
 }
 
diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 19278dfe7..39d18b9b2 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
@@ -104,12 +104,72 @@ static inline kvm_pfn_t kvm_pte_to_pfn(kvm_pte_t pte)
 	return __phys_to_pfn(kvm_pte_to_phys(pte));
 }
 
+/*
+ * 假设PAGE_SHIFT=12
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(-1) = ((12 - 3) * (4 - (-1)) + 3) = 48
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(0)  = ((12 - 3) * (4 - (0)) + 3)  = 39
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(1)  = ((12 - 3) * (4 - (1)) + 3)  = 30
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(2)  = ((12 - 3) * (4 - (2)) + 3)  = 21
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(3)  = ((12 - 3) * (4 - (3)) + 3)  = 12
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(4)  = ((12 - 3) * (4 - (4)) + 3)  = 3
+ *
+ * 下面的是Size mapped by an entry at level n ( 0 <= n <= 3)
+ * level 3的一个entry map的大小是 1 << 12 = 4K, 一个page table就是2M
+ * level 2的一个entry map的大小是 1 << 21 = 2M, 一个page table就是1G
+ * level 1的一个entry map的大小是 1 << 30 = 1G, 一个page table就是512G
+ * level 0的一个entry map的大小是 1 << 39 = 512G, 一个page table就是512G x 512
+ *
+ * level 2的一个entry map的大小是 1 << 21 = 2M
+ * // PMD_SHIFT determines the size a level 2 page table entry can map.
+ * #define PMD_SHIFT               ARM64_HW_PGTABLE_LEVEL_SHIFT(2)
+ *
+ * level 1的一个entry map的大小是 1 << 30 = 1G
+ * // PUD_SHIFT determines the size a level 1 page table entry can map.
+ * #define PUD_SHIFT               ARM64_HW_PGTABLE_LEVEL_SHIFT(1)
+ *
+ * 一个例子是CONFIG_PGTABLE_LEVELS=4.
+ * 4 - 4 = 0
+ * level 0的一个entry map的大小是 1 << 39 = 512G
+ * // PGDIR_SHIFT determines the size a top-level page table entry can map
+ * // (depending on the configuration, this level can be 0, 1 or 2).
+ * #define PGDIR_SHIFT             ARM64_HW_PGTABLE_LEVEL_SHIFT(4 - CONFIG_PGTABLE_LEVELS)
+ *
+ * level 3的一个entry叫PTE
+ * level 2的一个entry叫PMD
+ * level 1的一个entry叫PUD
+ * level 0的一个entry叫P4D
+ *
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(-1) = ((12 - 3) * (4 - (-1)) + 3) = 48
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(0)  = ((12 - 3) * (4 - (0)) + 3)  = 39
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(1)  = ((12 - 3) * (4 - (1)) + 3)  = 30
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(2)  = ((12 - 3) * (4 - (2)) + 3)  = 21
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(3)  = ((12 - 3) * (4 - (3)) + 3)  = 12
+ */
 static inline u64 kvm_granule_shift(s8 level)
 {
+	/*
+	 * ARM64_HW_PGTABLE_LEVEL_SHIFT(-1) = ((12 - 3) * (4 - (-1)) + 3) = 48
+	 * ARM64_HW_PGTABLE_LEVEL_SHIFT(0)  = ((12 - 3) * (4 - (0)) + 3)  = 39
+	 * ARM64_HW_PGTABLE_LEVEL_SHIFT(1)  = ((12 - 3) * (4 - (1)) + 3)  = 30
+	 * ARM64_HW_PGTABLE_LEVEL_SHIFT(2)  = ((12 - 3) * (4 - (2)) + 3)  = 21
+	 * ARM64_HW_PGTABLE_LEVEL_SHIFT(3)  = ((12 - 3) * (4 - (3)) + 3)  = 12
+	 *
+	 * Size mapped by an entry at level n ( 0 <= n <= 3)
+	 * level 3的一个entry map的大小是 1 << 12 = 4K, 一个page table就是2M
+	 * level 2的一个entry map的大小是 1 << 21 = 2M, 一个page table就是1G
+	 * level 1的一个entry map的大小是 1 << 30 = 1G, 一个page table就是512G
+	 * level 0的一个entry map的大小是 1 << 39 = 512G, 一个page table就是512G x 512
+	 */
 	/* Assumes KVM_PGTABLE_LAST_LEVEL is 3 */
 	return ARM64_HW_PGTABLE_LEVEL_SHIFT(level);
 }
 
+/*
+ * level 3的一个entry map的大小是 1 << 12 = 4K, 一个page table就是2M
+ * level 2的一个entry map的大小是 1 << 21 = 2M, 一个page table就是1G
+ * level 1的一个entry map的大小是 1 << 30 = 1G, 一个page table就是512G
+ * level 0的一个entry map的大小是 1 << 39 = 512G, 一个page table就是512G x 512
+ */
 static inline u64 kvm_granule_size(s8 level)
 {
 	return BIT(kvm_granule_shift(level));
@@ -177,6 +237,15 @@ struct kvm_pgtable_mm_ops {
 	void*		(*phys_to_virt)(phys_addr_t phys);
 	phys_addr_t	(*virt_to_phys)(void *addr);
 	void		(*dcache_clean_inval_poc)(void *addr, size_t size);
+	/*
+	 * 在以下使用kvm_pgtable_mm_ops->icache_inval_pou():
+	 *   - arch/arm64/kvm/mmu.c|855| <<global>> .icache_inval_pou = invalidate_icache_guest_page,
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|254| <<kvm_guest_prepare_stage2>> .icache_inval_pou = invalidate_icache_guest_page,
+	 *   - arch/arm64/kvm/hyp/pgtable.c|991| <<stage2_map_walker_try_leaf>> if (!kvm_pgtable_walk_skip_cmo(ctx) && mm_ops->icache_inval_pou &&
+	 *   - arch/arm64/kvm/hyp/pgtable.c|993| <<stage2_map_walker_try_leaf>> mm_ops->icache_inval_pou(kvm_pte_follow(new, mm_ops), granule);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1227| <<stage2_attr_walker>> if (mm_ops->icache_inval_pou &&
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1229| <<stage2_attr_walker>> mm_ops->icache_inval_pou(kvm_pte_follow(pte, mm_ops),
+	 */
 	void		(*icache_inval_pou)(void *addr, size_t size);
 };
 
@@ -308,6 +377,10 @@ static inline kvm_pte_t *kvm_dereference_pteref(struct kvm_pgtable_walker *walke
 	return pteref;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|361| <<kvm_pgtable_walk>> r = kvm_pgtable_walk_begin(walker);
+ */
 static inline int kvm_pgtable_walk_begin(struct kvm_pgtable_walker *walker)
 {
 	/*
diff --git a/arch/arm64/include/asm/pgtable-hwdef.h b/arch/arm64/include/asm/pgtable-hwdef.h
index ef207a0d4..d1faaa624 100644
--- a/arch/arm64/include/asm/pgtable-hwdef.h
+++ b/arch/arm64/include/asm/pgtable-hwdef.h
@@ -38,6 +38,13 @@
  * Rearranging it a bit we get :
  *   (4 - n) * (PAGE_SHIFT - 3) + 3
  */
+/*
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(-1) = ((12 - 3) * (4 - (-1)) + 3) = 48
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(0)  = ((12 - 3) * (4 - (0)) + 3)  = 39
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(1)  = ((12 - 3) * (4 - (1)) + 3)  = 30
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(2)  = ((12 - 3) * (4 - (2)) + 3)  = 21
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(3)  = ((12 - 3) * (4 - (3)) + 3)  = 12
+ */
 #define ARM64_HW_PGTABLE_LEVEL_SHIFT(n)	((PAGE_SHIFT - 3) * (4 - (n)) + 3)
 
 #define PTRS_PER_PTE		(1 << (PAGE_SHIFT - 3))
diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 617ae6dea..c96881acc 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -252,6 +252,10 @@ static int handle_svc(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用arm_exit_handlers[]:
+ *   - arch/arm64/kvm/handle_exit.c|288| <<kvm_get_exit_handler>> return arm_exit_handlers[esr_ec];
+ */
 static exit_handle_fn arm_exit_handlers[] = {
 	[0 ... ESR_ELx_EC_MAX]	= kvm_handle_unknown_ec,
 	[ESR_ELx_EC_WFx]	= kvm_handle_wfx,
@@ -280,6 +284,10 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_PAC]	= kvm_handle_ptrauth,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|311| <<handle_trap_exceptions>> exit_handler = kvm_get_exit_handler(vcpu);
+ */
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 {
 	u64 esr = kvm_vcpu_get_esr(vcpu);
@@ -294,6 +302,10 @@ static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
  * KVM_EXIT_DEBUG, otherwise userspace needs to complete its
  * emulation first.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|342| <<handle_exit>> return handle_trap_exceptions(vcpu);
+ */
 static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
 {
 	int handled;
@@ -319,6 +331,10 @@ static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
  * Return > 0 to return to guest, < 0 on error, 0 (and set exit_reason) on
  * proper exit to userspace.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1146| <<kvm_arch_vcpu_ioctl_run>> ret = handle_exit(vcpu, ret);
+ */
 int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 {
 	struct kvm_run *run = vcpu->run;
diff --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 861c76021..2a3ed96f4 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
@@ -586,6 +586,9 @@ static int __check_page_state_visitor(const struct kvm_pgtable_visit_ctx *ctx,
 	return d->get_page_state(ctx->old, ctx->addr) == d->desired ? 0 : -EPERM;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 static int check_page_state_range(struct kvm_pgtable *pgt, u64 addr, u64 size,
 				  struct check_walk_data *data)
 {
diff --git a/arch/arm64/kvm/hyp/nvhe/mm.c b/arch/arm64/kvm/hyp/nvhe/mm.c
index 8850b591d..5aea1178b 100644
--- a/arch/arm64/kvm/hyp/nvhe/mm.c
+++ b/arch/arm64/kvm/hyp/nvhe/mm.c
@@ -291,6 +291,9 @@ static int __create_fixmap_slot_cb(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 static int create_fixmap_slot(u64 addr, u64 cpu)
 {
 	struct kvm_pgtable_walker walker = {
diff --git a/arch/arm64/kvm/hyp/nvhe/setup.c b/arch/arm64/kvm/hyp/nvhe/setup.c
index bc58d1b51..bd4713dbc 100644
--- a/arch/arm64/kvm/hyp/nvhe/setup.c
+++ b/arch/arm64/kvm/hyp/nvhe/setup.c
@@ -223,6 +223,9 @@ static int fix_hyp_pgtable_refcnt_walker(const struct kvm_pgtable_visit_ctx *ctx
 	return 0;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 static int fix_host_ownership(void)
 {
 	struct kvm_pgtable_walker walker = {
@@ -243,6 +246,9 @@ static int fix_host_ownership(void)
 	return 0;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 static int fix_hyp_pgtable_refcnt(void)
 {
 	struct kvm_pgtable_walker walker = {
diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 5a59ef88b..cc044e56c 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
@@ -11,6 +11,41 @@
 #include <asm/kvm_pgtable.h>
 #include <asm/stage2_pgtable.h>
 
+/*
+ * 假设PAGE_SHIFT=12
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(-1) = ((12 - 3) * (4 - (-1)) + 3) = 48
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(0)  = ((12 - 3) * (4 - (0)) + 3)  = 39
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(1)  = ((12 - 3) * (4 - (1)) + 3)  = 30
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(2)  = ((12 - 3) * (4 - (2)) + 3)  = 21
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(3)  = ((12 - 3) * (4 - (3)) + 3)  = 12
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(4)  = ((12 - 3) * (4 - (4)) + 3)  = 3
+ *
+ * 下面的是Size mapped by an entry at level n ( 0 <= n <= 3)
+ * level 3的一个entry map的大小是 1 << 12 = 4K, 一个page table就是2M
+ * level 2的一个entry map的大小是 1 << 21 = 2M, 一个page table就是1G
+ * level 1的一个entry map的大小是 1 << 30 = 1G, 一个page table就是512G
+ * level 0的一个entry map的大小是 1 << 39 = 512G, 一个page table就是512G x 512
+ *
+ * level 2的一个entry map的大小是 1 << 21 = 2M
+ * // PMD_SHIFT determines the size a level 2 page table entry can map.
+ * #define PMD_SHIFT               ARM64_HW_PGTABLE_LEVEL_SHIFT(2)
+ *
+ * level 1的一个entry map的大小是 1 << 30 = 1G
+ * // PUD_SHIFT determines the size a level 1 page table entry can map.
+ * #define PUD_SHIFT               ARM64_HW_PGTABLE_LEVEL_SHIFT(1)
+ *
+ * 一个例子是CONFIG_PGTABLE_LEVELS=4.
+ * 4 - 4 = 0
+ * level 0的一个entry map的大小是 1 << 39 = 512G
+ * // PGDIR_SHIFT determines the size a top-level page table entry can map
+ * // (depending on the configuration, this level can be 0, 1 or 2).
+ * #define PGDIR_SHIFT             ARM64_HW_PGTABLE_LEVEL_SHIFT(4 - CONFIG_PGTABLE_LEVELS)
+ *
+ * level 3的一个entry叫PTE
+ * level 2的一个entry叫PMD
+ * level 1的一个entry叫PUD
+ * level 0的一个entry叫P4D
+ */
 
 #define KVM_PTE_TYPE			BIT(1)
 #define KVM_PTE_TYPE_BLOCK		0
@@ -85,8 +120,17 @@ static bool kvm_phys_is_valid(u64 phys)
 	return phys < BIT(shift);
 }
 
+/*
+ * block似乎就是多个page, 也就是huge page!
+ */
 static bool kvm_block_mapping_supported(const struct kvm_pgtable_visit_ctx *ctx, u64 phys)
 {
+	/*
+	 * level 3的一个entry map的大小是 1 << 12 = 4K, 一个page table就是2M
+	 * level 2的一个entry map的大小是 1 << 21 = 2M, 一个page table就是1G
+	 * level 1的一个entry map的大小是 1 << 30 = 1G, 一个page table就是512G
+	 * level 0的一个entry map的大小是 1 << 39 = 512G, 一个page table就是512G x 512
+	 */
 	u64 granule = kvm_granule_size(ctx->level);
 
 	if (!kvm_level_supports_block_mapping(ctx->level))
@@ -101,6 +145,46 @@ static bool kvm_block_mapping_supported(const struct kvm_pgtable_visit_ctx *ctx,
 	return IS_ALIGNED(ctx->addr, granule);
 }
 
+/*
+ * 假设PAGE_SHIFT=12
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(-1) = ((12 - 3) * (4 - (-1)) + 3) = 48
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(0)  = ((12 - 3) * (4 - (0)) + 3)  = 39
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(1)  = ((12 - 3) * (4 - (1)) + 3)  = 30
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(2)  = ((12 - 3) * (4 - (2)) + 3)  = 21
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(3)  = ((12 - 3) * (4 - (3)) + 3)  = 12
+ * ARM64_HW_PGTABLE_LEVEL_SHIFT(4)  = ((12 - 3) * (4 - (4)) + 3)  = 3
+ *
+ * 下面的是Size mapped by an entry at level n ( 0 <= n <= 3)
+ * level 3的一个entry map的大小是 1 << 12 = 4K, 一个page table就是2M
+ * level 2的一个entry map的大小是 1 << 21 = 2M, 一个page table就是1G
+ * level 1的一个entry map的大小是 1 << 30 = 1G, 一个page table就是512G
+ * level 0的一个entry map的大小是 1 << 39 = 512G, 一个page table就是512G x 512
+ *
+ * level 2的一个entry map的大小是 1 << 21 = 2M
+ * // PMD_SHIFT determines the size a level 2 page table entry can map.
+ * #define PMD_SHIFT               ARM64_HW_PGTABLE_LEVEL_SHIFT(2)
+ *
+ * level 1的一个entry map的大小是 1 << 30 = 1G
+ * // PUD_SHIFT determines the size a level 1 page table entry can map.
+ * #define PUD_SHIFT               ARM64_HW_PGTABLE_LEVEL_SHIFT(1)
+ *
+ * 一个例子是CONFIG_PGTABLE_LEVELS=4.
+ * 4 - 4 = 0
+ * level 0的一个entry map的大小是 1 << 39 = 512G
+ * // PGDIR_SHIFT determines the size a top-level page table entry can map
+ * // (depending on the configuration, this level can be 0, 1 or 2).
+ * #define PGDIR_SHIFT             ARM64_HW_PGTABLE_LEVEL_SHIFT(4 - CONFIG_PGTABLE_LEVELS)
+ *
+ * level 3的一个entry叫PTE
+ * level 2的一个entry叫PMD
+ * level 1的一个entry叫PUD
+ * level 0的一个entry叫P4D
+ */
+
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|435| <<__kvm_pgtable_walk>> for (idx = kvm_pgtable_idx(data, level); idx < PTRS_PER_PTE; ++idx) {
+ */
 static u32 kvm_pgtable_idx(struct kvm_pgtable_walk_data *data, s8 level)
 {
 	u64 shift = kvm_granule_shift(level);
@@ -109,16 +193,59 @@ static u32 kvm_pgtable_idx(struct kvm_pgtable_walk_data *data, s8 level)
 	return (data->addr >> shift) & mask;
 }
 
+/*
+ * level 3的一个entry叫PTE
+ * level 2的一个entry叫PMD
+ * level 1的一个entry叫PUD
+ * level 0的一个entry叫P4D
+ *
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|127| <<kvm_pgd_pages>> return kvm_pgd_page_idx(&pgt, -1ULL) + 1;
+ *   - arch/arm64/kvm/hyp/pgtable.c|320| <<_kvm_pgtable_walk>> for (idx = kvm_pgd_page_idx(pgt, data->addr); data->addr < data->end; ++idx) {
+ *
+ * 根据pgt->start_level, 计算addr的index
+ * 应该算是pgd的index吧(最高的???)
+ */
 static u32 kvm_pgd_page_idx(struct kvm_pgtable *pgt, u64 addr)
 {
+	/*
+	 * ARM64_HW_PGTABLE_LEVEL_SHIFT(-1) = ((12 - 3) * (4 - (-1)) + 3) = 48
+	 * ARM64_HW_PGTABLE_LEVEL_SHIFT(0)  = ((12 - 3) * (4 - (0)) + 3)  = 39
+	 * ARM64_HW_PGTABLE_LEVEL_SHIFT(1)  = ((12 - 3) * (4 - (1)) + 3)  = 30
+	 * ARM64_HW_PGTABLE_LEVEL_SHIFT(2)  = ((12 - 3) * (4 - (2)) + 3)  = 21
+	 * ARM64_HW_PGTABLE_LEVEL_SHIFT(3)  = ((12 - 3) * (4 - (3)) + 3)  = 12
+	 */
 	u64 shift = kvm_granule_shift(pgt->start_level - 1); /* May underflow */
 	u64 mask = BIT(pgt->ia_bits) - 1;
 
 	return (addr & mask) >> shift;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1856| <<__kvm_pgtable_stage2_init>> pgd_sz = kvm_pgd_pages(ia_bits, start_level) * PAGE_SIZE;
+ *   - arch/arm64/kvm/hyp/pgtable.c|1879| <<kvm_pgtable_stage2_pgd_size>> return kvm_pgd_pages(ia_bits, start_level) * PAGE_SIZE;
+ *   - arch/arm64/kvm/hyp/pgtable.c|1908| <<kvm_pgtable_stage2_destroy>> pgd_sz = kvm_pgd_pages(pgt->ia_bits, pgt->start_level) * PAGE_SIZE;
+ *
+ * 假设ia_bits = 40和start_level = 1
+ * ipa物理地址最大是(1<<40)-1=0xffffffffff (10个ff)
+ * 从level 1开始的话, 两个page就能代表所有的ipa地址
+ * 也就是说, 从eptp开始的指针(类似cr3), 两个page就能表示所有的ipa range
+ */
 static u32 kvm_pgd_pages(u32 ia_bits, s8 start_level)
 {
+	/*
+	 * 比如:
+	 * crash> struct kvm_pgtable ffff3fffe4412800
+	 * struct kvm_pgtable {
+	 *   ia_bits = 40,
+	 *   start_level = 1,
+	 *   pgd = 0xffff3fff920cc000,
+	 *   mm_ops = 0xffffbb087c3a3768 <kvm_s2_mm_ops>,
+	 *   mmu = 0xffff800080e1d9c8,
+	 *   flags = 0,
+	 *   force_pte_cb = 0x0
+	 */
 	struct kvm_pgtable pgt = {
 		.ia_bits	= ia_bits,
 		.start_level	= start_level,
@@ -138,6 +265,20 @@ static bool kvm_pte_table(kvm_pte_t pte, s8 level)
 	return FIELD_GET(KVM_PTE_TYPE, pte) == KVM_PTE_TYPE_TABLE;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|378| <<__kvm_pgtable_visit>> childp = (kvm_pteref_t)kvm_pte_follow(ctx.old, mm_ops);
+ *   - arch/arm64/kvm/hyp/pgtable.c|702| <<hyp_unmap_walker>> childp = kvm_pte_follow(ctx->old, mm_ops);
+ *   - arch/arm64/kvm/hyp/pgtable.c|780| <<hyp_free_walker>> mm_ops->put_page(kvm_pte_follow(ctx->old, mm_ops));
+ *   - arch/arm64/kvm/hyp/pgtable.c|1182| <<stage2_map_walker_try_leaf>> mm_ops->dcache_clean_inval_poc(kvm_pte_follow(new, mm_ops), granule);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1187| <<stage2_map_walker_try_leaf>> mm_ops->icache_inval_pou(kvm_pte_follow(new, mm_ops), granule);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1198| <<stage2_map_walk_table_pre>> kvm_pte_t *childp = kvm_pte_follow(ctx->old, mm_ops);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1364| <<stage2_unmap_walker>> childp = kvm_pte_follow(ctx->old, mm_ops);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1380| <<stage2_unmap_walker>> mm_ops->dcache_clean_inval_poc(kvm_pte_follow(ctx->old, mm_ops), kvm_granule_size(ctx->level));
+ *   - arch/arm64/kvm/hyp/pgtable.c|1450| <<stage2_attr_walker>> mm_ops->icache_inval_pou(kvm_pte_follow(pte, mm_ops), kvm_granule_size(ctx->level));
+ *   - arch/arm64/kvm/hyp/pgtable.c|1690| <<stage2_flush_walker>> mm_ops->dcache_clean_inval_poc(kvm_pte_follow(ctx->old, mm_ops), kvm_granule_size(ctx->level));
+ *   - arch/arm64/kvm/hyp/pgtable.c|1936| <<stage2_free_walker>> mm_ops->put_page(kvm_pte_follow(ctx->old, mm_ops))
+ */
 static kvm_pte_t *kvm_pte_follow(kvm_pte_t pte, struct kvm_pgtable_mm_ops *mm_ops)
 {
 	return mm_ops->phys_to_virt(kvm_pte_to_phys(pte));
@@ -157,6 +298,11 @@ static kvm_pte_t kvm_init_table_pte(kvm_pte_t *childp, struct kvm_pgtable_mm_ops
 	return pte;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|757| <<hyp_map_walker_try_leaf>> new = kvm_init_valid_leaf_pte(phys, data->attr, ctx->level);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1357| <<stage2_map_walker_try_leaf>> new = kvm_init_valid_leaf_pte(phys, data->attr, ctx->level);
+ */
 static kvm_pte_t kvm_init_valid_leaf_pte(u64 pa, kvm_pte_t attr, s8 level)
 {
 	kvm_pte_t pte = kvm_phys_to_pte(pa);
@@ -175,6 +321,12 @@ static kvm_pte_t kvm_init_invalid_leaf_owner(u8 owner_id)
 	return FIELD_PREP(KVM_INVALID_PTE_OWNER_MASK, owner_id);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|375| <<__kvm_pgtable_visit>> ret = kvm_pgtable_visitor_cb(data, &ctx, KVM_PGTABLE_WALK_TABLE_PRE);
+ *   - arch/arm64/kvm/hyp/pgtable.c|380| <<__kvm_pgtable_visit>> ret = kvm_pgtable_visitor_cb(data, &ctx, KVM_PGTABLE_WALK_LEAF);
+ *   - arch/arm64/kvm/hyp/pgtable.c|409| <<__kvm_pgtable_visit>> ret = kvm_pgtable_visitor_cb(data, &ctx, KVM_PGTABLE_WALK_TABLE_POST);
+ */
 static int kvm_pgtable_visitor_cb(struct kvm_pgtable_walk_data *data,
 				  const struct kvm_pgtable_visit_ctx *ctx,
 				  enum kvm_pgtable_walk_flags visit)
@@ -186,6 +338,12 @@ static int kvm_pgtable_visitor_cb(struct kvm_pgtable_walk_data *data,
 	return walker->cb(ctx, visit);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|394| <<__kvm_pgtable_visit>> if (!kvm_pgtable_walk_continue(data->walker, ret))
+ *   - arch/arm64/kvm/hyp/pgtable.c|405| <<__kvm_pgtable_visit>> if (!kvm_pgtable_walk_continue(data->walker, ret))
+ *   - arch/arm64/kvm/hyp/pgtable.c|412| <<__kvm_pgtable_visit>> if (kvm_pgtable_walk_continue(data->walker, ret))
+ */
 static bool kvm_pgtable_walk_continue(const struct kvm_pgtable_walker *walker,
 				      int r)
 {
@@ -199,6 +357,11 @@ static bool kvm_pgtable_walk_continue(const struct kvm_pgtable_walker *walker,
 	 * (e.g. write protecting a range of memory) and chug along with the
 	 * page table walk.
 	 */
+	/*
+	 * 注释:
+	 * @KVM_PGTABLE_WALK_HANDLE_FAULT:      Indicates the page-table walk was
+	 *                                      invoked from a fault handler.
+	 */
 	if (r == -EAGAIN)
 		return !(walker->flags & KVM_PGTABLE_WALK_HANDLE_FAULT);
 
@@ -208,6 +371,12 @@ static bool kvm_pgtable_walk_continue(const struct kvm_pgtable_walker *walker,
 static int __kvm_pgtable_walk(struct kvm_pgtable_walk_data *data,
 			      struct kvm_pgtable_mm_ops *mm_ops, kvm_pteref_t pgtable, s8 level);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|300| <<__kvm_pgtable_walk>> ret = __kvm_pgtable_visit(data, mm_ops, pteref, level);
+ *
+ * __kvm_pgtable_walk()的使用者
+ */
 static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
 				      struct kvm_pgtable_mm_ops *mm_ops,
 				      kvm_pteref_t pteref, s8 level)
@@ -228,9 +397,18 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
 	int ret = 0;
 	bool reload = false;
 	kvm_pteref_t childp;
+	/*
+	 * 是table还是huge page??
+	 */
 	bool table = kvm_pte_table(ctx.old, level);
 
 	if (table && (ctx.flags & KVM_PGTABLE_WALK_TABLE_PRE)) {
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/hyp/pgtable.c|375| <<__kvm_pgtable_visit>> ret = kvm_pgtable_visitor_cb(data, &ctx, KVM_PGTABLE_WALK_TABLE_PRE);
+		 *   - arch/arm64/kvm/hyp/pgtable.c|380| <<__kvm_pgtable_visit>> ret = kvm_pgtable_visitor_cb(data, &ctx, KVM_PGTABLE_WALK_LEAF);
+		 *   - arch/arm64/kvm/hyp/pgtable.c|409| <<__kvm_pgtable_visit>> ret = kvm_pgtable_visitor_cb(data, &ctx, KVM_PGTABLE_WALK_TABLE_POST);
+		 */
 		ret = kvm_pgtable_visitor_cb(data, &ctx, KVM_PGTABLE_WALK_TABLE_PRE);
 		reload = true;
 	}
@@ -260,10 +438,23 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
 	}
 
 	childp = (kvm_pteref_t)kvm_pte_follow(ctx.old, mm_ops);
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/hyp/pgtable.c|263| <<__kvm_pgtable_visit>> ret = __kvm_pgtable_walk(data, mm_ops, childp, level + 1);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|316| <<_kvm_pgtable_walk>> ret = __kvm_pgtable_walk(data, pgt->mm_ops, pteref, pgt->start_level);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1449| <<kvm_pgtable_stage2_create_unlinked>> ret = __kvm_pgtable_walk(&data, mm_ops, (kvm_pteref_t)pgtable,
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1649| <<kvm_pgtable_stage2_free_unlinked>> WARN_ON(__kvm_pgtable_walk(&data, mm_ops, ptep, level + 1));
+	 */
 	ret = __kvm_pgtable_walk(data, mm_ops, childp, level + 1);
 	if (!kvm_pgtable_walk_continue(data->walker, ret))
 		goto out;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/hyp/pgtable.c|375| <<__kvm_pgtable_visit>> ret = kvm_pgtable_visitor_cb(data, &ctx, KVM_PGTABLE_WALK_TABLE_PRE);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|380| <<__kvm_pgtable_visit>> ret = kvm_pgtable_visitor_cb(data, &ctx, KVM_PGTABLE_WALK_LEAF);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|409| <<__kvm_pgtable_visit>> ret = kvm_pgtable_visitor_cb(data, &ctx, KVM_PGTABLE_WALK_TABLE_POST);
+	 */
 	if (ctx.flags & KVM_PGTABLE_WALK_TABLE_POST)
 		ret = kvm_pgtable_visitor_cb(data, &ctx, KVM_PGTABLE_WALK_TABLE_POST);
 
@@ -274,6 +465,13 @@ static inline int __kvm_pgtable_visit(struct kvm_pgtable_walk_data *data,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|263| <<__kvm_pgtable_visit>> ret = __kvm_pgtable_walk(data, mm_ops, childp, level + 1);
+ *   - arch/arm64/kvm/hyp/pgtable.c|316| <<_kvm_pgtable_walk>> ret = __kvm_pgtable_walk(data, pgt->mm_ops, pteref, pgt->start_level);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1449| <<kvm_pgtable_stage2_create_unlinked>> ret = __kvm_pgtable_walk(&data, mm_ops, (kvm_pteref_t)pgtable,
+ *   - arch/arm64/kvm/hyp/pgtable.c|1649| <<kvm_pgtable_stage2_free_unlinked>> WARN_ON(__kvm_pgtable_walk(&data, mm_ops, ptep, level + 1));
+ */
 static int __kvm_pgtable_walk(struct kvm_pgtable_walk_data *data,
 			      struct kvm_pgtable_mm_ops *mm_ops, kvm_pteref_t pgtable, s8 level)
 {
@@ -284,12 +482,29 @@ static int __kvm_pgtable_walk(struct kvm_pgtable_walk_data *data,
 			 level > KVM_PGTABLE_LAST_LEVEL))
 		return -EINVAL;
 
+	/*
+	 * 下面的是Size mapped by an entry at level n ( 0 <= n <= 3)
+	 * level 3的一个entry map的大小是 1 << 12 = 4K, 一个page table就是2M
+	 * level 2的一个entry map的大小是 1 << 21 = 2M, 一个page table就是1G
+	 * level 1的一个entry map的大小是 1 << 30 = 1G, 一个page table就是512G
+	 * level 0的一个entry map的大小是 1 << 39 = 512G, 一个page table就是512G x 512
+	 *
+	 * level 3的一个entry叫PTE
+	 * level 2的一个entry叫PMD
+	 * level 1的一个entry叫PUD
+	 * level 0的一个entry叫P4D
+	 *
+	 * 只在此处调用kvm_pgtable_idx()
+	 */
 	for (idx = kvm_pgtable_idx(data, level); idx < PTRS_PER_PTE; ++idx) {
 		kvm_pteref_t pteref = &pgtable[idx];
 
 		if (data->addr >= data->end)
 			break;
 
+		/*
+		 * 只在此处调用
+		 */
 		ret = __kvm_pgtable_visit(data, mm_ops, pteref, level);
 		if (ret)
 			break;
@@ -298,6 +513,22 @@ static int __kvm_pgtable_walk(struct kvm_pgtable_walk_data *data,
 	return ret;
 }
 
+/*
+ * struct kvm_pgtable_walker walker = {
+ *     .cb             = stage2_attr_walker,
+ *     .arg            = &data,
+ *     .flags          = flags | KVM_PGTABLE_WALK_LEAF,
+ * };
+ *
+ * struct kvm_pgtable_walk_data walk_data:
+ * -> struct kvm_pgtable_walker *walker;
+ *    -> void * const arg; --> 好多例子, 比如stage2_attr_data
+ *
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|365| <<kvm_pgtable_walk>> r = _kvm_pgtable_walk(pgt, &walk_data);
+ *
+ * __kvm_pgtable_walk()的使用者
+ */
 static int _kvm_pgtable_walk(struct kvm_pgtable *pgt, struct kvm_pgtable_walk_data *data)
 {
 	u32 idx;
@@ -310,9 +541,33 @@ static int _kvm_pgtable_walk(struct kvm_pgtable *pgt, struct kvm_pgtable_walk_da
 	if (!pgt->pgd)
 		return -EINVAL;
 
+	/*
+	 * level 3的一个entry叫PTE
+	 * level 2的一个entry叫PMD
+	 * level 1的一个entry叫PUD
+	 * level 0的一个entry叫P4D
+	 *
+	 * called by:
+	 *   - arch/arm64/kvm/hyp/pgtable.c|127| <<kvm_pgd_pages>> return kvm_pgd_page_idx(&pgt, -1ULL) + 1;
+	 *   - arch/arm64/kvm/hyp/pgtable.c|320| <<_kvm_pgtable_walk>> for (idx = kvm_pgd_page_idx(pgt, data->addr); data->addr < data->end; ++idx) {
+	 *
+	 * 根据pgt->start_level, 计算addr的index
+	 * 应该算是pgd的index吧(最高的???)
+	 */
 	for (idx = kvm_pgd_page_idx(pgt, data->addr); data->addr < data->end; ++idx) {
+		/*
+		 * struct kvm_pgtable *pgd:
+		 * -> kvm_pteref_t pgd;
+		 */
 		kvm_pteref_t pteref = &pgt->pgd[idx * PTRS_PER_PTE];
 
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/hyp/pgtable.c|263| <<__kvm_pgtable_visit>> ret = __kvm_pgtable_walk(data, mm_ops, childp, level + 1);
+		 *   - arch/arm64/kvm/hyp/pgtable.c|316| <<_kvm_pgtable_walk>> ret = __kvm_pgtable_walk(data, pgt->mm_ops, pteref, pgt->start_level);
+		 *   - arch/arm64/kvm/hyp/pgtable.c|1449| <<kvm_pgtable_stage2_create_unlinked>> ret = __kvm_pgtable_walk(&data, mm_ops, (kvm_pteref_t)pgtable,
+		 *   - arch/arm64/kvm/hyp/pgtable.c|1649| <<kvm_pgtable_stage2_free_unlinked>> WARN_ON(__kvm_pgtable_walk(&data, mm_ops, ptep, level + 1));
+		 */
 		ret = __kvm_pgtable_walk(data, pgt->mm_ops, pteref, pgt->start_level);
 		if (ret)
 			break;
@@ -321,9 +576,39 @@ static int _kvm_pgtable_walk(struct kvm_pgtable *pgt, struct kvm_pgtable_walk_da
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|598| <<check_page_state_range>> return kvm_pgtable_walk(pgt, addr, size, &walker);
+ *   - arch/arm64/kvm/hyp/nvhe/mm.c|302| <<create_fixmap_slot>> return kvm_pgtable_walk(&pkvm_pgtable, addr, PAGE_SIZE, &walker);
+ *   - arch/arm64/kvm/hyp/nvhe/setup.c|238| <<fix_host_ownership>> ret = kvm_pgtable_walk(&pkvm_pgtable, start, reg->size, &walker);
+ *   - arch/arm64/kvm/hyp/nvhe/setup.c|254| <<fix_hyp_pgtable_refcnt>> return kvm_pgtable_walk(&pkvm_pgtable, 0, BIT(pkvm_pgtable.ia_bits),
+ *   - arch/arm64/kvm/hyp/pgtable.c|372| <<kvm_pgtable_get_leaf>> ret = kvm_pgtable_walk(pgt, ALIGN_DOWN(addr, PAGE_SIZE),
+ *   - arch/arm64/kvm/hyp/pgtable.c|506| <<kvm_pgtable_hyp_map>> ret = kvm_pgtable_walk(pgt, addr, size, &walker);
+ *   - arch/arm64/kvm/hyp/pgtable.c|564| <<kvm_pgtable_hyp_unmap>> kvm_pgtable_walk(pgt, addr, size, &walker);
+ *   - arch/arm64/kvm/hyp/pgtable.c|614| <<kvm_pgtable_hyp_destroy>> WARN_ON(kvm_pgtable_walk(pgt, 0, BIT(pgt->ia_bits), &walker));
+ *   - arch/arm64/kvm/hyp/pgtable.c|1105| <<kvm_pgtable_stage2_map>> ret = kvm_pgtable_walk(pgt, addr, size, &walker);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1131| <<kvm_pgtable_stage2_set_owner>> ret = kvm_pgtable_walk(pgt, addr, size, &walker);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1187| <<kvm_pgtable_stage2_unmap>> ret = kvm_pgtable_walk(pgt, addr, size, &walker);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1266| <<stage2_update_leaf_attrs>> ret = kvm_pgtable_walk(pgt, addr, size, &walker);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1346| <<kvm_pgtable_stage2_test_clear_young>> WARN_ON(kvm_pgtable_walk(pgt, addr, size, &walker));
+ *   - arch/arm64/kvm/hyp/pgtable.c|1403| <<kvm_pgtable_stage2_flush>> return kvm_pgtable_walk(pgt, addr, size, &walker);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1558| <<kvm_pgtable_stage2_split>> return kvm_pgtable_walk(pgt, addr, size, &walker);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1623| <<kvm_pgtable_stage2_destroy>> WARN_ON(kvm_pgtable_walk(pgt, 0, BIT(pgt->ia_bits), &walker));
+ */
 int kvm_pgtable_walk(struct kvm_pgtable *pgt, u64 addr, u64 size,
 		     struct kvm_pgtable_walker *walker)
 {
+	/*
+	 * struct kvm_pgtable_walker walker = {
+	 *     .cb             = stage2_attr_walker,
+	 *     .arg            = &data,
+	 *     .flags          = flags | KVM_PGTABLE_WALK_LEAF,
+	 * };
+	 *
+	 * struct kvm_pgtable_walk_data walk_data:
+	 * -> struct kvm_pgtable_walker *walker;
+	 *    -> void * const arg; --> 好多例子, 比如stage2_attr_data
+	 */
 	struct kvm_pgtable_walk_data walk_data = {
 		.start	= ALIGN_DOWN(addr, PAGE_SIZE),
 		.addr	= ALIGN_DOWN(addr, PAGE_SIZE),
@@ -347,6 +632,10 @@ struct leaf_walk_data {
 	s8		level;
 };
 
+/*
+ * 在以下使用leaf_walker():
+ *   - arch/arm64/kvm/hyp/pgtable.c|649| <<kvm_pgtable_get_leaf>> .cb = leaf_walker,
+ */
 static int leaf_walker(const struct kvm_pgtable_visit_ctx *ctx,
 		       enum kvm_pgtable_walk_flags visit)
 {
@@ -358,6 +647,13 @@ static int leaf_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|450| <<host_stage2_adjust_range>> ret = kvm_pgtable_get_leaf(&host_mmu.pgt, addr, &pte, &level);
+ *   - arch/arm64/kvm/mmu.c|845| <<get_user_mapping_size>> ret = kvm_pgtable_get_leaf(&pgt, addr, &pte, &level);
+ *
+ * kvm_pgtable_walk()的使用者
+ */
 int kvm_pgtable_get_leaf(struct kvm_pgtable *pgt, u64 addr,
 			 kvm_pte_t *ptep, s8 *level)
 {
@@ -386,6 +682,10 @@ struct hyp_map_data {
 	kvm_pte_t			attr;
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|788| <<kvm_pgtable_hyp_map>> ret = hyp_set_prot_attr(prot, &map_data.attr);
+ */
 static int hyp_set_prot_attr(enum kvm_pgtable_prot prot, kvm_pte_t *ptep)
 {
 	bool device = prot & KVM_PGTABLE_PROT_DEVICE;
@@ -421,6 +721,11 @@ static int hyp_set_prot_attr(enum kvm_pgtable_prot prot, kvm_pte_t *ptep)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|720| <<hyp_get_page_state>> return pkvm_getstate(kvm_pgtable_hyp_pte_prot(pte));
+ *   - arch/arm64/kvm/hyp/nvhe/setup.c|195| <<fix_host_ownership_walker>> state = pkvm_getstate(kvm_pgtable_hyp_pte_prot(ctx->old));
+ */
 enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte)
 {
 	enum kvm_pgtable_prot prot = pte & KVM_PTE_LEAF_ATTR_HI_SW;
@@ -441,6 +746,10 @@ enum kvm_pgtable_prot kvm_pgtable_hyp_pte_prot(kvm_pte_t pte)
 	return prot;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|755| <<hyp_map_walker>> if (hyp_map_walker_try_leaf(ctx, data))
+ */
 static bool hyp_map_walker_try_leaf(const struct kvm_pgtable_visit_ctx *ctx,
 				    struct hyp_map_data *data)
 {
@@ -462,6 +771,10 @@ static bool hyp_map_walker_try_leaf(const struct kvm_pgtable_visit_ctx *ctx,
 	return true;
 }
 
+/*
+ * 在以下使用hyp_map_walker():
+ *   - arch/arm64/kvm/hyp/pgtable.c|796| <<kvm_pgtable_hyp_map>> .cb = hyp_map_walker,
+ */
 static int hyp_map_walker(const struct kvm_pgtable_visit_ctx *ctx,
 			  enum kvm_pgtable_walk_flags visit)
 {
@@ -486,6 +799,9 @@ static int hyp_map_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 int kvm_pgtable_hyp_map(struct kvm_pgtable *pgt, u64 addr, u64 size, u64 phys,
 			enum kvm_pgtable_prot prot)
 {
@@ -509,6 +825,10 @@ int kvm_pgtable_hyp_map(struct kvm_pgtable *pgt, u64 addr, u64 size, u64 phys,
 	return ret;
 }
 
+/*
+ * 在以下使用hyp_unmap_walker():
+ *   - arch/arm64/kvm/hyp/pgtable.c|858| <<kvm_pgtable_hyp_unmap>> .cb = hyp_unmap_walker,
+ */
 static int hyp_unmap_walker(const struct kvm_pgtable_visit_ctx *ctx,
 			    enum kvm_pgtable_walk_flags visit)
 {
@@ -549,6 +869,9 @@ static int hyp_unmap_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 u64 kvm_pgtable_hyp_unmap(struct kvm_pgtable *pgt, u64 addr, u64 size)
 {
 	u64 unmapped = 0;
@@ -565,6 +888,11 @@ u64 kvm_pgtable_hyp_unmap(struct kvm_pgtable *pgt, u64 addr, u64 size)
 	return unmapped;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/setup.c|81| <<recreate_hyp_mappings>> ret = kvm_pgtable_hyp_init(&pkvm_pgtable, hyp_va_bits, &hyp_early_alloc_mm_ops);
+ *   - arch/arm64/kvm/mmu.c|2050| <<kvm_mmu_init>> err = kvm_pgtable_hyp_init(hyp_pgtable, *hyp_va_bits, &kvm_hyp_mm_ops);
+ */
 int kvm_pgtable_hyp_init(struct kvm_pgtable *pgt, u32 va_bits,
 			 struct kvm_pgtable_mm_ops *mm_ops)
 {
@@ -588,6 +916,10 @@ int kvm_pgtable_hyp_init(struct kvm_pgtable *pgt, u32 va_bits,
 	return 0;
 }
 
+/*
+ * 在以下使用hyp_free_walker():
+ *   - arch/arm64/kvm/hyp/pgtable.c|915| <<kvm_pgtable_hyp_destroy>> .cb = hyp_free_walker,
+ */
 static int hyp_free_walker(const struct kvm_pgtable_visit_ctx *ctx,
 			   enum kvm_pgtable_walk_flags visit)
 {
@@ -604,6 +936,9 @@ static int hyp_free_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 void kvm_pgtable_hyp_destroy(struct kvm_pgtable *pgt)
 {
 	struct kvm_pgtable_walker walker = {
@@ -631,6 +966,11 @@ struct stage2_map_data {
 	bool				force_pte;
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|132| <<prepare_host_vtcr>> host_mmu.arch.mmu.vtcr = kvm_get_vtcr(id_aa64mmfr0_el1_sys_val, id_aa64mmfr1_el1_sys_val, phys_shift);
+ *   - arch/arm64/kvm/mmu.c|925| <<kvm_init_stage2_mmu>> mmu->vtcr = kvm_get_vtcr(mmfr0, mmfr1, phys_shift);
+ */
 u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
 {
 	u64 vtcr = VTCR_EL2_FLAGS;
@@ -684,6 +1024,13 @@ u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
 	return vtcr;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1045| <<KVM_S2_MEMATTR>> #define KVM_S2_MEMATTR(pgt, attr) PAGE_S2_MEMATTR(attr, stage2_has_fwb(pgt))
+ *   - arch/arm64/kvm/hyp/pgtable.c|1232| <<stage2_unmap_defer_tlb_flush>> return system_supports_tlb_range() && stage2_has_fwb(pgt);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1576| <<stage2_unmap_walker>> need_flush = !stage2_has_fwb(pgt);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1933| <<kvm_pgtable_stage2_flush>> if (stage2_has_fwb(pgt))
+ */
 static bool stage2_has_fwb(struct kvm_pgtable *pgt)
 {
 	if (!cpus_have_final_cap(ARM64_HAS_STAGE2_FWB))
@@ -692,6 +1039,12 @@ static bool stage2_has_fwb(struct kvm_pgtable *pgt)
 	return !(pgt->flags & KVM_PGTABLE_S2_NOFWB);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1186| <<stage2_try_break_pte>> kvm_tlb_flush_vmid_range(mmu, addr, size);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1611| <<kvm_pgtable_stage2_unmap>> kvm_tlb_flush_vmid_range(pgt->mmu, addr, size);
+ *   - arch/arm64/kvm/mmu.c|178| <<kvm_arch_flush_remote_tlbs_range>> kvm_tlb_flush_vmid_range(&kvm->arch.mmu, gfn << PAGE_SHIFT, nr_pages << PAGE_SHIFT);
+ */
 void kvm_tlb_flush_vmid_range(struct kvm_s2_mmu *mmu,
 				phys_addr_t addr, size_t size)
 {
@@ -714,6 +1067,11 @@ void kvm_tlb_flush_vmid_range(struct kvm_s2_mmu *mmu,
 
 #define KVM_S2_MEMATTR(pgt, attr) PAGE_S2_MEMATTR(attr, stage2_has_fwb(pgt))
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1512| <<kvm_pgtable_stage2_map>> ret = stage2_set_prot_attr(pgt, prot, &map_data.attr);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1977| <<kvm_pgtable_stage2_create_unlinked>> ret = stage2_set_prot_attr(pgt, prot, &map_data.attr);
+ */
 static int stage2_set_prot_attr(struct kvm_pgtable *pgt, enum kvm_pgtable_prot prot,
 				kvm_pte_t *ptep)
 {
@@ -797,6 +1155,12 @@ static bool stage2_pte_is_locked(kvm_pte_t pte)
 	return !kvm_pte_valid(pte) && (pte & KVM_INVALID_PTE_LOCKED);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1168| <<stage2_try_break_pte>> if (!stage2_try_set_pte(ctx, KVM_INVALID_PTE_LOCKED))
+ *   - arch/arm64/kvm/hyp/pgtable.c|1646| <<stage2_attr_walker>> if (!stage2_try_set_pte(ctx, pte))
+ *   - arch/arm64/kvm/hyp/pgtable.c|1788| <<stage2_age_walker>> if (data->mkold && !stage2_try_set_pte(ctx, new))
+ */
 static bool stage2_try_set_pte(const struct kvm_pgtable_visit_ctx *ctx, kvm_pte_t new)
 {
 	if (!kvm_pgtable_walk_shared(ctx)) {
@@ -860,6 +1224,12 @@ static bool stage2_try_break_pte(const struct kvm_pgtable_visit_ctx *ctx,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1366| <<stage2_map_walker_try_leaf>> stage2_make_pte(ctx, new);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1429| <<stage2_map_walk_leaf>> stage2_make_pte(ctx, new);
+ *   - arch/arm64/kvm/hyp/pgtable.c|2071| <<stage2_split_walker>> stage2_make_pte(ctx, new);
+ */
 static void stage2_make_pte(const struct kvm_pgtable_visit_ctx *ctx, kvm_pte_t new)
 {
 	struct kvm_pgtable_mm_ops *mm_ops = ctx->mm_ops;
@@ -872,6 +1242,11 @@ static void stage2_make_pte(const struct kvm_pgtable_visit_ctx *ctx, kvm_pte_t n
 	smp_store_release(ctx->ptep, new);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1239| <<stage2_unmap_put_pte>> } else if (!stage2_unmap_defer_tlb_flush(pgt)) {
+ *   - arch/arm64/kvm/hyp/pgtable.c|1592| <<kvm_pgtable_stage2_unmap>> if (stage2_unmap_defer_tlb_flush(pgt))
+ */
 static bool stage2_unmap_defer_tlb_flush(struct kvm_pgtable *pgt)
 {
 	/*
@@ -885,6 +1260,10 @@ static bool stage2_unmap_defer_tlb_flush(struct kvm_pgtable *pgt)
 	return system_supports_tlb_range() && stage2_has_fwb(pgt);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1509| <<stage2_unmap_walker>> stage2_unmap_put_pte(ctx, mmu, mm_ops);
+ */
 static void stage2_unmap_put_pte(const struct kvm_pgtable_visit_ctx *ctx,
 				struct kvm_s2_mmu *mmu,
 				struct kvm_pgtable_mm_ops *mm_ops)
@@ -911,17 +1290,33 @@ static void stage2_unmap_put_pte(const struct kvm_pgtable_visit_ctx *ctx,
 	mm_ops->put_page(ctx->ptep);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1308| <<stage2_map_walker_try_leaf>> stage2_pte_cacheable(pgt, new))
+ *   - arch/arm64/kvm/hyp/pgtable.c|1500| <<stage2_unmap_walker>> } else if (stage2_pte_cacheable(pgt, ctx->old)) {
+ *   - arch/arm64/kvm/hyp/pgtable.c|1838| <<stage2_flush_walker>> if (!kvm_pte_valid(ctx->old) || !stage2_pte_cacheable(pgt, ctx->old))
+ */
 static bool stage2_pte_cacheable(struct kvm_pgtable *pgt, kvm_pte_t pte)
 {
 	u64 memattr = pte & KVM_PTE_LEAF_ATTR_LO_S2_MEMATTR;
 	return memattr == KVM_S2_MEMATTR(pgt, NORMAL);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1313| <<stage2_map_walker_try_leaf>> stage2_pte_executable(new))
+ *   - arch/arm64/kvm/hyp/pgtable.c|1584| <<stage2_attr_walker>> stage2_pte_executable(pte) && !stage2_pte_executable(ctx->old))
+ */
 static bool stage2_pte_executable(kvm_pte_t pte)
 {
 	return !(pte & KVM_PTE_LEAF_ATTR_HI_S2_XN);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1253| <<stage2_leaf_mapping_allowed>> u64 phys = stage2_map_walker_phys_addr(ctx, data);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1281| <<stage2_map_walker_try_leaf>> u64 phys = stage2_map_walker_phys_addr(ctx, data);
+ */
 static u64 stage2_map_walker_phys_addr(const struct kvm_pgtable_visit_ctx *ctx,
 				       const struct stage2_map_data *data)
 {
@@ -942,6 +1337,11 @@ static u64 stage2_map_walker_phys_addr(const struct kvm_pgtable_visit_ctx *ctx,
 	return phys + (ctx->addr - ctx->start);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1286| <<stage2_map_walker_try_leaf>> if (!stage2_leaf_mapping_allowed(ctx, data))
+ *   - arch/arm64/kvm/hyp/pgtable.c|1328| <<stage2_map_walk_table_pre>> if (!stage2_leaf_mapping_allowed(ctx, data))
+ */
 static bool stage2_leaf_mapping_allowed(const struct kvm_pgtable_visit_ctx *ctx,
 					struct stage2_map_data *data)
 {
@@ -953,6 +1353,22 @@ static bool stage2_leaf_mapping_allowed(const struct kvm_pgtable_visit_ctx *ctx,
 	return kvm_block_mapping_supported(ctx, phys);
 }
 
+/*
+ * |              __kvm_pgtable_walk() {
+ * |                stage2_map_walker() { 
+ * |                  stage2_map_walk_leaf() {
+ * |                    stage2_map_walker_try_leaf() {
+ * |                      stage2_has_fwb();
+ * |                      kvm_host_va();
+ * |                      clean_dcache_guest_page();
+ * |                      kvm_host_va();
+ * |                      invalidate_icache_guest_page() {
+ * |                        do_interrupt_handler() {
+ *
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1010| <<stage2_map_walk_table_pre>> ret = stage2_map_walker_try_leaf(ctx, data);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1025| <<stage2_map_walk_leaf>> ret = stage2_map_walker_try_leaf(ctx, data);
+ */
 static int stage2_map_walker_try_leaf(const struct kvm_pgtable_visit_ctx *ctx,
 				      struct stage2_map_data *data)
 {
@@ -997,6 +1413,10 @@ static int stage2_map_walker_try_leaf(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1402| <<stage2_map_walker>> return stage2_map_walk_table_pre(ctx, data);
+ */
 static int stage2_map_walk_table_pre(const struct kvm_pgtable_visit_ctx *ctx,
 				     struct stage2_map_data *data)
 {
@@ -1015,6 +1435,10 @@ static int stage2_map_walk_table_pre(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1073| <<stage2_map_walker>> return stage2_map_walk_leaf(ctx, data);
+ */
 static int stage2_map_walk_leaf(const struct kvm_pgtable_visit_ctx *ctx,
 				struct stage2_map_data *data)
 {
@@ -1061,6 +1485,12 @@ static int stage2_map_walk_leaf(const struct kvm_pgtable_visit_ctx *ctx,
  * Otherwise, the LEAF callback performs the mapping at the existing leaves
  * instead.
  */
+/*
+ * 在以下使用stage2_map_walker():
+ *   - arch/arm64/kvm/hyp/pgtable.c|1091| <<kvm_pgtable_stage2_map>> .cb = stage2_map_walker,
+ *   - arch/arm64/kvm/hyp/pgtable.c|1122| <<kvm_pgtable_stage2_set_owner>> .cb = stage2_map_walker,
+ *   - arch/arm64/kvm/hyp/pgtable.c|1418| <<kvm_pgtable_stage2_create_unlinked>> .cb = stage2_map_walker,
+ */
 static int stage2_map_walker(const struct kvm_pgtable_visit_ctx *ctx,
 			     enum kvm_pgtable_walk_flags visit)
 {
@@ -1076,6 +1506,15 @@ static int stage2_map_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|413| <<__host_stage2_idmap>> return kvm_pgtable_stage2_map(&host_mmu.pgt, start, end - start, start, prot, &host_s2_pool, 0);
+ *   - arch/arm64/kvm/mmu.c|1111| <<kvm_phys_addr_ioremap>> ret = kvm_pgtable_stage2_map(pgt, addr, PAGE_SIZE, pa, prot, &cache, 0);
+ *   - arch/arm64/kvm/mmu.c|1603| <<user_mem_abort>> ret = kvm_pgtable_stage2_map(pgt, fault_ipa, vma_pagesize, __pfn_to_phys(pfn), prot, memcache, KVM_PGTABLE_WALK_HANDLE_FAULT | KVM_PGTABLE_WALK_SHARED);
+ *   - arch/arm64/kvm/mmu.c|1871| <<kvm_set_spte_gfn>> kvm_pgtable_stage2_map(kvm->arch.mmu.pgt, range->start << PAGE_SHIFT, PAGE_SIZE, __pfn_to_phys(pfn), KVM_PGTABLE_PROT_R, NULL, 0);
+ *
+ * kvm_pgtable_walk()的使用者
+ */
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			   u64 phys, enum kvm_pgtable_prot prot,
 			   void *mc, enum kvm_pgtable_walk_flags flags)
@@ -1107,6 +1546,9 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 	return ret;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 int kvm_pgtable_stage2_set_owner(struct kvm_pgtable *pgt, u64 addr, u64 size,
 				 void *mc, u8 owner_id)
 {
@@ -1132,6 +1574,10 @@ int kvm_pgtable_stage2_set_owner(struct kvm_pgtable *pgt, u64 addr, u64 size,
 	return ret;
 }
 
+/*
+ * 在以下使用stage2_unmap_walker():
+ *   - arch/arm64/kvm/hyp/pgtable.c|1528| <<kvm_pgtable_stage2_unmap>> .cb = stage2_unmap_walker,
+ */
 static int stage2_unmap_walker(const struct kvm_pgtable_visit_ctx *ctx,
 			       enum kvm_pgtable_walk_flags visit)
 {
@@ -1175,6 +1621,9 @@ static int stage2_unmap_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 int kvm_pgtable_stage2_unmap(struct kvm_pgtable *pgt, u64 addr, u64 size)
 {
 	int ret;
@@ -1199,6 +1648,16 @@ struct stage2_attr_data {
 	s8				level;
 };
 
+/*
+ * 在以下使用stage2_attr_walker():
+ *   - arch/arm64/kvm/hyp/pgtable.c|1251| <<stage2_update_leaf_attrs>> .cb = stage2_attr_walker,
+ *
+ * 1254         struct kvm_pgtable_walker walker = {
+ * 1255                 .cb             = stage2_attr_walker,
+ * 1256                 .arg            = &data,
+ * 1257                 .flags          = flags | KVM_PGTABLE_WALK_LEAF,
+ * 1258         };
+ */
 static int stage2_attr_walker(const struct kvm_pgtable_visit_ctx *ctx,
 			      enum kvm_pgtable_walk_flags visit)
 {
@@ -1236,6 +1695,33 @@ static int stage2_attr_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * flags的例子. 
+ * enum kvm_pgtable_walk_flags - Flags to control a depth-first page-table walk.
+ * @KVM_PGTABLE_WALK_LEAF:              Visit leaf entries, including invalid
+ *                                      entries.
+ * @KVM_PGTABLE_WALK_TABLE_PRE:         Visit table entries before their
+ *                                      children.
+ * @KVM_PGTABLE_WALK_TABLE_POST:        Visit table entries after their
+ *                                      children.
+ * @KVM_PGTABLE_WALK_SHARED:            Indicates the page-tables may be shared
+ *                                      with other software walkers.
+ * @KVM_PGTABLE_WALK_HANDLE_FAULT:      Indicates the page-table walk was
+ *                                      invoked from a fault handler.
+ * @KVM_PGTABLE_WALK_SKIP_BBM_TLBI:     Visit and update table entries
+ *                                      without Break-before-make's
+ *                                      TLB invalidation.
+ * @KVM_PGTABLE_WALK_SKIP_CMO:          Visit and update table entries
+ *                                      without Cache maintenance
+ *                                      operations required.
+ *
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1321| <<kvm_pgtable_stage2_wrprotect>> return stage2_update_leaf_attrs(pgt, addr, size, 0, KVM_PTE_LEAF_ATTR_LO_S2_S2AP_W, NULL, NULL, 0);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1331| <<kvm_pgtable_stage2_mkyoung>> ret = stage2_update_leaf_attrs(pgt, addr, 1, KVM_PTE_LEAF_ATTR_LO_S2_AF, 0, &pte, NULL, KVM_PGTABLE_WALK_HANDLE_FAULT | KVM_PGTABLE_WALK_SHARED);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1410| <<kvm_pgtable_stage2_relax_perms>> ret = stage2_update_leaf_attrs(pgt, addr, 1, set, clr, NULL, &level, KVM_PGTABLE_WALK_HANDLE_FAULT | KVM_PGTABLE_WALK_SHARED);
+ *
+ * kvm_pgtable_walk()的使用者
+ */
 static int stage2_update_leaf_attrs(struct kvm_pgtable *pgt, u64 addr,
 				    u64 size, kvm_pte_t attr_set,
 				    kvm_pte_t attr_clr, kvm_pte_t *orig_pte,
@@ -1243,6 +1729,14 @@ static int stage2_update_leaf_attrs(struct kvm_pgtable *pgt, u64 addr,
 {
 	int ret;
 	kvm_pte_t attr_mask = KVM_PTE_LEAF_ATTR_LO | KVM_PTE_LEAF_ATTR_HI;
+	/*
+	 * struct stage2_attr_data {
+	 *     kvm_pte_t attr_set;
+	 *     kvm_pte_t attr_clr;
+	 *     kvm_pte_t pte;
+	 *     s8 level;
+	 * };
+	 */
 	struct stage2_attr_data data = {
 		.attr_set	= attr_set & attr_mask,
 		.attr_clr	= attr_clr & attr_mask,
@@ -1253,6 +1747,25 @@ static int stage2_update_leaf_attrs(struct kvm_pgtable *pgt, u64 addr,
 		.flags		= flags | KVM_PGTABLE_WALK_LEAF,
 	};
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|598| <<check_page_state_range>> return kvm_pgtable_walk(pgt, addr, size, &walker);
+	 *   - arch/arm64/kvm/hyp/nvhe/mm.c|302| <<create_fixmap_slot>> return kvm_pgtable_walk(&pkvm_pgtable, addr, PAGE_SIZE, &walker);
+	 *   - arch/arm64/kvm/hyp/nvhe/setup.c|238| <<fix_host_ownership>> ret = kvm_pgtable_walk(&pkvm_pgtable, start, reg->size, &walker);
+	 *   - arch/arm64/kvm/hyp/nvhe/setup.c|254| <<fix_hyp_pgtable_refcnt>> return kvm_pgtable_walk(&pkvm_pgtable, 0, BIT(pkvm_pgtable.ia_bits),
+	 *   - arch/arm64/kvm/hyp/pgtable.c|372| <<kvm_pgtable_get_leaf>> ret = kvm_pgtable_walk(pgt, ALIGN_DOWN(addr, PAGE_SIZE),
+	 *   - arch/arm64/kvm/hyp/pgtable.c|506| <<kvm_pgtable_hyp_map>> ret = kvm_pgtable_walk(pgt, addr, size, &walker);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|564| <<kvm_pgtable_hyp_unmap>> kvm_pgtable_walk(pgt, addr, size, &walker);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|614| <<kvm_pgtable_hyp_destroy>> WARN_ON(kvm_pgtable_walk(pgt, 0, BIT(pgt->ia_bits), &walker));
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1105| <<kvm_pgtable_stage2_map>> ret = kvm_pgtable_walk(pgt, addr, size, &walker);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1131| <<kvm_pgtable_stage2_set_owner>> ret = kvm_pgtable_walk(pgt, addr, size, &walker);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1187| <<kvm_pgtable_stage2_unmap>> ret = kvm_pgtable_walk(pgt, addr, size, &walker);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1266| <<stage2_update_leaf_attrs>> ret = kvm_pgtable_walk(pgt, addr, size, &walker);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1346| <<kvm_pgtable_stage2_test_clear_young>> WARN_ON(kvm_pgtable_walk(pgt, addr, size, &walker));
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1403| <<kvm_pgtable_stage2_flush>> return kvm_pgtable_walk(pgt, addr, size, &walker);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1558| <<kvm_pgtable_stage2_split>> return kvm_pgtable_walk(pgt, addr, size, &walker);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1623| <<kvm_pgtable_stage2_destroy>> WARN_ON(kvm_pgtable_walk(pgt, 0, BIT(pgt->ia_bits), &walker));
+	 */
 	ret = kvm_pgtable_walk(pgt, addr, size, &walker);
 	if (ret)
 		return ret;
@@ -1272,6 +1785,10 @@ int kvm_pgtable_stage2_wrprotect(struct kvm_pgtable *pgt, u64 addr, u64 size)
 					NULL, NULL, 0);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1683| <<handle_access_fault>> pte = kvm_pgtable_stage2_mkyoung(mmu->pgt, fault_ipa);
+ */
 kvm_pte_t kvm_pgtable_stage2_mkyoung(struct kvm_pgtable *pgt, u64 addr)
 {
 	kvm_pte_t pte = 0;
@@ -1292,6 +1809,10 @@ struct stage2_age_data {
 	bool	young;
 };
 
+/*
+ * 在以下使用stage2_age_walker():
+ *   - arch/arm64/kvm/hyp/pgtable.c|1727| <<kvm_pgtable_stage2_test_clear_young>> .cb = stage2_age_walker,
+ */
 static int stage2_age_walker(const struct kvm_pgtable_visit_ctx *ctx,
 			     enum kvm_pgtable_walk_flags visit)
 {
@@ -1321,6 +1842,9 @@ static int stage2_age_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 bool kvm_pgtable_stage2_test_clear_young(struct kvm_pgtable *pgt, u64 addr,
 					 u64 size, bool mkold)
 {
@@ -1337,6 +1861,40 @@ bool kvm_pgtable_stage2_test_clear_young(struct kvm_pgtable *pgt, u64 addr,
 	return data.young;
 }
 
+/*
+ * |        kvm_pgtable_stage2_relax_perms() {
+ * |          do_interrupt_handler() {
+ * |
+ * |            gic_handle_irq() {
+ * |              irq_enter() {
+ * |                irq_enter_rcu();
+ * |              }
+ * |              ipi_handler() {
+ * |                do_handle_IPI() {
+ * |                  arch_send_call_function_single_ipi() {
+ * |                    smp_cross_call() {
+ * |                      cpu_logical_map();
+ * |                      cpu_logical_map();
+ * |                    }
+ * |                  }
+ * |                }
+ * |              }
+ * |              irq_exit();
+ * |            }
+ * |
+ * |          }
+ * |          kvm_pgtable_walk() {
+ * |            __kvm_pgtable_walk() {
+ * |              kvm_host_va();
+ * |              __kvm_pgtable_walk() {
+ * |                stage2_attr_walker() {
+ * |                  kvm_host_va();
+ * |                  invalidate_icache_guest_page() {
+ * |                    do_interrupt_handler() {
+ *
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1601| <<user_mem_abort>> ret = kvm_pgtable_stage2_relax_perms(pgt, fault_ipa, prot);
+ */
 int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
 				   enum kvm_pgtable_prot prot)
 {
@@ -1356,6 +1914,12 @@ int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
 	if (prot & KVM_PGTABLE_PROT_X)
 		clr |= KVM_PTE_LEAF_ATTR_HI_S2_XN;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1321| <<kvm_pgtable_stage2_wrprotect>> return stage2_update_leaf_attrs(pgt, addr, size, 0, KVM_PTE_LEAF_ATTR_LO_S2_S2AP_W, NULL, NULL, 0);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1331| <<kvm_pgtable_stage2_mkyoung>> ret = stage2_update_leaf_attrs(pgt, addr, 1, KVM_PTE_LEAF_ATTR_LO_S2_AF, 0, &pte, NULL, KVM_PGTABLE_WALK_HANDLE_FAULT | KVM_PGTABLE_WALK_SHARED);
+	 *   - arch/arm64/kvm/hyp/pgtable.c|1410| <<kvm_pgtable_stage2_relax_perms>> ret = stage2_update_leaf_attrs(pgt, addr, 1, set, clr, NULL, &level, KVM_PGTABLE_WALK_HANDLE_FAULT | KVM_PGTABLE_WALK_SHARED);
+	 */
 	ret = stage2_update_leaf_attrs(pgt, addr, 1, set, clr, NULL, &level,
 				       KVM_PGTABLE_WALK_HANDLE_FAULT |
 				       KVM_PGTABLE_WALK_SHARED);
@@ -1364,6 +1928,10 @@ int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
 	return ret;
 }
 
+/*
+ * 在以下使用stage2_flush_walker():
+ *   - arch/arm64/kvm/hyp/pgtable.c|1824| <<kvm_pgtable_stage2_flush>> .cb = stage2_flush_walker,
+ */
 static int stage2_flush_walker(const struct kvm_pgtable_visit_ctx *ctx,
 			       enum kvm_pgtable_walk_flags visit)
 {
@@ -1379,6 +1947,9 @@ static int stage2_flush_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 int kvm_pgtable_stage2_flush(struct kvm_pgtable *pgt, u64 addr, u64 size)
 {
 	struct kvm_pgtable_walker walker = {
@@ -1393,6 +1964,9 @@ int kvm_pgtable_stage2_flush(struct kvm_pgtable *pgt, u64 addr, u64 size)
 	return kvm_pgtable_walk(pgt, addr, size, &walker);
 }
 
+/*
+ * __kvm_pgtable_walk()的使用者
+ */
 kvm_pte_t *kvm_pgtable_stage2_create_unlinked(struct kvm_pgtable *pgt,
 					      u64 phys, s8 level,
 					      enum kvm_pgtable_prot prot,
@@ -1451,6 +2025,10 @@ kvm_pte_t *kvm_pgtable_stage2_create_unlinked(struct kvm_pgtable *pgt,
  * fully populated tree up to the PTE entries. Note that @level is
  * interpreted as in "level @level entry".
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1930| <<stage2_split_walker>> nr_pages = stage2_block_get_nr_page_tables(level);
+ */
 static int stage2_block_get_nr_page_tables(s8 level)
 {
 	switch (level) {
@@ -1467,6 +2045,10 @@ static int stage2_block_get_nr_page_tables(s8 level)
 	};
 }
 
+/*
+ * 在以下使用stage2_split_walker():
+ *   - arch/arm64/kvm/hyp/pgtable.c|1985| <<kvm_pgtable_stage2_split>> .cb = stage2_split_walker,
+ */
 static int stage2_split_walker(const struct kvm_pgtable_visit_ctx *ctx,
 			       enum kvm_pgtable_walk_flags visit)
 {
@@ -1536,6 +2118,9 @@ static int stage2_split_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 int kvm_pgtable_stage2_split(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			     struct kvm_mmu_memory_cache *mc)
 {
@@ -1548,6 +2133,24 @@ int kvm_pgtable_stage2_split(struct kvm_pgtable *pgt, u64 addr, u64 size,
 	return kvm_pgtable_walk(pgt, addr, size, &walker);
 }
 
+/*
+ * crash> struct kvm_pgtable ffff3fffe4412800
+ * struct kvm_pgtable {
+ *   ia_bits = 40,
+ *   start_level = 1,
+ *   pgd = 0xffff3fff920cc000,
+ *   mm_ops = 0xffffbb087c3a3768 <kvm_s2_mm_ops>,
+ *   mmu = 0xffff800080e1d9c8,
+ *   flags = 0,
+ *   force_pte_cb = 0x0
+ * }
+ */
+/*
+ * called by:
+ *   - arch/arm64/include/asm/kvm_pgtable.h|561| <<kvm_pgtable_stage2_init>> __kvm_pgtable_stage2_init(pgt, mmu, mm_ops, 0, NULL)
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|151| <<kvm_host_prepare_stage2>> ret = __kvm_pgtable_stage2_init(&host_mmu.pgt, mmu, &host_mmu.mm_ops, KVM_HOST_S2_FLAGS, host_stage2_force_pte_cb);
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|258| <<kvm_guest_prepare_stage2>> ret = __kvm_pgtable_stage2_init(mmu->pgt, mmu, &vm->mm_ops, 0, guest_stage2_force_pte_cb);
+ */
 int __kvm_pgtable_stage2_init(struct kvm_pgtable *pgt, struct kvm_s2_mmu *mmu,
 			      struct kvm_pgtable_mm_ops *mm_ops,
 			      enum kvm_pgtable_stage2_flags flags,
@@ -1559,6 +2162,12 @@ int __kvm_pgtable_stage2_init(struct kvm_pgtable *pgt, struct kvm_s2_mmu *mmu,
 	u32 sl0 = FIELD_GET(VTCR_EL2_SL0_MASK, vtcr);
 	s8 start_level = VTCR_EL2_TGRAN_SL0_BASE - sl0;
 
+	/*
+	 * 假设ia_bits = 40和start_level = 1
+	 * ipa物理地址最大是(1<<40)-1=0xffffffffff (10个ff)
+	 * 从level 1开始的话, 两个page就能代表所有的ipa地址
+	 * 也就是说, 从eptp开始的指针(类似cr3), 两个page就能表示所有的ipa range
+	 */
 	pgd_sz = kvm_pgd_pages(ia_bits, start_level) * PAGE_SIZE;
 	pgt->pgd = (kvm_pteref_t)mm_ops->zalloc_pages_exact(pgd_sz);
 	if (!pgt->pgd)
@@ -1576,6 +2185,12 @@ int __kvm_pgtable_stage2_init(struct kvm_pgtable *pgt, struct kvm_s2_mmu *mmu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|238| <<kvm_guest_prepare_stage2>> nr_pages = kvm_pgtable_stage2_pgd_size(mmu->vtcr) >> PAGE_SHIFT;
+ *   - arch/arm64/kvm/hyp/nvhe/pkvm.c|490| <<__pkvm_init_vm>> pgd_size = kvm_pgtable_stage2_pgd_size(host_mmu.arch.mmu.vtcr);
+ *   - arch/arm64/kvm/pkvm.c|137| <<__pkvm_create_hyp_vm>> pgd_sz = kvm_pgtable_stage2_pgd_size(host_kvm->arch.mmu.vtcr);
+ */
 size_t kvm_pgtable_stage2_pgd_size(u64 vtcr)
 {
 	u32 ia_bits = VTCR_EL2_IPA(vtcr);
@@ -1585,6 +2200,11 @@ size_t kvm_pgtable_stage2_pgd_size(u64 vtcr)
 	return kvm_pgd_pages(ia_bits, start_level) * PAGE_SIZE;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|2077| <<kvm_pgtable_stage2_destroy>> .cb = stage2_free_walker,
+ *   - arch/arm64/kvm/hyp/pgtable.c|2092| <<kvm_pgtable_stage2_free_unlinked>> .cb = stage2_free_walker,
+ */
 static int stage2_free_walker(const struct kvm_pgtable_visit_ctx *ctx,
 			      enum kvm_pgtable_walk_flags visit)
 {
@@ -1601,6 +2221,9 @@ static int stage2_free_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * kvm_pgtable_walk()的使用者
+ */
 void kvm_pgtable_stage2_destroy(struct kvm_pgtable *pgt)
 {
 	size_t pgd_sz;
@@ -1616,6 +2239,9 @@ void kvm_pgtable_stage2_destroy(struct kvm_pgtable *pgt)
 	pgt->pgd = NULL;
 }
 
+/*
+ * __kvm_pgtable_walk()的使用者
+ */
 void kvm_pgtable_stage2_free_unlinked(struct kvm_pgtable_mm_ops *mm_ops, void *pgtable, s8 level)
 {
 	kvm_pteref_t ptep = (kvm_pteref_t)pgtable;
diff --git a/arch/arm64/kvm/mmio.c b/arch/arm64/kvm/mmio.c
index 200c8019a..afacf3cfe 100644
--- a/arch/arm64/kvm/mmio.c
+++ b/arch/arm64/kvm/mmio.c
@@ -120,6 +120,10 @@ int kvm_handle_mmio_return(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1790| <<kvm_handle_guest_abort>> ret = io_mem_abort(vcpu, fault_ipa);
+ */
 int io_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
 {
 	struct kvm_run *run = vcpu->run;
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index dc04bc767..d29365fae 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -275,6 +275,29 @@ static void clean_dcache_guest_page(void *va, size_t size)
 	__clean_dcache_guest_page(va, size);
 }
 
+/*
+ * 在以下使用kvm_pgtable_mm_ops->icache_inval_pou():
+ *   - arch/arm64/kvm/mmu.c|855| <<global>> .icache_inval_pou = invalidate_icache_guest_page,
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|254| <<kvm_guest_prepare_stage2>> .icache_inval_pou = invalidate_icache_guest_page,
+ *   - arch/arm64/kvm/hyp/pgtable.c|991| <<stage2_map_walker_try_leaf>> if (!kvm_pgtable_walk_skip_cmo(ctx) && mm_ops->icache_inval_pou &&
+ *   - arch/arm64/kvm/hyp/pgtable.c|993| <<stage2_map_walker_try_leaf>> mm_ops->icache_inval_pou(kvm_pte_follow(new, mm_ops), granule);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1227| <<stage2_attr_walker>> if (mm_ops->icache_inval_pou &&
+ *   - arch/arm64/kvm/hyp/pgtable.c|1229| <<stage2_attr_walker>> mm_ops->icache_inval_pou(kvm_pte_follow(pte, mm_ops),
+ *
+ * 844 static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
+ * 845         .zalloc_page            = stage2_memcache_zalloc_page,
+ * 846         .zalloc_pages_exact     = kvm_s2_zalloc_pages_exact,
+ * 847         .free_pages_exact       = kvm_s2_free_pages_exact,
+ * 848         .free_unlinked_table    = stage2_free_unlinked_table,
+ * 849         .get_page               = kvm_host_get_page,
+ * 850         .put_page               = kvm_s2_put_page,
+ * 851         .page_count             = kvm_host_page_count,
+ * 852         .phys_to_virt           = kvm_host_va,
+ * 853         .virt_to_phys           = kvm_host_pa,
+ * 854         .dcache_clean_inval_poc = clean_dcache_guest_page,
+ * 855         .icache_inval_pou       = invalidate_icache_guest_page,
+ * 856 };
+ */
 static void invalidate_icache_guest_page(void *va, size_t size)
 {
 	__invalidate_icache_guest_page(va, size);
@@ -841,6 +864,11 @@ static int get_user_mapping_size(struct kvm *kvm, u64 addr)
 	return BIT(ARM64_HW_PGTABLE_LEVEL_SHIFT(level));
 }
 
+/*
+ * 在以下使用kvm_s2_mm_ops:
+ *   - arch/arm64/kvm/mmu.c|228| <<stage2_free_unlinked_table_rcu_cb>> kvm_pgtable_stage2_free_unlinked(&kvm_s2_mm_ops, pgtable, level);
+ *   - arch/arm64/kvm/mmu.c|909| <<kvm_init_stage2_mmu>> err = kvm_pgtable_stage2_init(pgt, mmu, &kvm_s2_mm_ops);
+ */
 static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
 	.zalloc_page		= stage2_memcache_zalloc_page,
 	.zalloc_pages_exact	= kvm_s2_zalloc_pages_exact,
@@ -1374,6 +1402,45 @@ static bool kvm_vma_mte_allowed(struct vm_area_struct *vma)
 	return vma->vm_flags & VM_MTE_ALLOWED;
 }
 
+/*
+ * |        kvm_pgtable_stage2_relax_perms() {
+ * |          do_interrupt_handler() {
+ * |
+ * |            gic_handle_irq() {
+ * |              irq_enter() {
+ * |                irq_enter_rcu();
+ * |              }
+ * |              ipi_handler() {
+ * |                do_handle_IPI() {
+ * |                  arch_send_call_function_single_ipi() {
+ * |                    smp_cross_call() {
+ * |                      cpu_logical_map();
+ * |                      cpu_logical_map();
+ * |                    }
+ * |                  }
+ * |                }
+ * |              }
+ * |              irq_exit();
+ * |            }
+ * |
+ * |          }
+ * |          kvm_pgtable_walk() {
+ * |            __kvm_pgtable_walk() {
+ * |              kvm_host_va();
+ * |              __kvm_pgtable_walk() {
+ * |                stage2_attr_walker() {
+ * |                  kvm_host_va();
+ * |                  invalidate_icache_guest_page() {
+ * |                    do_interrupt_handler() {
+ */
+/*
+ * 在以下使用kvm_handle_guest_abort():
+ *   - arch/arm64/kvm/handle_exit.c|272| <<global>> [ESR_ELx_EC_IABT_LOW] = kvm_handle_guest_abort,
+ *   - arch/arm64/kvm/handle_exit.c|273| <<global>> [ESR_ELx_EC_DABT_LOW] = kvm_handle_guest_abort,
+ *
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1765| <<kvm_handle_guest_abort>> ret = user_mem_abort(vcpu, fault_ipa, memslot, hva, esr_fsc_is_permission_fault(esr));
+ */
 static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 			  struct kvm_memory_slot *memslot, unsigned long hva,
 			  bool fault_is_perm)
@@ -1470,6 +1537,9 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 		fault_ipa &= ~(vma_pagesize - 1);
 
 	gfn = fault_ipa >> PAGE_SHIFT;
+	/*
+	 * 似乎tagged memory?
+	 */
 	mte_allowed = kvm_vma_mte_allowed(vma);
 
 	vfio_allow_any_uc = vma->vm_flags & VM_ALLOW_ANY_UNCACHED;
@@ -1521,6 +1591,12 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 		return -ENOEXEC;
 
 	read_lock(&kvm->mmu_lock);
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_s2_mmu *hw_mmu;
+	 *       -> struct kvm_pgtable *pgt;
+	 */
 	pgt = vcpu->arch.hw_mmu->pgt;
 	if (mmu_invalidate_retry(kvm, mmu_seq))
 		goto out_unlock;
@@ -1611,6 +1687,35 @@ static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
 		kvm_set_pfn_accessed(kvm_pte_to_pfn(pte));
 }
 
+/*
+ * 255 static exit_handle_fn arm_exit_handlers[] = {
+ * 256         [0 ... ESR_ELx_EC_MAX]  = kvm_handle_unknown_ec,
+ * 257         [ESR_ELx_EC_WFx]        = kvm_handle_wfx,
+ * 258         [ESR_ELx_EC_CP15_32]    = kvm_handle_cp15_32,
+ * 259         [ESR_ELx_EC_CP15_64]    = kvm_handle_cp15_64,
+ * 260         [ESR_ELx_EC_CP14_MR]    = kvm_handle_cp14_32,
+ * 261         [ESR_ELx_EC_CP14_LS]    = kvm_handle_cp14_load_store,
+ * 262         [ESR_ELx_EC_CP10_ID]    = kvm_handle_cp10_id,
+ * 263         [ESR_ELx_EC_CP14_64]    = kvm_handle_cp14_64,
+ * 264         [ESR_ELx_EC_HVC32]      = handle_hvc,
+ * 265         [ESR_ELx_EC_SMC32]      = handle_smc,
+ * 266         [ESR_ELx_EC_HVC64]      = handle_hvc,
+ * 267         [ESR_ELx_EC_SMC64]      = handle_smc,
+ * 268         [ESR_ELx_EC_SVC64]      = handle_svc,
+ * 269         [ESR_ELx_EC_SYS64]      = kvm_handle_sys_reg,
+ * 270         [ESR_ELx_EC_SVE]        = handle_sve,
+ * 271         [ESR_ELx_EC_ERET]       = kvm_handle_eret,
+ * 272         [ESR_ELx_EC_IABT_LOW]   = kvm_handle_guest_abort,
+ * 273         [ESR_ELx_EC_DABT_LOW]   = kvm_handle_guest_abort,
+ * 274         [ESR_ELx_EC_SOFTSTP_LOW]= kvm_handle_guest_debug,
+ * 275         [ESR_ELx_EC_WATCHPT_LOW]= kvm_handle_guest_debug,
+ * 276         [ESR_ELx_EC_BREAKPT_LOW]= kvm_handle_guest_debug,
+ * 277         [ESR_ELx_EC_BKPT32]     = kvm_handle_guest_debug,
+ * 278         [ESR_ELx_EC_BRK64]      = kvm_handle_guest_debug,
+ * 279         [ESR_ELx_EC_FP_ASIMD]   = handle_no_fpsimd,
+ * 280         [ESR_ELx_EC_PAC]        = kvm_handle_ptrauth,
+ * 281 };
+ */
 /**
  * kvm_handle_guest_abort - handles all 2nd stage aborts
  * @vcpu:	the VCPU pointer
@@ -1622,6 +1727,11 @@ static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
  * space. The distinction is based on the IPA causing the fault and whether this
  * memory region has been registered as standard RAM by user space.
  */
+/*
+ * 在以下使用kvm_handle_guest_abort():
+ *   - arch/arm64/kvm/handle_exit.c|272| <<global>> [ESR_ELx_EC_IABT_LOW] = kvm_handle_guest_abort,
+ *   - arch/arm64/kvm/handle_exit.c|273| <<global>> [ESR_ELx_EC_DABT_LOW] = kvm_handle_guest_abort,
+ */
 int kvm_handle_guest_abort(struct kvm_vcpu *vcpu)
 {
 	unsigned long esr;
@@ -1632,9 +1742,24 @@ int kvm_handle_guest_abort(struct kvm_vcpu *vcpu)
 	gfn_t gfn;
 	int ret, idx;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_vcpu_fault_info fault;
+	 *       -> u64 esr_el2;            // Hyp Syndrom Register
+	 *       -> u64 far_el2;            // Hyp Fault Address Register
+	 *       -> u64 hpfar_el2;          // Hyp IPA Fault Address Register
+	 *       -> u64 disr_el1;           // Deferred [SError] Status Register
+	 */
 	esr = kvm_vcpu_get_esr(vcpu);
 
 	fault_ipa = kvm_vcpu_get_fault_ipa(vcpu);
+	/*
+	 * #define ESR_ELx_EC_IABT_LOW     (0x20)
+	 * #define ESR_ELx_EC_IABT_CUR     (0x21)
+	 * #define ESR_ELx_EC_DABT_LOW     (0x24)
+	 * #define ESR_ELx_EC_DABT_CUR     (0x25)
+	 */
 	is_iabt = kvm_vcpu_trap_is_iabt(vcpu);
 
 	if (esr_fsc_is_translation_fault(esr)) {
diff --git a/arch/arm64/kvm/nested.c b/arch/arm64/kvm/nested.c
index ced30c905..b46b17c3b 100644
--- a/arch/arm64/kvm/nested.c
+++ b/arch/arm64/kvm/nested.c
@@ -181,6 +181,20 @@ u64 kvm_vcpu_sanitise_vncr_reg(const struct kvm_vcpu *vcpu, enum vcpu_sysreg sr)
 	return v;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/nested.c|219| <<kvm_init_nv_sysregs>> set_sysreg_masks(kvm, VTTBR_EL2, res0, res1);
+ *   - arch/arm64/kvm/nested.c|224| <<kvm_init_nv_sysregs>> set_sysreg_masks(kvm, VTCR_EL2, res0, res1);
+ *   - arch/arm64/kvm/nested.c|229| <<kvm_init_nv_sysregs>> set_sysreg_masks(kvm, VMPIDR_EL2, res0, res1);
+ *   - arch/arm64/kvm/nested.c|266| <<kvm_init_nv_sysregs>> set_sysreg_masks(kvm, HCR_EL2, res0, res1);
+ *   - arch/arm64/kvm/nested.c|308| <<kvm_init_nv_sysregs>> set_sysreg_masks(kvm, HCRX_EL2, res0, res1);
+ *   - arch/arm64/kvm/nested.c|348| <<kvm_init_nv_sysregs>> set_sysreg_masks(kvm, HFGRTR_EL2, res0 | __HFGRTR_EL2_RES0, res1);
+ *   - arch/arm64/kvm/nested.c|349| <<kvm_init_nv_sysregs>> set_sysreg_masks(kvm, HFGWTR_EL2, res0 | __HFGWTR_EL2_RES0, res1);
+ *   - arch/arm64/kvm/nested.c|387| <<kvm_init_nv_sysregs>> set_sysreg_masks(kvm, HDFGRTR_EL2, res0 | HDFGRTR_EL2_RES0, res1);
+ *   - arch/arm64/kvm/nested.c|396| <<kvm_init_nv_sysregs>> set_sysreg_masks(kvm, HFGWTR_EL2, res0 | HDFGWTR_EL2_RES0, res1);
+ *   - arch/arm64/kvm/nested.c|430| <<kvm_init_nv_sysregs>> set_sysreg_masks(kvm, HFGITR_EL2, res0, res1);
+ *   - arch/arm64/kvm/nested.c|437| <<kvm_init_nv_sysregs>> set_sysreg_masks(kvm, HAFGRTR_EL2, res0, res1);
+ */
 static void set_sysreg_masks(struct kvm *kvm, int sr, u64 res0, u64 res1)
 {
 	int i = sr - __VNCR_START__;
@@ -189,6 +203,10 @@ static void set_sysreg_masks(struct kvm *kvm, int sr, u64 res0, u64 res1)
 	kvm->arch.sysreg_masks->mask[i].res1 = res1;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|677| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_init_nv_sysregs(vcpu->kvm);
+ */
 int kvm_init_nv_sysregs(struct kvm *kvm)
 {
 	u64 res0, res1;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6efd1497b..c1c25e56a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1317,6 +1317,12 @@ struct kvm_arch {
 #define __KVM_HAVE_ARCH_NONCOHERENT_DMA
 	atomic_t noncoherent_dma_count;
 #define __KVM_HAVE_ARCH_ASSIGNED_DEVICE
+	/*
+	 * 在以下使用kvm_arch->assigned_device_count:
+	 *   - arch/x86/kvm/x86.c|13416| <<kvm_arch_start_assignment>> if (atomic_inc_return(&kvm->arch.assigned_device_count) == 1)
+	 *   - arch/x86/kvm/x86.c|13430| <<kvm_arch_end_assignment>> atomic_dec(&kvm->arch.assigned_device_count);
+	 *   - arch/x86/kvm/x86.c|13444| <<kvm_arch_has_assigned_device>> return raw_atomic_read(&kvm->arch.assigned_device_count);
+	 */
 	atomic_t assigned_device_count;
 	struct kvm_pic *vpic;
 	struct kvm_ioapic *vioapic;
@@ -1341,6 +1347,16 @@ struct kvm_arch {
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|572| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3139| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3145| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3311| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3391| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|6995| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|12567| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	s64 kvmclock_offset;
 
 	/*
@@ -1566,6 +1582,13 @@ struct kvm_vcpu_stat {
 	u64 insn_emulation_fail;
 	u64 hypercalls;
 	u64 irq_injections;
+	/*
+	 * 在以下使用kvm_vcpu_stat->nmi_injections:
+	 *   - arch/x86/kvm/x86.c|297| <<global>> STATS_DESC_COUNTER(VCPU, nmi_injections),
+	 *   - arch/x86/kvm/svm/svm.c|3573| <<svm_inject_nmi>> ++vcpu->stat.nmi_injections;
+	 *   - arch/x86/kvm/svm/svm.c|3604| <<svm_set_vnmi_pending>> ++vcpu->stat.nmi_injections;
+	 *   - arch/x86/kvm/vmx/vmx.c|4970| <<vmx_inject_nmi>> ++vcpu->stat.nmi_injections;
+	 */
 	u64 nmi_injections;
 	u64 req_event;
 	u64 nested_run;
@@ -1833,6 +1856,13 @@ struct kvm_x86_nested_ops {
 
 struct kvm_x86_init_ops {
 	int (*hardware_setup)(void);
+	/*
+	 * 在以下使用kvm_x86_init_ops->handle_intel_pt_intr:
+	 *   - arch/x86/kvm/vmx/vmx.c|8718| <<global>> struct kvm_x86_init_ops vmx_init_ops.handle_intel_pt_intr = NULL,
+	 *   - arch/x86/kvm/vmx/vmx.c|8691| <<hardware_setup>> vmx_init_ops.handle_intel_pt_intr = vmx_handle_intel_pt_intr;
+	 *   - arch/x86/kvm/vmx/vmx.c|8693| <<hardware_setup>> vmx_init_ops.handle_intel_pt_intr = NULL;
+	 *   - arch/x86/kvm/x86.c|9796| <<kvm_x86_vendor_init>> kvm_register_perf_callbacks(ops->handle_intel_pt_intr);
+	 */
 	unsigned int (*handle_intel_pt_intr)(void);
 
 	struct kvm_x86_ops *runtime_ops;
diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c
index 185738c72..c01278dd4 100644
--- a/arch/x86/kernel/apic/vector.c
+++ b/arch/x86/kernel/apic/vector.c
@@ -1071,6 +1071,10 @@ void irq_complete_move(struct irq_cfg *cfg)
 /*
  * Called from fixup_irqs() with @desc->lock held and interrupts disabled.
  */
+/*
+ * called by:
+ *   - kernel/irq/cpuhotplug.c|96| <<migrate_one_irq>> irq_force_complete_move(desc);
+ */
 void irq_force_complete_move(struct irq_desc *desc)
 {
 	struct apic_chip_data *apicd;
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index 8a47f8541..c31190384 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -1106,6 +1106,17 @@ static int kvm_hv_msr_set_crash_data(struct kvm *kvm, u32 index, u64 data)
  *
  * These two equivalencies are implemented in this function.
  */
+/*
+ *  98 struct ms_hyperv_tsc_page {
+ *  99         volatile u32 tsc_sequence;
+ * 100         u32 reserved1;
+ * 101         volatile u64 tsc_scale;
+ * 102         volatile s64 tsc_offset;
+ * 103 } __packed;
+ *
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|1204| <<kvm_hv_setup_tsc_page>> if (!compute_tsc_page_parameters(hv_clock, &hv->tsc_ref))
+ */
 static bool compute_tsc_page_parameters(struct pvclock_vcpu_time_info *hv_clock,
 					struct ms_hyperv_tsc_page *tsc_ref)
 {
@@ -1155,9 +1166,18 @@ static inline bool tsc_page_update_unsafe(struct kvm_hv *hv)
 		hv->hv_tsc_emulation_control;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3347| <<kvm_guest_time_update>> kvm_hv_setup_tsc_page(v->kvm, &vcpu->hv_clock);
+ */
 void kvm_hv_setup_tsc_page(struct kvm *kvm,
 			   struct pvclock_vcpu_time_info *hv_clock)
 {
+	/*
+	 * struct kvm kvm:
+	 * -> struct kvm_arch arch;
+	 *    -> struct kvm_hv hyperv;
+	 */
 	struct kvm_hv *hv = to_kvm_hv(kvm);
 	u32 tsc_seq;
 	u64 gfn;
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 68f3f6c26..de0e56496 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -44,6 +44,16 @@ static int kvm_set_ioapic_irq(struct kvm_kernel_irq_routing_entry *e,
 				line_status);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|495| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+ *   - arch/x86/kvm/ioapic.c|473| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+ *   - arch/x86/kvm/ioapic.c|477| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+ *   - arch/x86/kvm/irq_comm.c|144| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1528| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+ *   - arch/x86/kvm/x86.c|9967| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+ *   - arch/x86/kvm/xen.c|581| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+ */
 int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, struct dest_map *dest_map)
 {
@@ -101,6 +111,14 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|142| <<kvm_set_msi>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|177| <<kvm_arch_set_irq_inatomic>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|441| <<kvm_scan_ioapic_routes>> kvm_set_msi_irq(vcpu->kvm, entry, &irq);
+ *   - arch/x86/kvm/svm/avic.c|867| <<get_pi_vcpu_info>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/vmx/posted_intr.c|433| <<vmx_pi_update_irte>> kvm_set_msi_irq(kvm, e, &irq);
+ */
 void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 		     struct kvm_lapic_irq *irq)
 {
@@ -128,9 +146,26 @@ static inline bool kvm_msi_route_invalid(struct kvm *kvm,
 	return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
 }
 
+/*
+ * 在以下使用kvm_set_msi():
+ *   - arch/x86/kvm/irq_comm.c|321| <<kvm_set_routing_entry>> e->set = kvm_set_msi;
+ *   - virt/kvm/irqchip.c|98| <<kvm_send_userspace_msi>> return kvm_set_msi(&route, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false);
+ */
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 		struct kvm *kvm, int irq_source_id, int level, bool line_status)
 {
+	/*
+	 * 1601 struct kvm_lapic_irq {
+	 * 1602         u32 vector;
+	 * 1603         u16 delivery_mode;
+	 * 1604         u16 dest_mode;
+	 * 1605         bool level;
+	 * 1606         u16 trig_mode;
+	 * 1607         u32 shorthand;
+	 * 1608         u32 dest_id;
+	 * 1609         bool msi_redir_hint;
+	 * 1610 };
+	 */
 	struct kvm_lapic_irq irq;
 
 	if (kvm_msi_route_invalid(kvm, e))
@@ -139,8 +174,26 @@ int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 	if (!level)
 		return -1;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/irq_comm.c|142| <<kvm_set_msi>> kvm_set_msi_irq(kvm, e, &irq);
+	 *   - arch/x86/kvm/irq_comm.c|177| <<kvm_arch_set_irq_inatomic>> kvm_set_msi_irq(kvm, e, &irq);
+	 *   - arch/x86/kvm/irq_comm.c|441| <<kvm_scan_ioapic_routes>> kvm_set_msi_irq(vcpu->kvm, entry, &irq);
+	 *   - arch/x86/kvm/svm/avic.c|867| <<get_pi_vcpu_info>> kvm_set_msi_irq(kvm, e, &irq);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|433| <<vmx_pi_update_irte>> kvm_set_msi_irq(kvm, e, &irq);
+	 */
 	kvm_set_msi_irq(kvm, e, &irq);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/hyperv.c|495| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|473| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+	 *   - arch/x86/kvm/ioapic.c|477| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq_comm.c|144| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|1528| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|9967| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 *   - arch/x86/kvm/xen.c|581| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+	 */
 	return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
 }
 
@@ -256,6 +309,11 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 	synchronize_srcu(&kvm->irq_srcu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/i8259.c|374| <<pic_ioport_write>> kvm_fire_mask_notifiers(s->pics_state->kvm, SELECT_PIC(irq + off), irq + off, !!(s->imr & (1 << irq)));
+ *   - arch/x86/kvm/ioapic.c|370| <<ioapic_write_indirect>> kvm_fire_mask_notifiers(ioapic->kvm, KVM_IRQCHIP_IOAPIC, index, mask_after);
+ */
 void kvm_fire_mask_notifiers(struct kvm *kvm, unsigned irqchip, unsigned pin,
 			     bool mask)
 {
@@ -276,6 +334,10 @@ bool kvm_arch_can_set_irq_routing(struct kvm *kvm)
 	return irqchip_in_kernel(kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|209| <<setup_routing_entry>> r = kvm_set_routing_entry(kvm, e, ue);
+ */
 int kvm_set_routing_entry(struct kvm *kvm,
 			  struct kvm_kernel_irq_routing_entry *e,
 			  const struct kvm_irq_routing_entry *ue)
@@ -335,6 +397,11 @@ int kvm_set_routing_entry(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|869| <<get_pi_vcpu_info>> if (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu) || !kvm_irq_is_postable(&irq)) {
+ *   - arch/x86/kvm/vmx/posted_intr.c|434| <<vmx_pi_update_irte>> if (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu) || !kvm_irq_is_postable(&irq)) {
+ */
 bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 			     struct kvm_vcpu **dest_vcpu)
 {
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index a593b03c9..55d090a2f 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -604,6 +604,10 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * 处理KVM_REQ_PMI:
+ *   - arch/x86/kvm/x86.c|10827| <<vcpu_enter_guest>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
diff --git a/arch/x86/kvm/vmx/posted_intr.c b/arch/x86/kvm/vmx/posted_intr.c
index af662312f..526e07ba8 100644
--- a/arch/x86/kvm/vmx/posted_intr.c
+++ b/arch/x86/kvm/vmx/posted_intr.c
@@ -21,6 +21,13 @@
  * wake the target vCPUs.  vCPUs are removed from the list and the notification
  * vector is reset when the vCPU is scheduled in.
  */
+/*
+ * 在以下使用wakeup_vcpus_on_cpu:
+ *   - arch/x86/kvm/vmx/posted_intr.c|24| <<global>> static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|157| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|221| <<pi_wakeup_handler>> struct list_head *wakeup_list = &per_cpu(wakeup_vcpus_on_cpu, cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|236| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
+ */
 static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
 /*
  * Protect the per-CPU list with a per-CPU spinlock to handle task migration.
@@ -29,13 +36,36 @@ static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
  * CPU.  IRQs must be disabled when taking this lock, otherwise deadlock will
  * occur if a wakeup IRQ arrives and attempts to acquire the lock.
  */
+/*
+ * 在以下使用wakeup_vcpus_on_cpu_lock:
+ *   - arch/x86/kvm/vmx/posted_intr.c|32| <<global>> static DEFINE_PER_CPU(raw_spinlock_t, wakeup_vcpus_on_cpu_lock);
+ *   - arch/x86/kvm/vmx/posted_intr.c|92| <<vmx_vcpu_pi_load>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|94| <<vmx_vcpu_pi_load>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_enable_wakeup_handler>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|158| <<pi_enable_wakeup_handler>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|222| <<pi_wakeup_handler>> raw_spinlock_t *spinlock = &per_cpu(wakeup_vcpus_on_cpu_lock, cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|237| <<pi_init_cpu>> raw_spin_lock_init(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+ */
 static DEFINE_PER_CPU(raw_spinlock_t, wakeup_vcpus_on_cpu_lock);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|55| <<vmx_vcpu_pi_load>> struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|148| <<pi_enable_wakeup_handler>> struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|198| <<vmx_vcpu_pi_put>> struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|242| <<pi_has_pending_interrupt>> struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|331| <<vmx_pi_update_irte>> vcpu_info.pi_desc_addr = __pa(vcpu_to_pi_desc(vcpu));
+ */
 static inline struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)
 {
 	return &(to_vmx(vcpu)->pi_desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|117| <<vmx_vcpu_pi_load>> } while (pi_try_set_control(pi_desc, &old.control, new.control));
+ *   - arch/x86/kvm/vmx/posted_intr.c|167| <<pi_enable_wakeup_handler>> } while (pi_try_set_control(pi_desc, &old.control, new.control));
+ */
 static int pi_try_set_control(struct pi_desc *pi_desc, u64 *pold, u64 new)
 {
 	/*
@@ -50,6 +80,10 @@ static int pi_try_set_control(struct pi_desc *pi_desc, u64 *pold, u64 new)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1486| <<vmx_vcpu_load>> vmx_vcpu_pi_load(vcpu, cpu);
+ */
 void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -90,6 +124,13 @@ void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 	 */
 	if (pi_desc->nv == POSTED_INTR_WAKEUP_VECTOR) {
 		raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+		/*
+		 * 在以下使用vcpu_vmx->pi_wakeup_list:
+		 *   - arch/x86/kvm/vmx/posted_intr.c|127| <<vmx_vcpu_pi_load>> list_del(&vmx->pi_wakeup_list);
+		 *   - arch/x86/kvm/vmx/posted_intr.c|206| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+		 *   - arch/x86/kvm/vmx/posted_intr.c|295| <<pi_wakeup_handler>> list_for_each_entry(vmx, wakeup_list, pi_wakeup_list) {
+		 *   - arch/x86/kvm/vmx/vmx.c|7486| <<vmx_vcpu_create>> INIT_LIST_HEAD(&vmx->pi_wakeup_list);
+		 */
 		list_del(&vmx->pi_wakeup_list);
 		raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
 	}
@@ -114,6 +155,11 @@ void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 		 * descriptor was modified on "put" to use the wakeup vector.
 		 */
 		new.nv = POSTED_INTR_VECTOR;
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/vmx/posted_intr.c|117| <<vmx_vcpu_pi_load>> } while (pi_try_set_control(pi_desc, &old.control, new.control));
+		 *   - arch/x86/kvm/vmx/posted_intr.c|167| <<pi_enable_wakeup_handler>> } while (pi_try_set_control(pi_desc, &old.control, new.control));
+		 */
 	} while (pi_try_set_control(pi_desc, &old.control, new.control));
 
 	local_irq_restore(flags);
@@ -132,6 +178,11 @@ void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 		pi_set_on(pi_desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|193| <<vmx_needs_pi_wakeup>> return vmx_can_use_ipiv(vcpu) || vmx_can_use_vtd_pi(vcpu->kvm);
+ *   - arch/x86/kvm/vmx/posted_intr.c|282| <<vmx_pi_update_irte>> if (!vmx_can_use_vtd_pi(kvm))
+ */
 static bool vmx_can_use_vtd_pi(struct kvm *kvm)
 {
 	return irqchip_in_kernel(kvm) && enable_apicv &&
@@ -143,6 +194,10 @@ static bool vmx_can_use_vtd_pi(struct kvm *kvm)
  * Put the vCPU on this pCPU's list of vCPUs that needs to be awakened and set
  * WAKEUP as the notification vector in the PI descriptor.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|204| <<vmx_vcpu_pi_put>> pi_enable_wakeup_handler(vcpu);
+ */
 static void pi_enable_wakeup_handler(struct kvm_vcpu *vcpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -153,6 +208,19 @@ static void pi_enable_wakeup_handler(struct kvm_vcpu *vcpu)
 	local_irq_save(flags);
 
 	raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+	/*
+	 * 在以下使用vcpu_vmx->pi_wakeup_list:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|127| <<vmx_vcpu_pi_load>> list_del(&vmx->pi_wakeup_list);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|206| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+	 *   - arch/x86/kvm/vmx/posted_intr.c|295| <<pi_wakeup_handler>> list_for_each_entry(vmx, wakeup_list, pi_wakeup_list) {
+	 *   - arch/x86/kvm/vmx/vmx.c|7486| <<vmx_vcpu_create>> INIT_LIST_HEAD(&vmx->pi_wakeup_list);
+	 *
+	 * 在以下使用wakeup_vcpus_on_cpu:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|24| <<global>> static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|157| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+	 *   - arch/x86/kvm/vmx/posted_intr.c|221| <<pi_wakeup_handler>> struct list_head *wakeup_list = &per_cpu(wakeup_vcpus_on_cpu, cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|236| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
+	 */
 	list_add_tail(&vmx->pi_wakeup_list,
 		      &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
 	raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
@@ -180,6 +248,10 @@ static void pi_enable_wakeup_handler(struct kvm_vcpu *vcpu)
 	local_irq_restore(flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|200| <<vmx_vcpu_pi_put>> if (!vmx_needs_pi_wakeup(vcpu))
+ */
 static bool vmx_needs_pi_wakeup(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -193,6 +265,10 @@ static bool vmx_needs_pi_wakeup(struct kvm_vcpu *vcpu)
 	return vmx_can_use_ipiv(vcpu) || vmx_can_use_vtd_pi(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1493| <<vmx_vcpu_put>> vmx_vcpu_pi_put(vcpu);
+ */
 void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -215,14 +291,32 @@ void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)
 /*
  * Handler for POSTED_INTERRUPT_WAKEUP_VECTOR.
  */
+/*
+ * 在以下使用pi_wakeup_handler():
+ *   - arch/x86/kvm/vmx/vmx.c|8721| <<hardware_setup>> kvm_set_posted_intr_wakeup_handler(pi_wakeup_handler);
+ */
 void pi_wakeup_handler(void)
 {
 	int cpu = smp_processor_id();
+	/*
+	 * 在以下使用wakeup_vcpus_on_cpu:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|24| <<global>> static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|157| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+	 *   - arch/x86/kvm/vmx/posted_intr.c|221| <<pi_wakeup_handler>> struct list_head *wakeup_list = &per_cpu(wakeup_vcpus_on_cpu, cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|236| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
+	 */
 	struct list_head *wakeup_list = &per_cpu(wakeup_vcpus_on_cpu, cpu);
 	raw_spinlock_t *spinlock = &per_cpu(wakeup_vcpus_on_cpu_lock, cpu);
 	struct vcpu_vmx *vmx;
 
 	raw_spin_lock(spinlock);
+	/*
+	 * 在以下使用vcpu_vmx->pi_wakeup_list:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|127| <<vmx_vcpu_pi_load>> list_del(&vmx->pi_wakeup_list);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|206| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+	 *   - arch/x86/kvm/vmx/posted_intr.c|295| <<pi_wakeup_handler>> list_for_each_entry(vmx, wakeup_list, pi_wakeup_list) {
+	 *   - arch/x86/kvm/vmx/vmx.c|7486| <<vmx_vcpu_create>> INIT_LIST_HEAD(&vmx->pi_wakeup_list);
+	 */
 	list_for_each_entry(vmx, wakeup_list, pi_wakeup_list) {
 
 		if (pi_test_on(&vmx->pi_desc))
@@ -231,12 +325,23 @@ void pi_wakeup_handler(void)
 	raw_spin_unlock(spinlock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8793| <<vmx_init>> pi_init_cpu(cpu);
+ */
 void __init pi_init_cpu(int cpu)
 {
 	INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
 	raw_spin_lock_init(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
 }
 
+/*
+ *
+ * 在以下调用pi_has_pending_interrupt():
+ *   - arch/x86/kvm/x86.c|13108| <<kvm_arch_dy_has_pending_interrupt>> static_call(kvm_x86_dy_apicv_has_pending_interrupt)(vcpu);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.dy_apicv_has_pending_interrupt = pi_has_pending_interrupt()
+ */
 bool pi_has_pending_interrupt(struct kvm_vcpu *vcpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -252,6 +357,12 @@ bool pi_has_pending_interrupt(struct kvm_vcpu *vcpu)
  * PI.NV to the wakeup vector, i.e. the assigned device
  * came along after the initial check in vmx_vcpu_pi_put().
  */
+/*
+ * 在以下调用vmx_pi_start_assignment():
+ *   - arch/x86/kvm/x86.c|13412| <<kvm_arch_start_assignment>> static_call_cond(kvm_x86_pi_start_assignment)(kvm);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.pi_start_assignment = vmx_pi_start_assignment()
+ */
 void vmx_pi_start_assignment(struct kvm *kvm)
 {
 	if (!irq_remapping_cap(IRQ_POSTING_CAP))
@@ -269,6 +380,15 @@ void vmx_pi_start_assignment(struct kvm *kvm)
  * @set: set or unset PI
  * returns 0 on success, < 0 on failure
  */
+/*
+ * 在以下使用vmx_pi_update_irte():
+ *   - arch/x86/kvm/x86.c|13475| <<kvm_arch_irq_bypass_add_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 1);
+ *   - arch/x86/kvm/x86.c|13500| <<kvm_arch_irq_bypass_del_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);
+ *   - arch/x86/kvm/x86.c|13511| <<kvm_arch_update_irqfd_routing>> return static_call(kvm_x86_pi_update_irte)(kvm, host_irq, guest_irq, set);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.pi_update_irte = vmx_pi_update_irte
+ * struct kvm_x86_ops svm_x86_ops.pi_update_irte = avic_pi_update_irte
+ */
 int vmx_pi_update_irte(struct kvm *kvm, unsigned int host_irq,
 		       uint32_t guest_irq, bool set)
 {
diff --git a/arch/x86/kvm/vmx/posted_intr.h b/arch/x86/kvm/vmx/posted_intr.h
index 269920765..f143673f9 100644
--- a/arch/x86/kvm/vmx/posted_intr.h
+++ b/arch/x86/kvm/vmx/posted_intr.h
@@ -30,64 +30,112 @@ struct pi_desc {
 	u32 rsvd[6];
 } __aligned(64);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4255| <<vmx_deliver_posted_interrupt>> if (pi_test_and_set_on(&vmx->pi_desc))
+ */
 static inline bool pi_test_and_set_on(struct pi_desc *pi_desc)
 {
 	return test_and_set_bit(POSTED_INTR_ON,
 			(unsigned long *)&pi_desc->control);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3874| <<vmx_complete_nested_posted_interrupt>> if (!pi_test_and_clear_on(vmx->nested.pi_desc))
+ */
 static inline bool pi_test_and_clear_on(struct pi_desc *pi_desc)
 {
 	return test_and_clear_bit(POSTED_INTR_ON,
 			(unsigned long *)&pi_desc->control);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|79| <<vmx_vcpu_pi_load>> if (pi_test_and_clear_sn(pi_desc))
+ */
 static inline bool pi_test_and_clear_sn(struct pi_desc *pi_desc)
 {
 	return test_and_clear_bit(POSTED_INTR_SN,
 			(unsigned long *)&pi_desc->control);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4251| <<vmx_deliver_posted_interrupt>> if (pi_test_and_set_pir(vector, &vmx->pi_desc))
+ */
 static inline bool pi_test_and_set_pir(int vector, struct pi_desc *pi_desc)
 {
 	return test_and_set_bit(vector, (unsigned long *)pi_desc->pir);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|131| <<vmx_vcpu_pi_load>> if (!pi_is_pir_empty(pi_desc))
+ *   - arch/x86/kvm/vmx/posted_intr.c|245| <<pi_has_pending_interrupt>> (pi_test_sn(pi_desc) && !pi_is_pir_empty(pi_desc));
+ */
 static inline bool pi_is_pir_empty(struct pi_desc *pi_desc)
 {
 	return bitmap_empty((unsigned long *)pi_desc->pir, NR_VECTORS);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|212| <<vmx_vcpu_pi_put>> pi_set_sn(pi_desc);
+ */
 static inline void pi_set_sn(struct pi_desc *pi_desc)
 {
 	set_bit(POSTED_INTR_SN,
 		(unsigned long *)&pi_desc->control);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|132| <<vmx_vcpu_pi_load>> pi_set_on(pi_desc);
+ */
 static inline void pi_set_on(struct pi_desc *pi_desc)
 {
 	set_bit(POSTED_INTR_ON,
 		(unsigned long *)&pi_desc->control);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6889| <<vmx_sync_pir_to_irr>> pi_clear_on(&vmx->pi_desc);
+ *   - arch/x86/kvm/vmx/vmx.c|6940| <<vmx_apicv_pre_state_restore>> pi_clear_on(&vmx->pi_desc);
+ */
 static inline void pi_clear_on(struct pi_desc *pi_desc)
 {
 	clear_bit(POSTED_INTR_ON,
 		(unsigned long *)&pi_desc->control);
 }
 
+/*
+ * 没人直接调用
+ */
 static inline void pi_clear_sn(struct pi_desc *pi_desc)
 {
 	clear_bit(POSTED_INTR_SN,
 		(unsigned long *)&pi_desc->control);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|177| <<pi_enable_wakeup_handler>> if (pi_test_on(&new))
+ *   - arch/x86/kvm/vmx/posted_intr.c|228| <<pi_wakeup_handler>> if (pi_test_on(&vmx->pi_desc))
+ *   - arch/x86/kvm/vmx/posted_intr.c|244| <<pi_has_pending_interrupt>> return pi_test_on(pi_desc) ||
+ *   - arch/x86/kvm/vmx/vmx.c|6888| <<vmx_sync_pir_to_irr>> if (pi_test_on(&vmx->pi_desc)) {
+ */
 static inline bool pi_test_on(struct pi_desc *pi_desc)
 {
 	return test_bit(POSTED_INTR_ON,
 			(unsigned long *)&pi_desc->control);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|245| <<pi_has_pending_interrupt>> (pi_test_sn(pi_desc) && !pi_is_pir_empty(pi_desc));
+ */
 static inline bool pi_test_sn(struct pi_desc *pi_desc)
 {
 	return test_bit(POSTED_INTR_SN,
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 22411f4af..0f57b137a 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -4152,6 +4152,11 @@ static void vmx_msr_filter_changed(struct kvm_vcpu *vcpu)
 		pt_update_intercept_for_msr(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4226| <<vmx_deliver_nested_posted_interrupt>> kvm_vcpu_trigger_posted_interrupt(vcpu, POSTED_INTR_NESTED_VECTOR);
+ *   - arch/x86/kvm/vmx/vmx.c|4264| <<vmx_deliver_posted_interrupt>> kvm_vcpu_trigger_posted_interrupt(vcpu, POSTED_INTR_VECTOR);
+ */
 static inline void kvm_vcpu_trigger_posted_interrupt(struct kvm_vcpu *vcpu,
 						     int pi_vec)
 {
@@ -7483,6 +7488,13 @@ static int vmx_vcpu_create(struct kvm_vcpu *vcpu)
 	BUILD_BUG_ON(offsetof(struct vcpu_vmx, vcpu) != 0);
 	vmx = to_vmx(vcpu);
 
+	/*
+	 * 在以下使用vcpu_vmx->pi_wakeup_list:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|127| <<vmx_vcpu_pi_load>> list_del(&vmx->pi_wakeup_list);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|206| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+	 *   - arch/x86/kvm/vmx/posted_intr.c|295| <<pi_wakeup_handler>> list_for_each_entry(vmx, wakeup_list, pi_wakeup_list) {
+	 *   - arch/x86/kvm/vmx/vmx.c|7486| <<vmx_vcpu_create>> INIT_LIST_HEAD(&vmx->pi_wakeup_list);
+	 */
 	INIT_LIST_HEAD(&vmx->pi_wakeup_list);
 
 	err = -ENOMEM;
@@ -8455,6 +8467,16 @@ static struct kvm_x86_ops vmx_x86_ops __initdata = {
 	.get_untagged_addr = vmx_get_untagged_addr,
 };
 
+/*
+ * 在以下使用kvm_x86_init_ops->handle_intel_pt_intr:
+ *   - arch/x86/kvm/vmx/vmx.c|8718| <<global>> struct kvm_x86_init_ops vmx_init_ops.handle_intel_pt_intr = NULL,
+ *   - arch/x86/kvm/vmx/vmx.c|8691| <<hardware_setup>> vmx_init_ops.handle_intel_pt_intr = vmx_handle_intel_pt_intr;
+ *   - arch/x86/kvm/vmx/vmx.c|8693| <<hardware_setup>> vmx_init_ops.handle_intel_pt_intr = NULL;
+ *   - arch/x86/kvm/x86.c|9796| <<kvm_x86_vendor_init>> kvm_register_perf_callbacks(ops->handle_intel_pt_intr);
+ *
+ * 在以下使用vmx_handle_intel_pt_intr():
+ *   - arch/x86/kvm/vmx/vmx.c|8691| <<hardware_setup>> vmx_init_ops.handle_intel_pt_intr = vmx_handle_intel_pt_intr;
+ */
 static unsigned int vmx_handle_intel_pt_intr(void)
 {
 	struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 90f9e4434..d7deb53c8 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -325,6 +325,13 @@ struct vcpu_vmx {
 	/* Posted interrupt descriptor */
 	struct pi_desc pi_desc;
 
+	/*
+	 * 在以下使用vcpu_vmx->pi_wakeup_list:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|127| <<vmx_vcpu_pi_load>> list_del(&vmx->pi_wakeup_list);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|206| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+	 *   - arch/x86/kvm/vmx/posted_intr.c|295| <<pi_wakeup_handler>> list_for_each_entry(vmx, wakeup_list, pi_wakeup_list) {
+	 *   - arch/x86/kvm/vmx/vmx.c|7486| <<vmx_vcpu_create>> INIT_LIST_HEAD(&vmx->pi_wakeup_list);
+	 */
 	/* Used if this vCPU is waiting for PI notification wakeup. */
 	struct list_head pi_wakeup_list;
 
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 91478b769..87cc96c1b 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -2496,6 +2496,10 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3291| <<kvm_guest_time_update>> u64 tsc = compute_guest_tsc(v, kernel_ns);
+ */
 static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 {
 	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
@@ -2558,6 +2562,15 @@ static inline u64 __scale_tsc(u64 ratio, u64 tsc)
 	return mul_u64_u64_shr(tsc, ratio, kvm_caps.tsc_scaling_ratio_frac_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2575| <<kvm_compute_l1_tsc_offset>> tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2583| <<kvm_read_l1_tsc>> kvm_scale_tsc(host_tsc, vcpu->arch.l1_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2798| <<adjust_tsc_offset_host>> adjustment = kvm_scale_tsc((u64) adjustment, vcpu->arch.l1_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|3290| <<kvm_guest_time_update>> tgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz, v->arch.l1_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|4310| <<kvm_get_msr_common>> msr_info->data = kvm_scale_tsc(rdtsc(), ratio) + offset;
+ *   - arch/x86/kvm/x86.c|5761| <<kvm_arch_tsc_set_attr>> tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio) + offset;
+ */
 u64 kvm_scale_tsc(u64 tsc, u64 ratio)
 {
 	u64 _tsc = tsc;
@@ -3103,6 +3116,10 @@ static unsigned long get_cpu_tsc_khz(void)
 		return __this_cpu_read(cpu_tsc_khz);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3158| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ */
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
@@ -3212,6 +3229,10 @@ static void kvm_setup_guest_pvclock(struct kvm_vcpu *v,
 	trace_kvm_pvclock_update(v->vcpu_id, &vcpu->hv_clock);
 }
 
+/*
+ * 在以下处理:
+ *   - arch/x86/kvm/x86.c|10782| <<vcpu_enter_guest(KVM_REQ_CLOCK_UPDATE)>> r = kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -3275,6 +3296,9 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 *	very slowly.
 	 */
 	if (vcpu->tsc_catchup) {
+		/*
+		 * 只在此处调用
+		 */
 		u64 tsc = compute_guest_tsc(v, kernel_ns);
 		if (tsc > tsc_timestamp) {
 			adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
@@ -13406,25 +13430,68 @@ bool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu)
 		return kvm_lapic_enabled(vcpu) && apf_pageready_slot_free(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13482| <<kvm_arch_irq_bypass_add_producer>> kvm_arch_start_assignment(irqfd->kvm);
+ *   - virt/kvm/vfio.c|178| <<kvm_vfio_file_add>> kvm_arch_start_assignment(dev->kvm);
+ */
 void kvm_arch_start_assignment(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->assigned_device_count:
+	 *   - arch/x86/kvm/x86.c|13416| <<kvm_arch_start_assignment>> if (atomic_inc_return(&kvm->arch.assigned_device_count) == 1)
+	 *   - arch/x86/kvm/x86.c|13430| <<kvm_arch_end_assignment>> atomic_dec(&kvm->arch.assigned_device_count);
+	 *   - arch/x86/kvm/x86.c|13444| <<kvm_arch_has_assigned_device>> return raw_atomic_read(&kvm->arch.assigned_device_count);
+	 */
 	if (atomic_inc_return(&kvm->arch.assigned_device_count) == 1)
 		static_call_cond(kvm_x86_pi_start_assignment)(kvm);
 }
 EXPORT_SYMBOL_GPL(kvm_arch_start_assignment);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13496| <<kvm_arch_irq_bypass_add_producer>> kvm_arch_end_assignment(irqfd->kvm);
+ *   - arch/x86/kvm/x86.c|13535| <<kvm_arch_irq_bypass_del_producer>> kvm_arch_end_assignment(irqfd->kvm);
+ *   - virt/kvm/vfio.c|209| <<kvm_vfio_file_del>> kvm_arch_end_assignment(dev->kvm);
+ *   - virt/kvm/vfio.c|345| <<kvm_vfio_release>> kvm_arch_end_assignment(dev->kvm);
+ */
 void kvm_arch_end_assignment(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->assigned_device_count:
+	 *   - arch/x86/kvm/x86.c|13416| <<kvm_arch_start_assignment>> if (atomic_inc_return(&kvm->arch.assigned_device_count) == 1)
+	 *   - arch/x86/kvm/x86.c|13430| <<kvm_arch_end_assignment>> atomic_dec(&kvm->arch.assigned_device_count);
+	 *   - arch/x86/kvm/x86.c|13444| <<kvm_arch_has_assigned_device>> return raw_atomic_read(&kvm->arch.assigned_device_count);
+	 */
 	atomic_dec(&kvm->arch.assigned_device_count);
 }
 EXPORT_SYMBOL_GPL(kvm_arch_end_assignment);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|751| <<avic_set_pi_irte_mode>> if (!kvm_arch_has_assigned_device(vcpu->kvm))
+ *   - arch/x86/kvm/svm/avic.c|901| <<avic_pi_update_irte>> if (!kvm_arch_has_assigned_device(kvm) || !irq_remapping_cap(IRQ_POSTING_CAP))
+ *   - arch/x86/kvm/svm/avic.c|1010| <<avic_update_iommu_vcpu_affinity>> if (!kvm_arch_has_assigned_device(vcpu->kvm))
+ *   - arch/x86/kvm/vmx/posted_intr.c|177| <<vmx_can_use_vtd_pi>> return irqchip_in_kernel(kvm) && enable_apicv && kvm_arch_has_assigned_device(kvm) && irq_remapping_cap(IRQ_POSTING_CAP);
+ *   - arch/x86/kvm/vmx/vmx.c|7270| <<vmx_vcpu_enter_exit>> else if (static_branch_unlikely(&mmio_stale_data_clear) && kvm_arch_has_assigned_device(vcpu->kvm))
+ */
 bool noinstr kvm_arch_has_assigned_device(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->assigned_device_count:
+	 *   - arch/x86/kvm/x86.c|13416| <<kvm_arch_start_assignment>> if (atomic_inc_return(&kvm->arch.assigned_device_count) == 1)
+	 *   - arch/x86/kvm/x86.c|13430| <<kvm_arch_end_assignment>> atomic_dec(&kvm->arch.assigned_device_count);
+	 *   - arch/x86/kvm/x86.c|13444| <<kvm_arch_has_assigned_device>> return raw_atomic_read(&kvm->arch.assigned_device_count);
+	 */
 	return raw_atomic_read(&kvm->arch.assigned_device_count);
 }
 EXPORT_SYMBOL_GPL(kvm_arch_has_assigned_device);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13452| <<kvm_arch_register_noncoherent_dma>> kvm_noncoherent_dma_assignment_start_or_stop(kvm);
+ *   - arch/x86/kvm/x86.c|13459| <<kvm_arch_unregister_noncoherent_dma>> kvm_noncoherent_dma_assignment_start_or_stop(kvm);
+ */
 static void kvm_noncoherent_dma_assignment_start_or_stop(struct kvm *kvm)
 {
 	/*
@@ -13438,6 +13505,10 @@ static void kvm_noncoherent_dma_assignment_start_or_stop(struct kvm *kvm)
 		kvm_zap_gfn_range(kvm, gpa_to_gfn(0), gpa_to_gfn(~0ULL));
 }
 
+/*
+ * called by:
+ *   - virt/kvm/vfio.c|137| <<kvm_vfio_update_coherency>> kvm_arch_register_noncoherent_dma(dev->kvm);
+ */
 void kvm_arch_register_noncoherent_dma(struct kvm *kvm)
 {
 	if (atomic_inc_return(&kvm->arch.noncoherent_dma_count) == 1)
@@ -13445,6 +13516,10 @@ void kvm_arch_register_noncoherent_dma(struct kvm *kvm)
 }
 EXPORT_SYMBOL_GPL(kvm_arch_register_noncoherent_dma);
 
+/*
+ * called by:
+ *   - virt/kvm/vfio.c|139| <<kvm_vfio_update_coherency>> kvm_arch_unregister_noncoherent_dma(dev->kvm);
+ */
 void kvm_arch_unregister_noncoherent_dma(struct kvm *kvm)
 {
 	if (!atomic_dec_return(&kvm->arch.noncoherent_dma_count))
@@ -13452,17 +13527,31 @@ void kvm_arch_unregister_noncoherent_dma(struct kvm *kvm)
 }
 EXPORT_SYMBOL_GPL(kvm_arch_unregister_noncoherent_dma);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|252| <<kvm_mmu_honors_guest_mtrrs>> return __kvm_mmu_honors_guest_mtrrs(kvm_arch_has_noncoherent_dma(kvm));
+ *   - arch/x86/kvm/vmx/vmx.c|7647| <<vmx_get_mt_mask>> if (!kvm_arch_has_noncoherent_dma(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|5008| <<need_emulate_wbinvd>> return kvm_arch_has_noncoherent_dma(vcpu->kvm);
+ */
 bool kvm_arch_has_noncoherent_dma(struct kvm *kvm)
 {
 	return atomic_read(&kvm->arch.noncoherent_dma_count);
 }
 EXPORT_SYMBOL_GPL(kvm_arch_has_noncoherent_dma);
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|432| <<kvm_irqfd_assign>> if (kvm_arch_has_irq_bypass()) {
+ */
 bool kvm_arch_has_irq_bypass(void)
 {
 	return enable_apicv && irq_remapping_cap(IRQ_POSTING_CAP);
 }
 
+/*
+ * 在以下使用kvm_arch_irq_bypass_add_producer():
+ *   - virt/kvm/eventfd.c|434| <<kvm_irqfd_assign>> irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+ */
 int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
 				      struct irq_bypass_producer *prod)
 {
@@ -13472,6 +13561,15 @@ int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
 
 	irqfd->producer = prod;
 	kvm_arch_start_assignment(irqfd->kvm);
+	/*
+	 * 在以下使用vmx_pi_update_irte():
+	 *   - arch/x86/kvm/x86.c|13475| <<kvm_arch_irq_bypass_add_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 1);
+	 *   - arch/x86/kvm/x86.c|13500| <<kvm_arch_irq_bypass_del_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);
+	 *   - arch/x86/kvm/x86.c|13511| <<kvm_arch_update_irqfd_routing>> return static_call(kvm_x86_pi_update_irte)(kvm, host_irq, guest_irq, set);
+	 *
+	 * struct kvm_x86_ops vmx_x86_ops.pi_update_irte = vmx_pi_update_irte
+	 * struct kvm_x86_ops svm_x86_ops.pi_update_irte = avic_pi_update_irte
+	 */
 	ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm,
 					 prod->irq, irqfd->gsi, 1);
 
@@ -13481,6 +13579,10 @@ int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
 	return ret;
 }
 
+/*
+ * 在以下使用kvm_arch_irq_bypass_del_producer():
+ *   - virt/kvm/eventfd.c|435| <<kvm_irqfd_assign>> irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
+ */
 void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
 				      struct irq_bypass_producer *prod)
 {
@@ -13497,6 +13599,15 @@ void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
 	 * when the irq is masked/disabled or the consumer side (KVM
 	 * int this case doesn't want to receive the interrupts.
 	*/
+	/*
+	 * 在以下使用vmx_pi_update_irte():
+	 *   - arch/x86/kvm/x86.c|13475| <<kvm_arch_irq_bypass_add_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 1);
+	 *   - arch/x86/kvm/x86.c|13500| <<kvm_arch_irq_bypass_del_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);
+	 *   - arch/x86/kvm/x86.c|13511| <<kvm_arch_update_irqfd_routing>> return static_call(kvm_x86_pi_update_irte)(kvm, host_irq, guest_irq, set);
+	 *
+	 * struct kvm_x86_ops vmx_x86_ops.pi_update_irte = vmx_pi_update_irte
+	 * struct kvm_x86_ops svm_x86_ops.pi_update_irte = avic_pi_update_irte
+	 */
 	ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);
 	if (ret)
 		printk(KERN_INFO "irq bypass consumer (token %p) unregistration"
@@ -13505,12 +13616,29 @@ void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
 	kvm_arch_end_assignment(irqfd->kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|634| <<kvm_irq_routing_update>> int ret = kvm_arch_update_irqfd_routing(irqfd->kvm, irqfd->producer->irq, irqfd->gsi, 1);
+ */
 int kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,
 				   uint32_t guest_irq, bool set)
 {
+	/*
+	 * 在以下使用vmx_pi_update_irte():
+	 *   - arch/x86/kvm/x86.c|13475| <<kvm_arch_irq_bypass_add_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 1);
+	 *   - arch/x86/kvm/x86.c|13500| <<kvm_arch_irq_bypass_del_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);
+	 *   - arch/x86/kvm/x86.c|13511| <<kvm_arch_update_irqfd_routing>> return static_call(kvm_x86_pi_update_irte)(kvm, host_irq, guest_irq, set);
+	 *
+	 * struct kvm_x86_ops vmx_x86_ops.pi_update_irte = vmx_pi_update_irte
+	 * struct kvm_x86_ops svm_x86_ops.pi_update_irte = avic_pi_update_irte
+	 */
 	return static_call(kvm_x86_pi_update_irte)(kvm, host_irq, guest_irq, set);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|633| <<kvm_irq_routing_update>> if (irqfd->producer && kvm_arch_irqfd_route_changed(&old, &irqfd->irq_entry)) {
+ */
 bool kvm_arch_irqfd_route_changed(struct kvm_kernel_irq_routing_entry *old,
 				  struct kvm_kernel_irq_routing_entry *new)
 {
diff --git a/arch/x86/kvm/xen.c b/arch/x86/kvm/xen.c
index f65b35a05..4ac5c021d 100644
--- a/arch/x86/kvm/xen.c
+++ b/arch/x86/kvm/xen.c
@@ -35,6 +35,12 @@ static bool kvm_xen_hcall_evtchn_send(struct kvm_vcpu *vcpu, u64 param, u64 *r);
 
 DEFINE_STATIC_KEY_DEFERRED_FALSE(kvm_xen_enabled, HZ);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|718| <<kvm_xen_hvm_set_attr>> kvm_xen_shared_info_init(kvm) : 0;
+ *   - arch/x86/kvm/xen.c|758| <<kvm_xen_hvm_set_attr>> r = kvm_xen_shared_info_init(kvm);
+ *   - arch/x86/kvm/xen.c|1248| <<kvm_xen_write_hypercall_page>> kvm_xen_shared_info_init(kvm))
+ */
 static int kvm_xen_shared_info_init(struct kvm *kvm)
 {
 	struct gfn_to_pfn_cache *gpc = &kvm->arch.xen.shinfo_cache;
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index e21a3b412..5e6fb2ef9 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -100,8 +100,24 @@ static inline u16 mlx5e_calc_min_inline(enum mlx5_inline_modes mode,
 	"performance sensitive, splitting the copy is "		\
 	"undesirable."
 
+/*
+ * called by:
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|454| <<mlx5e_sq_xmit_wqe>> mlx5e_insert_vlan(start, skb, ETH_HLEN + sizeof(*h6));
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|471| <<mlx5e_sq_xmit_wqe>> mlx5e_insert_vlan(start, skb, ihs);
+ */
 static inline void mlx5e_insert_vlan(void *start, struct sk_buff *skb, u16 ihs)
 {
+	/*
+	 * 48 struct vlan_ethhdr {
+	 * 49         struct_group(addrs,
+	 * 50                 unsigned char   h_dest[ETH_ALEN];
+	 * 51                 unsigned char   h_source[ETH_ALEN];
+	 * 52         );
+	 * 53         __be16          h_vlan_proto;
+	 * 54         __be16          h_vlan_TCI;
+	 * 55         __be16          h_vlan_encapsulated_proto;
+	 * 56 };
+	 */
 	struct vlan_ethhdr *vhdr = (struct vlan_ethhdr *)start;
 	int cpy1_sz = 2 * ETH_ALEN;
 	int cpy2_sz = ihs - cpy1_sz;
@@ -229,6 +245,14 @@ struct mlx5e_tx_attr {
 	__be16 mss;
 	u16 insz;
 	u8 opcode;
+	/*
+	 * 在以下使用mlx5e_tx_attr->hopbyhop:
+	 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|278| <<mlx5e_sq_xmit_prepare>> .hopbyhop = hopbyhop,
+	 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|449| <<mlx5e_sq_xmit_wqe>> if (unlikely(attr->hopbyhop)) {
+	 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|490| <<mlx5e_sq_xmit_wqe>> num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + attr->ihs + attr->hopbyhop,
+	 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|1011| <<mlx5i_sq_xmit>> if (unlikely(attr.hopbyhop)) {
+	 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|1038| <<mlx5i_sq_xmit>> num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + attr.ihs + attr.hopbyhop,
+	 */
 	u8 hopbyhop;
 };
 
@@ -259,6 +283,11 @@ mlx5e_tx_wqe_inline_mode(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 	return mode;
 }
 
+/*
+ * called by:
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|756| <<mlx5e_xmit>> mlx5e_sq_xmit_prepare(sq, skb, &accel, &attr);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|1040| <<mlx5i_sq_xmit>> mlx5e_sq_xmit_prepare(sq, skb, NULL, &attr);
+ */
 static void mlx5e_sq_xmit_prepare(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 				  struct mlx5e_accel_tx_state *accel,
 				  struct mlx5e_tx_attr *attr)
@@ -269,6 +298,14 @@ static void mlx5e_sq_xmit_prepare(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 		int hopbyhop;
 		u16 ihs = mlx5e_tx_get_gso_ihs(sq, skb, &hopbyhop);
 
+		/*
+		 * 在以下使用mlx5e_tx_attr->hopbyhop:
+		 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|278| <<mlx5e_sq_xmit_prepare>> .hopbyhop = hopbyhop,
+		 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|449| <<mlx5e_sq_xmit_wqe>> if (unlikely(attr->hopbyhop)) {
+		 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|490| <<mlx5e_sq_xmit_wqe>> num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + attr->ihs + attr->hopbyhop,
+		 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|1011| <<mlx5i_sq_xmit>> if (unlikely(attr.hopbyhop)) {
+		 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|1038| <<mlx5i_sq_xmit>> num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + attr.ihs + attr.hopbyhop,
+		 */
 		*attr = (struct mlx5e_tx_attr) {
 			.opcode    = MLX5_OPCODE_LSO,
 			.mss       = cpu_to_be16(skb_shinfo(skb)->gso_size),
@@ -419,6 +456,10 @@ mlx5e_txwqe_complete(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 		mlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg);
 }
 
+/*
+ * called by:
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|727| <<mlx5e_xmit>> mlx5e_sq_xmit_wqe(sq, skb, &attr, &wqe_attr, wqe, pi, netdev_xmit_more());
+ */
 static void
 mlx5e_sq_xmit_wqe(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 		  const struct mlx5e_tx_attr *attr, const struct mlx5e_tx_wqe_attr *wqe_attr,
@@ -446,6 +487,14 @@ mlx5e_sq_xmit_wqe(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 	if (ihs) {
 		u8 *start = eseg->inline_hdr.start;
 
+		/*
+		 * 在以下使用mlx5e_tx_attr->hopbyhop:
+		 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|278| <<mlx5e_sq_xmit_prepare>> .hopbyhop = hopbyhop,
+		 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|449| <<mlx5e_sq_xmit_wqe>> if (unlikely(attr->hopbyhop)) {
+		 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|490| <<mlx5e_sq_xmit_wqe>> num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + attr->ihs + attr->hopbyhop,
+		 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|1011| <<mlx5i_sq_xmit>> if (unlikely(attr.hopbyhop)) {
+		 *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|1038| <<mlx5i_sq_xmit>> num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + attr.ihs + attr.hopbyhop,
+		 */
 		if (unlikely(attr->hopbyhop)) {
 			/* remove the HBH header.
 			 * Layout: [Ethernet header][IPv6 header][HBH][TCP header]
@@ -673,6 +722,9 @@ netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	struct mlx5e_accel_tx_state accel = {};
+	/*
+	 * wqe是Work Queue Entries
+	 */
 	struct mlx5e_tx_wqe_attr wqe_attr;
 	struct mlx5e_tx_attr attr;
 	struct mlx5e_tx_wqe *wqe;
@@ -686,6 +738,10 @@ netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev)
 	 * mlx5e_xmit is running on queue number qid. smb_wmb is paired with
 	 * HARD_TX_LOCK around ndo_start_xmit, which serves as an ACQUIRE.
 	 */
+	/*
+	 * struct mlx5e_priv *priv:
+	 * -> struct mlx5e_txqsq **txq2sq;
+	 */
 	sq = priv->txq2sq[skb_get_queue_mapping(skb)];
 	if (unlikely(!sq)) {
 		/* Two cases when sq can be NULL:
diff --git a/drivers/net/tap.c b/drivers/net/tap.c
index 9f0495e8d..af8ad9c62 100644
--- a/drivers/net/tap.c
+++ b/drivers/net/tap.c
@@ -26,6 +26,12 @@
 #include <linux/virtio_net.h>
 #include <linux/skb_array.h>
 
+/*
+ * 在以下使用TAP_IFFEATURES:
+ *   - drivers/net/tap.c|1078| <<tap_ioctl(TUNSETIFF)>> if ((u & ~TAP_IFFEATURES) != (IFF_NO_PI | IFF_TAP))
+ *   - drivers/net/tap.c|1081| <<tap_ioctl(TUNSETIFF)>> q->flags = (q->flags & ~TAP_IFFEATURES) | u;
+ *   - drivers/net/tap.c|1111| <<tap_ioctl(TUNGETFEATURES)>> if (put_user(IFF_TAP | IFF_NO_PI | TAP_IFFEATURES, up))
+ */
 #define TAP_IFFEATURES (IFF_VNET_HDR | IFF_MULTI_QUEUE)
 
 #define TAP_VNET_LE 0x80000000
@@ -605,6 +611,10 @@ static __poll_t tap_poll(struct file *file, poll_table *wait)
 	return mask;
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|707| <<tap_get_user>> skb = tap_alloc_skb(&q->sk, TAP_RESERVE, copylen, linear, noblock, &err);
+ */
 static inline struct sk_buff *tap_alloc_skb(struct sock *sk, size_t prepad,
 					    size_t len, size_t linear,
 						int noblock, int *err)
@@ -633,6 +643,11 @@ static inline struct sk_buff *tap_alloc_skb(struct sock *sk, size_t prepad,
 /* Neighbour code has some assumptions on HH_DATA_MOD alignment */
 #define TAP_RESERVE HH_DATA_OFF(ETH_HLEN)
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|786| <<tap_write_iter>> return tap_get_user(q, NULL, from, noblock);
+ *   - drivers/net/tap.c|1248| <<tap_sendmsg>> return tap_get_user(q, ctl ? ctl->ptr : NULL, &m->msg_iter, m->msg_flags & MSG_DONTWAIT);
+ */
 /* Get packet from user space buffer */
 static ssize_t tap_get_user(struct tap_queue *q, void *msg_control,
 			    struct iov_iter *from, int noblock)
@@ -652,6 +667,26 @@ static ssize_t tap_get_user(struct tap_queue *q, void *msg_control,
 	enum skb_drop_reason drop_reason;
 
 	if (q->flags & IFF_VNET_HDR) {
+		/*
+		 * 188 struct virtio_net_hdr {
+		 * 189         // See VIRTIO_NET_HDR_F_*
+		 * 190         __u8 flags;
+		 * 191         // See VIRTIO_NET_HDR_GSO_*
+		 * 192         __u8 gso_type;
+		 * 193         __virtio16 hdr_len;             // Ethernet + IP + tcp/udp hdrs
+		 * 194         __virtio16 gso_size;            // Bytes to append to hdr_len per frame
+		 * 195         __virtio16 csum_start;  // Position to start checksumming from
+		 * 196         __virtio16 csum_offset; // Offset after that to place checksum
+		 * 197 };
+		 *
+		 * 在以下使用tap_queue->vnet_hdr_sz:
+		 *   - drivers/net/tap.c|542| <<tap_open>> q->vnet_hdr_sz = sizeof(struct virtio_net_hdr);
+		 *   - drivers/net/tap.c|664| <<tap_get_user>> vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
+		 *   - drivers/net/tap.c|812| <<tap_put_user>> vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
+		 *   - drivers/net/tap.c|1085| <<tap_ioctl(TUNGETVNETHDRSZ)>> s = q->vnet_hdr_sz;
+		 *   - drivers/net/tap.c|1096| <<tap_ioctl>> q->vnet_hdr_sz = s;
+		 *   - drivers/net/tap.c|1190| <<tap_get_user_xdp>> vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
+		 */
 		vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
 
 		err = -EINVAL;
@@ -660,6 +695,9 @@ static ssize_t tap_get_user(struct tap_queue *q, void *msg_control,
 		len -= vnet_hdr_len;
 
 		err = -EFAULT;
+		/*
+		 * 参数中的: struct iov_iter *from;
+		 */
 		if (!copy_from_iter_full(&vnet_hdr, sizeof(vnet_hdr), from))
 			goto err;
 		iov_iter_advance(from, vnet_hdr_len - sizeof(vnet_hdr));
@@ -800,10 +838,39 @@ static ssize_t tap_put_user(struct tap_queue *q,
 		int vlan_hlen = skb_vlan_tag_present(skb) ? VLAN_HLEN : 0;
 		struct virtio_net_hdr vnet_hdr;
 
+		/*
+		 * 188 struct virtio_net_hdr {
+		 * 189         // See VIRTIO_NET_HDR_F_*
+		 * 190         __u8 flags;
+		 * 191         // See VIRTIO_NET_HDR_GSO_*
+		 * 192         __u8 gso_type;
+		 * 193         __virtio16 hdr_len;             // Ethernet + IP + tcp/udp hdrs
+		 * 194         __virtio16 gso_size;            // Bytes to append to hdr_len per frame
+		 * 195         __virtio16 csum_start;  // Position to start checksumming from
+		 * 196         __virtio16 csum_offset; // Offset after that to place checksum
+		 * 197 };
+		 *
+		 * 在以下使用tap_queue->vnet_hdr_sz:
+		 *   - drivers/net/tap.c|542| <<tap_open>> q->vnet_hdr_sz = sizeof(struct virtio_net_hdr);
+		 *   - drivers/net/tap.c|664| <<tap_get_user>> vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
+		 *   - drivers/net/tap.c|812| <<tap_put_user>> vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
+		 *   - drivers/net/tap.c|1085| <<tap_ioctl(TUNGETVNETHDRSZ)>> s = q->vnet_hdr_sz;
+		 *   - drivers/net/tap.c|1096| <<tap_ioctl>> q->vnet_hdr_sz = s;
+		 *   - drivers/net/tap.c|1190| <<tap_get_user_xdp>> vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
+		 */
 		vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
 		if (iov_iter_count(iter) < vnet_hdr_len)
 			return -EINVAL;
 
+		/*
+		 * called by:
+		 *   - arch/um/drivers/vector_transports.c|119| <<raw_form_header>> virtio_net_hdr_from_skb(skb, vheader, virtio_legacy_is_little_endian(), false, 0);
+		 *   - drivers/net/tap.c|865| <<tap_put_user>> if (virtio_net_hdr_from_skb(skb, &vnet_hdr, tap_is_little_endian(q), true, vlan_hlen))
+		 *   - drivers/net/tun.c|2124| <<tun_put_user>> if (virtio_net_hdr_from_skb(skb, &gso, tun_is_little_endian(tun), true, vlan_hlen)) {
+		 *   - drivers/net/virtio_net.c|2384| <<xmit_skb>> if (virtio_net_hdr_from_skb(skb, &hdr->hdr, virtio_is_little_endian(vi->vdev), false, 0))
+		 *   - net/packet/af_packet.c|2103| <<packet_rcv_vnet>> if (virtio_net_hdr_from_skb(skb, (struct virtio_net_hdr *)&vnet_hdr, vio_le(), true, 0))
+		 *   - net/packet/af_packet.c|2369| <<tpacket_rcv>> virtio_net_hdr_from_skb(skb, h.raw + macoff - sizeof(struct virtio_net_hdr), vio_le(), true, 0)) {
+		 */
 		if (virtio_net_hdr_from_skb(skb, &vnet_hdr,
 					    tap_is_little_endian(q), true,
 					    vlan_hlen))
@@ -1167,6 +1234,20 @@ static const struct file_operations tap_fops = {
 	.compat_ioctl	= compat_ptr_ioctl,
 };
 
+/*
+ * 80 struct xdp_buff {
+ * 81         void *data;
+ * 82         void *data_end;
+ * 83         void *data_meta;
+ * 84         void *data_hard_start;
+ * 85         struct xdp_rxq_info *rxq;
+ * 86         struct xdp_txq_info *txq;
+ * 87         u32 frame_sz; // frame size to deduce data_hard_end/reserved tailroom
+ * 88         u32 flags; // supported values defined in xdp_buff_flags
+ *
+ * called by:
+ *   - drivers/net/tap.c|1301| <<tap_sendmsg>> tap_get_user_xdp(q, xdp);
+ */
 static int tap_get_user_xdp(struct tap_queue *q, struct xdp_buff *xdp)
 {
 	struct tun_xdp_hdr *hdr = xdp->data_hard_start;
@@ -1180,17 +1261,34 @@ static int tap_get_user_xdp(struct tap_queue *q, struct xdp_buff *xdp)
 	if (q->flags & IFF_VNET_HDR)
 		vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
 
+	/*
+	 * 设置head和data
+	 */
 	skb = build_skb(xdp->data_hard_start, buflen);
 	if (!skb) {
 		err = -ENOMEM;
 		goto err;
 	}
 
+	/*
+	 * data往前, 指向真正数据的地方
+	 */
 	skb_reserve(skb, xdp->data - xdp->data_hard_start);
 	skb_put(skb, xdp->data_end - xdp->data);
 
 	skb_set_network_header(skb, ETH_HLEN);
 	skb_reset_mac_header(skb);
+	/*
+	 * 172 #if __UAPI_DEF_ETHHDR
+	 * 173 struct ethhdr { 
+	 * 174         unsigned char   h_dest[ETH_ALEN];       // destination eth addr
+	 * 175         unsigned char   h_source[ETH_ALEN];     // source ether addr
+	 * 176         __be16          h_proto;                // packet type ID field
+	 * 177 } __attribute__((packed));
+	 * 178 #endif
+	 *
+	 * return (struct ethhdr *)skb_mac_header(skb);
+	 */
 	skb->protocol = eth_hdr(skb)->h_proto;
 
 	if (vnet_hdr_len) {
@@ -1238,7 +1336,26 @@ static int tap_sendmsg(struct socket *sock, struct msghdr *m,
 
 	if (m->msg_controllen == sizeof(struct tun_msg_ctl) &&
 	    ctl && ctl->type == TUN_MSG_PTR) {
+		/*
+		 * struct tun_msg_ctl {
+		 *     unsigned short type;
+		 *     unsigned short num;
+		 *     void *ptr;
+		 * };
+		 */
 		for (i = 0; i < ctl->num; i++) {
+			/*
+			 * 80 struct xdp_buff {
+			 * 81         void *data;
+			 * 82         void *data_end;
+			 * 83         void *data_meta;
+			 * 84         void *data_hard_start;
+			 * 85         struct xdp_rxq_info *rxq;
+			 * 86         struct xdp_txq_info *txq;
+			 * 87         u32 frame_sz; // frame size to deduce data_hard_end/reserved tailroom
+			 * 88         u32 flags; // supported values defined in xdp_buff_flags
+			 * 89 };
+			 */
 			xdp = &((struct xdp_buff *)ctl->ptr)[i];
 			tap_get_user_xdp(q, xdp);
 		}
diff --git a/drivers/net/tun.c b/drivers/net/tun.c
index 92da8c03d..1b137cd16 100644
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@ -2121,6 +2121,15 @@ static ssize_t tun_put_user(struct tun_struct *tun,
 		if (iov_iter_count(iter) < vnet_hdr_sz)
 			return -EINVAL;
 
+		/*
+		 * called by:
+		 *   - arch/um/drivers/vector_transports.c|119| <<raw_form_header>> virtio_net_hdr_from_skb(skb, vheader, virtio_legacy_is_little_endian(), false, 0);
+		 *   - drivers/net/tap.c|865| <<tap_put_user>> if (virtio_net_hdr_from_skb(skb, &vnet_hdr, tap_is_little_endian(q), true, vlan_hlen))
+		 *   - drivers/net/tun.c|2124| <<tun_put_user>> if (virtio_net_hdr_from_skb(skb, &gso, tun_is_little_endian(tun), true, vlan_hlen)) {
+		 *   - drivers/net/virtio_net.c|2384| <<xmit_skb>> if (virtio_net_hdr_from_skb(skb, &hdr->hdr, virtio_is_little_endian(vi->vdev), false, 0))
+		 *   - net/packet/af_packet.c|2103| <<packet_rcv_vnet>> if (virtio_net_hdr_from_skb(skb, (struct virtio_net_hdr *)&vnet_hdr, vio_le(), true, 0))
+		 *   - net/packet/af_packet.c|2369| <<tpacket_rcv>> virtio_net_hdr_from_skb(skb, h.raw + macoff - sizeof(struct virtio_net_hdr), vio_le(), true, 0)) {
+		 */
 		if (virtio_net_hdr_from_skb(skb, &gso,
 					    tun_is_little_endian(tun), true,
 					    vlan_hlen)) {
@@ -2434,6 +2443,18 @@ static void tun_put_page(struct tun_page *tpage)
 		__page_frag_cache_drain(tpage->page, tpage->count);
 }
 
+/*
+ * 80 struct xdp_buff {
+ * 81         void *data;
+ * 82         void *data_end;
+ * 83         void *data_meta;
+ * 84         void *data_hard_start;
+ * 85         struct xdp_rxq_info *rxq;
+ * 86         struct xdp_txq_info *txq;
+ * 87         u32 frame_sz; // frame size to deduce data_hard_end/reserved tailroom
+ * 88         u32 flags; // supported values defined in xdp_buff_flags
+ * 89 }
+ */
 static int tun_xdp_one(struct tun_struct *tun,
 		       struct tun_file *tfile,
 		       struct xdp_buff *xdp, int *flush,
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 115c3c541..afbc39582 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -291,9 +291,50 @@ struct virtnet_info {
 	/* Has control virtqueue */
 	bool has_cvq;
 
+	/*
+	 * 4845         if (virtio_has_feature(vdev, VIRTIO_F_ANY_LAYOUT) ||
+	 * 4846             virtio_has_feature(vdev, VIRTIO_F_VERSION_1))
+	 * 4847                 vi->any_header_sg = true;
+	 *
+	 * 在以下使用virtnet_info->any_header_sg:
+	 *   - drivers/net/virtio_net.c|2374| <<xmit_skb>> can_push = vi->any_header_sg &&
+	 *   - drivers/net/virtio_net.c|4027| <<virtnet_xdp_set>> if (vi->mergeable_rx_bufs && !vi->any_header_sg) {
+	 *   - drivers/net/virtio_net.c|4796| <<virtnet_probe>> vi->any_header_sg = true;
+	 *   - drivers/net/virtio_net.c|4822| <<virtnet_probe>> if (vi->any_header_sg)
+	 */
 	/* Host can handle any s/g split between our header and packet data */
 	bool any_header_sg;
 
+	/*
+	 * 在以下使用virtnet_info->hdr_len:
+	 *   - drivers/net/virtio_net.c|592| <<page_to_skb>> hdr_len = vi->hdr_len;
+	 *   - drivers/net/virtio_net.c|909| <<__virtnet_xdp_xmit_one>> if (unlikely(xdpf->headroom < vi->hdr_len))
+	 *   - drivers/net/virtio_net.c|924| <<__virtnet_xdp_xmit_one>> xdpf->headroom -= vi->hdr_len;
+	 *   - drivers/net/virtio_net.c|925| <<__virtnet_xdp_xmit_one>> xdpf->data -= vi->hdr_len;
+	 *   - drivers/net/virtio_net.c|928| <<__virtnet_xdp_xmit_one>> memset(hdr, 0, vi->hdr_len);
+	 *   - drivers/net/virtio_net.c|929| <<__virtnet_xdp_xmit_one>> xdpf->len += vi->hdr_len;
+	 *   - drivers/net/virtio_net.c|1198| <<receive_small_build_skb>> headroom = vi->hdr_len + header_offset;
+	 *   - drivers/net/virtio_net.c|1207| <<receive_small_build_skb>> memcpy(skb_vnet_common_hdr(skb), buf, vi->hdr_len);
+	 *   - drivers/net/virtio_net.c|1223| <<receive_small_xdp>> unsigned int headroom = vi->hdr_len + header_offset;
+	 *   - drivers/net/virtio_net.c|1241| <<receive_small_xdp>> unsigned int tlen = len + vi->hdr_len;
+	 *   - drivers/net/virtio_net.c|1246| <<receive_small_xdp>> headroom = vi->hdr_len + header_offset;
+	 *   - drivers/net/virtio_net.c|1261| <<receive_small_xdp>> xdp_prepare_buff(&xdp, buf + VIRTNET_RX_PAD + vi->hdr_len,
+	 *   - drivers/net/virtio_net.c|1311| <<receive_small>> len -= vi->hdr_len;
+	 *   - drivers/net/virtio_net.c|1357| <<receive_big>> u64_stats_add(&stats->bytes, len - vi->hdr_len);
+	 *   - drivers/net/virtio_net.c|1464| <<virtnet_build_xdp_buff_mrg>> VIRTIO_XDP_HEADROOM + vi->hdr_len, len - vi->hdr_len, true);
+	 *   - drivers/net/virtio_net.c|1684| <<receive_mergeable>> u64_stats_add(&stats->bytes, len - vi->hdr_len);
+	 *   - drivers/net/virtio_net.c|1820| <<receive_buf>> if (unlikely(len < vi->hdr_len + ETH_HLEN)) {
+	 *   - drivers/net/virtio_net.c|1877| <<add_recvbuf_small>> int len = vi->hdr_len + VIRTNET_RX_PAD + GOOD_PACKET_LEN + xdp_headroom;
+	 *   - drivers/net/virtio_net.c|1888| <<add_recvbuf_small>> vi->hdr_len + GOOD_PACKET_LEN);
+	 *   - drivers/net/virtio_net.c|1933| <<add_recvbuf_big>> sg_set_buf(&rq->sg[0], p, vi->hdr_len);
+	 *   - drivers/net/virtio_net.c|1954| <<get_mergeable_buf_len>> const size_t hdr_len = vi->hdr_len;
+	 *   - drivers/net/virtio_net.c|2369| <<xmit_skb>> unsigned hdr_len = vi->hdr_len;
+	 *   - drivers/net/virtio_net.c|4358| <<mergeable_min_buf_len>> const unsigned int hdr_len = vi->hdr_len;
+	 *   - drivers/net/virtio_net.c|4787| <<virtnet_probe>> vi->hdr_len = sizeof(struct virtio_net_hdr_v1_hash);
+	 *   - drivers/net/virtio_net.c|4790| <<virtnet_probe>> vi->hdr_len = sizeof(struct virtio_net_hdr_mrg_rxbuf);
+	 *   - drivers/net/virtio_net.c|4792| <<virtnet_probe>> vi->hdr_len = sizeof(struct virtio_net_hdr);
+	 *   - drivers/net/virtio_net.c|4823| <<virtnet_probe>> dev->needed_headroom = vi->hdr_len;
+	 */
 	/* Packet virtio header size */
 	u8 hdr_len;
 
@@ -854,6 +895,11 @@ static bool is_xdp_raw_buffer_queue(struct virtnet_info *vi, int q)
 		return false;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1018| <<virtnet_xdp_xmit>> check_sq_full_and_disable(vi, dev, sq);
+ *   - drivers/net/virtio_net.c|2446| <<start_xmit>> check_sq_full_and_disable(vi, dev, sq);
+ */
 static void check_sq_full_and_disable(struct virtnet_info *vi,
 				      struct net_device *dev,
 				      struct send_queue *sq)
@@ -874,6 +920,9 @@ static void check_sq_full_and_disable(struct virtnet_info *vi,
 	 * early means 16 slots are typically wasted.
 	 */
 	if (sq->vq->num_free < 2+MAX_SKB_FRAGS) {
+		/*
+		 * 关掉queue
+		 */
 		netif_stop_subqueue(dev, qnum);
 		if (use_napi) {
 			if (unlikely(!virtqueue_enable_cb_delayed(sq->vq)))
@@ -2358,11 +2407,37 @@ static int xmit_skb(struct send_queue *sq, struct sk_buff *skb)
 	const unsigned char *dest = ((struct ethhdr *)skb->data)->h_dest;
 	struct virtnet_info *vi = sq->vq->vdev->priv;
 	int num_sg;
+	/*
+	 * 一般是else if: 长度12
+	 *
+	 * 4819         if (vi->has_rss_hash_report)
+	 * 4820                 vi->hdr_len = sizeof(struct virtio_net_hdr_v1_hash);
+	 * 4821         else if (virtio_has_feature(vdev, VIRTIO_NET_F_MRG_RXBUF) ||
+	 * 4822                  virtio_has_feature(vdev, VIRTIO_F_VERSION_1))
+	 * 4823                 vi->hdr_len = sizeof(struct virtio_net_hdr_mrg_rxbuf);
+	 * 4824         else
+	 * 4825                 vi->hdr_len = sizeof(struct virtio_net_hdr);
+	 */
 	unsigned hdr_len = vi->hdr_len;
 	bool can_push;
 
 	pr_debug("%s: xmit %p %pM\n", vi->dev->name, skb, dest);
 
+	/*
+	 * 4845         if (virtio_has_feature(vdev, VIRTIO_F_ANY_LAYOUT) ||
+	 * 4846             virtio_has_feature(vdev, VIRTIO_F_VERSION_1))
+	 * 4847                 vi->any_header_sg = true;
+	 *
+	 * 在以下使用virtnet_info->any_header_sg:
+	 *   - drivers/net/virtio_net.c|2374| <<xmit_skb>> can_push = vi->any_header_sg &&
+	 *   - drivers/net/virtio_net.c|4027| <<virtnet_xdp_set>> if (vi->mergeable_rx_bufs && !vi->any_header_sg) {
+	 *   - drivers/net/virtio_net.c|4796| <<virtnet_probe>> vi->any_header_sg = true;
+	 *   - drivers/net/virtio_net.c|4822| <<virtnet_probe>> if (vi->any_header_sg)
+	 *
+	 * 一般vi->any_header_sg是true
+	 *
+	 * skb_headroom(skb)就是检查skb->data和skb->head之间的距离
+	 */
 	can_push = vi->any_header_sg &&
 		!((unsigned long)skb->data & (__alignof__(*hdr) - 1)) &&
 		!skb_header_cloned(skb) && skb_headroom(skb) >= hdr_len;
@@ -2373,6 +2448,15 @@ static int xmit_skb(struct send_queue *sq, struct sk_buff *skb)
 	else
 		hdr = &skb_vnet_common_hdr(skb)->mrg_hdr;
 
+	/*
+	 * called by:
+	 *   - arch/um/drivers/vector_transports.c|119| <<raw_form_header>> virtio_net_hdr_from_skb(skb, vheader, virtio_legacy_is_little_endian(), false, 0);
+	 *   - drivers/net/tap.c|865| <<tap_put_user>> if (virtio_net_hdr_from_skb(skb, &vnet_hdr, tap_is_little_endian(q), true, vlan_hlen))
+	 *   - drivers/net/tun.c|2124| <<tun_put_user>> if (virtio_net_hdr_from_skb(skb, &gso, tun_is_little_endian(tun), true, vlan_hlen)) {
+	 *   - drivers/net/virtio_net.c|2384| <<xmit_skb>> if (virtio_net_hdr_from_skb(skb, &hdr->hdr, virtio_is_little_endian(vi->vdev), false, 0))
+	 *   - net/packet/af_packet.c|2103| <<packet_rcv_vnet>> if (virtio_net_hdr_from_skb(skb, (struct virtio_net_hdr *)&vnet_hdr, vio_le(), true, 0))
+	 *   - net/packet/af_packet.c|2369| <<tpacket_rcv>> virtio_net_hdr_from_skb(skb, h.raw + macoff - sizeof(struct virtio_net_hdr), vio_le(), true, 0)) {
+	 */
 	if (virtio_net_hdr_from_skb(skb, &hdr->hdr,
 				    virtio_is_little_endian(vi->vdev), false,
 				    0))
@@ -2381,8 +2465,15 @@ static int xmit_skb(struct send_queue *sq, struct sk_buff *skb)
 	if (vi->mergeable_rx_bufs)
 		hdr->num_buffers = 0;
 
+	/*
+	 * struct send_queue *sq:
+	 * -> struct scatterlist sg[MAX_SKB_FRAGS + 2];
+	 */
 	sg_init_table(sq->sg, skb_shinfo(skb)->nr_frags + (can_push ? 1 : 2));
 	if (can_push) {
+		/*
+		 * 把virtio_net_hdr_mrg_rxbuf给push到data之前
+		 */
 		__skb_push(skb, hdr_len);
 		num_sg = skb_to_sgvec(skb, sq->sg, 0, skb->len);
 		if (unlikely(num_sg < 0))
@@ -2391,11 +2482,19 @@ static int xmit_skb(struct send_queue *sq, struct sk_buff *skb)
 		__skb_pull(skb, hdr_len);
 	} else {
 		sg_set_buf(sq->sg, hdr, hdr_len);
+		/*
+		 * skb->data是一个sg[i]
+		 * 剩下的frags是若干个sg[i]
+		 */
 		num_sg = skb_to_sgvec(skb, sq->sg + 1, 0, skb->len);
 		if (unlikely(num_sg < 0))
 			return num_sg;
 		num_sg++;
 	}
+	/*
+	 * struct send_queue *sq:
+	 * -> struct scatterlist sg[MAX_SKB_FRAGS + 2];
+	 */
 	return virtqueue_add_outbuf(sq->vq, sq->sg, num_sg, skb, GFP_ATOMIC);
 }
 
@@ -4767,6 +4866,9 @@ static int virtnet_probe(struct virtio_device *vdev)
 		dev->hw_features |= NETIF_F_RXHASH;
 	}
 
+	/*
+	 * 一般是else if: 长度12
+	 */
 	if (vi->has_rss_hash_report)
 		vi->hdr_len = sizeof(struct virtio_net_hdr_v1_hash);
 	else if (virtio_has_feature(vdev, VIRTIO_NET_F_MRG_RXBUF) ||
diff --git a/drivers/pci/hotplug/pciehp_ctrl.c b/drivers/pci/hotplug/pciehp_ctrl.c
index dcdbfcf40..36cae35ea 100644
--- a/drivers/pci/hotplug/pciehp_ctrl.c
+++ b/drivers/pci/hotplug/pciehp_ctrl.c
@@ -158,6 +158,10 @@ void pciehp_queue_pushbutton_work(struct work_struct *work)
 	mutex_unlock(&ctrl->state_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_hpc.c|721| <<pciehp_ist>> pciehp_handle_button_press(ctrl);
+ */
 void pciehp_handle_button_press(struct controller *ctrl)
 {
 	mutex_lock(&ctrl->state_lock);
diff --git a/drivers/pci/hotplug/pciehp_hpc.c b/drivers/pci/hotplug/pciehp_hpc.c
index b1d0a1b39..4cefb6bf7 100644
--- a/drivers/pci/hotplug/pciehp_hpc.c
+++ b/drivers/pci/hotplug/pciehp_hpc.c
@@ -691,6 +691,11 @@ static irqreturn_t pciehp_isr(int irq, void *dev_id)
 	return IRQ_WAKE_THREAD;
 }
 
+/*
+ * 在以下使用pciehp_ist():
+ *   - drivers/pci/hotplug/pciehp_hpc.c|70| <<pciehp_request_irq>> retval = request_threaded_irq(irq, pciehp_isr, pciehp_ist, IRQF_SHARED, "pciehp", ctrl);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|769| <<pciehp_poll>> pciehp_ist(IRQ_NOTCONNECTED, ctrl);
+ */
 static irqreturn_t pciehp_ist(int irq, void *dev_id)
 {
 	struct controller *ctrl = (struct controller *)dev_id;
diff --git a/drivers/scsi/scsi_ioctl.c b/drivers/scsi/scsi_ioctl.c
index 6f6c5973c..e214ded23 100644
--- a/drivers/scsi/scsi_ioctl.c
+++ b/drivers/scsi/scsi_ioctl.c
@@ -832,6 +832,34 @@ static int scsi_cdrom_send_packet(struct scsi_device *sdev, bool open_for_write,
 	return err;
 }
 
+/*
+ *  40 typedef struct sg_io_hdr
+ 41 {
+ 42     int interface_id;           /* [i] 'S' for SCSI generic (required) */
+ 43     int dxfer_direction;        /* [i] data transfer direction  */
+ 44     unsigned char cmd_len;      /* [i] SCSI command length */
+ 45     unsigned char mx_sb_len;    /* [i] max length to write to sbp */
+ 46     unsigned short iovec_count; /* [i] 0 implies no scatter gather */
+ 47     unsigned int dxfer_len;     /* [i] byte count of data transfer */
+ 48     void __user *dxferp;        /* [i], [*io] points to data transfer memory
+ 49                                               or scatter gather list */
+ 50     unsigned char __user *cmdp; /* [i], [*i] points to command to perform */
+ 51     void __user *sbp;           /* [i], [*o] points to sense_buffer memory */
+ 52     unsigned int timeout;       /* [i] MAX_UINT->no timeout (unit: millisec) */
+ 53     unsigned int flags;         /* [i] 0 -> default, see SG_FLAG... */
+ 54     int pack_id;                /* [i->o] unused internally (normally) */
+ 55     void __user * usr_ptr;      /* [i->o] unused internally */
+ 56     unsigned char status;       /* [o] scsi status */
+ 57     unsigned char masked_status;/* [o] shifted, masked scsi status */
+ 58     unsigned char msg_status;   /* [o] messaging level data (optional) */
+ 59     unsigned char sb_len_wr;    /* [o] byte count actually written to sbp */
+ 60     unsigned short host_status; /* [o] errors from host adapter */
+ 61     unsigned short driver_status;/* [o] errors from software driver */
+ 62     int resid;                  /* [o] dxfer_len - actual_transferred */
+ 63     unsigned int duration;      /* [o] time taken by cmd (unit: millisec) */
+ 64     unsigned int info;          /* [o] auxiliary information */
+ 65 } sg_io_hdr_t;  /* 64 bytes long (on i386) */
+ */
 static int scsi_ioctl_sg_io(struct scsi_device *sdev, bool open_for_write,
 		void __user *argp)
 {
diff --git a/drivers/vfio/pci/vfio_pci_core.c b/drivers/vfio/pci/vfio_pci_core.c
index d94d61b92..c2e1242ae 100644
--- a/drivers/vfio/pci/vfio_pci_core.c
+++ b/drivers/vfio/pci/vfio_pci_core.c
@@ -1189,6 +1189,10 @@ static int vfio_pci_ioctl_get_irq_info(struct vfio_pci_core_device *vdev,
 	return copy_to_user(arg, &info, minsz) ? -EFAULT : 0;
 }
 
+/*
+ * 处理VFIO_DEVICE_SET_IRQS:
+ *   - drivers/vfio/pci/vfio_pci_core.c|1463| <<vfio_pci_core_ioctl>> return vfio_pci_ioctl_set_irqs(vdev, uarg);
+ */
 static int vfio_pci_ioctl_set_irqs(struct vfio_pci_core_device *vdev,
 				   struct vfio_irq_set __user *arg)
 {
diff --git a/drivers/vfio/pci/vfio_pci_intrs.c b/drivers/vfio/pci/vfio_pci_intrs.c
index fb5392b74..58c1e5a00 100644
--- a/drivers/vfio/pci/vfio_pci_intrs.c
+++ b/drivers/vfio/pci/vfio_pci_intrs.c
@@ -62,6 +62,11 @@ static void vfio_irq_ctx_free(struct vfio_pci_core_device *vdev,
 	kfree(ctx);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|279| <<vfio_intx_enable>> ctx = vfio_irq_ctx_alloc(vdev, 0);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|486| <<vfio_msi_set_vector_signal>> ctx = vfio_irq_ctx_alloc(vdev, vector);
+ */
 static struct vfio_pci_irq_ctx *
 vfio_irq_ctx_alloc(struct vfio_pci_core_device *vdev, unsigned long index)
 {
@@ -444,6 +449,12 @@ static int vfio_msi_alloc_irq(struct vfio_pci_core_device *vdev,
 	return map.index < 0 ? map.index : map.virq;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|546| <<vfio_msi_set_block>> ret = vfio_msi_set_vector_signal(vdev, j, fd, msix);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|551| <<vfio_msi_set_block>> vfio_msi_set_vector_signal(vdev, i, -1, msix);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|567| <<vfio_msi_disable>> vfio_msi_set_vector_signal(vdev, i, -1, msix);
+ */
 static int vfio_msi_set_vector_signal(struct vfio_pci_core_device *vdev,
 				      unsigned int vector, int fd, bool msix)
 {
@@ -535,6 +546,11 @@ static int vfio_msi_set_vector_signal(struct vfio_pci_core_device *vdev,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|706| <<vfio_pci_set_msi_trigger>> return vfio_msi_set_block(vdev, start, count, fds, msix);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|713| <<vfio_pci_set_msi_trigger>> ret = vfio_msi_set_block(vdev, start, count, fds, msix);
+ */
 static int vfio_msi_set_block(struct vfio_pci_core_device *vdev, unsigned start,
 			      unsigned count, int32_t *fds, bool msix)
 {
@@ -682,6 +698,10 @@ static int vfio_pci_set_intx_trigger(struct vfio_pci_core_device *vdev,
 	return 0;
 }
 
+/*
+ * 在以下使用vfio_pci_set_msi_trigger():
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|845| <<vfio_pci_set_irqs_ioctl>> func = vfio_pci_set_msi_trigger;
+ */
 static int vfio_pci_set_msi_trigger(struct vfio_pci_core_device *vdev,
 				    unsigned index, unsigned start,
 				    unsigned count, uint32_t flags, void *data)
diff --git a/drivers/vhost/iotlb.c b/drivers/vhost/iotlb.c
index ea61330a3..affa114ef 100644
--- a/drivers/vhost/iotlb.c
+++ b/drivers/vhost/iotlb.c
@@ -47,6 +47,14 @@ EXPORT_SYMBOL_GPL(vhost_iotlb_map_free);
  * Returns an error last is smaller than start or memory allocation
  * fails
  */
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|721| <<vdpasim_dma_map>> ret = vhost_iotlb_add_range_ctx(&vdpasim->iommu[asid], iova, iova + size - 1, pa, perm, opaque);
+ *   - drivers/vdpa/vdpa_user/iova_domain.c|35| <<vduse_iotlb_add_range>> ret = vhost_iotlb_add_range_ctx(domain->iotlb, start, last, addr, perm, map_file);
+ *   - drivers/vhost/iotlb.c|65| <<vhost_iotlb_add_range_ctx>> int err = vhost_iotlb_add_range_ctx(iotlb, start, mid, addr, perm, opaque);
+ *   - drivers/vhost/iotlb.c|107| <<vhost_iotlb_add_range>> return vhost_iotlb_add_range_ctx(iotlb, start, last, addr, perm, NULL);
+ *   - drivers/vhost/vdpa.c|992| <<vhost_vdpa_map>> r = vhost_iotlb_add_range_ctx(iotlb, iova, iova + size - 1, pa, perm, opaque);
+ */
 int vhost_iotlb_add_range_ctx(struct vhost_iotlb *iotlb,
 			      u64 start, u64 last,
 			      u64 addr, unsigned int perm,
@@ -100,6 +108,17 @@ int vhost_iotlb_add_range_ctx(struct vhost_iotlb *iotlb,
 }
 EXPORT_SYMBOL_GPL(vhost_iotlb_add_range_ctx);
 
+/*
+ * called by:
+ *   - drivers/vdpa/mlx5/core/mr.c|467| <<dup_iotlb>> err = vhost_iotlb_add_range(dst, start, last, start, VHOST_ACCESS_RW);
+ *   - drivers/vdpa/mlx5/core/mr.c|473| <<dup_iotlb>> err = vhost_iotlb_add_range(dst, map->start, map->last, map->addr, map->perm);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|157| <<vdpasim_do_reset>> vhost_iotlb_add_range(&vdpasim->iommu[i], 0, ULONG_MAX, 0, VHOST_MAP_RW);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|266| <<vdpasim_create>> vhost_iotlb_add_range(&vdpasim->iommu[i], 0, ULONG_MAX, 0, VHOST_MAP_RW);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|649| <<vdpasim_set_map>> ret = vhost_iotlb_add_range(iommu, map->start, map->last, map->addr, map->perm);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|674| <<vdpasim_reset_map>> vhost_iotlb_add_range(&vdpasim->iommu[asid], 0, ULONG_MAX, 0, VHOST_MAP_RW);
+ *   - drivers/vhost/vhost.c|1449| <<vhost_process_iotlb_msg>> if (vhost_iotlb_add_range(dev->iotlb, msg->iova, msg->iova + msg->size - 1, msg->uaddr, msg->perm)) {
+ *   - drivers/vhost/vhost.c|1814| <<vhost_set_memory>> if (vhost_iotlb_add_range(newumem, region->guest_phys_addr, region->guest_phys_addr + region->memory_size - 1, region->userspace_addr, VHOST_MAP_RW))
+ */
 int vhost_iotlb_add_range(struct vhost_iotlb *iotlb,
 			  u64 start, u64 last,
 			  u64 addr, unsigned int perm)
@@ -131,6 +150,12 @@ EXPORT_SYMBOL_GPL(vhost_iotlb_del_range);
  * @limit: maximum number of IOTLB entries
  * @flags: VHOST_IOTLB_FLAG_XXX
  */
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|265| <<vdpasim_create>> vhost_iotlb_init(&vdpasim->iommu[i], max_iotlb_entries, 0);
+ *   - drivers/vhost/iotlb.c|167| <<vhost_iotlb_alloc>> vhost_iotlb_init(iotlb, limit, flags);
+ *   - drivers/vhost/vdpa.c|117| <<vhost_vdpa_alloc_as>> vhost_iotlb_init(&as->iotlb, 0, 0);
+ */
 void vhost_iotlb_init(struct vhost_iotlb *iotlb, unsigned int limit,
 		      unsigned int flags)
 {
@@ -149,6 +174,13 @@ EXPORT_SYMBOL_GPL(vhost_iotlb_init);
  *
  * Returns an error is memory allocation fails
  */
+/*
+ * called by:
+ *   - drivers/vdpa/mlx5/core/mr.c|606| <<_mlx5_vdpa_create_mr>> mr->iotlb = vhost_iotlb_alloc(0, 0);
+ *   - drivers/vdpa/mlx5/core/resources.c|232| <<init_ctrl_vq>> mvdev->cvq.iotlb = vhost_iotlb_alloc(0, 0);
+ *   - drivers/vdpa/vdpa_user/iova_domain.c|589| <<vduse_domain_create>> domain->iotlb = vhost_iotlb_alloc(0, 0);
+ *   - drivers/vhost/vhost.c|968| <<iotlb_alloc>> return vhost_iotlb_alloc(max_iotlb_entries, VHOST_IOTLB_FLAG_RETIRE);
+ */
 struct vhost_iotlb *vhost_iotlb_alloc(unsigned int limit, unsigned int flags)
 {
 	struct vhost_iotlb *iotlb = kzalloc(sizeof(*iotlb), GFP_KERNEL);
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index c64ded183..7268249ef 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -35,6 +35,76 @@
 
 #include "vhost.h"
 
+/*
+ * From 0a0be13b8fe2cac11da2063fb03f0f39359b3069 Mon Sep 17 00:00:00 2001
+ * From: Jason Wang <jasowang@redhat.com>
+ * Date: Wed, 12 Sep 2018 11:17:09 +0800
+ * Subject: [PATCH 1/1] vhost_net: batch submitting XDP buffers to underlayer sockets
+ *
+ * This patch implements XDP batching for vhost_net. The idea is first to
+ * try to do userspace copy and build XDP buff directly in vhost. Instead
+ * of submitting the packet immediately, vhost_net will batch them in an
+ * array and submit every 64 (VHOST_NET_BATCH) packets to the under layer
+ * sockets through msg_control of sendmsg().
+ *
+ * When XDP is enabled on the TUN/TAP, TUN/TAP can process XDP inside a
+ * loop without caring GUP thus it can do batch map flushing. When XDP is
+ * not enabled or not supported, the underlayer socket need to build skb
+ * and pass it to network core. The batched packet submission allows us
+ * to do batching like netif_receive_skb_list() in the future.
+ *
+ * This saves lots of indirect calls for better cache utilization. For
+ * the case that we can't so batching e.g when sndbuf is limited or
+ * packet size is too large, we will go for usual one packet per
+ * sendmsg() way.
+ *
+ * Doing testpmd on various setups gives us:
+ *
+ * Test                /+pps%
+ * XDP_DROP on TAP     /+44.8%
+ * XDP_REDIRECT on TAP /+29%
+ * macvtap (skb)       /+26%
+ *
+ * Netperf tests shows obvious improvements for small packet transmission:
+ *
+ * size/session/+thu%/+normalize%
+ *    64/     1/   +2%/    0%
+ *    64/     2/   +3%/   +1%
+ *    64/     4/   +7%/   +5%
+ *    64/     8/   +8%/   +6%
+ *   256/     1/   +3%/    0%
+ *   256/     2/  +10%/   +7%
+ *   256/     4/  +26%/  +22%
+ *   256/     8/  +27%/  +23%
+ *   512/     1/   +3%/   +2%
+ *   512/     2/  +19%/  +14%
+ *   512/     4/  +43%/  +40%
+ *   512/     8/  +45%/  +41%
+ *  1024/     1/   +4%/    0%
+ *  1024/     2/  +27%/  +21%
+ *  1024/     4/  +38%/  +73%
+ *  1024/     8/  +15%/  +24%
+ *  2048/     1/  +10%/   +7%
+ *  2048/     2/  +16%/  +12%
+ *  2048/     4/    0%/   +2%
+ *  2048/     8/    0%/   +2%
+ *  4096/     1/  +36%/  +60%
+ *  4096/     2/  -11%/  -26%
+ *  4096/     4/    0%/  +14%
+ *  4096/     8/    0%/   +4%
+ * 16384/     1/   -1%/   +5%
+ * 16384/     2/    0%/   +2%
+ * 16384/     4/    0%/   -3%
+ * 16384/     8/    0%/   +4%
+ * 65535/     1/    0%/  +10%
+ * 65535/     2/    0%/   +8%
+ * 65535/     4/    0%/   +1%
+ * 65535/     8/    0%/   +3%
+ *
+ * Signed-off-by: Jason Wang <jasowang@redhat.com>
+ * Signed-off-by: David S. Miller <davem@davemloft.net>
+ */
+
 static int experimental_zcopytx = 0;
 module_param(experimental_zcopytx, int, 0444);
 MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
@@ -107,17 +177,93 @@ struct vhost_net_buf {
 
 struct vhost_net_virtqueue {
 	struct vhost_virtqueue vq;
+	/*
+	 * 在以下使用vhost_net_virtqueue->vhost_hlen:
+	 *   - drivers/vhost/net.c|352| <<vhost_net_vq_reset>> n->vqs[i].vhost_hlen = 0;
+	 *   - drivers/vhost/net.c|774| <<get_tx_bufs>> *len = init_iov_iter(vq, &msg->msg_iter, nvq->vhost_hlen, *out);
+	 *   - drivers/vhost/net.c|777| <<get_tx_bufs>> vq_err(vq, "Unexpected header len for TX: %zd expected %zd\n", *len, nvq->vhost_hlen);
+	 *   - drivers/vhost/net.c|1334| <<handle_rx>> vhost_hlen = nvq->vhost_hlen;
+	 *   - drivers/vhost/net.c|1524| <<vhost_net_open>> n->vqs[i].vhost_hlen = 0;
+	 *   - drivers/vhost/net.c|1845| <<vhost_net_set_features>> n->vqs[i].vhost_hlen = vhost_hlen;
+	 */
 	size_t vhost_hlen;
+	/*
+	 * 在以下使用vhost_net_virtqueue->sock_hlen:
+	 *   - drivers/vhost/net.c|353| <<vhost_net_vq_reset>> n->vqs[i].sock_hlen = 0;
+	 *   - drivers/vhost/net.c|822| <<vhost_net_build_xdp>> int pad = SKB_DATA_ALIGN(VHOST_NET_RX_PAD + headroom + nvq->sock_hlen);
+	 *   - drivers/vhost/net.c|823| <<vhost_net_build_xdp>> int sock_hlen = nvq->sock_hlen;
+	 *   - drivers/vhost/net.c|828| <<vhost_net_build_xdp>> if (unlikely(len < nvq->sock_hlen))
+	 *   - drivers/vhost/net.c|1335| <<handle_rx>> sock_hlen = nvq->sock_hlen;
+	 *   - drivers/vhost/net.c|1525| <<vhost_net_open>> n->vqs[i].sock_hlen = 0;
+	 *   - drivers/vhost/net.c|1846| <<vhost_net_set_features>> n->vqs[i].sock_hlen = sock_hlen;
+	 */
 	size_t sock_hlen;
+	/*
+	 * 在以下设置vhost_net_virtqueue->upend_idx:
+	 *   - drivers/vhost/net.c|439| <<vhost_net_vq_reset>> n->vqs[i].upend_idx = 0;
+	 *   - drivers/vhost/net.c|1310| <<handle_tx_zerocopy>> nvq->upend_idx = (nvq->upend_idx + 1) % UIO_MAXIOV;
+	 *   - drivers/vhost/net.c|1331| <<handle_tx_zerocopy>> nvq->upend_idx = ((unsigned )nvq->upend_idx - 1)
+	 *   - drivers/vhost/net.c|1740| <<vhost_net_open>> n->vqs[i].upend_idx = 0;
+	 * 在以下使用vhost_net_virtqueue->upend_idx:
+	 *   - drivers/vhost/net.c|501| <<vhost_zerocopy_signal_used>> for (i = nvq->done_idx; i != nvq->upend_idx; i = (i + 1) % UIO_MAXIOV) {
+	 *   - drivers/vhost/net.c|857| <<vhost_exceeds_maxpend>> return (nvq->upend_idx + UIO_MAXIOV - nvq->done_idx) % UIO_MAXIOV >
+	 *   - drivers/vhost/net.c|1296| <<handle_tx_zerocopy>> ubuf = nvq->ubuf_info + nvq->upend_idx;
+	 *   - drivers/vhost/net.c|1297| <<handle_tx_zerocopy>> vq->heads[nvq->upend_idx].id = cpu_to_vhost32(vq, head);
+	 *   - drivers/vhost/net.c|1298| <<handle_tx_zerocopy>> vq->heads[nvq->upend_idx].len = VHOST_DMA_IN_PROGRESS;
+	 *   - drivers/vhost/net.c|1300| <<handle_tx_zerocopy>> ubuf->desc = nvq->upend_idx;
+	 */
 	/* vhost zerocopy support fields below: */
 	/* last used idx for outstanding DMA zerocopy buffers */
 	int upend_idx;
 	/* For TX, first used idx for DMA done zerocopy buffers
 	 * For RX, number of batched heads
 	 */
+	/*
+	 * 在以下设置vhost_net_virtqueue->done_idx:
+	 *   - drivers/vhost/net.c|308| <<vhost_net_vq_reset>> n->vqs[i].done_idx = 0;
+	 *   - drivers/vhost/net.c|470| <<vhost_net_signal_used>> nvq->done_idx = 0;
+	 *   - drivers/vhost/net.c|501| <<vhost_tx_batch>> nvq->done_idx = 0;
+	 *   - drivers/vhost/net.c|868| <<handle_tx_copy>> ++nvq->done_idx;
+	 *   - drivers/vhost/net.c|1274| <<handle_rx>> nvq->done_idx += headcount;
+	 *   - drivers/vhost/net.c|1373| <<vhost_net_open>> n->vqs[i].done_idx = 0;
+	 * 在以下使用vhost_net_virtqueue->done_idx:
+	 *   - drivers/vhost/net.c|371| <<vhost_zerocopy_signal_used>> for (i = nvq->done_idx; i != nvq->upend_idx; i = (i + 1) % UIO_MAXIOV) {
+	 *   - drivers/vhost/net.c|381| <<vhost_zerocopy_signal_used>> add = min(UIO_MAXIOV - nvq->done_idx, j);
+	 *   - drivers/vhost/net.c|383| <<vhost_zerocopy_signal_used>> &vq->heads[nvq->done_idx], add);
+	 *   - drivers/vhost/net.c|384| <<vhost_zerocopy_signal_used>> nvq->done_idx = (nvq->done_idx + add) % UIO_MAXIOV;
+	 *   - drivers/vhost/net.c|466| <<vhost_net_signal_used>> if (!nvq->done_idx)
+	 *   - drivers/vhost/net.c|469| <<vhost_net_signal_used>> vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
+	 *   - drivers/vhost/net.c|616| <<vhost_exceeds_maxpend>> return (nvq->upend_idx + UIO_MAXIOV - nvq->done_idx) % UIO_MAXIOV >
+	 *   - drivers/vhost/net.c|800| <<handle_tx_copy>> if (nvq->done_idx == VHOST_NET_BATCH)
+	 *   - drivers/vhost/net.c|866| <<handle_tx_copy>> vq->heads[nvq->done_idx].id = cpu_to_vhost32(vq, head);
+	 *   - drivers/vhost/net.c|867| <<handle_tx_copy>> vq->heads[nvq->done_idx].len = 0;
+	 *   - drivers/vhost/net.c|1199| <<handle_rx>> headcount = get_rx_bufs(vq, vq->heads + nvq->done_idx,
+	 *   - drivers/vhost/net.c|1275| <<handle_rx>> if (nvq->done_idx > VHOST_NET_BATCH)
+	 */
 	int done_idx;
+	/*
+	 * 在以下使用vhost_net_virtqueue->batched_xdp:
+	 *   - drivers/vhost/net.c|510| <<vhost_tx_batch>> .num = nvq->batched_xdp,
+	 *   - drivers/vhost/net.c|515| <<vhost_tx_batch>> if (nvq->batched_xdp == 0)
+	 *   - drivers/vhost/net.c|528| <<vhost_tx_batch>> for (i = 0; i < nvq->batched_xdp; ++i)
+	 *   - drivers/vhost/net.c|530| <<vhost_tx_batch>> nvq->batched_xdp = 0;
+	 *   - drivers/vhost/net.c|537| <<vhost_tx_batch>> nvq->batched_xdp = 0;
+	 *   - drivers/vhost/net.c|718| <<vhost_net_build_xdp>> struct xdp_buff *xdp = &nvq->xdp[nvq->batched_xdp];
+	 *   - drivers/vhost/net.c|780| <<vhost_net_build_xdp>> ++nvq->batched_xdp;
+	 *   - drivers/vhost/net.c|1413| <<vhost_net_open>> n->vqs[i].batched_xdp = 0;
+	 */
 	/* Number of XDP frames batched */
 	int batched_xdp;
+	/*
+	 * 在以下使用vhost_net_virtqueue->ubuf_info:
+	 *   - drivers/vhost/net.c|403| <<vhost_net_clear_ubuf_info>> kfree(n->vqs[i].ubuf_info);
+	 *   - drivers/vhost/net.c|404| <<vhost_net_clear_ubuf_info>> n->vqs[i].ubuf_info = NULL;
+	 *   - drivers/vhost/net.c|417| <<vhost_net_set_ubuf_info>> n->vqs[i].ubuf_info = kmalloc_array(UIO_MAXIOV, sizeof(*n->vqs[i].ubuf_info), GFP_KERNEL);
+	 *   - drivers/vhost/net.c|419| <<vhost_net_set_ubuf_info>> n->vqs[i].ubuf_info = kmalloc_array(UIO_MAXIOV, sizeof(*n->vqs[i].ubuf_info), GFP_KERNEL);
+	 *   - drivers/vhost/net.c|421| <<vhost_net_set_ubuf_info>> if (!n->vqs[i].ubuf_info)
+	 *   - drivers/vhost/net.c|1296| <<handle_tx_zerocopy>> ubuf = nvq->ubuf_info + nvq->upend_idx;
+	 *   - drivers/vhost/net.c|1739| <<vhost_net_open>> n->vqs[i].ubuf_info = NULL;
+	 */
 	/* an array of userspace buffers info */
 	struct ubuf_info_msgzc *ubuf_info;
 	/* Reference counting for outstanding ubufs.
@@ -125,6 +271,14 @@ struct vhost_net_virtqueue {
 	struct vhost_net_ubuf_ref *ubufs;
 	struct ptr_ring *rx_ring;
 	struct vhost_net_buf rxq;
+	/*
+	 * 在以下使用vhost_net_virtqueue->xdp:
+	 *   - drivers/vhost/net.c|511| <<vhost_tx_batch>> .ptr = nvq->xdp,
+	 *   - drivers/vhost/net.c|529| <<vhost_tx_batch>> put_page(virt_to_head_page(nvq->xdp[i].data));
+	 *   - drivers/vhost/net.c|718| <<vhost_net_build_xdp>> struct xdp_buff *xdp = &nvq->xdp[nvq->batched_xdp];
+	 *   - drivers/vhost/net.c|1401| <<vhost_net_open>> n->vqs[VHOST_NET_VQ_TX].xdp = xdp;
+	 *   - drivers/vhost/net.c|1496| <<vhost_net_release>> kfree(n->vqs[VHOST_NET_VQ_TX].xdp);
+	 */
 	/* Batched XDP buffs */
 	struct xdp_buff *xdp;
 };
@@ -338,6 +492,12 @@ static bool vhost_net_tx_select_zcopy(struct vhost_net *net)
 		net->tx_packets / 64 >= net->tx_zcopy_err;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|584| <<vhost_net_tx_get_vq_desc>> if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
+ *   - drivers/vhost/net.c|980| <<handle_tx>> if (vhost_sock_zcopy(sock))
+ *   - drivers/vhost/net.c|1533| <<vhost_net_set_backend>> sock && vhost_sock_zcopy(sock));
+ */
 static bool vhost_sock_zcopy(struct socket *sock)
 {
 	return unlikely(experimental_zcopytx) &&
@@ -419,6 +579,13 @@ static bool vhost_can_busy_poll(unsigned long endtime)
 		      !signal_pending(current));
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|978| <<handle_tx>> vhost_net_disable_vq(net, vq);
+ *   - drivers/vhost/net.c|1147| <<handle_rx>> vhost_net_disable_vq(net, vq);
+ *   - drivers/vhost/net.c|1365| <<vhost_net_stop_vq>> vhost_net_disable_vq(n, vq);
+ *   - drivers/vhost/net.c|1539| <<vhost_net_set_backend>> vhost_net_disable_vq(n, vq);
+ */
 static void vhost_net_disable_vq(struct vhost_net *n,
 				 struct vhost_virtqueue *vq)
 {
@@ -445,11 +612,27 @@ static int vhost_net_enable_vq(struct vhost_net *n,
 	return vhost_poll_start(poll, sock->file);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|595| <<vhost_tx_batch>> vhost_net_signal_used(nvq);
+ *   - drivers/vhost/net.c|1237| <<vhost_net_rx_peek_head_len>> vhost_net_signal_used(rnvq);
+ *   - drivers/vhost/net.c|1456| <<handle_rx>> vhost_net_signal_used(nvq);
+ *   - drivers/vhost/net.c|1468| <<handle_rx>> vhost_net_signal_used(nvq);
+ */
 static void vhost_net_signal_used(struct vhost_net_virtqueue *nvq)
 {
 	struct vhost_virtqueue *vq = &nvq->vq;
 	struct vhost_dev *dev = vq->dev;
 
+	/*
+	 * 在以下设置vhost_net_virtqueue->done_idx:
+	 *   - drivers/vhost/net.c|308| <<vhost_net_vq_reset>> n->vqs[i].done_idx = 0;
+	 *   - drivers/vhost/net.c|470| <<vhost_net_signal_used>> nvq->done_idx = 0;
+	 *   - drivers/vhost/net.c|501| <<vhost_tx_batch>> nvq->done_idx = 0;
+	 *   - drivers/vhost/net.c|868| <<handle_tx_copy>> ++nvq->done_idx;
+	 *   - drivers/vhost/net.c|1274| <<handle_rx>> nvq->done_idx += headcount;
+	 *   - drivers/vhost/net.c|1373| <<vhost_net_open>> n->vqs[i].done_idx = 0;
+	 */
 	if (!nvq->done_idx)
 		return;
 
@@ -457,23 +640,82 @@ static void vhost_net_signal_used(struct vhost_net_virtqueue *nvq)
 	nvq->done_idx = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|598| <<vhost_net_tx_get_vq_desc>> vhost_tx_batch(net, tnvq, vhost_vq_get_backend(tvq), msghdr);
+ *   - drivers/vhost/net.c|801| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|830| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|840| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|871| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ */
 static void vhost_tx_batch(struct vhost_net *net,
 			   struct vhost_net_virtqueue *nvq,
 			   struct socket *sock,
 			   struct msghdr *msghdr)
 {
+	/*
+	 * 在以下使用vhost_net_virtqueue->batched_xdp:
+	 *   - drivers/vhost/net.c|510| <<vhost_tx_batch>> .num = nvq->batched_xdp,
+	 *   - drivers/vhost/net.c|515| <<vhost_tx_batch>> if (nvq->batched_xdp == 0)
+	 *   - drivers/vhost/net.c|528| <<vhost_tx_batch>> for (i = 0; i < nvq->batched_xdp; ++i)
+	 *   - drivers/vhost/net.c|530| <<vhost_tx_batch>> nvq->batched_xdp = 0;
+	 *   - drivers/vhost/net.c|537| <<vhost_tx_batch>> nvq->batched_xdp = 0;
+	 *   - drivers/vhost/net.c|718| <<vhost_net_build_xdp>> struct xdp_buff *xdp = &nvq->xdp[nvq->batched_xdp];
+	 *   - drivers/vhost/net.c|780| <<vhost_net_build_xdp>> ++nvq->batched_xdp;
+	 *   - drivers/vhost/net.c|1413| <<vhost_net_open>> n->vqs[i].batched_xdp = 0;
+	 *
+	 * 在以下使用vhost_net_virtqueue->xdp:
+	 *   - drivers/vhost/net.c|511| <<vhost_tx_batch>> .ptr = nvq->xdp,
+	 *   - drivers/vhost/net.c|529| <<vhost_tx_batch>> put_page(virt_to_head_page(nvq->xdp[i].data));
+	 *   - drivers/vhost/net.c|718| <<vhost_net_build_xdp>> struct xdp_buff *xdp = &nvq->xdp[nvq->batched_xdp];
+	 *   - drivers/vhost/net.c|1401| <<vhost_net_open>> n->vqs[VHOST_NET_VQ_TX].xdp = xdp;
+	 *   - drivers/vhost/net.c|1496| <<vhost_net_release>> kfree(n->vqs[VHOST_NET_VQ_TX].xdp);
+	 *
+	 * 地址会交给msghdr->msg_control
+	 */
 	struct tun_msg_ctl ctl = {
 		.type = TUN_MSG_PTR,
 		.num = nvq->batched_xdp,
 		.ptr = nvq->xdp,
 	};
+	/*
+	 * struct vhost_net_virtqueue *nvq:
+	 * -> int batched_xdp;
+	 * -> struct xdp_buff *xdp;
+	 *
+	 * 80 struct xdp_buff {
+	 * 81         void *data;
+	 * 82         void *data_end;
+	 * 83         void *data_meta;
+	 * 84         void *data_hard_start;
+	 * 85         struct xdp_rxq_info *rxq;
+	 * 86         struct xdp_txq_info *txq;
+	 * 87         u32 frame_sz; // frame size to deduce data_hard_end/reserved tailroom
+	 * 88         u32 flags; // supported values defined in xdp_buff_flags
+	 * 89 };
+	 */
 	int i, err;
 
+	/*
+	 * 在以下使用vhost_net_virtqueue->batched_xdp:
+	 *   - drivers/vhost/net.c|510| <<vhost_tx_batch>> .num = nvq->batched_xdp,
+	 *   - drivers/vhost/net.c|515| <<vhost_tx_batch>> if (nvq->batched_xdp == 0)
+	 *   - drivers/vhost/net.c|528| <<vhost_tx_batch>> for (i = 0; i < nvq->batched_xdp; ++i)
+	 *   - drivers/vhost/net.c|530| <<vhost_tx_batch>> nvq->batched_xdp = 0;
+	 *   - drivers/vhost/net.c|537| <<vhost_tx_batch>> nvq->batched_xdp = 0;
+	 *   - drivers/vhost/net.c|718| <<vhost_net_build_xdp>> struct xdp_buff *xdp = &nvq->xdp[nvq->batched_xdp];
+	 *   - drivers/vhost/net.c|780| <<vhost_net_build_xdp>> ++nvq->batched_xdp;
+	 *   - drivers/vhost/net.c|1413| <<vhost_net_open>> n->vqs[i].batched_xdp = 0;
+	 */
 	if (nvq->batched_xdp == 0)
 		goto signal_used;
 
 	msghdr->msg_control = &ctl;
 	msghdr->msg_controllen = sizeof(ctl);
+	/*
+	 * tap_sendmsg()
+	 * tun_sendmsg()
+	 */
 	err = sock->ops->sendmsg(sock, msghdr, 0);
 	if (unlikely(err < 0)) {
 		vq_err(&nvq->vq, "Fail to batch sending packets\n");
@@ -516,6 +758,11 @@ static void vhost_net_busy_poll_try_queue(struct vhost_net *net,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|680| <<vhost_net_tx_get_vq_desc>> vhost_net_busy_poll(net, rvq, tvq, busyloop_intr, false);
+ *   - drivers/vhost/net.c|1170| <<vhost_net_rx_peek_head_len>> vhost_net_busy_poll(net, rvq, tvq, busyloop_intr, true);
+ */
 static void vhost_net_busy_poll(struct vhost_net *net,
 				struct vhost_virtqueue *rvq,
 				struct vhost_virtqueue *tvq,
@@ -567,6 +814,10 @@ static void vhost_net_busy_poll(struct vhost_net *net,
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|716| <<get_tx_bufs>> ret = vhost_net_tx_get_vq_desc(net, nvq, out, in, msg, busyloop_intr);
+ */
 static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 				    struct vhost_net_virtqueue *tnvq,
 				    unsigned int *out_num, unsigned int *in_num,
@@ -576,18 +827,64 @@ static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 	struct vhost_virtqueue *rvq = &rnvq->vq;
 	struct vhost_virtqueue *tvq = &tnvq->vq;
 
+	/*
+	 * called by:                       
+	 *   - drivers/vhost/net.c|662| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|674| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1203| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|466| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/scsi.c|940| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|507| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *
+	 * struct vhost_virtqueue *tvq:
+	 * -> struct iovec iov[UIO_MAXIOV];
+	 *
+	 * vhost_get_vq_desc():
+	 * 这个函数会把一个avail index的(若干个desc)的addr都放到
+	 * iovec->iov_base和iovec->iov_len数组中
+	 */
 	int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov),
 				  out_num, in_num, NULL, NULL);
 
+	/*
+	 * 在以下使用vhost_virtqueue->busyloop_timeout:
+	 *   - drivers/vhost/net.c|623| <<vhost_net_busy_poll>> busyloop_timeout = poll_rx ? rvq->busyloop_timeout:
+	 *   - drivers/vhost/net.c|624| <<vhost_net_busy_poll>> tvq->busyloop_timeout;
+	 *   - drivers/vhost/net.c|673| <<vhost_net_tx_get_vq_desc>> if (r == tvq->num && tvq->busyloop_timeout) {
+	 *   - drivers/vhost/net.c|1166| <<vhost_net_rx_peek_head_len>> if (!len && rvq->busyloop_timeout) {
+	 *   - drivers/vhost/vhost.c|411| <<vhost_vq_reset>> vq->busyloop_timeout = 0;
+	 *   - drivers/vhost/vhost.c|2047| <<vhost_vring_ioctl(VHOST_SET_VRING_BUSYLOOP_TIMEOUT)>> vq->busyloop_timeout = s.num;
+	 *   - drivers/vhost/vhost.c|2051| <<vhost_vring_ioctl(VHOST_GET_VRING_BUSYLOOP_TIMEOUT)>> s.num = vq->busyloop_timeout;
+	 */
 	if (r == tvq->num && tvq->busyloop_timeout) {
 		/* Flush batched packets first */
 		if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
+			/*
+			 * called by:
+			 *   - drivers/vhost/net.c|598| <<vhost_net_tx_get_vq_desc>> vhost_tx_batch(net, tnvq, vhost_vq_get_backend(tvq), msghdr);
+			 *   - drivers/vhost/net.c|801| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+			 *   - drivers/vhost/net.c|830| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+			 *   - drivers/vhost/net.c|840| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+			 *   - drivers/vhost/net.c|871| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+			 */
 			vhost_tx_batch(net, tnvq,
 				       vhost_vq_get_backend(tvq),
 				       msghdr);
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|680| <<vhost_net_tx_get_vq_desc>> vhost_net_busy_poll(net, rvq, tvq, busyloop_intr, false);
+		 *   - drivers/vhost/net.c|1170| <<vhost_net_rx_peek_head_len>> vhost_net_busy_poll(net, rvq, tvq, busyloop_intr, true);
+		 */
 		vhost_net_busy_poll(net, rvq, tvq, busyloop_intr, false);
 
+		/*
+		 * vhost_get_vq_desc():
+		 * 这个函数会把一个avail index的(若干个desc)的addr都放到
+		 * iovec->iov_base和iovec->iov_len数组中
+		 */
 		r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov),
 				      out_num, in_num, NULL, NULL);
 	}
@@ -595,6 +892,11 @@ static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 	return r;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1381| <<handle_tx_zerocopy>> zcopy_used = len >= VHOST_GOODCOPY_LEN && !vhost_exceeds_maxpend(net) && vhost_net_tx_select_zcopy(net);
+ *   - drivers/vhost/net.c|1407| <<handle_tx_zerocopy>> if (tx_can_batch(vq, total_len) && likely(!vhost_exceeds_maxpend(net))) {
+ */
 static bool vhost_exceeds_maxpend(struct vhost_net *net)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
@@ -604,9 +906,16 @@ static bool vhost_exceeds_maxpend(struct vhost_net *net)
 	       min_t(unsigned int, VHOST_MAX_PEND, vq->num >> 2);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|805| <<get_tx_bufs>> *len = init_iov_iter(vq, &msg->msg_iter, nvq->vhost_hlen, *out);
+ */
 static size_t init_iov_iter(struct vhost_virtqueue *vq, struct iov_iter *iter,
 			    size_t hdr_size, int out)
 {
+	/*
+	 * 把所有的iov[seg].iov_len加起来
+	 */
 	/* Skip header. TODO: support TSO. */
 	size_t len = iov_length(vq->iov, out);
 
@@ -616,6 +925,11 @@ static size_t init_iov_iter(struct vhost_virtqueue *vq, struct iov_iter *iter,
 	return iov_iter_count(iter);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|842| <<handle_tx_copy>> head = get_tx_bufs(net, nvq, &msg, &out, &in, &len, &busyloop_intr);
+ *   - drivers/vhost/net.c|945| <<handle_tx_zerocopy>> head = get_tx_bufs(net, nvq, &msg, &out, &in, &len, &busyloop_intr);
+ */
 static int get_tx_bufs(struct vhost_net *net,
 		       struct vhost_net_virtqueue *nvq,
 		       struct msghdr *msg,
@@ -625,6 +939,20 @@ static int get_tx_bufs(struct vhost_net *net,
 	struct vhost_virtqueue *vq = &nvq->vq;
 	int ret;
 
+	/*
+	 * 根据情况desc有两种:
+	 *
+	 * 第一种, virtio的header独立一个desc:
+	 * 1. virtio的header
+	 * 2. skb->data的数据
+	 * 3 .. N. skb的frags们
+	 *
+	 * 第二种, virtio的header不是独立的：
+	 * 1. virtio的header和skb->data的数据
+	 * 2 .. N. skb的frags们
+	 *
+	 * 只在此处调用
+	 */
 	ret = vhost_net_tx_get_vq_desc(net, nvq, out, in, msg, busyloop_intr);
 
 	if (ret < 0 || ret == vq->num)
@@ -636,6 +964,18 @@ static int get_tx_bufs(struct vhost_net *net,
 		return -EFAULT;
 	}
 
+	/*
+	 * struct msghdr *msg:
+	 * -> struct iov_iter msg_iter;
+	 *
+	 * 在以下使用vhost_net_virtqueue->vhost_hlen:
+	 *   - drivers/vhost/net.c|352| <<vhost_net_vq_reset>> n->vqs[i].vhost_hlen = 0;
+	 *   - drivers/vhost/net.c|774| <<get_tx_bufs>> *len = init_iov_iter(vq, &msg->msg_iter, nvq->vhost_hlen, *out);
+	 *   - drivers/vhost/net.c|777| <<get_tx_bufs>> vq_err(vq, "Unexpected header len for TX: %zd expected %zd\n", *len, nvq->vhost_hlen);
+	 *   - drivers/vhost/net.c|1334| <<handle_rx>> vhost_hlen = nvq->vhost_hlen;
+	 *   - drivers/vhost/net.c|1524| <<vhost_net_open>> n->vqs[i].vhost_hlen = 0;
+	 *   - drivers/vhost/net.c|1845| <<vhost_net_set_features>> n->vqs[i].vhost_hlen = vhost_hlen;
+	 */
 	/* Sanity check */
 	*len = init_iov_iter(vq, &msg->msg_iter, nvq->vhost_hlen, *out);
 	if (*len == 0) {
@@ -647,6 +987,11 @@ static int get_tx_bufs(struct vhost_net *net,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1023| <<handle_tx_copy>> if (tx_can_batch(vq, total_len))
+ *   - drivers/vhost/net.c|1128| <<handle_tx_zerocopy>> if (tx_can_batch(vq, total_len) &&
+ */
 static bool tx_can_batch(struct vhost_virtqueue *vq, size_t total_len)
 {
 	return total_len < VHOST_NET_WEIGHT &&
@@ -655,6 +1000,15 @@ static bool tx_can_batch(struct vhost_virtqueue *vq, size_t total_len)
 
 #define VHOST_NET_RX_PAD (NET_IP_ALIGN + NET_SKB_PAD)
 
+/*
+ * handle_tx_net() or handle_tx_kick()
+ * -> handle_tx()
+ *    -> handle_tx_copy()
+ *       -> vhost_net_build_xdp()
+ *
+ * called by:
+ *   - drivers/vhost/net.c|784| <<handle_tx_copy>> err = vhost_net_build_xdp(nvq, &msg.msg_iter);
+ */
 static int vhost_net_build_xdp(struct vhost_net_virtqueue *nvq,
 			       struct iov_iter *from)
 {
@@ -663,17 +1017,84 @@ static int vhost_net_build_xdp(struct vhost_net_virtqueue *nvq,
 					     dev);
 	struct socket *sock = vhost_vq_get_backend(vq);
 	struct virtio_net_hdr *gso;
+	/*
+	 * 80 struct xdp_buff {
+	 * 81         void *data;
+	 * 82         void *data_end;
+	 * 83         void *data_meta;
+	 * 84         void *data_hard_start;
+	 * 85         struct xdp_rxq_info *rxq;
+	 * 86         struct xdp_txq_info *txq;
+	 * 87         u32 frame_sz; // frame size to deduce data_hard_end/reserved tailroom
+	 * 88         u32 flags; // supported values defined in xdp_buff_flags
+	 * 89 };
+	 *
+	 * 在以下使用vhost_net_virtqueue->batched_xdp:
+	 *   - drivers/vhost/net.c|510| <<vhost_tx_batch>> .num = nvq->batched_xdp,
+	 *   - drivers/vhost/net.c|515| <<vhost_tx_batch>> if (nvq->batched_xdp == 0)
+	 *   - drivers/vhost/net.c|528| <<vhost_tx_batch>> for (i = 0; i < nvq->batched_xdp; ++i)
+	 *   - drivers/vhost/net.c|530| <<vhost_tx_batch>> nvq->batched_xdp = 0;
+	 *   - drivers/vhost/net.c|537| <<vhost_tx_batch>> nvq->batched_xdp = 0;
+	 *   - drivers/vhost/net.c|718| <<vhost_net_build_xdp>> struct xdp_buff *xdp = &nvq->xdp[nvq->batched_xdp];
+	 *   - drivers/vhost/net.c|780| <<vhost_net_build_xdp>> ++nvq->batched_xdp;
+	 *   - drivers/vhost/net.c|1413| <<vhost_net_open>> n->vqs[i].batched_xdp = 0;
+	 *
+	 * 在以下使用vhost_net_virtqueue->xdp:
+	 *   - drivers/vhost/net.c|511| <<vhost_tx_batch>> .ptr = nvq->xdp,
+	 *   - drivers/vhost/net.c|529| <<vhost_tx_batch>> put_page(virt_to_head_page(nvq->xdp[i].data));
+	 *   - drivers/vhost/net.c|718| <<vhost_net_build_xdp>> struct xdp_buff *xdp = &nvq->xdp[nvq->batched_xdp];
+	 *   - drivers/vhost/net.c|1401| <<vhost_net_open>> n->vqs[VHOST_NET_VQ_TX].xdp = xdp;
+	 *   - drivers/vhost/net.c|1496| <<vhost_net_release>> kfree(n->vqs[VHOST_NET_VQ_TX].xdp);
+	 */
 	struct xdp_buff *xdp = &nvq->xdp[nvq->batched_xdp];
 	struct tun_xdp_hdr *hdr;
 	size_t len = iov_iter_count(from);
+	/*
+	 * 没有xdp的话headroom应该是0
+	 */
 	int headroom = vhost_sock_xdp(sock) ? XDP_PACKET_HEADROOM : 0;
+	/*
+	 * buflne和skb_shared_info有关
+	 * 比如320 (0x140)
+	 */
 	int buflen = SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+	/*
+	 * 比如
+	 * sizeof(struct skb_shared_info) = 320
+	 * SMP_CACHE_BYTES = 64
+	 * VHOST_NET_RX_PAD = 64
+	 *
+	 * 比如0x80=128
+	 */
 	int pad = SKB_DATA_ALIGN(VHOST_NET_RX_PAD + headroom + nvq->sock_hlen);
+	/*
+	 * 在以下使用vhost_net_virtqueue->sock_hlen:
+	 *   - drivers/vhost/net.c|353| <<vhost_net_vq_reset>> n->vqs[i].sock_hlen = 0;
+	 *   - drivers/vhost/net.c|822| <<vhost_net_build_xdp>> int pad = SKB_DATA_ALIGN(VHOST_NET_RX_PAD + headroom + nvq->sock_hlen);
+	 *   - drivers/vhost/net.c|823| <<vhost_net_build_xdp>> int sock_hlen = nvq->sock_hlen;
+	 *   - drivers/vhost/net.c|828| <<vhost_net_build_xdp>> if (unlikely(len < nvq->sock_hlen))
+	 *   - drivers/vhost/net.c|1335| <<handle_rx>> sock_hlen = nvq->sock_hlen;
+	 *   - drivers/vhost/net.c|1525| <<vhost_net_open>> n->vqs[i].sock_hlen = 0;
+	 *   - drivers/vhost/net.c|1846| <<vhost_net_set_features>> n->vqs[i].sock_hlen = sock_hlen;
+	 *
+	 * 比如
+	 * vhost_hlen = 0,
+	 * sock_hlen = 12, --> 对应的virtio_net_hdr_mrg_rxbuf, 由sock提供hdr
+	 */
 	int sock_hlen = nvq->sock_hlen;
 	void *buf;
 	int copied;
 	int ret;
 
+	/*
+	 * 上面结束后,一共有这些长度:
+	 * - len
+	 * - headroom
+	 * - buflen
+	 * - pad
+	 * - sock_hlen
+	 */
+
 	if (unlikely(len < nvq->sock_hlen))
 		return -EFAULT;
 
@@ -687,6 +1108,14 @@ static int vhost_net_build_xdp(struct vhost_net_virtqueue *nvq,
 	if (unlikely(!buf))
 		return -ENOMEM;
 
+	/*
+	 * struct tun_xdp_hdr {                 
+	 *     int buflen;
+	 *     struct virtio_net_hdr gso;
+	 * };
+	 *
+	 * 从from这个"struct iov_iter"拷贝sock_hlen的数据
+	 */
 	copied = copy_from_iter(buf + offsetof(struct tun_xdp_hdr, gso),
 				sock_hlen, from);
 	if (copied != sock_hlen) {
@@ -694,6 +1123,9 @@ static int vhost_net_build_xdp(struct vhost_net_virtqueue *nvq,
 		goto err;
 	}
 
+	/*
+	 * struct tun_xdp_hdr *hdr;
+	 */
 	hdr = buf;
 	gso = &hdr->gso;
 
@@ -714,17 +1146,62 @@ static int vhost_net_build_xdp(struct vhost_net_virtqueue *nvq,
 		}
 	}
 
+	/*
+	 * 减去sock_hlen很小怎么办??
+	 */
 	len -= sock_hlen;
+	/*
+	 *  int pad = SKB_DATA_ALIGN(VHOST_NET_RX_PAD + headroom + nvq->sock_hlen);
+	 *
+	 * 还是从from这个"struct iov_iter"拷贝
+	 */
 	copied = copy_from_iter(buf + pad, len, from);
 	if (copied != len) {
 		ret = -EFAULT;
 		goto err;
 	}
 
+	/*
+	 * 就是个初始化:
+	 * 116 static __always_inline void
+	 * 117 xdp_init_buff(struct xdp_buff *xdp, u32 frame_sz, struct xdp_rxq_info *rxq)
+	 * 118 {
+	 * 119         xdp->frame_sz = frame_sz;
+	 * 120         xdp->rxq = rxq;
+	 * 121         xdp->flags = 0;
+	 * 122 }
+	 */
 	xdp_init_buff(xdp, buflen, NULL);
+	/*
+	 * int pad = SKB_DATA_ALIGN(VHOST_NET_RX_PAD + headroom + nvq->sock_hlen);
+	 *
+	 * 很重要的函数
+	 * 124 static __always_inline void
+	 * 125 xdp_prepare_buff(struct xdp_buff *xdp, unsigned char *hard_start,
+	 * 126                  int headroom, int data_len, const bool meta_valid)
+	 * 127 {
+	 * 128         unsigned char *data = hard_start + headroom;
+	 * 129
+	 * 130         xdp->data_hard_start = hard_start;
+	 * 131         xdp->data = data;
+	 * 132         xdp->data_end = data + data_len;
+	 * 133         xdp->data_meta = meta_valid ? data : data + 1;
+	 * 134 }
+	 */
 	xdp_prepare_buff(xdp, buf, pad, len, true);
 	hdr->buflen = buflen;
 
+	/*
+	 * 在以下使用vhost_net_virtqueue->batched_xdp:
+	 *   - drivers/vhost/net.c|510| <<vhost_tx_batch>> .num = nvq->batched_xdp,
+	 *   - drivers/vhost/net.c|515| <<vhost_tx_batch>> if (nvq->batched_xdp == 0)
+	 *   - drivers/vhost/net.c|528| <<vhost_tx_batch>> for (i = 0; i < nvq->batched_xdp; ++i)
+	 *   - drivers/vhost/net.c|530| <<vhost_tx_batch>> nvq->batched_xdp = 0;
+	 *   - drivers/vhost/net.c|537| <<vhost_tx_batch>> nvq->batched_xdp = 0;
+	 *   - drivers/vhost/net.c|718| <<vhost_net_build_xdp>> struct xdp_buff *xdp = &nvq->xdp[nvq->batched_xdp];
+	 *   - drivers/vhost/net.c|780| <<vhost_net_build_xdp>> ++nvq->batched_xdp;
+	 *   - drivers/vhost/net.c|1413| <<vhost_net_open>> n->vqs[i].batched_xdp = 0;
+	 */
 	++nvq->batched_xdp;
 
 	return 0;
@@ -734,6 +1211,37 @@ static int vhost_net_build_xdp(struct vhost_net_virtqueue *nvq,
 	return ret;
 }
 
+/*
+ * 132 struct vhost_net {
+ * 133         struct vhost_dev dev;
+ * 134         struct vhost_net_virtqueue vqs[VHOST_NET_VQ_MAX];
+ * 135         struct vhost_poll poll[VHOST_NET_VQ_MAX];
+ * 136         // Number of TX recently submitted.
+ * 137         // Protected by tx vq lock.
+ * 138         unsigned tx_packets;
+ * 139         // Number of times zerocopy TX recently failed.
+ * 140         // Protected by tx vq lock.
+ * 141         unsigned tx_zcopy_err;
+ * 142         // Flush in progress. Protected by tx vq lock.
+ * 143         bool tx_flush;
+ * 144         // Private page frag cache
+ * 145         struct page_frag_cache pf_cache;
+ * 146 };
+ *
+ * called by:
+ *   - drivers/vhost/net.c|956| <<handle_tx>> handle_tx_copy(net, sock);
+ *
+ * 根据情况desc有两种:
+ *
+ * 第一种, virtio的header独立一个desc:
+ * 1. virtio的header
+ * 2. skb->data的数据
+ * 3 .. N. skb的frags们
+ *
+ * 第二种, virtio的header不是独立的：
+ * 1. virtio的header和skb->data的数据
+ * 2 .. N. skb的frags们
+ */
 static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
@@ -755,6 +1263,15 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 	do {
 		bool busyloop_intr = false;
 
+		/*
+		 * 在以下设置vhost_net_virtqueue->done_idx:
+		 *   - drivers/vhost/net.c|308| <<vhost_net_vq_reset>> n->vqs[i].done_idx = 0;
+		 *   - drivers/vhost/net.c|470| <<vhost_net_signal_used>> nvq->done_idx = 0;
+		 *   - drivers/vhost/net.c|501| <<vhost_tx_batch>> nvq->done_idx = 0;
+		 *   - drivers/vhost/net.c|868| <<handle_tx_copy>> ++nvq->done_idx;
+		 *   - drivers/vhost/net.c|1274| <<handle_rx>> nvq->done_idx += headcount;
+		 *   - drivers/vhost/net.c|1373| <<vhost_net_open>> n->vqs[i].done_idx = 0;
+		 */
 		if (nvq->done_idx == VHOST_NET_BATCH)
 			vhost_tx_batch(net, nvq, sock, &msg);
 
@@ -781,6 +1298,9 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 		 * sndbuf is unlimited.
 		 */
 		if (sock_can_batch) {
+			/*
+			 * 只在以下调用
+			 */
 			err = vhost_net_build_xdp(nvq, &msg.msg_iter);
 			if (!err) {
 				goto done;
@@ -804,6 +1324,11 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 				msg.msg_flags &= ~MSG_MORE;
 		}
 
+		/*
+		 * tap_sendmsg()
+		 *
+		 * struct msghdr msg
+		 */
 		err = sock->ops->sendmsg(sock, &msg, len);
 		if (unlikely(err < 0)) {
 			if (err == -EAGAIN || err == -ENOMEM || err == -ENOBUFS) {
@@ -816,14 +1341,30 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 			pr_debug("Truncated TX packet: len %d != %zd\n",
 				 err, len);
 done:
+		/*
+		 * 要给ring的used的信息
+		 */
 		vq->heads[nvq->done_idx].id = cpu_to_vhost32(vq, head);
 		vq->heads[nvq->done_idx].len = 0;
+		/*
+		 * 在以下设置vhost_net_virtqueue->done_idx:
+		 *   - drivers/vhost/net.c|308| <<vhost_net_vq_reset>> n->vqs[i].done_idx = 0;
+		 *   - drivers/vhost/net.c|470| <<vhost_net_signal_used>> nvq->done_idx = 0;
+		 *   - drivers/vhost/net.c|501| <<vhost_tx_batch>> nvq->done_idx = 0;
+		 *   - drivers/vhost/net.c|868| <<handle_tx_copy>> ++nvq->done_idx;
+		 *   - drivers/vhost/net.c|1274| <<handle_rx>> nvq->done_idx += headcount;
+		 *   - drivers/vhost/net.c|1373| <<vhost_net_open>> n->vqs[i].done_idx = 0;
+		 */
 		++nvq->done_idx;
 	} while (likely(!vhost_exceeds_weight(vq, ++sent_pkts, total_len)));
 
 	vhost_tx_batch(net, nvq, sock, &msg);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|954| <<handle_tx>> handle_tx_zerocopy(net, sock);
+ */
 static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
@@ -933,6 +1474,11 @@ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
 
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1235| <<handle_tx_kick>> handle_tx(net);
+ *   - drivers/vhost/net.c|1251| <<handle_tx_net>> handle_tx(net);
+ */
 static void handle_tx(struct vhost_net *net)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
@@ -944,6 +1490,13 @@ static void handle_tx(struct vhost_net *net)
 	if (!sock)
 		goto out;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|974| <<handle_tx>> if (!vq_meta_prefetch(vq))
+	 *   - drivers/vhost/net.c|1143| <<handle_rx>> if (!vq_meta_prefetch(vq))
+	 *   - drivers/vhost/vsock.c|103| <<vhost_transport_do_send_pkt>> if (!vq_meta_prefetch(vq))
+	 *   - drivers/vhost/vsock.c|492| <<vhost_vsock_handle_tx_kick>> if (!vq_meta_prefetch(vq))
+	 */
 	if (!vq_meta_prefetch(vq))
 		goto out;
 
@@ -1226,6 +1779,10 @@ static void handle_rx(struct vhost_net *net)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用handle_tx_kick():
+ *   - drivers/vhost/net.c|1327| <<vhost_net_open>> n->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;
+ */
 static void handle_tx_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 8995730ce..061f164b8 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -224,6 +224,14 @@ EXPORT_SYMBOL_GPL(vhost_poll_start);
 
 /* Stop polling a file. After this function returns, it becomes safe to drop the
  * file reference. You must also flush afterwards. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|430| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+ *   - drivers/vhost/test.c|285| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+ *   - drivers/vhost/vhost.c|217| <<vhost_poll_start>> vhost_poll_stop(poll);
+ *   - drivers/vhost/vhost.c|944| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+ *   - drivers/vhost/vhost.c|1997| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+ */
 void vhost_poll_stop(struct vhost_poll *poll)
 {
 	if (poll->wqh) {
@@ -326,6 +334,22 @@ bool vhost_vq_has_work(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_has_work);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|412| <<vhost_zerocopy_callback>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|525| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|528| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|811| <<handle_tx_copy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|914| <<handle_tx_zerocopy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1208| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1284| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|176| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+ *   - drivers/vhost/vhost.c|479| <<vhost_exceeds_weight>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|1350| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+ *   - drivers/vhost/vhost.h|56| <<vhost_iotlb_notify_vq>> void vhost_poll_queue(struct vhost_poll *poll);
+ *   - drivers/vhost/vsock.c|257| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+ *   - drivers/vhost/vsock.c|320| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+ */
 void vhost_poll_queue(struct vhost_poll *poll)
 {
 	vhost_vq_work_queue(poll->vq, &poll->work);
@@ -461,13 +485,45 @@ static void vhost_dev_free_iovecs(struct vhost_dev *dev)
 		vhost_vq_free_iovecs(dev->vqs[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|869| <<handle_tx_copy>> } while (likely(!vhost_exceeds_weight(vq, ++sent_pkts, total_len)));
+ *   - drivers/vhost/net.c|982| <<handle_tx_zerocopy>> } while (likely(!vhost_exceeds_weight(vq, ++sent_pkts, total_len)));
+ *   - drivers/vhost/net.c|1281| <<handle_rx>> } while (likely(!vhost_exceeds_weight(vq, ++recv_pkts, total_len)));
+ *   - drivers/vhost/scsi.c|1242| <<vhost_scsi_handle_vq>> } while (likely(!vhost_exceeds_weight(vq, ++c, 0)));
+ *   - drivers/vhost/scsi.c|1472| <<vhost_scsi_ctl_handle_vq>> } while (likely(!vhost_exceeds_weight(vq, ++c, 0)));
+ *   - drivers/vhost/test.c|89| <<handle_vq>> if (unlikely(vhost_exceeds_weight(vq, 0, total_len)))
+ *   - drivers/vhost/vsock.c|249| <<vhost_transport_do_send_pkt>> } while (likely(!vhost_exceeds_weight(vq, ++pkts, total_len)));
+ *   - drivers/vhost/vsock.c|543| <<vhost_vsock_handle_tx_kick>> } while (likely(!vhost_exceeds_weight(vq, ++pkts, total_len)));
+ */
 bool vhost_exceeds_weight(struct vhost_virtqueue *vq,
 			  int pkts, int total_len)
 {
 	struct vhost_dev *dev = vq->dev;
 
+	/*
+	 * 在以下使用vhost_dev->byte_weight:
+	 *   - drivers/vhost/vhost.c|477| <<vhost_exceeds_weight>> if ((dev->byte_weight && total_len >= dev->byte_weight) ||
+	 *   - drivers/vhost/vhost.c|530| <<vhost_dev_init>> dev->byte_weight = byte_weight;
+	 */
 	if ((dev->byte_weight && total_len >= dev->byte_weight) ||
 	    pkts >= dev->weight) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|412| <<vhost_zerocopy_callback>> vhost_poll_queue(&vq->poll);
+		 *   - drivers/vhost/net.c|525| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+		 *   - drivers/vhost/net.c|528| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+		 *   - drivers/vhost/net.c|811| <<handle_tx_copy>> vhost_poll_queue(&vq->poll);
+		 *   - drivers/vhost/net.c|914| <<handle_tx_zerocopy>> vhost_poll_queue(&vq->poll);
+		 *   - drivers/vhost/net.c|1208| <<handle_rx>> vhost_poll_queue(&vq->poll);
+		 *   - drivers/vhost/net.c|1284| <<handle_rx>> vhost_poll_queue(&vq->poll);
+		 *   - drivers/vhost/vhost.c|176| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+		 *   - drivers/vhost/vhost.c|479| <<vhost_exceeds_weight>> vhost_poll_queue(&vq->poll);
+		 *   - drivers/vhost/vhost.c|1350| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+		 *   - drivers/vhost/vhost.h|56| <<vhost_iotlb_notify_vq>> void vhost_poll_queue(struct vhost_poll *poll);
+		 *   - drivers/vhost/vsock.c|257| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+		 *   - drivers/vhost/vsock.c|320| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+		 */
 		vhost_poll_queue(&vq->poll);
 		return true;
 	}
@@ -558,6 +614,10 @@ bool vhost_dev_has_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_has_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|932| <<vhost_dev_set_owner>> vhost_attach_mm(dev);
+ */
 static void vhost_attach_mm(struct vhost_dev *dev)
 {
 	/* No owner, become one */
@@ -907,6 +967,12 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_set_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|974| <<vhost_dev_reset_owner_prepare>> return iotlb_alloc();
+ *   - drivers/vhost/vhost.c|1805| <<vhost_set_memory>> newumem = iotlb_alloc();
+ *   - drivers/vhost/vhost.c|2087| <<vhost_init_device_iotlb>> niotlb = iotlb_alloc();
+ */
 static struct vhost_iotlb *iotlb_alloc(void)
 {
 	return vhost_iotlb_alloc(max_iotlb_entries,
@@ -1321,6 +1387,10 @@ static inline int vhost_get_used_idx(struct vhost_virtqueue *vq,
 	return vhost_get_used(vq, *idx, &vq->used->idx);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2646| <<vhost_get_vq_desc>> ret = vhost_get_desc(vq, &desc, i);
+ */
 static inline int vhost_get_desc(struct vhost_virtqueue *vq,
 				 struct vring_desc *desc, int idx)
 {
@@ -1604,6 +1674,10 @@ static bool vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,
 	       access_ok(used, vhost_get_used_size(vq, num));
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1703| <<iotlb_access_ok>> vhost_vq_meta_update(vq, map, type);
+ */
 static void vhost_vq_meta_update(struct vhost_virtqueue *vq,
 				 const struct vhost_iotlb_map *map,
 				 int type)
@@ -1649,6 +1723,13 @@ static bool iotlb_access_ok(struct vhost_virtqueue *vq,
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|974| <<handle_tx>> if (!vq_meta_prefetch(vq))
+ *   - drivers/vhost/net.c|1143| <<handle_rx>> if (!vq_meta_prefetch(vq))
+ *   - drivers/vhost/vsock.c|103| <<vhost_transport_do_send_pkt>> if (!vq_meta_prefetch(vq))
+ *   - drivers/vhost/vsock.c|492| <<vhost_vsock_handle_tx_kick>> if (!vq_meta_prefetch(vq))
+ */
 int vq_meta_prefetch(struct vhost_virtqueue *vq)
 {
 	unsigned int num = vq->num;
@@ -2329,6 +2410,17 @@ int vhost_vq_init_access(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_init_access);
 
+/*
+ * called by:
+ *   -  drivers/vhost/vhost.c|1169| <<vhost_copy_to_user>> ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_WO);
+ *   - drivers/vhost/vhost.c|1204| <<vhost_copy_from_user>> ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|1229| <<__vhost_get_user_slow>> ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|2273| <<log_used>> ret = translate_desc(vq, (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+ *   - drivers/vhost/vhost.c|2477| <<get_indirect>> ret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV, VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|2518| <<get_indirect>> ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len), iov + iov_count,
+ *                                                   iov_size - iov_count, access);
+ *   - drivers/vhost/vhost.c|2669| <<vhost_get_vq_desc>> ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr), out_num, in_num, log, log_num, &desc)
+ */
 static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			  struct iovec iov[], int iov_size, int access)
 {
@@ -2377,6 +2469,11 @@ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 /* Each buffer in the virtqueues is actually a chain of descriptors.  This
  * function returns the next descriptor in the chain,
  * or -1U if we're at the end. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2545| <<get_indirect>> } while ((i = next_desc(vq, &desc)) != -1);
+ *   - drivers/vhost/vhost.c|2697| <<vhost_get_vq_desc>> } while ((i = next_desc(vq, &desc)) != -1);
+ */
 static unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)
 {
 	unsigned int next;
@@ -2390,6 +2487,10 @@ static unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)
 	return next;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2653| <<vhost_get_vq_desc>> ret = get_indirect(vq, iov, iov_size, out_num, in_num, log, log_num, &desc);
+ */
 static int get_indirect(struct vhost_virtqueue *vq,
 			struct iovec iov[], unsigned int iov_size,
 			unsigned int *out_num, unsigned int *in_num,
@@ -2479,6 +2580,11 @@ static int get_indirect(struct vhost_virtqueue *vq,
 			}
 			*out_num += ret;
 		}
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2545| <<get_indirect>> } while ((i = next_desc(vq, &desc)) != -1);
+		 *   - drivers/vhost/vhost.c|2697| <<vhost_get_vq_desc>> } while ((i = next_desc(vq, &desc)) != -1);
+		 */
 	} while ((i = next_desc(vq, &desc)) != -1);
 	return 0;
 }
@@ -2491,6 +2597,21 @@ static int get_indirect(struct vhost_virtqueue *vq,
  * This function returns the descriptor number found, or vq->num (which is
  * never a valid descriptor number) if none was found.  A negative code is
  * returned on error. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|662| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|674| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|1203| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+ *   - drivers/vhost/scsi.c|466| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/scsi.c|940| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+ *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|507| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *
+ * vhost_get_vq_desc():
+ * 这个函数会把一个avail index的(若干个desc)的addr都放到
+ * iovec->iov_base和iovec->iov_len数组中
+ */
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
@@ -2503,15 +2624,34 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 	__virtio16 ring_head;
 	int ret, access;
 
+	/*
+	 * 在以下设置vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|394| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1974| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_avail_idx = s.num & 0xffff;
+	 *   - drivers/vhost/vhost.c|1981| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|2700| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2712| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 */
 	/* Check it isn't doing very strange things with descriptor numbers. */
 	last_avail_idx = vq->last_avail_idx;
 
 	if (vq->avail_idx == vq->last_avail_idx) {
+		/*
+		 * 从ring把avail的index数值拷贝出来
+		 */
 		if (unlikely(vhost_get_avail_idx(vq, &avail_idx))) {
 			vq_err(vq, "Failed to access avail idx at %p\n",
 				&vq->avail->idx);
 			return -EFAULT;
 		}
+		/*
+		 * 在以下设置vhost_virtqueue->avail_idx:
+		 *   - drivers/vhost/vhost.c|395| <<vhost_vq_reset>> vq->avail_idx = 0;
+		 *   - drivers/vhost/vhost.c|1984| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->avail_idx = vq->last_avail_idx;
+		 *   - drivers/vhost/vhost.c|2589| <<vhost_get_vq_desc>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+		 *   - drivers/vhost/vhost.c|2877| <<vhost_vq_avail_empty>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+		 *   - drivers/vhost/vhost.c|2926| <<vhost_enable_notify>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+		 */
 		vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
 
 		if (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {
@@ -2534,6 +2674,9 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 
 	/* Grab the next descriptor number they're advertising, and increment
 	 * the index we've seen. */
+	/*
+	 * 从avail的ring里根据idx的到对应的desc的index
+	 */
 	if (unlikely(vhost_get_avail_head(vq, &ring_head, last_avail_idx))) {
 		vq_err(vq, "Failed to read head: idx %d address %p\n",
 		       last_avail_idx,
@@ -2555,6 +2698,9 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 	if (unlikely(log))
 		*log_num = 0;
 
+	/*
+	 * desc的index
+	 */
 	i = head;
 	do {
 		unsigned iov_count = *in_num + *out_num;
@@ -2569,6 +2715,16 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			       i, vq->num, head);
 			return -EINVAL;
 		}
+		/*
+		 * 107 struct vring_desc {
+		 * 108         __virtio64 addr;
+		 * 109         __virtio32 len;
+		 * 110         __virtio16 flags;
+		 * 111         __virtio16 next;
+		 * 112 };
+		 *
+		 * 拷贝处vring_desc
+		 */
 		ret = vhost_get_desc(vq, &desc, i);
 		if (unlikely(ret)) {
 			vq_err(vq, "Failed to get descriptor: idx %d addr %p\n",
@@ -2576,6 +2732,9 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			return -EFAULT;
 		}
 		if (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT)) {
+			/*
+			 * 只在此处调用
+			 */
 			ret = get_indirect(vq, iov, iov_size,
 					   out_num, in_num,
 					   log, log_num, &desc);
@@ -2592,6 +2751,21 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			access = VHOST_ACCESS_WO;
 		else
 			access = VHOST_ACCESS_RO;
+		/*
+		 * 上面iov_count是unsigned iov_count = *in_num + *out_num;
+		 *
+		 * called by:
+		 *   - drivers/vhost/vhost.c|1169| <<vhost_copy_to_user>> ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|1204| <<vhost_copy_from_user>> ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1229| <<__vhost_get_user_slow>> ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2273| <<log_used>> ret = translate_desc(vq, (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2477| <<get_indirect>> ret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV, VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2518| <<get_indirect>> ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len), iov + iov_count,
+		 *                                                   iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2669| <<vhost_get_vq_desc>> ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr), out_num, in_num, log, log_num, &desc)
+		 *
+		 * 把desc的addr和len, 翻译成iovec->iov_base和iovec->iov_len
+		 */
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
 				     iov_size - iov_count, access);
@@ -2620,8 +2794,21 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			}
 			*out_num += ret;
 		}
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2545| <<get_indirect>> } while ((i = next_desc(vq, &desc)) != -1);
+		 *   - drivers/vhost/vhost.c|2697| <<vhost_get_vq_desc>> } while ((i = next_desc(vq, &desc)) != -1);
+		 */
 	} while ((i = next_desc(vq, &desc)) != -1);
 
+	/*
+	 * 在以下设置vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|394| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1974| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_avail_idx = s.num & 0xffff;
+	 *   - drivers/vhost/vhost.c|1981| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|2700| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2712| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 */
 	/* On success, increment avail index. */
 	vq->last_avail_idx++;
 
@@ -2777,6 +2964,11 @@ void vhost_add_used_and_signal(struct vhost_dev *dev,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|512| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq, &vq->heads[nvq->done_idx], add);
+ *   - drivers/vhost/net.c|615| <<vhost_net_signal_used>> vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
+ */
 /* multi-buffer version of vhost_add_used_and_signal */
 void vhost_add_used_and_signal_n(struct vhost_dev *dev,
 				 struct vhost_virtqueue *vq,
@@ -2864,6 +3056,26 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_enable_notify);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|514| <<vhost_net_busy_poll_try_queue>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|537| <<vhost_net_busy_poll>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|785| <<handle_tx_copy>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|887| <<handle_tx_zerocopy>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|977| <<handle_tx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1146| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1176| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/scsi.c|465| <<vhost_scsi_do_evt_work>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|954| <<vhost_scsi_get_desc>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1066| <<vhost_scsi_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1391| <<vhost_scsi_ctl_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/test.c|58| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/test.c|71| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/vsock.c|107| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|140| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|495| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|514| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ */
 /* We don't need to be notified again. */
 void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
@@ -2881,6 +3093,10 @@ void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_disable_notify);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1631| <<vhost_iotlb_miss>> node = vhost_new_msg(vq, v2 ? VHOST_IOTLB_MSG_V2 : VHOST_IOTLB_MSG);
+ */
 /* Create a new message. */
 struct vhost_msg_node *vhost_new_msg(struct vhost_virtqueue *vq, int type)
 {
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index 9e942fcda..32ca0d11e 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -86,6 +86,13 @@ struct vhost_virtqueue {
 	vring_desc_t __user *desc;
 	vring_avail_t __user *avail;
 	vring_used_t __user *used;
+	/*
+	 * 在以下使用vhost_virtqueue->meta_iotlb:
+	 *   - drivers/vhost/vhost.h|89| <<global>> const struct vhost_iotlb_map *meta_iotlb[VHOST_NUM_ADDRS];
+	 *   - drivers/vhost/vhost.c|364| <<__vhost_vq_meta_reset>> vq->meta_iotlb[j] = NULL;
+	 *   - drivers/vhost/vhost.c|1111| <<vhost_vq_meta_fetch>> const struct vhost_iotlb_map *map = vq->meta_iotlb[type];
+	 *   - drivers/vhost/vhost.c|1675| <<vhost_vq_meta_update>> vq->meta_iotlb[type] = map;
+	 */
 	const struct vhost_iotlb_map *meta_iotlb[VHOST_NUM_ADDRS];
 	struct file *kick;
 	struct vhost_vring_call call_ctx;
@@ -100,14 +107,63 @@ struct vhost_virtqueue {
 	/* Last available index we saw.
 	 * Values are limited to 0x7fff, and the high bit is used as
 	 * a wrap counter when using VIRTIO_F_RING_PACKED. */
+	/*
+	 * 在以下设置vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|394| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1974| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_avail_idx = s.num & 0xffff;
+	 *   - drivers/vhost/vhost.c|1981| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|2700| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2712| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 * 在以下使用vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|1984| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|1989| <<vhost_vring_ioctl(VHOST_GET_VRING_BASE)>> s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
+	 *   - drivers/vhost/vhost.c|1991| <<vhost_vring_ioctl(VHOST_GET_VRING_BASE)>> s.num = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2581| <<vhost_get_vq_desc>> last_avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2583| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2809| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2870| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2878| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2927| <<vhost_enable_notify>> if (vq->avail_idx != vq->last_avail_idx) {
+	 */
 	u16 last_avail_idx;
 
+	/*
+	 * 在以下设置vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|395| <<vhost_vq_reset>> vq->avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1984| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2589| <<vhost_get_vq_desc>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2877| <<vhost_vq_avail_empty>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2926| <<vhost_enable_notify>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 * 在以下使用vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|1285| <<vhost_put_avail_event>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
+	 *   - drivers/vhost/vhost.c|2583| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2591| <<vhost_get_vq_desc>> if (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {
+	 *   - drivers/vhost/vhost.c|2593| <<vhost_get_vq_desc>> vq_err(vq, "Guest moved avail index from %u to %u", last_avail_idx, vq->avail_idx);
+	 *   - drivers/vhost/vhost.c|2600| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2809| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2870| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2878| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx) {
+	 */
 	/* Caches available index value from user. */
 	u16 avail_idx;
 
 	/* Last index we used.
 	 * Values are limited to 0x7fff, and the high bit is used as
 	 * a wrap counter when using VIRTIO_F_RING_PACKED. */
+	/*
+	 * 在以下设置vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|396| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1975| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_used_idx = (s.num >> 16) & 0xffff;
+	 *   - drivers/vhost/vhost.c|2386| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2751| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|1307| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|1989| <<vhost_vring_ioctl(VHOST_GET_VRING_BASE)>> s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
+	 *   - drivers/vhost/vhost.c|2737| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2750| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2768| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2822| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 last_used_idx;
 
 	/* Used flags */
@@ -145,6 +201,16 @@ struct vhost_virtqueue {
 	/* Ring endianness requested by userspace for cross-endian support. */
 	bool user_be;
 #endif
+	/*
+	 * 在以下使用vhost_virtqueue->busyloop_timeout:
+	 *   - drivers/vhost/net.c|623| <<vhost_net_busy_poll>> busyloop_timeout = poll_rx ? rvq->busyloop_timeout:
+	 *   - drivers/vhost/net.c|624| <<vhost_net_busy_poll>> tvq->busyloop_timeout;
+	 *   - drivers/vhost/net.c|673| <<vhost_net_tx_get_vq_desc>> if (r == tvq->num && tvq->busyloop_timeout) {
+	 *   - drivers/vhost/net.c|1166| <<vhost_net_rx_peek_head_len>> if (!len && rvq->busyloop_timeout) {
+	 *   - drivers/vhost/vhost.c|411| <<vhost_vq_reset>> vq->busyloop_timeout = 0;
+	 *   - drivers/vhost/vhost.c|2047| <<vhost_vring_ioctl(VHOST_SET_VRING_BUSYLOOP_TIMEOUT)>> vq->busyloop_timeout = s.num;
+	 *   - drivers/vhost/vhost.c|2051| <<vhost_vring_ioctl(VHOST_GET_VRING_BUSYLOOP_TIMEOUT)>> s.num = vq->busyloop_timeout;
+	 */
 	u32 busyloop_timeout;
 };
 
@@ -171,8 +237,24 @@ struct vhost_dev {
 	wait_queue_head_t wait;
 	int iov_limit;
 	int weight;
+	/*
+	 * 在以下使用vhost_dev->byte_weight:
+	 *   - drivers/vhost/vhost.c|477| <<vhost_exceeds_weight>> if ((dev->byte_weight && total_len >= dev->byte_weight) ||
+	 *   - drivers/vhost/vhost.c|530| <<vhost_dev_init>> dev->byte_weight = byte_weight;
+	 */
 	int byte_weight;
 	struct xarray worker_xa;
+	/*
+	 * 在以下使用vhost_dev->use_worker:
+	 *   - drivers/vhost/vhost.c|173| <<vhost_poll_wakeup>> if (!poll->dev->use_worker)
+	 *   - drivers/vhost/vhost.c|579| <<vhost_dev_init>> dev->use_worker = use_worker;
+	 *   - drivers/vhost/vhost.c|620| <<vhost_attach_mm>> if (dev->use_worker) {
+	 *   - drivers/vhost/vhost.c|639| <<vhost_detach_mm>> if (dev->use_worker)
+	 *   - drivers/vhost/vhost.c|664| <<vhost_workers_free>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|776| <<vhost_vq_attach_worker>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|855| <<vhost_worker_ioctl>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|938| <<vhost_dev_set_owner>> if (dev->use_worker) {
+	 */
 	bool use_worker;
 	int (*msg_handler)(struct vhost_dev *dev, u32 asid,
 			   struct vhost_iotlb_msg *msg);
@@ -283,6 +365,34 @@ static inline void vhost_vq_set_backend(struct vhost_virtqueue *vq,
  * Context: Need to call with vq->mutex acquired.
  * Return: Private data previously set with vhost_vq_set_backend.
  */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|428| <<vhost_net_disable_vq>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/net.c|441| <<vhost_net_enable_vq>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|538| <<vhost_net_busy_poll>> sock = vhost_vq_get_backend(rvq);
+ *   - drivers/vhost/net.c|584| <<vhost_net_tx_get_vq_desc>> if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
+ *   - drivers/vhost/net.c|586| <<vhost_net_tx_get_vq_desc>> vhost_vq_get_backend(tvq),
+ *   - drivers/vhost/net.c|673| <<vhost_net_build_xdp>> struct socket *sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|970| <<handle_tx>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|1139| <<handle_rx>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|1364| <<vhost_net_stop_vq>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|1530| <<vhost_net_set_backend>> oldsock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/scsi.c|459| <<vhost_scsi_do_evt_work>> if (!vhost_vq_get_backend(vq)) {
+ *   - drivers/vhost/scsi.c|1017| <<vhost_scsi_get_req>> vs_tpg = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/scsi.c|1059| <<vhost_scsi_handle_vq>> vs_tpg = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/scsi.c|1386| <<vhost_scsi_ctl_handle_vq>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/scsi.c|1522| <<vhost_scsi_evt_handle_kick>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/scsi.c|1770| <<vhost_scsi_set_endpoint>> if (!vhost_vq_get_backend(&vs->vqs[i].vq))
+ *   - drivers/vhost/scsi.c|2150| <<vhost_scsi_do_plug>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/test.c|52| <<handle_vq>> private = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/test.c|136| <<vhost_test_stop_vq>> private = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/test.c|194| <<vhost_test_run>> oldpriv = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/test.c|286| <<vhost_test_set_backend>> backend = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/vhost.c|691| <<__vhost_vq_attach_worker>> if (!vhost_vq_get_backend(vq) && !vq->kick) {
+ *   - drivers/vhost/vsock.c|100| <<vhost_transport_do_send_pkt>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/vsock.c|489| <<vhost_vsock_handle_tx_kick>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/vsock.c|585| <<vhost_vsock_start>> if (!vhost_vq_get_backend(vq)) {
+ */
 static inline void *vhost_vq_get_backend(struct vhost_virtqueue *vq)
 {
 	return vq->private_data;
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 6f7e5010a..73a45189c 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -2194,6 +2194,13 @@ static int virtqueue_enable_after_reset(struct virtqueue *_vq)
  * Generic functions and exported symbols.
  */
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2244| <<virtqueue_add_sgs>> return virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|2267| <<virtqueue_add_outbuf>> return virtqueue_add(vq, &sg, num, 1, 0, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|2289| <<virtqueue_add_inbuf>> return virtqueue_add(vq, &sg, num, 0, 1, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|2313| <<virtqueue_add_inbuf_ctx>> return virtqueue_add(vq, &sg, num, 0, 1, data, ctx, gfp);
+ */
 static inline int virtqueue_add(struct virtqueue *_vq,
 				struct scatterlist *sgs[],
 				unsigned int total_sg,
@@ -2259,6 +2266,25 @@ EXPORT_SYMBOL_GPL(virtqueue_add_sgs);
  *
  * Returns zero or a negative error (ie. ENOSPC, ENOMEM, EIO).
  */
+/*
+ * called by:
+ *   - drivers/bluetooth/virtio_bt.c|102| <<virtbt_send_frame>> err = virtqueue_add_outbuf(vbt->vqs[VIRTBT_VQ_TX], sg, 1, skb, GFP_KERNEL);
+ *   - drivers/char/virtio_console.c|558| <<__send_control_msg>> if (virtqueue_add_outbuf(vq, sg, 1, &portdev->cpkt, GFP_ATOMIC) == 0) {
+ *   - drivers/char/virtio_console.c|610| <<__send_to_port>> err = virtqueue_add_outbuf(out_vq, sg, nents, data, GFP_ATOMIC);
+ *   - drivers/misc/nsm.c|209| <<nsm_sendrecv_msg_locked>> rc = virtqueue_add_outbuf(vq, &sg_out, 1, msg->req.data, GFP_KERNEL);
+ *   - drivers/net/caif/caif_virtio.c|575| <<cfv_netdev_tx>> ret = virtqueue_add_outbuf(cfv->vq_tx, &sg, 1, buf_info, GFP_ATOMIC);
+ *   - drivers/net/virtio_net.c|981| <<__virtnet_xdp_xmit_one>> err = virtqueue_add_outbuf(sq->vq, sq->sg, nr_frags + 1, xdp_to_ptr(xdpf), GFP_ATOMIC);
+ *   - drivers/net/virtio_net.c|2482| <<xmit_skb>> return virtqueue_add_outbuf(sq->vq, sq->sg, num_sg, skb, GFP_ATOMIC);
+ *   - drivers/net/wireless/virtual/mac80211_hwsim.c|946| <<hwsim_tx_virtio>> err = virtqueue_add_outbuf(hwsim_vqs[HWSIM_VQ_TX], sg, 1, skb, GFP_ATOMIC);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|633| <<rpmsg_send_offchannel_raw>> err = virtqueue_add_outbuf(vrp->svq, &sg, 1, msg, GFP_KERNEL);
+ *   - drivers/virtio/virtio_balloon.c|158| <<tell_host>> virtqueue_add_outbuf(vq, &sg, 1, vb, GFP_KERNEL);
+ *   - drivers/virtio/virtio_balloon.c|390| <<stats_handle_request>> virtqueue_add_outbuf(vq, &sg, 1, vb, GFP_KERNEL);
+ *   - drivers/virtio/virtio_balloon.c|585| <<init_vqs>> err = virtqueue_add_outbuf(vb->stats_vq, &sg, 1, vb, GFP_KERNEL);
+ *   - drivers/virtio/virtio_balloon.c|630| <<send_cmd_id_start>> err = virtqueue_add_outbuf(vq, &sg, 1, &vb->cmd_id_active, GFP_KERNEL);
+ *   - drivers/virtio/virtio_balloon.c|647| <<send_cmd_id_stop>> err = virtqueue_add_outbuf(vq, &sg, 1, &vb->cmd_id_stop, GFP_KERNEL);
+ *   - drivers/virtio/virtio_input.c|94| <<virtinput_send_status>> rc = virtqueue_add_outbuf(vi->sts, sg, 1, stsbuf, GFP_ATOMIC);
+ *   - fs/fuse/virtio_fs.c|490| <<send_forget_request>> ret = virtqueue_add_outbuf(vq, &sg, 1, forget, GFP_ATOMIC);
+ */
 int virtqueue_add_outbuf(struct virtqueue *vq,
 			 struct scatterlist *sg, unsigned int num,
 			 void *data,
diff --git a/fs/open.c b/fs/open.c
index ee8460c83..f51a2aacb 100644
--- a/fs/open.c
+++ b/fs/open.c
@@ -630,6 +630,12 @@ SYSCALL_DEFINE1(chroot, const char __user *, filename)
 	return error;
 }
 
+/*
+ * called by:
+ *   - fs/init.c|108| <<init_chmod>> error = chmod_common(&path, mode);
+ *   - fs/open.c|666| <<vfs_fchmod>> return chmod_common(&file->f_path, mode);
+ *   - fs/open.c|698| <<do_fchmodat>> error = chmod_common(&path, mode);
+ */
 int chmod_common(const struct path *path, umode_t mode)
 {
 	struct inode *inode = path->dentry->d_inode;
@@ -645,6 +651,23 @@ int chmod_common(const struct path *path, umode_t mode)
 	error = security_path_chmod(path, mode);
 	if (error)
 		goto out_unlock;
+	/*
+	 * 例子
+	 * #define S_IRWXU 00700
+	 * #define S_IRUSR 00400
+	 * #define S_IWUSR 00200
+	 * #define S_IXUSR 00100
+	 *
+	 * #define S_IRWXG 00070
+	 * #define S_IRGRP 00040
+	 * #define S_IWGRP 00020
+	 * #define S_IXGRP 00010
+	 *
+	 * #define S_IRWXO 00007
+	 * #define S_IROTH 00004
+	 * #define S_IWOTH 00002
+	 * #define S_IXOTH 00001
+	 */
 	newattrs.ia_mode = (mode & S_IALLUGO) | (inode->i_mode & ~S_IALLUGO);
 	newattrs.ia_valid = ATTR_MODE | ATTR_CTIME;
 	error = notify_change(mnt_idmap(path->mnt), path->dentry,
diff --git a/include/linux/gfp.h b/include/linux/gfp.h
index c775ea3c6..dded9e875 100644
--- a/include/linux/gfp.h
+++ b/include/linux/gfp.h
@@ -316,6 +316,11 @@ extern void __page_frag_cache_drain(struct page *page, unsigned int count);
 void *__page_frag_alloc_align(struct page_frag_cache *nc, unsigned int fragsz,
 			      gfp_t gfp_mask, unsigned int align_mask);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1106| <<vhost_net_build_xdp>> buf = page_frag_alloc_align(&net->pf_cache, buflen, GFP_KERNEL, SMP_CACHE_BYTES);
+ *   - net/rxrpc/txbuf.c|36| <<rxrpc_alloc_data_txbuf>> buf = page_frag_alloc_align(&call->conn->tx_data_alloc, total, gfp, data_align);
+ */
 static inline void *page_frag_alloc_align(struct page_frag_cache *nc,
 					  unsigned int fragsz, gfp_t gfp_mask,
 					  unsigned int align)
diff --git a/include/linux/if_tap.h b/include/linux/if_tap.h
index 553552fa6..b97e9a7b8 100644
--- a/include/linux/if_tap.h
+++ b/include/linux/if_tap.h
@@ -63,6 +63,26 @@ struct tap_dev {
 struct tap_queue {
 	struct sock sk;
 	struct socket sock;
+	/*
+	 * 188 struct virtio_net_hdr {
+	 * 189         // See VIRTIO_NET_HDR_F_*
+	 * 190         __u8 flags;
+	 * 191         // See VIRTIO_NET_HDR_GSO_*
+	 * 192         __u8 gso_type;
+	 * 193         __virtio16 hdr_len;             // Ethernet + IP + tcp/udp hdrs
+	 * 194         __virtio16 gso_size;            // Bytes to append to hdr_len per frame
+	 * 195         __virtio16 csum_start;  // Position to start checksumming from
+	 * 196         __virtio16 csum_offset; // Offset after that to place checksum
+	 * 197 };
+	 *
+	 * 在以下使用tap_queue->vnet_hdr_sz:
+	 *   - drivers/net/tap.c|542| <<tap_open>> q->vnet_hdr_sz = sizeof(struct virtio_net_hdr);
+	 *   - drivers/net/tap.c|664| <<tap_get_user>> vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
+	 *   - drivers/net/tap.c|812| <<tap_put_user>> vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
+	 *   - drivers/net/tap.c|1085| <<tap_ioctl(TUNGETVNETHDRSZ)>> s = q->vnet_hdr_sz;
+	 *   - drivers/net/tap.c|1096| <<tap_ioctl>> q->vnet_hdr_sz = s;
+	 *   - drivers/net/tap.c|1190| <<tap_get_user_xdp>> vnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);
+	 */
 	int vnet_hdr_sz;
 	struct tap_dev __rcu *tap;
 	struct file *file;
diff --git a/include/linux/irqbypass.h b/include/linux/irqbypass.h
index 9bdb2a781..f0bb5586c 100644
--- a/include/linux/irqbypass.h
+++ b/include/linux/irqbypass.h
@@ -46,8 +46,20 @@ struct irq_bypass_producer {
 	struct list_head node;
 	void *token;
 	int irq;
+	/*
+	 * 在以下使用irq_bypass_producer->add_consumer:
+	 *   - virt/lib/irqbypass.c|61| <<__connect>> if (prod->add_consumer)
+	 *   - virt/lib/irqbypass.c|62| <<__connect>> ret = prod->add_consumer(prod, cons);
+	 */
 	int (*add_consumer)(struct irq_bypass_producer *,
 			    struct irq_bypass_consumer *);
+	/*
+	 * 在以下使用irq_bypass_producer->del_consumer:
+	 *   - virt/lib/irqbypass.c|66| <<__connect>> if (ret && prod->del_consumer)
+	 *   - virt/lib/irqbypass.c|67| <<__connect>> prod->del_consumer(prod, cons);
+	 *   - virt/lib/irqbypass.c|94| <<__disconnect>> if (prod->del_consumer)
+	 *   - virt/lib/irqbypass.c|95| <<__disconnect>> prod->del_consumer(prod, cons);
+	 */
 	void (*del_consumer)(struct irq_bypass_producer *,
 			     struct irq_bypass_consumer *);
 	void (*stop)(struct irq_bypass_producer *);
@@ -71,8 +83,20 @@ struct irq_bypass_producer {
 struct irq_bypass_consumer {
 	struct list_head node;
 	void *token;
+	/*
+	 * 在以下使用irq_bypass_consumer->add_producer:
+	 *   - virt/kvm/eventfd.c|452| <<kvm_irqfd_assign>> irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+	 *   - virt/lib/irqbypass.c|65| <<__connect>> ret = cons->add_producer(cons, prod);
+	 *   - virt/lib/irqbypass.c|234| <<irq_bypass_register_consumer>> !consumer->add_producer || !consumer->del_producer)
+	 */
 	int (*add_producer)(struct irq_bypass_consumer *,
 			    struct irq_bypass_producer *);
+	/*
+	 * 在以下使用del_producer:
+	 *   - virt/kvm/eventfd.c|453| <<kvm_irqfd_assign>> irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
+	 *   - virt/lib/irqbypass.c|92| <<__disconnect>> cons->del_producer(cons, prod);
+	 *   - virt/lib/irqbypass.c|234| <<irq_bypass_register_consumer>> !consumer->add_producer || !consumer->del_producer)
+	 */
 	void (*del_producer)(struct irq_bypass_consumer *,
 			     struct irq_bypass_producer *);
 	void (*stop)(struct irq_bypass_consumer *);
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 48f31dcd3..a3138875b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -789,12 +789,28 @@ struct kvm {
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
 	struct {
 		spinlock_t        lock;
+		/*
+		 * 在以下使用kvm.irqfds.items:
+		 *   - virt/kvm/eventfd.c|410| <<kvm_irqfd_assign>> list_for_each_entry(tmp, &kvm->irqfds.items, list) {
+		 *   - virt/kvm/eventfd.c|422| <<kvm_irqfd_assign>> list_add_tail(&irqfd->list, &kvm->irqfds.items);
+		 *   - virt/kvm/eventfd.c|558| <<kvm_irqfd_deassign>> list_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list) {
+		 *   - virt/kvm/eventfd.c|613| <<kvm_irqfd_release>> list_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list)
+		 *   - virt/kvm/eventfd.c|636| <<kvm_irq_routing_update>> list_for_each_entry(irqfd, &kvm->irqfds.items, list) {
+		 *   - virt/kvm/eventfd.c|1032| <<kvm_eventfd_init>> INIT_LIST_HEAD(&kvm->irqfds.items);
+		 */
 		struct list_head  items;
 		/* resampler_list update side is protected by resampler_lock. */
 		struct list_head  resampler_list;
 		struct mutex      resampler_lock;
 	} irqfds;
 #endif
+	/*
+	 * 在以下使用kvm->ioeventfds:
+	 *   - virt/kvm/eventfd.c|853| <<ioeventfd_check_collision>> list_for_each_entry(_p, &kvm->ioeventfds, list)
+	 *   - virt/kvm/eventfd.c|931| <<kvm_assign_ioeventfd_idx>> list_add_tail(&p->list, &kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|971| <<kvm_deassign_ioeventfd_idx>> list_for_each_entry(p, &kvm->ioeventfds, list) {
+	 *   - virt/kvm/eventfd.c|1094| <<kvm_eventfd_init>> INIT_LIST_HEAD(&kvm->ioeventfds);
+	 */
 	struct list_head ioeventfds;
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
@@ -810,8 +826,28 @@ struct kvm {
 	/*
 	 * Update side is protected by irq_lock.
 	 */
+	/*
+	 * 在以下使用kvm->irq_routing:
+	 *   - arch/x86/kvm/hyperv.c|544| <<kvm_hv_irq_routing_update>> irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu,
+	 *   - arch/x86/kvm/irq_comm.c|422| <<kvm_scan_ioapic_routes>> table = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - arch/x86/kvm/svm/avic.c|909| <<avic_pi_update_irte>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|406| <<vmx_pi_update_irte>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - virt/kvm/irqchip.c|28| <<kvm_irq_map_gsi>> irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu, lockdep_is_held(&kvm->irq_lock));
+	 *   - virt/kvm/irqchip.c|44| <<kvm_irq_map_chip_pin>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - virt/kvm/irqchip.c|123| <<kvm_free_irq_routing>> struct kvm_irq_routing_table *rt = rcu_access_pointer(kvm->irq_routing);
+	 *   - virt/kvm/irqchip.c|219| <<kvm_set_irq_routing>> old = rcu_dereference_protected(kvm->irq_routing, 1);
+	 *   - virt/kvm/irqchip.c|220| <<kvm_set_irq_routing>> rcu_assign_pointer(kvm->irq_routing, new);
+	 */
 	struct kvm_irq_routing_table __rcu *irq_routing;
 
+	/*
+	 * 在以下使用kvm->iirq_ack_notifier_list:
+	 *   - arch/powerpc/kvm/book3s_hv_rm_xics.c|709| <<ics_rm_eoi>> if (!hlist_empty(&vcpu->kvm->irq_ack_notifier_list)) {
+	 *   - virt/kvm/eventfd.c|496| <<kvm_irq_has_notifier>> hlist_for_each_entry_srcu(kian, &kvm->irq_ack_notifier_list,
+	 *   - virt/kvm/eventfd.c|513| <<kvm_notify_acked_gsi>> hlist_for_each_entry_srcu(kian, &kvm->irq_ack_notifier_list,
+	 *   - virt/kvm/eventfd.c|536| <<kvm_register_irq_ack_notifier>> hlist_add_head_rcu(&kian->link, &kvm->irq_ack_notifier_list);
+	 *   - virt/kvm/kvm_main.c|1269| <<kvm_create_vm>> INIT_HLIST_HEAD(&kvm->irq_ack_notifier_list);
+	 */
 	struct hlist_head irq_ack_notifier_list;
 #endif
 
diff --git a/include/linux/kvm_irqfd.h b/include/linux/kvm_irqfd.h
index 8ad43692e..817680d8a 100644
--- a/include/linux/kvm_irqfd.h
+++ b/include/linux/kvm_irqfd.h
@@ -40,6 +40,9 @@ struct kvm_kernel_irqfd {
 	/* Used for MSI fast-path */
 	struct kvm *kvm;
 	wait_queue_entry_t wait;
+	/*
+	 * 存放的是一个copy!
+	 */
 	/* Update side is protected by irqfds.lock */
 	struct kvm_kernel_irq_routing_entry irq_entry;
 	seqcount_spinlock_t irq_entry_sc;
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index d2a15c0c6..8e12e16cb 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -921,8 +921,45 @@ struct perf_event_context {
 	 */
 	struct mutex			mutex;
 
+	/*
+	 * 在以下使用perf_event_context->pmu_ctx_list:
+	 *   - kernel/events/core.c|692| <<perf_ctx_disable>> list_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|703| <<perf_ctx_enable>> list_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|3334| <<ctx_sched_out>> list_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|3460| <<perf_event_swap_task_ctx_data>> double_list_for_each_entry(prev_epc, next_epc, &prev_ctx->pmu_ctx_list, &next_ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|3484| <<perf_ctx_sched_task_cb>> list_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|3880| <<ctx_groups_sched_in>> list_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|4719| <<__perf_event_init_context>> INIT_LIST_HEAD(&ctx->pmu_ctx_list);
+	 *   - kernel/events/core.c|4869| <<find_get_pmu_context>> list_add(&epc->pmu_ctx_entry, &ctx->pmu_ctx_list);
+	 *   - kernel/events/core.c|4903| <<find_get_pmu_context>> list_for_each_entry(epc, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|4914| <<find_get_pmu_context>> list_add(&epc->pmu_ctx_entry, &ctx->pmu_ctx_list);
+	 */
 	struct list_head		pmu_ctx_list;
+	/*
+	 * 在以下使用perf_event_context->pinned_groups:
+	 *   - kernel/events/core.c|1851| <<get_event_groups>> return &ctx->pinned_groups;
+	 *   - kernel/events/core.c|4259| <<ctx_sched_in>> ctx_groups_sched_in(ctx, &ctx->pinned_groups, cgroup);
+	 *   - kernel/events/core.c|4304| <<perf_event_context_sched_in>> if (!RB_EMPTY_ROOT(&ctx->pinned_groups.tree)) {
+	 *   - kernel/events/core.c|4313| <<perf_event_context_sched_in>> if (!RB_EMPTY_ROOT(&ctx->pinned_groups.tree))
+	 *   - kernel/events/core.c|5046| <<__perf_event_init_context>> perf_event_groups_init(&ctx->pinned_groups);
+	 *   - kernel/events/core.c|10551| <<perf_tp_event_target_task>> perf_event_groups_for_cpu_pmu(event, &ctx->pinned_groups, cpu, pmu) {
+	 *   - kernel/events/core.c|13362| <<perf_pmu_migrate_context>> __perf_pmu_remove(src_ctx, src_cpu, pmu, &src_ctx->pinned_groups, &events);
+	 *   - kernel/events/core.c|13903| <<perf_event_init_context>> perf_event_groups_for_each(event, &parent_ctx->pinned_groups) {
+	 */
 	struct perf_event_groups	pinned_groups;
+	/*
+	 * 在以下使用perf_event_context->flexible_groups:
+	 *   - kernel/events/core.c|1853| <<get_event_groups>> return &ctx->flexible_groups;
+	 *   - kernel/events/core.c|4195| <<__pmu_ctx_sched_in>> pmu_groups_sched_in(ctx, &ctx->flexible_groups, pmu);
+	 *   - kernel/events/core.c|4263| <<ctx_sched_in>> ctx_groups_sched_in(ctx, &ctx->flexible_groups, cgroup);
+	 *   - kernel/events/core.c|4535| <<rotate_ctx>> perf_event_groups_delete(&ctx->flexible_groups, event);
+	 *   - kernel/events/core.c|4536| <<rotate_ctx>> perf_event_groups_insert(&ctx->flexible_groups, event);
+	 *   - kernel/events/core.c|4557| <<ctx_event_to_rotate>> tree = &pmu_ctx->ctx->flexible_groups.tree;
+	 *   - kernel/events/core.c|5047| <<__perf_event_init_context>> perf_event_groups_init(&ctx->flexible_groups);
+	 *   - kernel/events/core.c|10557| <<perf_tp_event_target_task>> perf_event_groups_for_cpu_pmu(event, &ctx->flexible_groups, cpu, pmu) {
+	 *   - kernel/events/core.c|13363| <<perf_pmu_migrate_context>> __perf_pmu_remove(src_ctx, src_cpu, pmu, &src_ctx->flexible_groups, &events);
+	 *   - kernel/events/core.c|13919| <<perf_event_init_context>> perf_event_groups_for_each(event, &parent_ctx->flexible_groups) {
+	 */
 	struct perf_event_groups	flexible_groups;
 	struct list_head		event_list;
 
@@ -952,6 +989,15 @@ struct perf_event_context {
 	struct perf_event_context	*parent_ctx;
 	u64				parent_gen;
 	u64				generation;
+	/*
+	 * 在以下使用perf_event_context->pin_count:
+	 *   - kernel/events/core.c|1738| <<perf_pin_task_context>> ++ctx->pin_count;
+	 *   - kernel/events/core.c|1749| <<perf_unpin_context>> --ctx->pin_count;
+	 *   - kernel/events/core.c|3661| <<context_equiv>> if (ctx1->pin_count || ctx2->pin_count)
+	 *   - kernel/events/core.c|5119| <<find_get_context>> ++ctx->pin_count;
+	 *   - kernel/events/core.c|5130| <<find_get_context>> ++ctx->pin_count;
+	 *   - kernel/events/core.c|5154| <<find_get_context>> ++ctx->pin_count;
+	 */
 	int				pin_count;
 #ifdef CONFIG_CGROUP_PERF
 	int				nr_cgroups;	 /* cgroup evts */
@@ -986,6 +1032,12 @@ struct perf_cpu_pmu_context {
 	raw_spinlock_t			hrtimer_lock;
 	struct hrtimer			hrtimer;
 	ktime_t				hrtimer_interval;
+	/*
+	 * 在以下使用perf_cpu_pmu_context->hrtimer_active:
+	 *   - kernel/events/core.c|1396| <<perf_mux_hrtimer_handler>> cpc->hrtimer_active = 0;
+	 *   - kernel/events/core.c|1433| <<perf_mux_hrtimer_restart>> if (!cpc->hrtimer_active) {
+	 *   - kernel/events/core.c|1434| <<perf_mux_hrtimer_restart>> cpc->hrtimer_active = 1;
+	 */
 	unsigned int			hrtimer_active;
 };
 
@@ -994,6 +1046,12 @@ struct perf_cpu_pmu_context {
  */
 struct perf_cpu_context {
 	struct perf_event_context	ctx;
+	/*
+	 * 在以下设置perf_cpu_context->task_ctx:
+	 *   - kernel/events/core.c|2413| <<__perf_remove_from_context>> cpuctx->task_ctx = NULL;
+	 *   - kernel/events/core.c|3352| <<ctx_sched_out>> cpuctx->task_ctx = NULL;
+	 *   - kernel/events/core.c|3965| <<ctx_sched_in>> cpuctx->task_ctx = ctx;
+	 */
 	struct perf_event_context	*task_ctx;
 	int				online;
 
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3c2abbc58..1e0d11402 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1250,8 +1250,61 @@ struct task_struct {
 	unsigned int			futex_state;
 #endif
 #ifdef CONFIG_PERF_EVENTS
+	/*
+	 * 在以下使用task_struct->perf_event_ctxp:
+	 *   - kernel/events/core.c|1386| <<perf_lock_task_context>> ctx = rcu_dereference(task->perf_event_ctxp);
+	 *   - kernel/events/core.c|1399| <<perf_lock_task_context>> if (ctx != rcu_dereference(task->perf_event_ctxp)) {
+	 *   - kernel/events/core.c|3495| <<perf_event_context_sched_out>> struct perf_event_context *ctx = task->perf_event_ctxp;
+	 *   - kernel/events/core.c|3504| <<perf_event_context_sched_out>> next_ctx = rcu_dereference(next->perf_event_ctxp);
+	 *   - kernel/events/core.c|3558| <<perf_event_context_sched_out>> RCU_INIT_POINTER(task->perf_event_ctxp, next_ctx);
+	 *   - kernel/events/core.c|3559| <<perf_event_context_sched_out>> RCU_INIT_POINTER(next->perf_event_ctxp, ctx);
+	 *   - kernel/events/core.c|3946| <<perf_event_context_sched_in>> ctx = rcu_dereference(task->perf_event_ctxp);
+	 *   - kernel/events/core.c|4338| <<perf_event_task_tick>> ctx = rcu_dereference(current->perf_event_ctxp);
+	 *   - kernel/events/core.c|4373| <<perf_event_enable_on_exec>> if (WARN_ON_ONCE(current->perf_event_ctxp != ctx))
+	 *   - kernel/events/core.c|4824| <<find_get_context>> else if (task->perf_event_ctxp)
+	 *   - kernel/events/core.c|4829| <<find_get_context>> rcu_assign_pointer(task->perf_event_ctxp, ctx);
+	 *   - kernel/events/core.c|8023| <<perf_iterate_sb>> ctx = rcu_dereference(current->perf_event_ctxp);
+	 *   - kernel/events/core.c|8904| <<perf_addr_filters_adjust>> ctx = rcu_dereference(current->perf_event_ctxp);
+	 *   - kernel/events/core.c|10253| <<perf_tp_event>> ctx = rcu_dereference(task->perf_event_ctxp);
+	 *   - kernel/events/core.c|13133| <<perf_event_exit_task_context>> RCU_INIT_POINTER(child->perf_event_ctxp, NULL);
+	 *   - kernel/events/core.c|13227| <<perf_event_free_task>> ctx = rcu_access_pointer(task->perf_event_ctxp);
+	 *   - kernel/events/core.c|13239| <<perf_event_free_task>> RCU_INIT_POINTER(task->perf_event_ctxp, NULL);
+	 *   - kernel/events/core.c|13270| <<perf_event_delayed_put>> WARN_ON_ONCE(task->perf_event_ctxp);
+	 *   - kernel/events/core.c|13485| <<inherit_task_group>> child_ctx = child->perf_event_ctxp;
+	 *   - kernel/events/core.c|13497| <<inherit_task_group>> child->perf_event_ctxp = child_ctx;
+	 *   - kernel/events/core.c|13520| <<perf_event_init_context>> if (likely(!parent->perf_event_ctxp))
+	 *   - kernel/events/core.c|13574| <<perf_event_init_context>> child_ctx = child->perf_event_ctxp;
+	 *   - kernel/events/core.c|13612| <<perf_event_init_task>> child->perf_event_ctxp = NULL;
+	 */
 	struct perf_event_context	*perf_event_ctxp;
+	/*
+	 * 在以下使用task_struct->perf_event_mutex:
+	 *   - init/init_task.c|137| <<global>> .perf_event_mutex = __MUTEX_INITIALIZER(init_task.perf_event_mutex),
+	 *   - kernel/events/core.c|4817| <<find_get_context>> mutex_lock(&task->perf_event_mutex);
+	 *   - kernel/events/core.c|4831| <<find_get_context>> mutex_unlock(&task->perf_event_mutex);
+	 *   - kernel/events/core.c|5298| <<perf_remove_from_owner>> mutex_lock_nested(&owner->perf_event_mutex, SINGLE_DEPTH_NESTING);
+	 *   - kernel/events/core.c|5310| <<perf_remove_from_owner>> mutex_unlock(&owner->perf_event_mutex);
+	 *   - kernel/events/core.c|6034| <<perf_event_task_enable>> mutex_lock(&current->perf_event_mutex);
+	 *   - kernel/events/core.c|6040| <<perf_event_task_enable>> mutex_unlock(&current->perf_event_mutex);
+	 *   - kernel/events/core.c|6050| <<perf_event_task_disable>> mutex_lock(&current->perf_event_mutex);
+	 *   - kernel/events/core.c|6056| <<perf_event_task_disable>> mutex_unlock(&current->perf_event_mutex);
+	 *   - kernel/events/core.c|12774| <<SYSCALL_DEFINE5(perf_event_open)>> mutex_lock(&current->perf_event_mutex);
+	 *   - kernel/events/core.c|12776| <<SYSCALL_DEFINE5(perf_event_open)>> mutex_unlock(&current->perf_event_mutex);
+	 *   - kernel/events/core.c|13169| <<perf_event_exit_task>> mutex_lock(&child->perf_event_mutex);
+	 *   - kernel/events/core.c|13181| <<perf_event_exit_task>> mutex_unlock(&child->perf_event_mutex);
+	 *   - kernel/events/core.c|13613| <<perf_event_init_task>> mutex_init(&child->perf_event_mutex);
+	 *   - kernel/events/hw_breakpoint.c|112| <<get_task_bps_mutex>> return tsk ? &tsk->perf_event_mutex : NULL;
+	 */
 	struct mutex			perf_event_mutex;
+	/*
+	 * 在以下使用task_struct->perf_event_list:
+	 *   - init/init_task.c|138| <<global>> .perf_event_list = LIST_HEAD_INIT(init_task.perf_event_list),
+	 *   - kernel/events/core.c|6035| <<perf_event_task_enable>> list_for_each_entry(event, &current->perf_event_list, owner_entry) {
+	 *   - kernel/events/core.c|6051| <<perf_event_task_disable>> list_for_each_entry(event, &current->perf_event_list, owner_entry) {
+	 *   - kernel/events/core.c|12775| <<SYSCALL_DEFINE5(perf_event_open)>> list_add_tail(&event->owner_entry, &current->perf_event_list);
+	 *   - kernel/events/core.c|13170| <<perf_event_exit_task>> list_for_each_entry_safe(event, tmp, &child->perf_event_list,
+	 *   - kernel/events/core.c|13614| <<perf_event_init_task>> INIT_LIST_HEAD(&child->perf_event_list);
+	 */
 	struct list_head		perf_event_list;
 #endif
 #ifdef CONFIG_DEBUG_PREEMPT
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 4ff48eda3..73a9fe99b 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -2952,6 +2952,11 @@ static inline void skb_set_transport_header(struct sk_buff *skb,
 
 static inline unsigned char *skb_network_header(const struct sk_buff *skb)
 {
+	/*
+	 * 其实算是:
+	 * skb->head + skb->data - skb->head + ETH_HLEN
+	 * 也就是skb->data + ETH_HLEN
+	 */
 	return skb->head + skb->network_header;
 }
 
diff --git a/include/linux/uio.h b/include/linux/uio.h
index 00cebe2b7..1a76c7301 100644
--- a/include/linux/uio.h
+++ b/include/linux/uio.h
@@ -148,6 +148,9 @@ static inline bool user_backed_iter(const struct iov_iter *i)
  * segment lengths have been validated.  Because the individual lengths can
  * overflow a size_t when added together.
  */
+/*
+ * 把所有的iov[seg].iov_len加起来
+ */
 static inline size_t iov_length(const struct iovec *iov, unsigned long nr_segs)
 {
 	unsigned long seg;
@@ -197,6 +200,53 @@ size_t copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - crypto/testmgr.c|646| <<build_test_sglist>> copied = copy_from_iter(addr, copy_len, data);
+ *   - drivers/char/random.c|1409| <<write_pool_user>> copied = copy_from_iter(block, sizeof(block), iter);
+ *   - drivers/tty/tty_io.c|1018| <<iterate_tty_write>> if (copy_from_iter(tty->write_buf, size, from) != size)
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|382| <<vduse_dev_write_iter>> ret = copy_from_iter(&resp, sizeof(resp), from);
+ *   - drivers/vhost/net.c|1000| <<vhost_net_build_xdp>> copied = copy_from_iter(buf + offsetof(struct tun_xdp_hdr, gso),
+ *   - drivers/vhost/net.c|1028| <<vhost_net_build_xdp>> copied = copy_from_iter(buf + pad, len, from);
+ *   - drivers/vhost/vhost.c|1214| <<vhost_copy_from_user>> ret = copy_from_iter(to, size, &f);
+ *   - drivers/vhost/vhost.c|1484| <<vhost_chr_write_iter>> ret = copy_from_iter(&type, sizeof(type), from);
+ *   - drivers/vhost/vhost.c|1500| <<vhost_chr_write_iter>> ret = copy_from_iter(&asid, sizeof(asid), from);
+ *   - drivers/vhost/vhost.c|1515| <<vhost_chr_write_iter>> ret = copy_from_iter(&msg, sizeof(msg), from);
+ *   - drivers/vhost/vringh.c|1211| <<copy_from_iotlb>> ret = copy_from_iter(dst, translated, &iter);
+ *   - drivers/vhost/vsock.c|355| <<vhost_vsock_alloc_skb>> nbytes = copy_from_iter(hdr, sizeof(*hdr), &iov_iter);
+ *   - drivers/vhost/vsock.c|378| <<vhost_vsock_alloc_skb>> nbytes = copy_from_iter(skb->data, payload_len, &iov_iter);
+ *   - drivers/xen/pvcalls-front.c|519| <<__write_ring>> len = copy_from_iter(data->out + masked_prod, len, msg_iter);
+ *   - drivers/xen/pvcalls-front.c|522| <<__write_ring>> int ret = copy_from_iter(data->out + masked_prod,
+ *   - drivers/xen/pvcalls-front.c|528| <<__write_ring>> len = ret + copy_from_iter(data->out, len - ret, msg_iter);
+ *   - drivers/xen/pvcalls-front.c|530| <<__write_ring>> len = copy_from_iter(data->out + masked_prod, len, msg_iter);
+ *   - fs/btrfs/inode.c|10423| <<btrfs_do_encoded_write>> if (copy_from_iter(kaddr, bytes, from) != bytes) {
+ *   - fs/configfs/file.c|191| <<fill_write_buffer>> copied = copy_from_iter(buffer->page, SIMPLE_ATTR_SIZE - 1, from);
+ *   - fs/configfs/file.c|282| <<configfs_bin_write_iter>> len = copy_from_iter(buffer->bin_buffer + iocb->ki_pos,
+ *   - fs/iomap/direct-io.c|473| <<iomap_dio_inline_iter>> copied = copy_from_iter(inline_data, length, iter);
+ *   - fs/kernfs/file.c|315| <<kernfs_fop_write_iter>> if (copy_from_iter(buf, len, iter) != len) {
+ *   - include/linux/uio.h|211| <<copy_from_iter_full>> size_t copied = copy_from_iter(addr, bytes, i);
+ *   - kernel/trace/trace_events_user.c|2177| <<user_events_write_core>> if (unlikely(copy_from_iter(&idx, sizeof(idx), i) != sizeof(idx)))
+ *   - lib/kunit_iov_iter.c|172| <<iov_kunit_copy_from_kvec>> copied = copy_from_iter(scratch, size, &iter);
+ *   - lib/kunit_iov_iter.c|335| <<iov_kunit_copy_from_bvec>> copied = copy_from_iter(scratch, size, &iter);
+ *   - lib/kunit_iov_iter.c|492| <<iov_kunit_copy_from_xarray>> copied = copy_from_iter(scratch + i, size, &iter);
+ *   - net/core/datagram.c|585| <<skb_copy_datagram_from_iter>> if (copy_from_iter(skb->data + offset, copy, from) != copy)
+ *   - net/core/skmsg.c|394| <<sk_msg_memcopy_from_iter>> ret = copy_from_iter(to, copy, from);
+ *   - net/tipc/msg.c|235| <<tipc_msg_append>> if (cpy != copy_from_iter(skb->data + mlen, cpy, &m->msg_iter))
+ *   - net/tls/tls_device.c|403| <<tls_device_copy_data>> if (copy_from_iter(addr, pre_copy, i) != pre_copy)
+ *   - net/tls/tls_device.c|415| <<tls_device_copy_data>> if (bytes && copy_from_iter(addr, bytes, i) != bytes)
+ *   - sound/core/memory.c|99| <<copy_from_iter_toio>> return copy_from_iter((void __force *)dst, count, src) == count ? 0 : -EFAULT;
+ *   - sound/core/memory.c|106| <<copy_from_iter_toio>> if (copy_from_iter(buf, c, src) != c)
+ *   - sound/core/pcm_lib.c|2025| <<default_write_copy>> if (copy_from_iter(get_dma_ptr(substream->runtime, channel, hwoff),
+ *   - sound/isa/gus/gus_pcm.c|382| <<snd_gf1_pcm_playback_copy>> if (copy_from_iter(runtime->dma_area + bpos, len, src) != len)
+ *   - sound/isa/sb/emu8000_pcm.c|416| <<GET_VAL>> else if (copy_from_iter(&sval, 2, iter) != 2) \
+ *   - sound/pci/korg1212/korg1212.c|1347| <<snd_korg1212_copy_from>> if (copy_from_iter(dst, size, src) != size)
+ *   - sound/pci/rme9652/hdsp.c|3975| <<snd_hdsp_playback_copy>> if (copy_from_iter(channel_buf + pos, count, src) != count)
+ *   - sound/pci/rme9652/rme9652.c|1860| <<snd_rme9652_playback_copy>> if (copy_from_iter(channel_buf + pos, count, src) != count)
+ *   - sound/soc/mediatek/common/mtk-btcvsd.c|834| <<mtk_btcvsd_snd_write>> if (copy_from_iter(bt->tx_packet_buf + cur_write_idx,
+ *   - sound/soc/qcom/lpass-platform.c|1237| <<lpass_platform_copy>> if (copy_from_iter((void __force *)dma_buf, bytes, buf) != bytes)
+ *   - sound/soc/soc-generic-dmaengine-pcm.c|305| <<dmaengine_copy>> if (copy_from_iter(dma_ptr, bytes, iter) != bytes)
+ *   - sound/xen/xen_snd_front_alsa.c|614| <<alsa_pb_copy>> if (copy_from_iter(stream->buffer + pos, count, src) != count)
+ */
 static __always_inline __must_check
 size_t copy_from_iter(void *addr, size_t bytes, struct iov_iter *i)
 {
diff --git a/include/linux/virtio_net.h b/include/linux/virtio_net.h
index 4dfa9b69c..2e5486759 100644
--- a/include/linux/virtio_net.h
+++ b/include/linux/virtio_net.h
@@ -191,6 +191,15 @@ static inline int virtio_net_hdr_to_skb(struct sk_buff *skb,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/um/drivers/vector_transports.c|119| <<raw_form_header>> virtio_net_hdr_from_skb(skb, vheader, virtio_legacy_is_little_endian(), false, 0);
+ *   - drivers/net/tap.c|865| <<tap_put_user>> if (virtio_net_hdr_from_skb(skb, &vnet_hdr, tap_is_little_endian(q), true, vlan_hlen))
+ *   - drivers/net/tun.c|2124| <<tun_put_user>> if (virtio_net_hdr_from_skb(skb, &gso, tun_is_little_endian(tun), true, vlan_hlen)) {
+ *   - drivers/net/virtio_net.c|2384| <<xmit_skb>> if (virtio_net_hdr_from_skb(skb, &hdr->hdr, virtio_is_little_endian(vi->vdev), false, 0))
+ *   - net/packet/af_packet.c|2103| <<packet_rcv_vnet>> if (virtio_net_hdr_from_skb(skb, (struct virtio_net_hdr *)&vnet_hdr, vio_le(), true, 0))
+ *   - net/packet/af_packet.c|2369| <<tpacket_rcv>> virtio_net_hdr_from_skb(skb, h.raw + macoff - sizeof(struct virtio_net_hdr), vio_le(), true, 0)) {
+ */
 static inline int virtio_net_hdr_from_skb(const struct sk_buff *skb,
 					  struct virtio_net_hdr *hdr,
 					  bool little_endian,
diff --git a/include/uapi/linux/if_tun.h b/include/uapi/linux/if_tun.h
index 287cdc81c..9cec0662e 100644
--- a/include/uapi/linux/if_tun.h
+++ b/include/uapi/linux/if_tun.h
@@ -72,6 +72,24 @@
 #define IFF_NO_PI	0x1000
 /* This flag has no real effect */
 #define IFF_ONE_QUEUE	0x2000
+/*
+ * 内核在以下使用IFF_VNET_HDR:
+ *   - arch/um/drivers/vector_user.c|143| <<create_tap_fd>> ifr.ifr_flags = IFF_TAP | IFF_NO_PI | IFF_VNET_HDR;
+ *   - arch/um/drivers/vector_user.c|512| <<uml_tap_enable_vnet_headers>> if ((features & IFF_VNET_HDR) == 0) {
+ *   - drivers/net/tap.c|29| <<TAP_IFFEATURES>> #define TAP_IFFEATURES (IFF_VNET_HDR | IFF_MULTI_QUEUE)
+ *   - drivers/net/tap.c|343| <<tap_handle_frame>> if (q->flags & IFF_VNET_HDR)
+ *   - drivers/net/tap.c|541| <<tap_open>> q->flags = IFF_VNET_HDR | IFF_NO_PI | IFF_TAP;
+ *   - drivers/net/tap.c|663| <<tap_get_user>> if (q->flags & IFF_VNET_HDR) {
+ *   - drivers/net/tap.c|828| <<tap_put_user>> if (q->flags & IFF_VNET_HDR) {
+ *   - drivers/net/tap.c|1229| <<tap_get_user_xdp>> if (q->flags & IFF_VNET_HDR)
+ *   - drivers/net/tun.c|101| <<TUN_FEATURES>> #define TUN_FEATURES (IFF_NO_PI | IFF_ONE_QUEUE | IFF_VNET_HDR | \
+ *   - drivers/net/tun.c|1770| <<tun_get_user>> if (tun->flags & IFF_VNET_HDR) {
+ *   - drivers/net/tun.c|2063| <<tun_put_user_xdp>> if (tun->flags & IFF_VNET_HDR) {
+ *   - drivers/net/tun.c|2099| <<tun_put_user>> if (tun->flags & IFF_VNET_HDR)
+ *   - drivers/net/tun.c|2381| <<tun_fill_info>> if (nla_put_u8(skb, IFLA_TUN_VNET_HDR, !!(tun->flags & IFF_VNET_HDR)))
+ *   - tools/testing/selftests/net/tap.c|215| <<opentap>> ifr.ifr_flags = IFF_TAP | IFF_NO_PI | IFF_VNET_HDR | IFF_MULTI_QUEUE;
+ *   - tools/virtio/vhost_net_test.c|74| <<tun_alloc>> ifr.ifr_flags = IFF_TAP | IFF_NO_PI | IFF_VNET_HDR;
+ */
 #define IFF_VNET_HDR	0x4000
 #define IFF_TUN_EXCL	0x8000
 #define IFF_MULTI_QUEUE 0x0100
diff --git a/include/uapi/linux/pci_regs.h b/include/uapi/linux/pci_regs.h
index a39193213..870e18937 100644
--- a/include/uapi/linux/pci_regs.h
+++ b/include/uapi/linux/pci_regs.h
@@ -610,6 +610,11 @@
 #define  PCI_EXP_SLTCTL_ATTN_IND_OFF   0x00c0 /* Attention Indicator off */
 #define  PCI_EXP_SLTCTL_PIC	0x0300	/* Power Indicator Control */
 #define  PCI_EXP_SLTCTL_PWR_IND_ON     0x0100 /* Power Indicator on */
+/*
+ * 在以下使用PCI_EXP_SLTCTL_PWR_IND_BLINK:
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|71| <<board_added>> pciehp_set_indicators(ctrl, PCI_EXP_SLTCTL_PWR_IND_BLINK,
+  4 drivers/pci/hotplug/pciehp_ctrl.c|177| <<pciehp_handle_button_press>> pciehp_set_indicators(ctrl, PCI_EXP_SLTCTL_PWR_IND_BLINK,
+ */
 #define  PCI_EXP_SLTCTL_PWR_IND_BLINK  0x0200 /* Power Indicator blinking */
 #define  PCI_EXP_SLTCTL_PWR_IND_OFF    0x0300 /* Power Indicator off */
 #define  PCI_EXP_SLTCTL_PCC	0x0400	/* Power Controller Control */
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 724e6d7e1..381b46c4e 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -69,6 +69,11 @@ struct remote_function_call {
 	int			ret;
 };
 
+/*
+ * 在以下使用remote_function():
+ *   - kernel/events/core.c|120| <<task_function_call>> ret = smp_call_function_single(task_cpu(p), remote_function, &data, 1);
+ *   - kernel/events/core.c|153| <<cpu_function_call>> smp_call_function_single(cpu, remote_function, &data, 1);
+ */
 static void remote_function(void *data)
 {
 	struct remote_function_call *tfc = data;
@@ -105,6 +110,12 @@ static void remote_function(void *data)
  *
  * returns @func return value or -ESRCH or -ENXIO when the process isn't running
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|291| <<event_function_call>> if (!task_function_call(task, event_function, &efs))
+ *   - kernel/events/core.c|2909| <<perf_install_in_context>> if (!task_function_call(task, __perf_install_in_context, event))
+ *   - kernel/events/core.c|13869| <<perf_cgroup_attach>> task_function_call(task, __perf_cgroup_move, task);
+ */
 static int
 task_function_call(struct task_struct *p, remote_function_f func, void *info)
 {
@@ -141,6 +152,14 @@ task_function_call(struct task_struct *p, remote_function_f func, void *info)
  *
  * returns: @func return value or -ENXIO when the cpu is offline
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|283| <<event_function_call>> cpu_function_call(event->cpu, event_function, &efs);
+ *   - kernel/events/core.c|2867| <<perf_install_in_context>> cpu_function_call(cpu, __perf_install_in_context, event);
+ *   - kernel/events/core.c|3097| <<perf_event_stop>> ret = cpu_function_call(READ_ONCE(event->oncpu), __perf_event_stop, &sd);
+ *   - kernel/events/core.c|8153| <<perf_pmu_output_stop>> err = cpu_function_call(cpu, __perf_pmu_output_stop, event);
+ *   - kernel/events/core.c|11420| <<perf_event_mux_interval_ms_store>> cpu_function_call(cpu, perf_mux_hrtimer_restart_ipi, cpc);
+ */
 static int cpu_function_call(int cpu, remote_function_f func, void *info)
 {
 	struct remote_function_call data = {
@@ -155,22 +174,87 @@ static int cpu_function_call(int cpu, remote_function_f func, void *info)
 	return data.ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|247| <<event_function>> perf_ctx_lock(cpuctx, task_ctx);
+ *   - kernel/events/core.c|351| <<event_function_local>> perf_ctx_lock(cpuctx, task_ctx);
+ *   - kernel/events/core.c|884| <<perf_cgroup_switch>> perf_ctx_lock(cpuctx, cpuctx->task_ctx);
+ *   - kernel/events/core.c|2771| <<perf_pmu_resched>> perf_ctx_lock(cpuctx, task_ctx);
+ *   - kernel/events/core.c|3652| <<__perf_pmu_sched_task>> perf_ctx_lock(cpuctx, cpuctx->task_ctx);
+ *   - kernel/events/core.c|3995| <<perf_event_context_sched_in>> perf_ctx_lock(cpuctx, ctx);
+ *   - kernel/events/core.c|4005| <<perf_event_context_sched_in>> perf_ctx_lock(cpuctx, ctx);
+ *   - kernel/events/core.c|4331| <<perf_rotate_context>> perf_ctx_lock(cpuctx, cpuctx->task_ctx);
+ *   - kernel/events/core.c|4424| <<perf_event_enable_on_exec>> perf_ctx_lock(cpuctx, ctx);
+ */
 static void perf_ctx_lock(struct perf_cpu_context *cpuctx,
 			  struct perf_event_context *ctx)
 {
+	/*
+	 * struct task_struct:
+	 * -> struct perf_event_context *perf_event_ctxp;
+	 * -> struct mutex perf_event_mutex;
+	 * -> struct list_head perf_event_list;
+	 *
+	 * struct perf_cpu_context {
+	 * -> struct perf_event_context ctx;
+	 * -> struct perf_event_context *task_ctx;
+	 */
 	raw_spin_lock(&cpuctx->ctx.lock);
 	if (ctx)
 		raw_spin_lock(&ctx->lock);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|277| <<event_function>> perf_ctx_unlock(cpuctx, task_ctx);
+ *   - kernel/events/core.c|376| <<event_function_local>> perf_ctx_unlock(cpuctx, task_ctx);
+ *   - kernel/events/core.c|902| <<perf_cgroup_switch>> perf_ctx_unlock(cpuctx, cpuctx->task_ctx);
+ *   - kernel/events/core.c|2773| <<perf_pmu_resched>> perf_ctx_unlock(cpuctx, task_ctx);
+ *   - kernel/events/core.c|2836| <<__perf_install_in_context>> perf_ctx_unlock(cpuctx, task_ctx);
+ *   - kernel/events/core.c|3658| <<__perf_pmu_sched_task>> perf_ctx_unlock(cpuctx, cpuctx->task_ctx);
+ *   - kernel/events/core.c|4001| <<perf_event_context_sched_in>> perf_ctx_unlock(cpuctx, ctx);
+ *   - kernel/events/core.c|4037| <<perf_event_context_sched_in>> perf_ctx_unlock(cpuctx, ctx);
+ *   - kernel/events/core.c|4362| <<perf_rotate_context>> perf_ctx_unlock(cpuctx, cpuctx->task_ctx);
+ *   - kernel/events/core.c|4441| <<perf_event_enable_on_exec>> perf_ctx_unlock(cpuctx, ctx);
+ */
 static void perf_ctx_unlock(struct perf_cpu_context *cpuctx,
 			    struct perf_event_context *ctx)
 {
+	/*
+	 * struct task_struct:
+	 * -> struct perf_event_context *perf_event_ctxp;
+	 * -> struct mutex perf_event_mutex;
+	 * -> struct list_head perf_event_list;
+	 *
+	 * struct perf_cpu_context {
+	 * -> struct perf_event_context ctx;
+	 * -> struct perf_event_context *task_ctx;
+	 */
 	if (ctx)
 		raw_spin_unlock(&ctx->lock);
 	raw_spin_unlock(&cpuctx->ctx.lock);
 }
 
+/*
+ * 在以下使用TASK_TOMBSTONE:
+ *   - kernel/events/core.c|197| <<is_kernel_event>> return READ_ONCE(event->owner) == TASK_TOMBSTONE;
+ *   - kernel/events/core.c|306| <<event_function_call>> if (task == TASK_TOMBSTONE)
+ *   - kernel/events/core.c|319| <<event_function_call>> if (task == TASK_TOMBSTONE) {
+ *   - kernel/events/core.c|345| <<event_function_local>> if (task == TASK_TOMBSTONE)
+ *   - kernel/events/core.c|354| <<event_function_local>> if (task == TASK_TOMBSTONE)
+ *   - kernel/events/core.c|1217| <<put_ctx>> if (ctx->task && ctx->task != TASK_TOMBSTONE)
+ *   - kernel/events/core.c|1429| <<perf_lock_task_context>> if (ctx->task == TASK_TOMBSTONE ||
+ *   - kernel/events/core.c|2880| <<perf_install_in_context>> if (ctx->task == TASK_TOMBSTONE) {
+ *   - kernel/events/core.c|2897| <<perf_install_in_context>> if (WARN_ON_ONCE(task == TASK_TOMBSTONE))
+ *   - kernel/events/core.c|2937| <<perf_install_in_context>> if (WARN_ON_ONCE(task == TASK_TOMBSTONE)) {
+ *   - kernel/events/core.c|10776| <<perf_event_addr_filters_apply>> if (task == TASK_TOMBSTONE)
+ *   - kernel/events/core.c|12645| <<SYSCALL_DEFINE5(perf_event_open)>> if (ctx->task == TASK_TOMBSTONE) {
+ *   - kernel/events/core.c|12910| <<perf_event_create_kernel_counter>> event->owner = TASK_TOMBSTONE;
+ *   - kernel/events/core.c|12927| <<perf_event_create_kernel_counter>> if (ctx->task == TASK_TOMBSTONE) {
+ *   - kernel/events/core.c|13096| <<sync_child_event>> if (task && task != TASK_TOMBSTONE)
+ *   - kernel/events/core.c|13199| <<perf_event_exit_task_context>> WRITE_ONCE(child_ctx->task, TASK_TOMBSTONE);
+ *   - kernel/events/core.c|13304| <<perf_event_free_task>> WRITE_ONCE(ctx->task, TASK_TOMBSTONE);
+ */
 #define TASK_TOMBSTONE ((void *)-1L)
 
 static bool is_kernel_event(struct perf_event *event)
@@ -178,10 +262,35 @@ static bool is_kernel_event(struct perf_event *event)
 	return READ_ONCE(event->owner) == TASK_TOMBSTONE;
 }
 
+/*
+ * struct task_struct:
+ * -> struct perf_event_context *perf_event_ctxp;
+ * -> struct mutex perf_event_mutex;
+ * -> struct list_head perf_event_list;
+ *
+ * struct perf_cpu_context {
+ * -> struct perf_event_context ctx;
+ * -> struct perf_event_context *task_ctx;
+ */
 static DEFINE_PER_CPU(struct perf_cpu_context, perf_cpu_context);
 
 struct perf_event_context *perf_cpu_task_ctx(void)
 {
+	/*
+	 * struct task_struct:
+	 * -> struct perf_event_context *perf_event_ctxp;
+	 * -> struct mutex perf_event_mutex;
+	 * -> struct list_head perf_event_list;
+	 *
+	 * struct perf_cpu_context {
+	 * -> struct perf_event_context ctx;
+	 * -> struct perf_event_context *task_ctx;
+	 *
+	 * 在以下设置perf_cpu_context->task_ctx:
+	 *   - kernel/events/core.c|2413| <<__perf_remove_from_context>> cpuctx->task_ctx = NULL;
+	 *   - kernel/events/core.c|3352| <<ctx_sched_out>> cpuctx->task_ctx = NULL;
+	 *   - kernel/events/core.c|3965| <<ctx_sched_in>> cpuctx->task_ctx = ctx;
+	 */
 	lockdep_assert_irqs_disabled();
 	return this_cpu_ptr(&perf_cpu_context)->task_ctx;
 }
@@ -214,10 +323,25 @@ struct event_function_struct {
 	void *data;
 };
 
+/*
+ * 在以下使用event_function():
+ *   - kernel/events/core.c|302| <<event_function_call>> cpu_function_call(event->cpu, event_function, &efs);
+ *   - kernel/events/core.c|310| <<event_function_call>> if (!task_function_call(task, event_function, &efs))
+ */
 static int event_function(void *info)
 {
 	struct event_function_struct *efs = info;
 	struct perf_event *event = efs->event;
+	/*
+	 * struct task_struct:
+	 * -> struct perf_event_context *perf_event_ctxp;
+	 * -> struct mutex perf_event_mutex;
+	 * -> struct list_head perf_event_list;
+	 *
+	 * struct perf_cpu_context {
+	 * -> struct perf_event_context ctx;
+	 * -> struct perf_event_context *task_ctx;
+	 */
 	struct perf_event_context *ctx = event->ctx;
 	struct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);
 	struct perf_event_context *task_ctx = cpuctx->task_ctx;
@@ -260,8 +384,26 @@ static int event_function(void *info)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2448| <<perf_remove_from_context>> event_function_call(event, __perf_remove_from_context, (void *)flags);
+ *   - kernel/events/core.c|2505| <<_perf_event_disable>> event_function_call(event, __perf_event_disable, NULL);
+ *   - kernel/events/core.c|3043| <<_perf_event_enable>> event_function_call(event, __perf_event_enable, NULL);
+ *   - kernel/events/core.c|5898| <<_perf_event_period>> event_function_call(event, __perf_event_period, &value);
+ *
+ * 远程指挥一个cpu/task调用一些函数:
+ * - __perf_remove_from_context()
+ * - __perf_event_disable()
+ * - __perf_event_enable()
+ * - __perf_event_period()
+ */
 static void event_function_call(struct perf_event *event, event_f func, void *data)
 {
+	/*
+	 * struct perf_event *event:
+	 * -> struct perf_event_context *ctx;
+	 *    -> struct task_struct *task;
+	 */
 	struct perf_event_context *ctx = event->ctx;
 	struct task_struct *task = READ_ONCE(ctx->task); /* verified in event_function */
 	struct event_function_struct efs = {
@@ -313,6 +455,10 @@ static void event_function_call(struct perf_event *event, event_f func, void *da
  * Similar to event_function_call() + event_function(), but hard assumes IRQs
  * are already disabled and we're on the right CPU.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|2625| <<perf_event_disable_local>> event_function_local(event, __perf_event_disable, NULL);
+ */
 static void event_function_local(struct perf_event *event, event_f func, void *data)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -369,6 +515,24 @@ static void event_function_local(struct perf_event *event, event_f func, void *d
 	(PERF_SAMPLE_BRANCH_KERNEL |\
 	 PERF_SAMPLE_BRANCH_HV)
 
+/*
+ * 在以下使用EVENT_TIME:
+ *   - kernel/events/core.c|1639| <<perf_event_time_now>> if (!(__load_acquire(&ctx->is_active) & EVENT_TIME))
+ *   - kernel/events/core.c|2489| <<__perf_remove_from_context>> if (ctx->is_active & EVENT_TIME) {
+ *   - kernel/events/core.c|2577| <<__perf_event_disable>> if (ctx->is_active & EVENT_TIME) {
+ *   - kernel/events/core.c|2943| <<__perf_install_in_context>> ctx_sched_out(ctx, EVENT_TIME);
+ *   - kernel/events/core.c|3089| <<__perf_event_enable>> ctx_sched_out(ctx, EVENT_TIME);
+ *   - kernel/events/core.c|3098| <<__perf_event_enable>> ctx_sched_in(ctx, EVENT_TIME);
+ *   - kernel/events/core.c|3107| <<__perf_event_enable>> ctx_sched_in(ctx, EVENT_TIME);
+ *   - kernel/events/core.c|3449| <<ctx_sched_out>> if (is_active & EVENT_TIME) {
+ *   - kernel/events/core.c|4066| <<ctx_sched_in>> if (!(is_active & EVENT_TIME)) {
+ *   - kernel/events/core.c|4077| <<ctx_sched_in>> ctx->is_active |= (event_type | EVENT_TIME);
+ *   - kernel/events/core.c|4540| <<perf_event_enable_on_exec>> ctx_sched_out(ctx, EVENT_TIME);
+ *   - kernel/events/core.c|4554| <<perf_event_enable_on_exec>> ctx_sched_in(ctx, EVENT_TIME);
+ *   - kernel/events/core.c|4657| <<__perf_event_read>> if (ctx->is_active & EVENT_TIME) {
+ *   - kernel/events/core.c|4857| <<perf_event_read>> if (ctx->is_active & EVENT_TIME) {
+ *   - kernel/events/core.c|13854| <<__perf_event_exit_context>> ctx_sched_out(ctx, EVENT_TIME);
+ */
 enum event_type_t {
 	EVENT_FLEXIBLE = 0x1,
 	EVENT_PINNED = 0x2,
@@ -387,10 +551,25 @@ static void perf_sched_delayed(struct work_struct *work);
 DEFINE_STATIC_KEY_FALSE(perf_sched_events);
 static DECLARE_DELAYED_WORK(perf_sched_work, perf_sched_delayed);
 static DEFINE_MUTEX(perf_sched_mutex);
+/*
+ * 在以下使用perf_sched_count:
+ *   - kernel/events/core.c|5265| <<unaccount_event>> if (!atomic_add_unless(&perf_sched_count, -1, 1))
+ *   - kernel/events/core.c|5275| <<perf_sched_delayed>> if (atomic_dec_and_test(&perf_sched_count))
+ *   - kernel/events/core.c|12034| <<account_event>> if (atomic_inc_not_zero(&perf_sched_count))
+ *   - kernel/events/core.c|12038| <<account_event>> if (!atomic_read(&perf_sched_count)) {
+ *   - kernel/events/core.c|12051| <<account_event>> atomic_inc(&perf_sched_count);
+ */
 static atomic_t perf_sched_count;
 
 static DEFINE_PER_CPU(struct pmu_event_list, pmu_sb_events);
 
+/*
+ * 在以下使用nr_mmap_events:
+ *   - kernel/events/core.c|5236| <<unaccount_event>> atomic_dec(&nr_mmap_events);
+ *   - kernel/events/core.c|8419| <<perf_event_task>> !atomic_read(&nr_mmap_events) &&
+ *   - kernel/events/core.c|9093| <<perf_event_mmap>> if (!atomic_read(&nr_mmap_events))
+ *   - kernel/events/core.c|12000| <<account_event>> atomic_inc(&nr_mmap_events);
+ */
 static atomic_t nr_mmap_events __read_mostly;
 static atomic_t nr_comm_events __read_mostly;
 static atomic_t nr_namespaces_events __read_mostly;
@@ -403,9 +582,25 @@ static atomic_t nr_cgroup_events __read_mostly;
 static atomic_t nr_text_poke_events __read_mostly;
 static atomic_t nr_build_id_events __read_mostly;
 
+/*
+ * 在以下使用pmus链表:
+ *   - kernel/events/core.c|540| <<global>> static LIST_HEAD(pmus);
+ *   - kernel/events/core.c|11768| <<perf_pmu_register>> list_add_rcu(&pmu->entry, &pmus);
+ *   - kernel/events/core.c|11929| <<perf_init_event>> list_for_each_entry_rcu(pmu, &pmus, entry, lockdep_is_held(&pmus_srcu)) {
+ *   - kernel/events/core.c|13981| <<perf_event_sysfs_init>> list_for_each_entry(pmu, &pmus, entry) {
+ */
 static LIST_HEAD(pmus);
 static DEFINE_MUTEX(pmus_lock);
 static struct srcu_struct pmus_srcu;
+/*
+ * 在以下使用perf_online_mask:
+ *   - kernel/events/core.c|543| <<global>> static cpumask_var_t perf_online_mask;
+ *   - kernel/events/core.c|10150| <<swevent_hlist_get_cpu>> cpumask_test_cpu(cpu, perf_online_mask)) {
+ *   - kernel/events/core.c|13810| <<perf_event_init_all_cpus>> zalloc_cpumask_var(&perf_online_mask, GFP_KERNEL);
+ *   - kernel/events/core.c|13825| <<perf_event_init_all_cpus>> cpuctx->online = cpumask_test_cpu(cpu, perf_online_mask);
+ *   - kernel/events/core.c|13874| <<perf_event_exit_cpu_context>> cpumask_clear_cpu(cpu, perf_online_mask);
+ *   - kernel/events/core.c|13891| <<perf_event_init_cpu>> cpumask_set_cpu(cpu, perf_online_mask);
+ */
 static cpumask_var_t perf_online_mask;
 static struct kmem_cache *perf_event_cache;
 
@@ -424,6 +619,12 @@ int sysctl_perf_event_mlock __read_mostly = 512 + (PAGE_SIZE / 1024); /* 'free'
 /*
  * max perf event sample rate
  */
+/*
+ * 在以下使用DEFAULT_MAX_SAMPLE_RATE:
+ *   - kernel/events/core.c|565| <<global>> int sysctl_perf_event_sample_rate __read_mostly = DEFAULT_MAX_SAMPLE_RATE;
+ *   - kernel/events/core.c|567| <<global>> static int max_samples_per_tick __read_mostly = DIV_ROUND_UP(DEFAULT_MAX_SAMPLE_RATE, HZ);
+ *   - kernel/events/core.c|562| <<DEFAULT_SAMPLE_PERIOD_NS>> #define DEFAULT_SAMPLE_PERIOD_NS (NSEC_PER_SEC / DEFAULT_MAX_SAMPLE_RATE)
+ */
 #define DEFAULT_MAX_SAMPLE_RATE		100000
 #define DEFAULT_SAMPLE_PERIOD_NS	(NSEC_PER_SEC / DEFAULT_MAX_SAMPLE_RATE)
 #define DEFAULT_CPU_TIME_MAX_PERCENT	25
@@ -472,6 +673,17 @@ int perf_event_max_sample_rate_handler(struct ctl_table *table, int write,
 	return 0;
 }
 
+/*
+ * 在以下使用sysctl_perf_cpu_time_max_percent:
+ *   - kernel/events/core.c|609| <<global>> int sysctl_perf_cpu_time_max_percent __read_mostly = DEFAULT_CPU_TIME_MAX_PERCENT;
+ *   - kernel/sysctl.c|1982| <<global>> .data = &sysctl_perf_cpu_time_max_percent,
+ *   - kernel/sysctl.c|1983| <<global>> .maxlen = sizeof(sysctl_perf_cpu_time_max_percent),
+ *   - kernel/events/core.c|577| <<update_perf_cpu_limits>> tmp *= sysctl_perf_cpu_time_max_percent;
+ *   - kernel/events/core.c|591| <<perf_event_max_sample_rate_handler>> int perf_cpu = sysctl_perf_cpu_time_max_percent;
+ *   - kernel/events/core.c|619| <<perf_cpu_time_max_percent_handler>> if (sysctl_perf_cpu_time_max_percent == 100 ||
+ *   - kernel/events/core.c|620| <<perf_cpu_time_max_percent_handler>> sysctl_perf_cpu_time_max_percent == 0) {
+ *   - kernel/events/core.c|686| <<perf_sample_event_took>> max = (TICK_NSEC / 100) * sysctl_perf_cpu_time_max_percent;
+ */
 int sysctl_perf_cpu_time_max_percent __read_mostly = DEFAULT_CPU_TIME_MAX_PERCENT;
 
 int perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,
@@ -500,6 +712,11 @@ int perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,
  * get any real work done.  This will drop the sample rate when
  * we detect that events are taking too long.
  */
+/*
+ * 在以下使用NR_ACCUMULATED_SAMPLES:
+ *   - kernel/events/core.c|666| <<perf_sample_event_took>> running_len -= running_len/NR_ACCUMULATED_SAMPLES;
+ *   - kernel/events/core.c|675| <<perf_sample_event_took>> avg_len = running_len/NR_ACCUMULATED_SAMPLES;
+ */
 #define NR_ACCUMULATED_SAMPLES 128
 static DEFINE_PER_CPU(u64, running_sample_length);
 
@@ -517,6 +734,15 @@ static void perf_duration_warn(struct irq_work *w)
 
 static DEFINE_IRQ_WORK(perf_duration_work, perf_duration_warn);
 
+/*
+ * called by:
+ *   - arch/powerpc/perf/core-book3s.c|2491| <<perf_event_interrupt>> perf_sample_event_took(sched_clock() - start_clock);
+ *   - arch/sparc/kernel/perf_event.c|1677| <<perf_event_nmi_handler>> perf_sample_event_took(finish_clock - start_clock);
+ *   - arch/x86/events/amd/ibs.c|1170| <<perf_ibs_nmi_handler>> perf_sample_event_took(sched_clock() - stamp);
+ *   - arch/x86/events/core.c|1747| <<perf_event_nmi_handler>> perf_sample_event_took(finish_clock - start_clock);
+ *   - drivers/perf/arm_pmu.c|439| <<armpmu_dispatch_irq>> perf_sample_event_took(finish_clock - start_clock);
+ *   - drivers/perf/riscv_pmu_sbi.c|782| <<pmu_sbi_ovf_handler>> perf_sample_event_took(sched_clock() - start_clock);
+ */
 void perf_sample_event_took(u64 sample_len_ns)
 {
 	u64 max_len = READ_ONCE(perf_sample_allowed_ns);
@@ -569,6 +795,10 @@ void perf_sample_event_took(u64 sample_len_ns)
 	}
 }
 
+/*
+ * 在以下使用perf_event_id:
+ *   - kernel/events/core.c|12128| <<perf_event_alloc>> event->id = atomic64_inc_return(&perf_event_id);
+ */
 static atomic64_t perf_event_id;
 
 static void update_context_time(struct perf_event_context *ctx);
@@ -583,6 +813,10 @@ static inline u64 perf_clock(void)
 
 static inline u64 perf_event_clock(struct perf_event *event)
 {
+	/*
+	 * struct perf_event *event:
+	 * -> u64 (*clock)(void);
+	 */
 	return event->clock();
 }
 
@@ -613,18 +847,39 @@ __perf_effective_state(struct perf_event *event)
 {
 	struct perf_event *leader = event->group_leader;
 
+	/*
+	 * enum perf_event_state {
+	 *     PERF_EVENT_STATE_DEAD           = -4,
+	 *     PERF_EVENT_STATE_EXIT           = -3,
+	 *     PERF_EVENT_STATE_ERROR          = -2,
+	 *     PERF_EVENT_STATE_OFF            = -1,
+	 *     PERF_EVENT_STATE_INACTIVE       =  0,
+	 *     PERF_EVENT_STATE_ACTIVE         =  1,
+	 * };
+	 */
 	if (leader->state <= PERF_EVENT_STATE_OFF)
 		return leader->state;
 
 	return event->state;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|775| <<perf_event_update_time>> __perf_update_times(event, now, &event->total_time_enabled, &event->total_time_running);
+ *   - kernel/events/core.c|4709| <<calc_timer_values>> __perf_update_times(event, ctx_time, enabled, running);
+ */
 static __always_inline void
 __perf_update_times(struct perf_event *event, u64 now, u64 *enabled, u64 *running)
 {
 	enum perf_event_state state = __perf_effective_state(event);
 	u64 delta = now - event->tstamp;
 
+	/*
+	 * struct perf_event *event:
+	 * -> u64 total_time_enabled;
+	 * -> u64 total_time_running;
+	 * -> u64 tstamp;
+	 */
 	*enabled = event->total_time_enabled;
 	if (state >= PERF_EVENT_STATE_INACTIVE)
 		*enabled += delta;
@@ -647,10 +902,29 @@ static void perf_event_update_sibling_time(struct perf_event *leader)
 {
 	struct perf_event *sibling;
 
+	/*
+	 * struct perf_event *leader:
+	 * -> struct list_head sibling_list;
+	 * -> struct list_head active_list;
+	 */
 	for_each_sibling_event(sibling, leader)
 		perf_event_update_time(sibling);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2292| <<list_del_event>> perf_event_set_state(event, PERF_EVENT_STATE_OFF);
+ *   - kernel/events/core.c|2347| <<perf_put_aux_event>> perf_event_set_state(event, PERF_EVENT_STATE_ERROR);
+ *   - kernel/events/core.c|2410| <<perf_remove_sibling_event>> perf_event_set_state(event, PERF_EVENT_STATE_ERROR);
+ *   - kernel/events/core.c|2554| <<event_sched_out>> perf_event_set_state(event, state);
+ *   - kernel/events/core.c|2704| <<__perf_event_disable>> perf_event_set_state(event, PERF_EVENT_STATE_OFF);
+ *   - kernel/events/core.c|2789| <<event_sched_in>> perf_event_set_state(event, PERF_EVENT_STATE_ACTIVE)
+ *   - kernel/events/core.c|2806| <<event_sched_in>> perf_event_set_state(event, PERF_EVENT_STATE_INACTIVE);
+ *   - kernel/events/core.c|3206| <<__perf_event_enable>> perf_event_set_state(event, PERF_EVENT_STATE_INACTIVE);
+ *   - kernel/events/core.c|4104| <<merge_sched_in>> perf_event_set_state(event, PERF_EVENT_STATE_ERROR);
+ *   - kernel/events/core.c|4628| <<event_enable_on_exec>> perf_event_set_state(event, PERF_EVENT_STATE_INACTIVE);
+ *   - kernel/events/core.c|13378| <<perf_event_exit_event>> perf_event_set_state(event, PERF_EVENT_STATE_EXIT);
+ */
 static void
 perf_event_set_state(struct perf_event *event, enum perf_event_state state)
 {
@@ -685,6 +959,17 @@ do {									\
 	___p;								\
 })
 
+/*
+ * called by:
+ *   - kernel/events/core.c|1115| <<perf_cgroup_switch>> perf_ctx_disable(&cpuctx->ctx, true);
+ *   - kernel/events/core.c|2971| <<ctx_resched>> perf_ctx_disable(&cpuctx->ctx, false);
+ *   - kernel/events/core.c|2973| <<ctx_resched>> perf_ctx_disable(task_ctx, false);
+ *   - kernel/events/core.c|3782| <<perf_event_context_sched_out>> perf_ctx_disable(ctx, false);
+ *   - kernel/events/core.c|3826| <<perf_event_context_sched_out>> perf_ctx_disable(ctx, false);
+ *   - kernel/events/core.c|4226| <<perf_event_context_sched_in>> perf_ctx_disable(ctx, false);
+ *   - kernel/events/core.c|4243| <<perf_event_context_sched_in>> perf_ctx_disable(ctx, false);
+ *   - kernel/events/core.c|4253| <<perf_event_context_sched_in>> perf_ctx_disable(&cpuctx->ctx, false);
+ */
 static void perf_ctx_disable(struct perf_event_context *ctx, bool cgroup)
 {
 	struct perf_event_pmu_context *pmu_ctx;
@@ -696,6 +981,17 @@ static void perf_ctx_disable(struct perf_event_context *ctx, bool cgroup)
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|1131| <<perf_cgroup_switch>> perf_ctx_enable(&cpuctx->ctx, true);
+ *   - kernel/events/core.c|2991| <<ctx_resched>> perf_ctx_enable(&cpuctx->ctx, false);
+ *   - kernel/events/core.c|2993| <<ctx_resched>> perf_ctx_enable(task_ctx, false);
+ *   - kernel/events/core.c|3802| <<perf_event_context_sched_out>> perf_ctx_enable(ctx, false);
+ *   - kernel/events/core.c|3832| <<perf_event_context_sched_out>> perf_ctx_enable(ctx, false);
+ *   - kernel/events/core.c|4230| <<perf_event_context_sched_in>> perf_ctx_enable(ctx, false);
+ *   - kernel/events/core.c|4262| <<perf_event_context_sched_in>> perf_ctx_enable(&cpuctx->ctx, false);
+ *   - kernel/events/core.c|4264| <<perf_event_context_sched_in>> perf_ctx_enable(ctx, false);
+ */
 static void perf_ctx_enable(struct perf_event_context *ctx, bool cgroup)
 {
 	struct perf_event_pmu_context *pmu_ctx;
@@ -844,6 +1140,11 @@ perf_cgroup_set_timestamp(struct perf_cpu_context *cpuctx)
 /*
  * reschedule events based on the cgroup constraint of task.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|3983| <<__perf_event_task_sched_out>> perf_cgroup_switch(next);
+ *   - kernel/events/core.c|14207| <<__perf_cgroup_move>> perf_cgroup_switch(task);
+ */
 static void perf_cgroup_switch(struct task_struct *task)
 {
 	struct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);
@@ -964,6 +1265,11 @@ static inline int perf_cgroup_connect(int fd, struct perf_event *event,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2107| <<list_add_event>> perf_cgroup_event_enable(event, ctx);
+ *   - kernel/events/core.c|3259| <<__perf_event_enable>> perf_cgroup_event_enable(event, ctx);
+ */
 static inline void
 perf_cgroup_event_enable(struct perf_event *event, struct perf_event_context *ctx)
 {
@@ -986,6 +1292,13 @@ perf_cgroup_event_enable(struct perf_event *event, struct perf_event_context *ct
 	cpuctx->cgrp = perf_cgroup_from_task(current, ctx);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2343| <<list_del_event>> perf_cgroup_event_disable(event, ctx);
+ *   - kernel/events/core.c|2587| <<event_sched_out>> perf_cgroup_event_disable(event, ctx);
+ *   - kernel/events/core.c|2757| <<__perf_event_disable>> perf_cgroup_event_disable(event, ctx);
+ *   - kernel/events/core.c|4155| <<merge_sched_in>> perf_cgroup_event_disable(event, ctx);
+ */
 static inline void
 perf_cgroup_event_disable(struct perf_event *event, struct perf_event_context *ctx)
 {
@@ -1074,10 +1387,18 @@ static void perf_cgroup_switch(struct task_struct *task)
  * set default to be dependent on timer tick just
  * like original code
  */
+/*
+ * 只在以下使用PERF_CPU_HRTIMER:
+ *   - kernel/events/core.c|1418| <<__perf_mux_hrtimer_init>> interval = pmu->hrtimer_interval_ms = PERF_CPU_HRTIMER;
+ */
 #define PERF_CPU_HRTIMER (1000 / HZ)
 /*
  * function must be called with interrupts disabled
  */
+/*
+ * 在以下使用perf_mux_hrtimer_handler():
+ *   - kernel/events/core.c|1424| <<__perf_mux_hrtimer_init>> timer->function = perf_mux_hrtimer_handler;
+ */
 static enum hrtimer_restart perf_mux_hrtimer_handler(struct hrtimer *hr)
 {
 	struct perf_cpu_pmu_context *cpc;
@@ -1089,6 +1410,14 @@ static enum hrtimer_restart perf_mux_hrtimer_handler(struct hrtimer *hr)
 	rotations = perf_rotate_context(cpc);
 
 	raw_spin_lock(&cpc->hrtimer_lock);
+	/*
+	 * 在以下使用perf_cpu_pmu_context->hrtimer_active:
+	 *   - kernel/events/core.c|1396| <<perf_mux_hrtimer_handler>> cpc->hrtimer_active = 0;
+	 *   - kernel/events/core.c|1433| <<perf_mux_hrtimer_restart>> if (!cpc->hrtimer_active) {
+	 *   - kernel/events/core.c|1434| <<perf_mux_hrtimer_restart>> cpc->hrtimer_active = 1;
+	 *
+	 * 注释forward the timer expiry so it expires after now
+	 */
 	if (rotations)
 		hrtimer_forward_now(hr, cpc->hrtimer_interval);
 	else
@@ -1098,6 +1427,10 @@ static enum hrtimer_restart perf_mux_hrtimer_handler(struct hrtimer *hr)
 	return rotations ? HRTIMER_RESTART : HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|11558| <<perf_pmu_register>> __perf_mux_hrtimer_init(cpc, cpu);
+ */
 static void __perf_mux_hrtimer_init(struct perf_cpu_pmu_context *cpc, int cpu)
 {
 	struct hrtimer *timer = &cpc->hrtimer;
@@ -1119,6 +1452,11 @@ static void __perf_mux_hrtimer_init(struct perf_cpu_pmu_context *cpc, int cpu)
 	timer->function = perf_mux_hrtimer_handler;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|1445| <<perf_mux_hrtimer_restart_ipi>> return perf_mux_hrtimer_restart(arg);
+ *   - kernel/events/core.c|4162| <<merge_sched_in>> perf_mux_hrtimer_restart(cpc);
+ */
 static int perf_mux_hrtimer_restart(struct perf_cpu_pmu_context *cpc)
 {
 	struct hrtimer *timer = &cpc->hrtimer;
@@ -1135,6 +1473,10 @@ static int perf_mux_hrtimer_restart(struct perf_cpu_pmu_context *cpc)
 	return 0;
 }
 
+/*
+ * 在以下使用perf_mux_hrtimer_restart_ipi():
+ *   - kernel/events/core.c|11775| <<perf_event_mux_interval_ms_store>> cpu_function_call(cpu, perf_mux_hrtimer_restart_ipi, cpc);
+ */
 static int perf_mux_hrtimer_restart_ipi(void *arg)
 {
 	return perf_mux_hrtimer_restart(arg);
@@ -1164,6 +1506,10 @@ static void get_ctx(struct perf_event_context *ctx)
 	refcount_inc(&ctx->refcount);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5217| <<find_get_pmu_context>> task_ctx_data = alloc_task_ctx_data(pmu);
+ */
 static void *alloc_task_ctx_data(struct pmu *pmu)
 {
 	if (pmu->task_ctx_cache)
@@ -1263,6 +1609,11 @@ static void put_ctx(struct perf_event_context *ctx)
  *      pmus_lock
  *	  cpuctx->mutex / perf_event_context::mutex
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|1598| <<perf_event_ctx_lock>> return perf_event_ctx_lock_nested(event, 0);
+ *   - kernel/events/core.c|12017| <<perf_try_init_event>> ctx = perf_event_ctx_lock_nested(event->group_leader, SINGLE_DEPTH_NESTING);
+ */
 static struct perf_event_context *
 perf_event_ctx_lock_nested(struct perf_event *event, int nesting)
 {
@@ -1287,6 +1638,20 @@ perf_event_ctx_lock_nested(struct perf_event *event, int nesting)
 	return ctx;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2803| <<perf_event_disable>> ctx = perf_event_ctx_lock(event);
+ *   - kernel/events/core.c|3335| <<perf_event_enable>> ctx = perf_event_ctx_lock(event);
+ *   - kernel/events/core.c|3469| <<perf_event_refresh>> ctx = perf_event_ctx_lock(event);
+ *   - kernel/events/core.c|5693| <<perf_event_release_kernel>> ctx = perf_event_ctx_lock(event);
+ *   - kernel/events/core.c|5826| <<perf_event_read_value>> ctx = perf_event_ctx_lock(event);
+ *   - kernel/events/core.c|6031| <<perf_read>> ctx = perf_event_ctx_lock(event);
+ *   - kernel/events/core.c|6074| <<perf_event_pause>> ctx = perf_event_ctx_lock(event);
+ *   - kernel/events/core.c|6190| <<perf_event_period>> ctx = perf_event_ctx_lock(event);
+ *   - kernel/events/core.c|6346| <<perf_ioctl>> ctx = perf_event_ctx_lock(event);
+ *   - kernel/events/core.c|6382| <<perf_event_task_enable>> ctx = perf_event_ctx_lock(event);
+ *   - kernel/events/core.c|6398| <<perf_event_task_disable>> ctx = perf_event_ctx_lock(event);
+ */
 static inline struct perf_event_context *
 perf_event_ctx_lock(struct perf_event *event)
 {
@@ -1319,6 +1684,15 @@ unclone_ctx(struct perf_event_context *ctx)
 	return parent_ctx;
 }
 
+/*
+ * enum pid_type {
+ *     PIDTYPE_PID,
+ *     PIDTYPE_TGID,
+ *     PIDTYPE_PGID,
+ *     PIDTYPE_SID,
+ *     PIDTYPE_MAX,
+ * };
+ */
 static u32 perf_event_pid_type(struct perf_event *event, struct task_struct *p,
 				enum pid_type type)
 {
@@ -1329,6 +1703,9 @@ static u32 perf_event_pid_type(struct perf_event *event, struct task_struct *p,
 	if (event->parent)
 		event = event->parent;
 
+	/*
+	 * 从特定的namespace中返回pid号?
+	 */
 	nr = __task_pid_nr_ns(p, type, event->ns);
 	/* avoid -1 if it is idle thread or runs in another ns */
 	if (!nr && !pid_alive(p))
@@ -1430,6 +1807,15 @@ perf_pin_task_context(struct task_struct *task)
 
 	ctx = perf_lock_task_context(task, &flags);
 	if (ctx) {
+		/*
+		 * 在以下使用perf_event_context->pin_count:
+		 *   - kernel/events/core.c|1738| <<perf_pin_task_context>> ++ctx->pin_count;
+		 *   - kernel/events/core.c|1749| <<perf_unpin_context>> --ctx->pin_count;
+		 *   - kernel/events/core.c|3661| <<context_equiv>> if (ctx1->pin_count || ctx2->pin_count)
+		 *   - kernel/events/core.c|5119| <<find_get_context>> ++ctx->pin_count;
+		 *   - kernel/events/core.c|5130| <<find_get_context>> ++ctx->pin_count;
+		 *   - kernel/events/core.c|5154| <<find_get_context>> ++ctx->pin_count;
+		 */
 		++ctx->pin_count;
 		raw_spin_unlock_irqrestore(&ctx->lock, flags);
 	}
@@ -1505,6 +1891,17 @@ static u64 perf_event_time_now(struct perf_event *event, u64 now)
 	return now;
 }
 
+/*
+ * enum event_type_t {
+ *     EVENT_FLEXIBLE = 0x1,
+ *     EVENT_PINNED = 0x2,
+ *     EVENT_TIME = 0x4,
+ *     // see ctx_resched() for details
+ *     EVENT_CPU = 0x8,
+ *     EVENT_CGROUP = 0x10,
+ *     EVENT_ALL = EVENT_FLEXIBLE | EVENT_PINNED,
+ * };
+ */
 static enum event_type_t get_event_type(struct perf_event *event)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -1542,6 +1939,13 @@ static void init_event_group(struct perf_event *event)
 static struct perf_event_groups *
 get_event_groups(struct perf_event *event, struct perf_event_context *ctx)
 {
+	/*
+	 * struct perf_event_context *ctx:
+	 * -> struct list_head                pmu_ctx_list;
+	 * -> struct perf_event_groups        pinned_groups;
+	 * -> struct perf_event_groups        flexible_groups;
+	 * -> struct list_head                event_list;
+	 */
 	if (event->attr.pinned)
 		return &ctx->pinned_groups;
 	else
@@ -1575,6 +1979,12 @@ static inline struct cgroup *event_cgroup(const struct perf_event *event)
  * Implements complex key that first sorts by CPU and then by virtual index
  * which provides ordering when rotating groups for the same CPU.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|1942| <<__group_less>> return perf_event_groups_cmp(e->cpu, e->pmu_ctx->pmu, event_cgroup(e), e->group_index, __node_2_pe(b)) < 0;
+  3 kernel/events/core.c|1958| <<__group_cmp>> return perf_event_groups_cmp(a->cpu, a->pmu, a->cgroup, b->group_index, b);
+  4 kernel/events/core.c|1968| <<__group_cmp_ignore_cgroup>> return perf_event_groups_cmp(a->cpu, a->pmu, event_cgroup(b), b->group_index, b);
+ */
 static __always_inline int
 perf_event_groups_cmp(const int left_cpu, const struct pmu *left_pmu,
 		      const struct cgroup *left_cgroup, const u64 left_group_index,
@@ -1669,6 +2079,11 @@ __group_cmp_ignore_cgroup(const void *key, const struct rb_node *node)
  *   {@event->cpu, @event->pmu_ctx->pmu, event_cgroup(@event), ++@groups->index}
  * as key. This places it last inside the {cpu,pmu,cgroup} subtree.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|2100| <<add_event_to_groups>> perf_event_groups_insert(groups, event);
+ *   - kernel/events/core.c|4641| <<rotate_ctx>> perf_event_groups_insert(&ctx->flexible_groups, event);
+ */
 static void
 perf_event_groups_insert(struct perf_event_groups *groups,
 			 struct perf_event *event)
@@ -1681,6 +2096,11 @@ perf_event_groups_insert(struct perf_event_groups *groups,
 /*
  * Helper function to insert event into the pinned or flexible groups.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|2201| <<list_add_event>> add_event_to_groups(event, ctx);
+ *   - kernel/events/core.c|2615| <<perf_group_detach>> add_event_to_groups(sibling, event->ctx);
+ */
 static void
 add_event_to_groups(struct perf_event *event, struct perf_event_context *ctx)
 {
@@ -1693,6 +2113,11 @@ add_event_to_groups(struct perf_event *event, struct perf_event_context *ctx)
 /*
  * Delete a group from a tree.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|2126| <<del_event_from_groups>> perf_event_groups_delete(groups, event);
+ *   - kernel/events/core.c|4640| <<rotate_ctx>> perf_event_groups_delete(&ctx->flexible_groups, event);
+ */
 static void
 perf_event_groups_delete(struct perf_event_groups *groups,
 			 struct perf_event *event)
@@ -1707,6 +2132,10 @@ perf_event_groups_delete(struct perf_event_groups *groups,
 /*
  * Helper function to delete event from its groups.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|2438| <<list_del_event>> del_event_from_groups(event, ctx);
+ */
 static void
 del_event_from_groups(struct perf_event *event, struct perf_event_context *ctx)
 {
@@ -1754,6 +2183,12 @@ perf_event_groups_next(struct perf_event *event, struct pmu *pmu)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|10660| <<perf_tp_event_target_task>> perf_event_groups_for_cpu_pmu(event, &ctx->pinned_groups, cpu, pmu) {
+ *   - kernel/events/core.c|10666| <<perf_tp_event_target_task>> perf_event_groups_for_cpu_pmu(event, &ctx->flexible_groups, cpu, pmu) {
+ *   - kernel/events/core.c|13386| <<__perf_pmu_remove>> perf_event_groups_for_cpu_pmu(event, groups, cpu, pmu) {
+ */
 #define perf_event_groups_for_cpu_pmu(event, groups, cpu, pmu)		\
 	for (event = perf_event_groups_first(groups, cpu, pmu, NULL);	\
 	     event; event = perf_event_groups_next(event, pmu))
@@ -1761,6 +2196,11 @@ perf_event_groups_next(struct perf_event *event, struct pmu *pmu)
 /*
  * Iterate through the whole groups tree.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|14012| <<perf_event_init_context>> perf_event_groups_for_each(event, &parent_ctx->pinned_groups) {
+ *   - kernel/events/core.c|14028| <<perf_event_init_context>> perf_event_groups_for_each(event, &parent_ctx->flexible_groups) {
+ */
 #define perf_event_groups_for_each(event, groups)			\
 	for (event = rb_entry_safe(rb_first(&((groups)->tree)),		\
 				typeof(*event), group_node); event;	\
@@ -3890,6 +4330,27 @@ static void __pmu_ctx_sched_in(struct perf_event_context *ctx,
 	pmu_groups_sched_in(ctx, &ctx->flexible_groups, pmu);
 }
 
+/*
+ * 372 enum event_type_t {
+ * 373         EVENT_FLEXIBLE = 0x1,
+ * 374         EVENT_PINNED = 0x2,
+ * 375         EVENT_TIME = 0x4,
+ * 376         // see ctx_resched() for details
+ * 377         EVENT_CPU = 0x8,
+ * 378         EVENT_CGROUP = 0x10,
+ * 379         EVENT_ALL = EVENT_FLEXIBLE | EVENT_PINNED,
+ * 380 };
+ *
+ * called by:
+ *   - kernel/events/core.c|880| <<perf_cgroup_switch>> ctx_sched_in(&cpuctx->ctx, EVENT_ALL|EVENT_CGROUP);
+ *   - kernel/events/core.c|2675| <<perf_event_sched_in>> ctx_sched_in(&cpuctx->ctx, EVENT_PINNED);
+ *   - kernel/events/core.c|2677| <<perf_event_sched_in>> ctx_sched_in(ctx, EVENT_PINNED);
+ *   - kernel/events/core.c|2678| <<perf_event_sched_in>> ctx_sched_in(&cpuctx->ctx, EVENT_FLEXIBLE);
+ *   - kernel/events/core.c|2680| <<perf_event_sched_in>> ctx_sched_in(ctx, EVENT_FLEXIBLE);
+ *   - kernel/events/core.c|2960| <<__perf_event_enable>> ctx_sched_in(ctx, EVENT_TIME);
+ *   - kernel/events/core.c|2969| <<__perf_event_enable>> ctx_sched_in(ctx, EVENT_TIME);
+ *   - kernel/events/core.c|4395| <<perf_event_enable_on_exec>> ctx_sched_in(ctx, EVENT_TIME);
+ */
 static void
 ctx_sched_in(struct perf_event_context *ctx, enum event_type_t event_type)
 {
@@ -4261,6 +4722,10 @@ ctx_event_to_rotate(struct perf_event_pmu_context *pmu_ctx)
 	return event;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|1390| <<perf_mux_hrtimer_handler>> rotations = perf_rotate_context(cpc);
+ */
 static bool perf_rotate_context(struct perf_cpu_pmu_context *cpc)
 {
 	struct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);
@@ -4845,6 +5310,13 @@ find_get_context(struct task_struct *task, struct perf_event *event)
 	return ERR_PTR(err);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|12670| <<SYSCALL_DEFINE5>> pmu_ctx = find_get_pmu_context(pmu, ctx, event);
+ *   - kernel/events/core.c|12868| <<perf_event_create_kernel_counter>> pmu_ctx = find_get_pmu_context(pmu, ctx, event);
+ *   - kernel/events/core.c|12945| <<__perf_pmu_install_event>> epc = find_get_pmu_context(pmu, ctx, event);
+ *   - kernel/events/core.c|13341| <<inherit_event>> pmu_ctx = find_get_pmu_context(child_event->pmu, child_ctx, child_event);
+ */
 static struct perf_event_pmu_context *
 find_get_pmu_context(struct pmu *pmu, struct perf_event_context *ctx,
 		     struct perf_event *event)
@@ -4911,6 +5383,19 @@ find_get_pmu_context(struct pmu *pmu, struct perf_event_context *ctx,
 	epc = new;
 	new = NULL;
 
+	/*
+	 * 在以下使用perf_event_context->pmu_ctx_list:
+	 *   - kernel/events/core.c|692| <<perf_ctx_disable>> list_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|703| <<perf_ctx_enable>> list_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|3334| <<ctx_sched_out>> list_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|3460| <<perf_event_swap_task_ctx_data>> double_list_for_each_entry(prev_epc, next_epc, &prev_ctx->pmu_ctx_list, &next_ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|3484| <<perf_ctx_sched_task_cb>> list_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|3880| <<ctx_groups_sched_in>> list_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|4719| <<__perf_event_init_context>> INIT_LIST_HEAD(&ctx->pmu_ctx_list);
+	 *   - kernel/events/core.c|4869| <<find_get_pmu_context>> list_add(&epc->pmu_ctx_entry, &ctx->pmu_ctx_list);
+	 *   - kernel/events/core.c|4903| <<find_get_pmu_context>> list_for_each_entry(epc, &ctx->pmu_ctx_list, pmu_ctx_entry) {
+	 *   - kernel/events/core.c|4914| <<find_get_pmu_context>> list_add(&epc->pmu_ctx_entry, &ctx->pmu_ctx_list);
+	 */
 	list_add(&epc->pmu_ctx_entry, &ctx->pmu_ctx_list);
 	epc->ctx = ctx;
 
@@ -8907,6 +9392,15 @@ static void perf_addr_filters_adjust(struct vm_area_struct *vma)
 	rcu_read_unlock();
 }
 
+/*
+ * called by:
+ *   - mm/mmap.c|2052| <<expand_upwards>> perf_event_mmap(vma);
+ *   - mm/mmap.c|2145| <<expand_downwards>> perf_event_mmap(vma);
+ *   - mm/mmap.c|2908| <<mmap_region>> perf_event_mmap(vma);
+ *   - mm/mmap.c|3177| <<do_brk_flags>> perf_event_mmap(vma);
+ *   - mm/mmap.c|3585| <<__install_special_mapping>> perf_event_mmap(vma);
+ *   - mm/mprotect.c|669| <<mprotect_fixup>> perf_event_mmap(vma);
+ */
 void perf_event_mmap(struct vm_area_struct *vma)
 {
 	struct perf_mmap_event mmap_event;
diff --git a/net/core/datagram.c b/net/core/datagram.c
index a8b625abe..f09d4ebbd 100644
--- a/net/core/datagram.c
+++ b/net/core/datagram.c
@@ -557,6 +557,19 @@ EXPORT_SYMBOL(skb_copy_datagram_iter);
  *
  *	Returns 0 or -EFAULT.
  */
+/*
+ * called by:
+ *   - drivers/net/tap.c|715| <<tap_get_user>> err = skb_copy_datagram_from_iter(skb, 0, from, len);
+ *   - drivers/net/tun.c|1857| <<tun_get_user>> err = skb_copy_datagram_from_iter(skb, 0, from, len);
+ *   - net/core/datagram.c|614| <<skb_copy_datagram_from_iter>> if (skb_copy_datagram_from_iter(frag_iter, offset - start, from, copy))
+ *   - net/core/datagram.c|731| <<zerocopy_sg_from_iter>> if (skb_copy_datagram_from_iter(skb, 0, from, copy))
+ *   - net/ipv4/tcp_input.c|5119| <<tcp_send_rcvq>> err = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, size);
+ *   - net/iucv/af_iucv.c|1006| <<iucv_sock_sendmsg>> err = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, len);
+ *   - net/packet/af_packet.c|3044| <<packet_snd>> err = skb_copy_datagram_from_iter(skb, offset, &msg->msg_iter, len);
+ *   - net/unix/af_unix.c|2032| <<unix_dgram_sendmsg>> err = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, len);
+ *   - net/unix/af_unix.c|2201| <<queue_oob>> err = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, 1);
+ *   - net/unix/af_unix.c|2322| <<unix_stream_sendmsg>> err = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, size);
+ */
 int skb_copy_datagram_from_iter(struct sk_buff *skb, int offset,
 				 struct iov_iter *from,
 				 int len)
@@ -723,6 +736,11 @@ EXPORT_SYMBOL(__zerocopy_sg_from_iter);
  *
  *	Returns 0, -EFAULT or -EMSGSIZE.
  */
+/*
+ * called by:
+ *   - drivers/net/tap.c|713| <<tap_get_user>> err = zerocopy_sg_from_iter(skb, from);
+ *   - drivers/net/tun.c|1855| <<tun_get_user>> err = zerocopy_sg_from_iter(skb, from);
+ */
 int zerocopy_sg_from_iter(struct sk_buff *skb, struct iov_iter *from)
 {
 	int copy = min_t(int, skb_headlen(skb), iov_iter_count(from));
diff --git a/tools/include/uapi/linux/kvm.h b/tools/include/uapi/linux/kvm.h
index 2190adbe3..4626d3637 100644
--- a/tools/include/uapi/linux/kvm.h
+++ b/tools/include/uapi/linux/kvm.h
@@ -1158,10 +1158,23 @@ struct kvm_vfio_spapr_tce {
 #define KVM_GET_DIRTY_LOG         _IOW(KVMIO,  0x42, struct kvm_dirty_log)
 #define KVM_SET_NR_MMU_PAGES      _IO(KVMIO,   0x44)
 #define KVM_GET_NR_MMU_PAGES      _IO(KVMIO,   0x45)  /* deprecated */
+/*
+ * 在以下使用KVM_SET_USER_MEMORY_REGION:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|921| <<__vm_set_user_memory_region>> return ioctl(vm->fd, KVM_SET_USER_MEMORY_REGION, &region);
+ */
 #define KVM_SET_USER_MEMORY_REGION _IOW(KVMIO, 0x46, \
 					struct kvm_userspace_memory_region)
 #define KVM_SET_TSS_ADDR          _IO(KVMIO,   0x47)
 #define KVM_SET_IDENTITY_MAP_ADDR _IOW(KVMIO,  0x48, __u64)
+/*
+ * tools在以下使用KVM_SET_USER_MEMORY_REGION2:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|513| <<kvm_vm_restart>> int ret = ioctl(vmp->fd, KVM_SET_USER_MEMORY_REGION2, &region->region);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|718| <<__vm_mem_region_delete>> vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION2, &region->region);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|947| <<__vm_set_user_memory_region2>> return ioctl(vm->fd, KVM_SET_USER_MEMORY_REGION2, &region);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1112| <<vm_mem_add>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION2, &region->region);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1204| <<vm_mem_region_set_flags>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION2, &region->region);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1234| <<vm_mem_region_move>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION2, &region->region);
+ */
 #define KVM_SET_USER_MEMORY_REGION2 _IOW(KVMIO, 0x49, \
 					 struct kvm_userspace_memory_region2)
 
diff --git a/tools/perf/util/cpumap.c b/tools/perf/util/cpumap.c
index 356e30c42..edd411316 100644
--- a/tools/perf/util/cpumap.c
+++ b/tools/perf/util/cpumap.c
@@ -693,6 +693,14 @@ size_t cpu_map__snprint_mask(struct perf_cpu_map *map, char *buf, size_t size)
 	return ptr - buf;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evlist.c|2515| <<evlist__warn_user_requested_cpus>> to_test = pmu && pmu->is_core ? pmu->cpus : cpu_map__online();
+ *   - tools/perf/util/expr.c|431| <<expr__get_literal>> struct perf_cpu_map *online = cpu_map__online();
+ *   - tools/perf/util/mmap.c|251| <<build_node_mask>> cpu_map = cpu_map__online();
+ *   - tools/perf/util/pmu.c|715| <<pmu_cpumask>> return is_core ? perf_cpu_map__get(cpu_map__online()) : NULL;
+ *   - tools/perf/util/pmu.c|1066| <<perf_pmu__create_placeholder_core_pmu>> pmu->cpus = cpu_map__online();
+ */
 struct perf_cpu_map *cpu_map__online(void) /* thread unsafe */
 {
 	static struct perf_cpu_map *online;
diff --git a/tools/testing/selftests/kvm/aarch64/page_fault_test.c b/tools/testing/selftests/kvm/aarch64/page_fault_test.c
index 597290527..e12a5556c 100644
--- a/tools/testing/selftests/kvm/aarch64/page_fault_test.c
+++ b/tools/testing/selftests/kvm/aarch64/page_fault_test.c
@@ -18,69 +18,438 @@
 #include "guest_modes.h"
 #include "userfaultfd_util.h"
 
+/*
+ * 在以下使用TEST_GVA:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|27| <<global>> static uint64_t *guest_test_memory = (uint64_t *)TEST_GVA;
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|23| <<TEST_EXEC_GVA>> #define TEST_EXEC_GVA (TEST_GVA + 0x8)
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|231| <<guest_ld_preidx>> uint64_t addr = TEST_GVA - 8;
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|240| <<guest_ld_preidx>> GUEST_ASSERT_EQ(addr, TEST_GVA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|246| <<guest_st_preidx>> uint64_t addr = TEST_GVA - 8;
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|251| <<guest_st_preidx>> GUEST_ASSERT_EQ(addr, TEST_GVA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|275| <<guest_clear_pte_af>> flush_tlb_page(TEST_GVA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|625| <<handle_cmd>> pte_gpa = addr_hva2gpa(vm, virt_get_pte_hva(vm, TEST_GVA));
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|685| <<load_exec_code_for_test>> assert(TEST_EXEC_GVA > TEST_GVA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|686| <<load_exec_code_for_test>> code = hva + TEST_EXEC_GVA - TEST_GVA;
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|713| <<setup_gva_maps>> virt_pg_map(vm, TEST_GVA, region->region.guest_phys_addr);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|715| <<setup_gva_maps>> pte_gpa = addr_hva2gpa(vm, virt_get_pte_hva(vm, TEST_GVA));
+ *
+ * 在以下使用TEST_EXEC_GVA:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|516| <<guest_exec>> int (*code)(void ) = (int (*)(void ))TEST_EXEC_GVA;
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|919| <<load_exec_code_for_test>> assert(TEST_EXEC_GVA > TEST_GVA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|920| <<load_exec_code_for_test>> code = hva + TEST_EXEC_GVA - TEST_GVA;
+ *
+ * 在以下使用TEST_PTE_GVA:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|466| <<guest_clear_pte_af>> *((uint64_t *)TEST_PTE_GVA) &= ~PTE_AF;
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|475| <<guest_check_pte_af>> GUEST_ASSERT_EQ(*((uint64_t *)TEST_PTE_GVA) & PTE_AF, PTE_AF);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|986| <<setup_gva_maps>> virt_pg_map(vm, TEST_PTE_GVA, pte_gpa);
+ *
+ * 在以下使用TEST_DATA:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|150| <<guest_write64>> WRITE_ONCE(*guest_test_memory, TEST_DATA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|152| <<guest_write64>> GUEST_ASSERT_EQ(val, TEST_DATA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|181| <<guest_cas>> :: "r" (0ul), "r" (TEST_DATA), "r" (guest_test_memory));
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|183| <<guest_cas>> GUEST_ASSERT_EQ(val, TEST_DATA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|245| <<guest_st_preidx>> uint64_t val = TEST_DATA;
+ */
+
 /* Guest virtual addresses that point to the test page and its PTE. */
 #define TEST_GVA				0xc0000000
 #define TEST_EXEC_GVA				(TEST_GVA + 0x8)
 #define TEST_PTE_GVA				0xb0000000
 #define TEST_DATA				0x0123456789ABCDEF
 
+/*
+ * 在以下使用guest_test_memory:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|150| <<guest_write64>> WRITE_ONCE(*guest_test_memory, TEST_DATA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|151| <<guest_write64>> val = READ_ONCE(*guest_test_memory);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|181| <<guest_cas>> :: "r" (0ul), "r" (TEST_DATA), "r" (guest_test_memory));
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|182| <<guest_cas>> val = READ_ONCE(*guest_test_memory);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|190| <<guest_read64>> val = READ_ONCE(*guest_test_memory);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|199| <<guest_at>> asm volatile("at s1e1r, %0" :: "r" (guest_test_memory));
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|216| <<guest_dc_zva>> asm volatile("dc zva, %0" :: "r" (guest_test_memory));
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|218| <<guest_dc_zva>> val = READ_ONCE(*guest_test_memory);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|252| <<guest_st_preidx>> val = READ_ONCE(*guest_test_memory);
+ */
 static uint64_t *guest_test_memory = (uint64_t *)TEST_GVA;
 
+/*
+ * 在以下使用CMD_NONE:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1615| <<global>> TEST_ACCESS(guest_read64, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1616| <<global>> TEST_ACCESS(guest_ld_preidx, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1617| <<global>> TEST_ACCESS(guest_cas, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1618| <<global>> TEST_ACCESS(guest_write64, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1619| <<global>> TEST_ACCESS(guest_st_preidx, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1620| <<global>> TEST_ACCESS(guest_dc_zva, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1621| <<global>> TEST_ACCESS(guest_exec, with_af, CMD_NONE),
+ *
+ * 在以下使用CMD_HOLE_PT:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1723| <<global>> TEST_UFFD(guest_read64, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1725| <<global>> TEST_UFFD(guest_read64, no_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1727| <<global>> TEST_UFFD(guest_cas, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1733| <<global>> TEST_UFFD(guest_at, no_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1735| <<global>> TEST_UFFD(guest_ld_preidx, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1737| <<global>> TEST_UFFD(guest_write64, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1739| <<global>> TEST_UFFD(guest_dc_zva, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1741| <<global>> TEST_UFFD(guest_st_preidx, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1743| <<global>> TEST_UFFD(guest_exec, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|956| <<handle_cmd>> if (cmd & CMD_HOLE_PT) --> continue_test = punch_hole_in_backing_store(vm, pt_region);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1583| <<TEST_UFFD_AND_DIRTY_LOG>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1643| <<TEST_RO_MEMSLOT_AND_UFFD>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1659| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+ *
+ * 在以下使用CMD_HOLE_DATA:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|942| <<global>> TEST_UFFD(guest_read64, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|944| <<global>> TEST_UFFD(guest_read64, no_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|946| <<global>> TEST_UFFD(guest_cas, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|952| <<global>> TEST_UFFD(guest_at, no_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|954| <<global>> TEST_UFFD(guest_ld_preidx, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|956| <<global>> TEST_UFFD(guest_write64, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|958| <<global>> TEST_UFFD(guest_dc_zva, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|960| <<global>> TEST_UFFD(guest_st_preidx, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|962| <<global>> TEST_UFFD(guest_exec, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|482| <<handle_cmd>> if (cmd & CMD_HOLE_PT)
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|822| <<TEST_UFFD_AND_DIRTY_LOG>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|882| <<TEST_RO_MEMSLOT_AND_UFFD>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|898| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+ *
+ * 主要给mem_mark_cmd使用吧
+ *
+ * 在以下使用CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|543| <<guest_check_s1ptw_wr_in_dirty_log>> GUEST_SYNC(CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|891| <<handle_cmd>> if (cmd & CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG)
+ */
+
 #define CMD_NONE				(0)
+/*
+ * 在以下使用CMD_SKIP_TEST:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|642| <<guest_code>> GUEST_SYNC(CMD_SKIP_TEST);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|881| <<handle_cmd>> if (cmd == CMD_SKIP_TEST) --> continue_test = false;
+ */
 #define CMD_SKIP_TEST				(1ULL << 1)
+/*
+ * 处理的例子:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|956| <<handle_cmd>> if (cmd & CMD_HOLE_PT) --> continue_test = punch_hole_in_backing_store(vm, pt_region);
+ */
 #define CMD_HOLE_PT				(1ULL << 2)
 #define CMD_HOLE_DATA				(1ULL << 3)
+/*
+ * 在以下使用CMD_CHECK_WRITE_IN_DIRTY_LOG:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|711| <<guest_check_write_in_dirty_log>> GUEST_SYNC(CMD_CHECK_WRITE_IN_DIRTY_LOG);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1117| <<handle_cmd>> if (cmd & CMD_CHECK_WRITE_IN_DIRTY_LOG)
+ *                                      --> TEST_ASSERT(check_write_in_dirty_log(vm, data_region, 0), "Missing write in dirty log");
+ */
 #define CMD_CHECK_WRITE_IN_DIRTY_LOG		(1ULL << 4)
+/*
+ * 在以下使用CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|721| <<guest_check_s1ptw_wr_in_dirty_log>> GUEST_SYNC(CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1125| <<handle_cmd>> if (cmd & CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG)
+ *                                     --> TEST_ASSERT(check_write_in_dirty_log(vm, pt_region, pte_pg), "Missing s1ptw write in dirty log");
+ */
 #define CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG		(1ULL << 5)
 #define CMD_CHECK_NO_WRITE_IN_DIRTY_LOG		(1ULL << 6)
 #define CMD_CHECK_NO_S1PTW_WR_IN_DIRTY_LOG	(1ULL << 7)
+/*
+ * 没人使用
+ */
 #define CMD_SET_PTE_AF				(1ULL << 8)
 
 #define PREPARE_FN_NR				10
 #define CHECK_FN_NR				10
 
 static struct event_cnt {
+	/*
+	 * 在以下使用event_cnt->mmio_exits:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|621| <<mmio_on_test_gpa_handler>> events.mmio_exits += 1;
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|921| <<check_event_counts>> TEST_ASSERT_EQ(test->expected_events.mmio_exits, events.mmio_exits);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1180| <<TEST_RO_MEMSLOT>> .expected_events = { .mmio_exits = _mmio_exits }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1204| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .expected_events = { .mmio_exits = _mmio_exits}, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1231| <<TEST_RO_MEMSLOT_AND_UFFD>> .expected_events = { .mmio_exits = _mmio_exits, \
+	 */
 	int mmio_exits;
+	/*
+	 * 在以下使用event_cnt->fail_vcpu_runs:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|703| <<fail_vcpu_run_mmio_no_syndrome_handler>> events.fail_vcpu_runs += 1;
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|930| <<check_event_counts>> TEST_ASSERT_EQ(test->expected_events.fail_vcpu_runs, events.fail_vcpu_runs);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1211| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .expected_events = { .fail_vcpu_runs = 1 }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1236| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .expected_events = { .fail_vcpu_runs = 1 }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1267| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .expected_events = { .fail_vcpu_runs = 1, \
+	 */
 	int fail_vcpu_runs;
+	/*
+	 * 在以下使用event_cnt->uffd_faults:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|530| <<uffd_generic_handler>> events.uffd_faults += 1;
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|928| <<check_event_counts>> TEST_ASSERT_EQ(test->expected_events.uffd_faults, events.uffd_faults);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1161| <<TEST_UFFD>> .expected_events = { .uffd_faults = _uffd_faults, }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1189| <<TEST_UFFD_AND_DIRTY_LOG>> .expected_events = { .uffd_faults = _uffd_faults, }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1252| <<TEST_RO_MEMSLOT_AND_UFFD>> .uffd_faults = _uffd_faults }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1268| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .uffd_faults = _uffd_faults }, \
+	 */
 	int uffd_faults;
+	/*
+	 * 在以下使用event_cnt->uffd_faults_mutex:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|529| <<uffd_generic_handler>> pthread_mutex_lock(&events.uffd_faults_mutex);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|531| <<uffd_generic_handler>> pthread_mutex_unlock(&events.uffd_faults_mutex);
+	 */
 	/* uffd_faults is incremented from multiple threads. */
 	pthread_mutex_t uffd_faults_mutex;
 } events;
 
+/*
+ * 关于UFFD.
+ *
+ * Userspace creates a new userfaultfd, initializes it, and registers one or
+ * more regions of virtual memory with it. Then, any page faults which occur
+ * within the region(s) result in a message being delivered to the userfaultfd,
+ * notifying userspace of the fault.
+ */
+
 struct test_desc {
 	const char *name;
+	/*
+	 * 在以下使用test_desc->mem_mark_cmd:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|344| <<guest_code>> GUEST_SYNC(test->mem_mark_cmd);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|902| <<TEST_ACCESS>> .mem_mark_cmd = _mark_cmd, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|915| <<TEST_UFFD>> .mem_mark_cmd = _mark_cmd, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|943| <<TEST_UFFD_AND_DIRTY_LOG>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1003| <<TEST_RO_MEMSLOT_AND_UFFD>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1019| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+	 */
 	uint64_t mem_mark_cmd;
+	/*
+	 * 在以下使用test_desc->guest_prepare[PREPARE_FN_NR]:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|484| <<guest_prepare>> prepare_fn = test->guest_prepare[i];
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1317| <<TEST_ACCESS>> .guest_prepare = { _PREPARE(_with_af), \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1329| <<TEST_UFFD>> .guest_prepare = { _PREPARE(_with_af), \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1344| <<TEST_DIRTY_LOG>> .guest_prepare = { _PREPARE(_with_af), \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1357| <<TEST_UFFD_AND_DIRTY_LOG>> .guest_prepare = { _PREPARE(_with_af), \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1372| <<TEST_RO_MEMSLOT>> .guest_prepare = { _PREPARE(_access) }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1383| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .guest_prepare = { _PREPARE(_access) }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1395| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .guest_prepare = { _PREPARE(_access) }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1407| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .guest_prepare = { _PREPARE(_access) }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1421| <<TEST_RO_MEMSLOT_AND_UFFD>> .guest_prepare = { _PREPARE(_access) }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1437| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .guest_prepare = { _PREPARE(_access) }, \
+	 */
 	/* Skip the test if any prepare function returns false */
 	bool (*guest_prepare[PREPARE_FN_NR])(void);
+	/*
+	 * 在以下使用test_desc->guest_test:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|607| <<guest_code>> if (test->guest_test)
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|608| <<guest_code>> test->guest_test();
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1364| <<TEST_ACCESS>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1375| <<TEST_UFFD>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1390| <<TEST_DIRTY_LOG>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1403| <<TEST_UFFD_AND_DIRTY_LOG>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1417| <<TEST_RO_MEMSLOT>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1428| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1440| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1452| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1466| <<TEST_RO_MEMSLOT_AND_UFFD>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1482| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .guest_test = _access, \
+	 */
 	void (*guest_test)(void);
+	/*
+	 * 在以下使用test_desc->guest_test_check[CHECK_FN_NR]:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|512| <<guest_test_check>> check_fn = test->guest_test_check[i];
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1321| <<TEST_ACCESS>> .guest_test_check = { _CHECK(_with_af) }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1333| <<TEST_UFFD>> .guest_test_check = { _CHECK(_with_af) }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1347| <<TEST_DIRTY_LOG>> .guest_test_check = { _CHECK(_with_af), _test_check, _pt_check }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1361| <<TEST_UFFD_AND_DIRTY_LOG>> .guest_test_check = { _CHECK(_with_af), _test_check, _pt_check }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1397| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .guest_test_check = { _test_check }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1409| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .guest_test_check = { _test_check }, \
+	 */
 	void (*guest_test_check[CHECK_FN_NR])(void);
+	/*
+	 * 在以下使用test_desc->uffd_pt_handler:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|655| <<setup_uffd>> if (test->uffd_pt_handler)
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|659| <<setup_uffd>> *pt_uffd = uffd_setup_demand_paging(uffd_mode, 0, pt_args.hva, pt_args.paging_size, test->uffd_pt_handler);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|672| <<free_uffd>> if (test->uffd_pt_handler)
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1335| <<TEST_UFFD>> .uffd_pt_handler = _uffd_pt_handler, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1363| <<TEST_UFFD_AND_DIRTY_LOG>> .uffd_pt_handler = uffd_pt_handler, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1424| <<TEST_RO_MEMSLOT_AND_UFFD>> .uffd_pt_handler = uffd_pt_handler, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1440| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .uffd_pt_handler = uffd_pt_handler, \
+	 */
 	uffd_handler_t uffd_pt_handler;
+	/*
+	 * 在以下使用test_desc->uffd_data_handler:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|662| <<setup_uffd>> if (test->uffd_data_handler)
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|666| <<setup_uffd>> *data_uffd = uffd_setup_demand_paging(uffd_mode, 0, data_args.hva, data_args.paging_size, test->uffd_data_handler);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|674| <<free_uffd>> if (test->uffd_data_handler)
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1334| <<TEST_UFFD>> .uffd_data_handler = _uffd_data_handler, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1362| <<TEST_UFFD_AND_DIRTY_LOG>> .uffd_data_handler = _uffd_data_handler, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1423| <<TEST_RO_MEMSLOT_AND_UFFD>> .uffd_data_handler = _uffd_data_handler, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1439| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .uffd_data_handler = _uffd_data_handler, \
+	 */
 	uffd_handler_t uffd_data_handler;
+	/*
+	 * 没见到使用test_desc->dabt_handler()的
+	 */
 	void (*dabt_handler)(struct ex_regs *regs);
+	/*
+	 * 没见到使用test_desc->iabt_handler()的
+	 */
 	void (*iabt_handler)(struct ex_regs *regs);
+	/*
+	 * 在以下使用test_desc->mmio_handler:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1021| <<setup_default_handlers>> if (!test->mmio_handler)
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1022| <<setup_default_handlers>> test->mmio_handler = mmio_no_handler;
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1100| <<vcpu_run_loop>> test->mmio_handler(vm, run);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1313| <<TEST_RO_MEMSLOT>> .mmio_handler = _mmio_handler, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1337| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .mmio_handler = _mmio_handler, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1364| <<TEST_RO_MEMSLOT_AND_UFFD>> .mmio_handler = _mmio_handler, \
+	 */
 	void (*mmio_handler)(struct kvm_vm *vm, struct kvm_run *run);
+	/*
+	 * 在以下使用test_desc->fail_vcpu_run_handler:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1024| <<setup_default_handlers>> if (!test->fail_vcpu_run_handler)
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1025| <<setup_default_handlers>> test->fail_vcpu_run_handler = fail_vcpu_run_no_handler;
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1082| <<vcpu_run_loop>> test->fail_vcpu_run_handler(ret);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1324| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .fail_vcpu_run_handler = fail_vcpu_run_mmio_no_syndrome_handler, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1349| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .fail_vcpu_run_handler = fail_vcpu_run_mmio_no_syndrome_handler, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1380| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .fail_vcpu_run_handler = fail_vcpu_run_mmio_no_syndrome_handler, \
+	 */
 	void (*fail_vcpu_run_handler)(int ret);
+	/*
+	 * 在以下使用test_desc->pt_memslot_flags:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|987| <<setup_memslots>> vm_userspace_mem_region_add(vm, p->src_type, data_gpa - pt_size, PAGE_TABLE_MEMSLOT,
+	 *                                    pt_size / guest_page_size, p->test_desc->pt_memslot_flags);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1282| <<TEST_DIRTY_LOG>> .pt_memslot_flags = KVM_MEM_LOG_DIRTY_PAGES, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1295| <<TEST_UFFD_AND_DIRTY_LOG>> .pt_memslot_flags = KVM_MEM_LOG_DIRTY_PAGES, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1310| <<TEST_RO_MEMSLOT>> .pt_memslot_flags = KVM_MEM_READONLY, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1321| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .pt_memslot_flags = KVM_MEM_READONLY, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1333| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .pt_memslot_flags = KVM_MEM_READONLY | KVM_MEM_LOG_DIRTY_PAGES, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1345| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .pt_memslot_flags = KVM_MEM_READONLY | KVM_MEM_LOG_DIRTY_PAGES, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1358| <<TEST_RO_MEMSLOT_AND_UFFD>> .pt_memslot_flags = KVM_MEM_READONLY, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1374| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .pt_memslot_flags = KVM_MEM_READONLY, \
+	 */
 	uint32_t pt_memslot_flags;
+	/*
+	 * 在以下使用test_desc->data_memslot_flags:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|663| <<setup_memslots>> vm_userspace_mem_region_add(vm, p->src_type, data_gpa, TEST_DATA_MEMSLOT,
+	 *                                 data_size / guest_page_size, p->test_desc->data_memslot_flags);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|885| <<TEST_DIRTY_LOG>> .data_memslot_flags = KVM_MEM_LOG_DIRTY_PAGES, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|898| <<TEST_UFFD_AND_DIRTY_LOG>> .data_memslot_flags = KVM_MEM_LOG_DIRTY_PAGES, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|913| <<TEST_RO_MEMSLOT>> .data_memslot_flags = KVM_MEM_READONLY, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|924| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .data_memslot_flags = KVM_MEM_READONLY, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|936| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .data_memslot_flags = KVM_MEM_READONLY | KVM_MEM_LOG_DIRTY_PAGES, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|948| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .data_memslot_flags = KVM_MEM_READONLY | KVM_MEM_LOG_DIRTY_PAGES, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|961| <<TEST_RO_MEMSLOT_AND_UFFD>> .data_memslot_flags = KVM_MEM_READONLY, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|977| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .data_memslot_flags = KVM_MEM_READONLY, \
+	 */
 	uint32_t data_memslot_flags;
+	/*
+	 * 在以下使用test_desc->skip:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1089| <<vcpu_run_loop>> test->skip = true;
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1108| <<vcpu_run_loop>> pr_debug(test->skip ? "Skipped.\n" : "Done.\n");
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1178| <<run_test>> if (!test->skip)
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1623| <<for_each_test_and_guest_mode>> if (t->skip)
+	 */
 	bool skip;
+	/*
+	 * 在以下使用test_desc->expected_events:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1034| <<check_event_counts>> TEST_ASSERT_EQ(test->expected_events.uffd_faults, events.uffd_faults);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1035| <<check_event_counts>> TEST_ASSERT_EQ(test->expected_events.mmio_exits, events.mmio_exits);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1036| <<check_event_counts>> TEST_ASSERT_EQ(test->expected_events.fail_vcpu_runs, events.fail_vcpu_runs);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1261| <<TEST_ACCESS>> .expected_events = { 0 }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1275| <<TEST_UFFD>> .expected_events = { .uffd_faults = _uffd_faults, }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1287| <<TEST_DIRTY_LOG>> .expected_events = { 0 }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1303| <<TEST_UFFD_AND_DIRTY_LOG>> .expected_events = { .uffd_faults = _uffd_faults, }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1314| <<TEST_RO_MEMSLOT>> .expected_events = { .mmio_exits = _mmio_exits }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1325| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .expected_events = { .fail_vcpu_runs = 1 }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1338| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .expected_events = { .mmio_exits = _mmio_exits}, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1350| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .expected_events = { .fail_vcpu_runs = 1 }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1365| <<TEST_RO_MEMSLOT_AND_UFFD>> .expected_events = { .mmio_exits = _mmio_exits, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1381| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .expected_events = { .fail_vcpu_runs = 1, \
+	 */
 	struct event_cnt expected_events;
 };
 
+/*
+ *  97 enum vm_mem_backing_src_type {
+ *  98         VM_MEM_SRC_ANONYMOUS,
+ *  99         VM_MEM_SRC_ANONYMOUS_THP,
+ * 100         VM_MEM_SRC_ANONYMOUS_HUGETLB,
+ * 101         VM_MEM_SRC_ANONYMOUS_HUGETLB_16KB,
+ * 102         VM_MEM_SRC_ANONYMOUS_HUGETLB_64KB,
+ * 103         VM_MEM_SRC_ANONYMOUS_HUGETLB_512KB,
+ * 104         VM_MEM_SRC_ANONYMOUS_HUGETLB_1MB,
+ * 105         VM_MEM_SRC_ANONYMOUS_HUGETLB_2MB,
+ * 106         VM_MEM_SRC_ANONYMOUS_HUGETLB_8MB,
+ * 107         VM_MEM_SRC_ANONYMOUS_HUGETLB_16MB,
+ * 108         VM_MEM_SRC_ANONYMOUS_HUGETLB_32MB,
+ * 109         VM_MEM_SRC_ANONYMOUS_HUGETLB_256MB,
+ * 110         VM_MEM_SRC_ANONYMOUS_HUGETLB_512MB,
+ * 111         VM_MEM_SRC_ANONYMOUS_HUGETLB_1GB,
+ * 112         VM_MEM_SRC_ANONYMOUS_HUGETLB_2GB,
+ * 113         VM_MEM_SRC_ANONYMOUS_HUGETLB_16GB,
+ * 114         VM_MEM_SRC_SHMEM,
+ * 115         VM_MEM_SRC_SHARED_HUGETLB,
+ * 116         NUM_SRC_TYPES,
+ * 117 };
+ */
 struct test_params {
 	enum vm_mem_backing_src_type src_type;
 	struct test_desc *test_desc;
 };
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|213| <<guest_clear_pte_af>> flush_tlb_page(TEST_GVA);
+ */
 static inline void flush_tlb_page(uint64_t vaddr)
 {
 	uint64_t page = vaddr >> 12;
 
+	/*
+	 * 注释
+	 * DSB
+	 * Data Synchronization Barrier acts as a special kind of memory
+	 * barrier. No instruction in program order after this instruction
+	 * executes until this instruction completes. This instruction
+	 * completes when:
+	 *
+	 * All explicit memory accesses before this instruction complete.
+	 *
+	 * All Cache, Branch predictor and TLB maintenance operations before
+	 * this instruction complete.
+	 *
+	 * ISH: DSB operation only applies to the inner shareable domain.
+	 * ISHST: DSB operation that waits only for stores to complete, and
+	 * only applies to the inner shareable domain.
+	 */
 	dsb(ishst);
 	asm volatile("tlbi vaae1is, %0" :: "r" (page));
 	dsb(ish);
+	/*
+	 * 注释:
+	 * ISB
+	 * Instruction Synchronization Barrier flushes the pipeline in the
+	 * processor, so that all instructions following the ISB are fetched
+	 * from cache or memory, after the instruction has been completed. It
+	 * ensures that the effects of context altering operations, such as
+	 * changing the ASID, or completed TLB maintenance operations, or
+	 * branch predictor maintenance operations, in addition to all changes
+	 * to the CP15 registers, executed before the ISB instruction are
+	 * visible to the instructions fetched after the ISB.
+	 *
+	 * In addition, the ISB instruction ensures that any branches that
+	 * appear in program order after it are always written into the branch
+	 * prediction logic with the context that is visible after the ISB
+	 * instruction. This is required to ensure correct execution of the
+	 * instruction stream.
+	 */
 	isb();
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|914| <<global>> TEST_ACCESS(guest_write64, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|931| <<global>> TEST_ACCESS(guest_write64, no_af, CMD_HOLE_DATA),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|956| <<global>> TEST_UFFD(guest_write64, with_af, CMD_HOLE_DATA | CMD_HOLE_PT,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|980| <<global>> TEST_DIRTY_LOG(guest_write64, with_af, guest_check_write_in_dirty_log,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1016| <<global>> TEST_UFFD_AND_DIRTY_LOG(guest_write64, with_af,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1043| <<global>> TEST_RO_MEMSLOT(guest_write64, mmio_on_test_gpa_handler, 1),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1064| <<global>> TEST_RO_MEMSLOT_AND_DIRTY_LOG(guest_write64, mmio_on_test_gpa_handler,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1086| <<global>> TEST_RO_MEMSLOT_AND_UFFD(guest_write64, mmio_on_test_gpa_handler, 1,
+ */
 static void guest_write64(void)
 {
 	uint64_t val;
@@ -90,16 +459,35 @@ static void guest_write64(void)
 	GUEST_ASSERT_EQ(val, TEST_DATA);
 }
 
+/*
+ * 在以下使用guest_check_lse():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|350| <<guest_cas>> GUEST_ASSERT(guest_check_lse());
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1311| <<_PREPARE_guest_cas>> #define _PREPARE_guest_cas guest_check_lse
+ *
+ * 是否支持atomic instruction?
+ */
 /* Check the system for atomic instructions. */
 static bool guest_check_lse(void)
 {
 	uint64_t isar0 = read_sysreg(id_aa64isar0_el1);
 	uint64_t atomic;
 
+	/*
+	 * id_aa64isar0_el1是一个寄存器
+	 */
 	atomic = FIELD_GET(ARM64_FEATURE_MASK(ID_AA64ISAR0_EL1_ATOMIC), isar0);
 	return atomic >= 2;
 }
 
+/*
+ * 查看是否支持DC ZVA:
+ * Zero data cache by address. Zeroes a naturally aligned block of N bytes,
+ * where the size of N is identified in DCZID_EL0.
+ *
+ * DZP:
+ * Data Zero Prohibited. This field indicates whether use of DC ZVA
+ * instructions is permitted or prohibited.
+ */
 static bool guest_check_dc_zva(void)
 {
 	uint64_t dczid = read_sysreg(dczid_el0);
@@ -108,6 +496,10 @@ static bool guest_check_dc_zva(void)
 	return dzp == 0;
 }
 
+/*
+ * 关于CAS指令:
+ * Compare and Swap word or doubleword in memory reads
+ */
 /* Compare and swap instruction. */
 static void guest_cas(void)
 {
@@ -129,6 +521,21 @@ static void guest_read64(void)
 	GUEST_ASSERT_EQ(val, 0);
 }
 
+/*
+ * 在以下使用guest_at():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1733| <<global>> TEST_ACCESS(guest_at, no_af, CMD_HOLE_DATA),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1752| <<global>> TEST_UFFD(guest_at, no_af, CMD_HOLE_DATA | CMD_HOLE_PT, uffd_no_handler, uffd_pt_handler, 1),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1776| <<global>> TEST_DIRTY_LOG(guest_at, no_af, guest_check_no_write_in_dirty_log, guest_check_no_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1809| <<global>> TEST_UFFD_AND_DIRTY_LOG(guest_at, with_af, uffd_no_handler, 1,
+ *                                                                                       guest_check_no_write_in_dirty_log, guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1841| <<global>> TEST_RO_MEMSLOT(guest_at, 0, 0),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1860| <<global>> TEST_RO_MEMSLOT_AND_DIRTY_LOG(guest_at, 0, 0, guest_check_no_write_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1884| <<global>> TEST_RO_MEMSLOT_AND_UFFD(guest_at, 0, 0, uffd_no_handler, 1),
+ *
+ * 似乎是强行发起一次address translation (通过"at s1e1r")
+ * 结果通过PAR_EL1返回, bit 0是成功了吗, 比如[47:12]是物理地址
+ * 应该会刷新MMU的cache (tlb??)
+ */
 /* Address translation instruction */
 static void guest_at(void)
 {
@@ -147,6 +554,11 @@ static void guest_at(void)
  * 0) and (2 << 9), which is safe in our case as we need the write to happen
  * for at least a word, and not more than a page.
  */
+/*
+ * 注释DC ZVA:
+ * Zero data cache by address. Zeroes a naturally aligned block of N bytes,
+ * where the size of N is identified in DCZID_EL0.
+ */
 static void guest_dc_zva(void)
 {
 	uint16_t val;
@@ -157,6 +569,20 @@ static void guest_dc_zva(void)
 	GUEST_ASSERT_EQ(val, 0);
 }
 
+/*
+ * 在以下是他guest_ld_preidx():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1822| <<global>> TEST_ACCESS(guest_ld_preidx, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1840| <<global>> TEST_ACCESS(guest_ld_preidx, no_af, CMD_HOLE_DATA),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1864| <<global>> TEST_UFFD(guest_ld_preidx, with_af, CMD_HOLE_DATA | CMD_HOLE_PT, uffd_data_handler, uffd_pt_handler, 2),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1883| <<global>> TEST_DIRTY_LOG(guest_ld_preidx, with_af, guest_check_no_write_in_dirty_log, guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1915| <<global>> TEST_UFFD_AND_DIRTY_LOG(guest_ld_preidx, with_af, uffd_data_handler, 2,
+ *                                                                      guest_check_no_write_in_dirty_log, guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1950| <<global>> TEST_RO_MEMSLOT(guest_ld_preidx, 0, 0),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1968| <<global>> TEST_RO_MEMSLOT_AND_DIRTY_LOG(guest_ld_preidx, 0, 0, guest_check_no_write_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1993| <<global>> TEST_RO_MEMSLOT_AND_UFFD(guest_ld_preidx, 0, 0, uffd_data_handler, 2),
+ *
+ * load并且检查结果
+ */
 /*
  * Pre-indexing loads and stores don't have a valid syndrome (ESR_EL2.ISV==0).
  * And that's special because KVM must take special care with those: they
@@ -178,6 +604,9 @@ static void guest_ld_preidx(void)
 	GUEST_ASSERT_EQ(addr, TEST_GVA);
 }
 
+/*
+ * store并且检查结果
+ */
 static void guest_st_preidx(void)
 {
 	uint64_t val = TEST_DATA;
@@ -190,16 +619,32 @@ static void guest_st_preidx(void)
 	val = READ_ONCE(*guest_test_memory);
 }
 
+/*
+ * 在以下使用guest_set_ha():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1643| <<_PREPARE_with_af>> #define _PREPARE_with_af guest_set_ha, guest_clear_pte_af
+ *
+ * 通过id_aa64mmfr1_el1寄存器查看是否支持HAFDBS(自动更新access和dirty access)
+ * 如果支持, 在TCR_EL1设置HA(没管HD)
+ */
 static bool guest_set_ha(void)
 {
 	uint64_t mmfr1 = read_sysreg(id_aa64mmfr1_el1);
 	uint64_t hadbs, tcr;
 
+	/*
+	 * 注释:HAFDBS, [3:0]
+	 * Indicates the support for hardware updates to Access flag and dirty
+	 * state in translation tables.
+	 */
 	/* Skip if HA is not supported. */
 	hadbs = FIELD_GET(ARM64_FEATURE_MASK(ID_AA64MMFR1_EL1_HAFDBS), mmfr1);
 	if (hadbs == 0)
 		return false;
 
+	/*
+	 * Hardware Access flag update in stage 1 translations from EL0 and
+	 * EL1.
+	 */
 	tcr = read_sysreg(tcr_el1) | TCR_EL1_HA;
 	write_sysreg(tcr, tcr_el1);
 	isb();
@@ -207,22 +652,86 @@ static bool guest_set_ha(void)
 	return true;
 }
 
+/*
+ * 在以下使用guest_clear_pte_af():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1512| <<_PREPARE_with_af>> #define _PREPARE_with_af guest_set_ha, guest_clear_pte_a
+ *
+ * 去掉pte的access记录
+ */
 static bool guest_clear_pte_af(void)
 {
 	*((uint64_t *)TEST_PTE_GVA) &= ~PTE_AF;
+	/*
+	 * 只在此处调用
+	 */
 	flush_tlb_page(TEST_GVA);
 
 	return true;
 }
 
+/*
+ * 注释:
+ * https://developer.arm.com/documentation/den0024/a/The-Memory-Management-Unit/Operating-system-use-of-translation-table-descriptors
+ *
+ * Another memory attribute bit in the descriptor, the Access Flag (AF),
+ * indicates when a block entry is used for the first time.
+ *
+ * AF = 0: This block entry has not yet been used.
+ *
+ * AF = 1: This block entry has been used.
+ *
+ * Operating systems use an access flag bit to keep track of which pages are
+ * being used. Software manages the flag. When the page is first created, its
+ * entry has AF set to 0. The first time the page is accessed by code, if it
+ * has AF at 0, this triggers an MMU fault. The Page fault handler records that
+ * this page is now being used and manually sets the AF bit in the table entry.
+ * For example, the Linux kernel uses the [AF] bit for PTE_AF on ARM64 (the
+ * Linux kernel name for AArch64), which is used to check whether a page has
+ * ever been accessed. This influences some of the kernel memory management
+ * choices. For example, when a page must be swapped out of memory, it is less
+ * likely to swap out pages that are being actively used.
+ *
+ * Bits [58:55] of the descriptor are marked as Reserved for Software Use and
+ * can be used to record OS-specific information in the translation tables. For
+ * example, the Linux kernel uses one of these bits to mark an entry as clean
+ * or dirty. The dirty status records whether the page has been written to. If
+ * the page is later swapped out of memory, a clean page can simply be
+ * discarded, but a dirty page must have its contents saved first.
+ *
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1346| <<_CHECK_with_af>> #define _CHECK_with_af guest_check_pte_af
+ *
+ * 检查pte的access记录
+ */
 static void guest_check_pte_af(void)
 {
 	dsb(ish);
 	GUEST_ASSERT_EQ(*((uint64_t *)TEST_PTE_GVA) & PTE_AF, PTE_AF);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1612| <<global>> TEST_DIRTY_LOG(guest_write64, with_af, guest_check_write_in_dirty_log, guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1614| <<global>> TEST_DIRTY_LOG(guest_cas, with_af, guest_check_write_in_dirty_log, guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1616| <<global>> TEST_DIRTY_LOG(guest_dc_zva, with_af, guest_check_write_in_dirty_log, guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1618| <<global>> TEST_DIRTY_LOG(guest_st_preidx, with_af, guest_check_write_in_dirty_log, guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1650| <<global>> TEST_UFFD_AND_DIRTY_LOG(guest_write64, with_af, uffd_data_handler, 2, guest_check_write_in_dirty_log,
+ *                                                                                         guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1654| <<global>> TEST_UFFD_AND_DIRTY_LOG(guest_cas, with_af, uffd_data_handler, 2, guest_check_write_in_dirty_log,
+ *                                                                                         guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1658| <<global>> TEST_UFFD_AND_DIRTY_LOG(guest_dc_zva, with_af, uffd_data_handler, guest_check_write_in_dirty_log,
+ *                                                                                         guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1662| <<global>> TEST_UFFD_AND_DIRTY_LOG(guest_st_preidx, with_af, uffd_data_handler, 2, guest_check_write_in_dirty_log,
+ *                                                                                         guest_check_s1ptw_wr_in_dirty_log),
+ */
 static void guest_check_write_in_dirty_log(void)
 {
+	/*
+	 * 在以下使用CMD_CHECK_WRITE_IN_DIRTY_LOG:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|711| <<guest_check_write_in_dirty_log>> GUEST_SYNC(CMD_CHECK_WRITE_IN_DIRTY_LOG);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1117| <<handle_cmd>> if (cmd & CMD_CHECK_WRITE_IN_DIRTY_LOG)
+	 *                                                 --> TEST_ASSERT(check_write_in_dirty_log(vm, data_region, 0), "Missing write in dirty log");
+	 */
 	GUEST_SYNC(CMD_CHECK_WRITE_IN_DIRTY_LOG);
 }
 
@@ -233,16 +742,53 @@ static void guest_check_no_write_in_dirty_log(void)
 
 static void guest_check_s1ptw_wr_in_dirty_log(void)
 {
+	/*
+	 * 在以下使用CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|721| <<guest_check_s1ptw_wr_in_dirty_log>> GUEST_SYNC(CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1125| <<handle_cmd>> if (cmd & CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG)
+	 *                                                 --> TEST_ASSERT(check_write_in_dirty_log(vm, pt_region, pte_pg), "Missing s1ptw write in dirty log");
+	 */
 	GUEST_SYNC(CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1004| <<global>> TEST_DIRTY_LOG(guest_read64, no_af, guest_check_no_write_in_dirty_log, guest_check_no_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1009| <<global>> TEST_DIRTY_LOG(guest_at, no_af, guest_check_no_write_in_dirty_log, guest_check_no_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1036| <<global>> TEST_UFFD_AND_DIRTY_LOG(guest_read64, no_af, uffd_data_handler, 2,
+ *                                                                           guest_check_no_write_in_dirty_log, guest_check_no_s1ptw_wr_in_dirty_log),
+ */
 static void guest_check_no_s1ptw_wr_in_dirty_log(void)
 {
 	GUEST_SYNC(CMD_CHECK_NO_S1PTW_WR_IN_DIRTY_LOG);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|949| <<global>> TEST_ACCESS(guest_exec, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|994| <<global>> TEST_UFFD(guest_exec, with_af, CMD_HOLE_DATA | CMD_HOLE_PT, uffd_data_handler, uffd_pt_handler, 2),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1010| <<global>> TEST_DIRTY_LOG(guest_exec, with_af, guest_check_no_write_in_dirty_log, guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1044| <<global>> TEST_UFFD_AND_DIRTY_LOG(guest_exec, with_af, uffd_data_handler, 2,
+ *                                                                        guest_check_no_write_in_dirty_log, guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1074| <<global>> TEST_RO_MEMSLOT(guest_exec, 0, 0),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1094| <<global>> TEST_RO_MEMSLOT_AND_DIRTY_LOG(guest_exec, 0, 0, guest_check_no_write_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1117| <<global>> TEST_RO_MEMSLOT_AND_UFFD(guest_exec, 0, 0, uffd_data_handler, 2),
+ */
 static void guest_exec(void)
 {
+	/*
+	 * 加载的代码.
+	 * 1193 noinline void __return_0x77(void)
+	 * 1194 {
+	 * 1195         asm volatile("__exec_test: mov x0, #0x77\n"
+	 * 1196                      "ret\n");
+	 * 1197 }
+	 *
+	 * 在以下使用TEST_EXEC_GVA:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|516| <<guest_exec>> int (*code)(void ) = (int (*)(void ))TEST_EXEC_GVA;
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|919| <<load_exec_code_for_test>> assert(TEST_EXEC_GVA > TEST_GVA);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|920| <<load_exec_code_for_test>> code = hva + TEST_EXEC_GVA - TEST_GVA;
+	 */
 	int (*code)(void) = (int (*)(void))TEST_EXEC_GVA;
 	int ret;
 
@@ -250,12 +796,31 @@ static void guest_exec(void)
 	GUEST_ASSERT_EQ(ret, 0x77);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|341| <<guest_code>> if (!guest_prepare(test))
+ */
 static bool guest_prepare(struct test_desc *test)
 {
 	bool (*prepare_fn)(void);
 	int i;
 
 	for (i = 0; i < PREPARE_FN_NR; i++) {
+		/*
+		 * 在以下使用test_desc->guest_prepare:
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|319| <<guest_prepare>> prepare_fn = test->guest_prepare[i];
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|341| <<guest_code>> if (!guest_prepare(test))
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|900| <<TEST_ACCESS>> .guest_prepare = { _PREPARE(_with_af), \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|912| <<TEST_UFFD>> .guest_prepare = { _PREPARE(_with_af), \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|927| <<TEST_DIRTY_LOG>> .guest_prepare = { _PREPARE(_with_af), \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|940| <<TEST_UFFD_AND_DIRTY_LOG>> .guest_prepare = { _PREPARE(_with_af), \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|955| <<TEST_RO_MEMSLOT>> .guest_prepare = { _PREPARE(_access) }, \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|966| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .guest_prepare = { _PREPARE(_access) }, \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|978| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .guest_prepare = { _PREPARE(_access) }, \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|990| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .guest_prepare = { _PREPARE(_access) }, \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1004| <<TEST_RO_MEMSLOT_AND_UFFD>> .guest_prepare = { _PREPARE(_access) }, \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1020| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .guest_prepare = { _PREPARE(_access) }, \
+		 */
 		prepare_fn = test->guest_prepare[i];
 		if (prepare_fn && !prepare_fn())
 			return false;
@@ -264,25 +829,113 @@ static bool guest_prepare(struct test_desc *test)
 	return true;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|409| <<guest_code>> guest_test_check(test);
+ */
 static void guest_test_check(struct test_desc *test)
 {
 	void (*check_fn)(void);
 	int i;
 
 	for (i = 0; i < CHECK_FN_NR; i++) {
+		/*
+		 * 在以下使用test_desc->guest_test_check:
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|370| <<guest_test_check>> check_fn = test->guest_test_check[i];
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|990| <<TEST_ACCESS>> .guest_test_check = { _CHECK(_with_af) }, \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1002| <<TEST_UFFD>> .guest_test_check = { _CHECK(_with_af) }, \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1016| <<TEST_DIRTY_LOG>> .guest_test_check = { _CHECK(_with_af), _test_check, _pt_check }, \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1030| <<TEST_UFFD_AND_DIRTY_LOG>> .guest_test_check = { _CHECK(_with_af), _test_check, _pt_check }, \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1066| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .guest_test_check = { _test_check }, \
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1078| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .guest_test_check = { _test_check }, \
+		 */
 		check_fn = test->guest_test_check[i];
 		if (check_fn)
 			check_fn();
 	}
 }
 
+/*
+ * 例子.
+ *
+ * TEST_ACCESS(guest_read64, with_af, CMD_NONE),
+ *
+ * 展开后是:
+ *
+ * 1836 #define TEST_ACCESS(_access, _with_af, _mark_cmd)                               \
+ * 1837 {                                                                               \
+ * 1838         .name                   = SCAT3(_access, _with_af, #_mark_cmd),         \
+ * 1839         .guest_prepare          = { _PREPARE(_with_af),                         \
+ * 1840                                     _PREPARE(_access) },                        \
+ * 1841         .mem_mark_cmd           = _mark_cmd,                                    \
+ * 1842         .guest_test             = _access,                                      \
+ * 1843         .guest_test_check       = { _CHECK(_with_af) },                         \
+ * 1844         .expected_events        = { 0 },                                        \
+ * 1845 }
+ *
+ * #define _PREPARE_guest_read64           NULL
+ * #define _PREPARE_with_af                guest_set_ha, guest_clear_pte_af
+ *
+ * #define _CHECK_with_af                  guest_check_pte_af
+ *
+ * 实际是
+ *
+ * name = guest_read64_SCAT2(with_af, "CMD_NONE")
+ * guest_prepare = guest_set_ha, guest_clear_pte_af
+ * mem_mark_cmd = CMD_NONE
+ * guest_test = guest_read64
+ * guest_test_check = guest_check_pte_af
+ * expected_events = {0}
+ *
+ * 在以下使用guest_code():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|812| <<run_test>> vcpu = vm_vcpu_add(vm, 0, guest_code);
+ */
 static void guest_code(struct test_desc *test)
 {
+	/*
+	 * 在以下使用CMD_SKIP_TEST:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|642| <<guest_code>> GUEST_SYNC(CMD_SKIP_TEST);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|881| <<handle_cmd>> if (cmd == CMD_SKIP_TEST) --> continue_test = false;
+	 */
 	if (!guest_prepare(test))
 		GUEST_SYNC(CMD_SKIP_TEST);
 
+	/*
+	 * #define CMD_NONE                                (0)
+	 * #define CMD_SKIP_TEST                           (1ULL << 1)
+	 * #define CMD_HOLE_PT                             (1ULL << 2)
+	 * #define CMD_HOLE_DATA                           (1ULL << 3)
+	 * #define CMD_CHECK_WRITE_IN_DIRTY_LOG            (1ULL << 4)
+	 * #define CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG         (1ULL << 5)
+	 * #define CMD_CHECK_NO_WRITE_IN_DIRTY_LOG         (1ULL << 6)
+	 * #define CMD_CHECK_NO_S1PTW_WR_IN_DIRTY_LOG      (1ULL << 7)
+	 * #define CMD_SET_PTE_AF                          (1ULL << 8)
+	 *
+	 * 在以下使用test_desc->mem_mark_cmd:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|344| <<guest_code>> GUEST_SYNC(test->mem_mark_cmd);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|902| <<TEST_ACCESS>> .mem_mark_cmd = _mark_cmd, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|915| <<TEST_UFFD>> .mem_mark_cmd = _mark_cmd, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|943| <<TEST_UFFD_AND_DIRTY_LOG>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1003| <<TEST_RO_MEMSLOT_AND_UFFD>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1019| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .mem_mark_cmd = CMD_HOLE_DATA | CMD_HOLE_PT, \
+	 */
 	GUEST_SYNC(test->mem_mark_cmd);
 
+	/*
+	 * 在以下使用test_desc->guest_test:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|420| <<guest_code>> if (test->guest_test)
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|421| <<guest_code>> test->guest_test();
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1003| <<TEST_ACCESS>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1014| <<TEST_UFFD>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1029| <<TEST_DIRTY_LOG>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1042| <<TEST_UFFD_AND_DIRTY_LOG>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1056| <<TEST_RO_MEMSLOT>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1067| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1079| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1091| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1105| <<TEST_RO_MEMSLOT_AND_UFFD>> .guest_test = _access, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1121| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .guest_test = _access, \
+	 */
 	if (test->guest_test)
 		test->guest_test();
 
@@ -290,11 +943,19 @@ static void guest_code(struct test_desc *test)
 	GUEST_DONE();
 }
 
+/*
+ * 在以下使用no_dabt_handler():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1254| <<setup_abort_handlers>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_DABT, no_dabt_handler);
+ */
 static void no_dabt_handler(struct ex_regs *regs)
 {
 	GUEST_FAIL("Unexpected dabt, far_el1 = 0x%lx", read_sysreg(far_el1));
 }
 
+/*
+ * 在以下使用no_iabt_handler():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1256| <<setup_abort_handlers>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_IABT, no_iabt_handler);
+ */
 static void no_iabt_handler(struct ex_regs *regs)
 {
 	GUEST_FAIL("Unexpected iabt, pc = 0x%lx", regs->pc);
@@ -306,6 +967,11 @@ static struct uffd_args {
 	uint64_t paging_size;
 } pt_args, data_args;
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|922| <<uffd_pt_handler>> return uffd_generic_handler(mode, uffd, msg, &pt_args);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|927| <<uffd_data_handler>> return uffd_generic_handler(mode, uffd, msg, &data_args);
+ */
 /* Returns true to continue the test, and false if it should be skipped. */
 static int uffd_generic_handler(int uffd_mode, int uffd, struct uffd_msg *msg,
 				struct uffd_args *args)
@@ -334,7 +1000,21 @@ static int uffd_generic_handler(int uffd_mode, int uffd, struct uffd_msg *msg,
 		return ret;
 	}
 
+	/*
+	 * 在以下使用event_cnt->uffd_faults_mutex:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|529| <<uffd_generic_handler>> pthread_mutex_lock(&events.uffd_faults_mutex);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|531| <<uffd_generic_handler>> pthread_mutex_unlock(&events.uffd_faults_mutex);
+	 */
 	pthread_mutex_lock(&events.uffd_faults_mutex);
+	/*
+	 * 在以下使用event_cnt->uffd_faults:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|530| <<uffd_generic_handler>> events.uffd_faults += 1;
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|928| <<check_event_counts>> TEST_ASSERT_EQ(test->expected_events.uffd_faults, events.uffd_faults);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1161| <<TEST_UFFD>> .expected_events = { .uffd_faults = _uffd_faults, }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1189| <<TEST_UFFD_AND_DIRTY_LOG>> .expected_events = { .uffd_faults = _uffd_faults, }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1252| <<TEST_RO_MEMSLOT_AND_UFFD>> .uffd_faults = _uffd_faults }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1268| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .uffd_faults = _uffd_faults }, \
+	 */
 	events.uffd_faults += 1;
 	pthread_mutex_unlock(&events.uffd_faults_mutex);
 	return 0;
@@ -350,6 +1030,11 @@ static int uffd_data_handler(int mode, int uffd, struct uffd_msg *msg)
 	return uffd_generic_handler(mode, uffd, msg, &data_args);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|951| <<setup_uffd>> setup_uffd_args(vm_get_mem_region(vm, MEM_REGION_PT), &pt_args);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|952| <<setup_uffd>> setup_uffd_args(vm_get_mem_region(vm, MEM_REGION_TEST_DATA), &data_args);
+ */
 static void setup_uffd_args(struct userspace_mem_region *region,
 			    struct uffd_args *args)
 {
@@ -361,6 +1046,10 @@ static void setup_uffd_args(struct userspace_mem_region *region,
 	memcpy(args->copy, args->hva, args->paging_size);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1050| <<run_test>> setup_uffd(vm, p, &pt_uffd, &data_uffd);
+ */
 static void setup_uffd(struct kvm_vm *vm, struct test_params *p,
 		       struct uffd_desc **pt_uffd, struct uffd_desc **data_uffd)
 {
@@ -370,6 +1059,14 @@ static void setup_uffd(struct kvm_vm *vm, struct test_params *p,
 	setup_uffd_args(vm_get_mem_region(vm, MEM_REGION_PT), &pt_args);
 	setup_uffd_args(vm_get_mem_region(vm, MEM_REGION_TEST_DATA), &data_args);
 
+	/*
+	 * 在以下使用uffd_setup_demand_paging():
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1021| <<setup_uffd>> *pt_uffd = uffd_setup_demand_paging(uffd_mode, 0,
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1028| <<setup_uffd>> *data_uffd = uffd_setup_demand_paging(uffd_mode, 0,
+	 *   - tools/testing/selftests/kvm/demand_paging_test.c|171| <<run_test>> uffd_descs[i] = uffd_setup_demand_paging(p->uffd_mode, p->uffd_delay, vcpu_hva,
+	 *                                                     vcpu_args->pages * memstress_args.guest_page_size, &handle_uffd_page_request);
+	 */
+
 	*pt_uffd = NULL;
 	if (test->uffd_pt_handler)
 		*pt_uffd = uffd_setup_demand_paging(uffd_mode, 0,
@@ -385,6 +1082,10 @@ static void setup_uffd(struct kvm_vm *vm, struct test_params *p,
 						      test->uffd_data_handler);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1830| <<run_test>> free_uffd(test, pt_uffd, data_uffd);
+ */
 static void free_uffd(struct test_desc *test, struct uffd_desc *pt_uffd,
 		      struct uffd_desc *data_uffd)
 {
@@ -397,12 +1098,26 @@ static void free_uffd(struct test_desc *test, struct uffd_desc *pt_uffd,
 	free(data_args.copy);
 }
 
+/*
+ * 在以下使用uffd_no_handler():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1972| <<global>> TEST_UFFD(guest_at, no_af, CMD_HOLE_DATA | CMD_HOLE_PT, uffd_no_handler, uffd_pt_handler, 1),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|2028| <<global>> TEST_UFFD_AND_DIRTY_LOG(guest_at, with_af, uffd_no_handler, 1, guest_check_no_write_in_dirty_log,
+ *                                                                                 guest_check_s1ptw_wr_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|2103| <<global>> TEST_RO_MEMSLOT_AND_UFFD(guest_at, 0, 0, uffd_no_handler, 1),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|2108| <<global>> TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD(guest_dc_zva, uffd_no_handler, 1),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|2109| <<global>> TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD(guest_st_preidx, uffd_no_handler, 1),
+ */
 static int uffd_no_handler(int mode, int uffd, struct uffd_msg *msg)
 {
 	TEST_FAIL("There was no UFFD fault expected.");
 	return -1;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|885| <<handle_cmd(CMD_HOLE_PT)>> continue_test = punch_hole_in_backing_store(vm, pt_region);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|887| <<handle_cmd(CMD_HOLE_DATA)>> continue_test = punch_hole_in_backing_store(vm, data_region);
+ */
 /* Returns false if the test should be skipped. */
 static bool punch_hole_in_backing_store(struct kvm_vm *vm,
 					struct userspace_mem_region *region)
@@ -412,6 +1127,9 @@ static bool punch_hole_in_backing_store(struct kvm_vm *vm,
 	int ret, fd = region->fd;
 
 	if (fd != -1) {
+		/*
+		 * 似乎FALLOC_FL_PUNCH_HOLE就是把这段清零
+		 */
 		ret = fallocate(fd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				0, paging_size);
 		TEST_ASSERT(ret == 0, "fallocate failed");
@@ -423,6 +1141,12 @@ static bool punch_hole_in_backing_store(struct kvm_vm *vm,
 	return true;
 }
 
+/*
+ * 在以下使用mmio_on_test_gpa_handler():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|2070| <<global>> TEST_RO_MEMSLOT(guest_write64, mmio_on_test_gpa_handler, 1),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|2091| <<global>> TEST_RO_MEMSLOT_AND_DIRTY_LOG(guest_write64, mmio_on_test_gpa_handler, 1, guest_check_no_write_in_dirty_log),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|2113| <<global>> TEST_RO_MEMSLOT_AND_UFFD(guest_write64, mmio_on_test_gpa_handler, 1, uffd_data_handler, 2),
+ */
 static void mmio_on_test_gpa_handler(struct kvm_vm *vm, struct kvm_run *run)
 {
 	struct userspace_mem_region *region;
@@ -434,9 +1158,29 @@ static void mmio_on_test_gpa_handler(struct kvm_vm *vm, struct kvm_run *run)
 	TEST_ASSERT_EQ(run->mmio.phys_addr, region->region.guest_phys_addr);
 
 	memcpy(hva, run->mmio.data, run->mmio.len);
+	/*
+	 * 在以下使用event_cnt->mmio_exits:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|621| <<mmio_on_test_gpa_handler>> events.mmio_exits += 1;
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|921| <<check_event_counts>> TEST_ASSERT_EQ(test->expected_events.mmio_exits, events.mmio_exits);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1180| <<TEST_RO_MEMSLOT>> .expected_events = { .mmio_exits = _mmio_exits }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1204| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .expected_events = { .mmio_exits = _mmio_exits}, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1231| <<TEST_RO_MEMSLOT_AND_UFFD>> .expected_events = { .mmio_exits = _mmio_exits, \
+	 */
 	events.mmio_exits += 1;
 }
 
+/*
+ * 在以下使用test_desc->mmio_handler:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1021| <<setup_default_handlers>> if (!test->mmio_handler)
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1022| <<setup_default_handlers>> test->mmio_handler = mmio_no_handler;
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1100| <<vcpu_run_loop>> test->mmio_handler(vm, run);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1313| <<TEST_RO_MEMSLOT>> .mmio_handler = _mmio_handler, \
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1337| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .mmio_handler = _mmio_handler, \
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1364| <<TEST_RO_MEMSLOT_AND_UFFD>> .mmio_handler = _mmio_handler, \
+ *
+ * 在以下使用mmio_no_handler():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1507| <<setup_default_handlers>> test->mmio_handler = mmio_no_handler;
+ */
 static void mmio_no_handler(struct kvm_vm *vm, struct kvm_run *run)
 {
 	uint64_t data;
@@ -448,12 +1192,24 @@ static void mmio_no_handler(struct kvm_vm *vm, struct kvm_run *run)
 	TEST_FAIL("There was no MMIO exit expected.");
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|889| <<handle_cmd>> TEST_ASSERT(check_write_in_dirty_log(vm, data_region, 0),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|892| <<handle_cmd>> TEST_ASSERT(check_write_in_dirty_log(vm, pt_region, pte_pg),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|895| <<handle_cmd>> TEST_ASSERT(!check_write_in_dirty_log(vm, data_region, 0),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|898| <<handle_cmd>> TEST_ASSERT(!check_write_in_dirty_log(vm, pt_region, pte_pg),
+ */
 static bool check_write_in_dirty_log(struct kvm_vm *vm,
 				     struct userspace_mem_region *region,
 				     uint64_t host_pg_nr)
 {
 	unsigned long *bmap;
 	bool first_page_dirty;
+	/*
+	 * struct userspace_mem_region *region:
+	 * -> struct kvm_userspace_memory_region2 region;
+	 *    -> __u64 memory_size;
+	 */
 	uint64_t size = region->region.memory_size;
 
 	/* getpage_size() is not always equal to vm->page_size */
@@ -464,6 +1220,10 @@ static bool check_write_in_dirty_log(struct kvm_vm *vm,
 	return first_page_dirty;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1021| <<vcpu_run_loop>> if (!handle_cmd(vm, uc.args[1])) {
+ */
 /* Returns true to continue the test, and false if it should be skipped. */
 static bool handle_cmd(struct kvm_vm *vm, int cmd)
 {
@@ -471,14 +1231,40 @@ static bool handle_cmd(struct kvm_vm *vm, int cmd)
 	bool continue_test = true;
 	uint64_t pte_gpa, pte_pg;
 
+	/*
+	 * 在以下调用vm_get_mem_region():
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|768| <<setup_uffd>> setup_uffd_args(vm_get_mem_region(vm, MEM_REGION_PT), &pt_args);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|769| <<setup_uffd>> setup_uffd_args(vm_get_mem_region(vm, MEM_REGION_TEST_DATA), &data_args);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|829| <<mmio_on_test_gpa_handler>> region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|876| <<handle_cmd>> data_region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|877| <<handle_cmd>> pt_region = vm_get_mem_region(vm, MEM_REGION_PT);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|953| <<load_exec_code_for_test>> region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1034| <<setup_gva_maps>> region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1186| <<setup_ucall>> struct userspace_mem_region *region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
+	 */
 	data_region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
 	pt_region = vm_get_mem_region(vm, MEM_REGION_PT);
+	/*
+	 * 在以下调用virt_get_pte_hva():
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|878| <<handle_cmd>> pte_gpa = addr_hva2gpa(vm, virt_get_pte_hva(vm, TEST_GVA));
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1038| <<setup_gva_maps>> pte_gpa = addr_hva2gpa(vm, virt_get_pte_hva(vm, TEST_GVA));
+	 *   - tools/testing/selftests/kvm/include/aarch64/processor.h|134| <<setup_gva_maps>> uint64_t *virt_get_pte_hva(struct kvm_vm *vm, vm_vaddr_t gva);
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|218| <<addr_arch_gva2gpa>> uint64_t *ptep = virt_get_pte_hva(vm, gva);
+	 */
 	pte_gpa = addr_hva2gpa(vm, virt_get_pte_hva(vm, TEST_GVA));
 	pte_pg = (pte_gpa - pt_region->region.guest_phys_addr) / getpagesize();
 
+	/*
+	 * 在以下使用CMD_SKIP_TEST:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|642| <<guest_code>> GUEST_SYNC(CMD_SKIP_TEST);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|881| <<handle_cmd>> if (cmd == CMD_SKIP_TEST) --> continue_test = false;
+	 */
 	if (cmd == CMD_SKIP_TEST)
 		continue_test = false;
 
+	/*
+	 * 似乎FALLOC_FL_PUNCH_HOLE就是把这段清零
+	 */
 	if (cmd & CMD_HOLE_PT)
 		continue_test = punch_hole_in_backing_store(vm, pt_region);
 	if (cmd & CMD_HOLE_DATA)
@@ -486,6 +1272,11 @@ static bool handle_cmd(struct kvm_vm *vm, int cmd)
 	if (cmd & CMD_CHECK_WRITE_IN_DIRTY_LOG)
 		TEST_ASSERT(check_write_in_dirty_log(vm, data_region, 0),
 			    "Missing write in dirty log");
+	/*
+	 * 关于S1PTW
+	 * When 1, indicates the instruction fault came from a second stage
+	 * fault during a first stage translation table walk.
+	 */
 	if (cmd & CMD_CHECK_S1PTW_WR_IN_DIRTY_LOG)
 		TEST_ASSERT(check_write_in_dirty_log(vm, pt_region, pte_pg),
 			    "Missing s1ptw write in dirty log");
@@ -499,11 +1290,29 @@ static bool handle_cmd(struct kvm_vm *vm, int cmd)
 	return continue_test;
 }
 
+/*
+ * 在以下使用test_desc->fail_vcpu_run_handler:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1024| <<setup_default_handlers>> if (!test->fail_vcpu_run_handler)
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1025| <<setup_default_handlers>> test->fail_vcpu_run_handler = fail_vcpu_run_no_handler;
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1082| <<vcpu_run_loop>> test->fail_vcpu_run_handler(ret);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1324| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .fail_vcpu_run_handler = fail_vcpu_run_mmio_no_syndrome_handler, \
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1349| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .fail_vcpu_run_handler = fail_vcpu_run_mmio_no_syndrome_handler, \
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1380| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .fail_vcpu_run_handler = fail_vcpu_run_mmio_no_syndrome_handler, \
+ *
+ * 在以下使用fail_vcpu_run_no_handler():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|958| <<setup_default_handlers>> test->fail_vcpu_run_handler = fail_vcpu_run_no_handler;
+ */
 void fail_vcpu_run_no_handler(int ret)
 {
 	TEST_FAIL("Unexpected vcpu run failure");
 }
 
+/*
+ * 在以下使用fail_vcpu_run_mmio_no_syndrome_handler():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1257| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .fail_vcpu_run_handler = fail_vcpu_run_mmio_no_syndrome_handler, \
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1282| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .fail_vcpu_run_handler = fail_vcpu_run_mmio_no_syndrome_handler, \
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1313| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .fail_vcpu_run_handler = fail_vcpu_run_mmio_no_syndrome_handler, \
+ */
 void fail_vcpu_run_mmio_no_syndrome_handler(int ret)
 {
 	TEST_ASSERT(errno == ENOSYS,
@@ -514,6 +1323,10 @@ void fail_vcpu_run_mmio_no_syndrome_handler(int ret)
 typedef uint32_t aarch64_insn_t;
 extern aarch64_insn_t __exec_test[2];
 
+/*
+ * 没人使用__return_0x77()
+ * 主要使用其中的symbol
+ */
 noinline void __return_0x77(void)
 {
 	asm volatile("__exec_test: mov x0, #0x77\n"
@@ -524,6 +1337,10 @@ noinline void __return_0x77(void)
  * Note that this function runs on the host before the test VM starts: there's
  * no need to sync the D$ and I$ caches.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1096| <<run_test>> load_exec_code_for_test(vm);
+ */
 static void load_exec_code_for_test(struct kvm_vm *vm)
 {
 	uint64_t *code;
@@ -531,6 +1348,22 @@ static void load_exec_code_for_test(struct kvm_vm *vm)
 	void *hva;
 
 	region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
+	/*
+	 * struct kvm_userspace_memory_region2 {
+	 *     __u32 slot;  
+	 *     __u32 flags;
+	 *     __u64 guest_phys_addr;
+	 *     __u64 memory_size;
+	 *     __u64 userspace_addr;
+	 *     __u64 guest_memfd_offset;
+	 *     __u32 guest_memfd;
+	 *     __u32 pad1;
+	 *     __u64 pad2[14];
+	 * };
+	 *
+	 * struct userspace_mem_region *region:
+	 * -> struct kvm_userspace_memory_region2 region;
+	 */
 	hva = (void *)region->region.userspace_addr;
 
 	assert(TEST_EXEC_GVA > TEST_GVA);
@@ -538,18 +1371,58 @@ static void load_exec_code_for_test(struct kvm_vm *vm)
 	memcpy(code, __exec_test, sizeof(__exec_test));
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1051| <<run_test>> setup_abort_handlers(vm, vcpu, test);
+ */
 static void setup_abort_handlers(struct kvm_vm *vm, struct kvm_vcpu *vcpu,
 				 struct test_desc *test)
 {
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/aarch64/arch_timer.c|194| <<test_vm_create>> vm_init_descriptor_tables(vm);
+	 *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|432| <<test_guest_debug_exceptions>> vm_init_descriptor_tables(vm);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|768| <<setup_abort_handlers>> vm_init_descriptor_tables(vm);
+	 *   - tools/testing/selftests/kvm/aarch64/vgic_irq.c|759| <<test_vgic>> vm_init_descriptor_tables(vm);
+	 *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|430| <<create_vpmu_vm>> vm_init_descriptor_tables(vpmu_vm.vm);
+	 */
 	vm_init_descriptor_tables(vm);
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/aarch64/arch_timer.c|209| <<test_vm_create>> vcpu_init_descriptor_tables(vcpus[i]);
+	 *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|433| <<test_guest_debug_exceptions>> vcpu_init_descriptor_tables(vcpu);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|769| <<setup_abort_handlers>> vcpu_init_descriptor_tables(vcpu);
+	 *   - tools/testing/selftests/kvm/aarch64/vgic_irq.c|760| <<test_vgic>> vcpu_init_descriptor_tables(vcpu);
+	 *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|440| <<create_vpmu_vm>> vcpu_init_descriptor_tables(vpmu_vm.vcpu);
+	 *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|539| <<run_access_test>> vcpu_init_descriptor_tables(vcpu);
+	 *
+	 * 初始化vectors的地址寄存器
+	 */
 	vcpu_init_descriptor_tables(vcpu);
 
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|435| <<test_guest_debug_exceptions>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_BRK_INS, guest_sw_bp_handler);
+	 *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|437| <<test_guest_debug_exceptions>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_HW_BP_CURRENT, guest_hw_bp_handler);
+	 *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|439| <<test_guest_debug_exceptions>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_WP_CURRENT, guest_wp_handler);
+	 *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|441| <<test_guest_debug_exceptions>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_SSTEP_CURRENT, guest_ss_handler);
+	 *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|443| <<test_guest_debug_exceptions>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_SVC64, guest_svc_handler);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|774| <<setup_abort_handlers>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_DABT, no_dabt_handler);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|776| <<setup_abort_handlers>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_IABT, no_iabt_handler);
+	 *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|432| <<create_vpmu_vm>> vm_install_sync_handler(vpmu_vm.vm, VECTOR_SYNC_CURRENT, ec, guest_sync_handler);
+	 *
+	 * no_dabt_handler()和no_iabt_handler()返回UCALL_ABORT
+	 */
 	vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT,
 				ESR_EC_DABT, no_dabt_handler);
 	vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT,
 				ESR_EC_IABT, no_iabt_handler);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|770| <<run_test>> setup_gva_maps(vm);
+ */
 static void setup_gva_maps(struct kvm_vm *vm)
 {
 	struct userspace_mem_region *region;
@@ -558,6 +1431,13 @@ static void setup_gva_maps(struct kvm_vm *vm)
 	region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
 	/* Map TEST_GVA first. This will install a new PTE. */
 	virt_pg_map(vm, TEST_GVA, region->region.guest_phys_addr);
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|878| <<handle_cmd>> pte_gpa = addr_hva2gpa(vm, virt_get_pte_hva(vm, TEST_GVA));
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1038| <<setup_gva_maps>> pte_gpa = addr_hva2gpa(vm, virt_get_pte_hva(vm, TEST_GVA));
+	 *   - tools/testing/selftests/kvm/include/aarch64/processor.h|134| <<setup_gva_maps>> uint64_t *virt_get_pte_hva(struct kvm_vm *vm, vm_vaddr_t gva);
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|218| <<addr_arch_gva2gpa>> uint64_t *ptep = virt_get_pte_hva(vm, gva);
+	 */
 	/* Then map TEST_PTE_GVA to the above PTE. */
 	pte_gpa = addr_hva2gpa(vm, virt_get_pte_hva(vm, TEST_GVA));
 	virt_pg_map(vm, TEST_PTE_GVA, pte_gpa);
@@ -569,6 +1449,104 @@ enum pf_test_memslots {
 	TEST_DATA_MEMSLOT,
 };
 
+/*
+ * $ sudo ./page_fault_test -s anonymous_hugetlb
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                          max_gfn=68719476735, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                          max_gfn=17179869183, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                          max_gfn=4294967295, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                          max_gfn=268435455, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                          max_gfn=67108863, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                          max_gfn=16777215, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                          max_gfn=16777215, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                          max_gfn=4194303, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                          max_gfn=1048575, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                          max_gfn=4194303, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                             max_gfn=68719476735, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                             max_gfn=17179869183, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                             max_gfn=4294967295, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                             max_gfn=268435455, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                             max_gfn=67108863, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                             max_gfn=16777215, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                             max_gfn=16777215, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                             max_gfn=4194303, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                             max_gfn=1048575, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                             max_gfn=4194303, data_gpa=0x0000000fffc00000
+ * ... ...
+ * setup_memslots() name=ro_memslot_guest_at, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096, max_gfn=68719476735,
+ *                                            data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=ro_memslot_guest_at, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=17179869183,
+ *                                            data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=ro_memslot_guest_at, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536, max_gfn=4294967295,
+ *                                            data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=ro_memslot_guest_at, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096, max_gfn=268435455, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=ro_memslot_guest_at, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=67108863, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=ro_memslot_guest_at, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536, max_gfn=16777215, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=ro_memslot_guest_at, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096, max_gfn=16777215, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=ro_memslot_guest_at, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=4194303, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=ro_memslot_guest_at, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536, max_gfn=1048575, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=ro_memslot_guest_at, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=4194303, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=ro_memslot_guest_exec, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096, max_gfn=68719476735,
+ *                                              data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=ro_memslot_guest_exec, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=17179869183,
+ *                                              data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=ro_memslot_guest_exec, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536, max_gfn=4294967295,
+ *                                              data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=ro_memslot_guest_exec, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096, max_gfn=268435455,
+ *                                              data_gpa=0x000000ffffc00000
+ * setup_memslots() name=ro_memslot_guest_exec, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=67108863,
+ *                                              data_gpa=0x000000ffffc00000
+ * setup_memslots() name=ro_memslot_guest_exec, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536, max_gfn=16777215,
+ *                                              data_gpa=0x000000ffffc00000
+ * setup_memslots() name=ro_memslot_guest_exec, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096, max_gfn=16777215,
+ *                                              data_gpa=0x0000000fffc00000
+ * setup_memslots() name=ro_memslot_guest_exec, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=4194303,
+*                                               data_gpa=0x0000000fffc00000
+ * setup_memslots() name=ro_memslot_guest_exec, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536, max_gfn=1048575,
+ *                                              data_gpa=0x0000000fffc00000
+ * setup_memslots() name=ro_memslot_guest_exec, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=4194303,
+ *                                              data_gpa=0x0000000fffc00000
+ * setup_memslots() name=ro_memslot_guest_write64, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096, max_gfn=68719476735,
+ *                                              data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=ro_memslot_guest_write64, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=17179869183,
+ *                                              data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=ro_memslot_guest_write64, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536, max_gfn=4294967295,
+ *                                              data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=ro_memslot_guest_write64, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096, max_gfn=268435455,
+ *                                              data_gpa=0x000000ffffc00000
+ * setup_memslots() name=ro_memslot_guest_write64, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=67108863,
+ *                                              data_gpa=0x000000ffffc00000
+ * setup_memslots() name=ro_memslot_guest_write64, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536, max_gfn=16777215,
+ *                                              data_gpa=0x000000ffffc00000
+ * setup_memslots() name=ro_memslot_guest_write64, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096, max_gfn=16777215,
+ *                                              data_gpa=0x0000000fffc00000
+ * setup_memslots() name=ro_memslot_guest_write64, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=4194303,
+ *                                              data_gpa=0x0000000fffc00000
+ * setup_memslots() name=ro_memslot_guest_write64, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536, max_gfn=1048575,
+ *                                              data_gpa=0x0000000fffc00000
+ * setup_memslots() name=ro_memslot_guest_write64, src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384, max_gfn=4194303,
+ *                                              data_gpa=0x0000000fffc00000
+ * ... ...
+ */
 /*
  * Create a memslot for code and data at pfn=0, and test-data and PT ones
  * at max_gfn.
@@ -596,8 +1574,36 @@ static void setup_memslots(struct kvm_vm *vm, struct test_params *p)
 	data_gpa = (max_gfn * guest_page_size) - data_size;
 	data_gpa = align_down(data_gpa, backing_src_pagesz);
 
+	/*
+	 * 1171 void vm_userspace_mem_region_add(struct kvm_vm *vm,
+	 * 1172                                  enum vm_mem_backing_src_type src_type,
+	 * 1173                                  uint64_t guest_paddr, uint32_t slot,
+	 * 1174                                  uint64_t npages, uint32_t flags)
+	 */
+
+	/*
+	 * 一般的代码和数据的size是code_npages
+	 */
 	vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, 0,
 				    CODE_AND_DATA_MEMSLOT, code_npages, 0);
+	/*
+	 * 85 enum kvm_mem_region_type {
+	 * 86         MEM_REGION_CODE,
+	 * 87         MEM_REGION_DATA,
+	 * 88         MEM_REGION_PT,
+	 * 89         MEM_REGION_TEST_DATA,       
+	 * 90         NR_MEM_REGIONS,
+	 * 91 };
+	 *
+	 * 618 enum pf_test_memslots {
+	 * 619         CODE_AND_DATA_MEMSLOT,
+	 * 620         PAGE_TABLE_MEMSLOT,
+	 * 621         TEST_DATA_MEMSLOT,
+	 * 622 };
+	 *
+	 * struct kvm_vm *vm:
+	 * -> uint32_t memslots[NR_MEM_REGIONS];
+	 */
 	vm->memslots[MEM_REGION_CODE] = CODE_AND_DATA_MEMSLOT;
 	vm->memslots[MEM_REGION_DATA] = CODE_AND_DATA_MEMSLOT;
 
@@ -606,19 +1612,39 @@ static void setup_memslots(struct kvm_vm *vm, struct test_params *p)
 				    p->test_desc->pt_memslot_flags);
 	vm->memslots[MEM_REGION_PT] = PAGE_TABLE_MEMSLOT;
 
+	/*
+	 * flags的例子:
+	 * #define KVM_MEM_LOG_DIRTY_PAGES (1UL << 0)
+	 * #define KVM_MEM_READONLY        (1UL << 1)
+	 * #define KVM_MEM_GUEST_MEMFD     (1UL << 2)
+	 */
 	vm_userspace_mem_region_add(vm, p->src_type, data_gpa, TEST_DATA_MEMSLOT,
 				    data_size / guest_page_size,
 				    p->test_desc->data_memslot_flags);
 	vm->memslots[MEM_REGION_TEST_DATA] = TEST_DATA_MEMSLOT;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|811| <<run_test>> setup_ucall(vm);
+ */
 static void setup_ucall(struct kvm_vm *vm)
 {
 	struct userspace_mem_region *region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
 
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|711| <<setup_ucall>> ucall_init(vm, region->region.guest_phys_addr + region->region.memory_size);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|434| <<__vm_create>> ucall_init(vm, slot0->region.guest_phys_addr + slot0->region.memory_size);
+	 *   - tools/testing/selftests/kvm/s390x/cmma_test.c|146| <<finish_vm_setup>> ucall_init(vm, slot0->region.guest_phys_addr + slot0->region.memory_size);
+	 */
 	ucall_init(vm, region->region.guest_phys_addr + region->region.memory_size);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1677| <<run_test>> setup_default_handlers(test);
+ */
 static void setup_default_handlers(struct test_desc *test)
 {
 	if (!test->mmio_handler)
@@ -628,13 +1654,37 @@ static void setup_default_handlers(struct test_desc *test)
 		test->fail_vcpu_run_handler = fail_vcpu_run_no_handler;
 }
 
+/*
+ * called by:
+ *  - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1065| <<run_test>> check_event_counts(test);
+ */
 static void check_event_counts(struct test_desc *test)
 {
+	/*
+	 * 在以下使用test_desc->expected_events:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1034| <<check_event_counts>> TEST_ASSERT_EQ(test->expected_events.uffd_faults, events.uffd_faults);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1035| <<check_event_counts>> TEST_ASSERT_EQ(test->expected_events.mmio_exits, events.mmio_exits);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1036| <<check_event_counts>> TEST_ASSERT_EQ(test->expected_events.fail_vcpu_runs, events.fail_vcpu_runs);
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1261| <<TEST_ACCESS>> .expected_events = { 0 }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1275| <<TEST_UFFD>> .expected_events = { .uffd_faults = _uffd_faults, }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1287| <<TEST_DIRTY_LOG>> .expected_events = { 0 }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1303| <<TEST_UFFD_AND_DIRTY_LOG>> .expected_events = { .uffd_faults = _uffd_faults, }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1314| <<TEST_RO_MEMSLOT>> .expected_events = { .mmio_exits = _mmio_exits }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1325| <<TEST_RO_MEMSLOT_NO_SYNDROME>> .expected_events = { .fail_vcpu_runs = 1 }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1338| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .expected_events = { .mmio_exits = _mmio_exits}, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1350| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_DIRTY_LOG>> .expected_events = { .fail_vcpu_runs = 1 }, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1365| <<TEST_RO_MEMSLOT_AND_UFFD>> .expected_events = { .mmio_exits = _mmio_exits, \
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1381| <<TEST_RO_MEMSLOT_NO_SYNDROME_AND_UFFD>> .expected_events = { .fail_vcpu_runs = 1, \
+	 */
 	TEST_ASSERT_EQ(test->expected_events.uffd_faults, events.uffd_faults);
 	TEST_ASSERT_EQ(test->expected_events.mmio_exits, events.mmio_exits);
 	TEST_ASSERT_EQ(test->expected_events.fail_vcpu_runs, events.fail_vcpu_runs);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1646| <<run_test>> print_test_banner(mode, p);
+ */
 static void print_test_banner(enum vm_guest_mode mode, struct test_params *p)
 {
 	struct test_desc *test = p->test_desc;
@@ -645,8 +1695,20 @@ static void print_test_banner(enum vm_guest_mode mode, struct test_params *p)
 		 vm_mem_backing_src_alias(p->src_type)->name);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1666| <<run_test>> reset_event_counts();
+ */
 static void reset_event_counts(void)
 {
+	/*
+	 * static struct event_cnt {
+	 *     int mmio_exits;
+	 *     int fail_vcpu_runs;
+	 *     int uffd_faults;
+	 *     pthread_mutex_t uffd_faults_mutex;
+	 * } events;
+	 */
 	memset(&events, 0, sizeof(events));
 }
 
@@ -654,6 +1716,10 @@ static void reset_event_counts(void)
  * This function either succeeds, skips the test (after setting test->skip), or
  * fails with a TEST_FAIL that aborts all tests.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|778| <<run_test>> vcpu_run_loop(vm, vcpu, test);
+ */
 static void vcpu_run_loop(struct kvm_vm *vm, struct kvm_vcpu *vcpu,
 			  struct test_desc *test)
 {
@@ -683,6 +1749,15 @@ static void vcpu_run_loop(struct kvm_vm *vm, struct kvm_vcpu *vcpu,
 		case UCALL_DONE:
 			goto done;
 		case UCALL_NONE:
+			/*
+			 * 在以下使用test_desc->mmio_handler:
+			 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1021| <<setup_default_handlers>> if (!test->mmio_handler)
+			 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1022| <<setup_default_handlers>> test->mmio_handler = mmio_no_handler;
+			 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1100| <<vcpu_run_loop>> test->mmio_handler(vm, run);
+			 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1313| <<TEST_RO_MEMSLOT>> .mmio_handler = _mmio_handler, \
+			 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1337| <<TEST_RO_MEMSLOT_AND_DIRTY_LOG>> .mmio_handler = _mmio_handler, \
+			 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1364| <<TEST_RO_MEMSLOT_AND_UFFD>> .mmio_handler = _mmio_handler, \
+			 */
 			if (run->exit_reason == KVM_EXIT_MMIO)
 				test->mmio_handler(vm, run);
 			break;
@@ -695,6 +1770,10 @@ static void vcpu_run_loop(struct kvm_vm *vm, struct kvm_vcpu *vcpu,
 	pr_debug(test->skip ? "Skipped.\n" : "Done.\n");
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1156| <<for_each_test_and_guest_mode>> for_each_guest_mode(run_test, &p);
+ */
 static void run_test(enum vm_guest_mode mode, void *arg)
 {
 	struct test_params *p = (struct test_params *)arg;
@@ -703,11 +1782,33 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 	struct kvm_vcpu *vcpu;
 	struct uffd_desc *pt_uffd, *data_uffd;
 
+	/*
+	 * 例子:
+	 * Testing guest mode: PA-bits:48,  VA-bits:48,  4K pages
+	 * Testing memory backing src type: anonymous_hugetlb
+	 * Testing guest mode: PA-bits:48,  VA-bits:48, 16K pages
+	 * Testing memory backing src type: anonymous_hugetlb
+	 * Testing guest mode: PA-bits:48,  VA-bits:48, 64K pages
+	 * Testing memory backing src type: anonymous_hugetlb
+	 * Testing guest mode: PA-bits:40,  VA-bits:48,  4K pages
+	 * Testing memory backing src type: anonymous_hugetlb
+	 * Testing guest mode: PA-bits:40,  VA-bits:48, 16K pages
+	 */
 	print_test_banner(mode, p);
 
 	vm = ____vm_create(VM_SHAPE(mode));
 	setup_memslots(vm, p);
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|810| <<run_test>> kvm_vm_elf_load(vm, program_invocation_name);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|425| <<__vm_create>> kvm_vm_elf_load(vm, program_invocation_name);
+	 *   - tools/testing/selftests/kvm/s390x/cmma_test.c|143| <<finish_vm_setup>> kvm_vm_elf_load(vm, program_invocation_name);
+	 */
 	kvm_vm_elf_load(vm, program_invocation_name);
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|811| <<run_test>> setup_ucall(vm);
+	 */
 	setup_ucall(vm);
 	vcpu = vm_vcpu_add(vm, 0, guest_code);
 
@@ -766,12 +1867,79 @@ static void help(char *name)
 #define _PREPARE_guest_dc_zva		guest_check_dc_zva
 #define _PREPARE_guest_cas		guest_check_lse
 
+/*
+ * 注释:
+ * https://developer.arm.com/documentation/den0024/a/The-Memory-Management-Unit/Operating-system-use-of-translation-table-descriptors
+ *
+ * Another memory attribute bit in the descriptor, the Access Flag (AF),
+ * indicates when a block entry is used for the first time.
+ *
+ * AF = 0: This block entry has not yet been used.
+ *
+ * AF = 1: This block entry has been used.
+ *
+ * Operating systems use an access flag bit to keep track of which pages are
+ * being used. Software manages the flag. When the page is first created, its
+ * entry has AF set to 0. The first time the page is accessed by code, if it
+ * has AF at 0, this triggers an MMU fault. The Page fault handler records that
+ * this page is now being used and manually sets the AF bit in the table entry.
+ * For example, the Linux kernel uses the [AF] bit for PTE_AF on ARM64 (the
+ * Linux kernel name for AArch64), which is used to check whether a page has
+ * ever been accessed. This influences some of the kernel memory management
+ * choices. For example, when a page must be swapped out of memory, it is less
+ * likely to swap out pages that are being actively used.
+ *
+ * Bits [58:55] of the descriptor are marked as Reserved for Software Use and
+ * can be used to record OS-specific information in the translation tables. For
+ * example, the Linux kernel uses one of these bits to mark an entry as clean
+ * or dirty. The dirty status records whether the page has been written to. If
+ * the page is later swapped out of memory, a clean page can simply be
+ * discarded, but a dirty page must have its contents saved first.
+ */
 /* With or without access flag checks */
 #define _PREPARE_with_af		guest_set_ha, guest_clear_pte_af
 #define _PREPARE_no_af			NULL
 #define _CHECK_with_af			guest_check_pte_af
 #define _CHECK_no_af			NULL
 
+/*
+ * 67 struct test_desc {
+ * 68         const char *name;
+ * 69         uint64_t mem_mark_cmd;
+ * 70         // Skip the test if any prepare function returns false
+ * 71         bool (*guest_prepare[PREPARE_FN_NR])(void);
+ * 72         void (*guest_test)(void);
+ * 73         void (*guest_test_check[CHECK_FN_NR])(void);
+ * 74         uffd_handler_t uffd_pt_handler;
+ * 75         uffd_handler_t uffd_data_handler;
+ * 76         void (*dabt_handler)(struct ex_regs *regs);
+ * 77         void (*iabt_handler)(struct ex_regs *regs);
+ * 78         void (*mmio_handler)(struct kvm_vm *vm, struct kvm_run *run);
+ * 79         void (*fail_vcpu_run_handler)(int ret);
+ * 80         uint32_t pt_memslot_flags;
+ * 81         uint32_t data_memslot_flags;
+ * 82         bool skip;
+ * 83         struct event_cnt expected_events;
+ * 84 };
+ */
+
+/*
+ * 在以下使用TEST_ACCESS():
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1152| <<global>> TEST_ACCESS(guest_read64, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1153| <<global>> TEST_ACCESS(guest_ld_preidx, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1154| <<global>> TEST_ACCESS(guest_cas, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1155| <<global>> TEST_ACCESS(guest_write64, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1156| <<global>> TEST_ACCESS(guest_st_preidx, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1157| <<global>> TEST_ACCESS(guest_dc_zva, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1158| <<global>> TEST_ACCESS(guest_exec, with_af, CMD_NONE),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1169| <<global>> TEST_ACCESS(guest_read64, no_af, CMD_HOLE_DATA),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1170| <<global>> TEST_ACCESS(guest_cas, no_af, CMD_HOLE_DATA),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1171| <<global>> TEST_ACCESS(guest_ld_preidx, no_af, CMD_HOLE_DATA),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1172| <<global>> TEST_ACCESS(guest_write64, no_af, CMD_HOLE_DATA),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1173| <<global>> TEST_ACCESS(guest_st_preidx, no_af, CMD_HOLE_DATA),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1174| <<global>> TEST_ACCESS(guest_at, no_af, CMD_HOLE_DATA),
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1175| <<global>> TEST_ACCESS(guest_dc_zva, no_af, CMD_HOLE_DATA),
+ */
 /* Performs an access and checks that no faults were triggered. */
 #define TEST_ACCESS(_access, _with_af, _mark_cmd)				\
 {										\
@@ -905,6 +2073,26 @@ static void help(char *name)
 				    .uffd_faults = _uffd_faults },		\
 }
 
+/*
+ * 67 struct test_desc {
+ * 68         const char *name;
+ * 69         uint64_t mem_mark_cmd;
+ * 70         // Skip the test if any prepare function returns false
+ * 71         bool (*guest_prepare[PREPARE_FN_NR])(void);
+ * 72         void (*guest_test)(void);
+ * 73         void (*guest_test_check[CHECK_FN_NR])(void);
+ * 74         uffd_handler_t uffd_pt_handler;
+ * 75         uffd_handler_t uffd_data_handler;
+ * 76         void (*dabt_handler)(struct ex_regs *regs);
+ * 77         void (*iabt_handler)(struct ex_regs *regs);
+ * 78         void (*mmio_handler)(struct kvm_vm *vm, struct kvm_run *run);
+ * 79         void (*fail_vcpu_run_handler)(int ret);
+ * 80         uint32_t pt_memslot_flags;
+ * 81         uint32_t data_memslot_flags;
+ * 82         bool skip;
+ * 83         struct event_cnt expected_events;
+ * 84 };
+ */
 static struct test_desc tests[] = {
 
 	/* Check that HW is setting the Access Flag (AF) (sanity checks). */
@@ -1092,10 +2280,59 @@ static struct test_desc tests[] = {
 	{ 0 }
 };
 
+/*
+ * 一些例子:
+ * for_each_test_and_guest_mode() name=guest_read64_SCAT2(with_af, "CMD_NONE")
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                                                max_gfn=68719476735, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                                                max_gfn=17179869183, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                                                max_gfn=4294967295, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                                                max_gfn=268435455, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                                                max_gfn=67108863, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                                                max_gfn=16777215, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                                                max_gfn=16777215, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                                                max_gfn=4194303, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                                                max_gfn=1048575, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_read64_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                                                max_gfn=4194303, data_gpa=0x0000000fffc00000
+ * for_each_test_and_guest_mode() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE")
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                                                   max_gfn=68719476735, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                                                   max_gfn=17179869183, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                                                   max_gfn=4294967295, data_gpa=0x0000ffffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                                                   max_gfn=268435455, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                                                   max_gfn=67108863, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                                                   max_gfn=16777215, data_gpa=0x000000ffffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=4096,
+ *                                                                   max_gfn=16777215, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                                                   max_gfn=4194303, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=65536,
+ *                                                                   max_gfn=1048575, data_gpa=0x0000000fffc00000
+ * setup_memslots() name=guest_ld_preidx_SCAT2(with_af, "CMD_NONE"), src_type=2 code_npages=512, pt_size=2097152, data_size=2097152, backing_src_pagesz=2097152, guest_page_size=16384,
+ *                                                                   max_gfn=4194303, data_gpa=0x0000000fffc00000
+ * ... ...
+ */
 static void for_each_test_and_guest_mode(enum vm_mem_backing_src_type src_type)
 {
 	struct test_desc *t;
 
+	/*
+	 * 上面定义的static struct test_desc tests[] = {
+	 */
 	for (t = &tests[0]; t->name; t++) {
 		if (t->skip)
 			continue;
@@ -1105,6 +2342,27 @@ static void for_each_test_and_guest_mode(enum vm_mem_backing_src_type src_type)
 			.test_desc = t,
 		};
 
+		/*
+		 * 205 enum vm_guest_mode {
+		 * 206         VM_MODE_P52V48_4K,
+		 * 207         VM_MODE_P52V48_16K,
+		 * 208         VM_MODE_P52V48_64K,
+		 * 209         VM_MODE_P48V48_4K,
+		 * 210         VM_MODE_P48V48_16K,
+		 * 211         VM_MODE_P48V48_64K,
+		 * 212         VM_MODE_P40V48_4K,
+		 * 213         VM_MODE_P40V48_16K,
+		 * 214         VM_MODE_P40V48_64K,
+		 * 215         VM_MODE_PXXV48_4K,      // For 48bits VA but ANY bits PA
+		 * 216         VM_MODE_P47V64_4K,
+		 * 217         VM_MODE_P44V64_4K,
+		 * 218         VM_MODE_P36V48_4K,
+		 * 219         VM_MODE_P36V48_16K,
+		 * 220         VM_MODE_P36V48_64K,
+		 * 221         VM_MODE_P36V47_16K,
+		 * 222         NUM_VM_MODES,
+		 * 223 };
+		 */
 		for_each_guest_mode(run_test, &p);
 	}
 }
@@ -1119,6 +2377,17 @@ int main(int argc, char *argv[])
 	while ((opt = getopt(argc, argv, "hm:s:")) != -1) {
 		switch (opt) {
 		case 'm':
+			/*
+			 * called by:
+			 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1485| <<main>> guest_modes_cmdline(optarg);
+			 *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|365| <<main>> guest_modes_cmdline(optarg);
+			 *   - tools/testing/selftests/kvm/demand_paging_test.c|245| <<main>> guest_modes_cmdline(optarg);
+			 *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|386| <<main>> guest_modes_cmdline(optarg);
+			 *   - tools/testing/selftests/kvm/dirty_log_test.c|898| <<main>> guest_modes_cmdline(optarg);
+			 *   - tools/testing/selftests/kvm/include/guest_modes.h|21| <<main>> void guest_modes_cmdline(const char *arg);
+			 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|456| <<main>> guest_modes_cmdline(optarg);
+			 *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|152| <<main>> guest_modes_cmdline(optarg);
+			 */
 			guest_modes_cmdline(optarg);
 			break;
 		case 's':
diff --git a/tools/testing/selftests/kvm/include/kvm_util_base.h b/tools/testing/selftests/kvm/include/kvm_util_base.h
index 3e0db283a..97a21c474 100644
--- a/tools/testing/selftests/kvm/include/kvm_util_base.h
+++ b/tools/testing/selftests/kvm/include/kvm_util_base.h
@@ -113,6 +113,20 @@ struct kvm_vm {
 	vm_vaddr_t gdt;
 	vm_vaddr_t tss;
 	vm_vaddr_t idt;
+	/*
+	 * 在以下使用kvm_vm->handlers:
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|496| <<vm_init_descriptor_tables>> vm->handlers = __vm_vaddr_alloc(vm, sizeof(struct handlers),
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|499| <<vm_init_descriptor_tables>> *(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|505| <<vm_install_sync_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|516| <<vm_install_exception_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|435| <<vm_init_vector_tables>> vm->handlers = __vm_vaddr_alloc(vm, sizeof(struct handlers),
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|438| <<vm_init_vector_tables>> *(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|443| <<vm_install_exception_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|451| <<vm_install_interrupt_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|1148| <<vm_init_descriptor_tables>> vm->handlers = __vm_vaddr_alloc_page(vm, MEM_REGION_DATA);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|1167| <<vcpu_init_descriptor_tables>> *(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|1173| <<vm_install_exception_handler>> vm_vaddr_t *handlers = (vm_vaddr_t *)addr_gva2hva(vm, vm->handlers);
+	 */
 	vm_vaddr_t handlers;
 	uint32_t dirty_ring_size;
 	uint64_t gpa_tag_mask;
@@ -163,6 +177,17 @@ struct vcpu_reg_list {
 struct userspace_mem_region *
 memslot2region(struct kvm_vm *vm, uint32_t memslot);
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|768| <<setup_uffd>> setup_uffd_args(vm_get_mem_region(vm, MEM_REGION_PT), &pt_args);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|769| <<setup_uffd>> setup_uffd_args(vm_get_mem_region(vm, MEM_REGION_TEST_DATA), &data_args);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|829| <<mmio_on_test_gpa_handler>> region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|876| <<handle_cmd>> data_region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|877| <<handle_cmd>> pt_region = vm_get_mem_region(vm, MEM_REGION_PT);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|953| <<load_exec_code_for_test>> region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1034| <<setup_gva_maps>> region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1186| <<setup_ucall>> struct userspace_mem_region *region = vm_get_mem_region(vm, MEM_REGION_TEST_DATA);
+ */
 static inline struct userspace_mem_region *vm_get_mem_region(struct kvm_vm *vm,
 							     enum kvm_mem_region_type type)
 {
@@ -455,6 +480,14 @@ int kvm_memfd_alloc(size_t size, bool hugepages);
 
 void vm_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent);
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|859| <<check_write_in_dirty_log>> kvm_vm_get_dirty_log(vm, region->region.slot, bmap);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|235| <<dirty_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|242| <<clear_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|358| <<memstress_get_dirty_log>> kvm_vm_get_dirty_log(vm, slot, bitmaps[i]);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_dirty_log_test.c|149| <<test_vmx_dirty_log>> kvm_vm_get_dirty_log(vm, TEST_MEM_SLOT_INDEX, bmap);
+ */
 static inline void kvm_vm_get_dirty_log(struct kvm_vm *vm, int slot, void *log)
 {
 	struct kvm_dirty_log args = { .dirty_bitmap = log, .slot = slot };
@@ -701,6 +734,33 @@ static inline void vcpu_get_reg(struct kvm_vcpu *vcpu, uint64_t id, void *addr)
 
 	vcpu_ioctl(vcpu, KVM_GET_ONE_REG, &reg);
 }
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/aarch32_id_regs.c|107| <<test_user_raz_wi>> vcpu_set_reg(vcpu, reg_id, BAD_ID_REG_VAL);
+ *   - tools/testing/selftests/kvm/aarch64/set_id_regs.c|346| <<test_reg_set_success>> vcpu_set_reg(vcpu, reg, val);
+ *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|499| <<test_create_vpmu_vm_with_pmcr_n>> vcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_PMCR_EL0), pmcr);
+ *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|540| <<run_access_test>> vcpu_set_reg(vcpu, ARM64_CORE_REG(sp_el1), sp);
+ *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|541| <<run_access_test>> vcpu_set_reg(vcpu, ARM64_CORE_REG(regs.pc), (uint64_t)guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|594| <<run_pmregs_validity_test>> vcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(set_reg_id), max_counters_mask);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|280| <<aarch64_vcpu_setup>> vcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_CPACR_EL1), 3 << 20);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|350| <<aarch64_vcpu_setup>> vcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_SCTLR_EL1), sctlr_el1);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|351| <<aarch64_vcpu_setup>> vcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_TCR_EL1), tcr_el1);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|352| <<aarch64_vcpu_setup>> vcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_MAIR_EL1), DEFAULT_MAIR_EL1);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|353| <<aarch64_vcpu_setup>> vcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_TTBR0_EL1), ttbr0_el1);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|354| <<aarch64_vcpu_setup>> vcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_TPIDR_EL1), vcpu->id);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|370| <<vcpu_arch_set_entry_point>> vcpu_set_reg(vcpu, ARM64_CORE_REG(regs.pc), (uint64_t)guest_code);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|388| <<__aarch64_vcpu_add>> vcpu_set_reg(vcpu, ARM64_CORE_REG(sp_el1), stack_vaddr + stack_size);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|418| <<vcpu_args_set>> vcpu_set_reg(vcpu, ARM64_CORE_REG(regs.regs[i]),
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|458| <<vcpu_init_descriptor_tables>> vcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_VBAR_EL1), (uint64_t)&vectors);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|216| <<riscv_vcpu_mmu_setup>> vcpu_set_reg(vcpu, RISCV_GENERAL_CSR_REG(satp), satp);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|294| <<vcpu_arch_set_entry_point>> vcpu_set_reg(vcpu, RISCV_CORE_REG(regs.pc), (unsigned long )guest_code);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|327| <<vm_arch_vcpu_add>> vcpu_set_reg(vcpu, RISCV_CORE_REG(regs.gp), current_gp);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|330| <<vm_arch_vcpu_add>> vcpu_set_reg(vcpu, RISCV_CORE_REG(regs.sp), stack_vaddr + stack_size);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|333| <<vm_arch_vcpu_add>> vcpu_set_reg(vcpu, RISCV_GENERAL_CSR_REG(sscratch), vcpu_id);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|336| <<vm_arch_vcpu_add>> vcpu_set_reg(vcpu, RISCV_GENERAL_CSR_REG(stvec), (unsigned long )guest_unexp_trap);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|379| <<vcpu_args_set>> vcpu_set_reg(vcpu, id, va_arg(ap, uint64_t));
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|430| <<vcpu_init_vector_tables>> vcpu_set_reg(vcpu, RISCV_GENERAL_CSR_REG(stvec), (unsigned long )&exception_vectors);
+ */
 static inline void vcpu_set_reg(struct kvm_vcpu *vcpu, uint64_t id, uint64_t val)
 {
 	struct kvm_one_reg reg = { .id = id, .addr = (uint64_t)&val };
diff --git a/tools/testing/selftests/kvm/lib/aarch64/processor.c b/tools/testing/selftests/kvm/lib/aarch64/processor.c
index a9eb17295..7e67c723c 100644
--- a/tools/testing/selftests/kvm/lib/aarch64/processor.c
+++ b/tools/testing/selftests/kvm/lib/aarch64/processor.c
@@ -175,6 +175,13 @@ void virt_arch_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr)
 	_virt_pg_map(vm, vaddr, paddr, attr_idx);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|878| <<handle_cmd>> pte_gpa = addr_hva2gpa(vm, virt_get_pte_hva(vm, TEST_GVA));
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1038| <<setup_gva_maps>> pte_gpa = addr_hva2gpa(vm, virt_get_pte_hva(vm, TEST_GVA));
+ *   - tools/testing/selftests/kvm/include/aarch64/processor.h|134| <<setup_gva_maps>> uint64_t *virt_get_pte_hva(struct kvm_vm *vm, vm_vaddr_t gva);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|218| <<addr_arch_gva2gpa>> uint64_t *ptep = virt_get_pte_hva(vm, gva);
+ */
 uint64_t *virt_get_pte_hva(struct kvm_vm *vm, vm_vaddr_t gva)
 {
 	uint64_t *ptep;
@@ -451,13 +458,35 @@ struct handlers {
 	handler_fn exception_handlers[VECTOR_NUM][ESR_EC_NUM];
 };
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/arch_timer.c|209| <<test_vm_create>> vcpu_init_descriptor_tables(vcpus[i]);
+ *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|433| <<test_guest_debug_exceptions>> vcpu_init_descriptor_tables(vcpu);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|769| <<setup_abort_handlers>> vcpu_init_descriptor_tables(vcpu);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_irq.c|760| <<test_vgic>> vcpu_init_descriptor_tables(vcpu);
+ *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|440| <<create_vpmu_vm>> vcpu_init_descriptor_tables(vpmu_vm.vcpu);
+ *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|539| <<run_access_test>> vcpu_init_descriptor_tables(vcpu);
+ */
 void vcpu_init_descriptor_tables(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * tools/testing/selftests/kvm/lib/aarch64/handlers.S
+	 */
 	extern char vectors;
 
 	vcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_VBAR_EL1), (uint64_t)&vectors);
 }
 
+/*
+ * tools/testing/selftests/kvm/lib/aarch64/handlers.S:
+ *  75 .macro HANDLER, label
+ *  76 handler_\label:
+ *  77         save_registers
+ *  78         mov     x0, sp
+ *  79         mov     x1, #vector
+ *  80         bl      route_exception
+ *  81         restore_registers
+ */
 void route_exception(struct ex_regs *regs, int vector)
 {
 	struct handlers *handlers = (struct handlers *)exception_handlers;
@@ -491,17 +520,70 @@ void route_exception(struct ex_regs *regs, int vector)
 	kvm_exit_unexpected_exception(vector, ec, valid_ec);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/arch_timer.c|194| <<test_vm_create>> vm_init_descriptor_tables(vm);
+ *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|432| <<test_guest_debug_exceptions>> vm_init_descriptor_tables(vm);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|768| <<setup_abort_handlers>> vm_init_descriptor_tables(vm);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_irq.c|759| <<test_vgic>> vm_init_descriptor_tables(vm);
+ *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|430| <<create_vpmu_vm>> vm_init_descriptor_tables(vpmu_vm.vm);
+ */
 void vm_init_descriptor_tables(struct kvm_vm *vm)
 {
+	/*
+	 * 在以下使用kvm_vm->handlers:
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|496| <<vm_init_descriptor_tables>> vm->handlers = __vm_vaddr_alloc(vm, sizeof(struct handlers),
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|499| <<vm_init_descriptor_tables>> *(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|505| <<vm_install_sync_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|516| <<vm_install_exception_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|435| <<vm_init_vector_tables>> vm->handlers = __vm_vaddr_alloc(vm, sizeof(struct handlers),
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|438| <<vm_init_vector_tables>> *(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|443| <<vm_install_exception_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|451| <<vm_install_interrupt_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|1148| <<vm_init_descriptor_tables>> vm->handlers = __vm_vaddr_alloc_page(vm, MEM_REGION_DATA);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|1167| <<vcpu_init_descriptor_tables>> *(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|1173| <<vm_install_exception_handler>> vm_vaddr_t *handlers = (vm_vaddr_t *)addr_gva2hva(vm, vm->handlers);
+	 *
+	 * struct kvm_vm *vm:
+	 * -> vm_vaddr_t handlers;
+	 */
 	vm->handlers = __vm_vaddr_alloc(vm, sizeof(struct handlers),
 					vm->page_size, MEM_REGION_DATA);
 
 	*(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|435| <<test_guest_debug_exceptions>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_BRK_INS, guest_sw_bp_handler);
+ *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|437| <<test_guest_debug_exceptions>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_HW_BP_CURRENT, guest_hw_bp_handler);
+ *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|439| <<test_guest_debug_exceptions>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_WP_CURRENT, guest_wp_handler);
+ *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|441| <<test_guest_debug_exceptions>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_SSTEP_CURRENT, guest_ss_handler);
+ *   - tools/testing/selftests/kvm/aarch64/debug-exceptions.c|443| <<test_guest_debug_exceptions>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_SVC64, guest_svc_handler);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|774| <<setup_abort_handlers>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_DABT, no_dabt_handler);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|776| <<setup_abort_handlers>> vm_install_sync_handler(vm, VECTOR_SYNC_CURRENT, ESR_EC_IABT, no_iabt_handler);
+ *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|432| <<create_vpmu_vm>> vm_install_sync_handler(vpmu_vm.vm, VECTOR_SYNC_CURRENT, ec, guest_sync_handler);
+ */
 void vm_install_sync_handler(struct kvm_vm *vm, int vector, int ec,
 			 void (*handler)(struct ex_regs *))
 {
+	/*
+	 * 在以下使用kvm_vm->handlers:
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|496| <<vm_init_descriptor_tables>> vm->handlers = __vm_vaddr_alloc(vm, sizeof(struct handlers),
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|499| <<vm_init_descriptor_tables>> *(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|505| <<vm_install_sync_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|516| <<vm_install_exception_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|435| <<vm_init_vector_tables>> vm->handlers = __vm_vaddr_alloc(vm, sizeof(struct handlers),
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|438| <<vm_init_vector_tables>> *(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|443| <<vm_install_exception_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|451| <<vm_install_interrupt_handler>> struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|1148| <<vm_init_descriptor_tables>> vm->handlers = __vm_vaddr_alloc_page(vm, MEM_REGION_DATA);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|1167| <<vcpu_init_descriptor_tables>> *(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|1173| <<vm_install_exception_handler>> vm_vaddr_t *handlers = (vm_vaddr_t *)addr_gva2hva(vm, vm->handlers);
+	 *
+	 * struct kvm_vm *vm:
+	 * -> vm_vaddr_t handlers;
+	 */
 	struct handlers *handlers = addr_gva2hva(vm, vm->handlers);
 
 	assert(VECTOR_IS_SYNC(vector));
diff --git a/tools/testing/selftests/kvm/lib/elf.c b/tools/testing/selftests/kvm/lib/elf.c
index f34d926d9..d96b8eba3 100644
--- a/tools/testing/selftests/kvm/lib/elf.c
+++ b/tools/testing/selftests/kvm/lib/elf.c
@@ -111,6 +111,12 @@ static void elfhdr_get(const char *filename, Elf64_Ehdr *hdrp)
  * by the image and it needs to have sufficient available physical pages, to
  * back the virtual pages used to load the image.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|810| <<run_test>> kvm_vm_elf_load(vm, program_invocation_name);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|425| <<__vm_create>> kvm_vm_elf_load(vm, program_invocation_name);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|143| <<finish_vm_setup>> kvm_vm_elf_load(vm, program_invocation_name);
+ */
 void kvm_vm_elf_load(struct kvm_vm *vm, const char *filename)
 {
 	off_t offset, offset_rv;
diff --git a/tools/testing/selftests/kvm/lib/guest_modes.c b/tools/testing/selftests/kvm/lib/guest_modes.c
index b04901e55..84c4bda35 100644
--- a/tools/testing/selftests/kvm/lib/guest_modes.c
+++ b/tools/testing/selftests/kvm/lib/guest_modes.c
@@ -111,6 +111,17 @@ void guest_modes_help(void)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1485| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|365| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|245| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|386| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|898| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/include/guest_modes.h|21| <<main>> void guest_modes_cmdline(const char *arg);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|456| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|152| <<main>> guest_modes_cmdline(optarg);
+ */
 void guest_modes_cmdline(const char *arg)
 {
 	static bool mode_selected;
diff --git a/tools/testing/selftests/kvm/lib/kvm_util.c b/tools/testing/selftests/kvm/lib/kvm_util.c
index b2262b5fa..087bacf74 100644
--- a/tools/testing/selftests/kvm/lib/kvm_util.c
+++ b/tools/testing/selftests/kvm/lib/kvm_util.c
@@ -907,6 +907,12 @@ static void vm_userspace_mem_region_hva_insert(struct rb_root *hva_tree,
 }
 
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|927| <<vm_set_user_memory_region>> int ret = __vm_set_user_memory_region(vm, slot, flags, gpa, size, hva);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|354| <<test_invalid_memory_region_flags>> r = __vm_set_user_memory_region(vm, 0, BIT(i), 0, MEM_REGION_SIZE, NULL);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|436| <<test_add_max_memory_regions>> ret = __vm_set_user_memory_region(vm, max_mem_slots, 0, (uint64_t)max_mem_slots * MEM_REGION_SIZE, MEM_REGION_SIZE, mem_extra);
+ */
 int __vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags,
 				uint64_t gpa, uint64_t size, void *hva)
 {
@@ -921,6 +927,12 @@ int __vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags
 	return ioctl(vm->fd, KVM_SET_USER_MEMORY_REGION, &region);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|238| <<main>> vm_set_user_memory_region(vm, slot, 0, gpa, slot_size, mem);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|279| <<main>> vm_set_user_memory_region(vm, slot, 0, 0, 0, NULL);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|426| <<test_add_max_memory_regions>> vm_set_user_memory_region(vm, slot, 0, ((uint64_t)slot * MEM_REGION_SIZE), MEM_REGION_SIZE, mem_aligned + (uint64_t)slot * MEM_REGION_SIZE);
+ */
 void vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags,
 			       uint64_t gpa, uint64_t size, void *hva)
 {
@@ -930,6 +942,18 @@ void vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags,
 		    errno, strerror(errno));
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|966| <<vm_set_user_memory_region2>> int ret = __vm_set_user_memory_region2(vm, slot, flags, gpa, size, hva, guest_memfd, guest_memfd_offset);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|363| <<test_invalid_memory_region_flags>> r = __vm_set_user_memory_region2(vm, 0, BIT(i), 0, MEM_REGION_SIZE, NULL, 0, 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|372| <<test_invalid_memory_region_flags>> r = __vm_set_user_memory_region2(vm, 0, KVM_MEM_LOG_DIRTY_PAGES | KVM_MEM_GUEST_MEMFD, 0, MEM_REGION_SIZE, NULL, guest_memfd, 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|378| <<test_invalid_memory_region_flags>> r = __vm_set_user_memory_region2(vm, 0, KVM_MEM_READONLY | KVM_MEM_GUEST_MEMFD, 0, MEM_REGION_SIZE, NULL, guest_memfd, 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|452| <<test_invalid_guest_memfd>> int r = __vm_set_user_memory_region2(vm, MEM_REGION_SLOT, KVM_MEM_GUEST_MEMFD, MEM_REGION_GPA, MEM_REGION_SIZE, 0, memfd, offset);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|521| <<test_add_overlapping_private_memory_regions>> r = __vm_set_user_memory_region2(vm, MEM_REGION_SLOT, KVM_MEM_GUEST_MEMFD, MEM_REGION_GPA * 2 - MEM_REGION_SIZE,
+ *                                MEM_REGION_SIZE * 2, 0, memfd, 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|529| <<test_add_overlapping_private_memory_regions>> r = __vm_set_user_memory_region2(vm, MEM_REGION_SLOT, KVM_MEM_GUEST_MEMFD, MEM_REGION_GPA * 2 + MEM_REGION_SIZE,
+ *                                MEM_REGION_SIZE * 2, 0, memfd, 0);
+ */
 int __vm_set_user_memory_region2(struct kvm_vm *vm, uint32_t slot, uint32_t flags,
 				 uint64_t gpa, uint64_t size, void *hva,
 				 uint32_t guest_memfd, uint64_t guest_memfd_offset)
@@ -1137,11 +1161,23 @@ void vm_mem_add(struct kvm_vm *vm, enum vm_mem_backing_src_type src_type,
 	}
 }
 
+/*
+ * 比较vm_set_user_memory_region()
+ *
+ * 936 void vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags,
+ * 937                                uint64_t gpa, uint64_t size, void *hva)
+ */
+
 void vm_userspace_mem_region_add(struct kvm_vm *vm,
 				 enum vm_mem_backing_src_type src_type,
 				 uint64_t guest_paddr, uint32_t slot,
 				 uint64_t npages, uint32_t flags)
 {
+	/*
+	 * 987 void vm_mem_add(struct kvm_vm *vm, enum vm_mem_backing_src_type src_type,
+	 * 988                 uint64_t guest_paddr, uint32_t slot, uint64_t npages,
+	 * 989                 uint32_t flags, int guest_memfd, uint64_t guest_memfd_offset)
+	 */
 	vm_mem_add(vm, src_type, guest_paddr, slot, npages, flags, -1, 0);
 }
 
@@ -1894,6 +1930,11 @@ void kvm_gsi_routing_irqchip_add(struct kvm_irq_routing *routing,
 	routing->nr++;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/vgic_irq.c|576| <<kvm_set_gsi_routing_irqchip_check>> ret = _kvm_gsi_routing_write(vm, routing);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1948| <<kvm_gsi_routing_write>> ret = _kvm_gsi_routing_write(vm, routing);
+ */
 int _kvm_gsi_routing_write(struct kvm_vm *vm, struct kvm_irq_routing *routing)
 {
 	int ret;
@@ -1905,6 +1946,10 @@ int _kvm_gsi_routing_write(struct kvm_vm *vm, struct kvm_irq_routing *routing)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/vgic_irq.c|574| <<kvm_set_gsi_routing_irqchip_check>> kvm_gsi_routing_write(vm, routing);
+ */
 void kvm_gsi_routing_write(struct kvm_vm *vm, struct kvm_irq_routing *routing)
 {
 	int ret;
diff --git a/tools/testing/selftests/kvm/lib/ucall_common.c b/tools/testing/selftests/kvm/lib/ucall_common.c
index f5af65a41..a8e26bffe 100644
--- a/tools/testing/selftests/kvm/lib/ucall_common.c
+++ b/tools/testing/selftests/kvm/lib/ucall_common.c
@@ -22,6 +22,12 @@ int ucall_nr_pages_required(uint64_t page_size)
  */
 static struct ucall_header *ucall_pool;
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|711| <<setup_ucall>> ucall_init(vm, region->region.guest_phys_addr + region->region.memory_size);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|434| <<__vm_create>> ucall_init(vm, slot0->region.guest_phys_addr + slot0->region.memory_size);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|146| <<finish_vm_setup>> ucall_init(vm, slot0->region.guest_phys_addr + slot0->region.memory_size);
+ */
 void ucall_init(struct kvm_vm *vm, vm_paddr_t mmio_gpa)
 {
 	struct ucall_header *hdr;
diff --git a/tools/testing/selftests/kvm/lib/userfaultfd_util.c b/tools/testing/selftests/kvm/lib/userfaultfd_util.c
index f4eef6eb2..0d8e2f1b1 100644
--- a/tools/testing/selftests/kvm/lib/userfaultfd_util.c
+++ b/tools/testing/selftests/kvm/lib/userfaultfd_util.c
@@ -108,6 +108,13 @@ static void *uffd_handler_thread_fn(void *arg)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1021| <<setup_uffd>> *pt_uffd = uffd_setup_demand_paging(uffd_mode, 0,
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1028| <<setup_uffd>> *data_uffd = uffd_setup_demand_paging(uffd_mode, 0,
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|171| <<run_test>> uffd_descs[i] = uffd_setup_demand_paging(p->uffd_mode, p->uffd_delay, vcpu_hva,
+ *                                                     vcpu_args->pages * memstress_args.guest_page_size, &handle_uffd_page_request);
+ */
 struct uffd_desc *uffd_setup_demand_paging(int uffd_mode, useconds_t delay,
 					   void *hva, uint64_t len,
 					   uffd_handler_t handler)
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 229570059..e7969bda7 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -38,6 +38,10 @@ kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
 	return true;
 }
 
+/*
+ * 在以下使用irqfd_inject():
+ *   - virt/kvm/eventfd.c|333| <<kvm_irqfd_assign>> INIT_WORK(&irqfd->inject, irqfd_inject);
+ */
 static void
 irqfd_inject(struct work_struct *work)
 {
@@ -117,6 +121,10 @@ irqfd_resampler_shutdown(struct kvm_kernel_irqfd *irqfd)
 /*
  * Race-free decouple logic (ordering is critical)
  */
+/*
+ * 在以下使用irqfd_shutdown():
+ *   - virt/kvm/eventfd.c|330| <<kvm_irqfd_assign>> INIT_WORK(&irqfd->shutdown, irqfd_shutdown);
+ */
 static void
 irqfd_shutdown(struct work_struct *work)
 {
@@ -253,6 +261,11 @@ irqfd_ptable_queue_proc(struct file *file, wait_queue_head_t *wqh,
 	add_wait_queue_priority(wqh, &irqfd->wait);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|424| <<kvm_irqfd_assign>> irqfd_update(kvm, irqfd);
+ *   - virt/kvm/eventfd.c|659| <<kvm_irq_routing_update>> irqfd_update(kvm, irqfd);
+ */
 /* Must be called under irqfds.lock */
 static void irqfd_update(struct kvm *kvm, struct kvm_kernel_irqfd *irqfd)
 {
@@ -299,6 +312,10 @@ bool __attribute__((weak)) kvm_arch_irqfd_route_changed(
 }
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|582| <<kvm_irqfd>> return kvm_irqfd_assign(kvm, args);
+ */
 static int
 kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -426,9 +443,17 @@ kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 
 #ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
 	if (kvm_arch_has_irq_bypass()) {
+		/*
+		 * struct kvm_kernel_irqfd *irqfd:
+		 * -> struct irq_bypass_consumer consumer;
+		 * -> struct irq_bypass_producer *producer;
+		 */
 		irqfd->consumer.token = (void *)irqfd->eventfd;
 		irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
 		irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
+		/*
+		 * x86没有实现start/stop
+		 */
 		irqfd->consumer.stop = kvm_arch_irq_bypass_stop;
 		irqfd->consumer.start = kvm_arch_irq_bypass_start;
 		ret = irq_bypass_register_consumer(&irqfd->consumer);
@@ -464,6 +489,10 @@ kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|295| <<kvm_ioapic_scan_entry>> if (e->fields.trig_mode == IOAPIC_LEVEL_TRIG || kvm_irq_has_notifier(ioapic->kvm, KVM_IRQCHIP_IOAPIC, index) || index == RTC_GSI) {
+ */
 bool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin)
 {
 	struct kvm_irq_ack_notifier *kian;
@@ -570,6 +599,10 @@ kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 	return 0;
 }
 
+/*
+ * 在以下处理KVM_IRQFD:
+ *   - virt/kvm/kvm_main.c|5217| <<kvm_vm_ioctl>> r = kvm_irqfd(kvm, &data);
+ */
 int
 kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -610,12 +643,25 @@ kvm_irqfd_release(struct kvm *kvm)
  * Take note of a change in irq routing.
  * Caller must invoke synchronize_srcu(&kvm->irq_srcu) afterwards.
  */
+/*
+ * called by;
+ *   - virt/kvm/irqchip.c|328| <<kvm_set_irq_routing>> kvm_irq_routing_update(kvm);
+ */
 void kvm_irq_routing_update(struct kvm *kvm)
 {
 	struct kvm_kernel_irqfd *irqfd;
 
 	spin_lock_irq(&kvm->irqfds.lock);
 
+	/*
+	 * 在以下使用kvm.irqfds.items:
+	 *   - virt/kvm/eventfd.c|410| <<kvm_irqfd_assign>> list_for_each_entry(tmp, &kvm->irqfds.items, list) {
+	 *   - virt/kvm/eventfd.c|422| <<kvm_irqfd_assign>> list_add_tail(&irqfd->list, &kvm->irqfds.items);
+	 *   - virt/kvm/eventfd.c|558| <<kvm_irqfd_deassign>> list_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list) {
+	 *   - virt/kvm/eventfd.c|613| <<kvm_irqfd_release>> list_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list)
+	 *   - virt/kvm/eventfd.c|636| <<kvm_irq_routing_update>> list_for_each_entry(irqfd, &kvm->irqfds.items, list) {
+	 *   - virt/kvm/eventfd.c|1032| <<kvm_eventfd_init>> INIT_LIST_HEAD(&kvm->irqfds.items);
+	 */
 	list_for_each_entry(irqfd, &kvm->irqfds.items, list) {
 #ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
 		/* Under irqfds.lock, so can read irq_entry safely */
@@ -677,6 +723,11 @@ int kvm_irqfd_init(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|6496| <<kvm_init>> kvm_irqfd_exit();
+ *   - virt/kvm/kvm_main.c|6532| <<kvm_exit>> kvm_irqfd_exit();
+ */
 void kvm_irqfd_exit(void)
 {
 	destroy_workqueue(irqfd_cleanup_wq);
@@ -717,6 +768,10 @@ ioeventfd_release(struct _ioeventfd *p)
 	kfree(p);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|789| <<ioeventfd_write>> if (!ioeventfd_in_range(p, addr, len, val))
+ */
 static bool
 ioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)
 {
@@ -788,11 +843,19 @@ ioeventfd_destructor(struct kvm_io_device *this)
 	ioeventfd_release(p);
 }
 
+/*
+ * 在以下使用ioeventfd_ops:
+ *   - virt/kvm/eventfd.c|879| <<kvm_assign_ioeventfd_idx>> kvm_iodevice_init(&p->dev, &ioeventfd_ops);
+ */
 static const struct kvm_io_device_ops ioeventfd_ops = {
 	.write      = ioeventfd_write,
 	.destructor = ioeventfd_destructor,
 };
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|874| <<kvm_assign_ioeventfd_idx>> if (ioeventfd_check_collision(kvm, p)) {
+ */
 /* assumes kvm->slots_lock held */
 static bool
 ioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)
@@ -811,6 +874,11 @@ ioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)
 	return false;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|949| <<kvm_deassign_ioeventfd>> enum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);
+ *   - virt/kvm/eventfd.c|968| <<kvm_assign_ioeventfd>> bus_idx = ioeventfd_bus_from_flags(args->flags);
+ */
 static enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)
 {
 	if (flags & KVM_IOEVENTFD_FLAG_PIO)
@@ -820,6 +888,11 @@ static enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)
 	return KVM_MMIO_BUS;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|993| <<kvm_assign_ioeventfd>> ret = kvm_assign_ioeventfd_idx(kvm, bus_idx, args);
+ *   - virt/kvm/eventfd.c|1001| <<kvm_assign_ioeventfd>> ret = kvm_assign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);
+ */
 static int kvm_assign_ioeventfd_idx(struct kvm *kvm,
 				enum kvm_bus bus_idx,
 				struct kvm_ioeventfd *args)
@@ -883,6 +956,12 @@ static int kvm_assign_ioeventfd_idx(struct kvm *kvm,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|950| <<kvm_deassign_ioeventfd>> int ret = kvm_deassign_ioeventfd_idx(kvm, bus_idx, args);
+ *   - virt/kvm/eventfd.c|953| <<kvm_deassign_ioeventfd>> kvm_deassign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);
+ *   - virt/kvm/eventfd.c|1009| <<kvm_assign_ioeventfd>> kvm_deassign_ioeventfd_idx(kvm, bus_idx, args);
+ */
 static int
 kvm_deassign_ioeventfd_idx(struct kvm *kvm, enum kvm_bus bus_idx,
 			   struct kvm_ioeventfd *args)
@@ -927,6 +1006,10 @@ kvm_deassign_ioeventfd_idx(struct kvm *kvm, enum kvm_bus bus_idx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|1022| <<kvm_ioeventfd>> return kvm_deassign_ioeventfd(kvm, args);
+ */
 static int kvm_deassign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
 	enum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);
@@ -938,6 +1021,10 @@ static int kvm_deassign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|1012| <<kvm_ioeventfd>> return kvm_assign_ioeventfd(kvm, args);
+ */
 static int
 kvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
@@ -990,6 +1077,10 @@ kvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 	return ret;
 }
 
+/*
+ * 处理KVM_IOEVENTFD:
+ *   - virt/kvm/kvm_main.c|5226| <<kvm_vm_ioctl>> r = kvm_ioeventfd(kvm, &data);
+ */
 int
 kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
@@ -999,6 +1090,10 @@ kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 	return kvm_assign_ioeventfd(kvm, args);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1201| <<kvm_create_vm>> kvm_eventfd_init(kvm);
+ */
 void
 kvm_eventfd_init(struct kvm *kvm)
 {
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 1e567d1f6..45a855873 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -18,13 +18,46 @@
 #include <linux/export.h>
 #include <trace/events/kvm.h>
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|267| <<irqfd_update>> n_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);
+ *   - virt/kvm/irqchip.c|83| <<kvm_set_irq>> i = kvm_irq_map_gsi(kvm, irq_set, irq);
+ */
 int kvm_irq_map_gsi(struct kvm *kvm,
 		    struct kvm_kernel_irq_routing_entry *entries, int gsi)
 {
+	/*
+	 * #define KVM_IRQCHIP_PIC_MASTER   0
+	 * #define KVM_IRQCHIP_PIC_SLAVE    1
+	 * #define KVM_IRQCHIP_IOAPIC       2
+	 * #define KVM_NR_IRQCHIPS          3
+	 *
+	 * 677 struct kvm_irq_routing_table {
+	 * 678         int chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS];
+	 * 679         u32 nr_rt_entries;
+	 * 680         //
+	 * 681          * Array indexed by gsi. Each entry contains list of irq chips
+	 * 682          * the gsi is connected to.
+	 * 683          //
+	 * 684         struct hlist_head map[] __counted_by(nr_rt_entries);
+	 * 685 };
+	 */
 	struct kvm_irq_routing_table *irq_rt;
 	struct kvm_kernel_irq_routing_entry *e;
 	int n = 0;
 
+	/*
+	 * 在以下使用kvm->irq_routing:
+	 *   - arch/x86/kvm/hyperv.c|544| <<kvm_hv_irq_routing_update>> irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu,
+	 *   - arch/x86/kvm/irq_comm.c|422| <<kvm_scan_ioapic_routes>> table = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - arch/x86/kvm/svm/avic.c|909| <<avic_pi_update_irte>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|406| <<vmx_pi_update_irte>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - virt/kvm/irqchip.c|28| <<kvm_irq_map_gsi>> irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu, lockdep_is_held(&kvm->irq_lock));
+	 *   - virt/kvm/irqchip.c|44| <<kvm_irq_map_chip_pin>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - virt/kvm/irqchip.c|123| <<kvm_free_irq_routing>> struct kvm_irq_routing_table *rt = rcu_access_pointer(kvm->irq_routing);
+	 *   - virt/kvm/irqchip.c|219| <<kvm_set_irq_routing>> old = rcu_dereference_protected(kvm->irq_routing, 1);
+	 *   - virt/kvm/irqchip.c|220| <<kvm_set_irq_routing>> rcu_assign_pointer(kvm->irq_routing, new);
+	 */
 	irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu,
 					lockdep_is_held(&kvm->irq_lock));
 	if (irq_rt && gsi < irq_rt->nr_rt_entries) {
@@ -45,8 +78,16 @@ int kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin)
 	return irq_rt->chip[irqchip][pin];
 }
 
+/*
+ * 在以下处理KVM_SIGNAL_MSI:
+ *   - virt/kvm/kvm_main.c|5236| <<kvm_vm_ioctl(KVM_SIGNAL_MSI)>> r = kvm_send_userspace_msi(kvm, &msi);
+ */
 int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi)
 {
+	/*
+	 * 这里是现场分配一个kvm_kernel_irq_routing_entry
+	 * 所以kernel这边才没有备份吧
+	 */
 	struct kvm_kernel_irq_routing_entry route;
 
 	if (!kvm_arch_irqchip_in_kernel(kvm) || (msi->flags & ~KVM_MSI_VALID_DEVID))
@@ -67,6 +108,17 @@ int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi)
  *  = 0   Interrupt was coalesced (previous irq is still pending)
  *  > 0   Number of CPUs interrupt was delivered to
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|251| <<pit_do_work>> kvm_set_irq(kvm, pit->irq_source_id, 0, 1, false);
+ *   - arch/x86/kvm/i8254.c|252| <<pit_do_work>> kvm_set_irq(kvm, pit->irq_source_id, 0, 0, false);
+ *   - arch/x86/kvm/x86.c|6496| <<kvm_vm_ioctl_irq_line>> irq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irq_event->irq, irq_event->level, line_status);
+ *   - virt/kvm/eventfd.c|49| <<irqfd_inject>> kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 1, false);
+ *   - virt/kvm/eventfd.c|51| <<irqfd_inject>> kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 0, false);
+ *   - virt/kvm/eventfd.c|54| <<irqfd_inject>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID, irqfd->gsi, 1, false);
+ *   - virt/kvm/eventfd.c|83| <<irqfd_resampler_ack>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID, resampler->notifier.gsi, 0, false);
+ *   - virt/kvm/eventfd.c|109| <<irqfd_resampler_shutdown>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID, resampler->notifier.gsi, 0, false);
+ */
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 		bool line_status)
 {
@@ -80,6 +132,11 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 	 * writes to the unused one.
 	 */
 	idx = srcu_read_lock(&kvm->irq_srcu);
+	/*
+	 * called by:
+	 *   - virt/kvm/eventfd.c|267| <<irqfd_update>> n_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);
+	 *   - virt/kvm/irqchip.c|83| <<kvm_set_irq>> i = kvm_irq_map_gsi(kvm, irq_set, irq);
+	 */
 	i = kvm_irq_map_gsi(kvm, irq_set, irq);
 	srcu_read_unlock(&kvm->irq_srcu, idx);
 
@@ -116,6 +173,10 @@ static void free_irq_routing_table(struct kvm_irq_routing_table *rt)
 	kfree(rt);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1352| <<kvm_destroy_vm>> kvm_free_irq_routing(kvm);
+ */
 void kvm_free_irq_routing(struct kvm *kvm)
 {
 	/* Called only during vm destruction. Nobody can use the pointer
@@ -124,6 +185,10 @@ void kvm_free_irq_routing(struct kvm *kvm)
 	free_irq_routing_table(rt);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|212| <<kvm_set_irq_routing>> r = setup_routing_entry(kvm, new, e, ue);
+ */
 static int setup_routing_entry(struct kvm *kvm,
 			       struct kvm_irq_routing_table *rt,
 			       struct kvm_kernel_irq_routing_entry *e,
@@ -148,6 +213,14 @@ static int setup_routing_entry(struct kvm *kvm,
 	r = kvm_set_routing_entry(kvm, e, ue);
 	if (r)
 		return r;
+	/*
+	 * 957 // gsi routing entry types
+	 * 958 #define KVM_IRQ_ROUTING_IRQCHIP 1
+	 * 959 #define KVM_IRQ_ROUTING_MSI 2
+	 * 960 #define KVM_IRQ_ROUTING_S390_ADAPTER 3
+	 * 961 #define KVM_IRQ_ROUTING_HV_SINT 4
+	 * 962 #define KVM_IRQ_ROUTING_XEN_EVTCHN 5
+	 */
 	if (e->type == KVM_IRQ_ROUTING_IRQCHIP)
 		rt->chip[e->irqchip.irqchip][e->irqchip.pin] = e->gsi;
 
@@ -165,6 +238,32 @@ bool __weak kvm_arch_can_set_irq_routing(struct kvm *kvm)
 	return true;
 }
 
+/*
+ * 651 struct kvm_kernel_irq_routing_entry {
+ * 652         u32 gsi;
+ * 653         u32 type;
+ * 654         int (*set)(struct kvm_kernel_irq_routing_entry *e,
+ * 655                    struct kvm *kvm, int irq_source_id, int level,
+ * 656                    bool line_status);
+ * 657         union {
+ * 658                 struct {
+ * 659                         unsigned irqchip;
+ * 660                         unsigned pin;
+ * 661                 } irqchip;
+ * 662                 struct {
+ * 663                         u32 address_lo;
+ * 664                         u32 address_hi;
+ * 665                         u32 data;
+ * 666                         u32 flags;
+ * 667                         u32 devid;
+ * 668                 } msi;
+ * 669                 struct kvm_s390_adapter_int adapter;
+ * 670                 struct kvm_hv_sint hv_sint;
+ * 671                 struct kvm_xen_evtchn xen_evtchn;
+ * 672         };
+ * 673         struct hlist_node link;
+ * 674 };
+ */
 int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *ue,
 			unsigned nr,
@@ -216,6 +315,18 @@ int kvm_set_irq_routing(struct kvm *kvm,
 	}
 
 	mutex_lock(&kvm->irq_lock);
+	/*
+	 * 在以下使用kvm->irq_routing:
+	 *   - arch/x86/kvm/hyperv.c|544| <<kvm_hv_irq_routing_update>> irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu,
+	 *   - arch/x86/kvm/irq_comm.c|422| <<kvm_scan_ioapic_routes>> table = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - arch/x86/kvm/svm/avic.c|909| <<avic_pi_update_irte>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|406| <<vmx_pi_update_irte>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - virt/kvm/irqchip.c|28| <<kvm_irq_map_gsi>> irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu, lockdep_is_held(&kvm->irq_lock));
+	 *   - virt/kvm/irqchip.c|44| <<kvm_irq_map_chip_pin>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - virt/kvm/irqchip.c|123| <<kvm_free_irq_routing>> struct kvm_irq_routing_table *rt = rcu_access_pointer(kvm->irq_routing);
+	 *   - virt/kvm/irqchip.c|219| <<kvm_set_irq_routing>> old = rcu_dereference_protected(kvm->irq_routing, 1);
+	 *   - virt/kvm/irqchip.c|220| <<kvm_set_irq_routing>> rcu_assign_pointer(kvm->irq_routing, new);
+	 */
 	old = rcu_dereference_protected(kvm->irq_routing, 1);
 	rcu_assign_pointer(kvm->irq_routing, new);
 	kvm_irq_routing_update(kvm);
diff --git a/virt/lib/irqbypass.c b/virt/lib/irqbypass.c
index 28fda42e4..83abb6c13 100644
--- a/virt/lib/irqbypass.c
+++ b/virt/lib/irqbypass.c
@@ -22,10 +22,31 @@
 MODULE_LICENSE("GPL v2");
 MODULE_DESCRIPTION("IRQ bypass manager utility module");
 
+/*
+ * 在以下使用LIST_HEAD(producers):
+ *   - virt/lib/irqbypass.c|100| <<irq_bypass_register_producer>> list_for_each_entry(tmp, &producers, node) {
+ *   - virt/lib/irqbypass.c|116| <<irq_bypass_register_producer>> list_add(&producer->node, &producers);
+ *   - virt/lib/irqbypass.c|156| <<irq_bypass_unregister_producer>> list_for_each_entry(tmp, &producers, node) {
+ *   - virt/lib/irqbypass.c|209| <<irq_bypass_register_consumer>> list_for_each_entry(producer, &producers, node) {
+ *   - virt/lib/irqbypass.c|256| <<irq_bypass_unregister_consumer>> list_for_each_entry(producer, &producers, node) {
+ */
 static LIST_HEAD(producers);
+/*
+ * 在以下使用LIST_HEAD(consumers):
+ *   - virt/lib/irqbypass.c|107| <<irq_bypass_register_producer>> list_for_each_entry(consumer, &consumers, node) {
+ *   - virt/lib/irqbypass.c|160| <<irq_bypass_unregister_producer>> list_for_each_entry(consumer, &consumers, node) {
+ *   - virt/lib/irqbypass.c|202| <<irq_bypass_register_consumer>> list_for_each_entry(tmp, &consumers, node) {
+ *   - virt/lib/irqbypass.c|218| <<irq_bypass_register_consumer>> list_add(&consumer->node, &consumers);
+ *   - virt/lib/irqbypass.c|252| <<irq_bypass_unregister_consumer>> list_for_each_entry(tmp, &consumers, node) {
+ */
 static LIST_HEAD(consumers);
 static DEFINE_MUTEX(lock);
 
+/*
+ * called by:
+ *   - virt/lib/irqbypass.c|109| <<irq_bypass_register_producer>> ret = __connect(producer, consumer);
+ *   - virt/lib/irqbypass.c|211| <<irq_bypass_register_consumer>> ret = __connect(producer, consumer);
+ */
 /* @lock must be held when calling connect */
 static int __connect(struct irq_bypass_producer *prod,
 		     struct irq_bypass_consumer *cons)
@@ -54,6 +75,11 @@ static int __connect(struct irq_bypass_producer *prod,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - virt/lib/irqbypass.c|162| <<irq_bypass_unregister_producer>> __disconnect(producer, consumer);
+ *   - virt/lib/irqbypass.c|258| <<irq_bypass_unregister_consumer>> __disconnect(producer, consumer);
+ */
 /* @lock must be held when calling disconnect */
 static void __disconnect(struct irq_bypass_producer *prod,
 			 struct irq_bypass_consumer *cons)
@@ -81,6 +107,11 @@ static void __disconnect(struct irq_bypass_producer *prod,
  * Add the provided IRQ producer to the list of producers and connect
  * with any matching token found on the IRQ consumers list.
  */
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|523| <<vfio_msi_set_vector_signal>> ret = irq_bypass_register_producer(&ctx->producer);
+ *   - drivers/vhost/vdpa.c|218| <<vhost_vdpa_setup_vq_irq>> ret = irq_bypass_register_producer(&vq->call_ctx.producer);
+ */
 int irq_bypass_register_producer(struct irq_bypass_producer *producer)
 {
 	struct irq_bypass_producer *tmp;
@@ -132,6 +163,12 @@ EXPORT_SYMBOL_GPL(irq_bypass_register_producer);
  * Remove a previously registered IRQ producer from the list of producers
  * and disconnect it from any connected IRQ consumer.
  */
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|459| <<vfio_msi_set_vector_signal>> irq_bypass_unregister_producer(&ctx->producer);
+ *   - drivers/vhost/vdpa.c|212| <<vhost_vdpa_setup_vq_irq>> irq_bypass_unregister_producer(&vq->call_ctx.producer);
+ *   - drivers/vhost/vdpa.c|228| <<vhost_vdpa_unsetup_vq_irq>> irq_bypass_unregister_producer(&vq->call_ctx.producer);
+ */
 void irq_bypass_unregister_producer(struct irq_bypass_producer *producer)
 {
 	struct irq_bypass_producer *tmp;
@@ -176,6 +213,17 @@ EXPORT_SYMBOL_GPL(irq_bypass_unregister_producer);
  * Add the provided IRQ consumer to the list of consumers and connect
  * with any matching token found on the IRQ producer list.
  */
+/*
+ * 438                 irqfd->consumer.token = (void *)irqfd->eventfd;
+ * 439                 irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+ * 440                 irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
+ * 441                 irqfd->consumer.stop = kvm_arch_irq_bypass_stop;
+ * 442                 irqfd->consumer.start = kvm_arch_irq_bypass_start;
+ * 443                 ret = irq_bypass_register_consumer(&irqfd->consumer);
+ *
+ * called by:
+ *   - virt/kvm/eventfd.c|434| <<kvm_irqfd_assign>> ret = irq_bypass_register_consumer(&irqfd->consumer);
+ */
 int irq_bypass_register_consumer(struct irq_bypass_consumer *consumer)
 {
 	struct irq_bypass_consumer *tmp;
@@ -228,6 +276,10 @@ EXPORT_SYMBOL_GPL(irq_bypass_register_consumer);
  * Remove a previously registered IRQ consumer from the list of consumers
  * and disconnect it from any connected IRQ producer.
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|152| <<irqfd_shutdown>> irq_bypass_unregister_consumer(&irqfd->consumer);
+ */
 void irq_bypass_unregister_consumer(struct irq_bypass_consumer *consumer)
 {
 	struct irq_bypass_consumer *tmp;
-- 
2.34.1

