From e7d899a9e8f22f25ef5ff6d87b17abba72dcfcf6 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang@oracle.com>
Date: Thu, 27 Dec 2018 15:57:59 +0800
Subject: [PATCH 1/1] kvm for 4.18.12

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/kvm_host.h | 102 ++++++++
 arch/x86/kernel/apic/apic.c     |   7 +
 arch/x86/kernel/kvmclock.c      |  77 ++++++
 arch/x86/kvm/i8254.c            |  10 +
 arch/x86/kvm/lapic.c            |  30 +++
 arch/x86/kvm/mmu.c              | 249 +++++++++++++++++++-
 arch/x86/kvm/mmu.h              |   6 +
 arch/x86/kvm/page_track.c       |   4 +
 arch/x86/kvm/pmu_intel.c        |  39 +++
 arch/x86/kvm/vmx.c              | 509 ++++++++++++++++++++++++++++++++++++++++
 arch/x86/kvm/x86.c              | 122 ++++++++++
 include/linux/kvm_host.h        |  15 ++
 virt/kvm/eventfd.c              |   4 +
 virt/kvm/irqchip.c              |   4 +
 virt/kvm/kvm_main.c             | 205 +++++++++++++++-
 virt/kvm/vfio.c                 |  27 +++
 16 files changed, 1406 insertions(+), 4 deletions(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0722b77..0b74910 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -102,12 +102,45 @@
 
 /* KVM Hugepage definitions for x86 */
 #define KVM_NR_PAGE_SIZES	3
+/*
+ * x = 1: KVM_HPAGE_GFN_SHIFT(1) = 0   none
+ * x = 2: KVM_HPAGE_GFN_SHIFT(2) = 9   2M
+ * x = 3: KVM_HPAGE_GFN_SHIFT(3) = 18  1G
+ */
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+/*
+ * KVM_HPAGE_SHIFT(1) = 12 +  0 = 12
+ * KVM_HPAGE_SHIFT(2) = 12 +  9 = 21
+ * KVM_HPAGE_SHIFT(3) = 12 + 18 = 30
+ */
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
+/*
+ * KVM_HPAGE_SIZE(1) = 1 << 12 = 4K
+ * KVM_HPAGE_SIZE(2) = 1 << 21 = 2M
+ * KVM_HPAGE_SIZE(3) = 1 << 30 = 1G
+ */
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
+/*
+ * KVM_HPAGE_MASK(1) : mask了4K
+ * KVM_HPAGE_MASK(2) : mask了2M
+ * KVM_HPAGE_MASK(3) : mask了1G
+ */
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
+/*
+ * KVM_PAGES_PER_HPAGE(1) : 4K有多少4K的page
+ * KVM_PAGES_PER_HPAGE(2) : 2M有多少4K的page
+ * KVM_PAGES_PER_HPAGE(3) : 1G有多少4K的page
+ */
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
+/*
+ * base_gfn是基于4k开始的gfn
+ * gfn是基于4k结束的gfn
+ * 计算从开始到结束需要用到几个hugepage
+ *   level是1的时候hugepage大小是4K
+ *   level是2的时候hugepage大小是2M
+ *   level是3的时候hugepage大小是1G
+ */
 static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 {
 	/* KVM_HPAGE_GFN_SHIFT(PT_PAGE_TABLE_LEVEL) must be 0. */
@@ -332,13 +365,58 @@ struct rsvd_bits_validate {
  * current mmu mode.
  */
 struct kvm_mmu {
+	/*
+	 * 设置set_cr3的地方:
+	 *   - arch/x86/kvm/mmu.c|4563| <<init_kvm_tdp_mmu>> context->set_cr3 = kvm_x86_ops->set_tdp_cr3;
+	 *   - arch/x86/kvm/mmu.c|4660| <<init_kvm_softmmu>> context->set_cr3 = kvm_x86_ops->set_cr3;
+	 *   - arch/x86/kvm/svm.c|2923| <<nested_svm_init_mmu_context>> vcpu->arch.mmu.set_cr3 = nested_svm_set_tdp_cr3;
+	 *   - arch/x86/kvm/vmx.c|11331| <<nested_ept_init_mmu_context>> vcpu->arch.mmu.set_cr3 = vmx_set_cr3;
+	 */
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
+	/*
+	 * 设置get_cr3的地方:
+	 *   - arch/x86/kvm/mmu.c|4564| <<init_kvm_tdp_mmu>> context->get_cr3 = get_cr3;
+	 *   - arch/x86/kvm/mmu.c|4661| <<init_kvm_softmmu>> context->get_cr3 = get_cr3;
+	 *   - arch/x86/kvm/mmu.c|4670| <<init_kvm_nested_mmu>> g_context->get_cr3 = get_cr3;
+	 *   - arch/x86/kvm/svm.c|2924| <<nested_svm_init_mmu_context>> vcpu->arch.mmu.get_cr3 = nested_svm_get_tdp_cr3;
+	 *   - arch/x86/kvm/vmx.c|11332| <<nested_ept_init_mmu_context>> vcpu->arch.mmu.get_cr3 = nested_ept_get_cr3;
+	 */
 	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
+	/*
+	 * 设置get_pdptr的地方:
+	 *   - arch/x86/kvm/mmu.c|4565| <<init_kvm_tdp_mmu>> context->get_pdptr = kvm_pdptr_read;
+	 *   - arch/x86/kvm/mmu.c|4662| <<init_kvm_softmmu>> context->get_pdptr = kvm_pdptr_read;
+	 *   - arch/x86/kvm/mmu.c|4671| <<init_kvm_nested_mmu>> g_context->get_pdptr = kvm_pdptr_read;
+	 *   - arch/x86/kvm/svm.c|2925| <<nested_svm_init_mmu_context>> vcpu->arch.mmu.get_pdptr = nested_svm_get_tdp_pdptr;
+	 */
 	u64 (*get_pdptr)(struct kvm_vcpu *vcpu, int index);
+	/*
+	 * 设置page_fault的地方:
+	 *   - arch/x86/kvm/mmu.c|4013| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+	 *   - arch/x86/kvm/mmu.c|4498| <<paging64_init_context_common>> context->page_fault = paging64_page_fault;
+	 *   - arch/x86/kvm/mmu.c|4528| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+	 *   - arch/x86/kvm/mmu.c|4556| <<init_kvm_tdp_mmu>> context->page_fault = tdp_page_fault;
+	 *   - arch/x86/kvm/mmu.c|4637| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault;
+	 */
 	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err,
 			  bool prefault);
 	void (*inject_page_fault)(struct kvm_vcpu *vcpu,
 				  struct x86_exception *fault);
+	/*
+	 * 设置gva_to_gpa的地方:
+	 *   - arch/x86/kvm/mmu.c|4014| <<nonpaging_init_context>> context->gva_to_gpa = nonpaging_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4499| <<paging64_init_context_common>> context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4529| <<paging32_init_context>> context->gva_to_gpa = paging32_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4570| <<init_kvm_tdp_mmu>> context->gva_to_gpa = nonpaging_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4577| <<init_kvm_tdp_mmu>> context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4582| <<init_kvm_tdp_mmu>> context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4587| <<init_kvm_tdp_mmu>> context->gva_to_gpa = paging32_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4638| <<kvm_init_shadow_ept_mmu>> context->gva_to_gpa = ept_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4685| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = nonpaging_gva_to_gpa_nested;
+	 *   - arch/x86/kvm/mmu.c|4691| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging64_gva_to_gpa_nested;
+	 *   - arch/x86/kvm/mmu.c|4696| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging64_gva_to_gpa_nested;
+	 *   - arch/x86/kvm/mmu.c|4701| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging32_gva_to_gpa_nested;
+	 */
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
 			    struct x86_exception *exception);
 	gpa_t (*translate_gpa)(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
@@ -348,11 +426,27 @@ struct kvm_mmu {
 	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva);
 	void (*update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			   u64 *spte, const void *pte);
+	/*
+	 * 设置root_hpa valid值的地方:
+	 *   - arch/x86/kvm/mmu.c|3481| <<mmu_alloc_direct_roots>> vcpu->arch.mmu.root_hpa = __pa(sp->spt);
+	 *   - arch/x86/kvm/mmu.c|3499| <<mmu_alloc_direct_roots>> vcpu->arch.mmu.root_hpa = __pa(vcpu->arch.mmu.pae_root);
+	 *   - arch/x86/kvm/mmu.c|3537| <<mmu_alloc_shadow_roots>> vcpu->arch.mmu.root_hpa = root;
+	 *   - arch/x86/kvm/mmu.c|3577| <<mmu_alloc_shadow_roots>> vcpu->arch.mmu.root_hpa = __pa(vcpu->arch.mmu.pae_root);
+	 *   - arch/x86/kvm/mmu.c|3601| <<mmu_alloc_shadow_roots>> vcpu->arch.mmu.root_hpa = __pa(vcpu->arch.mmu.lm_root);
+	 */
 	hpa_t root_hpa;
 	union kvm_mmu_page_role base_role;
 	u8 root_level;
 	u8 shadow_root_level;
 	u8 ept_ad;
+	/*
+	 * 设置direct_map的地方:
+	 *   - arch/x86/kvm/mmu.c|4003| <<nonpaging_init_context>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|4487| <<paging64_init_context_common>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4517| <<paging32_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4544| <<init_kvm_tdp_mmu>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|4626| <<kvm_init_shadow_ept_mmu>> context->direct_map = false;
+	 */
 	bool direct_map;
 
 	/*
@@ -922,6 +1016,10 @@ struct kvm_lapic_irq {
 	bool msi_redir_hint;
 };
 
+/*
+ * intel : vmx_x86_ops
+ * amd   : svm_x86_ops
+ */
 struct kvm_x86_ops {
 	int (*cpu_has_kvm_support)(void);          /* __init */
 	int (*disabled_by_bios)(void);             /* __init */
@@ -1114,6 +1212,10 @@ extern struct kvm_x86_ops *kvm_x86_ops;
 #define __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)
 {
+	/*
+	 * intel: vmx_vm_alloc()
+	 * amd  : svm_vm_alloc()
+	 */
 	return kvm_x86_ops->vm_alloc();
 }
 
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index 3b3a2d0..c0f7ab8 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -998,6 +998,10 @@ void setup_secondary_APIC_clock(void)
 /*
  * The guts of the apic timer interrupt
  */
+/*
+ * called by only:
+ *   - arch/x86/kernel/apic/apic.c|1054| <<smp_apic_timer_interrupt>> local_apic_timer_interrupt();
+ */
 static void local_apic_timer_interrupt(void)
 {
 	struct clock_event_device *evt = this_cpu_ptr(&lapic_events);
@@ -1037,6 +1041,9 @@ static void local_apic_timer_interrupt(void)
  * [ if a single-CPU system runs an SMP kernel then we call the local
  *   interrupt as well. Thus we cannot inline the local irq ... ]
  */
+/*
+ * arch/x86/entry/entry_64.S: 用作LOCAL_TIMER_VECTOR (0xec)的处理函数
+ */
 __visible void __irq_entry smp_apic_timer_interrupt(struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 3b8e7c1..a3e8828 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -32,6 +32,76 @@
 #include <asm/reboot.h>
 #include <asm/kvmclock.h>
 
+/*
+ * FROM SUSE!!!
+ * When using kvm-clock, it is not recommended to use NTP in the VM Guest, as
+ * well. Using NTP on the VM Host Server, however, is still recommended. 
+ */
+
+/*
+ * Clocksource is a device that can give a timestamp whenever you need it. In
+ * other words, Clocksource is any ticking counter that allows you to get its
+ * value.
+ *
+ * Clockevent device is an alarm clock—you ask the device to signal a time in
+ * the future (e.g., "wake me up in 1ms") and when the alarm is triggered, you
+ * get the signal.
+ *
+ * sched_clock() function is similar to clocksource, but this particular one
+ * should be "cheap" to read (meaning that one can get its value fast), as
+ * sched_clock() is used for task-scheduling purposes and scheduling happens
+ * often. We're ready to sacrifice accuracy and other characteristics for
+ * speed.
+ *
+ * > CLOCK_REALTIME clock gives the time passed since January 1, 1970. This
+ *   clock is affected by NTP adjustments and can jump forward and backward when
+ *   a system administrator adjusts system time.
+ *
+ * > CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually
+ *   since you booted the system. This clock is affected by NTP, but it can't
+ *   jump backward.
+ *
+ * > CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this
+ *   clock is not affected by NTP adjustments.
+ *
+ * > CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but
+ *   less-accurate variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ *
+ *
+ * Hardware extensions for virtualizing TSC
+ *
+ * Since the early days of hardware-assisted virtualization, Intel was
+ * supplying an option to do TSC offsetting for virtual guests in hardware,
+ * which would mean that a guest's rdtsc reading will return a host's TSC value
+ * + offset. Unfortunately, this wasn't enough to support migration between
+ * different hosts because TSC frequency may differ, so pvclock and TSC page
+ * protocol were introduced. In late 2015, Intel introduced the TSC scaling
+ * feature (which was already present in AMD processors for several years) and,
+ * in theory, this is a game changer making pvclock and TSC page protocols
+ * redundant. However, an immediate switch to using plain TSC as a clocksource
+ * for virtualized guests seems impractical; one must be sure that all
+ * potential migration recipient hosts support the feature, but it is not yet
+ * widely available. Extensive testing also must be performed to make sure
+ * there are no drawbacks to switching from paravirtualized protocols.
+ */
+
+/*
+ * To get the current TSC reading, guests must do the following math:
+ *
+ * PerCPUTime = ((RDTSC() - tsc_timestamp) >> tsc_shift) * tsc_to_system_mul + system_time 
+ *
+ *
+ *
+ * kvmclock or KVM pvclock lets guests read the host's wall clock time. It's
+ * really very simple: the guest sets aside a page of its RAM and asks the host
+ * to write time into that page (using an MSR). The host writes a structure
+ * containing the current time to this page - in theory the host updates this
+ * page constantly, but in reality that would be wasteful and the structure is
+ * only updated just before reentering the guest after some VM event.
+ * host更新clock的时间的函数在kvm_guest_time_update(), 只被vcpu_enter_guest()调用
+ */
+
 static int kvmclock __ro_after_init = 1;
 static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
@@ -45,6 +115,9 @@ static int parse_no_kvmclock(char *arg)
 early_param("no-kvmclock", parse_no_kvmclock);
 
 /* The hypervisor will put information about time periodically here */
+/*
+ * 相当与xen的vcpuinfo里的时间的部分 和host共享
+ */
 static struct pvclock_vsyscall_time_info *hv_clock;
 static struct pvclock_wall_clock *wall_clock;
 
@@ -273,6 +346,10 @@ static void __init kvm_memblock_free(phys_addr_t addr, phys_addr_t size)
 	memblock_free(addr, size);
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kernel/setup.c|1207| <<setup_arch>> kvmclock_init();
+ */
 void __init kvmclock_init(void)
 {
 	struct pvclock_vcpu_time_info *vcpu_time;
diff --git a/arch/x86/kvm/i8254.c b/arch/x86/kvm/i8254.c
index af19289..f5d8eb1 100644
--- a/arch/x86/kvm/i8254.c
+++ b/arch/x86/kvm/i8254.c
@@ -264,6 +264,12 @@ static void pit_do_work(struct kthread_work *work)
 			kvm_apic_nmi_wd_deliver(vcpu);
 }
 
+/*
+ * used by:
+ *   - arch/x86/kvm/i8254.c|680| <<kvm_create_pit>> pit_state->timer.function = pit_timer_fn;
+ *
+ * 传统pit的时钟函数, 都用lapic了
+ */
 static enum hrtimer_restart pit_timer_fn(struct hrtimer *data)
 {
 	struct kvm_kpit_state *ps = container_of(data, struct kvm_kpit_state, timer);
@@ -645,6 +651,10 @@ static const struct kvm_io_device_ops speaker_dev_ops = {
 	.write    = speaker_ioport_write,
 };
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/x86.c|4390| <<kvm_arch_vm_ioctl>> kvm->arch.vpit = kvm_create_pit(kvm, u.pit_config.flags);
+ */
 struct kvm_pit *kvm_create_pit(struct kvm *kvm, u32 flags)
 {
 	struct kvm_pit *pit;
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index b5cd846..78ba778 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -2021,6 +2021,10 @@ void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/x86.c|8473| <<kvm_vcpu_reset>> kvm_lapic_reset(vcpu, init_event);
+ */
 void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2139,6 +2143,12 @@ static const struct kvm_io_device_ops apic_mmio_ops = {
 	.write    = apic_mmio_write,
 };
 
+/*
+ * used by:
+ *   - arch/x86/kvm/lapic.c|2191| <<kvm_create_lapic>> apic->lapic_timer.timer.function = apic_timer_fn;
+ *
+ * kvm基于内核的apic timer的handler
+ */
 static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 {
 	struct kvm_timer *ktimer = container_of(data, struct kvm_timer, timer);
@@ -2154,6 +2164,20 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 		return HRTIMER_NORESTART;
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/x86.c|8737| <<kvm_arch_vcpu_init>> r = kvm_create_lapic(vcpu);
+ *
+ * QEMU和KVM都实现了对中断芯片的模拟,这是由于历史原因造成的.早在KVM诞生之前,
+ * QEMU就提供了一整套对设备的模拟,包括中断芯片.而KVM诞生之后,为了进一步提高
+ * 中断性能,因此又在KVM中实现了一套中断芯片.我们可以通过QEMU的启动参数
+ * kernel-irqchip来决定使用谁的中断芯片(irq chip)/
+ *   on: KVM模拟全部
+ *   split: QEMU模拟IOAPIC和PIC,KVM模拟LAPIC
+ *   off: QEMU模拟全部
+ *
+ * 分配并且初始化vcpu的apic (在内核中模拟)
+ */
 int kvm_create_lapic(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic;
@@ -2167,6 +2191,11 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 
 	vcpu->arch.apic = apic;
 
+	/*
+	 * 在vmx_vcpu_reset()写入vmcs的VIRTUAL_APIC_PAGE_ADDR
+	 *
+	 * 在kvm_lapic_reset()初始化regs的一些默认值
+	 */
 	apic->regs = (void *)get_zeroed_page(GFP_KERNEL);
 	if (!apic->regs) {
 		printk(KERN_ERR "malloc apic regs error for vcpu %x\n",
@@ -2177,6 +2206,7 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 
 	hrtimer_init(&apic->lapic_timer.timer, CLOCK_MONOTONIC,
 		     HRTIMER_MODE_ABS_PINNED);
+	/* apic timer的函数 */
 	apic->lapic_timer.timer.function = apic_timer_fn;
 
 	/*
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 97d4175..478b4e7 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -50,12 +50,34 @@
 #include "trace.h"
 
 /*
+ * 当QEMU通过ioctl创建vcpu时,调用kvm_mmu_create初始化mmu相关信息,
+ * 为页表项结构分配slab cache.
+ *
+ * 当KVM要进入Guest前,vcpu_enter_guest => kvm_mmu_reload会将根级
+ * 页表地址加载到VMCS,让Guest使用该页表.
+ *
+ * 当EPT Violation发生时,VMEXIT到KVM中.如果是缺页,则拿到对应的GPA,
+ * 根据GPA算出gfn,根据gfn找到对应的memory slot,得到对应的HVA.然后
+ * 根据HVA找到对应的pfn,确保该page位于内存.在把缺的页填上后,需要更
+ * 新EPT,完善其中缺少的页表项.于是从L4开始,逐层补全页表,对于在某层
+ * 上缺少的页表页,会从slab中分配后将新页的HPA填入到上一级页表中.
+ *
+ * 除了建立上级页表到下级页表的关联外,KVM 还会建立反向映射,可以直
+ * 接根据GPA找到gfn相关的页表项,而无需再次走EPT查询.
+ */
+
+/*
  * When setting this variable to true it enables Two-Dimensional-Paging
  * where the hardware walks 2 page tables:
  * 1. the guest-virtual to guest-physical
  * 2. while doing 1. it walks guest-physical to host-physical
  * If the hardware supports that we don't need to do shadow paging.
  */
+/*
+ * 在以下修改:
+ *   - arch/x86/kvm/mmu.c|5063| <<kvm_enable_tdp>> tdp_enabled = true;
+ *   - arch/x86/kvm/mmu.c|5069| <<kvm_disable_tdp>> tdp_enabled = false;
+ */
 bool tdp_enabled = false;
 
 enum {
@@ -171,10 +193,15 @@ struct pte_list_desc {
 };
 
 struct kvm_shadow_walk_iterator {
+	/* 发生page fault的GPA,迭代过程就是要把GPA所涉及的页表项都填上 */
 	u64 addr;
+	/* 当前页表项的 HPA，在 shadow_walk_init 中设置为 vcpu->arch.mmu.root_hpa */
 	hpa_t shadow_addr;
+	/* 指向当前页表项, 在shadow_walk_okay()中更新 */
 	u64 *sptep;
+	/* 当前层级,在shadow_walk_okay()中设置为4 (x86_64 PT64_ROOT_LEVEL), 在shadow_walk_next()中减1 */
 	int level;
+	/* 在当前level页表中的索引,在shadow_walk_okey中更新 */
 	unsigned index;
 };
 
@@ -1071,6 +1098,11 @@ static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	kvm_mmu_gfn_allow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1120| <<mmu_gfn_lpage_is_disallowed>> return __mmu_gfn_lpage_is_disallowed(gfn, level, slot);
+ *   - arch/x86/kvm/mmu.c|1198| <<mapping_level>> if (__mmu_gfn_lpage_is_disallowed(large_gfn, level, slot))
+ */
 static bool __mmu_gfn_lpage_is_disallowed(gfn_t gfn, int level,
 					  struct kvm_memory_slot *slot)
 {
@@ -1134,6 +1166,12 @@ gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu, gfn_t gfn,
 	return slot;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3977| <<tdp_page_fault>> level = mapping_level(vcpu, gfn, &force_pt_level);
+ *   - arch/x86/kvm/mmu.c|3345| <<nonpaging_map>> level = mapping_level(vcpu, gfn, &force_pt_level);
+ *   - arch/x86/kvm/paging_tmpl.h|786| <<FNAME(page_fault)>> level = mapping_level(vcpu, walker.gfn, &force_pt_level);
+ */
 static int mapping_level(struct kvm_vcpu *vcpu, gfn_t large_gfn,
 			 bool *force_pt_level)
 {
@@ -1143,6 +1181,12 @@ static int mapping_level(struct kvm_vcpu *vcpu, gfn_t large_gfn,
 	if (unlikely(*force_pt_level))
 		return PT_PAGE_TABLE_LEVEL;
 
+	/*
+	 * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+	 * struct kvm_memslots中有:
+	 *       struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+	 *       short id_to_index[KVM_MEM_SLOTS_NUM];
+	 */
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, large_gfn);
 	*force_pt_level = !memslot_valid_for_gpte(slot, true);
 	if (unlikely(*force_pt_level))
@@ -2334,6 +2378,16 @@ static void clear_sp_write_flooding_count(u64 *spte)
 	__clear_sp_write_flooding_count(sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3051| <<__direct_map>> sp = kvm_mmu_get_page(vcpu, pseudo_gfn, iterator.addr,
+ *   - arch/x86/kvm/mmu.c|3477| <<mmu_alloc_direct_roots>> sp = kvm_mmu_get_page(vcpu, 0, 0,
+ *   - arch/x86/kvm/mmu.c|3492| <<mmu_alloc_direct_roots>> sp = kvm_mmu_get_page(vcpu, i << (30 - PAGE_SHIFT),
+ *   - arch/x86/kvm/mmu.c|3532| <<mmu_alloc_shadow_roots>> sp = kvm_mmu_get_page(vcpu, root_gfn, 0,
+ *   - arch/x86/kvm/mmu.c|3569| <<mmu_alloc_shadow_roots>> sp = kvm_mmu_get_page(vcpu, root_gfn, i << 30, PT32_ROOT_LEVEL,
+ *   - arch/x86/kvm/paging_tmpl.h|633| <<FNAME>> sp = kvm_mmu_get_page(vcpu, table_gfn, addr, it.level-1,
+ *   - arch/x86/kvm/paging_tmpl.h|663| <<FNAME>> sp = kvm_mmu_get_page(vcpu, direct_gfn, addr, it.level-1,
+ */
 static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 					     gfn_t gfn,
 					     gva_t gaddr,
@@ -2999,9 +3053,19 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 	__direct_pte_prefetch(vcpu, sp, sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3999| <<tdp_page_fault>> r = __direct_map(vcpu, write, map_writable, level, gfn, pfn, prefault);
+ *   - arch/x86/kvm/mmu.c|3372| <<nonpaging_map>> r = __direct_map(vcpu, write, map_writable, level, gfn, pfn, prefault);
+ *
+ * 该函数建立了gfn到pfn的映射,同时将该page pin死在host的内存中
+ */
 static int __direct_map(struct kvm_vcpu *vcpu, int write, int map_writable,
 			int level, gfn_t gfn, kvm_pfn_t pfn, bool prefault)
 {
+	/*
+	 * 通过迭代器将ept中与该gpa相关的页表补充完整
+	 */
 	struct kvm_shadow_walk_iterator iterator;
 	struct kvm_mmu_page *sp;
 	int emulate = 0;
@@ -3010,6 +3074,31 @@ static int __direct_map(struct kvm_vcpu *vcpu, int write, int map_writable,
 	if (!VALID_PAGE(vcpu->arch.mmu.root_hpa))
 		return 0;
 
+	/*
+	 * 从leve4(root)开始,逐层补全页表对于每一层.
+	 *
+	 * 在每轮迭代中,sptep都会指向GPA在当前级页表中所对应的页表项,
+	 * 我们的目的就是把下一级页表的GPA填到该页表项内(即设置 sptep).
+	 * 因为是缺页,可能会出现下一级的页表页不存在的问题,这时候需要
+	 * 分配一个页表页,然后再将该页的GPA填进到sptep中.
+	 *
+	 * 举个例子,对于GPA(如0xfffff001),其二进制为:
+	 *
+	 * 000000000 000000011 111111111 111111111 000000000001
+	 *   PML4      PDPT       PD        PT        Offset
+	 *
+	 * 初始化状态: level = 4, shadow_addr = root_hpa, addr = GPA
+	 *
+	 * 执行流程：
+	 *
+	 * 1. index = addr在当前 level分段的值.如在level = 4时为0(000000000),在level = 3时为3(000000011)
+	 * 2. sptep = va(shadow_addr) + index, 得到GPA在当前地址中所对应的页表项HVA
+	 * 3. 如果sptep没值,分配一个page作为下级页表,同时将sptep设置为该page的HPA
+	 * 4. shadow_addr = *sptep,进入下级页表,循环
+	 *
+	 * 开启hugepage时,由于页表项管理的范围变大,所需页表级数减少,
+	 * 在默认情况下pag大小为2M,因此无需 level 1.
+	 */
 	for_each_shadow_entry(vcpu, (u64)gfn << PAGE_SHIFT, iterator) {
 		if (iterator.level == level) {
 			emulate = mmu_set_spte(vcpu, iterator.sptep, ACC_ALL,
@@ -3021,14 +3110,23 @@ static int __direct_map(struct kvm_vcpu *vcpu, int write, int map_writable,
 		}
 
 		drop_large_spte(vcpu, iterator.sptep);
+		/*
+		 * 如果下一级页表不存在, 即当前页表项没值
+		 */
 		if (!is_shadow_present_pte(*iterator.sptep)) {
 			u64 base_addr = iterator.addr;
 
 			base_addr &= PT64_LVL_ADDR_MASK(iterator.level);
 			pseudo_gfn = base_addr >> PAGE_SHIFT;
+			/*
+			 * 分配一个页表页结构
+			 */
 			sp = kvm_mmu_get_page(vcpu, pseudo_gfn, iterator.addr,
 					      iterator.level - 1, 1, ACC_ALL);
 
+			/*
+			 * 将新页表的hpa填入到当前页表项目(sptep)中
+			 */
 			link_shadow_page(vcpu, iterator.sptep, sp);
 		}
 	}
@@ -3213,6 +3311,11 @@ static bool is_access_allowed(u32 fault_err_code, u64 spte)
  * - true: let the vcpu to access on the same address again.
  * - false: let the real page fault path to fix it.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3997| <<tdp_page_fault>> if (fast_page_fault(vcpu, gpa, level, error_code))
+ *   - arch/x86/kvm/mmu.c|3370| <<nonpaging_map>> if (fast_page_fault(vcpu, v, level, error_code))
+ */
 static bool fast_page_fault(struct kvm_vcpu *vcpu, gva_t gva, int level,
 			    u32 error_code)
 {
@@ -3430,6 +3533,12 @@ static int mmu_check_root(struct kvm_vcpu *vcpu, gfn_t root_gfn)
 	return ret;
 }
 
+/*
+ * called by (vcpu->arch.mmu.direct_map是true的情况):
+ *   -  arch/x86/kvm/mmu.c|3582| <<mmu_alloc_roots>> return mmu_alloc_direct_roots(vcpu);
+ *
+ * 应该是分配ept的root的
+ */
 static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu_page *sp;
@@ -3470,6 +3579,10 @@ static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by (vcpu->arch.mmu.direct_map是false的情况):
+ *   - arch/x86/kvm/mmu.c|3624| <<mmu_alloc_roots>> return mmu_alloc_shadow_roots(vcpu);
+ */
 static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu_page *sp;
@@ -3571,12 +3684,24 @@ static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4739| <<kvm_mmu_load>> r = mmu_alloc_roots(vcpu);
+ */
 static int mmu_alloc_roots(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 设置direct_map的地方:
+	 *   - arch/x86/kvm/mmu.c|4003| <<nonpaging_init_context>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|4487| <<paging64_init_context_common>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4517| <<paging32_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4544| <<init_kvm_tdp_mmu>> context->direct_map = true;  ---------> ept似乎用这个!
+	 *   - arch/x86/kvm/mmu.c|4626| <<kvm_init_shadow_ept_mmu>> context->direct_map = false;
+	 */
 	if (vcpu->arch.mmu.direct_map)
-		return mmu_alloc_direct_roots(vcpu);
+		return mmu_alloc_direct_roots(vcpu);  // --> ept
 	else
-		return mmu_alloc_shadow_roots(vcpu);
+		return mmu_alloc_shadow_roots(vcpu);  // -->shadow
 }
 
 static void mmu_sync_roots(struct kvm_vcpu *vcpu)
@@ -3834,6 +3959,20 @@ bool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)
 	return kvm_x86_ops->interrupt_allowed(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4003| <<tdp_page_fault>> if (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, write, &map_writable))
+ *   - arch/x86/kvm/mmu.c|3376| <<nonpaging_map>> if (try_async_pf(vcpu, prefault, gfn, v, &pfn, write, &map_writable))
+ *   - arch/x86/kvm/paging_tmpl.h|797| <<FNAME(page_fault)>> if (try_async_pf(vcpu, prefault, walker.gfn, addr, &pfn, write_fault,
+ *
+ * 1. 根据gfn找到对应的memslot
+ * 2. 用memslot的起始hva(userspace_addr)+(gfn-slot中的起始gfn(base_gfn))*页大小(PAGE_SIZE)
+ *    得到gfn对应的起始hva
+ * 3. 为该hva分配一个物理页,有hva_to_pfn_fast()和hva_to_pfn_slow()两种,hva_to_pfn_fast()
+ *    实际上是调用__get_user_pages_fast(),会尝试去pin该page,即确保该地址所在的物理页在内存中.
+ *    如果失败,退化到hva_to_pfn_slow(),会先去拿mm->mmap_sem的锁然后调用__get_user_pages()来pin
+ * 4. 如果分配成功,对其返回的struct page调用page_to_pfn()得到对应的pfn
+ */
 static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 			 gva_t gva, kvm_pfn_t *pfn, bool write, bool *writable)
 {
@@ -3848,8 +3987,14 @@ static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 		return false;
 	}
 
+	/*
+	 * 找到gfn对应的slot
+	 */
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
 	async = false;
+	/*
+	 * 找到gfn对应的pfn
+	 */
 	*pfn = __gfn_to_pfn_memslot(slot, gfn, false, &async, write, writable);
 	if (!async)
 		return false; /* *pfn has correct page already */
@@ -3868,6 +4013,11 @@ static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|7094| <<handle_exception>> return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
+ *   - arch/x86/kvm/svm.c|2644| <<pf_interception>> return kvm_handle_page_fault(&svm->vcpu, error_code, fault_address,
+ */
 int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 				u64 fault_address, char *insn, int insn_len)
 {
@@ -3910,6 +4060,18 @@ check_hugepage_cache_consistency(struct kvm_vcpu *vcpu, gfn_t gfn, int level)
 	return kvm_mtrr_check_gfn_range_consistency(vcpu, gfn, page_num);
 }
 
+/*
+ * used by:
+ *   - arch/x86/kvm/mmu.c|4521| <<init_kvm_tdp_mmu>> context->page_fault = tdp_page_fault
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5000| <<kvm_mmu_page_fault>> r = vcpu->arch.mmu.page_fault(vcpu, cr2, lower_32_bits(error_code),
+ *   - arch/x86/kvm/x86.c|9270| <<kvm_arch_async_page_ready>> vcpu->arch.mmu.page_fault(vcpu, work->gva, 0, true);
+ *
+ * 主要有两步:
+ * 1. 获取gpa对应的物理页,如果没有会进行分配
+ * 2. 更新ept
+ */
 static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 			  bool prefault)
 {
@@ -3947,6 +4109,9 @@ static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 	mmu_seq = vcpu->kvm->mmu_notifier_seq;
 	smp_rmb();
 
+	/*
+	 * 将gfn转换为pfn
+	 */
 	if (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, write, &map_writable))
 		return RET_PF_RETRY;
 
@@ -3960,6 +4125,9 @@ static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 		goto out_unlock;
 	if (likely(!force_pt_level))
 		transparent_hugepage_adjust(vcpu, &gfn, &pfn, &level);
+	/*
+	 * 更新ept, 将新的映射关系逐层添加到ept中
+	 */
 	r = __direct_map(vcpu, write, map_writable, level, gfn, pfn, prefault);
 	spin_unlock(&vcpu->kvm->mmu_lock);
 
@@ -4505,6 +4673,12 @@ static void paging32E_init_context(struct kvm_vcpu *vcpu,
 	paging64_init_context_common(vcpu, context, PT32E_ROOT_LEVEL);
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/mmu.c|4692| <<init_kvm_mmu>> init_kvm_tdp_mmu(vcpu);
+ *
+ * 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu)
+ */
 static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *context = &vcpu->arch.mmu;
@@ -4666,11 +4840,18 @@ static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu)
 	update_last_nonleaf_level(vcpu, g_context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4682| <<kvm_mmu_reset_context>> init_kvm_mmu(vcpu);
+ *   - arch/x86/kvm/mmu.c|5114| <<kvm_mmu_setup>> init_kvm_mmu(vcpu);
+ *
+ * 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu)
+ */
 static void init_kvm_mmu(struct kvm_vcpu *vcpu)
 {
 	if (mmu_is_nested(vcpu))
 		init_kvm_nested_mmu(vcpu);
-	else if (tdp_enabled)
+	else if (tdp_enabled) // 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu)
 		init_kvm_tdp_mmu(vcpu);
 	else
 		init_kvm_softmmu(vcpu);
@@ -4683,6 +4864,11 @@ void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_reset_context);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|85| <<kvm_mmu_reload>> return kvm_mmu_load(vcpu);
+ *   - arch/x86/kvm/svm.c|3384| <<nested_svm_vmexit>> kvm_mmu_load(&svm->vcpu);
+ */
 int kvm_mmu_load(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -4695,6 +4881,10 @@ int kvm_mmu_load(struct kvm_vcpu *vcpu)
 	if (r)
 		goto out;
 	/* set_cr3() should ensure TLB has been flushed */
+	/*
+	 * intel : vmx_set_cr3()
+	 * amd   : svm_set_cr3()
+	 */
 	vcpu->arch.mmu.set_cr3(vcpu, vcpu->arch.mmu.root_hpa);
 out:
 	return r;
@@ -4957,14 +5147,30 @@ static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3888| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+ *   - arch/x86/kvm/vmx.c|7702| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx.c|7734| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ *   - arch/x86/kvm/svm.c|2656| <<npf_interception>> return kvm_mmu_page_fault(&svm->vcpu, fault_address, error_code,
+ */
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t cr2, u64 error_code,
 		       void *insn, int insn_len)
 {
 	int r, emulation_type = 0;
 	enum emulation_result er;
+	/* struct kvm_vcpu_arch arch是struct kvm_cpu的一部分 */
 	bool direct = vcpu->arch.mmu.direct_map;
 
 	/* With shadow page tables, fault_address contains a GVA or nGPA.  */
+	/*
+	 * 设置direct_map的地方:
+	 *   - arch/x86/kvm/mmu.c|4003| <<nonpaging_init_context>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|4487| <<paging64_init_context_common>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4517| <<paging32_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4544| <<init_kvm_tdp_mmu>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|4626| <<kvm_init_shadow_ept_mmu>> context->direct_map = false;
+	 */
 	if (vcpu->arch.mmu.direct_map) {
 		vcpu->arch.gpa_available = true;
 		vcpu->arch.gpa_val = cr2;
@@ -4977,7 +5183,15 @@ int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t cr2, u64 error_code,
 			goto emulate;
 	}
 
+	/*
+	 * struct kvm_vcpu_arch arch是struct kvm_cpu的一部分
+	 * struct kvm_mmu mmu是struct kvm_vcpu_arch arch的一部分
+	 */
+
 	if (r == RET_PF_INVALID) {
+		/*
+		 * 似乎是tdp_page_fault()??
+		 */
 		r = vcpu->arch.mmu.page_fault(vcpu, cr2, lower_32_bits(error_code),
 					      false);
 		WARN_ON(r == RET_PF_INVALID);
@@ -5049,6 +5263,11 @@ void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_invlpg);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|7870| <<vmx_enable_tdp>> kvm_enable_tdp();
+ *   - arch/x86/kvm/svm.c|1362| <<svm_hardware_setup>> kvm_enable_tdp();
+ */
 void kvm_enable_tdp(void)
 {
 	tdp_enabled = true;
@@ -5067,6 +5286,10 @@ static void free_mmu_pages(struct kvm_vcpu *vcpu)
 	free_page((unsigned long)vcpu->arch.mmu.lm_root);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5167| <<kvm_mmu_create>> return alloc_mmu_pages(vcpu);
+ */
 static int alloc_mmu_pages(struct kvm_vcpu *vcpu)
 {
 	struct page *page;
@@ -5088,6 +5311,9 @@ static int alloc_mmu_pages(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by only kvm_arch_vcpu_init()
+ */
 int kvm_mmu_create(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.walk_mmu = &vcpu->arch.mmu;
@@ -5098,10 +5324,18 @@ int kvm_mmu_create(struct kvm_vcpu *vcpu)
 	return alloc_mmu_pages(vcpu);
 }
 
+/*
+ * called by only:
+ *   - arch/x86/kvm/x86.c|8426| <<kvm_arch_vcpu_setup>> kvm_mmu_setup(vcpu);
+ *
+ * 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu)
+ */
 void kvm_mmu_setup(struct kvm_vcpu *vcpu)
 {
+	/* 如果root_hpa已经有效了就要warning! */
 	MMU_WARN_ON(VALID_PAGE(vcpu->arch.mmu.root_hpa));
 
+	/* 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu) */
 	init_kvm_mmu(vcpu);
 }
 
@@ -5413,6 +5647,12 @@ static void kvm_zap_obsolete_pages(struct kvm *kvm)
 }
 
 /*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5346| <<kvm_mmu_invalidate_zap_pages_in_memslot>> kvm_mmu_invalidate_zap_all_pages(kvm);
+ *   - arch/x86/kvm/mmu.c|5692| <<kvm_mmu_invalidate_mmio_sptes>> kvm_mmu_invalidate_zap_all_pages(kvm);
+ *   - arch/x86/kvm/x86.c|9234| <<kvm_arch_flush_shadow_all>> kvm_mmu_invalidate_zap_all_pages(kvm);
+ */
+/*
  * Fast invalidate all shadow pages and use lock-break technique
  * to zap obsolete pages.
  *
@@ -5538,6 +5778,9 @@ static void mmu_destroy_caches(void)
 	kmem_cache_destroy(mmu_page_header_cache);
 }
 
+/*
+ * called only by kvm_arch_init()
+ */
 int kvm_mmu_module_init(void)
 {
 	int ret = -ENOMEM;
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 5b408c0..60c5781 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -77,6 +77,12 @@ static inline unsigned int kvm_mmu_available_pages(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|9303| <<nested_vmx_eptp_switching>> kvm_mmu_reload(vcpu);
+ *   - arch/x86/kvm/x86.c|7450| <<vcpu_enter_guest>> r = kvm_mmu_reload(vcpu); --> 进入guest mode前更新eptp
+ *   - arch/x86/kvm/x86.c|9309| <<kvm_arch_async_page_ready>> r = kvm_mmu_reload(vcpu);
+ */
 static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
 {
 	if (likely(vcpu->arch.mmu.root_hpa != INVALID_PAGE))
diff --git a/arch/x86/kvm/page_track.c b/arch/x86/kvm/page_track.c
index 3052a59..73ec703 100644
--- a/arch/x86/kvm/page_track.c
+++ b/arch/x86/kvm/page_track.c
@@ -34,6 +34,10 @@ void kvm_page_track_free_memslot(struct kvm_memory_slot *free,
 		}
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/x86.c|9092| <<kvm_arch_create_memslot>> if (kvm_page_track_create_memslot(slot, npages))
+ */
 int kvm_page_track_create_memslot(struct kvm_memory_slot *slot,
 				  unsigned long npages)
 {
diff --git a/arch/x86/kvm/pmu_intel.c b/arch/x86/kvm/pmu_intel.c
index 5ab4a36..9842174 100644
--- a/arch/x86/kvm/pmu_intel.c
+++ b/arch/x86/kvm/pmu_intel.c
@@ -67,6 +67,9 @@ static void global_ctrl_changed(struct kvm_pmu *pmu, u64 data)
 		reprogram_counter(pmu, bit);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.find_arch_event = intel_find_arch_event()
+ */
 static unsigned intel_find_arch_event(struct kvm_pmu *pmu,
 				      u8 event_select,
 				      u8 unit_mask)
@@ -85,6 +88,9 @@ static unsigned intel_find_arch_event(struct kvm_pmu *pmu,
 	return intel_arch_events[i].event_type;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.find_fixed_event = intel_find_fixed_event()
+ */
 static unsigned intel_find_fixed_event(int idx)
 {
 	if (idx >= ARRAY_SIZE(fixed_pmc_events))
@@ -94,6 +100,9 @@ static unsigned intel_find_fixed_event(int idx)
 }
 
 /* check if a PMC is enabled by comparing it with globl_ctrl bits. */
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.pmc_is_enabled = intel_pmc_is_enabled()
+ */
 static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -101,6 +110,9 @@ static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 	return test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.pmc_idx_to_pmc = intel_pmc_idx_to_pmc()
+ */
 static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
 	if (pmc_idx < INTEL_PMC_IDX_FIXED)
@@ -114,6 +126,9 @@ static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 }
 
 /* returns 0 if idx's corresponding MSR exists; otherwise returns 1. */
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.is_valid_msr_idx = intel_is_valid_msr_idx()
+ */
 static int intel_is_valid_msr_idx(struct kvm_vcpu *vcpu, unsigned idx)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -125,6 +140,9 @@ static int intel_is_valid_msr_idx(struct kvm_vcpu *vcpu, unsigned idx)
 		(fixed && idx >= pmu->nr_arch_fixed_counters);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.msr_idx_to_pmc = intel_msr_idx_to_pmc()
+ */
 static struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu,
 					    unsigned idx)
 {
@@ -142,6 +160,9 @@ static struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu,
 	return &counters[idx];
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.is_valid_msr = intel_is_valid_msr()
+ */
 static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -164,6 +185,9 @@ static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 	return ret;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.get_msr = intel_pmu_get_msr()
+ */
 static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -196,6 +220,9 @@ static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.set_msr = intel_pmu_set_msr()
+ */
 static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -254,6 +281,9 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.refresh = intel_pmu_refresh()
+ */
 static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -305,6 +335,9 @@ static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 		pmu->reserved_bits ^= HSW_IN_TX|HSW_IN_TX_CHECKPOINTED;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.init = intel_pmu_init()
+ */
 static void intel_pmu_init(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -323,6 +356,9 @@ static void intel_pmu_init(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.reset = intel_pmu_reset()
+ */
 static void intel_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -342,6 +378,9 @@ static void intel_pmu_reset(struct kvm_vcpu *vcpu)
 		pmu->global_ovf_ctrl = 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.pmu_ops = &intel_pmu_ops
+ */
 struct kvm_pmu_ops intel_pmu_ops = {
 	.find_arch_event = intel_find_arch_event,
 	.find_fixed_event = intel_find_fixed_event,
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index d0c3be3..03792f1 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -1801,6 +1801,9 @@ static inline bool cpu_has_virtual_nmis(void)
 	return vmcs_config.pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.has_wbinvd_exit = cpu_has_vmx_wbinvd_exit()
+ */
 static inline bool cpu_has_vmx_wbinvd_exit(void)
 {
 	return vmcs_config.cpu_based_2nd_exec_ctrl &
@@ -1836,12 +1839,18 @@ static inline bool cpu_has_vmx_vmfunc(void)
 		SECONDARY_EXEC_ENABLE_VMFUNC;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.umip_emulated = vmx_umip_emulated()
+ */
 static bool vmx_umip_emulated(void)
 {
 	return vmcs_config.cpu_based_2nd_exec_ctrl &
 		SECONDARY_EXEC_DESC;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cpu_has_accelerated_tpr = report_flexpriority()
+ */
 static inline bool report_flexpriority(void)
 {
 	return flexpriority_enabled;
@@ -2448,6 +2457,9 @@ static u32 vmx_read_guest_seg_ar(struct vcpu_vmx *vmx, unsigned seg)
 	return *p;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.update_bp_intercept = update_exception_bitmap()
+ */
 static void update_exception_bitmap(struct kvm_vcpu *vcpu)
 {
 	u32 eb;
@@ -2745,6 +2757,9 @@ static unsigned long segment_base(u16 selector)
 }
 #endif
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.prepare_guest_switch = vmx_save_host_state()
+ */
 static void vmx_save_host_state(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -2920,6 +2935,9 @@ static void decache_tsc_multiplier(struct vcpu_vmx *vmx)
  * Switches to specified vcpu, until a matching vcpu_put(), but assumes
  * vcpu mutex is already taken.
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_load = vmx_vcpu_load()
+ */
 static void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3001,6 +3019,9 @@ static void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)
 		pi_set_sn(pi_desc);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_put = vmx_vcpu_put()
+ */
 static void vmx_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	vmx_vcpu_pi_put(vcpu);
@@ -3031,6 +3052,9 @@ static inline unsigned long nested_read_cr4(struct vmcs12 *fields)
 		(fields->cr4_read_shadow & fields->cr4_guest_host_mask);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_rflags = vmx_get_rflags()
+ */
 static unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)
 {
 	unsigned long rflags, save_rflags;
@@ -3048,6 +3072,9 @@ static unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)
 	return to_vmx(vcpu)->rflags;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_rflags = vmx_set_rflags()
+ */
 static void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 {
 	unsigned long old_rflags = vmx_get_rflags(vcpu);
@@ -3064,6 +3091,9 @@ static void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 		to_vmx(vcpu)->emulation_required = emulation_required(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_interrupt_shadow = vmx_get_interrupt_shadow()
+ */
 static u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)
 {
 	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
@@ -3077,6 +3107,9 @@ static u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)
 	return ret;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_interrupt_shadow = vmx_set_interrupt_shadow()
+ */
 static void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)
 {
 	u32 interruptibility_old = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
@@ -3093,6 +3126,9 @@ static void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)
 		vmcs_write32(GUEST_INTERRUPTIBILITY_INFO, interruptibility);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.skip_emulated_instruction = skip_emulated_instruction()
+ */
 static void skip_emulated_instruction(struct kvm_vcpu *vcpu)
 {
 	unsigned long rip;
@@ -3183,6 +3219,9 @@ static void vmx_clear_hlt(struct kvm_vcpu *vcpu)
 		vmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.queue_exception = vmx_queue_exception()
+ */
 static void vmx_queue_exception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3219,11 +3258,17 @@ static void vmx_queue_exception(struct kvm_vcpu *vcpu)
 	vmx_clear_hlt(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.rdtscp_supported = vmx_rdtscp_supported()
+ */
 static bool vmx_rdtscp_supported(void)
 {
 	return cpu_has_vmx_rdtscp();
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.invpcid_supported = vmx_invpcid_supported()
+ */
 static bool vmx_invpcid_supported(void)
 {
 	return cpu_has_vmx_invpcid() && enable_ept;
@@ -3284,6 +3329,9 @@ static void setup_msrs(struct vcpu_vmx *vmx)
 		vmx_update_msr_bitmap(&vmx->vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.read_l1_tsc_offset = vmx_read_l1_tsc_offset()
+ */
 static u64 vmx_read_l1_tsc_offset(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -3298,6 +3346,9 @@ static u64 vmx_read_l1_tsc_offset(struct kvm_vcpu *vcpu)
 /*
  * writes 'offset' into guest's timestamp counter offset register
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.write_tsc_offset = vmx_write_tsc_offset()
+ */
 static void vmx_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)
 {
 	if (is_guest_mode(vcpu)) {
@@ -3903,6 +3954,9 @@ static inline bool vmx_feature_control_msr_valid(struct kvm_vcpu *vcpu,
 	return !(val & ~valid_bits);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_msr_feature = vmx_get_msr_feature()
+ */
 static int vmx_get_msr_feature(struct kvm_msr_entry *msr)
 {
 	switch (msr->index) {
@@ -3922,6 +3976,9 @@ static int vmx_get_msr_feature(struct kvm_msr_entry *msr)
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_msr = vmx_get_msr()
+ */
 static int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4015,6 +4072,9 @@ static void vmx_leave_nested(struct kvm_vcpu *vcpu);
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_msr = vmx_set_msr()
+ */
 static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4203,6 +4263,9 @@ static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return ret;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cache_reg = vmx_cache_reg()
+ */
 static void vmx_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)
 {
 	__set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
@@ -4222,11 +4285,17 @@ static void vmx_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cpu_has_kvm_support = cpu_has_kvm_support()
+ */
 static __init int cpu_has_kvm_support(void)
 {
 	return cpu_has_vmx();
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.disabled_by_bios = vmx_disabled_by_bios()
+ */
 static __init int vmx_disabled_by_bios(void)
 {
 	u64 msr;
@@ -4264,6 +4333,9 @@ static void kvm_cpu_vmxon(u64 addr)
 			: "memory", "cc");
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hardware_enable = hardware_enable()
+ */
 static int hardware_enable(void)
 {
 	int cpu = raw_smp_processor_id();
@@ -4336,6 +4408,9 @@ static void kvm_cpu_vmxoff(void)
 	cr4_clear_bits(X86_CR4_VMXE);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hardware_disable = hardware_disable()
+ */
 static void hardware_disable(void)
 {
 	vmclear_local_loaded_vmcss();
@@ -4917,6 +4992,9 @@ static void enter_rmode(struct kvm_vcpu *vcpu)
 	kvm_mmu_reset_context(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_efer = vmx_set_efer()
+ */
 static void vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4981,11 +5059,17 @@ static inline void __vmx_flush_tlb(struct kvm_vcpu *vcpu, int vpid,
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.tlb_flush = vmx_flush_tlb()
+ */
 static void vmx_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 {
 	__vmx_flush_tlb(vcpu, to_vmx(vcpu)->vpid, invalidate_gpa);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.decache_cr0_guest_bits = vmx_decache_cr0_guest_bits()
+ */
 static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu)
 {
 	ulong cr0_guest_owned_bits = vcpu->arch.cr0_guest_owned_bits;
@@ -4994,6 +5078,9 @@ static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu)
 	vcpu->arch.cr0 |= vmcs_readl(GUEST_CR0) & cr0_guest_owned_bits;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.decache_cr3 = vmx_decache_cr3()
+ */
 static void vmx_decache_cr3(struct kvm_vcpu *vcpu)
 {
 	if (enable_unrestricted_guest || (enable_ept && is_paging(vcpu)))
@@ -5001,6 +5088,9 @@ static void vmx_decache_cr3(struct kvm_vcpu *vcpu)
 	__set_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.decache_cr4_guest_bits = vmx_decache_cr4_guest_bits()
+ */
 static void vmx_decache_cr4_guest_bits(struct kvm_vcpu *vcpu)
 {
 	ulong cr4_guest_owned_bits = vcpu->arch.cr4_guest_owned_bits;
@@ -5106,6 +5196,9 @@ static void ept_update_paging_mode_cr0(unsigned long *hw_cr0,
 		*hw_cr0 &= ~X86_CR0_WP;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_cr0 = vmx_set_cr0()
+ */
 static void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5144,6 +5237,9 @@ static void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 	vmx->emulation_required = emulation_required(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_tdp_level = get_ept_level()
+ */
 static int get_ept_level(struct kvm_vcpu *vcpu)
 {
 	if (cpu_has_vmx_ept_5levels() && (cpuid_maxphyaddr(vcpu) > 48))
@@ -5165,6 +5261,11 @@ static u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa)
 	return eptp;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_cr3 = vmx_set_cr3()
+ *
+ * called only by kvm_mmu_reload()-->kvm_mmu_load()
+ */
 static void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)
 {
 	unsigned long guest_cr3;
@@ -5186,6 +5287,9 @@ static void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)
 	vmcs_writel(GUEST_CR3, guest_cr3);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_cr4 = vmx_set_cr4()
+ */
 static int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 {
 	/*
@@ -5260,6 +5364,9 @@ static int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_segment = vmx_get_segment()
+ */
 static void vmx_get_segment(struct kvm_vcpu *vcpu,
 			    struct kvm_segment *var, int seg)
 {
@@ -5297,6 +5404,9 @@ static void vmx_get_segment(struct kvm_vcpu *vcpu,
 	var->g = (ar >> 15) & 1;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_segment_base = vmx_get_segment_base()
+ */
 static u64 vmx_get_segment_base(struct kvm_vcpu *vcpu, int seg)
 {
 	struct kvm_segment s;
@@ -5308,6 +5418,9 @@ static u64 vmx_get_segment_base(struct kvm_vcpu *vcpu, int seg)
 	return vmx_read_guest_seg_base(to_vmx(vcpu), seg);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_cpl = vmx_get_cpl()
+ */
 static int vmx_get_cpl(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5340,6 +5453,9 @@ static u32 vmx_segment_access_rights(struct kvm_segment *var)
 	return ar;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_segment = vmx_set_segment()
+ */
 static void vmx_set_segment(struct kvm_vcpu *vcpu,
 			    struct kvm_segment *var, int seg)
 {
@@ -5381,6 +5497,9 @@ static void vmx_set_segment(struct kvm_vcpu *vcpu,
 	vmx->emulation_required = emulation_required(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_cs_db_l_bits = vmx_get_cs_db_l_bits()
+ */
 static void vmx_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l)
 {
 	u32 ar = vmx_read_guest_seg_ar(to_vmx(vcpu), VCPU_SREG_CS);
@@ -5389,24 +5508,36 @@ static void vmx_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l)
 	*l = (ar >> 13) & 1;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_idt = vmx_get_idt()
+ */
 static void vmx_get_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 {
 	dt->size = vmcs_read32(GUEST_IDTR_LIMIT);
 	dt->address = vmcs_readl(GUEST_IDTR_BASE);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_idt = vmx_set_idt()
+ */
 static void vmx_set_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 {
 	vmcs_write32(GUEST_IDTR_LIMIT, dt->size);
 	vmcs_writel(GUEST_IDTR_BASE, dt->address);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_gdt = vmx_get_gdt()
+ */
 static void vmx_get_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 {
 	dt->size = vmcs_read32(GUEST_GDTR_LIMIT);
 	dt->address = vmcs_readl(GUEST_GDTR_BASE);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_gdt = vmx_set_gdt()
+ */
 static void vmx_set_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 {
 	vmcs_write32(GUEST_GDTR_LIMIT, dt->size);
@@ -5950,6 +6081,9 @@ static void vmx_update_msr_bitmap(struct kvm_vcpu *vcpu)
 	vmx->msr_bitmap_mode = mode;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_enable_apicv = vmx_get_enable_apicv()
+ */
 static bool vmx_get_enable_apicv(struct kvm_vcpu *vcpu)
 {
 	return enable_apicv;
@@ -6075,6 +6209,9 @@ static int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,
  * 2. If target vcpu isn't running(root mode), kick it to pick up the
  * interrupt from PIR in next vmentry.
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.deliver_posted_interrupt = vmx_deliver_posted_interrupt()
+ */
 static void vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6184,6 +6321,9 @@ static u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)
 	return pin_based_exec_ctrl;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.refresh_apicv_exec_ctrl = vmx_refresh_apicv_exec_ctrl()
+ */
 static void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6501,6 +6641,9 @@ static void vmx_vcpu_setup(struct vcpu_vmx *vmx)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_reset = vmx_vcpu_reset()
+ */
 static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6573,6 +6716,10 @@ static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 
 	if (cpu_has_vmx_tpr_shadow() && !init_event) {
 		vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
+		/*
+		 * This 64-bit field is the physical address of the 4-KByte
+		 * virtual APIC page.
+		 */
 		if (cpu_need_tpr_shadow(vcpu))
 			vmcs_write64(VIRTUAL_APIC_PAGE_ADDR,
 				     __pa(vcpu->arch.apic->regs));
@@ -6622,12 +6769,18 @@ static bool nested_exit_on_nmi(struct kvm_vcpu *vcpu)
 	return nested_cpu_has_nmi_exiting(get_vmcs12(vcpu));
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.enable_irq_window = enable_irq_window()
+ */
 static void enable_irq_window(struct kvm_vcpu *vcpu)
 {
 	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,
 		      CPU_BASED_VIRTUAL_INTR_PENDING);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.enable_nmi_window = enable_nmi_window()
+ */
 static void enable_nmi_window(struct kvm_vcpu *vcpu)
 {
 	if (!enable_vnmi ||
@@ -6640,6 +6793,9 @@ static void enable_nmi_window(struct kvm_vcpu *vcpu)
 		      CPU_BASED_VIRTUAL_NMI_PENDING);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_irq = vmx_inject_irq()
+ */
 static void vmx_inject_irq(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6669,6 +6825,9 @@ static void vmx_inject_irq(struct kvm_vcpu *vcpu)
 	vmx_clear_hlt(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_nmi = vmx_inject_nmi()
+ */
 static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6701,6 +6860,9 @@ static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 	vmx_clear_hlt(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_nmi_mask = vmx_get_nmi_mask()
+ */
 static bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6715,6 +6877,9 @@ static bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu)
 	return masked;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_nmi_mask = vmx_set_nmi_mask()
+ */
 static void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6735,6 +6900,9 @@ static void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.nmi_allowed = vmx_nmi_allowed()
+ */
 static int vmx_nmi_allowed(struct kvm_vcpu *vcpu)
 {
 	if (to_vmx(vcpu)->nested.nested_run_pending)
@@ -6749,6 +6917,9 @@ static int vmx_nmi_allowed(struct kvm_vcpu *vcpu)
 		   | GUEST_INTR_STATE_NMI));
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.interrupt_allowed = vmx_interrupt_allowed()
+ */
 static int vmx_interrupt_allowed(struct kvm_vcpu *vcpu)
 {
 	return (!to_vmx(vcpu)->nested.nested_run_pending &&
@@ -6757,6 +6928,9 @@ static int vmx_interrupt_allowed(struct kvm_vcpu *vcpu)
 			(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_tss_addr = vmx_set_tss_addr()
+ */
 static int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr)
 {
 	int ret;
@@ -6772,6 +6946,9 @@ static int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr)
 	return init_rmode_tss(kvm);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_identity_map_addr = vmx_set_identity_map_addr()
+ */
 static int vmx_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)
 {
 	to_kvm_vmx(kvm)->ept_identity_map_addr = ident_addr;
@@ -6856,12 +7033,18 @@ static void kvm_machine_check(void)
 #endif
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MCE_DURING_VMENTRY]
+ */
 static int handle_machine_check(struct kvm_vcpu *vcpu)
 {
 	/* already handled by vcpu_run */
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_EXCEPTION_NMI]
+ */
 static int handle_exception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6967,12 +7150,18 @@ static int handle_exception(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_EXTERNAL_INTERRUPT]
+ */
 static int handle_external_interrupt(struct kvm_vcpu *vcpu)
 {
 	++vcpu->stat.irq_exits;
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_TRIPLE_FAULT]
+ */
 static int handle_triple_fault(struct kvm_vcpu *vcpu)
 {
 	vcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;
@@ -6980,6 +7169,9 @@ static int handle_triple_fault(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_IO_INSTRUCTION]
+ */
 static int handle_io(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
@@ -7001,6 +7193,9 @@ static int handle_io(struct kvm_vcpu *vcpu)
 	return kvm_fast_pio(vcpu, size, port, in);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.patch_hypercall = vmx_patch_hypercall()
+ */
 static void
 vmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 {
@@ -7063,12 +7258,19 @@ static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)
 		return kvm_set_cr4(vcpu, val);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_GDTR_IDTR]
+ * kvm_vmx_exit_handlers[EXIT_REASON_LDTR_TR]
+ */
 static int handle_desc(struct kvm_vcpu *vcpu)
 {
 	WARN_ON(!(vcpu->arch.cr4 & X86_CR4_UMIP));
 	return emulate_instruction(vcpu, 0) == EMULATE_DONE;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_CR_ACCESS]
+ */
 static int handle_cr(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification, val;
@@ -7149,6 +7351,9 @@ static int handle_cr(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_DR_ACCESS]
+ */
 static int handle_dr(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
@@ -7213,15 +7418,24 @@ static int handle_dr(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_dr6 = vmx_get_dr6()
+ */
 static u64 vmx_get_dr6(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.dr6;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_dr6 = vmx_set_dr6()
+ */
 static void vmx_set_dr6(struct kvm_vcpu *vcpu, unsigned long val)
 {
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.sync_dirty_debug_regs = vmx_sync_dirty_debug_regs()
+ */
 static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
 {
 	get_debugreg(vcpu->arch.db[0], 0);
@@ -7235,16 +7449,25 @@ static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
 	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_dr7 = vmx_set_dr7()
+ */
 static void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)
 {
 	vmcs_writel(GUEST_DR7, val);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_CPUID]
+ */
 static int handle_cpuid(struct kvm_vcpu *vcpu)
 {
 	return kvm_emulate_cpuid(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MSR_READ]
+ */
 static int handle_rdmsr(struct kvm_vcpu *vcpu)
 {
 	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
@@ -7266,6 +7489,9 @@ static int handle_rdmsr(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MSR_WRITE]
+ */
 static int handle_wrmsr(struct kvm_vcpu *vcpu)
 {
 	struct msr_data msr;
@@ -7286,12 +7512,18 @@ static int handle_wrmsr(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_TPR_BELOW_THRESHOLD]
+ */
 static int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)
 {
 	kvm_apic_update_ppr(vcpu);
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_PENDING_INTERRUPT]
+ */
 static int handle_interrupt_window(struct kvm_vcpu *vcpu)
 {
 	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
@@ -7303,21 +7535,33 @@ static int handle_interrupt_window(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_HLT]
+ */
 static int handle_halt(struct kvm_vcpu *vcpu)
 {
 	return kvm_emulate_halt(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMCALL]
+ */
 static int handle_vmcall(struct kvm_vcpu *vcpu)
 {
 	return kvm_emulate_hypercall(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_INVD]
+ */
 static int handle_invd(struct kvm_vcpu *vcpu)
 {
 	return emulate_instruction(vcpu, 0) == EMULATE_DONE;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_INVLPG]
+ */
 static int handle_invlpg(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
@@ -7326,6 +7570,9 @@ static int handle_invlpg(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_RDPMC]
+ */
 static int handle_rdpmc(struct kvm_vcpu *vcpu)
 {
 	int err;
@@ -7334,11 +7581,17 @@ static int handle_rdpmc(struct kvm_vcpu *vcpu)
 	return kvm_complete_insn_gp(vcpu, err);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_WBINVD]
+ */
 static int handle_wbinvd(struct kvm_vcpu *vcpu)
 {
 	return kvm_emulate_wbinvd(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_XSETBV]
+ */
 static int handle_xsetbv(struct kvm_vcpu *vcpu)
 {
 	u64 new_bv = kvm_read_edx_eax(vcpu);
@@ -7349,6 +7602,9 @@ static int handle_xsetbv(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_XSAVES]
+ */
 static int handle_xsaves(struct kvm_vcpu *vcpu)
 {
 	kvm_skip_emulated_instruction(vcpu);
@@ -7356,6 +7612,9 @@ static int handle_xsaves(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_XRSTORS]
+ */
 static int handle_xrstors(struct kvm_vcpu *vcpu)
 {
 	kvm_skip_emulated_instruction(vcpu);
@@ -7363,6 +7622,9 @@ static int handle_xrstors(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_APIC_ACCESS]
+ */
 static int handle_apic_access(struct kvm_vcpu *vcpu)
 {
 	if (likely(fasteoi)) {
@@ -7385,6 +7647,9 @@ static int handle_apic_access(struct kvm_vcpu *vcpu)
 	return emulate_instruction(vcpu, 0) == EMULATE_DONE;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_EOI_INDUCED]
+ */
 static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
@@ -7395,6 +7660,9 @@ static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_APIC_WRITE]
+ */
 static int handle_apic_write(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
@@ -7405,6 +7673,9 @@ static int handle_apic_write(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_TASK_SWITCH]
+ */
 static int handle_task_switch(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7470,6 +7741,15 @@ static int handle_task_switch(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 当Guest第一次访问某个页面时,由于没有GVA到GPA的映射,触发Guest OS的page fault.
+ * 于是Guest OS会建立对应的pte并修复好各级页表,最后访问对应的GPA.由于没有建立
+ * GPA到HVA的映射,于是触发EPT Violation,VMEXIT到KVM.  KVM在vmx_handle_exit中执
+ * 行kvm_vmx_exit_handlers[exit_reason],发现exit_reason是
+ * EXIT_REASON_EPT_VIOLATION,因此调用 handle_ept_violation.
+ *
+ * kvm_vmx_exit_handlers[EXIT_REASON_EPT_VIOLATION]
+ */
 static int handle_ept_violation(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
@@ -7489,9 +7769,14 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 			(exit_qualification & INTR_INFO_UNBLOCK_NMI))
 		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
 
+	/* 获取发生缺页的gpa */
 	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 	trace_kvm_page_fault(gpa, exit_qualification);
 
+	/*
+	 * 根据exit_qualification内容得到error_code, 可能是
+	 * read fault/write fault/fetch fault/ept page tableis not present
+	 */
 	/* Is it a read fault? */
 	error_code = (exit_qualification & EPT_VIOLATION_ACC_READ)
 		     ? PFERR_USER_MASK : 0;
@@ -7514,6 +7799,9 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_EPT_MISCONFIG]
+ */
 static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 {
 	gpa_t gpa;
@@ -7546,6 +7834,9 @@ static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_NMI_WINDOW]
+ */
 static int handle_nmi_window(struct kvm_vcpu *vcpu)
 {
 	WARN_ON_ONCE(!enable_vnmi);
@@ -7669,6 +7960,10 @@ static void wakeup_handler(void)
 	spin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/vmx.c|7960| <<hardware_setup>> vmx_enable_tdp();
+ */
 static void vmx_enable_tdp(void)
 {
 	kvm_mmu_set_mask_ptes(VMX_EPT_READABLE_MASK,
@@ -7682,6 +7977,12 @@ static void vmx_enable_tdp(void)
 	kvm_enable_tdp();
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hardware_setup = hardware_setup()
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|8637| <<kvm_arch_hardware_setup>> r = kvm_x86_ops->hardware_setup();
+ */
 static __init int hardware_setup(void)
 {
 	int r = -ENOMEM, i;
@@ -7814,6 +8115,9 @@ static __init int hardware_setup(void)
     return r;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hardware_unsetup = hardware_unsetup()
+ */
 static __exit void hardware_unsetup(void)
 {
 	int i;
@@ -7828,6 +8132,9 @@ static __exit void hardware_unsetup(void)
  * Indicate a busy-waiting vcpu in spinlock. We do not enable the PAUSE
  * exiting, so only get here on cpu with PAUSE-Loop-Exiting.
  */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_PAUSE_INSTRUCTION]
+ */
 static int handle_pause(struct kvm_vcpu *vcpu)
 {
 	if (!kvm_pause_in_guest(vcpu->kvm))
@@ -7848,23 +8155,36 @@ static int handle_nop(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MWAIT_INSTRUCTION]
+ */
 static int handle_mwait(struct kvm_vcpu *vcpu)
 {
 	printk_once(KERN_WARNING "kvm: MWAIT instruction emulated as NOP!\n");
 	return handle_nop(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_RDRAND]
+ * kvm_vmx_exit_handlers[EXIT_REASON_RDSEED]
+ */
 static int handle_invalid_op(struct kvm_vcpu *vcpu)
 {
 	kvm_queue_exception(vcpu, UD_VECTOR);
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MONITOR_TRAP_FLAG]
+ */
 static int handle_monitor_trap(struct kvm_vcpu *vcpu)
 {
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MONITOR_INSTRUCTION]
+ */
 static int handle_monitor(struct kvm_vcpu *vcpu)
 {
 	printk_once(KERN_WARNING "kvm: MONITOR instruction emulated as NOP!\n");
@@ -8098,6 +8418,9 @@ static int enter_vmx_operation(struct kvm_vcpu *vcpu)
  * region. Consequently, VMCLEAR and VMPTRLD also do not verify that the their
  * argument is different from the VMXON pointer (which the spec says they do).
  */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMON]
+ */
 static int handle_vmon(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -8267,6 +8590,9 @@ static void free_nested(struct vcpu_vmx *vmx)
 }
 
 /* Emulate the VMXOFF instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMOFF]
+ */
 static int handle_vmoff(struct kvm_vcpu *vcpu)
 {
 	if (!nested_vmx_check_permission(vcpu))
@@ -8277,6 +8603,9 @@ static int handle_vmoff(struct kvm_vcpu *vcpu)
 }
 
 /* Emulate the VMCLEAR instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMCLEAR]
+ */
 static int handle_vmclear(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8313,12 +8642,18 @@ static int handle_vmclear(struct kvm_vcpu *vcpu)
 static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch);
 
 /* Emulate the VMLAUNCH instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMLAUNCH]
+ */
 static int handle_vmlaunch(struct kvm_vcpu *vcpu)
 {
 	return nested_vmx_run(vcpu, true);
 }
 
 /* Emulate the VMRESUME instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMRESUME]
+ */
 static int handle_vmresume(struct kvm_vcpu *vcpu)
 {
 
@@ -8477,6 +8812,9 @@ static int nested_vmx_check_vmcs12(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMREAD]
+ */
 static int handle_vmread(struct kvm_vcpu *vcpu)
 {
 	unsigned long field;
@@ -8520,6 +8858,9 @@ static int handle_vmread(struct kvm_vcpu *vcpu)
 }
 
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMWRITE]
+ */
 static int handle_vmwrite(struct kvm_vcpu *vcpu)
 {
 	unsigned long field;
@@ -8607,6 +8948,9 @@ static void set_current_vmptr(struct vcpu_vmx *vmx, gpa_t vmptr)
 }
 
 /* Emulate the VMPTRLD instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMPTRLD]
+ */
 static int handle_vmptrld(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8662,6 +9006,9 @@ static int handle_vmptrld(struct kvm_vcpu *vcpu)
 }
 
 /* Emulate the VMPTRST instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMPTRST]
+ */
 static int handle_vmptrst(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qual = vmcs_readl(EXIT_QUALIFICATION);
@@ -8686,6 +9033,9 @@ static int handle_vmptrst(struct kvm_vcpu *vcpu)
 }
 
 /* Emulate the INVEPT instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_INVEPT]
+ */
 static int handle_invept(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8748,6 +9098,9 @@ static int handle_invept(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_INVVPID]
+ */
 static int handle_invvpid(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8835,6 +9188,9 @@ static int handle_invvpid(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_PML_FULL]
+ */
 static int handle_pml_full(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
@@ -8860,6 +9216,9 @@ static int handle_pml_full(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_PREEMPTION_TIMER]
+ */
 static int handle_preemption_timer(struct kvm_vcpu *vcpu)
 {
 	kvm_lapic_expired_hv_timer(vcpu);
@@ -8947,6 +9306,9 @@ static int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMFUNC]
+ */
 static int handle_vmfunc(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8989,6 +9351,11 @@ static int handle_vmfunc(struct kvm_vcpu *vcpu)
  * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
  * to be done to userspace and return 0.
  */
+/*
+ * used by:
+ *   - arch/x86/kvm/vmx.c|9930| <<vmx_handle_exit>> && kvm_vmx_exit_handlers[exit_reason])
+ *   - arch/x86/kvm/vmx.c|9931| <<vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_reason](vcpu);
+ */
 static int (*const kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 	[EXIT_REASON_EXCEPTION_NMI]           = handle_exception,
 	[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,
@@ -9409,6 +9776,9 @@ static int nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason)
 	return 1;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_exit_info = vmx_get_exit_info()
+ */
 static void vmx_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)
 {
 	*info1 = vmcs_readl(EXIT_QUALIFICATION);
@@ -9634,6 +10004,9 @@ static void dump_vmcs(void)
  * The guest has exited.  See if we can fix it or if we need userspace
  * assistance.
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.handle_exit = vmx_handle_exit()
+ */
 static int vmx_handle_exit(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -9803,6 +10176,9 @@ static void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 		: "eax", "ebx", "ecx", "edx");
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.update_cr8_intercept = update_cr8_intercept()
+ */
 static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -9819,6 +10195,9 @@ static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 	vmcs_write32(TPR_THRESHOLD, irr);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_virtual_apic_mode = vmx_set_virtual_apic_mode()
+ */
 static void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 {
 	u32 sec_exec_control;
@@ -9862,6 +10241,9 @@ static void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 	vmx_update_msr_bitmap(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_apic_access_page_addr = vmx_set_apic_access_page_addr()
+ */
 static void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu, hpa_t hpa)
 {
 	if (!is_guest_mode(vcpu)) {
@@ -9870,6 +10252,9 @@ static void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu, hpa_t hpa)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hwapic_isr_update = vmx_hwapic_isr_update()
+ */
 static void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 {
 	u16 status;
@@ -9904,6 +10289,9 @@ static void vmx_set_rvi(int vector)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hwapic_irr_update = vmx_hwapic_irr_update()
+ */
 static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 {
 	/*
@@ -9918,6 +10306,9 @@ static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 		vmx_set_rvi(max_irr);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+ */
 static int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -9956,6 +10347,9 @@ static int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
 	return max_irr;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.load_eoi_exitmap = vmx_load_eoi_exitmap()
+ */
 static void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
 {
 	if (!kvm_vcpu_apicv_active(vcpu))
@@ -9967,6 +10361,9 @@ static void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
 	vmcs_write64(EOI_EXIT_BITMAP3, eoi_exit_bitmap[3]);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.apicv_post_state_restore = vmx_apicv_post_state_restore()
+ */
 static void vmx_apicv_post_state_restore(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -10005,6 +10402,9 @@ static void vmx_complete_atomic_exit(struct vcpu_vmx *vmx)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.handle_external_intr = vmx_handle_external_intr()
+ */
 static void vmx_handle_external_intr(struct kvm_vcpu *vcpu)
 {
 	u32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
@@ -10046,6 +10446,9 @@ static void vmx_handle_external_intr(struct kvm_vcpu *vcpu)
 }
 STACK_FRAME_NON_STANDARD(vmx_handle_external_intr);
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.has_emulated_msr = vmx_has_emulated_msr()
+ */
 static bool vmx_has_emulated_msr(int index)
 {
 	switch (index) {
@@ -10063,12 +10466,18 @@ static bool vmx_has_emulated_msr(int index)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.mpx_supported = vmx_mpx_supported()
+ */
 static bool vmx_mpx_supported(void)
 {
 	return (vmcs_config.vmexit_ctrl & VM_EXIT_CLEAR_BNDCFGS) &&
 		(vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_BNDCFGS);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.xsaves_supported = vmx_xsaves_supported()
+ */
 static bool vmx_xsaves_supported(void)
 {
 	return vmcs_config.cpu_based_2nd_exec_ctrl &
@@ -10179,6 +10588,9 @@ static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
 				  IDT_VECTORING_ERROR_CODE);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cancel_injection = vmx_cancel_injection()
+ */
 static void vmx_cancel_injection(struct kvm_vcpu *vcpu)
 {
 	__vmx_complete_interrupts(vcpu,
@@ -10227,6 +10639,9 @@ static void vmx_arm_hv_timer(struct kvm_vcpu *vcpu)
 	vmcs_write32(VMX_PREEMPTION_TIMER_VALUE, delta_tsc);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.run = vmx_vcpu_run()
+ */
 static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -10505,12 +10920,21 @@ static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 }
 STACK_FRAME_NON_STANDARD(vmx_vcpu_run);
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vm_alloc = vmx_vm_alloc()
+ *
+ * called only by:
+ *   - arch/x86/include/asm/kvm_host.h|1121| <<kvm_arch_alloc_vm>> return kvm_x86_ops->vm_alloc();
+ */
 static struct kvm *vmx_vm_alloc(void)
 {
 	struct kvm_vmx *kvm_vmx = vzalloc(sizeof(struct kvm_vmx));
 	return &kvm_vmx->kvm;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vm_free = vmx_vm_free()
+ */
 static void vmx_vm_free(struct kvm *kvm)
 {
 	vfree(to_kvm_vmx(kvm));
@@ -10545,6 +10969,9 @@ static void vmx_free_vcpu_nested(struct kvm_vcpu *vcpu)
        vcpu_put(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_free = vmx_free_vcpu()
+ */
 static void vmx_free_vcpu(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -10560,6 +10987,9 @@ static void vmx_free_vcpu(struct kvm_vcpu *vcpu)
 	kmem_cache_free(kvm_vcpu_cache, vmx);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_create = vmx_create_vcpu()
+ */
 static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 {
 	int err;
@@ -10664,6 +11094,9 @@ static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 #define L1TF_MSG_SMT "L1TF CPU bug present and SMT on, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/l1tf.html for details.\n"
 #define L1TF_MSG_L1D "L1TF CPU bug present and virtualization mitigation disabled, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/l1tf.html for details.\n"
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vm_init = vmx_vm_init()
+ */
 static int vmx_vm_init(struct kvm *kvm)
 {
 	if (!ple_gap)
@@ -10695,6 +11128,9 @@ static int vmx_vm_init(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.check_processor_compatibility = vmx_check_processor_compat()
+ */
 static void __init vmx_check_processor_compat(void *rtn)
 {
 	struct vmcs_config vmcs_conf;
@@ -10710,6 +11146,9 @@ static void __init vmx_check_processor_compat(void *rtn)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_mt_mask = vmx_get_mt_mask()
+ */
 static u64 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 {
 	u8 cache;
@@ -10752,6 +11191,9 @@ static u64 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 	return (cache << VMX_EPT_MT_EPTE_SHIFT) | ipat;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_lpage_level = vmx_get_lpage_level()
+ */
 static int vmx_get_lpage_level(void)
 {
 	if (enable_ept && !cpu_has_vmx_ept_1g_page())
@@ -10824,6 +11266,9 @@ static void nested_vmx_cr_fixed1_bits_update(struct kvm_vcpu *vcpu)
 #undef cr4_fixed1_update
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cpuid_update = vmx_cpuid_update()
+ */
 static void vmx_cpuid_update(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -10844,6 +11289,9 @@ static void vmx_cpuid_update(struct kvm_vcpu *vcpu)
 		nested_vmx_cr_fixed1_bits_update(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_supported_cpuid = vmx_set_supported_cpuid()
+ */
 static void vmx_set_supported_cpuid(u32 func, struct kvm_cpuid_entry2 *entry)
 {
 	if (func == 1 && nested)
@@ -12281,6 +12729,9 @@ static void vmcs12_save_pending_event(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.check_nested_events = vmx_check_nested_events()
+ */
 static int vmx_check_nested_events(struct kvm_vcpu *vcpu, bool external_intr)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -12841,6 +13292,9 @@ static void nested_vmx_entry_failure(struct kvm_vcpu *vcpu,
 		to_vmx(vcpu)->nested.sync_shadow_vmcs = true;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.check_intercept = vmx_check_intercept()
+ */
 static int vmx_check_intercept(struct kvm_vcpu *vcpu,
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage)
@@ -12882,6 +13336,9 @@ static inline int u64_shl_div_u64(u64 a, unsigned int shift,
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_hv_timer = vmx_set_hv_timer()
+ */
 static int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc)
 {
 	struct vcpu_vmx *vmx;
@@ -12925,6 +13382,9 @@ static int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc)
 	return delta_tsc == 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cancel_hv_timer = vmx_cancel_hv_timer()
+ */
 static void vmx_cancel_hv_timer(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -12934,12 +13394,18 @@ static void vmx_cancel_hv_timer(struct kvm_vcpu *vcpu)
 }
 #endif
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.sched_in = vmx_sched_in()
+ */
 static void vmx_sched_in(struct kvm_vcpu *vcpu, int cpu)
 {
 	if (!kvm_pause_in_guest(vcpu->kvm))
 		shrink_ple_window(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.slot_enable_log_dirty = vmx_slot_enable_log_dirty()
+ */
 static void vmx_slot_enable_log_dirty(struct kvm *kvm,
 				     struct kvm_memory_slot *slot)
 {
@@ -12947,17 +13413,26 @@ static void vmx_slot_enable_log_dirty(struct kvm *kvm,
 	kvm_mmu_slot_largepage_remove_write_access(kvm, slot);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.slot_disable_log_dirty = vmx_slot_disable_log_dirty()
+ */
 static void vmx_slot_disable_log_dirty(struct kvm *kvm,
 				       struct kvm_memory_slot *slot)
 {
 	kvm_mmu_slot_set_dirty(kvm, slot);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.flush_log_dirty = vmx_flush_log_dirty()
+ */
 static void vmx_flush_log_dirty(struct kvm *kvm)
 {
 	kvm_flush_pml_buffers(kvm);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.write_log_dirty = vmx_write_pml_buffer()
+ */
 static int vmx_write_pml_buffer(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12;
@@ -12998,6 +13473,9 @@ static int vmx_write_pml_buffer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.enable_log_dirty_pt_masked = vmx_enable_log_dirty_pt_masked()
+ */
 static void vmx_enable_log_dirty_pt_masked(struct kvm *kvm,
 					   struct kvm_memory_slot *memslot,
 					   gfn_t offset, unsigned long mask)
@@ -13106,6 +13584,9 @@ static int pi_pre_block(struct kvm_vcpu *vcpu)
 	return (vcpu->pre_pcpu == -1);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.pre_block = vmx_pre_block()
+ */
 static int vmx_pre_block(struct kvm_vcpu *vcpu)
 {
 	if (pi_pre_block(vcpu))
@@ -13117,6 +13598,9 @@ static int vmx_pre_block(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.
+ */
 static void pi_post_block(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->pre_pcpu == -1)
@@ -13128,6 +13612,9 @@ static void pi_post_block(struct kvm_vcpu *vcpu)
 	local_irq_enable();
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.post_block = vmx_post_block()
+ */
 static void vmx_post_block(struct kvm_vcpu *vcpu)
 {
 	if (kvm_x86_ops->set_hv_timer)
@@ -13145,6 +13632,9 @@ static void vmx_post_block(struct kvm_vcpu *vcpu)
  * @set: set or unset PI
  * returns 0 on success, < 0 on failure
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.update_pi_irte = vmx_update_pi_irte()
+ */
 static int vmx_update_pi_irte(struct kvm *kvm, unsigned int host_irq,
 			      uint32_t guest_irq, bool set)
 {
@@ -13226,6 +13716,9 @@ static int vmx_update_pi_irte(struct kvm *kvm, unsigned int host_irq,
 	return ret;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.setup_mce = vmx_setup_mce()
+ */
 static void vmx_setup_mce(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->arch.mcg_cap & MCG_LMCE_P)
@@ -13236,6 +13729,9 @@ static void vmx_setup_mce(struct kvm_vcpu *vcpu)
 			~FEATURE_CONTROL_LMCE;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.smi_allowed = vmx_smi_allowed()
+ */
 static int vmx_smi_allowed(struct kvm_vcpu *vcpu)
 {
 	/* we need a nested vmexit to enter SMM, postpone if run is pending */
@@ -13244,6 +13740,9 @@ static int vmx_smi_allowed(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.pre_enter_smm = vmx_pre_enter_smm()
+ */
 static int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -13258,6 +13757,9 @@ static int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.pre_leave_smm = vmx_pre_leave_smm()
+ */
 static int vmx_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -13280,11 +13782,18 @@ static int vmx_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.enable_smi_window = enable_smi_window()
+ */
 static int enable_smi_window(struct kvm_vcpu *vcpu)
 {
 	return 0;
 }
 
+/*
+ * used by:
+ *   - arch/x86/kvm/vmx.c|13506| <<vmx_init>> r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
+ */
 static struct kvm_x86_ops vmx_x86_ops __ro_after_init = {
 	.cpu_has_kvm_support = cpu_has_kvm_support,
 	.disabled_by_bios = vmx_disabled_by_bios,
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 97fcac3..dfdda40 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -2026,6 +2026,10 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v)
 				sizeof(vcpu->hv_clock.version));
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/x86.c|7325| <<vcpu_enter_guest>> r = kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -4759,9 +4763,15 @@ gpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva,
 }
 
 /* uses this to access any guest's mapped memory without checking CPL */
+/*
+ * called only by kvm_arch_vcpu_ioctl_translate()
+ */
 gpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,
 				struct x86_exception *exception)
 {
+	/*
+	 * 情况太多, 一种情况是paging64_gva_to_gpa()
+	 */
 	return vcpu->arch.walk_mmu->gva_to_gpa(vcpu, gva, 0, exception);
 }
 
@@ -6738,6 +6748,11 @@ void kvm_vcpu_deactivate_apicv(struct kvm_vcpu *vcpu)
 	kvm_x86_ops->refresh_apicv_exec_ctrl(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|7501| <<handle_vmcall>> return kvm_emulate_hypercall(vcpu);
+ *   - arch/x86/kvm/svm.c|2856| <<vmmcall_interception>> return kvm_emulate_hypercall(&svm->vcpu);
+ */
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
 {
 	unsigned long nr, a0, a1, a2, a3, ret;
@@ -7288,6 +7303,10 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_reload_apic_access_page);
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7629| <<vcpu_run>> r = vcpu_enter_guest(vcpu);
+ */
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -7501,6 +7520,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_RELOAD;
 	}
 
+	/*
+	 * intel : vmx_x86_ops --> vmx_vcpu_run()
+	 * amd   : svm_x86_ops --> svm_vcpu_run()
+	 */
 	kvm_x86_ops->run(vcpu);
 
 	/*
@@ -7563,6 +7586,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		kvm_lapic_sync_from_vapic(vcpu);
 
 	vcpu->arch.gpa_available = false;
+	/*
+	 * intel : vmx_x86_ops --> vmx_handle_exit()
+	 * amd   : svm_x86_ops --> handle_exit()
+	 */
 	r = kvm_x86_ops->handle_exit(vcpu);
 	return r;
 
@@ -7574,6 +7601,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7631| <<vcpu_run>> r = vcpu_block(kvm, vcpu);
+ */
 static inline int vcpu_block(struct kvm *kvm, struct kvm_vcpu *vcpu)
 {
 	if (!kvm_arch_vcpu_runnable(vcpu) &&
@@ -7616,6 +7647,10 @@ static inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
 		!vcpu->arch.apf.halted);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7806| <<kvm_arch_vcpu_ioctl_run>> r = vcpu_run(vcpu);
+ */
 static int vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -7747,6 +7782,10 @@ static int complete_emulated_mmio(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2600| <<kvm_vcpu_ioctl>> r = kvm_arch_vcpu_ioctl_run(vcpu, vcpu->run);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 {
 	int r;
@@ -8208,6 +8247,10 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 /*
  * Translate a guest virtual address to a guest physical address.
  */
+/*
+ * called by KVM_TRANSLATE (vcpu dev fd):
+ *   - virt/kvm/kvm_main.c|2711| <<kvm_vcpu_ioctl>> r = kvm_arch_vcpu_ioctl_translate(vcpu, &tr);
+ */
 int kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,
 				    struct kvm_translation *tr)
 {
@@ -8357,6 +8400,10 @@ void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 	free_cpumask_var(wbinvd_dirty_mask);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2481| <<kvm_vm_ioctl_create_vcpu>> vcpu = kvm_arch_vcpu_create(kvm, id);
+ */
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 						unsigned int id)
 {
@@ -8367,16 +8414,24 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 		"kvm: SMP vm created on host with unstable TSC; "
 		"guest TSC will not be reliable\n");
 
+	/*
+	 * intel: vmx_x86_ops --> vmx_create_vcpu()
+	 */
 	vcpu = kvm_x86_ops->vcpu_create(kvm, id);
 
 	return vcpu;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2518| <<kvm_vm_ioctl_create_vcpu>> r = kvm_arch_vcpu_setup(vcpu);
+ */
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	kvm_vcpu_mtrr_init(vcpu);
 	vcpu_load(vcpu);
 	kvm_vcpu_reset(vcpu, false);
+	/* 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu) */
 	kvm_mmu_setup(vcpu);
 	vcpu_put(vcpu);
 	return 0;
@@ -8599,10 +8654,18 @@ void kvm_arch_hardware_disable(void)
 	drop_user_return_notifiers();
 }
 
+/*
+ * called only by:
+ *   - virt/kvm/kvm_main.c|4084| <<kvm_init>> r = kvm_arch_hardware_setup();  <---一个调用者是vmx_init()
+ */
 int kvm_arch_hardware_setup(void)
 {
 	int r;
 
+	/*
+	 * intel : hardware_setup() in arch/x86/kvm/vmx.c
+	 * amd   : svm_hardware_setup()
+	 */
 	r = kvm_x86_ops->hardware_setup();
 	if (r != 0)
 		return r;
@@ -8746,6 +8809,10 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 	kvm_x86_ops->sched_in(vcpu, cpu);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|638| <<kvm_create_vm>> r = kvm_arch_init_vm(kvm, type);
+ */
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
 	if (type)
@@ -8777,6 +8844,10 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm_page_track_init(kvm);
 	kvm_mmu_init_vm(kvm);
 
+	/*
+	 * intel: vmx_vm_init()
+	 * amd  : avic_vm_init()
+	 */
 	if (kvm_x86_ops->vm_init)
 		return kvm_x86_ops->vm_init(kvm);
 
@@ -8820,6 +8891,12 @@ void kvm_arch_sync_events(struct kvm *kvm)
 	kvm_free_pit(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|5796| <<init_rmode_identity_map>> r = __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/vmx.c|5847| <<alloc_apic_access_page>> r = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|8919| <<x86_set_memory_region>> r = __x86_set_memory_region(kvm, id, gpa, size);
+ */
 int __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size)
 {
 	int i, r;
@@ -8872,6 +8949,15 @@ int __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size)
 }
 EXPORT_SYMBOL_GPL(__x86_set_memory_region);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm.c|1667| <<avic_init_access_page>> ret = x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx.c|6937| <<vmx_set_tss_addr>> ret = x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,
+ *   - arch/x86/kvm/x86.c|8914| <<x86_set_memory_region>> int x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size)
+ *   - arch/x86/kvm/x86.c|8934| <<kvm_arch_destroy_vm>> x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+ *   - arch/x86/kvm/x86.c|8935| <<kvm_arch_destroy_vm>> x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT, 0, 0);
+ *   - arch/x86/kvm/x86.c|8936| <<kvm_arch_destroy_vm>> x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);
+ */
 int x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size)
 {
 	int r;
@@ -8930,6 +9016,16 @@ void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 	kvm_page_track_free_memslot(free, dont);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1070| <<__kvm_set_memory_region>> if (kvm_arch_create_memslot(kvm, &new, npages))
+ *
+ * 核心思想是为参数的kvm_memory_slot初始化其arch
+ * 分配管理hugepage内存的元数据:
+ *     kvm_memory_slot->arch.rmap[i]
+ *     kvm_memory_slot->arch.lpage_info[i]
+ * 根据实际情况设置.disallow_lpage
+ */
 int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 			    unsigned long npages)
 {
@@ -8939,11 +9035,28 @@ int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 		struct kvm_lpage_info *linfo;
 		unsigned long ugfn;
 		int lpages;
+		/*
+		 * 如果全部执行, KVM_NR_PAGE_SIZES = 3
+		 * level 分别是 1, 2, 3
+		 */
 		int level = i + 1;
 
+		/*
+		 * base_gfn是基于4k开始的gfn
+		 * gfn是基于4k结束的gfn
+		 * 计算从开始到结束需要用到几个hugepage
+		 *   level是1的时候hugepage大小是4K
+		 *   level是2的时候hugepage大小是2M
+		 *   level是3的时候hugepage大小是1G
+		 */
 		lpages = gfn_to_index(slot->base_gfn + npages - 1,
 				      slot->base_gfn, level) + 1;
 
+		/*
+		 * arch是struct kvm_arch_memory_slot
+		 *
+		 * 为某个level分配需要的hugepage数目的kvm_rmap_head
+		 */
 		slot->arch.rmap[i] =
 			kvcalloc(lpages, sizeof(*slot->arch.rmap[i]),
 				 GFP_KERNEL);
@@ -8952,6 +9065,11 @@ int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 		if (i == 0)
 			continue;
 
+		/*
+		 * 为当前level分配需要的hugepage数目的kvm_lpage_info
+		 *
+		 * 下面赋值给slot->arch.lpage_info[i - 1]
+		 */
 		linfo = kvcalloc(lpages, sizeof(*linfo), GFP_KERNEL);
 		if (!linfo)
 			goto out_free;
@@ -9062,6 +9180,10 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 	}
 }
 
+/*
+ * called only by:
+ *   - virt/kvm/kvm_main.c|1128| <<__kvm_set_memory_region>> kvm_arch_commit_memory_region(kvm, mem, &old, &new, change);
+ */
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				const struct kvm_userspace_memory_region *mem,
 				const struct kvm_memory_slot *old,
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4ee7bc5..e86c80c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -569,8 +569,15 @@ void kvm_exit(void);
 void kvm_get_kvm(struct kvm *kvm);
 void kvm_put_kvm(struct kvm *kvm);
 
+/*
+ * 利用srcu返回kvm->memslots[as_id]
+ * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+ */
 static inline struct kvm_memslots *__kvm_memslots(struct kvm *kvm, int as_id)
 {
+	/*
+	 * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+	 */
 	return srcu_dereference_check(kvm->memslots[as_id], &kvm->srcu,
 			lockdep_is_held(&kvm->slots_lock) ||
 			!refcount_read(&kvm->users_count));
@@ -588,6 +595,14 @@ static inline struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu)
 	return __kvm_memslots(vcpu->kvm, as_id);
 }
 
+/*
+ * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+ * struct kvm_memslots有以下两个
+ *     struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+ *     short id_to_index[KVM_MEM_SLOTS_NUM];
+ * 这个函数先把id通过id_to_index[]转换成index
+ * 然后用来索引并返回slots->memslots[index]
+ */
 static inline struct kvm_memory_slot *
 id_to_memslot(struct kvm_memslots *slots, int id)
 {
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index b20b751..c0c44ae 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -565,6 +565,10 @@ kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 	return 0;
 }
 
+/*
+ * called by KVM_IRQFD (每个vm的dev的ioctl):
+ *   - virt/kvm/kvm_main.c|3073| <<kvm_vm_ioctl>> r = kvm_irqfd(kvm, &data);
+ */
 int
 kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
 {
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index b1286c4..e6d947f 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -58,6 +58,10 @@ int kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin)
 	return irq_rt->chip[irqchip][pin];
 }
 
+/*
+ * called only by:
+ *   - virt/kvm/kvm_main.c|3130| <<kvm_vm_ioctl>> r = kvm_send_userspace_msi(kvm, &msi);
+ */
 int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct kvm_kernel_irq_routing_entry route;
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 8b47507..7ca3efe 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -94,7 +94,7 @@ EXPORT_SYMBOL_GPL(halt_poll_ns_shrink);
 
 DEFINE_SPINLOCK(kvm_lock);
 static DEFINE_RAW_SPINLOCK(kvm_count_lock);
-LIST_HEAD(vm_list);
+LIST_HEAD(vm_list); // 挂载所有的struct kvm
 
 static cpumask_var_t cpus_hardware_enabled;
 static int kvm_usage_count;
@@ -518,6 +518,10 @@ static int kvm_init_mmu_notifier(struct kvm *kvm)
 
 #endif /* CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER */
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|659| <<kvm_create_vm>> struct kvm_memslots *slots = kvm_alloc_memslots();
+ */
 static struct kvm_memslots *kvm_alloc_memslots(void)
 {
 	int i;
@@ -617,9 +621,14 @@ static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   -  virt/kvm/kvm_main.c|3252| <<kvm_dev_ioctl_create_vm>> kvm = kvm_create_vm(type);
+ */
 static struct kvm *kvm_create_vm(unsigned long type)
 {
 	int r, i;
+	/* 分配的kvm包含在kvm_vmx中 */
 	struct kvm *kvm = kvm_arch_alloc_vm();
 
 	if (!kvm)
@@ -769,6 +778,9 @@ void kvm_put_kvm(struct kvm *kvm)
 EXPORT_SYMBOL_GPL(kvm_put_kvm);
 
 
+/*
+ * struct file_operations kvm_vm_fops.release = kvm_vm_release()
+ */
 static int kvm_vm_release(struct inode *inode, struct file *filp)
 {
 	struct kvm *kvm = filp->private_data;
@@ -904,6 +916,11 @@ static struct kvm_memslots *install_new_memslots(struct kvm *kvm,
  *
  * Must be called holding kvm->slots_lock for write.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8913| <<__x86_set_memory_region>> r = __kvm_set_memory_region(kvm, &m);
+ *   - virt/kvm/kvm_main.c|1080| <<kvm_set_memory_region>> r = __kvm_set_memory_region(kvm, mem);
+ */
 int __kvm_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region *mem)
 {
@@ -941,6 +958,30 @@ int __kvm_set_memory_region(struct kvm *kvm,
 	if (mem->guest_phys_addr + mem->memory_size < mem->guest_phys_addr)
 		goto out;
 
+	/*
+	 * 获得一个新的slot (struct kvm_memory_slot)
+	 *
+	 * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM]; // KVM_ADDRESS_SPACE_NUM最多可能就是2
+	 * struct kvm_memslots中有:
+	 *     struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+	 *     short id_to_index[KVM_MEM_SLOTS_NUM];
+	 *
+	 * __kvm_memslots():
+	 *   利用srcu返回kvm->memslots[as_id]
+	 *   struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+	 *
+	 * id来自qemu传进来的kvm_userspace_memory_region->slot
+	 *
+	 * id_to_memslot():
+	 *   struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+	 *   struct kvm_memslots有以下两个
+	 *       struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+	 *       short id_to_index[KVM_MEM_SLOTS_NUM];
+	 *   这个函数先把id通过id_to_index[]转换成index
+	 *   然后用来索引并返回slots->memslots[index]
+	 *
+	 * slot是struct kvm_memory_slot
+	 */
 	slot = id_to_memslot(__kvm_memslots(kvm, as_id), id);
 	base_gfn = mem->guest_phys_addr >> PAGE_SHIFT;
 	npages = mem->memory_size >> PAGE_SHIFT;
@@ -948,6 +989,9 @@ int __kvm_set_memory_region(struct kvm *kvm,
 	if (npages > KVM_MEM_MAX_NR_PAGES)
 		goto out;
 
+	/*
+	 * new和old都是struct kvm_memory_slot
+	 */
 	new = old = *slot;
 
 	new.id = id;
@@ -955,6 +999,14 @@ int __kvm_set_memory_region(struct kvm *kvm,
 	new.npages = npages;
 	new.flags = mem->flags;
 
+	/*
+	 * 就四种操作
+	 * KVM_MR_CREATE : create a new memory slot
+	 * KVM_MR_DELETE : delete an existing memory slot
+	 * KVM_MR_MOVE   : modify an existing memory slot
+	 * KVM_MR_FLAGS_ONLY
+	 */
+
 	if (npages) {
 		if (!old.npages)
 			change = KVM_MR_CREATE;
@@ -982,9 +1034,22 @@ int __kvm_set_memory_region(struct kvm *kvm,
 		new.flags = 0;
 	}
 
+	/*
+	 * 新分配的话change就是KVM_MR_CREATE
+	 */
+
+	/*
+	 * 这里只是做一个检查
+	 * 就像是注释说的: Check for overlaps
+	 */
 	if ((change == KVM_MR_CREATE) || (change == KVM_MR_MOVE)) {
 		/* Check for overlaps */
 		r = -EEXIST;
+		/*
+		 * __kvm_memslots():
+		 *   利用srcu返回kvm->memslots[as_id]
+		 *   struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+		 */
 		kvm_for_each_memslot(slot, __kvm_memslots(kvm, as_id)) {
 			if (slot->id == id)
 				continue;
@@ -1002,6 +1067,13 @@ int __kvm_set_memory_region(struct kvm *kvm,
 	if (change == KVM_MR_CREATE) {
 		new.userspace_addr = mem->userspace_addr;
 
+		/*
+		 * 核心思想是为参数的kvm_memory_slot初始化其arch 
+		 * 分配管理hugepage内存的元数据:
+		 *     kvm_memory_slot->arch.rmap[i]
+		 *     kvm_memory_slot->arch.lpage_info[i]
+		 * 根据实际情况设置.disallow_lpage
+		 */
 		if (kvm_arch_create_memslot(kvm, &new, npages))
 			goto out_free;
 	}
@@ -1068,6 +1140,10 @@ int __kvm_set_memory_region(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(__kvm_set_memory_region);
 
+/*
+ * called by kvm_vm_ioctl() --> KVM_SET_USER_MEMORY_REGION --> kvm_vm_ioctl_set_memory_region():
+ *   - virt/kvm/kvm_main.c|1092| <<kvm_vm_ioctl_set_memory_region>> return kvm_set_memory_region(kvm, mem);
+ */
 int kvm_set_memory_region(struct kvm *kvm,
 			  const struct kvm_userspace_memory_region *mem)
 {
@@ -1080,6 +1156,10 @@ int kvm_set_memory_region(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(kvm_set_memory_region);
 
+/*
+ * kvm_vm_ioctl() --> KVM_SET_USER_MEMORY_REGION:
+ *   - virt/kvm/kvm_main.c|3006| <<kvm_vm_ioctl>> r = kvm_vm_ioctl_set_memory_region(kvm, &kvm_userspace_mem);
+ */
 static int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
 					  struct kvm_userspace_memory_region *mem)
 {
@@ -1539,6 +1619,9 @@ kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
 			       bool atomic, bool *async, bool write_fault,
 			       bool *writable)
 {
+	/*
+	 * 计算gfn对应的其实hva
+	 */
 	unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);
 
 	if (addr == KVM_HVA_ERR_RO_BAD) {
@@ -1559,6 +1642,9 @@ kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
 		writable = NULL;
 	}
 
+	/*
+	 * 计算hva对应的pfn, 同时确保该物理页在内存中
+	 */
 	return hva_to_pfn(addr, atomic, async, write_fault,
 			  writable);
 }
@@ -2383,12 +2469,18 @@ static const struct vm_operations_struct kvm_vcpu_vm_ops = {
 	.fault = kvm_vcpu_fault,
 };
 
+/*
+ * struct file_operations kvm_vcpu_fops.mmap = kvm_vcpu_mmap()
+ */
 static int kvm_vcpu_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	vma->vm_ops = &kvm_vcpu_vm_ops;
 	return 0;
 }
 
+/*
+ * struct file_operations kvm_vcpu_fops.release = kvm_vcpu_release()
+ */
 static int kvm_vcpu_release(struct inode *inode, struct file *filp)
 {
 	struct kvm_vcpu *vcpu = filp->private_data;
@@ -2398,6 +2490,12 @@ static int kvm_vcpu_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - virt/kvm/kvm_main.c|2420| <<create_vcpu_fd>> return anon_inode_getfd(name, &kvm_vcpu_fops, vcpu, O_RDWR | O_CLOEXEC);
+ *
+ * vcpu的dev fd
+ */
 static struct file_operations kvm_vcpu_fops = {
 	.release        = kvm_vcpu_release,
 	.unlocked_ioctl = kvm_vcpu_ioctl,
@@ -2446,9 +2544,14 @@ static int kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
 /*
  * Creates some virtual cpus.  Good luck creating more than one.
  */
+/*
+ * called by KVM_CREATE_VCPU (每个vm的dev的ioctl):
+ *   - virt/kvm/kvm_main.c|3025| <<kvm_vm_ioctl>> r = kvm_vm_ioctl_create_vcpu(kvm, arg);
+ */
 static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)
 {
 	int r;
+	/* struct kvm有struct kvm_vcpu *vcpus[KVM_MAX_VCPUS]; */
 	struct kvm_vcpu *vcpu;
 
 	if (id >= KVM_MAX_VCPU_ID)
@@ -2531,6 +2634,11 @@ static int kvm_vcpu_ioctl_set_sigmask(struct kvm_vcpu *vcpu, sigset_t *sigset)
 	return 0;
 }
 
+/*
+ * struct file_operations kvm_vcpu_fops.unlocked_ioctl = kvm_vcpu_ioctl()
+ *
+ * vcpu的dev fd的ioctl
+ */
 static long kvm_vcpu_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -2550,6 +2658,7 @@ static long kvm_vcpu_ioctl(struct file *filp,
 	 * Some architectures have vcpu ioctls that are asynchronous to vcpu
 	 * execution; mutex_lock() would break them.
 	 */
+	/* x86可以忽略kvm_arch_vcpu_async_ioctl() */
 	r = kvm_arch_vcpu_async_ioctl(filp, ioctl, arg);
 	if (r != -ENOIOCTLCMD)
 		return r;
@@ -2739,6 +2848,9 @@ static long kvm_vcpu_ioctl(struct file *filp,
 }
 
 #ifdef CONFIG_KVM_COMPAT
+/*
+ * struct file_operations kvm_vcpu_fops.compat_ioctl = kvm_vcpu_compat_ioctl()
+ */
 static long kvm_vcpu_compat_ioctl(struct file *filp,
 				  unsigned int ioctl, unsigned long arg)
 {
@@ -2796,6 +2908,9 @@ static int kvm_device_ioctl_attr(struct kvm_device *dev,
 	return accessor(dev, &attr);
 }
 
+/*
+ * struct file_operations kvm_device_fops.unlocked_ioctl = kvm_device_ioctl()
+ */
 static long kvm_device_ioctl(struct file *filp, unsigned int ioctl,
 			     unsigned long arg)
 {
@@ -2816,6 +2931,9 @@ static long kvm_device_ioctl(struct file *filp, unsigned int ioctl,
 	}
 }
 
+/*
+ * struct file_operations kvm_device_fops.unlocked_ioctl = kvm_device_release()
+ */
 static int kvm_device_release(struct inode *inode, struct file *filp)
 {
 	struct kvm_device *dev = filp->private_data;
@@ -2825,6 +2943,10 @@ static int kvm_device_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - virt/kvm/kvm_main.c|2963| <<kvm_ioctl_create_device>> ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
+ */
 static const struct file_operations kvm_device_fops = {
 	.unlocked_ioctl = kvm_device_ioctl,
 	.release = kvm_device_release,
@@ -2839,6 +2961,10 @@ struct kvm_device *kvm_device_from_filp(struct file *filp)
 	return filp->private_data;
 }
 
+/*
+ * 主要在如下使用:
+ *   - virt/kvm/kvm_main.c|2965| <<kvm_ioctl_create_device>> ops = kvm_device_ops_table[cd->type];
+ */
 static struct kvm_device_ops *kvm_device_ops_table[KVM_DEV_TYPE_MAX] = {
 #ifdef CONFIG_KVM_MPIC
 	[KVM_DEV_TYPE_FSL_MPIC_20]	= &kvm_mpic_ops,
@@ -2846,6 +2972,16 @@ static struct kvm_device_ops *kvm_device_ops_table[KVM_DEV_TYPE_MAX] = {
 #endif
 };
 
+/*
+ * called by:
+ *   - virt/kvm/vfio.c|441| <<kvm_vfio_ops_init>> return kvm_register_device_ops(&kvm_vfio_ops, KVM_DEV_TYPE_VFIO);  -----> 唯一x86会用的
+ *   - arch/powerpc/kvm/book3s.c|1015| <<kvmppc_book3s_init>> kvm_register_device_ops(&kvm_xive_ops, KVM_DEV_TYPE_XICS);
+ *   - arch/powerpc/kvm/book3s.c|1018| <<kvmppc_book3s_init>> kvm_register_device_ops(&kvm_xics_ops, KVM_DEV_TYPE_XICS);
+ *   - arch/s390/kvm/kvm-s390.c|427| <<kvm_arch_init>> return kvm_register_device_ops(&kvm_flic_ops, KVM_DEV_TYPE_FLIC);
+ *   - virt/kvm/arm/vgic/vgic-its.c|2576| <<kvm_vgic_register_its_device>> return kvm_register_device_ops(&kvm_arm_vgic_its_ops,
+ *   - virt/kvm/arm/vgic/vgic-kvm-device.c|273| <<kvm_register_vgic_device>> ret = kvm_register_device_ops(&kvm_arm_vgic_v2_ops,
+ *   - virt/kvm/arm/vgic/vgic-kvm-device.c|277| <<kvm_register_vgic_device>> ret = kvm_register_device_ops(&kvm_arm_vgic_v3_ops,
+ */
 int kvm_register_device_ops(struct kvm_device_ops *ops, u32 type)
 {
 	if (type >= ARRAY_SIZE(kvm_device_ops_table))
@@ -2858,12 +2994,21 @@ int kvm_register_device_ops(struct kvm_device_ops *ops, u32 type)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/vfio.c|446| <<kvm_vfio_ops_exit>> kvm_unregister_device_ops(KVM_DEV_TYPE_VFIO);
+ *   - virt/kvm/arm/vgic/vgic-v3.c|633| <<vgic_v3_probe>> kvm_unregister_device_ops(KVM_DEV_TYPE_ARM_VGIC_V2);
+ */
 void kvm_unregister_device_ops(u32 type)
 {
 	if (kvm_device_ops_table[type] != NULL)
 		kvm_device_ops_table[type] = NULL;
 }
 
+/*
+ * called by (KVM_CREATE_DEVICE):
+ *   - virt/kvm/kvm_main.c|3177| <<kvm_vm_ioctl>> r = kvm_ioctl_create_device(kvm, &cd);
+ */
 static int kvm_ioctl_create_device(struct kvm *kvm,
 				   struct kvm_create_device *cd)
 {
@@ -2953,6 +3098,11 @@ static long kvm_vm_ioctl_check_extension_generic(struct kvm *kvm, long arg)
 	return kvm_vm_ioctl_check_extension(kvm, arg);
 }
 
+/*
+ * struct file_operations kvm_vm_fops.unlocked_ioctl = kvm_vm_ioctl()
+ *
+ * 每个vm的dev的ioctl
+ */
 static long kvm_vm_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -3132,6 +3282,9 @@ struct compat_kvm_dirty_log {
 	};
 };
 
+/*
+ * struct file_operations kvm_vm_fops.compat_ioctl = kvm_vm_compat_ioctl()
+ */
 static long kvm_vm_compat_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -3163,6 +3316,20 @@ static long kvm_vm_compat_ioctl(struct file *filp,
 }
 #endif
 
+/*
+ * used by:
+ *   - virt/kvm/kvm_main.c|3227| <<kvm_dev_ioctl_create_vm>> file = anon_inode_getfile("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);
+ *
+ * 每个vm的dev
+ *
+ * 两个cpu的例子
+ * #sudo lsof -p 11857 | grep kvm
+ * qemu-syst 11857 root  mem       REG   0,11                 9671 anon_inode:kvm-vcpu:1 (stat: No such file or directory)
+ * qemu-syst 11857 root   12u      CHR 10,232         0t0    12510 /dev/kvm
+ * qemu-syst 11857 root   13u  a_inode   0,11           0     9671 kvm-vm
+ * qemu-syst 11857 root   16u  a_inode   0,11           0     9671 kvm-vcpu:0
+ * qemu-syst 11857 root   17u  a_inode   0,11           0     9671 kvm-vcpu:1
+ */
 static struct file_operations kvm_vm_fops = {
 	.release        = kvm_vm_release,
 	.unlocked_ioctl = kvm_vm_ioctl,
@@ -3170,6 +3337,9 @@ static struct file_operations kvm_vm_fops = {
 	KVM_COMPAT(kvm_vm_compat_ioctl),
 };
 
+/*
+ * called only by kvm_dev_ioctl() <-- /dev/kvm的接口
+ */
 static int kvm_dev_ioctl_create_vm(unsigned long type)
 {
 	int r;
@@ -3184,10 +3354,20 @@ static int kvm_dev_ioctl_create_vm(unsigned long type)
 	if (r < 0)
 		goto put_kvm;
 #endif
+	/*
+	 * 其实就是__alloc_fd(): allocate a file descriptor, mark it busy.
+	 */
 	r = get_unused_fd_flags(O_CLOEXEC);
 	if (r < 0)
 		goto put_kvm;
 
+	/*
+	 * creates a new file instance by hooking it up to an
+	 * anonymous inode, and a dentry that describe the "class"
+	 * of the file
+	 *
+	 * 这里把struct kvm列为private data
+	 */
 	file = anon_inode_getfile("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);
 	if (IS_ERR(file)) {
 		put_unused_fd(r);
@@ -3216,6 +3396,11 @@ static int kvm_dev_ioctl_create_vm(unsigned long type)
 	return r;
 }
 
+/*
+ * struct file_operations kvm_chardev_ops..unlocked_ioctl = kvm_dev_ioctl()
+ *
+ * /dev/kvm的接口
+ */
 static long kvm_dev_ioctl(struct file *filp,
 			  unsigned int ioctl, unsigned long arg)
 {
@@ -3256,12 +3441,24 @@ static long kvm_dev_ioctl(struct file *filp,
 	return r;
 }
 
+/*
+ * struct miscdevice kvm_dev.fops = kvm_chardev_ops
+ *
+ * /dev/kvm的接口
+ */
 static struct file_operations kvm_chardev_ops = {
 	.unlocked_ioctl = kvm_dev_ioctl,
 	.llseek		= noop_llseek,
 	KVM_COMPAT(kvm_dev_ioctl),
 };
 
+/*
+ * x86使用:
+ *   - virt/kvm/kvm_main.c|4044| <<kvm_init>> r = misc_register(&kvm_dev);
+ *   - virt/kvm/kvm_main.c|4086| <<kvm_exit>> misc_deregister(&kvm_dev);
+ *   - virt/kvm/kvm_main.c|3871| <<kvm_uevent_notify_change>> if (!kvm_dev.this_device || !kvm)
+ *   - virt/kvm/kvm_main.c|3912| <<kvm_uevent_notify_change>> kobject_uevent_env(&kvm_dev.this_device->kobj, KOBJ_CHANGE, env->envp);
+ */
 static struct miscdevice kvm_dev = {
 	KVM_MINOR,
 	"kvm",
@@ -3975,6 +4172,12 @@ static void kvm_sched_out(struct preempt_notifier *pn,
 	kvm_arch_vcpu_put(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|13506| <<vmx_init>> r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
+ *   - arch/x86/kvm/svm.c|7168| <<svm_init>> return kvm_init(&svm_x86_ops, sizeof(struct vcpu_svm),
+ *   - virt/kvm/arm/arm.c|1645| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
diff --git a/virt/kvm/vfio.c b/virt/kvm/vfio.c
index d99850c..ace37cf 100644
--- a/virt/kvm/vfio.c
+++ b/virt/kvm/vfio.c
@@ -184,6 +184,9 @@ static void kvm_vfio_update_coherency(struct kvm_device *dev)
 	mutex_unlock(&kv->lock);
 }
 
+/*
+ * called only by kvm_vfio_set_attr()
+ */
 static int kvm_vfio_set_group(struct kvm_device *dev, long attr, u64 arg)
 {
 	struct kvm_vfio *kv = dev->private;
@@ -332,6 +335,9 @@ static int kvm_vfio_set_group(struct kvm_device *dev, long attr, u64 arg)
 	return -ENXIO;
 }
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.set_attr = kvm_vfio_set_attr()
+ */
 static int kvm_vfio_set_attr(struct kvm_device *dev,
 			     struct kvm_device_attr *attr)
 {
@@ -343,6 +349,9 @@ static int kvm_vfio_set_attr(struct kvm_device *dev,
 	return -ENXIO;
 }
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.has_attr = kvm_vfio_has_attr()
+ */
 static int kvm_vfio_has_attr(struct kvm_device *dev,
 			     struct kvm_device_attr *attr)
 {
@@ -363,6 +372,9 @@ static int kvm_vfio_has_attr(struct kvm_device *dev,
 	return -ENXIO;
 }
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.destroy = kvm_vfio_destroy()
+ */
 static void kvm_vfio_destroy(struct kvm_device *dev)
 {
 	struct kvm_vfio *kv = dev->private;
@@ -387,6 +399,10 @@ static void kvm_vfio_destroy(struct kvm_device *dev)
 
 static int kvm_vfio_create(struct kvm_device *dev, u32 type);
 
+/*
+ * used by:
+ *   - virt/kvm/vfio.c|434| <<kvm_vfio_ops_init>> return kvm_register_device_ops(&kvm_vfio_ops, KVM_DEV_TYPE_VFIO);
+ */
 static struct kvm_device_ops kvm_vfio_ops = {
 	.name = "kvm-vfio",
 	.create = kvm_vfio_create,
@@ -395,6 +411,9 @@ static struct kvm_device_ops kvm_vfio_ops = {
 	.has_attr = kvm_vfio_has_attr,
 };
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.create = kvm_vfio_create()
+ */
 static int kvm_vfio_create(struct kvm_device *dev, u32 type)
 {
 	struct kvm_device *tmp;
@@ -417,11 +436,19 @@ static int kvm_vfio_create(struct kvm_device *dev, u32 type)
 	return 0;
 }
 
+/*
+ * called only by:
+ *   - virt/kvm/kvm_main.c|4187| <<kvm_init>> r = kvm_vfio_ops_init();
+ */
 int kvm_vfio_ops_init(void)
 {
 	return kvm_register_device_ops(&kvm_vfio_ops, KVM_DEV_TYPE_VFIO);
 }
 
+/*
+ * called only by:
+ *   - virt/kvm/kvm_main.c|4227| <<kvm_exit>> kvm_vfio_ops_exit();
+ */
 void kvm_vfio_ops_exit(void)
 {
 	kvm_unregister_device_ops(KVM_DEV_TYPE_VFIO);
-- 
2.7.4

