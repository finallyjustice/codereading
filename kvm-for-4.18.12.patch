From 4620fe4e64d1d2a275a7931d5d876759d214730f Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang@oracle.com>
Date: Thu, 24 Jan 2019 16:14:15 +0800
Subject: [PATCH 1/1] kvm for 4.18.12

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/kvm_host.h | 281 ++++++++++++++
 arch/x86/kernel/apic/apic.c     |   7 +
 arch/x86/kernel/kvmclock.c      |  77 ++++
 arch/x86/kvm/i8254.c            |  10 +
 arch/x86/kvm/lapic.c            |  30 ++
 arch/x86/kvm/mmu.c              | 825 +++++++++++++++++++++++++++++++++++++++-
 arch/x86/kvm/mmu.h              |   7 +
 arch/x86/kvm/page_track.c       | 150 ++++++++
 arch/x86/kvm/pmu_intel.c        |  39 ++
 arch/x86/kvm/vmx.c              | 517 +++++++++++++++++++++++++
 arch/x86/kvm/x86.c              | 130 +++++++
 include/linux/kvm_host.h        |  29 ++
 virt/kvm/eventfd.c              |   4 +
 virt/kvm/irqchip.c              |   4 +
 virt/kvm/kvm_main.c             | 241 +++++++++++-
 virt/kvm/vfio.c                 |  27 ++
 16 files changed, 2374 insertions(+), 4 deletions(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0722b77..d838175 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -102,12 +102,45 @@
 
 /* KVM Hugepage definitions for x86 */
 #define KVM_NR_PAGE_SIZES	3
+/*
+ * x = 1: KVM_HPAGE_GFN_SHIFT(1) = 0   none
+ * x = 2: KVM_HPAGE_GFN_SHIFT(2) = 9   2M
+ * x = 3: KVM_HPAGE_GFN_SHIFT(3) = 18  1G
+ */
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+/*
+ * KVM_HPAGE_SHIFT(1) = 12 +  0 = 12
+ * KVM_HPAGE_SHIFT(2) = 12 +  9 = 21
+ * KVM_HPAGE_SHIFT(3) = 12 + 18 = 30
+ */
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
+/*
+ * KVM_HPAGE_SIZE(1) = 1 << 12 = 4K
+ * KVM_HPAGE_SIZE(2) = 1 << 21 = 2M
+ * KVM_HPAGE_SIZE(3) = 1 << 30 = 1G
+ */
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
+/*
+ * KVM_HPAGE_MASK(1) : mask了4K
+ * KVM_HPAGE_MASK(2) : mask了2M
+ * KVM_HPAGE_MASK(3) : mask了1G
+ */
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
+/*
+ * KVM_PAGES_PER_HPAGE(1) : 4K有多少4K的page
+ * KVM_PAGES_PER_HPAGE(2) : 2M有多少4K的page
+ * KVM_PAGES_PER_HPAGE(3) : 1G有多少4K的page
+ */
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
+/*
+ * base_gfn是基于4k开始的gfn
+ * gfn是基于4k结束的gfn
+ * 计算从开始到结束需要用到几个hugepage (或者普通page)
+ *   level是1的时候hugepage大小是4K
+ *   level是2的时候hugepage大小是2M
+ *   level是3的时候hugepage大小是1G
+ */
 static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 {
 	/* KVM_HPAGE_GFN_SHIFT(PT_PAGE_TABLE_LEVEL) must be 0. */
@@ -248,9 +281,27 @@ struct kvm_mmu_memory_cache {
 union kvm_mmu_page_role {
 	unsigned word;
 	struct {
+		/*
+		 * The level in the shadow paging hierarchy that this shadow page belongs to.
+		 * 1=4k sptes, 2=2M sptes, 3=1G sptes, etc.
+		 *
+		 */
 		unsigned level:4;
 		unsigned cr4_pae:1;
 		unsigned quadrant:2;
+		/*
+		 * If set, leaf sptes reachable from this page are for a linear range.
+		 * Examples include real mode translation, large guest pages backed by small
+		 * host pages, and gpa->hpa translations when NPT or EPT is active.
+		 * The linear range starts at (gfn << PAGE_SHIFT) and its size is determined
+		 * by role.level (2MB for first level, 1GB for second level, 0.5TB for third
+		 * level, 256TB for fourth level)
+		 * If clear, this page corresponds to a guest page table denoted by the gfn
+		 * field.
+		 *
+		 * 在以下修改:
+		 *   - arch/x86/kvm/mmu.c|2588| <<kvm_mmu_get_page>> role.direct = direct;
+		 */
 		unsigned direct:1;
 		unsigned access:3;
 		unsigned invalid:1;
@@ -258,6 +309,11 @@ union kvm_mmu_page_role {
 		unsigned cr0_wp:1;
 		unsigned smep_andnot_wp:1;
 		unsigned smap_andnot_wp:1;
+		/*
+		 * Is 1 if the MMU instance cannot use A/D bits.  EPT did not have A/D
+		 * bits before Haswell; shadow EPT page tables also cannot use A/D bits
+		 * if the L1 hypervisor does not enable them.
+		 */
 		unsigned ad_disabled:1;
 		unsigned guest_mode:1;
 		unsigned :6;
@@ -289,15 +345,70 @@ struct kvm_mmu_page {
 
 	u64 *spt;
 	/* hold the gfn of each spte inside spt */
+	/* 似乎tdp不用, shadow才用 */
 	gfn_t *gfns;
+	/*
+	 * 在以下修改:
+	 *   - arch/x86/kvm/mmu.c|3036| <<kvm_unsync_page>> sp->unsync = 1;
+	 *   - arch/x86/kvm/mmu.c|2303| <<kvm_unlink_unsync_page>> sp->unsync = 0;
+	 */
 	bool unsync;
+	/*
+	 * 在以下设置或者修改:
+	 *   - arch/x86/kvm/mmu.c|3656| <<mmu_free_root_page>> --sp->root_count;
+	 *   - arch/x86/kvm/mmu.c|3721| <<mmu_alloc_direct_roots>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu.c|3737| <<mmu_alloc_direct_roots>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu.c|3781| <<mmu_alloc_shadow_roots>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu.c|3818| <<mmu_alloc_shadow_roots>> ++sp->root_count;
+	 */
 	int root_count;          /* Currently serving as active root */
+	/*
+	 * 在以下被增加:
+	 *   - arch/x86/kvm/mmu.c|2118| <<mark_unsync>> if (sp->unsync_children++)
+	 *
+	 * 在以下被减少:
+	 *   - arch/x86/kvm/mmu.c|2168| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 */
 	unsigned int unsync_children;
+	/*
+	 * 在以下使用或修改:
+	 *   - arch/x86/kvm/mmu.c|2045| <<mmu_page_add_parent_pte>> pte_list_add(vcpu, parent_pte, &sp->parent_ptes);
+	 *   - arch/x86/kvm/mmu.c|2051| <<mmu_page_remove_parent_pte>> pte_list_remove(parent_pte, &sp->parent_ptes);
+	 *   - arch/x86/kvm/mmu.c|2104| <<kvm_mmu_mark_parents_unsync>> for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
+	 *   - arch/x86/kvm/mmu.c|2743| <<kvm_mmu_unlink_parents>> while ((sptep = rmap_get_first(&sp->parent_ptes, &iter)))
+	 */
 	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
 
 	/* The page is obsolete if mmu_valid_gen != kvm->arch.mmu_valid_gen.  */
+	/*
+	 * Zapping all pages (page generation count)
+	 *
+	 * For the large memory guests, walking and zapping all pages is really slow
+	 * (because there are a lot of pages), and also blocks memory accesses of
+	 * all VCPUs because it needs to hold the MMU lock.
+	 *
+	 * To make it be more scalable, kvm maintains a global generation number
+	 * which is stored in kvm->arch.mmu_valid_gen.  Every shadow page stores
+	 * the current global generation-number into sp->mmu_valid_gen when it
+	 * is created.  Pages with a mismatching generation number are "obsolete".
+	 *
+	 * When KVM need zap all shadow pages sptes, it just simply increases the global
+	 * generation-number then reload root shadow pages on all vcpus.  As the VCPUs
+	 * create new shadow page tables, the old pages are not used because of the
+	 * mismatching generation number.
+	 *
+	 * KVM then walks through all pages and zaps obsolete pages.  While the zap
+	 * operation needs to take the MMU lock, the lock can be released periodically
+	 * so that the VCPUs can make progress.
+	 */
 	unsigned long mmu_valid_gen;
 
+	/*
+	 * 在以下被使用:
+	 *   - arch/x86/kvm/mmu.c|2116| <<mark_unsync>> if (__test_and_set_bit(index, sp->unsync_child_bitmap))
+	 *   - arch/x86/kvm/mmu.c|2170| <<clear_unsync_child_bit>> __clear_bit(idx, sp->unsync_child_bitmap);
+	 *   - arch/x86/kvm/mmu.c|2178| <<__mmu_unsync_walk>> for_each_set_bit(i, sp->unsync_child_bitmap, 512) {
+	 */
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
 #ifdef CONFIG_X86_32
@@ -332,27 +443,124 @@ struct rsvd_bits_validate {
  * current mmu mode.
  */
 struct kvm_mmu {
+	/*
+	 * 设置set_cr3的地方:
+	 *   - arch/x86/kvm/mmu.c|4563| <<init_kvm_tdp_mmu>> context->set_cr3 = kvm_x86_ops->set_tdp_cr3;
+	 *   - arch/x86/kvm/mmu.c|4660| <<init_kvm_softmmu>> context->set_cr3 = kvm_x86_ops->set_cr3;
+	 *   - arch/x86/kvm/svm.c|2923| <<nested_svm_init_mmu_context>> vcpu->arch.mmu.set_cr3 = nested_svm_set_tdp_cr3;
+	 *   - arch/x86/kvm/vmx.c|11331| <<nested_ept_init_mmu_context>> vcpu->arch.mmu.set_cr3 = vmx_set_cr3;
+	 */
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long root);
+	/*
+	 * 设置get_cr3的地方:
+	 *   - arch/x86/kvm/mmu.c|4564| <<init_kvm_tdp_mmu>> context->get_cr3 = get_cr3;
+	 *   - arch/x86/kvm/mmu.c|4661| <<init_kvm_softmmu>> context->get_cr3 = get_cr3;
+	 *   - arch/x86/kvm/mmu.c|4670| <<init_kvm_nested_mmu>> g_context->get_cr3 = get_cr3;
+	 *   - arch/x86/kvm/svm.c|2924| <<nested_svm_init_mmu_context>> vcpu->arch.mmu.get_cr3 = nested_svm_get_tdp_cr3;
+	 *   - arch/x86/kvm/vmx.c|11332| <<nested_ept_init_mmu_context>> vcpu->arch.mmu.get_cr3 = nested_ept_get_cr3;
+	 */
 	unsigned long (*get_cr3)(struct kvm_vcpu *vcpu);
+	/*
+	 * 设置get_pdptr的地方:
+	 *   - arch/x86/kvm/mmu.c|4565| <<init_kvm_tdp_mmu>> context->get_pdptr = kvm_pdptr_read;
+	 *   - arch/x86/kvm/mmu.c|4662| <<init_kvm_softmmu>> context->get_pdptr = kvm_pdptr_read;
+	 *   - arch/x86/kvm/mmu.c|4671| <<init_kvm_nested_mmu>> g_context->get_pdptr = kvm_pdptr_read;
+	 *   - arch/x86/kvm/svm.c|2925| <<nested_svm_init_mmu_context>> vcpu->arch.mmu.get_pdptr = nested_svm_get_tdp_pdptr;
+	 */
 	u64 (*get_pdptr)(struct kvm_vcpu *vcpu, int index);
+	/*
+	 * 设置page_fault的地方:
+	 *   - arch/x86/kvm/mmu.c|4013| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+	 *   - arch/x86/kvm/mmu.c|4498| <<paging64_init_context_common>> context->page_fault = paging64_page_fault;
+	 *   - arch/x86/kvm/mmu.c|4528| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+	 *   - arch/x86/kvm/mmu.c|4556| <<init_kvm_tdp_mmu>> context->page_fault = tdp_page_fault;
+	 *   - arch/x86/kvm/mmu.c|4637| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault;
+	 */
 	int (*page_fault)(struct kvm_vcpu *vcpu, gva_t gva, u32 err,
 			  bool prefault);
 	void (*inject_page_fault)(struct kvm_vcpu *vcpu,
 				  struct x86_exception *fault);
+	/*
+	 * 设置gva_to_gpa的地方:
+	 *   - arch/x86/kvm/mmu.c|4014| <<nonpaging_init_context>> context->gva_to_gpa = nonpaging_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4499| <<paging64_init_context_common>> context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4529| <<paging32_init_context>> context->gva_to_gpa = paging32_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4570| <<init_kvm_tdp_mmu>> context->gva_to_gpa = nonpaging_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4577| <<init_kvm_tdp_mmu>> context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4582| <<init_kvm_tdp_mmu>> context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4587| <<init_kvm_tdp_mmu>> context->gva_to_gpa = paging32_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4638| <<kvm_init_shadow_ept_mmu>> context->gva_to_gpa = ept_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu.c|4685| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = nonpaging_gva_to_gpa_nested;
+	 *   - arch/x86/kvm/mmu.c|4691| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging64_gva_to_gpa_nested;
+	 *   - arch/x86/kvm/mmu.c|4696| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging64_gva_to_gpa_nested;
+	 *   - arch/x86/kvm/mmu.c|4701| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging32_gva_to_gpa_nested;
+	 */
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t gva, u32 access,
 			    struct x86_exception *exception);
 	gpa_t (*translate_gpa)(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 			       struct x86_exception *exception);
+	/*
+	 * 设置sync_page的地方:
+	 *   - arch/x86/kvm/mmu.c|4298| <<nonpaging_init_context>> context->sync_page = nonpaging_sync_page;
+	 *   - arch/x86/kvm/mmu.c|4783| <<paging64_init_context_common>> context->sync_page = paging64_sync_page;
+	 *   - arch/x86/kvm/mmu.c|4813| <<paging32_init_context>> context->sync_page = paging32_sync_page;
+	 *   - arch/x86/kvm/mmu.c|4842| <<init_kvm_tdp_mmu>> context->sync_page = nonpaging_sync_page;
+	 *   - arch/x86/kvm/mmu.c|4924| <<kvm_init_shadow_ept_mmu>> context->sync_page = ept_sync_page;
+	 */
 	int (*sync_page)(struct kvm_vcpu *vcpu,
 			 struct kvm_mmu_page *sp);
 	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva);
 	void (*update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			   u64 *spte, const void *pte);
+	/*
+	 * 设置root_hpa valid值的地方:
+	 *   - arch/x86/kvm/mmu.c|3481| <<mmu_alloc_direct_roots>> vcpu->arch.mmu.root_hpa = __pa(sp->spt);
+	 *   - arch/x86/kvm/mmu.c|3499| <<mmu_alloc_direct_roots>> vcpu->arch.mmu.root_hpa = __pa(vcpu->arch.mmu.pae_root);
+	 *   - arch/x86/kvm/mmu.c|3537| <<mmu_alloc_shadow_roots>> vcpu->arch.mmu.root_hpa = root;
+	 *   - arch/x86/kvm/mmu.c|3577| <<mmu_alloc_shadow_roots>> vcpu->arch.mmu.root_hpa = __pa(vcpu->arch.mmu.pae_root);
+	 *   - arch/x86/kvm/mmu.c|3601| <<mmu_alloc_shadow_roots>> vcpu->arch.mmu.root_hpa = __pa(vcpu->arch.mmu.lm_root);
+	 */
 	hpa_t root_hpa;
 	union kvm_mmu_page_role base_role;
+	/*
+	 * 设置的地方:
+	 *   - arch/x86/kvm/mmu.c|4383| <<nonpaging_init_context>> context->root_level = 0;
+	 *   - arch/x86/kvm/mmu.c|4855| <<paging64_init_context_common>> context->root_level = level;
+	 *   - arch/x86/kvm/mmu.c|4886| <<paging32_init_context>> context->root_level = PT32_ROOT_LEVEL;
+	 *   - arch/x86/kvm/mmu.c|4938| <<init_kvm_tdp_mmu>> context->root_level = 0;
+	 *   - arch/x86/kvm/mmu.c|4941| <<init_kvm_tdp_mmu>> context->root_level = is_la57_mode(vcpu) ?
+	 *   - arch/x86/kvm/mmu.c|4947| <<init_kvm_tdp_mmu>> context->root_level = PT32E_ROOT_LEVEL;
+	 *   - arch/x86/kvm/mmu.c|4952| <<init_kvm_tdp_mmu>> context->root_level = PT32_ROOT_LEVEL;
+	 *   - arch/x86/kvm/mmu.c|5013| <<kvm_init_shadow_ept_mmu>> context->root_level = PT64_ROOT_4LEVEL;
+	 *   - arch/x86/kvm/mmu.c|5055| <<init_kvm_nested_mmu>> g_context->root_level = 0;
+	 *   - arch/x86/kvm/mmu.c|5059| <<init_kvm_nested_mmu>> g_context->root_level = is_la57_mode(vcpu) ?
+	 *   - arch/x86/kvm/mmu.c|5065| <<init_kvm_nested_mmu>> g_context->root_level = PT32E_ROOT_LEVEL;
+	 *   - arch/x86/kvm/mmu.c|5070| <<init_kvm_nested_mmu>> g_context->root_level = PT32_ROOT_LEVEL;
+	 *
+	 * 在tdp下如果开了long mode, 则不是4就是5
+	 */
 	u8 root_level;
+	/*
+	 * 设置的地方:
+	 *   - arch/x86/kvm/mmu.c|4379| <<nonpaging_init_context>> context->shadow_root_level = PT32E_ROOT_LEVEL;
+	 *   - arch/x86/kvm/mmu.c|4863| <<paging64_init_context_common>> context->shadow_root_level = level;
+	 *   - arch/x86/kvm/mmu.c|4893| <<paging32_init_context>> context->shadow_root_level = PT32E_ROOT_LEVEL;
+	 *   - arch/x86/kvm/mmu.c|4922| <<init_kvm_tdp_mmu>> context->shadow_root_level = kvm_x86_ops->get_tdp_level(vcpu);
+	 *   - arch/x86/kvm/mmu.c|4999| <<kvm_init_shadow_ept_mmu>> context->shadow_root_level = PT64_ROOT_4LEVEL;
+	 *   - arch/x86/kvm/svm.c|2927| <<nested_svm_init_mmu_context>> vcpu->arch.mmu.shadow_root_level = get_npt_level(vcpu);
+	 *
+	 * 对于tdp不是4就是5
+	 */
 	u8 shadow_root_level;
 	u8 ept_ad;
+	/*
+	 * 设置direct_map的地方:
+	 *   - arch/x86/kvm/mmu.c|4003| <<nonpaging_init_context>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|4487| <<paging64_init_context_common>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4517| <<paging32_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4544| <<init_kvm_tdp_mmu>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|4626| <<kvm_init_shadow_ept_mmu>> context->direct_map = false;
+	 */
 	bool direct_map;
 
 	/*
@@ -726,6 +934,10 @@ struct kvm_lpage_info {
 struct kvm_arch_memory_slot {
 	struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+	/*
+	 * 在kvm_page_track_create_memslot()初始化第二维
+	 * 应该是每个slot中page的数量吧
+	 */
 	unsigned short *gfn_track[KVM_PAGE_TRACK_MAX];
 };
 
@@ -778,16 +990,77 @@ enum kvm_irqchip_mode {
 };
 
 struct kvm_arch {
+	/*
+	 * 被以下修改:
+	 *   - arch/x86/kvm/mmu.c|1973| <<kvm_mod_used_mmu_pages>> kvm->arch.n_used_mmu_pages += nr;
+	 *
+	 * 被以下使用:
+	 *   - arch/x86/kvm/mmu.c|2727| <<kvm_mmu_change_mmu_pages>> if (kvm->arch.n_used_mmu_pages > goal_nr_mmu_pages) {
+	 *   - arch/x86/kvm/mmu.c|2729| <<kvm_mmu_change_mmu_pages>> while (kvm->arch.n_used_mmu_pages > goal_nr_mmu_pages)
+	 *   - arch/x86/kvm/mmu.c|2734| <<kvm_mmu_change_mmu_pages>> goal_nr_mmu_pages = kvm->arch.n_used_mmu_pages;
+	 *   - arch/x86/kvm/mmu.c|5729| <<mmu_shrink_scan>> if (!kvm->arch.n_used_mmu_pages &&
+	 *   - arch/x86/kvm/mmu.h|73| <<kvm_mmu_available_pages>> if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
+	 *   - arch/x86/kvm/mmu.h|75| <<kvm_mmu_available_pages>> kvm->arch.n_used_mmu_pages;
+	 */
 	unsigned int n_used_mmu_pages;
+	/*
+	 * 被以下修改:
+	 *   - arch/x86/kvm/x86.c|4035| <<kvm_vm_ioctl_set_nr_mmu_pages>> kvm->arch.n_requested_mmu_pages = kvm_nr_mmu_pages;
+	 *
+	 * 在以下使用:
+	 *   - arch/x86/kvm/x86.c|9196| <<kvm_arch_commit_memory_region>> if (!kvm->arch.n_requested_mmu_pages)
+	 */
 	unsigned int n_requested_mmu_pages;
 	unsigned int n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
+	/*
+	 * Zapping all pages (page generation count)
+	 *
+	 * For the large memory guests, walking and zapping all pages is really slow
+	 * (because there are a lot of pages), and also blocks memory accesses of
+	 * all VCPUs because it needs to hold the MMU lock.
+	 *
+	 * To make it be more scalable, kvm maintains a global generation number
+	 * which is stored in kvm->arch.mmu_valid_gen.  Every shadow page stores
+	 * the current global generation-number into sp->mmu_valid_gen when it
+	 * is created.  Pages with a mismatching generation number are "obsolete".
+	 *
+	 * When KVM need zap all shadow pages sptes, it just simply increases the global
+	 * generation-number then reload root shadow pages on all vcpus.  As the VCPUs
+	 * create new shadow page tables, the old pages are not used because of the
+	 * mismatching generation number.
+	 *
+	 * KVM then walks through all pages and zaps obsolete pages.  While the zap
+	 * operation needs to take the MMU lock, the lock can be released periodically
+	 * so that the VCPUs can make progress.
+	 */
 	unsigned long mmu_valid_gen;
+	/*
+	 * 通过hash可以快速获得一个gfn对应的也表页面
+	 */
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*
 	 * Hash table of struct kvm_mmu_page.
 	 */
+	/*
+	 * used by:
+	 *   - arch/x86/kvm/mmu.c|2030| <<kvm_mmu_alloc_page>> list_add(&sp->link, &vcpu->kvm->arch.active_mmu_pages);
+	 *   - arch/x86/kvm/mmu.c|2665| <<kvm_mmu_prepare_zap_page>> list_move(&sp->link, &kvm->arch.active_mmu_pages);
+	 *   - arch/x86/kvm/mmu.c|2709| <<prepare_zap_oldest_mmu_page>> if (list_empty(&kvm->arch.active_mmu_pages))
+	 *   - arch/x86/kvm/mmu.c|2712| <<prepare_zap_oldest_mmu_page>> sp = list_last_entry(&kvm->arch.active_mmu_pages,
+	 *   - arch/x86/kvm/mmu.c|5606| <<kvm_zap_obsolete_pages>> &kvm->arch.active_mmu_pages, link) {
+	 *   - arch/x86/kvm/mmu_audit.c|92| <<walk_all_active_sps>> list_for_each_entry(sp, &kvm->arch.active_mmu_pages, link)
+	 *   - arch/x86/kvm/x86.c|8822| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
+	 */
 	struct list_head active_mmu_pages;
+	/*
+	 * used by:
+	 *   - arch/x86/kvm/mmu.c|6107| <<kvm_zap_obsolete_pages>> &kvm->arch.zapped_obsolete_pages);
+	 *   - arch/x86/kvm/mmu.c|6118| <<kvm_zap_obsolete_pages>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+	 *   - arch/x86/kvm/mmu.c|6159| <<kvm_has_zapped_obsolete_pages>> return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));
+	 *   - arch/x86/kvm/mmu.c|6214| <<mmu_shrink_scan>> &kvm->arch.zapped_obsolete_pages);
+	 *   - arch/x86/kvm/x86.c|8826| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.zapped_obsolete_pages);
+	 */
 	struct list_head zapped_obsolete_pages;
 	struct kvm_page_track_notifier_node mmu_sp_tracker;
 	struct kvm_page_track_notifier_head track_notifier_head;
@@ -922,6 +1195,10 @@ struct kvm_lapic_irq {
 	bool msi_redir_hint;
 };
 
+/*
+ * intel : vmx_x86_ops
+ * amd   : svm_x86_ops
+ */
 struct kvm_x86_ops {
 	int (*cpu_has_kvm_support)(void);          /* __init */
 	int (*disabled_by_bios)(void);             /* __init */
@@ -1114,6 +1391,10 @@ extern struct kvm_x86_ops *kvm_x86_ops;
 #define __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)
 {
+	/*
+	 * intel: vmx_vm_alloc()
+	 * amd  : svm_vm_alloc()
+	 */
 	return kvm_x86_ops->vm_alloc();
 }
 
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index 3b3a2d0..c0f7ab8 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -998,6 +998,10 @@ void setup_secondary_APIC_clock(void)
 /*
  * The guts of the apic timer interrupt
  */
+/*
+ * called by only:
+ *   - arch/x86/kernel/apic/apic.c|1054| <<smp_apic_timer_interrupt>> local_apic_timer_interrupt();
+ */
 static void local_apic_timer_interrupt(void)
 {
 	struct clock_event_device *evt = this_cpu_ptr(&lapic_events);
@@ -1037,6 +1041,9 @@ static void local_apic_timer_interrupt(void)
  * [ if a single-CPU system runs an SMP kernel then we call the local
  *   interrupt as well. Thus we cannot inline the local irq ... ]
  */
+/*
+ * arch/x86/entry/entry_64.S: 用作LOCAL_TIMER_VECTOR (0xec)的处理函数
+ */
 __visible void __irq_entry smp_apic_timer_interrupt(struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 3b8e7c1..a3e8828 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -32,6 +32,76 @@
 #include <asm/reboot.h>
 #include <asm/kvmclock.h>
 
+/*
+ * FROM SUSE!!!
+ * When using kvm-clock, it is not recommended to use NTP in the VM Guest, as
+ * well. Using NTP on the VM Host Server, however, is still recommended. 
+ */
+
+/*
+ * Clocksource is a device that can give a timestamp whenever you need it. In
+ * other words, Clocksource is any ticking counter that allows you to get its
+ * value.
+ *
+ * Clockevent device is an alarm clock—you ask the device to signal a time in
+ * the future (e.g., "wake me up in 1ms") and when the alarm is triggered, you
+ * get the signal.
+ *
+ * sched_clock() function is similar to clocksource, but this particular one
+ * should be "cheap" to read (meaning that one can get its value fast), as
+ * sched_clock() is used for task-scheduling purposes and scheduling happens
+ * often. We're ready to sacrifice accuracy and other characteristics for
+ * speed.
+ *
+ * > CLOCK_REALTIME clock gives the time passed since January 1, 1970. This
+ *   clock is affected by NTP adjustments and can jump forward and backward when
+ *   a system administrator adjusts system time.
+ *
+ * > CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually
+ *   since you booted the system. This clock is affected by NTP, but it can't
+ *   jump backward.
+ *
+ * > CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this
+ *   clock is not affected by NTP adjustments.
+ *
+ * > CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but
+ *   less-accurate variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ *
+ *
+ * Hardware extensions for virtualizing TSC
+ *
+ * Since the early days of hardware-assisted virtualization, Intel was
+ * supplying an option to do TSC offsetting for virtual guests in hardware,
+ * which would mean that a guest's rdtsc reading will return a host's TSC value
+ * + offset. Unfortunately, this wasn't enough to support migration between
+ * different hosts because TSC frequency may differ, so pvclock and TSC page
+ * protocol were introduced. In late 2015, Intel introduced the TSC scaling
+ * feature (which was already present in AMD processors for several years) and,
+ * in theory, this is a game changer making pvclock and TSC page protocols
+ * redundant. However, an immediate switch to using plain TSC as a clocksource
+ * for virtualized guests seems impractical; one must be sure that all
+ * potential migration recipient hosts support the feature, but it is not yet
+ * widely available. Extensive testing also must be performed to make sure
+ * there are no drawbacks to switching from paravirtualized protocols.
+ */
+
+/*
+ * To get the current TSC reading, guests must do the following math:
+ *
+ * PerCPUTime = ((RDTSC() - tsc_timestamp) >> tsc_shift) * tsc_to_system_mul + system_time 
+ *
+ *
+ *
+ * kvmclock or KVM pvclock lets guests read the host's wall clock time. It's
+ * really very simple: the guest sets aside a page of its RAM and asks the host
+ * to write time into that page (using an MSR). The host writes a structure
+ * containing the current time to this page - in theory the host updates this
+ * page constantly, but in reality that would be wasteful and the structure is
+ * only updated just before reentering the guest after some VM event.
+ * host更新clock的时间的函数在kvm_guest_time_update(), 只被vcpu_enter_guest()调用
+ */
+
 static int kvmclock __ro_after_init = 1;
 static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
@@ -45,6 +115,9 @@ static int parse_no_kvmclock(char *arg)
 early_param("no-kvmclock", parse_no_kvmclock);
 
 /* The hypervisor will put information about time periodically here */
+/*
+ * 相当与xen的vcpuinfo里的时间的部分 和host共享
+ */
 static struct pvclock_vsyscall_time_info *hv_clock;
 static struct pvclock_wall_clock *wall_clock;
 
@@ -273,6 +346,10 @@ static void __init kvm_memblock_free(phys_addr_t addr, phys_addr_t size)
 	memblock_free(addr, size);
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kernel/setup.c|1207| <<setup_arch>> kvmclock_init();
+ */
 void __init kvmclock_init(void)
 {
 	struct pvclock_vcpu_time_info *vcpu_time;
diff --git a/arch/x86/kvm/i8254.c b/arch/x86/kvm/i8254.c
index af19289..f5d8eb1 100644
--- a/arch/x86/kvm/i8254.c
+++ b/arch/x86/kvm/i8254.c
@@ -264,6 +264,12 @@ static void pit_do_work(struct kthread_work *work)
 			kvm_apic_nmi_wd_deliver(vcpu);
 }
 
+/*
+ * used by:
+ *   - arch/x86/kvm/i8254.c|680| <<kvm_create_pit>> pit_state->timer.function = pit_timer_fn;
+ *
+ * 传统pit的时钟函数, 都用lapic了
+ */
 static enum hrtimer_restart pit_timer_fn(struct hrtimer *data)
 {
 	struct kvm_kpit_state *ps = container_of(data, struct kvm_kpit_state, timer);
@@ -645,6 +651,10 @@ static const struct kvm_io_device_ops speaker_dev_ops = {
 	.write    = speaker_ioport_write,
 };
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/x86.c|4390| <<kvm_arch_vm_ioctl>> kvm->arch.vpit = kvm_create_pit(kvm, u.pit_config.flags);
+ */
 struct kvm_pit *kvm_create_pit(struct kvm *kvm, u32 flags)
 {
 	struct kvm_pit *pit;
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index b5cd846..78ba778 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -2021,6 +2021,10 @@ void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/x86.c|8473| <<kvm_vcpu_reset>> kvm_lapic_reset(vcpu, init_event);
+ */
 void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2139,6 +2143,12 @@ static const struct kvm_io_device_ops apic_mmio_ops = {
 	.write    = apic_mmio_write,
 };
 
+/*
+ * used by:
+ *   - arch/x86/kvm/lapic.c|2191| <<kvm_create_lapic>> apic->lapic_timer.timer.function = apic_timer_fn;
+ *
+ * kvm基于内核的apic timer的handler
+ */
 static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 {
 	struct kvm_timer *ktimer = container_of(data, struct kvm_timer, timer);
@@ -2154,6 +2164,20 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 		return HRTIMER_NORESTART;
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/x86.c|8737| <<kvm_arch_vcpu_init>> r = kvm_create_lapic(vcpu);
+ *
+ * QEMU和KVM都实现了对中断芯片的模拟,这是由于历史原因造成的.早在KVM诞生之前,
+ * QEMU就提供了一整套对设备的模拟,包括中断芯片.而KVM诞生之后,为了进一步提高
+ * 中断性能,因此又在KVM中实现了一套中断芯片.我们可以通过QEMU的启动参数
+ * kernel-irqchip来决定使用谁的中断芯片(irq chip)/
+ *   on: KVM模拟全部
+ *   split: QEMU模拟IOAPIC和PIC,KVM模拟LAPIC
+ *   off: QEMU模拟全部
+ *
+ * 分配并且初始化vcpu的apic (在内核中模拟)
+ */
 int kvm_create_lapic(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic;
@@ -2167,6 +2191,11 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 
 	vcpu->arch.apic = apic;
 
+	/*
+	 * 在vmx_vcpu_reset()写入vmcs的VIRTUAL_APIC_PAGE_ADDR
+	 *
+	 * 在kvm_lapic_reset()初始化regs的一些默认值
+	 */
 	apic->regs = (void *)get_zeroed_page(GFP_KERNEL);
 	if (!apic->regs) {
 		printk(KERN_ERR "malloc apic regs error for vcpu %x\n",
@@ -2177,6 +2206,7 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 
 	hrtimer_init(&apic->lapic_timer.timer, CLOCK_MONOTONIC,
 		     HRTIMER_MODE_ABS_PINNED);
+	/* apic timer的函数 */
 	apic->lapic_timer.timer.function = apic_timer_fn;
 
 	/*
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 97d4175..72ce75e 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -50,12 +50,34 @@
 #include "trace.h"
 
 /*
+ * 当QEMU通过ioctl创建vcpu时,调用kvm_mmu_create初始化mmu相关信息,
+ * 为页表项结构分配slab cache.
+ *
+ * 当KVM要进入Guest前,vcpu_enter_guest => kvm_mmu_reload会将根级
+ * 页表地址加载到VMCS,让Guest使用该页表.
+ *
+ * 当EPT Violation发生时,VMEXIT到KVM中.如果是缺页,则拿到对应的GPA,
+ * 根据GPA算出gfn,根据gfn找到对应的memory slot,得到对应的HVA.然后
+ * 根据HVA找到对应的pfn,确保该page位于内存.在把缺的页填上后,需要更
+ * 新EPT,完善其中缺少的页表项.于是从L4开始,逐层补全页表,对于在某层
+ * 上缺少的页表页,会从slab中分配后将新页的HPA填入到上一级页表中.
+ *
+ * 除了建立上级页表到下级页表的关联外,KVM 还会建立反向映射,可以直
+ * 接根据GPA找到gfn相关的页表项,而无需再次走EPT查询.
+ */
+
+/*
  * When setting this variable to true it enables Two-Dimensional-Paging
  * where the hardware walks 2 page tables:
  * 1. the guest-virtual to guest-physical
  * 2. while doing 1. it walks guest-physical to host-physical
  * If the hardware supports that we don't need to do shadow paging.
  */
+/*
+ * 在以下修改:
+ *   - arch/x86/kvm/mmu.c|5063| <<kvm_enable_tdp>> tdp_enabled = true;
+ *   - arch/x86/kvm/mmu.c|5069| <<kvm_disable_tdp>> tdp_enabled = false;
+ */
 bool tdp_enabled = false;
 
 enum {
@@ -84,14 +106,31 @@ module_param(dbg, bool, 0644);
 
 #define PTE_PREFETCH_NUM		8
 
+/*
+ * used by:
+ *   - arch/x86/kvm/mmu.c|169| <<SPTE_HOST_WRITEABLE>> #define SPTE_HOST_WRITEABLE (1ULL << PT_FIRST_AVAIL_BITS_SHIFT)
+ *   - arch/x86/kvm/mmu.c|171| <<SPTE_MMU_WRITEABLE>> #define SPTE_MMU_WRITEABLE (1ULL << (PT_FIRST_AVAIL_BITS_SHIFT + 1))
+ */
 #define PT_FIRST_AVAIL_BITS_SHIFT 10
+/*
+ * used only by:
+ *   - arch/x86/kvm/mmu.c|273| <<global>> static const u64 shadow_acc_track_saved_bits_shift = PT64_SECOND_AVAIL_BITS_SHIFT;
+ */
 #define PT64_SECOND_AVAIL_BITS_SHIFT 52
 
 #define PT64_LEVEL_BITS 9
 
+/*
+ * PT64_LEVEL_SHIFT(1) = 12 + (1 - 1) * 9 = 12
+ * PT64_LEVEL_SHIFT(2) = 12 + (2 - 1) * 9 = 21
+ * PT64_LEVEL_SHIFT(3) = 12 + (3 - 1) * 9 = 30
+ */
 #define PT64_LEVEL_SHIFT(level) \
 		(PAGE_SHIFT + (level - 1) * PT64_LEVEL_BITS)
 
+/*
+ * 先把adress根据level右移若干位, 然后取出最后9位作为index
+ */
 #define PT64_INDEX(address, level)\
 	(((address) >> PT64_LEVEL_SHIFT(level)) & ((1 << PT64_LEVEL_BITS) - 1))
 
@@ -126,16 +165,33 @@ module_param(dbg, bool, 0644);
 	(PAGE_MASK & ~((1ULL << (PAGE_SHIFT + (((level) - 1) \
 					    * PT32_LEVEL_BITS))) - 1))
 
+/*
+ * PT_PRESENT_MASK: 1往左移动0位
+ * PT_WRITABLE_MASK: 1往左移动1位
+ */
 #define PT64_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \
 			| shadow_x_mask | shadow_nx_mask | shadow_me_mask)
 
+/* 1往左移动0位 */
 #define ACC_EXEC_MASK    1
+/* 1往左移动1位 */
 #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+/* 1往左移动2位 */
 #define ACC_USER_MASK    PT_USER_MASK
+/*
+ * 最后的0, 1, 2三位都是1
+ */
 #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
 
 /* The mask for the R/X bits in EPT PTEs */
+/*
+ * used only by:
+ *   - arch/x86/kvm/mmu.c|298| <<global>> static const u64 shadow_acc_track_saved_bits_mask = PT64_EPT_READABLE_MASK |
+ */
 #define PT64_EPT_READABLE_MASK			0x1ull
+/*
+ * 只在设置shadow_acc_track_saved_bits_mask的时候用到
+ */
 #define PT64_EPT_EXECUTABLE_MASK		0x4ull
 
 #include <trace/events/kvm.h>
@@ -143,9 +199,14 @@ module_param(dbg, bool, 0644);
 #define CREATE_TRACE_POINTS
 #include "mmutrace.h"
 
+/* 1往左移10位 */
 #define SPTE_HOST_WRITEABLE	(1ULL << PT_FIRST_AVAIL_BITS_SHIFT)
+/* 1往左移11位 */
 #define SPTE_MMU_WRITEABLE	(1ULL << (PT_FIRST_AVAIL_BITS_SHIFT + 1))
 
+/*
+ * 先把addr根据level右移若干位, 然后取出最后9位作为index
+ */
 #define SHADOW_PT_INDEX(addr, level) PT64_INDEX(addr, level)
 
 /* make pte_list_desc fit well in cache line */
@@ -171,10 +232,15 @@ struct pte_list_desc {
 };
 
 struct kvm_shadow_walk_iterator {
+	/* 发生page fault的GPA,迭代过程就是要把GPA所涉及的页表项都填上 */
 	u64 addr;
+	/* 当前页表项的 HPA，在 shadow_walk_init 中设置为 vcpu->arch.mmu.root_hpa */
 	hpa_t shadow_addr;
+	/* 指向当前页表项, 在shadow_walk_okay()中更新 */
 	u64 *sptep;
+	/* 当前层级,在shadow_walk_okay()中设置为4 (x86_64 PT64_ROOT_LEVEL), 在shadow_walk_next()中减1 */
 	int level;
+	/* 在当前level页表中的索引,在shadow_walk_okey中更新 */
 	unsigned index;
 };
 
@@ -189,16 +255,47 @@ struct kvm_shadow_walk_iterator {
 		({ spte = mmu_spte_get_lockless(_walker.sptep); 1; });	\
 	     __shadow_walk_next(&(_walker), spte))
 
+/*
+ * used by:
+ *   - arch/x86/kvm/mmu.c|1146| <<mmu_topup_memory_caches>> pte_list_desc_cache, 8 + PTE_PREFETCH_NUM);
+ *   - arch/x86/kvm/mmu.c|1165| <<mmu_free_memory_caches>> pte_list_desc_cache);
+ *   - arch/x86/kvm/mmu.c|1194| <<mmu_free_pte_list_desc>> kmem_cache_free(pte_list_desc_cache, pte_list_desc);
+ *   - arch/x86/kvm/mmu.c|6319| <<mmu_destroy_caches>> kmem_cache_destroy(pte_list_desc_cache);
+ *   - arch/x86/kvm/mmu.c|6332| <<kvm_mmu_module_init>> pte_list_desc_cache = kmem_cache_create("pte_list_desc",
+ *   - arch/x86/kvm/mmu.c|6335| <<kvm_mmu_module_init>> if (!pte_list_desc_cache)
+ */
 static struct kmem_cache *pte_list_desc_cache;
 static struct kmem_cache *mmu_page_header_cache;
 static struct percpu_counter kvm_total_used_mmu_pages;
 
+/*
+ * 设置的地方:
+ *   - arch/x86/kvm/mmu.c|467| <<kvm_mmu_set_mask_ptes>> shadow_nx_mask = nx_mask;
+ *   - arch/x86/kvm/mmu.c|483| <<kvm_mmu_reset_all_pte_masks>> shadow_nx_mask = 0;
+ */
 static u64 __read_mostly shadow_nx_mask;
 static u64 __read_mostly shadow_x_mask;	/* mutual exclusive with nx_mask */
 static u64 __read_mostly shadow_user_mask;
 static u64 __read_mostly shadow_accessed_mask;
 static u64 __read_mostly shadow_dirty_mask;
+/*
+ * 在两处设置:
+ *   - arch/x86/kvm/mmu.c|274| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_mask = mmio_mask | SPTE_SPECIAL_MASK;
+ *   - arch/x86/kvm/mmu.c|453| <<kvm_mmu_reset_all_pte_masks>> shadow_mmio_mask = 0;
+ *
+ * vmx_enable_tdp()-->ept_set_mmio_spte_mask()设置
+ *     shadow_mmio_mask  = 111
+ *     shadow_mmio_value = 110
+ */
 static u64 __read_mostly shadow_mmio_mask;
+/*
+ * 在一处设置:
+ *   - arch/x86/kvm/mmu.c|273| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_value = mmio_value | SPTE_SPECIAL_MASK;
+ *
+ * vmx_enable_tdp()-->ept_set_mmio_spte_mask()设置
+ *     shadow_mmio_mask  = 111
+ *     shadow_mmio_value = 110
+ */
 static u64 __read_mostly shadow_mmio_value;
 static u64 __read_mostly shadow_present_mask;
 static u64 __read_mostly shadow_me_mask;
@@ -232,8 +329,23 @@ static u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
  */
 static const u64 shadow_nonpresent_or_rsvd_mask_len = 5;
 
+/* x86_64下就是WRITE_ONCE(*sptep, spte) */
 static void mmu_spte_set(u64 *sptep, u64 spte);
 
+/*
+ * For a summary, the following shows the process of MMIO implementation:
+ *
+ * 1. QEMU declares a memory region(but not allocate ram or commit it to kvm)
+ * 2. Guest first access the MMIO address, cause a EPT violation VM-exit
+ * 3. KVM construct the EPT page table and marks the page table entry with special mark(110b)
+ * 4. Later the guest access these MMIO, it will be processed by EPT misconfig VM-exit handler
+ */
+
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|6518| <<ept_set_mmio_spte_mask>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_RWX_MASK,
+ *   - arch/x86/kvm/x86.c|6545| <<kvm_set_mmio_spte_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask);
+ */
 void kvm_mmu_set_mmio_spte_mask(u64 mmio_mask, u64 mmio_value)
 {
 	BUG_ON((mmio_mask & mmio_value) != mmio_value);
@@ -244,6 +356,11 @@ EXPORT_SYMBOL_GPL(kvm_mmu_set_mmio_spte_mask);
 
 static inline bool sp_ad_disabled(struct kvm_mmu_page *sp)
 {
+	/*
+	 * Is 1 if the MMU instance cannot use A/D bits.  EPT did not have A/D
+	 * bits before Haswell; shadow EPT page tables also cannot use A/D bits
+	 * if the L1 hypervisor does not enable them.
+	 */
 	return sp->role.ad_disabled;
 }
 
@@ -265,6 +382,9 @@ static inline u64 spte_shadow_dirty_mask(u64 spte)
 	return spte_ad_enabled(spte) ? shadow_dirty_mask : 0;
 }
 
+/*
+ * !spte_ad_enabled(spte)和(spte & shadow_acc_track_mask)必须都返回0
+ */
 static inline bool is_access_track_spte(u64 spte)
 {
 	return !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;
@@ -284,13 +404,22 @@ static inline bool is_access_track_spte(u64 spte)
 
 #define MMIO_GEN_SHIFT			20
 #define MMIO_GEN_LOW_SHIFT		10
+/* 9个1和最后一个0 */
 #define MMIO_GEN_LOW_MASK		((1 << MMIO_GEN_LOW_SHIFT) - 2)
+/* 20个1 */
 #define MMIO_GEN_MASK			((1 << MMIO_GEN_SHIFT) - 1)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|399| <<mark_mmio_spte>> u64 mask = generation_mmio_spte_mask(gen);
+ *   - arch/x86/kvm/mmu.c|420| <<get_mmio_spte_gfn>> u64 mask = generation_mmio_spte_mask(MMIO_GEN_MASK) | shadow_mmio_mask |
+ *   - arch/x86/kvm/mmu.c|432| <<get_mmio_spte_access>> u64 mask = generation_mmio_spte_mask(MMIO_GEN_MASK) | shadow_mmio_mask;
+ */
 static u64 generation_mmio_spte_mask(unsigned int gen)
 {
 	u64 mask;
 
+	/* 20个1 */
 	WARN_ON(gen & ~MMIO_GEN_MASK);
 
 	mask = (gen & MMIO_GEN_LOW_MASK) << MMIO_SPTE_GEN_LOW_SHIFT;
@@ -298,6 +427,9 @@ static u64 generation_mmio_spte_mask(unsigned int gen)
 	return mask;
 }
 
+/*
+ * 给定一个spte, 获得其中的mmio generation
+ */
 static unsigned int get_mmio_spte_generation(u64 spte)
 {
 	unsigned int gen;
@@ -328,11 +460,20 @@ static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 		<< shadow_nonpresent_or_rsvd_mask_len;
 
 	trace_mark_mmio_spte(sptep, gfn, access, gen);
+	/* x86_64下就是WRITE_ONCE(*sptep, spte) */
 	mmu_spte_set(sptep, mask);
 }
 
 static bool is_mmio_spte(u64 spte)
 {
+	/*
+	 * shadow_mmio_mask和shadow_mmio_value在一处设置:
+	 *   - arch/x86/kvm/mmu.c|273| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_value = mmio_value | SPTE_SPECIAL_MASK;
+	 *
+	 * vmx_enable_tdp()-->ept_set_mmio_spte_mask()设置
+	 *     shadow_mmio_mask  = 111
+	 *     shadow_mmio_value = 110
+	 */
 	return (spte & shadow_mmio_mask) == shadow_mmio_value;
 }
 
@@ -383,6 +524,11 @@ static bool check_mmio_spte(struct kvm_vcpu *vcpu, u64 spte)
  *  - Setting either @accessed_mask or @dirty_mask requires setting both
  *  - At least one of @accessed_mask or @acc_track_mask must be set
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|7973| <<vmx_enable_tdp>> kvm_mmu_set_mask_ptes(VMX_EPT_READABLE_MASK,
+ *   - arch/x86/kvm/x86.c|6629| <<kvm_arch_init>> kvm_mmu_set_mask_ptes(PT_USER_MASK, PT_ACCESSED_MASK,
+ */
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 		u64 dirty_mask, u64 nx_mask, u64 x_mask, u64 p_mask,
 		u64 acc_track_mask, u64 me_mask)
@@ -402,6 +548,9 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mask_ptes);
 
+/*
+ * called by only kvm_mmu_module_init()
+ */
 static void kvm_mmu_reset_all_pte_masks(void)
 {
 	shadow_user_mask = 0;
@@ -443,6 +592,10 @@ static int is_shadow_present_pte(u64 pte)
 
 static int is_large_pte(u64 pte)
 {
+	/*
+	 * 1后面跟着7个0
+	 * 从0开始数是第7位
+	 */
 	return pte & PT_PAGE_SIZE_MASK;
 }
 
@@ -493,6 +646,7 @@ static u64 __get_spte_lockless(u64 *sptep)
 	return READ_ONCE(*sptep);
 }
 #else
+/* 不是CONFIG_X86_64使用 */
 union split_spte {
 	struct {
 		u32 spte_low;
@@ -501,6 +655,7 @@ union split_spte {
 	u64 spte;
 };
 
+/* 不是CONFIG_X86_64使用 */
 static void count_spte_clear(u64 *sptep, u64 spte)
 {
 	struct kvm_mmu_page *sp =  page_header(__pa(sptep));
@@ -513,6 +668,7 @@ static void count_spte_clear(u64 *sptep, u64 spte)
 	sp->clear_spte_count++;
 }
 
+/* 不是CONFIG_X86_64使用 */
 static void __set_spte(u64 *sptep, u64 spte)
 {
 	union split_spte *ssptep, sspte;
@@ -532,6 +688,7 @@ static void __set_spte(u64 *sptep, u64 spte)
 	WRITE_ONCE(ssptep->spte_low, sspte.spte_low);
 }
 
+/* 不是CONFIG_X86_64使用 */
 static void __update_clear_spte_fast(u64 *sptep, u64 spte)
 {
 	union split_spte *ssptep, sspte;
@@ -551,6 +708,7 @@ static void __update_clear_spte_fast(u64 *sptep, u64 spte)
 	count_spte_clear(sptep, spte);
 }
 
+/* 不是CONFIG_X86_64使用 */
 static u64 __update_clear_spte_slow(u64 *sptep, u64 spte)
 {
 	union split_spte *ssptep, sspte, orig;
@@ -585,6 +743,7 @@ static u64 __update_clear_spte_slow(u64 *sptep, u64 spte)
  * present->non-present updates: if it changed while reading the spte,
  * we might have hit the race.  This is done using clear_spte_count.
  */
+/* 不是CONFIG_X86_64使用 */
 static u64 __get_spte_lockless(u64 *sptep)
 {
 	struct kvm_mmu_page *sp =  page_header(__pa(sptep));
@@ -609,12 +768,25 @@ static u64 __get_spte_lockless(u64 *sptep)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|726| <<spte_has_volatile_bits>> if (spte_can_locklessly_be_made_writable(spte) ||
+ *   - arch/x86/kvm/mmu.c|818| <<mmu_spte_update>> if (spte_can_locklessly_be_made_writable(old_spte) &&
+ *   - arch/x86/kvm/mmu.c|911| <<mark_spte_for_access_track>> !spte_can_locklessly_be_made_writable(spte),
+ *   - arch/x86/kvm/mmu.c|1576| <<spte_write_protect>> !(pt_protect && spte_can_locklessly_be_made_writable(spte)))
+ *   - arch/x86/kvm/mmu.c|3684| <<fast_page_fault>> spte_can_locklessly_be_made_writable(spte))
+ */
 static bool spte_can_locklessly_be_made_writable(u64 spte)
 {
 	return (spte & (SPTE_HOST_WRITEABLE | SPTE_MMU_WRITEABLE)) ==
 		(SPTE_HOST_WRITEABLE | SPTE_MMU_WRITEABLE);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|784| <<mmu_spte_update_no_track>> if (!spte_has_volatile_bits(old_spte))
+ *   - arch/x86/kvm/mmu.c|856| <<mmu_spte_clear_track_bits>> if (!spte_has_volatile_bits(old_spte))
+ */
 static bool spte_has_volatile_bits(u64 spte)
 {
 	if (!is_shadow_present_pte(spte))
@@ -639,6 +811,13 @@ static bool spte_has_volatile_bits(u64 spte)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|827| <<mmu_spte_update>> if (is_accessed_spte(old_spte) && !is_accessed_spte(new_spte)) {
+ *   - arch/x86/kvm/mmu.c|873| <<mmu_spte_clear_track_bits>> if (is_accessed_spte(old_spte))
+ *   - arch/x86/kvm/mmu.c|948| <<mmu_spte_age>> if (!is_accessed_spte(spte))
+ *   - arch/x86/kvm/mmu.c|2011| <<kvm_test_age_rmapp>> if (is_accessed_spte(*sptep))
+ */
 static bool is_accessed_spte(u64 spte)
 {
 	u64 accessed_mask = spte_shadow_accessed_mask(spte);
@@ -647,6 +826,11 @@ static bool is_accessed_spte(u64 spte)
 			     : !is_access_track_spte(spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|832| <<mmu_spte_update>> if (is_dirty_spte(old_spte) && !is_dirty_spte(new_spte)) {
+ *   - arch/x86/kvm/mmu.c|876| <<mmu_spte_clear_track_bits>> if (is_dirty_spte(old_spte))
+ */
 static bool is_dirty_spte(u64 spte)
 {
 	u64 dirty_mask = spte_shadow_dirty_mask(spte);
@@ -660,6 +844,9 @@ static bool is_dirty_spte(u64 spte)
  * or in a state where the hardware will not attempt to update
  * the spte.
  */
+/*
+ * x86_64下就是WRITE_ONCE(*sptep, spte)
+ */
 static void mmu_spte_set(u64 *sptep, u64 new_spte)
 {
 	WARN_ON(is_shadow_present_pte(*sptep));
@@ -670,6 +857,11 @@ static void mmu_spte_set(u64 *sptep, u64 new_spte)
  * Update the SPTE (excluding the PFN), but do not track changes in its
  * accessed/dirty status.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|833| <<mmu_spte_update>> u64 old_spte = mmu_spte_update_no_track(sptep, new_spte);
+ *   - arch/x86/kvm/mmu.c|988| <<mmu_spte_age>> mmu_spte_update_no_track(sptep, spte);
+ */
 static u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)
 {
 	u64 old_spte = *sptep;
@@ -677,6 +869,7 @@ static u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)
 	WARN_ON(!is_shadow_present_pte(new_spte));
 
 	if (!is_shadow_present_pte(old_spte)) {
+		/* x86_64下就是WRITE_ONCE(*sptep, spte) */
 		mmu_spte_set(sptep, new_spte);
 		return old_spte;
 	}
@@ -743,6 +936,11 @@ static bool mmu_spte_update(u64 *sptep, u64 new_spte)
  * state bits, it is used to clear the last level sptep.
  * Returns non-zero if the PTE was previously valid.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1499| <<drop_spte>> if (mmu_spte_clear_track_bits(sptep))
+ *   - arch/x86/kvm/mmu.c|1801| <<kvm_set_pte_rmapp>> mmu_spte_clear_track_bits(sptep);
+ */
 static int mmu_spte_clear_track_bits(u64 *sptep)
 {
 	kvm_pfn_t pfn;
@@ -887,6 +1085,11 @@ static void walk_shadow_page_lockless_end(struct kvm_vcpu *vcpu)
 	local_irq_enable();
 }
 
+/*
+ * x86下的调用:
+ *   - arch/x86/kvm/mmu.c|988| <<mmu_topup_memory_caches>> r = mmu_topup_memory_cache(&vcpu->arch.mmu_pte_list_desc_cache,
+ *   - arch/x86/kvm/mmu.c|995| <<mmu_topup_memory_caches>> r = mmu_topup_memory_cache(&vcpu->arch.mmu_page_header_cache,
+ */
 static int mmu_topup_memory_cache(struct kvm_mmu_memory_cache *cache,
 				  struct kmem_cache *base_cache, int min)
 {
@@ -915,6 +1118,10 @@ static void mmu_free_memory_cache(struct kvm_mmu_memory_cache *mc,
 		kmem_cache_free(cache, mc->objects[--mc->nobjs]);
 }
 
+/*
+ * called by only:
+ *   - arch/x86/kvm/mmu.c|1092| <<mmu_topup_memory_caches>> r = mmu_topup_memory_cache_page(&vcpu->arch.mmu_page_cache, 8);
+ */
 static int mmu_topup_memory_cache_page(struct kvm_mmu_memory_cache *cache,
 				       int min)
 {
@@ -937,6 +1144,15 @@ static void mmu_free_memory_cache_page(struct kvm_mmu_memory_cache *mc)
 		free_page((unsigned long)mc->objects[--mc->nobjs]);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4030| <<nonpaging_page_fault>> r = mmu_topup_memory_caches(vcpu);
+ *   - arch/x86/kvm/mmu.c|4203| <<tdp_page_fault>> r = mmu_topup_memory_caches(vcpu);
+ *   - arch/x86/kvm/mmu.c|4987| <<kvm_mmu_load>> r = mmu_topup_memory_caches(vcpu);
+ *   - arch/x86/kvm/mmu.c|5189| <<kvm_mmu_pte_write>> mmu_topup_memory_caches(vcpu);
+ *   - arch/x86/kvm/paging_tmpl.h|749| <<FNAME(page_fault)>> r = mmu_topup_memory_caches(vcpu);
+ *   - arch/x86/kvm/paging_tmpl.h|872| <<FNAME(invlpg)>> mmu_topup_memory_caches(vcpu);
+ */
 static int mmu_topup_memory_caches(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -954,6 +1170,10 @@ static int mmu_topup_memory_caches(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/mmu.c|6249| <<kvm_mmu_destroy>> mmu_free_memory_caches(vcpu);
+ */
 static void mmu_free_memory_caches(struct kvm_vcpu *vcpu)
 {
 	mmu_free_memory_cache(&vcpu->arch.mmu_pte_list_desc_cache,
@@ -963,6 +1183,13 @@ static void mmu_free_memory_caches(struct kvm_vcpu *vcpu)
 				mmu_page_header_cache);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1135| <<mmu_alloc_pte_list_desc>> return mmu_memory_cache_alloc(&vcpu->arch.mmu_pte_list_desc_cache);
+ *   - arch/x86/kvm/mmu.c|2174| <<kvm_mmu_alloc_page>> sp = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_header_cache);
+ *   - arch/x86/kvm/mmu.c|2176| <<kvm_mmu_alloc_page>> sp->spt = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_cache);
+ *   - arch/x86/kvm/mmu.c|2178| <<kvm_mmu_alloc_page>> sp->gfns = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_cache);
+ */
 static void *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)
 {
 	void *p;
@@ -990,6 +1217,9 @@ static gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index)
 	return sp->gfn + (index << ((sp->role.level - 1) * PT64_LEVEL_BITS));
 }
 
+/*
+ * direct==true什么也不做
+ */
 static void kvm_mmu_page_set_gfn(struct kvm_mmu_page *sp, int index, gfn_t gfn)
 {
 	if (sp->role.direct)
@@ -1002,12 +1232,25 @@ static void kvm_mmu_page_set_gfn(struct kvm_mmu_page *sp, int index, gfn_t gfn)
  * Return the pointer to the large page information for a given gfn,
  * handling slots that are not large page aligned.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1180| <<update_gfn_disallow_lpage_count>> linfo = lpage_info_slot(gfn, slot, i);
+ *   - arch/x86/kvm/mmu.c|1243| <<__mmu_gfn_lpage_is_disallowed>> linfo = lpage_info_slot(gfn, slot, level);
+ */
 static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
 					      struct kvm_memory_slot *slot,
 					      int level)
 {
 	unsigned long idx;
 
+	/*
+	 * base_gfn是基于4k开始的gfn
+	 * gfn是基于4k结束的gfn
+	 * 计算从开始到结束需要用到几个hugepage
+	 *   level是1的时候hugepage大小是4K
+	 *   level是2的时候hugepage大小是2M
+	 *   level是3的时候hugepage大小是1G
+	 */
 	idx = gfn_to_index(gfn, slot->base_gfn, level);
 	return &slot->arch.lpage_info[level - 2][idx];
 }
@@ -1030,6 +1273,11 @@ void kvm_mmu_gfn_disallow_lpage(struct kvm_memory_slot *slot, gfn_t gfn)
 	update_gfn_disallow_lpage_count(slot, gfn, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1241| <<unaccount_shadowed>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ *   - arch/x86/kvm/page_track.c|144| <<kvm_slot_page_track_remove_page>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_allow_lpage(struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, -1);
@@ -1071,6 +1319,11 @@ static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	kvm_mmu_gfn_allow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1120| <<mmu_gfn_lpage_is_disallowed>> return __mmu_gfn_lpage_is_disallowed(gfn, level, slot);
+ *   - arch/x86/kvm/mmu.c|1198| <<mapping_level>> if (__mmu_gfn_lpage_is_disallowed(large_gfn, level, slot))
+ */
 static bool __mmu_gfn_lpage_is_disallowed(gfn_t gfn, int level,
 					  struct kvm_memory_slot *slot)
 {
@@ -1093,14 +1346,32 @@ static bool mmu_gfn_lpage_is_disallowed(struct kvm_vcpu *vcpu, gfn_t gfn,
 	return __mmu_gfn_lpage_is_disallowed(gfn, level, slot);
 }
 
+/*
+ * 只被以下调用:
+ *   - arch/x86/kvm/mmu.c|1338| <<mapping_level>> host_level = host_mapping_level(vcpu->kvm, large_gfn);
+ *
+ * 根据gfn获取其在ept(tdp)中的page size (4K,2M还是1G)
+ *     根据gfn获取在qemu中对应的hva, 把hva转化为vm_area_struct
+ *     然后就知道vm_area_struct的page size了
+ */
 static int host_mapping_level(struct kvm *kvm, gfn_t gfn)
 {
 	unsigned long page_size;
 	int i, ret = 0;
 
+	/*
+	 * 根据gfn获取其在ept(tdp)中的page size (4K,2M还是1G)
+	 *     根据gfn获取在qemu中对应的hva, 把hva转化为vm_area_struct
+	 *     然后就知道vm_area_struct的page size了
+	 */
 	page_size = kvm_host_page_size(kvm, gfn);
 
 	for (i = PT_PAGE_TABLE_LEVEL; i <= PT_MAX_HUGEPAGE_LEVEL; ++i) {
+		/*
+		 * KVM_HPAGE_SIZE(1) = 1 << 12 = 4K
+		 * KVM_HPAGE_SIZE(2) = 1 << 21 = 2M
+		 * KVM_HPAGE_SIZE(3) = 1 << 30 = 1G
+		 */
 		if (page_size >= KVM_HPAGE_SIZE(i))
 			ret = i;
 		else
@@ -1134,6 +1405,12 @@ gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu, gfn_t gfn,
 	return slot;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3977| <<tdp_page_fault>> level = mapping_level(vcpu, gfn, &force_pt_level);
+ *   - arch/x86/kvm/mmu.c|3345| <<nonpaging_map>> level = mapping_level(vcpu, gfn, &force_pt_level);
+ *   - arch/x86/kvm/paging_tmpl.h|786| <<FNAME(page_fault)>> level = mapping_level(vcpu, walker.gfn, &force_pt_level);
+ */
 static int mapping_level(struct kvm_vcpu *vcpu, gfn_t large_gfn,
 			 bool *force_pt_level)
 {
@@ -1143,6 +1420,12 @@ static int mapping_level(struct kvm_vcpu *vcpu, gfn_t large_gfn,
 	if (unlikely(*force_pt_level))
 		return PT_PAGE_TABLE_LEVEL;
 
+	/*
+	 * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+	 * struct kvm_memslots中有:
+	 *       struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+	 *       short id_to_index[KVM_MEM_SLOTS_NUM];
+	 */
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, large_gfn);
 	*force_pt_level = !memslot_valid_for_gpte(slot, true);
 	if (unlikely(*force_pt_level))
@@ -1153,6 +1436,9 @@ static int mapping_level(struct kvm_vcpu *vcpu, gfn_t large_gfn,
 	if (host_level == PT_PAGE_TABLE_LEVEL)
 		return host_level;
 
+	/*
+	 * vmx_get_lpage_level()根据配置返回2或者3, large page不是2M就是1G
+	 */
 	max_level = min(kvm_x86_ops->get_lpage_level(), host_level);
 
 	for (level = PT_DIRECTORY_LEVEL; level <= max_level; ++level)
@@ -1271,6 +1557,14 @@ static struct kvm_rmap_head *__gfn_to_rmap(gfn_t gfn, int level,
 {
 	unsigned long idx;
 
+	/*
+	 * base_gfn是基于4k开始的gfn
+	 * gfn是基于4k结束的gfn
+	 * 计算从开始到结束需要用到几个hugepage
+	 *   level是1的时候hugepage大小是4K
+	 *   level是2的时候hugepage大小是2M
+	 *   level是3的时候hugepage大小是1G
+	 */
 	idx = gfn_to_index(gfn, slot->base_gfn, level);
 	return &slot->arch.rmap[level - PT_PAGE_TABLE_LEVEL][idx];
 }
@@ -1300,6 +1594,7 @@ static int rmap_add(struct kvm_vcpu *vcpu, u64 *spte, gfn_t gfn)
 	struct kvm_rmap_head *rmap_head;
 
 	sp = page_header(__pa(spte));
+	/* direct==true什么也不做 */
 	kvm_mmu_page_set_gfn(sp, spte - sp->spt, gfn);
 	rmap_head = gfn_to_rmap(vcpu->kvm, gfn, sp);
 	return pte_list_add(vcpu, spte, rmap_head);
@@ -1393,6 +1688,17 @@ static u64 *rmap_get_next(struct rmap_iterator *iter)
 	for (_spte_ = rmap_get_first(_rmap_head_, _iter_);		\
 	     _spte_; _spte_ = rmap_get_next(_iter_))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1637| <<__drop_large_spte>> drop_spte(kvm, sptep);
+ *   - arch/x86/kvm/mmu.c|1882| <<kvm_zap_rmapp>> drop_spte(kvm, sptep);
+ *   - arch/x86/kvm/mmu.c|1918| <<kvm_set_pte_rmapp>> drop_spte(kvm, sptep);
+ *   - arch/x86/kvm/mmu.c|2906| <<mmu_page_zap_pte>> drop_spte(kvm, spte);
+ *   - arch/x86/kvm/mmu.c|3341| <<mmu_set_spte>> drop_spte(vcpu->kvm, sptep);
+ *   - arch/x86/kvm/mmu.c|5975| <<kvm_mmu_zap_collapsible_spte>> drop_spte(kvm, sptep);
+ *   - arch/x86/kvm/paging_tmpl.h|174| <<FNAME(prefetch_invalid_gpte)>> drop_spte(vcpu->kvm, spte);
+ *   - arch/x86/kvm/paging_tmpl.h|1013| <<FNAME(sync_page)>> drop_spte(vcpu->kvm, &sp->spt[i]);
+ */
 static void drop_spte(struct kvm *kvm, u64 *sptep)
 {
 	if (mmu_spte_clear_track_bits(sptep))
@@ -1924,18 +2230,37 @@ static int is_empty_shadow_page(u64 *spt)
  * aggregate version in order to make the slab shrinker
  * faster
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2031| <<kvm_mmu_alloc_page>> kvm_mod_used_mmu_pages(vcpu->kvm, +1);
+ *   - arch/x86/kvm/mmu.c|2663| <<kvm_mmu_prepare_zap_page>> kvm_mod_used_mmu_pages(kvm, -1);
+ *
+ * 增加nr到kvm->arch.n_used_mmu_pages
+ * 增加nr到oercpu counter kvm_total_used_mmu_pages
+ */
 static inline void kvm_mod_used_mmu_pages(struct kvm *kvm, int nr)
 {
 	kvm->arch.n_used_mmu_pages += nr;
 	percpu_counter_add(&kvm_total_used_mmu_pages, nr);
 }
 
+/*
+ * 只在一处调用:
+ *   - arch/x86/kvm/mmu.c|2818| <<kvm_mmu_commit_zap_page>> kvm_mmu_free_page(sp);
+ *
+ * 把kvm_mmu_page再各种链表取下来
+ * 然后释放其4k page
+ * 最后回收kvm_mmu_page
+ */
 static void kvm_mmu_free_page(struct kvm_mmu_page *sp)
 {
 	MMU_WARN_ON(!is_empty_shadow_page(sp->spt));
 	hlist_del(&sp->hash_link);
 	list_del(&sp->link);
 	free_page((unsigned long)sp->spt);
+	/*
+	 * tdp的direct是true
+	 */
 	if (!sp->role.direct)
 		free_page((unsigned long)sp->gfns);
 	kmem_cache_free(mmu_page_header_cache, sp);
@@ -1968,14 +2293,27 @@ static void drop_parent_pte(struct kvm_mmu_page *sp,
 	mmu_spte_clear_no_track(parent_pte);
 }
 
+/*
+ * 在tdp情况下direct似乎是true
+ *
+ * 分配一个struct kvm_mmu_page, 再分配kvm_mmu_page->spt (真正的4k)
+ * 把元数据kvm_mmu_page设置为4k struct page的private
+ * 把kvm_mmu_page链接到vcpu->kvm->arch.active_mmu_pages
+ * 增加nr到kvm->arch.n_used_mmu_pages, 增加nr到percpu counter kvm_total_used_mmu_pages 
+ */
 static struct kvm_mmu_page *kvm_mmu_alloc_page(struct kvm_vcpu *vcpu, int direct)
 {
 	struct kvm_mmu_page *sp;
 
+	/* 分配kvm_mmu_page */
 	sp = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_header_cache);
+	/* 分配真正的4k page */
 	sp->spt = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_cache);
 	if (!direct)
 		sp->gfns = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_cache);
+	/*
+	 * struct page的private设置成struct kvm_mmu_page
+	 */
 	set_page_private(virt_to_page(sp->spt), (unsigned long)sp);
 
 	/*
@@ -1984,21 +2322,52 @@ static struct kvm_mmu_page *kvm_mmu_alloc_page(struct kvm_vcpu *vcpu, int direct
 	 * this feature. See the comments in kvm_zap_obsolete_pages().
 	 */
 	list_add(&sp->link, &vcpu->kvm->arch.active_mmu_pages);
+	/*
+	 * 增加nr到kvm->arch.n_used_mmu_pages
+	 * 增加nr到percpu counter kvm_total_used_mmu_pages
+	 */
 	kvm_mod_used_mmu_pages(vcpu->kvm, +1);
 	return sp;
 }
 
 static void mark_unsync(u64 *spte);
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2120| <<mark_unsync>> kvm_mmu_mark_parents_unsync(sp);
+ *   - arch/x86/kvm/mmu.c|2936| <<kvm_unsync_page>> kvm_mmu_mark_parents_unsync(sp);
+ *
+ * 对struct kvm_mmu_page的每一个sp->parent_ptes的kvm_mmu_page()调用mark_unsync():
+ *     把这一条spte在4k page对应的struct kvm_mmu_page中的index算出来
+ *     在index对应的sp->unsync_child_bitmap设置bit
+ *     如果bit已经设置过了直接返回
+ *     否则根据情况是否用kvm_mmu_mark_parents_unsync()对上一级的kvm_mmu_page也进行相应的操作
+ */
 static void kvm_mmu_mark_parents_unsync(struct kvm_mmu_page *sp)
 {
 	u64 *sptep;
 	struct rmap_iterator iter;
 
 	for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
+		/*
+		 * 把这一条spte在4k page对应的struct kvm_mmu_page中的index算出来
+		 * 在index对应的sp->unsync_child_bitmap设置bit
+		 * 如果bit已经设置过了直接返回
+		 * 否则根据情况是否用kvm_mmu_mark_parents_unsync()对上一级的kvm_mmu_page也进行相应的操作
+		 */
 		mark_unsync(sptep);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2105| <<kvm_mmu_mark_parents_unsync>> mark_unsync(sptep);
+ *   - arch/x86/kvm/mmu.c|2679| <<link_shadow_page>> mark_unsync(sptep);
+ *
+ * 把这一条spte在4k page对应的struct kvm_mmu_page中的index算出来
+ * 在index对应的sp->unsync_child_bitmap设置bit
+ * 如果bit已经设置过了直接返回
+ * 否则根据情况是否用kvm_mmu_mark_parents_unsync()对上一级的kvm_mmu_page也进行相应的操作
+ */
 static void mark_unsync(u64 *spte)
 {
 	struct kvm_mmu_page *sp;
@@ -2063,6 +2432,11 @@ static inline void clear_unsync_child_bit(struct kvm_mmu_page *sp, int idx)
 	__clear_bit(idx, sp->unsync_child_bitmap);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2193| <<__mmu_unsync_walk>> ret = __mmu_unsync_walk(child, pvec);
+ *   - arch/x86/kvm/mmu.c|2222| <<mmu_unsync_walk>> return __mmu_unsync_walk(sp, pvec);
+ */
 static int __mmu_unsync_walk(struct kvm_mmu_page *sp,
 			   struct kvm_mmu_pages *pvec)
 {
@@ -2108,6 +2482,13 @@ static int mmu_unsync_walk(struct kvm_mmu_page *sp,
 			   struct kvm_mmu_pages *pvec)
 {
 	pvec->nr = 0;
+	/*
+	 * unsync_children在以下被增加:
+	 *   - arch/x86/kvm/mmu.c|2118| <<mark_unsync>> if (sp->unsync_children++)
+	 *
+	 * unsync_children在以下被减少:
+	 *   - arch/x86/kvm/mmu.c|2168| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 */
 	if (!sp->unsync_children)
 		return 0;
 
@@ -2115,6 +2496,10 @@ static int mmu_unsync_walk(struct kvm_mmu_page *sp,
 	return __mmu_unsync_walk(sp, pvec);
 }
 
+/*
+ * 把kvm_mmu_page->unsync设置成0
+ * 并且--kvm->stat.mmu_unsync
+ */
 static void kvm_unlink_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	WARN_ON(!sp->unsync);
@@ -2147,14 +2532,21 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 		if ((_sp)->gfn != (_gfn) || (_sp)->role.direct) {} else
 
 /* @sp->gfn should be write-protected at the call site */
+/*
+ * 在64-bit的tdp下, 调用kvm_mmu_prepare_zap_page()然后返回false
+ */
 static bool __kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			    struct list_head *invalid_list)
 {
+	/* 似乎在64-bit下ept不执行 */
 	if (sp->role.cr4_pae != !!is_pae(vcpu)) {
 		kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
 		return false;
 	}
 
+	/*
+	 * tdp下是nonpaging_sync_page(), 永远返回0
+	 */
 	if (vcpu->arch.mmu.sync_page(vcpu, sp) == 0) {
 		kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
 		return false;
@@ -2163,15 +2555,32 @@ static bool __kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	return true;
 }
 
+/*
+ * 如果invalid_list不为空,用kvm_flush_remote_tlbs()让每个vcpu调用tlb flush
+ * 然后把invalid_list上的一个个kvm_mmu_page释放 (包括真正的4k page)
+ *
+ * 否则, 根据情况调用某个cpu或全部cpu的tlb flush
+ */
 static void kvm_mmu_flush_or_zap(struct kvm_vcpu *vcpu,
 				 struct list_head *invalid_list,
 				 bool remote_flush, bool local_flush)
 {
 	if (!list_empty(invalid_list)) {
+		/*
+		 * 如果参数的invalid_list为空就退出
+		 * 否则用kvm_flush_remote_tlbs()让每个vcpu调用tlb flush
+		 * 然后把invalid_list上的一个个kvm_mmu_page释放 (包括真正的4k page)
+		 */
 		kvm_mmu_commit_zap_page(vcpu->kvm, invalid_list);
 		return;
 	}
 
+	/*
+	 * kvm_flush_remote_tlbs()
+	 *     应该就是让每个vcpu调用硬件的tlb flush吧
+	 *
+	 * 否则只flush某个vcpu的tlb
+	 */
 	if (remote_flush)
 		kvm_flush_remote_tlbs(vcpu->kvm);
 	else if (local_flush)
@@ -2190,14 +2599,33 @@ static bool is_obsolete_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 	return unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2340| <<kvm_sync_pages>> ret |= kvm_sync_page(vcpu, s, invalid_list);
+ *   - arch/x86/kvm/mmu.c|2440| <<mmu_sync_children>> flush |= kvm_sync_page(vcpu, sp, &invalid_list);
+ *
+ * 把kvm_mmu_page->unsync设置成0, 并且--kvm->stat.mmu_unsync
+ * 在64-bit的tdp下, 调用kvm_mmu_prepare_zap_page()然后返回false
+ */
 static bool kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			 struct list_head *invalid_list)
 {
+	/*
+	 * 把kvm_mmu_page->unsync设置成0
+	 * 并且--kvm->stat.mmu_unsync
+	 */
 	kvm_unlink_unsync_page(vcpu->kvm, sp);
+	/*
+	 * 在64-bit的tdp下, 调用kvm_mmu_prepare_zap_page()然后返回false
+	 */
 	return __kvm_sync_page(vcpu, sp, invalid_list);
 }
 
 /* @gfn should be write-protected at the call site */
+/*
+ * 只在以下的函数中使用 但是tdp不使用:
+ *   - arch/x86/kvm/mmu.c|2562| <<kvm_mmu_get_page>> flush |= kvm_sync_pages(vcpu, gfn, &invalid_list);
+ */
 static bool kvm_sync_pages(struct kvm_vcpu *vcpu, gfn_t gfn,
 			   struct list_head *invalid_list)
 {
@@ -2287,6 +2715,11 @@ static void mmu_pages_clear_parents(struct mmu_page_path *parents)
 	} while (!sp->unsync_children);
 }
 
+/*
+ * 在mmu_sync_roots()的两处被调用:
+ *   - arch/x86/kvm/mmu.c|3849| <<mmu_sync_roots>> mmu_sync_children(vcpu, sp);
+ *   - arch/x86/kvm/mmu.c|3859| <<mmu_sync_roots>> mmu_sync_children(vcpu, sp);
+ */
 static void mmu_sync_children(struct kvm_vcpu *vcpu,
 			      struct kvm_mmu_page *parent)
 {
@@ -2334,6 +2767,16 @@ static void clear_sp_write_flooding_count(u64 *spte)
 	__clear_sp_write_flooding_count(sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3051| <<__direct_map>> sp = kvm_mmu_get_page(vcpu, pseudo_gfn, iterator.addr,
+ *   - arch/x86/kvm/mmu.c|3477| <<mmu_alloc_direct_roots>> sp = kvm_mmu_get_page(vcpu, 0, 0,
+ *   - arch/x86/kvm/mmu.c|3492| <<mmu_alloc_direct_roots>> sp = kvm_mmu_get_page(vcpu, i << (30 - PAGE_SHIFT),
+ *   - arch/x86/kvm/mmu.c|3532| <<mmu_alloc_shadow_roots>> sp = kvm_mmu_get_page(vcpu, root_gfn, 0,
+ *   - arch/x86/kvm/mmu.c|3569| <<mmu_alloc_shadow_roots>> sp = kvm_mmu_get_page(vcpu, root_gfn, i << 30, PT32_ROOT_LEVEL,
+ *   - arch/x86/kvm/paging_tmpl.h|633| <<FNAME>> sp = kvm_mmu_get_page(vcpu, table_gfn, addr, it.level-1,
+ *   - arch/x86/kvm/paging_tmpl.h|663| <<FNAME>> sp = kvm_mmu_get_page(vcpu, direct_gfn, addr, it.level-1,
+ */
 static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 					     gfn_t gfn,
 					     gva_t gaddr,
@@ -2355,6 +2798,9 @@ static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 	if (role.direct)
 		role.cr4_pae = 0;
 	role.access = access;
+	/*
+	 * 在tdp下direct_map是true
+	 */
 	if (!vcpu->arch.mmu.direct_map
 	    && vcpu->arch.mmu.root_level <= PT32_ROOT_LEVEL) {
 		quadrant = gaddr >> (PAGE_SHIFT + (PT64_PT_BITS * level));
@@ -2394,12 +2840,22 @@ static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 
 	++vcpu->kvm->stat.mmu_cache_miss;
 
+	/*
+	 * 分配一个struct kvm_mmu_page, 再分配kvm_mmu_page->spt (真正的4k)
+	 * 把元数据kvm_mmu_page设置为4k struct page的private
+	 * 把kvm_mmu_page链接到vcpu->kvm->arch.active_mmu_pages
+	 * 增加nr到kvm->arch.n_used_mmu_pages, 增加nr到percpu counter kvm_total_used_mmu_pages
+	 */
 	sp = kvm_mmu_alloc_page(vcpu, direct);
 
 	sp->gfn = gfn;
 	sp->role = role;
+	/*
+	 * 把struct kvm_mmu_page添加到hash
+	 */
 	hlist_add_head(&sp->hash_link,
 		&vcpu->kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)]);
+	/* 在tdp情况下direct似乎是true */
 	if (!direct) {
 		/*
 		 * we should do write protection before syncing pages
@@ -2549,6 +3005,9 @@ static void kvm_mmu_page_unlink_children(struct kvm *kvm,
 {
 	unsigned i;
 
+	/*
+	 * sp->spt是u64 *spt
+	 */
 	for (i = 0; i < PT64_ENT_PER_PAGE; ++i)
 		mmu_page_zap_pte(kvm, sp, sp->spt + i);
 }
@@ -2562,6 +3021,10 @@ static void kvm_mmu_unlink_parents(struct kvm *kvm, struct kvm_mmu_page *sp)
 		drop_parent_pte(sp, sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2821| <<kvm_mmu_prepare_zap_page>> ret = mmu_zap_unsync_children(kvm, sp, invalid_list);
+ */
 static int mmu_zap_unsync_children(struct kvm *kvm,
 				   struct kvm_mmu_page *parent,
 				   struct list_head *invalid_list)
@@ -2586,6 +3049,17 @@ static int mmu_zap_unsync_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2226| <<__kvm_sync_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu.c|2231| <<__kvm_sync_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu.c|2692| <<mmu_zap_unsync_children>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu.c|2790| <<prepare_zap_oldest_mmu_page>> return kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu.c|2831| <<kvm_mmu_unprotect_page>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu.c|3568| <<mmu_free_root_page>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu.c|5170| <<kvm_mmu_pte_write>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu.c|5717| <<kvm_zap_obsolete_pages>> ret = kvm_mmu_prepare_zap_page(kvm, sp,
+ */
 static int kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				    struct list_head *invalid_list)
 {
@@ -2593,13 +3067,24 @@ static int kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 
 	trace_kvm_mmu_prepare_zap_page(sp);
 	++kvm->stat.mmu_shadow_zapped;
+	/*
+	 * 这是核心函数!!!!!!!!
+	 */
 	ret = mmu_zap_unsync_children(kvm, sp, invalid_list);
 	kvm_mmu_page_unlink_children(kvm, sp);
 	kvm_mmu_unlink_parents(kvm, sp);
 
+	/*
+	 * tdp的direct应该是true吧
+	 */
 	if (!sp->role.invalid && !sp->role.direct)
 		unaccount_shadowed(kvm, sp);
 
+	/*
+	 * kvm_unlink_unsync_page():
+	 *     把kvm_mmu_page->unsync设置成0
+	 *     并且--kvm->stat.mmu_unsync
+	 */
 	if (sp->unsync)
 		kvm_unlink_unsync_page(kvm, sp);
 	if (!sp->root_count) {
@@ -2614,6 +3099,9 @@ static int kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 		 * The obsolete pages can not be used on any vcpus.
 		 * See the comments in kvm_mmu_invalidate_zap_all_pages().
 		 */
+		/*
+		 * 让每个cpu在在vcpu_enter_guest()的时候调用kvm_mmu_unload()
+		 */
 		if (!sp->role.invalid && !is_obsolete_sp(kvm, sp))
 			kvm_reload_remote_mmus(kvm);
 	}
@@ -2622,6 +3110,21 @@ static int kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2240| <<kvm_mmu_flush_or_zap>> kvm_mmu_commit_zap_page(vcpu->kvm, invalid_list);
+ *   - arch/x86/kvm/mmu.c|2771| <<kvm_mmu_change_mmu_pages>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu.c|2795| <<kvm_mmu_unprotect_page>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu.c|3557| <<kvm_mmu_free_roots>> kvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu.c|5188| <<make_mmu_pages_available>> kvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu.c|5691| <<kvm_zap_obsolete_pages>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+ *   - arch/x86/kvm/mmu.c|5782| <<mmu_shrink_scan>> kvm_mmu_commit_zap_page(kvm,
+ *   - arch/x86/kvm/mmu.c|5789| <<mmu_shrink_scan>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *
+ * 如果参数的invalid_list为空就退出
+ * 否则用kvm_flush_remote_tlbs()让每个vcpu调用tlb flush
+ * 然后把invalid_list上的一个个kvm_mmu_page释放 (包括真正的4k page)
+ */
 static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 				    struct list_head *invalid_list)
 {
@@ -2639,6 +3142,9 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 	 * In addition, kvm_flush_remote_tlbs waits for all vcpus to exit
 	 * guest mode and/or lockless shadow page table walks.
 	 */
+	/*
+	 * 应该就是让每个vcpu调用硬件的tlb flush吧
+	 */
 	kvm_flush_remote_tlbs(kvm);
 
 	list_for_each_entry_safe(sp, nsp, invalid_list, link) {
@@ -2647,6 +3153,15 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2887| <<kvm_mmu_change_mmu_pages>> if (!prepare_zap_oldest_mmu_page(kvm, &invalid_list))
+ *   - arch/x86/kvm/mmu.c|5306| <<make_mmu_pages_available>> if (!prepare_zap_oldest_mmu_page(vcpu->kvm, &invalid_list))
+ *   - arch/x86/kvm/mmu.c|5910| <<mmu_shrink_scan>> if (prepare_zap_oldest_mmu_page(kvm, &invalid_list))
+ *
+ * 如果kvm->arch.active_mmu_pages是空,直接返回false
+ * 否则取出一个kvm->arch.active_mmu_pages上的kvm_mmu_page, 调用kvm_mmu_prepare_zap_page()
+ */
 static bool prepare_zap_oldest_mmu_page(struct kvm *kvm,
 					struct list_head *invalid_list)
 {
@@ -2707,12 +3222,30 @@ int kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_unprotect_page);
 
+/*
+ * called by only:
+ *   - arch/x86/kvm/mmu.c|2970| <<mmu_need_write_protect>> kvm_unsync_page(vcpu, sp);
+ *
+ * 把kvm_mmu_page->unsync设置成1
+ * 对struct kvm_mmu_page的每一个sp->parent_ptes的kvm_mmu_page()调用mark_unsync():
+ *     把这一条spte在4k page对应的struct kvm_mmu_page中的index算出来
+ *     在index对应的sp->unsync_child_bitmap设置bit
+ *     如果bit已经设置过了直接返回
+ *     否则根据情况是否用kvm_mmu_mark_parents_unsync()对上一级的kvm_mmu_page也进行相应的操作
+ */
 static void kvm_unsync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
 {
 	trace_kvm_mmu_unsync_page(sp);
 	++vcpu->kvm->stat.mmu_unsync;
 	sp->unsync = 1;
 
+	/*
+	 * 对struct kvm_mmu_page的每一个sp->parent_ptes的kvm_mmu_page()调用mark_unsync():
+	 *     把这一条spte在4k page对应的struct kvm_mmu_page中的index算出来
+	 *     在index对应的sp->unsync_child_bitmap设置bit
+	 *     如果bit已经设置过了直接返回
+	 *     否则根据情况是否用kvm_mmu_mark_parents_unsync()对上一级的kvm_mmu_page也进行相应的操作
+	 */
 	kvm_mmu_mark_parents_unsync(sp);
 }
 
@@ -2738,6 +3271,11 @@ static bool mmu_need_write_protect(struct kvm_vcpu *vcpu, gfn_t gfn,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3033| <<set_spte>> kvm_is_mmio_pfn(pfn));
+ *   - arch/x86/kvm/mmu.c|3040| <<set_spte>> if (!kvm_is_mmio_pfn(pfn))
+ */
 static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
 {
 	if (pfn_valid(pfn))
@@ -2999,9 +3537,19 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 	__direct_pte_prefetch(vcpu, sp, sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3999| <<tdp_page_fault>> r = __direct_map(vcpu, write, map_writable, level, gfn, pfn, prefault);
+ *   - arch/x86/kvm/mmu.c|3372| <<nonpaging_map>> r = __direct_map(vcpu, write, map_writable, level, gfn, pfn, prefault);
+ *
+ * 该函数建立了gfn到pfn的映射,同时将该page pin死在host的内存中
+ */
 static int __direct_map(struct kvm_vcpu *vcpu, int write, int map_writable,
 			int level, gfn_t gfn, kvm_pfn_t pfn, bool prefault)
 {
+	/*
+	 * 通过迭代器将ept中与该gpa相关的页表补充完整
+	 */
 	struct kvm_shadow_walk_iterator iterator;
 	struct kvm_mmu_page *sp;
 	int emulate = 0;
@@ -3010,6 +3558,31 @@ static int __direct_map(struct kvm_vcpu *vcpu, int write, int map_writable,
 	if (!VALID_PAGE(vcpu->arch.mmu.root_hpa))
 		return 0;
 
+	/*
+	 * 从leve4(root)开始,逐层补全页表对于每一层.
+	 *
+	 * 在每轮迭代中,sptep都会指向GPA在当前级页表中所对应的页表项,
+	 * 我们的目的就是把下一级页表的GPA填到该页表项内(即设置 sptep).
+	 * 因为是缺页,可能会出现下一级的页表页不存在的问题,这时候需要
+	 * 分配一个页表页,然后再将该页的GPA填进到sptep中.
+	 *
+	 * 举个例子,对于GPA(如0xfffff001),其二进制为:
+	 *
+	 * 000000000 000000011 111111111 111111111 000000000001
+	 *   PML4      PDPT       PD        PT        Offset
+	 *
+	 * 初始化状态: level = 4, shadow_addr = root_hpa, addr = GPA
+	 *
+	 * 执行流程：
+	 *
+	 * 1. index = addr在当前 level分段的值.如在level = 4时为0(000000000),在level = 3时为3(000000011)
+	 * 2. sptep = va(shadow_addr) + index, 得到GPA在当前地址中所对应的页表项HVA
+	 * 3. 如果sptep没值,分配一个page作为下级页表,同时将sptep设置为该page的HPA
+	 * 4. shadow_addr = *sptep,进入下级页表,循环
+	 *
+	 * 开启hugepage时,由于页表项管理的范围变大,所需页表级数减少,
+	 * 在默认情况下pag大小为2M,因此无需 level 1.
+	 */
 	for_each_shadow_entry(vcpu, (u64)gfn << PAGE_SHIFT, iterator) {
 		if (iterator.level == level) {
 			emulate = mmu_set_spte(vcpu, iterator.sptep, ACC_ALL,
@@ -3021,14 +3594,23 @@ static int __direct_map(struct kvm_vcpu *vcpu, int write, int map_writable,
 		}
 
 		drop_large_spte(vcpu, iterator.sptep);
+		/*
+		 * 如果下一级页表不存在, 即当前页表项没值
+		 */
 		if (!is_shadow_present_pte(*iterator.sptep)) {
 			u64 base_addr = iterator.addr;
 
 			base_addr &= PT64_LVL_ADDR_MASK(iterator.level);
 			pseudo_gfn = base_addr >> PAGE_SHIFT;
+			/*
+			 * 分配一个页表页结构
+			 */
 			sp = kvm_mmu_get_page(vcpu, pseudo_gfn, iterator.addr,
 					      iterator.level - 1, 1, ACC_ALL);
 
+			/*
+			 * 将新页表的hpa填入到当前页表项目(sptep)中
+			 */
 			link_shadow_page(vcpu, iterator.sptep, sp);
 		}
 	}
@@ -3213,6 +3795,11 @@ static bool is_access_allowed(u32 fault_err_code, u64 spte)
  * - true: let the vcpu to access on the same address again.
  * - false: let the real page fault path to fix it.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3997| <<tdp_page_fault>> if (fast_page_fault(vcpu, gpa, level, error_code))
+ *   - arch/x86/kvm/mmu.c|3370| <<nonpaging_map>> if (fast_page_fault(vcpu, v, level, error_code))
+ */
 static bool fast_page_fault(struct kvm_vcpu *vcpu, gva_t gva, int level,
 			    u32 error_code)
 {
@@ -3375,6 +3962,11 @@ static int nonpaging_map(struct kvm_vcpu *vcpu, gva_t v, u32 error_code,
 	return RET_PF_RETRY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3749| <<kvm_mmu_free_roots>> mmu_free_root_page(vcpu->kvm, &mmu->root_hpa, &invalid_list);
+ *   - arch/x86/kvm/mmu.c|3753| <<kvm_mmu_free_roots>> mmu_free_root_page(vcpu->kvm, &mmu->pae_root[i],
+ */
 static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 			       struct list_head *invalid_list)
 {
@@ -3384,6 +3976,14 @@ static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 		return;
 
 	sp = page_header(*root_hpa & PT64_BASE_ADDR_MASK);
+	/*
+	 * 在以下设置或者修改sp->root_count:
+	 *   - arch/x86/kvm/mmu.c|3656| <<mmu_free_root_page>> --sp->root_count;
+	 *   - arch/x86/kvm/mmu.c|3721| <<mmu_alloc_direct_roots>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu.c|3737| <<mmu_alloc_direct_roots>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu.c|3781| <<mmu_alloc_shadow_roots>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu.c|3818| <<mmu_alloc_shadow_roots>> ++sp->root_count;
+	 */
 	--sp->root_count;
 	if (!sp->root_count && sp->role.invalid)
 		kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
@@ -3391,6 +3991,11 @@ static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 	*root_hpa = INVALID_PAGE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4387| <<kvm_mmu_new_cr3>> kvm_mmu_free_roots(vcpu);
+ *   - arch/x86/kvm/mmu.c|5128| <<kvm_mmu_unload>> kvm_mmu_free_roots(vcpu);
+ */
 void kvm_mmu_free_roots(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -3402,6 +4007,11 @@ void kvm_mmu_free_roots(struct kvm_vcpu *vcpu)
 
 	spin_lock(&vcpu->kvm->mmu_lock);
 
+	/*
+	 * shadow_root_level对于tdp不是4就是5
+	 * root_level对于tdp在long mode下不是4就是5
+	 * direct_map在tdp下是true
+	 */
 	if (mmu->shadow_root_level >= PT64_ROOT_4LEVEL &&
 	    (mmu->root_level >= PT64_ROOT_4LEVEL || mmu->direct_map)) {
 		mmu_free_root_page(vcpu->kvm, &mmu->root_hpa, &invalid_list);
@@ -3413,6 +4023,11 @@ void kvm_mmu_free_roots(struct kvm_vcpu *vcpu)
 		mmu->root_hpa = INVALID_PAGE;
 	}
 
+	/*
+	 * 如果参数的invalid_list为空就退出
+	 * 否则用kvm_flush_remote_tlbs()让每个vcpu调用tlb flush
+	 * 然后把invalid_list上的一个个kvm_mmu_page释放 (包括真正的4k page)
+	 */
 	kvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);
 	spin_unlock(&vcpu->kvm->mmu_lock);
 }
@@ -3430,6 +4045,12 @@ static int mmu_check_root(struct kvm_vcpu *vcpu, gfn_t root_gfn)
 	return ret;
 }
 
+/*
+ * called by (vcpu->arch.mmu.direct_map是true的情况):
+ *   -  arch/x86/kvm/mmu.c|3582| <<mmu_alloc_roots>> return mmu_alloc_direct_roots(vcpu);
+ *
+ * 应该是分配ept的root的
+ */
 static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu_page *sp;
@@ -3470,6 +4091,10 @@ static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by (vcpu->arch.mmu.direct_map是false的情况):
+ *   - arch/x86/kvm/mmu.c|3624| <<mmu_alloc_roots>> return mmu_alloc_shadow_roots(vcpu);
+ */
 static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu_page *sp;
@@ -3571,12 +4196,24 @@ static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4739| <<kvm_mmu_load>> r = mmu_alloc_roots(vcpu);
+ */
 static int mmu_alloc_roots(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 设置direct_map的地方:
+	 *   - arch/x86/kvm/mmu.c|4003| <<nonpaging_init_context>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|4487| <<paging64_init_context_common>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4517| <<paging32_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4544| <<init_kvm_tdp_mmu>> context->direct_map = true;  ---------> ept似乎用这个!
+	 *   - arch/x86/kvm/mmu.c|4626| <<kvm_init_shadow_ept_mmu>> context->direct_map = false;
+	 */
 	if (vcpu->arch.mmu.direct_map)
-		return mmu_alloc_direct_roots(vcpu);
+		return mmu_alloc_direct_roots(vcpu);  // --> ept
 	else
-		return mmu_alloc_shadow_roots(vcpu);
+		return mmu_alloc_shadow_roots(vcpu);  // -->shadow
 }
 
 static void mmu_sync_roots(struct kvm_vcpu *vcpu)
@@ -3750,6 +4387,9 @@ static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 	return RET_PF_RETRY;
 }
 
+/*
+ * 对于因为write造成的error, 检查pfn对应的slot->arch.gfn_track[mode][index]
+ */
 static bool page_fault_handle_page_track(struct kvm_vcpu *vcpu,
 					 u32 error_code, gfn_t gfn)
 {
@@ -3764,6 +4404,11 @@ static bool page_fault_handle_page_track(struct kvm_vcpu *vcpu,
 	 * guest is writing the page which is write tracked which can
 	 * not be fixed by page fault handler.
 	 */
+	/*
+	 * 目前参数中的mode都是KVM_PAGE_TRACK_WRITE
+	 * 根据gfn获得kvm_memory_slot和这个gfn在slot中的index
+	 * 最后返回slot->arch.gfn_track[mode][index]
+	 */
 	if (kvm_page_track_is_active(vcpu, gfn, KVM_PAGE_TRACK_WRITE))
 		return true;
 
@@ -3834,6 +4479,20 @@ bool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)
 	return kvm_x86_ops->interrupt_allowed(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4003| <<tdp_page_fault>> if (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, write, &map_writable))
+ *   - arch/x86/kvm/mmu.c|3376| <<nonpaging_map>> if (try_async_pf(vcpu, prefault, gfn, v, &pfn, write, &map_writable))
+ *   - arch/x86/kvm/paging_tmpl.h|797| <<FNAME(page_fault)>> if (try_async_pf(vcpu, prefault, walker.gfn, addr, &pfn, write_fault,
+ *
+ * 1. 根据gfn找到对应的memslot
+ * 2. 用memslot的起始hva(userspace_addr)+(gfn-slot中的起始gfn(base_gfn))*页大小(PAGE_SIZE)
+ *    得到gfn对应的起始hva
+ * 3. 为该hva分配一个物理页,有hva_to_pfn_fast()和hva_to_pfn_slow()两种,hva_to_pfn_fast()
+ *    实际上是调用__get_user_pages_fast(),会尝试去pin该page,即确保该地址所在的物理页在内存中.
+ *    如果失败,退化到hva_to_pfn_slow(),会先去拿mm->mmap_sem的锁然后调用__get_user_pages()来pin
+ * 4. 如果分配成功,对其返回的struct page调用page_to_pfn()得到对应的pfn
+ */
 static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 			 gva_t gva, kvm_pfn_t *pfn, bool write, bool *writable)
 {
@@ -3848,8 +4507,14 @@ static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 		return false;
 	}
 
+	/*
+	 * 找到gfn对应的slot
+	 */
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
 	async = false;
+	/*
+	 * 找到gfn对应的pfn
+	 */
 	*pfn = __gfn_to_pfn_memslot(slot, gfn, false, &async, write, writable);
 	if (!async)
 		return false; /* *pfn has correct page already */
@@ -3868,6 +4533,11 @@ static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|7094| <<handle_exception>> return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
+ *   - arch/x86/kvm/svm.c|2644| <<pf_interception>> return kvm_handle_page_fault(&svm->vcpu, error_code, fault_address,
+ */
 int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 				u64 fault_address, char *insn, int insn_len)
 {
@@ -3910,6 +4580,25 @@ check_hugepage_cache_consistency(struct kvm_vcpu *vcpu, gfn_t gfn, int level)
 	return kvm_mtrr_check_gfn_range_consistency(vcpu, gfn, page_num);
 }
 
+/*
+ * used by:
+ *   - arch/x86/kvm/mmu.c|4521| <<init_kvm_tdp_mmu>> context->page_fault = tdp_page_fault
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5000| <<kvm_mmu_page_fault>> r = vcpu->arch.mmu.page_fault(vcpu, cr2, lower_32_bits(error_code),
+ *   - arch/x86/kvm/x86.c|9270| <<kvm_arch_async_page_ready>> vcpu->arch.mmu.page_fault(vcpu, work->gva, 0, true);
+ *
+ * 主要有两步:
+ * 1. 获取gpa对应的物理页,如果没有会进行分配
+ * 2. 更新ept
+ *
+ * 1. 首先根据gpa计算出标准页面情况下的gfn
+ * 2. 得到level, level就是影子页表中, level级页表作为影子页表,
+ *    其目录指向页面
+ * 3. 根据gfn和gpa, 在memslot中查找, 得到qemu中分配页面的HVA,
+ *    再通过__get_user_pages_fast()得到这个hva页面的pfn
+ * 4. __direct_map完成ept够造
+ */
 static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 			  bool prefault)
 {
@@ -3924,6 +4613,11 @@ static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 
 	MMU_WARN_ON(!VALID_PAGE(vcpu->arch.mmu.root_hpa));
 
+	/*
+	 * 对于因为write造成的error, 检查pfn对应的slot->arch.gfn_track[mode][index]
+	 *
+	 * 如果是true, 就要RET_PF_EMULATE???
+	 */
 	if (page_fault_handle_page_track(vcpu, error_code, gfn))
 		return RET_PF_EMULATE;
 
@@ -3947,6 +4641,9 @@ static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 	mmu_seq = vcpu->kvm->mmu_notifier_seq;
 	smp_rmb();
 
+	/*
+	 * 将gfn转换为pfn
+	 */
 	if (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, write, &map_writable))
 		return RET_PF_RETRY;
 
@@ -3960,6 +4657,9 @@ static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 		goto out_unlock;
 	if (likely(!force_pt_level))
 		transparent_hugepage_adjust(vcpu, &gfn, &pfn, &level);
+	/*
+	 * 更新ept, 将新的映射关系逐层添加到ept中
+	 */
 	r = __direct_map(vcpu, write, map_writable, level, gfn, pfn, prefault);
 	spin_unlock(&vcpu->kvm->mmu_lock);
 
@@ -4505,6 +5205,12 @@ static void paging32E_init_context(struct kvm_vcpu *vcpu,
 	paging64_init_context_common(vcpu, context, PT32E_ROOT_LEVEL);
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/mmu.c|4692| <<init_kvm_mmu>> init_kvm_tdp_mmu(vcpu);
+ *
+ * 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu)
+ */
 static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *context = &vcpu->arch.mmu;
@@ -4583,6 +5289,10 @@ void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_init_shadow_mmu);
 
+/*
+ * 只在nested_ept_init_mmu_context()被调用():
+ *   - arch/x86/kvm/vmx.c|11342| <<nested_ept_init_mmu_context>> kvm_init_shadow_ept_mmu(vcpu,
+ */
 void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 			     bool accessed_dirty)
 {
@@ -4666,11 +5376,18 @@ static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu)
 	update_last_nonleaf_level(vcpu, g_context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4682| <<kvm_mmu_reset_context>> init_kvm_mmu(vcpu);
+ *   - arch/x86/kvm/mmu.c|5114| <<kvm_mmu_setup>> init_kvm_mmu(vcpu);
+ *
+ * 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu)
+ */
 static void init_kvm_mmu(struct kvm_vcpu *vcpu)
 {
 	if (mmu_is_nested(vcpu))
 		init_kvm_nested_mmu(vcpu);
-	else if (tdp_enabled)
+	else if (tdp_enabled) // 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu)
 		init_kvm_tdp_mmu(vcpu);
 	else
 		init_kvm_softmmu(vcpu);
@@ -4683,6 +5400,11 @@ void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_reset_context);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|85| <<kvm_mmu_reload>> return kvm_mmu_load(vcpu);
+ *   - arch/x86/kvm/svm.c|3384| <<nested_svm_vmexit>> kvm_mmu_load(&svm->vcpu);
+ */
 int kvm_mmu_load(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -4695,12 +5417,27 @@ int kvm_mmu_load(struct kvm_vcpu *vcpu)
 	if (r)
 		goto out;
 	/* set_cr3() should ensure TLB has been flushed */
+	/*
+	 * intel : vmx_set_cr3()
+	 * amd   : svm_set_cr3()
+	 */
 	vcpu->arch.mmu.set_cr3(vcpu, vcpu->arch.mmu.root_hpa);
 out:
 	return r;
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_load);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5094| <<kvm_mmu_reset_context>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/mmu.c|6090| <<kvm_mmu_destroy>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/svm.c|3446| <<enter_svm_guest_mode>> kvm_mmu_unload(&svm->vcpu);
+ *   - arch/x86/kvm/vmx.c|9294| <<nested_vmx_eptp_switching>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/vmx.c|11341| <<nested_ept_init_mmu_context>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|7324| <<vcpu_enter_guest>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|8472| <<kvm_arch_vcpu_destroy>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|8863| <<kvm_unload_vcpu_mmu>> kvm_mmu_unload(vcpu);
+ */
 void kvm_mmu_unload(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_free_roots(vcpu);
@@ -4846,6 +5583,12 @@ static u64 *get_written_sptes(struct kvm_mmu_page *sp, gpa_t gpa, int *nspte)
 	return spte;
 }
 
+/*
+ * used by:
+ *   - arch/x86/kvm/mmu.c|5838| <<kvm_mmu_init_vm>> node->track_write = kvm_mmu_pte_write;
+ *
+ * 被page track使用作为track_write
+ */
 static void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 			      const u8 *new, int bytes,
 			      struct kvm_page_track_notifier_node *node)
@@ -4957,14 +5700,30 @@ static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3888| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+ *   - arch/x86/kvm/vmx.c|7702| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx.c|7734| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ *   - arch/x86/kvm/svm.c|2656| <<npf_interception>> return kvm_mmu_page_fault(&svm->vcpu, fault_address, error_code,
+ */
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t cr2, u64 error_code,
 		       void *insn, int insn_len)
 {
 	int r, emulation_type = 0;
 	enum emulation_result er;
+	/* struct kvm_vcpu_arch arch是struct kvm_cpu的一部分 */
 	bool direct = vcpu->arch.mmu.direct_map;
 
 	/* With shadow page tables, fault_address contains a GVA or nGPA.  */
+	/*
+	 * 设置direct_map的地方:
+	 *   - arch/x86/kvm/mmu.c|4003| <<nonpaging_init_context>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|4487| <<paging64_init_context_common>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4517| <<paging32_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|4544| <<init_kvm_tdp_mmu>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|4626| <<kvm_init_shadow_ept_mmu>> context->direct_map = false;
+	 */
 	if (vcpu->arch.mmu.direct_map) {
 		vcpu->arch.gpa_available = true;
 		vcpu->arch.gpa_val = cr2;
@@ -4977,7 +5736,15 @@ int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t cr2, u64 error_code,
 			goto emulate;
 	}
 
+	/*
+	 * struct kvm_vcpu_arch arch是struct kvm_cpu的一部分
+	 * struct kvm_mmu mmu是struct kvm_vcpu_arch arch的一部分
+	 */
+
 	if (r == RET_PF_INVALID) {
+		/*
+		 * 似乎是tdp_page_fault()??
+		 */
 		r = vcpu->arch.mmu.page_fault(vcpu, cr2, lower_32_bits(error_code),
 					      false);
 		WARN_ON(r == RET_PF_INVALID);
@@ -5049,6 +5816,11 @@ void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_invlpg);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|7870| <<vmx_enable_tdp>> kvm_enable_tdp();
+ *   - arch/x86/kvm/svm.c|1362| <<svm_hardware_setup>> kvm_enable_tdp();
+ */
 void kvm_enable_tdp(void)
 {
 	tdp_enabled = true;
@@ -5067,6 +5839,10 @@ static void free_mmu_pages(struct kvm_vcpu *vcpu)
 	free_page((unsigned long)vcpu->arch.mmu.lm_root);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5167| <<kvm_mmu_create>> return alloc_mmu_pages(vcpu);
+ */
 static int alloc_mmu_pages(struct kvm_vcpu *vcpu)
 {
 	struct page *page;
@@ -5088,6 +5864,9 @@ static int alloc_mmu_pages(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by only kvm_arch_vcpu_init()
+ */
 int kvm_mmu_create(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.walk_mmu = &vcpu->arch.mmu;
@@ -5098,10 +5877,18 @@ int kvm_mmu_create(struct kvm_vcpu *vcpu)
 	return alloc_mmu_pages(vcpu);
 }
 
+/*
+ * called by only:
+ *   - arch/x86/kvm/x86.c|8426| <<kvm_arch_vcpu_setup>> kvm_mmu_setup(vcpu);
+ *
+ * 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu)
+ */
 void kvm_mmu_setup(struct kvm_vcpu *vcpu)
 {
+	/* 如果root_hpa已经有效了就要warning! */
 	MMU_WARN_ON(VALID_PAGE(vcpu->arch.mmu.root_hpa));
 
+	/* 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu) */
 	init_kvm_mmu(vcpu);
 }
 
@@ -5413,6 +6200,12 @@ static void kvm_zap_obsolete_pages(struct kvm *kvm)
 }
 
 /*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5346| <<kvm_mmu_invalidate_zap_pages_in_memslot>> kvm_mmu_invalidate_zap_all_pages(kvm);
+ *   - arch/x86/kvm/mmu.c|5692| <<kvm_mmu_invalidate_mmio_sptes>> kvm_mmu_invalidate_zap_all_pages(kvm);
+ *   - arch/x86/kvm/x86.c|9234| <<kvm_arch_flush_shadow_all>> kvm_mmu_invalidate_zap_all_pages(kvm);
+ */
+/*
  * Fast invalidate all shadow pages and use lock-break technique
  * to zap obsolete pages.
  *
@@ -5447,6 +6240,10 @@ static bool kvm_has_zapped_obsolete_pages(struct kvm *kvm)
 	return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9125| <<kvm_arch_memslots_updated>> kvm_mmu_invalidate_mmio_sptes(kvm, slots);
+ */
 void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, struct kvm_memslots *slots)
 {
 	/*
@@ -5538,6 +6335,9 @@ static void mmu_destroy_caches(void)
 	kmem_cache_destroy(mmu_page_header_cache);
 }
 
+/*
+ * called only by kvm_arch_init()
+ */
 int kvm_mmu_module_init(void)
 {
 	int ret = -ENOMEM;
@@ -5573,6 +6373,13 @@ int kvm_mmu_module_init(void)
 /*
  * Caculate mmu pages needed for kvm.
  */
+/*
+ * called only by:
+ *   - arch/x86/kvm/x86.c|9200| <<kvm_arch_commit_memory_region>> nr_mmu_pages = kvm_mmu_calculate_mmu_pages(kvm);
+ *
+ * 核心思想是计算所有这个kvm虚拟机需要的slot中的page的总和
+ * 再除以50 (什么操作???)
+ */
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm)
 {
 	unsigned int nr_mmu_pages;
@@ -5588,6 +6395,9 @@ unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm)
 			nr_pages += memslot->npages;
 	}
 
+	/*
+	 * 相当于 除以50?? 什么操作?
+	 */
 	nr_mmu_pages = nr_pages * KVM_PERMILLE_MMU_PAGES / 1000;
 	nr_mmu_pages = max(nr_mmu_pages,
 			   (unsigned int) KVM_MIN_ALLOC_MMU_PAGES);
@@ -5595,6 +6405,11 @@ unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm)
 	return nr_mmu_pages;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8786| <<kvm_arch_vcpu_init>> kvm_mmu_destroy(vcpu);
+ *   - arch/x86/kvm/x86.c|8802| <<kvm_arch_vcpu_uninit>> kvm_mmu_destroy(vcpu);
+ */
 void kvm_mmu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_unload(vcpu);
@@ -5602,6 +6417,10 @@ void kvm_mmu_destroy(struct kvm_vcpu *vcpu)
 	mmu_free_memory_caches(vcpu);
 }
 
+/*
+ * called by only:
+ *   - arch/x86/kvm/x86.c|6672| <<kvm_arch_exit>> kvm_mmu_module_exit();
+ */
 void kvm_mmu_module_exit(void)
 {
 	mmu_destroy_caches();
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 5b408c0..7ede252 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -23,6 +23,7 @@
 #define PT_DIRTY_SHIFT 6
 #define PT_DIRTY_MASK (1ULL << PT_DIRTY_SHIFT)
 #define PT_PAGE_SIZE_SHIFT 7
+/* 1后面跟着7个0 */
 #define PT_PAGE_SIZE_MASK (1ULL << PT_PAGE_SIZE_SHIFT)
 #define PT_PAT_MASK (1ULL << 7)
 #define PT_GLOBAL_MASK (1ULL << 8)
@@ -77,6 +78,12 @@ static inline unsigned int kvm_mmu_available_pages(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|9303| <<nested_vmx_eptp_switching>> kvm_mmu_reload(vcpu);
+ *   - arch/x86/kvm/x86.c|7450| <<vcpu_enter_guest>> r = kvm_mmu_reload(vcpu); --> 进入guest mode前更新eptp
+ *   - arch/x86/kvm/x86.c|9309| <<kvm_arch_async_page_ready>> r = kvm_mmu_reload(vcpu);
+ */
 static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
 {
 	if (likely(vcpu->arch.mmu.root_hpa != INVALID_PAGE))
diff --git a/arch/x86/kvm/page_track.c b/arch/x86/kvm/page_track.c
index 3052a59..7ba90ea 100644
--- a/arch/x86/kvm/page_track.c
+++ b/arch/x86/kvm/page_track.c
@@ -21,11 +21,22 @@
 
 #include "mmu.h"
 
+/*
+ * called by:
+ *   - arch/x86/kvm/page_track.c|57| <<kvm_page_track_create_memslot>> kvm_page_track_free_memslot(slot, NULL);
+ *   - arch/x86/kvm/x86.c|9019| <<kvm_arch_free_memslot>> kvm_page_track_free_memslot(free, dont);
+ *
+ * 对于每一个(其实就KVM_PAGE_TRACK_WRITE)如果dont为NULL或者free->arch.gfn_track[i]不等于dont->arch.gfn_track[i]
+ * 则释放free->arch.gfn_track[i]
+ */
 void kvm_page_track_free_memslot(struct kvm_memory_slot *free,
 				 struct kvm_memory_slot *dont)
 {
 	int i;
 
+	/*
+	 * 目前只支持KVM_PAGE_TRACK_WRITE
+	 */
 	for (i = 0; i < KVM_PAGE_TRACK_MAX; i++)
 		if (!dont || free->arch.gfn_track[i] !=
 		      dont->arch.gfn_track[i]) {
@@ -34,6 +45,12 @@ void kvm_page_track_free_memslot(struct kvm_memory_slot *free,
 		}
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/x86.c|9092| <<kvm_arch_create_memslot>> if (kvm_page_track_create_memslot(slot, npages))
+ *
+ * 为每一个i(其实就KVM_PAGE_TRACK_WRITE), 分配slot->arch.gfn_track[i] (size是npages)
+ */
 int kvm_page_track_create_memslot(struct kvm_memory_slot *slot,
 				  unsigned long npages)
 {
@@ -50,10 +67,22 @@ int kvm_page_track_create_memslot(struct kvm_memory_slot *slot,
 	return 0;
 
 track_free:
+	/*
+	 * 对于每一个(其实就KVM_PAGE_TRACK_WRITE)
+	 * 释放slot->arch.gfn_track[i]
+	 */
 	kvm_page_track_free_memslot(slot, NULL);
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/page_track.c|118| <<kvm_slot_page_track_add_page>> if (WARN_ON(!page_track_mode_is_valid(mode)))
+ *   - arch/x86/kvm/page_track.c|152| <<kvm_slot_page_track_remove_page>> if (WARN_ON(!page_track_mode_is_valid(mode)))
+ *   - arch/x86/kvm/page_track.c|183| <<kvm_page_track_is_active>> if (WARN_ON(!page_track_mode_is_valid(mode)))
+ *
+ * 检查mode是否合规, 其实目前就KVM_PAGE_TRACK_WRITE
+ */
 static inline bool page_track_mode_is_valid(enum kvm_page_track_mode mode)
 {
 	if (mode < 0 || mode >= KVM_PAGE_TRACK_MAX)
@@ -62,13 +91,35 @@ static inline bool page_track_mode_is_valid(enum kvm_page_track_mode mode)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/page_track.c|121| <<kvm_slot_page_track_add_page>> update_gfn_track(slot, gfn, mode, 1);
+ *   - arch/x86/kvm/page_track.c|155| <<kvm_slot_page_track_remove_page>> update_gfn_track(slot, gfn, mode, -1);
+ *
+ * 在kvm_slot_page_track_add_page()和kvm_slot_page_track_remove_page()两个函数可能调用
+ * 通过gfn获取gfn在slot中对应的index
+ * 然后设置slot->arch.gfn_track[mode][index]
+ */
 static void update_gfn_track(struct kvm_memory_slot *slot, gfn_t gfn,
 			     enum kvm_page_track_mode mode, short count)
 {
 	int index, val;
 
+	/*
+	 * PT_PAGE_TABLE_LEVEL是 1
+	 *
+	 * base_gfn是基于4k开始的gfn
+	 * gfn是基于4k结束的gfn
+	 * 计算从开始到结束需要用到几个hugepage (或者普通page)
+	 *   level是1的时候hugepage大小是4K
+	 *   level是2的时候hugepage大小是2M
+	 *   level是3的时候hugepage大小是1G
+	 */
 	index = gfn_to_index(gfn, slot->base_gfn, PT_PAGE_TABLE_LEVEL);
 
+	/*
+	 * 目前mode就支持KVM_PAGE_TRACK_WRITE
+	 */
 	val = slot->arch.gfn_track[mode][index];
 
 	if (WARN_ON(val + count < 0 || val + count > USHRT_MAX))
@@ -89,14 +140,29 @@ static void update_gfn_track(struct kvm_memory_slot *slot, gfn_t gfn,
  * @gfn: the guest page.
  * @mode: tracking mode, currently only write track is supported.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1237| <<account_shadowed>> return kvm_slot_page_track_add_page(kvm, slot, gfn,
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1440| <<kvmgt_page_track_add>> kvm_slot_page_track_add_page(kvm, slot, gfn, KVM_PAGE_TRACK_WRITE);
+ *
+ * 通过gfn获取gfn在slot中对应的index, 然后设置slot->arch.gfn_track[mode][index] (增加1)
+ * 禁止这个gfn使用large page
+ * 最后把对应的page(pte)写保护了
+ */
 void kvm_slot_page_track_add_page(struct kvm *kvm,
 				  struct kvm_memory_slot *slot, gfn_t gfn,
 				  enum kvm_page_track_mode mode)
 {
 
+	/* 检查mode是否合规, 其实目前就KVM_PAGE_TRACK_WRITE */
 	if (WARN_ON(!page_track_mode_is_valid(mode)))
 		return;
 
+	/*
+	 * 在kvm_slot_page_track_add_page()和kvm_slot_page_track_remove_page()两个函数可能调用
+	 * 通过gfn获取gfn在slot中对应的index
+	 * 然后设置slot->arch.gfn_track[mode][index] (增加1)
+	 */
 	update_gfn_track(slot, gfn, mode, 1);
 
 	/*
@@ -105,6 +171,10 @@ void kvm_slot_page_track_add_page(struct kvm *kvm,
 	 */
 	kvm_mmu_gfn_disallow_lpage(slot, gfn);
 
+	/*
+	 * 似乎就是如果是KVM_PAGE_TRACK_WRITE
+	 * 则要把对应的page(pte)写保护了
+	 */
 	if (mode == KVM_PAGE_TRACK_WRITE)
 		if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn))
 			kvm_flush_remote_tlbs(kvm);
@@ -124,6 +194,15 @@ EXPORT_SYMBOL_GPL(kvm_slot_page_track_add_page);
  * @gfn: the guest page.
  * @mode: tracking mode, currently only write track is supported.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1254| <<unaccount_shadowed>> return kvm_slot_page_track_remove_page(kvm, slot, gfn,
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1474| <<kvmgt_page_track_remove>> kvm_slot_page_track_remove_page(kvm, slot, gfn, KVM_PAGE_TRACK_WRITE);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1508| <<kvmgt_page_track_flush_slot>> kvm_slot_page_track_remove_page(kvm, slot, gfn,
+ *
+ * 通过gfn获取gfn在slot中对应的index, 然后设置slot->arch.gfn_track[mode][index] (增加1)
+ * 禁止这个gfn使用large page
+ */
 void kvm_slot_page_track_remove_page(struct kvm *kvm,
 				     struct kvm_memory_slot *slot, gfn_t gfn,
 				     enum kvm_page_track_mode mode)
@@ -131,6 +210,11 @@ void kvm_slot_page_track_remove_page(struct kvm *kvm,
 	if (WARN_ON(!page_track_mode_is_valid(mode)))
 		return;
 
+	/*
+	 * 在kvm_slot_page_track_add_page()和kvm_slot_page_track_remove_page()两个函数可能调用
+	 * 通过gfn获取gfn在slot中对应的index
+	 * 然后设置slot->arch.gfn_track[mode][index] (减少1)
+	 */
 	update_gfn_track(slot, gfn, mode, -1);
 
 	/*
@@ -144,6 +228,15 @@ EXPORT_SYMBOL_GPL(kvm_slot_page_track_remove_page);
 /*
  * check if the corresponding access on the specified guest page is tracked.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3195| <<mmu_need_write_protect>> if (kvm_page_track_is_active(vcpu, gfn, KVM_PAGE_TRACK_WRITE))
+ *   - arch/x86/kvm/mmu.c|4342| <<page_fault_handle_page_track>> if (kvm_page_track_is_active(vcpu, gfn, KVM_PAGE_TRACK_WRITE))
+ *
+ * 目前参数中的mode都是KVM_PAGE_TRACK_WRITE
+ * 根据gfn获得kvm_memory_slot和这个gfn在slot中的index
+ * 最后返回slot->arch.gfn_track[mode][index]
+ */
 bool kvm_page_track_is_active(struct kvm_vcpu *vcpu, gfn_t gfn,
 			      enum kvm_page_track_mode mode)
 {
@@ -157,10 +250,28 @@ bool kvm_page_track_is_active(struct kvm_vcpu *vcpu, gfn_t gfn,
 	if (!slot)
 		return false;
 
+	/*
+	 * PT_PAGE_TABLE_LEVEL是1
+	 *
+	 * base_gfn是基于4k开始的gfn
+	 * gfn是基于4k结束的gfn
+	 * 计算从开始到结束需要用到几个hugepage (或者普通page)
+	 *   level是1的时候hugepage大小是4K
+	 *   level是2的时候hugepage大小是2M
+	 *   level是3的时候hugepage大小是1G
+	 */
 	index = gfn_to_index(gfn, slot->base_gfn, PT_PAGE_TABLE_LEVEL);
+	/*
+	 * gfn_track在kvm_page_track_create_memslot()初始化第二维
+	 * 应该是每个slot中page的数量吧
+	 */
 	return !!READ_ONCE(slot->arch.gfn_track[mode][index]);
 }
 
+/*
+ * 只被以下调用:
+ *   - arch/x86/kvm/x86.c|8995| <<kvm_arch_destroy_vm>> kvm_page_track_cleanup(kvm);
+ */
 void kvm_page_track_cleanup(struct kvm *kvm)
 {
 	struct kvm_page_track_notifier_head *head;
@@ -169,6 +280,10 @@ void kvm_page_track_cleanup(struct kvm *kvm)
 	cleanup_srcu_struct(&head->track_srcu);
 }
 
+/*
+ * 只被以下调用:
+ *   - arch/x86/kvm/x86.c|8847| <<kvm_arch_init_vm>> kvm_page_track_init(kvm);
+ */
 void kvm_page_track_init(struct kvm *kvm)
 {
 	struct kvm_page_track_notifier_head *head;
@@ -182,6 +297,13 @@ void kvm_page_track_init(struct kvm *kvm)
  * register the notifier so that event interception for the tracked guest
  * pages can be received.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5840| <<kvm_mmu_init_vm>> kvm_page_track_register_notifier(kvm, node);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1575| <<kvmgt_guest_init>> kvm_page_track_register_notifier(kvm, &info->track_node);
+ *
+ * 把一个struct kvm_page_track_notifier_node加入kvm->arch.track_notifier_head链表
+ */
 void
 kvm_page_track_register_notifier(struct kvm *kvm,
 				 struct kvm_page_track_notifier_node *n)
@@ -200,6 +322,13 @@ EXPORT_SYMBOL_GPL(kvm_page_track_register_notifier);
  * stop receiving the event interception. It is the opposed operation of
  * kvm_page_track_register_notifier().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5847| <<kvm_mmu_uninit_vm>> kvm_page_track_unregister_notifier(kvm, node);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1591| <<kvmgt_guest_exit>> kvm_page_track_unregister_notifier(info->kvm, &info->track_node);
+ *
+ * 把一个struct kvm_page_track_notifier_node从kvm->arch.track_notifier_head链表移除
+ */
 void
 kvm_page_track_unregister_notifier(struct kvm *kvm,
 				   struct kvm_page_track_notifier_node *n)
@@ -222,6 +351,13 @@ EXPORT_SYMBOL_GPL(kvm_page_track_unregister_notifier);
  * The node should figure out if the written page is the one that node is
  * interested in by itself.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5004| <<emulator_write_phys>> kvm_page_track_write(vcpu, gpa, val, bytes);
+ *   - arch/x86/kvm/x86.c|5271| <<emulator_cmpxchg_emulated>> kvm_page_track_write(vcpu, gpa, new, bytes);
+ *
+ * 对于write emulate, 调用kvm_page_track_register_notifier()注册的函数
+ */
 void kvm_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 			  int bytes)
 {
@@ -235,6 +371,11 @@ void kvm_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 		return;
 
 	idx = srcu_read_lock(&head->track_srcu);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu.c|5838| <<kvm_mmu_init_vm>> node->track_write = kvm_mmu_pte_write;
+	 *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1573| <<kvmgt_guest_init>> info->track_node.track_write = kvmgt_page_track_write;
+	 */
 	hlist_for_each_entry_rcu(n, &head->track_notifier_list, node)
 		if (n->track_write)
 			n->track_write(vcpu, gpa, new, bytes, n);
@@ -248,6 +389,10 @@ void kvm_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
  * The node should figure out it has any write-protected pages in this slot
  * by itself.
  */
+/*
+ * 只被以下调用:
+ *   - arch/x86/kvm/x86.c|9244| <<kvm_arch_flush_shadow_memslot>> kvm_page_track_flush_slot(kvm, slot);
+ */
 void kvm_page_track_flush_slot(struct kvm *kvm, struct kvm_memory_slot *slot)
 {
 	struct kvm_page_track_notifier_head *head;
@@ -260,6 +405,11 @@ void kvm_page_track_flush_slot(struct kvm *kvm, struct kvm_memory_slot *slot)
 		return;
 
 	idx = srcu_read_lock(&head->track_srcu);
+	/*
+	 * 在两处被设置:
+	 *   - arch/x86/kvm/mmu.c|5839| <<kvm_mmu_init_vm>> node->track_flush_slot = kvm_mmu_invalidate_zap_pages_in_memslot;
+	 *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1574| <<kvmgt_guest_init>> info->track_node.track_flush_slot = kvmgt_page_track_flush_slot;
+	 */
 	hlist_for_each_entry_rcu(n, &head->track_notifier_list, node)
 		if (n->track_flush_slot)
 			n->track_flush_slot(kvm, slot, n);
diff --git a/arch/x86/kvm/pmu_intel.c b/arch/x86/kvm/pmu_intel.c
index 5ab4a36..9842174 100644
--- a/arch/x86/kvm/pmu_intel.c
+++ b/arch/x86/kvm/pmu_intel.c
@@ -67,6 +67,9 @@ static void global_ctrl_changed(struct kvm_pmu *pmu, u64 data)
 		reprogram_counter(pmu, bit);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.find_arch_event = intel_find_arch_event()
+ */
 static unsigned intel_find_arch_event(struct kvm_pmu *pmu,
 				      u8 event_select,
 				      u8 unit_mask)
@@ -85,6 +88,9 @@ static unsigned intel_find_arch_event(struct kvm_pmu *pmu,
 	return intel_arch_events[i].event_type;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.find_fixed_event = intel_find_fixed_event()
+ */
 static unsigned intel_find_fixed_event(int idx)
 {
 	if (idx >= ARRAY_SIZE(fixed_pmc_events))
@@ -94,6 +100,9 @@ static unsigned intel_find_fixed_event(int idx)
 }
 
 /* check if a PMC is enabled by comparing it with globl_ctrl bits. */
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.pmc_is_enabled = intel_pmc_is_enabled()
+ */
 static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -101,6 +110,9 @@ static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 	return test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.pmc_idx_to_pmc = intel_pmc_idx_to_pmc()
+ */
 static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
 	if (pmc_idx < INTEL_PMC_IDX_FIXED)
@@ -114,6 +126,9 @@ static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 }
 
 /* returns 0 if idx's corresponding MSR exists; otherwise returns 1. */
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.is_valid_msr_idx = intel_is_valid_msr_idx()
+ */
 static int intel_is_valid_msr_idx(struct kvm_vcpu *vcpu, unsigned idx)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -125,6 +140,9 @@ static int intel_is_valid_msr_idx(struct kvm_vcpu *vcpu, unsigned idx)
 		(fixed && idx >= pmu->nr_arch_fixed_counters);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.msr_idx_to_pmc = intel_msr_idx_to_pmc()
+ */
 static struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu,
 					    unsigned idx)
 {
@@ -142,6 +160,9 @@ static struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu,
 	return &counters[idx];
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.is_valid_msr = intel_is_valid_msr()
+ */
 static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -164,6 +185,9 @@ static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 	return ret;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.get_msr = intel_pmu_get_msr()
+ */
 static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -196,6 +220,9 @@ static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.set_msr = intel_pmu_set_msr()
+ */
 static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -254,6 +281,9 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.refresh = intel_pmu_refresh()
+ */
 static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -305,6 +335,9 @@ static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 		pmu->reserved_bits ^= HSW_IN_TX|HSW_IN_TX_CHECKPOINTED;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.init = intel_pmu_init()
+ */
 static void intel_pmu_init(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -323,6 +356,9 @@ static void intel_pmu_init(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.reset = intel_pmu_reset()
+ */
 static void intel_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -342,6 +378,9 @@ static void intel_pmu_reset(struct kvm_vcpu *vcpu)
 		pmu->global_ovf_ctrl = 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.pmu_ops = &intel_pmu_ops
+ */
 struct kvm_pmu_ops intel_pmu_ops = {
 	.find_arch_event = intel_find_arch_event,
 	.find_fixed_event = intel_find_fixed_event,
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index d0c3be3..ac24c0e 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -1294,6 +1294,10 @@ static struct vmcs_config {
 	struct nested_vmx_msrs nested;
 } vmcs_config;
 
+/*
+ * 获取capability:
+ *   - arch/x86/kvm/vmx.c|4523| <<setup_vmcs_config>> &vmx_capability.ept, &vmx_capability.vpid);
+ */
 static struct vmx_capability {
 	u32 ept;
 	u32 vpid;
@@ -1801,6 +1805,9 @@ static inline bool cpu_has_virtual_nmis(void)
 	return vmcs_config.pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.has_wbinvd_exit = cpu_has_vmx_wbinvd_exit()
+ */
 static inline bool cpu_has_vmx_wbinvd_exit(void)
 {
 	return vmcs_config.cpu_based_2nd_exec_ctrl &
@@ -1836,12 +1843,18 @@ static inline bool cpu_has_vmx_vmfunc(void)
 		SECONDARY_EXEC_ENABLE_VMFUNC;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.umip_emulated = vmx_umip_emulated()
+ */
 static bool vmx_umip_emulated(void)
 {
 	return vmcs_config.cpu_based_2nd_exec_ctrl &
 		SECONDARY_EXEC_DESC;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cpu_has_accelerated_tpr = report_flexpriority()
+ */
 static inline bool report_flexpriority(void)
 {
 	return flexpriority_enabled;
@@ -2448,6 +2461,9 @@ static u32 vmx_read_guest_seg_ar(struct vcpu_vmx *vmx, unsigned seg)
 	return *p;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.update_bp_intercept = update_exception_bitmap()
+ */
 static void update_exception_bitmap(struct kvm_vcpu *vcpu)
 {
 	u32 eb;
@@ -2745,6 +2761,9 @@ static unsigned long segment_base(u16 selector)
 }
 #endif
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.prepare_guest_switch = vmx_save_host_state()
+ */
 static void vmx_save_host_state(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -2920,6 +2939,9 @@ static void decache_tsc_multiplier(struct vcpu_vmx *vmx)
  * Switches to specified vcpu, until a matching vcpu_put(), but assumes
  * vcpu mutex is already taken.
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_load = vmx_vcpu_load()
+ */
 static void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3001,6 +3023,9 @@ static void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)
 		pi_set_sn(pi_desc);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_put = vmx_vcpu_put()
+ */
 static void vmx_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	vmx_vcpu_pi_put(vcpu);
@@ -3031,6 +3056,9 @@ static inline unsigned long nested_read_cr4(struct vmcs12 *fields)
 		(fields->cr4_read_shadow & fields->cr4_guest_host_mask);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_rflags = vmx_get_rflags()
+ */
 static unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)
 {
 	unsigned long rflags, save_rflags;
@@ -3048,6 +3076,9 @@ static unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)
 	return to_vmx(vcpu)->rflags;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_rflags = vmx_set_rflags()
+ */
 static void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 {
 	unsigned long old_rflags = vmx_get_rflags(vcpu);
@@ -3064,6 +3095,9 @@ static void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 		to_vmx(vcpu)->emulation_required = emulation_required(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_interrupt_shadow = vmx_get_interrupt_shadow()
+ */
 static u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)
 {
 	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
@@ -3077,6 +3111,9 @@ static u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)
 	return ret;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_interrupt_shadow = vmx_set_interrupt_shadow()
+ */
 static void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)
 {
 	u32 interruptibility_old = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
@@ -3093,6 +3130,9 @@ static void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)
 		vmcs_write32(GUEST_INTERRUPTIBILITY_INFO, interruptibility);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.skip_emulated_instruction = skip_emulated_instruction()
+ */
 static void skip_emulated_instruction(struct kvm_vcpu *vcpu)
 {
 	unsigned long rip;
@@ -3183,6 +3223,9 @@ static void vmx_clear_hlt(struct kvm_vcpu *vcpu)
 		vmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.queue_exception = vmx_queue_exception()
+ */
 static void vmx_queue_exception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3219,11 +3262,17 @@ static void vmx_queue_exception(struct kvm_vcpu *vcpu)
 	vmx_clear_hlt(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.rdtscp_supported = vmx_rdtscp_supported()
+ */
 static bool vmx_rdtscp_supported(void)
 {
 	return cpu_has_vmx_rdtscp();
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.invpcid_supported = vmx_invpcid_supported()
+ */
 static bool vmx_invpcid_supported(void)
 {
 	return cpu_has_vmx_invpcid() && enable_ept;
@@ -3284,6 +3333,9 @@ static void setup_msrs(struct vcpu_vmx *vmx)
 		vmx_update_msr_bitmap(&vmx->vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.read_l1_tsc_offset = vmx_read_l1_tsc_offset()
+ */
 static u64 vmx_read_l1_tsc_offset(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -3298,6 +3350,9 @@ static u64 vmx_read_l1_tsc_offset(struct kvm_vcpu *vcpu)
 /*
  * writes 'offset' into guest's timestamp counter offset register
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.write_tsc_offset = vmx_write_tsc_offset()
+ */
 static void vmx_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)
 {
 	if (is_guest_mode(vcpu)) {
@@ -3903,6 +3958,9 @@ static inline bool vmx_feature_control_msr_valid(struct kvm_vcpu *vcpu,
 	return !(val & ~valid_bits);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_msr_feature = vmx_get_msr_feature()
+ */
 static int vmx_get_msr_feature(struct kvm_msr_entry *msr)
 {
 	switch (msr->index) {
@@ -3922,6 +3980,9 @@ static int vmx_get_msr_feature(struct kvm_msr_entry *msr)
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_msr = vmx_get_msr()
+ */
 static int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4015,6 +4076,9 @@ static void vmx_leave_nested(struct kvm_vcpu *vcpu);
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_msr = vmx_set_msr()
+ */
 static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4203,6 +4267,9 @@ static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return ret;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cache_reg = vmx_cache_reg()
+ */
 static void vmx_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)
 {
 	__set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
@@ -4222,11 +4289,17 @@ static void vmx_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cpu_has_kvm_support = cpu_has_kvm_support()
+ */
 static __init int cpu_has_kvm_support(void)
 {
 	return cpu_has_vmx();
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.disabled_by_bios = vmx_disabled_by_bios()
+ */
 static __init int vmx_disabled_by_bios(void)
 {
 	u64 msr;
@@ -4264,6 +4337,9 @@ static void kvm_cpu_vmxon(u64 addr)
 			: "memory", "cc");
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hardware_enable = hardware_enable()
+ */
 static int hardware_enable(void)
 {
 	int cpu = raw_smp_processor_id();
@@ -4336,6 +4412,9 @@ static void kvm_cpu_vmxoff(void)
 	cr4_clear_bits(X86_CR4_VMXE);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hardware_disable = hardware_disable()
+ */
 static void hardware_disable(void)
 {
 	vmclear_local_loaded_vmcss();
@@ -4917,6 +4996,9 @@ static void enter_rmode(struct kvm_vcpu *vcpu)
 	kvm_mmu_reset_context(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_efer = vmx_set_efer()
+ */
 static void vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4981,11 +5063,17 @@ static inline void __vmx_flush_tlb(struct kvm_vcpu *vcpu, int vpid,
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.tlb_flush = vmx_flush_tlb()
+ */
 static void vmx_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 {
 	__vmx_flush_tlb(vcpu, to_vmx(vcpu)->vpid, invalidate_gpa);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.decache_cr0_guest_bits = vmx_decache_cr0_guest_bits()
+ */
 static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu)
 {
 	ulong cr0_guest_owned_bits = vcpu->arch.cr0_guest_owned_bits;
@@ -4994,6 +5082,9 @@ static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu)
 	vcpu->arch.cr0 |= vmcs_readl(GUEST_CR0) & cr0_guest_owned_bits;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.decache_cr3 = vmx_decache_cr3()
+ */
 static void vmx_decache_cr3(struct kvm_vcpu *vcpu)
 {
 	if (enable_unrestricted_guest || (enable_ept && is_paging(vcpu)))
@@ -5001,6 +5092,9 @@ static void vmx_decache_cr3(struct kvm_vcpu *vcpu)
 	__set_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.decache_cr4_guest_bits = vmx_decache_cr4_guest_bits()
+ */
 static void vmx_decache_cr4_guest_bits(struct kvm_vcpu *vcpu)
 {
 	ulong cr4_guest_owned_bits = vcpu->arch.cr4_guest_owned_bits;
@@ -5106,6 +5200,9 @@ static void ept_update_paging_mode_cr0(unsigned long *hw_cr0,
 		*hw_cr0 &= ~X86_CR0_WP;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_cr0 = vmx_set_cr0()
+ */
 static void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5144,6 +5241,9 @@ static void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 	vmx->emulation_required = emulation_required(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_tdp_level = get_ept_level()
+ */
 static int get_ept_level(struct kvm_vcpu *vcpu)
 {
 	if (cpu_has_vmx_ept_5levels() && (cpuid_maxphyaddr(vcpu) > 48))
@@ -5165,6 +5265,11 @@ static u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa)
 	return eptp;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_cr3 = vmx_set_cr3()
+ *
+ * called only by kvm_mmu_reload()-->kvm_mmu_load()
+ */
 static void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)
 {
 	unsigned long guest_cr3;
@@ -5186,6 +5291,9 @@ static void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)
 	vmcs_writel(GUEST_CR3, guest_cr3);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_cr4 = vmx_set_cr4()
+ */
 static int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 {
 	/*
@@ -5260,6 +5368,9 @@ static int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_segment = vmx_get_segment()
+ */
 static void vmx_get_segment(struct kvm_vcpu *vcpu,
 			    struct kvm_segment *var, int seg)
 {
@@ -5297,6 +5408,9 @@ static void vmx_get_segment(struct kvm_vcpu *vcpu,
 	var->g = (ar >> 15) & 1;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_segment_base = vmx_get_segment_base()
+ */
 static u64 vmx_get_segment_base(struct kvm_vcpu *vcpu, int seg)
 {
 	struct kvm_segment s;
@@ -5308,6 +5422,9 @@ static u64 vmx_get_segment_base(struct kvm_vcpu *vcpu, int seg)
 	return vmx_read_guest_seg_base(to_vmx(vcpu), seg);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_cpl = vmx_get_cpl()
+ */
 static int vmx_get_cpl(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5340,6 +5457,9 @@ static u32 vmx_segment_access_rights(struct kvm_segment *var)
 	return ar;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_segment = vmx_set_segment()
+ */
 static void vmx_set_segment(struct kvm_vcpu *vcpu,
 			    struct kvm_segment *var, int seg)
 {
@@ -5381,6 +5501,9 @@ static void vmx_set_segment(struct kvm_vcpu *vcpu,
 	vmx->emulation_required = emulation_required(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_cs_db_l_bits = vmx_get_cs_db_l_bits()
+ */
 static void vmx_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l)
 {
 	u32 ar = vmx_read_guest_seg_ar(to_vmx(vcpu), VCPU_SREG_CS);
@@ -5389,24 +5512,36 @@ static void vmx_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l)
 	*l = (ar >> 13) & 1;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_idt = vmx_get_idt()
+ */
 static void vmx_get_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 {
 	dt->size = vmcs_read32(GUEST_IDTR_LIMIT);
 	dt->address = vmcs_readl(GUEST_IDTR_BASE);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_idt = vmx_set_idt()
+ */
 static void vmx_set_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 {
 	vmcs_write32(GUEST_IDTR_LIMIT, dt->size);
 	vmcs_writel(GUEST_IDTR_BASE, dt->address);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_gdt = vmx_get_gdt()
+ */
 static void vmx_get_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 {
 	dt->size = vmcs_read32(GUEST_GDTR_LIMIT);
 	dt->address = vmcs_readl(GUEST_GDTR_BASE);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_gdt = vmx_set_gdt()
+ */
 static void vmx_set_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)
 {
 	vmcs_write32(GUEST_GDTR_LIMIT, dt->size);
@@ -5950,6 +6085,9 @@ static void vmx_update_msr_bitmap(struct kvm_vcpu *vcpu)
 	vmx->msr_bitmap_mode = mode;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_enable_apicv = vmx_get_enable_apicv()
+ */
 static bool vmx_get_enable_apicv(struct kvm_vcpu *vcpu)
 {
 	return enable_apicv;
@@ -6075,6 +6213,9 @@ static int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,
  * 2. If target vcpu isn't running(root mode), kick it to pick up the
  * interrupt from PIR in next vmentry.
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.deliver_posted_interrupt = vmx_deliver_posted_interrupt()
+ */
 static void vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6184,6 +6325,9 @@ static u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)
 	return pin_based_exec_ctrl;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.refresh_apicv_exec_ctrl = vmx_refresh_apicv_exec_ctrl()
+ */
 static void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6369,6 +6513,10 @@ static void vmx_compute_secondary_exec_control(struct vcpu_vmx *vmx)
 	vmx->secondary_exec_control = exec_control;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|7980| <<vmx_enable_tdp>> ept_set_mmio_spte_mask();
+ */
 static void ept_set_mmio_spte_mask(void)
 {
 	/*
@@ -6501,6 +6649,9 @@ static void vmx_vcpu_setup(struct vcpu_vmx *vmx)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_reset = vmx_vcpu_reset()
+ */
 static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6573,6 +6724,10 @@ static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 
 	if (cpu_has_vmx_tpr_shadow() && !init_event) {
 		vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
+		/*
+		 * This 64-bit field is the physical address of the 4-KByte
+		 * virtual APIC page.
+		 */
 		if (cpu_need_tpr_shadow(vcpu))
 			vmcs_write64(VIRTUAL_APIC_PAGE_ADDR,
 				     __pa(vcpu->arch.apic->regs));
@@ -6622,12 +6777,18 @@ static bool nested_exit_on_nmi(struct kvm_vcpu *vcpu)
 	return nested_cpu_has_nmi_exiting(get_vmcs12(vcpu));
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.enable_irq_window = enable_irq_window()
+ */
 static void enable_irq_window(struct kvm_vcpu *vcpu)
 {
 	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,
 		      CPU_BASED_VIRTUAL_INTR_PENDING);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.enable_nmi_window = enable_nmi_window()
+ */
 static void enable_nmi_window(struct kvm_vcpu *vcpu)
 {
 	if (!enable_vnmi ||
@@ -6640,6 +6801,9 @@ static void enable_nmi_window(struct kvm_vcpu *vcpu)
 		      CPU_BASED_VIRTUAL_NMI_PENDING);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_irq = vmx_inject_irq()
+ */
 static void vmx_inject_irq(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6669,6 +6833,9 @@ static void vmx_inject_irq(struct kvm_vcpu *vcpu)
 	vmx_clear_hlt(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_nmi = vmx_inject_nmi()
+ */
 static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6701,6 +6868,9 @@ static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 	vmx_clear_hlt(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_nmi_mask = vmx_get_nmi_mask()
+ */
 static bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6715,6 +6885,9 @@ static bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu)
 	return masked;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_nmi_mask = vmx_set_nmi_mask()
+ */
 static void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6735,6 +6908,9 @@ static void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.nmi_allowed = vmx_nmi_allowed()
+ */
 static int vmx_nmi_allowed(struct kvm_vcpu *vcpu)
 {
 	if (to_vmx(vcpu)->nested.nested_run_pending)
@@ -6749,6 +6925,9 @@ static int vmx_nmi_allowed(struct kvm_vcpu *vcpu)
 		   | GUEST_INTR_STATE_NMI));
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.interrupt_allowed = vmx_interrupt_allowed()
+ */
 static int vmx_interrupt_allowed(struct kvm_vcpu *vcpu)
 {
 	return (!to_vmx(vcpu)->nested.nested_run_pending &&
@@ -6757,6 +6936,9 @@ static int vmx_interrupt_allowed(struct kvm_vcpu *vcpu)
 			(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_tss_addr = vmx_set_tss_addr()
+ */
 static int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr)
 {
 	int ret;
@@ -6772,6 +6954,9 @@ static int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr)
 	return init_rmode_tss(kvm);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_identity_map_addr = vmx_set_identity_map_addr()
+ */
 static int vmx_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)
 {
 	to_kvm_vmx(kvm)->ept_identity_map_addr = ident_addr;
@@ -6856,12 +7041,18 @@ static void kvm_machine_check(void)
 #endif
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MCE_DURING_VMENTRY]
+ */
 static int handle_machine_check(struct kvm_vcpu *vcpu)
 {
 	/* already handled by vcpu_run */
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_EXCEPTION_NMI]
+ */
 static int handle_exception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6967,12 +7158,18 @@ static int handle_exception(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_EXTERNAL_INTERRUPT]
+ */
 static int handle_external_interrupt(struct kvm_vcpu *vcpu)
 {
 	++vcpu->stat.irq_exits;
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_TRIPLE_FAULT]
+ */
 static int handle_triple_fault(struct kvm_vcpu *vcpu)
 {
 	vcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;
@@ -6980,6 +7177,9 @@ static int handle_triple_fault(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_IO_INSTRUCTION]
+ */
 static int handle_io(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
@@ -7001,6 +7201,9 @@ static int handle_io(struct kvm_vcpu *vcpu)
 	return kvm_fast_pio(vcpu, size, port, in);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.patch_hypercall = vmx_patch_hypercall()
+ */
 static void
 vmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 {
@@ -7063,12 +7266,19 @@ static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)
 		return kvm_set_cr4(vcpu, val);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_GDTR_IDTR]
+ * kvm_vmx_exit_handlers[EXIT_REASON_LDTR_TR]
+ */
 static int handle_desc(struct kvm_vcpu *vcpu)
 {
 	WARN_ON(!(vcpu->arch.cr4 & X86_CR4_UMIP));
 	return emulate_instruction(vcpu, 0) == EMULATE_DONE;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_CR_ACCESS]
+ */
 static int handle_cr(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification, val;
@@ -7149,6 +7359,9 @@ static int handle_cr(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_DR_ACCESS]
+ */
 static int handle_dr(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
@@ -7213,15 +7426,24 @@ static int handle_dr(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_dr6 = vmx_get_dr6()
+ */
 static u64 vmx_get_dr6(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.dr6;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_dr6 = vmx_set_dr6()
+ */
 static void vmx_set_dr6(struct kvm_vcpu *vcpu, unsigned long val)
 {
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.sync_dirty_debug_regs = vmx_sync_dirty_debug_regs()
+ */
 static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
 {
 	get_debugreg(vcpu->arch.db[0], 0);
@@ -7235,16 +7457,25 @@ static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
 	vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_dr7 = vmx_set_dr7()
+ */
 static void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)
 {
 	vmcs_writel(GUEST_DR7, val);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_CPUID]
+ */
 static int handle_cpuid(struct kvm_vcpu *vcpu)
 {
 	return kvm_emulate_cpuid(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MSR_READ]
+ */
 static int handle_rdmsr(struct kvm_vcpu *vcpu)
 {
 	u32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];
@@ -7266,6 +7497,9 @@ static int handle_rdmsr(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MSR_WRITE]
+ */
 static int handle_wrmsr(struct kvm_vcpu *vcpu)
 {
 	struct msr_data msr;
@@ -7286,12 +7520,18 @@ static int handle_wrmsr(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_TPR_BELOW_THRESHOLD]
+ */
 static int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)
 {
 	kvm_apic_update_ppr(vcpu);
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_PENDING_INTERRUPT]
+ */
 static int handle_interrupt_window(struct kvm_vcpu *vcpu)
 {
 	vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
@@ -7303,21 +7543,33 @@ static int handle_interrupt_window(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_HLT]
+ */
 static int handle_halt(struct kvm_vcpu *vcpu)
 {
 	return kvm_emulate_halt(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMCALL]
+ */
 static int handle_vmcall(struct kvm_vcpu *vcpu)
 {
 	return kvm_emulate_hypercall(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_INVD]
+ */
 static int handle_invd(struct kvm_vcpu *vcpu)
 {
 	return emulate_instruction(vcpu, 0) == EMULATE_DONE;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_INVLPG]
+ */
 static int handle_invlpg(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
@@ -7326,6 +7578,9 @@ static int handle_invlpg(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_RDPMC]
+ */
 static int handle_rdpmc(struct kvm_vcpu *vcpu)
 {
 	int err;
@@ -7334,11 +7589,17 @@ static int handle_rdpmc(struct kvm_vcpu *vcpu)
 	return kvm_complete_insn_gp(vcpu, err);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_WBINVD]
+ */
 static int handle_wbinvd(struct kvm_vcpu *vcpu)
 {
 	return kvm_emulate_wbinvd(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_XSETBV]
+ */
 static int handle_xsetbv(struct kvm_vcpu *vcpu)
 {
 	u64 new_bv = kvm_read_edx_eax(vcpu);
@@ -7349,6 +7610,9 @@ static int handle_xsetbv(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_XSAVES]
+ */
 static int handle_xsaves(struct kvm_vcpu *vcpu)
 {
 	kvm_skip_emulated_instruction(vcpu);
@@ -7356,6 +7620,9 @@ static int handle_xsaves(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_XRSTORS]
+ */
 static int handle_xrstors(struct kvm_vcpu *vcpu)
 {
 	kvm_skip_emulated_instruction(vcpu);
@@ -7363,6 +7630,9 @@ static int handle_xrstors(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_APIC_ACCESS]
+ */
 static int handle_apic_access(struct kvm_vcpu *vcpu)
 {
 	if (likely(fasteoi)) {
@@ -7385,6 +7655,9 @@ static int handle_apic_access(struct kvm_vcpu *vcpu)
 	return emulate_instruction(vcpu, 0) == EMULATE_DONE;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_EOI_INDUCED]
+ */
 static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
@@ -7395,6 +7668,9 @@ static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_APIC_WRITE]
+ */
 static int handle_apic_write(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
@@ -7405,6 +7681,9 @@ static int handle_apic_write(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_TASK_SWITCH]
+ */
 static int handle_task_switch(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7470,6 +7749,15 @@ static int handle_task_switch(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 当Guest第一次访问某个页面时,由于没有GVA到GPA的映射,触发Guest OS的page fault.
+ * 于是Guest OS会建立对应的pte并修复好各级页表,最后访问对应的GPA.由于没有建立
+ * GPA到HVA的映射,于是触发EPT Violation,VMEXIT到KVM.  KVM在vmx_handle_exit中执
+ * 行kvm_vmx_exit_handlers[exit_reason],发现exit_reason是
+ * EXIT_REASON_EPT_VIOLATION,因此调用 handle_ept_violation.
+ *
+ * kvm_vmx_exit_handlers[EXIT_REASON_EPT_VIOLATION]
+ */
 static int handle_ept_violation(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
@@ -7489,9 +7777,14 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 			(exit_qualification & INTR_INFO_UNBLOCK_NMI))
 		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
 
+	/* 获取发生缺页的gpa */
 	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 	trace_kvm_page_fault(gpa, exit_qualification);
 
+	/*
+	 * 根据exit_qualification内容得到error_code, 可能是
+	 * read fault/write fault/fetch fault/ept page tableis not present
+	 */
 	/* Is it a read fault? */
 	error_code = (exit_qualification & EPT_VIOLATION_ACC_READ)
 		     ? PFERR_USER_MASK : 0;
@@ -7514,6 +7807,9 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_EPT_MISCONFIG]
+ */
 static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 {
 	gpa_t gpa;
@@ -7546,6 +7842,9 @@ static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_NMI_WINDOW]
+ */
 static int handle_nmi_window(struct kvm_vcpu *vcpu)
 {
 	WARN_ON_ONCE(!enable_vnmi);
@@ -7669,6 +7968,10 @@ static void wakeup_handler(void)
 	spin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/vmx.c|7960| <<hardware_setup>> vmx_enable_tdp();
+ */
 static void vmx_enable_tdp(void)
 {
 	kvm_mmu_set_mask_ptes(VMX_EPT_READABLE_MASK,
@@ -7682,6 +7985,12 @@ static void vmx_enable_tdp(void)
 	kvm_enable_tdp();
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hardware_setup = hardware_setup()
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|8637| <<kvm_arch_hardware_setup>> r = kvm_x86_ops->hardware_setup();
+ */
 static __init int hardware_setup(void)
 {
 	int r = -ENOMEM, i;
@@ -7814,6 +8123,9 @@ static __init int hardware_setup(void)
     return r;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hardware_unsetup = hardware_unsetup()
+ */
 static __exit void hardware_unsetup(void)
 {
 	int i;
@@ -7828,6 +8140,9 @@ static __exit void hardware_unsetup(void)
  * Indicate a busy-waiting vcpu in spinlock. We do not enable the PAUSE
  * exiting, so only get here on cpu with PAUSE-Loop-Exiting.
  */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_PAUSE_INSTRUCTION]
+ */
 static int handle_pause(struct kvm_vcpu *vcpu)
 {
 	if (!kvm_pause_in_guest(vcpu->kvm))
@@ -7848,23 +8163,36 @@ static int handle_nop(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MWAIT_INSTRUCTION]
+ */
 static int handle_mwait(struct kvm_vcpu *vcpu)
 {
 	printk_once(KERN_WARNING "kvm: MWAIT instruction emulated as NOP!\n");
 	return handle_nop(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_RDRAND]
+ * kvm_vmx_exit_handlers[EXIT_REASON_RDSEED]
+ */
 static int handle_invalid_op(struct kvm_vcpu *vcpu)
 {
 	kvm_queue_exception(vcpu, UD_VECTOR);
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MONITOR_TRAP_FLAG]
+ */
 static int handle_monitor_trap(struct kvm_vcpu *vcpu)
 {
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_MONITOR_INSTRUCTION]
+ */
 static int handle_monitor(struct kvm_vcpu *vcpu)
 {
 	printk_once(KERN_WARNING "kvm: MONITOR instruction emulated as NOP!\n");
@@ -8098,6 +8426,9 @@ static int enter_vmx_operation(struct kvm_vcpu *vcpu)
  * region. Consequently, VMCLEAR and VMPTRLD also do not verify that the their
  * argument is different from the VMXON pointer (which the spec says they do).
  */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMON]
+ */
 static int handle_vmon(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -8267,6 +8598,9 @@ static void free_nested(struct vcpu_vmx *vmx)
 }
 
 /* Emulate the VMXOFF instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMOFF]
+ */
 static int handle_vmoff(struct kvm_vcpu *vcpu)
 {
 	if (!nested_vmx_check_permission(vcpu))
@@ -8277,6 +8611,9 @@ static int handle_vmoff(struct kvm_vcpu *vcpu)
 }
 
 /* Emulate the VMCLEAR instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMCLEAR]
+ */
 static int handle_vmclear(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8313,12 +8650,18 @@ static int handle_vmclear(struct kvm_vcpu *vcpu)
 static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch);
 
 /* Emulate the VMLAUNCH instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMLAUNCH]
+ */
 static int handle_vmlaunch(struct kvm_vcpu *vcpu)
 {
 	return nested_vmx_run(vcpu, true);
 }
 
 /* Emulate the VMRESUME instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMRESUME]
+ */
 static int handle_vmresume(struct kvm_vcpu *vcpu)
 {
 
@@ -8477,6 +8820,9 @@ static int nested_vmx_check_vmcs12(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMREAD]
+ */
 static int handle_vmread(struct kvm_vcpu *vcpu)
 {
 	unsigned long field;
@@ -8520,6 +8866,9 @@ static int handle_vmread(struct kvm_vcpu *vcpu)
 }
 
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMWRITE]
+ */
 static int handle_vmwrite(struct kvm_vcpu *vcpu)
 {
 	unsigned long field;
@@ -8607,6 +8956,9 @@ static void set_current_vmptr(struct vcpu_vmx *vmx, gpa_t vmptr)
 }
 
 /* Emulate the VMPTRLD instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMPTRLD]
+ */
 static int handle_vmptrld(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8662,6 +9014,9 @@ static int handle_vmptrld(struct kvm_vcpu *vcpu)
 }
 
 /* Emulate the VMPTRST instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMPTRST]
+ */
 static int handle_vmptrst(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qual = vmcs_readl(EXIT_QUALIFICATION);
@@ -8686,6 +9041,9 @@ static int handle_vmptrst(struct kvm_vcpu *vcpu)
 }
 
 /* Emulate the INVEPT instruction */
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_INVEPT]
+ */
 static int handle_invept(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8748,6 +9106,9 @@ static int handle_invept(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_INVVPID]
+ */
 static int handle_invvpid(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8835,6 +9196,9 @@ static int handle_invvpid(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_PML_FULL]
+ */
 static int handle_pml_full(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
@@ -8860,6 +9224,9 @@ static int handle_pml_full(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_PREEMPTION_TIMER]
+ */
 static int handle_preemption_timer(struct kvm_vcpu *vcpu)
 {
 	kvm_lapic_expired_hv_timer(vcpu);
@@ -8947,6 +9314,9 @@ static int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_VMFUNC]
+ */
 static int handle_vmfunc(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8989,6 +9359,11 @@ static int handle_vmfunc(struct kvm_vcpu *vcpu)
  * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
  * to be done to userspace and return 0.
  */
+/*
+ * used by:
+ *   - arch/x86/kvm/vmx.c|9930| <<vmx_handle_exit>> && kvm_vmx_exit_handlers[exit_reason])
+ *   - arch/x86/kvm/vmx.c|9931| <<vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_reason](vcpu);
+ */
 static int (*const kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 	[EXIT_REASON_EXCEPTION_NMI]           = handle_exception,
 	[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,
@@ -9409,6 +9784,9 @@ static int nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason)
 	return 1;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_exit_info = vmx_get_exit_info()
+ */
 static void vmx_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)
 {
 	*info1 = vmcs_readl(EXIT_QUALIFICATION);
@@ -9634,6 +10012,9 @@ static void dump_vmcs(void)
  * The guest has exited.  See if we can fix it or if we need userspace
  * assistance.
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.handle_exit = vmx_handle_exit()
+ */
 static int vmx_handle_exit(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -9803,6 +10184,9 @@ static void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 		: "eax", "ebx", "ecx", "edx");
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.update_cr8_intercept = update_cr8_intercept()
+ */
 static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -9819,6 +10203,9 @@ static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 	vmcs_write32(TPR_THRESHOLD, irr);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_virtual_apic_mode = vmx_set_virtual_apic_mode()
+ */
 static void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 {
 	u32 sec_exec_control;
@@ -9862,6 +10249,9 @@ static void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 	vmx_update_msr_bitmap(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_apic_access_page_addr = vmx_set_apic_access_page_addr()
+ */
 static void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu, hpa_t hpa)
 {
 	if (!is_guest_mode(vcpu)) {
@@ -9870,6 +10260,9 @@ static void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu, hpa_t hpa)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hwapic_isr_update = vmx_hwapic_isr_update()
+ */
 static void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 {
 	u16 status;
@@ -9904,6 +10297,9 @@ static void vmx_set_rvi(int vector)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hwapic_irr_update = vmx_hwapic_irr_update()
+ */
 static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 {
 	/*
@@ -9918,6 +10314,9 @@ static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 		vmx_set_rvi(max_irr);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+ */
 static int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -9956,6 +10355,9 @@ static int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
 	return max_irr;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.load_eoi_exitmap = vmx_load_eoi_exitmap()
+ */
 static void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
 {
 	if (!kvm_vcpu_apicv_active(vcpu))
@@ -9967,6 +10369,9 @@ static void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
 	vmcs_write64(EOI_EXIT_BITMAP3, eoi_exit_bitmap[3]);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.apicv_post_state_restore = vmx_apicv_post_state_restore()
+ */
 static void vmx_apicv_post_state_restore(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -10005,6 +10410,9 @@ static void vmx_complete_atomic_exit(struct vcpu_vmx *vmx)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.handle_external_intr = vmx_handle_external_intr()
+ */
 static void vmx_handle_external_intr(struct kvm_vcpu *vcpu)
 {
 	u32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
@@ -10046,6 +10454,9 @@ static void vmx_handle_external_intr(struct kvm_vcpu *vcpu)
 }
 STACK_FRAME_NON_STANDARD(vmx_handle_external_intr);
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.has_emulated_msr = vmx_has_emulated_msr()
+ */
 static bool vmx_has_emulated_msr(int index)
 {
 	switch (index) {
@@ -10063,12 +10474,18 @@ static bool vmx_has_emulated_msr(int index)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.mpx_supported = vmx_mpx_supported()
+ */
 static bool vmx_mpx_supported(void)
 {
 	return (vmcs_config.vmexit_ctrl & VM_EXIT_CLEAR_BNDCFGS) &&
 		(vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_BNDCFGS);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.xsaves_supported = vmx_xsaves_supported()
+ */
 static bool vmx_xsaves_supported(void)
 {
 	return vmcs_config.cpu_based_2nd_exec_ctrl &
@@ -10179,6 +10596,9 @@ static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
 				  IDT_VECTORING_ERROR_CODE);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cancel_injection = vmx_cancel_injection()
+ */
 static void vmx_cancel_injection(struct kvm_vcpu *vcpu)
 {
 	__vmx_complete_interrupts(vcpu,
@@ -10227,6 +10647,9 @@ static void vmx_arm_hv_timer(struct kvm_vcpu *vcpu)
 	vmcs_write32(VMX_PREEMPTION_TIMER_VALUE, delta_tsc);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.run = vmx_vcpu_run()
+ */
 static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -10505,12 +10928,21 @@ static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 }
 STACK_FRAME_NON_STANDARD(vmx_vcpu_run);
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vm_alloc = vmx_vm_alloc()
+ *
+ * called only by:
+ *   - arch/x86/include/asm/kvm_host.h|1121| <<kvm_arch_alloc_vm>> return kvm_x86_ops->vm_alloc();
+ */
 static struct kvm *vmx_vm_alloc(void)
 {
 	struct kvm_vmx *kvm_vmx = vzalloc(sizeof(struct kvm_vmx));
 	return &kvm_vmx->kvm;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vm_free = vmx_vm_free()
+ */
 static void vmx_vm_free(struct kvm *kvm)
 {
 	vfree(to_kvm_vmx(kvm));
@@ -10545,6 +10977,9 @@ static void vmx_free_vcpu_nested(struct kvm_vcpu *vcpu)
        vcpu_put(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_free = vmx_free_vcpu()
+ */
 static void vmx_free_vcpu(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -10560,6 +10995,9 @@ static void vmx_free_vcpu(struct kvm_vcpu *vcpu)
 	kmem_cache_free(kvm_vcpu_cache, vmx);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_create = vmx_create_vcpu()
+ */
 static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 {
 	int err;
@@ -10664,6 +11102,9 @@ static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 #define L1TF_MSG_SMT "L1TF CPU bug present and SMT on, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/l1tf.html for details.\n"
 #define L1TF_MSG_L1D "L1TF CPU bug present and virtualization mitigation disabled, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/l1tf.html for details.\n"
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.vm_init = vmx_vm_init()
+ */
 static int vmx_vm_init(struct kvm *kvm)
 {
 	if (!ple_gap)
@@ -10695,6 +11136,9 @@ static int vmx_vm_init(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.check_processor_compatibility = vmx_check_processor_compat()
+ */
 static void __init vmx_check_processor_compat(void *rtn)
 {
 	struct vmcs_config vmcs_conf;
@@ -10710,6 +11154,9 @@ static void __init vmx_check_processor_compat(void *rtn)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_mt_mask = vmx_get_mt_mask()
+ */
 static u64 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 {
 	u8 cache;
@@ -10752,6 +11199,9 @@ static u64 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 	return (cache << VMX_EPT_MT_EPTE_SHIFT) | ipat;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_lpage_level = vmx_get_lpage_level()
+ */
 static int vmx_get_lpage_level(void)
 {
 	if (enable_ept && !cpu_has_vmx_ept_1g_page())
@@ -10824,6 +11274,9 @@ static void nested_vmx_cr_fixed1_bits_update(struct kvm_vcpu *vcpu)
 #undef cr4_fixed1_update
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cpuid_update = vmx_cpuid_update()
+ */
 static void vmx_cpuid_update(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -10844,6 +11297,9 @@ static void vmx_cpuid_update(struct kvm_vcpu *vcpu)
 		nested_vmx_cr_fixed1_bits_update(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_supported_cpuid = vmx_set_supported_cpuid()
+ */
 static void vmx_set_supported_cpuid(u32 func, struct kvm_cpuid_entry2 *entry)
 {
 	if (func == 1 && nested)
@@ -12281,6 +12737,9 @@ static void vmcs12_save_pending_event(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.check_nested_events = vmx_check_nested_events()
+ */
 static int vmx_check_nested_events(struct kvm_vcpu *vcpu, bool external_intr)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -12841,6 +13300,9 @@ static void nested_vmx_entry_failure(struct kvm_vcpu *vcpu,
 		to_vmx(vcpu)->nested.sync_shadow_vmcs = true;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.check_intercept = vmx_check_intercept()
+ */
 static int vmx_check_intercept(struct kvm_vcpu *vcpu,
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage)
@@ -12882,6 +13344,9 @@ static inline int u64_shl_div_u64(u64 a, unsigned int shift,
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.set_hv_timer = vmx_set_hv_timer()
+ */
 static int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc)
 {
 	struct vcpu_vmx *vmx;
@@ -12925,6 +13390,9 @@ static int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc)
 	return delta_tsc == 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cancel_hv_timer = vmx_cancel_hv_timer()
+ */
 static void vmx_cancel_hv_timer(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -12934,12 +13402,18 @@ static void vmx_cancel_hv_timer(struct kvm_vcpu *vcpu)
 }
 #endif
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.sched_in = vmx_sched_in()
+ */
 static void vmx_sched_in(struct kvm_vcpu *vcpu, int cpu)
 {
 	if (!kvm_pause_in_guest(vcpu->kvm))
 		shrink_ple_window(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.slot_enable_log_dirty = vmx_slot_enable_log_dirty()
+ */
 static void vmx_slot_enable_log_dirty(struct kvm *kvm,
 				     struct kvm_memory_slot *slot)
 {
@@ -12947,17 +13421,26 @@ static void vmx_slot_enable_log_dirty(struct kvm *kvm,
 	kvm_mmu_slot_largepage_remove_write_access(kvm, slot);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.slot_disable_log_dirty = vmx_slot_disable_log_dirty()
+ */
 static void vmx_slot_disable_log_dirty(struct kvm *kvm,
 				       struct kvm_memory_slot *slot)
 {
 	kvm_mmu_slot_set_dirty(kvm, slot);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.flush_log_dirty = vmx_flush_log_dirty()
+ */
 static void vmx_flush_log_dirty(struct kvm *kvm)
 {
 	kvm_flush_pml_buffers(kvm);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.write_log_dirty = vmx_write_pml_buffer()
+ */
 static int vmx_write_pml_buffer(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12;
@@ -12998,6 +13481,9 @@ static int vmx_write_pml_buffer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.enable_log_dirty_pt_masked = vmx_enable_log_dirty_pt_masked()
+ */
 static void vmx_enable_log_dirty_pt_masked(struct kvm *kvm,
 					   struct kvm_memory_slot *memslot,
 					   gfn_t offset, unsigned long mask)
@@ -13106,6 +13592,9 @@ static int pi_pre_block(struct kvm_vcpu *vcpu)
 	return (vcpu->pre_pcpu == -1);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.pre_block = vmx_pre_block()
+ */
 static int vmx_pre_block(struct kvm_vcpu *vcpu)
 {
 	if (pi_pre_block(vcpu))
@@ -13117,6 +13606,9 @@ static int vmx_pre_block(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.
+ */
 static void pi_post_block(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->pre_pcpu == -1)
@@ -13128,6 +13620,9 @@ static void pi_post_block(struct kvm_vcpu *vcpu)
 	local_irq_enable();
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.post_block = vmx_post_block()
+ */
 static void vmx_post_block(struct kvm_vcpu *vcpu)
 {
 	if (kvm_x86_ops->set_hv_timer)
@@ -13145,6 +13640,9 @@ static void vmx_post_block(struct kvm_vcpu *vcpu)
  * @set: set or unset PI
  * returns 0 on success, < 0 on failure
  */
+/*
+ * struct kvm_x86_ops vmx_x86_ops.update_pi_irte = vmx_update_pi_irte()
+ */
 static int vmx_update_pi_irte(struct kvm *kvm, unsigned int host_irq,
 			      uint32_t guest_irq, bool set)
 {
@@ -13226,6 +13724,9 @@ static int vmx_update_pi_irte(struct kvm *kvm, unsigned int host_irq,
 	return ret;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.setup_mce = vmx_setup_mce()
+ */
 static void vmx_setup_mce(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->arch.mcg_cap & MCG_LMCE_P)
@@ -13236,6 +13737,9 @@ static void vmx_setup_mce(struct kvm_vcpu *vcpu)
 			~FEATURE_CONTROL_LMCE;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.smi_allowed = vmx_smi_allowed()
+ */
 static int vmx_smi_allowed(struct kvm_vcpu *vcpu)
 {
 	/* we need a nested vmexit to enter SMM, postpone if run is pending */
@@ -13244,6 +13748,9 @@ static int vmx_smi_allowed(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.pre_enter_smm = vmx_pre_enter_smm()
+ */
 static int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -13258,6 +13765,9 @@ static int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.pre_leave_smm = vmx_pre_leave_smm()
+ */
 static int vmx_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -13280,11 +13790,18 @@ static int vmx_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.enable_smi_window = enable_smi_window()
+ */
 static int enable_smi_window(struct kvm_vcpu *vcpu)
 {
 	return 0;
 }
 
+/*
+ * used by:
+ *   - arch/x86/kvm/vmx.c|13506| <<vmx_init>> r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
+ */
 static struct kvm_x86_ops vmx_x86_ops __ro_after_init = {
 	.cpu_has_kvm_support = cpu_has_kvm_support,
 	.disabled_by_bios = vmx_disabled_by_bios,
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 97fcac3..9c889e5 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -2026,6 +2026,10 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v)
 				sizeof(vcpu->hv_clock.version));
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/x86.c|7325| <<vcpu_enter_guest>> r = kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -2280,6 +2284,9 @@ static void kvmclock_reset(struct kvm_vcpu *vcpu)
 static void kvm_vcpu_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 {
 	++vcpu->stat.tlb_flush;
+	/*
+	 * vmx: vmx_flush_tlb()
+	 */
 	kvm_x86_ops->tlb_flush(vcpu, invalidate_gpa);
 }
 
@@ -4759,9 +4766,15 @@ gpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva,
 }
 
 /* uses this to access any guest's mapped memory without checking CPL */
+/*
+ * called only by kvm_arch_vcpu_ioctl_translate()
+ */
 gpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,
 				struct x86_exception *exception)
 {
+	/*
+	 * 情况太多, 一种情况是paging64_gva_to_gpa()
+	 */
 	return vcpu->arch.walk_mmu->gva_to_gpa(vcpu, gva, 0, exception);
 }
 
@@ -6503,6 +6516,10 @@ static struct perf_guest_info_callbacks kvm_guest_cbs = {
 	.get_guest_ip		= kvm_get_guest_ip,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6625| <<kvm_arch_init>> kvm_set_mmio_spte_mask();
+ */
 static void kvm_set_mmio_spte_mask(void)
 {
 	u64 mask;
@@ -6738,6 +6755,11 @@ void kvm_vcpu_deactivate_apicv(struct kvm_vcpu *vcpu)
 	kvm_x86_ops->refresh_apicv_exec_ctrl(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|7501| <<handle_vmcall>> return kvm_emulate_hypercall(vcpu);
+ *   - arch/x86/kvm/svm.c|2856| <<vmmcall_interception>> return kvm_emulate_hypercall(&svm->vcpu);
+ */
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
 {
 	unsigned long nr, a0, a1, a2, a3, ret;
@@ -7288,6 +7310,10 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_reload_apic_access_page);
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7629| <<vcpu_run>> r = vcpu_enter_guest(vcpu);
+ */
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -7501,6 +7527,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_RELOAD;
 	}
 
+	/*
+	 * intel : vmx_x86_ops --> vmx_vcpu_run()
+	 * amd   : svm_x86_ops --> svm_vcpu_run()
+	 */
 	kvm_x86_ops->run(vcpu);
 
 	/*
@@ -7563,6 +7593,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		kvm_lapic_sync_from_vapic(vcpu);
 
 	vcpu->arch.gpa_available = false;
+	/*
+	 * intel : vmx_x86_ops --> vmx_handle_exit()
+	 * amd   : svm_x86_ops --> handle_exit()
+	 */
 	r = kvm_x86_ops->handle_exit(vcpu);
 	return r;
 
@@ -7574,6 +7608,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7631| <<vcpu_run>> r = vcpu_block(kvm, vcpu);
+ */
 static inline int vcpu_block(struct kvm *kvm, struct kvm_vcpu *vcpu)
 {
 	if (!kvm_arch_vcpu_runnable(vcpu) &&
@@ -7616,6 +7654,10 @@ static inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
 		!vcpu->arch.apf.halted);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7806| <<kvm_arch_vcpu_ioctl_run>> r = vcpu_run(vcpu);
+ */
 static int vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -7747,6 +7789,10 @@ static int complete_emulated_mmio(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2600| <<kvm_vcpu_ioctl>> r = kvm_arch_vcpu_ioctl_run(vcpu, vcpu->run);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 {
 	int r;
@@ -8208,6 +8254,10 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 /*
  * Translate a guest virtual address to a guest physical address.
  */
+/*
+ * called by KVM_TRANSLATE (vcpu dev fd):
+ *   - virt/kvm/kvm_main.c|2711| <<kvm_vcpu_ioctl>> r = kvm_arch_vcpu_ioctl_translate(vcpu, &tr);
+ */
 int kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,
 				    struct kvm_translation *tr)
 {
@@ -8357,6 +8407,10 @@ void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 	free_cpumask_var(wbinvd_dirty_mask);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2481| <<kvm_vm_ioctl_create_vcpu>> vcpu = kvm_arch_vcpu_create(kvm, id);
+ */
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 						unsigned int id)
 {
@@ -8367,16 +8421,24 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 		"kvm: SMP vm created on host with unstable TSC; "
 		"guest TSC will not be reliable\n");
 
+	/*
+	 * intel: vmx_x86_ops --> vmx_create_vcpu()
+	 */
 	vcpu = kvm_x86_ops->vcpu_create(kvm, id);
 
 	return vcpu;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2518| <<kvm_vm_ioctl_create_vcpu>> r = kvm_arch_vcpu_setup(vcpu);
+ */
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 {
 	kvm_vcpu_mtrr_init(vcpu);
 	vcpu_load(vcpu);
 	kvm_vcpu_reset(vcpu, false);
+	/* 主要是初始化mmu的各个field和函数指针(struct kvm_mmu *context = &vcpu->arch.mmu) */
 	kvm_mmu_setup(vcpu);
 	vcpu_put(vcpu);
 	return 0;
@@ -8599,10 +8661,18 @@ void kvm_arch_hardware_disable(void)
 	drop_user_return_notifiers();
 }
 
+/*
+ * called only by:
+ *   - virt/kvm/kvm_main.c|4084| <<kvm_init>> r = kvm_arch_hardware_setup();  <---一个调用者是vmx_init()
+ */
 int kvm_arch_hardware_setup(void)
 {
 	int r;
 
+	/*
+	 * intel : hardware_setup() in arch/x86/kvm/vmx.c
+	 * amd   : svm_hardware_setup()
+	 */
 	r = kvm_x86_ops->hardware_setup();
 	if (r != 0)
 		return r;
@@ -8746,6 +8816,10 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 	kvm_x86_ops->sched_in(vcpu, cpu);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|638| <<kvm_create_vm>> r = kvm_arch_init_vm(kvm, type);
+ */
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
 	if (type)
@@ -8777,6 +8851,10 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm_page_track_init(kvm);
 	kvm_mmu_init_vm(kvm);
 
+	/*
+	 * intel: vmx_vm_init()
+	 * amd  : avic_vm_init()
+	 */
 	if (kvm_x86_ops->vm_init)
 		return kvm_x86_ops->vm_init(kvm);
 
@@ -8820,6 +8898,12 @@ void kvm_arch_sync_events(struct kvm *kvm)
 	kvm_free_pit(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|5796| <<init_rmode_identity_map>> r = __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/vmx.c|5847| <<alloc_apic_access_page>> r = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|8919| <<x86_set_memory_region>> r = __x86_set_memory_region(kvm, id, gpa, size);
+ */
 int __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size)
 {
 	int i, r;
@@ -8872,6 +8956,15 @@ int __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size)
 }
 EXPORT_SYMBOL_GPL(__x86_set_memory_region);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm.c|1667| <<avic_init_access_page>> ret = x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx.c|6937| <<vmx_set_tss_addr>> ret = x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,
+ *   - arch/x86/kvm/x86.c|8914| <<x86_set_memory_region>> int x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size)
+ *   - arch/x86/kvm/x86.c|8934| <<kvm_arch_destroy_vm>> x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+ *   - arch/x86/kvm/x86.c|8935| <<kvm_arch_destroy_vm>> x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT, 0, 0);
+ *   - arch/x86/kvm/x86.c|8936| <<kvm_arch_destroy_vm>> x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);
+ */
 int x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size)
 {
 	int r;
@@ -8930,6 +9023,16 @@ void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 	kvm_page_track_free_memslot(free, dont);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1070| <<__kvm_set_memory_region>> if (kvm_arch_create_memslot(kvm, &new, npages))
+ *
+ * 核心思想是为参数的kvm_memory_slot初始化其arch
+ * 分配管理hugepage内存的元数据:
+ *     kvm_memory_slot->arch.rmap[i]
+ *     kvm_memory_slot->arch.lpage_info[i]
+ * 根据实际情况设置.disallow_lpage
+ */
 int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 			    unsigned long npages)
 {
@@ -8939,11 +9042,28 @@ int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 		struct kvm_lpage_info *linfo;
 		unsigned long ugfn;
 		int lpages;
+		/*
+		 * 如果全部执行, KVM_NR_PAGE_SIZES = 3
+		 * level 分别是 1, 2, 3
+		 */
 		int level = i + 1;
 
+		/*
+		 * base_gfn是基于4k开始的gfn
+		 * gfn是基于4k结束的gfn
+		 * 计算从开始到结束需要用到几个hugepage
+		 *   level是1的时候hugepage大小是4K
+		 *   level是2的时候hugepage大小是2M
+		 *   level是3的时候hugepage大小是1G
+		 */
 		lpages = gfn_to_index(slot->base_gfn + npages - 1,
 				      slot->base_gfn, level) + 1;
 
+		/*
+		 * arch是struct kvm_arch_memory_slot
+		 *
+		 * 为某个level分配需要的hugepage数目的kvm_rmap_head
+		 */
 		slot->arch.rmap[i] =
 			kvcalloc(lpages, sizeof(*slot->arch.rmap[i]),
 				 GFP_KERNEL);
@@ -8952,6 +9072,11 @@ int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 		if (i == 0)
 			continue;
 
+		/*
+		 * 为当前level分配需要的hugepage数目的kvm_lpage_info
+		 *
+		 * 下面赋值给slot->arch.lpage_info[i - 1]
+		 */
 		linfo = kvcalloc(lpages, sizeof(*linfo), GFP_KERNEL);
 		if (!linfo)
 			goto out_free;
@@ -9062,6 +9187,10 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 	}
 }
 
+/*
+ * called only by:
+ *   - virt/kvm/kvm_main.c|1128| <<__kvm_set_memory_region>> kvm_arch_commit_memory_region(kvm, mem, &old, &new, change);
+ */
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				const struct kvm_userspace_memory_region *mem,
 				const struct kvm_memory_slot *old,
@@ -9070,6 +9199,7 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 {
 	int nr_mmu_pages = 0;
 
+	/* 用作mmu的page?? */
 	if (!kvm->arch.n_requested_mmu_pages)
 		nr_mmu_pages = kvm_mmu_calculate_mmu_pages(kvm);
 
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4ee7bc5..ecee712 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -123,7 +123,13 @@ static inline bool is_error_page(struct page *page)
  * Architecture-independent vcpu->requests bit members
  * Bits 4-7 are reserved for more arch-independent bits.
  */
+/*
+ * 在vcpu_enter_guest()的时候调用kvm_vcpu_flush_tlb()
+ */
 #define KVM_REQ_TLB_FLUSH         (0 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在vcpu_enter_guest()的时候调用kvm_mmu_unload()
+ */
 #define KVM_REQ_MMU_RELOAD        (1 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_PENDING_TIMER     2
 #define KVM_REQ_UNHALT            3
@@ -440,6 +446,14 @@ struct kvm {
 	unsigned long mmu_notifier_seq;
 	long mmu_notifier_count;
 #endif
+	/*
+	 * 在以下修改:
+	 *   - arch/x86/kvm/paging_tmpl.h|999| <<FNAME(sync_page)>> vcpu->kvm->tlbs_dirty++;
+	 *   - arch/x86/kvm/paging_tmpl.h|1019| <<FNAME(sync_page)>> vcpu->kvm->tlbs_dirty++;
+	 *   - virt/kvm/kvm_main.c|278| <<kvm_flush_remote_tlbs>> cmpxchg(&kvm->tlbs_dirty, dirty_count, 0);
+	 *   - virt/kvm/kvm_main.c|263| <<kvm_flush_remote_tlbs>> long dirty_count = smp_load_acquire(&kvm->tlbs_dirty);
+	 *   - virt/kvm/kvm_main.c|379| <<kvm_mmu_notifier_invalidate_range_start>> need_tlb_flush |= kvm->tlbs_dirty;
+	 */
 	long tlbs_dirty;
 	struct list_head devices;
 	struct dentry *debugfs_dentry;
@@ -569,8 +583,15 @@ void kvm_exit(void);
 void kvm_get_kvm(struct kvm *kvm);
 void kvm_put_kvm(struct kvm *kvm);
 
+/*
+ * 利用srcu返回kvm->memslots[as_id]
+ * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+ */
 static inline struct kvm_memslots *__kvm_memslots(struct kvm *kvm, int as_id)
 {
+	/*
+	 * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+	 */
 	return srcu_dereference_check(kvm->memslots[as_id], &kvm->srcu,
 			lockdep_is_held(&kvm->slots_lock) ||
 			!refcount_read(&kvm->users_count));
@@ -588,6 +609,14 @@ static inline struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu)
 	return __kvm_memslots(vcpu->kvm, as_id);
 }
 
+/*
+ * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+ * struct kvm_memslots有以下两个
+ *     struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+ *     short id_to_index[KVM_MEM_SLOTS_NUM];
+ * 这个函数先把id通过id_to_index[]转换成index
+ * 然后用来索引并返回slots->memslots[index]
+ */
 static inline struct kvm_memory_slot *
 id_to_memslot(struct kvm_memslots *slots, int id)
 {
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index b20b751..c0c44ae 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -565,6 +565,10 @@ kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 	return 0;
 }
 
+/*
+ * called by KVM_IRQFD (每个vm的dev的ioctl):
+ *   - virt/kvm/kvm_main.c|3073| <<kvm_vm_ioctl>> r = kvm_irqfd(kvm, &data);
+ */
 int
 kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
 {
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index b1286c4..e6d947f 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -58,6 +58,10 @@ int kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin)
 	return irq_rt->chip[irqchip][pin];
 }
 
+/*
+ * called only by:
+ *   - virt/kvm/kvm_main.c|3130| <<kvm_vm_ioctl>> r = kvm_send_userspace_msi(kvm, &msi);
+ */
 int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct kvm_kernel_irq_routing_entry route;
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 8b47507..5a054eb 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -94,7 +94,7 @@ EXPORT_SYMBOL_GPL(halt_poll_ns_shrink);
 
 DEFINE_SPINLOCK(kvm_lock);
 static DEFINE_RAW_SPINLOCK(kvm_count_lock);
-LIST_HEAD(vm_list);
+LIST_HEAD(vm_list); // 挂载所有的struct kvm
 
 static cpumask_var_t cpus_hardware_enabled;
 static int kvm_usage_count;
@@ -254,6 +254,9 @@ bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req)
 }
 
 #ifndef CONFIG_HAVE_KVM_ARCH_TLB_FLUSH_ALL
+/*
+ * 应该就是让每个vcpu调用硬件的tlb flush吧
+ */
 void kvm_flush_remote_tlbs(struct kvm *kvm)
 {
 	/*
@@ -273,6 +276,10 @@ void kvm_flush_remote_tlbs(struct kvm *kvm)
 	 * kvm_make_all_cpus_request() reads vcpu->mode. We reuse that
 	 * barrier here.
 	 */
+	/*
+	 * 在vcpu_enter_guest()的时候调用kvm_vcpu_flush_tlb()
+	 * 应该就是调用硬件的tlb flush吧
+	 */
 	if (kvm_make_all_cpus_request(kvm, KVM_REQ_TLB_FLUSH))
 		++kvm->stat.remote_tlb_flush;
 	cmpxchg(&kvm->tlbs_dirty, dirty_count, 0);
@@ -280,8 +287,14 @@ void kvm_flush_remote_tlbs(struct kvm *kvm)
 EXPORT_SYMBOL_GPL(kvm_flush_remote_tlbs);
 #endif
 
+/*
+ * 让每个cpu在在vcpu_enter_guest()的时候调用kvm_mmu_unload()
+ */
 void kvm_reload_remote_mmus(struct kvm *kvm)
 {
+	/*
+	 * 在vcpu_enter_guest()的时候调用kvm_mmu_unload()
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MMU_RELOAD);
 }
 
@@ -518,6 +531,10 @@ static int kvm_init_mmu_notifier(struct kvm *kvm)
 
 #endif /* CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER */
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|659| <<kvm_create_vm>> struct kvm_memslots *slots = kvm_alloc_memslots();
+ */
 static struct kvm_memslots *kvm_alloc_memslots(void)
 {
 	int i;
@@ -617,9 +634,14 @@ static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   -  virt/kvm/kvm_main.c|3252| <<kvm_dev_ioctl_create_vm>> kvm = kvm_create_vm(type);
+ */
 static struct kvm *kvm_create_vm(unsigned long type)
 {
 	int r, i;
+	/* 分配的kvm包含在kvm_vmx中 */
 	struct kvm *kvm = kvm_arch_alloc_vm();
 
 	if (!kvm)
@@ -769,6 +791,9 @@ void kvm_put_kvm(struct kvm *kvm)
 EXPORT_SYMBOL_GPL(kvm_put_kvm);
 
 
+/*
+ * struct file_operations kvm_vm_fops.release = kvm_vm_release()
+ */
 static int kvm_vm_release(struct inode *inode, struct file *filp)
 {
 	struct kvm *kvm = filp->private_data;
@@ -904,6 +929,11 @@ static struct kvm_memslots *install_new_memslots(struct kvm *kvm,
  *
  * Must be called holding kvm->slots_lock for write.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8913| <<__x86_set_memory_region>> r = __kvm_set_memory_region(kvm, &m);
+ *   - virt/kvm/kvm_main.c|1080| <<kvm_set_memory_region>> r = __kvm_set_memory_region(kvm, mem);
+ */
 int __kvm_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region *mem)
 {
@@ -941,6 +971,30 @@ int __kvm_set_memory_region(struct kvm *kvm,
 	if (mem->guest_phys_addr + mem->memory_size < mem->guest_phys_addr)
 		goto out;
 
+	/*
+	 * 获得一个新的slot (struct kvm_memory_slot)
+	 *
+	 * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM]; // KVM_ADDRESS_SPACE_NUM最多可能就是2
+	 * struct kvm_memslots中有:
+	 *     struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+	 *     short id_to_index[KVM_MEM_SLOTS_NUM];
+	 *
+	 * __kvm_memslots():
+	 *   利用srcu返回kvm->memslots[as_id]
+	 *   struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+	 *
+	 * id来自qemu传进来的kvm_userspace_memory_region->slot
+	 *
+	 * id_to_memslot():
+	 *   struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+	 *   struct kvm_memslots有以下两个
+	 *       struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+	 *       short id_to_index[KVM_MEM_SLOTS_NUM];
+	 *   这个函数先把id通过id_to_index[]转换成index
+	 *   然后用来索引并返回slots->memslots[index]
+	 *
+	 * slot是struct kvm_memory_slot
+	 */
 	slot = id_to_memslot(__kvm_memslots(kvm, as_id), id);
 	base_gfn = mem->guest_phys_addr >> PAGE_SHIFT;
 	npages = mem->memory_size >> PAGE_SHIFT;
@@ -948,6 +1002,9 @@ int __kvm_set_memory_region(struct kvm *kvm,
 	if (npages > KVM_MEM_MAX_NR_PAGES)
 		goto out;
 
+	/*
+	 * new和old都是struct kvm_memory_slot
+	 */
 	new = old = *slot;
 
 	new.id = id;
@@ -955,10 +1012,22 @@ int __kvm_set_memory_region(struct kvm *kvm,
 	new.npages = npages;
 	new.flags = mem->flags;
 
+	/*
+	 * 就四种操作
+	 * KVM_MR_CREATE : create a new memory slot
+	 * KVM_MR_DELETE : delete an existing memory slot
+	 * KVM_MR_MOVE   : modify an existing memory slot
+	 * KVM_MR_FLAGS_ONLY
+	 */
+
 	if (npages) {
 		if (!old.npages)
 			change = KVM_MR_CREATE;
 		else { /* Modify an existing slot. */
+			/*
+			 * 修改的区域的HVA不同或者大小不同或者flag中的
+			 * KVM_MEM_READONLY标记不同，直接退出
+			 */
 			if ((mem->userspace_addr != old.userspace_addr) ||
 			    (npages != old.npages) ||
 			    ((new.flags ^ old.flags) & KVM_MEM_READONLY))
@@ -967,6 +1036,7 @@ int __kvm_set_memory_region(struct kvm *kvm,
 			if (base_gfn != old.base_gfn)
 				change = KVM_MR_MOVE;
 			else if (new.flags != old.flags)
+				/* 如果仅仅是flag不同,则仅修改标记,设置KVM_MR_FLAGS_ONLY标记 */
 				change = KVM_MR_FLAGS_ONLY;
 			else { /* Nothing to change. */
 				r = 0;
@@ -982,9 +1052,22 @@ int __kvm_set_memory_region(struct kvm *kvm,
 		new.flags = 0;
 	}
 
+	/*
+	 * 新分配的话change就是KVM_MR_CREATE
+	 */
+
+	/*
+	 * 这里只是做一个检查
+	 * 就像是注释说的: Check for overlaps
+	 */
 	if ((change == KVM_MR_CREATE) || (change == KVM_MR_MOVE)) {
 		/* Check for overlaps */
 		r = -EEXIST;
+		/*
+		 * __kvm_memslots():
+		 *   利用srcu返回kvm->memslots[as_id]
+		 *   struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+		 */
 		kvm_for_each_memslot(slot, __kvm_memslots(kvm, as_id)) {
 			if (slot->id == id)
 				continue;
@@ -1002,6 +1085,13 @@ int __kvm_set_memory_region(struct kvm *kvm,
 	if (change == KVM_MR_CREATE) {
 		new.userspace_addr = mem->userspace_addr;
 
+		/*
+		 * 核心思想是为参数的kvm_memory_slot初始化其arch 
+		 * 分配管理hugepage内存的元数据:
+		 *     kvm_memory_slot->arch.rmap[i]
+		 *     kvm_memory_slot->arch.lpage_info[i]
+		 * 根据实际情况设置.disallow_lpage
+		 */
 		if (kvm_arch_create_memslot(kvm, &new, npages))
 			goto out_free;
 	}
@@ -1017,6 +1107,7 @@ int __kvm_set_memory_region(struct kvm *kvm,
 		goto out_free;
 	memcpy(slots, __kvm_memslots(kvm, as_id), sizeof(struct kvm_memslots));
 
+	/* 如果是删除或者修改 */
 	if ((change == KVM_MR_DELETE) || (change == KVM_MR_MOVE)) {
 		slot = id_to_memslot(slots, id);
 		slot->flags |= KVM_MEMSLOT_INVALID;
@@ -1030,6 +1121,9 @@ int __kvm_set_memory_region(struct kvm *kvm,
 		 *	- gfn_to_hva (kvm_read_guest, gfn_to_pfn)
 		 *	- kvm_is_visible_gfn (mmu_check_roots)
 		 */
+		/*
+		 * flush影子页表中的条目?
+		 */
 		kvm_arch_flush_shadow_memslot(kvm, slot);
 
 		/*
@@ -1040,6 +1134,9 @@ int __kvm_set_memory_region(struct kvm *kvm,
 		slots = old_memslots;
 	}
 
+	/*
+	 * x86什么也不做
+	 */
 	r = kvm_arch_prepare_memory_region(kvm, &new, mem, change);
 	if (r)
 		goto out_slots;
@@ -1050,6 +1147,12 @@ int __kvm_set_memory_region(struct kvm *kvm,
 		memset(&new.arch, 0, sizeof(new.arch));
 	}
 
+	/*
+	 * Insert memslot and re-sort memslots based on their GFN,
+	 * so binary search could be used to lookup GFN.
+	 * Sorting algorithm takes advantage of having initially
+	 * sorted array and known changed memslot position.
+	 */
 	update_memslots(slots, &new);
 	old_memslots = install_new_memslots(kvm, as_id, slots);
 
@@ -1068,6 +1171,10 @@ int __kvm_set_memory_region(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(__kvm_set_memory_region);
 
+/*
+ * called by kvm_vm_ioctl() --> KVM_SET_USER_MEMORY_REGION --> kvm_vm_ioctl_set_memory_region():
+ *   - virt/kvm/kvm_main.c|1092| <<kvm_vm_ioctl_set_memory_region>> return kvm_set_memory_region(kvm, mem);
+ */
 int kvm_set_memory_region(struct kvm *kvm,
 			  const struct kvm_userspace_memory_region *mem)
 {
@@ -1080,6 +1187,10 @@ int kvm_set_memory_region(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(kvm_set_memory_region);
 
+/*
+ * kvm_vm_ioctl() --> KVM_SET_USER_MEMORY_REGION:
+ *   - virt/kvm/kvm_main.c|3006| <<kvm_vm_ioctl>> r = kvm_vm_ioctl_set_memory_region(kvm, &kvm_userspace_mem);
+ */
 static int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
 					  struct kvm_userspace_memory_region *mem)
 {
@@ -1235,6 +1346,11 @@ bool kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_is_visible_gfn);
 
+/*
+ * 根据gfn获取其在ept(tdp)中的page size (4K,2M还是1G)
+ *     根据gfn获取在qemu中对应的hva, 把hva转化为vm_area_struct
+ *     然后就知道vm_area_struct的page size了
+ */
 unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)
 {
 	struct vm_area_struct *vma;
@@ -1539,6 +1655,9 @@ kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
 			       bool atomic, bool *async, bool write_fault,
 			       bool *writable)
 {
+	/*
+	 * 计算gfn对应的其实hva
+	 */
 	unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);
 
 	if (addr == KVM_HVA_ERR_RO_BAD) {
@@ -1559,6 +1678,9 @@ kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
 		writable = NULL;
 	}
 
+	/*
+	 * 计算hva对应的pfn, 同时确保该物理页在内存中
+	 */
 	return hva_to_pfn(addr, atomic, async, write_fault,
 			  writable);
 }
@@ -2383,12 +2505,18 @@ static const struct vm_operations_struct kvm_vcpu_vm_ops = {
 	.fault = kvm_vcpu_fault,
 };
 
+/*
+ * struct file_operations kvm_vcpu_fops.mmap = kvm_vcpu_mmap()
+ */
 static int kvm_vcpu_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	vma->vm_ops = &kvm_vcpu_vm_ops;
 	return 0;
 }
 
+/*
+ * struct file_operations kvm_vcpu_fops.release = kvm_vcpu_release()
+ */
 static int kvm_vcpu_release(struct inode *inode, struct file *filp)
 {
 	struct kvm_vcpu *vcpu = filp->private_data;
@@ -2398,6 +2526,12 @@ static int kvm_vcpu_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - virt/kvm/kvm_main.c|2420| <<create_vcpu_fd>> return anon_inode_getfd(name, &kvm_vcpu_fops, vcpu, O_RDWR | O_CLOEXEC);
+ *
+ * vcpu的dev fd
+ */
 static struct file_operations kvm_vcpu_fops = {
 	.release        = kvm_vcpu_release,
 	.unlocked_ioctl = kvm_vcpu_ioctl,
@@ -2446,9 +2580,14 @@ static int kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
 /*
  * Creates some virtual cpus.  Good luck creating more than one.
  */
+/*
+ * called by KVM_CREATE_VCPU (每个vm的dev的ioctl):
+ *   - virt/kvm/kvm_main.c|3025| <<kvm_vm_ioctl>> r = kvm_vm_ioctl_create_vcpu(kvm, arg);
+ */
 static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)
 {
 	int r;
+	/* struct kvm有struct kvm_vcpu *vcpus[KVM_MAX_VCPUS]; */
 	struct kvm_vcpu *vcpu;
 
 	if (id >= KVM_MAX_VCPU_ID)
@@ -2531,6 +2670,11 @@ static int kvm_vcpu_ioctl_set_sigmask(struct kvm_vcpu *vcpu, sigset_t *sigset)
 	return 0;
 }
 
+/*
+ * struct file_operations kvm_vcpu_fops.unlocked_ioctl = kvm_vcpu_ioctl()
+ *
+ * vcpu的dev fd的ioctl
+ */
 static long kvm_vcpu_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -2550,6 +2694,7 @@ static long kvm_vcpu_ioctl(struct file *filp,
 	 * Some architectures have vcpu ioctls that are asynchronous to vcpu
 	 * execution; mutex_lock() would break them.
 	 */
+	/* x86可以忽略kvm_arch_vcpu_async_ioctl() */
 	r = kvm_arch_vcpu_async_ioctl(filp, ioctl, arg);
 	if (r != -ENOIOCTLCMD)
 		return r;
@@ -2739,6 +2884,9 @@ static long kvm_vcpu_ioctl(struct file *filp,
 }
 
 #ifdef CONFIG_KVM_COMPAT
+/*
+ * struct file_operations kvm_vcpu_fops.compat_ioctl = kvm_vcpu_compat_ioctl()
+ */
 static long kvm_vcpu_compat_ioctl(struct file *filp,
 				  unsigned int ioctl, unsigned long arg)
 {
@@ -2796,6 +2944,9 @@ static int kvm_device_ioctl_attr(struct kvm_device *dev,
 	return accessor(dev, &attr);
 }
 
+/*
+ * struct file_operations kvm_device_fops.unlocked_ioctl = kvm_device_ioctl()
+ */
 static long kvm_device_ioctl(struct file *filp, unsigned int ioctl,
 			     unsigned long arg)
 {
@@ -2816,6 +2967,9 @@ static long kvm_device_ioctl(struct file *filp, unsigned int ioctl,
 	}
 }
 
+/*
+ * struct file_operations kvm_device_fops.unlocked_ioctl = kvm_device_release()
+ */
 static int kvm_device_release(struct inode *inode, struct file *filp)
 {
 	struct kvm_device *dev = filp->private_data;
@@ -2825,6 +2979,10 @@ static int kvm_device_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - virt/kvm/kvm_main.c|2963| <<kvm_ioctl_create_device>> ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
+ */
 static const struct file_operations kvm_device_fops = {
 	.unlocked_ioctl = kvm_device_ioctl,
 	.release = kvm_device_release,
@@ -2839,6 +2997,10 @@ struct kvm_device *kvm_device_from_filp(struct file *filp)
 	return filp->private_data;
 }
 
+/*
+ * 主要在如下使用:
+ *   - virt/kvm/kvm_main.c|2965| <<kvm_ioctl_create_device>> ops = kvm_device_ops_table[cd->type];
+ */
 static struct kvm_device_ops *kvm_device_ops_table[KVM_DEV_TYPE_MAX] = {
 #ifdef CONFIG_KVM_MPIC
 	[KVM_DEV_TYPE_FSL_MPIC_20]	= &kvm_mpic_ops,
@@ -2846,6 +3008,16 @@ static struct kvm_device_ops *kvm_device_ops_table[KVM_DEV_TYPE_MAX] = {
 #endif
 };
 
+/*
+ * called by:
+ *   - virt/kvm/vfio.c|441| <<kvm_vfio_ops_init>> return kvm_register_device_ops(&kvm_vfio_ops, KVM_DEV_TYPE_VFIO);  -----> 唯一x86会用的
+ *   - arch/powerpc/kvm/book3s.c|1015| <<kvmppc_book3s_init>> kvm_register_device_ops(&kvm_xive_ops, KVM_DEV_TYPE_XICS);
+ *   - arch/powerpc/kvm/book3s.c|1018| <<kvmppc_book3s_init>> kvm_register_device_ops(&kvm_xics_ops, KVM_DEV_TYPE_XICS);
+ *   - arch/s390/kvm/kvm-s390.c|427| <<kvm_arch_init>> return kvm_register_device_ops(&kvm_flic_ops, KVM_DEV_TYPE_FLIC);
+ *   - virt/kvm/arm/vgic/vgic-its.c|2576| <<kvm_vgic_register_its_device>> return kvm_register_device_ops(&kvm_arm_vgic_its_ops,
+ *   - virt/kvm/arm/vgic/vgic-kvm-device.c|273| <<kvm_register_vgic_device>> ret = kvm_register_device_ops(&kvm_arm_vgic_v2_ops,
+ *   - virt/kvm/arm/vgic/vgic-kvm-device.c|277| <<kvm_register_vgic_device>> ret = kvm_register_device_ops(&kvm_arm_vgic_v3_ops,
+ */
 int kvm_register_device_ops(struct kvm_device_ops *ops, u32 type)
 {
 	if (type >= ARRAY_SIZE(kvm_device_ops_table))
@@ -2858,12 +3030,21 @@ int kvm_register_device_ops(struct kvm_device_ops *ops, u32 type)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/vfio.c|446| <<kvm_vfio_ops_exit>> kvm_unregister_device_ops(KVM_DEV_TYPE_VFIO);
+ *   - virt/kvm/arm/vgic/vgic-v3.c|633| <<vgic_v3_probe>> kvm_unregister_device_ops(KVM_DEV_TYPE_ARM_VGIC_V2);
+ */
 void kvm_unregister_device_ops(u32 type)
 {
 	if (kvm_device_ops_table[type] != NULL)
 		kvm_device_ops_table[type] = NULL;
 }
 
+/*
+ * called by (KVM_CREATE_DEVICE):
+ *   - virt/kvm/kvm_main.c|3177| <<kvm_vm_ioctl>> r = kvm_ioctl_create_device(kvm, &cd);
+ */
 static int kvm_ioctl_create_device(struct kvm *kvm,
 				   struct kvm_create_device *cd)
 {
@@ -2953,6 +3134,11 @@ static long kvm_vm_ioctl_check_extension_generic(struct kvm *kvm, long arg)
 	return kvm_vm_ioctl_check_extension(kvm, arg);
 }
 
+/*
+ * struct file_operations kvm_vm_fops.unlocked_ioctl = kvm_vm_ioctl()
+ *
+ * 每个vm的dev的ioctl
+ */
 static long kvm_vm_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -3132,6 +3318,9 @@ struct compat_kvm_dirty_log {
 	};
 };
 
+/*
+ * struct file_operations kvm_vm_fops.compat_ioctl = kvm_vm_compat_ioctl()
+ */
 static long kvm_vm_compat_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -3163,6 +3352,20 @@ static long kvm_vm_compat_ioctl(struct file *filp,
 }
 #endif
 
+/*
+ * used by:
+ *   - virt/kvm/kvm_main.c|3227| <<kvm_dev_ioctl_create_vm>> file = anon_inode_getfile("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);
+ *
+ * 每个vm的dev
+ *
+ * 两个cpu的例子
+ * #sudo lsof -p 11857 | grep kvm
+ * qemu-syst 11857 root  mem       REG   0,11                 9671 anon_inode:kvm-vcpu:1 (stat: No such file or directory)
+ * qemu-syst 11857 root   12u      CHR 10,232         0t0    12510 /dev/kvm
+ * qemu-syst 11857 root   13u  a_inode   0,11           0     9671 kvm-vm
+ * qemu-syst 11857 root   16u  a_inode   0,11           0     9671 kvm-vcpu:0
+ * qemu-syst 11857 root   17u  a_inode   0,11           0     9671 kvm-vcpu:1
+ */
 static struct file_operations kvm_vm_fops = {
 	.release        = kvm_vm_release,
 	.unlocked_ioctl = kvm_vm_ioctl,
@@ -3170,6 +3373,9 @@ static struct file_operations kvm_vm_fops = {
 	KVM_COMPAT(kvm_vm_compat_ioctl),
 };
 
+/*
+ * called only by kvm_dev_ioctl() <-- /dev/kvm的接口
+ */
 static int kvm_dev_ioctl_create_vm(unsigned long type)
 {
 	int r;
@@ -3184,10 +3390,20 @@ static int kvm_dev_ioctl_create_vm(unsigned long type)
 	if (r < 0)
 		goto put_kvm;
 #endif
+	/*
+	 * 其实就是__alloc_fd(): allocate a file descriptor, mark it busy.
+	 */
 	r = get_unused_fd_flags(O_CLOEXEC);
 	if (r < 0)
 		goto put_kvm;
 
+	/*
+	 * creates a new file instance by hooking it up to an
+	 * anonymous inode, and a dentry that describe the "class"
+	 * of the file
+	 *
+	 * 这里把struct kvm列为private data
+	 */
 	file = anon_inode_getfile("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);
 	if (IS_ERR(file)) {
 		put_unused_fd(r);
@@ -3216,6 +3432,11 @@ static int kvm_dev_ioctl_create_vm(unsigned long type)
 	return r;
 }
 
+/*
+ * struct file_operations kvm_chardev_ops..unlocked_ioctl = kvm_dev_ioctl()
+ *
+ * /dev/kvm的接口
+ */
 static long kvm_dev_ioctl(struct file *filp,
 			  unsigned int ioctl, unsigned long arg)
 {
@@ -3256,12 +3477,24 @@ static long kvm_dev_ioctl(struct file *filp,
 	return r;
 }
 
+/*
+ * struct miscdevice kvm_dev.fops = kvm_chardev_ops
+ *
+ * /dev/kvm的接口
+ */
 static struct file_operations kvm_chardev_ops = {
 	.unlocked_ioctl = kvm_dev_ioctl,
 	.llseek		= noop_llseek,
 	KVM_COMPAT(kvm_dev_ioctl),
 };
 
+/*
+ * x86使用:
+ *   - virt/kvm/kvm_main.c|4044| <<kvm_init>> r = misc_register(&kvm_dev);
+ *   - virt/kvm/kvm_main.c|4086| <<kvm_exit>> misc_deregister(&kvm_dev);
+ *   - virt/kvm/kvm_main.c|3871| <<kvm_uevent_notify_change>> if (!kvm_dev.this_device || !kvm)
+ *   - virt/kvm/kvm_main.c|3912| <<kvm_uevent_notify_change>> kobject_uevent_env(&kvm_dev.this_device->kobj, KOBJ_CHANGE, env->envp);
+ */
 static struct miscdevice kvm_dev = {
 	KVM_MINOR,
 	"kvm",
@@ -3975,6 +4208,12 @@ static void kvm_sched_out(struct preempt_notifier *pn,
 	kvm_arch_vcpu_put(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|13506| <<vmx_init>> r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
+ *   - arch/x86/kvm/svm.c|7168| <<svm_init>> return kvm_init(&svm_x86_ops, sizeof(struct vcpu_svm),
+ *   - virt/kvm/arm/arm.c|1645| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
diff --git a/virt/kvm/vfio.c b/virt/kvm/vfio.c
index d99850c..ace37cf 100644
--- a/virt/kvm/vfio.c
+++ b/virt/kvm/vfio.c
@@ -184,6 +184,9 @@ static void kvm_vfio_update_coherency(struct kvm_device *dev)
 	mutex_unlock(&kv->lock);
 }
 
+/*
+ * called only by kvm_vfio_set_attr()
+ */
 static int kvm_vfio_set_group(struct kvm_device *dev, long attr, u64 arg)
 {
 	struct kvm_vfio *kv = dev->private;
@@ -332,6 +335,9 @@ static int kvm_vfio_set_group(struct kvm_device *dev, long attr, u64 arg)
 	return -ENXIO;
 }
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.set_attr = kvm_vfio_set_attr()
+ */
 static int kvm_vfio_set_attr(struct kvm_device *dev,
 			     struct kvm_device_attr *attr)
 {
@@ -343,6 +349,9 @@ static int kvm_vfio_set_attr(struct kvm_device *dev,
 	return -ENXIO;
 }
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.has_attr = kvm_vfio_has_attr()
+ */
 static int kvm_vfio_has_attr(struct kvm_device *dev,
 			     struct kvm_device_attr *attr)
 {
@@ -363,6 +372,9 @@ static int kvm_vfio_has_attr(struct kvm_device *dev,
 	return -ENXIO;
 }
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.destroy = kvm_vfio_destroy()
+ */
 static void kvm_vfio_destroy(struct kvm_device *dev)
 {
 	struct kvm_vfio *kv = dev->private;
@@ -387,6 +399,10 @@ static void kvm_vfio_destroy(struct kvm_device *dev)
 
 static int kvm_vfio_create(struct kvm_device *dev, u32 type);
 
+/*
+ * used by:
+ *   - virt/kvm/vfio.c|434| <<kvm_vfio_ops_init>> return kvm_register_device_ops(&kvm_vfio_ops, KVM_DEV_TYPE_VFIO);
+ */
 static struct kvm_device_ops kvm_vfio_ops = {
 	.name = "kvm-vfio",
 	.create = kvm_vfio_create,
@@ -395,6 +411,9 @@ static struct kvm_device_ops kvm_vfio_ops = {
 	.has_attr = kvm_vfio_has_attr,
 };
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.create = kvm_vfio_create()
+ */
 static int kvm_vfio_create(struct kvm_device *dev, u32 type)
 {
 	struct kvm_device *tmp;
@@ -417,11 +436,19 @@ static int kvm_vfio_create(struct kvm_device *dev, u32 type)
 	return 0;
 }
 
+/*
+ * called only by:
+ *   - virt/kvm/kvm_main.c|4187| <<kvm_init>> r = kvm_vfio_ops_init();
+ */
 int kvm_vfio_ops_init(void)
 {
 	return kvm_register_device_ops(&kvm_vfio_ops, KVM_DEV_TYPE_VFIO);
 }
 
+/*
+ * called only by:
+ *   - virt/kvm/kvm_main.c|4227| <<kvm_exit>> kvm_vfio_ops_exit();
+ */
 void kvm_vfio_ops_exit(void)
 {
 	kvm_unregister_device_ops(KVM_DEV_TYPE_VFIO);
-- 
2.7.4

