From c24deda94de0bd058ad2eb91e9f0b228f4d5eca6 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 12 Apr 2021 09:07:08 -0700
Subject: [PATCH 1/1] linux v5.11

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/kvm_host.h      | 236 ++++++++++++
 arch/x86/include/asm/pvclock-abi.h   |  20 +
 arch/x86/include/uapi/asm/kvm_para.h |   7 +
 arch/x86/kernel/cpu/hypervisor.c     |   4 +
 arch/x86/kernel/cpu/mshyperv.c       |   5 +
 arch/x86/kernel/hpet.c               |   4 +
 arch/x86/kernel/kvm.c                |   7 +
 arch/x86/kernel/kvmclock.c           | 202 ++++++++++
 arch/x86/kernel/pvclock.c            |  14 +
 arch/x86/kernel/smpboot.c            |   7 +
 arch/x86/kernel/tsc.c                |  35 ++
 arch/x86/kvm/cpuid.c                 |  13 +
 arch/x86/kvm/kvm_cache_regs.h        |  33 ++
 arch/x86/kvm/mmu/mmu.c               | 151 ++++++++
 arch/x86/kvm/mmu/mmu_internal.h      |  29 ++
 arch/x86/kvm/mmu/tdp_mmu.c           |   8 +
 arch/x86/kvm/svm/nested.c            |  11 +
 arch/x86/kvm/svm/svm.c               |  18 +
 arch/x86/kvm/vmx/capabilities.h      |   5 +
 arch/x86/kvm/vmx/nested.c            | 206 +++++++++-
 arch/x86/kvm/vmx/nested.h            |  14 +
 arch/x86/kvm/vmx/posted_intr.c       |  91 +++++
 arch/x86/kvm/vmx/posted_intr.h       |  45 +++
 arch/x86/kvm/vmx/vmcs12.c            |   7 +
 arch/x86/kvm/vmx/vmcs12.h            |  53 +++
 arch/x86/kvm/vmx/vmx.c               |  46 +++
 arch/x86/kvm/vmx/vmx.h               |  53 +++
 arch/x86/kvm/x86.c                   | 539 +++++++++++++++++++++++++++
 drivers/clocksource/hyperv_timer.c   |   8 +
 drivers/iommu/intel/iommu.c          |  20 +
 drivers/iommu/intel/irq_remapping.c  |   5 +
 drivers/iommu/intel/svm.c            |   7 +
 drivers/iommu/iommu.c                |  30 ++
 drivers/net/xen-netback/common.h     |  18 +
 drivers/net/xen-netback/rx.c         | 154 ++++++++
 drivers/nvme/host/core.c             |  14 +
 drivers/vfio/pci/vfio_pci_intrs.c    |  39 ++
 drivers/vfio/pci/vfio_pci_private.h  |   9 +
 drivers/vfio/vfio_iommu_type1.c      |  89 +++++
 drivers/xen/events/events_base.c     |  62 +++
 drivers/xen/events/events_internal.h |   4 +
 include/linux/clocksource.h          |  12 +
 include/linux/kvm_host.h             |  33 ++
 include/uapi/linux/kvm.h             |  12 +
 kernel/cpu.c                         |  12 +
 kernel/sched/core.c                  |   8 +
 virt/kvm/eventfd.c                   |   4 +
 virt/kvm/irqchip.c                   |   9 +
 virt/kvm/kvm_main.c                  | 266 +++++++++++++
 49 files changed, 2677 insertions(+), 1 deletion(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3d6616f6f6ef..0a91507d5aba 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -57,20 +57,50 @@
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2552| <<kvm_gen_update_masterclock>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2681| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|2767| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2776| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|4013| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4656| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5700| <<kvm_arch_vm_ioctl>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *   - arch/x86/kvm/x86.c|7682| <<kvm_hyperv_tsc_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|7745| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8867| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) { --> kvm_guest_time_update()
+ *   - arch/x86/kvm/x86.c|9121| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10242| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ */
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
+/*
+ * 在以下使用KVM_REQ_STEAL_UPDATE:
+ *   - arch/x86/kvm/x86.c|3212| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4010| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8867| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_STEAL_UPDATE, vcpu))
+ */
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
+/*
+ * 调用kvm_gen_update_masterclock()
+ */
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
 #define KVM_REQ_MCLOCK_INPROGRESS \
 	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|1992| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4037| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8865| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu)) --> kvm_gen_kvmclock_update()
+ */
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
 #define KVM_REQ_APIC_PAGE_RELOAD \
 	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
@@ -666,18 +696,74 @@ struct kvm_vcpu_arch {
 		struct gfn_to_pfn_cache cache;
 	} st;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|407| <<nested_prepare_vmcb_control>> svm->vcpu.arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|658| <<nested_svm_vmexit>> svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2222| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+	 *   - arch/x86/kvm/x86.c|2228| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|2352| <<adjust_tsc_offset_guest>> u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3257| <<kvm_set_msr_common>> u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3561| <<kvm_get_msr_common>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset :
+	 */
 	u64 l1_tsc_offset;
 	u64 tsc_offset;
 	u64 last_guest_tsc;
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_host_tsc:
+	 *   - arch/x86/kvm/x86.c|4426| <<kvm_arch_vcpu_put>> vcpu->arch.last_host_tsc = rdtsc();
+	 *   - arch/x86/kvm/x86.c|10627| <<kvm_arch_hardware_enable>> vcpu->arch.last_host_tsc = local_tsc;
+	 * 在以下使用kvm_vcpu_arch->last_host_tsc:
+	 *   - arch/x86/kvm/x86.c|4349| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
+	 *   - arch/x86/kvm/x86.c|4350| <<kvm_arch_vcpu_load>> rdtsc() - vcpu->arch.last_host_tsc;
+	 *   - arch/x86/kvm/x86.c|10575| <<kvm_arch_hardware_enable>> if (stable && vcpu->arch.last_host_tsc > local_tsc) {
+	 *   - arch/x86/kvm/x86.c|10577| <<kvm_arch_hardware_enable>> if (vcpu->arch.last_host_tsc > max_tsc)
+	 *   - arch/x86/kvm/x86.c|10578| <<kvm_arch_hardware_enable>> max_tsc = vcpu->arch.last_host_tsc;
+	 */
 	u64 last_host_tsc;
 	u64 tsc_offset_adjustment;
 	u64 this_tsc_nsec;
 	u64 this_tsc_write;
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2216| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3035| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4428| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2217| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|9534| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_mult:
+	 *   - arch/x86/kvm/x86.c|2166| <<kvm_set_tsc_khz>> &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2187| <<compute_guest_tsc>> vcpu->arch.virtual_tsc_mult,
+	 *   -arch/x86/kvm/x86.h|301| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
+	 */
 	u32 virtual_tsc_mult;
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2266| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/hyperv.c|1426| <<kvm_hv_get_msr>> data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
+	 *   - arch/x86/kvm/lapic.c|1552| <<__wait_lapic_expire>> do_div(delay_ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1572| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1577| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1662| <<start_sw_tscdeadline>> unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/vmx/nested.c|2173| <<vmx_start_preemption_timer>> if (vcpu->arch.virtual_tsc_khz == 0)
+	 *   - arch/x86/kvm/vmx/nested.c|2178| <<vmx_start_preemption_timer>> do_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/vmx/nested.c|4022| <<vmx_get_preemption_timer_value>> value = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2491| <<kvm_synchronize_tsc>> if (vcpu->arch.virtual_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2502| <<kvm_synchronize_tsc>> u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
+	 *   - arch/x86/kvm/x86.c|2520| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2565| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|5440| <<kvm_arch_vcpu_ioctl>> r = vcpu->arch.virtual_tsc_khz;
+	 */
 	u32 virtual_tsc_khz;
 	s64 ia32_tsc_adjust_msr;
 	u64 msr_ia32_power_ctl;
@@ -906,13 +992,53 @@ struct kvm_arch {
 	unsigned long n_requested_mmu_pages;
 	unsigned long n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
+	/*
+	 * 在以下使用kvm_mmu_page->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1698| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|1909| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *
+	 * 在以下使用kvm_arch->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1698| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|1909| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmu.c|5510| <<kvm_mmu_zap_all_fast>> kvm->arch.mmu_valid_gen = kvm->arch.mmu_valid_gen ? 0 : 1;
+	 */
 	u8 mmu_valid_gen;
+	/*
+	 * 在以下使用kvm_arch->mmu_page_hash:
+	 *   - arch/x86/kvm/mmu/mmu.c|1838| <<for_each_gfn_indirect_valid_sp>> for_each_valid_sp(_kvm, _sp, &(_kvm)->arch.mmu_page_hash[kvm_page_table_hashfn(_gfn)]) \
+	 *   - arch/x86/kvm/mmu/mmu.c|2068| <<kvm_mmu_get_page>> sp_list = &vcpu->kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)];
+	 */
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*
 	 * Hash table of struct kvm_mmu_page.
 	 */
+	/*
+	 * 在以下使用kvm_arch->active_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1690| <<kvm_mmu_alloc_page>> list_add(&sp->link, &vcpu->kvm->arch.active_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|2416| <<kvm_mmu_zap_oldest_mmu_pages>> if (list_empty(&kvm->arch.active_mmu_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|2420| <<kvm_mmu_zap_oldest_mmu_pages>> list_for_each_entry_safe(sp, tmp, &kvm->arch.active_mmu_pages, link) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5370| <<kvm_zap_obsolete_pages>> list_for_each_entry_safe_reverse(sp, node, &kvm->arch.active_mmu_pages, link) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5686| <<kvm_mmu_zap_all>> list_for_each_entry_safe(sp, node, &kvm->arch.active_mmu_pages, link) {
+	 *   - arch/x86/kvm/mmu/mmu_audit.c|89| <<walk_all_active_sps>> list_for_each_entry(sp, &kvm->arch.active_mmu_pages, link)
+	 *   - arch/x86/kvm/x86.c|10381| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
+	 */
 	struct list_head active_mmu_pages;
+	/*
+	 * 在以下使用kvm_arch->zapped_obsolete_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|5399| <<kvm_zap_obsolete_pages>> __kvm_mmu_prepare_zap_page(kvm, sp, &kvm->arch.zapped_obsolete_pages, &nr_zapped)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5410| <<kvm_zap_obsolete_pages>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|5458| <<kvm_has_zapped_obsolete_pages>> return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));
+	 *   - arch/x86/kvm/mmu/mmu.c|5764| <<mmu_shrink_scan>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+	 *   - arch/x86/kvm/x86.c|10382| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.zapped_obsolete_pages);
+	 */
 	struct list_head zapped_obsolete_pages;
+	/*
+	 * 在以下使用kvm_arch->lpage_disallowed_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|786| <<account_huge_nx_page>> list_add_tail(&sp->lpage_disallowed_link, &kvm->arch.lpage_disallowed_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|5996| <<kvm_recover_nx_lpages>> if (list_empty(&kvm->arch.lpage_disallowed_mmu_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|6004| <<kvm_recover_nx_lpages>> sp = list_first_entry(&kvm->arch.lpage_disallowed_mmu_pages,
+	 *   - arch/x86/kvm/x86.c|10383| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.lpage_disallowed_mmu_pages);
+	 */
 	struct list_head lpage_disallowed_mmu_pages;
 	struct kvm_page_track_notifier_node mmu_sp_tracker;
 	struct kvm_page_track_notifier_head track_notifier_head;
@@ -943,22 +1069,88 @@ struct kvm_arch {
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * system_time = kernel_ns + kvm->arch.kvmclock_offset
+	 * 其中kvmclock_offset的初始值为VM创建时的Host Boot Time的相反数,
+	 * 即令VM创建时的system_time为0,system_time表示Guest的Boot Time.
+	 * 同时,用户态可以通过KVM_SET_CLOCK ioctl来设置kvmclock的system_time值,
+	 * 从而修改kvmclock_offset,这就允许QEMU采用不同的初始化行为.
+	 *
+	 * 在以下设置kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/x86.c|5760| <<kvm_arch_vm_ioctl>> kvm->arch.kvmclock_offset += user_ns.clock - now_ns;
+	 *   - arch/x86/kvm/x86.c|10495| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	s64 kvmclock_offset;
 	raw_spinlock_t tsc_write_lock;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2271| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|2333| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|10422| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	u64 last_tsc_nsec;
 	u64 last_tsc_write;
 	u32 last_tsc_khz;
 	u64 cur_tsc_nsec;
 	u64 cur_tsc_write;
 	u64 cur_tsc_offset;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2311| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+	 *   - arch/x86/kvm/x86.c|2322| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+	 *   - arch/x86/kvm/x86.c|2340| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 */
 	u64 cur_tsc_generation;
+	/*
+	 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2145| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2160| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2317| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2319| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|2516| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	int nr_vcpus_matched_tsc;
 
 	spinlock_t pvclock_gtod_sync_lock;
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2521| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 * 在以下使用kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2156| <<kvm_track_tsc_matching>> if (ka->use_master_clock ||
+	 *   - arch/x86/kvm/x86.c|2162| <<kvm_track_tsc_matching>> ka->use_master_clock, gtod->clock.vclock_mode);
+	 *   - arch/x86/kvm/x86.c|2525| <<pvclock_update_vm_gtod_copy>> if (ka->use_master_clock)
+	 *   - arch/x86/kvm/x86.c|2529| <<pvclock_update_vm_gtod_copy>> trace_kvm_update_master_clock(ka->use_master_clock, vclock_mode,
+	 *   - arch/x86/kvm/x86.c|2569| <<get_kvmclock_ns>> if (!ka->use_master_clock) {
+	 *   - arch/x86/kvm/x86.c|2669| <<kvm_guest_time_update>> use_master_clock = ka->use_master_clock;
+	 *   - arch/x86/kvm/x86.c|4036| <<kvm_arch_vcpu_load>> if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
+	 *   - arch/x86/kvm/x86.c|5709| <<kvm_arch_vm_ioctl>> user_ns.flags = kvm->arch.use_master_clock ? KVM_CLOCK_TSC_STABLE : 0;
+	 */
 	bool use_master_clock;
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|2543| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|2605| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|2733| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 */
 	u64 master_kernel_ns;
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|2823| <<kvmclock_update_fn>> kvmclock_update_work);
+	 *   - arch/x86/kvm/x86.c|2838| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|2854| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|10500| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|10548| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|2848| <<kvmclock_sync_fn>> kvmclock_sync_work);
+	 *   - arch/x86/kvm/x86.c|2855| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|10166| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|10501| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|10547| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	struct delayed_work kvmclock_sync_work;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
@@ -972,7 +1164,18 @@ struct kvm_arch {
 	int audit_point;
 	#endif
 
+	/*
+	 * 在以下使用kvm_arch->backwards_tsc_observed:
+	 *   - arch/x86/kvm/x86.c|2724| <<pvclock_update_vm_gtod_copy>> && !ka->backwards_tsc_observed
+	 *   - arch/x86/kvm/x86.c|10608| <<kvm_arch_hardware_enable>> kvm->arch.backwards_tsc_observed = true;
+	 */
 	bool backwards_tsc_observed;
+	/*
+	 * 在以下使用kvm_arch->boot_vcpu_runs_old_kvmclock:
+	 *   - arch/x86/kvm/x86.c|2053| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+	 *   - arch/x86/kvm/x86.c|2056| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+	 *   - arch/x86/kvm/x86.c|2725| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+	 */
 	bool boot_vcpu_runs_old_kvmclock;
 	u32 bsp_vcpu_id;
 
@@ -1029,11 +1232,21 @@ struct kvm_arch {
 };
 
 struct kvm_vm_stat {
+	/*
+	 * 在以下使用kvm_vm_stat->mmu_shadow_zapped:
+	 *   - arch/x86/kvm/x86.c|235| <<global>> VM_STAT("mmu_shadow_zapped", mmu_shadow_zapped),
+	 *   - arch/x86/kvm/mmu/mmu.c|2330| <<__kvm_mmu_prepare_zap_page>> ++kvm->stat.mmu_shadow_zapped;
+	 */
 	ulong mmu_shadow_zapped;
 	ulong mmu_pte_write;
 	ulong mmu_pte_updated;
 	ulong mmu_pde_zapped;
 	ulong mmu_flooded;
+	/*
+	 * 在以下使用kvm_vm_stat->mmu_recycled:
+	 *   - arch/x86/kvm/x86.c|240| <<global>> VM_STAT("mmu_recycled", mmu_recycled),
+	 *   - arch/x86/kvm/mmu/mmu.c|2465| <<kvm_mmu_zap_oldest_mmu_pages>> kvm->stat.mmu_recycled += total_zapped;
+	 */
 	ulong mmu_recycled;
 	ulong mmu_cache_miss;
 	ulong mmu_unsync;
@@ -1050,6 +1263,22 @@ struct kvm_vcpu_stat {
 	u64 invlpg;
 
 	u64 exits;
+	/*
+	 * 在以下修改kvm_vcpu_stat->io_exits:
+	 *   - arch/x86/kvm/svm/svm.c|2077| <<io_interception>> ++svm->vcpu.stat.io_exits
+	 *   - arch/x86/kvm/vmx/vmx.c|4904| <<handle_io>> ++vcpu->stat.io_exits;
+	 *
+	 * vcpu_enter_guest()
+	 * -> kvm_x86_ops.handle_exit = vmx_handle_exit()
+	 *    -> kvm_vmx_exit_handlers[EXIT_REASON_IO_INSTRUCTION](vcpu) = handle_io()
+	 *       -> ++vcpu->stat.io_exits;
+	 *
+	 * vcpu_enter_guest()
+	 * -> kvm_x86_ops.handle_exit = handle_exit()
+	 *    -> svm_invoke_exit_handler()
+	 *       -> svm_exit_handlers[SVM_EXIT_IOIO](svm) = io_interception()
+	 *          -> ++svm->vcpu.stat.io_exits
+	 */
 	u64 io_exits;
 	u64 mmio_exits;
 	u64 signal_exits;
@@ -1639,6 +1868,13 @@ enum {
 #define HF_GIF_MASK		(1 << 0)
 #define HF_NMI_MASK		(1 << 3)
 #define HF_IRET_MASK		(1 << 4)
+/*
+ * 在以下使用HF_GUEST_MASK:
+ *   - arch/x86/kvm/kvm_cache_regs.h|159| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|164| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|174| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+ *   - arch/x86/kvm/x86.c|6994| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+ */
 #define HF_GUEST_MASK		(1 << 5) /* VCPU is in guest-mode */
 #define HF_SMM_MASK		(1 << 6)
 #define HF_SMM_INSIDE_NMI_MASK	(1 << 7)
diff --git a/arch/x86/include/asm/pvclock-abi.h b/arch/x86/include/asm/pvclock-abi.h
index 1436226efe3e..fb9222374bac 100644
--- a/arch/x86/include/asm/pvclock-abi.h
+++ b/arch/x86/include/asm/pvclock-abi.h
@@ -26,7 +26,13 @@
 struct pvclock_vcpu_time_info {
 	u32   version;
 	u32   pad0;
+	/*
+	 * host最后一次tsc的值(可能用软件的方式involve了offset和scaling)
+	 */
 	u64   tsc_timestamp;
+	/*
+	 * VM已经启动了的时间 (clocksource 1GHZ)
+	 */
 	u64   system_time;
 	u32   tsc_to_system_mul;
 	s8    tsc_shift;
@@ -40,6 +46,20 @@ struct pvclock_wall_clock {
 	u32   nsec;
 } __attribute__((__packed__));
 
+/*
+ * 在以下使用PVCLOCK_TSC_STABLE_BIT:
+ *   - arch/x86/include/asm/vdso/gettimeofday.h|227| <<vread_pvclock>> if (unlikely(!(pvti->flags & PVCLOCK_TSC_STABLE_BIT)))
+ *   - arch/x86/kernel/kvmclock.c|410| <<kvm_setup_vsyscall_timeinfo>> if (!(flags & PVCLOCK_TSC_STABLE_BIT))
+ *   - arch/x86/kernel/kvmclock.c|521| <<kvmclock_init>> pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
+ *   - arch/x86/kernel/kvmclock.c|532| <<kvmclock_init>> kvm_sched_clock_init(flags & PVCLOCK_TSC_STABLE_BIT);
+ *   - arch/x86/kernel/pvclock.c|85| <<pvclock_clocksource_read>> if ((valid_flags & PVCLOCK_TSC_STABLE_BIT) &&
+ *   - arch/x86/kernel/pvclock.c|86| <<pvclock_clocksource_read>> (flags & PVCLOCK_TSC_STABLE_BIT))
+ *   - arch/x86/kvm/hyperv.c|1039| <<compute_tsc_page_parameters>> if (!(hv_clock->flags & PVCLOCK_TSC_STABLE_BIT))
+ *   - arch/x86/kvm/x86.c|2736| <<kvm_guest_time_update>> pvclock_flags |= PVCLOCK_TSC_STABLE_BIT;
+ *   - arch/x86/xen/time.c|465| <<xen_setup_vsyscall_time_info>> if (!(ti->pvti.flags & PVCLOCK_TSC_STABLE_BIT)) {
+ *   - arch/x86/xen/time.c|513| <<xen_time_init>> if (pvti->flags & PVCLOCK_TSC_STABLE_BIT) {
+ *   - arch/x86/xen/time.c|514| <<xen_time_init>> pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
+ */
 #define PVCLOCK_TSC_STABLE_BIT	(1 << 0)
 #define PVCLOCK_GUEST_STOPPED	(1 << 1)
 /* PVCLOCK_COUNTS_FROM_ZERO broke ABI and can't be used anymore. */
diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index 950afebfba88..894bc545d00d 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -49,6 +49,13 @@
 #define MSR_KVM_WALL_CLOCK_NEW  0x4b564d00
 #define MSR_KVM_SYSTEM_TIME_NEW 0x4b564d01
 #define MSR_KVM_ASYNC_PF_EN 0x4b564d02
+/*
+ * 主要在以下使用MSR_KVM_STEAL_TIME:
+ *   - arch/x86/kernel/kvm.c|310| <<kvm_register_steal_time>> wrmsrl(MSR_KVM_STEAL_TIME, (slow_virt_to_phys(st) | KVM_MSR_ENABLED));
+ *   - arch/x86/kernel/kvm.c|424| <<kvm_disable_steal_time>> wrmsr(MSR_KVM_STEAL_TIME, 0, 0);
+ *   - arch/x86/kvm/x86.c|3197| <<kvm_set_msr_common>> case MSR_KVM_STEAL_TIME:
+ *   - arch/x86/kvm/x86.c|3524| <<kvm_get_msr_common>> case MSR_KVM_STEAL_TIME:
+ */
 #define MSR_KVM_STEAL_TIME  0x4b564d03
 #define MSR_KVM_PV_EOI_EN      0x4b564d04
 #define MSR_KVM_POLL_CONTROL	0x4b564d05
diff --git a/arch/x86/kernel/cpu/hypervisor.c b/arch/x86/kernel/cpu/hypervisor.c
index 553bfbfc3a1b..3f02ef8b251a 100644
--- a/arch/x86/kernel/cpu/hypervisor.c
+++ b/arch/x86/kernel/cpu/hypervisor.c
@@ -68,6 +68,10 @@ detect_hypervisor_vendor(void)
 		if (unlikely(nopv) && !(*p)->ignore_nopv)
 			continue;
 
+		/*
+		 * ms_hyperv_platform() 返回 HYPERV_CPUID_VENDOR_AND_MAX_FUNCTIONS
+		 * kvm_detect()
+		 */
 		pri = (*p)->detect();
 		if (pri > max_pri) {
 			max_pri = pri;
diff --git a/arch/x86/kernel/cpu/mshyperv.c b/arch/x86/kernel/cpu/mshyperv.c
index 43b54bef5448..ed53dfb4aa98 100644
--- a/arch/x86/kernel/cpu/mshyperv.c
+++ b/arch/x86/kernel/cpu/mshyperv.c
@@ -32,6 +32,11 @@
 #include <asm/nmi.h>
 #include <clocksource/hyperv_timer.h>
 
+/*
+ * "ENLIGHTENING" KVM HYPER-V EMULATION, FOSDEM 2019
+ * https://archive.fosdem.org/2019/schedule/event/vai_enlightening_kvm/attachments/slides/2860/export/events/attachments/vai_enlightening_kvm/slides/2860/vkuznets_fosdem2019_enlightening_kvm.pdf
+ */
+
 struct ms_hyperv_info ms_hyperv;
 EXPORT_SYMBOL_GPL(ms_hyperv);
 
diff --git a/arch/x86/kernel/hpet.c b/arch/x86/kernel/hpet.c
index 08651a4e6aa0..dca36e861950 100644
--- a/arch/x86/kernel/hpet.c
+++ b/arch/x86/kernel/hpet.c
@@ -854,6 +854,10 @@ static u64 read_hpet(struct clocksource *cs)
 }
 #endif
 
+/*
+ * 在以下使用clocksource_hpet:
+ *   - arch/x86/kernel/hpet.c|1013| <<hpet_enable>> clocksource_register_hz(&clocksource_hpet, (u32)hpet_freq);
+ */
 static struct clocksource clocksource_hpet = {
 	.name		= "hpet",
 	.rating		= 250,
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index 5e78e01ca3b4..abcdbabc1d09 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -307,6 +307,13 @@ static void kvm_register_steal_time(void)
 	if (!has_steal_clock)
 		return;
 
+	/*
+	 * 主要在以下使用MSR_KVM_STEAL_TIME:
+	 *   - arch/x86/kernel/kvm.c|310| <<kvm_register_steal_time>> wrmsrl(MSR_KVM_STEAL_TIME, (slow_virt_to_phys(st) | KVM_MSR_ENABLED));
+	 *   - arch/x86/kernel/kvm.c|424| <<kvm_disable_steal_time>> wrmsr(MSR_KVM_STEAL_TIME, 0, 0);
+	 *   - arch/x86/kvm/x86.c|3197| <<kvm_set_msr_common>> case MSR_KVM_STEAL_TIME:
+	 *   - arch/x86/kvm/x86.c|3524| <<kvm_get_msr_common>> case MSR_KVM_STEAL_TIME:
+	 */
 	wrmsrl(MSR_KVM_STEAL_TIME, (slow_virt_to_phys(st) | KVM_MSR_ENABLED));
 	pr_info("stealtime: cpu %d, msr %llx\n", cpu,
 		(unsigned long long) slow_virt_to_phys(st));
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index aa593743acf6..99c9c6120b6f 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -24,7 +24,20 @@
 #include <asm/kvmclock.h>
 
 static int kvmclock __initdata = 1;
+/*
+ * 在以下使用kvmclock_vsyscall:
+ *   - arch/x86/kernel/kvmclock.c|49| <<parse_no_kvmclock_vsyscall>> kvmclock_vsyscall = 0;
+ *   - arch/x86/kernel/kvmclock.c|364| <<kvm_setup_vsyscall_timeinfo>> if (!per_cpu(hv_clock_per_cpu, 0) || !kvmclock_vsyscall)
+ */
 static int kvmclock_vsyscall __initdata = 1;
+/*
+ * 在以下使用msr_kvm_system_time:
+ *   - arch/x86/kernel/kvmclock.c|231| <<kvm_register_clock>> wrmsrl(msr_kvm_system_time, pa);
+ *   - arch/x86/kernel/kvmclock.c|273| <<kvm_crash_shutdown>> native_write_msr(msr_kvm_system_time, 0, 0);
+ *   - arch/x86/kernel/kvmclock.c|281| <<kvm_shutdown>> native_write_msr(msr_kvm_system_time, 0, 0);
+ *   - arch/x86/kernel/kvmclock.c|432| <<kvmclock_init>> msr_kvm_system_time = MSR_KVM_SYSTEM_TIME_NEW;
+ *   - arch/x86/kernel/kvmclock.c|444| <<kvmclock_init>> msr_kvm_system_time, msr_kvm_wall_clock);
+ */
 static int msr_kvm_system_time __ro_after_init = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock __ro_after_init = MSR_KVM_WALL_CLOCK;
 static u64 kvm_sched_clock_offset __ro_after_init;
@@ -47,17 +60,56 @@ early_param("no-kvmclock-vsyscall", parse_no_kvmclock_vsyscall);
 #define HVC_BOOT_ARRAY_SIZE \
 	(PAGE_SIZE / sizeof(struct pvclock_vsyscall_time_info))
 
+/*
+ * 在以下使用hv_clock_boot:
+ *   - arch/x86/kernel/kvmclock.c|277| <<kvm_setup_vsyscall_timeinfo>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+ *   - arch/x86/kernel/kvmclock.c|304| <<kvmclock_setup_percpu>> p = &hv_clock_boot[cpu];
+ *   - arch/x86/kernel/kvmclock.c|336| <<kvmclock_init>> this_cpu_write(hv_clock_per_cpu, &hv_clock_boot[0]);
+ *   - arch/x86/kernel/kvmclock.c|338| <<kvmclock_init>> pvclock_set_pvti_cpu0_va(hv_clock_boot);
+ *   - arch/x86/kernel/kvmclock.c|343| <<kvmclock_init>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+ */
 static struct pvclock_vsyscall_time_info
 			hv_clock_boot[HVC_BOOT_ARRAY_SIZE] __bss_decrypted __aligned(PAGE_SIZE);
 static struct pvclock_wall_clock wall_clock __bss_decrypted;
+/*
+ * 在以下使用hv_clock_per_cpu:
+ *   - arch/x86/kernel/kvmclock.c|58| <<this_cpu_pvti>> return &this_cpu_read(hv_clock_per_cpu)->pvti;
+ *   - arch/x86/kernel/kvmclock.c|63| <<this_cpu_hvclock>> return this_cpu_read(hv_clock_per_cpu);
+ *   - arch/x86/kernel/kvmclock.c|274| <<kvm_setup_vsyscall_timeinfo>> if (!per_cpu(hv_clock_per_cpu, 0) || !kvmclock_vsyscall)
+ *   - arch/x86/kernel/kvmclock.c|292| <<kvmclock_setup_percpu>> struct pvclock_vsyscall_time_info *p = per_cpu(hv_clock_per_cpu, cpu);
+ *   - arch/x86/kernel/kvmclock.c|299| <<kvmclock_setup_percpu>> if (!cpu || (p && p != per_cpu(hv_clock_per_cpu, 0)))
+ *   - arch/x86/kernel/kvmclock.c|310| <<kvmclock_setup_percpu>> per_cpu(hv_clock_per_cpu, cpu) = p;
+ *   - arch/x86/kernel/kvmclock.c|336| <<kvmclock_init>> this_cpu_write(hv_clock_per_cpu, &hv_clock_boot[0]);
+ */
 static DEFINE_PER_CPU(struct pvclock_vsyscall_time_info *, hv_clock_per_cpu);
+/*
+ * 在以下使用hvclock_mem:
+ *   - arch/x86/kernel/kvmclock.c|241| <<kvmclock_init_mem>> order = get_order(ncpus * sizeof(*hvclock_mem));
+ *   - arch/x86/kernel/kvmclock.c|249| <<kvmclock_init_mem>> hvclock_mem = page_address(p);
+ *   - arch/x86/kernel/kvmclock.c|256| <<kvmclock_init_mem>> r = set_memory_decrypted((unsigned long ) hvclock_mem,
+ *   - arch/x86/kernel/kvmclock.c|260| <<kvmclock_init_mem>> hvclock_mem = NULL;
+ *   - arch/x86/kernel/kvmclock.c|266| <<kvmclock_init_mem>> memset(hvclock_mem, 0, PAGE_SIZE << order);
+ *   - arch/x86/kernel/kvmclock.c|305| <<kvmclock_setup_percpu>> else if (hvclock_mem)
+ *   - arch/x86/kernel/kvmclock.c|306| <<kvmclock_setup_percpu>> p = hvclock_mem + cpu - HVC_BOOT_ARRAY_SIZE;
+ */
 static struct pvclock_vsyscall_time_info *hvclock_mem;
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|103| <<kvm_get_wallclock>> pvclock_read_wallclock(&wall_clock, this_cpu_pvti(), now);
+ *   - arch/x86/kernel/kvmclock.c|117| <<kvm_clock_read>> ret = pvclock_clocksource_read(this_cpu_pvti());
+ *   - arch/x86/kernel/kvmclock.c|158| <<kvm_get_tsc_khz>> return pvclock_tsc_khz(this_cpu_pvti());
+ */
 static inline struct pvclock_vcpu_time_info *this_cpu_pvti(void)
 {
 	return &this_cpu_read(hv_clock_per_cpu)->pvti;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|175| <<kvm_check_and_clear_guest_paused>> struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
+ *   - arch/x86/kernel/kvmclock.c|207| <<kvm_register_clock>> struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
+ */
 static inline struct pvclock_vsyscall_time_info *this_cpu_hvclock(void)
 {
 	return this_cpu_read(hv_clock_per_cpu);
@@ -101,6 +153,10 @@ static u64 kvm_sched_clock_read(void)
 	return kvm_clock_read() - kvm_sched_clock_offset;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|528| <<kvmclock_init>> kvm_sched_clock_init(flags & PVCLOCK_TSC_STABLE_BIT);
+ */
 static inline void kvm_sched_clock_init(bool stable)
 {
 	if (!stable)
@@ -127,6 +183,9 @@ static inline void kvm_sched_clock_init(bool stable)
 static unsigned long kvm_get_tsc_khz(void)
 {
 	setup_force_cpu_cap(X86_FEATURE_TSC_KNOWN_FREQ);
+	/*
+	 * 将pv_tsc_khz根据local cpu的pvti的tsc_shift和tsc_to_system_mul做校准
+	 */
 	return pvclock_tsc_khz(this_cpu_pvti());
 }
 
@@ -174,6 +233,12 @@ struct clocksource kvm_clock = {
 };
 EXPORT_SYMBOL_GPL(kvm_clock);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|224| <<kvm_restore_sched_clock_state>> kvm_register_clock("primary cpu clock, resume");
+ *   - arch/x86/kernel/kvmclock.c|230| <<kvm_setup_secondary_clock>> kvm_register_clock("secondary cpu clock");
+ *   - arch/x86/kernel/kvmclock.c|419| <<kvmclock_init>> kvm_register_clock("primary cpu clock");
+ */
 static void kvm_register_clock(char *txt)
 {
 	struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
@@ -191,12 +256,23 @@ static void kvm_save_sched_clock_state(void)
 {
 }
 
+/*
+ * 在以下使用kvm_restore_sched_clock_state():
+ *   - arch/x86/kernel/kvmclock.c|444| <<kvmclock_init>> x86_platform.restore_sched_clock_state = kvm_restore_sched_clock_state;
+ */
 static void kvm_restore_sched_clock_state(void)
 {
 	kvm_register_clock("primary cpu clock, resume");
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC
+/*
+ * 在以下使用kvm_setup_secondary_clock():
+ *   - arch/x86/kernel/kvmclock.c|441| <<kvmclock_init>> x86_cpuinit.early_percpu_clock_init = kvm_setup_secondary_clock;
+ *
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|238| <<start_secondary>> x86_cpuinit.early_percpu_clock_init();
+ */
 static void kvm_setup_secondary_clock(void)
 {
 	kvm_register_clock("secondary cpu clock");
@@ -227,6 +303,19 @@ static void kvm_shutdown(void)
 	native_machine_shutdown();
 }
 
+/*
+ * [0] kvmclock_init_mem
+ * [0] kvm_setup_vsyscall_timeinfo
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|358| <<kvm_setup_vsyscall_timeinfo>> kvmclock_init_mem();
+ *
+ * 在prefill_possible_map()之后执行
+ */
 static void __init kvmclock_init_mem(void)
 {
 	unsigned long ncpus;
@@ -234,6 +323,9 @@ static void __init kvmclock_init_mem(void)
 	struct page *p;
 	int r;
 
+	/*
+	 * 在prefill_possible_map()之后执行
+	 */
 	if (HVC_BOOT_ARRAY_SIZE >= num_possible_cpus())
 		return;
 
@@ -246,6 +338,16 @@ static void __init kvmclock_init_mem(void)
 		return;
 	}
 
+	/*
+	 * 在以下使用hvclock_mem:
+	 *   - arch/x86/kernel/kvmclock.c|241| <<kvmclock_init_mem>> order = get_order(ncpus * sizeof(*hvclock_mem));
+	 *   - arch/x86/kernel/kvmclock.c|249| <<kvmclock_init_mem>> hvclock_mem = page_address(p);
+	 *   - arch/x86/kernel/kvmclock.c|256| <<kvmclock_init_mem>> r = set_memory_decrypted((unsigned long ) hvclock_mem,
+	 *   - arch/x86/kernel/kvmclock.c|260| <<kvmclock_init_mem>> hvclock_mem = NULL;
+	 *   - arch/x86/kernel/kvmclock.c|266| <<kvmclock_init_mem>> memset(hvclock_mem, 0, PAGE_SIZE << order);
+	 *   - arch/x86/kernel/kvmclock.c|305| <<kvmclock_setup_percpu>> else if (hvclock_mem)
+	 *   - arch/x86/kernel/kvmclock.c|306| <<kvmclock_setup_percpu>> p = hvclock_mem + cpu - HVC_BOOT_ARRAY_SIZE;
+	 */
 	hvclock_mem = page_address(p);
 
 	/*
@@ -266,14 +368,50 @@ static void __init kvmclock_init_mem(void)
 	memset(hvclock_mem, 0, PAGE_SIZE << order);
 }
 
+/*
+ * 先调用kvmclock_init(), 再调用kvm_setup_vsyscall_timeinfo()
+ */
 static int __init kvm_setup_vsyscall_timeinfo(void)
 {
 #ifdef CONFIG_X86_64
 	u8 flags;
 
+	/*
+	 * 在以下使用hv_clock_per_cpu:
+	 *   - arch/x86/kernel/kvmclock.c|58| <<this_cpu_pvti>> return &this_cpu_read(hv_clock_per_cpu)->pvti;
+	 *   - arch/x86/kernel/kvmclock.c|63| <<this_cpu_hvclock>> return this_cpu_read(hv_clock_per_cpu);
+	 *   - arch/x86/kernel/kvmclock.c|274| <<kvm_setup_vsyscall_timeinfo>> if (!per_cpu(hv_clock_per_cpu, 0) || !kvmclock_vsyscall)
+	 *   - arch/x86/kernel/kvmclock.c|292| <<kvmclock_setup_percpu>> struct pvclock_vsyscall_time_info *p = per_cpu(hv_clock_per_cpu, cpu);
+	 *   - arch/x86/kernel/kvmclock.c|299| <<kvmclock_setup_percpu>> if (!cpu || (p && p != per_cpu(hv_clock_per_cpu, 0)))
+	 *   - arch/x86/kernel/kvmclock.c|310| <<kvmclock_setup_percpu>> per_cpu(hv_clock_per_cpu, cpu) = p;
+	 *   - arch/x86/kernel/kvmclock.c|336| <<kvmclock_init>> this_cpu_write(hv_clock_per_cpu, &hv_clock_boot[0]);
+	 *
+	 * 在以下使用kvmclock_vsyscall:
+	 *   - arch/x86/kernel/kvmclock.c|49| <<parse_no_kvmclock_vsyscall>> kvmclock_vsyscall = 0;
+	 *   - arch/x86/kernel/kvmclock.c|364| <<kvm_setup_vsyscall_timeinfo>> if (!per_cpu(hv_clock_per_cpu, 0) || !kvmclock_vsyscall)
+	 */
 	if (!per_cpu(hv_clock_per_cpu, 0) || !kvmclock_vsyscall)
 		return 0;
 
+	/*
+	 * 在以下使用hv_clock_boot:
+	 *   - arch/x86/kernel/kvmclock.c|277| <<kvm_setup_vsyscall_timeinfo>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+	 *   - arch/x86/kernel/kvmclock.c|304| <<kvmclock_setup_percpu>> p = &hv_clock_boot[cpu];
+	 *   - arch/x86/kernel/kvmclock.c|336| <<kvmclock_init>> this_cpu_write(hv_clock_per_cpu, &hv_clock_boot[0]);
+	 *   - arch/x86/kernel/kvmclock.c|338| <<kvmclock_init>> pvclock_set_pvti_cpu0_va(hv_clock_boot);
+	 *   - arch/x86/kernel/kvmclock.c|343| <<kvmclock_init>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+	 *
+	 * struct pvclock_vcpu_time_info {
+	 *     u32   version;
+	 *     u32   pad0;
+	 *     u64   tsc_timestamp;
+	 *     u64   system_time;
+	 *     u32   tsc_to_system_mul;
+	 *     s8    tsc_shift;                     
+	 *     u8    flags;
+	 *     u8    pad[2];
+	 * } __attribute__((__packed__));
+	 */
 	flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
 	if (!(flags & PVCLOCK_TSC_STABLE_BIT))
 		return 0;
@@ -289,6 +427,16 @@ early_initcall(kvm_setup_vsyscall_timeinfo);
 
 static int kvmclock_setup_percpu(unsigned int cpu)
 {
+	/*
+	 * 在以下使用hv_clock_per_cpu:
+	 *   - arch/x86/kernel/kvmclock.c|58| <<this_cpu_pvti>> return &this_cpu_read(hv_clock_per_cpu)->pvti;
+	 *   - arch/x86/kernel/kvmclock.c|63| <<this_cpu_hvclock>> return this_cpu_read(hv_clock_per_cpu);
+	 *   - arch/x86/kernel/kvmclock.c|274| <<kvm_setup_vsyscall_timeinfo>> if (!per_cpu(hv_clock_per_cpu, 0) || !kvmclock_vsyscall)
+	 *   - arch/x86/kernel/kvmclock.c|292| <<kvmclock_setup_percpu>> struct pvclock_vsyscall_time_info *p = per_cpu(hv_clock_per_cpu, cpu);
+	 *   - arch/x86/kernel/kvmclock.c|299| <<kvmclock_setup_percpu>> if (!cpu || (p && p != per_cpu(hv_clock_per_cpu, 0)))
+	 *   - arch/x86/kernel/kvmclock.c|310| <<kvmclock_setup_percpu>> per_cpu(hv_clock_per_cpu, cpu) = p;
+	 *   - arch/x86/kernel/kvmclock.c|336| <<kvmclock_init>> this_cpu_write(hv_clock_per_cpu, &hv_clock_boot[0]);
+	 */
 	struct pvclock_vsyscall_time_info *p = per_cpu(hv_clock_per_cpu, cpu);
 
 	/*
@@ -300,6 +448,26 @@ static int kvmclock_setup_percpu(unsigned int cpu)
 		return 0;
 
 	/* Use the static page for the first CPUs, allocate otherwise */
+	/*
+	 * 在以下使用hv_clock_boot:
+	 *   - arch/x86/kernel/kvmclock.c|277| <<kvm_setup_vsyscall_timeinfo>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+	 *   - arch/x86/kernel/kvmclock.c|304| <<kvmclock_setup_percpu>> p = &hv_clock_boot[cpu];
+	 *   - arch/x86/kernel/kvmclock.c|336| <<kvmclock_init>> this_cpu_write(hv_clock_per_cpu, &hv_clock_boot[0]);
+	 *   - arch/x86/kernel/kvmclock.c|338| <<kvmclock_init>> pvclock_set_pvti_cpu0_va(hv_clock_boot);
+	 *   - arch/x86/kernel/kvmclock.c|343| <<kvmclock_init>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+	 *
+	 * 在HVC_BOOT_ARRAY_SIZE=64
+	 *
+	 * static struct pvclock_vsyscall_time_info *hvclock_mem;
+	 * 在以下使用hvclock_mem:
+	 *   - arch/x86/kernel/kvmclock.c|241| <<kvmclock_init_mem>> order = get_order(ncpus * sizeof(*hvclock_mem));
+	 *   - arch/x86/kernel/kvmclock.c|249| <<kvmclock_init_mem>> hvclock_mem = page_address(p);
+	 *   - arch/x86/kernel/kvmclock.c|256| <<kvmclock_init_mem>> r = set_memory_decrypted((unsigned long ) hvclock_mem,
+	 *   - arch/x86/kernel/kvmclock.c|260| <<kvmclock_init_mem>> hvclock_mem = NULL;
+	 *   - arch/x86/kernel/kvmclock.c|266| <<kvmclock_init_mem>> memset(hvclock_mem, 0, PAGE_SIZE << order);
+	 *   - arch/x86/kernel/kvmclock.c|305| <<kvmclock_setup_percpu>> else if (hvclock_mem)
+	 *   - arch/x86/kernel/kvmclock.c|306| <<kvmclock_setup_percpu>> p = hvclock_mem + cpu - HVC_BOOT_ARRAY_SIZE;
+	 */
 	if (cpu < HVC_BOOT_ARRAY_SIZE)
 		p = &hv_clock_boot[cpu];
 	else if (hvclock_mem)
@@ -307,10 +475,26 @@ static int kvmclock_setup_percpu(unsigned int cpu)
 	else
 		return -ENOMEM;
 
+	/*
+	 * 在以下使用hv_clock_per_cpu:
+	 *   - arch/x86/kernel/kvmclock.c|58| <<this_cpu_pvti>> return &this_cpu_read(hv_clock_per_cpu)->pvti;
+	 *   - arch/x86/kernel/kvmclock.c|63| <<this_cpu_hvclock>> return this_cpu_read(hv_clock_per_cpu);
+	 *   - arch/x86/kernel/kvmclock.c|274| <<kvm_setup_vsyscall_timeinfo>> if (!per_cpu(hv_clock_per_cpu, 0) || !kvmclock_vsyscall)
+	 *   - arch/x86/kernel/kvmclock.c|292| <<kvmclock_setup_percpu>> struct pvclock_vsyscall_time_info *p = per_cpu(hv_clock_per_cpu, cpu);
+	 *   - arch/x86/kernel/kvmclock.c|299| <<kvmclock_setup_percpu>> if (!cpu || (p && p != per_cpu(hv_clock_per_cpu, 0)))
+	 *   - arch/x86/kernel/kvmclock.c|310| <<kvmclock_setup_percpu>> per_cpu(hv_clock_per_cpu, cpu) = p;
+	 *   - arch/x86/kernel/kvmclock.c|336| <<kvmclock_init>> this_cpu_write(hv_clock_per_cpu, &hv_clock_boot[0]);
+	 */
 	per_cpu(hv_clock_per_cpu, cpu) = p;
 	return p ? 0 : -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|757| <<kvm_init_platform>> kvmclock_init();
+ *
+ * 先调用kvmclock_init(), 再调用kvm_setup_vsyscall_timeinfo()
+ */
 void __init kvmclock_init(void)
 {
 	u8 flags;
@@ -333,6 +517,10 @@ void __init kvmclock_init(void)
 	pr_info("kvm-clock: Using msrs %x and %x",
 		msr_kvm_system_time, msr_kvm_wall_clock);
 
+	/*
+	 * 定义是 ... 每个都是一个指针 ...
+	 * static DEFINE_PER_CPU(struct pvclock_vsyscall_time_info *, hv_clock_per_cpu);
+	 */
 	this_cpu_write(hv_clock_per_cpu, &hv_clock_boot[0]);
 	kvm_register_clock("primary cpu clock");
 	pvclock_set_pvti_cpu0_va(hv_clock_boot);
@@ -340,11 +528,22 @@ void __init kvmclock_init(void)
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE_STABLE_BIT))
 		pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
 
+	/*
+	 * 在以下使用hv_clock_boot:
+	 *   - arch/x86/kernel/kvmclock.c|277| <<kvm_setup_vsyscall_timeinfo>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+	 *   - arch/x86/kernel/kvmclock.c|304| <<kvmclock_setup_percpu>> p = &hv_clock_boot[cpu];
+	 *   - arch/x86/kernel/kvmclock.c|336| <<kvmclock_init>> this_cpu_write(hv_clock_per_cpu, &hv_clock_boot[0]);
+	 *   - arch/x86/kernel/kvmclock.c|338| <<kvmclock_init>> pvclock_set_pvti_cpu0_va(hv_clock_boot);
+	 *   - arch/x86/kernel/kvmclock.c|343| <<kvmclock_init>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+	 */
 	flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
 	kvm_sched_clock_init(flags & PVCLOCK_TSC_STABLE_BIT);
 
 	x86_platform.calibrate_tsc = kvm_get_tsc_khz;
 	x86_platform.calibrate_cpu = kvm_get_tsc_khz;
+	/*
+	 * 获得系统boot时的秒数和纳秒数
+	 */
 	x86_platform.get_wallclock = kvm_get_wallclock;
 	x86_platform.set_wallclock = kvm_set_wallclock;
 #ifdef CONFIG_X86_LOCAL_APIC
@@ -371,6 +570,9 @@ void __init kvmclock_init(void)
 	    !check_tsc_unstable())
 		kvm_clock.rating = 299;
 
+	/*
+	 * #define NSEC_PER_SEC 1000000000L
+	 */
 	clocksource_register_hz(&kvm_clock, NSEC_PER_SEC);
 	pv_info.name = "KVM";
 }
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index 11065dc03f5b..158835781c64 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -16,6 +16,12 @@
 #include <asm/pvclock.h>
 #include <asm/vgtod.h>
 
+/*
+ * 在以下使用valid_flags:
+ *   - arch/x86/kernel/pvclock.c|24| <<pvclock_set_flags>> valid_flags = flags; 
+ *   - arch/x86/kernel/pvclock.c|64| <<pvclock_read_flags>> return flags & valid_flags;
+ *   - arch/x86/kernel/pvclock.c|85| <<pvclock_clocksource_read>> if ((valid_flags & PVCLOCK_TSC_STABLE_BIT) &&
+ */
 static u8 valid_flags __read_mostly = 0;
 static struct pvclock_vsyscall_time_info *pvti_cpu0_va __read_mostly;
 
@@ -24,6 +30,9 @@ void pvclock_set_flags(u8 flags)
 	valid_flags = flags;
 }
 
+/*
+ * 将pv_tsc_khz根据local cpu的pvti的tsc_shift和tsc_to_system_mul做校准
+ */
 unsigned long pvclock_tsc_khz(struct pvclock_vcpu_time_info *src)
 {
 	u64 pv_tsc_khz = 1000000ULL << 32;
@@ -110,6 +119,11 @@ u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|127| <<kvm_get_wallclock>> pvclock_read_wallclock(&wall_clock, this_cpu_pvti(), now);
+ *   - arch/x86/xen/time.c|75| <<xen_read_wallclock>> pvclock_read_wallclock(wall_clock, vcpu_time, ts);
+ */
 void pvclock_read_wallclock(struct pvclock_wall_clock *wall_clock,
 			    struct pvclock_vcpu_time_info *vcpu_time,
 			    struct timespec64 *ts)
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 02813a7f3a7c..8a19d4f18dde 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -235,6 +235,9 @@ static void notrace start_secondary(void *unused)
 	cpu_init_exception_handling();
 	cpu_init();
 	rcu_cpu_starting(raw_smp_processor_id());
+	/*
+	 * 例子: kvm_setup_secondary_clock()
+	 */
 	x86_cpuinit.early_percpu_clock_init();
 	preempt_disable();
 	smp_callin();
@@ -1455,6 +1458,10 @@ early_param("possible_cpus", _setup_possible_cpus);
  * We do this because additional CPUs waste a lot of memory.
  * -AK
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/setup.c|1202| <<setup_arch>> prefill_possible_map();
+ */
 __init void prefill_possible_map(void)
 {
 	int i, possible;
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index f70dffc2771f..e5718c85ba83 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -32,6 +32,9 @@
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
 
+/*
+ * host的tsc frequency
+ */
 unsigned int __read_mostly tsc_khz;
 EXPORT_SYMBOL(tsc_khz);
 
@@ -40,6 +43,20 @@ EXPORT_SYMBOL(tsc_khz);
 /*
  * TSC can be unstable due to cpufreq or due to unsynced TSCs
  */
+/*
+ * 在以下修改tsc_unstable:
+ *   - arch/x86/kernel/tsc.c|1102| <<tsc_cs_mark_unstable>> tsc_unstable = 1;
+ *   - arch/x86/kernel/tsc.c|1168| <<mark_tsc_unstable>> tsc_unstable = 1;
+ * 在以下使用tsc_unstable:
+ *   - arch/x86/kernel/tsc.c|268| <<check_tsc_unstable>> return tsc_unstable;
+ *   - arch/x86/kernel/tsc.c|1099| <<tsc_cs_mark_unstable>> if (tsc_unstable)
+ *   - arch/x86/kernel/tsc.c|1111| <<tsc_cs_tick_stable>> if (tsc_unstable)
+ *   - arch/x86/kernel/tsc.c|1165| <<mark_tsc_unstable>> if (tsc_unstable)
+ *   - arch/x86/kernel/tsc.c|1204| <<unsynchronized_tsc>> if (!boot_cpu_has(X86_FEATURE_TSC) || tsc_unstable)
+ *   - arch/x86/kernel/tsc.c|1314| <<tsc_refine_calibration_work>> if (tsc_unstable)
+ *   - arch/x86/kernel/tsc.c|1368| <<tsc_refine_calibration_work>> if (tsc_unstable)
+ *   - arch/x86/kernel/tsc.c|1384| <<init_tsc_clocksource>> if (tsc_unstable)
+ */
 static int __read_mostly tsc_unstable;
 static unsigned int __initdata tsc_early_khz;
 
@@ -1160,6 +1177,24 @@ static struct clocksource clocksource_tsc = {
 	.list			= LIST_HEAD_INIT(clocksource_tsc.list),
 };
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/x2apic_uv_x.c|223| <<uv_tsc_check_sync>> mark_tsc_unstable("UV BIOS");
+ *   - arch/x86/kernel/cpu/cyrix.c|291| <<init_cyrix>> mark_tsc_unstable("cyrix 5510/5520 detected");
+ *   - arch/x86/kernel/cpu/mshyperv.c|326| <<ms_hyperv_init_platform>> mark_tsc_unstable("running on Hyper-V");
+ *   - arch/x86/kernel/process.c|861| <<arch_post_acpi_subsys_init>> mark_tsc_unstable("TSC halt in AMD C1E");
+ *   - arch/x86/kernel/tsc.c|275| <<notsc_setup>> mark_tsc_unstable("boot parameter notsc");
+ *   - arch/x86/kernel/tsc.c|302| <<tsc_setup>> mark_tsc_unstable("boot parameter");
+ *   - arch/x86/kernel/tsc.c|988| <<time_cpufreq_notifier>> mark_tsc_unstable("cpufreq changes on SMP");
+ *   - arch/x86/kernel/tsc.c|1005| <<time_cpufreq_notifier>> mark_tsc_unstable("cpufreq changes");
+ *   - arch/x86/kernel/tsc.c|1504| <<tsc_init>> mark_tsc_unstable("could not calculate TSC khz");
+ *   - arch/x86/kernel/tsc.c|1522| <<tsc_init>> mark_tsc_unstable("TSCs unsynchronized");
+ *   - arch/x86/kernel/tsc_sync.c|372| <<check_tsc_sync_source>> mark_tsc_unstable("check_tsc_sync_source failed");
+ *   - arch/x86/kvm/x86.c|3987| <<kvm_arch_vcpu_load>> mark_tsc_unstable("KVM discovered backwards TSC");
+ *   - drivers/acpi/acpi_pad.c|162| <<power_saving_thread>> mark_tsc_unstable("TSC halts in idle");
+ *   - drivers/acpi/processor_idle.c|202| <<tsc_check_state>> mark_tsc_unstable("TSC halts in idle");
+ *   - drivers/idle/intel_idle.c|1497| <<intel_idle_verify_cstate>> mark_tsc_unstable("TSC halts in idle states deeper than C2");
+ */
 void mark_tsc_unstable(char *reason)
 {
 	if (tsc_unstable)
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index 38172ca627d3..6e02bc8ae535 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -582,6 +582,10 @@ static int __do_cpuid_func_emulated(struct kvm_cpuid_array *array, u32 func)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|891| <<do_cpuid_func>> return __do_cpuid_func(array, func);
+ */
 static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 {
 	struct kvm_cpuid_entry2 *entry;
@@ -882,6 +886,11 @@ static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|906| <<get_cpuid_func>> r = do_cpuid_func(array, func, type);
+ *   - arch/x86/kvm/cpuid.c|912| <<get_cpuid_func>> r = do_cpuid_func(array, func, type);
+ */
 static int do_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 			 unsigned int type)
 {
@@ -893,6 +902,10 @@ static int do_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 
 #define CENTAUR_CPUID_SIGNATURE 0xC0000000
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|976| <<kvm_dev_ioctl_get_cpuid>> r = get_cpuid_func(&array, funcs[i], type);
+ */
 static int get_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 			  unsigned int type)
 {
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index a889563ad02d..ced7967957dc 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -154,13 +154,39 @@ static inline u64 kvm_read_edx_eax(struct kvm_vcpu *vcpu)
 		| ((u64)(kvm_rdx_read(vcpu) & -1u) << 32);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|423| <<nested_prepare_vmcb_control>> enter_guest_mode(&svm->vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3403| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+ */
 static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用HF_GUEST_MASK:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|159| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|164| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|174| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+	 *   - arch/x86/kvm/x86.c|6994| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+	 */
 	vcpu->arch.hflags |= HF_GUEST_MASK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|591| <<nested_svm_vmexit>> leave_guest_mode(&svm->vcpu);
+ *   - arch/x86/kvm/svm/nested.c|753| <<svm_leave_nested>> leave_guest_mode(&svm->vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3477| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4490| <<nested_vmx_vmexit>> leave_guest_mode(vcpu);
+ */
 static inline void leave_guest_mode(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用HF_GUEST_MASK:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|159| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|164| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|174| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+	 *   - arch/x86/kvm/x86.c|6994| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+	 */
 	vcpu->arch.hflags &= ~HF_GUEST_MASK;
 
 	if (vcpu->arch.load_eoi_exitmap_pending) {
@@ -171,6 +197,13 @@ static inline void leave_guest_mode(struct kvm_vcpu *vcpu)
 
 static inline bool is_guest_mode(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用HF_GUEST_MASK:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|159| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|164| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|174| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+	 *   - arch/x86/kvm/x86.c|6994| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+	 */
 	return vcpu->arch.hflags & HF_GUEST_MASK;
 }
 
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 6d16481aa29d..516196b791b5 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -1347,6 +1347,10 @@ static int kvm_unmap_rmapp(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 	return kvm_zap_rmapp(kvm, rmap_head);
 }
 
+/*
+ * 在以下使用kvm_set_pte_rmapp():
+ *   - arch/x86/kvm/mmu/mmu.c|1534| <<kvm_set_spte_hva>> r = kvm_handle_hva(kvm, hva, (unsigned long )&pte, kvm_set_pte_rmapp);
+ */
 static int kvm_set_pte_rmapp(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 			     struct kvm_memory_slot *slot, gfn_t gfn, int level,
 			     unsigned long data)
@@ -1527,6 +1531,10 @@ int kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end,
 	return r;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|499| <<kvm_mmu_notifier_change_pte>> if (kvm_set_spte_hva(kvm, address, pte))
+ */
 int kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte)
 {
 	int r;
@@ -1627,12 +1635,21 @@ static int is_empty_shadow_page(u64 *spt)
  * aggregate version in order to make the slab shrinker
  * faster
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1691| <<kvm_mmu_alloc_page>> kvm_mod_used_mmu_pages(vcpu->kvm, +1);
+ *   - arch/x86/kvm/mmu/mmu.c|2356| <<__kvm_mmu_prepare_zap_page>> kvm_mod_used_mmu_pages(kvm, -1);
+ */
 static inline void kvm_mod_used_mmu_pages(struct kvm *kvm, unsigned long nr)
 {
 	kvm->arch.n_used_mmu_pages += nr;
 	percpu_counter_add(&kvm_total_used_mmu_pages, nr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2446| <<kvm_mmu_commit_zap_page>> kvm_mmu_free_page(sp)l
+ */
 static void kvm_mmu_free_page(struct kvm_mmu_page *sp)
 {
 	MMU_WARN_ON(!is_empty_shadow_page(sp->spt));
@@ -1888,8 +1905,24 @@ static void kvm_mmu_audit(struct kvm_vcpu *vcpu, int point) { }
 static void mmu_audit_disable(void) { }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1838| <<for_each_valid_sp>> if (is_obsolete_sp((_kvm), (_sp))) { \
+ *   - arch/x86/kvm/mmu/mmu.c|2382| <<__kvm_mmu_prepare_zap_page>> if (!is_obsolete_sp(kvm, sp))
+ *   - arch/x86/kvm/mmu/mmu.c|5439| <<kvm_zap_obsolete_pages>> if (!is_obsolete_sp(kvm, sp))
+ */
 static bool is_obsolete_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
+	/*
+	 * 在以下使用kvm_mmu_page->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1698| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|1909| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *
+	 * 在以下使用kvm_arch->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1698| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|1909| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmu.c|5510| <<kvm_mmu_zap_all_fast>> kvm->arch.mmu_valid_gen = kvm->arch.mmu_valid_gen ? 0 : 1;
+	 */
 	return sp->role.invalid ||
 	       unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
 }
@@ -2234,6 +2267,12 @@ static void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 }
 
 /* Returns the number of zapped non-leaf child shadow pages. */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2292| <<kvm_mmu_page_unlink_children>> zapped += mmu_page_zap_pte(kvm, sp, sp->spt + i, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|5100| <<kvm_mmu_pte_write>> mmu_page_zap_pte(vcpu->kvm, sp, spte, NULL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|939| <<FNAME>> mmu_page_zap_pte(vcpu->kvm, sp, sptep, NULL);
+ */
 static int mmu_page_zap_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 			    u64 *spte, struct list_head *invalid_list)
 {
@@ -2266,6 +2305,10 @@ static int mmu_page_zap_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2352| <<__kvm_mmu_prepare_zap_page>> *nr_zapped += kvm_mmu_page_unlink_children(kvm, sp, invalid_list);
+ */
 static int kvm_mmu_page_unlink_children(struct kvm *kvm,
 					struct kvm_mmu_page *sp,
 					struct list_head *invalid_list)
@@ -2279,6 +2322,10 @@ static int kvm_mmu_page_unlink_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2353| <<__kvm_mmu_prepare_zap_page>> kvm_mmu_unlink_parents(kvm, sp);
+ */
 static void kvm_mmu_unlink_parents(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	u64 *sptep;
@@ -2312,6 +2359,13 @@ static int mmu_zap_unsync_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2378| <<kvm_mmu_prepare_zap_page>> __kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|2428| <<kvm_mmu_zap_oldest_mmu_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list,
+ *   - arch/x86/kvm/mmu/mmu.c|5398| <<kvm_zap_obsolete_pages>> if (__kvm_mmu_prepare_zap_page(kvm, sp,
+ *   - arch/x86/kvm/mmu/mmu.c|5689| <<kvm_mmu_zap_all>> if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))
+ */
 static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 				       struct kvm_mmu_page *sp,
 				       struct list_head *invalid_list,
@@ -2320,6 +2374,11 @@ static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 	bool list_unstable;
 
 	trace_kvm_mmu_prepare_zap_page(sp);
+	/*
+	 * 在以下使用kvm_vm_stat->mmu_shadow_zapped:
+	 *   - arch/x86/kvm/x86.c|235| <<global>> VM_STAT("mmu_shadow_zapped", mmu_shadow_zapped),
+	 *   - arch/x86/kvm/mmu/mmu.c|2330| <<__kvm_mmu_prepare_zap_page>> ++kvm->stat.mmu_shadow_zapped;
+	 */
 	++kvm->stat.mmu_shadow_zapped;
 	*nr_zapped = mmu_zap_unsync_children(kvm, sp, invalid_list);
 	*nr_zapped += kvm_mmu_page_unlink_children(kvm, sp, invalid_list);
@@ -2333,6 +2392,9 @@ static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 
 	if (sp->unsync)
 		kvm_unlink_unsync_page(kvm, sp);
+	/*
+	 * 从kvm_mmu_zap_oldest_mmu_pages()进来的话root_count必须是0
+	 */
 	if (!sp->root_count) {
 		/* Count self */
 		(*nr_zapped)++;
@@ -2363,9 +2425,24 @@ static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 			kvm_reload_remote_mmus(kvm);
 	}
 
+	/*
+	 * 在以下修改kvm_mmu_page->lpage_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|787| <<account_huge_nx_page>> sp->lpage_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|810| <<unaccount_huge_nx_page>> sp->lpage_disallowed = false;
+	 * 在以下使用kvm_mmu_page->lpage_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|781| <<account_huge_nx_page>> if (sp->lpage_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2373| <<__kvm_mmu_prepare_zap_page>> if (sp->lpage_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|6032| <<kvm_recover_nx_lpages>> WARN_ON_ONCE(!sp->lpage_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|6038| <<kvm_recover_nx_lpages>> WARN_ON_ONCE(sp->lpage_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|332| <<__handle_changed_spte>> if (sp->lpage_disallowed)
+	 */
 	if (sp->lpage_disallowed)
 		unaccount_huge_nx_page(kvm, sp);
 
+	/*
+	 * 只在此处修改:
+	 *   - arch/x86/kvm/mmu/mmu.c|2410| <<__kvm_mmu_prepare_zap_page>> sp->role.invalid = 1;
+	 */
 	sp->role.invalid = 1;
 	return list_unstable;
 }
@@ -2379,6 +2456,18 @@ static bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 	return nr_zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1867| <<kvm_mmu_remote_flush_or_zap>> kvm_mmu_commit_zap_page(kvm, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2438| <<kvm_mmu_zap_oldest_mmu_pages>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2502| <<kvm_mmu_unprotect_page>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3217| <<kvm_mmu_free_roots>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|5410| <<kvm_zap_obsolete_pages>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+ *   - arch/x86/kvm/mmu/mmu.c|5695| <<kvm_mmu_zap_all>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|5763| <<mmu_shrink_scan>> kvm_mmu_commit_zap_page(kvm,
+ *   - arch/x86/kvm/mmu/mmu.c|6017| <<kvm_recover_nx_lpages>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|6021| <<kvm_recover_nx_lpages>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ */
 static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 				    struct list_head *invalid_list)
 {
@@ -2404,6 +2493,12 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2460| <<make_mmu_pages_available>> kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm, KVM_REFILL_PAGES - avail);
+ *   - arch/x86/kvm/mmu/mmu.c|2476| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
+ *   - arch/x86/kvm/mmu/mmu.c|5768| <<mmu_shrink_scan>> freed = kvm_mmu_zap_oldest_mmu_pages(kvm, sc->nr_to_scan);
+ */
 static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 						  unsigned long nr_to_zap)
 {
@@ -2413,6 +2508,16 @@ static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 	bool unstable;
 	int nr_zapped;
 
+	/*
+	 * 在以下使用kvm_arch->active_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1690| <<kvm_mmu_alloc_page>> list_add(&sp->link, &vcpu->kvm->arch.active_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|2416| <<kvm_mmu_zap_oldest_mmu_pages>> if (list_empty(&kvm->arch.active_mmu_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|2420| <<kvm_mmu_zap_oldest_mmu_pages>> list_for_each_entry_safe(sp, tmp, &kvm->arch.active_mmu_pages, link) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5370| <<kvm_zap_obsolete_pages>> list_for_each_entry_safe_reverse(sp, node, &kvm->arch.active_mmu_pages, link) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5686| <<kvm_mmu_zap_all>> list_for_each_entry_safe(sp, node, &kvm->arch.active_mmu_pages, link) {
+	 *   - arch/x86/kvm/mmu/mmu_audit.c|89| <<walk_all_active_sps>> list_for_each_entry(sp, &kvm->arch.active_mmu_pages, link)
+	 *   - arch/x86/kvm/x86.c|10381| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
+	 */
 	if (list_empty(&kvm->arch.active_mmu_pages))
 		return 0;
 
@@ -2437,6 +2542,11 @@ static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 
 	kvm_mmu_commit_zap_page(kvm, &invalid_list);
 
+	/*
+	 * 在以下使用kvm_vm_stat->mmu_recycled:
+	 *   - arch/x86/kvm/x86.c|240| <<global>> VM_STAT("mmu_recycled", mmu_recycled),
+	 *   - arch/x86/kvm/mmu/mmu.c|2465| <<kvm_mmu_zap_oldest_mmu_pages>> kvm->stat.mmu_recycled += total_zapped;
+	 */
 	kvm->stat.mmu_recycled += total_zapped;
 	return total_zapped;
 }
@@ -2866,6 +2976,10 @@ void disallowed_hugepage_adjust(u64 spte, gfn_t gfn, int cur_level,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3845| <<direct_page_fault>> r = __direct_map(vcpu, gpa, error_code, map_writable, max_level, pfn,
+ */
 static int __direct_map(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
 			int map_writable, int max_level, kvm_pfn_t pfn,
 			bool prefault, bool is_tdp)
@@ -3690,6 +3804,11 @@ static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3827| <<nonpaging_page_fault>> return direct_page_fault(vcpu, gpa & PAGE_MASK, error_code, prefault,
+ *   - arch/x86/kvm/mmu/mmu.c|3879| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, gpa, error_code, prefault,
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
 			     bool prefault, int max_level, bool is_tdp)
 {
@@ -4705,6 +4824,15 @@ static void init_kvm_softmmu(struct kvm_vcpu *vcpu)
 	context->inject_page_fault = kvm_inject_page_fault;
 }
 
+/*
+ * 仅仅是一个例子:
+ * nested_vmx_load_cr3()
+ * -> kvm_init_mmu()
+ *    -> init_kvm_nested_mmu()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4881| <<kvm_init_mmu>> init_kvm_nested_mmu(vcpu);
+ */
 static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu)
 {
 	union kvm_mmu_role new_role = kvm_calc_mmu_role_common(vcpu, false);
@@ -4759,6 +4887,13 @@ static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu)
 	update_last_nonleaf_level(vcpu, g_context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4905| <<kvm_mmu_reset_context>> kvm_init_mmu(vcpu, true);
+ *   - arch/x86/kvm/svm/nested.c|367| <<nested_svm_load_cr3>> kvm_init_mmu(vcpu, false);
+ *   - arch/x86/kvm/vmx/nested.c|1153| <<nested_vmx_load_cr3>> kvm_init_mmu(vcpu, false);
+ *   - arch/x86/kvm/x86.c|10027| <<kvm_arch_vcpu_create>> kvm_init_mmu(vcpu, false);
+ */
 void kvm_init_mmu(struct kvm_vcpu *vcpu, bool reset_roots)
 {
 	if (reset_roots) {
@@ -5455,6 +5590,14 @@ static void kvm_mmu_zap_all_fast(struct kvm *kvm)
 
 static bool kvm_has_zapped_obsolete_pages(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->zapped_obsolete_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|5399| <<kvm_zap_obsolete_pages>> __kvm_mmu_prepare_zap_page(kvm, sp, &kvm->arch.zapped_obsolete_pages, &nr_zapped)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5410| <<kvm_zap_obsolete_pages>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|5458| <<kvm_has_zapped_obsolete_pages>> return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));
+	 *   - arch/x86/kvm/mmu/mmu.c|5764| <<mmu_shrink_scan>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+	 *   - arch/x86/kvm/x86.c|10382| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.zapped_obsolete_pages);
+	 */
 	return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));
 }
 
@@ -5760,6 +5903,14 @@ mmu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)
 		spin_lock(&kvm->mmu_lock);
 
 		if (kvm_has_zapped_obsolete_pages(kvm)) {
+			/*
+			 * 在以下使用kvm_arch->zapped_obsolete_pages:
+			 *   - arch/x86/kvm/mmu/mmu.c|5399| <<kvm_zap_obsolete_pages>> __kvm_mmu_prepare_zap_page(kvm, sp, &kvm->arch.zapped_obsolete_pages, &nr_zapped)) {
+			 *   - arch/x86/kvm/mmu/mmu.c|5410| <<kvm_zap_obsolete_pages>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+			 *   - arch/x86/kvm/mmu/mmu.c|5458| <<kvm_has_zapped_obsolete_pages>> return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));
+			 *   - arch/x86/kvm/mmu/mmu.c|5764| <<mmu_shrink_scan>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+			 *   - arch/x86/kvm/x86.c|10382| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.zapped_obsolete_pages);
+			 */
 			kvm_mmu_commit_zap_page(kvm,
 			      &kvm->arch.zapped_obsolete_pages);
 			goto unlock;
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index bfc6389edc28..a78efd18bf6a 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -28,6 +28,17 @@ struct kvm_mmu_page {
 	bool unsync;
 	u8 mmu_valid_gen;
 	bool mmio_cached;
+	/*
+	 * 在以下修改kvm_mmu_page->lpage_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|787| <<account_huge_nx_page>> sp->lpage_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|810| <<unaccount_huge_nx_page>> sp->lpage_disallowed = false;
+	 * 在以下使用kvm_mmu_page->lpage_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|781| <<account_huge_nx_page>> if (sp->lpage_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2373| <<__kvm_mmu_prepare_zap_page>> if (sp->lpage_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|6032| <<kvm_recover_nx_lpages>> WARN_ON_ONCE(!sp->lpage_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|6038| <<kvm_recover_nx_lpages>> WARN_ON_ONCE(sp->lpage_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|332| <<__handle_changed_spte>> if (sp->lpage_disallowed)
+	 */
 	bool lpage_disallowed; /* Can't be replaced by an equiv large page */
 
 	/*
@@ -40,6 +51,24 @@ struct kvm_mmu_page {
 	u64 *spt;
 	/* hold the gfn of each spte inside spt */
 	gfn_t *gfns;
+	/*
+	 * 在以下修改kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|3246| <<mmu_alloc_root>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|103| <<kvm_mmu_get_root>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|109| <<kvm_mmu_put_root>> --sp->root_count;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|179| <<get_tdp_mmu_vcpu_root>> root->root_count = 1;
+	 * 在以下使用kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|2336| <<__kvm_mmu_prepare_zap_page>> if (!sp->root_count) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2402| <<kvm_mmu_commit_zap_page>> WARN_ON(!sp->role.invalid || sp->root_count);
+	 *   - arch/x86/kvm/mmu/mmu.c|2425| <<kvm_mmu_zap_oldest_mmu_pages>> if (sp->root_count)
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|100| <<kvm_mmu_get_root>> BUG_ON(!sp->root_count);
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|111| <<kvm_mmu_put_root>> return !sp->root_count;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|15| <<KVM_MMU_PAGE_FIELDS>> __field(__u32, root_count) \
+	 *   - arch/x86/kvm/mmu/mmutrace.h|22| <<KVM_MMU_PAGE_ASSIGN>> __entry->root_count = sp->root_count; \
+	 *   - arch/x86/kvm/mmu/mmutrace.h|45| <<KVM_MMU_PAGE_PRINTK>> __entry->root_count, \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|104| <<is_tdp_mmu_root>> return sp->tdp_mmu_page && sp->root_count;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|116| <<kvm_tdp_mmu_free_root>> WARN_ON(root->root_count);
+	 */
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
 	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index b56d604809b8..b3c4fdd83c07 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -508,6 +508,10 @@ void kvm_tdp_mmu_zap_all(struct kvm *kvm)
  * Installs a last-level SPTE to handle a TDP page fault.
  * (NPT/EPT violation/misconfiguration)
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|636| <<kvm_tdp_mmu_map>> ret = tdp_mmu_map_handle_target_level(vcpu, write, map_writable, &iter,
+ */
 static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu, int write,
 					  int map_writable,
 					  struct tdp_iter *iter,
@@ -559,6 +563,10 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu, int write,
  * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
  * page tables and SPTEs to translate the faulting guest physical address.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3809| <<direct_page_fault>> r = kvm_tdp_mmu_map(vcpu, gpa, error_code, map_writable, max_level,
+ */
 int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
 		    int map_writable, int max_level, kvm_pfn_t pfn,
 		    bool prefault)
diff --git a/arch/x86/kvm/svm/nested.c b/arch/x86/kvm/svm/nested.c
index db30670dd8c4..3dfe2e75cfe8 100644
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@ -451,6 +451,10 @@ int enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb12_gpa,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2178| <<vmrun_interception>> return nested_svm_vmrun(svm);
+ */
 int nested_svm_vmrun(struct vcpu_svm *svm)
 {
 	int ret;
@@ -466,6 +470,10 @@ int nested_svm_vmrun(struct vcpu_svm *svm)
 	}
 
 	vmcb12_gpa = svm->vmcb->save.rax;
+	/*
+	 * struct vcpu_svm:
+	 * -> struct kvm_vcpu vcpu;
+	 */
 	ret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb12_gpa), &map);
 	if (ret == -EINVAL) {
 		kvm_inject_gp(&svm->vcpu, 0);
@@ -1214,6 +1222,9 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * struct kvm_x86_ops svm_x86_ops.nested_ops = &svm_nested_ops;
+ */
 struct kvm_x86_nested_ops svm_nested_ops = {
 	.check_events = svm_check_nested_events,
 	.get_nested_state_pages = svm_get_nested_state_pages,
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 3442d44ca53b..c2b0f1522b71 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -2067,6 +2067,9 @@ static int shutdown_interception(struct vcpu_svm *svm)
 	return 0;
 }
 
+/*
+ * svm_exit_handlers[SVM_EXIT_IOIO] = io_interception()
+ */
 static int io_interception(struct vcpu_svm *svm)
 {
 	struct kvm_vcpu *vcpu = &svm->vcpu;
@@ -2170,6 +2173,9 @@ static int vmsave_interception(struct vcpu_svm *svm)
 	return ret;
 }
 
+/*
+ * svm_exit_handlers[SVM_EXIT_VMRUN] = vmrun_interception()
+ */
 static int vmrun_interception(struct vcpu_svm *svm)
 {
 	if (nested_svm_check_permissions(svm))
@@ -3010,6 +3016,12 @@ static int invpcid_interception(struct vcpu_svm *svm)
 	return kvm_handle_invpcid(vcpu, type, gva);
 }
 
+/*
+ * 在以下使用svm_exit_handlers[]:
+ *   - arch/x86/kvm/svm/svm.c|3212| <<svm_handle_invalid_exit>> if (exit_code < ARRAY_SIZE(svm_exit_handlers) &&
+ *   - arch/x86/kvm/svm/svm.c|3213| <<svm_handle_invalid_exit>> svm_exit_handlers[exit_code])
+ *   - arch/x86/kvm/svm/svm.c|3244| <<svm_invoke_exit_handler>> return svm_exit_handlers[exit_code](svm);
+ */
 static int (*const svm_exit_handlers[])(struct vcpu_svm *svm) = {
 	[SVM_EXIT_READ_CR0]			= cr_interception,
 	[SVM_EXIT_READ_CR3]			= cr_interception,
@@ -3256,6 +3268,12 @@ static void svm_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2,
 		*error_code = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9089| <<vcpu_enter_guest>> r = kvm_x86_ops.handle_exit(vcpu, exit_fastpath);
+ *
+ * static struct kvm_x86_ops svm_x86_ops.handle_exit = handle_exit()
+ */
 static int handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
diff --git a/arch/x86/kvm/vmx/capabilities.h b/arch/x86/kvm/vmx/capabilities.h
index 3a1861403d73..e9a4ad95e06d 100644
--- a/arch/x86/kvm/vmx/capabilities.h
+++ b/arch/x86/kvm/vmx/capabilities.h
@@ -369,6 +369,11 @@ static inline bool vmx_pt_mode_is_host_guest(void)
 	return pt_mode == PT_MODE_HOST_GUEST;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|344| <<intel_pmu_refresh>> vcpu->arch.perf_capabilities = vmx_get_perf_capabilities();
+ *   - arch/x86/kvm/vmx/vmx.c|1794| <<vmx_get_msr_feature>> msr->data = vmx_get_perf_capabilities();
+ */
 static inline u64 vmx_get_perf_capabilities(void)
 {
 	/*
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index f2b9bfb58206..4dd959e2ed9e 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -17,6 +17,14 @@
 static bool __read_mostly enable_shadow_vmcs = 1;
 module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
 
+/*
+ * 在以下使用nested_early_check:
+ *   - arch/x86/kvm/vmx/nested.c|21| <<global>> module_param(nested_early_check, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|2177| <<prepare_vmcs02_constant_state>> if (enable_ept && nested_early_check)
+ *   - arch/x86/kvm/vmx/nested.c|3040| <<nested_vmx_check_vmentry_hw>> if (!nested_early_check)
+ *   - arch/x86/kvm/vmx/nested.c|3384| <<nested_vmx_enter_non_root_mode>> if (!enable_ept && !nested_early_check)
+ *   - arch/x86/kvm/vmx/nested.c|4536| <<nested_vmx_vmexit>> WARN_ON_ONCE(nested_early_check);
+ */
 static bool __read_mostly nested_early_check = 0;
 module_param(nested_early_check, bool, S_IRUGO);
 
@@ -32,6 +40,11 @@ module_param(nested_early_check, bool, S_IRUGO);
  * Hyper-V requires all of these, so mark them as supported even though
  * they are just treated the same as all-context.
  */
+/*
+ * 在以下使用VMX_VPID_EXTENT_SUPPORTED_MASK:
+ *   - arch/x86/kvm/vmx/nested.c|5449| <<handle_invvpid>> VMX_VPID_EXTENT_SUPPORTED_MASK) >> 8;
+ *   - arch/x86/kvm/vmx/nested.c|6561| <<nested_vmx_setup_ctls_msrs>> VMX_VPID_EXTENT_SUPPORTED_MASK;
+ */
 #define VMX_VPID_EXTENT_SUPPORTED_MASK		\
 	(VMX_VPID_EXTENT_INDIVIDUAL_ADDR_BIT |	\
 	VMX_VPID_EXTENT_SINGLE_CONTEXT_BIT |	\
@@ -41,6 +54,10 @@ module_param(nested_early_check, bool, S_IRUGO);
 #define VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE 5
 
 enum {
+	/*
+	 * 在以下使用VMX_VMREAD_BITMAP:
+	 *   - arch/x86/kvm/vmx/nested.c|63| <<vmx_vmread_bitmap>> #define vmx_vmread_bitmap (vmx_bitmap[VMX_VMREAD_BITMAP])
+	 */
 	VMX_VMREAD_BITMAP,
 	VMX_VMWRITE_BITMAP,
 	VMX_BITMAP_NR
@@ -151,6 +168,18 @@ static void init_vmcs_shadow_fields(void)
  * by Vol 2B, VMX Instruction Reference, "Conventions"), and skip the emulated
  * instruction.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4913| <<handle_vmon>> return nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4954| <<handle_vmoff>> return nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4999| <<handle_vmclear>> return nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5072| <<handle_vmread>> return nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5203| <<handle_vmwrite>> return nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5278| <<handle_vmptrld>> return nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5306| <<handle_vmptrst>> return nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5394| <<handle_invept>> return nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5483| <<handle_invvpid>> return nested_vmx_succeed(vcpu);
+ */
 static int nested_vmx_succeed(struct kvm_vcpu *vcpu)
 {
 	vmx_set_rflags(vcpu, vmx_get_rflags(vcpu)
@@ -252,6 +281,15 @@ static void vmx_sync_vmcs_host_state(struct vcpu_vmx *vmx,
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|295| <<free_nested>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3387| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+ *   - arch/x86/kvm/vmx/nested.c|3393| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3398| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3487| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|4539| <<nested_vmx_vmexit>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ */
 static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -275,6 +313,11 @@ static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
  * Free whatever needs to be freed from vmx->nested when L1 goes down, or
  * just stops using VMX.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5058| <<handle_vmoff>> free_nested(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6262| <<vmx_leave_nested>> free_nested(vcpu);
+ */
 static void free_nested(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -350,6 +393,10 @@ static void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,
 	vmcs12->guest_physical_address = fault->address;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2567| <<prepare_vmcs02>> nested_ept_init_mmu_context(vcpu);
+ */
 static void nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
 {
 	WARN_ON(mmu_is_nested(vcpu));
@@ -2145,6 +2192,10 @@ static u64 nested_vmx_calc_efer(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
 		return vmx->vcpu.arch.efer & ~(EFER_LMA | EFER_LME);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2252| <<prepare_vmcs02_early_rare>> prepare_vmcs02_constant_state(vmx);
+ */
 static void prepare_vmcs02_constant_state(struct vcpu_vmx *vmx)
 {
 	/*
@@ -2203,6 +2254,10 @@ static void prepare_vmcs02_constant_state(struct vcpu_vmx *vmx)
 	vmx_set_constant_host_state(vmx);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2270| <<prepare_vmcs02_early>> prepare_vmcs02_early_rare(vmx, vmcs12);
+ */
 static void prepare_vmcs02_early_rare(struct vcpu_vmx *vmx,
 				      struct vmcs12 *vmcs12)
 {
@@ -2218,6 +2273,10 @@ static void prepare_vmcs02_early_rare(struct vcpu_vmx *vmx,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3441| <<nested_vmx_enter_non_root_mode>> prepare_vmcs02_early(vmx, vmcs12);
+ */
 static void prepare_vmcs02_early(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
 {
 	u32 exec_control, vmcs12_exec_ctrl;
@@ -3287,6 +3346,18 @@ static int nested_vmx_write_pml_buffer(struct kvm_vcpu *vcpu, gpa_t gpa)
  * Note that many of these exceptions have priority over VM exits, so they
  * don't have to be checked again here.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3542| <<nested_vmx_run>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5055| <<handle_vmoff>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5075| <<handle_vmclear>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5138| <<handle_vmread>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5230| <<handle_vmwrite>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5334| <<handle_vmptrld>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5400| <<handle_vmptrst>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5447| <<handle_invept>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5527| <<handle_invvpid>> if (!nested_vmx_check_permission(vcpu))
+ */
 static int nested_vmx_check_permission(struct kvm_vcpu *vcpu)
 {
 	if (!to_vmx(vcpu)->nested.vmxon) {
@@ -3323,6 +3394,13 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
  *	NVMX_VMENTRY_VMEXIT:  Consistency check VMExit
  *	NVMX_VMENTRY_KVM_INTERNAL_ERROR: KVM internal error
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3567| <<nested_vmx_run>> status = nested_vmx_enter_non_root_mode(vcpu, true);
+ *   - arch/x86/kvm/vmx/nested.c|6336| <<vmx_set_nested_state>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+ *   - arch/x86/kvm/vmx/nested.h|25| <<vmx_set_nested_state>> enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
+ *   - arch/x86/kvm/vmx/vmx.c|7555| <<vmx_pre_leave_smm>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+ */
 enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 							bool from_vmentry)
 {
@@ -3365,6 +3443,15 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 	if (!enable_ept && !nested_early_check)
 		vmcs_writel(GUEST_CR3, vcpu->arch.cr3);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/vmx/nested.c|295| <<free_nested>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3387| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|3393| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3398| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3487| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|4539| <<nested_vmx_vmexit>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 */
 	vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
 
 	prepare_vmcs02_early(vmx, vmcs12);
@@ -3481,6 +3568,11 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
  * nested_vmx_run() handles a nested entry, i.e., a VMLAUNCH or VMRESUME on L1
  * for running an L2 nested guest.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5017| <<handle_vmlaunch>> return nested_vmx_run(vcpu, true);
+ *   - arch/x86/kvm/vmx/nested.c|5024| <<handle_vmresume>> return nested_vmx_run(vcpu, false);
+ */
 static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 {
 	struct vmcs12 *vmcs12;
@@ -4743,6 +4835,12 @@ void nested_vmx_pmu_entry_exit_ctls_update(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4940| <<handle_vmon>> if (nested_vmx_get_vmptr(vcpu, &vmptr, &ret))
+ *   - arch/x86/kvm/vmx/nested.c|5019| <<handle_vmclear>> if (nested_vmx_get_vmptr(vcpu, &vmptr, &r))
+ *   - arch/x86/kvm/vmx/nested.c|5278| <<handle_vmptrld>> if (nested_vmx_get_vmptr(vcpu, &vmptr, &r))
+ */
 static int nested_vmx_get_vmptr(struct kvm_vcpu *vcpu, gpa_t *vmpointer,
 				int *ret)
 {
@@ -4792,11 +4890,32 @@ static struct vmcs *alloc_shadow_vmcs(struct kvm_vcpu *vcpu)
 	return loaded_vmcs->shadow_vmcs;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4959| <<handle_vmon>> ret = enter_vmx_operation(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6282| <<vmx_set_nested_state>> ret = enter_vmx_operation(vcpu);
+ */
 static int enter_vmx_operation(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	int r;
 
+	/*
+	 * 在以下使用nested_vmx->vmcs02:
+	 *   - arch/x86/kvm/vmx/nested.c|352| <<free_nested>> free_loaded_vmcs(&vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|606| <<nested_vmx_prepare_msr_bitmap>> unsigned long *msr_bitmap_l0 = to_vmx(vcpu)->nested.vmcs02.msr_bitmap;
+	 *   - arch/x86/kvm/vmx/nested.c|2215| <<prepare_vmcs02_constant_state>> vmcs_write64(MSR_BITMAP, __pa(vmx->nested.vmcs02.msr_bitmap));
+	 *   - arch/x86/kvm/vmx/nested.c|3413| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|4092| <<copy_vmcs02_to_vmcs12_rare>> vmx->loaded_vmcs = &vmx->nested.vmcs02;
+	 *   - arch/x86/kvm/vmx/nested.c|4098| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu, &vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|4850| <<enter_vmx_operation>> r = alloc_loaded_vmcs(&vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|4888| <<enter_vmx_operation>> free_loaded_vmcs(&vmx->nested.vmcs02);
+	 *
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> struct nested_vmx nested;
+	 *    -> struct loaded_vmcs vmcs02;
+	 */
 	r = alloc_loaded_vmcs(&vmx->nested.vmcs02);
 	if (r < 0)
 		goto out_vmcs02;
@@ -4818,6 +4937,12 @@ static int enter_vmx_operation(struct kvm_vcpu *vcpu)
 
 	vmx->nested.vpid02 = allocate_vpid();
 
+	/*
+	 * 在以下使用nested_vmx->vmcs02_initialized:
+	 *   - arch/x86/kvm/vmx/nested.c|2194| <<prepare_vmcs02_constant_state>> if (vmx->nested.vmcs02_initialized)
+	 *   - arch/x86/kvm/vmx/nested.c|2196| <<prepare_vmcs02_constant_state>> vmx->nested.vmcs02_initialized = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4871| <<enter_vmx_operation>> vmx->nested.vmcs02_initialized = false;
+	 */
 	vmx->nested.vmcs02_initialized = false;
 	vmx->nested.vmxon = true;
 
@@ -4849,6 +4974,10 @@ static int enter_vmx_operation(struct kvm_vcpu *vcpu)
  * region. Consequently, VMCLEAR and VMPTRLD also do not verify that the their
  * argument is different from the VMXON pointer (which the spec says they do).
  */
+/*
+ * 在nested_vmx_hardware_setup()中设置:
+ *   - arch/x86/kvm/vmx/nested.c|6688| <<nested_vmx_hardware_setup>> exit_handlers[EXIT_REASON_VMON] = handle_vmon;
+ */
 static int handle_vmon(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -4878,6 +5007,19 @@ static int handle_vmon(struct kvm_vcpu *vcpu)
 		return 1;
 	}
 
+	/*
+	 * struct vcpu_vmx *vmx:
+	 * -> struct nested_vmx nested;
+	 *    -> bool vmxon;
+	 *
+	 * 在以下修改nested_vmx->vmxon:
+	 *   - arch/x86/kvm/vmx/nested.c|290| <<free_nested>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4822| <<enter_vmx_operation>> vmx->nested.vmxon = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6252| <<vmx_set_nested_state>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7539| <<vmx_pre_leave_smm>> vmx->nested.vmxon = true;
+	 *
+	 * has the level 1 guest done vmxon?
+	 */
 	if (vmx->nested.vmxon)
 		return nested_vmx_fail(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);
 
@@ -4905,6 +5047,15 @@ static int handle_vmon(struct kvm_vcpu *vcpu)
 	    revision != VMCS12_REVISION)
 		return nested_vmx_failInvalid(vcpu);
 
+	/*
+	 * 在以下设置nested_vmx->vmxon_ptr:
+	 *   - arch/x86/kvm/vmx/nested.c|4958| <<handle_vmon>> vmx->nested.vmxon_ptr = vmptr;
+	 *   - arch/x86/kvm/vmx/nested.c|6281| <<vmx_set_nested_state>> vmx->nested.vmxon_ptr = kvm_state->hdr.vmx.vmxon_pa;
+	 * 在以下使用nested_vmx->vmxon_ptr:
+	 *   - arch/x86/kvm/vmx/nested.c|5025| <<handle_vmclear>> if (vmptr == vmx->nested.vmxon_ptr)
+	 *   - arch/x86/kvm/vmx/nested.c|5284| <<handle_vmptrld>> if (vmptr == vmx->nested.vmxon_ptr)
+	 *   - arch/x86/kvm/vmx/nested.c|6105| <<vmx_get_nested_state>> kvm_state.hdr.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
+	 */
 	vmx->nested.vmxon_ptr = vmptr;
 	ret = enter_vmx_operation(vcpu);
 	if (ret)
@@ -4972,6 +5123,15 @@ static int handle_vmclear(struct kvm_vcpu *vcpu)
 	if (!page_address_valid(vcpu, vmptr))
 		return nested_vmx_fail(vcpu, VMXERR_VMCLEAR_INVALID_ADDRESS);
 
+	/*
+	 * 在以下设置nested_vmx->vmxon_ptr:
+	 *   - arch/x86/kvm/vmx/nested.c|4958| <<handle_vmon>> vmx->nested.vmxon_ptr = vmptr;
+	 *   - arch/x86/kvm/vmx/nested.c|6281| <<vmx_set_nested_state>> vmx->nested.vmxon_ptr = kvm_state->hdr.vmx.vmxon_pa;
+	 * 在以下使用nested_vmx->vmxon_ptr:
+	 *   - arch/x86/kvm/vmx/nested.c|5025| <<handle_vmclear>> if (vmptr == vmx->nested.vmxon_ptr)
+	 *   - arch/x86/kvm/vmx/nested.c|5284| <<handle_vmptrld>> if (vmptr == vmx->nested.vmxon_ptr)
+	 *   - arch/x86/kvm/vmx/nested.c|6105| <<vmx_get_nested_state>> kvm_state.hdr.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
+	 */
 	if (vmptr == vmx->nested.vmxon_ptr)
 		return nested_vmx_fail(vcpu, VMXERR_VMCLEAR_VMXON_POINTER);
 
@@ -5000,15 +5160,32 @@ static int handle_vmclear(struct kvm_vcpu *vcpu)
 }
 
 /* Emulate the VMLAUNCH instruction */
+/*
+ * 在以下使用handle_vmlaunch():
+ *   - arch/x86/kvm/vmx/nested.c|6800| <<global>> exit_handlers[EXIT_REASON_VMLAUNCH] = handle_vmlaunch;
+ */
 static int handle_vmlaunch(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/vmx/nested.c|5017| <<handle_vmlaunch>> return nested_vmx_run(vcpu, true);
+	 *   - arch/x86/kvm/vmx/nested.c|5024| <<handle_vmresume>> return nested_vmx_run(vcpu, false);
+	 */
 	return nested_vmx_run(vcpu, true);
 }
 
 /* Emulate the VMRESUME instruction */
+/*
+ * 在以下使用handle_vmresume():
+ *   - arch/x86/kvm/vmx/nested.c|6804| <<global>> exit_handlers[EXIT_REASON_VMRESUME] = handle_vmresume;
+ */
 static int handle_vmresume(struct kvm_vcpu *vcpu)
 {
-
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/vmx/nested.c|5017| <<handle_vmlaunch>> return nested_vmx_run(vcpu, true);
+	 *   - arch/x86/kvm/vmx/nested.c|5024| <<handle_vmresume>> return nested_vmx_run(vcpu, false);
+	 */
 	return nested_vmx_run(vcpu, false);
 }
 
@@ -5838,6 +6015,10 @@ static bool nested_vmx_l0_wants_exit(struct kvm_vcpu *vcpu, u32 exit_reason)
  * Return 1 if L1 wants to intercept an exit from L2.  Only call this when in
  * is_guest_mode (L2).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|6003| <<nested_vmx_reflect_vmexit>> if (!nested_vmx_l1_wants_exit(vcpu, exit_reason))
+ */
 static bool nested_vmx_l1_wants_exit(struct kvm_vcpu *vcpu, u32 exit_reason)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -5959,6 +6140,10 @@ static bool nested_vmx_l1_wants_exit(struct kvm_vcpu *vcpu, u32 exit_reason)
  * Conditionally reflect a VM-Exit into L1.  Returns %true if the VM-Exit was
  * reflected into L1.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5964| <<vmx_handle_exit>> if (nested_vmx_reflect_vmexit(vcpu))
+ */
 bool nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6035,6 +6220,13 @@ static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 	vmx = to_vmx(vcpu);
 	vmcs12 = get_vmcs12(vcpu);
 
+	/*
+	 * 在以下修改nested_vmx->vmxon:
+	 *   - arch/x86/kvm/vmx/nested.c|290| <<free_nested>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4822| <<enter_vmx_operation>> vmx->nested.vmxon = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6252| <<vmx_set_nested_state>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7539| <<vmx_pre_leave_smm>> vmx->nested.vmxon = true
+	 */
 	if (nested_vmx_allowed(vcpu) &&
 	    (vmx->nested.vmxon || vmx->nested.smm.vmxon)) {
 		kvm_state.hdr.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
@@ -6335,6 +6527,11 @@ void nested_vmx_set_vmcs_shadowing_bitmap(void)
  * bit in the high half is on if the corresponding bit in the control field
  * may be on. See also vmx_control_verify().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7013| <<vmx_check_processor_compat>> nested_vmx_setup_ctls_msrs(&vmcs_conf.nested, vmx_cap.ept);
+ *   - arch/x86/kvm/vmx/vmx.c|7887| <<hardware_setup>> nested_vmx_setup_ctls_msrs(&vmcs_config.nested,
+ */
 void nested_vmx_setup_ctls_msrs(struct nested_vmx_msrs *msrs, u32 ept_caps)
 {
 	/*
@@ -6580,6 +6777,10 @@ void nested_vmx_hardware_unsetup(void)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7901| <<hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+ */
 __init int nested_vmx_hardware_setup(int (*exit_handlers[])(struct kvm_vcpu *))
 {
 	int i;
@@ -6619,6 +6820,9 @@ __init int nested_vmx_hardware_setup(int (*exit_handlers[])(struct kvm_vcpu *))
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.nested_ops = &vmx_nested_ops
+ */
 struct kvm_x86_nested_ops vmx_nested_ops = {
 	.check_events = vmx_check_nested_events,
 	.hv_timer_pending = nested_vmx_preemption_timer_pending,
diff --git a/arch/x86/kvm/vmx/nested.h b/arch/x86/kvm/vmx/nested.h
index 197148d76b8f..aea9b37e069b 100644
--- a/arch/x86/kvm/vmx/nested.h
+++ b/arch/x86/kvm/vmx/nested.h
@@ -39,11 +39,25 @@ bool nested_vmx_check_io_bitmaps(struct kvm_vcpu *vcpu, unsigned int port,
 
 static inline struct vmcs12 *get_vmcs12(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 */
 	return to_vmx(vcpu)->nested.cached_vmcs12;
 }
 
 static inline struct vmcs12 *get_shadow_vmcs12(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 */
 	return to_vmx(vcpu)->nested.cached_shadow_vmcs12;
 }
 
diff --git a/arch/x86/kvm/vmx/posted_intr.c b/arch/x86/kvm/vmx/posted_intr.c
index f02962dcc72c..af6bae898d0a 100644
--- a/arch/x86/kvm/vmx/posted_intr.c
+++ b/arch/x86/kvm/vmx/posted_intr.c
@@ -9,10 +9,34 @@
 #include "trace.h"
 #include "vmx.h"
 
+/*
+ * 在以下使用POSTED_INTR_DESC_ADDR:
+ *   - arch/x86/include/asm/vmx.h|197| <<global>> POSTED_INTR_DESC_ADDR = 0x00002016,
+ *   - arch/x86/kvm/vmx/vmcs12.c|42| <<global>> FIELD64(POSTED_INTR_DESC_ADDR, posted_intr_desc_addr),
+ *   - arch/x86/kvm/vmx/nested.c|3282| <<nested_get_vmcs12_pages>> vmcs_write64(POSTED_INTR_DESC_ADDR,
+ *   - arch/x86/kvm/vmx/vmx.c|4341| <<init_vmcs>> vmcs_write64(POSTED_INTR_DESC_ADDR, __pa((&vmx->pi_desc)));
+ *
+ *
+ * 在以下使用POSTED_INTR_VECTOR:
+ *   - arch/x86/include/asm/irq_vectors.h|95| <<global>> #define POSTED_INTR_VECTOR 0xf2
+ *   - arch/x86/kernel/idt.c|138| <<global>> INTG(POSTED_INTR_VECTOR, asm_sysvec_kvm_posted_intr_ipi),
+ *   - arch/x86/include/asm/idtentry.h|660| <<SYM_CODE_END>> DECLARE_IDTENTRY_SYSVEC(POSTED_INTR_VECTOR, sysvec_kvm_posted_intr_ipi);
+ *   - arch/x86/kvm/vmx/posted_intr.c|155| <<__pi_post_block>> new.nv = POSTED_INTR_VECTOR;
+ *   - arch/x86/kvm/vmx/vmx.c|3933| <<kvm_vcpu_trigger_posted_interrupt>> int pi_vec = nested ? POSTED_INTR_NESTED_VECTOR : POSTED_INTR_VECTOR;
+ *   - arch/x86/kvm/vmx/vmx.c|4344| <<init_vmcs>> vmcs_write16(POSTED_INTR_NV, POSTED_INTR_VECTOR);
+ *   - arch/x86/kvm/vmx/vmx.c|6988| <<vmx_create_vcpu>> vmx->pi_desc.nv = POSTED_INTR_VECTOR;
+ */
+
 /*
  * We maintian a per-CPU linked-list of vCPU, so in wakeup_handler() we
  * can find which vCPU should be waken up.
  */
+/*
+ * 在以下使用blocked_vcpu_on_cpu:
+ *   - arch/x86/kvm/vmx/posted_intr.c|207| <<pi_pre_block>> list_add_tail(&vcpu->blocked_vcpu_list, &per_cpu(blocked_vcpu_on_cpu, vcpu->pre_pcpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|271| <<pi_wakeup_handler>> list_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/posted_intr.c|283| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));
+ */
 static DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);
 static DEFINE_PER_CPU(spinlock_t, blocked_vcpu_on_cpu_lock);
 
@@ -21,6 +45,10 @@ static inline struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)
 	return &(to_vmx(vcpu)->pi_desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1402| <<vmx_vcpu_load>> vmx_vcpu_pi_load(vcpu, cpu);
+ */
 void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -77,6 +105,36 @@ void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 		pi_set_on(pi_desc);
 }
 
+/*
+ * vmx_vcpu_pi_put
+ * kvm_arch_vcpu_put
+ * kvm_sched_out
+ * __fire_sched_out_preempt_notifiers
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * vmx_vcpu_pi_put
+ * kvm_arch_vcpu_put
+ * vcpu_put
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1409| <<vmx_vcpu_put>> vmx_vcpu_pi_put(vcpu);
+ */
 void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -151,6 +209,12 @@ int pi_pre_block(struct kvm_vcpu *vcpu)
 	if (!WARN_ON_ONCE(vcpu->pre_pcpu != -1)) {
 		vcpu->pre_pcpu = vcpu->cpu;
 		spin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));
+		/*
+		 * 在以下使用blocked_vcpu_on_cpu:
+		 *   - arch/x86/kvm/vmx/posted_intr.c|207| <<pi_pre_block>> list_add_tail(&vcpu->blocked_vcpu_list, &per_cpu(blocked_vcpu_on_cpu, vcpu->pre_pcpu));
+		 *   - arch/x86/kvm/vmx/posted_intr.c|271| <<pi_wakeup_handler>> list_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),
+		 *   - arch/x86/kvm/vmx/posted_intr.c|283| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));
+		 */
 		list_add_tail(&vcpu->blocked_vcpu_list,
 			      &per_cpu(blocked_vcpu_on_cpu,
 				       vcpu->pre_pcpu));
@@ -206,6 +270,10 @@ void pi_post_block(struct kvm_vcpu *vcpu)
 /*
  * Handler for POSTED_INTERRUPT_WAKEUP_VECTOR.
  */
+/*
+ * 在以下使用pi_wakeup_handler():
+ *   - arch/x86/kvm/vmx/vmx.c|7915| <<hardware_setup>> kvm_set_posted_intr_wakeup_handler(pi_wakeup_handler);
+ */
 void pi_wakeup_handler(void)
 {
 	struct kvm_vcpu *vcpu;
@@ -246,6 +314,23 @@ bool pi_has_pending_interrupt(struct kvm_vcpu *vcpu)
  * @set: set or unset PI
  * returns 0 on success, < 0 on failure
  */
+/*
+ * kvm_arch_update_irqfd_routing
+ * kvm_set_irq_routing
+ * kvm_vm_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|11713| <<kvm_arch_irq_bypass_add_producer>> ret = kvm_x86_ops.update_pi_irte(irqfd->kvm,
+ *   - arch/x86/kvm/x86.c|11738| <<kvm_arch_irq_bypass_del_producer>> ret = kvm_x86_ops.update_pi_irte(irqfd->kvm, prod->irq, irqfd->gsi, 0);
+ *   - arch/x86/kvm/x86.c|11749| <<kvm_arch_update_irqfd_routing>> return kvm_x86_ops.update_pi_irte(kvm, host_irq, guest_irq, set);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.update_pi_irte = pi_update_irte()
+ */
 int pi_update_irte(struct kvm *kvm, unsigned int host_irq, uint32_t guest_irq,
 		   bool set)
 {
@@ -313,6 +398,12 @@ int pi_update_irte(struct kvm *kvm, unsigned int host_irq, uint32_t guest_irq,
 		trace_kvm_pi_irte_update(host_irq, vcpu->vcpu_id, e->gsi,
 				vcpu_info.vector, vcpu_info.pi_desc_addr, set);
 
+		/*
+		 * 唯一设置urgent的地方:
+		 *   - drivers/iommu/intel/irq_remapping.c|1222| <<intel_ir_set_vcpu_affinity>> irte_pi.p_urgent = 0;
+		 *
+		 * 最终会调用intel_ir_set_vcpu_affinity()
+		 */
 		if (set)
 			ret = irq_set_vcpu_affinity(host_irq, &vcpu_info);
 		else
diff --git a/arch/x86/kvm/vmx/posted_intr.h b/arch/x86/kvm/vmx/posted_intr.h
index 0bdc41391c5b..1324142dce30 100644
--- a/arch/x86/kvm/vmx/posted_intr.h
+++ b/arch/x86/kvm/vmx/posted_intr.h
@@ -5,8 +5,35 @@
 #define POSTED_INTR_ON  0
 #define POSTED_INTR_SN  1
 
+/*
+ * 在以下使用POSTED_INTR_DESC_ADDR:
+ *   - arch/x86/include/asm/vmx.h|197| <<global>> POSTED_INTR_DESC_ADDR = 0x00002016,
+ *   - arch/x86/kvm/vmx/vmcs12.c|42| <<global>> FIELD64(POSTED_INTR_DESC_ADDR, posted_intr_desc_addr),
+ *   - arch/x86/kvm/vmx/nested.c|3282| <<nested_get_vmcs12_pages>> vmcs_write64(POSTED_INTR_DESC_ADDR,
+ *   - arch/x86/kvm/vmx/vmx.c|4341| <<init_vmcs>> vmcs_write64(POSTED_INTR_DESC_ADDR, __pa((&vmx->pi_desc)));
+ *
+ *
+ * 在以下使用POSTED_INTR_VECTOR:
+ *   - arch/x86/include/asm/irq_vectors.h|95| <<global>> #define POSTED_INTR_VECTOR 0xf2
+ *   - arch/x86/kernel/idt.c|138| <<global>> INTG(POSTED_INTR_VECTOR, asm_sysvec_kvm_posted_intr_ipi),
+ *   - arch/x86/include/asm/idtentry.h|660| <<SYM_CODE_END>> DECLARE_IDTENTRY_SYSVEC(POSTED_INTR_VECTOR, sysvec_kvm_posted_intr_ipi);
+ *   - arch/x86/kvm/vmx/posted_intr.c|155| <<__pi_post_block>> new.nv = POSTED_INTR_VECTOR;
+ *   - arch/x86/kvm/vmx/vmx.c|3933| <<kvm_vcpu_trigger_posted_interrupt>> int pi_vec = nested ? POSTED_INTR_NESTED_VECTOR : POSTED_INTR_VECTOR;
+ *   - arch/x86/kvm/vmx/vmx.c|4344| <<init_vmcs>> vmcs_write16(POSTED_INTR_NV, POSTED_INTR_VECTOR);
+ *   - arch/x86/kvm/vmx/vmx.c|6988| <<vmx_create_vcpu>> vmx->pi_desc.nv = POSTED_INTR_VECTOR;
+ */
+
 /* Posted-Interrupt Descriptor */
 struct pi_desc {
+	/*
+	 * 在以下使用pi_desc->pir:
+	 *   - arch/x86/kvm/vmx/nested.c|3824| <<vmx_complete_nested_posted_interrupt>> max_irr = find_last_bit((unsigned long *)vmx->nested.pi_desc->pir, 256);
+	 *   - arch/x86/kvm/vmx/nested.c|3830| <<vmx_complete_nested_posted_interrupt>> __kvm_apic_update_irr(vmx->nested.pi_desc->pir,
+	 *   - arch/x86/kvm/vmx/posted_intr.h|50| <<pi_test_and_set_pir>> return test_and_set_bit(vector, (unsigned long *)pi_desc->pir);
+	 *   - arch/x86/kvm/vmx/posted_intr.h|59| <<pi_is_pir_empty>> return bitmap_empty((unsigned long *)pi_desc->pir, NR_VECTORS);
+	 *   - arch/x86/kvm/vmx/vmx.c|6330| <<vmx_sync_pir_to_irr>> kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
+	 *   - arch/x86/kvm/vmx/vmx.c|6369| <<vmx_apicv_post_state_restore>> memset(vmx->pi_desc.pir, 0, sizeof(vmx->pi_desc.pir));
+	 */
 	u32 pir[8];     /* Posted interrupt requested */
 	union {
 		struct {
@@ -21,6 +48,15 @@ struct pi_desc {
 				/* bit 287:280 - Reserved */
 			u8	rsvd_2;
 				/* bit 319:288 - Notification Destination */
+			/*
+			 * 在以下修改pi_desc->ndst:
+			 *   - arch/x86/kvm/vmx/posted_intr.c|62| <<vmx_vcpu_pi_load>> new.ndst = dest;
+			 *   - arch/x86/kvm/vmx/posted_intr.c|64| <<vmx_vcpu_pi_load>> new.ndst = (dest << 8) & 0xFF00;
+			 *   - arch/x86/kvm/vmx/posted_intr.c|142| <<__pi_post_block>> new.ndst = dest;
+			 *   - arch/x86/kvm/vmx/posted_intr.c|144| <<__pi_post_block>> new.ndst = (dest << 8) & 0xFF00;
+			 *   - arch/x86/kvm/vmx/posted_intr.c|212| <<pi_pre_block>> new.ndst = dest;
+			 *   - arch/x86/kvm/vmx/posted_intr.c|214| <<pi_pre_block>> new.ndst = (dest << 8) & 0xFF00;
+			 */
 			u32	ndst;
 		};
 		u64 control;
@@ -40,11 +76,20 @@ static inline bool pi_test_and_clear_on(struct pi_desc *pi_desc)
 			(unsigned long *)&pi_desc->control);
 }
 
+/*
+ * __apic_accept_irq()
+ * -> kvm_x86_ops.deliver_posted_interrupt = vmx_deliver_posted_interrupt()
+ *    -> pi_test_and_set_pir()
+ */
 static inline int pi_test_and_set_pir(int vector, struct pi_desc *pi_desc)
 {
 	return test_and_set_bit(vector, (unsigned long *)pi_desc->pir);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|95| <<vmx_vcpu_pi_put>> pi_set_sn(pi_desc);
+ */
 static inline bool pi_is_pir_empty(struct pi_desc *pi_desc)
 {
 	return bitmap_empty((unsigned long *)pi_desc->pir, NR_VECTORS);
diff --git a/arch/x86/kvm/vmx/vmcs12.c b/arch/x86/kvm/vmx/vmcs12.c
index c8e51c004f78..17a64bc76564 100644
--- a/arch/x86/kvm/vmx/vmcs12.c
+++ b/arch/x86/kvm/vmx/vmcs12.c
@@ -150,4 +150,11 @@ const unsigned short vmcs_field_to_offset_table[] = {
 	FIELD(HOST_RSP, host_rsp),
 	FIELD(HOST_RIP, host_rip),
 };
+/*
+ * 在以下使用nr_vmcs12_fields:
+ *   - arch/x86/kvm/vmx/vmcs12.c|153| <<global>> const unsigned int nr_vmcs12_fields = ARRAY_SIZE(vmcs_field_to_offset_table);
+ *   - arch/x86/kvm/vmx/vmcs12.h|367| <<global>> extern const unsigned int nr_vmcs12_fields;
+ *   - arch/x86/kvm/vmx/vmcs12.h|385| <<vmcs_field_to_offset>> if (index >= nr_vmcs12_fields)
+ *   - arch/x86/kvm/vmx/vmcs12.h|388| <<vmcs_field_to_offset>> index = array_index_nospec(index, nr_vmcs12_fields);
+ */
 const unsigned int nr_vmcs12_fields = ARRAY_SIZE(vmcs_field_to_offset_table);
diff --git a/arch/x86/kvm/vmx/vmcs12.h b/arch/x86/kvm/vmx/vmcs12.h
index 80232daf00ff..5cb3dcadef0a 100644
--- a/arch/x86/kvm/vmx/vmcs12.h
+++ b/arch/x86/kvm/vmx/vmcs12.h
@@ -364,10 +364,22 @@ static inline void vmx_check_vmcs12_offsets(void)
 }
 
 extern const unsigned short vmcs_field_to_offset_table[];
+/*
+ * 在以下使用nr_vmcs12_fields:
+ *   - arch/x86/kvm/vmx/vmcs12.c|153| <<global>> const unsigned int nr_vmcs12_fields = ARRAY_SIZE(vmcs_field_to_offset_table);
+ *   - arch/x86/kvm/vmx/vmcs12.h|367| <<global>> extern const unsigned int nr_vmcs12_fields;
+ *   - arch/x86/kvm/vmx/vmcs12.h|385| <<vmcs_field_to_offset>> if (index >= nr_vmcs12_fields)
+ *   - arch/x86/kvm/vmx/vmcs12.h|388| <<vmcs_field_to_offset>> index = array_index_nospec(index, nr_vmcs12_fields);
+ */
 extern const unsigned int nr_vmcs12_fields;
 
 #define ROL16(val, n) ((u16)(((u16)(val) << (n)) | ((u16)(val) >> (16 - (n)))))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5153| <<handle_vmread>> offset = vmcs_field_to_offset(field);
+ *   - arch/x86/kvm/vmx/nested.c|5256| <<handle_vmwrite>> offset = vmcs_field_to_offset(field);
+ */
 static inline short vmcs_field_to_offset(unsigned long field)
 {
 	unsigned short offset;
@@ -377,6 +389,13 @@ static inline short vmcs_field_to_offset(unsigned long field)
 		return -ENOENT;
 
 	index = ROL16(field, 6);
+	/*
+	 * 在以下使用nr_vmcs12_fields:
+	 *   - arch/x86/kvm/vmx/vmcs12.c|153| <<global>> const unsigned int nr_vmcs12_fields = ARRAY_SIZE(vmcs_field_to_offset_table);
+	 *   - arch/x86/kvm/vmx/vmcs12.h|367| <<global>> extern const unsigned int nr_vmcs12_fields;
+	 *   - arch/x86/kvm/vmx/vmcs12.h|385| <<vmcs_field_to_offset>> if (index >= nr_vmcs12_fields)
+	 *   - arch/x86/kvm/vmx/vmcs12.h|388| <<vmcs_field_to_offset>> index = array_index_nospec(index, nr_vmcs12_fields);
+	 */
 	if (index >= nr_vmcs12_fields)
 		return -ENOENT;
 
@@ -389,9 +408,26 @@ static inline short vmcs_field_to_offset(unsigned long field)
 
 #undef ROL16
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|1635| <<copy_vmcs12_to_shadow>> val = vmcs12_read_any(vmcs12, field.encoding,
+ *   - arch/x86/kvm/vmx/nested.c|5217| <<handle_vmread>> value = vmcs12_read_any(vmcs12, field, offset);
+ */
 static inline u64 vmcs12_read_any(struct vmcs12 *vmcs12, unsigned long field,
 				  u16 offset)
 {
+	/*
+	 * struct vmcs12 describes the state that our guest hypervisor (L1) keeps for a
+	 * single nested guest (L2), hence the name vmcs12. Any VMX implementation has
+	 * a VMCS structure, and vmcs12 is our emulated VMX's VMCS. This structure is
+	 * stored in guest memory specified by VMPTRLD, but is opaque to the guest,
+	 * which must access it using VMREAD/VMWRITE/VMCLEAR instructions.
+	 * More than one of these structures may exist, if L1 runs multiple L2 guests.
+	 * nested_vmx_run() will use the data here to build the vmcs02: a VMCS for the
+	 * underlying hardware which will be used to run L2.
+	 * This structure is packed to ensure that its layout is identical across
+	 * machines (necessary for live migration).
+	 */
 	char *p = (char *)vmcs12 + offset;
 
 	switch (vmcs_field_width(field)) {
@@ -409,9 +445,26 @@ static inline u64 vmcs12_read_any(struct vmcs12 *vmcs12, unsigned long field,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|1602| <<copy_shadow_to_vmcs12>> vmcs12_write_any(vmcs12, field.encoding, field.offset, val);
+ *   - arch/x86/kvm/vmx/nested.c|5342| <<handle_vmwrite>> vmcs12_write_any(vmcs12, field, offset, value);
+ */
 static inline void vmcs12_write_any(struct vmcs12 *vmcs12, unsigned long field,
 				    u16 offset, u64 field_value)
 {
+	/*
+	 * struct vmcs12 describes the state that our guest hypervisor (L1) keeps for a
+	 * single nested guest (L2), hence the name vmcs12. Any VMX implementation has
+	 * a VMCS structure, and vmcs12 is our emulated VMX's VMCS. This structure is
+	 * stored in guest memory specified by VMPTRLD, but is opaque to the guest,
+	 * which must access it using VMREAD/VMWRITE/VMCLEAR instructions.
+	 * More than one of these structures may exist, if L1 runs multiple L2 guests.
+	 * nested_vmx_run() will use the data here to build the vmcs02: a VMCS for the
+	 * underlying hardware which will be used to run L2.
+	 * This structure is packed to ensure that its layout is identical across
+	 * machines (necessary for live migration).
+	 */
 	char *p = (char *)vmcs12 + offset;
 
 	switch (vmcs_field_width(field)) {
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index eb69fef57485..1f38bc24cd88 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -1759,6 +1759,17 @@ static u64 vmx_write_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)
  * all guests if the "nested" module option is off, and can also be disabled
  * for a single guest by disabling its VMX cpuid bit.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4729| <<nested_vmx_pmu_entry_exit_ctls_update>> if (!nested_vmx_allowed(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|6038| <<vmx_get_nested_state>> if (nested_vmx_allowed(vcpu) &&
+ *   - arch/x86/kvm/vmx/nested.c|6174| <<vmx_set_nested_state>> if (!nested_vmx_allowed(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|6208| <<vmx_set_nested_state>> (!nested_vmx_allowed(vcpu) || !vmx->nested.enlightened_vmcs_enabled))
+ *   - arch/x86/kvm/vmx/vmx.c|1860| <<vmx_get_msr>> if (!nested_vmx_allowed(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|2128| <<vmx_set_msr>> if (!nested_vmx_allowed(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|7233| <<vmx_vcpu_after_set_cpuid>> if (nested_vmx_allowed(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|7242| <<vmx_vcpu_after_set_cpuid>> if (nested_vmx_allowed(vcpu)) {
+ */
 bool nested_vmx_allowed(struct kvm_vcpu *vcpu)
 {
 	return nested && guest_cpuid_has(vcpu, X86_FEATURE_VMX);
@@ -2610,6 +2621,11 @@ void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
 	WARN_ON(loaded_vmcs->shadow_vmcs != NULL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4850| <<enter_vmx_operation>> r = alloc_loaded_vmcs(&vmx->nested.vmcs02);
+ *   - arch/x86/kvm/vmx/vmx.c|6900| <<vmx_create_vcpu>> err = alloc_loaded_vmcs(&vmx->vmcs01);
+ */
 int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
 {
 	loaded_vmcs->vmcs = alloc_vmcs(false);
@@ -3976,6 +3992,12 @@ static int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,
  * 2. If target vcpu isn't running(root mode), kick it to pick up the
  * interrupt from PIR in next vmentry.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1093| <<__apic_accept_irq>> if (kvm_x86_ops.deliver_posted_interrupt(vcpu, vector)) {
+ *
+ * struct kvm_x86_ops vmx_x86_ops.deliver_posted_interrupt = vmx_deliver_posted_interrupt()
+ */
 static int vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4287,6 +4309,10 @@ static void ept_set_mmio_spte_mask(void)
  * Noting that the initialization of Guest-state Area of VMCS is in
  * vmx_vcpu_reset().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6954| <<vmx_create_vcpu>> init_vmcs(vmx);
+ */
 static void init_vmcs(struct vcpu_vmx *vmx)
 {
 	if (nested)
@@ -4876,6 +4902,9 @@ static int handle_triple_fault(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_IO_INSTRUCTION] = handle_io()
+ */
 static int handle_io(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
@@ -5605,6 +5634,13 @@ static int handle_encls(struct kvm_vcpu *vcpu)
  * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
  * to be done to userspace and return 0.
  */
+/*
+ * 在以下使用kvm_vmx_exit_handlers[]:
+ *   - arch/x86/kvm/vmx/vmx.c|5678| <<global>> static const int kvm_vmx_max_exit_handlers = ARRAY_SIZE(kvm_vmx_exit_handlers);
+ *   - arch/x86/kvm/vmx/vmx.c|6061| <<vmx_handle_exit>> if (!kvm_vmx_exit_handlers[exit_reason])
+ *   - arch/x86/kvm/vmx/vmx.c|6064| <<vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_reason](vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7906| <<hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+ */
 static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 	[EXIT_REASON_EXCEPTION_NMI]           = handle_exception_nmi,
 	[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,
@@ -5908,6 +5944,12 @@ void dump_vmcs(void)
  * The guest has exited.  See if we can fix it or if we need userspace
  * assistance.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9089| <<vcpu_enter_guest>> r = kvm_x86_ops.handle_exit(vcpu, exit_fastpath);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.handle_exit = vmx_handle_exit()
+ */
 static int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6531,6 +6573,10 @@ static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
 					msrs[i].host, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6701| <<vmx_vcpu_run>> vmx_update_hv_timer(vcpu);
+ */
 static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 9d3a557949ac..58dee7141605 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -76,11 +76,47 @@ struct pt_desc {
  */
 struct nested_vmx {
 	/* Has the level1 guest done vmxon? */
+	/*
+	 * 在以下修改nested_vmx->vmxon:
+	 *   - arch/x86/kvm/vmx/nested.c|290| <<free_nested>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4822| <<enter_vmx_operation>> vmx->nested.vmxon = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6252| <<vmx_set_nested_state>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7539| <<vmx_pre_leave_smm>> vmx->nested.vmxon = true;
+	 */
 	bool vmxon;
+	/*
+	 * 在以下设置nested_vmx->vmxon_ptr:
+	 *   - arch/x86/kvm/vmx/nested.c|4958| <<handle_vmon>> vmx->nested.vmxon_ptr = vmptr;
+	 *   - arch/x86/kvm/vmx/nested.c|6281| <<vmx_set_nested_state>> vmx->nested.vmxon_ptr = kvm_state->hdr.vmx.vmxon_pa;
+	 * 在以下使用nested_vmx->vmxon_ptr:
+	 *   - arch/x86/kvm/vmx/nested.c|5025| <<handle_vmclear>> if (vmptr == vmx->nested.vmxon_ptr)
+	 *   - arch/x86/kvm/vmx/nested.c|5284| <<handle_vmptrld>> if (vmptr == vmx->nested.vmxon_ptr)
+	 *   - arch/x86/kvm/vmx/nested.c|6105| <<vmx_get_nested_state>> kvm_state.hdr.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
+	 */
 	gpa_t vmxon_ptr;
 	bool pml_full;
 
 	/* The guest-physical address of the current VMCS L1 keeps for L2 */
+	/*
+	 * 在以下设置nested_vmx->current_vmptr:
+	 *   - arch/x86/kvm/vmx/nested.c|328| <<free_nested>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx/nested.c|2039| <<nested_vmx_handle_enlightened_vmptrld>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx/nested.c|5049| <<nested_release_vmcs12>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx/nested.c|5317| <<set_current_vmptr>> vmx->nested.current_vmptr = vmptr;
+	 *   - arch/x86/kvm/vmx/vmx.c|6953| <<vmx_create_vcpu>> vmx->nested.current_vmptr = -1ull;
+	 * 在以下使用nested_vmx->current_vmptr:
+	 *   - arch/x86/kvm/vmx/nested.c|219| <<nested_vmx_fail>> if (vmx->nested.current_vmptr == -1ull && !vmx->nested.hv_evmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|3553| <<nested_vmx_run>> if (CC(!vmx->nested.hv_evmcs && vmx->nested.current_vmptr == -1ull))
+	 *   - arch/x86/kvm/vmx/nested.c|5029| <<nested_release_vmcs12>> if (vmx->nested.current_vmptr == -1ull)
+	 *   - arch/x86/kvm/vmx/nested.c|5044| <<nested_release_vmcs12>> vmx->nested.current_vmptr >> PAGE_SHIFT,
+	 *   - arch/x86/kvm/vmx/nested.c|5099| <<handle_vmclear>> if (vmptr == vmx->nested.current_vmptr)
+	 *   - arch/x86/kvm/vmx/nested.c|5145| <<handle_vmread>> if (vmx->nested.current_vmptr == -1ull ||
+	 *   - arch/x86/kvm/vmx/nested.c|5237| <<handle_vmwrite>> if (vmx->nested.current_vmptr == -1ull ||
+	 *   - arch/x86/kvm/vmx/nested.c|5350| <<handle_vmptrld>> if (vmx->nested.current_vmptr != vmptr) {
+	 *   - arch/x86/kvm/vmx/nested.c|5395| <<handle_vmptrst>> gpa_t current_vmptr = to_vmx(vcpu)->nested.current_vmptr;
+	 *   - arch/x86/kvm/vmx/nested.c|6165| <<vmx_get_nested_state>> kvm_state.hdr.vmx.vmcs12_pa = vmx->nested.current_vmptr;
+	 *   - arch/x86/kvm/vmx/nested.h|79| <<vmx_has_valid_vmcs12>> return is_guest_mode(vcpu) || vmx->nested.current_vmptr != -1ull ||
+	 */
 	gpa_t current_vmptr;
 	/*
 	 * Cache of the guest's VMCS, existing outside of guest memory.
@@ -113,6 +149,12 @@ struct nested_vmx {
 	 * vmcs02 has been written to the backing VMCS.  Initialization
 	 * is delayed until L1 actually attempts to run a nested VM.
 	 */
+	/*
+	 * 在以下使用nested_vmx->vmcs02_initialized:
+	 *   - arch/x86/kvm/vmx/nested.c|2194| <<prepare_vmcs02_constant_state>> if (vmx->nested.vmcs02_initialized)
+	 *   - arch/x86/kvm/vmx/nested.c|2196| <<prepare_vmcs02_constant_state>> vmx->nested.vmcs02_initialized = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4871| <<enter_vmx_operation>> vmx->nested.vmcs02_initialized = false;
+	 */
 	bool vmcs02_initialized;
 
 	bool change_vmcs01_virtual_apic_mode;
@@ -131,6 +173,17 @@ struct nested_vmx {
 	/* Pending MTF VM-exit into L1.  */
 	bool mtf_pending;
 
+	/*
+	 * 在以下使用nested_vmx->vmcs02:
+	 *   - arch/x86/kvm/vmx/nested.c|352| <<free_nested>> free_loaded_vmcs(&vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|606| <<nested_vmx_prepare_msr_bitmap>> unsigned long *msr_bitmap_l0 = to_vmx(vcpu)->nested.vmcs02.msr_bitmap;
+	 *   - arch/x86/kvm/vmx/nested.c|2215| <<prepare_vmcs02_constant_state>> vmcs_write64(MSR_BITMAP, __pa(vmx->nested.vmcs02.msr_bitmap));
+	 *   - arch/x86/kvm/vmx/nested.c|3413| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|4092| <<copy_vmcs02_to_vmcs12_rare>> vmx->loaded_vmcs = &vmx->nested.vmcs02;
+	 *   - arch/x86/kvm/vmx/nested.c|4098| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu, &vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|4850| <<enter_vmx_operation>> r = alloc_loaded_vmcs(&vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|4888| <<enter_vmx_operation>> free_loaded_vmcs(&vmx->nested.vmcs02);
+	 */
 	struct loaded_vmcs vmcs02;
 
 	/*
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 1b404e4d7dd8..69449129addd 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -123,6 +123,12 @@ module_param(report_ignored_msrs, bool, S_IRUGO | S_IWUSR);
 unsigned int min_timer_period_us = 200;
 module_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);
 
+/*
+ * 在以下使用kvmclock_periodic_sync:
+ *   - arch/x86/kvm/x86.c|127| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+ *   - arch/x86/kvm/x86.c|2845| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+ *   - arch/x86/kvm/x86.c|10159| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+ */
 static bool __read_mostly kvmclock_periodic_sync = true;
 module_param(kvmclock_periodic_sync, bool, S_IRUGO);
 
@@ -1871,6 +1877,11 @@ static int do_set_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 
 #ifdef CONFIG_X86_64
 struct pvclock_clock {
+	/*
+	 * 在以下修改pvclock_clock->vclock_mode:
+	 *   - arch/x86/kvm/x86.c|1933| <<update_pvclock_gtod>> vdata->clock.vclock_mode = tk->tkr_mono.clock->vdso_clock_mode;
+	 *   - arch/x86/kvm/x86.c|1944| <<update_pvclock_gtod>> vdata->raw_clock.vclock_mode = tk->tkr_raw.clock->vdso_clock_mode;
+	 */
 	int vclock_mode;
 	u64 cycle_last;
 	u64 mask;
@@ -1886,19 +1897,80 @@ struct pvclock_gtod_data {
 	struct pvclock_clock clock; /* extract of a clocksource struct */
 	struct pvclock_clock raw_clock; /* extract of a clocksource struct */
 
+	/*
+	 * 在以下使用pvclock_gtod_data->offs_boot:
+	 *   - arch/x86/kvm/x86.c|1926| <<update_pvclock_gtod>> vdata->offs_boot = tk->offs_boot;
+	 *   - arch/x86/kvm/x86.c|1934| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+	 *   - arch/x86/kvm/x86.c|2443| <<do_monotonic_raw>> ns += ktime_to_ns(ktime_add(gtod->raw_clock.offset, gtod->offs_boot));
+	 *
+	 * Offset clock monotonic -> clock boottime
+	 */
 	ktime_t		offs_boot;
+	/*
+	 * 在以下使用pvclock_gtod_data->wall_time_sec:
+	 *   - arch/x86/kvm/x86.c|1957| <<update_pvclock_gtod>> vdata->wall_time_sec = tk->xtime_sec;
+	 *   - arch/x86/kvm/x86.c|2640| <<do_realtime>> ts->tv_sec = gtod->wall_time_sec;
+	 */
 	u64		wall_time_sec;
 };
 
+/*
+ * 在以下使用pvclock_gtod_data:
+ *   - arch/x86/kvm/x86.c|1903| <<update_pvclock_gtod>> struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|1934| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+ *   - arch/x86/kvm/x86.c|2164| <<kvm_track_tsc_matching>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2248| <<kvm_check_tsc_unstable>> if (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)
+ *   - arch/x86/kvm/x86.c|2378| <<read_tsc>> u64 last = pvclock_gtod_data.clock.cycle_last;
+ *   - arch/x86/kvm/x86.c|2433| <<do_monotonic_raw>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2452| <<do_realtime>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2475| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2487| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2574| <<pvclock_update_vm_gtod_copy>> vclock_mode = pvclock_gtod_data.clock.vclock_mode;
+ *   - arch/x86/kvm/x86.c|8007| <<pvclock_gtod_notify>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ */
 static struct pvclock_gtod_data pvclock_gtod_data;
 
+/*
+ * 一个例子:
+ * pvclock_gtod_notify
+ * raw_notifier_call_chain
+ * timekeeping_update
+ * update_wall_time
+ * tick_do_update_jiffies64.part.13
+ * tick_irq_enter
+ * irq_enter
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|8256| <<pvclock_gtod_notify>> update_pvclock_gtod(tk);
+ */
 static void update_pvclock_gtod(struct timekeeper *tk)
 {
 	struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
 
 	write_seqcount_begin(&vdata->seq);
 
+	/*
+	 * - CLOCK_REALTIME clock gives the time passed since January 1, 1970.
+	 *   This clock is affected by NTP adjustments and can jump forward and
+	 *   backward when a system administrator adjusts system time.
+	 *
+	 * - CLOCK_MONOTONIC clock gives the time since a fixed starting
+	 *   point-usually since you booted the system. This clock is affected by NTP,
+	 *   but it can't jump backward.
+	 *
+	 * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this
+	 *   clock is not affected by NTP adjustments.
+	 *
+	 * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but less-accurate
+	 *   variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+	 */
+
 	/* copy pvclock gtod data */
+	/*
+	 * CLOCK_MONOTONIC
+	 */
 	vdata->clock.vclock_mode	= tk->tkr_mono.clock->vdso_clock_mode;
 	vdata->clock.cycle_last		= tk->tkr_mono.cycle_last;
 	vdata->clock.mask		= tk->tkr_mono.mask;
@@ -1907,6 +1979,9 @@ static void update_pvclock_gtod(struct timekeeper *tk)
 	vdata->clock.base_cycles	= tk->tkr_mono.xtime_nsec;
 	vdata->clock.offset		= tk->tkr_mono.base;
 
+	/*
+	 * CLOCK_MONOTONIC_RAW
+	 */
 	vdata->raw_clock.vclock_mode	= tk->tkr_raw.clock->vdso_clock_mode;
 	vdata->raw_clock.cycle_last	= tk->tkr_raw.cycle_last;
 	vdata->raw_clock.mask		= tk->tkr_raw.mask;
@@ -1915,15 +1990,46 @@ static void update_pvclock_gtod(struct timekeeper *tk)
 	vdata->raw_clock.base_cycles	= tk->tkr_raw.xtime_nsec;
 	vdata->raw_clock.offset		= tk->tkr_raw.base;
 
+	/*
+	 * Current CLOCK_REALTIME time in seconds
+	 */
 	vdata->wall_time_sec            = tk->xtime_sec;
 
+	/*
+	 * Offset clock monotonic -> clock boottime
+	 */
 	vdata->offs_boot		= tk->offs_boot;
 
 	write_seqcount_end(&vdata->seq);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2270| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|2622| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+ *   - arch/x86/kvm/x86.c|2638| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+ *   - arch/x86/kvm/x86.c|2768| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|10551| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+ *
+ * 返回的是基于CLOCK_MONOTONIC_RAW的host启动了的时间
+ */
 static s64 get_kvmclock_base_ns(void)
 {
+	/*
+	 * - CLOCK_REALTIME clock gives the time passed since January 1, 1970.
+	 *   This clock is affected by NTP adjustments and can jump forward and
+	 *   backward when a system administrator adjusts system time.
+	 *
+	 * - CLOCK_MONOTONIC clock gives the time since a fixed starting
+	 *   point-usually since you booted the system. This clock is affected by NTP,
+	 *   but it can't jump backward.
+	 *
+	 * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this
+	 *   clock is not affected by NTP adjustments.
+	 *
+	 * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but less-accurate
+	 *   variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+	 */
 	/* Count up from boot time, but with the frequency of the raw clock.  */
 	return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
 }
@@ -1935,6 +2041,11 @@ static s64 get_kvmclock_base_ns(void)
 }
 #endif
 
+/*
+ * 处理MSR_KVM_WALL_CLOCK或者MSR_KVM_WALL_CLOCK_NEW:
+ *   - arch/x86/kvm/x86.c|3410| <<kvm_set_msr_common>> kvm_write_wall_clock(vcpu->kvm, data);
+ *   - arch/x86/kvm/x86.c|3416| <<kvm_set_msr_common>> kvm_write_wall_clock(vcpu->kvm, data);
+ */
 static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock)
 {
 	int version;
@@ -1964,6 +2075,11 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock)
 	 * system time (updated by kvm_guest_time_update below) to the
 	 * wall clock specified here.  We do the reverse here.
 	 */
+	/*
+	 * 当前时间减去VM已经运行的system time就是wall (boot) time???
+	 *
+	 * 当前时间减去VM启动了的时间, 就是vm的wall clock
+	 */
 	wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
 
 	wc.nsec = do_div(wall_nsec, 1000000000);
@@ -1976,6 +2092,11 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock)
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3504| <<kvm_set_msr_common>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|3510| <<kvm_set_msr_common>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
@@ -2010,6 +2131,14 @@ static uint32_t div_frac(uint32_t dividend, uint32_t divisor)
 	return dividend;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2202| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+ *   - arch/x86/kvm/x86.c|2815| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+ *   - arch/x86/kvm/x86.c|2997| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+ *
+ * 计算pvclock的shift和multiplier
+ */
 static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 			       s8 *pshift, u32 *pmultiplier)
 {
@@ -2039,9 +2168,27 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用kvm_guest_has_master_clock:
+ *   - arch/x86/kvm/x86.c|2688| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+ *   - arch/x86/kvm/x86.c|8122| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+ *   - arch/x86/kvm/x86.c|8143| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+ */
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
+/*
+ * 在以下使用percpu的cpu_tsc_khz:
+ *   - arch/x86/kvm/x86.c|2611| <<get_kvmclock_ns>> if (__this_cpu_read(cpu_tsc_khz)) {
+ *   - arch/x86/kvm/x86.c|2612| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+ *   - arch/x86/kvm/x86.c|2739| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|7734| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+ *   - arch/x86/kvm/x86.c|7749| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+ *   - arch/x86/kvm/x86.c|7767| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+ *
+ * 若支持TSC Scaling,则Guset TSC的运行频率为cpu_tsc_khz * scale
+ * 若不支持TSC Scaling,则Guest TSC的运行频率为cpu_tsc_khz,即与Host相同
+ */
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
 static unsigned long max_tsc_khz;
 
@@ -2052,6 +2199,10 @@ static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 	return v;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2181| <<kvm_set_tsc_khz>> return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
+ */
 static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 {
 	u64 ratio;
@@ -2064,6 +2215,9 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 
 	/* TSC scaling supported? */
 	if (!kvm_has_tsc_control) {
+		/*
+		 * tsc_khz为Host的pTSCfreq
+		 */
 		if (user_tsc_khz > tsc_khz) {
 			vcpu->arch.tsc_catchup = 1;
 			vcpu->arch.tsc_always_catchup = 1;
@@ -2088,6 +2242,11 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5382| <<kvm_arch_vcpu_ioctl>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|10406| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, max_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2104,6 +2263,24 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
 			   &vcpu->arch.virtual_tsc_shift,
 			   &vcpu->arch.virtual_tsc_mult);
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2266| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/hyperv.c|1426| <<kvm_hv_get_msr>> data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
+	 *   - arch/x86/kvm/lapic.c|1552| <<__wait_lapic_expire>> do_div(delay_ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1572| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1577| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1662| <<start_sw_tscdeadline>> unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/vmx/nested.c|2173| <<vmx_start_preemption_timer>> if (vcpu->arch.virtual_tsc_khz == 0)
+	 *   - arch/x86/kvm/vmx/nested.c|2178| <<vmx_start_preemption_timer>> do_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/vmx/nested.c|4022| <<vmx_get_preemption_timer_value>> value = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2491| <<kvm_synchronize_tsc>> if (vcpu->arch.virtual_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2502| <<kvm_synchronize_tsc>> u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
+	 *   - arch/x86/kvm/x86.c|2520| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2565| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|5440| <<kvm_arch_vcpu_ioctl>> r = vcpu->arch.virtual_tsc_khz;
+	 */
 	vcpu->arch.virtual_tsc_khz = user_tsc_khz;
 
 	/*
@@ -2135,6 +2312,10 @@ static inline int gtod_is_based_on_tsc(int mode)
 	return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2570| <<kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_X86_64
@@ -2153,6 +2334,9 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	 * and the vcpus need to have matched TSCs.  When that happens,
 	 * perform request to enable masterclock.
 	 */
+	/*
+	 * 调用kvm_gen_update_masterclock()
+	 */
 	if (ka->use_master_clock ||
 	    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
 		kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -2178,11 +2362,24 @@ static inline u64 __scale_tsc(u64 ratio, u64 tsc)
 	return mul_u64_u64_shr(tsc, ratio, kvm_tsc_scaling_ratio_frac_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2218| <<kvm_compute_tsc_offset>> tsc = kvm_scale_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/x86.c|2225| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+ *   - arch/x86/kvm/x86.c|2369| <<adjust_tsc_offset_host>> adjustment = kvm_scale_tsc(vcpu, (u64) adjustment);
+ *   - arch/x86/kvm/x86.c|2799| <<kvm_guest_time_update>> tgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz);
+ *   - arch/x86/kvm/x86.c|3573| <<kvm_get_msr_common>> msr_info->data = kvm_scale_tsc(vcpu, rdtsc()) + tsc_offset;
+ *
+ * 比如参数tsc是host的tsc
+ */
 u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc)
 {
 	u64 _tsc = tsc;
 	u64 ratio = vcpu->arch.tsc_scaling_ratio;
 
+	/*
+	 * Multiply tsc by a fixed point number represented by ratio.
+	 */
 	if (ratio != kvm_default_tsc_scaling_ratio)
 		_tsc = __scale_tsc(ratio, tsc);
 
@@ -2190,6 +2387,16 @@ u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc)
 }
 EXPORT_SYMBOL_GPL(kvm_scale_tsc);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2269| <<kvm_synchronize_tsc>> offset = kvm_compute_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|2308| <<kvm_synchronize_tsc>> offset = kvm_compute_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3266| <<kvm_set_msr_common>> u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+ *   - arch/x86/kvm/x86.c|4140| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu,
+ *
+ * 先通过kvm_scale_tsc(vcpu, rdtsc())把host的tsc给转换成guest tsc
+ * 然后计算两者的offset
+ */
 static u64 kvm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
@@ -2199,18 +2406,48 @@ static u64 kvm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 	return target_tsc - tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|529| <<get_time_ref_counter>> tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1593| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1672| <<start_sw_tscdeadline>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1756| <<set_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/lapic.c|1780| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/vmx/nested.c|995| <<nested_vmx_get_vmexit_msr_value>> *data = kvm_read_l1_tsc(vcpu, val);
+ *   - arch/x86/kvm/vmx/nested.c|2148| <<vmx_calc_preemption_timer_value>> u64 l1_scaled_tsc = kvm_read_l1_tsc(vcpu, rdtsc()) >>
+ *   - arch/x86/kvm/vmx/vmx.c|7445| <<vmx_set_hv_timer>> guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
+ *   - arch/x86/kvm/x86.c|2890| <<kvm_guest_time_update>> tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
+ *   - arch/x86/kvm/x86.c|8322| <<kvm_pv_clock_pairing>> clock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);
+ *   - arch/x86/kvm/x86.c|9323| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *
+ * 获得(计算)L1 VM的tsc, 应该等价在VM里运行rdtsc?
+ */
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
 	return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
 }
 EXPORT_SYMBOL_GPL(kvm_read_l1_tsc);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2335| <<kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2353| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|4133| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)
 {
 	vcpu->arch.l1_tsc_offset = offset;
 	vcpu->arch.tsc_offset = kvm_x86_ops.write_l1_tsc_offset(vcpu, offset);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2492| <<kvm_synchronize_tsc>> if (!kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|4418| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|4424| <<kvm_arch_vcpu_load>> if (kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|10389| <<kvm_arch_vcpu_precreate>> if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
+ *   - arch/x86/kvm/x86.c|10652| <<kvm_arch_hardware_enable>> stable = !kvm_check_tsc_unstable();
+ */
 static inline bool kvm_check_tsc_unstable(void)
 {
 #ifdef CONFIG_X86_64
@@ -2224,6 +2461,24 @@ static inline bool kvm_check_tsc_unstable(void)
 	return check_tsc_unstable();
 }
 
+/*
+ * 100个vcpu: 200次
+ * kvm_set_msr_common
+ * kvm_set_msr
+ * do_set_msr
+ * msr_io
+ * kvm_arch_vcpu_ioctl --> 处理KVM_SET_MSRS或者KVM_GET_MSRS
+ * __dta_kvm_vcpu_ioctl_639
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *    200
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|3184| <<kvm_set_msr_common(MSR_IA32_TSC)>> kvm_synchronize_tsc(vcpu, data);
+ *   - arch/x86/kvm/x86.c|10133| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, 0);
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -2234,8 +2489,21 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	bool synchronizing = false;
 
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	/*
+	 * 先通过kvm_scale_tsc(vcpu, rdtsc())把host的tsc给转换成guest tsc
+	 * 然后计算两者的offset
+	 */
 	offset = kvm_compute_tsc_offset(vcpu, data);
+	/*
+	 * 似乎是host启动以后经历的时间
+	 */
 	ns = get_kvmclock_base_ns();
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2271| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|2333| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|10422| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	elapsed = ns - kvm->arch.last_tsc_nsec;
 
 	if (vcpu->arch.virtual_tsc_khz) {
@@ -2276,6 +2544,12 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			offset = kvm_compute_tsc_offset(vcpu, data);
 		}
 		matched = true;
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_generation:
+		 *   - arch/x86/kvm/x86.c|2311| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+		 *   - arch/x86/kvm/x86.c|2322| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+		 *   - arch/x86/kvm/x86.c|2340| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+		 */
 		already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
 	} else {
 		/*
@@ -2287,6 +2561,12 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 		 *
 		 * These values are tracked in kvm->arch.cur_xxx variables.
 		 */
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_generation:
+		 *   - arch/x86/kvm/x86.c|2311| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+		 *   - arch/x86/kvm/x86.c|2322| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+		 *   - arch/x86/kvm/x86.c|2340| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+		 */
 		kvm->arch.cur_tsc_generation++;
 		kvm->arch.cur_tsc_nsec = ns;
 		kvm->arch.cur_tsc_write = data;
@@ -2309,11 +2589,26 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
 	vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
 
+	/*
+	 * 这个注释不对, 仅仅看着玩.
+	 * Guest中使用rdtsc指令读取TSC时,会因为EXIT_REASON_RDTSC导致VM Exit.
+	 * VMM读取Host的TSC和VMCS中的TSC_OFFSET,然后把host_tst+tsc_offset返回给Guest.
+	 *
+	 * 要做出OFFSET的原因是考虑到vcpu热插拔和Guest会在不同的Host间迁移.
+	 */
 	kvm_vcpu_write_tsc_offset(vcpu, offset);
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 
 	spin_lock(&kvm->arch.pvclock_gtod_sync_lock);
 	if (!matched) {
+		/*
+		 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+		 *   - arch/x86/kvm/x86.c|2145| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+		 *   - arch/x86/kvm/x86.c|2160| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+		 *   - arch/x86/kvm/x86.c|2317| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+		 *   - arch/x86/kvm/x86.c|2319| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+		 *   - arch/x86/kvm/x86.c|2516| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+		 */
 		kvm->arch.nr_vcpus_matched_tsc = 0;
 	} else if (!already_matched) {
 		kvm->arch.nr_vcpus_matched_tsc++;
@@ -2340,6 +2635,10 @@ static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 
 #ifdef CONFIG_X86_64
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2633| <<vgettsc>> *tsc_timestamp = read_tsc();
+ */
 static u64 read_tsc(void)
 {
 	u64 ret = (u64)rdtsc_ordered();
@@ -2396,6 +2695,10 @@ static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 	return v * clock->mult;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2654| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
+ */
 static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2415,6 +2718,10 @@ static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 	return mode;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2747| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ */
 static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2437,6 +2744,10 @@ static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 }
 
 /* returns true if host is using TSC based clocksource */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2716| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+ */
 static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 {
 	/* checked again under seqlock below */
@@ -2448,6 +2759,10 @@ static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 }
 
 /* returns true if host is using TSC based clocksource */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8520| <<kvm_pv_clock_pairing>> if (kvm_get_walltime_and_clockread(&ts, &cycle) == false)
+ */
 static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
 					   u64 *tsc_timestamp)
 {
@@ -2500,6 +2815,14 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2549| <<kvm_gen_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|7679| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|10435| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ *
+ * 确认guest能否使用master_clock(用于vcpu之间的时间同步)
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2507,6 +2830,14 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	int vclock_mode;
 	bool host_tsc_clocksource, vcpus_matched;
 
+	/*
+	 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2145| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2160| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2317| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2319| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|2516| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			atomic_read(&kvm->online_vcpus));
 
@@ -2514,10 +2845,20 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	 * If the host uses TSC clock, then passthrough TSC as stable
 	 * to the guest.
 	 */
+	/*
+	 * returns true if host is using TSC based clocksource
+	 */
 	host_tsc_clocksource = kvm_get_time_and_clockread(
 					&ka->master_kernel_ns,
 					&ka->master_cycle_now);
 
+	/*
+	 * 满足以下四个条件:
+	 * 1. host_tsc_clocksource
+	 * 2. vcpus_matched
+	 * 3. !ka->backwards_tsc_observed
+	 * 4. !ka->boot_vcpu_runs_old_kvmclock
+	 */
 	ka->use_master_clock = host_tsc_clocksource && vcpus_matched
 				&& !ka->backwards_tsc_observed
 				&& !ka->boot_vcpu_runs_old_kvmclock;
@@ -2536,6 +2877,11 @@ void kvm_make_mclock_inprogress_request(struct kvm *kvm)
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5734| <<kvm_arch_vm_ioctl(处理KVM_SET_CLOCK)>> kvm_gen_update_masterclock(kvm);
+ *   - arch/x86/kvm/x86.c|8901| <<vcpu_enter_guest(处理KVM_REQ_MASTERCLOCK_UPDATE)>> kvm_gen_update_masterclock(vcpu->kvm);
+ */
 static void kvm_gen_update_masterclock(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2559,6 +2905,13 @@ static void kvm_gen_update_masterclock(struct kvm *kvm)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|526| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|2083| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/x86.c|6133| <<kvm_arch_vm_ioctl>> now_ns = get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/x86.c|6142| <<kvm_arch_vm_ioctl>> now_ns = get_kvmclock_ns(kvm);
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2591,6 +2944,10 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3127| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v);
+ */
 static void kvm_setup_pvclock_page(struct kvm_vcpu *v)
 {
 	struct kvm_vcpu_arch *vcpu = &v->arch;
@@ -2648,6 +3005,24 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v)
 				sizeof(vcpu->hv_clock.version));
 }
 
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2552| <<kvm_gen_update_masterclock>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2681| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|2767| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2776| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|4013| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4656| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5700| <<kvm_arch_vm_ioctl>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *   - arch/x86/kvm/x86.c|7682| <<kvm_hyperv_tsc_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|7745| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8867| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) { --> kvm_guest_time_update()
+ *   - arch/x86/kvm/x86.c|9121| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10242| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *
+ * 处理KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|8874| <<vcpu_enter_guest>> r = kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -2666,6 +3041,19 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 * to the guest.
 	 */
 	spin_lock(&ka->pvclock_gtod_sync_lock);
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2521| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 * 在以下使用kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2156| <<kvm_track_tsc_matching>> if (ka->use_master_clock ||
+	 *   - arch/x86/kvm/x86.c|2162| <<kvm_track_tsc_matching>> ka->use_master_clock, gtod->clock.vclock_mode);
+	 *   - arch/x86/kvm/x86.c|2525| <<pvclock_update_vm_gtod_copy>> if (ka->use_master_clock)
+	 *   - arch/x86/kvm/x86.c|2529| <<pvclock_update_vm_gtod_copy>> trace_kvm_update_master_clock(ka->use_master_clock, vclock_mode,
+	 *   - arch/x86/kvm/x86.c|2569| <<get_kvmclock_ns>> if (!ka->use_master_clock) {
+	 *   - arch/x86/kvm/x86.c|2669| <<kvm_guest_time_update>> use_master_clock = ka->use_master_clock;
+	 *   - arch/x86/kvm/x86.c|4036| <<kvm_arch_vcpu_load>> if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
+	 *   - arch/x86/kvm/x86.c|5709| <<kvm_arch_vm_ioctl>> user_ns.flags = kvm->arch.use_master_clock ? KVM_CLOCK_TSC_STABLE : 0;
+	 */
 	use_master_clock = ka->use_master_clock;
 	if (use_master_clock) {
 		host_tsc = ka->master_cycle_now;
@@ -2675,6 +3063,18 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	/* Keep irq disabled to prevent changes to the clock */
 	local_irq_save(flags);
+	/*
+	 * 在以下使用percpu的cpu_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2611| <<get_kvmclock_ns>> if (__this_cpu_read(cpu_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|2612| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+	 *   - arch/x86/kvm/x86.c|2739| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|7734| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+	 *   - arch/x86/kvm/x86.c|7749| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+	 *   - arch/x86/kvm/x86.c|7767| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+	 *
+	 * 若支持TSC Scaling,则Guset TSC的运行频率为cpu_tsc_khz * scale
+	 * 若不支持TSC Scaling,则Guest TSC的运行频率为cpu_tsc_khz,即与Host相同
+	 */
 	tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
 	if (unlikely(tgt_tsc_khz == 0)) {
 		local_irq_restore(flags);
@@ -2686,6 +3086,10 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		kernel_ns = get_kvmclock_base_ns();
 	}
 
+	/*
+	 * 返回return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+	 * 获得(计算)L1 VM的tsc, 应该等价在VM里运行rdtsc?
+	 */
 	tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
 
 	/*
@@ -2714,13 +3118,38 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		tgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz);
 
 	if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+		/*
+		 * PerCPUTime = ((RDTSC() - tsc_timestamp) >> tsc_shift) * tsc_to_system_mul + system_time
+		 *
+		 * 似乎KVMCLOCK时钟频率固定在1GHZ???
+		 * 所以这里是NSEC_PER_SEC???
+		 */
 		kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
 				   &vcpu->hv_clock.tsc_shift,
 				   &vcpu->hv_clock.tsc_to_system_mul);
 		vcpu->hw_tsc_khz = tgt_tsc_khz;
 	}
 
+	/*
+	 * struct kvm_vcpu_arch *vcpu:
+	 * -> struct pvclock_vcpu_time_info hv_clock;
+	 *    -> u32   version;
+	 *    -> u32   pad0;
+	 *    -> u64   tsc_timestamp;
+	 *    -> u64   system_time;           
+	 *    -> u32   tsc_to_system_mul;     
+	 *    -> s8    tsc_shift;
+	 *    -> u8    flags;
+	 *    -> u8    pad[2];
+	 */
 	vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
+	/*
+	 * system_time = kernel_ns + kvm->arch.kvmclock_offset
+	 * 其中kvmclock_offset的初始值为VM创建时的Host Boot Time的相反数,
+	 * 即令VM创建时的system_time为0,system_time表示Guest的Boot Time.
+	 * 同时,用户态可以通过KVM_SET_CLOCK ioctl来设置kvmclock的system_time值,
+	 * 从而修改kvmclock_offset,这就允许QEMU采用不同的初始化行为.
+	 */
 	vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
 	vcpu->last_guest_tsc = tsc_timestamp;
 
@@ -2754,6 +3183,14 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 #define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)
 
+/*
+ * 在以下使用kvm_arch->kvmclock_update_work:
+ *   - arch/x86/kvm/x86.c|2823| <<kvmclock_update_fn>> kvmclock_update_work);
+ *   - arch/x86/kvm/x86.c|2838| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+ *   - arch/x86/kvm/x86.c|2854| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+ *   - arch/x86/kvm/x86.c|10500| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ *   - arch/x86/kvm/x86.c|10548| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+ */
 static void kvmclock_update_fn(struct work_struct *work)
 {
 	int i;
@@ -2764,22 +3201,40 @@ static void kvmclock_update_fn(struct work_struct *work)
 	struct kvm_vcpu *vcpu;
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/*
+		 * kvm_guest_time_update()
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		kvm_vcpu_kick(vcpu);
 	}
 }
 
+/*
+ * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|8927| <<vcpu_enter_guest>> kvm_gen_kvmclock_update(vcpu);
+ */
 static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 {
 	struct kvm *kvm = v->kvm;
 
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	/*
+	 * kvmclock_update_fn()
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
 }
 
 #define KVMCLOCK_SYNC_PERIOD (300 * HZ)
 
+/*
+ * 在以下使用kvm_arch->kvmclock_sync_work:
+ *   - arch/x86/kvm/x86.c|2848| <<kvmclock_sync_fn>> kvmclock_sync_work);
+ *   - arch/x86/kvm/x86.c|2855| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+ *   - arch/x86/kvm/x86.c|10166| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+ *   - arch/x86/kvm/x86.c|10501| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+ *   - arch/x86/kvm/x86.c|10547| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+ */
 static void kvmclock_sync_fn(struct work_struct *work)
 {
 	struct delayed_work *dwork = to_delayed_work(work);
@@ -2787,6 +3242,12 @@ static void kvmclock_sync_fn(struct work_struct *work)
 					   kvmclock_sync_work);
 	struct kvm *kvm = container_of(ka, struct kvm, arch);
 
+	/*
+	 * 在以下使用kvmclock_periodic_sync:
+	 *   - arch/x86/kvm/x86.c|127| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+	 *   - arch/x86/kvm/x86.c|2845| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+	 *   - arch/x86/kvm/x86.c|10159| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+	 */
 	if (!kvmclock_periodic_sync)
 		return;
 
@@ -2963,6 +3424,10 @@ static void kvm_vcpu_flush_tlb_guest(struct kvm_vcpu *vcpu)
 	kvm_x86_ops.tlb_flush_guest(vcpu);
 }
 
+/*
+ * 处理KVM_REQ_STEAL_UPDATE:
+ *   - arch/x86/kvm/x86.c|8868| <<vcpu_enter_guest>> record_steal_time(vcpu);
+ */
 static void record_steal_time(struct kvm_vcpu *vcpu)
 {
 	struct kvm_host_map map;
@@ -2999,6 +3464,23 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
 
 	smp_wmb();
 
+	/*
+	 * struct kvm_steal_time *st:
+	 * -> __u64 steal;
+	 * -> __u32 version;
+	 * -> __u32 flags;
+	 * -> __u8  preempted;
+	 * -> __u8  u8_pad[3];
+	 * -> __u32 pad[11];
+	 *
+	 * struct kvm_vcpu_arch:
+	 *     struct {
+	 *         u8 preempted;
+	 *         u64 msr_val;
+	 *         u64 last_steal;
+	 *         struct gfn_to_pfn_cache cache;
+	 *     } st;
+	 */
 	st->steal += current->sched_info.run_delay -
 		vcpu->arch.st.last_steal;
 	vcpu->arch.st.last_steal = current->sched_info.run_delay;
@@ -3010,6 +3492,17 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
 	kvm_unmap_gfn(vcpu, &map, &vcpu->arch.st.cache, true, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3537| <<global>> EXPORT_SYMBOL_GPL(kvm_set_msr_common);
+ *   - arch/x86/kvm/svm/svm.c|2602| <<efer_trap>> ret = kvm_set_msr_common(&svm->vcpu, &msr_info);
+ *   - arch/x86/kvm/svm/svm.c|2922| <<svm_set_msr>> return kvm_set_msr_common(vcpu, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|1977| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2016| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2114| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2117| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2217| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ */
 int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	bool pr = false;
@@ -3194,6 +3687,13 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 			kvm_check_async_pf_completion(vcpu);
 		}
 		break;
+	/*
+	 * 主要在以下使用MSR_KVM_STEAL_TIME:
+	 *   - arch/x86/kernel/kvm.c|310| <<kvm_register_steal_time>> wrmsrl(MSR_KVM_STEAL_TIME, (slow_virt_to_phys(st) | KVM_MSR_ENABLED));
+	 *   - arch/x86/kernel/kvm.c|424| <<kvm_disable_steal_time>> wrmsr(MSR_KVM_STEAL_TIME, 0, 0);
+	 *   - arch/x86/kvm/x86.c|3197| <<kvm_set_msr_common>> case MSR_KVM_STEAL_TIME:
+	 *   - arch/x86/kvm/x86.c|3524| <<kvm_get_msr_common>> case MSR_KVM_STEAL_TIME:
+	 */
 	case MSR_KVM_STEAL_TIME:
 		if (!guest_pv_has(vcpu, KVM_FEATURE_STEAL_TIME))
 			return 1;
@@ -3957,6 +4457,11 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|210| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|4831| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Address WBINVD may be executed by guest */
@@ -7851,6 +8356,18 @@ static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
 /*
  * Notification about pvclock gtod data update.
  */
+/*
+ * 一个例子:
+ * pvclock_gtod_notify
+ * raw_notifier_call_chain
+ * timekeeping_update
+ * update_wall_time
+ * tick_do_update_jiffies64.part.13
+ * tick_irq_enter
+ * irq_enter
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ */
 static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 			       void *priv)
 {
@@ -8757,6 +9274,10 @@ static void vcpu_load_eoi_exitmap(struct kvm_vcpu *vcpu)
 	kvm_x86_ops.load_eoi_exitmap(vcpu, eoi_exit_bitmap);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|483| <<kvm_mmu_notifier_invalidate_range>> kvm_arch_mmu_notifier_invalidate_range(kvm, start, end);
+ */
 void kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
 					    unsigned long start, unsigned long end)
 {
@@ -11139,6 +11660,11 @@ bool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu)
 		return apf_pageready_slot_free(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11712| <<kvm_arch_irq_bypass_add_producer>> kvm_arch_start_assignment(irqfd->kvm);
+ *   - virt/kvm/vfio.c|229| <<kvm_vfio_set_group>> kvm_arch_start_assignment(dev->kvm);
+ */
 void kvm_arch_start_assignment(struct kvm *kvm)
 {
 	atomic_inc(&kvm->arch.assigned_device_count);
@@ -11222,6 +11748,19 @@ void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
 	kvm_arch_end_assignment(irqfd->kvm);
 }
 
+/*
+ * kvm_arch_update_irqfd_routing
+ * kvm_set_irq_routing
+ * kvm_vm_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - virt/kvm/eventfd.c|622| <<kvm_irq_routing_update>> int ret = kvm_arch_update_irqfd_routing(
+ */
 int kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,
 				   uint32_t guest_irq, bool set)
 {
diff --git a/drivers/clocksource/hyperv_timer.c b/drivers/clocksource/hyperv_timer.c
index ba04cb381cd3..fda896358526 100644
--- a/drivers/clocksource/hyperv_timer.c
+++ b/drivers/clocksource/hyperv_timer.c
@@ -418,6 +418,10 @@ static struct clocksource hyperv_cs_msr = {
 	.flags	= CLOCK_SOURCE_IS_CONTINUOUS,
 };
 
+/*
+ * called by:
+ *   - drivers/clocksource/hyperv_timer.c|463| <<hv_init_clocksource>> if (hv_init_tsc_clocksource())
+ */
 static bool __init hv_init_tsc_clocksource(void)
 {
 	u64		tsc_msr;
@@ -450,6 +454,10 @@ static bool __init hv_init_tsc_clocksource(void)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/mshyperv.c|383| <<ms_hyperv_init_platform>> hv_init_clocksource();
+ */
 void __init hv_init_clocksource(void)
 {
 	/*
diff --git a/drivers/iommu/intel/iommu.c b/drivers/iommu/intel/iommu.c
index 06b00b5363d8..ef3f2a07bd6a 100644
--- a/drivers/iommu/intel/iommu.c
+++ b/drivers/iommu/intel/iommu.c
@@ -982,6 +982,12 @@ static void free_context_table(struct intel_iommu *iommu)
 	spin_unlock_irqrestore(&iommu->lock, flags);
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/iommu.c|2325| <<__domain_mapping>> first_pte = pte = pfn_to_dma_pte(domain, iov_pfn, &largepage_lvl);
+ *   - drivers/iommu/intel/iommu.c|4987| <<intel_iommu_unmap>> BUG_ON(!pfn_to_dma_pte(dmar_domain, iova >> VTD_PAGE_SHIFT, &level));
+ *   - drivers/iommu/intel/iommu.c|5034| <<intel_iommu_iova_to_phys>> pte = pfn_to_dma_pte(dmar_domain, iova >> VTD_PAGE_SHIFT, &level);
+ */
 static struct dma_pte *pfn_to_dma_pte(struct dmar_domain *domain,
 				      unsigned long pfn, int *target_level)
 {
@@ -2396,6 +2402,20 @@ __domain_mapping(struct dmar_domain *domain, unsigned long iov_pfn,
 	return 0;
 }
 
+/*
+ * domain_mapping
+ * iommu_map
+ * vfio_pin_map_dma_chunk
+ * __padata_do_multithreaded
+ * vfio_dma_do_map
+ * vfio_iommu_type1_ioctl
+ * vfio_fops_unl_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static int
 domain_mapping(struct dmar_domain *domain, unsigned long iov_pfn,
 	       unsigned long phys_pfn, unsigned long nr_pages, int prot)
diff --git a/drivers/iommu/intel/irq_remapping.c b/drivers/iommu/intel/irq_remapping.c
index 685200a5cff0..8fa957958b23 100644
--- a/drivers/iommu/intel/irq_remapping.c
+++ b/drivers/iommu/intel/irq_remapping.c
@@ -786,6 +786,11 @@ static int __init intel_prepare_irq_remapping(void)
 /*
  * Set Posted-Interrupts capability.
  */
+/*
+ * called by:
+ *   - drivers/iommu/intel/irq_remapping.c|835| <<intel_enable_irq_remapping>> set_irq_posting_cap();
+ *   - drivers/iommu/intel/irq_remapping.c|1076| <<reenable_irq_remapping>> set_irq_posting_cap();
+ */
 static inline void set_irq_posting_cap(void)
 {
 	struct dmar_drhd_unit *drhd;
diff --git a/drivers/iommu/intel/svm.c b/drivers/iommu/intel/svm.c
index 18a9f05df407..d2dc0d7ec425 100644
--- a/drivers/iommu/intel/svm.c
+++ b/drivers/iommu/intel/svm.c
@@ -497,6 +497,10 @@ static void load_pasid(struct mm_struct *mm, u32 pasid)
 }
 
 /* Caller must hold pasid_mutex, mm reference */
+/*
+ * called by:
+ *   - drivers/iommu/intel/svm.c|1108| <<intel_svm_bind>> ret = intel_svm_bind_mm(dev, flags, NULL, mm, &sdev);
+ */
 static int
 intel_svm_bind_mm(struct device *dev, unsigned int flags,
 		  struct svm_dev_ops *ops,
@@ -1141,6 +1145,9 @@ u32 intel_svm_get_pasid(struct iommu_sva *sva)
 	return pasid;
 }
 
+/*
+ * struct iommu_ops intel_iommu_ops.page_response = intel_svm_page_response()
+ */
 int intel_svm_page_response(struct device *dev,
 			    struct iommu_fault_event *evt,
 			    struct iommu_page_response *msg)
diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c
index ffeebda8d6de..21c3fb9c5beb 100644
--- a/drivers/iommu/iommu.c
+++ b/drivers/iommu/iommu.c
@@ -2438,6 +2438,30 @@ static int __iommu_map(struct iommu_domain *domain, unsigned long iova,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm/mm/dma-mapping.c|1335| <<__iommu_create_mapping>> ret = iommu_map(mapping->domain, iova, phys, len,
+ *   - arch/arm/mm/dma-mapping.c|1605| <<__map_sg_chunk>> ret = iommu_map(mapping->domain, iova, phys, len, prot);
+ *   - arch/arm/mm/dma-mapping.c|1816| <<arm_coherent_iommu_map_page>> ret = iommu_map(mapping->domain, dma_addr, page_to_phys(page), len, prot);
+ *   - arch/arm/mm/dma-mapping.c|1922| <<arm_iommu_map_resource>> ret = iommu_map(mapping->domain, dma_addr, addr, len, prot);
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/instmem/gk20a.c|477| <<gk20a_instobj_ctor_iommu>> ret = iommu_map(imem->domain, offset, node->dma_addrs[i],
+ *   - drivers/gpu/drm/tegra/drm.c|1004| <<tegra_drm_alloc>> err = iommu_map(tegra->domain, *dma, virt_to_phys(virt),
+ *   - drivers/gpu/host1x/cdma.c|107| <<host1x_pushbuffer_init>> err = iommu_map(host1x->domain, pb->dma, pb->phys, size,
+ *   - drivers/infiniband/hw/usnic/usnic_uiom.c|284| <<usnic_uiom_map_sorted_intervals>> err = iommu_map(pd->domain, va_start, pa_start,
+ *   - drivers/infiniband/hw/usnic/usnic_uiom.c|301| <<usnic_uiom_map_sorted_intervals>> err = iommu_map(pd->domain, va_start, pa_start,
+ *   - drivers/iommu/dma-iommu.c|1364| <<iommu_dma_get_msi_page>> if (iommu_map(domain, iova, msi_addr, size, prot))
+ *   - drivers/iommu/iommu.c|775| <<iommu_create_device_direct_mappings>> ret = iommu_map(domain, addr - map_size,
+ *   - drivers/media/platform/qcom/venus/firmware.c|143| <<venus_boot_no_tz>> ret = iommu_map(iommu, VENUS_FW_START_ADDR, mem_phys, mem_size,
+ *   - drivers/net/ipa/ipa_mem.c|314| <<ipa_imem_init>> ret = iommu_map(domain, iova, phys, size, IOMMU_READ | IOMMU_WRITE);
+ *   - drivers/net/ipa/ipa_mem.c|422| <<ipa_smem_init>> ret = iommu_map(domain, iova, phys, size, IOMMU_READ | IOMMU_WRITE);
+ *   - drivers/net/wireless/ath/ath10k/snoc.c|1567| <<ath10k_fw_init>> ret = iommu_map(iommu_dom, ar_snoc->fw.fw_start_addr,
+ *   - drivers/remoteproc/remoteproc_core.c|745| <<rproc_handle_devmem>> ret = iommu_map(rproc->domain, rsc->da, rsc->pa, rsc->len, rsc->flags);
+ *   - drivers/remoteproc/remoteproc_core.c|836| <<rproc_alloc_carveout>> ret = iommu_map(rproc->domain, mem->da, dma, mem->len,
+ *   - drivers/vfio/vfio_iommu_type1.c|1234| <<vfio_iommu_map>> ret = iommu_map(d->domain, iova, (phys_addr_t)pfn << PAGE_SHIFT,
+ *   - drivers/vfio/vfio_iommu_type1.c|1512| <<vfio_iommu_replay>> ret = iommu_map(domain->domain, iova, phys,
+ *   - drivers/vfio/vfio_iommu_type1.c|1596| <<vfio_test_domain_fgsp>> ret = iommu_map(domain->domain, 0, page_to_phys(pages), PAGE_SIZE * 2,
+ *   - drivers/vhost/vdpa.c|572| <<vhost_vdpa_map>> r = iommu_map(v->domain, iova, pa, size,
+ */
 int iommu_map(struct iommu_domain *domain, unsigned long iova,
 	      phys_addr_t paddr, size_t size, int prot)
 {
@@ -2971,6 +2995,12 @@ EXPORT_SYMBOL_GPL(iommu_aux_get_pasid);
  *
  * On error, returns an ERR_PTR value.
  */
+/*
+ * called by:
+ *   - drivers/dma/idxd/cdev.c|106| <<idxd_cdev_open>> sva = iommu_sva_bind_device(dev, current->mm, NULL);
+ *   - drivers/dma/idxd/init.c|305| <<idxd_enable_system_pasid>> sva = iommu_sva_bind_device(&idxd->pdev->dev, NULL, &flags);
+ *   - drivers/misc/uacce/uacce.c|102| <<uacce_bind_queue>> handle = iommu_sva_bind_device(uacce->parent, current->mm, NULL);
+ */
 struct iommu_sva *
 iommu_sva_bind_device(struct device *dev, struct mm_struct *mm, void *drvdata)
 {
diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 8ee24e351bdc..045553a0957d 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -198,6 +198,24 @@ struct xenvif_queue { /* Per-queue data for xenvif */
 	/* Only used when feature-split-event-channels = 1 */
 	char rx_irq_name[IRQ_NAME_SIZE]; /* DEVNAME-qN-rx */
 	struct xen_netif_rx_back_ring rx;
+	/*
+	 * 在以下使用xenvirt_queue->rx_queue:
+	 *   - drivers/net/xen-netback/interface.c|582| <<xenvif_init_queue>> skb_queue_head_init(&queue->rx_queue);
+	 *   - drivers/net/xen-netback/rx.c|43| <<xenvif_rx_ring_slots_available>> spin_lock_irqsave(&queue->rx_queue.lock, flags);
+	 *   - drivers/net/xen-netback/rx.c|45| <<xenvif_rx_ring_slots_available>> skb = skb_peek(&queue->rx_queue);
+	 *   - drivers/net/xen-netback/rx.c|47| <<xenvif_rx_ring_slots_available>> spin_unlock_irqrestore(&queue->rx_queue.lock, flags);
+	 *   - drivers/net/xen-netback/rx.c|57| <<xenvif_rx_ring_slots_available>> spin_unlock_irqrestore(&queue->rx_queue.lock, flags);
+	 *   - drivers/net/xen-netback/rx.c|81| <<xenvif_rx_queue_tail>> spin_lock_irqsave(&queue->rx_queue.lock, flags);
+	 *   - drivers/net/xen-netback/rx.c|83| <<xenvif_rx_queue_tail>> __skb_queue_tail(&queue->rx_queue, skb);
+	 *   - drivers/net/xen-netback/rx.c|92| <<xenvif_rx_queue_tail>> spin_unlock_irqrestore(&queue->rx_queue.lock, flags);
+	 *   - drivers/net/xen-netback/rx.c|99| <<xenvif_rx_dequeue>> spin_lock_irq(&queue->rx_queue.lock);
+	 *   - drivers/net/xen-netback/rx.c|101| <<xenvif_rx_dequeue>> skb = __skb_dequeue(&queue->rx_queue);
+	 *   - drivers/net/xen-netback/rx.c|112| <<xenvif_rx_dequeue>> spin_unlock_irq(&queue->rx_queue.lock);
+	 *   - drivers/net/xen-netback/rx.c|130| <<xenvif_rx_queue_drop_expired>> skb = skb_peek(&queue->rx_queue);
+	 *   - drivers/net/xen-netback/rx.c|528| <<xenvif_rx_queue_timeout>> skb = skb_peek(&queue->rx_queue);
+	 *   - drivers/net/xen-netback/rx.c|628| <<xenvif_kthread_guest_rx>> if (!skb_queue_empty(&queue->rx_queue))
+	 *   - drivers/net/xen-netback/xenbus.c|96| <<xenvif_read_io_ring>> skb_queue_len(&queue->rx_queue),
+	 */
 	struct sk_buff_head rx_queue;
 
 	unsigned int rx_queue_max;
diff --git a/drivers/net/xen-netback/rx.c b/drivers/net/xen-netback/rx.c
index accc991d153f..bb92992397f0 100644
--- a/drivers/net/xen-netback/rx.c
+++ b/drivers/net/xen-netback/rx.c
@@ -33,6 +33,13 @@
 #include <xen/xen.h>
 #include <xen/events.h>
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|480| <<xenvif_rx_action>> while (xenvif_rx_ring_slots_available(queue) &&
+ *   - drivers/net/xen-netback/rx.c|515| <<xenvif_have_rx_work>> return xenvif_rx_ring_slots_available(queue) ||
+ *
+ * 用&queue->rx_queue.lock保护queue->rx_queue
+ */
 static bool xenvif_rx_ring_slots_available(struct xenvif_queue *queue)
 {
 	RING_IDX prod, cons;
@@ -42,6 +49,24 @@ static bool xenvif_rx_ring_slots_available(struct xenvif_queue *queue)
 
 	spin_lock_irqsave(&queue->rx_queue.lock, flags);
 
+	/*
+	 * 在以下使用xenvirt_queue->rx_queue:
+	 *   - drivers/net/xen-netback/interface.c|582| <<xenvif_init_queue>> skb_queue_head_init(&queue->rx_queue);
+	 *   - drivers/net/xen-netback/rx.c|43| <<xenvif_rx_ring_slots_available>> spin_lock_irqsave(&queue->rx_queue.lock, flags);
+	 *   - drivers/net/xen-netback/rx.c|45| <<xenvif_rx_ring_slots_available>> skb = skb_peek(&queue->rx_queue);
+	 *   - drivers/net/xen-netback/rx.c|47| <<xenvif_rx_ring_slots_available>> spin_unlock_irqrestore(&queue->rx_queue.lock, flags);
+	 *   - drivers/net/xen-netback/rx.c|57| <<xenvif_rx_ring_slots_available>> spin_unlock_irqrestore(&queue->rx_queue.lock, flags);
+	 *   - drivers/net/xen-netback/rx.c|81| <<xenvif_rx_queue_tail>> spin_lock_irqsave(&queue->rx_queue.lock, flags);
+	 *   - drivers/net/xen-netback/rx.c|83| <<xenvif_rx_queue_tail>> __skb_queue_tail(&queue->rx_queue, skb);
+	 *   - drivers/net/xen-netback/rx.c|92| <<xenvif_rx_queue_tail>> spin_unlock_irqrestore(&queue->rx_queue.lock, flags);
+	 *   - drivers/net/xen-netback/rx.c|99| <<xenvif_rx_dequeue>> spin_lock_irq(&queue->rx_queue.lock);
+	 *   - drivers/net/xen-netback/rx.c|101| <<xenvif_rx_dequeue>> skb = __skb_dequeue(&queue->rx_queue);
+	 *   - drivers/net/xen-netback/rx.c|112| <<xenvif_rx_dequeue>> spin_unlock_irq(&queue->rx_queue.lock);
+	 *   - drivers/net/xen-netback/rx.c|130| <<xenvif_rx_queue_drop_expired>> skb = skb_peek(&queue->rx_queue);
+	 *   - drivers/net/xen-netback/rx.c|528| <<xenvif_rx_queue_timeout>> skb = skb_peek(&queue->rx_queue);
+	 *   - drivers/net/xen-netback/rx.c|628| <<xenvif_kthread_guest_rx>> if (!skb_queue_empty(&queue->rx_queue))
+	 *   - drivers/net/xen-netback/xenbus.c|96| <<xenvif_read_io_ring>> skb_queue_len(&queue->rx_queue),
+	 */
 	skb = skb_peek(&queue->rx_queue);
 	if (!skb) {
 		spin_unlock_irqrestore(&queue->rx_queue.lock, flags);
@@ -74,6 +99,12 @@ static bool xenvif_rx_ring_slots_available(struct xenvif_queue *queue)
 	return false;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/interface.c|270| <<xenvif_start_xmit>> xenvif_rx_queue_tail(queue, skb);
+ *
+ * 用&queue->rx_queue.lock保护queue->rx_queue
+ */
 void xenvif_rx_queue_tail(struct xenvif_queue *queue, struct sk_buff *skb)
 {
 	unsigned long flags;
@@ -92,6 +123,14 @@ void xenvif_rx_queue_tail(struct xenvif_queue *queue, struct sk_buff *skb)
 	spin_unlock_irqrestore(&queue->rx_queue.lock, flags);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|121| <<xenvif_rx_queue_purge>> while ((skb = xenvif_rx_dequeue(queue)) != NULL)
+ *   - drivers/net/xen-netback/rx.c|135| <<xenvif_rx_queue_drop_expired>> xenvif_rx_dequeue(queue);
+ *   - drivers/net/xen-netback/rx.c|239| <<xenvif_rx_next_skb>> skb = xenvif_rx_dequeue(queue);
+ *
+ * 用&queue->rx_queue.lock保护了queue->rx_queue
+ */
 static struct sk_buff *xenvif_rx_dequeue(struct xenvif_queue *queue)
 {
 	struct sk_buff *skb;
@@ -114,6 +153,10 @@ static struct sk_buff *xenvif_rx_dequeue(struct xenvif_queue *queue)
 	return skb;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|682| <<xenvif_kthread_guest_rx>> xenvif_rx_queue_purge(queue);
+ */
 static void xenvif_rx_queue_purge(struct xenvif_queue *queue)
 {
 	struct sk_buff *skb;
@@ -122,6 +165,12 @@ static void xenvif_rx_queue_purge(struct xenvif_queue *queue)
 		kfree_skb(skb);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|676| <<xenvif_kthread_guest_rx>> xenvif_rx_queue_drop_expired(queue);
+ *
+ * 没有用&queue->rx_queue.lock保护queue->rx_queue
+ */
 static void xenvif_rx_queue_drop_expired(struct xenvif_queue *queue)
 {
 	struct sk_buff *skb;
@@ -137,6 +186,11 @@ static void xenvif_rx_queue_drop_expired(struct xenvif_queue *queue)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|198| <<xenvif_rx_copy_add>> xenvif_rx_copy_flush(queue);
+ *   - drivers/net/xen-netback/rx.c|506| <<xenvif_rx_action>> xenvif_rx_copy_flush(queue);
+ */
 static void xenvif_rx_copy_flush(struct xenvif_queue *queue)
 {
 	unsigned int i;
@@ -171,6 +225,10 @@ static void xenvif_rx_copy_flush(struct xenvif_queue *queue)
 	__skb_queue_purge(queue->rx_copy.completed);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|402| <<xenvif_rx_data_slot>> xenvif_rx_copy_add(queue, req, offset, data, len);
+ */
 static void xenvif_rx_copy_add(struct xenvif_queue *queue,
 			       struct xen_netif_rx_request *req,
 			       unsigned int offset, void *data, size_t len)
@@ -208,6 +266,10 @@ static void xenvif_rx_copy_add(struct xenvif_queue *queue,
 	queue->rx_copy.num++;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|267| <<xenvif_rx_next_skb>> gso_type = xenvif_gso_type(skb);
+ */
 static unsigned int xenvif_gso_type(struct sk_buff *skb)
 {
 	if (skb_is_gso(skb)) {
@@ -230,6 +292,15 @@ struct xenvif_pkt_state {
 	unsigned int slot;
 };
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|461| <<xenvif_rx_skb>> xenvif_rx_next_skb(queue, &pkt);
+ *
+ * xenvif_kthread_guest_rx()
+ * -> xenvif_rx_action()
+ *    -> xenvif_rx_skb()
+ *       -> xenvif_rx_next_skb()
+ */
 static void xenvif_rx_next_skb(struct xenvif_queue *queue,
 			       struct xenvif_pkt_state *pkt)
 {
@@ -306,6 +377,10 @@ static void xenvif_rx_next_skb(struct xenvif_queue *queue,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|482| <<xenvif_rx_skb>> xenvif_rx_complete(queue, &pkt);
+ */
 static void xenvif_rx_complete(struct xenvif_queue *queue,
 			       struct xenvif_pkt_state *pkt)
 {
@@ -315,6 +390,10 @@ static void xenvif_rx_complete(struct xenvif_queue *queue,
 	__skb_queue_tail(queue->rx_copy.completed, pkt->skb);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|383| <<xenvif_rx_next_chunk>> xenvif_rx_next_frag(pkt);
+ */
 static void xenvif_rx_next_frag(struct xenvif_pkt_state *pkt)
 {
 	struct sk_buff *frag_iter = pkt->frag_iter;
@@ -333,6 +412,10 @@ static void xenvif_rx_next_frag(struct xenvif_pkt_state *pkt)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|401| <<xenvif_rx_data_slot>> xenvif_rx_next_chunk(queue, pkt, offset, &data, &len);
+ */
 static void xenvif_rx_next_chunk(struct xenvif_queue *queue,
 				 struct xenvif_pkt_state *pkt,
 				 unsigned int offset, void **data,
@@ -371,6 +454,10 @@ static void xenvif_rx_next_chunk(struct xenvif_queue *queue,
 	*len = chunk_len;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|476| <<xenvif_rx_skb>> xenvif_rx_data_slot(queue, &pkt, req, rsp);
+ */
 static void xenvif_rx_data_slot(struct xenvif_queue *queue,
 				struct xenvif_pkt_state *pkt,
 				struct xen_netif_rx_request *req,
@@ -415,6 +502,10 @@ static void xenvif_rx_data_slot(struct xenvif_queue *queue,
 	rsp->status = (s16)offset;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|474| <<xenvif_rx_skb>> xenvif_rx_extra_slot(queue, &pkt, req, rsp);
+ */
 static void xenvif_rx_extra_slot(struct xenvif_queue *queue,
 				 struct xenvif_pkt_state *pkt,
 				 struct xen_netif_rx_request *req,
@@ -439,6 +530,10 @@ static void xenvif_rx_extra_slot(struct xenvif_queue *queue,
 	BUG();
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|501| <<xenvif_rx_action>> xenvif_rx_skb(queue);
+ */
 static void xenvif_rx_skb(struct xenvif_queue *queue)
 {
 	struct xenvif_pkt_state pkt;
@@ -467,8 +562,16 @@ static void xenvif_rx_skb(struct xenvif_queue *queue)
 	xenvif_rx_complete(queue, &pkt);
 }
 
+/*
+ * 在以下使用RX_BATCH_SIZE:
+ *   - drivers/net/xen-netback/rx.c|500| <<xenvif_rx_action>> work_done < RX_BATCH_SIZE) {
+ */
 #define RX_BATCH_SIZE 64
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|629| <<xenvif_kthread_guest_rx>> xenvif_rx_action(queue);
+ */
 void xenvif_rx_action(struct xenvif_queue *queue)
 {
 	struct sk_buff_head completed_skbs;
@@ -487,6 +590,11 @@ void xenvif_rx_action(struct xenvif_queue *queue)
 	xenvif_rx_copy_flush(queue);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|542| <<xenvif_have_rx_work>> (xenvif_rx_queue_stalled(queue) ||
+ *   - drivers/net/xen-netback/rx.c|665| <<xenvif_kthread_guest_rx>> if (xenvif_rx_queue_stalled(queue))
+ */
 static bool xenvif_rx_queue_stalled(struct xenvif_queue *queue)
 {
 	RING_IDX prod, cons;
@@ -500,6 +608,11 @@ static bool xenvif_rx_queue_stalled(struct xenvif_queue *queue)
 			   queue->last_rx_time + queue->vif->stall_timeout);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|543| <<xenvif_have_rx_work>> xenvif_rx_queue_ready(queue))) ||
+ *   - drivers/net/xen-netback/rx.c|667| <<xenvif_kthread_guest_rx>> else if (xenvif_rx_queue_ready(queue))
+ */
 static bool xenvif_rx_queue_ready(struct xenvif_queue *queue)
 {
 	RING_IDX prod, cons;
@@ -510,6 +623,16 @@ static bool xenvif_rx_queue_ready(struct xenvif_queue *queue)
 	return queue->stalled && prod - cons >= 1;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/interface.c|139| <<xenvif_handle_rx_interrupt>> rc = xenvif_have_rx_work(queue, false);
+ *   - drivers/net/xen-netback/rx.c|550| <<xenvif_wait_for_rx_work>> if (xenvif_have_rx_work(queue, true))
+ *   - drivers/net/xen-netback/rx.c|557| <<xenvif_wait_for_rx_work>> if (xenvif_have_rx_work(queue, true))
+ *
+ * xenvif_kthread_guest_rx()
+ * -> xenvif_wait_for_rx_work()
+ *    -> xenvif_have_rx_work()
+ */
 bool xenvif_have_rx_work(struct xenvif_queue *queue, bool test_kthread)
 {
 	return xenvif_rx_ring_slots_available(queue) ||
@@ -520,6 +643,16 @@ bool xenvif_have_rx_work(struct xenvif_queue *queue, bool test_kthread)
 		queue->vif->disabled;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|593| <<xenvif_wait_for_rx_work>> ret = schedule_timeout(xenvif_rx_queue_timeout(queue));
+ *
+ * xenvif_kthread_guest_rx()
+ * -> xenvif_wait_for_rx_work()
+ *    -> xenvif_rx_queue_timeout()
+ *
+ * 没有用&queue->rx_queue.lock保护queue->rx_queue
+ */
 static long xenvif_rx_queue_timeout(struct xenvif_queue *queue)
 {
 	struct sk_buff *skb;
@@ -543,6 +676,10 @@ static long xenvif_rx_queue_timeout(struct xenvif_queue *queue)
  * This cannot be done with wait_event_timeout() because it only
  * calculates the timeout once.
  */
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|611| <<xenvif_kthread_guest_rx>> xenvif_wait_for_rx_work(queue);
+ */
 static void xenvif_wait_for_rx_work(struct xenvif_queue *queue)
 {
 	DEFINE_WAIT(wait);
@@ -568,6 +705,10 @@ static void xenvif_wait_for_rx_work(struct xenvif_queue *queue)
 	finish_wait(&queue->wq, &wait);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|666| <<xenvif_kthread_guest_rx>> xenvif_queue_carrier_off(queue);
+ */
 static void xenvif_queue_carrier_off(struct xenvif_queue *queue)
 {
 	struct xenvif *vif = queue->vif;
@@ -583,6 +724,11 @@ static void xenvif_queue_carrier_off(struct xenvif_queue *queue)
 	spin_unlock(&vif->lock);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|637| <<xenvif_kthread_guest_rx>> xenvif_queue_carrier_on(queue);
+ *   - drivers/net/xen-netback/rx.c|668| <<xenvif_kthread_guest_rx>> xenvif_queue_carrier_on(queue);
+ */
 static void xenvif_queue_carrier_on(struct xenvif_queue *queue)
 {
 	struct xenvif *vif = queue->vif;
@@ -599,6 +745,10 @@ static void xenvif_queue_carrier_on(struct xenvif_queue *queue)
 	spin_unlock(&vif->lock);
 }
 
+/*
+ * 在以下使用xenvif_kthread_guest_rx():
+ *   - drivers/net/xen-netback/interface.c|741| <<xenvif_connect_data>> task = kthread_run(xenvif_kthread_guest_rx, queue,
+ */
 int xenvif_kthread_guest_rx(void *data)
 {
 	struct xenvif_queue *queue = data;
@@ -625,6 +775,10 @@ int xenvif_kthread_guest_rx(void *data)
 			break;
 		}
 
+		/*
+		 * 没有用&queue->rx_queue.lock保护queue->rx_queue
+		 * 不过没关系!!
+		 */
 		if (!skb_queue_empty(&queue->rx_queue))
 			xenvif_rx_action(queue);
 
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index f13eb4ded95f..f6f25b75b26f 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -1259,6 +1259,11 @@ static bool nvme_ctrl_limited_cns(struct nvme_ctrl *ctrl)
 	return ctrl->vs < NVME_VS(1, 1, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3050| <<nvme_init_identify>> ret = nvme_identify_ctrl(ctrl, &id);
+ *   - drivers/nvme/host/core.c|4141| <<nvme_scan_ns_sequential>> if (nvme_identify_ctrl(ctrl, &id))
+ */
 static int nvme_identify_ctrl(struct nvme_ctrl *dev, struct nvme_id_ctrl **id)
 {
 	struct nvme_command c = { };
@@ -3029,6 +3034,15 @@ static int nvme_get_effects_log(struct nvme_ctrl *ctrl, u8 csi,
  * register in our nvme_ctrl structure.  This should be called as soon as
  * the admin queue is fully up and running.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1088| <<nvme_passthru_end>> nvme_init_identify(ctrl);
+ *   - drivers/nvme/host/fc.c|3088| <<nvme_fc_create_association>> ret = nvme_init_identify(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|2652| <<nvme_reset_work>> result = nvme_init_identify(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|920| <<nvme_rdma_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1879| <<nvme_tcp_configure_admin_queue>> error = nvme_init_identify(ctrl);
+ *   - drivers/nvme/target/loop.c|399| <<nvme_loop_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+ */
 int nvme_init_identify(struct nvme_ctrl *ctrl)
 {
 	struct nvme_id_ctrl *id;
diff --git a/drivers/vfio/pci/vfio_pci_intrs.c b/drivers/vfio/pci/vfio_pci_intrs.c
index 869dce5f134d..909f3af12dc8 100644
--- a/drivers/vfio/pci/vfio_pci_intrs.c
+++ b/drivers/vfio/pci/vfio_pci_intrs.c
@@ -236,6 +236,10 @@ static void vfio_intx_disable(struct vfio_pci_device *vdev)
 /*
  * MSI/MSI-X
  */
+/*
+ * 在以下使用vfio_msihandler():
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|343| <<vfio_msi_set_vector_signal>> ret = request_irq(irq, vfio_msihandler, 0,
+ */
 static irqreturn_t vfio_msihandler(int irq, void *arg)
 {
 	struct eventfd_ctx *trigger = arg;
@@ -313,6 +317,21 @@ static int vfio_msi_set_vector_signal(struct vfio_pci_device *vdev,
 	if (fd < 0)
 		return 0;
 
+	/*
+	 * 虚拟机内部.
+	 *  24:          0          0          0          0         13         87   PCI-MSI 65536-edge      iavf-0000:00:04.0:mbx
+	 *  25:          0          0          0          0          0          0   PCI-MSI 65537-edge      iavf-ens4-TxRx-0
+	 *  26:          0          0          0          0          0          0   PCI-MSI 65538-edge      iavf-ens4-TxRx-1
+	 *  27:          0          0          0          0          0          0   PCI-MSI 65539-edge      iavf-ens4-TxRx-2
+	 *  28:          0          0          0          0          0          0   PCI-MSI 65540-edge      iavf-ens4-TxRx-3
+	 *
+	 * host上的irq.
+	 * IR-PCI-MSI 1802240-edge      vfio-msix[0](0000:03:0e.0)
+	 * IR-PCI-MSI 1802241-edge      vfio-msix[1](0000:03:0e.0)
+	 * IR-PCI-MSI 1802242-edge      vfio-msix[2](0000:03:0e.0)
+	 * IR-PCI-MSI 1802243-edge      vfio-msix[3](0000:03:0e.0)
+	 * IR-PCI-MSI 1802244-edge      vfio-msix[4](0000:03:0e.0)
+	 */
 	vdev->ctx[vector].name = kasprintf(GFP_KERNEL, "vfio-msi%s[%d](%s)",
 					   msix ? "x" : "", vector,
 					   pci_name(pdev));
@@ -340,6 +359,17 @@ static int vfio_msi_set_vector_signal(struct vfio_pci_device *vdev,
 		pci_write_msi_msg(irq, &msg);
 	}
 
+	/*
+	 * struct vfio_pci_device *vdev:
+	 * -> struct vfio_pci_irq_ctx *ctx;
+	 *    -> struct eventfd_ctx      *trigger;
+	 *    -> struct virqfd           *unmask;
+	 *    -> struct virqfd           *mask;
+	 *    -> char                    *name;
+	 *    -> bool                    masked;
+	 *    -> struct irq_bypass_producer      producer;
+	 * -> num_ctx;
+	 */
 	ret = request_irq(irq, vfio_msihandler, 0,
 			  vdev->ctx[vector].name, trigger);
 	vfio_pci_memory_unlock_and_restore(vdev, cmd);
@@ -558,6 +588,11 @@ static int vfio_pci_set_msi_trigger(struct vfio_pci_device *vdev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|623| <<vfio_pci_set_err_trigger>> return vfio_pci_set_ctx_trigger_single(&vdev->err_trigger,
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|634| <<vfio_pci_set_req_trigger>> return vfio_pci_set_ctx_trigger_single(&vdev->req_trigger,
+ */
 static int vfio_pci_set_ctx_trigger_single(struct eventfd_ctx **ctx,
 					   unsigned int count, uint32_t flags,
 					   void *data)
@@ -631,6 +666,10 @@ static int vfio_pci_set_req_trigger(struct vfio_pci_device *vdev,
 	if (index != VFIO_PCI_REQ_IRQ_INDEX || start != 0 || count > 1)
 		return -EINVAL;
 
+	/*
+	 * struct vfio_pci_device:
+	 * -> struct eventfd_ctx *req_trigger;
+	 */
 	return vfio_pci_set_ctx_trigger_single(&vdev->req_trigger,
 					       count, flags, data);
 }
diff --git a/drivers/vfio/pci/vfio_pci_private.h b/drivers/vfio/pci/vfio_pci_private.h
index 5c90e560c5c7..64dcf198b3d6 100644
--- a/drivers/vfio/pci/vfio_pci_private.h
+++ b/drivers/vfio/pci/vfio_pci_private.h
@@ -133,6 +133,15 @@ struct vfio_pci_device {
 	int			refcnt;
 	int			ioeventfds_nr;
 	struct eventfd_ctx	*err_trigger;
+	/*
+	 * 在以下使用vfio_pci_device:
+	 *   - drivers/vfio/pci/vfio_pci.c|572| <<vfio_pci_release>> if (vdev->req_trigger) {
+	 *   - drivers/vfio/pci/vfio_pci.c|573| <<vfio_pci_release>> eventfd_ctx_put(vdev->req_trigger);
+	 *   - drivers/vfio/pci/vfio_pci.c|574| <<vfio_pci_release>> vdev->req_trigger = NULL;
+	 *   - drivers/vfio/pci/vfio_pci.c|1726| <<vfio_pci_request>> if (vdev->req_trigger) {
+	 *   - drivers/vfio/pci/vfio_pci.c|1731| <<vfio_pci_request>> eventfd_signal(vdev->req_trigger, 1);
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|634| <<vfio_pci_set_req_trigger>> return vfio_pci_set_ctx_trigger_single(&vdev->req_trigger,
+	 */
 	struct eventfd_ctx	*req_trigger;
 	struct list_head	dummy_resources_list;
 	struct mutex		ioeventfds_lock;
diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c
index 0b4dedaa9128..104d9e4acc06 100644
--- a/drivers/vfio/vfio_iommu_type1.c
+++ b/drivers/vfio/vfio_iommu_type1.c
@@ -90,6 +90,17 @@ struct vfio_dma {
 	unsigned long		vaddr;		/* Process virtual addr */
 	size_t			size;		/* Map size (bytes) */
 	int			prot;		/* IOMMU_READ/WRITE */
+	/*
+	 * 在以下设置vfio_dma->iommu_mapped:
+	 *   - drivers/vfio/vfio_iommu_type1.c|948| <<vfio_unmap_unpin>> dma->iommu_mapped = false;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1286| <<vfio_pin_map_dma>> dma->iommu_mapped = true;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1531| <<vfio_iommu_replay>> dma->iommu_mapped = true;
+	 * 在以下使用vfio_dma->iommu_mapped:
+	 *   - drivers/vfio/vfio_iommu_type1.c|1010| <<update_user_bitmap>> if (!iommu->pinned_page_dirty_scope && dma->iommu_mapped)
+	 *   - drivers/vfio/vfio_iommu_type1.c|1467| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1515| <<vfio_iommu_replay>> if (!dma->iommu_mapped)
+	 *   - drivers/vfio/vfio_iommu_type1.c|1541| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+	 */
 	bool			iommu_mapped;
 	bool			lock_cap;	/* capable(CAP_IPC_LOCK) */
 	struct task_struct	*task;
@@ -173,6 +184,10 @@ static struct vfio_dma *vfio_find_dma(struct vfio_iommu *iommu,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1410| <<vfio_dma_do_map>> vfio_link_dma(iommu, dma);
+ */
 static void vfio_link_dma(struct vfio_iommu *iommu, struct vfio_dma *new)
 {
 	struct rb_node **link = &iommu->dma_list.rb_node, *parent = NULL;
@@ -441,6 +456,14 @@ static int follow_fault_pfn(struct vm_area_struct *vma, struct mm_struct *mm,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|516| <<vfio_pin_pages_remote>> ret = vaddr_get_pfn(current->mm, vaddr, dma->prot, pfn_base);
+ *   - drivers/vfio/vfio_iommu_type1.c|543| <<vfio_pin_pages_remote>> ret = vaddr_get_pfn(current->mm, vaddr, dma->prot, &pfn);
+ *   - drivers/vfio/vfio_iommu_type1.c|613| <<vfio_pin_page_external>> ret = vaddr_get_pfn(mm, vaddr, dma->prot, pfn_base);
+ *
+ * 一次处理一个page
+ */
 static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,
 			 int prot, unsigned long *pfn)
 {
@@ -453,6 +476,9 @@ static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,
 		flags |= FOLL_WRITE;
 
 	mmap_read_lock(mm);
+	/*
+	 * 一次pin 1个page !!
+	 */
 	ret = pin_user_pages_remote(mm, vaddr, 1, flags | FOLL_LONGTERM,
 				    page, NULL, NULL);
 	if (ret == 1) {
@@ -484,6 +510,19 @@ static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,
  * the iommu can only map chunks of consecutive pfns anyway, so get the
  * first page and all consecutive pages with the same locking.
  */
+/*
+ * [0] __alloc_pages_slowpath
+ * [0] __alloc_pages_nodemask
+ * [0] alloc_pages_vma
+ * [0] do_anonymous_page
+ * [0] __handle_mm_fault
+ * [0] handle_mm_fault
+ * [0] __get_user_pages
+ * [0] get_user_pages_remote
+ * [0] vaddr_get_pfn
+ * [0] vfio_pin_pages_remote
+ * [0] ret_from_fork
+ */
 static long vfio_pin_pages_remote(struct vfio_dma *dma, unsigned long vaddr,
 				  long npage, unsigned long *pfn_base,
 				  unsigned long limit)
@@ -524,6 +563,9 @@ static long vfio_pin_pages_remote(struct vfio_dma *dma, unsigned long vaddr,
 	/* Lock all the consecutive pages from pfn_base */
 	for (vaddr += PAGE_SIZE, iova += PAGE_SIZE; pinned < npage;
 	     pinned++, vaddr += PAGE_SIZE, iova += PAGE_SIZE) {
+		/*
+		 * 一次处理一个page
+		 */
 		ret = vaddr_get_pfn(current->mm, vaddr, dma->prot, &pfn);
 		if (ret)
 			break;
@@ -1234,9 +1276,27 @@ static int vfio_iommu_map(struct vfio_iommu *iommu, dma_addr_t iova,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1416| <<vfio_dma_do_map>> ret = vfio_pin_map_dma(iommu, dma, size);
+ */
 static int vfio_pin_map_dma(struct vfio_iommu *iommu, struct vfio_dma *dma,
 			    size_t map_size)
 {
+	/*
+	 * struct vfio_dma {
+	 *     struct rb_node          node;
+	 *     dma_addr_t              iova;           // Device address
+	 *     unsigned long           vaddr;          // Process virtual addr
+	 *     size_t                  size;           // Map size (bytes)
+	 *     int                     prot;           // IOMMU_READ/WRITE
+	 *     bool                    iommu_mapped;
+	 *     bool                    lock_cap;       // capable(CAP_IPC_LOCK)
+	 *     struct task_struct      *task;
+	 *     struct rb_root          pfn_list;       // Ex-user pinned pfn list
+	 *     unsigned long           *bitmap;
+	 * };
+	 */
 	dma_addr_t iova = dma->iova;
 	unsigned long vaddr = dma->vaddr;
 	size_t size = map_size;
@@ -1267,6 +1327,17 @@ static int vfio_pin_map_dma(struct vfio_iommu *iommu, struct vfio_dma *dma,
 		dma->size += npage << PAGE_SHIFT;
 	}
 
+	/*
+	 * 在以下设置vfio_dma->iommu_mapped:
+	 *   - drivers/vfio/vfio_iommu_type1.c|948| <<vfio_unmap_unpin>> dma->iommu_mapped = false;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1286| <<vfio_pin_map_dma>> dma->iommu_mapped = true;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1531| <<vfio_iommu_replay>> dma->iommu_mapped = true;
+	 * 在以下使用vfio_dma->iommu_mapped:
+	 *   - drivers/vfio/vfio_iommu_type1.c|1010| <<update_user_bitmap>> if (!iommu->pinned_page_dirty_scope && dma->iommu_mapped)
+	 *   - drivers/vfio/vfio_iommu_type1.c|1467| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1515| <<vfio_iommu_replay>> if (!dma->iommu_mapped)
+	 *   - drivers/vfio/vfio_iommu_type1.c|1541| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+	 */
 	dma->iommu_mapped = true;
 
 	if (ret)
@@ -1296,9 +1367,24 @@ static bool vfio_iommu_iova_dma_valid(struct vfio_iommu *iommu,
 	return list_empty(iova);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|2717| <<vfio_iommu_type1_map_dma>> return vfio_dma_do_map(iommu, &map);
+ */
 static int vfio_dma_do_map(struct vfio_iommu *iommu,
 			   struct vfio_iommu_type1_dma_map *map)
 {
+	/*
+	 * struct vfio_iommu_type1_dma_map {
+	 *     __u32   argsz;
+	 *     __u32   flags;
+	 * #define VFIO_DMA_MAP_FLAG_READ (1 << 0)         // readable from device
+	 * #define VFIO_DMA_MAP_FLAG_WRITE (1 << 1)        // writable from device
+	 *     __u64   vaddr;                          // Process virtual address
+	 *     __u64   iova;                           // IO virtual address
+	 *     __u64   size;                           // Size of mapping (bytes)
+	 * };
+	 */
 	dma_addr_t iova = map->iova;
 	unsigned long vaddr = map->vaddr;
 	size_t size = map->size;
@@ -2851,6 +2937,9 @@ static int vfio_iommu_type1_dirty_pages(struct vfio_iommu *iommu,
 	return -EINVAL;
 }
 
+/*
+ * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.ioctl = vfio_iommu_type1_ioctl()
+ */
 static long vfio_iommu_type1_ioctl(void *iommu_data,
 				   unsigned int cmd, unsigned long arg)
 {
diff --git a/drivers/xen/events/events_base.c b/drivers/xen/events/events_base.c
index e850f79351cb..308fff4f98da 100644
--- a/drivers/xen/events/events_base.c
+++ b/drivers/xen/events/events_base.c
@@ -118,6 +118,12 @@ struct irq_info {
 	} u;
 };
 
+/*
+ * 在以下使用PIRQ_NEEDS_EOI:
+ *   - drivers/xen/events/events_base.c|464| <<pirq_needs_eoi_flag>> return info->u.pirq.flags & PIRQ_NEEDS_EOI;
+ *   - drivers/xen/events/events_base.c|768| <<pirq_query_unmask>> info->u.pirq.flags &= ~PIRQ_NEEDS_EOI;
+ *   - drivers/xen/events/events_base.c|770| <<pirq_query_unmask>> info->u.pirq.flags |= PIRQ_NEEDS_EOI;
+ */
 #define PIRQ_NEEDS_EOI	(1 << 0)
 #define PIRQ_SHAREABLE	(1 << 1)
 #define PIRQ_MSI_GROUP	(1 << 2)
@@ -154,6 +160,14 @@ static DEFINE_RWLOCK(evtchn_rwlock);
  *       percpu eoi_list_lock
  */
 
+/*
+ * 在以下使用xen_irq_list_head:
+ *   - drivers/xen/events/events_base.c|673| <<xen_irq_init>> list_add_tail(&info->list, &xen_irq_list_head);
+ *   - drivers/xen/events/events_base.c|879| <<xen_irq_from_gsi>> list_for_each_entry(info, &xen_irq_list_head, list) {
+ *   - drivers/xen/events/events_base.c|1098| <<xen_irq_from_pirq>> list_for_each_entry(info, &xen_irq_list_head, list) {
+ *   - drivers/xen/events/events_base.c|1823| <<restore_pirqs>> list_for_each_entry(info, &xen_irq_list_head, list) {
+ *   - drivers/xen/events/events_base.c|1987| <<xen_irq_resume>> list_for_each_entry(info, &xen_irq_list_head, list) {
+ */
 static LIST_HEAD(xen_irq_list_head);
 
 /* IRQ <-> VIRQ mapping. */
@@ -163,10 +177,24 @@ static DEFINE_PER_CPU(int [NR_VIRQS], virq_to_irq) = {[0 ... NR_VIRQS-1] = -1};
 static DEFINE_PER_CPU(int [XEN_NR_IPIS], ipi_to_irq) = {[0 ... XEN_NR_IPIS-1] = -1};
 
 /* Event channel distribution data */
+/*
+ * 在以下使用channels_on_cpu[NR_CPUS]:
+ *   - drivers/xen/events/events_base.c|275| <<channels_on_cpu_dec>> WARN_ON_ONCE(!atomic_add_unless(&channels_on_cpu[info->cpu], -1 , 0));
+ *   - drivers/xen/events/events_base.c|283| <<channels_on_cpu_inc>> if (WARN_ON_ONCE(!atomic_add_unless(&channels_on_cpu[info->cpu], 1,
+ *   - drivers/xen/events/events_base.c|1738| <<select_target_cpu>> unsigned int curch = atomic_read(&channels_on_cpu[cpu]);
+ */
 static atomic_t channels_on_cpu[NR_CPUS];
 
 static int **evtchn_to_irq;
 #ifdef CONFIG_X86
+/*
+ * called by:
+ *   - drivers/xen/events/events_base.c|494| <<pirq_check_eoi_map>> return test_bit(pirq_from_irq(irq), pirq_eoi_map);
+ *   - drivers/xen/events/events_base.c|2210| <<xen_init_IRQ>> pirq_eoi_map = (void *)__get_free_page(GFP_KERNEL|__GFP_ZERO);
+ *   - drivers/xen/events/events_base.c|2211| <<xen_init_IRQ>> eoi_gmfn.gmfn = virt_to_gfn(pirq_eoi_map);
+ *   - drivers/xen/events/events_base.c|2214| <<xen_init_IRQ>> free_page((unsigned long ) pirq_eoi_map);
+ *   - drivers/xen/events/events_base.c|2215| <<xen_init_IRQ>> pirq_eoi_map = NULL;
+ */
 static unsigned long *pirq_eoi_map;
 #endif
 static bool (*pirq_needs_eoi)(unsigned irq);
@@ -375,6 +403,25 @@ static void xen_irq_info_cleanup(struct irq_info *info)
 /*
  * Accessors for packed IRQ information.
  */
+/*
+ * called by:
+ *   - drivers/xen/events/events_2l.c|179| <<evtchn_2l_handle_events>> evtchn_port_t evtchn = evtchn_from_irq(irq);
+ *   - drivers/xen/events/events_base.c|498| <<notify_remote_via_irq>> evtchn_port_t evtchn = evtchn_from_irq(irq);
+ *   - drivers/xen/events/events_base.c|775| <<eoi_pirq>> evtchn_port_t evtchn = evtchn_from_irq(data->irq);
+ *   - drivers/xen/events/events_base.c|800| <<__startup_pirq>> evtchn_port_t evtchn = evtchn_from_irq(irq);
+ *   - drivers/xen/events/events_base.c|853| <<shutdown_pirq>> evtchn_port_t evtchn = evtchn_from_irq(irq);
+ *   - drivers/xen/events/events_base.c|893| <<__unbind_from_irq>> evtchn_port_t evtchn = evtchn_from_irq(irq);
+ *   - drivers/xen/events/events_base.c|1478| <<xen_set_irq_priority>> set_priority.port = evtchn_from_irq(irq);
+ *   - drivers/xen/events/events_base.c|1762| <<set_affinity_irq>> ret = xen_rebind_evtchn_to_cpu(evtchn_from_irq(data->irq), tcpu);
+ *   - drivers/xen/events/events_base.c|1771| <<enable_dynirq>> evtchn_port_t evtchn = evtchn_from_irq(data->irq);
+ *   - drivers/xen/events/events_base.c|1779| <<disable_dynirq>> evtchn_port_t evtchn = evtchn_from_irq(data->irq);
+ *   - drivers/xen/events/events_base.c|1787| <<ack_dynirq>> evtchn_port_t evtchn = evtchn_from_irq(data->irq);
+ *   - drivers/xen/events/events_base.c|1803| <<retrigger_dynirq>> evtchn_port_t evtchn = evtchn_from_irq(data->irq);
+ *   - drivers/xen/events/events_base.c|1911| <<xen_clear_irq_pending>> evtchn_port_t evtchn = evtchn_from_irq(irq);
+ *   - drivers/xen/events/events_base.c|1919| <<xen_set_irq_pending>> evtchn_port_t evtchn = evtchn_from_irq(irq);
+ *   - drivers/xen/events/events_base.c|1927| <<xen_test_irq_pending>> evtchn_port_t evtchn = evtchn_from_irq(irq);
+ *   - drivers/xen/events/events_base.c|1940| <<xen_poll_irq_timeout>> evtchn_port_t evtchn = evtchn_from_irq(irq);
+ */
 evtchn_port_t evtchn_from_irq(unsigned irq)
 {
 	const struct irq_info *info = NULL;
@@ -505,6 +552,15 @@ EXPORT_SYMBOL_GPL(notify_remote_via_irq);
 struct lateeoi_work {
 	struct delayed_work delayed;
 	spinlock_t eoi_list_lock;
+	/*
+	 * 在以下使用lateeio_work->eoi_list:
+	 *   - drivers/xen/events/events_base.c|585| <<lateeoi_list_add>> if (list_empty(&eoi->eoi_list)) {
+	 *   - drivers/xen/events/events_base.c|586| <<lateeoi_list_add>> list_add(&info->eoi_list, &eoi->eoi_list);
+	 *   - drivers/xen/events/events_base.c|590| <<lateeoi_list_add>> list_for_each_entry_reverse(elem, &eoi->eoi_list, eoi_list) {
+	 *   - drivers/xen/events/events_base.c|594| <<lateeoi_list_add>> list_add(&info->eoi_list, &elem->eoi_list);
+	 *   - drivers/xen/events/events_base.c|652| <<xen_irq_lateeoi_worker>> info = list_first_entry_or_null(&eoi->eoi_list, struct irq_info,
+	 *   - drivers/xen/events/events_base.c|682| <<xen_cpu_init_eoi>> INIT_LIST_HEAD(&eoi->eoi_list);
+	 */
 	struct list_head eoi_list;
 };
 
@@ -1906,6 +1962,12 @@ static void restore_cpu_ipis(unsigned int cpu)
 }
 
 /* Clear an irq's pending state, in preparation for polling on it */
+/*
+ * called by:
+ *   - arch/x86/xen/spinlock.c|51| <<xen_qlock_wait>> xen_clear_irq_pending(irq);
+ *   - drivers/pci/xen-pcifront.c|138| <<do_pci_op>> xen_clear_irq_pending(irq);
+ *   - drivers/pci/xen-pcifront.c|143| <<do_pci_op>> xen_clear_irq_pending(irq);
+ */
 void xen_clear_irq_pending(int irq)
 {
 	evtchn_port_t evtchn = evtchn_from_irq(irq);
diff --git a/drivers/xen/events/events_internal.h b/drivers/xen/events/events_internal.h
index 0a97c0549db7..17d3ae1fe490 100644
--- a/drivers/xen/events/events_internal.h
+++ b/drivers/xen/events/events_internal.h
@@ -63,6 +63,10 @@ static inline void xen_evtchn_port_bind_to_cpu(evtchn_port_t evtchn,
 
 static inline void clear_evtchn(evtchn_port_t port)
 {
+	/*
+	 * evtchn_2l_clear_pending()
+	 * evtchn_fifo_clear_pending()
+	 */
 	evtchn_ops->clear_pending(port);
 }
 
diff --git a/include/linux/clocksource.h b/include/linux/clocksource.h
index 86d143db6523..d83e4137900b 100644
--- a/include/linux/clocksource.h
+++ b/include/linux/clocksource.h
@@ -232,6 +232,18 @@ static inline int __clocksource_register(struct clocksource *cs)
 	return __clocksource_register_scale(cs, 1, 0);
 }
 
+/*
+ * 部分调用的例子:
+ *   - arch/x86/kernel/hpet.c|1013| <<hpet_enable>> clocksource_register_hz(&clocksource_hpet, (u32)hpet_freq);
+ *   - arch/x86/kernel/kvmclock.c|374| <<kvmclock_init>> clocksource_register_hz(&kvm_clock, NSEC_PER_SEC);
+ *   - arch/x86/platform/uv/uv_time.c|348| <<uv_rtc_setup_clock>> rc = clocksource_register_hz(&clocksource_uv, sn_rtc_cycles_per_second);
+ *   - arch/x86/xen/time.c|492| <<xen_time_init>> clocksource_register_hz(&xen_clocksource, NSEC_PER_SEC);
+ *   - drivers/char/hpet.c|950| <<hpet_alloc>> clocksource_register_hz(&clocksource_hpet, hpetp->hp_tick_freq);
+ *   - drivers/clocksource/acpi_pm.c|213| <<init_acpi_pm_clocksource>> return clocksource_register_hz(&clocksource_acpi_pm,
+ *   - drivers/clocksource/hyperv_timer.c|445| <<hv_init_tsc_clocksource>> clocksource_register_hz(&hyperv_cs_tsc, NSEC_PER_SEC/100);
+ *   - drivers/clocksource/hyperv_timer.c|470| <<hv_init_clocksource>> clocksource_register_hz(&hyperv_cs_msr, NSEC_PER_SEC/100);
+ *   - drivers/clocksource/i8253.c|106| <<clocksource_i8253_init>> return clocksource_register_hz(&i8253_cs, PIT_TICK_RATE);
+ */
 static inline int clocksource_register_hz(struct clocksource *cs, u32 hz)
 {
 	return __clocksource_register_scale(cs, 1, hz);
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f3b1013fb22c..ea1e10907de3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -263,6 +263,14 @@ struct kvm_mmio_fragment {
 struct kvm_vcpu {
 	struct kvm *kvm;
 #ifdef CONFIG_PREEMPT_NOTIFIERS
+	/*
+	 * 在以下使用kvm_vcpu->preempt_notifier:
+	 *  - virt/kvm/kvm_main.c|209| <<vcpu_load>> preempt_notifier_register(&vcpu->preempt_notifier);
+	 *  - virt/kvm/kvm_main.c|219| <<vcpu_put>> preempt_notifier_unregister(&vcpu->preempt_notifier);
+	 *  - virt/kvm/kvm_main.c|415| <<kvm_vcpu_init>> preempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);
+	 *  - virt/kvm/kvm_main.c|4817| <<preempt_notifier_to_vcpu>> struct kvm_vcpu *preempt_notifier_to_vcpu(struct preempt_notifier *pn)
+	 *  - virt/kvm/kvm_main.c|4819| <<preempt_notifier_to_vcpu>> return container_of(pn, struct kvm_vcpu, preempt_notifier);
+	 */
 	struct preempt_notifier preempt_notifier;
 #endif
 	int cpu;
@@ -463,6 +471,20 @@ struct kvm {
 	 * incremented after storing the kvm_vcpu pointer in vcpus,
 	 * and is accessed atomically.
 	 */
+	/*
+	 * x86在以下修改kvm->online_vcpus:
+	 *   - arch/x86/kvm/x86.c|10517| <<kvm_free_vcpus>> atomic_set(&kvm->online_vcpus, 0);
+	 *   - virt/kvm/kvm_main.c|3427| <<kvm_vm_ioctl_create_vcpu>> atomic_inc(&kvm->online_vcpus);
+	 * x86在以下使用kvm->online_vcpus:
+	 *   - arch/x86/kvm/x86.c|2146| <<kvm_track_tsc_matching>> atomic_read(&vcpu->kvm->online_vcpus));
+	 *   - arch/x86/kvm/x86.c|2161| <<kvm_track_tsc_matching>> atomic_read(&vcpu->kvm->online_vcpus),
+	 *   - arch/x86/kvm/x86.c|2517| <<pvclock_update_vm_gtod_copy>> atomic_read(&kvm->online_vcpus));
+	 *   - arch/x86/kvm/x86.c|10012| <<kvm_arch_vcpu_precreate>> if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
+	 *   - arch/x86/kvm/x86.c|10514| <<kvm_free_vcpus>> for (i = 0; i < atomic_read(&kvm->online_vcpus); i++)
+	 *   - include/linux/kvm_host.h|577| <<kvm_get_vcpu>> int num_vcpus = atomic_read(&kvm->online_vcpus);
+	 *   - include/linux/kvm_host.h|587| <<kvm_for_each_vcpu>> idx < atomic_read(&kvm->online_vcpus) && \
+	 *   - virt/kvm/kvm_main.c|3409| <<kvm_vm_ioctl_create_vcpu>> vcpu->vcpu_idx = atomic_read(&kvm->online_vcpus);
+	 */
 	atomic_t online_vcpus;
 	int created_vcpus;
 	int last_boosted_vcpu;
@@ -480,6 +502,17 @@ struct kvm {
 #endif
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|561| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|651| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|798| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|856| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|926| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|932| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|946| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|4568| <<kvm_debugfs_open>> if (!refcount_inc_not_zero(&stat_data->kvm->users_count))
+	 */
 	refcount_t users_count;
 #ifdef CONFIG_KVM_MMIO
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index 374c67875cdb..f784a924e641 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -108,6 +108,18 @@ struct kvm_userspace_memory_region {
  * include/linux/kvm_host.h.
  */
 #define KVM_MEM_LOG_DIRTY_PAGES	(1UL << 0)
+/*
+ * 在以下使用KVM_MEM_READONLY:
+ *   - arch/arm64/kvm/mmu.c|72| <<memslot_is_logging>> return memslot->dirty_bitmap && !(memslot->flags & KVM_MEM_READONLY);
+ *   - arch/arm64/kvm/mmu.c|1301| <<kvm_arch_prepare_memory_region>> bool writable = !(mem->flags & KVM_MEM_READONLY);
+ *   - arch/mips/kvm/mmu.c|513| <<kvm_set_spte_handler>> else if (memslot->flags & KVM_MEM_READONLY)
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|963| <<kvmppc_book3s_radix_page_fault>> if (memslot->flags & KVM_MEM_READONLY) {
+ *   - arch/powerpc/kvm/book3s_hv_nested.c|1365| <<__kvmhv_nested_page_fault>> if (memslot->flags & KVM_MEM_READONLY) {
+ *   - arch/x86/kvm/x86.c|10729| <<kvm_mmu_slot_apply_flags>> if ((change != KVM_MR_FLAGS_ONLY) || (new->flags & KVM_MEM_READONLY))
+ *   - virt/kvm/kvm_main.c|1173| <<check_memory_region_flags>> valid_flags |= KVM_MEM_READONLY;
+ *   - virt/kvm/kvm_main.c|1408| <<__kvm_set_memory_region>> ((new.flags ^ old.flags) & KVM_MEM_READONLY))
+ *   - virt/kvm/kvm_main.c|1811| <<memslot_is_readonly>> return slot->flags & KVM_MEM_READONLY;
+ */
 #define KVM_MEM_READONLY	(1UL << 1)
 
 /* for KVM_IRQ_LINE */
diff --git a/kernel/cpu.c b/kernel/cpu.c
index 4e11e91010e1..a21131530723 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -145,6 +145,18 @@ static struct cpuhp_step *cpuhp_get_step(enum cpuhp_state state)
  *
  * Called from cpu hotplug and from the state register machinery.
  */
+/*
+ * called by:
+ *   - kernel/cpu.c|590| <<undo_cpu_up>> cpuhp_invoke_callback(cpu, st->state, false, NULL, NULL);
+ *   - kernel/cpu.c|615| <<cpuhp_up_callbacks>> ret = cpuhp_invoke_callback(cpu, st->state, true, NULL, NULL);
+ *   - kernel/cpu.c|703| <<cpuhp_thread_fun>> st->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);
+ *   - kernel/cpu.c|711| <<cpuhp_thread_fun>> st->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);
+ *   - kernel/cpu.c|753| <<cpuhp_invoke_ap_callback>> return cpuhp_invoke_callback(cpu, state, bringup, node, NULL);
+ *   - kernel/cpu.c|884| <<take_cpu_down>> ret = cpuhp_invoke_callback(cpu, st->state, false, NULL, NULL);
+ *   - kernel/cpu.c|974| <<undo_cpu_down>> cpuhp_invoke_callback(cpu, st->state, true, NULL, NULL);
+ *   - kernel/cpu.c|984| <<cpuhp_down_callbacks>> ret = cpuhp_invoke_callback(cpu, st->state, false, NULL, NULL);
+ *   - kernel/cpu.c|1162| <<notify_cpu_starting>> ret = cpuhp_invoke_callback(cpu, st->state, true, NULL, NULL);
+ */
 static int cpuhp_invoke_callback(unsigned int cpu, enum cpuhp_state state,
 				 bool bringup, struct hlist_node *node,
 				 struct hlist_node **lastp)
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index ff74fca39ed2..adfe79bc55dc 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3864,6 +3864,14 @@ void wake_up_new_task(struct task_struct *p)
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 
+/*
+ * 在以下使用preempt_notifier_key:
+ *   - kernel/sched/core.c|3871| <<preempt_notifier_inc>> static_branch_inc(&preempt_notifier_key);
+ *   - kernel/sched/core.c|3877| <<preempt_notifier_dec>> static_branch_dec(&preempt_notifier_key);
+ *   - kernel/sched/core.c|3887| <<preempt_notifier_register>> if (!static_branch_unlikely(&preempt_notifier_key))
+ *   - kernel/sched/core.c|3916| <<fire_sched_in_preempt_notifiers>> if (static_branch_unlikely(&preempt_notifier_key))
+ *   - kernel/sched/core.c|3934| <<fire_sched_out_preempt_notifiers>> if (static_branch_unlikely(&preempt_notifier_key))
+ */
 static DEFINE_STATIC_KEY_FALSE(preempt_notifier_key);
 
 void preempt_notifier_inc(void)
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index e996989cd580..9e1991af6268 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -608,6 +608,10 @@ kvm_irqfd_release(struct kvm *kvm)
  * Take note of a change in irq routing.
  * Caller must invoke synchronize_srcu(&kvm->irq_srcu) afterwards.
  */
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|222| <<kvm_set_irq_routing>> kvm_irq_routing_update(kvm);
+ */
 void kvm_irq_routing_update(struct kvm *kvm)
 {
 	struct kvm_kernel_irqfd *irqfd;
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 58e4f88b2b9f..2f8ba72efff7 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -166,6 +166,15 @@ bool __weak kvm_arch_can_set_irq_routing(struct kvm *kvm)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|152| <<kvm_vgic_setup_default_irq_routing>> ret = kvm_set_irq_routing(kvm, entries, nr, 0);
+ *   - arch/powerpc/kvm/mpic.c|1649| <<mpic_set_default_irq_routing>> kvm_set_irq_routing(opp->kvm, routing, 0, 0);
+ *   - arch/s390/kvm/kvm-s390.c|2391| <<kvm_arch_vm_ioctl>> r = kvm_set_irq_routing(kvm, &routing, 0, 0);
+ *   - arch/x86/kvm/irq_comm.c|375| <<kvm_setup_default_irq_routing>> return kvm_set_irq_routing(kvm, default_routing,
+ *   - arch/x86/kvm/irq_comm.c|383| <<kvm_setup_empty_irq_routing>> return kvm_set_irq_routing(kvm, empty_routing, 0, 0);
+ *   - virt/kvm/kvm_main.c|4171| <<kvm_vm_ioctl>> r = kvm_set_irq_routing(kvm, entries, routing.nr,
+ */
 int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *ue,
 			unsigned nr,
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 8367d88ce39b..27af678d79f8 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -107,7 +107,22 @@ static atomic_t hardware_enable_failed;
 
 static struct kmem_cache *kvm_vcpu_cache;
 
+/*
+ * 在以下使用kvm_preempt_ops:
+ *   - virt/kvm/kvm_main.c|415| <<kvm_vcpu_init>> preempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);
+ *   - virt/kvm/kvm_main.c|4963| <<kvm_init>> kvm_preempt_ops.sched_in = kvm_sched_in;
+ *   - virt/kvm/kvm_main.c|4964| <<kvm_init>> kvm_preempt_ops.sched_out = kvm_sched_out;
+ */
 static __read_mostly struct preempt_ops kvm_preempt_ops;
+/*
+ * 在以下使用kvm_running_vcpu:
+ *   - virt/kvm/kvm_main.c|219| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|231| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|5036| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|5051| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|5068| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+ *   - virt/kvm/kvm_main.c|5080| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+ */
 static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
 
 struct dentry *kvm_debugfs_dir;
@@ -145,6 +160,11 @@ static void hardware_disable_all(void);
 
 static void kvm_io_bus_destroy(struct kvm_io_bus *bus);
 
+/*
+ * 在以下使用kvm_rebooting:
+ *   - arch/x86/kvm/x86.c|419| <<kvm_spurious_fault>> BUG_ON(!kvm_rebooting);
+ *   - virt/kvm/kvm_main.c|4225| <<kvm_reboot>> kvm_rebooting = true;
+ */
 __visible bool kvm_rebooting;
 EXPORT_SYMBOL_GPL(kvm_rebooting);
 
@@ -382,6 +402,18 @@ void kvm_mmu_free_memory_cache(struct kvm_mmu_memory_cache *mc)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|538| <<stage2_map_walk_leaf>> childp = kvm_mmu_memory_cache_alloc(data->memcache);
+ *   - arch/mips/kvm/mmu.c|124| <<kvm_mips_walk_pgd>> new_pmd = kvm_mmu_memory_cache_alloc(cache);
+ *   - arch/mips/kvm/mmu.c|135| <<kvm_mips_walk_pgd>> new_pte = kvm_mmu_memory_cache_alloc(cache);
+ *   - arch/x86/kvm/mmu/mmu.c|693| <<mmu_alloc_pte_list_desc>> return kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_pte_list_desc_cache);
+ *   - arch/x86/kvm/mmu/mmu.c|1687| <<kvm_mmu_alloc_page>> sp = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_page_header_cache);
+ *   - arch/x86/kvm/mmu/mmu.c|1688| <<kvm_mmu_alloc_page>> sp->spt = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_shadow_page_cache);
+ *   - arch/x86/kvm/mmu/mmu.c|1690| <<kvm_mmu_alloc_page>> sp->gfns = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_gfn_array_cache);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|146| <<alloc_tdp_mmu_page>> sp = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_page_header_cache);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|147| <<alloc_tdp_mmu_page>> sp->spt = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_shadow_page_cache);
+ */
 void *kvm_mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)
 {
 	void *p;
@@ -433,6 +465,17 @@ void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_vcpu_destroy);
 
 #if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|445| <<kvm_mmu_notifier_invalidate_range>> struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ *   - virt/kvm/kvm_main.c|458| <<kvm_mmu_notifier_change_pte>> struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ *   - virt/kvm/kvm_main.c|475| <<kvm_mmu_notifier_invalidate_range_start>> struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ *   - virt/kvm/kvm_main.c|501| <<kvm_mmu_notifier_invalidate_range_end>> struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ *   - virt/kvm/kvm_main.c|527| <<kvm_mmu_notifier_clear_flush_young>> struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ *   - virt/kvm/kvm_main.c|548| <<kvm_mmu_notifier_clear_young>> struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ *   - virt/kvm/kvm_main.c|577| <<kvm_mmu_notifier_test_young>> struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ *   - virt/kvm/kvm_main.c|592| <<kvm_mmu_notifier_release>> struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ */
 static inline struct kvm *mmu_notifier_to_kvm(struct mmu_notifier *mn)
 {
 	return container_of(mn, struct kvm, mmu_notifier);
@@ -597,6 +640,10 @@ static void kvm_mmu_notifier_release(struct mmu_notifier *mn,
 	srcu_read_unlock(&kvm->srcu, idx);
 }
 
+/*
+ * 只在以下使用kvm_mmu_notifier_ops:
+ *   - virt/kvm/kvm_main.c|647| <<kvm_init_mmu_notifier>> kvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;
+ */
 static const struct mmu_notifier_ops kvm_mmu_notifier_ops = {
 	.invalidate_range	= kvm_mmu_notifier_invalidate_range,
 	.invalidate_range_start	= kvm_mmu_notifier_invalidate_range_start,
@@ -608,6 +655,10 @@ static const struct mmu_notifier_ops kvm_mmu_notifier_ops = {
 	.release		= kvm_mmu_notifier_release,
 };
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|830| <<kvm_create_vm>> r = kvm_init_mmu_notifier(kvm);
+ */
 static int kvm_init_mmu_notifier(struct kvm *kvm)
 {
 	kvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;
@@ -686,6 +737,10 @@ static void kvm_destroy_vm_debugfs(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4097| <<kvm_dev_ioctl_create_vm>> if (kvm_create_vm_debugfs(kvm, r) < 0) {
+ */
 static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 {
 	char dir_name[ITOA_MAX_LEN * 2];
@@ -761,6 +816,17 @@ static struct kvm *kvm_create_vm(unsigned long type)
 	if (init_srcu_struct(&kvm->irq_srcu))
 		goto out_err_no_irq_srcu;
 
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|561| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|651| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|798| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|856| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|926| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|932| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|946| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|4568| <<kvm_debugfs_open>> if (!refcount_inc_not_zero(&stat_data->kvm->users_count))
+	 */
 	refcount_set(&kvm->users_count, 1);
 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
 		struct kvm_memslots *slots = kvm_alloc_memslots();
@@ -927,6 +993,10 @@ static int kvm_vm_release(struct inode *inode, struct file *filp)
  * Allocation size is twice as large as the actual dirty bitmap size.
  * See kvm_vm_ioctl_get_dirty_log() why this is needed.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1426| <<__kvm_set_memory_region>> r = kvm_alloc_dirty_bitmap(&new);
+ */
 static int kvm_alloc_dirty_bitmap(struct kvm_memory_slot *memslot)
 {
 	unsigned long dirty_bytes = 2 * kvm_dirty_bitmap_bytes(memslot);
@@ -1076,6 +1146,10 @@ static inline int kvm_memslot_move_forward(struct kvm_memslots *slots,
  * advantageous.  The current binary search starts from the middle of the array
  * and uses an LRU pointer to improve performance for all memslots and GFNs.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1280| <<kvm_set_memslot>> update_memslots(slots, new, change);
+ */
 static void update_memslots(struct kvm_memslots *slots,
 			    struct kvm_memory_slot *memslot,
 			    enum kvm_mr_change change)
@@ -1155,6 +1229,10 @@ static struct kvm_memslots *install_new_memslots(struct kvm *kvm,
  * when deleting a memslot, as we need a complete duplicate of the memslots for
  * use when invalidating a memslot prior to deleting/moving the memslot.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1246| <<kvm_set_memslot>> slots = kvm_dup_memslots(__kvm_memslots(kvm, as_id), change);
+ */
 static struct kvm_memslots *kvm_dup_memslots(struct kvm_memslots *old,
 					     enum kvm_mr_change change)
 {
@@ -1481,6 +1559,10 @@ EXPORT_SYMBOL_GPL(kvm_get_dirty_log);
  * exiting to userspace will be logged for the next call.
  *
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1639| <<kvm_vm_ioctl_get_dirty_log>> r = kvm_get_dirty_log_protect(kvm, log);
+ */
 static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 {
 	struct kvm_memslots *slots;
@@ -1709,6 +1791,10 @@ bool kvm_vcpu_is_visible_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_is_visible_gfn);
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_xive_native.c|644| <<kvmppc_xive_native_set_queue_config>> page_size = kvm_host_page_size(vcpu, gfn);
+ */
 unsigned long kvm_host_page_size(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	struct vm_area_struct *vma;
@@ -1750,6 +1836,9 @@ static unsigned long __gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,
 	if (nr_pages)
 		*nr_pages = slot->npages - (gfn - slot->base_gfn);
 
+	/*
+	 * 返回slot->userspace_addr + (gfn - slot->base_gfn) * PAGE_SIZE;
+	 */
 	return __gfn_to_hva_memslot(slot, gfn);
 }
 
@@ -1824,6 +1913,10 @@ static inline int check_user_page_hwpoison(unsigned long addr)
  * true indicates success, otherwise false is returned.  It's also the
  * only part that runs if we can in atomic context.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2047| <<hva_to_pfn>> if (hva_to_pfn_fast(addr, write_fault, writable, &pfn))
+ */
 static bool hva_to_pfn_fast(unsigned long addr, bool write_fault,
 			    bool *writable, kvm_pfn_t *pfn)
 {
@@ -1852,6 +1945,10 @@ static bool hva_to_pfn_fast(unsigned long addr, bool write_fault,
  * The slow path to get the pfn of the specified host virtual address,
  * 1 indicates success, -errno is returned if error is detected.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2053| <<hva_to_pfn>> npages = hva_to_pfn_slow(addr, async, write_fault, writable, &pfn);
+ */
 static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,
 			   bool *writable, kvm_pfn_t *pfn)
 {
@@ -1898,6 +1995,10 @@ static bool vma_is_valid(struct vm_area_struct *vma, bool write_fault)
 	return true;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2070| <<hva_to_pfn>> r = hva_to_pfn_remapped(vma, addr, async, write_fault, writable, &pfn);
+ */
 static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 			       unsigned long addr, bool *async,
 			       bool write_fault, bool *writable,
@@ -1961,6 +2062,10 @@ static int hva_to_pfn_remapped(struct vm_area_struct *vma,
  * 2): @write_fault = false && @writable, @writable will tell the caller
  *     whether the mapping is writable.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2109| <<__gfn_to_pfn_memslot>> return hva_to_pfn(addr, atomic, async, write_fault,
+ */
 static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,
 			bool write_fault, bool *writable)
 {
@@ -2130,6 +2235,10 @@ void kvm_release_pfn(kvm_pfn_t pfn, bool dirty, struct gfn_to_pfn_cache *cache)
 		kvm_release_pfn_clean(pfn);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2236| <<__kvm_map_gfn>> kvm_cache_gfn_to_pfn(slot, gfn, cache, gen);
+ */
 static void kvm_cache_gfn_to_pfn(struct kvm_memory_slot *slot, gfn_t gfn,
 				 struct gfn_to_pfn_cache *cache, u64 gen)
 {
@@ -2141,6 +2250,11 @@ static void kvm_cache_gfn_to_pfn(struct kvm_memory_slot *slot, gfn_t gfn,
 	cache->generation = gen;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2275| <<kvm_map_gfn>> return __kvm_map_gfn(kvm_memslots(vcpu->kvm), gfn, map,
+ *   - virt/kvm/kvm_main.c|2282| <<kvm_vcpu_map>> return __kvm_map_gfn(kvm_vcpu_memslots(vcpu), gfn, map,
+ */
 static int __kvm_map_gfn(struct kvm_memslots *slots, gfn_t gfn,
 			 struct kvm_host_map *map,
 			 struct gfn_to_pfn_cache *cache,
@@ -2196,6 +2310,11 @@ static int __kvm_map_gfn(struct kvm_memslots *slots, gfn_t gfn,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2979| <<record_steal_time>> if (kvm_map_gfn(vcpu, vcpu->arch.st.msr_val >> PAGE_SHIFT,
+ *   - arch/x86/kvm/x86.c|4057| <<kvm_steal_time_set_preempted>> if (kvm_map_gfn(vcpu, vcpu->arch.st.msr_val >> PAGE_SHIFT, &map,
+ */
 int kvm_map_gfn(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map,
 		struct gfn_to_pfn_cache *cache, bool atomic)
 {
@@ -2204,6 +2323,23 @@ int kvm_map_gfn(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map,
 }
 EXPORT_SYMBOL_GPL(kvm_map_gfn);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|477| <<nested_svm_vmrun>> ret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb12_gpa), &map);
+ *   - arch/x86/kvm/svm/nested.c|589| <<nested_svm_vmexit>> rc = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(svm->nested.vmcb12_gpa), &map);
+ *   - arch/x86/kvm/svm/sev.c|1847| <<sev_handle_vmgexit>> if (kvm_vcpu_map(&svm->vcpu, ghcb_gpa >> PAGE_SHIFT, &svm->ghcb_map)) {
+ *   - arch/x86/kvm/svm/svm.c|2133| <<vmload_interception>> ret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(svm->vmcb->save.rax), &map);
+ *   - arch/x86/kvm/svm/svm.c|2159| <<vmsave_interception>> ret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(svm->vmcb->save.rax), &map);
+ *   - arch/x86/kvm/svm/svm.c|4291| <<svm_pre_leave_smm>> if (kvm_vcpu_map(&svm->vcpu,
+ *   - arch/x86/kvm/vmx/nested.c|623| <<nested_vmx_prepare_msr_bitmap>> if (kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->msr_bitmap), map))
+ *   - arch/x86/kvm/vmx/nested.c|719| <<nested_cache_shadow_vmcs12>> if (kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->vmcs_link_pointer), &map))
+ *   - arch/x86/kvm/vmx/nested.c|2052| <<nested_vmx_handle_enlightened_vmptrld>> if (kvm_vcpu_map(vcpu, gpa_to_gfn(evmcs_gpa),
+ *   - arch/x86/kvm/vmx/nested.c|2995| <<nested_vmx_check_vmcs_link_ptr>> if (CC(kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->vmcs_link_pointer), &map)))
+ *   - arch/x86/kvm/vmx/nested.c|3252| <<nested_get_vmcs12_pages>> if (!kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->virtual_apic_page_addr), map)) {
+ *   - arch/x86/kvm/vmx/nested.c|3278| <<nested_get_vmcs12_pages>> if (!kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->posted_intr_desc_addr), map)) {
+ *   - arch/x86/kvm/vmx/nested.c|5422| <<handle_vmptrld>> if (kvm_vcpu_map(vcpu, gpa_to_gfn(vmptr), &map)) {
+ *   - arch/x86/kvm/x86.c|6438| <<emulator_cmpxchg_emulated>> if (kvm_vcpu_map(vcpu, gpa_to_gfn(gpa), &map))
+ */
 int kvm_vcpu_map(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map)
 {
 	return __kvm_map_gfn(kvm_vcpu_memslots(vcpu), gfn, map,
@@ -2304,6 +2440,17 @@ void kvm_release_pfn_dirty(kvm_pfn_t pfn)
 }
 EXPORT_SYMBOL_GPL(kvm_release_pfn_dirty);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|884| <<user_mem_abort>> kvm_set_pfn_dirty(pfn);
+ *   - arch/mips/kvm/mmu.c|622| <<_kvm_mips_map_page_fast>> kvm_set_pfn_dirty(pfn);
+ *   - arch/mips/kvm/mmu.c|736| <<kvm_mips_map_page>> kvm_set_pfn_dirty(pfn);
+ *   - arch/powerpc/kvm/book3s_64_mmu_host.c|128| <<kvmppc_mmu_map_page>> kvm_set_pfn_dirty(pfn);
+ *   - arch/powerpc/kvm/e500_mmu_host.c|259| <<kvmppc_e500_ref_setup>> kvm_set_pfn_dirty(pfn);
+ *   - arch/x86/kvm/mmu/mmu.c|532| <<mmu_spte_update>> kvm_set_pfn_dirty(spte_to_pfn(old_spte));
+ *   - arch/x86/kvm/mmu/mmu.c|570| <<mmu_spte_clear_track_bits>> kvm_set_pfn_dirty(pfn);
+ *   - arch/x86/kvm/mmu/mmu.c|625| <<mmu_spte_age>> kvm_set_pfn_dirty(spte_to_pfn(spte));
+ */
 void kvm_set_pfn_dirty(kvm_pfn_t pfn)
 {
 	if (!kvm_is_reserved_pfn(pfn) && !kvm_is_zone_device_pfn(pfn))
@@ -2348,6 +2495,10 @@ static int __kvm_read_guest_page(struct kvm_memory_slot *slot, gfn_t gfn,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2450| <<kvm_read_guest>> ret = kvm_read_guest_page(kvm, gfn, data, offset, seg);
+ */
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len)
 {
@@ -2366,6 +2517,30 @@ int kvm_vcpu_read_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn, void *data,
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_read_guest_page);
 
+/*
+ * called by:
+ *   - arch/arm64/include/asm/kvm_mmu.h|257| <<kvm_read_guest_lock>> int ret = kvm_read_guest(kvm, gpa, data, len);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|165| <<kvmppc_mmu_walk_radix_tree>> ret = kvm_read_guest(kvm, addr, &rpte, sizeof(rpte));
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|243| <<kvmppc_mmu_radix_translate_table>> ret = kvm_read_guest(kvm, ptbl, &entry, sizeof(entry));
+ *   - arch/powerpc/kvm/book3s_hv_nested.c|558| <<kvmhv_update_ptbl_cache>> ret = kvm_read_guest(kvm, ptbl_addr,
+ *   - arch/powerpc/kvm/book3s_rtas.c|233| <<kvmppc_rtas_hcall>> rc = kvm_read_guest(vcpu->kvm, args_phys, &args, sizeof(args));
+ *   - arch/powerpc/kvm/powerpc.c|407| <<kvmppc_ld>> rc = kvm_read_guest(vcpu->kvm, pte.raddr, ptr, size);
+ *   - arch/s390/kvm/gaccess.c|590| <<deref_table>> return kvm_read_guest(kvm, gpa, val, sizeof(*val));
+ *   - arch/s390/kvm/gaccess.c|863| <<access_guest>> rc = kvm_read_guest(vcpu->kvm, gpa, data, _len);
+ *   - arch/s390/kvm/gaccess.h|152| <<read_guest_lc>> return kvm_read_guest(vcpu->kvm, gpa, data, len);
+ *   - arch/s390/kvm/gaccess.h|308| <<read_guest_abs>> return kvm_read_guest(vcpu->kvm, gpa, data, len);
+ *   - arch/x86/kvm/hyperv.c|1090| <<kvm_hv_setup_tsc_page>> if (unlikely(kvm_read_guest(kvm, gfn_to_gpa(gfn),
+ *   - arch/x86/kvm/hyperv.c|1509| <<kvm_hv_flush_tlb>> if (unlikely(kvm_read_guest(kvm, ingpa, &flush, sizeof(flush))))
+ *   - arch/x86/kvm/hyperv.c|1528| <<kvm_hv_flush_tlb>> if (unlikely(kvm_read_guest(kvm, ingpa, &flush_ex,
+ *   - arch/x86/kvm/hyperv.c|1549| <<kvm_hv_flush_tlb>> kvm_read_guest(kvm,
+ *   - arch/x86/kvm/hyperv.c|1612| <<kvm_hv_send_ipi>> if (unlikely(kvm_read_guest(kvm, ingpa, &send_ipi,
+ *   - arch/x86/kvm/hyperv.c|1629| <<kvm_hv_send_ipi>> if (unlikely(kvm_read_guest(kvm, ingpa, &send_ipi_ex,
+ *   - arch/x86/kvm/hyperv.c|1648| <<kvm_hv_send_ipi>> kvm_read_guest(kvm,
+ *   - arch/x86/kvm/svm/sev.c|1710| <<setup_vmgexit_scratch>> if (kvm_read_guest(svm->vcpu.kvm, scratch_gpa_beg, scratch_va, len)) {
+ *   - arch/x86/kvm/vmx/nested.c|5046| <<handle_vmon>> if (kvm_read_guest(vcpu->kvm, vmptr, &revision, sizeof(revision)) ||
+ *   - arch/x86/kvm/x86.c|1950| <<kvm_write_wall_clock>> r = kvm_read_guest(kvm, wall_clock, &version, sizeof(version));
+ *   - virt/kvm/kvm_main.c|2692| <<kvm_read_guest_offset_cached>> return kvm_read_guest(kvm, gpa, data, len);
+ */
 int kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len)
 {
 	gfn_t gfn = gpa >> PAGE_SHIFT;
@@ -2670,6 +2845,22 @@ void mark_page_dirty_in_slot(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(mark_page_dirty_in_slot);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|885| <<user_mem_abort>> mark_page_dirty(kvm, gfn);
+ *   - arch/mips/kvm/mmu.c|621| <<_kvm_mips_map_page_fast>> mark_page_dirty(kvm, gfn);
+ *   - arch/mips/kvm/mmu.c|735| <<kvm_mips_map_page>> mark_page_dirty(kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_32_mmu_host.c|200| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, orig_pte->raddr >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_64_mmu_host.c|127| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_hv.c|836| <<kvmppc_copy_guest>> mark_page_dirty(kvm, to >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_xive_native.c|903| <<kvmppc_xive_native_vcpu_eq_sync>> mark_page_dirty(vcpu->kvm, gpa_to_gfn(q->guest_qaddr));
+ *   - arch/s390/kvm/interrupt.c|2800| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->ind_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/interrupt.c|2806| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->summary_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/kvm-s390.c|614| <<kvm_arch_sync_dirty_log>> mark_page_dirty(kvm, cur_gfn + i);
+ *   - arch/s390/kvm/vsie.c|660| <<unpin_guest_page>> mark_page_dirty(kvm, gpa_to_gfn(gpa));
+ *   - arch/x86/kvm/hyperv.c|1162| <<kvm_hv_set_msr_pw>> mark_page_dirty(kvm, gfn);
+ *   - include/linux/kvm_host.h|809| <<__kvm_put_guest>> mark_page_dirty(kvm, gfn); \
+ */
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn)
 {
 	struct kvm_memory_slot *memslot;
@@ -2780,6 +2971,11 @@ update_halt_poll_stats(struct kvm_vcpu *vcpu, u64 poll_ns, bool waited)
 /*
  * The vCPU has executed a HLT instruction with in-kernel mode enabled.
  */
+/*
+ * x86在以下调用:
+ *   - arch/x86/kvm/x86.c|9144| <<vcpu_block>> kvm_vcpu_block(vcpu);
+ *   - arch/x86/kvm/x86.c|9376| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_block(vcpu);
+ */
 void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	ktime_t start, cur, poll_end;
@@ -2870,6 +3066,26 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_wake_up);
 /*
  * Kick a sleeping VCPU, or a guest VCPU in guest mode, into host kernel mode.
  */
+/*
+ * x86在以下调用kvm_vcpu_kick():
+ *   - arch/x86/kvm/hyperv.c|543| <<stimer_mark_pending>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/i8259.c|63| <<pic_unlock>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1096| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1104| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1110| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1116| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1125| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1136| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1652| <<apic_timer_expired>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|2138| <<vmx_preemption_timer_fn>> kvm_vcpu_kick(&vmx->vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|220| <<pi_wakeup_handler>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|3983| <<vmx_deliver_nested_posted_interrupt>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4016| <<vmx_deliver_posted_interrupt>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5764| <<kvm_flush_pml_buffers>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/x86.c|2768| <<kvmclock_update_fn>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/x86.c|10706| <<kvm_arch_memslots_updated>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/x86.c|11168| <<kvm_arch_async_page_present_queued>> kvm_vcpu_kick(vcpu);
+ */
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
 {
 	int me;
@@ -2969,6 +3185,14 @@ static bool vcpu_dy_runnable(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|94| <<kvm_handle_wfx>> kvm_vcpu_on_spin(vcpu, vcpu_mode_priv(vcpu));
+ *   - arch/s390/kvm/diag.c|149| <<__diag_time_slice_end>> kvm_vcpu_on_spin(vcpu, true);
+ *   - arch/x86/kvm/hyperv.c|1783| <<kvm_hv_hypercall>> kvm_vcpu_on_spin(vcpu, true);
+ *   - arch/x86/kvm/svm/svm.c|2971| <<pause_interception>> kvm_vcpu_on_spin(vcpu, in_kernel);
+ *   - arch/x86/kvm/vmx/vmx.c|5491| <<handle_pause>> kvm_vcpu_on_spin(vcpu, true);
+ */
 void kvm_vcpu_on_spin(struct kvm_vcpu *me, bool yield_to_kernel_mode)
 {
 	struct kvm *kvm = me->kvm;
@@ -4819,6 +5043,20 @@ struct kvm_vcpu *preempt_notifier_to_vcpu(struct preempt_notifier *pn)
 	return container_of(pn, struct kvm_vcpu, preempt_notifier);
 }
 
+/*
+ * kvm_sched_in
+ * finish_task_switch
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static void kvm_sched_in(struct preempt_notifier *pn, int cpu)
 {
 	struct kvm_vcpu *vcpu = preempt_notifier_to_vcpu(pn);
@@ -4831,6 +5069,19 @@ static void kvm_sched_in(struct preempt_notifier *pn, int cpu)
 	kvm_arch_vcpu_load(vcpu, cpu);
 }
 
+/*
+ * kvm_sched_out
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static void kvm_sched_out(struct preempt_notifier *pn,
 			  struct task_struct *next)
 {
@@ -4885,6 +5136,17 @@ static void check_processor_compat(void *data)
 	*c->ret = kvm_arch_check_processor_compat(c->opaque);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1981| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/mips/kvm/mips.c|1678| <<kvm_mips_init>> ret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/book3s.c|1039| <<kvmppc_book3s_init>> r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500.c|533| <<kvmppc_e500_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500mc.c|403| <<kvmppc_e500mc_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/s390/kvm/kvm-s390.c|5054| <<kvm_s390_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/x86/kvm/svm/svm.c|4577| <<svm_init>> return kvm_init(&svm_init_ops, sizeof(struct vcpu_svm),
+ *   - arch/x86/kvm/vmx/vmx.c|8028| <<vmx_init>> r = kvm_init(&vmx_init_ops, sizeof(struct vcpu_vmx),
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
@@ -5061,6 +5323,10 @@ static int kvm_vm_worker_thread(void *context)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6214| <<kvm_mmu_post_init_vm>> err = kvm_vm_create_worker_thread(kvm, kvm_nx_lpage_recovery_worker, 0,
+ */
 int kvm_vm_create_worker_thread(struct kvm *kvm, kvm_vm_thread_fn_t thread_fn,
 				uintptr_t data, const char *name,
 				struct task_struct **thread_ptr)
-- 
2.17.1

