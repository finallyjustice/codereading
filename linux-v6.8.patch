From 35d7cfbdd6e78d7dd3cd413dcd41ec8cf32efe5a Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 26 May 2024 15:43:39 -0700
Subject: [PATCH 1/1] linux-v6.8

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/arch_gicv3.h           |  55 +
 arch/arm64/include/asm/kvm_emulate.h          |  13 +
 arch/arm64/include/asm/kvm_host.h             |   9 +
 arch/arm64/kvm/arch_timer.c                   |  13 +
 arch/arm64/kvm/arm.c                          |   4 +
 arch/arm64/kvm/fpsimd.c                       |   4 +
 arch/arm64/kvm/hyp/nvhe/debug-sr.c            |   4 +
 arch/arm64/kvm/hyp/nvhe/hyp-main.c            |   4 +
 arch/arm64/kvm/hyp/nvhe/switch.c              |   7 +
 arch/arm64/kvm/hyp/pgtable.c                  |   6 +
 arch/arm64/kvm/hyp/vgic-v3-sr.c               |  13 +
 arch/arm64/kvm/hyp/vhe/switch.c               |   4 +
 arch/arm64/kvm/pmu-emul.c                     |  11 +
 arch/arm64/kvm/vgic/vgic-irqfd.c              |   5 +
 arch/arm64/kvm/vgic/vgic-its.c                |   9 +
 arch/arm64/kvm/vgic/vgic-kvm-device.c         |   5 +
 arch/arm64/kvm/vgic/vgic-mmio-v3.c            |  10 +
 arch/arm64/kvm/vgic/vgic-mmio.c               |  11 +
 arch/arm64/kvm/vgic/vgic.c                    |  48 +
 arch/x86/events/core.c                        |  44 +
 arch/x86/include/asm/hw_irq.h                 |  20 +
 arch/x86/include/asm/kvm_host.h               | 327 ++++++
 arch/x86/include/uapi/asm/mtrr.h              |  13 +
 arch/x86/kernel/apic/msi.c                    |   3 +
 arch/x86/kernel/apic/vector.c                 | 893 ++++++++++++++++
 arch/x86/kernel/irq.c                         |  45 +
 arch/x86/kernel/irqinit.c                     |  21 +
 arch/x86/kernel/kvmclock.c                    |  26 +
 arch/x86/kernel/paravirt.c                    |   7 +
 arch/x86/kernel/pvclock.c                     |  26 +
 arch/x86/kernel/smp.c                         |  40 +
 arch/x86/kernel/smpboot.c                     |  26 +
 arch/x86/kvm/cpuid.c                          |  13 +
 arch/x86/kvm/emulate.c                        |   6 +
 arch/x86/kvm/mmu.h                            | 220 ++++
 arch/x86/kvm/mmu/mmu.c                        | 987 ++++++++++++++++++
 arch/x86/kvm/mmu/mmu_internal.h               |  88 ++
 arch/x86/kvm/mmu/page_track.c                 |   4 +
 arch/x86/kvm/mmu/page_track.h                 |   5 +
 arch/x86/kvm/mmu/paging_tmpl.h                | 379 +++++++
 arch/x86/kvm/mmu/spte.c                       | 876 ++++++++++++++++
 arch/x86/kvm/mmu/spte.h                       | 367 +++++++
 arch/x86/kvm/mmu/tdp_iter.c                   | 106 ++
 arch/x86/kvm/mmu/tdp_iter.h                   | 209 ++++
 arch/x86/kvm/mmu/tdp_mmu.c                    | 211 ++++
 arch/x86/kvm/mmu/tdp_mmu.h                    |  13 +
 arch/x86/kvm/mtrr.c                           | 615 +++++++++++
 arch/x86/kvm/pmu.c                            |  10 +
 arch/x86/kvm/pmu.h                            |  22 +
 arch/x86/kvm/svm/svm.c                        |  10 +
 arch/x86/kvm/vmx/nested.c                     |  49 +
 arch/x86/kvm/vmx/vmx.c                        |  60 ++
 arch/x86/kvm/vmx/vmx.h                        |  13 +
 arch/x86/kvm/x86.c                            | 714 +++++++++++++
 arch/x86/kvm/x86.h                            |  59 ++
 arch/x86/kvm/xen.c                            |   4 +
 drivers/clocksource/arm_arch_timer.c          |   8 +
 drivers/iommu/iommu.c                         |   4 +
 drivers/irqchip/irq-gic-v3.c                  |   4 +
 drivers/net/virtio_net.c                      |  47 +
 drivers/pci/msi/api.c                         |  32 +
 drivers/perf/arm_pmu.c                        |   8 +
 drivers/scsi/scsi_lib.c                       |   4 +
 drivers/scsi/virtio_scsi.c                    |  16 +
 include/kvm/arm_vgic.h                        |  38 +
 include/linux/interrupt.h                     |  37 +
 include/linux/irq.h                           | 120 +++
 include/linux/irqchip/arm-gic-v4.h            |  15 +
 include/linux/irqdesc.h                       |  26 +
 include/linux/kvm_host.h                      |  50 +
 include/linux/perf_event.h                    |   9 +
 include/linux/virtio_config.h                 |   9 +
 include/uapi/linux/perf_event.h               |   9 +
 kernel/cpu.c                                  |  20 +
 kernel/events/core.c                          | 172 +++
 kernel/irq/chip.c                             |  40 +
 kernel/irq/cpuhotplug.c                       | 148 +++
 kernel/irq/internals.h                        |  43 +
 kernel/irq/manage.c                           |  83 ++
 kernel/irq/matrix.c                           | 426 ++++++++
 kernel/irq/migration.c                        |  52 +
 kernel/irq/proc.c                             |  15 +
 kernel/stop_machine.c                         |  33 +
 kernel/time/hrtimer.c                         |   5 +
 net/core/dev.c                                |   7 +
 net/ethtool/ioctl.c                           |   8 +
 tools/include/linux/bitmap.h                  |   9 +
 tools/include/uapi/linux/perf_event.h         |   9 +
 tools/lib/perf/cpumap.c                       |  40 +
 tools/lib/perf/evlist.c                       |  68 ++
 tools/lib/perf/evsel.c                        | 137 +++
 tools/lib/perf/include/internal/xyarray.h     |   4 +
 tools/lib/perf/mmap.c                         |   4 +
 tools/perf/builtin-stat.c                     |  45 +
 tools/perf/util/counts.c                      |   5 +
 tools/perf/util/counts.h                      |  33 +
 tools/perf/util/evlist.c                      |  22 +
 tools/perf/util/evlist.h                      |  10 +
 tools/perf/util/evsel.c                       |  23 +
 tools/perf/util/pmu.c                         |  32 +
 tools/perf/util/stat-display.c                |  26 +
 tools/perf/util/stat-shadow.c                 |  22 +
 tools/perf/util/stat.c                        |  18 +
 tools/perf/util/time-utils.h                  |   7 +
 tools/testing/selftests/kvm/dirty_log_test.c  | 217 ++++
 .../selftests/kvm/include/kvm_util_base.h     |  22 +
 .../selftests/kvm/include/x86_64/processor.h  |   5 +
 tools/testing/selftests/kvm/lib/guest_modes.c |  11 +
 tools/testing/selftests/kvm/lib/kvm_util.c    |   4 +
 .../kvm/x86_64/private_mem_conversions_test.c | 108 ++
 virt/kvm/dirty_ring.c                         |   9 +
 virt/kvm/irqchip.c                            |   8 +
 virt/kvm/kvm_main.c                           | 194 ++++
 113 files changed, 9346 insertions(+)

diff --git a/arch/arm64/include/asm/arch_gicv3.h b/arch/arm64/include/asm/arch_gicv3.h
index 5f1726116..43b60c128 100644
--- a/arch/arm64/include/asm/arch_gicv3.h
+++ b/arch/arm64/include/asm/arch_gicv3.h
@@ -16,6 +16,61 @@
 #include <asm/barrier.h>
 #include <asm/cacheflush.h>
 
+/*
+ * 在以下调用write_gicreg():
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|65| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR0_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|68| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR1_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|71| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR2_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|74| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR3_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|77| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR4_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|80| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR5_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|83| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR6_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|86| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR7_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|89| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR8_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|92| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR9_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|95| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR10_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|98| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR11_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|101| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR12_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|104| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR13_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|107| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR14_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|110| <<__gic_v3_set_lr>> write_gicreg(val, ICH_LR15_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|119| <<__vgic_v3_write_ap0rn>> write_gicreg(val, ICH_AP0R0_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|122| <<__vgic_v3_write_ap0rn>> write_gicreg(val, ICH_AP0R1_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|125| <<__vgic_v3_write_ap0rn>> write_gicreg(val, ICH_AP0R2_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|128| <<__vgic_v3_write_ap0rn>> write_gicreg(val, ICH_AP0R3_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|137| <<__vgic_v3_write_ap1rn>> write_gicreg(val, ICH_AP1R0_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|140| <<__vgic_v3_write_ap1rn>> write_gicreg(val, ICH_AP1R1_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|143| <<__vgic_v3_write_ap1rn>> write_gicreg(val, ICH_AP1R2_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|146| <<__vgic_v3_write_ap1rn>> write_gicreg(val, ICH_AP1R3_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|221| <<__vgic_v3_save_state>> write_gicreg(cpu_if->vgic_hcr & ~ICH_HCR_EN, ICH_HCR_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|240| <<__vgic_v3_restore_state>> write_gicreg(cpu_if->vgic_hcr, ICH_HCR_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|273| <<__vgic_v3_activate_traps>> write_gicreg(0, ICC_SRE_EL1);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|275| <<__vgic_v3_activate_traps>> write_gicreg(cpu_if->vgic_vmcr, ICH_VMCR_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|294| <<__vgic_v3_activate_traps>> write_gicreg(read_gicreg(ICC_SRE_EL2) & ~ICC_SRE_EL2_ENABLE,
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|304| <<__vgic_v3_activate_traps>> write_gicreg(cpu_if->vgic_hcr, ICH_HCR_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|316| <<__vgic_v3_deactivate_traps>> write_gicreg(val | ICC_SRE_EL2_ENABLE, ICC_SRE_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|321| <<__vgic_v3_deactivate_traps>> write_gicreg(1, ICC_SRE_EL1);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|330| <<__vgic_v3_deactivate_traps>> write_gicreg(0, ICH_HCR_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|439| <<__vgic_v3_get_gic_config>> write_gicreg(0, ICC_SRE_EL1);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|444| <<__vgic_v3_get_gic_config>> write_gicreg(sre, ICC_SRE_EL1);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|465| <<__vgic_v3_write_vmcr>> write_gicreg(vmcr, ICH_VMCR_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|727| <<__vgic_v3_bump_eoicount>> write_gicreg(hcr, ICH_HCR_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|968| <<__vgic_v3_write_pmr>> write_gicreg(vmcr, ICH_VMCR_EL2);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|1013| <<__vgic_v3_write_ctlr>> write_gicreg(vmcr, ICH_VMCR_EL2);
+ *   - drivers/irqchip/irq-gic-v3.c|629| <<gic_eoi_irq>> write_gicreg(gic_irq(d), ICC_EOIR1_EL1);
+ *   - drivers/irqchip/irq-gic-v3.c|730| <<gic_deactivate_unhandled>> write_gicreg(irqnr, ICC_EOIR1_EL1);
+ *   - drivers/irqchip/irq-gic-v3.c|757| <<gic_complete_ack>> write_gicreg(irqnr, ICC_EOIR1_EL1);
+ *   - drivers/irqchip/irq-gic-v3.c|1163| <<gic_cpu_sys_reg_init>> write_gicreg(DEFAULT_PMR_VALUE, ICC_PMR_EL1);
+ *   - drivers/irqchip/irq-gic-v3.c|1201| <<gic_cpu_sys_reg_init>> write_gicreg(0, ICC_AP0R3_EL1);
+ *   - drivers/irqchip/irq-gic-v3.c|1202| <<gic_cpu_sys_reg_init>> write_gicreg(0, ICC_AP0R2_EL1);
+ *   - drivers/irqchip/irq-gic-v3.c|1205| <<gic_cpu_sys_reg_init>> write_gicreg(0, ICC_AP0R1_EL1);
+ *   - drivers/irqchip/irq-gic-v3.c|1209| <<gic_cpu_sys_reg_init>> write_gicreg(0, ICC_AP0R0_EL1);
+ *   - drivers/irqchip/irq-gic-v3.c|1218| <<gic_cpu_sys_reg_init>> write_gicreg(0, ICC_AP1R3_EL1);
+ *   - drivers/irqchip/irq-gic-v3.c|1219| <<gic_cpu_sys_reg_init>> write_gicreg(0, ICC_AP1R2_EL1);
+ *   - drivers/irqchip/irq-gic-v3.c|1222| <<gic_cpu_sys_reg_init>> write_gicreg(0, ICC_AP1R1_EL1);
+ *   - drivers/irqchip/irq-gic-v3.c|1226| <<gic_cpu_sys_reg_init>> write_gicreg(0, ICC_AP1R0_EL1);
+ */
+
 #define read_gicreg(r)			read_sysreg_s(SYS_ ## r)
 #define write_gicreg(v, r)		write_sysreg_s(v, SYS_ ## r)
 
diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h
index b804fe832..08fca22f0 100644
--- a/arch/arm64/include/asm/kvm_emulate.h
+++ b/arch/arm64/include/asm/kvm_emulate.h
@@ -109,9 +109,22 @@ static inline unsigned long *vcpu_hcr(struct kvm_vcpu *vcpu)
 	return (unsigned long *)&vcpu->arch.hcr_el2;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|460| <<kvm_arch_vcpu_load>> vcpu_clear_wfx_traps(vcpu);
+ */
 static inline void vcpu_clear_wfx_traps(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hcr_el2 &= ~HCR_TWE;
+	/*
+	 * 在以下使用its_vpe->vlpi_count:
+	 *   - arch/arm64/include/asm/kvm_emulate.h|119| <<vcpu_clear_wfx_traps>> if (atomic_read(&vcpu->arch.vgic_cpu.vgic_v3.its_vpe.vlpi_count) ||
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|205| <<kvm_vgic_vcpu_init>> atomic_set(&vgic_cpu->vgic_v3.its_vpe.vlpi_count, 0);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|371| <<update_affinity>> atomic_dec(&map.vpe->vlpi_count);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|373| <<update_affinity>> atomic_inc(&map.vpe->vlpi_count);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|465| <<kvm_vgic_v4_set_forwarding>> atomic_inc(&map.vpe->vlpi_count);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|517| <<kvm_vgic_v4_unset_forwarding>> atomic_dec(&irq->target_vcpu->arch.vgic_cpu.vgic_v3.its_vpe.vlpi_count);
+	 */
 	if (atomic_read(&vcpu->arch.vgic_cpu.vgic_v3.its_vpe.vlpi_count) ||
 	    vcpu->kvm->arch.vgic.nassgireq)
 		vcpu->arch.hcr_el2 &= ~HCR_TWI;
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 21c57b812..8be835820 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -603,6 +603,15 @@ struct kvm_vcpu_arch {
 	struct user_fpsimd_state *host_fpsimd_state;	/* hyp VA */
 	struct task_struct *parent_task;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->host_debug_state:
+	 *   - arch/arm64/kvm/hyp/include/hyp/debug-sr.h|140| <<__debug_switch_to_guest_common>> host_dbg = &vcpu->arch.host_debug_state.regs;
+	 *   - arch/arm64/kvm/hyp/include/hyp/debug-sr.h|159| <<__debug_switch_to_host_common>> host_dbg = &vcpu->arch.host_debug_state.regs;
+	 *   - arch/arm64/kvm/hyp/nvhe/debug-sr.c|86| <<__debug_save_host_buffers_nvhe>> __debug_save_spe(&vcpu->arch.host_debug_state.pmscr_el1);
+	 *   - arch/arm64/kvm/hyp/nvhe/debug-sr.c|89| <<__debug_save_host_buffers_nvhe>> __debug_save_trace(&vcpu->arch.host_debug_state.trfcr_el1);
+	 *   - arch/arm64/kvm/hyp/nvhe/debug-sr.c|100| <<__debug_restore_host_buffers_nvhe>> __debug_restore_spe(vcpu->arch.host_debug_state.pmscr_el1);
+	 *   - arch/arm64/kvm/hyp/nvhe/debug-sr.c|102| <<__debug_restore_host_buffers_nvhe>> __debug_restore_trace(vcpu->arch.host_debug_state.trfcr_el1);
+	 */
 	struct {
 		/* {Break,watch}point registers */
 		struct kvm_guest_debug_arch regs;
diff --git a/arch/arm64/kvm/arch_timer.c b/arch/arm64/kvm/arch_timer.c
index 9dec8c419..0f0c93e12 100644
--- a/arch/arm64/kvm/arch_timer.c
+++ b/arch/arm64/kvm/arch_timer.c
@@ -23,6 +23,15 @@
 
 #include "trace.h"
 
+/*
+ * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+ * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+ * 然后再次进入guest时,就有中断了.
+ *
+ * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+ * timer到期的时候(还在host mode)唤醒guest并inject irq.
+ * KVM注册了一个hrtimer来处理这种情况.
+ */
 static struct timecounter *timecounter;
 static unsigned int host_vtimer_irq;
 static unsigned int host_ptimer_irq;
@@ -761,6 +770,10 @@ static void kvm_timer_vcpu_load_nested_switch(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|857| <<kvm_timer_vcpu_load>> timer_set_traps(vcpu, &map);
+ */
 static void timer_set_traps(struct kvm_vcpu *vcpu, struct timer_map *map)
 {
 	bool tpt, tpc;
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index a25265aca..55a5570b3 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -2001,6 +2001,10 @@ static void cpu_hyp_uninit(void *discard)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5571| <<__hardware_enable_nolock>> if (kvm_arch_hardware_enable()) {
+ */
 int kvm_arch_hardware_enable(void)
 {
 	/*
diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
index 8c1d0d485..39beb3213 100644
--- a/arch/arm64/kvm/fpsimd.c
+++ b/arch/arm64/kvm/fpsimd.c
@@ -36,6 +36,10 @@ void kvm_vcpu_unshare_task_fp(struct kvm_vcpu *vcpu)
  * such that on entering hyp the relevant parts of current are already
  * mapped.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|650| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_arch_vcpu_run_map_fp(vcpu);
+ */
 int kvm_arch_vcpu_run_map_fp(struct kvm_vcpu *vcpu)
 {
 	int ret;
diff --git a/arch/arm64/kvm/hyp/nvhe/debug-sr.c b/arch/arm64/kvm/hyp/nvhe/debug-sr.c
index 4558c02eb..3ffc40d9f 100644
--- a/arch/arm64/kvm/hyp/nvhe/debug-sr.c
+++ b/arch/arm64/kvm/hyp/nvhe/debug-sr.c
@@ -94,6 +94,10 @@ void __debug_switch_to_guest(struct kvm_vcpu *vcpu)
 	__debug_switch_to_guest_common(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|348| <<__kvm_vcpu_run>> __debug_restore_host_buffers_nvhe(vcpu);
+ */
 void __debug_restore_host_buffers_nvhe(struct kvm_vcpu *vcpu)
 {
 	if (vcpu_get_flag(vcpu, DEBUG_STATE_SAVE_SPE))
diff --git a/arch/arm64/kvm/hyp/nvhe/hyp-main.c b/arch/arm64/kvm/hyp/nvhe/hyp-main.c
index 2385fd03e..8a2735566 100644
--- a/arch/arm64/kvm/hyp/nvhe/hyp-main.c
+++ b/arch/arm64/kvm/hyp/nvhe/hyp-main.c
@@ -23,6 +23,10 @@ DEFINE_PER_CPU(struct kvm_nvhe_init_params, kvm_init_params);
 
 void __kvm_hyp_host_forward_smc(struct kvm_cpu_context *host_ctxt);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|93| <<handle___kvm_vcpu_run>> flush_hyp_vcpu(hyp_vcpu);
+ */
 static void flush_hyp_vcpu(struct pkvm_hyp_vcpu *hyp_vcpu)
 {
 	struct kvm_vcpu *host_vcpu = hyp_vcpu->host_vcpu;
diff --git a/arch/arm64/kvm/hyp/nvhe/switch.c b/arch/arm64/kvm/hyp/nvhe/switch.c
index c50f8459e..f9f49f1c3 100644
--- a/arch/arm64/kvm/hyp/nvhe/switch.c
+++ b/arch/arm64/kvm/hyp/nvhe/switch.c
@@ -244,6 +244,13 @@ static void early_exit_filter(struct kvm_vcpu *vcpu, u64 *exit_code)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|339| <<global>> HANDLE_FUNC(__kvm_vcpu_run),
+ *   - arch/arm64/kvm/arm.c|949| <<kvm_arm_vcpu_enter_exit>> ret = kvm_call_hyp_ret(__kvm_vcpu_run, vcpu);
+ *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|99| <<handle___kvm_vcpu_run>> ret = __kvm_vcpu_run(&hyp_vcpu->vcpu);
+ *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|105| <<handle___kvm_vcpu_run>> ret = __kvm_vcpu_run(host_vcpu);
+ */
 /* Switch to the guest for legacy non-VHE systems */
 int __kvm_vcpu_run(struct kvm_vcpu *vcpu)
 {
diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index ab9d05fcf..2bfdd6443 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
@@ -1215,6 +1215,12 @@ static int stage2_attr_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1249| <<kvm_pgtable_stage2_wrprotect>> return stage2_update_leaf_attrs(pgt, addr, size, 0, KVM_PTE_LEAF_ATTR_LO_S2_S2AP_W, NULL, NULL, 0);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1259| <<kvm_pgtable_stage2_mkyoung>> ret = stage2_update_leaf_attrs(pgt, addr, 1, KVM_PTE_LEAF_ATTR_LO_S2_AF, 0, &pte, NULL, KVM_PGTABLE_WALK_HANDLE_FAULT | KVM_PGTABLE_WALK_SHARED);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1338| <<kvm_pgtable_stage2_relax_perms>> ret = stage2_update_leaf_attrs(pgt, addr, 1, set, clr, NULL, &level, KVM_PGTABLE_WALK_HANDLE_FAULT | KVM_PGTABLE_WALK_SHARED);
+ */
 static int stage2_update_leaf_attrs(struct kvm_pgtable *pgt, u64 addr,
 				    u64 size, kvm_pte_t attr_set,
 				    kvm_pte_t attr_clr, kvm_pte_t *orig_pte,
diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
index 6cb638b18..cfcf0cf55 100644
--- a/arch/arm64/kvm/hyp/vgic-v3-sr.c
+++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
@@ -58,6 +58,14 @@ static u64 __gic_v3_get_lr(unsigned int lr)
 	unreachable();
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|229| <<__vgic_v3_save_state>> __gic_v3_set_lr(0, i);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|243| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|405| <<__vgic_v3_init_lrs>> __gic_v3_set_lr(0, i);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|699| <<__vgic_v3_read_iar>> __gic_v3_set_lr(lr_val, lr);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|718| <<__vgic_v3_clear_active_lr>> __gic_v3_set_lr(lr_val, lr);
+ */
 static void __gic_v3_set_lr(u64 val, int lr)
 {
 	switch (lr & 0xf) {
@@ -231,6 +239,11 @@ void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|129| <<__hyp_vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ *   - arch/arm64/kvm/vgic/vgic.c|901| <<vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ */
 void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 used_lrs = cpu_if->used_lrs;
diff --git a/arch/arm64/kvm/hyp/vhe/switch.c b/arch/arm64/kvm/hyp/vhe/switch.c
index 1581df6ae..7e313c1f1 100644
--- a/arch/arm64/kvm/hyp/vhe/switch.c
+++ b/arch/arm64/kvm/hyp/vhe/switch.c
@@ -267,6 +267,10 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 }
 NOKPROBE_SYMBOL(__kvm_vcpu_run_vhe);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|949| <<kvm_arm_vcpu_enter_exit>> ret = kvm_call_hyp_ret(__kvm_vcpu_run, vcpu);
+ */
 int __kvm_vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int ret;
diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
index 3d9467ff7..c97ca6dd7 100644
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@ -282,6 +282,11 @@ u64 kvm_pmu_valid_counter_mask(struct kvm_vcpu *vcpu)
  *
  * Call perf_event_enable to start counting the perf event
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|566| <<kvm_pmu_handle_pmcr>> kvm_pmu_enable_counter_mask(vcpu, __vcpu_sys_reg(vcpu, PMCNTENSET_EL0));
+ *   - arch/arm64/kvm/sys_regs.c|1167| <<access_pmcnten>> kvm_pmu_enable_counter_mask(vcpu, val);
+ */
 void kvm_pmu_enable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 {
 	int i;
@@ -593,6 +598,12 @@ static bool kvm_pmu_counter_is_enabled(struct kvm_pmc *pmc)
  * kvm_pmu_create_perf_event - create a perf event for a counter
  * @pmc: Counter context
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|173| <<kvm_pmu_set_pmc_value>> kvm_pmu_create_perf_event(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|303| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|690| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(pmc);
+ */
 static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
diff --git a/arch/arm64/kvm/vgic/vgic-irqfd.c b/arch/arm64/kvm/vgic/vgic-irqfd.c
index 8c711deb2..a8936910f 100644
--- a/arch/arm64/kvm/vgic/vgic-irqfd.c
+++ b/arch/arm64/kvm/vgic/vgic-irqfd.c
@@ -66,6 +66,11 @@ int kvm_set_routing_entry(struct kvm *kvm,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|97| <<kvm_set_msi>> kvm_populate_msi(e, &msi);
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|118| <<kvm_arch_set_irq_inatomic>> kvm_populate_msi(e, &msi);
+ */
 static void kvm_populate_msi(struct kvm_kernel_irq_routing_entry *e,
 			     struct kvm_msi *msi)
 {
diff --git a/arch/arm64/kvm/vgic/vgic-its.c b/arch/arm64/kvm/vgic/vgic-its.c
index 28a93074e..ff2571f84 100644
--- a/arch/arm64/kvm/vgic/vgic-its.c
+++ b/arch/arm64/kvm/vgic/vgic-its.c
@@ -762,6 +762,11 @@ static int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|119| <<kvm_arch_set_irq_inatomic>> return vgic_its_inject_cached_translation(kvm, &msi);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|795| <<vgic_its_inject_msi>> if (!vgic_its_inject_cached_translation(kvm, msi))
+ */
 int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct vgic_irq *irq;
@@ -787,6 +792,10 @@ int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
  * We then call vgic_its_trigger_msi() with the decoded data.
  * According to the KVM_SIGNAL_MSI API description returns 1 on success.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|98| <<kvm_set_msi>> return vgic_its_inject_msi(kvm, &msi);
+ */
 int vgic_its_inject_msi(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct vgic_its *its;
diff --git a/arch/arm64/kvm/vgic/vgic-kvm-device.c b/arch/arm64/kvm/vgic/vgic-kvm-device.c
index f48b8dab8..e8f1979ce 100644
--- a/arch/arm64/kvm/vgic/vgic-kvm-device.c
+++ b/arch/arm64/kvm/vgic/vgic-kvm-device.c
@@ -505,6 +505,11 @@ int vgic_v3_parse_attr(struct kvm_device *dev, struct kvm_device_attr *attr,
  * @attr:     kvm device attribute
  * @is_write: true if userspace is writing a register
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|606| <<vgic_v3_set_attr>> return vgic_v3_attr_regs_access(dev, attr, true);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|620| <<vgic_v3_get_attr>> return vgic_v3_attr_regs_access(dev, attr, false);
+ */
 static int vgic_v3_attr_regs_access(struct kvm_device *dev,
 				    struct kvm_device_attr *attr,
 				    bool is_write)
diff --git a/arch/arm64/kvm/vgic/vgic-mmio-v3.c b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
index c15ee1df0..1c56b707a 100644
--- a/arch/arm64/kvm/vgic/vgic-mmio-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
@@ -101,6 +101,12 @@ static unsigned long vgic_mmio_read_v3_misc(struct kvm_vcpu *vcpu,
 	return value;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|596| <<global>> REGISTER_DESC_WITH_LENGTH_UACCESS(GICD_CTLR,
+ *                                                           vgic_mmio_read_v3_misc, vgic_mmio_write_v3_misc, NULL, vgic_mmio_uaccess_write_v3_misc, 16, VGIC_ACCESS_32bit),
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|189| <<vgic_mmio_uaccess_write_v3_misc>> vgic_mmio_write_v3_misc(vcpu, addr, len, val);
+ */
 static void vgic_mmio_write_v3_misc(struct kvm_vcpu *vcpu,
 				    gpa_t addr, unsigned int len,
 				    unsigned long val)
@@ -1092,6 +1098,10 @@ void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg, bool allow_group1)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|557| <<vgic_v3_attr_regs_access>> ret = vgic_v3_dist_uaccess(vcpu, is_write, addr, &val);
+ */
 int vgic_v3_dist_uaccess(struct kvm_vcpu *vcpu, bool is_write,
 			 int offset, u32 *val)
 {
diff --git a/arch/arm64/kvm/vgic/vgic-mmio.c b/arch/arm64/kvm/vgic/vgic-mmio.c
index cf76523a2..05e7ff299 100644
--- a/arch/arm64/kvm/vgic/vgic-mmio.c
+++ b/arch/arm64/kvm/vgic/vgic-mmio.c
@@ -981,6 +981,10 @@ static int vgic_uaccess_read(struct kvm_vcpu *vcpu, struct vgic_io_device *iodev
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|1009| <<vgic_uaccess>> return vgic_uaccess_write(vcpu, dev, offset, val);
+ */
 static int vgic_uaccess_write(struct kvm_vcpu *vcpu, struct vgic_io_device *iodev,
 			      gpa_t addr, const u32 *val)
 {
@@ -999,6 +1003,13 @@ static int vgic_uaccess_write(struct kvm_vcpu *vcpu, struct vgic_io_device *iode
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|548| <<vgic_v2_cpuif_uaccess>> return vgic_uaccess(vcpu, &dev, is_write, offset, val);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|560| <<vgic_v2_dist_uaccess>> return vgic_uaccess(vcpu, &dev, is_write, offset, val);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1109| <<vgic_v3_dist_uaccess>> return vgic_uaccess(vcpu, &dev, is_write, offset, val);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1120| <<vgic_v3_redist_uaccess>> return vgic_uaccess(vcpu, &rd_dev, is_write, offset, val);
+ */
 /*
  * Userland access to VGIC registers.
  */
diff --git a/arch/arm64/kvm/vgic/vgic.c b/arch/arm64/kvm/vgic/vgic.c
index db2a95762..60b578666 100644
--- a/arch/arm64/kvm/vgic/vgic.c
+++ b/arch/arm64/kvm/vgic/vgic.c
@@ -333,6 +333,25 @@ static bool vgic_validate_injection(struct vgic_irq *irq, bool level, void *owne
  * Needs to be entered with the IRQ lock already held, but will return
  * with all locks dropped.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|301| <<update_lpi_config>> vgic_queue_irq_unlock(kvm, irq, flags); 
+ *   - arch/arm64/kvm/vgic/vgic-its.c|476| <<its_sync_lpi_pending_table>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|760| <<vgic_its_trigger_msi>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|778| <<vgic_its_inject_cached_translation>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|157| <<vgic_mmio_write_sgir>> vgic_queue_irq_unlock(source_vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|264| <<vgic_mmio_write_sgipends>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1031| <<vgic_v3_queue_sgi>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|85| <<vgic_mmio_write_group>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|159| <<vgic_mmio_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|200| <<vgic_uaccess_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|348| <<__set_pending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|588| <<vgic_mmio_change_active>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|813| <<vgic_write_irq_line_level_info>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|338| <<vgic_v3_lpi_sync_pending_status>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|480| <<kvm_vgic_v4_set_forwarding>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic.c|473| <<kvm_vgic_inject_irq>> vgic_queue_irq_unlock(kvm, irq, flags);
+ */
 bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 			   unsigned long flags)
 {
@@ -793,6 +812,10 @@ static int compute_ap_list_depth(struct kvm_vcpu *vcpu,
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|927| <<kvm_vgic_flush_hwstate>> vgic_flush_lr_state(vcpu);
+ */
 /* Requires the VCPU's ap_list_lock to be held. */
 static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 {
@@ -811,6 +834,23 @@ static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 
 	count = 0;
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+366          *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<__kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+367          *   - arch/arm64/kvm/vgic/vgic.c|160| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+368          *   - arch/arm64/kvm/vgic/vgic.c|305| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+369          *   - arch/arm64/kvm/vgic/vgic.c|410| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+370          *   - arch/arm64/kvm/vgic/vgic.c|643| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+371          *   - arch/arm64/kvm/vgic/vgic.c|715| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+372          *   - arch/arm64/kvm/vgic/vgic.c|782| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+373          *   - arch/arm64/kvm/vgic/vgic.c|814| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+374          *   - arch/arm64/kvm/vgic/vgic.c|840| <<vgic_flush_lr_state>> if (!list_is_last(&irq->ap_list, &vgic_cpu->ap_list_head))
+375          *   - arch/arm64/kvm/vgic/vgic.c|880| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+376          *   - arch/arm64/kvm/vgic/vgic.c|919| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+377          *   - arch/arm64/kvm/vgic/vgic.c|925| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+378          *   - arch/arm64/kvm/vgic/vgic.c|989| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 */
 	list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
 		raw_spin_lock(&irq->irq_lock);
 
@@ -893,6 +933,10 @@ void kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)
 	vgic_prune_ap_list(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|932| <<kvm_vgic_flush_hwstate>> vgic_restore_state(vcpu);
+ */
 static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 {
 	if (!static_branch_unlikely(&kvm_vgic_global_state.gicv3_cpuif))
@@ -901,6 +945,10 @@ static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 		__vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1022| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_flush_hwstate(vcpu);
+ */
 /* Flush our emulation state into the GIC hardware before entering the guest. */
 void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 {
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 09050641c..a441dbe81 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1131,6 +1131,11 @@ static void del_nr_metric_event(struct cpu_hw_events *cpuc,
 		cpuc->n_metric--;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1198| <<collect_events>> if (collect_event(cpuc, leader, max_count, n))
+ *   - arch/x86/events/core.c|1210| <<collect_events>> if (collect_event(cpuc, event, max_count, n))
+ */
 static int collect_event(struct cpu_hw_events *cpuc, struct perf_event *event,
 			 int max_count, int n)
 {
@@ -1155,6 +1160,12 @@ static int collect_event(struct cpu_hw_events *cpuc, struct perf_event *event,
  * dogrp: true if must collect siblings events (group)
  * returns total number of events and error code
  */
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1447| <<x86_pmu_add>> ret = n = collect_events(cpuc, event, false);
+ *   - arch/x86/events/core.c|2415| <<validate_group>> n = collect_events(fake_cpuc, leader, true);
+ *   - arch/x86/events/core.c|2420| <<validate_group>> n = collect_events(fake_cpuc, event, false);
+ */
 static int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader, bool dogrp)
 {
 	int num_counters = hybrid(cpuc->pmu, num_counters);
@@ -1209,6 +1220,10 @@ static int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader,
 	return n;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1342| <<x86_pmu_enable>> x86_assign_hw_event(event, cpuc, i);
+ */
 static inline void x86_assign_hw_event(struct perf_event *event,
 				struct cpu_hw_events *cpuc, int i)
 {
@@ -1280,6 +1295,15 @@ static inline int match_prev_assignment(struct hw_perf_event *hwc,
 
 static void x86_pmu_start(struct perf_event *event, int flags);
 
+/*
+ * 在以下使用x86_pmu_enable():
+ *   - arch/x86/events/core.c|66| <<global>> DEFINE_STATIC_CALL_NULL(x86_pmu_enable, *x86_pmu.enable);
+ *   - arch/x86/events/core.c|2682| <<global>> .pmu_enable = x86_pmu_enable,
+ *   - arch/x86/events/core.c|1521| <<x86_pmu_start>> static_call(x86_pmu_enable)(event);
+ *   - arch/x86/events/core.c|2010| <<x86_pmu_static_call_update>> static_call_update(x86_pmu_enable, x86_pmu.enable);
+ *
+ * struct pmu pmu.pmu_enable = x86_pmu_enable()
+ */
 static void x86_pmu_enable(struct pmu *pmu)
 {
 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
@@ -1428,6 +1452,26 @@ void x86_pmu_enable_event(struct perf_event *event)
 				       ARCH_PERFMON_EVENTSEL_ENABLE);
 }
 
+/*
+ * 5.15的例子:
+ * x86_pmu_add
+ * event_sched_in.part.0
+ * merge_sched_in
+ * visit_groups_merge.constprop.0.isra.0
+ * ctx_sched_in
+ * perf_event_sched_in
+ * __perf_event_task_sched_in
+ * finish_task_switch.isra.0
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 /*
  * Add a single event to the PMU.
  *
diff --git a/arch/x86/include/asm/hw_irq.h b/arch/x86/include/asm/hw_irq.h
index b02c3cd3c..8c46e7a7d 100644
--- a/arch/x86/include/asm/hw_irq.h
+++ b/arch/x86/include/asm/hw_irq.h
@@ -123,6 +123,26 @@ extern char irq_entries_start[];
 
 extern char spurious_entries_start[];
 
+/*
+ * 在以下使用VECTOR_UNUSED:
+ *   - arch/x86/kernel/irqinit.c|71| <<global>> [0 ... NR_VECTORS - 1] = VECTOR_UNUSED,
+ *   - arch/x86/kernel/apic/vector.c|1261| <<__setup_vector_irq>> return VECTOR_UNUSED;
+ *   - arch/x86/kernel/apic/vector.c|1264| <<__setup_vector_irq>> return VECTOR_UNUSED;
+ *   - arch/x86/kernel/apic/vector.c|1443| <<free_moved_vector>> per_cpu(vector_irq, cpu)[vector] = VECTOR_UNUSED;
+ *   - arch/x86/kernel/irq.c|261| <<DEFINE_IDTENTRY_IRQ(common_interrupt)>> if (desc == VECTOR_UNUSED) {
+ *   - arch/x86/kernel/irq.c|266| <<DEFINE_IDTENTRY_IRQ(common_interrupt)>> __this_cpu_write(vector_irq[vector], VECTOR_UNUSED);
+ *   - arch/x86/kernel/irq.c|387| <<fixup_irqs>> __this_cpu_write(vector_irq[vector], VECTOR_UNUSED);
+ *
+ * 在以下使用VECTOR_SHUTDOWN:
+ *   - arch/x86/kernel/apic/vector.c|762| <<clear_irq_vector>> per_cpu(vector_irq, apicd->cpu)[vector] = VECTOR_SHUTDOWN;
+ *   - arch/x86/kernel/apic/vector.c|771| <<clear_irq_vector>> per_cpu(vector_irq, apicd->prev_cpu)[vector] = VECTOR_SHUTDOWN;
+ *
+ * 在以下使用VECTOR_RETRIGGERED:
+ *   - arch/x86/kernel/apic/msi.c|112| <<msi_set_affinity>> this_cpu_write(vector_irq[cfg->vector], VECTOR_RETRIGGERED);
+ *   - arch/x86/kernel/irq.c|382| <<fixup_irqs>> __this_cpu_write(vector_irq[vector], VECTOR_RETRIGGERED);
+ *   - arch/x86/kernel/irq.c|386| <<fixup_irqs>> if (__this_cpu_read(vector_irq[vector]) != VECTOR_RETRIGGERED)
+ */
+
 #define VECTOR_UNUSED		NULL
 #define VECTOR_SHUTDOWN		((void *)-1L)
 #define VECTOR_RETRIGGERED	((void *)-2L)
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d271ba20a..0dd8d202d 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -82,7 +82,34 @@
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|3147| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3329| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3501| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3518| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|4004| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5114| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5769| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9674| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11009| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|11345| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|12698| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|760| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|775| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|1194| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *
+ * 处理的函数: kvm_guest_time_update(vcpu);
+ */
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
+/*
+ * 在以下使用KVM_REQ_LOAD_MMU_PGD:
+ *   - arch/x86/kvm/mmu/mmu.c|5063| <<kvm_mmu_new_pgd>> kvm_make_request(KVM_REQ_LOAD_MMU_PGD, vcpu);
+ *   - arch/x86/kvm/x86.c|913| <<load_pdptrs>> kvm_make_request(KVM_REQ_LOAD_MMU_PGD, vcpu);
+ *   - arch/x86/kvm/x86.c|10859| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_LOAD_MMU_PGD, vcpu))
+ *
+ * 处理的函数: kvm_mmu_load_pgd()
+ */
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
@@ -93,11 +120,31 @@
 #ifdef CONFIG_KVM_SMM
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
 #endif
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1439| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2393| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2570| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9658| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10868| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|12615| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * 处理函数: kvm_update_masterclock(vcpu->kvm);
+ */
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
 #define KVM_REQ_MCLOCK_INPROGRESS \
 	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2399| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5035| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10870| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * 处理函数: kvm_gen_kvmclock_update(vcpu);
+ */
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
 #define KVM_REQ_APIC_PAGE_RELOAD \
 	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
@@ -145,11 +192,49 @@
 
 /* KVM Hugepage definitions for x86 */
 #define KVM_MAX_HUGEPAGE_LEVEL	PG_LEVEL_1G
+/*
+ * 3 - 1 + 1 = 3
+ */
 #define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)
+/*
+ * KVM_HPAGE_GFN_SHIFT(1) : (((1) - 1) * 9) =  0
+ * KVM_HPAGE_GFN_SHIFT(2) : (((2) - 1) * 9) =  9
+ * KVM_HPAGE_GFN_SHIFT(3) : (((3) - 1) * 9) = 18
+ * KVM_HPAGE_GFN_SHIFT(4) : (((4) - 1) * 9) = 27
+ * KVM_HPAGE_GFN_SHIFT(5) : (((5) - 1) * 9) = 36
+ */
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+/*
+ * KVM_HPAGE_SHIFT(1) : 12 +  0 = 12
+ * KVM_HPAGE_SHIFT(2) : 12 +  9 = 21
+ * KVM_HPAGE_SHIFT(3) : 12 + 18 = 30
+ * KVM_HPAGE_SHIFT(4) : 12 + 27 = 39
+ * KVM_HPAGE_SHIFT(5) : 12 + 36 = 48
+ */
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
+/*
+ * KVM_HPAGE_SIZE(1) : 1 << 12 = 4096 (4K)
+ * KVM_HPAGE_SIZE(2) : 1 << 21 = 2097152 (2M)
+ * KVM_HPAGE_SIZE(3) : 1 << 30 = 1073741824 (1G)
+ * KVM_HPAGE_SIZE(4) : 1 << 39 = 549755813888 (512G)
+ * KVM_HPAGE_SIZE(5) : 1 << 48 = 281474976710656 (262144G=256T)
+ */
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
+/*
+ * KVM_HPAGE_MASK(1) : 4K的mask
+ * KVM_HPAGE_MASK(2) : 2M的mask
+ * KVM_HPAGE_MASK(3) : 1G的mask
+ * KVM_HPAGE_MASK(4) : 512G的mask
+ * KVM_HPAGE_MASK(5) : 262144G(256T)的mask
+ */
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
+/*
+ * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+ * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+ * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+ * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+ * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+ */
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
 #define KVM_MEMSLOT_PAGES_TO_MMU_PAGES_RATIO 50
@@ -332,15 +417,37 @@ struct kvm_kernel_irq_routing_entry;
  * single gfn is a bit less than 2^13.
  */
 union kvm_mmu_page_role {
+	/*
+	 * 在下面修改kvm_mmu_page_role->word整个单位:
+	 *   - arch/x86/kvm/mmu/mmu.c|5890| <<init_kvm_tdp_mmu>> context->root_role.word = root_role.word;
+	 *   - arch/x86/kvm/mmu/mmu.c|5937| <<shadow_mmu_init_context>> context->root_role.word = root_role.word;
+	 *   - arch/x86/kvm/mmu/mmu.c|6063| <<kvm_init_shadow_ept_mmu>> context->root_role.word = new_mode.base.word;
+	 *   - arch/x86/kvm/mmu/mmu.c|6193| <<kvm_mmu_after_set_cpuid>> vcpu->arch.root_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|6194| <<kvm_mmu_after_set_cpuid>> vcpu->arch.guest_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|6195| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|32| <<KVM_MMU_PAGE_PRINTK>> role.word = __entry->role; \
+	 */
 	u32 word;
 	struct {
 		unsigned level:4;
+		/*
+		 * 在以下设置kvm_mmu_page_role->has_4_byte_gpte:
+		 *   - arch/x86/kvm/mmu/mmu.c|5743| <<kvm_calc_cpu_role>> role.base.has_4_byte_gpte = !____is_cr4_pae(regs);
+		 *   - arch/x86/kvm/mmu/mmu.c|5867| <<kvm_calc_tdp_mmu_root_page_role>> role.has_4_byte_gpte = false;
+		 *   - arch/x86/kvm/mmu/mmu.c|6033| <<kvm_calc_shadow_ept_root_page_role>> role.base.has_4_byte_gpte = false;
+		 */
 		unsigned has_4_byte_gpte:1;
 		unsigned quadrant:2;
 		unsigned direct:1;
 		unsigned access:3;
 		unsigned invalid:1;
 		unsigned efer_nx:1;
+		/*
+		 * 在以下使用kvm_mmu_page_role->cr0_wp:
+		 *   - arch/x86/kvm/mmu/mmu.c|5520| <<kvm_calc_cpu_role>> role.base.cr0_wp = ____is_cr0_wp(regs);
+		 *   - arch/x86/kvm/mmu/mmu.c|5555| <<__kvm_mmu_refresh_passthrough_bits>> mmu->cpu_role.base.cr0_wp = cr0_wp;
+		 *   - arch/x86/kvm/mmu/mmu.c|5579| <<kvm_calc_tdp_mmu_root_page_role>> role.cr0_wp = true;
+		 */
 		unsigned cr0_wp:1;
 		unsigned smep_andnot_wp:1;
 		unsigned smap_andnot_wp:1;
@@ -393,6 +500,13 @@ union kvm_mmu_extended_role {
 };
 
 union kvm_cpu_role {
+	/*
+	 * 在以下整个单位修改kvm_cpu_role->as_u64:
+	 *   - arch/x86/kvm/mmu/mmu.c|5889| <<init_kvm_tdp_mmu>> context->cpu_role.as_u64 = cpu_role.as_u64;
+	 *   - arch/x86/kvm/mmu/mmu.c|5936| <<shadow_mmu_init_context>> context->cpu_role.as_u64 = cpu_role.as_u64;
+	 *   - arch/x86/kvm/mmu/mmu.c|6062| <<kvm_init_shadow_ept_mmu>> context->cpu_role.as_u64 = new_mode.as_u64;
+	 *   - arch/x86/kvm/mmu/mmu.c|6115| <<init_kvm_nested_mmu>> g_context->cpu_role.as_u64 = new_mode.as_u64;
+	 */
 	u64 as_u64;
 	struct {
 		union kvm_mmu_page_role base;
@@ -415,7 +529,18 @@ struct kvm_pio_request {
 #define PT64_ROOT_MAX_LEVEL 5
 
 struct rsvd_bits_validate {
+	/*
+	 * 在4个不同的函数reset:
+	 * - __reset_rsvds_bits_mask()
+	 * - __reset_rsvds_bits_mask_ept()
+	 * - reset_shadow_zero_bits_mask()
+	 * - reset_tdp_shadow_zero_bits_mask()
+	 */
 	u64 rsvd_bits_mask[2][PT64_ROOT_MAX_LEVEL];
+	/*
+	 * 除了那些reset的, 主要在下面使用bad_mt_xwr:
+	 *   - arch/x86/kvm/mmu/spte.h|485| <<__is_bad_mt_xwr>> return rsvd_check->bad_mt_xwr & BIT_ULL(pte & 0x3f);
+	 */
 	u64 bad_mt_xwr;
 };
 
@@ -446,12 +571,53 @@ struct kvm_page_fault;
 struct kvm_mmu {
 	unsigned long (*get_guest_pgd)(struct kvm_vcpu *vcpu);
 	u64 (*get_pdptr)(struct kvm_vcpu *vcpu, int index);
+	/*
+	 * 在以下设置kvm_mmu->page_fault:
+	 *   - arch/x86/kvm/mmu/mmu.c|4808| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5424| <<paging64_init_context>> context->page_fault = paging64_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5435| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5543| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5702| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault;
+	 */
 	int (*page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);
+	/*
+	 * 在以下使用kvm_mmu->inject_page_fault:
+	 *   - arch/x86/kvm/mmu/mmu.c|5937| <<init_kvm_tdp_mmu>> context->inject_page_fault = kvm_inject_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|6144| <<init_kvm_softmmu>> context->inject_page_fault = kvm_inject_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|6162| <<init_kvm_nested_mmu>> g_context->inject_page_fault = kvm_inject_page_fault;
+	 *   - arch/x86/kvm/svm/nested.c|98| <<nested_svm_init_mmu_context>> vcpu->arch.mmu->inject_page_fault = nested_svm_inject_npf_exit;
+	 *   - arch/x86/kvm/vmx/nested.c|467| <<nested_ept_init_mmu_context>> vcpu->arch.mmu->inject_page_fault = nested_ept_inject_page_fault;
+	 *   - arch/x86/kvm/x86.c|821| <<kvm_inject_emulated_page_fault>> fault_mmu->inject_page_fault(vcpu, fault);
+	 *   - arch/x86/kvm/x86.c|13819| <<kvm_fixup_and_inject_pf_error>> vcpu->arch.walk_mmu->inject_page_fault(vcpu, &fault);
+	 */
 	void (*inject_page_fault)(struct kvm_vcpu *vcpu,
 				  struct x86_exception *fault);
+	/*
+	 * 在以下设置kvm_mmu->gva_to_gpa:
+	 *   - arch/x86/kvm/mmu/mmu.c|4809| <<nonpaging_init_context>> context->gva_to_gpa = nonpaging_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5425| <<paging64_init_context>> context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5436| <<paging32_init_context>> context->gva_to_gpa = paging32_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5550| <<init_kvm_tdp_mmu>> context->gva_to_gpa = nonpaging_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5552| <<init_kvm_tdp_mmu>> context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5554| <<init_kvm_tdp_mmu>> context->gva_to_gpa = paging32_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5703| <<kvm_init_shadow_ept_mmu>> context->gva_to_gpa = ept_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5764| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = nonpaging_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5766| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5768| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5770| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging32_gva_to_gpa;
+	 */
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			    gpa_t gva_or_gpa, u64 access,
 			    struct x86_exception *exception);
+	/*
+	 * 在以下设置kvm_mmu->sync_spte:
+	 *   - arch/x86/kvm/mmu/mmu.c|4810| <<nonpaging_init_context>> context->sync_spte = NULL;
+	 *   - arch/x86/kvm/mmu/mmu.c|5426| <<paging64_init_context>> context->sync_spte = paging64_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|5437| <<paging32_init_context>> context->sync_spte = paging32_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|5544| <<init_kvm_tdp_mmu>> context->sync_spte = NULL;
+	 *   - arch/x86/kvm/mmu/mmu.c|5704| <<kvm_init_shadow_ept_mmu>> context->sync_spte = ept_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|5753| <<init_kvm_nested_mmu>> g_context->sync_spte = NULL;
+	 */
 	int (*sync_spte)(struct kvm_vcpu *vcpu,
 			 struct kvm_mmu_page *sp, int i);
 	struct kvm_mmu_root_info root;
@@ -473,6 +639,14 @@ struct kvm_mmu {
 	 * Byte index: page fault error code [4:1]
 	 * Bit index: pte permissions in ACC_* format
 	 */
+	/*
+	 * 在以下使用kvm_mmu->permissions[16] (u8)
+	 *   - arch/x86/include/asm/kvm_host.h|556| <<global>> u8 permissions[16];
+	 *   - arch/x86/kvm/mmu.h|246| <<permission_fault>> fault = (mmu->permissions[index] >> pte_access) & 1;
+	 *   - arch/x86/kvm/mmu/mmu.c|5332| <<update_permission_bitmask>> for (byte = 0; byte < ARRAY_SIZE(mmu->permissions); ++byte) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5387| <<update_permission_bitmask>> mmu->permissions[byte] = ff | uf | wf | smepf | smapf;
+	 *   - arch/x86/kvm/mmu/mmu.c|5427| <<update_pkru_bitmask>> for (bit = 0; bit < ARRAY_SIZE(mmu->permissions); ++bit) {
+	 */
 	u8 permissions[16];
 
 	u64 *pae_root;
@@ -748,6 +922,19 @@ struct kvm_vcpu_arch {
 	u32 regs_dirty;
 
 	unsigned long cr0;
+	/*
+	 * 在以下使用kvm_vcpu_arch->cr0_guest_owned_bits:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|154| <<kvm_read_cr0_bits>> if ((tmask & vcpu->arch.cr0_guest_owned_bits) &&
+	 *   - arch/x86/kvm/vmx/nested.c|2622| <<prepare_vmcs02>> vcpu->arch.cr0_guest_owned_bits &= ~vmcs12->cr0_guest_host_mask;
+	 *   - arch/x86/kvm/vmx/nested.c|2623| <<prepare_vmcs02>> vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu->arch.cr0_guest_owned_bits);
+	 *   - arch/x86/kvm/vmx/nested.c|3773| <<vmcs12_guest_cr0>>  (vmcs_readl(GUEST_CR0) & vcpu->arch.cr0_guest_owned_bits) |
+	 *   - arch/x86/kvm/vmx/nested.c|3776| <<vmcs12_guest_cr0>> vcpu->arch.cr0_guest_owned_bits));
+	 *   - arch/x86/kvm/vmx/nested.c|4553| <<load_vmcs12_host_state>> vcpu->arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+	 *   - arch/x86/kvm/vmx/nested.c|4708| <<nested_vmx_restore_host_state>> vcpu->arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+	 *   - arch/x86/kvm/vmx/vmx.c|2485| <<vmx_cache_reg>> guest_owned_bits = vcpu->arch.cr0_guest_owned_bits;
+	 *   - arch/x86/kvm/vmx/vmx.c|4786| <<init_vmcs>> vmx->vcpu.arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+	 *   - arch/x86/kvm/vmx/vmx.c|4787| <<init_vmcs>> vmcs_writel(CR0_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr0_guest_owned_bits);
+	 */
 	unsigned long cr0_guest_owned_bits;
 	unsigned long cr2;
 	unsigned long cr3;
@@ -784,6 +971,14 @@ struct kvm_vcpu_arch {
 	 * the paging mode of the l1 guest. This context is always used to
 	 * handle faults.
 	 */
+	/*
+	 * 在以下设置kvm_vcpu_arch->mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|6447| <<kvm_mmu_create>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|86| <<nested_svm_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|104| <<nested_svm_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|455| <<nested_ept_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|466| <<nested_ept_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 */
 	struct kvm_mmu *mmu;
 
 	/* Non-nested MMU for L1 */
@@ -800,12 +995,30 @@ struct kvm_vcpu_arch {
 	 * walking and not for faulting since we never handle l2 page faults on
 	 * the host.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->nested_mmu:
+	 *   - arch/x86/kvm/mmu.h|376| <<kvm_translate_gpa>> if (mmu != &vcpu->arch.nested_mmu)
+	 *   - arch/x86/kvm/mmu/mmu.c|5739| <<init_kvm_nested_mmu>> struct kvm_mmu *g_context = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|5817| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|5820| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.cpu_role.ext.valid = 0;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|461| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/x86.h|185| <<mmu_is_nested>> return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;
+	 */
 	struct kvm_mmu nested_mmu;
 
 	/*
 	 * Pointer to the mmu context currently used for
 	 * gva_to_gpa translations.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->walk_mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|6448| <<kvm_mmu_create>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|105| <<nested_svm_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|461| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|467| <<nested_ept_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 */
 	struct kvm_mmu *walk_mmu;
 
 	struct kvm_mmu_memory_cache mmu_pte_list_desc_cache;
@@ -908,6 +1121,12 @@ struct kvm_vcpu_arch {
 	u64 this_tsc_generation;
 	bool tsc_catchup;
 	bool tsc_always_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_shift:
+	 *   - arch/x86/kvm/x86.c|2595| <<kvm_set_tsc_khz>>  kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC, &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2623| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec, vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|451| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 */
 	s8 virtual_tsc_shift;
 	u32 virtual_tsc_mult;
 	u32 virtual_tsc_khz;
@@ -944,6 +1163,12 @@ struct kvm_vcpu_arch {
 
 	/* Cache MMIO info */
 	u64 mmio_gva;
+	/*
+	 * 在以下使用mmio_access:
+	 *   - arch/x86/kvm/x86.c|7739| <<vcpu_mmio_gva_to_gpa>> if (vcpu_match_mmio_gva(vcpu, gva) &&
+	 *                                         (!is_paging(vcpu) || !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access))) {
+	 *   - arch/x86/kvm/x86.h|244| <<vcpu_cache_mmio_info>> vcpu->arch.mmio_access = access;
+	 */
 	unsigned mmio_access;
 	gfn_t mmio_gfn;
 	u64 mmio_gen;
@@ -1045,11 +1270,43 @@ struct kvm_vcpu_arch {
 };
 
 struct kvm_lpage_info {
+	/*
+	 * 在以下使用kvm_lpage_info->disallow_lpage:
+	 *   - arch/x86/kvm/mmu/mmu.c|811| <<update_gfn_disallow_lpage_count>> old = linfo->disallow_lpage;
+	 *   - arch/x86/kvm/mmu/mmu.c|812| <<update_gfn_disallow_lpage_count>> linfo->disallow_lpage += count;
+	 *   - arch/x86/kvm/mmu/mmu.c|813| <<update_gfn_disallow_lpage_count>> WARN_ON_ONCE((old ^ linfo->disallow_lpage) & KVM_LPAGE_MIXED_FLAG);
+	 *   - arch/x86/kvm/mmu/mmu.c|3156| <<__kvm_mmu_max_mapping_level>> if (!linfo->disallow_lpage)
+	 *   - arch/x86/kvm/mmu/mmu.c|7372| <<hugepage_test_mixed>> return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/mmu/mmu.c|7378| <<hugepage_clear_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/mmu/mmu.c|7384| <<hugepage_set_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/x86.c|12807| <<kvm_alloc_memslot_metadata>> linfo[0].disallow_lpage = 1;
+	 *   - arch/x86/kvm/x86.c|12809| <<kvm_alloc_memslot_metadata>> linfo[lpages - 1].disallow_lpage = 1;
+	 *   - arch/x86/kvm/x86.c|12819| <<kvm_alloc_memslot_metadata>> linfo[j].disallow_lpage = 1;
+	 *
+	 *  The kvm_lpage_info denotes whether a specific huge
+	 *  page (GFN and page size) on the memslot is supported.
+	 *
+	 *  Preventing huge pages from spanning adjacent memslot is covered by
+	 *  incrementing the count in head and tail kvm_lpage_info when the
+	 *  memslot is allocated, but disallowing huge pages for memory that has
+	 *  mixed attributes has to be done in a more complicated way.
+	 */
 	int disallow_lpage;
 };
 
 struct kvm_arch_memory_slot {
 	struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
+	/*
+	 * 在以下使用kvm_arch_memory_slot->lpage_info[]:
+	 *   - arch/x86/kvm/mmu/mmu.c|791| <<lpage_info_slot>> return &slot->arch.lpage_info[level - 2][idx];
+	 *   - arch/x86/kvm/x86.c|12744| <<kvm_arch_free_memslot>> kvfree(slot->arch.lpage_info[i - 1]);
+	 *   - arch/x86/kvm/x86.c|12745| <<kvm_arch_free_memslot>> slot->arch.lpage_info[i - 1] = NULL;
+	 *   - arch/x86/kvm/x86.c|12804| <<kvm_alloc_memslot_metadata>> slot->arch.lpage_info[i - 1] = linfo;
+	 *   - arch/x86/kvm/x86.c|12836| <<kvm_alloc_memslot_metadata>> kvfree(slot->arch.lpage_info[i - 1]);
+	 *   - arch/x86/kvm/x86.c|12837| <<kvm_alloc_memslot_metadata>> slot->arch.lpage_info[i - 1] = NULL;
+	 *
+	 * 2个数组: 2M和1G?
+	 */
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
 	unsigned short *gfn_write_track;
 };
@@ -1313,6 +1570,13 @@ struct kvm_arch {
 	struct iommu_domain *iommu_domain;
 	bool iommu_noncoherent;
 #define __KVM_HAVE_ARCH_NONCOHERENT_DMA
+	/*
+	 * 在以下使用kvm_arch->noncoherent_dma_count:
+	 *   - arch/x86/kvm/x86.c|12540| <<kvm_arch_init_vm>> atomic_set(&kvm->arch.noncoherent_dma_count, 0);
+	 *   - arch/x86/kvm/x86.c|13446| <<kvm_arch_register_noncoherent_dma>> if (atomic_inc_return(&kvm->arch.noncoherent_dma_count) == 1)
+	 *   - arch/x86/kvm/x86.c|13453| <<kvm_arch_unregister_noncoherent_dma>> if (!atomic_dec_return(&kvm->arch.noncoherent_dma_count))
+	 *   - arch/x86/kvm/x86.c|13460| <<kvm_arch_has_noncoherent_dma>> return atomic_read(&kvm->arch.noncoherent_dma_count);
+	 */
 	atomic_t noncoherent_dma_count;
 #define __KVM_HAVE_ARCH_ASSIGNED_DEVICE
 	atomic_t assigned_device_count;
@@ -1361,9 +1625,42 @@ struct kvm_arch {
 
 	seqcount_raw_spinlock_t pvclock_sc;
 	bool use_master_clock;
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|3139| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3303| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3471| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|3621| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7287| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+	 */
 	u64 master_kernel_ns;
+	/*
+	 * 在以下使用kvm_arch->master_cycle_now:
+	 *   - arch/x86/kvm/x86.c|3140| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3302| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3470| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3620| <<kvm_get_wall_clock_epoch>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 */
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvmclock_update_work:
+	 *   - arch/x86/include/asm/kvm_host.h|1585| <<global>> struct delayed_work kvmclock_update_work;
+	 *   - arch/x86/kvm/x86.c|3405| <<kvmclock_update_fn>> struct kvm_arch *ka = container_of(dwork, struct kvm_arch, kvmclock_update_work);
+	 *   - arch/x86/kvm/x86.c|3420| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|3433| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|12719| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|12761| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
+	/*
+	 * 在以下使用kvmclock_sync_work:
+	 *   - arch/x86/include/asm/kvm_host.h|1586| <<global>> struct delayed_work kvmclock_sync_work;
+	 *   - arch/x86/kvm/x86.c|3430| <<kvmclock_sync_fn>> struct kvm_arch *ka = container_of(dwork, struct kvm_arch, kvmclock_sync_work);
+	 *   - arch/x86/kvm/x86.c|3434| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|12335| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|12720| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|12760| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	struct delayed_work kvmclock_sync_work;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
@@ -1443,6 +1740,18 @@ struct kvm_arch {
 	 * count to zero should removed the root from the list and clean
 	 * it up, freeing the root after an RCU grace period.
 	 */
+	/*
+	 * 在以下设置kvm_arch->tdp_mmu_roots:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|47| <<kvm_mmu_init_tdp_mmu>> INIT_LIST_HEAD(&kvm->arch.tdp_mmu_roots);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|289| <<kvm_tdp_mmu_get_vcpu_root_hpa>> list_add_rcu(&root->link, &kvm->arch.tdp_mmu_roots);
+	 * 在以下使用kvm_arch->tdp_mmu_roots:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|78| <<kvm_mmu_uninit_tdp_mmu>> WARN_ON(!list_empty(&kvm->arch.tdp_mmu_roots));
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|153| <<tdp_mmu_next_root>> next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|157| <<tdp_mmu_next_root>> next_root = list_first_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|165| <<tdp_mmu_next_root>> next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|209| <<for_each_tdp_mmu_root>> list_for_each_entry(_root, &_kvm->arch.tdp_mmu_roots, link) \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1073| <<kvm_tdp_mmu_invalidate_all_roots>> list_for_each_entry(root, &kvm->arch.tdp_mmu_roots, link) {
+	 */
 	struct list_head tdp_mmu_roots;
 
 	/*
@@ -1534,6 +1843,12 @@ struct kvm_vcpu_stat {
 	u64 pf_emulate;
 	u64 pf_spurious;
 	u64 pf_fast;
+	/*
+	 * 在以下使用kvm_vcpu_stat->pf_mmio_spte_created:
+	 *   - arch/x86/kvm/x86.c|284| <<global>> STATS_DESC_COUNTER(VCPU, pf_mmio_spte_created),
+	 *   - arch/x86/kvm/mmu/mmu.c|2946| <<mmu_set_spte>> vcpu->stat.pf_mmio_spte_created++;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1088| <<tdp_mmu_map_handle_target_level>> vcpu->stat.pf_mmio_spte_created++
+	 */
 	u64 pf_mmio_spte_created;
 	u64 pf_guest;
 	u64 tlb_flush;
@@ -1741,6 +2056,18 @@ struct kvm_x86_ops {
 	 * Size of the CPU's dirty log buffer, i.e. VMX's PML buffer.  A zero
 	 * value indicates CPU dirty logging is unsupported or disabled.
 	 */
+	/*
+	 * 在以下设置kvm_x86_ops.cpu_dirty_log_size:
+	 *   - arch/x86/kvm/vmx/vmx.c|8424| <<global>> .cpu_dirty_log_size = PML_ENTITY_NUM,
+	 *   - arch/x86/kvm/vmx/vmx.c|8664| <<hardware_setup>> vmx_x86_ops.cpu_dirty_log_size = 0;
+	 * 在以下使用kvm_x86_ops.cpu_dirty_log_size:
+	 *   - arch/x86/kvm/mmu/mmu.c|1500| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> if (kvm_x86_ops.cpu_dirty_log_size)
+	 *   - arch/x86/kvm/mmu/mmu.c|1508| <<kvm_cpu_dirty_log_size>> return kvm_x86_ops.cpu_dirty_log_size;
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|205| <<kvm_mmu_page_ad_need_write_protect>> return kvm_x86_ops.cpu_dirty_log_size && sp->role.guest_mode;
+	 *   - arch/x86/kvm/x86.c|6457| <<kvm_arch_sync_dirty_log>> if (!kvm_x86_ops.cpu_dirty_log_size)
+	 *   - arch/x86/kvm/x86.c|13113| <<kvm_mmu_update_cpu_dirty_logging>> if (!kvm_x86_ops.cpu_dirty_log_size)
+	 *   - arch/x86/kvm/x86.c|13189| <<kvm_mmu_slot_apply_flags>> if (kvm_x86_ops.cpu_dirty_log_size) {
+	 */
 	int cpu_dirty_log_size;
 	void (*update_cpu_dirty_logging)(struct kvm_vcpu *vcpu);
 
diff --git a/arch/x86/include/uapi/asm/mtrr.h b/arch/x86/include/uapi/asm/mtrr.h
index 3a8a8eb8a..73f1d91d9 100644
--- a/arch/x86/include/uapi/asm/mtrr.h
+++ b/arch/x86/include/uapi/asm/mtrr.h
@@ -81,6 +81,19 @@ typedef __u8 mtrr_type;
 #define MTRR_NUM_FIXED_RANGES 88
 #define MTRR_MAX_VAR_RANGES 256
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|556| <<get_mtrr_var_range>> rdmsr(MTRRphysBase_MSR(index), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|825| <<generic_get_mtrr>> rdmsr(MTRRphysBase_MSR(reg), base_lo, base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|886| <<set_mtrr_var_ranges>> rdmsr(MTRRphysBase_MSR(index), lo, hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|890| <<set_mtrr_var_ranges>> mtrr_wrmsr(MTRRphysBase_MSR(index), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|1008| <<generic_set_mtrr>> mtrr_wrmsr(MTRRphysBase_MSR(reg), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kvm/mtrr.c|74| <<var_mtrr_msr_to_range>> int index = (msr - MTRRphysBase_MSR(0)) / 2;
+ *   - arch/x86/kvm/mtrr.c|96| <<msr_mtrr_valid>> case MTRRphysBase_MSR(0) ... MTRRphysMask_MSR(KVM_NR_VAR_MTRR - 1):
+ *   - arch/x86/kvm/mtrr.c|169| <<kvm_mtrr_valid>> WARN_ON(!(msr >= MTRRphysBase_MSR(0) &&
+ *   - arch/x86/kvm/x86.c|3851| <<kvm_set_msr_common>> case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ *   - arch/x86/kvm/x86.c|4266| <<kvm_get_msr_common>> case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ */
 #define MTRRphysBase_MSR(reg) (0x200 + 2 * (reg))
 #define MTRRphysMask_MSR(reg) (0x200 + 2 * (reg) + 1)
 
diff --git a/arch/x86/kernel/apic/msi.c b/arch/x86/kernel/apic/msi.c
index d9651f15a..074dfb6f3 100644
--- a/arch/x86/kernel/apic/msi.c
+++ b/arch/x86/kernel/apic/msi.c
@@ -43,6 +43,9 @@ msi_set_affinity(struct irq_data *irqd, const struct cpumask *mask, bool force)
 	cpu = cpumask_first(irq_data_get_effective_affinity_mask(irqd));
 	old_cfg = *cfg;
 
+	/*
+	 * apic_set_affinity()
+	 */
 	/* Allocate a new target vector */
 	ret = parent->chip->irq_set_affinity(parent, mask, force);
 	if (ret < 0 || ret == IRQ_SET_MASK_OK_DONE)
diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c
index 185738c72..bb34f0cef 100644
--- a/arch/x86/kernel/apic/vector.c
+++ b/arch/x86/kernel/apic/vector.c
@@ -23,41 +23,362 @@
 
 #include <asm/trace/irq_vectors.h>
 
+/*
+ * CPU的online是在arch/x86/kernel/smpboot.c:start_secondary().
+ *
+ * 253 static void notrace start_secondary(void *unused)
+ * 254 {
+ * ... ...
+ * 324         lock_vector_lock();
+ * 325         set_cpu_online(smp_processor_id(), true);
+ * 326         lapic_online();
+ * 327         unlock_vector_lock();
+ *
+ * CPU的offline是在arch/x86/kernel/smpboot.c:cpu_disable_common().
+ *
+ * 1436 void cpu_disable_common(void)
+ * 1437 {
+ * 1438         int cpu = smp_processor_id();
+ * 1439
+ * 1440         remove_siblinginfo(cpu);
+ * 1441
+ * 1442         // It's now safe to remove this processor from the online map
+ * 1443         lock_vector_lock();
+ * 1444         remove_cpu_from_maps(cpu);
+ * 1445         unlock_vector_lock();
+ * 1446         fixup_irqs();
+ * 1447         lapic_offline();
+ * 1448 }
+ *
+ * 在旧的CPU上.
+ * [0] irq_matrix_alloc
+ * [0] assign_vector_locked
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_move_masked_irq
+ * [0] __irq_move_irq
+ * [0] apic_ack_edge
+ * [0] handle_edge_irq
+ * [0] __common_interrupt
+ * [0] common_interrupt
+ *
+ * 在新的CPU上
+ * [0] hlist_add_head(&apicd->clist, &cl->head)
+ * [0] __vector_schedule_cleanup --> 清空move_in_progress
+ * [0] irq_complete_move --> 检查apicd->move_in_progress, 不然退出
+ * [0] apic_ack_edge
+ * [0] handle_edge_irq
+ * [0] __common_interrupt
+ * [0] common_interrupt
+ *
+ * [0] irq_matrix_free
+ * [0] free_moved_vector
+ * [0] __vector_cleanup
+ * [0] vector_cleanup_callback
+ * [0] call_timer_fn
+ * [0] __run_timers
+ * [0] run_timer_softirq
+ * [0] __do_softirq
+ * [0] __irq_exit_rcu
+ * [0] sysvec_apic_timer_interrupt
+ */
+
+/*
+ * 下面是换affinity的情况:
+ *
+ * 1. CPU offline, affinity要迁移走.
+ *
+ * [0] irq_matrix_alloc
+ * [0] assign_vector_locked
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_migrate_all_off_this_cpu
+ * [0] fixup_irqs
+ * [0] native_cpu_disable
+ * [0] take_cpu_down
+ * [0] multi_cpu_stop
+ * [0] cpu_stopper_thread
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] irq_matrix_free
+ * [0] apic_update_vector
+ * [0] assign_vector_locked
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_migrate_all_off_this_cpu
+ * [0] fixup_irqs
+ * [0] native_cpu_disable
+ * [0] take_cpu_down
+ * [0] multi_cpu_stop
+ * [0] cpu_stopper_thread
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * 2. 通过procfs修改affinity. CPU offline/online的时候virtio-net会用API做这些.
+ *
+ * [0] irq_matrix_alloc
+ * [0] assign_vector_locked
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_move_masked_irq
+ * [0] __irq_move_irq
+ * [0] apic_ack_edge
+ * [0] handle_edge_irq
+ * [0] __common_interrupt
+ * [0] common_interrupt
+ *
+ * [0] irq_matrix_free
+ * [0] free_moved_vector
+ * [0] __vector_cleanup
+ * [0] vector_cleanup_callback
+ * [0] call_timer_fn
+ * [0] __run_timers
+ * [0] run_timer_softirq
+ * [0] __do_softirq
+ * [0] __irq_exit_rcu
+ * [0] sysvec_apic_timer_interrupt
+ */
+
+/*
+ * 90 struct irq_cfg {
+ * 91         unsigned int            dest_apicid;
+ * 92         unsigned int            vector;
+ * 93 };
+ */
+
 struct apic_chip_data {
 	struct irq_cfg		hw_irq_cfg;
 	unsigned int		vector;
+	/*
+	 * 在以下设置apic_chip_data->prev_vector:
+	 *   - arch/x86/kernel/apic/vector.c|341| <<apic_update_vector>> apicd->prev_vector = 0;
+	 *   - arch/x86/kernel/apic/vector.c|354| <<apic_update_vector>> apicd->prev_vector = apicd->vector;
+	 *   - arch/x86/kernel/apic/vector.c|636| <<clear_irq_vector>> apicd->prev_vector = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1292| <<free_moved_vector>> apicd->prev_vector = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1397| <<__vector_schedule_cleanup>> apicd->prev_vector = 0;
+	 * 在以下使用apic_chip_data->prev_vector:
+	 *   - arch/x86/kernel/apic/vector.c|622| <<clear_irq_vector>> trace_vector_clear(irqd->irq, vector, apicd->cpu, apicd->prev_vector,
+	 *   - arch/x86/kernel/apic/vector.c|630| <<clear_irq_vector>> vector = apicd->prev_vector;
+	 *   - arch/x86/kernel/apic/vector.c|926| <<x86_vector_debug_show>> if (apicd.prev_vector) {
+	 *   - arch/x86/kernel/apic/vector.c|927| <<x86_vector_debug_show>> seq_printf(m, "%*sPrevious vector: %5u\n", ind, "", apicd.prev_vector);
+	 *   - arch/x86/kernel/apic/vector.c|1274| <<free_moved_vector>> unsigned int vector = apicd->prev_vector;
+	 *   - arch/x86/kernel/apic/vector.c|1310| <<__vector_cleanup>> unsigned int irr, vector = apicd->prev_vector;
+	 *   - arch/x86/kernel/apic/vector.c|1477| <<irq_force_complete_move>> vector = apicd->prev_vector;
+	 */
 	unsigned int		prev_vector;
 	unsigned int		cpu;
+	/*
+	 * 在以下设置apic_chip_data->prev_cpu:
+	 *   - arch/x86/kernel/apic/vector.c|389| <<apic_update_vector>> apicd->prev_cpu = apicd->cpu; 
+	 * 在以下使用apic_chip_data->prev_cpu:
+	 *   - arch/x86/kernel/apic/vector.c|657| <<clear_irq_vector>> trace_vector_clear(irqd->irq, vector, apicd->cpu, apicd->prev_vector, apicd->prev_cpu);
+	 *   - arch/x86/kernel/apic/vector.c|668| <<clear_irq_vector>> per_cpu(vector_irq, apicd->prev_cpu)[vector] = VECTOR_SHUTDOWN;
+	 *   - arch/x86/kernel/apic/vector.c|669| <<clear_irq_vector>> irq_matrix_free(vector_matrix, apicd->prev_cpu, vector, managed);
+	 *   - arch/x86/kernel/apic/vector.c|967| <<x86_vector_debug_show>> seq_printf(m, "%*sPrevious target: %5u\n", ind, "", apicd.prev_cpu);
+	 *   - arch/x86/kernel/apic/vector.c|1318| <<free_moved_vector>> unsigned int cpu = apicd->prev_cpu;
+	 *   - arch/x86/kernel/apic/vector.c|1432| <<__vector_schedule_cleanup>> unsigned int cpu = apicd->prev_cpu;
+	 */
 	unsigned int		prev_cpu;
 	unsigned int		irq;
+	/*
+	 * 在以下使用apic_chip_data->clist:
+	 *   - arch/x86/kernel/apic/vector.c|296| <<alloc_apic_chip_data>> INIT_HLIST_NODE(&apicd->clist);
+	 *   - arch/x86/kernel/apic/vector.c|511| <<assign_vector_locked>> if (apicd->move_in_progress || !hlist_unhashed(&apicd->clist))
+	 *   - arch/x86/kernel/apic/vector.c|662| <<clear_irq_vector>> hlist_del_init(&apicd->clist);
+	 *   - arch/x86/kernel/apic/vector.c|963| <<x86_vector_debug_show>> seq_printf(m, "%*scleanup_pending: %u\n", ind, "", !hlist_unhashed(&apicd.clist));
+	 *   - arch/x86/kernel/apic/vector.c|1331| <<free_moved_vector>> hlist_del_init(&apicd->clist);
+	 *   - arch/x86/kernel/apic/vector.c|1362| <<__vector_cleanup>> hlist_for_each_entry_safe(apicd, tmp, &cl->head, clist) {
+	 *   - arch/x86/kernel/apic/vector.c|1429| <<__vector_schedule_cleanup>> hlist_add_head(&apicd->clist, &cl->head);
+	 */
 	struct hlist_node	clist;
+	/*
+	 * 在以下设置apic_chip_data->is_managed:
+	 *   - arch/x86/kernel/apic/vector.c|203| <<reserve_managed_vector>> apicd->is_managed = true;
+	 *
+	 * 在以下设置apic_chip_data->move_in_progress:
+	 *   - arch/x86/kernel/apic/vector.c|278| <<apic_update_vector>> apicd->move_in_progress = true;
+	 *   - arch/x86/kernel/apic/vector.c|549| <<clear_irq_vector>> apicd->move_in_progress = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1205| <<free_moved_vector>> apicd->move_in_progress = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1268| <<__vector_schedule_cleanup>> apicd->move_in_progress = 0;
+	 * 在以下使用apic_chip_data->move_in_progress:
+	 *   - arch/x86/kernel/apic/vector.c|405| <<assign_vector_locked>> if (apicd->move_in_progress || !hlist_unhashed(&apicd->clist))
+	 *   - arch/x86/kernel/apic/vector.c|842| <<x86_vector_debug_show>> seq_printf(m, "%*smove_in_progress: %u\n", ind, "", apicd.move_in_progress ? 1 : 0);
+	 *   - arch/x86/kernel/apic/vector.c|1303| <<vector_schedule_cleanup>> if (apicd->move_in_progress)
+	 *   - arch/x86/kernel/apic/vector.c|1312| <<irq_complete_move>> if (likely(!apicd->move_in_progress))
+	 *   - arch/x86/kernel/apic/vector.c|1379| <<irq_force_complete_move>> if (apicd->move_in_progress) {
+	 */
 	unsigned int		move_in_progress	: 1,
 				is_managed		: 1,
 				can_reserve		: 1,
 				has_reserved		: 1;
 };
 
+/*
+ * 在以下使用x86_vector_domain:
+ *   - arch/x86/hyperv/irqdomain.c|331| <<hv_create_pci_msi_domain>> d = pci_msi_create_irq_domain(fn, &hv_pci_msi_domain_info, x86_vector_domain);
+ *   - arch/x86/include/asm/irq_remapping.h|50| <<arch_get_ir_parent_domain>> return x86_vector_domain;
+ *   - arch/x86/kernel/apic/io_apic.c|3047| <<mp_irqdomain_alloc>> irq_data->chip = (domain->parent == x86_vector_domain) ?
+ *   - arch/x86/kernel/apic/msi.c|159| <<pci_dev_has_default_msi_parent_domain>> return domain == x86_vector_domain;
+ *   - arch/x86/kernel/apic/msi.c|273| <<native_create_pci_msi_domain>> x86_vector_domain->flags |= IRQ_DOMAIN_FLAG_MSI_PARENT;
+ *   - arch/x86/kernel/apic/msi.c|274| <<native_create_pci_msi_domain>> x86_vector_domain->msi_parent_ops = &x86_vector_msi_parent_ops;
+ *   - arch/x86/kernel/apic/msi.c|275| <<native_create_pci_msi_domain>> return x86_vector_domain;
+ *   - arch/x86/kernel/apic/msi.c|360| <<dmar_get_irq_domain>> x86_vector_domain);
+ *   - arch/x86/kernel/apic/vector.c|509| <<x86_vector_free_irqs>> irqd = irq_domain_get_irq_data(x86_vector_domain, virq + i);
+ *   - arch/x86/kernel/apic/vector.c|807| <<arch_early_irq_init>> x86_vector_domain = irq_domain_create_tree(fn, &x86_vector_domain_ops,
+ *   - arch/x86/kernel/apic/vector.c|809| <<arch_early_irq_init>> BUG_ON(x86_vector_domain == NULL);
+ *   - arch/x86/kernel/apic/vector.c|810| <<arch_early_irq_init>> irq_set_default_host(x86_vector_domain);
+ *   - arch/x86/kernel/apic/vector.c|1097| <<irq_force_complete_move>> irqd = irq_domain_get_irq_data(x86_vector_domain,
+ *   - arch/x86/kernel/hpet.c|550| <<hpet_create_irq_domain>> if (x86_vector_domain == NULL)
+ *   - arch/x86/kernel/hpet.c|577| <<hpet_create_irq_domain>> if (parent != x86_vector_domain)
+ *   - arch/x86/platform/uv/uv_irq.c|169| <<uv_get_irq_domain>> uv_domain = irq_domain_create_hierarchy(x86_vector_domain, 0, 0, fn,
+ *   - drivers/iommu/amd/init.c|2398| <<iommu_get_irqdomain>> iommu_irqdomain = irq_domain_create_hierarchy(x86_vector_domain, 0, 0,
+ *   - drivers/pci/controller/pci-hyperv.c|590| <<hv_pci_get_root_domain>> return x86_vector_domain;
+ */
 struct irq_domain *x86_vector_domain;
 EXPORT_SYMBOL_GPL(x86_vector_domain);
+/*
+ * 在以下使用vector_lock:
+ *   - arch/x86/kernel/apic/vector.c|187| <<global>> static DEFINE_RAW_SPINLOCK(vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|250| <<lock_vector_lock>> raw_spin_lock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|255| <<unlock_vector_lock>> raw_spin_unlock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|338| <<apic_update_irq_cfg>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|375| <<apic_update_vector>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|460| <<reserve_managed_vector>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|463| <<reserve_managed_vector>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|493| <<reserve_irq_vector>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|495| <<reserve_irq_vector>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|516| <<assign_vector_locked>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|570| <<assign_irq_vector>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|573| <<assign_irq_vector>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|662| <<clear_irq_vector>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|701| <<x86_vector_deactivate>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|707| <<x86_vector_deactivate>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|783| <<x86_vector_activate>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|792| <<x86_vector_activate>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|830| <<x86_vector_free_irqs>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|835| <<x86_vector_free_irqs>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|850| <<vector_configure_legacy>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|865| <<vector_configure_legacy>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|970| <<x86_vector_debug_show>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|972| <<x86_vector_debug_show>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|1181| <<lapic_online>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1259| <<apic_set_affinity>> raw_spin_lock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1268| <<apic_set_affinity>> raw_spin_unlock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1281| <<apic_retrigger_irq>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|1283| <<apic_retrigger_irq>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|1381| <<__vector_cleanup>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1431| <<vector_cleanup_callback>> raw_spin_lock_irq(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1433| <<vector_cleanup_callback>> raw_spin_unlock_irq(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1456| <<__vector_schedule_cleanup>> raw_spin_lock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1535| <<__vector_schedule_cleanup>> raw_spin_unlock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1620| <<irq_force_complete_move>> raw_spin_lock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1719| <<irq_force_complete_move>> raw_spin_unlock(&vector_lock);
+ */
 static DEFINE_RAW_SPINLOCK(vector_lock);
+/*
+ * 很多调用
+ */
 static cpumask_var_t vector_searchmask;
+/*
+ * 956 static struct irq_chip lapic_controller = {
+ * 957         .name                   = "APIC",
+ * 958         .irq_ack                = apic_ack_edge,
+ * 959         .irq_set_affinity       = apic_set_affinity,
+ * 960         .irq_compose_msi_msg    = x86_vector_msi_compose_msg,
+ * 961         .irq_retrigger          = apic_retrigger_irq,
+ * 962 };
+ *
+ * 在以下使用lapic_controller:
+ *   - arch/x86/kernel/apic/vector.c|933| <<global>> static struct irq_chip lapic_controller = {
+ *   - arch/x86/kernel/apic/vector.c|581| <<x86_vector_alloc_irqs>> irqd->chip = &lapic_controller;
+ */
 static struct irq_chip lapic_controller;
+/*
+ * 被很多调用
+ */
 static struct irq_matrix *vector_matrix;
 #ifdef CONFIG_SMP
 
 static void vector_cleanup_callback(struct timer_list *tmr);
 
 struct vector_cleanup {
+	/*
+	 * 在以下使用vector_cleanup->head:
+	 *   - arch/x86/kernel/apic/vector.c|188| <<global>> .head = HLIST_HEAD_INIT,
+	 *   - arch/x86/kernel/apic/vector.c|1162| <<lapic_offline>> WARN_ON_ONCE(!hlist_empty(&cl->head));
+	 *   - arch/x86/kernel/apic/vector.c|1309| <<__vector_cleanup>> hlist_for_each_entry_safe(apicd, tmp, &cl->head, clist) {
+	 *   - arch/x86/kernel/apic/vector.c|1376| <<__vector_schedule_cleanup>> hlist_add_head(&apicd->clist, &cl->head);
+	 */
 	struct hlist_head	head;
 	struct timer_list	timer;
 };
 
+/*
+ * 在以下使用percpu的vector_cleanup:
+ *   - arch/x86/kernel/apic/vector.c|55| <<global>> static DEFINE_PER_CPU(struct vector_cleanup, vector_cleanup) = {
+ *   - arch/x86/kernel/apic/vector.c|867| <<lapic_offline>> struct vector_cleanup *cl = this_cpu_ptr(&vector_cleanup);
+ *   - arch/x86/kernel/apic/vector.c|1024| <<__vector_schedule_cleanup>> struct vector_cleanup *cl = per_cpu_ptr(&vector_cleanup, cpu);
+ */
 static DEFINE_PER_CPU(struct vector_cleanup, vector_cleanup) = {
 	.head	= HLIST_HEAD_INIT,
 	.timer	= __TIMER_INITIALIZER(vector_cleanup_callback, TIMER_PINNED),
 };
 #endif
 
+/*
+ * 在以下使用vector_lock:
+ *   - arch/x86/kernel/apic/vector.c|187| <<global>> static DEFINE_RAW_SPINLOCK(vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|250| <<lock_vector_lock>> raw_spin_lock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|255| <<unlock_vector_lock>> raw_spin_unlock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|338| <<apic_update_irq_cfg>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|375| <<apic_update_vector>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|460| <<reserve_managed_vector>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|463| <<reserve_managed_vector>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|493| <<reserve_irq_vector>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|495| <<reserve_irq_vector>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|516| <<assign_vector_locked>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|570| <<assign_irq_vector>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|573| <<assign_irq_vector>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|662| <<clear_irq_vector>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|701| <<x86_vector_deactivate>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|707| <<x86_vector_deactivate>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|783| <<x86_vector_activate>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|792| <<x86_vector_activate>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|830| <<x86_vector_free_irqs>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|835| <<x86_vector_free_irqs>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|850| <<vector_configure_legacy>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|865| <<vector_configure_legacy>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|970| <<x86_vector_debug_show>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|972| <<x86_vector_debug_show>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|1181| <<lapic_online>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1259| <<apic_set_affinity>> raw_spin_lock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1268| <<apic_set_affinity>> raw_spin_unlock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1281| <<apic_retrigger_irq>> raw_spin_lock_irqsave(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|1283| <<apic_retrigger_irq>> raw_spin_unlock_irqrestore(&vector_lock, flags);
+ *   - arch/x86/kernel/apic/vector.c|1381| <<__vector_cleanup>> lockdep_assert_held(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1431| <<vector_cleanup_callback>> raw_spin_lock_irq(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1433| <<vector_cleanup_callback>> raw_spin_unlock_irq(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1456| <<__vector_schedule_cleanup>> raw_spin_lock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1535| <<__vector_schedule_cleanup>> raw_spin_unlock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1620| <<irq_force_complete_move>> raw_spin_lock(&vector_lock);
+ *   - arch/x86/kernel/apic/vector.c|1719| <<irq_force_complete_move>> raw_spin_unlock(&vector_lock);
+ */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/msi.c|99| <<msi_set_affinity>> lock_vector_lock();
+ *   - arch/x86/kernel/apic/vector.c|869| <<lapic_offline>> lock_vector_lock();
+ *   - arch/x86/kernel/smpboot.c|324| <<start_secondary>> lock_vector_lock();
+ *   - arch/x86/kernel/smpboot.c|1438| <<cpu_disable_common>> lock_vector_lock();
+ */
 void lock_vector_lock(void)
 {
 	/* Used to the online set of cpus does not change
@@ -71,6 +392,20 @@ void unlock_vector_lock(void)
 	raw_spin_unlock(&vector_lock);
 }
 
+/*
+ * 注释:
+ * irq_alloc_info - X86 specific interrupt allocation info
+ * @type:       X86 specific allocation type
+ * @flags:      Flags for allocation tweaks
+ * @devid:      Device ID for allocations
+ * @hwirq:      Associated hw interrupt number in the domain
+ * @mask:       CPU mask for vector allocation
+ * @desc:       Pointer to msi descriptor
+ * @data:       Allocation specific data
+ *
+ * @ioapic:     IOAPIC specific allocation data
+ * @uv:         UV specific allocation data
+ */
 void init_irq_alloc_info(struct irq_alloc_info *info,
 			 const struct cpumask *mask)
 {
@@ -125,6 +460,13 @@ static void free_apic_chip_data(struct apic_chip_data *apicd)
 	kfree(apicd);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|309| <<vector_assign_managed_shutdown>> apic_update_irq_cfg(irqd, MANAGED_IRQ_SHUTDOWN_VECTOR, cpu);
+ *   - arch/x86/kernel/apic/vector.c|399| <<assign_vector_locked>> apic_update_irq_cfg(irqd, vector, cpu);
+ *   - arch/x86/kernel/apic/vector.c|489| <<assign_managed_vector>> apic_update_irq_cfg(irqd, vector, cpu);
+ *   - arch/x86/kernel/apic/vector.c|682| <<vector_configure_legacy>> apic_update_irq_cfg(irqd, apicd->vector, apicd->cpu);
+ */
 static void apic_update_irq_cfg(struct irq_data *irqd, unsigned int vector,
 				unsigned int cpu)
 {
@@ -134,11 +476,141 @@ static void apic_update_irq_cfg(struct irq_data *irqd, unsigned int vector,
 
 	apicd->hw_irq_cfg.vector = vector;
 	apicd->hw_irq_cfg.dest_apicid = apic->calc_dest_apicid(cpu);
+	/*
+	 * 在这里更新effective!
+	 */
 	irq_data_update_effective_affinity(irqd, cpumask_of(cpu));
 	trace_vector_config(irqd->irq, vector, cpu,
 			    apicd->hw_irq_cfg.dest_apicid);
 }
 
+/*
+ * 下面是换affinity的情况:
+ *
+ * 1. CPU offline, affinity要迁移走.
+ *
+ * [0] irq_matrix_alloc
+ * [0] assign_vector_locked
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_migrate_all_off_this_cpu
+ * [0] fixup_irqs
+ * [0] native_cpu_disable
+ * [0] take_cpu_down
+ * [0] multi_cpu_stop
+ * [0] cpu_stopper_thread
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] irq_matrix_free
+ * [0] apic_update_vector
+ * [0] assign_vector_locked
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_migrate_all_off_this_cpu
+ * [0] fixup_irqs
+ * [0] native_cpu_disable
+ * [0] take_cpu_down
+ * [0] multi_cpu_stop
+ * [0] cpu_stopper_thread
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * 2. 通过procfs修改affinity. CPU offline/online的时候virtio-net会用API做这些.
+ *
+ * [0] irq_matrix_alloc
+ * [0] assign_vector_locked
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_move_masked_irq
+ * [0] __irq_move_irq
+ * [0] apic_ack_edge
+ * [0] handle_edge_irq
+ * [0] __common_interrupt
+ * [0] common_interrupt
+ */
+
+/*
+ * [0] irq_matrix_alloc
+ * [0] assign_vector_locked
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_move_masked_irq
+ * [0] __irq_move_irq
+ * [0] apic_ack_edge
+ * [0] handle_edge_irq
+ * [0] __common_interrupt
+ * [0] common_interrupt
+ *
+ * [0] irq_matrix_free
+ * [0] free_moved_vector
+ * [0] __vector_cleanup
+ * [0] vector_cleanup_callback
+ * [0] call_timer_fn
+ * [0] __run_timers
+ * [0] run_timer_softirq
+ * [0] __do_softirq
+ * [0] __irq_exit_rcu
+ * [0] sysvec_apic_timer_interrupt
+ *
+ * [0] irq_matrix_alloc
+ * [0] assign_vector_locked
+ * [0] assign_irq_vector_any_locked
+ * [0] x86_vector_activate
+ * [0] __irq_domain_activate_irq
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] vp_find_vqs_msix
+ * [0] vp_find_vqs
+ * [0] vp_modern_find_vqs
+ * [0] init_vqs
+ * [0] virtnet_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] virtio_net_driver_init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ *
+ * write_irq_affinity()
+ */
+/*
+ * struct irq_data - per irq chip data passed down to chip functions
+ * @mask:               precomputed bitmask for accessing the chip registers
+ * @irq:                interrupt number
+ * @hwirq:              hardware interrupt number, local to the interrupt domain
+ * @common:             point to data shared by all irqchips
+ * @chip:               low level interrupt hardware access
+ * @domain:             Interrupt translation domain; responsible for mapping
+ *                      between hwirq number and linux irq number.
+ * @parent_data:        pointer to parent struct irq_data to support hierarchy
+ *                      irq_domain
+ * @chip_data:          platform-specific per-chip private data for the chip
+ *                      methods, to allow shared chip implementations
+ *
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|263| <<assign_vector_locked>> apic_update_vector(irqd, vector, cpu);
+ *   - arch/x86/kernel/apic/vector.c|348| <<assign_managed_vector>> apic_update_vector(irqd, vector, cpu);
+ */
 static void apic_update_vector(struct irq_data *irqd, unsigned int newvec,
 			       unsigned int newcpu)
 {
@@ -157,6 +629,14 @@ static void apic_update_vector(struct irq_data *irqd, unsigned int newvec,
 	 * shutdown mode work, then there is nothing to release. Clear out
 	 * prev_vector for this and the offlined target case.
 	 */
+	/*
+	 * 在以下设置apic_chip_data->prev_vector:
+	 *   - arch/x86/kernel/apic/vector.c|341| <<apic_update_vector>> apicd->prev_vector = 0;
+	 *   - arch/x86/kernel/apic/vector.c|354| <<apic_update_vector>> apicd->prev_vector = apicd->vector;
+	 *   - arch/x86/kernel/apic/vector.c|636| <<clear_irq_vector>> apicd->prev_vector = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1292| <<free_moved_vector>> apicd->prev_vector = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1397| <<__vector_schedule_cleanup>> apicd->prev_vector = 0;
+	 */
 	apicd->prev_vector = 0;
 	if (!apicd->vector || apicd->vector == MANAGED_IRQ_SHUTDOWN_VECTOR)
 		goto setnew;
@@ -169,11 +649,39 @@ static void apic_update_vector(struct irq_data *irqd, unsigned int newvec,
 	 * in the underlying matrix allocator.
 	 */
 	if (cpu_online(apicd->cpu)) {
+		/*
+		 * 在以下设置apic_chip_data->move_in_progress:
+		 *   - arch/x86/kernel/apic/vector.c|278| <<apic_update_vector>> apicd->move_in_progress = true;
+		 *   - arch/x86/kernel/apic/vector.c|549| <<clear_irq_vector>> apicd->move_in_progress = 0;
+		 *   - arch/x86/kernel/apic/vector.c|1205| <<free_moved_vector>> apicd->move_in_progress = 0;
+		 *   - arch/x86/kernel/apic/vector.c|1268| <<__vector_schedule_cleanup>> apicd->move_in_progress = 0;
+		 * 在以下使用apic_chip_data->move_in_progress:
+		 *   - arch/x86/kernel/apic/vector.c|405| <<assign_vector_locked>> if (apicd->move_in_progress || !hlist_unhashed(&apicd->clist))
+		 *   - arch/x86/kernel/apic/vector.c|842| <<x86_vector_debug_show>> seq_printf(m, "%*smove_in_progress: %u\n", ind, "", apicd.move_in_progress ? 1 : 0);
+		 *   - arch/x86/kernel/apic/vector.c|1303| <<vector_schedule_cleanup>> if (apicd->move_in_progress)
+		 *   - arch/x86/kernel/apic/vector.c|1312| <<irq_complete_move>> if (likely(!apicd->move_in_progress))
+		 *   - arch/x86/kernel/apic/vector.c|1379| <<irq_force_complete_move>> if (apicd->move_in_progress) {
+		 */
 		apicd->move_in_progress = true;
+		/*
+		 * 在以下设置apic_chip_data->prev_vector:
+		 *   - arch/x86/kernel/apic/vector.c|341| <<apic_update_vector>> apicd->prev_vector = 0;
+		 *   - arch/x86/kernel/apic/vector.c|354| <<apic_update_vector>> apicd->prev_vector = apicd->vector;
+		 *   - arch/x86/kernel/apic/vector.c|636| <<clear_irq_vector>> apicd->prev_vector = 0;
+		 *   - arch/x86/kernel/apic/vector.c|1292| <<free_moved_vector>> apicd->prev_vector = 0;
+		 *   - arch/x86/kernel/apic/vector.c|1397| <<__vector_schedule_cleanup>> apicd->prev_vector = 0;
+		 */
 		apicd->prev_vector = apicd->vector;
 		apicd->prev_cpu = apicd->cpu;
 		WARN_ON_ONCE(apicd->cpu == newcpu);
 	} else {
+		/*
+		 * called by:
+		 *   - arch/x86/kernel/apic/vector.c|273| <<apic_update_vector>> irq_matrix_free(vector_matrix, apicd->cpu, apicd->vector, managed);
+		 *   - arch/x86/kernel/apic/vector.c|503| <<clear_irq_vector>> irq_matrix_free(vector_matrix, apicd->cpu, vector, managed);
+		 *   - arch/x86/kernel/apic/vector.c|512| <<clear_irq_vector>> irq_matrix_free(vector_matrix, apicd->prev_cpu, vector, managed);
+		 *   - arch/x86/kernel/apic/vector.c|1100| <<free_moved_vector>> irq_matrix_free(vector_matrix, cpu, vector, managed);
+		 */
 		irq_matrix_free(vector_matrix, apicd->cpu, apicd->vector,
 				managed);
 	}
@@ -182,6 +690,27 @@ static void apic_update_vector(struct irq_data *irqd, unsigned int newvec,
 	apicd->vector = newvec;
 	apicd->cpu = newcpu;
 	BUG_ON(!IS_ERR_OR_NULL(per_cpu(vector_irq, newcpu)[newvec]));
+	/*
+	 * 在以下使用percpu的vector_irq:
+	 *   - arch/x86/include/asm/hw_irq.h|131| <<global>> DECLARE_PER_CPU(vector_irq_t, vector_irq);
+	 *   - arch/x86/kernel/irqinit.c|49| <<global>> DEFINE_PER_CPU(vector_irq_t, vector_irq) = {
+	 *   - arch/x86/kernel/apic/msi.c|111| <<msi_set_affinity>> if (IS_ERR_OR_NULL(this_cpu_read(vector_irq[cfg->vector])))
+	 *   - arch/x86/kernel/apic/msi.c|112| <<msi_set_affinity>> this_cpu_write(vector_irq[cfg->vector], VECTOR_RETRIGGERED);
+	 *   - arch/x86/kernel/apic/vector.c|184| <<apic_update_vector>> BUG_ON(!IS_ERR_OR_NULL(per_cpu(vector_irq, newcpu)[newvec]));
+	 *   - arch/x86/kernel/apic/vector.c|185| <<apic_update_vector>> per_cpu(vector_irq, newcpu)[newvec] = desc;
+	 *   - arch/x86/kernel/apic/vector.c|367| <<clear_irq_vector>> per_cpu(vector_irq, apicd->cpu)[vector] = VECTOR_SHUTDOWN;
+	 *   - arch/x86/kernel/apic/vector.c|376| <<clear_irq_vector>> per_cpu(vector_irq, apicd->prev_cpu)[vector] = VECTOR_SHUTDOWN;
+	 *   - arch/x86/kernel/apic/vector.c|860| <<lapic_online>> this_cpu_write(vector_irq[vector], __setup_vector_irq(vector));
+	 *   - arch/x86/kernel/apic/vector.c|961| <<free_moved_vector>> per_cpu(vector_irq, cpu)[vector] = VECTOR_UNUSED;
+	 *   - arch/x86/kernel/irq.c|255| <<DEFINE_IDTENTRY_IRQ(common_interrupt)>> desc = __this_cpu_read(vector_irq[vector]);
+	 *   - arch/x86/kernel/irq.c|266| <<DEFINE_IDTENTRY_IRQ(common_interrupt)>> __this_cpu_write(vector_irq[vector], VECTOR_UNUSED);
+	 *   - arch/x86/kernel/irq.c|366| <<fixup_irqs>> if (IS_ERR_OR_NULL(__this_cpu_read(vector_irq[vector])))
+	 *   - arch/x86/kernel/irq.c|371| <<fixup_irqs>> desc = __this_cpu_read(vector_irq[vector]);
+	 *   - arch/x86/kernel/irq.c|378| <<fixup_irqs>> __this_cpu_write(vector_irq[vector], VECTOR_RETRIGGERED);
+	 *   - arch/x86/kernel/irq.c|382| <<fixup_irqs>> if (__this_cpu_read(vector_irq[vector]) != VECTOR_RETRIGGERED)
+	 *   - arch/x86/kernel/irq.c|383| <<fixup_irqs>> __this_cpu_write(vector_irq[vector], VECTOR_UNUSED);
+	 *   - arch/x86/kernel/irqinit.c|87| <<init_IRQ>> per_cpu(vector_irq, 0)[ISA_IRQ_VECTOR(i)] = irq_to_desc(i);
+	 */
 	per_cpu(vector_irq, newcpu)[newvec] = desc;
 }
 
@@ -192,6 +721,10 @@ static void vector_assign_managed_shutdown(struct irq_data *irqd)
 	apic_update_irq_cfg(irqd, MANAGED_IRQ_SHUTDOWN_VECTOR, cpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|321| <<assign_irq_vector_policy>> return reserve_managed_vector(irqd);
+ */
 static int reserve_managed_vector(struct irq_data *irqd)
 {
 	const struct cpumask *affmsk = irq_data_get_affinity_mask(irqd);
@@ -207,6 +740,11 @@ static int reserve_managed_vector(struct irq_data *irqd)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|348| <<reserve_irq_vector>> reserve_irq_vector_locked(irqd);
+ *   - arch/x86/kernel/apic/vector.c|536| <<x86_vector_deactivate>> reserve_irq_vector_locked(irqd);
+ */
 static void reserve_irq_vector_locked(struct irq_data *irqd)
 {
 	struct apic_chip_data *apicd = apic_chip_data(irqd);
@@ -219,6 +757,10 @@ static void reserve_irq_vector_locked(struct irq_data *irqd)
 	vector_assign_managed_shutdown(irqd);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|489| <<assign_irq_vector_policy>> return reserve_irq_vector(irqd);
+ */
 static int reserve_irq_vector(struct irq_data *irqd)
 {
 	unsigned long flags;
@@ -229,6 +771,15 @@ static int reserve_irq_vector(struct irq_data *irqd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|280| <<assign_irq_vector>> ret = assign_vector_locked(irqd, vector_searchmask);
+ *   - arch/x86/kernel/apic/vector.c|294| <<assign_irq_vector_any_locked>> if (!assign_vector_locked(irqd, vector_searchmask))
+ *   - arch/x86/kernel/apic/vector.c|300| <<assign_irq_vector_any_locked>> if (!assign_vector_locked(irqd, vector_searchmask))
+ *   - arch/x86/kernel/apic/vector.c|305| <<assign_irq_vector_any_locked>> if (!assign_vector_locked(irqd, cpumask_of_node(node)))
+ *   - arch/x86/kernel/apic/vector.c|310| <<assign_irq_vector_any_locked>> return assign_vector_locked(irqd, cpu_online_mask);
+ *   - arch/x86/kernel/apic/vector.c|894| <<apic_set_affinity>> err = assign_vector_locked(irqd, vector_searchmask);
+ */
 static int
 assign_vector_locked(struct irq_data *irqd, const struct cpumask *dest)
 {
@@ -256,16 +807,36 @@ assign_vector_locked(struct irq_data *irqd, const struct cpumask *dest)
 	if (apicd->move_in_progress || !hlist_unhashed(&apicd->clist))
 		return -EBUSY;
 
+	/*
+	 * 只在此处调用
+	 * 从所有mask的cpu中选一个cpu分配一个regular interrupt (不是managed)
+	 */
 	vector = irq_matrix_alloc(vector_matrix, dest, resvd, &cpu);
 	trace_vector_alloc(irqd->irq, vector, resvd, vector);
 	if (vector < 0)
 		return vector;
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/apic/vector.c|263| <<assign_vector_locked>> apic_update_vector(irqd, vector, cpu);
+	 *   - arch/x86/kernel/apic/vector.c|348| <<assign_managed_vector>> apic_update_vector(irqd, vector, cpu);
+	 */
 	apic_update_vector(irqd, vector, cpu);
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/apic/vector.c|309| <<vector_assign_managed_shutdown>> apic_update_irq_cfg(irqd, MANAGED_IRQ_SHUTDOWN_VECTOR, cpu);
+	 *   - arch/x86/kernel/apic/vector.c|399| <<assign_vector_locked>> apic_update_irq_cfg(irqd, vector, cpu);
+	 *   - arch/x86/kernel/apic/vector.c|489| <<assign_managed_vector>> apic_update_irq_cfg(irqd, vector, cpu);
+	 *   - arch/x86/kernel/apic/vector.c|682| <<vector_configure_legacy>> apic_update_irq_cfg(irqd, apicd->vector, apicd->cpu);
+	 */
 	apic_update_irq_cfg(irqd, vector, cpu);
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|315| <<assign_irq_vector_policy>> return assign_irq_vector(irqd, info->mask);
+ */
 static int assign_irq_vector(struct irq_data *irqd, const struct cpumask *dest)
 {
 	unsigned long flags;
@@ -306,6 +877,10 @@ static int assign_irq_vector_any_locked(struct irq_data *irqd)
 	return assign_vector_locked(irqd, cpu_online_mask);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|599| <<x86_vector_alloc_irqs>> err = assign_irq_vector_policy(irqd, info);
+ */
 static int
 assign_irq_vector_policy(struct irq_data *irqd, struct irq_alloc_info *info)
 {
@@ -320,6 +895,11 @@ assign_irq_vector_policy(struct irq_data *irqd, struct irq_alloc_info *info)
 	return reserve_irq_vector(irqd);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|451| <<activate_managed>> ret = assign_managed_vector(irqd, vector_searchmask);
+ *   - arch/x86/kernel/apic/vector.c|892| <<apic_set_affinity>> err = assign_managed_vector(irqd, vector_searchmask);
+ */
 static int
 assign_managed_vector(struct irq_data *irqd, const struct cpumask *dest)
 {
@@ -342,6 +922,12 @@ assign_managed_vector(struct irq_data *irqd, const struct cpumask *dest)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|569| <<x86_vector_deactivate>> clear_irq_vector(irqd);
+ *   - arch/x86/kernel/apic/vector.c|693| <<x86_vector_free_irqs>> clear_irq_vector(irqd);
+ *   - arch/x86/kernel/apic/vector.c|724| <<vector_configure_legacy>> clear_irq_vector(irqd);
+ */
 static void clear_irq_vector(struct irq_data *irqd)
 {
 	struct apic_chip_data *apicd = apic_chip_data(irqd);
@@ -428,6 +1014,10 @@ static int activate_reserved(struct irq_data *irqd)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|645| <<x86_vector_activate>> ret = activate_managed(irqd);
+ */
 static int activate_managed(struct irq_data *irqd)
 {
 	const struct cpumask *dest = irq_data_get_affinity_mask(irqd);
@@ -452,6 +1042,9 @@ static int activate_managed(struct irq_data *irqd)
 	return ret;
 }
 
+/*
+ * struct irq_domain_ops x86_vector_domain_ops.activate = x86_vector_activate()
+ */
 static int x86_vector_activate(struct irq_domain *dom, struct irq_data *irqd,
 			       bool reserve)
 {
@@ -475,6 +1068,10 @@ static int x86_vector_activate(struct irq_domain *dom, struct irq_data *irqd,
 	return ret;
 }
 
+/*
+ * called by;
+ *   - arch/x86/kernel/apic/vector.c|648| <<x86_vector_free_irqs>> vector_free_reserved_and_managed(irqd);
+ */
 static void vector_free_reserved_and_managed(struct irq_data *irqd)
 {
 	const struct cpumask *dest = irq_data_get_affinity_mask(irqd);
@@ -489,6 +1086,11 @@ static void vector_free_reserved_and_managed(struct irq_data *irqd)
 		irq_matrix_remove_managed(vector_matrix, dest);
 }
 
+/*
+ * 在以下使用x86_vector_free_irqs():
+ *   - struct irq_domain_ops x86_vector_domain_ops.free = x86_vector_free_irqs()
+ *   - arch/x86/kernel/apic/vector.c|915| <<x86_vector_alloc_irqs>> x86_vector_free_irqs(domain, virq, i);
+ */
 static void x86_vector_free_irqs(struct irq_domain *domain,
 				 unsigned int virq, unsigned int nr_irqs)
 {
@@ -539,6 +1141,9 @@ static bool vector_configure_legacy(unsigned int virq, struct irq_data *irqd,
 	return realloc;
 }
 
+/*
+ * struct irq_domain_ops x86_vector_domain_ops.alloc = x86_vector_alloc_irqs()
+ */
 static int x86_vector_alloc_irqs(struct irq_domain *domain, unsigned int virq,
 				 unsigned int nr_irqs, void *arg)
 {
@@ -697,6 +1302,10 @@ static int x86_vector_select(struct irq_domain *d, struct irq_fwspec *fwspec,
 	return x86_fwspec_is_ioapic(fwspec) || x86_fwspec_is_hpet(fwspec);
 }
 
+/*
+ * 在以下使用x86_vector_domain_ops:
+ *   - arch/x86/kernel/apic/vector.c|961| <<arch_early_irq_init>> x86_vector_domain = irq_domain_create_tree(fn, &x86_vector_domain_ops, NULL);
+ */
 static const struct irq_domain_ops x86_vector_domain_ops = {
 	.select		= x86_vector_select,
 	.alloc		= x86_vector_alloc_irqs,
@@ -735,6 +1344,12 @@ int __init arch_probe_nr_irqs(void)
 	return legacy_pic->probe();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|907| <<lapic_update_legacy_vectors>> lapic_assign_legacy_vector(i, true);
+ *   - arch/x86/kernel/apic/vector.c|919| <<lapic_assign_system_vectors>> lapic_assign_legacy_vector(PIC_CASCADE_IR, false);
+ *   - arch/x86/kernel/i8259.c|120| <<make_8259A_irq>> lapic_assign_legacy_vector(irq, true);
+ */
 void lapic_assign_legacy_vector(unsigned int irq, bool replace)
 {
 	/*
@@ -765,6 +1380,10 @@ void __init lapic_update_legacy_vectors(void)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/irqinit.c|121| <<native_init_IRQ>> lapic_assign_system_vectors();
+ */
 void __init lapic_assign_system_vectors(void)
 {
 	unsigned int i, vector;
@@ -775,6 +1394,11 @@ void __init lapic_assign_system_vectors(void)
 	if (nr_legacy_irqs() > 1)
 		lapic_assign_legacy_vector(PIC_CASCADE_IR, false);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/apic/vector.c|922| <<lapic_assign_system_vectors>> irq_matrix_online(vector_matrix);
+	 *   - arch/x86/kernel/apic/vector.c|983| <<lapic_online>> irq_matrix_online(vector_matrix);
+	 */
 	/* System vectors are reserved, online it */
 	irq_matrix_online(vector_matrix);
 
@@ -816,6 +1440,10 @@ int __init arch_early_irq_init(void)
 
 #ifdef CONFIG_SMP
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|1288| <<lapic_online>> this_cpu_write(vector_irq[vector], __setup_vector_irq(vector));
+ */
 static struct irq_desc *__setup_vector_irq(int vector)
 {
 	int isairq = vector - ISA_IRQ_VECTOR(0);
@@ -829,6 +1457,10 @@ static struct irq_desc *__setup_vector_irq(int vector)
 	return irq_to_desc(isairq);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|326| <<start_secondary>> lapic_online();
+ */
 /* Online the local APIC infrastructure and initialize the vectors */
 void lapic_online(void)
 {
@@ -836,6 +1468,11 @@ void lapic_online(void)
 
 	lockdep_assert_held(&vector_lock);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/apic/vector.c|922| <<lapic_assign_system_vectors>> irq_matrix_online(vector_matrix);
+	 *   - arch/x86/kernel/apic/vector.c|983| <<lapic_online>> irq_matrix_online(vector_matrix);
+	 */
 	/* Online the vector matrix array for this CPU */
 	irq_matrix_online(vector_matrix);
 
@@ -854,12 +1491,23 @@ void lapic_online(void)
 
 static void __vector_cleanup(struct vector_cleanup *cl, bool check_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1447| <<cpu_disable_common>> lapic_offline();
+ */
 void lapic_offline(void)
 {
 	struct vector_cleanup *cl = this_cpu_ptr(&vector_cleanup);
 
 	lock_vector_lock();
 
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/apic/vector.c|1070| <<lapic_offline>> __vector_cleanup(cl, false);
+	 *   - arch/x86/kernel/apic/vector.c|1249| <<vector_cleanup_callback>> __vector_cleanup(cl, true);
+	 *
+	 * 核心思想是清理percpu的&vector_cleanup list
+	 */
 	/* In case the vector cleanup timer has not expired */
 	__vector_cleanup(cl, false);
 
@@ -870,6 +1518,48 @@ void lapic_offline(void)
 	unlock_vector_lock();
 }
 
+/*
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_setup_affinity
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] vp_find_vqs_msix
+ * [0] vp_find_vqs
+ * [0] vp_modern_find_vqs
+ * [0] init_vqs
+ * [0] virtnet_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] virtio_net_driver_init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] irq_matrix_alloc
+ * [0] assign_vector_locked
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_move_masked_irq
+ * [0] __irq_move_irq
+ * [0] apic_ack_edge
+ * [0] handle_edge_irq
+ * [0] __common_interrupt
+ * [0] common_interrupt
+ *
+ * 中断号是irq_data->irq
+ */
 static int apic_set_affinity(struct irq_data *irqd,
 			     const struct cpumask *dest, bool force)
 {
@@ -880,6 +1570,9 @@ static int apic_set_affinity(struct irq_data *irqd,
 
 	raw_spin_lock(&vector_lock);
 	cpumask_and(vector_searchmask, dest, cpu_online_mask);
+	/*
+	 * 注释Affinity is auto-managed by the kernel
+	 */
 	if (irqd_affinity_is_managed(irqd))
 		err = assign_managed_vector(irqd, vector_searchmask);
 	else
@@ -922,6 +1615,11 @@ static void x86_vector_msi_compose_msg(struct irq_data *data,
        __irq_msi_compose_msg(irqd_cfg(data), msg, false);
 }
 
+/*
+ * 在以下使用lapic_controller:
+ *   - arch/x86/kernel/apic/vector.c|933| <<global>> static struct irq_chip lapic_controller = {
+ *   - arch/x86/kernel/apic/vector.c|581| <<x86_vector_alloc_irqs>> irqd->chip = &lapic_controller;
+ */
 static struct irq_chip lapic_controller = {
 	.name			= "APIC",
 	.irq_ack		= apic_ack_edge,
@@ -932,6 +1630,11 @@ static struct irq_chip lapic_controller = {
 
 #ifdef CONFIG_SMP
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|1232| <<__vector_cleanup>> free_moved_vector(apicd);
+ *   - arch/x86/kernel/apic/vector.c|1401| <<irq_force_complete_move>> free_moved_vector(apicd);
+ */
 static void free_moved_vector(struct apic_chip_data *apicd)
 {
 	unsigned int vector = apicd->prev_vector;
@@ -949,13 +1652,38 @@ static void free_moved_vector(struct apic_chip_data *apicd)
 	 *    affinity mask comes online.
 	 */
 	trace_vector_free_moved(apicd->irq, cpu, vector, managed);
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/apic/vector.c|273| <<apic_update_vector>> irq_matrix_free(vector_matrix, apicd->cpu, apicd->vector, managed);
+	 *   - arch/x86/kernel/apic/vector.c|503| <<clear_irq_vector>> irq_matrix_free(vector_matrix, apicd->cpu, vector, managed);
+	 *   - arch/x86/kernel/apic/vector.c|512| <<clear_irq_vector>> irq_matrix_free(vector_matrix, apicd->prev_cpu, vector, managed);
+	 *   - arch/x86/kernel/apic/vector.c|1100| <<free_moved_vector>> irq_matrix_free(vector_matrix, cpu, vector, managed);
+	 */
 	irq_matrix_free(vector_matrix, cpu, vector, managed);
 	per_cpu(vector_irq, cpu)[vector] = VECTOR_UNUSED;
 	hlist_del_init(&apicd->clist);
 	apicd->prev_vector = 0;
+	/*
+	 * 在以下设置apic_chip_data->move_in_progress:
+	 *   - arch/x86/kernel/apic/vector.c|278| <<apic_update_vector>> apicd->move_in_progress = true;
+	 *   - arch/x86/kernel/apic/vector.c|549| <<clear_irq_vector>> apicd->move_in_progress = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1205| <<free_moved_vector>> apicd->move_in_progress = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1268| <<__vector_schedule_cleanup>> apicd->move_in_progress = 0;
+	 * 在以下使用apic_chip_data->move_in_progress:
+	 *   - arch/x86/kernel/apic/vector.c|405| <<assign_vector_locked>> if (apicd->move_in_progress || !hlist_unhashed(&apicd->clist))
+	 *   - arch/x86/kernel/apic/vector.c|842| <<x86_vector_debug_show>> seq_printf(m, "%*smove_in_progress: %u\n", ind, "", apicd.move_in_progress ? 1 : 0);
+	 *   - arch/x86/kernel/apic/vector.c|1303| <<vector_schedule_cleanup>> if (apicd->move_in_progress)
+	 *   - arch/x86/kernel/apic/vector.c|1312| <<irq_complete_move>> if (likely(!apicd->move_in_progress))
+	 *   - arch/x86/kernel/apic/vector.c|1379| <<irq_force_complete_move>> if (apicd->move_in_progress) {
+	 */
 	apicd->move_in_progress = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|1070| <<lapic_offline>> __vector_cleanup(cl, false);
+ *   - arch/x86/kernel/apic/vector.c|1249| <<vector_cleanup_callback>> __vector_cleanup(cl, true);
+ */
 static void __vector_cleanup(struct vector_cleanup *cl, bool check_irr)
 {
 	struct apic_chip_data *apicd;
@@ -985,6 +1713,11 @@ static void __vector_cleanup(struct vector_cleanup *cl, bool check_irr)
 			rearm = true;
 			continue;
 		}
+		/*
+		 * called by:
+		 *   - arch/x86/kernel/apic/vector.c|1232| <<__vector_cleanup>> free_moved_vector(apicd);
+		 *   - arch/x86/kernel/apic/vector.c|1401| <<irq_force_complete_move>> free_moved_vector(apicd);
+		 */
 		free_moved_vector(apicd);
 	}
 
@@ -996,6 +1729,17 @@ static void __vector_cleanup(struct vector_cleanup *cl, bool check_irr)
 		mod_timer(&cl->timer, jiffies + 1);
 }
 
+/*
+ * 在以下使用percpu的vector_cleanup:
+ *   - arch/x86/kernel/apic/vector.c|55| <<global>> static DEFINE_PER_CPU(struct vector_cleanup, vector_cleanup) = {
+ *   - arch/x86/kernel/apic/vector.c|867| <<lapic_offline>> struct vector_cleanup *cl = this_cpu_ptr(&vector_cleanup);
+ *   - arch/x86/kernel/apic/vector.c|1024| <<__vector_schedule_cleanup>> struct vector_cleanup *cl = per_cpu_ptr(&vector_cleanup, cpu);
+ *
+ * static DEFINE_PER_CPU(struct vector_cleanup, vector_cleanup) = {
+ *     .head   = HLIST_HEAD_INIT,
+ *     .timer  = __TIMER_INITIALIZER(vector_cleanup_callback, TIMER_PINNED),
+ * };
+ */
 static void vector_cleanup_callback(struct timer_list *tmr)
 {
 	struct vector_cleanup *cl = container_of(tmr, typeof(*cl), timer);
@@ -1006,15 +1750,71 @@ static void vector_cleanup_callback(struct timer_list *tmr)
 	raw_spin_unlock_irq(&vector_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|1328| <<vector_schedule_cleanup>> __vector_schedule_cleanup(apicd);
+ *   - arch/x86/kernel/apic/vector.c|1346| <<irq_complete_move>> __vector_schedule_cleanup(apicd);
+ */
 static void __vector_schedule_cleanup(struct apic_chip_data *apicd)
 {
+	/*
+	 * 在以下设置apic_chip_data->prev_cpu:
+	 *   - arch/x86/kernel/apic/vector.c|389| <<apic_update_vector>> apicd->prev_cpu = apicd->cpu;
+	 * 在以下使用apic_chip_data->prev_cpu:
+	 *   - arch/x86/kernel/apic/vector.c|657| <<clear_irq_vector>> trace_vector_clear(irqd->irq, vector, apicd->cpu, apicd->prev_vector, apicd->prev_cpu);
+	 *   - arch/x86/kernel/apic/vector.c|668| <<clear_irq_vector>> per_cpu(vector_irq, apicd->prev_cpu)[vector] = VECTOR_SHUTDOWN;
+	 *   - arch/x86/kernel/apic/vector.c|669| <<clear_irq_vector>> irq_matrix_free(vector_matrix, apicd->prev_cpu, vector, managed);
+	 *   - arch/x86/kernel/apic/vector.c|967| <<x86_vector_debug_show>> seq_printf(m, "%*sPrevious target: %5u\n", ind, "", apicd.prev_cpu);
+	 *   - arch/x86/kernel/apic/vector.c|1318| <<free_moved_vector>> unsigned int cpu = apicd->prev_cpu;
+	 *   - arch/x86/kernel/apic/vector.c|1432| <<__vector_schedule_cleanup>> unsigned int cpu = apicd->prev_cpu;
+	 */
 	unsigned int cpu = apicd->prev_cpu;
 
 	raw_spin_lock(&vector_lock);
+	/*
+	 * 在以下设置apic_chip_data->move_in_progress:
+	 *   - arch/x86/kernel/apic/vector.c|278| <<apic_update_vector>> apicd->move_in_progress = true;
+	 *   - arch/x86/kernel/apic/vector.c|549| <<clear_irq_vector>> apicd->move_in_progress = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1205| <<free_moved_vector>> apicd->move_in_progress = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1268| <<__vector_schedule_cleanup>> apicd->move_in_progress = 0;
+	 * 在以下使用apic_chip_data->move_in_progress:
+	 *   - arch/x86/kernel/apic/vector.c|405| <<assign_vector_locked>> if (apicd->move_in_progress || !hlist_unhashed(&apicd->clist))
+	 *   - arch/x86/kernel/apic/vector.c|842| <<x86_vector_debug_show>> seq_printf(m, "%*smove_in_progress: %u\n", ind, "", apicd.move_in_progress ? 1 : 0);
+	 *   - arch/x86/kernel/apic/vector.c|1303| <<vector_schedule_cleanup>> if (apicd->move_in_progress)
+	 *   - arch/x86/kernel/apic/vector.c|1312| <<irq_complete_move>> if (likely(!apicd->move_in_progress))
+	 *   - arch/x86/kernel/apic/vector.c|1379| <<irq_force_complete_move>> if (apicd->move_in_progress) {
+	 */
 	apicd->move_in_progress = 0;
+	/*
+	 * 查看cpu_disable_common()
+	 */
 	if (cpu_online(cpu)) {
+		/*
+		 * 在以下使用percpu的vector_cleanup:
+		 *   - arch/x86/kernel/apic/vector.c|55| <<global>> static DEFINE_PER_CPU(struct vector_cleanup, vector_cleanup) = {
+		 *   - arch/x86/kernel/apic/vector.c|867| <<lapic_offline>> struct vector_cleanup *cl = this_cpu_ptr(&vector_cleanup);
+		 *   - arch/x86/kernel/apic/vector.c|1024| <<__vector_schedule_cleanup>> struct vector_cleanup *cl = per_cpu_ptr(&vector_cleanup, cpu);
+		 *
+		 * struct vector_cleanup {
+		 *     struct hlist_head head;
+		 *     struct timer_list timer;
+		 * };
+		 */
 		struct vector_cleanup *cl = per_cpu_ptr(&vector_cleanup, cpu);
 
+		/*
+		 * 在以下使用apic_chip_data->clist:
+		 *   - arch/x86/kernel/apic/vector.c|296| <<alloc_apic_chip_data>> INIT_HLIST_NODE(&apicd->clist);
+		 *   - arch/x86/kernel/apic/vector.c|511| <<assign_vector_locked>> if (apicd->move_in_progress || !hlist_unhashed(&apicd->clist))
+		 *   - arch/x86/kernel/apic/vector.c|662| <<clear_irq_vector>> hlist_del_init(&apicd->clist);
+		 *   - arch/x86/kernel/apic/vector.c|963| <<x86_vector_debug_show>> seq_printf(m, "%*scleanup_pending: %u\n", ind, "", !hlist_unhashed(&apicd.clist));
+		 *   - arch/x86/kernel/apic/vector.c|1331| <<free_moved_vector>> hlist_del_init(&apicd->clist);
+		 *   - arch/x86/kernel/apic/vector.c|1362| <<__vector_cleanup>> hlist_for_each_entry_safe(apicd, tmp, &cl->head, clist) {
+		 *   - arch/x86/kernel/apic/vector.c|1429| <<__vector_schedule_cleanup>> hlist_add_head(&apicd->clist, &cl->head);
+		 *
+		 * struct apic_chip_data *apicd:
+		 * -> struct hlist_node       clist;
+		 */
 		hlist_add_head(&apicd->clist, &cl->head);
 
 		/*
@@ -1033,14 +1833,41 @@ static void __vector_schedule_cleanup(struct apic_chip_data *apicd)
 		 */
 		if (!timer_pending(&cl->timer)) {
 			cl->timer.expires = jiffies + 1;
+			/*
+			 * timer在prev_cpu
+			 */
 			add_timer_on(&cl->timer, cpu);
 		}
 	} else {
+		/*
+		 * 在以下设置apic_chip_data->prev_vector:
+		 *   - arch/x86/kernel/apic/vector.c|341| <<apic_update_vector>> apicd->prev_vector = 0;
+		 *   - arch/x86/kernel/apic/vector.c|354| <<apic_update_vector>> apicd->prev_vector = apicd->vector;
+		 *   - arch/x86/kernel/apic/vector.c|636| <<clear_irq_vector>> apicd->prev_vector = 0;
+		 *   - arch/x86/kernel/apic/vector.c|1292| <<free_moved_vector>> apicd->prev_vector = 0;
+		 *   - arch/x86/kernel/apic/vector.c|1397| <<__vector_schedule_cleanup>> apicd->prev_vector = 0;
+		 * 在以下使用apic_chip_data->prev_vector:
+		 *   - arch/x86/kernel/apic/vector.c|622| <<clear_irq_vector>> trace_vector_clear(irqd->irq, vector, apicd->cpu, apicd->prev_vector,
+		 *   - arch/x86/kernel/apic/vector.c|630| <<clear_irq_vector>> vector = apicd->prev_vector;
+		 *   - arch/x86/kernel/apic/vector.c|926| <<x86_vector_debug_show>> if (apicd.prev_vector) {
+		 *   - arch/x86/kernel/apic/vector.c|927| <<x86_vector_debug_show>> seq_printf(m, "%*sPrevious vector: %5u\n", ind, "", apicd.prev_vector);
+		 *   - arch/x86/kernel/apic/vector.c|1274| <<free_moved_vector>> unsigned int vector = apicd->prev_vector;
+		 *   - arch/x86/kernel/apic/vector.c|1310| <<__vector_cleanup>> unsigned int irr, vector = apicd->prev_vector;
+		 *   - arch/x86/kernel/apic/vector.c|1477| <<irq_force_complete_move>> vector = apicd->prev_vector;
+		 */
 		apicd->prev_vector = 0;
 	}
 	raw_spin_unlock(&vector_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/platform/uv/uv_irq.c|61| <<uv_set_irq_affinity>> vector_schedule_cleanup(cfg);
+ *   - drivers/iommu/amd/iommu.c|3735| <<amd_ir_set_affinity>> vector_schedule_cleanup(cfg);
+ *   - drivers/iommu/hyperv-iommu.c|54| <<hyperv_ir_set_affinity>> vector_schedule_cleanup(cfg);
+ *   - drivers/iommu/hyperv-iommu.c|260| <<hyperv_root_ir_set_affinity>> vector_schedule_cleanup(cfg);
+ *   - drivers/iommu/intel/irq_remapping.c|1179| <<intel_ir_set_affinity>> vector_schedule_cleanup(cfg);
+ */
 void vector_schedule_cleanup(struct irq_cfg *cfg)
 {
 	struct apic_chip_data *apicd;
@@ -1050,11 +1877,31 @@ void vector_schedule_cleanup(struct irq_cfg *cfg)
 		__vector_schedule_cleanup(apicd);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/io_apic.c|1784| <<ioapic_ack_level>> irq_complete_move(cfg);
+ *   - arch/x86/kernel/apic/vector.c|1178| <<apic_ack_edge>> irq_complete_move(irqd_cfg(irqd));
+ */
 void irq_complete_move(struct irq_cfg *cfg)
 {
 	struct apic_chip_data *apicd;
 
 	apicd = container_of(cfg, struct apic_chip_data, hw_irq_cfg);
+	/*
+	 * 在以下设置apic_chip_data->move_in_progress:
+	 *   - arch/x86/kernel/apic/vector.c|278| <<apic_update_vector>> apicd->move_in_progress = true;
+	 *   - arch/x86/kernel/apic/vector.c|549| <<clear_irq_vector>> apicd->move_in_progress = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1205| <<free_moved_vector>> apicd->move_in_progress = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1268| <<__vector_schedule_cleanup>> apicd->move_in_progress = 0;
+	 * 在以下使用apic_chip_data->move_in_progress:
+	 *   - arch/x86/kernel/apic/vector.c|405| <<assign_vector_locked>> if (apicd->move_in_progress || !hlist_unhashed(&apicd->clist))
+	 *   - arch/x86/kernel/apic/vector.c|842| <<x86_vector_debug_show>> seq_printf(m, "%*smove_in_progress: %u\n", ind, "", apicd.move_in_progress ? 1 : 0);
+	 *   - arch/x86/kernel/apic/vector.c|1303| <<vector_schedule_cleanup>> if (apicd->move_in_progress)
+	 *   - arch/x86/kernel/apic/vector.c|1312| <<irq_complete_move>> if (likely(!apicd->move_in_progress))
+	 *   - arch/x86/kernel/apic/vector.c|1379| <<irq_force_complete_move>> if (apicd->move_in_progress) {
+	 *
+	 * 这里会提前退出
+	 */
 	if (likely(!apicd->move_in_progress))
 		return;
 
@@ -1071,6 +1918,10 @@ void irq_complete_move(struct irq_cfg *cfg)
 /*
  * Called from fixup_irqs() with @desc->lock held and interrupts disabled.
  */
+/*
+ * called by:
+ *   - kernel/irq/cpuhotplug.c|100| <<migrate_one_irq>> irq_force_complete_move(desc);
+ */
 void irq_force_complete_move(struct irq_desc *desc)
 {
 	struct apic_chip_data *apicd;
@@ -1096,6 +1947,22 @@ void irq_force_complete_move(struct irq_desc *desc)
 	if (!apicd)
 		goto unlock;
 
+	/*
+	 * 在以下设置apic_chip_data->prev_vector:
+	 *   - arch/x86/kernel/apic/vector.c|341| <<apic_update_vector>> apicd->prev_vector = 0;
+	 *   - arch/x86/kernel/apic/vector.c|354| <<apic_update_vector>> apicd->prev_vector = apicd->vector;
+	 *   - arch/x86/kernel/apic/vector.c|636| <<clear_irq_vector>> apicd->prev_vector = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1292| <<free_moved_vector>> apicd->prev_vector = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1397| <<__vector_schedule_cleanup>> apicd->prev_vector = 0;
+	 * 在以下使用apic_chip_data->prev_vector:
+	 *   - arch/x86/kernel/apic/vector.c|622| <<clear_irq_vector>> trace_vector_clear(irqd->irq, vector, apicd->cpu, apicd->prev_vector,
+	 *   - arch/x86/kernel/apic/vector.c|630| <<clear_irq_vector>> vector = apicd->prev_vector;
+	 *   - arch/x86/kernel/apic/vector.c|926| <<x86_vector_debug_show>> if (apicd.prev_vector) {
+	 *   - arch/x86/kernel/apic/vector.c|927| <<x86_vector_debug_show>> seq_printf(m, "%*sPrevious vector: %5u\n", ind, "", apicd.prev_vector);
+	 *   - arch/x86/kernel/apic/vector.c|1274| <<free_moved_vector>> unsigned int vector = apicd->prev_vector;
+	 *   - arch/x86/kernel/apic/vector.c|1310| <<__vector_cleanup>> unsigned int irr, vector = apicd->prev_vector;
+	 *   - arch/x86/kernel/apic/vector.c|1477| <<irq_force_complete_move>> vector = apicd->prev_vector;
+	 */
 	/*
 	 * If prev_vector is empty, no action required.
 	 */
@@ -1118,6 +1985,19 @@ void irq_force_complete_move(struct irq_desc *desc)
 	 * 2) The interrupt has fired on the new vector, but the cleanup IPIs
 	 *    have not been processed yet.
 	 */
+	/*
+	 * 在以下设置apic_chip_data->move_in_progress:
+	 *   - arch/x86/kernel/apic/vector.c|278| <<apic_update_vector>> apicd->move_in_progress = true;
+	 *   - arch/x86/kernel/apic/vector.c|549| <<clear_irq_vector>> apicd->move_in_progress = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1205| <<free_moved_vector>> apicd->move_in_progress = 0;
+	 *   - arch/x86/kernel/apic/vector.c|1268| <<__vector_schedule_cleanup>> apicd->move_in_progress = 0;
+	 * 在以下使用apic_chip_data->move_in_progress:
+	 *   - arch/x86/kernel/apic/vector.c|405| <<assign_vector_locked>> if (apicd->move_in_progress || !hlist_unhashed(&apicd->clist))
+	 *   - arch/x86/kernel/apic/vector.c|842| <<x86_vector_debug_show>> seq_printf(m, "%*smove_in_progress: %u\n", ind, "", apicd.move_in_progress ? 1 : 0);
+	 *   - arch/x86/kernel/apic/vector.c|1303| <<vector_schedule_cleanup>> if (apicd->move_in_progress)
+	 *   - arch/x86/kernel/apic/vector.c|1312| <<irq_complete_move>> if (likely(!apicd->move_in_progress))
+	 *   - arch/x86/kernel/apic/vector.c|1379| <<irq_force_complete_move>> if (apicd->move_in_progress) {
+	 */
 	if (apicd->move_in_progress) {
 		/*
 		 * In theory there is a race:
@@ -1154,6 +2034,11 @@ void irq_force_complete_move(struct irq_desc *desc)
 		pr_warn("IRQ fixup: irq %d move in progress, old vector %d\n",
 			irqd->irq, vector);
 	}
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/apic/vector.c|1232| <<__vector_cleanup>> free_moved_vector(apicd);
+	 *   - arch/x86/kernel/apic/vector.c|1401| <<irq_force_complete_move>> free_moved_vector(apicd);
+	 */
 	free_moved_vector(apicd);
 unlock:
 	raw_spin_unlock(&vector_lock);
@@ -1164,6 +2049,10 @@ void irq_force_complete_move(struct irq_desc *desc)
  * Note, this is not accurate accounting, but at least good enough to
  * prevent that the actual interrupt move will run out of vectors.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1449| <<native_cpu_disable>> ret = lapic_can_unplug_cpu();
+ */
 int lapic_can_unplug_cpu(void)
 {
 	unsigned int rsvd, avl, tomove, cpu = smp_processor_id();
@@ -1309,6 +2198,10 @@ static void __init print_local_APIC(void *dummy)
 	pr_cont("\n");
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|1851| <<print_ICs>> print_local_APICs(show_lapic);
+ */
 static void __init print_local_APICs(int maxcpu)
 {
 	int cpu;
diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c
index 11761c124..ccc461f2c 100644
--- a/arch/x86/kernel/irq.c
+++ b/arch/x86/kernel/irq.c
@@ -263,6 +263,25 @@ DEFINE_IDTENTRY_IRQ(common_interrupt)
 					     __func__, smp_processor_id(),
 					     vector);
 		} else {
+			/*
+			 * 在以下使用VECTOR_UNUSED:
+			 *   - arch/x86/kernel/irqinit.c|71| <<global>> [0 ... NR_VECTORS - 1] = VECTOR_UNUSED,
+			 *   - arch/x86/kernel/apic/vector.c|1261| <<__setup_vector_irq>> return VECTOR_UNUSED;
+			 *   - arch/x86/kernel/apic/vector.c|1264| <<__setup_vector_irq>> return VECTOR_UNUSED;
+			 *   - arch/x86/kernel/apic/vector.c|1443| <<free_moved_vector>> per_cpu(vector_irq, cpu)[vector] = VECTOR_UNUSED;
+			 *   - arch/x86/kernel/irq.c|261| <<DEFINE_IDTENTRY_IRQ(common_interrupt)>> if (desc == VECTOR_UNUSED) {
+			 *   - arch/x86/kernel/irq.c|266| <<DEFINE_IDTENTRY_IRQ(common_interrupt)>> __this_cpu_write(vector_irq[vector], VECTOR_UNUSED);
+			 *   - arch/x86/kernel/irq.c|387| <<fixup_irqs>> __this_cpu_write(vector_irq[vector], VECTOR_UNUSED);
+			 *
+			 * 在以下使用VECTOR_SHUTDOWN:
+			 *   - arch/x86/kernel/apic/vector.c|762| <<clear_irq_vector>> per_cpu(vector_irq, apicd->cpu)[vector] = VECTOR_SHUTDOWN;
+			 *   - arch/x86/kernel/apic/vector.c|771| <<clear_irq_vector>> per_cpu(vector_irq, apicd->prev_cpu)[vector] = VECTOR_SHUTDOWN;
+			 *
+			 * 在以下使用VECTOR_RETRIGGERED:
+			 *   - arch/x86/kernel/apic/msi.c|112| <<msi_set_affinity>> this_cpu_write(vector_irq[cfg->vector], VECTOR_RETRIGGERED);
+			 *   - arch/x86/kernel/irq.c|382| <<fixup_irqs>> __this_cpu_write(vector_irq[vector], VECTOR_RETRIGGERED);
+			 *   - arch/x86/kernel/irq.c|386| <<fixup_irqs>> if (__this_cpu_read(vector_irq[vector]) != VECTOR_RETRIGGERED)
+			 */
 			__this_cpu_write(vector_irq[vector], VECTOR_UNUSED);
 		}
 	}
@@ -336,6 +355,10 @@ DEFINE_IDTENTRY_SYSVEC_SIMPLE(sysvec_kvm_posted_intr_nested_ipi)
 
 
 #ifdef CONFIG_HOTPLUG_CPU
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1441| <<cpu_disable_common>> fixup_irqs();
+ */
 /* A cpu has been removed from cpu_online_mask.  Reset irq affinities. */
 void fixup_irqs(void)
 {
@@ -363,6 +386,9 @@ void fixup_irqs(void)
 	 * nothing else will touch it.
 	 */
 	for (vector = FIRST_EXTERNAL_VECTOR; vector < NR_VECTORS; vector++) {
+		/*
+		 * 如果vector_irq[]是空, continue!!
+		 */
 		if (IS_ERR_OR_NULL(__this_cpu_read(vector_irq[vector])))
 			continue;
 
@@ -375,6 +401,25 @@ void fixup_irqs(void)
 			chip = irq_data_get_irq_chip(data);
 			if (chip->irq_retrigger) {
 				chip->irq_retrigger(data);
+				/*
+				 * 在以下使用VECTOR_UNUSED:
+				 *   - arch/x86/kernel/irqinit.c|71| <<global>> [0 ... NR_VECTORS - 1] = VECTOR_UNUSED,
+				 *   - arch/x86/kernel/apic/vector.c|1261| <<__setup_vector_irq>> return VECTOR_UNUSED;
+				 *   - arch/x86/kernel/apic/vector.c|1264| <<__setup_vector_irq>> return VECTOR_UNUSED;
+				 *   - arch/x86/kernel/apic/vector.c|1443| <<free_moved_vector>> per_cpu(vector_irq, cpu)[vector] = VECTOR_UNUSED;
+				 *   - arch/x86/kernel/irq.c|261| <<DEFINE_IDTENTRY_IRQ(common_interrupt)>> if (desc == VECTOR_UNUSED) {
+				 *   - arch/x86/kernel/irq.c|266| <<DEFINE_IDTENTRY_IRQ(common_interrupt)>> __this_cpu_write(vector_irq[vector], VECTOR_UNUSED);
+				 *   - arch/x86/kernel/irq.c|387| <<fixup_irqs>> __this_cpu_write(vector_irq[vector], VECTOR_UNUSED);
+				 *
+				 * 在以下使用VECTOR_SHUTDOWN:
+				 *   - arch/x86/kernel/apic/vector.c|762| <<clear_irq_vector>> per_cpu(vector_irq, apicd->cpu)[vector] = VECTOR_SHUTDOWN;
+				 *   - arch/x86/kernel/apic/vector.c|771| <<clear_irq_vector>> per_cpu(vector_irq, apicd->prev_cpu)[vector] = VECTOR_SHUTDOWN;
+				 *
+				 * 在以下使用VECTOR_RETRIGGERED:
+				 *   - arch/x86/kernel/apic/msi.c|112| <<msi_set_affinity>> this_cpu_write(vector_irq[cfg->vector], VECTOR_RETRIGGERED);
+				 *   - arch/x86/kernel/irq.c|382| <<fixup_irqs>> __this_cpu_write(vector_irq[vector], VECTOR_RETRIGGERED);
+				 *   - arch/x86/kernel/irq.c|386| <<fixup_irqs>> if (__this_cpu_read(vector_irq[vector]) != VECTOR_RETRIGGERED)
+				 */
 				__this_cpu_write(vector_irq[vector], VECTOR_RETRIGGERED);
 			}
 			raw_spin_unlock(&desc->lock);
diff --git a/arch/x86/kernel/irqinit.c b/arch/x86/kernel/irqinit.c
index c68366687..b9209787e 100644
--- a/arch/x86/kernel/irqinit.c
+++ b/arch/x86/kernel/irqinit.c
@@ -46,6 +46,27 @@
  * (these are usually mapped into the 0x30-0xff vector range)
  */
 
+/*
+ * 在以下使用percpu的vector_irq:
+ *   - arch/x86/include/asm/hw_irq.h|131| <<global>> DECLARE_PER_CPU(vector_irq_t, vector_irq);
+ *   - arch/x86/kernel/irqinit.c|49| <<global>> DEFINE_PER_CPU(vector_irq_t, vector_irq) = {
+ *   - arch/x86/kernel/apic/msi.c|111| <<msi_set_affinity>> if (IS_ERR_OR_NULL(this_cpu_read(vector_irq[cfg->vector])))
+ *   - arch/x86/kernel/apic/msi.c|112| <<msi_set_affinity>> this_cpu_write(vector_irq[cfg->vector], VECTOR_RETRIGGERED);
+ *   - arch/x86/kernel/apic/vector.c|184| <<apic_update_vector>> BUG_ON(!IS_ERR_OR_NULL(per_cpu(vector_irq, newcpu)[newvec]));
+ *   - arch/x86/kernel/apic/vector.c|185| <<apic_update_vector>> per_cpu(vector_irq, newcpu)[newvec] = desc;
+ *   - arch/x86/kernel/apic/vector.c|367| <<clear_irq_vector>> per_cpu(vector_irq, apicd->cpu)[vector] = VECTOR_SHUTDOWN;
+ *   - arch/x86/kernel/apic/vector.c|376| <<clear_irq_vector>> per_cpu(vector_irq, apicd->prev_cpu)[vector] = VECTOR_SHUTDOWN;
+ *   - arch/x86/kernel/apic/vector.c|860| <<lapic_online>> this_cpu_write(vector_irq[vector], __setup_vector_irq(vector));
+ *   - arch/x86/kernel/apic/vector.c|961| <<free_moved_vector>> per_cpu(vector_irq, cpu)[vector] = VECTOR_UNUSED;
+ *   - arch/x86/kernel/irq.c|255| <<DEFINE_IDTENTRY_IRQ(common_interrupt)>> desc = __this_cpu_read(vector_irq[vector]);
+ *   - arch/x86/kernel/irq.c|266| <<DEFINE_IDTENTRY_IRQ(common_interrupt)>> __this_cpu_write(vector_irq[vector], VECTOR_UNUSED);
+ *   - arch/x86/kernel/irq.c|366| <<fixup_irqs>> if (IS_ERR_OR_NULL(__this_cpu_read(vector_irq[vector])))
+ *   - arch/x86/kernel/irq.c|371| <<fixup_irqs>> desc = __this_cpu_read(vector_irq[vector]);
+ *   - arch/x86/kernel/irq.c|378| <<fixup_irqs>> __this_cpu_write(vector_irq[vector], VECTOR_RETRIGGERED);
+ *   - arch/x86/kernel/irq.c|382| <<fixup_irqs>> if (__this_cpu_read(vector_irq[vector]) != VECTOR_RETRIGGERED)
+ *   - arch/x86/kernel/irq.c|383| <<fixup_irqs>> __this_cpu_write(vector_irq[vector], VECTOR_UNUSED);
+ *   - arch/x86/kernel/irqinit.c|87| <<init_IRQ>> per_cpu(vector_irq, 0)[ISA_IRQ_VECTOR(i)] = irq_to_desc(i);
+ */
 DEFINE_PER_CPU(vector_irq_t, vector_irq) = {
 	[0 ... NR_VECTORS - 1] = VECTOR_UNUSED,
 };
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 5bb395551..f6dedb460 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -58,6 +58,10 @@ EXPORT_PER_CPU_SYMBOL_GPL(hv_clock_per_cpu);
  * have elapsed since the hypervisor wrote the data. So we try to account for
  * that with system time
  */
+/*
+ * 在以下使用kvm_get_wallclock():
+ *   - arch/x86/kernel/kvmclock.c|325| <<kvmclock_init>> x86_platform.get_wallclock = kvm_get_wallclock;
+ */
 static void kvm_get_wallclock(struct timespec64 *now)
 {
 	wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
@@ -164,6 +168,12 @@ struct clocksource kvm_clock = {
 };
 EXPORT_SYMBOL_GPL(kvm_clock);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|186| <<kvm_restore_sched_clock_state>> kvm_register_clock("primary cpu clock, resume");
+ *   - arch/x86/kernel/kvmclock.c|192| <<kvm_setup_secondary_clock>> kvm_register_clock("secondary cpu clock");
+ *   - arch/x86/kernel/kvmclock.c|314| <<kvmclock_init>> kvm_register_clock("primary cpu clock");
+ */
 static void kvm_register_clock(char *txt)
 {
 	struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
@@ -187,6 +197,10 @@ static void kvm_restore_sched_clock_state(void)
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC
+/*
+ * 在以下使用kvm_setup_secondary_clock():
+ *   - arch/x86/kernel/kvmclock.c|328| <<kvmclock_init>> x86_cpuinit.early_percpu_clock_init = kvm_setup_secondary_clock;
+ */
 static void kvm_setup_secondary_clock(void)
 {
 	kvm_register_clock("secondary cpu clock");
@@ -199,6 +213,10 @@ void kvmclock_disable(void)
 		native_write_msr(msr_kvm_system_time, 0, 0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|246| <<kvm_setup_vsyscall_timeinfo>> kvmclock_init_mem();
+ */
 static void __init kvmclock_init_mem(void)
 {
 	unsigned long ncpus;
@@ -261,6 +279,10 @@ static int __init kvm_setup_vsyscall_timeinfo(void)
 }
 early_initcall(kvm_setup_vsyscall_timeinfo);
 
+/*
+ * 在以下使用kvmclock_setup_percpu():
+ *   - arch/x86/kernel/kvmclock.c|306| <<kvmclock_init>> if (cpuhp_setup_state(CPUHP_BP_PREPARE_DYN, "kvmclock:setup_percpu", kvmclock_setup_percpu, NULL) < 0) {
+ */
 static int kvmclock_setup_percpu(unsigned int cpu)
 {
 	struct pvclock_vsyscall_time_info *p = per_cpu(hv_clock_per_cpu, cpu);
@@ -285,6 +307,10 @@ static int kvmclock_setup_percpu(unsigned int cpu)
 	return p ? 0 : -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|980| <<kvm_init_platform>> kvmclock_init();
+ */
 void __init kvmclock_init(void)
 {
 	u8 flags;
diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c
index 5358d4388..842be1613 100644
--- a/arch/x86/kernel/paravirt.c
+++ b/arch/x86/kernel/paravirt.c
@@ -76,6 +76,13 @@ static u64 native_steal_clock(int cpu)
 DEFINE_STATIC_CALL(pv_steal_clock, native_steal_clock);
 DEFINE_STATIC_CALL(pv_sched_clock, native_sched_clock);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/vmware.c|340| <<vmware_paravirt_ops_setup>> paravirt_set_sched_clock(vmware_sched_clock);
+ *   - arch/x86/kernel/kvmclock.c|103| <<kvm_sched_clock_init>> paravirt_set_sched_clock(kvm_sched_clock_read);
+ *   - arch/x86/xen/time.c|567| <<xen_init_time_common>> paravirt_set_sched_clock(xen_sched_clock);
+ *   - drivers/clocksource/hyperv_timer.c|514| <<hv_setup_sched_clock>> paravirt_set_sched_clock(sched_clock);
+ */
 void paravirt_set_sched_clock(u64 (*func)(void))
 {
 	static_call_update(pv_sched_clock, func);
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index b3f81379c..b99f13368 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -44,6 +44,13 @@ void pvclock_touch_watchdogs(void)
 	reset_hung_task_detector();
 }
 
+/*
+ * 在以下使用last_value:
+ *   - arch/x86/kernel/pvclock.c|47| <<global>> static atomic64_t last_value = ATOMIC64_INIT(0);
+ *   - arch/x86/kernel/pvclock.c|51| <<pvclock_resume>> atomic64_set(&last_value, 0);
+ *   - arch/x86/kernel/pvclock.c|109| <<__pvclock_clocksource_read>> last = raw_atomic64_read(&last_value);
+ *   - arch/x86/kernel/pvclock.c|113| <<__pvclock_clocksource_read>> } while (!raw_atomic64_try_cmpxchg(&last_value, &last, ret));
+ */
 static atomic64_t last_value = ATOMIC64_INIT(0);
 
 void pvclock_resume(void)
@@ -64,6 +71,11 @@ u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src)
 	return flags & valid_flags;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/pvclock.c|115| <<pvclock_clocksource_read>> return __pvclock_clocksource_read(src, true);
+ *   - arch/x86/kernel/pvclock.c|120| <<pvclock_clocksource_read_nowd>> return __pvclock_clocksource_read(src, false);
+ */
 static __always_inline
 u64 __pvclock_clocksource_read(struct pvclock_vcpu_time_info *src, bool dowd)
 {
@@ -101,10 +113,24 @@ u64 __pvclock_clocksource_read(struct pvclock_vcpu_time_info *src, bool dowd)
 	 * updating at the same time, and one of them could be slightly behind,
 	 * making the assumption that last_value always go forward fail to hold.
 	 */
+	/*
+	 * 在以下使用last_value:
+	 *   - arch/x86/kernel/pvclock.c|47| <<global>> static atomic64_t last_value = ATOMIC64_INIT(0);
+	 *   - arch/x86/kernel/pvclock.c|51| <<pvclock_resume>> atomic64_set(&last_value, 0);
+	 *   - arch/x86/kernel/pvclock.c|109| <<__pvclock_clocksource_read>> last = raw_atomic64_read(&last_value);
+	 *   - arch/x86/kernel/pvclock.c|113| <<__pvclock_clocksource_read>> } while (!raw_atomic64_try_cmpxchg(&last_value, &last, ret));
+	 */
 	last = raw_atomic64_read(&last_value);
 	do {
 		if (ret <= last)
 			return last;
+	/*
+	 * If (@v == @old), atomically updates @v to @new with full ordering.
+	 * Otherwise, updates @old to the current value of @v.
+	 * Return: @true if the exchange occured, @false otherwise.
+	 * 如果&last_value和&last相等, 把&last_value修改为ret,
+	 * 否则, 把&last修改为&last_value
+	 */
 	} while (!raw_atomic64_try_cmpxchg(&last_value, &last, ret));
 
 	return ret;
diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 96a771f9f..796278904 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -276,6 +276,46 @@ static int __init nonmi_ipi_setup(char *str)
 
 __setup("nonmi_ipi", nonmi_ipi_setup);
 
+/*
+ * 在以下使用smp_ops:
+ *   - arch/x86/include/asm/smp.h|54| <<smp_send_stop>> smp_ops.stop_other_cpus(0);
+ *   - arch/x86/include/asm/smp.h|59| <<stop_other_cpus>> smp_ops.stop_other_cpus(1);
+ *   - arch/x86/include/asm/smp.h|64| <<smp_prepare_boot_cpu>> smp_ops.smp_prepare_boot_cpu();
+ *   - arch/x86/include/asm/smp.h|69| <<smp_prepare_cpus>> smp_ops.smp_prepare_cpus(max_cpus);
+ *   - arch/x86/include/asm/smp.h|74| <<smp_cpus_done>> smp_ops.smp_cpus_done(max_cpus);
+ *   - arch/x86/include/asm/smp.h|79| <<__cpu_disable>> return smp_ops.cpu_disable();
+ *   - arch/x86/include/asm/smp.h|84| <<__cpu_die>> if (smp_ops.cpu_die)
+ *   - arch/x86/include/asm/smp.h|85| <<__cpu_die>> smp_ops.cpu_die(cpu);
+ *   - arch/x86/include/asm/smp.h|90| <<play_dead>> smp_ops.play_dead();
+ *   - arch/x86/include/asm/smp.h|96| <<arch_smp_send_reschedule>> smp_ops.smp_send_reschedule(cpu);
+ *   - arch/x86/include/asm/smp.h|101| <<arch_send_call_function_single_ipi>> smp_ops.send_call_func_single_ipi(cpu);
+ *   - arch/x86/include/asm/smp.h|106| <<arch_send_call_function_ipi_mask>> smp_ops.send_call_func_ipi(mask);
+ *   - arch/x86/kernel/cpu/mshyperv.c|558| <<ms_hyperv_init_platform>> smp_ops.smp_prepare_boot_cpu = hv_smp_prepare_boot_cpu;
+ *   - arch/x86/kernel/cpu/mshyperv.c|561| <<ms_hyperv_init_platform>> smp_ops.smp_prepare_cpus = hv_smp_prepare_cpus;
+ *   - arch/x86/kernel/cpu/vmware.c|350| <<vmware_paravirt_ops_setup>> smp_ops.smp_prepare_boot_cpu =
+ *   - arch/x86/kernel/crash.c|80| <<crash_smp_send_stop>> if (smp_ops.crash_stop_other_cpus)
+ *   - arch/x86/kernel/crash.c|81| <<crash_smp_send_stop>> smp_ops.crash_stop_other_cpus();
+ *   - arch/x86/kernel/kvm.c|843| <<kvm_guest_init>> smp_ops.smp_prepare_boot_cpu = kvm_smp_prepare_boot_cpu;
+ *   - arch/x86/kernel/kvm.c|845| <<kvm_guest_init>> smp_ops.send_call_func_ipi = kvm_smp_send_call_func_ipi;
+ *   - arch/x86/kernel/sev.c|1327| <<sev_es_setup_play_dead>> smp_ops.play_dead = sev_es_play_dead;
+ *   - arch/x86/kernel/smpboot.c|1099| <<arch_cpuhp_kick_ap_alive>> return smp_ops.kick_ap_alive(cpu, tidle);
+ *   - arch/x86/kernel/smpboot.c|1105| <<arch_cpuhp_cleanup_kick_cpu>> if (smp_ops.kick_ap_alive == native_kick_ap && x86_platform.legacy.warm_reset)
+ *   - arch/x86/kernel/smpboot.c|1111| <<arch_cpuhp_cleanup_dead_cpu>> if (smp_ops.cleanup_dead_cpu)
+ *   - arch/x86/kernel/smpboot.c|1112| <<arch_cpuhp_cleanup_dead_cpu>> smp_ops.cleanup_dead_cpu(cpu);
+ *   - arch/x86/kernel/smpboot.c|1120| <<arch_cpuhp_sync_state_poll>> if (smp_ops.poll_sync_state)
+ *   - arch/x86/kernel/smpboot.c|1121| <<arch_cpuhp_sync_state_poll>> smp_ops.poll_sync_state();
+ *   - arch/x86/power/cpu.c|300| <<hibernate_resume_nonboot_cpu_disable>> void (*play_dead)(void ) = smp_ops.play_dead;
+ *   - arch/x86/power/cpu.c|322| <<hibernate_resume_nonboot_cpu_disable>> smp_ops.play_dead = resume_play_dead;
+ *   - arch/x86/power/cpu.c|324| <<hibernate_resume_nonboot_cpu_disable>> smp_ops.play_dead = play_dead;
+ *   - arch/x86/xen/smp_hvm.c|75| <<xen_hvm_smp_init>> smp_ops.smp_prepare_boot_cpu = xen_hvm_smp_prepare_boot_cpu;
+ *   - arch/x86/xen/smp_hvm.c|76| <<xen_hvm_smp_init>> smp_ops.smp_prepare_cpus = xen_hvm_smp_prepare_cpus;
+ *   - arch/x86/xen/smp_hvm.c|77| <<xen_hvm_smp_init>> smp_ops.smp_cpus_done = xen_smp_cpus_done;
+ *   - arch/x86/xen/smp_hvm.c|78| <<xen_hvm_smp_init>> smp_ops.cleanup_dead_cpu = xen_hvm_cleanup_dead_cpu;
+ *   - arch/x86/xen/smp_hvm.c|87| <<xen_hvm_smp_init>> smp_ops.smp_send_reschedule = xen_smp_send_reschedule;
+ *   - arch/x86/xen/smp_hvm.c|88| <<xen_hvm_smp_init>> smp_ops.send_call_func_ipi = xen_smp_send_call_function_ipi;
+ *   - arch/x86/xen/smp_hvm.c|89| <<xen_hvm_smp_init>> smp_ops.send_call_func_single_ipi = xen_smp_send_call_function_single_ipi;
+ *   - arch/x86/xen/smp_pv.c|458| <<xen_smp_init>> smp_ops = xen_smp_ops;
+ */
 struct smp_ops smp_ops = {
 	.smp_prepare_boot_cpu	= native_smp_prepare_boot_cpu,
 	.smp_prepare_cpus	= native_smp_prepare_cpus,
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 3f57ce68a..c526c738f 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1428,6 +1428,21 @@ static void remove_cpu_from_maps(int cpu)
 	numa_remove_cpu(cpu);
 }
 
+/*
+ * [0] cpu_disable_common
+ * [0] native_cpu_disable
+ * [0] take_cpu_down
+ * [0] multi_cpu_stop
+ * [0] cpu_stopper_thread
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1453| <<native_cpu_disable>> cpu_disable_common();
+ *   - arch/x86/xen/smp_pv.c|358| <<xen_pv_cpu_disable>> cpu_disable_common();
+ */
 void cpu_disable_common(void)
 {
 	int cpu = smp_processor_id();
@@ -1438,10 +1453,21 @@ void cpu_disable_common(void)
 	lock_vector_lock();
 	remove_cpu_from_maps(cpu);
 	unlock_vector_lock();
+	/*
+	 * 只在此处调用
+	 */
 	fixup_irqs();
+	/*
+	 * 核心思想是清理percpu的&vector_cleanup list
+	 * 不检查IRR的pending
+	 */
 	lapic_offline();
 }
 
+/*
+ * 在以下使用native_cpu_disable():
+ *   - struct smp_ops smp_ops.cpu_disable = native_cpu_disable,
+ */
 int native_cpu_disable(void)
 {
 	int ret;
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index adba49afb..c41f59a54 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -325,6 +325,10 @@ static bool kvm_cpuid_has_hyperv(struct kvm_cpuid_entry2 *entries, int nent)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|464| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -405,11 +409,20 @@ int cpuid_query_maxphyaddr(struct kvm_vcpu *vcpu)
  * encryption technologies that usurp bits.  The raw mask should be used if and
  * only if hardware does _not_ strip the usurped bits, e.g. in virtual MTRRs.
  */
+/*
+ * 为gpa保留的地址部分生成mask, 从不支持的最高bit到63:
+ * rsvd_bits(cpuid_maxphyaddr(vcpu), 63)
+ */
 u64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)
 {
 	return rsvd_bits(cpuid_maxphyaddr(vcpu), 63);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|505| <<kvm_vcpu_ioctl_set_cpuid>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ *   - arch/x86/kvm/cpuid.c|531| <<kvm_vcpu_ioctl_set_cpuid2>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ */
 static int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,
                         int nent)
 {
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index e223043ef..4642e03bf 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -1392,6 +1392,12 @@ static int segmented_read(struct x86_emulate_ctxt *ctxt,
 	return read_emulated(ctxt, linear, data, size);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|1799| <<writeback>> return segmented_write(ctxt,
+ *   - arch/x86/kvm/emulate.c|1804| <<writeback>> return segmented_write(ctxt,
+ *   - arch/x86/kvm/emulate.c|1831| <<push>> return segmented_write(ctxt, addr, data, bytes);
+ */
 static int segmented_write(struct x86_emulate_ctxt *ctxt,
 			   struct segmented_address addr,
 			   const void *data,
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 60f21bb4c..af92cfbe7 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -13,6 +13,11 @@ extern bool __read_mostly enable_mmio_caching;
 
 #define PT_PRESENT_MASK (1ULL << 0)
 #define PT_WRITABLE_MASK (1ULL << PT_WRITABLE_SHIFT)
+/*
+ * 注释bit=2(U/S):
+ * User/supervisor; if 0, user-mode accesses are not allowed to the 4-KByte
+ * page referenced by this entry (see Section 4.6)
+ */
 #define PT_USER_MASK (1ULL << PT_USER_SHIFT)
 #define PT_PWT_MASK (1ULL << 3)
 #define PT_PCD_MASK (1ULL << 4)
@@ -28,9 +33,21 @@ extern bool __read_mostly enable_mmio_caching;
 #define PT64_NX_MASK (1ULL << PT64_NX_SHIFT)
 
 #define PT_PAT_SHIFT 7
+/*
+ * 下面两个没见到使用
+ */
 #define PT_DIR_PAT_SHIFT 12
 #define PT_DIR_PAT_MASK (1ULL << PT_DIR_PAT_SHIFT)
 
+/*
+ * 在以下使用PT64_ROOT_5LEVEL:
+ *   - arch/x86/kvm/mmu/mmu.c|4001| <<mmu_alloc_shadow_roots>> if (mmu->root_role.level == PT64_ROOT_5LEVEL) {
+ *   - arch/x86/kvm/mmu/mmu.c|4033| <<mmu_alloc_shadow_roots>> if (mmu->root_role.level == PT64_ROOT_5LEVEL)
+ *   - arch/x86/kvm/mmu/mmu.c|5111| <<__reset_rsvds_bits_mask>> case PT64_ROOT_5LEVEL:
+ *   - arch/x86/kvm/mmu/mmu.c|5526| <<kvm_calc_cpu_role>> role.base.level = ____is_cr4_la57(regs) ? PT64_ROOT_5LEVEL
+ *   - arch/x86/kvm/mmu/mmu.c|5721| <<kvm_init_shadow_npt_mmu>> if (root_role.level == PT64_ROOT_5LEVEL &&
+ *   - arch/x86/kvm/svm/svm.c|290| <<get_npt_level>> return pgtable_l5_enabled() ? PT64_ROOT_5LEVEL : PT64_ROOT_4LEVEL;
+ */
 #define PT64_ROOT_5LEVEL 5
 #define PT64_ROOT_4LEVEL 4
 #define PT32_ROOT_LEVEL 2
@@ -81,8 +98,22 @@ static inline gfn_t kvm_mmu_max_gfn(void)
 	return (1ULL << (max_gpa_bits - PAGE_SHIFT)) - 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|916| <<kvm_mmu_reset_all_pte_masks>> shadow_phys_bits = kvm_get_shadow_phys_bits();
+ *   - arch/x86/kvm/vmx/vmx.c|8482| <<vmx_setup_me_spte_mask>> if (boot_cpu_data.x86_phys_bits != kvm_get_shadow_phys_bits())
+ *   - arch/x86/kvm/vmx/vmx.c|8484| <<vmx_setup_me_spte_mask>> kvm_get_shadow_phys_bits() - 1);
+ */
 static inline u8 kvm_get_shadow_phys_bits(void)
 {
+	/*
+	 * $ cpuid -1 -l 0x80000008
+	 * CPU:
+	 *    Physical Address and Linear Address Size (0x80000008/eax):
+	 *       maximum physical address bits         = 0x2e (46)
+	 *       maximum linear (virtual) address bits = 0x30 (48)
+	 *       maximum guest physical address bits   = 0x0 (0)
+	 */
 	/*
 	 * boot_cpu_data.x86_phys_bits is reduced when MKTME or SME are detected
 	 * in CPU detection code, but the processor treats those reduced bits as
@@ -124,8 +155,23 @@ void kvm_mmu_sync_prev_roots(struct kvm_vcpu *vcpu);
 void kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 			 int bytes);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4487| <<kvm_arch_async_page_ready>> r = kvm_mmu_reload(vcpu);
+ *   - arch/x86/kvm/x86.c|11005| <<vcpu_enter_guest(没有event)>> r = kvm_mmu_reload(vcpu);
+ */
 static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_mmu *mmu;
+	 *       -> struct kvm_mmu_root_info root;
+	 *          -> gpa_t pgd;
+	 *          -> hpa_t hpa;
+	 *       -> union kvm_cpu_role cpu_role;
+	 *       -> union kvm_mmu_page_role root_role;
+	 */
 	if (likely(vcpu->arch.mmu->root.hpa != INVALID_PAGE))
 		return 0;
 
@@ -154,6 +200,18 @@ static inline unsigned long kvm_get_active_cr3_lam_bits(struct kvm_vcpu *vcpu)
 	return kvm_read_cr3(vcpu) & (X86_CR3_LAM_U48 | X86_CR3_LAM_U57);
 }
 
+/*
+ * 在以下使用KVM_REQ_LOAD_MMU_PGD:
+ *   - arch/x86/kvm/mmu/mmu.c|5063| <<kvm_mmu_new_pgd>> kvm_make_request(KVM_REQ_LOAD_MMU_PGD, vcpu);
+ *   - arch/x86/kvm/x86.c|913| <<load_pdptrs>> kvm_make_request(KVM_REQ_LOAD_MMU_PGD, vcpu);
+ *   - arch/x86/kvm/x86.c|10859| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_LOAD_MMU_PGD, vcpu))
+ *
+ * 处理的函数: kvm_mmu_load_pgd()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6131| <<kvm_mmu_load>> kvm_mmu_load_pgd(vcpu);
+ *   - arch/x86/kvm/x86.c|10860| <<vcpu_enter_guest(KVM_REQ_LOAD_MMU_PGD)>> kvm_mmu_load_pgd(vcpu);
+ */
 static inline void kvm_mmu_load_pgd(struct kvm_vcpu *vcpu)
 {
 	u64 root_hpa = vcpu->arch.mmu->root.hpa;
@@ -161,10 +219,29 @@ static inline void kvm_mmu_load_pgd(struct kvm_vcpu *vcpu)
 	if (!VALID_PAGE(root_hpa))
 		return;
 
+	/*
+	 * vmx_load_mmu_pgd()
+	 */
 	static_call(kvm_x86_load_mmu_pgd)(vcpu, root_hpa,
 					  vcpu->arch.mmu->root_role.level);
 }
 
+/*
+ * 关于cr0/cr4的passthrough:
+ * VM-execution control fields include guest/host masks and read shadows for the CR0 and CR4 registers. These
+ * fields control executions of instructions that access those registers (including CLTS, LMSW, MOV CR, and SMSW).
+ * They are 64 bits on processors that support Intel 64 architecture and 32 bits on processors that do not.
+ *
+ * In general, bits set to 1 in a guest/host mask correspond to bits "owned" by the host:
+ *
+ * 1. Guest attempts to set them (using CLTS, LMSW, or MOV to CR) to values differing from the corresponding bits
+ * in the corresponding read shadow cause VM exits.
+ *
+ * 2. Guest reads (using MOV from CR or SMSW) return values for these bits from the corresponding read shadow.
+ *
+ * Bits cleared to 0 correspond to bits "owned" by the guest; guest
+ * attempts to modify them succeed and guest reads return values for these bits from the control register itself.
+ */
 static inline void kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,
 						    struct kvm_mmu *mmu)
 {
@@ -191,6 +268,36 @@ static inline void kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,
  * Return zero if the access does not fault; return the page fault error code
  * if the access faults.
  */
+/*
+ * From 97d64b788114be1c4dc4bfe7a8ba2bf9643fe6af Mon Sep 17 00:00:00 2001
+ * From: Avi Kivity <avi@redhat.com>
+ * Date: Wed, 12 Sep 2012 14:52:00 +0300
+ * Subject: [PATCH 1/1] KVM: MMU: Optimize pte permission checks
+ *
+ * walk_addr_generic() permission checks are a maze of branchy code, which is
+ * performed four times per lookup.  It depends on the type of access, efer.nxe,
+ * cr0.wp, cr4.smep, and in the near future, cr4.smap.
+ *
+ * Optimize this away by precalculating all variants and storing them in a
+ * bitmap.  The bitmap is recalculated when rarely-changing variables change
+ * (cr0, cr4) and is indexed by the often-changing variables (page fault error
+ * code, pte access permissions).
+ *
+ * The permission check is moved to the end of the loop, otherwise an SMEP
+ * fault could be reported as a false positive, when PDE.U=1 but PTE.U=0.]
+ * Noted by Xiao Guangrong.
+ *
+ * The result is short, branch-free code.
+ *
+ * Reviewed-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
+ * Signed-off-by: Avi Kivity <avi@redhat.com>
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|436| <<FNAME(walk_addr_generic)>> errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
+ *   - arch/x86/kvm/x86.c|7758| <<vcpu_mmio_gva_to_gpa>> !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access))) {
+ *
+ * 我们应该是希望permission_fault()返回0
+ */
 static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 				  unsigned pte_access, unsigned pte_pkey,
 				  u64 access)
@@ -219,6 +326,20 @@ static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 
 	kvm_mmu_refresh_passthrough_bits(vcpu, mmu);
 
+	/*
+	 * 在以下使用kvm_mmu->permissions[16] (u8)
+	 *   - arch/x86/include/asm/kvm_host.h|556| <<global>> u8 permissions[16];
+	 *   - arch/x86/kvm/mmu.h|246| <<permission_fault>> fault = (mmu->permissions[index] >> pte_access) & 1;
+	 *   - arch/x86/kvm/mmu/mmu.c|5332| <<update_permission_bitmask>> for (byte = 0; byte < ARRAY_SIZE(mmu->permissions); ++byte) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5387| <<update_permission_bitmask>> mmu->permissions[byte] = ff | uf | wf | smepf | smapf;
+	 *   - arch/x86/kvm/mmu/mmu.c|5427| <<update_pkru_bitmask>> for (bit = 0; bit < ARRAY_SIZE(mmu->permissions); ++bit) {
+	 *
+	 * crash 6.6的例子:
+	 * crash> u8 ff11000191a40310 16
+	 * ff11000191a40310:  f0 f3 0f 3f 00 33 0f 3f f5 f7 5f 7f f5 f7 5f 7f
+	 *
+	 * 猜测, pte_access可能是: 000, 010, 100, 110 = 0, 2, 4, 6
+	 */
 	fault = (mmu->permissions[index] >> pte_access) & 1;
 
 	WARN_ON(pfec & (PFERR_PK_MASK | PFERR_RSVD_MASK));
@@ -247,6 +368,12 @@ static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 
 bool __kvm_mmu_honors_guest_mtrrs(bool vm_has_noncoherent_dma);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4630| <<kvm_tdp_page_fault>> if (kvm_mmu_honors_guest_mtrrs(vcpu->kvm)) {
+ *   - arch/x86/kvm/mtrr.c|499| <<update_mtrr>> if (!kvm_mmu_honors_guest_mtrrs(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|965| <<kvm_post_set_cr0>> kvm_mmu_honors_guest_mtrrs(vcpu->kvm) &&
+ */
 static inline bool kvm_mmu_honors_guest_mtrrs(struct kvm *kvm)
 {
 	return __kvm_mmu_honors_guest_mtrrs(kvm_arch_has_noncoherent_dma(kvm));
@@ -259,6 +386,13 @@ int kvm_arch_write_log_dirty(struct kvm_vcpu *vcpu);
 int kvm_mmu_post_init_vm(struct kvm *kvm);
 void kvm_mmu_pre_destroy_vm(struct kvm *kvm);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|347| <<kvm_memslots_have_rmaps>> return !tdp_mmu_enabled || kvm_shadow_root_allocated(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|3879| <<mmu_first_shadow_root_alloc>> if (kvm_shadow_root_allocated(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|3885| <<mmu_first_shadow_root_alloc>> if (kvm_shadow_root_allocated(kvm))
+ *   - arch/x86/kvm/mmu/page_track.c|26| <<kvm_page_track_write_tracking_enabled>> !tdp_enabled || kvm_shadow_root_allocated(kvm);
+ */
 static inline bool kvm_shadow_root_allocated(struct kvm *kvm)
 {
 	/*
@@ -276,26 +410,88 @@ extern bool tdp_mmu_enabled;
 #define tdp_mmu_enabled false
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|100| <<kvm_mmu_rmaps_stat_show>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1378| <<kvm_mmu_write_protect_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1411| <<kvm_mmu_clear_dirty_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1497| <<kvm_mmu_slot_gfn_write_protect>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|1672| <<kvm_unmap_gfn_range>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1689| <<kvm_set_spte_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1763| <<kvm_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1776| <<kvm_test_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|3892| <<mmu_first_shadow_root_alloc>> if (kvm_memslots_have_rmaps(kvm) &&
+ *   - arch/x86/kvm/mmu/mmu.c|6820| <<kvm_rmap_zap_gfn_range>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|6890| <<kvm_mmu_slot_remove_write_access>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7142| <<kvm_mmu_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|7163| <<kvm_mmu_slot_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7236| <<kvm_mmu_zap_collapsible_sptes>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7252| <<kvm_mmu_slot_leaf_clear_dirty>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/x86.c|12902| <<kvm_alloc_memslot_metadata>> if (kvm_memslots_have_rmaps(kvm)) {
+ */
 static inline bool kvm_memslots_have_rmaps(struct kvm *kvm)
 {
 	return !tdp_mmu_enabled || kvm_shadow_root_allocated(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|323| <<__kvm_mmu_slot_lpages>> return gfn_to_index(slot->base_gfn + npages - 1,
+ *   - arch/x86/kvm/mmu/mmu.c|790| <<lpage_info_slot>> idx = gfn_to_index(gfn, slot->base_gfn, level);
+ *   - arch/x86/kvm/mmu/mmu.c|1091| <<gfn_to_rmap>> idx = gfn_to_index(gfn, slot->base_gfn, level);
+ *   - arch/x86/kvm/mmu/page_track.c|67| <<update_gfn_write_track>> index = gfn_to_index(gfn, slot->base_gfn, PG_LEVEL_4K);
+ *   - arch/x86/kvm/mmu/page_track.c|134| <<kvm_gfn_is_write_tracked>> index = gfn_to_index(gfn, slot->base_gfn, PG_LEVEL_4K);
+ */
 static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 {
+	/*
+	 * KVM_HPAGE_GFN_SHIFT(1) : (((1) - 1) * 9) =  0
+	 * KVM_HPAGE_GFN_SHIFT(2) : (((2) - 1) * 9) =  9 ---> level 2
+	 * KVM_HPAGE_GFN_SHIFT(3) : (((3) - 1) * 9) = 18 ---> level 3
+	 * KVM_HPAGE_GFN_SHIFT(4) : (((4) - 1) * 9) = 27
+	 * KVM_HPAGE_GFN_SHIFT(5) : (((5) - 1) * 9) = 36
+	 *
+	 * 假设:
+	 * base_gfn: 133693442
+	 * gfn     : 134060443
+	 *
+	 * 对于level 2:
+	 * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 9       = 261836
+	 * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 9 = 261120
+	 * 261836 - 261120 = 716
+	 *
+	 * 对于level 3:
+	 * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 18       = 511
+	 * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 18 = 510
+	 * 511 - 510 = 0
+	 * 
+	 */
 	/* KVM_HPAGE_GFN_SHIFT(PG_LEVEL_4K) must be 0. */
 	return (gfn >> KVM_HPAGE_GFN_SHIFT(level)) -
 		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|401| <<kvm_mmu_slot_lpages>> return __kvm_mmu_slot_lpages(slot, slot->npages, level);
+ *   - arch/x86/kvm/x86.c|12849| <<memslot_rmap_alloc>> int lpages = __kvm_mmu_slot_lpages(slot, npages, level);
+ *   - arch/x86/kvm/x86.c|12945| <<kvm_alloc_memslot_metadata>> lpages = __kvm_mmu_slot_lpages(slot, npages, level);
+ */
 static inline unsigned long
 __kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, unsigned long npages,
 		      int level)
 {
+	/*
+	 * 1048576 + 1024 -1 = 1049599
+	 */
 	return gfn_to_index(slot->base_gfn + npages - 1,
 			    slot->base_gfn, level) + 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|121| <<kvm_mmu_rmaps_stat_show>> lpage_size = kvm_mmu_slot_lpages(slot, k + 1);
+ */
 static inline unsigned long
 kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, int level)
 {
@@ -310,11 +506,35 @@ static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u64 access,
 			   struct x86_exception *exception);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4215| <<nonpaging_gva_to_gpa>> return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|442| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn), nested_access, &walker->fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|553| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(gfn), access, &walker->fault);
+ *   - arch/x86/kvm/x86.c|886| <<load_pdptrs>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn), PFERR_USER_MASK | PFERR_WRITE_MASK, NULL);
+ */
 static inline gpa_t kvm_translate_gpa(struct kvm_vcpu *vcpu,
 				      struct kvm_mmu *mmu,
 				      gpa_t gpa, u64 access,
 				      struct x86_exception *exception)
 {
+	/*
+	 * Paging state of an L2 guest (used for nested npt)
+	 *
+	 * This context will save all necessary information to walk page tables
+	 * of an L2 guest. This context is only initialized for page table
+	 * walking and not for faulting since we never handle l2 page faults on
+	 * the host.
+	 *
+	 * 在以下使用kvm_vcpu_arch->nested_mmu:
+	 *   - arch/x86/kvm/mmu.h|376| <<kvm_translate_gpa>> if (mmu != &vcpu->arch.nested_mmu)
+	 *   - arch/x86/kvm/mmu/mmu.c|5739| <<init_kvm_nested_mmu>> struct kvm_mmu *g_context = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|5817| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|5820| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.cpu_role.ext.valid = 0;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|461| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/x86.h|185| <<mmu_is_nested>> return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;
+	 */
 	if (mmu != &vcpu->arch.nested_mmu)
 		return gpa;
 	return translate_nested_gpa(vcpu, gpa, access, exception);
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 0544700ca..b5afd4202 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -239,11 +239,18 @@ static inline bool is_cr0_pg(struct kvm_mmu *mmu)
         return mmu->cpu_role.base.level > 0;
 }
 
+/*
+ * 使用的是cpu_role, 是不是更新crX的时候要刷新???
+ */
 static inline bool is_cr4_pae(struct kvm_mmu *mmu)
 {
         return !mmu->cpu_role.base.has_4_byte_gpte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6094| <<kvm_init_mmu>> struct kvm_mmu_role_regs regs = vcpu_to_role_regs(vcpu);
+ */
 static struct kvm_mmu_role_regs vcpu_to_role_regs(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu_role_regs regs = {
@@ -289,6 +296,12 @@ static void kvm_flush_remote_tlbs_sptep(struct kvm *kvm, u64 *sptep)
 	kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2947| <<mmu_set_spte>> mark_mmio_spte(vcpu, sptep, gfn, pte_access);
+ *   - arch/x86/kvm/mmu/mmu.c|4870| <<sync_mmio_spte>> mark_mmio_spte(vcpu, sptep, gfn, access);
+ *   - arch/x86/kvm/mmu/mmutrace.h|211| <<KVM_MMU_PAGE_ASSIGN>> mark_mmio_spte,
+ */
 static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 			   unsigned int access)
 {
@@ -298,8 +311,23 @@ static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 	mmu_spte_set(sptep, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4292| <<handle_mmio_page_fault>> gfn_t gfn = get_mmio_spte_gfn(spte);
+ *   - arch/x86/kvm/mmu/mmu.c|4964| <<sync_mmio_spte>> if (gfn != get_mmio_spte_gfn(*sptep)) {
+ */
 static gfn_t get_mmio_spte_gfn(u64 spte)
 {
+	/*
+	 * 测试的例子 (不支持ME):
+	 * boot_cpu_data.x86_phys_bits = 46
+	 * boot_cpu_data.x86_cache_bits = 46
+	 * shadow_phys_bits=46
+	 * bit: 41 - 45 (5个bit)
+	 * shadow_nonpresent_or_rsvd_mask=0x00003e0000000000
+	 * bit: 12 - 40
+	 * shadow_nonpresent_or_rsvd_lower_gfn_mask=0x000001fffffff000
+	 */
 	u64 gpa = spte & shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
 	gpa |= (spte >> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)
@@ -308,8 +336,32 @@ static gfn_t get_mmio_spte_gfn(u64 spte)
 	return gpa >> PAGE_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4308| <<handle_mmio_page_fault>> unsigned int access = get_mmio_spte_access(spte);
+ *
+ * EXIT_REASON_EXCEPTION_NMI:handle_exception_nmi()
+ * -> kvm_handle_page_fault()
+ *    -> kvm_mmu_page_fault()
+ *       -> handle_mmio_page_fault() --> error_code有PFERR_RSVD_MASK的时候
+ *          -> get_mmio_spte_access()
+ *          -> vcpu_cache_mmio_info(vcpu, addr, gfn, access);
+ * 用在已经二次配置的mmio(等同与misconfig). 取出host shadow pte的ACC_WRITE_MASK|ACC_USER_MASK,
+ * 然后用vcpu_cache_mmio_info()缓存
+ */
 static unsigned get_mmio_spte_access(u64 spte)
 {
+	/*
+	 * 在以下设置shadow_mmio_access_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|446| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_access_mask = access_mask;
+	 * 在以下使用shadow_mmio_access_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|313| <<get_mmio_spte_access>> return spte & shadow_mmio_access_mask;
+	 *   - arch/x86/kvm/mmu/mmu.c|3340| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+	 *   - arch/x86/kvm/mmu/spte.c|106| <<make_mmio_spte>> access &= shadow_mmio_access_mask;
+	 *
+	 * 比如: ept是0
+	 * regular pte是ACC_WRITE_MASK | ACC_USER_MASK
+	 */
 	return spte & shadow_mmio_access_mask;
 }
 
@@ -325,6 +377,13 @@ static bool check_mmio_spte(struct kvm_vcpu *vcpu, u64 spte)
 	spte_gen = get_mmio_spte_generation(spte);
 
 	trace_check_mmio_spte(spte, kvm_gen, spte_gen);
+	/*
+	 * 注释:
+	 * When KVM finds an MMIO spte, it checks the generation number of the spte.
+	 * If the generation number of the spte does not equal the global generation
+	 * number, it will ignore the cached MMIO information and handle the page
+	 * fault through the slow path.
+	 */
 	return likely(kvm_gen == spte_gen);
 }
 
@@ -517,6 +576,13 @@ static u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)
  *
  * Returns true if the TLB needs to be flushed
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1316| <<spte_write_protect>> return mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu/mmu.c|1344| <<spte_clear_dirty>> return mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu/mmu.c|3105| <<mmu_set_spte>> flush |= mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1323| <<FNAME(sync_spte)>> return mmu_spte_update(sptep, spte);
+ */
 static bool mmu_spte_update(u64 *sptep, u64 new_spte)
 {
 	bool flush = false;
@@ -714,6 +780,20 @@ static void mmu_free_pte_list_desc(struct pte_list_desc *pte_list_desc)
 
 static bool sp_has_gptes(struct kvm_mmu_page *sp);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|294| <<kvm_flush_remote_tlbs_sptep>> gfn_t gfn = kvm_mmu_page_get_gfn(sp, spte_index(sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|833| <<kvm_mmu_page_set_translation>> WARN_ONCE(gfn != kvm_mmu_page_get_gfn(sp, index),
+ *   - arch/x86/kvm/mmu/mmu.c|836| <<kvm_mmu_page_set_translation>> sp->gfn, kvm_mmu_page_get_gfn(sp, index), gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|842| <<kvm_mmu_page_set_access>> gfn_t gfn = kvm_mmu_page_get_gfn(sp, index);
+ *   - arch/x86/kvm/mmu/mmu.c|1185| <<rmap_remove>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|1824| <<kvm_mmu_check_sptes_at_free>> kvm_mmu_page_get_gfn(sp, i));
+ *   - arch/x86/kvm/mmu/mmu.c|3162| <<direct_pte_prefetch_many>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(start));
+ *   - arch/x86/kvm/mmu/mmu.c|7335| <<shadow_mmu_get_sp_for_split>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|7371| <<shadow_mmu_split_huge_page>> gfn = kvm_mmu_page_get_gfn(sp, index);
+ *   - arch/x86/kvm/mmu/mmu.c|7410| <<shadow_mmu_try_split_huge_page>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1305| <<FNAME>> gfn != kvm_mmu_page_get_gfn(sp, i)) {
+ */
 static gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index)
 {
 	if (sp->role.passthrough)
@@ -782,6 +862,14 @@ static void kvm_mmu_page_set_access(struct kvm_mmu_page *sp, int index,
  * Return the pointer to the large page information for a given gfn,
  * handling slots that are not large page aligned.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|809| <<update_gfn_disallow_lpage_count>> linfo = lpage_info_slot(gfn, slot, i);
+ *   - arch/x86/kvm/mmu/mmu.c|3155| <<__kvm_mmu_max_mapping_level>> linfo = lpage_info_slot(gfn, slot, max_level);
+ *   - arch/x86/kvm/mmu/mmu.c|7376| <<hugepage_test_mixed>> return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7382| <<hugepage_clear_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7394| <<hugepage_set_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
+ */
 static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
 		const struct kvm_memory_slot *slot, int level)
 {
@@ -797,6 +885,13 @@ static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
  * The lower order bits are used to refcount other cases where a hugepage is
  * disallowed, e.g. if KVM has shadow a page table at the gfn.
  */
+/*
+ * 在以下使用KVM_LPAGE_MIXED_FLAG:
+ *   - arch/x86/kvm/mmu/mmu.c|821| <<update_gfn_disallow_lpage_count>> WARN_ON_ONCE((old ^ linfo->disallow_lpage) & KVM_LPAGE_MIXED_FLAG);
+ *   - arch/x86/kvm/mmu/mmu.c|7384| <<hugepage_test_mixed>> return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7390| <<hugepage_clear_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7402| <<hugepage_set_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
+ */
 #define KVM_LPAGE_MIXED_FLAG	BIT(31)
 
 static void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,
@@ -1226,6 +1321,10 @@ static void drop_large_spte(struct kvm *kvm, u64 *sptep, bool flush)
  *
  * Return true if tlb need be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1273| <<rmap_write_protect>> flush |= spte_write_protect(sptep, pt_protect);
+ */
 static bool spte_write_protect(u64 *sptep, bool pt_protect)
 {
 	u64 spte = *sptep;
@@ -1241,6 +1340,12 @@ static bool spte_write_protect(u64 *sptep, bool pt_protect)
 	return mmu_spte_update(sptep, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1344| <<kvm_mmu_write_protect_pt_masked>> rmap_write_protect(rmap_head, false);
+ *   - arch/x86/kvm/mmu/mmu.c|1446| <<kvm_mmu_slot_gfn_write_protect>> write_protected |= rmap_write_protect(rmap_head, true);
+ *   - arch/x86/kvm/mmu/mmu.c|6534| <<slot_rmap_write_protect>> return rmap_write_protect(rmap_head, false);
+ */
 static bool rmap_write_protect(struct kvm_rmap_head *rmap_head,
 			       bool pt_protect)
 {
@@ -1370,6 +1475,12 @@ static void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
  * We need to care about huge page mappings: e.g. during dirty logging we may
  * have such mappings.
  */
+/*
+ * called by:
+ *   - virt/kvm/dirty_ring.c|70| <<kvm_reset_dirty_gfn>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ *   - virt/kvm/kvm_main.c|2307| <<kvm_get_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ *   - virt/kvm/kvm_main.c|2424| <<kvm_clear_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ */
 void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 				struct kvm_memory_slot *slot,
 				gfn_t gfn_offset, unsigned long mask)
@@ -1411,6 +1522,14 @@ int kvm_cpu_dirty_log_size(void)
 	return kvm_x86_ops.cpu_dirty_log_size;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|865| <<account_shadowed>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+ *   - arch/x86/kvm/mmu/mmu.c|1414| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, start, PG_LEVEL_2M);
+ *   - arch/x86/kvm/mmu/mmu.c|1419| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, end, PG_LEVEL_2M);
+ *   - arch/x86/kvm/mmu/mmu.c|1462| <<kvm_vcpu_write_protect_gfn>> return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn, PG_LEVEL_4K);
+ *   - arch/x86/kvm/mmu/page_track.c|96| <<__kvm_write_track_add_gfn>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+ */
 bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 				    struct kvm_memory_slot *slot, u64 gfn,
 				    int min_level)
@@ -1433,6 +1552,10 @@ bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 	return write_protected;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2135| <<mmu_sync_children>> protected |= kvm_vcpu_write_protect_gfn(vcpu, sp->gfn);
+ */
 static bool kvm_vcpu_write_protect_gfn(struct kvm_vcpu *vcpu, u64 gfn)
 {
 	struct kvm_memory_slot *slot;
@@ -1564,6 +1687,13 @@ typedef bool (*rmap_handler_t)(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 			       struct kvm_memory_slot *slot, gfn_t gfn,
 			       int level, pte_t pte);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1602| <<kvm_unmap_gfn_range>> flush = kvm_handle_gfn_range(kvm, range, kvm_zap_rmap);
+ *   - arch/x86/kvm/mmu/mmu.c|1619| <<kvm_set_spte_gfn>> flush = kvm_handle_gfn_range(kvm, range, kvm_set_pte_rmap);
+ *   - arch/x86/kvm/mmu/mmu.c|1693| <<kvm_age_gfn>> young = kvm_handle_gfn_range(kvm, range, kvm_age_rmap);
+ *   - arch/x86/kvm/mmu/mmu.c|1706| <<kvm_test_age_gfn>> young = kvm_handle_gfn_range(kvm, range, kvm_test_age_rmap);
+ */
 static __always_inline bool kvm_handle_gfn_range(struct kvm *kvm,
 						 struct kvm_gfn_range *range,
 						 rmap_handler_t handler)
@@ -1948,6 +2078,11 @@ static bool kvm_sync_page_check(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2074| <<__kvm_sync_page>> int ret = kvm_sync_spte(vcpu, sp, i);
+ *   - arch/x86/kvm/mmu/mmu.c|6577| <<__kvm_mmu_invalidate_addr>> int ret = kvm_sync_spte(vcpu, sp, iterator.index);
+ */
 static int kvm_sync_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int i)
 {
 	if (!sp->spt[i])
@@ -1956,6 +2091,10 @@ static int kvm_sync_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int i)
 	return vcpu->arch.mmu->sync_spte(vcpu, sp, i);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2096| <<kvm_sync_page>> int ret = __kvm_sync_page(vcpu, sp);
+ */
 static int __kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
 {
 	int flush = 0;
@@ -1984,6 +2123,11 @@ static int __kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
 	return flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2228| <<mmu_sync_children>> flush |= kvm_sync_page(vcpu, sp, &invalid_list) > 0;
+ *   - arch/x86/kvm/mmu/mmu.c|2316| <<kvm_mmu_find_shadow_page>> ret = kvm_sync_page(vcpu, sp, &invalid_list);
+ */
 static int kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			 struct list_head *invalid_list)
 {
@@ -2090,6 +2234,12 @@ static void mmu_pages_clear_parents(struct mmu_page_path *parents)
 	} while (!sp->unsync_children);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4094| <<kvm_mmu_sync_roots>> mmu_sync_children(vcpu, sp, true);
+ *   - arch/x86/kvm/mmu/mmu.c|4106| <<kvm_mmu_sync_roots>> mmu_sync_children(vcpu, sp, true);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|695| <<FNAME(fetch)>> mmu_sync_children(vcpu, sp, false))
+ */
 static int mmu_sync_children(struct kvm_vcpu *vcpu,
 			     struct kvm_mmu_page *parent, bool can_yield)
 {
@@ -2350,6 +2500,12 @@ static union kvm_mmu_page_role kvm_mmu_child_role(u64 *sptep, bool direct,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3380| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|793| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|856| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn,
+ */
 static struct kvm_mmu_page *kvm_mmu_get_child_sp(struct kvm_vcpu *vcpu,
 						 u64 *sptep, gfn_t gfn,
 						 bool direct, unsigned int access)
@@ -2426,6 +2582,11 @@ static void shadow_walk_next(struct kvm_shadow_walk_iterator *iterator)
 	__shadow_walk_next(iterator, *iterator->sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2529| <<link_shadow_page>> __link_shadow_page(vcpu->kvm, &vcpu->arch.mmu_pte_list_desc_cache, sptep, sp, true);
+ *   - arch/x86/kvm/mmu/mmu.c|6712| <<shadow_mmu_split_huge_page>> __link_shadow_page(kvm, cache, huge_sptep, sp, flush);
+ */
 static void __link_shadow_page(struct kvm *kvm,
 			       struct kvm_mmu_memory_cache *cache, u64 *sptep,
 			       struct kvm_mmu_page *sp, bool flush)
@@ -2461,12 +2622,22 @@ static void __link_shadow_page(struct kvm *kvm,
 		mark_unsync(sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3339| <<direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|707| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|742| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ */
 static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 			     struct kvm_mmu_page *sp)
 {
 	__link_shadow_page(vcpu->kvm, &vcpu->arch.mmu_pte_list_desc_cache, sptep, sp, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|854| <<FNAME(fetch)>> validate_direct_spte(vcpu, it.sptep, direct_access);
+ */
 static void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 				   unsigned direct_access)
 {
@@ -2787,6 +2958,10 @@ static int kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2989| <<mmu_try_to_unsync_pages>> kvm_unsync_page(kvm, sp);
+ */
 static void kvm_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	trace_kvm_mmu_unsync_page(sp);
@@ -2802,6 +2977,10 @@ static void kvm_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)
  * were marked unsync (or if there is no shadow page), -EPERM if the SPTE must
  * be write-protected.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|688| <<make_spte>> if (mmu_try_to_unsync_pages(vcpu->kvm, slot, gfn, can_unsync, prefetch)) {
+ */
 int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 			    gfn_t gfn, bool can_unsync, bool prefetch)
 {
@@ -2903,6 +3082,13 @@ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3019| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, slot, start, access, gfn, page_to_pfn(pages[i]), NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|3294| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL, base_gfn, fault->pfn, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|556| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, slot, spte, pte_access, gfn, pfn, NULL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|751| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access, base_gfn, fault->pfn, fault);
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			u64 *sptep, unsigned int pte_access, gfn_t gfn,
 			kvm_pfn_t pfn, struct kvm_page_fault *fault)
@@ -2915,6 +3101,9 @@ static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 	bool wrprot;
 	u64 spte;
 
+	/*
+	 * 如果没kvm_page_fault肯定是host writable
+	 */
 	/* Prefetching always gets a writable pfn.  */
 	bool host_writable = !fault || fault->map_writable;
 	bool prefetch = !fault || fault->prefetch;
@@ -3234,6 +3423,10 @@ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_
 	}
 }
 
+/*
+ * called by:
+ * - arch/x86/kvm/mmu/mmu.c|4579| <<direct_page_fault>> r = direct_map(vcpu, fault);
+ */
 static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_shadow_walk_iterator it;
@@ -3308,15 +3501,51 @@ static int kvm_handle_error_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fa
 	return -EFAULT;
 }
 
+/*
+ * 在以下调用kvm_faultin_pfn():
+ *   - arch/x86/kvm/mmu/mmu.c|4532| <<direct_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|4612| <<kvm_tdp_mmu_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|815| <<FNAME(page_fault)>> r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4463| <<kvm_faultin_pfn>> return kvm_handle_noslot_fault(vcpu, fault, access);
+ *
+ * EXIT_REASON_EXCEPTION_NMI:handle_exception_nmi()
+ * -> kvm_handle_page_fault()
+ *    -> kvm_mmu_page_fault()
+ *       -> kvm_mmu_do_page_fault()
+ *          -> FNAME(page_fault)
+ *             -> kvm_faultin_pfn()
+ *                -> if (unlikely(!fault->slot)) kvm_handle_noslot_fault()
+ *                   -> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+ * shadow page table第一次page fault的时候,
+ * 准备做成mmio
+ */
 static int kvm_handle_noslot_fault(struct kvm_vcpu *vcpu,
 				   struct kvm_page_fault *fault,
 				   unsigned int access)
 {
 	gva_t gva = fault->is_tdp ? 0 : fault->addr;
 
+	/*
+	 * 在以下设置shadow_mmio_access_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|446| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_access_mask = access_mask;
+	 * 在以下使用shadow_mmio_access_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|313| <<get_mmio_spte_access>> return spte & shadow_mmio_access_mask;
+	 *   - arch/x86/kvm/mmu/mmu.c|3340| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+	 *   - arch/x86/kvm/mmu/spte.c|106| <<make_mmio_spte>> access &= shadow_mmio_access_mask;
+	 *
+	 * 比如: ept是0
+	 * regular pte是ACC_WRITE_MASK | ACC_USER_MASK
+	 */
 	vcpu_cache_mmio_info(vcpu, gva, fault->gfn,
 			     access & shadow_mmio_access_mask);
 
+	/*
+	 * 在以下设置enable_mmio_caching:
+	 *   - arch/x86/kvm/mmu/spte.c|388| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = allow_mmio_caching;
+	 *   - arch/x86/kvm/mmu/spte.c|422| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = false;
+	 */
 	/*
 	 * If MMIO caching is disabled, emulate immediately without
 	 * touching the shadow page tables as attempting to install an
@@ -3563,6 +3792,12 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3792| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->prev_roots[i].hpa, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3799| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->root.hpa, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3805| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->pae_root[i], &invalid_list);
+ */
 static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 			       struct list_head *invalid_list)
 {
@@ -3583,6 +3818,21 @@ static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 	*root_hpa = INVALID_PAGE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3842| <<kvm_mmu_free_guest_mode_roots>> kvm_mmu_free_roots(kvm, mmu, roots_to_free);
+ *   - arch/x86/kvm/mmu/mmu.c|4252| <<kvm_mmu_sync_prev_roots>> kvm_mmu_free_roots(vcpu->kvm, vcpu->arch.mmu, roots_to_free);
+ *   - arch/x86/kvm/mmu/mmu.c|4997| <<cached_root_find_and_keep_current>> kvm_mmu_free_roots(kvm, mmu, KVM_MMU_ROOT_CURRENT);
+ *   - arch/x86/kvm/mmu/mmu.c|5037| <<fast_pgd_switch>> kvm_mmu_free_roots(kvm, mmu, KVM_MMU_ROOT_CURRENT);
+ *   - arch/x86/kvm/mmu/mmu.c|6149| <<kvm_mmu_unload>> kvm_mmu_free_roots(kvm, &vcpu->arch.root_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|6151| <<kvm_mmu_unload>> kvm_mmu_free_roots(kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|6197| <<__kvm_mmu_free_obsolete_roots>> kvm_mmu_free_roots(kvm, mmu, roots_to_free);
+ *   - arch/x86/kvm/vmx/nested.c|361| <<free_nested>> kvm_mmu_free_roots(vcpu->kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/vmx/nested.c|5344| <<nested_release_vmcs12>> kvm_mmu_free_roots(vcpu->kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/vmx/nested.c|5818| <<handle_invept>> kvm_mmu_free_roots(vcpu->kvm, mmu, roots_to_free);
+ *   - arch/x86/kvm/x86.c|909| <<load_pdptrs>> kvm_mmu_free_roots(vcpu->kvm, mmu, KVM_MMU_ROOT_CURRENT);
+ *   - arch/x86/kvm/x86.c|1275| <<kvm_invalidate_pcid>> kvm_mmu_free_roots(vcpu->kvm, mmu, roots_to_free);
+ */
 /* roots_to_free must be some combination of the KVM_MMU_ROOT_* flags */
 void kvm_mmu_free_roots(struct kvm *kvm, struct kvm_mmu *mmu,
 			ulong roots_to_free)
@@ -3667,6 +3917,13 @@ void kvm_mmu_free_guest_mode_roots(struct kvm *kvm, struct kvm_mmu *mmu)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_free_guest_mode_roots);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3906| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+ *   - arch/x86/kvm/mmu/mmu.c|3917| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+ *   - arch/x86/kvm/mmu/mmu.c|4046| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+ *   - arch/x86/kvm/mmu/mmu.c|4100| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+ */
 static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, int quadrant,
 			    u8 level)
 {
@@ -3685,6 +3942,10 @@ static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, int quadrant,
 	return __pa(sp->spt);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6119| <<kvm_mmu_load>> r = mmu_alloc_direct_roots(vcpu);
+ */
 static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -3793,6 +4054,10 @@ static int mmu_first_shadow_root_alloc(struct kvm *kvm)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6154| <<kvm_mmu_load>> r = mmu_alloc_shadow_roots(vcpu);
+ */
 static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -4081,6 +4346,11 @@ static gpa_t nonpaging_gva_to_gpa(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 	return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4324| <<handle_mmio_page_fault>> if (mmio_info_in_cache(vcpu, addr, direct))
+ *   - arch/x86/kvm/mmu/mmu.c|6358| <<kvm_mmu_page_fault>> if (!mmio_info_in_cache(vcpu, cr2_or_gpa, direct) && !is_guest_mode(vcpu))
+ */
 static bool mmio_info_in_cache(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 {
 	/*
@@ -4121,6 +4391,10 @@ static int get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes, int *root_level
 	return leaf;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4321| <<handle_mmio_page_fault>> reserved = get_mmio_spte(vcpu, addr, &spte);
+ */
 /* return true if reserved bit(s) are detected on a valid, non-MMIO SPTE. */
 static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 {
@@ -4171,6 +4445,13 @@ static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 	return reserved;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5942| <<kvm_mmu_page_fault>> if (unlikely(error_code & PFERR_RSVD_MASK)) r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
+ *   - arch/x86/kvm/mmu/mmutrace.h|234| <<__field>> handle_mmio_page_fault,
+ *
+ * 处理mmio, 来自ept的misconfig或者regular的reserved fault
+ */
 static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 {
 	u64 spte;
@@ -4179,12 +4460,31 @@ static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 	if (mmio_info_in_cache(vcpu, addr, direct))
 		return RET_PF_EMULATE;
 
+	/*
+	 * spte是host shadow page table的
+	 */
 	reserved = get_mmio_spte(vcpu, addr, &spte);
 	if (WARN_ON_ONCE(reserved))
 		return -EINVAL;
 
 	if (is_mmio_spte(spte)) {
 		gfn_t gfn = get_mmio_spte_gfn(spte);
+		/*
+		 * 4af7715110a2617fc40ac2c1232f664019269f3a
+		 * KVM: x86/mmu: Add explicit access mask for MMIO SPTEs
+		 *
+		 * commit的一些注释:
+		 * When shadow paging is enabled, KVM tracks the allowed access type for
+		 * MMIO SPTEs so that it can do a permission check on a MMIO GVA cache hit
+		 * without having to walk the guest's page tables.  The tracking is done
+		 * by retaining the WRITE and USER bits of the access when inserting the
+		 * MMIO SPTE (read access is implicitly allowed), which allows the MMIO
+		 * page fault handler to retrieve and cache the WRITE/USER bits from the
+		 * SPTE. --> "retaining the WRITE and USER bits"是指在host的shadow table保留吗???
+		 *
+		 * 比如: ept是0
+		 * regular pte是ACC_WRITE_MASK | ACC_USER_MASK
+		 */
 		unsigned int access = get_mmio_spte_access(spte);
 
 		if (!check_mmio_spte(vcpu, spte))
@@ -4194,6 +4494,11 @@ static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 			addr = 0;
 
 		trace_handle_mmio_page_fault(addr, gfn, access);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3339| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+		 *   - arch/x86/kvm/mmu/mmu.c|4219| <<handle_mmio_page_fault>> vcpu_cache_mmio_info(vcpu, addr, gfn, access);
+		 */
 		vcpu_cache_mmio_info(vcpu, addr, gfn, access);
 		return RET_PF_EMULATE;
 	}
@@ -4397,6 +4702,12 @@ static int __kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	return RET_PF_CONTINUE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4532| <<direct_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|4612| <<kvm_tdp_mmu_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|815| <<FNAME(page_fault)>> r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
+ */
 static int kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 			   unsigned int access)
 {
@@ -4437,6 +4748,12 @@ static int kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	if (unlikely(is_error_pfn(fault->pfn)))
 		return kvm_handle_error_pfn(vcpu, fault);
 
+	/*
+	 * 只在此处调用:
+	 *   - arch/x86/kvm/mmu/mmu.c|4463| <<kvm_faultin_pfn>> return kvm_handle_noslot_fault(vcpu, fault, access);
+	 *
+	 * 就在这里使用access
+	 */
 	if (unlikely(!fault->slot))
 		return kvm_handle_noslot_fault(vcpu, fault, access);
 
@@ -4537,6 +4854,12 @@ static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
 	return direct_page_fault(vcpu, fault);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2045| <<pf_interception>> return kvm_handle_page_fault(vcpu, error_code, fault_address, static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+ *                                                                  svm->vmcb->control.insn_bytes : NULL, svm->vmcb->control.insn_len);
+ *   - arch/x86/kvm/vmx/vmx.c|5263| <<handle_exception_nmi>> return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
+ */
 int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 				u64 fault_address, char *insn, int insn_len)
 {
@@ -4551,6 +4874,12 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 
 	vcpu->arch.l1tf_flush_l1d = true;
 	if (!flags) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4686| <<kvm_handle_page_fault>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+		 *   - arch/x86/kvm/svm/svm.c|2058| <<npf_interception>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+		 *   - arch/x86/kvm/vmx/vmx.c|5765| <<handle_ept_violation>> trace_kvm_page_fault(vcpu, gpa, exit_qualification);
+		 */
 		trace_kvm_page_fault(vcpu, fault_address, error_code);
 
 		if (kvm_event_needs_reinjection(vcpu))
@@ -4571,6 +4900,10 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 EXPORT_SYMBOL_GPL(kvm_handle_page_fault);
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用kvm_tdp_mmu_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|4860| <<kvm_tdp_page_fault>> return kvm_tdp_mmu_page_fault(vcpu, fault);
+ */
 static int kvm_tdp_mmu_page_fault(struct kvm_vcpu *vcpu,
 				  struct kvm_page_fault *fault)
 {
@@ -4606,6 +4939,11 @@ static int kvm_tdp_mmu_page_fault(struct kvm_vcpu *vcpu,
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|258| <<kvm_mmu_honors_guest_mtrrs>> return __kvm_mmu_honors_guest_mtrrs(kvm_arch_has_noncoherent_dma(kvm));
+ *   - arch/x86/kvm/x86.c|13431| <<kvm_noncoherent_dma_assignment_start_or_stop>> if (__kvm_mmu_honors_guest_mtrrs(true))
+ */
 bool __kvm_mmu_honors_guest_mtrrs(bool vm_has_noncoherent_dma)
 {
 	/*
@@ -4617,9 +4955,22 @@ bool __kvm_mmu_honors_guest_mtrrs(bool vm_has_noncoherent_dma)
 	 * Note, KVM may still ultimately ignore guest MTRRs for certain PFNs,
 	 * e.g. KVM will force UC memtype for host MMIO.
 	 */
+	/*
+	 * 在以下使用shadow_memtype_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|4625| <<__kvm_mmu_honors_guest_mtrrs>> return vm_has_noncoherent_dma && shadow_memtype_mask;
+	 *   - arch/x86/kvm/mmu/spte.c|192| <<make_spte>> if (shadow_memtype_mask)
+	 *   - arch/x86/kvm/mmu/spte.c|439| <<kvm_mmu_set_ept_masks>> shadow_memtype_mask = VMX_EPT_MT_MASK | VMX_EPT_IPAT_BIT;
+	 *   - arch/x86/kvm/mmu/spte.c|496| <<kvm_mmu_reset_all_pte_masks>> shadow_memtype_mask = 0;
+	 */
 	return vm_has_noncoherent_dma && shadow_memtype_mask;
 }
 
+/*
+ * 在以下使用kvm_tdp_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|5361| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ *   - arch/x86/kvm/mmu/mmu_internal.h|294| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> r = kvm_tdp_page_fault(vcpu, &fault);
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	/*
@@ -4628,11 +4979,33 @@ int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	 * across the entire mapping.
 	 */
 	if (kvm_mmu_honors_guest_mtrrs(vcpu->kvm)) {
+		/*
+		 * enum pg_level {
+		 *     PG_LEVEL_NONE,
+		 *     PG_LEVEL_4K,
+		 *     PG_LEVEL_2M,
+		 *     PG_LEVEL_1G,
+		 *     PG_LEVEL_512G,
+		 *     PG_LEVEL_NUM
+		 * };
+		 *
+		 * 这里会减少fault->max_level
+		 */
 		for ( ; fault->max_level > PG_LEVEL_4K; --fault->max_level) {
+			/*
+			 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+			 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+			 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+			 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+			 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+			 */
 			int page_num = KVM_PAGES_PER_HPAGE(fault->max_level);
 			gfn_t base = gfn_round_for_level(fault->gfn,
 							 fault->max_level);
 
+			/*
+			 * 返回true就是不做什么
+			 */
 			if (kvm_mtrr_check_gfn_range_consistency(vcpu, base, page_num))
 				break;
 		}
@@ -4646,6 +5019,10 @@ int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	return direct_page_fault(vcpu, fault);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5577| <<shadow_mmu_init_context>> nonpaging_init_context(context);
+ */
 static void nonpaging_init_context(struct kvm_mmu *context)
 {
 	context->page_fault = nonpaging_page_fault;
@@ -4750,6 +5127,14 @@ static bool fast_pgd_switch(struct kvm *kvm, struct kvm_mmu *mmu,
 		return cached_root_find_without_current(kvm, mmu, new_pgd, new_role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5904| <<kvm_init_shadow_npt_mmu>> kvm_mmu_new_pgd(vcpu, nested_cr3);
+ *   - arch/x86/kvm/mmu/mmu.c|5962| <<kvm_init_shadow_ept_mmu>> kvm_mmu_new_pgd(vcpu, new_eptp);
+ *   - arch/x86/kvm/svm/nested.c|523| <<nested_svm_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ *   - arch/x86/kvm/vmx/nested.c|1155| <<nested_vmx_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ *   - arch/x86/kvm/x86.c|1306| <<kvm_set_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ */
 void kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -4796,6 +5181,10 @@ void kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_new_pgd);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1288| <<FNAME(sync_spte)>> if (sync_mmio_spte(vcpu, &sp->spt[i], gfn, pte_access))
+ */
 static bool sync_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,
 			   unsigned int access)
 {
@@ -4825,6 +5214,12 @@ static bool sync_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,
 #include "paging_tmpl.h"
 #undef PTTYPE
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5073| <<reset_guest_rsvds_bits_mask>> __reset_rsvds_bits_mask(&context->guest_rsvd_check,
+ *   - arch/x86/kvm/mmu/mmu.c|5150| <<reset_shadow_zero_bits_mask>> __reset_rsvds_bits_mask(shadow_zero_check, reserved_hpa_bits(),
+ *   - arch/x86/kvm/mmu/mmu.c|5192| <<reset_tdp_shadow_zero_bits_mask>> __reset_rsvds_bits_mask(shadow_zero_check, reserved_hpa_bits(),
+ */
 static void __reset_rsvds_bits_mask(struct rsvd_bits_validate *rsvd_check,
 				    u64 pa_bits_rsvd, int level, bool nx,
 				    bool gbpages, bool pse, bool amd)
@@ -4914,6 +5309,10 @@ static void __reset_rsvds_bits_mask(struct rsvd_bits_validate *rsvd_check,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5378| <<reset_guest_paging_metadata>> reset_guest_rsvds_bits_mask(vcpu, mmu);
+ */
 static void reset_guest_rsvds_bits_mask(struct kvm_vcpu *vcpu,
 					struct kvm_mmu *context)
 {
@@ -4925,6 +5324,12 @@ static void reset_guest_rsvds_bits_mask(struct kvm_vcpu *vcpu,
 				guest_cpuid_is_amd_or_hygon(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5122| <<reset_rsvds_bits_mask_ept>> __reset_rsvds_bits_mask_ept(&context->guest_rsvd_check,
+ *   - arch/x86/kvm/mmu/mmu.c|5197| <<reset_tdp_shadow_zero_bits_mask>> __reset_rsvds_bits_mask_ept(shadow_zero_check,
+ *   - arch/x86/kvm/mmu/mmu.c|5217| <<reset_ept_shadow_zero_bits_mask>> __reset_rsvds_bits_mask_ept(&context->shadow_zero_check,
+ */
 static void __reset_rsvds_bits_mask_ept(struct rsvd_bits_validate *rsvd_check,
 					u64 pa_bits_rsvd, bool execonly,
 					int huge_page_level)
@@ -4963,6 +5368,10 @@ static void __reset_rsvds_bits_mask_ept(struct rsvd_bits_validate *rsvd_check,
 	rsvd_check->bad_mt_xwr = bad_mt_xwr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5632| <<kvm_init_shadow_ept_mmu>> reset_rsvds_bits_mask_ept(vcpu, context, execonly, huge_page_level);
+ */
 static void reset_rsvds_bits_mask_ept(struct kvm_vcpu *vcpu,
 		struct kvm_mmu *context, bool execonly, int huge_page_level)
 {
@@ -4981,6 +5390,10 @@ static inline u64 reserved_hpa_bits(void)
  * table in guest or amd nested guest, its mmu features completely
  * follow the features in guest.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5532| <<shadow_mmu_init_context>> reset_shadow_zero_bits_mask(vcpu, context);
+ */
 static void reset_shadow_zero_bits_mask(struct kvm_vcpu *vcpu,
 					struct kvm_mmu *context)
 {
@@ -5076,6 +5489,11 @@ reset_ept_shadow_zero_bits_mask(struct kvm_mmu *context, bool execonly)
 	 (7 & (access) ? 128 : 0))
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5473| <<reset_guest_paging_metadata>> update_permission_bitmask(mmu, false);
+ *   - arch/x86/kvm/mmu/mmu.c|5778| <<kvm_init_shadow_ept_mmu>> update_permission_bitmask(context, true);
+ */
 static void update_permission_bitmask(struct kvm_mmu *mmu, bool ept)
 {
 	unsigned byte;
@@ -5084,12 +5502,60 @@ static void update_permission_bitmask(struct kvm_mmu *mmu, bool ept)
 	const u8 w = BYTE_MASK(ACC_WRITE_MASK);
 	const u8 u = BYTE_MASK(ACC_USER_MASK);
 
+	/*
+	 * 用的cpu_role
+	 */
 	bool cr4_smep = is_cr4_smep(mmu);
 	bool cr4_smap = is_cr4_smap(mmu);
 	bool cr0_wp = is_cr0_wp(mmu);
 	bool efer_nx = is_efer_nx(mmu);
 
+	/*
+	 * #define PFERR_PRESENT_BIT 0
+	 * #define PFERR_WRITE_BIT 1
+	 * #define PFERR_USER_BIT 2
+	 * #define PFERR_RSVD_BIT 3
+	 * #define PFERR_FETCH_BIT 4
+	 * #define PFERR_PK_BIT 5
+	 * #define PFERR_SGX_BIT 15
+	 * #define PFERR_GUEST_FINAL_BIT 32
+	 * #define PFERR_GUEST_PAGE_BIT 33
+	 * #define PFERR_IMPLICIT_ACCESS_BIT 48
+	 *
+	 * 在以下使用kvm_mmu->permissions[16] (u8)
+	 *   - arch/x86/include/asm/kvm_host.h|556| <<global>> u8 permissions[16];
+	 *   - arch/x86/kvm/mmu.h|246| <<permission_fault>> fault = (mmu->permissions[index] >> pte_access) & 1;
+	 *   - arch/x86/kvm/mmu/mmu.c|5332| <<update_permission_bitmask>> for (byte = 0; byte < ARRAY_SIZE(mmu->permissions); ++byte) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5387| <<update_permission_bitmask>> mmu->permissions[byte] = ff | uf | wf | smepf | smapf;
+	 *   - arch/x86/kvm/mmu/mmu.c|5427| <<update_pkru_bitmask>> for (bit = 0; bit < ARRAY_SIZE(mmu->permissions); ++bit) {
+	 *
+	 * 注释
+	 * // Bitmap; bit set = permission fault
+	 * // Byte index: page fault error code [4:1]
+	 * // Bit index: pte permissions in ACC_* format
+	 * u8 permissions[16];
+	 */
 	for (byte = 0; byte < ARRAY_SIZE(mmu->permissions); ++byte) {
+		/*
+		 * byte=0, pfec=0 -->    0
+		 * byte=1, pfec=2 -->   10
+		 * byte=2, pfec=4 -->  100
+		 * byte=3, pfec=6 -->  110
+		 * byte=4, pfec=8 --> 1000
+		 * byte=5, pfec=10 --> 
+		 * byte=6, pfec=12
+		 * byte=7, pfec=14
+		 * byte=8, pfec=16
+		 * byte=9, pfec=18
+		 * byte=10, pfec=20
+		 * byte=11, pfec=22 --> 10110
+		 * byte=12, pfec=24 --> 11000
+		 * byte=13, pfec=26 --> 11010
+		 * byte=14, pfec=28 --> 11100
+		 * byte=15, pfec=30 --> 11110
+		 *
+		 * pfec: page fault error code
+		 */
 		unsigned pfec = byte << 1;
 
 		/*
@@ -5216,6 +5682,13 @@ static void update_pkru_bitmask(struct kvm_mmu *mmu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5449| <<__kvm_mmu_refresh_passthrough_bits>> reset_guest_paging_metadata(vcpu, mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|5509| <<init_kvm_tdp_mmu>> reset_guest_paging_metadata(vcpu, context);
+ *   - arch/x86/kvm/mmu/mmu.c|5531| <<shadow_mmu_init_context>> reset_guest_paging_metadata(vcpu, context);
+ *   - arch/x86/kvm/mmu/mmu.c|5688| <<init_kvm_nested_mmu>> reset_guest_paging_metadata(vcpu, g_context);
+ */
 static void reset_guest_paging_metadata(struct kvm_vcpu *vcpu,
 					struct kvm_mmu *mmu)
 {
@@ -5227,6 +5700,10 @@ static void reset_guest_paging_metadata(struct kvm_vcpu *vcpu,
 	update_pkru_bitmask(mmu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5562| <<shadow_mmu_init_context>> else if (is_cr4_pae(context)) paging64_init_context(context);
+ */
 static void paging64_init_context(struct kvm_mmu *context)
 {
 	context->page_fault = paging64_page_fault;
@@ -5234,6 +5711,10 @@ static void paging64_init_context(struct kvm_mmu *context)
 	context->sync_spte = paging64_sync_spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5564| <<shadow_mmu_init_context>> else paging32_init_context(context);
+ */
 static void paging32_init_context(struct kvm_mmu *context)
 {
 	context->page_fault = paging32_page_fault;
@@ -5241,9 +5722,23 @@ static void paging32_init_context(struct kvm_mmu *context)
 	context->sync_spte = paging32_sync_spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5645| <<kvm_init_shadow_npt_mmu>> union kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &regs);
+ *   - arch/x86/kvm/mmu/mmu.c|5790| <<kvm_init_mmu>> union kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &regs);
+ */
 static union kvm_cpu_role kvm_calc_cpu_role(struct kvm_vcpu *vcpu,
 					    const struct kvm_mmu_role_regs *regs)
 {
+	/*
+	 * 451 union kvm_cpu_role {
+	 * 452         u64 as_u64;
+	 * 453         struct {
+	 * 454                 union kvm_mmu_page_role base;
+	 * 455                 union kvm_mmu_extended_role ext;
+	 * 456         };
+	 * 457 };
+	 */
 	union kvm_cpu_role role = {0};
 
 	role.base.access = ACC_ALL;
@@ -5260,6 +5755,11 @@ static union kvm_cpu_role kvm_calc_cpu_role(struct kvm_vcpu *vcpu,
 	role.base.cr0_wp = ____is_cr0_wp(regs);
 	role.base.smep_andnot_wp = ____is_cr4_smep(regs) && !____is_cr0_wp(regs);
 	role.base.smap_andnot_wp = ____is_cr4_smap(regs) && !____is_cr0_wp(regs);
+	/*
+	 * 注释:
+	 * Reflects the size of the guest PTE for which the page is valid, i.e. '0'
+	 * if direct map or 64-bit gptes are in use, '1' if 32-bit gptes are in use.
+	 */
 	role.base.has_4_byte_gpte = !____is_cr4_pae(regs);
 
 	if (____is_efer_lma(regs))
@@ -5281,9 +5781,45 @@ static union kvm_cpu_role kvm_calc_cpu_role(struct kvm_vcpu *vcpu,
 	return role;
 }
 
+/*
+ * 关于cr0/cr4的passthrough:
+ * VM-execution control fields include guest/host masks and read shadows for the CR0 and CR4 registers. These
+ * fields control executions of instructions that access those registers (including CLTS, LMSW, MOV CR, and SMSW).
+ * They are 64 bits on processors that support Intel 64 architecture and 32 bits on processors that do not.
+ *
+ * In general, bits set to 1 in a guest/host mask correspond to bits "owned" by the host:
+ *
+ * 1. Guest attempts to set them (using CLTS, LMSW, or MOV to CR) to values differing from the corresponding bits
+ * in the corresponding read shadow cause VM exits.
+ *
+ * 2. Guest reads (using MOV from CR or SMSW) return values for these bits from the corresponding read shadow.
+ *
+ * Bits cleared to 0 correspond to bits "owned" by the guest; guest
+ * attempts to modify them succeed and guest reads return values for these bits from the control register itself.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu.h|202| <<kvm_mmu_refresh_passthrough_bits>> __kvm_mmu_refresh_passthrough_bits(vcpu, mmu);
+ */
 void __kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,
 					struct kvm_mmu *mmu)
 {
+	/*
+	 *  223 #define BUILD_MMU_ROLE_ACCESSOR(base_or_ext, reg, name)         \
+	 *  224 static inline bool __maybe_unused is_##reg##_##name(struct kvm_mmu *mmu)        \
+	 *  225 {                                                               \
+	 *  226         return !!(mmu->cpu_role. base_or_ext . reg##_##name);   \
+	 *  227 }
+	 *
+	 * CR0.WP
+	 * Write Protect (bit 16 of CR0) - When set, inhibits supervisor-level
+	 * procedures from writing into read-only pages; when clear, allows
+	 * supervisor-level procedures to write into read-only pages (regardless
+	 * of the U/S bit setting; see Section 4.1.3 and Section 4.6). This flag
+	 * facilitates implementation of the copy-on-write method of creating a
+	 * new process (forking) used by operating systems such as UNIX. This flag
+	 * must be set before software can set CR4.CET, and it cannot be cleared
+	 * as long as CR4.CET = 1 (see below).
+	 */
 	const bool cr0_wp = kvm_is_cr0_bit_set(vcpu, X86_CR0_WP);
 
 	BUILD_BUG_ON((KVM_MMU_CR0_ROLE_BITS & KVM_POSSIBLE_CR0_GUEST_BITS) != X86_CR0_WP);
@@ -5292,6 +5828,22 @@ void __kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,
 	if (is_cr0_wp(mmu) == cr0_wp)
 		return;
 
+	/*
+	 * 在以下使用kvm_mmu_page_role->cr0_wp:
+	 *   - arch/x86/kvm/mmu/mmu.c|5329| <<update_permission_bitmask>> bool cr0_wp = is_cr0_wp(mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|5360| <<update_permission_bitmask>> if (!cr0_wp)
+	 *   - arch/x86/kvm/mmu/mmu.c|5520| <<kvm_calc_cpu_role>> role.base.cr0_wp = ____is_cr0_wp(regs);
+	 *   - arch/x86/kvm/mmu/mmu.c|5547| <<__kvm_mmu_refresh_passthrough_bits>> const bool cr0_wp = kvm_is_cr0_bit_set(vcpu, X86_CR0_WP);
+	 *   - arch/x86/kvm/mmu/mmu.c|5552| <<__kvm_mmu_refresh_passthrough_bits>> if (is_cr0_wp(mmu) == cr0_wp)
+	 *   - arch/x86/kvm/mmu/mmu.c|5555| <<__kvm_mmu_refresh_passthrough_bits>> mmu->cpu_role.base.cr0_wp = cr0_wp;
+	 *   - arch/x86/kvm/mmu/mmu.c|5579| <<kvm_calc_tdp_mmu_root_page_role>> role.cr0_wp = true;
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|5449| <<__kvm_mmu_refresh_passthrough_bits>> reset_guest_paging_metadata(vcpu, mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|5509| <<init_kvm_tdp_mmu>> reset_guest_paging_metadata(vcpu, context);
+	 *   - arch/x86/kvm/mmu/mmu.c|5531| <<shadow_mmu_init_context>> reset_guest_paging_metadata(vcpu, context);
+	 *   - arch/x86/kvm/mmu/mmu.c|5688| <<init_kvm_nested_mmu>> reset_guest_paging_metadata(vcpu, g_context);
+	 */
 	mmu->cpu_role.base.cr0_wp = cr0_wp;
 	reset_guest_paging_metadata(vcpu, mmu);
 }
@@ -5309,6 +5861,22 @@ static inline int kvm_mmu_get_tdp_level(struct kvm_vcpu *vcpu)
 	return max_tdp_level;
 }
 
+/*
+ * kvm_init_mmu()
+ * -> init_kvm_tdp_mmu()
+ *    -> kvm_calc_tdp_mmu_root_page_role()
+ *
+ * 每次更新完, 对tdp, root_role(mmu_role)只有下面的更新依赖cpu_role:
+ * - role.smm = cpu_role.base.smm;
+ * - role.guest_mode = cpu_role.base.guest_mode;
+ *
+ * 参考:
+ * [PATCH v2 00/18] KVM: MMU: do not unload MMU roots on all role changes
+ * https://lore.kernel.org/kvm/20220217210340.312449-1-pbonzini@redhat.com/
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5810| <<init_kvm_tdp_mmu>> union kvm_mmu_page_role root_role = kvm_calc_tdp_mmu_root_page_role(vcpu, cpu_role);
+ */
 static union kvm_mmu_page_role
 kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu,
 				union kvm_cpu_role cpu_role)
@@ -5318,28 +5886,91 @@ kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu,
 	role.access = ACC_ALL;
 	role.cr0_wp = true;
 	role.efer_nx = true;
+	/*
+	 * 每次更新了只有下面的不变
+	 */
 	role.smm = cpu_role.base.smm;
 	role.guest_mode = cpu_role.base.guest_mode;
 	role.ad_disabled = !kvm_ad_enabled();
 	role.level = kvm_mmu_get_tdp_level(vcpu);
 	role.direct = true;
+	/*
+	 * 注释:
+	 * Reflects the size of the guest PTE for which the page is valid, i.e. '0'
+	 * if direct map or 64-bit gptes are in use, '1' if 32-bit gptes are in use.
+	 */
 	role.has_4_byte_gpte = false;
 
 	return role;
 }
 
+/*
+ * 453 union kvm_cpu_role {
+ * 454         u64 as_u64;
+ * 455         struct {
+ * 456                 union kvm_mmu_page_role base;
+ * 457                 union kvm_mmu_extended_role ext;
+ * 458         };
+ * 459 };
+ *
+ *
+ * kvm_init_mmu()
+ * -> init_kvm_tdp_mmu()
+ *    -> kvm_calc_tdp_mmu_root_page_role()
+ *
+ * 每次更新完, 对tdp, root_role(mmu_role)只有下面的更新依赖cpu_role:
+ * - role.smm = cpu_role.base.smm;
+ * - role.guest_mode = cpu_role.base.guest_mode;
+ *
+ * 参考:
+ * [PATCH v2 00/18] KVM: MMU: do not unload MMU roots on all role changes
+ * https://lore.kernel.org/kvm/20220217210340.312449-1-pbonzini@redhat.com/
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5734| <<kvm_init_mmu>> init_kvm_tdp_mmu(vcpu, cpu_role);
+ */
 static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu,
 			     union kvm_cpu_role cpu_role)
 {
 	struct kvm_mmu *context = &vcpu->arch.root_mmu;
+	/*
+	 * 根据cpu_role, 生成root_role/mmu_role
+	 *
+	 * 只在此处调用
+	 */
 	union kvm_mmu_page_role root_role = kvm_calc_tdp_mmu_root_page_role(vcpu, cpu_role);
 
 	if (cpu_role.as_u64 == context->cpu_role.as_u64 &&
 	    root_role.word == context->root_role.word)
 		return;
 
+	/*
+	 * 在以下整个单位修改kvm_cpu_role->as_u64:
+	 *   - arch/x86/kvm/mmu/mmu.c|5889| <<init_kvm_tdp_mmu>> context->cpu_role.as_u64 = cpu_role.as_u64;
+	 *   - arch/x86/kvm/mmu/mmu.c|5936| <<shadow_mmu_init_context>> context->cpu_role.as_u64 = cpu_role.as_u64;
+	 *   - arch/x86/kvm/mmu/mmu.c|6062| <<kvm_init_shadow_ept_mmu>> context->cpu_role.as_u64 = new_mode.as_u64;
+	 *   - arch/x86/kvm/mmu/mmu.c|6115| <<init_kvm_nested_mmu>> g_context->cpu_role.as_u64 = new_mode.as_u64;
+	 */
 	context->cpu_role.as_u64 = cpu_role.as_u64;
+	/*
+	 * 在下面修改kvm_mmu_page_role->word整个单位:
+	 *   - arch/x86/kvm/mmu/mmu.c|5890| <<init_kvm_tdp_mmu>> context->root_role.word = root_role.word;
+	 *   - arch/x86/kvm/mmu/mmu.c|5937| <<shadow_mmu_init_context>> context->root_role.word = root_role.word;
+	 *   - arch/x86/kvm/mmu/mmu.c|6063| <<kvm_init_shadow_ept_mmu>> context->root_role.word = new_mode.base.word;
+	 *   - arch/x86/kvm/mmu/mmu.c|6193| <<kvm_mmu_after_set_cpuid>> vcpu->arch.root_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|6194| <<kvm_mmu_after_set_cpuid>> vcpu->arch.guest_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|6195| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|32| <<KVM_MMU_PAGE_PRINTK>> role.word = __entry->role; \
+	 */
 	context->root_role.word = root_role.word;
+	/*
+	 * 在以下设置kvm_mmu->page_fault:
+	 *   - arch/x86/kvm/mmu/mmu.c|4808| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5424| <<paging64_init_context>> context->page_fault = paging64_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5435| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5543| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5702| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault; --> 用的vcpu->arch.guest_mmu
+	 */
 	context->page_fault = kvm_tdp_page_fault;
 	context->sync_spte = NULL;
 	context->get_guest_pgd = get_guest_cr3;
@@ -5353,10 +5984,26 @@ static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu,
 	else
 		context->gva_to_gpa = paging32_gva_to_gpa;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|5449| <<__kvm_mmu_refresh_passthrough_bits>> reset_guest_paging_metadata(vcpu, mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|5509| <<init_kvm_tdp_mmu>> reset_guest_paging_metadata(vcpu, context);
+	 *   - arch/x86/kvm/mmu/mmu.c|5531| <<shadow_mmu_init_context>> reset_guest_paging_metadata(vcpu, context);
+	 *   - arch/x86/kvm/mmu/mmu.c|5688| <<init_kvm_nested_mmu>> reset_guest_paging_metadata(vcpu, g_context);
+	 */
 	reset_guest_paging_metadata(vcpu, context);
 	reset_tdp_shadow_zero_bits_mask(context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5592| <<kvm_init_shadow_mmu>> shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
+ *   - arch/x86/kvm/mmu/mmu.c|5616| <<kvm_init_shadow_npt_mmu>> shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
+ *
+ * 会直接拷贝root_role和cpu_role, 不修改:
+ *   - context->cpu_role.as_u64 = cpu_role.as_u64;
+ *   - context->root_role.word = root_role.word;
+ */
 static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *context,
 				    union kvm_cpu_role cpu_role,
 				    union kvm_mmu_page_role root_role)
@@ -5366,6 +6013,16 @@ static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *conte
 		return;
 
 	context->cpu_role.as_u64 = cpu_role.as_u64;
+	/*
+	 * 在下面修改kvm_mmu_page_role->word整个单位:
+	 *   - arch/x86/kvm/mmu/mmu.c|5890| <<init_kvm_tdp_mmu>> context->root_role.word = root_role.word;
+	 *   - arch/x86/kvm/mmu/mmu.c|5937| <<shadow_mmu_init_context>> context->root_role.word = root_role.word;
+	 *   - arch/x86/kvm/mmu/mmu.c|6063| <<kvm_init_shadow_ept_mmu>> context->root_role.word = new_mode.base.word;
+	 *   - arch/x86/kvm/mmu/mmu.c|6193| <<kvm_mmu_after_set_cpuid>> vcpu->arch.root_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|6194| <<kvm_mmu_after_set_cpuid>> vcpu->arch.guest_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|6195| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|32| <<KVM_MMU_PAGE_PRINTK>> role.word = __entry->role; \
+	 */
 	context->root_role.word = root_role.word;
 
 	if (!is_cr0_pg(context))
@@ -5375,16 +6032,46 @@ static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *conte
 	else
 		paging32_init_context(context);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|5449| <<__kvm_mmu_refresh_passthrough_bits>> reset_guest_paging_metadata(vcpu, mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|5509| <<init_kvm_tdp_mmu>> reset_guest_paging_metadata(vcpu, context);
+	 *   - arch/x86/kvm/mmu/mmu.c|5531| <<shadow_mmu_init_context>> reset_guest_paging_metadata(vcpu, context);
+	 *   - arch/x86/kvm/mmu/mmu.c|5688| <<init_kvm_nested_mmu>> reset_guest_paging_metadata(vcpu, g_context);
+	 */
 	reset_guest_paging_metadata(vcpu, context);
 	reset_shadow_zero_bits_mask(vcpu, context);
 }
 
+/*
+ * union kvm_cpu_role {
+ *     u64 as_u64;
+ *     struct {
+ *         union kvm_mmu_page_role base;
+ *         union kvm_mmu_extended_role ext;
+ *     };
+ * };
+ *
+ * 普通的shadow_mmu会在cpu_role的基础上修改下面的:
+ *   - root_role = cpu_role.base; --> 先整个拷贝
+ *   - root_role.level = max_t(u32, root_role.level, PT32E_ROOT_LEVEL); --> 根据cpu_role调整level
+ * 所以, 等同于全部修改!!!
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5680| <<init_kvm_softmmu>> kvm_init_shadow_mmu(vcpu, cpu_role);
+ */
 static void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu,
 				union kvm_cpu_role cpu_role)
 {
 	struct kvm_mmu *context = &vcpu->arch.root_mmu;
 	union kvm_mmu_page_role root_role;
 
+	/*
+	 * 普通的shadow_mmu会在cpu_role的基础上修改下面的:
+	 *   - root_role = cpu_role.base; --> 先整个拷贝
+	 *   - root_role.level = max_t(u32, root_role.level, PT32E_ROOT_LEVEL); --> 根据cpu_role调整level
+	 * 所以, 等同于全部修改!!!
+	 */
 	root_role = cpu_role.base;
 
 	/* KVM uses PAE paging whenever the guest isn't using 64-bit paging. */
@@ -5401,9 +6088,22 @@ static void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu,
 	 */
 	root_role.efer_nx = true;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|5592| <<kvm_init_shadow_mmu>> shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
+	 *   - arch/x86/kvm/mmu/mmu.c|5616| <<kvm_init_shadow_npt_mmu>> shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
+	 *
+	 * 会直接拷贝root_role和cpu_role, 不修改:
+	 *   - context->cpu_role.as_u64 = cpu_role.as_u64;
+	 *   - context->root_role.word = root_role.word;
+	 */
 	shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|93| <<nested_svm_init_mmu_context>> kvm_init_shadow_npt_mmu(vcpu, X86_CR0_PG, svm->vmcb01.ptr->save.cr4, svm->vmcb01.ptr->save.efer, svm->nested.ctl.nested_cr3);
+ */
 void kvm_init_shadow_npt_mmu(struct kvm_vcpu *vcpu, unsigned long cr0,
 			     unsigned long cr4, u64 efer, gpa_t nested_cr3)
 {
@@ -5430,6 +6130,10 @@ void kvm_init_shadow_npt_mmu(struct kvm_vcpu *vcpu, unsigned long cr0,
 }
 EXPORT_SYMBOL_GPL(kvm_init_shadow_npt_mmu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6101| <<kvm_init_shadow_ept_mmu>> kvm_calc_shadow_ept_root_page_role(vcpu, accessed_dirty, execonly, level);
+ */
 static union kvm_cpu_role
 kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty,
 				   bool execonly, u8 level)
@@ -5455,6 +6159,10 @@ kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|446| <<nested_ept_new_eptp>> kvm_init_shadow_ept_mmu(vcpu, execonly, ept_lpage_level, nested_ept_ad_enabled(vcpu), nested_ept_get_eptp(vcpu));
+ */
 void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 			     int huge_page_level, bool accessed_dirty,
 			     gpa_t new_eptp)
@@ -5484,11 +6192,27 @@ void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 }
 EXPORT_SYMBOL_GPL(kvm_init_shadow_ept_mmu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5736| <<kvm_init_mmu>> init_kvm_softmmu(vcpu, cpu_role);
+ */
 static void init_kvm_softmmu(struct kvm_vcpu *vcpu,
 			     union kvm_cpu_role cpu_role)
 {
+	/*
+	 * 在以下设置kvm_mmu->page_fault:
+	 *   - arch/x86/kvm/mmu/mmu.c|4808| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5424| <<paging64_init_context>> context->page_fault = paging64_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5435| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5543| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5702| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault;
+	 */
 	struct kvm_mmu *context = &vcpu->arch.root_mmu;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|5680| <<init_kvm_softmmu>> kvm_init_shadow_mmu(vcpu, cpu_role);
+	 */
 	kvm_init_shadow_mmu(vcpu, cpu_role);
 
 	context->get_guest_pgd     = get_guest_cr3;
@@ -5496,6 +6220,26 @@ static void init_kvm_softmmu(struct kvm_vcpu *vcpu,
 	context->inject_page_fault = kvm_inject_page_fault;
 }
 
+/*
+ * 下面提到的.
+ * https://lists.openwall.net/linux-kernel/2022/04/14/251
+ *
+ * Snapshot the state of the processor registers that govern page walk into
+ * a new field of struct kvm_mmu.  This is a more natural representation
+ * than having it *mostly* in mmu_role but not exclusively; the delta
+ * right now is represented in other fields, such as root_level.
+ *
+ * The nested MMU now has only the CPU role; and in fact the new function
+ * kvm_calc_cpu_role is analogous to the previous kvm_calc_nested_mmu_role,
+ * except that it has role.base.direct equal to !CR0.PG.  For a walk-only
+ * MMU, "direct" has no meaning, but we set it to !CR0.PG so that
+ * role.ext.cr0_pg can go away in a future patch.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5732| <<kvm_init_mmu>> init_kvm_nested_mmu(vcpu, cpu_role);
+ *
+ * nested mmu只用CPU role
+ */
 static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu,
 				union kvm_cpu_role new_mode)
 {
@@ -5504,6 +6248,15 @@ static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu,
 	if (new_mode.as_u64 == g_context->cpu_role.as_u64)
 		return;
 
+	/*
+	 * 在以下整个单位修改kvm_cpu_role->as_u64:
+	 *   - arch/x86/kvm/mmu/mmu.c|5889| <<init_kvm_tdp_mmu>> context->cpu_role.as_u64 = cpu_role.as_u64;
+	 *   - arch/x86/kvm/mmu/mmu.c|5936| <<shadow_mmu_init_context>> context->cpu_role.as_u64 = cpu_role.as_u64;
+	 *   - arch/x86/kvm/mmu/mmu.c|6062| <<kvm_init_shadow_ept_mmu>> context->cpu_role.as_u64 = new_mode.as_u64;
+	 *   - arch/x86/kvm/mmu/mmu.c|6115| <<init_kvm_nested_mmu>> g_context->cpu_role.as_u64 = new_mode.as_u64;
+	 *
+	 * 这里没有提到root_role(mmu_role)
+	 */
 	g_context->cpu_role.as_u64   = new_mode.as_u64;
 	g_context->get_guest_pgd     = get_guest_cr3;
 	g_context->get_pdptr         = kvm_pdptr_read;
@@ -5535,9 +6288,29 @@ static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu,
 	reset_guest_paging_metadata(vcpu, g_context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5772| <<kvm_mmu_reset_context>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|520| <<nested_svm_load_cr3>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|1138| <<nested_vmx_load_cr3>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/x86.c|956| <<kvm_post_set_cr0>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/x86.c|12206| <<kvm_arch_vcpu_create>> kvm_init_mmu(vcpu);
+ */
 void kvm_init_mmu(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_mmu_role_regs {
+	 *     const unsigned long cr0;
+	 *     const unsigned long cr4;
+	 *     const u64 efer;
+	 * };
+	 */
 	struct kvm_mmu_role_regs regs = vcpu_to_role_regs(vcpu);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|5645| <<kvm_init_shadow_npt_mmu>> union kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &regs);
+	 *   - arch/x86/kvm/mmu/mmu.c|5790| <<kvm_init_mmu>> union kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &regs);
+	 */
 	union kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &regs);
 
 	if (mmu_is_nested(vcpu))
@@ -5549,6 +6322,10 @@ void kvm_init_mmu(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_init_mmu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|386| <<kvm_vcpu_after_set_cpuid>> kvm_mmu_after_set_cpuid(vcpu);
+ */
 void kvm_mmu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -5578,13 +6355,38 @@ void kvm_mmu_after_set_cpuid(struct kvm_vcpu *vcpu)
 	KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5760| <<kvm_mmu_after_set_cpuid>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/smm.c|132| <<kvm_smm_changed>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/smm.c|369| <<enter_smm>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4705| <<nested_vmx_restore_host_state>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|974| <<kvm_post_set_cr0>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1165| <<kvm_post_set_cr4>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1790| <<set_efer>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|11837| <<__set_sregs>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|11881| <<__set_sregs2>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|12407| <<kvm_vcpu_reset>> kvm_mmu_reset_context(vcpu);
+ */
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_unload(vcpu);
+	/*
+	 *  called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|5772| <<kvm_mmu_reset_context>> kvm_init_mmu(vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|520| <<nested_svm_load_cr3>> kvm_init_mmu(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|1138| <<nested_vmx_load_cr3>> kvm_init_mmu(vcpu);
+	 *   - arch/x86/kvm/x86.c|956| <<kvm_post_set_cr0>> kvm_init_mmu(vcpu);
+	 *   - arch/x86/kvm/x86.c|12206| <<kvm_arch_vcpu_create>> kvm_init_mmu(vcpu);
+	 */
 	kvm_init_mmu(vcpu);
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_reset_context);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|151| <<kvm_mmu_reload>> return kvm_mmu_load(vcpu);
+ */
 int kvm_mmu_load(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -5827,6 +6629,16 @@ void kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 	write_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4608| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn, insn_len);
+ *   - arch/x86/kvm/svm/svm.c|2059| <<npf_interception>> return kvm_mmu_page_fault(vcpu, fault_address, error_code, static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+ *                                                           svm->vmcb->control.insn_bytes : NULL, svm->vmcb->control.insn_len);
+ *   - arch/x86/kvm/vmx/vmx.c|5796| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5817| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ *
+ * 如果是一般的shadow page fault过来, 用的是普通cr3 page fault的error_code
+ */
 int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -5848,13 +6660,28 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 		return RET_PF_RETRY;
 
 	r = RET_PF_INVALID;
+	/*
+	 * RSVD (bit=3):
+	 * 0 - The fault was not caused by reserved bit violation.
+	 * 1 - The fault was caused by a reserved bit set to 1 in some
+	 *     paging-structure entry.
+	 */
 	if (unlikely(error_code & PFERR_RSVD_MASK)) {
 		r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
 		if (r == RET_PF_EMULATE)
 			goto emulate;
 	}
 
+	/*
+	 * 注释:
+	 * RET_PF_INVALID: the spte is invalid, let the real page fault path update it.
+	 */
 	if (r == RET_PF_INVALID) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4399| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true, NULL);
+		 *   - arch/x86/kvm/mmu/mmu.c|6180| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, lower_32_bits(error_code), false, &emulation_type);
+		 */
 		r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa,
 					  lower_32_bits(error_code), false,
 					  &emulation_type);
@@ -5899,6 +6726,11 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6611| <<kvm_mmu_invalidate_addr>> __kvm_mmu_invalidate_addr(vcpu, mmu, addr, mmu->root.hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|6615| <<kvm_mmu_invalidate_addr>> __kvm_mmu_invalidate_addr(vcpu, mmu, addr, mmu->prev_roots[i].hpa);
+ */
 static void __kvm_mmu_invalidate_addr(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 				      u64 addr, hpa_t root_hpa)
 {
@@ -5936,6 +6768,13 @@ static void __kvm_mmu_invalidate_addr(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu
 	write_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6632| <<kvm_mmu_invlpg>> kvm_mmu_invalidate_addr(vcpu, vcpu->arch.walk_mmu, gva, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|6654| <<kvm_mmu_invpcid_gva>> kvm_mmu_invalidate_addr(vcpu, mmu, gva, roots);
+ *   - arch/x86/kvm/vmx/nested.c|404| <<nested_ept_invalidate_addr>> kvm_mmu_invalidate_addr(vcpu, vcpu->arch.mmu, addr, roots);
+ *   - arch/x86/kvm/x86.c|818| <<kvm_inject_emulated_page_fault>> kvm_mmu_invalidate_addr(vcpu, fault_mmu, fault->address, KVM_MMU_ROOT_CURRENT);
+ */
 void kvm_mmu_invalidate_addr(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			     u64 addr, unsigned long roots)
 {
@@ -6097,6 +6936,11 @@ static void free_mmu_pages(struct kvm_mmu *mmu)
 	free_page((unsigned long)mmu->pml5_root);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6454| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|6458| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.root_mmu);
+ */
 static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 {
 	struct page *page;
@@ -6150,6 +6994,10 @@ static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12125| <<kvm_arch_vcpu_create>> r = kvm_mmu_create(vcpu);
+ */
 int kvm_mmu_create(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -6301,6 +7149,10 @@ static bool kvm_has_zapped_obsolete_pages(struct kvm *kvm)
 	return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12657| <<kvm_arch_init_vm>> kvm_mmu_init_vm(kvm);
+ */
 void kvm_mmu_init_vm(struct kvm *kvm)
 {
 	INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
@@ -6366,6 +7218,13 @@ static bool kvm_rmap_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_e
 	return flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|625| <<update_mtrr>> kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));
+ *   - arch/x86/kvm/x86.c|967| <<kvm_post_set_cr0>> kvm_zap_gfn_range(vcpu->kvm, 0, ~0ULL);
+ *   - arch/x86/kvm/x86.c|10615| <<__kvm_set_or_clear_apicv_inhibit>> kvm_zap_gfn_range(kvm, gfn, gfn+1);
+ *   - arch/x86/kvm/x86.c|13432| <<kvm_noncoherent_dma_assignment_start_or_stop>> kvm_zap_gfn_range(kvm, gpa_to_gfn(0), gpa_to_gfn(~0ULL));
+ */
 /*
  * Invalidate (zap) SPTEs that cover GFNs from gfn_start and up to gfn_end
  * (not including it)
@@ -6403,6 +7262,11 @@ static bool slot_rmap_write_protect(struct kvm *kvm,
 	return rmap_write_protect(rmap_head, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13191| <<kvm_mmu_slot_apply_flags>> kvm_mmu_slot_remove_write_access(kvm, new, PG_LEVEL_2M);
+ *   - arch/x86/kvm/x86.c|13193| <<kvm_mmu_slot_apply_flags>> kvm_mmu_slot_remove_write_access(kvm, new, PG_LEVEL_4K);
+ */
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
 				      const struct kvm_memory_slot *memslot,
 				      int start_level)
@@ -7003,6 +7867,10 @@ static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
  * Forward the module init call to SPTE code so that it too can handle module
  * params that need to be resolved/snapshot.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|14059| <<kvm_x86_init>> kvm_mmu_x86_module_init();
+ */
 void __init kvm_mmu_x86_module_init(void)
 {
 	if (nx_huge_pages == -1)
@@ -7023,6 +7891,10 @@ void __init kvm_mmu_x86_module_init(void)
  * loaded as many of the masks/values may be modified by VMX or SVM, i.e. need
  * to be reset when a potentially different vendor module is loaded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9705| <<__kvm_x86_vendor_init>> r = kvm_mmu_vendor_module_init();
+ */
 int kvm_mmu_vendor_module_init(void)
 {
 	int ret = -ENOMEM;
@@ -7319,9 +8191,26 @@ bool kvm_arch_pre_set_memory_attributes(struct kvm *kvm,
 	return kvm_unmap_gfn_range(kvm, range);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7440| <<hugepage_has_attrs>> if (hugepage_test_mixed(slot, gfn, level - 1) ||
+ */
 static bool hugepage_test_mixed(struct kvm_memory_slot *slot, gfn_t gfn,
 				int level)
 {
+	/*
+	 * 在以下使用kvm_lpage_info->disallow_lpage:
+	 *   - arch/x86/kvm/mmu/mmu.c|811| <<update_gfn_disallow_lpage_count>> old = linfo->disallow_lpage;
+	 *   - arch/x86/kvm/mmu/mmu.c|812| <<update_gfn_disallow_lpage_count>> linfo->disallow_lpage += count;
+	 *   - arch/x86/kvm/mmu/mmu.c|813| <<update_gfn_disallow_lpage_count>> WARN_ON_ONCE((old ^ linfo->disallow_lpage) & KVM_LPAGE_MIXED_FLAG);
+	 *   - arch/x86/kvm/mmu/mmu.c|3156| <<__kvm_mmu_max_mapping_level>> if (!linfo->disallow_lpage)
+	 *   - arch/x86/kvm/mmu/mmu.c|7372| <<hugepage_test_mixed>> return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/mmu/mmu.c|7378| <<hugepage_clear_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/mmu/mmu.c|7384| <<hugepage_set_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/x86.c|12807| <<kvm_alloc_memslot_metadata>> linfo[0].disallow_lpage = 1;
+	 *   - arch/x86/kvm/x86.c|12809| <<kvm_alloc_memslot_metadata>> linfo[lpages - 1].disallow_lpage = 1;
+	 *   - arch/x86/kvm/x86.c|12819| <<kvm_alloc_memslot_metadata>> linfo[j].disallow_lpage = 1;
+	 */
 	return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
 }
 
@@ -7331,22 +8220,80 @@ static void hugepage_clear_mixed(struct kvm_memory_slot *slot, gfn_t gfn,
 	lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7471| <<kvm_arch_post_set_memory_attributes>> hugepage_set_mixed(slot, gfn, level);
+ *   - arch/x86/kvm/mmu/mmu.c|7493| <<kvm_arch_post_set_memory_attributes>> hugepage_set_mixed(slot, gfn, level);
+ *   - arch/x86/kvm/mmu/mmu.c|7531| <<kvm_mmu_init_memslot_memory_attributes>> hugepage_set_mixed(slot, gfn, level);
+ */
 static void hugepage_set_mixed(struct kvm_memory_slot *slot, gfn_t gfn,
 			       int level)
 {
 	lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
 }
 
+/*
+ * 假设:
+ * base_gfn: 133693442
+ * gfn     : 134060443
+ *
+ * 对于level 2:
+ * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 9       = 261836
+ * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 9 = 261120
+ * 261836 - 261120 = 716
+ *
+ * 对于level 3:
+ * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 18       = 511
+ * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 18 = 510
+ * 511 - 510 = 0
+ *
+ * level 3 (index 1, 1G)有1个slot
+ * level 2 (index 2, 2M)有717个slot
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7439| <<kvm_arch_post_set_memory_attributes>> if (hugepage_has_attrs(kvm, slot, gfn, level, attrs))
+ *   - arch/x86/kvm/mmu/mmu.c|7461| <<kvm_arch_post_set_memory_attributes>> if (hugepage_has_attrs(kvm, slot, gfn, level, attrs))
+ *   - arch/x86/kvm/mmu/mmu.c|7499| <<kvm_mmu_init_memslot_memory_attributes>> if (hugepage_has_attrs(kvm, slot, gfn, level, attrs))
+ */
 static bool hugepage_has_attrs(struct kvm *kvm, struct kvm_memory_slot *slot,
 			       gfn_t gfn, int level, unsigned long attrs)
 {
 	const unsigned long start = gfn;
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+	 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+	 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+	 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+	 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+	 */
 	const unsigned long end = start + KVM_PAGES_PER_HPAGE(level);
 
 	if (level == PG_LEVEL_2M)
 		return kvm_range_has_memory_attributes(kvm, start, end, attrs);
 
+	/*
+	 * KVM_HPAGE_GFN_SHIFT(1) : (((1) - 1) * 9) =  0
+	 * KVM_HPAGE_GFN_SHIFT(2) : (((2) - 1) * 9) =  9 ---> level 2
+	 * KVM_HPAGE_GFN_SHIFT(3) : (((3) - 1) * 9) = 18 ---> level 3
+	 * KVM_HPAGE_GFN_SHIFT(4) : (((4) - 1) * 9) = 27
+	 * KVM_HPAGE_GFN_SHIFT(5) : (((5) - 1) * 9) = 36
+	 *
+	 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+	 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+	 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+	 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+	 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+	 *
+	 * 到这里只能是level=3
+	 *
+	 * 比如start是1048576, end是1048576+262144=1310720.
+	 * 一次加512个
+	 * for (gfn = 1048576; gfn < 1310720; gfn += 512)
+	 */
 	for (gfn = start; gfn < end; gfn += KVM_PAGES_PER_HPAGE(level - 1)) {
+		/*
+		 * 只在此处调用hugepage_test_mixed()
+		 */
 		if (hugepage_test_mixed(slot, gfn, level - 1) ||
 		    attrs != kvm_get_memory_attributes(kvm, gfn))
 			return false;
@@ -7354,6 +8301,20 @@ static bool hugepage_has_attrs(struct kvm *kvm, struct kvm_memory_slot *slot,
 	return true;
 }
 
+/*
+ * struct kvm_gfn_range {
+ *     struct kvm_memory_slot *slot;
+ *     gfn_t start;
+ *     gfn_t end;
+ *     union kvm_mmu_notifier_arg arg;
+ *     bool may_block;
+ * };
+ *
+ * 在以下使用kvm_arch_post_set_memory_attributes():
+ *   - virt/kvm/kvm_main.c|2560| <<kvm_vm_set_mem_attributes>> .handler = kvm_arch_post_set_memory_attributes,
+ *
+ * 就是在attri和设置那些mix
+ */
 bool kvm_arch_post_set_memory_attributes(struct kvm *kvm,
 					 struct kvm_gfn_range *range)
 {
@@ -7377,10 +8338,30 @@ bool kvm_arch_post_set_memory_attributes(struct kvm *kvm,
 	 * The sequence matters here: upper levels consume the result of lower
 	 * level's scanning.
 	 */
+	/*
+	 * level从2变到3
+	 */
 	for (level = PG_LEVEL_2M; level <= KVM_MAX_HUGEPAGE_LEVEL; level++) {
+		/*
+		 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+		 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+		 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+		 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+		 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+		 */
 		gfn_t nr_pages = KVM_PAGES_PER_HPAGE(level);
+		/*
+		 * level 5: 向下round到每68719476736个 (256T)
+		 * level 4: 向下round到每134217728个 (512G)
+		 * level 3: 向下round到每262144个(1G)
+		 * level 2: 向下round到每512个(2M)
+		 * level 1: 向下round到每1个, 就是没round
+		 */
 		gfn_t gfn = gfn_round_for_level(range->start, level);
 
+		/*
+		 * 上面从range->start向下round, 不相等就是小于
+		 */
 		/* Process the head page if it straddles the range. */
 		if (gfn != range->start || gfn + nr_pages > range->end) {
 			/*
@@ -7388,7 +8369,13 @@ bool kvm_arch_post_set_memory_attributes(struct kvm *kvm,
 			 * by the memslot, KVM can't use a hugepage due to the
 			 * misaligned address regardless of memory attributes.
 			 */
+			/*
+			 * 要不要加上gfn + nr_pages <= slot->base_gfn + slot->npages???
+			 */
 			if (gfn >= slot->base_gfn) {
+				/*
+				 * 这里就是针对gfn一个!
+				 */
 				if (hugepage_has_attrs(kvm, slot, gfn, level, attrs))
 					hugepage_clear_mixed(slot, gfn, level);
 				else
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index 0669a8a66..6942759d1 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -6,6 +6,18 @@
 #include <linux/kvm_host.h>
 #include <asm/kvm_host.h>
 
+/*
+ * 在以下使用KVM_MMU_WARN_ON():
+ *   - arch/x86/kvm/mmu/mmu.c|1322| <<spte_clear_dirty>> KVM_MMU_WARN_ON(!spte_ad_enabled(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|1791| <<kvm_mmu_check_sptes_at_free>> if (KVM_MMU_WARN_ON(is_shadow_present_pte(sp->spt[i])))
+ *   - arch/x86/kvm/mmu/mmu.c|4440| <<kvm_max_level_for_order>> KVM_MMU_WARN_ON(order != KVM_HPAGE_GFN_SHIFT(PG_LEVEL_1G) &&
+ *   - arch/x86/kvm/mmu/spte.h|536| <<spte_ad_enabled>> KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
+ *   - arch/x86/kvm/mmu/spte.h|548| <<spte_ad_need_write_protect>> KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
+ *   - arch/x86/kvm/mmu/spte.h|566| <<spte_shadow_accessed_mask>> KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
+ *   - arch/x86/kvm/mmu/spte.h|572| <<spte_shadow_dirty_mask>> KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1649| <<clear_dirty_gfn_range>> KVM_MMU_WARN_ON(kvm_ad_enabled() &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1709| <<clear_dirty_pt_masked>> KVM_MMU_WARN_ON(kvm_ad_enabled() &&
+ */
 #ifdef CONFIG_KVM_PROVE_MMU
 #define KVM_MMU_WARN_ON(x) WARN_ON_ONCE(x)
 #else
@@ -58,6 +70,34 @@ struct kvm_mmu_page {
 	struct hlist_node hash_link;
 
 	bool tdp_mmu_page;
+	/*
+	 * 在以下设置kvm_mmu_page->unsync:
+	 *   - arch/x86/kvm/mmu/mmu.c|1987| <<kvm_unlink_unsync_page>> sp->unsync = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|2920| <<kvm_unsync_page>> sp->unsync = 1;
+	 * 在以下使用kvm_mmu_page->unsync:
+	 *   - arch/x86/kvm/mmu/mmu.c|1913| <<mmu_pages_add>> if (sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|1959| <<__mmu_unsync_walk>> } else if (child->unsync) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1985| <<kvm_unlink_unsync_page>> WARN_ON_ONCE(!sp->unsync);
+	 *   - arch/x86/kvm/mmu/mmu.c|2283| <<kvm_mmu_find_shadow_page>> if (role.level > PG_LEVEL_4K && sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|2293| <<kvm_mmu_find_shadow_page>> if (sp->unsync) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2576| <<__link_shadow_page>> if (WARN_ON_ONCE(sp->unsync_children) || sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|2715| <<__kvm_mmu_prepare_zap_page>> if (sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|2959| <<mmu_try_to_unsync_pages>> if (sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|2984| <<mmu_try_to_unsync_pages>> if (READ_ONCE(sp->unsync))
+	 *   - arch/x86/kvm/mmu/mmu.c|4231| <<is_unsync_root>> if (sp->unsync || sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/mmu.c|6556| <<__kvm_mmu_invalidate_addr>> if (sp->unsync) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7263| <<shadow_mmu_try_split_huge_pages>> if (WARN_ON_ONCE(sp->unsync))
+	 *   - arch/x86/kvm/mmu/mmutrace.h|16| <<KVM_MMU_PAGE_FIELDS>> __field(bool, unsync)
+	 *   - arch/x86/kvm/mmu/mmutrace.h|23| <<KVM_MMU_PAGE_ASSIGN>> __entry->unsync = sp->unsync;
+	 *
+	 * 注释:
+	 * unsync:
+	 * If true, then the translations in this page may not match the guest's
+	 * translation.  This is equivalent to the state of the tlb when a pte is
+	 * changed but before the tlb entry is flushed.  Accordingly, unsync ptes
+	 * are synchronized when the guest executes invlpg or flushes its tlb by
+	 * other means.  Valid for leaf pages.
+	 */
 	bool unsync;
 	union {
 		u8 mmu_valid_gen;
@@ -99,6 +139,13 @@ struct kvm_mmu_page {
 	/* Currently serving as active root */
 	union {
 		int root_count;
+		/*
+		 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|78| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|247| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 2);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|730| <<tdp_mmu_zap_root>> WARN_ON_ONCE(!refcount_read(&root->tdp_mmu_root_count));
+		 *   - arch/x86/kvm/mmu/tdp_mmu.h|17| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+		 */
 		refcount_t tdp_mmu_root_count;
 	};
 	unsigned int unsync_children;
@@ -158,8 +205,34 @@ static inline bool kvm_mmu_page_ad_need_write_protect(struct kvm_mmu_page *sp)
 	return kvm_x86_ops.cpu_dirty_log_size && sp->role.guest_mode;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3255| <<direct_map>> base_gfn = gfn_round_for_level(fault->gfn, it.level);
+ *   - arch/x86/kvm/mmu/mmu.c|4670| <<kvm_tdp_page_fault>> gfn_t base = gfn_round_for_level(fault->gfn, fault->max_level);
+ *   - arch/x86/kvm/mmu/mmu.c|7429| <<kvm_arch_post_set_memory_attributes>> gfn_t gfn = gfn_round_for_level(range->start, level);
+ *   - arch/x86/kvm/mmu/mmu.c|7484| <<kvm_mmu_init_memslot_memory_attributes>> gfn_t end = gfn_round_for_level(slot->base_gfn + slot->npages, level);
+ *   - arch/x86/kvm/mmu/mmu.c|7485| <<kvm_mmu_init_memslot_memory_attributes>> gfn_t start = gfn_round_for_level(slot->base_gfn, level);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|192| <<kvm_flush_remote_tlbs_gfn>> kvm_flush_remote_tlbs_range(kvm, gfn_round_for_level(gfn, level),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME>> base_gfn = gfn_round_for_level(fault->gfn, it.level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|40| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|116| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|158| <<try_step_up>> iter->gfn = gfn_round_for_level(iter->gfn, iter->level);
+ *
+ * level 5: 向下round到每68719476736个 (256T)
+ * level 4: 向下round到每134217728个 (512G)
+ * level 3: 向下round到每262144个(1G)
+ * level 2: 向下round到每512个(2M)
+ * level 1: 向下round到每1个, 就是没round
+ */
 static inline gfn_t gfn_round_for_level(gfn_t gfn, int level)
 {
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+	 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+	 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+	 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+	 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+	 */
 	return gfn & -KVM_PAGES_PER_HPAGE(level);
 }
 
@@ -239,6 +312,16 @@ struct kvm_page_fault {
 	unsigned long mmu_seq;
 	kvm_pfn_t pfn;
 	hva_t hva;
+	/*
+	 * 在以下使用kvm_page_fault->map_writable:
+	 *   - arch/x86/kvm/mmu/mmu.c|2954| <<mmu_set_spte>> bool host_writable = !fault || fault->map_writable;
+	 *   - arch/x86/kvm/mmu/mmu.c|4375| <<kvm_faultin_pfn_private>> fault->map_writable = !(fault->slot->flags & KVM_MEM_READONLY);
+	 *   - arch/x86/kvm/mmu/mmu.c|4398| <<__kvm_faultin_pfn>> fault->map_writable = false;
+	 *   - arch/x86/kvm/mmu/mmu.c|4422| <<__kvm_faultin_pfn>> fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, false, &async, fault->write, &fault->map_writable, &fault->hva);
+	 *   - arch/x86/kvm/mmu/mmu.c|4444| <<__kvm_faultin_pfn>> fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, true, NULL, fault->write, &fault->map_writable, &fault->hva);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1066| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn,
+	 *                                                fault->pfn, iter->old_spte, fault->prefetch, true, fault->map_writable, &new_spte);
+	 */
 	bool map_writable;
 
 	/*
@@ -279,6 +362,11 @@ enum {
 	RET_PF_SPURIOUS,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4399| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true, NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|6180| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, lower_32_bits(error_code), false, &emulation_type);
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u32 err, bool prefetch, int *emulation_type)
 {
diff --git a/arch/x86/kvm/mmu/page_track.c b/arch/x86/kvm/mmu/page_track.c
index c87da11f3..9cbf3103a 100644
--- a/arch/x86/kvm/mmu/page_track.c
+++ b/arch/x86/kvm/mmu/page_track.c
@@ -203,6 +203,10 @@ EXPORT_SYMBOL_GPL(kvm_page_track_unregister_notifier);
  * The node should figure out if the written page is the one that node is
  * interested in by itself.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/page_track.h|53| <<kvm_page_track_write>> __kvm_page_track_write(vcpu->kvm, gpa, new, bytes);
+ */
 void __kvm_page_track_write(struct kvm *kvm, gpa_t gpa, const u8 *new, int bytes)
 {
 	struct kvm_page_track_notifier_head *head;
diff --git a/arch/x86/kvm/mmu/page_track.h b/arch/x86/kvm/mmu/page_track.h
index d4d72ed99..6bc0c6566 100644
--- a/arch/x86/kvm/mmu/page_track.h
+++ b/arch/x86/kvm/mmu/page_track.h
@@ -47,6 +47,11 @@ static inline bool kvm_page_track_has_external_user(struct kvm *kvm) { return fa
 
 #endif /* CONFIG_KVM_EXTERNAL_WRITE_TRACKING */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7803| <<emulator_write_phys>> kvm_page_track_write(vcpu, gpa, val, bytes);
+ *   - arch/x86/kvm/x86.c|8096| <<emulator_cmpxchg_emulated>> kvm_page_track_write(vcpu, gpa, new, bytes);
+ */
 static inline void kvm_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 					const u8 *new, int bytes)
 {
diff --git a/arch/x86/kvm/mmu/paging_tmpl.h b/arch/x86/kvm/mmu/paging_tmpl.h
index 4d4e98fe4..afdb3bf40 100644
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@ -86,7 +86,30 @@ struct guest_walker {
 	gpa_t pte_gpa[PT_MAX_FULL_LEVELS];
 	pt_element_t __user *ptep_user[PT_MAX_FULL_LEVELS];
 	bool pte_writable[PT_MAX_FULL_LEVELS];
+	/*
+	 * 在以下使用pt_access:
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|598| <<FNAME(walk_addr_generic)>> walker->pt_access[walker->level - 1] = FNAME(gpte_access)(pt_access ^ walk_nx_mask);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|907| <<FNAME(fetch)>> access = gw->pt_access[it.level - 2];
+	 */
 	unsigned int pt_access[PT_MAX_FULL_LEVELS];
+	/*
+	 * 在以下设置guest_walker->pte_access:
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|522| <<FNAME(walk_addr_generic)>> walker->pte_access = FNAME(gpte_access)(pte_access ^ walk_nx_mask);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|560| <<FNAME>> FNAME(protect_clean_gpte)(mmu, &walker->pte_access, pte);
+	 * 在以下使用guest_walker->pte_access:
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|541| <<FNAME(walk_addr_generic)>> errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|759| <<FNAME(fetch)>> direct_access = gw->pte_access;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|880| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access, base_gfn, fault->pfn, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1008| <<FNAME(page_fault)>> r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1024| <<FNAME(page_fault)>> if (fault->write && !(walker.pte_access & ACC_WRITE_MASK) &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1026| <<FNAME(page_fault)>> walker.pte_access |= ACC_WRITE_MASK;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1027| <<FNAME(page_fault)>> walker.pte_access &= ~ACC_USER_MASK;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1036| <<FNAME(page_fault)>> walker.pte_access &= ~ACC_EXEC_MASK;
+	 *
+	 * 应该是一次walk_addr_generic后的最后一级的pte和上面level的pde..等等的and
+	 * 之前的都存在unsigned int pt_access[PT_MAX_FULL_LEVELS];
+	 * 而且这里是修改过格式的: (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 */
 	unsigned int pte_access;
 	gfn_t gfn;
 	struct x86_exception fault;
@@ -170,12 +193,41 @@ static bool FNAME(prefetch_invalid_gpte)(struct kvm_vcpu *vcpu,
 	return true;
 }
 
+/*
+ * 在mmu.c:
+ * 5021 #define PTTYPE_EPT 18 // arbitrary
+ * 5022 #define PTTYPE PTTYPE_EPT
+ * 5023 #include "paging_tmpl.h"
+ * 5024 #undef PTTYPE
+ * 5025
+ * 5026 #define PTTYPE 64
+ * 5027 #include "paging_tmpl.h"
+ * 5028 #undef PTTYPE
+ * 5029
+ * 5030 #define PTTYPE 32
+ * 5031 #include "paging_tmpl.h"
+ * 5032 #undef PTTYPE
+ *
+ * 根据gpte的内容(比如后3位或者最高位), 转换成ACC_xxx的模式
+ * 以记录这个gpte是否允许exec, read, write
+ * #define ACC_EXEC_MASK    1
+ * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+ * #define ACC_USER_MASK    PT_USER_MASK
+ * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+ */
 /*
  * For PTTYPE_EPT, a page table can be executable but not readable
  * on supported processors. Therefore, set_spte does not automatically
  * set bit 0 if execute only is supported. Here, we repurpose ACC_USER_MASK
  * to signify readability since it isn't used in the EPT case
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|529| <<FNAME(walk_addr_generic)>> walker->pt_access[walker->level - 1] = FNAME(gpte_access)(pt_access ^ walk_nx_mask);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|544| <<FNAME(walk_addr_generic)>> walker->pte_access = FNAME(gpte_access)(pte_access ^ walk_nx_mask);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|680| <<FNAME(prefetch_gpte)>> pte_access = sp->role.access & FNAME(gpte_access)(gpte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1160| <<FNAME(sync_spte)>> pte_access &= FNAME(gpte_access)(gpte);
+ */
 static inline unsigned FNAME(gpte_access)(u64 gpte)
 {
 	unsigned access;
@@ -186,13 +238,65 @@ static inline unsigned FNAME(gpte_access)(u64 gpte)
 #else
 	BUILD_BUG_ON(ACC_EXEC_MASK != PT_PRESENT_MASK);
 	BUILD_BUG_ON(ACC_EXEC_MASK != 1);
+	/*
+	 * PT_WRITABLE_MASK: 1 << 1
+	 * PT_USER_MASK:     1 << 2
+	 * PT_PRESENT_MASK:  1 << 0
+	 */
 	access = gpte & (PT_WRITABLE_MASK | PT_USER_MASK | PT_PRESENT_MASK);
 	/* Combine NX with P (which is set here) to get ACC_EXEC_MASK.  */
+	/*
+	 * #define ACC_EXEC_MASK    1
+	 * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+	 * #define ACC_USER_MASK    PT_USER_MASK
+	 * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 *
+	 * access和00X进程异或
+	 * 比如: 111 ^ 001 = 110
+	 * 比如: 110 ^ 001 = 111
+	 *
+	 * 最后以为目前是present, 和nx异或
+	 * present(0) ^ nx(0) = 1
+	 * present(0) ^ nx(1) = 1
+	 * present(1) ^ nx(0) = 1
+	 * present(1) ^ nx(1) = 0
+	 */
 	access ^= (gpte >> PT64_NX_SHIFT);
 #endif
 
 	return access;
 }
+/*
+ * 原始的patch, shi山一样的代码 :(
+ * commit bb9eadf0c35f2e7eb5ca6468f46ebb7473b85537
+ * Author: Paolo Bonzini <pbonzini@redhat.com>
+ * Date:   Tue Feb 23 14:19:20 2016 +0100
+ * 
+ *     KVM: MMU: micro-optimize gpte_access
+ *
+ *     Avoid AND-NOT, most x86 processor lack an instruction for it.
+ *
+ *     Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ *
+ * diff --git a/arch/x86/kvm/paging_tmpl.h b/arch/x86/kvm/paging_tmpl.h
+ * index a1f5459edcec..6013f3685ef4 100644
+ * --- a/arch/x86/kvm/paging_tmpl.h
+ * +++ b/arch/x86/kvm/paging_tmpl.h
+ * @@ -189,8 +189,11 @@ static inline unsigned FNAME(gpte_access)(struct kvm_vcpu *vcpu, u64 gpte)
+ *                 ((gpte & VMX_EPT_EXECUTABLE_MASK) ? ACC_EXEC_MASK : 0) |
+ *                 ACC_USER_MASK;
+ *  #else
+ * -       access = (gpte & (PT_WRITABLE_MASK | PT_USER_MASK)) | ACC_EXEC_MASK;
+ * -       access &= ~(gpte >> PT64_NX_SHIFT);
+ * +       BUILD_BUG_ON(ACC_EXEC_MASK != PT_PRESENT_MASK);
+ * +       BUILD_BUG_ON(ACC_EXEC_MASK != 1);
+ * +       access = gpte & (PT_WRITABLE_MASK | PT_USER_MASK | PT_PRESENT_MASK);
+ * +       // Combine NX with P (which is set here) to get ACC_EXEC_MASK.
+ * +       access ^= (gpte >> PT64_NX_SHIFT);
+ *  #endif
+ *
+ *         return access;
+ */
 
 static int FNAME(update_accessed_dirty_bits)(struct kvm_vcpu *vcpu,
 					     struct kvm_mmu *mmu,
@@ -296,9 +400,28 @@ static inline bool FNAME(is_last_gpte)(struct kvm_mmu *mmu,
 
 	return gpte & PT_PAGE_SIZE_MASK;
 }
+/*
+ * error code:
+ * - P  1 bit   Present When set, the page fault was caused by a
+ *   page-protection violation. When not set, it was caused by a
+ *   non-present page.
+ * - W  1 bit   Write   When set, the page fault was caused by a write
+ *   access. When not set, it was caused by a read access.
+ * - U  1 bit   User    When set, the page fault was caused while CPL =
+ *   3. This does not necessarily mean that the page fault was a
+ *   privilege violation.
+ * - R  1 bit   Reserved write  When set, one or more page directory
+ *   entries contain reserved bits which are set to 1. This only
+ *   applies when the PSE or PAE flags in CR4 are set to 1.
+ */
 /*
  * Fetch a guest pte for a guest virtual address, or for an L2's GPA.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|702| <<FNAME(walk_addr)>> return FNAME(walk_addr_generic)(walker, vcpu, vcpu->arch.mmu, addr, access);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1164| <<FNAME(gva_to_gpa)>> r = FNAME(walk_addr_generic)(&walker, vcpu, mmu, addr, access);
+ */
 static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 				    struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 				    gpa_t addr, u64 access)
@@ -321,6 +444,9 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 	gpa_t real_gpa;
 	gfn_t gfn;
 
+	/*
+	 * 只在这里trace
+	 */
 	trace_kvm_mmu_pagetable_walk(addr, access);
 retry_walk:
 	walker->level = mmu->cpu_role.base.level;
@@ -328,6 +454,9 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 	have_ad       = PT_HAVE_ACCESSED_DIRTY(mmu);
 
 #if PTTYPE == 64
+	/*
+	 * 为什么在64下呢?因为32-bit的不支持 :)
+	 */
 	walk_nx_mask = 1ULL << PT64_NX_SHIFT;
 	if (walker->level == PT32E_ROOT_LEVEL) {
 		pte = mmu->get_pdptr(vcpu, (addr >> 30) & 3);
@@ -364,10 +493,26 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 		struct kvm_memory_slot *slot;
 		unsigned long host_addr;
 
+		/*
+		 * pt_access=0xffffffffffffffff, pte_access=0xffffffffffffffff
+		 * pt_access=0x8000000005855027, pte_access=0x8000000005855027
+		 * pt_access=0x8000000005854027, pte_access=0x8000000005854027
+		 *
+		 * u64 pt_access, pte_access;
+		 *
+		 * pte_access一开始是pte_access = ~0;
+		 * 一开始walk_nx_mask bit是1, 因为下面要revert, 说明是可以exec
+		 */
 		pt_access = pte_access;
 		--walker->level;
 
 		index = PT_INDEX(addr, walker->level);
+		/*
+		 * pte一开始是kvm_mmu_get_guest_pgd(vcpu, mmu);
+		 * 应该是guest cr3
+		 *
+		 * gpte_to_gfn()应该是pte指向的gfn (gfn可能是页表页)
+		 */
 		table_gfn = gpte_to_gfn(pte);
 		offset    = index * sizeof(pt_element_t);
 		pte_gpa   = gfn_to_gpa(table_gfn) + offset;
@@ -376,6 +521,13 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 		walker->table_gfn[walker->level - 1] = table_gfn;
 		walker->pte_gpa[walker->level - 1] = pte_gpa;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4215| <<nonpaging_gva_to_gpa>> return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|442| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn), nested_access, &walker->fault);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|553| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(gfn), access, &walker->fault);
+		 *   - arch/x86/kvm/x86.c|886| <<load_pdptrs>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn), PFERR_USER_MASK | PFERR_WRITE_MASK, NULL);
+		 */
 		real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn),
 					     nested_access, &walker->fault);
 
@@ -402,12 +554,25 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 			goto error;
 
 		ptep_user = (pt_element_t __user *)((void *)host_addr + offset);
+		/*
+		 * pt_element_t pte;
+		 * 获取下一个level的pte
+		 */
 		if (unlikely(__get_user(pte, ptep_user)))
 			goto error;
 		walker->ptep_user[walker->level - 1] = ptep_user;
 
 		trace_kvm_mmu_paging_element(pte, walker->level);
 
+		/*
+		 *  384 #if PTTYPE == 64
+		 *  385         walk_nx_mask = 1ULL << PT64_NX_SHIFT;
+		 *
+		 * 这里是在计算和parent pte的继承关系?要不要把高位的nx bi传下去??
+		 *
+		 * 如果pte设置了(1), walk_nx_mask=1, 返回0, 此时这个bit代表x, 不可以exec
+		 * 如果pte没设置(0), walk_nx_mask=1, 返回1, 此时这个bit代表x, 可以exec
+		 */
 		/*
 		 * Inverting the NX it lets us AND it like other
 		 * permission bits.
@@ -424,6 +589,16 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 
 		walker->ptes[walker->level - 1] = pte;
 
+		/*
+		 * 根据gpte的内容(比如后3位或者最高位), 转换成ACC_xxx的模式
+		 * 以记录这个gpte是否允许exec, read, write
+		 * #define ACC_EXEC_MASK    1
+		 * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+		 * #define ACC_USER_MASK    PT_USER_MASK
+		 * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+		 *
+		 * 进入函数之前^ walk_nx_mask翻转回来
+		 */
 		/* Convert to ACC_*_MASK flags for struct guest_walker.  */
 		walker->pt_access[walker->level - 1] = FNAME(gpte_access)(pt_access ^ walk_nx_mask);
 	} while (!FNAME(is_last_gpte)(mmu, walker->level, pte));
@@ -431,8 +606,38 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 	pte_pkey = FNAME(gpte_pkeys)(vcpu, pte);
 	accessed_dirty = have_ad ? pte_access & PT_GUEST_ACCESSED_MASK : 0;
 
+	/*
+	 * 根据gpte的内容(比如后3位或者最高位), 转换成ACC_xxx的模式
+	 * 以记录这个gpte是否允许exec, read, write
+	 * #define ACC_EXEC_MASK    1
+	 * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+	 * #define ACC_USER_MASK    PT_USER_MASK
+	 * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 *
+	 * 应该是一次walk_addr_generic后的最后一级的pte里的access
+	 * 之前的都存在unsigned int pt_access[PT_MAX_FULL_LEVELS];
+	 * 而且这里是修改过格式的: (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 */
 	/* Convert to ACC_*_MASK flags for struct guest_walker.  */
 	walker->pte_access = FNAME(gpte_access)(pte_access ^ walk_nx_mask);
+	/*
+	 * error code:
+	 * - P  1 bit   Present When set, the page fault was caused by a
+	 *   page-protection violation. When not set, it was caused by a
+	 *   non-present page.
+	 * - W  1 bit   Write   When set, the page fault was caused by a write
+	 *   access. When not set, it was caused by a read access.
+	 * - U  1 bit   User    When set, the page fault was caused while CPL =
+	 *   3. This does not necessarily mean that the page fault was a
+	 *   privilege violation.
+	 * - R  1 bit   Reserved write  When set, one or more page directory
+	 *   entries contain reserved bits which are set to 1. This only
+	 *   applies when the PSE or PAE flags in CR4 are set to 1.
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|436| <<FNAME(walk_addr_generic)>> errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
+	 *   - arch/x86/kvm/x86.c|7758| <<vcpu_mmio_gva_to_gpa>> !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access))) {
+	 */
 	errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
 	if (unlikely(errcode))
 		goto error;
@@ -522,6 +727,10 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1040| <<FNAME>> r = FNAME(walk_addr)(&walker, vcpu, fault->addr, fault->error_code & ~PFERR_RSVD_MASK);
+ */
 static int FNAME(walk_addr)(struct guest_walker *walker,
 			    struct kvm_vcpu *vcpu, gpa_t addr, u64 access)
 {
@@ -529,6 +738,15 @@ static int FNAME(walk_addr)(struct guest_walker *walker,
 					access);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|799| <<FNAME(pte_prefetch)>> if (!FNAME(prefetch_gpte)(vcpu, sp, spte, gptep[i]))
+ *
+ * FNAME(page_fault)
+ * -> FNAME(fetch)
+ *    -> FNAME(pte_prefetch)
+ *       -> FNAME(prefetch_gpte)
+ */
 static bool
 FNAME(prefetch_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 		     u64 *spte, pt_element_t gpte)
@@ -542,6 +760,14 @@ FNAME(prefetch_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 		return false;
 
 	gfn = gpte_to_gfn(gpte);
+	/*
+	 * 根据gpte的内容(比如后3位或者最高位), 转换成ACC_xxx的模式
+	 * 以记录这个gpte是否允许exec, read, write
+	 * #define ACC_EXEC_MASK    1
+	 * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+	 * #define ACC_USER_MASK    PT_USER_MASK
+	 * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 */
 	pte_access = sp->role.access & FNAME(gpte_access)(gpte);
 	FNAME(protect_clean_gpte)(vcpu->arch.mmu, &pte_access, gpte);
 
@@ -581,6 +807,10 @@ static bool FNAME(gpte_changed)(struct kvm_vcpu *vcpu,
 	return r || curr_pte != gw->ptes[level - 1];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|954| <<FNAME(fetch)>> FNAME(pte_prefetch)(vcpu, gw, it.sptep);
+ */
 static void FNAME(pte_prefetch)(struct kvm_vcpu *vcpu, struct guest_walker *gw,
 				u64 *sptep)
 {
@@ -624,6 +854,9 @@ static void FNAME(pte_prefetch)(struct kvm_vcpu *vcpu, struct guest_walker *gw,
  * If the guest tries to write a write-protected page, we need to
  * emulate this operation, return 1 to indicate this case.
  */
+/*
+ * 只在FNAME(page_fault)调用
+ */
 static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 			 struct guest_walker *gw)
 {
@@ -631,9 +864,15 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	struct kvm_shadow_walk_iterator it;
 	unsigned int direct_access, access;
 	int top_level, ret;
+	/*
+	 * 这是要map的gfn??
+	 */
 	gfn_t base_gfn = fault->gfn;
 
 	WARN_ON_ONCE(gw->gfn != base_gfn);
+	/*
+	 * guest中pte(不是host shadow)的后几位
+	 */
 	direct_access = gw->pte_access;
 
 	top_level = vcpu->arch.mmu->cpu_role.base.level;
@@ -671,6 +910,12 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 
 		table_gfn = gw->table_gfn[it.level - 2];
 		access = gw->pt_access[it.level - 2];
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3380| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|793| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn,
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|856| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn,
+		 */
 		sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn,
 					  false, access);
 
@@ -748,6 +993,17 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	if (WARN_ON_ONCE(it.level != fault->goal_level))
 		return -EFAULT;
 
+	/*
+	 * 应该是一次walk_addr_generic后的最后一级的pte和上面level的pde..等等的and
+	 * 之前的都存在unsigned int pt_access[PT_MAX_FULL_LEVELS];
+	 * 而且这里是修改过格式的: (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3019| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, slot, start, access, gfn, page_to_pfn(pages[i]), NULL);
+	 *   - arch/x86/kvm/mmu/mmu.c|3294| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL, base_gfn, fault->pfn, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|556| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, slot, spte, pte_access, gfn, pfn, NULL);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|751| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access, base_gfn, fault->pfn, fault);
+	 */
 	ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access,
 			   base_gfn, fault->pfn, fault);
 	if (ret == RET_PF_SPURIOUS)
@@ -760,6 +1016,32 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	return RET_PF_RETRY;
 }
 
+/*
+ * 333         struct kvm_page_fault fault = {
+ * 334                 .addr = cr2_or_gpa,
+ * 335                 .error_code = err,
+ * 336                 .exec = err & PFERR_FETCH_MASK,
+ * 337                 .write = err & PFERR_WRITE_MASK,
+ * 338                 .present = err & PFERR_PRESENT_MASK,
+ * 339                 .rsvd = err & PFERR_RSVD_MASK,
+ * 340                 .user = err & PFERR_USER_MASK,
+ * 341                 .prefetch = prefetch,
+ * 342                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 343                 .nx_huge_page_workaround_enabled =
+ * 344                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 345
+ * 346                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 347                 .req_level = PG_LEVEL_4K,
+ * 348                 .goal_level = PG_LEVEL_4K,
+ * 349                 .is_private = kvm_mem_is_private(vcpu->kvm, cr2_or_gpa >> PAGE_SHIFT),
+ * 350         };
+ * 351         int r;
+ * 352
+ * 353         if (vcpu->arch.mmu->root_role.direct) {
+ * 354                 fault.gfn = fault.addr >> PAGE_SHIFT;
+ * 355                 fault.slot = kvm_vcpu_gfn_to_memslot(vcpu, fault.gfn);
+ * 356         }
+ */
 /*
  * Page fault handler.  There are several causes for a page fault:
  *   - there is no shadow pte for the guest pte
@@ -781,6 +1063,20 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 
 	WARN_ON_ONCE(fault->is_tdp);
 
+	/*
+	 * error code:
+	 * - P	1 bit	Present	When set, the page fault was caused by a
+	 *   page-protection violation. When not set, it was caused by a
+	 *   non-present page.
+	 * - W	1 bit	Write	When set, the page fault was caused by a write
+	 *   access. When not set, it was caused by a read access.
+	 * - U	1 bit	User	When set, the page fault was caused while CPL =
+	 *   3. This does not necessarily mean that the page fault was a
+	 *   privilege violation.
+	 * - R	1 bit	Reserved write	When set, one or more page directory
+	 *   entries contain reserved bits which are set to 1. This only
+	 *   applies when the PSE or PAE flags in CR4 are set to 1.
+	 */
 	/*
 	 * Look up the guest pte for the faulting address.
 	 * If PFEC.RSVD is set, this is a shadow page fault.
@@ -799,6 +1095,22 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 		return RET_PF_RETRY;
 	}
 
+	/*
+	 * 80 struct guest_walker {
+	 * 81         int level;
+	 * 82         unsigned max_level;
+	 * 83         gfn_t table_gfn[PT_MAX_FULL_LEVELS];
+	 * 84         pt_element_t ptes[PT_MAX_FULL_LEVELS];
+	 * 85         pt_element_t prefetch_ptes[PTE_PREFETCH_NUM];
+	 * 86         gpa_t pte_gpa[PT_MAX_FULL_LEVELS];
+	 * 87         pt_element_t __user *ptep_user[PT_MAX_FULL_LEVELS];
+	 * 88         bool pte_writable[PT_MAX_FULL_LEVELS];
+	 * 89         unsigned int pt_access[PT_MAX_FULL_LEVELS];
+	 * 90         unsigned int pte_access;
+	 * 91         gfn_t gfn;
+	 * 92         struct x86_exception fault;
+	 * 93 };
+	 */
 	fault->gfn = walker.gfn;
 	fault->max_level = walker.level;
 	fault->slot = kvm_vcpu_gfn_to_memslot(vcpu, fault->gfn);
@@ -812,10 +1124,33 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (r)
 		return r;
 
+	/*
+	 * 在以下设置guest_walker->pte_access:
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|522| <<FNAME(walk_addr_generic)>> walker->pte_access = FNAME(gpte_access)(pte_access ^ walk_nx_mask);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|560| <<FNAME>> FNAME(protect_clean_gpte)(mmu, &walker->pte_access, pte);
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4532| <<direct_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+	 *   - arch/x86/kvm/mmu/mmu.c|4612| <<kvm_tdp_mmu_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|815| <<FNAME(page_fault)>> r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
+	 *
+	 * walker.pte_access:
+	 * 应该是一次walk_addr_generic后的最后一级的pte和上面level的pde..等等的and
+	 * 之前的都存在unsigned int pt_access[PT_MAX_FULL_LEVELS];
+	 * 而且这里是修改过格式的: (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 */
 	r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
 	if (r != RET_PF_CONTINUE)
 		return r;
 
+	/*
+	 * 特别多and的条件:
+	 * 1. fault->write
+	 * 2. !(walker.pte_access & ACC_WRITE_MASK)
+	 * 3. !is_cr0_wp(vcpu->arch.mmu)
+	 * 4. !fault->user
+	 * 5. fault->slot
+	 */
 	/*
 	 * Do not change pte_access if the pfn is a mmio page, otherwise
 	 * we will cache the incorrect access into mmio spte.
@@ -900,6 +1235,19 @@ static gpa_t FNAME(gva_to_gpa)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
  *   0: the spte is synced and no tlb flushing is required
  * > 0: the spte is synced and tlb flushing is required
  */
+/*
+ * 在以下调用(sync_spte):
+ *   - arch/x86/kvm/mmu/mmu.c|2055| <<kvm_sync_spte>> return vcpu->arch.mmu->sync_spte(vcpu, sp, i);
+ * 在以下使用(sync_spte):
+ *   - arch/x86/kvm/mmu/mmu.c|2043| <<kvm_sync_page_check>> if (WARN_ON_ONCE(sp->role.direct || !vcpu->arch.mmu->sync_spte ||
+ *   - arch/x86/kvm/mmu/mmu.c|4981| <<nonpaging_init_context>> context->sync_spte = NULL;
+ *   - arch/x86/kvm/mmu/mmu.c|5655| <<paging64_init_context>> context->sync_spte = paging64_sync_spte;
+ *   - arch/x86/kvm/mmu/mmu.c|5666| <<paging32_init_context>> context->sync_spte = paging32_sync_spte;
+ *   - arch/x86/kvm/mmu/mmu.c|5864| <<init_kvm_tdp_mmu>> context->sync_spte = NULL;
+ *   - arch/x86/kvm/mmu/mmu.c|6031| <<kvm_init_shadow_ept_mmu>> context->sync_spte = ept_sync_spte;
+ *   - arch/x86/kvm/mmu/mmu.c|6088| <<init_kvm_nested_mmu>> g_context->sync_spte = NULL;
+ *   - arch/x86/kvm/mmu/mmu.c|6587| <<kvm_mmu_invalidate_addr>> if (!mmu->sync_spte)
+ */
 static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int i)
 {
 	bool host_writable;
@@ -917,6 +1265,9 @@ static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int
 	first_pte_gpa = FNAME(get_level1_sp_gpa)(sp);
 	pte_gpa = first_pte_gpa + i * sizeof(pt_element_t);
 
+	/*
+	 * gpte是i的guest的VM地址
+	 */
 	if (kvm_vcpu_read_guest_atomic(vcpu, pte_gpa, &gpte,
 				       sizeof(pt_element_t)))
 		return -1;
@@ -926,9 +1277,20 @@ static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int
 
 	gfn = gpte_to_gfn(gpte);
 	pte_access = sp->role.access;
+	/*
+	 * 根据gpte的内容(比如后3位或者最高位), 转换成ACC_xxx的模式
+	 * 以记录这个gpte是否允许exec, read, write
+	 * #define ACC_EXEC_MASK    1
+	 * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+	 * #define ACC_USER_MASK    PT_USER_MASK
+	 * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 */
 	pte_access &= FNAME(gpte_access)(gpte);
 	FNAME(protect_clean_gpte)(vcpu->arch.mmu, &pte_access, gpte);
 
+	/*
+	 * 只在这里调用, 返回true说明做好了mmio
+	 */
 	if (sync_mmio_spte(vcpu, &sp->spt[i], gfn, pte_access))
 		return 0;
 
@@ -964,6 +1326,23 @@ static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int
 		  spte_to_pfn(spte), spte, true, false,
 		  host_writable, &spte);
 
+	/*
+	 * 注释
+	 * Rules for using mmu_spte_update:
+	 * Update the state bits, it means the mapped pfn is not changed.
+	 *
+	 * Whenever an MMU-writable SPTE is overwritten with a read-only SPTE, remote
+	 * TLBs must be flushed. Otherwise rmap_write_protect will find a read-only
+	 * spte, even though the writable spte might be cached on a CPU's TLB.
+	 *
+	 * Returns true if the TLB needs to be flushed
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1316| <<spte_write_protect>> return mmu_spte_update(sptep, spte);
+	 *   - arch/x86/kvm/mmu/mmu.c|1344| <<spte_clear_dirty>> return mmu_spte_update(sptep, spte);
+	 *   - arch/x86/kvm/mmu/mmu.c|3105| <<mmu_set_spte>> flush |= mmu_spte_update(sptep, spte);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1323| <<FNAME(sync_spte)>> return mmu_spte_update(sptep, spte);
+	 */
 	return mmu_spte_update(sptep, spte);
 }
 
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 4a599130e..d977081ba 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -19,34 +19,514 @@
 #include <asm/memtype.h>
 #include <asm/vmx.h>
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> __kvm_x86_vendor_init()
+ *       -> kvm_mmu_vendor_module_init()
+ *          -> kvm_mmu_reset_all_pte_masks()
+ *             -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *    -> ops->hardware_setup = hardware_setup()
+ *       -> kvm_mmu_set_ept_masks()
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull; (1 << 8)
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ */
+
+/*
+ * 在以下设置enable_mmio_caching:
+ *   - arch/x86/kvm/mmu/spte.c|388| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = allow_mmio_caching;
+ *   - arch/x86/kvm/mmu/spte.c|422| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = false;
+ * 在以下使用enable_mmio_caching:
+ *   - arch/x86/kvm/mmu/spte.c|24| <<global>> module_param_named(mmio_caching, enable_mmio_caching, bool, 0444);
+ *   - arch/x86/kvm/mmu/mmu.c|3347| <<kvm_handle_noslot_fault>> if (unlikely(!enable_mmio_caching))
+ *   - arch/x86/kvm/mmu/spte.c|64| <<kvm_mmu_spte_module_init>> allow_mmio_caching = enable_mmio_caching;
+ *   - arch/x86/kvm/mmu/spte.c|389| <<kvm_mmu_set_mmio_spte_mask>> if (!enable_mmio_caching)
+ *   - arch/x86/kvm/mmu/spte.h|303| <<is_mmio_spte>> likely(enable_mmio_caching);
+ *   - arch/x86/kvm/svm/sev.c|2254| <<sev_hardware_setup>> if (!enable_mmio_caching)
+ */
 bool __read_mostly enable_mmio_caching = true;
+/*
+ * 在以下设置allow_mmio_caching:
+ *   - arch/x86/kvm/mmu/spte.c|64| <<kvm_mmu_spte_module_init>> allow_mmio_caching = enable_mmio_caching;
+ * 在以下使用allow_mmio_caching:
+ *   - arch/x86/kvm/mmu/spte.c|388| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = allow_mmio_caching;
+ */
 static bool __ro_after_init allow_mmio_caching;
 module_param_named(mmio_caching, enable_mmio_caching, bool, 0444);
 EXPORT_SYMBOL_GPL(enable_mmio_caching);
 
+/*
+ * 在以下设置shadow_host_writable_mask:
+ *   - arch/x86/kvm/mmu/spte.c|551| <<kvm_mmu_set_ept_masks>> shadow_host_writable_mask = EPT_SPTE_HOST_WRITABLE;
+ *   - arch/x86/kvm/mmu/spte.c|615| <<kvm_mmu_reset_all_pte_masks>> shadow_host_writable_mask = DEFAULT_SPTE_HOST_WRITABLE;
+ * 在以下使用shadow_host_writable_mask:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|961| <<FNAME(sync_spte)>> host_writable = spte & shadow_host_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|287| <<make_spte>> spte |= shadow_host_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|424| <<kvm_mmu_changed_pte_notifier_make_spte>> new_spte &= ~shadow_host_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.h|504| <<check_spte_writable_invariants>> WARN_ONCE(!(spte & shadow_host_writable_mask), KBUILD_MODNAME ": MMU-writable SPTE is not Host-writable: %llx", spte);
+ *
+ * 有三个mask:
+ * - shadow_host_writable_mask:
+ *   backend的page在host上是否writable(EPT用EPT_SPTE_HOST_WRITABLE, 普通用DEFAULT_SPTE_HOST_WRITABLE)
+ *   选取的是ept和普通分别不用的bit
+ *   更多是GUP从host获得指示
+ * - shadow_mmu_writable_mask
+ *   指示这个pte的page是不是writable的, 不能简单用bit=1, 没法分清是真的不能还是mmio或是别的
+ *   设置这个mask是告诉user这个pte一定是writable
+ *   EPT用EPT_SPTE_MMU_WRITABLE, 普通用DEFAULT_SPTE_MMU_WRITABLE
+ * - pte & PT_WRITABLE_MASK (看看bit设置了吗)
+ *   这个单纯是看pte的bie=1
+ *
+ *  shadow_mmu_writable_mask, aka MMU-writable -
+ *    Cleared on SPTEs that KVM is currently write-protecting for shadow paging
+ *    purposes (case 2 above). --> 保护影子页表
+ *
+ *  shadow_host_writable_mask, aka Host-writable -
+ *    Cleared on SPTEs that are not host-writable (case 3 above) --> backend memslot不可写
+ *
+ * 查看is_writable_pte()的注释
+ */
 u64 __read_mostly shadow_host_writable_mask;
+/*
+ * 在以下设置shadow_mmu_writable_mask:
+ *   - arch/x86/kvm/mmu/spte.c|609| <<kvm_mmu_set_ept_masks>> shadow_mmu_writable_mask = EPT_SPTE_MMU_WRITABLE;
+ *   - arch/x86/kvm/mmu/spte.c|673| <<kvm_mmu_reset_all_pte_masks>> shadow_mmu_writable_mask = DEFAULT_SPTE_MMU_WRITABLE;
+ * 在以下使用shadow_mmu_writable_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|1253| <<spte_write_protect>> spte &= ~shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|336| <<make_spte>> spte |= PT_WRITABLE_MASK | shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|356| <<make_spte>> spte &= ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ *   - arch/x86/kvm/mmu/spte.c|464| <<kvm_mmu_changed_pte_notifier_make_spte>> new_spte &= ~shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.h|503| <<check_spte_writable_invariants>> if (spte & shadow_mmu_writable_mask)
+ *   - arch/x86/kvm/mmu/spte.h|514| <<is_mmu_writable_spte>> return spte & shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1830| <<write_protect_gfn>> new_spte = iter.old_spte & ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ *
+ * 用来标记虚拟化的页表里的entry是否指示writable
+ */
 u64 __read_mostly shadow_mmu_writable_mask;
+/*
+ * 在以下设置shadow_nx_mask:
+ *   - arch/x86/kvm/mmu/spte.c|590| <<kvm_mmu_set_ept_masks>> shadow_nx_mask = 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|658| <<kvm_mmu_reset_all_pte_masks>> shadow_nx_mask = PT64_NX_MASK;
+ * 在以下使用shadow_nx_mask:
+ *   - arch/x86/kvm/mmu/spte.c|305| <<make_spte>> spte |= shadow_nx_mask;
+ *   - arch/x86/kvm/mmu/spte.c|388| <<make_spte_executable>> spte &= ~shadow_nx_mask;
+ *   - arch/x86/kvm/mmu/spte.h|73| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *   - arch/x86/kvm/mmu/spte.h|379| <<is_executable_pte>> return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
+ *
+ * 普通page table entry的bit=63 (XD):
+ * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+ * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+ * otherwise, reserved (must be 0)
+ *
+ * 普通pte支持nx bit, ept不支持nx bit
+ * 普通pte不支持x bit, ept支持x bit
+ */
 u64 __read_mostly shadow_nx_mask;
+/*
+ * 在以下设置shadow_x_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|5086| <<boot_cpu_is_amd>> return shadow_x_mask == 0;
+ *   - arch/x86/kvm/mmu/spte.c|612| <<kvm_mmu_set_ept_masks>> shadow_x_mask = VMX_EPT_EXECUTABLE_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|680| <<kvm_mmu_reset_all_pte_masks>> shadow_x_mask = 0;
+ * 在以下使用shadow_x_mask:
+ *   - arch/x86/kvm/mmu/spte.c|324| <<make_spte>> spte |= shadow_x_mask;
+ *   - arch/x86/kvm/mmu/spte.c|410| <<make_spte_executable>> spte |= shadow_x_mask;
+ *   - arch/x86/kvm/mmu/spte.c|466| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK | shadow_user_mask | shadow_x_mask | shadow_me_value;
+ *   - arch/x86/kvm/mmu/spte.h|73| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *   - arch/x86/kvm/mmu/spte.h|379| <<is_executable_pte>> return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
+ *
+ * 普通page table entry的bit=63 (XD):
+ * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+ * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+ * otherwise, reserved (must be 0)
+ *
+ * 普通pte支持nx bit, ept不支持nx bit
+ * 普通pte不支持x bit, ept支持x bit
+ */
 u64 __read_mostly shadow_x_mask; /* mutual exclusive with nx_mask */
+/*
+ * 在以下设置shadow_user_mask:
+ *   - arch/x86/kvm/mmu/spte.c|608| <<kvm_mmu_set_ept_masks>> shadow_user_mask = VMX_EPT_READABLE_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|676| <<kvm_mmu_reset_all_pte_masks>> shadow_user_mask = PT_USER_MASK;
+ * 在以下使用shadow_user_mask:
+ *   - arch/x86/kvm/mmu/mmutrace.h|356| <<__field>> __entry->u = shadow_user_mask ? !!(__entry->spte & shadow_user_mask) : -1;
+ *   - arch/x86/kvm/mmu/spte.c|329| <<make_spte>> spte |= shadow_user_mask;
+ *   - arch/x86/kvm/mmu/spte.c|466| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK | shadow_user_mask | shadow_x_mask | shadow_me_value;
+ *   - arch/x86/kvm/mmu/spte.h|72| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *
+ * 关于PT_USER_MASK, bit=2(U/S):
+ * User/supervisor; if 0, user-mode accesses are not allowed to the 4-KByte
+ * page referenced by this entry (see Section 4.6)
+ *
+ * 普通page fault来的时候error code会设置PT_USER_MASK
+ *
+ * 注释: 因为ept支持exec bit, 所以用shadow_user_mask表示readable.
+ * For the EPT case, shadow_present_mask is 0 if hardware
+ * supports exec-only page table entries.  In that case,
+ * ACC_USER_MASK and shadow_user_mask are used to represent
+ * read access.  See FNAME(gpte_access) in paging_tmpl.h.
+ */
 u64 __read_mostly shadow_user_mask;
+/*
+ * 在以下设置shadow_accessed_mask:
+ *   - arch/x86/kvm/mmu/spte.c|464| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|528| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+ * 在以下使用shadow_accessed_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|623| <<mmu_spte_age>> clear_bit((ffs(shadow_accessed_mask) - 1), (unsigned long *)sptep);
+ *   - arch/x86/kvm/mmu/spte.c|156| <<spte_has_volatile_bits>> if (!(spte & shadow_accessed_mask) || (is_writable_pte(spte) && !(spte & shadow_dirty_mask)))
+ *   - arch/x86/kvm/mmu/spte.c|356| <<make_nonleaf_spte>> spte |= shadow_accessed_mask;
+ *   - arch/x86/kvm/mmu/spte.c|380| <<mark_spte_for_access_track>> return spte & ~shadow_accessed_mask;
+ *   - arch/x86/kvm/mmu/spte.h|319| <<kvm_ad_enabled>> return !!shadow_accessed_mask;
+ *   - arch/x86/kvm/mmu/spte.h|353| <<spte_shadow_accessed_mask>> return spte_ad_enabled(spte) ? shadow_accessed_mask : 0;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1279| <<age_gfn_range>> iter->old_spte = tdp_mmu_clear_spte_bits(iter->sptep, iter->old_spte, shadow_accessed_mask, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1281| <<age_gfn_range>> new_spte = iter->old_spte & ~shadow_accessed_mask;
+ *
+ * 对于普通的页表:
+ * bit-5: Accessed: indicates whether software has accessed the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ * bit-6: Dirty: indicates whether software has written to the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ *
+ * 对于EPT:
+ * - bit-8: If bit 6 (ignore guest PAT) of EPTP is 1, accessed flag for EPT;
+ *   indicates whether software has accessed the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ * - bit-9: If bit 6 (ignore guest PAT) of EPTP is 1, dirty flag for EPT;
+ *   indicates whether software has written to the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ *
+ * 哪些bit用来记录pte被访问过
+ */
 u64 __read_mostly shadow_accessed_mask;
+/*
+ * 在以下设置shadow_dirty_mask:
+ *   - arch/x86/kvm/mmu/spte.c|610| <<kvm_mmu_set_ept_masks>> shadow_dirty_mask = has_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|678| <<kvm_mmu_reset_all_pte_masks>> shadow_dirty_mask = PT_DIRTY_MASK;
+ * 在以下使用shadow_dirty_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|1277| <<spte_clear_dirty>> spte &= ~shadow_dirty_mask;
+ *   - arch/x86/kvm/mmu/spte.c|272| <<spte_has_volatile_bits>> (is_writable_pte(spte) && !(spte & shadow_dirty_mask)))
+ *   - arch/x86/kvm/mmu/spte.h|359| <<spte_shadow_dirty_mask>> return spte_ad_enabled(spte) ? shadow_dirty_mask : 0;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1625| <<clear_dirty_gfn_range>> u64 dbit = kvm_ad_enabled() ? shadow_dirty_mask : PT_WRITABLE_MASK;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1688| <<clear_dirty_pt_masked>> u64 dbit = (wrprot || !kvm_ad_enabled()) ? PT_WRITABLE_MASK : shadow_dirty_mask;
+ *
+ * 对于普通的页表:
+ * bit-5: Accessed: indicates whether software has accessed the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ * bit-6: Dirty: indicates whether software has written to the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ *
+ * 对于EPT:
+ * - bit-8: If bit 6 (ignore guest PAT) of EPTP is 1, accessed flag for EPT;
+ *   indicates whether software has accessed the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ * - bit-9: If bit 6 (ignore guest PAT) of EPTP is 1, dirty flag for EPT;
+ *   indicates whether software has written to the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ *
+ * 哪些bit用来记录被dirty过
+ */
 u64 __read_mostly shadow_dirty_mask;
+/*
+ * 在以下设置shadow_mmio_value:
+ *   - arch/x86/kvm/mmu/spte.c|444| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_value = mmio_value;
+ * 在以下使用shadow_mmio_value:
+ *   - arch/x86/kvm/mmu/spte.c|104| <<make_mmio_spte>> WARN_ON_ONCE(!shadow_mmio_value);
+ *   - arch/x86/kvm/mmu/spte.c|107| <<make_mmio_spte>> spte |= shadow_mmio_value | access;
+ *   - arch/x86/kvm/mmu/spte.h|302| <<is_mmio_spte>> return (spte & shadow_mmio_mask) == shadow_mmio_value &&
+ *
+ * 被shadow_mmio_mask后什么样子的value代表这是mmio entry
+ *
+ * 应该是只有mmio cache被enable的时候才用
+ * 1. 如果disable了, 每次都page fault, 然后查看memslot有没有
+ * 没有就是mmio, 然后emulate. 这种混在一起每次查memslot会影响性能.
+ * 2. 如果enable了, 会尽可能优化来避免每次mmio查看memslot
+ * EPT的mmio value就是会产生ept misconfig的value, 在mis config处理mmio, 不需要查找memslot
+ * regular pte的话支持太多bit就不行了,
+ * 否则可以用一个high reserved bit产生因访问reserved bit而造成的page fault:
+ * Set a reserved PA bit in MMIO SPTEs to generate page faults with
+ * PFEC.RSVD=1 on MMIO accesses.  64-bit PTEs (PAE, x86-64, and EPT
+ * paging) support a maximum of 52 bits of PA, i.e. if the CPU supports
+ * 52-bit physical addresses then there are no reserved PA bits in the
+ * PTEs and so the reserved PA approach must be disabled.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|491| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK, 0);
+ *   - arch/x86/kvm/mmu/spte.c|559| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+ *   - arch/x86/kvm/svm/svm.c|5074| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+ */
 u64 __read_mostly shadow_mmio_value;
+/*
+ * 在以下设置shadow_mmio_mask:
+ *   - arch/x86/kvm/mmu/spte.c|445| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_mask = mmio_mask;
+ * 在以下使用shadow_mmio_mask:
+ *   - arch/x86/kvm/mmu/spte.h|302| <<is_mmio_spte>> return (spte & shadow_mmio_mask) == shadow_mmio_value &&
+ *
+ * shadow_mmio_mask表示哪些bit可能用作mmio
+ */
 u64 __read_mostly shadow_mmio_mask;
+/*
+ * error code:
+ * - P  1 bit   Present When set, the page fault was caused by a
+ *   page-protection violation. When not set, it was caused by a
+ *   non-present page.
+ * - W  1 bit   Write   When set, the page fault was caused by a write
+ *   access. When not set, it was caused by a read access.
+ * - U  1 bit   User    When set, the page fault was caused while CPL =
+ *   3. This does not necessarily mean that the page fault was a
+ *   privilege violation.
+ * - R  1 bit   Reserved write  When set, one or more page directory
+ *   entries contain reserved bits which are set to 1. This only
+ *   applies when the PSE or PAE flags in CR4 are set to 1.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|491| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK, 0);
+ *   - arch/x86/kvm/mmu/spte.c|559| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+ *   - arch/x86/kvm/svm/svm.c|5074| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+ *
+ * 在以下设置shadow_mmio_access_mask:
+ *   - arch/x86/kvm/mmu/spte.c|446| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_access_mask = access_mask;
+ * 在以下使用shadow_mmio_access_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|313| <<get_mmio_spte_access>> return spte & shadow_mmio_access_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|3340| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+ *   - arch/x86/kvm/mmu/spte.c|106| <<make_mmio_spte>> access &= shadow_mmio_access_mask;
+ *
+ * 4af7715110a2617fc40ac2c1232f664019269f3a
+ * KVM: x86/mmu: Add explicit access mask for MMIO SPTEs
+ *
+ * commit的一些注释:
+ * When shadow paging is enabled, KVM tracks the allowed access type for
+ * MMIO SPTEs so that it can do a permission check on a MMIO GVA cache hit
+ * without having to walk the guest's page tables.  The tracking is done
+ * by retaining the WRITE and USER bits of the access when inserting the
+ * MMIO SPTE (read access is implicitly allowed), which allows the MMIO
+ * page fault handler to retrieve and cache the WRITE/USER bits from the
+ * SPTE. --> "retaining the WRITE and USER bits"是指在host的shadow table保留吗???
+ *
+ * 比如: ept是0
+ * regular pte是ACC_WRITE_MASK | ACC_USER_MASK
+ *
+ *  针对shadow page table来说, 所以shadow_mmio_access_mask:
+ *
+ * 1. 从一个pte中取出R/W和U/S权限, 在fastpath给vcpu_cache_mmio_info()缓存的时候用.
+ *
+ * 2. 从FNAME(gpte_access)改装过的access中取出R/W和U/S权限, 在制作mmio pte的时候再放进去.
+ *
+ * 3. 从FNAME(gpte_access)改装过的access中取出R/W和U/S权限, 在slowpath给vcpu_cache_mmio_info()缓存的时候用.
+ *
+ * 表示在cache mmio_access的时候都cache哪些bit
+ * struct kvm_vcpu *vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> u64 mmio_gva;
+ *    -> unsigned mmio_access;
+ *    -> gfn_t mmio_gfn;
+ *    -> u64 mmio_gen;
+ */
 u64 __read_mostly shadow_mmio_access_mask;
+/*
+ * 在以下设置shadow_present_mask:
+ *   - arch/x86/kvm/mmu/spte.c|468| <<kvm_mmu_set_ept_masks>> shadow_present_mask = has_exec_only ? 0ull : VMX_EPT_READABLE_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|532| <<kvm_mmu_reset_all_pte_masks>> shadow_present_mask = PT_PRESENT_MASK;
+ * 在以下使用shadow_present_mask:
+ *   - rch/x86/kvm/mmu/mmutrace.h|354| <<__field>> __entry->r = shadow_present_mask || (__entry->spte & PT_PRESENT_MASK);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|942| <<FNAME(sync_spte)>> if ((!pte_access && !shadow_present_mask) ||
+ *   - arch/x86/kvm/mmu/spte.c|174| <<make_spte>> WARN_ON_ONCE(!pte_access && !shadow_present_mask);
+ *   - arch/x86/kvm/mmu/spte.c|187| <<make_spte>> spte |= shadow_present_mask;
+ *   - arch/x86/kvm/mmu/spte.c|350| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK |
+ *
+ * 注释: 因为ept支持exec bit, 所以用shadow_user_mask表示readable.
+ * For the EPT case, shadow_present_mask is 0 if hardware
+ * supports exec-only page table entries.  In that case,
+ * ACC_USER_MASK and shadow_user_mask are used to represent
+ * read access.  See FNAME(gpte_access) in paging_tmpl.h.
+ */
 u64 __read_mostly shadow_present_mask;
+/*
+ * 在以下设置shadow_memtype_mask:
+ *   - arch/x86/kvm/mmu/spte.c|439| <<kvm_mmu_set_ept_masks>> shadow_memtype_mask = VMX_EPT_MT_MASK | VMX_EPT_IPAT_BIT;
+ *   - arch/x86/kvm/mmu/spte.c|496| <<kvm_mmu_reset_all_pte_masks>> shadow_memtype_mask = 0;
+ * 在以下使用shadow_memtype_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|4625| <<__kvm_mmu_honors_guest_mtrrs>> return vm_has_noncoherent_dma && shadow_memtype_mask;
+ *   - arch/x86/kvm/mmu/spte.c|192| <<make_spte>> if (shadow_memtype_mask)
+ *
+ * 对于EPT,
+ * bit-3/4/5(VMX_EPT_MT_MASK): EPT memory type for this 4-KByte page (see Section 29.3.7).
+ * bit-6(VMX_EPT_IPAT_BIT): Ignore PAT memory type for this 4-KByte page (see Section 29.3.7).
+ */
 u64 __read_mostly shadow_memtype_mask;
+/*
+ * 在以下设置shadow_me_value:
+ *   - arch/x86/kvm/mmu/spte.c|625| <<kvm_mmu_set_me_spte_mask>> shadow_me_value = me_value;
+ *   - arch/x86/kvm/mmu/spte.c|721| <<kvm_mmu_reset_all_pte_masks>> shadow_me_value = 0;
+ * 在以下使用shadow_me_value:
+ *   - arch/x86/kvm/mmu/mmu.c|3741| <<mmu_alloc_direct_roots>> mmu->pae_root[i] = root | PT_PRESENT_MASK | shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|3879| <<mmu_alloc_shadow_roots>> pm_mask = PT_PRESENT_MASK | shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|5077| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[0][i] &= ~shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|5078| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[1][i] &= ~shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|6207| <<__kvm_mmu_create>> WARN_ON_ONCE(shadow_me_value);
+ *   - arch/x86/kvm/mmu/spte.c|381| <<make_spte>> if (shadow_me_value && !kvm_is_mmio_pfn(pfn))
+ *   - arch/x86/kvm/mmu/spte.c|382| <<make_spte>> spte |= shadow_me_value;
+ *   - arch/x86/kvm/mmu/spte.c|496| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK | shadow_user_mask | shadow_x_mask | shadow_me_value;
+ *
+ * 用在memory encryption
+ * 对于vmx: me_value = 0
+ */
 u64 __read_mostly shadow_me_value;
+/*
+ * 在以下设置shadow_me_mask:
+ *   - arch/x86/kvm/mmu/spte.c|518| <<kvm_mmu_set_me_spte_mask>> shadow_me_mask = me_mask;
+ *   - arch/x86/kvm/mmu/spte.c|612| <<kvm_mmu_reset_all_pte_masks>> shadow_me_mask = 0;
+ * 在以下使用shadow_me_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|5065| <<reset_shadow_zero_bits_mask>> if (!shadow_me_mask)
+ *   - arch/x86/kvm/mmu/mmu.c|5075| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[0][i] |= shadow_me_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|5076| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[1][i] |= shadow_me_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|5110| <<reset_tdp_shadow_zero_bits_mask>> if (!shadow_me_mask)
+ *   - arch/x86/kvm/mmu/mmu.c|5114| <<reset_tdp_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[0][i] &= ~shadow_me_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|5115| <<reset_tdp_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[1][i] &= ~shadow_me_mask;
+ *   - arch/x86/kvm/mmu/spte.h|73| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *
+ * 用在memory encryption
+ * 对于vmx: me_mask = rsvd_bits(boot_cpu_data.x86_phys_bits, kvm_get_shadow_phys_bits() - 1);
+ */
 u64 __read_mostly shadow_me_mask;
+/*
+ * 在以下设置shadow_acc_track_mask:
+ *   - arch/x86/kvm/mmu/spte.c|550| <<kvm_mmu_set_ept_masks>> shadow_acc_track_mask = VMX_EPT_RWX_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|611| <<kvm_mmu_reset_all_pte_masks>> shadow_acc_track_mask = 0;
+ * 在以下使用shadow_acc_track_mask:
+ *   - arch/x86/kvm/mmu/spte.c|448| <<mark_spte_for_access_track>> spte &= ~shadow_acc_track_mask;
+ *   - arch/x86/kvm/mmu/spte.h|364| <<is_access_track_spte>> return !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;
+ *   - arch/x86/kvm/mmu/spte.h|545| <<restore_acc_track_spte>> spte &= ~shadow_acc_track_mask;
+ *
+ * 注释:
+ * SPTEs in MMUs without A/D bits are marked with SPTE_TDP_AD_DISABLED;
+ * shadow_acc_track_mask is the set of bits to be cleared in non-accessed
+ * pages.
+ *
+ * 用来trap对page的access
+ * R, W, X
+ */
 u64 __read_mostly shadow_acc_track_mask;
 
+/*
+ * commit 28a1f3ac1d0c8558ee4453d9634dad891a6e922e
+ * Author: Junaid Shahid <junaids@google.com>
+ * Date:   Tue Aug 14 10:15:34 2018 -0700
+ *
+ * kvm: x86: Set highest physical address bits in non-present/reserved SPTEs
+ *
+ * Always set the 5 upper-most supported physical address bits to 1 for SPTEs
+ * that are marked as non-present or reserved, to make them unusable for
+ * L1TF attacks from the guest. Currently, this just applies to MMIO SPTEs.
+ * (We do not need to mark PTEs that are completely 0 as physical page 0
+ * is already reserved.)
+ *
+ * This allows mitigation of L1TF without disabling hyper-threading by using
+ * shadow paging mode instead of EPT.
+ *
+ * Signed-off-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ *
+ * 在以下设置shadow_nonpresent_or_rsvd_mask:
+ *   - arch/x86/kvm/mmu/spte.c|584| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_mask = 0;
+ *   - arch/x86/kvm/mmu/spte.c|591| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_mask = rsvd_bits(low_phys_bits, boot_cpu_data.x86_cache_bits - 1);
+ * 在以下使用shadow_nonpresent_or_rsvd_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|306| <<get_mmio_spte_gfn>> gpa |= (spte >> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN) & shadow_nonpresent_or_rsvd_mask;
+ *   - arch/x86/kvm/mmu/spte.c|163| <<make_mmio_spte>> spte |= gpa | shadow_nonpresent_or_rsvd_mask;
+ *   - arch/x86/kvm/mmu/spte.c|164| <<make_mmio_spte>> spte |= (gpa & shadow_nonpresent_or_rsvd_mask)
+ *   - arch/x86/kvm/mmu/spte.c|488| <<kvm_mmu_set_mmio_spte_mask>> if (WARN_ON(mmio_value & (shadow_nonpresent_or_rsvd_mask << SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)))
+ *
+ * 测试的例子 (不支持ME):
+ * boot_cpu_data.x86_phys_bits = 46
+ * boot_cpu_data.x86_cache_bits = 46
+ * shadow_phys_bits=46
+ * bit: 41 - 45 (5个bit)
+ * shadow_nonpresent_or_rsvd_mask=0x00003e0000000000
+ * bit: 12 - 40
+ * shadow_nonpresent_or_rsvd_lower_gfn_mask=0x000001fffffff000
+ *
+ * 注释:
+ * This mask must be set on all non-zero Non-Present or Reserved SPTEs in order
+ * to guard against L1TF attacks.
+ */
 u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
+/*
+ * 在以下设置shadow_nonpresent_or_rsvd_lower_gfn_mask:
+ *   - arch/x86/kvm/mmu/spte.c|524| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_lower_gfn_mask = rsvd_bits(low_phys_bits, boot_cpu_data.x86_cache_bits - 1);
+ * 在以下使用shadow_nonpresent_or_rsvd_lower_gfn_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|303| <<get_mmio_spte_gfn>> u64 gpa = spte & shadow_nonpresent_or_rsvd_lower_gfn_mask;
+ *   - arch/x86/kvm/mmu/spte.c|401| <<kvm_mmu_set_mmio_spte_mask>> WARN_ON(mmio_value & shadow_nonpresent_or_rsvd_lower_gfn_mask);
+ */
 u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
+/*
+ * 在以下设置shadow_phys_bits:
+ *   - arch/x86/kvm/mmu/spte.c|501| <<kvm_mmu_reset_all_pte_masks>> shadow_phys_bits = kvm_get_shadow_phys_bits();
+ * 在以下使用shadow_phys_bits:
+ *   - arch/x86/kvm/mmu.h|79| <<kvm_mmu_max_gfn>> int max_gpa_bits = likely(tdp_enabled) ? shadow_phys_bits : 52;
+ *   - arch/x86/kvm/mmu/mmu.c|5038| <<reserved_hpa_bits>> return rsvd_bits(shadow_phys_bits, 63);
+ *   - arch/x86/kvm/mmu/spte.c|554| <<kvm_mmu_reset_all_pte_masks>> if (shadow_phys_bits < 52)
+ *
+ * The number of non-reserved physical address bits irrespective of features
+ * that repurpose legal bits, e.g. MKTME.
+ * host phys bit最多支持的bit数目
+ */
 u8 __read_mostly shadow_phys_bits;
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> __kvm_x86_vendor_init()
+ *       -> kvm_mmu_vendor_module_init()
+ *          -> kvm_mmu_reset_all_pte_masks()
+ *             -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *    -> ops->hardware_setup = hardware_setup()
+ *       -> kvm_mmu_set_ept_masks()
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull; (1 << 8)
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7087| <<kvm_mmu_x86_module_init>> kvm_mmu_spte_module_init();
+ *
+ * module_init(kvm_x86_init): kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ */
 void __init kvm_mmu_spte_module_init(void)
 {
+	/*
+	 * 在以下设置enable_mmio_caching:
+	 *   - arch/x86/kvm/mmu/spte.c|388| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = allow_mmio_caching;
+	 *   - arch/x86/kvm/mmu/spte.c|422| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = false;
+	 *
+	 * 在以下设置allow_mmio_caching:
+	 *   - arch/x86/kvm/mmu/spte.c|64| <<kvm_mmu_spte_module_init>> allow_mmio_caching = enable_mmio_caching;
+	 *
+	 * 下面注释的vendor module应该是kvm_intel
+	 */
 	/*
 	 * Snapshot userspace's desire to allow MMIO caching.  Whether or not
 	 * KVM can actually enable MMIO caching depends on vendor-specific
@@ -57,6 +537,10 @@ void __init kvm_mmu_spte_module_init(void)
 	allow_mmio_caching = enable_mmio_caching;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|471| <<make_mmio_spte>> u64 spte = generation_mmio_spte_mask(gen);
+ */
 static u64 generation_mmio_spte_mask(u64 gen)
 {
 	u64 mask;
@@ -68,16 +552,62 @@ static u64 generation_mmio_spte_mask(u64 gen)
 	return mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|301| <<mark_mmio_spte>> u64 spte = make_mmio_spte(vcpu, gfn, access);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1062| <<tdp_mmu_map_handle_target_level>> new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
+ *
+ * EXIT_REASON_EXCEPTION_NMI:handle_exception_nmi()
+ * -> kvm_handle_page_fault()
+ *    -> kvm_mmu_page_fault()
+ *       -> kvm_mmu_do_page_fault()
+ *          -> FNAME(page_fault)
+ *             -> FNAME(fetch)
+ *                -> mmu_set_spte()
+ *                   -> mark_mmio_spte()
+ *                      -> make_mmio_spte()
+ *                         -> access &= shadow_mmio_access_mask;
+ *                         -> spte |= shadow_mmio_value | access; --> BIT_ULL(51) | PT_PRESENT_MASK;
+ * 第一次fault后设置mmio的情况(以后可以走cache的fastpath)
+ */
 u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)
 {
 	u64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;
+	/*
+	 * 只在此处调用
+	 */
 	u64 spte = generation_mmio_spte_mask(gen);
 	u64 gpa = gfn << PAGE_SHIFT;
 
+	/*
+	 * 在以下设置shadow_mmio_value:
+	 *   - arch/x86/kvm/mmu/spte.c|444| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_value = mmio_value;
+	 *
+	 * 应该是只有mmio cache被enable的时候才用
+	 *
+	 * 被shadow_mmio_mask后什么样子的value代表这是mmio entry
+	 */
 	WARN_ON_ONCE(!shadow_mmio_value);
 
+	/*
+	 * 比如: ept是0
+	 * regular pte是ACC_WRITE_MASK | ACC_USER_MASK
+	 */
 	access &= shadow_mmio_access_mask;
+	/*
+	 * regular: BIT_ULL(51) | PT_PRESENT_MASK;
+	 */
 	spte |= shadow_mmio_value | access;
+	/*
+	 * 测试的例子 (不支持ME):
+	 * boot_cpu_data.x86_phys_bits = 46
+	 * boot_cpu_data.x86_cache_bits = 46
+	 * shadow_phys_bits=46
+	 * bit: 41 - 45 (5个bit)
+	 * shadow_nonpresent_or_rsvd_mask=0x00003e0000000000
+	 * bit: 12 - 40
+	 * shadow_nonpresent_or_rsvd_lower_gfn_mask=0x000001fffffff000
+	 */
 	spte |= gpa | shadow_nonpresent_or_rsvd_mask;
 	spte |= (gpa & shadow_nonpresent_or_rsvd_mask)
 		<< SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;
@@ -85,6 +615,13 @@ u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|616| <<make_spte>> spte |= static_call(kvm_x86_get_mt_mask)(vcpu, gfn, kvm_is_mmio_pfn(pfn));
+ *   - arch/x86/kvm/mmu/spte.c|622| <<make_spte>> if (shadow_me_value && !kvm_is_mmio_pfn(pfn))
+ *
+ * 确认host的pfn是否是mmio
+ */
 static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
 {
 	if (pfn_valid(pfn))
@@ -111,6 +648,12 @@ static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
  * The caller is responsible for checking if the SPTE is shadow-present, and
  * for determining whether or not the caller cares about non-leaf SPTEs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|507| <<mmu_spte_update_no_track>> if (!spte_has_volatile_bits(old_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|575| <<mmu_spte_clear_track_bits>> !spte_has_volatile_bits(old_spte))
+ *   - arch/x86/kvm/mmu/tdp_iter.h|59| <<kvm_tdp_mmu_spte_need_atomic_write>> spte_has_volatile_bits(old_spte);
+ */
 bool spte_has_volatile_bits(u64 spte)
 {
 	/*
@@ -119,13 +662,23 @@ bool spte_has_volatile_bits(u64 spte)
 	 * also, it can help us to get a stable is_writable_pte()
 	 * to ensure tlb flush is not missed.
 	 */
+	/*
+	 * mmu entry是writable的, 只是bit=1没设置
+	 */
 	if (!is_writable_pte(spte) && is_mmu_writable_spte(spte))
 		return true;
 
+	/*
+	 * 比如: ept硬件不支持access bit, 并且ept的0/1/2都没设置
+	 */
 	if (is_access_track_spte(spte))
 		return true;
 
 	if (spte_ad_enabled(spte)) {
+		/*
+		 * spte的accessed的bit没设置, 或者
+		 * mmu entry是writable的, 但是dirty bit没设置
+		 */
 		if (!(spte & shadow_accessed_mask) ||
 		    (is_writable_pte(spte) && !(spte & shadow_dirty_mask)))
 			return true;
@@ -134,16 +687,44 @@ bool spte_has_volatile_bits(u64 spte)
 	return false;
 }
 
+/*
+ * 在以下调用make_nonleaf_spte():
+ *   - arch/x86/kvm/mmu/mmu.c|2473| <<__link_shadow_page>> spte = make_nonleaf_spte(sp->spt, sp_ad_disabled(sp));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1115| <<tdp_mmu_link_sp>> u64 spte = make_nonleaf_spte(sp->spt, !kvm_ad_enabled());
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2983| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch, true, host_writable, &spte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|963| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access, gfn, spte_to_pfn(spte), spte, true, false, host_writable, &spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1064| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte, fault->prefetch, true,
+ *                                                 fault->map_writable, &new_spte);
+ */
 bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	       const struct kvm_memory_slot *slot,
 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
 	       u64 old_spte, bool prefetch, bool can_unsync,
 	       bool host_writable, u64 *new_spte)
 {
+	/*
+	 * 猜测sp是要制作的pte所在的level
+	 */
 	int level = sp->role.level;
+	/*
+	 * BIT_ULL(11)
+	 */
 	u64 spte = SPTE_MMU_PRESENT_MASK;
 	bool wrprot = false;
 
+	/*
+	 * 在以下设置shadow_present_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|468| <<kvm_mmu_set_ept_masks>> shadow_present_mask = has_exec_only ? 0ull : VMX_EPT_READABLE_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|532| <<kvm_mmu_reset_all_pte_masks>> shadow_present_mask = PT_PRESENT_MASK;
+	 *
+	 * 注释: 因为ept支持exec bit, 所以用shadow_user_mask表示readable.
+	 * For the EPT case, shadow_present_mask is 0 if hardware
+	 * supports exec-only page table entries.  In that case,
+	 * ACC_USER_MASK and shadow_user_mask are used to represent
+	 * read access.  See FNAME(gpte_access) in paging_tmpl.h.
+	 */
 	WARN_ON_ONCE(!pte_access && !shadow_present_mask);
 
 	if (sp->role.ad_disabled)
@@ -151,6 +732,11 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	else if (kvm_mmu_page_ad_need_write_protect(sp))
 		spte |= SPTE_TDP_AD_WRPROT_ONLY;
 
+	/*
+	 * 在以下设置shadow_present_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|468| <<kvm_mmu_set_ept_masks>> shadow_present_mask = has_exec_only ? 0ull : VMX_EPT_READABLE_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|532| <<kvm_mmu_reset_all_pte_masks>> shadow_present_mask = PT_PRESENT_MASK;
+	 */
 	/*
 	 * For the EPT case, shadow_present_mask is 0 if hardware
 	 * supports exec-only page table entries.  In that case,
@@ -158,6 +744,9 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	 * read access.  See FNAME(gpte_access) in paging_tmpl.h.
 	 */
 	spte |= shadow_present_mask;
+	/*
+	 * 不是prefetch说明是page fault, 标记被访问过
+	 */
 	if (!prefetch)
 		spte |= spte_shadow_accessed_mask(spte);
 
@@ -175,9 +764,24 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	 */
 	if (level > PG_LEVEL_4K && (pte_access & ACC_EXEC_MASK) &&
 	    is_nx_huge_page_enabled(vcpu->kvm)) {
+		/*
+		 * 三个条件:
+		 * 1. level > PG_LEVEL_4K
+		 * 2. (pte_access & ACC_EXEC_MASK)
+		 * 3. is_nx_huge_page_enabled(vcpu->kvm)
+		 */
 		pte_access &= ~ACC_EXEC_MASK;
 	}
 
+	/*
+	 * 普通page table entry的bit=63 (XD):
+	 * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+	 * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+	 * otherwise, reserved (must be 0)
+	 *
+	 * 普通pte支持nx bit, ept不支持nx bit
+	 * 普通pte不支持x bit, ept支持x bit
+	 */
 	if (pte_access & ACC_EXEC_MASK)
 		spte |= shadow_x_mask;
 	else
@@ -186,9 +790,27 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	if (pte_access & ACC_USER_MASK)
 		spte |= shadow_user_mask;
 
+	/*
+	 * 1是page table entry,
+	 * 否则指向的是page (或者huge page)
+	 */
 	if (level > PG_LEVEL_4K)
 		spte |= PT_PAGE_SIZE_MASK;
 
+	/*
+	 * 在以下设置shadow_memtype_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|439| <<kvm_mmu_set_ept_masks>> shadow_memtype_mask = VMX_EPT_MT_MASK | VMX_EPT_IPAT_BIT;
+	 *   - arch/x86/kvm/mmu/spte.c|496| <<kvm_mmu_reset_all_pte_masks>> shadow_memtype_mask = 0;
+	 * 在以下使用shadow_memtype_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|4625| <<__kvm_mmu_honors_guest_mtrrs>> return vm_has_noncoherent_dma && shadow_memtype_mask;
+	 *   - arch/x86/kvm/mmu/spte.c|192| <<make_spte>> if (shadow_memtype_mask)
+	 *
+	 * 对于EPT,
+	 * bit-3/4/5(VMX_EPT_MT_MASK): EPT memory type for this 4-KByte page (see Section 29.3.7).
+	 * bit-6(VMX_EPT_IPAT_BIT): Ignore PAT memory type for this 4-KByte page (see Section 29.3.7).
+	 *
+	 * vmx_get_mt_mask()
+	 */
 	if (shadow_memtype_mask)
 		spte |= static_call(kvm_x86_get_mt_mask)(vcpu, gfn,
 							 kvm_is_mmio_pfn(pfn));
@@ -203,6 +825,9 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	spte |= (u64)pfn << PAGE_SHIFT;
 
 	if (pte_access & ACC_WRITE_MASK) {
+		/*
+		 * 标记mmu和entry本身writable
+		 */
 		spte |= PT_WRITABLE_MASK | shadow_mmu_writable_mask;
 
 		/*
@@ -231,6 +856,9 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 		spte |= spte_shadow_dirty_mask(spte);
 
 out:
+	/*
+	 * regular pte一定支持accessed, ept不一定
+	 */
 	if (prefetch)
 		spte = mark_spte_for_access_track(spte);
 
@@ -238,7 +866,13 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 		  "spte = 0x%llx, level = %d, rsvd bits = 0x%llx", spte, level,
 		  get_rsvd_bits(&vcpu->arch.mmu->shadow_zero_check, spte, level));
 
+	/*
+	 * 如果正在
+	 */
 	if ((spte & PT_WRITABLE_MASK) && kvm_slot_dirty_track_enabled(slot)) {
+		/*
+		 * 如果正在track就不能是大于4K了
+		 */
 		/* Enforced by kvm_mmu_hugepage_adjust. */
 		WARN_ON_ONCE(level > PG_LEVEL_4K);
 		mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
@@ -248,6 +882,10 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	return wrprot;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|758| <<make_huge_page_split_spte>> child_spte = make_spte_executable(child_spte);
+ */
 static u64 make_spte_executable(u64 spte)
 {
 	bool is_access_track = is_access_track_spte(spte);
@@ -255,6 +893,19 @@ static u64 make_spte_executable(u64 spte)
 	if (is_access_track)
 		spte = restore_acc_track_spte(spte);
 
+	/*
+	 * 在以下设置shadow_nx_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|590| <<kvm_mmu_set_ept_masks>> shadow_nx_mask = 0ull;
+	 *   - arch/x86/kvm/mmu/spte.c|658| <<kvm_mmu_reset_all_pte_masks>> shadow_nx_mask = PT64_NX_MASK;
+	 *
+	 * 普通page table entry的bit=63 (XD):
+	 * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+	 * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+	 * otherwise, reserved (must be 0)
+	 *
+	 * 普通pte支持nx bit, ept不支持nx bit
+	 * 普通pte不支持x bit, ept支持x bit
+	 */
 	spte &= ~shadow_nx_mask;
 	spte |= shadow_x_mask;
 
@@ -271,6 +922,11 @@ static u64 make_spte_executable(u64 spte)
  * This is used during huge page splitting to build the SPTEs that make up the
  * new page table.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6707| <<shadow_mmu_split_huge_page>> spte = make_huge_page_split_spte(kvm, huge_spte, sp->role, index);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1498| <<tdp_mmu_split_huge_page>> sp->spt[i] = make_huge_page_split_spte(kvm, huge_spte, sp->role, i);
+ */
 u64 make_huge_page_split_spte(struct kvm *kvm, u64 huge_spte, union kvm_mmu_page_role role,
 			      int index)
 {
@@ -284,6 +940,13 @@ u64 make_huge_page_split_spte(struct kvm *kvm, u64 huge_spte, union kvm_mmu_page
 
 	child_spte = huge_spte;
 
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+	 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+	 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+	 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+	 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+	 */
 	/*
 	 * The child_spte already has the base address of the huge page being
 	 * split. So we just have to OR in the offset to the page at the next
@@ -307,13 +970,28 @@ u64 make_huge_page_split_spte(struct kvm *kvm, u64 huge_spte, union kvm_mmu_page
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2473| <<__link_shadow_page>> spte = make_nonleaf_spte(sp->spt, sp_ad_disabled(sp));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1115| <<tdp_mmu_link_sp>> u64 spte = make_nonleaf_spte(sp->spt, !kvm_ad_enabled());
+ */
 u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 {
+	/*
+	 * bit 11
+	 */
 	u64 spte = SPTE_MMU_PRESENT_MASK;
 
 	spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK |
 		shadow_user_mask | shadow_x_mask | shadow_me_value;
 
+	/*
+	 * EPT: PML4E的bit8:
+	 *  If bit 6 of EPTP is 1, accessed flag for EPT; indicates whether
+	 *  software has accessed the 512-GByte region
+	 *  controlled by this entry (see Section 29.3.5). Ignored if bit 6 of
+	 *  EPTP is 0.
+	 */
 	if (ad_disabled)
 		spte |= SPTE_TDP_AD_DISABLED;
 	else
@@ -322,13 +1000,27 @@ u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1527| <<kvm_set_pte_rmap>> new_spte = kvm_mmu_changed_pte_notifier_make_spte(*sptep, new_pfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1338| <<set_spte_gfn>> new_spte = kvm_mmu_changed_pte_notifier_make_spte(iter->old_spte, pte_pfn(range->arg.pte));
+ */
 u64 kvm_mmu_changed_pte_notifier_make_spte(u64 old_spte, kvm_pfn_t new_pfn)
 {
 	u64 new_spte;
 
+	/*
+	 * 在没有CONFIG_DYNAMIC_PHYSICAL_MASK的时候是0xffffffffff000 (13个hex)
+	 * ~SPTE_BASE_ADDR_MASK
+	 */
 	new_spte = old_spte & ~SPTE_BASE_ADDR_MASK;
 	new_spte |= (u64)new_pfn << PAGE_SHIFT;
 
+	/*
+	 * pte本身不writable
+	 * host不writable
+	 * mmu不writable
+	 */
 	new_spte &= ~PT_WRITABLE_MASK;
 	new_spte &= ~shadow_host_writable_mask;
 	new_spte &= ~shadow_mmu_writable_mask;
@@ -338,27 +1030,101 @@ u64 kvm_mmu_changed_pte_notifier_make_spte(u64 old_spte, kvm_pfn_t new_pfn)
 	return new_spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|633| <<mmu_spte_age>> spte = mark_spte_for_access_track(spte);
+ *   - arch/x86/kvm/mmu/spte.c|326| <<make_spte>> spte = mark_spte_for_access_track(spte);
+ *   - arch/x86/kvm/mmu/spte.c|353| <<make_spte_executable>> spte = mark_spte_for_access_track(spte);
+ *   - arch/x86/kvm/mmu/spte.c|427| <<kvm_mmu_changed_pte_notifier_make_spte>> new_spte = mark_spte_for_access_track(new_spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1290| <<age_gfn_range>> new_spte = mark_spte_for_access_track(iter->old_spte);
+ *
+ * regular pte一定支持accessed, ept不一定
+ */
 u64 mark_spte_for_access_track(u64 spte)
 {
+	/*
+	 * 在以下设置shadow_accessed_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|464| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+	 *   - arch/x86/kvm/mmu/spte.c|528| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+	 *
+	 * 清空accessed的bit, 用于以后track
+	 */
 	if (spte_ad_enabled(spte))
 		return spte & ~shadow_accessed_mask;
 
+	/*
+	 * 下面的就是不支持硬件ad的情况了
+	 */
+
+	/*
+	 * 比如: ept硬件不支持access bit, 并且ept的0/1/2都没设置
+	 */
 	if (is_access_track_spte(spte))
 		return spte;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|500| <<mmu_spte_update_no_track>> check_spte_writable_invariants(new_spte);
+	 *   - arch/x86/kvm/mmu/spte.c|675| <<mark_spte_for_access_track>> check_spte_writable_invariants(spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|510| <<handle_changed_spte>> check_spte_writable_invariants(new_spte);
+	 *
+	 * 1. 如果mmu是writable, 但是host不writable, 错误!
+	 * 2. 如果mmu不writable, 但是pte本身writable, 错误! (可能mmio???)
+	 */
 	check_spte_writable_invariants(spte);
 
 	WARN_ONCE(spte & (SHADOW_ACC_TRACK_SAVED_BITS_MASK <<
 			  SHADOW_ACC_TRACK_SAVED_BITS_SHIFT),
 		  "Access Tracking saved bit locations are not zero\n");
 
+	/*
+	 * 把第一位和第四位保存到54位
+	 */
 	spte |= (spte & SHADOW_ACC_TRACK_SAVED_BITS_MASK) <<
 		SHADOW_ACC_TRACK_SAVED_BITS_SHIFT;
+	/*
+	 * 在以下设置shadow_acc_track_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|550| <<kvm_mmu_set_ept_masks>> shadow_acc_track_mask = VMX_EPT_RWX_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|611| <<kvm_mmu_reset_all_pte_masks>> shadow_acc_track_mask = 0;
+	 */
 	spte &= ~shadow_acc_track_mask;
 
 	return spte;
 }
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> __kvm_x86_vendor_init()
+ *       -> kvm_mmu_vendor_module_init()
+ *          -> kvm_mmu_reset_all_pte_masks()
+ *             -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *             -> kvm_mmu_set_mmio_spte_mask()
+ *    -> ops->hardware_setup = hardware_setup()
+ *       -> kvm_mmu_set_ept_masks()
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull; (1 << 8)
+ *          -> kvm_mmu_set_mmio_spte_mask()
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|491| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK, 0);
+ *   - arch/x86/kvm/mmu/spte.c|559| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+ *   - arch/x86/kvm/svm/svm.c|5074| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+ *
+ * 初始化那些mask和value
+ */
 void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 {
 	BUG_ON((u64)(unsigned)access_mask != access_mask);
@@ -411,6 +1177,13 @@ void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mmio_spte_mask);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5233| <<svm_hardware_setup>> kvm_mmu_set_me_spte_mask(sme_me_mask, sme_me_mask);
+ *   - arch/x86/kvm/vmx/vmx.c|8490| <<vmx_setup_me_spte_mask>> kvm_mmu_set_me_spte_mask(0, me_mask);
+ *
+ * 用在memory encryption
+ */
 void kvm_mmu_set_me_spte_mask(u64 me_value, u64 me_mask)
 {
 	/* shadow_me_value must be a subset of shadow_me_mask */
@@ -422,14 +1195,57 @@ void kvm_mmu_set_me_spte_mask(u64 me_value, u64 me_mask)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_me_spte_mask);
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> __kvm_x86_vendor_init()
+ *       -> kvm_mmu_vendor_module_init()
+ *          -> kvm_mmu_reset_all_pte_masks()
+ *             -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *             -> kvm_mmu_set_mmio_spte_mask()
+ *    -> ops->hardware_setup = hardware_setup()
+ *       -> kvm_mmu_set_ept_masks()
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull; (1 << 8)
+ *          -> kvm_mmu_set_mmio_spte_mask()
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8604| <<hardware_setup>> if (enable_ept) kvm_mmu_set_ept_masks(enable_ept_ad_bits, cpu_has_vmx_ept_execute_only());
+ *
+ * 只在vmx调用
+ */
 void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)
 {
 	shadow_user_mask	= VMX_EPT_READABLE_MASK;
+	/*
+	 * 在以下设置shadow_accessed_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|464| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+	 *   - arch/x86/kvm/mmu/spte.c|528| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+	 */
 	shadow_accessed_mask	= has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
 	shadow_dirty_mask	= has_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull;
 	shadow_nx_mask		= 0ull;
 	shadow_x_mask		= VMX_EPT_EXECUTABLE_MASK;
 	shadow_present_mask	= has_exec_only ? 0ull : VMX_EPT_READABLE_MASK;
+	/*
+	 * 在以下使用shadow_memtype_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|4625| <<__kvm_mmu_honors_guest_mtrrs>> return vm_has_noncoherent_dma && shadow_memtype_mask;
+	 *   - arch/x86/kvm/mmu/spte.c|192| <<make_spte>> if (shadow_memtype_mask)
+	 *   - arch/x86/kvm/mmu/spte.c|439| <<kvm_mmu_set_ept_masks>> shadow_memtype_mask = VMX_EPT_MT_MASK | VMX_EPT_IPAT_BIT;
+	 *   - arch/x86/kvm/mmu/spte.c|496| <<kvm_mmu_reset_all_pte_masks>> shadow_memtype_mask = 0;
+	 */
 	/*
 	 * EPT overrides the host MTRRs, and so KVM must program the desired
 	 * memtype directly into the SPTEs.  Note, this mask is just the mask
@@ -450,11 +1266,47 @@ void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_ept_masks);
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> __kvm_x86_vendor_init()
+ *       -> kvm_mmu_vendor_module_init()
+ *          -> kvm_mmu_reset_all_pte_masks()
+ *             -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *             -> kvm_mmu_set_mmio_spte_mask()
+ *    -> ops->hardware_setup = hardware_setup()
+ *       -> kvm_mmu_set_ept_masks()
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull; (1 << 8)
+ *          -> kvm_mmu_set_mmio_spte_mask()
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7113| <<kvm_mmu_vendor_module_init>> kvm_mmu_reset_all_pte_masks();
+ */
 void kvm_mmu_reset_all_pte_masks(void)
 {
 	u8 low_phys_bits;
 	u64 mask;
 
+	/*
+	 * 测试:
+	 * boot_cpu_data.x86_phys_bits = 46
+	 * boot_cpu_data.x86_cache_bits = 46
+	 * shadow_phys_bits=46
+	 * low_phys_bits=46
+	 */
 	shadow_phys_bits = kvm_get_shadow_phys_bits();
 
 	/*
@@ -468,16 +1320,37 @@ void kvm_mmu_reset_all_pte_masks(void)
 	 * the most significant bits of legal physical address space.
 	 */
 	shadow_nonpresent_or_rsvd_mask = 0;
+	/*
+	 * boot_cpu_data.x86_phys_bits的注释:
+	 * boot_cpu_data.x86_phys_bits is reduced when MKTME or SME are detected
+	 * in CPU detection code, but the processor treats those reduced bits as
+	 * 'keyID' thus they are not reserved bits. Therefore KVM needs to look at
+	 * the physical address bits reported by CPUID.
+	 */
 	low_phys_bits = boot_cpu_data.x86_phys_bits;
 	if (boot_cpu_has_bug(X86_BUG_L1TF) &&
 	    !WARN_ON_ONCE(boot_cpu_data.x86_cache_bits >=
 			  52 - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)) {
+		/*
+		 * 46 - 5 = 41
+		 */
 		low_phys_bits = boot_cpu_data.x86_cache_bits
 			- SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;
+		/*
+		 * bit: 41 - 45 (5个bit)
+		 * shadow_nonpresent_or_rsvd_mask=0x00003e0000000000
+		 */
 		shadow_nonpresent_or_rsvd_mask =
 			rsvd_bits(low_phys_bits, boot_cpu_data.x86_cache_bits - 1);
 	}
 
+	/*
+	 * bit: 12 - 40
+	 * shadow_nonpresent_or_rsvd_lower_gfn_mask=0x000001fffffff000
+	 *
+	 * high: low_phys_bits - 1
+	 * low : PAGE_SHIFT
+	 */
 	shadow_nonpresent_or_rsvd_lower_gfn_mask =
 		GENMASK_ULL(low_phys_bits - 1, PAGE_SHIFT);
 
@@ -513,5 +1386,8 @@ void kvm_mmu_reset_all_pte_masks(void)
 	else
 		mask = 0;
 
+	/*
+	 * 1 << 1 | 1 << 2
+	 */
 	kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
 }
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index a129951c9..8783c1390 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -6,6 +6,17 @@
 #include "mmu.h"
 #include "mmu_internal.h"
 
+/*
+ * 在以下使用SPTE_MMU_PRESENT_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|124| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+ *   - arch/x86/kvm/mmu/spte.h|139| <<global>> static_assert(!(SPTE_MMIO_ALLOWED_MASK & (SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+ *   - arch/x86/kvm/mmu/spte.h|200| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|151| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|328| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|260| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+ *
+ * mmio是不设置这个bit=11的
+ */
 /*
  * A MMU present SPTE is backed by actual memory and may or may not be present
  * in hardware.  E.g. MMIO SPTEs are not considered present.  Use bit 11, as it
@@ -15,6 +26,28 @@
  */
 #define SPTE_MMU_PRESENT_MASK		BIT_ULL(11)
 
+/*
+ * 在以下使用SPTE_TDP_AD_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|87| <<global>> static_assert(!(SPTE_TDP_AD_MASK & SHADOW_ACC_TRACK_SAVED_MASK));
+ *   - arch/x86/kvm/mmu/spte.h|106| <<global>> static_assert(!(EPT_SPTE_HOST_WRITABLE & SPTE_TDP_AD_MASK));
+ *   - arch/x86/kvm/mmu/spte.h|107| <<global>> static_assert(!(EPT_SPTE_MMU_WRITABLE & SPTE_TDP_AD_MASK));
+ *   - arch/x86/kvm/mmu/spte.h|316| <<spte_ad_enabled>> return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_DISABLED;
+ *   - arch/x86/kvm/mmu/spte.h|333| <<spte_ad_need_write_protect>> return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_ENABLED;
+ *
+ * 在以下使用SPTE_TDP_AD_ENABLED:
+ *   - arch/x86/kvm/mmu/spte.h|50| <<global>> static_assert(SPTE_TDP_AD_ENABLED == 0);
+ *   - arch/x86/kvm/mmu/spte.h|333| <<spte_ad_need_write_protect>> return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_ENABLED;
+ *
+ * 在以下使用SPTE_TDP_AD_DISABLED:
+ *   - arch/x86/kvm/mmu/spte.c|157| <<make_spte>> if (sp->role.ad_disabled) spte |= SPTE_TDP_AD_DISABLED;
+ *   - arch/x86/kvm/mmu/spte.c|334| <<make_nonleaf_spte>> if (ad_disabled) spte |= SPTE_TDP_AD_DISABLED;
+ *   - arch/x86/kvm/mmu/spte.h|310| <<spte_ad_enabled>> return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_DISABLED;
+ *
+ * 在以下使用SPTE_TDP_AD_WRPROT_ONLY:
+ *   - arch/x86/kvm/mmu/spte.c|159| <<make_spte>> spte |= SPTE_TDP_AD_WRPROT_ONLY;
+ *
+ * Page Modification Logging (PML)
+ */
 /*
  * TDP SPTES (more specifically, EPT SPTEs) may not have A/D bits, and may also
  * be restricted to using write-protection (for L2 when CPU dirty logging, i.e.
@@ -34,15 +67,43 @@
 #define SPTE_TDP_AD_WRPROT_ONLY		(2ULL << SPTE_TDP_AD_SHIFT)
 static_assert(SPTE_TDP_AD_ENABLED == 0);
 
+/*
+ * 在以下使用SPTE_BASE_ADDR_MASK:
+ *   - arch/x86/kvm/mmu/mmu.c|2450| <<shadow_walk_init_using_root>> iterator->shadow_addr &= SPTE_BASE_ADDR_MASK;
+ *   - arch/x86/kvm/mmu/mmu.c|2482| <<__shadow_walk_next>> iterator->shadow_addr = spte & SPTE_BASE_ADDR_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|892| <<kvm_mmu_changed_pte_notifier_make_spte>> new_spte = old_spte & ~SPTE_BASE_ADDR_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|294| <<spte_to_child_sp>> return to_shadow_page(spte & SPTE_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/spte.h|440| <<spte_to_pfn>> return (pte & SPTE_BASE_ADDR_MASK) >> PAGE_SHIFT;
+ */
 #ifdef CONFIG_DYNAMIC_PHYSICAL_MASK
 #define SPTE_BASE_ADDR_MASK (physical_mask & ~(u64)(PAGE_SIZE-1))
 #else
+/*
+ * ((1ULL << 52) - 1) = 0xfffffffffffff (13个f)
+ * ~(u64)(PAGE_SIZE-1) = 除了后12位都是0
+ * 在没有CONFIG_DYNAMIC_PHYSICAL_MASK的时候是0xffffffffff000 (13个hex)
+ */
 #define SPTE_BASE_ADDR_MASK (((1ULL << 52) - 1) & ~(u64)(PAGE_SIZE-1))
 #endif
 
 #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \
 			| shadow_x_mask | shadow_nx_mask | shadow_me_mask)
 
+/*
+ * 在以下使用ACC_ALL:
+ *   - arch/x86/kvm/mmu/mmu.c|773| <<kvm_mmu_page_get_access>> return sp->shadowed_translation[index] & ACC_ALL;
+ *   - arch/x86/kvm/mmu/mmu.c|3380| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|3393| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+ *   - arch/x86/kvm/mmu/mmu.c|4664| <<direct_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_tdp_mmu_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|5503| <<kvm_calc_cpu_role>> role.base.access = ACC_ALL;
+ *   - arch/x86/kvm/mmu/mmu.c|5572| <<kvm_calc_tdp_mmu_root_page_role>> role.access = ACC_ALL;
+ *   - arch/x86/kvm/mmu/mmu.c|5740| <<kvm_calc_shadow_ept_root_page_role>> role.base.access = ACC_ALL;
+ *   - arch/x86/kvm/mmu/mmutrace.h|225| <<__field>> __entry->access = spte & ACC_ALL;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1066| <<tdp_mmu_map_handle_target_level>> new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1068| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte, fault->prefetch, true,
+ *							fault->map_writable, &new_spte);
+ */
 #define ACC_EXEC_MASK    1
 #define ACC_WRITE_MASK   PT_WRITABLE_MASK
 #define ACC_USER_MASK    PT_USER_MASK
@@ -53,8 +114,27 @@ static_assert(SPTE_TDP_AD_ENABLED == 0);
 #define SPTE_EPT_EXECUTABLE_MASK		0x4ull
 
 #define SPTE_LEVEL_BITS			9
+/*
+ * 17 #define __PT_LEVEL_SHIFT(level, bits_per_level) \
+ * 18         (PAGE_SHIFT + ((level) - 1) * (bits_per_level))
+ *
+ * SPTE_LEVEL_SHIFT(5): 12 + 4 * 9 = 48
+ * SPTE_LEVEL_SHIFT(4): 12 + 3 * 9 = 39
+ * SPTE_LEVEL_SHIFT(3): 12 + 2 * 9 = 30
+ * SPTE_LEVEL_SHIFT(2): 12 + 1 * 9 = 21
+ * SPTE_LEVEL_SHIFT(1): 12 + 0 * 9 = 12
+ */
 #define SPTE_LEVEL_SHIFT(level)		__PT_LEVEL_SHIFT(level, SPTE_LEVEL_BITS)
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2499| <<shadow_walk_okay>> iterator->index = SPTE_INDEX(iterator->addr, iterator->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|21| <<tdp_iter_refresh_sptep>> iter->sptep = iter->pt_path[iter->level - 1] + SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|195| <<try_step_side>> if (SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level) == (SPTE_ENT_PER_PAGE - 1))
+ */
 #define SPTE_INDEX(address, level)	__PT_INDEX(address, level, SPTE_LEVEL_BITS)
+/*
+ * 512
+ */
 #define SPTE_ENT_PER_PAGE		__PT_ENT_PER_PAGE(SPTE_LEVEL_BITS)
 
 /*
@@ -64,9 +144,21 @@ static_assert(SPTE_TDP_AD_ENABLED == 0);
  * restored only when a write is attempted to the page.  This mask obviously
  * must not overlap the A/D type mask.
  */
+/*
+ * 在以下使用SHADOW_ACC_TRACK_SAVED_BITS_MASK:
+ *   - arch/x86/kvm/mmu/spte.c|697| <<mark_spte_for_access_track>> WARN_ONCE(spte & (SHADOW_ACC_TRACK_SAVED_BITS_MASK << SHADOW_ACC_TRACK_SAVED_BITS_SHIFT),
+ *                                                          "Access Tracking saved bit locations are not zero\n");
+ *   - arch/x86/kvm/mmu/spte.c|701| <<mark_spte_for_access_track>> spte |= (spte & SHADOW_ACC_TRACK_SAVED_BITS_MASK) << SHADOW_ACC_TRACK_SAVED_BITS_SHIFT;
+ *   - arch/x86/kvm/mmu/spte.h|99| <<SHADOW_ACC_TRACK_SAVED_MASK>> #define SHADOW_ACC_TRACK_SAVED_MASK (SHADOW_ACC_TRACK_SAVED_BITS_MASK << \ SHADOW_ACC_TRACK_SAVED_BITS_SHIFT)
+ *   - arch/x86/kvm/mmu/spte.h|597| <<restore_acc_track_spte>> u64 saved_bits = (spte >> SHADOW_ACC_TRACK_SAVED_BITS_SHIFT) & SHADOW_ACC_TRACK_SAVED_BITS_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|600| <<restore_acc_track_spte>> spte &= ~(SHADOW_ACC_TRACK_SAVED_BITS_MASK << SHADOW_ACC_TRACK_SAVED_BITS_SHIFT);
+ */
 #define SHADOW_ACC_TRACK_SAVED_BITS_MASK (SPTE_EPT_READABLE_MASK | \
 					  SPTE_EPT_EXECUTABLE_MASK)
 #define SHADOW_ACC_TRACK_SAVED_BITS_SHIFT 54
+/*
+ * 把bit0和bit2转移到bit54
+ */
 #define SHADOW_ACC_TRACK_SAVED_MASK	(SHADOW_ACC_TRACK_SAVED_BITS_MASK << \
 					 SHADOW_ACC_TRACK_SAVED_BITS_SHIFT)
 static_assert(!(SPTE_TDP_AD_MASK & SHADOW_ACC_TRACK_SAVED_MASK));
@@ -117,6 +209,13 @@ static_assert(!(EPT_SPTE_MMU_WRITABLE & SHADOW_ACC_TRACK_SAVED_MASK));
 #define MMIO_SPTE_GEN_HIGH_START	52
 #define MMIO_SPTE_GEN_HIGH_END		62
 
+/*
+ * 在以下使用
+ *   - arch/x86/kvm/mmu/spte.h|180| <<global>> (MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+ *   - arch/x86/kvm/mmu/spte.h|194| <<global>> (SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+ *   - arch/x86/kvm/mmu/spte.c|518| <<generation_mmio_spte_mask>> mask = (gen << MMIO_SPTE_GEN_LOW_SHIFT) & MMIO_SPTE_GEN_LOW_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|639| <<get_mmio_spte_generation>> gen = (spte & MMIO_SPTE_GEN_LOW_MASK) >> MMIO_SPTE_GEN_LOW_SHIFT;
+ */
 #define MMIO_SPTE_GEN_LOW_MASK		GENMASK_ULL(MMIO_SPTE_GEN_LOW_END, \
 						    MMIO_SPTE_GEN_LOW_START)
 #define MMIO_SPTE_GEN_HIGH_MASK		GENMASK_ULL(MMIO_SPTE_GEN_HIGH_END, \
@@ -134,10 +233,21 @@ static_assert(!(SPTE_MMU_PRESENT_MASK &
  * and so they're off-limits for generation; additional checks ensure the mask
  * doesn't overlap legal PA bits), and bit 63 (carved out for future usage).
  */
+/*
+ * 在以下使用SPTE_MMIO_ALLOWED_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|237| <<global>> static_assert(!(SPTE_MMIO_ALLOWED_MASK &
+ *   - arch/x86/kvm/mmu/spte.c|1106| <<kvm_mmu_set_mmio_spte_mask>> if (WARN_ON(mmio_mask & ~SPTE_MMIO_ALLOWED_MASK))
+ */
 #define SPTE_MMIO_ALLOWED_MASK (BIT_ULL(63) | GENMASK_ULL(51, 12) | GENMASK_ULL(2, 0))
 static_assert(!(SPTE_MMIO_ALLOWED_MASK &
 		(SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
 
+/*
+ * 在以下使用MMIO_SPTE_GEN_LOW_BITS:
+ *   - arch/x86/kvm/mmu/spte.h|244| <<global>> static_assert(MMIO_SPTE_GEN_LOW_BITS == 8 && MMIO_SPTE_GEN_HIGH_BITS == 11);
+ *   - arch/x86/kvm/mmu/spte.h|247| <<MMIO_SPTE_GEN_HIGH_SHIFT>> #define MMIO_SPTE_GEN_HIGH_SHIFT (MMIO_SPTE_GEN_HIGH_START - MMIO_SPTE_GEN_LOW_BITS)
+ *   - arch/x86/kvm/mmu/spte.h|249| <<MMIO_SPTE_GEN_MASK>> #define MMIO_SPTE_GEN_MASK GENMASK_ULL(MMIO_SPTE_GEN_LOW_BITS + MMIO_SPTE_GEN_HIGH_BITS - 1, 0)
+ */
 #define MMIO_SPTE_GEN_LOW_BITS		(MMIO_SPTE_GEN_LOW_END - MMIO_SPTE_GEN_LOW_START + 1)
 #define MMIO_SPTE_GEN_HIGH_BITS		(MMIO_SPTE_GEN_HIGH_END - MMIO_SPTE_GEN_HIGH_START + 1)
 
@@ -147,6 +257,13 @@ static_assert(MMIO_SPTE_GEN_LOW_BITS == 8 && MMIO_SPTE_GEN_HIGH_BITS == 11);
 #define MMIO_SPTE_GEN_LOW_SHIFT		(MMIO_SPTE_GEN_LOW_START - 0)
 #define MMIO_SPTE_GEN_HIGH_SHIFT	(MMIO_SPTE_GEN_HIGH_START - MMIO_SPTE_GEN_LOW_BITS)
 
+/*
+ * 在以下使用MMIO_SPTE_GEN_MASK:
+ *   - arch/x86/kvm/mmu/mmu.c|360| <<check_mmio_spte>> kvm_gen = gen & MMIO_SPTE_GEN_MASK;
+ *   - arch/x86/kvm/mmu/mmu.c|7216| <<kvm_mmu_invalidate_mmio_sptes>> gen &= MMIO_SPTE_GEN_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|533| <<generation_mmio_spte_mask>> WARN_ON_ONCE(gen & ~MMIO_SPTE_GEN_MASK);
+ *   - arch/x86/kvm/mmu/spte.c|547| <<make_mmio_spte>> u64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;
+ */
 #define MMIO_SPTE_GEN_MASK		GENMASK_ULL(MMIO_SPTE_GEN_LOW_BITS + MMIO_SPTE_GEN_HIGH_BITS - 1, 0)
 
 extern u64 __read_mostly shadow_host_writable_mask;
@@ -177,6 +294,15 @@ extern u64 __read_mostly shadow_acc_track_mask;
  */
 extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
 
+/*
+ * 在以下使用SHADOW_NONPRESENT_OR_RSVD_MASK_LEN:
+ *   - arch/x86/kvm/mmu/spte.h|226| <<global>> #define SHADOW_NONPRESENT_OR_RSVD_MASK_LEN 5
+ *   - arch/x86/kvm/mmu/mmu.c|311| <<get_mmio_spte_gfn>> gpa |= (spte >> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)
+ *   - arch/x86/kvm/mmu/spte.c|499| <<make_mmio_spte>> << SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;
+ *   - arch/x86/kvm/mmu/spte.c|1034| <<kvm_mmu_set_mmio_spte_mask>> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)))
+ *   - arch/x86/kvm/mmu/spte.c|1195| <<kvm_mmu_reset_all_pte_masks>> 52 - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)) {
+ *   - arch/x86/kvm/mmu/spte.c|1197| <<kvm_mmu_reset_all_pte_masks>> - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;
+ */
 /*
  * The number of high-order 1 bits to use in the mask above.
  */
@@ -194,16 +320,50 @@ extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
  *
  * Only used by the TDP MMU.
  */
+/*
+ * 在以下使用REMOVED_SPTE:
+ *   - arch/x86/kvm/mmu/spte.h|209| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/mmu.c|3471| <<fast_page_fault>> spte = REMOVED_SPTE;
+ *   - arch/x86/kvm/mmu/spte.c|418| <<kvm_mmu_set_mmio_spte_mask>> WARN_ON(mmio_value && (REMOVED_SPTE & mmio_mask) == mmio_value))
+ *   - arch/x86/kvm/mmu/spte.h|213| <<is_removed_spte>> return spte == REMOVED_SPTE;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|335| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|383| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, REMOVED_SPTE, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|386| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<tdp_mmu_zap_spte_atomic>> ret = tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE);
+ */
 #define REMOVED_SPTE	0x5a0ULL
 
 /* Removed SPTEs must not be misconstrued as shadow present PTEs. */
 static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|353| <<handle_removed_pt>> if (!is_removed_spte(old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|484| <<handle_changed_spte>> !is_removed_spte(new_spte)))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|546| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded || is_removed_spte(iter->old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|621| <<tdp_mmu_set_spte>> WARN_ON_ONCE(is_removed_spte(old_spte) || is_removed_spte(new_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1073| <<kvm_tdp_mmu_map>> if (is_removed_spte(iter.old_spte))
+ */
 static inline bool is_removed_spte(u64 spte)
 {
 	return spte == REMOVED_SPTE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|287| <<kvm_flush_remote_tlbs_sptep>> gfn_t gfn = kvm_mmu_page_get_gfn(sp, spte_index(sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|1155| <<rmap_remove>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|1737| <<__rmap_add>> kvm_mmu_page_set_translation(sp, spte_index(spte), gfn, access);
+ *   - arch/x86/kvm/mmu/mmu.c|1878| <<mark_unsync>> if (__test_and_set_bit(spte_index(spte), sp->unsync_child_bitmap))
+ *   - arch/x86/kvm/mmu/mmu.c|2439| <<kvm_mmu_child_role>> role.quadrant = spte_index(sptep) & 1;
+ *   - arch/x86/kvm/mmu/mmu.c|3088| <<mmu_set_spte>> kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
+ *   - arch/x86/kvm/mmu/mmu.c|3104| <<direct_pte_prefetch_many>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(start));
+ *   - arch/x86/kvm/mmu/mmu.c|3130| <<__direct_pte_prefetch>> i = spte_index(sptep) & ~(PTE_PREFETCH_NUM - 1);
+ *   - arch/x86/kvm/mmu/mmu.c|6863| <<shadow_mmu_get_sp_for_split>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|6864| <<shadow_mmu_get_sp_for_split>> access = kvm_mmu_page_get_access(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|6938| <<shadow_mmu_try_split_huge_page>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|720| <<FNAME>> i = spte_index(sptep) & ~(PTE_PREFETCH_NUM - 1);
+ */
 /* Get an SPTE's index into its parent's page table (and the spt array). */
 static inline int spte_index(u64 *sptep)
 {
@@ -220,6 +380,11 @@ static inline int spte_index(u64 *sptep)
  */
 extern u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.h|359| <<spte_to_child_sp>> return to_shadow_page(spte & SPTE_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/spte.h|364| <<sptep_to_sp>> return to_shadow_page(__pa(sptep));
+ */
 static inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)
 {
 	struct page *page = pfn_to_page((shadow_page) >> PAGE_SHIFT);
@@ -227,16 +392,65 @@ static inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)
 	return (struct kvm_mmu_page *)page_private(page);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1932| <<__mmu_unsync_walk>> child = spte_to_child_sp(ent);
+ *   - arch/x86/kvm/mmu/mmu.c|2586| <<validate_direct_spte>> child = spte_to_child_sp(*sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|2607| <<mmu_page_zap_pte>> child = spte_to_child_sp(pte);
+ *   - arch/x86/kvm/mmu/mmu.c|3055| <<mmu_set_spte>> child = spte_to_child_sp(pte);
+ *   - arch/x86/kvm/mmu/mmu.c|3339| <<disallowed_hugepage_adjust>> spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+ *   - arch/x86/kvm/mmu/mmu.c|4188| <<kvm_mmu_sync_roots>> sp = spte_to_child_sp(root);
+ *   - arch/x86/kvm/mmu/spte.h|376| <<root_to_sp>> return spte_to_child_sp(root);
+ */
 static inline struct kvm_mmu_page *spte_to_child_sp(u64 spte)
 {
 	return to_shadow_page(spte & SPTE_BASE_ADDR_MASK);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|286| <<kvm_flush_remote_tlbs_sptep>> struct kvm_mmu_page *sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|403| <<count_spte_clear>> struct kvm_mmu_page *sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|487| <<__get_spte_lockless>> struct kvm_mmu_page *sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|601| <<mmu_spte_clear_track_bits>> int level = sptep_to_sp(sptep)->role.level;
+ *   - arch/x86/kvm/mmu/mmu.c|1154| <<rmap_remove>> sp = sptep_to_sp(spte);
+ *   - arch/x86/kvm/mmu/mmu.c|1258| <<drop_large_spte>> sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|1736| <<__rmap_add>> sp = sptep_to_sp(spte); 
+ *   - arch/x86/kvm/mmu/mmu.c|1877| <<mark_unsync>> sp = sptep_to_sp(spte);
+ *   - arch/x86/kvm/mmu/mmu.c|2234| <<clear_sp_write_flooding_count>> __clear_sp_write_flooding_count(sptep_to_sp(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|2402| <<kvm_mmu_child_role>> struct kvm_mmu_page *parent_sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|3024| <<mmu_set_spte>> struct kvm_mmu_page *sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|3151| <<direct_pte_prefetch>> sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|3610| <<fast_page_fault>> sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|6284| <<__kvm_mmu_invalidate_addr>> struct kvm_mmu_page *sp = sptep_to_sp(iterator.sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|6857| <<shadow_mmu_get_sp_for_split>> struct kvm_mmu_page *huge_sp = sptep_to_sp(huge_sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|6932| <<shadow_mmu_try_split_huge_page>> struct kvm_mmu_page *huge_sp = sptep_to_sp(huge_sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|6978| <<shadow_mmu_try_split_huge_pages>> sp = sptep_to_sp(huge_sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|7093| <<kvm_mmu_zap_collapsible_spte>> sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|705| <<FNAME(pte_prefetch)>> sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|224| <<tdp_mmu_init_child_sp>> parent_sp = sptep_to_sp(rcu_dereference(iter->sptep));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|344| <<handle_removed_pt>> struct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(pt));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1057| <<tdp_mmu_map_handle_target_level>> struct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(iter->sptep));
+ */
 static inline struct kvm_mmu_page *sptep_to_sp(u64 *sptep)
 {
 	return to_shadow_page(__pa(sptep));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3708| <<mmu_free_root_page>> sp = root_to_sp(*root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|3756| <<kvm_mmu_free_roots>> } else if (root_to_sp(mmu->root.hpa)) {
+ *   - arch/x86/kvm/mmu/mmu.c|3795| <<kvm_mmu_free_guest_mode_roots>> sp = root_to_sp(root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4140| <<is_unsync_root>> sp = root_to_sp(root);
+ *   - arch/x86/kvm/mmu/mmu.c|4174| <<kvm_mmu_sync_roots>> sp = root_to_sp(root);
+ *   - arch/x86/kvm/mmu/mmu.c|4625| <<is_page_fault_stale>> struct kvm_mmu_page *sp = root_to_sp(vcpu->arch.mmu->root.hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4883| <<is_root_usable>> sp = root_to_sp(root->hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4960| <<fast_pgd_switch>> if (VALID_PAGE(mmu->root.hpa) && !root_to_sp(mmu->root.hpa))
+ *   - arch/x86/kvm/mmu/mmu.c|5007| <<kvm_mmu_new_pgd>> struct kvm_mmu_page *sp = root_to_sp(vcpu->arch.mmu->root.hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|6001| <<is_obsolete_root>> sp = root_to_sp(root_hpa);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|718| <<tdp_mmu_for_each_pte>> for_each_tdp_pte(_iter, root_to_sp(_mmu->root.hpa), _start, _end)
+ */
 static inline struct kvm_mmu_page *root_to_sp(hpa_t root)
 {
 	if (kvm_mmu_is_dummy_root(root))
@@ -249,6 +463,15 @@ static inline struct kvm_mmu_page *root_to_sp(hpa_t root)
 	return spte_to_child_sp(root);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2545| <<mmu_page_zap_pte>> } else if (is_mmio_spte(pte)) {
+ *   - arch/x86/kvm/mmu/mmu.c|4230| <<handle_mmio_page_fault>> if (is_mmio_spte(spte)) {
+ *   - arch/x86/kvm/mmu/mmu.c|4892| <<sync_mmio_spte>> if (unlikely(is_mmio_spte(*sptep))) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|524| <<handle_changed_spte>> if (WARN_ON_ONCE(!is_mmio_spte(old_spte) &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|525| <<handle_changed_spte>> !is_mmio_spte(new_spte) &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1087| <<tdp_mmu_map_handle_target_level>> if (unlikely(is_mmio_spte(new_spte))) {
+ */
 static inline bool is_mmio_spte(u64 spte)
 {
 	return (spte & shadow_mmio_mask) == shadow_mmio_value &&
@@ -257,6 +480,21 @@ static inline bool is_mmio_spte(u64 spte)
 
 static inline bool is_shadow_present_pte(u64 pte)
 {
+	/*
+	 * 在以下使用SPTE_MMU_PRESENT_MASK:
+	 *   - arch/x86/kvm/mmu/spte.h|124| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+	 *   - arch/x86/kvm/mmu/spte.h|139| <<global>> static_assert(!(SPTE_MMIO_ALLOWED_MASK & (SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+	 *   - arch/x86/kvm/mmu/spte.h|200| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/spte.c|151| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|328| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.h|260| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+	 *
+	 * A MMU present SPTE is backed by actual memory and may or may not be present
+	 * in hardware.  E.g. MMIO SPTEs are not considered present.  Use bit 11, as it
+	 * is ignored by all flavors of SPTEs and checking a low bit often generates
+	 * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
+	 * enough that the improved code generation is noticeable in KVM's footprint.
+	 */
 	return !!(pte & SPTE_MMU_PRESENT_MASK);
 }
 
@@ -268,6 +506,23 @@ static inline bool is_shadow_present_pte(u64 pte)
  */
 static inline bool kvm_ad_enabled(void)
 {
+	/*
+	 * 对于普通的页表:
+	 * bit-5: Accessed: indicates whether software has accessed the 4-KByte page
+	 *        referenced by this entry (see Section 4.8)
+	 * bit-6: Dirty: indicates whether software has written to the 4-KByte page
+	 *        referenced by this entry (see Section 4.8)
+	 *
+	 * 对于EPT:
+	 * - bit-8: If bit 6 (ignore guest PAT) of EPTP is 1, accessed flag for EPT;
+	 *   indicates whether software has accessed the 4-KByte page referenced by
+	 *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+	 * - bit-9: If bit 6 (ignore guest PAT) of EPTP is 1, dirty flag for EPT;
+	 *   indicates whether software has written to the 4-KByte page referenced by
+	 *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+	 *
+	 * 哪些bit用来记录pte被访问过
+	 */
 	return !!shadow_accessed_mask;
 }
 
@@ -282,9 +537,22 @@ static inline bool spte_ad_enabled(u64 spte)
 	return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_DISABLED;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1305| <<__rmap_clear_dirty>> if (spte_ad_need_write_protect(*sptep))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1637| <<clear_dirty_gfn_range>> KVM_MMU_WARN_ON(kvm_ad_enabled() && spte_ad_need_write_protect(iter.old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1697| <<clear_dirty_pt_masked>> KVM_MMU_WARN_ON(kvm_ad_enabled() && spte_ad_need_write_protect(iter.old_spte));
+ */
 static inline bool spte_ad_need_write_protect(u64 spte)
 {
 	KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
+	/*
+	 * #define SPTE_TDP_AD_SHIFT               52
+	 * #define SPTE_TDP_AD_MASK                (3ULL << SPTE_TDP_AD_SHIFT)
+	 * #define SPTE_TDP_AD_ENABLED             (0ULL << SPTE_TDP_AD_SHIFT)
+	 * #define SPTE_TDP_AD_DISABLED            (1ULL << SPTE_TDP_AD_SHIFT)
+	 * #define SPTE_TDP_AD_WRPROT_ONLY         (2ULL << SPTE_TDP_AD_SHIFT)
+	 */
 	/*
 	 * This is benign for non-TDP SPTEs as SPTE_TDP_AD_ENABLED is '0',
 	 * and non-TDP SPTEs will never set these bits.  Optimize for 64-bit
@@ -307,6 +575,9 @@ static inline u64 spte_shadow_dirty_mask(u64 spte)
 
 static inline bool is_access_track_spte(u64 spte)
 {
+	/*
+	 * 比如: ept硬件不支持access bit, 并且ept的0/1/2都没设置
+	 */
 	return !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;
 }
 
@@ -320,8 +591,28 @@ static inline bool is_last_spte(u64 pte, int level)
 	return (level == PG_LEVEL_4K) || is_large_pte(pte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3543| <<is_access_allowed>> return is_executable_pte(spte);
+ *   - arch/x86/kvm/mmu/mmutrace.h|355| <<__field>> __entry->x = is_executable_pte(__entry->spte);
+ */
 static inline bool is_executable_pte(u64 spte)
 {
+	/*
+	 * 在以下设置shadow_nx_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|590| <<kvm_mmu_set_ept_masks>> shadow_nx_mask = 0ull;
+	 *   - arch/x86/kvm/mmu/spte.c|658| <<kvm_mmu_reset_all_pte_masks>> shadow_nx_mask = PT64_NX_MASK;
+	 * 普通page table entry的bit=63 (XD):
+	 * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+	 * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+	 * otherwise, reserved (must be 0)
+	 *
+	 *
+	 * 在以下设置shadow_x_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|5086| <<boot_cpu_is_amd>> return shadow_x_mask == 0;
+	 *   - arch/x86/kvm/mmu/spte.c|612| <<kvm_mmu_set_ept_masks>> shadow_x_mask = VMX_EPT_EXECUTABLE_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|680| <<kvm_mmu_reset_all_pte_masks>> shadow_x_mask = 0;
+	 */
 	return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
 }
 
@@ -345,26 +636,53 @@ static inline bool is_dirty_spte(u64 spte)
 	return dirty_mask ? spte & dirty_mask : spte & PT_WRITABLE_MASK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4264| <<get_mmio_spte>> pr_err("------ spte = 0x%llx level = %d, rsvd bits = 0x%llx", sptes[level], level, get_rsvd_bits(rsvd_check, sptes[level], level));
+ *   - arch/x86/kvm/mmu/spte.c|704| <<make_spte>> WARN_ONCE(is_rsvd_spte(&vcpu->arch.mmu->shadow_zero_check, spte, level), "spte = 0x%llx, level = %d, rsvd bits = 0x%llx",
+ *                                                   spte, level, get_rsvd_bits(&vcpu->arch.mmu->shadow_zero_check, spte, level));
+ *   - arch/x86/kvm/mmu/spte.h|469| <<__is_rsvd_bits_set>> return pte & get_rsvd_bits(rsvd_check, pte, level);
+ */
 static inline u64 get_rsvd_bits(struct rsvd_bits_validate *rsvd_check, u64 pte,
 				int level)
 {
+	/*
+	 * 至少ept的bit=7是ignored
+	 * 但是ept的hugepage的bit=7是1
+	 */
 	int bit7 = (pte >> 7) & 1;
 
 	return rsvd_check->rsvd_bits_mask[bit7][level-1];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|147| <<FNAME>> return __is_rsvd_bits_set(&mmu->guest_rsvd_check, gpte, level) ||
+ *   - arch/x86/kvm/mmu/spte.h|645| <<is_rsvd_spte>> __is_rsvd_bits_set(rsvd_check, spte, level);
+ */
 static inline bool __is_rsvd_bits_set(struct rsvd_bits_validate *rsvd_check,
 				      u64 pte, int level)
 {
 	return pte & get_rsvd_bits(rsvd_check, pte, level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|141| <<FNAME(is_bad_mt_xwr)>> return __is_bad_mt_xwr(rsvd_check, gpte);
+ *   - arch/x86/kvm/mmu/spte.h|678| <<is_rsvd_spte>> return __is_bad_mt_xwr(rsvd_check, spte) ||
+ */
 static inline bool __is_bad_mt_xwr(struct rsvd_bits_validate *rsvd_check,
 				   u64 pte)
 {
 	return rsvd_check->bad_mt_xwr & BIT_ULL(pte & 0x3f);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4298| <<get_mmio_spte>> reserved |= is_rsvd_spte(rsvd_check, sptes[level], level);
+ *   - arch/x86/kvm/mmu/spte.c|824| <<make_spte>> WARN_ONCE(is_rsvd_spte(&vcpu->arch.mmu->shadow_zero_check, spte, level), "spte = 0x%llx, level = %d, rsvd bits = 0x%llx", spte, level,
+ *						get_rsvd_bits(&vcpu->arch.mmu->shadow_zero_check, spte, level));
+ */
 static __always_inline bool is_rsvd_spte(struct rsvd_bits_validate *rsvd_check,
 					 u64 spte, int level)
 {
@@ -438,14 +756,37 @@ static __always_inline bool is_rsvd_spte(struct rsvd_bits_validate *rsvd_check,
  * cleared when an SPTE is first faulted in from non-present and then remains
  * immutable.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|540| <<mmu_spte_update>> !is_writable_pte(new_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|636| <<mmu_spte_age>> if (is_writable_pte(spte))
+ *   - arch/x86/kvm/mmu/mmu.c|1254| <<spte_write_protect>> if (!is_writable_pte(spte) &&
+ *   - arch/x86/kvm/mmu/mmu.c|3444| <<fast_pf_fix_direct_spte>> if (is_writable_pte(new_spte) && !is_writable_pte(old_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|3456| <<is_access_allowed>> return is_writable_pte(spte);
+ *   - arch/x86/kvm/mmu/spte.c|409| <<spte_has_volatile_bits>> if (!is_writable_pte(spte) && is_mmu_writable_spte(spte))
+ *   - arch/x86/kvm/mmu/spte.c|417| <<spte_has_volatile_bits>> (is_writable_pte(spte) && !(spte & shadow_dirty_mask)))
+ *   - arch/x86/kvm/mmu/spte.c|522| <<make_spte>> if (is_writable_pte(old_spte))
+ *   - arch/x86/kvm/mmu/spte.h|532| <<check_spte_writable_invariants>> WARN_ONCE(is_writable_pte(spte),
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1287| <<age_gfn_range>> if (is_writable_pte(iter->old_spte))
+ */
 static inline bool is_writable_pte(unsigned long pte)
 {
 	return pte & PT_WRITABLE_MASK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|500| <<mmu_spte_update_no_track>> check_spte_writable_invariants(new_spte);
+ *   - arch/x86/kvm/mmu/spte.c|675| <<mark_spte_for_access_track>> check_spte_writable_invariants(spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|510| <<handle_changed_spte>> check_spte_writable_invariants(new_spte);
+ */
 /* Note: spte must be a shadow-present leaf SPTE. */
 static inline void check_spte_writable_invariants(u64 spte)
 {
+	/*
+	 * 1. 如果mmu是writable, 但是host不writable, 错误!
+	 * 2. 如果mmu不writable, 但是pte本身writable, 错误! (可能mmio???)
+	 */
 	if (spte & shadow_mmu_writable_mask)
 		WARN_ONCE(!(spte & shadow_host_writable_mask),
 			  KBUILD_MODNAME ": MMU-writable SPTE is not Host-writable: %llx",
@@ -455,11 +796,32 @@ static inline void check_spte_writable_invariants(u64 spte)
 			  KBUILD_MODNAME ": Writable SPTE is not MMU-writable: %llx", spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|539| <<mmu_spte_update>> if (is_mmu_writable_spte(old_spte) &&
+ *   - arch/x86/kvm/mmu/mmu.c|1255| <<spte_write_protect>> !(pt_protect && is_mmu_writable_spte(spte)))
+ *   - arch/x86/kvm/mmu/mmu.c|3561| <<fast_page_fault>> if (fault->write && is_mmu_writable_spte(spte)) {
+ *   - arch/x86/kvm/mmu/spte.c|397| <<spte_has_volatile_bits>> if (!is_writable_pte(spte) && is_mmu_writable_spte(spte))
+ */
 static inline bool is_mmu_writable_spte(u64 spte)
 {
+	/*
+	 * 在以下设置shadow_mmu_writable_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|609| <<kvm_mmu_set_ept_masks>> shadow_mmu_writable_mask = EPT_SPTE_MMU_WRITABLE;
+	 *   - arch/x86/kvm/mmu/spte.c|673| <<kvm_mmu_reset_all_pte_masks>> shadow_mmu_writable_mask = DEFAULT_SPTE_MMU_WRITABLE;
+	 *
+	 * 用来标记虚拟化的页表里的entry是否指示writable
+	 */
 	return spte & shadow_mmu_writable_mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|361| <<check_mmio_spte>> spte_gen = get_mmio_spte_generation(spte);
+ *   - arch/x86/kvm/mmu/mmutrace.h|226| <<__field>> __entry->gen = get_mmio_spte_generation(spte);
+ *
+ * 把gen从spte取出来
+ */
 static inline u64 get_mmio_spte_generation(u64 spte)
 {
 	u64 gen;
@@ -482,6 +844,11 @@ u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled);
 u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access);
 u64 mark_spte_for_access_track(u64 spte);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3604| <<fast_page_fault>> new_spte = restore_acc_track_spte(new_spte);
+ *   - arch/x86/kvm/mmu/spte.c|770| <<make_spte_executable>> spte = restore_acc_track_spte(spte);
+ */
 /* Restore an acc-track PTE back to a regular PTE */
 static inline u64 restore_acc_track_spte(u64 spte)
 {
diff --git a/arch/x86/kvm/mmu/tdp_iter.c b/arch/x86/kvm/mmu/tdp_iter.c
index 04c247bfe..2a78d2aae 100644
--- a/arch/x86/kvm/mmu/tdp_iter.c
+++ b/arch/x86/kvm/mmu/tdp_iter.c
@@ -9,6 +9,12 @@
  * Recalculates the pointer to the SPTE for the current GFN and level and
  * reread the SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|98| <<try_step_down>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|140| <<try_step_up>> tdp_iter_refresh_sptep(iter);
+ */
 static void tdp_iter_refresh_sptep(struct tdp_iter *iter)
 {
 	iter->sptep = iter->pt_path[iter->level - 1] +
@@ -20,13 +26,42 @@ static void tdp_iter_refresh_sptep(struct tdp_iter *iter)
  * Return the TDP iterator to the root PT and allow it to continue its
  * traversal over the paging structure from there.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|54| <<tdp_iter_start>> tdp_iter_restart(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> tdp_iter_restart(iter);
+ */
 void tdp_iter_restart(struct tdp_iter *iter)
 {
 	iter->yielded = false;
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|70| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|159| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 * 在以下使用tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|37| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|47| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|131| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|725| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|741| <<tdp_mmu_iter_cond_resched>> WARN_ON_ONCE(iter->gfn > iter->next_last_level_gfn);
+	 */
 	iter->yielded_gfn = iter->next_last_level_gfn;
 	iter->level = iter->root_level;
 
+	/*
+	 * level 5: 向下round到每68719476736个 (256T)
+	 * level 4: 向下round到每134217728个 (512G)
+	 * level 3: 向下round到每262144个(1G)
+	 * level 2: 向下round到每512个(2M)
+	 * level 1: 向下round到每1个, 就是没round
+	 */
 	iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> tdp_iter_refresh_sptep(iter);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|98| <<try_step_down>> tdp_iter_refresh_sptep(iter);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|140| <<try_step_up>> tdp_iter_refresh_sptep(iter);
+	 */
 	tdp_iter_refresh_sptep(iter);
 
 	iter->valid = true;
@@ -36,6 +71,12 @@ void tdp_iter_restart(struct tdp_iter *iter)
  * Sets a TDP iterator to walk a pre-order traversal of the paging structure
  * rooted at root_pt, starting with the walk to translate next_last_level_gfn.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|156| <<for_each_tdp_pte_min_level>> for (tdp_iter_start(&iter, root, min_level, start); \
+ *
+ * 在for循环只调用一次
+ */
 void tdp_iter_start(struct tdp_iter *iter, struct kvm_mmu_page *root,
 		    int min_level, gfn_t next_last_level_gfn)
 {
@@ -48,9 +89,17 @@ void tdp_iter_start(struct tdp_iter *iter, struct kvm_mmu_page *root,
 	iter->next_last_level_gfn = next_last_level_gfn;
 	iter->root_level = root->role.level;
 	iter->min_level = min_level;
+	/*
+	 * 4, 3, 2, 1, 0
+	 */
 	iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root->spt;
 	iter->as_id = kvm_mmu_page_as_id(root);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|54| <<tdp_iter_start>> tdp_iter_restart(iter);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> tdp_iter_restart(iter);
+	 */
 	tdp_iter_restart(iter);
 }
 
@@ -59,6 +108,11 @@ void tdp_iter_start(struct tdp_iter *iter, struct kvm_mmu_page *root,
  * address of the child page table referenced by the SPTE. Returns null if
  * there is no such entry.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|91| <<try_step_down>> child_pt = spte_to_child_pt(iter->old_spte, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|493| <<handle_changed_spte>> handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
+ */
 tdp_ptep_t spte_to_child_pt(u64 spte, int level)
 {
 	/*
@@ -75,6 +129,10 @@ tdp_ptep_t spte_to_child_pt(u64 spte, int level)
  * Steps down one level in the paging structure towards the goal GFN. Returns
  * true if the iterator was able to step down a level, false otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|187| <<tdp_iter_next>> if (try_step_down(iter))
+ */
 static bool try_step_down(struct tdp_iter *iter)
 {
 	tdp_ptep_t child_pt;
@@ -94,6 +152,23 @@ static bool try_step_down(struct tdp_iter *iter)
 
 	iter->level--;
 	iter->pt_path[iter->level - 1] = child_pt;
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|70| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|159| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 * 在以下使用tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|37| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|47| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|131| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|725| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|741| <<tdp_mmu_iter_cond_resched>> WARN_ON_ONCE(iter->gfn > iter->next_last_level_gfn)
+	 *
+	 * level 5: 向下round到每68719476736个 (256T)
+	 * level 4: 向下round到每134217728个 (512G)
+	 * level 3: 向下round到每262144个(1G)
+	 * level 2: 向下round到每512个(2M)
+	 * level 1: 向下round到每1个, 就是没round
+	 */
 	iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
 	tdp_iter_refresh_sptep(iter);
 
@@ -107,6 +182,10 @@ static bool try_step_down(struct tdp_iter *iter)
  * able to step to the next entry in the page table, false if the iterator was
  * already at the end of the current page table.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|191| <<tdp_iter_next>> if (try_step_side(iter))
+ */
 static bool try_step_side(struct tdp_iter *iter)
 {
 	/*
@@ -130,13 +209,30 @@ static bool try_step_side(struct tdp_iter *iter)
  * can continue from the next entry in the parent page table. Returns true on a
  * successful step up, false if already in the root page.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|193| <<tdp_iter_next>> } while (try_step_up(iter));
+ */
 static bool try_step_up(struct tdp_iter *iter)
 {
 	if (iter->level == iter->root_level)
 		return false;
 
 	iter->level++;
+	/*
+	 * level 5: 向下round到每68719476736个 (256T)
+	 * level 4: 向下round到每134217728个 (512G)
+	 * level 3: 向下round到每262144个(1G)
+	 * level 2: 向下round到每512个(2M)
+	 * level 1: 向下round到每1个, 就是没round
+	 */
 	iter->gfn = gfn_round_for_level(iter->gfn, iter->level);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> tdp_iter_refresh_sptep(iter);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|98| <<try_step_down>> tdp_iter_refresh_sptep(iter);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|140| <<try_step_up>> tdp_iter_refresh_sptep(iter);
+	 */
 	tdp_iter_refresh_sptep(iter);
 
 	return true;
@@ -158,6 +254,16 @@ static bool try_step_up(struct tdp_iter *iter)
  *    SPTE will have already been visited, and so the iterator must also step
  *    to the side again.
  */
+/*
+ * 在以下调用tdp_iter_next():
+ * 155 #define for_each_tdp_pte_min_level(iter, root, min_level, start, end) \
+ * 156         for (tdp_iter_start(&iter, root, min_level, start); \
+ * 157              iter.valid && iter.gfn < end;                   \
+ * 158              tdp_iter_next(&iter))
+ *
+ * Preorder traversal is an a kind of tree traversal in which the root node is
+ * visited first, then the left subtree, and finally the right subtree.
+ */
 void tdp_iter_next(struct tdp_iter *iter)
 {
 	if (iter->yielded) {
diff --git a/arch/x86/kvm/mmu/tdp_iter.h b/arch/x86/kvm/mmu/tdp_iter.h
index fae559559..8d751ff68 100644
--- a/arch/x86/kvm/mmu/tdp_iter.h
+++ b/arch/x86/kvm/mmu/tdp_iter.h
@@ -8,22 +8,67 @@
 #include "mmu.h"
 #include "spte.h"
 
+/*
+ * x86 kvm使用rcu的一些例子:
+ *   - arch/x86/kvm/mmu/mmu.c|7655| <<kvm_recover_nx_huge_pages>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/mmu.c|7720| <<kvm_recover_nx_huge_pages>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|124| <<tdp_mmu_next_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|764| <<tdp_mmu_iter_cond_resched>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|835| <<tdp_mmu_zap_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|895| <<tdp_mmu_zap_leafs>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1159| <<kvm_tdp_mmu_map>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1257| <<kvm_tdp_mmu_handle_gfn>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1388| <<wrprot_gfn_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1489| <<tdp_mmu_alloc_sp_for_split>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1542| <<tdp_mmu_split_huge_pages_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1638| <<clear_dirty_gfn_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1702| <<clear_dirty_pt_masked>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1761| <<zap_collapsible_spte_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1835| <<write_protect_gfn>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|67| <<kvm_tdp_mmu_walk_lockless_begin>> rcu_read_lock();
+ *   - arch/x86/kvm/x86.c|10080| <<kvm_sched_yield>> rcu_read_lock();
+ *   - arch/x86/kvm/xen.c|2087| <<kvm_xen_hcall_evtchn_send>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|2474| <<kvm_range_has_memory_attributes>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|4049| <<kvm_vcpu_yield_to>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|4273| <<vcpu_get_pid>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|6710| <<kvm_vm_worker_thread>> rcu_read_lock();
+ */
+
 /*
  * TDP MMU SPTEs are RCU protected to allow paging structures (non-leaf SPTEs)
  * to be zapped while holding mmu_lock for read, and to allow TLB flushes to be
  * batched without having to collect the list of zapped SPs.  Flows that can
  * remove SPs must service pending TLB flushes prior to dropping RCU protection.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|22| <<tdp_iter_refresh_sptep>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|147| <<try_step_down>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|202| <<try_step_side>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|390| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_read_spte(sptep);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|864| <<kvm_tdp_mmu_zap_sp>> old_spte = kvm_tdp_mmu_read_spte(sp->ptep);
+ */
 static inline u64 kvm_tdp_mmu_read_spte(tdp_ptep_t sptep)
 {
 	return READ_ONCE(*rcu_dereference(sptep));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|108| <<kvm_tdp_mmu_write_spte>> return kvm_tdp_mmu_write_spte_atomic(sptep, new_spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|401| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, REMOVED_SPTE);
+ */
 static inline u64 kvm_tdp_mmu_write_spte_atomic(tdp_ptep_t sptep, u64 new_spte)
 {
 	return xchg(rcu_dereference(sptep), new_spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|110| <<kvm_tdp_mmu_write_spte>> __kvm_tdp_mmu_write_spte(sptep, new_spte);
+ *   - arch/x86/kvm/mmu/tdp_iter.h|127| <<tdp_mmu_clear_spte_bits>> __kvm_tdp_mmu_write_spte(sptep, old_spte & ~mask);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|664| <<tdp_mmu_zap_spte_atomic>> __kvm_tdp_mmu_write_spte(iter->sptep, 0);
+ */
 static inline void __kvm_tdp_mmu_write_spte(tdp_ptep_t sptep, u64 new_spte)
 {
 	WRITE_ONCE(*rcu_dereference(sptep), new_spte);
@@ -41,13 +86,34 @@ static inline void __kvm_tdp_mmu_write_spte(tdp_ptep_t sptep, u64 new_spte)
  * logic needs to be reassessed if KVM were to use non-leaf Accessed
  * bits, e.g. to skip stepping down into child SPTEs when aging SPTEs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|54| <<kvm_tdp_mmu_write_spte>> if (kvm_tdp_mmu_spte_need_atomic_write(old_spte, level))
+ *   - arch/x86/kvm/mmu/tdp_iter.h|66| <<tdp_mmu_clear_spte_bits>> if (kvm_tdp_mmu_spte_need_atomic_write(old_spte, level)) {
+ *
+ * 上面的注释提到了non-leaf entry
+ */
 static inline bool kvm_tdp_mmu_spte_need_atomic_write(u64 old_spte, int level)
 {
+	/*
+	 * spte_has_volatile_bits()的注释:
+	 * Returns true if the SPTE has bits that may be set without holding mmu_lock.
+	 * The caller is responsible for checking if the SPTE is shadow-present, and
+	 * for determining whether or not the caller cares about non-leaf SPTEs.
+	 */
 	return is_shadow_present_pte(old_spte) &&
 	       is_last_spte(old_spte, level) &&
 	       spte_has_volatile_bits(old_spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|382| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, REMOVED_SPTE, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|606| <<tdp_mmu_set_spte>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, new_spte, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1185| <<age_gfn_range>> iter->old_spte = kvm_tdp_mmu_write_spte(iter->sptep, iter->old_spte, new_spte, iter->level);
+ *
+ * typedef u64 __rcu *tdp_ptep_t;
+ */
 static inline u64 kvm_tdp_mmu_write_spte(tdp_ptep_t sptep, u64 old_spte,
 					 u64 new_spte, int level)
 {
@@ -58,13 +124,48 @@ static inline u64 kvm_tdp_mmu_write_spte(tdp_ptep_t sptep, u64 old_spte,
 	return old_spte;
 }
 
+/*
+ * commit 89c313f20c1ed30e04cedae735994c902ee93ddb
+ * Author: Vipin Sharma <vipinsh@google.com>
+ * Date:   Tue Mar 21 15:00:12 2023 -0700
+ *
+ * KVM: x86/mmu: Atomically clear SPTE dirty state in the clear-dirty-log flow
+ *
+ * Optimize the clearing of dirty state in TDP MMU SPTEs by doing an
+ * atomic-AND (on SPTEs that have volatile bits) instead of the full XCHG
+ * that currently ends up being invoked (see kvm_tdp_mmu_write_spte()).
+ * Clearing _only_ the bit in question will allow KVM to skip the many
+ * irrelevant checks in __handle_changed_spte() by avoiding any collateral
+ * damage due to the XCHG writing all SPTE bits, e.g. the XCHG could race
+ * with fast_page_fault() setting the W-bit and the CPU setting the D-bit,
+ * and thus incorrectly drop the CPU's D-bit update.
+ *
+ * Link: https://lore.kernel.org/all/Y9hXmz%2FnDOr1hQal@google.com
+ * Signed-off-by: Vipin Sharma <vipinsh@google.com>
+ * Reviewed-by: David Matlack <dmatlack@google.com>
+ * [sean: split the switch to atomic-AND to a separate patch]
+ * Link: https://lore.kernel.org/r/20230321220021.2119033-5-seanjc@google.com
+ * Signed-off-by: Sean Christopherson <seanjc@google.com>
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1312| <<age_gfn_range>> iter->old_spte = tdp_mmu_clear_spte_bits(iter->sptep, iter->old_spte, shadow_accessed_mask, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1751| <<clear_dirty_pt_masked>> iter.old_spte = tdp_mmu_clear_spte_bits(iter.sptep, iter.old_spte, dbit, iter.level);
+ */
 static inline u64 tdp_mmu_clear_spte_bits(tdp_ptep_t sptep, u64 old_spte,
 					  u64 mask, int level)
 {
 	atomic64_t *sptep_atomic;
 
 	if (kvm_tdp_mmu_spte_need_atomic_write(old_spte, level)) {
+		/*
+		 * 注意: sptep要用rcu来reference的
+		 */
 		sptep_atomic = (atomic64_t *)rcu_dereference(sptep);
+		/*
+		 * 为啥跟么不用kvm_tdp_mmu_write_spte_atomic()写入呢???
+		 *
+		 * 注释: Atomically updates @v to (@v & @i) with full ordering.
+		 */
 		return (u64)atomic64_fetch_and(~mask, sptep_atomic);
 	}
 
@@ -80,39 +181,132 @@ struct tdp_iter {
 	 * The iterator will traverse the paging structure towards the mapping
 	 * for this GFN.
 	 */
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|70| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|159| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 * 在以下使用tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|37| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|47| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|131| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|725| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|741| <<tdp_mmu_iter_cond_resched>> WARN_ON_ONCE(iter->gfn > iter->next_last_level_gfn);
+	 *
+	 * tdp_iter经常用来遍历一个地址的区间
+	 * next_last_level_gfn被设置成区间的每一个addr (使用try_step_side()设置)
+	 */
 	gfn_t next_last_level_gfn;
 	/*
 	 * The next_last_level_gfn at the time when the thread last
 	 * yielded. Only yielding when the next_last_level_gfn !=
 	 * yielded_gfn helps ensure forward progress.
 	 */
+	/*
+	 * 在以下使用tdp_iter->yielded_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|48| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|776| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 */
 	gfn_t yielded_gfn;
+	/*
+	 * 在以下使用tdp_iter->pt_path[PT64_ROOT_MAX_LEVEL]:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|20| <<tdp_iter_refresh_sptep>> iter->sptep = iter->pt_path[iter->level - 1] + SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|95| <<tdp_iter_start>> iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root->spt;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|154| <<try_step_down>> iter->pt_path[iter->level - 1] = child_pt;
+	 *
+	 * 一个虚拟地址addr可以分成7份:
+	 * | reserved | level 5 | level 4 | level 3 | level 2 | level 1 | offset |
+	 */
 	/* Pointers to the page tables traversed to reach the current SPTE */
 	tdp_ptep_t pt_path[PT64_ROOT_MAX_LEVEL];
+	/*
+	 * 在以下使用tdp_iter->sptep:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|20| <<tdp_iter_refresh_sptep>> iter->sptep = iter->pt_path[iter->level - 1] + SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|201| <<try_step_side>> iter->sptep++;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|664| <<tdp_mmu_zap_spte_atomic>> __kvm_tdp_mmu_write_spte(iter->sptep, 0);
+	 */
 	/* A pointer to the current SPTE */
 	tdp_ptep_t sptep;
+	/*
+	 * 在以下使用tdp_iter->gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|58| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|172| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|199| <<try_step_side>> iter->gfn += KVM_PAGES_PER_HPAGE(iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|229| <<try_step_up>> iter->gfn = gfn_round_for_level(iter->gfn, iter->level);
+	 */
 	/* The lowest GFN mapped by the current SPTE */
 	gfn_t gfn;
+	/*
+	 * 在以下设置tdp_iter->root_level:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|90| <<tdp_iter_start>> iter->root_level = root->role.level;
+	 * 在以下使用tdp_iter->root_level:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_restart>> iter->level = iter->root_level;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|95| <<tdp_iter_start>> iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root->spt;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|218| <<try_step_up>> if (iter->level == iter->root_level)
+	 */
 	/* The level of the root page given to the iterator */
 	int root_level;
+	/*
+	 * 在以下设置tdp_iter->min_level:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|91| <<tdp_iter_start>> iter->min_level = min_level;
+	 * 在以下使用tdp_iter->min_level:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|140| <<try_step_down>> if (iter->level == iter->min_level)
+	 */
 	/* The lowest level the iterator should traverse to */
 	int min_level;
+	/*
+	 * 在以下设置tdp_iter->level:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_restart>> iter->level = iter->root_level;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|221| <<try_step_up>> iter->level++;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|153| <<try_step_down>> iter->level--;
+	 */
 	/* The iterator's current level within the paging structure */
 	int level;
+	/*
+	 * 在以下设置tdp_iter->as_id:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|96| <<tdp_iter_start>> iter->as_id = kvm_mmu_page_as_id(root);
+	 * 在以下使用tdp_iter->as_id:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|634| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte, new_spte, iter->level, true);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|712| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1347| <<age_gfn_range>> trace_kvm_tdp_mmu_spte_changed(iter->as_id, iter->gfn, iter->level, iter->old_spte, new_spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1771| <<clear_dirty_pt_masked>> trace_kvm_tdp_mmu_spte_changed(iter.as_id, iter.gfn, iter.level, iter.old_spte, iter.old_spte & ~dbit);
+	 */
 	/* The address space ID, i.e. SMM vs. regular. */
 	int as_id;
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|22| <<tdp_iter_refresh_sptep>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|147| <<try_step_down>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|202| <<try_step_side>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|712| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1328| <<age_gfn_range>> iter->old_spte = tdp_mmu_clear_spte_bits(iter->sptep, iter->old_spte, shadow_accessed_mask, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1342| <<age_gfn_range>> iter->old_spte = kvm_tdp_mmu_write_spte(iter->sptep, iter->old_spte, new_spte, iter->level);
+	 */
 	/* A snapshot of the value at sptep */
 	u64 old_spte;
 	/*
 	 * Whether the iterator has a valid state. This will be false if the
 	 * iterator walks off the end of the paging structure.
 	 */
+	/*
+	 * 在以下设置tdp_iter->valid:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|67| <<tdp_iter_restart>> iter->valid = true;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|85| <<tdp_iter_start>> iter->valid = false;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|281| <<tdp_iter_next>> iter->valid = false;
+	 * 在以下使用tdp_iter->valid:
+	 *   - arch/x86/kvm/mmu/tdp_iter.h|202| <<for_each_tdp_pte_min_level>> for (tdp_iter_start(&iter, root, min_level, start); iter.valid && iter.gfn < end; tdp_iter_next(&iter))
+	 */
 	bool valid;
 	/*
 	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
 	 * which case tdp_iter_next() needs to restart the walk at the root
 	 * level instead of advancing to the next entry.
 	 */
+	/*
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|36| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|800| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1507| <<tdp_mmu_alloc_sp_for_split>> iter->yielded = true;
+	 */
 	bool yielded;
 };
 
@@ -120,11 +314,26 @@ struct tdp_iter {
  * Iterates over every SPTE mapping the GFN range [start, end) in a
  * preorder traversal.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|129| <<for_each_tdp_pte>> for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|698| <<__tdp_mmu_zap_root>> for_each_tdp_pte_min_level(iter, root, zap_level, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|791| <<tdp_mmu_zap_leafs>> for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1273| <<wrprot_gfn_range>> for_each_tdp_pte_min_level(iter, root, min_level, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1436| <<tdp_mmu_split_huge_pages_root>> for_each_tdp_pte_min_level(iter, root, target_level + 1, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1640| <<zap_collapsible_spte_range>> for_each_tdp_pte_min_level(iter, root, PG_LEVEL_2M, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1710| <<write_protect_gfn>> for_each_tdp_pte_min_level(iter, root, min_level, gfn, gfn + 1) {
+ */
 #define for_each_tdp_pte_min_level(iter, root, min_level, start, end) \
 	for (tdp_iter_start(&iter, root, min_level, start); \
 	     iter.valid && iter.gfn < end;		     \
 	     tdp_iter_next(&iter))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|622| <<tdp_root_for_each_pte>> for_each_tdp_pte(_iter, _root, _start, _end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|632| <<tdp_mmu_for_each_pte>> for_each_tdp_pte(_iter, root_to_sp(_mmu->root.hpa), _start, _end)
+ */
 #define for_each_tdp_pte(iter, root, start, end) \
 	for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end)
 
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index 6ae19b4ee..2b1ea62ce 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -11,9 +11,53 @@
 #include <asm/cmpxchg.h>
 #include <trace/events/kvm.h>
 
+/*
+ * x86 kvm使用rcu的一些例子:
+ *   - arch/x86/kvm/mmu/mmu.c|7655| <<kvm_recover_nx_huge_pages>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/mmu.c|7720| <<kvm_recover_nx_huge_pages>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|124| <<tdp_mmu_next_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|764| <<tdp_mmu_iter_cond_resched>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|835| <<tdp_mmu_zap_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|895| <<tdp_mmu_zap_leafs>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1159| <<kvm_tdp_mmu_map>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1257| <<kvm_tdp_mmu_handle_gfn>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1388| <<wrprot_gfn_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1489| <<tdp_mmu_alloc_sp_for_split>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1542| <<tdp_mmu_split_huge_pages_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1638| <<clear_dirty_gfn_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1702| <<clear_dirty_pt_masked>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1761| <<zap_collapsible_spte_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1835| <<write_protect_gfn>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|67| <<kvm_tdp_mmu_walk_lockless_begin>> rcu_read_lock();
+ *   - arch/x86/kvm/x86.c|10080| <<kvm_sched_yield>> rcu_read_lock();
+ *   - arch/x86/kvm/xen.c|2087| <<kvm_xen_hcall_evtchn_send>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|2474| <<kvm_range_has_memory_attributes>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|4049| <<kvm_vcpu_yield_to>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|4273| <<vcpu_get_pid>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|6710| <<kvm_vm_worker_thread>> rcu_read_lock();
+ */
+
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6352| <<kvm_mmu_init_vm>> kvm_mmu_init_tdp_mmu(kvm);
+ */
 /* Initializes the TDP MMU for the VM, if enabled. */
 void kvm_mmu_init_tdp_mmu(struct kvm *kvm)
 {
+	/*
+	 * 在以下设置kvm_arch->tdp_mmu_roots:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|47| <<kvm_mmu_init_tdp_mmu>> INIT_LIST_HEAD(&kvm->arch.tdp_mmu_roots);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|289| <<kvm_tdp_mmu_get_vcpu_root_hpa>> list_add_rcu(&root->link, &kvm->arch.tdp_mmu_roots);
+	 * 在以下使用kvm_arch->tdp_mmu_roots:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|78| <<kvm_mmu_uninit_tdp_mmu>> WARN_ON(!list_empty(&kvm->arch.tdp_mmu_roots));
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|153| <<tdp_mmu_next_root>> next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|157| <<tdp_mmu_next_root>> next_root = list_first_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|165| <<tdp_mmu_next_root>> next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|209| <<for_each_tdp_mmu_root>> list_for_each_entry(_root, &_kvm->arch.tdp_mmu_roots, link) \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1073| <<kvm_tdp_mmu_invalidate_all_roots>> list_for_each_entry(root, &kvm->arch.tdp_mmu_roots, link) {
+	 *
+	 * 一个链表, 因为每个vcpu都有一个root page
+	 */
 	INIT_LIST_HEAD(&kvm->arch.tdp_mmu_roots);
 	spin_lock_init(&kvm->arch.tdp_mmu_pages_lock);
 }
@@ -30,6 +74,10 @@ static __always_inline bool kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6373| <<kvm_mmu_uninit_vm>> kvm_mmu_uninit_tdp_mmu(kvm);
+ */
 void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 {
 	/*
@@ -177,6 +225,9 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 		    kvm_mmu_page_as_id(_root) != _as_id) {		\
 		} else
 
+/*
+ * 分配一个sp和其sp->spt
+ */
 static struct kvm_mmu_page *tdp_mmu_alloc_sp(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu_page *sp;
@@ -187,6 +238,11 @@ static struct kvm_mmu_page *tdp_mmu_alloc_sp(struct kvm_vcpu *vcpu)
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|216| <<tdp_mmu_init_child_sp>> tdp_mmu_init_sp(child_sp, iter->sptep, iter->gfn, role);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|238| <<kvm_tdp_mmu_get_vcpu_root_hpa>> tdp_mmu_init_sp(root, NULL, 0, role);
+ */
 static void tdp_mmu_init_sp(struct kvm_mmu_page *sp, tdp_ptep_t sptep,
 			    gfn_t gfn, union kvm_mmu_page_role role)
 {
@@ -202,6 +258,11 @@ static void tdp_mmu_init_sp(struct kvm_mmu_page *sp, tdp_ptep_t sptep,
 	trace_kvm_mmu_get_page(sp, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1229| <<kvm_tdp_mmu_map>> tdp_mmu_init_child_sp(sp, &iter);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1623| <<tdp_mmu_split_huge_pages_root>> tdp_mmu_init_child_sp(sp, &iter);
+ */
 static void tdp_mmu_init_child_sp(struct kvm_mmu_page *child_sp,
 				  struct tdp_iter *iter)
 {
@@ -216,6 +277,10 @@ static void tdp_mmu_init_child_sp(struct kvm_mmu_page *child_sp,
 	tdp_mmu_init_sp(child_sp, iter->sptep, iter->gfn, role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3878| <<mmu_alloc_direct_roots>> root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);
+ */
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 {
 	union kvm_mmu_page_role role = vcpu->arch.mmu->root_role;
@@ -276,6 +341,10 @@ static void tdp_unaccount_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)
  * @kvm: kvm instance
  * @sp: the page to be removed
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|318| <<handle_removed_pt>> tdp_mmu_unlink_sp(kvm, sp);
+ */
 static void tdp_mmu_unlink_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	tdp_unaccount_mmu_page(kvm, sp);
@@ -306,6 +375,22 @@ static void tdp_mmu_unlink_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
  * this thread will be responsible for ensuring the page is freed. Hence the
  * early rcu_dereferences in the function.
  */
+/*
+ * 只在一个地方调用:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|510| <<handle_changed_spte>> handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
+ *
+ * 508          //
+ * 509		// Recursively handle child PTs if the change removed a subtree from
+ * 510          // the paging structure.  Note the WARN on the PFN changing without the
+ * 511          // SPTE being converted to a hugepage (leaf) or being zapped.  Shadow
+ * 512          // pages are kernel allocations and should never be migrated.
+ * 513          //
+ * 514          if (was_present && !was_leaf &&
+ * 515              (is_leaf || !is_present || WARN_ON_ONCE(pfn_changed)))
+ * 516                  handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
+ *
+ * 因为只在一个地方调用, 参数的pt一定是指向child page table (4k)
+ */
 static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 {
 	struct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(pt));
@@ -319,6 +404,13 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 
 	for (i = 0; i < SPTE_ENT_PER_PAGE; i++) {
 		tdp_ptep_t sptep = pt + i;
+		/*
+		 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+		 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+		 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+		 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+		 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+		 */
 		gfn_t gfn = base_gfn + i * KVM_PAGES_PER_HPAGE(level);
 		u64 old_spte;
 
@@ -382,6 +474,12 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 			old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte,
 							  REMOVED_SPTE, level);
 		}
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|402| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte, new_spte, iter->level, true);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|625| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+		 */
 		handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
 				    old_spte, REMOVED_SPTE, level, shared);
 	}
@@ -405,6 +503,12 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
  * dirty logging updates are handled in common code, not here (see make_spte()
  * and fast_pf_fix_direct_spte()).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|402| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte, new_spte, iter->level, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|625| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+ */
 static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared)
@@ -417,6 +521,13 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 
 	WARN_ON_ONCE(level > PT64_ROOT_MAX_LEVEL);
 	WARN_ON_ONCE(level < PG_LEVEL_4K);
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+	 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+	 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+	 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+	 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+	 */
 	WARN_ON_ONCE(gfn & (KVM_PAGES_PER_HPAGE(level) - 1));
 
 	/*
@@ -540,6 +651,12 @@ static inline int tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	if (!try_cmpxchg64(sptep, &iter->old_spte, new_spte))
 		return -EBUSY;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|402| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte, new_spte, iter->level, true);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|625| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+	 */
 	handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			    new_spte, iter->level, true);
 
@@ -589,6 +706,11 @@ static inline int tdp_mmu_zap_spte_atomic(struct kvm *kvm,
  * Returns the old SPTE value, which _may_ be different than @old_spte if the
  * SPTE had voldatile bits.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|633| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|784| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0, sp->gfn, sp->role.level + 1);
+ */
 static u64 tdp_mmu_set_spte(struct kvm *kvm, int as_id, tdp_ptep_t sptep,
 			    u64 old_spte, u64 new_spte, gfn_t gfn, int level)
 {
@@ -618,9 +740,19 @@ static inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 					  iter->gfn, iter->level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|695| <<tdp_root_for_each_leaf_pte>> tdp_root_for_each_pte(_iter, _root, _start, _end) \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1587| <<clear_dirty_gfn_range>> tdp_root_for_each_pte(iter, root, start, end) {
+ */
 #define tdp_root_for_each_pte(_iter, _root, _start, _end) \
 	for_each_tdp_pte(_iter, _root, _start, _end)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1214| <<kvm_tdp_mmu_handle_gfn>> tdp_root_for_each_leaf_pte(iter, root, range->start, range->end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1651| <<clear_dirty_pt_masked>> tdp_root_for_each_leaf_pte(iter, root, gfn + __ffs(mask), gfn + BITS_PER_LONG) {
+ */
 #define tdp_root_for_each_leaf_pte(_iter, _root, _start, _end)	\
 	tdp_root_for_each_pte(_iter, _root, _start, _end)		\
 		if (!is_shadow_present_pte(_iter.old_spte) ||		\
@@ -628,6 +760,12 @@ static inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 			continue;					\
 		else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1835| <<kvm_tdp_mmu_get_walk>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1862| <<kvm_tdp_mmu_fast_pf_get_last_sptep>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ */
 #define tdp_mmu_for_each_pte(_iter, _mmu, _start, _end)		\
 	for_each_tdp_pte(_iter, root_to_sp(_mmu->root.hpa), _start, _end)
 
@@ -645,12 +783,37 @@ static inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,
  *
  * Returns true if this function yielded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|786| <<__tdp_mmu_zap_root>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|879| <<tdp_mmu_zap_leafs>> tdp_mmu_iter_cond_resched(kvm, &iter, flush, false)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1361| <<wrprot_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1524| <<tdp_mmu_split_huge_pages_root>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1609| <<clear_dirty_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1728| <<zap_collapsible_spte_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ */
 static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 							  struct tdp_iter *iter,
 							  bool flush, bool shared)
 {
 	WARN_ON_ONCE(iter->yielded);
 
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|70| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|159| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 *
+	 * 在以下使用tdp_iter->yielded_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|48| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|776| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 *
+	 * 注释:
+	 * The next_last_level_gfn at the time when the thread last
+	 * yielded. Only yielding when the next_last_level_gfn !=
+	 * yielded_gfn helps ensure forward progress.
+	 *
+	 * 注释里用 > 不是更好吗
+	 */
 	/* Ensure forward progress has been made before yielding. */
 	if (iter->next_last_level_gfn == iter->yielded_gfn)
 		return false;
@@ -670,6 +833,12 @@ static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 
 		WARN_ON_ONCE(iter->gfn > iter->next_last_level_gfn);
 
+		/*
+		 * 注释:
+		 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+		 * which case tdp_iter_next() needs to restart the walk at the root
+		 * level instead of advancing to the next entry.
+		 */
 		iter->yielded = true;
 	}
 
@@ -777,6 +946,11 @@ bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
  * the caller must ensure it does not supply too large a GFN range, or the
  * operation can cause a soft lockup.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|912| <<kvm_tdp_mmu_zap_leafs>> flush = tdp_mmu_zap_leafs(kvm, root, start, end, true, flush);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1206| <<kvm_tdp_mmu_unmap_gfn_range>> flush = tdp_mmu_zap_leafs(kvm, root, range->start, range->end, range->may_block, flush);
+ */
 static bool tdp_mmu_zap_leafs(struct kvm *kvm, struct kvm_mmu_page *root,
 			      gfn_t start, gfn_t end, bool can_yield, bool flush)
 {
@@ -940,6 +1114,10 @@ void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
  * Installs a last-level SPTE to handle a TDP page fault.
  * (NPT/EPT violation/misconfiguration)
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1213| <<kvm_tdp_mmu_map>> ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
+ */
 static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 					  struct kvm_page_fault *fault,
 					  struct tdp_iter *iter)
@@ -1003,6 +1181,11 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
  * Returns: 0 if the new page table was installed. Non-0 if the page table
  *          could not be installed (e.g. the atomic compare-exchange failed).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1185| <<kvm_tdp_mmu_map>> r = tdp_mmu_link_sp(kvm, &iter, sp, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1508| <<tdp_mmu_split_huge_page>> ret = tdp_mmu_link_sp(kvm, iter, sp, shared);
+ */
 static int tdp_mmu_link_sp(struct kvm *kvm, struct tdp_iter *iter,
 			   struct kvm_mmu_page *sp, bool shared)
 {
@@ -1029,6 +1212,10 @@ static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
  * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
  * page tables and SPTEs to translate the faulting guest physical address.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4844| <<kvm_tdp_mmu_page_fault>> r = kvm_tdp_mmu_map(vcpu, fault);
+ */
 int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -1259,6 +1446,10 @@ bool kvm_tdp_mmu_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
  * [start, end). Returns true if an SPTE has been changed and the TLBs need to
  * be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1395| <<kvm_tdp_mmu_wrprot_slot>> spte_set |= wrprot_gfn_range(kvm, root, slot->base_gfn, slot->base_gfn + slot->npages, min_level);
+ */
 static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			     gfn_t start, gfn_t end, int min_level)
 {
@@ -1297,6 +1488,10 @@ static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
  * only affect leaf SPTEs down to min_level.
  * Returns true if an SPTE has been changed and the TLBs need to be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7264| <<kvm_mmu_slot_remove_write_access>> kvm_tdp_mmu_wrprot_slot(kvm, memslot, start_level);
+ */
 bool kvm_tdp_mmu_wrprot_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *slot, int min_level)
 {
@@ -1331,6 +1526,10 @@ static struct kvm_mmu_page *__tdp_mmu_alloc_sp_for_split(gfp_t gfp)
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1590| <<tdp_mmu_split_huge_pages_root>> sp = tdp_mmu_alloc_sp_for_split(kvm, &iter, shared);
+ */
 static struct kvm_mmu_page *tdp_mmu_alloc_sp_for_split(struct kvm *kvm,
 						       struct tdp_iter *iter,
 						       bool shared)
@@ -1505,6 +1704,10 @@ void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
  * each SPTE. Returns true if an SPTE has been changed and the TLBs need to
  * be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1667| <<kvm_tdp_mmu_clear_dirty_slot>> spte_set |= clear_dirty_gfn_range(kvm, root, slot->base_gfn, slot->base_gfn + slot->npages);
+ */
 static bool clear_dirty_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			   gfn_t start, gfn_t end)
 {
@@ -1696,6 +1899,10 @@ void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1858| <<kvm_tdp_mmu_write_protect_gfn>> spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
+ */
 static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
 			      gfn_t gfn, int min_level)
 {
@@ -1732,6 +1939,10 @@ static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1452| <<kvm_mmu_slot_gfn_write_protect>> kvm_tdp_mmu_write_protect_gfn(kvm, slot, gfn, min_level);
+ */
 bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level)
diff --git a/arch/x86/kvm/mmu/tdp_mmu.h b/arch/x86/kvm/mmu/tdp_mmu.h
index 20d97aa46..45b17bd7f 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@ -12,6 +12,11 @@ void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm);
 
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|128| <<tdp_mmu_next_root>> kvm_tdp_mmu_get_root(next_root))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|233| <<kvm_tdp_mmu_get_vcpu_root_hpa>> kvm_tdp_mmu_get_root(root))
+ */
 __must_check static inline bool kvm_tdp_mmu_get_root(struct kvm_mmu_page *root)
 {
 	return refcount_inc_not_zero(&root->tdp_mmu_root_count);
@@ -53,11 +58,19 @@ void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
 				      gfn_t start, gfn_t end,
 				      int target_level, bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|648| <<walk_shadow_page_lockless_begin>> kvm_tdp_mmu_walk_lockless_begin();
+ */
 static inline void kvm_tdp_mmu_walk_lockless_begin(void)
 {
 	rcu_read_lock();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|667| <<walk_shadow_page_lockless_end>> kvm_tdp_mmu_walk_lockless_end();
+ */
 static inline void kvm_tdp_mmu_walk_lockless_end(void)
 {
 	rcu_read_unlock();
diff --git a/arch/x86/kvm/mtrr.c b/arch/x86/kvm/mtrr.c
index a67c28a56..ad35380ab 100644
--- a/arch/x86/kvm/mtrr.c
+++ b/arch/x86/kvm/mtrr.c
@@ -21,24 +21,111 @@
 #include "cpuid.h"
 #include "mmu.h"
 
+/*
+ * 关于IA32_MTRR_DEF_TYPE MSR
+ *
+ * - FE(fixed MTRRs enabled)标志,bit10-当置1时,固定范围MTRRs enabled,反之亦反.
+ * 当固定范围MTRRs的启用时,他们较之于可变的MTRRS的有更高的优先级,所以他们的范围
+ * 发生重叠的时候,那么fixed-range MTRRs则优先于variable-range MTRRs.
+ * 如果在fixed-range MTRRs是disabled的,fixed-range MTRRs仍可使用,可以映射通常由
+ * 固定的范围MTRRs覆盖范围.
+ *
+ * - E(MTRRs enabled)标志(flag),bit11——当它置1时,MTRRs被启用,当被清零时,所有MTRRs
+ * 被禁用,并且UC内存类型适用于所有的物理内存.当此标志被置1,FE标志可以禁用固定范
+ * 围MTRRs(fixed-range MTRRs),当被清零时,是FE标志不会有影响.当E标志置1 ...
+ */
 #define IA32_MTRR_DEF_TYPE_E		(1ULL << 11)
 #define IA32_MTRR_DEF_TYPE_FE		(1ULL << 10)
 #define IA32_MTRR_DEF_TYPE_TYPE_MASK	(0xff)
 
+/*
+ * MTRRs的设置. 通过下面的三个MSR.
+ *
+ * 1. IA32_MTRR_DEF_TYPE MSR
+ *
+ * 2. 一些Fixed Range MTRRs: IA32_MTRR_FIX64K_00000, IA32_MTRR_FIX16K_80000,
+ * IA32_MTRR_FIX16K_A0000等11个64-bit的寄存器. 但是只能控制比较小的range. This
+ * range is divided into sixteen 16-KByte sub-ranges, 8 ranges per register.
+ *
+ * 3. Variable Range MTRRs. 数量由IA32_MTRRCAP决定. encoding和上面一样.
+ */
+
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|364| <<set_var_mtrr_msr>> if (is_mtrr_base_msr(msr))
+ *   - arch/x86/kvm/mtrr.c|423| <<kvm_mtrr_get_msr>> if (is_mtrr_base_msr(msr))
+ */
 static bool is_mtrr_base_msr(unsigned int msr)
 {
 	/* MTRR base MSRs use even numbers, masks use odd numbers. */
 	return !(msr & 0x1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|556| <<get_mtrr_var_range>> rdmsr(MTRRphysBase_MSR(index), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|825| <<generic_get_mtrr>> rdmsr(MTRRphysBase_MSR(reg), base_lo, base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|886| <<set_mtrr_var_ranges>> rdmsr(MTRRphysBase_MSR(index), lo, hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|890| <<set_mtrr_var_ranges>> mtrr_wrmsr(MTRRphysBase_MSR(index), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|1008| <<generic_set_mtrr>> mtrr_wrmsr(MTRRphysBase_MSR(reg), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kvm/mtrr.c|74| <<var_mtrr_msr_to_range>> int index = (msr - MTRRphysBase_MSR(0)) / 2;
+ *   - arch/x86/kvm/mtrr.c|96| <<msr_mtrr_valid>> case MTRRphysBase_MSR(0) ... MTRRphysMask_MSR(KVM_NR_VAR_MTRR - 1):
+ *   - arch/x86/kvm/mtrr.c|169| <<kvm_mtrr_valid>> WARN_ON(!(msr >= MTRRphysBase_MSR(0) &&
+ *   - arch/x86/kvm/x86.c|3851| <<kvm_set_msr_common>> case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ *   - arch/x86/kvm/x86.c|4266| <<kvm_get_msr_common>> case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ *
+ * 326 #define MSR_MTRRfix64K_00000            0x00000250
+ * 327 #define MSR_MTRRfix16K_80000            0x00000258
+ * 328 #define MSR_MTRRfix16K_A0000            0x00000259
+ * 329 #define MSR_MTRRfix4K_C0000             0x00000268
+ * 330 #define MSR_MTRRfix4K_C8000             0x00000269
+ * 331 #define MSR_MTRRfix4K_D0000             0x0000026a
+ * 332 #define MSR_MTRRfix4K_D8000             0x0000026b
+ * 333 #define MSR_MTRRfix4K_E0000             0x0000026c
+ * 334 #define MSR_MTRRfix4K_E8000             0x0000026d
+ * 335 #define MSR_MTRRfix4K_F0000             0x0000026e
+ * 336 #define MSR_MTRRfix4K_F8000             0x0000026f
+ * 337 #define MSR_MTRRdefType                 0x000002ff
+ */
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|338| <<update_mtrr>> var_mtrr_range(var_mtrr_msr_to_range(vcpu, msr), &start, &end);
+ *   - arch/x86/kvm/mtrr.c|354| <<set_var_mtrr_msr>> cur = var_mtrr_msr_to_range(vcpu, msr);
+ *   - arch/x86/kvm/mtrr.c|424| <<kvm_mtrr_get_msr>> *pdata = var_mtrr_msr_to_range(vcpu, msr)->base;
+ *   - arch/x86/kvm/mtrr.c|426| <<kvm_mtrr_get_msr>> *pdata = var_mtrr_msr_to_range(vcpu, msr)->mask;
+ *
+ * 根据msr(base或者mask)的index, 找到&vcpu->arch.mtrr_state.var_ranges[index]
+ */
 static struct kvm_mtrr_range *var_mtrr_msr_to_range(struct kvm_vcpu *vcpu,
 						    unsigned int msr)
 {
+	/*
+	 * #define MTRRphysBase_MSR(reg) (0x200 + 2 * (reg))
+	 * #define MTRRphysMask_MSR(reg) (0x200 + 2 * (reg) + 1)
+	 *
+	 * 一个index对应两个reg, 所以除以2
+	 */
 	int index = (msr - MTRRphysBase_MSR(0)) / 2;
 
+	/*
+	 * struct kvm_mtrr {
+	 *     struct kvm_mtrr_range var_ranges[KVM_NR_VAR_MTRR];
+	 *     mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
+	 *     u64 deftype;
+	 *
+	 *     struct list_head head;
+	 * };
+	 */
 	return &vcpu->arch.mtrr_state.var_ranges[index];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|73| <<kvm_mtrr_valid>> if (!msr_mtrr_valid(msr))
+ *   - arch/x86/kvm/mtrr.c|413| <<kvm_mtrr_get_msr>> if (!msr_mtrr_valid(msr))
+ *
+ * 返回msr是不是kvm支持的msr??
+ */
 static bool msr_mtrr_valid(unsigned msr)
 {
 	switch (msr) {
@@ -60,24 +147,80 @@ static bool msr_mtrr_valid(unsigned msr)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|79| <<kvm_mtrr_valid>> return valid_mtrr_type(data & 0xff);
+ *   - arch/x86/kvm/mtrr.c|82| <<kvm_mtrr_valid>> if (!valid_mtrr_type((data >> (i * 8)) & 0xff))
+ *   - arch/x86/kvm/mtrr.c|94| <<kvm_mtrr_valid>> if (!valid_mtrr_type(data & 0xff))
+ *
+ * 确认type是下面的之一:
+ * 0: UC
+ * 1: WC
+ * 4: WT
+ * 5: WP
+ * 6: WB
+ */
 static bool valid_mtrr_type(unsigned t)
 {
+	/*
+	 * 0: UC
+	 * 1: WC
+	 * 4: WT
+	 * 5: WP
+	 * 6: WB
+	 *
+	 * 0x73是01110011b
+	 */
 	return t < 8 && (1 << t) & 0x73; /* 0, 1, 4, 5, 6 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|382| <<kvm_mtrr_set_msr>> if (!kvm_mtrr_valid(vcpu, msr, data))
+ */
 static bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 {
 	int i;
 	u64 mask;
 
+	/*
+	 * 返回msr是不是kvm支持的msr
+	 */
 	if (!msr_mtrr_valid(msr))
 		return false;
 
+	/*
+	 * MTRRs的设置. 通过下面的三个MSR.
+	 *
+	 * 1. IA32_MTRR_DEF_TYPE MSR
+	 *
+	 * 2. 一些Fixed Range MTRRs: IA32_MTRR_FIX64K_00000, IA32_MTRR_FIX16K_80000,
+	 * IA32_MTRR_FIX16K_A0000等11个64-bit的寄存器. 但是只能控制比较小的range. This
+	 * range is divided into sixteen 16-KByte sub-ranges, 8 ranges per register.
+	 *
+	 * 3. Variable Range MTRRs. 数量由IA32_MTRRCAP决定. encoding和上面一样.
+	 */
 	if (msr == MSR_MTRRdefType) {
 		if (data & ~0xcff)
 			return false;
+		/*
+		 * 确认type是下面的之一:
+		 * 0: UC
+		 * 1: WC
+		 * 4: WT
+		 * 5: WP
+		 * 6: WB
+		 */
 		return valid_mtrr_type(data & 0xff);
 	} else if (msr >= MSR_MTRRfix64K_00000 && msr <= MSR_MTRRfix4K_F8000) {
+		/*
+		 * 确认type是下面的之一:
+		 * 0: UC
+		 * 1: WC
+		 * 4: WT
+		 * 5: WP
+		 * 6: WB
+		 */
 		for (i = 0; i < 8 ; i++)
 			if (!valid_mtrr_type((data >> (i * 8)) & 0xff))
 				return false;
@@ -88,6 +231,10 @@ static bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 	WARN_ON(!(msr >= MTRRphysBase_MSR(0) &&
 		  msr <= MTRRphysMask_MSR(KVM_NR_VAR_MTRR - 1)));
 
+	/*
+	 * 为gpa保留的地址部分生成mask, 从不支持的最高bit到63:
+	 * rsvd_bits(cpuid_maxphyaddr(vcpu), 63)
+	 */
 	mask = kvm_vcpu_reserved_gpa_bits_raw(vcpu);
 	if ((msr & 1) == 0) {
 		/* MTRR base */
@@ -101,21 +248,62 @@ static bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 	return (data & mask) == 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|326| <<update_mtrr>> if (!mtrr_is_enabled(mtrr_state) && msr != MSR_MTRRdefType)
+ *   - arch/x86/kvm/mtrr.c|564| <<mtrr_lookup_start>> if (!mtrr_is_enabled(iter->mtrr_state)) {
+ *
+ * 返回!!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_E)
+ */
 static bool mtrr_is_enabled(struct kvm_mtrr *mtrr_state)
 {
+	/*
+	 * 关于IA32_MTRR_DEF_TYPE MSR
+	 *
+	 * - FE(fixed MTRRs enabled)标志,bit10-当置1时,固定范围MTRRs enabled,反之亦反.
+	 * 当固定范围MTRRs的启用时,他们较之于可变的MTRRS的有更高的优先级,所以他们的范围
+	 * 发生重叠的时候,那么fixed-range MTRRs则优先于variable-range MTRRs.
+	 * 如果在fixed-range MTRRs是disabled的,fixed-range MTRRs仍可使用,可以映射通常由
+	 * 固定的范围MTRRs覆盖范围.
+	 *
+	 * - E(MTRRs enabled)标志(flag),bit11——当它置1时,MTRRs被启用,当被清零时,所有MTRRs
+	 * 被禁用,并且UC内存类型适用于所有的物理内存.当此标志被置1,FE标志可以禁用固定范
+	 * 围MTRRs(fixed-range MTRRs),当被清零时,是FE标志不会有影响.当E标志置1 ...
+	 */
 	return !!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_E);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|331| <<update_mtrr>> if (!fixed_mtrr_is_enabled(mtrr_state))
+ *   - arch/x86/kvm/mtrr.c|475| <<mtrr_lookup_fixed_start>> if (!fixed_mtrr_is_enabled(iter->mtrr_state))
+ *
+ * 返回!!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_FE)
+ */
 static bool fixed_mtrr_is_enabled(struct kvm_mtrr *mtrr_state)
 {
 	return !!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_FE);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|678| <<kvm_mtrr_get_guest_memory_type>> return mtrr_default_type(mtrr_state);
+ *   - arch/x86/kvm/mtrr.c|719| <<kvm_mtrr_check_gfn_range_consistency>> return type == mtrr_default_type(mtrr_state);
+ *
+ * 返回mtrr_state->deftype & IA32_MTRR_DEF_TYPE_TYPE_MASK
+ */
 static u8 mtrr_default_type(struct kvm_mtrr *mtrr_state)
 {
 	return mtrr_state->deftype & IA32_MTRR_DEF_TYPE_TYPE_MASK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|674| <<kvm_mtrr_get_guest_memory_type>> return mtrr_disabled_type(vcpu);
+ *
+ * 如果cpuid支持mtrr, 当IA32_MTRR_DEF_TYPE_E被disable的时候全用UC,
+ * 如果都不支持mtrr, 用WB.
+ */
 static u8 mtrr_disabled_type(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -150,6 +338,33 @@ struct fixed_mtrr_segment {
 	int range_start;
 };
 
+/*
+ * 326 #define MSR_MTRRfix64K_00000            0x00000250
+ * 327 #define MSR_MTRRfix16K_80000            0x00000258
+ * 328 #define MSR_MTRRfix16K_A0000            0x00000259
+ * 329 #define MSR_MTRRfix4K_C0000             0x00000268
+ * 330 #define MSR_MTRRfix4K_C8000             0x00000269
+ * 331 #define MSR_MTRRfix4K_D0000             0x0000026a
+ * 332 #define MSR_MTRRfix4K_D8000             0x0000026b
+ * 333 #define MSR_MTRRfix4K_E0000             0x0000026c
+ * 334 #define MSR_MTRRfix4K_E8000             0x0000026d
+ * 335 #define MSR_MTRRfix4K_F0000             0x0000026e
+ * 336 #define MSR_MTRRfix4K_F8000             0x0000026f
+ * 337 #define MSR_MTRRdefType                 0x000002ff
+ *
+ * 在以下使用fixed_seg_table[] (fixed_mtrr_segment类型):
+ *   - arch/x86/kvm/mtrr.c|191| <<fixed_mtrr_seg_unit_size>> return 8 << fixed_seg_table[seg].range_shift;
+ *   - arch/x86/kvm/mtrr.c|222| <<fixed_mtrr_seg_unit_range>> struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
+ *   - arch/x86/kvm/mtrr.c|232| <<fixed_mtrr_seg_unit_range_index>> struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
+ *   - arch/x86/kvm/mtrr.c|243| <<fixed_mtrr_seg_end_range_index>> struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
+ *   - arch/x86/kvm/mtrr.c|274| <<fixed_mtrr_addr_to_seg>> int seg, seg_num = ARRAY_SIZE(fixed_seg_table);
+ *   - arch/x86/kvm/mtrr.c|277| <<fixed_mtrr_addr_to_seg>> mtrr_seg = &fixed_seg_table[seg];
+ *   - arch/x86/kvm/mtrr.c|290| <<fixed_mtrr_addr_seg_to_range_index>> mtrr_seg = &fixed_seg_table[seg];
+ *   - arch/x86/kvm/mtrr.c|298| <<fixed_mtrr_range_end_addr>> struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
+ *
+ * https://github.com/finallyjustice/codereading/blob/master/comment/summary/x86_cache_pat/x86_cache_pat.md
+ * 64K是64K bytes!
+ */
 static struct fixed_mtrr_segment fixed_seg_table[] = {
 	/* MSR_MTRRfix64K_00000, 1 unit. 64K fixed mtrr. */
 	{
@@ -186,11 +401,28 @@ static struct fixed_mtrr_segment fixed_seg_table[] = {
  * The size of unit is covered in one MSR, one MSR entry contains
  * 8 ranges so that unit size is always 8 * 2^range_shift.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|223| <<fixed_mtrr_seg_unit_range>> u64 unit_size = fixed_mtrr_seg_unit_size(seg);
+ *   - arch/x86/kvm/mtrr.c|234| <<fixed_mtrr_seg_unit_range_index>> WARN_ON(mtrr_seg->start + unit * fixed_mtrr_seg_unit_size(seg)
+ *
+ * 一个msr能cover个8个和在一起是一个seg???
+ * 所以 8 << ...
+ */
 static u64 fixed_mtrr_seg_unit_size(int seg)
 {
 	return 8 << fixed_seg_table[seg].range_shift;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|254| <<fixed_msr_to_range>> if (!fixed_msr_to_seg_unit(msr, &seg, &unit))
+ *   - arch/x86/kvm/mtrr.c|265| <<fixed_msr_to_range_index>> if (!fixed_msr_to_seg_unit(msr, &seg, &unit))
+ *
+ * 把seg和unit在参数返回
+ * seg是相同size的(64K-bytes, 16K-bytes, 4K-bytes)的index: 0, 1, 2
+ * unit是在seg内的index
+ */
 static bool fixed_msr_to_seg_unit(u32 msr, int *seg, int *unit)
 {
 	switch (msr) {
@@ -217,6 +449,13 @@ static bool fixed_msr_to_seg_unit(u32 msr, int *seg, int *unit)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|391| <<fixed_msr_to_range>> fixed_mtrr_seg_unit_range(seg, unit, start, end);
+ *
+ * 通过参数的seg, unit,
+ * 获取对应的那一个(8个bit管理的range)的start和end地址
+ */
 static void fixed_mtrr_seg_unit_range(int seg, int unit, u64 *start, u64 *end)
 {
 	struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
@@ -227,6 +466,15 @@ static void fixed_mtrr_seg_unit_range(int seg, int unit, u64 *start, u64 *end)
 	WARN_ON(*end > mtrr_seg->end);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|402| <<fixed_msr_to_range_index>> return fixed_mtrr_seg_unit_range_index(seg, unit);
+ *
+ * 把seg和unit在参数返回
+ * seg是相同size的(64K-bytes, 16K-bytes, 4K-bytes)的index: 0, 1, 2
+ * unit是在seg内的index
+ * ---> 这里返回的这个(seg,unit)对应的起始的range的index
+ */
 static int fixed_mtrr_seg_unit_range_index(int seg, int unit)
 {
 	struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
@@ -238,6 +486,12 @@ static int fixed_mtrr_seg_unit_range_index(int seg, int unit)
 	return mtrr_seg->range_start + 8 * unit;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|730| <<mtrr_lookup_fixed_next>> if (iter->index > fixed_mtrr_seg_end_range_index(iter->seg))
+ *
+ * 这个seg(0, 1或者2)的range的最后一个的index
+ */
 static int fixed_mtrr_seg_end_range_index(int seg)
 {
 	struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
@@ -247,17 +501,41 @@ static int fixed_mtrr_seg_end_range_index(int seg)
 	return mtrr_seg->range_start + n - 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|485| <<update_mtrr>> if (fixed_msr_to_range(msr, &start, &end)) {
+ *
+ * 这里的输入的参数是msr, 一个msr可以管理8个range,
+ * 这里通过fixed_seg_table返回的是8个range的start/end地址
+ */
 static bool fixed_msr_to_range(u32 msr, u64 *start, u64 *end)
 {
 	int seg, unit;
 
+	/*
+	 * 把seg和unit在参数返回
+	 * seg是相同size的(64K-bytes, 16K-bytes, 4K-bytes)的index: 0, 1, 2
+	 * unit是在seg内的index
+	 */
 	if (!fixed_msr_to_seg_unit(msr, &seg, &unit))
 		return false;
 
+	/*
+	 * 通过参数的seg, unit,
+	 * 获取对应的那一个(8个bit管理的range)的start和end地址
+	 */
 	fixed_mtrr_seg_unit_range(seg, unit, start, end);
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|555| <<kvm_mtrr_set_msr>> index = fixed_msr_to_range_index(msr);
+ *   - arch/x86/kvm/mtrr.c|593| <<kvm_mtrr_get_msr>> index = fixed_msr_to_range_index(msr);
+ *
+ * 这里的输入的参数是msr, 一个msr可以管理8个range,
+ * 这里通过fixed_seg_table返回的是8个range的一起的start index
+ */
 static int fixed_msr_to_range_index(u32 msr)
 {
 	int seg, unit;
@@ -265,9 +543,21 @@ static int fixed_msr_to_range_index(u32 msr)
 	if (!fixed_msr_to_seg_unit(msr, &seg, &unit))
 		return -1;
 
+	/*
+	 * 把seg和unit在参数返回
+	 * seg是相同size的(64K-bytes, 16K-bytes, 4K-bytes)的index: 0, 1, 2
+	 * unit是在seg内的index
+	 * ---> 这里返回的这个(seg,unit)对应的起始的range的index
+	 */
 	return fixed_mtrr_seg_unit_range_index(seg, unit);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|478| <<mtrr_lookup_fixed_start>> seg = fixed_mtrr_addr_to_seg(iter->start);
+ *
+ * 给一个地址, 返回fixed_seg_table中的index (0, 1或者2)
+ */
 static int fixed_mtrr_addr_to_seg(u64 addr)
 {
 	struct fixed_mtrr_segment *mtrr_seg;
@@ -282,6 +572,12 @@ static int fixed_mtrr_addr_to_seg(u64 addr)
 	return -1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|483| <<mtrr_lookup_fixed_start>> index = fixed_mtrr_addr_seg_to_range_index(iter->start, seg);
+ *
+ * 返回一个地址在一个seg(fixed_seg_table)内部的range index
+ */
 static int fixed_mtrr_addr_seg_to_range_index(u64 addr, int seg)
 {
 	struct fixed_mtrr_segment *mtrr_seg;
@@ -293,6 +589,12 @@ static int fixed_mtrr_addr_seg_to_range_index(u64 addr, int seg)
 	return index;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|540| <<mtrr_lookup_fixed_next>> if (fixed_mtrr_range_end_addr(iter->seg, iter->index) >= iter->end) {
+ *
+ * 给一个seg, 和range在seg内部的index, 返回range的end address
+ */
 static u64 fixed_mtrr_range_end_addr(int seg, int index)
 {
 	struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
@@ -301,6 +603,19 @@ static u64 fixed_mtrr_range_end_addr(int seg, int index)
 	return mtrr_seg->start + ((pos + 1) << mtrr_seg->range_shift);
 }
 
+/*
+ * struct kvm_mtrr_range {
+ *     u64 base;
+ *     u64 mask;
+ *     struct list_head node;
+ * };
+ *
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|338| <<update_mtrr>> var_mtrr_range(var_mtrr_msr_to_range(vcpu, msr), &start, &end);
+ *   - arch/x86/kvm/mtrr.c|494| <<match_var_range>> var_mtrr_range(range, &start, &end);
+ *
+ * 输入的参数是一个kvm_mtrr_range, 返回start和end地址
+ */
 static void var_mtrr_range(struct kvm_mtrr_range *range, u64 *start, u64 *end)
 {
 	u64 mask;
@@ -315,17 +630,32 @@ static void var_mtrr_range(struct kvm_mtrr_range *range, u64 *start, u64 *end)
 	*end = (*start | ~mask) + 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|393| <<kvm_mtrr_set_msr>> update_mtrr(vcpu, msr);
+ */
 static void update_mtrr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_mtrr *mtrr_state = &vcpu->arch.mtrr_state;
 	gfn_t start, end;
 
+	/*
+	 * 不honor的话直接return
+	 */
 	if (!kvm_mmu_honors_guest_mtrrs(vcpu->kvm))
 		return;
 
+	/*
+	 * 返回!!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_E)
+	 */
 	if (!mtrr_is_enabled(mtrr_state) && msr != MSR_MTRRdefType)
 		return;
 
+	/*
+	 * 如果是fixed:
+	 * 这里的输入的参数是msr, 一个msr可以管理8个range,
+	 * 这里通过fixed_seg_table返回的是8个range的start/end地址
+	 */
 	/* fixed MTRRs. */
 	if (fixed_msr_to_range(msr, &start, &end)) {
 		if (!fixed_mtrr_is_enabled(mtrr_state))
@@ -334,23 +664,60 @@ static void update_mtrr(struct kvm_vcpu *vcpu, u32 msr)
 		start = 0x0;
 		end = ~0ULL;
 	} else {
+		/*
+		 * 输入的参数是一个kvm_mtrr_range, 返回start和end地址
+		 */
 		/* variable range MTRRs. */
 		var_mtrr_range(var_mtrr_msr_to_range(vcpu, msr), &start, &end);
 	}
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mtrr.c|625| <<update_mtrr>> kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));
+	 *   - arch/x86/kvm/x86.c|967| <<kvm_post_set_cr0>> kvm_zap_gfn_range(vcpu->kvm, 0, ~0ULL);
+	 *   - arch/x86/kvm/x86.c|10615| <<__kvm_set_or_clear_apicv_inhibit>> kvm_zap_gfn_range(kvm, gfn, gfn+1);
+	 *   - arch/x86/kvm/x86.c|13432| <<kvm_noncoherent_dma_assignment_start_or_stop>> kvm_zap_gfn_range(kvm, gpa_to_gfn(0), gpa_to_gfn(~0ULL));
+	 */
 	kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|357| <<set_var_mtrr_msr>> if (var_mtrr_range_is_valid(cur))
+ *   - arch/x86/kvm/mtrr.c|370| <<set_var_mtrr_msr>> if (var_mtrr_range_is_valid(cur)) {
+ *
+ * struct kvm_mtrr_range {
+ *     u64 base;
+ *     u64 mask;
+ *     struct list_head node;
+ * };
+ */
 static bool var_mtrr_range_is_valid(struct kvm_mtrr_range *range)
 {
 	return (range->mask & (1 << 11)) != 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|391| <<kvm_mtrr_set_msr>> set_var_mtrr_msr(vcpu, msr, data);
+ */
 static void set_var_mtrr_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 {
+	/*
+	 * struct kvm_mtrr {
+	 *     struct kvm_mtrr_range var_ranges[KVM_NR_VAR_MTRR];
+	 *     mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
+	 *     u64 deftype;
+	 *
+	 *     struct list_head head;
+	 * };
+	 */
 	struct kvm_mtrr *mtrr_state = &vcpu->arch.mtrr_state;
 	struct kvm_mtrr_range *tmp, *cur;
 
+	/*
+	 * 根据msr(base或者mask)的index, 找到&vcpu->arch.mtrr_state.var_ranges[index]
+	 */
 	cur = var_mtrr_msr_to_range(vcpu, msr);
 
 	/* remove the entry if it's in the list. */
@@ -371,17 +738,37 @@ static void set_var_mtrr_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 		list_for_each_entry(tmp, &mtrr_state->head, node)
 			if (cur->base >= tmp->base)
 				break;
+		/*
+		 * struct kvm_mtrr_range {
+		 *     u64 base;
+		 *     u64 mask;
+		 *     struct list_head node;
+		 * };
+		 */
 		list_add_tail(&cur->node, &tmp->node);
 	}
 }
 
+/*
+ * 处理下面的MSR:
+ * 3851         case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ * 3852         case MSR_MTRRdefType:
+ *   - arch/x86/kvm/x86.c|3853| <<kvm_set_msr_common>> return kvm_mtrr_set_msr(vcpu, msr, data);
+ */
 int kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 {
 	int index;
 
+	/*
+	 * 只在这里调用
+	 */
 	if (!kvm_mtrr_valid(vcpu, msr, data))
 		return 1;
 
+	/*
+	 * 这里的输入的参数是msr, 一个msr可以管理8个range,
+	 * 这里通过fixed_seg_table返回的是8个range的一起的start index
+	 */
 	index = fixed_msr_to_range_index(msr);
 	if (index >= 0)
 		*(u64 *)&vcpu->arch.mtrr_state.fixed_ranges[index] = data;
@@ -390,10 +777,20 @@ int kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 	else
 		set_var_mtrr_msr(vcpu, msr, data);
 
+	/*
+	 * 只在此处调用
+	 */
 	update_mtrr(vcpu, msr);
 	return 0;
 }
 
+/*
+ * 处理下面的MSR:
+ * 4265         case MSR_MTRRcap:
+ * 4266         case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ * 4267         case MSR_MTRRdefType:
+ *   - arch/x86/kvm/x86.c|4268| <<kvm_get_msr_common>> return kvm_mtrr_get_msr(vcpu, msr_info->index, &msr_info->data);
+ */
 int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata)
 {
 	int index;
@@ -431,21 +828,71 @@ int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12149| <<kvm_arch_vcpu_create>> kvm_vcpu_mtrr_init(vcpu);
+ */
 void kvm_vcpu_mtrr_init(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_mtrr {
+	 *     struct kvm_mtrr_range var_ranges[KVM_NR_VAR_MTRR];
+	 *     mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
+	 *     u64 deftype;
+	 *
+	 *     struct list_head head;
+	 * };
+	 */
 	INIT_LIST_HEAD(&vcpu->arch.mtrr_state.head);
 }
 
+/*
+ * 在以下使用mtrr_iter->mtrr_state:
+ *   - arch/x86/kvm/mtrr.c|690| <<mtrr_lookup_fixed_start>> if (!fixed_mtrr_is_enabled(iter->mtrr_state))
+ *   - arch/x86/kvm/mtrr.c|730| <<__mtrr_lookup_var_next>> struct kvm_mtrr *mtrr_state = iter->mtrr_state;
+ *   - arch/x86/kvm/mtrr.c|732| <<__mtrr_lookup_var_next>> list_for_each_entry_continue(iter->range, &mtrr_state->head, node)
+ *   - arch/x86/kvm/mtrr.c|742| <<mtrr_lookup_var_start>> struct kvm_mtrr *mtrr_state = iter->mtrr_state;
+ *   - arch/x86/kvm/mtrr.c|747| <<mtrr_lookup_var_start>> iter->range = list_prepare_entry(iter->range, &mtrr_state->head, node);
+ *   - arch/x86/kvm/mtrr.c|768| <<mtrr_lookup_fixed_next>> if (iter->index >= ARRAY_SIZE(iter->mtrr_state->fixed_ranges))
+ *   - arch/x86/kvm/mtrr.c|791| <<mtrr_lookup_start>> if (!mtrr_is_enabled(iter->mtrr_state)) {
+ *   - arch/x86/kvm/mtrr.c|807| <<mtrr_lookup_init>> iter->mtrr_state = mtrr_state;
+ *   - arch/x86/kvm/mtrr.c|825| <<mtrr_lookup_okay>> iter->mem_type = iter->mtrr_state->fixed_ranges[iter->index];
+ *   - arch/x86/kvm/mtrr.c|874| <<kvm_mtrr_get_guest_memory_type>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ *   - arch/x86/kvm/mtrr.c|961| <<kvm_mtrr_check_gfn_range_consistency>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ */
 struct mtrr_iter {
 	/* input fields. */
 	struct kvm_mtrr *mtrr_state;
 	u64 start;
 	u64 end;
 
+	/*
+	 * 在以下使用mtrr_iter->mem_type:
+	 *   - arch/x86/kvm/mtrr.c|825| <<mtrr_lookup_okay>> iter->mem_type = iter->mtrr_state->fixed_ranges[iter->index];
+	 *   - arch/x86/kvm/mtrr.c|830| <<mtrr_lookup_okay>> iter->mem_type = iter->range->base & 0xff;
+	 *   - arch/x86/kvm/mtrr.c|875| <<kvm_mtrr_get_guest_memory_type>> int curr_type = iter.mem_type;
+	 *   - arch/x86/kvm/mtrr.c|963| <<kvm_mtrr_check_gfn_range_consistency>> type = iter.mem_type; 
+	 *   - arch/x86/kvm/mtrr.c|967| <<kvm_mtrr_check_gfn_range_consistency>> if (type != iter.mem_type)
+	 */
 	/* output fields. */
 	int mem_type;
+	/*
+	 * 在以下使用mtrr_iter->mtrr_disabled:
+	 *   - arch/x86/kvm/mtrr.c|792| <<mtrr_lookup_start>> iter->mtrr_disabled = true;
+	 *   - arch/x86/kvm/mtrr.c|810| <<mtrr_lookup_init>> iter->mtrr_disabled = false;
+	 *   - arch/x86/kvm/mtrr.c|921| <<kvm_mtrr_get_guest_memory_type>> if (iter.mtrr_disabled)
+	 *   - arch/x86/kvm/mtrr.c|971| <<kvm_mtrr_check_gfn_range_consistency>> if (iter.mtrr_disabled)
+	 */
 	/* mtrr is completely disabled? */
 	bool mtrr_disabled;
+	/*
+	 * 在以下使用mtrr_iter->partial_map:
+	 *   - arch/x86/kvm/mtrr.c|855| <<match_var_range>> iter->partial_map |= iter->start_max < start;
+	 *   - arch/x86/kvm/mtrr.c|874| <<__mtrr_lookup_var_next>> iter->partial_map |= iter->start_max < iter->end;
+	 *   - arch/x86/kvm/mtrr.c|948| <<mtrr_lookup_init>> iter->partial_map = false;
+	 *   - arch/x86/kvm/mtrr.c|1069| <<kvm_mtrr_get_guest_memory_type>> WARN_ON(iter.partial_map);
+	 *   - arch/x86/kvm/mtrr.c|1111| <<kvm_mtrr_check_gfn_range_consistency>> if (!iter.partial_map)
+	 */
 	/* [start, end) is not fully covered in MTRRs? */
 	bool partial_map;
 
@@ -460,37 +907,73 @@ struct mtrr_iter {
 		/* used for var MTRRs. */
 		struct {
 			struct kvm_mtrr_range *range;
+			/*
+			 * 在以下使用mtrr_iter的start_max:
+			 *   - arch/x86/kvm/mtrr.c|855| <<match_var_range>> iter->partial_map |= iter->start_max < start;
+			 *   - arch/x86/kvm/mtrr.c|858| <<match_var_range>> iter->start_max = max(iter->start_max, end);
+			 *   - arch/x86/kvm/mtrr.c|874| <<__mtrr_lookup_var_next>> iter->partial_map |= iter->start_max < iter->end;
+			 *   - arch/x86/kvm/mtrr.c|882| <<mtrr_lookup_var_start>> iter->start_max = iter->start;
+			 */
 			/* max address has been covered in var MTRRs. */
 			u64 start_max;
 		};
 	};
 
+	/*
+	 * 在以下使用mtrr_iter->fixed:
+	 *   - arch/x86/kvm/mtrr.c|834| <<mtrr_lookup_fixed_start>> iter->fixed = true;
+	 *   - arch/x86/kvm/mtrr.c|881| <<mtrr_lookup_var_start>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|897| <<mtrr_lookup_fixed_next>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|949| <<mtrr_lookup_init>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|961| <<mtrr_lookup_okay>> if (iter->fixed) {
+	 *   - arch/x86/kvm/mtrr.c|980| <<mtrr_lookup_next>> if (iter->fixed)
+	 */
 	bool fixed;
 };
 
+/*
+ * called by;
+ *   - arch/x86/kvm/mtrr.c|1047| <<mtrr_lookup_start>> if (!mtrr_lookup_fixed_start(iter))
+ */
 static bool mtrr_lookup_fixed_start(struct mtrr_iter *iter)
 {
 	int seg, index;
 
+	/*
+	 * 返回!!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_FE)
+	 */
 	if (!fixed_mtrr_is_enabled(iter->mtrr_state))
 		return false;
 
+	/*
+	 * 给一个地址, 返回fixed_seg_table中的index (0, 1或者2)
+	 */
 	seg = fixed_mtrr_addr_to_seg(iter->start);
 	if (seg < 0)
 		return false;
 
 	iter->fixed = true;
+	/*
+	 * 返回一个地址在一个seg(fixed_seg_table)内部的range index
+	 */
 	index = fixed_mtrr_addr_seg_to_range_index(iter->start, seg);
 	iter->index = index;
 	iter->seg = seg;
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|984| <<__mtrr_lookup_var_next>> if (match_var_range(iter, iter->range))
+ */
 static bool match_var_range(struct mtrr_iter *iter,
 			    struct kvm_mtrr_range *range)
 {
 	u64 start, end;
 
+	/*
+	 * 输入的参数是一个kvm_mtrr_range, 返回start和end地址
+	 */
 	var_mtrr_range(range, &start, &end);
 	if (!(start >= iter->end || end <= iter->start)) {
 		iter->range = range;
@@ -510,6 +993,11 @@ static bool match_var_range(struct mtrr_iter *iter,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|1000| <<mtrr_lookup_var_start>> __mtrr_lookup_var_next(iter);
+ *   - arch/x86/kvm/mtrr.c|1033| <<mtrr_lookup_var_next>> __mtrr_lookup_var_next(iter);
+ */
 static void __mtrr_lookup_var_next(struct mtrr_iter *iter)
 {
 	struct kvm_mtrr *mtrr_state = iter->mtrr_state;
@@ -522,6 +1010,11 @@ static void __mtrr_lookup_var_next(struct mtrr_iter *iter)
 	iter->partial_map |= iter->start_max < iter->end;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|1020| <<mtrr_lookup_fixed_next>> return mtrr_lookup_var_start(iter);
+ *   - arch/x86/kvm/mtrr.c|1048| <<mtrr_lookup_start>> mtrr_lookup_var_start(iter);
+ */
 static void mtrr_lookup_var_start(struct mtrr_iter *iter)
 {
 	struct kvm_mtrr *mtrr_state = iter->mtrr_state;
@@ -529,13 +1022,34 @@ static void mtrr_lookup_var_start(struct mtrr_iter *iter)
 	iter->fixed = false;
 	iter->start_max = iter->start;
 	iter->range = NULL;
+	/*
+	 * struct kvm_mtrr_range *range;
+	 *
+	 * struct kvm_mtrr_range {
+	 *     u64 base;
+	 *     u64 mask;
+	 *     struct list_head node;
+	 * };
+	 */
 	iter->range = list_prepare_entry(iter->range, &mtrr_state->head, node);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mtrr.c|1000| <<mtrr_lookup_var_start>> __mtrr_lookup_var_next(iter);
+	 *   - arch/x86/kvm/mtrr.c|1033| <<mtrr_lookup_var_next>> __mtrr_lookup_var_next(iter);
+	 */
 	__mtrr_lookup_var_next(iter);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|809| <<mtrr_lookup_next>> mtrr_lookup_fixed_next(iter);
+ */
 static void mtrr_lookup_fixed_next(struct mtrr_iter *iter)
 {
+	/*
+	 * 给一个seg, 和range在seg内部的index, 返回range的end address
+	 */
 	/* terminate the lookup. */
 	if (fixed_mtrr_range_end_addr(iter->seg, iter->index) >= iter->end) {
 		iter->fixed = false;
@@ -549,19 +1063,45 @@ static void mtrr_lookup_fixed_next(struct mtrr_iter *iter)
 	if (iter->index >= ARRAY_SIZE(iter->mtrr_state->fixed_ranges))
 		return mtrr_lookup_var_start(iter);
 
+	/*
+	 * 这个seg(0, 1或者2)的range的最后一个的index
+	 */
 	/* switch to next segment. */
 	if (iter->index > fixed_mtrr_seg_end_range_index(iter->seg))
 		iter->seg++;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|811| <<mtrr_lookup_next>> mtrr_lookup_var_next(iter);
+ */
 static void mtrr_lookup_var_next(struct mtrr_iter *iter)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mtrr.c|1000| <<mtrr_lookup_var_start>> __mtrr_lookup_var_next(iter);
+	 *   - arch/x86/kvm/mtrr.c|1033| <<mtrr_lookup_var_next>> __mtrr_lookup_var_next(iter);
+	 */
 	__mtrr_lookup_var_next(iter);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|788| <<mtrr_lookup_init>> mtrr_lookup_start(iter);
+ */
 static void mtrr_lookup_start(struct mtrr_iter *iter)
 {
+	/*
+	 * 返回!!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_E)
+	 */
 	if (!mtrr_is_enabled(iter->mtrr_state)) {
+		/*
+		 * 在以下使用mtrr_iter->mtrr_disabled:
+		 *   - arch/x86/kvm/mtrr.c|792| <<mtrr_lookup_start>> iter->mtrr_disabled = true;
+		 *   - arch/x86/kvm/mtrr.c|810| <<mtrr_lookup_init>> iter->mtrr_disabled = false;
+		 *   - arch/x86/kvm/mtrr.c|921| <<kvm_mtrr_get_guest_memory_type>> if (iter.mtrr_disabled)
+		 *   - arch/x86/kvm/mtrr.c|971| <<kvm_mtrr_check_gfn_range_consistency>> if (iter.mtrr_disabled)
+		 */
 		iter->mtrr_disabled = true;
 		return;
 	}
@@ -570,6 +1110,14 @@ static void mtrr_lookup_start(struct mtrr_iter *iter)
 		mtrr_lookup_var_start(iter);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|830| <<kvm_mtrr_get_guest_memory_type>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ *   - arch/x86/kvm/mtrr.c|917| <<kvm_mtrr_check_gfn_range_consistency>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ *
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|815| <<mtrr_for_each_mem_type>> for (mtrr_lookup_init(_iter_, _mtrr_, _gpa_start_, _gpa_end_); \
+ */
 static void mtrr_lookup_init(struct mtrr_iter *iter,
 			     struct kvm_mtrr *mtrr_state, u64 start, u64 end)
 {
@@ -577,6 +1125,10 @@ static void mtrr_lookup_init(struct mtrr_iter *iter,
 	iter->start = start;
 	iter->end = end;
 	iter->mtrr_disabled = false;
+	/*
+	 * 注释:
+	 * [start, end) is not fully covered in MTRRs?
+	 */
 	iter->partial_map = false;
 	iter->fixed = false;
 	iter->range = NULL;
@@ -584,9 +1136,18 @@ static void mtrr_lookup_init(struct mtrr_iter *iter,
 	mtrr_lookup_start(iter);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|816| <<mtrr_for_each_mem_type>> mtrr_lookup_okay(_iter_); mtrr_lookup_next(_iter_))
+ */
 static bool mtrr_lookup_okay(struct mtrr_iter *iter)
 {
 	if (iter->fixed) {
+		/*
+		 * 注释:
+		 * In the Intel processor's MTRR interface, the MTRR type is always held in
+		 * an 8 bit field:
+		 */
 		iter->mem_type = iter->mtrr_state->fixed_ranges[iter->index];
 		return true;
 	}
@@ -599,18 +1160,51 @@ static bool mtrr_lookup_okay(struct mtrr_iter *iter)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|816| <<mtrr_for_each_mem_type>> mtrr_lookup_okay(_iter_); mtrr_lookup_next(_iter_))
+ */
 static void mtrr_lookup_next(struct mtrr_iter *iter)
 {
+	/*
+	 * 在以下使用mtrr_iter->fixed:
+	 *   - arch/x86/kvm/mtrr.c|834| <<mtrr_lookup_fixed_start>> iter->fixed = true;
+	 *   - arch/x86/kvm/mtrr.c|881| <<mtrr_lookup_var_start>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|897| <<mtrr_lookup_fixed_next>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|949| <<mtrr_lookup_init>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|961| <<mtrr_lookup_okay>> if (iter->fixed) {
+	 *   - arch/x86/kvm/mtrr.c|980| <<mtrr_lookup_next>> if (iter->fixed)
+	 */
 	if (iter->fixed)
 		mtrr_lookup_fixed_next(iter);
 	else
 		mtrr_lookup_var_next(iter);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|830| <<kvm_mtrr_get_guest_memory_type>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ *   - arch/x86/kvm/mtrr.c|917| <<kvm_mtrr_check_gfn_range_consistency>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ */
 #define mtrr_for_each_mem_type(_iter_, _mtrr_, _gpa_start_, _gpa_end_) \
 	for (mtrr_lookup_init(_iter_, _mtrr_, _gpa_start_, _gpa_end_); \
 	     mtrr_lookup_okay(_iter_); mtrr_lookup_next(_iter_))
 
+/*
+ * 返回下面这些:
+ * // MTRR memory types, which are defined in SDM
+ * #define MTRR_TYPE_UNCACHABLE 0
+ * #define MTRR_TYPE_WRCOMB     1
+ * //#define MTRR_TYPE_         2
+ * //#define MTRR_TYPE_         3
+ * #define MTRR_TYPE_WRTHROUGH  4
+ * #define MTRR_TYPE_WRPROT     5
+ * #define MTRR_TYPE_WRBACK     6
+ * #define MTRR_NUM_TYPES       7
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7624| <<vmx_get_mt_mask>> return kvm_mtrr_get_guest_memory_type(vcpu, gfn) << VMX_EPT_MT_EPTE_SHIFT;
+ */
 u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	struct kvm_mtrr *mtrr_state = &vcpu->arch.mtrr_state;
@@ -677,6 +1271,14 @@ u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn)
 	if (type == -1)
 		return mtrr_default_type(mtrr_state);
 
+	/*
+	 * 在以下使用mtrr_iter->partial_map:
+	 *   - arch/x86/kvm/mtrr.c|855| <<match_var_range>> iter->partial_map |= iter->start_max < start;
+	 *   - arch/x86/kvm/mtrr.c|874| <<__mtrr_lookup_var_next>> iter->partial_map |= iter->start_max < iter->end;
+	 *   - arch/x86/kvm/mtrr.c|948| <<mtrr_lookup_init>> iter->partial_map = false;
+	 *   - arch/x86/kvm/mtrr.c|1069| <<kvm_mtrr_get_guest_memory_type>> WARN_ON(iter.partial_map);
+	 *   - arch/x86/kvm/mtrr.c|1111| <<kvm_mtrr_check_gfn_range_consistency>> if (!iter.partial_map)
+	 */
 	/*
 	 * We just check one page, partially covered by MTRRs is
 	 * impossible.
@@ -687,9 +1289,22 @@ u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_mtrr_get_guest_memory_type);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4636| <<kvm_tdp_page_fault>> if (kvm_mtrr_check_gfn_range_consistency(vcpu, base, page_num))
+ */
 bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
 					  int page_num)
 {
+	/*
+	 * struct kvm_mtrr {
+	 *     struct kvm_mtrr_range var_ranges[KVM_NR_VAR_MTRR];
+	 *     mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
+	 *     u64 deftype;
+	 *
+	 *     struct list_head head;
+	 * };
+	 */
 	struct kvm_mtrr *mtrr_state = &vcpu->arch.mtrr_state;
 	struct mtrr_iter iter;
 	u64 start, end;
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 87cc6c880..75804e01f 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -170,6 +170,16 @@ static u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 	return sample_period;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|492| <<reprogram_counter>> if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ *
+ * 496         if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ * 497                                   (eventsel & pmu->raw_event_mask),
+ * 498                                   !(eventsel & ARCH_PERFMON_EVENTSEL_USR),
+ * 499                                   !(eventsel & ARCH_PERFMON_EVENTSEL_OS),
+ * 500                                   eventsel & ARCH_PERFMON_EVENTSEL_INT))
+ */
 static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 				 bool exclude_user, bool exclude_kernel,
 				 bool intr)
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 7caeb3d8d..634b8e95a 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -62,12 +62,30 @@ static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 	return pmu->counter_bitmask[pmc->type];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|298| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/pmu.c|324| <<pmc_write_counter>> pmc->counter += val - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/pmu.c|600| <<kvm_pmu_rdpmc>> *data = pmc_read_counter(pmc) & mask;
+ *   - arch/x86/kvm/svm/pmu.c|140| <<amd_pmu_get_msr>> msr_info->data = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|370| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|375| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ */
 static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 {
 	u64 counter, enabled, running;
 
 	counter = pmc->counter + pmc->emulated_counter;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|127| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|213| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|437| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/x86/kvm/pmu.h|72| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - include/uapi/linux/bpf.h|5739| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 *   - tools/include/uapi/linux/bpf.h|5739| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 */
 	if (pmc->perf_event && !pmc->is_paused)
 		counter += perf_event_read_value(pmc->perf_event,
 						 &enabled, &running);
@@ -138,6 +156,10 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 
 extern struct x86_pmu_capability kvm_pmu_cap;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9719| <<__kvm_x86_vendor_init>> kvm_init_pmu_capability(ops->pmu_ops);
+ */
 static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index e90b429c8..99406f0b0 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -2055,6 +2055,12 @@ static int npf_interception(struct kvm_vcpu *vcpu)
 	u64 fault_address = svm->vmcb->control.exit_info_2;
 	u64 error_code = svm->vmcb->control.exit_info_1;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4686| <<kvm_handle_page_fault>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+	 *   - arch/x86/kvm/svm/svm.c|2058| <<npf_interception>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+	 *   - arch/x86/kvm/vmx/vmx.c|5765| <<handle_ept_violation>> trace_kvm_page_fault(vcpu, gpa, exit_qualification);
+	 */
 	trace_kvm_page_fault(vcpu, fault_address, error_code);
 	return kvm_mmu_page_fault(vcpu, fault_address, error_code,
 			static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
@@ -5039,6 +5045,10 @@ static struct kvm_x86_ops svm_x86_ops __initdata = {
  * memory encryption support and override the default MMIO mask if
  * memory encryption is enabled.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5241| <<svm_hardware_setup>> svm_adjust_mmio_mask();
+ */
 static __init void svm_adjust_mmio_mask(void)
 {
 	unsigned int enc_bit, mask_bit;
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index 6329a3068..89fd23336 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -437,6 +437,11 @@ static void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,
 	vmcs12->guest_physical_address = fault->address;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|456| <<nested_ept_init_mmu_context>> nested_ept_new_eptp(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5899| <<nested_vmx_eptp_switching>> nested_ept_new_eptp(vcpu);
+ */
 static void nested_ept_new_eptp(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -448,6 +453,10 @@ static void nested_ept_new_eptp(struct kvm_vcpu *vcpu)
 				nested_ept_get_eptp(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2636| <<prepare_vmcs02>> nested_ept_init_mmu_context(vcpu);
+ */
 static void nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
 {
 	WARN_ON(mmu_is_nested(vcpu));
@@ -458,9 +467,22 @@ static void nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
 	vcpu->arch.mmu->inject_page_fault = nested_ept_inject_page_fault;
 	vcpu->arch.mmu->get_pdptr         = kvm_pdptr_read;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->walk_mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|6448| <<kvm_mmu_create>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|105| <<nested_svm_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|461| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|467| <<nested_ept_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 */
 	vcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4547| <<load_vmcs12_host_state>> nested_ept_uninit_mmu_context(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4697| <<nested_vmx_restore_host_state>> nested_ept_uninit_mmu_context(vcpu);
+ */
 static void nested_ept_uninit_mmu_context(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.mmu = &vcpu->arch.root_mmu;
@@ -2567,6 +2589,10 @@ static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
  * Returns 0 on success, 1 on failure. Invalid state exit qualification code
  * is assigned to entry_failure_code on failure.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3516| <<nested_vmx_enter_non_root_mode>> if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &entry_failure_code)) {
+ */
 static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			  bool from_vmentry,
 			  enum vm_entry_failure_code *entry_failure_code)
@@ -4654,6 +4680,10 @@ static inline u64 nested_vmx_get_vmcs01_guest_efer(struct vcpu_vmx *vmx)
 	return host_efer;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4951| <<nested_vmx_vmexit>> nested_vmx_restore_host_state(vcpu);
+ */
 static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -4759,6 +4789,25 @@ static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
 	nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/hyperv.c|228| <<vmx_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_vmx_vmexit(vcpu, HV_VMX_SYNTHETIC_EXIT_REASON_TRAP_AFTER_FLUSH, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|436| <<nested_ept_inject_page_fault>> nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification);
+ *   - arch/x86/kvm/vmx/nested.c|3953| <<nested_vmx_inject_exception_vmexit>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI, intr_info, exit_qual);
+ *   - arch/x86/kvm/vmx/nested.c|4129| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_INIT_SIGNAL, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4143| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_SIPI_SIGNAL, 0,
+ *   - arch/x86/kvm/vmx/nested.c|4180| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_MONITOR_TRAP_FLAG, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4201| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4217| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,
+ *   - arch/x86/kvm/vmx/nested.c|4234| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4959| <<nested_vmx_triple_fault>> nested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|5958| <<handle_vmfunc>> nested_vmx_vmexit(vcpu, vmx->exit_reason.full,
+ *   - arch/x86/kvm/vmx/nested.c|6446| <<nested_vmx_reflect_vmexit>> nested_vmx_vmexit(vcpu, exit_reason.full, exit_intr_info, exit_qual);
+ *   - arch/x86/kvm/vmx/nested.c|6577| <<vmx_leave_nested>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.h|29| <<vmx_leave_nested>> void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
+ *   - arch/x86/kvm/vmx/vmx.c|6471| <<__vmx_handle_exit>> nested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|8162| <<vmx_enter_smm>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ */
 /*
  * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
  * and modify vmcs12 to make it see what it would expect to see there if
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 88a4ff200..52578665f 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -1920,6 +1920,9 @@ static void vmx_write_tsc_offset(struct kvm_vcpu *vcpu)
 	vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.write_tsc_multiplier = vmx_write_tsc_multiplier()
+ */
 static void vmx_write_tsc_multiplier(struct kvm_vcpu *vcpu)
 {
 	vmcs_write64(TSC_MULTIPLIER, vcpu->arch.tsc_scaling_ratio);
@@ -4783,6 +4786,22 @@ static void init_vmcs(struct vcpu_vmx *vmx)
 	/* 22.2.1, 20.8.1 */
 	vm_entry_controls_set(vmx, vmx_vmentry_ctrl());
 
+	/*
+	 * 关于cr0/cr4的passthrough:
+	 * VM-execution control fields include guest/host masks and read shadows for the CR0 and CR4 registers. These
+	 * fields control executions of instructions that access those registers (including CLTS, LMSW, MOV CR, and SMSW).
+	 * They are 64 bits on processors that support Intel 64 architecture and 32 bits on processors that do not.
+	 *
+	 * In general, bits set to 1 in a guest/host mask correspond to bits "owned" by the host:
+	 *
+	 * 1. Guest attempts to set them (using CLTS, LMSW, or MOV to CR) to values differing from the corresponding bits
+	 * in the corresponding read shadow cause VM exits.
+	 *
+	 * 2. Guest reads (using MOV from CR or SMSW) return values for these bits from the corresponding read shadow.
+	 *
+	 * Bits cleared to 0 correspond to bits "owned" by the guest; guest
+	 * attempts to modify them succeed and guest reads return values for these bits from the control register itself.
+	 */
 	vmx->vcpu.arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
 	vmcs_writel(CR0_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr0_guest_owned_bits);
 
@@ -5180,6 +5199,10 @@ bool vmx_guest_inject_ac(struct kvm_vcpu *vcpu)
 	       (kvm_get_rflags(vcpu) & X86_EFLAGS_AC);
 }
 
+/*
+ * 在以下使用handle_exception_nmi():
+ *   - arch/x86/kvm/vmx/vmx.c|6110| <<global>> [EXIT_REASON_EXCEPTION_NMI] = handle_exception_nmi,
+ */
 static int handle_exception_nmi(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5762,6 +5785,12 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
 
 	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4686| <<kvm_handle_page_fault>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+	 *   - arch/x86/kvm/svm/svm.c|2058| <<npf_interception>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+	 *   - arch/x86/kvm/vmx/vmx.c|5765| <<handle_ept_violation>> trace_kvm_page_fault(vcpu, gpa, exit_qualification);
+	 */
 	trace_kvm_page_fault(vcpu, gpa, exit_qualification);
 
 	/* Is it a read fault? */
@@ -6408,6 +6437,10 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
  * The guest has exited.  See if we can fix it or if we need userspace
  * assistance.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6618| <<vmx_handle_exit>> int ret = __vmx_handle_exit(vcpu, exit_fastpath);
+ */
 static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7610,9 +7643,23 @@ static u8 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 	if (is_mmio)
 		return MTRR_TYPE_UNCACHABLE << VMX_EPT_MT_EPTE_SHIFT;
 
+	/*
+	 * 没有non-cohenrent设备的时候
+	 */
 	if (!kvm_arch_has_noncoherent_dma(vcpu->kvm))
 		return (MTRR_TYPE_WRBACK << VMX_EPT_MT_EPTE_SHIFT) | VMX_EPT_IPAT_BIT;
 
+	/*
+	 * 关于cr0.cd:
+	 * - If CR0.CD = 0, the memory type used for any such reference is the
+	 *   EPT paging-structure memory type, which is specified in bits 2:0 of
+	 *   the extended-page-table pointer (EPTP), a VM-execution control field
+	 *   (see Section 25.6.11). A value of 0 indicates the uncacheable type (UC),
+	 *   while a value of 6 indicates the write-back type (WB). Other values
+	 *   are reserved.
+	 *
+	 * - If CR0.CD = 1, the memory type used for any such reference is uncacheable (UC).
+	 */
 	if (kvm_read_cr0_bits(vcpu, X86_CR0_CD)) {
 		if (kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))
 			return MTRR_TYPE_WRBACK << VMX_EPT_MT_EPTE_SHIFT;
@@ -7621,6 +7668,9 @@ static u8 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 				VMX_EPT_IPAT_BIT;
 	}
 
+	/*
+	 * 只在此处调用
+	 */
 	return kvm_mtrr_get_guest_memory_type(vcpu, gfn) << VMX_EPT_MT_EPTE_SHIFT;
 }
 
@@ -8448,6 +8498,10 @@ static __init void vmx_setup_user_return_msrs(void)
 		kvm_add_user_return_msr(vmx_uret_msrs_list[i]);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8617| <<hardware_setup>> vmx_setup_me_spte_mask();
+ */
 static void __init vmx_setup_me_spte_mask(void)
 {
 	u64 me_mask = 0;
@@ -8475,6 +8529,12 @@ static void __init vmx_setup_me_spte_mask(void)
 
 static struct kvm_x86_init_ops vmx_init_ops __initdata;
 
+/*
+ * 在以下使用hardware_setup():
+ *   - arch/x86/kvm/svm/svm.c|5329| <<global>> .hardware_setup = svm_hardware_setup,
+ *   - arch/x86/kvm/vmx/vmx.c|8688| <<global>> .hardware_setup = hardware_setup,
+ *   - arch/x86/kvm/x86.c|9760| <<__kvm_x86_vendor_init>> r = ops->hardware_setup();
+ */
 static __init int hardware_setup(void)
 {
 	unsigned long host_bndcfgs;
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index e3b0985bb..defcdd29b 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -631,6 +631,19 @@ BUILD_CONTROLS_SHADOW(tertiary_exec, TERTIARY_VM_EXEC_CONTROL, 64)
 				(1 << VCPU_EXREG_EXIT_INFO_1) | \
 				(1 << VCPU_EXREG_EXIT_INFO_2))
 
+/*
+ * 在以下使用kvm_vcpu_arch->cr0_guest_owned_bits:
+ *   - arch/x86/kvm/kvm_cache_regs.h|154| <<kvm_read_cr0_bits>> if ((tmask & vcpu->arch.cr0_guest_owned_bits) &&
+ *   - arch/x86/kvm/vmx/nested.c|2622| <<prepare_vmcs02>> vcpu->arch.cr0_guest_owned_bits &= ~vmcs12->cr0_guest_host_mask;
+ *   - arch/x86/kvm/vmx/nested.c|2623| <<prepare_vmcs02>> vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu->arch.cr0_guest_owned_bits);
+ *   - arch/x86/kvm/vmx/nested.c|3773| <<vmcs12_guest_cr0>>  (vmcs_readl(GUEST_CR0) & vcpu->arch.cr0_guest_owned_bits) |
+ *   - arch/x86/kvm/vmx/nested.c|3776| <<vmcs12_guest_cr0>> vcpu->arch.cr0_guest_owned_bits));
+ *   - arch/x86/kvm/vmx/nested.c|4553| <<load_vmcs12_host_state>> vcpu->arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+ *   - arch/x86/kvm/vmx/nested.c|4708| <<nested_vmx_restore_host_state>> vcpu->arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+ *   - arch/x86/kvm/vmx/vmx.c|2485| <<vmx_cache_reg>> guest_owned_bits = vcpu->arch.cr0_guest_owned_bits;
+ *   - arch/x86/kvm/vmx/vmx.c|4786| <<init_vmcs>> vmx->vcpu.arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+ *   - arch/x86/kvm/vmx/vmx.c|4787| <<init_vmcs>> vmcs_writel(CR0_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr0_guest_owned_bits);
+ */
 static inline unsigned long vmx_l1_guest_owned_cr0_bits(void)
 {
 	unsigned long bits = KVM_POSSIBLE_CR0_GUEST_BITS;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index e02cc710f..ac2792409 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -132,6 +132,13 @@ static int kvm_vcpu_do_singlestep(struct kvm_vcpu *vcpu);
 static int __set_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2);
 static void __get_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2);
 
+/*
+ * 在以下使用vendor_module_lock:
+ *   - arch/x86/kvm/x86.c|9792| <<kvm_x86_vendor_init>> mutex_lock(&vendor_module_lock);
+ *   - arch/x86/kvm/x86.c|9794| <<kvm_x86_vendor_init>> mutex_unlock(&vendor_module_lock);
+ *   - arch/x86/kvm/x86.c|9828| <<kvm_x86_vendor_exit>> mutex_lock(&vendor_module_lock);
+ *   - arch/x86/kvm/x86.c|9830| <<kvm_x86_vendor_exit>> mutex_unlock(&vendor_module_lock);
+ */
 static DEFINE_MUTEX(vendor_module_lock);
 struct kvm_x86_ops kvm_x86_ops __read_mostly;
 
@@ -177,6 +184,64 @@ bool __read_mostly enable_vmware_backdoor = false;
 module_param(enable_vmware_backdoor, bool, 0444);
 EXPORT_SYMBOL_GPL(enable_vmware_backdoor);
 
+/*
+ * commit 6c86eedc206dd1f9d37a2796faa8e6f2278215d2
+ * Author: Wanpeng Li <wanpengli@tencent.com>
+ * Date:   Tue Apr 3 16:28:49 2018 -0700
+ *
+ * KVM: X86: Add Force Emulation Prefix for "emulate the next instruction"
+ *
+ * There is no easy way to force KVM to run an instruction through the emulator
+ * (by design as that will expose the x86 emulator as a significant attack-surface).
+ * However, we do wish to expose the x86 emulator in case we are testing it
+ * (e.g. via kvm-unit-tests). Therefore, this patch adds a "force emulation prefix"
+ * that is designed to raise #UD which KVM will trap and it's #UD exit-handler will
+ * match "force emulation prefix" to run instruction after prefix by the x86 emulator.
+ * To not expose the x86 emulator by default, we add a module parameter that should
+ * be off by default.
+ *
+ * A simple testcase here:
+ *
+ * #include <stdio.h>
+ * #include <string.h>
+ *
+ * #define HYPERVISOR_INFO 0x40000000
+ *
+ * #define CPUID(idx, eax, ebx, ecx, edx) \
+ *     asm volatile (\
+ *     "ud2a; .ascii \"kvm\"; cpuid" \
+ *     :"=b" (*ebx), "=a" (*eax), "=c" (*ecx), "=d" (*edx) \
+ *     :"0"(idx) );
+ *
+ * void main()
+ * {
+ *     unsigned int eax, ebx, ecx, edx;
+ *     char string[13];
+ *
+ *     CPUID(HYPERVISOR_INFO, &eax, &ebx, &ecx, &edx);
+ *     *(unsigned int *)(string + 0) = ebx;
+ *     *(unsigned int *)(string + 4) = ecx;
+ *     *(unsigned int *)(string + 8) = edx;
+ *
+ *     string[12] = 0;
+ *     if (strncmp(string, "KVMKVMKVM\0\0\0", 12) == 0)
+ *         printf("kvm guest\n");
+ *     else
+ *         printf("bare hardware\n");
+ * }
+ *
+ * Suggested-by: Andrew Cooper <andrew.cooper3@citrix.com>
+ * Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
+ * Reviewed-by: Liran Alon <liran.alon@oracle.com>
+ * Cc: Paolo Bonzini <pbonzini@redhat.com>
+ * Cc: Radim Krčmář <rkrcmar@redhat.com>
+ * Cc: Andrew Cooper <andrew.cooper3@citrix.com>
+ * Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
+ * Cc: Liran Alon <liran.alon@oracle.com>
+ * Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
+ * [Correctly handle usermode exits. - Paolo]
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ */
 /*
  * Flags to manipulate forced emulation behavior (any non-zero value will
  * enable forced emulation).
@@ -863,6 +928,18 @@ static inline u64 pdptr_rsvd_bits(struct kvm_vcpu *vcpu)
 /*
  * Load the pae pdptrs.  Return 1 if they are all valid, 0 otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|514| <<nested_svm_load_cr3>> CC(!load_pdptrs(vcpu, cr3)))
+ *   - arch/x86/kvm/svm/nested.c|1794| <<svm_get_nested_state_pages>> if (CC(!load_pdptrs(vcpu, vcpu->arch.cr3)))
+ *   - arch/x86/kvm/svm/svm.c|1623| <<svm_cache_reg>> load_pdptrs(vcpu, kvm_read_cr3(vcpu));
+ *   - arch/x86/kvm/vmx/nested.c|1143| <<nested_vmx_load_cr3>> CC(!load_pdptrs(vcpu, cr3))) {
+ *   - arch/x86/kvm/vmx/nested.c|3263| <<nested_get_vmcs12_pages>> if (CC(!load_pdptrs(vcpu, vcpu->arch.cr3)))
+ *   - arch/x86/kvm/x86.c|1009| <<kvm_set_cr0>> !load_pdptrs(vcpu, kvm_read_cr3(vcpu)))
+ *   - arch/x86/kvm/x86.c|1218| <<kvm_set_cr4>> && !load_pdptrs(vcpu, kvm_read_cr3(vcpu)))
+ *   - arch/x86/kvm/x86.c|1302| <<kvm_set_cr3>> if (is_pae_paging(vcpu) && !load_pdptrs(vcpu, cr3))
+ *   - arch/x86/kvm/x86.c|11863| <<__set_sregs_common>> load_pdptrs(vcpu, kvm_read_cr3(vcpu));
+ */
 int load_pdptrs(struct kvm_vcpu *vcpu, unsigned long cr3)
 {
 	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
@@ -926,6 +1003,11 @@ static bool kvm_is_valid_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 	return static_call(kvm_x86_is_valid_cr0)(vcpu, cr0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2714| <<cr_trap>> kvm_post_set_cr0(vcpu, old_value, new_value);
+ *   - arch/x86/kvm/x86.c|1006| <<kvm_set_cr0>> kvm_post_set_cr0(vcpu, old_cr0, cr0);
+ */
 void kvm_post_set_cr0(struct kvm_vcpu *vcpu, unsigned long old_cr0, unsigned long cr0)
 {
 	/*
@@ -941,6 +1023,14 @@ void kvm_post_set_cr0(struct kvm_vcpu *vcpu, unsigned long old_cr0, unsigned lon
 			return;
 
 		if (tdp_enabled) {
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/mmu/mmu.c|5772| <<kvm_mmu_reset_context>> kvm_init_mmu(vcpu);
+			 *   - arch/x86/kvm/svm/nested.c|520| <<nested_svm_load_cr3>> kvm_init_mmu(vcpu);
+			 *   - arch/x86/kvm/vmx/nested.c|1138| <<nested_vmx_load_cr3>> kvm_init_mmu(vcpu);
+			 *   - arch/x86/kvm/x86.c|956| <<kvm_post_set_cr0>> kvm_init_mmu(vcpu);
+			 *   - arch/x86/kvm/x86.c|12206| <<kvm_arch_vcpu_create>> kvm_init_mmu(vcpu);
+			 */
 			kvm_init_mmu(vcpu);
 			return;
 		}
@@ -2261,12 +2351,29 @@ struct pvclock_gtod_data {
 	struct pvclock_clock clock; /* extract of a clocksource struct */
 	struct pvclock_clock raw_clock; /* extract of a clocksource struct */
 
+	/*
+	 * 在以下使用pvclock_gtod_data->offs_boot:
+	 *   - arch/x86/kvm/x86.c|2327| <<update_pvclock_gtod>> vdata->offs_boot = tk->offs_boot;
+	 *   - arch/x86/kvm/x86.c|2335| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+	 *   - arch/x86/kvm/x86.c|2904| <<do_monotonic_raw>> ns += ktime_to_ns(ktime_add(gtod->raw_clock.offset, gtod->offs_boot));
+	 */
 	ktime_t		offs_boot;
+	/*
+	 * 在以下使用pvclock_gtod_data->wall_time_sec:
+	 *   - arch/x86/kvm/x86.c|2325| <<update_pvclock_gtod>> vdata->wall_time_sec = tk->xtime_sec;
+	 *   - arch/x86/kvm/x86.c|2920| <<do_realtime>> ts->tv_sec = gtod->wall_time_sec;
+	 */
 	u64		wall_time_sec;
 };
 
 static struct pvclock_gtod_data pvclock_gtod_data;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9686| <<pvclock_gtod_notify>> update_pvclock_gtod(tk);
+ *
+ * 这个是经常被调用的
+ */
 static void update_pvclock_gtod(struct timekeeper *tk)
 {
 	struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
@@ -2310,6 +2417,22 @@ static s64 get_kvmclock_base_ns(void)
 }
 #endif
 
+/*
+ * - CLOCK_REALTIME clock gives the time passed since January 1, 1970. This
+ *   clock is affected by NTP adjustments and can jump forward and backward
+ *   when a system administrator adjus    ts system time.
+ * - CLOCK_MONOTONIC clock gives the time since a fixed starting point—usually
+ *   since you booted the system. This clock is affected by NTP, but it can't
+ *   jump backward.
+ * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this
+ *   clock is not affected by NTP adjustments.
+ * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but
+ *   less-accurate variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ */
 static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 {
 	int version;
@@ -2351,6 +2474,13 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3984| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME_NEW)>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|3990| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME)>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ *
+ * 更多是激活kvmclock
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
@@ -2363,7 +2493,20 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 		ka->boot_vcpu_runs_old_kvmclock = old_msr;
 	}
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> gpa_t time;
+	 */
 	vcpu->arch.time = system_time;
+	/*
+	 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|2399| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5035| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10870| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+	 *
+	 * 处理函数: kvm_gen_kvmclock_update(vcpu);
+	 */
 	kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 
 	/* we verify if the enable bit is set... */
@@ -2382,6 +2525,17 @@ static uint32_t div_frac(uint32_t dividend, uint32_t divisor)
 	return dividend;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2583| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC, &vcpu->arch.virtual_tsc_shift,
+ *                                                        &vcpu->arch.virtual_tsc_mult);
+ *   - arch/x86/kvm/x86.c|3274| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL, &hv_clock.tsc_shift,
+ *                                                        &hv_clock.tsc_to_system_mul);
+ *   - arch/x86/kvm/x86.c|3462| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL, &vcpu->hv_clock.tsc_shift,
+ *                                                         &vcpu->hv_clock.tsc_to_system_mul);
+ *   - arch/x86/kvm/x86.c|3561| <<kvm_get_wall_clock_epoch>> kvm_get_time_scale(NSEC_PER_SEC, local_tsc_khz * NSEC_PER_USEC, &hv_clock.tsc_shift,
+ *                                                         &hv_clock.tsc_to_system_mul);
+ */
 static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 			       s8 *pshift, u32 *pmultiplier)
 {
@@ -2411,12 +2565,23 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用kvm_guest_has_master_clock:
+ *   - arch/x86/kvm/x86.c|3020| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+ *   - arch/x86/kvm/x86.c|9659| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+ *   - arch/x86/kvm/x86.c|9694| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+ */
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
 static unsigned long max_tsc_khz;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2521| <<kvm_set_tsc_khz>> thresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);
+ *   - arch/x86/kvm/x86.c|2522| <<kvm_set_tsc_khz>> thresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);
+ */
 static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 {
 	u64 v = (u64)khz * (1000000 + ppm);
@@ -2426,6 +2591,10 @@ static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2528| <<kvm_set_tsc_khz>> return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
+ */
 static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 {
 	u64 ratio;
@@ -2462,6 +2631,11 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6132| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|12296| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2496,8 +2670,18 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3259| <<kvm_guest_time_update>> u64 tsc = compute_guest_tsc(v, kernel_ns);
+ */
 static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_shift:
+	 *   - arch/x86/kvm/x86.c|2595| <<kvm_set_tsc_khz>>  kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC, &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2623| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec, vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|451| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 */
 	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
 				      vcpu->arch.virtual_tsc_mult,
 				      vcpu->arch.virtual_tsc_shift);
@@ -2512,6 +2696,10 @@ static inline bool gtod_is_based_on_tsc(int mode)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2746| <<__kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu, !matched);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu, bool new_generation)
 {
 #ifdef CONFIG_X86_64
@@ -2527,6 +2715,18 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu, bool new_generation)
 				 atomic_read(&vcpu->kvm->online_vcpus)) &&
 				gtod_is_based_on_tsc(gtod->clock.vclock_mode);
 
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ 125  *   - arch/x86/kvm/hyperv.c|1439| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ 126  *   - arch/x86/kvm/x86.c|2393| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ 127  *   - arch/x86/kvm/x86.c|2570| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ 128  *   - arch/x86/kvm/x86.c|9658| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ 129  *   - arch/x86/kvm/x86.c|10868| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ 130  *   - arch/x86/kvm/x86.c|12615| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ 131  *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ 132  *
+ 133  * 处理函数: kvm_update_masterclock(vcpu->kvm);
+	 */
 	/*
 	 * Request a masterclock update if the masterclock needs to be toggled
 	 * on/off, or when starting a new generation and the masterclock is
@@ -2667,6 +2867,11 @@ static inline bool kvm_check_tsc_unstable(void)
  * offset for the vcpu and tracks the TSC matching generation that the vcpu
  * participates in.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2815| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched);
+ *   - arch/x86/kvm/x86.c|5739| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched);
+ */
 static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 				  u64 ns, bool matched)
 {
@@ -2714,6 +2919,11 @@ static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 	kvm_track_tsc_matching(vcpu, !matched);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3940| <<kvm_set_msr_common(MSR_IA32_TSC)>> kvm_synchronize_tsc(vcpu, &data);
+ *   - arch/x86/kvm/x86.c|12326| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, NULL);
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 *user_value)
 {
 	u64 data = user_value ? *user_value : 0;
@@ -2961,6 +3171,13 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3158| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|7040| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|9613| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|12844| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2984,6 +3201,12 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 				&& !ka->backwards_tsc_observed
 				&& !ka->boot_vcpu_runs_old_kvmclock;
 
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|3020| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|9659| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|9694| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	if (ka->use_master_clock)
 		atomic_set(&kvm_guest_has_master_clock, 1);
 
@@ -2993,17 +3216,32 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3132| <<kvm_start_pvclock_update>> kvm_make_mclock_inprogress_request(kvm);
+ *   - arch/x86/kvm/x86.c|9599| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+ */
 static void kvm_make_mclock_inprogress_request(struct kvm *kvm)
 {
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3159| <<kvm_start_pvclock_update>> __kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9741| <<kvm_hyperv_tsc_notifier>> __kvm_start_pvclock_update(kvm);
+ */
 static void __kvm_start_pvclock_update(struct kvm *kvm)
 {
 	raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
 	write_seqcount_begin(&kvm->arch.pvclock_sc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3212| <<kvm_update_masterclock>> kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|7168| <<kvm_vm_ioctl_set_clock>> kvm_start_pvclock_update(kvm);
+ */
 static void kvm_start_pvclock_update(struct kvm *kvm)
 {
 	kvm_make_mclock_inprogress_request(kvm);
@@ -3012,6 +3250,12 @@ static void kvm_start_pvclock_update(struct kvm *kvm)
 	__kvm_start_pvclock_update(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3214| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|7193| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9743| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+ */
 static void kvm_end_pvclock_update(struct kvm *kvm)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -3020,6 +3264,25 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 
 	write_seqcount_end(&ka->pvclock_sc);
 	raw_spin_unlock_irq(&ka->tsc_write_lock);
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|3147| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3329| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3501| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3518| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|4004| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5114| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5769| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9674| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11009| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|11345| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|12698| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|760| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|775| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|1194| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+	 *
+	 * 处理的函数: kvm_guest_time_update(vcpu);
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
@@ -3028,6 +3291,18 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 		kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
 }
 
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1439| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2393| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2570| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9658| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10868| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|12615| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * 处理函数: kvm_update_masterclock(vcpu->kvm);
+ */
 static void kvm_update_masterclock(struct kvm *kvm)
 {
 	kvm_hv_request_tsc_page_update(kvm);
@@ -3052,6 +3327,19 @@ static unsigned long get_cpu_tsc_khz(void)
 		return __this_cpu_read(cpu_tsc_khz);
 }
 
+/*
+ * struct kvm_clock_data {
+ *     __u64 clock;
+ *     __u32 flags;
+ *     __u32 pad0;
+ *     __u64 realtime;
+ *     __u64 host_tsc;
+ *     __u32 pad[4];
+ * };
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|3292| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ */
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
@@ -3077,6 +3365,17 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 		data->flags |= KVM_CLOCK_TSC_STABLE;
 		hv_clock.tsc_timestamp = ka->master_cycle_now;
 		hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|2583| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC, &vcpu->arch.virtual_tsc_shift,
+		 *                                                        &vcpu->arch.virtual_tsc_mult);
+		 *   - arch/x86/kvm/x86.c|3274| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL, &hv_clock.tsc_shift,
+		 *                                                        &hv_clock.tsc_to_system_mul);
+		 *   - arch/x86/kvm/x86.c|3462| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL, &vcpu->hv_clock.tsc_shift,
+		 *                                                         &vcpu->hv_clock.tsc_to_system_mul);
+		 *   - arch/x86/kvm/x86.c|3561| <<kvm_get_wall_clock_epoch>> kvm_get_time_scale(NSEC_PER_SEC, local_tsc_khz * NSEC_PER_USEC, &hv_clock.tsc_shift,
+		 *                                                         &hv_clock.tsc_to_system_mul);
+		 */
 		kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
 				   &hv_clock.tsc_shift,
 				   &hv_clock.tsc_to_system_mul);
@@ -3088,6 +3387,20 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	put_cpu();
 }
 
+/*
+ * struct kvm_clock_data {
+ *     __u64 clock;
+ *     __u32 flags;
+ *     __u32 pad0;
+ *     __u64 realtime;
+ *     __u64 host_tsc;
+ *     __u32 pad[4];
+ * };
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|3300| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+ *   - arch/x86/kvm/x86.c|7160| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+ */
 static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -3099,6 +3412,17 @@ static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	} while (read_seqcount_retry(&ka->pvclock_sc, seq));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|579| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|3650| <<kvm_get_wall_clock_epoch>> return ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|468| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|856| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|897| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|949| <<kvm_xen_vcpu_set_attr>> get_kvmclock_ns(vcpu->kvm));
+ *   - arch/x86/kvm/xen.c|1434| <<kvm_xen_hcall_vcpu_op>> delta = oneshot.timeout_abs_ns - get_kvmclock_ns(vcpu->kvm);
+ *   - arch/x86/kvm/xen.c|1459| <<kvm_xen_hcall_set_timer_op>> uint64_t guest_now = get_kvmclock_ns(vcpu->kvm);
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_clock_data data;
@@ -3107,6 +3431,12 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	return data.clock;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3385| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0, false);
+ *   - arch/x86/kvm/x86.c|3388| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_info_cache, offsetof(struct compat_vcpu_info, time), xen_pvclock_tsc_unstable);
+ *   - arch/x86/kvm/x86.c|3392| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_time_info_cache, 0, xen_pvclock_tsc_unstable);
+ */
 static void kvm_setup_guest_pvclock(struct kvm_vcpu *v,
 				    struct gfn_to_pfn_cache *gpc,
 				    unsigned int offset,
@@ -3161,6 +3491,25 @@ static void kvm_setup_guest_pvclock(struct kvm_vcpu *v,
 	trace_kvm_pvclock_update(v->vcpu_id, &vcpu->hv_clock);
 }
 
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|3147| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3329| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3501| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3518| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|4004| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5114| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5769| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9674| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11009| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|11345| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|12698| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|760| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|775| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|1194| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *
+ * 处理的函数: kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -3240,6 +3589,17 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 					    v->arch.l1_tsc_scaling_ratio);
 
 	if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|2583| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC, &vcpu->arch.virtual_tsc_shift,
+		 *                                                        &vcpu->arch.virtual_tsc_mult);
+		 *   - arch/x86/kvm/x86.c|3274| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL, &hv_clock.tsc_shift,
+		 *                                                        &hv_clock.tsc_to_system_mul);
+		 *   - arch/x86/kvm/x86.c|3462| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL, &vcpu->hv_clock.tsc_shift,
+		 *                                                         &vcpu->hv_clock.tsc_to_system_mul);
+		 *   - arch/x86/kvm/x86.c|3561| <<kvm_get_wall_clock_epoch>> kvm_get_time_scale(NSEC_PER_SEC, local_tsc_khz * NSEC_PER_USEC, &hv_clock.tsc_shift,
+		 *                                                         &hv_clock.tsc_to_system_mul);
+		 */
 		kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
 				   &vcpu->hv_clock.tsc_shift,
 				   &vcpu->hv_clock.tsc_to_system_mul);
@@ -3292,6 +3652,11 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
  * Fall back to using their values at slightly different moments by
  * calling ktime_get_real_ns() and get_kvmclock_ns() separately.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2401| <<kvm_write_wall_clock>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+ *   - arch/x86/kvm/xen.c|62| <<kvm_xen_shared_info_init>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+ */
 uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -3339,6 +3704,17 @@ uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 	 * since 1970-01-01.
 	 */
 	if (local_tsc_khz) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|2583| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC, &vcpu->arch.virtual_tsc_shift,
+		 *                                                        &vcpu->arch.virtual_tsc_mult);
+		 *   - arch/x86/kvm/x86.c|3274| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL, &hv_clock.tsc_shift,
+		 *                                                        &hv_clock.tsc_to_system_mul);
+		 *   - arch/x86/kvm/x86.c|3462| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL, &vcpu->hv_clock.tsc_shift,
+		 *                                                         &vcpu->hv_clock.tsc_to_system_mul);
+		 *   - arch/x86/kvm/x86.c|3561| <<kvm_get_wall_clock_epoch>> kvm_get_time_scale(NSEC_PER_SEC, local_tsc_khz * NSEC_PER_USEC, &hv_clock.tsc_shift,
+		 *                                                         &hv_clock.tsc_to_system_mul);
+		 */
 		kvm_get_time_scale(NSEC_PER_SEC, local_tsc_khz * NSEC_PER_USEC,
 				   &hv_clock.tsc_shift,
 				   &hv_clock.tsc_to_system_mul);
@@ -3365,6 +3741,18 @@ uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 
 #define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)
 
+/*
+ * 在以下使用kvmclock_update_work:
+ *   - arch/x86/include/asm/kvm_host.h|1585| <<global>> struct delayed_work kvmclock_update_work;
+ *   - arch/x86/kvm/x86.c|3405| <<kvmclock_update_fn>> struct kvm_arch *ka = container_of(dwork, struct kvm_arch, kvmclock_update_work);
+ *   - arch/x86/kvm/x86.c|3420| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+ *   - arch/x86/kvm/x86.c|3433| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+ *   - arch/x86/kvm/x86.c|12719| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ *   - arch/x86/kvm/x86.c|12761| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+ *
+ * 在以下使用kvmclock_update_fn():
+ *   - arch/x86/kvm/x86.c|12856| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ */
 static void kvmclock_update_fn(struct work_struct *work)
 {
 	unsigned long i;
@@ -3374,12 +3762,39 @@ static void kvmclock_update_fn(struct work_struct *work)
 	struct kvm *kvm = container_of(ka, struct kvm, arch);
 	struct kvm_vcpu *vcpu;
 
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|3147| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3329| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3501| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3518| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|4004| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5114| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5769| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9674| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11009| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|11345| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|12698| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|760| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|775| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|1194| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+	 *
+	 * 处理的函数: kvm_guest_time_update(vcpu);
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm) {
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		kvm_vcpu_kick(vcpu);
 	}
 }
 
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2399| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5035| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10870| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * 处理函数: kvm_gen_kvmclock_update(vcpu);
+ */
 static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 {
 	struct kvm *kvm = v->kvm;
@@ -3391,6 +3806,10 @@ static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 
 #define KVMCLOCK_SYNC_PERIOD (300 * HZ)
 
+/*
+ * 在以下使用kvmclock_sync_fn():
+ *   - arch/x86/kvm/x86.c|12720| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+ */
 static void kvmclock_sync_fn(struct work_struct *work)
 {
 	struct delayed_work *dwork = to_delayed_work(work);
@@ -3398,7 +3817,25 @@ static void kvmclock_sync_fn(struct work_struct *work)
 					   kvmclock_sync_work);
 	struct kvm *kvm = container_of(ka, struct kvm, arch);
 
+	/*
+	 * 在以下使用kvmclock_update_work:
+	 *   - arch/x86/include/asm/kvm_host.h|1585| <<global>> struct delayed_work kvmclock_update_work;
+	 *   - arch/x86/kvm/x86.c|3405| <<kvmclock_update_fn>> struct kvm_arch *ka = container_of(dwork, struct kvm_arch, kvmclock_update_work);
+	 *   - arch/x86/kvm/x86.c|3420| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|3433| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|12719| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|12761| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	/*
+	 * 在以下使用kvmclock_sync_work:
+	 *   - arch/x86/include/asm/kvm_host.h|1586| <<global>> struct delayed_work kvmclock_sync_work;
+	 *   - arch/x86/kvm/x86.c|3430| <<kvmclock_sync_fn>> struct kvm_arch *ka = container_of(dwork, struct kvm_arch, kvmclock_sync_work);
+	 *   - arch/x86/kvm/x86.c|3434| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|12335| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|12720| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|12760| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 					KVMCLOCK_SYNC_PERIOD);
 }
@@ -7374,6 +7811,10 @@ static void kvm_probe_msr_to_save(u32 msr_index)
 	msrs_to_save[num_msrs_to_save++] = msr_index;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9773| <<__kvm_x86_vendor_init>> kvm_init_msr_lists();
+ */
 static void kvm_init_msr_lists(void)
 {
 	unsigned i;
@@ -7690,6 +8131,11 @@ int handle_ud(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(handle_ud);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7751| <<vcpu_mmio_gva_to_gpa>> return vcpu_is_mmio_gpa(vcpu, gva, *gpa, write);
+ *   - arch/x86/kvm/x86.c|7860| <<emulator_read_write_onepage>> ret = vcpu_is_mmio_gpa(vcpu, addr, gpa, write);
+ */
 static int vcpu_is_mmio_gpa(struct kvm_vcpu *vcpu, unsigned long gva,
 			    gpa_t gpa, bool write)
 {
@@ -7697,7 +8143,17 @@ static int vcpu_is_mmio_gpa(struct kvm_vcpu *vcpu, unsigned long gva,
 	if ((gpa & PAGE_MASK) == APIC_DEFAULT_PHYS_BASE)
 		return 1;
 
+	/*
+	 * 如果下面的满足返回true, 否则false:
+	 * 1. vcpu_arch缓存的mmio_gen和memslots的相等
+	 * 2. vcpu->arch.mmio_gfn和参数的gpa互相match
+	 */
 	if (vcpu_match_mmio_gpa(vcpu, gpa)) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|7717| <<vcpu_is_mmio_gpa>> trace_vcpu_match_mmio(gva, gpa, write, true);
+		 *   - arch/x86/kvm/x86.c|7742| <<vcpu_mmio_gva_to_gpa>> trace_vcpu_match_mmio(gva, *gpa, write, false);
+		 */
 		trace_vcpu_match_mmio(gva, gpa, write, true);
 		return 1;
 	}
@@ -7705,14 +8161,35 @@ static int vcpu_is_mmio_gpa(struct kvm_vcpu *vcpu, unsigned long gva,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7862| <<emulator_read_write_onepage>> ret = vcpu_mmio_gva_to_gpa(vcpu, addr, &gpa, exception, write);
+ */
 static int vcpu_mmio_gva_to_gpa(struct kvm_vcpu *vcpu, unsigned long gva,
 				gpa_t *gpa, struct x86_exception *exception,
 				bool write)
 {
 	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
+	/*
+	 * 如果cpl是3, access要有PFERR_USER_MASK
+	 * 如果是写操作, access要有PFERR_WRITE_MASK
+	 */
 	u64 access = ((static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0)
 		| (write ? PFERR_WRITE_MASK : 0);
 
+	/*
+	 * 在以下使用mmio_access:
+	 *   - arch/x86/kvm/x86.c|7739| <<vcpu_mmio_gva_to_gpa>> if (vcpu_match_mmio_gva(vcpu, gva) &&
+	 *                                         (!is_paging(vcpu) || !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access))) {
+	 *   - arch/x86/kvm/x86.h|244| <<vcpu_cache_mmio_info>> vcpu->arch.mmio_access = access;
+	 *
+	 * 下面的条件:
+	 * 1. vcpu_match_mmio_gva(vcpu, gva), 和(and)
+	 * 2. !is_paging(vcpu), or
+	 *    !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access)
+	 *
+	 * 我们应该是希望permission_fault()返回0
+	 */
 	/*
 	 * currently PKRU is only applied to ept enabled guest so
 	 * there is no pkey in EPT page table for L1 guest or EPT
@@ -7723,6 +8200,11 @@ static int vcpu_mmio_gva_to_gpa(struct kvm_vcpu *vcpu, unsigned long gva,
 			      vcpu->arch.mmio_access, 0, access))) {
 		*gpa = vcpu->arch.mmio_gfn << PAGE_SHIFT |
 					(gva & (PAGE_SIZE - 1));
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|7717| <<vcpu_is_mmio_gpa>> trace_vcpu_match_mmio(gva, gpa, write, true);
+		 *   - arch/x86/kvm/x86.c|7742| <<vcpu_mmio_gva_to_gpa>> trace_vcpu_match_mmio(gva, *gpa, write, false);
+		 */
 		trace_vcpu_match_mmio(gva, *gpa, write, false);
 		return 1;
 	}
@@ -7819,6 +8301,11 @@ static const struct read_write_emulator_ops write_emultor = {
 	.write = true,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7910| <<emulator_read_write>> rc = emulator_read_write_onepage(addr, val, now, exception, vcpu, ops);
+ *   - arch/x86/kvm/x86.c|7922| <<emulator_read_write>> rc = emulator_read_write_onepage(addr, val, bytes, exception, vcpu, ops);
+ */
 static int emulator_read_write_onepage(unsigned long addr, void *val,
 				       unsigned int bytes,
 				       struct x86_exception *exception,
@@ -7870,6 +8357,11 @@ static int emulator_read_write_onepage(unsigned long addr, void *val,
 	return X86EMUL_CONTINUE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7995| <<emulator_read_emulated>> return emulator_read_write(ctxt, addr, val, bytes, exception, &read_emultor);
+ *   - arch/x86/kvm/x86.c|8005| <<emulator_write_emulated>> return emulator_read_write(ctxt, addr, (void *)val, bytes, exception, &write_emultor);
+ */
 static int emulator_read_write(struct x86_emulate_ctxt *ctxt,
 			unsigned long addr,
 			void *val, unsigned int bytes,
@@ -7934,6 +8426,12 @@ static int emulator_read_emulated(struct x86_emulate_ctxt *ctxt,
 				   exception, &read_emultor);
 }
 
+/*
+ * 在以下使用emulator_write_emulated():
+ *   - arch/x86/kvm/x86.c|8584| <<global>> .write_emulated = emulator_write_emulated,
+ *   - arch/x86/kvm/x86.c|8092| <<emulator_cmpxchg_emulated>> return emulator_write_emulated(ctxt, addr, new, bytes, exception);
+ *   - arch/x86/kvm/x86.c|10239| <<emulator_fix_hypercall>> return emulator_write_emulated(ctxt, rip, instruction, 3,
+ */
 static int emulator_write_emulated(struct x86_emulate_ctxt *ctxt,
 			    unsigned long addr,
 			    const void *val,
@@ -9068,6 +9566,12 @@ int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 }
 EXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6433| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+ *   - arch/x86/kvm/x86.c|9334| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|9341| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len)
 {
@@ -9554,6 +10058,9 @@ static void pvclock_gtod_update_fn(struct work_struct *work)
 	unsigned long i;
 
 	mutex_lock(&kvm_lock);
+	/*
+	 * 处理函数: kvm_update_masterclock(vcpu->kvm);
+	 */
 	list_for_each_entry(kvm, &vm_list, vm_list)
 		kvm_for_each_vcpu(i, vcpu, kvm)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -9561,6 +10068,12 @@ static void pvclock_gtod_update_fn(struct work_struct *work)
 	mutex_unlock(&kvm_lock);
 }
 
+/*
+ * 在以下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|9663| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|9672| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|9949| <<kvm_x86_vendor_exit>> cancel_work_sync(&pvclock_gtod_work);
+ */
 static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
 
 /*
@@ -9568,16 +10081,31 @@ static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
  * region to prevent possible deadlocks against time accessors which
  * are invoked with work related locks held.
  */
+/*
+ * 在以下使用pvclock_irq_work:
+ *   - arch/x86/kvm/x86.c|9675| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *   - arch/x86/kvm/x86.c|9695| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+ *   - arch/x86/kvm/x86.c|9948| <<kvm_x86_vendor_exit>> irq_work_sync(&pvclock_irq_work);
+ */
 static void pvclock_irq_work_fn(struct irq_work *w)
 {
 	queue_work(system_long_wq, &pvclock_gtod_work);
 }
 
+/*
+ * 在以下使用pvclock_irq_work:
+ *   - arch/x86/kvm/x86.c|9675| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *   - arch/x86/kvm/x86.c|9695| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+ *   - arch/x86/kvm/x86.c|9948| <<kvm_x86_vendor_exit>> irq_work_sync(&pvclock_irq_work);
+ */
 static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
 
 /*
  * Notification about pvclock gtod data update.
  */
+/*
+ * struct notifier_block pvclock_gtod_notifier.notifier_call = pvclock_gtod_notif()
+ */
 static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 			       void *priv)
 {
@@ -9586,6 +10114,16 @@ static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 
 	update_pvclock_gtod(tk);
 
+	/*
+	 * 在以下使用pvclock_irq_work:
+	 *   - arch/x86/kvm/x86.c|9675| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+	 *   - arch/x86/kvm/x86.c|9695| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+	 *   - arch/x86/kvm/x86.c|9948| <<kvm_x86_vendor_exit>> irq_work_sync(&pvclock_irq_work);
+	 *
+	 * 触发的两个条件:
+	 * 1. 不是tsc
+	 * 2. 但是有用master clock的
+	 */
 	/*
 	 * Disable master clock if host does not trust, or does not use,
 	 * TSC based clocksource. Delegate queue_work() to irq_work as
@@ -9602,6 +10140,10 @@ static struct notifier_block pvclock_gtod_notifier = {
 };
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9728| <<__kvm_x86_vendor_init>> kvm_ops_update(ops);
+ */
 static inline void kvm_ops_update(struct kvm_x86_init_ops *ops)
 {
 	memcpy(&kvm_x86_ops, ops->runtime_ops, sizeof(kvm_x86_ops));
@@ -9645,6 +10187,10 @@ static void kvm_x86_check_cpu_compat(void *ret)
 	*(int *)ret = kvm_x86_check_processor_compatibility();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9793| <<kvm_x86_vendor_init>> r = __kvm_x86_vendor_init(ops);
+ */
 static int __kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 {
 	u64 host_pat;
@@ -9689,6 +10235,23 @@ static int __kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 		return -ENOMEM;
 	}
 
+	/*
+	 * 210 //
+	 * 211 // Restoring the host value for MSRs that are only consumed when running in
+	 * 212 // usermode, e.g. SYSCALL MSRs and TSC_AUX, can be deferred until the CPU
+	 * 213 // returns to userspace, i.e. the kernel can run with the guest's value.
+	 * 214 //
+	 * 215 #define KVM_MAX_NR_USER_RETURN_MSRS 16
+	 * 216
+	 * 217 struct kvm_user_return_msrs {
+	 * 218         struct user_return_notifier urn;
+	 * 219         bool registered;
+	 * 220         struct kvm_user_return_msr_values {
+	 * 221                 u64 host;
+	 * 222                 u64 curr;
+	 * 223         } values[KVM_MAX_NR_USER_RETURN_MSRS];
+	 * 224 };
+	 */
 	user_return_msrs = alloc_percpu(struct kvm_user_return_msrs);
 	if (!user_return_msrs) {
 		pr_err("failed to allocate percpu kvm_user_return_msrs\n");
@@ -9780,10 +10343,22 @@ static int __kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 	return r;
 }
 
+/*
+ * called by:
+ *   -  arch/x86/kvm/svm/svm.c|5351| <<svm_init>> r = kvm_x86_vendor_init(&svm_init_ops);
+ *   - arch/x86/kvm/vmx/vmx.c|8736| <<vmx_init>> r = kvm_x86_vendor_init(&vmx_init_ops);
+ */
 int kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 {
 	int r;
 
+	/*
+	 * 在以下使用vendor_module_lock:
+	 *   - arch/x86/kvm/x86.c|9792| <<kvm_x86_vendor_init>> mutex_lock(&vendor_module_lock);
+	 *   - arch/x86/kvm/x86.c|9794| <<kvm_x86_vendor_init>> mutex_unlock(&vendor_module_lock);
+	 *   - arch/x86/kvm/x86.c|9828| <<kvm_x86_vendor_exit>> mutex_lock(&vendor_module_lock);
+	 *   - arch/x86/kvm/x86.c|9830| <<kvm_x86_vendor_exit>> mutex_unlock(&vendor_module_lock);
+	 */
 	mutex_lock(&vendor_module_lock);
 	r = __kvm_x86_vendor_init(ops);
 	mutex_unlock(&vendor_module_lock);
@@ -11771,6 +12346,11 @@ static int __set_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11968| <<kvm_arch_vcpu_ioctl_set_sregs>> ret = __set_sregs(vcpu, sregs);
+ *   - arch/x86/kvm/x86.c|12151| <<sync_regs>> if (__set_sregs(vcpu, &sregs))
+ */
 static int __set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
 {
 	int pending_vec, max_bits;
@@ -12038,6 +12618,10 @@ static int sync_regs(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4343| <<kvm_vm_ioctl_create_vcpu>> r = kvm_arch_vcpu_precreate(kvm, id);
+ */
 int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 {
 	if (kvm_check_tsc_unstable() && kvm->created_vcpus)
@@ -12053,6 +12637,10 @@ int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 	return static_call(kvm_x86_vcpu_precreate)(kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4342| <<kvm_vm_ioctl_create_vcpu>> r = kvm_arch_vcpu_create(vcpu);
+ */
 int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 {
 	struct page *page;
@@ -12380,6 +12968,10 @@ void kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_deliver_sipi_vector);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5571| <<__hardware_enable_nolock>> if (kvm_arch_hardware_enable()) {
+ */
 int kvm_arch_hardware_enable(void)
 {
 	struct kvm *kvm;
@@ -12396,6 +12988,10 @@ int kvm_arch_hardware_enable(void)
 	if (ret)
 		return ret;
 
+	/*
+	 * vmx_hardware_enable()
+	 * svm_hardware_enable()
+	 */
 	ret = static_call(kvm_x86_hardware_enable)();
 	if (ret != 0)
 		return ret;
@@ -12602,6 +13198,10 @@ static void kvm_unload_vcpu_mmus(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1356| <<kvm_destroy_vm>> kvm_arch_sync_events(kvm);
+ */
 void kvm_arch_sync_events(struct kvm *kvm)
 {
 	cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
@@ -12765,6 +13365,31 @@ int memslot_rmap_alloc(struct kvm_memory_slot *slot, unsigned long npages)
 	return 0;
 }
 
+/*
+ * 580 struct kvm_memory_slot {
+ * 581         struct hlist_node id_node[2];
+ * 582         struct interval_tree_node hva_node[2];
+ * 583         struct rb_node gfn_node[2];
+ * 584         gfn_t base_gfn;
+ * 585         unsigned long npages;
+ * 586         unsigned long *dirty_bitmap;
+ * 587         struct kvm_arch_memory_slot arch;
+ * 588         unsigned long userspace_addr;
+ * 589         u32 flags;
+ * 590         short id;
+ * 591         u16 as_id;
+ * 592
+ * 593 #ifdef CONFIG_KVM_PRIVATE_MEM
+ * 594         struct {
+ * 595                 struct file __rcu *file;
+ * 596                 pgoff_t pgoff;
+ * 597         } gmem;
+ * 598 #endif
+ * 599 };
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|12874| <<kvm_arch_prepare_memory_region>> return kvm_alloc_memslot_metadata(kvm, new);
+ */
 static int kvm_alloc_memslot_metadata(struct kvm *kvm,
 				      struct kvm_memory_slot *slot)
 {
@@ -12784,20 +13409,66 @@ static int kvm_alloc_memslot_metadata(struct kvm *kvm,
 			return r;
 	}
 
+	/*
+	 * 就是1和2
+	 */
 	for (i = 1; i < KVM_NR_PAGE_SIZES; ++i) {
 		struct kvm_lpage_info *linfo;
 		unsigned long ugfn;
 		int lpages;
+		/*
+		 * level = 1 + 1 = 2
+		 * level = 2 + 1 = 3
+		 * 
+		 * 2和3
+		 */
 		int level = i + 1;
 
+		/*
+		 * 假设:
+		 * base_gfn: 133693442
+		 * gfn     : 134060443
+		 *
+		 * 对于level 2:
+		 * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 9       = 261836
+		 * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 9 = 261120
+		 * 261836 - 261120 = 716
+		 *
+		 * 对于level 3:
+		 * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 18       = 511
+		 * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 18 = 510
+		 * 511 - 510 = 0
+		 *
+		 * level 3 (index 1, 1G)有1个slot
+		 * level 2 (index 2, 2M)有717个slot
+		 *
+		 * 实际:
+		 * [  465.699816] kvm: orabug: alloc i=1, lpages=2, level=2
+		 * [  465.705963] kvm: orabug: alloc i=2, lpages=1, level=3
+		 */
 		lpages = __kvm_mmu_slot_lpages(slot, npages, level);
 
+		/*
+		 * struct kvm_lpage_info *linfo;
+		 */
 		linfo = __vcalloc(lpages, sizeof(*linfo), GFP_KERNEL_ACCOUNT);
 		if (!linfo)
 			goto out_free;
 
+		/*
+		 * i是1和2
+		 */
 		slot->arch.lpage_info[i - 1] = linfo;
 
+		/*
+		 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+		 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512           ---> level 2
+		 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144        ---> level 3
+		 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+		 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+		 *
+		 * level是2和3
+		 */
 		if (slot->base_gfn & (KVM_PAGES_PER_HPAGE(level) - 1))
 			linfo[0].disallow_lpage = 1;
 		if ((slot->base_gfn + npages) & (KVM_PAGES_PER_HPAGE(level) - 1))
@@ -12850,6 +13521,10 @@ void kvm_arch_memslots_updated(struct kvm *kvm, u64 gen)
 		kvm_vcpu_kick(vcpu);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1720| <<kvm_prepare_memory_region>> r = kvm_arch_prepare_memory_region(kvm, old, new, change);
+ */
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   const struct kvm_memory_slot *old,
 				   struct kvm_memory_slot *new,
@@ -12890,6 +13565,10 @@ static void kvm_mmu_update_cpu_dirty_logging(struct kvm *kvm, bool enable)
 		kvm_make_all_cpus_request(kvm, KVM_REQ_UPDATE_CPU_DIRTY_LOGGING);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13259| <<kvm_arch_commit_memory_region>> kvm_mmu_slot_apply_flags(kvm, old, new, change);
+ */
 static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 				     struct kvm_memory_slot *old,
 				     const struct kvm_memory_slot *new,
@@ -12958,6 +13637,18 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 		if (READ_ONCE(eager_page_split))
 			kvm_mmu_slot_try_split_huge_pages(kvm, new, PG_LEVEL_4K);
 
+		/*
+		 * 在以下设置kvm_x86_ops.cpu_dirty_log_size:
+		 *   - arch/x86/kvm/vmx/vmx.c|8424| <<global>> .cpu_dirty_log_size = PML_ENTITY_NUM,
+		 *   - arch/x86/kvm/vmx/vmx.c|8664| <<hardware_setup>> vmx_x86_ops.cpu_dirty_log_size = 0;
+		 * 在以下使用kvm_x86_ops.cpu_dirty_log_size:
+		 *   - arch/x86/kvm/mmu/mmu.c|1500| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> if (kvm_x86_ops.cpu_dirty_log_size)
+		 *   - arch/x86/kvm/mmu/mmu.c|1508| <<kvm_cpu_dirty_log_size>> return kvm_x86_ops.cpu_dirty_log_size;
+		 *   - arch/x86/kvm/mmu/mmu_internal.h|205| <<kvm_mmu_page_ad_need_write_protect>> return kvm_x86_ops.cpu_dirty_log_size && sp->role.guest_mode;
+		 *   - arch/x86/kvm/x86.c|6457| <<kvm_arch_sync_dirty_log>> if (!kvm_x86_ops.cpu_dirty_log_size)
+		 *   - arch/x86/kvm/x86.c|13113| <<kvm_mmu_update_cpu_dirty_logging>> if (!kvm_x86_ops.cpu_dirty_log_size)
+		 *   - arch/x86/kvm/x86.c|13189| <<kvm_mmu_slot_apply_flags>> if (kvm_x86_ops.cpu_dirty_log_size) {
+		 */
 		if (kvm_x86_ops.cpu_dirty_log_size) {
 			kvm_mmu_slot_leaf_clear_dirty(kvm, new);
 			kvm_mmu_slot_remove_write_access(kvm, new, PG_LEVEL_2M);
@@ -13011,6 +13702,10 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1766| <<kvm_commit_memory_region>> kvm_arch_commit_memory_region(kvm, old, new, change);
+ */
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *old,
 				const struct kvm_memory_slot *new,
@@ -13419,6 +14114,11 @@ bool noinstr kvm_arch_has_assigned_device(struct kvm *kvm)
 }
 EXPORT_SYMBOL_GPL(kvm_arch_has_assigned_device);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13438| <<kvm_arch_register_noncoherent_dma>> kvm_noncoherent_dma_assignment_start_or_stop(kvm);
+ *   - arch/x86/kvm/x86.c|13445| <<kvm_arch_unregister_noncoherent_dma>> kvm_noncoherent_dma_assignment_start_or_stop(kvm);
+ */
 static void kvm_noncoherent_dma_assignment_start_or_stop(struct kvm *kvm)
 {
 	/*
@@ -13432,6 +14132,10 @@ static void kvm_noncoherent_dma_assignment_start_or_stop(struct kvm *kvm)
 		kvm_zap_gfn_range(kvm, gpa_to_gfn(0), gpa_to_gfn(~0ULL));
 }
 
+/*
+ * called by:
+ *   - virt/kvm/vfio.c|137| <<kvm_vfio_update_coherency>> kvm_arch_register_noncoherent_dma(dev->kvm);
+ */
 void kvm_arch_register_noncoherent_dma(struct kvm *kvm)
 {
 	if (atomic_inc_return(&kvm->arch.noncoherent_dma_count) == 1)
@@ -13446,6 +14150,12 @@ void kvm_arch_unregister_noncoherent_dma(struct kvm *kvm)
 }
 EXPORT_SYMBOL_GPL(kvm_arch_unregister_noncoherent_dma);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|258| <<kvm_mmu_honors_guest_mtrrs>> return __kvm_mmu_honors_guest_mtrrs(kvm_arch_has_noncoherent_dma(kvm));
+ *   - arch/x86/kvm/vmx/vmx.c|7613| <<vmx_get_mt_mask>> if (!kvm_arch_has_noncoherent_dma(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|4956| <<need_emulate_wbinvd>> return kvm_arch_has_noncoherent_dma(vcpu->kvm);
+ */
 bool kvm_arch_has_noncoherent_dma(struct kvm *kvm)
 {
 	return atomic_read(&kvm->arch.noncoherent_dma_count);
@@ -13911,6 +14621,10 @@ EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_exit);
 EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_msr_protocol_enter);
 EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_msr_protocol_exit);
 
+/*
+ * module的entry:
+ *   - arch/x86/kvm/x86.c|14063| <<global>> module_init(kvm_x86_init);
+ */
 static int __init kvm_x86_init(void)
 {
 	kvm_mmu_x86_module_init();
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 2f7e19166..473135567 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -182,6 +182,14 @@ static inline bool x86_exception_has_error_code(unsigned int vector)
 
 static inline bool mmu_is_nested(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->walk_mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|6448| <<kvm_mmu_create>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|105| <<nested_svm_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|461| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|467| <<nested_ept_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 */
 	return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;
 }
 
@@ -215,6 +223,11 @@ static inline bool is_noncanonical_address(u64 la, struct kvm_vcpu *vcpu)
 	return !__is_canonical_address(la, vcpu_virt_addr_bits(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3339| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+ *   - arch/x86/kvm/mmu/mmu.c|4219| <<handle_mmio_page_fault>> vcpu_cache_mmio_info(vcpu, addr, gfn, access);
+ */
 static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 					gva_t gva, gfn_t gfn, unsigned access)
 {
@@ -223,16 +236,37 @@ static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 	if (unlikely(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS))
 		return;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> u64 mmio_gva;
+	 *    -> unsigned mmio_access;
+	 *    -> gfn_t mmio_gfn;
+	 *    -> u64 mmio_gen;
+	 */
 	/*
 	 * If this is a shadow nested page table, the "GVA" is
 	 * actually a nGPA.
 	 */
 	vcpu->arch.mmio_gva = mmu_is_nested(vcpu) ? 0 : gva & PAGE_MASK;
+	/*
+	 * 在以下使用mmio_access:
+	 *   - arch/x86/kvm/x86.c|7739| <<vcpu_mmio_gva_to_gpa>> if (vcpu_match_mmio_gva(vcpu, gva) &&
+	 *                                         (!is_paging(vcpu) || !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access))) {
+	 *   - arch/x86/kvm/x86.h|244| <<vcpu_cache_mmio_info>> vcpu->arch.mmio_access = access;
+	 */
 	vcpu->arch.mmio_access = access;
 	vcpu->arch.mmio_gfn = gfn;
 	vcpu->arch.mmio_gen = gen;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.h|257| <<vcpu_match_mmio_gva>> if (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gva &&
+ *   - arch/x86/kvm/x86.h|266| <<vcpu_match_mmio_gpa>> if (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gfn &&
+ *
+ * vcpu_arch缓存的mmio_gen和memslots的相等
+ */
 static inline bool vcpu_match_mmio_gen(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.mmio_gen == kvm_memslots(vcpu->kvm)->generation;
@@ -244,6 +278,13 @@ static inline bool vcpu_match_mmio_gen(struct kvm_vcpu *vcpu)
  */
 #define MMIO_GVA_ANY (~(gva_t)0)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4054| <<kvm_mmu_sync_roots>> vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
+ *   - arch/x86/kvm/mmu/mmu.c|4846| <<kvm_mmu_new_pgd>> vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
+ *   - arch/x86/kvm/mmu/mmu.c|5691| <<kvm_mmu_unload>> vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
+ *   - arch/x86/kvm/mmu/mmu.c|5969| <<__kvm_mmu_invalidate_addr>> vcpu_clear_mmio_info(vcpu, addr);
+ */
 static inline void vcpu_clear_mmio_info(struct kvm_vcpu *vcpu, gva_t gva)
 {
 	if (gva != MMIO_GVA_ANY && vcpu->arch.mmio_gva != (gva & PAGE_MASK))
@@ -252,6 +293,11 @@ static inline void vcpu_clear_mmio_info(struct kvm_vcpu *vcpu, gva_t gva)
 	vcpu->arch.mmio_gva = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4118| <<mmio_info_in_cache>> return vcpu_match_mmio_gva(vcpu, addr);
+ *   - arch/x86/kvm/x86.c|7737| <<vcpu_mmio_gva_to_gpa>> if (vcpu_match_mmio_gva(vcpu, gva) && (!is_paging(vcpu) ||
+ */
 static inline bool vcpu_match_mmio_gva(struct kvm_vcpu *vcpu, unsigned long gva)
 {
 	if (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gva &&
@@ -261,8 +307,21 @@ static inline bool vcpu_match_mmio_gva(struct kvm_vcpu *vcpu, unsigned long gva)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4116| <<mmio_info_in_cache>> return vcpu_match_mmio_gpa(vcpu, addr);
+ *   - arch/x86/kvm/x86.c|7716| <<vcpu_is_mmio_gpa>> if (vcpu_match_mmio_gpa(vcpu, gpa)) {
+ *
+ * 如果下面的满足返回true, 否则false:
+ * 1. vcpu_arch缓存的mmio_gen和memslots的相等
+ * 2. vcpu->arch.mmio_gfn和参数的gpa互相match
+ */
 static inline bool vcpu_match_mmio_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
+	/*
+	 * vcpu_match_mmio_gen(vcpu):
+	 * vcpu_arch缓存的mmio_gen和memslots的相等
+	 */
 	if (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gfn &&
 	      vcpu->arch.mmio_gfn == gpa >> PAGE_SHIFT)
 		return true;
diff --git a/arch/x86/kvm/xen.c b/arch/x86/kvm/xen.c
index 4b4e738c6..ba2387bdd 100644
--- a/arch/x86/kvm/xen.c
+++ b/arch/x86/kvm/xen.c
@@ -34,6 +34,10 @@ static bool kvm_xen_hcall_evtchn_send(struct kvm_vcpu *vcpu, u64 param, u64 *r);
 
 DEFINE_STATIC_KEY_DEFERRED_FALSE(kvm_xen_enabled, HZ);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|645| <<kvm_xen_hvm_set_attr(KVM_XEN_ATTR_TYPE_SHARED_INFO)>> r = kvm_xen_shared_info_init(kvm, data->u.shared_info.gfn);
+ */
 static int kvm_xen_shared_info_init(struct kvm *kvm, gfn_t gfn)
 {
 	struct gfn_to_pfn_cache *gpc = &kvm->arch.xen.shinfo_cache;
diff --git a/drivers/clocksource/arm_arch_timer.c b/drivers/clocksource/arm_arch_timer.c
index e054de92d..c674a5155 100644
--- a/drivers/clocksource/arm_arch_timer.c
+++ b/drivers/clocksource/arm_arch_timer.c
@@ -671,6 +671,9 @@ static __always_inline irqreturn_t timer_handler(const int access,
 	if (ctrl & ARCH_TIMER_CTRL_IT_STAT) {
 		ctrl |= ARCH_TIMER_CTRL_IT_MASK;
 		arch_timer_reg_write(access, ARCH_TIMER_REG_CTRL, ctrl, evt);
+		/*
+		 * 虚拟机里是hrtimer_interrupt()
+		 */
 		evt->event_handler(evt);
 		return IRQ_HANDLED;
 	}
@@ -1234,6 +1237,11 @@ static void __init arch_timer_cpu_pm_deinit(void)
 }
 #endif
 
+/*
+ * called by:
+ *   - drivers/clocksource/arm_arch_timer.c|1473| <<arch_timer_of_init>> ret = arch_timer_register();
+ *   - drivers/clocksource/arm_arch_timer.c|1796| <<arch_timer_acpi_init>> ret = arch_timer_register();
+ */
 static int __init arch_timer_register(void)
 {
 	int err;
diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c
index d14413916..ae7728eff 100644
--- a/drivers/iommu/iommu.c
+++ b/drivers/iommu/iommu.c
@@ -304,6 +304,10 @@ EXPORT_SYMBOL_GPL(iommu_device_unregister_bus);
  * selftest to create a mock iommu driver. The caller must provide
  * some memory to hold a notifier_block.
  */
+/*
+ * called by:
+ *   - drivers/iommu/iommufd/selftest.c|1489| <<iommufd_test_init>> rc = iommu_device_register_bus(&mock_iommu_device, &mock_ops, &iommufd_mock_bus_type.bus, &iommufd_mock_bus_type.nb);
+ */
 int iommu_device_register_bus(struct iommu_device *iommu,
 			      const struct iommu_ops *ops, struct bus_type *bus,
 			      struct notifier_block *nb)
diff --git a/drivers/irqchip/irq-gic-v3.c b/drivers/irqchip/irq-gic-v3.c
index 98b0329b7..dead0c798 100644
--- a/drivers/irqchip/irq-gic-v3.c
+++ b/drivers/irqchip/irq-gic-v3.c
@@ -1112,6 +1112,10 @@ static int __gic_update_rdist_properties(struct redist_region *region,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|2066| <<gic_init_bases>> gic_update_rdist_properties();
+ */
 static void gic_update_rdist_properties(void)
 {
 	gic_data.ppi_nr = UINT_MAX;
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index d7ce4a101..e3f6499d2 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -254,9 +254,19 @@ struct virtnet_info {
 	struct receive_queue *rq;
 	unsigned int status;
 
+	/*
+	 * 在以下修改virtnet_info->max_queue_pairs:
+	 *   - drivers/net/virtio_net.c|4776| <<virtnet_probe>> vi->max_queue_pairs = max_queue_pairs;
+	 */
 	/* Max # of queue pairs supported by the device */
 	u16 max_queue_pairs;
 
+	/*
+	 * 在以下修改virtnet_info->curr_queue_pairs:
+	 *   - drivers/net/virtio_net.c|2672| <<_virtnet_set_queues>> vi->curr_queue_pairs = queue_pairs;
+	 *   - drivers/net/virtio_net.c|4773| <<virtnet_probe>> vi->curr_queue_pairs = max_queue_pairs;
+	 *   - drivers/net/virtio_net.c|4775| <<virtnet_probe>> vi->curr_queue_pairs = num_online_cpus();
+	 */
 	/* # of queue pairs currently used by the driver */
 	u16 curr_queue_pairs;
 
@@ -2652,6 +2662,13 @@ static void virtnet_ack_link_announce(struct virtnet_info *vi)
 	rtnl_unlock();
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2686| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|3270| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|4022| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+ *   - drivers/net/virtio_net.c|4836| <<virtnet_probe>> _virtnet_set_queues(vi, vi->curr_queue_pairs);
+ */
 static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 {
 	struct scatterlist sg;
@@ -2678,6 +2695,10 @@ static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|4950| <<virtnet_restore>> virtnet_set_queues(vi, vi->curr_queue_pairs);
+ */
 static int virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 {
 	int err;
@@ -2820,6 +2841,13 @@ static void virtnet_clean_affinity(struct virtnet_info *vi)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2866| <<virtnet_cpu_online>> virtnet_set_affinity(vi);
+ *   - drivers/net/virtio_net.c|2874| <<virtnet_cpu_dead>> virtnet_set_affinity(vi);
+ *   - drivers/net/virtio_net.c|3252| <<virtnet_set_channels>> virtnet_set_affinity(vi);
+ *   - drivers/net/virtio_net.c|4443| <<init_vqs>> virtnet_set_affinity(vi);
+ */
 static void virtnet_set_affinity(struct virtnet_info *vi)
 {
 	cpumask_var_t mask;
@@ -2835,13 +2863,22 @@ static void virtnet_set_affinity(struct virtnet_info *vi)
 	}
 
 	num_cpu = num_online_cpus();
+	/*
+	 * 假设8个cpu, 128个queue pairs, stride是1
+	 */
 	stride = max_t(int, num_cpu / vi->curr_queue_pairs, 1);
+	/*
+	 * stragglers是0
+	 */
 	stragglers = num_cpu >= vi->curr_queue_pairs ?
 			num_cpu % vi->curr_queue_pairs :
 			0;
 	cpu = cpumask_first(cpu_online_mask);
 
 	for (i = 0; i < vi->curr_queue_pairs; i++) {
+		/*
+		 * group_size永远是1
+		 */
 		group_size = stride + (i < stragglers ? 1 : 0);
 
 		for (j = 0; j < group_size; j++) {
@@ -2851,6 +2888,13 @@ static void virtnet_set_affinity(struct virtnet_info *vi)
 		}
 		virtqueue_set_affinity(vi->rq[i].vq, mask);
 		virtqueue_set_affinity(vi->sq[i].vq, mask);
+		/*
+		 * called by:
+		 *   - drivers/net/ethernet/ibm/ibmvnic.c|299| <<ibmvnic_set_affinity>> rc = __netif_set_xps_queue(adapter->netdev, cpumask_bits(queue->affinity_mask), i_txqs - 1, XPS_CPUS);
+		 *   - drivers/net/virtio_net.c|2870| <<virtnet_set_affinity>> __netif_set_xps_queue(vi->dev, cpumask_bits(mask), i, XPS_CPUS);
+		 *   - net/core/dev.c|2765| <<netif_set_xps_queue>> ret = __netif_set_xps_queue(dev, cpumask_bits(mask), index, XPS_CPUS);
+		 *   - net/core/net-sysfs.c|1633| <<xps_rxqs_store>> err = __netif_set_xps_queue(dev, mask, index, XPS_RXQS);
+		 */
 		__netif_set_xps_queue(vi->dev, cpumask_bits(mask), i, XPS_CPUS);
 		cpumask_clear(mask);
 	}
@@ -3219,6 +3263,9 @@ static void virtnet_get_drvinfo(struct net_device *dev,
 
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.set_channels = virtnet_set_channels()
+ */
 /* TODO: Eliminate OOO packets during switching */
 static int virtnet_set_channels(struct net_device *dev,
 				struct ethtool_channels *channels)
diff --git a/drivers/pci/msi/api.c b/drivers/pci/msi/api.c
index be679aa5d..9985dbf41 100644
--- a/drivers/pci/msi/api.c
+++ b/drivers/pci/msi/api.c
@@ -231,6 +231,20 @@ EXPORT_SYMBOL(pci_disable_msix);
  * @max_vecs), -ENOSPC if less than @min_vecs interrupt vectors are
  * available, other errnos otherwise.
  */
+/*
+ * 重要的例子:
+ *   - drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c|787| <<mlx5_irq_table_create>> n = pci_alloc_irq_vectors(dev->pdev, 1, req_vec, PCI_IRQ_MSIX);
+ *   - drivers/net/ethernet/mellanox/mlxsw/pci.c|1581| <<mlxsw_pci_alloc_irq_vectors>> err = pci_alloc_irq_vectors(mlxsw_pci->pdev, 1, 1, PCI_IRQ_MSIX);
+ *   - drivers/nvme/host/pci.c|2490| <<nvme_pci_enable>> result = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_ALL_TYPES);
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|167| <<eni_vdpa_request_irq>> ret = pci_alloc_irq_vectors(pdev, vectors, vectors, PCI_IRQ_MSIX);
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|143| <<ifcvf_alloc_vectors>> ret = pci_alloc_irq_vectors(pdev, 1, max_intr, PCI_IRQ_MSIX | PCI_IRQ_AFFINITY);
+ *   - drivers/vdpa/pds/vdpa_dev.c|389| <<pds_vdpa_request_irqs>> nintrs = pci_alloc_irq_vectors(pdev, max_vq, max_vq, PCI_IRQ_MSIX);
+ *   - drivers/vdpa/solidrun/snet_main.c|772| <<psnet_alloc_irq_vector>> ret = pci_alloc_irq_vectors(pdev, irq_num, irq_num, PCI_IRQ_MSIX);
+ *   - drivers/vdpa/solidrun/snet_main.c|790| <<snet_alloc_irq_vector>> ret = pci_alloc_irq_vectors(pdev, irq_num, irq_num, PCI_IRQ_MSIX);
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|165| <<vp_vdpa_request_irq>> ret = pci_alloc_irq_vectors(pdev, vectors, vectors, PCI_IRQ_MSIX);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|361| <<vfio_msi_enable>> ret = pci_alloc_irq_vectors(pdev, 1, nvec, flag);
+ *   - drivers/virt/nitro_enclaves/ne_pci_dev.c|310| <<ne_setup_msix>> rc = pci_alloc_irq_vectors(pdev, nr_vecs, nr_vecs, PCI_IRQ_MSIX);
+ */
 int pci_alloc_irq_vectors(struct pci_dev *dev, unsigned int min_vecs,
 			  unsigned int max_vecs, unsigned int flags)
 {
@@ -251,6 +265,24 @@ EXPORT_SYMBOL(pci_alloc_irq_vectors);
  * Same as pci_alloc_irq_vectors(), but with the extra @affd parameter.
  * Check that function docs, and &struct irq_affinity, for more details.
  */
+/*
+ * called by:
+ *   - drivers/net/ethernet/wangxun/libwx/wx_lib.c|1627| <<wx_acquire_msix_vectors>> nvecs = pci_alloc_irq_vectors_affinity(wx->pdev, nvecs, nvecs, PCI_IRQ_MSIX | PCI_IRQ_AFFINITY, &affd);
+ *   - drivers/nvme/host/pci.c|2244| <<nvme_setup_irqs>> return pci_alloc_irq_vectors_affinity(pdev, 1, irq_queues, PCI_IRQ_ALL_TYPES | PCI_IRQ_AFFINITY, &affd);
+ *   - drivers/pci/msi/api.c|237| <<pci_alloc_irq_vectors>> return pci_alloc_irq_vectors_affinity(dev, min_vecs, max_vecs, flags, NULL);
+ *   - drivers/scsi/be2iscsi/be_main.c|3585| <<be2iscsi_enable_msix>> if (pci_alloc_irq_vectors_affinity(phba->pcidev, 2, nvec, PCI_IRQ_MSIX | PCI_IRQ_AFFINITY, &desc);
+ *   - drivers/scsi/csiostor/csio_isr.c|520| <<csio_enable_msix>> cnt = pci_alloc_irq_vectors_affinity(hw->pdev, min, cnt, PCI_IRQ_MSIX | PCI_IRQ_AFFINITY, &desc);
+ *   - drivers/scsi/hisi_sas/hisi_sas_v3_hw.c|2544| <<interrupt_preinit_v3_hw>> vectors = pci_alloc_irq_vectors_affinity(pdev, min_msi, max_msi, PCI_IRQ_MSI | PCI_IRQ_AFFINITY, &desc);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|5932| <<__megasas_alloc_irq_vectors>> i = pci_alloc_irq_vectors_affinity(instance->pdev, instance->low_latency_index_start,
+ *                                                             instance->msix_vectors - instance->iopoll_q_count, irq_flags, descp);
+ *   - drivers/scsi/mpi3mr/mpi3mr_fw.c|820| <<mpi3mr_setup_isr>> retval = pci_alloc_irq_vectors_affinity(mrioc->pdev, min_vec, max_vectors, irq_flags, &desc);
+ *   - drivers/scsi/mpt3sas/mpt3sas_base.c|3363| <<_base_alloc_irq_vectors>> i = pci_alloc_irq_vectors_affinity(ioc->pdev, ioc->high_iops_queues, nr_msix_vectors, irq_flags, descp);
+ *   - drivers/scsi/pm8001/pm8001_init.c|994| <<pm8001_setup_msix>> rc = pci_alloc_irq_vectors_affinity(pm8001_ha->pdev, 2, PM8001_MAX_MSIX_VEC, PCI_IRQ_MSIX | PCI_IRQ_AFFINITY, &desc);
+ *   - drivers/scsi/qla2xxx/qla_isr.c|4545| <<qla24xx_enable_msix>> ret = pci_alloc_irq_vectors_affinity(ha->pdev, min_vecs, min((u16)ha->msix_count, (u16)(num_online_cpus() + min_vecs)),
+ *                                                             PCI_IRQ_MSIX | PCI_IRQ_AFFINITY, &desc);
+ *   - drivers/virtio/virtio_pci_common.c|133| <<vp_request_msix_vectors>> err = pci_alloc_irq_vectors_affinity(vp_dev->pci_dev, nvectors, nvectors, flags, desc);
+ *   - include/linux/pci.h|1730| <<pci_alloc_irq_vectors>> return pci_alloc_irq_vectors_affinity(dev, min_vecs, max_vecs, flags, NULL);
+ */
 int pci_alloc_irq_vectors_affinity(struct pci_dev *dev, unsigned int min_vecs,
 				   unsigned int max_vecs, unsigned int flags,
 				   struct irq_affinity *affd)
diff --git a/drivers/perf/arm_pmu.c b/drivers/perf/arm_pmu.c
index 8458fe2ce..20db29c78 100644
--- a/drivers/perf/arm_pmu.c
+++ b/drivers/perf/arm_pmu.c
@@ -325,6 +325,9 @@ armpmu_del(struct perf_event *event, int flags)
 	hwc->idx = -1;
 }
 
+/*
+ * 查看armv8_pmu_init()
+ */
 static int
 armpmu_add(struct perf_event *event, int flags)
 {
@@ -909,6 +912,11 @@ void armpmu_free(struct arm_pmu *pmu)
 	kfree(pmu);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmu_acpi.c|418| <<arm_pmu_acpi_probe>> ret = armpmu_register(pmu);
+ *   - drivers/perf/arm_pmu_platform.c|231| <<arm_pmu_device_probe>> ret = armpmu_register(pmu);
+ */
 int armpmu_register(struct arm_pmu *pmu)
 {
 	int ret;
diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c
index df5ac03d5..8f20b727b 100644
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -1757,6 +1757,10 @@ static blk_status_t scsi_queue_rq(struct blk_mq_hw_ctx *hctx,
 	cmd->submitter = SUBMITTED_BY_BLOCK_LAYER;
 
 	blk_mq_start_request(req);
+	/*
+	 * struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+	 * -> unsigned char cmnd[32]; // SCSI CDB
+	 */
 	reason = scsi_dispatch_cmd(cmd);
 	if (reason) {
 		scsi_set_blocked(cmd, reason);
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 617eb892f..cc14395da 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -444,6 +444,18 @@ static int __virtscsi_add_cmd(struct virtqueue *vq,
 			in = &sc->sdb.table;
 	}
 
+	/*
+	 * struct virtio_scsi_cmd_req {
+	 *     __u8 lun[8];            // Logical Unit Number
+	 *     __virtio64 tag;         // Command identifier
+	 *     __u8 task_attr;         // Task attribute
+	 *     __u8 prio;              // SAM command priority field
+	 *     __u8 crn;
+	 *     __u8 cdb[VIRTIO_SCSI_CDB_SIZE];
+	 * } __attribute__((packed));
+	 *
+	 * struct virtio_scsi_cmd *cmd
+	 */
 	/* Request header.  */
 	sg_init_one(&req, &cmd->req, req_size);
 	sgs[out_num++] = &req;
@@ -562,6 +574,10 @@ static struct virtio_scsi_vq *virtscsi_pick_vq_mq(struct virtio_scsi *vscsi,
 	return &vscsi->req_vqs[hwq];
 }
 
+/*
+ * struct scsi_cmnd *sc:
+ * -> unsigned char cmnd[32]; // SCSI CDB
+ */
 static int virtscsi_queuecommand(struct Scsi_Host *shost,
 				 struct scsi_cmnd *sc)
 {
diff --git a/include/kvm/arm_vgic.h b/include/kvm/arm_vgic.h
index 8cc38e836..3d87fae77 100644
--- a/include/kvm/arm_vgic.h
+++ b/include/kvm/arm_vgic.h
@@ -255,6 +255,17 @@ struct vgic_dist {
 	/* distributor enabled */
 	bool			enabled;
 
+	/*
+	 * 在以下设置vgic_dist->nassgireq:
+	 *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|132| <<vgic_mmio_write_v3_misc>> dist->nassgireq = val & GICD_CTLR_nASSGIreq;
+	 *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|185| <<vgic_mmio_uaccess_write_v3_misc>> dist->nassgireq = val & GICD_CTLR_nASSGIreq;
+	 * 在以下使用vgic_dist->nassgireq:
+	 *   - arch/arm64/include/asm/kvm_emulate.h|116| <<vcpu_clear_wfx_traps>> if (atomic_read(&vcpu->arch.vgic_cpu.vgic_v3.its_vpe.vlpi_count) || vcpu->kvm->arch.vgic.nassgireq)
+	 *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|75| <<vgic_mmio_read_v3_misc>> if (vgic->nassgireq)
+	 *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|117| <<vgic_mmio_write_v3_misc>> is_hwsgi = dist->nassgireq;
+	 *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|133| <<vgic_mmio_write_v3_misc>> if (is_hwsgi != dist->nassgireq)
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|198| <<vgic_v4_configure_vsgis>> if (dist->nassgireq)
+	 */
 	/* Wants SGIs without active state */
 	bool			nassgireq;
 
@@ -309,6 +320,16 @@ struct vgic_v3_cpu_if {
 	u32		vgic_sre;	/* Restored only, change ignored */
 	u32		vgic_ap0r[4];
 	u32		vgic_ap1r[4];
+	/*
+	 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+	 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|75| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|243| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|186| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|191| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+	 */
 	u64		vgic_lr[VGIC_V3_MAX_LRS];
 
 	/*
@@ -339,6 +360,23 @@ struct vgic_cpu {
 	 * were one of the two and need to be migrated off this list to another
 	 * VCPU.
 	 */
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<__kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|160| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|305| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|410| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|643| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|715| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|782| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|814| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|840| <<vgic_flush_lr_state>> if (!list_is_last(&irq->ap_list, &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|880| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|919| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|925| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|989| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 */
 	struct list_head ap_list_head;
 
 	/*
diff --git a/include/linux/interrupt.h b/include/linux/interrupt.h
index 76121c2bb..f25dd468e 100644
--- a/include/linux/interrupt.h
+++ b/include/linux/interrupt.h
@@ -298,6 +298,10 @@ struct irq_affinity {
  */
 struct irq_affinity_desc {
 	struct cpumask	mask;
+	/*
+	 * 在以下设置irq_affinity_desc->is_managed:
+	 *   - kernel/irq/affinity.c|99| <<irq_create_affinity_masks>> masks[i].is_managed = 1;
+	 */
 	unsigned int	is_managed : 1;
 };
 
@@ -321,6 +325,39 @@ extern int __irq_apply_affinity_hint(unsigned int irq, const struct cpumask *m,
  *
  * Updates the affinity hint, but does not change the affinity of the interrupt.
  */
+/*
+ * called by:
+ *   - drivers/infiniband/hw/irdma/hw.c|572| <<irdma_destroy_irq>> irq_update_affinity_hint(msix_vec->irq, NULL);
+ *   - drivers/infiniband/hw/irdma/hw.c|1138| <<irdma_cfg_ceq_vector>> irq_update_affinity_hint(msix_vec->irq, &msix_vec->mask);
+ *   - drivers/mailbox/bcm-flexrm-mailbox.c|1291| <<flexrm_startup>> ret = irq_update_affinity_hint(ring->irq, &ring->irq_aff_hint);
+ *   - drivers/mailbox/bcm-flexrm-mailbox.c|1418| <<flexrm_shutdown>> irq_update_affinity_hint(ring->irq, NULL);
+ *   - drivers/net/ethernet/cisco/enic/enic_main.c|153| <<enic_set_affinity_hint>> err = irq_update_affinity_hint(enic->msix_entry[i].vector,
+ *   - drivers/net/ethernet/cisco/enic/enic_main.c|176| <<enic_unset_affinity_hint>> irq_update_affinity_hint(enic->msix_entry[i].vector, NULL);
+ *   - drivers/net/ethernet/emulex/benet/be_main.c|3495| <<be_msix_register>> irq_update_affinity_hint(vec, eqo->affinity_mask);
+ *   - drivers/net/ethernet/emulex/benet/be_main.c|3556| <<be_irq_unregister>> irq_update_affinity_hint(vec, NULL);
+ *   - drivers/net/ethernet/fungible/funeth/funeth_main.c|725| <<fun_disable_one_irq>> irq_update_affinity_hint(irq->irq, NULL);
+ *   - drivers/net/ethernet/huawei/hinic/hinic_rx.c|567| <<rx_free_irq>> irq_update_affinity_hint(rq->irq, NULL);
+ *   - drivers/net/ethernet/intel/i40e/i40e_main.c|4183| <<i40e_vsi_request_irq_msix>> irq_update_affinity_hint(irq_num, get_cpu_mask(cpu));
+ *   - drivers/net/ethernet/intel/i40e/i40e_main.c|4194| <<i40e_vsi_request_irq_msix>> irq_update_affinity_hint(irq_num, NULL);
+ *   - drivers/net/ethernet/intel/i40e/i40e_main.c|5009| <<i40e_vsi_free_irq>> irq_update_affinity_hint(irq_num, NULL);
+ *   - drivers/net/ethernet/intel/iavf/iavf_main.c|611| <<iavf_request_traffic_irqs>> irq_update_affinity_hint(irq_num, get_cpu_mask(cpu));
+ *   - drivers/net/ethernet/intel/iavf/iavf_main.c|621| <<iavf_request_traffic_irqs>> irq_update_affinity_hint(irq_num, NULL);
+ *   - drivers/net/ethernet/intel/iavf/iavf_main.c|673| <<iavf_free_traffic_irqs>> irq_update_affinity_hint(irq_num, NULL);
+ *   - drivers/net/ethernet/intel/ixgbe/ixgbe_main.c|3241| <<ixgbe_request_msix_irqs>> irq_update_affinity_hint(entry->vector,
+ *   - drivers/net/ethernet/intel/ixgbe/ixgbe_main.c|3258| <<ixgbe_request_msix_irqs>> irq_update_affinity_hint(adapter->msix_entries[vector].vector,
+ *   - drivers/net/ethernet/intel/ixgbe/ixgbe_main.c|3392| <<ixgbe_free_irq>> irq_update_affinity_hint(entry->vector, NULL);
+ *   - drivers/net/ethernet/mellanox/mlx4/eq.c|247| <<mlx4_set_eq_affinity_hint>> hint_err = irq_update_affinity_hint(eq->irq, eq->affinity_mask);
+ *   - drivers/net/ethernet/mellanox/mlx4/eq.c|1127| <<mlx4_free_irqs>> irq_update_affinity_hint(eq_table->eq[i].irq, NULL);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c|163| <<mlx5_system_free_irq>> irq_update_affinity_hint(irq->map.virq, NULL);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c|320| <<mlx5_irq_alloc>> irq_update_affinity_hint(irq->map.virq, NULL);
+ *   - drivers/net/ethernet/microsoft/mana/gdma_main.c|1315| <<mana_gd_setup_irqs>> irq_update_affinity_hint(irq, NULL);
+ *   - drivers/net/ethernet/microsoft/mana/gdma_main.c|1343| <<mana_gd_remove_irqs>> irq_update_affinity_hint(irq, NULL);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|5723| <<megasas_setup_irqs_msix>> irq_update_affinity_hint(
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|5766| <<megasas_destroy_irqs>> irq_update_affinity_hint(
+ *   - drivers/scsi/mpt3sas/mpt3sas_base.c|3115| <<mpt3sas_base_free_irq>> irq_update_affinity_hint(irq, NULL);
+ *   - drivers/virtio/virtio_pci_common.c|248| <<vp_del_vqs>> irq_update_affinity_hint(irq, NULL);
+ *   - drivers/virtio/virtio_pci_common.c|449| <<vp_set_vq_affinity>> irq_update_affinity_hint(irq, NULL);
+ */
 static inline int
 irq_update_affinity_hint(unsigned int irq, const struct cpumask *m)
 {
diff --git a/include/linux/irq.h b/include/linux/irq.h
index 90081afa1..39ef664b5 100644
--- a/include/linux/irq.h
+++ b/include/linux/irq.h
@@ -103,6 +103,12 @@ enum {
 	IRQ_NO_DEBUG		= (1 << 21),
 };
 
+/*
+ * 在以下使用IRQF_MODIFY_MASK:
+ *   - kernel/irq/settings.h|22| <<global>> _IRQF_MODIFY_MASK = IRQF_MODIFY_MASK,
+ *   - kernel/irq/settings.h|38| <<global>> #undef IRQF_MODIFY_MASK
+ *   - kernel/irq/settings.h|39| <<global>> #define IRQF_MODIFY_MASK GOT_YOU_MORON
+ */
 #define IRQF_MODIFY_MASK	\
 	(IRQ_TYPE_SENSE_MASK | IRQ_NOPROBE | IRQ_NOREQUEST | \
 	 IRQ_NOAUTOEN | IRQ_MOVE_PCNTXT | IRQ_LEVEL | IRQ_NO_BALANCING | \
@@ -155,6 +161,14 @@ struct irq_common_data {
 	cpumask_var_t		affinity;
 #endif
 #ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK
+	/*
+	 * 在以下使用irq_common_data->effective_affinity:
+	 *   - include/linux/irq.h|967| <<irq_data_get_effective_affinity_mask>> return d->common->effective_affinity;
+	 *   - include/linux/irq.h|1009| <<irq_data_update_effective_affinity>> cpumask_copy(d->common->effective_affinity, m);
+	 *   - kernel/irq/irqdesc.c|61| <<alloc_masks>> if (!zalloc_cpumask_var_node(&desc->irq_common_data.effective_affinity,
+	 *   - kernel/irq/irqdesc.c|71| <<alloc_masks>> free_cpumask_var(desc->irq_common_data.effective_affinity);
+	 *   - kernel/irq/irqdesc.c|395| <<free_masks>> free_cpumask_var(desc->irq_common_data.effective_affinity);
+	 */
 	cpumask_var_t		effective_affinity;
 #endif
 #ifdef CONFIG_GENERIC_IRQ_IPI
@@ -226,6 +240,13 @@ struct irq_data {
  */
 enum {
 	IRQD_TRIGGER_MASK		= 0xf,
+	/*
+	 * 在以下使用IRQD_SETAFFINITY_PENDING:
+	 *   - kernel/irq/debugfs.c|119| <<global>> BIT_MASK_DESCR(IRQD_SETAFFINITY_PENDING),
+	 *   - include/linux/irq.h|258| <<irqd_is_setaffinity_pending>> return __irqd_to_state(d) & IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|214| <<irqd_set_move_pending>> __irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|219| <<irqd_clr_move_pending>> __irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;
+	 */
 	IRQD_SETAFFINITY_PENDING	= BIT(8),
 	IRQD_ACTIVATED			= BIT(9),
 	IRQD_NO_BALANCING		= BIT(10),
@@ -233,6 +254,13 @@ enum {
 	IRQD_AFFINITY_SET		= BIT(12),
 	IRQD_LEVEL			= BIT(13),
 	IRQD_WAKEUP_STATE		= BIT(14),
+	/*
+	 * 在以下使用IRQD_MOVE_PCNTXT：
+	 *   - kernel/irq/debugfs.c|117| <<global>> BIT_MASK_DESCR(IRQD_MOVE_PCNTXT),
+	 *   - include/linux/irq.h|357| <<irqd_can_move_in_process_context>> return __irqd_to_state(d) & IRQD_MOVE_PCNTXT;
+	 *   - kernel/irq/chip.c|1117| <<irq_modify_status>> IRQD_TRIGGER_MASK | IRQD_LEVEL | IRQD_MOVE_PCNTXT);
+	 *   - kernel/irq/chip.c|1123| <<irq_modify_status>> irqd_set(&desc->irq_data, IRQD_MOVE_PCNTXT);
+	 */
 	IRQD_MOVE_PCNTXT		= BIT(15),
 	IRQD_IRQ_DISABLED		= BIT(16),
 	IRQD_IRQ_MASKED			= BIT(17),
@@ -240,6 +268,13 @@ enum {
 	IRQD_WAKEUP_ARMED		= BIT(19),
 	IRQD_FORWARDED_TO_VCPU		= BIT(20),
 	IRQD_AFFINITY_MANAGED		= BIT(21),
+	/*
+	 * 在以下使用IRQD_IRQ_STARTED:
+	 *   - kernel/irq/debugfs.c|108| <<global>> BIT_MASK_DESCR(IRQD_IRQ_STARTED),
+	 *   - include/linux/irq.h|441| <<irqd_is_started>> return __irqd_to_state(d) & IRQD_IRQ_STARTED;
+	 *   - kernel/irq/chip.c|175| <<irq_state_clr_started>> irqd_clear(&desc->irq_data, IRQD_IRQ_STARTED);
+	 *   - kernel/irq/chip.c|180| <<irq_state_set_started>> irqd_set(&desc->irq_data, IRQD_IRQ_STARTED);
+	 */
 	IRQD_IRQ_STARTED		= BIT(22),
 	IRQD_MANAGED_SHUTDOWN		= BIT(23),
 	IRQD_SINGLE_TARGET		= BIT(24),
@@ -253,8 +288,25 @@ enum {
 
 #define __irqd_to_state(d) ACCESS_PRIVATE((d)->common, state_use_accessors)
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/io_apic.c|1723| <<ioapic_prepare_move>> if (unlikely(irqd_is_setaffinity_pending(data))) {
+ *   - include/linux/irq.h|658| <<irq_move_irq>> if (unlikely(irqd_is_setaffinity_pending(data)))
+ *   - kernel/irq/internals.h|447| <<irq_move_pending>> return irqd_is_setaffinity_pending(data);
+ *   - kernel/irq/manage.c|378| <<irq_set_affinity_locked>> if (irq_can_move_pcntxt(data) && !irqd_is_setaffinity_pending(data)) {
+ *   - kernel/irq/migration.c|37| <<irq_fixup_move_pending>> if (!irqd_is_setaffinity_pending(data))
+ *   - kernel/irq/migration.c|64| <<irq_move_masked_irq>> if (likely(!irqd_is_setaffinity_pending(data)))
+ *   - kernel/irq/proc.c|56| <<show_irq_affinity>> if (irqd_is_setaffinity_pending(&desc->irq_data))
+ */
 static inline bool irqd_is_setaffinity_pending(struct irq_data *d)
 {
+	/*
+	 * 在以下使用IRQD_SETAFFINITY_PENDING:
+	 *   - kernel/irq/debugfs.c|119| <<global>> BIT_MASK_DESCR(IRQD_SETAFFINITY_PENDING),
+	 *   - include/linux/irq.h|258| <<irqd_is_setaffinity_pending>> return __irqd_to_state(d) & IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|214| <<irqd_set_move_pending>> __irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|219| <<irqd_clr_move_pending>> __irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;
+	 */
 	return __irqd_to_state(d) & IRQD_SETAFFINITY_PENDING;
 }
 
@@ -338,8 +390,19 @@ static inline bool irqd_is_wakeup_set(struct irq_data *d)
 	return __irqd_to_state(d) & IRQD_WAKEUP_STATE;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/internals.h|443| <<irq_can_move_pcntxt>> return irqd_can_move_in_process_context(data);
+ */
 static inline bool irqd_can_move_in_process_context(struct irq_data *d)
 {
+	/*
+	 * 在以下使用IRQD_MOVE_PCNTXT：
+	 *   - kernel/irq/debugfs.c|117| <<global>> BIT_MASK_DESCR(IRQD_MOVE_PCNTXT),
+	 *   - include/linux/irq.h|357| <<irqd_can_move_in_process_context>> return __irqd_to_state(d) & IRQD_MOVE_PCNTXT;
+	 *   - kernel/irq/chip.c|1117| <<irq_modify_status>> IRQD_TRIGGER_MASK | IRQD_LEVEL | IRQD_MOVE_PCNTXT);
+	 *   - kernel/irq/chip.c|1123| <<irq_modify_status>> irqd_set(&desc->irq_data, IRQD_MOVE_PCNTXT);
+	 */
 	return __irqd_to_state(d) & IRQD_MOVE_PCNTXT;
 }
 
@@ -621,6 +684,10 @@ extern int irq_affinity_online_cpu(unsigned int cpu);
 
 #if defined(CONFIG_SMP) && defined(CONFIG_GENERIC_PENDING_IRQ)
 void __irq_move_irq(struct irq_data *data);
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|1118| <<apic_ack_irq>> irq_move_irq(irqd);
+ */
 static inline void irq_move_irq(struct irq_data *data)
 {
 	if (unlikely(irqd_is_setaffinity_pending(data)))
@@ -905,11 +972,64 @@ static inline const struct cpumask *irq_get_affinity_mask(int irq)
 static inline
 const struct cpumask *irq_data_get_effective_affinity_mask(struct irq_data *d)
 {
+	/*
+	 * 在以下使用irq_common_data->effective_affinity:
+	 *   - include/linux/irq.h|967| <<irq_data_get_effective_affinity_mask>> return d->common->effective_affinity;
+	 *   - include/linux/irq.h|1009| <<irq_data_update_effective_affinity>> cpumask_copy(d->common->effective_affinity, m);
+	 *   - kernel/irq/irqdesc.c|61| <<alloc_masks>> if (!zalloc_cpumask_var_node(&desc->irq_common_data.effective_affinity,
+	 *   - kernel/irq/irqdesc.c|71| <<alloc_masks>> free_cpumask_var(desc->irq_common_data.effective_affinity);
+	 *   - kernel/irq/irqdesc.c|395| <<free_masks>> free_cpumask_var(desc->irq_common_data.effective_affinity);
+	 */
 	return d->common->effective_affinity;
 }
+/*
+ * called by:
+ *   - arch/mips/lantiq/irq.c|251| <<ltq_icu_irq_set_affinity>> irq_data_update_effective_affinity(d, &tmask);
+ *   - arch/mips/lantiq/irq.c|326| <<icu_map>> irq_data_update_effective_affinity(data, cpumask_of(0));
+ *   - arch/mips/sgi-ip27/ip27-irq.c|108| <<set_affinity_hub_irq>> irq_data_update_effective_affinity(d, cpumask_of(hd->cpu));
+ *   - arch/mips/sgi-ip30/ip30-irq.c|188| <<ip30_set_heart_irq_affinity>> irq_data_update_effective_affinity(d, cpumask_of(hd->cpu));
+ *   - arch/x86/kernel/apic/vector.c|215| <<apic_update_irq_cfg>> irq_data_update_effective_affinity(irqd, cpumask_of(cpu));
+ *   - drivers/gpio/gpio-realtek-otto.c|301| <<realtek_gpio_irq_set_affinity>> irq_data_update_effective_affinity(data, dest);
+ *   - drivers/irqchip/irq-apple-aic.c|421| <<aic_irq_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-armada-370-xp.c|232| <<armada_370_xp_msi_set_affinity>> irq_data_update_effective_affinity(irq_data, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-armada-370-xp.c|483| <<armada_xp_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-bcm6345-l1.c|221| <<bcm6345_l1_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(new_cpu));
+ *   - drivers/irqchip/irq-bcm7038-l1.c|216| <<bcm7038_l1_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(first_cpu));
+ *   - drivers/irqchip/irq-csky-mpintc.c|158| <<csky_irq_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-gic-v3-its.c|1701| <<its_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-gic-v3-its.c|1812| <<its_map_vm>> irq_data_update_effective_affinity(d, cpumask_of(vpe->col_idx));
+ *   - drivers/irqchip/irq-gic-v3-its.c|3644| <<its_irq_domain_activate>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-gic-v3-its.c|3867| <<its_vpe_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-gic-v3-its.c|4266| <<its_sgi_set_affinity>> irq_data_update_effective_affinity(d, mask_val);
+ *   - drivers/irqchip/irq-gic-v3-its.c|4600| <<its_vpe_irq_domain_activate>> irq_data_update_effective_affinity(d, cpumask_of(vpe->col_idx));
+ *   - drivers/irqchip/irq-gic-v3.c|1448| <<gic_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-gic.c|818| <<gic_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-hip04.c|170| <<hip04_irq_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-loongson-eiointc.c|123| <<eiointc_set_irq_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-ls-scfg-msi.c|126| <<ls_scfg_msi_set_affinity>> irq_data_update_effective_affinity(irq_data, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-mips-gic.c|281| <<gic_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-mips-gic.c|443| <<gic_shared_irq_domain_map>> irq_data_update_effective_affinity(data, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-sifive-plic.c|180| <<plic_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/irqchip/irq-xtensa-mx.c|138| <<xtensa_mx_irq_set_affinity>> irq_data_update_effective_affinity(d, cpumask_of(cpu));
+ *   - drivers/parisc/dino.c|365| <<dino_set_affinity_irq>> irq_data_update_effective_affinity(d, &tmask);
+ *   - drivers/parisc/gsc.c|159| <<gsc_set_affinity_irq>> irq_data_update_effective_affinity(d, &tmask);
+ *   - drivers/pci/controller/pci-hyperv.c|878| <<hv_pci_vec_irq_domain_activate>> irq_data_update_effective_affinity(irqd, cpumask_of(cpu));
+ *   - drivers/pci/controller/pcie-iproc-msi.c|223| <<iproc_msi_irq_set_affinity>> irq_data_update_effective_affinity(data, cpumask_of(target_cpu));
+ *   - drivers/xen/events/events_base.c|520| <<bind_evtchn_to_cpu>> irq_data_update_effective_affinity(data, cpumask_of(cpu));
+ *   - drivers/xen/events/events_base.c|1836| <<set_affinity_irq>> irq_data_update_effective_affinity(data, cpumask_of(tcpu));
+ *   - kernel/irq/manage.c|347| <<irq_set_affinity_deactivated>> irq_data_update_effective_affinity(data, mask);
+ */
 static inline void irq_data_update_effective_affinity(struct irq_data *d,
 						      const struct cpumask *m)
 {
+	/*
+	 * 在以下使用irq_common_data->effective_affinity:
+	 *   - include/linux/irq.h|967| <<irq_data_get_effective_affinity_mask>> return d->common->effective_affinity;
+	 *   - include/linux/irq.h|1009| <<irq_data_update_effective_affinity>> cpumask_copy(d->common->effective_affinity, m);
+	 *   - kernel/irq/irqdesc.c|61| <<alloc_masks>> if (!zalloc_cpumask_var_node(&desc->irq_common_data.effective_affinity,
+	 *   - kernel/irq/irqdesc.c|71| <<alloc_masks>> free_cpumask_var(desc->irq_common_data.effective_affinity);
+	 *   - kernel/irq/irqdesc.c|395| <<free_masks>> free_cpumask_var(desc->irq_common_data.effective_affinity);
+	 */
 	cpumask_copy(d->common->effective_affinity, m);
 }
 #else
diff --git a/include/linux/irqchip/arm-gic-v4.h b/include/linux/irqchip/arm-gic-v4.h
index 2c63375bb..c69263db9 100644
--- a/include/linux/irqchip/arm-gic-v4.h
+++ b/include/linux/irqchip/arm-gic-v4.h
@@ -32,6 +32,21 @@ struct its_vm {
 struct its_vpe {
 	struct page 		*vpt_page;
 	struct its_vm		*its_vm;
+	/*
+	 * 在以下使用its_vm->vlpi_count[GICv4_ITS_LIST_MAX]:
+	 *   - drivers/irqchip/irq-gic-v3-its.c|207| <<require_its_list_vmovp>> return (gic_rdists->has_rvpeid || vm->vlpi_count[its->list_nr]);
+	 *   - drivers/irqchip/irq-gic-v3-its.c|1799| <<its_map_vm>> vm->vlpi_count[its->list_nr]++;
+	 *   - drivers/irqchip/irq-gic-v3-its.c|1801| <<its_map_vm>> if (vm->vlpi_count[its->list_nr] == 1) {
+	 *   - drivers/irqchip/irq-gic-v3-its.c|1829| <<its_unmap_vm>> if (!--vm->vlpi_count[its->list_nr]) {
+	 *   - drivers/irqchip/irq-gic-v3-its.c|3942| <<its_vpe_invall>> if (its_list_map && !vpe->its_vm->vlpi_count[its->list_nr])
+	 * 在以下使用its_vpe->vlpi_count:
+	 *   - arch/arm64/include/asm/kvm_emulate.h|119| <<vcpu_clear_wfx_traps>> if (atomic_read(&vcpu->arch.vgic_cpu.vgic_v3.its_vpe.vlpi_count) ||
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|205| <<kvm_vgic_vcpu_init>> atomic_set(&vgic_cpu->vgic_v3.its_vpe.vlpi_count, 0);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|371| <<update_affinity>> atomic_dec(&map.vpe->vlpi_count);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|373| <<update_affinity>> atomic_inc(&map.vpe->vlpi_count);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|465| <<kvm_vgic_v4_set_forwarding>> atomic_inc(&map.vpe->vlpi_count);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|517| <<kvm_vgic_v4_unset_forwarding>> atomic_dec(&irq->target_vcpu->arch.vgic_cpu.vgic_v3.its_vpe.vlpi_count);
+	 */
 	/* per-vPE VLPI tracking */
 	atomic_t		vlpi_count;
 	/* Doorbell interrupt */
diff --git a/include/linux/irqdesc.h b/include/linux/irqdesc.h
index d9451d456..bd7847f8c 100644
--- a/include/linux/irqdesc.h
+++ b/include/linux/irqdesc.h
@@ -72,9 +72,35 @@ struct irq_desc {
 	struct cpumask		*percpu_enabled;
 	const struct cpumask	*percpu_affinity;
 #ifdef CONFIG_SMP
+	/*
+	 * 在以下使用irq_desc->affinity_hint:
+	 *   - kernel/irq/manage.c|509| <<__irq_apply_affinity_hint>> desc->affinity_hint = m;
+	 *   - kernel/irq/manage.c|1919| <<__free_irq>> if (WARN_ON_ONCE(desc->affinity_hint))
+	 *   - kernel/irq/manage.c|1920| <<__free_irq>> desc->affinity_hint = NULL;
+	 *   - kernel/irq/proc.c|93| <<irq_affinity_hint_proc_show>> if (desc->affinity_hint)
+	 *   - kernel/irq/proc.c|94| <<irq_affinity_hint_proc_show>> cpumask_copy(mask, desc->affinity_hint);
+	 *
+	 * 注释: hint to user space for preferred irq affinity
+	 */
 	const struct cpumask	*affinity_hint;
 	struct irq_affinity_notify *affinity_notify;
 #ifdef CONFIG_GENERIC_PENDING_IRQ
+	/*
+	 * 在以下使用irq_desc->pending_mask:
+	 *   - kernel/irq/debugfs.c|42| <<irq_debug_show_masks>> msk = desc->pending_mask;
+	 *   - kernel/irq/internals.h|452| <<irq_copy_pending>> cpumask_copy(desc->pending_mask, mask);
+	 *   - kernel/irq/internals.h|457| <<irq_get_pending>> cpumask_copy(mask, desc->pending_mask);
+	 *   - kernel/irq/internals.h|461| <<irq_desc_get_pending_mask>> return desc->pending_mask;
+	 *   - kernel/irq/irqdesc.c|69| <<alloc_masks>> if (!zalloc_cpumask_var_node(&desc->pending_mask, GFP_KERNEL, node)) {
+	 *   - kernel/irq/irqdesc.c|88| <<desc_smp_init>> cpumask_clear(desc->pending_mask);
+	 *   - kernel/irq/irqdesc.c|391| <<free_masks>> free_cpumask_var(desc->pending_mask);
+	 *   - kernel/irq/migration.c|34| <<irq_fixup_move_pending>> if (cpumask_any_and(desc->pending_mask, cpu_online_mask) >= nr_cpu_ids) {
+	 *   - kernel/irq/migration.c|67| <<irq_move_masked_irq>> if (unlikely(cpumask_empty(desc->pending_mask)))
+	 *   - kernel/irq/migration.c|87| <<irq_move_masked_irq>> if (cpumask_any_and(desc->pending_mask, cpu_online_mask) < nr_cpu_ids) {
+	 *   - kernel/irq/migration.c|90| <<irq_move_masked_irq>> ret = irq_do_set_affinity(data, desc->pending_mask, false);
+	 *   - kernel/irq/migration.c|101| <<irq_move_masked_irq>> cpumask_clear(desc->pending_mask);
+	 *   - kernel/irq/proc.c|57| <<show_irq_affinity>> mask = desc->pending_mask;
+	 */
 	cpumask_var_t		pending_mask;
 #endif
 #endif
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 179df96b2..527dba7f1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -818,15 +818,47 @@ struct kvm {
 	gfn_t mmu_invalidate_range_start;
 	gfn_t mmu_invalidate_range_end;
 #endif
+	/*
+	 * 在以下使用kvm->devices:
+	 *   - virt/kvm/kvm_main.c|1219| <<kvm_create_vm>> INIT_LIST_HEAD(&kvm->devices);
+	 *   - virt/kvm/kvm_main.c|1335| <<kvm_destroy_devices>> list_for_each_entry_safe(dev, tmp, &kvm->devices, vm_node) {
+	 *   - virt/kvm/kvm_main.c|4868| <<kvm_ioctl_create_device>> list_add(&dev->vm_node, &kvm->devices);
+	 *   - virt/kvm/vfio.c|370| <<kvm_vfio_create>> list_for_each_entry(tmp, &dev->kvm->devices, vm_node)
+	 */
 	struct list_head devices;
+	/*
+	 * 在以下使用kvm->manual_dirty_log_protect:
+	 *   - include/linux/kvm_host.h|958| <<kvm_dirty_log_manual_protect_and_init_set>> return !!(kvm->manual_dirty_log_protect & KVM_DIRTY_LOG_INITIALLY_SET);
+	 *   - virt/kvm/kvm_main.c|2291| <<kvm_get_dirty_log_protect>> if (kvm->manual_dirty_log_protect) {
+	 *   - virt/kvm/kvm_main.c|5071| <<kvm_vm_ioctl_enable_cap_generic(KVM_CAP_MANUAL_DIRTY_LOG_PROTECT2)>> kvm->manual_dirty_log_protect = cap->args[0];
+	 */
 	u64 manual_dirty_log_protect;
 	struct dentry *debugfs_dentry;
 	struct kvm_stat_data **debugfs_stat_data;
 	struct srcu_struct srcu;
 	struct srcu_struct irq_srcu;
 	pid_t userspace_pid;
+	/*
+	 * 在以下使用kvm->override_halt_poll_ns:
+	 *   - virt/kvm/kvm_main.c|3926| <<kvm_vcpu_max_halt_poll_ns>> if (kvm->override_halt_poll_ns) {
+	 *   - virt/kvm/kvm_main.c|5121| <<kvm_vm_ioctl_enable_cap_generic>> kvm->override_halt_poll_ns = true;
+	 */
 	bool override_halt_poll_ns;
 	unsigned int max_halt_poll_ns;
+	/*
+	 * 在以下使用kvm->dirty_ring_size:
+	 *   - virt/kvm/dirty_ring.c|28| <<kvm_use_dirty_bitmap>> return !kvm->dirty_ring_size || kvm->dirty_ring_with_bitmap;
+	 *   - virt/kvm/kvm_main.c|3752| <<mark_page_dirty_in_slot>> if (kvm->dirty_ring_size && vcpu)
+	 *   - virt/kvm/kvm_main.c|4217| <<kvm_page_in_dirty_ring>> return (pgoff >= KVM_DIRTY_LOG_PAGE_OFFSET) &&
+	 *                                            (pgoff < KVM_DIRTY_LOG_PAGE_OFFSET + kvm->dirty_ring_size / PAGE_SIZE);
+	 *   - virt/kvm/kvm_main.c|4372| <<kvm_vm_ioctl_create_vcpu>> if (kvm->dirty_ring_size) {
+	 *   - virt/kvm/kvm_main.c|4374| <<kvm_vm_ioctl_create_vcpu>> r = kvm_dirty_ring_alloc(&vcpu->dirty_ring, id, kvm->dirty_ring_size);
+	 *   - virt/kvm/kvm_main.c|5013| <<kvm_vm_ioctl_enable_dirty_log_ring>> if (kvm->dirty_ring_size)
+	 *   - virt/kvm/kvm_main.c|5022| <<kvm_vm_ioctl_enable_dirty_log_ring>> kvm->dirty_ring_size = size;
+	 *   - virt/kvm/kvm_main.c|5036| <<kvm_vm_ioctl_reset_dirty_pages>> if (!kvm->dirty_ring_size)
+	 *   - virt/kvm/kvm_main.c|5117| <<kvm_vm_ioctl_enable_cap_generic>> if (!IS_ENABLED(CONFIG_NEED_KVM_DIRTY_RING_WITH_BITMAP) ||
+	 *                                             !kvm->dirty_ring_size || cap->flags)
+	 */
 	u32 dirty_ring_size;
 	bool dirty_ring_with_bitmap;
 	bool vm_bugged;
@@ -836,6 +868,15 @@ struct kvm {
 	struct notifier_block pm_notifier;
 #endif
 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
+	/*
+	 * 在以下使用kvm->mem_attr_array:
+	 *   - include/linux/kvm_host.h|2381| <<kvm_get_memory_attributes>> return xa_to_value(xa_load(&kvm->mem_attr_array, gfn));
+	 *   - virt/kvm/kvm_main.c|1213| <<kvm_create_vm>> xa_init(&kvm->mem_attr_array);
+	 *   - virt/kvm/kvm_main.c|1395| <<kvm_destroy_vm>> xa_destroy(&kvm->mem_attr_array);
+	 *   - virt/kvm/kvm_main.c|2469| <<kvm_range_has_memory_attributes>> XA_STATE(xas, &kvm->mem_attr_array, start);
+	 *   - virt/kvm/kvm_main.c|2606| <<kvm_vm_set_mem_attributes>> r = xa_reserve(&kvm->mem_attr_array, i, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|2614| <<kvm_vm_set_mem_attributes>> r = xa_err(xa_store(&kvm->mem_attr_array, i, entry, GFP_KERNEL_ACCOUNT));
+	 */
 	/* Protected by slots_locks (for writes) and RCU (for reads) */
 	struct xarray mem_attr_array;
 #endif
@@ -2378,6 +2419,15 @@ static inline void kvm_prepare_memory_fault_exit(struct kvm_vcpu *vcpu,
 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
 static inline unsigned long kvm_get_memory_attributes(struct kvm *kvm, gfn_t gfn)
 {
+	/*
+	 * 在以下使用kvm->mem_attr_array:
+	 *   - include/linux/kvm_host.h|2381| <<kvm_get_memory_attributes>> return xa_to_value(xa_load(&kvm->mem_attr_array, gfn));
+	 *   - virt/kvm/kvm_main.c|1213| <<kvm_create_vm>> xa_init(&kvm->mem_attr_array);
+	 *   - virt/kvm/kvm_main.c|1395| <<kvm_destroy_vm>> xa_destroy(&kvm->mem_attr_array);
+	 *   - virt/kvm/kvm_main.c|2469| <<kvm_range_has_memory_attributes>> XA_STATE(xas, &kvm->mem_attr_array, start);
+	 *   - virt/kvm/kvm_main.c|2606| <<kvm_vm_set_mem_attributes>> r = xa_reserve(&kvm->mem_attr_array, i, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|2614| <<kvm_vm_set_mem_attributes>> r = xa_err(xa_store(&kvm->mem_attr_array, i, entry, GFP_KERNEL_ACCOUNT));
+	 */
 	return xa_to_value(xa_load(&kvm->mem_attr_array, gfn));
 }
 
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index d2a15c0c6..cf6e97064 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -894,6 +894,15 @@ struct perf_event_pmu_context {
 	 * events not necessary to be active due to scheduling constraints,
 	 * such as cgroups.
 	 */
+	/*
+	 * 在以下使用perf_event_pmu_context->rotate_necessary:
+	 *   - kernel/events/core.c|2406| <<__perf_remove_from_context>> pmu_ctx->rotate_necessary = 0;
+	 *   - kernel/events/core.c|3325| <<__pmu_ctx_sched_out>> pmu_ctx->rotate_necessary = 0;
+	 *   - kernel/events/core.c|3910| <<merge_sched_in>> event->pmu_ctx->rotate_necessary = 1;
+	 *   - kernel/events/core.c|4329| <<ctx_event_to_rotate>> pmu_ctx->rotate_necessary = 0;
+	 *   - kernel/events/core.c|4355| <<perf_rotate_context>> cpu_rotate = cpu_epc->rotate_necessary;
+	 *   - kernel/events/core.c|4356| <<perf_rotate_context>> task_rotate = task_epc ? task_epc->rotate_necessary : 0;
+	 */
 	int				rotate_necessary;
 };
 
diff --git a/include/linux/virtio_config.h b/include/linux/virtio_config.h
index da9b271b5..c39e9534a 100644
--- a/include/linux/virtio_config.h
+++ b/include/linux/virtio_config.h
@@ -318,6 +318,15 @@ const char *virtio_bus_name(struct virtio_device *vdev)
  * due to config support, irq type and sharing.
  *
  */
+/*
+ * called by:
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|191| <<virtcrypto_clean_affinity>> virtqueue_set_affinity(vi->data_vq[i].vq, NULL);
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|218| <<virtcrypto_set_affinity>> virtqueue_set_affinity(vcrypto->data_vq[i].vq, cpumask_of(cpu));
+ *   - drivers/net/virtio_net.c|2815| <<virtnet_clean_affinity>> virtqueue_set_affinity(vi->rq[i].vq, NULL);
+ *   - drivers/net/virtio_net.c|2816| <<virtnet_clean_affinity>> virtqueue_set_affinity(vi->sq[i].vq, NULL);
+ *   - drivers/net/virtio_net.c|2852| <<virtnet_set_affinity>> virtqueue_set_affinity(vi->rq[i].vq, mask);
+ *   - drivers/net/virtio_net.c|2853| <<virtnet_set_affinity>> virtqueue_set_affinity(vi->sq[i].vq, mask);
+ */
 static inline
 int virtqueue_set_affinity(struct virtqueue *vq, const struct cpumask *cpu_mask)
 {
diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 3a64499b0..98bc435a2 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -548,6 +548,15 @@ struct perf_event_query_bpf {
 /*
  * Ioctls that can be done on a perf event fd:
  */
+/*
+ * PERF_EVENT_IOC_ENABLE主要被perf相关的调用:
+ *   - include/uapi/linux/perf_event.h|551| <<global>> #define PERF_EVENT_IOC_ENABLE _IO ('$', 0)
+ *   - tools/include/uapi/linux/perf_event.h|551| <<global>> #define PERF_EVENT_IOC_ENABLE _IO ('$', 0)
+ *   - kernel/events/core.c|5880| <<_perf_ioctl>> case PERF_EVENT_IOC_ENABLE:
+ *   - tools/lib/perf/evsel.c|444| <<perf_evsel__enable_cpu>> return perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, cpu_map_idx);
+ *   - tools/lib/perf/evsel.c|454| <<perf_evsel__enable_thread>> err = perf_evsel__ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, idx, thread);
+ *   - tools/lib/perf/evsel.c|468| <<perf_evsel__enable>> err = perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, i);
+ */
 #define PERF_EVENT_IOC_ENABLE			_IO ('$', 0)
 #define PERF_EVENT_IOC_DISABLE			_IO ('$', 1)
 #define PERF_EVENT_IOC_REFRESH			_IO ('$', 2)
diff --git a/kernel/cpu.c b/kernel/cpu.c
index e6ec3ba49..b82aaf44e 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -1301,6 +1301,10 @@ void clear_tasks_mm_cpumask(int cpu)
 	rcu_read_unlock();
 }
 
+/*
+ * 在以下使用take_cpu_down():
+ *   - kernel/cpu.c|1353| <<takedown_cpu>> err = stop_machine_cpuslocked(take_cpu_down, NULL, cpumask_of(cpu));
+ */
 /* Take this CPU down. */
 static int take_cpu_down(void *_param)
 {
@@ -1333,6 +1337,22 @@ static int take_cpu_down(void *_param)
 	return 0;
 }
 
+/*
+ * [0] takedown_cpu
+ * [0] cpuhp_invoke_callback
+ * [0] __cpuhp_invoke_callback_range
+ * [0] _cpu_down
+ * [0] __cpu_down_maps_locked
+ * [0] work_for_cpu_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * 在以下使用takedown_cpu():
+ *   - kernel/cpu.c|2221| <<global>> .teardown.single = takedown_cpu,
+ */
 static int takedown_cpu(unsigned int cpu)
 {
 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
diff --git a/kernel/events/core.c b/kernel/events/core.c
index f0f0f7121..f567fabef 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -634,6 +634,16 @@ __perf_update_times(struct perf_event *event, u64 now, u64 *enabled, u64 *runnin
 		*running += delta;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|651| <<perf_event_update_sibling_time>> perf_event_update_time(sibling);
+ *   - kernel/events/core.c|660| <<perf_event_set_state>> perf_event_update_time(event);
+ *   - kernel/events/core.c|3397| <<__perf_event_sync_stat>> perf_event_update_time(event);
+ *   - kernel/events/core.c|3814| <<event_update_userpage>> perf_event_update_time(event);
+ *   - kernel/events/core.c|4503| <<__perf_event_read>> perf_event_update_time(event);
+ *   - kernel/events/core.c|4703| <<perf_event_read>> perf_event_update_time(event);
+ *   - kernel/events/core.c|6627| <<perf_mmap>> perf_event_update_time(event);
+ */
 static void perf_event_update_time(struct perf_event *event)
 {
 	u64 now = perf_event_time(event);
@@ -696,6 +706,17 @@ static void perf_ctx_disable(struct perf_event_context *ctx, bool cgroup)
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|882| <<perf_cgroup_switch>> perf_ctx_enable(&cpuctx->ctx, true);
+ *   - kernel/events/core.c|2738| <<ctx_resched>> perf_ctx_enable(&cpuctx->ctx, false);
+ *   - kernel/events/core.c|2740| <<ctx_resched>> perf_ctx_enable(task_ctx, false);
+ *   - kernel/events/core.c|3549| <<perf_event_context_sched_out>> perf_ctx_enable(ctx, false);
+ *   - kernel/events/core.c|3579| <<perf_event_context_sched_out>> perf_ctx_enable(ctx, false);
+ *   - kernel/events/core.c|3956| <<perf_event_context_sched_in>> perf_ctx_enable(ctx, false);
+ *   - kernel/events/core.c|3988| <<perf_event_context_sched_in>> perf_ctx_enable(&cpuctx->ctx, false);
+ *   - kernel/events/core.c|3990| <<perf_event_context_sched_in>> perf_ctx_enable(ctx, false);
+ */
 static void perf_ctx_enable(struct perf_event_context *ctx, bool cgroup)
 {
 	struct perf_event_pmu_context *pmu_ctx;
@@ -1098,6 +1119,10 @@ static enum hrtimer_restart perf_mux_hrtimer_handler(struct hrtimer *hr)
 	return rotations ? HRTIMER_RESTART : HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|11654| <<perf_pmu_register>> __perf_mux_hrtimer_init(cpc, cpu);
+ */
 static void __perf_mux_hrtimer_init(struct perf_cpu_pmu_context *cpc, int cpu)
 {
 	struct hrtimer *timer = &cpc->hrtimer;
@@ -1119,6 +1144,11 @@ static void __perf_mux_hrtimer_init(struct perf_cpu_pmu_context *cpc, int cpu)
 	timer->function = perf_mux_hrtimer_handler;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|1165| <<perf_mux_hrtimer_restart_ipi>> return perf_mux_hrtimer_restart(arg);
+ *   - kernel/events/core.c|3903| <<merge_sched_in>> perf_mux_hrtimer_restart(cpc);
+ */
 static int perf_mux_hrtimer_restart(struct perf_cpu_pmu_context *cpc)
 {
 	struct hrtimer *timer = &cpc->hrtimer;
@@ -1135,6 +1165,10 @@ static int perf_mux_hrtimer_restart(struct perf_cpu_pmu_context *cpc)
 	return 0;
 }
 
+/*
+ * 在以下使用perf_mux_hrtimer_restart_ipi():
+ *   - kernel/events/core.c|11520| <<perf_event_mux_interval_ms_store>> cpu_function_call(cpu, perf_mux_hrtimer_restart_ipi, cpc);
+ */
 static int perf_mux_hrtimer_restart_ipi(void *arg)
 {
 	return perf_mux_hrtimer_restart(arg);
@@ -2512,6 +2546,11 @@ void perf_event_disable_inatomic(struct perf_event *event)
 static void perf_log_throttle(struct perf_event *event, int enable);
 static void perf_log_itrace_start(struct perf_event *event);
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2605| <<group_sched_in>> if (event_sched_in(group_event, ctx))
+ *   - kernel/events/core.c|2612| <<group_sched_in>> if (event_sched_in(event, ctx)) {
+ */
 static int
 event_sched_in(struct perf_event *event, struct perf_event_context *ctx)
 {
@@ -2549,6 +2588,11 @@ event_sched_in(struct perf_event *event, struct perf_event_context *ctx)
 
 	perf_log_itrace_start(event);
 
+	/*
+	 * 似乎这个add比较慢
+	 *
+	 * x86_pmu_add()
+	 */
 	if (event->pmu->add(event, PERF_EF_START)) {
 		perf_event_set_state(event, PERF_EVENT_STATE_INACTIVE);
 		event->oncpu = -1;
@@ -2570,6 +2614,10 @@ event_sched_in(struct perf_event *event, struct perf_event_context *ctx)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|3869| <<merge_sched_in>> if (!group_sched_in(event, ctx))
+ */
 static int
 group_sched_in(struct perf_event *group_event, struct perf_event_context *ctx)
 {
@@ -2581,6 +2629,11 @@ group_sched_in(struct perf_event *group_event, struct perf_event_context *ctx)
 
 	pmu->start_txn(pmu, PERF_PMU_TXN_ADD);
 
+	/*
+	 * called by:
+	 *   - kernel/events/core.c|2605| <<group_sched_in>> if (event_sched_in(group_event, ctx))
+	 *   - kernel/events/core.c|2612| <<group_sched_in>> if (event_sched_in(event, ctx)) {
+	 */
 	if (event_sched_in(group_event, ctx))
 		goto error;
 
@@ -2588,6 +2641,11 @@ group_sched_in(struct perf_event *group_event, struct perf_event_context *ctx)
 	 * Schedule in siblings as one group (if any):
 	 */
 	for_each_sibling_event(event, group_event) {
+		/*
+		 * called by:
+		 *   - kernel/events/core.c|2605| <<group_sched_in>> if (event_sched_in(group_event, ctx)
+		 *   - kernel/events/core.c|2612| <<group_sched_in>> if (event_sched_in(event, ctx)) {
+		 */
 		if (event_sched_in(event, ctx)) {
 			partial_group = event;
 			goto group_error;
@@ -2669,6 +2727,11 @@ static void task_ctx_sched_out(struct perf_event_context *ctx,
 	ctx_sched_out(ctx, event_type);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2736| <<ctx_resched>> perf_event_sched_in(cpuctx, task_ctx);
+ *   - kernel/events/core.c|3983| <<perf_event_context_sched_in>> perf_event_sched_in(cpuctx, ctx);
+ */
 static void perf_event_sched_in(struct perf_cpu_context *cpuctx,
 				struct perf_event_context *ctx)
 {
@@ -3828,6 +3891,10 @@ static inline void group_update_userpage(struct perf_event *group_event)
 		event_update_userpage(event);
 }
 
+/*
+ * 在以下使用merge_sched_in():
+ *   - kernel/events/core.c|3897| <<pmu_groups_sched_in>> visit_groups_merge(ctx, groups, smp_processor_id(), pmu, merge_sched_in, &can_add_hw);
+ */
 static int merge_sched_in(struct perf_event *event, void *data)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -3839,7 +3906,13 @@ static int merge_sched_in(struct perf_event *event, void *data)
 	if (!event_filter_match(event))
 		return 0;
 
+	/*
+	 * 似乎这个if语句整体慢
+	 */
 	if (group_can_go_on(event, *can_add_hw)) {
+		/*
+		 * 只在此处调用
+		 */
 		if (!group_sched_in(event, ctx))
 			list_add_tail(&event->active_list, get_event_list(event));
 	}
@@ -3862,6 +3935,11 @@ static int merge_sched_in(struct perf_event *event, void *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|3938| <<ctx_groups_sched_in>> pmu_groups_sched_in(ctx, groups, pmu_ctx->pmu);
+ *   - kernel/events/core.c|3945| <<__pmu_ctx_sched_in>> pmu_groups_sched_in(ctx, &ctx->flexible_groups, pmu);
+ */
 static void pmu_groups_sched_in(struct perf_event_context *ctx,
 				struct perf_event_groups *groups,
 				struct pmu *pmu)
@@ -3871,6 +3949,11 @@ static void pmu_groups_sched_in(struct perf_event_context *ctx,
 			   merge_sched_in, &can_add_hw);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|4025| <<ctx_sched_in>> ctx_groups_sched_in(ctx, &ctx->pinned_groups, cgroup);
+ *   - kernel/events/core.c|4029| <<ctx_sched_in>> ctx_groups_sched_in(ctx, &ctx->flexible_groups, cgroup);
+ */
 static void ctx_groups_sched_in(struct perf_event_context *ctx,
 				struct perf_event_groups *groups,
 				bool cgroup)
@@ -3884,12 +3967,28 @@ static void ctx_groups_sched_in(struct perf_event_context *ctx,
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|4382| <<perf_rotate_context>> __pmu_ctx_sched_in(&cpuctx->ctx, pmu);
+ *   - kernel/events/core.c|4389| <<perf_rotate_context>> __pmu_ctx_sched_in(task_epc->ctx, pmu);
+ */
 static void __pmu_ctx_sched_in(struct perf_event_context *ctx,
 			       struct pmu *pmu)
 {
 	pmu_groups_sched_in(ctx, &ctx->flexible_groups, pmu);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|880| <<perf_cgroup_switch>> ctx_sched_in(&cpuctx->ctx, EVENT_ALL|EVENT_CGROUP);
+ *   - kernel/events/core.c|2675| <<perf_event_sched_in>> ctx_sched_in(&cpuctx->ctx, EVENT_PINNED);
+ *   - kernel/events/core.c|2677| <<perf_event_sched_in>> ctx_sched_in(ctx, EVENT_PINNED);
+ *   - kernel/events/core.c|2678| <<perf_event_sched_in>> ctx_sched_in(&cpuctx->ctx, EVENT_FLEXIBLE);
+ *   - kernel/events/core.c|2680| <<perf_event_sched_in>> ctx_sched_in(ctx, EVENT_FLEXIBLE);
+ *   - kernel/events/core.c|2960| <<__perf_event_enable>> ctx_sched_in(ctx, EVENT_TIME);
+ *   - kernel/events/core.c|2969| <<__perf_event_enable>> ctx_sched_in(ctx, EVENT_TIME);
+ *   - kernel/events/core.c|4395| <<perf_event_enable_on_exec>> ctx_sched_in(ctx, EVENT_TIME);
+ */
 static void
 ctx_sched_in(struct perf_event_context *ctx, enum event_type_t event_type)
 {
@@ -4006,6 +4105,10 @@ static void perf_event_context_sched_in(struct task_struct *task)
  * accessing the event control register. If a NMI hits, then it will
  * keep the event running.
  */
+/*
+ * called by:
+ *   - include/linux/perf_event.h|1484| <<perf_event_task_sched_in>> __perf_event_task_sched_in(prev, task);
+ */
 void __perf_event_task_sched_in(struct task_struct *prev,
 				struct task_struct *task)
 {
@@ -4261,6 +4364,10 @@ ctx_event_to_rotate(struct perf_event_pmu_context *pmu_ctx)
 	return event;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|1110| <<perf_mux_hrtimer_handler>> rotations = perf_rotate_context(cpc);
+ */
 static bool perf_rotate_context(struct perf_cpu_pmu_context *cpc)
 {
 	struct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);
@@ -5472,6 +5579,15 @@ static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *
 	return total;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|127| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/riscv/kvm/vcpu_pmu.c|213| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/riscv/kvm/vcpu_pmu.c|437| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/x86/kvm/pmu.h|72| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - include/uapi/linux/bpf.h|5739| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+ *   - tools/include/uapi/linux/bpf.h|5739| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+ */
 u64 perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)
 {
 	struct perf_event_context *ctx;
@@ -7192,6 +7308,10 @@ void perf_event__output_id_sample(struct perf_event *event,
 		__perf_event__output_id_sample(handle, sample);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|7306| <<perf_output_read>> perf_output_read_one(handle, event, enabled, running);
+ */
 static void perf_output_read_one(struct perf_output_handle *handle,
 				 struct perf_event *event,
 				 u64 enabled, u64 running)
@@ -7282,6 +7402,11 @@ static void perf_output_read_group(struct perf_output_handle *handle,
  * child list, not to mention that its impossible to IPI the children running
  * on another CPU, from interrupt/NMI context.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|7346| <<perf_output_sample>> perf_output_read(handle, event);
+ *   - kernel/events/core.c|7947| <<perf_event_read_event>> perf_output_read(&handle, event);
+ */
 static void perf_output_read(struct perf_output_handle *handle,
 			     struct perf_event *event)
 {
@@ -7306,6 +7431,13 @@ static void perf_output_read(struct perf_output_handle *handle,
 		perf_output_read_one(handle, event, enabled, running);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/perf/imc-pmu.c|1352| <<dump_trace_imc_data>> perf_output_sample(&handle, &header, &data, event);
+ *   - arch/s390/kernel/perf_cpum_sf.c|725| <<cpumsf_output_event_pid>> perf_output_sample(&handle, &header, data, event);
+ *   - arch/x86/events/intel/ds.c|833| <<intel_pmu_drain_bts_buffer>> perf_output_sample(&handle, &header, &data, event);
+ *   - kernel/events/core.c|7880| <<__perf_event_output>> perf_output_sample(&handle, &header, data, event);
+ */
 void perf_output_sample(struct perf_output_handle *handle,
 			struct perf_event_header *header,
 			struct perf_sample_data *data,
@@ -7854,6 +7986,12 @@ void perf_prepare_header(struct perf_event_header *header,
 	WARN_ON_ONCE(header->size & 7);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|7894| <<perf_event_output_forward>> __perf_event_output(event, data, regs, perf_output_begin_forward);
+ *   - kernel/events/core.c|7902| <<perf_event_output_backward>> __perf_event_output(event, data, regs, perf_output_begin_backward);
+ *   - kernel/events/core.c|7910| <<perf_event_output>> return __perf_event_output(event, data, regs, perf_output_begin);
+ */
 static __always_inline int
 __perf_event_output(struct perf_event *event,
 		    struct perf_sample_data *data,
@@ -7921,6 +8059,10 @@ struct perf_read_event {
 	u32				tid;
 };
 
+/*
+ * called by:
+ *   - kernel/events/core.c|13050| <<sync_child_event>> perf_event_read_event(child_event, task);
+ */
 static void
 perf_event_read_event(struct perf_event *event,
 			struct task_struct *task)
@@ -11554,6 +11696,23 @@ int perf_pmu_register(struct pmu *pmu, const char *name, int type)
 	if (!pmu->cpu_pmu_context)
 		goto free_dev;
 
+	/*
+	 * 976 struct perf_cpu_pmu_context {
+	 * 977         struct perf_event_pmu_context   epc;
+	 * 978         struct perf_event_pmu_context   *task_epc;
+	 * 979
+	 * 980         struct list_head                sched_cb_entry;
+	 * 981         int                             sched_cb_usage;
+	 * 982
+	 * 983         int                             active_oncpu;
+	 * 984         int                             exclusive;
+	 * 985
+	 * 986         raw_spinlock_t                  hrtimer_lock;
+	 * 987         struct hrtimer                  hrtimer;
+	 * 988         ktime_t                         hrtimer_interval;
+	 * 989         unsigned int                    hrtimer_active;
+	 * 990 };
+	 */
 	for_each_possible_cpu(cpu) {
 		struct perf_cpu_pmu_context *cpc;
 
@@ -12820,6 +12979,19 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|656| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/riscv/kvm/vcpu_pmu.c|250| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(attr, -1, current, NULL, pmc);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|968| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu,
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|973| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu,
+ *   - arch/x86/kvm/pmu.c|222| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|293| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1,
+ *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered,
+ *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL,
+ *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>> return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+ *   - kernel/watchdog_perf.c|123| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
diff --git a/kernel/irq/chip.c b/kernel/irq/chip.c
index dc94e0bf2..e3facdda5 100644
--- a/kernel/irq/chip.c
+++ b/kernel/irq/chip.c
@@ -175,6 +175,10 @@ static void irq_state_clr_started(struct irq_desc *desc)
 	irqd_clear(&desc->irq_data, IRQD_IRQ_STARTED);
 }
 
+/*
+ * called by:
+ *   - kernel/irq/chip.c|250| <<__irq_startup>> irq_state_set_started(desc);
+ */
 static void irq_state_set_started(struct irq_desc *desc)
 {
 	irqd_set(&desc->irq_data, IRQD_IRQ_STARTED);
@@ -232,6 +236,11 @@ __irq_startup_managed(struct irq_desc *desc, const struct cpumask *aff,
 }
 #endif
 
+/*
+ * called by:
+ *   - kernel/irq/chip.c|269| <<irq_startup>> ret = __irq_startup(desc);
+ *   - kernel/irq/chip.c|275| <<irq_startup>> ret = __irq_startup(desc);
+ */
 static int __irq_startup(struct irq_desc *desc)
 {
 	struct irq_data *d = irq_desc_get_irq_data(desc);
@@ -251,6 +260,13 @@ static int __irq_startup(struct irq_desc *desc)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/chip.c|301| <<irq_activate_and_startup>> return irq_startup(desc, resend, IRQ_START_FORCE);
+ *   - kernel/irq/cpuhotplug.c|247| <<irq_restore_affinity_of_irq>> irq_startup(desc, IRQ_RESEND, IRQ_START_COND);
+ *   - kernel/irq/manage.c|849| <<__enable_irq>> irq_startup(desc, IRQ_RESEND, IRQ_START_FORCE);
+ *   - kernel/irq/manage.c|1822| <<__setup_irq>> irq_startup(desc, IRQ_RESEND, IRQ_START_COND);
+ */
 int irq_startup(struct irq_desc *desc, bool resend, bool force)
 {
 	struct irq_data *d = irq_desc_get_irq_data(desc);
@@ -1095,6 +1111,30 @@ irq_set_chip_and_handler_name(unsigned int irq, const struct irq_chip *chip,
 }
 EXPORT_SYMBOL_GPL(irq_set_chip_and_handler_name);
 
+/*
+ * called by:
+ *   - arch/arm/mach-footbridge/isa-irq.c|172| <<isa_init_irq>> irq_modify_status(_ISA_IRQ(11),
+ *   - arch/arm/mach-rpc/irq.c|195| <<rpc_init_irq>> irq_modify_status(irq, clr, set);
+ *   - arch/arm/mach-rpc/irq.c|203| <<rpc_init_irq>> irq_modify_status(irq, clr, set);
+ *   - arch/arm/mach-rpc/irq.c|211| <<rpc_init_irq>> irq_modify_status(irq, clr, set);
+ *   - arch/arm/mach-rpc/irq.c|218| <<rpc_init_irq>> irq_modify_status(irq, clr, set); 
+ *   - drivers/iio/industrialio-trigger.c|524| <<iio_trig_release>> irq_modify_status(trig->subirq_base + i,
+ *   - drivers/iio/industrialio-trigger.c|601| <<viio_trigger_alloc>> irq_modify_status(trig->subirq_base + i,
+ *   - drivers/irqchip/irq-clps711x.c|148| <<clps711x_intc_irq_map>> irq_modify_status(virq, IRQ_NOPROBE, flags);
+ *   - drivers/memory/omap-gpmc.c|1409| <<gpmc_irq_map>> irq_modify_status(virq, IRQ_NOREQUEST, IRQ_NOAUTOEN);
+ *   - drivers/mfd/fsl-imx25-tsadc.c|54| <<mx25_tsadc_domain_map>> irq_modify_status(irq, IRQ_NOREQUEST, IRQ_NOPROBE);
+ *   - drivers/sh/intc/internals.h|103| <<activate_irq>> irq_modify_status(irq, IRQ_NOREQUEST, IRQ_NOPROBE);
+ *   - include/linux/irq.h|780| <<irq_set_status_flags>> irq_modify_status(irq, 0, set);
+ *   - include/linux/irq.h|785| <<irq_clear_status_flags>> irq_modify_status(irq, clr, 0);
+ *   - include/linux/irq.h|790| <<irq_set_noprobe>> irq_modify_status(irq, 0, IRQ_NOPROBE);
+ *   - include/linux/irq.h|795| <<irq_set_probe>> irq_modify_status(irq, IRQ_NOPROBE, 0);
+ *   - include/linux/irq.h|800| <<irq_set_nothread>> irq_modify_status(irq, 0, IRQ_NOTHREAD);
+ *   - include/linux/irq.h|805| <<irq_set_thread>> irq_modify_status(irq, IRQ_NOTHREAD, 0);
+ *   - kernel/irq/generic-chip.c|434| <<irq_map_generic_chip>> irq_modify_status(virq, dgc->irq_flags_to_clear, dgc->irq_flags_to_set);
+ *   - kernel/irq/generic-chip.c|509| <<irq_setup_generic_chip>> irq_modify_status(i, clr, set);
+ *   - kernel/irq/generic-chip.c|578| <<irq_remove_generic_chip>> irq_modify_status(virq, clr, set);
+ *   - kernel/irq/irq_sim.c|130| <<irq_sim_domain_map>> irq_modify_status(virq, IRQ_NOREQUEST | IRQ_NOAUTOEN, IRQ_NOPROBE);
+ */
 void irq_modify_status(unsigned int irq, unsigned long clr, unsigned long set)
 {
 	unsigned long flags, trigger, tmp;
diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 1ed2b1739..53dbe6c8c 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -16,9 +16,27 @@
 
 #include "internals.h"
 
+/*
+ * called by:
+ *   - kernel/irq/cpuhotplug.c|85| <<migrate_one_irq>> if (irqd_is_per_cpu(d) || !irqd_is_started(d) || !irq_needs_fixup(d)) {
+ *
+ * 查看d->common->effective_affinity或者d->common->affinity是否包含要offline的CPU
+ * 如果包含返回true (说明会被影响)
+ * 返回false说明不需要稍后irq_do_set_affinity()
+ */
 /* For !GENERIC_IRQ_EFFECTIVE_AFF_MASK this looks at general affinity mask */
 static inline bool irq_needs_fixup(struct irq_data *d)
 {
+	/*
+	 * 在以下使用irq_common_data->effective_affinity:
+	 *   - include/linux/irq.h|967| <<irq_data_get_effective_affinity_mask>> return d->common->effective_affinity;
+	 *   - include/linux/irq.h|1009| <<irq_data_update_effective_affinity>> cpumask_copy(d->common->effective_affinity, m);
+	 *   - kernel/irq/irqdesc.c|61| <<alloc_masks>> if (!zalloc_cpumask_var_node(&desc->irq_common_data.effective_affinity,
+	 *   - kernel/irq/irqdesc.c|71| <<alloc_masks>> free_cpumask_var(desc->irq_common_data.effective_affinity);
+	 *   - kernel/irq/irqdesc.c|395| <<free_masks>> free_cpumask_var(desc->irq_common_data.effective_affinity);
+	 *
+	 * 获取d->common->effective_affinity
+	 */
 	const struct cpumask *m = irq_data_get_effective_affinity_mask(d);
 	unsigned int cpu = smp_processor_id();
 
@@ -28,6 +46,9 @@ static inline bool irq_needs_fixup(struct irq_data *d)
 	 * which do not implement effective affinity, but the architecture has
 	 * enabled the config switch. Use the general affinity mask instead.
 	 */
+	/*
+	 * 查看d->common->affinity
+	 */
 	if (cpumask_empty(m))
 		m = irq_data_get_affinity_mask(d);
 
@@ -50,6 +71,50 @@ static inline bool irq_needs_fixup(struct irq_data *d)
 	return cpumask_test_cpu(cpu, m);
 }
 
+/*
+ * [0] irq_matrix_alloc
+ * [0] assign_vector_locked
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_migrate_all_off_this_cpu
+ * [0] fixup_irqs
+ * [0] native_cpu_disable
+ * [0] take_cpu_down
+ * [0] multi_cpu_stop
+ * [0] cpu_stopper_thread
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] irq_matrix_free
+ * [0] apic_update_vector
+ * [0] assign_vector_locked
+ * [0] apic_set_affinity
+ * [0] msi_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_migrate_all_off_this_cpu
+ * [0] fixup_irqs
+ * [0] native_cpu_disable
+ * [0] take_cpu_down
+ * [0] multi_cpu_stop
+ * [0] cpu_stopper_thread
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ */
+
+/*
+ * cpu_disable_common()
+ * -> fixup_irqs()
+ *    -> irq_migrate_all_off_this_cpu()
+ *       -> migrate_one_irq()
+ *
+ * called by:
+ *   - kernel/irq/cpuhotplug.c|165| <<irq_migrate_all_off_this_cpu>> affinity_broken = migrate_one_irq(desc);
+ */
 static bool migrate_one_irq(struct irq_desc *desc)
 {
 	struct irq_data *d = irq_desc_get_irq_data(desc);
@@ -69,6 +134,15 @@ static bool migrate_one_irq(struct irq_desc *desc)
 		return false;
 	}
 
+	/*
+	 * 在以下使用IRQD_SETAFFINITY_PENDING:
+	 *   - kernel/irq/debugfs.c|119| <<global>> BIT_MASK_DESCR(IRQD_SETAFFINITY_PENDING),
+	 *   - include/linux/irq.h|258| <<irqd_is_setaffinity_pending>> return __irqd_to_state(d) & IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|214| <<irqd_set_move_pending>> __irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|219| <<irqd_clr_move_pending>> __irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;
+	 *
+	 * Affinity setting is pending
+	 */
 	/*
 	 * No move required, if:
 	 * - Interrupt is per cpu
@@ -79,6 +153,21 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	 * interrupt.
 	 */
 	if (irqd_is_per_cpu(d) || !irqd_is_started(d) || !irq_needs_fixup(d)) {
+		/*
+		 * irq_fixup_move_pending - Cleanup irq move pending from a dying CPU
+		 * @desc:               Interrupt descriptor to clean up
+		 * @force_clear:        If set clear the move pending bit unconditionally.
+		 *                      If not set, clear it only when the dying CPU is the
+		 *                      last one in the pending mask.
+		 *
+		 * Returns true if the pending bit was set and the pending mask contains an
+		 * online CPU other than the dying CPU.
+		 * called by:
+		 *   - kernel/irq/cpuhotplug.c|90| <<migrate_one_irq>> irq_fixup_move_pending(desc, false);
+		 *   - kernel/irq/cpuhotplug.c|108| <<migrate_one_irq>> if (irq_fixup_move_pending(desc, true))
+		 *
+		 * 核心是是否清除IRQD_SETAFFINITY_PENDING
+		 */
 		/*
 		 * If an irq move is pending, abort it if the dying CPU is
 		 * the sole target.
@@ -93,6 +182,10 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	 * to be cleaned up. It can't wait until this interrupt actually
 	 * happens and this CPU was involved.
 	 */
+	/*
+	 * 只在此处调用:
+	 * Called from fixup_irqs() with @desc->lock held and interrupts disabled.
+	 */
 	irq_force_complete_move(desc);
 
 	/*
@@ -101,6 +194,29 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	 * there is no move pending or the pending mask does not contain
 	 * any online CPU, use the current affinity mask.
 	 */
+	/*
+	 * irq_fixup_move_pending - Cleanup irq move pending from a dying CPU
+	 * @desc:               Interrupt descriptor to clean up
+	 * @force_clear:        If set clear the move pending bit unconditionally.
+	 *                      If not set, clear it only when the dying CPU is the
+	 *                      last one in the pending mask.
+	 *
+	 * Returns true if the pending bit was set and the pending mask contains an
+	 * online CPU other than the dying CPU.
+	 * called by:
+	 *   - kernel/irq/cpuhotplug.c|90| <<migrate_one_irq>> irq_fixup_move_pending(desc, false);
+	 *   - kernel/irq/cpuhotplug.c|108| <<migrate_one_irq>> if (irq_fixup_move_pending(desc, true))
+	 *
+	 * 1. 给OS创建很多的virtio-net中断. 假设有64个vCPU.
+	 * 2. 把所有virtio-net的中断绑定到vCPU2-3.
+	 * 3. 但是实际上, 这些中断分布在所有的vCPU (effective)
+	 * 4. 抓紧offline除了0-7的vCPU
+	 * 5. offline的时候用pending_mask, 所以都会迁移到vCPU=2,3
+	 * 6. 尽管vector还很多, 但是vCPU=2,3的不够了!
+	 *
+	 * 如果有pending的vCPU, 并且pending的affinity的vCPU有online的,
+	 * 那么可以使用pending的affinity
+	 */
 	if (irq_fixup_move_pending(desc, true))
 		affinity = irq_desc_get_pending_mask(desc);
 	else
@@ -110,6 +226,9 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	if (maskchip && chip->irq_mask)
 		chip->irq_mask(d);
 
+	/*
+	 * 还有一个API: cpumask_equal
+	 */
 	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {
 		/*
 		 * If the interrupt is managed, then shut it down and leave
@@ -152,6 +271,20 @@ static bool migrate_one_irq(struct irq_desc *desc)
  * Note: we must iterate over all IRQs, whether they have an attached
  * action structure or not, as we need to get chained interrupts too.
  */
+/*
+ * called by:
+ *   - arch/arm/kernel/smp.c|275| <<__cpu_disable>> irq_migrate_all_off_this_cpu();
+ *   - arch/arm64/kernel/smp.c|322| <<__cpu_disable>> irq_migrate_all_off_this_cpu();
+ *   - arch/csky/kernel/smp.c|287| <<__cpu_disable>> irq_migrate_all_off_this_cpu();
+ *   - arch/loongarch/kernel/smp.c|376| <<loongson_cpu_disable>> irq_migrate_all_off_this_cpu();
+ *   - arch/mips/kernel/smp-bmips.c|379| <<bmips_cpu_disable>> irq_migrate_all_off_this_cpu();
+ *   - arch/mips/kernel/smp-cps.c|461| <<cps_cpu_disable>> irq_migrate_all_off_this_cpu();
+ *   - arch/parisc/kernel/smp.c|469| <<__cpu_disable>> irq_migrate_all_off_this_cpu();
+ *   - arch/powerpc/kernel/smp.c|1193| <<generic_cpu_disable>> irq_migrate_all_off_this_cpu();
+ *   - arch/powerpc/sysdev/xive/common.c|1610| <<xive_smp_disable_cpu>> irq_migrate_all_off_this_cpu();
+ *   - arch/riscv/kernel/cpu-hotplug.c|41| <<__cpu_disable>> irq_migrate_all_off_this_cpu();
+ *   - arch/x86/kernel/irq.c|347| <<fixup_irqs>> irq_migrate_all_off_this_cpu();
+ */
 void irq_migrate_all_off_this_cpu(void)
 {
 	struct irq_desc *desc;
@@ -172,6 +305,10 @@ void irq_migrate_all_off_this_cpu(void)
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/irq/cpuhotplug.c|242| <<irq_restore_affinity_of_irq>> if (!irqd_is_single_target(data) || hk_should_isolate(data, cpu))
+ */
 static bool hk_should_isolate(struct irq_data *data, unsigned int cpu)
 {
 	const struct cpumask *hk_mask;
@@ -186,6 +323,10 @@ static bool hk_should_isolate(struct irq_data *data, unsigned int cpu)
 	return cpumask_test_cpu(cpu, hk_mask);
 }
 
+/*
+ * called by:
+ *   - kernel/irq/cpuhotplug.c|259| <<irq_affinity_online_cpu>> irq_restore_affinity_of_irq(desc, cpu);
+ */
 static void irq_restore_affinity_of_irq(struct irq_desc *desc, unsigned int cpu)
 {
 	struct irq_data *data = irq_desc_get_irq_data(desc);
@@ -215,6 +356,13 @@ static void irq_restore_affinity_of_irq(struct irq_desc *desc, unsigned int cpu)
  * irq_affinity_online_cpu - Restore affinity for managed interrupts
  * @cpu:	Upcoming CPU for which interrupts should be restored
  */
+/*
+ * 2237         [CPUHP_AP_IRQ_AFFINITY_ONLINE] = {
+ * 2238                 .name                   = "irq/affinity:online",
+ * 2239                 .startup.single         = irq_affinity_online_cpu,
+ * 2240                 .teardown.single        = NULL,
+ * 2241         },
+ */
 int irq_affinity_online_cpu(unsigned int cpu)
 {
 	struct irq_desc *desc;
diff --git a/kernel/irq/internals.h b/kernel/irq/internals.h
index bcc7f21db..408544585 100644
--- a/kernel/irq/internals.h
+++ b/kernel/irq/internals.h
@@ -209,13 +209,49 @@ static inline unsigned int irqd_get(struct irq_data *d)
 /*
  * Manipulation functions for irq_data.state
  */
+/*
+ * 在以下使用IRQD_SETAFFINITY_PENDING:
+ *   - kernel/irq/debugfs.c|119| <<global>> BIT_MASK_DESCR(IRQD_SETAFFINITY_PENDING),
+ *   - include/linux/irq.h|258| <<irqd_is_setaffinity_pending>> return __irqd_to_state(d) & IRQD_SETAFFINITY_PENDING;
+ *   - kernel/irq/internals.h|214| <<irqd_set_move_pending>> __irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;
+ *   - kernel/irq/internals.h|219| <<irqd_clr_move_pending>> __irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;
+ *
+ * called by:
+ *   - kernel/irq/manage.c|301| <<irq_set_affinity_pending>> irqd_set_move_pending(data);
+ *   - kernel/irq/manage.c|368| <<irq_set_affinity_locked>> irqd_set_move_pending(data);
+ *   - kernel/irq/migration.c|87| <<irq_move_masked_irq>> irqd_set_move_pending(data);
+ *
+ * 设置IRQD_SETAFFINITY_PENDING (affinity setting is pending)
+ */
 static inline void irqd_set_move_pending(struct irq_data *d)
 {
+	/*
+	 * 在以下使用IRQD_SETAFFINITY_PENDING:
+	 *   - kernel/irq/debugfs.c|119| <<global>> BIT_MASK_DESCR(IRQD_SETAFFINITY_PENDING),
+	 *   - include/linux/irq.h|258| <<irqd_is_setaffinity_pending>> return __irqd_to_state(d) & IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|214| <<irqd_set_move_pending>> __irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|219| <<irqd_clr_move_pending>> __irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;
+	 */
 	__irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/migration.c|30| <<irq_fixup_move_pending>> irqd_clr_move_pending(data);
+ *   - kernel/irq/migration.c|34| <<irq_fixup_move_pending>> irqd_clr_move_pending(data);
+ *   - kernel/irq/migration.c|47| <<irq_move_masked_irq>> irqd_clr_move_pending(data);
+ */
 static inline void irqd_clr_move_pending(struct irq_data *d)
 {
+	/*
+	 * 在以下使用IRQD_SETAFFINITY_PENDING:
+	 *   - kernel/irq/debugfs.c|119| <<global>> BIT_MASK_DESCR(IRQD_SETAFFINITY_PENDING),
+	 *   - include/linux/irq.h|258| <<irqd_is_setaffinity_pending>> return __irqd_to_state(d) & IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|214| <<irqd_set_move_pending>> __irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|219| <<irqd_clr_move_pending>> __irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;
+	 *
+	 * Affinity setting is pending
+	 */
 	__irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;
 }
 
@@ -420,6 +456,13 @@ static inline bool irq_move_pending(struct irq_data *data)
 {
 	return irqd_is_setaffinity_pending(data);
 }
+/*
+ * called by:
+ *   - kernel/irq/manage.c|302| <<irq_set_affinity_pending>> irq_copy_pending(desc, dest);
+ *   - kernel/irq/manage.c|375| <<irq_set_affinity_locked>> irq_copy_pending(desc, mask);
+ *
+ * 把mask拷贝到irq_desc->pending_mask
+ */
 static inline void
 irq_copy_pending(struct irq_desc *desc, const struct cpumask *mask)
 {
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index 1782f90cd..21b04fa9a 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -214,6 +214,14 @@ static void irq_validate_effective_affinity(struct irq_data *data)
 static inline void irq_validate_effective_affinity(struct irq_data *data) { }
 #endif
 
+/*
+ * called by:
+ *   - kernel/irq/chip.c|290| <<irq_startup>> irq_do_set_affinity(d, aff, false);
+ *   - kernel/irq/cpuhotplug.c|158| <<migrate_one_irq>> err = irq_do_set_affinity(d, affinity, false);
+ *   - kernel/irq/manage.c|323| <<irq_try_set_affinity>> int ret = irq_do_set_affinity(data, dest, force);
+ *   - kernel/irq/manage.c|675| <<irq_setup_affinity>> ret = irq_do_set_affinity(&desc->irq_data, &mask, false);
+ *   - kernel/irq/migration.c|100| <<irq_move_masked_irq>> ret = irq_do_set_affinity(data, desc->pending_mask, false);
+ */
 int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,
 			bool force)
 {
@@ -269,6 +277,10 @@ int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,
 	 * case we do as we are told).
 	 */
 	cpumask_and(&tmp_mask, prog_mask, cpu_online_mask);
+	/*
+	 * msi_set_affinity()
+	 * apic_set_affinity()
+	 */
 	if (!force && !cpumask_empty(&tmp_mask))
 		ret = chip->irq_set_affinity(data, &tmp_mask, force);
 	else if (force)
@@ -293,6 +305,10 @@ int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,
 }
 
 #ifdef CONFIG_GENERIC_PENDING_IRQ
+/*
+ * called by:
+ *   - kernel/irq/manage.c|324| <<irq_try_set_affinity>> ret = irq_set_affinity_pending(data, dest);
+ */
 static inline int irq_set_affinity_pending(struct irq_data *data,
 					   const struct cpumask *dest)
 {
@@ -325,6 +341,10 @@ static int irq_try_set_affinity(struct irq_data *data,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/manage.c|384| <<irq_set_affinity_locked>> if (irq_set_affinity_deactivated(data, mask))
+ */
 static bool irq_set_affinity_deactivated(struct irq_data *data,
 					 const struct cpumask *mask)
 {
@@ -349,6 +369,12 @@ static bool irq_set_affinity_deactivated(struct irq_data *data,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/mips/cavium-octeon/octeon-irq.c|788| <<octeon_irq_cpu_offline_ciu>> irq_set_affinity_locked(data, &new_affinity, false);
+ *   - kernel/irq/cpuhotplug.c|232| <<irq_restore_affinity_of_irq>> irq_set_affinity_locked(data, affinity, false);
+ *   - kernel/irq/manage.c|466| <<__irq_set_affinity>> ret = irq_set_affinity_locked(irq_desc_get_irq_data(desc), mask, force);
+ */
 int irq_set_affinity_locked(struct irq_data *data, const struct cpumask *mask,
 			    bool force)
 {
@@ -362,10 +388,46 @@ int irq_set_affinity_locked(struct irq_data *data, const struct cpumask *mask,
 	if (irq_set_affinity_deactivated(data, mask))
 		return 0;
 
+	/*
+	 *
+	 * drivers/iommu/amd/iommu.c|3482| <<irq_remapping_alloc>> irq_set_status_flags(virq + i, IRQ_MOVE_PCNTXT);
+ 83 drivers/iommu/intel/irq_remapping.c|1366| <<intel_irq_remapping_alloc>> irq_set_status_flags(virq + i, IRQ_MOVE_PCNTXT);
+	 *
+	 * 在以下使用IRQD_SETAFFINITY_PENDING:
+	 *   - kernel/irq/debugfs.c|119| <<global>> BIT_MASK_DESCR(IRQD_SETAFFINITY_PENDING),
+	 *   - include/linux/irq.h|258| <<irqd_is_setaffinity_pending>> return __irqd_to_state(d) & IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|214| <<irqd_set_move_pending>> __irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|219| <<irqd_clr_move_pending>> __irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;
+	 *
+	 * 在测试的时候, irq_can_move_pcntxt(data)是false, !irqd_is_setaffinity_pending(data)是true
+	 *
+	 * 如果没有设置IRQD_SETAFFINITY_PENDING
+	 */
 	if (irq_can_move_pcntxt(data) && !irqd_is_setaffinity_pending(data)) {
 		ret = irq_try_set_affinity(data, mask, force);
 	} else {
+		/*
+		 * 在以下使用IRQD_SETAFFINITY_PENDING:
+		 *   - kernel/irq/debugfs.c|119| <<global>> BIT_MASK_DESCR(IRQD_SETAFFINITY_PENDING),
+		 *   - include/linux/irq.h|258| <<irqd_is_setaffinity_pending>> return __irqd_to_state(d) & IRQD_SETAFFINITY_PENDING;
+		 *   - kernel/irq/internals.h|214| <<irqd_set_move_pending>> __irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;
+		 *   - kernel/irq/internals.h|219| <<irqd_clr_move_pending>> __irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;
+		 *
+		 * called by:
+		 *   - kernel/irq/manage.c|301| <<irq_set_affinity_pending>> irqd_set_move_pending(data);
+		 *   - kernel/irq/manage.c|368| <<irq_set_affinity_locked>> irqd_set_move_pending(data);
+		 *   - kernel/irq/migration.c|87| <<irq_move_masked_irq>> irqd_set_move_pending(data);
+		 *
+		 * 设置IRQD_SETAFFINITY_PENDING (affinity setting is pending)
+		 */
 		irqd_set_move_pending(data);
+		/*
+		 * called by:
+		 *   - kernel/irq/manage.c|302| <<irq_set_affinity_pending>> irq_copy_pending(desc, dest);
+		 *   - kernel/irq/manage.c|375| <<irq_set_affinity_locked>> irq_copy_pending(desc, mask);
+		 *
+		 * 把mask拷贝到irq_desc->pending_mask
+		 */
 		irq_copy_pending(desc, mask);
 	}
 
@@ -452,6 +514,12 @@ int irq_update_affinity_desc(unsigned int irq,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/manage.c|493| <<irq_set_affinity>> return __irq_set_affinity(irq, cpumask, false);
+ *   - kernel/irq/manage.c|510| <<irq_force_affinity>> return __irq_set_affinity(irq, cpumask, true);
+ *   - kernel/irq/manage.c|540| <<__irq_apply_affinity_hint>> __irq_set_affinity(irq, m, false);
+ */
 static int __irq_set_affinity(unsigned int irq, const struct cpumask *mask,
 			      bool force)
 {
@@ -498,6 +566,11 @@ int irq_force_affinity(unsigned int irq, const struct cpumask *cpumask)
 }
 EXPORT_SYMBOL_GPL(irq_force_affinity);
 
+/*
+ * called by:
+ *   - include/linux/interrupt.h|331| <<irq_update_affinity_hint>> return __irq_apply_affinity_hint(irq, m, false);
+ *   - include/linux/interrupt.h|346| <<irq_set_affinity_and_hint>> return __irq_apply_affinity_hint(irq, m, true);
+ */
 int __irq_apply_affinity_hint(unsigned int irq, const struct cpumask *m,
 			      bool setaffinity)
 {
@@ -506,6 +579,16 @@ int __irq_apply_affinity_hint(unsigned int irq, const struct cpumask *m,
 
 	if (!desc)
 		return -EINVAL;
+	/*
+	 * 在以下使用irq_desc->affinity_hint:
+	 *   - kernel/irq/manage.c|509| <<__irq_apply_affinity_hint>> desc->affinity_hint = m;
+	 *   - kernel/irq/manage.c|1919| <<__free_irq>> if (WARN_ON_ONCE(desc->affinity_hint))
+	 *   - kernel/irq/manage.c|1920| <<__free_irq>> desc->affinity_hint = NULL;
+	 *   - kernel/irq/proc.c|93| <<irq_affinity_hint_proc_show>> if (desc->affinity_hint)
+	 *   - kernel/irq/proc.c|94| <<irq_affinity_hint_proc_show>> cpumask_copy(mask, desc->affinity_hint);
+	 *
+	 * 注释: hint to user space for preferred irq affinity
+	 */
 	desc->affinity_hint = m;
 	irq_put_desc_unlock(desc, flags);
 	if (m && setaffinity)
diff --git a/kernel/irq/matrix.c b/kernel/irq/matrix.c
index 75d0ae490..2af4bb8e8 100644
--- a/kernel/irq/matrix.c
+++ b/kernel/irq/matrix.c
@@ -11,28 +11,213 @@
 #define IRQ_MATRIX_SIZE	(BITS_TO_LONGS(IRQ_MATRIX_BITS))
 
 struct cpumap {
+	/*
+	 * 在以下修改cpumap->available:
+	 *   - kernel/irq/matrix.c|94| <<irq_matrix_online>> cm->available = m->alloc_size;
+	 *   - kernel/irq/matrix.c|95| <<irq_matrix_online>> cm->available -= cm->managed + m->systembits_inalloc;
+	 *   - kernel/irq/matrix.c|241| <<irq_matrix_reserve_managed>> cm->available--;
+	 *   - kernel/irq/matrix.c|291| <<irq_matrix_remove_managed>> cm->available++;
+	 *   - kernel/irq/matrix.c|350| <<irq_matrix_assign>> cm->available--;
+	 *   - kernel/irq/matrix.c|421| <<irq_matrix_alloc>> cm->available--;
+	 *   - kernel/irq/matrix.c|459| <<irq_matrix_free>> cm->available++;
+	 * 在以下使用cpumap->available:
+	 *   - kernel/irq/matrix.c|98| <<irq_matrix_online>> m->global_available += cm->available;
+	 *   - kernel/irq/matrix.c|113| <<irq_matrix_offline>> m->global_available -= cm->available;
+	 *   - kernel/irq/matrix.c|154| <<matrix_find_best_cpu>> if (!cm->online || cm->available <= maxavl)
+	 *   - kernel/irq/matrix.c|158| <<matrix_find_best_cpu>> maxavl = cm->available;
+	 *   - kernel/irq/matrix.c|478| <<irq_matrix_available>> return m->global_available - cm->available;
+	 *   - kernel/irq/matrix.c|533| <<irq_matrix_debug_show>> seq_printf(sf, "%*s %4d  %4u  %4u  %4u %4u  %*pbl\n", ind, " ", cpu, cm->available, cm->managed,
+	 */
 	unsigned int		available;
+	/*
+	 * 在以下设置cpumap->allocated:
+	 *   - kernel/irq/matrix.c|350| <<irq_matrix_assign_system>> cm->allocated--;
+	 *   - kernel/irq/matrix.c|482| <<irq_matrix_alloc_managed>> cm->allocated++;
+	 *   - kernel/irq/matrix.c|512| <<irq_matrix_assign>> cm->allocated++;
+	 *   - kernel/irq/matrix.c|614| <<irq_matrix_alloc>> cm->allocated++;
+	 *   - kernel/irq/matrix.c|652| <<irq_matrix_free>> cm->allocated--;
+	 * 在以下使用cpumap->allocated:
+	 *   - kernel/irq/matrix.c|713| <<irq_matrix_allocated>> return cm->allocated - cm->managed_allocated;
+	 *   - kernel/irq/matrix.c|747| <<irq_matrix_debug_show>> seq_printf(sf, "%*s %4d  %4u  %4u  %4u %4u  %*pbl\n", ind, " ", ... cm->managed_allocated, cm->allocated,
+	 */
 	unsigned int		allocated;
+	/*
+	 * 在以下设置cpumap->managed:
+	 *   - kernel/irq/matrix.c|388| <<irq_matrix_reserve_managed>> cm->managed++;
+	 *   - kernel/irq/matrix.c|442| <<irq_matrix_remove_managed>> cm->managed--;
+	 * 在以下使用cpumap->managed:
+	 *   - kernel/irq/matrix.c|190| <<irq_matrix_online>> cm->available -= cm->managed + m->systembits_inalloc;
+	 *   - kernel/irq/matrix.c|430| <<irq_matrix_remove_managed>> if (WARN_ON_ONCE(!cm->managed))
+	 *   - kernel/irq/matrix.c|746| <<irq_matrix_debug_show>> seq_printf(sf, "%*s %4d  %4u  %4u  %4u %4u  %*pbl\n", ind, " ", ... cpu, cm->available, cm->managed,
+	 */
 	unsigned int		managed;
+	/*
+	 * 在以下设置cpumap->managed_allocated:
+	 *   - kernel/irq/matrix.c|483| <<irq_matrix_alloc_managed>> cm->managed_allocated++;
+	 *   - kernel/irq/matrix.c|654| <<irq_matrix_free>> cm->managed_allocated--;
+	 * 在以下使用cpumap->managed_allocated:
+	 *   - kernel/irq/matrix.c|306| <<matrix_find_best_cpu_managed>> if (!cm->online || cm->managed_allocated > allocated)
+	 *   - kernel/irq/matrix.c|310| <<matrix_find_best_cpu_managed>> allocated = cm->managed_allocated;
+	 *   - kernel/irq/matrix.c|713| <<irq_matrix_allocated>> return cm->allocated - cm->managed_allocated;
+	 *   - kernel/irq/matrix.c|747| <<irq_matrix_debug_show>> seq_printf(sf, "%*s %4d  %4u  %4u  %4u %4u  %*pbl\n", ind, " ", ... cm->managed_allocated, cm->allocated,
+	 */
 	unsigned int		managed_allocated;
+	/*
+	 * 在以下使用cpumap->initialized:
+	 *   - kernel/irq/matrix.c|188| <<irq_matrix_online>> if (!cm->initialized) {
+	 *   - kernel/irq/matrix.c|191| <<irq_matrix_online>> cm->initialized = true;
+	 */
 	bool			initialized;
+	/*
+	 * 在以下设置cpumap->online:
+	 *   - kernel/irq/matrix.c|194| <<irq_matrix_online>> cm->online = true;
+	 *   - kernel/irq/matrix.c|213| <<irq_matrix_offline>> cm->online = false;
+	 * 在以下使用cpumap->online:
+	 *   - kernel/irq/matrix.c|186| <<irq_matrix_online>> BUG_ON(cm->online);
+	 *   - kernel/irq/matrix.c|281| <<matrix_find_best_cpu>> if (!cm->online || cm->available <= maxavl)
+	 *   - kernel/irq/matrix.c|306| <<matrix_find_best_cpu_managed>> if (!cm->online || cm->managed_allocated > allocated)
+	 *   - kernel/irq/matrix.c|389| <<irq_matrix_reserve_managed>> if (cm->online) {
+	 *   - kernel/irq/matrix.c|443| <<irq_matrix_remove_managed>> if (cm->online) {
+	 *   - kernel/irq/matrix.c|656| <<irq_matrix_free>> if (cm->online)
+	 *   - kernel/irq/matrix.c|661| <<irq_matrix_free>> if (cm->online)
+	 */
 	bool			online;
+	/*
+	 * 在以下修改cpumap->alloc_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|138| <<matrix_alloc_area>> bitmap_set(cm->alloc_map, area, num);
+	 *   - kernel/irq/matrix.c|205| <<irq_matrix_assign_system>> BUG_ON(!test_and_clear_bit(bit, cm->alloc_map));
+	 *   - kernel/irq/matrix.c|324| <<irq_matrix_alloc_managed>> set_bit(bit, cm->alloc_map);
+	 *   - kernel/irq/matrix.c|346| <<irq_matrix_assign>> if (WARN_ON_ONCE(test_and_set_bit(bit, cm->alloc_map)))
+	 *   - kernel/irq/matrix.c|448| <<irq_matrix_free>> if (WARN_ON_ONCE(!test_and_clear_bit(bit, cm->alloc_map)))
+	 * 在以下使用cpumap->alloc_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|131| <<matrix_alloc_area>> bitmap_or(m->scratch_map, m->scratch_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|281| <<irq_matrix_remove_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|320| <<irq_matrix_alloc_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|535| <<irq_matrix_debug_show>> seq_printf( ... ... m->matrix_bits, cm->alloc_map);
+	 */
 	unsigned long		alloc_map[IRQ_MATRIX_SIZE];
+	/*
+	 * 在以下设置cpumap->managed_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|122| <<matrix_alloc_area>> bitmap_set(cm->managed_map, area, num);
+	 *   - kernel/irq/matrix.c|269| <<irq_matrix_remove_managed>> clear_bit(bit, cm->managed_map);
+	 * 在以下使用cpumap->managed_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|21| <<global>> unsigned long managed_map[IRQ_MATRIX_SIZE];
+	 *   - kernel/irq/matrix.c|116| <<matrix_alloc_area>> bitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);
+	 *   - kernel/irq/matrix.c|263| <<irq_matrix_remove_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|302| <<irq_matrix_alloc_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end)
+	 */
 	unsigned long		managed_map[IRQ_MATRIX_SIZE];
 };
 
 struct irq_matrix {
+	/*
+	 * 在以下设置irq_matrix->matrix_bits:
+	 *   - kernel/irq/matrix.c|161| <<irq_alloc_matrix>> m->matrix_bits = matrix_bits;
+	 * 在以下使用irq_matrix->matrix_bits:
+	 *   - kernel/irq/matrix.c|154| <<irq_alloc_matrix>> if (matrix_bits > IRQ_MATRIX_BITS)
+	 *   - kernel/irq/matrix.c|336| <<irq_matrix_assign_system>> BUG_ON(bit > m->matrix_bits);
+	 *   - kernel/irq/matrix.c|731| <<irq_matrix_debug_show>> unsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);
+	 *   - kernel/irq/matrix.c|738| <<irq_matrix_debug_show>> seq_printf(sf, "System: %u: %*pbl\n", nsys, m->matrix_bits,
+	 *   - kernel/irq/matrix.c|748| <<irq_matrix_debug_show>> m->matrix_bits, cm->alloc_map);
+	 */
 	unsigned int		matrix_bits;
+	/*
+	 * 在以下修改irq_matrix->alloc_start:
+	 *   - kernel/irq/matrix.c|243| <<irq_alloc_matrix>> m->alloc_start = alloc_start;
+	 */
 	unsigned int		alloc_start;
+	/*
+	 * 在以下修改irq_matrix->alloc_end:
+	 *   - kernel/irq/matrix.c|240| <<irq_alloc_matrix>> m->alloc_end = alloc_end;
+	 */
 	unsigned int		alloc_end;
+	/*
+	 * 在以下使用irq_matrix->alloc_size:
+	 *   - kernel/irq/matrix.c|241| <<irq_alloc_matrix>> m->alloc_size = alloc_end - alloc_start;
+	 *   - kernel/irq/matrix.c|266| <<irq_matrix_online>> cm->available = m->alloc_size;
+	 */
 	unsigned int		alloc_size;
+	/*
+	 * 在以下使用irq_matrix->global_available:
+	 *   - kernel/irq/matrix.c|98| <<irq_matrix_online>> m->global_available += cm->available;
+	 *   - kernel/irq/matrix.c|113| <<irq_matrix_offline>> m->global_available -= cm->available;
+	 *   - kernel/irq/matrix.c|242| <<irq_matrix_reserve_managed>> m->global_available--;
+	 *   - kernel/irq/matrix.c|292| <<irq_matrix_remove_managed>> m->global_available++;
+	 *   - kernel/irq/matrix.c|351| <<irq_matrix_assign>> m->global_available--;
+	 *   - kernel/irq/matrix.c|366| <<irq_matrix_reserve>> if (m->global_reserved == m->global_available)
+	 *   - kernel/irq/matrix.c|423| <<irq_matrix_alloc>> m->global_available--;
+	 *   - kernel/irq/matrix.c|461| <<irq_matrix_free>> m->global_available++;
+	 *   - kernel/irq/matrix.c|477| <<irq_matrix_available>> return m->global_available;
+	 *   - kernel/irq/matrix.c|478| <<irq_matrix_available>> return m->global_available - cm->available;
+	 *   - kernel/irq/matrix.c|522| <<irq_matrix_debug_show>> seq_printf(sf, "Global available: %6u\n", m->global_available);
+	 */
 	unsigned int		global_available;
+	/*
+	 * 在以下使用irq_matrix->global_reserved:
+	 *   - kernel/irq/matrix.c|489| <<irq_matrix_reserve>> if (m->global_reserved == m->global_available)
+	 *   - kernel/irq/matrix.c|492| <<irq_matrix_reserve>> m->global_reserved++;
+	 *   - kernel/irq/matrix.c|507| <<irq_matrix_remove_reserved>> m->global_reserved--;
+	 *   - kernel/irq/matrix.c|552| <<irq_matrix_alloc>> m->global_reserved--;
+	 *   - kernel/irq/matrix.c|614| <<irq_matrix_reserved>> return m->global_reserved;
+	 *   - kernel/irq/matrix.c|650| <<irq_matrix_debug_show>> seq_printf(sf, "Global reserved: %6u\n", m->global_reserved);
+	 */
 	unsigned int		global_reserved;
+	/*
+	 * 在以下使用irq_matrix->systembits_inalloc:
+	 *   - kernel/irq/matrix.c|173| <<irq_matrix_online>> cm->available -= cm->managed + m->systembits_inalloc;
+	 *   - kernel/irq/matrix.c|321| <<irq_matrix_assign_system>> m->systembits_inalloc++;
+	 */
 	unsigned int		systembits_inalloc;
+	/*
+	 * 在以下使用irq_matrix->total_allocated:
+	 *   - kernel/irq/matrix.c|207| <<irq_matrix_assign_system>> m->total_allocated--;
+	 *   - kernel/irq/matrix.c|327| <<irq_matrix_alloc_managed>> m->total_allocated++;
+	 *   - kernel/irq/matrix.c|349| <<irq_matrix_assign>> m->total_allocated++; 
+	 *   - kernel/irq/matrix.c|422| <<irq_matrix_alloc>> m->total_allocated++;
+	 *   - kernel/irq/matrix.c|456| <<irq_matrix_free>> m->total_allocated--;
+	 *   - kernel/irq/matrix.c|524| <<irq_matrix_debug_show>> seq_printf(sf, "Total allocated: %6u\n", m->total_allocated);
+	 */
 	unsigned int		total_allocated;
 	unsigned int		online_maps;
+	/*
+	 * 在以下使用percpu的irq_matrix->maps (struct cpumap):
+	 *   - kernel/irq/matrix.c|165| <<irq_alloc_matrix>> m->maps = alloc_percpu(*m->maps);
+	 *   - kernel/irq/matrix.c|166| <<irq_alloc_matrix>> if (!m->maps) {
+	 *   - kernel/irq/matrix.c|184| <<irq_matrix_online>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|209| <<irq_matrix_offline>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|279| <<matrix_find_best_cpu>> cm = per_cpu_ptr(m->maps, cpu); 
+	 *   - kernel/irq/matrix.c|304| <<matrix_find_best_cpu_managed>> cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|334| <<irq_matrix_assign_system>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|377| <<irq_matrix_reserve_managed>> struct cpumap *cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|427| <<irq_matrix_remove_managed>> struct cpumap *cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|474| <<irq_matrix_alloc_managed>> cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|503| <<irq_matrix_assign>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|604| <<irq_matrix_alloc>> cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|644| <<irq_matrix_free>> struct cpumap *cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|679| <<irq_matrix_available>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|711| <<irq_matrix_allocated>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|743| <<irq_matrix_debug_show>> struct cpumap *cm = per_cpu_ptr(m->maps, cpu);
+	 */
 	struct cpumap __percpu	*maps;
+	/*
+	 * 在以下使用irq_matrix->scratch_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|130| <<matrix_alloc_area>> bitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);
+	 *   - kernel/irq/matrix.c|131| <<matrix_alloc_area>> bitmap_or(m->scratch_map, m->scratch_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|132| <<matrix_alloc_area>> area = bitmap_find_next_zero_area(m->scratch_map, end, start, num, 0);
+	 *   - kernel/irq/matrix.c|281| <<irq_matrix_remove_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|283| <<irq_matrix_remove_managed>> bit = find_first_bit(m->scratch_map, end);
+	 *   - kernel/irq/matrix.c|320| <<irq_matrix_alloc_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|321| <<irq_matrix_alloc_managed>> bit = find_first_bit(m->scratch_map, end);
+	 */
 	unsigned long		scratch_map[IRQ_MATRIX_SIZE];
+	/*
+	 * 在以下使用irq_matrix->system_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|45| <<global>> unsigned long system_map[IRQ_MATRIX_SIZE];
+	 *   - kernel/irq/matrix.c|130| <<matrix_alloc_area>> bitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);
+	 *   - kernel/irq/matrix.c|203| <<irq_matrix_assign_system>> set_bit(bit, m->system_map);
+	 *   - kernel/irq/matrix.c|518| <<irq_matrix_debug_show>> unsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);
+	 *   - kernel/irq/matrix.c|526| <<irq_matrix_debug_show>> seq_printf(sf, "System: %u: %*pbl\n", nsys, m->matrix_bits, m->system_map);
+	 */
 	unsigned long		system_map[IRQ_MATRIX_SIZE];
 };
 
@@ -46,6 +231,13 @@ struct irq_matrix {
  * @alloc_end:		At which bit the allocation search ends, i.e first
  *			invalid bit
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|953| <<arch_early_irq_init>> vector_matrix = irq_alloc_matrix(NR_VECTORS, FIRST_EXTERNAL_VECTOR, FIRST_SYSTEM_VECTOR);
+ *
+ * start = 0x20 =  32
+ * end   = 0xec = 236
+ */
 __init struct irq_matrix *irq_alloc_matrix(unsigned int matrix_bits,
 					   unsigned int alloc_start,
 					   unsigned int alloc_end)
@@ -71,6 +263,11 @@ __init struct irq_matrix *irq_alloc_matrix(unsigned int matrix_bits,
 	return m;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|922| <<lapic_assign_system_vectors>> irq_matrix_online(vector_matrix);
+ *   - arch/x86/kernel/apic/vector.c|983| <<lapic_online>> irq_matrix_online(vector_matrix);
+ */
 /**
  * irq_matrix_online - Bring the local CPU matrix online
  * @m:		Matrix pointer
@@ -81,6 +278,15 @@ void irq_matrix_online(struct irq_matrix *m)
 
 	BUG_ON(cm->online);
 
+	/*
+	 * 在以下使用cpumap->initialized:
+	 *   - kernel/irq/matrix.c|188| <<irq_matrix_online>> if (!cm->initialized) {
+	 *   - kernel/irq/matrix.c|191| <<irq_matrix_online>> cm->initialized = true;
+	 *
+	 * 在以下设置cpumap->online:
+	 *   - kernel/irq/matrix.c|194| <<irq_matrix_online>> cm->online = true;
+	 *   - kernel/irq/matrix.c|213| <<irq_matrix_offline>> cm->online = false;
+	 */
 	if (!cm->initialized) {
 		cm->available = m->alloc_size;
 		cm->available -= cm->managed + m->systembits_inalloc;
@@ -92,6 +298,10 @@ void irq_matrix_online(struct irq_matrix *m)
 	trace_irq_matrix_online(m);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|1009| <<lapic_offline>> irq_matrix_offline(vector_matrix);
+ */
 /**
  * irq_matrix_offline - Bring the local CPU matrix offline
  * @m:		Matrix pointer
@@ -107,12 +317,46 @@ void irq_matrix_offline(struct irq_matrix *m)
 	trace_irq_matrix_offline(m);
 }
 
+/*
+ * irq_matrix_alloc()是从所有mask的cpu中选一个cpu分配一个regular interrupt (不是managed)
+ *
+ * called by:
+ *   - kernel/irq/matrix.c|218| <<irq_matrix_reserve_managed>> bit = matrix_alloc_area(m, cm, 1, true);
+ *   - kernel/irq/matrix.c|395| <<irq_matrix_alloc>> bit = matrix_alloc_area(m, cm, 1, false);
+ *
+ * 分配的时候不能是下面的bit:
+ * - irq_matrix->system_map
+ * - cpumap->managed_map
+ * - cpumap->alloc_map
+ * managed_map里的vector可以是reserved也可以是已经分配的
+ * 真正的分配由alloc_map决定
+ *
+ * 如果要managed的: 从cm->managed_map来reserve一个 (以后真正alloc用cm->alloc_map)
+ * 如果要regular, 从cm->alloc_map来分配一个
+ */
 static unsigned int matrix_alloc_area(struct irq_matrix *m, struct cpumap *cm,
 				      unsigned int num, bool managed)
 {
 	unsigned int area, start = m->alloc_start;
 	unsigned int end = m->alloc_end;
 
+	/*
+	 * 在以下使用irq_matrix->scratch_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|130| <<matrix_alloc_area>> bitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);
+	 *   - kernel/irq/matrix.c|131| <<matrix_alloc_area>> bitmap_or(m->scratch_map, m->scratch_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|132| <<matrix_alloc_area>> area = bitmap_find_next_zero_area(m->scratch_map, end, start, num, 0);
+	 *   - kernel/irq/matrix.c|281| <<irq_matrix_remove_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|283| <<irq_matrix_remove_managed>> bit = find_first_bit(m->scratch_map, end);
+	 *   - kernel/irq/matrix.c|320| <<irq_matrix_alloc_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|321| <<irq_matrix_alloc_managed>> bit = find_first_bit(m->scratch_map, end);
+	 *
+	 * 在以下使用irq_matrix->system_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|45| <<global>> unsigned long system_map[IRQ_MATRIX_SIZE];
+	 *   - kernel/irq/matrix.c|130| <<matrix_alloc_area>> bitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);
+	 *   - kernel/irq/matrix.c|203| <<irq_matrix_assign_system>> set_bit(bit, m->system_map);
+	 *   - kernel/irq/matrix.c|518| <<irq_matrix_debug_show>> unsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);
+	 *   - kernel/irq/matrix.c|526| <<irq_matrix_debug_show>> seq_printf(sf, "System: %u: %*pbl\n", nsys, m->matrix_bits, m->system_map);
+	 */
 	bitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);
 	bitmap_or(m->scratch_map, m->scratch_map, cm->alloc_map, end);
 	area = bitmap_find_next_zero_area(m->scratch_map, end, start, num, 0);
@@ -125,6 +369,12 @@ static unsigned int matrix_alloc_area(struct irq_matrix *m, struct cpumap *cm,
 	return area;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/matrix.c|519| <<irq_matrix_alloc>> cpu = matrix_find_best_cpu(m, msk);
+ *
+ * 每个percpu的cpumap的cpumap->available不能是0
+ */
 /* Find the best CPU which has the lowest vector allocation count */
 static unsigned int matrix_find_best_cpu(struct irq_matrix *m,
 					const struct cpumask *msk)
@@ -146,6 +396,10 @@ static unsigned int matrix_find_best_cpu(struct irq_matrix *m,
 	return best_cpu;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/matrix.c|416| <<irq_matrix_alloc_managed>> cpu = matrix_find_best_cpu_managed(m, msk);
+ */
 /* Find the best CPU which has the lowest number of managed IRQs allocated */
 static unsigned int matrix_find_best_cpu_managed(struct irq_matrix *m,
 						const struct cpumask *msk)
@@ -158,10 +412,21 @@ static unsigned int matrix_find_best_cpu_managed(struct irq_matrix *m,
 	for_each_cpu(cpu, msk) {
 		cm = per_cpu_ptr(m->maps, cpu);
 
+		/*
+		 * 就没有大于UINT_MAX的
+		 * 所以返回错误的条件:
+		 * 1. cm->online
+		 * 2. mask里没有任何CPU
+		 */
 		if (!cm->online || cm->managed_allocated > allocated)
 			continue;
 
 		best_cpu = cpu;
+		/*
+		 * 在以下设置cpumap->managed_allocated:
+		 *   - kernel/irq/matrix.c|483| <<irq_matrix_alloc_managed>> cm->managed_allocated++;
+		 *   - kernel/irq/matrix.c|654| <<irq_matrix_free>> cm->managed_allocated--;
+		 */
 		allocated = cm->managed_allocated;
 	}
 	return best_cpu;
@@ -178,6 +443,11 @@ static unsigned int matrix_find_best_cpu_managed(struct irq_matrix *m,
  * early boot process, then the chance to survive is about zero.
  * If this happens when the system is life, it's not much better.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|888| <<lapic_assign_legacy_vector>> irq_matrix_assign_system(vector_matrix, ISA_IRQ_VECTOR(irq), replace);
+ *   - arch/x86/kernel/apic/vector.c|916| <<lapic_assign_system_vectors>> irq_matrix_assign_system(vector_matrix, vector, false);
+ */
 void irq_matrix_assign_system(struct irq_matrix *m, unsigned int bit,
 			      bool replace)
 {
@@ -186,6 +456,14 @@ void irq_matrix_assign_system(struct irq_matrix *m, unsigned int bit,
 	BUG_ON(bit > m->matrix_bits);
 	BUG_ON(m->online_maps > 1 || (m->online_maps && !replace));
 
+	/*
+	 * 在以下使用irq_matrix->system_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|45| <<global>> unsigned long system_map[IRQ_MATRIX_SIZE];
+	 *   - kernel/irq/matrix.c|130| <<matrix_alloc_area>> bitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);
+	 *   - kernel/irq/matrix.c|203| <<irq_matrix_assign_system>> set_bit(bit, m->system_map);
+	 *   - kernel/irq/matrix.c|518| <<irq_matrix_debug_show>> unsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);
+	 *   - kernel/irq/matrix.c|526| <<irq_matrix_debug_show>> seq_printf(sf, "System: %u: %*pbl\n", nsys, m->matrix_bits, m->system_map);
+	 */
 	set_bit(bit, m->system_map);
 	if (replace) {
 		BUG_ON(!test_and_clear_bit(bit, cm->alloc_map));
@@ -207,6 +485,10 @@ void irq_matrix_assign_system(struct irq_matrix *m, unsigned int bit,
  * on all CPUs in @msk, but it's not guaranteed that the bits are at the
  * same offset on all CPUs
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|204| <<reserve_managed_vector>> ret = irq_matrix_reserve_managed(vector_matrix, affmsk);
+ */
 int irq_matrix_reserve_managed(struct irq_matrix *m, const struct cpumask *msk)
 {
 	unsigned int cpu, failed_cpu;
@@ -215,11 +497,39 @@ int irq_matrix_reserve_managed(struct irq_matrix *m, const struct cpumask *msk)
 		struct cpumap *cm = per_cpu_ptr(m->maps, cpu);
 		unsigned int bit;
 
+		/*
+		 * 分配的时候不能是下面的bit:
+		 * - irq_matrix->system_map
+		 * - cpumap->managed_map
+		 * - cpumap->alloc_map
+		 * managed_map里的vector可以是reserved也可以是已经分配的
+		 * 真正的分配由alloc_map决定
+		 *
+		 * 如果要managed的: 从cm->managed_map来reserve一个 (以后真正alloc用cm->alloc_map)
+		 * 如果要regular, 从cm->alloc_map来分配一个
+		 *
+		 * called by:
+		 *   - kernel/irq/matrix.c|218| <<irq_matrix_reserve_managed>> bit = matrix_alloc_area(m, cm, 1, true);
+		 *   - kernel/irq/matrix.c|395| <<irq_matrix_alloc>> bit = matrix_alloc_area(m, cm, 1, false);
+		 */
 		bit = matrix_alloc_area(m, cm, 1, true);
 		if (bit >= m->alloc_end)
 			goto cleanup;
+		/*
+		 * 在以下设置cpumap->managed:
+		 *   - kernel/irq/matrix.c|388| <<irq_matrix_reserve_managed>> cm->managed++;
+		 *   - kernel/irq/matrix.c|442| <<irq_matrix_remove_managed>> cm->managed--;
+		 */
 		cm->managed++;
 		if (cm->online) {
+			/*
+			 * 在以下设置cpumap->allocated:
+			 *   - kernel/irq/matrix.c|350| <<irq_matrix_assign_system>> cm->allocated--;
+			 *   - kernel/irq/matrix.c|482| <<irq_matrix_alloc_managed>> cm->allocated++;
+			 *   - kernel/irq/matrix.c|512| <<irq_matrix_assign>> cm->allocated++;
+			 *   - kernel/irq/matrix.c|614| <<irq_matrix_alloc>> cm->allocated++;
+			 *   - kernel/irq/matrix.c|652| <<irq_matrix_free>> cm->allocated--;
+			 */
 			cm->available--;
 			m->global_available--;
 		}
@@ -248,6 +558,10 @@ int irq_matrix_reserve_managed(struct irq_matrix *m, const struct cpumask *msk)
  * allocation when they shut down. If not, the accounting is screwed,
  * but all what can be done at this point is warn about it.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|632| <<vector_free_reserved_and_managed>> irq_matrix_remove_managed(vector_matrix, dest);
+ */
 void irq_matrix_remove_managed(struct irq_matrix *m, const struct cpumask *msk)
 {
 	unsigned int cpu;
@@ -283,6 +597,10 @@ void irq_matrix_remove_managed(struct irq_matrix *m, const struct cpumask *msk)
  * @msk:	Which CPUs to search in
  * @mapped_cpu:	Pointer to store the CPU for which the irq was allocated
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|478| <<assign_managed_vector>> vector = irq_matrix_alloc_managed(vector_matrix, vector_searchmask, &cpu);
+ */
 int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,
 			     unsigned int *mapped_cpu)
 {
@@ -298,13 +616,48 @@ int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,
 
 	cm = per_cpu_ptr(m->maps, cpu);
 	end = m->alloc_end;
+	/*
+	 * 在以下修改cpumap->alloc_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|138| <<matrix_alloc_area>> bitmap_set(cm->alloc_map, area, num);
+	 *   - kernel/irq/matrix.c|205| <<irq_matrix_assign_system>> BUG_ON(!test_and_clear_bit(bit, cm->alloc_map));
+	 *   - kernel/irq/matrix.c|324| <<irq_matrix_alloc_managed>> set_bit(bit, cm->alloc_map);
+	 *   - kernel/irq/matrix.c|346| <<irq_matrix_assign>> if (WARN_ON_ONCE(test_and_set_bit(bit, cm->alloc_map)))
+	 *   - kernel/irq/matrix.c|448| <<irq_matrix_free>> if (WARN_ON_ONCE(!test_and_clear_bit(bit, cm->alloc_map)))
+	 *
+	 * 在以下设置cpumap->managed_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|122| <<matrix_alloc_area>> bitmap_set(cm->managed_map, area, num);
+	 *   - kernel/irq/matrix.c|269| <<irq_matrix_remove_managed>> clear_bit(bit, cm->managed_map);
+	 *
+	 * 如果已经reserve了, 在这里一定会allocate吗???
+	 */
 	/* Get managed bit which are not allocated */
 	bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
 	bit = find_first_bit(m->scratch_map, end);
 	if (bit >= end)
 		return -ENOSPC;
+	/*
+	 * 在以下修改cpumap->alloc_map[IRQ_MATRIX_SIZE]:
+	 *   - kernel/irq/matrix.c|138| <<matrix_alloc_area>> bitmap_set(cm->alloc_map, area, num);
+	 *   - kernel/irq/matrix.c|205| <<irq_matrix_assign_system>> BUG_ON(!test_and_clear_bit(bit, cm->alloc_map));
+	 *   - kernel/irq/matrix.c|324| <<irq_matrix_alloc_managed>> set_bit(bit, cm->alloc_map);
+	 *   - kernel/irq/matrix.c|346| <<irq_matrix_assign>> if (WARN_ON_ONCE(test_and_set_bit(bit, cm->alloc_map)))
+	 *   - kernel/irq/matrix.c|448| <<irq_matrix_free>> if (WARN_ON_ONCE(!test_and_clear_bit(bit, cm->alloc_map)))
+	 */
 	set_bit(bit, cm->alloc_map);
+	/*
+	 * 在以下设置cpumap->allocated:
+	 *   - kernel/irq/matrix.c|350| <<irq_matrix_assign_system>> cm->allocated--;
+	 *   - kernel/irq/matrix.c|482| <<irq_matrix_alloc_managed>> cm->allocated++;
+	 *   - kernel/irq/matrix.c|512| <<irq_matrix_assign>> cm->allocated++;
+	 *   - kernel/irq/matrix.c|614| <<irq_matrix_alloc>> cm->allocated++; --> matrix_alloc_area()的managed是false
+	 *   - kernel/irq/matrix.c|652| <<irq_matrix_free>> cm->allocated--;
+	 */
 	cm->allocated++;
+	/*
+	 * 在以下设置cpumap->managed_allocated:
+	 *   - kernel/irq/matrix.c|483| <<irq_matrix_alloc_managed>> cm->managed_allocated++;
+	 *   - kernel/irq/matrix.c|654| <<irq_matrix_free>> cm->managed_allocated--;
+	 */
 	cm->managed_allocated++;
 	m->total_allocated++;
 	*mapped_cpu = cpu;
@@ -319,12 +672,21 @@ int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,
  *
  * This should only be used to mark preallocated vectors
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|932| <<lapic_assign_system_vectors>> irq_matrix_assign(vector_matrix, ISA_IRQ_VECTOR(i));
+ *
+ * 用来分配preallocated的system vector
+ */
 void irq_matrix_assign(struct irq_matrix *m, unsigned int bit)
 {
 	struct cpumap *cm = this_cpu_ptr(m->maps);
 
 	if (WARN_ON_ONCE(bit < m->alloc_start || bit >= m->alloc_end))
 		return;
+	/*
+	 * 这里会set
+	 */
 	if (WARN_ON_ONCE(test_and_set_bit(bit, cm->alloc_map)))
 		return;
 	cm->allocated++;
@@ -343,11 +705,24 @@ void irq_matrix_assign(struct irq_matrix *m, unsigned int bit)
  * setup interrupt descriptors w/o assigning low level resources to it.
  * The actual allocation happens when the interrupt gets activated.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|335| <<reserve_irq_vector_locked>> irq_matrix_reserve(vector_matrix);
+ */
 void irq_matrix_reserve(struct irq_matrix *m)
 {
 	if (m->global_reserved == m->global_available)
 		pr_warn("Interrupt reservation exceeds available resources\n");
 
+	/*
+	 * 在以下使用irq_matrix->global_reserved:
+	 *   - kernel/irq/matrix.c|489| <<irq_matrix_reserve>> if (m->global_reserved == m->global_available)
+	 *   - kernel/irq/matrix.c|492| <<irq_matrix_reserve>> m->global_reserved++;
+	 *   - kernel/irq/matrix.c|507| <<irq_matrix_remove_reserved>> m->global_reserved--;
+	 *   - kernel/irq/matrix.c|552| <<irq_matrix_alloc>> m->global_reserved--;
+	 *   - kernel/irq/matrix.c|614| <<irq_matrix_reserved>> return m->global_reserved;
+	 *   - kernel/irq/matrix.c|650| <<irq_matrix_debug_show>> seq_printf(sf, "Global reserved: %6u\n", m->global_reserved);
+	 */
 	m->global_reserved++;
 	trace_irq_matrix_reserve(m);
 }
@@ -361,6 +736,10 @@ void irq_matrix_reserve(struct irq_matrix *m)
  * interrupt was never in use and a real vector allocated, which undid the
  * reservation.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|630| <<vector_free_reserved_and_managed>> irq_matrix_remove_reserved(vector_matrix);
+ */
 void irq_matrix_remove_reserved(struct irq_matrix *m)
 {
 	m->global_reserved--;
@@ -374,6 +753,12 @@ void irq_matrix_remove_reserved(struct irq_matrix *m)
  * @reserved:	Allocate previously reserved interrupts
  * @mapped_cpu: Pointer to store the CPU for which the irq was allocated
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|389| <<assign_vector_locked>> vector = irq_matrix_alloc(vector_matrix, dest, resvd, &cpu);
+ *
+ * 从所有mask的cpu中选一个cpu分配一个regular interrupt (不是managed)
+ */
 int irq_matrix_alloc(struct irq_matrix *m, const struct cpumask *msk,
 		     bool reserved, unsigned int *mapped_cpu)
 {
@@ -387,11 +772,29 @@ int irq_matrix_alloc(struct irq_matrix *m, const struct cpumask *msk,
 	if (cpumask_empty(msk))
 		return -EINVAL;
 
+	/*
+	 * 每个percpu的cpumap的cpumap->available不能是0
+	 */
 	cpu = matrix_find_best_cpu(m, msk);
 	if (cpu == UINT_MAX)
 		return -ENOSPC;
 
+	/*
+	 * struct irq_matrix {
+	 * -> struct cpumap __percpu  *maps;
+	 */
 	cm = per_cpu_ptr(m->maps, cpu);
+	/*
+	 * 分配的时候不能是下面的bit:
+	 * - irq_matrix->system_map
+	 * - cpumap->managed_map
+	 * - cpumap->alloc_map
+	 * managed_map里的vector可以是reserved也可以是已经分配的
+	 * 真正的分配由alloc_map决定
+	 *
+	 * 如果要managed的: 从cm->managed_map来reserve一个 (以后真正alloc用cm->alloc_map)
+	 * 如果要regular, 从cm->alloc_map来分配一个
+	 */
 	bit = matrix_alloc_area(m, cm, 1, false);
 	if (bit >= m->alloc_end)
 		return -ENOSPC;
@@ -415,6 +818,13 @@ int irq_matrix_alloc(struct irq_matrix *m, const struct cpumask *msk,
  * @managed:	If true, the interrupt is managed and not accounted
  *		as available.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|273| <<apic_update_vector>> irq_matrix_free(vector_matrix, apicd->cpu, apicd->vector, managed);
+ *   - arch/x86/kernel/apic/vector.c|503| <<clear_irq_vector>> irq_matrix_free(vector_matrix, apicd->cpu, vector, managed);
+ *   - arch/x86/kernel/apic/vector.c|512| <<clear_irq_vector>> irq_matrix_free(vector_matrix, apicd->prev_cpu, vector, managed);
+ *   - arch/x86/kernel/apic/vector.c|1100| <<free_moved_vector>> irq_matrix_free(vector_matrix, cpu, vector, managed);
+ */
 void irq_matrix_free(struct irq_matrix *m, unsigned int cpu,
 		     unsigned int bit, bool managed)
 {
@@ -447,6 +857,10 @@ void irq_matrix_free(struct irq_matrix *m, unsigned int cpu,
  * @cpudown:	If true, the local CPU is about to go down, adjust
  *		the number of available irqs accordingly
  */
+/*
+ * called by:
+ *   arch/x86/kernel/apic/vector.c|1322| <<lapic_can_unplug_cpu>> avl = irq_matrix_available(vector_matrix, true);
+ */
 unsigned int irq_matrix_available(struct irq_matrix *m, bool cpudown)
 {
 	struct cpumap *cm = this_cpu_ptr(m->maps);
@@ -460,6 +874,10 @@ unsigned int irq_matrix_available(struct irq_matrix *m, bool cpudown)
  * irq_matrix_reserved - Get the number of globally reserved irqs
  * @m:		Pointer to the matrix to query
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|1329| <<lapic_can_unplug_cpu>> rsvd = irq_matrix_reserved(vector_matrix);
+ */
 unsigned int irq_matrix_reserved(struct irq_matrix *m)
 {
 	return m->global_reserved;
@@ -471,6 +889,10 @@ unsigned int irq_matrix_reserved(struct irq_matrix *m)
  *
  * This returns number of allocated non-managed interrupts.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|1321| <<lapic_can_unplug_cpu>> tomove = irq_matrix_allocated(vector_matrix);
+ */
 unsigned int irq_matrix_allocated(struct irq_matrix *m)
 {
 	struct cpumap *cm = this_cpu_ptr(m->maps);
@@ -487,6 +909,10 @@ unsigned int irq_matrix_allocated(struct irq_matrix *m)
  *
  * Note, this is a lockless snapshot.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|632| <<x86_vector_debug_show>> irq_matrix_debug_show(m, vector_matrix, ind);
+ */
 void irq_matrix_debug_show(struct seq_file *sf, struct irq_matrix *m, int ind)
 {
 	unsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);
diff --git a/kernel/irq/migration.c b/kernel/irq/migration.c
index 61ca924ef..f6bdc66cd 100644
--- a/kernel/irq/migration.c
+++ b/kernel/irq/migration.c
@@ -15,10 +15,25 @@
  * Returns true if the pending bit was set and the pending mask contains an
  * online CPU other than the dying CPU.
  */
+/*
+ * called by:
+ *   - kernel/irq/cpuhotplug.c|90| <<migrate_one_irq>> irq_fixup_move_pending(desc, false);
+ *   - kernel/irq/cpuhotplug.c|108| <<migrate_one_irq>> if (irq_fixup_move_pending(desc, true))
+ */
 bool irq_fixup_move_pending(struct irq_desc *desc, bool force_clear)
 {
 	struct irq_data *data = irq_desc_get_irq_data(desc);
 
+	/*
+	 * 在以下使用IRQD_SETAFFINITY_PENDING:
+	 *   - kernel/irq/debugfs.c|119| <<global>> BIT_MASK_DESCR(IRQD_SETAFFINITY_PENDING),
+	 *   - include/linux/irq.h|258| <<irqd_is_setaffinity_pending>> return __irqd_to_state(d) & IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|214| <<irqd_set_move_pending>> __irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|219| <<irqd_clr_move_pending>> __irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;
+	 *
+	 * 是否设置了IRQD_SETAFFINITY_PENDING
+	 * 没设置直接return false
+	 */
 	if (!irqd_is_setaffinity_pending(data))
 		return false;
 
@@ -35,6 +50,11 @@ bool irq_fixup_move_pending(struct irq_desc *desc, bool force_clear)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/io_apic.c|1761| <<ioapic_finish_move>> irq_move_masked_irq(data);
+ *   - kernel/irq/migration.c|116| <<__irq_move_irq>> irq_move_masked_irq(idata);
+ */
 void irq_move_masked_irq(struct irq_data *idata)
 {
 	struct irq_desc *desc = irq_data_to_desc(idata);
@@ -44,6 +64,18 @@ void irq_move_masked_irq(struct irq_data *idata)
 	if (likely(!irqd_is_setaffinity_pending(data)))
 		return;
 
+	/*
+	 * 在以下使用IRQD_SETAFFINITY_PENDING:
+	 *   - kernel/irq/debugfs.c|119| <<global>> BIT_MASK_DESCR(IRQD_SETAFFINITY_PENDING),
+	 *   - include/linux/irq.h|258| <<irqd_is_setaffinity_pending>> return __irqd_to_state(d) & IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|214| <<irqd_set_move_pending>> __irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;
+	 *   - kernel/irq/internals.h|219| <<irqd_clr_move_pending>> __irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;
+	 *
+	 * called by:
+	 *   - kernel/irq/migration.c|30| <<irq_fixup_move_pending>> irqd_clr_move_pending(data);
+	 *   - kernel/irq/migration.c|34| <<irq_fixup_move_pending>> irqd_clr_move_pending(data);
+	 *   - kernel/irq/migration.c|47| <<irq_move_masked_irq>> irqd_clr_move_pending(data);
+	 */
 	irqd_clr_move_pending(data);
 
 	/*
@@ -54,6 +86,22 @@ void irq_move_masked_irq(struct irq_data *idata)
 		return;
 	}
 
+	/*
+	 * 在以下使用irq_desc->pending_mask:
+	 *   - kernel/irq/debugfs.c|42| <<irq_debug_show_masks>> msk = desc->pending_mask;
+	 *   - kernel/irq/internals.h|452| <<irq_copy_pending>> cpumask_copy(desc->pending_mask, mask);
+	 *   - kernel/irq/internals.h|457| <<irq_get_pending>> cpumask_copy(mask, desc->pending_mask);
+	 *   - kernel/irq/internals.h|461| <<irq_desc_get_pending_mask>> return desc->pending_mask;
+	 *   - kernel/irq/irqdesc.c|69| <<alloc_masks>> if (!zalloc_cpumask_var_node(&desc->pending_mask, GFP_KERNEL, node)) {
+	 *   - kernel/irq/irqdesc.c|88| <<desc_smp_init>> cpumask_clear(desc->pending_mask);
+	 *   - kernel/irq/irqdesc.c|391| <<free_masks>> free_cpumask_var(desc->pending_mask);
+	 *   - kernel/irq/migration.c|34| <<irq_fixup_move_pending>> if (cpumask_any_and(desc->pending_mask, cpu_online_mask) >= nr_cpu_ids) {
+	 *   - kernel/irq/migration.c|67| <<irq_move_masked_irq>> if (unlikely(cpumask_empty(desc->pending_mask)))
+	 *   - kernel/irq/migration.c|87| <<irq_move_masked_irq>> if (cpumask_any_and(desc->pending_mask, cpu_online_mask) < nr_cpu_ids) {
+	 *   - kernel/irq/migration.c|90| <<irq_move_masked_irq>> ret = irq_do_set_affinity(data, desc->pending_mask, false);
+	 *   - kernel/irq/migration.c|101| <<irq_move_masked_irq>> cpumask_clear(desc->pending_mask);
+	 *   - kernel/irq/proc.c|57| <<show_irq_affinity>> mask = desc->pending_mask;
+	 */
 	if (unlikely(cpumask_empty(desc->pending_mask)))
 		return;
 
@@ -91,6 +139,10 @@ void irq_move_masked_irq(struct irq_data *idata)
 	cpumask_clear(desc->pending_mask);
 }
 
+/*
+ * called by:
+ *   - include/linux/irq.h|627| <<irq_move_irq>> __irq_move_irq(data);
+ */
 void __irq_move_irq(struct irq_data *idata)
 {
 	bool masked;
diff --git a/kernel/irq/proc.c b/kernel/irq/proc.c
index 623b8136e..8ff5d47fd 100644
--- a/kernel/irq/proc.c
+++ b/kernel/irq/proc.c
@@ -90,6 +90,16 @@ static int irq_affinity_hint_proc_show(struct seq_file *m, void *v)
 		return -ENOMEM;
 
 	raw_spin_lock_irqsave(&desc->lock, flags);
+	/*
+	 * 在以下使用irq_desc->affinity_hint:
+	 *   - kernel/irq/manage.c|509| <<__irq_apply_affinity_hint>> desc->affinity_hint = m;
+	 *   - kernel/irq/manage.c|1919| <<__free_irq>> if (WARN_ON_ONCE(desc->affinity_hint))
+	 *   - kernel/irq/manage.c|1920| <<__free_irq>> desc->affinity_hint = NULL;
+	 *   - kernel/irq/proc.c|93| <<irq_affinity_hint_proc_show>> if (desc->affinity_hint)
+	 *   - kernel/irq/proc.c|94| <<irq_affinity_hint_proc_show>> cpumask_copy(mask, desc->affinity_hint);
+	 *
+	 * 注释: hint to user space for preferred irq affinity
+	 */
 	if (desc->affinity_hint)
 		cpumask_copy(mask, desc->affinity_hint);
 	raw_spin_unlock_irqrestore(&desc->lock, flags);
@@ -134,6 +144,11 @@ static inline int irq_select_affinity_usr(unsigned int irq)
 }
 #endif
 
+/*
+ * called by:
+ *   - kernel/irq/proc.c|182| <<irq_affinity_proc_write>> return write_irq_affinity(0, file, buffer, count, pos);
+ *   - kernel/irq/proc.c|188| <<irq_affinity_list_proc_write>> return write_irq_affinity(1, file, buffer, count, pos);
+ */
 static ssize_t write_irq_affinity(int type, struct file *file,
 		const char __user *buffer, size_t count, loff_t *pos)
 {
diff --git a/kernel/stop_machine.c b/kernel/stop_machine.c
index cedb17ba1..2bc6d80ad 100644
--- a/kernel/stop_machine.c
+++ b/kernel/stop_machine.c
@@ -199,6 +199,14 @@ notrace void __weak stop_machine_yield(const struct cpumask *cpumask)
 	cpu_relax();
 }
 
+/*
+ * [0] multi_cpu_stop
+ * [0] cpu_stopper_thread
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ */
 /* This is the cpu_stop function which stops the CPU. */
 static int multi_cpu_stop(void *data)
 {
@@ -220,6 +228,9 @@ static int multi_cpu_stop(void *data)
 		is_active = cpu == cpumask_first(cpumask);
 	} else {
 		cpumask = msdata->active_cpus;
+		/*
+		 * 检测自己在mask上吗
+		 */
 		is_active = cpumask_test_cpu(cpu, cpumask);
 	}
 
@@ -583,6 +594,28 @@ static int __init cpu_stop_init(void)
 }
 early_initcall(cpu_stop_init);
 
+/*
+ * called by:
+ *   - arch/arm/kernel/patch.c|127| <<patch_text>> stop_machine_cpuslocked(patch_text_stop_machine, &patch, NULL);
+ *   - arch/arm/probes/kprobes/core.c|174| <<kprobes_remove_breakpoint>> stop_machine_cpuslocked(__kprobes_remove_breakpoint, &p,
+ *   - arch/arm64/kernel/patching.c|165| <<aarch64_insn_patch_text>> return stop_machine_cpuslocked(aarch64_insn_patch_text_cb, &patch,
+ *   - arch/csky/kernel/probes/kprobes.c|51| <<patch_text>> return stop_machine_cpuslocked(patch_text_cb, &param, cpu_online_mask);
+ *   - arch/parisc/kernel/patch.c|117| <<patch_text>> stop_machine_cpuslocked(patch_text_stop_machine, &patch, NULL);
+ *   - arch/parisc/kernel/patch.c|129| <<patch_text_multiple>> stop_machine_cpuslocked(patch_text_stop_machine, &patch, NULL);
+ *   - arch/powerpc/mm/book3s64/hash_pgtable.c|530| <<hash__change_memory_range>> stop_machine_cpuslocked(change_memory_range_fn, &chmem_parms,
+ *   - arch/powerpc/platforms/powernv/subcore.c|368| <<set_subcores_per_core>> stop_machine_cpuslocked(cpu_update_split_mode, &new_mode,
+ *   - arch/powerpc/platforms/pseries/lpar.c|1680| <<pseries_lpar_resize_hpt>> rc = stop_machine_cpuslocked(pseries_lpar_resize_hpt_commit,
+ *   - arch/riscv/kernel/patch.c|269| <<patch_text>> ret = stop_machine_cpuslocked(patch_text_cb, &patch, cpu_online_mask);
+ *   - arch/s390/kernel/kprobes.c|205| <<arch_arm_kprobe>> stop_machine_cpuslocked(swap_instruction, &args, NULL);
+ *   - arch/s390/kernel/kprobes.c|213| <<arch_disarm_kprobe>> stop_machine_cpuslocked(swap_instruction, &args, NULL);
+ *   - arch/s390/kernel/time.c|699| <<stp_work_fn>> stop_machine_cpuslocked(stp_sync_clock, &stp_sync, cpu_online_mask);
+ *   - arch/x86/kernel/cpu/microcode/core.c|562| <<load_late_stop_cpus>> stop_machine_cpuslocked(load_cpus_stopped, NULL, cpu_online_mask);
+ *   - arch/x86/kernel/cpu/mtrr/mtrr.c|185| <<set_mtrr>> stop_machine_cpuslocked(mtrr_rendezvous_handler, &data, cpu_online_mask);
+ *   - arch/xtensa/kernel/jump_label.c|63| <<patch_text>> stop_machine_cpuslocked(patch_text_stop_machine,
+ *   - include/linux/stop_machine.h|161| <<stop_machine>> return stop_machine_cpuslocked(fn, data, cpus);
+ *   - kernel/cpu.c|1353| <<takedown_cpu>> err = stop_machine_cpuslocked(take_cpu_down, NULL, cpumask_of(cpu));
+ *   - kernel/stop_machine.c|628| <<stop_machine>> ret = stop_machine_cpuslocked(fn, data, cpus);
+ */
 int stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,
 			    const struct cpumask *cpus)
 {
diff --git a/kernel/time/hrtimer.c b/kernel/time/hrtimer.c
index edb0f821d..87d3ab9b2 100644
--- a/kernel/time/hrtimer.c
+++ b/kernel/time/hrtimer.c
@@ -1750,6 +1750,11 @@ static void __hrtimer_run_queues(struct hrtimer_cpu_base *cpu_base, ktime_t now,
 			if (basenow < hrtimer_get_softexpires_tv64(timer))
 				break;
 
+			/*
+			 * 例子:
+			 * - perf_mux_hrtimer_handler
+			 * - tick_nohz_highres_handler
+			 */
 			__run_hrtimer(cpu_base, base, timer, &basenow, flags);
 			if (active_mask == HRTIMER_ACTIVE_SOFT)
 				hrtimer_sync_wait_running(cpu_base, flags);
diff --git a/net/core/dev.c b/net/core/dev.c
index 76e6438f4..4070cc35f 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -2551,6 +2551,13 @@ static void xps_copy_dev_maps(struct xps_dev_maps *dev_maps,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/ethernet/ibm/ibmvnic.c|299| <<ibmvnic_set_affinity>> rc = __netif_set_xps_queue(adapter->netdev, cpumask_bits(queue->affinity_mask), i_txqs - 1, XPS_CPUS);
+ *   - drivers/net/virtio_net.c|2870| <<virtnet_set_affinity>> __netif_set_xps_queue(vi->dev, cpumask_bits(mask), i, XPS_CPUS);
+ *   - net/core/dev.c|2765| <<netif_set_xps_queue>> ret = __netif_set_xps_queue(dev, cpumask_bits(mask), index, XPS_CPUS);
+ *   - net/core/net-sysfs.c|1633| <<xps_rxqs_store>> err = __netif_set_xps_queue(dev, mask, index, XPS_RXQS);
+ */
 /* Must be called under cpus_read_lock */
 int __netif_set_xps_queue(struct net_device *dev, const unsigned long *mask,
 			  u16 index, enum xps_map_type type)
diff --git a/net/ethtool/ioctl.c b/net/ethtool/ioctl.c
index 7519b0818..aa1187e64 100644
--- a/net/ethtool/ioctl.c
+++ b/net/ethtool/ioctl.c
@@ -1845,6 +1845,10 @@ static noinline_for_stack int ethtool_get_channels(struct net_device *dev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - net/ethtool/ioctl.c|3049| <<__dev_ethtool(ETHTOOL_SCHANNELS)>> rc = ethtool_set_channels(dev, useraddr);
+ */
 static noinline_for_stack int ethtool_set_channels(struct net_device *dev,
 						   void __user *useraddr)
 {
@@ -2812,6 +2816,10 @@ static int ethtool_set_fecparam(struct net_device *dev, void __user *useraddr)
 
 /* The main entry point in this file.  Called from net/core/dev_ioctl.c */
 
+/*
+ * called by:
+ *   - net/ethtool/ioctl.c|3139| <<dev_ethtool>> rc = __dev_ethtool(net, ifr, useraddr, ethcmd, state);
+ */
 static int
 __dev_ethtool(struct net *net, struct ifreq *ifr, void __user *useraddr,
 	      u32 ethcmd, struct ethtool_devlink_compat *devlink_state)
diff --git a/tools/include/linux/bitmap.h b/tools/include/linux/bitmap.h
index f3566ea0f..02e0d525b 100644
--- a/tools/include/linux/bitmap.h
+++ b/tools/include/linux/bitmap.h
@@ -77,6 +77,15 @@ static inline void bitmap_or(unsigned long *dst, const unsigned long *src1,
 		__bitmap_or(dst, src1, src2, nbits);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/arch_timer.c|328| <<test_run>> vcpu_done_map = bitmap_zalloc(test_args.nr_vcpus);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|460| <<check_write_in_dirty_log>> bmap = bitmap_zalloc(size / getpagesize());
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|754| <<run_test>> bmap = bitmap_zalloc(host_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|755| <<run_test>> host_bmap_track = bitmap_zalloc(host_num_pages);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|383| <<memstress_alloc_bitmaps>> bitmaps[i] = bitmap_zalloc(pages_per_slot);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_dirty_log_test.c|112| <<main>> bmap = bitmap_zalloc(TEST_MEM_PAGES);
+ */
 /**
  * bitmap_zalloc - Allocate bitmap
  * @nbits: Number of bits
diff --git a/tools/include/uapi/linux/perf_event.h b/tools/include/uapi/linux/perf_event.h
index 3a64499b0..98bc435a2 100644
--- a/tools/include/uapi/linux/perf_event.h
+++ b/tools/include/uapi/linux/perf_event.h
@@ -548,6 +548,15 @@ struct perf_event_query_bpf {
 /*
  * Ioctls that can be done on a perf event fd:
  */
+/*
+ * PERF_EVENT_IOC_ENABLE主要被perf相关的调用:
+ *   - include/uapi/linux/perf_event.h|551| <<global>> #define PERF_EVENT_IOC_ENABLE _IO ('$', 0)
+ *   - tools/include/uapi/linux/perf_event.h|551| <<global>> #define PERF_EVENT_IOC_ENABLE _IO ('$', 0)
+ *   - kernel/events/core.c|5880| <<_perf_ioctl>> case PERF_EVENT_IOC_ENABLE:
+ *   - tools/lib/perf/evsel.c|444| <<perf_evsel__enable_cpu>> return perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, cpu_map_idx);
+ *   - tools/lib/perf/evsel.c|454| <<perf_evsel__enable_thread>> err = perf_evsel__ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, idx, thread);
+ *   - tools/lib/perf/evsel.c|468| <<perf_evsel__enable>> err = perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, i);
+ */
 #define PERF_EVENT_IOC_ENABLE			_IO ('$', 0)
 #define PERF_EVENT_IOC_DISABLE			_IO ('$', 1)
 #define PERF_EVENT_IOC_REFRESH			_IO ('$', 2)
diff --git a/tools/lib/perf/cpumap.c b/tools/lib/perf/cpumap.c
index 4adcd7920..5b8d806e6 100644
--- a/tools/lib/perf/cpumap.c
+++ b/tools/lib/perf/cpumap.c
@@ -16,6 +16,35 @@ void perf_cpu_map__set_nr(struct perf_cpu_map *map, int nr_cpus)
 	RC_CHK_ACCESS(map)->nr = nr_cpus;
 }
 
+/*
+ * (gdb) bt
+ * #0  perf_cpu_map__alloc (nr_cpus=4) at cpumap.c:21
+ * #1  0x0000aaaaaac46f50 in cpu_map__trim_new (nr_cpus=4, tmp_cpus=0xaaaaaadcf8f0) at cpumap.c:135
+ * #2  0x0000aaaaaac4731c in perf_cpu_map__read (file=0xaaaaaadb2d30) at cpumap.c:207
+ * #3  0x0000aaaaaabb5a20 in pmu_cpumask (dirfd=dirfd@entry=3, name=name@entry=0xaaaaaadc67d3 "armv8_pmuv3_0", is_core=is_core@entry=true) at util/pmu.c:707
+ * #4  0x0000aaaaaabb9cb0 in perf_pmu__lookup (pmus=0xaaaaaad97938 <core_pmus>, dirfd=dirfd@entry=3, name=name@entry=0xaaaaaadc67d3 "armv8_pmuv3_0") at util/pmu.c:1027
+ * #5  0x0000aaaaaabba4d0 in perf_pmu__find2 (name=0xaaaaaadc67d3 "armv8_pmuv3_0", dirfd=3) at util/pmus.c:161
+ * #6  pmu_read_sysfs (core_only=core_only@entry=false) at util/pmus.c:209
+ * #7  0x0000aaaaaabba73c in pmu_read_sysfs (core_only=false) at util/pmus.c:190
+ * #8  perf_pmus__scan (pmu=pmu@entry=0x0) at util/pmus.c:263
+ * #9  0x0000aaaaaab6e8d8 in parse_events_multi_pmu_add (parse_state=parse_state@entry=0xffffffffa200, event_name=0xaaaaaadc66a0 "mem_access", const_parsed_terms=const_parsed_terms@entry=0x0,
+ *     listp=listp@entry=0xffffffff8770, loc_=loc_@entry=0xffffffff9450) at util/parse-events.c:1576
+ * #10 0x0000aaaaaabb4db4 in parse_events_parse (_parse_state=_parse_state@entry=0xffffffffa200, scanner=0xaaaaaadb6500) at util/parse-events.y:355
+ * #11 0x0000aaaaaab6b6b8 in parse_events__scanner (parse_state=0xffffffffa200, input=0x0, str=0xfffffffff852 "mem_access") at util/parse-events.c:1869
+ * #12 __parse_events (evlist=0xaaaaaadb5a50, str=str@entry=0xfffffffff852 "mem_access", pmu_filter=<optimized out>, err=err@entry=0xffffffffa288, fake_pmu=fake_pmu@entry=0x0,
+ *     warn_if_reordered=warn_if_reordered@entry=true) at util/parse-events.c:2137
+ * #13 0x0000aaaaaab6bf18 in parse_events_option (opt=<optimized out>, str=0xfffffffff852 "mem_access", unset=<optimized out>) at util/parse-events.c:2322
+ * #14 0x0000aaaaaac508c4 in get_value (p=p@entry=0xffffffffa488, opt=0xaaaaaad90318 <stat_options+96>, flags=flags@entry=1) at parse-options.c:251
+ * #15 0x0000aaaaaac51650 in parse_short_opt (options=<optimized out>, p=<optimized out>) at parse-options.c:351
+ * #16 parse_options_step (usagestr=0xffffffffa628, options=0xaaaaaad902b8 <stat_options>, ctx=0xffffffffa488) at parse-options.c:539
+ * #17 parse_options_subcommand (argc=argc@entry=3, argv=argv@entry=0xfffffffff5d0, options=options@entry=0xaaaaaad902b8 <stat_options>, subcommands=0xffffffffa638, subcommands@entry=0xfffffffff378,
+ *     usagestr=usagestr@entry=0xffffffffa628, flags=flags@entry=2) at parse-options.c:654
+ * #18 0x0000aaaaaaae4e08 in cmd_stat (argc=3, argv=0xfffffffff5d0) at builtin-stat.c:2506
+ * #19 0x0000aaaaaab4178c in run_builtin (p=p@entry=0xaaaaaad92140 <commands+336>, argc=argc@entry=3, argv=argv@entry=0xfffffffff5d0) at perf.c:349
+ * #20 0x0000aaaaaaac8738 in handle_internal_command (argv=0xfffffffff5d0, argc=3) at perf.c:402
+ * #21 run_argv (argv=<synthetic pointer>, argcp=<synthetic pointer>) at perf.c:446
+ * #22 main (argc=3, argv=0xfffffffff5d0) at perf.c:562
+ */
 struct perf_cpu_map *perf_cpu_map__alloc(int nr_cpus)
 {
 	RC_STRUCT(perf_cpu_map) *cpus = malloc(sizeof(*cpus) + sizeof(struct perf_cpu) * nr_cpus);
@@ -154,6 +183,11 @@ static struct perf_cpu_map *cpu_map__trim_new(int nr_cpus, const struct perf_cpu
 	return cpus;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/cpumap.c|103| <<cpu_map__new_sysfs_online>> cpus = perf_cpu_map__read(onlnf);
+ *   - tools/perf/util/pmu.c|707| <<pmu_cpumask>> cpus = perf_cpu_map__read(file);
+ */
 struct perf_cpu_map *perf_cpu_map__read(FILE *file)
 {
 	struct perf_cpu_map *cpus = NULL;
@@ -412,6 +446,12 @@ bool perf_cpu_map__is_subset(const struct perf_cpu_map *a, const struct perf_cpu
  * other has its reference count increased.
  */
 
+/*
+ * called by:
+ *   - tools/lib/perf/evlist.c|78| <<__perf_evlist__propagate_maps>> evlist->all_cpus = perf_cpu_map__merge(evlist->all_cpus, evsel->cpus);
+ *   - tools/perf/tests/cpumap.c|163| <<test__cpu_map_merge>> struct perf_cpu_map *c = perf_cpu_map__merge(a, b);
+ *   - tools/perf/tests/cpumap.c|236| <<test__cpu_map_equal>> tmp = perf_cpu_map__merge(perf_cpu_map__get(one), two);
+ */
 struct perf_cpu_map *perf_cpu_map__merge(struct perf_cpu_map *orig,
 					 struct perf_cpu_map *other)
 {
diff --git a/tools/lib/perf/evlist.c b/tools/lib/perf/evlist.c
index 058e3ff10..1b3f4c80d 100644
--- a/tools/lib/perf/evlist.c
+++ b/tools/lib/perf/evlist.c
@@ -33,6 +33,42 @@ void perf_evlist__init(struct perf_evlist *evlist)
 	perf_evlist__reset_id_hash(evlist);
 }
 
+/*
+ * (gdb) bt
+ * #0  __perf_evlist__propagate_maps (evlist=0xaaaaaadb5a50, evsel=0xaaaaaadd2b80) at evlist.c:39
+ * #1  0x0000aaaaaac4a728 in perf_evlist__add (evlist=0xaaaaaadb5a50, evsel=0xaaaaaadd2b80) at evlist.c:110
+ * #2  0x0000aaaaaab589e0 in evlist__add (entry=0xaaaaaadd2b80, evlist=0xaaaaaadb5a50) at util/evlist.c:192
+ * #3  evlist__splice_list_tail (evlist=evlist@entry=0xaaaaaadb5a50, list=list@entry=0xffffffffa200) at util/evlist.c:213
+ * #4  0x0000aaaaaab6b9d8 in __parse_events (evlist=0xaaaaaadb5a50, str=str@entry=0xfffffffff852 "mem_access", pmu_filter=<optimized out>, err=err@entry=0xffffffffa288, fake_pmu=fake_pmu@entry=0x0,
+ *     warn_if_reordered=warn_if_reordered@entry=true) at util/parse-events.c:2154
+ * #5  0x0000aaaaaab6bf18 in parse_events_option (opt=<optimized out>, str=0xfffffffff852 "mem_access", unset=<optimized out>) at util/parse-events.c:2322
+ * #6  0x0000aaaaaac508c4 in get_value (p=p@entry=0xffffffffa488, opt=0xaaaaaad90318 <stat_options+96>, flags=flags@entry=1) at parse-options.c:251
+ * #7  0x0000aaaaaac51650 in parse_short_opt (options=<optimized out>, p=<optimized out>) at parse-options.c:351
+ * #8  parse_options_step (usagestr=0xffffffffa628, options=0xaaaaaad902b8 <stat_options>, ctx=0xffffffffa488) at parse-options.c:539
+ * #9  parse_options_subcommand (argc=argc@entry=3, argv=argv@entry=0xfffffffff5d0, options=options@entry=0xaaaaaad902b8 <stat_options>, subcommands=0xffffffffa638, subcommands@entry=0xfffffffff378,
+ *     usagestr=usagestr@entry=0xffffffffa628, flags=flags@entry=2) at parse-options.c:654
+ * #10 0x0000aaaaaaae4e08 in cmd_stat (argc=3, argv=0xfffffffff5d0) at builtin-stat.c:2506
+ * #11 0x0000aaaaaab4178c in run_builtin (p=p@entry=0xaaaaaad92140 <commands+336>, argc=argc@entry=3, argv=argv@entry=0xfffffffff5d0) at perf.c:349
+ * #12 0x0000aaaaaaac8738 in handle_internal_command (argv=0xfffffffff5d0, argc=3) at perf.c:402
+ * #13 run_argv (argv=<synthetic pointer>, argcp=<synthetic pointer>) at perf.c:446
+ * #14 main (argc=3, argv=0xfffffffff5d0) at perf.c:562
+ *
+ * (gdb) bt
+ * #0  __perf_evlist__propagate_maps (evlist=0xaaaaaadb5a50, evsel=0xaaaaaadd2b80) at evlist.c:39
+ * #1  0x0000aaaaaac4a6a0 in perf_evlist__propagate_maps (evlist=0xaaaaaadb5a50) at evlist.c:99
+ * #2  0x0000aaaaaac4a9c4 in perf_evlist__set_maps (evlist=0xaaaaaadb5a50, cpus=0xaaaaaadd5380, threads=0xaaaaaadd58c0) at evlist.c:206
+ * #3  0x0000aaaaaab5b0ec in evlist__create_maps (evlist=0xaaaaaadb5a50, target=target@entry=0xaaaaaad8bc78 <target>) at util/evlist.c:1079
+ * #4  0x0000aaaaaaae5668 in cmd_stat (argc=-1429397504, argv=0xfffffffff5d0) at builtin-stat.c:2739
+ * #5  0x0000aaaaaab4178c in run_builtin (p=p@entry=0xaaaaaad92140 <commands+336>, argc=argc@entry=3, argv=argv@entry=0xfffffffff5d0) at perf.c:349
+ * #6  0x0000aaaaaaac8738 in handle_internal_command (argv=0xfffffffff5d0, argc=3) at perf.c:402
+ * #7  run_argv (argv=<synthetic pointer>, argcp=<synthetic pointer>) at perf.c:446
+ * #8  main (argc=3, argv=0xfffffffff5d0) at perf.c:562
+ *
+ * called by:
+ *   - tools/lib/perf/evlist.c|88| <<perf_evlist__propagate_maps>> __perf_evlist__propagate_maps(evlist, evsel);
+ *   - tools/lib/perf/evlist.c|99| <<perf_evlist__add>> __perf_evlist__propagate_maps(evlist, evsel);
+ *   - tools/lib/perf/evlist.c|747| <<perf_evlist__go_system_wide>> __perf_evlist__propagate_maps(evlist, evsel);
+ */
 static void __perf_evlist__propagate_maps(struct perf_evlist *evlist,
 					  struct perf_evsel *evsel)
 {
@@ -75,9 +111,25 @@ static void __perf_evlist__propagate_maps(struct perf_evlist *evlist,
 		evsel->threads = perf_thread_map__get(evlist->threads);
 	}
 
+	/*
+	 * struct perf_evsel *evsel:
+	 * -> struct perf_cpu_map     *cpus;
+	 *
+	 * DECLARE_RC_STRUCT(perf_cpu_map) {
+	 *     refcount_t      refcnt;
+	 *     // Length of the map array.
+	 *     int             nr;
+	 *     // The CPU values.
+	 *     struct perf_cpu map[];
+	 * };
+	 */
 	evlist->all_cpus = perf_cpu_map__merge(evlist->all_cpus, evsel->cpus);
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/evlist.c|195| <<perf_evlist__set_maps>> perf_evlist__propagate_maps(evlist);
+ */
 static void perf_evlist__propagate_maps(struct perf_evlist *evlist)
 {
 	struct perf_evsel *evsel;
@@ -88,6 +140,22 @@ static void perf_evlist__propagate_maps(struct perf_evlist *evlist)
 		__perf_evlist__propagate_maps(evlist, evsel);
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/Documentation/examples/counting.c|56| <<main>> perf_evlist__add(evlist, evsel);
+ *   - tools/lib/perf/Documentation/examples/counting.c|62| <<main>> perf_evlist__add(evlist, evsel);
+ *   - tools/lib/perf/Documentation/examples/sampling.c|60| <<main>> perf_evlist__add(evlist, evsel);
+ *   - tools/lib/perf/tests/test-evlist.c|58| <<test_stat_cpu>> perf_evlist__add(evlist, evsel);
+ *   - tools/lib/perf/tests/test-evlist.c|63| <<test_stat_cpu>> perf_evlist__add(evlist, evsel);
+ *   - tools/lib/perf/tests/test-evlist.c|119| <<test_stat_thread>> perf_evlist__add(evlist, evsel);
+ *   - tools/lib/perf/tests/test-evlist.c|124| <<test_stat_thread>> perf_evlist__add(evlist, evsel);
+ *   - tools/lib/perf/tests/test-evlist.c|176| <<test_stat_thread_enable>> perf_evlist__add(evlist, evsel);
+ *   - tools/lib/perf/tests/test-evlist.c|181| <<test_stat_thread_enable>> perf_evlist__add(evlist, evsel);
+ *   - tools/lib/perf/tests/test-evlist.c|276| <<test_mmap_thread>> perf_evlist__add(evlist, evsel);
+ *   - tools/lib/perf/tests/test-evlist.c|363| <<test_mmap_cpus>> perf_evlist__add(evlist, evsel);
+ *   - tools/lib/perf/tests/test-evlist.c|505| <<test_stat_multiplexing>> perf_evlist__add(evlist, evsel);
+ *   - tools/perf/util/evlist.c|192| <<evlist__add>> perf_evlist__add(&evlist->core, &entry->core);
+ */
 void perf_evlist__add(struct perf_evlist *evlist,
 		      struct perf_evsel *evsel)
 {
diff --git a/tools/lib/perf/evsel.c b/tools/lib/perf/evsel.c
index c07160953..c0e8035d4 100644
--- a/tools/lib/perf/evsel.c
+++ b/tools/lib/perf/evsel.c
@@ -76,6 +76,31 @@ static int perf_evsel__alloc_mmap(struct perf_evsel *evsel, int ncpus, int nthre
 	return evsel->mmap != NULL ? 0 : -ENOMEM;
 }
 
+/*
+ * 运行cycles
+ * (gdb) bt
+ * #0  sys_perf_event_open (flags=8, group_fd=-1, cpu=0, pid=-1, attr=0xaaaaaadcdae0) at util/../perf-sys.h:17
+ * #1  evsel__open_cpu (evsel=0xaaaaaadcdad0, cpus=0xaaaaaadb55d0, threads=<optimized out>, start_cpu_map_idx=<optimized out>, end_cpu_map_idx=<optimized out>) at util/evsel.c:2054
+ * #2  0x0000aaaaaaae6814 in __run_perf_stat (run_idx=2, argv=0xfffffffff600, argc=-1429401600) at builtin-stat.c:745
+ * #3  run_perf_stat (run_idx=2, argv=0xfffffffff600, argc=-1429401600) at builtin-stat.c:960
+ * #4  cmd_stat (argc=-1429401600, argv=0xfffffffff600) at builtin-stat.c:2832
+ * #5  0x0000aaaaaab416fc in run_builtin (p=p@entry=0xaaaaaad91140 <commands+336>, argc=argc@entry=3, argv=argv@entry=0xfffffffff600) at perf.c:349
+ * * #6  0x0000aaaaaaac86f8 in handle_internal_command (argv=0xfffffffff600, argc=3) at perf.c:402
+ * #7  run_argv (argv=<synthetic pointer>, argcp=<synthetic pointer>) at perf.c:446
+ * #8  main (argc=3, argv=0xfffffffff600) at perf.c:562
+ *
+ * 运行mem_access
+ * (gdb) bt
+ * #0  sys_perf_event_open (flags=8, group_fd=-1, cpu=3, pid=-1, attr=0xaaaaaadd1b80) at util/../perf-sys.h:17
+ * #1  evsel__open_cpu (evsel=0xaaaaaadd1b70, cpus=0xaaaaaadd08f0, threads=<optimized out>, start_cpu_map_idx=<optimized out>, end_cpu_map_idx=<optimized out>) at util/evsel.c:2054
+ * #2  0x0000aaaaaaae6814 in __run_perf_stat (run_idx=2, argv=0xfffffffff600, argc=-1429401600) at builtin-stat.c:745
+ * #3  run_perf_stat (run_idx=2, argv=0xfffffffff600, argc=-1429401600) at builtin-stat.c:960
+ * #4  cmd_stat (argc=-1429401600, argv=0xfffffffff600) at builtin-stat.c:2832
+ * #5  0x0000aaaaaab416fc in run_builtin (p=p@entry=0xaaaaaad91140 <commands+336>, argc=argc@entry=3, argv=argv@entry=0xfffffffff600) at perf.c:349
+ * #6  0x0000aaaaaaac86f8 in handle_internal_command (argv=0xfffffffff600, argc=3) at perf.c:402
+ * #7  run_argv (argv=<synthetic pointer>, argcp=<synthetic pointer>) at perf.c:446
+ * #8  main (argc=3, argv=0xfffffffff600) at perf.c:562
+ */
 static int
 sys_perf_event_open(struct perf_event_attr *attr,
 		    pid_t pid, struct perf_cpu cpu, int group_fd,
@@ -242,6 +267,11 @@ void perf_evsel__munmap(struct perf_evsel *evsel)
 	evsel->mmap = NULL;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/tests/test-evsel.c|153| <<test_stat_user_read>> err = perf_evsel__mmap(evsel, 0);
+ *   - tools/perf/tests/mmap-basic.c|204| <<test_stat_user_read>> err = perf_evsel__mmap(evsel, 0);
+ */
 int perf_evsel__mmap(struct perf_evsel *evsel, int pages)
 {
 	int ret, idx, thread;
@@ -279,6 +309,11 @@ int perf_evsel__mmap(struct perf_evsel *evsel, int pages)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/tests/test-evsel.c|156| <<test_stat_user_read>> pc = perf_evsel__mmap_base(evsel, 0, 0);
+ *   - tools/perf/tests/mmap-basic.c|211| <<test_stat_user_read>> pc = perf_evsel__mmap_base(evsel, 0, 0);
+ */
 void *perf_evsel__mmap_base(struct perf_evsel *evsel, int cpu_map_idx, int thread)
 {
 	int *fd = FD(evsel, cpu_map_idx, thread);
@@ -289,6 +324,12 @@ void *perf_evsel__mmap_base(struct perf_evsel *evsel, int cpu_map_idx, int threa
 	return MMAP(evsel, cpu_map_idx, thread)->base;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/evsel.c|324| <<perf_evsel__read_group>> size_t size = perf_evsel__read_size(evsel);
+ *   - tools/lib/perf/evsel.c|390| <<perf_evsel__read>> size_t size = perf_evsel__read_size(evsel);
+ *   - tools/perf/util/evsel.c|1577| <<evsel__read_group>> int size = perf_evsel__read_size(&leader->core);
+ */
 int perf_evsel__read_size(struct perf_evsel *evsel)
 {
 	u64 read_format = evsel->attr.read_format;
@@ -317,6 +358,10 @@ int perf_evsel__read_size(struct perf_evsel *evsel)
 	return size;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/evsel.c|401| <<perf_evsel__read>> return perf_evsel__read_group(evsel, cpu_map_idx, thread, count);
+ */
 /* This only reads values for the leader */
 static int perf_evsel__read_group(struct perf_evsel *evsel, int cpu_map_idx,
 				  int thread, struct perf_counts_values *count)
@@ -363,6 +408,10 @@ static int perf_evsel__read_group(struct perf_evsel *evsel, int cpu_map_idx,
  * The perf read format is very flexible.  It needs to set the proper
  * values according to the read format.
  */
+/*
+ * called by:
+ *   - tools/lib/perf/evsel.c|411| <<perf_evsel__read>> perf_evsel__adjust_values(evsel, buf.values, count);
+ */
 static void perf_evsel__adjust_values(struct perf_evsel *evsel, u64 *buf,
 				      struct perf_counts_values *count)
 {
@@ -384,6 +433,32 @@ static void perf_evsel__adjust_values(struct perf_evsel *evsel, u64 *buf,
 		count->lost = buf[n++];
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/Documentation/examples/counting.c|73| <<main>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evlist.c|80| <<test_stat_cpu>> perf_evsel__read(evsel, idx, 0, &counts);
+ *   - tools/lib/perf/tests/test-evlist.c|136| <<test_stat_thread>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evlist.c|193| <<test_stat_thread_enable>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evlist.c|200| <<test_stat_thread_enable>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evlist.c|479| <<test_stat_multiplexing>> perf_evsel__read(evsel, 0, 0, &expected_counts);
+ *   - tools/lib/perf/tests/test-evlist.c|521| <<test_stat_multiplexing>> perf_evsel__read(evsel, 0, 0, &counts[i]);
+ *   - tools/lib/perf/tests/test-evsel.c|42| <<test_stat_cpu>> perf_evsel__read(evsel, idx, 0, &counts);
+ *   - tools/lib/perf/tests/test-evsel.c|75| <<test_stat_thread>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evsel.c|108| <<test_stat_thread_enable>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evsel.c|114| <<test_stat_thread_enable>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evsel.c|165| <<test_stat_user_read>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evsel.c|174| <<test_stat_user_read>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evsel.c|179| <<test_stat_user_read>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evsel.c|213| <<test_stat_read_format_single>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evsel.c|259| <<test_stat_read_format_group>> perf_evsel__read(leader, 0, 0, &counts);
+ *   - tools/lib/perf/tests/test-evsel.c|272| <<test_stat_read_format_group>> perf_evsel__read(member, 0, 0, &counts);
+ *   - tools/perf/builtin-record.c|1950| <<record__read_lost_samples>> if (perf_evsel__read(&evsel->core, x, y, &count) < 0) {
+ *   - tools/perf/tests/event-times.c|200| <<record__read_lost_samples>> perf_evsel__read(&evsel->core, 0, 0, &count);
+ *   - tools/perf/tests/mmap-basic.c|228| <<test_stat_user_read>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/perf/tests/mmap-basic.c|240| <<test_stat_user_read>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/perf/tests/mmap-basic.c|245| <<test_stat_user_read>> perf_evsel__read(evsel, 0, 0, &counts);
+ *   - tools/perf/util/evsel.c|1521| <<evsel__read_one>> return perf_evsel__read(&evsel->core, cpu_map_idx, thread, count);
+ */
 int perf_evsel__read(struct perf_evsel *evsel, int cpu_map_idx, int thread,
 		     struct perf_counts_values *count)
 {
@@ -412,6 +487,11 @@ int perf_evsel__read(struct perf_evsel *evsel, int cpu_map_idx, int thread,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/evsel.c|433| <<perf_evsel__run_ioctl>> int err = perf_evsel__ioctl(evsel, ioc, arg, cpu_map_idx, thread);
+ *   - tools/lib/perf/evsel.c|472| <<perf_evsel__enable_thread>> err = perf_evsel__ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, idx, thread);
+ */
 static int perf_evsel__ioctl(struct perf_evsel *evsel, int ioc, void *arg,
 			     int cpu_map_idx, int thread)
 {
@@ -423,6 +503,14 @@ static int perf_evsel__ioctl(struct perf_evsel *evsel, int ioc, void *arg,
 	return ioctl(*fd, ioc, arg);
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/evsel.c|453| <<perf_evsel__enable_cpu>> return perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, cpu_map_idx);
+ *   - tools/lib/perf/evsel.c|495| <<perf_evsel__enable>> err = perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, i);
+ *   - tools/lib/perf/evsel.c|501| <<perf_evsel__disable_cpu>> return perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_DISABLE, NULL, cpu_map_idx);
+ *   - tools/lib/perf/evsel.c|510| <<perf_evsel__disable>> err = perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_DISABLE, NULL, i);
+ *   - tools/lib/perf/evsel.c|519| <<perf_evsel__apply_filter>> err = perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_SET_FILTER, (void *)filter, i);
+ */
 static int perf_evsel__run_ioctl(struct perf_evsel *evsel,
 				 int ioc,  void *arg,
 				 int cpu_map_idx)
@@ -439,17 +527,44 @@ static int perf_evsel__run_ioctl(struct perf_evsel *evsel,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/auxtrace.c|659| <<evlist__enable_event_idx>> return perf_evsel__enable_cpu(&evsel->core, cpu_map_idx);
+ *   - tools/perf/util/evsel.c|1414| <<evsel__enable_cpu>> return perf_evsel__enable_cpu(&evsel->core, cpu_map_idx);
+ */
 int perf_evsel__enable_cpu(struct perf_evsel *evsel, int cpu_map_idx)
 {
+	/*
+	 * PERF_EVENT_IOC_ENABLE主要被perf相关的调用:
+	 *   - include/uapi/linux/perf_event.h|551| <<global>> #define PERF_EVENT_IOC_ENABLE _IO ('$', 0)
+	 *   - tools/include/uapi/linux/perf_event.h|551| <<global>> #define PERF_EVENT_IOC_ENABLE _IO ('$', 0)
+	 *   - kernel/events/core.c|5880| <<_perf_ioctl>> case PERF_EVENT_IOC_ENABLE:
+	 *   - tools/lib/perf/evsel.c|444| <<perf_evsel__enable_cpu>> return perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, cpu_map_idx);
+	 *   - tools/lib/perf/evsel.c|454| <<perf_evsel__enable_thread>> err = perf_evsel__ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, idx, thread);
+	 *   - tools/lib/perf/evsel.c|468| <<perf_evsel__enable>> err = perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, i);
+	 */
 	return perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, cpu_map_idx);
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/auxtrace.c|662| <<evlist__enable_event_idx>> return perf_evsel__enable_thread(&evsel->core, idx);
+ */
 int perf_evsel__enable_thread(struct perf_evsel *evsel, int thread)
 {
 	struct perf_cpu cpu __maybe_unused;
 	int idx;
 	int err;
 
+	/*
+	 * PERF_EVENT_IOC_ENABLE主要被perf相关的调用:
+	 *   - include/uapi/linux/perf_event.h|551| <<global>> #define PERF_EVENT_IOC_ENABLE _IO ('$', 0)
+	 *   - tools/include/uapi/linux/perf_event.h|551| <<global>> #define PERF_EVENT_IOC_ENABLE _IO ('$', 0)
+	 *   - kernel/events/core.c|5880| <<_perf_ioctl>> case PERF_EVENT_IOC_ENABLE:
+	 *   - tools/lib/perf/evsel.c|444| <<perf_evsel__enable_cpu>> return perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, cpu_map_idx);
+	 *   - tools/lib/perf/evsel.c|454| <<perf_evsel__enable_thread>> err = perf_evsel__ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, idx, thread);
+	 *   - tools/lib/perf/evsel.c|468| <<perf_evsel__enable>> err = perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, i);
+	 */
 	perf_cpu_map__for_each_cpu(cpu, idx, evsel->cpus) {
 		err = perf_evsel__ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, idx, thread);
 		if (err)
@@ -459,11 +574,27 @@ int perf_evsel__enable_thread(struct perf_evsel *evsel, int thread)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/evlist.c|229| <<perf_evlist__enable>> perf_evsel__enable(evsel);
+ *   - tools/lib/perf/tests/test-evlist.c|471| <<test_stat_multiplexing>> err = perf_evsel__enable(evsel);
+ *   - tools/lib/perf/tests/test-evsel.c|111| <<test_stat_thread_enable>> err = perf_evsel__enable(evsel);
+ *   - tools/perf/util/evsel.c|1419| <<evsel__enable>> int err = perf_evsel__enable(&evsel->core);
+ */
 int perf_evsel__enable(struct perf_evsel *evsel)
 {
 	int i;
 	int err = 0;
 
+	/*
+	 * PERF_EVENT_IOC_ENABLE主要被perf相关的调用:
+	 *   - include/uapi/linux/perf_event.h|551| <<global>> #define PERF_EVENT_IOC_ENABLE _IO ('$', 0)
+	 *   - tools/include/uapi/linux/perf_event.h|551| <<global>> #define PERF_EVENT_IOC_ENABLE _IO ('$', 0)
+	 *   - kernel/events/core.c|5880| <<_perf_ioctl>> case PERF_EVENT_IOC_ENABLE:
+	 *   - tools/lib/perf/evsel.c|444| <<perf_evsel__enable_cpu>> return perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, cpu_map_idx);
+	 *   - tools/lib/perf/evsel.c|454| <<perf_evsel__enable_thread>> err = perf_evsel__ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, idx, thread);
+	 *   - tools/lib/perf/evsel.c|468| <<perf_evsel__enable>> err = perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, i);
+	 */
 	for (i = 0; i < xyarray__max_x(evsel->fd) && !err; i++)
 		err = perf_evsel__run_ioctl(evsel, PERF_EVENT_IOC_ENABLE, NULL, i);
 	return err;
@@ -537,6 +668,12 @@ void perf_evsel__free_id(struct perf_evsel *evsel)
 	evsel->ids = 0;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/tests/test-evlist.c|533| <<test_stat_multiplexing>> perf_counts_values__scale(&counts[i], true, &scaled);
+ *   - tools/perf/util/evsel.c|1636| <<__evsel__read_on_cpu>> perf_counts_values__scale(&count, scale, NULL);
+ *   - tools/perf/util/stat.c|404| <<process_counter_values>> perf_counts_values__scale(count, config->scale, NULL);
+ */
 void perf_counts_values__scale(struct perf_counts_values *count,
 			       bool scale, __s8 *pscaled)
 {
diff --git a/tools/lib/perf/include/internal/xyarray.h b/tools/lib/perf/include/internal/xyarray.h
index f10af3da7..cfe9187ac 100644
--- a/tools/lib/perf/include/internal/xyarray.h
+++ b/tools/lib/perf/include/internal/xyarray.h
@@ -18,6 +18,10 @@ struct xyarray *xyarray__new(int xlen, int ylen, size_t entry_size);
 void xyarray__delete(struct xyarray *xy);
 void xyarray__reset(struct xyarray *xy);
 
+/*
+ * called by:
+ *   - tools/lib/perf/include/internal/xyarray.h|30| <<xyarray__entry>> return __xyarray__entry(xy, x, y);
+ */
 static inline void *__xyarray__entry(struct xyarray *xy, int x, int y)
 {
 	return &xy->contents[x * xy->row_size + y * xy->entry_size];
diff --git a/tools/lib/perf/mmap.c b/tools/lib/perf/mmap.c
index 0c903c237..8d7775338 100644
--- a/tools/lib/perf/mmap.c
+++ b/tools/lib/perf/mmap.c
@@ -477,6 +477,10 @@ static u64 read_perf_counter(unsigned int counter __maybe_unused) { return 0; }
 static u64 read_timestamp(void) { return 0; }
 #endif
 
+/*
+ * called by:
+ *   - tools/lib/perf/include/internal/mmap.h|58| <<perf_evsel__read>> int perf_mmap__read_self(struct perf_mmap *map, struct perf_counts_values *count);
+ */
 int perf_mmap__read_self(struct perf_mmap *map, struct perf_counts_values *count)
 {
 	struct perf_event_mmap_page *pc = map->base;
diff --git a/tools/perf/builtin-stat.c b/tools/perf/builtin-stat.c
index 5fe9abc6a..b0dc16460 100644
--- a/tools/perf/builtin-stat.c
+++ b/tools/perf/builtin-stat.c
@@ -275,6 +275,9 @@ static int evsel__write_stat_event(struct evsel *counter, int cpu_map_idx, u32 t
 					   process_synthesized_event, NULL);
 }
 
+/*
+ * tools/perf/builtin-stat.c|334| <<read_counter_cpu>> read_single_counter(counter, cpu_map_idx, thread, rs)) {
+ */
 static int read_single_counter(struct evsel *counter, int cpu_map_idx,
 			       int thread, struct timespec *rs)
 {
@@ -313,6 +316,10 @@ static int read_single_counter(struct evsel *counter, int cpu_map_idx,
  * Read out the results of a single counter:
  * do not aggregate counts across CPUs in system-wide mode
  */
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|389| <<read_affinity_counters>> counter->err = read_counter_cpu(counter, rs,
+ */
 static int read_counter_cpu(struct evsel *counter, struct timespec *rs, int cpu_map_idx)
 {
 	int nthreads = perf_thread_map__nr(evsel_list->core.threads);
@@ -360,6 +367,10 @@ static int read_counter_cpu(struct evsel *counter, struct timespec *rs, int cpu_
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|420| <<read_counters>> read_affinity_counters(rs))
+ */
 static int read_affinity_counters(struct timespec *rs)
 {
 	struct evlist_cpu_iterator evlist_cpu_itr;
@@ -408,6 +419,11 @@ static int read_bpf_map_counters(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|446| <<process_interval>> if (read_counters(&rs) == 0)
+ *   - tools/perf/builtin-stat.c|934| <<__run_perf_stat>> if (read_counters(&(struct timespec) { .tv_nsec = t1-t0 }) == 0)
+ */
 static int read_counters(struct timespec *rs)
 {
 	if (!stat_config.stop_read_counter) {
@@ -418,6 +434,12 @@ static int read_counters(struct timespec *rs)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|468| <<process_interval>> process_counters();
+ *   - tools/perf/builtin-stat.c|960| <<__run_perf_stat>> process_counters();
+ *   - tools/perf/builtin-stat.c|2298| <<process_stat_round_event>> process_counters();
+ */
 static void process_counters(void)
 {
 	struct evsel *counter;
@@ -434,6 +456,11 @@ static void process_counters(void)
 	perf_stat_process_percore(&stat_config, evsel_list);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|471| <<handle_interval>> process_interval();
+ *   - tools/perf/builtin-stat.c|566| <<process_evlist>> process_interval();
+ */
 static void process_interval(void)
 {
 	struct timespec ts, rs;
@@ -686,6 +713,10 @@ static enum counter_recovery stat_handle_error(struct evsel *counter)
 	return COUNTER_FATAL;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|960| <<run_perf_stat>> ret = __run_perf_stat(argc, argv, run_idx);
+ */
 static int __run_perf_stat(int argc, const char **argv, int run_idx)
 {
 	int interval = stat_config.interval;
@@ -699,6 +730,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 	const bool forks = (argc > 0);
 	bool is_pipe = STAT_RECORD ? perf_stat.data.is_pipe : false;
 	struct evlist_cpu_iterator evlist_cpu_itr;
+	/*
+	 * struct affinity {
+	 *     unsigned long *orig_cpus;
+	 *     unsigned long *sched_cpus;
+	 *     bool changed;
+	 * };
+	 */
 	struct affinity saved_affinity, *affinity = NULL;
 	int err;
 	bool second_pass = false;
@@ -727,6 +765,9 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 
 	evlist__reset_aggr_stats(evsel_list);
 
+	/*
+	 * struct evlist_cpu_iterator evlist_cpu_itr;
+	 */
 	evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
 		counter = evlist_cpu_itr.evsel;
 
@@ -944,6 +985,10 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 	return WEXITSTATUS(status);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|2832| <<cmd_stat>> status = run_perf_stat(argc, argv, run_idx);
+ */
 static int run_perf_stat(int argc, const char **argv, int run_idx)
 {
 	int ret;
diff --git a/tools/perf/util/counts.c b/tools/perf/util/counts.c
index 11cd85b27..d3cd01d61 100644
--- a/tools/perf/util/counts.c
+++ b/tools/perf/util/counts.c
@@ -55,6 +55,11 @@ void evsel__reset_counts(struct evsel *evsel)
 	perf_counts__reset(evsel->counts);
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evsel.c|1621| <<__evsel__read_on_cpu>> if (evsel->counts == NULL && evsel__alloc_counts(evsel) < 0)
+ *   - tools/perf/util/stat.c|182| <<evsel__alloc_stats>> if (evsel__alloc_stat_priv(evsel, nr_aggr) < 0 || evsel__alloc_counts(evsel) < 0 ||
+ */
 int evsel__alloc_counts(struct evsel *evsel)
 {
 	struct perf_cpu_map *cpus = evsel__cpus(evsel);
diff --git a/tools/perf/util/counts.h b/tools/perf/util/counts.h
index 42760242e..6ef5ae1a7 100644
--- a/tools/perf/util/counts.h
+++ b/tools/perf/util/counts.h
@@ -16,6 +16,39 @@ struct perf_counts {
 };
 
 
+/*
+ * called by:
+ *   - tools/perf/arch/x86/util/iostat.c|434| <<iostat_print_metric>> struct perf_counts_values *count = perf_counts(evsel->counts, die, 0);
+ *   - tools/perf/arch/x86/util/iostat.c|439| <<iostat_print_metric>> perf_counts(evsel->prev_raw_counts, die, 0);
+ *   - tools/perf/builtin-script.c|2293| <<__process_stat>> counts = perf_counts(counter->counts, idx, thread);
+ *   - tools/perf/builtin-stat.c|285| <<read_single_counter>> perf_counts(counter->counts, cpu_map_idx, thread);
+ *   - tools/perf/builtin-stat.c|294| <<read_single_counter>> perf_counts(counter->counts, cpu_map_idx, thread);
+ *   - tools/perf/builtin-stat.c|327| <<read_counter_cpu>> count = perf_counts(counter->counts, cpu_map_idx, thread);
+ *   - tools/perf/builtin-stat.c|336| <<read_counter_cpu>> perf_counts(counter->counts, cpu_map_idx, thread)->ena = 0;
+ *   - tools/perf/builtin-stat.c|337| <<read_counter_cpu>> perf_counts(counter->counts, cpu_map_idx, thread)->run = 0;
+ *   - tools/perf/tests/openat-syscall-all-cpus.c|108| <<test__openat_syscall_event_on_all_cpus>> if (perf_counts(evsel->counts, idx, 0)->val != expected) {
+ *   - tools/perf/tests/openat-syscall-all-cpus.c|110| <<test__openat_syscall_event_on_all_cpus>> pr_debug("evsel__read_on_cpu: expected to intercept %d calls on cpu %d, got %" PRIu64 "\n",
+ *                                                                                expected, cpu.cpu, perf_counts(evsel->counts, idx, 0)->val);
+ *   - tools/perf/tests/openat-syscall.c|58| <<test__openat_syscall_event>> if (perf_counts(evsel->counts, 0, 0)->val != nr_openat_calls) {
+ *   - tools/perf/tests/openat-syscall.c|60| <<test__openat_syscall_event>> pr_debug("evsel__read_on_cpu: expected to intercept %d calls, got %" PRIu64 "\n",
+ *                                                                 nr_openat_calls, perf_counts(evsel->counts, 0, 0)->val);
+ *   - tools/perf/util/bpf_counter.c|245| <<bpf_program_profiler__read>> counts = perf_counts(evsel->counts, idx, 0);
+ *   - tools/perf/util/bpf_counter.c|267| <<bpf_program_profiler__read>> counts = perf_counts(evsel->counts, idx, 0);
+ *   - tools/perf/util/bpf_counter.c|638| <<bperf__read>> counts = perf_counts(evsel->counts, j, 0);
+ *   - tools/perf/util/bpf_counter.c|647| <<bperf__read>> counts = perf_counts(evsel->counts, i, 0);
+ *   - tools/perf/util/bpf_counter.c|654| <<bperf__read>> counts = perf_counts(evsel->counts, 0, i);
+ *   - tools/perf/util/bpf_counter_cgroup.c|265| <<bperf_cgrp__read>> counts = perf_counts(evsel->counts, i, 0);
+ *   - tools/perf/util/evsel.c|1509| <<evsel__compute_deltas>> tmp = *perf_counts(evsel->prev_raw_counts, cpu_map_idx, thread);
+ *   - tools/perf/util/evsel.c|1510| <<evsel__compute_deltas>> *perf_counts(evsel->prev_raw_counts, cpu_map_idx, thread) = *count;
+ *   - tools/perf/util/evsel.c|1519| <<evsel__read_one>> struct perf_counts_values *count = perf_counts(evsel->counts, cpu_map_idx, thread);
+ *   - tools/perf/util/evsel.c|1529| <<evsel__set_count>> count = perf_counts(counter->counts, cpu_map_idx, thread);
+ *   - tools/perf/util/evsel.c|1629| <<__evsel__read_on_cpu>> *perf_counts(evsel->counts, cpu_map_idx, thread) = count;
+ *   - tools/perf/util/scripting-engines/trace-event-python.c|1702| <<python_process_stat>> perf_counts(counter->counts, cpu, thread));
+ *   - tools/perf/util/stat.c|253| <<evsel__copy_prev_raw_counts>> *perf_counts(evsel->counts, idx, thread) =
+ *   - tools/perf/util/stat.c|254| <<evsel__copy_prev_raw_counts>> *perf_counts(evsel->prev_raw_counts, idx, thread);
+ *   - tools/perf/util/stat.c|475| <<process_counter_maps>> perf_counts(counter->counts, idx, thread)))
+ *   - tools/perf/util/stat.c|699| <<perf_event__process_stat_event>> ptr = perf_counts(counter->counts, cpu_map_idx, st->thread);
+ */
 static inline struct perf_counts_values*
 perf_counts(struct perf_counts *counts, int cpu_map_idx, int thread)
 {
diff --git a/tools/perf/util/evlist.c b/tools/perf/util/evlist.c
index 55a300a09..440038e09 100644
--- a/tools/perf/util/evlist.c
+++ b/tools/perf/util/evlist.c
@@ -397,8 +397,20 @@ int evlist__add_newtp(struct evlist *evlist, const char *sys, const char *name,
 }
 #endif
 
+/*
+ * called by:
+ *   tools/perf/util/evlist.h|377| <<evlist__for_each_cpu>> for ((evlist_cpu_itr) = evlist__cpu_begin(evlist, affinity); \
+ */
 struct evlist_cpu_iterator evlist__cpu_begin(struct evlist *evlist, struct affinity *affinity)
 {
+	/*
+	 * struct evlist:
+	 * -> struct perf_evlist core;
+	 *    -> struct list_head         entries;
+	 *    -> int                      nr_entries;
+	 *    -> struct perf_cpu_map     *all_cpus;
+	 *    -> struct perf_thread_map  *threads;
+	 */
 	struct evlist_cpu_iterator itr = {
 		.container = evlist,
 		.evsel = NULL,
@@ -429,6 +441,12 @@ struct evlist_cpu_iterator evlist__cpu_begin(struct evlist *evlist, struct affin
 	return itr;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evlist.c|426| <<evlist__cpu_begin>> evlist_cpu_iterator__next(&itr);
+ *   - tools/perf/util/evlist.c|458| <<evlist_cpu_iterator__next>> evlist_cpu_iterator__next(evlist_cpu_itr);
+ *   - tools/perf/util/evlist.h|379| <<evlist__for_each_cpu>> evlist_cpu_iterator__next(&evlist_cpu_itr))
+ */
 void evlist_cpu_iterator__next(struct evlist_cpu_iterator *evlist_cpu_itr)
 {
 	while (evlist_cpu_itr->evsel != evlist__last(evlist_cpu_itr->container)) {
@@ -459,6 +477,10 @@ void evlist_cpu_iterator__next(struct evlist_cpu_iterator *evlist_cpu_itr)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evlist.h|378| <<evlist__for_each_cpu>> !evlist_cpu_iterator__end(&evlist_cpu_itr); \
+ */
 bool evlist_cpu_iterator__end(const struct evlist_cpu_iterator *evlist_cpu_itr)
 {
 	return evlist_cpu_itr->evlist_cpu_map_idx >= evlist_cpu_itr->evlist_cpu_map_nr;
diff --git a/tools/perf/util/evlist.h b/tools/perf/util/evlist.h
index cb91dc911..b2c756572 100644
--- a/tools/perf/util/evlist.h
+++ b/tools/perf/util/evlist.h
@@ -373,6 +373,16 @@ struct evlist_cpu_iterator {
  * @evlist: evlist instance to iterate.
  * @affinity: NULL or used to set the affinity to the current CPU.
  */
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|389| <<read_affinity_counters>> evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
+ *   - tools/perf/builtin-stat.c|768| <<__run_perf_stat>> evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
+ *   - tools/perf/builtin-stat.c|824| <<__run_perf_stat>> evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
+ *   - tools/perf/builtin-stat.c|833| <<__run_perf_stat>> evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
+ *   - tools/perf/util/evlist.c|506| <<__evlist__disable>> evlist__for_each_cpu(evlist_cpu_itr, evlist, affinity) {
+ *   - tools/perf/util/evlist.c|573| <<__evlist__enable>> evlist__for_each_cpu(evlist_cpu_itr, evlist, affinity) {
+ *   - tools/perf/util/evlist.c|1335| <<evlist__close>> evlist__for_each_cpu(evlist_cpu_itr, evlist, &affinity) {
+ */
 #define evlist__for_each_cpu(evlist_cpu_itr, evlist, affinity)		\
 	for ((evlist_cpu_itr) = evlist__cpu_begin(evlist, affinity);	\
 	     !evlist_cpu_iterator__end(&evlist_cpu_itr);		\
diff --git a/tools/perf/util/evsel.c b/tools/perf/util/evsel.c
index 6d7c9c58a..3250094ca 100644
--- a/tools/perf/util/evsel.c
+++ b/tools/perf/util/evsel.c
@@ -1514,6 +1514,10 @@ void evsel__compute_deltas(struct evsel *evsel, int cpu_map_idx, int thread,
 	count->run = count->run - tmp.run;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evsel.c|1610| <<evsel__read_counter>> return evsel__read_one(evsel, cpu_map_idx, thread);
+ */
 static int evsel__read_one(struct evsel *evsel, int cpu_map_idx, int thread)
 {
 	struct perf_counts_values *count = perf_counts(evsel->counts, cpu_map_idx, thread);
@@ -1600,6 +1604,10 @@ static int evsel__read_group(struct evsel *leader, int cpu_map_idx, int thread)
 	return evsel__process_group_data(leader, cpu_map_idx, thread, data);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|305| <<read_single_counter>> return evsel__read_counter(counter, cpu_map_idx, thread);
+ */
 int evsel__read_counter(struct evsel *evsel, int cpu_map_idx, int thread)
 {
 	u64 read_format = evsel->core.attr.read_format;
@@ -1997,6 +2005,12 @@ bool evsel__detect_missing_features(struct evsel *evsel)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evsel.c|2163| <<evsel__open>> return evsel__open_cpu(evsel, cpus, threads, 0, perf_cpu_map__nr(cpus));
+ *   - tools/perf/util/evsel.c|2175| <<evsel__open_per_cpu>> return evsel__open_cpu(evsel, cpus, NULL, 0, perf_cpu_map__nr(cpus));
+ *   - tools/perf/util/evsel.c|2177| <<evsel__open_per_cpu>> return evsel__open_cpu(evsel, cpus, NULL, cpu_map_idx, cpu_map_idx + 1);
+ */
 static int evsel__open_cpu(struct evsel *evsel, struct perf_cpu_map *cpus,
 		struct perf_thread_map *threads,
 		int start_cpu_map_idx, int end_cpu_map_idx)
@@ -2161,6 +2175,15 @@ void evsel__close(struct evsel *evsel)
 	perf_evsel__free_id(&evsel->core);
 }
 
+/*
+ * called by:
+ *   - tools/perf/tests/event-times.c|128| <<attach__cpu_disabled>> err = evsel__open_per_cpu(evsel, cpus, -1);
+ *   - tools/perf/tests/event-times.c|155| <<attach__cpu_enabled>> err = evsel__open_per_cpu(evsel, cpus, -1);
+ *   - tools/perf/util/bpf_counter.c|439| <<bperf_reload_leader_program>> evsel__open_per_cpu(evsel, all_cpu_map, -1);
+ *   - tools/perf/util/bpf_counter_cgroup.c|91| <<bperf_load_program>> if (evsel__open_per_cpu(cgrp_switch, evlist->core.all_cpus, -1) < 0) {
+ *   - tools/perf/util/bpf_counter_cgroup.c|118| <<bperf_load_program>> err = evsel__open_per_cpu(evsel, evsel->core.cpus, -1);
+ *   - tools/perf/util/stat.c|811| <<create_perf_stat_counter>> return evsel__open_per_cpu(evsel, evsel__cpus(evsel), cpu_map_idx);
+ */
 int evsel__open_per_cpu(struct evsel *evsel, struct perf_cpu_map *cpus, int cpu_map_idx)
 {
 	if (cpu_map_idx == -1)
diff --git a/tools/perf/util/pmu.c b/tools/perf/util/pmu.c
index 3c9609944..c685269f5 100644
--- a/tools/perf/util/pmu.c
+++ b/tools/perf/util/pmu.c
@@ -686,6 +686,10 @@ static int pmu_alias_terms(struct perf_pmu_alias *alias, struct list_head *terms
  * Uncore PMUs have a "cpumask" file under sysfs. CPU PMUs (e.g. on arm/arm64)
  * may have a "cpus" file.
  */
+/*
+ * called by:
+ *   - tools/perf/util/pmu.c|1027| <<perf_pmu__lookup>> pmu->cpus = pmu_cpumask(dirfd, name, pmu->is_core);
+ */
 static struct perf_cpu_map *pmu_cpumask(int dirfd, const char *name, bool is_core)
 {
 	struct perf_cpu_map *cpus;
@@ -2052,6 +2056,34 @@ int perf_pmu__pathname_scnprintf(char *buf, size_t size,
 	return scnprintf(buf + len, size - len, "%s/%s", pmu_name, filename);
 }
 
+/*
+ * (gdb) bt
+ * #0  perf_pmu__pathname_fd (dirfd=3, pmu_name=0xaaaaaadce8c0 "armv8_pmuv3_0", filename=0xaaaaaacb34c8 "type", flags=0) at util/pmu.c:2056
+ * #1  0x0000aaaaaabb9b2c in perf_pmu__open_file_at (name=0xaaaaaacb34c8 "type", dirfd=3, pmu=0xaaaaaacaae30) at util/pmu.c:1792
+ * #2  perf_pmu__scan_file_at (pmu=pmu@entry=0xaaaaaadce800, dirfd=dirfd@entry=3, name=name@entry=0xaaaaaacb34c8 "type", fmt=fmt@entry=0xaaaaaacaae30 "%u") at util/pmu.c:1824
+ * #3  0x0000aaaaaabb9c10 in perf_pmu__lookup (pmus=0xaaaaaad97938 <core_pmus>, dirfd=dirfd@entry=3, name=name@entry=0xaaaaaadc67d3 "armv8_pmuv3_0") at util/pmu.c:1010
+ * #4  0x0000aaaaaabba4d0 in perf_pmu__find2 (name=0xaaaaaadc67d3 "armv8_pmuv3_0", dirfd=3) at util/pmus.c:161
+ * #5  pmu_read_sysfs (core_only=core_only@entry=false) at util/pmus.c:209
+ * #6  0x0000aaaaaabba73c in pmu_read_sysfs (core_only=false) at util/pmus.c:190
+ * #7  perf_pmus__scan (pmu=pmu@entry=0x0) at util/pmus.c:263
+ * #8  0x0000aaaaaab6e8d8 in parse_events_multi_pmu_add (parse_state=parse_state@entry=0xffffffffa200, event_name=0xaaaaaadc66a0 "mem_access", const_parsed_terms=const_parsed_terms@entry=0x0,
+ *     listp=listp@entry=0xffffffff8770, loc_=loc_@entry=0xffffffff9450) at util/parse-events.c:1576
+ * #9  0x0000aaaaaabb4db4 in parse_events_parse (_parse_state=_parse_state@entry=0xffffffffa200, scanner=0xaaaaaadb6500) at util/parse-events.y:355
+ * #10 0x0000aaaaaab6b6b8 in parse_events__scanner (parse_state=0xffffffffa200, input=0x0, str=0xfffffffff852 "mem_access") at util/parse-events.c:1869
+ * #11 __parse_events (evlist=0xaaaaaadb5a50, str=str@entry=0xfffffffff852 "mem_access", pmu_filter=<optimized out>, err=err@entry=0xffffffffa288, fake_pmu=fake_pmu@entry=0x0,
+ *     warn_if_reordered=warn_if_reordered@entry=true) at util/parse-events.c:2137
+ * #12 0x0000aaaaaab6bf18 in parse_events_option (opt=<optimized out>, str=0xfffffffff852 "mem_access", unset=<optimized out>) at util/parse-events.c:2322
+ * #13 0x0000aaaaaac508c4 in get_value (p=p@entry=0xffffffffa488, opt=0xaaaaaad90318 <stat_options+96>, flags=flags@entry=1) at parse-options.c:251
+ * #14 0x0000aaaaaac51650 in parse_short_opt (options=<optimized out>, p=<optimized out>) at parse-options.c:351
+ * #15 parse_options_step (usagestr=0xffffffffa628, options=0xaaaaaad902b8 <stat_options>, ctx=0xffffffffa488) at parse-options.c:539
+ * #16 parse_options_subcommand (argc=argc@entry=3, argv=argv@entry=0xfffffffff5d0, options=options@entry=0xaaaaaad902b8 <stat_options>, subcommands=0xffffffffa638, subcommands@entry=0xfffffffff378,
+ *     usagestr=usagestr@entry=0xffffffffa628, flags=flags@entry=2) at parse-options.c:654
+ * #17 0x0000aaaaaaae4e08 in cmd_stat (argc=3, argv=0xfffffffff5d0) at builtin-stat.c:2506
+ * #18 0x0000aaaaaab4178c in run_builtin (p=p@entry=0xaaaaaad92140 <commands+336>, argc=argc@entry=3, argv=argv@entry=0xfffffffff5d0) at perf.c:349
+ * #19 0x0000aaaaaaac8738 in handle_internal_command (argv=0xfffffffff5d0, argc=3) at perf.c:402
+ * #20 run_argv (argv=<synthetic pointer>, argcp=<synthetic pointer>) at perf.c:446
+ * #21 main (argc=3, argv=0xfffffffff5d0) at perf.c:562
+ */
 int perf_pmu__pathname_fd(int dirfd, const char *pmu_name, const char *filename, int flags)
 {
 	char path[PATH_MAX];
diff --git a/tools/perf/util/stat-display.c b/tools/perf/util/stat-display.c
index 8c61f8627..e8d3e5ccf 100644
--- a/tools/perf/util/stat-display.c
+++ b/tools/perf/util/stat-display.c
@@ -1403,8 +1403,34 @@ static double timeval2double(struct timeval *t)
 	return t->tv_sec + (double) t->tv_usec/USEC_PER_SEC;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat-display.c|1594| <<evlist__print_counters>> print_footer(config);
+ */
 static void print_footer(struct perf_stat_config *config)
 {
+	/*
+	 * 在以下使用walltime_nsecs_stats:
+	 *   - tools/perf/util/stat-shadow.c|19| <<global>> struct stats walltime_nsecs_stats;
+	 *   - tools/perf/builtin-stat.c|159| <<global>> .walltime_nsecs_stats = &walltime_nsecs_stats,
+	 *   - tools/perf/builtin-stat.c|481| <<process_interval>> init_stats(&walltime_nsecs_stats);
+	 *   - tools/perf/builtin-stat.c|482| <<process_interval>> update_stats(&walltime_nsecs_stats, stat_config.interval * 1000000ULL);
+	 *   - tools/perf/builtin-stat.c|948| <<__run_perf_stat>> init_stats(&walltime_nsecs_stats);
+	 *   - tools/perf/builtin-stat.c|949| <<__run_perf_stat>> update_stats(&walltime_nsecs_stats, t1 - t0);
+	 *   - tools/perf/builtin-stat.c|955| <<__run_perf_stat>> update_stats(&walltime_nsecs_stats, t1 - t0);
+	 *   - tools/perf/builtin-stat.c|2307| <<process_stat_round_event>> update_stats(&walltime_nsecs_stats, stat_round->time);
+	 *   - tools/perf/builtin-stat.c|2906| <<cmd_stat>> if (WRITE_STAT_ROUND_EVENT(walltime_nsecs_stats.max, FINAL))
+	 *   - tools/perf/tests/parse-metric.c|44| <<load_runtime_stat>> update_stats(&walltime_nsecs_stats, count);
+	 *   - tools/perf/tests/pmu-events.c|927| <<test__parsing_callback>> update_stats(&walltime_nsecs_stats, k);
+	 *   - tools/perf/util/stat-shadow.c|75| <<perf_stat__reset_shadow_stats>> memset(&walltime_nsecs_stats, 0, sizeof(walltime_nsecs_stats));
+	 *   - tools/perf/util/stat-shadow.c|349| <<print_nsecs>> double wall_time = avg_stats(&walltime_nsecs_stats);
+	 *   - tools/perf/util/stat-shadow.c|376| <<prepare_metric>> stats = &walltime_nsecs_stats;
+	 * 在以下使用perf_stat_config->walltime_nsecs_stats:
+	 *   - tools/perf/util/stat.h|103| <<aggr_cpu_id>> struct stats *walltime_nsecs_stats;
+	 *   - tools/perf/builtin-stat.c|159| <<global>> .walltime_nsecs_stats = &walltime_nsecs_stats,
+	 *   - tools/perf/util/stat-display.c|1408| <<print_footer>> double avg = avg_stats(config->walltime_nsecs_stats) / NSEC_PER_SEC;
+	 *   - tools/perf/util/stat-display.c|1429| <<print_footer>> double sd = stddev_stats(config->walltime_nsecs_stats) / NSEC_PER_SEC;
+	 */
 	double avg = avg_stats(config->walltime_nsecs_stats) / NSEC_PER_SEC;
 	FILE *output = config->output;
 
diff --git a/tools/perf/util/stat-shadow.c b/tools/perf/util/stat-shadow.c
index e31426167..c37d41515 100644
--- a/tools/perf/util/stat-shadow.c
+++ b/tools/perf/util/stat-shadow.c
@@ -16,6 +16,28 @@
 #include "iostat.h"
 #include "util/hashmap.h"
 
+/*
+ * 在以下使用walltime_nsecs_stats:
+ *   - tools/perf/util/stat-shadow.c|19| <<global>> struct stats walltime_nsecs_stats;
+ *   - tools/perf/builtin-stat.c|159| <<global>> .walltime_nsecs_stats = &walltime_nsecs_stats,
+ *   - tools/perf/builtin-stat.c|481| <<process_interval>> init_stats(&walltime_nsecs_stats);
+ *   - tools/perf/builtin-stat.c|482| <<process_interval>> update_stats(&walltime_nsecs_stats, stat_config.interval * 1000000ULL);
+ *   - tools/perf/builtin-stat.c|948| <<__run_perf_stat>> init_stats(&walltime_nsecs_stats);
+ *   - tools/perf/builtin-stat.c|949| <<__run_perf_stat>> update_stats(&walltime_nsecs_stats, t1 - t0);
+ *   - tools/perf/builtin-stat.c|955| <<__run_perf_stat>> update_stats(&walltime_nsecs_stats, t1 - t0);
+ *   - tools/perf/builtin-stat.c|2307| <<process_stat_round_event>> update_stats(&walltime_nsecs_stats, stat_round->time);
+ *   - tools/perf/builtin-stat.c|2906| <<cmd_stat>> if (WRITE_STAT_ROUND_EVENT(walltime_nsecs_stats.max, FINAL))
+ *   - tools/perf/tests/parse-metric.c|44| <<load_runtime_stat>> update_stats(&walltime_nsecs_stats, count);
+ *   - tools/perf/tests/pmu-events.c|927| <<test__parsing_callback>> update_stats(&walltime_nsecs_stats, k);
+ *   - tools/perf/util/stat-shadow.c|75| <<perf_stat__reset_shadow_stats>> memset(&walltime_nsecs_stats, 0, sizeof(walltime_nsecs_stats));
+ *   - tools/perf/util/stat-shadow.c|349| <<print_nsecs>> double wall_time = avg_stats(&walltime_nsecs_stats);
+ *   - tools/perf/util/stat-shadow.c|376| <<prepare_metric>> stats = &walltime_nsecs_stats;
+ * 在以下使用perf_stat_config->walltime_nsecs_stats:
+ *   - tools/perf/util/stat.h|103| <<aggr_cpu_id>> struct stats *walltime_nsecs_stats;
+ *   - tools/perf/builtin-stat.c|159| <<global>> .walltime_nsecs_stats = &walltime_nsecs_stats,
+ *   - tools/perf/util/stat-display.c|1408| <<print_footer>> double avg = avg_stats(config->walltime_nsecs_stats) / NSEC_PER_SEC;
+ *   - tools/perf/util/stat-display.c|1429| <<print_footer>> double sd = stddev_stats(config->walltime_nsecs_stats) / NSEC_PER_SEC;
+ */
 struct stats walltime_nsecs_stats;
 struct rusage_stats ru_stats;
 
diff --git a/tools/perf/util/stat.c b/tools/perf/util/stat.c
index b0bcf92f0..b8b1fd48c 100644
--- a/tools/perf/util/stat.c
+++ b/tools/perf/util/stat.c
@@ -382,6 +382,10 @@ static bool evsel__count_has_error(struct evsel *evsel,
 	return true;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat.c|474| <<process_counter_maps>> if (process_counter_values(config, counter, idx, thread, perf_counts(counter->counts, idx, thread)))
+ */
 static int
 process_counter_values(struct perf_stat_config *config, struct evsel *evsel,
 		       int cpu_map_idx, int thread,
@@ -458,6 +462,10 @@ process_counter_values(struct perf_stat_config *config, struct evsel *evsel,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat.c|489| <<perf_stat_process_counter>> ret = process_counter_maps(config, counter);
+ */
 static int process_counter_maps(struct perf_stat_config *config,
 				struct evsel *counter)
 {
@@ -476,6 +484,11 @@ static int process_counter_maps(struct perf_stat_config *config,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-script.c|3654| <<process_stat_round_event>> perf_stat_process_counter(&stat_config, counter);
+ *   - tools/perf/builtin-stat.c|444| <<process_counters>> if (counter->err == 0 && perf_stat_process_counter(&stat_config, counter))
+ */
 int perf_stat_process_counter(struct perf_stat_config *config,
 			      struct evsel *counter)
 {
@@ -742,6 +755,11 @@ size_t perf_event__fprintf_stat_config(union perf_event *event, FILE *fp)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|776| <<__run_perf_stat>> if (create_perf_stat_counter(counter, &stat_config, &target,
+  2 tools/perf/builtin-stat.c|833| <<__run_perf_stat>> if (create_perf_stat_counter(counter, &stat_config, &target,
+ */
 int create_perf_stat_counter(struct evsel *evsel,
 			     struct perf_stat_config *config,
 			     struct target *target,
diff --git a/tools/perf/util/time-utils.h b/tools/perf/util/time-utils.h
index 1142b0bdd..fc1229dec 100644
--- a/tools/perf/util/time-utils.h
+++ b/tools/perf/util/time-utils.h
@@ -40,6 +40,13 @@ int timestamp__scnprintf_nsec(u64 timestamp, char *buf, size_t sz);
 
 int fetch_current_timestamp(char *buf, size_t sz);
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|914| <<__run_perf_stat>> t0 = rdclock();
+ *   - tools/perf/builtin-stat.c|940| <<__run_perf_stat>> t1 = rdclock();
+ *   - tools/perf/util/synthetic-events.c|454| <<perf_event__synthesize_mmap_events>> t = rdclock();
+ *   - tools/perf/util/synthetic-events.c|477| <<perf_event__synthesize_mmap_events>> if ((rdclock() - t) > timeout) {
+ */
 static inline unsigned long long rdclock(void)
 {
 	struct timespec ts;
diff --git a/tools/testing/selftests/kvm/dirty_log_test.c b/tools/testing/selftests/kvm/dirty_log_test.c
index eaad5b208..636b17d7c 100644
--- a/tools/testing/selftests/kvm/dirty_log_test.c
+++ b/tools/testing/selftests/kvm/dirty_log_test.c
@@ -24,9 +24,25 @@
 #include "guest_modes.h"
 #include "processor.h"
 
+/*
+ * 在以下使用DIRTY_MEM_BITS:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|372| <<dirty_ring_create_vm_done>> pages = (1ul << (DIRTY_MEM_BITS - vm->page_shift)) + 3;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|863| <<run_test>> 2ul << (DIRTY_MEM_BITS - PAGE_SHIFT_4K), guest_code);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|870| <<run_test>> guest_num_pages = (1ul << (DIRTY_MEM_BITS - vm->page_shift)) + 3;
+ */
 #define DIRTY_MEM_BITS 30 /* 1G */
+/*
+ * 在以下使用PAGE_SHIFT_4K:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|863| <<run_test>> 2ul << (DIRTY_MEM_BITS - PAGE_SHIFT_4K), guest_code);
+ */
 #define PAGE_SHIFT_4K  12
 
+/*
+ * 在以下使用TEST_MEM_SLOT_INDEX:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|917| <<run_test>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, guest_test_phys_mem,
+ *                                 TEST_MEM_SLOT_INDEX, guest_num_pages, KVM_MEM_LOG_DIRTY_PAGES);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|971| <<run_test>> log_mode_collect_dirty_pages(vcpu, TEST_MEM_SLOT_INDEX, bmap, host_num_pages, &ring_buf_idx);
+ */
 /* The memory slot index to track dirty pages */
 #define TEST_MEM_SLOT_INDEX		1
 
@@ -77,6 +93,28 @@ static uint64_t host_page_size;
 static uint64_t guest_page_size;
 static uint64_t guest_num_pages;
 static uint64_t random_array[TEST_PAGES_PER_LOOP];
+/*
+ * 在以下修改iteration:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|825| <<run_test>> iteration = 1;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|826| <<run_test>> sync_global_to_guest(vm, iteration);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|876| <<run_test>> if (++iteration == p->iterations)
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|880| <<run_test>> sync_global_to_guest(vm, iteration);
+ * 在以下使用iteration:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|123| <<guest_code>> *(uint64_t *)addr = READ_ONCE(iteration);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|132| <<guest_code>> *(uint64_t *)addr = READ_ONCE(iteration);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|421| <<dirty_ring_collect_dirty_pages>> pr_info("Iteration %ld collected %u pages\n", iteration, count);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|634| <<vm_dirty_log_verify>> matched = (*value_ptr == iteration ||  *value_ptr == iteration - 1);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|635| <<vm_dirty_log_verify>> matched = (*value_ptr == iteration ||  *value_ptr == iteration - 1);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|638| <<vm_dirty_log_verify>> if (*value_ptr == iteration - 2 && min_iter <= iteration - 2) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|671| <<vm_dirty_log_verify>> min_iter = iteration - 1;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|685| <<vm_dirty_log_verify>> TEST_ASSERT(matched, "Set page %"PRIu64" value %"PRIu64 " incorrect (iteration=%"PRIu64")", page, *value_ptr, iteration);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|707| <<vm_dirty_log_verify>> TEST_ASSERT(*value_ptr <= iteration, "Clear page %"PRIu64" value %"PRIu64
+ *                                                             " incorrect (iteration=%"PRIu64")", page, *value_ptr, iteration);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|710| <<vm_dirty_log_verify>> TEST_ASSERT(*value_ptr <= iteration, "Clear page %"PRIu64" value %"PRIu64
+ *                                                             " incorrect (iteration=%"PRIu64")", page, *value_ptr, iteration);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|711| <<vm_dirty_log_verify>> if (*value_ptr == iteration) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|845| <<run_test>> while (iteration < p->iterations) {
+ */
 static uint64_t iteration;
 
 /*
@@ -84,12 +122,31 @@ static uint64_t iteration;
  * This will be set to the topmost valid physical address minus
  * the test memory size.
  */
+/*
+ * 在以下使用guest_test_phys_mem:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|740| <<run_test>> guest_test_phys_mem = (vm->max_gfn - guest_num_pages) * guest_page_size;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|742| <<run_test>> guest_test_phys_mem = align_down(guest_test_phys_mem, host_page_size);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|744| <<run_test>> guest_test_phys_mem = p->phys_offset;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|749| <<run_test>> guest_test_phys_mem = align_down(guest_test_phys_mem, 1 << 20);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|752| <<run_test>> pr_info("guest physical test memory offset: 0x%lx\n", guest_test_phys_mem);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|759| <<run_test>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS,
+ *                                                                     guest_test_phys_mem, TEST_MEM_SLOT_INDEX, guest_num_pages, KVM_MEM_LOG_DIRTY_PAGES);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|765| <<run_test>> virt_map(vm, guest_test_virt_mem, guest_test_phys_mem, guest_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|768| <<run_test>> host_test_mem = addr_gpa2hva(vm, (vm_paddr_t)guest_test_phys_mem);
+ */
 static uint64_t guest_test_phys_mem;
 
 /*
  * Guest virtual memory offset of the testing memory slot.
  * Must not conflict with identity mapped test code.
  */
+/*
+ * 在以下使用guest_test_virt_mem:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|122| <<guest_code>> addr = guest_test_virt_mem + i * guest_page_size;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|128| <<guest_code>> addr = guest_test_virt_mem;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|813| <<run_test>> virt_map(vm, guest_test_virt_mem, guest_test_phys_mem, guest_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|821| <<run_test>> sync_global_to_guest(vm, guest_test_virt_mem);
+ */
 static uint64_t guest_test_virt_mem = DEFAULT_GUEST_TEST_MEM;
 
 /*
@@ -107,6 +164,11 @@ static void guest_code(void)
 	 * To compensate this specialty in this test, we need to touch all
 	 * pages during the first iteration.
 	 */
+	/*
+	 * 在每个page的前8个byte写字
+	 *
+	 * 因为是第一次启动执行, 所以特别慢
+	 */
 	for (i = 0; i < guest_num_pages; i++) {
 		addr = guest_test_virt_mem + i * guest_page_size;
 		*(uint64_t *)addr = READ_ONCE(iteration);
@@ -138,8 +200,26 @@ static uint64_t host_dirty_count;
 static uint64_t host_clear_count;
 static uint64_t host_track_next_count;
 
+/*
+ * 在以下使用sem_vcpu_stop:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|252| <<vcpu_handle_sync_stop>> sem_post(&sem_vcpu_stop);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|345| <<dirty_ring_wait_vcpu>> sem_wait_until(&sem_vcpu_stop);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|408| <<dirty_ring_after_vcpu_run>> sem_post(&sem_vcpu_stop);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|790| <<run_test>> sem_getvalue(&sem_vcpu_stop, &sem_val);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|809| <<run_test>> sem_wait_until(&sem_vcpu_stop);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|878| <<main>> sem_init(&sem_vcpu_stop, 0, 0);
+ */
 /* Whether dirty ring reset is requested, or finished */
 static sem_t sem_vcpu_stop;
+/*
+ * 在以下使用sem_vcpu_cont:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|253| <<vcpu_handle_sync_stop>> sem_wait_until(&sem_vcpu_cont);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|351| <<dirty_ring_continue_vcpu>> sem_post(&sem_vcpu_cont);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|412| <<dirty_ring_after_vcpu_run>> sem_wait_until(&sem_vcpu_cont);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|792| <<run_test>> sem_getvalue(&sem_vcpu_cont, &sem_val);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|831| <<run_test>> sem_post(&sem_vcpu_cont);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|879| <<main>> sem_init(&sem_vcpu_cont, 0, 0);
+ */
 static sem_t sem_vcpu_cont;
 /*
  * This is only set by main thread, and only cleared by vcpu thread.  It is
@@ -148,12 +228,27 @@ static sem_t sem_vcpu_cont;
  * will match.  E.g., SIG_IPI won't guarantee that if the vcpu is interrupted
  * after setting dirty bit but before the data is written.
  */
+/*
+ * 在以下使用vcpu_sync_stop_requested:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|308| <<vcpu_handle_sync_stop>> if (atomic_read(&vcpu_sync_stop_requested)) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|310| <<vcpu_handle_sync_stop>> atomic_set(&vcpu_sync_stop_requested, false);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|931| <<run_test>> atomic_set(&vcpu_sync_stop_requested, true);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|941| <<run_test>> atomic_read(&vcpu_sync_stop_requested) == false);
+ */
 static atomic_t vcpu_sync_stop_requested;
 /*
  * This is updated by the vcpu thread to tell the host whether it's a
  * ring-full event.  It should only be read until a sem_wait() of
  * sem_vcpu_stop and before vcpu continues to run.
  */
+/*
+ * 在以下使用dirty_ring_vcpu_ring_full:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|440| <<dirty_ring_collect_dirty_pages>> if (!dirty_ring_vcpu_ring_full) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|464| <<dirty_ring_collect_dirty_pages>> TEST_ASSERT(dirty_ring_vcpu_ring_full, "Didn't continue vcpu even without ring full");
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|483| <<dirty_ring_after_vcpu_run>> WRITE_ONCE(dirty_ring_vcpu_ring_full, run->exit_reason == KVM_EXIT_DIRTY_RING_FULL);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|487| <<dirty_ring_after_vcpu_run>> pr_info("vcpu stops because %s...\n", dirty_ring_vcpu_ring_full ? "dirty ring is full" : "vcpu is kicked out");
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|929| <<run_test>> WRITE_ONCE(dirty_ring_vcpu_ring_full, false);
+ */
 static bool dirty_ring_vcpu_ring_full;
 /*
  * This is only used for verifying the dirty pages.  Dirty ring has a very
@@ -243,9 +338,20 @@ static void clear_log_collect_dirty_pages(struct kvm_vcpu *vcpu, int slot,
 	kvm_vm_clear_dirty_log(vcpu->vm, slot, bitmap, 0, num_pages);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|327| <<default_after_vcpu_run>> vcpu_handle_sync_stop();
+ */
 /* Should only be called after a GUEST_SYNC */
 static void vcpu_handle_sync_stop(void)
 {
+	/*
+	 * 在以下使用vcpu_sync_stop_requested:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|308| <<vcpu_handle_sync_stop>> if (atomic_read(&vcpu_sync_stop_requested)) {
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|310| <<vcpu_handle_sync_stop>> atomic_set(&vcpu_sync_stop_requested, false);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|931| <<run_test>> atomic_set(&vcpu_sync_stop_requested, true);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|941| <<run_test>> atomic_read(&vcpu_sync_stop_requested) == false);
+	 */
 	if (atomic_read(&vcpu_sync_stop_requested)) {
 		/* It means main thread is sleeping waiting */
 		atomic_set(&vcpu_sync_stop_requested, false);
@@ -312,6 +418,10 @@ static inline void dirty_gfn_set_collected(struct kvm_dirty_gfn *gfn)
 	smp_store_release(&gfn->flags, KVM_DIRTY_GFN_F_RESET);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|471| <<dirty_ring_collect_dirty_pages>> count = dirty_ring_collect_one(vcpu_map_dirty_ring(vcpu), slot, bitmap, num_pages, ring_buf_idx);
+ */
 static uint32_t dirty_ring_collect_one(struct kvm_dirty_gfn *dirty_gfns,
 				       int slot, void *bitmap,
 				       uint32_t num_pages, uint32_t *fetch_index)
@@ -360,6 +470,19 @@ static void dirty_ring_collect_dirty_pages(struct kvm_vcpu *vcpu, int slot,
 
 	dirty_ring_wait_vcpu();
 
+	/*
+	 * 在以下使用dirty_ring_vcpu_ring_full:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|440| <<dirty_ring_collect_dirty_pages>> if (!dirty_ring_vcpu_ring_full) {
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|464| <<dirty_ring_collect_dirty_pages>> TEST_ASSERT(dirty_ring_vcpu_ring_full, "Didn't continue vcpu even without ring full");
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|483| <<dirty_ring_after_vcpu_run>> WRITE_ONCE(dirty_ring_vcpu_ring_full, run->exit_reason == KVM_EXIT_DIRTY_RING_FULL);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|487| <<dirty_ring_after_vcpu_run>> pr_info("vcpu stops because %s...\n", dirty_ring_vcpu_ring_full ? "dirty ring is full" : "vcpu is kicked out");
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|929| <<run_test>> WRITE_ONCE(dirty_ring_vcpu_ring_full, false);
+	 *
+	 * 注释:
+	 * This is updated by the vcpu thread to tell the host whether it's a
+	 * ring-full event.  It should only be read until a sem_wait() of
+	 * sem_vcpu_stop and before vcpu continues to run.
+	 */
 	if (!dirty_ring_vcpu_ring_full) {
 		/*
 		 * This is not a ring-full event, it's safe to allow
@@ -459,6 +582,13 @@ struct log_mode {
  * page bit is cleared in the latest bitmap, then the system must
  * report that write in the next get dirty log call.
  */
+/*
+ * 在以下使用host_bmap_track:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|575| <<vm_dirty_log_verify>> if (__test_and_clear_bit_le(page, host_bmap_track)) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|676| <<vm_dirty_log_verify>> __set_bit_le(page, host_bmap_track);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|755| <<run_test>> host_bmap_track = bitmap_zalloc(host_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|842| <<run_test>> free(host_bmap_track);
+ */
 static unsigned long *host_bmap_track;
 
 static void log_modes_dump(void)
@@ -475,6 +605,11 @@ static bool log_mode_supported(void)
 {
 	struct log_mode *mode = &log_modes[host_log_mode];
 
+	/*
+	 * .name = "dirty-log",
+	 * .name = "clear-log", .supported = clear_log_supported,
+	 * .name = "dirty-ring", .supported = dirty_ring_supported,
+	 */
 	if (mode->supported)
 		return mode->supported();
 
@@ -485,10 +620,19 @@ static void log_mode_create_vm_done(struct kvm_vm *vm)
 {
 	struct log_mode *mode = &log_modes[host_log_mode];
 
+	/*
+	 * .name = "dirty-log",
+	 * .name = "clear-log", .create_vm_done = clear_log_create_vm_done,
+	 * .name = "dirty-ring", .create_vm_done = dirty_ring_create_vm_done,
+	 */
 	if (mode->create_vm_done)
 		mode->create_vm_done(vm);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|894| <<run_test>> log_mode_collect_dirty_pages(vcpu, TEST_MEM_SLOT_INDEX, bmap, host_num_pages, &ring_buf_idx);
+ */
 static void log_mode_collect_dirty_pages(struct kvm_vcpu *vcpu, int slot,
 					 void *bitmap, uint32_t num_pages,
 					 uint32_t *ring_buf_idx)
@@ -504,6 +648,11 @@ static void log_mode_after_vcpu_run(struct kvm_vcpu *vcpu, int ret, int err)
 {
 	struct log_mode *mode = &log_modes[host_log_mode];
 
+	/*
+	 * .name = "dirty-log", .after_vcpu_run = default_after_vcpu_run,
+	 * .name = "clear-log", .after_vcpu_run = default_after_vcpu_run,
+	 * .name = "dirty-ring", .after_vcpu_run = dirty_ring_after_vcpu_run,
+	 */
 	if (mode->after_vcpu_run)
 		mode->after_vcpu_run(vcpu, ret, err);
 }
@@ -553,6 +702,11 @@ static void *vcpu_worker(void *data)
 			sigwait(sigset, &sig);
 			assert(sig == SIG_IPI);
 		}
+		/*
+		 * .name = "dirty-log", .after_vcpu_run = default_after_vcpu_run,
+		 * .name = "clear-log", .after_vcpu_run = default_after_vcpu_run,
+		 * .name = "dirty-ring", .after_vcpu_run = dirty_ring_after_vcpu_run,
+		 */
 		log_mode_after_vcpu_run(vcpu, ret, errno);
 	}
 
@@ -561,6 +715,10 @@ static void *vcpu_worker(void *data)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|913| <<run_test>> vm_dirty_log_verify(mode, bmap);
+ */
 static void vm_dirty_log_verify(enum vm_guest_mode mode, unsigned long *bmap)
 {
 	uint64_t step = vm_num_host_pages(mode, 1);
@@ -752,8 +910,28 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 	pr_info("guest physical test memory offset: 0x%lx\n", guest_test_phys_mem);
 
 	bmap = bitmap_zalloc(host_num_pages);
+	/*
+	 * 在以下使用host_bmap_track:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|575| <<vm_dirty_log_verify>> if (__test_and_clear_bit_le(page, host_bmap_track)) {
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|676| <<vm_dirty_log_verify>> __set_bit_le(page, host_bmap_track);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|755| <<run_test>> host_bmap_track = bitmap_zalloc(host_num_pages);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|842| <<run_test>> free(host_bmap_track);
+	 */
 	host_bmap_track = bitmap_zalloc(host_num_pages);
 
+	/*
+	 * 在以下使用guest_test_phys_mem:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|740| <<run_test>> guest_test_phys_mem = (vm->max_gfn - guest_num_pages) * guest_page_size;
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|742| <<run_test>> guest_test_phys_mem = align_down(guest_test_phys_mem, host_page_size);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|744| <<run_test>> guest_test_phys_mem = p->phys_offset;
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|749| <<run_test>> guest_test_phys_mem = align_down(guest_test_phys_mem, 1 << 20);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|752| <<run_test>> pr_info("guest physical test memory offset: 0x%lx\n", guest_test_phys_mem);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|759| <<run_test>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS,
+	 *                                                                     guest_test_phys_mem, TEST_MEM_SLOT_INDEX, guest_num_pages, KVM_MEM_LOG_DIRTY_PAGES);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|765| <<run_test>> virt_map(vm, guest_test_virt_mem, guest_test_phys_mem, guest_num_pages);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|768| <<run_test>> host_test_mem = addr_gpa2hva(vm, (vm_paddr_t)guest_test_phys_mem);
+	 */
+
 	/* Add an extra memory slot for testing dirty logging */
 	vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS,
 				    guest_test_phys_mem,
@@ -761,6 +939,13 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 				    guest_num_pages,
 				    KVM_MEM_LOG_DIRTY_PAGES);
 
+	/*
+	 * 在以下使用guest_test_virt_mem:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|122| <<guest_code>> addr = guest_test_virt_mem + i * guest_page_size;
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|128| <<guest_code>> addr = guest_test_virt_mem;
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|813| <<run_test>> virt_map(vm, guest_test_virt_mem, guest_test_phys_mem, guest_num_pages);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|821| <<run_test>> sync_global_to_guest(vm, guest_test_virt_mem);
+	 */
 	/* Do mapping for the dirty track memory slot */
 	virt_map(vm, guest_test_virt_mem, guest_test_phys_mem, guest_num_pages);
 
@@ -773,6 +958,13 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 	sync_global_to_guest(vm, guest_test_virt_mem);
 	sync_global_to_guest(vm, guest_num_pages);
 
+	/*
+	 * 在以下修改iteration:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|825| <<run_test>> iteration = 1;
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|826| <<run_test>> sync_global_to_guest(vm, iteration);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|876| <<run_test>> if (++iteration == p->iterations)
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|880| <<run_test>> sync_global_to_guest(vm, iteration);
+	 */
 	/* Start the iterations */
 	iteration = 1;
 	sync_global_to_guest(vm, iteration);
@@ -805,6 +997,13 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 		 * See vcpu_sync_stop_requested definition for details on why
 		 * we need to stop vcpu when verify data.
 		 */
+		/*
+		 * 在以下使用vcpu_sync_stop_requested:
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|308| <<vcpu_handle_sync_stop>> if (atomic_read(&vcpu_sync_stop_requested)) {
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|310| <<vcpu_handle_sync_stop>> atomic_set(&vcpu_sync_stop_requested, false);
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|931| <<run_test>> atomic_set(&vcpu_sync_stop_requested, true);
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|941| <<run_test>> atomic_read(&vcpu_sync_stop_requested) == false);
+		 */
 		atomic_set(&vcpu_sync_stop_requested, true);
 		sem_wait_until(&sem_vcpu_stop);
 		/*
@@ -875,7 +1074,25 @@ int main(int argc, char *argv[])
 	int opt, i;
 	sigset_t sigset;
 
+	/*
+	 * 在以下使用sem_vcpu_stop:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|252| <<vcpu_handle_sync_stop>> sem_post(&sem_vcpu_stop);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|345| <<dirty_ring_wait_vcpu>> sem_wait_until(&sem_vcpu_stop);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|408| <<dirty_ring_after_vcpu_run>> sem_post(&sem_vcpu_stop);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|790| <<run_test>> sem_getvalue(&sem_vcpu_stop, &sem_val);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|809| <<run_test>> sem_wait_until(&sem_vcpu_stop);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|878| <<main>> sem_init(&sem_vcpu_stop, 0, 0);
+	 */
 	sem_init(&sem_vcpu_stop, 0, 0);
+	/*
+	 * 在以下使用sem_vcpu_cont:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|253| <<vcpu_handle_sync_stop>> sem_wait_until(&sem_vcpu_cont);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|351| <<dirty_ring_continue_vcpu>> sem_post(&sem_vcpu_cont);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|412| <<dirty_ring_after_vcpu_run>> sem_wait_until(&sem_vcpu_cont);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|792| <<run_test>> sem_getvalue(&sem_vcpu_cont, &sem_val);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|831| <<run_test>> sem_post(&sem_vcpu_cont);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|879| <<main>> sem_init(&sem_vcpu_cont, 0, 0);
+	 */
 	sem_init(&sem_vcpu_cont, 0, 0);
 
 	guest_modes_append_default();
diff --git a/tools/testing/selftests/kvm/include/kvm_util_base.h b/tools/testing/selftests/kvm/include/kvm_util_base.h
index 9e5afc472..8528acd00 100644
--- a/tools/testing/selftests/kvm/include/kvm_util_base.h
+++ b/tools/testing/selftests/kvm/include/kvm_util_base.h
@@ -440,6 +440,14 @@ int kvm_memfd_alloc(size_t size, bool hugepages);
 
 void vm_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent);
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|461| <<check_write_in_dirty_log>> kvm_vm_get_dirty_log(vm, region->region.slot, bmap);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|235| <<dirty_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|242| <<clear_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|358| <<memstress_get_dirty_log>> kvm_vm_get_dirty_log(vm, slot, bitmaps[i]);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_dirty_log_test.c|129| <<main>> kvm_vm_get_dirty_log(vm, TEST_MEM_SLOT_INDEX, bmap);
+ */
 static inline void kvm_vm_get_dirty_log(struct kvm_vm *vm, int slot, void *log)
 {
 	struct kvm_dirty_log args = { .dirty_bitmap = log, .slot = slot };
@@ -460,6 +468,10 @@ static inline void kvm_vm_clear_dirty_log(struct kvm_vm *vm, int slot, void *log
 	vm_ioctl(vm, KVM_CLEAR_DIRTY_LOG, &args);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|475| <<dirty_ring_collect_dirty_pages>> cleared = kvm_vm_reset_dirty_ring(vcpu->vm);
+ */
 static inline uint32_t kvm_vm_reset_dirty_ring(struct kvm_vm *vm)
 {
 	return __vm_ioctl(vm, KVM_RESET_DIRTY_RINGS, NULL);
@@ -536,6 +548,16 @@ static inline int __vm_create_guest_memfd(struct kvm_vm *vm, uint64_t size,
 	return __vm_ioctl(vm, KVM_CREATE_GUEST_MEMFD, &guest_memfd);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/guest_memfd_test.c|189| <<main>> fd = vm_create_guest_memfd(vm, total_size, 0);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1031| <<vm_mem_add>> guest_memfd = vm_create_guest_memfd(vm, mem_size, guest_memfd_flags);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|370| <<test_invalid_memory_region_flags>> int guest_memfd = vm_create_guest_memfd(vm, MEM_REGION_SIZE, 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|475| <<test_add_private_memory_region>> memfd = vm_create_guest_memfd(vm2, MEM_REGION_SIZE, 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|483| <<test_add_private_memory_region>> memfd = vm_create_guest_memfd(vm, MEM_REGION_SIZE, 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|504| <<test_add_overlapping_private_memory_regions>> memfd = vm_create_guest_memfd(vm, MEM_REGION_SIZE * 4, 0);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|398| <<test_mem_conversions>> memfd = vm_create_guest_memfd(vm, memfd_size, 0);
+ */
 static inline int vm_create_guest_memfd(struct kvm_vm *vm, uint64_t size,
 					uint64_t flags)
 {
diff --git a/tools/testing/selftests/kvm/include/x86_64/processor.h b/tools/testing/selftests/kvm/include/x86_64/processor.h
index 5bca8c947..7b7b25b7a 100644
--- a/tools/testing/selftests/kvm/include/x86_64/processor.h
+++ b/tools/testing/selftests/kvm/include/x86_64/processor.h
@@ -1201,6 +1201,11 @@ static inline uint64_t __kvm_hypercall_map_gpa_range(uint64_t gpa,
 	return kvm_hypercall(KVM_HC_MAP_GPA_RANGE, gpa, size >> PAGE_SHIFT, flags, 0);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|99| <<guest_map_mem>> kvm_hypercall_map_gpa_range(gpa, size, flags);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|223| <<guest_punch_hole>> kvm_hypercall_map_gpa_range(gpa, size, flags);
+ */
 static inline void kvm_hypercall_map_gpa_range(uint64_t gpa, uint64_t size,
 					       uint64_t flags)
 {
diff --git a/tools/testing/selftests/kvm/lib/guest_modes.c b/tools/testing/selftests/kvm/lib/guest_modes.c
index b04901e55..b86222cd9 100644
--- a/tools/testing/selftests/kvm/lib/guest_modes.c
+++ b/tools/testing/selftests/kvm/lib/guest_modes.c
@@ -97,6 +97,17 @@ void for_each_guest_mode(void (*func)(enum vm_guest_mode, void *), void *arg)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|748| <<help>> guest_modes_help();
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|338| <<help>> guest_modes_help();
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|212| <<help>> guest_modes_help();
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|315| <<help>> guest_modes_help();
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|864| <<help>> guest_modes_help();
+ *   - tools/testing/selftests/kvm/include/guest_modes.h|20| <<help>> void guest_modes_help(void );
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|430| <<help>> guest_modes_help();
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|121| <<help>> guest_modes_help();
+ */
 void guest_modes_help(void)
 {
 	int i;
diff --git a/tools/testing/selftests/kvm/lib/kvm_util.c b/tools/testing/selftests/kvm/lib/kvm_util.c
index 1b197426f..d3f7b70a8 100644
--- a/tools/testing/selftests/kvm/lib/kvm_util.c
+++ b/tools/testing/selftests/kvm/lib/kvm_util.c
@@ -126,6 +126,10 @@ unsigned int kvm_check_cap(long cap)
 	return (unsigned int)ret;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|378| <<dirty_ring_create_vm_done>> vm_enable_dirty_ring(vm, test_dirty_ring_count * sizeof(struct kvm_dirty_gfn));
+ */
 void vm_enable_dirty_ring(struct kvm_vm *vm, uint32_t ring_size)
 {
 	if (vm_check_cap(vm, KVM_CAP_DIRTY_LOG_RING_ACQ_REL))
diff --git a/tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c b/tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c
index 65ad38b6b..e1d7b53d9 100644
--- a/tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c
+++ b/tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c
@@ -24,9 +24,19 @@
 #include <processor.h>
 
 #define BASE_DATA_SLOT		10
+/*
+ * 4G: 4294967296
+ */
 #define BASE_DATA_GPA		((uint64_t)(1ull << 32))
+/*
+ * 0x00200000 = 2MB
+ * 2MB + 4KB
+ */
 #define PER_CPU_DATA_SIZE	((uint64_t)(SZ_2M + PAGE_SIZE))
 
+/*
+ * size的每一个是uint8_t
+ */
 /* Horrific macro so that the line info is captured accurately :-( */
 #define memcmp_g(gpa, pattern,  size)								\
 do {												\
@@ -71,12 +81,23 @@ enum ucall_syncs {
 	SYNC_PRIVATE,
 };
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|133| <<guest_test_explicit_conversion>> guest_sync_shared(base_gpa, PER_CPU_DATA_SIZE, def_p, init_p);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|187| <<guest_test_explicit_conversion>> guest_sync_shared(gpa + j, PAGE_SIZE, p1, p3);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|203| <<guest_test_explicit_conversion>> guest_sync_shared(gpa, size, p3, p4);
+ */
 static void guest_sync_shared(uint64_t gpa, uint64_t size,
 			      uint8_t current_pattern, uint8_t new_pattern)
 {
 	GUEST_SYNC5(SYNC_SHARED, gpa, size, current_pattern, new_pattern);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|167| <<guest_test_explicit_conversion>> guest_sync_private(gpa, size, p1);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|191| <<guest_test_explicit_conversion>> guest_sync_private(gpa + j, PAGE_SIZE, p1);
+ */
 static void guest_sync_private(uint64_t gpa, uint64_t size, uint8_t pattern)
 {
 	GUEST_SYNC4(SYNC_PRIVATE, gpa, size, pattern);
@@ -87,6 +108,11 @@ static void guest_sync_private(uint64_t gpa, uint64_t size, uint8_t pattern)
 #define MAP_GPA_SHARED		BIT(1)
 #define MAP_GPA_DO_FALLOCATE	BIT(2)
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|131| <<guest_map_shared>> guest_map_mem(gpa, size, true, do_fallocate);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|141| <<guest_map_private>> guest_map_mem(gpa, size, false, do_fallocate);
+ */
 static void guest_map_mem(uint64_t gpa, uint64_t size, bool map_shared,
 			  bool do_fallocate)
 {
@@ -99,16 +125,36 @@ static void guest_map_mem(uint64_t gpa, uint64_t size, bool map_shared,
 	kvm_hypercall_map_gpa_range(gpa, size, flags);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|186| <<guest_test_explicit_conversion>> guest_map_shared(gpa + j, PAGE_SIZE, do_fallocate);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|201| <<guest_test_explicit_conversion>> guest_map_shared(gpa, size, do_fallocate);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|214| <<guest_test_explicit_conversion>> guest_map_shared(base_gpa, PER_CPU_DATA_SIZE, true);
+ */
 static void guest_map_shared(uint64_t gpa, uint64_t size, bool do_fallocate)
 {
 	guest_map_mem(gpa, size, true, do_fallocate);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|159| <<guest_test_explicit_conversion>> guest_map_private(gpa, size, do_fallocate);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|240| <<guest_test_punch_hole>> guest_map_private(base_gpa, PER_CPU_DATA_SIZE, false);
+ */
 static void guest_map_private(uint64_t gpa, uint64_t size, bool do_fallocate)
 {
 	guest_map_mem(gpa, size, false, do_fallocate);
 }
 
+/*
+ * 在以下使用test_ranges[]:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|137| <<guest_test_explicit_conversion>> for (i = 0; i < ARRAY_SIZE(test_ranges); i++) {
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|138| <<guest_test_explicit_conversion>> uint64_t gpa = base_gpa + test_ranges[i].offset;
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|139| <<guest_test_explicit_conversion>> uint64_t size = test_ranges[i].size;
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|242| <<guest_test_punch_hole>> for (i = 0; i < ARRAY_SIZE(test_ranges); i++) {
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|243| <<guest_test_punch_hole>> uint64_t gpa = base_gpa + test_ranges[i].offset;
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|244| <<guest_test_punch_hole>> uint64_t size = test_ranges[i].size;
+ */
 struct {
 	uint64_t offset;
 	uint64_t size;
@@ -120,6 +166,11 @@ struct {
 	GUEST_STAGE(SZ_2M, PAGE_SIZE),
 };
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|319| <<guest_code>> guest_test_explicit_conversion(base_gpa, false);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|320| <<guest_code>> guest_test_explicit_conversion(base_gpa, true);
+ */
 static void guest_test_explicit_conversion(uint64_t base_gpa, bool do_fallocate)
 {
 	const uint8_t def_p = 0xaa;
@@ -134,6 +185,14 @@ static void guest_test_explicit_conversion(uint64_t base_gpa, bool do_fallocate)
 
 	memcmp_g(base_gpa, init_p, PER_CPU_DATA_SIZE);
 
+	/*
+	 * ranges:
+	 * 162         GUEST_STAGE(0, PAGE_SIZE),
+	 * 163         GUEST_STAGE(0, SZ_2M),
+	 * 164         GUEST_STAGE(PAGE_SIZE, PAGE_SIZE),
+	 * 165         GUEST_STAGE(PAGE_SIZE, SZ_2M),
+	 * 166         GUEST_STAGE(SZ_2M, PAGE_SIZE),
+	 */
 	for (i = 0; i < ARRAY_SIZE(test_ranges); i++) {
 		uint64_t gpa = base_gpa + test_ranges[i].offset;
 		uint64_t size = test_ranges[i].size;
@@ -310,8 +369,18 @@ static void handle_exit_hypercall(struct kvm_vcpu *vcpu)
 	run->hypercall.ret = 0;
 }
 
+/*
+ * 在以下使用run_vcpus:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|357| <<global>> static bool run_vcpus;
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|370| <<__test_mem_conversions>> while (!READ_ONCE(run_vcpus))
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|494| <<test_mem_conversions>> WRITE_ONCE(run_vcpus, true);
+ */
 static bool run_vcpus;
 
+/*
+ * 在以下使用__test_mem_conversions():
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|416| <<test_mem_conversions>> pthread_create(&threads[i], NULL, __test_mem_conversions, vcpus[i]);
+ */
 static void *__test_mem_conversions(void *__vcpu)
 {
 	struct kvm_vcpu *vcpu = __vcpu;
@@ -319,6 +388,12 @@ static void *__test_mem_conversions(void *__vcpu)
 	struct kvm_vm *vm = vcpu->vm;
 	struct ucall uc;
 
+	/*
+	 * 在以下使用run_vcpus:
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|357| <<global>> static bool run_vcpus;
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|370| <<__test_mem_conversions>> while (!READ_ONCE(run_vcpus))
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|494| <<test_mem_conversions>> WRITE_ONCE(run_vcpus, true);
+	 */
 	while (!READ_ONCE(run_vcpus))
 		;
 
@@ -367,6 +442,10 @@ static void *__test_mem_conversions(void *__vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|479| <<main>> test_mem_conversions(src_type, nr_vcpus, nr_memslots);
+ */
 static void test_mem_conversions(enum vm_mem_backing_src_type src_type, uint32_t nr_vcpus,
 				 uint32_t nr_memslots)
 {
@@ -374,9 +453,21 @@ static void test_mem_conversions(enum vm_mem_backing_src_type src_type, uint32_t
 	 * Allocate enough memory so that each vCPU's chunk of memory can be
 	 * naturally aligned with respect to the size of the backing store.
 	 */
+	/*
+	 * 默认2097152 = 2MB
+	 */
 	const size_t alignment = max_t(size_t, SZ_2M, get_backing_src_pagesz(src_type));
+	/*
+	 * 2MB + 4K, 会align_up到4194304=4MB
+	 */
 	const size_t per_cpu_size = align_up(PER_CPU_DATA_SIZE, alignment);
+	/*
+	 * 如果1个vcpu, 是4MB
+	 */
 	const size_t memfd_size = per_cpu_size * nr_vcpus;
+	/*
+	 * 如果一个slot, 是4MB ---> 1024个4k的page
+	 */
 	const size_t slot_size = memfd_size / nr_memslots;
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
 	pthread_t threads[KVM_MAX_VCPUS];
@@ -391,12 +482,23 @@ static void test_mem_conversions(enum vm_mem_backing_src_type src_type, uint32_t
 	TEST_ASSERT(slot_size * nr_memslots == memfd_size,
 		    "The memfd size (0x%lx) needs to be cleanly divisible by the number of memslots (%u)",
 		    memfd_size, nr_memslots);
+	/*
+	 * 在kvm调用kvm_alloc_memslot_metadata() 2次:
+	 * 1. base=0, npages=657, id=0
+	 * 2. base=1043968, npages=1, id=32765
+	 */
 	vm = __vm_create_with_vcpus(shape, nr_vcpus, 0, guest_code, vcpus);
 
 	vm_enable_cap(vm, KVM_CAP_EXIT_HYPERCALL, (1 << KVM_HC_MAP_GPA_RANGE));
 
 	memfd = vm_create_guest_memfd(vm, memfd_size, 0);
 
+	/*
+	 * 如果只有一个slot, base=1048576, npages=1024, id=10:
+	 * BASE_DATA_GPA + slot_size * i : 4GB + 4MB  ---> guest_paddr
+	 * BASE_DATA_SLOT + i : 10                    ---> slot
+	 * slot_size / vm->page_size : 1024           ---> npages
+	 */
 	for (i = 0; i < nr_memslots; i++)
 		vm_mem_add(vm, src_type, BASE_DATA_GPA + slot_size * i,
 			   BASE_DATA_SLOT + i, slot_size / vm->page_size,
@@ -416,6 +518,12 @@ static void test_mem_conversions(enum vm_mem_backing_src_type src_type, uint32_t
 		pthread_create(&threads[i], NULL, __test_mem_conversions, vcpus[i]);
 	}
 
+	/*
+	 * 在以下使用run_vcpus:
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|357| <<global>> static bool run_vcpus;
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|370| <<__test_mem_conversions>> while (!READ_ONCE(run_vcpus))
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|494| <<test_mem_conversions>> WRITE_ONCE(run_vcpus, true);
+	 */
 	WRITE_ONCE(run_vcpus, true);
 
 	for (i = 0; i < nr_vcpus; i++)
diff --git a/virt/kvm/dirty_ring.c b/virt/kvm/dirty_ring.c
index 86d267db8..7a8172355 100644
--- a/virt/kvm/dirty_ring.c
+++ b/virt/kvm/dirty_ring.c
@@ -50,6 +50,11 @@ static bool kvm_dirty_ring_full(struct kvm_dirty_ring *ring)
 	return kvm_dirty_ring_used(ring) >= ring->size;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/dirty_ring.c|150| <<kvm_dirty_ring_reset>> kvm_reset_dirty_gfn(kvm, cur_slot, cur_offset, mask);
+ *   - virt/kvm/dirty_ring.c|157| <<kvm_dirty_ring_reset>> kvm_reset_dirty_gfn(kvm, cur_slot, cur_offset, mask);
+ */
 static void kvm_reset_dirty_gfn(struct kvm *kvm, u32 slot, u64 offset, u64 mask)
 {
 	struct kvm_memory_slot *memslot;
@@ -71,6 +76,10 @@ static void kvm_reset_dirty_gfn(struct kvm *kvm, u32 slot, u64 offset, u64 mask)
 	KVM_MMU_UNLOCK(kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4373| <<kvm_vm_ioctl_create_vcpu>> r = kvm_dirty_ring_alloc(&vcpu->dirty_ring, id, kvm->dirty_ring_size);
+ */
 int kvm_dirty_ring_alloc(struct kvm_dirty_ring *ring, int index, u32 size)
 {
 	ring->dirty_gfns = vzalloc(size);
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 1e567d1f6..b17524c52 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -18,6 +18,11 @@
 #include <linux/export.h>
 #include <trace/events/kvm.h>
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|263| <<irqfd_update>> n_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);
+ *   - virt/kvm/irqchip.c|83| <<kvm_set_irq>> i = kvm_irq_map_gsi(kvm, irq_set, irq);
+ */
 int kvm_irq_map_gsi(struct kvm *kvm,
 		    struct kvm_kernel_irq_routing_entry *entries, int gsi)
 {
@@ -85,6 +90,9 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 
 	while (i--) {
 		int r;
+		/*
+		 * 比如: kvm_set_msi()
+		 */
 		r = irq_set[i].set(&irq_set[i], kvm, irq_source_id, level,
 				   line_status);
 		if (r < 0)
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 0f50960b0..806a36e7d 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -76,6 +76,12 @@
 MODULE_AUTHOR("Qumranet");
 MODULE_LICENSE("GPL");
 
+/*
+ * 非powerpc在以下使用global的halt_poll_ns:
+ *   - virt/kvm/kvm_main.c|81| <<global>> module_param(halt_poll_ns, uint, 0644);
+ *   - virt/kvm/kvm_main.c|82| <<global>> EXPORT_SYMBOL_GPL(halt_poll_ns);
+ *   - virt/kvm/kvm_main.c|3954| <<kvm_vcpu_max_halt_poll_ns>> return READ_ONCE(halt_poll_ns);
+ */
 /* Architectures should define their poll value according to the halt latency */
 unsigned int halt_poll_ns = KVM_HALT_POLL_NS_DEFAULT;
 module_param(halt_poll_ns, uint, 0644);
@@ -1216,6 +1222,13 @@ static struct kvm *kvm_create_vm(unsigned long type, const char *fdname)
 	INIT_LIST_HEAD(&kvm->gpc_list);
 	spin_lock_init(&kvm->gpc_lock);
 
+	/*
+	 * 在以下使用kvm->devices:
+	 *   - virt/kvm/kvm_main.c|1219| <<kvm_create_vm>> INIT_LIST_HEAD(&kvm->devices);
+	 *   - virt/kvm/kvm_main.c|1335| <<kvm_destroy_devices>> list_for_each_entry_safe(dev, tmp, &kvm->devices, vm_node) {
+	 *   - virt/kvm/kvm_main.c|4868| <<kvm_ioctl_create_device>> list_add(&dev->vm_node, &kvm->devices);
+	 *   - virt/kvm/vfio.c|370| <<kvm_vfio_create>> list_for_each_entry(tmp, &dev->kvm->devices, vm_node)
+	 */
 	INIT_LIST_HEAD(&kvm->devices);
 	kvm->max_vcpus = KVM_MAX_VCPUS;
 
@@ -1450,6 +1463,10 @@ static int kvm_vm_release(struct inode *inode, struct file *filp)
  * Allocation size is twice as large as the actual dirty bitmap size.
  * See kvm_vm_ioctl_get_dirty_log() why this is needed.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1722| <<kvm_prepare_memory_region>> r = kvm_alloc_dirty_bitmap(new);
+ */
 static int kvm_alloc_dirty_bitmap(struct kvm_memory_slot *memslot)
 {
 	unsigned long dirty_bytes = kvm_dirty_bitmap_bytes(memslot);
@@ -1688,6 +1705,10 @@ static void kvm_swap_active_memslots(struct kvm *kvm, int as_id)
 	slots->generation = gen;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1950| <<kvm_set_memslot>> r = kvm_prepare_memory_region(kvm, old, new, change);
+ */
 static int kvm_prepare_memory_region(struct kvm *kvm,
 				     const struct kvm_memory_slot *old,
 				     struct kvm_memory_slot *new,
@@ -1901,6 +1922,11 @@ static void kvm_update_flags_memslot(struct kvm *kvm,
 	kvm_activate_memslot(kvm, old, new);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2076| <<__kvm_set_memory_region>> return kvm_set_memslot(kvm, old, NULL, KVM_MR_DELETE);
+ *   - virt/kvm/kvm_main.c|2129| <<__kvm_set_memory_region>> r = kvm_set_memslot(kvm, old, new, change);
+ */
 static int kvm_set_memslot(struct kvm *kvm,
 			   struct kvm_memory_slot *old,
 			   struct kvm_memory_slot *new,
@@ -2017,6 +2043,11 @@ static bool kvm_check_memslot_overlap(struct kvm_memslots *slots, int id,
  *
  * Must be called holding kvm->slots_lock for write.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12680| <<__x86_set_memory_region>> r = __kvm_set_memory_region(kvm, &m);
+ *   - virt/kvm/kvm_main.c|2150| <<kvm_set_memory_region>> r = __kvm_set_memory_region(kvm, mem);
+ */
 int __kvm_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region2 *mem)
 {
@@ -2233,6 +2264,10 @@ EXPORT_SYMBOL_GPL(kvm_get_dirty_log);
  * exiting to userspace will be logged for the next call.
  *
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2370| <<kvm_vm_ioctl_get_dirty_log>> r = kvm_get_dirty_log_protect(kvm, log);
+ */
 static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 {
 	struct kvm_memslots *slots;
@@ -2331,6 +2366,17 @@ static int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 
 	mutex_lock(&kvm->slots_lock);
 
+	/*
+	 * // for KVM_GET_DIRTY_LOG
+	 * struct kvm_dirty_log {
+	 *     __u32 slot;
+	 *     __u32 padding1;
+	 *     union {
+	 *         void __user *dirty_bitmap; // one bit per page
+	 *         __u64 padding2;
+	 *     };
+	 * };
+	 */
 	r = kvm_get_dirty_log_protect(kvm, log);
 
 	mutex_unlock(&kvm->slots_lock);
@@ -2438,9 +2484,20 @@ static int kvm_vm_ioctl_clear_dirty_log(struct kvm *kvm,
  * Returns true if _all_ gfns in the range [@start, @end) have attributes
  * matching @attrs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7407| <<hugepage_has_attrs>> return kvm_range_has_memory_attributes(kvm, start, end, attrs);
+ *   - virt/kvm/kvm_main.c|2573| <<kvm_vm_set_mem_attributes>> if (kvm_range_has_memory_attributes(kvm, start, end, attributes))
+ */
 bool kvm_range_has_memory_attributes(struct kvm *kvm, gfn_t start, gfn_t end,
 				     unsigned long attrs)
 {
+	/*
+	 * 838 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
+	 * 839         // Protected by slots_locks (for writes) and RCU (for reads)
+	 * 840         struct xarray mem_attr_array;
+	 * 841 #endif
+	 */
 	XA_STATE(xas, &kvm->mem_attr_array, start);
 	unsigned long index;
 	bool has_attrs;
@@ -2541,6 +2598,9 @@ static bool kvm_pre_set_memory_attributes(struct kvm *kvm,
 	return kvm_arch_pre_set_memory_attributes(kvm, range);
 }
 
+/*
+ * attributes可以是0或者KVM_MEMORY_ATTRIBUTE_PRIVATE???
+ */
 /* Set @attributes for the gfn range [@start, @end). */
 static int kvm_vm_set_mem_attributes(struct kvm *kvm, gfn_t start, gfn_t end,
 				     unsigned long attributes)
@@ -2585,6 +2645,9 @@ static int kvm_vm_set_mem_attributes(struct kvm *kvm, gfn_t start, gfn_t end,
 
 	kvm_handle_gfn_range(kvm, &pre_set_range);
 
+	/*
+	 * 这里是set, 设置!!!
+	 */
 	for (i = start; i < end; i++) {
 		r = xa_err(xa_store(&kvm->mem_attr_array, i, entry,
 				    GFP_KERNEL_ACCOUNT));
@@ -2759,6 +2822,19 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_hva);
  * @writable: used to return the read/write attribute of the @slot if the hva
  * is valid and @writable is not NULL
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1683| <<kvm_handle_guest_abort>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+ *   - arch/loongarch/kvm/mmu.c|819| <<kvm_map_page>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writeable);
+ *   - arch/riscv/kvm/vcpu_exit.c|25| <<gstage_page_fault>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+ *   - arch/s390/kvm/gaccess.c|1006| <<access_guest_page_with_key>> hva = gfn_to_hva_memslot_prot(slot, gfn, &writable);
+ *   - arch/s390/kvm/gaccess.c|1184| <<cmpxchg_guest_abs_with_key>> hva = gfn_to_hva_memslot_prot(slot, gfn, &writable);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|399| <<FNAME(walk_addr_generic)>> host_addr = gfn_to_hva_memslot_prot(slot, gpa_to_gfn(real_gpa), &walker->pte_writable[walker->level - 1]);
+ *   - virt/kvm/kvm_main.c|2808| <<gfn_to_hva_prot>> return gfn_to_hva_memslot_prot(slot, gfn, writable);
+ *   - virt/kvm/kvm_main.c|2815| <<kvm_vcpu_gfn_to_hva_prot>> return gfn_to_hva_memslot_prot(slot, gfn, writable);
+ *   - virt/kvm/kvm_main.c|3361| <<__kvm_read_guest_page>> addr = gfn_to_hva_memslot_prot(slot, gfn, NULL);
+ *   - virt/kvm/kvm_main.c|3434| <<__kvm_read_guest_atomic>> addr = gfn_to_hva_memslot_prot(slot, gfn, NULL);
+ */
 unsigned long gfn_to_hva_memslot_prot(struct kvm_memory_slot *slot,
 				      gfn_t gfn, bool *writable)
 {
@@ -3028,6 +3104,17 @@ kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool interruptible,
 	return pfn;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1489| <<user_mem_abort>> pfn = __gfn_to_pfn_memslot(memslot, gfn, false, false, NULL, write_fault, &writable, NULL);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|616| <<kvmppc_book3s_hv_page_fault>> pfn = __gfn_to_pfn_memslot(memslot, gfn, false, false, NULL, writing, &write_ok, NULL);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|855| <<kvmppc_book3s_instantiate_page>> pfn = __gfn_to_pfn_memslot(memslot, gfn, false, false, NULL, writing, upgrade_p, NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|4425| <<__kvm_faultin_pfn>> fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, false, &async, fault->write, &fault->map_writable, &fault->hva);
+ *   - arch/x86/kvm/mmu/mmu.c|4447| <<__kvm_faultin_pfn>> fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, true, NULL, fault->write, &fault->map_writable, &fault->hva);
+ *   - virt/kvm/kvm_main.c|3097| <<gfn_to_pfn_prot>> return __gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn, false, false, NULL, write_fault, writable, NULL);
+ *   - virt/kvm/kvm_main.c|3104| <<gfn_to_pfn_memslot>> return __gfn_to_pfn_memslot(slot, gfn, false, false, NULL, true, NULL, NULL);
+ *   - virt/kvm/kvm_main.c|3111| <<gfn_to_pfn_memslot_atomic>> return __gfn_to_pfn_memslot(slot, gfn, true, false, NULL, true, NULL, NULL);
+ */
 kvm_pfn_t __gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn,
 			       bool atomic, bool interruptible, bool *async,
 			       bool write_fault, bool *writable, hva_t *hva)
@@ -3632,6 +3719,25 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 }
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1582| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn); 
+ *   - arch/loongarch/kvm/mmu.c|923| <<kvm_map_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - arch/s390/kvm/gaccess.c|1024| <<access_guest_page_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/s390/kvm/gaccess.c|1246| <<cmpxchg_guest_abs_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|3497| <<fast_pf_fix_direct_spte>> mark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);
+ *   - arch/x86/kvm/mmu/spte.c|709| <<make_spte>> mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
+ *   - arch/x86/kvm/x86.c|3170| <<kvm_setup_guest_pvclock>> mark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/x86.c|3731| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/x86.c|5068| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/xen.c|460| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, gpc1->memslot, gpc1->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/xen.c|462| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, gpc2->memslot, gpc2->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/xen.c|574| <<kvm_xen_inject_pending_events>> mark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3469| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3607| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3707| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3716| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+ */
 void mark_page_dirty_in_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *memslot,
 		 	     gfn_t gfn)
@@ -3761,6 +3867,10 @@ static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
  * pending.  This is mostly used when halting a vCPU, but may also be used
  * directly for other vCPU non-runnable states, e.g. x86's Wait-For-SIPI.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3968| <<kvm_vcpu_halt>> waited = kvm_vcpu_block(vcpu);
+ */
 bool kvm_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	struct rcuwait *wait = kvm_arch_vcpu_get_wait(vcpu);
@@ -3793,6 +3903,13 @@ bool kvm_vcpu_block(struct kvm_vcpu *vcpu)
 	return waited;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4010| <<kvm_vcpu_halt>> update_halt_poll_stats(vcpu, start, poll_end, !waited);
+ *
+ * waited=true说明schedule了, 没有pending events
+ * waited=false说明没有schedule, poll一会对了, 否则直接schedule就慢了       --> success是true
+ */
 static inline void update_halt_poll_stats(struct kvm_vcpu *vcpu, ktime_t start,
 					  ktime_t end, bool success)
 {
@@ -3815,10 +3932,20 @@ static inline void update_halt_poll_stats(struct kvm_vcpu *vcpu, ktime_t start,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3967| <<kvm_vcpu_halt>> unsigned int max_halt_poll_ns = kvm_vcpu_max_halt_poll_ns(vcpu);
+ *   - virt/kvm/kvm_main.c|4014| <<kvm_vcpu_halt>> max_halt_poll_ns = kvm_vcpu_max_halt_poll_ns(vcpu);
+ */
 static unsigned int kvm_vcpu_max_halt_poll_ns(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
 
+	/*
+	 * 在以下使用kvm->override_halt_poll_ns:
+	 *   - virt/kvm/kvm_main.c|3926| <<kvm_vcpu_max_halt_poll_ns>> if (kvm->override_halt_poll_ns) {
+	 *   - virt/kvm/kvm_main.c|5121| <<kvm_vm_ioctl_enable_cap_generic>> kvm->override_halt_poll_ns = true;
+	 */
 	if (kvm->override_halt_poll_ns) {
 		/*
 		 * Ensure kvm->max_halt_poll_ns is not read before
@@ -3839,6 +3966,20 @@ static unsigned int kvm_vcpu_max_halt_poll_ns(struct kvm_vcpu *vcpu)
  * expensive block+unblock sequence if a wake event arrives soon after the vCPU
  * is halted.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|787| <<kvm_vcpu_wfi>> kvm_vcpu_halt(vcpu);
+ *   - arch/loongarch/kvm/exit.c|204| <<kvm_emu_idle>> kvm_vcpu_halt(vcpu);
+ *   - arch/mips/kvm/emulate.c|955| <<kvm_mips_emul_wait>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr.c|501| <<kvmppc_set_msr_pr>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr_papr.c|395| <<kvmppc_h_pr>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/booke.c|724| <<kvmppc_core_prepare_to_enter>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|240| <<kvmppc_kvm_pv>> kvm_vcpu_halt(vcpu);
+ *   - arch/riscv/kvm/vcpu_insn.c|192| <<kvm_riscv_vcpu_wfi>> kvm_vcpu_halt(vcpu);
+ *   - arch/s390/kvm/interrupt.c|1334| <<kvm_s390_handle_wait>> kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/x86.c|11684| <<vcpu_block>> kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/xen.c|1347| <<kvm_xen_schedop_poll>> kvm_vcpu_halt(vcpu);
+ */
 void kvm_vcpu_halt(struct kvm_vcpu *vcpu)
 {
 	unsigned int max_halt_poll_ns = kvm_vcpu_max_halt_poll_ns(vcpu);
@@ -5278,6 +5419,14 @@ static long kvm_vm_ioctl(struct file *filp,
 #endif /* CONFIG_HAVE_KVM_IRQ_ROUTING */
 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
 	case KVM_SET_MEMORY_ATTRIBUTES: {
+		/*
+		 * struct kvm_memory_attributes {
+		 *     __u64 address;
+		 *     __u64 size;
+		 *     __u64 attributes;
+		 *     __u64 flags;
+		 * };
+		 */
 		struct kvm_memory_attributes attrs;
 
 		r = -EFAULT;
@@ -5521,9 +5670,34 @@ static struct miscdevice kvm_dev = {
 __visible bool kvm_rebooting;
 EXPORT_SYMBOL_GPL(kvm_rebooting);
 
+/*
+ * 在以下使用percpu的hardware_enabled:
+ *   - virt/kvm/kvm_main.c|5568| <<__hardware_enable_nolock>> if (__this_cpu_read(hardware_enabled))
+ *   - virt/kvm/kvm_main.c|5577| <<__hardware_enable_nolock>> __this_cpu_write(hardware_enabled, true);
+ *   - virt/kvm/kvm_main.c|5609| <<hardware_disable_nolock>> if (!__this_cpu_read(hardware_enabled))
+ *   - virt/kvm/kvm_main.c|5614| <<hardware_disable_nolock>> __this_cpu_write(hardware_enabled, false);
+ */
 static DEFINE_PER_CPU(bool, hardware_enabled);
+/*
+ * 在以下使用kvm_usage_count:
+ *   - virt/kvm/kvm_main.c|5597| <<kvm_online_cpu>> if (kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|5620| <<kvm_offline_cpu>> if (kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|5628| <<hardware_disable_all_nolock>> BUG_ON(!kvm_usage_count);
+ *   - virt/kvm/kvm_main.c|5630| <<hardware_disable_all_nolock>> kvm_usage_count--;
+ *   - virt/kvm/kvm_main.c|5631| <<hardware_disable_all_nolock>> if (!kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|5675| <<hardware_enable_all>> kvm_usage_count++;
+ *   - virt/kvm/kvm_main.c|5676| <<hardware_enable_all>> if (kvm_usage_count == 1) {
+ *   - virt/kvm/kvm_main.c|5722| <<kvm_suspend>> if (kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|5732| <<kvm_resume>> if (kvm_usage_count)
+ */
 static int kvm_usage_count;
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5583| <<hardware_enable_nolock>> if (__hardware_enable_nolock())
+ *   - virt/kvm/kvm_main.c|5598| <<kvm_online_cpu>> ret = __hardware_enable_nolock();
+ *   - virt/kvm/kvm_main.c|5733| <<kvm_resume>> WARN_ON_ONCE(__hardware_enable_nolock());
+ */
 static int __hardware_enable_nolock(void)
 {
 	if (__this_cpu_read(hardware_enabled))
@@ -5539,12 +5713,20 @@ static int __hardware_enable_nolock(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5677| <<hardware_enable_all>> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ */
 static void hardware_enable_nolock(void *failed)
 {
 	if (__hardware_enable_nolock())
 		atomic_inc(failed);
 }
 
+/*
+ * 在以下使用kvm_online_cpu:
+ *   - virt/kvm/kvm_main.c|6453| <<kvm_init>> r = cpuhp_setup_state_nocalls(CPUHP_AP_KVM_ONLINE, "kvm/cpu:online", kvm_online_cpu, kvm_offline_cpu);
+ */
 static int kvm_online_cpu(unsigned int cpu)
 {
 	int ret = 0;
@@ -5602,6 +5784,10 @@ static void hardware_disable_all(void)
 	cpus_read_unlock();
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1267| <<kvm_create_vm>> r = hardware_enable_all();
+ */
 static int hardware_enable_all(void)
 {
 	atomic_t failed = ATOMIC_INIT(0);
@@ -5634,6 +5820,9 @@ static int hardware_enable_all(void)
 	r = 0;
 
 	kvm_usage_count++;
+	/*
+	 * 这里只执行一次
+	 */
 	if (kvm_usage_count == 1) {
 		on_each_cpu(hardware_enable_nolock, &failed, 1);
 
@@ -6388,6 +6577,11 @@ static struct perf_guest_info_callbacks kvm_guest_cbs = {
 	.handle_intel_pt_intr	= NULL,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2169| <<init_subsystems>> kvm_register_perf_callbacks(NULL);
+ *   - arch/x86/kvm/x86.c|9752| <<__kvm_x86_vendor_init>> kvm_register_perf_callbacks(ops->handle_intel_pt_intr);
+ */
 void kvm_register_perf_callbacks(unsigned int (*pt_intr_handler)(void))
 {
 	kvm_guest_cbs.handle_intel_pt_intr = pt_intr_handler;
-- 
2.34.1

