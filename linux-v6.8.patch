From 2f8c122b7ab602938a7c22cf3255bd18df203815 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 25 Mar 2024 15:04:50 -0700
Subject: [PATCH 1/1] linux-v6.8

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/kvm_host.h             |   9 +
 arch/arm64/kvm/arm.c                          |   4 +
 arch/arm64/kvm/fpsimd.c                       |   4 +
 arch/arm64/kvm/hyp/nvhe/debug-sr.c            |   4 +
 arch/arm64/kvm/hyp/nvhe/hyp-main.c            |   4 +
 arch/arm64/kvm/hyp/nvhe/switch.c              |   7 +
 arch/arm64/kvm/hyp/vhe/switch.c               |   4 +
 arch/x86/events/core.c                        |  19 +
 arch/x86/include/asm/kvm_host.h               | 188 ++++
 arch/x86/include/uapi/asm/mtrr.h              |  13 +
 arch/x86/kvm/cpuid.c                          |  13 +
 arch/x86/kvm/mmu.h                            | 190 ++++
 arch/x86/kvm/mmu/mmu.c                        | 603 +++++++++++++
 arch/x86/kvm/mmu/mmu_internal.h               |  60 ++
 arch/x86/kvm/mmu/paging_tmpl.h                | 175 ++++
 arch/x86/kvm/mmu/spte.c                       | 837 ++++++++++++++++++
 arch/x86/kvm/mmu/spte.h                       | 367 ++++++++
 arch/x86/kvm/mmu/tdp_iter.c                   | 106 +++
 arch/x86/kvm/mmu/tdp_iter.h                   |  82 ++
 arch/x86/kvm/mmu/tdp_mmu.c                    | 157 ++++
 arch/x86/kvm/mmu/tdp_mmu.h                    |  13 +
 arch/x86/kvm/mtrr.c                           | 615 +++++++++++++
 arch/x86/kvm/pmu.c                            |  10 +
 arch/x86/kvm/pmu.h                            |   4 +
 arch/x86/kvm/svm/svm.c                        |   6 +
 arch/x86/kvm/vmx/nested.c                     |  41 +
 arch/x86/kvm/vmx/vmx.c                        |  49 +
 arch/x86/kvm/vmx/vmx.h                        |  13 +
 arch/x86/kvm/x86.c                            | 201 +++++
 arch/x86/kvm/x86.h                            |  51 ++
 drivers/perf/arm_pmu.c                        |   5 +
 drivers/scsi/scsi_lib.c                       |   4 +
 drivers/scsi/virtio_scsi.c                    |  16 +
 include/linux/kvm_host.h                      |  18 +
 .../selftests/kvm/include/kvm_util_base.h     |  10 +
 .../selftests/kvm/include/x86_64/processor.h  |   5 +
 .../kvm/x86_64/private_mem_conversions_test.c | 108 +++
 virt/kvm/kvm_main.c                           | 127 +++
 38 files changed, 4142 insertions(+)

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 21c57b812..8be835820 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -603,6 +603,15 @@ struct kvm_vcpu_arch {
 	struct user_fpsimd_state *host_fpsimd_state;	/* hyp VA */
 	struct task_struct *parent_task;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->host_debug_state:
+	 *   - arch/arm64/kvm/hyp/include/hyp/debug-sr.h|140| <<__debug_switch_to_guest_common>> host_dbg = &vcpu->arch.host_debug_state.regs;
+	 *   - arch/arm64/kvm/hyp/include/hyp/debug-sr.h|159| <<__debug_switch_to_host_common>> host_dbg = &vcpu->arch.host_debug_state.regs;
+	 *   - arch/arm64/kvm/hyp/nvhe/debug-sr.c|86| <<__debug_save_host_buffers_nvhe>> __debug_save_spe(&vcpu->arch.host_debug_state.pmscr_el1);
+	 *   - arch/arm64/kvm/hyp/nvhe/debug-sr.c|89| <<__debug_save_host_buffers_nvhe>> __debug_save_trace(&vcpu->arch.host_debug_state.trfcr_el1);
+	 *   - arch/arm64/kvm/hyp/nvhe/debug-sr.c|100| <<__debug_restore_host_buffers_nvhe>> __debug_restore_spe(vcpu->arch.host_debug_state.pmscr_el1);
+	 *   - arch/arm64/kvm/hyp/nvhe/debug-sr.c|102| <<__debug_restore_host_buffers_nvhe>> __debug_restore_trace(vcpu->arch.host_debug_state.trfcr_el1);
+	 */
 	struct {
 		/* {Break,watch}point registers */
 		struct kvm_guest_debug_arch regs;
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index a25265aca..55a5570b3 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -2001,6 +2001,10 @@ static void cpu_hyp_uninit(void *discard)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5571| <<__hardware_enable_nolock>> if (kvm_arch_hardware_enable()) {
+ */
 int kvm_arch_hardware_enable(void)
 {
 	/*
diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
index 8c1d0d485..39beb3213 100644
--- a/arch/arm64/kvm/fpsimd.c
+++ b/arch/arm64/kvm/fpsimd.c
@@ -36,6 +36,10 @@ void kvm_vcpu_unshare_task_fp(struct kvm_vcpu *vcpu)
  * such that on entering hyp the relevant parts of current are already
  * mapped.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|650| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_arch_vcpu_run_map_fp(vcpu);
+ */
 int kvm_arch_vcpu_run_map_fp(struct kvm_vcpu *vcpu)
 {
 	int ret;
diff --git a/arch/arm64/kvm/hyp/nvhe/debug-sr.c b/arch/arm64/kvm/hyp/nvhe/debug-sr.c
index 4558c02eb..3ffc40d9f 100644
--- a/arch/arm64/kvm/hyp/nvhe/debug-sr.c
+++ b/arch/arm64/kvm/hyp/nvhe/debug-sr.c
@@ -94,6 +94,10 @@ void __debug_switch_to_guest(struct kvm_vcpu *vcpu)
 	__debug_switch_to_guest_common(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|348| <<__kvm_vcpu_run>> __debug_restore_host_buffers_nvhe(vcpu);
+ */
 void __debug_restore_host_buffers_nvhe(struct kvm_vcpu *vcpu)
 {
 	if (vcpu_get_flag(vcpu, DEBUG_STATE_SAVE_SPE))
diff --git a/arch/arm64/kvm/hyp/nvhe/hyp-main.c b/arch/arm64/kvm/hyp/nvhe/hyp-main.c
index 2385fd03e..8a2735566 100644
--- a/arch/arm64/kvm/hyp/nvhe/hyp-main.c
+++ b/arch/arm64/kvm/hyp/nvhe/hyp-main.c
@@ -23,6 +23,10 @@ DEFINE_PER_CPU(struct kvm_nvhe_init_params, kvm_init_params);
 
 void __kvm_hyp_host_forward_smc(struct kvm_cpu_context *host_ctxt);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|93| <<handle___kvm_vcpu_run>> flush_hyp_vcpu(hyp_vcpu);
+ */
 static void flush_hyp_vcpu(struct pkvm_hyp_vcpu *hyp_vcpu)
 {
 	struct kvm_vcpu *host_vcpu = hyp_vcpu->host_vcpu;
diff --git a/arch/arm64/kvm/hyp/nvhe/switch.c b/arch/arm64/kvm/hyp/nvhe/switch.c
index c50f8459e..f9f49f1c3 100644
--- a/arch/arm64/kvm/hyp/nvhe/switch.c
+++ b/arch/arm64/kvm/hyp/nvhe/switch.c
@@ -244,6 +244,13 @@ static void early_exit_filter(struct kvm_vcpu *vcpu, u64 *exit_code)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|339| <<global>> HANDLE_FUNC(__kvm_vcpu_run),
+ *   - arch/arm64/kvm/arm.c|949| <<kvm_arm_vcpu_enter_exit>> ret = kvm_call_hyp_ret(__kvm_vcpu_run, vcpu);
+ *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|99| <<handle___kvm_vcpu_run>> ret = __kvm_vcpu_run(&hyp_vcpu->vcpu);
+ *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|105| <<handle___kvm_vcpu_run>> ret = __kvm_vcpu_run(host_vcpu);
+ */
 /* Switch to the guest for legacy non-VHE systems */
 int __kvm_vcpu_run(struct kvm_vcpu *vcpu)
 {
diff --git a/arch/arm64/kvm/hyp/vhe/switch.c b/arch/arm64/kvm/hyp/vhe/switch.c
index 1581df6ae..7e313c1f1 100644
--- a/arch/arm64/kvm/hyp/vhe/switch.c
+++ b/arch/arm64/kvm/hyp/vhe/switch.c
@@ -267,6 +267,10 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 }
 NOKPROBE_SYMBOL(__kvm_vcpu_run_vhe);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|949| <<kvm_arm_vcpu_enter_exit>> ret = kvm_call_hyp_ret(__kvm_vcpu_run, vcpu);
+ */
 int __kvm_vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int ret;
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 09050641c..d55ec3264 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1155,6 +1155,12 @@ static int collect_event(struct cpu_hw_events *cpuc, struct perf_event *event,
  * dogrp: true if must collect siblings events (group)
  * returns total number of events and error code
  */
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1447| <<x86_pmu_add>> ret = n = collect_events(cpuc, event, false);
+ *   - arch/x86/events/core.c|2415| <<validate_group>> n = collect_events(fake_cpuc, leader, true);
+ *   - arch/x86/events/core.c|2420| <<validate_group>> n = collect_events(fake_cpuc, event, false);
+ */
 static int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader, bool dogrp)
 {
 	int num_counters = hybrid(cpuc->pmu, num_counters);
@@ -1209,6 +1215,10 @@ static int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader,
 	return n;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1342| <<x86_pmu_enable>> x86_assign_hw_event(event, cpuc, i);
+ */
 static inline void x86_assign_hw_event(struct perf_event *event,
 				struct cpu_hw_events *cpuc, int i)
 {
@@ -1280,6 +1290,15 @@ static inline int match_prev_assignment(struct hw_perf_event *hwc,
 
 static void x86_pmu_start(struct perf_event *event, int flags);
 
+/*
+ * 在以下使用x86_pmu_enable():
+ *   - arch/x86/events/core.c|66| <<global>> DEFINE_STATIC_CALL_NULL(x86_pmu_enable, *x86_pmu.enable);
+ *   - arch/x86/events/core.c|2682| <<global>> .pmu_enable = x86_pmu_enable,
+ *   - arch/x86/events/core.c|1521| <<x86_pmu_start>> static_call(x86_pmu_enable)(event);
+ *   - arch/x86/events/core.c|2010| <<x86_pmu_static_call_update>> static_call_update(x86_pmu_enable, x86_pmu.enable);
+ *
+ * struct pmu pmu.pmu_enable = x86_pmu_enable()
+ */
 static void x86_pmu_enable(struct pmu *pmu)
 {
 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d271ba20a..88e144903 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -145,11 +145,49 @@
 
 /* KVM Hugepage definitions for x86 */
 #define KVM_MAX_HUGEPAGE_LEVEL	PG_LEVEL_1G
+/*
+ * 3 - 1 + 1 = 3
+ */
 #define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)
+/*
+ * KVM_HPAGE_GFN_SHIFT(1) : (((1) - 1) * 9) =  0
+ * KVM_HPAGE_GFN_SHIFT(2) : (((2) - 1) * 9) =  9
+ * KVM_HPAGE_GFN_SHIFT(3) : (((3) - 1) * 9) = 18
+ * KVM_HPAGE_GFN_SHIFT(4) : (((4) - 1) * 9) = 27
+ * KVM_HPAGE_GFN_SHIFT(5) : (((5) - 1) * 9) = 36
+ */
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+/*
+ * KVM_HPAGE_SHIFT(1) : 12 +  0 = 12
+ * KVM_HPAGE_SHIFT(2) : 12 +  9 = 21
+ * KVM_HPAGE_SHIFT(3) : 12 + 18 = 30
+ * KVM_HPAGE_SHIFT(4) : 12 + 27 = 39
+ * KVM_HPAGE_SHIFT(5) : 12 + 36 = 48
+ */
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
+/*
+ * KVM_HPAGE_SIZE(1) : 1 << 12 = 4096 (4K)
+ * KVM_HPAGE_SIZE(2) : 1 << 21 = 2097152 (2M)
+ * KVM_HPAGE_SIZE(3) : 1 << 30 = 1073741824 (1G)
+ * KVM_HPAGE_SIZE(4) : 1 << 39 = 549755813888 (512G)
+ * KVM_HPAGE_SIZE(5) : 1 << 48 = 281474976710656 (262144G=256T)
+ */
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
+/*
+ * KVM_HPAGE_MASK(1) : 4K的mask
+ * KVM_HPAGE_MASK(2) : 2M的mask
+ * KVM_HPAGE_MASK(3) : 1G的mask
+ * KVM_HPAGE_MASK(4) : 512G的mask
+ * KVM_HPAGE_MASK(5) : 262144G(256T)的mask
+ */
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
+/*
+ * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+ * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+ * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+ * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+ * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+ */
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
 #define KVM_MEMSLOT_PAGES_TO_MMU_PAGES_RATIO 50
@@ -341,6 +379,16 @@ union kvm_mmu_page_role {
 		unsigned access:3;
 		unsigned invalid:1;
 		unsigned efer_nx:1;
+		/*
+		 * 在以下使用kvm_mmu_page_role->cr0_wp:
+		 *   - arch/x86/kvm/mmu/mmu.c|5329| <<update_permission_bitmask>> bool cr0_wp = is_cr0_wp(mmu);
+		 *   - arch/x86/kvm/mmu/mmu.c|5360| <<update_permission_bitmask>> if (!cr0_wp)
+		 *   - arch/x86/kvm/mmu/mmu.c|5520| <<kvm_calc_cpu_role>> role.base.cr0_wp = ____is_cr0_wp(regs);
+		 *   - arch/x86/kvm/mmu/mmu.c|5547| <<__kvm_mmu_refresh_passthrough_bits>> const bool cr0_wp = kvm_is_cr0_bit_set(vcpu, X86_CR0_WP);
+		 *   - arch/x86/kvm/mmu/mmu.c|5552| <<__kvm_mmu_refresh_passthrough_bits>> if (is_cr0_wp(mmu) == cr0_wp)
+		 *   - arch/x86/kvm/mmu/mmu.c|5555| <<__kvm_mmu_refresh_passthrough_bits>> mmu->cpu_role.base.cr0_wp = cr0_wp;
+		 *   - arch/x86/kvm/mmu/mmu.c|5579| <<kvm_calc_tdp_mmu_root_page_role>> role.cr0_wp = true;
+		 */
 		unsigned cr0_wp:1;
 		unsigned smep_andnot_wp:1;
 		unsigned smap_andnot_wp:1;
@@ -415,7 +463,18 @@ struct kvm_pio_request {
 #define PT64_ROOT_MAX_LEVEL 5
 
 struct rsvd_bits_validate {
+	/*
+	 * 在4个不同的函数reset:
+	 * - __reset_rsvds_bits_mask()
+	 * - __reset_rsvds_bits_mask_ept()
+	 * - reset_shadow_zero_bits_mask()
+	 * - reset_tdp_shadow_zero_bits_mask()
+	 */
 	u64 rsvd_bits_mask[2][PT64_ROOT_MAX_LEVEL];
+	/*
+	 * 除了那些reset的, 主要在下面使用bad_mt_xwr:
+	 *   - arch/x86/kvm/mmu/spte.h|485| <<__is_bad_mt_xwr>> return rsvd_check->bad_mt_xwr & BIT_ULL(pte & 0x3f);
+	 */
 	u64 bad_mt_xwr;
 };
 
@@ -446,12 +505,43 @@ struct kvm_page_fault;
 struct kvm_mmu {
 	unsigned long (*get_guest_pgd)(struct kvm_vcpu *vcpu);
 	u64 (*get_pdptr)(struct kvm_vcpu *vcpu, int index);
+	/*
+	 * 在以下设置kvm_mmu->page_fault:
+	 *   - arch/x86/kvm/mmu/mmu.c|4808| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5424| <<paging64_init_context>> context->page_fault = paging64_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5435| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5543| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5702| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault;
+	 */
 	int (*page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);
 	void (*inject_page_fault)(struct kvm_vcpu *vcpu,
 				  struct x86_exception *fault);
+	/*
+	 * 在以下设置kvm_mmu->gva_to_gpa:
+	 *   - arch/x86/kvm/mmu/mmu.c|4809| <<nonpaging_init_context>> context->gva_to_gpa = nonpaging_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5425| <<paging64_init_context>> context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5436| <<paging32_init_context>> context->gva_to_gpa = paging32_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5550| <<init_kvm_tdp_mmu>> context->gva_to_gpa = nonpaging_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5552| <<init_kvm_tdp_mmu>> context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5554| <<init_kvm_tdp_mmu>> context->gva_to_gpa = paging32_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5703| <<kvm_init_shadow_ept_mmu>> context->gva_to_gpa = ept_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5764| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = nonpaging_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5766| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5768| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging64_gva_to_gpa;
+	 *   - arch/x86/kvm/mmu/mmu.c|5770| <<init_kvm_nested_mmu>> g_context->gva_to_gpa = paging32_gva_to_gpa;
+	 */
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			    gpa_t gva_or_gpa, u64 access,
 			    struct x86_exception *exception);
+	/*
+	 * 在以下设置kvm_mmu->sync_spte:
+	 *   - arch/x86/kvm/mmu/mmu.c|4810| <<nonpaging_init_context>> context->sync_spte = NULL;
+	 *   - arch/x86/kvm/mmu/mmu.c|5426| <<paging64_init_context>> context->sync_spte = paging64_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|5437| <<paging32_init_context>> context->sync_spte = paging32_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|5544| <<init_kvm_tdp_mmu>> context->sync_spte = NULL;
+	 *   - arch/x86/kvm/mmu/mmu.c|5704| <<kvm_init_shadow_ept_mmu>> context->sync_spte = ept_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|5753| <<init_kvm_nested_mmu>> g_context->sync_spte = NULL;
+	 */
 	int (*sync_spte)(struct kvm_vcpu *vcpu,
 			 struct kvm_mmu_page *sp, int i);
 	struct kvm_mmu_root_info root;
@@ -473,6 +563,14 @@ struct kvm_mmu {
 	 * Byte index: page fault error code [4:1]
 	 * Bit index: pte permissions in ACC_* format
 	 */
+	/*
+	 * 在以下使用kvm_mmu->permissions[16] (u8)
+	 *   - arch/x86/include/asm/kvm_host.h|556| <<global>> u8 permissions[16];
+	 *   - arch/x86/kvm/mmu.h|246| <<permission_fault>> fault = (mmu->permissions[index] >> pte_access) & 1;
+	 *   - arch/x86/kvm/mmu/mmu.c|5332| <<update_permission_bitmask>> for (byte = 0; byte < ARRAY_SIZE(mmu->permissions); ++byte) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5387| <<update_permission_bitmask>> mmu->permissions[byte] = ff | uf | wf | smepf | smapf;
+	 *   - arch/x86/kvm/mmu/mmu.c|5427| <<update_pkru_bitmask>> for (bit = 0; bit < ARRAY_SIZE(mmu->permissions); ++bit) {
+	 */
 	u8 permissions[16];
 
 	u64 *pae_root;
@@ -748,6 +846,19 @@ struct kvm_vcpu_arch {
 	u32 regs_dirty;
 
 	unsigned long cr0;
+	/*
+	 * 在以下使用kvm_vcpu_arch->cr0_guest_owned_bits:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|154| <<kvm_read_cr0_bits>> if ((tmask & vcpu->arch.cr0_guest_owned_bits) &&
+	 *   - arch/x86/kvm/vmx/nested.c|2622| <<prepare_vmcs02>> vcpu->arch.cr0_guest_owned_bits &= ~vmcs12->cr0_guest_host_mask;
+	 *   - arch/x86/kvm/vmx/nested.c|2623| <<prepare_vmcs02>> vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu->arch.cr0_guest_owned_bits);
+	 *   - arch/x86/kvm/vmx/nested.c|3773| <<vmcs12_guest_cr0>>  (vmcs_readl(GUEST_CR0) & vcpu->arch.cr0_guest_owned_bits) |
+	 *   - arch/x86/kvm/vmx/nested.c|3776| <<vmcs12_guest_cr0>> vcpu->arch.cr0_guest_owned_bits));
+	 *   - arch/x86/kvm/vmx/nested.c|4553| <<load_vmcs12_host_state>> vcpu->arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+	 *   - arch/x86/kvm/vmx/nested.c|4708| <<nested_vmx_restore_host_state>> vcpu->arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+	 *   - arch/x86/kvm/vmx/vmx.c|2485| <<vmx_cache_reg>> guest_owned_bits = vcpu->arch.cr0_guest_owned_bits;
+	 *   - arch/x86/kvm/vmx/vmx.c|4786| <<init_vmcs>> vmx->vcpu.arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+	 *   - arch/x86/kvm/vmx/vmx.c|4787| <<init_vmcs>> vmcs_writel(CR0_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr0_guest_owned_bits);
+	 */
 	unsigned long cr0_guest_owned_bits;
 	unsigned long cr2;
 	unsigned long cr3;
@@ -784,6 +895,14 @@ struct kvm_vcpu_arch {
 	 * the paging mode of the l1 guest. This context is always used to
 	 * handle faults.
 	 */
+	/*
+	 * 在以下设置kvm_vcpu_arch->mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|6447| <<kvm_mmu_create>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|86| <<nested_svm_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|104| <<nested_svm_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|455| <<nested_ept_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|466| <<nested_ept_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 */
 	struct kvm_mmu *mmu;
 
 	/* Non-nested MMU for L1 */
@@ -800,12 +919,30 @@ struct kvm_vcpu_arch {
 	 * walking and not for faulting since we never handle l2 page faults on
 	 * the host.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->nested_mmu:
+	 *   - arch/x86/kvm/mmu.h|376| <<kvm_translate_gpa>> if (mmu != &vcpu->arch.nested_mmu)
+	 *   - arch/x86/kvm/mmu/mmu.c|5739| <<init_kvm_nested_mmu>> struct kvm_mmu *g_context = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|5817| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|5820| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.cpu_role.ext.valid = 0;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|461| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/x86.h|185| <<mmu_is_nested>> return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;
+	 */
 	struct kvm_mmu nested_mmu;
 
 	/*
 	 * Pointer to the mmu context currently used for
 	 * gva_to_gpa translations.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->walk_mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|6448| <<kvm_mmu_create>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|105| <<nested_svm_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|461| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|467| <<nested_ept_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 */
 	struct kvm_mmu *walk_mmu;
 
 	struct kvm_mmu_memory_cache mmu_pte_list_desc_cache;
@@ -944,6 +1081,12 @@ struct kvm_vcpu_arch {
 
 	/* Cache MMIO info */
 	u64 mmio_gva;
+	/*
+	 * 在以下使用mmio_access:
+	 *   - arch/x86/kvm/x86.c|7739| <<vcpu_mmio_gva_to_gpa>> if (vcpu_match_mmio_gva(vcpu, gva) &&
+	 *                                         (!is_paging(vcpu) || !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access))) {
+	 *   - arch/x86/kvm/x86.h|244| <<vcpu_cache_mmio_info>> vcpu->arch.mmio_access = access;
+	 */
 	unsigned mmio_access;
 	gfn_t mmio_gfn;
 	u64 mmio_gen;
@@ -1045,11 +1188,43 @@ struct kvm_vcpu_arch {
 };
 
 struct kvm_lpage_info {
+	/*
+	 * 在以下使用kvm_lpage_info->disallow_lpage:
+	 *   - arch/x86/kvm/mmu/mmu.c|811| <<update_gfn_disallow_lpage_count>> old = linfo->disallow_lpage;
+	 *   - arch/x86/kvm/mmu/mmu.c|812| <<update_gfn_disallow_lpage_count>> linfo->disallow_lpage += count;
+	 *   - arch/x86/kvm/mmu/mmu.c|813| <<update_gfn_disallow_lpage_count>> WARN_ON_ONCE((old ^ linfo->disallow_lpage) & KVM_LPAGE_MIXED_FLAG);
+	 *   - arch/x86/kvm/mmu/mmu.c|3156| <<__kvm_mmu_max_mapping_level>> if (!linfo->disallow_lpage)
+	 *   - arch/x86/kvm/mmu/mmu.c|7372| <<hugepage_test_mixed>> return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/mmu/mmu.c|7378| <<hugepage_clear_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/mmu/mmu.c|7384| <<hugepage_set_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/x86.c|12807| <<kvm_alloc_memslot_metadata>> linfo[0].disallow_lpage = 1;
+	 *   - arch/x86/kvm/x86.c|12809| <<kvm_alloc_memslot_metadata>> linfo[lpages - 1].disallow_lpage = 1;
+	 *   - arch/x86/kvm/x86.c|12819| <<kvm_alloc_memslot_metadata>> linfo[j].disallow_lpage = 1;
+	 *
+	 *  The kvm_lpage_info denotes whether a specific huge
+	 *  page (GFN and page size) on the memslot is supported.
+	 *
+	 *  Preventing huge pages from spanning adjacent memslot is covered by
+	 *  incrementing the count in head and tail kvm_lpage_info when the
+	 *  memslot is allocated, but disallowing huge pages for memory that has
+	 *  mixed attributes has to be done in a more complicated way.
+	 */
 	int disallow_lpage;
 };
 
 struct kvm_arch_memory_slot {
 	struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
+	/*
+	 * 在以下使用kvm_arch_memory_slot->lpage_info[]:
+	 *   - arch/x86/kvm/mmu/mmu.c|791| <<lpage_info_slot>> return &slot->arch.lpage_info[level - 2][idx];
+	 *   - arch/x86/kvm/x86.c|12744| <<kvm_arch_free_memslot>> kvfree(slot->arch.lpage_info[i - 1]);
+	 *   - arch/x86/kvm/x86.c|12745| <<kvm_arch_free_memslot>> slot->arch.lpage_info[i - 1] = NULL;
+	 *   - arch/x86/kvm/x86.c|12804| <<kvm_alloc_memslot_metadata>> slot->arch.lpage_info[i - 1] = linfo;
+	 *   - arch/x86/kvm/x86.c|12836| <<kvm_alloc_memslot_metadata>> kvfree(slot->arch.lpage_info[i - 1]);
+	 *   - arch/x86/kvm/x86.c|12837| <<kvm_alloc_memslot_metadata>> slot->arch.lpage_info[i - 1] = NULL;
+	 *
+	 * 2个数组: 2M和1G?
+	 */
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
 	unsigned short *gfn_write_track;
 };
@@ -1313,6 +1488,13 @@ struct kvm_arch {
 	struct iommu_domain *iommu_domain;
 	bool iommu_noncoherent;
 #define __KVM_HAVE_ARCH_NONCOHERENT_DMA
+	/*
+	 * 在以下使用kvm_arch->noncoherent_dma_count:
+	 *   - arch/x86/kvm/x86.c|12540| <<kvm_arch_init_vm>> atomic_set(&kvm->arch.noncoherent_dma_count, 0);
+	 *   - arch/x86/kvm/x86.c|13446| <<kvm_arch_register_noncoherent_dma>> if (atomic_inc_return(&kvm->arch.noncoherent_dma_count) == 1)
+	 *   - arch/x86/kvm/x86.c|13453| <<kvm_arch_unregister_noncoherent_dma>> if (!atomic_dec_return(&kvm->arch.noncoherent_dma_count))
+	 *   - arch/x86/kvm/x86.c|13460| <<kvm_arch_has_noncoherent_dma>> return atomic_read(&kvm->arch.noncoherent_dma_count);
+	 */
 	atomic_t noncoherent_dma_count;
 #define __KVM_HAVE_ARCH_ASSIGNED_DEVICE
 	atomic_t assigned_device_count;
@@ -1534,6 +1716,12 @@ struct kvm_vcpu_stat {
 	u64 pf_emulate;
 	u64 pf_spurious;
 	u64 pf_fast;
+	/*
+	 * 在以下使用kvm_vcpu_stat->pf_mmio_spte_created:
+	 *   - arch/x86/kvm/x86.c|284| <<global>> STATS_DESC_COUNTER(VCPU, pf_mmio_spte_created),
+	 *   - arch/x86/kvm/mmu/mmu.c|2946| <<mmu_set_spte>> vcpu->stat.pf_mmio_spte_created++;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1088| <<tdp_mmu_map_handle_target_level>> vcpu->stat.pf_mmio_spte_created++
+	 */
 	u64 pf_mmio_spte_created;
 	u64 pf_guest;
 	u64 tlb_flush;
diff --git a/arch/x86/include/uapi/asm/mtrr.h b/arch/x86/include/uapi/asm/mtrr.h
index 3a8a8eb8a..73f1d91d9 100644
--- a/arch/x86/include/uapi/asm/mtrr.h
+++ b/arch/x86/include/uapi/asm/mtrr.h
@@ -81,6 +81,19 @@ typedef __u8 mtrr_type;
 #define MTRR_NUM_FIXED_RANGES 88
 #define MTRR_MAX_VAR_RANGES 256
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|556| <<get_mtrr_var_range>> rdmsr(MTRRphysBase_MSR(index), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|825| <<generic_get_mtrr>> rdmsr(MTRRphysBase_MSR(reg), base_lo, base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|886| <<set_mtrr_var_ranges>> rdmsr(MTRRphysBase_MSR(index), lo, hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|890| <<set_mtrr_var_ranges>> mtrr_wrmsr(MTRRphysBase_MSR(index), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|1008| <<generic_set_mtrr>> mtrr_wrmsr(MTRRphysBase_MSR(reg), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kvm/mtrr.c|74| <<var_mtrr_msr_to_range>> int index = (msr - MTRRphysBase_MSR(0)) / 2;
+ *   - arch/x86/kvm/mtrr.c|96| <<msr_mtrr_valid>> case MTRRphysBase_MSR(0) ... MTRRphysMask_MSR(KVM_NR_VAR_MTRR - 1):
+ *   - arch/x86/kvm/mtrr.c|169| <<kvm_mtrr_valid>> WARN_ON(!(msr >= MTRRphysBase_MSR(0) &&
+ *   - arch/x86/kvm/x86.c|3851| <<kvm_set_msr_common>> case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ *   - arch/x86/kvm/x86.c|4266| <<kvm_get_msr_common>> case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ */
 #define MTRRphysBase_MSR(reg) (0x200 + 2 * (reg))
 #define MTRRphysMask_MSR(reg) (0x200 + 2 * (reg) + 1)
 
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index adba49afb..c41f59a54 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -325,6 +325,10 @@ static bool kvm_cpuid_has_hyperv(struct kvm_cpuid_entry2 *entries, int nent)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|464| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -405,11 +409,20 @@ int cpuid_query_maxphyaddr(struct kvm_vcpu *vcpu)
  * encryption technologies that usurp bits.  The raw mask should be used if and
  * only if hardware does _not_ strip the usurped bits, e.g. in virtual MTRRs.
  */
+/*
+ * 为gpa保留的地址部分生成mask, 从不支持的最高bit到63:
+ * rsvd_bits(cpuid_maxphyaddr(vcpu), 63)
+ */
 u64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)
 {
 	return rsvd_bits(cpuid_maxphyaddr(vcpu), 63);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|505| <<kvm_vcpu_ioctl_set_cpuid>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ *   - arch/x86/kvm/cpuid.c|531| <<kvm_vcpu_ioctl_set_cpuid2>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ */
 static int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,
                         int nent)
 {
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 60f21bb4c..26b5d8070 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -13,6 +13,11 @@ extern bool __read_mostly enable_mmio_caching;
 
 #define PT_PRESENT_MASK (1ULL << 0)
 #define PT_WRITABLE_MASK (1ULL << PT_WRITABLE_SHIFT)
+/*
+ * 注释bit=2(U/S):
+ * User/supervisor; if 0, user-mode accesses are not allowed to the 4-KByte
+ * page referenced by this entry (see Section 4.6)
+ */
 #define PT_USER_MASK (1ULL << PT_USER_SHIFT)
 #define PT_PWT_MASK (1ULL << 3)
 #define PT_PCD_MASK (1ULL << 4)
@@ -28,9 +33,21 @@ extern bool __read_mostly enable_mmio_caching;
 #define PT64_NX_MASK (1ULL << PT64_NX_SHIFT)
 
 #define PT_PAT_SHIFT 7
+/*
+ * 下面两个没见到使用
+ */
 #define PT_DIR_PAT_SHIFT 12
 #define PT_DIR_PAT_MASK (1ULL << PT_DIR_PAT_SHIFT)
 
+/*
+ * 在以下使用PT64_ROOT_5LEVEL:
+ *   - arch/x86/kvm/mmu/mmu.c|4001| <<mmu_alloc_shadow_roots>> if (mmu->root_role.level == PT64_ROOT_5LEVEL) {
+ *   - arch/x86/kvm/mmu/mmu.c|4033| <<mmu_alloc_shadow_roots>> if (mmu->root_role.level == PT64_ROOT_5LEVEL)
+ *   - arch/x86/kvm/mmu/mmu.c|5111| <<__reset_rsvds_bits_mask>> case PT64_ROOT_5LEVEL:
+ *   - arch/x86/kvm/mmu/mmu.c|5526| <<kvm_calc_cpu_role>> role.base.level = ____is_cr4_la57(regs) ? PT64_ROOT_5LEVEL
+ *   - arch/x86/kvm/mmu/mmu.c|5721| <<kvm_init_shadow_npt_mmu>> if (root_role.level == PT64_ROOT_5LEVEL &&
+ *   - arch/x86/kvm/svm/svm.c|290| <<get_npt_level>> return pgtable_l5_enabled() ? PT64_ROOT_5LEVEL : PT64_ROOT_4LEVEL;
+ */
 #define PT64_ROOT_5LEVEL 5
 #define PT64_ROOT_4LEVEL 4
 #define PT32_ROOT_LEVEL 2
@@ -81,8 +98,22 @@ static inline gfn_t kvm_mmu_max_gfn(void)
 	return (1ULL << (max_gpa_bits - PAGE_SHIFT)) - 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|916| <<kvm_mmu_reset_all_pte_masks>> shadow_phys_bits = kvm_get_shadow_phys_bits();
+ *   - arch/x86/kvm/vmx/vmx.c|8482| <<vmx_setup_me_spte_mask>> if (boot_cpu_data.x86_phys_bits != kvm_get_shadow_phys_bits())
+ *   - arch/x86/kvm/vmx/vmx.c|8484| <<vmx_setup_me_spte_mask>> kvm_get_shadow_phys_bits() - 1);
+ */
 static inline u8 kvm_get_shadow_phys_bits(void)
 {
+	/*
+	 * $ cpuid -1 -l 0x80000008
+	 * CPU:
+	 *    Physical Address and Linear Address Size (0x80000008/eax):
+	 *       maximum physical address bits         = 0x2e (46)
+	 *       maximum linear (virtual) address bits = 0x30 (48)
+	 *       maximum guest physical address bits   = 0x0 (0)
+	 */
 	/*
 	 * boot_cpu_data.x86_phys_bits is reduced when MKTME or SME are detected
 	 * in CPU detection code, but the processor treats those reduced bits as
@@ -165,6 +196,22 @@ static inline void kvm_mmu_load_pgd(struct kvm_vcpu *vcpu)
 					  vcpu->arch.mmu->root_role.level);
 }
 
+/*
+ * 关于cr0/cr4的passthrough:
+ * VM-execution control fields include guest/host masks and read shadows for the CR0 and CR4 registers. These
+ * fields control executions of instructions that access those registers (including CLTS, LMSW, MOV CR, and SMSW).
+ * They are 64 bits on processors that support Intel 64 architecture and 32 bits on processors that do not.
+ *
+ * In general, bits set to 1 in a guest/host mask correspond to bits "owned" by the host:
+ *
+ * 1. Guest attempts to set them (using CLTS, LMSW, or MOV to CR) to values differing from the corresponding bits
+ * in the corresponding read shadow cause VM exits.
+ *
+ * 2. Guest reads (using MOV from CR or SMSW) return values for these bits from the corresponding read shadow.
+ *
+ * Bits cleared to 0 correspond to bits "owned" by the guest; guest
+ * attempts to modify them succeed and guest reads return values for these bits from the control register itself.
+ */
 static inline void kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,
 						    struct kvm_mmu *mmu)
 {
@@ -191,6 +238,36 @@ static inline void kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,
  * Return zero if the access does not fault; return the page fault error code
  * if the access faults.
  */
+/*
+ * From 97d64b788114be1c4dc4bfe7a8ba2bf9643fe6af Mon Sep 17 00:00:00 2001
+ * From: Avi Kivity <avi@redhat.com>
+ * Date: Wed, 12 Sep 2012 14:52:00 +0300
+ * Subject: [PATCH 1/1] KVM: MMU: Optimize pte permission checks
+ *
+ * walk_addr_generic() permission checks are a maze of branchy code, which is
+ * performed four times per lookup.  It depends on the type of access, efer.nxe,
+ * cr0.wp, cr4.smep, and in the near future, cr4.smap.
+ *
+ * Optimize this away by precalculating all variants and storing them in a
+ * bitmap.  The bitmap is recalculated when rarely-changing variables change
+ * (cr0, cr4) and is indexed by the often-changing variables (page fault error
+ * code, pte access permissions).
+ *
+ * The permission check is moved to the end of the loop, otherwise an SMEP
+ * fault could be reported as a false positive, when PDE.U=1 but PTE.U=0.]
+ * Noted by Xiao Guangrong.
+ *
+ * The result is short, branch-free code.
+ *
+ * Reviewed-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
+ * Signed-off-by: Avi Kivity <avi@redhat.com>
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|436| <<FNAME(walk_addr_generic)>> errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
+ *   - arch/x86/kvm/x86.c|7758| <<vcpu_mmio_gva_to_gpa>> !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access))) {
+ *
+ * 我们应该是希望permission_fault()返回0
+ */
 static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 				  unsigned pte_access, unsigned pte_pkey,
 				  u64 access)
@@ -219,6 +296,20 @@ static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 
 	kvm_mmu_refresh_passthrough_bits(vcpu, mmu);
 
+	/*
+	 * 在以下使用kvm_mmu->permissions[16] (u8)
+	 *   - arch/x86/include/asm/kvm_host.h|556| <<global>> u8 permissions[16];
+	 *   - arch/x86/kvm/mmu.h|246| <<permission_fault>> fault = (mmu->permissions[index] >> pte_access) & 1;
+	 *   - arch/x86/kvm/mmu/mmu.c|5332| <<update_permission_bitmask>> for (byte = 0; byte < ARRAY_SIZE(mmu->permissions); ++byte) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5387| <<update_permission_bitmask>> mmu->permissions[byte] = ff | uf | wf | smepf | smapf;
+	 *   - arch/x86/kvm/mmu/mmu.c|5427| <<update_pkru_bitmask>> for (bit = 0; bit < ARRAY_SIZE(mmu->permissions); ++bit) {
+	 *
+	 * crash 6.6的例子:
+	 * crash> u8 ff11000191a40310 16
+	 * ff11000191a40310:  f0 f3 0f 3f 00 33 0f 3f f5 f7 5f 7f f5 f7 5f 7f
+	 *
+	 * 猜测, pte_access可能是: 000, 010, 100, 110 = 0, 2, 4, 6
+	 */
 	fault = (mmu->permissions[index] >> pte_access) & 1;
 
 	WARN_ON(pfec & (PFERR_PK_MASK | PFERR_RSVD_MASK));
@@ -247,6 +338,12 @@ static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 
 bool __kvm_mmu_honors_guest_mtrrs(bool vm_has_noncoherent_dma);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4630| <<kvm_tdp_page_fault>> if (kvm_mmu_honors_guest_mtrrs(vcpu->kvm)) {
+ *   - arch/x86/kvm/mtrr.c|499| <<update_mtrr>> if (!kvm_mmu_honors_guest_mtrrs(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|965| <<kvm_post_set_cr0>> kvm_mmu_honors_guest_mtrrs(vcpu->kvm) &&
+ */
 static inline bool kvm_mmu_honors_guest_mtrrs(struct kvm *kvm)
 {
 	return __kvm_mmu_honors_guest_mtrrs(kvm_arch_has_noncoherent_dma(kvm));
@@ -259,6 +356,13 @@ int kvm_arch_write_log_dirty(struct kvm_vcpu *vcpu);
 int kvm_mmu_post_init_vm(struct kvm *kvm);
 void kvm_mmu_pre_destroy_vm(struct kvm *kvm);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|347| <<kvm_memslots_have_rmaps>> return !tdp_mmu_enabled || kvm_shadow_root_allocated(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|3879| <<mmu_first_shadow_root_alloc>> if (kvm_shadow_root_allocated(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|3885| <<mmu_first_shadow_root_alloc>> if (kvm_shadow_root_allocated(kvm))
+ *   - arch/x86/kvm/mmu/page_track.c|26| <<kvm_page_track_write_tracking_enabled>> !tdp_enabled || kvm_shadow_root_allocated(kvm);
+ */
 static inline bool kvm_shadow_root_allocated(struct kvm *kvm)
 {
 	/*
@@ -276,26 +380,88 @@ extern bool tdp_mmu_enabled;
 #define tdp_mmu_enabled false
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|100| <<kvm_mmu_rmaps_stat_show>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1378| <<kvm_mmu_write_protect_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1411| <<kvm_mmu_clear_dirty_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1497| <<kvm_mmu_slot_gfn_write_protect>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|1672| <<kvm_unmap_gfn_range>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1689| <<kvm_set_spte_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1763| <<kvm_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1776| <<kvm_test_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|3892| <<mmu_first_shadow_root_alloc>> if (kvm_memslots_have_rmaps(kvm) &&
+ *   - arch/x86/kvm/mmu/mmu.c|6820| <<kvm_rmap_zap_gfn_range>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|6890| <<kvm_mmu_slot_remove_write_access>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7142| <<kvm_mmu_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|7163| <<kvm_mmu_slot_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7236| <<kvm_mmu_zap_collapsible_sptes>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7252| <<kvm_mmu_slot_leaf_clear_dirty>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/x86.c|12902| <<kvm_alloc_memslot_metadata>> if (kvm_memslots_have_rmaps(kvm)) {
+ */
 static inline bool kvm_memslots_have_rmaps(struct kvm *kvm)
 {
 	return !tdp_mmu_enabled || kvm_shadow_root_allocated(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|323| <<__kvm_mmu_slot_lpages>> return gfn_to_index(slot->base_gfn + npages - 1,
+ *   - arch/x86/kvm/mmu/mmu.c|790| <<lpage_info_slot>> idx = gfn_to_index(gfn, slot->base_gfn, level);
+ *   - arch/x86/kvm/mmu/mmu.c|1091| <<gfn_to_rmap>> idx = gfn_to_index(gfn, slot->base_gfn, level);
+ *   - arch/x86/kvm/mmu/page_track.c|67| <<update_gfn_write_track>> index = gfn_to_index(gfn, slot->base_gfn, PG_LEVEL_4K);
+ *   - arch/x86/kvm/mmu/page_track.c|134| <<kvm_gfn_is_write_tracked>> index = gfn_to_index(gfn, slot->base_gfn, PG_LEVEL_4K);
+ */
 static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 {
+	/*
+	 * KVM_HPAGE_GFN_SHIFT(1) : (((1) - 1) * 9) =  0
+	 * KVM_HPAGE_GFN_SHIFT(2) : (((2) - 1) * 9) =  9 ---> level 2
+	 * KVM_HPAGE_GFN_SHIFT(3) : (((3) - 1) * 9) = 18 ---> level 3
+	 * KVM_HPAGE_GFN_SHIFT(4) : (((4) - 1) * 9) = 27
+	 * KVM_HPAGE_GFN_SHIFT(5) : (((5) - 1) * 9) = 36
+	 *
+	 * 假设:
+	 * base_gfn: 133693442
+	 * gfn     : 134060443
+	 *
+	 * 对于level 2:
+	 * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 9       = 261836
+	 * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 9 = 261120
+	 * 261836 - 261120 = 716
+	 *
+	 * 对于level 3:
+	 * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 18       = 511
+	 * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 18 = 510
+	 * 511 - 510 = 0
+	 * 
+	 */
 	/* KVM_HPAGE_GFN_SHIFT(PG_LEVEL_4K) must be 0. */
 	return (gfn >> KVM_HPAGE_GFN_SHIFT(level)) -
 		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|401| <<kvm_mmu_slot_lpages>> return __kvm_mmu_slot_lpages(slot, slot->npages, level);
+ *   - arch/x86/kvm/x86.c|12849| <<memslot_rmap_alloc>> int lpages = __kvm_mmu_slot_lpages(slot, npages, level);
+ *   - arch/x86/kvm/x86.c|12945| <<kvm_alloc_memslot_metadata>> lpages = __kvm_mmu_slot_lpages(slot, npages, level);
+ */
 static inline unsigned long
 __kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, unsigned long npages,
 		      int level)
 {
+	/*
+	 * 1048576 + 1024 -1 = 1049599
+	 */
 	return gfn_to_index(slot->base_gfn + npages - 1,
 			    slot->base_gfn, level) + 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|121| <<kvm_mmu_rmaps_stat_show>> lpage_size = kvm_mmu_slot_lpages(slot, k + 1);
+ */
 static inline unsigned long
 kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, int level)
 {
@@ -310,11 +476,35 @@ static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u64 access,
 			   struct x86_exception *exception);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4215| <<nonpaging_gva_to_gpa>> return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|442| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn), nested_access, &walker->fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|553| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(gfn), access, &walker->fault);
+ *   - arch/x86/kvm/x86.c|886| <<load_pdptrs>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn), PFERR_USER_MASK | PFERR_WRITE_MASK, NULL);
+ */
 static inline gpa_t kvm_translate_gpa(struct kvm_vcpu *vcpu,
 				      struct kvm_mmu *mmu,
 				      gpa_t gpa, u64 access,
 				      struct x86_exception *exception)
 {
+	/*
+	 * Paging state of an L2 guest (used for nested npt)
+	 *
+	 * This context will save all necessary information to walk page tables
+	 * of an L2 guest. This context is only initialized for page table
+	 * walking and not for faulting since we never handle l2 page faults on
+	 * the host.
+	 *
+	 * 在以下使用kvm_vcpu_arch->nested_mmu:
+	 *   - arch/x86/kvm/mmu.h|376| <<kvm_translate_gpa>> if (mmu != &vcpu->arch.nested_mmu)
+	 *   - arch/x86/kvm/mmu/mmu.c|5739| <<init_kvm_nested_mmu>> struct kvm_mmu *g_context = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|5817| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|5820| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.cpu_role.ext.valid = 0;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|461| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/x86.h|185| <<mmu_is_nested>> return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;
+	 */
 	if (mmu != &vcpu->arch.nested_mmu)
 		return gpa;
 	return translate_nested_gpa(vcpu, gpa, access, exception);
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 0544700ca..87f04bf0d 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -289,6 +289,12 @@ static void kvm_flush_remote_tlbs_sptep(struct kvm *kvm, u64 *sptep)
 	kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2947| <<mmu_set_spte>> mark_mmio_spte(vcpu, sptep, gfn, pte_access);
+ *   - arch/x86/kvm/mmu/mmu.c|4870| <<sync_mmio_spte>> mark_mmio_spte(vcpu, sptep, gfn, access);
+ *   - arch/x86/kvm/mmu/mmutrace.h|211| <<KVM_MMU_PAGE_ASSIGN>> mark_mmio_spte,
+ */
 static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 			   unsigned int access)
 {
@@ -298,8 +304,23 @@ static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 	mmu_spte_set(sptep, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4292| <<handle_mmio_page_fault>> gfn_t gfn = get_mmio_spte_gfn(spte);
+ *   - arch/x86/kvm/mmu/mmu.c|4964| <<sync_mmio_spte>> if (gfn != get_mmio_spte_gfn(*sptep)) {
+ */
 static gfn_t get_mmio_spte_gfn(u64 spte)
 {
+	/*
+	 * 测试的例子 (不支持ME):
+	 * boot_cpu_data.x86_phys_bits = 46
+	 * boot_cpu_data.x86_cache_bits = 46
+	 * shadow_phys_bits=46
+	 * bit: 41 - 45 (5个bit)
+	 * shadow_nonpresent_or_rsvd_mask=0x00003e0000000000
+	 * bit: 12 - 40
+	 * shadow_nonpresent_or_rsvd_lower_gfn_mask=0x000001fffffff000
+	 */
 	u64 gpa = spte & shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
 	gpa |= (spte >> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)
@@ -308,8 +329,23 @@ static gfn_t get_mmio_spte_gfn(u64 spte)
 	return gpa >> PAGE_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4308| <<handle_mmio_page_fault>> unsigned int access = get_mmio_spte_access(spte);
+ */
 static unsigned get_mmio_spte_access(u64 spte)
 {
+	/*
+	 * 在以下设置shadow_mmio_access_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|446| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_access_mask = access_mask;
+	 * 在以下使用shadow_mmio_access_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|313| <<get_mmio_spte_access>> return spte & shadow_mmio_access_mask;
+	 *   - arch/x86/kvm/mmu/mmu.c|3340| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+	 *   - arch/x86/kvm/mmu/spte.c|106| <<make_mmio_spte>> access &= shadow_mmio_access_mask;
+	 *
+	 * 比如: ept是0
+	 * regular pte是ACC_WRITE_MASK | ACC_USER_MASK
+	 */
 	return spte & shadow_mmio_access_mask;
 }
 
@@ -782,6 +818,14 @@ static void kvm_mmu_page_set_access(struct kvm_mmu_page *sp, int index,
  * Return the pointer to the large page information for a given gfn,
  * handling slots that are not large page aligned.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|809| <<update_gfn_disallow_lpage_count>> linfo = lpage_info_slot(gfn, slot, i);
+ *   - arch/x86/kvm/mmu/mmu.c|3155| <<__kvm_mmu_max_mapping_level>> linfo = lpage_info_slot(gfn, slot, max_level);
+ *   - arch/x86/kvm/mmu/mmu.c|7376| <<hugepage_test_mixed>> return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7382| <<hugepage_clear_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7394| <<hugepage_set_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
+ */
 static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
 		const struct kvm_memory_slot *slot, int level)
 {
@@ -797,6 +841,13 @@ static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
  * The lower order bits are used to refcount other cases where a hugepage is
  * disallowed, e.g. if KVM has shadow a page table at the gfn.
  */
+/*
+ * 在以下使用KVM_LPAGE_MIXED_FLAG:
+ *   - arch/x86/kvm/mmu/mmu.c|821| <<update_gfn_disallow_lpage_count>> WARN_ON_ONCE((old ^ linfo->disallow_lpage) & KVM_LPAGE_MIXED_FLAG);
+ *   - arch/x86/kvm/mmu/mmu.c|7384| <<hugepage_test_mixed>> return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7390| <<hugepage_clear_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7402| <<hugepage_set_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
+ */
 #define KVM_LPAGE_MIXED_FLAG	BIT(31)
 
 static void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,
@@ -1226,6 +1277,10 @@ static void drop_large_spte(struct kvm *kvm, u64 *sptep, bool flush)
  *
  * Return true if tlb need be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1273| <<rmap_write_protect>> flush |= spte_write_protect(sptep, pt_protect);
+ */
 static bool spte_write_protect(u64 *sptep, bool pt_protect)
 {
 	u64 spte = *sptep;
@@ -1241,6 +1296,12 @@ static bool spte_write_protect(u64 *sptep, bool pt_protect)
 	return mmu_spte_update(sptep, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1344| <<kvm_mmu_write_protect_pt_masked>> rmap_write_protect(rmap_head, false);
+ *   - arch/x86/kvm/mmu/mmu.c|1446| <<kvm_mmu_slot_gfn_write_protect>> write_protected |= rmap_write_protect(rmap_head, true);
+ *   - arch/x86/kvm/mmu/mmu.c|6534| <<slot_rmap_write_protect>> return rmap_write_protect(rmap_head, false);
+ */
 static bool rmap_write_protect(struct kvm_rmap_head *rmap_head,
 			       bool pt_protect)
 {
@@ -1370,6 +1431,12 @@ static void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
  * We need to care about huge page mappings: e.g. during dirty logging we may
  * have such mappings.
  */
+/*
+ * called by:
+ *   - virt/kvm/dirty_ring.c|70| <<kvm_reset_dirty_gfn>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ *   - virt/kvm/kvm_main.c|2307| <<kvm_get_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ *   - virt/kvm/kvm_main.c|2424| <<kvm_clear_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ */
 void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 				struct kvm_memory_slot *slot,
 				gfn_t gfn_offset, unsigned long mask)
@@ -1411,6 +1478,14 @@ int kvm_cpu_dirty_log_size(void)
 	return kvm_x86_ops.cpu_dirty_log_size;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|865| <<account_shadowed>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+ *   - arch/x86/kvm/mmu/mmu.c|1414| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, start, PG_LEVEL_2M);
+ *   - arch/x86/kvm/mmu/mmu.c|1419| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, end, PG_LEVEL_2M);
+ *   - arch/x86/kvm/mmu/mmu.c|1462| <<kvm_vcpu_write_protect_gfn>> return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn, PG_LEVEL_4K);
+ *   - arch/x86/kvm/mmu/page_track.c|96| <<__kvm_write_track_add_gfn>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+ */
 bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 				    struct kvm_memory_slot *slot, u64 gfn,
 				    int min_level)
@@ -1433,6 +1508,10 @@ bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 	return write_protected;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2135| <<mmu_sync_children>> protected |= kvm_vcpu_write_protect_gfn(vcpu, sp->gfn);
+ */
 static bool kvm_vcpu_write_protect_gfn(struct kvm_vcpu *vcpu, u64 gfn)
 {
 	struct kvm_memory_slot *slot;
@@ -1564,6 +1643,13 @@ typedef bool (*rmap_handler_t)(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 			       struct kvm_memory_slot *slot, gfn_t gfn,
 			       int level, pte_t pte);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1602| <<kvm_unmap_gfn_range>> flush = kvm_handle_gfn_range(kvm, range, kvm_zap_rmap);
+ *   - arch/x86/kvm/mmu/mmu.c|1619| <<kvm_set_spte_gfn>> flush = kvm_handle_gfn_range(kvm, range, kvm_set_pte_rmap);
+ *   - arch/x86/kvm/mmu/mmu.c|1693| <<kvm_age_gfn>> young = kvm_handle_gfn_range(kvm, range, kvm_age_rmap);
+ *   - arch/x86/kvm/mmu/mmu.c|1706| <<kvm_test_age_gfn>> young = kvm_handle_gfn_range(kvm, range, kvm_test_age_rmap);
+ */
 static __always_inline bool kvm_handle_gfn_range(struct kvm *kvm,
 						 struct kvm_gfn_range *range,
 						 rmap_handler_t handler)
@@ -2090,6 +2176,12 @@ static void mmu_pages_clear_parents(struct mmu_page_path *parents)
 	} while (!sp->unsync_children);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4094| <<kvm_mmu_sync_roots>> mmu_sync_children(vcpu, sp, true);
+ *   - arch/x86/kvm/mmu/mmu.c|4106| <<kvm_mmu_sync_roots>> mmu_sync_children(vcpu, sp, true);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|695| <<FNAME(fetch)>> mmu_sync_children(vcpu, sp, false))
+ */
 static int mmu_sync_children(struct kvm_vcpu *vcpu,
 			     struct kvm_mmu_page *parent, bool can_yield)
 {
@@ -2426,6 +2518,11 @@ static void shadow_walk_next(struct kvm_shadow_walk_iterator *iterator)
 	__shadow_walk_next(iterator, *iterator->sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2529| <<link_shadow_page>> __link_shadow_page(vcpu->kvm, &vcpu->arch.mmu_pte_list_desc_cache, sptep, sp, true);
+ *   - arch/x86/kvm/mmu/mmu.c|6712| <<shadow_mmu_split_huge_page>> __link_shadow_page(kvm, cache, huge_sptep, sp, flush);
+ */
 static void __link_shadow_page(struct kvm *kvm,
 			       struct kvm_mmu_memory_cache *cache, u64 *sptep,
 			       struct kvm_mmu_page *sp, bool flush)
@@ -2461,6 +2558,12 @@ static void __link_shadow_page(struct kvm *kvm,
 		mark_unsync(sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3339| <<direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+  3 arch/x86/kvm/mmu/paging_tmpl.h|707| <<FNAME>> link_shadow_page(vcpu, it.sptep, sp);
+  4 arch/x86/kvm/mmu/paging_tmpl.h|742| <<FNAME>> link_shadow_page(vcpu, it.sptep, sp);
+ */
 static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 			     struct kvm_mmu_page *sp)
 {
@@ -2802,6 +2905,10 @@ static void kvm_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)
  * were marked unsync (or if there is no shadow page), -EPERM if the SPTE must
  * be write-protected.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|688| <<make_spte>> if (mmu_try_to_unsync_pages(vcpu->kvm, slot, gfn, can_unsync, prefetch)) {
+ */
 int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 			    gfn_t gfn, bool can_unsync, bool prefetch)
 {
@@ -2903,6 +3010,13 @@ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3019| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, slot, start, access, gfn, page_to_pfn(pages[i]), NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|3294| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL, base_gfn, fault->pfn, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|556| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, slot, spte, pte_access, gfn, pfn, NULL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|751| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access, base_gfn, fault->pfn, fault);
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			u64 *sptep, unsigned int pte_access, gfn_t gfn,
 			kvm_pfn_t pfn, struct kvm_page_fault *fault)
@@ -2915,6 +3029,9 @@ static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 	bool wrprot;
 	u64 spte;
 
+	/*
+	 * 如果没kvm_page_fault肯定是host writable
+	 */
 	/* Prefetching always gets a writable pfn.  */
 	bool host_writable = !fault || fault->map_writable;
 	bool prefetch = !fault || fault->prefetch;
@@ -3234,6 +3351,10 @@ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_
 	}
 }
 
+/*
+ * called by:
+ * - arch/x86/kvm/mmu/mmu.c|4579| <<direct_page_fault>> r = direct_map(vcpu, fault);
+ */
 static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_shadow_walk_iterator it;
@@ -3308,15 +3429,28 @@ static int kvm_handle_error_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fa
 	return -EFAULT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4463| <<kvm_faultin_pfn>> return kvm_handle_noslot_fault(vcpu, fault, access);
+ */
 static int kvm_handle_noslot_fault(struct kvm_vcpu *vcpu,
 				   struct kvm_page_fault *fault,
 				   unsigned int access)
 {
 	gva_t gva = fault->is_tdp ? 0 : fault->addr;
 
+	/*
+	 * 比如: ept是0
+	 * regular pte是ACC_WRITE_MASK | ACC_USER_MASK
+	 */
 	vcpu_cache_mmio_info(vcpu, gva, fault->gfn,
 			     access & shadow_mmio_access_mask);
 
+	/*
+	 * 在以下设置enable_mmio_caching:
+	 *   - arch/x86/kvm/mmu/spte.c|388| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = allow_mmio_caching;
+	 *   - arch/x86/kvm/mmu/spte.c|422| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = false;
+	 */
 	/*
 	 * If MMIO caching is disabled, emulate immediately without
 	 * touching the shadow page tables as attempting to install an
@@ -4121,6 +4255,10 @@ static int get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes, int *root_level
 	return leaf;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4321| <<handle_mmio_page_fault>> reserved = get_mmio_spte(vcpu, addr, &spte);
+ */
 /* return true if reserved bit(s) are detected on a valid, non-MMIO SPTE. */
 static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 {
@@ -4171,6 +4309,13 @@ static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 	return reserved;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5942| <<kvm_mmu_page_fault>> if (unlikely(error_code & PFERR_RSVD_MASK)) r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
+ *   - arch/x86/kvm/mmu/mmutrace.h|234| <<__field>> handle_mmio_page_fault,
+ *
+ * 处理mmio, 来自ept的misconfig或者regular的reserved fault
+ */
 static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 {
 	u64 spte;
@@ -4185,6 +4330,10 @@ static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 
 	if (is_mmio_spte(spte)) {
 		gfn_t gfn = get_mmio_spte_gfn(spte);
+		/*
+		 * 比如: ept是0
+		 * regular pte是ACC_WRITE_MASK | ACC_USER_MASK
+		 */
 		unsigned int access = get_mmio_spte_access(spte);
 
 		if (!check_mmio_spte(vcpu, spte))
@@ -4194,6 +4343,11 @@ static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 			addr = 0;
 
 		trace_handle_mmio_page_fault(addr, gfn, access);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3339| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+		 *   - arch/x86/kvm/mmu/mmu.c|4219| <<handle_mmio_page_fault>> vcpu_cache_mmio_info(vcpu, addr, gfn, access);
+		 */
 		vcpu_cache_mmio_info(vcpu, addr, gfn, access);
 		return RET_PF_EMULATE;
 	}
@@ -4397,6 +4551,12 @@ static int __kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	return RET_PF_CONTINUE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4532| <<direct_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|4612| <<kvm_tdp_mmu_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|815| <<FNAME(page_fault)>> r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
+ */
 static int kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 			   unsigned int access)
 {
@@ -4537,6 +4697,12 @@ static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
 	return direct_page_fault(vcpu, fault);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2045| <<pf_interception>> return kvm_handle_page_fault(vcpu, error_code, fault_address, static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+ *                                                                  svm->vmcb->control.insn_bytes : NULL, svm->vmcb->control.insn_len);
+ *   - arch/x86/kvm/vmx/vmx.c|5263| <<handle_exception_nmi>> return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
+ */
 int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 				u64 fault_address, char *insn, int insn_len)
 {
@@ -4551,6 +4717,12 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 
 	vcpu->arch.l1tf_flush_l1d = true;
 	if (!flags) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4686| <<kvm_handle_page_fault>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+		 *   - arch/x86/kvm/svm/svm.c|2058| <<npf_interception>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+		 *   - arch/x86/kvm/vmx/vmx.c|5765| <<handle_ept_violation>> trace_kvm_page_fault(vcpu, gpa, exit_qualification);
+		 */
 		trace_kvm_page_fault(vcpu, fault_address, error_code);
 
 		if (kvm_event_needs_reinjection(vcpu))
@@ -4606,6 +4778,11 @@ static int kvm_tdp_mmu_page_fault(struct kvm_vcpu *vcpu,
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|258| <<kvm_mmu_honors_guest_mtrrs>> return __kvm_mmu_honors_guest_mtrrs(kvm_arch_has_noncoherent_dma(kvm));
+ *   - arch/x86/kvm/x86.c|13431| <<kvm_noncoherent_dma_assignment_start_or_stop>> if (__kvm_mmu_honors_guest_mtrrs(true))
+ */
 bool __kvm_mmu_honors_guest_mtrrs(bool vm_has_noncoherent_dma)
 {
 	/*
@@ -4617,9 +4794,22 @@ bool __kvm_mmu_honors_guest_mtrrs(bool vm_has_noncoherent_dma)
 	 * Note, KVM may still ultimately ignore guest MTRRs for certain PFNs,
 	 * e.g. KVM will force UC memtype for host MMIO.
 	 */
+	/*
+	 * 在以下使用shadow_memtype_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|4625| <<__kvm_mmu_honors_guest_mtrrs>> return vm_has_noncoherent_dma && shadow_memtype_mask;
+	 *   - arch/x86/kvm/mmu/spte.c|192| <<make_spte>> if (shadow_memtype_mask)
+	 *   - arch/x86/kvm/mmu/spte.c|439| <<kvm_mmu_set_ept_masks>> shadow_memtype_mask = VMX_EPT_MT_MASK | VMX_EPT_IPAT_BIT;
+	 *   - arch/x86/kvm/mmu/spte.c|496| <<kvm_mmu_reset_all_pte_masks>> shadow_memtype_mask = 0;
+	 */
 	return vm_has_noncoherent_dma && shadow_memtype_mask;
 }
 
+/*
+ * 在以下使用kvm_tdp_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|5361| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ *   - arch/x86/kvm/mmu/mmu_internal.h|294| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> r = kvm_tdp_page_fault(vcpu, &fault);
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	/*
@@ -4628,11 +4818,33 @@ int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	 * across the entire mapping.
 	 */
 	if (kvm_mmu_honors_guest_mtrrs(vcpu->kvm)) {
+		/*
+		 * enum pg_level {
+		 *     PG_LEVEL_NONE,
+		 *     PG_LEVEL_4K,
+		 *     PG_LEVEL_2M,
+		 *     PG_LEVEL_1G,
+		 *     PG_LEVEL_512G,
+		 *     PG_LEVEL_NUM
+		 * };
+		 *
+		 * 这里会减少fault->max_level
+		 */
 		for ( ; fault->max_level > PG_LEVEL_4K; --fault->max_level) {
+			/*
+			 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+			 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+			 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+			 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+			 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+			 */
 			int page_num = KVM_PAGES_PER_HPAGE(fault->max_level);
 			gfn_t base = gfn_round_for_level(fault->gfn,
 							 fault->max_level);
 
+			/*
+			 * 返回true就是不做什么
+			 */
 			if (kvm_mtrr_check_gfn_range_consistency(vcpu, base, page_num))
 				break;
 		}
@@ -4646,6 +4858,10 @@ int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	return direct_page_fault(vcpu, fault);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5577| <<shadow_mmu_init_context>> nonpaging_init_context(context);
+ */
 static void nonpaging_init_context(struct kvm_mmu *context)
 {
 	context->page_fault = nonpaging_page_fault;
@@ -4825,6 +5041,12 @@ static bool sync_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,
 #include "paging_tmpl.h"
 #undef PTTYPE
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5073| <<reset_guest_rsvds_bits_mask>> __reset_rsvds_bits_mask(&context->guest_rsvd_check,
+ *   - arch/x86/kvm/mmu/mmu.c|5150| <<reset_shadow_zero_bits_mask>> __reset_rsvds_bits_mask(shadow_zero_check, reserved_hpa_bits(),
+ *   - arch/x86/kvm/mmu/mmu.c|5192| <<reset_tdp_shadow_zero_bits_mask>> __reset_rsvds_bits_mask(shadow_zero_check, reserved_hpa_bits(),
+ */
 static void __reset_rsvds_bits_mask(struct rsvd_bits_validate *rsvd_check,
 				    u64 pa_bits_rsvd, int level, bool nx,
 				    bool gbpages, bool pse, bool amd)
@@ -4914,6 +5136,10 @@ static void __reset_rsvds_bits_mask(struct rsvd_bits_validate *rsvd_check,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5378| <<reset_guest_paging_metadata>> reset_guest_rsvds_bits_mask(vcpu, mmu);
+ */
 static void reset_guest_rsvds_bits_mask(struct kvm_vcpu *vcpu,
 					struct kvm_mmu *context)
 {
@@ -4925,6 +5151,12 @@ static void reset_guest_rsvds_bits_mask(struct kvm_vcpu *vcpu,
 				guest_cpuid_is_amd_or_hygon(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5122| <<reset_rsvds_bits_mask_ept>> __reset_rsvds_bits_mask_ept(&context->guest_rsvd_check,
+ *   - arch/x86/kvm/mmu/mmu.c|5197| <<reset_tdp_shadow_zero_bits_mask>> __reset_rsvds_bits_mask_ept(shadow_zero_check,
+ *   - arch/x86/kvm/mmu/mmu.c|5217| <<reset_ept_shadow_zero_bits_mask>> __reset_rsvds_bits_mask_ept(&context->shadow_zero_check,
+ */
 static void __reset_rsvds_bits_mask_ept(struct rsvd_bits_validate *rsvd_check,
 					u64 pa_bits_rsvd, bool execonly,
 					int huge_page_level)
@@ -4963,6 +5195,10 @@ static void __reset_rsvds_bits_mask_ept(struct rsvd_bits_validate *rsvd_check,
 	rsvd_check->bad_mt_xwr = bad_mt_xwr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5632| <<kvm_init_shadow_ept_mmu>> reset_rsvds_bits_mask_ept(vcpu, context, execonly, huge_page_level);
+ */
 static void reset_rsvds_bits_mask_ept(struct kvm_vcpu *vcpu,
 		struct kvm_mmu *context, bool execonly, int huge_page_level)
 {
@@ -4981,6 +5217,10 @@ static inline u64 reserved_hpa_bits(void)
  * table in guest or amd nested guest, its mmu features completely
  * follow the features in guest.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5532| <<shadow_mmu_init_context>> reset_shadow_zero_bits_mask(vcpu, context);
+ */
 static void reset_shadow_zero_bits_mask(struct kvm_vcpu *vcpu,
 					struct kvm_mmu *context)
 {
@@ -5076,6 +5316,11 @@ reset_ept_shadow_zero_bits_mask(struct kvm_mmu *context, bool execonly)
 	 (7 & (access) ? 128 : 0))
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5473| <<reset_guest_paging_metadata>> update_permission_bitmask(mmu, false);
+ *   - arch/x86/kvm/mmu/mmu.c|5778| <<kvm_init_shadow_ept_mmu>> update_permission_bitmask(context, true);
+ */
 static void update_permission_bitmask(struct kvm_mmu *mmu, bool ept)
 {
 	unsigned byte;
@@ -5089,7 +5334,52 @@ static void update_permission_bitmask(struct kvm_mmu *mmu, bool ept)
 	bool cr0_wp = is_cr0_wp(mmu);
 	bool efer_nx = is_efer_nx(mmu);
 
+	/*
+	 * #define PFERR_PRESENT_BIT 0
+	 * #define PFERR_WRITE_BIT 1
+	 * #define PFERR_USER_BIT 2
+	 * #define PFERR_RSVD_BIT 3
+	 * #define PFERR_FETCH_BIT 4
+	 * #define PFERR_PK_BIT 5
+	 * #define PFERR_SGX_BIT 15
+	 * #define PFERR_GUEST_FINAL_BIT 32
+	 * #define PFERR_GUEST_PAGE_BIT 33
+	 * #define PFERR_IMPLICIT_ACCESS_BIT 48
+	 *
+	 * 在以下使用kvm_mmu->permissions[16] (u8)
+	 *   - arch/x86/include/asm/kvm_host.h|556| <<global>> u8 permissions[16];
+	 *   - arch/x86/kvm/mmu.h|246| <<permission_fault>> fault = (mmu->permissions[index] >> pte_access) & 1;
+	 *   - arch/x86/kvm/mmu/mmu.c|5332| <<update_permission_bitmask>> for (byte = 0; byte < ARRAY_SIZE(mmu->permissions); ++byte) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5387| <<update_permission_bitmask>> mmu->permissions[byte] = ff | uf | wf | smepf | smapf;
+	 *   - arch/x86/kvm/mmu/mmu.c|5427| <<update_pkru_bitmask>> for (bit = 0; bit < ARRAY_SIZE(mmu->permissions); ++bit) {
+	 *
+	 * 注释
+	 * // Bitmap; bit set = permission fault
+	 * // Byte index: page fault error code [4:1]
+	 * // Bit index: pte permissions in ACC_* format
+	 * u8 permissions[16];
+	 */
 	for (byte = 0; byte < ARRAY_SIZE(mmu->permissions); ++byte) {
+		/*
+		 * byte=0, pfec=0 -->    0
+		 * byte=1, pfec=2 -->   10
+		 * byte=2, pfec=4 -->  100
+		 * byte=3, pfec=6 -->  110
+		 * byte=4, pfec=8 --> 1000
+		 * byte=5, pfec=10 --> 
+		 * byte=6, pfec=12
+		 * byte=7, pfec=14
+		 * byte=8, pfec=16
+		 * byte=9, pfec=18
+		 * byte=10, pfec=20
+		 * byte=11, pfec=22 --> 10110
+		 * byte=12, pfec=24 --> 11000
+		 * byte=13, pfec=26 --> 11010
+		 * byte=14, pfec=28 --> 11100
+		 * byte=15, pfec=30 --> 11110
+		 *
+		 * pfec: page fault error code
+		 */
 		unsigned pfec = byte << 1;
 
 		/*
@@ -5216,6 +5506,13 @@ static void update_pkru_bitmask(struct kvm_mmu *mmu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5449| <<__kvm_mmu_refresh_passthrough_bits>> reset_guest_paging_metadata(vcpu, mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|5509| <<init_kvm_tdp_mmu>> reset_guest_paging_metadata(vcpu, context);
+ *   - arch/x86/kvm/mmu/mmu.c|5531| <<shadow_mmu_init_context>> reset_guest_paging_metadata(vcpu, context);
+ *   - arch/x86/kvm/mmu/mmu.c|5688| <<init_kvm_nested_mmu>> reset_guest_paging_metadata(vcpu, g_context);
+ */
 static void reset_guest_paging_metadata(struct kvm_vcpu *vcpu,
 					struct kvm_mmu *mmu)
 {
@@ -5227,6 +5524,10 @@ static void reset_guest_paging_metadata(struct kvm_vcpu *vcpu,
 	update_pkru_bitmask(mmu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5562| <<shadow_mmu_init_context>> else if (is_cr4_pae(context)) paging64_init_context(context);
+ */
 static void paging64_init_context(struct kvm_mmu *context)
 {
 	context->page_fault = paging64_page_fault;
@@ -5234,6 +5535,10 @@ static void paging64_init_context(struct kvm_mmu *context)
 	context->sync_spte = paging64_sync_spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5564| <<shadow_mmu_init_context>> else paging32_init_context(context);
+ */
 static void paging32_init_context(struct kvm_mmu *context)
 {
 	context->page_fault = paging32_page_fault;
@@ -5241,6 +5546,11 @@ static void paging32_init_context(struct kvm_mmu *context)
 	context->sync_spte = paging32_sync_spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5645| <<kvm_init_shadow_npt_mmu>> union kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &regs);
+ *   - arch/x86/kvm/mmu/mmu.c|5790| <<kvm_init_mmu>> union kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &regs);
+ */
 static union kvm_cpu_role kvm_calc_cpu_role(struct kvm_vcpu *vcpu,
 					    const struct kvm_mmu_role_regs *regs)
 {
@@ -5281,9 +5591,45 @@ static union kvm_cpu_role kvm_calc_cpu_role(struct kvm_vcpu *vcpu,
 	return role;
 }
 
+/*
+ * 关于cr0/cr4的passthrough:
+ * VM-execution control fields include guest/host masks and read shadows for the CR0 and CR4 registers. These
+ * fields control executions of instructions that access those registers (including CLTS, LMSW, MOV CR, and SMSW).
+ * They are 64 bits on processors that support Intel 64 architecture and 32 bits on processors that do not.
+ *
+ * In general, bits set to 1 in a guest/host mask correspond to bits "owned" by the host:
+ *
+ * 1. Guest attempts to set them (using CLTS, LMSW, or MOV to CR) to values differing from the corresponding bits
+ * in the corresponding read shadow cause VM exits.
+ *
+ * 2. Guest reads (using MOV from CR or SMSW) return values for these bits from the corresponding read shadow.
+ *
+ * Bits cleared to 0 correspond to bits "owned" by the guest; guest
+ * attempts to modify them succeed and guest reads return values for these bits from the control register itself.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu.h|202| <<kvm_mmu_refresh_passthrough_bits>> __kvm_mmu_refresh_passthrough_bits(vcpu, mmu);
+ */
 void __kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,
 					struct kvm_mmu *mmu)
 {
+	/*
+	 *  223 #define BUILD_MMU_ROLE_ACCESSOR(base_or_ext, reg, name)         \
+	 *  224 static inline bool __maybe_unused is_##reg##_##name(struct kvm_mmu *mmu)        \
+	 *  225 {                                                               \
+	 *  226         return !!(mmu->cpu_role. base_or_ext . reg##_##name);   \
+	 *  227 }
+	 *
+	 * CR0.WP
+	 * Write Protect (bit 16 of CR0) - When set, inhibits supervisor-level
+	 * procedures from writing into read-only pages; when clear, allows
+	 * supervisor-level procedures to write into read-only pages (regardless
+	 * of the U/S bit setting; see Section 4.1.3 and Section 4.6). This flag
+	 * facilitates implementation of the copy-on-write method of creating a
+	 * new process (forking) used by operating systems such as UNIX. This flag
+	 * must be set before software can set CR4.CET, and it cannot be cleared
+	 * as long as CR4.CET = 1 (see below).
+	 */
 	const bool cr0_wp = kvm_is_cr0_bit_set(vcpu, X86_CR0_WP);
 
 	BUILD_BUG_ON((KVM_MMU_CR0_ROLE_BITS & KVM_POSSIBLE_CR0_GUEST_BITS) != X86_CR0_WP);
@@ -5292,6 +5638,22 @@ void __kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,
 	if (is_cr0_wp(mmu) == cr0_wp)
 		return;
 
+	/*
+	 * 在以下使用kvm_mmu_page_role->cr0_wp:
+	 *   - arch/x86/kvm/mmu/mmu.c|5329| <<update_permission_bitmask>> bool cr0_wp = is_cr0_wp(mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|5360| <<update_permission_bitmask>> if (!cr0_wp)
+	 *   - arch/x86/kvm/mmu/mmu.c|5520| <<kvm_calc_cpu_role>> role.base.cr0_wp = ____is_cr0_wp(regs);
+	 *   - arch/x86/kvm/mmu/mmu.c|5547| <<__kvm_mmu_refresh_passthrough_bits>> const bool cr0_wp = kvm_is_cr0_bit_set(vcpu, X86_CR0_WP);
+	 *   - arch/x86/kvm/mmu/mmu.c|5552| <<__kvm_mmu_refresh_passthrough_bits>> if (is_cr0_wp(mmu) == cr0_wp)
+	 *   - arch/x86/kvm/mmu/mmu.c|5555| <<__kvm_mmu_refresh_passthrough_bits>> mmu->cpu_role.base.cr0_wp = cr0_wp;
+	 *   - arch/x86/kvm/mmu/mmu.c|5579| <<kvm_calc_tdp_mmu_root_page_role>> role.cr0_wp = true;
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|5449| <<__kvm_mmu_refresh_passthrough_bits>> reset_guest_paging_metadata(vcpu, mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|5509| <<init_kvm_tdp_mmu>> reset_guest_paging_metadata(vcpu, context);
+	 *   - arch/x86/kvm/mmu/mmu.c|5531| <<shadow_mmu_init_context>> reset_guest_paging_metadata(vcpu, context);
+	 *   - arch/x86/kvm/mmu/mmu.c|5688| <<init_kvm_nested_mmu>> reset_guest_paging_metadata(vcpu, g_context);
+	 */
 	mmu->cpu_role.base.cr0_wp = cr0_wp;
 	reset_guest_paging_metadata(vcpu, mmu);
 }
@@ -5328,6 +5690,10 @@ kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5734| <<kvm_init_mmu>> init_kvm_tdp_mmu(vcpu, cpu_role);
+ */
 static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu,
 			     union kvm_cpu_role cpu_role)
 {
@@ -5340,6 +5706,14 @@ static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu,
 
 	context->cpu_role.as_u64 = cpu_role.as_u64;
 	context->root_role.word = root_role.word;
+	/*
+	 * 在以下设置kvm_mmu->page_fault:
+	 *   - arch/x86/kvm/mmu/mmu.c|4808| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5424| <<paging64_init_context>> context->page_fault = paging64_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5435| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5543| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5702| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault; --> 用的vcpu->arch.guest_mmu
+	 */
 	context->page_fault = kvm_tdp_page_fault;
 	context->sync_spte = NULL;
 	context->get_guest_pgd = get_guest_cr3;
@@ -5357,6 +5731,11 @@ static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu,
 	reset_tdp_shadow_zero_bits_mask(context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5592| <<kvm_init_shadow_mmu>> shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
+ *   - arch/x86/kvm/mmu/mmu.c|5616| <<kvm_init_shadow_npt_mmu>> shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
+ */
 static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *context,
 				    union kvm_cpu_role cpu_role,
 				    union kvm_mmu_page_role root_role)
@@ -5375,10 +5754,21 @@ static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *conte
 	else
 		paging32_init_context(context);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|5449| <<__kvm_mmu_refresh_passthrough_bits>> reset_guest_paging_metadata(vcpu, mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|5509| <<init_kvm_tdp_mmu>> reset_guest_paging_metadata(vcpu, context);
+	 *   - arch/x86/kvm/mmu/mmu.c|5531| <<shadow_mmu_init_context>> reset_guest_paging_metadata(vcpu, context);
+	 *   - arch/x86/kvm/mmu/mmu.c|5688| <<init_kvm_nested_mmu>> reset_guest_paging_metadata(vcpu, g_context);
+	 */
 	reset_guest_paging_metadata(vcpu, context);
 	reset_shadow_zero_bits_mask(vcpu, context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5680| <<init_kvm_softmmu>> kvm_init_shadow_mmu(vcpu, cpu_role);
+ */
 static void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu,
 				union kvm_cpu_role cpu_role)
 {
@@ -5401,9 +5791,18 @@ static void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu,
 	 */
 	root_role.efer_nx = true;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|5592| <<kvm_init_shadow_mmu>> shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
+	 *   - arch/x86/kvm/mmu/mmu.c|5616| <<kvm_init_shadow_npt_mmu>> shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
+	 */
 	shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|93| <<nested_svm_init_mmu_context>> kvm_init_shadow_npt_mmu(vcpu, X86_CR0_PG, svm->vmcb01.ptr->save.cr4, svm->vmcb01.ptr->save.efer, svm->nested.ctl.nested_cr3);
+ */
 void kvm_init_shadow_npt_mmu(struct kvm_vcpu *vcpu, unsigned long cr0,
 			     unsigned long cr4, u64 efer, gpa_t nested_cr3)
 {
@@ -5455,6 +5854,10 @@ kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|446| <<nested_ept_new_eptp>> kvm_init_shadow_ept_mmu(vcpu, execonly, ept_lpage_level, nested_ept_ad_enabled(vcpu), nested_ept_get_eptp(vcpu));
+ */
 void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 			     int huge_page_level, bool accessed_dirty,
 			     gpa_t new_eptp)
@@ -5484,9 +5887,21 @@ void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 }
 EXPORT_SYMBOL_GPL(kvm_init_shadow_ept_mmu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5736| <<kvm_init_mmu>> init_kvm_softmmu(vcpu, cpu_role);
+ */
 static void init_kvm_softmmu(struct kvm_vcpu *vcpu,
 			     union kvm_cpu_role cpu_role)
 {
+	/*
+	 * 在以下设置kvm_mmu->page_fault:
+	 *   - arch/x86/kvm/mmu/mmu.c|4808| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5424| <<paging64_init_context>> context->page_fault = paging64_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5435| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5543| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5702| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault;
+	 */
 	struct kvm_mmu *context = &vcpu->arch.root_mmu;
 
 	kvm_init_shadow_mmu(vcpu, cpu_role);
@@ -5496,6 +5911,10 @@ static void init_kvm_softmmu(struct kvm_vcpu *vcpu,
 	context->inject_page_fault = kvm_inject_page_fault;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5732| <<kvm_init_mmu>> init_kvm_nested_mmu(vcpu, cpu_role);
+ */
 static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu,
 				union kvm_cpu_role new_mode)
 {
@@ -5535,6 +5954,14 @@ static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu,
 	reset_guest_paging_metadata(vcpu, g_context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5772| <<kvm_mmu_reset_context>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|520| <<nested_svm_load_cr3>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|1138| <<nested_vmx_load_cr3>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/x86.c|956| <<kvm_post_set_cr0>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/x86.c|12206| <<kvm_arch_vcpu_create>> kvm_init_mmu(vcpu);
+ */
 void kvm_init_mmu(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu_role_regs regs = vcpu_to_role_regs(vcpu);
@@ -5549,6 +5976,10 @@ void kvm_init_mmu(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_init_mmu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|386| <<kvm_vcpu_after_set_cpuid>> kvm_mmu_after_set_cpuid(vcpu);
+ */
 void kvm_mmu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -5578,6 +6009,19 @@ void kvm_mmu_after_set_cpuid(struct kvm_vcpu *vcpu)
 	KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5760| <<kvm_mmu_after_set_cpuid>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/smm.c|132| <<kvm_smm_changed>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/smm.c|369| <<enter_smm>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4705| <<nested_vmx_restore_host_state>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|974| <<kvm_post_set_cr0>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1165| <<kvm_post_set_cr4>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1790| <<set_efer>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|11837| <<__set_sregs>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|11881| <<__set_sregs2>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|12407| <<kvm_vcpu_reset>> kvm_mmu_reset_context(vcpu);
+ */
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_unload(vcpu);
@@ -5585,6 +6029,10 @@ void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_reset_context);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|151| <<kvm_mmu_reload>> return kvm_mmu_load(vcpu);
+ */
 int kvm_mmu_load(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -5827,6 +6275,16 @@ void kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 	write_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4608| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn, insn_len);
+ *   - arch/x86/kvm/svm/svm.c|2059| <<npf_interception>> return kvm_mmu_page_fault(vcpu, fault_address, error_code, static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+ *                                                           svm->vmcb->control.insn_bytes : NULL, svm->vmcb->control.insn_len);
+ *   - arch/x86/kvm/vmx/vmx.c|5796| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5817| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ *
+ * 如果是一般的shadow page fault过来, 用的是普通cr3 page fault的error_code
+ */
 int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -5848,6 +6306,12 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 		return RET_PF_RETRY;
 
 	r = RET_PF_INVALID;
+	/*
+	 * RSVD (bit=3):
+	 * 0 - The fault was not caused by reserved bit violation.
+	 * 1 - The fault was caused by a reserved bit set to 1 in some
+	 *     paging-structure entry.
+	 */
 	if (unlikely(error_code & PFERR_RSVD_MASK)) {
 		r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
 		if (r == RET_PF_EMULATE)
@@ -6097,6 +6561,11 @@ static void free_mmu_pages(struct kvm_mmu *mmu)
 	free_page((unsigned long)mmu->pml5_root);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6454| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|6458| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.root_mmu);
+ */
 static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 {
 	struct page *page;
@@ -6150,6 +6619,10 @@ static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12125| <<kvm_arch_vcpu_create>> r = kvm_mmu_create(vcpu);
+ */
 int kvm_mmu_create(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -6366,6 +6839,13 @@ static bool kvm_rmap_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_e
 	return flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|625| <<update_mtrr>> kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));
+ *   - arch/x86/kvm/x86.c|967| <<kvm_post_set_cr0>> kvm_zap_gfn_range(vcpu->kvm, 0, ~0ULL);
+ *   - arch/x86/kvm/x86.c|10615| <<__kvm_set_or_clear_apicv_inhibit>> kvm_zap_gfn_range(kvm, gfn, gfn+1);
+ *   - arch/x86/kvm/x86.c|13432| <<kvm_noncoherent_dma_assignment_start_or_stop>> kvm_zap_gfn_range(kvm, gpa_to_gfn(0), gpa_to_gfn(~0ULL));
+ */
 /*
  * Invalidate (zap) SPTEs that cover GFNs from gfn_start and up to gfn_end
  * (not including it)
@@ -7003,6 +7483,10 @@ static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
  * Forward the module init call to SPTE code so that it too can handle module
  * params that need to be resolved/snapshot.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|14059| <<kvm_x86_init>> kvm_mmu_x86_module_init();
+ */
 void __init kvm_mmu_x86_module_init(void)
 {
 	if (nx_huge_pages == -1)
@@ -7023,6 +7507,10 @@ void __init kvm_mmu_x86_module_init(void)
  * loaded as many of the masks/values may be modified by VMX or SVM, i.e. need
  * to be reset when a potentially different vendor module is loaded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9705| <<__kvm_x86_vendor_init>> r = kvm_mmu_vendor_module_init();
+ */
 int kvm_mmu_vendor_module_init(void)
 {
 	int ret = -ENOMEM;
@@ -7319,9 +7807,26 @@ bool kvm_arch_pre_set_memory_attributes(struct kvm *kvm,
 	return kvm_unmap_gfn_range(kvm, range);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7440| <<hugepage_has_attrs>> if (hugepage_test_mixed(slot, gfn, level - 1) ||
+ */
 static bool hugepage_test_mixed(struct kvm_memory_slot *slot, gfn_t gfn,
 				int level)
 {
+	/*
+	 * 在以下使用kvm_lpage_info->disallow_lpage:
+	 *   - arch/x86/kvm/mmu/mmu.c|811| <<update_gfn_disallow_lpage_count>> old = linfo->disallow_lpage;
+	 *   - arch/x86/kvm/mmu/mmu.c|812| <<update_gfn_disallow_lpage_count>> linfo->disallow_lpage += count;
+	 *   - arch/x86/kvm/mmu/mmu.c|813| <<update_gfn_disallow_lpage_count>> WARN_ON_ONCE((old ^ linfo->disallow_lpage) & KVM_LPAGE_MIXED_FLAG);
+	 *   - arch/x86/kvm/mmu/mmu.c|3156| <<__kvm_mmu_max_mapping_level>> if (!linfo->disallow_lpage)
+	 *   - arch/x86/kvm/mmu/mmu.c|7372| <<hugepage_test_mixed>> return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/mmu/mmu.c|7378| <<hugepage_clear_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/mmu/mmu.c|7384| <<hugepage_set_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
+	 *   - arch/x86/kvm/x86.c|12807| <<kvm_alloc_memslot_metadata>> linfo[0].disallow_lpage = 1;
+	 *   - arch/x86/kvm/x86.c|12809| <<kvm_alloc_memslot_metadata>> linfo[lpages - 1].disallow_lpage = 1;
+	 *   - arch/x86/kvm/x86.c|12819| <<kvm_alloc_memslot_metadata>> linfo[j].disallow_lpage = 1;
+	 */
 	return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
 }
 
@@ -7331,22 +7836,80 @@ static void hugepage_clear_mixed(struct kvm_memory_slot *slot, gfn_t gfn,
 	lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7471| <<kvm_arch_post_set_memory_attributes>> hugepage_set_mixed(slot, gfn, level);
+ *   - arch/x86/kvm/mmu/mmu.c|7493| <<kvm_arch_post_set_memory_attributes>> hugepage_set_mixed(slot, gfn, level);
+ *   - arch/x86/kvm/mmu/mmu.c|7531| <<kvm_mmu_init_memslot_memory_attributes>> hugepage_set_mixed(slot, gfn, level);
+ */
 static void hugepage_set_mixed(struct kvm_memory_slot *slot, gfn_t gfn,
 			       int level)
 {
 	lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
 }
 
+/*
+ * 假设:
+ * base_gfn: 133693442
+ * gfn     : 134060443
+ *
+ * 对于level 2:
+ * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 9       = 261836
+ * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 9 = 261120
+ * 261836 - 261120 = 716
+ *
+ * 对于level 3:
+ * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 18       = 511
+ * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 18 = 510
+ * 511 - 510 = 0
+ *
+ * level 3 (index 1, 1G)有1个slot
+ * level 2 (index 2, 2M)有717个slot
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7439| <<kvm_arch_post_set_memory_attributes>> if (hugepage_has_attrs(kvm, slot, gfn, level, attrs))
+ *   - arch/x86/kvm/mmu/mmu.c|7461| <<kvm_arch_post_set_memory_attributes>> if (hugepage_has_attrs(kvm, slot, gfn, level, attrs))
+ *   - arch/x86/kvm/mmu/mmu.c|7499| <<kvm_mmu_init_memslot_memory_attributes>> if (hugepage_has_attrs(kvm, slot, gfn, level, attrs))
+ */
 static bool hugepage_has_attrs(struct kvm *kvm, struct kvm_memory_slot *slot,
 			       gfn_t gfn, int level, unsigned long attrs)
 {
 	const unsigned long start = gfn;
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+	 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+	 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+	 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+	 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+	 */
 	const unsigned long end = start + KVM_PAGES_PER_HPAGE(level);
 
 	if (level == PG_LEVEL_2M)
 		return kvm_range_has_memory_attributes(kvm, start, end, attrs);
 
+	/*
+	 * KVM_HPAGE_GFN_SHIFT(1) : (((1) - 1) * 9) =  0
+	 * KVM_HPAGE_GFN_SHIFT(2) : (((2) - 1) * 9) =  9 ---> level 2
+	 * KVM_HPAGE_GFN_SHIFT(3) : (((3) - 1) * 9) = 18 ---> level 3
+	 * KVM_HPAGE_GFN_SHIFT(4) : (((4) - 1) * 9) = 27
+	 * KVM_HPAGE_GFN_SHIFT(5) : (((5) - 1) * 9) = 36
+	 *
+	 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+	 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+	 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+	 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+	 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+	 *
+	 * 到这里只能是level=3
+	 *
+	 * 比如start是1048576, end是1048576+262144=1310720.
+	 * 一次加512个
+	 * for (gfn = 1048576; gfn < 1310720; gfn += 512)
+	 */
 	for (gfn = start; gfn < end; gfn += KVM_PAGES_PER_HPAGE(level - 1)) {
+		/*
+		 * 只在此处调用hugepage_test_mixed()
+		 */
 		if (hugepage_test_mixed(slot, gfn, level - 1) ||
 		    attrs != kvm_get_memory_attributes(kvm, gfn))
 			return false;
@@ -7354,6 +7917,20 @@ static bool hugepage_has_attrs(struct kvm *kvm, struct kvm_memory_slot *slot,
 	return true;
 }
 
+/*
+ * struct kvm_gfn_range {
+ *     struct kvm_memory_slot *slot;
+ *     gfn_t start;
+ *     gfn_t end;
+ *     union kvm_mmu_notifier_arg arg;
+ *     bool may_block;
+ * };
+ *
+ * 在以下使用kvm_arch_post_set_memory_attributes():
+ *   - virt/kvm/kvm_main.c|2560| <<kvm_vm_set_mem_attributes>> .handler = kvm_arch_post_set_memory_attributes,
+ *
+ * 就是在attri和设置那些mix
+ */
 bool kvm_arch_post_set_memory_attributes(struct kvm *kvm,
 					 struct kvm_gfn_range *range)
 {
@@ -7377,10 +7954,30 @@ bool kvm_arch_post_set_memory_attributes(struct kvm *kvm,
 	 * The sequence matters here: upper levels consume the result of lower
 	 * level's scanning.
 	 */
+	/*
+	 * level从2变到3
+	 */
 	for (level = PG_LEVEL_2M; level <= KVM_MAX_HUGEPAGE_LEVEL; level++) {
+		/*
+		 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+		 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+		 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+		 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+		 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+		 */
 		gfn_t nr_pages = KVM_PAGES_PER_HPAGE(level);
+		/*
+		 * level 5: 向下round到每68719476736个 (256T)
+		 * level 4: 向下round到每134217728个 (512G)
+		 * level 3: 向下round到每262144个(1G)
+		 * level 2: 向下round到每512个(2M)
+		 * level 1: 向下round到每1个, 就是没round
+		 */
 		gfn_t gfn = gfn_round_for_level(range->start, level);
 
+		/*
+		 * 上面从range->start向下round, 不相等就是小于
+		 */
 		/* Process the head page if it straddles the range. */
 		if (gfn != range->start || gfn + nr_pages > range->end) {
 			/*
@@ -7388,7 +7985,13 @@ bool kvm_arch_post_set_memory_attributes(struct kvm *kvm,
 			 * by the memslot, KVM can't use a hugepage due to the
 			 * misaligned address regardless of memory attributes.
 			 */
+			/*
+			 * 要不要加上gfn + nr_pages <= slot->base_gfn + slot->npages???
+			 */
 			if (gfn >= slot->base_gfn) {
+				/*
+				 * 这里就是针对gfn一个!
+				 */
 				if (hugepage_has_attrs(kvm, slot, gfn, level, attrs))
 					hugepage_clear_mixed(slot, gfn, level);
 				else
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index 0669a8a66..461d049a9 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -6,6 +6,18 @@
 #include <linux/kvm_host.h>
 #include <asm/kvm_host.h>
 
+/*
+ * 在以下使用KVM_MMU_WARN_ON():
+ *   - arch/x86/kvm/mmu/mmu.c|1322| <<spte_clear_dirty>> KVM_MMU_WARN_ON(!spte_ad_enabled(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|1791| <<kvm_mmu_check_sptes_at_free>> if (KVM_MMU_WARN_ON(is_shadow_present_pte(sp->spt[i])))
+ *   - arch/x86/kvm/mmu/mmu.c|4440| <<kvm_max_level_for_order>> KVM_MMU_WARN_ON(order != KVM_HPAGE_GFN_SHIFT(PG_LEVEL_1G) &&
+ *   - arch/x86/kvm/mmu/spte.h|536| <<spte_ad_enabled>> KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
+ *   - arch/x86/kvm/mmu/spte.h|548| <<spte_ad_need_write_protect>> KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
+ *   - arch/x86/kvm/mmu/spte.h|566| <<spte_shadow_accessed_mask>> KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
+ *   - arch/x86/kvm/mmu/spte.h|572| <<spte_shadow_dirty_mask>> KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1649| <<clear_dirty_gfn_range>> KVM_MMU_WARN_ON(kvm_ad_enabled() &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1709| <<clear_dirty_pt_masked>> KVM_MMU_WARN_ON(kvm_ad_enabled() &&
+ */
 #ifdef CONFIG_KVM_PROVE_MMU
 #define KVM_MMU_WARN_ON(x) WARN_ON_ONCE(x)
 #else
@@ -99,6 +111,13 @@ struct kvm_mmu_page {
 	/* Currently serving as active root */
 	union {
 		int root_count;
+		/*
+		 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|78| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|247| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 2);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|730| <<tdp_mmu_zap_root>> WARN_ON_ONCE(!refcount_read(&root->tdp_mmu_root_count));
+		 *   - arch/x86/kvm/mmu/tdp_mmu.h|17| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+		 */
 		refcount_t tdp_mmu_root_count;
 	};
 	unsigned int unsync_children;
@@ -158,8 +177,34 @@ static inline bool kvm_mmu_page_ad_need_write_protect(struct kvm_mmu_page *sp)
 	return kvm_x86_ops.cpu_dirty_log_size && sp->role.guest_mode;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3255| <<direct_map>> base_gfn = gfn_round_for_level(fault->gfn, it.level);
+ *   - arch/x86/kvm/mmu/mmu.c|4670| <<kvm_tdp_page_fault>> gfn_t base = gfn_round_for_level(fault->gfn, fault->max_level);
+ *   - arch/x86/kvm/mmu/mmu.c|7429| <<kvm_arch_post_set_memory_attributes>> gfn_t gfn = gfn_round_for_level(range->start, level);
+ *   - arch/x86/kvm/mmu/mmu.c|7484| <<kvm_mmu_init_memslot_memory_attributes>> gfn_t end = gfn_round_for_level(slot->base_gfn + slot->npages, level);
+ *   - arch/x86/kvm/mmu/mmu.c|7485| <<kvm_mmu_init_memslot_memory_attributes>> gfn_t start = gfn_round_for_level(slot->base_gfn, level);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|192| <<kvm_flush_remote_tlbs_gfn>> kvm_flush_remote_tlbs_range(kvm, gfn_round_for_level(gfn, level),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME>> base_gfn = gfn_round_for_level(fault->gfn, it.level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|40| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|116| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|158| <<try_step_up>> iter->gfn = gfn_round_for_level(iter->gfn, iter->level);
+ *
+ * level 5: 向下round到每68719476736个 (256T)
+ * level 4: 向下round到每134217728个 (512G)
+ * level 3: 向下round到每262144个(1G)
+ * level 2: 向下round到每512个(2M)
+ * level 1: 向下round到每1个, 就是没round
+ */
 static inline gfn_t gfn_round_for_level(gfn_t gfn, int level)
 {
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+	 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+	 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+	 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+	 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+	 */
 	return gfn & -KVM_PAGES_PER_HPAGE(level);
 }
 
@@ -239,6 +284,16 @@ struct kvm_page_fault {
 	unsigned long mmu_seq;
 	kvm_pfn_t pfn;
 	hva_t hva;
+	/*
+	 * 在以下使用kvm_page_fault->map_writable:
+	 *   - arch/x86/kvm/mmu/mmu.c|2954| <<mmu_set_spte>> bool host_writable = !fault || fault->map_writable;
+	 *   - arch/x86/kvm/mmu/mmu.c|4375| <<kvm_faultin_pfn_private>> fault->map_writable = !(fault->slot->flags & KVM_MEM_READONLY);
+	 *   - arch/x86/kvm/mmu/mmu.c|4398| <<__kvm_faultin_pfn>> fault->map_writable = false;
+	 *   - arch/x86/kvm/mmu/mmu.c|4422| <<__kvm_faultin_pfn>> fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, false, &async, fault->write, &fault->map_writable, &fault->hva);
+	 *   - arch/x86/kvm/mmu/mmu.c|4444| <<__kvm_faultin_pfn>> fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, true, NULL, fault->write, &fault->map_writable, &fault->hva);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1066| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn,
+	 *                                                fault->pfn, iter->old_spte, fault->prefetch, true, fault->map_writable, &new_spte);
+	 */
 	bool map_writable;
 
 	/*
@@ -279,6 +334,11 @@ enum {
 	RET_PF_SPURIOUS,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4399| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true, NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|6180| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, lower_32_bits(error_code), false, &emulation_type);
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u32 err, bool prefetch, int *emulation_type)
 {
diff --git a/arch/x86/kvm/mmu/paging_tmpl.h b/arch/x86/kvm/mmu/paging_tmpl.h
index 4d4e98fe4..97bec5ade 100644
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@ -170,6 +170,28 @@ static bool FNAME(prefetch_invalid_gpte)(struct kvm_vcpu *vcpu,
 	return true;
 }
 
+/*
+ * 在mmu.c:
+ * 5021 #define PTTYPE_EPT 18 // arbitrary
+ * 5022 #define PTTYPE PTTYPE_EPT
+ * 5023 #include "paging_tmpl.h"
+ * 5024 #undef PTTYPE
+ * 5025
+ * 5026 #define PTTYPE 64
+ * 5027 #include "paging_tmpl.h"
+ * 5028 #undef PTTYPE
+ * 5029
+ * 5030 #define PTTYPE 32
+ * 5031 #include "paging_tmpl.h"
+ * 5032 #undef PTTYPE
+ *
+ * 根据gpte的内容(比如后3位或者最高位), 转换成ACC_xxx的模式
+ * 以记录这个gpte是否允许exec, read, write
+ * #define ACC_EXEC_MASK    1
+ * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+ * #define ACC_USER_MASK    PT_USER_MASK
+ * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+ */
 /*
  * For PTTYPE_EPT, a page table can be executable but not readable
  * on supported processors. Therefore, set_spte does not automatically
@@ -186,8 +208,23 @@ static inline unsigned FNAME(gpte_access)(u64 gpte)
 #else
 	BUILD_BUG_ON(ACC_EXEC_MASK != PT_PRESENT_MASK);
 	BUILD_BUG_ON(ACC_EXEC_MASK != 1);
+	/*
+	 * PT_WRITABLE_MASK: 1 << 1
+	 * PT_USER_MASK:     1 << 2
+	 * PT_PRESENT_MASK:  1 << 0
+	 */
 	access = gpte & (PT_WRITABLE_MASK | PT_USER_MASK | PT_PRESENT_MASK);
 	/* Combine NX with P (which is set here) to get ACC_EXEC_MASK.  */
+	/*
+	 * #define ACC_EXEC_MASK    1
+	 * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+	 * #define ACC_USER_MASK    PT_USER_MASK
+	 * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 *
+	 * access和00X进程异或
+	 * 比如: 111 ^ 001 = 110
+	 * 比如: 110 ^ 001 = 111
+	 */
 	access ^= (gpte >> PT64_NX_SHIFT);
 #endif
 
@@ -296,6 +333,20 @@ static inline bool FNAME(is_last_gpte)(struct kvm_mmu *mmu,
 
 	return gpte & PT_PAGE_SIZE_MASK;
 }
+/*
+ * error code:
+ * - P  1 bit   Present When set, the page fault was caused by a
+ *   page-protection violation. When not set, it was caused by a
+ *   non-present page.
+ * - W  1 bit   Write   When set, the page fault was caused by a write
+ *   access. When not set, it was caused by a read access.
+ * - U  1 bit   User    When set, the page fault was caused while CPL =
+ *   3. This does not necessarily mean that the page fault was a
+ *   privilege violation.
+ * - R  1 bit   Reserved write  When set, one or more page directory
+ *   entries contain reserved bits which are set to 1. This only
+ *   applies when the PSE or PAE flags in CR4 are set to 1.
+ */
 /*
  * Fetch a guest pte for a guest virtual address, or for an L2's GPA.
  */
@@ -321,6 +372,9 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 	gpa_t real_gpa;
 	gfn_t gfn;
 
+	/*
+	 * 只在这里trace
+	 */
 	trace_kvm_mmu_pagetable_walk(addr, access);
 retry_walk:
 	walker->level = mmu->cpu_role.base.level;
@@ -364,6 +418,15 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 		struct kvm_memory_slot *slot;
 		unsigned long host_addr;
 
+		/*
+		 * pt_access=0xffffffffffffffff, pte_access=0xffffffffffffffff
+		 * pt_access=0x8000000005855027, pte_access=0x8000000005855027
+		 * pt_access=0x8000000005854027, pte_access=0x8000000005854027
+		 *
+		 * u64 pt_access, pte_access;
+		 *
+		 * pte_access一开始是pte_access = ~0;
+		 */
 		pt_access = pte_access;
 		--walker->level;
 
@@ -402,12 +465,20 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 			goto error;
 
 		ptep_user = (pt_element_t __user *)((void *)host_addr + offset);
+		/*
+		 * pt_element_t pte;
+		 * 获取下一个level的pte
+		 */
 		if (unlikely(__get_user(pte, ptep_user)))
 			goto error;
 		walker->ptep_user[walker->level - 1] = ptep_user;
 
 		trace_kvm_mmu_paging_element(pte, walker->level);
 
+		/*
+		 *  384 #if PTTYPE == 64
+		 *  385         walk_nx_mask = 1ULL << PT64_NX_SHIFT;
+		 */
 		/*
 		 * Inverting the NX it lets us AND it like other
 		 * permission bits.
@@ -424,6 +495,14 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 
 		walker->ptes[walker->level - 1] = pte;
 
+		/*
+		 * 根据gpte的内容(比如后3位或者最高位), 转换成ACC_xxx的模式
+		 * 以记录这个gpte是否允许exec, read, write
+		 * #define ACC_EXEC_MASK    1
+		 * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+		 * #define ACC_USER_MASK    PT_USER_MASK
+		 * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+		 */
 		/* Convert to ACC_*_MASK flags for struct guest_walker.  */
 		walker->pt_access[walker->level - 1] = FNAME(gpte_access)(pt_access ^ walk_nx_mask);
 	} while (!FNAME(is_last_gpte)(mmu, walker->level, pte));
@@ -431,8 +510,34 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 	pte_pkey = FNAME(gpte_pkeys)(vcpu, pte);
 	accessed_dirty = have_ad ? pte_access & PT_GUEST_ACCESSED_MASK : 0;
 
+	/*
+	 * 根据gpte的内容(比如后3位或者最高位), 转换成ACC_xxx的模式
+	 * 以记录这个gpte是否允许exec, read, write
+	 * #define ACC_EXEC_MASK    1
+	 * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+	 * #define ACC_USER_MASK    PT_USER_MASK
+	 * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 */
 	/* Convert to ACC_*_MASK flags for struct guest_walker.  */
 	walker->pte_access = FNAME(gpte_access)(pte_access ^ walk_nx_mask);
+	/*
+	 * error code:
+	 * - P  1 bit   Present When set, the page fault was caused by a
+	 *   page-protection violation. When not set, it was caused by a
+	 *   non-present page.
+	 * - W  1 bit   Write   When set, the page fault was caused by a write
+	 *   access. When not set, it was caused by a read access.
+	 * - U  1 bit   User    When set, the page fault was caused while CPL =
+	 *   3. This does not necessarily mean that the page fault was a
+	 *   privilege violation.
+	 * - R  1 bit   Reserved write  When set, one or more page directory
+	 *   entries contain reserved bits which are set to 1. This only
+	 *   applies when the PSE or PAE flags in CR4 are set to 1.
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|436| <<FNAME(walk_addr_generic)>> errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
+	 *   - arch/x86/kvm/x86.c|7758| <<vcpu_mmio_gva_to_gpa>> !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access))) {
+	 */
 	errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
 	if (unlikely(errcode))
 		goto error;
@@ -542,6 +647,14 @@ FNAME(prefetch_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 		return false;
 
 	gfn = gpte_to_gfn(gpte);
+	/*
+	 * 根据gpte的内容(比如后3位或者最高位), 转换成ACC_xxx的模式
+	 * 以记录这个gpte是否允许exec, read, write
+	 * #define ACC_EXEC_MASK    1
+	 * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+	 * #define ACC_USER_MASK    PT_USER_MASK
+	 * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 */
 	pte_access = sp->role.access & FNAME(gpte_access)(gpte);
 	FNAME(protect_clean_gpte)(vcpu->arch.mmu, &pte_access, gpte);
 
@@ -624,6 +737,9 @@ static void FNAME(pte_prefetch)(struct kvm_vcpu *vcpu, struct guest_walker *gw,
  * If the guest tries to write a write-protected page, we need to
  * emulate this operation, return 1 to indicate this case.
  */
+/*
+ * 只在FNAME(page_fault)调用
+ */
 static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 			 struct guest_walker *gw)
 {
@@ -760,6 +876,32 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	return RET_PF_RETRY;
 }
 
+/*
+ * 333         struct kvm_page_fault fault = {
+ * 334                 .addr = cr2_or_gpa,
+ * 335                 .error_code = err,
+ * 336                 .exec = err & PFERR_FETCH_MASK,
+ * 337                 .write = err & PFERR_WRITE_MASK,
+ * 338                 .present = err & PFERR_PRESENT_MASK,
+ * 339                 .rsvd = err & PFERR_RSVD_MASK,
+ * 340                 .user = err & PFERR_USER_MASK,
+ * 341                 .prefetch = prefetch,
+ * 342                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 343                 .nx_huge_page_workaround_enabled =
+ * 344                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 345
+ * 346                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 347                 .req_level = PG_LEVEL_4K,
+ * 348                 .goal_level = PG_LEVEL_4K,
+ * 349                 .is_private = kvm_mem_is_private(vcpu->kvm, cr2_or_gpa >> PAGE_SHIFT),
+ * 350         };
+ * 351         int r;
+ * 352
+ * 353         if (vcpu->arch.mmu->root_role.direct) {
+ * 354                 fault.gfn = fault.addr >> PAGE_SHIFT;
+ * 355                 fault.slot = kvm_vcpu_gfn_to_memslot(vcpu, fault.gfn);
+ * 356         }
+ */
 /*
  * Page fault handler.  There are several causes for a page fault:
  *   - there is no shadow pte for the guest pte
@@ -781,6 +923,20 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 
 	WARN_ON_ONCE(fault->is_tdp);
 
+	/*
+	 * error code:
+	 * - P	1 bit	Present	When set, the page fault was caused by a
+	 *   page-protection violation. When not set, it was caused by a
+	 *   non-present page.
+	 * - W	1 bit	Write	When set, the page fault was caused by a write
+	 *   access. When not set, it was caused by a read access.
+	 * - U	1 bit	User	When set, the page fault was caused while CPL =
+	 *   3. This does not necessarily mean that the page fault was a
+	 *   privilege violation.
+	 * - R	1 bit	Reserved write	When set, one or more page directory
+	 *   entries contain reserved bits which are set to 1. This only
+	 *   applies when the PSE or PAE flags in CR4 are set to 1.
+	 */
 	/*
 	 * Look up the guest pte for the faulting address.
 	 * If PFEC.RSVD is set, this is a shadow page fault.
@@ -812,10 +968,21 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (r)
 		return r;
 
+	/*
+	 * walker.pte_access应该是最后一级guest内部的pte的access rights
+	 */
 	r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
 	if (r != RET_PF_CONTINUE)
 		return r;
 
+	/*
+	 * 特别多and的条件:
+	 * 1. fault->write
+	 * 2. !(walker.pte_access & ACC_WRITE_MASK)
+	 * 3. !is_cr0_wp(vcpu->arch.mmu)
+	 * 4. !fault->user
+	 * 5. fault->slot
+	 */
 	/*
 	 * Do not change pte_access if the pfn is a mmio page, otherwise
 	 * we will cache the incorrect access into mmio spte.
@@ -926,6 +1093,14 @@ static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int
 
 	gfn = gpte_to_gfn(gpte);
 	pte_access = sp->role.access;
+	/*
+	 * 根据gpte的内容(比如后3位或者最高位), 转换成ACC_xxx的模式
+	 * 以记录这个gpte是否允许exec, read, write
+	 * #define ACC_EXEC_MASK    1
+	 * #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+	 * #define ACC_USER_MASK    PT_USER_MASK
+	 * #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+	 */
 	pte_access &= FNAME(gpte_access)(gpte);
 	FNAME(protect_clean_gpte)(vcpu->arch.mmu, &pte_access, gpte);
 
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 4a599130e..9036c11f0 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -19,34 +19,501 @@
 #include <asm/memtype.h>
 #include <asm/vmx.h>
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> __kvm_x86_vendor_init()
+ *       -> kvm_mmu_vendor_module_init()
+ *          -> kvm_mmu_reset_all_pte_masks()
+ *             -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *    -> ops->hardware_setup = hardware_setup()
+ *       -> kvm_mmu_set_ept_masks()
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull; (1 << 8)
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ */
+
+/*
+ * 在以下设置enable_mmio_caching:
+ *   - arch/x86/kvm/mmu/spte.c|388| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = allow_mmio_caching;
+ *   - arch/x86/kvm/mmu/spte.c|422| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = false;
+ * 在以下使用enable_mmio_caching:
+ *   - arch/x86/kvm/mmu/spte.c|24| <<global>> module_param_named(mmio_caching, enable_mmio_caching, bool, 0444);
+ *   - arch/x86/kvm/mmu/mmu.c|3347| <<kvm_handle_noslot_fault>> if (unlikely(!enable_mmio_caching))
+ *   - arch/x86/kvm/mmu/spte.c|64| <<kvm_mmu_spte_module_init>> allow_mmio_caching = enable_mmio_caching;
+ *   - arch/x86/kvm/mmu/spte.c|389| <<kvm_mmu_set_mmio_spte_mask>> if (!enable_mmio_caching)
+ *   - arch/x86/kvm/mmu/spte.h|303| <<is_mmio_spte>> likely(enable_mmio_caching);
+ *   - arch/x86/kvm/svm/sev.c|2254| <<sev_hardware_setup>> if (!enable_mmio_caching)
+ */
 bool __read_mostly enable_mmio_caching = true;
+/*
+ * 在以下设置allow_mmio_caching:
+ *   - arch/x86/kvm/mmu/spte.c|64| <<kvm_mmu_spte_module_init>> allow_mmio_caching = enable_mmio_caching;
+ * 在以下使用allow_mmio_caching:
+ *   - arch/x86/kvm/mmu/spte.c|388| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = allow_mmio_caching;
+ */
 static bool __ro_after_init allow_mmio_caching;
 module_param_named(mmio_caching, enable_mmio_caching, bool, 0444);
 EXPORT_SYMBOL_GPL(enable_mmio_caching);
 
+/*
+ * 在以下设置shadow_host_writable_mask:
+ *   - arch/x86/kvm/mmu/spte.c|551| <<kvm_mmu_set_ept_masks>> shadow_host_writable_mask = EPT_SPTE_HOST_WRITABLE;
+ *   - arch/x86/kvm/mmu/spte.c|615| <<kvm_mmu_reset_all_pte_masks>> shadow_host_writable_mask = DEFAULT_SPTE_HOST_WRITABLE;
+ * 在以下使用shadow_host_writable_mask:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|961| <<FNAME(sync_spte)>> host_writable = spte & shadow_host_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|287| <<make_spte>> spte |= shadow_host_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|424| <<kvm_mmu_changed_pte_notifier_make_spte>> new_spte &= ~shadow_host_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.h|504| <<check_spte_writable_invariants>> WARN_ONCE(!(spte & shadow_host_writable_mask), KBUILD_MODNAME ": MMU-writable SPTE is not Host-writable: %llx", spte);
+ *
+ * 有三个mask:
+ * - shadow_host_writable_mask:
+ *   backend的page在host上是否writable(EPT用EPT_SPTE_HOST_WRITABLE, 普通用DEFAULT_SPTE_HOST_WRITABLE)
+ *   选取的是ept和普通分别不用的bit
+ *   更多是GUP从host获得指示
+ * - shadow_mmu_writable_mask
+ *   指示这个pte的page是不是writable的, 不能简单用bit=1, 没法分清是真的不能还是mmio或是别的
+ *   设置这个mask是告诉user这个pte一定是writable
+ *   EPT用EPT_SPTE_MMU_WRITABLE, 普通用DEFAULT_SPTE_MMU_WRITABLE
+ * - pte & PT_WRITABLE_MASK (看看bit设置了吗)
+ *   这个单纯是看pte的bie=1
+ *
+ *  shadow_mmu_writable_mask, aka MMU-writable -
+ *    Cleared on SPTEs that KVM is currently write-protecting for shadow paging
+ *    purposes (case 2 above). --> 保护影子页表
+ *
+ *  shadow_host_writable_mask, aka Host-writable -
+ *    Cleared on SPTEs that are not host-writable (case 3 above) --> backend memslot不可写
+ *
+ * 查看is_writable_pte()的注释
+ */
 u64 __read_mostly shadow_host_writable_mask;
+/*
+ * 在以下设置shadow_mmu_writable_mask:
+ *   - arch/x86/kvm/mmu/spte.c|609| <<kvm_mmu_set_ept_masks>> shadow_mmu_writable_mask = EPT_SPTE_MMU_WRITABLE;
+ *   - arch/x86/kvm/mmu/spte.c|673| <<kvm_mmu_reset_all_pte_masks>> shadow_mmu_writable_mask = DEFAULT_SPTE_MMU_WRITABLE;
+ * 在以下使用shadow_mmu_writable_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|1253| <<spte_write_protect>> spte &= ~shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|336| <<make_spte>> spte |= PT_WRITABLE_MASK | shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|356| <<make_spte>> spte &= ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ *   - arch/x86/kvm/mmu/spte.c|464| <<kvm_mmu_changed_pte_notifier_make_spte>> new_spte &= ~shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.h|503| <<check_spte_writable_invariants>> if (spte & shadow_mmu_writable_mask)
+ *   - arch/x86/kvm/mmu/spte.h|514| <<is_mmu_writable_spte>> return spte & shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1830| <<write_protect_gfn>> new_spte = iter.old_spte & ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ *
+ * 用来标记虚拟化的页表里的entry是否指示writable
+ */
 u64 __read_mostly shadow_mmu_writable_mask;
+/*
+ * 在以下设置shadow_nx_mask:
+ *   - arch/x86/kvm/mmu/spte.c|590| <<kvm_mmu_set_ept_masks>> shadow_nx_mask = 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|658| <<kvm_mmu_reset_all_pte_masks>> shadow_nx_mask = PT64_NX_MASK;
+ * 在以下使用shadow_nx_mask:
+ *   - arch/x86/kvm/mmu/spte.c|305| <<make_spte>> spte |= shadow_nx_mask;
+ *   - arch/x86/kvm/mmu/spte.c|388| <<make_spte_executable>> spte &= ~shadow_nx_mask;
+ *   - arch/x86/kvm/mmu/spte.h|73| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *   - arch/x86/kvm/mmu/spte.h|379| <<is_executable_pte>> return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
+ *
+ * 普通page table entry的bit=63 (XD):
+ * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+ * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+ * otherwise, reserved (must be 0)
+ *
+ * 普通pte支持nx bit, ept不支持nx bit
+ * 普通pte不支持x bit, ept支持x bit
+ */
 u64 __read_mostly shadow_nx_mask;
+/*
+ * 在以下设置shadow_x_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|5086| <<boot_cpu_is_amd>> return shadow_x_mask == 0;
+ *   - arch/x86/kvm/mmu/spte.c|612| <<kvm_mmu_set_ept_masks>> shadow_x_mask = VMX_EPT_EXECUTABLE_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|680| <<kvm_mmu_reset_all_pte_masks>> shadow_x_mask = 0;
+ * 在以下使用shadow_x_mask:
+ *   - arch/x86/kvm/mmu/spte.c|324| <<make_spte>> spte |= shadow_x_mask;
+ *   - arch/x86/kvm/mmu/spte.c|410| <<make_spte_executable>> spte |= shadow_x_mask;
+ *   - arch/x86/kvm/mmu/spte.c|466| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK | shadow_user_mask | shadow_x_mask | shadow_me_value;
+ *   - arch/x86/kvm/mmu/spte.h|73| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *   - arch/x86/kvm/mmu/spte.h|379| <<is_executable_pte>> return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
+ *
+ * 普通page table entry的bit=63 (XD):
+ * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+ * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+ * otherwise, reserved (must be 0)
+ *
+ * 普通pte支持nx bit, ept不支持nx bit
+ * 普通pte不支持x bit, ept支持x bit
+ */
 u64 __read_mostly shadow_x_mask; /* mutual exclusive with nx_mask */
+/*
+ * 在以下设置shadow_user_mask:
+ *   - arch/x86/kvm/mmu/spte.c|608| <<kvm_mmu_set_ept_masks>> shadow_user_mask = VMX_EPT_READABLE_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|676| <<kvm_mmu_reset_all_pte_masks>> shadow_user_mask = PT_USER_MASK;
+ * 在以下使用shadow_user_mask:
+ *   - arch/x86/kvm/mmu/mmutrace.h|356| <<__field>> __entry->u = shadow_user_mask ? !!(__entry->spte & shadow_user_mask) : -1;
+ *   - arch/x86/kvm/mmu/spte.c|329| <<make_spte>> spte |= shadow_user_mask;
+ *   - arch/x86/kvm/mmu/spte.c|466| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK | shadow_user_mask | shadow_x_mask | shadow_me_value;
+ *   - arch/x86/kvm/mmu/spte.h|72| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *
+ * 关于PT_USER_MASK, bit=2(U/S):
+ * User/supervisor; if 0, user-mode accesses are not allowed to the 4-KByte
+ * page referenced by this entry (see Section 4.6)
+ *
+ * 普通page fault来的时候error code会设置PT_USER_MASK
+ *
+ * 注释: 因为ept支持exec bit, 所以用shadow_user_mask表示readable.
+ * For the EPT case, shadow_present_mask is 0 if hardware
+ * supports exec-only page table entries.  In that case,
+ * ACC_USER_MASK and shadow_user_mask are used to represent
+ * read access.  See FNAME(gpte_access) in paging_tmpl.h.
+ */
 u64 __read_mostly shadow_user_mask;
+/*
+ * 在以下设置shadow_accessed_mask:
+ *   - arch/x86/kvm/mmu/spte.c|464| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|528| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+ * 在以下使用shadow_accessed_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|623| <<mmu_spte_age>> clear_bit((ffs(shadow_accessed_mask) - 1), (unsigned long *)sptep);
+ *   - arch/x86/kvm/mmu/spte.c|156| <<spte_has_volatile_bits>> if (!(spte & shadow_accessed_mask) || (is_writable_pte(spte) && !(spte & shadow_dirty_mask)))
+ *   - arch/x86/kvm/mmu/spte.c|356| <<make_nonleaf_spte>> spte |= shadow_accessed_mask;
+ *   - arch/x86/kvm/mmu/spte.c|380| <<mark_spte_for_access_track>> return spte & ~shadow_accessed_mask;
+ *   - arch/x86/kvm/mmu/spte.h|319| <<kvm_ad_enabled>> return !!shadow_accessed_mask;
+ *   - arch/x86/kvm/mmu/spte.h|353| <<spte_shadow_accessed_mask>> return spte_ad_enabled(spte) ? shadow_accessed_mask : 0;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1279| <<age_gfn_range>> iter->old_spte = tdp_mmu_clear_spte_bits(iter->sptep, iter->old_spte, shadow_accessed_mask, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1281| <<age_gfn_range>> new_spte = iter->old_spte & ~shadow_accessed_mask;
+ *
+ * 对于普通的页表:
+ * bit-5: Accessed: indicates whether software has accessed the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ * bit-6: Dirty: indicates whether software has written to the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ *
+ * 对于EPT:
+ * - bit-8: If bit 6 (ignore guest PAT) of EPTP is 1, accessed flag for EPT;
+ *   indicates whether software has accessed the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ * - bit-9: If bit 6 (ignore guest PAT) of EPTP is 1, dirty flag for EPT;
+ *   indicates whether software has written to the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ *
+ * 哪些bit用来记录pte被访问过
+ */
 u64 __read_mostly shadow_accessed_mask;
+/*
+ * 在以下设置shadow_dirty_mask:
+ *   - arch/x86/kvm/mmu/spte.c|610| <<kvm_mmu_set_ept_masks>> shadow_dirty_mask = has_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|678| <<kvm_mmu_reset_all_pte_masks>> shadow_dirty_mask = PT_DIRTY_MASK;
+ * 在以下使用shadow_dirty_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|1277| <<spte_clear_dirty>> spte &= ~shadow_dirty_mask;
+ *   - arch/x86/kvm/mmu/spte.c|272| <<spte_has_volatile_bits>> (is_writable_pte(spte) && !(spte & shadow_dirty_mask)))
+ *   - arch/x86/kvm/mmu/spte.h|359| <<spte_shadow_dirty_mask>> return spte_ad_enabled(spte) ? shadow_dirty_mask : 0;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1625| <<clear_dirty_gfn_range>> u64 dbit = kvm_ad_enabled() ? shadow_dirty_mask : PT_WRITABLE_MASK;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1688| <<clear_dirty_pt_masked>> u64 dbit = (wrprot || !kvm_ad_enabled()) ? PT_WRITABLE_MASK : shadow_dirty_mask;
+ *
+ * 对于普通的页表:
+ * bit-5: Accessed: indicates whether software has accessed the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ * bit-6: Dirty: indicates whether software has written to the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ *
+ * 对于EPT:
+ * - bit-8: If bit 6 (ignore guest PAT) of EPTP is 1, accessed flag for EPT;
+ *   indicates whether software has accessed the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ * - bit-9: If bit 6 (ignore guest PAT) of EPTP is 1, dirty flag for EPT;
+ *   indicates whether software has written to the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ *
+ * 哪些bit用来记录被dirty过
+ */
 u64 __read_mostly shadow_dirty_mask;
+/*
+ * 在以下设置shadow_mmio_value:
+ *   - arch/x86/kvm/mmu/spte.c|444| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_value = mmio_value;
+ * 在以下使用shadow_mmio_value:
+ *   - arch/x86/kvm/mmu/spte.c|104| <<make_mmio_spte>> WARN_ON_ONCE(!shadow_mmio_value);
+ *   - arch/x86/kvm/mmu/spte.c|107| <<make_mmio_spte>> spte |= shadow_mmio_value | access;
+ *   - arch/x86/kvm/mmu/spte.h|302| <<is_mmio_spte>> return (spte & shadow_mmio_mask) == shadow_mmio_value &&
+ *
+ * 被shadow_mmio_mask后什么样子的value代表这是mmio entry
+ *
+ * 应该是只有mmio cache被enable的时候才用
+ * 1. 如果disable了, 每次都page fault, 然后查看memslot有没有
+ * 没有就是mmio, 然后emulate. 这种混在一起每次查memslot会影响性能.
+ * 2. 如果enable了, 会尽可能优化来避免每次mmio查看memslot
+ * EPT的mmio value就是会产生ept misconfig的value, 在mis config处理mmio, 不需要查找memslot
+ * regular pte的话支持太多bit就不行了,
+ * 否则可以用一个high reserved bit产生因访问reserved bit而造成的page fault:
+ * Set a reserved PA bit in MMIO SPTEs to generate page faults with
+ * PFEC.RSVD=1 on MMIO accesses.  64-bit PTEs (PAE, x86-64, and EPT
+ * paging) support a maximum of 52 bits of PA, i.e. if the CPU supports
+ * 52-bit physical addresses then there are no reserved PA bits in the
+ * PTEs and so the reserved PA approach must be disabled.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|491| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK, 0);
+ *   - arch/x86/kvm/mmu/spte.c|559| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+ *   - arch/x86/kvm/svm/svm.c|5074| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+ */
 u64 __read_mostly shadow_mmio_value;
+/*
+ * 在以下设置shadow_mmio_mask:
+ *   - arch/x86/kvm/mmu/spte.c|445| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_mask = mmio_mask;
+ * 在以下使用shadow_mmio_mask:
+ *   - arch/x86/kvm/mmu/spte.h|302| <<is_mmio_spte>> return (spte & shadow_mmio_mask) == shadow_mmio_value &&
+ *
+ * shadow_mmio_mask表示哪些bit可能用作mmio
+ */
 u64 __read_mostly shadow_mmio_mask;
+/*
+ * error code:
+ * - P  1 bit   Present When set, the page fault was caused by a
+ *   page-protection violation. When not set, it was caused by a
+ *   non-present page.
+ * - W  1 bit   Write   When set, the page fault was caused by a write
+ *   access. When not set, it was caused by a read access.
+ * - U  1 bit   User    When set, the page fault was caused while CPL =
+ *   3. This does not necessarily mean that the page fault was a
+ *   privilege violation.
+ * - R  1 bit   Reserved write  When set, one or more page directory
+ *   entries contain reserved bits which are set to 1. This only
+ *   applies when the PSE or PAE flags in CR4 are set to 1.
+ *
+ * 在以下设置shadow_mmio_access_mask:
+ *   - arch/x86/kvm/mmu/spte.c|446| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_access_mask = access_mask;
+ * 在以下使用shadow_mmio_access_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|313| <<get_mmio_spte_access>> return spte & shadow_mmio_access_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|3340| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+ *   - arch/x86/kvm/mmu/spte.c|106| <<make_mmio_spte>> access &= shadow_mmio_access_mask;
+ *
+ * 4af7715110a2617fc40ac2c1232f664019269f3a
+ * KVM: x86/mmu: Add explicit access mask for MMIO SPTEs
+ *
+ * commit的一些注释:
+ * When shadow paging is enabled, KVM tracks the allowed access type for
+ * MMIO SPTEs so that it can do a permission check on a MMIO GVA cache hit
+ * without having to walk the guest's page tables.  The tracking is done
+ * by retaining the WRITE and USER bits of the access when inserting the
+ * MMIO SPTE (read access is implicitly allowed), which allows the MMIO
+ * page fault handler to retrieve and cache the WRITE/USER bits from the
+ * SPTE.
+ *
+ * 比如: ept是0
+ * regular pte是ACC_WRITE_MASK | ACC_USER_MASK
+ *
+ * 表示在cache mmio_access的时候都cache哪些bit
+ * struct kvm_vcpu *vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> u64 mmio_gva;
+ *    -> unsigned mmio_access;
+ *    -> gfn_t mmio_gfn;
+ *    -> u64 mmio_gen;
+ */
 u64 __read_mostly shadow_mmio_access_mask;
+/*
+ * 在以下设置shadow_present_mask:
+ *   - arch/x86/kvm/mmu/spte.c|468| <<kvm_mmu_set_ept_masks>> shadow_present_mask = has_exec_only ? 0ull : VMX_EPT_READABLE_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|532| <<kvm_mmu_reset_all_pte_masks>> shadow_present_mask = PT_PRESENT_MASK;
+ * 在以下使用shadow_present_mask:
+ *   - rch/x86/kvm/mmu/mmutrace.h|354| <<__field>> __entry->r = shadow_present_mask || (__entry->spte & PT_PRESENT_MASK);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|942| <<FNAME(sync_spte)>> if ((!pte_access && !shadow_present_mask) ||
+ *   - arch/x86/kvm/mmu/spte.c|174| <<make_spte>> WARN_ON_ONCE(!pte_access && !shadow_present_mask);
+ *   - arch/x86/kvm/mmu/spte.c|187| <<make_spte>> spte |= shadow_present_mask;
+ *   - arch/x86/kvm/mmu/spte.c|350| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK |
+ *
+ * 注释: 因为ept支持exec bit, 所以用shadow_user_mask表示readable.
+ * For the EPT case, shadow_present_mask is 0 if hardware
+ * supports exec-only page table entries.  In that case,
+ * ACC_USER_MASK and shadow_user_mask are used to represent
+ * read access.  See FNAME(gpte_access) in paging_tmpl.h.
+ */
 u64 __read_mostly shadow_present_mask;
+/*
+ * 在以下设置shadow_memtype_mask:
+ *   - arch/x86/kvm/mmu/spte.c|439| <<kvm_mmu_set_ept_masks>> shadow_memtype_mask = VMX_EPT_MT_MASK | VMX_EPT_IPAT_BIT;
+ *   - arch/x86/kvm/mmu/spte.c|496| <<kvm_mmu_reset_all_pte_masks>> shadow_memtype_mask = 0;
+ * 在以下使用shadow_memtype_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|4625| <<__kvm_mmu_honors_guest_mtrrs>> return vm_has_noncoherent_dma && shadow_memtype_mask;
+ *   - arch/x86/kvm/mmu/spte.c|192| <<make_spte>> if (shadow_memtype_mask)
+ *
+ * 对于EPT,
+ * bit-3/4/5(VMX_EPT_MT_MASK): EPT memory type for this 4-KByte page (see Section 29.3.7).
+ * bit-6(VMX_EPT_IPAT_BIT): Ignore PAT memory type for this 4-KByte page (see Section 29.3.7).
+ */
 u64 __read_mostly shadow_memtype_mask;
+/*
+ * 在以下设置shadow_me_value:
+ *   - arch/x86/kvm/mmu/spte.c|625| <<kvm_mmu_set_me_spte_mask>> shadow_me_value = me_value;
+ *   - arch/x86/kvm/mmu/spte.c|721| <<kvm_mmu_reset_all_pte_masks>> shadow_me_value = 0;
+ * 在以下使用shadow_me_value:
+ *   - arch/x86/kvm/mmu/mmu.c|3741| <<mmu_alloc_direct_roots>> mmu->pae_root[i] = root | PT_PRESENT_MASK | shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|3879| <<mmu_alloc_shadow_roots>> pm_mask = PT_PRESENT_MASK | shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|5077| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[0][i] &= ~shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|5078| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[1][i] &= ~shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|6207| <<__kvm_mmu_create>> WARN_ON_ONCE(shadow_me_value);
+ *   - arch/x86/kvm/mmu/spte.c|381| <<make_spte>> if (shadow_me_value && !kvm_is_mmio_pfn(pfn))
+ *   - arch/x86/kvm/mmu/spte.c|382| <<make_spte>> spte |= shadow_me_value;
+ *   - arch/x86/kvm/mmu/spte.c|496| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK | shadow_user_mask | shadow_x_mask | shadow_me_value;
+ *
+ * 用在memory encryption
+ * 对于vmx: me_value = 0
+ */
 u64 __read_mostly shadow_me_value;
+/*
+ * 在以下设置shadow_me_mask:
+ *   - arch/x86/kvm/mmu/spte.c|518| <<kvm_mmu_set_me_spte_mask>> shadow_me_mask = me_mask;
+ *   - arch/x86/kvm/mmu/spte.c|612| <<kvm_mmu_reset_all_pte_masks>> shadow_me_mask = 0;
+ * 在以下使用shadow_me_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|5065| <<reset_shadow_zero_bits_mask>> if (!shadow_me_mask)
+ *   - arch/x86/kvm/mmu/mmu.c|5075| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[0][i] |= shadow_me_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|5076| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[1][i] |= shadow_me_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|5110| <<reset_tdp_shadow_zero_bits_mask>> if (!shadow_me_mask)
+ *   - arch/x86/kvm/mmu/mmu.c|5114| <<reset_tdp_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[0][i] &= ~shadow_me_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|5115| <<reset_tdp_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[1][i] &= ~shadow_me_mask;
+ *   - arch/x86/kvm/mmu/spte.h|73| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *
+ * 用在memory encryption
+ * 对于vmx: me_mask = rsvd_bits(boot_cpu_data.x86_phys_bits, kvm_get_shadow_phys_bits() - 1);
+ */
 u64 __read_mostly shadow_me_mask;
+/*
+ * 在以下设置shadow_acc_track_mask:
+ *   - arch/x86/kvm/mmu/spte.c|550| <<kvm_mmu_set_ept_masks>> shadow_acc_track_mask = VMX_EPT_RWX_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|611| <<kvm_mmu_reset_all_pte_masks>> shadow_acc_track_mask = 0;
+ * 在以下使用shadow_acc_track_mask:
+ *   - arch/x86/kvm/mmu/spte.c|448| <<mark_spte_for_access_track>> spte &= ~shadow_acc_track_mask;
+ *   - arch/x86/kvm/mmu/spte.h|364| <<is_access_track_spte>> return !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;
+ *   - arch/x86/kvm/mmu/spte.h|545| <<restore_acc_track_spte>> spte &= ~shadow_acc_track_mask;
+ *
+ * 注释:
+ * SPTEs in MMUs without A/D bits are marked with SPTE_TDP_AD_DISABLED;
+ * shadow_acc_track_mask is the set of bits to be cleared in non-accessed
+ * pages.
+ *
+ * 用来trap对page的access
+ * R, W, X
+ */
 u64 __read_mostly shadow_acc_track_mask;
 
+/*
+ * commit 28a1f3ac1d0c8558ee4453d9634dad891a6e922e
+ * Author: Junaid Shahid <junaids@google.com>
+ * Date:   Tue Aug 14 10:15:34 2018 -0700
+ *
+ * kvm: x86: Set highest physical address bits in non-present/reserved SPTEs
+ *
+ * Always set the 5 upper-most supported physical address bits to 1 for SPTEs
+ * that are marked as non-present or reserved, to make them unusable for
+ * L1TF attacks from the guest. Currently, this just applies to MMIO SPTEs.
+ * (We do not need to mark PTEs that are completely 0 as physical page 0
+ * is already reserved.)
+ *
+ * This allows mitigation of L1TF without disabling hyper-threading by using
+ * shadow paging mode instead of EPT.
+ *
+ * Signed-off-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ *
+ * 在以下设置shadow_nonpresent_or_rsvd_mask:
+ *   - arch/x86/kvm/mmu/spte.c|584| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_mask = 0;
+ *   - arch/x86/kvm/mmu/spte.c|591| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_mask = rsvd_bits(low_phys_bits, boot_cpu_data.x86_cache_bits - 1);
+ * 在以下使用shadow_nonpresent_or_rsvd_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|306| <<get_mmio_spte_gfn>> gpa |= (spte >> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN) & shadow_nonpresent_or_rsvd_mask;
+ *   - arch/x86/kvm/mmu/spte.c|163| <<make_mmio_spte>> spte |= gpa | shadow_nonpresent_or_rsvd_mask;
+ *   - arch/x86/kvm/mmu/spte.c|164| <<make_mmio_spte>> spte |= (gpa & shadow_nonpresent_or_rsvd_mask)
+ *   - arch/x86/kvm/mmu/spte.c|488| <<kvm_mmu_set_mmio_spte_mask>> if (WARN_ON(mmio_value & (shadow_nonpresent_or_rsvd_mask << SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)))
+ *
+ * 测试的例子 (不支持ME):
+ * boot_cpu_data.x86_phys_bits = 46
+ * boot_cpu_data.x86_cache_bits = 46
+ * shadow_phys_bits=46
+ * bit: 41 - 45 (5个bit)
+ * shadow_nonpresent_or_rsvd_mask=0x00003e0000000000
+ * bit: 12 - 40
+ * shadow_nonpresent_or_rsvd_lower_gfn_mask=0x000001fffffff000
+ *
+ * 注释:
+ * This mask must be set on all non-zero Non-Present or Reserved SPTEs in order
+ * to guard against L1TF attacks.
+ */
 u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
+/*
+ * 在以下设置shadow_nonpresent_or_rsvd_lower_gfn_mask:
+ *   - arch/x86/kvm/mmu/spte.c|524| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_lower_gfn_mask = rsvd_bits(low_phys_bits, boot_cpu_data.x86_cache_bits - 1);
+ * 在以下使用shadow_nonpresent_or_rsvd_lower_gfn_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|303| <<get_mmio_spte_gfn>> u64 gpa = spte & shadow_nonpresent_or_rsvd_lower_gfn_mask;
+ *   - arch/x86/kvm/mmu/spte.c|401| <<kvm_mmu_set_mmio_spte_mask>> WARN_ON(mmio_value & shadow_nonpresent_or_rsvd_lower_gfn_mask);
+ */
 u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
+/*
+ * 在以下设置shadow_phys_bits:
+ *   - arch/x86/kvm/mmu/spte.c|501| <<kvm_mmu_reset_all_pte_masks>> shadow_phys_bits = kvm_get_shadow_phys_bits();
+ * 在以下使用shadow_phys_bits:
+ *   - arch/x86/kvm/mmu.h|79| <<kvm_mmu_max_gfn>> int max_gpa_bits = likely(tdp_enabled) ? shadow_phys_bits : 52;
+ *   - arch/x86/kvm/mmu/mmu.c|5038| <<reserved_hpa_bits>> return rsvd_bits(shadow_phys_bits, 63);
+ *   - arch/x86/kvm/mmu/spte.c|554| <<kvm_mmu_reset_all_pte_masks>> if (shadow_phys_bits < 52)
+ *
+ * The number of non-reserved physical address bits irrespective of features
+ * that repurpose legal bits, e.g. MKTME.
+ * host phys bit最多支持的bit数目
+ */
 u8 __read_mostly shadow_phys_bits;
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> __kvm_x86_vendor_init()
+ *       -> kvm_mmu_vendor_module_init()
+ *          -> kvm_mmu_reset_all_pte_masks()
+ *             -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *    -> ops->hardware_setup = hardware_setup()
+ *       -> kvm_mmu_set_ept_masks()
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull; (1 << 8)
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7087| <<kvm_mmu_x86_module_init>> kvm_mmu_spte_module_init();
+ *
+ * module_init(kvm_x86_init): kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ */
 void __init kvm_mmu_spte_module_init(void)
 {
+	/*
+	 * 在以下设置enable_mmio_caching:
+	 *   - arch/x86/kvm/mmu/spte.c|388| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = allow_mmio_caching;
+	 *   - arch/x86/kvm/mmu/spte.c|422| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = false;
+	 *
+	 * 在以下设置allow_mmio_caching:
+	 *   - arch/x86/kvm/mmu/spte.c|64| <<kvm_mmu_spte_module_init>> allow_mmio_caching = enable_mmio_caching;
+	 *
+	 * 下面注释的vendor module应该是kvm_intel
+	 */
 	/*
 	 * Snapshot userspace's desire to allow MMIO caching.  Whether or not
 	 * KVM can actually enable MMIO caching depends on vendor-specific
@@ -57,6 +524,10 @@ void __init kvm_mmu_spte_module_init(void)
 	allow_mmio_caching = enable_mmio_caching;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|471| <<make_mmio_spte>> u64 spte = generation_mmio_spte_mask(gen);
+ */
 static u64 generation_mmio_spte_mask(u64 gen)
 {
 	u64 mask;
@@ -68,16 +539,46 @@ static u64 generation_mmio_spte_mask(u64 gen)
 	return mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|301| <<mark_mmio_spte>> u64 spte = make_mmio_spte(vcpu, gfn, access);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1062| <<tdp_mmu_map_handle_target_level>> new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
+ */
 u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)
 {
 	u64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;
+	/*
+	 * 只在此处调用
+	 */
 	u64 spte = generation_mmio_spte_mask(gen);
 	u64 gpa = gfn << PAGE_SHIFT;
 
+	/*
+	 * 在以下设置shadow_mmio_value:
+	 *   - arch/x86/kvm/mmu/spte.c|444| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_value = mmio_value;
+	 *
+	 * 应该是只有mmio cache被enable的时候才用
+	 *
+	 * 被shadow_mmio_mask后什么样子的value代表这是mmio entry
+	 */
 	WARN_ON_ONCE(!shadow_mmio_value);
 
+	/*
+	 * 比如: ept是0
+	 * regular pte是ACC_WRITE_MASK | ACC_USER_MASK
+	 */
 	access &= shadow_mmio_access_mask;
 	spte |= shadow_mmio_value | access;
+	/*
+	 * 测试的例子 (不支持ME):
+	 * boot_cpu_data.x86_phys_bits = 46
+	 * boot_cpu_data.x86_cache_bits = 46
+	 * shadow_phys_bits=46
+	 * bit: 41 - 45 (5个bit)
+	 * shadow_nonpresent_or_rsvd_mask=0x00003e0000000000
+	 * bit: 12 - 40
+	 * shadow_nonpresent_or_rsvd_lower_gfn_mask=0x000001fffffff000
+	 */
 	spte |= gpa | shadow_nonpresent_or_rsvd_mask;
 	spte |= (gpa & shadow_nonpresent_or_rsvd_mask)
 		<< SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;
@@ -85,6 +586,13 @@ u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|616| <<make_spte>> spte |= static_call(kvm_x86_get_mt_mask)(vcpu, gfn, kvm_is_mmio_pfn(pfn));
+ *   - arch/x86/kvm/mmu/spte.c|622| <<make_spte>> if (shadow_me_value && !kvm_is_mmio_pfn(pfn))
+ *
+ * 确认host的pfn是否是mmio
+ */
 static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
 {
 	if (pfn_valid(pfn))
@@ -111,6 +619,12 @@ static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
  * The caller is responsible for checking if the SPTE is shadow-present, and
  * for determining whether or not the caller cares about non-leaf SPTEs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|507| <<mmu_spte_update_no_track>> if (!spte_has_volatile_bits(old_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|575| <<mmu_spte_clear_track_bits>> !spte_has_volatile_bits(old_spte))
+ *   - arch/x86/kvm/mmu/tdp_iter.h|59| <<kvm_tdp_mmu_spte_need_atomic_write>> spte_has_volatile_bits(old_spte);
+ */
 bool spte_has_volatile_bits(u64 spte)
 {
 	/*
@@ -134,16 +648,44 @@ bool spte_has_volatile_bits(u64 spte)
 	return false;
 }
 
+/*
+ * 在以下调用make_nonleaf_spte():
+ *   - arch/x86/kvm/mmu/mmu.c|2473| <<__link_shadow_page>> spte = make_nonleaf_spte(sp->spt, sp_ad_disabled(sp));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1115| <<tdp_mmu_link_sp>> u64 spte = make_nonleaf_spte(sp->spt, !kvm_ad_enabled());
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2983| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch, true, host_writable, &spte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|963| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access, gfn, spte_to_pfn(spte), spte, true, false, host_writable, &spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1064| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte, fault->prefetch, true,
+ *                                                 fault->map_writable, &new_spte);
+ */
 bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	       const struct kvm_memory_slot *slot,
 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
 	       u64 old_spte, bool prefetch, bool can_unsync,
 	       bool host_writable, u64 *new_spte)
 {
+	/*
+	 * 猜测sp是要制作的pte所在的level
+	 */
 	int level = sp->role.level;
+	/*
+	 * BIT_ULL(11)
+	 */
 	u64 spte = SPTE_MMU_PRESENT_MASK;
 	bool wrprot = false;
 
+	/*
+	 * 在以下设置shadow_present_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|468| <<kvm_mmu_set_ept_masks>> shadow_present_mask = has_exec_only ? 0ull : VMX_EPT_READABLE_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|532| <<kvm_mmu_reset_all_pte_masks>> shadow_present_mask = PT_PRESENT_MASK;
+	 *
+	 * 注释: 因为ept支持exec bit, 所以用shadow_user_mask表示readable.
+	 * For the EPT case, shadow_present_mask is 0 if hardware
+	 * supports exec-only page table entries.  In that case,
+	 * ACC_USER_MASK and shadow_user_mask are used to represent
+	 * read access.  See FNAME(gpte_access) in paging_tmpl.h.
+	 */
 	WARN_ON_ONCE(!pte_access && !shadow_present_mask);
 
 	if (sp->role.ad_disabled)
@@ -151,6 +693,11 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	else if (kvm_mmu_page_ad_need_write_protect(sp))
 		spte |= SPTE_TDP_AD_WRPROT_ONLY;
 
+	/*
+	 * 在以下设置shadow_present_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|468| <<kvm_mmu_set_ept_masks>> shadow_present_mask = has_exec_only ? 0ull : VMX_EPT_READABLE_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|532| <<kvm_mmu_reset_all_pte_masks>> shadow_present_mask = PT_PRESENT_MASK;
+	 */
 	/*
 	 * For the EPT case, shadow_present_mask is 0 if hardware
 	 * supports exec-only page table entries.  In that case,
@@ -158,6 +705,9 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	 * read access.  See FNAME(gpte_access) in paging_tmpl.h.
 	 */
 	spte |= shadow_present_mask;
+	/*
+	 * 不是prefetch说明是page fault, 标记被访问过
+	 */
 	if (!prefetch)
 		spte |= spte_shadow_accessed_mask(spte);
 
@@ -175,9 +725,24 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	 */
 	if (level > PG_LEVEL_4K && (pte_access & ACC_EXEC_MASK) &&
 	    is_nx_huge_page_enabled(vcpu->kvm)) {
+		/*
+		 * 三个条件:
+		 * 1. level > PG_LEVEL_4K
+		 * 2. (pte_access & ACC_EXEC_MASK)
+		 * 3. is_nx_huge_page_enabled(vcpu->kvm)
+		 */
 		pte_access &= ~ACC_EXEC_MASK;
 	}
 
+	/*
+	 * 普通page table entry的bit=63 (XD):
+	 * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+	 * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+	 * otherwise, reserved (must be 0)
+	 *
+	 * 普通pte支持nx bit, ept不支持nx bit
+	 * 普通pte不支持x bit, ept支持x bit
+	 */
 	if (pte_access & ACC_EXEC_MASK)
 		spte |= shadow_x_mask;
 	else
@@ -186,9 +751,27 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	if (pte_access & ACC_USER_MASK)
 		spte |= shadow_user_mask;
 
+	/*
+	 * 1是page table entry,
+	 * 否则指向的是page (或者huge page)
+	 */
 	if (level > PG_LEVEL_4K)
 		spte |= PT_PAGE_SIZE_MASK;
 
+	/*
+	 * 在以下设置shadow_memtype_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|439| <<kvm_mmu_set_ept_masks>> shadow_memtype_mask = VMX_EPT_MT_MASK | VMX_EPT_IPAT_BIT;
+	 *   - arch/x86/kvm/mmu/spte.c|496| <<kvm_mmu_reset_all_pte_masks>> shadow_memtype_mask = 0;
+	 * 在以下使用shadow_memtype_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|4625| <<__kvm_mmu_honors_guest_mtrrs>> return vm_has_noncoherent_dma && shadow_memtype_mask;
+	 *   - arch/x86/kvm/mmu/spte.c|192| <<make_spte>> if (shadow_memtype_mask)
+	 *
+	 * 对于EPT,
+	 * bit-3/4/5(VMX_EPT_MT_MASK): EPT memory type for this 4-KByte page (see Section 29.3.7).
+	 * bit-6(VMX_EPT_IPAT_BIT): Ignore PAT memory type for this 4-KByte page (see Section 29.3.7).
+	 *
+	 * vmx_get_mt_mask()
+	 */
 	if (shadow_memtype_mask)
 		spte |= static_call(kvm_x86_get_mt_mask)(vcpu, gfn,
 							 kvm_is_mmio_pfn(pfn));
@@ -203,6 +786,9 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	spte |= (u64)pfn << PAGE_SHIFT;
 
 	if (pte_access & ACC_WRITE_MASK) {
+		/*
+		 * 标记mmu和entry本身writable
+		 */
 		spte |= PT_WRITABLE_MASK | shadow_mmu_writable_mask;
 
 		/*
@@ -231,6 +817,9 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 		spte |= spte_shadow_dirty_mask(spte);
 
 out:
+	/*
+	 * regular pte一定支持accessed, ept不一定
+	 */
 	if (prefetch)
 		spte = mark_spte_for_access_track(spte);
 
@@ -238,7 +827,13 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 		  "spte = 0x%llx, level = %d, rsvd bits = 0x%llx", spte, level,
 		  get_rsvd_bits(&vcpu->arch.mmu->shadow_zero_check, spte, level));
 
+	/*
+	 * 如果正在
+	 */
 	if ((spte & PT_WRITABLE_MASK) && kvm_slot_dirty_track_enabled(slot)) {
+		/*
+		 * 如果正在track就不能是大于4K了
+		 */
 		/* Enforced by kvm_mmu_hugepage_adjust. */
 		WARN_ON_ONCE(level > PG_LEVEL_4K);
 		mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
@@ -248,6 +843,10 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	return wrprot;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|758| <<make_huge_page_split_spte>> child_spte = make_spte_executable(child_spte);
+ */
 static u64 make_spte_executable(u64 spte)
 {
 	bool is_access_track = is_access_track_spte(spte);
@@ -255,6 +854,19 @@ static u64 make_spte_executable(u64 spte)
 	if (is_access_track)
 		spte = restore_acc_track_spte(spte);
 
+	/*
+	 * 在以下设置shadow_nx_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|590| <<kvm_mmu_set_ept_masks>> shadow_nx_mask = 0ull;
+	 *   - arch/x86/kvm/mmu/spte.c|658| <<kvm_mmu_reset_all_pte_masks>> shadow_nx_mask = PT64_NX_MASK;
+	 *
+	 * 普通page table entry的bit=63 (XD):
+	 * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+	 * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+	 * otherwise, reserved (must be 0)
+	 *
+	 * 普通pte支持nx bit, ept不支持nx bit
+	 * 普通pte不支持x bit, ept支持x bit
+	 */
 	spte &= ~shadow_nx_mask;
 	spte |= shadow_x_mask;
 
@@ -271,6 +883,11 @@ static u64 make_spte_executable(u64 spte)
  * This is used during huge page splitting to build the SPTEs that make up the
  * new page table.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6707| <<shadow_mmu_split_huge_page>> spte = make_huge_page_split_spte(kvm, huge_spte, sp->role, index);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1498| <<tdp_mmu_split_huge_page>> sp->spt[i] = make_huge_page_split_spte(kvm, huge_spte, sp->role, i);
+ */
 u64 make_huge_page_split_spte(struct kvm *kvm, u64 huge_spte, union kvm_mmu_page_role role,
 			      int index)
 {
@@ -284,6 +901,13 @@ u64 make_huge_page_split_spte(struct kvm *kvm, u64 huge_spte, union kvm_mmu_page
 
 	child_spte = huge_spte;
 
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+	 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+	 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+	 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+	 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+	 */
 	/*
 	 * The child_spte already has the base address of the huge page being
 	 * split. So we just have to OR in the offset to the page at the next
@@ -307,13 +931,28 @@ u64 make_huge_page_split_spte(struct kvm *kvm, u64 huge_spte, union kvm_mmu_page
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2473| <<__link_shadow_page>> spte = make_nonleaf_spte(sp->spt, sp_ad_disabled(sp));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1115| <<tdp_mmu_link_sp>> u64 spte = make_nonleaf_spte(sp->spt, !kvm_ad_enabled());
+ */
 u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 {
+	/*
+	 * bit 11
+	 */
 	u64 spte = SPTE_MMU_PRESENT_MASK;
 
 	spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK |
 		shadow_user_mask | shadow_x_mask | shadow_me_value;
 
+	/*
+	 * EPT: PML4E的bit8:
+	 *  If bit 6 of EPTP is 1, accessed flag for EPT; indicates whether
+	 *  software has accessed the 512-GByte region
+	 *  controlled by this entry (see Section 29.3.5). Ignored if bit 6 of
+	 *  EPTP is 0.
+	 */
 	if (ad_disabled)
 		spte |= SPTE_TDP_AD_DISABLED;
 	else
@@ -322,13 +961,27 @@ u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1527| <<kvm_set_pte_rmap>> new_spte = kvm_mmu_changed_pte_notifier_make_spte(*sptep, new_pfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1338| <<set_spte_gfn>> new_spte = kvm_mmu_changed_pte_notifier_make_spte(iter->old_spte, pte_pfn(range->arg.pte));
+ */
 u64 kvm_mmu_changed_pte_notifier_make_spte(u64 old_spte, kvm_pfn_t new_pfn)
 {
 	u64 new_spte;
 
+	/*
+	 * 在没有CONFIG_DYNAMIC_PHYSICAL_MASK的时候是0xffffffffff000 (13个hex)
+	 * ~SPTE_BASE_ADDR_MASK
+	 */
 	new_spte = old_spte & ~SPTE_BASE_ADDR_MASK;
 	new_spte |= (u64)new_pfn << PAGE_SHIFT;
 
+	/*
+	 * pte本身不writable
+	 * host不writable
+	 * mmu不writable
+	 */
 	new_spte &= ~PT_WRITABLE_MASK;
 	new_spte &= ~shadow_host_writable_mask;
 	new_spte &= ~shadow_mmu_writable_mask;
@@ -338,27 +991,101 @@ u64 kvm_mmu_changed_pte_notifier_make_spte(u64 old_spte, kvm_pfn_t new_pfn)
 	return new_spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|633| <<mmu_spte_age>> spte = mark_spte_for_access_track(spte);
+ *   - arch/x86/kvm/mmu/spte.c|326| <<make_spte>> spte = mark_spte_for_access_track(spte);
+ *   - arch/x86/kvm/mmu/spte.c|353| <<make_spte_executable>> spte = mark_spte_for_access_track(spte);
+ *   - arch/x86/kvm/mmu/spte.c|427| <<kvm_mmu_changed_pte_notifier_make_spte>> new_spte = mark_spte_for_access_track(new_spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1290| <<age_gfn_range>> new_spte = mark_spte_for_access_track(iter->old_spte);
+ *
+ * regular pte一定支持accessed, ept不一定
+ */
 u64 mark_spte_for_access_track(u64 spte)
 {
+	/*
+	 * 在以下设置shadow_accessed_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|464| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+	 *   - arch/x86/kvm/mmu/spte.c|528| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+	 *
+	 * 清空accessed的bit, 用于以后track
+	 */
 	if (spte_ad_enabled(spte))
 		return spte & ~shadow_accessed_mask;
 
+	/*
+	 * 下面的就是不支持硬件ad的情况了
+	 */
+
+	/*
+	 * 比如: ept硬件不支持access bit, 并且ept的0/1/2都没设置
+	 */
 	if (is_access_track_spte(spte))
 		return spte;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|500| <<mmu_spte_update_no_track>> check_spte_writable_invariants(new_spte);
+	 *   - arch/x86/kvm/mmu/spte.c|675| <<mark_spte_for_access_track>> check_spte_writable_invariants(spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|510| <<handle_changed_spte>> check_spte_writable_invariants(new_spte);
+	 *
+	 * 1. 如果mmu是writable, 但是host不writable, 错误!
+	 * 2. 如果mmu不writable, 但是pte本身writable, 错误! (可能mmio???)
+	 */
 	check_spte_writable_invariants(spte);
 
 	WARN_ONCE(spte & (SHADOW_ACC_TRACK_SAVED_BITS_MASK <<
 			  SHADOW_ACC_TRACK_SAVED_BITS_SHIFT),
 		  "Access Tracking saved bit locations are not zero\n");
 
+	/*
+	 * 把第一位和第四位保存到54位
+	 */
 	spte |= (spte & SHADOW_ACC_TRACK_SAVED_BITS_MASK) <<
 		SHADOW_ACC_TRACK_SAVED_BITS_SHIFT;
+	/*
+	 * 在以下设置shadow_acc_track_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|550| <<kvm_mmu_set_ept_masks>> shadow_acc_track_mask = VMX_EPT_RWX_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|611| <<kvm_mmu_reset_all_pte_masks>> shadow_acc_track_mask = 0;
+	 */
 	spte &= ~shadow_acc_track_mask;
 
 	return spte;
 }
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> __kvm_x86_vendor_init()
+ *       -> kvm_mmu_vendor_module_init()
+ *          -> kvm_mmu_reset_all_pte_masks()
+ *             -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *             -> kvm_mmu_set_mmio_spte_mask()
+ *    -> ops->hardware_setup = hardware_setup()
+ *       -> kvm_mmu_set_ept_masks()
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull; (1 << 8)
+ *          -> kvm_mmu_set_mmio_spte_mask()
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|491| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK, 0);
+ *   - arch/x86/kvm/mmu/spte.c|559| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+ *   - arch/x86/kvm/svm/svm.c|5074| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+ *
+ * 初始化那些mask和value
+ */
 void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 {
 	BUG_ON((u64)(unsigned)access_mask != access_mask);
@@ -411,6 +1138,13 @@ void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mmio_spte_mask);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5233| <<svm_hardware_setup>> kvm_mmu_set_me_spte_mask(sme_me_mask, sme_me_mask);
+ *   - arch/x86/kvm/vmx/vmx.c|8490| <<vmx_setup_me_spte_mask>> kvm_mmu_set_me_spte_mask(0, me_mask);
+ *
+ * 用在memory encryption
+ */
 void kvm_mmu_set_me_spte_mask(u64 me_value, u64 me_mask)
 {
 	/* shadow_me_value must be a subset of shadow_me_mask */
@@ -422,14 +1156,57 @@ void kvm_mmu_set_me_spte_mask(u64 me_value, u64 me_mask)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_me_spte_mask);
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> __kvm_x86_vendor_init()
+ *       -> kvm_mmu_vendor_module_init()
+ *          -> kvm_mmu_reset_all_pte_masks()
+ *             -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *             -> kvm_mmu_set_mmio_spte_mask()
+ *    -> ops->hardware_setup = hardware_setup()
+ *       -> kvm_mmu_set_ept_masks()
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull; (1 << 8)
+ *          -> kvm_mmu_set_mmio_spte_mask()
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8604| <<hardware_setup>> if (enable_ept) kvm_mmu_set_ept_masks(enable_ept_ad_bits, cpu_has_vmx_ept_execute_only());
+ *
+ * 只在vmx调用
+ */
 void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)
 {
 	shadow_user_mask	= VMX_EPT_READABLE_MASK;
+	/*
+	 * 在以下设置shadow_accessed_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|464| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+	 *   - arch/x86/kvm/mmu/spte.c|528| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+	 */
 	shadow_accessed_mask	= has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
 	shadow_dirty_mask	= has_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull;
 	shadow_nx_mask		= 0ull;
 	shadow_x_mask		= VMX_EPT_EXECUTABLE_MASK;
 	shadow_present_mask	= has_exec_only ? 0ull : VMX_EPT_READABLE_MASK;
+	/*
+	 * 在以下使用shadow_memtype_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|4625| <<__kvm_mmu_honors_guest_mtrrs>> return vm_has_noncoherent_dma && shadow_memtype_mask;
+	 *   - arch/x86/kvm/mmu/spte.c|192| <<make_spte>> if (shadow_memtype_mask)
+	 *   - arch/x86/kvm/mmu/spte.c|439| <<kvm_mmu_set_ept_masks>> shadow_memtype_mask = VMX_EPT_MT_MASK | VMX_EPT_IPAT_BIT;
+	 *   - arch/x86/kvm/mmu/spte.c|496| <<kvm_mmu_reset_all_pte_masks>> shadow_memtype_mask = 0;
+	 */
 	/*
 	 * EPT overrides the host MTRRs, and so KVM must program the desired
 	 * memtype directly into the SPTEs.  Note, this mask is just the mask
@@ -450,11 +1227,47 @@ void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_ept_masks);
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> __kvm_x86_vendor_init()
+ *       -> kvm_mmu_vendor_module_init()
+ *          -> kvm_mmu_reset_all_pte_masks()
+ *             -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *             -> kvm_mmu_set_mmio_spte_mask()
+ *    -> ops->hardware_setup = hardware_setup()
+ *       -> kvm_mmu_set_ept_masks()
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull; (1 << 8)
+ *          -> kvm_mmu_set_mmio_spte_mask()
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7113| <<kvm_mmu_vendor_module_init>> kvm_mmu_reset_all_pte_masks();
+ */
 void kvm_mmu_reset_all_pte_masks(void)
 {
 	u8 low_phys_bits;
 	u64 mask;
 
+	/*
+	 * 测试:
+	 * boot_cpu_data.x86_phys_bits = 46
+	 * boot_cpu_data.x86_cache_bits = 46
+	 * shadow_phys_bits=46
+	 * low_phys_bits=46
+	 */
 	shadow_phys_bits = kvm_get_shadow_phys_bits();
 
 	/*
@@ -468,16 +1281,37 @@ void kvm_mmu_reset_all_pte_masks(void)
 	 * the most significant bits of legal physical address space.
 	 */
 	shadow_nonpresent_or_rsvd_mask = 0;
+	/*
+	 * boot_cpu_data.x86_phys_bits的注释:
+	 * boot_cpu_data.x86_phys_bits is reduced when MKTME or SME are detected
+	 * in CPU detection code, but the processor treats those reduced bits as
+	 * 'keyID' thus they are not reserved bits. Therefore KVM needs to look at
+	 * the physical address bits reported by CPUID.
+	 */
 	low_phys_bits = boot_cpu_data.x86_phys_bits;
 	if (boot_cpu_has_bug(X86_BUG_L1TF) &&
 	    !WARN_ON_ONCE(boot_cpu_data.x86_cache_bits >=
 			  52 - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)) {
+		/*
+		 * 46 - 5 = 41
+		 */
 		low_phys_bits = boot_cpu_data.x86_cache_bits
 			- SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;
+		/*
+		 * bit: 41 - 45 (5个bit)
+		 * shadow_nonpresent_or_rsvd_mask=0x00003e0000000000
+		 */
 		shadow_nonpresent_or_rsvd_mask =
 			rsvd_bits(low_phys_bits, boot_cpu_data.x86_cache_bits - 1);
 	}
 
+	/*
+	 * bit: 12 - 40
+	 * shadow_nonpresent_or_rsvd_lower_gfn_mask=0x000001fffffff000
+	 *
+	 * high: low_phys_bits - 1
+	 * low : PAGE_SHIFT
+	 */
 	shadow_nonpresent_or_rsvd_lower_gfn_mask =
 		GENMASK_ULL(low_phys_bits - 1, PAGE_SHIFT);
 
@@ -513,5 +1347,8 @@ void kvm_mmu_reset_all_pte_masks(void)
 	else
 		mask = 0;
 
+	/*
+	 * 1 << 1 | 1 << 2
+	 */
 	kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
 }
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index a129951c9..cfac9d223 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -6,6 +6,17 @@
 #include "mmu.h"
 #include "mmu_internal.h"
 
+/*
+ * 在以下使用SPTE_MMU_PRESENT_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|124| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+ *   - arch/x86/kvm/mmu/spte.h|139| <<global>> static_assert(!(SPTE_MMIO_ALLOWED_MASK & (SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+ *   - arch/x86/kvm/mmu/spte.h|200| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|151| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|328| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|260| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+ *
+ * mmio是不设置这个bit=11的
+ */
 /*
  * A MMU present SPTE is backed by actual memory and may or may not be present
  * in hardware.  E.g. MMIO SPTEs are not considered present.  Use bit 11, as it
@@ -15,6 +26,28 @@
  */
 #define SPTE_MMU_PRESENT_MASK		BIT_ULL(11)
 
+/*
+ * 在以下使用SPTE_TDP_AD_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|87| <<global>> static_assert(!(SPTE_TDP_AD_MASK & SHADOW_ACC_TRACK_SAVED_MASK));
+ *   - arch/x86/kvm/mmu/spte.h|106| <<global>> static_assert(!(EPT_SPTE_HOST_WRITABLE & SPTE_TDP_AD_MASK));
+ *   - arch/x86/kvm/mmu/spte.h|107| <<global>> static_assert(!(EPT_SPTE_MMU_WRITABLE & SPTE_TDP_AD_MASK));
+ *   - arch/x86/kvm/mmu/spte.h|316| <<spte_ad_enabled>> return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_DISABLED;
+ *   - arch/x86/kvm/mmu/spte.h|333| <<spte_ad_need_write_protect>> return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_ENABLED;
+ *
+ * 在以下使用SPTE_TDP_AD_ENABLED:
+ *   - arch/x86/kvm/mmu/spte.h|50| <<global>> static_assert(SPTE_TDP_AD_ENABLED == 0);
+ *   - arch/x86/kvm/mmu/spte.h|333| <<spte_ad_need_write_protect>> return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_ENABLED;
+ *
+ * 在以下使用SPTE_TDP_AD_DISABLED:
+ *   - arch/x86/kvm/mmu/spte.c|157| <<make_spte>> if (sp->role.ad_disabled) spte |= SPTE_TDP_AD_DISABLED;
+ *   - arch/x86/kvm/mmu/spte.c|334| <<make_nonleaf_spte>> if (ad_disabled) spte |= SPTE_TDP_AD_DISABLED;
+ *   - arch/x86/kvm/mmu/spte.h|310| <<spte_ad_enabled>> return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_DISABLED;
+ *
+ * 在以下使用SPTE_TDP_AD_WRPROT_ONLY:
+ *   - arch/x86/kvm/mmu/spte.c|159| <<make_spte>> spte |= SPTE_TDP_AD_WRPROT_ONLY;
+ *
+ * Page Modification Logging (PML)
+ */
 /*
  * TDP SPTES (more specifically, EPT SPTEs) may not have A/D bits, and may also
  * be restricted to using write-protection (for L2 when CPU dirty logging, i.e.
@@ -34,15 +67,43 @@
 #define SPTE_TDP_AD_WRPROT_ONLY		(2ULL << SPTE_TDP_AD_SHIFT)
 static_assert(SPTE_TDP_AD_ENABLED == 0);
 
+/*
+ * 在以下使用SPTE_BASE_ADDR_MASK:
+ *   - arch/x86/kvm/mmu/mmu.c|2450| <<shadow_walk_init_using_root>> iterator->shadow_addr &= SPTE_BASE_ADDR_MASK;
+ *   - arch/x86/kvm/mmu/mmu.c|2482| <<__shadow_walk_next>> iterator->shadow_addr = spte & SPTE_BASE_ADDR_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|892| <<kvm_mmu_changed_pte_notifier_make_spte>> new_spte = old_spte & ~SPTE_BASE_ADDR_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|294| <<spte_to_child_sp>> return to_shadow_page(spte & SPTE_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/spte.h|440| <<spte_to_pfn>> return (pte & SPTE_BASE_ADDR_MASK) >> PAGE_SHIFT;
+ */
 #ifdef CONFIG_DYNAMIC_PHYSICAL_MASK
 #define SPTE_BASE_ADDR_MASK (physical_mask & ~(u64)(PAGE_SIZE-1))
 #else
+/*
+ * ((1ULL << 52) - 1) = 0xfffffffffffff (13个f)
+ * ~(u64)(PAGE_SIZE-1) = 除了后12位都是0
+ * 在没有CONFIG_DYNAMIC_PHYSICAL_MASK的时候是0xffffffffff000 (13个hex)
+ */
 #define SPTE_BASE_ADDR_MASK (((1ULL << 52) - 1) & ~(u64)(PAGE_SIZE-1))
 #endif
 
 #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \
 			| shadow_x_mask | shadow_nx_mask | shadow_me_mask)
 
+/*
+ * 在以下使用ACC_ALL:
+ *   - arch/x86/kvm/mmu/mmu.c|773| <<kvm_mmu_page_get_access>> return sp->shadowed_translation[index] & ACC_ALL;
+ *   - arch/x86/kvm/mmu/mmu.c|3380| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|3393| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+ *   - arch/x86/kvm/mmu/mmu.c|4664| <<direct_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_tdp_mmu_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|5503| <<kvm_calc_cpu_role>> role.base.access = ACC_ALL;
+ *   - arch/x86/kvm/mmu/mmu.c|5572| <<kvm_calc_tdp_mmu_root_page_role>> role.access = ACC_ALL;
+ *   - arch/x86/kvm/mmu/mmu.c|5740| <<kvm_calc_shadow_ept_root_page_role>> role.base.access = ACC_ALL;
+ *   - arch/x86/kvm/mmu/mmutrace.h|225| <<__field>> __entry->access = spte & ACC_ALL;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1066| <<tdp_mmu_map_handle_target_level>> new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1068| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte, fault->prefetch, true,
+ *							fault->map_writable, &new_spte);
+ */
 #define ACC_EXEC_MASK    1
 #define ACC_WRITE_MASK   PT_WRITABLE_MASK
 #define ACC_USER_MASK    PT_USER_MASK
@@ -53,8 +114,27 @@ static_assert(SPTE_TDP_AD_ENABLED == 0);
 #define SPTE_EPT_EXECUTABLE_MASK		0x4ull
 
 #define SPTE_LEVEL_BITS			9
+/*
+ * 17 #define __PT_LEVEL_SHIFT(level, bits_per_level) \
+ * 18         (PAGE_SHIFT + ((level) - 1) * (bits_per_level))
+ *
+ * SPTE_LEVEL_SHIFT(5): 12 + 4 * 9 = 48
+ * SPTE_LEVEL_SHIFT(4): 12 + 3 * 9 = 39
+ * SPTE_LEVEL_SHIFT(3): 12 + 2 * 9 = 30
+ * SPTE_LEVEL_SHIFT(2): 12 + 1 * 9 = 21
+ * SPTE_LEVEL_SHIFT(1): 12 + 0 * 9 = 12
+ */
 #define SPTE_LEVEL_SHIFT(level)		__PT_LEVEL_SHIFT(level, SPTE_LEVEL_BITS)
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2499| <<shadow_walk_okay>> iterator->index = SPTE_INDEX(iterator->addr, iterator->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|21| <<tdp_iter_refresh_sptep>> iter->sptep = iter->pt_path[iter->level - 1] + SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|195| <<try_step_side>> if (SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level) == (SPTE_ENT_PER_PAGE - 1))
+ */
 #define SPTE_INDEX(address, level)	__PT_INDEX(address, level, SPTE_LEVEL_BITS)
+/*
+ * 512
+ */
 #define SPTE_ENT_PER_PAGE		__PT_ENT_PER_PAGE(SPTE_LEVEL_BITS)
 
 /*
@@ -64,9 +144,21 @@ static_assert(SPTE_TDP_AD_ENABLED == 0);
  * restored only when a write is attempted to the page.  This mask obviously
  * must not overlap the A/D type mask.
  */
+/*
+ * 在以下使用SHADOW_ACC_TRACK_SAVED_BITS_MASK:
+ *   - arch/x86/kvm/mmu/spte.c|697| <<mark_spte_for_access_track>> WARN_ONCE(spte & (SHADOW_ACC_TRACK_SAVED_BITS_MASK << SHADOW_ACC_TRACK_SAVED_BITS_SHIFT),
+ *                                                          "Access Tracking saved bit locations are not zero\n");
+ *   - arch/x86/kvm/mmu/spte.c|701| <<mark_spte_for_access_track>> spte |= (spte & SHADOW_ACC_TRACK_SAVED_BITS_MASK) << SHADOW_ACC_TRACK_SAVED_BITS_SHIFT;
+ *   - arch/x86/kvm/mmu/spte.h|99| <<SHADOW_ACC_TRACK_SAVED_MASK>> #define SHADOW_ACC_TRACK_SAVED_MASK (SHADOW_ACC_TRACK_SAVED_BITS_MASK << \ SHADOW_ACC_TRACK_SAVED_BITS_SHIFT)
+ *   - arch/x86/kvm/mmu/spte.h|597| <<restore_acc_track_spte>> u64 saved_bits = (spte >> SHADOW_ACC_TRACK_SAVED_BITS_SHIFT) & SHADOW_ACC_TRACK_SAVED_BITS_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|600| <<restore_acc_track_spte>> spte &= ~(SHADOW_ACC_TRACK_SAVED_BITS_MASK << SHADOW_ACC_TRACK_SAVED_BITS_SHIFT);
+ */
 #define SHADOW_ACC_TRACK_SAVED_BITS_MASK (SPTE_EPT_READABLE_MASK | \
 					  SPTE_EPT_EXECUTABLE_MASK)
 #define SHADOW_ACC_TRACK_SAVED_BITS_SHIFT 54
+/*
+ * 把bit0和bit2转移到bit54
+ */
 #define SHADOW_ACC_TRACK_SAVED_MASK	(SHADOW_ACC_TRACK_SAVED_BITS_MASK << \
 					 SHADOW_ACC_TRACK_SAVED_BITS_SHIFT)
 static_assert(!(SPTE_TDP_AD_MASK & SHADOW_ACC_TRACK_SAVED_MASK));
@@ -117,6 +209,13 @@ static_assert(!(EPT_SPTE_MMU_WRITABLE & SHADOW_ACC_TRACK_SAVED_MASK));
 #define MMIO_SPTE_GEN_HIGH_START	52
 #define MMIO_SPTE_GEN_HIGH_END		62
 
+/*
+ * 在以下使用
+ *   - arch/x86/kvm/mmu/spte.h|180| <<global>> (MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+ *   - arch/x86/kvm/mmu/spte.h|194| <<global>> (SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+ *   - arch/x86/kvm/mmu/spte.c|518| <<generation_mmio_spte_mask>> mask = (gen << MMIO_SPTE_GEN_LOW_SHIFT) & MMIO_SPTE_GEN_LOW_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|639| <<get_mmio_spte_generation>> gen = (spte & MMIO_SPTE_GEN_LOW_MASK) >> MMIO_SPTE_GEN_LOW_SHIFT;
+ */
 #define MMIO_SPTE_GEN_LOW_MASK		GENMASK_ULL(MMIO_SPTE_GEN_LOW_END, \
 						    MMIO_SPTE_GEN_LOW_START)
 #define MMIO_SPTE_GEN_HIGH_MASK		GENMASK_ULL(MMIO_SPTE_GEN_HIGH_END, \
@@ -134,10 +233,21 @@ static_assert(!(SPTE_MMU_PRESENT_MASK &
  * and so they're off-limits for generation; additional checks ensure the mask
  * doesn't overlap legal PA bits), and bit 63 (carved out for future usage).
  */
+/*
+ * 在以下使用SPTE_MMIO_ALLOWED_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|237| <<global>> static_assert(!(SPTE_MMIO_ALLOWED_MASK &
+ *   - arch/x86/kvm/mmu/spte.c|1106| <<kvm_mmu_set_mmio_spte_mask>> if (WARN_ON(mmio_mask & ~SPTE_MMIO_ALLOWED_MASK))
+ */
 #define SPTE_MMIO_ALLOWED_MASK (BIT_ULL(63) | GENMASK_ULL(51, 12) | GENMASK_ULL(2, 0))
 static_assert(!(SPTE_MMIO_ALLOWED_MASK &
 		(SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
 
+/*
+ * 在以下使用MMIO_SPTE_GEN_LOW_BITS:
+ *   - arch/x86/kvm/mmu/spte.h|244| <<global>> static_assert(MMIO_SPTE_GEN_LOW_BITS == 8 && MMIO_SPTE_GEN_HIGH_BITS == 11);
+ *   - arch/x86/kvm/mmu/spte.h|247| <<MMIO_SPTE_GEN_HIGH_SHIFT>> #define MMIO_SPTE_GEN_HIGH_SHIFT (MMIO_SPTE_GEN_HIGH_START - MMIO_SPTE_GEN_LOW_BITS)
+ *   - arch/x86/kvm/mmu/spte.h|249| <<MMIO_SPTE_GEN_MASK>> #define MMIO_SPTE_GEN_MASK GENMASK_ULL(MMIO_SPTE_GEN_LOW_BITS + MMIO_SPTE_GEN_HIGH_BITS - 1, 0)
+ */
 #define MMIO_SPTE_GEN_LOW_BITS		(MMIO_SPTE_GEN_LOW_END - MMIO_SPTE_GEN_LOW_START + 1)
 #define MMIO_SPTE_GEN_HIGH_BITS		(MMIO_SPTE_GEN_HIGH_END - MMIO_SPTE_GEN_HIGH_START + 1)
 
@@ -147,6 +257,13 @@ static_assert(MMIO_SPTE_GEN_LOW_BITS == 8 && MMIO_SPTE_GEN_HIGH_BITS == 11);
 #define MMIO_SPTE_GEN_LOW_SHIFT		(MMIO_SPTE_GEN_LOW_START - 0)
 #define MMIO_SPTE_GEN_HIGH_SHIFT	(MMIO_SPTE_GEN_HIGH_START - MMIO_SPTE_GEN_LOW_BITS)
 
+/*
+ * 在以下使用MMIO_SPTE_GEN_MASK:
+ *   - arch/x86/kvm/mmu/mmu.c|360| <<check_mmio_spte>> kvm_gen = gen & MMIO_SPTE_GEN_MASK;
+ *   - arch/x86/kvm/mmu/mmu.c|7216| <<kvm_mmu_invalidate_mmio_sptes>> gen &= MMIO_SPTE_GEN_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|533| <<generation_mmio_spte_mask>> WARN_ON_ONCE(gen & ~MMIO_SPTE_GEN_MASK);
+ *   - arch/x86/kvm/mmu/spte.c|547| <<make_mmio_spte>> u64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;
+ */
 #define MMIO_SPTE_GEN_MASK		GENMASK_ULL(MMIO_SPTE_GEN_LOW_BITS + MMIO_SPTE_GEN_HIGH_BITS - 1, 0)
 
 extern u64 __read_mostly shadow_host_writable_mask;
@@ -177,6 +294,15 @@ extern u64 __read_mostly shadow_acc_track_mask;
  */
 extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
 
+/*
+ * 在以下使用SHADOW_NONPRESENT_OR_RSVD_MASK_LEN:
+ *   - arch/x86/kvm/mmu/spte.h|226| <<global>> #define SHADOW_NONPRESENT_OR_RSVD_MASK_LEN 5
+ *   - arch/x86/kvm/mmu/mmu.c|311| <<get_mmio_spte_gfn>> gpa |= (spte >> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)
+ *   - arch/x86/kvm/mmu/spte.c|499| <<make_mmio_spte>> << SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;
+ *   - arch/x86/kvm/mmu/spte.c|1034| <<kvm_mmu_set_mmio_spte_mask>> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)))
+ *   - arch/x86/kvm/mmu/spte.c|1195| <<kvm_mmu_reset_all_pte_masks>> 52 - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)) {
+ *   - arch/x86/kvm/mmu/spte.c|1197| <<kvm_mmu_reset_all_pte_masks>> - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;
+ */
 /*
  * The number of high-order 1 bits to use in the mask above.
  */
@@ -194,16 +320,50 @@ extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
  *
  * Only used by the TDP MMU.
  */
+/*
+ * 在以下使用REMOVED_SPTE:
+ *   - arch/x86/kvm/mmu/spte.h|209| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/mmu.c|3471| <<fast_page_fault>> spte = REMOVED_SPTE;
+ *   - arch/x86/kvm/mmu/spte.c|418| <<kvm_mmu_set_mmio_spte_mask>> WARN_ON(mmio_value && (REMOVED_SPTE & mmio_mask) == mmio_value))
+ *   - arch/x86/kvm/mmu/spte.h|213| <<is_removed_spte>> return spte == REMOVED_SPTE;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|335| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|383| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, REMOVED_SPTE, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|386| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<tdp_mmu_zap_spte_atomic>> ret = tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE);
+ */
 #define REMOVED_SPTE	0x5a0ULL
 
 /* Removed SPTEs must not be misconstrued as shadow present PTEs. */
 static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|353| <<handle_removed_pt>> if (!is_removed_spte(old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|484| <<handle_changed_spte>> !is_removed_spte(new_spte)))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|546| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded || is_removed_spte(iter->old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|621| <<tdp_mmu_set_spte>> WARN_ON_ONCE(is_removed_spte(old_spte) || is_removed_spte(new_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1073| <<kvm_tdp_mmu_map>> if (is_removed_spte(iter.old_spte))
+ */
 static inline bool is_removed_spte(u64 spte)
 {
 	return spte == REMOVED_SPTE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|287| <<kvm_flush_remote_tlbs_sptep>> gfn_t gfn = kvm_mmu_page_get_gfn(sp, spte_index(sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|1155| <<rmap_remove>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|1737| <<__rmap_add>> kvm_mmu_page_set_translation(sp, spte_index(spte), gfn, access);
+ *   - arch/x86/kvm/mmu/mmu.c|1878| <<mark_unsync>> if (__test_and_set_bit(spte_index(spte), sp->unsync_child_bitmap))
+ *   - arch/x86/kvm/mmu/mmu.c|2439| <<kvm_mmu_child_role>> role.quadrant = spte_index(sptep) & 1;
+ *   - arch/x86/kvm/mmu/mmu.c|3088| <<mmu_set_spte>> kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
+ *   - arch/x86/kvm/mmu/mmu.c|3104| <<direct_pte_prefetch_many>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(start));
+ *   - arch/x86/kvm/mmu/mmu.c|3130| <<__direct_pte_prefetch>> i = spte_index(sptep) & ~(PTE_PREFETCH_NUM - 1);
+ *   - arch/x86/kvm/mmu/mmu.c|6863| <<shadow_mmu_get_sp_for_split>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|6864| <<shadow_mmu_get_sp_for_split>> access = kvm_mmu_page_get_access(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|6938| <<shadow_mmu_try_split_huge_page>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|720| <<FNAME>> i = spte_index(sptep) & ~(PTE_PREFETCH_NUM - 1);
+ */
 /* Get an SPTE's index into its parent's page table (and the spt array). */
 static inline int spte_index(u64 *sptep)
 {
@@ -220,6 +380,11 @@ static inline int spte_index(u64 *sptep)
  */
 extern u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.h|359| <<spte_to_child_sp>> return to_shadow_page(spte & SPTE_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/spte.h|364| <<sptep_to_sp>> return to_shadow_page(__pa(sptep));
+ */
 static inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)
 {
 	struct page *page = pfn_to_page((shadow_page) >> PAGE_SHIFT);
@@ -227,16 +392,65 @@ static inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)
 	return (struct kvm_mmu_page *)page_private(page);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1932| <<__mmu_unsync_walk>> child = spte_to_child_sp(ent);
+ *   - arch/x86/kvm/mmu/mmu.c|2586| <<validate_direct_spte>> child = spte_to_child_sp(*sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|2607| <<mmu_page_zap_pte>> child = spte_to_child_sp(pte);
+ *   - arch/x86/kvm/mmu/mmu.c|3055| <<mmu_set_spte>> child = spte_to_child_sp(pte);
+ *   - arch/x86/kvm/mmu/mmu.c|3339| <<disallowed_hugepage_adjust>> spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+ *   - arch/x86/kvm/mmu/mmu.c|4188| <<kvm_mmu_sync_roots>> sp = spte_to_child_sp(root);
+ *   - arch/x86/kvm/mmu/spte.h|376| <<root_to_sp>> return spte_to_child_sp(root);
+ */
 static inline struct kvm_mmu_page *spte_to_child_sp(u64 spte)
 {
 	return to_shadow_page(spte & SPTE_BASE_ADDR_MASK);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|286| <<kvm_flush_remote_tlbs_sptep>> struct kvm_mmu_page *sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|403| <<count_spte_clear>> struct kvm_mmu_page *sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|487| <<__get_spte_lockless>> struct kvm_mmu_page *sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|601| <<mmu_spte_clear_track_bits>> int level = sptep_to_sp(sptep)->role.level;
+ *   - arch/x86/kvm/mmu/mmu.c|1154| <<rmap_remove>> sp = sptep_to_sp(spte);
+ *   - arch/x86/kvm/mmu/mmu.c|1258| <<drop_large_spte>> sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|1736| <<__rmap_add>> sp = sptep_to_sp(spte); 
+ *   - arch/x86/kvm/mmu/mmu.c|1877| <<mark_unsync>> sp = sptep_to_sp(spte);
+ *   - arch/x86/kvm/mmu/mmu.c|2234| <<clear_sp_write_flooding_count>> __clear_sp_write_flooding_count(sptep_to_sp(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|2402| <<kvm_mmu_child_role>> struct kvm_mmu_page *parent_sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|3024| <<mmu_set_spte>> struct kvm_mmu_page *sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|3151| <<direct_pte_prefetch>> sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|3610| <<fast_page_fault>> sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|6284| <<__kvm_mmu_invalidate_addr>> struct kvm_mmu_page *sp = sptep_to_sp(iterator.sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|6857| <<shadow_mmu_get_sp_for_split>> struct kvm_mmu_page *huge_sp = sptep_to_sp(huge_sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|6932| <<shadow_mmu_try_split_huge_page>> struct kvm_mmu_page *huge_sp = sptep_to_sp(huge_sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|6978| <<shadow_mmu_try_split_huge_pages>> sp = sptep_to_sp(huge_sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|7093| <<kvm_mmu_zap_collapsible_spte>> sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|705| <<FNAME(pte_prefetch)>> sp = sptep_to_sp(sptep);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|224| <<tdp_mmu_init_child_sp>> parent_sp = sptep_to_sp(rcu_dereference(iter->sptep));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|344| <<handle_removed_pt>> struct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(pt));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1057| <<tdp_mmu_map_handle_target_level>> struct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(iter->sptep));
+ */
 static inline struct kvm_mmu_page *sptep_to_sp(u64 *sptep)
 {
 	return to_shadow_page(__pa(sptep));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3708| <<mmu_free_root_page>> sp = root_to_sp(*root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|3756| <<kvm_mmu_free_roots>> } else if (root_to_sp(mmu->root.hpa)) {
+ *   - arch/x86/kvm/mmu/mmu.c|3795| <<kvm_mmu_free_guest_mode_roots>> sp = root_to_sp(root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4140| <<is_unsync_root>> sp = root_to_sp(root);
+ *   - arch/x86/kvm/mmu/mmu.c|4174| <<kvm_mmu_sync_roots>> sp = root_to_sp(root);
+ *   - arch/x86/kvm/mmu/mmu.c|4625| <<is_page_fault_stale>> struct kvm_mmu_page *sp = root_to_sp(vcpu->arch.mmu->root.hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4883| <<is_root_usable>> sp = root_to_sp(root->hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4960| <<fast_pgd_switch>> if (VALID_PAGE(mmu->root.hpa) && !root_to_sp(mmu->root.hpa))
+ *   - arch/x86/kvm/mmu/mmu.c|5007| <<kvm_mmu_new_pgd>> struct kvm_mmu_page *sp = root_to_sp(vcpu->arch.mmu->root.hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|6001| <<is_obsolete_root>> sp = root_to_sp(root_hpa);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|718| <<tdp_mmu_for_each_pte>> for_each_tdp_pte(_iter, root_to_sp(_mmu->root.hpa), _start, _end)
+ */
 static inline struct kvm_mmu_page *root_to_sp(hpa_t root)
 {
 	if (kvm_mmu_is_dummy_root(root))
@@ -249,6 +463,15 @@ static inline struct kvm_mmu_page *root_to_sp(hpa_t root)
 	return spte_to_child_sp(root);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2545| <<mmu_page_zap_pte>> } else if (is_mmio_spte(pte)) {
+ *   - arch/x86/kvm/mmu/mmu.c|4230| <<handle_mmio_page_fault>> if (is_mmio_spte(spte)) {
+ *   - arch/x86/kvm/mmu/mmu.c|4892| <<sync_mmio_spte>> if (unlikely(is_mmio_spte(*sptep))) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|524| <<handle_changed_spte>> if (WARN_ON_ONCE(!is_mmio_spte(old_spte) &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|525| <<handle_changed_spte>> !is_mmio_spte(new_spte) &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1087| <<tdp_mmu_map_handle_target_level>> if (unlikely(is_mmio_spte(new_spte))) {
+ */
 static inline bool is_mmio_spte(u64 spte)
 {
 	return (spte & shadow_mmio_mask) == shadow_mmio_value &&
@@ -257,6 +480,21 @@ static inline bool is_mmio_spte(u64 spte)
 
 static inline bool is_shadow_present_pte(u64 pte)
 {
+	/*
+	 * 在以下使用SPTE_MMU_PRESENT_MASK:
+	 *   - arch/x86/kvm/mmu/spte.h|124| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+	 *   - arch/x86/kvm/mmu/spte.h|139| <<global>> static_assert(!(SPTE_MMIO_ALLOWED_MASK & (SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+	 *   - arch/x86/kvm/mmu/spte.h|200| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/spte.c|151| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|328| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *
+	 *   - arch/x86/kvm/mmu/spte.h|260| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+	 * A MMU present SPTE is backed by actual memory and may or may not be present
+	 * in hardware.  E.g. MMIO SPTEs are not considered present.  Use bit 11, as it
+	 * is ignored by all flavors of SPTEs and checking a low bit often generates
+	 * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
+	 * enough that the improved code generation is noticeable in KVM's footprint.
+	 */
 	return !!(pte & SPTE_MMU_PRESENT_MASK);
 }
 
@@ -268,6 +506,23 @@ static inline bool is_shadow_present_pte(u64 pte)
  */
 static inline bool kvm_ad_enabled(void)
 {
+	/*
+	 * 对于普通的页表:
+	 * bit-5: Accessed: indicates whether software has accessed the 4-KByte page
+	 *        referenced by this entry (see Section 4.8)
+	 * bit-6: Dirty: indicates whether software has written to the 4-KByte page
+	 *        referenced by this entry (see Section 4.8)
+	 *
+	 * 对于EPT:
+	 * - bit-8: If bit 6 (ignore guest PAT) of EPTP is 1, accessed flag for EPT;
+	 *   indicates whether software has accessed the 4-KByte page referenced by
+	 *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+	 * - bit-9: If bit 6 (ignore guest PAT) of EPTP is 1, dirty flag for EPT;
+	 *   indicates whether software has written to the 4-KByte page referenced by
+	 *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+	 *
+	 * 哪些bit用来记录pte被访问过
+	 */
 	return !!shadow_accessed_mask;
 }
 
@@ -282,9 +537,22 @@ static inline bool spte_ad_enabled(u64 spte)
 	return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_DISABLED;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1305| <<__rmap_clear_dirty>> if (spte_ad_need_write_protect(*sptep))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1637| <<clear_dirty_gfn_range>> KVM_MMU_WARN_ON(kvm_ad_enabled() && spte_ad_need_write_protect(iter.old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1697| <<clear_dirty_pt_masked>> KVM_MMU_WARN_ON(kvm_ad_enabled() && spte_ad_need_write_protect(iter.old_spte));
+ */
 static inline bool spte_ad_need_write_protect(u64 spte)
 {
 	KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
+	/*
+	 * #define SPTE_TDP_AD_SHIFT               52
+	 * #define SPTE_TDP_AD_MASK                (3ULL << SPTE_TDP_AD_SHIFT)
+	 * #define SPTE_TDP_AD_ENABLED             (0ULL << SPTE_TDP_AD_SHIFT)
+	 * #define SPTE_TDP_AD_DISABLED            (1ULL << SPTE_TDP_AD_SHIFT)
+	 * #define SPTE_TDP_AD_WRPROT_ONLY         (2ULL << SPTE_TDP_AD_SHIFT)
+	 */
 	/*
 	 * This is benign for non-TDP SPTEs as SPTE_TDP_AD_ENABLED is '0',
 	 * and non-TDP SPTEs will never set these bits.  Optimize for 64-bit
@@ -307,6 +575,9 @@ static inline u64 spte_shadow_dirty_mask(u64 spte)
 
 static inline bool is_access_track_spte(u64 spte)
 {
+	/*
+	 * 比如: ept硬件不支持access bit, 并且ept的0/1/2都没设置
+	 */
 	return !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;
 }
 
@@ -320,8 +591,28 @@ static inline bool is_last_spte(u64 pte, int level)
 	return (level == PG_LEVEL_4K) || is_large_pte(pte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3543| <<is_access_allowed>> return is_executable_pte(spte);
+ *   - arch/x86/kvm/mmu/mmutrace.h|355| <<__field>> __entry->x = is_executable_pte(__entry->spte);
+ */
 static inline bool is_executable_pte(u64 spte)
 {
+	/*
+	 * 在以下设置shadow_nx_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|590| <<kvm_mmu_set_ept_masks>> shadow_nx_mask = 0ull;
+	 *   - arch/x86/kvm/mmu/spte.c|658| <<kvm_mmu_reset_all_pte_masks>> shadow_nx_mask = PT64_NX_MASK;
+	 * 普通page table entry的bit=63 (XD):
+	 * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+	 * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+	 * otherwise, reserved (must be 0)
+	 *
+	 *
+	 * 在以下设置shadow_x_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|5086| <<boot_cpu_is_amd>> return shadow_x_mask == 0;
+	 *   - arch/x86/kvm/mmu/spte.c|612| <<kvm_mmu_set_ept_masks>> shadow_x_mask = VMX_EPT_EXECUTABLE_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|680| <<kvm_mmu_reset_all_pte_masks>> shadow_x_mask = 0;
+	 */
 	return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
 }
 
@@ -345,26 +636,53 @@ static inline bool is_dirty_spte(u64 spte)
 	return dirty_mask ? spte & dirty_mask : spte & PT_WRITABLE_MASK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4264| <<get_mmio_spte>> pr_err("------ spte = 0x%llx level = %d, rsvd bits = 0x%llx", sptes[level], level, get_rsvd_bits(rsvd_check, sptes[level], level));
+ *   - arch/x86/kvm/mmu/spte.c|704| <<make_spte>> WARN_ONCE(is_rsvd_spte(&vcpu->arch.mmu->shadow_zero_check, spte, level), "spte = 0x%llx, level = %d, rsvd bits = 0x%llx",
+ *                                                   spte, level, get_rsvd_bits(&vcpu->arch.mmu->shadow_zero_check, spte, level));
+ *   - arch/x86/kvm/mmu/spte.h|469| <<__is_rsvd_bits_set>> return pte & get_rsvd_bits(rsvd_check, pte, level);
+ */
 static inline u64 get_rsvd_bits(struct rsvd_bits_validate *rsvd_check, u64 pte,
 				int level)
 {
+	/*
+	 * 至少ept的bit=7是ignored
+	 * 但是ept的hugepage的bit=7是1
+	 */
 	int bit7 = (pte >> 7) & 1;
 
 	return rsvd_check->rsvd_bits_mask[bit7][level-1];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|147| <<FNAME>> return __is_rsvd_bits_set(&mmu->guest_rsvd_check, gpte, level) ||
+ *   - arch/x86/kvm/mmu/spte.h|645| <<is_rsvd_spte>> __is_rsvd_bits_set(rsvd_check, spte, level);
+ */
 static inline bool __is_rsvd_bits_set(struct rsvd_bits_validate *rsvd_check,
 				      u64 pte, int level)
 {
 	return pte & get_rsvd_bits(rsvd_check, pte, level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|141| <<FNAME(is_bad_mt_xwr)>> return __is_bad_mt_xwr(rsvd_check, gpte);
+ *   - arch/x86/kvm/mmu/spte.h|678| <<is_rsvd_spte>> return __is_bad_mt_xwr(rsvd_check, spte) ||
+ */
 static inline bool __is_bad_mt_xwr(struct rsvd_bits_validate *rsvd_check,
 				   u64 pte)
 {
 	return rsvd_check->bad_mt_xwr & BIT_ULL(pte & 0x3f);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4298| <<get_mmio_spte>> reserved |= is_rsvd_spte(rsvd_check, sptes[level], level);
+ *   - arch/x86/kvm/mmu/spte.c|824| <<make_spte>> WARN_ONCE(is_rsvd_spte(&vcpu->arch.mmu->shadow_zero_check, spte, level), "spte = 0x%llx, level = %d, rsvd bits = 0x%llx", spte, level,
+ *						get_rsvd_bits(&vcpu->arch.mmu->shadow_zero_check, spte, level));
+ */
 static __always_inline bool is_rsvd_spte(struct rsvd_bits_validate *rsvd_check,
 					 u64 spte, int level)
 {
@@ -438,14 +756,37 @@ static __always_inline bool is_rsvd_spte(struct rsvd_bits_validate *rsvd_check,
  * cleared when an SPTE is first faulted in from non-present and then remains
  * immutable.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|540| <<mmu_spte_update>> !is_writable_pte(new_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|636| <<mmu_spte_age>> if (is_writable_pte(spte))
+ *   - arch/x86/kvm/mmu/mmu.c|1254| <<spte_write_protect>> if (!is_writable_pte(spte) &&
+ *   - arch/x86/kvm/mmu/mmu.c|3444| <<fast_pf_fix_direct_spte>> if (is_writable_pte(new_spte) && !is_writable_pte(old_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|3456| <<is_access_allowed>> return is_writable_pte(spte);
+ *   - arch/x86/kvm/mmu/spte.c|409| <<spte_has_volatile_bits>> if (!is_writable_pte(spte) && is_mmu_writable_spte(spte))
+ *   - arch/x86/kvm/mmu/spte.c|417| <<spte_has_volatile_bits>> (is_writable_pte(spte) && !(spte & shadow_dirty_mask)))
+ *   - arch/x86/kvm/mmu/spte.c|522| <<make_spte>> if (is_writable_pte(old_spte))
+ *   - arch/x86/kvm/mmu/spte.h|532| <<check_spte_writable_invariants>> WARN_ONCE(is_writable_pte(spte),
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1287| <<age_gfn_range>> if (is_writable_pte(iter->old_spte))
+ */
 static inline bool is_writable_pte(unsigned long pte)
 {
 	return pte & PT_WRITABLE_MASK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|500| <<mmu_spte_update_no_track>> check_spte_writable_invariants(new_spte);
+ *   - arch/x86/kvm/mmu/spte.c|675| <<mark_spte_for_access_track>> check_spte_writable_invariants(spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|510| <<handle_changed_spte>> check_spte_writable_invariants(new_spte);
+ */
 /* Note: spte must be a shadow-present leaf SPTE. */
 static inline void check_spte_writable_invariants(u64 spte)
 {
+	/*
+	 * 1. 如果mmu是writable, 但是host不writable, 错误!
+	 * 2. 如果mmu不writable, 但是pte本身writable, 错误! (可能mmio???)
+	 */
 	if (spte & shadow_mmu_writable_mask)
 		WARN_ONCE(!(spte & shadow_host_writable_mask),
 			  KBUILD_MODNAME ": MMU-writable SPTE is not Host-writable: %llx",
@@ -455,11 +796,32 @@ static inline void check_spte_writable_invariants(u64 spte)
 			  KBUILD_MODNAME ": Writable SPTE is not MMU-writable: %llx", spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|539| <<mmu_spte_update>> if (is_mmu_writable_spte(old_spte) &&
+ *   - arch/x86/kvm/mmu/mmu.c|1255| <<spte_write_protect>> !(pt_protect && is_mmu_writable_spte(spte)))
+ *   - arch/x86/kvm/mmu/mmu.c|3561| <<fast_page_fault>> if (fault->write && is_mmu_writable_spte(spte)) {
+ *   - arch/x86/kvm/mmu/spte.c|397| <<spte_has_volatile_bits>> if (!is_writable_pte(spte) && is_mmu_writable_spte(spte))
+ */
 static inline bool is_mmu_writable_spte(u64 spte)
 {
+	/*
+	 * 在以下设置shadow_mmu_writable_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|609| <<kvm_mmu_set_ept_masks>> shadow_mmu_writable_mask = EPT_SPTE_MMU_WRITABLE;
+	 *   - arch/x86/kvm/mmu/spte.c|673| <<kvm_mmu_reset_all_pte_masks>> shadow_mmu_writable_mask = DEFAULT_SPTE_MMU_WRITABLE;
+	 *
+	 * 用来标记虚拟化的页表里的entry是否指示writable
+	 */
 	return spte & shadow_mmu_writable_mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|361| <<check_mmio_spte>> spte_gen = get_mmio_spte_generation(spte);
+ *   - arch/x86/kvm/mmu/mmutrace.h|226| <<__field>> __entry->gen = get_mmio_spte_generation(spte);
+ *
+ * 把gen从spte取出来
+ */
 static inline u64 get_mmio_spte_generation(u64 spte)
 {
 	u64 gen;
@@ -482,6 +844,11 @@ u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled);
 u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access);
 u64 mark_spte_for_access_track(u64 spte);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3604| <<fast_page_fault>> new_spte = restore_acc_track_spte(new_spte);
+ *   - arch/x86/kvm/mmu/spte.c|770| <<make_spte_executable>> spte = restore_acc_track_spte(spte);
+ */
 /* Restore an acc-track PTE back to a regular PTE */
 static inline u64 restore_acc_track_spte(u64 spte)
 {
diff --git a/arch/x86/kvm/mmu/tdp_iter.c b/arch/x86/kvm/mmu/tdp_iter.c
index 04c247bfe..2a78d2aae 100644
--- a/arch/x86/kvm/mmu/tdp_iter.c
+++ b/arch/x86/kvm/mmu/tdp_iter.c
@@ -9,6 +9,12 @@
  * Recalculates the pointer to the SPTE for the current GFN and level and
  * reread the SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|98| <<try_step_down>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|140| <<try_step_up>> tdp_iter_refresh_sptep(iter);
+ */
 static void tdp_iter_refresh_sptep(struct tdp_iter *iter)
 {
 	iter->sptep = iter->pt_path[iter->level - 1] +
@@ -20,13 +26,42 @@ static void tdp_iter_refresh_sptep(struct tdp_iter *iter)
  * Return the TDP iterator to the root PT and allow it to continue its
  * traversal over the paging structure from there.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|54| <<tdp_iter_start>> tdp_iter_restart(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> tdp_iter_restart(iter);
+ */
 void tdp_iter_restart(struct tdp_iter *iter)
 {
 	iter->yielded = false;
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|70| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|159| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 * 在以下使用tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|37| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|47| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|131| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|725| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|741| <<tdp_mmu_iter_cond_resched>> WARN_ON_ONCE(iter->gfn > iter->next_last_level_gfn);
+	 */
 	iter->yielded_gfn = iter->next_last_level_gfn;
 	iter->level = iter->root_level;
 
+	/*
+	 * level 5: 向下round到每68719476736个 (256T)
+	 * level 4: 向下round到每134217728个 (512G)
+	 * level 3: 向下round到每262144个(1G)
+	 * level 2: 向下round到每512个(2M)
+	 * level 1: 向下round到每1个, 就是没round
+	 */
 	iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> tdp_iter_refresh_sptep(iter);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|98| <<try_step_down>> tdp_iter_refresh_sptep(iter);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|140| <<try_step_up>> tdp_iter_refresh_sptep(iter);
+	 */
 	tdp_iter_refresh_sptep(iter);
 
 	iter->valid = true;
@@ -36,6 +71,12 @@ void tdp_iter_restart(struct tdp_iter *iter)
  * Sets a TDP iterator to walk a pre-order traversal of the paging structure
  * rooted at root_pt, starting with the walk to translate next_last_level_gfn.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|156| <<for_each_tdp_pte_min_level>> for (tdp_iter_start(&iter, root, min_level, start); \
+ *
+ * 在for循环只调用一次
+ */
 void tdp_iter_start(struct tdp_iter *iter, struct kvm_mmu_page *root,
 		    int min_level, gfn_t next_last_level_gfn)
 {
@@ -48,9 +89,17 @@ void tdp_iter_start(struct tdp_iter *iter, struct kvm_mmu_page *root,
 	iter->next_last_level_gfn = next_last_level_gfn;
 	iter->root_level = root->role.level;
 	iter->min_level = min_level;
+	/*
+	 * 4, 3, 2, 1, 0
+	 */
 	iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root->spt;
 	iter->as_id = kvm_mmu_page_as_id(root);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|54| <<tdp_iter_start>> tdp_iter_restart(iter);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> tdp_iter_restart(iter);
+	 */
 	tdp_iter_restart(iter);
 }
 
@@ -59,6 +108,11 @@ void tdp_iter_start(struct tdp_iter *iter, struct kvm_mmu_page *root,
  * address of the child page table referenced by the SPTE. Returns null if
  * there is no such entry.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|91| <<try_step_down>> child_pt = spte_to_child_pt(iter->old_spte, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|493| <<handle_changed_spte>> handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
+ */
 tdp_ptep_t spte_to_child_pt(u64 spte, int level)
 {
 	/*
@@ -75,6 +129,10 @@ tdp_ptep_t spte_to_child_pt(u64 spte, int level)
  * Steps down one level in the paging structure towards the goal GFN. Returns
  * true if the iterator was able to step down a level, false otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|187| <<tdp_iter_next>> if (try_step_down(iter))
+ */
 static bool try_step_down(struct tdp_iter *iter)
 {
 	tdp_ptep_t child_pt;
@@ -94,6 +152,23 @@ static bool try_step_down(struct tdp_iter *iter)
 
 	iter->level--;
 	iter->pt_path[iter->level - 1] = child_pt;
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|70| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|159| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 * 在以下使用tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|37| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|47| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|131| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|725| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|741| <<tdp_mmu_iter_cond_resched>> WARN_ON_ONCE(iter->gfn > iter->next_last_level_gfn)
+	 *
+	 * level 5: 向下round到每68719476736个 (256T)
+	 * level 4: 向下round到每134217728个 (512G)
+	 * level 3: 向下round到每262144个(1G)
+	 * level 2: 向下round到每512个(2M)
+	 * level 1: 向下round到每1个, 就是没round
+	 */
 	iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
 	tdp_iter_refresh_sptep(iter);
 
@@ -107,6 +182,10 @@ static bool try_step_down(struct tdp_iter *iter)
  * able to step to the next entry in the page table, false if the iterator was
  * already at the end of the current page table.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|191| <<tdp_iter_next>> if (try_step_side(iter))
+ */
 static bool try_step_side(struct tdp_iter *iter)
 {
 	/*
@@ -130,13 +209,30 @@ static bool try_step_side(struct tdp_iter *iter)
  * can continue from the next entry in the parent page table. Returns true on a
  * successful step up, false if already in the root page.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|193| <<tdp_iter_next>> } while (try_step_up(iter));
+ */
 static bool try_step_up(struct tdp_iter *iter)
 {
 	if (iter->level == iter->root_level)
 		return false;
 
 	iter->level++;
+	/*
+	 * level 5: 向下round到每68719476736个 (256T)
+	 * level 4: 向下round到每134217728个 (512G)
+	 * level 3: 向下round到每262144个(1G)
+	 * level 2: 向下round到每512个(2M)
+	 * level 1: 向下round到每1个, 就是没round
+	 */
 	iter->gfn = gfn_round_for_level(iter->gfn, iter->level);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> tdp_iter_refresh_sptep(iter);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|98| <<try_step_down>> tdp_iter_refresh_sptep(iter);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|140| <<try_step_up>> tdp_iter_refresh_sptep(iter);
+	 */
 	tdp_iter_refresh_sptep(iter);
 
 	return true;
@@ -158,6 +254,16 @@ static bool try_step_up(struct tdp_iter *iter)
  *    SPTE will have already been visited, and so the iterator must also step
  *    to the side again.
  */
+/*
+ * 在以下调用tdp_iter_next():
+ * 155 #define for_each_tdp_pte_min_level(iter, root, min_level, start, end) \
+ * 156         for (tdp_iter_start(&iter, root, min_level, start); \
+ * 157              iter.valid && iter.gfn < end;                   \
+ * 158              tdp_iter_next(&iter))
+ *
+ * Preorder traversal is an a kind of tree traversal in which the root node is
+ * visited first, then the left subtree, and finally the right subtree.
+ */
 void tdp_iter_next(struct tdp_iter *iter)
 {
 	if (iter->yielded) {
diff --git a/arch/x86/kvm/mmu/tdp_iter.h b/arch/x86/kvm/mmu/tdp_iter.h
index fae559559..a3186934f 100644
--- a/arch/x86/kvm/mmu/tdp_iter.h
+++ b/arch/x86/kvm/mmu/tdp_iter.h
@@ -8,12 +8,46 @@
 #include "mmu.h"
 #include "spte.h"
 
+/*
+ * x86 kvm使用rcu的一些例子:
+ *   - arch/x86/kvm/mmu/mmu.c|7655| <<kvm_recover_nx_huge_pages>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/mmu.c|7720| <<kvm_recover_nx_huge_pages>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|124| <<tdp_mmu_next_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|764| <<tdp_mmu_iter_cond_resched>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|835| <<tdp_mmu_zap_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|895| <<tdp_mmu_zap_leafs>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1159| <<kvm_tdp_mmu_map>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1257| <<kvm_tdp_mmu_handle_gfn>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1388| <<wrprot_gfn_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1489| <<tdp_mmu_alloc_sp_for_split>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1542| <<tdp_mmu_split_huge_pages_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1638| <<clear_dirty_gfn_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1702| <<clear_dirty_pt_masked>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1761| <<zap_collapsible_spte_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1835| <<write_protect_gfn>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|67| <<kvm_tdp_mmu_walk_lockless_begin>> rcu_read_lock();
+ *   - arch/x86/kvm/x86.c|10080| <<kvm_sched_yield>> rcu_read_lock();
+ *   - arch/x86/kvm/xen.c|2087| <<kvm_xen_hcall_evtchn_send>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|2474| <<kvm_range_has_memory_attributes>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|4049| <<kvm_vcpu_yield_to>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|4273| <<vcpu_get_pid>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|6710| <<kvm_vm_worker_thread>> rcu_read_lock();
+ */
+
 /*
  * TDP MMU SPTEs are RCU protected to allow paging structures (non-leaf SPTEs)
  * to be zapped while holding mmu_lock for read, and to allow TLB flushes to be
  * batched without having to collect the list of zapped SPs.  Flows that can
  * remove SPs must service pending TLB flushes prior to dropping RCU protection.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|22| <<tdp_iter_refresh_sptep>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|147| <<try_step_down>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|202| <<try_step_side>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|390| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_read_spte(sptep);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|864| <<kvm_tdp_mmu_zap_sp>> old_spte = kvm_tdp_mmu_read_spte(sp->ptep);
+ */
 static inline u64 kvm_tdp_mmu_read_spte(tdp_ptep_t sptep)
 {
 	return READ_ONCE(*rcu_dereference(sptep));
@@ -41,13 +75,32 @@ static inline void __kvm_tdp_mmu_write_spte(tdp_ptep_t sptep, u64 new_spte)
  * logic needs to be reassessed if KVM were to use non-leaf Accessed
  * bits, e.g. to skip stepping down into child SPTEs when aging SPTEs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|54| <<kvm_tdp_mmu_write_spte>> if (kvm_tdp_mmu_spte_need_atomic_write(old_spte, level))
+ *   - arch/x86/kvm/mmu/tdp_iter.h|66| <<tdp_mmu_clear_spte_bits>> if (kvm_tdp_mmu_spte_need_atomic_write(old_spte, level)) {
+ */
 static inline bool kvm_tdp_mmu_spte_need_atomic_write(u64 old_spte, int level)
 {
+	/*
+	 * spte_has_volatile_bits()的注释:
+	 * Returns true if the SPTE has bits that may be set without holding mmu_lock.
+	 * The caller is responsible for checking if the SPTE is shadow-present, and
+	 * for determining whether or not the caller cares about non-leaf SPTEs.
+	 */
 	return is_shadow_present_pte(old_spte) &&
 	       is_last_spte(old_spte, level) &&
 	       spte_has_volatile_bits(old_spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|382| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, REMOVED_SPTE, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|606| <<tdp_mmu_set_spte>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, new_spte, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1185| <<age_gfn_range>> iter->old_spte = kvm_tdp_mmu_write_spte(iter->sptep, iter->old_spte, new_spte, iter->level);
+ *
+ * typedef u64 __rcu *tdp_ptep_t;
+ */
 static inline u64 kvm_tdp_mmu_write_spte(tdp_ptep_t sptep, u64 old_spte,
 					 u64 new_spte, int level)
 {
@@ -65,6 +118,9 @@ static inline u64 tdp_mmu_clear_spte_bits(tdp_ptep_t sptep, u64 old_spte,
 
 	if (kvm_tdp_mmu_spte_need_atomic_write(old_spte, level)) {
 		sptep_atomic = (atomic64_t *)rcu_dereference(sptep);
+		/*
+		 * 注释: Atomically updates @v to (@v & @i) with full ordering.
+		 */
 		return (u64)atomic64_fetch_and(~mask, sptep_atomic);
 	}
 
@@ -80,6 +136,17 @@ struct tdp_iter {
 	 * The iterator will traverse the paging structure towards the mapping
 	 * for this GFN.
 	 */
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|70| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|159| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 * 在以下使用tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|37| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|47| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|131| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|725| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|741| <<tdp_mmu_iter_cond_resched>> WARN_ON_ONCE(iter->gfn > iter->next_last_level_gfn);
+	 */
 	gfn_t next_last_level_gfn;
 	/*
 	 * The next_last_level_gfn at the time when the thread last
@@ -120,11 +187,26 @@ struct tdp_iter {
  * Iterates over every SPTE mapping the GFN range [start, end) in a
  * preorder traversal.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|129| <<for_each_tdp_pte>> for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|698| <<__tdp_mmu_zap_root>> for_each_tdp_pte_min_level(iter, root, zap_level, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|791| <<tdp_mmu_zap_leafs>> for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1273| <<wrprot_gfn_range>> for_each_tdp_pte_min_level(iter, root, min_level, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1436| <<tdp_mmu_split_huge_pages_root>> for_each_tdp_pte_min_level(iter, root, target_level + 1, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1640| <<zap_collapsible_spte_range>> for_each_tdp_pte_min_level(iter, root, PG_LEVEL_2M, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1710| <<write_protect_gfn>> for_each_tdp_pte_min_level(iter, root, min_level, gfn, gfn + 1) {
+ */
 #define for_each_tdp_pte_min_level(iter, root, min_level, start, end) \
 	for (tdp_iter_start(&iter, root, min_level, start); \
 	     iter.valid && iter.gfn < end;		     \
 	     tdp_iter_next(&iter))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|622| <<tdp_root_for_each_pte>> for_each_tdp_pte(_iter, _root, _start, _end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|632| <<tdp_mmu_for_each_pte>> for_each_tdp_pte(_iter, root_to_sp(_mmu->root.hpa), _start, _end)
+ */
 #define for_each_tdp_pte(iter, root, start, end) \
 	for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end)
 
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index 6ae19b4ee..85904d224 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -11,6 +11,36 @@
 #include <asm/cmpxchg.h>
 #include <trace/events/kvm.h>
 
+/*
+ * x86 kvm使用rcu的一些例子:
+ *   - arch/x86/kvm/mmu/mmu.c|7655| <<kvm_recover_nx_huge_pages>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/mmu.c|7720| <<kvm_recover_nx_huge_pages>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|124| <<tdp_mmu_next_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|764| <<tdp_mmu_iter_cond_resched>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|835| <<tdp_mmu_zap_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|895| <<tdp_mmu_zap_leafs>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1159| <<kvm_tdp_mmu_map>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1257| <<kvm_tdp_mmu_handle_gfn>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1388| <<wrprot_gfn_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1489| <<tdp_mmu_alloc_sp_for_split>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1542| <<tdp_mmu_split_huge_pages_root>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1638| <<clear_dirty_gfn_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1702| <<clear_dirty_pt_masked>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1761| <<zap_collapsible_spte_range>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1835| <<write_protect_gfn>> rcu_read_lock();
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|67| <<kvm_tdp_mmu_walk_lockless_begin>> rcu_read_lock();
+ *   - arch/x86/kvm/x86.c|10080| <<kvm_sched_yield>> rcu_read_lock();
+ *   - arch/x86/kvm/xen.c|2087| <<kvm_xen_hcall_evtchn_send>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|2474| <<kvm_range_has_memory_attributes>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|4049| <<kvm_vcpu_yield_to>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|4273| <<vcpu_get_pid>> rcu_read_lock();
+ *   - virt/kvm/kvm_main.c|6710| <<kvm_vm_worker_thread>> rcu_read_lock();
+ */
+
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6352| <<kvm_mmu_init_vm>> kvm_mmu_init_tdp_mmu(kvm);
+ */
 /* Initializes the TDP MMU for the VM, if enabled. */
 void kvm_mmu_init_tdp_mmu(struct kvm *kvm)
 {
@@ -30,6 +60,10 @@ static __always_inline bool kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6373| <<kvm_mmu_uninit_vm>> kvm_mmu_uninit_tdp_mmu(kvm);
+ */
 void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 {
 	/*
@@ -187,6 +221,11 @@ static struct kvm_mmu_page *tdp_mmu_alloc_sp(struct kvm_vcpu *vcpu)
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|216| <<tdp_mmu_init_child_sp>> tdp_mmu_init_sp(child_sp, iter->sptep, iter->gfn, role);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|238| <<kvm_tdp_mmu_get_vcpu_root_hpa>> tdp_mmu_init_sp(root, NULL, 0, role);
+ */
 static void tdp_mmu_init_sp(struct kvm_mmu_page *sp, tdp_ptep_t sptep,
 			    gfn_t gfn, union kvm_mmu_page_role role)
 {
@@ -276,6 +315,10 @@ static void tdp_unaccount_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)
  * @kvm: kvm instance
  * @sp: the page to be removed
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|318| <<handle_removed_pt>> tdp_mmu_unlink_sp(kvm, sp);
+ */
 static void tdp_mmu_unlink_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	tdp_unaccount_mmu_page(kvm, sp);
@@ -306,6 +349,22 @@ static void tdp_mmu_unlink_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
  * this thread will be responsible for ensuring the page is freed. Hence the
  * early rcu_dereferences in the function.
  */
+/*
+ * 只在一个地方调用:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|510| <<handle_changed_spte>> handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
+ *
+ * 508          //
+ * 509		// Recursively handle child PTs if the change removed a subtree from
+ * 510          // the paging structure.  Note the WARN on the PFN changing without the
+ * 511          // SPTE being converted to a hugepage (leaf) or being zapped.  Shadow
+ * 512          // pages are kernel allocations and should never be migrated.
+ * 513          //
+ * 514          if (was_present && !was_leaf &&
+ * 515              (is_leaf || !is_present || WARN_ON_ONCE(pfn_changed)))
+ * 516                  handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
+ *
+ * 因为只在一个地方调用, 参数的pt一定是指向child page table (4k)
+ */
 static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 {
 	struct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(pt));
@@ -319,6 +378,13 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 
 	for (i = 0; i < SPTE_ENT_PER_PAGE; i++) {
 		tdp_ptep_t sptep = pt + i;
+		/*
+		 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+		 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+		 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+		 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+		 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+		 */
 		gfn_t gfn = base_gfn + i * KVM_PAGES_PER_HPAGE(level);
 		u64 old_spte;
 
@@ -382,6 +448,12 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 			old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte,
 							  REMOVED_SPTE, level);
 		}
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|402| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte, new_spte, iter->level, true);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|625| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+		 */
 		handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
 				    old_spte, REMOVED_SPTE, level, shared);
 	}
@@ -405,6 +477,12 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
  * dirty logging updates are handled in common code, not here (see make_spte()
  * and fast_pf_fix_direct_spte()).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|402| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte, new_spte, iter->level, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|625| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+ */
 static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared)
@@ -417,6 +495,13 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 
 	WARN_ON_ONCE(level > PT64_ROOT_MAX_LEVEL);
 	WARN_ON_ONCE(level < PG_LEVEL_4K);
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+	 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+	 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+	 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+	 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+	 */
 	WARN_ON_ONCE(gfn & (KVM_PAGES_PER_HPAGE(level) - 1));
 
 	/*
@@ -540,6 +625,12 @@ static inline int tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	if (!try_cmpxchg64(sptep, &iter->old_spte, new_spte))
 		return -EBUSY;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|402| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte, new_spte, iter->level, true);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|625| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+	 */
 	handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			    new_spte, iter->level, true);
 
@@ -589,6 +680,11 @@ static inline int tdp_mmu_zap_spte_atomic(struct kvm *kvm,
  * Returns the old SPTE value, which _may_ be different than @old_spte if the
  * SPTE had voldatile bits.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|633| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|784| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0, sp->gfn, sp->role.level + 1);
+ */
 static u64 tdp_mmu_set_spte(struct kvm *kvm, int as_id, tdp_ptep_t sptep,
 			    u64 old_spte, u64 new_spte, gfn_t gfn, int level)
 {
@@ -618,9 +714,19 @@ static inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 					  iter->gfn, iter->level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|695| <<tdp_root_for_each_leaf_pte>> tdp_root_for_each_pte(_iter, _root, _start, _end) \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1587| <<clear_dirty_gfn_range>> tdp_root_for_each_pte(iter, root, start, end) {
+ */
 #define tdp_root_for_each_pte(_iter, _root, _start, _end) \
 	for_each_tdp_pte(_iter, _root, _start, _end)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1214| <<kvm_tdp_mmu_handle_gfn>> tdp_root_for_each_leaf_pte(iter, root, range->start, range->end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1651| <<clear_dirty_pt_masked>> tdp_root_for_each_leaf_pte(iter, root, gfn + __ffs(mask), gfn + BITS_PER_LONG) {
+ */
 #define tdp_root_for_each_leaf_pte(_iter, _root, _start, _end)	\
 	tdp_root_for_each_pte(_iter, _root, _start, _end)		\
 		if (!is_shadow_present_pte(_iter.old_spte) ||		\
@@ -628,6 +734,12 @@ static inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 			continue;					\
 		else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1835| <<kvm_tdp_mmu_get_walk>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1862| <<kvm_tdp_mmu_fast_pf_get_last_sptep>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ */
 #define tdp_mmu_for_each_pte(_iter, _mmu, _start, _end)		\
 	for_each_tdp_pte(_iter, root_to_sp(_mmu->root.hpa), _start, _end)
 
@@ -645,6 +757,15 @@ static inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,
  *
  * Returns true if this function yielded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|786| <<__tdp_mmu_zap_root>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|879| <<tdp_mmu_zap_leafs>> tdp_mmu_iter_cond_resched(kvm, &iter, flush, false)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1361| <<wrprot_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1524| <<tdp_mmu_split_huge_pages_root>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1609| <<clear_dirty_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1728| <<zap_collapsible_spte_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ */
 static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 							  struct tdp_iter *iter,
 							  bool flush, bool shared)
@@ -670,6 +791,12 @@ static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 
 		WARN_ON_ONCE(iter->gfn > iter->next_last_level_gfn);
 
+		/*
+		 * 注释:
+		 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+		 * which case tdp_iter_next() needs to restart the walk at the root
+		 * level instead of advancing to the next entry.
+		 */
 		iter->yielded = true;
 	}
 
@@ -777,6 +904,11 @@ bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
  * the caller must ensure it does not supply too large a GFN range, or the
  * operation can cause a soft lockup.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|912| <<kvm_tdp_mmu_zap_leafs>> flush = tdp_mmu_zap_leafs(kvm, root, start, end, true, flush);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1206| <<kvm_tdp_mmu_unmap_gfn_range>> flush = tdp_mmu_zap_leafs(kvm, root, range->start, range->end, range->may_block, flush);
+ */
 static bool tdp_mmu_zap_leafs(struct kvm *kvm, struct kvm_mmu_page *root,
 			      gfn_t start, gfn_t end, bool can_yield, bool flush)
 {
@@ -940,6 +1072,10 @@ void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
  * Installs a last-level SPTE to handle a TDP page fault.
  * (NPT/EPT violation/misconfiguration)
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1213| <<kvm_tdp_mmu_map>> ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
+ */
 static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 					  struct kvm_page_fault *fault,
 					  struct tdp_iter *iter)
@@ -1003,6 +1139,11 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
  * Returns: 0 if the new page table was installed. Non-0 if the page table
  *          could not be installed (e.g. the atomic compare-exchange failed).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1185| <<kvm_tdp_mmu_map>> r = tdp_mmu_link_sp(kvm, &iter, sp, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1508| <<tdp_mmu_split_huge_page>> ret = tdp_mmu_link_sp(kvm, iter, sp, shared);
+ */
 static int tdp_mmu_link_sp(struct kvm *kvm, struct tdp_iter *iter,
 			   struct kvm_mmu_page *sp, bool shared)
 {
@@ -1259,6 +1400,10 @@ bool kvm_tdp_mmu_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
  * [start, end). Returns true if an SPTE has been changed and the TLBs need to
  * be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1395| <<kvm_tdp_mmu_wrprot_slot>> spte_set |= wrprot_gfn_range(kvm, root, slot->base_gfn, slot->base_gfn + slot->npages, min_level);
+ */
 static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			     gfn_t start, gfn_t end, int min_level)
 {
@@ -1505,6 +1650,10 @@ void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
  * each SPTE. Returns true if an SPTE has been changed and the TLBs need to
  * be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1667| <<kvm_tdp_mmu_clear_dirty_slot>> spte_set |= clear_dirty_gfn_range(kvm, root, slot->base_gfn, slot->base_gfn + slot->npages);
+ */
 static bool clear_dirty_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			   gfn_t start, gfn_t end)
 {
@@ -1696,6 +1845,10 @@ void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1858| <<kvm_tdp_mmu_write_protect_gfn>> spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
+ */
 static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
 			      gfn_t gfn, int min_level)
 {
@@ -1732,6 +1885,10 @@ static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1452| <<kvm_mmu_slot_gfn_write_protect>> kvm_tdp_mmu_write_protect_gfn(kvm, slot, gfn, min_level);
+ */
 bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level)
diff --git a/arch/x86/kvm/mmu/tdp_mmu.h b/arch/x86/kvm/mmu/tdp_mmu.h
index 20d97aa46..45b17bd7f 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@ -12,6 +12,11 @@ void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm);
 
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|128| <<tdp_mmu_next_root>> kvm_tdp_mmu_get_root(next_root))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|233| <<kvm_tdp_mmu_get_vcpu_root_hpa>> kvm_tdp_mmu_get_root(root))
+ */
 __must_check static inline bool kvm_tdp_mmu_get_root(struct kvm_mmu_page *root)
 {
 	return refcount_inc_not_zero(&root->tdp_mmu_root_count);
@@ -53,11 +58,19 @@ void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
 				      gfn_t start, gfn_t end,
 				      int target_level, bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|648| <<walk_shadow_page_lockless_begin>> kvm_tdp_mmu_walk_lockless_begin();
+ */
 static inline void kvm_tdp_mmu_walk_lockless_begin(void)
 {
 	rcu_read_lock();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|667| <<walk_shadow_page_lockless_end>> kvm_tdp_mmu_walk_lockless_end();
+ */
 static inline void kvm_tdp_mmu_walk_lockless_end(void)
 {
 	rcu_read_unlock();
diff --git a/arch/x86/kvm/mtrr.c b/arch/x86/kvm/mtrr.c
index a67c28a56..ad35380ab 100644
--- a/arch/x86/kvm/mtrr.c
+++ b/arch/x86/kvm/mtrr.c
@@ -21,24 +21,111 @@
 #include "cpuid.h"
 #include "mmu.h"
 
+/*
+ * 关于IA32_MTRR_DEF_TYPE MSR
+ *
+ * - FE(fixed MTRRs enabled)标志,bit10-当置1时,固定范围MTRRs enabled,反之亦反.
+ * 当固定范围MTRRs的启用时,他们较之于可变的MTRRS的有更高的优先级,所以他们的范围
+ * 发生重叠的时候,那么fixed-range MTRRs则优先于variable-range MTRRs.
+ * 如果在fixed-range MTRRs是disabled的,fixed-range MTRRs仍可使用,可以映射通常由
+ * 固定的范围MTRRs覆盖范围.
+ *
+ * - E(MTRRs enabled)标志(flag),bit11——当它置1时,MTRRs被启用,当被清零时,所有MTRRs
+ * 被禁用,并且UC内存类型适用于所有的物理内存.当此标志被置1,FE标志可以禁用固定范
+ * 围MTRRs(fixed-range MTRRs),当被清零时,是FE标志不会有影响.当E标志置1 ...
+ */
 #define IA32_MTRR_DEF_TYPE_E		(1ULL << 11)
 #define IA32_MTRR_DEF_TYPE_FE		(1ULL << 10)
 #define IA32_MTRR_DEF_TYPE_TYPE_MASK	(0xff)
 
+/*
+ * MTRRs的设置. 通过下面的三个MSR.
+ *
+ * 1. IA32_MTRR_DEF_TYPE MSR
+ *
+ * 2. 一些Fixed Range MTRRs: IA32_MTRR_FIX64K_00000, IA32_MTRR_FIX16K_80000,
+ * IA32_MTRR_FIX16K_A0000等11个64-bit的寄存器. 但是只能控制比较小的range. This
+ * range is divided into sixteen 16-KByte sub-ranges, 8 ranges per register.
+ *
+ * 3. Variable Range MTRRs. 数量由IA32_MTRRCAP决定. encoding和上面一样.
+ */
+
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|364| <<set_var_mtrr_msr>> if (is_mtrr_base_msr(msr))
+ *   - arch/x86/kvm/mtrr.c|423| <<kvm_mtrr_get_msr>> if (is_mtrr_base_msr(msr))
+ */
 static bool is_mtrr_base_msr(unsigned int msr)
 {
 	/* MTRR base MSRs use even numbers, masks use odd numbers. */
 	return !(msr & 0x1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|556| <<get_mtrr_var_range>> rdmsr(MTRRphysBase_MSR(index), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|825| <<generic_get_mtrr>> rdmsr(MTRRphysBase_MSR(reg), base_lo, base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|886| <<set_mtrr_var_ranges>> rdmsr(MTRRphysBase_MSR(index), lo, hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|890| <<set_mtrr_var_ranges>> mtrr_wrmsr(MTRRphysBase_MSR(index), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kernel/cpu/mtrr/generic.c|1008| <<generic_set_mtrr>> mtrr_wrmsr(MTRRphysBase_MSR(reg), vr->base_lo, vr->base_hi);
+ *   - arch/x86/kvm/mtrr.c|74| <<var_mtrr_msr_to_range>> int index = (msr - MTRRphysBase_MSR(0)) / 2;
+ *   - arch/x86/kvm/mtrr.c|96| <<msr_mtrr_valid>> case MTRRphysBase_MSR(0) ... MTRRphysMask_MSR(KVM_NR_VAR_MTRR - 1):
+ *   - arch/x86/kvm/mtrr.c|169| <<kvm_mtrr_valid>> WARN_ON(!(msr >= MTRRphysBase_MSR(0) &&
+ *   - arch/x86/kvm/x86.c|3851| <<kvm_set_msr_common>> case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ *   - arch/x86/kvm/x86.c|4266| <<kvm_get_msr_common>> case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ *
+ * 326 #define MSR_MTRRfix64K_00000            0x00000250
+ * 327 #define MSR_MTRRfix16K_80000            0x00000258
+ * 328 #define MSR_MTRRfix16K_A0000            0x00000259
+ * 329 #define MSR_MTRRfix4K_C0000             0x00000268
+ * 330 #define MSR_MTRRfix4K_C8000             0x00000269
+ * 331 #define MSR_MTRRfix4K_D0000             0x0000026a
+ * 332 #define MSR_MTRRfix4K_D8000             0x0000026b
+ * 333 #define MSR_MTRRfix4K_E0000             0x0000026c
+ * 334 #define MSR_MTRRfix4K_E8000             0x0000026d
+ * 335 #define MSR_MTRRfix4K_F0000             0x0000026e
+ * 336 #define MSR_MTRRfix4K_F8000             0x0000026f
+ * 337 #define MSR_MTRRdefType                 0x000002ff
+ */
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|338| <<update_mtrr>> var_mtrr_range(var_mtrr_msr_to_range(vcpu, msr), &start, &end);
+ *   - arch/x86/kvm/mtrr.c|354| <<set_var_mtrr_msr>> cur = var_mtrr_msr_to_range(vcpu, msr);
+ *   - arch/x86/kvm/mtrr.c|424| <<kvm_mtrr_get_msr>> *pdata = var_mtrr_msr_to_range(vcpu, msr)->base;
+ *   - arch/x86/kvm/mtrr.c|426| <<kvm_mtrr_get_msr>> *pdata = var_mtrr_msr_to_range(vcpu, msr)->mask;
+ *
+ * 根据msr(base或者mask)的index, 找到&vcpu->arch.mtrr_state.var_ranges[index]
+ */
 static struct kvm_mtrr_range *var_mtrr_msr_to_range(struct kvm_vcpu *vcpu,
 						    unsigned int msr)
 {
+	/*
+	 * #define MTRRphysBase_MSR(reg) (0x200 + 2 * (reg))
+	 * #define MTRRphysMask_MSR(reg) (0x200 + 2 * (reg) + 1)
+	 *
+	 * 一个index对应两个reg, 所以除以2
+	 */
 	int index = (msr - MTRRphysBase_MSR(0)) / 2;
 
+	/*
+	 * struct kvm_mtrr {
+	 *     struct kvm_mtrr_range var_ranges[KVM_NR_VAR_MTRR];
+	 *     mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
+	 *     u64 deftype;
+	 *
+	 *     struct list_head head;
+	 * };
+	 */
 	return &vcpu->arch.mtrr_state.var_ranges[index];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|73| <<kvm_mtrr_valid>> if (!msr_mtrr_valid(msr))
+ *   - arch/x86/kvm/mtrr.c|413| <<kvm_mtrr_get_msr>> if (!msr_mtrr_valid(msr))
+ *
+ * 返回msr是不是kvm支持的msr??
+ */
 static bool msr_mtrr_valid(unsigned msr)
 {
 	switch (msr) {
@@ -60,24 +147,80 @@ static bool msr_mtrr_valid(unsigned msr)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|79| <<kvm_mtrr_valid>> return valid_mtrr_type(data & 0xff);
+ *   - arch/x86/kvm/mtrr.c|82| <<kvm_mtrr_valid>> if (!valid_mtrr_type((data >> (i * 8)) & 0xff))
+ *   - arch/x86/kvm/mtrr.c|94| <<kvm_mtrr_valid>> if (!valid_mtrr_type(data & 0xff))
+ *
+ * 确认type是下面的之一:
+ * 0: UC
+ * 1: WC
+ * 4: WT
+ * 5: WP
+ * 6: WB
+ */
 static bool valid_mtrr_type(unsigned t)
 {
+	/*
+	 * 0: UC
+	 * 1: WC
+	 * 4: WT
+	 * 5: WP
+	 * 6: WB
+	 *
+	 * 0x73是01110011b
+	 */
 	return t < 8 && (1 << t) & 0x73; /* 0, 1, 4, 5, 6 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|382| <<kvm_mtrr_set_msr>> if (!kvm_mtrr_valid(vcpu, msr, data))
+ */
 static bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 {
 	int i;
 	u64 mask;
 
+	/*
+	 * 返回msr是不是kvm支持的msr
+	 */
 	if (!msr_mtrr_valid(msr))
 		return false;
 
+	/*
+	 * MTRRs的设置. 通过下面的三个MSR.
+	 *
+	 * 1. IA32_MTRR_DEF_TYPE MSR
+	 *
+	 * 2. 一些Fixed Range MTRRs: IA32_MTRR_FIX64K_00000, IA32_MTRR_FIX16K_80000,
+	 * IA32_MTRR_FIX16K_A0000等11个64-bit的寄存器. 但是只能控制比较小的range. This
+	 * range is divided into sixteen 16-KByte sub-ranges, 8 ranges per register.
+	 *
+	 * 3. Variable Range MTRRs. 数量由IA32_MTRRCAP决定. encoding和上面一样.
+	 */
 	if (msr == MSR_MTRRdefType) {
 		if (data & ~0xcff)
 			return false;
+		/*
+		 * 确认type是下面的之一:
+		 * 0: UC
+		 * 1: WC
+		 * 4: WT
+		 * 5: WP
+		 * 6: WB
+		 */
 		return valid_mtrr_type(data & 0xff);
 	} else if (msr >= MSR_MTRRfix64K_00000 && msr <= MSR_MTRRfix4K_F8000) {
+		/*
+		 * 确认type是下面的之一:
+		 * 0: UC
+		 * 1: WC
+		 * 4: WT
+		 * 5: WP
+		 * 6: WB
+		 */
 		for (i = 0; i < 8 ; i++)
 			if (!valid_mtrr_type((data >> (i * 8)) & 0xff))
 				return false;
@@ -88,6 +231,10 @@ static bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 	WARN_ON(!(msr >= MTRRphysBase_MSR(0) &&
 		  msr <= MTRRphysMask_MSR(KVM_NR_VAR_MTRR - 1)));
 
+	/*
+	 * 为gpa保留的地址部分生成mask, 从不支持的最高bit到63:
+	 * rsvd_bits(cpuid_maxphyaddr(vcpu), 63)
+	 */
 	mask = kvm_vcpu_reserved_gpa_bits_raw(vcpu);
 	if ((msr & 1) == 0) {
 		/* MTRR base */
@@ -101,21 +248,62 @@ static bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 	return (data & mask) == 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|326| <<update_mtrr>> if (!mtrr_is_enabled(mtrr_state) && msr != MSR_MTRRdefType)
+ *   - arch/x86/kvm/mtrr.c|564| <<mtrr_lookup_start>> if (!mtrr_is_enabled(iter->mtrr_state)) {
+ *
+ * 返回!!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_E)
+ */
 static bool mtrr_is_enabled(struct kvm_mtrr *mtrr_state)
 {
+	/*
+	 * 关于IA32_MTRR_DEF_TYPE MSR
+	 *
+	 * - FE(fixed MTRRs enabled)标志,bit10-当置1时,固定范围MTRRs enabled,反之亦反.
+	 * 当固定范围MTRRs的启用时,他们较之于可变的MTRRS的有更高的优先级,所以他们的范围
+	 * 发生重叠的时候,那么fixed-range MTRRs则优先于variable-range MTRRs.
+	 * 如果在fixed-range MTRRs是disabled的,fixed-range MTRRs仍可使用,可以映射通常由
+	 * 固定的范围MTRRs覆盖范围.
+	 *
+	 * - E(MTRRs enabled)标志(flag),bit11——当它置1时,MTRRs被启用,当被清零时,所有MTRRs
+	 * 被禁用,并且UC内存类型适用于所有的物理内存.当此标志被置1,FE标志可以禁用固定范
+	 * 围MTRRs(fixed-range MTRRs),当被清零时,是FE标志不会有影响.当E标志置1 ...
+	 */
 	return !!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_E);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|331| <<update_mtrr>> if (!fixed_mtrr_is_enabled(mtrr_state))
+ *   - arch/x86/kvm/mtrr.c|475| <<mtrr_lookup_fixed_start>> if (!fixed_mtrr_is_enabled(iter->mtrr_state))
+ *
+ * 返回!!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_FE)
+ */
 static bool fixed_mtrr_is_enabled(struct kvm_mtrr *mtrr_state)
 {
 	return !!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_FE);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|678| <<kvm_mtrr_get_guest_memory_type>> return mtrr_default_type(mtrr_state);
+ *   - arch/x86/kvm/mtrr.c|719| <<kvm_mtrr_check_gfn_range_consistency>> return type == mtrr_default_type(mtrr_state);
+ *
+ * 返回mtrr_state->deftype & IA32_MTRR_DEF_TYPE_TYPE_MASK
+ */
 static u8 mtrr_default_type(struct kvm_mtrr *mtrr_state)
 {
 	return mtrr_state->deftype & IA32_MTRR_DEF_TYPE_TYPE_MASK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|674| <<kvm_mtrr_get_guest_memory_type>> return mtrr_disabled_type(vcpu);
+ *
+ * 如果cpuid支持mtrr, 当IA32_MTRR_DEF_TYPE_E被disable的时候全用UC,
+ * 如果都不支持mtrr, 用WB.
+ */
 static u8 mtrr_disabled_type(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -150,6 +338,33 @@ struct fixed_mtrr_segment {
 	int range_start;
 };
 
+/*
+ * 326 #define MSR_MTRRfix64K_00000            0x00000250
+ * 327 #define MSR_MTRRfix16K_80000            0x00000258
+ * 328 #define MSR_MTRRfix16K_A0000            0x00000259
+ * 329 #define MSR_MTRRfix4K_C0000             0x00000268
+ * 330 #define MSR_MTRRfix4K_C8000             0x00000269
+ * 331 #define MSR_MTRRfix4K_D0000             0x0000026a
+ * 332 #define MSR_MTRRfix4K_D8000             0x0000026b
+ * 333 #define MSR_MTRRfix4K_E0000             0x0000026c
+ * 334 #define MSR_MTRRfix4K_E8000             0x0000026d
+ * 335 #define MSR_MTRRfix4K_F0000             0x0000026e
+ * 336 #define MSR_MTRRfix4K_F8000             0x0000026f
+ * 337 #define MSR_MTRRdefType                 0x000002ff
+ *
+ * 在以下使用fixed_seg_table[] (fixed_mtrr_segment类型):
+ *   - arch/x86/kvm/mtrr.c|191| <<fixed_mtrr_seg_unit_size>> return 8 << fixed_seg_table[seg].range_shift;
+ *   - arch/x86/kvm/mtrr.c|222| <<fixed_mtrr_seg_unit_range>> struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
+ *   - arch/x86/kvm/mtrr.c|232| <<fixed_mtrr_seg_unit_range_index>> struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
+ *   - arch/x86/kvm/mtrr.c|243| <<fixed_mtrr_seg_end_range_index>> struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
+ *   - arch/x86/kvm/mtrr.c|274| <<fixed_mtrr_addr_to_seg>> int seg, seg_num = ARRAY_SIZE(fixed_seg_table);
+ *   - arch/x86/kvm/mtrr.c|277| <<fixed_mtrr_addr_to_seg>> mtrr_seg = &fixed_seg_table[seg];
+ *   - arch/x86/kvm/mtrr.c|290| <<fixed_mtrr_addr_seg_to_range_index>> mtrr_seg = &fixed_seg_table[seg];
+ *   - arch/x86/kvm/mtrr.c|298| <<fixed_mtrr_range_end_addr>> struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
+ *
+ * https://github.com/finallyjustice/codereading/blob/master/comment/summary/x86_cache_pat/x86_cache_pat.md
+ * 64K是64K bytes!
+ */
 static struct fixed_mtrr_segment fixed_seg_table[] = {
 	/* MSR_MTRRfix64K_00000, 1 unit. 64K fixed mtrr. */
 	{
@@ -186,11 +401,28 @@ static struct fixed_mtrr_segment fixed_seg_table[] = {
  * The size of unit is covered in one MSR, one MSR entry contains
  * 8 ranges so that unit size is always 8 * 2^range_shift.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|223| <<fixed_mtrr_seg_unit_range>> u64 unit_size = fixed_mtrr_seg_unit_size(seg);
+ *   - arch/x86/kvm/mtrr.c|234| <<fixed_mtrr_seg_unit_range_index>> WARN_ON(mtrr_seg->start + unit * fixed_mtrr_seg_unit_size(seg)
+ *
+ * 一个msr能cover个8个和在一起是一个seg???
+ * 所以 8 << ...
+ */
 static u64 fixed_mtrr_seg_unit_size(int seg)
 {
 	return 8 << fixed_seg_table[seg].range_shift;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|254| <<fixed_msr_to_range>> if (!fixed_msr_to_seg_unit(msr, &seg, &unit))
+ *   - arch/x86/kvm/mtrr.c|265| <<fixed_msr_to_range_index>> if (!fixed_msr_to_seg_unit(msr, &seg, &unit))
+ *
+ * 把seg和unit在参数返回
+ * seg是相同size的(64K-bytes, 16K-bytes, 4K-bytes)的index: 0, 1, 2
+ * unit是在seg内的index
+ */
 static bool fixed_msr_to_seg_unit(u32 msr, int *seg, int *unit)
 {
 	switch (msr) {
@@ -217,6 +449,13 @@ static bool fixed_msr_to_seg_unit(u32 msr, int *seg, int *unit)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|391| <<fixed_msr_to_range>> fixed_mtrr_seg_unit_range(seg, unit, start, end);
+ *
+ * 通过参数的seg, unit,
+ * 获取对应的那一个(8个bit管理的range)的start和end地址
+ */
 static void fixed_mtrr_seg_unit_range(int seg, int unit, u64 *start, u64 *end)
 {
 	struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
@@ -227,6 +466,15 @@ static void fixed_mtrr_seg_unit_range(int seg, int unit, u64 *start, u64 *end)
 	WARN_ON(*end > mtrr_seg->end);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|402| <<fixed_msr_to_range_index>> return fixed_mtrr_seg_unit_range_index(seg, unit);
+ *
+ * 把seg和unit在参数返回
+ * seg是相同size的(64K-bytes, 16K-bytes, 4K-bytes)的index: 0, 1, 2
+ * unit是在seg内的index
+ * ---> 这里返回的这个(seg,unit)对应的起始的range的index
+ */
 static int fixed_mtrr_seg_unit_range_index(int seg, int unit)
 {
 	struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
@@ -238,6 +486,12 @@ static int fixed_mtrr_seg_unit_range_index(int seg, int unit)
 	return mtrr_seg->range_start + 8 * unit;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|730| <<mtrr_lookup_fixed_next>> if (iter->index > fixed_mtrr_seg_end_range_index(iter->seg))
+ *
+ * 这个seg(0, 1或者2)的range的最后一个的index
+ */
 static int fixed_mtrr_seg_end_range_index(int seg)
 {
 	struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
@@ -247,17 +501,41 @@ static int fixed_mtrr_seg_end_range_index(int seg)
 	return mtrr_seg->range_start + n - 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|485| <<update_mtrr>> if (fixed_msr_to_range(msr, &start, &end)) {
+ *
+ * 这里的输入的参数是msr, 一个msr可以管理8个range,
+ * 这里通过fixed_seg_table返回的是8个range的start/end地址
+ */
 static bool fixed_msr_to_range(u32 msr, u64 *start, u64 *end)
 {
 	int seg, unit;
 
+	/*
+	 * 把seg和unit在参数返回
+	 * seg是相同size的(64K-bytes, 16K-bytes, 4K-bytes)的index: 0, 1, 2
+	 * unit是在seg内的index
+	 */
 	if (!fixed_msr_to_seg_unit(msr, &seg, &unit))
 		return false;
 
+	/*
+	 * 通过参数的seg, unit,
+	 * 获取对应的那一个(8个bit管理的range)的start和end地址
+	 */
 	fixed_mtrr_seg_unit_range(seg, unit, start, end);
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|555| <<kvm_mtrr_set_msr>> index = fixed_msr_to_range_index(msr);
+ *   - arch/x86/kvm/mtrr.c|593| <<kvm_mtrr_get_msr>> index = fixed_msr_to_range_index(msr);
+ *
+ * 这里的输入的参数是msr, 一个msr可以管理8个range,
+ * 这里通过fixed_seg_table返回的是8个range的一起的start index
+ */
 static int fixed_msr_to_range_index(u32 msr)
 {
 	int seg, unit;
@@ -265,9 +543,21 @@ static int fixed_msr_to_range_index(u32 msr)
 	if (!fixed_msr_to_seg_unit(msr, &seg, &unit))
 		return -1;
 
+	/*
+	 * 把seg和unit在参数返回
+	 * seg是相同size的(64K-bytes, 16K-bytes, 4K-bytes)的index: 0, 1, 2
+	 * unit是在seg内的index
+	 * ---> 这里返回的这个(seg,unit)对应的起始的range的index
+	 */
 	return fixed_mtrr_seg_unit_range_index(seg, unit);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|478| <<mtrr_lookup_fixed_start>> seg = fixed_mtrr_addr_to_seg(iter->start);
+ *
+ * 给一个地址, 返回fixed_seg_table中的index (0, 1或者2)
+ */
 static int fixed_mtrr_addr_to_seg(u64 addr)
 {
 	struct fixed_mtrr_segment *mtrr_seg;
@@ -282,6 +572,12 @@ static int fixed_mtrr_addr_to_seg(u64 addr)
 	return -1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|483| <<mtrr_lookup_fixed_start>> index = fixed_mtrr_addr_seg_to_range_index(iter->start, seg);
+ *
+ * 返回一个地址在一个seg(fixed_seg_table)内部的range index
+ */
 static int fixed_mtrr_addr_seg_to_range_index(u64 addr, int seg)
 {
 	struct fixed_mtrr_segment *mtrr_seg;
@@ -293,6 +589,12 @@ static int fixed_mtrr_addr_seg_to_range_index(u64 addr, int seg)
 	return index;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|540| <<mtrr_lookup_fixed_next>> if (fixed_mtrr_range_end_addr(iter->seg, iter->index) >= iter->end) {
+ *
+ * 给一个seg, 和range在seg内部的index, 返回range的end address
+ */
 static u64 fixed_mtrr_range_end_addr(int seg, int index)
 {
 	struct fixed_mtrr_segment *mtrr_seg = &fixed_seg_table[seg];
@@ -301,6 +603,19 @@ static u64 fixed_mtrr_range_end_addr(int seg, int index)
 	return mtrr_seg->start + ((pos + 1) << mtrr_seg->range_shift);
 }
 
+/*
+ * struct kvm_mtrr_range {
+ *     u64 base;
+ *     u64 mask;
+ *     struct list_head node;
+ * };
+ *
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|338| <<update_mtrr>> var_mtrr_range(var_mtrr_msr_to_range(vcpu, msr), &start, &end);
+ *   - arch/x86/kvm/mtrr.c|494| <<match_var_range>> var_mtrr_range(range, &start, &end);
+ *
+ * 输入的参数是一个kvm_mtrr_range, 返回start和end地址
+ */
 static void var_mtrr_range(struct kvm_mtrr_range *range, u64 *start, u64 *end)
 {
 	u64 mask;
@@ -315,17 +630,32 @@ static void var_mtrr_range(struct kvm_mtrr_range *range, u64 *start, u64 *end)
 	*end = (*start | ~mask) + 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|393| <<kvm_mtrr_set_msr>> update_mtrr(vcpu, msr);
+ */
 static void update_mtrr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_mtrr *mtrr_state = &vcpu->arch.mtrr_state;
 	gfn_t start, end;
 
+	/*
+	 * 不honor的话直接return
+	 */
 	if (!kvm_mmu_honors_guest_mtrrs(vcpu->kvm))
 		return;
 
+	/*
+	 * 返回!!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_E)
+	 */
 	if (!mtrr_is_enabled(mtrr_state) && msr != MSR_MTRRdefType)
 		return;
 
+	/*
+	 * 如果是fixed:
+	 * 这里的输入的参数是msr, 一个msr可以管理8个range,
+	 * 这里通过fixed_seg_table返回的是8个range的start/end地址
+	 */
 	/* fixed MTRRs. */
 	if (fixed_msr_to_range(msr, &start, &end)) {
 		if (!fixed_mtrr_is_enabled(mtrr_state))
@@ -334,23 +664,60 @@ static void update_mtrr(struct kvm_vcpu *vcpu, u32 msr)
 		start = 0x0;
 		end = ~0ULL;
 	} else {
+		/*
+		 * 输入的参数是一个kvm_mtrr_range, 返回start和end地址
+		 */
 		/* variable range MTRRs. */
 		var_mtrr_range(var_mtrr_msr_to_range(vcpu, msr), &start, &end);
 	}
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mtrr.c|625| <<update_mtrr>> kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));
+	 *   - arch/x86/kvm/x86.c|967| <<kvm_post_set_cr0>> kvm_zap_gfn_range(vcpu->kvm, 0, ~0ULL);
+	 *   - arch/x86/kvm/x86.c|10615| <<__kvm_set_or_clear_apicv_inhibit>> kvm_zap_gfn_range(kvm, gfn, gfn+1);
+	 *   - arch/x86/kvm/x86.c|13432| <<kvm_noncoherent_dma_assignment_start_or_stop>> kvm_zap_gfn_range(kvm, gpa_to_gfn(0), gpa_to_gfn(~0ULL));
+	 */
 	kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|357| <<set_var_mtrr_msr>> if (var_mtrr_range_is_valid(cur))
+ *   - arch/x86/kvm/mtrr.c|370| <<set_var_mtrr_msr>> if (var_mtrr_range_is_valid(cur)) {
+ *
+ * struct kvm_mtrr_range {
+ *     u64 base;
+ *     u64 mask;
+ *     struct list_head node;
+ * };
+ */
 static bool var_mtrr_range_is_valid(struct kvm_mtrr_range *range)
 {
 	return (range->mask & (1 << 11)) != 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|391| <<kvm_mtrr_set_msr>> set_var_mtrr_msr(vcpu, msr, data);
+ */
 static void set_var_mtrr_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 {
+	/*
+	 * struct kvm_mtrr {
+	 *     struct kvm_mtrr_range var_ranges[KVM_NR_VAR_MTRR];
+	 *     mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
+	 *     u64 deftype;
+	 *
+	 *     struct list_head head;
+	 * };
+	 */
 	struct kvm_mtrr *mtrr_state = &vcpu->arch.mtrr_state;
 	struct kvm_mtrr_range *tmp, *cur;
 
+	/*
+	 * 根据msr(base或者mask)的index, 找到&vcpu->arch.mtrr_state.var_ranges[index]
+	 */
 	cur = var_mtrr_msr_to_range(vcpu, msr);
 
 	/* remove the entry if it's in the list. */
@@ -371,17 +738,37 @@ static void set_var_mtrr_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 		list_for_each_entry(tmp, &mtrr_state->head, node)
 			if (cur->base >= tmp->base)
 				break;
+		/*
+		 * struct kvm_mtrr_range {
+		 *     u64 base;
+		 *     u64 mask;
+		 *     struct list_head node;
+		 * };
+		 */
 		list_add_tail(&cur->node, &tmp->node);
 	}
 }
 
+/*
+ * 处理下面的MSR:
+ * 3851         case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ * 3852         case MSR_MTRRdefType:
+ *   - arch/x86/kvm/x86.c|3853| <<kvm_set_msr_common>> return kvm_mtrr_set_msr(vcpu, msr, data);
+ */
 int kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 {
 	int index;
 
+	/*
+	 * 只在这里调用
+	 */
 	if (!kvm_mtrr_valid(vcpu, msr, data))
 		return 1;
 
+	/*
+	 * 这里的输入的参数是msr, 一个msr可以管理8个range,
+	 * 这里通过fixed_seg_table返回的是8个range的一起的start index
+	 */
 	index = fixed_msr_to_range_index(msr);
 	if (index >= 0)
 		*(u64 *)&vcpu->arch.mtrr_state.fixed_ranges[index] = data;
@@ -390,10 +777,20 @@ int kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 	else
 		set_var_mtrr_msr(vcpu, msr, data);
 
+	/*
+	 * 只在此处调用
+	 */
 	update_mtrr(vcpu, msr);
 	return 0;
 }
 
+/*
+ * 处理下面的MSR:
+ * 4265         case MSR_MTRRcap:
+ * 4266         case MTRRphysBase_MSR(0) ... MSR_MTRRfix4K_F8000:
+ * 4267         case MSR_MTRRdefType:
+ *   - arch/x86/kvm/x86.c|4268| <<kvm_get_msr_common>> return kvm_mtrr_get_msr(vcpu, msr_info->index, &msr_info->data);
+ */
 int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata)
 {
 	int index;
@@ -431,21 +828,71 @@ int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12149| <<kvm_arch_vcpu_create>> kvm_vcpu_mtrr_init(vcpu);
+ */
 void kvm_vcpu_mtrr_init(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_mtrr {
+	 *     struct kvm_mtrr_range var_ranges[KVM_NR_VAR_MTRR];
+	 *     mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
+	 *     u64 deftype;
+	 *
+	 *     struct list_head head;
+	 * };
+	 */
 	INIT_LIST_HEAD(&vcpu->arch.mtrr_state.head);
 }
 
+/*
+ * 在以下使用mtrr_iter->mtrr_state:
+ *   - arch/x86/kvm/mtrr.c|690| <<mtrr_lookup_fixed_start>> if (!fixed_mtrr_is_enabled(iter->mtrr_state))
+ *   - arch/x86/kvm/mtrr.c|730| <<__mtrr_lookup_var_next>> struct kvm_mtrr *mtrr_state = iter->mtrr_state;
+ *   - arch/x86/kvm/mtrr.c|732| <<__mtrr_lookup_var_next>> list_for_each_entry_continue(iter->range, &mtrr_state->head, node)
+ *   - arch/x86/kvm/mtrr.c|742| <<mtrr_lookup_var_start>> struct kvm_mtrr *mtrr_state = iter->mtrr_state;
+ *   - arch/x86/kvm/mtrr.c|747| <<mtrr_lookup_var_start>> iter->range = list_prepare_entry(iter->range, &mtrr_state->head, node);
+ *   - arch/x86/kvm/mtrr.c|768| <<mtrr_lookup_fixed_next>> if (iter->index >= ARRAY_SIZE(iter->mtrr_state->fixed_ranges))
+ *   - arch/x86/kvm/mtrr.c|791| <<mtrr_lookup_start>> if (!mtrr_is_enabled(iter->mtrr_state)) {
+ *   - arch/x86/kvm/mtrr.c|807| <<mtrr_lookup_init>> iter->mtrr_state = mtrr_state;
+ *   - arch/x86/kvm/mtrr.c|825| <<mtrr_lookup_okay>> iter->mem_type = iter->mtrr_state->fixed_ranges[iter->index];
+ *   - arch/x86/kvm/mtrr.c|874| <<kvm_mtrr_get_guest_memory_type>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ *   - arch/x86/kvm/mtrr.c|961| <<kvm_mtrr_check_gfn_range_consistency>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ */
 struct mtrr_iter {
 	/* input fields. */
 	struct kvm_mtrr *mtrr_state;
 	u64 start;
 	u64 end;
 
+	/*
+	 * 在以下使用mtrr_iter->mem_type:
+	 *   - arch/x86/kvm/mtrr.c|825| <<mtrr_lookup_okay>> iter->mem_type = iter->mtrr_state->fixed_ranges[iter->index];
+	 *   - arch/x86/kvm/mtrr.c|830| <<mtrr_lookup_okay>> iter->mem_type = iter->range->base & 0xff;
+	 *   - arch/x86/kvm/mtrr.c|875| <<kvm_mtrr_get_guest_memory_type>> int curr_type = iter.mem_type;
+	 *   - arch/x86/kvm/mtrr.c|963| <<kvm_mtrr_check_gfn_range_consistency>> type = iter.mem_type; 
+	 *   - arch/x86/kvm/mtrr.c|967| <<kvm_mtrr_check_gfn_range_consistency>> if (type != iter.mem_type)
+	 */
 	/* output fields. */
 	int mem_type;
+	/*
+	 * 在以下使用mtrr_iter->mtrr_disabled:
+	 *   - arch/x86/kvm/mtrr.c|792| <<mtrr_lookup_start>> iter->mtrr_disabled = true;
+	 *   - arch/x86/kvm/mtrr.c|810| <<mtrr_lookup_init>> iter->mtrr_disabled = false;
+	 *   - arch/x86/kvm/mtrr.c|921| <<kvm_mtrr_get_guest_memory_type>> if (iter.mtrr_disabled)
+	 *   - arch/x86/kvm/mtrr.c|971| <<kvm_mtrr_check_gfn_range_consistency>> if (iter.mtrr_disabled)
+	 */
 	/* mtrr is completely disabled? */
 	bool mtrr_disabled;
+	/*
+	 * 在以下使用mtrr_iter->partial_map:
+	 *   - arch/x86/kvm/mtrr.c|855| <<match_var_range>> iter->partial_map |= iter->start_max < start;
+	 *   - arch/x86/kvm/mtrr.c|874| <<__mtrr_lookup_var_next>> iter->partial_map |= iter->start_max < iter->end;
+	 *   - arch/x86/kvm/mtrr.c|948| <<mtrr_lookup_init>> iter->partial_map = false;
+	 *   - arch/x86/kvm/mtrr.c|1069| <<kvm_mtrr_get_guest_memory_type>> WARN_ON(iter.partial_map);
+	 *   - arch/x86/kvm/mtrr.c|1111| <<kvm_mtrr_check_gfn_range_consistency>> if (!iter.partial_map)
+	 */
 	/* [start, end) is not fully covered in MTRRs? */
 	bool partial_map;
 
@@ -460,37 +907,73 @@ struct mtrr_iter {
 		/* used for var MTRRs. */
 		struct {
 			struct kvm_mtrr_range *range;
+			/*
+			 * 在以下使用mtrr_iter的start_max:
+			 *   - arch/x86/kvm/mtrr.c|855| <<match_var_range>> iter->partial_map |= iter->start_max < start;
+			 *   - arch/x86/kvm/mtrr.c|858| <<match_var_range>> iter->start_max = max(iter->start_max, end);
+			 *   - arch/x86/kvm/mtrr.c|874| <<__mtrr_lookup_var_next>> iter->partial_map |= iter->start_max < iter->end;
+			 *   - arch/x86/kvm/mtrr.c|882| <<mtrr_lookup_var_start>> iter->start_max = iter->start;
+			 */
 			/* max address has been covered in var MTRRs. */
 			u64 start_max;
 		};
 	};
 
+	/*
+	 * 在以下使用mtrr_iter->fixed:
+	 *   - arch/x86/kvm/mtrr.c|834| <<mtrr_lookup_fixed_start>> iter->fixed = true;
+	 *   - arch/x86/kvm/mtrr.c|881| <<mtrr_lookup_var_start>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|897| <<mtrr_lookup_fixed_next>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|949| <<mtrr_lookup_init>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|961| <<mtrr_lookup_okay>> if (iter->fixed) {
+	 *   - arch/x86/kvm/mtrr.c|980| <<mtrr_lookup_next>> if (iter->fixed)
+	 */
 	bool fixed;
 };
 
+/*
+ * called by;
+ *   - arch/x86/kvm/mtrr.c|1047| <<mtrr_lookup_start>> if (!mtrr_lookup_fixed_start(iter))
+ */
 static bool mtrr_lookup_fixed_start(struct mtrr_iter *iter)
 {
 	int seg, index;
 
+	/*
+	 * 返回!!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_FE)
+	 */
 	if (!fixed_mtrr_is_enabled(iter->mtrr_state))
 		return false;
 
+	/*
+	 * 给一个地址, 返回fixed_seg_table中的index (0, 1或者2)
+	 */
 	seg = fixed_mtrr_addr_to_seg(iter->start);
 	if (seg < 0)
 		return false;
 
 	iter->fixed = true;
+	/*
+	 * 返回一个地址在一个seg(fixed_seg_table)内部的range index
+	 */
 	index = fixed_mtrr_addr_seg_to_range_index(iter->start, seg);
 	iter->index = index;
 	iter->seg = seg;
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|984| <<__mtrr_lookup_var_next>> if (match_var_range(iter, iter->range))
+ */
 static bool match_var_range(struct mtrr_iter *iter,
 			    struct kvm_mtrr_range *range)
 {
 	u64 start, end;
 
+	/*
+	 * 输入的参数是一个kvm_mtrr_range, 返回start和end地址
+	 */
 	var_mtrr_range(range, &start, &end);
 	if (!(start >= iter->end || end <= iter->start)) {
 		iter->range = range;
@@ -510,6 +993,11 @@ static bool match_var_range(struct mtrr_iter *iter,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|1000| <<mtrr_lookup_var_start>> __mtrr_lookup_var_next(iter);
+ *   - arch/x86/kvm/mtrr.c|1033| <<mtrr_lookup_var_next>> __mtrr_lookup_var_next(iter);
+ */
 static void __mtrr_lookup_var_next(struct mtrr_iter *iter)
 {
 	struct kvm_mtrr *mtrr_state = iter->mtrr_state;
@@ -522,6 +1010,11 @@ static void __mtrr_lookup_var_next(struct mtrr_iter *iter)
 	iter->partial_map |= iter->start_max < iter->end;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|1020| <<mtrr_lookup_fixed_next>> return mtrr_lookup_var_start(iter);
+ *   - arch/x86/kvm/mtrr.c|1048| <<mtrr_lookup_start>> mtrr_lookup_var_start(iter);
+ */
 static void mtrr_lookup_var_start(struct mtrr_iter *iter)
 {
 	struct kvm_mtrr *mtrr_state = iter->mtrr_state;
@@ -529,13 +1022,34 @@ static void mtrr_lookup_var_start(struct mtrr_iter *iter)
 	iter->fixed = false;
 	iter->start_max = iter->start;
 	iter->range = NULL;
+	/*
+	 * struct kvm_mtrr_range *range;
+	 *
+	 * struct kvm_mtrr_range {
+	 *     u64 base;
+	 *     u64 mask;
+	 *     struct list_head node;
+	 * };
+	 */
 	iter->range = list_prepare_entry(iter->range, &mtrr_state->head, node);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mtrr.c|1000| <<mtrr_lookup_var_start>> __mtrr_lookup_var_next(iter);
+	 *   - arch/x86/kvm/mtrr.c|1033| <<mtrr_lookup_var_next>> __mtrr_lookup_var_next(iter);
+	 */
 	__mtrr_lookup_var_next(iter);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|809| <<mtrr_lookup_next>> mtrr_lookup_fixed_next(iter);
+ */
 static void mtrr_lookup_fixed_next(struct mtrr_iter *iter)
 {
+	/*
+	 * 给一个seg, 和range在seg内部的index, 返回range的end address
+	 */
 	/* terminate the lookup. */
 	if (fixed_mtrr_range_end_addr(iter->seg, iter->index) >= iter->end) {
 		iter->fixed = false;
@@ -549,19 +1063,45 @@ static void mtrr_lookup_fixed_next(struct mtrr_iter *iter)
 	if (iter->index >= ARRAY_SIZE(iter->mtrr_state->fixed_ranges))
 		return mtrr_lookup_var_start(iter);
 
+	/*
+	 * 这个seg(0, 1或者2)的range的最后一个的index
+	 */
 	/* switch to next segment. */
 	if (iter->index > fixed_mtrr_seg_end_range_index(iter->seg))
 		iter->seg++;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|811| <<mtrr_lookup_next>> mtrr_lookup_var_next(iter);
+ */
 static void mtrr_lookup_var_next(struct mtrr_iter *iter)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mtrr.c|1000| <<mtrr_lookup_var_start>> __mtrr_lookup_var_next(iter);
+	 *   - arch/x86/kvm/mtrr.c|1033| <<mtrr_lookup_var_next>> __mtrr_lookup_var_next(iter);
+	 */
 	__mtrr_lookup_var_next(iter);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|788| <<mtrr_lookup_init>> mtrr_lookup_start(iter);
+ */
 static void mtrr_lookup_start(struct mtrr_iter *iter)
 {
+	/*
+	 * 返回!!(mtrr_state->deftype & IA32_MTRR_DEF_TYPE_E)
+	 */
 	if (!mtrr_is_enabled(iter->mtrr_state)) {
+		/*
+		 * 在以下使用mtrr_iter->mtrr_disabled:
+		 *   - arch/x86/kvm/mtrr.c|792| <<mtrr_lookup_start>> iter->mtrr_disabled = true;
+		 *   - arch/x86/kvm/mtrr.c|810| <<mtrr_lookup_init>> iter->mtrr_disabled = false;
+		 *   - arch/x86/kvm/mtrr.c|921| <<kvm_mtrr_get_guest_memory_type>> if (iter.mtrr_disabled)
+		 *   - arch/x86/kvm/mtrr.c|971| <<kvm_mtrr_check_gfn_range_consistency>> if (iter.mtrr_disabled)
+		 */
 		iter->mtrr_disabled = true;
 		return;
 	}
@@ -570,6 +1110,14 @@ static void mtrr_lookup_start(struct mtrr_iter *iter)
 		mtrr_lookup_var_start(iter);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|830| <<kvm_mtrr_get_guest_memory_type>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ *   - arch/x86/kvm/mtrr.c|917| <<kvm_mtrr_check_gfn_range_consistency>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ *
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|815| <<mtrr_for_each_mem_type>> for (mtrr_lookup_init(_iter_, _mtrr_, _gpa_start_, _gpa_end_); \
+ */
 static void mtrr_lookup_init(struct mtrr_iter *iter,
 			     struct kvm_mtrr *mtrr_state, u64 start, u64 end)
 {
@@ -577,6 +1125,10 @@ static void mtrr_lookup_init(struct mtrr_iter *iter,
 	iter->start = start;
 	iter->end = end;
 	iter->mtrr_disabled = false;
+	/*
+	 * 注释:
+	 * [start, end) is not fully covered in MTRRs?
+	 */
 	iter->partial_map = false;
 	iter->fixed = false;
 	iter->range = NULL;
@@ -584,9 +1136,18 @@ static void mtrr_lookup_init(struct mtrr_iter *iter,
 	mtrr_lookup_start(iter);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|816| <<mtrr_for_each_mem_type>> mtrr_lookup_okay(_iter_); mtrr_lookup_next(_iter_))
+ */
 static bool mtrr_lookup_okay(struct mtrr_iter *iter)
 {
 	if (iter->fixed) {
+		/*
+		 * 注释:
+		 * In the Intel processor's MTRR interface, the MTRR type is always held in
+		 * an 8 bit field:
+		 */
 		iter->mem_type = iter->mtrr_state->fixed_ranges[iter->index];
 		return true;
 	}
@@ -599,18 +1160,51 @@ static bool mtrr_lookup_okay(struct mtrr_iter *iter)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|816| <<mtrr_for_each_mem_type>> mtrr_lookup_okay(_iter_); mtrr_lookup_next(_iter_))
+ */
 static void mtrr_lookup_next(struct mtrr_iter *iter)
 {
+	/*
+	 * 在以下使用mtrr_iter->fixed:
+	 *   - arch/x86/kvm/mtrr.c|834| <<mtrr_lookup_fixed_start>> iter->fixed = true;
+	 *   - arch/x86/kvm/mtrr.c|881| <<mtrr_lookup_var_start>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|897| <<mtrr_lookup_fixed_next>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|949| <<mtrr_lookup_init>> iter->fixed = false;
+	 *   - arch/x86/kvm/mtrr.c|961| <<mtrr_lookup_okay>> if (iter->fixed) {
+	 *   - arch/x86/kvm/mtrr.c|980| <<mtrr_lookup_next>> if (iter->fixed)
+	 */
 	if (iter->fixed)
 		mtrr_lookup_fixed_next(iter);
 	else
 		mtrr_lookup_var_next(iter);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|830| <<kvm_mtrr_get_guest_memory_type>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ *   - arch/x86/kvm/mtrr.c|917| <<kvm_mtrr_check_gfn_range_consistency>> mtrr_for_each_mem_type(&iter, mtrr_state, start, end) {
+ */
 #define mtrr_for_each_mem_type(_iter_, _mtrr_, _gpa_start_, _gpa_end_) \
 	for (mtrr_lookup_init(_iter_, _mtrr_, _gpa_start_, _gpa_end_); \
 	     mtrr_lookup_okay(_iter_); mtrr_lookup_next(_iter_))
 
+/*
+ * 返回下面这些:
+ * // MTRR memory types, which are defined in SDM
+ * #define MTRR_TYPE_UNCACHABLE 0
+ * #define MTRR_TYPE_WRCOMB     1
+ * //#define MTRR_TYPE_         2
+ * //#define MTRR_TYPE_         3
+ * #define MTRR_TYPE_WRTHROUGH  4
+ * #define MTRR_TYPE_WRPROT     5
+ * #define MTRR_TYPE_WRBACK     6
+ * #define MTRR_NUM_TYPES       7
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7624| <<vmx_get_mt_mask>> return kvm_mtrr_get_guest_memory_type(vcpu, gfn) << VMX_EPT_MT_EPTE_SHIFT;
+ */
 u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	struct kvm_mtrr *mtrr_state = &vcpu->arch.mtrr_state;
@@ -677,6 +1271,14 @@ u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn)
 	if (type == -1)
 		return mtrr_default_type(mtrr_state);
 
+	/*
+	 * 在以下使用mtrr_iter->partial_map:
+	 *   - arch/x86/kvm/mtrr.c|855| <<match_var_range>> iter->partial_map |= iter->start_max < start;
+	 *   - arch/x86/kvm/mtrr.c|874| <<__mtrr_lookup_var_next>> iter->partial_map |= iter->start_max < iter->end;
+	 *   - arch/x86/kvm/mtrr.c|948| <<mtrr_lookup_init>> iter->partial_map = false;
+	 *   - arch/x86/kvm/mtrr.c|1069| <<kvm_mtrr_get_guest_memory_type>> WARN_ON(iter.partial_map);
+	 *   - arch/x86/kvm/mtrr.c|1111| <<kvm_mtrr_check_gfn_range_consistency>> if (!iter.partial_map)
+	 */
 	/*
 	 * We just check one page, partially covered by MTRRs is
 	 * impossible.
@@ -687,9 +1289,22 @@ u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_mtrr_get_guest_memory_type);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4636| <<kvm_tdp_page_fault>> if (kvm_mtrr_check_gfn_range_consistency(vcpu, base, page_num))
+ */
 bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
 					  int page_num)
 {
+	/*
+	 * struct kvm_mtrr {
+	 *     struct kvm_mtrr_range var_ranges[KVM_NR_VAR_MTRR];
+	 *     mtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];
+	 *     u64 deftype;
+	 *
+	 *     struct list_head head;
+	 * };
+	 */
 	struct kvm_mtrr *mtrr_state = &vcpu->arch.mtrr_state;
 	struct mtrr_iter iter;
 	u64 start, end;
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 87cc6c880..75804e01f 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -170,6 +170,16 @@ static u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 	return sample_period;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|492| <<reprogram_counter>> if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ *
+ * 496         if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ * 497                                   (eventsel & pmu->raw_event_mask),
+ * 498                                   !(eventsel & ARCH_PERFMON_EVENTSEL_USR),
+ * 499                                   !(eventsel & ARCH_PERFMON_EVENTSEL_OS),
+ * 500                                   eventsel & ARCH_PERFMON_EVENTSEL_INT))
+ */
 static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 				 bool exclude_user, bool exclude_kernel,
 				 bool intr)
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 7caeb3d8d..1735d78bb 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -138,6 +138,10 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 
 extern struct x86_pmu_capability kvm_pmu_cap;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9719| <<__kvm_x86_vendor_init>> kvm_init_pmu_capability(ops->pmu_ops);
+ */
 static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index e90b429c8..07b8aacdd 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -2055,6 +2055,12 @@ static int npf_interception(struct kvm_vcpu *vcpu)
 	u64 fault_address = svm->vmcb->control.exit_info_2;
 	u64 error_code = svm->vmcb->control.exit_info_1;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4686| <<kvm_handle_page_fault>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+	 *   - arch/x86/kvm/svm/svm.c|2058| <<npf_interception>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+	 *   - arch/x86/kvm/vmx/vmx.c|5765| <<handle_ept_violation>> trace_kvm_page_fault(vcpu, gpa, exit_qualification);
+	 */
 	trace_kvm_page_fault(vcpu, fault_address, error_code);
 	return kvm_mmu_page_fault(vcpu, fault_address, error_code,
 			static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index 6329a3068..658f46583 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -437,6 +437,11 @@ static void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,
 	vmcs12->guest_physical_address = fault->address;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|456| <<nested_ept_init_mmu_context>> nested_ept_new_eptp(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5899| <<nested_vmx_eptp_switching>> nested_ept_new_eptp(vcpu);
+ */
 static void nested_ept_new_eptp(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -448,6 +453,10 @@ static void nested_ept_new_eptp(struct kvm_vcpu *vcpu)
 				nested_ept_get_eptp(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2636| <<prepare_vmcs02>> nested_ept_init_mmu_context(vcpu);
+ */
 static void nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
 {
 	WARN_ON(mmu_is_nested(vcpu));
@@ -461,6 +470,11 @@ static void nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
 	vcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4547| <<load_vmcs12_host_state>> nested_ept_uninit_mmu_context(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4697| <<nested_vmx_restore_host_state>> nested_ept_uninit_mmu_context(vcpu);
+ */
 static void nested_ept_uninit_mmu_context(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.mmu = &vcpu->arch.root_mmu;
@@ -2567,6 +2581,10 @@ static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
  * Returns 0 on success, 1 on failure. Invalid state exit qualification code
  * is assigned to entry_failure_code on failure.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3516| <<nested_vmx_enter_non_root_mode>> if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &entry_failure_code)) {
+ */
 static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			  bool from_vmentry,
 			  enum vm_entry_failure_code *entry_failure_code)
@@ -4654,6 +4672,10 @@ static inline u64 nested_vmx_get_vmcs01_guest_efer(struct vcpu_vmx *vmx)
 	return host_efer;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4951| <<nested_vmx_vmexit>> nested_vmx_restore_host_state(vcpu);
+ */
 static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -4759,6 +4781,25 @@ static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
 	nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/hyperv.c|228| <<vmx_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_vmx_vmexit(vcpu, HV_VMX_SYNTHETIC_EXIT_REASON_TRAP_AFTER_FLUSH, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|436| <<nested_ept_inject_page_fault>> nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification);
+ *   - arch/x86/kvm/vmx/nested.c|3953| <<nested_vmx_inject_exception_vmexit>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI, intr_info, exit_qual);
+ *   - arch/x86/kvm/vmx/nested.c|4129| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_INIT_SIGNAL, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4143| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_SIPI_SIGNAL, 0,
+ *   - arch/x86/kvm/vmx/nested.c|4180| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_MONITOR_TRAP_FLAG, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4201| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4217| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,
+ *   - arch/x86/kvm/vmx/nested.c|4234| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4959| <<nested_vmx_triple_fault>> nested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|5958| <<handle_vmfunc>> nested_vmx_vmexit(vcpu, vmx->exit_reason.full,
+ *   - arch/x86/kvm/vmx/nested.c|6446| <<nested_vmx_reflect_vmexit>> nested_vmx_vmexit(vcpu, exit_reason.full, exit_intr_info, exit_qual);
+ *   - arch/x86/kvm/vmx/nested.c|6577| <<vmx_leave_nested>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.h|29| <<vmx_leave_nested>> void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
+ *   - arch/x86/kvm/vmx/vmx.c|6471| <<__vmx_handle_exit>> nested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|8162| <<vmx_enter_smm>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ */
 /*
  * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
  * and modify vmcs12 to make it see what it would expect to see there if
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 88a4ff200..a00d544d9 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -4783,6 +4783,22 @@ static void init_vmcs(struct vcpu_vmx *vmx)
 	/* 22.2.1, 20.8.1 */
 	vm_entry_controls_set(vmx, vmx_vmentry_ctrl());
 
+	/*
+	 * 关于cr0/cr4的passthrough:
+	 * VM-execution control fields include guest/host masks and read shadows for the CR0 and CR4 registers. These
+	 * fields control executions of instructions that access those registers (including CLTS, LMSW, MOV CR, and SMSW).
+	 * They are 64 bits on processors that support Intel 64 architecture and 32 bits on processors that do not.
+	 *
+	 * In general, bits set to 1 in a guest/host mask correspond to bits "owned" by the host:
+	 *
+	 * 1. Guest attempts to set them (using CLTS, LMSW, or MOV to CR) to values differing from the corresponding bits
+	 * in the corresponding read shadow cause VM exits.
+	 *
+	 * 2. Guest reads (using MOV from CR or SMSW) return values for these bits from the corresponding read shadow.
+	 *
+	 * Bits cleared to 0 correspond to bits "owned" by the guest; guest
+	 * attempts to modify them succeed and guest reads return values for these bits from the control register itself.
+	 */
 	vmx->vcpu.arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
 	vmcs_writel(CR0_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr0_guest_owned_bits);
 
@@ -5762,6 +5778,12 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 		vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
 
 	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4686| <<kvm_handle_page_fault>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+	 *   - arch/x86/kvm/svm/svm.c|2058| <<npf_interception>> trace_kvm_page_fault(vcpu, fault_address, error_code);
+	 *   - arch/x86/kvm/vmx/vmx.c|5765| <<handle_ept_violation>> trace_kvm_page_fault(vcpu, gpa, exit_qualification);
+	 */
 	trace_kvm_page_fault(vcpu, gpa, exit_qualification);
 
 	/* Is it a read fault? */
@@ -7610,9 +7632,23 @@ static u8 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 	if (is_mmio)
 		return MTRR_TYPE_UNCACHABLE << VMX_EPT_MT_EPTE_SHIFT;
 
+	/*
+	 * 没有non-cohenrent设备的时候
+	 */
 	if (!kvm_arch_has_noncoherent_dma(vcpu->kvm))
 		return (MTRR_TYPE_WRBACK << VMX_EPT_MT_EPTE_SHIFT) | VMX_EPT_IPAT_BIT;
 
+	/*
+	 * 关于cr0.cd:
+	 * - If CR0.CD = 0, the memory type used for any such reference is the
+	 *   EPT paging-structure memory type, which is specified in bits 2:0 of
+	 *   the extended-page-table pointer (EPTP), a VM-execution control field
+	 *   (see Section 25.6.11). A value of 0 indicates the uncacheable type (UC),
+	 *   while a value of 6 indicates the write-back type (WB). Other values
+	 *   are reserved.
+	 *
+	 * - If CR0.CD = 1, the memory type used for any such reference is uncacheable (UC).
+	 */
 	if (kvm_read_cr0_bits(vcpu, X86_CR0_CD)) {
 		if (kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))
 			return MTRR_TYPE_WRBACK << VMX_EPT_MT_EPTE_SHIFT;
@@ -7621,6 +7657,9 @@ static u8 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 				VMX_EPT_IPAT_BIT;
 	}
 
+	/*
+	 * 只在此处调用
+	 */
 	return kvm_mtrr_get_guest_memory_type(vcpu, gfn) << VMX_EPT_MT_EPTE_SHIFT;
 }
 
@@ -8448,6 +8487,10 @@ static __init void vmx_setup_user_return_msrs(void)
 		kvm_add_user_return_msr(vmx_uret_msrs_list[i]);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8617| <<hardware_setup>> vmx_setup_me_spte_mask();
+ */
 static void __init vmx_setup_me_spte_mask(void)
 {
 	u64 me_mask = 0;
@@ -8475,6 +8518,12 @@ static void __init vmx_setup_me_spte_mask(void)
 
 static struct kvm_x86_init_ops vmx_init_ops __initdata;
 
+/*
+ * 在以下使用hardware_setup():
+ *   - arch/x86/kvm/svm/svm.c|5329| <<global>> .hardware_setup = svm_hardware_setup,
+ *   - arch/x86/kvm/vmx/vmx.c|8688| <<global>> .hardware_setup = hardware_setup,
+ *   - arch/x86/kvm/x86.c|9760| <<__kvm_x86_vendor_init>> r = ops->hardware_setup();
+ */
 static __init int hardware_setup(void)
 {
 	unsigned long host_bndcfgs;
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index e3b0985bb..defcdd29b 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -631,6 +631,19 @@ BUILD_CONTROLS_SHADOW(tertiary_exec, TERTIARY_VM_EXEC_CONTROL, 64)
 				(1 << VCPU_EXREG_EXIT_INFO_1) | \
 				(1 << VCPU_EXREG_EXIT_INFO_2))
 
+/*
+ * 在以下使用kvm_vcpu_arch->cr0_guest_owned_bits:
+ *   - arch/x86/kvm/kvm_cache_regs.h|154| <<kvm_read_cr0_bits>> if ((tmask & vcpu->arch.cr0_guest_owned_bits) &&
+ *   - arch/x86/kvm/vmx/nested.c|2622| <<prepare_vmcs02>> vcpu->arch.cr0_guest_owned_bits &= ~vmcs12->cr0_guest_host_mask;
+ *   - arch/x86/kvm/vmx/nested.c|2623| <<prepare_vmcs02>> vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu->arch.cr0_guest_owned_bits);
+ *   - arch/x86/kvm/vmx/nested.c|3773| <<vmcs12_guest_cr0>>  (vmcs_readl(GUEST_CR0) & vcpu->arch.cr0_guest_owned_bits) |
+ *   - arch/x86/kvm/vmx/nested.c|3776| <<vmcs12_guest_cr0>> vcpu->arch.cr0_guest_owned_bits));
+ *   - arch/x86/kvm/vmx/nested.c|4553| <<load_vmcs12_host_state>> vcpu->arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+ *   - arch/x86/kvm/vmx/nested.c|4708| <<nested_vmx_restore_host_state>> vcpu->arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+ *   - arch/x86/kvm/vmx/vmx.c|2485| <<vmx_cache_reg>> guest_owned_bits = vcpu->arch.cr0_guest_owned_bits;
+ *   - arch/x86/kvm/vmx/vmx.c|4786| <<init_vmcs>> vmx->vcpu.arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();
+ *   - arch/x86/kvm/vmx/vmx.c|4787| <<init_vmcs>> vmcs_writel(CR0_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr0_guest_owned_bits);
+ */
 static inline unsigned long vmx_l1_guest_owned_cr0_bits(void)
 {
 	unsigned long bits = KVM_POSSIBLE_CR0_GUEST_BITS;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index e02cc710f..7090d7cdb 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -132,6 +132,13 @@ static int kvm_vcpu_do_singlestep(struct kvm_vcpu *vcpu);
 static int __set_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2);
 static void __get_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2);
 
+/*
+ * 在以下使用vendor_module_lock:
+ *   - arch/x86/kvm/x86.c|9792| <<kvm_x86_vendor_init>> mutex_lock(&vendor_module_lock);
+ *   - arch/x86/kvm/x86.c|9794| <<kvm_x86_vendor_init>> mutex_unlock(&vendor_module_lock);
+ *   - arch/x86/kvm/x86.c|9828| <<kvm_x86_vendor_exit>> mutex_lock(&vendor_module_lock);
+ *   - arch/x86/kvm/x86.c|9830| <<kvm_x86_vendor_exit>> mutex_unlock(&vendor_module_lock);
+ */
 static DEFINE_MUTEX(vendor_module_lock);
 struct kvm_x86_ops kvm_x86_ops __read_mostly;
 
@@ -926,6 +933,11 @@ static bool kvm_is_valid_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 	return static_call(kvm_x86_is_valid_cr0)(vcpu, cr0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2714| <<cr_trap>> kvm_post_set_cr0(vcpu, old_value, new_value);
+ *   - arch/x86/kvm/x86.c|1006| <<kvm_set_cr0>> kvm_post_set_cr0(vcpu, old_cr0, cr0);
+ */
 void kvm_post_set_cr0(struct kvm_vcpu *vcpu, unsigned long old_cr0, unsigned long cr0)
 {
 	/*
@@ -7374,6 +7386,10 @@ static void kvm_probe_msr_to_save(u32 msr_index)
 	msrs_to_save[num_msrs_to_save++] = msr_index;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9773| <<__kvm_x86_vendor_init>> kvm_init_msr_lists();
+ */
 static void kvm_init_msr_lists(void)
 {
 	unsigned i;
@@ -7690,6 +7706,11 @@ int handle_ud(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(handle_ud);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7751| <<vcpu_mmio_gva_to_gpa>> return vcpu_is_mmio_gpa(vcpu, gva, *gpa, write);
+ *   - arch/x86/kvm/x86.c|7860| <<emulator_read_write_onepage>> ret = vcpu_is_mmio_gpa(vcpu, addr, gpa, write);
+ */
 static int vcpu_is_mmio_gpa(struct kvm_vcpu *vcpu, unsigned long gva,
 			    gpa_t gpa, bool write)
 {
@@ -7697,7 +7718,17 @@ static int vcpu_is_mmio_gpa(struct kvm_vcpu *vcpu, unsigned long gva,
 	if ((gpa & PAGE_MASK) == APIC_DEFAULT_PHYS_BASE)
 		return 1;
 
+	/*
+	 * 如果下面的满足返回true, 否则false:
+	 * 1. vcpu_arch缓存的mmio_gen和memslots的相等
+	 * 2. vcpu->arch.mmio_gfn和参数的gpa互相match
+	 */
 	if (vcpu_match_mmio_gpa(vcpu, gpa)) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|7717| <<vcpu_is_mmio_gpa>> trace_vcpu_match_mmio(gva, gpa, write, true);
+		 *   - arch/x86/kvm/x86.c|7742| <<vcpu_mmio_gva_to_gpa>> trace_vcpu_match_mmio(gva, *gpa, write, false);
+		 */
 		trace_vcpu_match_mmio(gva, gpa, write, true);
 		return 1;
 	}
@@ -7705,14 +7736,35 @@ static int vcpu_is_mmio_gpa(struct kvm_vcpu *vcpu, unsigned long gva,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7862| <<emulator_read_write_onepage>> ret = vcpu_mmio_gva_to_gpa(vcpu, addr, &gpa, exception, write);
+ */
 static int vcpu_mmio_gva_to_gpa(struct kvm_vcpu *vcpu, unsigned long gva,
 				gpa_t *gpa, struct x86_exception *exception,
 				bool write)
 {
 	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
+	/*
+	 * 如果cpl是3, access要有PFERR_USER_MASK
+	 * 如果是写操作, access要有PFERR_WRITE_MASK
+	 */
 	u64 access = ((static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0)
 		| (write ? PFERR_WRITE_MASK : 0);
 
+	/*
+	 * 在以下使用mmio_access:
+	 *   - arch/x86/kvm/x86.c|7739| <<vcpu_mmio_gva_to_gpa>> if (vcpu_match_mmio_gva(vcpu, gva) &&
+	 *                                         (!is_paging(vcpu) || !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access))) {
+	 *   - arch/x86/kvm/x86.h|244| <<vcpu_cache_mmio_info>> vcpu->arch.mmio_access = access;
+	 *
+	 * 下面的条件:
+	 * 1. vcpu_match_mmio_gva(vcpu, gva), 和(and)
+	 * 2. !is_paging(vcpu), or
+	 *    !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access)
+	 *
+	 * 我们应该是希望permission_fault()返回0
+	 */
 	/*
 	 * currently PKRU is only applied to ept enabled guest so
 	 * there is no pkey in EPT page table for L1 guest or EPT
@@ -7723,6 +7775,11 @@ static int vcpu_mmio_gva_to_gpa(struct kvm_vcpu *vcpu, unsigned long gva,
 			      vcpu->arch.mmio_access, 0, access))) {
 		*gpa = vcpu->arch.mmio_gfn << PAGE_SHIFT |
 					(gva & (PAGE_SIZE - 1));
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|7717| <<vcpu_is_mmio_gpa>> trace_vcpu_match_mmio(gva, gpa, write, true);
+		 *   - arch/x86/kvm/x86.c|7742| <<vcpu_mmio_gva_to_gpa>> trace_vcpu_match_mmio(gva, *gpa, write, false);
+		 */
 		trace_vcpu_match_mmio(gva, *gpa, write, false);
 		return 1;
 	}
@@ -7819,6 +7876,11 @@ static const struct read_write_emulator_ops write_emultor = {
 	.write = true,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7910| <<emulator_read_write>> rc = emulator_read_write_onepage(addr, val, now, exception, vcpu, ops);
+ *   - arch/x86/kvm/x86.c|7922| <<emulator_read_write>> rc = emulator_read_write_onepage(addr, val, bytes, exception, vcpu, ops);
+ */
 static int emulator_read_write_onepage(unsigned long addr, void *val,
 				       unsigned int bytes,
 				       struct x86_exception *exception,
@@ -9602,6 +9664,10 @@ static struct notifier_block pvclock_gtod_notifier = {
 };
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9728| <<__kvm_x86_vendor_init>> kvm_ops_update(ops);
+ */
 static inline void kvm_ops_update(struct kvm_x86_init_ops *ops)
 {
 	memcpy(&kvm_x86_ops, ops->runtime_ops, sizeof(kvm_x86_ops));
@@ -9645,6 +9711,10 @@ static void kvm_x86_check_cpu_compat(void *ret)
 	*(int *)ret = kvm_x86_check_processor_compatibility();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9793| <<kvm_x86_vendor_init>> r = __kvm_x86_vendor_init(ops);
+ */
 static int __kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 {
 	u64 host_pat;
@@ -9689,6 +9759,23 @@ static int __kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 		return -ENOMEM;
 	}
 
+	/*
+	 * 210 //
+	 * 211 // Restoring the host value for MSRs that are only consumed when running in
+	 * 212 // usermode, e.g. SYSCALL MSRs and TSC_AUX, can be deferred until the CPU
+	 * 213 // returns to userspace, i.e. the kernel can run with the guest's value.
+	 * 214 //
+	 * 215 #define KVM_MAX_NR_USER_RETURN_MSRS 16
+	 * 216
+	 * 217 struct kvm_user_return_msrs {
+	 * 218         struct user_return_notifier urn;
+	 * 219         bool registered;
+	 * 220         struct kvm_user_return_msr_values {
+	 * 221                 u64 host;
+	 * 222                 u64 curr;
+	 * 223         } values[KVM_MAX_NR_USER_RETURN_MSRS];
+	 * 224 };
+	 */
 	user_return_msrs = alloc_percpu(struct kvm_user_return_msrs);
 	if (!user_return_msrs) {
 		pr_err("failed to allocate percpu kvm_user_return_msrs\n");
@@ -9780,10 +9867,22 @@ static int __kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 	return r;
 }
 
+/*
+ * called by:
+ *   -  arch/x86/kvm/svm/svm.c|5351| <<svm_init>> r = kvm_x86_vendor_init(&svm_init_ops);
+ *   - arch/x86/kvm/vmx/vmx.c|8736| <<vmx_init>> r = kvm_x86_vendor_init(&vmx_init_ops);
+ */
 int kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 {
 	int r;
 
+	/*
+	 * 在以下使用vendor_module_lock:
+	 *   - arch/x86/kvm/x86.c|9792| <<kvm_x86_vendor_init>> mutex_lock(&vendor_module_lock);
+	 *   - arch/x86/kvm/x86.c|9794| <<kvm_x86_vendor_init>> mutex_unlock(&vendor_module_lock);
+	 *   - arch/x86/kvm/x86.c|9828| <<kvm_x86_vendor_exit>> mutex_lock(&vendor_module_lock);
+	 *   - arch/x86/kvm/x86.c|9830| <<kvm_x86_vendor_exit>> mutex_unlock(&vendor_module_lock);
+	 */
 	mutex_lock(&vendor_module_lock);
 	r = __kvm_x86_vendor_init(ops);
 	mutex_unlock(&vendor_module_lock);
@@ -12380,6 +12479,10 @@ void kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_deliver_sipi_vector);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5571| <<__hardware_enable_nolock>> if (kvm_arch_hardware_enable()) {
+ */
 int kvm_arch_hardware_enable(void)
 {
 	struct kvm *kvm;
@@ -12396,6 +12499,10 @@ int kvm_arch_hardware_enable(void)
 	if (ret)
 		return ret;
 
+	/*
+	 * vmx_hardware_enable()
+	 * svm_hardware_enable()
+	 */
 	ret = static_call(kvm_x86_hardware_enable)();
 	if (ret != 0)
 		return ret;
@@ -12765,6 +12872,31 @@ int memslot_rmap_alloc(struct kvm_memory_slot *slot, unsigned long npages)
 	return 0;
 }
 
+/*
+ * 580 struct kvm_memory_slot {
+ * 581         struct hlist_node id_node[2];
+ * 582         struct interval_tree_node hva_node[2];
+ * 583         struct rb_node gfn_node[2];
+ * 584         gfn_t base_gfn;
+ * 585         unsigned long npages;
+ * 586         unsigned long *dirty_bitmap;
+ * 587         struct kvm_arch_memory_slot arch;
+ * 588         unsigned long userspace_addr;
+ * 589         u32 flags;
+ * 590         short id;
+ * 591         u16 as_id;
+ * 592
+ * 593 #ifdef CONFIG_KVM_PRIVATE_MEM
+ * 594         struct {
+ * 595                 struct file __rcu *file;
+ * 596                 pgoff_t pgoff;
+ * 597         } gmem;
+ * 598 #endif
+ * 599 };
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|12874| <<kvm_arch_prepare_memory_region>> return kvm_alloc_memslot_metadata(kvm, new);
+ */
 static int kvm_alloc_memslot_metadata(struct kvm *kvm,
 				      struct kvm_memory_slot *slot)
 {
@@ -12784,20 +12916,66 @@ static int kvm_alloc_memslot_metadata(struct kvm *kvm,
 			return r;
 	}
 
+	/*
+	 * 就是1和2
+	 */
 	for (i = 1; i < KVM_NR_PAGE_SIZES; ++i) {
 		struct kvm_lpage_info *linfo;
 		unsigned long ugfn;
 		int lpages;
+		/*
+		 * level = 1 + 1 = 2
+		 * level = 2 + 1 = 3
+		 * 
+		 * 2和3
+		 */
 		int level = i + 1;
 
+		/*
+		 * 假设:
+		 * base_gfn: 133693442
+		 * gfn     : 134060443
+		 *
+		 * 对于level 2:
+		 * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 9       = 261836
+		 * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 9 = 261120
+		 * 261836 - 261120 = 716
+		 *
+		 * 对于level 3:
+		 * gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 134060443 >> 18       = 511
+		 * (base_gfn >> KVM_HPAGE_GFN_SHIFT(level)) = 133693442 >> 18 = 510
+		 * 511 - 510 = 0
+		 *
+		 * level 3 (index 1, 1G)有1个slot
+		 * level 2 (index 2, 2M)有717个slot
+		 *
+		 * 实际:
+		 * [  465.699816] kvm: orabug: alloc i=1, lpages=2, level=2
+		 * [  465.705963] kvm: orabug: alloc i=2, lpages=1, level=3
+		 */
 		lpages = __kvm_mmu_slot_lpages(slot, npages, level);
 
+		/*
+		 * struct kvm_lpage_info *linfo;
+		 */
 		linfo = __vcalloc(lpages, sizeof(*linfo), GFP_KERNEL_ACCOUNT);
 		if (!linfo)
 			goto out_free;
 
+		/*
+		 * i是1和2
+		 */
 		slot->arch.lpage_info[i - 1] = linfo;
 
+		/*
+		 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+		 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512           ---> level 2
+		 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144        ---> level 3
+		 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+		 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+		 *
+		 * level是2和3
+		 */
 		if (slot->base_gfn & (KVM_PAGES_PER_HPAGE(level) - 1))
 			linfo[0].disallow_lpage = 1;
 		if ((slot->base_gfn + npages) & (KVM_PAGES_PER_HPAGE(level) - 1))
@@ -12850,6 +13028,10 @@ void kvm_arch_memslots_updated(struct kvm *kvm, u64 gen)
 		kvm_vcpu_kick(vcpu);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1720| <<kvm_prepare_memory_region>> r = kvm_arch_prepare_memory_region(kvm, old, new, change);
+ */
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   const struct kvm_memory_slot *old,
 				   struct kvm_memory_slot *new,
@@ -13419,6 +13601,11 @@ bool noinstr kvm_arch_has_assigned_device(struct kvm *kvm)
 }
 EXPORT_SYMBOL_GPL(kvm_arch_has_assigned_device);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13438| <<kvm_arch_register_noncoherent_dma>> kvm_noncoherent_dma_assignment_start_or_stop(kvm);
+ *   - arch/x86/kvm/x86.c|13445| <<kvm_arch_unregister_noncoherent_dma>> kvm_noncoherent_dma_assignment_start_or_stop(kvm);
+ */
 static void kvm_noncoherent_dma_assignment_start_or_stop(struct kvm *kvm)
 {
 	/*
@@ -13432,6 +13619,10 @@ static void kvm_noncoherent_dma_assignment_start_or_stop(struct kvm *kvm)
 		kvm_zap_gfn_range(kvm, gpa_to_gfn(0), gpa_to_gfn(~0ULL));
 }
 
+/*
+ * called by:
+ *   - virt/kvm/vfio.c|137| <<kvm_vfio_update_coherency>> kvm_arch_register_noncoherent_dma(dev->kvm);
+ */
 void kvm_arch_register_noncoherent_dma(struct kvm *kvm)
 {
 	if (atomic_inc_return(&kvm->arch.noncoherent_dma_count) == 1)
@@ -13446,6 +13637,12 @@ void kvm_arch_unregister_noncoherent_dma(struct kvm *kvm)
 }
 EXPORT_SYMBOL_GPL(kvm_arch_unregister_noncoherent_dma);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|258| <<kvm_mmu_honors_guest_mtrrs>> return __kvm_mmu_honors_guest_mtrrs(kvm_arch_has_noncoherent_dma(kvm));
+ *   - arch/x86/kvm/vmx/vmx.c|7613| <<vmx_get_mt_mask>> if (!kvm_arch_has_noncoherent_dma(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|4956| <<need_emulate_wbinvd>> return kvm_arch_has_noncoherent_dma(vcpu->kvm);
+ */
 bool kvm_arch_has_noncoherent_dma(struct kvm *kvm)
 {
 	return atomic_read(&kvm->arch.noncoherent_dma_count);
@@ -13911,6 +14108,10 @@ EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_exit);
 EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_msr_protocol_enter);
 EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_msr_protocol_exit);
 
+/*
+ * module的entry:
+ *   - arch/x86/kvm/x86.c|14063| <<global>> module_init(kvm_x86_init);
+ */
 static int __init kvm_x86_init(void)
 {
 	kvm_mmu_x86_module_init();
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 2f7e19166..16f625134 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -215,6 +215,11 @@ static inline bool is_noncanonical_address(u64 la, struct kvm_vcpu *vcpu)
 	return !__is_canonical_address(la, vcpu_virt_addr_bits(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3339| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+ *   - arch/x86/kvm/mmu/mmu.c|4219| <<handle_mmio_page_fault>> vcpu_cache_mmio_info(vcpu, addr, gfn, access);
+ */
 static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 					gva_t gva, gfn_t gfn, unsigned access)
 {
@@ -223,16 +228,37 @@ static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 	if (unlikely(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS))
 		return;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> u64 mmio_gva;
+	 *    -> unsigned mmio_access;
+	 *    -> gfn_t mmio_gfn;
+	 *    -> u64 mmio_gen;
+	 */
 	/*
 	 * If this is a shadow nested page table, the "GVA" is
 	 * actually a nGPA.
 	 */
 	vcpu->arch.mmio_gva = mmu_is_nested(vcpu) ? 0 : gva & PAGE_MASK;
+	/*
+	 * 在以下使用mmio_access:
+	 *   - arch/x86/kvm/x86.c|7739| <<vcpu_mmio_gva_to_gpa>> if (vcpu_match_mmio_gva(vcpu, gva) &&
+	 *                                         (!is_paging(vcpu) || !permission_fault(vcpu, vcpu->arch.walk_mmu, vcpu->arch.mmio_access, 0, access))) {
+	 *   - arch/x86/kvm/x86.h|244| <<vcpu_cache_mmio_info>> vcpu->arch.mmio_access = access;
+	 */
 	vcpu->arch.mmio_access = access;
 	vcpu->arch.mmio_gfn = gfn;
 	vcpu->arch.mmio_gen = gen;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.h|257| <<vcpu_match_mmio_gva>> if (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gva &&
+ *   - arch/x86/kvm/x86.h|266| <<vcpu_match_mmio_gpa>> if (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gfn &&
+ *
+ * vcpu_arch缓存的mmio_gen和memslots的相等
+ */
 static inline bool vcpu_match_mmio_gen(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.mmio_gen == kvm_memslots(vcpu->kvm)->generation;
@@ -244,6 +270,13 @@ static inline bool vcpu_match_mmio_gen(struct kvm_vcpu *vcpu)
  */
 #define MMIO_GVA_ANY (~(gva_t)0)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4054| <<kvm_mmu_sync_roots>> vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
+ *   - arch/x86/kvm/mmu/mmu.c|4846| <<kvm_mmu_new_pgd>> vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
+ *   - arch/x86/kvm/mmu/mmu.c|5691| <<kvm_mmu_unload>> vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
+ *   - arch/x86/kvm/mmu/mmu.c|5969| <<__kvm_mmu_invalidate_addr>> vcpu_clear_mmio_info(vcpu, addr);
+ */
 static inline void vcpu_clear_mmio_info(struct kvm_vcpu *vcpu, gva_t gva)
 {
 	if (gva != MMIO_GVA_ANY && vcpu->arch.mmio_gva != (gva & PAGE_MASK))
@@ -252,6 +285,11 @@ static inline void vcpu_clear_mmio_info(struct kvm_vcpu *vcpu, gva_t gva)
 	vcpu->arch.mmio_gva = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4118| <<mmio_info_in_cache>> return vcpu_match_mmio_gva(vcpu, addr);
+ *   - arch/x86/kvm/x86.c|7737| <<vcpu_mmio_gva_to_gpa>> if (vcpu_match_mmio_gva(vcpu, gva) && (!is_paging(vcpu) ||
+ */
 static inline bool vcpu_match_mmio_gva(struct kvm_vcpu *vcpu, unsigned long gva)
 {
 	if (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gva &&
@@ -261,8 +299,21 @@ static inline bool vcpu_match_mmio_gva(struct kvm_vcpu *vcpu, unsigned long gva)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4116| <<mmio_info_in_cache>> return vcpu_match_mmio_gpa(vcpu, addr);
+ *   - arch/x86/kvm/x86.c|7716| <<vcpu_is_mmio_gpa>> if (vcpu_match_mmio_gpa(vcpu, gpa)) {
+ *
+ * 如果下面的满足返回true, 否则false:
+ * 1. vcpu_arch缓存的mmio_gen和memslots的相等
+ * 2. vcpu->arch.mmio_gfn和参数的gpa互相match
+ */
 static inline bool vcpu_match_mmio_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
+	/*
+	 * vcpu_match_mmio_gen(vcpu):
+	 * vcpu_arch缓存的mmio_gen和memslots的相等
+	 */
 	if (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gfn &&
 	      vcpu->arch.mmio_gfn == gpa >> PAGE_SHIFT)
 		return true;
diff --git a/drivers/perf/arm_pmu.c b/drivers/perf/arm_pmu.c
index 8458fe2ce..3b8ae453f 100644
--- a/drivers/perf/arm_pmu.c
+++ b/drivers/perf/arm_pmu.c
@@ -909,6 +909,11 @@ void armpmu_free(struct arm_pmu *pmu)
 	kfree(pmu);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmu_acpi.c|418| <<arm_pmu_acpi_probe>> ret = armpmu_register(pmu);
+ *   - drivers/perf/arm_pmu_platform.c|231| <<arm_pmu_device_probe>> ret = armpmu_register(pmu);
+ */
 int armpmu_register(struct arm_pmu *pmu)
 {
 	int ret;
diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c
index df5ac03d5..8f20b727b 100644
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -1757,6 +1757,10 @@ static blk_status_t scsi_queue_rq(struct blk_mq_hw_ctx *hctx,
 	cmd->submitter = SUBMITTED_BY_BLOCK_LAYER;
 
 	blk_mq_start_request(req);
+	/*
+	 * struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+	 * -> unsigned char cmnd[32]; // SCSI CDB
+	 */
 	reason = scsi_dispatch_cmd(cmd);
 	if (reason) {
 		scsi_set_blocked(cmd, reason);
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 617eb892f..cc14395da 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -444,6 +444,18 @@ static int __virtscsi_add_cmd(struct virtqueue *vq,
 			in = &sc->sdb.table;
 	}
 
+	/*
+	 * struct virtio_scsi_cmd_req {
+	 *     __u8 lun[8];            // Logical Unit Number
+	 *     __virtio64 tag;         // Command identifier
+	 *     __u8 task_attr;         // Task attribute
+	 *     __u8 prio;              // SAM command priority field
+	 *     __u8 crn;
+	 *     __u8 cdb[VIRTIO_SCSI_CDB_SIZE];
+	 * } __attribute__((packed));
+	 *
+	 * struct virtio_scsi_cmd *cmd
+	 */
 	/* Request header.  */
 	sg_init_one(&req, &cmd->req, req_size);
 	sgs[out_num++] = &req;
@@ -562,6 +574,10 @@ static struct virtio_scsi_vq *virtscsi_pick_vq_mq(struct virtio_scsi *vscsi,
 	return &vscsi->req_vqs[hwq];
 }
 
+/*
+ * struct scsi_cmnd *sc:
+ * -> unsigned char cmnd[32]; // SCSI CDB
+ */
 static int virtscsi_queuecommand(struct Scsi_Host *shost,
 				 struct scsi_cmnd *sc)
 {
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 179df96b2..a7f36e693 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -836,6 +836,15 @@ struct kvm {
 	struct notifier_block pm_notifier;
 #endif
 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
+	/*
+	 * 在以下使用kvm->mem_attr_array:
+	 *   - include/linux/kvm_host.h|2381| <<kvm_get_memory_attributes>> return xa_to_value(xa_load(&kvm->mem_attr_array, gfn));
+	 *   - virt/kvm/kvm_main.c|1213| <<kvm_create_vm>> xa_init(&kvm->mem_attr_array);
+	 *   - virt/kvm/kvm_main.c|1395| <<kvm_destroy_vm>> xa_destroy(&kvm->mem_attr_array);
+	 *   - virt/kvm/kvm_main.c|2469| <<kvm_range_has_memory_attributes>> XA_STATE(xas, &kvm->mem_attr_array, start);
+	 *   - virt/kvm/kvm_main.c|2606| <<kvm_vm_set_mem_attributes>> r = xa_reserve(&kvm->mem_attr_array, i, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|2614| <<kvm_vm_set_mem_attributes>> r = xa_err(xa_store(&kvm->mem_attr_array, i, entry, GFP_KERNEL_ACCOUNT));
+	 */
 	/* Protected by slots_locks (for writes) and RCU (for reads) */
 	struct xarray mem_attr_array;
 #endif
@@ -2378,6 +2387,15 @@ static inline void kvm_prepare_memory_fault_exit(struct kvm_vcpu *vcpu,
 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
 static inline unsigned long kvm_get_memory_attributes(struct kvm *kvm, gfn_t gfn)
 {
+	/*
+	 * 在以下使用kvm->mem_attr_array:
+	 *   - include/linux/kvm_host.h|2381| <<kvm_get_memory_attributes>> return xa_to_value(xa_load(&kvm->mem_attr_array, gfn));
+	 *   - virt/kvm/kvm_main.c|1213| <<kvm_create_vm>> xa_init(&kvm->mem_attr_array);
+	 *   - virt/kvm/kvm_main.c|1395| <<kvm_destroy_vm>> xa_destroy(&kvm->mem_attr_array);
+	 *   - virt/kvm/kvm_main.c|2469| <<kvm_range_has_memory_attributes>> XA_STATE(xas, &kvm->mem_attr_array, start);
+	 *   - virt/kvm/kvm_main.c|2606| <<kvm_vm_set_mem_attributes>> r = xa_reserve(&kvm->mem_attr_array, i, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|2614| <<kvm_vm_set_mem_attributes>> r = xa_err(xa_store(&kvm->mem_attr_array, i, entry, GFP_KERNEL_ACCOUNT));
+	 */
 	return xa_to_value(xa_load(&kvm->mem_attr_array, gfn));
 }
 
diff --git a/tools/testing/selftests/kvm/include/kvm_util_base.h b/tools/testing/selftests/kvm/include/kvm_util_base.h
index 9e5afc472..e06a44811 100644
--- a/tools/testing/selftests/kvm/include/kvm_util_base.h
+++ b/tools/testing/selftests/kvm/include/kvm_util_base.h
@@ -536,6 +536,16 @@ static inline int __vm_create_guest_memfd(struct kvm_vm *vm, uint64_t size,
 	return __vm_ioctl(vm, KVM_CREATE_GUEST_MEMFD, &guest_memfd);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/guest_memfd_test.c|189| <<main>> fd = vm_create_guest_memfd(vm, total_size, 0);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1031| <<vm_mem_add>> guest_memfd = vm_create_guest_memfd(vm, mem_size, guest_memfd_flags);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|370| <<test_invalid_memory_region_flags>> int guest_memfd = vm_create_guest_memfd(vm, MEM_REGION_SIZE, 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|475| <<test_add_private_memory_region>> memfd = vm_create_guest_memfd(vm2, MEM_REGION_SIZE, 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|483| <<test_add_private_memory_region>> memfd = vm_create_guest_memfd(vm, MEM_REGION_SIZE, 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|504| <<test_add_overlapping_private_memory_regions>> memfd = vm_create_guest_memfd(vm, MEM_REGION_SIZE * 4, 0);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|398| <<test_mem_conversions>> memfd = vm_create_guest_memfd(vm, memfd_size, 0);
+ */
 static inline int vm_create_guest_memfd(struct kvm_vm *vm, uint64_t size,
 					uint64_t flags)
 {
diff --git a/tools/testing/selftests/kvm/include/x86_64/processor.h b/tools/testing/selftests/kvm/include/x86_64/processor.h
index 5bca8c947..7b7b25b7a 100644
--- a/tools/testing/selftests/kvm/include/x86_64/processor.h
+++ b/tools/testing/selftests/kvm/include/x86_64/processor.h
@@ -1201,6 +1201,11 @@ static inline uint64_t __kvm_hypercall_map_gpa_range(uint64_t gpa,
 	return kvm_hypercall(KVM_HC_MAP_GPA_RANGE, gpa, size >> PAGE_SHIFT, flags, 0);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|99| <<guest_map_mem>> kvm_hypercall_map_gpa_range(gpa, size, flags);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|223| <<guest_punch_hole>> kvm_hypercall_map_gpa_range(gpa, size, flags);
+ */
 static inline void kvm_hypercall_map_gpa_range(uint64_t gpa, uint64_t size,
 					       uint64_t flags)
 {
diff --git a/tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c b/tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c
index 65ad38b6b..e1d7b53d9 100644
--- a/tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c
+++ b/tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c
@@ -24,9 +24,19 @@
 #include <processor.h>
 
 #define BASE_DATA_SLOT		10
+/*
+ * 4G: 4294967296
+ */
 #define BASE_DATA_GPA		((uint64_t)(1ull << 32))
+/*
+ * 0x00200000 = 2MB
+ * 2MB + 4KB
+ */
 #define PER_CPU_DATA_SIZE	((uint64_t)(SZ_2M + PAGE_SIZE))
 
+/*
+ * size的每一个是uint8_t
+ */
 /* Horrific macro so that the line info is captured accurately :-( */
 #define memcmp_g(gpa, pattern,  size)								\
 do {												\
@@ -71,12 +81,23 @@ enum ucall_syncs {
 	SYNC_PRIVATE,
 };
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|133| <<guest_test_explicit_conversion>> guest_sync_shared(base_gpa, PER_CPU_DATA_SIZE, def_p, init_p);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|187| <<guest_test_explicit_conversion>> guest_sync_shared(gpa + j, PAGE_SIZE, p1, p3);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|203| <<guest_test_explicit_conversion>> guest_sync_shared(gpa, size, p3, p4);
+ */
 static void guest_sync_shared(uint64_t gpa, uint64_t size,
 			      uint8_t current_pattern, uint8_t new_pattern)
 {
 	GUEST_SYNC5(SYNC_SHARED, gpa, size, current_pattern, new_pattern);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|167| <<guest_test_explicit_conversion>> guest_sync_private(gpa, size, p1);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|191| <<guest_test_explicit_conversion>> guest_sync_private(gpa + j, PAGE_SIZE, p1);
+ */
 static void guest_sync_private(uint64_t gpa, uint64_t size, uint8_t pattern)
 {
 	GUEST_SYNC4(SYNC_PRIVATE, gpa, size, pattern);
@@ -87,6 +108,11 @@ static void guest_sync_private(uint64_t gpa, uint64_t size, uint8_t pattern)
 #define MAP_GPA_SHARED		BIT(1)
 #define MAP_GPA_DO_FALLOCATE	BIT(2)
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|131| <<guest_map_shared>> guest_map_mem(gpa, size, true, do_fallocate);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|141| <<guest_map_private>> guest_map_mem(gpa, size, false, do_fallocate);
+ */
 static void guest_map_mem(uint64_t gpa, uint64_t size, bool map_shared,
 			  bool do_fallocate)
 {
@@ -99,16 +125,36 @@ static void guest_map_mem(uint64_t gpa, uint64_t size, bool map_shared,
 	kvm_hypercall_map_gpa_range(gpa, size, flags);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|186| <<guest_test_explicit_conversion>> guest_map_shared(gpa + j, PAGE_SIZE, do_fallocate);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|201| <<guest_test_explicit_conversion>> guest_map_shared(gpa, size, do_fallocate);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|214| <<guest_test_explicit_conversion>> guest_map_shared(base_gpa, PER_CPU_DATA_SIZE, true);
+ */
 static void guest_map_shared(uint64_t gpa, uint64_t size, bool do_fallocate)
 {
 	guest_map_mem(gpa, size, true, do_fallocate);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|159| <<guest_test_explicit_conversion>> guest_map_private(gpa, size, do_fallocate);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|240| <<guest_test_punch_hole>> guest_map_private(base_gpa, PER_CPU_DATA_SIZE, false);
+ */
 static void guest_map_private(uint64_t gpa, uint64_t size, bool do_fallocate)
 {
 	guest_map_mem(gpa, size, false, do_fallocate);
 }
 
+/*
+ * 在以下使用test_ranges[]:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|137| <<guest_test_explicit_conversion>> for (i = 0; i < ARRAY_SIZE(test_ranges); i++) {
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|138| <<guest_test_explicit_conversion>> uint64_t gpa = base_gpa + test_ranges[i].offset;
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|139| <<guest_test_explicit_conversion>> uint64_t size = test_ranges[i].size;
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|242| <<guest_test_punch_hole>> for (i = 0; i < ARRAY_SIZE(test_ranges); i++) {
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|243| <<guest_test_punch_hole>> uint64_t gpa = base_gpa + test_ranges[i].offset;
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|244| <<guest_test_punch_hole>> uint64_t size = test_ranges[i].size;
+ */
 struct {
 	uint64_t offset;
 	uint64_t size;
@@ -120,6 +166,11 @@ struct {
 	GUEST_STAGE(SZ_2M, PAGE_SIZE),
 };
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|319| <<guest_code>> guest_test_explicit_conversion(base_gpa, false);
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|320| <<guest_code>> guest_test_explicit_conversion(base_gpa, true);
+ */
 static void guest_test_explicit_conversion(uint64_t base_gpa, bool do_fallocate)
 {
 	const uint8_t def_p = 0xaa;
@@ -134,6 +185,14 @@ static void guest_test_explicit_conversion(uint64_t base_gpa, bool do_fallocate)
 
 	memcmp_g(base_gpa, init_p, PER_CPU_DATA_SIZE);
 
+	/*
+	 * ranges:
+	 * 162         GUEST_STAGE(0, PAGE_SIZE),
+	 * 163         GUEST_STAGE(0, SZ_2M),
+	 * 164         GUEST_STAGE(PAGE_SIZE, PAGE_SIZE),
+	 * 165         GUEST_STAGE(PAGE_SIZE, SZ_2M),
+	 * 166         GUEST_STAGE(SZ_2M, PAGE_SIZE),
+	 */
 	for (i = 0; i < ARRAY_SIZE(test_ranges); i++) {
 		uint64_t gpa = base_gpa + test_ranges[i].offset;
 		uint64_t size = test_ranges[i].size;
@@ -310,8 +369,18 @@ static void handle_exit_hypercall(struct kvm_vcpu *vcpu)
 	run->hypercall.ret = 0;
 }
 
+/*
+ * 在以下使用run_vcpus:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|357| <<global>> static bool run_vcpus;
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|370| <<__test_mem_conversions>> while (!READ_ONCE(run_vcpus))
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|494| <<test_mem_conversions>> WRITE_ONCE(run_vcpus, true);
+ */
 static bool run_vcpus;
 
+/*
+ * 在以下使用__test_mem_conversions():
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|416| <<test_mem_conversions>> pthread_create(&threads[i], NULL, __test_mem_conversions, vcpus[i]);
+ */
 static void *__test_mem_conversions(void *__vcpu)
 {
 	struct kvm_vcpu *vcpu = __vcpu;
@@ -319,6 +388,12 @@ static void *__test_mem_conversions(void *__vcpu)
 	struct kvm_vm *vm = vcpu->vm;
 	struct ucall uc;
 
+	/*
+	 * 在以下使用run_vcpus:
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|357| <<global>> static bool run_vcpus;
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|370| <<__test_mem_conversions>> while (!READ_ONCE(run_vcpus))
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|494| <<test_mem_conversions>> WRITE_ONCE(run_vcpus, true);
+	 */
 	while (!READ_ONCE(run_vcpus))
 		;
 
@@ -367,6 +442,10 @@ static void *__test_mem_conversions(void *__vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|479| <<main>> test_mem_conversions(src_type, nr_vcpus, nr_memslots);
+ */
 static void test_mem_conversions(enum vm_mem_backing_src_type src_type, uint32_t nr_vcpus,
 				 uint32_t nr_memslots)
 {
@@ -374,9 +453,21 @@ static void test_mem_conversions(enum vm_mem_backing_src_type src_type, uint32_t
 	 * Allocate enough memory so that each vCPU's chunk of memory can be
 	 * naturally aligned with respect to the size of the backing store.
 	 */
+	/*
+	 * 默认2097152 = 2MB
+	 */
 	const size_t alignment = max_t(size_t, SZ_2M, get_backing_src_pagesz(src_type));
+	/*
+	 * 2MB + 4K, 会align_up到4194304=4MB
+	 */
 	const size_t per_cpu_size = align_up(PER_CPU_DATA_SIZE, alignment);
+	/*
+	 * 如果1个vcpu, 是4MB
+	 */
 	const size_t memfd_size = per_cpu_size * nr_vcpus;
+	/*
+	 * 如果一个slot, 是4MB ---> 1024个4k的page
+	 */
 	const size_t slot_size = memfd_size / nr_memslots;
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
 	pthread_t threads[KVM_MAX_VCPUS];
@@ -391,12 +482,23 @@ static void test_mem_conversions(enum vm_mem_backing_src_type src_type, uint32_t
 	TEST_ASSERT(slot_size * nr_memslots == memfd_size,
 		    "The memfd size (0x%lx) needs to be cleanly divisible by the number of memslots (%u)",
 		    memfd_size, nr_memslots);
+	/*
+	 * 在kvm调用kvm_alloc_memslot_metadata() 2次:
+	 * 1. base=0, npages=657, id=0
+	 * 2. base=1043968, npages=1, id=32765
+	 */
 	vm = __vm_create_with_vcpus(shape, nr_vcpus, 0, guest_code, vcpus);
 
 	vm_enable_cap(vm, KVM_CAP_EXIT_HYPERCALL, (1 << KVM_HC_MAP_GPA_RANGE));
 
 	memfd = vm_create_guest_memfd(vm, memfd_size, 0);
 
+	/*
+	 * 如果只有一个slot, base=1048576, npages=1024, id=10:
+	 * BASE_DATA_GPA + slot_size * i : 4GB + 4MB  ---> guest_paddr
+	 * BASE_DATA_SLOT + i : 10                    ---> slot
+	 * slot_size / vm->page_size : 1024           ---> npages
+	 */
 	for (i = 0; i < nr_memslots; i++)
 		vm_mem_add(vm, src_type, BASE_DATA_GPA + slot_size * i,
 			   BASE_DATA_SLOT + i, slot_size / vm->page_size,
@@ -416,6 +518,12 @@ static void test_mem_conversions(enum vm_mem_backing_src_type src_type, uint32_t
 		pthread_create(&threads[i], NULL, __test_mem_conversions, vcpus[i]);
 	}
 
+	/*
+	 * 在以下使用run_vcpus:
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|357| <<global>> static bool run_vcpus;
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|370| <<__test_mem_conversions>> while (!READ_ONCE(run_vcpus))
+	 *   - tools/testing/selftests/kvm/x86_64/private_mem_conversions_test.c|494| <<test_mem_conversions>> WRITE_ONCE(run_vcpus, true);
+	 */
 	WRITE_ONCE(run_vcpus, true);
 
 	for (i = 0; i < nr_vcpus; i++)
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 0f50960b0..3b4a294fd 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -1688,6 +1688,10 @@ static void kvm_swap_active_memslots(struct kvm *kvm, int as_id)
 	slots->generation = gen;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1950| <<kvm_set_memslot>> r = kvm_prepare_memory_region(kvm, old, new, change);
+ */
 static int kvm_prepare_memory_region(struct kvm *kvm,
 				     const struct kvm_memory_slot *old,
 				     struct kvm_memory_slot *new,
@@ -1901,6 +1905,11 @@ static void kvm_update_flags_memslot(struct kvm *kvm,
 	kvm_activate_memslot(kvm, old, new);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2076| <<__kvm_set_memory_region>> return kvm_set_memslot(kvm, old, NULL, KVM_MR_DELETE);
+ *   - virt/kvm/kvm_main.c|2129| <<__kvm_set_memory_region>> r = kvm_set_memslot(kvm, old, new, change);
+ */
 static int kvm_set_memslot(struct kvm *kvm,
 			   struct kvm_memory_slot *old,
 			   struct kvm_memory_slot *new,
@@ -2017,6 +2026,11 @@ static bool kvm_check_memslot_overlap(struct kvm_memslots *slots, int id,
  *
  * Must be called holding kvm->slots_lock for write.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12680| <<__x86_set_memory_region>> r = __kvm_set_memory_region(kvm, &m);
+ *   - virt/kvm/kvm_main.c|2150| <<kvm_set_memory_region>> r = __kvm_set_memory_region(kvm, mem);
+ */
 int __kvm_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region2 *mem)
 {
@@ -2438,9 +2452,20 @@ static int kvm_vm_ioctl_clear_dirty_log(struct kvm *kvm,
  * Returns true if _all_ gfns in the range [@start, @end) have attributes
  * matching @attrs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7407| <<hugepage_has_attrs>> return kvm_range_has_memory_attributes(kvm, start, end, attrs);
+ *   - virt/kvm/kvm_main.c|2573| <<kvm_vm_set_mem_attributes>> if (kvm_range_has_memory_attributes(kvm, start, end, attributes))
+ */
 bool kvm_range_has_memory_attributes(struct kvm *kvm, gfn_t start, gfn_t end,
 				     unsigned long attrs)
 {
+	/*
+	 * 838 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
+	 * 839         // Protected by slots_locks (for writes) and RCU (for reads)
+	 * 840         struct xarray mem_attr_array;
+	 * 841 #endif
+	 */
 	XA_STATE(xas, &kvm->mem_attr_array, start);
 	unsigned long index;
 	bool has_attrs;
@@ -2541,6 +2566,9 @@ static bool kvm_pre_set_memory_attributes(struct kvm *kvm,
 	return kvm_arch_pre_set_memory_attributes(kvm, range);
 }
 
+/*
+ * attributes可以是0或者KVM_MEMORY_ATTRIBUTE_PRIVATE???
+ */
 /* Set @attributes for the gfn range [@start, @end). */
 static int kvm_vm_set_mem_attributes(struct kvm *kvm, gfn_t start, gfn_t end,
 				     unsigned long attributes)
@@ -2585,6 +2613,9 @@ static int kvm_vm_set_mem_attributes(struct kvm *kvm, gfn_t start, gfn_t end,
 
 	kvm_handle_gfn_range(kvm, &pre_set_range);
 
+	/*
+	 * 这里是set, 设置!!!
+	 */
 	for (i = start; i < end; i++) {
 		r = xa_err(xa_store(&kvm->mem_attr_array, i, entry,
 				    GFP_KERNEL_ACCOUNT));
@@ -2759,6 +2790,19 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_hva);
  * @writable: used to return the read/write attribute of the @slot if the hva
  * is valid and @writable is not NULL
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1683| <<kvm_handle_guest_abort>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+ *   - arch/loongarch/kvm/mmu.c|819| <<kvm_map_page>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writeable);
+ *   - arch/riscv/kvm/vcpu_exit.c|25| <<gstage_page_fault>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+ *   - arch/s390/kvm/gaccess.c|1006| <<access_guest_page_with_key>> hva = gfn_to_hva_memslot_prot(slot, gfn, &writable);
+ *   - arch/s390/kvm/gaccess.c|1184| <<cmpxchg_guest_abs_with_key>> hva = gfn_to_hva_memslot_prot(slot, gfn, &writable);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|399| <<FNAME(walk_addr_generic)>> host_addr = gfn_to_hva_memslot_prot(slot, gpa_to_gfn(real_gpa), &walker->pte_writable[walker->level - 1]);
+ *   - virt/kvm/kvm_main.c|2808| <<gfn_to_hva_prot>> return gfn_to_hva_memslot_prot(slot, gfn, writable);
+ *   - virt/kvm/kvm_main.c|2815| <<kvm_vcpu_gfn_to_hva_prot>> return gfn_to_hva_memslot_prot(slot, gfn, writable);
+ *   - virt/kvm/kvm_main.c|3361| <<__kvm_read_guest_page>> addr = gfn_to_hva_memslot_prot(slot, gfn, NULL);
+ *   - virt/kvm/kvm_main.c|3434| <<__kvm_read_guest_atomic>> addr = gfn_to_hva_memslot_prot(slot, gfn, NULL);
+ */
 unsigned long gfn_to_hva_memslot_prot(struct kvm_memory_slot *slot,
 				      gfn_t gfn, bool *writable)
 {
@@ -3028,6 +3072,17 @@ kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool interruptible,
 	return pfn;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1489| <<user_mem_abort>> pfn = __gfn_to_pfn_memslot(memslot, gfn, false, false, NULL, write_fault, &writable, NULL);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|616| <<kvmppc_book3s_hv_page_fault>> pfn = __gfn_to_pfn_memslot(memslot, gfn, false, false, NULL, writing, &write_ok, NULL);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|855| <<kvmppc_book3s_instantiate_page>> pfn = __gfn_to_pfn_memslot(memslot, gfn, false, false, NULL, writing, upgrade_p, NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|4425| <<__kvm_faultin_pfn>> fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, false, &async, fault->write, &fault->map_writable, &fault->hva);
+ *   - arch/x86/kvm/mmu/mmu.c|4447| <<__kvm_faultin_pfn>> fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, true, NULL, fault->write, &fault->map_writable, &fault->hva);
+ *   - virt/kvm/kvm_main.c|3097| <<gfn_to_pfn_prot>> return __gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn, false, false, NULL, write_fault, writable, NULL);
+ *   - virt/kvm/kvm_main.c|3104| <<gfn_to_pfn_memslot>> return __gfn_to_pfn_memslot(slot, gfn, false, false, NULL, true, NULL, NULL);
+ *   - virt/kvm/kvm_main.c|3111| <<gfn_to_pfn_memslot_atomic>> return __gfn_to_pfn_memslot(slot, gfn, true, false, NULL, true, NULL, NULL);
+ */
 kvm_pfn_t __gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn,
 			       bool atomic, bool interruptible, bool *async,
 			       bool write_fault, bool *writable, hva_t *hva)
@@ -3632,6 +3687,25 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 }
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1582| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn); 
+ *   - arch/loongarch/kvm/mmu.c|923| <<kvm_map_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - arch/s390/kvm/gaccess.c|1024| <<access_guest_page_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/s390/kvm/gaccess.c|1246| <<cmpxchg_guest_abs_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|3497| <<fast_pf_fix_direct_spte>> mark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);
+ *   - arch/x86/kvm/mmu/spte.c|709| <<make_spte>> mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
+ *   - arch/x86/kvm/x86.c|3170| <<kvm_setup_guest_pvclock>> mark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/x86.c|3731| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/x86.c|5068| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/xen.c|460| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, gpc1->memslot, gpc1->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/xen.c|462| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, gpc2->memslot, gpc2->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/xen.c|574| <<kvm_xen_inject_pending_events>> mark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3469| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3607| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3707| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3716| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+ */
 void mark_page_dirty_in_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *memslot,
 		 	     gfn_t gfn)
@@ -5278,6 +5352,14 @@ static long kvm_vm_ioctl(struct file *filp,
 #endif /* CONFIG_HAVE_KVM_IRQ_ROUTING */
 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
 	case KVM_SET_MEMORY_ATTRIBUTES: {
+		/*
+		 * struct kvm_memory_attributes {
+		 *     __u64 address;
+		 *     __u64 size;
+		 *     __u64 attributes;
+		 *     __u64 flags;
+		 * };
+		 */
 		struct kvm_memory_attributes attrs;
 
 		r = -EFAULT;
@@ -5521,9 +5603,34 @@ static struct miscdevice kvm_dev = {
 __visible bool kvm_rebooting;
 EXPORT_SYMBOL_GPL(kvm_rebooting);
 
+/*
+ * 在以下使用percpu的hardware_enabled:
+ *   - virt/kvm/kvm_main.c|5568| <<__hardware_enable_nolock>> if (__this_cpu_read(hardware_enabled))
+ *   - virt/kvm/kvm_main.c|5577| <<__hardware_enable_nolock>> __this_cpu_write(hardware_enabled, true);
+ *   - virt/kvm/kvm_main.c|5609| <<hardware_disable_nolock>> if (!__this_cpu_read(hardware_enabled))
+ *   - virt/kvm/kvm_main.c|5614| <<hardware_disable_nolock>> __this_cpu_write(hardware_enabled, false);
+ */
 static DEFINE_PER_CPU(bool, hardware_enabled);
+/*
+ * 在以下使用kvm_usage_count:
+ *   - virt/kvm/kvm_main.c|5597| <<kvm_online_cpu>> if (kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|5620| <<kvm_offline_cpu>> if (kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|5628| <<hardware_disable_all_nolock>> BUG_ON(!kvm_usage_count);
+ *   - virt/kvm/kvm_main.c|5630| <<hardware_disable_all_nolock>> kvm_usage_count--;
+ *   - virt/kvm/kvm_main.c|5631| <<hardware_disable_all_nolock>> if (!kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|5675| <<hardware_enable_all>> kvm_usage_count++;
+ *   - virt/kvm/kvm_main.c|5676| <<hardware_enable_all>> if (kvm_usage_count == 1) {
+ *   - virt/kvm/kvm_main.c|5722| <<kvm_suspend>> if (kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|5732| <<kvm_resume>> if (kvm_usage_count)
+ */
 static int kvm_usage_count;
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5583| <<hardware_enable_nolock>> if (__hardware_enable_nolock())
+ *   - virt/kvm/kvm_main.c|5598| <<kvm_online_cpu>> ret = __hardware_enable_nolock();
+ *   - virt/kvm/kvm_main.c|5733| <<kvm_resume>> WARN_ON_ONCE(__hardware_enable_nolock());
+ */
 static int __hardware_enable_nolock(void)
 {
 	if (__this_cpu_read(hardware_enabled))
@@ -5539,12 +5646,20 @@ static int __hardware_enable_nolock(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5677| <<hardware_enable_all>> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ */
 static void hardware_enable_nolock(void *failed)
 {
 	if (__hardware_enable_nolock())
 		atomic_inc(failed);
 }
 
+/*
+ * 在以下使用kvm_online_cpu:
+ *   - virt/kvm/kvm_main.c|6453| <<kvm_init>> r = cpuhp_setup_state_nocalls(CPUHP_AP_KVM_ONLINE, "kvm/cpu:online", kvm_online_cpu, kvm_offline_cpu);
+ */
 static int kvm_online_cpu(unsigned int cpu)
 {
 	int ret = 0;
@@ -5602,6 +5717,10 @@ static void hardware_disable_all(void)
 	cpus_read_unlock();
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1267| <<kvm_create_vm>> r = hardware_enable_all();
+ */
 static int hardware_enable_all(void)
 {
 	atomic_t failed = ATOMIC_INIT(0);
@@ -5634,6 +5753,9 @@ static int hardware_enable_all(void)
 	r = 0;
 
 	kvm_usage_count++;
+	/*
+	 * 这里只执行一次
+	 */
 	if (kvm_usage_count == 1) {
 		on_each_cpu(hardware_enable_nolock, &failed, 1);
 
@@ -6388,6 +6510,11 @@ static struct perf_guest_info_callbacks kvm_guest_cbs = {
 	.handle_intel_pt_intr	= NULL,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2169| <<init_subsystems>> kvm_register_perf_callbacks(NULL);
+ *   - arch/x86/kvm/x86.c|9752| <<__kvm_x86_vendor_init>> kvm_register_perf_callbacks(ops->handle_intel_pt_intr);
+ */
 void kvm_register_perf_callbacks(unsigned int (*pt_intr_handler)(void))
 {
 	kvm_guest_cbs.handle_intel_pt_intr = pt_intr_handler;
-- 
2.34.1

