From ac87956d881efeb9c98355612ab4febe33c0cbf0 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Thu, 15 Feb 2024 22:23:14 -0800
Subject: [PATCH 1/1] linux-v6.7

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/kvm_emulate.h          |    6 +
 arch/arm64/include/asm/kvm_mmu.h              |   11 +
 arch/arm64/kernel/cpufeature.c                |    6 +
 arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h    |    8 +
 arch/arm64/kvm/hyp/nvhe/tlb.c                 |    8 +
 arch/arm64/kvm/hyp/vhe/switch.c               |    4 +
 arch/arm64/kvm/hyp/vhe/sysreg-sr.c            |    4 +
 arch/arm64/kvm/hyp/vhe/tlb.c                  |   43 +
 arch/arm64/kvm/mmu.c                          |   46 +
 arch/arm64/kvm/vmid.c                         |    4 +
 arch/x86/events/core.c                        |    4 +
 arch/x86/include/asm/kvm_host.h               |  424 ++++
 arch/x86/include/asm/vmx.h                    |   19 +
 arch/x86/include/uapi/asm/kvm_para.h          |    6 +
 arch/x86/kernel/apic/apic.c                   |   27 +
 arch/x86/kernel/cpu/intel.c                   |    9 +
 arch/x86/kernel/kvm.c                         |    8 +
 arch/x86/kernel/smpboot.c                     |   37 +
 arch/x86/kernel/traps.c                       |    5 +
 arch/x86/kvm/cpuid.c                          |   22 +
 arch/x86/kvm/cpuid.h                          |   19 +
 arch/x86/kvm/emulate.c                        |   61 +
 arch/x86/kvm/irq_comm.c                       |    4 +
 arch/x86/kvm/kvm_cache_regs.h                 |   12 +
 arch/x86/kvm/kvm_emulate.h                    |   10 +
 arch/x86/kvm/lapic.c                          |   71 +
 arch/x86/kvm/lapic.h                          |   22 +
 arch/x86/kvm/mmu.h                            |   49 +
 arch/x86/kvm/mmu/mmu.c                        | 2144 +++++++++++++++++
 arch/x86/kvm/mmu/mmu_internal.h               |  186 ++
 arch/x86/kvm/mmu/page_track.c                 |   21 +
 arch/x86/kvm/mmu/page_track.h                 |    9 +
 arch/x86/kvm/mmu/paging_tmpl.h                |  336 +++
 arch/x86/kvm/mmu/spte.c                       |   78 +
 arch/x86/kvm/mmu/spte.h                       |   93 +
 arch/x86/kvm/mmu/tdp_iter.c                   |  171 ++
 arch/x86/kvm/mmu/tdp_iter.h                   |  123 +
 arch/x86/kvm/mmu/tdp_mmu.c                    |  872 +++++++
 arch/x86/kvm/mmu/tdp_mmu.h                    |   25 +
 arch/x86/kvm/pmu.c                            |    7 +
 arch/x86/kvm/smm.c                            |  225 ++
 arch/x86/kvm/smm.h                            |   49 +
 arch/x86/kvm/svm/svm.c                        |   10 +
 arch/x86/kvm/vmx/capabilities.h               |   24 +
 arch/x86/kvm/vmx/hyperv.c                     |    9 +
 arch/x86/kvm/vmx/nested.c                     |  460 ++++
 arch/x86/kvm/vmx/nested.h                     |   54 +
 arch/x86/kvm/vmx/pmu_intel.c                  |   17 +
 arch/x86/kvm/vmx/vmcs.h                       |   64 +
 arch/x86/kvm/vmx/vmx.c                        |  801 ++++++
 arch/x86/kvm/vmx/vmx.h                        |  164 ++
 arch/x86/kvm/x86.c                            | 1005 ++++++++
 arch/x86/kvm/x86.h                            |   32 +
 include/linux/kvm_host.h                      |   37 +
 kernel/cpu.c                                  |  273 +++
 kernel/smp.c                                  |    6 +
 kernel/user-return-notifier.c                 |    8 +
 lib/nmi_backtrace.c                           |   21 +
 mm/hugetlb.c                                  |   62 +
 mm/mmu_notifier.c                             |   10 +
 tools/testing/selftests/kselftest_harness.h   |   11 +
 .../kvm/aarch64/vpmu_counter_access.c         |    5 +
 tools/testing/selftests/kvm/dirty_log_test.c  |  564 +++++
 .../selftests/kvm/include/kvm_util_base.h     |   54 +
 .../testing/selftests/kvm/include/memstress.h |   13 +
 tools/testing/selftests/kvm/lib/kvm_util.c    |   93 +
 tools/testing/selftests/kvm/lib/memstress.c   |  226 ++
 .../selftests/kvm/lib/x86_64/memstress.c      |   12 +
 .../selftests/kvm/lib/x86_64/processor.c      |   13 +
 tools/testing/selftests/kvm/lib/x86_64/vmx.c  |   16 +
 .../x86_64/dirty_log_page_splitting_test.c    |   78 +
 tools/testing/selftests/kvm/x86_64/smm_test.c |   63 +
 .../kvm/x86_64/vmx_set_nested_state_test.c    |   11 +
 virt/kvm/async_pf.c                           |  179 ++
 virt/kvm/irqchip.c                            |   10 +
 virt/kvm/kvm_main.c                           |  208 ++
 76 files changed, 9901 insertions(+)

diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h
index 78a550537..7613084df 100644
--- a/arch/arm64/include/asm/kvm_emulate.h
+++ b/arch/arm64/include/asm/kvm_emulate.h
@@ -317,6 +317,12 @@ static __always_inline unsigned long kvm_vcpu_get_hfar(const struct kvm_vcpu *vc
 	return vcpu->arch.fault.far_el2;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vgic-v2-cpuif-proxy.c|46| <<__vgic_v2_perform_cpuif_access>> fault_ipa = kvm_vcpu_get_fault_ipa(vcpu);
+ *   - arch/arm64/kvm/inject_fault.c|194| <<kvm_inject_size_fault>> addr = kvm_vcpu_get_fault_ipa(vcpu);
+ *   - arch/arm64/kvm/mmu.c|1637| <<kvm_handle_guest_abort>> fault_ipa = kvm_vcpu_get_fault_ipa(vcpu);
+ */
 static __always_inline phys_addr_t kvm_vcpu_get_fault_ipa(const struct kvm_vcpu *vcpu)
 {
 	return ((phys_addr_t)vcpu->arch.fault.hpfar_el2 & HPFAR_MASK) << 8;
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index 49e0d4b36..06338d06e 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -317,6 +317,17 @@ static __always_inline u64 kvm_get_vttbr(struct kvm_s2_mmu *mmu)
 	return kvm_phys_to_vttbr(baddr) | vmid_field | cnp;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1010| <<kvm_arch_vcpu_ioctl_run>> __load_stage2(vcpu->arch.hw_mmu,
+ *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|89| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|310| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|308| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|65| <<__tlb_switch_to_guest>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|167| <<kvm_vcpu_load_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|63| <<__tlb_switch_to_guest>> __load_stage2(mmu, mmu->arch);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|81| <<__tlb_switch_to_host>> __load_stage2(cxt->mmu, cxt->mmu->arch);
+ */
 /*
  * Must be called from hyp code running at EL2 with an updated VTTBR
  * and interrupts disabled.
diff --git a/arch/arm64/kernel/cpufeature.c b/arch/arm64/kernel/cpufeature.c
index 91d2d6714..6a44a8707 100644
--- a/arch/arm64/kernel/cpufeature.c
+++ b/arch/arm64/kernel/cpufeature.c
@@ -1399,6 +1399,12 @@ EXPORT_SYMBOL_GPL(read_sanitised_ftr_reg);
  * __read_sysreg_by_encoding() - Used by a STARTING cpu before cpuinfo is populated.
  * Read the system register on the current CPU
  */
+/*
+ * called by:
+ *   - arch/arm64/kernel/cpufeature.c|1487| <<read_scoped_sysreg>> return __read_sysreg_by_encoding(entry->sys_reg);
+ *   - arch/arm64/kernel/cpufeature.c|2095| <<has_address_auth_cpucap>> sec_val = cpuid_feature_extract_field(__read_sysreg_by_encoding(entry->sys_reg),
+ *   - arch/arm64/mm/mmu.c|714| <<arm64_early_this_cpu_has_bti>> pfr1 = __read_sysreg_by_encoding(SYS_ID_AA64PFR1_EL1);
+ */
 u64 __read_sysreg_by_encoding(u32 sys_id)
 {
 	struct arm64_ftr_reg *regp;
diff --git a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
index bb6b571ec..e31238951 100644
--- a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
+++ b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
@@ -97,6 +97,11 @@ static inline void __sysreg_restore_user_state(struct kvm_cpu_context *ctxt)
 	write_sysreg(ctxt_sys_reg(ctxt, TPIDRRO_EL0),	tpidrro_el0);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/sysreg-sr.c|31| <<__sysreg_restore_state_nvhe>> __sysreg_restore_el1_state(ctxt);
+ *   - arch/arm64/kvm/hyp/vhe/sysreg-sr.c|92| <<__vcpu_load_switch_sysregs>> __sysreg_restore_el1_state(guest_ctxt);
+ */
 static inline void __sysreg_restore_el1_state(struct kvm_cpu_context *ctxt)
 {
 	write_sysreg(ctxt_sys_reg(ctxt, MPIDR_EL1),	vmpidr_el2);
@@ -118,6 +123,9 @@ static inline void __sysreg_restore_el1_state(struct kvm_cpu_context *ctxt)
 	}
 
 	write_sysreg_el1(ctxt_sys_reg(ctxt, CPACR_EL1),	SYS_CPACR);
+	/*
+	 * 设置系统中页表的寄存器
+	 */
 	write_sysreg_el1(ctxt_sys_reg(ctxt, TTBR0_EL1),	SYS_TTBR0);
 	write_sysreg_el1(ctxt_sys_reg(ctxt, TTBR1_EL1),	SYS_TTBR1);
 	if (cpus_have_final_cap(ARM64_HAS_TCR2))
diff --git a/arch/arm64/kvm/hyp/nvhe/tlb.c b/arch/arm64/kvm/hyp/nvhe/tlb.c
index 1b265713d..af363e9fe 100644
--- a/arch/arm64/kvm/hyp/nvhe/tlb.c
+++ b/arch/arm64/kvm/hyp/nvhe/tlb.c
@@ -14,6 +14,14 @@ struct tlb_inv_context {
 	u64		tcr;
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|87| <<__kvm_tlb_flush_vmid_ipa>> __tlb_switch_to_guest(mmu, &cxt, false);
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|139| <<__kvm_tlb_flush_vmid_ipa_nsh>> __tlb_switch_to_guest(mmu, &cxt, true);
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|199| <<__kvm_tlb_flush_vmid_range>> __tlb_switch_to_guest(mmu, &cxt, false);
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|220| <<__kvm_tlb_flush_vmid>> __tlb_switch_to_guest(mmu, &cxt, false);
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|234| <<__kvm_flush_cpu_context>> __tlb_switch_to_guest(mmu, &cxt, false);
+ */
 static void __tlb_switch_to_guest(struct kvm_s2_mmu *mmu,
 				  struct tlb_inv_context *cxt,
 				  bool nsh)
diff --git a/arch/arm64/kvm/hyp/vhe/switch.c b/arch/arm64/kvm/hyp/vhe/switch.c
index 1581df6ae..121706578 100644
--- a/arch/arm64/kvm/hyp/vhe/switch.c
+++ b/arch/arm64/kvm/hyp/vhe/switch.c
@@ -160,6 +160,10 @@ static void __vcpu_put_deactivate_traps(struct kvm_vcpu *vcpu)
 	local_irq_restore(flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|454| <<kvm_arch_vcpu_load>> kvm_vcpu_load_vhe(vcpu);
+ */
 void kvm_vcpu_load_vhe(struct kvm_vcpu *vcpu)
 {
 	__vcpu_load_switch_sysregs(vcpu);
diff --git a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
index 8e1e0d503..3f5591533 100644
--- a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
+++ b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
@@ -62,6 +62,10 @@ NOKPROBE_SYMBOL(sysreg_restore_guest_state_vhe);
  * and loading system register state early avoids having to load them on
  * every entry to the VM.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|165| <<kvm_vcpu_load_vhe>> __vcpu_load_switch_sysregs(vcpu);
+ */
 void __vcpu_load_switch_sysregs(struct kvm_vcpu *vcpu)
 {
 	struct kvm_cpu_context *guest_ctxt = &vcpu->arch.ctxt;
diff --git a/arch/arm64/kvm/hyp/vhe/tlb.c b/arch/arm64/kvm/hyp/vhe/tlb.c
index b636b4111..e25a34087 100644
--- a/arch/arm64/kvm/hyp/vhe/tlb.c
+++ b/arch/arm64/kvm/hyp/vhe/tlb.c
@@ -17,6 +17,14 @@ struct tlb_inv_context {
 	u64			sctlr;
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|100| <<__kvm_tlb_flush_vmid_ipa>> __tlb_switch_to_guest(mmu, &cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|132| <<__kvm_tlb_flush_vmid_ipa_nsh>> __tlb_switch_to_guest(mmu, &cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|172| <<__kvm_tlb_flush_vmid_range>> __tlb_switch_to_guest(mmu, &cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|191| <<__kvm_tlb_flush_vmid>> __tlb_switch_to_guest(mmu, &cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|205| <<__kvm_flush_cpu_context>> __tlb_switch_to_guest(mmu, &cxt);
+ */
 static void __tlb_switch_to_guest(struct kvm_s2_mmu *mmu,
 				  struct tlb_inv_context *cxt)
 {
@@ -67,6 +75,14 @@ static void __tlb_switch_to_guest(struct kvm_s2_mmu *mmu,
 	isb();
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|129| <<__kvm_tlb_flush_vmid_ipa>> __tlb_switch_to_host(&cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|165| <<__kvm_tlb_flush_vmid_ipa_nsh>> __tlb_switch_to_host(&cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|198| <<__kvm_tlb_flush_vmid_range>> __tlb_switch_to_host(&cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|219| <<__kvm_tlb_flush_vmid>> __tlb_switch_to_host(&cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|238| <<__kvm_flush_cpu_context>> __tlb_switch_to_host(&cxt);
+ */
 static void __tlb_switch_to_host(struct tlb_inv_context *cxt)
 {
 	/*
@@ -89,6 +105,11 @@ static void __tlb_switch_to_host(struct tlb_inv_context *cxt)
 	local_irq_restore(cxt->flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|812| <<stage2_try_break_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu,
+ *   - arch/arm64/kvm/hyp/pgtable.c|862| <<stage2_unmap_put_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu,
+ */
 void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,
 			      phys_addr_t ipa, int level)
 {
@@ -121,6 +142,10 @@ void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,
 	__tlb_switch_to_host(&cxt);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|1318| <<kvm_pgtable_stage2_relax_perms>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa_nsh, pgt->mmu, addr, level);
+ */
 void __kvm_tlb_flush_vmid_ipa_nsh(struct kvm_s2_mmu *mmu,
 				  phys_addr_t ipa, int level)
 {
@@ -153,6 +178,11 @@ void __kvm_tlb_flush_vmid_ipa_nsh(struct kvm_s2_mmu *mmu,
 	__tlb_switch_to_host(&cxt);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|193| <<__kvm_tlb_flush_vmid_range>> void __kvm_tlb_flush_vmid_range(struct kvm_s2_mmu *mmu,
+ *   - arch/arm64/kvm/hyp/pgtable.c|686| <<kvm_tlb_flush_vmid_range>> kvm_call_hyp(__kvm_tlb_flush_vmid_range, mmu, addr, inval_pages);
+ */
 void __kvm_tlb_flush_vmid_range(struct kvm_s2_mmu *mmu,
 				phys_addr_t start, unsigned long pages)
 {
@@ -181,6 +211,11 @@ void __kvm_tlb_flush_vmid_range(struct kvm_s2_mmu *mmu,
 	__tlb_switch_to_host(&cxt);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|679| <<kvm_tlb_flush_vmid_range>> kvm_call_hyp(__kvm_tlb_flush_vmid, mmu);
+ *   - arch/arm64/kvm/mmu.c|171| <<kvm_arch_flush_remote_tlbs>> kvm_call_hyp(__kvm_tlb_flush_vmid, &kvm->arch.mmu);
+ */
 void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)
 {
 	struct tlb_inv_context cxt;
@@ -197,6 +232,10 @@ void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)
 	__tlb_switch_to_host(&cxt);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|445| <<kvm_arch_vcpu_load>> kvm_call_hyp(__kvm_flush_cpu_context, mmu);
+ */
 void __kvm_flush_cpu_context(struct kvm_s2_mmu *mmu)
 {
 	struct tlb_inv_context cxt;
@@ -212,6 +251,10 @@ void __kvm_flush_cpu_context(struct kvm_s2_mmu *mmu)
 	__tlb_switch_to_host(&cxt);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vmid.c|69| <<flush_context>> kvm_call_hyp(__kvm_flush_vm_context);
+ */
 void __kvm_flush_vm_context(void)
 {
 	dsb(ishst);
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index d87c8fcc4..d1706eab6 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -22,6 +22,23 @@
 
 #include "trace.h"
 
+/*
+ * 在以下使用hyp_pgtable:
+ *   - arch/arm64/kvm/mmu.c|381| <<free_hyp_pgds>> if (hyp_pgtable) {
+ *   - arch/arm64/kvm/mmu.c|382| <<free_hyp_pgds>> kvm_pgtable_hyp_destroy(hyp_pgtable);
+ *   - arch/arm64/kvm/mmu.c|383| <<free_hyp_pgds>> kfree(hyp_pgtable);
+ *   - arch/arm64/kvm/mmu.c|384| <<free_hyp_pgds>> hyp_pgtable = NULL;
+ *   - arch/arm64/kvm/mmu.c|402| <<kvm_host_owns_hyp_mappings>> if (!hyp_pgtable && is_protected_kvm_enabled())
+ *   - arch/arm64/kvm/mmu.c|405| <<kvm_host_owns_hyp_mappings>> WARN_ON(!hyp_pgtable);
+ *   - arch/arm64/kvm/mmu.c|419| <<__create_hyp_mappings>> err = kvm_pgtable_hyp_map(hyp_pgtable, start, size, phys, prot);
+ *   - arch/arm64/kvm/mmu.c|1830| <<kvm_mmu_get_httbr>> return __pa(hyp_pgtable->pgd);
+ *   - arch/arm64/kvm/mmu.c|1924| <<kvm_mmu_init>> hyp_pgtable = kzalloc(sizeof(*hyp_pgtable), GFP_KERNEL);
+ *   - arch/arm64/kvm/mmu.c|1925| <<kvm_mmu_init>> if (!hyp_pgtable) {
+ *   - arch/arm64/kvm/mmu.c|1931| <<kvm_mmu_init>> err = kvm_pgtable_hyp_init(hyp_pgtable, *hyp_va_bits, &kvm_hyp_mm_ops);
+ *   - arch/arm64/kvm/mmu.c|1943| <<kvm_mmu_init>> kvm_pgtable_hyp_destroy(hyp_pgtable);
+ *   - arch/arm64/kvm/mmu.c|1945| <<kvm_mmu_init>> kfree(hyp_pgtable);
+ *   - arch/arm64/kvm/mmu.c|1946| <<kvm_mmu_init>> hyp_pgtable = NULL;
+ */
 static struct kvm_pgtable *hyp_pgtable;
 static DEFINE_MUTEX(kvm_hyp_pgd_mutex);
 
@@ -87,6 +104,11 @@ static int stage2_apply_range(struct kvm_s2_mmu *mmu, phys_addr_t addr,
  * of blocks into PAGE_SIZE PTEs. It assumes the range is already
  * mapped at level 2, or at level 1 if allowed.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|109| <<need_split_memcache_topup_or_resched>> min = kvm_mmu_split_nr_page_tables(chunk_size);
+ *   - arch/arm64/kvm/mmu.c|125| <<kvm_mmu_split_huge_pages>> cache_capacity = kvm_mmu_split_nr_page_tables(chunk_size);
+ */
 static int kvm_mmu_split_nr_page_tables(u64 range)
 {
 	int n = 0;
@@ -232,6 +254,12 @@ static void stage2_free_unlinked_table(void *addr, u32 level)
 {
 	struct page *page = virt_to_page(addr);
 
+	/*
+	 * 在kvm一共有三处调用:
+	 *   - arch/arm64/kvm/mmu.c|235| <<stage2_free_unlinked_table>> set_page_private(page, (unsigned long )level);
+	 *   - arch/x86/kvm/mmu/mmu.c|2724| <<kvm_mmu_alloc_shadow_page>> set_page_private(virt_to_page(sp->spt), (unsigned long )sp);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|224| <<tdp_mmu_init_sp>> set_page_private(virt_to_page(sp->spt), (unsigned long )sp);
+	 */
 	set_page_private(page, (unsigned long)level);
 	call_rcu(&page->rcu_head, stage2_free_unlinked_table_rcu_cb);
 }
@@ -839,6 +867,11 @@ static int get_user_mapping_size(struct kvm *kvm, u64 addr)
 	return BIT(ARM64_HW_PGTABLE_LEVEL_SHIFT(level));
 }
 
+/*
+ * 在以下使用kvm_s2_mm_ops:
+ *   - arch/arm64/kvm/mmu.c|228| <<stage2_free_unlinked_table_rcu_cb>> kvm_pgtable_stage2_free_unlinked(&kvm_s2_mm_ops, pgtable, level);
+ *   - arch/arm64/kvm/mmu.c|913| <<kvm_init_stage2_mmu>> err = kvm_pgtable_stage2_init(pgt, mmu, &kvm_s2_mm_ops);
+ */
 static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
 	.zalloc_page		= stage2_memcache_zalloc_page,
 	.zalloc_pages_exact	= kvm_s2_zalloc_pages_exact,
@@ -1372,6 +1405,10 @@ static bool kvm_vma_mte_allowed(struct vm_area_struct *vma)
 	return vma->vm_flags & VM_MTE_ALLOWED;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1745| <<kvm_handle_guest_abort>> ret = user_mem_abort(vcpu, fault_ipa, memslot, hva, fault_status);
+ */
 static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 			  struct kvm_memory_slot *memslot, unsigned long hva,
 			  unsigned long fault_status)
@@ -1616,6 +1653,11 @@ static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
  * space. The distinction is based on the IPA causing the fault and whether this
  * memory region has been registered as standard RAM by user space.
  */
+/*
+ * 在以下使用kvm_handle_guest_abort():
+ *   - arch/arm64/kvm/handle_exit.c|272| <<global>> [ESR_ELx_EC_IABT_LOW] = kvm_handle_guest_abort,
+ *   - arch/arm64/kvm/handle_exit.c|273| <<global>> [ESR_ELx_EC_DABT_LOW] = kvm_handle_guest_abort,
+ */
 int kvm_handle_guest_abort(struct kvm_vcpu *vcpu)
 {
 	unsigned long fault_status;
@@ -1854,6 +1896,10 @@ static struct kvm_pgtable_mm_ops kvm_hyp_mm_ops = {
 	.virt_to_phys		= kvm_host_pa,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2304| <<init_hyp_mode>> err = kvm_mmu_init(&hyp_va_bits);
+ */
 int __init kvm_mmu_init(u32 *hyp_va_bits)
 {
 	int err;
diff --git a/arch/arm64/kvm/vmid.c b/arch/arm64/kvm/vmid.c
index 806223b70..e781cc1e6 100644
--- a/arch/arm64/kvm/vmid.c
+++ b/arch/arm64/kvm/vmid.c
@@ -42,6 +42,10 @@ static DEFINE_PER_CPU(u64, reserved_vmids);
 #define vmid_gen_match(vmid) \
 	(!(((vmid) ^ atomic64_read(&vmid_generation)) >> kvm_arm_vmid_bits))
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vmid.c|119| <<new_vmid>> flush_context();
+ */
 static void flush_context(void)
 {
 	int cpu;
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 40ad1425f..532d14bb1 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -697,6 +697,10 @@ void x86_pmu_disable_all(void)
 
 struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr, void *data)
 {
+	/*
+	 * - arch/x86/events/intel/core.c|4999| <<global>> .guest_get_msrs = core_guest_get_msrs,
+	 * - arch/x86/events/intel/core.c|5060| <<global>> .guest_get_msrs = intel_guest_get_msrs,
+	 */
 	return static_call(x86_pmu_guest_get_msrs)(nr, data);
 }
 EXPORT_SYMBOL_GPL(perf_guest_get_msrs);
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d70369823..84a60642c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -91,6 +91,16 @@
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #ifdef CONFIG_KVM_SMM
+/*
+ * 在以下使用KVM_REQ_SMI:
+ *   - arch/x86/kvm/smm.h|163| <<kvm_inject_smi>> kvm_make_request(KVM_REQ_SMI, vcpu);
+ *   - arch/x86/kvm/x86.c|5257| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> if (kvm_check_request(KVM_REQ_SMI, vcpu))
+ *   - arch/x86/kvm/x86.c|10740| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_SMI, vcpu))
+ *   - arch/x86/kvm/x86.c|12998| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_SMI, vcpu) ||
+ *   - arch/x86/kvm/x86.c|13047| <<kvm_arch_dy_runnable>> kvm_test_request(KVM_REQ_SMI, vcpu) ||
+ *
+ * 处理的函数是process_smi()
+ */
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
 #endif
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
@@ -113,7 +123,25 @@
 #define KVM_REQ_TLB_FLUSH_CURRENT	KVM_ARCH_REQ(26)
 #define KVM_REQ_TLB_FLUSH_GUEST \
 	KVM_ARCH_REQ_FLAGS(27, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_APF_READY:
+ *   - arch/x86/kvm/lapic.c|503| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+ *   - arch/x86/kvm/lapic.c|2573| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+ *   - arch/x86/kvm/x86.c|11001| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+ *   - arch/x86/kvm/x86.c|13617| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+ *
+ * 处理函数是kvm_check_async_pf_completion()
+ */
 #define KVM_REQ_APF_READY		KVM_ARCH_REQ(28)
+/*
+ * 在以下使用KVM_REQ_MSR_FILTER_CHANGED:
+ *   - arch/x86/kvm/x86.c|7246| <<kvm_vm_ioctl_set_msr_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_MSR_FILTER_CHANGED);
+ *   - arch/x86/kvm/x86.c|11533| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MSR_FILTER_CHANGED, vcpu))
+ *
+ * 处理函数static_call(kvm_x86_msr_filter_changed)(vcpu)
+ * -> vmx_msr_filter_changed()
+ * -> svm_msr_filter_changed()
+ */
 #define KVM_REQ_MSR_FILTER_CHANGED	KVM_ARCH_REQ(29)
 #define KVM_REQ_UPDATE_CPU_DIRTY_LOGGING \
 	KVM_ARCH_REQ_FLAGS(30, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
@@ -145,10 +173,45 @@
 /* KVM Hugepage definitions for x86 */
 #define KVM_MAX_HUGEPAGE_LEVEL	PG_LEVEL_1G
 #define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)
+/*
+ * level 5: (((5) - 1) * 9) = 36
+ * level 4: (((4) - 1) * 9) = 27
+ * level 3: (((3) - 1) * 9) = 18
+ * level 2: (((2) - 1) * 9) =  9
+ * level 1: (((1) - 1) * 9) =  0
+ */
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+/*
+ * level 5: 12 + 36 = 48
+ * level 4: 12 + 27 = 39
+ * level 3: 12 + 18 = 30
+ * level 2: 12 +  9 = 21
+ * level 1: 12 +  0 = 12
+ */
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
+/*
+ * level 5: 1 << 48  = 281474976710656 (262144GB = 256TB)
+ * level 4: 1 << 39  = 549755813888 (512G)
+ * level 3: 1 << 30  = 1073741824 (1G)
+ * level 2: 1 << 21  = 2097152 (2M)
+ * level 1: 1 << 12  = 4096 (4K)
+ */
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
+/*
+ * KVM_HPAGE_MASK(5): mask 48位
+ * KVM_HPAGE_MASK(4): mask 39位
+ * KVM_HPAGE_MASK(3): mask 30位
+ * KVM_HPAGE_MASK(2): mask 21位
+ * KVM_HPAGE_MASK(1): mask 12位
+ */
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
+/*
+ * level 5: 256T / 4k = 68719476736
+ * level 4: 512G / 4K = 134217728
+ * level 3: 1G   / 4K = 262144
+ * level 2: 2M   / 4K = 512
+ * level 1: 4K   / 4K = 1
+ */
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
 #define KVM_MEMSLOT_PAGES_TO_MMU_PAGES_RATIO 50
@@ -345,6 +408,16 @@ union kvm_mmu_page_role {
 		unsigned smap_andnot_wp:1;
 		unsigned ad_disabled:1;
 		unsigned guest_mode:1;
+		/*
+		 * 在以下直接使用role的passthrough:
+		 *   - arch/x86/kvm/mmu/mmu.c|806| <<kvm_mmu_page_get_gfn>> if (sp->role.passthrough)
+		 *   - arch/x86/kvm/mmu/mmu.c|856| <<kvm_mmu_page_set_translation>> sp->role.passthrough ? "passthrough" : "direct",
+		 *   - arch/x86/kvm/mmu/mmu.c|861| <<kvm_mmu_page_set_translation>> sp->role.passthrough ? "passthrough" : "direct",
+		 *   - arch/x86/kvm/mmu/mmu.c|2323| <<sp_has_gptes>> if (sp->role.passthrough)
+		 *   - arch/x86/kvm/mmu/mmu.c|2366| <<kvm_sync_page_check>> .passthrough = 0x1,
+		 *   - arch/x86/kvm/mmu/mmu.c|2847| <<kvm_mmu_child_role>> role.passthrough = 0;
+		 *   - arch/x86/kvm/mmu/mmu.c|6120| <<kvm_init_shadow_npt_mmu>> root_role.passthrough = 1;
+		 */
 		unsigned passthrough:1;
 		unsigned :5;
 
@@ -443,16 +516,55 @@ struct kvm_page_fault;
  * current mmu mode.
  */
 struct kvm_mmu {
+	/*
+	 * 在以下使用kvm_mmu->get_guest_pgd:
+	 *   - arch/x86/kvm/mmu/mmu.c|275| <<kvm_mmu_get_guest_pgd>> if (IS_ENABLED(CONFIG_RETPOLINE) && mmu->get_guest_pgd == get_guest_cr3)
+	 *   - arch/x86/kvm/mmu/mmu.c|278| <<kvm_mmu_get_guest_pgd>> return mmu->get_guest_pgd(vcpu);
+	 *   - arch/x86/kvm/mmu/mmu.c|5802| <<init_kvm_tdp_mmu>> context->get_guest_pgd = get_guest_cr3;
+	 *   - arch/x86/kvm/mmu/mmu.c|5951| <<init_kvm_softmmu>> context->get_guest_pgd = get_guest_cr3;
+	 *   - arch/x86/kvm/mmu/mmu.c|5965| <<init_kvm_nested_mmu>> g_context->get_guest_pgd = get_guest_cr3;
+	 *   - arch/x86/kvm/svm/nested.c|96| <<nested_svm_init_mmu_context>> vcpu->arch.mmu->get_guest_pgd = nested_svm_get_tdp_cr3;
+	 *   - arch/x86/kvm/vmx/nested.c|428| <<nested_ept_init_mmu_context>> vcpu->arch.mmu->get_guest_pgd = nested_ept_get_eptp;
+	 */
 	unsigned long (*get_guest_pgd)(struct kvm_vcpu *vcpu);
 	u64 (*get_pdptr)(struct kvm_vcpu *vcpu, int index);
+	/*
+	 * 在以下使用kvm_mmu->page_fault:
+	 *   - arch/x86/kvm/mmu/mmu.c|4532| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5113| <<paging64_init_context>> context->page_fault = paging64_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5120| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5224| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5354| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|292| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|318| <<kvm_mmu_do_page_fault>> r = vcpu->arch.mmu->page_fault(vcpu, &fault);
+	 */
 	int (*page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);
 	void (*inject_page_fault)(struct kvm_vcpu *vcpu,
 				  struct x86_exception *fault);
 	gpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			    gpa_t gva_or_gpa, u64 access,
 			    struct x86_exception *exception);
+	/*
+	 * 在以下使用kvm_mmu->sync_spte:
+	 *   - arch/x86/kvm/mmu/mmu.c|2351| <<kvm_sync_page_check>> if (WARN_ON_ONCE(sp->role.direct || !vcpu->arch.mmu->sync_spte ||
+	 *   - arch/x86/kvm/mmu/mmu.c|2363| <<kvm_sync_spte>> return vcpu->arch.mmu->sync_spte(vcpu, sp, i);
+	 *   - arch/x86/kvm/mmu/mmu.c|5219| <<nonpaging_init_context>> context->sync_spte = NULL;
+	 *   - arch/x86/kvm/mmu/mmu.c|5812| <<paging64_init_context>> context->sync_spte = paging64_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|5819| <<paging32_init_context>> context->sync_spte = paging32_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|5922| <<init_kvm_tdp_mmu>> context->sync_spte = NULL;
+	 *   - arch/x86/kvm/mmu/mmu.c|6053| <<kvm_init_shadow_ept_mmu>> context->sync_spte = ept_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|6094| <<init_kvm_nested_mmu>> g_context->sync_spte = NULL;
+	 *   - arch/x86/kvm/mmu/mmu.c|6574| <<kvm_mmu_invalidate_addr>> if (!mmu->sync_spte)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|979| <<FNAME>> static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int i)
+	 */
 	int (*sync_spte)(struct kvm_vcpu *vcpu,
 			 struct kvm_mmu_page *sp, int i);
+	/*
+	 * struct kvm_mmu_root_info {
+	 *     gpa_t pgd;
+	 *     hpa_t hpa;
+	 * };
+	 */
 	struct kvm_mmu_root_info root;
 	union kvm_cpu_role cpu_role;
 	union kvm_mmu_page_role root_role;
@@ -751,7 +863,32 @@ struct kvm_vcpu_arch {
 	int32_t apic_arb_prio;
 	int mp_state;
 	u64 ia32_misc_enable_msr;
+	/*
+	 * 在以下使用kvm_vcpu_arch->smbase:
+	 *   - arch/x86/kvm/smm.c|114| <<kvm_smm_changed>> trace_kvm_smm_transition(vcpu->vcpu_id, vcpu->arch.smbase, entering_smm);
+	 *   - arch/x86/kvm/smm.c|224| <<enter_smm_save_state_32>> smram->smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|253| <<enter_smm_save_state_64>> smram->smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|312| <<enter_smm>> if (kvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, &smram, sizeof(smram)))
+	 *   - arch/x86/kvm/smm.c|337| <<enter_smm>> cs.selector = (vcpu->arch.smbase >> 4) & 0xffff;
+	 *   - arch/x86/kvm/smm.c|338| <<enter_smm>> cs.base = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|579| <<emulator_leave_smm>> smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|581| <<emulator_leave_smm>> ret = kvm_vcpu_read_guest(vcpu, smbase + 0xfe00, smram.bytes, sizeof(smram));
+	 *
+	 * 在以下设置kvm_vcpu_arch->smbase:
+	 *   - arch/x86/kvm/smm.c|503| <<rsm_load_state_32>> vcpu->arch.smbase = smstate->smbase;
+	 *   - arch/x86/kvm/smm.c|536| <<rsm_load_state_64>> vcpu->arch.smbase = smstate->smbase;
+	 *   - arch/x86/kvm/x86.c|3879| <<kvm_set_msr_common(MSR_IA32_SMBASE)>> vcpu->arch.smbase = data;
+	 *   - arch/x86/kvm/x86.c|4279| <<kvm_get_msr_common(MSR_IA32_SMBASE)>> msr_info->data = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/x86.c|12211| <<kvm_vcpu_reset>> vcpu->arch.smbase = 0x30000;
+	 */
 	u64 smbase;
+	/*
+	 * 在以下使用kvm_vcpu_arch->smi_count:
+	 *   - arch/x86/kvm/x86.c|3910| <<kvm_set_msr_common(MSR_SMI_COUNT)>> vcpu->arch.smi_count = data;
+	 *   - arch/x86/kvm/x86.c|4282| <<kvm_get_msr_common(MSR_SMI_COUNT)>> msr_info->data = vcpu->arch.smi_count;
+	 *   - arch/x86/kvm/x86.c|10318| <<kvm_check_and_inject_events>> ++vcpu->arch.smi_count;
+	 *   - arch/x86/kvm/x86.c|12166| <<kvm_vcpu_reset>> vcpu->arch.smi_count = 0;
+	 */
 	u64 smi_count;
 	bool at_instruction_boundary;
 	bool tpr_access_reporting;
@@ -759,6 +896,14 @@ struct kvm_vcpu_arch {
 	u64 ia32_xss;
 	u64 microcode_version;
 	u64 arch_capabilities;
+	/*
+	 * 在以下使用kvm_vcpu_arch->perf_capabilities:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|183| <<vcpu_get_perf_capabilities>> return vcpu->arch.perf_capabilities; 
+	 *   - arch/x86/kvm/x86.c|3874| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> if (vcpu->arch.perf_capabilities == data)
+	 *   - arch/x86/kvm/x86.c|3877| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> vcpu->arch.perf_capabilities = data;
+	 *   - arch/x86/kvm/x86.c|4353| <<kvm_get_msr_common(MSR_IA32_PERF_CAPABILITIES)>> msr_info->data = vcpu->arch.perf_capabilities;
+	 *   - arch/x86/kvm/x86.c|12397| <<kvm_arch_vcpu_create>> vcpu->arch.perf_capabilities = kvm_caps.supported_perf_cap;
+	 */
 	u64 perf_capabilities;
 
 	/*
@@ -768,11 +913,60 @@ struct kvm_vcpu_arch {
 	 * the paging mode of the l1 guest. This context is always used to
 	 * handle faults.
 	 */
+	/*
+	 * 在以下设置kvm_vcpu_arch->mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|8010| <<kvm_mmu_create>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|86| <<nested_svm_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|104| <<nested_svm_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|466| <<nested_ept_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|477| <<nested_ept_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 */
 	struct kvm_mmu *mmu;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->root_mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|7027| <<init_kvm_tdp_mmu>> struct kvm_mmu *context = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|7087| <<kvm_init_shadow_mmu>> struct kvm_mmu *context = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|7208| <<init_kvm_softmmu>> struct kvm_mmu *context = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|7296| <<kvm_mmu_after_set_cpuid>> vcpu->arch.root_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|7299| <<kvm_mmu_after_set_cpuid>> vcpu->arch.root_mmu.cpu_role.ext.valid = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|7379| <<kvm_mmu_unload>> kvm_mmu_free_roots(kvm, &vcpu->arch.root_mmu, KVM_MMU_ROOTS_ALL);
+	 *   - arch/x86/kvm/mmu/mmu.c|7380| <<kvm_mmu_unload>> WARN_ON_ONCE(VALID_PAGE(vcpu->arch.root_mmu.root.hpa));
+	 *   - arch/x86/kvm/mmu/mmu.c|7432| <<kvm_mmu_free_obsolete_roots>> __kvm_mmu_free_obsolete_roots(vcpu->kvm, &vcpu->arch.root_mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|8010| <<kvm_mmu_create>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|8011| <<kvm_mmu_create>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|8017| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.root_mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|9014| <<kvm_mmu_destroy>> free_mmu_pages(&vcpu->arch.root_mmu);
+	 *   - arch/x86/kvm/svm/nested.c|104| <<nested_svm_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|105| <<nested_svm_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|473| <<nested_ept_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|474| <<nested_ept_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|5903| <<handle_invvpid>> kvm_mmu_free_guest_mode_roots(vcpu->kvm, &vcpu->arch.root_mmu);
+	 */
 	/* Non-nested MMU for L1 */
 	struct kvm_mmu root_mmu;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->guest_mmu:
+	 *   - arch/x86/kvm/mmu.h|207| <<kvm_mmu_refresh_passthrough_bits>> if (!tdp_enabled || mmu == &vcpu->arch.guest_mmu)
+	 *   - arch/x86/kvm/mmu/mmu.c|7116| <<kvm_init_shadow_npt_mmu>> struct kvm_mmu *context = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|7176| <<kvm_init_shadow_ept_mmu>> struct kvm_mmu *context = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|7297| <<kvm_mmu_after_set_cpuid>> vcpu->arch.guest_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|7300| <<kvm_mmu_after_set_cpuid>> vcpu->arch.guest_mmu.cpu_role.ext.valid = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|7381| <<kvm_mmu_unload>> kvm_mmu_free_roots(kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+	 *   - arch/x86/kvm/mmu/mmu.c|7382| <<kvm_mmu_unload>> WARN_ON_ONCE(VALID_PAGE(vcpu->arch.guest_mmu.root.hpa));
+	 *   - arch/x86/kvm/mmu/mmu.c|7433| <<kvm_mmu_free_obsolete_roots>> __kvm_mmu_free_obsolete_roots(vcpu->kvm, &vcpu->arch.guest_mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|7777| <<kvm_mmu_invalidate_addr>> if (mmu != &vcpu->arch.guest_mmu) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7952| <<__kvm_mmu_create>> if (!tdp_enabled && mmu == &vcpu->arch.guest_mmu)
+	 *   - arch/x86/kvm/mmu/mmu.c|8013| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|8023| <<kvm_mmu_create>> free_mmu_pages(&vcpu->arch.guest_mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|9015| <<kvm_mmu_destroy>> free_mmu_pages(&vcpu->arch.guest_mmu);
+	 *   - arch/x86/kvm/svm/nested.c|86| <<nested_svm_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|332| <<free_nested>> kvm_mmu_free_roots(vcpu->kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+	 *   - arch/x86/kvm/vmx/nested.c|462| <<nested_ept_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|5327| <<nested_release_vmcs12>> kvm_mmu_free_roots(vcpu->kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+	 *   - arch/x86/kvm/vmx/nested.c|5785| <<handle_invept>> mmu = &vcpu->arch.guest_mmu;
+	 */
 	/* L1 MMU when running nested */
 	struct kvm_mmu guest_mmu;
 
@@ -784,12 +978,30 @@ struct kvm_vcpu_arch {
 	 * walking and not for faulting since we never handle l2 page faults on
 	 * the host.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->nested_mmu:
+	 *   - arch/x86/kvm/mmu.h|359| <<kvm_translate_gpa>> if (mmu != &vcpu->arch.nested_mmu)
+	 *   - arch/x86/kvm/mmu/mmu.c|7224| <<init_kvm_nested_mmu>> struct kvm_mmu *g_context = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/mmu/mmu.c|7298| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.root_role.word = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|7301| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.cpu_role.ext.valid = 0;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|468| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/x86.h|205| <<mmu_is_nested>> return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;
+	 */
 	struct kvm_mmu nested_mmu;
 
 	/*
 	 * Pointer to the mmu context currently used for
 	 * gva_to_gpa translations.
 	 */
+	/*
+	 * 在以下设置kvm_vcpu_arch->walk_mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|8011| <<kvm_mmu_create>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|105| <<nested_svm_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|468| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|474| <<nested_ept_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 */
 	struct kvm_mmu *walk_mmu;
 
 	struct kvm_mmu_memory_cache mmu_pte_list_desc_cache;
@@ -866,6 +1078,29 @@ struct kvm_vcpu_arch {
 	struct x86_emulate_ctxt *emulate_ctxt;
 	bool emulate_regs_need_sync_to_vcpu;
 	bool emulate_regs_need_sync_from_vcpu;
+	/*
+	 * 在以下使用kvm_vcpu_arch->complete_userspace_io:
+	 *   - arch/x86/kvm/hyperv.c|357| <<syndbg_exit>> vcpu->arch.complete_userspace_io = kvm_hv_syndbg_complete_userspace;
+	 *   - arch/x86/kvm/hyperv.c|2635| <<kvm_hv_hypercall>> vcpu->arch.complete_userspace_io = kvm_hv_hypercall_complete_userspace;
+	 *   - arch/x86/kvm/x86.c|2144| <<kvm_msr_user_space>> vcpu->arch.complete_userspace_io = completion;
+	 *   - arch/x86/kvm/x86.c|9516| <<x86_emulate_instruction>> vcpu->arch.complete_userspace_io = complete_emulated_pio;
+	 *   - arch/x86/kvm/x86.c|9525| <<x86_emulate_instruction>> vcpu->arch.complete_userspace_io = complete_emulated_mmio;
+	 *   - arch/x86/kvm/x86.c|9526| <<x86_emulate_instruction>> } else if (vcpu->arch.complete_userspace_io) {
+	 *   - arch/x86/kvm/x86.c|9639| <<kvm_fast_pio_out>> vcpu->arch.complete_userspace_io = complete_fast_pio_out_port_0x7e;
+	 *   - arch/x86/kvm/x86.c|9644| <<kvm_fast_pio_out>> vcpu->arch.complete_userspace_io = complete_fast_pio_out;
+	 *   - arch/x86/kvm/x86.c|9686| <<kvm_fast_pio_in>> vcpu->arch.complete_userspace_io = complete_fast_pio_in;
+	 *   - arch/x86/kvm/x86.c|10440| <<kvm_emulate_hypercall>> vcpu->arch.complete_userspace_io = complete_hypercall_exit;
+	 *   - arch/x86/kvm/x86.c|11742| <<complete_emulated_mmio>> vcpu->arch.complete_userspace_io = complete_emulated_mmio;
+	 *   - arch/x86/kvm/x86.c|11846| <<kvm_arch_vcpu_ioctl_run>> if (unlikely(vcpu->arch.complete_userspace_io)) {
+	 *   - arch/x86/kvm/x86.c|11847| <<kvm_arch_vcpu_ioctl_run>> int (*cui)(struct kvm_vcpu *) = vcpu->arch.complete_userspace_io;
+	 *   - arch/x86/kvm/x86.c|11848| <<kvm_arch_vcpu_ioctl_run>> vcpu->arch.complete_userspace_io = NULL;
+	 *   - arch/x86/kvm/x86.c|14356| <<complete_sev_es_emulated_mmio>> vcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;
+	 *   - arch/x86/kvm/x86.c|14394| <<kvm_sev_es_mmio_write>> vcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;
+	 *   - arch/x86/kvm/x86.c|14432| <<kvm_sev_es_mmio_read>> vcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;
+	 *   - arch/x86/kvm/x86.c|14476| <<kvm_sev_es_outs>> vcpu->arch.complete_userspace_io = complete_sev_es_emulated_outs;
+	 *   - arch/x86/kvm/x86.c|14511| <<kvm_sev_es_ins>> vcpu->arch.complete_userspace_io = complete_sev_es_emulated_ins;
+	 *   - arch/x86/kvm/xen.c|1570| <<kvm_xen_hypercall>> vcpu->arch.complete_userspace_io = kvm_xen_hypercall_complete_userspace;
+	 */
 	int (*complete_userspace_io)(struct kvm_vcpu *vcpu);
 
 	gpa_t time;
@@ -900,10 +1135,57 @@ struct kvm_vcpu_arch {
 	u64 l1_tsc_scaling_ratio;
 	u64 tsc_scaling_ratio; /* current scaling ratio */
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->nmi_queued:
+	 *   - arch/x86/kvm/x86.c|820| <<kvm_inject_nmi>> atomic_inc(&vcpu->arch.nmi_queued);
+	 *   - arch/x86/kvm/x86.c|5412| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
+	 *   - arch/x86/kvm/x86.c|10439| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+	 *   - arch/x86/kvm/x86.c|12193| <<kvm_vcpu_reset>> atomic_set(&vcpu->arch.nmi_queued, 0);
+	 */
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
 	/* Number of NMIs pending injection, not including hardware vNMIs. */
+	/*
+	 * 在以下设置kvm_vcpu_arch->nmi_pending:
+	 *   - arch/x86/kvm/svm/nested.c|671| <<nested_vmcb02_prepare_control>> svm->vcpu.arch.nmi_pending++;
+	 *   - arch/x86/kvm/svm/nested.c|1093| <<nested_svm_vmexit>> vcpu->arch.nmi_pending--;
+	 *   - arch/x86/kvm/vmx/nested.c|4175| <<vmx_check_nested_events>> vcpu->arch.nmi_pending = 0;
+	 *   - arch/x86/kvm/x86.c|5418| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.nmi_pending = 0;
+	 *   - arch/x86/kvm/x86.c|10375| <<kvm_check_and_inject_events>> --vcpu->arch.nmi_pending;
+	 *   - arch/x86/kvm/x86.c|10471| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+	 *   - arch/x86/kvm/x86.c|10472| <<process_nmi>> vcpu->arch.nmi_pending = min(vcpu->arch.nmi_pending, limit);
+	 *   - arch/x86/kvm/x86.c|10479| <<process_nmi>> vcpu->arch.nmi_pending--;
+	 *   - arch/x86/kvm/x86.c|12240| <<kvm_vcpu_reset>> vcpu->arch.nmi_pending = 0;
+	 */
 	unsigned int nmi_pending;
+	/*
+	 * 在以下使用kvm_vcpu_arch->nmi_injected:
+	 *   - arch/x86/kvm/svm/nested.c|1156| <<nested_svm_vmexit>> svm->vcpu.arch.nmi_injected = false;
+	 *   - arch/x86/kvm/svm/svm.c|2519| <<task_switch_interception>> vcpu->arch.nmi_injected = false;
+	 *   - arch/x86/kvm/svm/svm.c|4024| <<svm_complete_interrupts>> vcpu->arch.nmi_injected = false;
+	 *   - arch/x86/kvm/svm/svm.c|4041| <<svm_complete_interrupts>> vcpu->arch.nmi_injected = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4803| <<nested_vmx_vmexit>> vcpu->arch.nmi_injected = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|5699| <<handle_task_switch>> vcpu->arch.nmi_injected = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7068| <<__vmx_complete_interrupts>> vcpu->arch.nmi_injected = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7082| <<__vmx_complete_interrupts>> vcpu->arch.nmi_injected = true;
+	 *   - arch/x86/kvm/x86.c|5416| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.nmi_injected = events->nmi.injected;
+	 *   - arch/x86/kvm/x86.c|10376| <<kvm_check_and_inject_events>> vcpu->arch.nmi_injected = true;
+	 *   - arch/x86/kvm/x86.c|12241| <<kvm_vcpu_reset>> vcpu->arch.nmi_injected = false;
+	 */
 	bool nmi_injected;    /* Trying to inject an NMI this entry */
+	/*
+	 * 在以下使用kvm_vcpu_arch->smi_pending:
+	 *   - arch/x86/kvm/smm.c|161| <<process_smi>> vcpu->arch.smi_pending = true;
+	 *   - arch/x86/kvm/svm/nested.c|1477| <<svm_check_nested_events>> if (vcpu->arch.smi_pending && !svm_smi_blocked(vcpu)) {
+	 *   - arch/x86/kvm/svm/svm.c|2421| <<svm_set_gif>> if (svm->vcpu.arch.smi_pending ||
+	 *   - arch/x86/kvm/vmx/nested.c|4156| <<vmx_check_nested_events>> if (vcpu->arch.smi_pending && !is_smm(vcpu)) {
+	 *   - arch/x86/kvm/x86.c|5325| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> events->smi.pending = vcpu->arch.smi_pending;
+	 *   - arch/x86/kvm/x86.c|5423| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.smi_pending = events->smi.pending;
+	 *   - arch/x86/kvm/x86.c|10316| <<kvm_check_and_inject_events>> if (vcpu->arch.smi_pending) {
+	 *   - arch/x86/kvm/x86.c|10321| <<kvm_check_and_inject_events>> vcpu->arch.smi_pending = false;
+	 *   - arch/x86/kvm/x86.c|11561| <<kvm_arch_vcpu_ioctl_set_mpstate>> if ((!kvm_apic_init_sipi_allowed(vcpu) || vcpu->arch.smi_pending) &&
+	 *   - arch/x86/kvm/x86.c|12176| <<kvm_vcpu_reset>> vcpu->arch.smi_pending = 0;
+	 *   - arch/x86/kvm/x86.c|12984| <<kvm_vcpu_has_events>> (vcpu->arch.smi_pending &&
+	 */
 	bool smi_pending;    /* SMI queued after currently running handler */
 	u8 handling_intr_from_guest;
 
@@ -955,9 +1237,27 @@ struct kvm_vcpu_arch {
 		u64 msr_int_val; /* MSR_KVM_ASYNC_PF_INT */
 		u16 vec;
 		u32 id;
+		/*
+		 * 在以下使用apf.send_user_only:
+		 *   - arch/x86/kvm/x86.c|3558| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.send_user_only = !(data & KVM_ASYNC_PF_SEND_ALWAYS);
+		 *   - arch/x86/kvm/x86.c|13587| <<kvm_can_deliver_async_pf>> if (vcpu->arch.apf.send_user_only && static_call(kvm_x86_get_cpl)(vcpu) == 0)
+		 */
 		bool send_user_only;
 		u32 host_apf_flags;
+		/*
+		 * 在以下使用apf.delivery_as_pf_vmexit:
+		 *   - arch/x86/kvm/x86.c|3559| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.delivery_as_pf_vmexit = data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;
+		 *   - arch/x86/kvm/x86.c|13596| <<kvm_can_deliver_async_pf>> return vcpu->arch.apf.delivery_as_pf_vmexit;
+		 * 
+		 * 似乎是nested用的
+		 */
 		bool delivery_as_pf_vmexit;
+		/*
+		 * 在以下使用apf.pageready_pending:
+		 *   - arch/x86/kvm/x86.c|3998| <<kvm_set_msr_common(MSR_KVM_ASYNC_PF_ACK)>> vcpu->arch.apf.pageready_pending = false;
+		 *   - arch/x86/kvm/x86.c|13602| <<kvm_arch_async_page_present>> vcpu->arch.apf.pageready_pending = true;
+		 *   - arch/x86/kvm/x86.c|13618| <<kvm_arch_async_page_present_queued>> if (!vcpu->arch.apf.pageready_pending)
+		 */
 		bool pageready_pending;
 	} apf;
 
@@ -974,6 +1274,15 @@ struct kvm_vcpu_arch {
 
 	u64 msr_kvm_poll_control;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->exit_qualification:
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|595| <<FNAME(walk_addr_generic)>> vcpu->arch.exit_qualification &= (EPT_VIOLATION_GVA_IS_VALID |
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|598| <<FNAME(walk_addr_generic)>> vcpu->arch.exit_qualification |= EPT_VIOLATION_ACC_WRITE;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|600| <<FNAME(walk_addr_generic)>> vcpu->arch.exit_qualification |= EPT_VIOLATION_ACC_READ;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|602| <<FNAME(walk_addr_generic)>> vcpu->arch.exit_qualification |= EPT_VIOLATION_ACC_INSTR;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|608| <<FNAME(walk_addr_generic)>> vcpu->arch.exit_qualification |= (pte_access & VMX_EPT_RWX_MASK) <<
+	 *   - arch/x86/kvm/vmx/vmx.c|5877| <<handle_ept_violation>> vcpu->arch.exit_qualification = exit_qualification;
+	 */
 	/* set at EPT violation at this point */
 	unsigned long exit_qualification;
 
@@ -1033,6 +1342,9 @@ struct kvm_lpage_info {
 struct kvm_arch_memory_slot {
 	struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+	/*
+	 * 只在page_track.c使用
+	 */
 	unsigned short *gfn_write_track;
 };
 
@@ -1258,10 +1570,30 @@ struct kvm_arch {
 	unsigned long n_used_mmu_pages;
 	unsigned long n_requested_mmu_pages;
 	unsigned long n_max_mmu_pages;
+	/*
+	 * 在以下使用kvm_arch->indirect_shadow_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1084| <<account_shadowed>> kvm->arch.indirect_shadow_pages++;
+	 *   - arch/x86/kvm/mmu/mmu.c|1161| <<unaccount_shadowed>> kvm->arch.indirect_shadow_pages--;
+	 *   - arch/x86/kvm/mmu/mmu.c|7419| <<kvm_mmu_track_write>> if (!READ_ONCE(vcpu->kvm->arch.indirect_shadow_pages))
+	 *   - arch/x86/kvm/x86.c|9045| <<reexecute_instruction>> indirect_shadow_pages = vcpu->kvm->arch.indirect_shadow_pages;
+	 */
 	unsigned int indirect_shadow_pages;
 	u8 mmu_valid_gen;
+	/*
+	 * 在以下使用kvm_arch->mmu_page_hash[KVM_NUM_MMU_PAGES]:
+	 *   - arch/x86/kvm/mmu/mmu.c|2328| <<for_each_gfn_valid_sp_with_gptes>> &(_kvm)->arch.mmu_page_hash[kvm_page_table_hashfn(_gfn)]) \
+	 *   - arch/x86/kvm/mmu/mmu.c|2753| <<__kvm_mmu_get_shadow_page>> sp_list = &kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)];
+	 */
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	struct list_head active_mmu_pages;
+	/*
+	 * 在以下使用kvm_arch->zapped_obsolete_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|6263| <<kvm_zap_obsolete_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &kvm->arch.zapped_obsolete_pages, &nr_zapped);
+	 *   - arch/x86/kvm/mmu/mmu.c|6279| <<kvm_zap_obsolete_pages>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6344| <<kvm_has_zapped_obsolete_pages>> return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));
+	 *   - arch/x86/kvm/mmu/mmu.c|6350| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.zapped_obsolete_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6936| <<mmu_shrink_scan>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+	 */
 	struct list_head zapped_obsolete_pages;
 	/*
 	 * A list of kvm_mmu_page structs that, if zapped, could possibly be
@@ -1274,6 +1606,13 @@ struct kvm_arch {
 	 * guest attempts to execute from the region then KVM obviously can't
 	 * create an NX huge page (without hanging the guest).
 	 */
+	/*
+	 * 在以下使用kvm_arch->possible_nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|869| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link, &kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6351| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7213| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|7223| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages, struct kvm_mmu_page, possible_nx_huge_page_link);
+	 */
 	struct list_head possible_nx_huge_pages;
 #ifdef CONFIG_KVM_EXTERNAL_WRITE_TRACKING
 	struct kvm_page_track_notifier_head track_notifier_head;
@@ -1381,8 +1720,24 @@ struct kvm_arch {
 	 */
 	bool exit_on_emulation_error;
 
+	/*
+	 * 在以下使用kvm_arch->user_space_msr_mask:
+	 *   - arch/x86/kvm/x86.c|2119| <<kvm_msr_user_space>> if (!(vcpu->kvm->arch.user_space_msr_mask & msr_reason))
+	 *   - arch/x86/kvm/x86.c|6773| <<kvm_vm_ioctl_enable_cap>> kvm->arch.user_space_msr_mask = cap->args[0];
+	 */
 	/* Deflect RDMSR and WRMSR to user space when they trigger a #GP */
 	u32 user_space_msr_mask;
+	/*
+	 * 在以下使用kvm_arch->msr_filter:
+	 *   - arch/x86/kvm/x86.c|1871| <<kvm_msr_allowed>> msr_filter = srcu_dereference(kvm->arch.msr_filter, &kvm->srcu);
+	 *   - arch/x86/kvm/x86.c|1872| <<kvm_msr_allowed>> if (!msr_filter) {
+	 *   - arch/x86/kvm/x86.c|1877| <<kvm_msr_allowed>> allowed = msr_filter->default_allow;
+	 *   - arch/x86/kvm/x86.c|1878| <<kvm_msr_allowed>> ranges = msr_filter->ranges;
+	 *   - arch/x86/kvm/x86.c|1880| <<kvm_msr_allowed>> for (i = 0; i < msr_filter->count; i++) {
+	 *   - arch/x86/kvm/x86.c|6916| <<kvm_alloc_msr_filter>> struct kvm_x86_msr_filter *msr_filter;
+	 *   - arch/x86/kvm/x86.c|6918| <<kvm_alloc_msr_filter>> msr_filter = kzalloc(sizeof(*msr_filter), GFP_KERNEL_ACCOUNT);
+	 *   - arch/x86/kvm/x86.c|6919| <<kvm_alloc_msr_filter>> if (!msr_filter)
+	 */
 	struct kvm_x86_msr_filter __rcu *msr_filter;
 
 	u32 hypercall_exit_enabled;
@@ -1391,9 +1746,25 @@ struct kvm_arch {
 	bool sgx_provisioning_allowed;
 
 	struct kvm_x86_pmu_event_filter __rcu *pmu_event_filter;
+	/*
+	 * 在以下使用kvm_arch->nx_huge_page_recovery_thread:
+	 *   - arch/x86/kvm/mmu/mmu.c|7447| <<set_nx_huge_pages>> wake_up_process(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7595| <<set_nx_huge_pages_recovery_param>> wake_up_process(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7743| <<kvm_mmu_post_init_vm>> err = kvm_vm_create_worker_thread(kvm, kvm_nx_huge_page_recovery_worker, 0,
+	 *                                                                         "kvm-nx-lpage-recovery", &kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7745| <<kvm_mmu_post_init_vm>> kthread_unpark(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7752| <<kvm_mmu_pre_destroy_vm>> if (kvm->arch.nx_huge_page_recovery_thread)
+	 *   - arch/x86/kvm/mmu/mmu.c|7753| <<kvm_mmu_pre_destroy_vm>> kthread_stop(kvm->arch.nx_huge_page_recovery_thread);
+	 */
 	struct task_struct *nx_huge_page_recovery_thread;
 
 #ifdef CONFIG_X86_64
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_pages:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|56| <<kvm_mmu_uninit_tdp_mmu>> WARN_ON(atomic64_read(&kvm->arch.tdp_mmu_pages));
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|299| <<tdp_account_mmu_page>> atomic64_inc(&kvm->arch.tdp_mmu_pages);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|305| <<tdp_unaccount_mmu_page>> atomic64_dec(&kvm->arch.tdp_mmu_pages);
+	 */
 	/* The number of TDP MMU pages across all roots. */
 	atomic64_t tdp_mmu_pages;
 
@@ -1415,6 +1786,17 @@ struct kvm_arch {
 	 * count to zero should removed the root from the list and clean
 	 * it up, freeing the root after an RCU grace period.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_roots:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|17| <<kvm_mmu_init_tdp_mmu>> INIT_LIST_HEAD(&kvm->arch.tdp_mmu_roots);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|57| <<kvm_mmu_uninit_tdp_mmu>> WARN_ON(!list_empty(&kvm->arch.tdp_mmu_roots));
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|140| <<tdp_mmu_next_root>> next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|144| <<tdp_mmu_next_root>> next_root = list_first_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|152| <<tdp_mmu_next_root>> next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|200| <<for_each_tdp_mmu_root>> list_for_each_entry(_root, &_kvm->arch.tdp_mmu_roots, link) \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|285| <<kvm_tdp_mmu_get_vcpu_root_hpa>> list_add_rcu(&root->link, &kvm->arch.tdp_mmu_roots);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|989| <<kvm_tdp_mmu_invalidate_all_roots>> list_for_each_entry(root, &kvm->arch.tdp_mmu_roots, link) {
+	 */
 	struct list_head tdp_mmu_roots;
 
 	/*
@@ -1428,6 +1810,18 @@ struct kvm_arch {
 	 * It is acceptable, but not necessary, to acquire this lock when
 	 * the thread holds the MMU lock in write mode.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_pages_lock:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|22| <<kvm_mmu_init_tdp_mmu>> spin_lock_init(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|147| <<kvm_tdp_mmu_put_root>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|149| <<kvm_tdp_mmu_put_root>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|358| <<kvm_tdp_mmu_get_vcpu_root_hpa>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|360| <<kvm_tdp_mmu_get_vcpu_root_hpa>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|400| <<tdp_mmu_unlink_sp>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|408| <<tdp_mmu_unlink_sp>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1283| <<kvm_tdp_mmu_map>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1286| <<kvm_tdp_mmu_map>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 */
 	spinlock_t tdp_mmu_pages_lock;
 #endif /* CONFIG_X86_64 */
 
@@ -1436,6 +1830,11 @@ struct kvm_arch {
 	 * is used as one input when determining whether certain memslot
 	 * related allocations are necessary.
 	 */
+	/*
+	 * 在以下使用kvm_arch->shadow_root_allocated:
+	 *   - arch/x86/kvm/mmu.h|297| <<kvm_shadow_root_allocated>> return smp_load_acquire(&kvm->arch.shadow_root_allocated);
+	 *   - arch/x86/kvm/mmu/mmu.c|5363| <<mmu_first_shadow_root_alloc>> smp_store_release(&kvm->arch.shadow_root_allocated, true);
+	 */
 	bool shadow_root_allocated;
 
 #if IS_ENABLED(CONFIG_HYPERV)
@@ -1477,11 +1876,22 @@ struct kvm_arch {
 struct kvm_vm_stat {
 	struct kvm_vm_stat_generic generic;
 	u64 mmu_shadow_zapped;
+	/*
+	 * 在以下使用kvm_vm_stat->mmu_pte_write:
+	 *   - arch/x86/kvm/x86.c|247| <<global>> STATS_DESC_COUNTER(VM, mmu_pte_write),
+	 *   - arch/x86/kvm/mmu/mmu.c|7426| <<kvm_mmu_track_write>> ++vcpu->kvm->stat.mmu_pte_write;
+	 */
 	u64 mmu_pte_write;
 	u64 mmu_pde_zapped;
 	u64 mmu_flooded;
 	u64 mmu_recycled;
 	u64 mmu_cache_miss;
+	/*
+	 * 在以下使用kvm_vm_stat->mmu_unsync:
+	 *   - arch/x86/kvm/x86.c|252| <<global>> STATS_DESC_ICOUNTER(VM, mmu_unsync),
+	 *   - arch/x86/kvm/mmu/mmu.c|2736| <<kvm_unlink_unsync_page>> --kvm->stat.mmu_unsync;
+	 *   - arch/x86/kvm/mmu/mmu.c|4061| <<kvm_unsync_page>> ++kvm->stat.mmu_unsync;
+	 */
 	u64 mmu_unsync;
 	union {
 		struct {
@@ -1825,8 +2235,16 @@ static inline struct kvm *kvm_arch_alloc_vm(void)
 void kvm_arch_free_vm(struct kvm *kvm);
 
 #define __KVM_HAVE_ARCH_FLUSH_REMOTE_TLBS
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|361| <<kvm_flush_remote_tlbs>> if (!kvm_arch_flush_remote_tlbs(kvm)
+ */
 static inline int kvm_arch_flush_remote_tlbs(struct kvm *kvm)
 {
+	/*
+	 * arch/x86/kvm/svm/svm_onhyperv.h|48| <<svm_hv_hardware_setup>> svm_x86_ops.flush_remote_tlbs = hv_flush_remote_tlbs;
+	 * arch/x86/kvm/vmx/vmx.c|8618| <<hardware_setup>> vmx_x86_ops.flush_remote_tlbs = hv_flush_remote_tlbs;
+	 */
 	if (kvm_x86_ops.flush_remote_tlbs &&
 	    !static_call(kvm_x86_flush_remote_tlbs)(kvm))
 		return 0;
@@ -2158,6 +2576,12 @@ int kvm_add_user_return_msr(u32 msr);
 int kvm_find_user_return_msr(u32 msr);
 int kvm_set_user_return_msr(unsigned index, u64 val, u64 mask);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|817| <<kvm_set_cpu_caps>> !kvm_is_supported_user_return_msr(MSR_TSC_AUX))) {
+ *   - arch/x86/kvm/x86.c|2090| <<__kvm_set_msr>> if (!kvm_is_supported_user_return_msr(MSR_TSC_AUX))
+ *   - arch/x86/kvm/x86.c|2165| <<__kvm_get_msr>> if (!kvm_is_supported_user_return_msr(MSR_TSC_AUX))
+ */
 static inline bool kvm_is_supported_user_return_msr(u32 msr)
 {
 	return kvm_find_user_return_msr(msr) >= 0;
diff --git a/arch/x86/include/asm/vmx.h b/arch/x86/include/asm/vmx.h
index 0e73616b8..7ee3d9d71 100644
--- a/arch/x86/include/asm/vmx.h
+++ b/arch/x86/include/asm/vmx.h
@@ -133,6 +133,11 @@
 #define VMX_VMFUNC_EPTP_SWITCHING               VMFUNC_CONTROL_BIT(EPTP_SWITCHING)
 #define VMFUNC_EPTP_ENTRIES  512
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|1355| <<vmx_restore_vmx_basic>> if (vmx_basic_vmcs_revision_id(vmx_basic) !=
+ *   - arch/x86/kvm/vmx/nested.c|1356| <<vmx_restore_vmx_basic>> vmx_basic_vmcs_revision_id(data))
+ */
 static inline u32 vmx_basic_vmcs_revision_id(u64 vmx_basic)
 {
 	return vmx_basic & GENMASK_ULL(30, 0);
@@ -383,6 +388,20 @@ enum vmcs_field {
 #define INTR_TYPE_SOFT_EXCEPTION	(6 << 8) /* software exception */
 #define INTR_TYPE_OTHER_EVENT           (7 << 8) /* other event */
 
+/*
+ * 在以下使用GUEST_INTR_STATE_NMI:
+ *   - arch/x86/kvm/vmx/nested.c|2574| <<prepare_vmcs02_early>> !(vmcs12->guest_interruptibility_info & GUEST_INTR_STATE_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|5324| <<vmx_get_nmi_mask>> masked = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI;
+ *   - arch/x86/kvm/vmx/vmx.c|5342| <<vmx_set_nmi_mask>> vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|5345| <<vmx_set_nmi_mask>> vmcs_clear_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|5359| <<vmx_nmi_blocked>> (vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & (GUEST_INTR_STATE_MOV_SS | GUEST_INTR_STATE_STI | GUEST_INTR_STATE_NMI));
+ *   - arch/x86/kvm/vmx/vmx.c|6089| <<handle_ept_violation>> vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|6329| <<handle_pml_full>> vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|6404| <<handle_notify>> vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|7466| <<vmx_recover_nmi_blocking>> vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|7470| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->nmi_known_unmasked = !(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI);
+ */
+
 /* GUEST_INTERRUPTIBILITY_INFO flags. */
 #define GUEST_INTR_STATE_STI		0x00000001
 #define GUEST_INTR_STATE_MOV_SS		0x00000002
diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index 6e64b27b2..e6a676f59 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -56,6 +56,12 @@
 #define MSR_KVM_PV_EOI_EN      0x4b564d04
 #define MSR_KVM_POLL_CONTROL	0x4b564d05
 #define MSR_KVM_ASYNC_PF_INT	0x4b564d06
+/*
+ * 在以下使用MSR_KVM_ASYNC_PF_ACK:
+ *   - arch/x86/kernel/kvm.c|302| <<DEFINE_IDTENTRY_SYSVEC(sysvec_kvm_asyncpf_interrupt)>> wrmsrl(MSR_KVM_ASYNC_PF_ACK, 1);
+ *   - arch/x86/kvm/x86.c|4002| <<kvm_set_msr_common>> case MSR_KVM_ASYNC_PF_ACK:
+ *   - arch/x86/kvm/x86.c|4385| <<kvm_get_msr_common>> case MSR_KVM_ASYNC_PF_ACK:
+ */
 #define MSR_KVM_ASYNC_PF_ACK	0x4b564d07
 #define MSR_KVM_MIGRATION_CONTROL	0x4b564d08
 
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index 41093cf20..05783a1f9 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -2362,6 +2362,23 @@ early_initcall(smp_init_primary_thread_mask);
 static inline void cpu_mark_primary_thread(unsigned int cpu, unsigned int apicid) { }
 #endif
 
+/*
+ * CPU: 0 PID: 0 Comm: swapper Not tainted 6.7.0 #17
+ * [0] allocate_logical_cpuid
+ * [0] ? generic_processor_info
+ * [0] ? acpi_register_lapic
+ * [0] ? acpi_parse_lapic
+ * [0] ? acpi_parse_entries_array
+ * [0] ? acpi_table_parse_entries_array
+ * [0] ? acpi_table_parse_madt
+ * [0] ? __pfx_acpi_parse_lapic
+ * [0] ? acpi_boot_init
+ * [0] ? setup_arch
+ * [0] ? start_kernel
+ * [0] ? x86_64_start_reservations
+ * [0] ? x86_64_start_kernel
+ * [0]? secondary_startup_64_no_verify
+ */
 /*
  * Should use this API to allocate logical CPU IDs to keep nr_logical_cpuids
  * and cpuid_to_apicid[] synchronized.
@@ -2405,6 +2422,16 @@ static void cpu_update_apic(int cpu, u32 apicid)
 		cpu_mark_primary_thread(cpu, apicid);
 }
 
+/*
+ * [0] cpu_set_boot_apic() apicid
+ * [0] ? apic_read_boot_cpu_id
+ * [0] ? early_acpi_boot_init
+ * [0] ? setup_arch
+ * [0] ? start_kernel
+ * [0] ? x86_64_start_reservations
+ * [0] ? x86_64_start_kernel
+ * [0] ? secondary_startup_64_no_verify
+ */
 static __init void cpu_set_boot_apic(void)
 {
 	cpuid_to_apicid[0] = boot_cpu_physical_apicid;
diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c
index a927a8fc9..eb05aacef 100644
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@ -1155,6 +1155,11 @@ static int splitlock_cpu_offline(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/intel.c|1197| <<handle_guest_split_lock>> split_lock_warn(ip);
+ *   - arch/x86/kernel/cpu/intel.c|1240| <<handle_user_split_lock>> split_lock_warn(regs->ip);
+ */
 static void split_lock_warn(unsigned long ip)
 {
 	struct delayed_work *work;
@@ -1191,6 +1196,10 @@ static void split_lock_warn(unsigned long ip)
 	put_cpu();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5429| <<handle_exception_nmi>> if (handle_guest_split_lock(kvm_rip_read(vcpu)))
+ */
 bool handle_guest_split_lock(unsigned long ip)
 {
 	if (sld_state == sld_warn) {
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index 0ddb3bd0f..9f5d59163 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -107,6 +107,10 @@ static struct kvm_task_sleep_node *_find_apf_task(struct kvm_task_sleep_head *b,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|148| <<kvm_async_pf_task_wait_schedule>> if (!kvm_async_pf_queue_task(token, &n))
+ */
 static bool kvm_async_pf_queue_task(u32 token, struct kvm_task_sleep_node *n)
 {
 	u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
@@ -187,6 +191,10 @@ static void apf_task_wake_all(void)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|300| <<DEFINE_IDTENTRY_SYSVEC(sysvec_kvm_asyncpf_interrupt)>> kvm_async_pf_task_wake(token);
+ */
 void kvm_async_pf_task_wake(u32 token)
 {
 	u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 2cc2aa120..02ea9d655 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -289,6 +289,16 @@ static void notrace start_secondary(void *unused)
 	 */
 	load_ucode_ap();
 
+	/*
+	 * 注释
+	 * cpuhp_ap_sync_alive - Synchronize AP with the control CPU once it is alive
+	 *
+	 * Updates the AP synchronization state to SYNC_STATE_ALIVE and waits
+	 * for the BP to release it.
+	 * called by:
+	 *   - arch/x86/kernel/smpboot.c|297| <<start_secondary>> cpuhp_ap_sync_alive();
+	 *   - arch/x86/xen/smp_pv.c|64| <<cpu_bringup>> cpuhp_ap_sync_alive();
+	 */
 	/*
 	 * Synchronization point with the hotplug core. Sets this CPUs
 	 * synchronization state to ALIVE and spin-waits for the control CPU to
@@ -819,6 +829,10 @@ static void __init smp_quirk_init_udelay(void)
 /*
  * Wake up AP by INIT, INIT, STARTUP sequence.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|865| <<wakeup_secondary_cpu_via_init>> send_init_sequence(phys_apicid);
+ */
 static void send_init_sequence(u32 phys_apicid)
 {
 	int maxlvt = lapic_get_maxlvt();
@@ -845,6 +859,10 @@ static void send_init_sequence(u32 phys_apicid)
 /*
  * Wake up AP by INIT, INIT, STARTUP sequence.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1063| <<do_boot_cpu>> ret = wakeup_secondary_cpu_via_init(apicid, start_ip);
+ */
 static int wakeup_secondary_cpu_via_init(u32 phys_apicid, unsigned long start_eip)
 {
 	unsigned long send_status = 0, accept_status = 0;
@@ -992,6 +1010,10 @@ int common_cpu_up(unsigned int cpu, struct task_struct *idle)
  * Returns zero if startup was successfully sent, else error code from
  * ->wakeup_secondary_cpu.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1107| <<native_kick_ap>> err = do_boot_cpu(apicid, cpu, tidle);
+ */
 static int do_boot_cpu(u32 apicid, int cpu, struct task_struct *idle)
 {
 	unsigned long start_ip = real_mode_header->trampoline_start;
@@ -1058,6 +1080,9 @@ static int do_boot_cpu(u32 apicid, int cpu, struct task_struct *idle)
 	return ret;
 }
 
+/*
+ * struct smp_ops smp_ops.kick_ap_alive = native_kick_ap()
+ */
 int native_kick_ap(unsigned int cpu, struct task_struct *tidle)
 {
 	u32 apicid = apic->cpu_present_to_apicid(cpu);
@@ -1093,8 +1118,16 @@ int native_kick_ap(unsigned int cpu, struct task_struct *tidle)
 	return err;
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|838| <<cpuhp_kick_ap_alive>> return arch_cpuhp_kick_ap_alive(cpu, idle_thread_get(cpu));
+ */
 int arch_cpuhp_kick_ap_alive(unsigned int cpu, struct task_struct *tidle)
 {
+	/*
+	 * native_kick_ap()
+	 * xen_pv_kick_ap()
+	 */
 	return smp_ops.kick_ap_alive(cpu, tidle);
 }
 
@@ -1187,6 +1220,10 @@ void __init smp_prepare_cpus_common(void)
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * called by:
+ *   - kernel/cpu.c|1994| <<cpuhp_bringup_cpus_parallel>> __cpuhp_parallel_bringup = arch_cpuhp_init_parallel_bringup();
+ */
 /* Establish whether parallel bringup can be supported. */
 bool __init arch_cpuhp_init_parallel_bringup(void)
 {
diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index c876f1d36..fc3059629 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -770,6 +770,11 @@ DEFINE_IDTENTRY_RAW(exc_int3)
  * to switch to the normal thread stack if the interrupted code was in
  * user mode. The actual stack switch is done in entry_64.S
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11409| <<kvm_arch_vcpu_ioctl_run>> r = sync_regs(vcpu);
+ *   - arch/x86/kvm/x86.c|12063| <<sync_regs>> static int sync_regs(struct kvm_vcpu *vcpu)
+ */
 asmlinkage __visible noinstr struct pt_regs *sync_regs(struct pt_regs *eregs)
 {
 	struct pt_regs *regs = (struct pt_regs *)this_cpu_read(pcpu_hot.top_of_stack) - 1;
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index dda6fc4cf..1d9638269 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -29,6 +29,21 @@
 #include "pmu.h"
 #include "xen.h"
 
+/*
+ * 在以下使用kvm_cpu_caps[NR_KVM_CPU_CAPS]:
+ *   - arch/x86/kvm/cpuid.c|554| <<__kvm_cpu_cap_mask>> kvm_cpu_caps[leaf] &= *__cpuid_entry_get_reg(&entry, cpuid.reg);
+ *   - arch/x86/kvm/cpuid.c|563| <<kvm_cpu_cap_init_kvm_defined>> kvm_cpu_caps[leaf] = mask;
+ *   - arch/x86/kvm/cpuid.c|573| <<kvm_cpu_cap_mask>> kvm_cpu_caps[leaf] &= mask;
+ *   - arch/x86/kvm/cpuid.c|589| <<kvm_set_cpu_caps>> memset(kvm_cpu_caps, 0, sizeof(kvm_cpu_caps));
+ *   - arch/x86/kvm/cpuid.c|591| <<kvm_set_cpu_caps>> BUILD_BUG_ON(sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)) >
+ *   - arch/x86/kvm/cpuid.c|594| <<kvm_set_cpu_caps>> memcpy(&kvm_cpu_caps, &boot_cpu_data.x86_capability,
+ *   - arch/x86/kvm/cpuid.c|595| <<kvm_set_cpu_caps>> sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)));
+ *   - arch/x86/kvm/cpuid.h|90| <<cpuid_entry_override>> BUILD_BUG_ON(leaf >= ARRAY_SIZE(kvm_cpu_caps));
+ *   - arch/x86/kvm/cpuid.h|91| <<cpuid_entry_override>> *reg = kvm_cpu_caps[leaf];
+ *   - arch/x86/kvm/cpuid.h|216| <<kvm_cpu_cap_clear>> kvm_cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
+ *   - arch/x86/kvm/cpuid.h|224| <<kvm_cpu_cap_set>> kvm_cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
+ *   - arch/x86/kvm/cpuid.h|232| <<kvm_cpu_cap_get>> return kvm_cpu_caps[x86_leaf] & __feature_bit(x86_feature);
+ */
 /*
  * Unlike "struct cpuinfo_x86.x86_capability", kvm_cpu_caps doesn't need to be
  * aligned to sizeof(unsigned long) because it's not accessed via bitops.
@@ -1425,6 +1440,13 @@ int kvm_dev_ioctl_get_cpuid(struct kvm_cpuid2 *cpuid,
 struct kvm_cpuid_entry2 *kvm_find_cpuid_entry_index(struct kvm_vcpu *vcpu,
 						    u32 function, u32 index)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> int cpuid_nent;
+	 *    -> struct kvm_cpuid_entry2 *cpuid_entries;
+	 *    -> struct kvm_hypervisor_cpuid kvm_cpuid;
+	 */
 	return cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,
 				 function, index);
 }
diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h
index 0b90532b6..478c828ea 100644
--- a/arch/x86/kvm/cpuid.h
+++ b/arch/x86/kvm/cpuid.h
@@ -42,11 +42,30 @@ static inline int cpuid_maxphyaddr(struct kvm_vcpu *vcpu)
 	return vcpu->arch.maxphyaddr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.h|52| <<kvm_vcpu_is_illegal_gpa>> return !kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/cpuid.h|58| <<kvm_vcpu_is_legal_aligned_gpa>> return IS_ALIGNED(gpa, alignment) && kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/svm/nested.c|246| <<nested_svm_check_bitmap_pa>> return kvm_vcpu_is_legal_gpa(vcpu, addr) &&
+ *   - arch/x86/kvm/svm/nested.c|247| <<nested_svm_check_bitmap_pa>> kvm_vcpu_is_legal_gpa(vcpu, addr + size - 1);
+ *   - arch/x86/kvm/vmx/nested.c|780| <<nested_vmx_check_msr_switch>> !kvm_vcpu_is_legal_gpa(vcpu, (addr + count * sizeof(struct vmx_msr_entry) - 1)))
+ */
 static inline bool kvm_vcpu_is_legal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !(gpa & vcpu->arch.reserved_gpa_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|314| <<__nested_vmcb_check_save>> CC(kvm_vcpu_is_illegal_gpa(vcpu, save->cr3)))
+ *   - arch/x86/kvm/svm/nested.c|523| <<nested_svm_load_cr3>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3)))
+ *   - arch/x86/kvm/vmx/nested.c|1088| <<nested_vmx_load_cr3>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3))) {
+ *   - arch/x86/kvm/vmx/nested.c|2720| <<nested_vmx_check_eptp>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, new_eptp) || ((new_eptp >> 7) & 0x1f)))
+ *   - arch/x86/kvm/vmx/nested.c|2915| <<nested_vmx_check_host_state>> CC(kvm_vcpu_is_illegal_gpa(vcpu, vmcs12->host_cr3)))
+ *   - arch/x86/kvm/vmx/vmx.c|5884| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && kvm_vcpu_is_illegal_gpa(vcpu, gpa)))
+ *   - arch/x86/kvm/x86.c|1299| <<kvm_set_cr3>> if (kvm_vcpu_is_illegal_gpa(vcpu, cr3))
+ *   - arch/x86/kvm/x86.c|11798| <<kvm_is_valid_sregs>> if (kvm_vcpu_is_illegal_gpa(vcpu, sregs->cr3))
+ */
 static inline bool kvm_vcpu_is_illegal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !kvm_vcpu_is_legal_gpa(vcpu, gpa);
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index 2673cd5c4..e9d67a35f 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -2319,6 +2319,12 @@ static int em_rsm(struct x86_emulate_ctxt *ctxt)
 	if (!ctxt->ops->is_smm(ctxt))
 		return emulate_ud(ctxt);
 
+	/*
+	 * struct x86_emulate_ctxt *ctxt:
+	 * -> const struct x86_emulate_ops *ops;
+	 *
+	 * emulator_leave_smm()
+	 */
 	if (ctxt->ops->leave_smm(ctxt))
 		ctxt->ops->triple_fault(ctxt);
 
@@ -4493,6 +4499,20 @@ static const struct opcode twobyte_table[256] = {
 	N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N
 };
 
+/*
+ * #define NotImpl     (1 << 30)   // instruction is not implemented
+ *
+ * #define D(_y) { .flags = (_y) }
+ *
+ * #define N    D(NotImpl)
+ *
+ * #define I(_f, _e) { .flags = (_f), .u.execute = (_e) }
+ *
+ * struct instr_dual {
+ *     struct opcode mod012;
+ *     struct opcode mod3;
+ * };
+ */
 static const struct instr_dual instr_dual_0f_38_f0 = {
 	I(DstReg | SrcMem | Mov, em_movbe), N
 };
@@ -4501,6 +4521,19 @@ static const struct instr_dual instr_dual_0f_38_f1 = {
 	I(DstMem | SrcReg | Mov, em_movbe), N
 };
 
+/*
+ * struct gprefix {
+ *     struct opcode pfx_no;
+ *     struct opcode pfx_66;
+ *     struct opcode pfx_f2;
+ *     struct opcode pfx_f3;
+ * };
+ *
+ * #define ID(_f, _i) { .flags = ((_f) | InstrDual | ModRM), .u.idual = (_i) }
+ *
+ * 在以下使用three_byte_0f_38_f0:
+ *   - arch/x86/kvm/emulate.c|4522| <<global>> GP(EmulateOnUD | ModRM, &three_byte_0f_38_f0),
+ */
 static const struct gprefix three_byte_0f_38_f0 = {
 	ID(0, &instr_dual_0f_38_f0), N, N, N
 };
@@ -4509,6 +4542,26 @@ static const struct gprefix three_byte_0f_38_f1 = {
 	ID(0, &instr_dual_0f_38_f1), N, N, N
 };
 
+/*
+ * 193 struct opcode {
+ * 194         u64 flags;
+ * 195         u8 intercept;
+ * 196         u8 pad[7];
+ * 197         union {
+ * 198                 int (*execute)(struct x86_emulate_ctxt *ctxt);
+ * 199                 const struct opcode *group;
+ * 200                 const struct group_dual *gdual;
+ * 201                 const struct gprefix *gprefix;
+ * 202                 const struct escape *esc;
+ * 203                 const struct instr_dual *idual;
+ * 204                 const struct mode_dual *mdual;
+ * 205                 void (*fastop)(struct fastop *fake);
+ * 206         } u;
+ * 207         int (*check_perm)(struct x86_emulate_ctxt *ctxt);
+ * 208 };
+ *
+ * #define GP(_f, _g) { .flags = ((_f) | Prefix), .u.gprefix = (_g) }
+ */
 /*
  * Insns below are selected by the prefix which indexed by the third opcode
  * byte.
@@ -4757,6 +4810,10 @@ static int decode_operand(struct x86_emulate_ctxt *ctxt, struct operand *op,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9344| <<x86_decode_emulated_instruction>> r = x86_decode_insn(ctxt, insn, insn_len, emulation_type);
+ */
 int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len, int emulation_type)
 {
 	int rc = X86EMUL_CONTINUE;
@@ -5134,6 +5191,10 @@ void init_decode_cache(struct x86_emulate_ctxt *ctxt)
 	ctxt->mem_read.end = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9116| <<x86_emulate_instruction>> r = x86_emulate_insn(ctxt);
+ */
 int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)
 {
 	const struct x86_emulate_ops *ops = ctxt->ops;
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 16d076a1b..d4012f2ca 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -392,6 +392,10 @@ int kvm_setup_default_irq_routing(struct kvm *kvm)
 
 static const struct kvm_irq_routing_entry empty_routing[] = {};
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6677| <<kvm_vm_ioctl_enable_cap>> r = kvm_setup_empty_irq_routing(kvm);
+ */
 int kvm_setup_empty_irq_routing(struct kvm *kvm)
 {
 	return kvm_set_irq_routing(kvm, empty_routing, 0, 0);
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index 75eae9c49..923bf1709 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -205,12 +205,24 @@ static inline u64 kvm_read_edx_eax(struct kvm_vcpu *vcpu)
 		| ((u64)(kvm_rdx_read(vcpu) & -1u) << 32);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|777| <<nested_vmcb02_prepare_control>> enter_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3707| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+ */
 static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags |= HF_GUEST_MASK;
 	vcpu->stat.guest_mode = 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|999| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|1250| <<svm_leave_nested>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3774| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5070| <<nested_vmx_vmexit>> leave_guest_mode(vcpu);
+ */
 static inline void leave_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags &= ~HF_GUEST_MASK;
diff --git a/arch/x86/kvm/kvm_emulate.h b/arch/x86/kvm/kvm_emulate.h
index be7aeb9b8..ad6f5c23b 100644
--- a/arch/x86/kvm/kvm_emulate.h
+++ b/arch/x86/kvm/kvm_emulate.h
@@ -305,6 +305,16 @@ struct x86_emulate_ctxt {
 	/* Emulated execution mode, represented by an X86EMUL_MODE value. */
 	enum x86emul_mode mode;
 
+	/*
+	 * 在以下使用x86_emulate_ctxt->interruptibility:
+	 *   - arch/x86/kvm/emulate.c|1964| <<em_pop_sreg>> ctxt->interruptibility = KVM_X86_SHADOW_INT_MOV_SS;
+	 *   - arch/x86/kvm/emulate.c|3409| <<em_mov_sreg_rm>> ctxt->interruptibility = KVM_X86_SHADOW_INT_MOV_SS;
+	 *   - arch/x86/kvm/emulate.c|3622| <<em_sti>> ctxt->interruptibility = KVM_X86_SHADOW_INT_STI;
+	 *   - arch/x86/kvm/smm.c|679| <<rsm_load_state_32>> ctxt->interruptibility = (u8)smstate->int_shadow;
+	 *   - arch/x86/kvm/smm.c|736| <<rsm_load_state_64>> ctxt->interruptibility = (u8)smstate->int_shadow;
+	 *   - arch/x86/kvm/x86.c|8866| <<init_emulate_ctxt>> ctxt->interruptibility = 0;
+	 *   - arch/x86/kvm/x86.c|9513| <<x86_emulate_instruction>> toggle_interruptibility(vcpu, ctxt->interruptibility);
+	 */
 	/* interruptibility state, as a result of execution of STI or MOV SS */
 	int interruptibility;
 
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 245b20973..15df06a96 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -498,6 +498,15 @@ static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 		atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 	}
 
+	/*
+	 * 在以下使用KVM_REQ_APF_READY:
+	 *   - arch/x86/kvm/lapic.c|503| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+	 *   - arch/x86/kvm/lapic.c|2573| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+	 *   - arch/x86/kvm/x86.c|11001| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+	 *   - arch/x86/kvm/x86.c|13617| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+	 *
+	 * 处理函数是kvm_check_async_pf_completion()
+	 */
 	/* Check if there are APF page ready requests pending */
 	if (enabled)
 		kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
@@ -817,6 +826,17 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|841| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|2163| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq_comm.c|77| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq_comm.c|99| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|842| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1221| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|1234| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/x86.c|13381| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
@@ -1202,6 +1222,12 @@ static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|55| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+ *   - arch/x86/kvm/irq_comm.c|176| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+ *   - arch/x86/kvm/xen.c|509| <<kvm_xen_inject_vcpu_vector>> WARN_ON_ONCE(!kvm_irq_delivery_to_apic_fast(v->kvm, NULL, &irq, &r, NULL));
+ */
 bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)
 {
@@ -1285,6 +1311,11 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
  * Add a pending IRQ into lapic.
  * Return 1 if successfully added and 0 if discarded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|825| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector, irq->level, irq->trig_mode, dest_map);
+ *   - arch/x86/kvm/lapic.c|2771| <<kvm_apic_local_deliver>> r = __apic_accept_irq(apic, mode, vector, 1, trig_mode, NULL);
+ */
 static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map)
@@ -1322,6 +1353,10 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 						       apic->regs + APIC_TMR);
 		}
 
+		/*
+		 * vmx_deliver_interrupt()
+		 * svm_deliver_interrupt()
+		 */
 		static_call(kvm_x86_deliver_interrupt)(apic, delivery_mode,
 						       trig_mode, vector);
 		break;
@@ -2463,6 +2498,11 @@ void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_write_nodecode);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12577| <<kvm_arch_vcpu_create>> kvm_free_lapic(vcpu);
+ *   - arch/x86/kvm/x86.c|12620| <<kvm_arch_vcpu_destroy>> kvm_free_lapic(vcpu);
+ */
 void kvm_free_lapic(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2543,6 +2583,15 @@ void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 		if (value & MSR_IA32_APICBASE_ENABLE) {
 			kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
 			static_branch_slow_dec_deferred(&apic_hw_disabled);
+			/*
+			 * 在以下使用KVM_REQ_APF_READY:
+			 *   - arch/x86/kvm/lapic.c|503| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+			 *   - arch/x86/kvm/lapic.c|2573| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+			 *   - arch/x86/kvm/x86.c|11001| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+			 *   - arch/x86/kvm/x86.c|13617| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+			 *
+			 * 处理函数是kvm_check_async_pf_completion()
+			 */
 			/* Check if there are APF page ready requests pending */
 			kvm_make_request(KVM_REQ_APF_READY, vcpu);
 		} else {
@@ -2757,6 +2806,13 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1881| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2784| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|531| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ *   - arch/x86/kvm/x86.c|5198| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -2804,6 +2860,10 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 		return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12486| <<kvm_arch_vcpu_create>> r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ */
 int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 {
 	struct kvm_lapic *apic;
@@ -3204,6 +3264,10 @@ int kvm_x2apic_msr_write(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 	return kvm_lapic_msr_write(apic, reg, data);
 }
 
+/*
+ * called by (APIC_BASE_MSR ... APIC_BASE_MSR + 0xff:):
+ *   - arch/x86/kvm/x86.c|4469| <<kvm_get_msr_common>> return kvm_x2apic_msr_read(vcpu, msr_info->index, &msr_info->data);
+ */
 int kvm_x2apic_msr_read(struct kvm_vcpu *vcpu, u32 msr, u64 *data)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -3257,6 +3321,13 @@ int kvm_lapic_set_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10969| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11244| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+ *   - arch/x86/kvm/x86.c|11445| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+ *   - arch/x86/kvm/x86.c|11693| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+ */
 int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 0a0ea4b5d..4e27fb53b 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -78,6 +78,24 @@ struct kvm_lapic {
 	void *regs;
 	gpa_t vapic_addr;
 	struct gfn_to_hva_cache vapic_cache;
+	/*
+	 * 在以下使用kvm_lapic->pending_events:
+	 *   - arch/x86/kvm/lapic.c|1379| <<__apic_accept_irq>> apic->pending_events = (1UL << KVM_APIC_INIT);
+	 *   - arch/x86/kvm/lapic.c|1390| <<__apic_accept_irq>> set_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3320| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3324| <<kvm_apic_accept_events>> if (test_and_clear_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/lapic.c|3331| <<kvm_apic_accept_events>> if (test_and_clear_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/lapic.h|233| <<kvm_apic_has_pending_init_or_sipi>> return lapic_in_kernel(vcpu) && vcpu->arch.apic->pending_events;
+	 *   - arch/x86/kvm/lapic.h|250| <<kvm_lapic_latched_init>> return lapic_in_kernel(vcpu) && test_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/svm/nested.c|1454| <<svm_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4074| <<vmx_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4078| <<vmx_check_nested_events>> clear_bit(KVM_APIC_INIT, &apic->pending_events);
+	 *   - arch/x86/kvm/vmx/nested.c|4088| <<vmx_check_nested_events>> test_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4092| <<vmx_check_nested_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|5474| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> set_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|5476| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> clear_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|11693| <<kvm_arch_vcpu_ioctl_set_mpstate>> set_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);
+	 */
 	unsigned long pending_events;
 	unsigned int sipi_vector;
 	int nr_lvt_entries;
@@ -235,6 +253,10 @@ static inline bool kvm_apic_has_pending_init_or_sipi(struct kvm_vcpu *vcpu)
 
 static inline bool kvm_apic_init_sipi_allowed(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * vmx_apic_init_signal_blocked()
+	 * svm_apic_init_signal_blocked()
+	 */
 	return !is_smm(vcpu) &&
 	       !static_call(kvm_x86_apic_init_signal_blocked)(vcpu);
 }
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index bb8c86eef..75ff5adb8 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -46,6 +46,9 @@ static __always_inline u64 rsvd_bits(int s, int e)
 {
 	BUILD_BUG_ON(__builtin_constant_p(e) && __builtin_constant_p(s) && e < s);
 
+	/*
+	 * 63 = 111111b ---> 6个1
+	 */
 	if (__builtin_constant_p(e))
 		BUILD_BUG_ON(e > 63);
 	else
@@ -124,6 +127,11 @@ void kvm_mmu_sync_prev_roots(struct kvm_vcpu *vcpu);
 void kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 			 int bytes);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4251| <<kvm_arch_async_page_ready>> r = kvm_mmu_reload(vcpu);
+ *   - arch/x86/kvm/x86.c|10929| <<vcpu_enter_guest>> r = kvm_mmu_reload(vcpu);
+ */
 static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
 {
 	if (likely(vcpu->arch.mmu->root.hpa != INVALID_PAGE))
@@ -132,6 +140,12 @@ static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
 	return kvm_mmu_load(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|151| <<kvm_get_active_pcid>> return kvm_get_pcid(vcpu, kvm_read_cr3(vcpu));
+ *   - arch/x86/kvm/mmu/mmu.c|7699| <<kvm_mmu_invpcid_gva>> pcid == kvm_get_pcid(vcpu, mmu->prev_roots[i].pgd))
+ *   - arch/x86/kvm/x86.c|1288| <<kvm_invalidate_pcid>> if (kvm_get_pcid(vcpu, mmu->prev_roots[i].pgd) == pcid)
+ */
 static inline unsigned long kvm_get_pcid(struct kvm_vcpu *vcpu, gpa_t cr3)
 {
 	BUILD_BUG_ON((X86_CR3_PCID_MASK & PAGE_MASK) != 0);
@@ -141,11 +155,24 @@ static inline unsigned long kvm_get_pcid(struct kvm_vcpu *vcpu, gpa_t cr3)
 	       : 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7694| <<kvm_mmu_invpcid_gva>> if (pcid == kvm_get_active_pcid(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|4260| <<svm_load_mmu_pgd>> cr3 = __sme_set(root_hpa) | kvm_get_active_pcid(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|4263| <<svm_load_mmu_pgd>> WARN_ON_ONCE(kvm_get_active_pcid(vcpu));
+ *   - arch/x86/kvm/vmx/vmx.c|3408| <<vmx_load_mmu_pgd>> guest_cr3 = root_hpa | kvm_get_active_pcid(vcpu);
+ *   - arch/x86/kvm/x86.c|1274| <<kvm_invalidate_pcid>> if (kvm_get_active_pcid(vcpu) == pcid) {
+ */
 static inline unsigned long kvm_get_active_pcid(struct kvm_vcpu *vcpu)
 {
 	return kvm_get_pcid(vcpu, kvm_read_cr3(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6278| <<kvm_mmu_load>> kvm_mmu_load_pgd(vcpu);
+ *   - arch/x86/kvm/x86.c|10882| <<vcpu_enter_guest(KVM_REQ_LOAD_MMU_PGD)>> kvm_mmu_load_pgd(vcpu);
+ */
 static inline void kvm_mmu_load_pgd(struct kvm_vcpu *vcpu)
 {
 	u64 root_hpa = vcpu->arch.mmu->root.hpa;
@@ -153,10 +180,18 @@ static inline void kvm_mmu_load_pgd(struct kvm_vcpu *vcpu)
 	if (!VALID_PAGE(root_hpa))
 		return;
 
+	/*
+	 * vmx_load_mmu_pgd()
+	 * svm_load_mmu_pgd()
+	 */
 	static_call(kvm_x86_load_mmu_pgd)(vcpu, root_hpa,
 					  vcpu->arch.mmu->root_role.level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|226| <<permission_fault>> kvm_mmu_refresh_passthrough_bits(vcpu, mmu);
+ */
 static inline void kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,
 						    struct kvm_mmu *mmu)
 {
@@ -294,6 +329,13 @@ kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, int level)
 	return __kvm_mmu_slot_lpages(slot, slot->npages, level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|735| <<mmu_spte_clear_track_bits>> kvm_update_page_stats(kvm, level, -1);
+ *   - arch/x86/kvm/mmu/mmu.c|2268| <<__rmap_add>> kvm_update_page_stats(kvm, sp->role.level, 1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|598| <<handle_changed_spte>> kvm_update_page_stats(kvm, level, is_leaf ? 1 : -1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1577| <<tdp_mmu_split_huge_page>> kvm_update_page_stats(kvm, level - 1, SPTE_ENT_PER_PAGE);
+ */
 static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 {
 	atomic64_add(count, &kvm->stat.pages[level - 1]);
@@ -302,6 +344,13 @@ static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u64 access,
 			   struct x86_exception *exception);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4061| <<nonpaging_gva_to_gpa>> return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|379| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|448| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(gfn), access, &walker->fault);
+ *   - arch/x86/kvm/x86.c|891| <<load_pdptrs>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn),
+ */
 static inline gpa_t kvm_translate_gpa(struct kvm_vcpu *vcpu,
 				      struct kvm_mmu *mmu,
 				      gpa_t gpa, u64 access,
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index c57e181bb..dbe2769d7 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -57,8 +57,25 @@
 
 #include "trace.h"
 
+/*
+ * https://github.com/0voice/Introduce_to_virtualization/blob/main/virtualization_type/memory_virtualization/%E5%86%85%E5%AD%98%E8%99%9A%E6%8B%9F%E5%8C%96-shadow%E5%AE%9E%E7%8E%B0.md
+ *
+ * 影子页表
+ * 首先遍历虚机页表,获取GVA映射的GPA,
+ * 然后通过kvm slot,获取GPA对应的HPA,进而在shadow中建立GVA->HPA的映射.
+ */
+
 extern bool itlb_multihit_kvm_mitigation;
 
+/*
+ * 在以下使用nx_hugepage_mitigation_hard_disabled:
+ *   - arch/x86/kvm/mmu/mmu.c|62| <<global>> static bool nx_hugepage_mitigation_hard_disabled;
+ *   - arch/x86/kvm/mmu/mmu.c|7389| <<get_nx_huge_pages>> if (nx_hugepage_mitigation_hard_disabled)
+ *   - arch/x86/kvm/mmu/mmu.c|7411| <<set_nx_huge_pages>> if (nx_hugepage_mitigation_hard_disabled)
+ *   - arch/x86/kvm/mmu/mmu.c|7429| <<set_nx_huge_pages>> nx_hugepage_mitigation_hard_disabled = true;
+ *   - arch/x86/kvm/mmu/mmu.c|7577| <<set_nx_huge_pages_recovery_param>> if (nx_hugepage_mitigation_hard_disabled)
+ *   - arch/x86/kvm/mmu/mmu.c|7738| <<kvm_mmu_post_init_vm>> if (nx_hugepage_mitigation_hard_disabled)
+ */
 static bool nx_hugepage_mitigation_hard_disabled;
 
 int __read_mostly nx_huge_pages = -1;
@@ -153,13 +170,34 @@ struct pte_list_desc {
 };
 
 struct kvm_shadow_walk_iterator {
+	/*
+	 * 发生page fault的GPA,迭代过程就是要把GPA所涉及的页表项都填上
+	 */
 	u64 addr;
+	/*
+	 * 在以下使用kvm_shadow_walk_iterator->shadow_addr:
+	 *   - arch/x86/kvm/mmu/mmu.c|2779| <<shadow_walk_init_using_root>> iterator->shadow_addr = root;
+	 *   - arch/x86/kvm/mmu/mmu.c|2794| <<shadow_walk_init_using_root>> iterator->shadow_addr
+	 *   - arch/x86/kvm/mmu/mmu.c|2796| <<shadow_walk_init_using_root>> iterator->shadow_addr &= SPTE_BASE_ADDR_MASK;
+	 *   - arch/x86/kvm/mmu/mmu.c|2798| <<shadow_walk_init_using_root>> if (!iterator->shadow_addr)
+	 *   - arch/x86/kvm/mmu/mmu.c|2816| <<shadow_walk_okay>> iterator->sptep = ((u64 *)__va(iterator->shadow_addr)) + iterator->index;
+	 *   - arch/x86/kvm/mmu/mmu.c|2828| <<__shadow_walk_next>> iterator->shadow_addr = spte & SPTE_BASE_ADDR_MASK;
+	 *
+	 * 当前页表项的 HPA，在 shadow_walk_init 中设置为 vcpu->arch.mmu.root_hpa
+	 */
 	hpa_t shadow_addr;
+	/*
+	 * 指向当前页表项, 在shadow_walk_okay()中更新
+	 */
 	u64 *sptep;
 	int level;
 	unsigned index;
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6419| <<__kvm_mmu_invalidate_addr>> for_each_shadow_entry_using_root(vcpu, root_hpa, addr, iterator) {
+ */
 #define for_each_shadow_entry_using_root(_vcpu, _root, _addr, _walker)     \
 	for (shadow_walk_init_using_root(&(_walker), (_vcpu),              \
 					 (_root), (_addr));                \
@@ -266,14 +304,33 @@ static inline unsigned long kvm_mmu_get_guest_pgd(struct kvm_vcpu *vcpu,
 	if (IS_ENABLED(CONFIG_RETPOLINE) && mmu->get_guest_pgd == get_guest_cr3)
 		return kvm_read_cr3(vcpu);
 
+	/*
+	 * 在以下使用kvm_mmu->get_guest_pgd:
+	 *   - arch/x86/kvm/mmu/mmu.c|275| <<kvm_mmu_get_guest_pgd>> if (IS_ENABLED(CONFIG_RETPOLINE) && mmu->get_guest_pgd == get_guest_cr3)
+	 *   - arch/x86/kvm/mmu/mmu.c|278| <<kvm_mmu_get_guest_pgd>> return mmu->get_guest_pgd(vcpu);
+	 *   - arch/x86/kvm/mmu/mmu.c|5802| <<init_kvm_tdp_mmu>> context->get_guest_pgd = get_guest_cr3;
+	 *   - arch/x86/kvm/mmu/mmu.c|5951| <<init_kvm_softmmu>> context->get_guest_pgd = get_guest_cr3;
+	 *   - arch/x86/kvm/mmu/mmu.c|5965| <<init_kvm_nested_mmu>> g_context->get_guest_pgd = get_guest_cr3;
+	 *   - arch/x86/kvm/svm/nested.c|96| <<nested_svm_init_mmu_context>> vcpu->arch.mmu->get_guest_pgd = nested_svm_get_tdp_cr3;
+	 *   - arch/x86/kvm/vmx/nested.c|428| <<nested_ept_init_mmu_context>> vcpu->arch.mmu->get_guest_pgd = nested_ept_get_eptp;
+	 */
 	return mmu->get_guest_pgd(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1772| <<kvm_set_pte_rmap>> if (need_flush && kvm_available_flush_remote_tlbs_range()) {
+ *   - arch/x86/kvm/mmu/mmu.c|7492| <<kvm_mmu_zap_collapsible_spte>> if (kvm_available_flush_remote_tlbs_range())
+ */
 static inline bool kvm_available_flush_remote_tlbs_range(void)
 {
 	return kvm_x86_ops.flush_remote_tlbs_range;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|369| <<kvm_flush_remote_tlbs_range>> if (!kvm_arch_flush_remote_tlbs_range(kvm, gfn, nr_pages))
+ */
 int kvm_arch_flush_remote_tlbs_range(struct kvm *kvm, gfn_t gfn, u64 nr_pages)
 {
 	if (!kvm_x86_ops.flush_remote_tlbs_range)
@@ -284,15 +341,41 @@ int kvm_arch_flush_remote_tlbs_range(struct kvm *kvm, gfn_t gfn, u64 nr_pages)
 
 static gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1384| <<drop_large_spte>> kvm_flush_remote_tlbs_sptep(kvm, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|2689| <<validate_direct_spte>> kvm_flush_remote_tlbs_sptep(vcpu->kvm, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|6205| <<__kvm_mmu_invalidate_addr>> kvm_flush_remote_tlbs_sptep(vcpu->kvm, iterator.sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|7003| <<kvm_mmu_zap_collapsible_spte>> kvm_flush_remote_tlbs_sptep(kvm, sptep);
+ */
 /* Flush the range of guest memory mapped by the given SPTE. */
 static void kvm_flush_remote_tlbs_sptep(struct kvm *kvm, u64 *sptep)
 {
 	struct kvm_mmu_page *sp = sptep_to_sp(sptep);
+	/*
+	 * 给出一个kvm_mmu_page, 这个page存了一个页表的页, 里面有512个entry
+	 * 每个entry都代表了一个起始的gfn
+	 * sp->gfn本身是这个页表的第一个entry指向的gfn
+	 * kvm_mmu_page_get_gfn()返回参数index代表的entry所指向的gfn
+	 * index是页表中entry的index
+	 */
 	gfn_t gfn = kvm_mmu_page_get_gfn(sp, spte_index(sptep));
 
 	kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2919| <<mmu_set_spte>> mark_mmio_spte(vcpu, sptep, gfn, pte_access);
+ *   - arch/x86/kvm/mmu/mmu.c|4689| <<sync_mmio_spte>> mark_mmio_spte(vcpu, sptep, gfn, access);
+ *
+ * 以前的注释
+ * 根据gfn为sptep设置entry, 最后3位是110, 用来作为mmio的entry
+ * 似乎是设置完mmio之后:
+ * 1. SPTE_SPECIAL_MASK(1往左移62位)是1
+ * 2. 结尾是110
+ * 3. 还有gen的信息
+ */
 static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 			   unsigned int access)
 {
@@ -300,8 +383,24 @@ static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 
 	trace_mark_mmio_spte(sptep, gfn, spte);
 	mmu_spte_set(sptep, spte);
+	/*
+	 * 以前的注释
+	 * 根据gfn为sptep设置entry, 最后3位是110, 用来作为mmio的entry
+	 * 似乎是设置完mmio之后:
+	 * 1. SPTE_SPECIAL_MASK(1往左移62位)是1
+	 * 2. 结尾是110
+	 * 3. 还有gen的信息
+	 */
 }
 
+/*
+ * 旧的注释
+ * spte bits 3-11 are used as bits 1-9 of the generation number,
+ * the bits 52-61 are used as bits 10-19 of the generation number.
+ *
+ * 过滤掉spte中作为gen的部分 (3-11位和52-61位, 还有SPTE_SPECIAL_MASK是1往左移62位)
+ * 向右移动12位变成gfn
+ */
 static gfn_t get_mmio_spte_gfn(u64 spte)
 {
 	u64 gpa = spte & shadow_nonpresent_or_rsvd_lower_gfn_mask;
@@ -317,6 +416,11 @@ static unsigned get_mmio_spte_access(u64 spte)
 	return spte & shadow_mmio_access_mask;
 }
 
+/*
+ * 旧的注释
+ * 判断spte中的gen (把spte的3-11位和52-61位取出来拼在一起, 组成gen)
+ * 是否和kvm_vcpu_memslots(vcpu)->generation & MMIO_GEN_MASK(20位)相等
+ */
 static bool check_mmio_spte(struct kvm_vcpu *vcpu, u64 spte)
 {
 	u64 kvm_gen, spte_gen, gen;
@@ -480,9 +584,23 @@ static u64 __get_spte_lockless(u64 *sptep)
  * or in a state where the hardware will not attempt to update
  * the spte.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|376| <<mark_mmio_spte>> mmu_spte_set(sptep, spte);
+ *   - arch/x86/kvm/mmu/mmu.c|575| <<mmu_spte_update_no_track>> mmu_spte_set(sptep, new_spte);
+ *   - arch/x86/kvm/mmu/mmu.c|1832| <<kvm_set_pte_rmap>> mmu_spte_set(sptep, new_spte);
+ *   - arch/x86/kvm/mmu/mmu.c|3090| <<__link_shadow_page>> mmu_spte_set(sptep, spte);
+ *   - arch/x86/kvm/mmu/mmu.c|7373| <<shadow_mmu_split_huge_page>> mmu_spte_set(sptep, spte);
+ */
 static void mmu_spte_set(u64 *sptep, u64 new_spte)
 {
+	/*
+	 * 查看bit 11是否设置了
+	 */
 	WARN_ON_ONCE(is_shadow_present_pte(*sptep));
+	/*
+	 * 把新的spte的value写入sptep指向的内存
+	 */
 	__set_spte(sptep, new_spte);
 }
 
@@ -490,6 +608,11 @@ static void mmu_spte_set(u64 *sptep, u64 new_spte)
  * Update the SPTE (excluding the PFN), but do not track changes in its
  * accessed/dirty status.
  */
+/*
+ * Update the SPTE (excluding the PFN), but do not track changes in its
+ * accessed/dirty status.
+ * 根据是否有volatile bits用WRITE_ONCE(*sptep, spte)或者xchg(sptep, spte)更新spte
+ */
 static u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)
 {
 	u64 old_spte = *sptep;
@@ -502,6 +625,14 @@ static u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)
 		return old_spte;
 	}
 
+	/*
+	 * volatile bits: bits that can be set outside of mmu_lock. The Writable bit
+	 * can be set by KVM's fast page fault handler, and Accessed and Dirty bits
+	 * can be set by the CPU.
+	 *
+	 * 也就是说,volatile bits是那些可以在lock外修改的bit,比如在fast page fault中
+	 * 的writable,或者硬件支持的access/dirty bit.
+	 */
 	if (!spte_has_volatile_bits(old_spte))
 		__update_clear_spte_fast(sptep, new_spte);
 	else
@@ -521,9 +652,21 @@ static u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)
  *
  * Returns true if the TLB needs to be flushed
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1562| <<spte_write_protect>> return mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu/mmu.c|1597| <<spte_clear_dirty>> return mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu/mmu.c|3668| <<mmu_set_spte>> flush |= mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1167| <<FNAME(sync_spte)>> return mmu_spte_update(sptep, spte);
+ */
 static bool mmu_spte_update(u64 *sptep, u64 new_spte)
 {
 	bool flush = false;
+	/*
+	 * Update the SPTE (excluding the PFN), but do not track changes in its
+	 * accessed/dirty status.
+	 * 根据是否有volatile bits用WRITE_ONCE(*sptep, spte)或者xchg(sptep, spte)更新spte
+	 */
 	u64 old_spte = mmu_spte_update_no_track(sptep, new_spte);
 
 	if (!is_shadow_present_pte(old_spte))
@@ -562,10 +705,21 @@ static bool mmu_spte_update(u64 *sptep, u64 new_spte)
  * state bits, it is used to clear the last level sptep.
  * Returns the old PTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1081| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|1101| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+ *   - arch/x86/kvm/mmu/mmu.c|1109| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+ *   - arch/x86/kvm/mmu/mmu.c|1258| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|1547| <<kvm_set_pte_rmap>> mmu_spte_clear_track_bits(kvm, sptep);
+ */
 static u64 mmu_spte_clear_track_bits(struct kvm *kvm, u64 *sptep)
 {
 	kvm_pfn_t pfn;
 	u64 old_spte = *sptep;
+	/*
+	 * 比如4, 3, 2, 1
+	 */
 	int level = sptep_to_sp(sptep)->role.level;
 	struct page *page;
 
@@ -605,16 +759,32 @@ static u64 mmu_spte_clear_track_bits(struct kvm *kvm, u64 *sptep)
  * Directly clear spte without caring the state bits of sptep,
  * it is used to set the upper level spte.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2237| <<drop_parent_pte>> mmu_spte_clear_no_track(parent_pte);
+ *   - arch/x86/kvm/mmu/mmu.c|3173| <<mmu_page_zap_pte>> mmu_spte_clear_no_track(spte);
+ *   - arch/x86/kvm/mmu/mmu.c|5562| <<sync_mmio_spte>> mmu_spte_clear_no_track(sptep);
+ */
 static void mmu_spte_clear_no_track(u64 *sptep)
 {
 	__update_clear_spte_fast(sptep, 0ull);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|213| <<for_each_shadow_entry_lockless>> ({ spte = mmu_spte_get_lockless(_walker.sptep); 1; }); \
+ *   - arch/x86/kvm/mmu/mmu.c|707| <<mmu_spte_age>> u64 spte = mmu_spte_get_lockless(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|4882| <<get_walk>> spte = mmu_spte_get_lockless(iterator.sptep);
+ */
 static u64 mmu_spte_get_lockless(u64 *sptep)
 {
 	return __get_spte_lockless(sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1922| <<kvm_age_rmap>> young |= mmu_spte_age(sptep);
+ */
 /* Returns the Accessed status of the PTE and resets it at the same time. */
 static bool mmu_spte_age(u64 *sptep)
 {
@@ -646,6 +816,12 @@ static inline bool is_tdp_mmu_active(struct kvm_vcpu *vcpu)
 	return tdp_mmu_enabled && vcpu->arch.mmu->root_role.direct;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4819| <<fast_page_fault>> walk_shadow_page_lockless_begin(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|5640| <<get_mmio_spte>> walk_shadow_page_lockless_begin(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|5750| <<shadow_page_table_clear_flood>> walk_shadow_page_lockless_begin(vcpu);
+ */
 static void walk_shadow_page_lockless_begin(struct kvm_vcpu *vcpu)
 {
 	if (is_tdp_mmu_active(vcpu)) {
@@ -665,6 +841,12 @@ static void walk_shadow_page_lockless_begin(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4922| <<fast_page_fault>> walk_shadow_page_lockless_end(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|5647| <<get_mmio_spte>> walk_shadow_page_lockless_end(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|5753| <<shadow_page_table_clear_flood>> walk_shadow_page_lockless_end(vcpu);
+ */
 static void walk_shadow_page_lockless_end(struct kvm_vcpu *vcpu)
 {
 	if (is_tdp_mmu_active(vcpu)) {
@@ -680,6 +862,13 @@ static void walk_shadow_page_lockless_end(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4387| <<direct_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+ *   - arch/x86/kvm/mmu/mmu.c|4467| <<kvm_tdp_mmu_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+ *   - arch/x86/kvm/mmu/mmu.c|5490| <<kvm_mmu_load>> r = mmu_topup_memory_caches(vcpu, !vcpu->arch.mmu->root_role.direct);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|811| <<FNAME(page_fault)>> r = mmu_topup_memory_caches(vcpu, true);
+ */
 static int mmu_topup_memory_caches(struct kvm_vcpu *vcpu, bool maybe_indirect)
 {
 	int r;
@@ -718,14 +907,62 @@ static void mmu_free_pte_list_desc(struct pte_list_desc *pte_list_desc)
 
 static bool sp_has_gptes(struct kvm_mmu_page *sp);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|291| <<kvm_flush_remote_tlbs_sptep>> gfn_t gfn = kvm_mmu_page_get_gfn(sp, spte_index(sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|791| <<kvm_mmu_page_set_translation>> WARN_ONCE(gfn != kvm_mmu_page_get_gfn(sp, index),
+ *   - arch/x86/kvm/mmu/mmu.c|794| <<kvm_mmu_page_set_translation>> sp->gfn, kvm_mmu_page_get_gfn(sp, index), gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|800| <<kvm_mmu_page_set_access>> gfn_t gfn = kvm_mmu_page_get_gfn(sp, index);
+ *   - arch/x86/kvm/mmu/mmu.c|1201| <<rmap_remove>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|1873| <<kvm_mmu_check_sptes_at_free>> kvm_mmu_page_get_gfn(sp, i));
+ *   - arch/x86/kvm/mmu/mmu.c|3209| <<direct_pte_prefetch_many>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(start));
+ *   - arch/x86/kvm/mmu/mmu.c|6729| <<shadow_mmu_get_sp_for_split>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|6765| <<shadow_mmu_split_huge_page>> gfn = kvm_mmu_page_get_gfn(sp, index);
+ *   - arch/x86/kvm/mmu/mmu.c|6804| <<shadow_mmu_try_split_huge_page>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|949| <<FNAME(sync_spte)>> gfn != kvm_mmu_page_get_gfn(sp, i)) {
+ *
+ * 给出一个kvm_mmu_page, 这个page存了一个页表的页, 里面有512个entry
+ * 每个entry都代表了一个起始的gfn
+ * sp->gfn本身是这个页表的第一个entry指向的gfn
+ * kvm_mmu_page_get_gfn()返回参数index代表的entry所指向的gfn
+ * index是页表中entry的index
+ */
 static gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index)
 {
+	/*
+	 * 在以下直接使用role的passthrough:
+	 *   - arch/x86/kvm/mmu/mmu.c|806| <<kvm_mmu_page_get_gfn>> if (sp->role.passthrough)
+	 *   - arch/x86/kvm/mmu/mmu.c|856| <<kvm_mmu_page_set_translation>> sp->role.passthrough ? "passthrough" : "direct",
+	 *   - arch/x86/kvm/mmu/mmu.c|861| <<kvm_mmu_page_set_translation>> sp->role.passthrough ? "passthrough" : "direct",
+	 *   - arch/x86/kvm/mmu/mmu.c|2323| <<sp_has_gptes>> if (sp->role.passthrough)
+	 *   - arch/x86/kvm/mmu/mmu.c|2366| <<kvm_sync_page_check>> .passthrough = 0x1,
+	 *   - arch/x86/kvm/mmu/mmu.c|2847| <<kvm_mmu_child_role>> role.passthrough = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|6120| <<kvm_init_shadow_npt_mmu>> root_role.passthrough = 1;
+	 */
 	if (sp->role.passthrough)
 		return sp->gfn;
 
+	/*
+	 * 在以下使用kvm_mmu_page->shadowed_translation:
+	 *   - arch/x86/kvm/mmu/mmu.c|781| <<kvm_mmu_page_get_gfn>> return sp->shadowed_translation[index] >> PAGE_SHIFT;
+	 *   - arch/x86/kvm/mmu/mmu.c|795| <<kvm_mmu_page_get_access>> return sp->shadowed_translation[index] & ACC_ALL;
+	 *   - arch/x86/kvm/mmu/mmu.c|816| <<kvm_mmu_page_set_translation>> sp->shadowed_translation[index] = (gfn << PAGE_SHIFT) | access;
+	 *   - arch/x86/kvm/mmu/mmu.c|2104| <<kvm_mmu_free_shadow_page>> free_page((unsigned long )sp->shadowed_translation);
+	 *   - arch/x86/kvm/mmu/mmu.c|2655| <<kvm_mmu_alloc_shadow_page>> sp->shadowed_translation = kvm_mmu_memory_cache_alloc(caches->shadowed_info_cache);
+	 */
 	if (!sp->role.direct)
 		return sp->shadowed_translation[index] >> PAGE_SHIFT;
 
+	/*
+	 * SPTE_LEVEL_BITS是9
+	 *
+	 * (sp->role.level - 1) * SPTE_LEVEL_BITS:
+	 * (1 - 1) * SPTE_LEVEL_BITS = 0 * SPTE_LEVEL_BITS = 0
+	 * (2 - 1) * SPTE_LEVEL_BITS = 1 * SPTE_LEVEL_BITS = 9
+	 * (3 - 1) * SPTE_LEVEL_BITS = 2 * SPTE_LEVEL_BITS = 18
+	 * (4 - 1) * SPTE_LEVEL_BITS = 3 * SPTE_LEVEL_BITS = 27
+	 * (5 - 1) * SPTE_LEVEL_BITS = 4 * SPTE_LEVEL_BITS = 36
+	 */
 	return sp->gfn + (index << ((sp->role.level - 1) * SPTE_LEVEL_BITS));
 }
 
@@ -755,10 +992,25 @@ static u32 kvm_mmu_page_get_access(struct kvm_mmu_page *sp, int index)
 	return sp->role.access;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|836| <<kvm_mmu_page_set_access>> kvm_mmu_page_set_translation(sp, index, gfn, access);
+ *   - arch/x86/kvm/mmu/mmu.c|1980| <<__rmap_add>> kvm_mmu_page_set_translation(sp, spte_index(spte), gfn, access);
+ */
 static void kvm_mmu_page_set_translation(struct kvm_mmu_page *sp, int index,
 					 gfn_t gfn, unsigned int access)
 {
 	if (sp_has_gptes(sp)) {
+		/*
+		 * 在以下使用kvm_mmu_page->shadowed_translation:
+		 *   - arch/x86/kvm/mmu/mmu.c|781| <<kvm_mmu_page_get_gfn>> return sp->shadowed_translation[index] >> PAGE_SHIFT;
+		 *   - arch/x86/kvm/mmu/mmu.c|795| <<kvm_mmu_page_get_access>> return sp->shadowed_translation[index] & ACC_ALL;
+		 *   - arch/x86/kvm/mmu/mmu.c|816| <<kvm_mmu_page_set_translation>> sp->shadowed_translation[index] = (gfn << PAGE_SHIFT) | access;
+		 *   - arch/x86/kvm/mmu/mmu.c|2104| <<kvm_mmu_free_shadow_page>> free_page((unsigned long )sp->shadowed_translation);
+		 *   - arch/x86/kvm/mmu/mmu.c|2655| <<kvm_mmu_alloc_shadow_page>> sp->shadowed_translation = kvm_mmu_memory_cache_alloc(caches->shadowed_info_cache);
+		 *
+		 * 以前是gfns? -> 所有页表项(pte)对应的gfn
+		 */
 		sp->shadowed_translation[index] = (gfn << PAGE_SHIFT) | access;
 		return;
 	}
@@ -774,6 +1026,11 @@ static void kvm_mmu_page_set_translation(struct kvm_mmu_page *sp, int index,
 	          sp->gfn, kvm_mmu_page_get_gfn(sp, index), gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4242| <<mmu_set_spte>> kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1236| <<FNAME(sync_spte)>> kvm_mmu_page_set_access(sp, i, pte_access);
+ */
 static void kvm_mmu_page_set_access(struct kvm_mmu_page *sp, int index,
 				    unsigned int access)
 {
@@ -808,6 +1065,11 @@ static void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1071| <<account_shadowed>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|94| <<__kvm_write_track_add_gfn>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, 1);
@@ -818,27 +1080,78 @@ void kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 	update_gfn_disallow_lpage_count(slot, gfn, -1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3235| <<kvm_mmu_alloc_shadow_page>> account_shadowed(kvm, sp);
+ *
+ * __kvm_mmu_get_shadow_page()
+ * -> kvm_mmu_alloc_shadow_page()
+ *    -> account_shadowed()
+ *
+ * account_shadowed()会把shadow的mmu page设置成readonly!!!!!
+ */
 static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
 	struct kvm_memory_slot *slot;
 	gfn_t gfn;
 
+	/*
+	 * 在以下使用kvm_arch->indirect_shadow_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1084| <<account_shadowed>> kvm->arch.indirect_shadow_pages++;
+	 *   - arch/x86/kvm/mmu/mmu.c|1161| <<unaccount_shadowed>> kvm->arch.indirect_shadow_pages--;
+	 *   - arch/x86/kvm/mmu/mmu.c|7419| <<kvm_mmu_track_write>> if (!READ_ONCE(vcpu->kvm->arch.indirect_shadow_pages))
+	 *   - arch/x86/kvm/x86.c|9045| <<reexecute_instruction>> indirect_shadow_pages = vcpu->kvm->arch.indirect_shadow_pages;
+	 */
 	kvm->arch.indirect_shadow_pages++;
+	/*
+	 * 似乎对direct是管理地址范围的起始地址对应的gfn
+	 * 对于shadow, 是当前sp的对应的guest的gfn
+	 */
 	gfn = sp->gfn;
 	slots = kvm_memslots_for_spte_role(kvm, sp->role);
 	slot = __gfn_to_memslot(slots, gfn);
 
+	/*
+	 * #define PT64_ROOT_5LEVEL 5
+	 * #define PT64_ROOT_4LEVEL 4
+	 * #define PT32_ROOT_LEVEL 2
+	 * #define PT32E_ROOT_LEVEL 3
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1088| <<account_shadowed>> return __kvm_write_track_add_gfn(kvm, slot, gfn);
+	 *   - arch/x86/kvm/mmu/page_track.c|269| <<kvm_write_track_add_gfn>> __kvm_write_track_add_gfn(kvm, slot, gfn);
+	 *
+	 *
+	 * 主要是这里把shadow的mmu page设置成readonly!!!!!
+	 * __kvm_write_track_add_gfn()
+	 * -> update_gfn_write_track()
+	 * -> kvm_mmu_gfn_disallow_lpage()
+	 * -> kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K)
+	 */
 	/* the non-leaf shadow pages are keeping readonly. */
 	if (sp->role.level > PG_LEVEL_4K)
 		return __kvm_write_track_add_gfn(kvm, slot, gfn);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1071| <<account_shadowed>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+	 *   - arch/x86/kvm/mmu/page_track.c|94| <<__kvm_write_track_add_gfn>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+	 */
 	kvm_mmu_gfn_disallow_lpage(slot, gfn);
 
+	/*
+	 * 主要是这里把shadow的mmu page设置成readonly!!!!!
+	 */
 	if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
 		kvm_flush_remote_tlbs_gfn(kvm, gfn, PG_LEVEL_4K);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1167| <<account_nx_huge_page>> track_possible_nx_huge_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1707| <<kvm_tdp_mmu_map>> track_possible_nx_huge_page(kvm, sp);
+ */
 void track_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	/*
@@ -872,6 +1185,13 @@ static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	struct kvm_memory_slot *slot;
 	gfn_t gfn;
 
+	/*
+	 * 在以下使用kvm_arch->indirect_shadow_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1084| <<account_shadowed>> kvm->arch.indirect_shadow_pages++;
+	 *   - arch/x86/kvm/mmu/mmu.c|1161| <<unaccount_shadowed>> kvm->arch.indirect_shadow_pages--;
+	 *   - arch/x86/kvm/mmu/mmu.c|7419| <<kvm_mmu_track_write>> if (!READ_ONCE(vcpu->kvm->arch.indirect_shadow_pages))
+	 *   - arch/x86/kvm/x86.c|9045| <<reexecute_instruction>> indirect_shadow_pages = vcpu->kvm->arch.indirect_shadow_pages;
+	 */
 	kvm->arch.indirect_shadow_pages--;
 	gfn = sp->gfn;
 	slots = kvm_memslots_for_spte_role(kvm, sp->role);
@@ -898,6 +1218,11 @@ static void unaccount_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 	untrack_possible_nx_huge_page(kvm, sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3018| <<direct_pte_prefetch_many>> slot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, access & ACC_WRITE_MASK);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|548| <<FNAME(prefetch_gpte)>> slot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, pte_access & ACC_WRITE_MASK);
+ */
 static struct kvm_memory_slot *gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu,
 							   gfn_t gfn,
 							   bool no_dirty_log)
@@ -924,6 +1249,13 @@ static struct kvm_memory_slot *gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu
 /*
  * Returns the number of pointers in the rmap chain, not counting the new one.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1661| <<__rmap_add>> rmap_count = pte_list_add(cache, spte, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1766| <<mmu_page_add_parent_pte>> pte_list_add(cache, parent_pte, &sp->parent_ptes);
+ *
+ * spte应该是指向某一个entry的地址
+ */
 static int pte_list_add(struct kvm_mmu_memory_cache *cache, u64 *spte,
 			struct kvm_rmap_head *rmap_head)
 {
@@ -933,6 +1265,16 @@ static int pte_list_add(struct kvm_mmu_memory_cache *cache, u64 *spte,
 	if (!rmap_head->val) {
 		rmap_head->val = (unsigned long)spte;
 	} else if (!(rmap_head->val & 1)) {
+		/*
+		 * struct pte_list_desc {
+		 *     struct pte_list_desc *more;
+		 *     // The number of PTEs stored in _this_ descriptor.
+		 *     u32 spte_count;
+		 *     // The number of PTEs stored in all tails of this descriptor.
+		 *     u32 tail_count;
+		 *     u64 *sptes[PTE_LIST_EXT];
+		 * };
+		 */
 		desc = kvm_mmu_memory_cache_alloc(cache);
 		desc->sptes[0] = (u64 *)rmap_head->val;
 		desc->sptes[1] = spte;
@@ -955,11 +1297,18 @@ static int pte_list_add(struct kvm_mmu_memory_cache *cache, u64 *spte,
 			desc->tail_count = count;
 			rmap_head->val = (unsigned long)desc | 1;
 		}
+		/*
+		 * 不到14个就用下一个slot
+		 */
 		desc->sptes[desc->spte_count++] = spte;
 	}
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1031| <<pte_list_remove>> pte_list_desc_remove_entry(kvm, rmap_head, desc, i);
+ */
 static void pte_list_desc_remove_entry(struct kvm *kvm,
 				       struct kvm_rmap_head *rmap_head,
 				       struct pte_list_desc *desc, int i)
@@ -997,6 +1346,14 @@ static void pte_list_desc_remove_entry(struct kvm *kvm,
 	mmu_free_pte_list_desc(head_desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1047| <<kvm_zap_one_rmap_spte>> pte_list_remove(kvm, sptep, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1122| <<rmap_remove>> pte_list_remove(kvm, spte, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1772| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+ *
+ * 把spte对应的entry从kvm_rmap_head删除
+ */
 static void pte_list_remove(struct kvm *kvm, u64 *spte,
 			    struct kvm_rmap_head *rmap_head)
 {
@@ -1013,6 +1370,9 @@ static void pte_list_remove(struct kvm *kvm, u64 *spte,
 		rmap_head->val = 0;
 	} else {
 		desc = (struct pte_list_desc *)(rmap_head->val & ~1ul);
+		/*
+		 * 这里是循环, 不行就换下一个desc
+		 */
 		while (desc) {
 			for (i = 0; i < desc->spte_count; ++i) {
 				if (desc->sptes[i] == spte) {
@@ -1028,6 +1388,11 @@ static void pte_list_remove(struct kvm *kvm, u64 *spte,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1541| <<kvm_set_pte_rmap>> kvm_zap_one_rmap_spte(kvm, rmap_head, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|6872| <<kvm_mmu_zap_collapsible_spte>> kvm_zap_one_rmap_spte(kvm, rmap_head, sptep);
+ */
 static void kvm_zap_one_rmap_spte(struct kvm *kvm,
 				  struct kvm_rmap_head *rmap_head, u64 *sptep)
 {
@@ -1035,6 +1400,13 @@ static void kvm_zap_one_rmap_spte(struct kvm *kvm,
 	pte_list_remove(kvm, sptep, rmap_head);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1453| <<__kvm_zap_rmap>> return kvm_zap_all_rmap_sptes(kvm, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1666| <<__rmap_add>> kvm_zap_all_rmap_sptes(kvm, rmap_head);
+ *
+ * 把所有kvm_rmap_head中的spte(指向同一个page的spte)清空clear
+ */
 /* Return true if at least one SPTE was zapped, false otherwise */
 static bool kvm_zap_all_rmap_sptes(struct kvm *kvm,
 				   struct kvm_rmap_head *rmap_head)
@@ -1064,6 +1436,12 @@ static bool kvm_zap_all_rmap_sptes(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|124| <<kvm_mmu_rmaps_stat_show>> index = ffs(pte_list_count(&rmap[l]));
+ *
+ * kvm_rmap_head中spte的数量
+ */
 unsigned int pte_list_count(struct kvm_rmap_head *rmap_head)
 {
 	struct pte_list_desc *desc;
@@ -1077,15 +1455,41 @@ unsigned int pte_list_count(struct kvm_rmap_head *rmap_head)
 	return desc->tail_count + desc->spte_count;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1160| <<rmap_remove>> rmap_head = gfn_to_rmap(gfn, sp->role.level, slot);
+ *   - arch/x86/kvm/mmu/mmu.c|1387| <<kvm_mmu_write_protect_pt_masked>> rmap_head = gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
+ *   - arch/x86/kvm/mmu/mmu.c|1420| <<kvm_mmu_clear_dirty_pt_masked>> rmap_head = gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
+ *   - arch/x86/kvm/mmu/mmu.c|1490| <<kvm_mmu_slot_gfn_write_protect>> rmap_head = gfn_to_rmap(gfn, i, slot);
+ *   - arch/x86/kvm/mmu/mmu.c|1582| <<rmap_walk_init_level>> iterator->rmap = gfn_to_rmap(iterator->gfn, level, iterator->slot);
+ *   - arch/x86/kvm/mmu/mmu.c|1583| <<rmap_walk_init_level>> iterator->end_rmap = gfn_to_rmap(iterator->end_gfn, level, iterator->slot);
+ *   - arch/x86/kvm/mmu/mmu.c|1720| <<__rmap_add>> rmap_head = gfn_to_rmap(gfn, sp->role.level, slot);
+ */
 static struct kvm_rmap_head *gfn_to_rmap(gfn_t gfn, int level,
 					 const struct kvm_memory_slot *slot)
 {
 	unsigned long idx;
 
+	/*
+	 * KVM_NR_PAGE_SIZES应该是3
+	 *
+	 * struct kvm_memory_slot *slot:
+	 * -> struct kvm_arch_memory_slot arch;
+	 *    -> struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
+	 *    -> struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+	 *    -> unsigned short *gfn_write_track;
+	 */
 	idx = gfn_to_index(gfn, slot->base_gfn, level);
 	return &slot->arch.rmap[level - PG_LEVEL_4K][idx];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1261| <<drop_spte>> rmap_remove(kvm, sptep);
+ *
+ * 假设sptp是一个指针, 指向一个entry, entry的内容是*spte
+ * 把这个entry对应的下一级的gfn的对应的所有rmap清空
+ */
 static void rmap_remove(struct kvm *kvm, u64 *spte)
 {
 	struct kvm_memslots *slots;
@@ -1095,6 +1499,26 @@ static void rmap_remove(struct kvm *kvm, u64 *spte)
 	struct kvm_rmap_head *rmap_head;
 
 	sp = sptep_to_sp(spte);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|291| <<kvm_flush_remote_tlbs_sptep>> gfn_t gfn = kvm_mmu_page_get_gfn(sp, spte_index(sptep));
+	 *   - arch/x86/kvm/mmu/mmu.c|791| <<kvm_mmu_page_set_translation>> WARN_ONCE(gfn != kvm_mmu_page_get_gfn(sp, index),
+	 *   - arch/x86/kvm/mmu/mmu.c|794| <<kvm_mmu_page_set_translation>> sp->gfn, kvm_mmu_page_get_gfn(sp, index), gfn);
+	 *   - arch/x86/kvm/mmu/mmu.c|800| <<kvm_mmu_page_set_access>> gfn_t gfn = kvm_mmu_page_get_gfn(sp, index);
+	 *   - arch/x86/kvm/mmu/mmu.c|1201| <<rmap_remove>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));
+	 *   - arch/x86/kvm/mmu/mmu.c|1873| <<kvm_mmu_check_sptes_at_free>> kvm_mmu_page_get_gfn(sp, i));
+	 *   - arch/x86/kvm/mmu/mmu.c|3209| <<direct_pte_prefetch_many>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(start));
+	 *   - arch/x86/kvm/mmu/mmu.c|6729| <<shadow_mmu_get_sp_for_split>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+	 *   - arch/x86/kvm/mmu/mmu.c|6765| <<shadow_mmu_split_huge_page>> gfn = kvm_mmu_page_get_gfn(sp, index);
+	 *   - arch/x86/kvm/mmu/mmu.c|6804| <<shadow_mmu_try_split_huge_page>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|949| <<FNAME>> gfn != kvm_mmu_page_get_gfn(sp, i)) {
+	 *
+	 * 给出一个kvm_mmu_page, 这个page存了一个页表的页, 里面有512个entry
+	 * 每个entry都代表了一个起始的gfn
+	 * sp->gfn本身是这个页表的第一个entry指向的gfn
+	 * kvm_mmu_page_get_gfn()返回参数index代表的entry所指向的gfn
+	 * index是页表中entry的index
+	 */
 	gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));
 
 	/*
@@ -1127,20 +1551,51 @@ struct rmap_iterator {
  *
  * Returns sptep if found, NULL otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1198| <<for_each_rmap_spte>> for (_spte_ = rmap_get_first(_rmap_head_, _iter_); \
+ *   - arch/x86/kvm/mmu/mmu.c|2553| <<kvm_mmu_unlink_parents>> while ((sptep = rmap_get_first(&sp->parent_ptes, &iter)))
+ *
+ * 如果没有数据, 就返回NULL
+ * 如果只有一个数据, 返回这个数据rmap_head->val
+ * 如果有很多, 返回desc的第一个iter->desc->sptes[0]
+ */
 static u64 *rmap_get_first(struct kvm_rmap_head *rmap_head,
 			   struct rmap_iterator *iter)
 {
 	u64 *sptep;
 
+	/*
+	 * struct kvm_rmap_head {
+	 *     unsigned long val;
+	 * };
+	 *
+	 * 如果没有数据, 就返回NULL
+	 */
 	if (!rmap_head->val)
 		return NULL;
 
+	/*
+	 * 如果只有一个数据, 返回这个数据rmap_head->val
+	 */
 	if (!(rmap_head->val & 1)) {
 		iter->desc = NULL;
 		sptep = (u64 *)rmap_head->val;
 		goto out;
 	}
 
+	/*
+	 * struct pte_list_desc {
+	 *     struct pte_list_desc *more;
+	 *     // The number of PTEs stored in _this_ descriptor.
+	 *     u32 spte_count;
+	 *     // The number of PTEs stored in all tails of this descriptor.
+	 *     u32 tail_count;
+	 *     u64 *sptes[PTE_LIST_EXT];
+	 * };
+	 *
+	 * 如果有很多, 返回desc的第一个iter->desc->sptes[0]
+	 */
 	iter->desc = (struct pte_list_desc *)(rmap_head->val & ~1ul);
 	iter->pos = 0;
 	sptep = iter->desc->sptes[iter->pos];
@@ -1154,10 +1609,21 @@ static u64 *rmap_get_first(struct kvm_rmap_head *rmap_head,
  *
  * Returns sptep if found, NULL otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1254| <<for_each_rmap_spte>> _spte_; _spte_ = rmap_get_next(_iter_))
+ */
 static u64 *rmap_get_next(struct rmap_iterator *iter)
 {
 	u64 *sptep;
 
+	/*
+	 * struct rmap_iterator {
+	 *     // private fields
+	 *     struct pte_list_desc *desc;     // holds the sptep if not NULL
+	 *     int pos;                        // index of the sptep
+	 * };
+	 */
 	if (iter->desc) {
 		if (iter->pos < PTE_LIST_EXT - 1) {
 			++iter->pos;
@@ -1182,18 +1648,49 @@ static u64 *rmap_get_next(struct rmap_iterator *iter)
 	return sptep;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1312| <<rmap_write_protect>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu/mmu.c|1355| <<__rmap_clear_dirty>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu/mmu.c|1537| <<kvm_set_pte_rmap>> for_each_rmap_spte(rmap_head, &iter, sptep) {
+ *   - arch/x86/kvm/mmu/mmu.c|1686| <<kvm_age_rmap>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu/mmu.c|1699| <<kvm_test_age_rmap>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu/mmu.c|1855| <<kvm_mmu_mark_parents_unsync>> for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
+ *   - arch/x86/kvm/mmu/mmu.c|6744| <<shadow_mmu_try_split_huge_pages>> for_each_rmap_spte(rmap_head, &iter, huge_sptep) {
+ *   - arch/x86/kvm/mmu/mmu.c|6859| <<kvm_mmu_zap_collapsible_spte>> for_each_rmap_spte(rmap_head, &iter, sptep) {
+ *
+ * 对于kvm_rmap_head中的每一个spte
+ */
 #define for_each_rmap_spte(_rmap_head_, _iter_, _spte_)			\
 	for (_spte_ = rmap_get_first(_rmap_head_, _iter_);		\
 	     _spte_; _spte_ = rmap_get_next(_iter_))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1271| <<drop_large_spte>> drop_spte(kvm, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|2581| <<mmu_page_zap_pte>> drop_spte(kvm, spte);
+ *   - arch/x86/kvm/mmu/mmu.c|3065| <<mmu_set_spte>> drop_spte(vcpu->kvm, sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|169| <<FNAME(prefetch_invalid_gpte)>> drop_spte(vcpu->kvm, spte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|950| <<FNAME(sync_spte)>> drop_spte(vcpu->kvm, &sp->spt[i]);
+ */
 static void drop_spte(struct kvm *kvm, u64 *sptep)
 {
+	/*
+	 * 假设sptep是一个指针, 指向一个entry, entry的内容是*sptep
+	 */
 	u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
 
+	/*
+	 * 应该是把这个spte对应的gfn的rmap全部删除
+	 */
 	if (is_shadow_present_pte(old_spte))
 		rmap_remove(kvm, sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2516| <<__link_shadow_page>> drop_large_spte(kvm, sptep, flush);
+ */
 static void drop_large_spte(struct kvm *kvm, u64 *sptep, bool flush)
 {
 	struct kvm_mmu_page *sp;
@@ -1220,6 +1717,10 @@ static void drop_large_spte(struct kvm *kvm, u64 *sptep, bool flush)
  *
  * Return true if tlb need be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1313| <<rmap_write_protect>> flush |= spte_write_protect(sptep, pt_protect);
+ */
 static bool spte_write_protect(u64 *sptep, bool pt_protect)
 {
 	u64 spte = *sptep;
@@ -1235,6 +1736,12 @@ static bool spte_write_protect(u64 *sptep, bool pt_protect)
 	return mmu_spte_update(sptep, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1389| <<kvm_mmu_write_protect_pt_masked>> rmap_write_protect(rmap_head, false);
+ *   - arch/x86/kvm/mmu/mmu.c|1491| <<kvm_mmu_slot_gfn_write_protect>> write_protected |= rmap_write_protect(rmap_head, true);
+ *   - arch/x86/kvm/mmu/mmu.c|6548| <<slot_rmap_write_protect>> return rmap_write_protect(rmap_head, false);
+ */
 static bool rmap_write_protect(struct kvm_rmap_head *rmap_head,
 			       bool pt_protect)
 {
@@ -1242,25 +1749,64 @@ static bool rmap_write_protect(struct kvm_rmap_head *rmap_head,
 	struct rmap_iterator iter;
 	bool flush = false;
 
+	/*
+	 * 对于kvm_rmap_head中的每一个spte
+	 */
 	for_each_rmap_spte(rmap_head, &iter, sptep)
 		flush |= spte_write_protect(sptep, pt_protect);
 
 	return flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1359| <<__rmap_clear_dirty>> flush |= spte_clear_dirty(sptep);
+ */
 static bool spte_clear_dirty(u64 *sptep)
 {
 	u64 spte = *sptep;
 
+	/*
+	 * TDP SPTES (more specifically, EPT SPTEs) may not have A/D bits, and may also
+	 * be restricted to using write-protection (for L2 when CPU dirty logging, i.e.
+	 * PML, is enabled).  Use bits 52 and 53 to hold the type of A/D tracking that
+	 * is must be employed for a given TDP SPTE.
+	 *
+	 * Note, the "enabled" mask must be '0', as bits 62:52 are _reserved_ for PAE
+	 * paging, including NPT PAE.  This scheme works because legacy shadow paging
+	 * is guaranteed to have A/D bits and write-protection is forced only for
+	 * TDP with CPU dirty logging (PML).  If NPT ever gains PML-like support, it
+	 * must be restricted to 64-bit KVM.
+	 *
+	 * 如果spte的SPTE_SPECIAL_MASK=1<<52设置了, 说明ad不支持, 没enabled, 返回false
+	 */
 	KVM_MMU_WARN_ON(!spte_ad_enabled(spte));
 	spte &= ~shadow_dirty_mask;
 	return mmu_spte_update(sptep, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1357| <<__rmap_clear_dirty>> flush |= spte_wrprot_for_clear_dirty(sptep);
+ */
 static bool spte_wrprot_for_clear_dirty(u64 *sptep)
 {
 	bool was_writable = test_and_clear_bit(PT_WRITABLE_SHIFT,
 					       (unsigned long *)sptep);
+	/*
+	 * TDP SPTES (more specifically, EPT SPTEs) may not have A/D bits, and may also
+	 * be restricted to using write-protection (for L2 when CPU dirty logging, i.e.
+	 * PML, is enabled).  Use bits 52 and 53 to hold the type of A/D tracking that
+	 * is must be employed for a given TDP SPTE.
+	 *
+	 * Note, the "enabled" mask must be '0', as bits 62:52 are _reserved_ for PAE
+	 * paging, including NPT PAE.  This scheme works because legacy shadow paging
+	 * is guaranteed to have A/D bits and write-protection is forced only for
+	 * TDP with CPU dirty logging (PML).  If NPT ever gains PML-like support, it
+	 * must be restricted to 64-bit KVM.
+	 *
+	 * 如果spte的SPTE_SPECIAL_MASK=1<<52设置了, 说明ad不支持, 没enabled, 返回false
+	 */
 	if (was_writable && !spte_ad_enabled(*sptep))
 		kvm_set_pfn_dirty(spte_to_pfn(*sptep));
 
@@ -1273,6 +1819,11 @@ static bool spte_wrprot_for_clear_dirty(u64 *sptep)
  *	- W bit on ad-disabled SPTEs.
  * Returns true iff any D or W bits were cleared.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1362| <<kvm_mmu_clear_dirty_pt_masked>> __rmap_clear_dirty(kvm, rmap_head, slot);
+ *   - arch/x86/kvm/mmu/mmu.c|6830| <<kvm_mmu_slot_leaf_clear_dirty>> walk_slot_rmaps_4k(kvm, memslot, __rmap_clear_dirty, false);
+ */
 static bool __rmap_clear_dirty(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 			       const struct kvm_memory_slot *slot)
 {
@@ -1280,6 +1831,9 @@ static bool __rmap_clear_dirty(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 	struct rmap_iterator iter;
 	bool flush = false;
 
+	/*
+	 * 对于kvm_rmap_head中的每一个spte
+	 */
 	for_each_rmap_spte(rmap_head, &iter, sptep)
 		if (spte_ad_need_write_protect(*sptep))
 			flush |= spte_wrprot_for_clear_dirty(sptep);
@@ -1298,6 +1852,10 @@ static bool __rmap_clear_dirty(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
  *
  * Used when we do not need to care about huge page mappings.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1600| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);
+ */
 static void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
 				     struct kvm_memory_slot *slot,
 				     gfn_t gfn_offset, unsigned long mask)
@@ -1364,6 +1922,12 @@ static void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
  * We need to care about huge page mappings: e.g. during dirty logging we may
  * have such mappings.
  */
+/*
+ * called by:
+ *   - virt/kvm/dirty_ring.c|70| <<kvm_reset_dirty_gfn>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ *   - virt/kvm/kvm_main.c|2209| <<kvm_get_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ *   - virt/kvm/kvm_main.c|2326| <<kvm_clear_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ */
 void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 				struct kvm_memory_slot *slot,
 				gfn_t gfn_offset, unsigned long mask)
@@ -1405,6 +1969,14 @@ int kvm_cpu_dirty_log_size(void)
 	return kvm_x86_ops.cpu_dirty_log_size;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1073| <<account_shadowed>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+ *   - arch/x86/kvm/mmu/mmu.c|1860| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, start, PG_LEVEL_2M);
+ *   - arch/x86/kvm/mmu/mmu.c|1865| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, end, PG_LEVEL_2M);
+ *   - arch/x86/kvm/mmu/mmu.c|1908| <<kvm_vcpu_write_protect_gfn>> return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn, PG_LEVEL_4K);
+ *   - arch/x86/kvm/mmu/page_track.c|96| <<__kvm_write_track_add_gfn>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+ */
 bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 				    struct kvm_memory_slot *slot, u64 gfn,
 				    int min_level)
@@ -1427,6 +1999,10 @@ bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 	return write_protected;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2902| <<mmu_sync_children>> protected |= kvm_vcpu_write_protect_gfn(vcpu, sp->gfn);
+ */
 static bool kvm_vcpu_write_protect_gfn(struct kvm_vcpu *vcpu, u64 gfn)
 {
 	struct kvm_memory_slot *slot;
@@ -1502,6 +2078,11 @@ struct slot_rmap_walk_iterator {
 	struct kvm_rmap_head *end_rmap;
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1770| <<slot_rmap_walk_init>> rmap_walk_init_level(iterator, iterator->start_level);
+ *   - arch/x86/kvm/mmu/mmu.c|1800| <<slot_rmap_walk_next>> rmap_walk_init_level(iterator, iterator->level);
+ */
 static void rmap_walk_init_level(struct slot_rmap_walk_iterator *iterator,
 				 int level)
 {
@@ -1511,6 +2092,10 @@ static void rmap_walk_init_level(struct slot_rmap_walk_iterator *iterator,
 	iterator->end_rmap = gfn_to_rmap(iterator->end_gfn, level, iterator->slot);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1810| <<for_each_slot_rmap_range>> for (slot_rmap_walk_init(_iter_, _slot_, _start_level_, \
+ */
 static void slot_rmap_walk_init(struct slot_rmap_walk_iterator *iterator,
 				const struct kvm_memory_slot *slot,
 				int start_level, int end_level,
@@ -1522,14 +2107,27 @@ static void slot_rmap_walk_init(struct slot_rmap_walk_iterator *iterator,
 	iterator->start_gfn = start_gfn;
 	iterator->end_gfn = end_gfn;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1770| <<slot_rmap_walk_init>> rmap_walk_init_level(iterator, iterator->start_level);
+	 *   - arch/x86/kvm/mmu/mmu.c|1800| <<slot_rmap_walk_next>> rmap_walk_init_level(iterator, iterator->level);
+	 */
 	rmap_walk_init_level(iterator, iterator->start_level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1799| <<for_each_slot_rmap_range>> slot_rmap_walk_okay(_iter_); \
+ */
 static bool slot_rmap_walk_okay(struct slot_rmap_walk_iterator *iterator)
 {
 	return !!iterator->rmap;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1800| <<for_each_slot_rmap_range>> slot_rmap_walk_next(_iter_))
+ */
 static void slot_rmap_walk_next(struct slot_rmap_walk_iterator *iterator)
 {
 	while (++iterator->rmap <= iterator->end_rmap) {
@@ -1547,6 +2145,13 @@ static void slot_rmap_walk_next(struct slot_rmap_walk_iterator *iterator)
 	rmap_walk_init_level(iterator, iterator->level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1813| <<kvm_handle_gfn_range>> for_each_slot_rmap_range(range->slot, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, range->start, range->end - 1, &iterator)
+ *   - arch/x86/kvm/mmu/mmu.c|6387| <<__walk_slot_rmaps>> for_each_slot_rmap_range(slot, start_level, end_level, start_gfn, end_gfn, &iterator) {
+ *
+ * 对于一个slot的每一个level的每一个gfn的rmap
+ */
 #define for_each_slot_rmap_range(_slot_, _start_level_, _end_level_,	\
 	   _start_gfn, _end_gfn, _iter_)				\
 	for (slot_rmap_walk_init(_iter_, _slot_, _start_level_,		\
@@ -1558,6 +2163,13 @@ typedef bool (*rmap_handler_t)(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 			       struct kvm_memory_slot *slot, gfn_t gfn,
 			       int level, pte_t pte);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1826| <<kvm_unmap_gfn_range>> flush = kvm_handle_gfn_range(kvm, range, kvm_zap_rmap);
+ *   - arch/x86/kvm/mmu/mmu.c|1843| <<kvm_set_spte_gfn>> flush = kvm_handle_gfn_range(kvm, range, kvm_set_pte_rmap);
+ *   - arch/x86/kvm/mmu/mmu.c|1922| <<kvm_age_gfn>> young = kvm_handle_gfn_range(kvm, range, kvm_age_rmap);
+ *   - arch/x86/kvm/mmu/mmu.c|1939| <<kvm_test_age_gfn>> young = kvm_handle_gfn_range(kvm, range, kvm_test_age_rmap);
+ */
 static __always_inline bool kvm_handle_gfn_range(struct kvm *kvm,
 						 struct kvm_gfn_range *range,
 						 rmap_handler_t handler)
@@ -1565,6 +2177,9 @@ static __always_inline bool kvm_handle_gfn_range(struct kvm *kvm,
 	struct slot_rmap_walk_iterator iterator;
 	bool ret = false;
 
+	/*
+	 * 对于一个slot的每一个level的每一个gfn的rmap
+	 */
 	for_each_slot_rmap_range(range->slot, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL,
 				 range->start, range->end - 1, &iterator)
 		ret |= handler(kvm, iterator.rmap, range->slot, iterator.gfn,
@@ -1573,6 +2188,24 @@ static __always_inline bool kvm_handle_gfn_range(struct kvm *kvm,
 	return ret;
 }
 
+/*
+ * 768 static int kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
+ * 769                                         const struct mmu_notifier_range *range)
+ * 770 {       
+ * 771         struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ * 772         const struct kvm_hva_range hva_range = {
+ * 773                 .start          = range->start,
+ * 774                 .end            = range->end,
+ * 775                 .handler        = kvm_unmap_gfn_range,
+ * 776                 .on_lock        = kvm_mmu_invalidate_begin,
+ * 777                 .on_unlock      = kvm_arch_guest_memory_reclaimed,
+ * 778                 .flush_on_ret   = true,
+ * 779                 .may_block      = mmu_notifier_range_blockable(range),
+ * 780         };
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|775| <<kvm_mmu_notifier_invalidate_range_start>> .handler = kvm_unmap_gfn_range,
+ */
 bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	bool flush = false;
@@ -1590,6 +2223,19 @@ bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 	return flush;
 }
 
+/*
+ * struct kvm_gfn_range {
+ *     struct kvm_memory_slot *slot;
+ *     gfn_t start; 
+ *     gfn_t end;
+ *     union kvm_mmu_notifier_arg arg;
+ *     -> pte_t pte;
+ *     bool may_block; 
+ * };
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|711| <<kvm_change_spte_gfn>> return kvm_set_spte_gfn(kvm, range);
+ */
 bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	bool flush = false;
@@ -1603,6 +2249,10 @@ bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 	return flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1922| <<kvm_age_gfn>> young = kvm_handle_gfn_range(kvm, range, kvm_age_rmap);
+ */
 static bool kvm_age_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 			 struct kvm_memory_slot *slot, gfn_t gfn, int level,
 			 pte_t unused)
@@ -1617,6 +2267,10 @@ static bool kvm_age_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 	return young;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1939| <<kvm_test_age_gfn>> young = kvm_handle_gfn_range(kvm, range, kvm_test_age_rmap);
+ */
 static bool kvm_test_age_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 			      struct kvm_memory_slot *slot, gfn_t gfn,
 			      int level, pte_t unused)
@@ -1624,6 +2278,9 @@ static bool kvm_test_age_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 	u64 *sptep;
 	struct rmap_iterator iter;
 
+	/*
+	 * 对于kvm_rmap_head中的每一个spte
+	 */
 	for_each_rmap_spte(rmap_head, &iter, sptep)
 		if (is_accessed_spte(*sptep))
 			return true;
@@ -1632,6 +2289,11 @@ static bool kvm_test_age_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 
 #define RMAP_RECYCLE_THRESHOLD 1000
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1909| <<rmap_add>> __rmap_add(vcpu->kvm, cache, slot, spte, gfn, access);
+ *   - arch/x86/kvm/mmu/mmu.c|6875| <<shadow_mmu_split_huge_page>> __rmap_add(kvm, cache, slot, sptep, gfn, sp->role.access);
+ */
 static void __rmap_add(struct kvm *kvm,
 		       struct kvm_mmu_memory_cache *cache,
 		       const struct kvm_memory_slot *slot,
@@ -1656,14 +2318,28 @@ static void __rmap_add(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3277| <<mmu_set_spte>> rmap_add(vcpu, slot, sptep, gfn, pte_access);
+ */
 static void rmap_add(struct kvm_vcpu *vcpu, const struct kvm_memory_slot *slot,
 		     u64 *spte, gfn_t gfn, unsigned int access)
 {
 	struct kvm_mmu_memory_cache *cache = &vcpu->arch.mmu_pte_list_desc_cache;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1909| <<rmap_add>> __rmap_add(vcpu->kvm, cache, slot, spte, gfn, access);
+	 *   - arch/x86/kvm/mmu/mmu.c|6875| <<shadow_mmu_split_huge_page>> __rmap_add(kvm, cache, slot, sptep, gfn, sp->role.access);
+	 */
 	__rmap_add(vcpu->kvm, cache, slot, spte, gfn, access);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|872| <<kvm_mmu_notifier_clear_flush_young>>  return kvm_handle_hva_range(mn, start, end, KVM_MMU_NOTIFIER_NO_ARG, kvm_age_gfn);
+ *   - virt/kvm/kvm_main.c|895| <<kvm_mmu_notifier_clear_young>> return kvm_handle_hva_range_no_flush(mn, start, end, kvm_age_gfn);
+ */
 bool kvm_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	bool young = false;
@@ -1677,6 +2353,18 @@ bool kvm_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 	return young;
 }
 
+/*
+ * struct kvm_gfn_range {
+ *     struct kvm_memory_slot *slot;
+ *     gfn_t start;
+ *     gfn_t end;
+ *     union kvm_mmu_notifier_arg arg;
+ *     bool may_block;
+ * };
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|905| <<kvm_mmu_notifier_test_young>> return kvm_handle_hva_range_no_flush(mn, address, address + 1, kvm_test_age_gfn);
+ */
 bool kvm_test_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	bool young = false;
@@ -1690,6 +2378,10 @@ bool kvm_test_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 	return young;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1933| <<kvm_mmu_free_shadow_page>> kvm_mmu_check_sptes_at_free(sp);
+ */
 static void kvm_mmu_check_sptes_at_free(struct kvm_mmu_page *sp)
 {
 #ifdef CONFIG_KVM_PROVE_MMU
@@ -1740,26 +2432,62 @@ static void kvm_mmu_free_shadow_page(struct kvm_mmu_page *sp)
 	kmem_cache_free(mmu_page_header_cache, sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2517| <<for_each_gfn_valid_sp_with_gptes>> for_each_valid_sp(_kvm, _sp, &(_kvm)->arch.mmu_page_hash[kvm_page_table_hashfn(_gfn)]) \
+ *   - arch/x86/kvm/mmu/mmu.c|2988| <<__kvm_mmu_get_shadow_page>> sp_list = &kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)];
+ */
 static unsigned kvm_page_table_hashfn(gfn_t gfn)
 {
 	return hash_64(gfn, KVM_MMU_HASH_SHIFT);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3212| <<__link_shadow_page>> mmu_page_add_parent_pte(cache, sp, sptep);
+ */
 static void mmu_page_add_parent_pte(struct kvm_mmu_memory_cache *cache,
 				    struct kvm_mmu_page *sp, u64 *parent_pte)
 {
 	if (!parent_pte)
 		return;
 
+	/*
+	 * 在以下使用kvm_mmu_page->parent_ptes:
+	 *   - arch/x86/kvm/mmu/mmu.c|2324| <<mmu_page_add_parent_pte>> pte_list_add(cache, parent_pte, &sp->parent_ptes);
+	 *   - arch/x86/kvm/mmu/mmu.c|2330| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+	 *   - arch/x86/kvm/mmu/mmu.c|2361| <<kvm_mmu_mark_parents_unsync>> for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
+	 *   - arch/x86/kvm/mmu/mmu.c|3288| <<mmu_page_zap_pte>> if (tdp_enabled && invalid_list && child->role.guest_mode && !child->parent_ptes.val)
+	 *   - arch/x86/kvm/mmu/mmu.c|3331| <<kvm_mmu_unlink_parents>> while ((sptep = rmap_get_first(&sp->parent_ptes, &iter)))
+	 */
 	pte_list_add(cache, parent_pte, &sp->parent_ptes);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2343| <<drop_parent_pte>> mmu_page_remove_parent_pte(kvm, sp, parent_pte);
+ */
 static void mmu_page_remove_parent_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 				       u64 *parent_pte)
 {
+	/*
+	 * 在以下使用kvm_mmu_page->parent_ptes:
+	 *   - arch/x86/kvm/mmu/mmu.c|2324| <<mmu_page_add_parent_pte>> pte_list_add(cache, parent_pte, &sp->parent_ptes);
+	 *   - arch/x86/kvm/mmu/mmu.c|2330| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+	 *   - arch/x86/kvm/mmu/mmu.c|2361| <<kvm_mmu_mark_parents_unsync>> for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
+	 *   - arch/x86/kvm/mmu/mmu.c|3288| <<mmu_page_zap_pte>> if (tdp_enabled && invalid_list && child->role.guest_mode && !child->parent_ptes.val)
+	 *   - arch/x86/kvm/mmu/mmu.c|3331| <<kvm_mmu_unlink_parents>> while ((sptep = rmap_get_first(&sp->parent_ptes, &iter)))
+	 */
 	pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2493| <<validate_direct_spte>> drop_parent_pte(vcpu->kvm, child, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|2517| <<mmu_page_zap_pte>> drop_parent_pte(kvm, child, spte);
+ *   - arch/x86/kvm/mmu/mmu.c|2554| <<kvm_mmu_unlink_parents>> drop_parent_pte(kvm, sp, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|2969| <<mmu_set_spte>> drop_parent_pte(vcpu->kvm, child, sptep);
+ */
 static void drop_parent_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 			    u64 *parent_pte)
 {
@@ -1768,25 +2496,79 @@ static void drop_parent_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 }
 
 static void mark_unsync(u64 *spte);
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2162| <<mark_unsync>> kvm_mmu_mark_parents_unsync(sp);
+ *   - arch/x86/kvm/mmu/mmu.c|3220| <<kvm_unsync_page>> kvm_mmu_mark_parents_unsync(sp);
+ *
+ * 为每个指向这个sp的pte进行mark_unsync()
+ */
 static void kvm_mmu_mark_parents_unsync(struct kvm_mmu_page *sp)
 {
 	u64 *sptep;
 	struct rmap_iterator iter;
 
+	/*
+	 * 对于kvm_rmap_head中的每一个spte
+	 */
 	for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
 		mark_unsync(sptep);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2149| <<kvm_mmu_mark_parents_unsync>> mark_unsync(sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|2842| <<__link_shadow_page>> mark_unsync(sptep);
+ */
 static void mark_unsync(u64 *spte)
 {
 	struct kvm_mmu_page *sp;
 
 	sp = sptep_to_sp(spte);
+	/*
+	 * 在以下使用kvm_mmu_page->unsync_child_bitmap:
+	 *   - arch/x86/kvm/mmu/mmu.c|2205| <<mark_unsync>> if (__test_and_set_bit(spte_index(spte), sp->unsync_child_bitmap))
+	 *   - arch/x86/kvm/mmu/mmu.c|2242| <<clear_unsync_child_bit>> __clear_bit(idx, sp->unsync_child_bitmap);
+	 *   - arch/x86/kvm/mmu/mmu.c|2250| <<__mmu_unsync_walk>> for_each_set_bit(i, sp->unsync_child_bitmap, 512) {
+	 *
+	 * unsync_children: 页表页中unsync的pte数
+	 * unsync: 用于最后一级页表页,表示该页的页表项(pte)是否与guest同步(guest是否已更新tlb)
+	 * unsync_child_bitmap: 页表页中unsync的spte bitmap
+	 */
 	if (__test_and_set_bit(spte_index(spte), sp->unsync_child_bitmap))
 		return;
+	/*
+	 * 在以下设置unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|2160| <<mark_unsync>> if (sp->unsync_children++)
+	 *   - arch/x86/kvm/mmu/mmu.c|2193| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 * 在以下使用unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|2194| <<clear_unsync_child_bit>> WARN_ON_ONCE((int )sp->unsync_children < 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|2214| <<__mmu_unsync_walk>> if (child->unsync_children) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2243| <<mmu_unsync_walk>> if (!sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/mmu.c|2471| <<mmu_pages_clear_parents>> } while (!sp->unsync_children);
+	 *   - arch/x86/kvm/mmu/mmu.c|2841| <<__link_shadow_page>> if (WARN_ON_ONCE(sp->unsync_children) || sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|4445| <<is_unsync_root>> if (sp->unsync || sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/mmu.c|6388| <<__kvm_mmu_invalidate_addr>> if (!sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|694| <<FNAME(fetch)>> if (sp->unsync_children &&
+	 */
+	/*
+	 * 在以下设置unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|2160| <<mark_unsync>> if (sp->unsync_children++)
+	 *   - arch/x86/kvm/mmu/mmu.c|2193| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 *
+	 * 猜测: 如果这个sp已经有children被unsync过了,
+	 * 说明parent也早unsync过了
+	 */
 	if (sp->unsync_children++)
 		return;
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2162| <<mark_unsync>> kvm_mmu_mark_parents_unsync(sp);
+	 *   - arch/x86/kvm/mmu/mmu.c|3220| <<kvm_unsync_page>> kvm_mmu_mark_parents_unsync(sp);
+	 *
+	 * 为每个指向这个sp的pte进行mark_unsync()
+	 */
 	kvm_mmu_mark_parents_unsync(sp);
 }
 
@@ -1800,11 +2582,38 @@ struct kvm_mmu_pages {
 	unsigned int nr;
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2482| <<__mmu_unsync_walk>> if (mmu_pages_add(pvec, child, i))
+ *   - arch/x86/kvm/mmu/mmu.c|2495| <<__mmu_unsync_walk>> if (mmu_pages_add(pvec, child, i))
+ *   - arch/x86/kvm/mmu/mmu.c|2513| <<mmu_unsync_walk>> mmu_pages_add(pvec, sp, INVALID_INDEX);
+ *
+ * 可能pvec已经有了, 也可能要加入新的
+ * 只要执行结束后满了返回1, 没满返回0
+ */
 static int mmu_pages_add(struct kvm_mmu_pages *pvec, struct kvm_mmu_page *sp,
 			 int idx)
 {
 	int i;
 
+	/*
+	 * 在以下使用kvm_mmu_page->unsync:
+	 *   - arch/x86/kvm/mmu/mmu.c|2227| <<mmu_pages_add>> if (sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|2273| <<__mmu_unsync_walk>> } else if (child->unsync) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2299| <<kvm_unlink_unsync_page>> WARN_ON_ONCE(!sp->unsync);
+	 *   - arch/x86/kvm/mmu/mmu.c|2301| <<kvm_unlink_unsync_page>> sp->unsync = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|2657| <<kvm_mmu_find_shadow_page>> if (role.level > PG_LEVEL_4K && sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|2667| <<kvm_mmu_find_shadow_page>> if (sp->unsync) {
+	 *   - arch/x86/kvm/mmu/mmu.c|3016| <<__link_shadow_page>> if (WARN_ON_ONCE(sp->unsync_children) || sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|3183| <<__kvm_mmu_prepare_zap_page>> if (sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|3403| <<kvm_unsync_page>> sp->unsync = 1;
+	 *   - arch/x86/kvm/mmu/mmu.c|3442| <<mmu_try_to_unsync_pages>> if (sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|3467| <<mmu_try_to_unsync_pages>> if (READ_ONCE(sp->unsync))
+	 *   - arch/x86/kvm/mmu/mmu.c|4681| <<is_unsync_root>> if (sp->unsync || sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/mmu.c|6629| <<__kvm_mmu_invalidate_addr>> if (sp->unsync) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7340| <<shadow_mmu_try_split_huge_pages>> if (WARN_ON_ONCE(sp->unsync))
+	 *   - arch/x86/kvm/mmu/mmutrace.h|23| <<KVM_MMU_PAGE_ASSIGN>> __entry->unsync = sp->unsync;
+	 */
 	if (sp->unsync)
 		for (i=0; i < pvec->nr; i++)
 			if (pvec->page[i].sp == sp)
@@ -1816,18 +2625,45 @@ static int mmu_pages_add(struct kvm_mmu_pages *pvec, struct kvm_mmu_page *sp,
 	return (pvec->nr == KVM_PAGE_ARRAY_NR);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2475| <<__mmu_unsync_walk>> clear_unsync_child_bit(sp, i);
+ *   - arch/x86/kvm/mmu/mmu.c|2487| <<__mmu_unsync_walk>> clear_unsync_child_bit(sp, i);
+ *   - arch/x86/kvm/mmu/mmu.c|2498| <<__mmu_unsync_walk>> clear_unsync_child_bit(sp, i);
+ *   - arch/x86/kvm/mmu/mmu.c|2795| <<mmu_pages_clear_parents>> clear_unsync_child_bit(sp, idx);
+ */
 static inline void clear_unsync_child_bit(struct kvm_mmu_page *sp, int idx)
 {
+	/*
+	 * 在以下设置unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|2160| <<mark_unsync>> if (sp->unsync_children++)
+	 *   - arch/x86/kvm/mmu/mmu.c|2193| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 */
 	--sp->unsync_children;
 	WARN_ON_ONCE((int)sp->unsync_children < 0);
 	__clear_bit(idx, sp->unsync_child_bitmap);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2485| <<__mmu_unsync_walk>> ret = __mmu_unsync_walk(child, pvec);
+ *   - arch/x86/kvm/mmu/mmu.c|2514| <<mmu_unsync_walk>> return __mmu_unsync_walk(sp, pvec);
+ *
+ * kvm_mmu_pages临时满了会返回-ENOSPC
+ * 感觉__mmu_unsync_walk唯一返回0的机会就是其(当前sp)下面都clear unsync了,
+ * 也就是clear_unsync_child_bit(sp, i)
+ */
 static int __mmu_unsync_walk(struct kvm_mmu_page *sp,
 			   struct kvm_mmu_pages *pvec)
 {
 	int i, ret, nr_unsync_leaf = 0;
 
+	/*
+	 * 在以下使用kvm_mmu_page->unsync_child_bitmap:
+	 *   - arch/x86/kvm/mmu/mmu.c|2205| <<mark_unsync>> if (__test_and_set_bit(spte_index(spte), sp->unsync_child_bitmap))
+	 *   - arch/x86/kvm/mmu/mmu.c|2242| <<clear_unsync_child_bit>> __clear_bit(idx, sp->unsync_child_bitmap);
+	 *   - arch/x86/kvm/mmu/mmu.c|2250| <<__mmu_unsync_walk>> for_each_set_bit(i, sp->unsync_child_bitmap, 512) {
+	 */
 	for_each_set_bit(i, sp->unsync_child_bitmap, 512) {
 		struct kvm_mmu_page *child;
 		u64 ent = sp->spt[i];
@@ -1839,7 +2675,29 @@ static int __mmu_unsync_walk(struct kvm_mmu_page *sp,
 
 		child = spte_to_child_sp(ent);
 
+		/*
+		 * 在以下设置unsync_children:
+		 *   - arch/x86/kvm/mmu/mmu.c|2160| <<mark_unsync>> if (sp->unsync_children++)
+		 *   - arch/x86/kvm/mmu/mmu.c|2193| <<clear_unsync_child_bit>> --sp->unsync_children;
+		 * 在以下使用unsync_children:
+		 *   - arch/x86/kvm/mmu/mmu.c|2194| <<clear_unsync_child_bit>> WARN_ON_ONCE((int )sp->unsync_children < 0);
+		 *   - arch/x86/kvm/mmu/mmu.c|2214| <<__mmu_unsync_walk>> if (child->unsync_children) {
+		 *   - arch/x86/kvm/mmu/mmu.c|2243| <<mmu_unsync_walk>> if (!sp->unsync_children)
+		 *   - arch/x86/kvm/mmu/mmu.c|2471| <<mmu_pages_clear_parents>> } while (!sp->unsync_children);
+		 *   - arch/x86/kvm/mmu/mmu.c|2841| <<__link_shadow_page>> if (WARN_ON_ONCE(sp->unsync_children) || sp->unsync)
+		 *   - arch/x86/kvm/mmu/mmu.c|4445| <<is_unsync_root>> if (sp->unsync || sp->unsync_children)
+		 *   - arch/x86/kvm/mmu/mmu.c|6388| <<__kvm_mmu_invalidate_addr>> if (!sp->unsync_children)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|694| <<FNAME(fetch)>> if (sp->unsync_children &&
+		 *
+		 * 页表页中unsync的pte数
+		 */
 		if (child->unsync_children) {
+			/*
+			 * 可能pvec已经有了, 也可能要加入新的
+			 * 只要执行结束后满了返回1, 没满返回0
+			 *
+			 * 把child加进去!
+			 */
 			if (mmu_pages_add(pvec, child, i))
 				return -ENOSPC;
 
@@ -1852,7 +2710,14 @@ static int __mmu_unsync_walk(struct kvm_mmu_page *sp,
 			} else
 				return ret;
 		} else if (child->unsync) {
+			/*
+			 * child->unsync是用于最后一级页表页,表示该页的页表项(pte)是否与guest同步(guest是否已更新tlb)
+			 */
 			nr_unsync_leaf++;
+			/*
+			 * 可能pvec已经有了, 也可能要加入新的
+			 * 只要执行结束后满了返回1, 没满返回0
+			 */
 			if (mmu_pages_add(pvec, child, i))
 				return -ENOSPC;
 		} else
@@ -1864,19 +2729,63 @@ static int __mmu_unsync_walk(struct kvm_mmu_page *sp,
 
 #define INVALID_INDEX (-1)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2816| <<mmu_sync_children>> while (mmu_unsync_walk(parent, &pages)) {
+ *   - arch/x86/kvm/mmu/mmu.c|3410| <<mmu_zap_unsync_children>> while (mmu_unsync_walk(parent, &pages)) {
+ *
+ * 如果当前sp都不是unsync了直接返回0,
+ * 否则把自己加入kvm_mmu_pages,
+ * 然后用__mmu_unsync_walk(sp, pvec)把unsync的子子孙孙加入kvm_mmu_paghes:
+ * kvm_mmu_pages临时满了会返回-ENOSPC
+ * 感觉__mmu_unsync_walk唯一返回0的机会就是其(当前sp)下面都clear unsync了,
+ * 也就是clear_unsync_child_bit(sp, i)
+ */
 static int mmu_unsync_walk(struct kvm_mmu_page *sp,
 			   struct kvm_mmu_pages *pvec)
 {
 	pvec->nr = 0;
+	/*
+	 * 在以下设置unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|2160| <<mark_unsync>> if (sp->unsync_children++)
+	 *   - arch/x86/kvm/mmu/mmu.c|2193| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 *
+	 * 页表页中unsync的pte数
+	 */
 	if (!sp->unsync_children)
 		return 0;
 
+	/*
+	 * 可能pvec已经有了, 也可能要加入新的
+	 * 只要执行结束后满了返回1, 没满返回0
+	 */
 	mmu_pages_add(pvec, sp, INVALID_INDEX);
+	/*
+	 * kvm_mmu_pages临时满了会返回-ENOSPC
+	 * 感觉__mmu_unsync_walk唯一返回0的机会就是其(当前sp)下面都clear unsync了,
+	 * 也就是clear_unsync_child_bit(sp, i)
+	 */
 	return __mmu_unsync_walk(sp, pvec);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2828| <<mmu_sync_children>> kvm_unlink_unsync_page(vcpu->kvm, sp);
+ *   - arch/x86/kvm/mmu/mmu.c|3451| <<__kvm_mmu_prepare_zap_page>> kvm_unlink_unsync_page(kvm, sp);
+ */
 static void kvm_unlink_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
+	/*
+	 * 在以下设置kvm_mmu_page->unsync:
+	 *   - arch/x86/kvm/mmu/mmu.c|2301| <<kvm_unlink_unsync_page>> sp->unsync = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|3403| <<kvm_unsync_page>> sp->unsync = 1;
+	 *
+	 * kvm_unsync_page()在被mmu_try_to_unsync_pages()调用之前有一个:
+	 *   WARN_ON_ONCE(sp->role.level != PG_LEVEL_4K);
+	 *   kvm_unsync_page(kvm, sp);
+	 *
+	 * 用于最后一级页表页,表示该页的页表项(pte)是否与guest同步(guest是否已更新tlb)
+	 */
 	WARN_ON_ONCE(!sp->unsync);
 	trace_kvm_mmu_sync_page(sp);
 	sp->unsync = 0;
@@ -1888,6 +2797,14 @@ static bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 				    struct list_head *invalid_list);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|823| <<kvm_mmu_page_get_access>> if (sp_has_gptes(sp))
+ *   - arch/x86/kvm/mmu/mmu.c|849| <<kvm_mmu_page_set_translation>> if (sp_has_gptes(sp)) {
+ *   - arch/x86/kvm/mmu/mmu.c|2334| <<for_each_gfn_valid_sp_with_gptes>> if ((_sp)->gfn != (_gfn) || !sp_has_gptes(_sp)) {} else
+ *   - arch/x86/kvm/mmu/mmu.c|2740| <<kvm_mmu_alloc_shadow_page>> if (sp_has_gptes(sp))
+ *   - arch/x86/kvm/mmu/mmu.c|3166| <<__kvm_mmu_prepare_zap_page>> if (!sp->role.invalid && sp_has_gptes(sp))
+ */
 static bool sp_has_gptes(struct kvm_mmu_page *sp)
 {
 	if (sp->role.direct)
@@ -1899,16 +2816,39 @@ static bool sp_has_gptes(struct kvm_mmu_page *sp)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2340| <<for_each_gfn_valid_sp_with_gptes>> for_each_valid_sp(_kvm, _sp, \
+ *   - arch/x86/kvm/mmu/mmu.c|2641| <<kvm_mmu_find_shadow_page>> for_each_valid_sp(kvm, sp, sp_list) {
+ */
 #define for_each_valid_sp(_kvm, _sp, _list)				\
 	hlist_for_each_entry(_sp, _list, hash_link)			\
 		if (is_obsolete_sp((_kvm), (_sp))) {			\
 		} else
 
+/*
+ * 在以下使用kvm_arch->mmu_page_hash[KVM_NUM_MMU_PAGES]:
+ *   - arch/x86/kvm/mmu/mmu.c|2328| <<for_each_gfn_valid_sp_with_gptes>> &(_kvm)->arch.mmu_page_hash[kvm_page_table_hashfn(_gfn)]) \
+ *   - arch/x86/kvm/mmu/mmu.c|2753| <<__kvm_mmu_get_shadow_page>> sp_list = &kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)];
+ *
+ * 在以下调用for_each_gfn_valid_sp_with_gptes():
+ *   - arch/x86/kvm/mmu/mmu.c|4038| <<kvm_mmu_unprotect_page>> for_each_gfn_valid_sp_with_gptes(kvm, sp, gfn) {
+ *   - arch/x86/kvm/mmu/mmu.c|4128| <<mmu_try_to_unsync_pages>> for_each_gfn_valid_sp_with_gptes(kvm, sp, gfn) {
+ *   - arch/x86/kvm/mmu/mmu.c|7428| <<kvm_mmu_track_write>> for_each_gfn_valid_sp_with_gptes(vcpu->kvm, sp, gfn) {
+ *
+ * 满足下面的条件会跳过:
+ * - (_sp)->gfn != (_gfn), 或者
+ * - !sp_has_gptes(_sp) --> 是direct
+ */
 #define for_each_gfn_valid_sp_with_gptes(_kvm, _sp, _gfn)		\
 	for_each_valid_sp(_kvm, _sp,					\
 	  &(_kvm)->arch.mmu_page_hash[kvm_page_table_hashfn(_gfn)])	\
 		if ((_sp)->gfn != (_gfn) || !sp_has_gptes(_sp)) {} else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2637| <<__kvm_sync_page>> if (!kvm_sync_page_check(vcpu, sp))
+ */
 static bool kvm_sync_page_check(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
 {
 	union kvm_mmu_page_role root_role = vcpu->arch.mmu->root_role;
@@ -1942,14 +2882,38 @@ static bool kvm_sync_page_check(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2375| <<__kvm_sync_page>> int ret = kvm_sync_spte(vcpu, sp, i);
+ *   - arch/x86/kvm/mmu/mmu.c|6544| <<__kvm_mmu_invalidate_addr>> int ret = kvm_sync_spte(vcpu, sp, iterator.index);
+ */
 static int kvm_sync_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int i)
 {
 	if (!sp->spt[i])
 		return 0;
 
+	/*
+	 * 在以下使用kvm_mmu->sync_spte:
+	 *   - arch/x86/kvm/mmu/mmu.c|2351| <<kvm_sync_page_check>> if (WARN_ON_ONCE(sp->role.direct || !vcpu->arch.mmu->sync_spte ||
+	 *   - arch/x86/kvm/mmu/mmu.c|2363| <<kvm_sync_spte>> return vcpu->arch.mmu->sync_spte(vcpu, sp, i);
+	 *   - arch/x86/kvm/mmu/mmu.c|5219| <<nonpaging_init_context>> context->sync_spte = NULL;
+	 *   - arch/x86/kvm/mmu/mmu.c|5812| <<paging64_init_context>> context->sync_spte = paging64_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|5819| <<paging32_init_context>> context->sync_spte = paging32_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|5922| <<init_kvm_tdp_mmu>> context->sync_spte = NULL;
+	 *   - arch/x86/kvm/mmu/mmu.c|6053| <<kvm_init_shadow_ept_mmu>> context->sync_spte = ept_sync_spte;
+	 *   - arch/x86/kvm/mmu/mmu.c|6094| <<init_kvm_nested_mmu>> g_context->sync_spte = NULL;
+	 *   - arch/x86/kvm/mmu/mmu.c|6574| <<kvm_mmu_invalidate_addr>> if (!mmu->sync_spte)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|979| <<FNAME>> static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int i)
+	 *
+	 * 猜测: 对于paging64_sync_spte(), 似乎只sync attr???
+	 */
 	return vcpu->arch.mmu->sync_spte(vcpu, sp, i);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2397| <<kvm_sync_page>> int ret = __kvm_sync_page(vcpu, sp);
+ */
 static int __kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
 {
 	int flush = 0;
@@ -1978,9 +2942,17 @@ static int __kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
 	return flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2552| <<mmu_sync_children>> flush |= kvm_sync_page(vcpu, sp, &invalid_list) > 0;
+ *   - arch/x86/kvm/mmu/mmu.c|2640| <<kvm_mmu_find_shadow_page>> ret = kvm_sync_page(vcpu, sp, &invalid_list);
+ */
 static int kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			 struct list_head *invalid_list)
 {
+	/*
+	 * 只在此处调用
+	 */
 	int ret = __kvm_sync_page(vcpu, sp);
 
 	if (ret < 0)
@@ -1988,6 +2960,15 @@ static int kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2467| <<mmu_sync_children>> kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, true);
+ *   - arch/x86/kvm/mmu/mmu.c|2477| <<mmu_sync_children>> kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);
+ *   - arch/x86/kvm/mmu/mmu.c|2488| <<mmu_sync_children>> kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);
+ *   - arch/x86/kvm/mmu/mmu.c|6241| <<kvm_mmu_track_write>> kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);
+ *   - arch/x86/kvm/mmu/mmu.c|7681| <<kvm_recover_nx_huge_pages>> kvm_mmu_remote_flush_or_zap(kvm, &invalid_list, flush);
+ *   - arch/x86/kvm/mmu/mmu.c|7690| <<kvm_recover_nx_huge_pages>> kvm_mmu_remote_flush_or_zap(kvm, &invalid_list, flush);
+ */
 static bool kvm_mmu_remote_flush_or_zap(struct kvm *kvm,
 					struct list_head *invalid_list,
 					bool remote_flush)
@@ -2017,6 +2998,12 @@ struct mmu_page_path {
 	unsigned int idx[PT64_ROOT_MAX_LEVEL];
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2463| <<mmu_sync_children>> for_each_sp(pages, sp, parents, i)
+ *   - arch/x86/kvm/mmu/mmu.c|2471| <<mmu_sync_children>> for_each_sp(pages, sp, parents, i) {
+ *   - arch/x86/kvm/mmu/mmu.c|2941| <<mmu_zap_unsync_children>> for_each_sp(pages, sp, parents, i) {
+ */
 #define for_each_sp(pvec, sp, parents, i)			\
 		for (i = mmu_pages_first(&pvec, &parents);	\
 			i < pvec.nr && ({ sp = pvec.page[i].sp; 1;});	\
@@ -2043,6 +3030,20 @@ static int mmu_pages_next(struct kvm_mmu_pages *pvec,
 	return n;
 }
 
+/*
+ * struct mmu_page_path {
+ *     struct kvm_mmu_page *parent[PT64_ROOT_MAX_LEVEL];
+ *     unsigned int idx[PT64_ROOT_MAX_LEVEL];
+ * };
+ *
+ * struct kvm_mmu_pages {
+ *     struct mmu_page_and_offset {
+ *         struct kvm_mmu_page *sp;
+ *         unsigned int idx;
+ *     } page[KVM_PAGE_ARRAY_NR];
+ *     unsigned int nr;
+ * };
+ */
 static int mmu_pages_first(struct kvm_mmu_pages *pvec,
 			   struct mmu_page_path *parents)
 {
@@ -2067,6 +3068,11 @@ static int mmu_pages_first(struct kvm_mmu_pages *pvec,
 	return mmu_pages_next(pvec, parents, 0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3012| <<mmu_sync_children>> mmu_pages_clear_parents(&parents);
+ *   - arch/x86/kvm/mmu/mmu.c|3632| <<mmu_zap_unsync_children>> mmu_pages_clear_parents(&parents);
+ */
 static void mmu_pages_clear_parents(struct mmu_page_path *parents)
 {
 	struct kvm_mmu_page *sp;
@@ -2084,6 +3090,12 @@ static void mmu_pages_clear_parents(struct mmu_page_path *parents)
 	} while (!sp->unsync_children);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4629| <<kvm_mmu_sync_roots>> mmu_sync_children(vcpu, sp, true);
+ *   - arch/x86/kvm/mmu/mmu.c|4641| <<kvm_mmu_sync_roots>> mmu_sync_children(vcpu, sp, true);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|749| <<FNAME(fetch)>> mmu_sync_children(vcpu, sp, false))
+ */
 static int mmu_sync_children(struct kvm_vcpu *vcpu,
 			     struct kvm_mmu_page *parent, bool can_yield)
 {
@@ -2094,9 +3106,32 @@ static int mmu_sync_children(struct kvm_vcpu *vcpu,
 	LIST_HEAD(invalid_list);
 	bool flush = false;
 
+	/*
+	 * 在以下调用mmu_sync_children():
+	 *   - arch/x86/kvm/mmu/mmu.c|2816| <<mmu_sync_children>> while (mmu_unsync_walk(parent, &pages)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|3410| <<mmu_zap_unsync_children>> while (mmu_unsync_walk(parent, &pages)) {
+	 *
+	 * 如果当前sp都不是unsync了直接返回0,
+	 * 否则把自己加入kvm_mmu_pages,
+	 * 然后用__mmu_unsync_walk(sp, pvec)把unsync的子子孙孙加入kvm_mmu_paghes:
+	 * kvm_mmu_pages临时满了会返回-ENOSPC
+	 * 感觉__mmu_unsync_walk唯一返回0的机会就是其(当前sp)下面都clear unsync了,
+	 * 也就是clear_unsync_child_bit(sp, i)
+	 */
 	while (mmu_unsync_walk(parent, &pages)) {
 		bool protected = false;
 
+		/*
+		 * 在以下调用for_each_sp():
+		 *   - arch/x86/kvm/mmu/mmu.c|2463| <<mmu_sync_children>> for_each_sp(pages, sp, parents, i)
+		 *   - arch/x86/kvm/mmu/mmu.c|2471| <<mmu_sync_children>> for_each_sp(pages, sp, parents, i) {
+		 *   - arch/x86/kvm/mmu/mmu.c|2941| <<mmu_zap_unsync_children>> for_each_sp(pages, sp, parents, i) {
+		 *
+		 * sp->gfn : 对于direct似乎是管理地址范围的起始地址对应的gfn
+		 * 对于shadow, 是当前sp的对应的guest的gfn
+		 *
+		 * 只在这里调用kvm_vcpu_write_protect_gfn()
+		 */
 		for_each_sp(pages, sp, parents, i)
 			protected |= kvm_vcpu_write_protect_gfn(vcpu, sp->gfn);
 
@@ -2106,10 +3141,29 @@ static int mmu_sync_children(struct kvm_vcpu *vcpu,
 		}
 
 		for_each_sp(pages, sp, parents, i) {
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/mmu/mmu.c|2828| <<mmu_sync_children>> kvm_unlink_unsync_page(vcpu->kvm, sp);
+			 *   - arch/x86/kvm/mmu/mmu.c|3451| <<__kvm_mmu_prepare_zap_page>> kvm_unlink_unsync_page(kvm, sp);
+			 */
 			kvm_unlink_unsync_page(vcpu->kvm, sp);
 			flush |= kvm_sync_page(vcpu, sp, &invalid_list) > 0;
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/mmu/mmu.c|3012| <<mmu_sync_children>> mmu_pages_clear_parents(&parents);
+			 *   - arch/x86/kvm/mmu/mmu.c|3632| <<mmu_zap_unsync_children>> mmu_pages_clear_parents(&parents);
+			 */
 			mmu_pages_clear_parents(&parents);
 		}
+		/*
+		 * 关于rwlock_needbreak()的注释
+		 * Check if a rwlock is contended.
+		 * Returns non-zero if there is another task waiting on the rwlock.
+		 * Returns zero if the lock is not contended or the system / underlying
+		 * rwlock implementation does not support contention detection.
+		 * Technically does not depend on CONFIG_PREEMPTION, but a general need
+		 * for low latency.
+		 */
 		if (need_resched() || rwlock_needbreak(&vcpu->kvm->mmu_lock)) {
 			kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);
 			if (!can_yield) {
@@ -2122,6 +3176,15 @@ static int mmu_sync_children(struct kvm_vcpu *vcpu,
 		}
 	}
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2467| <<mmu_sync_children>> kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, true);
+	 *   - arch/x86/kvm/mmu/mmu.c|2477| <<mmu_sync_children>> kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);
+	 *   - arch/x86/kvm/mmu/mmu.c|2488| <<mmu_sync_children>> kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);
+	 *   - arch/x86/kvm/mmu/mmu.c|6241| <<kvm_mmu_track_write>> kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);
+	 *   - arch/x86/kvm/mmu/mmu.c|7681| <<kvm_recover_nx_huge_pages>> kvm_mmu_remote_flush_or_zap(kvm, &invalid_list, flush);
+	 *   - arch/x86/kvm/mmu/mmu.c|7690| <<kvm_recover_nx_huge_pages>> kvm_mmu_remote_flush_or_zap(kvm, &invalid_list, flush);
+	 */
 	kvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);
 	return 0;
 }
@@ -2142,6 +3205,10 @@ static void clear_sp_write_flooding_count(u64 *spte)
  * order to read guest page tables.  Direct shadow pages are never
  * unsync, thus @vcpu can be NULL if @role.direct is true.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2720| <<__kvm_mmu_get_shadow_page>> sp = kvm_mmu_find_shadow_page(kvm, vcpu, gfn, sp_list, role);
+ */
 static struct kvm_mmu_page *kvm_mmu_find_shadow_page(struct kvm *kvm,
 						     struct kvm_vcpu *vcpu,
 						     gfn_t gfn,
@@ -2154,11 +3221,32 @@ static struct kvm_mmu_page *kvm_mmu_find_shadow_page(struct kvm *kvm,
 	LIST_HEAD(invalid_list);
 
 	for_each_valid_sp(kvm, sp, sp_list) {
+		/*
+		 * 假设找到了相同的gfn, 会继续运行
+		 * 并不会立刻返回这个page
+		 */
 		if (sp->gfn != gfn) {
 			collisions++;
 			continue;
 		}
 
+		/*
+		 * role.direct:
+		 * If set, leaf sptes reachable from this page are for a linear range.
+		 * Examples include real mode translation, large guest pages backed by small
+		 * host pages, and gpa->hpa translations when NPT or EPT is active.
+		 * The linear range starts at (gfn << PAGE_SHIFT) and its size is determined
+		 * by role.level (2MB for first level, 1GB for second level, 0.5TB for third
+		 * level, 256TB for fourth level)
+		 * If clear, this page corresponds to a guest page table denoted by the gfn
+		 * field.
+		 *
+		 * union kvm_mmu_page_role role
+		 * -> u32 word;
+		 * -> struct
+		 *    -> unsigned level:4;
+		 *    -> unsigned direct:1;
+		 */
 		if (sp->role.word != role.word) {
 			/*
 			 * If the guest is creating an upper-level page, zap
@@ -2179,6 +3267,24 @@ static struct kvm_mmu_page *kvm_mmu_find_shadow_page(struct kvm *kvm,
 		if (sp->role.direct)
 			goto out;
 
+		/*
+		 * 在以下使用kvm_mmu_page->unsync:
+		 *   - arch/x86/kvm/mmu/mmu.c|2227| <<mmu_pages_add>> if (sp->unsync)
+		 *   - arch/x86/kvm/mmu/mmu.c|2273| <<__mmu_unsync_walk>> } else if (child->unsync) {
+		 *   - arch/x86/kvm/mmu/mmu.c|2299| <<kvm_unlink_unsync_page>> WARN_ON_ONCE(!sp->unsync);
+		 *   - arch/x86/kvm/mmu/mmu.c|2301| <<kvm_unlink_unsync_page>> sp->unsync = 0;
+		 *   - arch/x86/kvm/mmu/mmu.c|2657| <<kvm_mmu_find_shadow_page>> if (role.level > PG_LEVEL_4K && sp->unsync)
+		 *   - arch/x86/kvm/mmu/mmu.c|2667| <<kvm_mmu_find_shadow_page>> if (sp->unsync) {
+		 *   - arch/x86/kvm/mmu/mmu.c|3016| <<__link_shadow_page>> if (WARN_ON_ONCE(sp->unsync_children) || sp->unsync)
+		 *   - arch/x86/kvm/mmu/mmu.c|3183| <<__kvm_mmu_prepare_zap_page>> if (sp->unsync)
+		 *   - arch/x86/kvm/mmu/mmu.c|3403| <<kvm_unsync_page>> sp->unsync = 1;
+		 *   - arch/x86/kvm/mmu/mmu.c|3442| <<mmu_try_to_unsync_pages>> if (sp->unsync)
+		 *   - arch/x86/kvm/mmu/mmu.c|3467| <<mmu_try_to_unsync_pages>> if (READ_ONCE(sp->unsync))
+		 *   - arch/x86/kvm/mmu/mmu.c|4681| <<is_unsync_root>> if (sp->unsync || sp->unsync_children)
+		 *   - arch/x86/kvm/mmu/mmu.c|6629| <<__kvm_mmu_invalidate_addr>> if (sp->unsync) {
+		 *   - arch/x86/kvm/mmu/mmu.c|7340| <<shadow_mmu_try_split_huge_pages>> if (WARN_ON_ONCE(sp->unsync))
+		 *   - arch/x86/kvm/mmu/mmutrace.h|23| <<KVM_MMU_PAGE_ASSIGN>> __entry->unsync = sp->unsync;
+		 */
 		if (sp->unsync) {
 			if (KVM_BUG_ON(!vcpu, kvm))
 				break;
@@ -2227,6 +3333,13 @@ struct shadow_page_caches {
 	struct kvm_mmu_memory_cache *shadowed_info_cache;
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2758| <<__kvm_mmu_get_shadow_page>> sp = kvm_mmu_alloc_shadow_page(kvm, caches, gfn, sp_list, role);
+ *
+ * 如果不是direct/tdp_mmu:
+ * account_shadowed()会把shadow的mmu page设置成readonly!!!!!
+ */
 static struct kvm_mmu_page *kvm_mmu_alloc_shadow_page(struct kvm *kvm,
 						      struct shadow_page_caches *caches,
 						      gfn_t gfn,
@@ -2237,9 +3350,25 @@ static struct kvm_mmu_page *kvm_mmu_alloc_shadow_page(struct kvm *kvm,
 
 	sp = kvm_mmu_memory_cache_alloc(caches->page_header_cache);
 	sp->spt = kvm_mmu_memory_cache_alloc(caches->shadow_page_cache);
+	/*
+	 * 在以下使用kvm_mmu_page->shadowed_translation:
+	 *   - arch/x86/kvm/mmu/mmu.c|781| <<kvm_mmu_page_get_gfn>> return sp->shadowed_translation[index] >> PAGE_SHIFT;
+	 *   - arch/x86/kvm/mmu/mmu.c|795| <<kvm_mmu_page_get_access>> return sp->shadowed_translation[index] & ACC_ALL;
+	 *   - arch/x86/kvm/mmu/mmu.c|816| <<kvm_mmu_page_set_translation>> sp->shadowed_translation[index] = (gfn << PAGE_SHIFT) | access;
+	 *   - arch/x86/kvm/mmu/mmu.c|2104| <<kvm_mmu_free_shadow_page>> free_page((unsigned long )sp->shadowed_translation);
+	 *   - arch/x86/kvm/mmu/mmu.c|2655| <<kvm_mmu_alloc_shadow_page>> sp->shadowed_translation = kvm_mmu_memory_cache_alloc(caches->shadowed_info_cache);
+	 *
+	 * 以前是gfns? -> 所有页表项(pte)对应的gfn
+	 */
 	if (!role.direct)
 		sp->shadowed_translation = kvm_mmu_memory_cache_alloc(caches->shadowed_info_cache);
 
+	/*
+	 * 在kvm一共有三处调用:
+	 *   - arch/arm64/kvm/mmu.c|235| <<stage2_free_unlinked_table>> set_page_private(page, (unsigned long )level);
+	 *   - arch/x86/kvm/mmu/mmu.c|2724| <<kvm_mmu_alloc_shadow_page>> set_page_private(virt_to_page(sp->spt), (unsigned long )sp);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|224| <<tdp_mmu_init_sp>> set_page_private(virt_to_page(sp->spt), (unsigned long )sp);
+	 */
 	set_page_private(virt_to_page(sp->spt), (unsigned long)sp);
 
 	INIT_LIST_HEAD(&sp->possible_nx_huge_page_link);
@@ -2253,15 +3382,32 @@ static struct kvm_mmu_page *kvm_mmu_alloc_shadow_page(struct kvm *kvm,
 	list_add(&sp->link, &kvm->arch.active_mmu_pages);
 	kvm_account_mmu_page(kvm, sp);
 
+	/*
+	 * 似乎对direct是管理地址范围的起始地址对应的gfn
+	 * 对于shadow, 是当前sp的对应的guest的gfn
+	 * 这个值就是当前shadow页表对应的虚机页表在guest os中的gfn
+	 */
 	sp->gfn = gfn;
 	sp->role = role;
 	hlist_add_head(&sp->hash_link, sp_list);
+	/*
+	 * 只在以下调用account_shadowed()
+	 * account_shadowed()会把shadow的mmu page设置成readonly!!!!!
+	 */
 	if (sp_has_gptes(sp))
 		account_shadowed(kvm, sp);
 
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2740| <<kvm_mmu_get_shadow_page>> return __kvm_mmu_get_shadow_page(vcpu->kvm, vcpu, &caches, gfn, role);
+ *   - arch/x86/kvm/mmu/mmu.c|7144| <<shadow_mmu_get_sp_for_split>> return __kvm_mmu_get_shadow_page(kvm, NULL, &caches, gfn, role);
+ *
+ * 如果不是direct/tdp_mmu:
+ * account_shadowed()会把shadow的mmu page设置成readonly!!!!!
+ */
 /* Note, @vcpu may be NULL if @role.direct is true; see kvm_mmu_find_shadow_page. */
 static struct kvm_mmu_page *__kvm_mmu_get_shadow_page(struct kvm *kvm,
 						      struct kvm_vcpu *vcpu,
@@ -2273,18 +3419,43 @@ static struct kvm_mmu_page *__kvm_mmu_get_shadow_page(struct kvm *kvm,
 	struct kvm_mmu_page *sp;
 	bool created = false;
 
+	/*
+	 * 在以下使用kvm_arch->mmu_page_hash[KVM_NUM_MMU_PAGES]:
+	 *   - arch/x86/kvm/mmu/mmu.c|2328| <<for_each_gfn_valid_sp_with_gptes>> &(_kvm)->arch.mmu_page_hash[kvm_page_table_hashfn(_gfn)]) \
+	 *   - arch/x86/kvm/mmu/mmu.c|2753| <<__kvm_mmu_get_shadow_page>> sp_list = &kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)];
+	 */
 	sp_list = &kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)];
 
+	/*
+	 * 只在这里调用
+	 */
 	sp = kvm_mmu_find_shadow_page(kvm, vcpu, gfn, sp_list, role);
 	if (!sp) {
 		created = true;
+		/*
+		 * 如果不是direct/tdp_mmu:
+		 * account_shadowed()会把shadow的mmu page设置成readonly!!!!!
+		 */
 		sp = kvm_mmu_alloc_shadow_page(kvm, caches, gfn, sp_list, role);
 	}
 
+	/*
+	 * 在以下调用trace_kvm_mmu_get_page():
+	 *   - arch/x86/kvm/mmu/mmu.c|3384| <<__kvm_mmu_get_shadow_page>> trace_kvm_mmu_get_page(sp, created);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|361| <<tdp_mmu_init_sp>> trace_kvm_mmu_get_page(sp, true);
+	 */
 	trace_kvm_mmu_get_page(sp, created);
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2840| <<kvm_mmu_get_child_sp>> return kvm_mmu_get_shadow_page(vcpu, gfn, role);
+ *   - arch/x86/kvm/mmu/mmu.c|4297| <<mmu_alloc_root>> sp = kvm_mmu_get_shadow_page(vcpu, gfn, role);
+ *
+ * 如果不是direct/tdp_mmu:
+ * account_shadowed()会把shadow的mmu page设置成readonly!!!!!
+ */
 static struct kvm_mmu_page *kvm_mmu_get_shadow_page(struct kvm_vcpu *vcpu,
 						    gfn_t gfn,
 						    union kvm_mmu_page_role role)
@@ -2295,9 +3466,22 @@ static struct kvm_mmu_page *kvm_mmu_get_shadow_page(struct kvm_vcpu *vcpu,
 		.shadowed_info_cache = &vcpu->arch.mmu_shadowed_info_cache,
 	};
 
+	/*
+	 * 在以下调用__kvm_mmu_get_shadow_page():
+	 *   - arch/x86/kvm/mmu/mmu.c|2840| <<kvm_mmu_get_child_sp>> return kvm_mmu_get_shadow_page(vcpu, gfn, role);
+	 *   - arch/x86/kvm/mmu/mmu.c|4297| <<mmu_alloc_root>> sp = kvm_mmu_get_shadow_page(vcpu, gfn, role);
+	 *
+	 * 如果不是direct/tdp_mmu:
+	 * account_shadowed()会把shadow的mmu page设置成readonly!!!!!
+	 */
 	return __kvm_mmu_get_shadow_page(vcpu->kvm, vcpu, &caches, gfn, role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3469| <<kvm_mmu_get_child_sp>> role = kvm_mmu_child_role(sptep, direct, access);
+ *   - arch/x86/kvm/mmu/mmu.c|8246| <<shadow_mmu_get_sp_for_split>> role = kvm_mmu_child_role(huge_sptep, true, access);
+ */
 static union kvm_mmu_page_role kvm_mmu_child_role(u64 *sptep, bool direct,
 						  unsigned int access)
 {
@@ -2344,6 +3528,15 @@ static union kvm_mmu_page_role kvm_mmu_child_role(u64 *sptep, bool direct,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3792| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|728| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn, false, access);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|791| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, direct_access);
+ *
+ * 如果不是direct/tdp_mmu:
+ * account_shadowed()会把shadow的mmu page设置成readonly!!!!!
+ */
 static struct kvm_mmu_page *kvm_mmu_get_child_sp(struct kvm_vcpu *vcpu,
 						 u64 *sptep, gfn_t gfn,
 						 bool direct, unsigned int access)
@@ -2353,15 +3546,48 @@ static struct kvm_mmu_page *kvm_mmu_get_child_sp(struct kvm_vcpu *vcpu,
 	if (is_shadow_present_pte(*sptep) && !is_large_pte(*sptep))
 		return ERR_PTR(-EEXIST);
 
+	/*
+	 * 注释
+	 * role.direct:
+	 * If set, leaf sptes reachable from this page are for a linear range.
+	 * Examples include real mode translation, large guest pages backed by small
+	 * host pages, and gpa->hpa translations when NPT or EPT is active.
+	 * The linear range starts at (gfn << PAGE_SHIFT) and its size is determined
+	 * by role.level (2MB for first level, 1GB for second level, 0.5TB for third
+	 * level, 256TB for fourth level)
+	 * If clear, this page corresponds to a guest page table denoted by the gfn
+	 * field.
+	 */
 	role = kvm_mmu_child_role(sptep, direct, access);
+	/*
+	 * 如果不是direct/tdp_mmu:
+	 * account_shadowed()会把shadow的mmu page设置成readonly!!!!!
+	 */
 	return kvm_mmu_get_shadow_page(vcpu, gfn, role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|173| <<for_each_shadow_entry_using_root>> for (shadow_walk_init_using_root(&(_walker), (_vcpu), \
+ *   - arch/x86/kvm/mmu/mmu.c|2806| <<shadow_walk_init>> shadow_walk_init_using_root(iterator, vcpu, vcpu->arch.mmu->root.hpa, addr);
+ */
 static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterator,
 					struct kvm_vcpu *vcpu, hpa_t root,
 					u64 addr)
 {
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;
+	 *     hpa_t shadow_addr;
+	 *     u64 *sptep;
+	 *     int level;
+	 *     unsigned index;
+	 * };
+	 */
 	iterator->addr = addr;
+	/*
+	 * 应该是当前页表的base
+	 */
 	iterator->shadow_addr = root;
 	iterator->level = vcpu->arch.mmu->root_role.level;
 
@@ -2386,6 +3612,12 @@ static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterato
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|179| <<for_each_shadow_entry>> for (shadow_walk_init(&(_walker), _vcpu, _addr); \
+ *   - arch/x86/kvm/mmu/mmu.c|184| <<for_each_shadow_entry_lockless>> for (shadow_walk_init(&(_walker), _vcpu, _addr); \
+ *   - arch/x86/kvm/mmu/mmu.c|4582| <<get_walk>> for (shadow_walk_init(&iterator, vcpu, addr),
+ */
 static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,
 			     struct kvm_vcpu *vcpu, u64 addr)
 {
@@ -2393,6 +3625,14 @@ static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,
 				    addr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|175| <<for_each_shadow_entry_using_root>> shadow_walk_okay(&(_walker)); \
+ *   - arch/x86/kvm/mmu/mmu.c|180| <<for_each_shadow_entry>> shadow_walk_okay(&(_walker)); \
+ *   - arch/x86/kvm/mmu/mmu.c|185| <<for_each_shadow_entry_lockless>> shadow_walk_okay(&(_walker)) && \
+ *   - arch/x86/kvm/mmu/mmu.c|4584| <<get_walk>> shadow_walk_okay(&iterator);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|723| <<FNAME(fetch)>> for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+ */
 static bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator)
 {
 	if (iterator->level < PG_LEVEL_4K)
@@ -2403,6 +3643,12 @@ static bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|187| <<for_each_shadow_entry_lockless>> __shadow_walk_next(&(_walker), spte))
+ *   - arch/x86/kvm/mmu/mmu.c|2834| <<shadow_walk_next>> __shadow_walk_next(iterator, *iterator->sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|4585| <<get_walk>> __shadow_walk_next(&iterator, spte)) {
+ */
 static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,
 			       u64 spte)
 {
@@ -2415,11 +3661,30 @@ static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,
 	--iterator->level;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|176| <<for_each_shadow_entry_using_root>> shadow_walk_next(&(_walker)))
+ *   - arch/x86/kvm/mmu/mmu.c|181| <<for_each_shadow_entry>> shadow_walk_next(&(_walker)))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|723| <<FNAME(fetch)>> for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+ */
 static void shadow_walk_next(struct kvm_shadow_walk_iterator *iterator)
 {
 	__shadow_walk_next(iterator, *iterator->sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3236| <<link_shadow_page>> __link_shadow_page(vcpu->kvm, &vcpu->arch.mmu_pte_list_desc_cache, sptep, sp, true);
+ *   - arch/x86/kvm/mmu/mmu.c|7497| <<shadow_mmu_split_huge_page>> __link_shadow_page(kvm, cache, huge_sptep, sp, flush);
+ *
+ * link shadow page, link的是shadow page,
+ * 这里的shadow似乎是指页表页?
+ * 所以用的是make_nonleaf_spte()
+ *
+ * __link_shadow_page()为sptep指向的entry填充
+ * 使用的pfn是参数sp的sp->spt
+ * 还要把sptep加入到这个sp的sp->parent_ptes (kvm_rmap_head)
+ */
 static void __link_shadow_page(struct kvm *kvm,
 			       struct kvm_mmu_memory_cache *cache, u64 *sptep,
 			       struct kvm_mmu_page *sp, bool flush)
@@ -2433,6 +3698,9 @@ static void __link_shadow_page(struct kvm *kvm,
 	 * a large one.  Drop it, and flush the TLB if needed, before
 	 * installing sp.
 	 */
+	/*
+	 * 查看bit 11是否设置了
+	 */
 	if (is_shadow_present_pte(*sptep))
 		drop_large_spte(kvm, sptep, flush);
 
@@ -2440,6 +3708,9 @@ static void __link_shadow_page(struct kvm *kvm,
 
 	mmu_spte_set(sptep, spte);
 
+	/*
+	 * sptep指向的entry, 应该是指向这个sp的
+	 */
 	mmu_page_add_parent_pte(cache, sp, sptep);
 
 	/*
@@ -2451,13 +3722,45 @@ static void __link_shadow_page(struct kvm *kvm,
 	 * FNAME(fetch)(), so sp->unsync_children can only be false.
 	 * WARN_ON_ONCE() if anything happens unexpectedly.
 	 */
+	/*
+	 * 在以下设置unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|2160| <<mark_unsync>> if (sp->unsync_children++)
+	 *   - arch/x86/kvm/mmu/mmu.c|2193| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 *
+	 * unsync_children: 页表页中unsync的pte数
+	 * unsync: 用于最后一级页表页,表示该页的页表项(pte)是否与guest同步(guest是否已更新tlb)
+	 * unsync_child_bitmap: 页表页中unsync的spte bitmap
+	 */
 	if (WARN_ON_ONCE(sp->unsync_children) || sp->unsync)
 		mark_unsync(sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3719| <<direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|707| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|742| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ *
+ * link shadow page, link的是shadow page,
+ * 这里的shadow似乎是指页表页?
+ * 所以用的是make_nonleaf_spte()
+ *
+ * __link_shadow_page()为sptep指向的entry填充
+ * 使用的pfn是参数sp的sp->spt
+ * 还要把sptep加入到这个sp的sp->parent_ptes (kvm_rmap_head)
+ */
 static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 			     struct kvm_mmu_page *sp)
 {
+	/*
+	 * link shadow page, link的是shadow page,
+	 * 这里的shadow似乎是指页表页?
+	 * 所以用的是make_nonleaf_spte()
+	 *
+	 * __link_shadow_page()为sptep指向的entry填充
+	 * 使用的pfn是参数sp的sp->spt
+	 * 还要把sptep加入到这个sp的sp->parent_ptes (kvm_rmap_head)
+	 */
 	__link_shadow_page(vcpu->kvm, &vcpu->arch.mmu_pte_list_desc_cache, sptep, sp, true);
 }
 
@@ -2483,6 +3786,12 @@ static void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2525| <<kvm_mmu_page_unlink_children>> zapped += mmu_page_zap_pte(kvm, sp, sp->spt + i, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|5716| <<kvm_mmu_track_write>> mmu_page_zap_pte(vcpu->kvm, sp, spte, NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|5826| <<__kvm_mmu_invalidate_addr>> mmu_page_zap_pte(vcpu->kvm, sp, iterator.sptep, NULL);
+ */
 /* Returns the number of zapped non-leaf child shadow pages. */
 static int mmu_page_zap_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 			    u64 *spte, struct list_head *invalid_list)
@@ -2514,6 +3823,10 @@ static int mmu_page_zap_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2592| <<__kvm_mmu_prepare_zap_page>> *nr_zapped += kvm_mmu_page_unlink_children(kvm, sp, invalid_list);
+ */
 static int kvm_mmu_page_unlink_children(struct kvm *kvm,
 					struct kvm_mmu_page *sp,
 					struct list_head *invalid_list)
@@ -2527,15 +3840,30 @@ static int kvm_mmu_page_unlink_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2593| <<__kvm_mmu_prepare_zap_page>> kvm_mmu_unlink_parents(kvm, sp);
+ */
 static void kvm_mmu_unlink_parents(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	u64 *sptep;
+	/*
+	 * struct rmap_iterator {
+	 *     // private fields
+	 *     struct pte_list_desc *desc;     // holds the sptep if not NULL
+	 *     int pos;                        // index of the sptep
+	 * };
+	 */
 	struct rmap_iterator iter;
 
 	while ((sptep = rmap_get_first(&sp->parent_ptes, &iter)))
 		drop_parent_pte(kvm, sp, sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2591| <<__kvm_mmu_prepare_zap_page>> *nr_zapped = mmu_zap_unsync_children(kvm, sp, invalid_list);
+ */
 static int mmu_zap_unsync_children(struct kvm *kvm,
 				   struct kvm_mmu_page *parent,
 				   struct list_head *invalid_list)
@@ -2560,6 +3888,13 @@ static int mmu_zap_unsync_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2662| <<kvm_mmu_prepare_zap_page>> __kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|2712| <<kvm_mmu_zap_oldest_mmu_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|6273| <<kvm_zap_obsolete_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &kvm->arch.zapped_obsolete_pages, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|6861| <<kvm_mmu_zap_all>> if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))
+ */
 static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 				       struct kvm_mmu_page *sp,
 				       struct list_head *invalid_list,
@@ -2625,15 +3960,44 @@ static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 	return list_unstable;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1999| <<kvm_sync_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2185| <<kvm_mmu_find_shadow_page>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2526| <<mmu_page_zap_pte>> return kvm_mmu_prepare_zap_page(kvm, child, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2572| <<mmu_zap_unsync_children>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2779| <<kvm_mmu_unprotect_page>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3599| <<mmu_free_root_page>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|5834| <<kvm_mmu_track_write>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|7263| <<kvm_recover_nx_huge_pages>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ */
 static bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				     struct list_head *invalid_list)
 {
 	int nr_zapped;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2662| <<kvm_mmu_prepare_zap_page>> __kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);
+	 *   - arch/x86/kvm/mmu/mmu.c|2712| <<kvm_mmu_zap_oldest_mmu_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &nr_zapped);
+	 *   - arch/x86/kvm/mmu/mmu.c|6273| <<kvm_zap_obsolete_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &kvm->arch.zapped_obsolete_pages, &nr_zapped);
+	 *   - arch/x86/kvm/mmu/mmu.c|6861| <<kvm_mmu_zap_all>> if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))
+	 */
 	__kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);
 	return nr_zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2860| <<kvm_mmu_remote_flush_or_zap>> kvm_mmu_commit_zap_page(kvm, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3181| <<kvm_mmu_find_shadow_page>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3812| <<kvm_mmu_zap_oldest_mmu_pages>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3897| <<kvm_mmu_unprotect_page>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|4833| <<kvm_mmu_free_roots>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|7537| <<kvm_zap_obsolete_pages>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+ *   - arch/x86/kvm/mmu/mmu.c|8145| <<kvm_mmu_zap_all>> kvm_mmu_commit_zap_page(kvm, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|8224| <<mmu_shrink_scan>> kvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);
+ */
 static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 				    struct list_head *invalid_list)
 {
@@ -2705,6 +4069,13 @@ static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4359| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|4498| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|5124| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|934| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+ */
 static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 {
 	unsigned long avail = kvm_mmu_available_pages(vcpu->kvm);
@@ -2748,6 +4119,18 @@ void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned long goal_nr_mmu_pages)
 	write_unlock(&kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3905| <<kvm_mmu_unprotect_page_virt>> r = kvm_mmu_unprotect_page(vcpu->kvm, gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/mmu/mmu.c|7145| <<kvm_mmu_page_fault>> kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(cr2_or_gpa));
+ *   - arch/x86/kvm/x86.c|8816| <<reexecute_instruction>> kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));
+ *   - arch/x86/kvm/x86.c|8826| <<reexecute_instruction>> kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));
+ *   - arch/x86/kvm/x86.c|8879| <<retry_instruction>> kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));
+ *
+ * 核心思想是zap那些不满足下面条件(或/or)的影子sp (必须相同的gfn)
+ * - (_sp)->gfn != (_gfn), 或者
+ * - !sp_has_gptes(_sp) --> 是direct
+ */
 int kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn)
 {
 	struct kvm_mmu_page *sp;
@@ -2756,6 +4139,11 @@ int kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn)
 
 	r = 0;
 	write_lock(&kvm->mmu_lock);
+	/*
+	 * 满足下面的条件会跳过:
+	 * - (_sp)->gfn != (_gfn), 或者
+	 * - !sp_has_gptes(_sp) --> 是direct
+	 */
 	for_each_gfn_valid_sp_with_gptes(kvm, sp, gfn) {
 		r = 1;
 		kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
@@ -2766,6 +4154,10 @@ int kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5728| <<kvm_handle_page_fault>> kvm_mmu_unprotect_page_virt(vcpu, fault_address);
+ */
 static int kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva)
 {
 	gpa_t gpa;
@@ -2781,12 +4173,34 @@ static int kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3283| <<mmu_try_to_unsync_pages>> kvm_unsync_page(kvm, sp);
+ *
+ * kvm_unsync_page()在被mmu_try_to_unsync_pages()调用之前有一个:
+ *   WARN_ON_ONCE(sp->role.level != PG_LEVEL_4K);
+ *   kvm_unsync_page(kvm, sp);
+ */
 static void kvm_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	trace_kvm_mmu_unsync_page(sp);
+	/*
+	 * 在以下使用kvm_vm_stat->mmu_unsync:
+	 *   - arch/x86/kvm/x86.c|252| <<global>> STATS_DESC_ICOUNTER(VM, mmu_unsync),
+	 *   - arch/x86/kvm/mmu/mmu.c|2736| <<kvm_unlink_unsync_page>> --kvm->stat.mmu_unsync;
+	 *   - arch/x86/kvm/mmu/mmu.c|4061| <<kvm_unsync_page>> ++kvm->stat.mmu_unsync;
+	 */
 	++kvm->stat.mmu_unsync;
+	/*
+	 * 在以下设置kvm_mmu_page->unsync:
+	 *   - arch/x86/kvm/mmu/mmu.c|2301| <<kvm_unlink_unsync_page>> sp->unsync = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|3403| <<kvm_unsync_page>> sp->unsync = 1;
+	 */
 	sp->unsync = 1;
 
+	/*
+	 * 为每个指向这个sp的pte进行mark_unsync()
+	 */
 	kvm_mmu_mark_parents_unsync(sp);
 }
 
@@ -2796,6 +4210,10 @@ static void kvm_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)
  * were marked unsync (or if there is no shadow page), -EPERM if the SPTE must
  * be write-protected.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|230| <<make_spte>> if (mmu_try_to_unsync_pages(vcpu->kvm, slot, gfn, can_unsync, prefetch)) {
+ */
 int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 			    gfn_t gfn, bool can_unsync, bool prefetch)
 {
@@ -2850,6 +4268,9 @@ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 		}
 
 		WARN_ON_ONCE(sp->role.level != PG_LEVEL_4K);
+		/*
+		 * 只在这里调用
+		 */
 		kvm_unsync_page(kvm, sp);
 	}
 	if (locked)
@@ -2897,6 +4318,13 @@ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2991| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, slot, start, access, gfn, page_to_pfn(pages[i]), NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|3252| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL, base_gfn, fault->pfn, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|556| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, slot, spte, pte_access, gfn, pfn, NULL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|751| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access, base_gfn, fault->pfn, fault);
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			u64 *sptep, unsigned int pte_access, gfn_t gfn,
 			kvm_pfn_t pfn, struct kvm_page_fault *fault)
@@ -2968,6 +4396,11 @@ static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4155| <<__direct_pte_prefetch>> if (direct_pte_prefetch_many(vcpu, sp, start, spte) < 0)
+ *   - arch/x86/kvm/mmu/mmu.c|4162| <<__direct_pte_prefetch>> direct_pte_prefetch_many(vcpu, sp, start, spte);
+ */
 static int direct_pte_prefetch_many(struct kvm_vcpu *vcpu,
 				    struct kvm_mmu_page *sp,
 				    u64 *start, u64 *end)
@@ -2996,6 +4429,11 @@ static int direct_pte_prefetch_many(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4189| <<direct_pte_prefetch>> __direct_pte_prefetch(vcpu, sp, sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|729| <<FNAME(pte_prefetch)>> return __direct_pte_prefetch(vcpu, sp, sptep);
+ */
 static void __direct_pte_prefetch(struct kvm_vcpu *vcpu,
 				  struct kvm_mmu_page *sp, u64 *sptep)
 {
@@ -3021,6 +4459,10 @@ static void __direct_pte_prefetch(struct kvm_vcpu *vcpu,
 		direct_pte_prefetch_many(vcpu, sp, start, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4402| <<direct_map>> direct_pte_prefetch(vcpu, it.sptep);
+ */
 static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 {
 	struct kvm_mmu_page *sp;
@@ -3073,6 +4515,12 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
  * the above "rules" ensure KVM will not _consume_ the result of the walk if a
  * race with the primary MMU occurs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4327| <<kvm_mmu_max_mapping_level>> host_level = host_pfn_mapping_level(kvm, gfn, slot);
+ *
+ * 返回PG_LEVEL_4K, PG_LEVEL_2M或者PG_LEVEL_1G,
+ */
 static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn,
 				  const struct kvm_memory_slot *slot)
 {
@@ -3137,6 +4585,12 @@ static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn,
 	return level;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4351| <<kvm_mmu_hugepage_adjust>> fault->req_level = kvm_mmu_max_mapping_level(vcpu->kvm, slot, fault->gfn, fault->max_level);
+ *   - arch/x86/kvm/mmu/mmu.c|8055| <<kvm_mmu_zap_collapsible_spte>> sp->role.level < kvm_mmu_max_mapping_level(kvm, slot, sp->gfn, PG_LEVEL_NUM)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1742| <<zap_collapsible_spte_range>> max_mapping_level = kvm_mmu_max_mapping_level(kvm, slot, iter.gfn, PG_LEVEL_NUM);
+ */
 int kvm_mmu_max_mapping_level(struct kvm *kvm,
 			      const struct kvm_memory_slot *slot, gfn_t gfn,
 			      int max_level)
@@ -3158,6 +4612,12 @@ int kvm_mmu_max_mapping_level(struct kvm *kvm,
 	return min(host_level, max_level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4603| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|868| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1614| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ */
 void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_memory_slot *slot = fault->slot;
@@ -3214,15 +4674,39 @@ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4405| <<direct_page_fault>> r = direct_map(vcpu, fault);
+ *
+ * 下面的三个配对:
+ * - kvm_tdp_mmu_map()
+ * - direct_map()
+ * - FNAME(fetch)
+ */
 static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_shadow_walk_iterator it;
 	struct kvm_mmu_page *sp;
 	int ret;
+	/*
+	 * fault->gfn应该是触发page fault的page的gfn
+	 */
 	gfn_t base_gfn = fault->gfn;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4603| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|868| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1614| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace:
+	 *   - arch/x86/kvm/mmu/mmu.c|4605| <<direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|870| <<FNAME>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1616| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 	for_each_shadow_entry(vcpu, fault->addr, it) {
 		/*
@@ -3236,10 +4720,23 @@ static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		if (it.level == fault->goal_level)
 			break;
 
+		/*
+		 * 如果不是direct/tdp_mmu:
+		 * account_shadowed()会把shadow的mmu page设置成readonly!!!!!
+		 */
 		sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
 		if (sp == ERR_PTR(-EEXIST))
 			continue;
 
+		/*
+		 * link shadow page, link的是shadow page,
+		 * 这里的shadow似乎是指页表页?
+		 * 所以用的是make_nonleaf_spte()
+		 *
+		 * __link_shadow_page()为sptep指向的entry填充
+		 * 使用的pfn是参数sp的sp->spt
+		 * 还要把sptep加入到这个sp的sp->parent_ptes (kvm_rmap_head)
+		 */
 		link_shadow_page(vcpu, it.sptep, sp);
 		if (fault->huge_page_disallowed)
 			account_nx_huge_page(vcpu->kvm, sp,
@@ -3288,6 +4785,10 @@ static int kvm_handle_error_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fa
 	return -EFAULT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4339| <<kvm_faultin_pfn>> return kvm_handle_noslot_fault(vcpu, fault, access);
+ */
 static int kvm_handle_noslot_fault(struct kvm_vcpu *vcpu,
 				   struct kvm_page_fault *fault,
 				   unsigned int access)
@@ -3421,6 +4922,11 @@ static u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)
 /*
  * Returns one of RET_PF_INVALID, RET_PF_FIXED or RET_PF_SPURIOUS.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4383| <<direct_page_fault>> r = fast_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4463| <<kvm_tdp_mmu_page_fault>> r = fast_page_fault(vcpu, fault);
+ */
 static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu_page *sp;
@@ -3543,6 +5049,12 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4081| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->prev_roots[i].hpa, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|4088| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->root.hpa, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|4094| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->pae_root[i], &invalid_list);
+ */
 static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 			       struct list_head *invalid_list)
 {
@@ -3555,6 +5067,17 @@ static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 	if (WARN_ON_ONCE(!sp))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1999| <<kvm_sync_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
+	 *   - arch/x86/kvm/mmu/mmu.c|2185| <<kvm_mmu_find_shadow_page>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+	 *   - arch/x86/kvm/mmu/mmu.c|2526| <<mmu_page_zap_pte>> return kvm_mmu_prepare_zap_page(kvm, child, invalid_list);
+	 *   - arch/x86/kvm/mmu/mmu.c|2572| <<mmu_zap_unsync_children>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+	 *   - arch/x86/kvm/mmu/mmu.c|2779| <<kvm_mmu_unprotect_page>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+	 *   - arch/x86/kvm/mmu/mmu.c|3599| <<mmu_free_root_page>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+	 *   - arch/x86/kvm/mmu/mmu.c|5834| <<kvm_mmu_track_write>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);
+	 *   - arch/x86/kvm/mmu/mmu.c|7263| <<kvm_recover_nx_huge_pages>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+	 */
 	if (is_tdp_mmu_page(sp))
 		kvm_tdp_mmu_put_root(kvm, sp, false);
 	else if (!--sp->root_count && sp->role.invalid)
@@ -3563,6 +5086,21 @@ static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 	*root_hpa = INVALID_PAGE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4131| <<kvm_mmu_free_guest_mode_roots>> kvm_mmu_free_roots(kvm, mmu, roots_to_free);
+ *   - arch/x86/kvm/mmu/mmu.c|4543| <<kvm_mmu_sync_prev_roots>> kvm_mmu_free_roots(vcpu->kvm, vcpu->arch.mmu, roots_to_free);
+ *   - arch/x86/kvm/mmu/mmu.c|5150| <<cached_root_find_and_keep_current>> kvm_mmu_free_roots(kvm, mmu, KVM_MMU_ROOT_CURRENT);
+ *   - arch/x86/kvm/mmu/mmu.c|5190| <<fast_pgd_switch>> kvm_mmu_free_roots(kvm, mmu, KVM_MMU_ROOT_CURRENT);
+ *   - arch/x86/kvm/mmu/mmu.c|6107| <<kvm_mmu_unload>> kvm_mmu_free_roots(kvm, &vcpu->arch.root_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|6109| <<kvm_mmu_unload>> kvm_mmu_free_roots(kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|6155| <<__kvm_mmu_free_obsolete_roots>> kvm_mmu_free_roots(kvm, mmu, roots_to_free);
+ *   - arch/x86/kvm/vmx/nested.c|332| <<free_nested>> kvm_mmu_free_roots(vcpu->kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/vmx/nested.c|5256| <<nested_release_vmcs12>> kvm_mmu_free_roots(vcpu->kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/vmx/nested.c|5743| <<handle_invept>> kvm_mmu_free_roots(vcpu->kvm, mmu, roots_to_free);
+ *   - arch/x86/kvm/x86.c|914| <<load_pdptrs>> kvm_mmu_free_roots(vcpu->kvm, mmu, KVM_MMU_ROOT_CURRENT);
+ *   - arch/x86/kvm/x86.c|1275| <<kvm_invalidate_pcid>> kvm_mmu_free_roots(vcpu->kvm, mmu, roots_to_free);
+ */
 /* roots_to_free must be some combination of the KVM_MMU_ROOT_* flags */
 void kvm_mmu_free_roots(struct kvm *kvm, struct kvm_mmu *mmu,
 			ulong roots_to_free)
@@ -3620,6 +5158,10 @@ void kvm_mmu_free_roots(struct kvm *kvm, struct kvm_mmu *mmu,
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_free_roots);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5832| <<handle_invvpid>> kvm_mmu_free_guest_mode_roots(vcpu->kvm, &vcpu->arch.root_mmu);
+ */
 void kvm_mmu_free_guest_mode_roots(struct kvm *kvm, struct kvm_mmu *mmu)
 {
 	unsigned long roots_to_free = 0;
@@ -3647,24 +5189,65 @@ void kvm_mmu_free_guest_mode_roots(struct kvm *kvm, struct kvm_mmu *mmu)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_free_guest_mode_roots);
 
+/*
+ * nested的例子:
+ * [0] mmu_alloc_root
+ * [0] kvm_mmu_load
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4320| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+ *   - arch/x86/kvm/mmu/mmu.c|4331| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+ *   - arch/x86/kvm/mmu/mmu.c|4456| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+ *   - arch/x86/kvm/mmu/mmu.c|4510| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+ */
 static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, int quadrant,
 			    u8 level)
 {
 	union kvm_mmu_page_role role = vcpu->arch.mmu->root_role;
 	struct kvm_mmu_page *sp;
 
+	/*
+	 * 在VM as KVM上的ept=0上不停出现:
+	 * gfn=262144, level=2
+	 *
+	 * 在nested上会出现:
+	 * gfn=384, level=4
+	 */
+
 	role.level = level;
 	role.quadrant = quadrant;
 
 	WARN_ON_ONCE(quadrant && !role.has_4_byte_gpte);
 	WARN_ON_ONCE(role.direct && role.has_4_byte_gpte);
 
+	/*
+	 * 如果不是direct/tdp_mmu:
+	 * account_shadowed()会把shadow的mmu page设置成readonly!!!!!
+	 */
 	sp = kvm_mmu_get_shadow_page(vcpu, gfn, role);
+	/*
+	 * 在以下使用kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|3722| <<__kvm_mmu_prepare_zap_page>> if (!sp->root_count) {
+	 *   - arch/x86/kvm/mmu/mmu.c|3823| <<kvm_mmu_commit_zap_page>> WARN_ON_ONCE(!sp->role.invalid || sp->root_count);
+	 *   - arch/x86/kvm/mmu/mmu.c|3846| <<kvm_mmu_zap_oldest_mmu_pages>> if (sp->root_count)
+	 *   - arch/x86/kvm/mmu/mmu.c|4819| <<mmu_free_root_page>> else if (!--sp->root_count && sp->role.invalid)
+	 *   - arch/x86/kvm/mmu/mmu.c|4944| <<mmu_alloc_root>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|22| <<KVM_MMU_PAGE_ASSIGN>> __entry->root_count = sp->root_count; \
+	 */
 	++sp->root_count;
 
 	return __pa(sp->spt);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6237| <<kvm_mmu_load>> r = mmu_alloc_direct_roots(vcpu);
+ */
 static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -3682,6 +5265,13 @@ static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 		root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);
 		mmu->root.hpa = root;
 	} else if (shadow_root_level >= PT64_ROOT_4LEVEL) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4320| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4331| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+		 *   - arch/x86/kvm/mmu/mmu.c|4456| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4510| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+		 */
 		root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
 		mmu->root.hpa = root;
 	} else if (shadow_root_level == PT32E_ROOT_LEVEL) {
@@ -3712,6 +5302,16 @@ static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5210| <<mmu_alloc_shadow_roots>> r = mmu_first_shadow_root_alloc(vcpu->kvm);
+ *
+ * vcpu_enter_guest() or kvm_arch_async_page_ready()
+ * -> kvm_mmu_reload()
+ *    -> kvm_mmu_load()
+ *       -> mmu_alloc_shadow_roots()
+ *          -> mmu_first_shadow_root_alloc()
+ */
 static int mmu_first_shadow_root_alloc(struct kvm *kvm)
 {
 	struct kvm_memslots *slots;
@@ -3766,6 +5366,11 @@ static int mmu_first_shadow_root_alloc(struct kvm *kvm)
 	 * all the related pointers are set.
 	 */
 out_success:
+	/*
+	 * 在以下使用kvm_arch->shadow_root_allocated:
+	 *   - arch/x86/kvm/mmu.h|297| <<kvm_shadow_root_allocated>> return smp_load_acquire(&kvm->arch.shadow_root_allocated);
+	 *   - arch/x86/kvm/mmu/mmu.c|5363| <<mmu_first_shadow_root_alloc>> smp_store_release(&kvm->arch.shadow_root_allocated, true);
+	 */
 	smp_store_release(&kvm->arch.shadow_root_allocated, true);
 
 out_unlock:
@@ -3773,6 +5378,10 @@ static int mmu_first_shadow_root_alloc(struct kvm *kvm)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6272| <<kvm_mmu_load>> r = mmu_alloc_shadow_roots(vcpu);
+ */
 static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -3789,6 +5398,13 @@ static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 		return 0;
 	}
 
+	/*
+	 * #define PT64_ROOT_5LEVEL 5
+	 * #define PT64_ROOT_4LEVEL 4
+	 * #define PT32_ROOT_LEVEL 2
+	 * #define PT32E_ROOT_LEVEL 3
+	 */
+
 	/*
 	 * On SVM, reading PDPTRs might access guest memory, which might fault
 	 * and thus might sleep.  Grab the PDPTRs before acquiring mmu_lock.
@@ -3891,6 +5507,10 @@ static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7111| <<kvm_mmu_load>> r = mmu_alloc_special_roots(vcpu);
+ */
 static int mmu_alloc_special_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -3963,6 +5583,13 @@ static int mmu_alloc_special_roots(struct kvm_vcpu *vcpu)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5065| <<kvm_mmu_sync_roots>> if (!is_unsync_root(root))
+ *   - arch/x86/kvm/mmu/mmu.c|5096| <<kvm_mmu_sync_prev_roots>> if (is_unsync_root(vcpu->arch.mmu->prev_roots[i].hpa))
+ *
+ * 如果(sp->unsync || sp->unsync_children), 返回true
+ */
 static bool is_unsync_root(hpa_t root)
 {
 	struct kvm_mmu_page *sp;
@@ -3998,6 +5625,12 @@ static bool is_unsync_root(hpa_t root)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5503| <<kvm_mmu_load>> kvm_mmu_sync_roots(vcpu);
+ *   - arch/x86/kvm/x86.c|3578| <<kvm_vcpu_flush_tlb_guest>> kvm_mmu_sync_roots(vcpu);
+ *   - arch/x86/kvm/x86.c|10850| <<vcpu_enter_guest(KVM_REQ_MMU_SYNC)>> kvm_mmu_sync_roots(vcpu);
+ */
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -4006,6 +5639,14 @@ void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.mmu->root_role.direct)
 		return;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_mmu *mmu;
+	 *       -> struct kvm_mmu_root_info root;
+	 *          -> gpa_t pgd;
+	 *          -> hpa_t hpa;
+	 */
 	if (!VALID_PAGE(vcpu->arch.mmu->root.hpa))
 		return;
 
@@ -4014,12 +5655,25 @@ void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.mmu->cpu_role.base.level >= PT64_ROOT_4LEVEL) {
 		hpa_t root = vcpu->arch.mmu->root.hpa;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|5065| <<kvm_mmu_sync_roots>> if (!is_unsync_root(root))
+		 *   - arch/x86/kvm/mmu/mmu.c|5096| <<kvm_mmu_sync_prev_roots>> if (is_unsync_root(vcpu->arch.mmu->prev_roots[i].hpa))
+		 *
+		 * 如果(sp->unsync || sp->unsync_children), 返回true
+		 */
 		if (!is_unsync_root(root))
 			return;
 
 		sp = root_to_sp(root);
 
 		write_lock(&vcpu->kvm->mmu_lock);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4629| <<kvm_mmu_sync_roots>> mmu_sync_children(vcpu, sp, true);
+		 *   - arch/x86/kvm/mmu/mmu.c|4641| <<kvm_mmu_sync_roots>> mmu_sync_children(vcpu, sp, true);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|749| <<FNAME(fetch)>> mmu_sync_children(vcpu, sp, false))
+		 */
 		mmu_sync_children(vcpu, sp, true);
 		write_unlock(&vcpu->kvm->mmu_lock);
 		return;
@@ -4028,6 +5682,13 @@ void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu)
 	write_lock(&vcpu->kvm->mmu_lock);
 
 	for (i = 0; i < 4; ++i) {
+		/* struct kvm_vcpu *vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct kvm_mmu *mmu;
+		 *       -> u64 *pae_root;
+		 *       -> u64 *pml4_root;
+		 *       -> u64 *pml5_root;
+		 */
 		hpa_t root = vcpu->arch.mmu->pae_root[i];
 
 		if (IS_VALID_PAE_ROOT(root)) {
@@ -4151,6 +5812,10 @@ static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 	return reserved;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5750| <<kvm_mmu_page_fault>> r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
+ */
 static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 {
 	u64 spte;
@@ -4185,6 +5850,12 @@ static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 	return RET_PF_RETRY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4380| <<direct_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+ *   - arch/x86/kvm/mmu/mmu.c|4460| <<kvm_tdp_mmu_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|806| <<FNAME(page_fault)>> if (page_fault_handle_page_track(vcpu, fault)) {
+ */
 static bool page_fault_handle_page_track(struct kvm_vcpu *vcpu,
 					 struct kvm_page_fault *fault)
 {
@@ -4226,6 +5897,10 @@ static u32 alloc_apf_token(struct kvm_vcpu *vcpu)
 	return (vcpu->arch.apf.id++ << 12) | vcpu->vcpu_id;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5845| <<__kvm_faultin_pfn>> } else if (kvm_arch_setup_async_pf(vcpu, fault->addr, fault->gfn)) {
+ */
 static bool kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 				    gfn_t gfn)
 {
@@ -4240,6 +5915,10 @@ static bool kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 				  kvm_vcpu_gfn_to_hva(vcpu, gfn), &arch);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|148| <<kvm_check_async_pf_completion>> kvm_arch_async_page_ready(vcpu, work);
+ */
 void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 {
 	int r;
@@ -4256,9 +5935,18 @@ void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 	      work->arch.cr3 != kvm_mmu_get_guest_pgd(vcpu, vcpu->arch.mmu))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4259| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true, NULL);
+	 *   - arch/x86/kvm/mmu/mmu.c|5756| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, lower_32_bits(error_code), false, &emulation_type);
+	 */
 	kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true, NULL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5395| <<kvm_faultin_pfn>> ret = __kvm_faultin_pfn(vcpu, fault);
+ */
 static int __kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_memory_slot *slot = fault->slot;
@@ -4320,6 +6008,12 @@ static int __kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	return RET_PF_CONTINUE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4391| <<direct_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|4471| <<kvm_tdp_mmu_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|815| <<FNAME(page_fault)>> r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
+ */
 static int kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 			   unsigned int access)
 {
@@ -4328,6 +6022,16 @@ static int kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	fault->mmu_seq = vcpu->kvm->mmu_invalidate_seq;
 	smp_rmb();
 
+	/*
+	 * enum {
+	 *     RET_PF_CONTINUE = 0,
+	 *     RET_PF_RETRY,
+	 *     RET_PF_EMULATE,
+	 *     RET_PF_INVALID,
+	 *     RET_PF_FIXED,
+	 *     RET_PF_SPURIOUS,
+	 * };
+	 */
 	ret = __kvm_faultin_pfn(vcpu, fault);
 	if (ret != RET_PF_CONTINUE)
 		return ret;
@@ -4345,6 +6049,12 @@ static int kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
  * Returns true if the page fault is stale and needs to be retried, i.e. if the
  * root was invalidated by a memslot update or a relevant mmu_notifier fired.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4398| <<direct_page_fault>> if (is_page_fault_stale(vcpu, fault))
+ *   - arch/x86/kvm/mmu/mmu.c|4478| <<kvm_tdp_mmu_page_fault>> if (is_page_fault_stale(vcpu, fault))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|841| <<FNAME(page_fault)>> if (is_page_fault_stale(vcpu, fault))
+ */
 static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 				struct kvm_page_fault *fault)
 {
@@ -4369,6 +6079,13 @@ static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 	       mmu_invalidate_retry_hva(vcpu->kvm, fault->mmu_seq, fault->hva);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5867| <<nonpaging_page_fault>> return direct_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|6024| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, fault);
+ *
+ * 和kvm_tdp_mmu_page_fault()配对
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	int r;
@@ -4388,6 +6105,12 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (r)
 		return r;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4391| <<direct_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+	 *   - arch/x86/kvm/mmu/mmu.c|4471| <<kvm_tdp_mmu_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|815| <<FNAME(page_fault)>> r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
+	 */
 	r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
 	if (r != RET_PF_CONTINUE)
 		return r;
@@ -4410,6 +6133,10 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	return r;
 }
 
+/*
+ * 在以下使用nonpaging_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|6063| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+ */
 static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
 				struct kvm_page_fault *fault)
 {
@@ -4418,6 +6145,12 @@ static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
 	return direct_page_fault(vcpu, fault);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2045| <<pf_interception>> return kvm_handle_page_fault(vcpu, error_code, fault_address, static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+ *                                                                   svm->vmcb->control.insn_bytes : NULL, svm->vmcb->control.insn_len);
+ *   - arch/x86/kvm/vmx/vmx.c|5354| <<handle_exception_nmi>> return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
+ */
 int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 				u64 fault_address, char *insn, int insn_len)
 {
@@ -4452,22 +6185,52 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 EXPORT_SYMBOL_GPL(kvm_handle_page_fault);
 
 #ifdef CONFIG_X86_64
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4524| <<kvm_tdp_page_fault>> return kvm_tdp_mmu_page_fault(vcpu, fault);
+ *
+ * 和direct_page_fault()配对
+ */
 static int kvm_tdp_mmu_page_fault(struct kvm_vcpu *vcpu,
 				  struct kvm_page_fault *fault)
 {
 	int r;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4380| <<direct_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/mmu.c|4460| <<kvm_tdp_mmu_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|806| <<FNAME(page_fault)>> if (page_fault_handle_page_track(vcpu, fault)) {
+	 */
 	if (page_fault_handle_page_track(vcpu, fault))
 		return RET_PF_EMULATE;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4383| <<direct_page_fault>> r = fast_page_fault(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/mmu.c|4463| <<kvm_tdp_mmu_page_fault>> r = fast_page_fault(vcpu, fault);
+	 */
 	r = fast_page_fault(vcpu, fault);
 	if (r != RET_PF_INVALID)
 		return r;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4387| <<direct_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|4467| <<kvm_tdp_mmu_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|5490| <<kvm_mmu_load>> r = mmu_topup_memory_caches(vcpu, !vcpu->arch.mmu->root_role.direct);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|811| <<FNAME(page_fault)>> r = mmu_topup_memory_caches(vcpu, true);
+	 */
 	r = mmu_topup_memory_caches(vcpu, false);
 	if (r)
 		return r;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4391| <<direct_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+	 *   - arch/x86/kvm/mmu/mmu.c|4471| <<kvm_tdp_mmu_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|815| <<FNAME(page_fault)>> r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
+	 */
 	r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
 	if (r != RET_PF_CONTINUE)
 		return r;
@@ -4475,6 +6238,12 @@ static int kvm_tdp_mmu_page_fault(struct kvm_vcpu *vcpu,
 	r = RET_PF_RETRY;
 	read_lock(&vcpu->kvm->mmu_lock);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4398| <<direct_page_fault>> if (is_page_fault_stale(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/mmu.c|4478| <<kvm_tdp_mmu_page_fault>> if (is_page_fault_stale(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|841| <<FNAME(page_fault)>> if (is_page_fault_stale(vcpu, fault))
+	 */
 	if (is_page_fault_stale(vcpu, fault))
 		goto out_unlock;
 
@@ -4501,6 +6270,12 @@ bool __kvm_mmu_honors_guest_mtrrs(bool vm_has_noncoherent_dma)
 	return vm_has_noncoherent_dma && shadow_memtype_mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5224| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ *   - arch/x86/kvm/mmu/mmu_internal.h|292| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *   - arch/x86/kvm/mmu/mmu_internal.h|316| <<kvm_mmu_do_page_fault>> r = kvm_tdp_page_fault(vcpu, &fault);
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	/*
@@ -4520,6 +6295,9 @@ int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	}
 
 #ifdef CONFIG_X86_64
+	/*
+	 * 这里用的是read_lock(&vcpu->kvm->mmu_lock);
+	 */
 	if (tdp_mmu_enabled)
 		return kvm_tdp_mmu_page_fault(vcpu, fault);
 #endif
@@ -4560,6 +6338,10 @@ static inline bool is_root_usable(struct kvm_mmu_root_info *root, gpa_t pgd,
  * If no match is found, kvm_mmu->root is left invalid, the LRU root is
  * evicted to make room for the current root, and false is returned.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6160| <<fast_pgd_switch>> return cached_root_find_and_keep_current(kvm, mmu, new_pgd, new_role);
+ */
 static bool cached_root_find_and_keep_current(struct kvm *kvm, struct kvm_mmu *mmu,
 					      gpa_t new_pgd,
 					      union kvm_mmu_page_role new_role)
@@ -4615,6 +6397,10 @@ static bool cached_root_find_without_current(struct kvm *kvm, struct kvm_mmu *mm
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6182| <<kvm_mmu_new_pgd>> if (!fast_pgd_switch(vcpu->kvm, mmu, new_pgd, new_role))
+ */
 static bool fast_pgd_switch(struct kvm *kvm, struct kvm_mmu *mmu,
 			    gpa_t new_pgd, union kvm_mmu_page_role new_role)
 {
@@ -4631,6 +6417,14 @@ static bool fast_pgd_switch(struct kvm *kvm, struct kvm_mmu *mmu,
 		return cached_root_find_without_current(kvm, mmu, new_pgd, new_role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5310| <<kvm_init_shadow_npt_mmu>> kvm_mmu_new_pgd(vcpu, nested_cr3);
+ *   - arch/x86/kvm/mmu/mmu.c|5364| <<kvm_init_shadow_ept_mmu>> kvm_mmu_new_pgd(vcpu, new_eptp);
+ *   - arch/x86/kvm/svm/nested.c|536| <<nested_svm_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ *   - arch/x86/kvm/vmx/nested.c|1110| <<nested_vmx_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ *   - arch/x86/kvm/x86.c|1306| <<kvm_set_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ */
 void kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -4677,6 +6471,10 @@ void kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_new_pgd);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|932| <<FNAME(sync_spte)>> if (sync_mmio_spte(vcpu, &sp->spt[i], gfn, pte_access))
+ */
 static bool sync_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,
 			   unsigned int access)
 {
@@ -5097,6 +6895,13 @@ static void update_pkru_bitmask(struct kvm_mmu *mmu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6621| <<__kvm_mmu_refresh_passthrough_bits>> reset_guest_paging_metadata(vcpu, mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|6685| <<init_kvm_tdp_mmu>> reset_guest_paging_metadata(vcpu, context);
+ *   - arch/x86/kvm/mmu/mmu.c|6707| <<shadow_mmu_init_context>> reset_guest_paging_metadata(vcpu, context);
+ *   - arch/x86/kvm/mmu/mmu.c|6880| <<init_kvm_nested_mmu>> reset_guest_paging_metadata(vcpu, g_context);
+ */
 static void reset_guest_paging_metadata(struct kvm_vcpu *vcpu,
 					struct kvm_mmu *mmu)
 {
@@ -5108,6 +6913,10 @@ static void reset_guest_paging_metadata(struct kvm_vcpu *vcpu,
 	update_pkru_bitmask(mmu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6703| <<shadow_mmu_init_context>> paging64_init_context(context);
+ */
 static void paging64_init_context(struct kvm_mmu *context)
 {
 	context->page_fault = paging64_page_fault;
@@ -5115,6 +6924,10 @@ static void paging64_init_context(struct kvm_mmu *context)
 	context->sync_spte = paging64_sync_spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6705| <<shadow_mmu_init_context>> paging32_init_context(context);
+ */
 static void paging32_init_context(struct kvm_mmu *context)
 {
 	context->page_fault = paging32_page_fault;
@@ -5122,6 +6935,11 @@ static void paging32_init_context(struct kvm_mmu *context)
 	context->sync_spte = paging32_sync_spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6736| <<kvm_init_shadow_npt_mmu>> union kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &regs);
+ *   - arch/x86/kvm/mmu/mmu.c|6869| <<kvm_init_mmu>> union kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &regs);
+ */
 static union kvm_cpu_role kvm_calc_cpu_role(struct kvm_vcpu *vcpu,
 					    const struct kvm_mmu_role_regs *regs)
 {
@@ -5162,6 +6980,10 @@ static union kvm_cpu_role kvm_calc_cpu_role(struct kvm_vcpu *vcpu,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|185| <<kvm_mmu_refresh_passthrough_bits>> __kvm_mmu_refresh_passthrough_bits(vcpu, mmu);
+ */
 void __kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,
 					struct kvm_mmu *mmu)
 {
@@ -5177,6 +6999,12 @@ void __kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,
 	reset_guest_paging_metadata(vcpu, mmu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6649| <<kvm_calc_tdp_mmu_root_page_role>> role.level = kvm_mmu_get_tdp_level(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|6752| <<kvm_init_shadow_npt_mmu>> root_role.level = kvm_mmu_get_tdp_level(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|7517| <<__kvm_mmu_create>> if (tdp_enabled && kvm_mmu_get_tdp_level(vcpu) > PT32E_ROOT_LEVEL)
+ */
 static inline int kvm_mmu_get_tdp_level(struct kvm_vcpu *vcpu)
 {
 	/* tdp_root_level is architecture forced level, use it if nonzero */
@@ -5190,6 +7018,10 @@ static inline int kvm_mmu_get_tdp_level(struct kvm_vcpu *vcpu)
 	return max_tdp_level;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6664| <<init_kvm_tdp_mmu>> union kvm_mmu_page_role root_role = kvm_calc_tdp_mmu_root_page_role(vcpu, cpu_role);
+ */
 static union kvm_mmu_page_role
 kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu,
 				union kvm_cpu_role cpu_role)
@@ -5209,6 +7041,10 @@ kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu,
 	return role;
 }
 
+/*
+ * 如果是tdp_enabled:
+ *   - arch/x86/kvm/mmu/mmu.c|6874| <<kvm_init_mmu>> init_kvm_tdp_mmu(vcpu, cpu_role);
+ */
 static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu,
 			     union kvm_cpu_role cpu_role)
 {
@@ -5238,6 +7074,11 @@ static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu,
 	reset_tdp_shadow_zero_bits_mask(context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6733| <<kvm_init_shadow_mmu>> shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
+ *   - arch/x86/kvm/mmu/mmu.c|6757| <<kvm_init_shadow_npt_mmu>> shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
+ */
 static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *context,
 				    union kvm_cpu_role cpu_role,
 				    union kvm_mmu_page_role root_role)
@@ -5260,6 +7101,10 @@ static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *conte
 	reset_shadow_zero_bits_mask(vcpu, context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6833| <<init_kvm_softmmu>> kvm_init_shadow_mmu(vcpu, cpu_role);
+ */
 static void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu,
 				union kvm_cpu_role cpu_role)
 {
@@ -5285,6 +7130,10 @@ static void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu,
 	shadow_mmu_init_context(vcpu, context, cpu_role, root_role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|93| <<nested_svm_init_mmu_context>> kvm_init_shadow_npt_mmu(vcpu, X86_CR0_PG, svm->vmcb01.ptr->save.cr4, svm->vmcb01.ptr->save.efer, svm->nested.ctl.nested_cr3);
+ */
 void kvm_init_shadow_npt_mmu(struct kvm_vcpu *vcpu, unsigned long cr0,
 			     unsigned long cr4, u64 efer, gpa_t nested_cr3)
 {
@@ -5311,6 +7160,10 @@ void kvm_init_shadow_npt_mmu(struct kvm_vcpu *vcpu, unsigned long cr0,
 }
 EXPORT_SYMBOL_GPL(kvm_init_shadow_npt_mmu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6785| <<kvm_init_shadow_ept_mmu>> kvm_calc_shadow_ept_root_page_role(vcpu, accessed_dirty, execonly, level);
+ */
 static union kvm_cpu_role
 kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty,
 				   bool execonly, u8 level)
@@ -5336,6 +7189,10 @@ kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|417| <<nested_ept_new_eptp>> kvm_init_shadow_ept_mmu(vcpu, execonly, ept_lpage_level, nested_ept_ad_enabled(vcpu), nested_ept_get_eptp(vcpu));
+ */
 void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 			     int huge_page_level, bool accessed_dirty,
 			     gpa_t new_eptp)
@@ -5365,6 +7222,10 @@ void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 }
 EXPORT_SYMBOL_GPL(kvm_init_shadow_ept_mmu);
 
+/*
+ * 如果不是tdp_enabled也不是mmu_is_nested(vcpu):
+ *   - arch/x86/kvm/mmu/mmu.c|6876| <<kvm_init_mmu>> init_kvm_softmmu(vcpu, cpu_role);
+ */
 static void init_kvm_softmmu(struct kvm_vcpu *vcpu,
 			     union kvm_cpu_role cpu_role)
 {
@@ -5377,6 +7238,10 @@ static void init_kvm_softmmu(struct kvm_vcpu *vcpu,
 	context->inject_page_fault = kvm_inject_page_fault;
 }
 
+/*
+ * 如果是mmu_is_nested(vcpu):
+ *   - arch/x86/kvm/mmu/mmu.c|6872| <<kvm_init_mmu>> init_kvm_nested_mmu(vcpu, cpu_role);
+ */
 static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu,
 				union kvm_cpu_role new_mode)
 {
@@ -5416,6 +7281,14 @@ static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu,
 	reset_guest_paging_metadata(vcpu, g_context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6054| <<kvm_mmu_reset_context>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|533| <<nested_svm_load_cr3>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|1107| <<nested_vmx_load_cr3>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/x86.c|956| <<kvm_post_set_cr0>> kvm_init_mmu(vcpu);
+ *   - arch/x86/kvm/x86.c|12268| <<kvm_arch_vcpu_create>> kvm_init_mmu(vcpu);
+ */
 void kvm_init_mmu(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu_role_regs regs = vcpu_to_role_regs(vcpu);
@@ -5459,6 +7332,19 @@ void kvm_mmu_after_set_cpuid(struct kvm_vcpu *vcpu)
 	KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5453| <<kvm_mmu_after_set_cpuid>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/smm.c|132| <<kvm_smm_changed>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/smm.c|369| <<enter_smm>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4661| <<nested_vmx_restore_host_state>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|962| <<kvm_post_set_cr0>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1153| <<kvm_post_set_cr4>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1780| <<set_efer>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|11698| <<__set_sregs>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|11742| <<__set_sregs2>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|12269| <<kvm_vcpu_reset>> kvm_mmu_reset_context(vcpu);
+ */
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_unload(vcpu);
@@ -5466,6 +7352,10 @@ void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_reset_context);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|132| <<kvm_mmu_reload>> return kvm_mmu_load(vcpu);
+ */
 int kvm_mmu_load(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -5476,6 +7366,14 @@ int kvm_mmu_load(struct kvm_vcpu *vcpu)
 	r = mmu_alloc_special_roots(vcpu);
 	if (r)
 		goto out;
+	/*
+	 * 在以下设置kvm_vcpu_arch->mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|8010| <<kvm_mmu_create>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|86| <<nested_svm_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|104| <<nested_svm_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|466| <<nested_ept_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|477| <<nested_ept_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 */
 	if (vcpu->arch.mmu->root_role.direct)
 		r = mmu_alloc_direct_roots(vcpu);
 	else
@@ -5499,6 +7397,13 @@ int kvm_mmu_load(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7095| <<kvm_mmu_reset_context>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|8668| <<kvm_mmu_destroy>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|1186| <<kvm_post_set_cr4>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|12749| <<kvm_unload_vcpu_mmu>> kvm_mmu_unload(vcpu);
+ */
 void kvm_mmu_unload(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -5560,6 +7465,10 @@ void kvm_mmu_free_obsolete_roots(struct kvm_vcpu *vcpu)
 	__kvm_mmu_free_obsolete_roots(vcpu->kvm, &vcpu->arch.guest_mmu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7424| <<kvm_mmu_track_write>> gentry = mmu_pte_write_fetch_gpte(vcpu, &gpa, &bytes);
+ */
 static u64 mmu_pte_write_fetch_gpte(struct kvm_vcpu *vcpu, gpa_t *gpa,
 				    int *bytes)
 {
@@ -5590,6 +7499,10 @@ static u64 mmu_pte_write_fetch_gpte(struct kvm_vcpu *vcpu, gpa_t *gpa,
  * If we're seeing too many writes to a page, it may no longer be a page table,
  * or we may be forking, in which case it is better to unmap the page.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7176| <<kvm_mmu_track_write>> detect_write_flooding(sp)) {
+ */
 static bool detect_write_flooding(struct kvm_mmu_page *sp)
 {
 	/*
@@ -5599,6 +7512,12 @@ static bool detect_write_flooding(struct kvm_mmu_page *sp)
 	if (sp->role.level == PG_LEVEL_4K)
 		return false;
 
+	/*
+	 * 在以下使用kvm_mmu_page->write_flooding_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|3093| <<__clear_sp_write_flooding_count>> atomic_set(&sp->write_flooding_count, 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|7091| <<detect_write_flooding>> atomic_inc(&sp->write_flooding_count);
+	 *   - arch/x86/kvm/mmu/mmu.c|7092| <<detect_write_flooding>> return atomic_read(&sp->write_flooding_count) >= 3;
+	 */
 	atomic_inc(&sp->write_flooding_count);
 	return atomic_read(&sp->write_flooding_count) >= 3;
 }
@@ -5628,6 +7547,10 @@ static bool detect_write_misaligned(struct kvm_mmu_page *sp, gpa_t gpa,
 	return misaligned;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7436| <<kvm_mmu_track_write>> spte = get_written_sptes(sp, gpa, &npte);
+ */
 static u64 *get_written_sptes(struct kvm_mmu_page *sp, gpa_t gpa, int *nspte)
 {
 	unsigned page_offset, quadrant;
@@ -5659,6 +7582,14 @@ static u64 *get_written_sptes(struct kvm_mmu_page *sp, gpa_t gpa, int *nspte)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/page_track.h|55| <<kvm_page_track_write>> kvm_mmu_track_write(vcpu, gpa, new, bytes);
+ *
+ * emulator_write_phys() or emulator_cmpxchg_emulated()
+ * -> kvm_page_track_write()
+ *    -> kvm_mmu_track_write()
+ */
 void kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 			 int bytes)
 {
@@ -5673,6 +7604,13 @@ void kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 	 * If we don't have indirect shadow pages, it means no page is
 	 * write-protected, so we can exit simply.
 	 */
+	/*
+	 * 在以下使用kvm_arch->indirect_shadow_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1084| <<account_shadowed>> kvm->arch.indirect_shadow_pages++;
+	 *   - arch/x86/kvm/mmu/mmu.c|1161| <<unaccount_shadowed>> kvm->arch.indirect_shadow_pages--;
+	 *   - arch/x86/kvm/mmu/mmu.c|7419| <<kvm_mmu_track_write>> if (!READ_ONCE(vcpu->kvm->arch.indirect_shadow_pages))
+	 *   - arch/x86/kvm/x86.c|9045| <<reexecute_instruction>> indirect_shadow_pages = vcpu->kvm->arch.indirect_shadow_pages;
+	 */
 	if (!READ_ONCE(vcpu->kvm->arch.indirect_shadow_pages))
 		return;
 
@@ -5680,8 +7618,21 @@ void kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 
 	gentry = mmu_pte_write_fetch_gpte(vcpu, &gpa, &bytes);
 
+	/*
+	 * 在以下使用kvm_vm_stat->mmu_pte_write:
+	 *   - arch/x86/kvm/x86.c|247| <<global>> STATS_DESC_COUNTER(VM, mmu_pte_write),
+	 *   - arch/x86/kvm/mmu/mmu.c|7426| <<kvm_mmu_track_write>> ++vcpu->kvm->stat.mmu_pte_write;
+	 */
 	++vcpu->kvm->stat.mmu_pte_write;
 
+	/*
+	 * 在以下调用for_each_gfn_valid_sp_with_gptes():
+	 *   - arch/x86/kvm/mmu/mmu.c|4038| <<kvm_mmu_unprotect_page>> for_each_gfn_valid_sp_with_gptes(kvm, sp, gfn) {
+	 *   - arch/x86/kvm/mmu/mmu.c|4128| <<mmu_try_to_unsync_pages>> for_each_gfn_valid_sp_with_gptes(kvm, sp, gfn) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7428| <<kvm_mmu_track_write>> for_each_gfn_valid_sp_with_gptes(vcpu->kvm, sp, gfn) {
+	 *
+	 * 遍历kvm->arch.mmu_page_hash[kvm_page_table_hashfn(_gfn)]中所有和gfn相等的sp
+	 */
 	for_each_gfn_valid_sp_with_gptes(vcpu->kvm, sp, gfn) {
 		if (detect_write_misaligned(sp, gpa, bytes) ||
 		      detect_write_flooding(sp)) {
@@ -5690,12 +7641,19 @@ void kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 			continue;
 		}
 
+		/*
+		 * 把写入后的对应的spte读出来(可能只修改了一部分??)
+		 * 64位下npte是1??
+		 */
 		spte = get_written_sptes(sp, gpa, &npte);
 		if (!spte)
 			continue;
 
 		while (npte--) {
 			entry = *spte;
+			/*
+			 * 这里是核心把, 不会sync, 而是直接zap了!!
+			 */
 			mmu_page_zap_pte(vcpu->kvm, sp, spte, NULL);
 			if (gentry && sp->role.level != PG_LEVEL_4K)
 				++vcpu->kvm->stat.mmu_pde_zapped;
@@ -5708,6 +7666,14 @@ void kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 	write_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4439| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn, insn_len);
+ *   - arch/x86/kvm/svm/svm.c|2059| <<npf_interception>> return kvm_mmu_page_fault(vcpu, fault_address, error_code, static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+ *                                                         svm->vmcb->control.insn_bytes : NULL, svm->vmcb->control.insn_len);
+ *   - arch/x86/kvm/vmx/vmx.c|5887| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5908| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -5725,6 +7691,14 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 	if (WARN_ON_ONCE(error_code & PFERR_IMPLICIT_ACCESS))
 		error_code &= ~PFERR_IMPLICIT_ACCESS;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_mmu *mmu;
+	 *       -> struct kvm_mmu_root_info root;
+	 *          -> gpa_t pgd;     
+	 *          -> hpa_t hpa;
+	 */
 	if (WARN_ON_ONCE(!VALID_PAGE(vcpu->arch.mmu->root.hpa)))
 		return RET_PF_RETRY;
 
@@ -5817,6 +7791,13 @@ static void __kvm_mmu_invalidate_addr(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu
 	write_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7526| <<kvm_mmu_invlpg>> kvm_mmu_invalidate_addr(vcpu, vcpu->arch.walk_mmu, gva, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|7548| <<kvm_mmu_invpcid_gva>> kvm_mmu_invalidate_addr(vcpu, mmu, gva, roots);
+ *   - arch/x86/kvm/vmx/nested.c|375| <<nested_ept_invalidate_addr>> kvm_mmu_invalidate_addr(vcpu, vcpu->arch.mmu, addr, roots);
+ *   - arch/x86/kvm/x86.c|820| <<kvm_inject_emulated_page_fault>> kvm_mmu_invalidate_addr(vcpu, fault_mmu, fault->address, KVM_MMU_ROOT_CURRENT);
+ */
 void kvm_mmu_invalidate_addr(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			     u64 addr, unsigned long roots)
 {
@@ -5890,6 +7871,11 @@ void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)
 	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5214| <<svm_hardware_setup>> kvm_configure_mmu(npt_enabled, get_npt_level(), get_npt_level(), PG_LEVEL_1G);
+ *   - arch/x86/kvm/vmx/vmx.c|8662| <<hardware_setup>> kvm_configure_mmu(enable_ept, 0, vmx_get_max_ept_level(), ept_caps_to_lpage_level(vmx_capability.ept));
+ */
 void kvm_configure_mmu(bool enable_tdp, int tdp_forced_root_level,
 		       int tdp_max_root_level, int tdp_huge_page_level)
 {
@@ -5921,6 +7907,12 @@ typedef bool (*slot_rmaps_handler) (struct kvm *kvm,
 				    struct kvm_rmap_head *rmap_head,
 				    const struct kvm_memory_slot *slot);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6475| <<walk_slot_rmaps>> return __walk_slot_rmaps(kvm, slot, fn, start_level, end_level, slot->base_gfn, slot->base_gfn + slot->npages - 1, flush_on_yield, false);
+ *   - arch/x86/kvm/mmu/mmu.c|6757| <<kvm_rmap_zap_gfn_range>> flush = __walk_slot_rmaps(kvm, memslot, __kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1, true, flush);
+ *   - arch/x86/kvm/mmu/mmu.c|7044| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot, shadow_mmu_try_split_huge_pages, level, level, start, end - 1, true, false);
+ */
 static __always_inline bool __walk_slot_rmaps(struct kvm *kvm,
 					      const struct kvm_memory_slot *slot,
 					      slot_rmaps_handler fn,
@@ -6031,6 +8023,10 @@ static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12227| <<kvm_arch_vcpu_create>> r = kvm_mmu_create(vcpu);
+ */
 int kvm_mmu_create(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -6043,7 +8039,23 @@ int kvm_mmu_create(struct kvm_vcpu *vcpu)
 
 	vcpu->arch.mmu_shadow_page_cache.gfp_zero = __GFP_ZERO;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->mmu: --> 指针
+	 *   - arch/x86/kvm/mmu/mmu.c|8010| <<kvm_mmu_create>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|86| <<nested_svm_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|104| <<nested_svm_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|466| <<nested_ept_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|477| <<nested_ept_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 */
 	vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	/*
+	 * 在以下设置kvm_vcpu_arch->walk_mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|8011| <<kvm_mmu_create>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|105| <<nested_svm_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|468| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|474| <<nested_ept_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 */
 	vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
 
 	ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
@@ -6208,6 +8220,11 @@ static void mmu_free_vm_memory_caches(struct kvm *kvm)
 	kvm_mmu_free_memory_cache(&kvm->arch.split_shadow_page_cache);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12983| <<kvm_arch_init_vm>> kvm_mmu_uninit_vm(kvm);
+ *   - arch/x86/kvm/x86.c|13123| <<kvm_arch_destroy_vm>> kvm_mmu_uninit_vm(kvm);
+ */
 void kvm_mmu_uninit_vm(struct kvm *kvm)
 {
 	if (tdp_mmu_enabled)
@@ -6216,6 +8233,10 @@ void kvm_mmu_uninit_vm(struct kvm *kvm)
 	mmu_free_vm_memory_caches(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6803| <<kvm_zap_gfn_range>> flush = kvm_rmap_zap_gfn_range(kvm, gfn_start, gfn_end);
+ */
 static bool kvm_rmap_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
 {
 	const struct kvm_memory_slot *memslot;
@@ -6251,6 +8272,13 @@ static bool kvm_rmap_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_e
  * Invalidate (zap) SPTEs that cover GFNs from gfn_start and up to gfn_end
  * (not including it)
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|341| <<update_mtrr>> kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));
+ *   - arch/x86/kvm/x86.c|979| <<kvm_post_set_cr0>> kvm_zap_gfn_range(vcpu->kvm, 0, ~0ULL);
+ *   - arch/x86/kvm/x86.c|10724| <<__kvm_set_or_clear_apicv_inhibit>> kvm_zap_gfn_range(kvm, gfn, gfn+1);
+ *   - arch/x86/kvm/x86.c|13543| <<kvm_noncoherent_dma_assignment_start_or_stop>> kvm_zap_gfn_range(kvm, gpa_to_gfn(0), gpa_to_gfn(~0ULL));
+ */
 void kvm_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
 {
 	bool flush;
@@ -6275,6 +8303,10 @@ void kvm_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
 	write_unlock(&kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6829| <<kvm_mmu_slot_remove_write_access>> walk_slot_rmaps(kvm, memslot, slot_rmap_write_protect, start_level, KVM_MAX_HUGEPAGE_LEVEL, false);
+ */
 static bool slot_rmap_write_protect(struct kvm *kvm,
 				    struct kvm_rmap_head *rmap_head,
 				    const struct kvm_memory_slot *slot)
@@ -6282,6 +8314,11 @@ static bool slot_rmap_write_protect(struct kvm *kvm,
 	return rmap_write_protect(rmap_head, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13074| <<kvm_mmu_slot_apply_flags>> kvm_mmu_slot_remove_write_access(kvm, new, PG_LEVEL_2M);
+ *   - arch/x86/kvm/x86.c|13076| <<kvm_mmu_slot_apply_flags>> kvm_mmu_slot_remove_write_access(kvm, new, PG_LEVEL_4K);
+ */
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
 				      const struct kvm_memory_slot *memslot,
 				      int start_level)
@@ -6353,6 +8390,17 @@ static int topup_split_caches(struct kvm *kvm)
 	return kvm_mmu_topup_memory_cache(&kvm->arch.split_shadow_page_cache, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8269| <<shadow_mmu_split_huge_page>> sp = shadow_mmu_get_sp_for_split(kvm, huge_sptep);
+ *
+ * kvm_mmu_try_split_huge_pages() or kvm_mmu_slot_try_split_huge_pages()
+ * -> kvm_shadow_mmu_try_split_huge_pages()
+ *    -> shadow_mmu_try_split_huge_pages()
+ *       -> shadow_mmu_try_split_huge_page()
+ *          -> shadow_mmu_split_huge_page()
+ *             -> shadow_mmu_get_sp_for_split()
+ */
 static struct kvm_mmu_page *shadow_mmu_get_sp_for_split(struct kvm *kvm, u64 *huge_sptep)
 {
 	struct kvm_mmu_page *huge_sp = sptep_to_sp(huge_sptep);
@@ -6361,6 +8409,13 @@ static struct kvm_mmu_page *shadow_mmu_get_sp_for_split(struct kvm *kvm, u64 *hu
 	unsigned int access;
 	gfn_t gfn;
 
+	/*
+	 * 给出一个kvm_mmu_page, 这个page存了一个页表的页, 里面有512个entry
+	 * 每个entry都代表了一个起始的gfn
+	 * sp->gfn本身是这个页表的第一个entry指向的gfn
+	 * kvm_mmu_page_get_gfn()返回参数index代表的entry所指向的gfn
+	 * index是页表中entry的index
+	 */
 	gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
 	access = kvm_mmu_page_get_access(huge_sp, spte_index(huge_sptep));
 
@@ -6380,6 +8435,10 @@ static struct kvm_mmu_page *shadow_mmu_get_sp_for_split(struct kvm *kvm, u64 *hu
 	return __kvm_mmu_get_shadow_page(kvm, NULL, &caches, gfn, role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8343| <<shadow_mmu_try_split_huge_page>> shadow_mmu_split_huge_page(kvm, slot, huge_sptep);
+ */
 static void shadow_mmu_split_huge_page(struct kvm *kvm,
 				       const struct kvm_memory_slot *slot,
 				       u64 *huge_sptep)
@@ -6423,9 +8482,22 @@ static void shadow_mmu_split_huge_page(struct kvm *kvm,
 		__rmap_add(kvm, cache, slot, sptep, gfn, sp->role.access);
 	}
 
+	/*
+	 * link shadow page, link的是shadow page,
+	 * 这里的shadow似乎是指页表页?
+	 * 所以用的是make_nonleaf_spte()
+	 *
+	 * __link_shadow_page()为sptep指向的entry填充
+	 * 使用的pfn是参数sp的sp->spt
+	 * 还要把sptep加入到这个sp的sp->parent_ptes (kvm_rmap_head)
+	 */
 	__link_shadow_page(kvm, cache, huge_sptep, sp, flush);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8379| <<shadow_mmu_try_split_huge_pages>> r = shadow_mmu_try_split_huge_page(kvm, slot, huge_sptep);
+ */
 static int shadow_mmu_try_split_huge_page(struct kvm *kvm,
 					  const struct kvm_memory_slot *slot,
 					  u64 *huge_sptep)
@@ -6465,6 +8537,11 @@ static int shadow_mmu_try_split_huge_page(struct kvm *kvm,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8410| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot, shadow_mmu_try_split_huge_pages,
+ *                                                    level, level, start, end - 1, true, false);
+ */
 static bool shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 					    struct kvm_rmap_head *rmap_head,
 					    const struct kvm_memory_slot *slot)
@@ -6511,6 +8588,11 @@ static bool shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8424| <<kvm_mmu_try_split_huge_pages>> kvm_shadow_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level);
+ *   - arch/x86/kvm/mmu/mmu.c|8450| <<kvm_mmu_slot_try_split_huge_pages>> kvm_shadow_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level);
+ */
 static void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 						const struct kvm_memory_slot *slot,
 						gfn_t start, gfn_t end,
@@ -6529,6 +8611,10 @@ static void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 				  level, level, start, end - 1, true, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1918| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_try_split_huge_pages(kvm, slot, start, end, PG_LEVEL_4K);
+ */
 /* Must be called with the mmu_lock held in write-mode. */
 void kvm_mmu_try_split_huge_pages(struct kvm *kvm,
 				   const struct kvm_memory_slot *memslot,
@@ -6549,6 +8635,10 @@ void kvm_mmu_try_split_huge_pages(struct kvm *kvm,
 	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13074| <<kvm_mmu_slot_apply_flags>> kvm_mmu_slot_try_split_huge_pages(kvm, new, PG_LEVEL_4K);
+ */
 void kvm_mmu_slot_try_split_huge_pages(struct kvm *kvm,
 					const struct kvm_memory_slot *memslot,
 					int target_level)
@@ -6674,6 +8764,16 @@ void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8653| <<kvm_arch_flush_shadow_all>> kvm_mmu_zap_all(kvm);
+ *
+ * kvm_mmu_notifier_release() or kvm_destroy_vm()
+ * -> kvm_flush_shadow_all()
+ *    -> kvm_arch_flush_shadow_all()
+ *       -> kvm_mmu_zap_all()
+ *          -> kvm_tdp_mmu_zap_all()
+ */
 static void kvm_mmu_zap_all(struct kvm *kvm)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -6699,6 +8799,10 @@ static void kvm_mmu_zap_all(struct kvm *kvm)
 	write_unlock(&kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|412| <<kvm_flush_shadow_all>> kvm_arch_flush_shadow_all(kvm);
+ */
 void kvm_arch_flush_shadow_all(struct kvm *kvm)
 {
 	kvm_mmu_zap_all(kvm);
@@ -6971,6 +9075,12 @@ void kvm_mmu_vendor_module_exit(void)
  * Calculate the effective recovery period, accounting for '0' meaning "let KVM
  * select a halving time of 1 hour".  Returns true if recovery is enabled.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7580| <<set_nx_huge_pages_recovery_param>> was_recovery_enabled = calc_nx_huge_pages_recovery_period(&old_period);
+ *   - arch/x86/kvm/mmu/mmu.c|7586| <<set_nx_huge_pages_recovery_param>> is_recovery_enabled = calc_nx_huge_pages_recovery_period(&new_period);
+ *   - arch/x86/kvm/mmu/mmu.c|7703| <<get_nx_huge_page_recovery_timeout>> enabled = calc_nx_huge_pages_recovery_period(&period);
+ */
 static bool calc_nx_huge_pages_recovery_period(uint *period)
 {
 	/*
@@ -7048,6 +9158,13 @@ static void kvm_recover_nx_huge_pages(struct kvm *kvm)
 	ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
 	to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
 	for ( ; to_zap; --to_zap) {
+		/*
+		 * 在以下使用kvm_arch->possible_nx_huge_pages:
+		 *   - arch/x86/kvm/mmu/mmu.c|869| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link, &kvm->arch.possible_nx_huge_pages);
+		 *   - arch/x86/kvm/mmu/mmu.c|6351| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+		 *   - arch/x86/kvm/mmu/mmu.c|7213| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+		 *   - arch/x86/kvm/mmu/mmu.c|7223| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages, struct kvm_mmu_page, possible_nx_huge_page_link);
+		 */
 		if (list_empty(&kvm->arch.possible_nx_huge_pages))
 			break;
 
@@ -7155,10 +9272,23 @@ static int kvm_nx_huge_page_recovery_worker(struct kvm *kvm, uintptr_t data)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12699| <<kvm_arch_post_init_vm>> return kvm_mmu_post_init_vm(kvm);
+ */
 int kvm_mmu_post_init_vm(struct kvm *kvm)
 {
 	int err;
 
+	/*
+	 * 在以下使用nx_hugepage_mitigation_hard_disabled:
+	 *   - arch/x86/kvm/mmu/mmu.c|62| <<global>> static bool nx_hugepage_mitigation_hard_disabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|7389| <<get_nx_huge_pages>> if (nx_hugepage_mitigation_hard_disabled)
+	 *   - arch/x86/kvm/mmu/mmu.c|7411| <<set_nx_huge_pages>> if (nx_hugepage_mitigation_hard_disabled)
+	 *   - arch/x86/kvm/mmu/mmu.c|7429| <<set_nx_huge_pages>> nx_hugepage_mitigation_hard_disabled = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|7577| <<set_nx_huge_pages_recovery_param>> if (nx_hugepage_mitigation_hard_disabled)
+	 *   - arch/x86/kvm/mmu/mmu.c|7738| <<kvm_mmu_post_init_vm>> if (nx_hugepage_mitigation_hard_disabled)
+	 */
 	if (nx_hugepage_mitigation_hard_disabled)
 		return 0;
 
@@ -7171,8 +9301,22 @@ int kvm_mmu_post_init_vm(struct kvm *kvm)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12804| <<kvm_arch_pre_destroy_vm>> kvm_mmu_pre_destroy_vm(kvm);
+ */
 void kvm_mmu_pre_destroy_vm(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->nx_huge_page_recovery_thread:
+	 *   - arch/x86/kvm/mmu/mmu.c|7447| <<set_nx_huge_pages>> wake_up_process(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7595| <<set_nx_huge_pages_recovery_param>> wake_up_process(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7743| <<kvm_mmu_post_init_vm>> err = kvm_vm_create_worker_thread(kvm, kvm_nx_huge_page_recovery_worker, 0,
+	 *                                                                         "kvm-nx-lpage-recovery", &kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7745| <<kvm_mmu_post_init_vm>> kthread_unpark(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7752| <<kvm_mmu_pre_destroy_vm>> if (kvm->arch.nx_huge_page_recovery_thread)
+	 *   - arch/x86/kvm/mmu/mmu.c|7753| <<kvm_mmu_pre_destroy_vm>> kthread_stop(kvm->arch.nx_huge_page_recovery_thread);
+	 */
 	if (kvm->arch.nx_huge_page_recovery_thread)
 		kthread_stop(kvm->arch.nx_huge_page_recovery_thread);
 }
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index decc1f153..f36e1e378 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -36,6 +36,10 @@
 #define INVALID_PAE_ROOT	0
 #define IS_VALID_PAE_ROOT(x)	(!!(x))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3788| <<mmu_alloc_shadow_roots>> mmu->root.hpa = kvm_mmu_get_dummy_root();
+ */
 static inline hpa_t kvm_mmu_get_dummy_root(void)
 {
 	return my_zero_pfn(0) << PAGE_SHIFT;
@@ -56,11 +60,50 @@ struct kvm_mmu_page {
 	struct list_head link;
 	struct hlist_node hash_link;
 
+	/*
+	 * 在以下使用kvm_mmu_page->tdp_mmu_page:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|309| <<tdp_mmu_init_sp>> sp->tdp_mmu_page = true;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|82| <<is_tdp_mmu_page>> static inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return sp->tdp_mmu_page; }
+	 */
 	bool tdp_mmu_page;
+	/*
+	 * 在以下设置kvm_mmu_page->unsync:
+	 *   - arch/x86/kvm/mmu/mmu.c|2301| <<kvm_unlink_unsync_page>> sp->unsync = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|3403| <<kvm_unsync_page>> sp->unsync = 1;
+	 * 在以下使用kvm_mmu_page->unsync:
+	 *   - arch/x86/kvm/mmu/mmu.c|2227| <<mmu_pages_add>> if (sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|2273| <<__mmu_unsync_walk>> } else if (child->unsync) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2299| <<kvm_unlink_unsync_page>> WARN_ON_ONCE(!sp->unsync);
+	 *   - arch/x86/kvm/mmu/mmu.c|2657| <<kvm_mmu_find_shadow_page>> if (role.level > PG_LEVEL_4K && sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|2667| <<kvm_mmu_find_shadow_page>> if (sp->unsync) {
+	 *   - arch/x86/kvm/mmu/mmu.c|3016| <<__link_shadow_page>> if (WARN_ON_ONCE(sp->unsync_children) || sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|3183| <<__kvm_mmu_prepare_zap_page>> if (sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|3442| <<mmu_try_to_unsync_pages>> if (sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|3467| <<mmu_try_to_unsync_pages>> if (READ_ONCE(sp->unsync))
+	 *   - arch/x86/kvm/mmu/mmu.c|4681| <<is_unsync_root>> if (sp->unsync || sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/mmu.c|6629| <<__kvm_mmu_invalidate_addr>> if (sp->unsync) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7340| <<shadow_mmu_try_split_huge_pages>> if (WARN_ON_ONCE(sp->unsync))
+	 *   - arch/x86/kvm/mmu/mmutrace.h|23| <<KVM_MMU_PAGE_ASSIGN>> __entry->unsync = sp->unsync;
+	 *
+	 * kvm_unsync_page()在被mmu_try_to_unsync_pages()调用之前有一个:
+	 *   WARN_ON_ONCE(sp->role.level != PG_LEVEL_4K);
+	 *   kvm_unsync_page(kvm, sp);
+	 *
+	 * 用于最后一级页表页,表示该页的页表项(pte)是否与guest同步(guest是否已更新tlb)
+	 */
 	bool unsync;
 	union {
+		/*
+		 * 如果比kvm->arch.mmu_valid_gen小则表示已失效
+		 */
 		u8 mmu_valid_gen;
 
+		/*
+		 * 在以下使用tdp_mmu_scheduled_root_to_zap:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|927| <<kvm_tdp_mmu_zap_invalidated_roots>> if (!root->tdp_mmu_scheduled_root_to_zap)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|930| <<kvm_tdp_mmu_zap_invalidated_roots>> root->tdp_mmu_scheduled_root_to_zap = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|997| <<kvm_tdp_mmu_invalidate_all_roots>> root->tdp_mmu_scheduled_root_to_zap = true;
+		 */
 		/* Only accessed under slots_lock.  */
 		bool tdp_mmu_scheduled_root_to_zap;
 	};
@@ -77,6 +120,11 @@ struct kvm_mmu_page {
 	 * hash table.
 	 */
 	union kvm_mmu_page_role role;
+	/*
+	 * 似乎对direct是管理地址范围的起始地址对应的gfn
+	 * 对于shadow, 是当前sp的对应的guest的gfn
+	 * 这个值就是当前shadow页表对应的虚机页表在guest os中的gfn
+	 */
 	gfn_t gfn;
 
 	u64 *spt;
@@ -93,18 +141,89 @@ struct kvm_mmu_page {
 	 * convenience and uniformity across guests, the access permissions are
 	 * stored in KVM format (e.g.  ACC_EXEC_MASK) not the raw guest format.
 	 */
+	/*
+	 * 在以下使用kvm_mmu_page->shadowed_translation:
+	 *   - arch/x86/kvm/mmu/mmu.c|781| <<kvm_mmu_page_get_gfn>> return sp->shadowed_translation[index] >> PAGE_SHIFT;
+	 *   - arch/x86/kvm/mmu/mmu.c|795| <<kvm_mmu_page_get_access>> return sp->shadowed_translation[index] & ACC_ALL;
+	 *   - arch/x86/kvm/mmu/mmu.c|816| <<kvm_mmu_page_set_translation>> sp->shadowed_translation[index] = (gfn << PAGE_SHIFT) | access;
+	 *   - arch/x86/kvm/mmu/mmu.c|2104| <<kvm_mmu_free_shadow_page>> free_page((unsigned long )sp->shadowed_translation);
+	 *   - arch/x86/kvm/mmu/mmu.c|2655| <<kvm_mmu_alloc_shadow_page>> sp->shadowed_translation = kvm_mmu_memory_cache_alloc(caches->shadowed_info_cache);
+	 *
+	 * 以前是gfns? -> 所有页表项(pte)对应的gfn
+	 */
 	u64 *shadowed_translation;
 
+	/*
+	 * 在以下使用kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|3722| <<__kvm_mmu_prepare_zap_page>> if (!sp->root_count) {
+	 *   - arch/x86/kvm/mmu/mmu.c|3823| <<kvm_mmu_commit_zap_page>> WARN_ON_ONCE(!sp->role.invalid || sp->root_count);
+	 *   - arch/x86/kvm/mmu/mmu.c|3846| <<kvm_mmu_zap_oldest_mmu_pages>> if (sp->root_count)
+	 *   - arch/x86/kvm/mmu/mmu.c|4819| <<mmu_free_root_page>> else if (!--sp->root_count && sp->role.invalid)
+	 *   - arch/x86/kvm/mmu/mmu.c|4944| <<mmu_alloc_root>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|22| <<KVM_MMU_PAGE_ASSIGN>> __entry->root_count = sp->root_count; \
+	 *
+	 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|105| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|282| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 2);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|791| <<tdp_mmu_zap_root>> WARN_ON_ONCE(!refcount_read(&root->tdp_mmu_root_count));
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|22| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count); 
+	 * 一开始在kvm_tdp_mmu_get_vcpu_root_hpa()设置成了2
+	 */
 	/* Currently serving as active root */
 	union {
 		int root_count;
+		/*
+		 * 一开始在kvm_tdp_mmu_get_vcpu_root_hpa()设置成了2
+		 */
 		refcount_t tdp_mmu_root_count;
 	};
+	/*
+	 * 在以下设置unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|2160| <<mark_unsync>> if (sp->unsync_children++)
+	 *   - arch/x86/kvm/mmu/mmu.c|2193| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 * 在以下使用unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|2194| <<clear_unsync_child_bit>> WARN_ON_ONCE((int )sp->unsync_children < 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|2214| <<__mmu_unsync_walk>> if (child->unsync_children) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2243| <<mmu_unsync_walk>> if (!sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/mmu.c|2471| <<mmu_pages_clear_parents>> } while (!sp->unsync_children);
+	 *   - arch/x86/kvm/mmu/mmu.c|2841| <<__link_shadow_page>> if (WARN_ON_ONCE(sp->unsync_children) || sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|4445| <<is_unsync_root>> if (sp->unsync || sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/mmu.c|6388| <<__kvm_mmu_invalidate_addr>> if (!sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|694| <<FNAME(fetch)>> if (sp->unsync_children &&
+	 *
+	 * 页表页中unsync的pte数
+	 */
 	unsigned int unsync_children;
+	/*
+	 * 在以下使用kvm_mmu_page->parent_ptes:
+	 *   - arch/x86/kvm/mmu/mmu.c|2324| <<mmu_page_add_parent_pte>> pte_list_add(cache, parent_pte, &sp->parent_ptes);
+	 *   - arch/x86/kvm/mmu/mmu.c|2330| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+	 *   - arch/x86/kvm/mmu/mmu.c|2361| <<kvm_mmu_mark_parents_unsync>> for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
+	 *   - arch/x86/kvm/mmu/mmu.c|3288| <<mmu_page_zap_pte>> if (tdp_enabled && invalid_list && child->role.guest_mode && !child->parent_ptes.val)
+	 *   - arch/x86/kvm/mmu/mmu.c|3331| <<kvm_mmu_unlink_parents>> while ((sptep = rmap_get_first(&sp->parent_ptes, &iter)))
+	 *
+	 * 所有指向这个kvm_mmu_page的pte
+	 * 反向映射(rmap), 维护指向自己的上级页表项
+	 */
 	union {
 		struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
+		/*
+		 * 似乎只有tdp_mmu使用kvm_mmu_page->ptep:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|361| <<tdp_mmu_init_sp>> sp->ptep = sptep;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|985| <<kvm_tdp_mmu_zap_sp>> if (WARN_ON_ONCE(!sp->ptep))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|988| <<kvm_tdp_mmu_zap_sp>> old_spte = kvm_tdp_mmu_read_spte(sp->ptep);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|992| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0, sp->gfn, sp->role.level + 1);
+		 */
 		tdp_ptep_t ptep;
 	};
+	/*
+	 * 在以下使用kvm_mmu_page->unsync_child_bitmap:
+	 *   - arch/x86/kvm/mmu/mmu.c|2205| <<mark_unsync>> if (__test_and_set_bit(spte_index(spte), sp->unsync_child_bitmap))
+	 *   - arch/x86/kvm/mmu/mmu.c|2242| <<clear_unsync_child_bit>> __clear_bit(idx, sp->unsync_child_bitmap);
+	 *   - arch/x86/kvm/mmu/mmu.c|2250| <<__mmu_unsync_walk>> for_each_set_bit(i, sp->unsync_child_bitmap, 512) {
+	 *
+	 * 页表页中unsync的spte bitmap
+	 */
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
 	/*
@@ -120,9 +239,21 @@ struct kvm_mmu_page {
 	 * Used out of the mmu-lock to avoid reading spte values while an
 	 * update is in progress; see the comments in __get_spte_lockless().
 	 */
+	/*
+	 * 32bit下,对spte的修改是原子的,因此通过该计数来检测是否正在被修改,
+	 * 如果被改了需要redo
+	 */
 	int clear_spte_count;
 #endif
 
+	/*
+	 * 在以下使用kvm_mmu_page->write_flooding_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|3093| <<__clear_sp_write_flooding_count>> atomic_set(&sp->write_flooding_count, 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|7091| <<detect_write_flooding>> atomic_inc(&sp->write_flooding_count);
+	 *   - arch/x86/kvm/mmu/mmu.c|7092| <<detect_write_flooding>> return atomic_read(&sp->write_flooding_count) >= 3;
+	 *
+	 * 统计从上次使用以来的emulation次数,如果超过一定次数,会把该page给unmap掉
+	 */
 	/* Number of writes since the last time traversal visited this page.  */
 	atomic_t write_flooding_count;
 
@@ -157,8 +288,31 @@ static inline bool kvm_mmu_page_ad_need_write_protect(struct kvm_mmu_page *sp)
 	return kvm_x86_ops.cpu_dirty_log_size && sp->role.guest_mode;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4597| <<direct_map>> base_gfn = gfn_round_for_level(fault->gfn, it.level);
+ *   - arch/x86/kvm/mmu/mmu.c|6156| <<kvm_tdp_page_fault>> gfn_t base = gfn_round_for_level(fault->gfn, fault->max_level);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|292| <<kvm_flush_remote_tlbs_gfn>> kvm_flush_remote_tlbs_range(kvm, gfn_round_for_level(gfn, level),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|880| <<FNAME>> base_gfn = gfn_round_for_level(fault->gfn, it.level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|82| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|209| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|259| <<try_step_up>> iter->gfn = gfn_round_for_level(iter->gfn, iter->level);
+ *
+ * level 5: 向下round到每68719476736个 (256T)
+ * level 4: 向下round到每134217728个 (512G)
+ * level 3: 向下round到每262144个(1G)
+ * level 2: 向下round到每512个(2M)
+ * level 1: 向下round到每1个, 就是没round
+ */
 static inline gfn_t gfn_round_for_level(gfn_t gfn, int level)
 {
+	/*
+	 * level 5: 256T / 4k = 68719476736
+	 * level 4: 512G / 4K = 134217728
+	 * level 3: 1G   / 4K = 262144
+	 * level 2: 2M   / 4K = 512
+	 * level 1: 4K   / 4K = 1
+	 */
 	return gfn & -KVM_PAGES_PER_HPAGE(level);
 }
 
@@ -171,6 +325,16 @@ bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 				    struct kvm_memory_slot *slot, u64 gfn,
 				    int min_level);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|364| <<kvm_flush_remote_tlbs_sptep>> kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
+ *   - arch/x86/kvm/mmu/mmu.c|1140| <<account_shadowed>> kvm_flush_remote_tlbs_gfn(kvm, gfn, PG_LEVEL_4K);
+ *   - arch/x86/kvm/mmu/mmu.c|2038| <<kvm_set_pte_rmap>> kvm_flush_remote_tlbs_gfn(kvm, gfn, level);
+ *   - arch/x86/kvm/mmu/mmu.c|2289| <<__rmap_add>> kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
+ *   - arch/x86/kvm/mmu/mmu.c|4307| <<mmu_set_spte>> kvm_flush_remote_tlbs_gfn(vcpu->kvm, gfn, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|910| <<tdp_mmu_zap_spte_atomic>> kvm_flush_remote_tlbs_gfn(kvm, iter->gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1389| <<tdp_mmu_map_handle_target_level>> kvm_flush_remote_tlbs_gfn(vcpu->kvm, iter->gfn, iter->level);
+ */
 /* Flush the given page (huge or not) of guest memory. */
 static inline void kvm_flush_remote_tlbs_gfn(struct kvm *kvm, gfn_t gfn, int level)
 {
@@ -277,9 +441,21 @@ enum {
 	RET_PF_SPURIOUS,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4259| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true, NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|5756| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, lower_32_bits(error_code), false, &emulation_type);
+ *
+ * 在kvm_arch_async_page_ready()的调用是apf完成后的
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u32 err, bool prefetch, int *emulation_type)
 {
+	/*
+	 * .max_level = KVM_MAX_HUGEPAGE_LEVEL, ---> 3
+	 * .req_level = PG_LEVEL_4K,            ---> 1
+	 * .goal_level = PG_LEVEL_4K,           ---> 1
+	 */
 	struct kvm_page_fault fault = {
 		.addr = cr2_or_gpa,
 		.error_code = err,
@@ -312,6 +488,16 @@ static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	if (!prefetch)
 		vcpu->stat.pf_taken++;
 
+	/*
+	 * 在以下使用kvm_mmu->page_fault:
+	 *   - arch/x86/kvm/mmu/mmu.c|4532| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5113| <<paging64_init_context>> context->page_fault = paging64_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5120| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5224| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu.c|5354| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault;
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|292| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|318| <<kvm_mmu_do_page_fault>> r = vcpu->arch.mmu->page_fault(vcpu, &fault);
+	 */
 	if (IS_ENABLED(CONFIG_RETPOLINE) && fault.is_tdp)
 		r = kvm_tdp_page_fault(vcpu, &fault);
 	else
diff --git a/arch/x86/kvm/mmu/page_track.c b/arch/x86/kvm/mmu/page_track.c
index c87da11f3..029ac5433 100644
--- a/arch/x86/kvm/mmu/page_track.c
+++ b/arch/x86/kvm/mmu/page_track.c
@@ -59,6 +59,11 @@ int kvm_page_track_write_tracking_alloc(struct kvm_memory_slot *slot)
 	return __kvm_page_track_write_tracking_alloc(slot, slot->npages);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/page_track.c|88| <<__kvm_write_track_add_gfn>> update_gfn_write_track(slot, gfn, 1);
+ *   - arch/x86/kvm/mmu/page_track.c|111| <<__kvm_write_track_remove_gfn>> update_gfn_write_track(slot, gfn, -1);
+ */
 static void update_gfn_write_track(struct kvm_memory_slot *slot, gfn_t gfn,
 				   short count)
 {
@@ -66,6 +71,9 @@ static void update_gfn_write_track(struct kvm_memory_slot *slot, gfn_t gfn,
 
 	index = gfn_to_index(gfn, slot->base_gfn, PG_LEVEL_4K);
 
+	/*
+	 * 只在page_track.c使用gfn_write_track[]
+	 */
 	val = slot->arch.gfn_write_track[index];
 
 	if (WARN_ON_ONCE(val + count < 0 || val + count > USHRT_MAX))
@@ -74,6 +82,11 @@ static void update_gfn_write_track(struct kvm_memory_slot *slot, gfn_t gfn,
 	slot->arch.gfn_write_track[index] += count;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1088| <<account_shadowed>> return __kvm_write_track_add_gfn(kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|269| <<kvm_write_track_add_gfn>> __kvm_write_track_add_gfn(kvm, slot, gfn);
+ */
 void __kvm_write_track_add_gfn(struct kvm *kvm, struct kvm_memory_slot *slot,
 			       gfn_t gfn)
 {
@@ -93,6 +106,14 @@ void __kvm_write_track_add_gfn(struct kvm *kvm, struct kvm_memory_slot *slot,
 	 */
 	kvm_mmu_gfn_disallow_lpage(slot, gfn);
 
+	/*
+	 * 在以下调用kvm_mmu_slot_gfn_write_protect():
+	 *   - arch/x86/kvm/mmu/mmu.c|1073| <<account_shadowed>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+	 *   - arch/x86/kvm/mmu/mmu.c|1860| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, start, PG_LEVEL_2M);
+	 *   - arch/x86/kvm/mmu/mmu.c|1865| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, end, PG_LEVEL_2M);
+	 *   - arch/x86/kvm/mmu/mmu.c|1908| <<kvm_vcpu_write_protect_gfn>> return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn, PG_LEVEL_4K);
+	 *   - arch/x86/kvm/mmu/page_track.c|96| <<__kvm_write_track_add_gfn>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+	 */
 	if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
 		kvm_flush_remote_tlbs(kvm);
 }
diff --git a/arch/x86/kvm/mmu/page_track.h b/arch/x86/kvm/mmu/page_track.h
index d4d72ed99..ab5d6f5ec 100644
--- a/arch/x86/kvm/mmu/page_track.h
+++ b/arch/x86/kvm/mmu/page_track.h
@@ -47,6 +47,15 @@ static inline bool kvm_page_track_has_external_user(struct kvm *kvm) { return fa
 
 #endif /* CONFIG_KVM_EXTERNAL_WRITE_TRACKING */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7772| <<emulator_write_phys>> kvm_page_track_write(vcpu, gpa, val, bytes);
+ *   - arch/x86/kvm/x86.c|8039| <<emulator_cmpxchg_emulated>> kvm_page_track_write(vcpu, gpa, new, bytes);
+ *
+ * emulator_write_phys() or emulator_cmpxchg_emulated()
+ * -> kvm_page_track_write()
+ *    -> kvm_mmu_track_write()
+ */
 static inline void kvm_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 					const u8 *new, int bytes)
 {
diff --git a/arch/x86/kvm/mmu/paging_tmpl.h b/arch/x86/kvm/mmu/paging_tmpl.h
index c85255073..f98fa6394 100644
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@ -15,6 +15,12 @@
  *   Avi Kivity   <avi@qumranet.com>
  */
 
+/*
+ * 影子页表
+ * 首先遍历虚机页表,获取GVA映射的GPA,
+ * 然后通过kvm slot,获取GPA对应的HPA,进而在shadow中建立GVA->HPA的映射.
+ */
+
 /*
  * The MMU needs to be able to access/walk 32-bit and 64-bit guest page tables,
  * as well as guest EPT tables, so the code in this file is compiled thrice,
@@ -83,6 +89,11 @@ struct guest_walker {
 	gfn_t table_gfn[PT_MAX_FULL_LEVELS];
 	pt_element_t ptes[PT_MAX_FULL_LEVELS];
 	pt_element_t prefetch_ptes[PTE_PREFETCH_NUM];
+	/*
+	 * 在以下使用guest_walker->pte_gpa[PT_MAX_FULL_LEVELS]:
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|394| <<FNAME(walk_addr_generic)>> walker->pte_gpa[walker->level - 1] = pte_gpa;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|625| <<FNAME(gpte_changed)>> gpa_t base_gpa, pte_gpa = gw->pte_gpa[level - 1];
+	 */
 	gpa_t pte_gpa[PT_MAX_FULL_LEVELS];
 	pt_element_t __user *ptep_user[PT_MAX_FULL_LEVELS];
 	bool pte_writable[PT_MAX_FULL_LEVELS];
@@ -299,6 +310,14 @@ static inline bool FNAME(is_last_gpte)(struct kvm_mmu *mmu,
 /*
  * Fetch a guest pte for a guest virtual address, or for an L2's GPA.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|528| <<FNAME(walk_addr)>> return FNAME(walk_addr_generic)(walker, vcpu, vcpu->arch.mmu, addr,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|887| <<FNAME(gva_to_gpa)>> r = FNAME(walk_addr_generic)(&walker, vcpu, mmu, addr, access);
+ *
+ * 对于64-bit核心思想大概是:
+ * 给一个VM空间的addr, 获取其在VM空间对应的gfn (e.g., guest_walker->gfn)
+ */
 static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 				    struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 				    gpa_t addr, u64 access)
@@ -324,6 +343,9 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 	trace_kvm_mmu_pagetable_walk(addr, access);
 retry_walk:
 	walker->level = mmu->cpu_role.base.level;
+	/*
+	 * 对于64位是cr3的value, 也就是VM中页表的物理地址gpa
+	 */
 	pte           = kvm_mmu_get_guest_pgd(vcpu, mmu);
 	have_ad       = PT_HAVE_ACCESSED_DIRTY(mmu);
 
@@ -368,14 +390,36 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 		--walker->level;
 
 		index = PT_INDEX(addr, walker->level);
+		/*
+		 * 在FNAME(walk_addr_generic)的一开始,
+		 * 对于64位, pte就是cr3的值,
+		 * 也就是VM中页表的root pde的物理地址gpa
+		 *
+		 * 似乎就是>>一下
+		 */
 		table_gfn = gpte_to_gfn(pte);
 		offset    = index * sizeof(pt_element_t);
+		/*
+		 * 不懂, 还是gpa (比如cr3的gpa)
+		 * 除了在下面保存没看到别的用处: walker->pte_gpa[walker->level - 1]
+		 * 还是用table_gfn用的多
+		 */
 		pte_gpa   = gfn_to_gpa(table_gfn) + offset;
 
 		BUG_ON(walker->level < 1);
 		walker->table_gfn[walker->level - 1] = table_gfn;
+		/*
+		 * 在以下使用guest_walker->pte_gpa[PT_MAX_FULL_LEVELS]:
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|394| <<FNAME(walk_addr_generic)>> walker->pte_gpa[walker->level - 1] = pte_gpa;
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|625| <<FNAME(gpte_changed)>> gpa_t base_gpa, pte_gpa = gw->pte_gpa[level - 1];
+		 */
 		walker->pte_gpa[walker->level - 1] = pte_gpa;
 
+		/*
+		 * 对于第一次进入循环, table_gfn应该是M中页表的root pde的物理gfn
+		 *
+		 * 如果没有nested, 似乎直接返回gpa
+		 */
 		real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn),
 					     nested_access, &walker->fault);
 
@@ -392,15 +436,57 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 		if (unlikely(real_gpa == INVALID_GPA))
 			return 0;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|994| <<gfn_to_memslot_dirty_bitmap>> slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+		 *   - arch/x86/kvm/mmu/mmu.c|1720| <<kvm_vcpu_write_protect_gfn>> slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+		 *   - arch/x86/kvm/mmu/mmu_internal.h|335| <<kvm_mmu_do_page_fault>> fault.slot = kvm_vcpu_gfn_to_memslot(vcpu, fault.gfn);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|400| <<FNAME(walk_addr_generic)>> slot = kvm_vcpu_gfn_to_memslot(vcpu, gpa_to_gfn(real_gpa));
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|809| <<FNAME(page_fault)>> fault->slot = kvm_vcpu_gfn_to_memslot(vcpu, fault->gfn);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|989| <<FNAME(sync_spte)>> slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+		 *   - virt/kvm/kvm_main.c|2401| <<kvm_vcpu_is_visible_gfn>> struct kvm_memory_slot *memslot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+		 *   - virt/kvm/kvm_main.c|2472| <<kvm_vcpu_gfn_to_hva>> return gfn_to_hva_many(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, NULL);
+		 *   - virt/kvm/kvm_main.c|2504| <<kvm_vcpu_gfn_to_hva_prot>> struct kvm_memory_slot *slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+		 *   - virt/kvm/kvm_main.c|2809| <<kvm_vcpu_gfn_to_pfn_atomic>> return gfn_to_pfn_memslot_atomic(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
+		 *   - virt/kvm/kvm_main.c|2821| <<kvm_vcpu_gfn_to_pfn>> return gfn_to_pfn_memslot(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
+		 *   - virt/kvm/kvm_main.c|3062| <<kvm_vcpu_read_guest_page>> struct kvm_memory_slot *slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+		 *   - virt/kvm/kvm_main.c|3129| <<kvm_vcpu_read_guest_atomic>> struct kvm_memory_slot *slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+		 *   - virt/kvm/kvm_main.c|3165| <<kvm_vcpu_write_guest_page>> struct kvm_memory_slot *slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+		 *   - virt/kvm/kvm_main.c|3410| <<kvm_vcpu_mark_page_dirty>> memslot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+		 *
+		 * 对于第一次进入循环, table_gfn应该是VM中页表的root pde的物理gfn
+		 * 所以real_gpa应该是VM中页表的root pde的物理gpa
+		 */
 		slot = kvm_vcpu_gfn_to_memslot(vcpu, gpa_to_gfn(real_gpa));
 		if (!kvm_is_visible_memslot(slot))
 			goto error;
 
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/mmu.c|1683| <<kvm_handle_guest_abort>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+		 *   - arch/loongarch/kvm/mmu.c|771| <<kvm_map_page>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writeable);
+		 *   - arch/riscv/kvm/vcpu_exit.c|25| <<gstage_page_fault>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+		 *   - arch/s390/kvm/gaccess.c|1006| <<access_guest_page_with_key>> hva = gfn_to_hva_memslot_prot(slot, gfn, &writable);
+		 *   - arch/s390/kvm/gaccess.c|1184| <<cmpxchg_guest_abs_with_key>> hva = gfn_to_hva_memslot_prot(slot, gfn, &writable);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|413| <<FNAME(walk_addr_generic)>> host_addr = gfn_to_hva_memslot_prot(slot, gpa_to_gfn(real_gpa), &walker->pte_writable[walker->level - 1]);
+		 *   - virt/kvm/kvm_main.c|2517| <<gfn_to_hva_prot>> return gfn_to_hva_memslot_prot(slot, gfn, writable);
+		 *   - virt/kvm/kvm_main.c|2524| <<kvm_vcpu_gfn_to_hva_prot>> return gfn_to_hva_memslot_prot(slot, gfn, writable);
+		 *   - virt/kvm/kvm_main.c|3059| <<__kvm_read_guest_page>> addr = gfn_to_hva_memslot_prot(slot, gfn, NULL);
+		 *   - virt/kvm/kvm_main.c|3132| <<__kvm_read_guest_atomic>> addr = gfn_to_hva_memslot_prot(slot, gfn, NULL);
+		 *
+		 * 对于第一次进入循环, table_gfn应该是VM中页表的root pde的物理gfn
+		 * 所以real_gpa应该是VM中页表的root pde的物理gpa
+		 * 这里获得根目录的页面在host space的hva
+		 */
 		host_addr = gfn_to_hva_memslot_prot(slot, gpa_to_gfn(real_gpa),
 					    &walker->pte_writable[walker->level - 1]);
 		if (unlikely(kvm_is_error_hva(host_addr)))
 			goto error;
 
+		/*
+		 * pt_element_t __user *ptep_user;
+		 * pt_element_t pte;
+		 */
 		ptep_user = (pt_element_t __user *)((void *)host_addr + offset);
 		if (unlikely(__get_user(pte, ptep_user)))
 			goto error;
@@ -445,6 +531,15 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 		gfn += pse36_gfn_delta(pte);
 #endif
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4061| <<nonpaging_gva_to_gpa>> return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|379| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn),
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|448| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(gfn), access, &walker->fault);
+	 *   - arch/x86/kvm/x86.c|891| <<load_pdptrs>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn),
+	 *
+	 * 如果没有nested, 似乎直接返回gpa
+	 */
 	real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(gfn), access, &walker->fault);
 	if (real_gpa == INVALID_GPA)
 		return 0;
@@ -558,10 +653,39 @@ FNAME(prefetch_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	return true;
 }
 
+/*
+ * //
+ * // The guest_walker structure emulates the behavior of the hardware page
+ * // table walker.
+ * //
+ * struct guest_walker {
+ *     int level;
+ *     unsigned max_level;
+ *     gfn_t table_gfn[PT_MAX_FULL_LEVELS];
+ *     pt_element_t ptes[PT_MAX_FULL_LEVELS];
+ *     pt_element_t prefetch_ptes[PTE_PREFETCH_NUM];
+ *     gpa_t pte_gpa[PT_MAX_FULL_LEVELS];
+ *     pt_element_t __user *ptep_user[PT_MAX_FULL_LEVELS];
+ *     bool pte_writable[PT_MAX_FULL_LEVELS];
+ *     unsigned int pt_access[PT_MAX_FULL_LEVELS];
+ *     unsigned int pte_access; 
+ *     gfn_t gfn;
+ *     struct x86_exception fault;
+ * };
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|708| <<FNAME(fetch)>> if (FNAME(gpte_changed)(vcpu, gw, top_level))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|763| <<FNAME(fetch)>> if (FNAME(gpte_changed)(vcpu, gw, it.level - 1))
+ */
 static bool FNAME(gpte_changed)(struct kvm_vcpu *vcpu,
 				struct guest_walker *gw, int level)
 {
 	pt_element_t curr_pte;
+	/*
+	 * 在以下使用guest_walker->pte_gpa[PT_MAX_FULL_LEVELS]:
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|394| <<FNAME(walk_addr_generic)>> walker->pte_gpa[walker->level - 1] = pte_gpa;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|625| <<FNAME(gpte_changed)>> gpa_t base_gpa, pte_gpa = gw->pte_gpa[level - 1];
+	 */
 	gpa_t base_gpa, pte_gpa = gw->pte_gpa[level - 1];
 	u64 mask;
 	int r, index;
@@ -619,6 +743,18 @@ static void FNAME(pte_prefetch)(struct kvm_vcpu *vcpu, struct guest_walker *gw,
 	}
 }
 
+/*
+ * 遍历影子页表,完成创建影子页表(填充影子页表).
+ * 在填充过程中,将客户机页目录结构页对应影子页表页表项标记为写保护,
+ * 目的截获对于页目录的修改(页目录也是内存页的一部分,在页表中也是有映射的,
+ * guest对页目录有写权限,那么在影子页表的页目录也是可写的,
+ * 这样对页目录的修改导致VMM失去截获的机会)
+ *
+ * 下面的三个配对:
+ * - kvm_tdp_mmu_map()
+ * - direct_map()
+ * - FNAME(fetch)
+ */
 /*
  * Fetch a shadow pte for a specific level in the paging hierarchy.
  * If the guest tries to write a write-protected page, we need to
@@ -662,6 +798,15 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 		goto out_gpte_changed;
 	}
 
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;
+	 *     hpa_t shadow_addr;
+	 *     u64 *sptep;
+	 *     int level;
+	 *     unsigned index;
+	 * };
+	 */
 	for_each_shadow_entry(vcpu, fault->addr, it) {
 		gfn_t table_gfn;
 
@@ -703,6 +848,15 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 		if (FNAME(gpte_changed)(vcpu, gw, it.level - 1))
 			goto out_gpte_changed;
 
+		/*
+		 * link shadow page, link的是shadow page,
+		 * 这里的shadow似乎是指页表页?
+		 * 所以用的是make_nonleaf_spte()
+		 *
+		 * __link_shadow_page()为sptep指向的entry填充
+		 * 使用的pfn是参数sp的sp->spt
+		 * 还要把sptep加入到这个sp的sp->parent_ptes (kvm_rmap_head)
+		 */
 		if (sp != ERR_PTR(-EEXIST))
 			link_shadow_page(vcpu, it.sptep, sp);
 
@@ -710,6 +864,12 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 			fault->write_fault_to_shadow_pgtable = true;
 	}
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4603| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|868| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1614| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	/*
 	 * Adjust the hugepage size _after_ resolving indirect shadow pages.
 	 * KVM doesn't support mapping hugepages into the guest for gfns that
@@ -718,6 +878,12 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace:
+	 *   - arch/x86/kvm/mmu/mmu.c|4605| <<direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|870| <<FNAME>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1616| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
@@ -739,6 +905,15 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 		if (sp == ERR_PTR(-EEXIST))
 			continue;
 
+		/*
+		 * link shadow page, link的是shadow page,
+		 * 这里的shadow似乎是指页表页?
+		 * 所以用的是make_nonleaf_spte()
+		 *
+		 * __link_shadow_page()为sptep指向的entry填充
+		 * 使用的pfn是参数sp的sp->spt
+		 * 还要把sptep加入到这个sp的sp->parent_ptes (kvm_rmap_head)
+		 */
 		link_shadow_page(vcpu, it.sptep, sp);
 		if (fault->huge_page_disallowed)
 			account_nx_huge_page(vcpu->kvm, sp,
@@ -748,6 +923,15 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	if (WARN_ON_ONCE(it.level != fault->goal_level))
 		return -EFAULT;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2991| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, slot, start, access, gfn, page_to_pfn(pages[i]), NULL);
+	 *   - arch/x86/kvm/mmu/mmu.c|3252| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL, base_gfn, fault->pfn, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|556| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, slot, spte, pte_access, gfn, pfn, NULL);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|751| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access, base_gfn, fault->pfn, fault);
+	 *
+	 * 在kvm_faultin_pfn()获得的fault->pfn
+	 */
 	ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access,
 			   base_gfn, fault->pfn, fault);
 	if (ret == RET_PF_SPURIOUS)
@@ -760,6 +944,30 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	return RET_PF_RETRY;
 }
 
+/*
+ * 设置.
+ *
+ * 317 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
+ * 318                                         u32 err, bool prefetch, int *emulation_type)
+ * 319 {
+ * 320         struct kvm_page_fault fault = {
+ * 321                 .addr = cr2_or_gpa,
+ * 322                 .error_code = err,
+ * 323                 .exec = err & PFERR_FETCH_MASK,
+ * 324                 .write = err & PFERR_WRITE_MASK,
+ * 325                 .present = err & PFERR_PRESENT_MASK,
+ * 326                 .rsvd = err & PFERR_RSVD_MASK,
+ * 327                 .user = err & PFERR_USER_MASK,
+ * 328                 .prefetch = prefetch,
+ * 329                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 330                 .nx_huge_page_workaround_enabled =
+ * 331                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 332
+ * 333                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 334                 .req_level = PG_LEVEL_4K,
+ * 335                 .goal_level = PG_LEVEL_4K,
+ * 336         };
+ */
 /*
  * Page fault handler.  There are several causes for a page fault:
  *   - there is no shadow pte for the guest pte
@@ -774,6 +982,32 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
  *  Returns: 1 if we need to emulate the instruction, 0 otherwise, or
  *           a negative value on error.
  */
+/*
+ * 1. 初始状态,虚机初始化了页表,不过此时的SPTE为空
+ *
+ * 2. 当实际访问的过程中,会使用SPTE作为页表,由于此时页表为空,虚机会因为产生
+ *    page fault而vm_exit,这里简单说下,虚机产生page fault时,是否会vm_exit是
+ *    由于vmcs中的EXCEPTION_BITMAP域决定的,比如具有EPT功能时,虚机page fault
+ *    就无需vm_exit.而对于没有EPT情况,是需要vm_exit.当产生vm_exit时,
+ *    handle_exception中判断,如果是由于page fault引起的,就会调用FNAME(page_fault)
+ *    来处理,进而处理shadow缺页情况.
+ *
+ * 3. 每有一个虚机页表,都有一个shadow页表与之对应,而每一个shadow页表都有一个
+ *    struct kvm_mmu_page来描述,这里有个关键成员就是gfn,这个值就是当前shadow页
+ *    表对应的虚机页表在guest os中的GPA. 同时,所有的struct kvm_mmu_page结构会
+ *    以gfn为键值,维护在mmu_page_hash链式哈希中.
+ *
+   4. SPT一级级建立好,最后一级的SPTE中对应的就是GPA对应的HPA,这样便可以访问了.
+ *
+ * 在以下使用kvm_mmu->page_fault:
+ *   - arch/x86/kvm/mmu/mmu.c|4532| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+ *   - arch/x86/kvm/mmu/mmu.c|5113| <<paging64_init_context>> context->page_fault = paging64_page_fault;
+ *   - arch/x86/kvm/mmu/mmu.c|5120| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+ *   - arch/x86/kvm/mmu/mmu.c|5224| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ *   - arch/x86/kvm/mmu/mmu.c|5354| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault;
+ *   - arch/x86/kvm/mmu/mmu_internal.h|292| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *   - arch/x86/kvm/mmu/mmu_internal.h|318| <<kvm_mmu_do_page_fault>> r = vcpu->arch.mmu->page_fault(vcpu, &fault);
+ */
 static int FNAME(page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct guest_walker walker;
@@ -786,6 +1020,10 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	 * If PFEC.RSVD is set, this is a shadow page fault.
 	 * The bit needs to be cleared before walking guest page tables.
 	 */
+	/*
+	 * 对于64-bit核心思想大概是:
+	 * 给一个VM空间的addr, 获取其在VM空间对应的gfn (e.g., guest_walker->gfn)
+	 */
 	r = FNAME(walk_addr)(&walker, vcpu, fault->addr,
 			     fault->error_code & ~PFERR_RSVD_MASK);
 
@@ -803,6 +1041,12 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	fault->max_level = walker.level;
 	fault->slot = kvm_vcpu_gfn_to_memslot(vcpu, fault->gfn);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4380| <<direct_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/mmu.c|4460| <<kvm_tdp_mmu_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|806| <<FNAME(page_fault)>> if (page_fault_handle_page_track(vcpu, fault)) {
+	 */
 	if (page_fault_handle_page_track(vcpu, fault)) {
 		shadow_page_table_clear_flood(vcpu, fault->addr);
 		return RET_PF_EMULATE;
@@ -812,6 +1056,14 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (r)
 		return r;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4391| <<direct_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+	 *   - arch/x86/kvm/mmu/mmu.c|4471| <<kvm_tdp_mmu_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|815| <<FNAME(page_fault)>> r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
+	 *
+	 * 似乎是获得fault->pfn???
+	 */
 	r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
 	if (r != RET_PF_CONTINUE)
 		return r;
@@ -838,9 +1090,22 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	r = RET_PF_RETRY;
 	write_lock(&vcpu->kvm->mmu_lock);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4398| <<direct_page_fault>> if (is_page_fault_stale(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/mmu.c|4478| <<kvm_tdp_mmu_page_fault>> if (is_page_fault_stale(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|841| <<FNAME(page_fault)>> if (is_page_fault_stale(vcpu, fault))
+	 */
 	if (is_page_fault_stale(vcpu, fault))
 		goto out_unlock;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4359| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+	 *   - arch/x86/kvm/mmu/mmu.c|4498| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+	 *   - arch/x86/kvm/mmu/mmu.c|5124| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|934| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+	 */
 	r = make_mmu_pages_available(vcpu);
 	if (r)
 		goto out_unlock;
@@ -852,6 +1117,17 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1160| <<FNAME(sync_spte)>> first_pte_gpa = FNAME(get_level1_sp_gpa)(sp);
+ *
+ * 关于sp->gfn:
+ * 似乎对direct是管理地址范围的起始地址对应的gfn
+ * 对于shadow, 是当前sp的对应的guest的gfn
+ *
+ * 对于64位,这个FNAME(get_level1_sp_gpa)返回sp对应的页表的第一个entry的VM的gfn
+ * 32位不太一样
+ */
 static gpa_t FNAME(get_level1_sp_gpa)(struct kvm_mmu_page *sp)
 {
 	int offset = 0;
@@ -861,9 +1137,29 @@ static gpa_t FNAME(get_level1_sp_gpa)(struct kvm_mmu_page *sp)
 	if (PTTYPE == 32)
 		offset = sp->role.quadrant << SPTE_LEVEL_BITS;
 
+	/*
+	 * 似乎对direct是管理地址范围的起始地址对应的gfn
+	 * 对于shadow, 是当前sp的对应的guest的gfn
+	 */
 	return gfn_to_gpa(sp->gfn) + offset * sizeof(pt_element_t);
 }
 
+/*
+ * struct guest_walker {
+ *     int level;
+ *     unsigned max_level;
+ *     gfn_t table_gfn[PT_MAX_FULL_LEVELS];
+ *     pt_element_t ptes[PT_MAX_FULL_LEVELS];
+ *     pt_element_t prefetch_ptes[PTE_PREFETCH_NUM];
+ *     gpa_t pte_gpa[PT_MAX_FULL_LEVELS];
+ *     pt_element_t __user *ptep_user[PT_MAX_FULL_LEVELS];
+ *     bool pte_writable[PT_MAX_FULL_LEVELS];
+ *     unsigned int pt_access[PT_MAX_FULL_LEVELS];
+ *     unsigned int pte_access;
+ *     gfn_t gfn;
+ *     struct x86_exception fault;
+ * };
+ */
 /* Note, @addr is a GPA when gva_to_gpa() translates an L2 GPA to an L1 GPA. */
 static gpa_t FNAME(gva_to_gpa)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 			       gpa_t addr, u64 access,
@@ -900,6 +1196,22 @@ static gpa_t FNAME(gva_to_gpa)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
  *   0: the spte is synced and no tlb flushing is required
  * > 0: the spte is synced and tlb flushing is required
  */
+/*
+ * 在以下使用kvm_mmu->sync_spte:
+ *   - arch/x86/kvm/mmu/mmu.c|2351| <<kvm_sync_page_check>> if (WARN_ON_ONCE(sp->role.direct || !vcpu->arch.mmu->sync_spte ||
+ *   - arch/x86/kvm/mmu/mmu.c|2363| <<kvm_sync_spte>> return vcpu->arch.mmu->sync_spte(vcpu, sp, i);
+ *   - arch/x86/kvm/mmu/mmu.c|5219| <<nonpaging_init_context>> context->sync_spte = NULL;
+ *   - arch/x86/kvm/mmu/mmu.c|5812| <<paging64_init_context>> context->sync_spte = paging64_sync_spte;
+ *   - arch/x86/kvm/mmu/mmu.c|5819| <<paging32_init_context>> context->sync_spte = paging32_sync_spte;
+ *   - arch/x86/kvm/mmu/mmu.c|5922| <<init_kvm_tdp_mmu>> context->sync_spte = NULL;
+ *   - arch/x86/kvm/mmu/mmu.c|6053| <<kvm_init_shadow_ept_mmu>> context->sync_spte = ept_sync_spte;
+ *   - arch/x86/kvm/mmu/mmu.c|6094| <<init_kvm_nested_mmu>> g_context->sync_spte = NULL;
+ *   - arch/x86/kvm/mmu/mmu.c|6574| <<kvm_mmu_invalidate_addr>> if (!mmu->sync_spte)
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|979| <<FNAME>> static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int i)
+ *
+ * 给一个sp, 通过sp->gfn和一个index, 读出这个entry在VM的内存的值
+ * 这个值就是这个page table entry所指向的VM的gfn
+ */
 static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int i)
 {
 	bool host_writable;
@@ -914,6 +1226,16 @@ static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int
 	if (WARN_ON_ONCE(!sp->spt[i]))
 		return 0;
 
+	/*
+	 * 对于64位, 比如说first_pte_gpa是sp所能指向的最早的gpa --> 似乎不对
+	 *
+	 * 关于sp->gfn:
+	 * 似乎对direct是管理地址范围的起始地址对应的gfn
+	 * 对于shadow, 是当前sp的对应的guest的gfn
+	 *
+	 * 对于64位,这个FNAME(get_level1_sp_gpa)返回sp对应的页表的第一个entry的VM的gfn
+	 * 32位不太一样
+	 */
 	first_pte_gpa = FNAME(get_level1_sp_gpa)(sp);
 	pte_gpa = first_pte_gpa + i * sizeof(pt_element_t);
 
@@ -956,10 +1278,24 @@ static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int
 	/* Update the shadowed access bits in case they changed. */
 	kvm_mmu_page_set_access(sp, i, pte_access);
 
+	/*
+	 * struct kvm_mmu_page *sp:
+	 * -> u64 *spt;
+	 * -> u64 *shadowed_translation;
+	 */
 	sptep = &sp->spt[i];
 	spte = *sptep;
 	host_writable = spte & shadow_host_writable_mask;
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2942| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch, true, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|963| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access, gfn, spte_to_pfn(spte), spte, true, false, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|976| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn,
+	 *                                                                                              iter->old_spte, fault->prefetch, true, fault->map_writable, &new_spte);
+	 *
+	 * gfn在这里用到了!!!!!!!
+	 */
 	make_spte(vcpu, sp, slot, pte_access, gfn,
 		  spte_to_pfn(spte), spte, true, false,
 		  host_writable, &spte);
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 4a599130e..946621b63 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -25,10 +25,27 @@ module_param_named(mmio_caching, enable_mmio_caching, bool, 0444);
 EXPORT_SYMBOL_GPL(enable_mmio_caching);
 
 u64 __read_mostly shadow_host_writable_mask;
+/*
+ * 在以下使用shadow_mmu_writable_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|1714| <<spte_write_protect>> spte &= ~shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|252| <<make_spte>> spte |= PT_WRITABLE_MASK | shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|272| <<make_spte>> spte &= ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ *   - arch/x86/kvm/mmu/spte.c|395| <<kvm_mmu_changed_pte_notifier_make_spte>> new_spte &= ~shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|503| <<kvm_mmu_set_ept_masks>> shadow_mmu_writable_mask = EPT_SPTE_MMU_WRITABLE;
+ *   - arch/x86/kvm/mmu/spte.c|563| <<kvm_mmu_reset_all_pte_masks>> shadow_mmu_writable_mask = DEFAULT_SPTE_MMU_WRITABLE;
+ *   - arch/x86/kvm/mmu/spte.h|532| <<check_spte_writable_invariants>> if (spte & shadow_mmu_writable_mask)
+ *   - arch/x86/kvm/mmu/spte.h|543| <<is_mmu_writable_spte>> return spte & shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|2072| <<write_protect_gfn>> ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ */
 u64 __read_mostly shadow_mmu_writable_mask;
 u64 __read_mostly shadow_nx_mask;
 u64 __read_mostly shadow_x_mask; /* mutual exclusive with nx_mask */
 u64 __read_mostly shadow_user_mask;
+/*
+ * 在以下设置shadow_accessed_mask:
+ *   - arch/x86/kvm/mmu/spte.c|466| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|523| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+ */
 u64 __read_mostly shadow_accessed_mask;
 u64 __read_mostly shadow_dirty_mask;
 u64 __read_mostly shadow_mmio_value;
@@ -57,6 +74,15 @@ void __init kvm_mmu_spte_module_init(void)
 	allow_mmio_caching = enable_mmio_caching;
 }
 
+/*
+ * 以前的注释
+ * spte bits 3-11 are used as bits 1-9 of the generation number,
+ * the bits 52-61 are used as bits 10-19 of the generation number.
+ *
+ * 把gen的第1-9位取出来左移2位, 变成spte的3-11位
+ * 把gen的第10-19位取出来左移52位, 变成spte的52-61位
+ * 把两个拼在一起, 成为了spte的gen mask
+ */
 static u64 generation_mmio_spte_mask(u64 gen)
 {
 	u64 mask;
@@ -68,9 +94,23 @@ static u64 generation_mmio_spte_mask(u64 gen)
 	return mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|373| <<mark_mmio_spte>> u64 spte = make_mmio_spte(vcpu, gfn, access);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1024| <<tdp_mmu_map_handle_target_level>> new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
+ */
 u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)
 {
 	u64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;
+	/*
+	 * 以前的注释
+	 * spte bits 3-11 are used as bits 1-9 of the generation number,
+	 * the bits 52-61 are used as bits 10-19 of the generation number.
+	 *
+	 * 把gen的第1-9位取出来左移2位, 变成spte的3-11位
+	 * 把gen的第10-19位取出来左移52位, 变成spte的52-61位
+	 * 把两个拼在一起, 成为了spte的gen mask
+	 */
 	u64 spte = generation_mmio_spte_mask(gen);
 	u64 gpa = gfn << PAGE_SHIFT;
 
@@ -111,6 +151,14 @@ static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
  * The caller is responsible for checking if the SPTE is shadow-present, and
  * for determining whether or not the caller cares about non-leaf SPTEs.
  */
+/*
+ * volatile bits: bits that can be set outside of mmu_lock. The Writable bit
+ * can be set by KVM's fast page fault handler, and Accessed and Dirty bits
+ * can be set by the CPU.
+ *
+ * 也就是说,volatile bits是那些可以在lock外修改的bit,比如在fast page fault中
+ * 的writable,或者硬件支持的access/dirty bit.
+ */
 bool spte_has_volatile_bits(u64 spte)
 {
 	/*
@@ -134,6 +182,13 @@ bool spte_has_volatile_bits(u64 spte)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2942| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch, true, host_writable, &spte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|963| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access, gfn, spte_to_pfn(spte), spte, true, false, host_writable, &spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|976| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn,
+ *                                                                                              iter->old_spte, fault->prefetch, true, fault->map_writable, &new_spte);
+ */
 bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	       const struct kvm_memory_slot *slot,
 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
@@ -141,6 +196,9 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	       bool host_writable, u64 *new_spte)
 {
 	int level = sp->role.level;
+	/*
+	 * spte是本地变量新申请的!!!
+	 */
 	u64 spte = SPTE_MMU_PRESENT_MASK;
 	bool wrprot = false;
 
@@ -307,13 +365,28 @@ u64 make_huge_page_split_spte(struct kvm *kvm, u64 huge_spte, union kvm_mmu_page
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3347| <<__link_shadow_page>> spte = make_nonleaf_spte(sp->spt, sp_ad_disabled(sp));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1077| <<tdp_mmu_link_sp>> u64 spte = make_nonleaf_spte(sp->spt, !kvm_ad_enabled());
+ */
 u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 {
+	/*
+	 * bit 11
+	 */
 	u64 spte = SPTE_MMU_PRESENT_MASK;
 
 	spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK |
 		shadow_user_mask | shadow_x_mask | shadow_me_value;
 
+	/*
+	 * 在以下设置shadow_accessed_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|466| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+	 *   - arch/x86/kvm/mmu/spte.c|523| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+	 *
+	 * SPTE_TDP_AD_DISABLED是1<<52, 不支持ad就用bit=52
+	 */
 	if (ad_disabled)
 		spte |= SPTE_TDP_AD_DISABLED;
 	else
@@ -322,6 +395,11 @@ u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2029| <<kvm_set_pte_rmap>> new_spte = kvm_mmu_changed_pte_notifier_make_spte(*sptep, new_pfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1908| <<set_spte_gfn>> new_spte = kvm_mmu_changed_pte_notifier_make_spte(iter->old_spte, pte_pfn(range->arg.pte));
+ */
 u64 kvm_mmu_changed_pte_notifier_make_spte(u64 old_spte, kvm_pfn_t new_pfn)
 {
 	u64 new_spte;
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index a129951c9..73d9a164e 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -6,6 +6,15 @@
 #include "mmu.h"
 #include "mmu_internal.h"
 
+/*
+ * 在以下使用SPTE_MMU_PRESENT_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|124| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK & (MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+ *   - arch/x86/kvm/mmu/spte.h|139| <<global>> static_assert(!(SPTE_MMIO_ALLOWED_MASK & (SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+ *   - arch/x86/kvm/mmu/spte.h|200| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|190| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|363| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|308| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+ */
 /*
  * A MMU present SPTE is backed by actual memory and may or may not be present
  * in hardware.  E.g. MMIO SPTEs are not considered present.  Use bit 11, as it
@@ -199,6 +208,14 @@ extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
 /* Removed SPTEs must not be misconstrued as shadow present PTEs. */
 static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|356| <<handle_removed_pt>> if (!is_removed_spte(old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|487| <<handle_changed_spte>> !is_removed_spte(new_spte)))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|549| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded || is_removed_spte(iter->old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|624| <<tdp_mmu_set_spte>> WARN_ON_ONCE(is_removed_spte(old_spte) || is_removed_spte(new_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1074| <<kvm_tdp_mmu_map>> if (is_removed_spte(iter.old_spte))
+ */
 static inline bool is_removed_spte(u64 spte)
 {
 	return spte == REMOVED_SPTE;
@@ -220,6 +237,11 @@ static inline int spte_index(u64 *sptep)
  */
 extern u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.h|232| <<spte_to_child_sp>> return to_shadow_page(spte & SPTE_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/spte.h|237| <<sptep_to_sp>> return to_shadow_page(__pa(sptep));
+ */
 static inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)
 {
 	struct page *page = pfn_to_page((shadow_page) >> PAGE_SHIFT);
@@ -237,6 +259,20 @@ static inline struct kvm_mmu_page *sptep_to_sp(u64 *sptep)
 	return to_shadow_page(__pa(sptep));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3554| <<mmu_free_root_page>> sp = root_to_sp(*root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|3602| <<kvm_mmu_free_roots>> } else if (root_to_sp(mmu->root.hpa)) {
+ *   - arch/x86/kvm/mmu/mmu.c|3641| <<kvm_mmu_free_guest_mode_roots>> sp = root_to_sp(root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|3986| <<is_unsync_root>> sp = root_to_sp(root);
+ *   - arch/x86/kvm/mmu/mmu.c|4020| <<kvm_mmu_sync_roots>> sp = root_to_sp(root);
+ *   - arch/x86/kvm/mmu/mmu.c|4351| <<is_page_fault_stale>> struct kvm_mmu_page *sp = root_to_sp(vcpu->arch.mmu->root.hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4548| <<is_root_usable>> sp = root_to_sp(root->hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4625| <<fast_pgd_switch>> if (VALID_PAGE(mmu->root.hpa) && !root_to_sp(mmu->root.hpa))
+ *   - arch/x86/kvm/mmu/mmu.c|4672| <<kvm_mmu_new_pgd>> struct kvm_mmu_page *sp = root_to_sp(vcpu->arch.mmu->root.hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|5553| <<is_obsolete_root>> sp = root_to_sp(root_hpa);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|652| <<tdp_mmu_for_each_pte>> for_each_tdp_pte(_iter, root_to_sp(_mmu->root.hpa), _start, _end)
+ */
 static inline struct kvm_mmu_page *root_to_sp(hpa_t root)
 {
 	if (kvm_mmu_is_dummy_root(root))
@@ -249,14 +285,35 @@ static inline struct kvm_mmu_page *root_to_sp(hpa_t root)
 	return spte_to_child_sp(root);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2511| <<mmu_page_zap_pte>> } else if (is_mmio_spte(pte)) {
+ *   - arch/x86/kvm/mmu/mmu.c|4166| <<handle_mmio_page_fault>> if (is_mmio_spte(spte)) {
+ *   - arch/x86/kvm/mmu/mmu.c|4683| <<sync_mmio_spte>> if (unlikely(is_mmio_spte(*sptep))) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|485| <<handle_changed_spte>> if (WARN_ON_ONCE(!is_mmio_spte(old_spte) &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|486| <<handle_changed_spte>> !is_mmio_spte(new_spte) &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|999| <<tdp_mmu_map_handle_target_level>> if (unlikely(is_mmio_spte(new_spte))) {
+ */
 static inline bool is_mmio_spte(u64 spte)
 {
 	return (spte & shadow_mmio_mask) == shadow_mmio_value &&
 	       likely(enable_mmio_caching);
 }
 
+/*
+ * 查看bit 11是否设置了
+ */
 static inline bool is_shadow_present_pte(u64 pte)
 {
+	/*
+	 * A MMU present SPTE is backed by actual memory and may or may not be present
+	 * in hardware.  E.g. MMIO SPTEs are not considered present.  Use bit 11, as it
+	 * is ignored by all flavors of SPTEs and checking a low bit often generates
+	 * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
+	 * enough that the improved code generation is noticeable in KVM's footprint.
+	 *
+	 * #define SPTE_MMU_PRESENT_MASK           BIT_ULL(11)
+	 */
 	return !!(pte & SPTE_MMU_PRESENT_MASK);
 }
 
@@ -271,17 +328,40 @@ static inline bool kvm_ad_enabled(void)
 	return !!shadow_accessed_mask;
 }
 
+/*
+ * 应该是用来判断硬件ept是否支持ad bit (access & dirty)
+ */
 static inline bool sp_ad_disabled(struct kvm_mmu_page *sp)
 {
 	return sp->role.ad_disabled;
 }
 
+/*
+ * TDP SPTES (more specifically, EPT SPTEs) may not have A/D bits, and may also
+ * be restricted to using write-protection (for L2 when CPU dirty logging, i.e.
+ * PML, is enabled).  Use bits 52 and 53 to hold the type of A/D tracking that
+ * is must be employed for a given TDP SPTE.
+ *
+ * Note, the "enabled" mask must be '0', as bits 62:52 are _reserved_ for PAE
+ * paging, including NPT PAE.  This scheme works because legacy shadow paging
+ * is guaranteed to have A/D bits and write-protection is forced only for
+ * TDP with CPU dirty logging (PML).  If NPT ever gains PML-like support, it
+ * must be restricted to 64-bit KVM.
+ *
+ * 如果spte的SPTE_SPECIAL_MASK=1<<52设置了, 说明ad不支持, 没enabled, 返回false
+ */
 static inline bool spte_ad_enabled(u64 spte)
 {
 	KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
 	return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_DISABLED;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1284| <<__rmap_clear_dirty>> if (spte_ad_need_write_protect(*sptep))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1543| <<clear_dirty_gfn_range>> spte_ad_need_write_protect(iter.old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1604| <<clear_dirty_pt_masked>> spte_ad_need_write_protect(iter.old_spte));
+ */
 static inline bool spte_ad_need_write_protect(u64 spte)
 {
 	KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
@@ -440,6 +520,9 @@ static __always_inline bool is_rsvd_spte(struct rsvd_bits_validate *rsvd_check,
  */
 static inline bool is_writable_pte(unsigned long pte)
 {
+	/*
+	 * 1 << 1 = 10b
+	 */
 	return pte & PT_WRITABLE_MASK;
 }
 
@@ -460,6 +543,11 @@ static inline bool is_mmu_writable_spte(u64 spte)
 	return spte & shadow_mmu_writable_mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|329| <<check_mmio_spte>> spte_gen = get_mmio_spte_generation(spte);
+ *   - arch/x86/kvm/mmu/mmutrace.h|226| <<__field>> __entry->gen = get_mmio_spte_generation(spte);
+ */
 static inline u64 get_mmio_spte_generation(u64 spte)
 {
 	u64 gen;
@@ -482,6 +570,11 @@ u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled);
 u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access);
 u64 mark_spte_for_access_track(u64 spte);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3484| <<fast_page_fault>> new_spte = restore_acc_track_spte(new_spte);
+ *   - arch/x86/kvm/mmu/spte.c|256| <<make_spte_executable>> spte = restore_acc_track_spte(spte);
+ */
 /* Restore an acc-track PTE back to a regular PTE */
 static inline u64 restore_acc_track_spte(u64 spte)
 {
diff --git a/arch/x86/kvm/mmu/tdp_iter.c b/arch/x86/kvm/mmu/tdp_iter.c
index bd30ebfb2..a9b2a718b 100644
--- a/arch/x86/kvm/mmu/tdp_iter.c
+++ b/arch/x86/kvm/mmu/tdp_iter.c
@@ -9,10 +9,35 @@
  * Recalculates the pointer to the SPTE for the current GFN and level and
  * reread the SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|33| <<tdp_iter_restart>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|101| <<try_step_down>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|143| <<try_step_up>> tdp_iter_refresh_sptep(iter);
+ *
+ * 核心思想是修改tdp_iter->sptep, 现在的pointer
+ */
 static void tdp_iter_refresh_sptep(struct tdp_iter *iter)
 {
+	/*
+	 * pt_path[]是Pointers to the page tables traversed to reach the current SPTE
+	 *
+	 * struct tdp_iter *iter:
+	 * -> tdp_ptep_t pt_path[PT64_ROOT_MAX_LEVEL]; --> 5级level: 0-4
+	 * -> tdp_ptep_t sptep; --> tdp_ptep_t类型是指针
+	 * -> u64 old_spte;
+	 */
 	iter->sptep = iter->pt_path[iter->level - 1] +
 		SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+	/*
+	 * 注释:
+	 * TDP MMU SPTEs are RCU protected to allow paging structures (non-leaf SPTEs)
+	 * to be zapped while holding mmu_lock for read, and to allow TLB flushes to be
+	 * batched without having to collect the list of zapped SPs.  Flows that can
+	 * remove SPs must service pending TLB flushes prior to dropping RCU protection.
+	 *
+	 * 猜测sptep指向的是一个page, 读取里面一个entry
+	 */
 	iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
 }
 
@@ -20,13 +45,50 @@ static void tdp_iter_refresh_sptep(struct tdp_iter *iter)
  * Return the TDP iterator to the root PT and allow it to continue its
  * traversal over the paging structure from there.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|57| <<tdp_iter_start>> tdp_iter_restart(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|167| <<tdp_iter_next>> tdp_iter_restart(iter);
+ */
 void tdp_iter_restart(struct tdp_iter *iter)
 {
+	/*
+	 * 注释:
+	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+	 * which case tdp_iter_next() needs to restart the walk at the root
+	 * level instead of advancing to the next entry.
+	 */
 	iter->yielded = false;
+	/*
+	 * 在以下使用tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|80| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|166| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 * 关于next_last_level_gfn:
+	 * The iterator will traverse the paging structure towards the mapping
+	 * for this GFN.
+	 */
 	iter->yielded_gfn = iter->next_last_level_gfn;
 	iter->level = iter->root_level;
 
+	/*
+	 * 在以下使用tdp_iter->gfn:
+	 *   - 587 arch/x86/kvm/mmu/tdp_iter.c|55| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|138| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|165| <<try_step_side>> iter->gfn += KVM_PAGES_PER_HPAGE(iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|188| <<try_step_up>> iter->gfn = gfn_round_for_level(iter->gfn, iter->level);
+	 *
+	 * level 5: 向下round到每68719476736个 (256T)
+	 * level 4: 向下round到每134217728个 (512G)
+	 * level 3: 向下round到每262144个(1G)
+	 * level 2: 向下round到每512个(2M)
+	 * level 1: 向下round到每1个, 就是没round
+	 *
+	 * The lowest GFN mapped by the current SPTE
+	 */
 	iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	/*
+	 * 核心思想是修改tdp_iter->sptep, 现在的pointer
+	 */
 	tdp_iter_refresh_sptep(iter);
 
 	iter->valid = true;
@@ -36,6 +98,12 @@ void tdp_iter_restart(struct tdp_iter *iter)
  * Sets a TDP iterator to walk a pre-order traversal of the paging structure
  * rooted at root_pt, starting with the walk to translate next_last_level_gfn.
  */
+/*
+ * pre-order就是先中间, 后两边
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|140| <<for_each_tdp_pte_min_level>> for (tdp_iter_start(&iter, root, min_level, start); \
+ */
 void tdp_iter_start(struct tdp_iter *iter, struct kvm_mmu_page *root,
 		    int min_level, gfn_t next_last_level_gfn)
 {
@@ -45,12 +113,38 @@ void tdp_iter_start(struct tdp_iter *iter, struct kvm_mmu_page *root,
 		return;
 	}
 
+	/*
+	 * The iterator will traverse the paging structure towards the mapping
+	 * for this GFN.
+	 */
 	iter->next_last_level_gfn = next_last_level_gfn;
+	/*
+	 *  The level of the root page given to the iterator
+	 */
 	iter->root_level = root->role.level;
+	/*
+	 * The lowest level the iterator should traverse to
+	 */
 	iter->min_level = min_level;
+	/*
+	 * Pointers to the page tables traversed to reach the current SPTE
+	 */
 	iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root->spt;
+	/*
+	 * role.smm ? 1 : 0;
+	 *
+	 * The address space ID, i.e. SMM vs. regular.
+	 */
 	iter->as_id = kvm_mmu_page_as_id(root);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|57| <<tdp_iter_start>> tdp_iter_restart(iter);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|167| <<tdp_iter_next>> tdp_iter_restart(iter);
+	 *
+	 * Return the TDP iterator to the root PT and allow it to continue its
+	 * traversal over the paging structure from there.
+	 */
 	tdp_iter_restart(iter);
 }
 
@@ -59,12 +153,25 @@ void tdp_iter_start(struct tdp_iter *iter, struct kvm_mmu_page *root,
  * address of the child page table referenced by the SPTE. Returns null if
  * there is no such entry.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|94| <<try_step_down>> child_pt = spte_to_child_pt(iter->old_spte, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|525| <<handle_changed_spte>> handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
+ *
+ * Given an SPTE and its level, returns a pointer containing the host virtual
+ * address of the child page table referenced by the SPTE. Returns null if
+ * there is no such entry.
+ * 返回的是4k的page table, 不是sp
+ */
 tdp_ptep_t spte_to_child_pt(u64 spte, int level)
 {
 	/*
 	 * There's no child entry if this entry isn't present or is a
 	 * last-level entry.
 	 */
+	/*
+	 * 查看bit 11是否设置了
+	 */
 	if (!is_shadow_present_pte(spte) || is_last_spte(spte, level))
 		return NULL;
 
@@ -75,6 +182,10 @@ tdp_ptep_t spte_to_child_pt(u64 spte, int level)
  * Steps down one level in the paging structure towards the goal GFN. Returns
  * true if the iterator was able to step down a level, false otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|171| <<tdp_iter_next>> if (try_step_down(iter))
+ */
 static bool try_step_down(struct tdp_iter *iter)
 {
 	tdp_ptep_t child_pt;
@@ -86,6 +197,15 @@ static bool try_step_down(struct tdp_iter *iter)
 	 * Reread the SPTE before stepping down to avoid traversing into page
 	 * tables that are no longer linked from this entry.
 	 */
+	/*
+	 * 在以下使用tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|31| <<tdp_iter_refresh_sptep>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|130| <<try_step_down>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|168| <<try_step_side>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|648| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1217| <<age_gfn_range>> iter->old_spte = tdp_mmu_clear_spte_bits(iter->sptep, iter->old_spte, shadow_accessed_mask, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1231| <<age_gfn_range>> iter->old_spte = kvm_tdp_mmu_write_spte(iter->sptep, iter->old_spte, new_spte, iter->level);
+	 */
 	iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
 
 	child_pt = spte_to_child_pt(iter->old_spte, iter->level);
@@ -94,6 +214,13 @@ static bool try_step_down(struct tdp_iter *iter)
 
 	iter->level--;
 	iter->pt_path[iter->level - 1] = child_pt;
+	/*
+	 * level 5: 向下round到每68719476736个 (256T)
+	 * level 4: 向下round到每134217728个 (512G)
+	 * level 3: 向下round到每262144个(1G)
+	 * level 2: 向下round到每512个(2M)
+	 * level 1: 向下round到每1个, 就是没round
+	 */
 	iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
 	tdp_iter_refresh_sptep(iter);
 
@@ -107,6 +234,10 @@ static bool try_step_down(struct tdp_iter *iter)
  * able to step to the next entry in the page table, false if the iterator was
  * already at the end of the current page table.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|175| <<tdp_iter_next>> if (try_step_side(iter))
+ */
 static bool try_step_side(struct tdp_iter *iter)
 {
 	/*
@@ -117,6 +248,13 @@ static bool try_step_side(struct tdp_iter *iter)
 	    (SPTE_ENT_PER_PAGE - 1))
 		return false;
 
+	/*
+	 * level 5: 256T / 4k = 68719476736
+	 * level 4: 512G / 4K = 134217728
+	 * level 3: 1G   / 4K = 262144
+	 * level 2: 2M   / 4K = 512
+	 * level 1: 4K   / 4K = 1
+	 */
 	iter->gfn += KVM_PAGES_PER_HPAGE(iter->level);
 	iter->next_last_level_gfn = iter->gfn;
 	iter->sptep++;
@@ -130,13 +268,27 @@ static bool try_step_side(struct tdp_iter *iter)
  * can continue from the next entry in the parent page table. Returns true on a
  * successful step up, false if already in the root page.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|177| <<tdp_iter_next>> } while (try_step_up(iter));
+ */
 static bool try_step_up(struct tdp_iter *iter)
 {
 	if (iter->level == iter->root_level)
 		return false;
 
 	iter->level++;
+	/*
+	 * level 5: 向下round到每68719476736个 (256T)
+	 * level 4: 向下round到每134217728个 (512G)
+	 * level 3: 向下round到每262144个(1G)
+	 * level 2: 向下round到每512个(2M)
+	 * level 1: 向下round到每1个, 就是没round
+	 */
 	iter->gfn = gfn_round_for_level(iter->gfn, iter->level);
+	/*
+	 * 核心思想是修改tdp_iter->sptep, 现在的pointer
+	 */
 	tdp_iter_refresh_sptep(iter);
 
 	return true;
@@ -158,8 +310,23 @@ static bool try_step_up(struct tdp_iter *iter)
  *    SPTE will have already been visited, and so the iterator must also step
  *    to the side again.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|142| <<for_each_tdp_pte_min_level>> tdp_iter_next(&iter))
+ */
 void tdp_iter_next(struct tdp_iter *iter)
 {
+	/*
+	 * 注释:
+	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+	 * which case tdp_iter_next() needs to restart the walk at the root
+	 * level instead of advancing to the next entry.
+	 *
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|51| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|721| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1406| <<tdp_mmu_alloc_sp_for_split>> iter->yielded = true;
+	 */
 	if (iter->yielded) {
 		tdp_iter_restart(iter);
 		return;
@@ -171,6 +338,10 @@ void tdp_iter_next(struct tdp_iter *iter)
 	do {
 		if (try_step_side(iter))
 			return;
+		/*
+		 * 如果step up了, 返回true,
+		 * 还要继续step aside
+		 */
 	} while (try_step_up(iter));
 	iter->valid = false;
 }
diff --git a/arch/x86/kvm/mmu/tdp_iter.h b/arch/x86/kvm/mmu/tdp_iter.h
index fae559559..6a66c6fa1 100644
--- a/arch/x86/kvm/mmu/tdp_iter.h
+++ b/arch/x86/kvm/mmu/tdp_iter.h
@@ -41,13 +41,49 @@ static inline void __kvm_tdp_mmu_write_spte(tdp_ptep_t sptep, u64 new_spte)
  * logic needs to be reassessed if KVM were to use non-leaf Accessed
  * bits, e.g. to skip stepping down into child SPTEs when aging SPTEs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|54| <<kvm_tdp_mmu_write_spte>> if (kvm_tdp_mmu_spte_need_atomic_write(old_spte, level))
+ *   - arch/x86/kvm/mmu/tdp_iter.h|66| <<tdp_mmu_clear_spte_bits>> if (kvm_tdp_mmu_spte_need_atomic_write(old_spte, level)) {
+ *
+ * 一开始猜测这个函数可能会在mmu_lock外调用???但是应该不是!!
+ * 因为有一些bit是不需要mmu lock就可以更改的, 所以会有race:
+ * -> 在mmu_lock下更改, 和
+ * -> 在fast page fault更改
+ * 所以要atomic, 防止race的错误
+ */
 static inline bool kvm_tdp_mmu_spte_need_atomic_write(u64 old_spte, int level)
 {
+	/*
+	 * volatile bits: bits that can be set outside of mmu_lock. The Writable bit
+	 * can be set by KVM's fast page fault handler, and Accessed and Dirty bits
+	 * can be set by the CPU.
+	 *
+	 * 也就是说,volatile bits是那些可以在lock外修改的bit,比如在fast page fault中
+	 * 的writable,或者硬件支持的access/dirty bit.
+	 *
+	 * 下面3个条件的and:
+	 * - is_shadow_present_pte(old_spte)
+	 * - is_last_spte(old_spte, level)
+	 * -  spte_has_volatile_bits(old_spte)
+	 */
 	return is_shadow_present_pte(old_spte) &&
 	       is_last_spte(old_spte, level) &&
 	       spte_has_volatile_bits(old_spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|402| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, REMOVED_SPTE, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|626| <<tdp_mmu_set_spte>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, new_spte, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1203| <<age_gfn_range>> iter->old_spte = kvm_tdp_mmu_write_spte(iter->sptep, iter->old_spte, new_spte, iter->level);
+ *
+ * 一开始猜测这个函数可能会在mmu_lock外调用???但是应该不是!!
+ * 因为有一些bit是不需要mmu lock就可以更改的, 所以会有race:
+ * -> 在mmu_lock下更改, 和
+ * -> 在fast page fault更改
+ * 所以要atomic, 防止race的错误
+ */
 static inline u64 kvm_tdp_mmu_write_spte(tdp_ptep_t sptep, u64 old_spte,
 					 u64 new_spte, int level)
 {
@@ -58,6 +94,17 @@ static inline u64 kvm_tdp_mmu_write_spte(tdp_ptep_t sptep, u64 old_spte,
 	return old_spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1189| <<age_gfn_range>> iter->old_spte = tdp_mmu_clear_spte_bits(iter->sptep, iter->old_spte, shadow_accessed_mask, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1615| <<clear_dirty_pt_masked>> iter.old_spte = tdp_mmu_clear_spte_bits(iter.sptep, iter.old_spte, dbit, iter.level);
+ *
+ * 一开始猜测这个函数可能会在mmu_lock外调用???但是应该不是!!
+ * 因为有一些bit是不需要mmu lock就可以更改的, 所以会有race:
+ * -> 在mmu_lock下更改, 和
+ * -> 在fast page fault更改
+ * 所以要atomic, 防止race的错误
+ */
 static inline u64 tdp_mmu_clear_spte_bits(tdp_ptep_t sptep, u64 old_spte,
 					  u64 mask, int level)
 {
@@ -80,17 +127,40 @@ struct tdp_iter {
 	 * The iterator will traverse the paging structure towards the mapping
 	 * for this GFN.
 	 */
+	/*
+	 * 在以下使用tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|80| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|166| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	gfn_t next_last_level_gfn;
 	/*
 	 * The next_last_level_gfn at the time when the thread last
 	 * yielded. Only yielding when the next_last_level_gfn !=
 	 * yielded_gfn helps ensure forward progress.
 	 */
+	/*
+	 * 在以下使用tdp_iter->yielded_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|52| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|703| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 */
 	gfn_t yielded_gfn;
+	/*
+	 * 在以下使用tdp_iter->pt_path[PT64_ROOT_MAX_LEVEL]:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|26| <<tdp_iter_refresh_sptep>> iter->sptep = iter->pt_path[iter->level - 1] +
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|83| <<tdp_iter_start>> iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root->spt;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|137| <<try_step_down>> iter->pt_path[iter->level - 1] = child_pt;
+	 */
 	/* Pointers to the page tables traversed to reach the current SPTE */
 	tdp_ptep_t pt_path[PT64_ROOT_MAX_LEVEL];
 	/* A pointer to the current SPTE */
 	tdp_ptep_t sptep;
+	/*
+	 * 在以下使用tdp_iter->gfn:
+	 *   - 587 arch/x86/kvm/mmu/tdp_iter.c|55| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|138| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|165| <<try_step_side>> iter->gfn += KVM_PAGES_PER_HPAGE(iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|188| <<try_step_up>> iter->gfn = gfn_round_for_level(iter->gfn, iter->level);
+	 */
 	/* The lowest GFN mapped by the current SPTE */
 	gfn_t gfn;
 	/* The level of the root page given to the iterator */
@@ -101,21 +171,60 @@ struct tdp_iter {
 	int level;
 	/* The address space ID, i.e. SMM vs. regular. */
 	int as_id;
+	/*
+	 * 在以下使用tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|31| <<tdp_iter_refresh_sptep>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|130| <<try_step_down>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|168| <<try_step_side>> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|648| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1217| <<age_gfn_range>> iter->old_spte = tdp_mmu_clear_spte_bits(iter->sptep, iter->old_spte, shadow_accessed_mask, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1231| <<age_gfn_range>> iter->old_spte = kvm_tdp_mmu_write_spte(iter->sptep, iter->old_spte, new_spte, iter->level);
+	 */
 	/* A snapshot of the value at sptep */
 	u64 old_spte;
 	/*
 	 * Whether the iterator has a valid state. This will be false if the
 	 * iterator walks off the end of the paging structure.
 	 */
+	/*
+	 * 在以下使用tdp_iter->valid:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|58| <<tdp_iter_restart>> iter->valid = true;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|76| <<tdp_iter_start>> iter->valid = false;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|234| <<tdp_iter_next>> iter->valid = false;
+	 *   - arch/x86/kvm/mmu/tdp_iter.h|151| <<for_each_tdp_pte_min_level>> for (tdp_iter_start(&iter, root, min_level, start); iter.valid && iter.gfn < end; tdp_iter_next(&iter))
+	 */
 	bool valid;
 	/*
 	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
 	 * which case tdp_iter_next() needs to restart the walk at the root
 	 * level instead of advancing to the next entry.
 	 */
+	/*
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|51| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|721| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1406| <<tdp_mmu_alloc_sp_for_split>> iter->yielded = true;
+	 * 在以下使用tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|222| <<tdp_iter_next>> if (iter->yielded) {
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|561| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded || is_removed_spte(iter->old_spte));
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|647| <<tdp_mmu_iter_set_spte>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|700| <<tdp_mmu_iter_cond_resched>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|724| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1498| <<tdp_mmu_split_huge_pages_root>> if (iter.yielded)
+	 */
 	bool yielded;
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|145| <<for_each_tdp_pte>> for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|730| <<__tdp_mmu_zap_root>> for_each_tdp_pte_min_level(iter, root, zap_level, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|823| <<tdp_mmu_zap_leafs>> for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1303| <<wrprot_gfn_range>> for_each_tdp_pte_min_level(iter, root, min_level, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1464| <<tdp_mmu_split_huge_pages_root>> for_each_tdp_pte_min_level(iter, root, target_level + 1, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1669| <<zap_collapsible_spte_range>> for_each_tdp_pte_min_level(iter, root, PG_LEVEL_2M, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1740| <<write_protect_gfn>> for_each_tdp_pte_min_level(iter, root, min_level, gfn, gfn + 1) {
+ */
 /*
  * Iterates over every SPTE mapping the GFN range [start, end) in a
  * preorder traversal.
@@ -125,6 +234,20 @@ struct tdp_iter {
 	     iter.valid && iter.gfn < end;		     \
 	     tdp_iter_next(&iter))
 
+/*
+ * enum pg_level {
+ *     PG_LEVEL_NONE,
+ *     PG_LEVEL_4K,  ---> 1
+ *     PG_LEVEL_2M,
+ *     PG_LEVEL_1G,
+ *     PG_LEVEL_512G,
+ *     PG_LEVEL_NUM
+ * };
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|654| <<tdp_root_for_each_pte>> for_each_tdp_pte(_iter, _root, _start, _end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|664| <<tdp_mmu_for_each_pte>> for_each_tdp_pte(_iter, root_to_sp(_mmu->root.hpa), _start, _end)
+ */
 #define for_each_tdp_pte(iter, root, start, end) \
 	for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end)
 
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index 6cd4dd631..33ec35646 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -11,6 +11,14 @@
 #include <asm/cmpxchg.h>
 #include <trace/events/kvm.h>
 
+/*
+ * (似乎说明write lock让read lock的owner yield friendly?????)
+ */
+
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8029| <<kvm_mmu_init_vm>> kvm_mmu_init_tdp_mmu(kvm);
+ */
 /* Initializes the TDP MMU for the VM, if enabled. */
 void kvm_mmu_init_tdp_mmu(struct kvm *kvm)
 {
@@ -18,6 +26,15 @@ void kvm_mmu_init_tdp_mmu(struct kvm *kvm)
 	spin_lock_init(&kvm->arch.tdp_mmu_pages_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|79| <<kvm_tdp_mmu_put_root>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|154| <<__for_each_tdp_mmu_root_yield_safe>> if (kvm_lockdep_assert_mmu_lock_held(_kvm, _shared) && \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|165| <<for_each_tdp_mmu_root_yield_safe>> if (!kvm_lockdep_assert_mmu_lock_held(_kvm, _shared)) { \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|177| <<for_each_tdp_mmu_root>> if (kvm_lockdep_assert_mmu_lock_held(_kvm, false) && \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|743| <<tdp_mmu_zap_root>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1498| <<kvm_tdp_mmu_try_split_huge_pages>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ */
 /* Arbitrarily returns true so that this may be used in if statements. */
 static __always_inline bool kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
 							     bool shared)
@@ -30,6 +47,14 @@ static __always_inline bool kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6231| <<kvm_mmu_uninit_vm>> kvm_mmu_uninit_tdp_mmu(kvm);
+ *
+ * kvm_arch_init_vm() or kvm_arch_destroy_vm()
+ * -> kvm_mmu_uninit_vm()
+ *    -> kvm_mmu_uninit_tdp_mmu()
+ */
 void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 {
 	/*
@@ -37,7 +62,17 @@ void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 	 * for zapping and thus puts the TDP MMU's reference to each root, i.e.
 	 * ultimately frees all roots.
 	 */
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|7988| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_invalidate_all_roots(kvm);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|53| <<kvm_mmu_uninit_tdp_mmu>> kvm_tdp_mmu_invalidate_all_roots(kvm);
+	 */
 	kvm_tdp_mmu_invalidate_all_roots(kvm);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|8013| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_zap_invalidated_roots(kvm);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|54| <<kvm_mmu_uninit_tdp_mmu>> kvm_tdp_mmu_zap_invalidated_roots(kvm);
+	 */
 	kvm_tdp_mmu_zap_invalidated_roots(kvm);
 
 	WARN_ON(atomic64_read(&kvm->arch.tdp_mmu_pages));
@@ -51,8 +86,18 @@ void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 	rcu_barrier();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|90| <<tdp_mmu_free_sp_rcu_callback>> tdp_mmu_free_sp(sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1158| <<kvm_tdp_mmu_map>> tdp_mmu_free_sp(sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1548| <<tdp_mmu_split_huge_pages_root>> tdp_mmu_free_sp(sp);
+ */
 static void tdp_mmu_free_sp(struct kvm_mmu_page *sp)
 {
+	/*
+	 * struct kvm_mmu_page *sp:
+	 * -> u64 *spt;
+	 */
 	free_page((unsigned long)sp->spt);
 	kmem_cache_free(mmu_page_header_cache, sp);
 }
@@ -65,19 +110,49 @@ static void tdp_mmu_free_sp(struct kvm_mmu_page *sp)
  * section, and freeing it after a grace period, lockless access to that
  * memory won't use it after it is freed.
  */
+/*
+ * 在以下使用tdp_mmu_free_sp_rcu_callback():
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|118| <<kvm_tdp_mmu_put_root>> call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|434| <<handle_removed_pt>> call_rcu(&sp->rcu_head, tdp_mmu_free_sp_rcu_callback);
+ */
 static void tdp_mmu_free_sp_rcu_callback(struct rcu_head *head)
 {
 	struct kvm_mmu_page *sp = container_of(head, struct kvm_mmu_page,
 					       rcu_head);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|90| <<tdp_mmu_free_sp_rcu_callback>> tdp_mmu_free_sp(sp);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1158| <<kvm_tdp_mmu_map>> tdp_mmu_free_sp(sp);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1548| <<tdp_mmu_split_huge_pages_root>> tdp_mmu_free_sp(sp);
+	 */
 	tdp_mmu_free_sp(sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4044| <<mmu_free_root_page>> kvm_tdp_mmu_put_root(kvm, sp, false);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|152| <<tdp_mmu_next_root>> kvm_tdp_mmu_put_root(kvm, prev_root, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|936| <<kvm_tdp_mmu_zap_invalidated_roots>> kvm_tdp_mmu_put_root(kvm, root, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1557| <<kvm_tdp_mmu_try_split_huge_pages>> kvm_tdp_mmu_put_root(kvm, root, shared);
+ */
 void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 			  bool shared)
 {
 	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
 
+	/*
+	 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|105| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|282| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 2);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|791| <<tdp_mmu_zap_root>> WARN_ON_ONCE(!refcount_read(&root->tdp_mmu_root_count));
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|22| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+	 *
+	 * 一开始在kvm_tdp_mmu_get_vcpu_root_hpa()设置成了2
+	 *
+	 * decrement a refcount and test if it is 0
+	 * 是0返回true, 也就不return
+	 */
 	if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
 		return;
 
@@ -91,6 +166,11 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 	spin_lock(&kvm->arch.tdp_mmu_pages_lock);
 	list_del_rcu(&root->link);
 	spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	/*
+	 * 在以下使用tdp_mmu_free_sp_rcu_callback():
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|118| <<kvm_tdp_mmu_put_root>> call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|434| <<handle_removed_pt>> call_rcu(&sp->rcu_head, tdp_mmu_free_sp_rcu_callback);
+	 */
 	call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
 }
 
@@ -104,6 +184,17 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
  *
  * Returns NULL if the end of tdp_mmu_roots was reached.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|175| <<__for_each_tdp_mmu_root_yield_safe>> for (_root = tdp_mmu_next_root(_kvm, NULL, _shared, _only_valid); \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|177| <<__for_each_tdp_mmu_root_yield_safe>> _root = tdp_mmu_next_root(_kvm, _root, _shared, _only_valid)) \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|186| <<for_each_tdp_mmu_root_yield_safe>> for (_root = tdp_mmu_next_root(_kvm, NULL, _shared, false); \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|188| <<for_each_tdp_mmu_root_yield_safe>> _root = tdp_mmu_next_root(_kvm, _root, _shared, false)) \
+ *
+ * 注意, 会把参数中的prev_root给 kvm_tdp_mmu_put_root(kvm, prev_root, shared)
+ * 应该是之前在上一次调用tdp_mmu_next_root() break的时候get的
+ * 这个函数返回NULL说明到头了
+ */
 static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 					      struct kvm_mmu_page *prev_root,
 					      bool shared, bool only_valid)
@@ -147,6 +238,15 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
  * mode. In the unlikely event that this thread must free a root, the lock
  * will be temporarily dropped and reacquired in write mode.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|222| <<for_each_valid_tdp_mmu_root_yield_safe>> __for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared, true)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1273| <<kvm_tdp_mmu_unmap_gfn_range>> __for_each_tdp_mmu_root_yield_safe(kvm, root, range->slot->as_id, false, false)
+ *
+ * 从参数的_root开始 ...
+ * 通过tdp_mmu_next_root()遍历kvm->arch.tdp_mmu_roots中所有的root kvm_mmu_page
+ * 每个iteration开始的时候会get这次的page, 和put上一次的page
+ */
 #define __for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared, _only_valid)\
 	for (_root = tdp_mmu_next_root(_kvm, NULL, _shared, _only_valid);	\
 	     _root;								\
@@ -155,9 +255,31 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 		    kvm_mmu_page_as_id(_root) != _as_id) {			\
 		} else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1462| <<kvm_tdp_mmu_wrprot_slot>> for_each_valid_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1645| <<kvm_tdp_mmu_try_split_huge_pages>> for_each_valid_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, shared) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1709| <<kvm_tdp_mmu_clear_dirty_slot>> for_each_valid_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1844| <<kvm_tdp_mmu_zap_collapsible_sptes>> for_each_valid_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *
+ * 从参数的_root开始 ...
+ * 通过tdp_mmu_next_root()遍历kvm->arch.tdp_mmu_roots中所有的root kvm_mmu_page --> 必须是valid的
+ * 每个iteration开始的时候会get这次的page, 和put上一次的page
+ */
 #define for_each_valid_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared)	\
 	__for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared, true)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|946| <<kvm_tdp_mmu_zap_leafs>> for_each_tdp_mmu_root_yield_safe(kvm, root, false)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|968| <<kvm_tdp_mmu_zap_all>> for_each_tdp_mmu_root_yield_safe(kvm, root, false)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|987| <<kvm_tdp_mmu_zap_invalidated_roots>> for_each_tdp_mmu_root_yield_safe(kvm, root, true) {
+ *
+ * 从参数的头开始, 也就是从kvm->arch.tdp_mmu_roots开始 ...
+ * 通过tdp_mmu_next_root()遍历kvm->arch.tdp_mmu_roots中所有的root kvm_mmu_page
+ * 每个iteration开始的时候会get这次的page, 和put上一次的page
+ * 要求必须有read或者write锁
+ */
 #define for_each_tdp_mmu_root_yield_safe(_kvm, _root, _shared)			\
 	for (_root = tdp_mmu_next_root(_kvm, NULL, _shared, false);		\
 	     _root;								\
@@ -172,37 +294,104 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
  * Holding mmu_lock for write obviates the need for RCU protection as the list
  * is guaranteed to be stable.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|266| <<kvm_tdp_mmu_get_vcpu_root_hpa>> for_each_tdp_mmu_root(kvm, root, kvm_mmu_role_as_id(role)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1217| <<kvm_tdp_mmu_handle_gfn>> for_each_tdp_mmu_root(kvm, root, range->slot->as_id) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1700| <<kvm_tdp_mmu_clear_dirty_pt_masked>> for_each_tdp_mmu_root(kvm, root, slot->as_id)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1819| <<kvm_tdp_mmu_write_protect_gfn>> for_each_tdp_mmu_root(kvm, root, slot->as_id)
+ *
+ * 注释
+ * Iterate over all TDP MMU roots.  Requires that mmu_lock be held for write,
+ * the implication being that any flow that holds mmu_lock for read is
+ * inherently yield-friendly and should use the yield-safe variant above.
+ * Holding mmu_lock for write obviates the need for RCU protection as the list
+ * is guaranteed to be stable.
+ * 遍历_kvm->arch.tdp_mmu_roots, 要求相同的as_id
+ * 要求必须有write lock
+ * (似乎说明write lock让read lock的owner yield friendly?????)
+ */
 #define for_each_tdp_mmu_root(_kvm, _root, _as_id)			\
 	list_for_each_entry(_root, &_kvm->arch.tdp_mmu_roots, link)	\
 		if (kvm_lockdep_assert_mmu_lock_held(_kvm, false) &&	\
 		    kvm_mmu_page_as_id(_root) != _as_id) {		\
 		} else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|272| <<kvm_tdp_mmu_get_vcpu_root_hpa>> root = tdp_mmu_alloc_sp(vcpu);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1143| <<kvm_tdp_mmu_map>> sp = tdp_mmu_alloc_sp(vcpu);
+ *
+ * 分配kvm_mmu_page
+ * 分配kvm_mmu_page->sp (4k)
+ */
 static struct kvm_mmu_page *tdp_mmu_alloc_sp(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu_page *sp;
 
+	/*
+	 * struct kvm_mmu_page *sp:
+	 * -> u64 *spt;
+	 */
 	sp = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_page_header_cache);
 	sp->spt = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_shadow_page_cache);
 
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|251| <<tdp_mmu_init_child_sp>> tdp_mmu_init_sp(child_sp, iter->sptep, iter->gfn, role);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|273| <<kvm_tdp_mmu_get_vcpu_root_hpa>> tdp_mmu_init_sp(root, NULL, 0, role);
+ */
 static void tdp_mmu_init_sp(struct kvm_mmu_page *sp, tdp_ptep_t sptep,
 			    gfn_t gfn, union kvm_mmu_page_role role)
 {
 	INIT_LIST_HEAD(&sp->possible_nx_huge_page_link);
 
+	/*
+	 * 在kvm一共有三处调用:
+	 *   - arch/arm64/kvm/mmu.c|235| <<stage2_free_unlinked_table>> set_page_private(page, (unsigned long )level);
+	 *   - arch/x86/kvm/mmu/mmu.c|2724| <<kvm_mmu_alloc_shadow_page>> set_page_private(virt_to_page(sp->spt), (unsigned long )sp);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|224| <<tdp_mmu_init_sp>> set_page_private(virt_to_page(sp->spt), (unsigned long )sp);
+	 */
 	set_page_private(virt_to_page(sp->spt), (unsigned long)sp);
 
 	sp->role = role;
+	/*
+	 * 似乎对direct是管理地址范围的起始地址对应的gfn
+	 * 对于shadow, 是当前sp的对应的guest的gfn
+	 * 这个值就是当前shadow页表对应的虚机页表在guest os中的gfn
+	 */
 	sp->gfn = gfn;
+	/*
+	 * 似乎只有tdp_mmu使用kvm_mmu_page->ptep:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|361| <<tdp_mmu_init_sp>> sp->ptep = sptep;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|985| <<kvm_tdp_mmu_zap_sp>> if (WARN_ON_ONCE(!sp->ptep))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|988| <<kvm_tdp_mmu_zap_sp>> old_spte = kvm_tdp_mmu_read_spte(sp->ptep);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|992| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0, sp->gfn, sp->role.level + 1);
+	 */
 	sp->ptep = sptep;
+	/*
+	 * 在以下使用kvm_mmu_page->tdp_mmu_page:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|309| <<tdp_mmu_init_sp>> sp->tdp_mmu_page = true;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|82| <<is_tdp_mmu_page>> static inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return sp->tdp_mmu_page; }
+	 */
 	sp->tdp_mmu_page = true;
 
+	/*
+	 * 在以下调用trace_kvm_mmu_get_page():
+	 *   - arch/x86/kvm/mmu/mmu.c|3384| <<__kvm_mmu_get_shadow_page>> trace_kvm_mmu_get_page(sp, created);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|361| <<tdp_mmu_init_sp>> trace_kvm_mmu_get_page(sp, true);
+	 */
 	trace_kvm_mmu_get_page(sp, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1330| <<kvm_tdp_mmu_map>> tdp_mmu_init_child_sp(sp, &iter);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1718| <<tdp_mmu_split_huge_pages_root>> tdp_mmu_init_child_sp(sp, &iter);
+ */
 static void tdp_mmu_init_child_sp(struct kvm_mmu_page *child_sp,
 				  struct tdp_iter *iter)
 {
@@ -214,9 +403,16 @@ static void tdp_mmu_init_child_sp(struct kvm_mmu_page *child_sp,
 	role = parent_sp->role;
 	role.level--;
 
+	/*
+	 * iter->sptep应该是指向这个child的kvm_mmu_page->spt的ptep的virtual addr
+	 */
 	tdp_mmu_init_sp(child_sp, iter->sptep, iter->gfn, role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5143| <<mmu_alloc_direct_roots>> root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);
+ */
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 {
 	union kvm_mmu_page_role role = vcpu->arch.mmu->root_role;
@@ -225,6 +421,17 @@ hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
 
+	/*
+	 * 注释
+	 * Iterate over all TDP MMU roots.  Requires that mmu_lock be held for write,
+	 * the implication being that any flow that holds mmu_lock for read is
+	 * inherently yield-friendly and should use the yield-safe variant above.
+	 * Holding mmu_lock for write obviates the need for RCU protection as the list
+	 * is guaranteed to be stable.
+	 * 遍历_kvm->arch.tdp_mmu_roots, 要求相同的as_id
+	 * 要求必须有write lock
+	 * (似乎说明write lock让read lock的owner yield friendly?????)
+	 */
 	/*
 	 * Check for an existing root before allocating a new one.  Note, the
 	 * role check prevents consuming an invalid root.
@@ -235,6 +442,9 @@ hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 			goto out;
 	}
 
+	/*
+	 * 简单的测试: role.direct = 1
+	 */
 	root = tdp_mmu_alloc_sp(vcpu);
 	tdp_mmu_init_sp(root, NULL, 0, role);
 
@@ -259,12 +469,37 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1296| <<tdp_mmu_link_sp>> tdp_account_mmu_page(kvm, sp);
+ */
 static void tdp_account_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
+	/*
+	 * 在以下使用kvm_account_pgtable_pages():
+	 *   - arch/arm64/kvm/mmu.c|196| <<stage2_memcache_zalloc_page>> kvm_account_pgtable_pages(virt, 1);
+	 *   - arch/arm64/kvm/mmu.c|210| <<kvm_s2_zalloc_pages_exact>> kvm_account_pgtable_pages(virt, (size >> PAGE_SHIFT));
+	 *   - arch/arm64/kvm/mmu.c|216| <<kvm_s2_free_pages_exact>> kvm_account_pgtable_pages(virt, -(size >> PAGE_SHIFT));
+	 *   - arch/arm64/kvm/mmu.c|260| <<kvm_s2_put_page>> kvm_account_pgtable_pages(addr, -1);
+	 *   - arch/x86/kvm/mmu/mmu.c|2386| <<kvm_account_mmu_page>> kvm_account_pgtable_pages((void *)sp->spt, +1);
+	 *   - arch/x86/kvm/mmu/mmu.c|2392| <<kvm_unaccount_mmu_page>> kvm_account_pgtable_pages((void *)sp->spt, -1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|454| <<tdp_account_mmu_page>> kvm_account_pgtable_pages((void *)sp->spt, +1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|460| <<tdp_unaccount_mmu_page>> kvm_account_pgtable_pages((void *)sp->spt, -1);
+	 */
 	kvm_account_pgtable_pages((void *)sp->spt, +1);
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_pages:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|56| <<kvm_mmu_uninit_tdp_mmu>> WARN_ON(atomic64_read(&kvm->arch.tdp_mmu_pages));
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|299| <<tdp_account_mmu_page>> atomic64_inc(&kvm->arch.tdp_mmu_pages);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|305| <<tdp_unaccount_mmu_page>> atomic64_dec(&kvm->arch.tdp_mmu_pages);
+	 */
 	atomic64_inc(&kvm->arch.tdp_mmu_pages);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|476| <<tdp_mmu_unlink_sp>> tdp_unaccount_mmu_page(kvm, sp);
+ */
 static void tdp_unaccount_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	kvm_account_pgtable_pages((void *)sp->spt, -1);
@@ -280,6 +515,10 @@ static void tdp_unaccount_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)
  *	    the MMU lock and the operation must synchronize with other
  *	    threads that might be adding or removing pages.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|519| <<handle_removed_pt>> tdp_mmu_unlink_sp(kvm, sp, shared);
+ */
 static void tdp_mmu_unlink_sp(struct kvm *kvm, struct kvm_mmu_page *sp,
 			      bool shared)
 {
@@ -317,8 +556,27 @@ static void tdp_mmu_unlink_sp(struct kvm *kvm, struct kvm_mmu_page *sp,
  * this thread will be responsible for ensuring the page is freed. Hence the
  * early rcu_dereferences in the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|694| <<handle_changed_spte>> handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
+ *
+ * typedef u64 __rcu *tdp_ptep_t;
+ * pt是一个指向spte的指针
+ *
+ * 目前只有一个调用的地方
+ * 第二个参数pt是spte_to_child_pt(old_spte, level)
+ * 返回的是old_spte中的内容所对应的4k page table的虚拟地址
+ * 所以这里pt指向的是一个4k page table的虚拟地址
+ */
 static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 {
+	/*
+	 * 目前只有一个调用的地方
+	 * 第二个参数pt是spte_to_child_pt(old_spte, level)
+	 * 返回的是old_spte中的内容所对应的4k page table的虚拟地址
+	 * 所以这里pt指向的是一个4k page table的虚拟地址
+	 * 所以sp是要删除的page table (4k的kvm_mmu_page)
+	 */
 	struct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(pt));
 	int level = sp->role.level;
 	gfn_t base_gfn = sp->gfn;
@@ -343,7 +601,13 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 			 * value to the removed SPTE value.
 			 */
 			for (;;) {
+				/*
+				 * 用xchg交换成REMOVED_SPTE
+				 */
 				old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, REMOVED_SPTE);
+				/*
+				 * 看看旧的是不是也是REMOVED_SPTE
+				 */
 				if (!is_removed_spte(old_spte))
 					break;
 				cpu_relax();
@@ -393,10 +657,26 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 			old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte,
 							  REMOVED_SPTE, level);
 		}
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|586| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|744| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte, new_spte, iter->level, true);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|814| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+		 *
+		 * 当spte被改变的时候, 做出的一些反应
+		 * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+		 * dirty logging updates are handled in common code, not here (see make_spte()
+		 * and fast_pf_fix_direct_spte()).
+		 */
 		handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
 				    old_spte, REMOVED_SPTE, level, shared);
 	}
 
+	/*
+	 * 在以下使用tdp_mmu_free_sp_rcu_callback():
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|118| <<kvm_tdp_mmu_put_root>> call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|434| <<handle_removed_pt>> call_rcu(&sp->rcu_head, tdp_mmu_free_sp_rcu_callback);
+	 */
 	call_rcu(&sp->rcu_head, tdp_mmu_free_sp_rcu_callback);
 }
 
@@ -416,10 +696,24 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
  * dirty logging updates are handled in common code, not here (see make_spte()
  * and fast_pf_fix_direct_spte()).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|586| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|744| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte, new_spte, iter->level, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|814| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+ *
+ * 当spte被改变的时候, 做出的一些反应
+ * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+ * dirty logging updates are handled in common code, not here (see make_spte()
+ * and fast_pf_fix_direct_spte()).
+ */
 static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared)
 {
+	/*
+	 * 查看bit 11是否设置了
+	 */
 	bool was_present = is_shadow_present_pte(old_spte);
 	bool is_present = is_shadow_present_pte(new_spte);
 	bool was_leaf = was_present && is_last_spte(old_spte, level);
@@ -428,6 +722,13 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 
 	WARN_ON_ONCE(level > PT64_ROOT_MAX_LEVEL);
 	WARN_ON_ONCE(level < PG_LEVEL_4K);
+	/*
+	 * level 5: 256T / 4k = 68719476736
+	 * level 4: 512G / 4K = 134217728
+	 * level 3: 1G   / 4K = 262144
+	 * level 2: 2M   / 4K = 512
+	 * level 1: 4K   / 4K = 1
+	 */
 	WARN_ON_ONCE(gfn & (KVM_PAGES_PER_HPAGE(level) - 1));
 
 	/*
@@ -456,6 +757,12 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 	if (old_spte == new_spte)
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|699| <<handle_changed_spte>> trace_kvm_tdp_mmu_spte_changed(as_id, gfn, level, old_spte, new_spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1534| <<age_gfn_range>> trace_kvm_tdp_mmu_spte_changed(iter->as_id, iter->gfn, iter->level, iter->old_spte, new_spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1945| <<clear_dirty_pt_masked>> trace_kvm_tdp_mmu_spte_changed(iter.as_id, iter.gfn, iter.level, iter.old_spte, iter.old_spte & ~dbit);
+	 */
 	trace_kvm_tdp_mmu_spte_changed(as_id, gfn, level, old_spte, new_spte);
 
 	if (is_leaf)
@@ -493,6 +800,11 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 	    (!is_present || !is_dirty_spte(new_spte) || pfn_changed))
 		kvm_set_pfn_dirty(spte_to_pfn(old_spte));
 
+	/*
+	 * 关于下面recursive的调用
+	 * 第二个参数spte_to_child_pt(old_spte, level)
+	 * 返回的是old_spte中的内容所对应的4k page table的虚拟地址
+	 */
 	/*
 	 * Recursively handle child PTs if the change removed a subtree from
 	 * the paging structure.  Note the WARN on the PFN changing without the
@@ -525,6 +837,17 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
  *            no side-effects other than setting iter->old_spte to the last
  *            known value of the spte.
  */
+/*
+ * 在以下调用tdp_mmu_set_spte_atomic():
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|811| <<tdp_mmu_zap_spte_atomic>> ret = tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1001| <<__tdp_mmu_zap_root>> else if (tdp_mmu_set_spte_atomic(kvm, &iter, 0))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1290| <<tdp_mmu_map_handle_target_level>> else if (tdp_mmu_set_spte_atomic(vcpu->kvm, iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1349| <<tdp_mmu_link_sp>> ret = tdp_mmu_set_spte_atomic(kvm, iter, spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1629| <<wrprot_gfn_range>> if (tdp_mmu_set_spte_atomic(kvm, &iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1874| <<clear_dirty_gfn_range>> if (tdp_mmu_set_spte_atomic(kvm, &iter, iter.old_spte & ~dbit))
+ *
+ * 用atomic似乎是shared/read_lock, 因为fast_pf_fix_direct_spte()???
+ */
 static inline int tdp_mmu_set_spte_atomic(struct kvm *kvm,
 					  struct tdp_iter *iter,
 					  u64 new_spte)
@@ -551,12 +874,31 @@ static inline int tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	if (!try_cmpxchg64(sptep, &iter->old_spte, new_spte))
 		return -EBUSY;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|586| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|744| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte, new_spte, iter->level, true);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|814| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+	 *
+	 * 当spte被改变的时候, 做出的一些反应
+	 * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+	 * dirty logging updates are handled in common code, not here (see make_spte()
+	 * and fast_pf_fix_direct_spte()).
+	 */
 	handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			    new_spte, iter->level, true);
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|2020| <<zap_collapsible_spte_range>> if (tdp_mmu_zap_spte_atomic(kvm, &iter))
+ *
+ * 1. tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE);
+ * 2. kvm_flush_remote_tlbs_gfn(kvm, iter->gfn, iter->level);
+ * 3. __kvm_tdp_mmu_write_spte(iter->sptep, 0);
+ */
 static inline int tdp_mmu_zap_spte_atomic(struct kvm *kvm,
 					  struct tdp_iter *iter)
 {
@@ -600,6 +942,13 @@ static inline int tdp_mmu_zap_spte_atomic(struct kvm *kvm,
  * Returns the old SPTE value, which _may_ be different than @old_spte if the
  * SPTE had voldatile bits.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|735| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|911| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0, sp->gfn, sp->role.level + 1);
+ *
+ * 函数要求write的mmu_lock
+ */
 static u64 tdp_mmu_set_spte(struct kvm *kvm, int as_id, tdp_ptep_t sptep,
 			    u64 old_spte, u64 new_spte, gfn_t gfn, int level)
 {
@@ -614,24 +963,66 @@ static u64 tdp_mmu_set_spte(struct kvm *kvm, int as_id, tdp_ptep_t sptep,
 	 */
 	WARN_ON_ONCE(is_removed_spte(old_spte) || is_removed_spte(new_spte));
 
+	/*
+	 * 因为有一些bit是不需要mmu lock就可以更改的, 所以会有race:
+	 * -> 在mmu_lock下更改, 和
+	 * -> 在fast page fault更改
+	 * 所以要atomic, 防止race的错误
+	 */
 	old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, new_spte, level);
 
+	/*
+	 * 当spte被改变的时候, 做出的一些反应
+	 * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+	 * dirty logging updates are handled in common code, not here (see make_spte()
+	 * and fast_pf_fix_direct_spte()).
+	 */
 	handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
 	return old_spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|854| <<__tdp_mmu_zap_root>> tdp_mmu_iter_set_spte(kvm, &iter, 0);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|951| <<tdp_mmu_zap_leafs>> tdp_mmu_iter_set_spte(kvm, &iter, 0);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1190| <<tdp_mmu_link_sp>> tdp_mmu_iter_set_spte(kvm, iter, spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1410| <<set_spte_gfn>> tdp_mmu_iter_set_spte(kvm, iter, 0);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1416| <<set_spte_gfn>> tdp_mmu_iter_set_spte(kvm, iter, new_spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1914| <<write_protect_gfn>> tdp_mmu_iter_set_spte(kvm, &iter, new_spte);
+ *
+ * 函数要求write的mmu_lock
+ */
 static inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 					 u64 new_spte)
 {
 	WARN_ON_ONCE(iter->yielded);
+	/*
+	 * 函数要求write的mmu_lock
+	 */
 	iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep,
 					  iter->old_spte, new_spte,
 					  iter->gfn, iter->level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|657| <<tdp_root_for_each_leaf_pte>> tdp_root_for_each_pte(_iter, _root, _start, _end) \
+ *
+ * 不包括end
+ */
 #define tdp_root_for_each_pte(_iter, _root, _start, _end) \
 	for_each_tdp_pte(_iter, _root, _start, _end)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1174| <<kvm_tdp_mmu_handle_gfn>> tdp_root_for_each_leaf_pte(iter, root, range->start, range->end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1546| <<clear_dirty_gfn_range>> tdp_root_for_each_leaf_pte(iter, root, start, end) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1610| <<clear_dirty_pt_masked>> tdp_root_for_each_leaf_pte(iter, root, gfn + __ffs(mask),
+ *
+ * leaf的continue有两个条件二选一:
+ * 1. !is_shadow_present_pte(_iter.old_spte)
+ * 2. !is_last_spte(_iter.old_spte, _iter.level)
+ */
 #define tdp_root_for_each_leaf_pte(_iter, _root, _start, _end)	\
 	tdp_root_for_each_pte(_iter, _root, _start, _end)		\
 		if (!is_shadow_present_pte(_iter.old_spte) ||		\
@@ -639,6 +1030,12 @@ static inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 			continue;					\
 		else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1076| <<kvm_tdp_mmu_map>> tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1795| <<kvm_tdp_mmu_get_walk>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1822| <<kvm_tdp_mmu_fast_pf_get_last_sptep>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ */
 #define tdp_mmu_for_each_pte(_iter, _mmu, _start, _end)		\
 	for_each_tdp_pte(_iter, root_to_sp(_mmu->root.hpa), _start, _end)
 
@@ -656,6 +1053,15 @@ static inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,
  *
  * Returns true if this function yielded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|835| <<__tdp_mmu_zap_root>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|933| <<tdp_mmu_zap_leafs>> tdp_mmu_iter_cond_resched(kvm, &iter, flush, false)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1447| <<wrprot_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1608| <<tdp_mmu_split_huge_pages_root>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1690| <<clear_dirty_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1817| <<zap_collapsible_spte_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ */
 static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 							  struct tdp_iter *iter,
 							  bool flush, bool shared)
@@ -681,12 +1087,30 @@ static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 
 		WARN_ON_ONCE(iter->gfn > iter->next_last_level_gfn);
 
+		/*
+		 * 在以下设置tdp_iter->yielded:
+		 *   - arch/x86/kvm/mmu/tdp_iter.c|51| <<tdp_iter_restart>> iter->yielded = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|721| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1406| <<tdp_mmu_alloc_sp_for_split>> iter->yielded = true;
+		 * 在以下使用tdp_iter->yielded:
+		 *   - arch/x86/kvm/mmu/tdp_iter.c|222| <<tdp_iter_next>> if (iter->yielded) {
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|561| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded || is_removed_spte(iter->old_spte));
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|647| <<tdp_mmu_iter_set_spte>> WARN_ON_ONCE(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|700| <<tdp_mmu_iter_cond_resched>> WARN_ON_ONCE(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|724| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1498| <<tdp_mmu_split_huge_pages_root>> if (iter.yielded)
+		 */
 		iter->yielded = true;
 	}
 
 	return iter->yielded;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1109| <<__tdp_mmu_zap_root>> gfn_t end = tdp_mmu_max_gfn_exclusive();
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1211| <<tdp_mmu_zap_leafs>> end = min(end, tdp_mmu_max_gfn_exclusive());
+ */
 static inline gfn_t tdp_mmu_max_gfn_exclusive(void)
 {
 	/*
@@ -698,6 +1122,11 @@ static inline gfn_t tdp_mmu_max_gfn_exclusive(void)
 	return kvm_mmu_max_gfn() + 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1160| <<tdp_mmu_zap_root>> __tdp_mmu_zap_root(kvm, root, shared, PG_LEVEL_1G);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1161| <<tdp_mmu_zap_root>> __tdp_mmu_zap_root(kvm, root, shared, root->role.level);
+ */
 static void __tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
 			       bool shared, int zap_level)
 {
@@ -706,6 +1135,10 @@ static void __tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
 	gfn_t end = tdp_mmu_max_gfn_exclusive();
 	gfn_t start = 0;
 
+	/*
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 */
 	for_each_tdp_pte_min_level(iter, root, zap_level, start, end) {
 retry:
 		if (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
@@ -714,6 +1147,11 @@ static void __tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
 		if (!is_shadow_present_pte(iter.old_spte))
 			continue;
 
+		/*
+		 * 为什么caller的zap_level是下面的???
+		 * - PG_LEVEL_1G
+		 * - root->role.level
+		 */
 		if (iter.level > zap_level)
 			continue;
 
@@ -724,6 +1162,11 @@ static void __tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1273| <<kvm_tdp_mmu_zap_all>> tdp_mmu_zap_root(kvm, root, false);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1313| <<kvm_tdp_mmu_zap_invalidated_roots>> tdp_mmu_zap_root(kvm, root, true);
+ */
 static void tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
 			     bool shared)
 {
@@ -760,10 +1203,21 @@ static void tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
 	rcu_read_unlock();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|9061| <<kvm_recover_nx_huge_pages>> flush |= kvm_tdp_mmu_zap_sp(kvm, sp);
+ */
 bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	u64 old_spte;
 
+	/*
+	 * 似乎只有tdp_mmu使用kvm_mmu_page->ptep:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|361| <<tdp_mmu_init_sp>> sp->ptep = sptep;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|985| <<kvm_tdp_mmu_zap_sp>> if (WARN_ON_ONCE(!sp->ptep))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|988| <<kvm_tdp_mmu_zap_sp>> old_spte = kvm_tdp_mmu_read_spte(sp->ptep);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|992| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0, sp->gfn, sp->role.level + 1);
+	 */
 	/*
 	 * This helper intentionally doesn't allow zapping a root shadow page,
 	 * which doesn't have a parent page table and thus no associated entry.
@@ -771,10 +1225,18 @@ bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 	if (WARN_ON_ONCE(!sp->ptep))
 		return false;
 
+	/*
+	 * 似乎只有tdp_mmu使用kvm_mmu_page->ptep
+	 */
 	old_spte = kvm_tdp_mmu_read_spte(sp->ptep);
 	if (WARN_ON_ONCE(!is_shadow_present_pte(old_spte)))
 		return false;
 
+	/*
+	 * 函数要求write的mmu_lock
+	 *
+	 * 把old_spte改新的pte 0
+	 */
 	tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0,
 			 sp->gfn, sp->role.level + 1);
 
@@ -788,6 +1250,11 @@ bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
  * the caller must ensure it does not supply too large a GFN range, or the
  * operation can cause a soft lockup.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|873| <<kvm_tdp_mmu_zap_leafs>> flush = tdp_mmu_zap_leafs(kvm, root, start, end, true, flush);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1170| <<kvm_tdp_mmu_unmap_gfn_range>> flush = tdp_mmu_zap_leafs(kvm, root, range->start, range->end, range->may_block, flush);
+ */
 static bool tdp_mmu_zap_leafs(struct kvm *kvm, struct kvm_mmu_page *root,
 			      gfn_t start, gfn_t end, bool can_yield, bool flush)
 {
@@ -810,6 +1277,9 @@ static bool tdp_mmu_zap_leafs(struct kvm *kvm, struct kvm_mmu_page *root,
 		    !is_last_spte(iter.old_spte, iter.level))
 			continue;
 
+		/*
+		 * 函数要求write的mmu_lock
+		 */
 		tdp_mmu_iter_set_spte(kvm, &iter, 0);
 		flush = true;
 	}
@@ -828,16 +1298,40 @@ static bool tdp_mmu_zap_leafs(struct kvm *kvm, struct kvm_mmu_page *root,
  * true if a TLB flush is needed before releasing the MMU lock, i.e. if one or
  * more SPTEs were zapped since the MMU lock was last acquired.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8155| <<kvm_zap_gfn_range>> flush = kvm_tdp_mmu_zap_leafs(kvm, gfn_start, gfn_end, flush);
+ *
+ * 函数要求write的mmu_lock
+ */
 bool kvm_tdp_mmu_zap_leafs(struct kvm *kvm, gfn_t start, gfn_t end, bool flush)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * 从参数的头开始, 也就是从kvm->arch.tdp_mmu_roots开始 ...
+	 * 通过tdp_mmu_next_root()遍历kvm->arch.tdp_mmu_roots中所有的root kvm_mmu_page
+	 * 每个iteration开始的时候会get这次的page, 和put上一次的page
+	 * 要求必须有read或者write锁
+	 *
+	 * --> 这里是false, 所以必须有write的锁
+	 */
 	for_each_tdp_mmu_root_yield_safe(kvm, root, false)
 		flush = tdp_mmu_zap_leafs(kvm, root, start, end, true, flush);
 
 	return flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8646| <<kvm_mmu_zap_all>> kvm_tdp_mmu_zap_all(kvm);
+ *
+ * kvm_mmu_notifier_release() or kvm_destroy_vm()
+ * -> kvm_flush_shadow_all()
+ *    -> kvm_arch_flush_shadow_all()
+ *       -> kvm_mmu_zap_all()
+ *          -> kvm_tdp_mmu_zap_all()
+ */
 void kvm_tdp_mmu_zap_all(struct kvm *kvm)
 {
 	struct kvm_mmu_page *root;
@@ -854,6 +1348,12 @@ void kvm_tdp_mmu_zap_all(struct kvm *kvm)
 	 * is being destroyed or the userspace VMM has exited.  In both cases,
 	 * KVM_RUN is unreachable, i.e. no vCPUs will ever service the request.
 	 */
+	/*
+	 * 从参数的头开始, 也就是从kvm->arch.tdp_mmu_roots开始 ...
+	 * 通过tdp_mmu_next_root()遍历kvm->arch.tdp_mmu_roots中所有的root kvm_mmu_page
+	 * 每个iteration开始的时候会get这次的page, 和put上一次的page
+	 * 要求必须有read或者write锁
+	 */
 	for_each_tdp_mmu_root_yield_safe(kvm, root, false)
 		tdp_mmu_zap_root(kvm, root, false);
 }
@@ -862,16 +1362,33 @@ void kvm_tdp_mmu_zap_all(struct kvm *kvm)
  * Zap all invalidated roots to ensure all SPTEs are dropped before the "fast
  * zap" completes.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8013| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_zap_invalidated_roots(kvm);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|54| <<kvm_mmu_uninit_tdp_mmu>> kvm_tdp_mmu_zap_invalidated_roots(kvm);
+ */
 void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
 {
 	struct kvm_mmu_page *root;
 
 	read_lock(&kvm->mmu_lock);
 
+	/*
+	 * 从参数的头开始, 也就是从kvm->arch.tdp_mmu_roots开始 ...
+	 * 通过tdp_mmu_next_root()遍历kvm->arch.tdp_mmu_roots中所有的root kvm_mmu_page
+	 * 每个iteration开始的时候会get这次的page, 和put上一次的page
+	 * 要求必须有read或者write锁
+	 */
 	for_each_tdp_mmu_root_yield_safe(kvm, root, true) {
 		if (!root->tdp_mmu_scheduled_root_to_zap)
 			continue;
 
+		/*
+		 * 在以下使用tdp_mmu_scheduled_root_to_zap:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|927| <<kvm_tdp_mmu_zap_invalidated_roots>> if (!root->tdp_mmu_scheduled_root_to_zap)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|930| <<kvm_tdp_mmu_zap_invalidated_roots>> root->tdp_mmu_scheduled_root_to_zap = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|997| <<kvm_tdp_mmu_invalidate_all_roots>> root->tdp_mmu_scheduled_root_to_zap = true;
+		 */
 		root->tdp_mmu_scheduled_root_to_zap = false;
 		KVM_BUG_ON(!root->role.invalid, kvm);
 
@@ -907,6 +1424,11 @@ void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
  * Note, kvm_tdp_mmu_zap_invalidated_roots() is gifted the TDP MMU's reference.
  * See kvm_tdp_mmu_get_vcpu_root_hpa().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7988| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_invalidate_all_roots(kvm);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|53| <<kvm_mmu_uninit_tdp_mmu>> kvm_tdp_mmu_invalidate_all_roots(kvm);
+ */
 void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
 {
 	struct kvm_mmu_page *root;
@@ -939,6 +1461,12 @@ void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
 		 * root alive after its been zapped.
 		 */
 		if (!root->role.invalid) {
+			/*
+			 * 在以下使用tdp_mmu_scheduled_root_to_zap:
+			 *   - arch/x86/kvm/mmu/tdp_mmu.c|927| <<kvm_tdp_mmu_zap_invalidated_roots>> if (!root->tdp_mmu_scheduled_root_to_zap)
+			 *   - arch/x86/kvm/mmu/tdp_mmu.c|930| <<kvm_tdp_mmu_zap_invalidated_roots>> root->tdp_mmu_scheduled_root_to_zap = false;
+			 *   - arch/x86/kvm/mmu/tdp_mmu.c|997| <<kvm_tdp_mmu_invalidate_all_roots>> root->tdp_mmu_scheduled_root_to_zap = true;
+			 */
 			root->tdp_mmu_scheduled_root_to_zap = true;
 			root->role.invalid = true;
 		}
@@ -949,10 +1477,26 @@ void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
  * Installs a last-level SPTE to handle a TDP page fault.
  * (NPT/EPT violation/misconfiguration)
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1162| <<kvm_tdp_mmu_map>> ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
+ *
+ * kvm_tdp_page_fault() --> 和direct_page_fault()配对
+ * -> kvm_tdp_mmu_page_fault()
+ *    -> kvm_tdp_mmu_map()
+ *       -> tdp_mmu_map_handle_target_level()
+ *
+ * 注释:
+ * Installs a last-level SPTE to handle a TDP page fault.
+ * (NPT/EPT violation/misconfiguration)
+ */
 static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 					  struct kvm_page_fault *fault,
 					  struct tdp_iter *iter)
 {
+	/*
+	 * 获得当前sptep所在的(不是指向的)kvm_mmu_page
+	 */
 	struct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(iter->sptep));
 	u64 new_spte;
 	int ret = RET_PF_FIXED;
@@ -961,6 +1505,13 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 	if (WARN_ON_ONCE(sp->role.level != fault->goal_level))
 		return RET_PF_RETRY;
 
+	/*
+	 * 在以下调用make_spte():
+	 *   - arch/x86/kvm/mmu/mmu.c|2942| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch, true, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|963| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access, gfn, spte_to_pfn(spte), spte, true, false, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|976| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn,
+	 *                                                                                              iter->old_spte, fault->prefetch, true, fault->map_writable, &new_spte);
+	 */
 	if (unlikely(!fault->slot))
 		new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
 	else
@@ -1012,9 +1563,23 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
  * Returns: 0 if the new page table was installed. Non-0 if the page table
  *          could not be installed (e.g. the atomic compare-exchange failed).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1359| <<kvm_tdp_mmu_map>> r = tdp_mmu_link_sp(kvm, &iter, sp, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1680| <<tdp_mmu_split_huge_page>> ret = tdp_mmu_link_sp(kvm, iter, sp, shared);
+ *
+ * 对应tdp_mmu_unlink_sp()
+ * 注释:
+ * Replace the given spte with an spte pointing to the provided page table.
+ * 针对sp->spt做一个指向这个4k page table页面的spte
+ * 然后把新的spte设置给iter->sptep
+ */
 static int tdp_mmu_link_sp(struct kvm *kvm, struct tdp_iter *iter,
 			   struct kvm_mmu_page *sp, bool shared)
 {
+	/*
+	 * 制作一个指向sp->spt的pte, 奇怪nonleaf和leaf有什么不同
+	 */
 	u64 spte = make_nonleaf_spte(sp->spt, !kvm_ad_enabled());
 	int ret = 0;
 
@@ -1038,6 +1603,20 @@ static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
  * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
  * page tables and SPTEs to translate the faulting guest physical address.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4481| <<kvm_tdp_mmu_page_fault>> r = kvm_tdp_mmu_map(vcpu, fault);
+ *
+ * kvm_tdp_page_fault() --> 和direct_page_fault()配对
+ * -> kvm_tdp_mmu_page_fault()
+ *    -> kvm_tdp_mmu_map()
+ *       -> tdp_mmu_map_handle_target_level()
+ *
+ * 下面的三个配对:
+ * - kvm_tdp_mmu_map()
+ * - direct_map()
+ * - FNAME(fetch)
+ */
 int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -1046,12 +1625,32 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	struct kvm_mmu_page *sp;
 	int ret = RET_PF_RETRY;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4603| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|868| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1614| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace:
+	 *   - arch/x86/kvm/mmu/mmu.c|4605| <<direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|870| <<FNAME>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1616| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	rcu_read_lock();
 
+	/*
+	 * 从root_to_sp(_mmu->root.hpa)开始, 遍历gfn到gfn+1(不包含gfn+1)
+	 * 所以就是遍历gfn的那些每个level的pte
+	 *
+	 * 到达PG_LEVEL_4K=1这个level的时候, tdp_mmu_for_each_pte()已经完成了:
+	 * -> iter->sptep = iter->pt_path[iter->level - 1] + SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+	 * -> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 */
 	tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
 		int r;
 
@@ -1082,10 +1681,23 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 
 		sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
 
+		/*
+		 * 关于tdp_mmu_link_sp():
+		 * 对应tdp_mmu_unlink_sp()
+		 * 注释:
+		 * Replace the given spte with an spte pointing to the provided page table.
+		 * 针对sp->spt做一个指向这个4k page table页面的spte
+		 * 然后把新的spte设置给iter->sptep
+		 */
 		if (is_shadow_present_pte(iter.old_spte))
 			r = tdp_mmu_split_huge_page(kvm, &iter, sp, true);
 		else
 			r = tdp_mmu_link_sp(kvm, &iter, sp, true);
+		/*
+		 * 关于上面的tdp_mmu_link_sp():
+		 * Returns: 0 if the new page table was installed. Non-0 if the page table
+		 * could not be installed (e.g. the atomic compare-exchange failed).
+		 */
 
 		/*
 		 * Force the guest to retry if installing an upper level SPTE
@@ -1113,6 +1725,11 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	goto retry;
 
 map_target_level:
+	/*
+	 * 注释:
+	 * Installs a last-level SPTE to handle a TDP page fault.
+	 * (NPT/EPT violation/misconfiguration)
+	 */
 	ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
 
 retry:
@@ -1120,11 +1737,20 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1596| <<kvm_unmap_gfn_range>> flush = kvm_tdp_mmu_unmap_gfn_range(kvm, range, flush);
+ */
 bool kvm_tdp_mmu_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range,
 				 bool flush)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * 从参数的_root开始 ...
+	 * 通过tdp_mmu_next_root()遍历kvm->arch.tdp_mmu_roots中所有的root kvm_mmu_page
+	 * 每个iteration开始的时候会get这次的page, 和put上一次的page
+	 */
 	__for_each_tdp_mmu_root_yield_safe(kvm, root, range->slot->as_id, false, false)
 		flush = tdp_mmu_zap_leafs(kvm, root, range->start, range->end,
 					  range->may_block, flush);
@@ -1135,6 +1761,12 @@ bool kvm_tdp_mmu_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range,
 typedef bool (*tdp_handler_t)(struct kvm *kvm, struct tdp_iter *iter,
 			      struct kvm_gfn_range *range);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1786| <<kvm_tdp_mmu_age_gfn_range>> return kvm_tdp_mmu_handle_gfn(kvm, range, age_gfn_range);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1797| <<kvm_tdp_mmu_test_age_gfn>> return kvm_tdp_mmu_handle_gfn(kvm, range, test_age_gfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1843| <<kvm_tdp_mmu_set_spte_gfn>> return kvm_tdp_mmu_handle_gfn(kvm, range, set_spte_gfn);
+ */
 static __always_inline bool kvm_tdp_mmu_handle_gfn(struct kvm *kvm,
 						   struct kvm_gfn_range *range,
 						   tdp_handler_t handler)
@@ -1150,6 +1782,11 @@ static __always_inline bool kvm_tdp_mmu_handle_gfn(struct kvm *kvm,
 	for_each_tdp_mmu_root(kvm, root, range->slot->as_id) {
 		rcu_read_lock();
 
+		/*
+		 * leaf的continue有两个条件二选一:
+		 * 1. !is_shadow_present_pte(_iter.old_spte)
+		 * 2. !is_last_spte(_iter.old_spte, _iter.level)
+		 */
 		tdp_root_for_each_leaf_pte(iter, root, range->start, range->end)
 			ret |= handler(kvm, &iter, range);
 
@@ -1167,6 +1804,10 @@ static __always_inline bool kvm_tdp_mmu_handle_gfn(struct kvm *kvm,
  * from the clear_young() or clear_flush_young() notifier, which uses the
  * return value to determine if the page has been accessed.
  */
+/*
+ * 在以下使用age_gfn_range():
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1786| <<kvm_tdp_mmu_age_gfn_range>> return kvm_tdp_mmu_handle_gfn(kvm, range, age_gfn_range);
+ */
 static bool age_gfn_range(struct kvm *kvm, struct tdp_iter *iter,
 			  struct kvm_gfn_range *range)
 {
@@ -1196,27 +1837,70 @@ static bool age_gfn_range(struct kvm *kvm, struct tdp_iter *iter,
 							iter->level);
 	}
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|699| <<handle_changed_spte>> trace_kvm_tdp_mmu_spte_changed(as_id, gfn, level, old_spte, new_spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1534| <<age_gfn_range>> trace_kvm_tdp_mmu_spte_changed(iter->as_id, iter->gfn, iter->level, iter->old_spte, new_spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1945| <<clear_dirty_pt_masked>> trace_kvm_tdp_mmu_spte_changed(iter.as_id, iter.gfn, iter.level, iter.old_spte, iter.old_spte & ~dbit);
+	 */
 	trace_kvm_tdp_mmu_spte_changed(iter->as_id, iter->gfn, iter->level,
 				       iter->old_spte, new_spte);
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2323| <<kvm_age_gfn>> young |= kvm_tdp_mmu_age_gfn_range(kvm, range);
+ */
 bool kvm_tdp_mmu_age_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1786| <<kvm_tdp_mmu_age_gfn_range>> return kvm_tdp_mmu_handle_gfn(kvm, range, age_gfn_range);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1797| <<kvm_tdp_mmu_test_age_gfn>> return kvm_tdp_mmu_handle_gfn(kvm, range, test_age_gfn);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1843| <<kvm_tdp_mmu_set_spte_gfn>> return kvm_tdp_mmu_handle_gfn(kvm, range, set_spte_gfn);
+	 */
 	return kvm_tdp_mmu_handle_gfn(kvm, range, age_gfn_range);
 }
 
+/*
+ * 在以下使用test_age_gfn():
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1860| <<kvm_tdp_mmu_test_age_gfn>> return kvm_tdp_mmu_handle_gfn(kvm, range, test_age_gfn);
+ */
 static bool test_age_gfn(struct kvm *kvm, struct tdp_iter *iter,
 			 struct kvm_gfn_range *range)
 {
 	return is_accessed_spte(iter->old_spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2348| <<kvm_test_age_gfn>> young |= kvm_tdp_mmu_test_age_gfn(kvm, range);
+ */
 bool kvm_tdp_mmu_test_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1786| <<kvm_tdp_mmu_age_gfn_range>> return kvm_tdp_mmu_handle_gfn(kvm, range, age_gfn_range);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1797| <<kvm_tdp_mmu_test_age_gfn>> return kvm_tdp_mmu_handle_gfn(kvm, range, test_age_gfn);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1843| <<kvm_tdp_mmu_set_spte_gfn>> return kvm_tdp_mmu_handle_gfn(kvm, range, set_spte_gfn);
+	 */
 	return kvm_tdp_mmu_handle_gfn(kvm, range, test_age_gfn);
 }
 
+/*
+ * struct kvm_gfn_range {
+ *     struct kvm_memory_slot *slot;
+ *     gfn_t start;
+ *     gfn_t end;
+ *     union kvm_mmu_notifier_arg arg;
+ *     -> pte_t pte;
+ *     bool may_block;
+ * };
+ *
+ * 在以下使用set_spte_gfn():
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1906| <<kvm_tdp_mmu_set_spte_gfn>> return kvm_tdp_mmu_handle_gfn(kvm, range, set_spte_gfn);
+ */
 static bool set_spte_gfn(struct kvm *kvm, struct tdp_iter *iter,
 			 struct kvm_gfn_range *range)
 {
@@ -1235,6 +1919,9 @@ static bool set_spte_gfn(struct kvm *kvm, struct tdp_iter *iter,
 	 * invariant that the PFN of a present * leaf SPTE can never change.
 	 * See handle_changed_spte().
 	 */
+	/*
+	 * 为什么要先清0然后再设置呢????
+	 */
 	tdp_mmu_iter_set_spte(kvm, iter, 0);
 
 	if (!pte_write(range->arg.pte)) {
@@ -1253,6 +1940,10 @@ static bool set_spte_gfn(struct kvm *kvm, struct tdp_iter *iter,
  * notifier.
  * Returns non-zero if a flush is needed before releasing the MMU lock.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2219| <<kvm_set_spte_gfn>> flush |= kvm_tdp_mmu_set_spte_gfn(kvm, range);
+ */
 bool kvm_tdp_mmu_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	/*
@@ -1260,6 +1951,12 @@ bool kvm_tdp_mmu_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 	 * target SPTE _must_ be a leaf SPTE, i.e. cannot result in freeing a
 	 * shadow page. See the WARN on pfn_changed in handle_changed_spte().
 	 */
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1786| <<kvm_tdp_mmu_age_gfn_range>> return kvm_tdp_mmu_handle_gfn(kvm, range, age_gfn_range);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1797| <<kvm_tdp_mmu_test_age_gfn>> return kvm_tdp_mmu_handle_gfn(kvm, range, test_age_gfn);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1843| <<kvm_tdp_mmu_set_spte_gfn>> return kvm_tdp_mmu_handle_gfn(kvm, range, set_spte_gfn);
+	 */
 	return kvm_tdp_mmu_handle_gfn(kvm, range, set_spte_gfn);
 }
 
@@ -1268,6 +1965,10 @@ bool kvm_tdp_mmu_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
  * [start, end). Returns true if an SPTE has been changed and the TLBs need to
  * be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1991| <<kvm_tdp_mmu_wrprot_slot>> spte_set |= wrprot_gfn_range(kvm, root, slot->base_gfn, slot->base_gfn + slot->npages, min_level);
+ */
 static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			     gfn_t start, gfn_t end, int min_level)
 {
@@ -1279,6 +1980,11 @@ static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 
 	BUG_ON(min_level > KVM_MAX_HUGEPAGE_LEVEL);
 
+	/*
+	 * 注释
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 */
 	for_each_tdp_pte_min_level(iter, root, min_level, start, end) {
 retry:
 		if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
@@ -1306,6 +2012,10 @@ static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
  * only affect leaf SPTEs down to min_level.
  * Returns true if an SPTE has been changed and the TLBs need to be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8217| <<kvm_mmu_slot_remove_write_access>> kvm_tdp_mmu_wrprot_slot(kvm, memslot, start_level);
+ */
 bool kvm_tdp_mmu_wrprot_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *slot, int min_level)
 {
@@ -1321,6 +2031,11 @@ bool kvm_tdp_mmu_wrprot_slot(struct kvm *kvm,
 	return spte_set;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|2031| <<tdp_mmu_alloc_sp_for_split>> sp = __tdp_mmu_alloc_sp_for_split(GFP_NOWAIT | __GFP_ACCOUNT);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|2043| <<tdp_mmu_alloc_sp_for_split>> sp = __tdp_mmu_alloc_sp_for_split(GFP_KERNEL_ACCOUNT);
+ */
 static struct kvm_mmu_page *__tdp_mmu_alloc_sp_for_split(gfp_t gfp)
 {
 	struct kvm_mmu_page *sp;
@@ -1340,6 +2055,10 @@ static struct kvm_mmu_page *__tdp_mmu_alloc_sp_for_split(gfp_t gfp)
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|2134| <<tdp_mmu_split_huge_pages_root>> sp = tdp_mmu_alloc_sp_for_split(kvm, &iter, shared);
+ */
 static struct kvm_mmu_page *tdp_mmu_alloc_sp_for_split(struct kvm *kvm,
 						       struct tdp_iter *iter,
 						       bool shared)
@@ -1379,6 +2098,11 @@ static struct kvm_mmu_page *tdp_mmu_alloc_sp_for_split(struct kvm *kvm,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1629| <<kvm_tdp_mmu_map>> r = tdp_mmu_split_huge_page(kvm, &iter, sp, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|2025| <<tdp_mmu_split_huge_pages_root>> if (tdp_mmu_split_huge_page(kvm, &iter, sp, shared))
+ */
 /* Note, the caller is responsible for initializing @sp. */
 static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
 				   struct kvm_mmu_page *sp, bool shared)
@@ -1418,6 +2142,10 @@ static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|2157| <<kvm_tdp_mmu_try_split_huge_pages>> r = tdp_mmu_split_huge_pages_root(kvm, root, start, end, target_level, shared);
+ */
 static int tdp_mmu_split_huge_pages_root(struct kvm *kvm,
 					 struct kvm_mmu_page *root,
 					 gfn_t start, gfn_t end,
@@ -1484,6 +2212,11 @@ static int tdp_mmu_split_huge_pages_root(struct kvm *kvm,
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8512| <<kvm_mmu_try_split_huge_pages>> kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, false);
+ *   - arch/x86/kvm/mmu/mmu.c|8541| <<kvm_mmu_slot_try_split_huge_pages>> kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, true);
+ */
 /*
  * Try to split all huge pages mapped by the TDP MMU down to the target level.
  */
@@ -1497,6 +2230,11 @@ void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
 
 	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
 
+	/*
+	 * 从参数的_root开始 ...
+	 * 通过tdp_mmu_next_root()遍历kvm->arch.tdp_mmu_roots中所有的root kvm_mmu_page --> 必须是valid的
+	 * 每个iteration开始的时候会get这次的page, 和put上一次的page
+	 */
 	for_each_valid_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, shared) {
 		r = tdp_mmu_split_huge_pages_root(kvm, root, start, end, target_level, shared);
 		if (r) {
@@ -1513,6 +2251,10 @@ void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
  * each SPTE. Returns true if an SPTE has been changed and the TLBs need to
  * be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|2226| <<kvm_tdp_mmu_clear_dirty_slot>> spte_set |= clear_dirty_gfn_range(kvm, root, slot->base_gfn, slot->base_gfn + slot->npages);
+ */
 static bool clear_dirty_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			   gfn_t start, gfn_t end)
 {
@@ -1522,6 +2264,11 @@ static bool clear_dirty_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 
 	rcu_read_lock();
 
+	/*
+	 * leaf的continue有两个条件二选一:
+	 * 1. !is_shadow_present_pte(_iter.old_spte)
+	 * 2. !is_last_spte(_iter.old_spte, _iter.level)
+	 */
 	tdp_root_for_each_leaf_pte(iter, root, start, end) {
 retry:
 		if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
@@ -1553,6 +2300,10 @@ static bool clear_dirty_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
  * each SPTE. Returns true if an SPTE has been changed and the TLBs need to
  * be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8635| <<kvm_mmu_slot_leaf_clear_dirty>> kvm_tdp_mmu_clear_dirty_slot(kvm, memslot);
+ */
 bool kvm_tdp_mmu_clear_dirty_slot(struct kvm *kvm,
 				  const struct kvm_memory_slot *slot)
 {
@@ -1561,6 +2312,11 @@ bool kvm_tdp_mmu_clear_dirty_slot(struct kvm *kvm,
 
 	lockdep_assert_held_read(&kvm->mmu_lock);
 
+	/*
+	 * 从参数的_root开始 ...
+	 * 通过tdp_mmu_next_root()遍历kvm->arch.tdp_mmu_roots中所有的root kvm_mmu_page --> 必须是valid的
+	 * 每个iteration开始的时候会get这次的page, 和put上一次的page
+	 */
 	for_each_valid_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
 		spte_set |= clear_dirty_gfn_range(kvm, root, slot->base_gfn,
 				slot->base_gfn + slot->npages);
@@ -1575,6 +2331,10 @@ bool kvm_tdp_mmu_clear_dirty_slot(struct kvm *kvm,
  * clearing the dirty status will involve clearing the dirty bit on each SPTE
  * or, if AD bits are not enabled, clearing the writable bit on each SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|2306| <<kvm_tdp_mmu_clear_dirty_pt_masked>> clear_dirty_pt_masked(kvm, root, gfn, mask, wrprot);
+ */
 static void clear_dirty_pt_masked(struct kvm *kvm, struct kvm_mmu_page *root,
 				  gfn_t gfn, unsigned long mask, bool wrprot)
 {
@@ -1586,6 +2346,11 @@ static void clear_dirty_pt_masked(struct kvm *kvm, struct kvm_mmu_page *root,
 
 	rcu_read_lock();
 
+	/*
+	 * leaf的continue有两个条件二选一:
+	 * 1. !is_shadow_present_pte(_iter.old_spte)
+	 * 2. !is_last_spte(_iter.old_spte, _iter.level)
+	 */
 	tdp_root_for_each_leaf_pte(iter, root, gfn + __ffs(mask),
 				    gfn + BITS_PER_LONG) {
 		if (!mask)
@@ -1603,10 +2368,20 @@ static void clear_dirty_pt_masked(struct kvm *kvm, struct kvm_mmu_page *root,
 		if (!(iter.old_spte & dbit))
 			continue;
 
+		/*
+		 * 函数一开始设置的dbit:
+		 * u64 dbit = (wrprot || !kvm_ad_enabled()) ? PT_WRITABLE_MASK : shadow_dirty_mask;
+		 */
 		iter.old_spte = tdp_mmu_clear_spte_bits(iter.sptep,
 							iter.old_spte, dbit,
 							iter.level);
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|699| <<handle_changed_spte>> trace_kvm_tdp_mmu_spte_changed(as_id, gfn, level, old_spte, new_spte);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1534| <<age_gfn_range>> trace_kvm_tdp_mmu_spte_changed(iter->as_id, iter->gfn, iter->level, iter->old_spte, new_spte);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1945| <<clear_dirty_pt_masked>> trace_kvm_tdp_mmu_spte_changed(iter.as_id, iter.gfn, iter.level, iter.old_spte, iter.old_spte & ~dbit);
+		 */
 		trace_kvm_tdp_mmu_spte_changed(iter.as_id, iter.gfn, iter.level,
 					       iter.old_spte,
 					       iter.old_spte & ~dbit);
@@ -1623,6 +2398,11 @@ static void clear_dirty_pt_masked(struct kvm *kvm, struct kvm_mmu_page *root,
  * clearing the dirty status will involve clearing the dirty bit on each SPTE
  * or, if AD bits are not enabled, clearing the writable bit on each SPTE.
  */
+/*
+ * called by;
+ *   - arch/x86/kvm/mmu/mmu.c|1847| <<kvm_mmu_write_protect_pt_masked>> kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot, slot->base_gfn + gfn_offset, mask, true);
+ *   - arch/x86/kvm/mmu/mmu.c|1880| <<kvm_mmu_clear_dirty_pt_masked>> kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot, slot->base_gfn + gfn_offset, mask, false);
+ */
 void kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 				       struct kvm_memory_slot *slot,
 				       gfn_t gfn, unsigned long mask,
@@ -1634,6 +2414,14 @@ void kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 		clear_dirty_pt_masked(kvm, root, gfn, mask, wrprot);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1863| <<kvm_tdp_mmu_zap_collapsible_sptes>> zap_collapsible_spte_range(kvm, root, slot);
+ *
+ * 似乎注释应该是:
+ * Zap non-leaf SPTEs (and free their associated page tables) which could
+ * be replaced by huge pages, for GFNs within the slot.
+ */
 static void zap_collapsible_spte_range(struct kvm *kvm,
 				       struct kvm_mmu_page *root,
 				       const struct kvm_memory_slot *slot)
@@ -1645,6 +2433,13 @@ static void zap_collapsible_spte_range(struct kvm *kvm,
 
 	rcu_read_lock();
 
+	/*
+	 * 注释:
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 *
+	 * PG_LEVEL_2M-->指向4K的entry就不用了
+	 */
 	for_each_tdp_pte_min_level(iter, root, PG_LEVEL_2M, start, end) {
 retry:
 		if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
@@ -1677,6 +2472,11 @@ static void zap_collapsible_spte_range(struct kvm *kvm,
 		if (max_mapping_level < iter.level)
 			continue;
 
+		/*
+		 * 1. tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE);
+		 * 2. kvm_flush_remote_tlbs_gfn(kvm, iter->gfn, iter->level);
+		 * 3. __kvm_tdp_mmu_write_spte(iter->sptep, 0);
+		 */
 		/* Note, a successful atomic zap also does a remote TLB flush. */
 		if (tdp_mmu_zap_spte_atomic(kvm, &iter))
 			goto retry;
@@ -1689,6 +2489,14 @@ static void zap_collapsible_spte_range(struct kvm *kvm,
  * Zap non-leaf SPTEs (and free their associated page tables) which could
  * be replaced by huge pages, for GFNs within the slot.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8519| <<kvm_mmu_zap_collapsible_sptes>> kvm_tdp_mmu_zap_collapsible_sptes(kvm, slot);
+ *
+ * 注释:
+ *  Zap non-leaf SPTEs (and free their associated page tables) which could
+ * be replaced by huge pages, for GFNs within the slot.
+ */
 void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
 				       const struct kvm_memory_slot *slot)
 {
@@ -1696,8 +2504,16 @@ void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
 
 	lockdep_assert_held_read(&kvm->mmu_lock);
 
+	/*
+	 * 从参数的_root开始 ...
+	 * 通过tdp_mmu_next_root()遍历kvm->arch.tdp_mmu_roots中所有的root kvm_mmu_page --> 必须是valid的
+	 * 每个iteration开始的时候会get这次的page, 和put上一次的page
+	 */
 	for_each_valid_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
 		zap_collapsible_spte_range(kvm, root, slot);
+	/*
+	 * 只在此处调用zap_collapsible_spte_range()
+	 */
 }
 
 /*
@@ -1705,6 +2521,10 @@ void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1916| <<kvm_tdp_mmu_write_protect_gfn>> spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
+ */
 static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
 			      gfn_t gfn, int min_level)
 {
@@ -1716,7 +2536,17 @@ static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
 
 	rcu_read_lock();
 
+	/*
+	 * 注释
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 */
 	for_each_tdp_pte_min_level(iter, root, min_level, gfn, gfn + 1) {
+		/*
+		 * 二选一, 就会continue
+		 * 1: !is_shadow_present_pte(iter.old_spte) 
+		 * 2: !is_last_spte(iter.old_spte, iter.level)
+		 */
 		if (!is_shadow_present_pte(iter.old_spte) ||
 		    !is_last_spte(iter.old_spte, iter.level))
 			continue;
@@ -1741,6 +2571,10 @@ static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1966| <<kvm_mmu_slot_gfn_write_protect>> kvm_tdp_mmu_write_protect_gfn(kvm, slot, gfn, min_level);
+ */
 bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level)
@@ -1749,8 +2583,22 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 	bool spte_set = false;
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
+	/*
+	 * 注释
+	 * Iterate over all TDP MMU roots.  Requires that mmu_lock be held for write,
+	 * the implication being that any flow that holds mmu_lock for read is
+	 * inherently yield-friendly and should use the yield-safe variant above.
+	 * Holding mmu_lock for write obviates the need for RCU protection as the list
+	 * is guaranteed to be stable.
+	 * 遍历_kvm->arch.tdp_mmu_roots, 要求相同的as_id
+	 * 要求必须有write lock
+	 * (似乎说明write lock让read lock的owner yield friendly?????)
+	 */
 	for_each_tdp_mmu_root(kvm, root, slot->as_id)
 		spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
+	/*
+	 * 只在上面调用write_protect_gfn()
+	 */
 
 	return spte_set;
 }
@@ -1761,6 +2609,10 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
  *
  * Must be called between kvm_tdp_mmu_walk_lockless_{begin,end}.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5638| <<get_mmio_spte>> leaf = kvm_tdp_mmu_get_walk(vcpu, addr, sptes, &root);
+ */
 int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 			 int *root_level)
 {
@@ -1771,6 +2623,14 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 
 	*root_level = vcpu->arch.mmu->root_role.level;
 
+	/*
+	 * 从root_to_sp(_mmu->root.hpa)开始, 遍历gfn到gfn+1(不包含gfn+1)
+	 * 所以就是遍历gfn的那些每个level的pte
+	 *
+	 * 到达PG_LEVEL_4K=1这个level的时候, tdp_mmu_for_each_pte()已经完成了:
+	 * -> iter->sptep = iter->pt_path[iter->level - 1] + SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+	 * -> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 */
 	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
 		leaf = iter.level;
 		sptes[leaf] = iter.old_spte;
@@ -1790,6 +2650,10 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
  *
  * WARNING: This function is only intended to be called during fast_page_fault.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4820| <<fast_page_fault>> sptep = kvm_tdp_mmu_fast_pf_get_last_sptep(vcpu, fault->addr, &spte);
+ */
 u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 					u64 *spte)
 {
@@ -1798,6 +2662,14 @@ u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 	gfn_t gfn = addr >> PAGE_SHIFT;
 	tdp_ptep_t sptep = NULL;
 
+	/*
+	 * 从root_to_sp(_mmu->root.hpa)开始, 遍历gfn到gfn+1(不包含gfn+1)
+	 * 所以就是遍历gfn的那些每个level的pte
+	 *
+	 * 到达PG_LEVEL_4K=1这个level的时候, tdp_mmu_for_each_pte()已经完成了:
+	 * -> iter->sptep = iter->pt_path[iter->level - 1] + SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+	 * -> iter->old_spte = kvm_tdp_mmu_read_spte(iter->sptep);
+	 */
 	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
 		*spte = iter.old_spte;
 		sptep = iter.sptep;
diff --git a/arch/x86/kvm/mmu/tdp_mmu.h b/arch/x86/kvm/mmu/tdp_mmu.h
index 733a3aef3..bf2addb29 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@ -12,8 +12,20 @@ void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm);
 
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|125| <<tdp_mmu_next_root>> kvm_tdp_mmu_get_root(next_root))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|234| <<kvm_tdp_mmu_get_vcpu_root_hpa>> kvm_tdp_mmu_get_root(root))
+ */
 __must_check static inline bool kvm_tdp_mmu_get_root(struct kvm_mmu_page *root)
 {
+	/*
+	 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|105| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|282| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 2);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|791| <<tdp_mmu_zap_root>> WARN_ON_ONCE(!refcount_read(&root->tdp_mmu_root_count));
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|22| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+	 */
 	return refcount_inc_not_zero(&root->tdp_mmu_root_count);
 }
 
@@ -54,11 +66,19 @@ void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
 				      gfn_t start, gfn_t end,
 				      int target_level, bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|822| <<walk_shadow_page_lockless_begin>> kvm_tdp_mmu_walk_lockless_begin();
+ */
 static inline void kvm_tdp_mmu_walk_lockless_begin(void)
 {
 	rcu_read_lock();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|671| <<walk_shadow_page_lockless_end>> kvm_tdp_mmu_walk_lockless_end();
+ */
 static inline void kvm_tdp_mmu_walk_lockless_end(void)
 {
 	rcu_read_unlock();
@@ -70,6 +90,11 @@ u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 					u64 *spte);
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用kvm_mmu_page->tdp_mmu_page:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|309| <<tdp_mmu_init_sp>> sp->tdp_mmu_page = true;
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|82| <<is_tdp_mmu_page>> static inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return sp->tdp_mmu_page; }
+ */
 static inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return sp->tdp_mmu_page; }
 #else
 static inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return false; }
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 9ae07db6f..ceaa368b6 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -731,6 +731,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3820| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|9463| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|9787| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|9789| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/smm.c b/arch/x86/kvm/smm.c
index dc3d95fdc..abf366ebd 100644
--- a/arch/x86/kvm/smm.c
+++ b/arch/x86/kvm/smm.c
@@ -9,12 +9,25 @@
 #include "cpuid.h"
 #include "trace.h"
 
+/*
+ * SMM模式通过调用SMI进入,进入之后,SMI就会disable,不过系统会
+ * 暂存一个且只有一个SMI,当SMM模式退出时,检测到这个SMI会再次进入SMM模式.
+ *
+ * 进入SMM模式后,系统切换到SMRAM这个独立的环境中.
+ *
+ * RSM会使得系统离开SMM模式,RSM只有在SMM中才可以执行.
+ */
+
 #define CHECK_SMRAM32_OFFSET(field, offset) \
 	ASSERT_STRUCT_OFFSET(struct kvm_smram_state_32, field, offset - 0xFE00)
 
 #define CHECK_SMRAM64_OFFSET(field, offset) \
 	ASSERT_STRUCT_OFFSET(struct kvm_smram_state_64, field, offset - 0xFE00)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|288| <<enter_smm>> check_smram_offsets();
+ */
 static void check_smram_offsets(void)
 {
 	/* 32 bit SMRAM image */
@@ -109,10 +122,25 @@ static void check_smram_offsets(void)
 #undef CHECK_SMRAM32_OFFSET
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|310| <<enter_smm>> kvm_smm_changed(vcpu, true);
+ *   - arch/x86/kvm/smm.c|588| <<emulator_leave_smm>> kvm_smm_changed(vcpu, false);
+ *   - arch/x86/kvm/x86.c|5420| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_smm_changed(vcpu, events->smi.smm);
+ */
 void kvm_smm_changed(struct kvm_vcpu *vcpu, bool entering_smm)
 {
 	trace_kvm_smm_transition(vcpu->vcpu_id, vcpu->arch.smbase, entering_smm);
 
+	/*
+	 * 在以下使用HF_SMM_MASK:
+	 *   - arch/x86/include/asm/kvm_host.h|2133| <<global>> #define HF_SMM_MASK (1 << 1)
+	 *   - arch/x86/include/asm/kvm_host.h|2138| <<kvm_arch_vcpu_memslots_id>> #define kvm_arch_vcpu_memslots_id(vcpu) ((vcpu)->arch.hflags & HF_SMM_MASK ? 1 : 0)
+	 *   - arch/x86/kvm/smm.c|117| <<kvm_smm_changed>> vcpu->arch.hflags |= HF_SMM_MASK;
+	 *   - arch/x86/kvm/smm.c|119| <<kvm_smm_changed>> vcpu->arch.hflags &= ~(HF_SMM_MASK | HF_SMM_INSIDE_NMI_MASK);
+	 *   - arch/x86/kvm/smm.h|151| <<is_smm>> return vcpu->arch.hflags & HF_SMM_MASK;
+	 *   - arch/x86/kvm/x86.c|5418| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> if (!!(vcpu->arch.hflags & HF_SMM_MASK) != events->smi.smm) {
+	 */
 	if (entering_smm) {
 		vcpu->arch.hflags |= HF_SMM_MASK;
 	} else {
@@ -132,12 +160,36 @@ void kvm_smm_changed(struct kvm_vcpu *vcpu, bool entering_smm)
 	kvm_mmu_reset_context(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5258| <<kvm_vcpu_ioctl_x86_get_vcpu_events(KVM_REQ_SMI)>> process_smi(vcpu);
+ *   - arch/x86/kvm/x86.c|10715| <<vcpu_enter_guest(KVM_REQ_SMI)>> process_smi(vcpu);
+ */
 void process_smi(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->smi_pending:
+	 *   - arch/x86/kvm/smm.c|161| <<process_smi>> vcpu->arch.smi_pending = true;
+	 *   - arch/x86/kvm/svm/nested.c|1477| <<svm_check_nested_events>> if (vcpu->arch.smi_pending && !svm_smi_blocked(vcpu)) {
+	 *   - arch/x86/kvm/svm/svm.c|2421| <<svm_set_gif>> if (svm->vcpu.arch.smi_pending ||
+	 *   - arch/x86/kvm/vmx/nested.c|4156| <<vmx_check_nested_events>> if (vcpu->arch.smi_pending && !is_smm(vcpu)) {
+	 *   - arch/x86/kvm/x86.c|5325| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> events->smi.pending = vcpu->arch.smi_pending;
+	 *   - arch/x86/kvm/x86.c|5423| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.smi_pending = events->smi.pending;
+	 *   - arch/x86/kvm/x86.c|10316| <<kvm_check_and_inject_events>> if (vcpu->arch.smi_pending) {
+	 *   - arch/x86/kvm/x86.c|10321| <<kvm_check_and_inject_events>> vcpu->arch.smi_pending = false;
+	 *   - arch/x86/kvm/x86.c|11561| <<kvm_arch_vcpu_ioctl_set_mpstate>> if ((!kvm_apic_init_sipi_allowed(vcpu) || vcpu->arch.smi_pending) &&
+	 *   - arch/x86/kvm/x86.c|12176| <<kvm_vcpu_reset>> vcpu->arch.smi_pending = 0;
+	 *   - arch/x86/kvm/x86.c|12984| <<kvm_vcpu_has_events>> (vcpu->arch.smi_pending &&
+	 */
 	vcpu->arch.smi_pending = true;
 	kvm_make_request(KVM_REQ_EVENT, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|165| <<enter_smm_save_seg_32>> state->flags = enter_smm_get_segment_flags(&seg);
+ *   - arch/x86/kvm/smm.c|177| <<enter_smm_save_seg_64>> state->attributes = enter_smm_get_segment_flags(&seg) >> 8;
+ */
 static u32 enter_smm_get_segment_flags(struct kvm_segment *seg)
 {
 	u32 flags = 0;
@@ -152,6 +204,17 @@ static u32 enter_smm_get_segment_flags(struct kvm_segment *seg)
 	return flags;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|241| <<enter_smm_save_state_32>> enter_smm_save_seg_32(vcpu, &smram->tr, &smram->tr_sel, VCPU_SREG_TR);
+ *   - arch/x86/kvm/smm.c|242| <<enter_smm_save_state_32>> enter_smm_save_seg_32(vcpu, &smram->ldtr, &smram->ldtr_sel, VCPU_SREG_LDTR);
+ *   - arch/x86/kvm/smm.c|252| <<enter_smm_save_state_32>> enter_smm_save_seg_32(vcpu, &smram->es, &smram->es_sel, VCPU_SREG_ES);
+ *   - arch/x86/kvm/smm.c|253| <<enter_smm_save_state_32>> enter_smm_save_seg_32(vcpu, &smram->cs, &smram->cs_sel, VCPU_SREG_CS);
+ *   - arch/x86/kvm/smm.c|254| <<enter_smm_save_state_32>> enter_smm_save_seg_32(vcpu, &smram->ss, &smram->ss_sel, VCPU_SREG_SS);
+ *   - arch/x86/kvm/smm.c|256| <<enter_smm_save_state_32>> enter_smm_save_seg_32(vcpu, &smram->ds, &smram->ds_sel, VCPU_SREG_DS);
+ *   - arch/x86/kvm/smm.c|257| <<enter_smm_save_state_32>> enter_smm_save_seg_32(vcpu, &smram->fs, &smram->fs_sel, VCPU_SREG_FS);
+ *   - arch/x86/kvm/smm.c|258| <<enter_smm_save_state_32>> enter_smm_save_seg_32(vcpu, &smram->gs, &smram->gs_sel, VCPU_SREG_GS);
+ */
 static void enter_smm_save_seg_32(struct kvm_vcpu *vcpu,
 				  struct kvm_smm_seg_state_32 *state,
 				  u32 *selector, int n)
@@ -165,6 +228,17 @@ static void enter_smm_save_seg_32(struct kvm_vcpu *vcpu,
 	state->flags = enter_smm_get_segment_flags(&seg);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|300| <<enter_smm_save_state_64>> enter_smm_save_seg_64(vcpu, &smram->tr, VCPU_SREG_TR);
+ *   - arch/x86/kvm/smm.c|306| <<enter_smm_save_state_64>> enter_smm_save_seg_64(vcpu, &smram->ldtr, VCPU_SREG_LDTR);
+ *   - arch/x86/kvm/smm.c|312| <<enter_smm_save_state_64>> enter_smm_save_seg_64(vcpu, &smram->es, VCPU_SREG_ES);
+ *   - arch/x86/kvm/smm.c|313| <<enter_smm_save_state_64>> enter_smm_save_seg_64(vcpu, &smram->cs, VCPU_SREG_CS);
+ *   - arch/x86/kvm/smm.c|314| <<enter_smm_save_state_64>> enter_smm_save_seg_64(vcpu, &smram->ss, VCPU_SREG_SS);
+ *   - arch/x86/kvm/smm.c|315| <<enter_smm_save_state_64>> enter_smm_save_seg_64(vcpu, &smram->ds, VCPU_SREG_DS);
+ *   - arch/x86/kvm/smm.c|316| <<enter_smm_save_state_64>> enter_smm_save_seg_64(vcpu, &smram->fs, VCPU_SREG_FS);
+ *   - arch/x86/kvm/smm.c|317| <<enter_smm_save_state_64>> enter_smm_save_seg_64(vcpu, &smram->gs, VCPU_SREG_GS);
+ */
 #ifdef CONFIG_X86_64
 static void enter_smm_save_seg_64(struct kvm_vcpu *vcpu,
 				  struct kvm_smm_seg_state_64 *state,
@@ -180,6 +254,10 @@ static void enter_smm_save_seg_64(struct kvm_vcpu *vcpu,
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|347| <<enter_smm>> enter_smm_save_state_32(vcpu, &smram.smram32);
+ */
 static void enter_smm_save_state_32(struct kvm_vcpu *vcpu,
 				    struct kvm_smram_state_32 *smram)
 {
@@ -223,10 +301,18 @@ static void enter_smm_save_state_32(struct kvm_vcpu *vcpu,
 	smram->smm_revision = 0x00020000;
 	smram->smbase = vcpu->arch.smbase;
 
+	/*
+	 * vmx_get_interrupt_shadow()
+	 * svm_get_interrupt_shadow()
+	 */
 	smram->int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|340| <<enter_smm>> enter_smm_save_state_64(vcpu, &smram.smram64);
+ */
 static void enter_smm_save_state_64(struct kvm_vcpu *vcpu,
 				    struct kvm_smram_state_64 *smram)
 {
@@ -274,10 +360,28 @@ static void enter_smm_save_state_64(struct kvm_vcpu *vcpu,
 	enter_smm_save_seg_64(vcpu, &smram->fs, VCPU_SREG_FS);
 	enter_smm_save_seg_64(vcpu, &smram->gs, VCPU_SREG_GS);
 
+	/*
+	 * 在以下使用kvm_smram_state_64->int_shadow:
+	 *   - arch/x86/kvm/smm.c|264| <<enter_smm_save_state_32>> smram->int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);
+	 *   - arch/x86/kvm/smm.c|315| <<enter_smm_save_state_64>> smram->int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);
+	 *   - arch/x86/kvm/smm.c|573| <<rsm_load_state_32>> ctxt->interruptibility = (u8)smstate->int_shadow;
+	 *   - arch/x86/kvm/smm.c|630| <<rsm_load_state_64>> ctxt->interruptibility = (u8)smstate->int_shadow;
+	 *
+	 * vmx_get_interrupt_shadow()
+	 * svm_get_interrupt_shadow()
+	 */
 	smram->int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10319| <<kvm_check_and_inject_events>> enter_smm(vcpu);
+ *
+ * vcpu_enter_guest()
+ * -> kvm_check_and_inject_events()
+ *    -> enter_smm()
+ */
 void enter_smm(struct kvm_vcpu *vcpu)
 {
 	struct kvm_segment cs, ds;
@@ -285,6 +389,9 @@ void enter_smm(struct kvm_vcpu *vcpu)
 	unsigned long cr0;
 	union kvm_smram smram;
 
+	/*
+	 * 只在这里调用
+	 */
 	check_smram_offsets();
 
 	memset(smram.bytes, 0, sizeof(smram.bytes));
@@ -304,11 +411,21 @@ void enter_smm(struct kvm_vcpu *vcpu)
 	 * Kill the VM in the unlikely case of failure, because the VM
 	 * can be in undefined state in this case.
 	 */
+	/*
+	 * vmx_enter_smm()
+	 * svm_enter_smm()
+	 */
 	if (static_call(kvm_x86_enter_smm)(vcpu, &smram))
 		goto error;
 
 	kvm_smm_changed(vcpu, true);
 
+	/*
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> u64 smbase;
+	 *    -> u64 smi_count;
+	 */
 	if (kvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, &smram, sizeof(smram)))
 		goto error;
 
@@ -320,11 +437,21 @@ void enter_smm(struct kvm_vcpu *vcpu)
 	kvm_set_rflags(vcpu, X86_EFLAGS_FIXED);
 	kvm_rip_write(vcpu, 0x8000);
 
+	/*
+	 * vmx_set_interrupt_shadow()
+	 * svm_set_interrupt_shadow()
+	 *
+	 * 这里设置成0!!!
+	 */
 	static_call(kvm_x86_set_interrupt_shadow)(vcpu, 0);
 
 	cr0 = vcpu->arch.cr0 & ~(X86_CR0_PE | X86_CR0_EM | X86_CR0_TS | X86_CR0_PG);
 	static_call(kvm_x86_set_cr0)(vcpu, cr0);
 
+	/*
+	 * vmx_set_cr4()
+	 * svm_set_cr4()
+	 */
 	static_call(kvm_x86_set_cr4)(vcpu, 0);
 
 	/* Undocumented: IDT limit is set to zero on entry to SMM.  */
@@ -372,6 +499,11 @@ void enter_smm(struct kvm_vcpu *vcpu)
 	kvm_vm_dead(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|459| <<rsm_load_seg_32>> rsm_set_desc_flags(&desc, state->flags);
+ *   - arch/x86/kvm/smm.c|473| <<rsm_load_seg_64>> rsm_set_desc_flags(&desc, state->attributes << 8);
+ */
 static void rsm_set_desc_flags(struct kvm_segment *desc, u32 flags)
 {
 	desc->g    = (flags >> 23) & 1;
@@ -387,6 +519,17 @@ static void rsm_set_desc_flags(struct kvm_segment *desc, u32 flags)
 	desc->padding = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|602| <<rsm_load_state_32>> rsm_load_seg_32(vcpu, &smstate->tr, smstate->tr_sel, VCPU_SREG_TR);
+ *   - arch/x86/kvm/smm.c|603| <<rsm_load_state_32>> rsm_load_seg_32(vcpu, &smstate->ldtr, smstate->ldtr_sel, VCPU_SREG_LDTR);
+ *   - arch/x86/kvm/smm.c|613| <<rsm_load_state_32>> rsm_load_seg_32(vcpu, &smstate->es, smstate->es_sel, VCPU_SREG_ES);
+ *   - arch/x86/kvm/smm.c|614| <<rsm_load_state_32>> rsm_load_seg_32(vcpu, &smstate->cs, smstate->cs_sel, VCPU_SREG_CS);
+ *   - arch/x86/kvm/smm.c|615| <<rsm_load_state_32>> rsm_load_seg_32(vcpu, &smstate->ss, smstate->ss_sel, VCPU_SREG_SS);
+ *   - arch/x86/kvm/smm.c|617| <<rsm_load_state_32>> rsm_load_seg_32(vcpu, &smstate->ds, smstate->ds_sel, VCPU_SREG_DS);
+ *   - arch/x86/kvm/smm.c|618| <<rsm_load_state_32>> rsm_load_seg_32(vcpu, &smstate->fs, smstate->fs_sel, VCPU_SREG_FS);
+ *   - arch/x86/kvm/smm.c|619| <<rsm_load_state_32>> rsm_load_seg_32(vcpu, &smstate->gs, smstate->gs_sel, VCPU_SREG_GS);
+ */
 static int rsm_load_seg_32(struct kvm_vcpu *vcpu,
 			   const struct kvm_smm_seg_state_32 *state,
 			   u16 selector, int n)
@@ -403,6 +546,17 @@ static int rsm_load_seg_32(struct kvm_vcpu *vcpu,
 
 #ifdef CONFIG_X86_64
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|667| <<rsm_load_state_64>> rsm_load_seg_64(vcpu, &smstate->tr, VCPU_SREG_TR);
+ *   - arch/x86/kvm/smm.c|673| <<rsm_load_state_64>> rsm_load_seg_64(vcpu, &smstate->ldtr, VCPU_SREG_LDTR);
+ *   - arch/x86/kvm/smm.c|683| <<rsm_load_state_64>> rsm_load_seg_64(vcpu, &smstate->es, VCPU_SREG_ES);
+ *   - arch/x86/kvm/smm.c|684| <<rsm_load_state_64>> rsm_load_seg_64(vcpu, &smstate->cs, VCPU_SREG_CS);
+ *   - arch/x86/kvm/smm.c|685| <<rsm_load_state_64>> rsm_load_seg_64(vcpu, &smstate->ss, VCPU_SREG_SS);
+ *   - arch/x86/kvm/smm.c|686| <<rsm_load_state_64>> rsm_load_seg_64(vcpu, &smstate->ds, VCPU_SREG_DS);
+ *   - arch/x86/kvm/smm.c|687| <<rsm_load_state_64>> rsm_load_seg_64(vcpu, &smstate->fs, VCPU_SREG_FS);
+ *   - arch/x86/kvm/smm.c|688| <<rsm_load_state_64>> rsm_load_seg_64(vcpu, &smstate->gs, VCPU_SREG_GS);
+ */
 static int rsm_load_seg_64(struct kvm_vcpu *vcpu,
 			   const struct kvm_smm_seg_state_64 *state,
 			   int n)
@@ -418,6 +572,11 @@ static int rsm_load_seg_64(struct kvm_vcpu *vcpu,
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|505| <<rsm_load_state_32>> r = rsm_enter_protected_mode(vcpu, smstate->cr0,
+ *   - arch/x86/kvm/smm.c|553| <<rsm_load_state_64>> r = rsm_enter_protected_mode(vcpu, smstate->cr0, smstate->cr3, smstate->cr4);
+ */
 static int rsm_enter_protected_mode(struct kvm_vcpu *vcpu,
 				    u64 cr0, u64 cr3, u64 cr4)
 {
@@ -463,6 +622,10 @@ static int rsm_enter_protected_mode(struct kvm_vcpu *vcpu,
 	return X86EMUL_CONTINUE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|731| <<emulator_leave_smm>> return rsm_load_state_32(ctxt, &smram.smram32);
+ */
 static int rsm_load_state_32(struct x86_emulate_ctxt *ctxt,
 			     const struct kvm_smram_state_32 *smstate)
 {
@@ -508,6 +671,10 @@ static int rsm_load_state_32(struct x86_emulate_ctxt *ctxt,
 	if (r != X86EMUL_CONTINUE)
 		return r;
 
+	/*
+	 * vmx_set_interrupt_shadow()
+	 * svm_set_interrupt_shadow()
+	 */
 	static_call(kvm_x86_set_interrupt_shadow)(vcpu, 0);
 	ctxt->interruptibility = (u8)smstate->int_shadow;
 
@@ -515,6 +682,14 @@ static int rsm_load_state_32(struct x86_emulate_ctxt *ctxt,
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|643| <<emulator_leave_smm>> return rsm_load_state_64(ctxt, &smram.smram64);
+ *
+ * em_rsm()
+ * -> ctxt->ops->leave_smm = emulator_leave_smm()
+ *    -> rsm_load_state_64()
+ */
 static int rsm_load_state_64(struct x86_emulate_ctxt *ctxt,
 			     const struct kvm_smram_state_64 *smstate)
 {
@@ -561,6 +736,17 @@ static int rsm_load_state_64(struct x86_emulate_ctxt *ctxt,
 	rsm_load_seg_64(vcpu, &smstate->fs, VCPU_SREG_FS);
 	rsm_load_seg_64(vcpu, &smstate->gs, VCPU_SREG_GS);
 
+	/*
+	 * arch/x86/kvm/svm/svm.c|4963| <<global>> .set_interrupt_shadow = svm_set_interrupt_shadow,
+	 * arch/x86/kvm/vmx/vmx.c|8713| <<global>> .set_interrupt_shadow = vmx_set_interrupt_shadow,
+	 *
+	 * vmx:
+	 * 在VMCS:GUEST_INTERRUPTIBILITY_INFO设置GUEST_INTR_STATE_MOV_SS或者GUEST_INTR_STATE_STI
+	 * 如果参数是0就是清空那两个bit
+	 *
+	 * svm:
+	 * 根据mask, 清空或者设置SVM_INTERRUPT_SHADOW_MASK
+	 */
 	static_call(kvm_x86_set_interrupt_shadow)(vcpu, 0);
 	ctxt->interruptibility = (u8)smstate->int_shadow;
 
@@ -568,6 +754,29 @@ static int rsm_load_state_64(struct x86_emulate_ctxt *ctxt,
 }
 #endif
 
+/*
+ * [0] ctxt->ops->leave_smm = emulator_leave_smm()
+ * [0] em_rsm
+ * [0] x86_emulate_insn
+ * [0] x86_emulate_instruction
+ * [0] handle_ud
+ * [0] vmx_handle_exit
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * The only way to exit SMM is to execute the RSM instruction. The RSM
+ * instruction is only available to the SMI handler; if the processor is not in
+ * SMM, attempts to execute the RSM instruction result in an invalid-opcode
+ * excep- tion (#UD) being generated.
+ *
+ * called by:
+ *   - arch/x86/kvm/emulate.c|2322| <<em_rsm>> if (ctxt->ops->leave_smm(ctxt))
+ *
+ * struct x86_emulate_ops emulate_ops.leave_smm = emulator_leave_smm()
+ */
 int emulator_leave_smm(struct x86_emulate_ctxt *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -582,6 +791,19 @@ int emulator_leave_smm(struct x86_emulate_ctxt *ctxt)
 	if (ret < 0)
 		return X86EMUL_UNHANDLEABLE;
 
+	/*
+	 * 在以下使用HF_SMM_INSIDE_NMI_MASK:
+	 *   - arch/x86/include/asm/kvm_host.h|2159| <<global>> #define HF_SMM_INSIDE_NMI_MASK (1 << 2)
+	 *   - arch/x86/kvm/smm.c|138| <<kvm_smm_changed>> vcpu->arch.hflags &= ~(HF_SMM_MASK | HF_SMM_INSIDE_NMI_MASK);
+	 *   - arch/x86/kvm/smm.c|363| <<enter_smm>> vcpu->arch.hflags |= HF_SMM_INSIDE_NMI_MASK;
+	 *   - arch/x86/kvm/smm.c|647| <<emulator_leave_smm>> if ((vcpu->arch.hflags & HF_SMM_INSIDE_NMI_MASK) == 0)
+	 *   - arch/x86/kvm/x86.c|5327| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> !!(vcpu->arch.hflags & HF_SMM_INSIDE_NMI_MASK);
+	 *   - arch/x86/kvm/x86.c|5427| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.hflags |= HF_SMM_INSIDE_NMI_MASK;
+	 *   - arch/x86/kvm/x86.c|5429| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.hflags &= ~HF_SMM_INSIDE_NMI_MASK;
+	 *
+	 * vmx_set_nmi_mask()
+	 * svm_set_nmi_mask()
+	 */
 	if ((vcpu->arch.hflags & HF_SMM_INSIDE_NMI_MASK) == 0)
 		static_call(kvm_x86_set_nmi_mask)(vcpu, false);
 
@@ -635,6 +857,9 @@ int emulator_leave_smm(struct x86_emulate_ctxt *ctxt)
 	 * state (e.g. enter guest mode) before loading state from the SMM
 	 * state-save area.
 	 */
+	/*
+	 * vmx_leave_smm()
+	 */
 	if (static_call(kvm_x86_leave_smm)(vcpu, &smram))
 		return X86EMUL_UNHANDLEABLE;
 
diff --git a/arch/x86/kvm/smm.h b/arch/x86/kvm/smm.h
index a1cf2ac5b..bc8b80a08 100644
--- a/arch/x86/kvm/smm.h
+++ b/arch/x86/kvm/smm.h
@@ -98,6 +98,13 @@ struct kvm_smram_state_64 {
 	u8 io_inst_restart;
 	u8 auto_hlt_restart;
 	u8 amd_nmi_mask; /* Documented in AMD BKDG as NMI mask, not used by KVM */
+	/*
+	 * 在以下使用kvm_smram_state_64->int_shadow:
+	 *   - arch/x86/kvm/smm.c|264| <<enter_smm_save_state_32>> smram->int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);
+	 *   - arch/x86/kvm/smm.c|315| <<enter_smm_save_state_64>> smram->int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);
+	 *   - arch/x86/kvm/smm.c|573| <<rsm_load_state_32>> ctxt->interruptibility = (u8)smstate->int_shadow;
+	 *   - arch/x86/kvm/smm.c|630| <<rsm_load_state_64>> ctxt->interruptibility = (u8)smstate->int_shadow;
+	 */
 	u8 int_shadow;
 	u32 reserved2;
 
@@ -113,6 +120,24 @@ struct kvm_smram_state_64 {
 
 	u32 reserved3[3];
 	u32 smm_revison;
+	/*
+	 * 在以下使用kvm_vcpu_arch->smbase:
+	 *   - arch/x86/kvm/smm.c|114| <<kvm_smm_changed>> trace_kvm_smm_transition(vcpu->vcpu_id, vcpu->arch.smbase, entering_smm);
+	 *   - arch/x86/kvm/smm.c|224| <<enter_smm_save_state_32>> smram->smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|253| <<enter_smm_save_state_64>> smram->smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|312| <<enter_smm>> if (kvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, &smram, sizeof(smram)))
+	 *   - arch/x86/kvm/smm.c|337| <<enter_smm>> cs.selector = (vcpu->arch.smbase >> 4) & 0xffff;
+	 *   - arch/x86/kvm/smm.c|338| <<enter_smm>> cs.base = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|579| <<emulator_leave_smm>> smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|581| <<emulator_leave_smm>> ret = kvm_vcpu_read_guest(vcpu, smbase + 0xfe00, smram.bytes, sizeof(smram));
+	 *
+	 * 在以下设置kvm_vcpu_arch->smbase:
+	 *   - arch/x86/kvm/smm.c|503| <<rsm_load_state_32>> vcpu->arch.smbase = smstate->smbase;
+	 *   - arch/x86/kvm/smm.c|536| <<rsm_load_state_64>> vcpu->arch.smbase = smstate->smbase;
+	 *   - arch/x86/kvm/x86.c|3879| <<kvm_set_msr_common(MSR_IA32_SMBASE)>> vcpu->arch.smbase = data;
+	 *   - arch/x86/kvm/x86.c|4279| <<kvm_get_msr_common(MSR_IA32_SMBASE)>> msr_info->data = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/x86.c|12211| <<kvm_vcpu_reset>> vcpu->arch.smbase = 0x30000;
+	 */
 	u32 smbase;
 	u32 reserved4[5];
 
@@ -140,14 +165,38 @@ union kvm_smram {
 	u8 bytes[512];
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1337| <<__apic_accept_irq(APIC_DM_SMI)>> if (!kvm_inject_smi(vcpu)) {
+ *   - arch/x86/kvm/x86.c|5830| <<kvm_arch_vcpu_ioctl(KVM_SMI)>> r = kvm_inject_smi(vcpu);
+ */
 static inline int kvm_inject_smi(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用KVM_REQ_SMI:
+	 *   - arch/x86/kvm/smm.h|163| <<kvm_inject_smi>> kvm_make_request(KVM_REQ_SMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|5257| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> if (kvm_check_request(KVM_REQ_SMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|10740| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_SMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|12998| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_SMI, vcpu) ||
+	 *   - arch/x86/kvm/x86.c|13047| <<kvm_arch_dy_runnable>> kvm_test_request(KVM_REQ_SMI, vcpu) ||
+	 *
+	 * 处理的函数是process_smi()
+	 */
 	kvm_make_request(KVM_REQ_SMI, vcpu);
 	return 0;
 }
 
 static inline bool is_smm(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用HF_SMM_MASK:
+	 *   - arch/x86/include/asm/kvm_host.h|2133| <<global>> #define HF_SMM_MASK (1 << 1)
+	 *   - arch/x86/include/asm/kvm_host.h|2138| <<kvm_arch_vcpu_memslots_id>> #define kvm_arch_vcpu_memslots_id(vcpu) ((vcpu)->arch.hflags & HF_SMM_MASK ? 1 : 0)
+	 *   - arch/x86/kvm/smm.c|117| <<kvm_smm_changed>> vcpu->arch.hflags |= HF_SMM_MASK;
+	 *   - arch/x86/kvm/smm.c|119| <<kvm_smm_changed>> vcpu->arch.hflags &= ~(HF_SMM_MASK | HF_SMM_INSIDE_NMI_MASK);
+	 *   - arch/x86/kvm/smm.h|151| <<is_smm>> return vcpu->arch.hflags & HF_SMM_MASK;
+	 *   - arch/x86/kvm/x86.c|5418| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> if (!!(vcpu->arch.hflags & HF_SMM_MASK) != events->smi.smm) {
+	 */
 	return vcpu->arch.hflags & HF_SMM_MASK;
 }
 
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index a8bd4e909..659dfa4bb 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -355,6 +355,9 @@ static u32 svm_get_interrupt_shadow(struct kvm_vcpu *vcpu)
 	return ret;
 }
 
+/*
+ * 根据mask, 清空或者设置SVM_INTERRUPT_SHADOW_MASK
+ */
 static void svm_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -3470,6 +3473,13 @@ int svm_invoke_exit_handler(struct kvm_vcpu *vcpu, u64 exit_code)
 	return svm_exit_handlers[exit_code](vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/trace.h|314| <<TRACE_EVENT_KVM_EXIT>> static_call(kvm_x86_get_exit_info)(vcpu, \
+ *   - arch/x86/kvm/x86.c|8790| <<prepare_emulation_failure_exit>> static_call(kvm_x86_get_exit_info)(vcpu, (u32 *)&info[0], &info[1],
+ *
+ * vmx_get_exit_info()
+ */
 static void svm_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,
 			      u64 *info1, u64 *info2,
 			      u32 *intr_info, u32 *error_code)
diff --git a/arch/x86/kvm/vmx/capabilities.h b/arch/x86/kvm/vmx/capabilities.h
index 41a4533f9..9b7f0c4b4 100644
--- a/arch/x86/kvm/vmx/capabilities.h
+++ b/arch/x86/kvm/vmx/capabilities.h
@@ -47,6 +47,13 @@ struct nested_vmx_msrs {
 	u64 basic;
 	u64 cr0_fixed0;
 	u64 cr0_fixed1;
+	/*
+	 * 在以下使用nested_vmx_msrs->cr4_fixed:
+	 *   - arch/x86/kvm/vmx/nested.c|1518| <<vmx_get_fixed0_msr>> return &msrs->cr4_fixed0;
+	 *   - arch/x86/kvm/vmx/nested.c|1659| <<vmx_get_vmx_msr>> *pdata = msrs->cr4_fixed0;
+	 *   - arch/x86/kvm/vmx/nested.c|7360| <<nested_vmx_setup_cr_fixed>> msrs->cr4_fixed0 = VMXON_CR4_ALWAYSON;
+	 *   - arch/x86/kvm/vmx/nested.h|319| <<nested_cr4_valid>> u64 fixed0 = to_vmx(vcpu)->nested.msrs.cr4_fixed0;
+	 */
 	u64 cr4_fixed0;
 	u64 cr4_fixed1;
 	u64 vmcs_enum;
@@ -96,6 +103,16 @@ static inline bool cpu_has_vmx_posted_intr(void)
 	return vmcs_config.pin_based_exec_ctrl & PIN_BASED_POSTED_INTR;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2539| <<prepare_vmcs02_early>> if (cpu_has_load_ia32_efer()) {
+ *   - arch/x86/kvm/vmx/nested.c|2555| <<prepare_vmcs02_early>> if (cpu_has_load_ia32_efer() && guest_efer != host_efer)
+ *   - arch/x86/kvm/vmx/nested.c|4890| <<nested_vmx_get_vmcs01_guest_efer>> if (cpu_has_load_ia32_efer())
+ *   - arch/x86/kvm/vmx/vmx.c|990| <<clear_atomic_switch_msr>> if (cpu_has_load_ia32_efer()) {
+ *   - arch/x86/kvm/vmx/vmx.c|1043| <<add_atomic_switch_msr>> if (cpu_has_load_ia32_efer()) {
+ *   - arch/x86/kvm/vmx/vmx.c|1127| <<update_transition_efer>> if (cpu_has_load_ia32_efer() ||
+ *   - arch/x86/kvm/vmx/vmx.c|4380| <<vmx_set_constant_host_state>> if (cpu_has_load_ia32_efer())
+ */
 static inline bool cpu_has_load_ia32_efer(void)
 {
 	return vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_EFER;
@@ -222,6 +239,13 @@ static inline bool cpu_has_vmx_vmfunc(void)
 		SECONDARY_EXEC_ENABLE_VMFUNC;
 }
 
+/*
+ * 注释:
+ * VMCS Shadowing (Intel)
+ * Processor optimization that lets L0 define a shadow VMCS
+ * Guest hypervisor can access shadow VMCS directly (in hardware)
+ * Consequently, reducing the number of VM entries & exits
+ */
 static inline bool cpu_has_vmx_shadow_vmcs(void)
 {
 	/* check if the cpu supports writing r/o exit information fields */
diff --git a/arch/x86/kvm/vmx/hyperv.c b/arch/x86/kvm/vmx/hyperv.c
index 313b8bb5b..bfebfb894 100644
--- a/arch/x86/kvm/vmx/hyperv.c
+++ b/arch/x86/kvm/vmx/hyperv.c
@@ -439,6 +439,9 @@ u64 nested_get_evmptr(struct kvm_vcpu *vcpu)
 	return hv_vcpu->vp_assist_page.current_nested_vmcs;
 }
 
+/*
+ * struct kvm_x86_nested_ops vmx_nested_ops.get_evmcs_version = nested_get_evmcs_version()
+ */
 uint16_t nested_get_evmcs_version(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -642,6 +645,9 @@ void evmcs_sanitize_exec_ctrls(struct vmcs_config *vmcs_conf)
 }
 #endif
 
+/*
+ * struct kvm_x86_nested_ops vmx_nested_ops.enable_evmcs = nested_enable_evmcs()
+ */
 int nested_enable_evmcs(struct kvm_vcpu *vcpu,
 			uint16_t *vmcs_version)
 {
@@ -670,6 +676,9 @@ bool nested_evmcs_l2_tlb_flush_enabled(struct kvm_vcpu *vcpu)
 	return hv_vcpu->vp_assist_page.nested_control.features.directhypercall;
 }
 
+/*
+ * struct kvm_x86_nested_ops vmx_nested_ops.hv_inject_synthetic_vmexit_post_tlb_flush = vmx_hv_inject_synthetic_vmexit_post_tlb_flush
+ */
 void vmx_hv_inject_synthetic_vmexit_post_tlb_flush(struct kvm_vcpu *vcpu)
 {
 	nested_vmx_vmexit(vcpu, HV_VMX_SYNTHETIC_EXIT_REASON_TRAP_AFTER_FLUSH, 0, 0);
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index c5ec0ef51..b30a68ffc 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -18,6 +18,10 @@
 #include "x86.h"
 #include "smm.h"
 
+/*
+ * https://royhunter.github.io/2014/06/20/NESTED-EPT/
+ */
+
 static bool __read_mostly enable_shadow_vmcs = 1;
 module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
 
@@ -66,6 +70,10 @@ static struct shadow_vmcs_field shadow_read_write_fields[] = {
 static int max_shadow_read_write_fields =
 	ARRAY_SIZE(shadow_read_write_fields);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|7370| <<init_vmcs_shadow_fields>> init_vmcs_shadow_fields();
+ */
 static void init_vmcs_shadow_fields(void)
 {
 	int i, j;
@@ -185,6 +193,35 @@ static int nested_vmx_failValid(struct kvm_vcpu *vcpu,
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3813| <<nested_vmx_run>> return nested_vmx_fail(vcpu, VMXERR_ENTRY_EVENTS_BLOCKED_BY_MOV_SS);
+  3 arch/x86/kvm/vmx/nested.c|3816| <<nested_vmx_run>> return nested_vmx_fail(vcpu,
+  4 arch/x86/kvm/vmx/nested.c|3821| <<nested_vmx_run>> return nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);
+  5 arch/x86/kvm/vmx/nested.c|3824| <<nested_vmx_run>> return nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_HOST_STATE_FIELD);
+  6 arch/x86/kvm/vmx/nested.c|3827| <<nested_vmx_run>> return nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_HOST_STATE_FIELD);
+  7 arch/x86/kvm/vmx/nested.c|3894| <<nested_vmx_run>> return nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);
+  8 arch/x86/kvm/vmx/nested.c|5136| <<nested_vmx_vmexit>> (void )nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);
+  9 arch/x86/kvm/vmx/nested.c|5472| <<handle_vmxon>> return nested_vmx_fail(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);
+ 10 arch/x86/kvm/vmx/nested.c|5573| <<handle_vmclear>> return nested_vmx_fail(vcpu, VMXERR_VMCLEAR_INVALID_ADDRESS);
+ *   - arch/x86/kvm/vmx/nested.c|5576| <<handle_vmclear>> return nested_vmx_fail(vcpu, VMXERR_VMCLEAR_VMXON_POINTER);
+ 12 arch/x86/kvm/vmx/nested.c|5658| <<handle_vmread>> return nested_vmx_fail(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);
+ 13 arch/x86/kvm/vmx/nested.c|5681| <<handle_vmread>> return nested_vmx_fail(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);
+ 14 arch/x86/kvm/vmx/nested.c|5782| <<handle_vmwrite>> return nested_vmx_fail(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);
+ 15 arch/x86/kvm/vmx/nested.c|5790| <<handle_vmwrite>> return nested_vmx_fail(vcpu, VMXERR_VMWRITE_READ_ONLY_VMCS_COMPONENT);
+ 16 arch/x86/kvm/vmx/nested.c|5866| <<handle_vmptrld>> return nested_vmx_fail(vcpu, VMXERR_VMPTRLD_INVALID_ADDRESS);
+ 17 arch/x86/kvm/vmx/nested.c|5869| <<handle_vmptrld>> return nested_vmx_fail(vcpu, VMXERR_VMPTRLD_VMXON_POINTER);
+ 18 arch/x86/kvm/vmx/nested.c|5886| <<handle_vmptrld>> return nested_vmx_fail(vcpu,
+ 19 arch/x86/kvm/vmx/nested.c|5893| <<handle_vmptrld>> return nested_vmx_fail(vcpu,
+ *  - arch/x86/kvm/vmx/nested.c|5900| <<handle_vmptrld>> return nested_vmx_fail(vcpu,
+ 21 arch/x86/kvm/vmx/nested.c|5912| <<handle_vmptrld>> return nested_vmx_fail(vcpu,
+ 22 arch/x86/kvm/vmx/nested.c|5981| <<handle_invept>> return nested_vmx_fail(vcpu, VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
+ 23 arch/x86/kvm/vmx/nested.c|6002| <<handle_invept>> return nested_vmx_fail(vcpu,
+ 24 arch/x86/kvm/vmx/nested.c|6063| <<handle_invvpid>> return nested_vmx_fail(vcpu,
+ 25 arch/x86/kvm/vmx/nested.c|6077| <<handle_invvpid>> return nested_vmx_fail(vcpu,
+ 26 arch/x86/kvm/vmx/nested.c|6085| <<handle_invvpid>> return nested_vmx_fail(vcpu,
+ 27 arch/x86/kvm/vmx/nested.c|6092| <<handle_invvpid>> return nested_vmx_fail(vcpu,
+ */
 static int nested_vmx_fail(struct kvm_vcpu *vcpu, u32 vm_instruction_error)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -200,6 +237,13 @@ static int nested_vmx_fail(struct kvm_vcpu *vcpu, u32 vm_instruction_error)
 	return nested_vmx_failValid(vcpu, vm_instruction_error);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4668| <<prepare_vmcs12>> nested_vmx_abort(vcpu, VMX_ABORT_SAVE_GUEST_MSR_FAIL);
+ *   - arch/x86/kvm/vmx/nested.c|4727| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+ *   - arch/x86/kvm/vmx/nested.c|4807| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+ *   - arch/x86/kvm/vmx/nested.c|4941| <<nested_vmx_restore_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+ */
 static void nested_vmx_abort(struct kvm_vcpu *vcpu, u32 indicator)
 {
 	/* TODO: not to reset guest simply here. */
@@ -262,8 +306,29 @@ static void vmx_sync_vmcs_host_state(struct vcpu_vmx *vmx,
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|343| <<free_nested>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3670| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+ *   - arch/x86/kvm/vmx/nested.c|3685| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3690| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3777| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|5136| <<nested_vmx_vmexit>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ */
 static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
 {
+	/*
+	 * struct vcpu_vmx *vmx:
+	 *     //loaded_vmcs points to the VMCS currently used in this vcpu. For a
+	 *     non-nested (L1) guest, it always points to vmcs01. For a nested
+	 *     guest (L2), it points to a different VMCS.
+	 *
+	 *     struct loaded_vmcs    vmcs01;
+	 *     struct loaded_vmcs   *loaded_vmcs;
+	 *
+	 *     struct nested_vmx nested;
+	 *         struct loaded_vmcs vmcs02;
+	 */
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	struct loaded_vmcs *prev;
 	int cpu;
@@ -375,6 +440,10 @@ static void nested_ept_invalidate_addr(struct kvm_vcpu *vcpu, gpa_t eptp,
 		kvm_mmu_invalidate_addr(vcpu, vcpu->arch.mmu, addr, roots);
 }
 
+/*
+ * 在以下使用nested_ept_inject_page_fault():
+ *   - arch/x86/kvm/vmx/nested.c|545| <<nested_ept_init_mmu_context>> vcpu->arch.mmu->inject_page_fault = nested_ept_inject_page_fault;
+ */
 static void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,
 		struct x86_exception *fault)
 {
@@ -408,30 +477,155 @@ static void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,
 	vmcs12->guest_physical_address = fault->address;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|427| <<nested_ept_init_mmu_context>> nested_ept_new_eptp(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5896| <<nested_vmx_eptp_switching>> nested_ept_new_eptp(vcpu);
+ */
 static void nested_ept_new_eptp(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	bool execonly = vmx->nested.msrs.ept_caps & VMX_EPT_EXECUTE_ONLY_BIT;
 	int ept_lpage_level = ept_caps_to_lpage_level(vmx->nested.msrs.ept_caps);
 
+	/*
+	 * 只在此处调用
+	 */
 	kvm_init_shadow_ept_mmu(vcpu, execonly, ept_lpage_level,
 				nested_ept_ad_enabled(vcpu),
 				nested_ept_get_eptp(vcpu));
 }
 
+/*
+ * 在以下设置kvm_vcpu_arch->mmu: --> 指针
+ *   - arch/x86/kvm/mmu/mmu.c|8010| <<kvm_mmu_create>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/svm/nested.c|86| <<nested_svm_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+ *   - arch/x86/kvm/svm/nested.c|104| <<nested_svm_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/vmx/nested.c|466| <<nested_ept_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+ *   - arch/x86/kvm/vmx/nested.c|477| <<nested_ept_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+ *
+ * 在以下使用kvm_vcpu_arch->root_mmu:
+ *   - arch/x86/kvm/mmu/mmu.c|7027| <<init_kvm_tdp_mmu>> struct kvm_mmu *context = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/mmu/mmu.c|7087| <<kvm_init_shadow_mmu>> struct kvm_mmu *context = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/mmu/mmu.c|7208| <<init_kvm_softmmu>> struct kvm_mmu *context = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/mmu/mmu.c|7296| <<kvm_mmu_after_set_cpuid>> vcpu->arch.root_mmu.root_role.word = 0;
+ *   - arch/x86/kvm/mmu/mmu.c|7299| <<kvm_mmu_after_set_cpuid>> vcpu->arch.root_mmu.cpu_role.ext.valid = 0;
+ *   - arch/x86/kvm/mmu/mmu.c|7379| <<kvm_mmu_unload>> kvm_mmu_free_roots(kvm, &vcpu->arch.root_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|7380| <<kvm_mmu_unload>> WARN_ON_ONCE(VALID_PAGE(vcpu->arch.root_mmu.root.hpa));
+ *   - arch/x86/kvm/mmu/mmu.c|7432| <<kvm_mmu_free_obsolete_roots>> __kvm_mmu_free_obsolete_roots(vcpu->kvm, &vcpu->arch.root_mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|8010| <<kvm_mmu_create>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/mmu/mmu.c|8011| <<kvm_mmu_create>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/mmu/mmu.c|8017| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.root_mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|9014| <<kvm_mmu_destroy>> free_mmu_pages(&vcpu->arch.root_mmu);
+ *   - arch/x86/kvm/svm/nested.c|104| <<nested_svm_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/svm/nested.c|105| <<nested_svm_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/vmx/nested.c|473| <<nested_ept_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/vmx/nested.c|474| <<nested_ept_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/vmx/nested.c|5903| <<handle_invvpid>> kvm_mmu_free_guest_mode_roots(vcpu->kvm, &vcpu->arch.root_mmu);
+ *
+ * 在以下使用kvm_vcpu_arch->guest_mmu:
+ *   - arch/x86/kvm/mmu.h|207| <<kvm_mmu_refresh_passthrough_bits>> if (!tdp_enabled || mmu == &vcpu->arch.guest_mmu)
+ *   - arch/x86/kvm/mmu/mmu.c|7116| <<kvm_init_shadow_npt_mmu>> struct kvm_mmu *context = &vcpu->arch.guest_mmu;
+ *   - arch/x86/kvm/mmu/mmu.c|7176| <<kvm_init_shadow_ept_mmu>> struct kvm_mmu *context = &vcpu->arch.guest_mmu;
+ *   - arch/x86/kvm/mmu/mmu.c|7297| <<kvm_mmu_after_set_cpuid>> vcpu->arch.guest_mmu.root_role.word = 0;
+ *   - arch/x86/kvm/mmu/mmu.c|7300| <<kvm_mmu_after_set_cpuid>> vcpu->arch.guest_mmu.cpu_role.ext.valid = 0;
+ *   - arch/x86/kvm/mmu/mmu.c|7381| <<kvm_mmu_unload>> kvm_mmu_free_roots(kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|7382| <<kvm_mmu_unload>> WARN_ON_ONCE(VALID_PAGE(vcpu->arch.guest_mmu.root.hpa));
+ *   - arch/x86/kvm/mmu/mmu.c|7433| <<kvm_mmu_free_obsolete_roots>> __kvm_mmu_free_obsolete_roots(vcpu->kvm, &vcpu->arch.guest_mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|7777| <<kvm_mmu_invalidate_addr>> if (mmu != &vcpu->arch.guest_mmu) {
+ *   - arch/x86/kvm/mmu/mmu.c|7952| <<__kvm_mmu_create>> if (!tdp_enabled && mmu == &vcpu->arch.guest_mmu)
+ *   - arch/x86/kvm/mmu/mmu.c|8013| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|8023| <<kvm_mmu_create>> free_mmu_pages(&vcpu->arch.guest_mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|9015| <<kvm_mmu_destroy>> free_mmu_pages(&vcpu->arch.guest_mmu);
+ *   - arch/x86/kvm/svm/nested.c|86| <<nested_svm_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+ *   - arch/x86/kvm/vmx/nested.c|332| <<free_nested>> kvm_mmu_free_roots(vcpu->kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/vmx/nested.c|462| <<nested_ept_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+ *   - arch/x86/kvm/vmx/nested.c|5327| <<nested_release_vmcs12>> kvm_mmu_free_roots(vcpu->kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);
+ *   - arch/x86/kvm/vmx/nested.c|5785| <<handle_invept>> mmu = &vcpu->arch.guest_mmu;
+ *
+ * 在以下使用kvm_vcpu_arch->nested_mmu:
+ *   - arch/x86/kvm/mmu.h|359| <<kvm_translate_gpa>> if (mmu != &vcpu->arch.nested_mmu)
+ *   - arch/x86/kvm/mmu/mmu.c|7224| <<init_kvm_nested_mmu>> struct kvm_mmu *g_context = &vcpu->arch.nested_mmu;
+ *   - arch/x86/kvm/mmu/mmu.c|7298| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.root_role.word = 0;
+ *   - arch/x86/kvm/mmu/mmu.c|7301| <<kvm_mmu_after_set_cpuid>> vcpu->arch.nested_mmu.cpu_role.ext.valid = 0;
+ *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+ *   - arch/x86/kvm/vmx/nested.c|468| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+ *   - arch/x86/kvm/x86.h|205| <<mmu_is_nested>> return vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;
+ *
+ * 在以下设置kvm_vcpu_arch->walk_mmu: ---> 指针
+ *   - arch/x86/kvm/mmu/mmu.c|8011| <<kvm_mmu_create>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+ *   - arch/x86/kvm/svm/nested.c|105| <<nested_svm_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+ *   - arch/x86/kvm/vmx/nested.c|468| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+ *   - arch/x86/kvm/vmx/nested.c|474| <<nested_ept_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+ */
+/*
+ * 注意下init_kvm_nested_mmu()
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2594| <<prepare_vmcs02>> nested_ept_init_mmu_context(vcpu);
+ *
+ * nested_vmx_enter_non_root_mode()
+ * -> prepare_vmcs02()
+ *    -> nested_ept_init_mmu_context()
+ */
 static void nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
 {
 	WARN_ON(mmu_is_nested(vcpu));
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_mmu *mmu;
+	 *    // Non-nested MMU for L1
+	 *    -> struct kvm_mmu root_mmu;
+	 *    // L1 MMU when running nested
+	 *    -> struct kvm_mmu guest_mmu;
+	 *    //
+	 *    // Paging state of an L2 guest (used for nested npt)
+	 *    //
+	 *    // This context will save all necessary information to walk page tables
+	 *    // of an L2 guest. This context is only initialized for page table
+	 *    // walking and not for faulting since we never handle l2 page faults on
+	 *    // the host.
+	 *    //
+	 *    struct kvm_mmu nested_mmu;
+	 *
+	 *    //
+	 *    // Pointer to the mmu context currently used for
+	 *    // gva_to_gpa translations.
+	 *    //
+	 *    struct kvm_mmu *walk_mmu;
+	 *
+	 * 在以下设置kvm_vcpu_arch->mmu: --> 指针
+	 *   - arch/x86/kvm/mmu/mmu.c|8010| <<kvm_mmu_create>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|86| <<nested_svm_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|104| <<nested_svm_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|466| <<nested_ept_init_mmu_context>> vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|477| <<nested_ept_uninit_mmu_context>> vcpu->arch.mmu = &vcpu->arch.root_mmu;
+	 */
 	vcpu->arch.mmu = &vcpu->arch.guest_mmu;
 	nested_ept_new_eptp(vcpu);
 	vcpu->arch.mmu->get_guest_pgd     = nested_ept_get_eptp;
 	vcpu->arch.mmu->inject_page_fault = nested_ept_inject_page_fault;
 	vcpu->arch.mmu->get_pdptr         = kvm_pdptr_read;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->walk_mmu:
+	 *   - arch/x86/kvm/mmu/mmu.c|8011| <<kvm_mmu_create>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|99| <<nested_svm_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/svm/nested.c|105| <<nested_svm_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|468| <<nested_ept_init_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;
+	 *   - arch/x86/kvm/vmx/nested.c|474| <<nested_ept_uninit_mmu_context>> vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
+	 */
 	vcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4644| <<load_vmcs12_host_state>> nested_ept_uninit_mmu_context(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4794| <<nested_vmx_restore_host_state>> nested_ept_uninit_mmu_context(vcpu);
+ */
 static void nested_ept_uninit_mmu_context(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.mmu = &vcpu->arch.root_mmu;
@@ -450,6 +644,9 @@ static bool nested_vmx_is_page_fault_vmexit(struct vmcs12 *vmcs12,
 	return inequality ^ bit;
 }
 
+/*
+ * struct kvm_x86_nested_ops vmx_nested_ops.is_exception_vmexit = nested_vmx_is_exception_vmexit()
+ */
 static bool nested_vmx_is_exception_vmexit(struct kvm_vcpu *vcpu, u8 vector,
 					   u32 error_code)
 {
@@ -2530,6 +2727,12 @@ static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
  * Returns 0 on success, 1 on failure. Invalid state exit qualification code
  * is assigned to entry_failure_code on failure.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3511| <<nested_vmx_enter_non_root_mode>> if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &entry_failure_code)) {
+ *
+ * 通过VMCS1->2中的信息在L0中构建了VMCS0->2所需的信息
+ */
 static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			  bool from_vmentry,
 			  enum vm_entry_failure_code *entry_failure_code)
@@ -2880,6 +3083,11 @@ static int nested_check_vm_entry_controls(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3887| <<nested_vmx_run>> if (nested_vmx_check_controls(vcpu, vmcs12))
+ *   - arch/x86/kvm/vmx/nested.c|7078| <<vmx_set_nested_state>> if (nested_vmx_check_controls(vcpu, vmcs12) ||
+ */
 static int nested_vmx_check_controls(struct kvm_vcpu *vcpu,
 				     struct vmcs12 *vmcs12)
 {
@@ -2905,6 +3113,11 @@ static int nested_vmx_check_address_space_size(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3893| <<nested_vmx_run>> if (nested_vmx_check_host_state(vcpu, vmcs12))
+ *   - arch/x86/kvm/vmx/nested.c|7079| <<vmx_set_nested_state>> nested_vmx_check_host_state(vcpu, vmcs12) ||
+ */
 static int nested_vmx_check_host_state(struct kvm_vcpu *vcpu,
 				       struct vmcs12 *vmcs12)
 {
@@ -3017,6 +3230,11 @@ static int nested_check_guest_non_reg_state(struct vmcs12 *vmcs12)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3677| <<nested_vmx_enter_non_root_mode>> if (nested_vmx_check_guest_state(vcpu, vmcs12, &entry_failure_code)) {
+ *   - arch/x86/kvm/vmx/nested.c|7050| <<vmx_set_nested_state>> nested_vmx_check_guest_state(vcpu, vmcs12, &ignored))
+ */
 static int nested_vmx_check_guest_state(struct kvm_vcpu *vcpu,
 					struct vmcs12 *vmcs12,
 					enum vm_entry_failure_code *entry_failure_code)
@@ -3277,6 +3495,9 @@ static bool nested_get_vmcs12_pages(struct kvm_vcpu *vcpu)
 	return true;
 }
 
+/*
+ * struct kvm_x86_nested_ops vmx_nested_ops.get_nested_state_pages = vmx_get_nested_state_pages()
+ */
 static bool vmx_get_nested_state_pages(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -3302,6 +3523,9 @@ static bool vmx_get_nested_state_pages(struct kvm_vcpu *vcpu)
 	return true;
 }
 
+/*
+ * struct kvm_x86_nested_ops vmx_nested_ops.write_log_dirty = nested_vmx_write_pml_buffer()
+ */
 static int nested_vmx_write_pml_buffer(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	struct vmcs12 *vmcs12;
@@ -3346,6 +3570,18 @@ static int nested_vmx_write_pml_buffer(struct kvm_vcpu *vcpu, gpa_t gpa)
  * Note that many of these exceptions have priority over VM exits, so they
  * don't have to be checked again here.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3732| <<nested_vmx_run>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5530| <<handle_vmxoff>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5549| <<handle_vmclear>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5623| <<handle_vmread>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5737| <<handle_vmwrite>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5842| <<handle_vmptrld>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5915| <<handle_vmptrst>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5954| <<handle_invept>> if (!nested_vmx_check_permission(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|6035| <<handle_invvpid>> if (!nested_vmx_check_permission(vcpu))
+ */
 static int nested_vmx_check_permission(struct kvm_vcpu *vcpu)
 {
 	if (!to_vmx(vcpu)->nested.vmxon) {
@@ -3382,13 +3618,42 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
  *	NVMX_VMENTRY_VMEXIT:  Consistency check VMExit
  *	NVMX_VMENTRY_KVM_INTERNAL_ERROR: KVM internal error
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3628| <<nested_vmx_run>> status = nested_vmx_enter_non_root_mode(vcpu, true);
+ *   - arch/x86/kvm/vmx/nested.c|6711| <<vmx_set_nested_state>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+ *   - arch/x86/kvm/vmx/vmx.c|8342| <<vmx_leave_smm>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+ */
 enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 							bool from_vmentry)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	/*
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;   <---- 返回的 
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 *    -> struct gfn_to_hva_cache shadow_vmcs12_cache;
+	 *    -> struct gfn_to_hva_cache vmcs12_cache;
+	 *
+	 * to_vmx(vcpu)->nested.cached_vmcs12;
+	 */
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 	enum vm_entry_failure_code entry_failure_code;
 	bool evaluate_pending_interrupts;
+	/*
+	 * 在以下使用EXIT_REASON_INVALID_STATE:
+	 *   - arch/x86/include/uapi/asm/vmx.h|62| <<global>> #define EXIT_REASON_INVALID_STATE 33
+	 *   - tools/arch/x86/include/uapi/asm/vmx.h|62| <<global>> #define EXIT_REASON_INVALID_STATE 33
+	 *   - arch/x86/include/uapi/asm/vmx.h|126| <<VMX_EXIT_REASONS>> { EXIT_REASON_INVALID_STATE, "INVALID_STATE" }, \
+	 *   - arch/x86/kvm/vmx/nested.c|3679| <<nested_vmx_enter_non_root_mode>> exit_reason.basic = EXIT_REASON_INVALID_STATE;
+	 *   - arch/x86/kvm/vmx/nested.c|3691| <<nested_vmx_enter_non_root_mode>> exit_reason.basic = EXIT_REASON_INVALID_STATE;
+	 *   - arch/x86/kvm/vmx/nested.c|6632| <<nested_vmx_l1_wants_exit>> case EXIT_REASON_INVALID_STATE:
+	 *   - arch/x86/kvm/vmx/vmx.c|7511| <<vmx_vcpu_run>> vmx->exit_reason.full = EXIT_REASON_INVALID_STATE;
+	 *   - tools/arch/x86/include/uapi/asm/vmx.h|126| <<VMX_EXIT_REASONS>> { EXIT_REASON_INVALID_STATE, "INVALID_STATE" }, \
+	 *   - tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c|102| <<l1_guest_code>> (EXIT_REASON_FAILED_VMENTRY | EXIT_REASON_INVALID_STATE));
+	 */
 	union vmx_exit_reason exit_reason = {
 		.basic = EXIT_REASON_INVALID_STATE,
 		.failed_vmentry = 1,
@@ -3443,6 +3708,15 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 
 	vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
 
+	/*
+	 * struct vcpu_vmx:
+	 * //
+	 * //loaded_vmcs points to the VMCS currently used in this vcpu. For a
+	 * // non-nested (L1) guest, it always points to vmcs01. For a nested
+	 * //guest (L2), it points to a different VMCS.
+	 * -> struct loaded_vmcs    vmcs01;
+	 * -> struct loaded_vmcs   *loaded_vmcs;
+	 */
 	prepare_vmcs02_early(vmx, &vmx->vmcs01, vmcs12);
 
 	if (from_vmentry) {
@@ -3456,6 +3730,11 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 			return NVMX_VMENTRY_VMFAIL;
 		}
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/vmx/nested.c|3677| <<nested_vmx_enter_non_root_mode>> if (nested_vmx_check_guest_state(vcpu, vmcs12, &entry_failure_code)) {
+		 *   - arch/x86/kvm/vmx/nested.c|7050| <<vmx_set_nested_state>> nested_vmx_check_guest_state(vcpu, vmcs12, &ignored))
+		 */
 		if (nested_vmx_check_guest_state(vcpu, vmcs12,
 						 &entry_failure_code)) {
 			exit_reason.basic = EXIT_REASON_INVALID_STATE;
@@ -3464,8 +3743,16 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 		}
 	}
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/svm/nested.c|777| <<nested_vmcb02_prepare_control>> enter_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3707| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+	 */
 	enter_guest_mode(vcpu);
 
+	/*
+	 * 只在此处调用
+	 */
 	if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &entry_failure_code)) {
 		exit_reason.basic = EXIT_REASON_INVALID_STATE;
 		vmcs12->exit_qualification = entry_failure_code;
@@ -3536,6 +3823,11 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 	if (!from_vmentry)
 		return NVMX_VMENTRY_VMEXIT;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/vmx/nested.c|3720| <<nested_vmx_enter_non_root_mode>> load_vmcs12_host_state(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|5119| <<nested_vmx_vmexit>> load_vmcs12_host_state(vcpu, vmcs12);
+	 */
 	load_vmcs12_host_state(vcpu, vmcs12);
 	vmcs12->vm_exit_reason = exit_reason.full;
 	if (enable_shadow_vmcs || evmptr_is_valid(vmx->nested.hv_evmcs_vmptr))
@@ -3547,6 +3839,11 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
  * nested_vmx_run() handles a nested entry, i.e., a VMLAUNCH or VMRESUME on L1
  * for running an L2 nested guest.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5508| <<handle_vmlaunch>> return nested_vmx_run(vcpu, true);
+ *   - arch/x86/kvm/vmx/nested.c|5515| <<handle_vmresume>> return nested_vmx_run(vcpu, false);
+ */
 static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 {
 	struct vmcs12 *vmcs12;
@@ -3573,6 +3870,27 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 	       vmx->nested.current_vmptr == INVALID_GPA))
 		return nested_vmx_failInvalid(vcpu);
 
+	/*
+	 * 在以下使用nested_vmx->cached_vmcs12:
+	 *   - arch/x86/kvm/vmx/nested.c|318| <<free_nested>> kfree(vmx->nested.cached_vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|319| <<free_nested>> vmx->nested.cached_vmcs12 = NULL;
+	 *   - arch/x86/kvm/vmx/nested.c|1699| <<copy_enlightened_to_vmcs12>> struct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;
+	 *   - arch/x86/kvm/vmx/nested.c|1943| <<copy_vmcs12_to_enlightened>> struct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;
+	 *   - arch/x86/kvm/vmx/nested.c|5258| <<enter_vmx_operation>> vmx->nested.cached_vmcs12 = kzalloc(VMCS12_SIZE, GFP_KERNEL_ACCOUNT);
+	 *   - arch/x86/kvm/vmx/nested.c|5259| <<enter_vmx_operation>> if (!vmx->nested.cached_vmcs12)
+	 *   - arch/x86/kvm/vmx/nested.c|5290| <<enter_vmx_operation>> kfree(vmx->nested.cached_vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|5411| <<nested_release_vmcs12>> kvm_vcpu_write_guest_page(vcpu, vmx->nested.current_vmptr >> PAGE_SHIFT,vmx->nested.cached_vmcs12, 0, VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|5784| <<handle_vmptrld>> if (kvm_read_guest_cached(vcpu->kvm, ghc, vmx->nested.cached_vmcs12, VMCS12_SIZE)) {
+	 *   - arch/x86/kvm/vmx/nested.h|41| <<get_vmcs12>> return to_vmx(vcpu)->nested.cached_vmcs12;
+	 *
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;   <---- 返回的
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 *    -> struct gfn_to_hva_cache shadow_vmcs12_cache;
+	 *    -> struct gfn_to_hva_cache vmcs12_cache;
+	 */
 	vmcs12 = get_vmcs12(vcpu);
 
 	/*
@@ -3623,8 +3941,19 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 	 * We're finally done with prerequisite checking, and can start with
 	 * the nested entry.
 	 */
+	/*
+	 * 非常重要的tag
+	 */
 	vmx->nested.nested_run_pending = 1;
 	vmx->nested.has_preemption_timer_deadline = false;
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/vmx/nested.c|3628| <<nested_vmx_run>> status = nested_vmx_enter_non_root_mode(vcpu, true);
+	 *   - arch/x86/kvm/vmx/nested.c|6711| <<vmx_set_nested_state>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+	 *   - arch/x86/kvm/vmx/vmx.c|8342| <<vmx_leave_smm>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+	 *
+	 * 主要的函数
+	 */
 	status = nested_vmx_enter_non_root_mode(vcpu, true);
 	if (unlikely(status != NVMX_VMENTRY_SUCCESS))
 		goto vmentry_failed;
@@ -3652,6 +3981,13 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 	 */
 	nested_cache_shadow_vmcs12(vcpu, vmcs12);
 
+	/*
+	 * // GUEST_ACTIVITY_STATE flags
+	 * #define GUEST_ACTIVITY_ACTIVE           0
+	 * #define GUEST_ACTIVITY_HLT              1
+	 * #define GUEST_ACTIVITY_SHUTDOWN         2
+	 * #define GUEST_ACTIVITY_WAIT_SIPI        3
+	 */
 	switch (vmcs12->guest_activity_state) {
 	case GUEST_ACTIVITY_HLT:
 		/*
@@ -3681,6 +4017,9 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 	vmx->nested.nested_run_pending = 0;
 	if (status == NVMX_VMENTRY_KVM_INTERNAL_ERROR)
 		return 0;
+	/*
+	 * 这里应该是那种正常的退出, 用vmexit来返回error!
+	 */
 	if (status == NVMX_VMENTRY_VMEXIT)
 		return 1;
 	WARN_ON_ONCE(status != NVMX_VMENTRY_VMFAIL);
@@ -3962,6 +4301,9 @@ static bool nested_vmx_preemption_timer_pending(struct kvm_vcpu *vcpu)
 	       to_vmx(vcpu)->nested.preemption_timer_expired;
 }
 
+/*
+ * struct kvm_x86_nested_ops vmx_nested_ops.has_events = vmx_has_nested_events()
+ */
 static bool vmx_has_nested_events(struct kvm_vcpu *vcpu)
 {
 	return nested_vmx_preemption_timer_pending(vcpu) ||
@@ -4051,6 +4393,9 @@ static bool vmx_has_nested_events(struct kvm_vcpu *vcpu)
  *     delivery of a virtual interrupt; delivery of a virtual interrupt takes
  *     priority over external interrupts and lower priority events.
  */
+ /*
+  * struct kvm_x86_nested_ops vmx_nested_ops.check_events = vmx_check_nested_events()
+  */
 static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -4406,6 +4751,10 @@ static void sync_vmcs02_to_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
  * exit-information fields only. Other fields are modified by L1 with VMWRITE,
  * which already writes to vmcs12 directly.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4990| <<nested_vmx_vmexit>> prepare_vmcs12(vcpu, vmcs12, vm_exit_reason, exit_intr_info, exit_qualification);
+ */
 static void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			   u32 vm_exit_reason, u32 exit_intr_info,
 			   unsigned long exit_qualification)
@@ -4462,6 +4811,11 @@ static void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
  * Failures During or After Loading Guest State").
  * This function should be called when the active VMCS is L1's (vmcs01).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3720| <<nested_vmx_enter_non_root_mode>> load_vmcs12_host_state(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|5119| <<nested_vmx_vmexit>> load_vmcs12_host_state(vcpu, vmcs12);
+ */
 static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
 				   struct vmcs12 *vmcs12)
 {
@@ -4610,6 +4964,10 @@ static inline u64 nested_vmx_get_vmcs01_guest_efer(struct vcpu_vmx *vmx)
 	return host_efer;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5064| <<nested_vmx_vmexit>> nested_vmx_restore_host_state(vcpu);
+ */
 static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -4715,6 +5073,24 @@ static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
 	nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/hyperv.c|675| <<vmx_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_vmx_vmexit(vcpu, HV_VMX_SYNTHETIC_EXIT_REASON_TRAP_AFTER_FLUSH, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|407| <<nested_ept_inject_page_fault>> nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification);
+ *   - arch/x86/kvm/vmx/nested.c|3904| <<nested_vmx_inject_exception_vmexit>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI, intr_info, exit_qual);
+ *   - arch/x86/kvm/vmx/nested.c|4080| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_INIT_SIGNAL, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4094| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_SIPI_SIGNAL, 0, apic->sipi_vector & 0xFFUL);
+ *   - arch/x86/kvm/vmx/nested.c|4131| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_MONITOR_TRAP_FLAG, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4152| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4168| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI, NMI_VECTOR | INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4185| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4911| <<nested_vmx_triple_fault>> nested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|5918| <<handle_vmfunc>> nested_vmx_vmexit(vcpu, vmx->exit_reason.full, vmx_get_intr_info(vcpu), vmx_get_exit_qual(vcpu));
+ *   - arch/x86/kvm/vmx/nested.c|6404| <<nested_vmx_reflect_vmexit>> nested_vmx_vmexit(vcpu, exit_reason.full, exit_intr_info, exit_qual);
+ *   - arch/x86/kvm/vmx/nested.c|6535| <<vmx_leave_nested>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|6631| <<__vmx_handle_exit>> nested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|8323| <<vmx_enter_smm>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ */
 /*
  * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
  * and modify vmcs12 to make it see what it would expect to see there if
@@ -4782,6 +5158,21 @@ void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 		 */
 		nested_flush_cached_shadow_vmcs12(vcpu, vmcs12);
 	} else {
+		/*
+		 * 在以下使用VM_INSTRUCTION_ERROR:
+		 *   - arch/x86/include/asm/vmx.h|292| <<global>> VM_INSTRUCTION_ERROR = 0x00004400,
+		 *   - arch/x86/kvm/vmx/hyperv.c|362| <<global>> EVMCS1_FIELD(VM_INSTRUCTION_ERROR, vm_instruction_error, HV_VMX_ENLIGHTENED_CLEAN_FIELD_NONE),
+		 *   - arch/x86/kvm/vmx/vmcs12.c|85| <<global>> FIELD(VM_INSTRUCTION_ERROR, vm_instruction_error), VMXERR_ENTRY_INVALID_CONTROL_FIELD);
+		 *   - tools/testing/selftests/kvm/include/x86_64/vmx.h|214| <<global>> VM_INSTRUCTION_ERROR = 0x00004400,
+		 *   - arch/x86/kvm/vmx/nested.c|3275| <<nested_vmx_check_vmentry_hw>> u32 error = vmcs_read32(VM_INSTRUCTION_ERROR);
+		 *   - arch/x86/kvm/vmx/nested.c|5035| <<nested_vmx_vmexit>> WARN_ON_ONCE(vmcs_read32(VM_INSTRUCTION_ERROR) != VMXERR_ENTRY_INVALID_CONTROL_FIELD);
+		 *   - arch/x86/kvm/vmx/nested.c|6652| <<nested_vmx_reflect_vmexit>> trace_kvm_nested_vmenter_failed("hardware VM-instruction error: ", vmcs_read32(VM_INSTRUCTION_ERROR));
+		 *   - arch/x86/kvm/vmx/vmx.c|450| <<vmwrite_error>> vmx_insn_failed("vmwrite failed: field=%lx val=%lx err=%u\n", field, value, vmcs_read32(VM_INSTRUCTION_ERROR));
+		 *   - arch/x86/kvm/vmx/vmx.c|456| <<vmclear_error>> vmx_insn_failed("vmclear failed: %p/%llx err=%u\n", vmcs, phys_addr, vmcs_read32(VM_INSTRUCTION_ERROR));
+		 *   - arch/x86/kvm/vmx/vmx.c|462| <<vmptrld_error>> vmx_insn_failed("vmptrld failed: %p/%llx err=%u\n", vmcs, phys_addr, vmcs_read32(VM_INSTRUCTION_ERROR));
+		 *   - tools/testing/selftests/kvm/include/x86_64/evmcs.h|570| <<evmcs_vmread>> case VM_INSTRUCTION_ERROR:
+		 *   - tools/testing/selftests/kvm/include/x86_64/evmcs.h|1065| <<evmcs_vmwrite>> case VM_INSTRUCTION_ERROR:
+		 */
 		/*
 		 * The only expected VM-instruction error is "VM entry with
 		 * invalid control field(s)." Anything else indicates a
@@ -4902,6 +5293,17 @@ void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	vmx->fail = 0;
 }
 
+/*
+ * 在以下使用triple_fault:
+ *   - arch/x86/kvm/svm/nested.c|1828| <<global>> .triple_fault = nested_svm_triple_fault,
+ *   - arch/x86/kvm/vmx/nested.c|7121| <<global>> .triple_fault = nested_vmx_triple_fault,
+ *   - arch/x86/kvm/x86.c|8751| <<global>> .triple_fault = emulator_triple_fault,
+ *   - arch/x86/kvm/emulate.c|2323| <<em_rsm>> ctxt->ops->triple_fault(ctxt);
+ *   - arch/x86/kvm/x86.c|10425| <<kvm_check_nested_events>> kvm_x86_ops.nested_ops->triple_fault(vcpu);
+ *   - arch/x86/kvm/x86.c|11109| <<vcpu_enter_guest>> kvm_x86_ops.nested_ops->triple_fault(vcpu);
+ *
+ * struct kvm_x86_nested_ops vmx_nested_ops.triple_fault = nested_vmx_triple_fault()
+ */
 static void nested_vmx_triple_fault(struct kvm_vcpu *vcpu)
 {
 	kvm_clear_request(KVM_REQ_TRIPLE_FAULT, vcpu);
@@ -5089,15 +5491,38 @@ static struct vmcs *alloc_shadow_vmcs(struct kvm_vcpu *vcpu)
 	return loaded_vmcs->shadow_vmcs;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5402| <<handle_vmxon>> ret = enter_vmx_operation(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6798| <<vmx_set_nested_state>> ret = enter_vmx_operation(vcpu);
+ */
 static int enter_vmx_operation(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	int r;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/vmx/nested.c|5272| <<enter_vmx_operation>> r = alloc_loaded_vmcs(&vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/vmx.c|7664| <<vmx_vcpu_create>> err = alloc_loaded_vmcs(&vmx->vmcs01);
+	 */
 	r = alloc_loaded_vmcs(&vmx->nested.vmcs02);
 	if (r < 0)
 		goto out_vmcs02;
 
+	/*
+	 * 在以下使用nested_vmx->cached_vmcs12:
+	 *   - arch/x86/kvm/vmx/nested.c|318| <<free_nested>> kfree(vmx->nested.cached_vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|319| <<free_nested>> vmx->nested.cached_vmcs12 = NULL;
+	 *   - arch/x86/kvm/vmx/nested.c|1699| <<copy_enlightened_to_vmcs12>> struct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;
+	 *   - arch/x86/kvm/vmx/nested.c|1943| <<copy_vmcs12_to_enlightened>> struct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;
+	 *   - arch/x86/kvm/vmx/nested.c|5258| <<enter_vmx_operation>> vmx->nested.cached_vmcs12 = kzalloc(VMCS12_SIZE, GFP_KERNEL_ACCOUNT);
+	 *   - arch/x86/kvm/vmx/nested.c|5259| <<enter_vmx_operation>> if (!vmx->nested.cached_vmcs12)
+	 *   - arch/x86/kvm/vmx/nested.c|5290| <<enter_vmx_operation>> kfree(vmx->nested.cached_vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|5411| <<nested_release_vmcs12>> vmx->nested.cached_vmcs12, 0, VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|5784| <<handle_vmptrld>> if (kvm_read_guest_cached(vcpu->kvm, ghc, vmx->nested.cached_vmcs12, VMCS12_SIZE)) {
+	 *   - arch/x86/kvm/vmx/nested.h|41| <<get_vmcs12>> return to_vmx(vcpu)->nested.cached_vmcs12;
+	 */
 	vmx->nested.cached_vmcs12 = kzalloc(VMCS12_SIZE, GFP_KERNEL_ACCOUNT);
 	if (!vmx->nested.cached_vmcs12)
 		goto out_cached_vmcs12;
@@ -5185,6 +5610,11 @@ static int handle_vmxon(struct kvm_vcpu *vcpu)
 		return 1;
 	}
 
+	/*
+	 * struct vcpu_vmx:
+	 * -> struct nested_vmx nested;
+	 *    -> bool vmxon;
+	 */
 	if (vmx->nested.vmxon)
 		return nested_vmx_fail(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);
 
@@ -5834,6 +6264,10 @@ static int handle_invvpid(struct kvm_vcpu *vcpu)
 	return nested_vmx_succeed(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|6331| <<handle_vmfunc>> if (nested_vmx_eptp_switching(vcpu, vmcs12))
+ */
 static int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,
 				     struct vmcs12 *vmcs12)
 {
@@ -6402,6 +6836,9 @@ bool nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu)
 	return true;
 }
 
+/*
+ * struct kvm_x86_nested_ops vmx_nested_ops.get_state = vmx_get_nested_state()
+ */
 static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 				struct kvm_nested_state __user *user_kvm_nested_state,
 				u32 user_data_size)
@@ -6525,6 +6962,9 @@ static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 	return kvm_state.size;
 }
 
+/*
+ * struct kvm_x86_nested_ops vmx_nested_ops.leave_nested = vmx_leave_nested()
+ */
 void vmx_leave_nested(struct kvm_vcpu *vcpu)
 {
 	if (is_guest_mode(vcpu)) {
@@ -6534,6 +6974,11 @@ void vmx_leave_nested(struct kvm_vcpu *vcpu)
 	free_nested(vcpu);
 }
 
+/*
+ * KVM_SET_NESTED_STATE
+ *
+ * struct kvm_x86_nested_ops vmx_nested_ops.set_state = vmx_set_nested_state()
+ */
 static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 				struct kvm_nested_state __user *user_kvm_nested_state,
 				struct kvm_nested_state *kvm_state)
@@ -7047,6 +7492,10 @@ void nested_vmx_hardware_unsetup(void)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8786| <<hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+ */
 __init int nested_vmx_hardware_setup(int (*exit_handlers[])(struct kvm_vcpu *))
 {
 	int i;
@@ -7070,6 +7519,14 @@ __init int nested_vmx_hardware_setup(int (*exit_handlers[])(struct kvm_vcpu *))
 		init_vmcs_shadow_fields();
 	}
 
+	/*
+	 * 在以下使用:
+	 *   - arch/x86/kvm/vmx/vmx.c|6694| <<global>> static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
+	 *   - arch/x86/kvm/vmx/vmx.c|6750| <<global>> ARRAY_SIZE(kvm_vmx_exit_handlers);
+	 *   - arch/x86/kvm/vmx/vmx.c|7252| <<__vmx_handle_exit>> if (!kvm_vmx_exit_handlers[exit_handler_index])
+	 *   - arch/x86/kvm/vmx/vmx.c|7255| <<__vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_handler_index](vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|9334| <<hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+	 */
 	exit_handlers[EXIT_REASON_VMCLEAR]	= handle_vmclear;
 	exit_handlers[EXIT_REASON_VMLAUNCH]	= handle_vmlaunch;
 	exit_handlers[EXIT_REASON_VMPTRLD]	= handle_vmptrld;
@@ -7086,6 +7543,9 @@ __init int nested_vmx_hardware_setup(int (*exit_handlers[])(struct kvm_vcpu *))
 	return 0;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops..nested_ops = &vmx_nested_ops
+ */
 struct kvm_x86_nested_ops vmx_nested_ops = {
 	.leave_nested = vmx_leave_nested,
 	.is_exception_vmexit = nested_vmx_is_exception_vmexit,
diff --git a/arch/x86/kvm/vmx/nested.h b/arch/x86/kvm/vmx/nested.h
index b4b9d5143..1af90c164 100644
--- a/arch/x86/kvm/vmx/nested.h
+++ b/arch/x86/kvm/vmx/nested.h
@@ -38,11 +38,50 @@ bool nested_vmx_check_io_bitmaps(struct kvm_vcpu *vcpu, unsigned int port,
 
 static inline struct vmcs12 *get_vmcs12(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用nested_vmx->cached_vmcs12:
+	 *   - arch/x86/kvm/vmx/nested.c|318| <<free_nested>> kfree(vmx->nested.cached_vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|319| <<free_nested>> vmx->nested.cached_vmcs12 = NULL;
+	 *   - arch/x86/kvm/vmx/nested.c|1699| <<copy_enlightened_to_vmcs12>> struct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;
+	 *   - arch/x86/kvm/vmx/nested.c|1943| <<copy_vmcs12_to_enlightened>> struct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;
+	 *   - arch/x86/kvm/vmx/nested.c|5258| <<enter_vmx_operation>> vmx->nested.cached_vmcs12 = kzalloc(VMCS12_SIZE, GFP_KERNEL_ACCOUNT);
+	 *   - arch/x86/kvm/vmx/nested.c|5259| <<enter_vmx_operation>> if (!vmx->nested.cached_vmcs12)
+	 *   - arch/x86/kvm/vmx/nested.c|5290| <<enter_vmx_operation>> kfree(vmx->nested.cached_vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|5411| <<nested_release_vmcs12>> vmx->nested.cached_vmcs12, 0, VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|5784| <<handle_vmptrld>> if (kvm_read_guest_cached(vcpu->kvm, ghc, vmx->nested.cached_vmcs12, VMCS12_SIZE)) {
+	 *   - arch/x86/kvm/vmx/nested.h|41| <<get_vmcs12>> return to_vmx(vcpu)->nested.cached_vmcs12;
+	 *
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;   <---- 返回的
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 *    -> struct gfn_to_hva_cache shadow_vmcs12_cache;
+	 *    -> struct gfn_to_hva_cache vmcs12_cache;
+	 */
 	return to_vmx(vcpu)->nested.cached_vmcs12;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|803| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm, ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+ *   - arch/x86/kvm/vmx/nested.c|822| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm, ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+ *   - arch/x86/kvm/vmx/nested.c|5505| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ? get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5611| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ? get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6681| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12, get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+ *   - arch/x86/kvm/vmx/nested.c|6835| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+ */
 static inline struct vmcs12 *get_shadow_vmcs12(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 *    -> struct gfn_to_hva_cache shadow_vmcs12_cache;
+	 *    -> struct gfn_to_hva_cache vmcs12_cache;
+	 */
 	return to_vmx(vcpu)->nested.cached_shadow_vmcs12;
 }
 
@@ -277,6 +316,21 @@ static inline bool nested_host_cr0_valid(struct kvm_vcpu *vcpu, unsigned long va
 
 static inline bool nested_cr4_valid(struct kvm_vcpu *vcpu, unsigned long val)
 {
+	/*
+	 * struct vcpu_vmx:
+	 * -> struct nested_vmx nested;
+	 *    -> struct nested_vmx_msrs msrs;
+	 *       -> cr0_fixed0;
+	 *       -> u64 cr0_fixed1;
+	 *       -> u64 cr4_fixed0;
+	 *       -> u64 cr4_fixed1;
+	 *
+	 * 在以下使用nested_vmx_msrs->cr4_fixed:
+	 *   - arch/x86/kvm/vmx/nested.c|1518| <<vmx_get_fixed0_msr>> return &msrs->cr4_fixed0;
+	 *   - arch/x86/kvm/vmx/nested.c|1659| <<vmx_get_vmx_msr>> *pdata = msrs->cr4_fixed0;
+	 *   - arch/x86/kvm/vmx/nested.c|7360| <<nested_vmx_setup_cr_fixed>> msrs->cr4_fixed0 = VMXON_CR4_ALWAYSON;
+	 *   - arch/x86/kvm/vmx/nested.h|319| <<nested_cr4_valid>> u64 fixed0 = to_vmx(vcpu)->nested.msrs.cr4_fixed0;
+	 */
 	u64 fixed0 = to_vmx(vcpu)->nested.msrs.cr4_fixed0;
 	u64 fixed1 = to_vmx(vcpu)->nested.msrs.cr4_fixed1;
 
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index 820d3e1f6..b32687fce 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -68,9 +68,26 @@ static int fixed_pmc_events[] = {
 	[2] = PSEUDO_ARCH_REFERENCE_CYCLES,
 };
 
+/*
+ * 处理MSR_CORE_PERF_FIXED_CTR_CTRL:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|405| <<intel_pmu_set_msr>> reprogram_fixed_counters(pmu, data);
+ */
 static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 {
 	struct kvm_pmc *pmc;
+	/*
+	 * 每4个bit一组:
+	 * Enable field (lowest 2 bits within each 4-bit control) - When bit 0 is set, performance counting is
+	 * enabled in the corresponding fixed-function performance counter to increment while the target condition
+	 * associated with the architecture performance event occurred at ring 0. When bit 1 is set, performance counting
+	 * is enabled in the corresponding fixed-function performance counter to increment while the target condition
+	 * associated with the architecture performance event occurred at ring greater than 0. Writing 0 to both bits stops
+	 * the performance counter. Writing a value of 11B enables the counter to increment irrespective of privilege
+	 * levels.
+	 *
+	 * PMI field (the fourth bit within each 4-bit control) - When set, the logical processor generates an
+	 * exception through its local APIC on overflow condition of the respective fixed-function counter.
+	 */
 	u8 old_fixed_ctr_ctrl = pmu->fixed_ctr_ctrl;
 	int i;
 
diff --git a/arch/x86/kvm/vmx/vmcs.h b/arch/x86/kvm/vmx/vmcs.h
index 7c1996b43..a9cbf472b 100644
--- a/arch/x86/kvm/vmx/vmcs.h
+++ b/arch/x86/kvm/vmx/vmcs.h
@@ -60,18 +60,82 @@ struct vmcs_controls_shadow {
  */
 struct loaded_vmcs {
 	struct vmcs *vmcs;
+	/*
+	 * 注释
+	 * VMCS Shadowing (Intel)
+	 * Processor optimization that lets L0 define a shadow VMCS
+	 * Guest hypervisor can access shadow VMCS directly (in hardware)
+	 * Consequently, reducing the number of VM entries & exits
+	 */
 	struct vmcs *shadow_vmcs;
 	int cpu;
 	bool launched;
+	/*
+	 * 在以下使用loaded_vmcs->nmi_known_unmasked:
+	 *   - arch/x86/kvm/vmx/nested.c|2397| <<prepare_vmcs02_early>> vmx->loaded_vmcs->nmi_known_unmasked = !(vmcs12->guest_interruptibility_info & GUEST_INTR_STATE_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|4980| <<vmx_inject_nmi>> vmx->loaded_vmcs->nmi_known_unmasked = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|5010| <<vmx_get_nmi_mask>> if (vmx->loaded_vmcs->nmi_known_unmasked)
+	 *   - arch/x86/kvm/vmx/vmx.c|5013| <<vmx_get_nmi_mask>> vmx->loaded_vmcs->nmi_known_unmasked = !masked;
+	 *   - arch/x86/kvm/vmx/vmx.c|5027| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->nmi_known_unmasked = !masked;
+	 *   - arch/x86/kvm/vmx/vmx.c|7047| <<vmx_recover_nmi_blocking>> if (vmx->loaded_vmcs->nmi_known_unmasked)
+	 *   - arch/x86/kvm/vmx/vmx.c|7068| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->nmi_known_unmasked = !(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI);
+	 */
 	bool nmi_known_unmasked;
 	bool hv_timer_soft_disabled;
+	/*
+	 * 在以下使用loaded_vmcs->soft_vnmi_blocked:
+	 *   - arch/x86/kvm/vmx/vmx.c|4984| <<vmx_inject_nmi>> vmx->loaded_vmcs->soft_vnmi_blocked = 1;
+	 *   - arch/x86/kvm/vmx/vmx.c|5018| <<vmx_get_nmi_mask>> return vmx->loaded_vmcs->soft_vnmi_blocked;
+	 *   - arch/x86/kvm/vmx/vmx.c|5031| <<vmx_set_nmi_mask>> if (vmx->loaded_vmcs->soft_vnmi_blocked != masked) {
+	 *   - arch/x86/kvm/vmx/vmx.c|5032| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->soft_vnmi_blocked = masked;
+	 *   - arch/x86/kvm/vmx/vmx.c|5051| <<vmx_nmi_blocked>> if (!enable_vnmi && to_vmx(vcpu)->loaded_vmcs->soft_vnmi_blocked)
+	 *   - arch/x86/kvm/vmx/vmx.c|6553| <<__vmx_handle_exit>> vmx->loaded_vmcs->soft_vnmi_blocked)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6555| <<__vmx_handle_exit>> vmx->loaded_vmcs->soft_vnmi_blocked = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6567| <<__vmx_handle_exit>> vmx->loaded_vmcs->soft_vnmi_blocked = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7080| <<vmx_recover_nmi_blocking>> } else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))
+	 *   - arch/x86/kvm/vmx/vmx.c|7314| <<vmx_vcpu_run>> vmx->loaded_vmcs->soft_vnmi_blocked))
+	 */
 	/* Support for vnmi-less CPUs */
 	int soft_vnmi_blocked;
 	ktime_t entry_time;
+	/*
+	 * 在以下使用loaded_vmcs->vnmi_blocked_time:
+	 *   - arch/x86/kvm/vmx/vmx.c|5001| <<vmx_inject_nmi>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|5075| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6598| <<__vmx_handle_exit>> } else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&
+	 *   - arch/x86/kvm/vmx/vmx.c|7123| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->vnmi_blocked_time +=
+	 */
 	s64 vnmi_blocked_time;
+	/*
+	 * 在以下使用loaded_vmcs->msr_bitmap:
+	 *   - arch/x86/kvm/vmx/vmx.c|983| <<msr_write_intercepted>> return vmx_test_msr_bitmap_write(vmx->loaded_vmcs->msr_bitmap, msr);
+	 *   - arch/x86/kvm/vmx/vmx.c|2957| <<free_loaded_vmcs>> if (loaded_vmcs->msr_bitmap)
+	 *   - arch/x86/kvm/vmx/vmx.c|2958| <<free_loaded_vmcs>> free_page((unsigned long )loaded_vmcs->msr_bitmap);
+	 *   - arch/x86/kvm/vmx/vmx.c|2981| <<alloc_loaded_vmcs>> loaded_vmcs->msr_bitmap = (unsigned long *) __get_free_page(GFP_KERNEL_ACCOUNT);
+	 *   - arch/x86/kvm/vmx/vmx.c|2983| <<alloc_loaded_vmcs>> if (!loaded_vmcs->msr_bitmap)
+	 *   - arch/x86/kvm/vmx/vmx.c|2985| <<alloc_loaded_vmcs>> memset(loaded_vmcs->msr_bitmap, 0xff, PAGE_SIZE);
+	 *   - arch/x86/kvm/vmx/vmx.c|4042| <<vmx_disable_intercept_for_msr>> unsigned long *msr_bitmap = vmx->vmcs01.msr_bitmap;
+	 *   - arch/x86/kvm/vmx/vmx.c|4086| <<vmx_enable_intercept_for_msr>> unsigned long *msr_bitmap = vmx->vmcs01.msr_bitmap;
+	 *   - arch/x86/kvm/vmx/vmx.c|4125| <<vmx_update_msr_bitmap_x2apic>> u64 *msr_bitmap = (u64 *)vmx->vmcs01.msr_bitmap;
+	 *   - arch/x86/kvm/vmx/vmx.c|4795| <<init_vmcs>> vmcs_write64(MSR_BITMAP, __pa(vmx->vmcs01.msr_bitmap));
+	 *
+	 * 写入MSR_BITMAP的VMCS
+	 * 根据手册, 要分配1K x 4 = 4K, 分别用于:
+	 * - Read bitmap for low MSRs   -> 00000000H to 00001FFFH
+	 * - Read bitmap for high MSRs  -> C0000000H to C0001FFFH
+	 * - Write bitmap for low MSRs  -> 00000000H to 00001FFFH
+	 * - Write bitmap for high MSRs -> C0000000H to C0001FFFH
+	 */
 	unsigned long *msr_bitmap;
 	struct list_head loaded_vmcss_on_cpu_link;
 	struct vmcs_host_state host_state;
+	/*
+	 * 在以下使用loaded_vmcs->controls_shadow:
+	 *   - arch/x86/kvm/vmx/vmx.c|2989| <<alloc_loaded_vmcs>> memset(&loaded_vmcs->controls_shadow, 0, sizeof(struct vmcs_controls_shadow));
+	 *   - arch/x86/kvm/vmx/vmx.h|646| <<BUILD_CONTROLS_SHADOW>> if (vmx->loaded_vmcs->controls_shadow.lname != val) { \
+	 *   - arch/x86/kvm/vmx/vmx.h|648| <<BUILD_CONTROLS_SHADOW>> vmx->loaded_vmcs->controls_shadow.lname = val; \
+	 *   - arch/x86/kvm/vmx/vmx.h|653| <<BUILD_CONTROLS_SHADOW>> return vmcs->controls_shadow.lname; \
+	 */
 	struct vmcs_controls_shadow controls_shadow;
 };
 
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index be20a6004..3372d9a81 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -81,6 +81,11 @@ MODULE_DEVICE_TABLE(x86cpu, vmx_cpu_id);
 bool __read_mostly enable_vpid = 1;
 module_param_named(vpid, enable_vpid, bool, 0444);
 
+/*
+ * 在以下使用enable_vnmi:
+ *   - arch/x86/kvm/vmx/vmx.c|84| <<global>> static bool __read_mostly enable_vnmi = 1;
+ *   - arch/x86/kvm/vmx/vmx.c|8484| <<hardware_setup>> enable_vnmi = 0;
+ */
 static bool __read_mostly enable_vnmi = 1;
 module_param_named(vnmi, enable_vnmi, bool, 0444);
 
@@ -160,6 +165,13 @@ module_param(allow_smaller_maxphyaddr, bool, S_IRUGO);
  * List of MSRs that can be directly passed to the guest.
  * In addition to these x2apic and PT MSRs are handled specially.
  */
+/*
+ * 在以下使用vmx_possible_passthrough_msrs[MAX_POSSIBLE_PASSTHROUGH_MSRS]:
+ *   - arch/x86/kvm/vmx/vmx.c|677| <<possible_passthrough_msr_slot>> for (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++)
+ *   - arch/x86/kvm/vmx/vmx.c|678| <<possible_passthrough_msr_slot>> if (vmx_possible_passthrough_msrs[i] == msr)
+ *   - arch/x86/kvm/vmx/vmx.c|4435| <<vmx_msr_filter_changed>> for (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++) {
+ *   - arch/x86/kvm/vmx/vmx.c|4436| <<vmx_msr_filter_changed>> u32 msr = vmx_possible_passthrough_msrs[i];
+ */
 static u32 vmx_possible_passthrough_msrs[MAX_POSSIBLE_PASSTHROUGH_MSRS] = {
 	MSR_IA32_SPEC_CTRL,
 	MSR_IA32_PRED_CMD,
@@ -665,6 +677,12 @@ static inline bool cpu_need_virtualize_apic_accesses(struct kvm_vcpu *vcpu)
 	return flexpriority_enabled && lapic_in_kernel(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|709| <<is_valid_passthrough_msr>> r = possible_passthrough_msr_slot(msr) != -ENOENT;
+ *   - arch/x86/kvm/vmx/vmx.c|4265| <<vmx_disable_intercept_for_msr>> int idx = possible_passthrough_msr_slot(msr);
+ *   - arch/x86/kvm/vmx/vmx.c|4309| <<vmx_enable_intercept_for_msr>> int idx = possible_passthrough_msr_slot(msr);
+ */
 static int possible_passthrough_msr_slot(u32 msr)
 {
 	u32 i;
@@ -676,6 +694,11 @@ static int possible_passthrough_msr_slot(u32 msr)
 	return -ENOENT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4371| <<vmx_disable_intercept_for_msr>> if (is_valid_passthrough_msr(msr)) {
+ *   - arch/x86/kvm/vmx/vmx.c|4453| <<vmx_enable_intercept_for_msr>> if (is_valid_passthrough_msr(msr)) {
+ */
 static bool is_valid_passthrough_msr(u32 msr)
 {
 	bool r;
@@ -708,22 +731,47 @@ static bool is_valid_passthrough_msr(u32 msr)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4898| <<nested_vmx_get_vmcs01_guest_efer>> efer_msr = vmx_find_uret_msr(vmx, MSR_EFER);
+ *   - arch/x86/kvm/vmx/vmx.c|2153| <<vmx_setup_uret_msr>> uret_msr = vmx_find_uret_msr(vmx, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|2433| <<vmx_get_msr>> msr = vmx_find_uret_msr(vmx, msr_info->index);
+ *   - arch/x86/kvm/vmx/vmx.c|2763| <<vmx_set_msr>> msr = vmx_find_uret_msr(vmx, msr_index);
+ *   - arch/x86/kvm/vmx/vmx.c|3435| <<vmx_set_efer>> if (!vmx_find_uret_msr(vmx, MSR_EFER))
+ *   - arch/x86/kvm/vmx/vmx.c|8053| <<vmx_vcpu_create>> tsx_ctrl = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|8366| <<vmx_vcpu_after_set_cpuid>> msr = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);
+ */
 struct vmx_uret_msr *vmx_find_uret_msr(struct vcpu_vmx *vmx, u32 msr)
 {
 	int i;
 
+	/*
+	 * 返回msr(u32)在数组kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]的index
+	 */
 	i = kvm_find_user_return_msr(msr);
 	if (i >= 0)
 		return &vmx->guest_uret_msrs[i];
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2757| <<vmx_set_msr>> ret = vmx_set_guest_uret_msr(vmx, msr, data);
+ *   - arch/x86/kvm/vmx/vmx.c|8361| <<vmx_vcpu_after_set_cpuid>> vmx_set_guest_uret_msr(vmx, msr, enabled ? 0 : TSX_CTRL_RTM_DISABLE);
+ */
 static int vmx_set_guest_uret_msr(struct vcpu_vmx *vmx,
 				  struct vmx_uret_msr *msr, u64 data)
 {
+	/*
+	 * 记录下guest设置的MSR的值
+	 */
 	unsigned int slot = msr - vmx->guest_uret_msrs;
 	int ret = 0;
 
+	/*
+	 * 表示相关的值是否需要载入物理MSR.
+	 * 因为对于那些没有被激活的寄存器,不需要切换它们的寄存器值
+	 */
 	if (msr->load_into_hardware) {
 		preempt_disable();
 		ret = kvm_set_user_return_msr(slot, data, msr->mask);
@@ -928,17 +976,104 @@ void vmx_update_exception_bitmap(struct kvm_vcpu *vcpu)
 	vmcs_write32(EXCEPTION_BITMAP, eb);
 }
 
+/*
+ * 在dump_vmcs()的代码
+ * 6600         efer_slot = vmx_find_loadstore_msr_slot(&vmx->msr_autoload.guest, MSR_EFER);
+ * 6601         if (vmentry_ctl & VM_ENTRY_LOAD_IA32_EFER)
+ * 6602                 pr_err("EFER= 0x%016llx\n", vmcs_read64(GUEST_IA32_EFER));
+ * 6603         else if (efer_slot >= 0)
+ * 6604                 pr_err("EFER= 0x%016llx (autoload)\n",
+ * 6605                        vmx->msr_autoload.guest.val[efer_slot].value);
+ *
+ * vm exit:
+ * Save IA32_EFER: This control determines whether the IA32_EFER MSR is saved on VM exit.
+ * Load IA32_EFER: This control determines whether the IA32_EFER MSR is loaded on VM exit.
+ * vm entry:
+ * Load IA32_EFER: This control determines whether the IA32_EFER MSR is loaded on VM entry.
+ *
+ * #define BUILD_CONTROLS_SHADOW(lname, uname, bits)                                               \
+ * static inline void lname##_controls_set(struct vcpu_vmx *vmx, u##bits val)                      \
+ * {                                                                                               \
+ *	if (vmx->loaded_vmcs->controls_shadow.lname != val) {                                   \
+ *		vmcs_write##bits(uname, val);                                                   \
+ *		vmx->loaded_vmcs->controls_shadow.lname = val;                                  \
+ *	}                                                                                       \
+ * }                                                                                               \
+ * static inline u##bits __##lname##_controls_get(struct loaded_vmcs *vmcs)                        \
+ * {                                                                                               \
+ *	return vmcs->controls_shadow.lname;                                                     \
+ * }                                                                                               \
+ * static inline u##bits lname##_controls_get(struct vcpu_vmx *vmx)                                \
+ * {                                                                                               \
+ *	return __##lname##_controls_get(vmx->loaded_vmcs);                                      \
+ * }                                                                                               \
+ * static __always_inline void lname##_controls_setbit(struct vcpu_vmx *vmx, u##bits val)          \
+ * {                                                                                               \
+ *	BUILD_BUG_ON(!(val & (KVM_REQUIRED_VMX_##uname | KVM_OPTIONAL_VMX_##uname)));           \
+ *	lname##_controls_set(vmx, lname##_controls_get(vmx) | val);                             \
+ * }                                                                                               \
+ * static __always_inline void lname##_controls_clearbit(struct vcpu_vmx *vmx, u##bits val)        \
+ * {                                                                                               \
+ *	BUILD_BUG_ON(!(val & (KVM_REQUIRED_VMX_##uname | KVM_OPTIONAL_VMX_##uname)));           \
+ *	lname##_controls_set(vmx, lname##_controls_get(vmx) & ~val);                            \
+ * }
+ * BUILD_CONTROLS_SHADOW(vm_entry, VM_ENTRY_CONTROLS, 32)
+ * BUILD_CONTROLS_SHADOW(vm_exit, VM_EXIT_CONTROLS, 32)
+ * BUILD_CONTROLS_SHADOW(pin, PIN_BASED_VM_EXEC_CONTROL, 32)
+ * BUILD_CONTROLS_SHADOW(exec, CPU_BASED_VM_EXEC_CONTROL, 32)
+ * BUILD_CONTROLS_SHADOW(secondary_exec, SECONDARY_VM_EXEC_CONTROL, 32)
+ * BUILD_CONTROLS_SHADOW(tertiary_exec, TERTIARY_VM_EXEC_CONTROL, 64)
+ */
+
 /*
  * Check if MSR is intercepted for currently loaded MSR bitmap.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|959| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ */
 static bool msr_write_intercepted(struct vcpu_vmx *vmx, u32 msr)
 {
+	/*
+	 * This control determines whether MSR bitmaps are used to control
+	 * execution of the RDMSR and WRMSR instructions
+	 * (see Section 25.6.9 and Section 26.1.3).  For this control,
+	 * "0" means "do not use MSR bitmaps" and "1" means "use MSR bitmaps."
+	 * If the MSR bitmaps are not used, all executions of the 
+	 * RDMSR and WRMSR instructions cause VM exits.
+	 *
+	 * struct vmcs_controls_shadow {
+	 *     u32 vm_entry;
+	 *     u32 vm_exit;
+	 *     u32 pin;
+	 *     u32 exec;
+	 *     u32 secondary_exec;
+	 *     u64 tertiary_exec;
+	 * };
+	 *
+	 * 返回vmx->vmcs->controls_shadow.exec,
+	 * 如果不用CPU_BASED_USE_MSR_BITMAPS这个feature, 所有的rdmsr/wrmsr
+	 * 都要vm exit, 就没必要下面了
+	 */
 	if (!(exec_controls_get(vmx) & CPU_BASED_USE_MSR_BITMAPS))
 		return true;
 
+	/*
+	 * 写入MSR_BITMAP的VMCS
+	 * 根据手册, 要分配1K x 4 = 4K, 分别用于:
+	 * - Read bitmap for low MSRs   -> 00000000H to 00001FFFH
+	 * - Read bitmap for high MSRs  -> C0000000H to C0001FFFH
+	 * - Write bitmap for low MSRs  -> 00000000H to 00001FFFH
+	 * - Write bitmap for high MSRs -> C0000000H to C0001FFFH
+	 */
 	return vmx_test_msr_bitmap_write(vmx->loaded_vmcs->msr_bitmap, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3312| <<nested_vmx_check_vmentry_hw>> __vmx_vcpu_run_flags(vmx));
+ *   - arch/x86/kvm/vmx/vmx.c|7645| <<vmx_vcpu_run>> vmx_vcpu_enter_exit(vcpu, __vmx_vcpu_run_flags(vmx));
+ */
 unsigned int __vmx_vcpu_run_flags(struct vcpu_vmx *vmx)
 {
 	unsigned int flags = 0;
@@ -957,17 +1092,60 @@ unsigned int __vmx_vcpu_run_flags(struct vcpu_vmx *vmx)
 	return flags;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1030| <<clear_atomic_switch_msr>> clear_atomic_switch_msr_special(vmx, VM_ENTRY_LOAD_IA32_EFER, VM_EXIT_LOAD_IA32_EFER);
+ *   - arch/x86/kvm/vmx/vmx.c|1038| <<clear_atomic_switch_msr>> clear_atomic_switch_msr_special(vmx, VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL, VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);
+ *
+ * 猜测atomic的意思是指在entry control和exit control清空一些直接控制的VMCS,
+ * 决定在entry/exit的时候时候load/save MSR, 比如:
+ * - VM_ENTRY_LOAD_IA32_EFER, VM_EXIT_LOAD_IA32_EFER
+ * - VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL, VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL
+ */
 static __always_inline void clear_atomic_switch_msr_special(struct vcpu_vmx *vmx,
 		unsigned long entry, unsigned long exit)
 {
+	/*
+	 * 比方说entry control的Load_IA32_EFER
+	 * This control determines whether the IA32_EFER MSR is loaded on VM entry.
+	 *
+	 * 在VMCS guest state中也有对应的
+	 * IA32_EFER (64 bits). This field is supported only on processors that
+	 * support either the 1-setting of the "load IA32_EFER" VM-entry control
+	 * or that of the "save IA32_EFER" VM-exit control.
+	 */
 	vm_entry_controls_clearbit(vmx, entry);
 	vm_exit_controls_clearbit(vmx, exit);
 }
 
+/*
+ * 在dump_vmcs()的代码
+ * 6600         efer_slot = vmx_find_loadstore_msr_slot(&vmx->msr_autoload.guest, MSR_EFER);
+ * 6601         if (vmentry_ctl & VM_ENTRY_LOAD_IA32_EFER)
+ * 6602                 pr_err("EFER= 0x%016llx\n", vmcs_read64(GUEST_IA32_EFER));
+ * 6603         else if (efer_slot >= 0)
+ * 6604                 pr_err("EFER= 0x%016llx (autoload)\n",
+ * 6605                        vmx->msr_autoload.guest.val[efer_slot].value);
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|1131| <<nested_vmx_get_vmexit_msr_value>> int i = vmx_find_loadstore_msr_slot(&vmx->msr_autostore.guest,
+ *   - arch/x86/kvm/vmx/nested.c|1228| <<prepare_vmx_msr_autostore_list>> msr_autostore_slot = vmx_find_loadstore_msr_slot(autostore, msr_index);
+ *   - arch/x86/kvm/vmx/vmx.c|1045| <<clear_atomic_switch_msr>> i = vmx_find_loadstore_msr_slot(&m->guest, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|1053| <<clear_atomic_switch_msr>> i = vmx_find_loadstore_msr_slot(&m->host, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|1129| <<add_atomic_switch_msr>> i = vmx_find_loadstore_msr_slot(&m->guest, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|1131| <<add_atomic_switch_msr>> j = vmx_find_loadstore_msr_slot(&m->host, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|6531| <<dump_vmcs>> efer_slot = vmx_find_loadstore_msr_slot(&vmx->msr_autoload.guest, MSR_EFER);
+ */
 int vmx_find_loadstore_msr_slot(struct vmx_msrs *m, u32 msr)
 {
 	unsigned int i;
 
+	/*
+	 * struct vmx_msrs {
+	 *     unsigned int            nr;
+	 *     struct vmx_msr_entry    val[MAX_NR_LOADSTORE_MSRS];
+	 * };
+	 */
 	for (i = 0; i < m->nr; ++i) {
 		if (m->val[i].index == msr)
 			return i;
@@ -975,14 +1153,65 @@ int vmx_find_loadstore_msr_slot(struct vmx_msrs *m, u32 msr)
 	return -ENOENT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1273| <<update_transition_efer>> clear_atomic_switch_msr(vmx, MSR_EFER);
+ *   - arch/x86/kvm/vmx/vmx.c|1281| <<update_transition_efer>> clear_atomic_switch_msr(vmx, MSR_EFER);
+ *   - arch/x86/kvm/vmx/vmx.c|7503| <<atomic_switch_perf_msrs>> clear_atomic_switch_msr(vmx, msrs[i].msr);
+ *
+ * 清空!
+ * 如果是MSR_EFER或者MSR_CORE_PERF_GLOBAL_CTRL, 有专用的entry/exit control的bit控制
+ *     猜测atomic的意思是指在entry control和exit control清空一些直接控制的VMCS,
+ *     决定在entry/exit的时候时候load/save MSR, 比如:
+ *     - VM_ENTRY_LOAD_IA32_EFER, VM_EXIT_LOAD_IA32_EFER
+ *     - VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL, VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL
+ * 不然, 使用下面的
+ *     struct vcpu_vmx:
+ *         struct msr_autoload {
+ *         struct vmx_msrs guest;
+ *         struct vmx_msrs host;    
+ *     } msr_autoload;  
+ *
+ *     struct msr_autostore {  
+ *         struct vmx_msrs guest;
+ *     } msr_autostore;
+ *
+ *     - VM_EXIT_MSR_STORE_COUNT  --> MSRs to be stored on vm exit
+ *     - VM_EXIT_MSR_LOAD_COUNT   --> MSRS to be loaded on vm exit
+ *     - VM_ENTRY_MSR_LOAD_COUNT  --> MSRS to be loaded on vm entry
+ */
 static void clear_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr)
 {
 	int i;
+	/*
+	 * struct vcpu_vmx:
+	 *     struct msr_autoload {
+	 *         struct vmx_msrs guest;
+	 *         struct vmx_msrs host;    
+	 *     } msr_autoload;  
+	 *
+	 *     struct msr_autostore {  
+	 *         struct vmx_msrs guest;
+	 *     } msr_autostore;
+	 *
+	 * - VM_EXIT_MSR_STORE_COUNT  --> MSRs to be stored on vm exit
+	 * - VM_EXIT_MSR_LOAD_COUNT   --> MSRS to be loaded on vm exit
+	 * - VM_ENTRY_MSR_LOAD_COUNT  --> MSRS to be loaded on vm entry
+	 */
 	struct msr_autoload *m = &vmx->msr_autoload;
 
 	switch (msr) {
 	case MSR_EFER:
+		/*
+		 * return vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_EFER;
+		 */
 		if (cpu_has_load_ia32_efer()) {
+			/*
+			 * 猜测atomic的意思是指在entry control和exit control清空一些直接控制的VMCS,
+			 * 决定在entry/exit的时候时候load/save MSR, 比如:
+			 * - VM_ENTRY_LOAD_IA32_EFER, VM_EXIT_LOAD_IA32_EFER
+			 * - VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL, VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL
+			 */
 			clear_atomic_switch_msr_special(vmx,
 					VM_ENTRY_LOAD_IA32_EFER,
 					VM_EXIT_LOAD_IA32_EFER);
@@ -990,7 +1219,16 @@ static void clear_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr)
 		}
 		break;
 	case MSR_CORE_PERF_GLOBAL_CTRL:
+		/*
+		 * return vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL;
+		 */
 		if (cpu_has_load_perf_global_ctrl()) {
+			/*
+			 * 猜测atomic的意思是指在entry control和exit control清空一些直接控制的VMCS,
+			 * 决定在entry/exit的时候时候load/save MSR, 比如:
+			 * - VM_ENTRY_LOAD_IA32_EFER, VM_EXIT_LOAD_IA32_EFER
+			 * - VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL, VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL
+			 */
 			clear_atomic_switch_msr_special(vmx,
 					VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,
 					VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);
@@ -1015,6 +1253,54 @@ static void clear_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr)
 	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);
 }
 
+/*
+ * 关于add_atomic_switch_msr_special()中对HOST_IA32_EFER的特殊对待.
+ *
+ * commit 5a5e8a15d76e6dd62e3a94fea499057bd048abbc
+ * Author: Sean Christopherson <seanjc@google.com>
+ * Date:   Wed Sep 26 09:23:56 2018 -0700
+ *
+ * KVM: vmx: write HOST_IA32_EFER in vmx_set_constant_host_state()
+ *
+ * EFER is constant in the host and writing it once during setup means
+ * we can skip writing the host value in add_atomic_switch_msr_special().
+ *
+ * Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
+ * Reviewed-by: Jim Mattson <jmattson@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ *
+ * diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
+ * index 2d120de9054e..5367afe92c99 100644
+ * --- a/arch/x86/kvm/vmx.c
+ * +++ b/arch/x86/kvm/vmx.c
+ * @@ -2696,7 +2696,8 @@ static void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,
+ *                 u64 guest_val, u64 host_val)
+ *  {
+ *         vmcs_write64(guest_val_vmcs, guest_val);
+ * -       vmcs_write64(host_val_vmcs, host_val);
+ * +       if (host_val_vmcs != HOST_IA32_EFER)
+ * +               vmcs_write64(host_val_vmcs, host_val);
+ *         vm_entry_controls_setbit(vmx, entry);
+ *         vm_exit_controls_setbit(vmx, exit);
+ *  }
+ * @@ -6333,6 +6334,9 @@ static void vmx_set_constant_host_state(struct vcpu_vmx *vmx)
+ *                 rdmsr(MSR_IA32_CR_PAT, low32, high32);
+ *                 vmcs_write64(HOST_IA32_PAT, low32 | ((u64) high32 << 32));
+ *         }
+ * +
+ * +       if (cpu_has_load_ia32_efer)
+ * +               vmcs_write64(HOST_IA32_EFER, host_efer);
+ *  }
+ *
+ *  static void set_cr4_guest_host_mask(struct vcpu_vmx *vmx)
+ */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1044| <<add_atomic_switch_msr>> add_atomic_switch_msr_special(vmx, VM_ENTRY_LOAD_IA32_EFER, VM_EXIT_LOAD_IA32_EFER,
+ *									GUEST_IA32_EFER, HOST_IA32_EFER, guest_val, host_val);
+ *   - arch/x86/kvm/vmx/vmx.c|1055| <<add_atomic_switch_msr>> add_atomic_switch_msr_special(vmx, VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL, VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL,
+ *									GUEST_IA32_PERF_GLOBAL_CTRL, HOST_IA32_PERF_GLOBAL_CTRL, guest_val, host_val);
+ */
 static __always_inline void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,
 		unsigned long entry, unsigned long exit,
 		unsigned long guest_val_vmcs, unsigned long host_val_vmcs,
@@ -1027,6 +1313,11 @@ static __always_inline void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,
 	vm_exit_controls_setbit(vmx, exit);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1188| <<update_transition_efer>> add_atomic_switch_msr(vmx, MSR_EFER, guest_efer, host_efer, false);
+ *   - arch/x86/kvm/vmx/vmx.c|7423| <<atomic_switch_perf_msrs>> add_atomic_switch_msr(vmx, msrs[i].msr, msrs[i].guest, msrs[i].host, false);
+ */
 static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
 				  u64 guest_val, u64 host_val, bool entry_only)
 {
@@ -1035,6 +1326,16 @@ static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
 
 	switch (msr) {
 	case MSR_EFER:
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/vmx/nested.c|2539| <<prepare_vmcs02_early>> if (cpu_has_load_ia32_efer()) {
+		 *   - arch/x86/kvm/vmx/nested.c|2555| <<prepare_vmcs02_early>> if (cpu_has_load_ia32_efer() && guest_efer != host_efer)
+		 *   - arch/x86/kvm/vmx/nested.c|4890| <<nested_vmx_get_vmcs01_guest_efer>> if (cpu_has_load_ia32_efer())
+		 *   - arch/x86/kvm/vmx/vmx.c|990| <<clear_atomic_switch_msr>> if (cpu_has_load_ia32_efer()) {
+		 *   - arch/x86/kvm/vmx/vmx.c|1043| <<add_atomic_switch_msr>> if (cpu_has_load_ia32_efer()) {
+		 *   - arch/x86/kvm/vmx/vmx.c|1127| <<update_transition_efer>> if (cpu_has_load_ia32_efer() ||
+		 *   - arch/x86/kvm/vmx/vmx.c|4380| <<vmx_set_constant_host_state>> if (cpu_has_load_ia32_efer())
+		 */
 		if (cpu_has_load_ia32_efer()) {
 			add_atomic_switch_msr_special(vmx,
 					VM_ENTRY_LOAD_IA32_EFER,
@@ -1093,6 +1394,10 @@ static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
 	m->host.val[j].value = host_val;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2091| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_EFER, update_transition_efer(vmx));
+ */
 static bool update_transition_efer(struct vcpu_vmx *vmx)
 {
 	u64 guest_efer = vmx->vcpu.arch.efer;
@@ -1128,9 +1433,16 @@ static bool update_transition_efer(struct vcpu_vmx *vmx)
 					      guest_efer, host_efer, false);
 		else
 			clear_atomic_switch_msr(vmx, MSR_EFER);
+		/*
+		 * 这里尽量使用VMCS的方式(control或者bitmap)
+		 * 然后退出, 不然就要在if外面使用uret msr
+		 */
 		return false;
 	}
 
+	/*
+	 * 返回msr(u32)在数组kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]的index
+	 */
 	i = kvm_find_user_return_msr(MSR_EFER);
 	if (i < 0)
 		return false;
@@ -1277,6 +1589,11 @@ void vmx_set_host_fs_gs(struct vmcs_host_state *host, u16 fs_sel, u16 gs_sel,
 	}
 }
 
+/*
+ * 在以下使用vmx_prepare_switch_to_guest():
+ *   - arch/x86/kvm/vmx/vmx.c|8802| <<global>> struct kvm_x86_ops vmx_x86_ops.prepare_switch_to_guest = vmx_prepare_switch_to_guest,
+ *   - arch/x86/kvm/vmx/nested.c|3289| <<nested_vmx_check_vmentry_hw>> vmx_prepare_switch_to_guest(vcpu);
+ */
 void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -1295,12 +1612,30 @@ void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 	 * when guest state is loaded. This happens when guest transitions
 	 * to/from long-mode by setting MSR_EFER.LMA.
 	 */
+	/*
+	 * 表示guest的uret MSRs值是否已经载入到对应的物理寄存器
+	 */
 	if (!vmx->guest_uret_msrs_loaded) {
 		vmx->guest_uret_msrs_loaded = true;
 		for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+			/*
+			 * 表示相关的值是否需要载入物理MSR.
+			 * 因为对于那些没有被激活的寄存器,不需要切换它们的寄存器值
+			 */
 			if (!vmx->guest_uret_msrs[i].load_into_hardware)
 				continue;
 
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/svm/svm.c|1543| <<svm_prepare_switch_to_guest>> kvm_set_user_return_msr(tsc_aux_uret_slot, svm->tsc_aux, -1ull);
+			 *   - arch/x86/kvm/svm/svm.c|3113| <<svm_set_msr>> ret = kvm_set_user_return_msr(tsc_aux_uret_slot, data, -1ull);
+			 *   - arch/x86/kvm/vmx/vmx.c|747| <<vmx_set_guest_uret_msr>> ret = kvm_set_user_return_msr(slot, data, msr->mask);
+			 *   - arch/x86/kvm/vmx/vmx.c|1579| <<vmx_prepare_switch_to_guest>> kvm_set_user_return_msr(i, vmx->guest_uret_msrs[i].data, vmx->guest_uret_msrs[i].mask);
+			 *
+			 * 针对当前的CPU, 把给VM准备的msr的value写入硬件
+			 * 注册return to user的callback, 用来到时候改写
+			 * per_cpu_ptr(user_return_msrs, cpu)->values[slot].curr = value
+			 */
 			kvm_set_user_return_msr(i,
 						vmx->guest_uret_msrs[i].data,
 						vmx->guest_uret_msrs[i].mask);
@@ -1569,6 +1904,10 @@ u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)
 	return ret;
 }
 
+/*
+ * 在VMCS:GUEST_INTERRUPTIBILITY_INFO设置GUEST_INTR_STATE_MOV_SS或者GUEST_INTR_STATE_STI
+ * 如果参数是0就是清空那两个bit
+ */
 void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)
 {
 	u32 interruptibility_old = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
@@ -1837,15 +2176,35 @@ static void vmx_inject_exception(struct kvm_vcpu *vcpu)
 	vmx_clear_hlt(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2155| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_STAR, load_syscall_msrs);
+ *   - arch/x86/kvm/vmx/vmx.c|2156| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_LSTAR, load_syscall_msrs);
+ *   - arch/x86/kvm/vmx/vmx.c|2157| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_SYSCALL_MASK, load_syscall_msrs);
+ *   - arch/x86/kvm/vmx/vmx.c|2159| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_EFER, update_transition_efer(vmx));
+ *   - arch/x86/kvm/vmx/vmx.c|2161| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_TSC_AUX, guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDTSCP) || guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDPID));
+ *   - arch/x86/kvm/vmx/vmx.c|2171| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_IA32_TSX_CTRL, boot_cpu_has(X86_FEATURE_RTM));
+ */
 static void vmx_setup_uret_msr(struct vcpu_vmx *vmx, unsigned int msr,
 			       bool load_into_hardware)
 {
+	/*
+	 * struct vmx_uret_msr {
+	 *     bool load_into_hardware;
+	 *     u64 data;
+	 *     u64 mask;
+	 * };
+	 */
 	struct vmx_uret_msr *uret_msr;
 
 	uret_msr = vmx_find_uret_msr(vmx, msr);
 	if (!uret_msr)
 		return;
 
+	/*
+	 * 表示相关的值是否需要载入物理MSR.
+	 * 因为对于那些没有被激活的寄存器,不需要切换它们的寄存器值
+	 */
 	uret_msr->load_into_hardware = load_into_hardware;
 }
 
@@ -1855,6 +2214,12 @@ static void vmx_setup_uret_msr(struct vcpu_vmx *vmx, unsigned int msr,
  * an MSR here does _NOT_ mean it's not emulated, only that it will not be
  * loaded into hardware when running the guest.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|3352| <<vmx_set_efer>> vmx_setup_uret_msrs(vmx);
+ *   - arch/x86/kvm/vmx/vmx.c|5057| <<init_vmcs>> vmx_setup_uret_msrs(vmx);
+ *   - arch/x86/kvm/vmx/vmx.c|8201| <<vmx_vcpu_after_set_cpuid>> vmx_setup_uret_msrs(vmx);
+ */
 static void vmx_setup_uret_msrs(struct vcpu_vmx *vmx)
 {
 #ifdef CONFIG_X86_64
@@ -1885,6 +2250,9 @@ static void vmx_setup_uret_msrs(struct vcpu_vmx *vmx)
 	 */
 	vmx_setup_uret_msr(vmx, MSR_IA32_TSX_CTRL, boot_cpu_has(X86_FEATURE_RTM));
 
+	/*
+	 * 表示guest的uret MSRs值是否已经载入到对应的物理寄存器
+	 */
 	/*
 	 * The set of MSRs to load may have changed, reload MSRs before the
 	 * next VM-Enter.
@@ -2898,6 +3266,11 @@ void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
 	WARN_ON(loaded_vmcs->shadow_vmcs != NULL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5272| <<enter_vmx_operation>> r = alloc_loaded_vmcs(&vmx->nested.vmcs02);
+ *   - arch/x86/kvm/vmx/vmx.c|7664| <<vmx_vcpu_create>> err = alloc_loaded_vmcs(&vmx->vmcs01);
+ */
 int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
 {
 	loaded_vmcs->vmcs = alloc_vmcs(false);
@@ -3224,6 +3597,11 @@ static void vmx_flush_tlb_guest(struct kvm_vcpu *vcpu)
 	vpid_sync_context(vmx_get_current_vpid(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5068| <<nested_vmx_vmexit>> vmx_ept_load_pdptrs(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|3772| <<vmx_load_mmu_pgd>> vmx_ept_load_pdptrs(vcpu);
+ */
 void vmx_ept_load_pdptrs(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
@@ -3239,6 +3617,11 @@ void vmx_ept_load_pdptrs(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4955| <<nested_vmx_restore_host_state>> ept_save_pdptrs(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|2828| <<vmx_cache_reg>> ept_save_pdptrs(vcpu);
+ */
 void ept_save_pdptrs(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
@@ -3364,8 +3747,24 @@ static int vmx_get_max_ept_level(void)
 	return 4;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2316| <<prepare_vmcs02_constant_state>> construct_eptp(&vmx->vcpu, 0, PT64_ROOT_4LEVEL));
+ *   - arch/x86/kvm/vmx/vmx.c|3210| <<vmx_flush_tlb_current>> ept_sync_context(construct_eptp(vcpu, root_hpa, mmu->root_role.level));
+ *   - arch/x86/kvm/vmx/vmx.c|3400| <<vmx_load_mmu_pgd>> eptp = construct_eptp(vcpu, root_hpa, root_level);
+ */
 u64 construct_eptp(struct kvm_vcpu *vcpu, hpa_t root_hpa, int root_level)
 {
+	/*
+	 * A 4-KByte naturally aligned EPT PML4 table is located at the
+	 * physical address specified in bits 51:12 of the
+	 * extended-page-table pointer (EPTP), a VM-execution control field.
+	 *
+	 * bit 0-2:
+	 * EPT paging-structure memory type:
+	 * 0 = Uncacheable (UC)
+	 * 6 = Write-back (WB)
+	 */
 	u64 eptp = VMX_EPTP_MT_WB;
 
 	eptp |= (root_level == 5) ? VMX_EPTP_PWL_5 : VMX_EPTP_PWL_4;
@@ -3954,9 +4353,64 @@ static void vmx_msr_bitmap_l01_changed(struct vcpu_vmx *vmx)
 	vmx->nested.force_msr_bitmap_recalc = true;
 }
 
+/*
+ * 575 #define __BUILD_VMX_MSR_BITMAP_HELPER(rtype, action, bitop, access, base)      \
+ * 576 static inline rtype vmx_##action##_msr_bitmap_##access(unsigned long *bitmap,  \
+ * 577                                                        u32 msr)                \
+ * 578 {                                                                              \
+ * 579         int f = sizeof(unsigned long);                                         \
+ * 580                                                                                \
+ * 581         if (msr <= 0x1fff)                                                     \
+ * 582                 return bitop##_bit(msr, bitmap + base / f);                    \
+ * 583         else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff))                   \
+ * 584                 return bitop##_bit(msr & 0x1fff, bitmap + (base + 0x400) / f); \
+ * 585         return (rtype)true;                                                    \
+ * 586 }
+ * 587 #define BUILD_VMX_MSR_BITMAP_HELPERS(ret_type, action, bitop)                  \
+ * 588         __BUILD_VMX_MSR_BITMAP_HELPER(ret_type, action, bitop, read,  0x0)     \
+ * 589         __BUILD_VMX_MSR_BITMAP_HELPER(ret_type, action, bitop, write, 0x800)
+ * 590
+ * 591 BUILD_VMX_MSR_BITMAP_HELPERS(bool, test, test)
+ * 592 BUILD_VMX_MSR_BITMAP_HELPERS(void, clear, __clear)
+ * 593 BUILD_VMX_MSR_BITMAP_HELPERS(void, set, __set)
+ */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2511| <<vmx_set_msr(MSR_IA32_XFD)>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_XFD, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|2610| <<vmx_set_msr(MSR_IA32_SPEC_CTRL)>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|4426| <<vmx_update_msr_bitmap_x2apic>> vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|4427| <<vmx_update_msr_bitmap_x2apic>> vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|4429| <<vmx_update_msr_bitmap_x2apic>> vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_ICR), MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|4504| <<vmx_msr_filter_changed>> vmx_disable_intercept_for_msr(vcpu, msr, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|4507| <<vmx_msr_filter_changed>> vmx_disable_intercept_for_msr(vcpu, msr, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|8079| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_TSC, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|8081| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_FS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|8082| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_GS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|8083| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|8085| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|8086| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|8087| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|8089| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C1_RES, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|8090| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C3_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|8091| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C6_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|8092| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C7_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.h|541| <<vmx_set_intercept_for_msr>> vmx_disable_intercept_for_msr(vcpu, msr, type);
+ */
 void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	/*
+	 * 写入MSR_BITMAP的VMCS
+	 * 根据手册, 要分配1K x 4 = 4K, 分别用于:
+	 * - Read bitmap for low MSRs   -> 00000000H to 00001FFFH
+	 * - Read bitmap for high MSRs  -> C0000000H to C0001FFFH
+	 * - Write bitmap for low MSRs  -> 00000000H to 00001FFFH
+	 * - Write bitmap for high MSRs -> C0000000H to C0001FFFH
+	 *
+	 * struct vcpu_vmx *vmx:
+	 * -> struct loaded_vmcs vmcs01;
+	 *    -> unsigned long *msr_bitmap;
+	 */
 	unsigned long *msr_bitmap = vmx->vmcs01.msr_bitmap;
 
 	if (!cpu_has_vmx_msr_bitmap())
@@ -3971,7 +4425,31 @@ void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 	if (is_valid_passthrough_msr(msr)) {
 		int idx = possible_passthrough_msr_slot(msr);
 
+		/*
+		 * !=说明找到了
+		 */
 		if (idx != -ENOENT) {
+			/*
+			 * 在以下使用vcpu_vmx->shadow_msr_intercept:
+			 *   - arch/x86/kvm/vmx/vmx.c|4343| <<vmx_disable_intercept_for_msr>> clear_bit(idx, vmx->shadow_msr_intercept.read);
+			 *   - arch/x86/kvm/vmx/vmx.c|4345| <<vmx_disable_intercept_for_msr>> clear_bit(idx, vmx->shadow_msr_intercept.write);
+			 *   - arch/x86/kvm/vmx/vmx.c|4392| <<vmx_enable_intercept_for_msr>> set_bit(idx, vmx->shadow_msr_intercept.read);
+			 *   - arch/x86/kvm/vmx/vmx.c|4394| <<vmx_enable_intercept_for_msr>> set_bit(idx, vmx->shadow_msr_intercept.write);
+			 *   - arch/x86/kvm/vmx/vmx.c|4535| <<vmx_msr_filter_changed>> if (!test_bit(i, vmx->shadow_msr_intercept.read))
+			 *   - arch/x86/kvm/vmx/vmx.c|4538| <<vmx_msr_filter_changed>> if (!test_bit(i, vmx->shadow_msr_intercept.write))
+			 *   - arch/x86/kvm/vmx/vmx.c|8108| <<vmx_vcpu_create>> bitmap_fill(vmx->shadow_msr_intercept.read, MAX_POSSIBLE_PASSTHROUGH_MSRS);
+			 *   - arch/x86/kvm/vmx/vmx.c|8109| <<vmx_vcpu_create>> bitmap_fill(vmx->shadow_msr_intercept.write, MAX_POSSIBLE_PASSTHROUGH_MSRS);
+			 *
+			 * 应该就是为了vmx记录read/write哪个passthrough的msr要intercept
+			 * read/write的bitmap长度各自为16
+			 *
+			 * 460         // Save desired MSR intercept (read: pass-through) state
+			 * 461 #define MAX_POSSIBLE_PASSTHROUGH_MSRS   16
+			 * 462         struct {
+			 * 463                 DECLARE_BITMAP(read, MAX_POSSIBLE_PASSTHROUGH_MSRS);
+			 * 464                 DECLARE_BITMAP(write, MAX_POSSIBLE_PASSTHROUGH_MSRS);
+			 * 465         } shadow_msr_intercept;
+			 */
 			if (type & MSR_TYPE_R)
 				clear_bit(idx, vmx->shadow_msr_intercept.read);
 			if (type & MSR_TYPE_W)
@@ -3979,18 +4457,27 @@ void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 		}
 	}
 
+	/*
+	 * 如果msr filter不允许READ, 在type里and掉
+	 */
 	if ((type & MSR_TYPE_R) &&
 	    !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ)) {
 		vmx_set_msr_bitmap_read(msr_bitmap, msr);
 		type &= ~MSR_TYPE_R;
 	}
 
+	/*
+	 * 如果msr filter不允许WRITE, 在type里and掉
+	 */
 	if ((type & MSR_TYPE_W) &&
 	    !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE)) {
 		vmx_set_msr_bitmap_write(msr_bitmap, msr);
 		type &= ~MSR_TYPE_W;
 	}
 
+	/*
+	 * 不再intercept了!
+	 */
 	if (type & MSR_TYPE_R)
 		vmx_clear_msr_bitmap_read(msr_bitmap, msr);
 
@@ -3998,6 +4485,11 @@ void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 		vmx_clear_msr_bitmap_write(msr_bitmap, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4378| <<vmx_update_msr_bitmap_x2apic>> vmx_enable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TMCCT), MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.h|489| <<vmx_set_intercept_for_msr>> vmx_enable_intercept_for_msr(vcpu, msr, type);
+ */
 void vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4030,6 +4522,11 @@ void vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 		vmx_set_msr_bitmap_write(msr_bitmap, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4933| <<vmx_refresh_apicv_exec_ctrl>> vmx_update_msr_bitmap_x2apic(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7394| <<vmx_set_virtual_apic_mode>> vmx_update_msr_bitmap_x2apic(vcpu);
+ */
 static void vmx_update_msr_bitmap_x2apic(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -4090,6 +4587,12 @@ static void vmx_update_msr_bitmap_x2apic(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5487| <<enter_vmx_operation>> pt_update_intercept_for_msr(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|2643| <<vmx_set_msr>> pt_update_intercept_for_msr(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4447| <<vmx_msr_filter_changed>> pt_update_intercept_for_msr(vcpu);
+ */
 void pt_update_intercept_for_msr(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4126,6 +4629,20 @@ static bool vmx_guest_apic_has_interrupt(struct kvm_vcpu *vcpu)
 	return ((rvi & 0xf0) > (vppr & 0xf0));
 }
 
+/*
+ * 在以下使用KVM_REQ_MSR_FILTER_CHANGED:
+ *   - arch/x86/kvm/x86.c|7246| <<kvm_vm_ioctl_set_msr_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_MSR_FILTER_CHANGED);
+ *   - arch/x86/kvm/x86.c|11533| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MSR_FILTER_CHANGED, vcpu))
+ *
+ * 处理函数static_call(kvm_x86_msr_filter_changed)(vcpu)
+ * -> vmx_msr_filter_changed()
+ * -> svm_msr_filter_changed()
+ *
+ * 处理KVM_REQ_MSR_FILTER_CHANGED:
+ *   - arch/x86/kvm/x86.c|11534| <<vcpu_enter_guest>> static_call(kvm_x86_msr_filter_changed)(vcpu);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.msr_filter_changed = vmx_msr_filter_changed()
+ */
 static void vmx_msr_filter_changed(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4139,9 +4656,21 @@ static void vmx_msr_filter_changed(struct kvm_vcpu *vcpu)
 	 * refreshed since KVM is going to intercept them regardless of what
 	 * userspace wants.
 	 */
+	/*
+	 * List of MSRs that can be directly passed to the guest.
+	 * In addition to these x2apic and PT MSRs are handled specially.
+	 */
 	for (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++) {
 		u32 msr = vmx_possible_passthrough_msrs[i];
 
+		/*
+		 * 429         // Save desired MSR intercept (read: pass-through) state
+		 * 430 #define MAX_POSSIBLE_PASSTHROUGH_MSRS   16
+		 * 431         struct {
+		 * 432                 DECLARE_BITMAP(read, MAX_POSSIBLE_PASSTHROUGH_MSRS);
+		 * 433                 DECLARE_BITMAP(write, MAX_POSSIBLE_PASSTHROUGH_MSRS);
+		 * 434         } shadow_msr_intercept;
+		 */
 		if (!test_bit(i, vmx->shadow_msr_intercept.read))
 			vmx_disable_intercept_for_msr(vcpu, msr, MSR_TYPE_R);
 
@@ -4288,6 +4817,11 @@ static void vmx_deliver_interrupt(struct kvm_lapic *apic, int delivery_mode,
  * Note that host-state that does change is set elsewhere. E.g., host-state
  * that is set differently for each CPU is set in vmx_vcpu_load(), not here.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2397| <<prepare_vmcs02_constant_state>> vmx_set_constant_host_state(vmx);
+ *   - arch/x86/kvm/vmx/vmx.c|5012| <<init_vmcs>> vmx_set_constant_host_state(vmx);
+ */
 void vmx_set_constant_host_state(struct vcpu_vmx *vmx)
 {
 	u32 low32, high32;
@@ -4387,6 +4921,10 @@ static u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)
 	return pin_based_exec_ctrl;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5289| <<init_vmcs>> vm_entry_controls_set(vmx, vmx_vmentry_ctrl());
+ */
 static u32 vmx_vmentry_ctrl(void)
 {
 	u32 vmentry_ctrl = vmcs_config.vmentry_ctrl;
@@ -4701,6 +5239,10 @@ static int vmx_vcpu_precreate(struct kvm *kvm)
 
 #define VMX_XSS_EXIT_BITMAP 0
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5078| <<__vmx_vcpu_reset>> init_vmcs(vmx);
+ */
 static void init_vmcs(struct vcpu_vmx *vmx)
 {
 	struct kvm *kvm = vmx->vcpu.kvm;
@@ -4907,24 +5449,62 @@ static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 
 static void vmx_enable_irq_window(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * Intel注释
+	 * If the "interrupt-window exiting" VM-execution control is 1, a VM
+	 * exit occurs before execution of any instruction
+	 * if RFLAGS.IF = 1 and there is no blocking of events by STI or by MOV
+	 * SS (see Table 24-3). Such a VM exit occurs immediately after VM entry
+	 * if the above conditions are true.
+	 */
 	exec_controls_setbit(to_vmx(vcpu), CPU_BASED_INTR_WINDOW_EXITING);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10444| <<kvm_check_and_inject_events>> static_call(kvm_x86_enable_nmi_window)(vcpu);
+ */
 static void vmx_enable_nmi_window(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用enable_vnmi:
+	 *   - arch/x86/kvm/vmx/vmx.c|84| <<global>> static bool __read_mostly enable_vnmi = 1;
+	 *   - arch/x86/kvm/vmx/vmx.c|8484| <<hardware_setup>> enable_vnmi = 0;
+	 */
 	if (!enable_vnmi ||
 	    vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_STI) {
 		vmx_enable_irq_window(vcpu);
 		return;
 	}
 
+	/*
+	 * Intel注释
+	 * If the "NMI-window exiting" VM-execution control is 1, a VM exit
+	 * occurs before execution of any instruction if
+	 * there is no virtual-NMI blocking and there is no blocking of events by
+	 * MOV SS and no blocking of events by STI (see Table 24-3). Such a VM
+	 * exit occurs immediately after VM entry if the above conditions are
+	 * true (see Section 26.7.6).
+	 */
 	exec_controls_setbit(to_vmx(vcpu), CPU_BASED_NMI_WINDOW_EXITING);
 }
 
+/*
+ * 核心是写入VM_ENTRY_INTR_INFO_FIELD
+ */
 static void vmx_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	uint32_t intr;
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *      struct kvm_queued_interrupt {
+	 *          bool injected;
+	 *          bool soft;
+	 *          u8 nr;
+	 *	} interrupt;
+	 */
 	int irq = vcpu->arch.interrupt.nr;
 
 	trace_kvm_inj_virq(irq, vcpu->arch.interrupt.soft, reinjected);
@@ -4967,6 +5547,16 @@ static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 	}
 
 	++vcpu->stat.nmi_injections;
+	/*
+	 * 在以下使用loaded_vmcs->nmi_known_unmasked:
+	 *   - arch/x86/kvm/vmx/nested.c|2397| <<prepare_vmcs02_early>> vmx->loaded_vmcs->nmi_known_unmasked = !(vmcs12->guest_interruptibility_info & GUEST_INTR_STATE_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|4980| <<vmx_inject_nmi>> vmx->loaded_vmcs->nmi_known_unmasked = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|5010| <<vmx_get_nmi_mask>> if (vmx->loaded_vmcs->nmi_known_unmasked)
+	 *   - arch/x86/kvm/vmx/vmx.c|5013| <<vmx_get_nmi_mask>> vmx->loaded_vmcs->nmi_known_unmasked = !masked;
+	 *   - arch/x86/kvm/vmx/vmx.c|5027| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->nmi_known_unmasked = !masked;
+	 *   - arch/x86/kvm/vmx/vmx.c|7047| <<vmx_recover_nmi_blocking>> if (vmx->loaded_vmcs->nmi_known_unmasked)
+	 *   - arch/x86/kvm/vmx/vmx.c|7068| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->nmi_known_unmasked = !(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI);
+	 */
 	vmx->loaded_vmcs->nmi_known_unmasked = false;
 
 	if (vmx->rmode.vm86_active) {
@@ -4974,6 +5564,16 @@ static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 		return;
 	}
 
+	/*
+	 * #define INTR_TYPE_EXT_INTR              (0 << 8) // external interrupt
+	 * #define INTR_TYPE_RESERVED              (1 << 8) // reserved
+	 * #define INTR_TYPE_NMI_INTR              (2 << 8) // NMI
+	 * #define INTR_TYPE_HARD_EXCEPTION        (3 << 8) // processor exception
+	 * #define INTR_TYPE_SOFT_INTR             (4 << 8) // software interrupt
+	 * #define INTR_TYPE_PRIV_SW_EXCEPTION     (5 << 8) // ICE breakpoint - undocumented
+	 * #define INTR_TYPE_SOFT_EXCEPTION        (6 << 8) // software exception
+	 * #define INTR_TYPE_OTHER_EVENT           (7 << 8) // other event
+	 */
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD,
 			INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);
 
@@ -4985,10 +5585,46 @@ bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu)
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	bool masked;
 
+	/*
+	 * 在以下使用loaded_vmcs->soft_vnmi_blocked:
+	 *   - arch/x86/kvm/vmx/vmx.c|4984| <<vmx_inject_nmi>> vmx->loaded_vmcs->soft_vnmi_blocked = 1;
+	 *   - arch/x86/kvm/vmx/vmx.c|5018| <<vmx_get_nmi_mask>> return vmx->loaded_vmcs->soft_vnmi_blocked;
+	 *   - arch/x86/kvm/vmx/vmx.c|5031| <<vmx_set_nmi_mask>> if (vmx->loaded_vmcs->soft_vnmi_blocked != masked) {
+	 *   - arch/x86/kvm/vmx/vmx.c|5032| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->soft_vnmi_blocked = masked;
+	 *   - arch/x86/kvm/vmx/vmx.c|5051| <<vmx_nmi_blocked>> if (!enable_vnmi && to_vmx(vcpu)->loaded_vmcs->soft_vnmi_blocked)
+	 *   - arch/x86/kvm/vmx/vmx.c|6553| <<__vmx_handle_exit>> vmx->loaded_vmcs->soft_vnmi_blocked)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6555| <<__vmx_handle_exit>> vmx->loaded_vmcs->soft_vnmi_blocked = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6567| <<__vmx_handle_exit>> vmx->loaded_vmcs->soft_vnmi_blocked = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7080| <<vmx_recover_nmi_blocking>> } else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))
+	 *   - arch/x86/kvm/vmx/vmx.c|7314| <<vmx_vcpu_run>> vmx->loaded_vmcs->soft_vnmi_blocked))
+	 */
 	if (!enable_vnmi)
 		return vmx->loaded_vmcs->soft_vnmi_blocked;
+	/*
+	 * 在以下使用loaded_vmcs->nmi_known_unmasked:
+	 *   - arch/x86/kvm/vmx/nested.c|2397| <<prepare_vmcs02_early>> vmx->loaded_vmcs->nmi_known_unmasked = !(vmcs12->guest_interruptibility_info & GUEST_INTR_STATE_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|4980| <<vmx_inject_nmi>> vmx->loaded_vmcs->nmi_known_unmasked = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|5010| <<vmx_get_nmi_mask>> if (vmx->loaded_vmcs->nmi_known_unmasked)
+	 *   - arch/x86/kvm/vmx/vmx.c|5013| <<vmx_get_nmi_mask>> vmx->loaded_vmcs->nmi_known_unmasked = !masked;
+	 *   - arch/x86/kvm/vmx/vmx.c|5027| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->nmi_known_unmasked = !masked;
+	 *   - arch/x86/kvm/vmx/vmx.c|7047| <<vmx_recover_nmi_blocking>> if (vmx->loaded_vmcs->nmi_known_unmasked)
+	 *   - arch/x86/kvm/vmx/vmx.c|7068| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->nmi_known_unmasked = !(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI);
+	 */
 	if (vmx->loaded_vmcs->nmi_known_unmasked)
 		return false;
+	/*
+	 * Delivery of a non-maskable interrupt (NMI) or a system-management
+	 * interrupt (SMI) blocks subsequent NMIs until the next execution of
+	 * IRET. See Section 25.3 for how this behavior of IRET may change in
+	 * VMX non-root operation. Setting this bit indicates that blocking of
+	 * NMIs is in effect. Clearing this bit does not imply that NMIs are
+	 * not (temporarily) blocked for other reasons.
+	 *
+	 * If the "virtual NMIs" VM-execution control (see Section 24.6.1) is
+	 * 1, this bit does not control the
+	 * blocking of NMIs. Instead, it refers to "virtual-NMI blocking" (the
+	 * fact that guest software is not ready for an NMI).
+	 */
 	masked = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI;
 	vmx->loaded_vmcs->nmi_known_unmasked = !masked;
 	return masked;
@@ -5172,6 +5808,9 @@ bool vmx_guest_inject_ac(struct kvm_vcpu *vcpu)
 	       (kvm_get_rflags(vcpu) & X86_EFLAGS_AC);
 }
 
+/*
+ * static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu)[EXIT_REASON_EXCEPTION_NMI] = handle_exception_nmi,
+ */
 static int handle_exception_nmi(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5785,6 +6424,14 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 	if (unlikely(allow_smaller_maxphyaddr && kvm_vcpu_is_illegal_gpa(vcpu, gpa)))
 		return kvm_emulate_instruction(vcpu, 0);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4439| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn, insn_len);
+	 *   - arch/x86/kvm/svm/svm.c|2059| <<npf_interception>> return kvm_mmu_page_fault(vcpu, fault_address, error_code, static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+	 *                                                         svm->vmcb->control.insn_bytes : NULL, svm->vmcb->control.insn_len);
+	 *   - arch/x86/kvm/vmx/vmx.c|5887| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|5908| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 }
 
@@ -6134,6 +6781,58 @@ static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 static const int kvm_vmx_max_exit_handlers =
 	ARRAY_SIZE(kvm_vmx_exit_handlers);
 
+/*
+ * 在以下设置vcpu_vmx->exit_qualification:
+ *   - arch/x86/kvm/vmx/vmx.c|7420| <<vmx_vcpu_run>> vmx->exit_qualification = ENTRY_FAIL_DEFAULT;
+ *   - arch/x86/kvm/vmx/vmx.h|682| <<vmx_get_exit_qual>> vmx->exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
+ *
+ * 不同的exit reason有不同的EXIT_QUALIFICATION
+ *
+ *
+ * 在以下设置vcpu_vmx->exit_intr_info:
+ *   - arch/x86/kvm/vmx/vmx.c|7422| <<vmx_vcpu_run>> vmx->exit_intr_info = 0;
+ *   - arch/x86/kvm/vmx/vmx.h|692| <<vmx_get_intr_info>> vmx->exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
+ *
+ * 返回下面事件的信息:
+ *   - exceptions (包括INT1/INT3/INT0/BOUND/ID)
+ *   - external interrupts that occur while the "acknowledge interrupt on exit" VM-exit control is 1
+ *   - NMI
+ *
+ *
+ * 关于IDT_VECTORING_ERROR_CODE:
+ *
+ * - For VM exits that set both bit 31 (valid) and bit 11 (error code valid) in the VM-exit interruption-information
+ *   field, this field receives the error code that would have been pushed on the stack had the event causing the
+ *   VM exit been delivered normally through the IDT. The EXT bit is set in this field exactly when it would be set
+ *   normally. For exceptions that occur during the delivery of double fault (if the IDT-vectoring information field
+ *   indicates a double fault), the EXT bit is set to 1, assuming that (1) that the exception would produce an
+ *   error code normally (if not incident to double-fault delivery) and (2) that the error code uses the EXT bit
+ *   (not for page faults, which use a different format).
+ * -  For other VM exits, the value of this field is undefined.
+ *
+ *
+ * 在以下设置vcpu_vmx->idt_vectoring_info:
+ *   - arch/x86/kvm/vmx/vmx.c|7375| <<vmx_vcpu_enter_exit>> vmx->idt_vectoring_info = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|7386| <<vmx_vcpu_enter_exit>> vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
+ *
+ * This field receives basic information associated with the event that was
+ * being delivered when the VM exit occurred.
+ * - 0: External interrupt
+ * - 1: Not used
+ * - 2: Non-maskable interrupt (NMI)
+ * - 3: Hardware exception
+ * - 4: Software interrupt
+ * - 5: Privileged software exception
+ * - 6: Software exception
+ * - 7: Not used
+ *
+ *
+ * called by:
+ *   - arch/x86/kvm/trace.h|314| <<TRACE_EVENT_KVM_EXIT>> static_call(kvm_x86_get_exit_info)(vcpu, \
+ *   - arch/x86/kvm/x86.c|8790| <<prepare_emulation_failure_exit>> static_call(kvm_x86_get_exit_info)(vcpu, (u32 *)&info[0], &info[1],
+ *
+ * svm是: svm_get_exit_info()
+ */
 static void vmx_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,
 			      u64 *info1, u64 *info2,
 			      u32 *intr_info, u32 *error_code)
@@ -6143,6 +6842,12 @@ static void vmx_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,
 	*reason = vmx->exit_reason.full;
 	*info1 = vmx_get_exit_qual(vcpu);
 	if (!(vmx->exit_reason.failed_vmentry)) {
+		/*
+		 * 在下面的case:
+		 * info1 -> exit qualification
+		 * info2 -> idt_vectoring_info --> IDT_VECTORING_INFO_FIELD
+		 * intr_info -> VM_EXIT_INTR_INFO
+		 */
 		*info2 = vmx->idt_vectoring_info;
 		*intr_info = vmx_get_intr_info(vcpu);
 		if (is_exception_with_error_code(*intr_info))
@@ -6400,6 +7105,10 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
  * The guest has exited.  See if we can fix it or if we need userspace
  * assistance.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7272| <<vmx_handle_exit>> int ret = __vmx_handle_exit(vcpu, exit_fastpath);
+ */
 static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6481,6 +7190,21 @@ static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 		return 0;
 	}
 
+	/*
+	 * 在以下使用VM_INSTRUCTION_ERROR:
+	 *   - arch/x86/include/asm/vmx.h|292| <<global>> VM_INSTRUCTION_ERROR = 0x00004400,
+	 *   - arch/x86/kvm/vmx/hyperv.c|362| <<global>> EVMCS1_FIELD(VM_INSTRUCTION_ERROR, vm_instruction_error, HV_VMX_ENLIGHTENED_CLEAN_FIELD_NONE),
+	 *   - arch/x86/kvm/vmx/vmcs12.c|85| <<global>> FIELD(VM_INSTRUCTION_ERROR, vm_instruction_error), VMXERR_ENTRY_INVALID_CONTROL_FIELD);
+	 *   - tools/testing/selftests/kvm/include/x86_64/vmx.h|214| <<global>> VM_INSTRUCTION_ERROR = 0x00004400,
+	 *   - arch/x86/kvm/vmx/nested.c|3275| <<nested_vmx_check_vmentry_hw>> u32 error = vmcs_read32(VM_INSTRUCTION_ERROR);
+	 *   - arch/x86/kvm/vmx/nested.c|5035| <<nested_vmx_vmexit>> WARN_ON_ONCE(vmcs_read32(VM_INSTRUCTION_ERROR) != VMXERR_ENTRY_INVALID_CONTROL_FIELD);
+	 *   - arch/x86/kvm/vmx/nested.c|6652| <<nested_vmx_reflect_vmexit>> trace_kvm_nested_vmenter_failed("hardware VM-instruction error: ", vmcs_read32(VM_INSTRUCTION_ERROR));
+	 *   - arch/x86/kvm/vmx/vmx.c|450| <<vmwrite_error>> vmx_insn_failed("vmwrite failed: field=%lx val=%lx err=%u\n", field, value, vmcs_read32(VM_INSTRUCTION_ERROR));
+	 *   - arch/x86/kvm/vmx/vmx.c|456| <<vmclear_error>> vmx_insn_failed("vmclear failed: %p/%llx err=%u\n", vmcs, phys_addr, vmcs_read32(VM_INSTRUCTION_ERROR));
+	 *   - arch/x86/kvm/vmx/vmx.c|462| <<vmptrld_error>> vmx_insn_failed("vmptrld failed: %p/%llx err=%u\n", vmcs, phys_addr, vmcs_read32(VM_INSTRUCTION_ERROR));
+	 *   - tools/testing/selftests/kvm/include/x86_64/evmcs.h|570| <<evmcs_vmread>> case VM_INSTRUCTION_ERROR:
+	 *   - tools/testing/selftests/kvm/include/x86_64/evmcs.h|1065| <<evmcs_vmwrite>> case VM_INSTRUCTION_ERROR:
+	 */
 	if (unlikely(vmx->fail)) {
 		dump_vmcs(vcpu);
 		vcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;
@@ -6579,6 +7303,31 @@ static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 	return 0;
 }
 
+/*
+ * Steps 1-4 above perform checks that may cause VM entry to fail. Such failures
+ * occur in one of the following three ways:
+ * 一共可能有三种模式返回vm entry的错误:
+ *
+ * 1. Some of the checks in Section 27.1 may generate ordinary faults (for
+ * example, an invalid-opcode exception).  Such faults are delivered normally.
+ * 通过普通的方式(比如general protection fault).
+ * 目前猜测造成exception的RIP就是vmresume. 写了一些代码验证也是这样.
+ *
+ * 2. Some of the checks in Section 27.1 and all the checks in Section 27.2 cause
+ * control to pass to the instruction following the VM-entry instruction. The
+ * failure is indicated by setting RFLAGS.ZF (if there is a current VMCS) or
+ * RFLAGS.CF (if there is no current VMCS). If there is a current VMCS, an error
+ * number indicating the cause of the failure is stored in the VM-instruction
+ * error field. See Chapter 31 for the error numbers.
+ * 进行到下一条指令, 把ZF/CF置位. 设置VM-instruction error field.
+ *
+ * 3. The checks in Section 27.3 and Section 27.4 cause processor state to be
+ * loaded from the host-state area of the VMCS (as would be done on a VM exit).
+ * Information about the failure is stored in the VM-exit information fields.  See
+ * Section 27.8 for details.
+ * 像是普通的vmexit一样退出. 设置VM-exit information field.
+ */
+
 static int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	int ret = __vmx_handle_exit(vcpu, exit_fastpath);
@@ -7014,6 +7763,10 @@ static bool vmx_has_emulated_msr(struct kvm *kvm, u32 index)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7496| <<vmx_vcpu_run>> vmx_recover_nmi_blocking(vmx);
+ */
 static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
 {
 	u32 exit_intr_info;
@@ -7052,8 +7805,20 @@ static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
 		vmx->loaded_vmcs->vnmi_blocked_time +=
 			ktime_to_ns(ktime_sub(ktime_get(),
 					      vmx->loaded_vmcs->entry_time));
+	/*
+	 * 在以下使用loaded_vmcs->vnmi_blocked_time:
+	 *   - arch/x86/kvm/vmx/vmx.c|5001| <<vmx_inject_nmi>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|5075| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6598| <<__vmx_handle_exit>> } else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&
+	 *   - arch/x86/kvm/vmx/vmx.c|7123| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->vnmi_blocked_time +=
+	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7133| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info, VM_EXIT_INSTRUCTION_LEN, IDT_VECTORING_ERROR_CODE);
+ *   - arch/x86/kvm/vmx/vmx.c|7140| <<vmx_cancel_injection>> __vmx_complete_interrupts(vcpu, vmcs_read32(VM_ENTRY_INTR_INFO_FIELD), VM_ENTRY_INSTRUCTION_LEN, VM_ENTRY_EXCEPTION_ERROR_CODE);
+ */
 static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 				      u32 idt_vectoring_info,
 				      int instr_len_field,
@@ -7125,6 +7890,10 @@ static void vmx_cancel_injection(struct kvm_vcpu *vcpu)
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7791| <<vmx_vcpu_run>> atomic_switch_perf_msrs(vmx);
+ */
 static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
 {
 	int i, nr_msrs;
@@ -7140,6 +7909,14 @@ static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
 	if (!msrs)
 		return;
 
+	/*
+	 * struct perf_guest_switch_msr *msrs;
+	 *
+	 * struct perf_guest_switch_msr {
+	 *     unsigned msr;
+	 *     u64 host, guest;
+	 * };
+	 */
 	for (i = 0; i < nr_msrs; i++)
 		if (msrs[i].host == msrs[i].guest)
 			clear_atomic_switch_msr(vmx, msrs[i].msr);
@@ -7255,6 +8032,11 @@ static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
 		goto out;
 	}
 
+	/*
+	 * 还有一个EXIT_QUALIFICATION
+	 *
+	 * EXIT_QUALIFICATION是32-bit
+	 */
 	vmx->exit_reason.full = vmcs_read32(VM_EXIT_REASON);
 	if (likely(!vmx->exit_reason.failed_vmentry))
 		vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
@@ -7823,6 +8605,10 @@ static void vmx_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 	vmx_update_exception_bitmap(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8059| <<vmx_set_cpu_caps>> kvm_caps.supported_perf_cap = vmx_get_perf_capabilities();
+ */
 static u64 vmx_get_perf_capabilities(void)
 {
 	u64 perf_cap = PMU_CAP_FW_WRITES;
@@ -7850,6 +8636,10 @@ static u64 vmx_get_perf_capabilities(void)
 	return perf_cap;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8783| <<hardware_setup>> vmx_set_cpu_caps();
+ */
 static __init void vmx_set_cpu_caps(void)
 {
 	kvm_set_cpu_caps();
@@ -8362,6 +9152,10 @@ static unsigned int vmx_handle_intel_pt_intr(void)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|9125| <<hardware_setup>> vmx_setup_user_return_msrs();
+ */
 static __init void vmx_setup_user_return_msrs(void)
 {
 
@@ -8384,6 +9178,13 @@ static __init void vmx_setup_user_return_msrs(void)
 
 	BUILD_BUG_ON(ARRAY_SIZE(vmx_uret_msrs_list) != MAX_NR_USER_RETURN_MSRS);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/svm/svm.c|5194| <<svm_hardware_setup>> tsc_aux_uret_slot = kvm_add_user_return_msr(MSR_TSC_AUX);
+	 *   - arch/x86/kvm/vmx/vmx.c|8961| <<vmx_setup_user_return_msrs>> kvm_add_user_return_msr(vmx_uret_msrs_list[i]);
+	 *
+	 * 把一个msr(u32)加入数组kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]
+	 */
 	for (i = 0; i < ARRAY_SIZE(vmx_uret_msrs_list); ++i)
 		kvm_add_user_return_msr(vmx_uret_msrs_list[i]);
 }
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index c2130d2c8..254a8450b 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -36,6 +36,15 @@ struct vmx_msrs {
 };
 
 struct vmx_uret_msr {
+	/*
+	 * 在以下使用vmx_uret_msr->load_into_hardware:
+	 *   - arch/x86/kvm/vmx/vmx.c|745| <<vmx_set_guest_uret_msr>> if (msr->load_into_hardware) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1576| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs[i].load_into_hardware)
+	 *   - arch/x86/kvm/vmx/vmx.c|2128| <<vmx_setup_uret_msr>> uret_msr->load_into_hardware = load_into_hardware
+	 *
+	 * 表示相关的值是否需要载入物理MSR.
+	 * 因为对于那些没有被激活的寄存器,不需要切换它们的寄存器值
+	 */
 	bool load_into_hardware;
 	u64 data;
 	u64 mask;
@@ -126,7 +135,28 @@ struct nested_vmx {
 	 * Loaded from guest memory during VMPTRLD. Flushed to guest
 	 * memory during VMCLEAR and VMPTRLD.
 	 */
+	/*
+	 * 在以下使用nested_vmx->cached_vmcs12:
+	 *   - arch/x86/kvm/vmx/nested.c|318| <<free_nested>> kfree(vmx->nested.cached_vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|319| <<free_nested>> vmx->nested.cached_vmcs12 = NULL;
+	 *   - arch/x86/kvm/vmx/nested.c|1699| <<copy_enlightened_to_vmcs12>> struct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;
+	 *   - arch/x86/kvm/vmx/nested.c|1943| <<copy_vmcs12_to_enlightened>> struct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;
+	 *   - arch/x86/kvm/vmx/nested.c|5258| <<enter_vmx_operation>> vmx->nested.cached_vmcs12 = kzalloc(VMCS12_SIZE, GFP_KERNEL_ACCOUNT);
+	 *   - arch/x86/kvm/vmx/nested.c|5259| <<enter_vmx_operation>> if (!vmx->nested.cached_vmcs12)
+	 *   - arch/x86/kvm/vmx/nested.c|5290| <<enter_vmx_operation>> kfree(vmx->nested.cached_vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|5411| <<nested_release_vmcs12>> vmx->nested.cached_vmcs12, 0, VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|5784| <<handle_vmptrld>> if (kvm_read_guest_cached(vcpu->kvm, ghc, vmx->nested.cached_vmcs12, VMCS12_SIZE)) {
+	 *   - arch/x86/kvm/vmx/nested.h|41| <<get_vmcs12>> return to_vmx(vcpu)->nested.cached_vmcs12;
+	 */
 	struct vmcs12 *cached_vmcs12;
+
+	/*
+	 * 注释
+	 * VMCS Shadowing (Intel)
+	 * Processor optimization that lets L0 define a shadow VMCS
+	 * Guest hypervisor can access shadow VMCS directly (in hardware)
+	 * Consequently, reducing the number of VM entries & exits
+	 */
 	/*
 	 * Cache of the guest's shadow VMCS, existing outside of guest
 	 * memory. Loaded from guest memory during VM entry. Flushed
@@ -185,6 +215,37 @@ struct nested_vmx {
 	 */
 	bool enlightened_vmcs_enabled;
 
+	/*
+	 * 在以下使用nested_vmx->nested_run_pending:
+	 *   - arch/x86/kvm/vmx/nested.c|2330| <<nested_vmx_calc_efer>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx/nested.c|2564| <<prepare_vmcs02_early>> if (vmx->nested.nested_run_pending) {
+	 *   - arch/x86/kvm/vmx/nested.c|2645| <<prepare_vmcs02_rare>> if (kvm_mpx_supported() && vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx/nested.c|2731| <<prepare_vmcs02>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx/nested.c|2739| <<prepare_vmcs02>> if (kvm_mpx_supported() && (!vmx->nested.nested_run_pending ||
+	 *   - arch/x86/kvm/vmx/nested.c|2752| <<prepare_vmcs02>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx/nested.c|3253| <<nested_vmx_check_guest_state>> if (to_vmx(vcpu)->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx/nested.c|3643| <<nested_vmx_enter_non_root_mode>> if (!vmx->nested.nested_run_pending ||
+	 *   - arch/x86/kvm/vmx/nested.c|3647| <<nested_vmx_enter_non_root_mode>> (!vmx->nested.nested_run_pending ||
+	 *   - arch/x86/kvm/vmx/nested.c|3900| <<nested_vmx_run>> vmx->nested.nested_run_pending = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|3940| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|3945| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|3955| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|4346| <<vmx_check_nested_events>> bool block_nested_exceptions = vmx->nested.nested_run_pending;
+	 *   - arch/x86/kvm/vmx/nested.c|4643| <<sync_vmcs02_to_vmcs12>> !vmx->nested.nested_run_pending)
+	 *   - arch/x86/kvm/vmx/nested.c|5047| <<nested_vmx_vmexit>> WARN_ON_ONCE(vmx->nested.nested_run_pending);
+	 *   - arch/x86/kvm/vmx/nested.c|6728| <<nested_vmx_reflect_vmexit>> WARN_ON_ONCE(vmx->nested.nested_run_pending);
+	 *   - arch/x86/kvm/vmx/nested.c|6827| <<vmx_get_nested_state>> if (vmx->nested.nested_run_pending)
+	 *   - arch/x86/kvm/vmx/nested.c|6905| <<vmx_leave_nested>> to_vmx(vcpu)->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7043| <<vmx_set_nested_state>> vmx->nested.nested_run_pending =
+	 *   - arch/x86/kvm/vmx/nested.c|7095| <<vmx_set_nested_state>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|5636| <<vmx_nmi_allowed>> if (to_vmx(vcpu)->nested.nested_run_pending)
+	 *   - arch/x86/kvm/vmx/vmx.c|5658| <<vmx_interrupt_allowed>> if (to_vmx(vcpu)->nested.nested_run_pending)
+	 *   - arch/x86/kvm/vmx/vmx.c|7100| <<__vmx_handle_exit>> if (KVM_BUG_ON(vmx->nested.nested_run_pending, vcpu->kvm))
+	 *   - arch/x86/kvm/vmx/vmx.c|8121| <<vmx_vcpu_run>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx/vmx.c|8125| <<vmx_vcpu_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|8842| <<vmx_smi_allowed>> if (to_vmx(vcpu)->nested.nested_run_pending)
+	 *   - arch/x86/kvm/vmx/vmx.c|8883| <<vmx_leave_smm>> vmx->nested.nested_run_pending = 1;
+	 */
 	/* L2 must run next, and mustn't decide to exit to L1. */
 	bool nested_run_pending;
 
@@ -260,8 +321,23 @@ struct vcpu_vmx {
 	 */
 	bool		      guest_state_loaded;
 
+	/*
+	 * 在以下设置vcpu_vmx->exit_qualification:
+	 *   - arch/x86/kvm/vmx/vmx.c|7420| <<vmx_vcpu_run>> vmx->exit_qualification = ENTRY_FAIL_DEFAULT;
+	 *   - arch/x86/kvm/vmx/vmx.h|682| <<vmx_get_exit_qual>> vmx->exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
+	 */
 	unsigned long         exit_qualification;
+	/*
+	 * 在以下设置vcpu_vmx->exit_intr_info:
+	 *   - arch/x86/kvm/vmx/vmx.c|7422| <<vmx_vcpu_run>> vmx->exit_intr_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.h|692| <<vmx_get_intr_info>> vmx->exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
+	 */
 	u32                   exit_intr_info;
+	/*
+	 * 在以下设置vcpu_vmx->idt_vectoring_info:
+	 *   - arch/x86/kvm/vmx/vmx.c|7375| <<vmx_vcpu_enter_exit>> vmx->idt_vectoring_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7386| <<vmx_vcpu_enter_exit>> vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
+	 */
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
 
@@ -271,13 +347,43 @@ struct vcpu_vmx {
 	 * of 64-bit mode or if EFER.SCE=1, thus the SYSCALL MSRs don't need to
 	 * be loaded into hardware if those conditions aren't met.
 	 */
+	/*
+	 * 在以下使用vcpu_vmx->guest_uret_msrs[MAX_NR_USER_RETURN_MSRS]:
+	 *   - arch/x86/kvm/vmx/vmx.c|735| <<vmx_find_uret_msr>> return &vmx->guest_uret_msrs[i];
+	 *   - arch/x86/kvm/vmx/vmx.c|742| <<vmx_set_guest_uret_msr>> unsigned int slot = msr - vmx->guest_uret_msrs;
+	 *   - arch/x86/kvm/vmx/vmx.c|1418| <<update_transition_efer>> vmx->guest_uret_msrs[i].data = guest_efer;
+	 *   - arch/x86/kvm/vmx/vmx.c|1419| <<update_transition_efer>> vmx->guest_uret_msrs[i].mask = ~ignore_bits;
+	 *   - arch/x86/kvm/vmx/vmx.c|1581| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs[i].load_into_hardware)
+	 *   - arch/x86/kvm/vmx/vmx.c|1585| <<vmx_prepare_switch_to_guest>> kvm_set_user_return_msr(i, vmx->guest_uret_msrs[i].data, vmx->guest_uret_msrs[i].mask);
+	 *   - arch/x86/kvm/vmx/vmx.c|1586| <<vmx_prepare_switch_to_guest>> kvm_set_user_return_msr(i, vmx->guest_uret_msrs[i].data, vmx->guest_uret_msrs[i].mask);
+	 *   - arch/x86/kvm/vmx/vmx.c|8038| <<vmx_vcpu_create>> vmx->guest_uret_msrs[i].mask = -1ull;
+	 *
+	 * 记录下guest设置的MSR的值
+	 */
 	struct vmx_uret_msr   guest_uret_msrs[MAX_NR_USER_RETURN_MSRS];
+	/*
+	 * 在以下使用vcpu_vmx->guest_uret_msrs_loaded:
+	 *   - arch/x86/kvm/vmx/vmx.c|1578| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs_loaded) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1579| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs_loaded = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|1670| <<vmx_prepare_switch_to_host>> vmx->guest_uret_msrs_loaded = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|2198| <<vmx_setup_uret_msrs>> vmx->guest_uret_msrs_loaded = false;
+	 *
+	 * 表示guest的uret MSRs值是否已经载入到对应的物理寄存器
+	 */
 	bool                  guest_uret_msrs_loaded;
 #ifdef CONFIG_X86_64
 	u64		      msr_host_kernel_gs_base;
 	u64		      msr_guest_kernel_gs_base;
 #endif
 
+	/*
+	 * 在以下使用vcpu_vmx->spec_ctrl:
+	 *   - arch/x86/kvm/vmx/vmx.c|2302| <<vmx_get_msr(MSR_IA32_SPEC_CTRL)>> msr_info->data = to_vmx(vcpu)->spec_ctrl;
+	 *   - arch/x86/kvm/vmx/vmx.c|2565| <<vmx_set_msr(MSR_IA32_SPEC_CTRL)>> vmx->spec_ctrl = data;
+	 *   - arch/x86/kvm/vmx/vmx.c|5183| <<vmx_vcpu_reset>> vmx->spec_ctrl = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7726| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+	 *   - arch/x86/kvm/vmx/vmx.c|7736| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) || vmx->spec_ctrl != hostval)
+	 */
 	u64		      spec_ctrl;
 	u32		      msr_ia32_umwait_control;
 
@@ -289,6 +395,32 @@ struct vcpu_vmx {
 	struct loaded_vmcs    vmcs01;
 	struct loaded_vmcs   *loaded_vmcs;
 
+	/*
+	 * 在以下使用msr_autoload:
+	 *   - arch/x86/kvm/vmx/nested.c|2394| <<prepare_vmcs02_constant_state>> vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host.val));
+	 *   - arch/x86/kvm/vmx/nested.c|2395| <<prepare_vmcs02_constant_state>> vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest.val));
+	 *   - arch/x86/kvm/vmx/nested.c|2692| <<prepare_vmcs02_rare>> vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);
+	 *   - arch/x86/kvm/vmx/nested.c|2693| <<prepare_vmcs02_rare>> vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);
+	 *   - arch/x86/kvm/vmx/nested.c|3282| <<nested_vmx_check_vmentry_hw>> if (vmx->msr_autoload.host.nr)
+	 *   - arch/x86/kvm/vmx/nested.c|3284| <<nested_vmx_check_vmentry_hw>> if (vmx->msr_autoload.guest.nr)
+	 *   - arch/x86/kvm/vmx/nested.c|3314| <<nested_vmx_check_vmentry_hw>> if (vmx->msr_autoload.host.nr)
+	 *   - arch/x86/kvm/vmx/nested.c|3315| <<nested_vmx_check_vmentry_hw>> vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);
+	 *   - arch/x86/kvm/vmx/nested.c|3316| <<nested_vmx_check_vmentry_hw>> if (vmx->msr_autoload.guest.nr)
+	 *   - arch/x86/kvm/vmx/nested.c|3317| <<nested_vmx_check_vmentry_hw>> vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);
+	 *   - arch/x86/kvm/vmx/nested.c|4893| <<nested_vmx_get_vmcs01_guest_efer>> for (i = 0; i < vmx->msr_autoload.guest.nr; ++i) {
+	 *   - arch/x86/kvm/vmx/nested.c|4894| <<nested_vmx_get_vmcs01_guest_efer>> if (vmx->msr_autoload.guest.val[i].index == MSR_EFER)
+	 *   - arch/x86/kvm/vmx/nested.c|4895| <<nested_vmx_get_vmcs01_guest_efer>> return vmx->msr_autoload.guest.val[i].value;
+	 *   - arch/x86/kvm/vmx/nested.c|5150| <<nested_vmx_vmexit>> vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);
+	 *   - arch/x86/kvm/vmx/nested.c|5151| <<nested_vmx_vmexit>> vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);
+	 *   - arch/x86/kvm/vmx/vmx.c|986| <<clear_atomic_switch_msr>> struct msr_autoload *m = &vmx->msr_autoload;
+	 *   - arch/x86/kvm/vmx/vmx.c|1039| <<add_atomic_switch_msr>> struct msr_autoload *m = &vmx->msr_autoload;
+	 *   - arch/x86/kvm/vmx/vmx.c|4795| <<init_vmcs>> vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host.val));
+	 *   - arch/x86/kvm/vmx/vmx.c|4797| <<init_vmcs>> vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest.val));
+	 *   - arch/x86/kvm/vmx/vmx.c|6475| <<dump_vmcs>> efer_slot = vmx_find_loadstore_msr_slot(&vmx->msr_autoload.guest, MSR_EFER);
+	 *   - arch/x86/kvm/vmx/vmx.c|6480| <<dump_vmcs>> vmx->msr_autoload.guest.val[efer_slot].value);
+	 *   - arch/x86/kvm/vmx/vmx.c|6505| <<dump_vmcs>> vmx_dump_msrs("guest autoload", &vmx->msr_autoload.guest);
+	 *   - arch/x86/kvm/vmx/vmx.c|6538| <<dump_vmcs>> vmx_dump_msrs("host autoload", &vmx->msr_autoload.host);
+	 */
 	struct msr_autoload {
 		struct vmx_msrs guest;
 		struct vmx_msrs host;
@@ -356,6 +488,20 @@ struct vcpu_vmx {
 	struct pt_desc pt_desc;
 	struct lbr_desc lbr_desc;
 
+	/*
+	 * 在以下使用vcpu_vmx->shadow_msr_intercept:
+	 *   - arch/x86/kvm/vmx/vmx.c|4343| <<vmx_disable_intercept_for_msr>> clear_bit(idx, vmx->shadow_msr_intercept.read);
+	 *   - arch/x86/kvm/vmx/vmx.c|4345| <<vmx_disable_intercept_for_msr>> clear_bit(idx, vmx->shadow_msr_intercept.write);
+	 *   - arch/x86/kvm/vmx/vmx.c|4392| <<vmx_enable_intercept_for_msr>> set_bit(idx, vmx->shadow_msr_intercept.read);
+	 *   - arch/x86/kvm/vmx/vmx.c|4394| <<vmx_enable_intercept_for_msr>> set_bit(idx, vmx->shadow_msr_intercept.write);
+	 *   - arch/x86/kvm/vmx/vmx.c|4535| <<vmx_msr_filter_changed>> if (!test_bit(i, vmx->shadow_msr_intercept.read))
+	 *   - arch/x86/kvm/vmx/vmx.c|4538| <<vmx_msr_filter_changed>> if (!test_bit(i, vmx->shadow_msr_intercept.write))
+	 *   - arch/x86/kvm/vmx/vmx.c|8108| <<vmx_vcpu_create>> bitmap_fill(vmx->shadow_msr_intercept.read, MAX_POSSIBLE_PASSTHROUGH_MSRS);
+	 *   - arch/x86/kvm/vmx/vmx.c|8109| <<vmx_vcpu_create>> bitmap_fill(vmx->shadow_msr_intercept.write, MAX_POSSIBLE_PASSTHROUGH_MSRS);
+	 *
+	 * 应该就是为了vmx记录read/write哪个passthrough的msr要intercept
+	 * read/write的bitmap长度各自为16
+	 */
 	/* Save desired MSR intercept (read: pass-through) state */
 #define MAX_POSSIBLE_PASSTHROUGH_MSRS	16
 	struct {
@@ -420,6 +566,24 @@ void vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type);
 u64 vmx_get_l2_tsc_offset(struct kvm_vcpu *vcpu);
 u64 vmx_get_l2_tsc_multiplier(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|710| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|711| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|713| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|716| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|717| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/vmx.c|4374| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+ *   - arch/x86/kvm/vmx/vmx.c|4392| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4393| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4394| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4395| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4397| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4398| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|8309| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+ *   - arch/x86/kvm/vmx/vmx.c|8313| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/vmx/vmx.c|8317| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+ */
 static inline void vmx_set_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr,
 					     int type, bool value)
 {
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 1a3aaa7da..03017c986 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -216,9 +216,39 @@ struct kvm_user_return_msrs {
 	} values[KVM_MAX_NR_USER_RETURN_MSRS];
 };
 
+/*
+ * 在以下使用kvm_nr_uret_msrs:
+ *   - arch/x86/kvm/vmx/vmx.c|1575| <<vmx_prepare_switch_to_guest>> for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+ *   - arch/x86/kvm/vmx/vmx.c|8032| <<vmx_vcpu_create>> for (i = 0; i < kvm_nr_uret_msrs; ++i)
+ *   - arch/x86/kvm/x86.c|395| <<kvm_on_user_return>> for (slot = 0; slot < kvm_nr_uret_msrs; ++slot) {
+ *   - arch/x86/kvm/x86.c|421| <<kvm_add_user_return_msr>> BUG_ON(kvm_nr_uret_msrs >= KVM_MAX_NR_USER_RETURN_MSRS);
+ *   - arch/x86/kvm/x86.c|426| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+ *   - arch/x86/kvm/x86.c|427| <<kvm_add_user_return_msr>> return kvm_nr_uret_msrs++;
+ *   - arch/x86/kvm/x86.c|435| <<kvm_find_user_return_msr>> for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+ *   - arch/x86/kvm/x86.c|450| <<kvm_user_return_msr_cpu_online>> for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+ *   - arch/x86/kvm/x86.c|10159| <<__kvm_x86_vendor_init>> kvm_nr_uret_msrs = 0;
+ */
 u32 __read_mostly kvm_nr_uret_msrs;
 EXPORT_SYMBOL_GPL(kvm_nr_uret_msrs);
+/*
+ * 在以下使用kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]:
+ *   - arch/x86/kvm/x86.c|398| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+ *   - arch/x86/kvm/x86.c|426| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+ *   - arch/x86/kvm/x86.c|436| <<kvm_find_user_return_msr>> if (kvm_uret_msrs_list[i] == msr)
+ *   - arch/x86/kvm/x86.c|451| <<kvm_user_return_msr_cpu_online>> rdmsrl_safe(kvm_uret_msrs_list[i], &value);
+ *   - arch/x86/kvm/x86.c|473| <<kvm_set_user_return_msr>> err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
+ */
 static u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];
+/*
+ * 在以下使用user_return_msrs:
+ *   - arch/x86/kvm/x86.c|446| <<kvm_user_return_msr_cpu_online>> struct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);
+ *   - arch/x86/kvm/x86.c|467| <<kvm_set_user_return_msr>> struct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);
+ *   - arch/x86/kvm/x86.c|490| <<drop_user_return_notifiers>> struct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);
+ *   - arch/x86/kvm/x86.c|10153| <<__kvm_x86_vendor_init>> user_return_msrs = alloc_percpu(struct kvm_user_return_msrs);
+ *   - arch/x86/kvm/x86.c|10154| <<__kvm_x86_vendor_init>> if (!user_return_msrs) {
+ *   - arch/x86/kvm/x86.c|10238| <<__kvm_x86_vendor_init>> free_percpu(user_return_msrs);
+ *   - arch/x86/kvm/x86.c|10278| <<kvm_x86_vendor_exit>> free_percpu(user_return_msrs);
+ */
 static struct kvm_user_return_msrs __percpu *user_return_msrs;
 
 #define KVM_SUPPORTED_XCR0     (XFEATURE_MASK_FP | XFEATURE_MASK_SSE \
@@ -322,6 +352,14 @@ static struct kmem_cache *x86_emulator_cache;
  * When called, it means the previous get/set msr reached an invalid msr.
  * Return true if we want to ignore/silent this failed msr access.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1769| <<do_get_msr_feature>> if (kvm_msr_ignored_check(index, 0, false))
+ *   - arch/x86/kvm/x86.c|1967| <<kvm_set_msr_ignored_check>> if (kvm_msr_ignored_check(index, data, true))
+ *   - arch/x86/kvm/x86.c|2025| <<kvm_get_msr_ignored_check>> if (kvm_msr_ignored_check(index, 0, false))
+ *
+ * 如果设置了ignore_msrs, 返回true, 否则返回false
+ */
 static bool kvm_msr_ignored_check(u32 msr, u64 data, bool write)
 {
 	const char *op = write ? "wrmsr" : "rdmsr";
@@ -352,6 +390,13 @@ static struct kmem_cache *kvm_alloc_emulator_cache(void)
 
 static int emulator_fix_hypercall(struct x86_emulate_ctxt *ctxt);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|972| <<kvm_post_set_cr0>> kvm_async_pf_hash_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|3550| <<kvm_pv_enable_async_pf>> kvm_async_pf_hash_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|12308| <<kvm_arch_vcpu_create>> kvm_async_pf_hash_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|12460| <<kvm_vcpu_reset>> kvm_async_pf_hash_reset(vcpu);
+ */
 static inline void kvm_async_pf_hash_reset(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -359,6 +404,16 @@ static inline void kvm_async_pf_hash_reset(struct kvm_vcpu *vcpu)
 		vcpu->arch.apf.gfns[i] = ~0;
 }
 
+/*
+ * 在以下使用kvm_on_user_return():
+ *   - arch/x86/kvm/x86.c|479| <<kvm_set_user_return_msr>> msrs->urn.on_user_return = kvm_on_user_return;
+ *   - arch/x86/kvm/x86.c|493| <<drop_user_return_notifiers>> kvm_on_user_return(&msrs->urn);
+ *
+ * struct user_return_notifier {
+ *     void (*on_user_return)(struct user_return_notifier *urn);
+ *     struct hlist_node link;
+ * };
+ */
 static void kvm_on_user_return(struct user_return_notifier *urn)
 {
 	unsigned slot;
@@ -379,6 +434,23 @@ static void kvm_on_user_return(struct user_return_notifier *urn)
 	local_irq_restore(flags);
 	for (slot = 0; slot < kvm_nr_uret_msrs; ++slot) {
 		values = &msrs->values[slot];
+		/*
+		 * 在以下使用curr:
+		 *   - arch/x86/kvm/x86.c|437| <<kvm_on_user_return>> if (values->host != values->curr) {
+		 *   - arch/x86/kvm/x86.c|439| <<kvm_on_user_return>> values->curr = values->host;
+		 *   - arch/x86/kvm/x86.c|547| <<kvm_user_return_msr_cpu_online>> msrs->values[i].curr = value;
+		 *   - arch/x86/kvm/x86.c|582| <<kvm_set_user_return_msr>> if (value == msrs->values[slot].curr)
+		 *   - arch/x86/kvm/x86.c|588| <<kvm_set_user_return_msr>> msrs->values[slot].curr = value;
+		 *
+		 * 在以下使用host:
+		 *   - arch/x86/kvm/x86.c|437| <<kvm_on_user_return>> if (values->host != values->curr) {
+		 *   - arch/x86/kvm/x86.c|438| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+		 *   - arch/x86/kvm/x86.c|439| <<kvm_on_user_return>> values->curr = values->host;
+		 *   - arch/x86/kvm/x86.c|546| <<kvm_user_return_msr_cpu_online>> msrs->values[i].host = value;
+		 *   - arch/x86/kvm/x86.c|581| <<kvm_set_user_return_msr>> value = (value & mask) | (msrs->values[slot].host & ~mask);
+		 *
+		 * 似乎host的启动了就不变的
+		 */
 		if (values->host != values->curr) {
 			wrmsrl(kvm_uret_msrs_list[slot], values->host);
 			values->curr = values->host;
@@ -386,6 +458,12 @@ static void kvm_on_user_return(struct user_return_notifier *urn)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|423| <<kvm_add_user_return_msr>> if (kvm_probe_user_return_msr(msr))
+ *
+ * 就是先读再写相同的值, 为了测试???
+ */
 static int kvm_probe_user_return_msr(u32 msr)
 {
 	u64 val;
@@ -401,23 +479,57 @@ static int kvm_probe_user_return_msr(u32 msr)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5194| <<svm_hardware_setup>> tsc_aux_uret_slot = kvm_add_user_return_msr(MSR_TSC_AUX);
+ *   - arch/x86/kvm/vmx/vmx.c|8961| <<vmx_setup_user_return_msrs>> kvm_add_user_return_msr(vmx_uret_msrs_list[i]);
+ *
+ * 把一个msr(u32)加入数组kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]
+ */
 int kvm_add_user_return_msr(u32 msr)
 {
 	BUG_ON(kvm_nr_uret_msrs >= KVM_MAX_NR_USER_RETURN_MSRS);
 
+	/*
+	 * 就是先读再写相同的值, 为了测试???
+	 */
 	if (kvm_probe_user_return_msr(msr))
 		return -1;
 
+	/*
+	 * 在以下使用kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]:
+	 *   - arch/x86/kvm/x86.c|398| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+	 *   - arch/x86/kvm/x86.c|426| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+	 *   - arch/x86/kvm/x86.c|436| <<kvm_find_user_return_msr>> if (kvm_uret_msrs_list[i] == msr)
+	 *   - arch/x86/kvm/x86.c|451| <<kvm_user_return_msr_cpu_online>> rdmsrl_safe(kvm_uret_msrs_list[i], &value);
+	 *   - arch/x86/kvm/x86.c|473| <<kvm_set_user_return_msr>> err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
+	 */
 	kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
 	return kvm_nr_uret_msrs++;
 }
 EXPORT_SYMBOL_GPL(kvm_add_user_return_msr);
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/kvm_host.h|2572| <<kvm_is_supported_user_return_msr>> return kvm_find_user_return_msr(msr) >= 0;
+ *   - arch/x86/kvm/vmx/vmx.c|733| <<vmx_find_uret_msr>> i = kvm_find_user_return_msr(msr);
+ *   - arch/x86/kvm/vmx/vmx.c|1409| <<update_transition_efer>> i = kvm_find_user_return_msr(MSR_EFER);
+ *
+ * 返回msr(u32)在数组kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]的index
+ */
 int kvm_find_user_return_msr(u32 msr)
 {
 	int i;
 
 	for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+		/*
+		 * 在以下使用kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]:
+		 *   - arch/x86/kvm/x86.c|398| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+		 *   - arch/x86/kvm/x86.c|426| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+		 *   - arch/x86/kvm/x86.c|436| <<kvm_find_user_return_msr>> if (kvm_uret_msrs_list[i] == msr)
+		 *   - arch/x86/kvm/x86.c|451| <<kvm_user_return_msr_cpu_online>> rdmsrl_safe(kvm_uret_msrs_list[i], &value);
+		 *   - arch/x86/kvm/x86.c|473| <<kvm_set_user_return_msr>> err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
+		 */
 		if (kvm_uret_msrs_list[i] == msr)
 			return i;
 	}
@@ -425,6 +537,10 @@ int kvm_find_user_return_msr(u32 msr)
 }
 EXPORT_SYMBOL_GPL(kvm_find_user_return_msr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12990| <<kvm_arch_hardware_enable>> kvm_user_return_msr_cpu_online();
+ */
 static void kvm_user_return_msr_cpu_online(void)
 {
 	unsigned int cpu = smp_processor_id();
@@ -432,6 +548,16 @@ static void kvm_user_return_msr_cpu_online(void)
 	u64 value;
 	int i;
 
+	/*
+	 * struct kvm_user_return_msrs {
+	 *     struct user_return_notifier urn;
+	 *     bool registered;
+	 *     struct kvm_user_return_msr_values {
+	 *         u64 host;
+	 *         u64 curr;
+	 *     } values[KVM_MAX_NR_USER_RETURN_MSRS];
+	 * };
+	 */
 	for (i = 0; i < kvm_nr_uret_msrs; ++i) {
 		rdmsrl_safe(kvm_uret_msrs_list[i], &value);
 		msrs->values[i].host = value;
@@ -439,12 +565,36 @@ static void kvm_user_return_msr_cpu_online(void)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1543| <<svm_prepare_switch_to_guest>> kvm_set_user_return_msr(tsc_aux_uret_slot, svm->tsc_aux, -1ull);
+ *   - arch/x86/kvm/svm/svm.c|3113| <<svm_set_msr>> ret = kvm_set_user_return_msr(tsc_aux_uret_slot, data, -1ull);
+ *   - arch/x86/kvm/vmx/vmx.c|747| <<vmx_set_guest_uret_msr>> ret = kvm_set_user_return_msr(slot, data, msr->mask);
+ *   - arch/x86/kvm/vmx/vmx.c|1579| <<vmx_prepare_switch_to_guest>> kvm_set_user_return_msr(i, vmx->guest_uret_msrs[i].data, vmx->guest_uret_msrs[i].mask);
+ *
+ * 针对当前的CPU, 把给VM准备的msr的value写入硬件
+ * 注册return to user的callback, 用来到时候改写
+ * per_cpu_ptr(user_return_msrs, cpu)->values[slot].curr = value
+ */
 int kvm_set_user_return_msr(unsigned slot, u64 value, u64 mask)
 {
 	unsigned int cpu = smp_processor_id();
+	/*
+	 * struct kvm_user_return_msrs {
+	 *     struct user_return_notifier urn;
+	 *     bool registered;
+	 *     struct kvm_user_return_msr_values {
+	 *         u64 host;
+	 *         u64 curr;
+	 *     } values[KVM_MAX_NR_USER_RETURN_MSRS];
+	 * };
+	 */
 	struct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);
 	int err;
 
+	/*
+	 * value怎么来的?????
+	 */
 	value = (value & mask) | (msrs->values[slot].host & ~mask);
 	if (value == msrs->values[slot].curr)
 		return 0;
@@ -462,6 +612,10 @@ int kvm_set_user_return_msr(unsigned slot, u64 value, u64 mask)
 }
 EXPORT_SYMBOL_GPL(kvm_set_user_return_msr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13079| <<kvm_arch_hardware_disable>> drop_user_return_notifiers();
+ */
 static void drop_user_return_notifiers(void)
 {
 	unsigned int cpu = smp_processor_id();
@@ -637,6 +791,11 @@ static void kvm_queue_exception_vmexit(struct kvm_vcpu *vcpu, unsigned int vecto
 	ex->payload = payload;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5662| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_leave_nested(vcpu);
+ *   - arch/x86/kvm/x86.c|12648| <<kvm_vcpu_reset>> kvm_leave_nested(vcpu);
+ */
 /* Forcibly leave the nested mode in cases like a vCPU reset */
 static void kvm_leave_nested(struct kvm_vcpu *vcpu)
 {
@@ -793,6 +952,15 @@ void kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault)
 					fault->address);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|851| <<FNAME(page_fault)>> kvm_inject_emulated_page_fault(vcpu, &walker.fault);
+ *   - arch/x86/kvm/vmx/sgx.c|84| <<sgx_gva_to_gpa>> kvm_inject_emulated_page_fault(vcpu, &ex);
+ *   - arch/x86/kvm/vmx/sgx.c|133| <<sgx_inject_fault>> kvm_inject_emulated_page_fault(vcpu, &ex);
+ *   - arch/x86/kvm/vmx/sgx.c|238| <<handle_encls_ecreate>> kvm_inject_emulated_page_fault(vcpu, &ex);
+ *   - arch/x86/kvm/x86.c|8576| <<inject_emulated_exception>> kvm_inject_emulated_page_fault(vcpu, &ctxt->exception);
+ *   - arch/x86/kvm/x86.c|13732| <<kvm_handle_memory_failure>> kvm_inject_emulated_page_fault(vcpu, e);
+ */
 void kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
 				    struct x86_exception *fault)
 {
@@ -815,8 +983,20 @@ void kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
 }
 EXPORT_SYMBOL_GPL(kvm_inject_emulated_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1345| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|5123| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+ */
 void kvm_inject_nmi(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->nmi_queued:
+	 *   - arch/x86/kvm/x86.c|820| <<kvm_inject_nmi>> atomic_inc(&vcpu->arch.nmi_queued);
+	 *   - arch/x86/kvm/x86.c|5412| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
+	 *   - arch/x86/kvm/x86.c|10439| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+	 *   - arch/x86/kvm/x86.c|12193| <<kvm_vcpu_reset>> atomic_set(&vcpu->arch.nmi_queued, 0);
+	 */
 	atomic_inc(&vcpu->arch.nmi_queued);
 	kvm_make_request(KVM_REQ_NMI, vcpu);
 }
@@ -1263,6 +1443,15 @@ static void kvm_invalidate_pcid(struct kvm_vcpu *vcpu, unsigned long pcid)
 	kvm_mmu_free_roots(vcpu->kvm, mmu, roots_to_free);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1325| <<global>> EXPORT_SYMBOL_GPL(kvm_set_cr3);
+ *   - arch/x86/kvm/smm.c|593| <<rsm_enter_protected_mode>> bad = kvm_set_cr3(vcpu, cr3);
+ *   - arch/x86/kvm/smm.c|615| <<rsm_enter_protected_mode>> bad = kvm_set_cr3(vcpu, cr3 | pcid);
+ *   - arch/x86/kvm/svm/svm.c|2658| <<cr_interception>> err = kvm_set_cr3(vcpu, val);
+ *   - arch/x86/kvm/vmx/vmx.c|5561| <<handle_cr>> err = kvm_set_cr3(vcpu, val);
+ *   - arch/x86/kvm/x86.c|8220| <<emulator_set_cr>> res = kvm_set_cr3(vcpu, val);
+ */
 int kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)
 {
 	bool skip_tlb_flush = false;
@@ -1290,6 +1479,14 @@ int kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)
 	if (is_pae_paging(vcpu) && !load_pdptrs(vcpu, cr3))
 		return 1;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|5310| <<kvm_init_shadow_npt_mmu>> kvm_mmu_new_pgd(vcpu, nested_cr3);
+	 *   - arch/x86/kvm/mmu/mmu.c|5364| <<kvm_init_shadow_ept_mmu>> kvm_mmu_new_pgd(vcpu, new_eptp);
+	 *   - arch/x86/kvm/svm/nested.c|536| <<nested_svm_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+	 *   - arch/x86/kvm/vmx/nested.c|1110| <<nested_vmx_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+	 *   - arch/x86/kvm/x86.c|1306| <<kvm_set_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+	 */
 	if (cr3 != kvm_read_cr3(vcpu))
 		kvm_mmu_new_pgd(vcpu, cr3);
 
@@ -1678,6 +1875,11 @@ static u64 kvm_get_arch_capabilities(void)
 	return data;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1750| <<do_get_msr_feature>> r = kvm_get_msr_feature(&msr);
+ *   - arch/x86/kvm/x86.c|7435| <<kvm_probe_feature_msr>> if (kvm_get_msr_feature(&msr))
+ */
 static int kvm_get_msr_feature(struct kvm_msr_entry *msr)
 {
 	switch (msr->index) {
@@ -1696,6 +1898,10 @@ static int kvm_get_msr_feature(struct kvm_msr_entry *msr)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5047| <<kvm_arch_dev_ioctl(KVM_GET_MSRS)>> r = msr_io(NULL, argp, do_get_msr_feature, 1);
+ */
 static int do_get_msr_feature(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 {
 	struct kvm_msr_entry msr;
@@ -1788,8 +1994,22 @@ void kvm_enable_efer_bits(u64 mask)
 }
 EXPORT_SYMBOL_GPL(kvm_enable_efer_bits);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|840| <<set_msr_interception_bitmap>> if (read && !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ))
+ *   - arch/x86/kvm/svm/svm.c|843| <<set_msr_interception_bitmap>> if (write && !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE))
+ *   - arch/x86/kvm/vmx/vmx.c|4289| <<vmx_disable_intercept_for_msr>> !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ)) {
+ *   - arch/x86/kvm/vmx/vmx.c|4295| <<vmx_disable_intercept_for_msr>> !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE)) {
+ *   - arch/x86/kvm/x86.c|2045| <<kvm_get_msr_with_filter>> if (!kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_READ))
+ *   - arch/x86/kvm/x86.c|2052| <<kvm_set_msr_with_filter>> if (!kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_WRITE))
+ */
 bool kvm_msr_allowed(struct kvm_vcpu *vcpu, u32 index, u32 type)
 {
+	/*
+	 * type分为:
+	 * #define KVM_MSR_FILTER_READ  (1 << 0)
+	 * #define KVM_MSR_FILTER_WRITE (1 << 1)
+	 */
 	struct kvm_x86_msr_filter *msr_filter;
 	struct msr_bitmap_range *ranges;
 	struct kvm *kvm = vcpu->kvm;
@@ -1803,6 +2023,30 @@ bool kvm_msr_allowed(struct kvm_vcpu *vcpu, u32 index, u32 type)
 
 	idx = srcu_read_lock(&kvm->srcu);
 
+	/*
+	 * 在以下使用kvm_arch->msr_filter:
+	 *   - arch/x86/kvm/x86.c|1871| <<kvm_msr_allowed>> msr_filter = srcu_dereference(kvm->arch.msr_filter, &kvm->srcu);
+	 *   - arch/x86/kvm/x86.c|1872| <<kvm_msr_allowed>> if (!msr_filter) {
+	 *   - arch/x86/kvm/x86.c|1877| <<kvm_msr_allowed>> allowed = msr_filter->default_allow;
+	 *   - arch/x86/kvm/x86.c|1878| <<kvm_msr_allowed>> ranges = msr_filter->ranges;
+	 *   - arch/x86/kvm/x86.c|1880| <<kvm_msr_allowed>> for (i = 0; i < msr_filter->count; i++) {
+	 *   - arch/x86/kvm/x86.c|6916| <<kvm_alloc_msr_filter>> struct kvm_x86_msr_filter *msr_filter;
+	 *   - arch/x86/kvm/x86.c|6918| <<kvm_alloc_msr_filter>> msr_filter = kzalloc(sizeof(*msr_filter), GFP_KERNEL_ACCOUNT);
+	 *   - arch/x86/kvm/x86.c|6919| <<kvm_alloc_msr_filter>> if (!msr_filter)
+	 *
+	 * struct msr_bitmap_range {
+	 *     u32 flags;
+	 *     u32 nmsrs;
+	 *     u32 base;
+	 *     unsigned long *bitmap;
+	 * };
+	 *
+	 * struct kvm_x86_msr_filter {
+	 *     u8 count;
+	 *     bool default_allow:1;
+	 *     struct msr_bitmap_range ranges[16];
+	 * };
+	 */
 	msr_filter = srcu_dereference(kvm->arch.msr_filter, &kvm->srcu);
 	if (!msr_filter) {
 		allowed = true;
@@ -1899,11 +2143,24 @@ static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 	return static_call(kvm_x86_set_msr)(vcpu, &msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2092| <<kvm_set_msr_with_filter>> return kvm_set_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|2103| <<kvm_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|2373| <<do_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, *data, true);
+ *
+ * 调用__kvm_set_msr(), 成功了没事
+ * 如果返回KVM_MSR_RET_INVALID, 要看ignore_msrs
+ *     如果设置了ignore_msrs, 返回0, 否则返回原来的ret
+ */
 static int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,
 				     u32 index, u64 data, bool host_initiated)
 {
 	int ret = __kvm_set_msr(vcpu, index, data, host_initiated);
 
+	/*
+	 * 如果设置了ignore_msrs, 返回true, 否则返回false
+	 */
 	if (ret == KVM_MSR_RET_INVALID)
 		if (kvm_msr_ignored_check(index, data, true))
 			ret = 0;
@@ -1917,6 +2174,11 @@ static int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1553| <<kvm_cpuid>> if (!__kvm_get_msr(vcpu, MSR_IA32_TSX_CTRL, &data, true) &&
+ *   - arch/x86/kvm/x86.c|2004| <<kvm_get_msr_ignored_check>> int ret = __kvm_get_msr(vcpu, index, data, host_initiated);
+ */
 int __kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data,
 		  bool host_initiated)
 {
@@ -1944,6 +2206,12 @@ int __kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2020| <<kvm_get_msr_with_filter>> return kvm_get_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|2032| <<kvm_get_msr>> return kvm_get_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|2275| <<do_get_msr>> return kvm_get_msr_ignored_check(vcpu, index, data, true);
+ */
 static int kvm_get_msr_ignored_check(struct kvm_vcpu *vcpu,
 				     u32 index, u64 *data, bool host_initiated)
 {
@@ -1959,6 +2227,11 @@ static int kvm_get_msr_ignored_check(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2112| <<kvm_emulate_rdmsr>> r = kvm_get_msr_with_filter(vcpu, ecx, &data);
+ *   - arch/x86/kvm/x86.c|8540| <<emulator_get_msr_with_filter>> r = kvm_get_msr_with_filter(vcpu, msr_index, pdata);
+ */
 static int kvm_get_msr_with_filter(struct kvm_vcpu *vcpu, u32 index, u64 *data)
 {
 	if (!kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_READ))
@@ -2015,6 +2288,11 @@ static int complete_fast_rdmsr(struct kvm_vcpu *vcpu)
 	return complete_fast_msr_access(vcpu);
 }
 
+/*
+ * #define KVM_MSR_EXIT_REASON_INVAL       (1 << 0)
+ * #define KVM_MSR_EXIT_REASON_UNKNOWN     (1 << 1)
+ * #define KVM_MSR_EXIT_REASON_FILTER      (1 << 2)
+ */
 static u64 kvm_msr_reason(int r)
 {
 	switch (r) {
@@ -2027,6 +2305,13 @@ static u64 kvm_msr_reason(int r)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2148| <<kvm_emulate_rdmsr>> if (kvm_msr_user_space(vcpu, ecx, KVM_EXIT_X86_RDMSR, 0, complete_fast_rdmsr, r))
+ *   - arch/x86/kvm/x86.c|2170| <<kvm_emulate_wrmsr>> if (kvm_msr_user_space(vcpu, ecx, KVM_EXIT_X86_WRMSR, data, complete_fast_msr_access, r))
+ *   - arch/x86/kvm/x86.c|8605| <<emulator_get_msr_with_filter>> if (kvm_msr_user_space(vcpu, msr_index, KVM_EXIT_X86_RDMSR, 0, complete_emulated_rdmsr, r))
+ *   - arch/x86/kvm/x86.c|8628| <<emulator_set_msr_with_filter>> if (kvm_msr_user_space(vcpu, msr_index, KVM_EXIT_X86_WRMSR, data, complete_emulated_msr_access, r))
+ */
 static int kvm_msr_user_space(struct kvm_vcpu *vcpu, u32 index,
 			      u32 exit_reason, u64 data,
 			      int (*completion)(struct kvm_vcpu *vcpu),
@@ -3475,13 +3760,50 @@ static int set_msr_mce(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3548| <<kvm_pv_enable_async_pf>> if (!kvm_pv_async_pf_enabled(vcpu)) {
+ *   - arch/x86/kvm/x86.c|13584| <<kvm_can_deliver_async_pf>> if (!kvm_pv_async_pf_enabled(vcpu))
+ *   - arch/x86/kvm/x86.c|13694| <<kvm_arch_async_page_present>> kvm_pv_async_pf_enabled(vcpu) &&
+ *   - arch/x86/kvm/x86.c|13743| <<kvm_arch_can_dequeue_async_page_present>> if (!kvm_pv_async_pf_enabled(vcpu))
+ *
+ * 查看msr是否设置了KVM_ASYNC_PF_ENABLED | KVM_ASYNC_PF_DELIVERY_AS_INT(两个都要)
+ * vcpu->arch.apf.msr_en_val
+ */
 static inline bool kvm_pv_async_pf_enabled(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * #define KVM_ASYNC_PF_ENABLED                    (1 << 0)
+	 * #define KVM_ASYNC_PF_SEND_ALWAYS                (1 << 1)
+	 * #define KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT      (1 << 2)
+	 * #define KVM_ASYNC_PF_DELIVERY_AS_INT            (1 << 3)
+	 */
 	u64 mask = KVM_ASYNC_PF_ENABLED | KVM_ASYNC_PF_DELIVERY_AS_INT;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *        struct {
+	 *            bool halted;
+	 *            gfn_t gfns[ASYNC_PF_PER_VCPU];
+	 *            struct gfn_to_hva_cache data;
+	 *            u64 msr_en_val; // MSR_KVM_ASYNC_PF_EN
+	 *            u64 msr_int_val; // MSR_KVM_ASYNC_PF_INT
+	 *            u16 vec;
+	 *            u32 id;
+	 *            bool send_user_only;
+	 *            u32 host_apf_flags;
+	 *            bool delivery_as_pf_vmexit;
+	 *            bool pageready_pending;
+	 * } apf;
+	 */
 	return (vcpu->arch.apf.msr_en_val & mask) == mask;
 }
 
+/*
+ * 处理MSR_KVM_ASYNC_PF_EN:
+ *   - arch/x86/kvm/x86.c|3992| <<kvm_set_msr_common(MSR_KVM_ASYNC_PF_EN)>> if (kvm_pv_enable_async_pf(vcpu, data))
+ */
 static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)
 {
 	gpa_t gpa = data & ~0x3f;
@@ -3503,8 +3825,26 @@ static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)
 
 	vcpu->arch.apf.msr_en_val = data;
 
+	/*
+	 * #define KVM_ASYNC_PF_ENABLED                    (1 << 0)
+	 * #define KVM_ASYNC_PF_SEND_ALWAYS                (1 << 1)
+	 * #define KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT      (1 << 2)
+	 * #define KVM_ASYNC_PF_DELIVERY_AS_INT            (1 << 3)
+	 *
+	 * 查看msr是否设置了KVM_ASYNC_PF_ENABLED | KVM_ASYNC_PF_DELIVERY_AS_INT(两个都要)
+	 * vcpu->arch.apf.msr_en_val
+	 *
+	 * 也可能包含其他的, 没关系
+	 */
 	if (!kvm_pv_async_pf_enabled(vcpu)) {
 		kvm_clear_async_pf_completion_queue(vcpu);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|972| <<kvm_post_set_cr0>> kvm_async_pf_hash_reset(vcpu);
+		 *   - arch/x86/kvm/x86.c|3550| <<kvm_pv_enable_async_pf>> kvm_async_pf_hash_reset(vcpu);
+		 *   - arch/x86/kvm/x86.c|12308| <<kvm_arch_vcpu_create>> kvm_async_pf_hash_reset(vcpu);
+		 *   - arch/x86/kvm/x86.c|12460| <<kvm_vcpu_reset>> kvm_async_pf_hash_reset(vcpu);
+		 */
 		kvm_async_pf_hash_reset(vcpu);
 		return 0;
 	}
@@ -3513,14 +3853,33 @@ static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)
 					sizeof(u64)))
 		return 1;
 
+	/*
+	 * 在以下使用apf.send_user_only:
+	 *   - arch/x86/kvm/x86.c|3558| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.send_user_only = !(data & KVM_ASYNC_PF_SEND_ALWAYS);
+	 *   - arch/x86/kvm/x86.c|13587| <<kvm_can_deliver_async_pf>> if (vcpu->arch.apf.send_user_only && static_call(kvm_x86_get_cpl)(vcpu) == 0)
+	 */
 	vcpu->arch.apf.send_user_only = !(data & KVM_ASYNC_PF_SEND_ALWAYS);
+	/*
+	 * 在以下使用apf.delivery_as_pf_vmexit:
+	 *   - arch/x86/kvm/x86.c|3559| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.delivery_as_pf_vmexit = data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;
+	 *   - arch/x86/kvm/x86.c|13596| <<kvm_can_deliver_async_pf>> return vcpu->arch.apf.delivery_as_pf_vmexit;
+	 *
+	 * 似乎是nested用的
+	 */
 	vcpu->arch.apf.delivery_as_pf_vmexit = data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;
 
+	/*
+	 * 只在此处调用
+	 */
 	kvm_async_pf_wakeup_all(vcpu);
 
 	return 0;
 }
 
+/*
+ * called by (MSR_KVM_ASYNC_PF_INT):
+ *   - arch/x86/kvm/x86.c|3991| <<kvm_set_msr_common(MSR_KVM_ASYNC_PF_INT)>> if (kvm_pv_enable_async_pf_int(vcpu, data))
+ */
 static int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)
 {
 	/* Bits 8-63 are reserved */
@@ -3546,6 +3905,10 @@ static void kvmclock_reset(struct kvm_vcpu *vcpu)
 static void kvm_vcpu_flush_tlb_all(struct kvm_vcpu *vcpu)
 {
 	++vcpu->stat.tlb_flush;
+	/*
+	 * vmx_flush_tlb_all()
+	 * svm_flush_tlb_all()
+	 */
 	static_call(kvm_x86_flush_tlb_all)(vcpu);
 
 	/* Flushing all ASIDs flushes the current ASID... */
@@ -3740,6 +4103,17 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	case MSR_IA32_PERF_CAPABILITIES:
 		if (!msr_info->host_initiated)
 			return 1;
+		/*
+		 * 在以下使用kvm_caps->supported_perf_cap:
+		 *   - arch/x86/kvm/svm/svm.c|5081| <<svm_set_cpu_caps>> kvm_caps.supported_perf_cap = 0;
+		 *   - arch/x86/kvm/vmx/vmx.c|2153| <<vmx_get_supported_debugctl>> if ((kvm_caps.supported_perf_cap & PMU_CAP_LBR_FMT) &&
+		 *   - arch/x86/kvm/vmx/vmx.c|2433| <<vmx_set_msr(MSR_IA32_PERF_CAPABILITIES)>> (kvm_caps.supported_perf_cap & PMU_CAP_LBR_FMT))
+		 *   - arch/x86/kvm/vmx/vmx.c|2440| <<vmx_set_msr(MSR_IA32_PERF_CAPABILITIES)>> (kvm_caps.supported_perf_cap & PERF_CAP_PEBS_MASK))
+		 *   - arch/x86/kvm/vmx/vmx.c|8059| <<vmx_set_cpu_caps>> kvm_caps.supported_perf_cap = vmx_get_perf_capabilities();
+		 *   - arch/x86/kvm/x86.c|1733| <<kvm_get_msr_feature>> msr->data = kvm_caps.supported_perf_cap;
+		 *   - arch/x86/kvm/x86.c|3866| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> if (data & ~kvm_caps.supported_perf_cap)
+		 *   - arch/x86/kvm/x86.c|12429| <<kvm_arch_vcpu_create>> vcpu->arch.perf_capabilities = kvm_caps.supported_perf_cap;
+		 */
 		if (data & ~kvm_caps.supported_perf_cap)
 			return 1;
 
@@ -3953,7 +4327,27 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		if (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))
 			return 1;
 		if (data & 0x1) {
+			/*
+			 * 在以下使用apf.pageready_pending:
+			 *   - arch/x86/kvm/x86.c|3998| <<kvm_set_msr_common(MSR_KVM_ASYNC_PF_ACK)>> vcpu->arch.apf.pageready_pending = false;
+			 *   - arch/x86/kvm/x86.c|13602| <<kvm_arch_async_page_present>> vcpu->arch.apf.pageready_pending = true;
+			 *   - arch/x86/kvm/x86.c|13618| <<kvm_arch_async_page_present_queued>> if (!vcpu->arch.apf.pageready_pending)
+			 */
 			vcpu->arch.apf.pageready_pending = false;
+			/*
+			 * 在以下使用KVM_REQ_APF_READY:
+			 *   - arch/x86/kvm/lapic.c|503| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+			 *   - arch/x86/kvm/lapic.c|2573| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+			 *   - arch/x86/kvm/x86.c|11001| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+			 *   - arch/x86/kvm/x86.c|13617| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+			 *
+			 * 处理函数是kvm_check_async_pf_completion()
+			 *
+			 * called by:
+			 *   - arch/s390/kvm/kvm-s390.c|4686| <<vcpu_pre_run>> kvm_check_async_pf_completion(vcpu);
+			 *   - arch/x86/kvm/x86.c|3999| <<kvm_set_msr_common(MSR_KVM_ASYNC_PF_ACK)>> kvm_check_async_pf_completion(vcpu);
+			 *   - arch/x86/kvm/x86.c|11002| <<vcpu_enter_guest(KVM_REQ_APF_READY)>> kvm_check_async_pf_completion(vcpu);
+			 */
 			kvm_check_async_pf_completion(vcpu);
 		}
 		break;
@@ -4152,6 +4546,13 @@ static int get_msr_mce(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata, bool host)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2916| <<svm_get_msr(MSR_F15H_IC_CFG)>> return kvm_get_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/svm/svm.c|2929| <<svm_get_msr(default)>> return kvm_get_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2005| <<vmx_get_msr(MSR_EFER)>> return kvm_get_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2129| <<vmx_get_msr>> return kvm_get_msr_common(vcpu, msr_info);
+ */
 int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	switch (msr_info->index) {
@@ -4204,9 +4605,49 @@ int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		msr_info->data = vcpu->arch.arch_capabilities;
 		break;
 	case MSR_IA32_PERF_CAPABILITIES:
+		/*
+		 * 使用X86_FEATURE_PDCM的例子.
+		 * #define X86_FEATURE_MWAIT               KVM_X86_CPU_FEATURE(0x1, 0, ECX, 3)
+		 * #define X86_FEATURE_VMX                 KVM_X86_CPU_FEATURE(0x1, 0, ECX, 5)
+		 * #define X86_FEATURE_SMX                 KVM_X86_CPU_FEATURE(0x1, 0, ECX, 6)
+		 * #define X86_FEATURE_PDCM                KVM_X86_CPU_FEATURE(0x1, 0, ECX, 15)
+		 * #define X86_FEATURE_PCID                KVM_X86_CPU_FEATURE(0x1, 0, ECX, 17)
+		 * #define X86_FEATURE_X2APIC              KVM_X86_CPU_FEATURE(0x1, 0, ECX, 21)
+		 *
+		 *    feature information (1/ecx):
+		 *       PNI/SSE3: Prescott New Instructions     = true
+		 *       PCLMULDQ instruction                    = true
+		 *       DTES64: 64-bit debug store              = true
+		 *       MONITOR/MWAIT                           = true
+		 *       CPL-qualified debug store               = true
+		 *       VMX: virtual machine extensions         = true
+		 *       SMX: safer mode extensions              = true
+		 *       Enhanced Intel SpeedStep Technology     = true
+		 *       TM2: thermal monitor 2                  = true
+		 *       SSSE3 extensions                        = true
+		 *       context ID: adaptive or shared L1 data  = false
+		 *       SDBG: IA32_DEBUG_INTERFACE              = true
+		 *       FMA instruction                         = true
+		 *       CMPXCHG16B instruction                  = true
+		 *       xTPR disable                            = true
+		 *       PDCM: perfmon and debug                 = true
+		 *       PCID: process context identifiers       = true
+		 *       DCA: direct cache access                = false
+		 *       SSE4.1 extensions                       = true
+		 *       SSE4.2 extensions                       = true
+		 *       x2APIC: extended xAPIC support          = true
+		 */
 		if (!msr_info->host_initiated &&
 		    !guest_cpuid_has(vcpu, X86_FEATURE_PDCM))
 			return 1;
+		/*
+		 * 在以下使用kvm_vcpu_arch->perf_capabilities:
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|183| <<vcpu_get_perf_capabilities>> return vcpu->arch.perf_capabilities;
+		 *   - arch/x86/kvm/x86.c|3874| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> if (vcpu->arch.perf_capabilities == data)
+		 *   - arch/x86/kvm/x86.c|3877| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> vcpu->arch.perf_capabilities = data;
+		 *   - arch/x86/kvm/x86.c|4353| <<kvm_get_msr_common(MSR_IA32_PERF_CAPABILITIES)>> msr_info->data = vcpu->arch.perf_capabilities;
+		 *   - arch/x86/kvm/x86.c|12397| <<kvm_arch_vcpu_create>> vcpu->arch.perf_capabilities = kvm_caps.supported_perf_cap;
+		 */
 		msr_info->data = vcpu->arch.perf_capabilities;
 		break;
 	case MSR_IA32_POWER_CTL:
@@ -5246,6 +5687,11 @@ static int kvm_vcpu_ioctl_x86_set_mce(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5953| <<kvm_arch_vcpu_ioctl(KVM_GET_VCPU_EVENTS)>> kvm_vcpu_ioctl_x86_get_vcpu_events(vcpu, &events);
+ *   - arch/x86/kvm/x86.c|12059| <<store_regs>> kvm_vcpu_ioctl_x86_get_vcpu_events(vcpu, &vcpu->run->s.regs.events);
+ */
 static void kvm_vcpu_ioctl_x86_get_vcpu_events(struct kvm_vcpu *vcpu,
 					       struct kvm_vcpu_events *events)
 {
@@ -5339,6 +5785,11 @@ static void kvm_vcpu_ioctl_x86_get_vcpu_events(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5934| <<kvm_arch_vcpu_ioctl(KVM_SET_VCPU_EVENTS)>> r = kvm_vcpu_ioctl_x86_set_vcpu_events(vcpu, &events);
+ *   - arch/x86/kvm/x86.c|11957| <<sync_regs>> if (kvm_vcpu_ioctl_x86_set_vcpu_events(vcpu, &events))
+ */
 static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 					      struct kvm_vcpu_events *events)
 {
@@ -5404,6 +5855,13 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 	vcpu->arch.nmi_injected = events->nmi.injected;
 	if (events->flags & KVM_VCPUEVENT_VALID_NMI_PENDING) {
 		vcpu->arch.nmi_pending = 0;
+		/*
+		 * 在以下使用kvm_vcpu_arch->nmi_queued:
+		 *   - arch/x86/kvm/x86.c|820| <<kvm_inject_nmi>> atomic_inc(&vcpu->arch.nmi_queued);
+		 *   - arch/x86/kvm/x86.c|5412| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
+		 *   - arch/x86/kvm/x86.c|10439| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+		 *   - arch/x86/kvm/x86.c|12193| <<kvm_vcpu_reset>> atomic_set(&vcpu->arch.nmi_queued, 0);
+		 */
 		atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
 		kvm_make_request(KVM_REQ_NMI, vcpu);
 	}
@@ -5422,6 +5880,16 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 
 		vcpu->arch.smi_pending = events->smi.pending;
 
+		/*
+		 * 在以下使用HF_SMM_INSIDE_NMI_MASK:
+		 *   - arch/x86/include/asm/kvm_host.h|2159| <<global>> #define HF_SMM_INSIDE_NMI_MASK (1 << 2)
+		 *   - arch/x86/kvm/smm.c|138| <<kvm_smm_changed>> vcpu->arch.hflags &= ~(HF_SMM_MASK | HF_SMM_INSIDE_NMI_MASK);
+		 *   - arch/x86/kvm/smm.c|363| <<enter_smm>> vcpu->arch.hflags |= HF_SMM_INSIDE_NMI_MASK;
+		 *   - arch/x86/kvm/smm.c|647| <<emulator_leave_smm>> if ((vcpu->arch.hflags & HF_SMM_INSIDE_NMI_MASK) == 0)
+		 *   - arch/x86/kvm/x86.c|5327| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> !!(vcpu->arch.hflags & HF_SMM_INSIDE_NMI_MASK);
+		 *   - arch/x86/kvm/x86.c|5427| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.hflags |= HF_SMM_INSIDE_NMI_MASK;
+		 *   - arch/x86/kvm/x86.c|5429| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.hflags &= ~HF_SMM_INSIDE_NMI_MASK;
+		 */
 		if (events->smi.smm) {
 			if (events->smi.smm_inside_nmi)
 				vcpu->arch.hflags |= HF_SMM_INSIDE_NMI_MASK;
@@ -5436,6 +5904,13 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 #endif
 
 		if (lapic_in_kernel(vcpu)) {
+			/*
+			 * struct kvm_vcpu:
+			 * -> struct kvm_vcpu_arch arch;
+			 *    -> struct kvm_lapic *apic;
+			 *       -> unsigned long pending_events;
+			 *       -> unsigned int sipi_vector;
+			 */
 			if (events->smi.latched_init)
 				set_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
 			else
@@ -5457,12 +5932,29 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * 处理KVM_GET_DEBUGREGS:
+ *   - arch/x86/kvm/x86.c|6205| <<kvm_arch_vcpu_ioctl(KVM_GET_DEBUGREGS)>> kvm_vcpu_ioctl_x86_get_debugregs(vcpu, &dbgregs);
+ */
 static void kvm_vcpu_ioctl_x86_get_debugregs(struct kvm_vcpu *vcpu,
 					     struct kvm_debugregs *dbgregs)
 {
 	unsigned long val;
 
 	memset(dbgregs, 0, sizeof(*dbgregs));
+	/*
+	 * struct kvm_debugregs {
+	 *     __u64 db[4];
+	 *     __u64 dr6;
+	 *     __u64 dr7;
+	 *     __u64 flags;
+	 *     __u64 reserved[9];
+	 * };
+	 *
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> unsigned long db[KVM_NR_DB_REGS]; --> 4个
+	 */
 	memcpy(dbgregs->db, vcpu->arch.db, sizeof(vcpu->arch.db));
 	kvm_get_dr(vcpu, 6, &val);
 	dbgregs->dr6 = val;
@@ -6623,8 +7115,26 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7042| <<kvm_vm_ioctl_set_msr_filter>> new_filter = kvm_alloc_msr_filter(default_allow);
+ */
 static struct kvm_x86_msr_filter *kvm_alloc_msr_filter(bool default_allow)
 {
+	/*
+	 * struct msr_bitmap_range {
+	 *     u32 flags;
+	 *     u32 nmsrs;       
+	 *     u32 base;
+	 *     unsigned long *bitmap;
+	 * };
+	 *
+	 * struct kvm_x86_msr_filter {
+	 *     u8 count;
+	 *     bool default_allow:1;
+	 *     struct msr_bitmap_range ranges[16];
+	 * };
+	 */
 	struct kvm_x86_msr_filter *msr_filter;
 
 	msr_filter = kzalloc(sizeof(*msr_filter), GFP_KERNEL_ACCOUNT);
@@ -6635,6 +7145,12 @@ static struct kvm_x86_msr_filter *kvm_alloc_msr_filter(bool default_allow)
 	return msr_filter;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7049| <<kvm_vm_ioctl_set_msr_filter>> kvm_free_msr_filter(new_filter);
+ *   - arch/x86/kvm/x86.c|7060| <<kvm_vm_ioctl_set_msr_filter>> kvm_free_msr_filter(old_filter);
+ *   - arch/x86/kvm/x86.c|13215| <<kvm_arch_destroy_vm>> kvm_free_msr_filter(srcu_dereference_check(kvm->arch.msr_filter, &kvm->srcu, 1));
+ */
 static void kvm_free_msr_filter(struct kvm_x86_msr_filter *msr_filter)
 {
 	u32 i;
@@ -6648,6 +7164,10 @@ static void kvm_free_msr_filter(struct kvm_x86_msr_filter *msr_filter)
 	kfree(msr_filter);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7047| <<kvm_vm_ioctl_set_msr_filter>> r = kvm_add_msr_filter(new_filter, &filter->ranges[i]);
+ */
 static int kvm_add_msr_filter(struct kvm_x86_msr_filter *msr_filter,
 			      struct kvm_msr_filter_range *user_range)
 {
@@ -6667,6 +7187,9 @@ static int kvm_add_msr_filter(struct kvm_x86_msr_filter *msr_filter,
 	if (!bitmap_size || bitmap_size > KVM_MSR_FILTER_MAX_BITMAP_SIZE)
 		return -EINVAL;
 
+	/*
+	 * duplicate memory region from user space
+	 */
 	bitmap = memdup_user((__user u8*)user_range->bitmap, bitmap_size);
 	if (IS_ERR(bitmap))
 		return PTR_ERR(bitmap);
@@ -6682,6 +7205,22 @@ static int kvm_add_msr_filter(struct kvm_x86_msr_filter *msr_filter,
 	return 0;
 }
 
+/*
+ * #define KVM_MSR_FILTER_MAX_RANGES 16
+ * struct kvm_msr_filter {
+ * #ifndef __KERNEL__
+ * #define KVM_MSR_FILTER_DEFAULT_ALLOW (0 << 0)
+ * #endif
+ * #define KVM_MSR_FILTER_DEFAULT_DENY  (1 << 0)
+ * #define KVM_MSR_FILTER_VALID_MASK (KVM_MSR_FILTER_DEFAULT_DENY)
+ *	__u32 flags;
+ *	struct kvm_msr_filter_range ranges[KVM_MSR_FILTER_MAX_RANGES];
+ * };
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|7114| <<kvm_arch_vm_compat_ioctl>> r = kvm_vm_ioctl_set_msr_filter(kvm, &filter);
+ *   - arch/x86/kvm/x86.c|7548| <<kvm_arch_vm_ioctl(KVM_X86_SET_MSR_FILTER)>> r = kvm_vm_ioctl_set_msr_filter(kvm, &filter);
+ */
 static int kvm_vm_ioctl_set_msr_filter(struct kvm *kvm,
 				       struct kvm_msr_filter *filter)
 {
@@ -6701,11 +7240,21 @@ static int kvm_vm_ioctl_set_msr_filter(struct kvm *kvm,
 	if (empty && !default_allow)
 		return -EINVAL;
 
+	/*
+	 * struct kvm_x86_msr_filter {
+	 *     u8 count;
+	 *     bool default_allow:1;
+	 *     struct msr_bitmap_range ranges[16];
+	 * };
+	 */
 	new_filter = kvm_alloc_msr_filter(default_allow);
 	if (!new_filter)
 		return -ENOMEM;
 
 	for (i = 0; i < ARRAY_SIZE(filter->ranges); i++) {
+		/*
+		 * 只在此处调用
+		 */
 		r = kvm_add_msr_filter(new_filter, &filter->ranges[i]);
 		if (r) {
 			kvm_free_msr_filter(new_filter);
@@ -6721,6 +7270,11 @@ static int kvm_vm_ioctl_set_msr_filter(struct kvm *kvm,
 
 	kvm_free_msr_filter(old_filter);
 
+	/*
+	 * 调用static_call(kvm_x86_msr_filter_changed)(vcpu);
+	 * vmx_msr_filter_changed()
+	 * svm_msr_filter_changed()
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MSR_FILTER_CHANGED);
 
 	return 0;
@@ -6742,6 +7296,10 @@ struct kvm_msr_filter_compat {
 
 #define KVM_X86_SET_MSR_FILTER_COMPAT _IOW(KVMIO, 0xc6, struct kvm_msr_filter_compat)
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5221| <<kvm_vm_compat_ioctl>> r = kvm_arch_vm_compat_ioctl(filp, ioctl, arg);
+ */
 long kvm_arch_vm_compat_ioctl(struct file *filp, unsigned int ioctl,
 			      unsigned long arg)
 {
@@ -7402,6 +7960,11 @@ void kvm_get_segment(struct kvm_vcpu *vcpu,
 	static_call(kvm_x86_get_segment)(vcpu, var, seg);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|1988| <<kvm_hv_flush_tlb>> hc->ingpa = translate_nested_gpa(vcpu, hc->ingpa, 0, NULL);
+ *   - arch/x86/kvm/mmu.h|329| <<kvm_translate_gpa>> return translate_nested_gpa(vcpu, gpa, access, exception);
+ */
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u64 access,
 			   struct x86_exception *exception)
 {
@@ -7417,6 +7980,11 @@ gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u64 access,
 	return t_gpa;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3903| <<kvm_mmu_unprotect_page_virt>> gpa = kvm_mmu_gva_to_gpa_read(vcpu, gva, NULL);
+ *   - arch/x86/kvm/vmx/sgx.c|81| <<sgx_gva_to_gpa>> *gpa = kvm_mmu_gva_to_gpa_read(vcpu, gva, &ex);
+ */
 gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,
 			      struct x86_exception *exception)
 {
@@ -7427,6 +7995,15 @@ gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_gva_to_gpa_read);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/sgx.c|79| <<sgx_gva_to_gpa>> *gpa = kvm_mmu_gva_to_gpa_write(vcpu, gva, &ex);
+ *   - arch/x86/kvm/x86.c|8181| <<emulator_cmpxchg_emulated>> gpa = kvm_mmu_gva_to_gpa_write(vcpu, addr, NULL);
+ *   - arch/x86/kvm/x86.c|8989| <<reexecute_instruction>> gpa = kvm_mmu_gva_to_gpa_write(vcpu, cr2_or_gpa, NULL);
+ *   - arch/x86/kvm/x86.c|9092| <<retry_instruction>> gpa = kvm_mmu_gva_to_gpa_write(vcpu, cr2_or_gpa, NULL);
+ *
+ * 似乎是在转换的时候需要对write权限检查
+ */
 gpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva,
 			       struct x86_exception *exception)
 {
@@ -7675,6 +8252,10 @@ static int vcpu_mmio_gva_to_gpa(struct kvm_vcpu *vcpu, unsigned long gva,
 	return vcpu_is_mmio_gpa(vcpu, gva, *gpa, write);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7809| <<write_emulate>> return emulator_write_phys(vcpu, gpa, val, bytes);
+ */
 int emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,
 			const void *val, int bytes)
 {
@@ -8303,6 +8884,9 @@ static int emulator_get_msr_with_filter(struct x86_emulate_ctxt *ctxt,
 	return X86EMUL_CONTINUE;
 }
 
+/*
+ * struct x86_emulate_ops emulate_ops.set_msr_with_filter = emulator_set_msr_with_filter()
+ */
 static int emulator_set_msr_with_filter(struct x86_emulate_ctxt *ctxt,
 					u32 msr_index, u64 data)
 {
@@ -8478,8 +9062,15 @@ static const struct x86_emulate_ops emulate_ops = {
 	.set_xcr             = emulator_set_xcr,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9183| <<x86_emulate_instruction>> toggle_interruptibility(vcpu, ctxt->interruptibility);
+ */
 static void toggle_interruptibility(struct kvm_vcpu *vcpu, u32 mask)
 {
+	/*
+	 * vmx_get_interrupt_shadow()
+	 */
 	u32 int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);
 	/*
 	 * an sti; sti; sequence only disable interrupts for the first
@@ -8510,6 +9101,10 @@ static void inject_emulated_exception(struct kvm_vcpu *vcpu)
 		kvm_queue_exception(vcpu, ctxt->exception.vector);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12092| <<kvm_arch_vcpu_create>> if (!alloc_emulate_ctxt(vcpu))
+ */
 static struct x86_emulate_ctxt *alloc_emulate_ctxt(struct kvm_vcpu *vcpu)
 {
 	struct x86_emulate_ctxt *ctxt;
@@ -8675,12 +9270,23 @@ static int handle_emulation_failure(struct kvm_vcpu *vcpu, int emulation_type)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9316| <<x86_emulate_instruction>> if (reexecute_instruction(vcpu, cr2_or_gpa,
+ *   - arch/x86/kvm/x86.c|9395| <<x86_emulate_instruction>> if (reexecute_instruction(vcpu, cr2_or_gpa, emulation_type))
+ */
 static bool reexecute_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 				  int emulation_type)
 {
 	gpa_t gpa = cr2_or_gpa;
 	kvm_pfn_t pfn;
 
+	/*
+	 * EMULTYPE_ALLOW_RETRY_PF注释:
+	 * Set when the emulator should resume the guest to
+	 * retry native execution under certain conditions,
+	 * Can only be set in conjunction with EMULTYPE_PF.
+	 */
 	if (!(emulation_type & EMULTYPE_ALLOW_RETRY_PF))
 		return false;
 
@@ -8689,6 +9295,15 @@ static bool reexecute_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 		return false;
 
 	if (!vcpu->arch.mmu->root_role.direct) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/vmx/sgx.c|79| <<sgx_gva_to_gpa>> *gpa = kvm_mmu_gva_to_gpa_write(vcpu, gva, &ex);
+		 *   - arch/x86/kvm/x86.c|8181| <<emulator_cmpxchg_emulated>> gpa = kvm_mmu_gva_to_gpa_write(vcpu, addr, NULL);
+		 *   - arch/x86/kvm/x86.c|8989| <<reexecute_instruction>> gpa = kvm_mmu_gva_to_gpa_write(vcpu, cr2_or_gpa, NULL);
+		 *   - arch/x86/kvm/x86.c|9092| <<retry_instruction>> gpa = kvm_mmu_gva_to_gpa_write(vcpu, cr2_or_gpa, NULL);
+		 *
+		 * 似乎是在转换的时候需要对write权限检查
+		 */
 		/*
 		 * Write permission should be allowed since only
 		 * write access need to be emulated.
@@ -8725,9 +9340,32 @@ static bool reexecute_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 		unsigned int indirect_shadow_pages;
 
 		write_lock(&vcpu->kvm->mmu_lock);
+		/*
+		 * 在以下使用kvm_arch->indirect_shadow_pages:
+		 *   - arch/x86/kvm/mmu/mmu.c|1084| <<account_shadowed>> kvm->arch.indirect_shadow_pages++;
+		 *   - arch/x86/kvm/mmu/mmu.c|1161| <<unaccount_shadowed>> kvm->arch.indirect_shadow_pages--;
+		 *   - arch/x86/kvm/mmu/mmu.c|7419| <<kvm_mmu_track_write>> if (!READ_ONCE(vcpu->kvm->arch.indirect_shadow_pages))
+		 *   - arch/x86/kvm/x86.c|9045| <<reexecute_instruction>> indirect_shadow_pages = vcpu->kvm->arch.indirect_shadow_pages;
+		 */
 		indirect_shadow_pages = vcpu->kvm->arch.indirect_shadow_pages;
 		write_unlock(&vcpu->kvm->mmu_lock);
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3905| <<kvm_mmu_unprotect_page_virt>> r = kvm_mmu_unprotect_page(vcpu->kvm, gpa >> PAGE_SHIFT);
+		 *   - arch/x86/kvm/mmu/mmu.c|7145| <<kvm_mmu_page_fault>> kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(cr2_or_gpa));
+		 *   - arch/x86/kvm/x86.c|8816| <<reexecute_instruction>> kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));
+		 *   - arch/x86/kvm/x86.c|8826| <<reexecute_instruction>> kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));
+		 *   - arch/x86/kvm/x86.c|8879| <<retry_instruction>> kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));
+		 *
+		 * struct kvm_vcpu *vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct kvm_mmu *mmu;
+		 *    // Non-nested MMU for L1
+		 *    -> struct kvm_mmu root_mmu;
+		 *    // L1 MMU when running nested
+		 *    -> struct kvm_mmu guest_mmu;
+		 */
 		if (indirect_shadow_pages)
 			kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));
 
@@ -8971,6 +9609,11 @@ static bool is_vmware_backdoor_opcode(struct x86_emulate_ctxt *ctxt)
  * [*] Except #MC, which is higher priority, but KVM should never emulate in
  *     response to a machine check.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2377| <<gp_interception>> if (x86_decode_emulated_instruction(vcpu, 0, NULL, 0) != EMULATION_OK)
+ *   - arch/x86/kvm/x86.c|9388| <<x86_emulate_instruction>> r = x86_decode_emulated_instruction(vcpu, emulation_type, insn, insn_len);
+ */
 int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 				    void *insn, int insn_len)
 {
@@ -8988,6 +9631,12 @@ int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 }
 EXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5791| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn, insn_len);
+ *   - arch/x86/kvm/x86.c|9219| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|9226| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len)
 {
@@ -9176,12 +9825,36 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|713| <<avic_unaccelerated_access_interception>> ret = kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|391| <<__svm_skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+ *   - arch/x86/kvm/svm/svm.c|2228| <<io_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|2391| <<gp_interception>> return kvm_emulate_instruction(vcpu,
+ *   - arch/x86/kvm/svm/svm.c|2585| <<invlpg_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|2593| <<emulate_on_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|1735| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP)) 
+ *   - arch/x86/kvm/vmx/vmx.c|5159| <<handle_rmode_exception>> if (kvm_emulate_instruction(vcpu, 0)) {
+ *   - arch/x86/kvm/vmx/vmx.c|5253| <<handle_exception_nmi>> return kvm_emulate_instruction(vcpu, EMULTYPE_VMWARE_GP);
+ *   - arch/x86/kvm/vmx/vmx.c|5393| <<handle_io>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5466| <<handle_desc>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5679| <<handle_apic_access>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5815| <<handle_ept_violation>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5877| <<handle_invalid_guest_state>> if (!kvm_emulate_instruction(vcpu, 0))
+ *   - arch/x86/kvm/x86.c|775| <<complete_emulated_insn_gp>> return kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE | EMULTYPE_SKIP |
+ *   - arch/x86/kvm/x86.c|7663| <<handle_ud>> return kvm_emulate_instruction(vcpu, emul_type);
+ *   - arch/x86/kvm/x86.c|11230| <<complete_emulated_io>> return kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE);
+ */
 int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type)
 {
 	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2598| <<rsm_interception>> return kvm_emulate_instruction_from_buffer(vcpu, rsm_ins_bytes, 2);
+ */
 int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
 					void *insn, int insn_len)
 {
@@ -10179,6 +10852,10 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
  * ordering between that side effect, the instruction completing, _and_ the
  * delivery of the asynchronous event.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10790| <<vcpu_enter_guest>> r = kvm_check_and_inject_events(vcpu, &req_immediate_exit);
+ */
 static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 				       bool *req_immediate_exit)
 {
@@ -10218,6 +10895,13 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 	 * *previous* instruction and must be serviced prior to recognizing any
 	 * new events in order to fully complete the previous instruction.
 	 */
+	/*
+	 * vmx_inject_nmi()
+	 * svm_inject_nmi()
+	 *
+	 * vmx_inject_irq()
+	 * svm_inject_irq()
+	 */
 	if (vcpu->arch.exception.injected)
 		kvm_inject_exception(vcpu);
 	else if (kvm_is_exception_pending(vcpu))
@@ -10315,6 +10999,13 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 			goto out;
 		if (r) {
 			vcpu->arch.smi_pending = false;
+			/*
+			 * 在以下使用kvm_vcpu_arch->smi_count:
+			 *   - arch/x86/kvm/x86.c|3910| <<kvm_set_msr_common(MSR_SMI_COUNT)>> vcpu->arch.smi_count = data;
+			 *   - arch/x86/kvm/x86.c|4282| <<kvm_get_msr_common(MSR_SMI_COUNT)>> msr_info->data = vcpu->arch.smi_count;
+			 *   - arch/x86/kvm/x86.c|10318| <<kvm_check_and_inject_events>> ++vcpu->arch.smi_count;
+			 *   - arch/x86/kvm/x86.c|12166| <<kvm_vcpu_reset>> vcpu->arch.smi_count = 0;
+			 */
 			++vcpu->arch.smi_count;
 			enter_smm(vcpu);
 			can_inject = false;
@@ -10324,16 +11015,36 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 #endif
 
 	if (vcpu->arch.nmi_pending) {
+		/*
+		 * vmx_nmi_allowed()
+		 * svm_nmi_allowed()
+		 */
 		r = can_inject ? static_call(kvm_x86_nmi_allowed)(vcpu, true) : -EBUSY;
 		if (r < 0)
 			goto out;
 		if (r) {
 			--vcpu->arch.nmi_pending;
 			vcpu->arch.nmi_injected = true;
+			/*
+			 * vmx_inject_nmi()
+			 * svm_inject_nmi()
+			 */
 			static_call(kvm_x86_inject_nmi)(vcpu);
 			can_inject = false;
 			WARN_ON(static_call(kvm_x86_nmi_allowed)(vcpu, true) < 0);
 		}
+		/*
+		 * Intel注释
+		 * If the "NMI-window exiting" VM-execution control is 1, a VM exit
+		 * occurs before execution of any instruction if
+		 * there is no virtual-NMI blocking and there is no blocking of events by
+		 * MOV SS and no blocking of events by STI (see Table 24-3). Such a VM
+		 * exit occurs immediately after VM entry if the above conditions are
+		 * true (see Section 26.7.6).
+		 *
+		 * vmx_enable_nmi_window()
+		 * svm_enable_nmi_window()
+		 */
 		if (vcpu->arch.nmi_pending)
 			static_call(kvm_x86_enable_nmi_window)(vcpu);
 	}
@@ -10384,6 +11095,56 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+/*
+ * Intel关于nmi的注释
+ * NONMASKABLE INTERRUPT (NMI)
+ *
+ * The nonmaskable interrupt (NMI) can be generated in either of two ways:
+ *
+ * - External hardware asserts the NMI pin.
+ *
+ * - The processor receives a message on the system bus (Pentium 4, Intel Core Duo,
+ * Intel Core 2, Intel Atom, and Intel Xeon processors) or the APIC serial bus (P6
+ * family and Pentium processors) with a delivery mode NMI.
+ *
+ * When the processor receives a NMI from either of these sources, the processor
+ * handles it immediately by calling the NMI handler pointed to by interrupt
+ * vector number 2. The processor also invokes certain hardware conditions to
+ * ensure that no other interrupts, including NMI interrupts, are received until
+ * the NMI handler has completed executing (see Section 6.7.1, "Handling Multiple
+ * NMIs").
+ *
+ * Also, when an NMI is received from either of the above sources, it cannot be
+ * masked by the IF flag in the EFLAGS register.
+ *
+ * It is possible to issue a maskable hardware interrupt (through the INTR pin) to
+ * vector 2 to invoke the NMI interrupt handler; however, this interrupt will not
+ * truly be an NMI interrupt. A true NMI interrupt that activates the processor's
+ * NMI-handling hardware can only be delivered through one of the mechanisms
+ * listed above.
+ *
+ * Handling Multiple NMIs
+ *
+ * While an NMI interrupt handler is executing, the processor blocks delivery of
+ * subsequent NMIs until the next execution of the IRET instruction. This blocking
+ * of NMIs prevents nested execution of the NMI handler. It is recommended that
+ * the NMI interrupt handler be accessed through an interrupt gate to disable
+ * maskable hardware interrupts (see Section 6.8.1, "Masking Maskable Hardware
+ * Interrupts").
+ *
+ * An execution of the IRET instruction unblocks NMIs even if the instruction
+ * causes a fault. For example, if the IRET instruction executes with EFLAGS.VM =
+ * 1 and IOPL of less than 3, a general-protection exception is generated (see
+ * Section 20.2.7, "Sensitive Instructions"). In such a case, NMIs are unmasked
+ * before the exception handler is invoked.
+ */
+
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5261| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> process_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|5387| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> process_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|10783| <<vcpu_enter_guest(KVM_REQ_NMI)>> process_nmi(vcpu);
+ */
 static void process_nmi(struct kvm_vcpu *vcpu)
 {
 	unsigned int limit;
@@ -10398,6 +11159,10 @@ static void process_nmi(struct kvm_vcpu *vcpu)
 	 * blocks NMIs).  KVM will immediately inject one of the two NMIs, and
 	 * will request an NMI window to handle the second NMI.
 	 */
+	/*
+	 * vmx_get_nmi_mask()
+	 * svm_get_nmi_mask()
+	 */
 	if (static_call(kvm_x86_get_nmi_mask)(vcpu) || vcpu->arch.nmi_injected)
 		limit = 1;
 	else
@@ -10407,12 +11172,25 @@ static void process_nmi(struct kvm_vcpu *vcpu)
 	 * Adjust the limit to account for pending virtual NMIs, which aren't
 	 * tracked in vcpu->arch.nmi_pending.
 	 */
+	/*
+	 * svm_is_vnmi_pending()
+	 */
 	if (static_call(kvm_x86_is_vnmi_pending)(vcpu))
 		limit--;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->nmi_queued:
+	 *   - arch/x86/kvm/x86.c|820| <<kvm_inject_nmi>> atomic_inc(&vcpu->arch.nmi_queued);
+	 *   - arch/x86/kvm/x86.c|5412| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
+	 *   - arch/x86/kvm/x86.c|10439| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+	 *   - arch/x86/kvm/x86.c|12193| <<kvm_vcpu_reset>> atomic_set(&vcpu->arch.nmi_queued, 0);
+	 */
 	vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
 	vcpu->arch.nmi_pending = min(vcpu->arch.nmi_pending, limit);
 
+	/*
+	 * svm_set_vnmi_pending()
+	 */
 	if (vcpu->arch.nmi_pending &&
 	    (static_call(kvm_x86_set_vnmi_pending)(vcpu)))
 		vcpu->arch.nmi_pending--;
@@ -10421,6 +11199,10 @@ static void process_nmi(struct kvm_vcpu *vcpu)
 		kvm_make_request(KVM_REQ_EVENT, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5325| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> events->nmi.pending = kvm_get_nr_pending_nmis(vcpu);
+ */
 /* Return total number of NMIs pending injection to the VM */
 int kvm_get_nr_pending_nmis(struct kvm_vcpu *vcpu)
 {
@@ -10617,6 +11399,10 @@ EXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11604| <<vcpu_run>> r = vcpu_enter_guest(vcpu);
+ */
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -10765,8 +11551,26 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			kvm_hv_process_stimers(vcpu);
 		if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
 			kvm_vcpu_update_apicv(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_APF_READY:
+		 *   - arch/x86/kvm/lapic.c|503| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+		 *   - arch/x86/kvm/lapic.c|2573| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+		 *   - arch/x86/kvm/x86.c|11001| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+		 *   - arch/x86/kvm/x86.c|13617| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+		 *
+		 * 处理函数是kvm_check_async_pf_completion()
+		 */
 		if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
 			kvm_check_async_pf_completion(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_MSR_FILTER_CHANGED:
+		 *   - arch/x86/kvm/x86.c|7246| <<kvm_vm_ioctl_set_msr_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_MSR_FILTER_CHANGED);
+		 *   - arch/x86/kvm/x86.c|11533| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MSR_FILTER_CHANGED, vcpu))
+		 *
+		 * 处理函数static_call(kvm_x86_msr_filter_changed)(vcpu)
+		 * -> vmx_msr_filter_changed()
+		 * -> svm_msr_filter_changed()
+		 */
 		if (kvm_check_request(KVM_REQ_MSR_FILTER_CHANGED, vcpu))
 			static_call(kvm_x86_msr_filter_changed)(vcpu);
 
@@ -10843,6 +11647,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	 * status, KVM doesn't update assigned devices when APICv is inhibited,
 	 * i.e. they can post interrupts even if APICv is temporarily disabled.
 	 */
+	/*
+	 * vmx_sync_pir_to_irr()
+	 */
 	if (kvm_lapic_enabled(vcpu))
 		static_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);
 
@@ -11218,6 +12025,10 @@ static void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 	trace_kvm_fpu(0);
 }
 
+/*
+ * 处理KVM_RUN:
+ *   - virt/kvm/kvm_main.c|4278| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_queued_exception *ex = &vcpu->arch.exception;
@@ -11517,6 +12328,11 @@ int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+/*
+ * struct kvm_mp_state {
+ *     __u32 mp_state;
+ * };
+ */
 int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 				    struct kvm_mp_state *mp_state)
 {
@@ -12164,6 +12980,13 @@ void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 
 	vcpu->arch.smi_pending = 0;
 	vcpu->arch.smi_count = 0;
+	/*
+	 * 在以下使用kvm_vcpu_arch->nmi_queued:
+	 *   - arch/x86/kvm/x86.c|820| <<kvm_inject_nmi>> atomic_inc(&vcpu->arch.nmi_queued);
+	 *   - arch/x86/kvm/x86.c|5412| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
+	 *   - arch/x86/kvm/x86.c|10439| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+	 *   - arch/x86/kvm/x86.c|12193| <<kvm_vcpu_reset>> atomic_set(&vcpu->arch.nmi_queued, 0);
+	 */
 	atomic_set(&vcpu->arch.nmi_queued, 0);
 	vcpu->arch.nmi_pending = 0;
 	vcpu->arch.nmi_injected = false;
@@ -12797,6 +13620,10 @@ static void kvm_mmu_update_cpu_dirty_logging(struct kvm *kvm, bool enable)
 		kvm_make_all_cpus_request(kvm, KVM_REQ_UPDATE_CPU_DIRTY_LOGGING);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13142| <<kvm_arch_commit_memory_region>> kvm_mmu_slot_apply_flags(kvm, old, new, change);
+ */
 static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 				     struct kvm_memory_slot *old,
 				     const struct kvm_memory_slot *new,
@@ -12918,6 +13745,10 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1687| <<kvm_commit_memory_region>> kvm_arch_commit_memory_region(kvm, old, new, change);
+ */
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *old,
 				const struct kvm_memory_slot *new,
@@ -13097,6 +13928,14 @@ void kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 }
 EXPORT_SYMBOL_GPL(kvm_set_rflags);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13370| <<kvm_add_async_pf_gfn>> u32 key = kvm_async_pf_hash_fn(gfn);
+ *   - arch/x86/kvm/x86.c|13381| <<kvm_async_pf_gfn_slot>> u32 key = kvm_async_pf_hash_fn(gfn);
+ *   - arch/x86/kvm/x86.c|13411| <<kvm_del_async_pf_gfn>> k = kvm_async_pf_hash_fn(vcpu->arch.apf.gfns[j]);
+ *
+ * 用gfn计算一个hash
+ */
 static inline u32 kvm_async_pf_hash_fn(gfn_t gfn)
 {
 	BUILD_BUG_ON(!is_power_of_2(ASYNC_PF_PER_VCPU));
@@ -13104,21 +13943,59 @@ static inline u32 kvm_async_pf_hash_fn(gfn_t gfn)
 	return hash_32(gfn & 0xffffffff, order_base_2(ASYNC_PF_PER_VCPU));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13373| <<kvm_add_async_pf_gfn>> key = kvm_async_pf_next_probe(key);
+ *   - arch/x86/kvm/x86.c|13386| <<kvm_async_pf_gfn_slot>> key = kvm_async_pf_next_probe(key);
+ *   - arch/x86/kvm/x86.c|13408| <<kvm_del_async_pf_gfn>> j = kvm_async_pf_next_probe(j);
+ *
+ * 实际就是增加1
+ */
 static inline u32 kvm_async_pf_next_probe(u32 key)
 {
 	return (key + 1) & (ASYNC_PF_PER_VCPU - 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13504| <<kvm_arch_async_page_not_present>> kvm_add_async_pf_gfn(vcpu, work->arch.gfn);
+ */
 static void kvm_add_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	u32 key = kvm_async_pf_hash_fn(gfn);
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *        struct {
+	 *            bool halted;
+	 *            gfn_t gfns[ASYNC_PF_PER_VCPU]; --> 64个
+	 *            struct gfn_to_hva_cache data;
+	 *            u64 msr_en_val; // MSR_KVM_ASYNC_PF_EN
+	 *            u64 msr_int_val; // MSR_KVM_ASYNC_PF_INT
+	 *            u16 vec;
+	 *            u32 id;
+	 *            bool send_user_only;
+	 *            u32 host_apf_flags;
+	 *            bool delivery_as_pf_vmexit;
+	 *            bool pageready_pending;
+	 *        } apf;
+	 *
+	 * 不停为key增加1, 直到找到可用的gfns[key]
+	 */
 	while (vcpu->arch.apf.gfns[key] != ~0)
 		key = kvm_async_pf_next_probe(key);
 
 	vcpu->arch.apf.gfns[key] = gfn;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13393| <<kvm_find_async_pf_gfn>> return vcpu->arch.apf.gfns[kvm_async_pf_gfn_slot(vcpu, gfn)] == gfn;
+ *   - arch/x86/kvm/x86.c|13400| <<kvm_del_async_pf_gfn>> i = j = kvm_async_pf_gfn_slot(vcpu, gfn);
+ *
+ * 从vcpu->arch.apf.gfns[ASYNC_PF_PER_VCPU]搜索gfn对应的key
+ */
 static u32 kvm_async_pf_gfn_slot(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	int i;
@@ -13132,11 +14009,19 @@ static u32 kvm_async_pf_gfn_slot(struct kvm_vcpu *vcpu, gfn_t gfn)
 	return key;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5845| <<__kvm_faultin_pfn>> if (kvm_find_async_pf_gfn(vcpu, fault->gfn)) {
+ */
 bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	return vcpu->arch.apf.gfns[kvm_async_pf_gfn_slot(vcpu, gfn)] == gfn;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13541| <<kvm_arch_async_page_present>> kvm_del_async_pf_gfn(vcpu, work->arch.gfn);
+ */
 static void kvm_del_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	u32 i, j, k;
@@ -13164,14 +14049,56 @@ static void kvm_del_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13507| <<kvm_arch_async_page_not_present>> if (kvm_can_deliver_async_pf(vcpu) && !apf_put_user_notpresent(vcpu)) {
+ *
+ * 写入KVM_PV_REASON_PAGE_NOT_PRESENT
+ */
 static inline int apf_put_user_notpresent(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *        struct {
+	 *            bool halted;
+	 *            gfn_t gfns[ASYNC_PF_PER_VCPU]; --> 64个
+	 *            struct gfn_to_hva_cache data;
+	 *            u64 msr_en_val; // MSR_KVM_ASYNC_PF_EN
+	 *            u64 msr_int_val; // MSR_KVM_ASYNC_PF_INT
+	 *            u16 vec;
+	 *            u32 id;
+	 *            bool send_user_only;
+	 *            u32 host_apf_flags;
+	 *            bool delivery_as_pf_vmexit;
+	 *            bool pageready_pending;
+	 *        } apf;
+	 *
+	 * #define KVM_PV_REASON_PAGE_NOT_PRESENT 1
+	 * #define KVM_PV_REASON_PAGE_READY 2
+	 */
 	u32 reason = KVM_PV_REASON_PAGE_NOT_PRESENT;
 
+	/*
+	 * struct kvm_vcpu_pv_apf_data {
+	 *     // Used for 'page not present' events delivered via #PF
+	 *     __u32 flags;
+	 *
+	 *     // Used for 'page ready' events delivered via interrupt notification
+	 *     __u32 token;
+	 *
+	 *     __u8 pad[56];
+	 *     __u32 enabled;
+	 * };
+	 */
 	return kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apf.data, &reason,
 				      sizeof(reason));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13546| <<kvm_arch_async_page_present>> if ((work->wakeup_all || work->notpresent_injected) && kvm_pv_async_pf_enabled(vcpu) && !apf_put_user_ready(vcpu, work->arch.token)) {
+ */
 static inline int apf_put_user_ready(struct kvm_vcpu *vcpu, u32 token)
 {
 	unsigned int offset = offsetof(struct kvm_vcpu_pv_apf_data, token);
@@ -13180,6 +14107,12 @@ static inline int apf_put_user_ready(struct kvm_vcpu *vcpu, u32 token)
 					     &token, offset, sizeof(token));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13571| <<kvm_arch_can_dequeue_async_page_present>> return kvm_lapic_enabled(vcpu) && apf_pageready_slot_free(vcpu);
+ *
+ * 读取token
+ */
 static inline bool apf_pageready_slot_free(struct kvm_vcpu *vcpu)
 {
 	unsigned int offset = offsetof(struct kvm_vcpu_pv_apf_data, token);
@@ -13192,6 +14125,11 @@ static inline bool apf_pageready_slot_free(struct kvm_vcpu *vcpu)
 	return !val;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13488| <<kvm_can_do_async_pf>> if (kvm_hlt_in_guest(vcpu->kvm) && !kvm_can_deliver_async_pf(vcpu))
+ *   - arch/x86/kvm/x86.c|13506| <<kvm_arch_async_page_not_present>> if (kvm_can_deliver_async_pf(vcpu) &&
+ */
 static bool kvm_can_deliver_async_pf(struct kvm_vcpu *vcpu)
 {
 
@@ -13218,6 +14156,10 @@ static bool kvm_can_deliver_async_pf(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5839| <<__kvm_faultin_pfn>> if (!fault->prefetch && kvm_can_do_async_pf(vcpu)) {
+ */
 bool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)
 {
 	if (unlikely(!lapic_in_kernel(vcpu) ||
@@ -13235,6 +14177,10 @@ bool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)
 	return kvm_arch_interrupt_allowed(vcpu);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|245| <<kvm_setup_async_pf>> work->notpresent_injected = kvm_arch_async_page_not_present(vcpu, work);
+ */
 bool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 				     struct kvm_async_pf *work)
 {
@@ -13243,8 +14189,15 @@ bool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 	trace_kvm_async_pf_not_present(work->arch.token, work->cr2_or_gpa);
 	kvm_add_async_pf_gfn(vcpu, work->arch.gfn);
 
+	/*
+	 * apf_put_user_notpresent()
+	 * 写入KVM_PV_REASON_PAGE_NOT_PRESENT
+	 */
 	if (kvm_can_deliver_async_pf(vcpu) &&
 	    !apf_put_user_notpresent(vcpu)) {
+		/*
+		 * VM端收到interrupt会调用kvm_async_pf_task_wait_schedule(token)
+		 */
 		fault.vector = PF_VECTOR;
 		fault.error_code_valid = true;
 		fault.error_code = 0;
@@ -13267,6 +14220,14 @@ bool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|95| <<async_pf_execute>> if (IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC)) kvm_arch_async_page_present(vcpu, apf);
+ *   - virt/kvm/async_pf.c|195| <<kvm_check_async_pf_completion>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC)) kvm_arch_async_page_present(vcpu, work);
+ *
+ * 应该是插入中断, 告诉VM准备好了
+ * vcpu->arch.apf.vec应该是用MSR_KVM_ASYNC_PF_INT->kvm_pv_enable_async_pf_int()设置的
+ */
 void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work)
 {
@@ -13281,9 +14242,20 @@ void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 		kvm_del_async_pf_gfn(vcpu, work->arch.gfn);
 	trace_kvm_async_pf_ready(work->arch.token, work->cr2_or_gpa);
 
+	/*
+	 * 在以下使用work->notpresent_injected:
+	 *   - arch/x86/kvm/x86.c|13780| <<kvm_arch_async_page_present>> if ((work->wakeup_all || work->notpresent_injected) &&
+	 *   - virt/kvm/async_pf.c|358| <<kvm_setup_async_pf>> work->notpresent_injected = kvm_arch_async_page_not_present(vcpu, work);
+	 */
 	if ((work->wakeup_all || work->notpresent_injected) &&
 	    kvm_pv_async_pf_enabled(vcpu) &&
 	    !apf_put_user_ready(vcpu, work->arch.token)) {
+		/*
+		 * 在以下使用apf.pageready_pending:
+		 *   - arch/x86/kvm/x86.c|3998| <<kvm_set_msr_common(MSR_KVM_ASYNC_PF_ACK)>> vcpu->arch.apf.pageready_pending = false;
+		 *   - arch/x86/kvm/x86.c|13602| <<kvm_arch_async_page_present>> vcpu->arch.apf.pageready_pending = true;
+		 *   - arch/x86/kvm/x86.c|13618| <<kvm_arch_async_page_present_queued>> if (!vcpu->arch.apf.pageready_pending)
+		 */
 		vcpu->arch.apf.pageready_pending = true;
 		kvm_apic_set_irq(vcpu, &irq, NULL);
 	}
@@ -13292,15 +14264,43 @@ void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 	vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|104| <<async_pf_execute>> kvm_arch_async_page_present_queued(vcpu);
+ *   - virt/kvm/async_pf.c|277| <<kvm_async_pf_wakeup_all>> kvm_arch_async_page_present_queued(vcpu);
+ */
 void kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用KVM_REQ_APF_READY:
+	 *   - arch/x86/kvm/lapic.c|503| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+	 *   - arch/x86/kvm/lapic.c|2573| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+	 *   - arch/x86/kvm/x86.c|11001| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+	 *   - arch/x86/kvm/x86.c|13617| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+	 *
+	 * 处理函数是kvm_check_async_pf_completion()
+	 */
 	kvm_make_request(KVM_REQ_APF_READY, vcpu);
+	/*
+	 * 在以下使用apf.pageready_pending:
+	 *   - arch/x86/kvm/x86.c|3998| <<kvm_set_msr_common(MSR_KVM_ASYNC_PF_ACK)>> vcpu->arch.apf.pageready_pending = false;
+	 *   - arch/x86/kvm/x86.c|13602| <<kvm_arch_async_page_present>> vcpu->arch.apf.pageready_pending = true;
+	 *   - arch/x86/kvm/x86.c|13618| <<kvm_arch_async_page_present_queued>> if (!vcpu->arch.apf.pageready_pending)
+	 */
 	if (!vcpu->arch.apf.pageready_pending)
 		kvm_vcpu_kick(vcpu);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|141| <<kvm_check_async_pf_completion>> kvm_arch_can_dequeue_async_page_present(vcpu)) {
+ */
 bool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 查看msr是否设置了KVM_ASYNC_PF_ENABLED | KVM_ASYNC_PF_DELIVERY_AS_INT(两个都要)
+	 * vcpu->arch.apf.msr_en_val
+	 */
 	if (!kvm_pv_async_pf_enabled(vcpu))
 		return true;
 	else
@@ -13433,6 +14433,11 @@ bool kvm_arch_no_poll(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_arch_no_poll);
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3028| <<svm_set_msr>> if (kvm_spec_ctrl_test_value(data))
+ *   - arch/x86/kvm/vmx/vmx.c|2562| <<vmx_set_msr>> if (kvm_spec_ctrl_test_value(data))
+ */
 int kvm_spec_ctrl_test_value(u64 value)
 {
 	/*
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 5184fde1d..d913f9aa1 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -28,6 +28,17 @@ struct kvm_caps {
 	u64 supported_mce_cap;
 	u64 supported_xcr0;
 	u64 supported_xss;
+	/*
+	 * 在以下使用kvm_caps->supported_perf_cap:
+	 *   - arch/x86/kvm/svm/svm.c|5081| <<svm_set_cpu_caps>> kvm_caps.supported_perf_cap = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|2153| <<vmx_get_supported_debugctl>> if ((kvm_caps.supported_perf_cap & PMU_CAP_LBR_FMT) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|2433| <<vmx_set_msr(MSR_IA32_PERF_CAPABILITIES)>> (kvm_caps.supported_perf_cap & PMU_CAP_LBR_FMT))
+	 *   - arch/x86/kvm/vmx/vmx.c|2440| <<vmx_set_msr(MSR_IA32_PERF_CAPABILITIES)>> (kvm_caps.supported_perf_cap & PERF_CAP_PEBS_MASK))
+	 *   - arch/x86/kvm/vmx/vmx.c|8059| <<vmx_set_cpu_caps>> kvm_caps.supported_perf_cap = vmx_get_perf_capabilities();
+	 *   - arch/x86/kvm/x86.c|1733| <<kvm_get_msr_feature>> msr->data = kvm_caps.supported_perf_cap;
+	 *   - arch/x86/kvm/x86.c|3866| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> if (data & ~kvm_caps.supported_perf_cap)
+	 *   - arch/x86/kvm/x86.c|12429| <<kvm_arch_vcpu_create>> vcpu->arch.perf_capabilities = kvm_caps.supported_perf_cap;
+	 */
 	u64 supported_perf_cap;
 };
 
@@ -111,6 +122,15 @@ static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
 	vcpu->arch.exception_vmexit.pending = false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4059| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+ *   - arch/x86/kvm/svm/svm.c|4062| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+ *   - arch/x86/kvm/vmx/vmx.c|7129| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+ *   - arch/x86/kvm/x86.c|5106| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+ *   - arch/x86/kvm/x86.c|10414| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+ *   - arch/x86/kvm/x86.c|11799| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+ */
 static inline void kvm_queue_interrupt(struct kvm_vcpu *vcpu, u8 vector,
 	bool soft)
 {
@@ -215,6 +235,11 @@ static inline bool is_noncanonical_address(u64 la, struct kvm_vcpu *vcpu)
 	return !__is_canonical_address(la, vcpu_virt_addr_bits(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3297| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+ *   - arch/x86/kvm/mmu/mmu.c|4177| <<handle_mmio_page_fault>> vcpu_cache_mmio_info(vcpu, addr, gfn, access);
+ */
 static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 					gva_t gva, gfn_t gfn, unsigned access)
 {
@@ -244,6 +269,13 @@ static inline bool vcpu_match_mmio_gen(struct kvm_vcpu *vcpu)
  */
 #define MMIO_GVA_ANY (~(gva_t)0)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5060| <<kvm_mmu_sync_roots>> vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
+ *   - arch/x86/kvm/mmu/mmu.c|5804| <<kvm_mmu_new_pgd>> vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
+ *   - arch/x86/kvm/mmu/mmu.c|6678| <<kvm_mmu_unload>> vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
+ *   - arch/x86/kvm/mmu/mmu.c|6972| <<__kvm_mmu_invalidate_addr>> vcpu_clear_mmio_info(vcpu, addr);
+ */
 static inline void vcpu_clear_mmio_info(struct kvm_vcpu *vcpu, gva_t gva)
 {
 	if (gva != MMIO_GVA_ANY && vcpu->arch.mmio_gva != (gva & PAGE_MASK))
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4944136ef..c4bd97061 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -791,6 +791,12 @@ struct kvm {
 	unsigned long mmu_invalidate_range_end;
 #endif
 	struct list_head devices;
+	/*
+	 * 在以下使用kvm->manual_dirty_log_protect:
+	 *   - include/linux/kvm_host.h|921| <<kvm_dirty_log_manual_protect_and_init_set>> return !!(kvm->manual_dirty_log_protect & KVM_DIRTY_LOG_INITIALLY_SET);
+	 *   - virt/kvm/kvm_main.c|2190| <<kvm_get_dirty_log_protect>> if (kvm->manual_dirty_log_protect) {
+	 *   - virt/kvm/kvm_main.c|4761| <<kvm_vm_ioctl_enable_cap_generic(KVM_CAP_MANUAL_DIRTY_LOG_PROTECT2)>> kvm->manual_dirty_log_protect = cap->args[0];
+	 */
 	u64 manual_dirty_log_protect;
 	struct dentry *debugfs_dentry;
 	struct kvm_stat_data **debugfs_stat_data;
@@ -799,6 +805,17 @@ struct kvm {
 	pid_t userspace_pid;
 	bool override_halt_poll_ns;
 	unsigned int max_halt_poll_ns;
+	/*
+	 * 在以下使用kvm->dirty_ring_size:
+	 *   -  virt/kvm/kvm_main.c|3428| <<mark_page_dirty_in_slot>> if (kvm->dirty_ring_size && vcpu)
+	 *   - virt/kvm/kvm_main.c|3893| <<kvm_page_in_dirty_ring>> kvm->dirty_ring_size / PAGE_SIZE);
+	 *   - virt/kvm/kvm_main.c|4048| <<kvm_vm_ioctl_create_vcpu>> if (kvm->dirty_ring_size) {
+	 *   - virt/kvm/kvm_main.c|4050| <<kvm_vm_ioctl_create_vcpu>> id, kvm->dirty_ring_size);
+	 *   - virt/kvm/kvm_main.c|4677| <<kvm_vm_ioctl_enable_dirty_log_ring>> if (kvm->dirty_ring_size)
+	 *   - virt/kvm/kvm_main.c|4686| <<kvm_vm_ioctl_enable_dirty_log_ring>> kvm->dirty_ring_size = size;
+	 *   - virt/kvm/kvm_main.c|4700| <<kvm_vm_ioctl_reset_dirty_pages>> if (!kvm->dirty_ring_size)
+	 *   - virt/kvm/kvm_main.c|4781| <<kvm_vm_ioctl_enable_cap_generic>> !kvm->dirty_ring_size || cap->flags)
+	 */
 	u32 dirty_ring_size;
 	bool dirty_ring_with_bitmap;
 	bool vm_bugged;
@@ -2303,8 +2320,28 @@ static inline void kvm_handle_signal_exit(struct kvm_vcpu *vcpu)
  * kvm_account_pgtable_pages() is thread-safe because mod_lruvec_page_state()
  * is thread-safe.
  */
+/*
+ * 在以下使用kvm_account_pgtable_pages():
+ *   - arch/arm64/kvm/mmu.c|196| <<stage2_memcache_zalloc_page>> kvm_account_pgtable_pages(virt, 1);
+ *   - arch/arm64/kvm/mmu.c|210| <<kvm_s2_zalloc_pages_exact>> kvm_account_pgtable_pages(virt, (size >> PAGE_SHIFT));
+ *   - arch/arm64/kvm/mmu.c|216| <<kvm_s2_free_pages_exact>> kvm_account_pgtable_pages(virt, -(size >> PAGE_SHIFT));
+ *   - arch/arm64/kvm/mmu.c|260| <<kvm_s2_put_page>> kvm_account_pgtable_pages(addr, -1);
+ *   - arch/x86/kvm/mmu/mmu.c|2386| <<kvm_account_mmu_page>> kvm_account_pgtable_pages((void *)sp->spt, +1);
+ *   - arch/x86/kvm/mmu/mmu.c|2392| <<kvm_unaccount_mmu_page>> kvm_account_pgtable_pages((void *)sp->spt, -1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|454| <<tdp_account_mmu_page>> kvm_account_pgtable_pages((void *)sp->spt, +1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|460| <<tdp_unaccount_mmu_page>> kvm_account_pgtable_pages((void *)sp->spt, -1);
+ */
 static inline void kvm_account_pgtable_pages(void *virt, int nr)
 {
+	/*
+	 * 在以下使用NR_SECONDARY_PAGETABLE:
+	 *   - include/linux/mmzone.h|202| <<global>> NR_SECONDARY_PAGETABLE,
+	 *   - mm/memcontrol.c|1527| <<global>> { "sec_pagetables", NR_SECONDARY_PAGETABLE },
+	 *   - drivers/base/node.c|466| <<node_read_meminfo>> nid, K(node_page_state(pgdat, NR_SECONDARY_PAGETABLE)),
+	 *   - fs/proc/meminfo.c|121| <<meminfo_proc_show>> global_node_page_state(NR_SECONDARY_PAGETABLE));
+	 *   - mm/show_mem.c|225| <<show_free_areas>> global_node_page_state(NR_SECONDARY_PAGETABLE),
+	 *   - mm/show_mem.c|287| <<show_free_areas>> K(node_page_state(pgdat, NR_SECONDARY_PAGETABLE)),
+	 */
 	mod_lruvec_page_state(virt_to_page(virt), NR_SECONDARY_PAGETABLE, nr);
 }
 
diff --git a/kernel/cpu.c b/kernel/cpu.c
index a86972a91..d638783ab 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -279,6 +279,21 @@ static bool cpuhp_is_atomic_state(enum cpuhp_state state)
 	return CPUHP_AP_IDLE_DEAD <= state && state < CPUHP_AP_ONLINE;
 }
 
+/*
+ * enum cpuhp_sync_state {
+ *     SYNC_STATE_DEAD,
+ *     SYNC_STATE_KICKED,
+ *     SYNC_STATE_SHOULD_DIE,
+ *     SYNC_STATE_ALIVE,
+ *     SYNC_STATE_SHOULD_ONLINE,
+ *     SYNC_STATE_ONLINE,
+ * };
+ *
+ * 在以下使用SYNC_STATE_ALIVE:
+ *   - kernel/cpu.c|396| <<cpuhp_ap_sync_alive>> cpuhp_ap_update_sync_state(SYNC_STATE_ALIVE);
+ *   - kernel/cpu.c|421| <<cpuhp_can_boot_ap>> case SYNC_STATE_ALIVE:
+ *   - kernel/cpu.c|454| <<cpuhp_bp_sync_alive>> if (!cpuhp_wait_for_sync_state(cpu, SYNC_STATE_ALIVE, SYNC_STATE_SHOULD_ONLINE)) {
+ */
 /* Synchronization state management */
 enum cpuhp_sync_state {
 	SYNC_STATE_DEAD,
@@ -306,6 +321,11 @@ static inline void cpuhp_ap_update_sync_state(enum cpuhp_sync_state state)
 
 void __weak arch_cpuhp_sync_state_poll(void) { cpu_relax(); }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|372| <<cpuhp_bp_sync_dead>> if (cpuhp_wait_for_sync_state(cpu, SYNC_STATE_DEAD, SYNC_STATE_DEAD)) {
+ *   - kernel/cpu.c|454| <<cpuhp_bp_sync_alive>> if (!cpuhp_wait_for_sync_state(cpu, SYNC_STATE_ALIVE, SYNC_STATE_SHOULD_ONLINE)) {
+ */
 static bool cpuhp_wait_for_sync_state(unsigned int cpu, enum cpuhp_sync_state state,
 				      enum cpuhp_sync_state next_state)
 {
@@ -318,6 +338,21 @@ static bool cpuhp_wait_for_sync_state(unsigned int cpu, enum cpuhp_sync_state st
 	sync = atomic_read(st);
 	while (1) {
 		if (sync == state) {
+			/*
+			 * atomic_try_cmpxchg() - atomic compare and exchange with full ordering
+			 * @v: pointer to atomic_t
+			 * @old: pointer to int value to compare with
+			 * @new: int value to assign
+			 *
+			 * If (@v == @old), atomically updates @v to @new with full ordering.
+			 * Otherwise, updates @old to the current value of @v.
+			 *
+			 * Unsafe to use in noinstr code; use raw_atomic_try_cmpxchg() there.
+			 * *
+			 * Return: @true if the exchange occured, @false otherwise.
+			 *
+			 * 如果st是SYNC_STATE_ALIVE, 换成SYNC_STATE_SHOULD_ONLINE
+			 */
 			if (!atomic_try_cmpxchg(st, &sync, next_state))
 				continue;
 			return true;
@@ -389,10 +424,30 @@ static inline void cpuhp_bp_sync_dead(unsigned int cpu) { }
  * Updates the AP synchronization state to SYNC_STATE_ALIVE and waits
  * for the BP to release it.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|297| <<start_secondary>> cpuhp_ap_sync_alive();
+ *   - arch/x86/xen/smp_pv.c|64| <<cpu_bringup>> cpuhp_ap_sync_alive();
+ */
 void cpuhp_ap_sync_alive(void)
 {
 	atomic_t *st = this_cpu_ptr(&cpuhp_state.ap_sync_state);
 
+	/*
+	 * enum cpuhp_sync_state {
+	 *     SYNC_STATE_DEAD,
+	 *     SYNC_STATE_KICKED,
+	 *     SYNC_STATE_SHOULD_DIE,
+	 *     SYNC_STATE_ALIVE,
+	 *     SYNC_STATE_SHOULD_ONLINE,
+	 *     SYNC_STATE_ONLINE,
+	 * };
+	 *
+	 * 在以下使用SYNC_STATE_ALIVE:
+	 *   - kernel/cpu.c|396| <<cpuhp_ap_sync_alive>> cpuhp_ap_update_sync_state(SYNC_STATE_ALIVE);
+	 *   - kernel/cpu.c|421| <<cpuhp_can_boot_ap>> case SYNC_STATE_ALIVE:
+	 *   - kernel/cpu.c|454| <<cpuhp_bp_sync_alive>> if (!cpuhp_wait_for_sync_state(cpu, SYNC_STATE_ALIVE, SYNC_STATE_SHOULD_ONLINE)) {
+	 */
 	cpuhp_ap_update_sync_state(SYNC_STATE_ALIVE);
 
 	/* Wait for the control CPU to release it. */
@@ -400,6 +455,11 @@ void cpuhp_ap_sync_alive(void)
 		cpu_relax();
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|815| <<cpuhp_kick_ap_alive>> if (!cpuhp_can_boot_ap(cpu))
+ *   - kernel/cpu.c|925| <<bringup_cpu>> if (!cpuhp_can_boot_ap(cpu))
+ */
 static bool cpuhp_can_boot_ap(unsigned int cpu)
 {
 	atomic_t *st = per_cpu_ptr(&cpuhp_state.ap_sync_state, cpu);
@@ -434,6 +494,11 @@ void __weak arch_cpuhp_cleanup_kick_cpu(unsigned int cpu) { }
  * Early CPU bringup synchronization point. Cannot use cpuhp_state::done_up
  * because the AP cannot issue complete() so early in the bringup.
  */
+/*
+ * called by:
+ *   - kernel/cpu.c|828| <<cpuhp_bringup_ap>> ret = cpuhp_bp_sync_alive(cpu);
+ *   - kernel/cpu.c|872| <<bringup_cpu>> ret = cpuhp_bp_sync_alive(cpu);
+ */
 static int cpuhp_bp_sync_alive(unsigned int cpu)
 {
 	int ret = 0;
@@ -441,6 +506,21 @@ static int cpuhp_bp_sync_alive(unsigned int cpu)
 	if (!IS_ENABLED(CONFIG_HOTPLUG_CORE_SYNC_FULL))
 		return 0;
 
+	/*
+	 * enum cpuhp_sync_state {
+	 *     SYNC_STATE_DEAD,
+	 *     SYNC_STATE_KICKED,
+	 *     SYNC_STATE_SHOULD_DIE,
+	 *     SYNC_STATE_ALIVE,
+	 *     SYNC_STATE_SHOULD_ONLINE,
+	 *     SYNC_STATE_ONLINE,
+	 * };
+	 *
+	 * 在以下使用SYNC_STATE_ALIVE:
+	 *   - kernel/cpu.c|396| <<cpuhp_ap_sync_alive>> cpuhp_ap_update_sync_state(SYNC_STATE_ALIVE);
+	 *   - kernel/cpu.c|421| <<cpuhp_can_boot_ap>> case SYNC_STATE_ALIVE:
+	 *   - kernel/cpu.c|454| <<cpuhp_bp_sync_alive>> if (!cpuhp_wait_for_sync_state(cpu, SYNC_STATE_ALIVE, SYNC_STATE_SHOULD_ONLINE)) {
+	 */
 	if (!cpuhp_wait_for_sync_state(cpu, SYNC_STATE_ALIVE, SYNC_STATE_SHOULD_ONLINE)) {
 		pr_err("CPU%u failed to report alive state\n", cpu);
 		ret = -EIO;
@@ -764,6 +844,12 @@ static void __cpuhp_kick_ap(struct cpuhp_cpu_state *st)
 	wait_for_ap_thread(st, st->bringup);
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|991| <<cpuhp_bringup_ap>> return cpuhp_kick_ap(cpu, st, st->target);
+ *   - kernel/cpu.c|1068| <<bringup_cpu>> return cpuhp_kick_ap(cpu, st, st->target);
+ *   - kernel/cpu.c|1362| <<cpuhp_kick_ap_work>> ret = cpuhp_kick_ap(cpu, st, st->target);
+ */
 static int cpuhp_kick_ap(int cpu, struct cpuhp_cpu_state *st,
 			 enum cpuhp_state target)
 {
@@ -805,14 +891,79 @@ static int bringup_wait_for_ap_online(unsigned int cpu)
 }
 
 #ifdef CONFIG_HOTPLUG_SPLIT_STARTUP
+/*
+ * CPU: 0 PID: 1 Comm: swapper/0 Not tainted 6.7.0 #2
+ * [0] cpuhp_kick_ap_alive
+ * [0] cpuhp_invoke_callback
+ * [0] __cpuhp_invoke_callback_range
+ * [0] _cpu_up
+ * [0] cpu_up
+ * [0] cpuhp_bringup_mask
+ * [0] bringup_nonboot_cpus
+ * [0] smp_init
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ */
 static int cpuhp_kick_ap_alive(unsigned int cpu)
 {
 	if (!cpuhp_can_boot_ap(cpu))
 		return -EAGAIN;
 
+	/*
+	 * called by:
+	 *   - kernel/cpu.c|838| <<cpuhp_kick_ap_alive>> return arch_cpuhp_kick_ap_alive(cpu, idle_thread_get(cpu));
+	 */
 	return arch_cpuhp_kick_ap_alive(cpu, idle_thread_get(cpu));
 }
 
+/*
+ * 2164 #ifdef CONFIG_HOTPLUG_SPLIT_STARTUP
+ * 2165         //
+ * 2166          * Kicks the AP alive. AP will wait in cpuhp_ap_sync_alive() until
+ * 2167          * the next step will release it.
+ * 2168          //
+ * 2169         [CPUHP_BP_KICK_AP] = {
+ * 2170                 .name                   = "cpu:kick_ap",
+ * 2171                 .startup.single         = cpuhp_kick_ap_alive,
+ * 2172         },
+ * 2173
+ * 2174         //
+ * 2175          * Waits for the AP to reach cpuhp_ap_sync_alive() and then
+ * 2176          * releases it for the complete bringup.
+ * 2177          //
+ * 2178         [CPUHP_BRINGUP_CPU] = {
+ * 2179                 .name                   = "cpu:bringup",
+ * 2180                 .startup.single         = cpuhp_bringup_ap,
+ * 2181                 .teardown.single        = finish_cpu,
+ * 2182                 .cant_stop              = true,
+ * 2183         },
+ * 2184 #else
+ * 2185         //
+ * 2186          * All-in-one CPU bringup state which includes the kick alive.
+ * 2187          //
+ * 2188         [CPUHP_BRINGUP_CPU] = {
+ * 2189                 .name                   = "cpu:bringup",
+ * 2190                 .startup.single         = bringup_cpu,
+ * 2191                 .teardown.single        = finish_cpu,
+ * 2192                 .cant_stop              = true,
+ * 2193         },
+ * 2194 #endif
+ *
+ * [0] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 6.7.0 #2
+ * [0] cpuhp_bringup_ap
+ * [0] cpuhp_invoke_callback
+ * [0] __cpuhp_invoke_callback_range
+ * [0] _cpu_up
+ * [0] cpu_up
+ * [0] cpuhp_bringup_mask
+ * [0] smp_init
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ */
 static int cpuhp_bringup_ap(unsigned int cpu)
 {
 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
@@ -825,6 +976,11 @@ static int cpuhp_bringup_ap(unsigned int cpu)
 	 */
 	irq_lock_sparse();
 
+	/*
+	 * called by:
+	 *   - kernel/cpu.c|828| <<cpuhp_bringup_ap>> ret = cpuhp_bp_sync_alive(cpu);
+	 *   - kernel/cpu.c|872| <<bringup_cpu>> ret = cpuhp_bp_sync_alive(cpu);
+	 */
 	ret = cpuhp_bp_sync_alive(cpu);
 	if (ret)
 		goto out_unlock;
@@ -838,6 +994,12 @@ static int cpuhp_bringup_ap(unsigned int cpu)
 	if (st->target <= CPUHP_AP_ONLINE_IDLE)
 		return 0;
 
+	/*
+	 * called by:
+	 *   - kernel/cpu.c|991| <<cpuhp_bringup_ap>> return cpuhp_kick_ap(cpu, st, st->target);
+	 *   - kernel/cpu.c|1068| <<bringup_cpu>> return cpuhp_kick_ap(cpu, st, st->target);
+	 *   - kernel/cpu.c|1362| <<cpuhp_kick_ap_work>> ret = cpuhp_kick_ap(cpu, st, st->target);
+	 */
 	return cpuhp_kick_ap(cpu, st, st->target);
 
 out_unlock:
@@ -845,6 +1007,39 @@ static int cpuhp_bringup_ap(unsigned int cpu)
 	return ret;
 }
 #else
+/*
+ * 2164 #ifdef CONFIG_HOTPLUG_SPLIT_STARTUP
+ * 2165         //
+ * 2166          * Kicks the AP alive. AP will wait in cpuhp_ap_sync_alive() until
+ * 2167          * the next step will release it.
+ * 2168          //
+ * 2169         [CPUHP_BP_KICK_AP] = {
+ * 2170                 .name                   = "cpu:kick_ap",
+ * 2171                 .startup.single         = cpuhp_kick_ap_alive,
+ * 2172         },
+ * 2173
+ * 2174         //
+ * 2175          * Waits for the AP to reach cpuhp_ap_sync_alive() and then
+ * 2176          * releases it for the complete bringup.
+ * 2177          //
+ * 2178         [CPUHP_BRINGUP_CPU] = {
+ * 2179                 .name                   = "cpu:bringup",
+ * 2180                 .startup.single         = cpuhp_bringup_ap,
+ * 2181                 .teardown.single        = finish_cpu,
+ * 2182                 .cant_stop              = true,
+ * 2183         },
+ * 2184 #else
+ * 2185         //
+ * 2186          * All-in-one CPU bringup state which includes the kick alive.
+ * 2187          //
+ * 2188         [CPUHP_BRINGUP_CPU] = {
+ * 2189                 .name                   = "cpu:bringup",
+ * 2190                 .startup.single         = bringup_cpu,
+ * 2191                 .teardown.single        = finish_cpu,
+ * 2192                 .cant_stop              = true,
+ * 2193         },
+ * 2194 #endif
+ */
 static int bringup_cpu(unsigned int cpu)
 {
 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
@@ -1819,6 +2014,14 @@ int bringup_hibernate_cpu(unsigned int sleep_cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|1963| <<cpuhp_bringup_cpus_parallel>> cpuhp_bringup_mask(&tmp_mask, ncpus, CPUHP_BP_KICK_AP);
+ *   - kernel/cpu.c|1964| <<cpuhp_bringup_cpus_parallel>> cpuhp_bringup_mask(&tmp_mask, ncpus, CPUHP_ONLINE);
+ *   - kernel/cpu.c|1975| <<cpuhp_bringup_cpus_parallel>> cpuhp_bringup_mask(mask, ncpus, CPUHP_BP_KICK_AP);
+ *   - kernel/cpu.c|1976| <<cpuhp_bringup_cpus_parallel>> cpuhp_bringup_mask(mask, ncpus, CPUHP_ONLINE);
+ *   - kernel/cpu.c|1990| <<bringup_nonboot_cpus>> cpuhp_bringup_mask(cpu_present_mask, setup_max_cpus, CPUHP_ONLINE);
+ */
 static void __init cpuhp_bringup_mask(const struct cpumask *mask, unsigned int ncpus,
 				      enum cpuhp_state target)
 {
@@ -1828,6 +2031,9 @@ static void __init cpuhp_bringup_mask(const struct cpumask *mask, unsigned int n
 		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 
 		if (cpu_up(cpu, target) && can_rollback_cpu(st)) {
+			/*
+			 * 下面是如果失败的地方
+			 */
 			/*
 			 * If this failed then cpu_up() might have only
 			 * rolled back to CPUHP_BP_KICK_AP for the final
@@ -1842,6 +2048,13 @@ static void __init cpuhp_bringup_mask(const struct cpumask *mask, unsigned int n
 }
 
 #ifdef CONFIG_HOTPLUG_PARALLEL
+/*
+ * 在以下使用__cpuhp_parallel_bringup:
+ *   - kernel/cpu.c|1961| <<parallel_bringup_parse_param>> return kstrtobool(arg, &__cpuhp_parallel_bringup); --> "cpuhp.parallel"
+ *   - kernel/cpu.c|1993| <<cpuhp_bringup_cpus_parallel>> if (__cpuhp_parallel_bringup)
+ *   - kernel/cpu.c|1994| <<cpuhp_bringup_cpus_parallel>> __cpuhp_parallel_bringup = arch_cpuhp_init_parallel_bringup();
+ *   - kernel/cpu.c|1995| <<cpuhp_bringup_cpus_parallel>> if (!__cpuhp_parallel_bringup)
+ */
 static bool __cpuhp_parallel_bringup __ro_after_init = true;
 
 static int __init parallel_bringup_parse_param(char *arg)
@@ -1860,6 +2073,39 @@ static inline const struct cpumask *cpuhp_get_primary_thread_mask(void)
 	return cpu_primary_thread_mask;
 }
 
+/*
+ * 2164 #ifdef CONFIG_HOTPLUG_SPLIT_STARTUP
+ * 2165         //
+ * 2166          * Kicks the AP alive. AP will wait in cpuhp_ap_sync_alive() until
+ * 2167          * the next step will release it.
+ * 2168          //
+ * 2169         [CPUHP_BP_KICK_AP] = {
+ * 2170                 .name                   = "cpu:kick_ap",
+ * 2171                 .startup.single         = cpuhp_kick_ap_alive,
+ * 2172         },
+ * 2173
+ * 2174         //
+ * 2175          * Waits for the AP to reach cpuhp_ap_sync_alive() and then
+ * 2176          * releases it for the complete bringup.
+ * 2177          //
+ * 2178         [CPUHP_BRINGUP_CPU] = {
+ * 2179                 .name                   = "cpu:bringup",
+ * 2180                 .startup.single         = cpuhp_bringup_ap,
+ * 2181                 .teardown.single        = finish_cpu,
+ * 2182                 .cant_stop              = true,
+ * 2183         },
+ * 2184 #else
+ * 2185         //
+ * 2186          * All-in-one CPU bringup state which includes the kick alive.
+ * 2187          //
+ * 2188         [CPUHP_BRINGUP_CPU] = {
+ * 2189                 .name                   = "cpu:bringup",
+ * 2190                 .startup.single         = bringup_cpu,
+ * 2191                 .teardown.single        = finish_cpu,
+ * 2192                 .cant_stop              = true,
+ * 2193         },
+ * 2194 #endif
+ */
 /*
  * On architectures which have enabled parallel bringup this invokes all BP
  * prepare states for each of the to be onlined APs first. The last state
@@ -1870,10 +2116,21 @@ static inline const struct cpumask *cpuhp_get_primary_thread_mask(void)
  * This avoids waiting for each AP to respond to the startup IPI in
  * CPUHP_BRINGUP_CPU.
  */
+/*
+ * called by:
+ *   - kernel/cpu.c|2031| <<bringup_nonboot_cpus>> if (cpuhp_bringup_cpus_parallel(setup_max_cpus))
+ */
 static bool __init cpuhp_bringup_cpus_parallel(unsigned int ncpus)
 {
 	const struct cpumask *mask = cpu_present_mask;
 
+	/*
+	 * 在以下使用__cpuhp_parallel_bringup:
+	 *   - kernel/cpu.c|1961| <<parallel_bringup_parse_param>> return kstrtobool(arg, &__cpuhp_parallel_bringup); --> "cpuhp.parallel"
+	 *   - kernel/cpu.c|1993| <<cpuhp_bringup_cpus_parallel>> if (__cpuhp_parallel_bringup)
+	 *   - kernel/cpu.c|1994| <<cpuhp_bringup_cpus_parallel>> __cpuhp_parallel_bringup = arch_cpuhp_init_parallel_bringup();
+	 *   - kernel/cpu.c|1995| <<cpuhp_bringup_cpus_parallel>> if (!__cpuhp_parallel_bringup)
+	 */
 	if (__cpuhp_parallel_bringup)
 		__cpuhp_parallel_bringup = arch_cpuhp_init_parallel_bringup();
 	if (!__cpuhp_parallel_bringup)
@@ -1909,12 +2166,28 @@ static bool __init cpuhp_bringup_cpus_parallel(unsigned int ncpus)
 static inline bool cpuhp_bringup_cpus_parallel(unsigned int ncpus) { return false; }
 #endif /* CONFIG_HOTPLUG_PARALLEL */
 
+/*
+ * called by:
+ *   - kernel/smp.c|986| <<smp_init>> bringup_nonboot_cpus(setup_max_cpus);
+ */
 void __init bringup_nonboot_cpus(unsigned int setup_max_cpus)
 {
+	/*
+	 * called by:
+	 *   - kernel/cpu.c|2031| <<bringup_nonboot_cpus>> if (cpuhp_bringup_cpus_parallel(setup_max_cpus))
+	 */
 	/* Try parallel bringup optimization if enabled */
 	if (cpuhp_bringup_cpus_parallel(setup_max_cpus))
 		return;
 
+	/*
+	 * called by:
+	 *   - kernel/cpu.c|1963| <<cpuhp_bringup_cpus_parallel>> cpuhp_bringup_mask(&tmp_mask, ncpus, CPUHP_BP_KICK_AP);
+	 *   - kernel/cpu.c|1964| <<cpuhp_bringup_cpus_parallel>> cpuhp_bringup_mask(&tmp_mask, ncpus, CPUHP_ONLINE);
+	 *   - kernel/cpu.c|1975| <<cpuhp_bringup_cpus_parallel>> cpuhp_bringup_mask(mask, ncpus, CPUHP_BP_KICK_AP);
+	 *   - kernel/cpu.c|1976| <<cpuhp_bringup_cpus_parallel>> cpuhp_bringup_mask(mask, ncpus, CPUHP_ONLINE);
+	 *   - kernel/cpu.c|1990| <<bringup_nonboot_cpus>> cpuhp_bringup_mask(cpu_present_mask, setup_max_cpus, CPUHP_ONLINE);
+	 */
 	/* Full per CPU serialized bringup */
 	cpuhp_bringup_mask(cpu_present_mask, setup_max_cpus, CPUHP_ONLINE);
 }
diff --git a/kernel/smp.c b/kernel/smp.c
index f085ebcdf..3d79e3c75 100644
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@ -46,6 +46,12 @@ static DEFINE_PER_CPU_ALIGNED(struct call_function_data, cfd_data);
 
 static DEFINE_PER_CPU_SHARED_ALIGNED(struct llist_head, call_single_queue);
 
+/*
+ * 在以下使用trigger_backtrace:
+ *   - kernel/smp.c|49| <<global>> static DEFINE_PER_CPU(atomic_t, trigger_backtrace) = ATOMIC_INIT(1);
+ *   - kernel/smp.c|269| <<csd_lock_wait_toolong>> if (atomic_cmpxchg_acquire(&per_cpu(trigger_backtrace, cpu), 1, 0))
+ *   - kernel/smp.c|456| <<__flush_smp_call_function_queue>> tbt = this_cpu_ptr(&trigger_backtrace);
+ */
 static DEFINE_PER_CPU(atomic_t, trigger_backtrace) = ATOMIC_INIT(1);
 
 static void __flush_smp_call_function_queue(bool warn_cpu_offline);
diff --git a/kernel/user-return-notifier.c b/kernel/user-return-notifier.c
index 870ecd7c6..374e9bb50 100644
--- a/kernel/user-return-notifier.c
+++ b/kernel/user-return-notifier.c
@@ -12,6 +12,10 @@ static DEFINE_PER_CPU(struct hlist_head, return_notifier_list);
  * called in atomic context.  The notifier will also be called in atomic
  * context.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|480| <<kvm_set_user_return_msr>> user_return_notifier_register(&msrs->urn);
+ */
 void user_return_notifier_register(struct user_return_notifier *urn)
 {
 	set_tsk_thread_flag(current, TIF_USER_RETURN_NOTIFY);
@@ -31,6 +35,10 @@ void user_return_notifier_unregister(struct user_return_notifier *urn)
 }
 EXPORT_SYMBOL_GPL(user_return_notifier_unregister);
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/entry-common.h|51| <<arch_exit_to_user_mode_prepare>> fire_user_return_notifiers();
+ */
 /* Calls registered user return notifiers */
 void fire_user_return_notifiers(void)
 {
diff --git a/lib/nmi_backtrace.c b/lib/nmi_backtrace.c
index 33c154264..191efd818 100644
--- a/lib/nmi_backtrace.c
+++ b/lib/nmi_backtrace.c
@@ -22,8 +22,29 @@
 
 #ifdef arch_trigger_cpumask_backtrace
 /* For reliability, we're prepared to waste bits here. */
+/*
+ * 在以下使用backtrace_mask:
+ *   - lib/nmi_backtrace.c|25| <<DECLARE_BITMAP>> static DECLARE_BITMAP(backtrace_mask, NR_CPUS) __read_mostly;
+ *   - lib/nmi_backtrace.c|51| <<nmi_trigger_cpumask_backtrace>> cpumask_copy(to_cpumask(backtrace_mask), mask);
+ *   - lib/nmi_backtrace.c|53| <<nmi_trigger_cpumask_backtrace>> cpumask_clear_cpu(exclude_cpu, to_cpumask(backtrace_mask));
+ *   - lib/nmi_backtrace.c|61| <<nmi_trigger_cpumask_backtrace>> if (cpumask_test_cpu(this_cpu, to_cpumask(backtrace_mask)))
+ *   - lib/nmi_backtrace.c|64| <<nmi_trigger_cpumask_backtrace>> if (!cpumask_empty(to_cpumask(backtrace_mask))) {
+ *   - lib/nmi_backtrace.c|66| <<nmi_trigger_cpumask_backtrace>> this_cpu, nr_cpumask_bits, to_cpumask(backtrace_mask));
+ *   - lib/nmi_backtrace.c|67| <<nmi_trigger_cpumask_backtrace>> nmi_backtrace_stall_snap(to_cpumask(backtrace_mask));
+ *   - lib/nmi_backtrace.c|68| <<nmi_trigger_cpumask_backtrace>> raise(to_cpumask(backtrace_mask));
+ *   - lib/nmi_backtrace.c|73| <<nmi_trigger_cpumask_backtrace>> if (cpumask_empty(to_cpumask(backtrace_mask)))
+ *   - lib/nmi_backtrace.c|78| <<nmi_trigger_cpumask_backtrace>> nmi_backtrace_stall_check(to_cpumask(backtrace_mask));
+ *   - lib/nmi_backtrace.c|99| <<nmi_cpu_backtrace>> if (cpumask_test_cpu(cpu, to_cpumask(backtrace_mask))) {
+ *   - lib/nmi_backtrace.c|116| <<nmi_cpu_backtrace>> cpumask_clear_cpu(cpu, to_cpumask(backtrace_mask));
+ */
 static DECLARE_BITMAP(backtrace_mask, NR_CPUS) __read_mostly;
 
+/*
+ * 在以下使用backtrace_flag:
+ *   - lib/nmi_backtrace.c|28| <<DECLARE_BITMAP>> static unsigned long backtrace_flag;
+ *   - lib/nmi_backtrace.c|42| <<nmi_trigger_cpumask_backtrace>> if (test_and_set_bit(0, &backtrace_flag)) {
+ *   - lib/nmi_backtrace.c|86| <<nmi_trigger_cpumask_backtrace>> clear_bit_unlock(0, &backtrace_flag);
+ */
 /* "in progress" flag of arch_trigger_cpumask_backtrace */
 static unsigned long backtrace_flag;
 
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 6feb3e063..176844cc5 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -4824,6 +4824,27 @@ static nodemask_t *policy_mbind_nodemask(gfp_t gfp)
 	return NULL;
 }
 
+/*
+ * 比如:
+ * struct hugetlbfs_sb_info {
+ *     long    max_inodes;   // inodes allowed
+ *     long    free_inodes;  // inodes free
+ *     spinlock_t      stat_lock;
+ *     struct hstate *hstate;
+ *     struct hugepage_subpool *spool;
+ *     kuid_t  uid;
+ *     kgid_t  gid;
+ *     umode_t mode;
+ * };
+ *
+ * called by:
+ *   - mm/hugetlb.c|5096| <<hugetlb_acct_memory>> if (delta > allowed_mems_nr(h)) {
+ *
+ * hugetlbfs_file_mmap()
+ * -> hugetlb_reserve_pages()
+ *    -> hugetlb_acct_memory(h, gbl_reserve)
+ *       -> allowed_mems_nr()
+ */
 static unsigned int allowed_mems_nr(struct hstate *h)
 {
 	int node;
@@ -5046,6 +5067,31 @@ unsigned long hugetlb_total_pages(void)
 	return nr_total_pages;
 }
 
+/*
+ * called by:
+ *   - mm/hugetlb.c|124| <<unlock_or_release_subpool>> hugetlb_acct_memory(spool->hstate,
+ *   - mm/hugetlb.c|145| <<hugepage_new_subpool>> if (min_hpages != -1 && hugetlb_acct_memory(h, min_hpages)) {
+ *   - mm/hugetlb.c|944| <<hugetlb_fix_reserve_counts>> if (!hugetlb_acct_memory(h, 1))
+ *   - mm/hugetlb.c|3268| <<alloc_hugetlb_folio>> hugetlb_acct_memory(h, -rsv_adjust);
+ *   - mm/hugetlb.c|5162| <<hugetlb_vm_op_close>> hugetlb_acct_memory(h, -gbl_reserve);
+ *   - mm/hugetlb.c|7100| <<hugetlb_reserve_pages>> if (hugetlb_acct_memory(h, gbl_reserve) < 0)
+ *   - mm/hugetlb.c|7118| <<hugetlb_reserve_pages>> hugetlb_acct_memory(h, -gbl_reserve);
+ *   - mm/hugetlb.c|7140| <<hugetlb_reserve_pages>> hugetlb_acct_memory(h, -rsv_adjust);
+ *   - mm/hugetlb.c|7210| <<hugetlb_unreserve_pages>> hugetlb_acct_memory(h, -gbl_reserve);
+ *
+ * struct hugetlbfs_sb_info {
+ *     long    max_inodes;   // inodes allowed
+ *     long    free_inodes;  // inodes free
+ *     spinlock_t      stat_lock;
+ *     struct hstate *hstate;
+ *     struct hugepage_subpool *spool;
+ *     kuid_t  uid;
+ *     kgid_t  gid;
+ *     umode_t mode;
+ * };
+ *
+ * h是HUGETLBFS_SB(i->i_sb)->hstate;
+ */
 static int hugetlb_acct_memory(struct hstate *h, long delta)
 {
 	int ret = -ENOMEM;
@@ -6113,6 +6159,10 @@ static bool hugetlb_pte_stable(struct hstate *h, struct mm_struct *mm,
 	return same;
 }
 
+/*
+ * called by:
+ *   - mm/hugetlb.c|6412| <<hugetlb_fault>> return hugetlb_no_page(mm, vma, mapping, idx, address, ptep,
+ */
 static vm_fault_t hugetlb_no_page(struct mm_struct *mm,
 			struct vm_area_struct *vma,
 			struct address_space *mapping, pgoff_t idx,
@@ -6347,6 +6397,10 @@ u32 hugetlb_fault_mutex_hash(struct address_space *mapping, pgoff_t idx)
 }
 #endif
 
+/*
+ * called by:
+ *   - mm/memory.c|5343| <<handle_mm_fault>> ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
+ */
 vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, unsigned int flags)
 {
@@ -6997,6 +7051,11 @@ long hugetlb_change_protection(struct vm_area_struct *vma,
 	return pages > 0 ? (pages << h->order) : pages;
 }
 
+/*
+ * called by:
+ *   - fs/hugetlbfs/inode.c|144| <<hugetlbfs_file_mmap>> if (!hugetlb_reserve_pages(inode, vma->vm_pgoff >> huge_page_order(h), len >> huge_page_shift(h), vma, vma->vm_flags))
+ *   - fs/hugetlbfs/inode.c|1618| <<hugetlb_file_setup>> if (!hugetlb_reserve_pages(inode, 0, size >> huge_page_shift(hstate_inode(inode)), NULL, acctflag))
+ */
 /* Return true if reservation was successful, false otherwise.  */
 bool hugetlb_reserve_pages(struct inode *inode,
 					long from, long to,
@@ -7084,6 +7143,9 @@ bool hugetlb_reserve_pages(struct inode *inode,
 	 * Check enough hugepages are available for the reservation.
 	 * Hand the pages back to the subpool if there are not
 	 */
+	/*
+	 * h是HUGETLBFS_SB(i->i_sb)->hstate;
+	 */
 	if (hugetlb_acct_memory(h, gbl_reserve) < 0)
 		goto out_put_pages;
 
diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c
index ec3b068cb..5661006c2 100644
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@ -711,6 +711,16 @@ EXPORT_SYMBOL_GPL(__mmu_notifier_register);
  * While the caller has a mmu_notifier get the subscription->mm pointer will remain
  * valid, and can be converted to an active mm pointer via mmget_not_zero().
  */
+/*
+ * called by:
+ *   - arch/s390/kvm/pv.c|605| <<kvm_s390_pv_init_vm>> mmu_notifier_register(&kvm->arch.pv.mmu_notifier, kvm->mm);
+ *   - drivers/infiniband/hw/hfi1/mmu_rb.c|68| <<hfi1_mmu_rb_register>> ret = mmu_notifier_register(&h->mn, current->mm);
+ *   - drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c|320| <<arm_smmu_mmu_notifier_get>> ret = mmu_notifier_register(&smmu_mn->mn, mm);
+ *   - drivers/iommu/intel/svm.c|338| <<intel_svm_bind_mm>> ret = mmu_notifier_register(&svm->notifier, mm);
+ *   - drivers/misc/ocxl/link.c|596| <<ocxl_link_add_pe>> mmu_notifier_register(&pe_data->mmu_notifier, mm);
+ *   - mm/mmu_notifier.c|999| <<mmu_interval_notifier_insert>> ret = mmu_notifier_register(NULL, mm);
+ *   - virt/kvm/kvm_main.c|932| <<kvm_init_mmu_notifier>> return mmu_notifier_register(&kvm->mmu_notifier, current->mm);
+ */
 int mmu_notifier_register(struct mmu_notifier *subscription,
 			  struct mm_struct *mm)
 {
diff --git a/tools/testing/selftests/kselftest_harness.h b/tools/testing/selftests/kselftest_harness.h
index e05ac8261..d94c2a782 100644
--- a/tools/testing/selftests/kselftest_harness.h
+++ b/tools/testing/selftests/kselftest_harness.h
@@ -1153,6 +1153,17 @@ void __run_test(struct __fixture_metadata *f,
 			f->name, variant->name[0] ? "." : "", variant->name, t->name);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/drivers/s390x/uvdevice/test_uvdevice.c|275| <<main>> return test_harness_run(argc, argv);
+ *   - tools/testing/selftests/hid/hid_bpf.c|868| <<main>> return test_harness_run(argc, argv);
+ *   - tools/testing/selftests/kselftest_harness.h|436| <<TEST_HARNESS_MAIN>> return test_harness_run(argc, argv); \
+ *   - tools/testing/selftests/rtc/rtctest.c|433| <<main>> return test_harness_run(argc, argv);
+ *   - tools/testing/selftests/user_events/abi_test.c|288| <<main>> return test_harness_run(argc, argv);
+ *   - tools/testing/selftests/user_events/dyn_test.c|293| <<main>> return test_harness_run(argc, argv);
+ *   - tools/testing/selftests/user_events/ftrace_test.c|588| <<main>> return test_harness_run(argc, argv);
+ *   - tools/testing/selftests/user_events/perf_test.c|253| <<main>> return test_harness_run(argc, argv);
+ */
 static int test_harness_run(int argc, char **argv)
 {
 	struct __fixture_variant_metadata no_variant = { .name = "", };
diff --git a/tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c b/tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c
index 5ea78986e..42738e3da 100644
--- a/tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c
+++ b/tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c
@@ -424,6 +424,11 @@ static void guest_code(uint64_t expected_pmcr_n)
 #define GICD_BASE_GPA	0x8000000ULL
 #define GICR_BASE_GPA	0x80A0000ULL
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|505| <<test_create_vpmu_vm_with_pmcr_n>> create_vpmu_vm(guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vpmu_counter_access.c|648| <<get_pmcr_n_limit>> create_vpmu_vm(guest_code);
+ */
 /* Create a VM that has one vCPU with PMUv3 configured. */
 static void create_vpmu_vm(void *guest_code)
 {
diff --git a/tools/testing/selftests/kvm/dirty_log_test.c b/tools/testing/selftests/kvm/dirty_log_test.c
index 936f3a8d1..cd1ed1767 100644
--- a/tools/testing/selftests/kvm/dirty_log_test.c
+++ b/tools/testing/selftests/kvm/dirty_log_test.c
@@ -56,6 +56,9 @@
 # define __test_and_clear_bit_le(nr, addr) \
 	__test_and_clear_bit((nr) ^ BITOP_LE_SWIZZLE, addr)
 #else
+/*
+ * 下面的不是s390x
+ */
 # define test_bit_le			test_bit
 # define __set_bit_le			__set_bit
 # define __clear_bit_le			__clear_bit
@@ -75,7 +78,24 @@
  */
 static uint64_t host_page_size;
 static uint64_t guest_page_size;
+/*
+ * 在以下使用guest_page_size:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|110| <<guest_code>> for (i = 0; i < guest_num_pages; i++) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|118| <<guest_code>> addr += (READ_ONCE(random_array[i]) % guest_num_pages)
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|745| <<run_test>> guest_num_pages = (1ul << (DIRTY_MEM_BITS - vm->page_shift)) + 3;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|746| <<run_test>> guest_num_pages = vm_adjust_num_guest_pages(mode, guest_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|749| <<run_test>> host_num_pages = vm_num_host_pages(mode, guest_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|752| <<run_test>> guest_test_phys_mem = (vm->max_gfn - guest_num_pages) *
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|773| <<run_test>> guest_num_pages,
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|777| <<run_test>> virt_map(vm, guest_test_virt_mem, guest_test_phys_mem, guest_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|786| <<run_test>> sync_global_to_guest(vm, guest_num_pages);
+ */
 static uint64_t guest_num_pages;
+/*
+ * 在以下使用random_array[TEST_PAGES_PER_LOOP]:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|133| <<guest_code>> addr += (READ_ONCE(random_array[i]) % guest_num_pages)
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|585| <<vcpu_worker>> guest_array = addr_gva2hva(vm, (vm_vaddr_t)random_array);
+ */
 static uint64_t random_array[TEST_PAGES_PER_LOOP];
 static uint64_t iteration;
 
@@ -107,14 +127,27 @@ static void guest_code(void)
 	 * To compensate this specialty in this test, we need to touch all
 	 * pages during the first iteration.
 	 */
+	/*
+	 * 在每个page的前8个byte写字
+	 *
+	 * 因为是第一次启动执行, 所以特别慢
+	 */
 	for (i = 0; i < guest_num_pages; i++) {
 		addr = guest_test_virt_mem + i * guest_page_size;
 		*(uint64_t *)addr = READ_ONCE(iteration);
 	}
 
 	while (true) {
+		/*
+		 * 1024 for loop
+		 */
 		for (i = 0; i < TEST_PAGES_PER_LOOP; i++) {
 			addr = guest_test_virt_mem;
+			/*
+			 * 在以下使用random_array[TEST_PAGES_PER_LOOP]:
+			 *   - tools/testing/selftests/kvm/dirty_log_test.c|133| <<guest_code>> addr += (READ_ONCE(random_array[i]) % guest_num_pages)
+			 *   - tools/testing/selftests/kvm/dirty_log_test.c|585| <<vcpu_worker>> guest_array = addr_gva2hva(vm, (vm_vaddr_t)random_array);
+			 */
 			addr += (READ_ONCE(random_array[i]) % guest_num_pages)
 				* guest_page_size;
 			addr = align_down(addr, host_page_size);
@@ -127,19 +160,73 @@ static void guest_code(void)
 }
 
 /* Host variables */
+/*
+ * 在以下使用host_quit:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|587| <<vcpu_worker>> while (!READ_ONCE(host_quit)) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|835| <<run_test>> host_quit = false;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|879| <<run_test>> host_quit = true;
+ */
 static bool host_quit;
 
+/*
+ * 在以下使用host_test_mem:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|618| <<vm_dirty_log_verify>> value_ptr = host_test_mem + page * host_page_size;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|824| <<run_test>> host_test_mem = addr_gpa2hva(vm, (vm_paddr_t)guest_test_phys_mem);
+ */
 /* Points to the test VM memory region on which we track dirty logs */
 static void *host_test_mem;
+/*
+ * 在以下使用host_num_pages:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|649| <<vm_dirty_log_verify>> for (page = 0; page < host_num_pages; page += step) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|825| <<run_test>> host_num_pages = vm_num_host_pages(mode, guest_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|842| <<run_test>> bmap = bitmap_zalloc(host_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|843| <<run_test>> host_bmap_track = bitmap_zalloc(host_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|882| <<run_test>> log_mode_collect_dirty_pages(vcpu, TEST_MEM_SLOT_INDEX, bmap, host_num_pages, &ring_buf_idx);
+ */
 static uint64_t host_num_pages;
 
 /* For statistics only */
+/*
+ * 在以下使用host_dirty_count:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|614| <<vm_dirty_log_verify>> host_dirty_count++;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|818| <<run_test>> host_dirty_count = 0;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|860| <<run_test>> "track_next (%"PRIu64")\n", host_dirty_count, host_clear_count,
+ */
 static uint64_t host_dirty_count;
+/*
+ * 在以下使用host_clear_count:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1000| <<vm_dirty_log_verify>> host_clear_count++;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1148| <<run_test>> host_clear_count = 0;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1204| <<run_test>> "track_next (%"PRIu64")\n", host_dirty_count, host_clear_count,
+ */
 static uint64_t host_clear_count;
+/*
+ * 在以下使用host_track_next_count:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|995| <<vm_dirty_log_verify>> host_track_next_count++;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1220| <<run_test>> host_track_next_count = 0;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1276| <<run_test>> host_track_next_count);
+ */
 static uint64_t host_track_next_count;
 
 /* Whether dirty ring reset is requested, or finished */
+/*
+ * 在以下使用sem_vcpu_stop:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|374| <<vcpu_handle_sync_stop>> sem_post(&sem_vcpu_stop);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|492| <<dirty_ring_wait_vcpu>> sem_wait_until(&sem_vcpu_stop);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|569| <<dirty_ring_after_vcpu_run>> sem_post(&sem_vcpu_stop);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1062| <<run_test>> sem_wait_until(&sem_vcpu_stop);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1128| <<main>> sem_init(&sem_vcpu_stop, 0, 0);
+ */
 static sem_t sem_vcpu_stop;
+/*
+ * 在以下使用sem_vcpu_cont:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|375| <<vcpu_handle_sync_stop>> sem_wait_until(&sem_vcpu_cont);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|498| <<dirty_ring_continue_vcpu>> sem_post(&sem_vcpu_cont);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|573| <<dirty_ring_after_vcpu_run>> sem_wait_until(&sem_vcpu_cont);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|589| <<dirty_ring_before_vcpu_join>> sem_post(&sem_vcpu_cont);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1076| <<run_test>> sem_post(&sem_vcpu_cont);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1129| <<main>> sem_init(&sem_vcpu_cont, 0, 0);
+ */
 static sem_t sem_vcpu_cont;
 /*
  * This is only set by main thread, and only cleared by vcpu thread.  It is
@@ -148,12 +235,27 @@ static sem_t sem_vcpu_cont;
  * will match.  E.g., SIG_IPI won't guarantee that if the vcpu is interrupted
  * after setting dirty bit but before the data is written.
  */
+/*
+ * 在以下使用vcpu_sync_stop_requested:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|371| <<vcpu_handle_sync_stop>> if (atomic_read(&vcpu_sync_stop_requested)) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|373| <<vcpu_handle_sync_stop>> atomic_set(&vcpu_sync_stop_requested, false);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1061| <<run_test>> atomic_set(&vcpu_sync_stop_requested, true);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1071| <<run_test>> atomic_read(&vcpu_sync_stop_requested) == false);
+ */
 static atomic_t vcpu_sync_stop_requested;
 /*
  * This is updated by the vcpu thread to tell the host whether it's a
  * ring-full event.  It should only be read until a sem_wait() of
  * sem_vcpu_stop and before vcpu continues to run.
  */
+/*
+ * 在以下使用dirty_ring_vcpu_ring_full:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|411| <<dirty_ring_collect_dirty_pages>> if (!dirty_ring_vcpu_ring_full) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|432| <<dirty_ring_collect_dirty_pages>> TEST_ASSERT(dirty_ring_vcpu_ring_full,
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|451| <<dirty_ring_after_vcpu_run>> WRITE_ONCE(dirty_ring_vcpu_ring_full,
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|455| <<dirty_ring_after_vcpu_run>> dirty_ring_vcpu_ring_full ?
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|871| <<run_test>> WRITE_ONCE(dirty_ring_vcpu_ring_full, false);
+ */
 static bool dirty_ring_vcpu_ring_full;
 /*
  * This is only used for verifying the dirty pages.  Dirty ring has a very
@@ -169,6 +271,11 @@ static bool dirty_ring_vcpu_ring_full;
  * dirty gfn we've collected, so that if a mismatch of data found later in the
  * verifying process, we let it pass.
  */
+/*
+ * 在以下使用dirty_ring_last_page:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|545| <<dirty_ring_collect_one>> dirty_ring_last_page = cur->offset;
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1030| <<vm_dirty_log_verify>> } else if (page == dirty_ring_last_page) {
+ */
 static uint64_t dirty_ring_last_page;
 
 enum log_mode_t {
@@ -189,11 +296,40 @@ enum log_mode_t {
 
 /* Mode of logging to test.  Default is to run all supported modes */
 static enum log_mode_t host_log_mode_option = LOG_MODE_ALL;
+/*
+ * 在以下使用host_log_mode:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|529| <<log_mode_supported>> struct log_mode *mode = &log_modes[host_log_mode];
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|539| <<log_mode_create_vm_done>> struct log_mode *mode = &log_modes[host_log_mode];
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|553| <<log_mode_collect_dirty_pages>> struct log_mode *mode = &log_modes[host_log_mode];
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|562| <<log_mode_after_vcpu_run>> struct log_mode *mode = &log_modes[host_log_mode];
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|574| <<log_mode_before_vcpu_join>> struct log_mode *mode = &log_modes[host_log_mode];
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|674| <<vm_dirty_log_verify>> if (host_log_mode == LOG_MODE_DIRTY_RING && !matched) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|797| <<run_test>> log_modes[host_log_mode].name);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|898| <<run_test>> assert(host_log_mode == LOG_MODE_DIRTY_RING ||
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1025| <<main>> host_log_mode = i;
+ */
 /* Logging mode for current run */
 static enum log_mode_t host_log_mode;
 static pthread_t vcpu_thread;
+/*
+ * 在以下使用test_dirty_ring_count:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|428| <<dirty_ring_create_vm_done>> test_dirty_ring_count = 1 << (31 - __builtin_clz(test_dirty_ring_count));
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|429| <<dirty_ring_create_vm_done>> test_dirty_ring_count = min(limit, test_dirty_ring_count);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|430| <<dirty_ring_create_vm_done>> pr_info("dirty ring count: 0x%x\n", test_dirty_ring_count);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|436| <<dirty_ring_create_vm_done>> vm_enable_dirty_ring(vm, test_dirty_ring_count *
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|470| <<dirty_ring_collect_one>> cur = &dirty_gfns[*fetch_index % test_dirty_ring_count];
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1136| <<main>> test_dirty_ring_count = strtol(optarg, NULL, 10);
+ *
+ * 65536
+ */
 static uint32_t test_dirty_ring_count = TEST_DIRTY_RING_COUNT;
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|392| <<dirty_ring_wait_vcpu>> vcpu_kick();
+ *
+ * 调用pthread_kill(vcpu_thread, SIG_IPI)
+ */
 static void vcpu_kick(void)
 {
 	pthread_kill(vcpu_thread, SIG_IPI);
@@ -203,6 +339,13 @@ static void vcpu_kick(void)
  * In our test we do signal tricks, let's use a better version of
  * sem_wait to avoid signal interrupts
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|375| <<vcpu_handle_sync_stop>> sem_wait_until(&sem_vcpu_cont);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|492| <<dirty_ring_wait_vcpu>> sem_wait_until(&sem_vcpu_stop);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|573| <<dirty_ring_after_vcpu_run>> sem_wait_until(&sem_vcpu_cont);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1062| <<run_test>> sem_wait_until(&sem_vcpu_stop);
+ */
 static void sem_wait_until(sem_t *sem)
 {
 	int ret;
@@ -212,11 +355,19 @@ static void sem_wait_until(sem_t *sem)
 	while (ret == -1 && errno == EINTR);
 }
 
+/*
+ * 在以下使用clear_log_supported():
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|613| <<global("clear-log")>> .supported = clear_log_supported,
+ */
 static bool clear_log_supported(void)
 {
 	return kvm_has_cap(KVM_CAP_MANUAL_DIRTY_LOG_PROTECT2);
 }
 
+/*
+ * 在以下使用clear_log_create_vm_done():
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|614| <<global("clear-log")>> .create_vm_done = clear_log_create_vm_done,
+ */
 static void clear_log_create_vm_done(struct kvm_vm *vm)
 {
 	u64 manual_caps;
@@ -228,24 +379,70 @@ static void clear_log_create_vm_done(struct kvm_vm *vm)
 	vm_enable_cap(vm, KVM_CAP_MANUAL_DIRTY_LOG_PROTECT2, manual_caps);
 }
 
+/*
+ * 在以下使用dirty_log_collect_dirty_pages():
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|567| <<global("dirty-log")>> .collect_dirty_pages = dirty_log_collect_dirty_pages,
+ */
 static void dirty_log_collect_dirty_pages(struct kvm_vcpu *vcpu, int slot,
 					  void *bitmap, uint32_t num_pages,
 					  uint32_t *unused)
 {
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|461| <<check_write_in_dirty_log>> kvm_vm_get_dirty_log(vm, region->region.slot, bmap);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|235| <<dirty_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|242| <<clear_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|531| <<memstress_get_dirty_log>> kvm_vm_get_dirty_log(vm, slot, bitmaps[i]);
+	 *   - tools/testing/selftests/kvm/x86_64/vmx_dirty_log_test.c|129| <<main>> kvm_vm_get_dirty_log(vm, TEST_MEM_SLOT_INDEX, bmap);
+	 *
+	 * 调用KVM_GET_DIRTY_LOG
+	 */
 	kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
 }
 
+/*
+ * 在以下使用clear_log_collect_dirty_pages():
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|574| <<global("clear-log")>> .collect_dirty_pages = clear_log_collect_dirty_pages,
+ */
 static void clear_log_collect_dirty_pages(struct kvm_vcpu *vcpu, int slot,
 					  void *bitmap, uint32_t num_pages,
 					  uint32_t *unused)
 {
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|461| <<check_write_in_dirty_log>> kvm_vm_get_dirty_log(vm, region->region.slot, bmap);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|235| <<dirty_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|242| <<clear_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|531| <<memstress_get_dirty_log>> kvm_vm_get_dirty_log(vm, slot, bitmaps[i]);
+	 *   - tools/testing/selftests/kvm/x86_64/vmx_dirty_log_test.c|129| <<main>> kvm_vm_get_dirty_log(vm, TEST_MEM_SLOT_INDEX, bmap);
+	 *
+	 * 调用KVM_GET_DIRTY_LOG
+	 */
 	kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|243| <<clear_log_collect_dirty_pages>> kvm_vm_clear_dirty_log(vcpu->vm, slot, bitmap, 0, num_pages);
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|543| <<memstress_clear_dirty_log>> kvm_vm_clear_dirty_log(vm, slot, bitmaps[i], 0, pages_per_slot);
+	 *
+	 * 调用KVM_CLEAR_DIRTY_LOG
+	 */
 	kvm_vm_clear_dirty_log(vcpu->vm, slot, bitmap, 0, num_pages);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|316| <<default_after_vcpu_run>> vcpu_handle_sync_stop();
+ */
 /* Should only be called after a GUEST_SYNC */
 static void vcpu_handle_sync_stop(void)
 {
+	/*
+	 * 在以下使用vcpu_sync_stop_requested:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|371| <<vcpu_handle_sync_stop>> if (atomic_read(&vcpu_sync_stop_requested)) {
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|373| <<vcpu_handle_sync_stop>> atomic_set(&vcpu_sync_stop_requested, false);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|1061| <<run_test>> atomic_set(&vcpu_sync_stop_requested, true);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|1071| <<run_test>> atomic_read(&vcpu_sync_stop_requested) == false);
+	 */
 	if (atomic_read(&vcpu_sync_stop_requested)) {
 		/* It means main thread is sleeping waiting */
 		atomic_set(&vcpu_sync_stop_requested, false);
@@ -254,6 +451,11 @@ static void vcpu_handle_sync_stop(void)
 	}
 }
 
+/*
+ * 在以下使用default_after_vcpu_run():
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|555| <<global("dirty-log")>> .after_vcpu_run = default_after_vcpu_run,
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|562| <<global("clear-log")>> .after_vcpu_run = default_after_vcpu_run,
+ */
 static void default_after_vcpu_run(struct kvm_vcpu *vcpu, int ret, int err)
 {
 	struct kvm_run *run = vcpu->run;
@@ -268,12 +470,20 @@ static void default_after_vcpu_run(struct kvm_vcpu *vcpu, int ret, int err)
 	vcpu_handle_sync_stop();
 }
 
+/*
+ * 在以下使用:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|566| <<global("dirty-ring")>> .supported = dirty_ring_supported,
+ */
 static bool dirty_ring_supported(void)
 {
 	return (kvm_has_cap(KVM_CAP_DIRTY_LOG_RING) ||
 		kvm_has_cap(KVM_CAP_DIRTY_LOG_RING_ACQ_REL));
 }
 
+/*
+ * 在以下使用dirty_ring_create_vm_done():
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|567| <<global("clear-log")>> .create_vm_done = dirty_ring_create_vm_done,
+ */
 static void dirty_ring_create_vm_done(struct kvm_vm *vm)
 {
 	uint64_t pages;
@@ -284,16 +494,29 @@ static void dirty_ring_create_vm_done(struct kvm_vm *vm)
 	 * the ring buffer size to ensure we're able to reach the
 	 * full dirty ring state.
 	 */
+	/*
+	 * (1 << (30 - 12)) + 3 = 262147 pages
+	 */
 	pages = (1ul << (DIRTY_MEM_BITS - vm->page_shift)) + 3;
 	pages = vm_adjust_num_guest_pages(vm->mode, pages);
 	if (vm->page_size < getpagesize())
 		pages = vm_num_host_pages(vm->mode, pages);
 
 	limit = 1 << (31 - __builtin_clz(pages));
+	/*
+	 * test_dirty_ring_count一开始默认65536
+	 */
 	test_dirty_ring_count = 1 << (31 - __builtin_clz(test_dirty_ring_count));
 	test_dirty_ring_count = min(limit, test_dirty_ring_count);
 	pr_info("dirty ring count: 0x%x\n", test_dirty_ring_count);
 
+	/*
+	 * struct kvm_dirty_gfn {
+	 *     __u32 flags;
+	 *     __u32 slot;
+	 *     __u64 offset;
+	 * };
+	 */
 	/*
 	 * Switch to dirty ring mode after VM creation but before any
 	 * of the vcpu creation.
@@ -302,20 +525,41 @@ static void dirty_ring_create_vm_done(struct kvm_vm *vm)
 			     sizeof(struct kvm_dirty_gfn));
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|451| <<dirty_ring_collect_one>> if (!dirty_gfn_is_dirtied(cur))
+ */
 static inline bool dirty_gfn_is_dirtied(struct kvm_dirty_gfn *gfn)
 {
 	return smp_load_acquire(&gfn->flags) == KVM_DIRTY_GFN_F_DIRTY;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|460| <<dirty_ring_collect_one>> dirty_gfn_set_collected(cur);
+ */
 static inline void dirty_gfn_set_collected(struct kvm_dirty_gfn *gfn)
 {
 	smp_store_release(&gfn->flags, KVM_DIRTY_GFN_F_RESET);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|500| <<dirty_ring_collect_dirty_pages>> count = dirty_ring_collect_one(vcpu_map_dirty_ring(vcpu), slot, bitmap, num_pages, ring_buf_idx);
+ *
+ * 收集dirty的数目, 把对应的dirty更新成KVM_DIRTY_GFN_F_RESET
+ */
 static uint32_t dirty_ring_collect_one(struct kvm_dirty_gfn *dirty_gfns,
 				       int slot, void *bitmap,
 				       uint32_t num_pages, uint32_t *fetch_index)
 {
+	/*
+	 * struct kvm_dirty_gfn {
+	 *     __u32 flags;
+	 *     __u32 slot;
+	 *     __u64 offset;
+	 * };
+	 */
 	struct kvm_dirty_gfn *cur;
 	uint32_t count = 0;
 
@@ -338,19 +582,44 @@ static uint32_t dirty_ring_collect_one(struct kvm_dirty_gfn *dirty_gfns,
 	return count;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|512| <<dirty_ring_collect_dirty_pages>> dirty_ring_wait_vcpu();
+ */
 static void dirty_ring_wait_vcpu(void)
 {
+	/*
+	 * 调用pthread_kill(vcpu_thread, SIG_IPI)
+	 */
 	/* This makes sure that hardware PML cache flushed */
 	vcpu_kick();
 	sem_wait_until(&sem_vcpu_stop);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|525| <<dirty_ring_collect_dirty_pages>> dirty_ring_continue_vcpu();
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|546| <<dirty_ring_collect_dirty_pages>> dirty_ring_continue_vcpu();
+ */
 static void dirty_ring_continue_vcpu(void)
 {
 	pr_info("Notifying vcpu to continue\n");
+	/*
+	 * 在以下使用sem_vcpu_cont:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|375| <<vcpu_handle_sync_stop>> sem_wait_until(&sem_vcpu_cont);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|498| <<dirty_ring_continue_vcpu>> sem_post(&sem_vcpu_cont);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|573| <<dirty_ring_after_vcpu_run>> sem_wait_until(&sem_vcpu_cont);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|589| <<dirty_ring_before_vcpu_join>> sem_post(&sem_vcpu_cont);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|1076| <<run_test>> sem_post(&sem_vcpu_cont);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|1129| <<main>> sem_init(&sem_vcpu_cont, 0, 0);
+	 */
 	sem_post(&sem_vcpu_cont);
 }
 
+/*
+ * 在以下使用dirty_ring_collect_dirty_pages():
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|581| <<global("dirty-ring")>> .collect_dirty_pages = dirty_ring_collect_dirty_pages,
+ */
 static void dirty_ring_collect_dirty_pages(struct kvm_vcpu *vcpu, int slot,
 					   void *bitmap, uint32_t num_pages,
 					   uint32_t *ring_buf_idx)
@@ -358,8 +627,26 @@ static void dirty_ring_collect_dirty_pages(struct kvm_vcpu *vcpu, int slot,
 	uint32_t count = 0, cleared;
 	bool continued_vcpu = false;
 
+	/*
+	 * 代码:
+	 * vcpu_kick(); ---> pthread_kill(vcpu_thread, SIG_IPI)
+	 * sem_wait_until(&sem_vcpu_stop);
+	 */
 	dirty_ring_wait_vcpu();
 
+	/*
+	 * 在以下使用dirty_ring_vcpu_ring_full:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|411| <<dirty_ring_collect_dirty_pages>> if (!dirty_ring_vcpu_ring_full) {
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|432| <<dirty_ring_collect_dirty_pages>> TEST_ASSERT(dirty_ring_vcpu_ring_full,
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|451| <<dirty_ring_after_vcpu_run>> WRITE_ONCE(dirty_ring_vcpu_ring_full,
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|455| <<dirty_ring_after_vcpu_run>> dirty_ring_vcpu_ring_full ?
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|871| <<run_test>> WRITE_ONCE(dirty_ring_vcpu_ring_full, false);
+	 *
+	 * 注释:
+	 * This is updated by the vcpu thread to tell the host whether it's a
+	 * ring-full event.  It should only be read until a sem_wait() of
+	 * sem_vcpu_stop and before vcpu continues to run.
+	 */
 	if (!dirty_ring_vcpu_ring_full) {
 		/*
 		 * This is not a ring-full event, it's safe to allow
@@ -374,6 +661,10 @@ static void dirty_ring_collect_dirty_pages(struct kvm_vcpu *vcpu, int slot,
 				       slot, bitmap, num_pages,
 				       ring_buf_idx);
 
+	/*
+	 * 调用KVM_RESET_DIRTY_RINGS
+	 * 清空的总数(cleared)和上面的count必须一样
+	 */
 	cleared = kvm_vm_reset_dirty_ring(vcpu->vm);
 
 	/* Cleared pages should be the same as collected */
@@ -389,6 +680,15 @@ static void dirty_ring_collect_dirty_pages(struct kvm_vcpu *vcpu, int slot,
 	pr_info("Iteration %ld collected %u pages\n", iteration, count);
 }
 
+/*
+ * 在以下使用dirty_ring_after_vcpu_run():
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|607| <<global("dirty-ring")>> .after_vcpu_run = dirty_ring_after_vcpu_run,
+ *
+ * vcpu_worker()
+ * -> log_mode_after_vcpu_run()
+ *    -> mode->after_vcpu_run = dirty_ring_after_vcpu_run()
+ *       -> sem_post(&sem_vcpu_stop);
+ */
 static void dirty_ring_after_vcpu_run(struct kvm_vcpu *vcpu, int ret, int err)
 {
 	struct kvm_run *run = vcpu->run;
@@ -415,6 +715,10 @@ static void dirty_ring_after_vcpu_run(struct kvm_vcpu *vcpu, int ret, int err)
 	}
 }
 
+/*
+ * 在以下使用dirty_ring_before_vcpu_join():
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|606| <<global("dirty-ring")>> .before_vcpu_join = dirty_ring_before_vcpu_join,
+ */
 static void dirty_ring_before_vcpu_join(void)
 {
 	/* Kick another round of vcpu just to make sure it will quit */
@@ -464,8 +768,20 @@ struct log_mode {
  * page bit is cleared in the latest bitmap, then the system must
  * report that write in the next get dirty log call.
  */
+/*
+ * 在以下使用host_bmap_track:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|760| <<vm_dirty_log_verify>> if (__test_and_clear_bit_le(page, host_bmap_track)) {
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|861| <<vm_dirty_log_verify>> __set_bit_le(page, host_bmap_track);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|950| <<run_test>> host_bmap_track = bitmap_zalloc(host_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1027| <<run_test>> free(host_bmap_track);
+ */
 static unsigned long *host_bmap_track;
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1068| <<help>> log_modes_dump();
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1121| <<main>> log_modes_dump();
+ */
 static void log_modes_dump(void)
 {
 	int i;
@@ -476,24 +792,74 @@ static void log_modes_dump(void)
 	printf("\n");
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|922| <<run_test>> if (!log_mode_supported()) {
+ */
 static bool log_mode_supported(void)
 {
+	/*
+	 * 在以下使用host_log_mode:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|529| <<log_mode_supported>> struct log_mode *mode = &log_modes[host_log_mode];
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|539| <<log_mode_create_vm_done>> struct log_mode *mode = &log_modes[host_log_mode];
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|553| <<log_mode_collect_dirty_pages>> struct log_mode *mode = &log_modes[host_log_mode];
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|562| <<log_mode_after_vcpu_run>> struct log_mode *mode = &log_modes[host_log_mode];
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|574| <<log_mode_before_vcpu_join>> struct log_mode *mode = &log_modes[host_log_mode];
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|674| <<vm_dirty_log_verify>> if (host_log_mode == LOG_MODE_DIRTY_RING && !matched) {
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|797| <<run_test>> log_modes[host_log_mode].name);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|898| <<run_test>> assert(host_log_mode == LOG_MODE_DIRTY_RING ||
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|1025| <<main>> host_log_mode = i;
+	 */
 	struct log_mode *mode = &log_modes[host_log_mode];
 
+	/*
+	 * .name = "clear-log",
+	 * .supported = clear_log_supported,
+	 *
+	 * .name = "dirty-ring",
+	 * .supported = dirty_ring_supported,
+	 */
 	if (mode->supported)
 		return mode->supported();
 
 	return true;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|896| <<create_vm>> log_mode_create_vm_done(vm);
+ */
 static void log_mode_create_vm_done(struct kvm_vm *vm)
 {
+	/*
+	 * 在以下使用host_log_mode:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|529| <<log_mode_supported>> struct log_mode *mode = &log_modes[host_log_mode];
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|539| <<log_mode_create_vm_done>> struct log_mode *mode = &log_modes[host_log_mode];
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|553| <<log_mode_collect_dirty_pages>> struct log_mode *mode = &log_modes[host_log_mode];
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|562| <<log_mode_after_vcpu_run>> struct log_mode *mode = &log_modes[host_log_mode];
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|574| <<log_mode_before_vcpu_join>> struct log_mode *mode = &log_modes[host_log_mode];
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|674| <<vm_dirty_log_verify>> if (host_log_mode == LOG_MODE_DIRTY_RING && !matched) {
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|797| <<run_test>> log_modes[host_log_mode].name);
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|898| <<run_test>> assert(host_log_mode == LOG_MODE_DIRTY_RING ||
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|1025| <<main>> host_log_mode = i;
+	 */
 	struct log_mode *mode = &log_modes[host_log_mode];
 
+	/*
+	 * name = "clear-log",
+	 * .create_vm_done = clear_log_create_vm_done,
+	 *
+	 * .name = "dirty-ring",
+	 * .create_vm_done = dirty_ring_create_vm_done,
+	 */
 	if (mode->create_vm_done)
 		mode->create_vm_done(vm);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|828| <<run_test>> log_mode_collect_dirty_pages(vcpu, TEST_MEM_SLOT_INDEX, bmap, host_num_pages, &ring_buf_idx);
+ */
 static void log_mode_collect_dirty_pages(struct kvm_vcpu *vcpu, int slot,
 					 void *bitmap, uint32_t num_pages,
 					 uint32_t *ring_buf_idx)
@@ -502,25 +868,61 @@ static void log_mode_collect_dirty_pages(struct kvm_vcpu *vcpu, int slot,
 
 	TEST_ASSERT(mode->collect_dirty_pages != NULL,
 		    "collect_dirty_pages() is required for any log mode!");
+	/*
+	 * .name = "dirty-log",
+	 * .collect_dirty_pages = dirty_log_collect_dirty_pages, --> KVM_GET_DIRTY_LOG
+	 *
+	 * .name = "clear-log",
+	 * .collect_dirty_pages = clear_log_collect_dirty_pages, --> KVM_GET_DIRTY_LOG + KVM_CLEAR_DIRTY_LOG
+	 *
+	 * .name = "dirty-ring",
+	 * .collect_dirty_pages = dirty_ring_collect_dirty_pages,
+	 */
 	mode->collect_dirty_pages(vcpu, slot, bitmap, num_pages, ring_buf_idx);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|757| <<vcpu_worker>> log_mode_after_vcpu_run(vcpu, ret, errno);
+ */
 static void log_mode_after_vcpu_run(struct kvm_vcpu *vcpu, int ret, int err)
 {
 	struct log_mode *mode = &log_modes[host_log_mode];
 
+	/*
+	 * .name = "dirty-log",
+	 * .after_vcpu_run = default_after_vcpu_run,
+	 *
+	 * .name = "clear-log",
+	 * .after_vcpu_run = default_after_vcpu_run,
+	 *
+	 * .name = "dirty-ring",
+	 * .after_vcpu_run = dirty_ring_after_vcpu_run,
+	 */
 	if (mode->after_vcpu_run)
 		mode->after_vcpu_run(vcpu, ret, err);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|856| <<run_test>> log_mode_before_vcpu_join();
+ */
 static void log_mode_before_vcpu_join(void)
 {
 	struct log_mode *mode = &log_modes[host_log_mode];
 
+	/*
+	 * .name = "dirty-ring",
+	 * .before_vcpu_join = dirty_ring_before_vcpu_join,
+	 */
 	if (mode->before_vcpu_join)
 		mode->before_vcpu_join();
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|748| <<vcpu_worker>> generate_random_array(guest_array, TEST_PAGES_PER_LOOP);
+ */
 static void generate_random_array(uint64_t *guest_array, uint64_t size)
 {
 	uint64_t i;
@@ -536,6 +938,13 @@ static void *vcpu_worker(void *data)
 	struct kvm_vm *vm = vcpu->vm;
 	uint64_t *guest_array;
 	uint64_t pages_count = 0;
+	/*
+	 * // for KVM_SET_SIGNAL_MASK
+	 * struct kvm_signal_mask {
+	 *     __u32 len;
+	 *     __u8  sigset[];
+	 * };
+	 */
 	struct kvm_signal_mask *sigmask = alloca(offsetof(struct kvm_signal_mask, sigset)
 						 + sizeof(sigset_t));
 	sigset_t *sigset = (sigset_t *) &sigmask->sigset;
@@ -553,19 +962,57 @@ static void *vcpu_worker(void *data)
 	sigemptyset(sigset);
 	sigaddset(sigset, SIG_IPI);
 
+	/*
+	 * 在以下使用random_array[TEST_PAGES_PER_LOOP]:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|133| <<guest_code>> addr += (READ_ONCE(random_array[i]) % guest_num_pages)
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|585| <<vcpu_worker>> guest_array = addr_gva2hva(vm, (vm_vaddr_t)random_array);
+	 */
 	guest_array = addr_gva2hva(vm, (vm_vaddr_t)random_array);
 
 	while (!READ_ONCE(host_quit)) {
 		/* Clear any existing kick signals */
 		generate_random_array(guest_array, TEST_PAGES_PER_LOOP);
 		pages_count += TEST_PAGES_PER_LOOP;
+		/*
+		 * 注释, guest的code:
+		 * Continuously write to the first 8 bytes of a random pages within
+		 * the testing memory region.
+		 */
 		/* Let the guest dirty the random pages */
 		ret = __vcpu_run(vcpu);
 		if (ret == -1 && errno == EINTR) {
 			int sig = -1;
+			/*
+			 * 注释:
+			 * Select any of pending signals from SET or wait for any to arrive.
+			 */
 			sigwait(sigset, &sig);
 			assert(sig == SIG_IPI);
 		}
+		/*
+		 * 在以下使用sem_vcpu_cont:
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|375| <<vcpu_handle_sync_stop>> sem_wait_until(&sem_vcpu_cont);
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|498| <<dirty_ring_continue_vcpu>> sem_post(&sem_vcpu_cont);
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|573| <<dirty_ring_after_vcpu_run>> sem_wait_until(&sem_vcpu_cont);
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|589| <<dirty_ring_before_vcpu_join>> sem_post(&sem_vcpu_cont);
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|1076| <<run_test>> sem_post(&sem_vcpu_cont);
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|1129| <<main>> sem_init(&sem_vcpu_cont, 0, 0);
+		 *
+		 * .name = "dirty-log",
+		 * .after_vcpu_run = default_after_vcpu_run, --> vcpu_handle_sync_stop()
+		 *
+		 * .name = "clear-log",
+		 * .after_vcpu_run = default_after_vcpu_run, --> vcpu_handle_sync_stop()
+		 *
+		 * .name = "dirty-ring",
+		 * .after_vcpu_run = dirty_ring_after_vcpu_run,
+		 *
+		 * 比如:
+		 * vcpu_worker()
+		 * -> log_mode_after_vcpu_run()
+		 *    -> mode->after_vcpu_run = dirty_ring_after_vcpu_run()
+		 *       -> sem_post(&sem_vcpu_stop);
+		 */
 		log_mode_after_vcpu_run(vcpu, ret, errno);
 	}
 
@@ -574,6 +1021,10 @@ static void *vcpu_worker(void *data)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|847| <<run_test>> vm_dirty_log_verify(mode, bmap);
+ */
 static void vm_dirty_log_verify(enum vm_guest_mode mode, unsigned long *bmap)
 {
 	uint64_t step = vm_num_host_pages(mode, 1);
@@ -586,6 +1037,12 @@ static void vm_dirty_log_verify(enum vm_guest_mode mode, unsigned long *bmap)
 
 		/* If this is a special page that we were tracking... */
 		if (__test_and_clear_bit_le(page, host_bmap_track)) {
+			/*
+			 * 在以下使用host_track_next_count:
+			 *   - tools/testing/selftests/kvm/dirty_log_test.c|995| <<vm_dirty_log_verify>> host_track_next_count++;
+			 *   - tools/testing/selftests/kvm/dirty_log_test.c|1220| <<run_test>> host_track_next_count = 0;
+			 *   - tools/testing/selftests/kvm/dirty_log_test.c|1276| <<run_test>> host_track_next_count);
+			 */
 			host_track_next_count++;
 			TEST_ASSERT(test_bit_le(page, bmap),
 				    "Page %"PRIu64" should have its dirty bit "
@@ -596,6 +1053,12 @@ static void vm_dirty_log_verify(enum vm_guest_mode mode, unsigned long *bmap)
 		if (__test_and_clear_bit_le(page, bmap)) {
 			bool matched;
 
+			/*
+			 * 在以下使用host_dirty_count:
+			 *   - tools/testing/selftests/kvm/dirty_log_test.c|614| <<vm_dirty_log_verify>> host_dirty_count++;
+			 *   - tools/testing/selftests/kvm/dirty_log_test.c|818| <<run_test>> host_dirty_count = 0;
+			 *   - tools/testing/selftests/kvm/dirty_log_test.c|860| <<run_test>> "track_next (%"PRIu64")\n", host_dirty_count, host_clear_count,
+			 */
 			host_dirty_count++;
 
 			/*
@@ -712,14 +1175,33 @@ struct test_params {
 	uint64_t phys_offset;
 };
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1488| <<main>> for_each_guest_mode(run_test, &p);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1492| <<main>> for_each_guest_mode(run_test, &p);
+ */
 static void run_test(enum vm_guest_mode mode, void *arg)
 {
+	/*
+	 * struct test_params {
+	 *     unsigned long iterations;
+	 *     unsigned long interval;
+	 *     uint64_t phys_offset;
+	 * };
+	 */
 	struct test_params *p = arg;
 	struct kvm_vcpu *vcpu;
 	struct kvm_vm *vm;
 	unsigned long *bmap;
 	uint32_t ring_buf_idx = 0;
 
+	/*
+	 * .name = "clear-log",
+	 * .supported = clear_log_supported,
+	 *
+	 * .name = "dirty-ring",
+	 * .supported = dirty_ring_supported,
+	 */
 	if (!log_mode_supported()) {
 		print_skip("Log mode '%s' not supported",
 			   log_modes[host_log_mode].name);
@@ -734,6 +1216,12 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 	 * (e.g., 64K page size guest will need even less memory for
 	 * page tables).
 	 */
+	/*
+	 * 30 - 12 = 18
+	 * 2 << 18 = 524288 (page的数目, 2G)
+	 * 这里是创建VM的内存, 不是后面的slot
+	 * 这里的用处比方是: 页表
+	 */
 	vm = create_vm(mode, &vcpu,
 		       2ul << (DIRTY_MEM_BITS - PAGE_SHIFT_4K), guest_code);
 
@@ -763,7 +1251,13 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 
 	pr_info("guest physical test memory offset: 0x%lx\n", guest_test_phys_mem);
 
+	/*
+	 * bitmap是按照host来的
+	 */
 	bmap = bitmap_zalloc(host_num_pages);
+	/*
+	 * static unsigned long *host_bmap_track;
+	 */
 	host_bmap_track = bitmap_zalloc(host_num_pages);
 
 	/* Add an extra memory slot for testing dirty logging */
@@ -786,12 +1280,45 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 	sync_global_to_guest(vm, guest_num_pages);
 
 	/* Start the iterations */
+	/*
+	 * iteration在下面的while循环增加
+	 */
 	iteration = 1;
 	sync_global_to_guest(vm, iteration);
 	host_quit = false;
+	/*
+	 * 在以下使用host_dirty_count:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|614| <<vm_dirty_log_verify>> host_dirty_count++;
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|818| <<run_test>> host_dirty_count = 0;
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|860| <<run_test>> "track_next (%"PRIu64")\n", host_dirty_count, host_clear_count,
+	 */
 	host_dirty_count = 0;
+	/*
+	 * 在以下使用host_clear_count:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|1000| <<vm_dirty_log_verify>> host_clear_count++;
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|1148| <<run_test>> host_clear_count = 0;
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|1204| <<run_test>> "track_next (%"PRIu64")\n", host_dirty_count, host_clear_count,
+	 */
 	host_clear_count = 0;
+	/*
+	 * 在以下使用host_track_next_count:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|995| <<vm_dirty_log_verify>> host_track_next_count++;
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|1220| <<run_test>> host_track_next_count = 0;
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|1276| <<run_test>> host_track_next_count);
+	 */
 	host_track_next_count = 0;
+	/*
+	 * 在以下使用dirty_ring_vcpu_ring_full:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|411| <<dirty_ring_collect_dirty_pages>> if (!dirty_ring_vcpu_ring_full) {
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|432| <<dirty_ring_collect_dirty_pages>> TEST_ASSERT(dirty_ring_vcpu_ring_full,
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|451| <<dirty_ring_after_vcpu_run>> WRITE_ONCE(dirty_ring_vcpu_ring_full,
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|455| <<dirty_ring_after_vcpu_run>> dirty_ring_vcpu_ring_full ?
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|871| <<run_test>> WRITE_ONCE(dirty_ring_vcpu_ring_full, false);
+	 *
+	 * This is updated by the vcpu thread to tell the host whether it's a
+	 * ring-full event.  It should only be read until a sem_wait() of
+	 * sem_vcpu_stop and before vcpu continues to run.
+	 */
 	WRITE_ONCE(dirty_ring_vcpu_ring_full, false);
 
 	pthread_create(&vcpu_thread, NULL, vcpu_worker, vcpu);
@@ -799,6 +1326,18 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 	while (iteration < p->iterations) {
 		/* Give the vcpu thread some time to dirty some pages */
 		usleep(p->interval * 1000);
+		/*
+		 * 只在此处调用
+		 *
+		 * .name = "dirty-log",
+		 * .collect_dirty_pages = dirty_log_collect_dirty_pages, --> KVM_GET_DIRTY_LOG
+		 *
+		 * .name = "clear-log",
+		 * .collect_dirty_pages = clear_log_collect_dirty_pages, --> KVM_GET_DIRTY_LOG + KVM_CLEAR_DIRTY_LOG
+		 *
+		 * .name = "dirty-ring",
+		 * .collect_dirty_pages = dirty_ring_collect_dirty_pages,
+		 */
 		log_mode_collect_dirty_pages(vcpu, TEST_MEM_SLOT_INDEX,
 					     bmap, host_num_pages,
 					     &ring_buf_idx);
@@ -807,7 +1346,22 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 		 * See vcpu_sync_stop_requested definition for details on why
 		 * we need to stop vcpu when verify data.
 		 */
+		/*
+		 * 注释:
+		 * This is only set by main thread, and only cleared by vcpu thread.  It is
+		 * used to request vcpu thread to stop at the next GUEST_SYNC, since GUEST_SYNC
+		 * is the only place that we'll guarantee both "dirty bit" and "dirty data"
+		 * will match.  E.g., SIG_IPI won't guarantee that if the vcpu is interrupted
+		 * after setting dirty bit but before the data is written.
+		 */
 		atomic_set(&vcpu_sync_stop_requested, true);
+		/*
+		 * 比如:
+		 * vcpu_worker()
+		 * -> log_mode_after_vcpu_run()
+		 *    -> mode->after_vcpu_run = dirty_ring_after_vcpu_run()
+		 *       -> sem_post(&sem_vcpu_stop);
+		 */
 		sem_wait_until(&sem_vcpu_stop);
 		/*
 		 * NOTE: for dirty ring, it's possible that we didn't stop at
@@ -818,6 +1372,9 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 		 */
 		assert(host_log_mode == LOG_MODE_DIRTY_RING ||
 		       atomic_read(&vcpu_sync_stop_requested) == false);
+		/*
+		 * 只在此处调用
+		 */
 		vm_dirty_log_verify(mode, bmap);
 		sem_post(&sem_vcpu_cont);
 
@@ -827,6 +1384,10 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 
 	/* Tell the vcpu thread to quit */
 	host_quit = true;
+	/*
+	 * .name = "dirty-ring",
+	 * .before_vcpu_join = dirty_ring_before_vcpu_join,
+	 */
 	log_mode_before_vcpu_join();
 	pthread_join(vcpu_thread, NULL);
 
@@ -930,6 +1491,9 @@ int main(int argc, char *argv[])
 
 	/* Ensure that vCPU threads start with SIG_IPI blocked.  */
 	sigemptyset(&sigset);
+	/*
+	 * 把SIG_IPI加入
+	 */
 	sigaddset(&sigset, SIG_IPI);
 	pthread_sigmask(SIG_BLOCK, &sigset, NULL);
 
diff --git a/tools/testing/selftests/kvm/include/kvm_util_base.h b/tools/testing/selftests/kvm/include/kvm_util_base.h
index a18db6a7b..5bf717981 100644
--- a/tools/testing/selftests/kvm/include/kvm_util_base.h
+++ b/tools/testing/selftests/kvm/include/kvm_util_base.h
@@ -110,6 +110,12 @@ struct kvm_vm {
 	vm_vaddr_t tss;
 	vm_vaddr_t idt;
 	vm_vaddr_t handlers;
+	/*
+	 * 在以下使用kvm_vm->dirty_ring_size:
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|138| <<vm_enable_dirty_ring>> vm->dirty_ring_size = ring_size;
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|655| <<vm_vcpu_rm>> ret = munmap(vcpu->dirty_gfns, vm->dirty_ring_size);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1692| <<vcpu_map_dirty_ring>> uint32_t size = vcpu->vm->dirty_ring_size;
+	 */
 	uint32_t dirty_ring_size;
 
 	/* Cache of information for binary stats interface */
@@ -346,6 +352,16 @@ int kvm_memfd_alloc(size_t size, bool hugepages);
 
 void vm_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent);
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|461| <<check_write_in_dirty_log>> kvm_vm_get_dirty_log(vm, region->region.slot, bmap);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|235| <<dirty_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|242| <<clear_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|531| <<memstress_get_dirty_log>> kvm_vm_get_dirty_log(vm, slot, bitmaps[i]);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_dirty_log_test.c|129| <<main>> kvm_vm_get_dirty_log(vm, TEST_MEM_SLOT_INDEX, bmap);
+ *
+ * 调用KVM_GET_DIRTY_LOG
+ */
 static inline void kvm_vm_get_dirty_log(struct kvm_vm *vm, int slot, void *log)
 {
 	struct kvm_dirty_log args = { .dirty_bitmap = log, .slot = slot };
@@ -353,6 +369,13 @@ static inline void kvm_vm_get_dirty_log(struct kvm_vm *vm, int slot, void *log)
 	vm_ioctl(vm, KVM_GET_DIRTY_LOG, &args);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|243| <<clear_log_collect_dirty_pages>> kvm_vm_clear_dirty_log(vcpu->vm, slot, bitmap, 0, num_pages);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|543| <<memstress_clear_dirty_log>> kvm_vm_clear_dirty_log(vm, slot, bitmaps[i], 0, pages_per_slot);
+ *
+ * 调用KVM_CLEAR_DIRTY_LOG
+ */
 static inline void kvm_vm_clear_dirty_log(struct kvm_vm *vm, int slot, void *log,
 					  uint64_t first_page, uint32_t num_pages)
 {
@@ -825,6 +848,37 @@ static inline void vcpu_dump(FILE *stream, struct kvm_vcpu *vcpu,
 struct kvm_vcpu *vm_arch_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id,
 				  void *guest_code);
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|712| <<run_test>> vcpu = vm_vcpu_add(vm, 0, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|335| <<test_vgic_then_vcpus>> vcpus[i] = vm_vcpu_add(v.vm, i, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|421| <<test_v3_typer_accesses>> (void )vm_vcpu_add(v.vm, 0, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|425| <<test_v3_typer_accesses>> (void )vm_vcpu_add(v.vm, 3, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|430| <<test_v3_typer_accesses>> (void )vm_vcpu_add(v.vm, 1, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|435| <<test_v3_typer_accesses>> (void )vm_vcpu_add(v.vm, 2, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|482| <<vm_gic_v3_create_with_vcpuids>> vm_vcpu_add(v.vm, vcpuids[i], guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|578| <<test_v3_redist_ipa_range_check_at_vcpu_run>> vcpus[i] = vm_vcpu_add(v.vm, i, guest_code);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|1168| <<create_vm>> *vcpu = vm_vcpu_add(vm, 0, guest_code);
+ *   - tools/testing/selftests/kvm/hardware_disable_test.c|105| <<run_test>> vcpu = vm_vcpu_add(vm, i, guest_code);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|432| <<__vm_create_with_vcpus>> vcpus[i] = vm_vcpu_add(vm, i, guest_code);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|244| <<test_get_cmma_basic>> vcpu = vm_vcpu_add(vm, 1, guest_do_one_essa);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|295| <<test_migration_mode>> vcpu = vm_vcpu_add(vm, 1, guest_do_one_essa);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|458| <<test_get_inital_dirty>> vcpu = vm_vcpu_add(vm, 1, guest_do_one_essa);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|517| <<test_get_skip_holes>> vcpu = vm_vcpu_add(vm, 1, guest_dirty_test_data);
+ *   - tools/testing/selftests/kvm/s390x/resets.c|211| <<create_vm>> *vcpu = vm_vcpu_add(vm, ARBITRARY_NON_ZERO_VCPU_ID, guest_code_initial);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_ipi.c|259| <<main>> vcpu[1] = vm_vcpu_add(vm, RECEIVER_VCPU_ID_1, receiver_code);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_ipi.c|265| <<main>> vcpu[2] = vm_vcpu_add(vm, RECEIVER_VCPU_ID_2, receiver_code);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_tlb_flush.c|636| <<main>> vcpu[1] = vm_vcpu_add(vm, WORKER_VCPU_ID_1, worker_guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_tlb_flush.c|641| <<main>> vcpu[2] = vm_vcpu_add(vm, WORKER_VCPU_ID_2, worker_guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|130| <<run_test>> vcpu = vm_vcpu_add(vm, 0, guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|391| <<test_pmu_config_disable>> vcpu = vm_vcpu_add(vm, 0, guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|949| <<main>> vcpu2 = vm_vcpu_add(vm, 2, intel_masked_events_guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|951| <<main>> vcpu2 = vm_vcpu_add(vm, 2, amd_masked_events_guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/set_boot_cpu_id.c|87| <<create_vm>> vcpus[i] = vm_vcpu_add(vm, i, i == bsp_vcpu_id ? guest_bsp_vcpu :
+ *   - tools/testing/selftests/kvm/x86_64/tsc_scaling_sync.c|53| <<run_vcpu>> vcpu = vm_vcpu_add(vm, vcpu_id, guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/ucna_injection_test.c|259| <<create_vcpu_with_mce_cap>> struct kvm_vcpu *vcpu = vm_vcpu_add(vm, vcpuid, guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/xapic_ipi_test.c|419| <<main>> params[1].vcpu = vm_vcpu_add(vm, 1, sender_guest_code);
+ */
 static inline struct kvm_vcpu *vm_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id,
 					   void *guest_code)
 {
diff --git a/tools/testing/selftests/kvm/include/memstress.h b/tools/testing/selftests/kvm/include/memstress.h
index ce4e60305..05cd2db82 100644
--- a/tools/testing/selftests/kvm/include/memstress.h
+++ b/tools/testing/selftests/kvm/include/memstress.h
@@ -36,6 +36,13 @@ struct memstress_args {
 	uint64_t size;
 	uint64_t guest_page_size;
 	uint32_t random_seed;
+	/*
+	 * 在以下使用memstress_args->write_percent:
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|131| <<memstress_guest_code>> if (guest_random_u32(&rand_state) % 100 < args->write_percent)
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|211| <<memstress_create_vm>> args->write_percent = 100;
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|312| <<memstress_set_write_percent>> memstress_args.write_percent = write_percent;
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|313| <<memstress_set_write_percent>> sync_global_to_guest(vm, memstress_args.write_percent);
+	 */
 	uint32_t write_percent;
 
 	/* Run vCPUs in L2 instead of L1, if the architecture supports it. */
@@ -44,6 +51,12 @@ struct memstress_args {
 	bool random_access;
 	/* True if all vCPUs are pinned to pCPUs */
 	bool pin_vcpus;
+	/*
+	 * 在以下使用memstress_args->vcpu_to_pcpu[KVM_MAX_VCPUS]:
+	 *   - tools/testing/selftests/kvm/demand_paging_test.c|288| <<main>> kvm_parse_vcpu_pinning(cpulist, memstress_args.vcpu_to_pcpu, nr_vcpus);
+	 *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|423| <<main>> kvm_parse_vcpu_pinning(pcpu_list, memstress_args.vcpu_to_pcpu, nr_vcpus);
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|406| <<vcpu_thread_main>> kvm_pin_this_task_to_pcpu(memstress_args.vcpu_to_pcpu[vcpu_idx]);
+	 */
 	/* The vCPU=>pCPU pinning map. Only valid if pin_vcpus is true. */
 	uint32_t vcpu_to_pcpu[KVM_MAX_VCPUS];
 
diff --git a/tools/testing/selftests/kvm/lib/kvm_util.c b/tools/testing/selftests/kvm/lib/kvm_util.c
index 7a8af1821..be3a07e49 100644
--- a/tools/testing/selftests/kvm/lib/kvm_util.c
+++ b/tools/testing/selftests/kvm/lib/kvm_util.c
@@ -125,12 +125,22 @@ unsigned int kvm_check_cap(long cap)
 	return (unsigned int)ret;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|415| <<dirty_ring_create_vm_done>> vm_enable_dirty_ring(vm, test_dirty_ring_count * sizeof(struct kvm_dirty_gfn));
+ */
 void vm_enable_dirty_ring(struct kvm_vm *vm, uint32_t ring_size)
 {
 	if (vm_check_cap(vm, KVM_CAP_DIRTY_LOG_RING_ACQ_REL))
 		vm_enable_cap(vm, KVM_CAP_DIRTY_LOG_RING_ACQ_REL, ring_size);
 	else
 		vm_enable_cap(vm, KVM_CAP_DIRTY_LOG_RING, ring_size);
+	/*
+	 * 在以下使用kvm_vm->dirty_ring_size:
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|138| <<vm_enable_dirty_ring>> vm->dirty_ring_size = ring_size;
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|655| <<vm_vcpu_rm>> ret = munmap(vcpu->dirty_gfns, vm->dirty_ring_size);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1692| <<vcpu_map_dirty_ring>> uint32_t size = vcpu->vm->dirty_ring_size;
+	 */
 	vm->dirty_ring_size = ring_size;
 }
 
@@ -400,6 +410,13 @@ struct kvm_vm *__vm_create(enum vm_guest_mode mode, uint32_t nr_runnable_vcpus,
  * extra_mem_pages is only used to calculate the maximum page table size,
  * no real memory allocation for non-slot0 memory in this function.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/include/kvm_util_base.h|738| <<vm_create_with_vcpus>> return __vm_create_with_vcpus(VM_MODE_DEFAULT, nr_vcpus, 0, guest_code, vcpus);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|257| <<pre_init_before_test>> vm = __vm_create_with_vcpus(mode, nr_vcpus, guest_num_pages, guest_code, test_args.vcpus);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|427| <<__vm_create_with_one_vcpu>> vm = __vm_create_with_vcpus(VM_MODE_DEFAULT, 1, extra_mem_pages, guest_code, vcpus);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|242| <<memstress_create_vm>> vm = __vm_create_with_vcpus(mode, nr_vcpus, slot0_pages + guest_num_pages, memstress_guest_code, vcpus);
+ */
 struct kvm_vm *__vm_create_with_vcpus(enum vm_guest_mode mode, uint32_t nr_vcpus,
 				      uint64_t extra_mem_pages,
 				      void *guest_code, struct kvm_vcpu *vcpus[])
@@ -920,6 +937,42 @@ void vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags,
  * given by slot, which must be unique and < KVM_MEM_SLOTS_NUM.  The
  * region is created with the flags given by flags.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/hypercalls.c|160| <<steal_time_init>> vm_userspace_mem_region_add(vcpu->vm, VM_MEM_SRC_ANONYMOUS, ST_GPA_BASE, 1, gpages, 0);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|599| <<setup_memslots>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, 0, CODE_AND_DATA_MEMSLOT, code_npages, 0);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|604| <<setup_memslots>> vm_userspace_mem_region_add(vm, p->src_type, data_gpa - pt_size, PAGE_TABLE_MEMSLOT,
+ *                                                                    pt_size / guest_page_size, p->test_desc->pt_memslot_flags);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|609| <<setup_memslots>> vm_userspace_mem_region_add(vm, p->src_type, data_gpa, TEST_DATA_MEMSLOT, data_size / guest_page_size,
+ *                                                                    p->test_desc->data_memslot_flags);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|792| <<run_test>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, guest_test_phys_mem, TEST_MEM_SLOT_INDEX,
+ *                                                                    guest_num_pages, KVM_MEM_LOG_DIRTY_PAGES);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|282| <<pre_init_before_test>> vm_userspace_mem_region_add(vm, src_type, guest_test_phys_mem, TEST_MEM_SLOT_INDEX, guest_num_pages, 0);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|364| <<__vm_create>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, 0, 0, nr_pages, 0);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|299| <<memstress_create_vm>> vm_userspace_mem_region_add(vm, backing_src, region_start, MEMSTRESS_MEM_SLOT_INDEX + i, region_pages, 0);
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|80| <<add_remove_memslot>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, gpa, DUMMY_MEMSLOT_INDEX, pages, 0);
+ *   - tools/testing/selftests/kvm/memslot_perf_test.c|322| <<prepare_vm>> vm_userspace_mem_region_add(data->vm, VM_MEM_SRC_ANONYMOUS, guest_addr, slot, npages, 0);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|106| <<create_main_memslot>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, 0, 0, MAIN_PAGE_COUNT, 0);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|114| <<create_test_memslot>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS,
+ *                                                                    TEST_DATA_START_GFN << vm->page_shift, TEST_DATA_MEMSLOT, TEST_DATA_PAGE_COUNT, 0);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|331| <<test_migration_mode>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS,
+ *                                                                    TEST_DATA_TWO_START_GFN << vm->page_shift, TEST_DATA_TWO_MEMSLOT, TEST_DATA_TWO_PAGE_COUNT, 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|120| <<spawn_vm>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS_THP, MEM_REGION_GPA, MEM_REGION_SLOT,
+ *                                                                    MEM_REGION_SIZE / getpagesize(), 0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|273| <<test_delete_memory_region>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS_THP, MEM_REGION_GPA, MEM_REGION_SLOT,
+ *                                                                    MEM_REGION_SIZE / getpagesize(), 0);
+ *   - tools/testing/selftests/kvm/steal_time.c|267| <<main>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, ST_GPA_BASE, 1, gpages, 0);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|132| <<run_test>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS_HUGETLB, HPAGE_GPA, HPAGE_SLOT, HPAGE_SLOT_NPAGES, 0);
+ *   - tools/testing/selftests/kvm/x86_64/smaller_maxphyaddr_emulation_test.c|72| <<main>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, MEM_REGION_GPA, MEM_REGION_SLOT,
+ *                                                                    MEM_REGION_SIZE / PAGE_SIZE, 0);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|190| <<main>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, SMRAM_GPA, SMRAM_MEMSLOT, SMRAM_PAGES, 0);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_dirty_log_test.c|88| <<main>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, GUEST_TEST_MEM,
+ *                                                                    TEST_MEM_SLOT_INDEX, TEST_MEM_PAGES, KVM_MEM_LOG_DIRTY_PAGES);
+ *   - tools/testing/selftests/kvm/x86_64/xen_shinfo_test.c|451| <<main>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, SHINFO_REGION_GPA, SHINFO_REGION_SLOT, 3, 0);
+ *   - tools/testing/selftests/kvm/x86_64/xen_shinfo_test.c|723| <<main>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, DUMMY_REGION_GPA, DUMMY_REGION_SLOT, 1, 0);
+ *   - tools/testing/selftests/kvm/x86_64/xen_shinfo_test.c|768| <<main>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, DUMMY_REGION_GPA_2, DUMMY_REGION_SLOT_2, 1, 0);
+ *   - tools/testing/selftests/kvm/x86_64/xen_vmcall_test.c|100| <<main>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, HCALL_REGION_GPA, HCALL_REGION_SLOT, 2, 0);
+ */
 void vm_userspace_mem_region_add(struct kvm_vm *vm,
 	enum vm_mem_backing_src_type src_type,
 	uint64_t guest_paddr, uint32_t slot, uint64_t npages,
@@ -1117,6 +1170,16 @@ memslot2region(struct kvm_vm *vm, uint32_t memslot)
  * Sets the flags of the memory region specified by the value of slot,
  * to the values given by flags.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|378| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX, KVM_MEM_LOG_DIRTY_PAGES);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|391| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX, 0);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|510| <<toggle_dirty_logging>> vm_mem_region_set_flags(vm, slot, flags);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|179| <<enable_dirty_tracking>> vm_mem_region_set_flags(vm, 0, KVM_MEM_LOG_DIRTY_PAGES);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|180| <<enable_dirty_tracking>> vm_mem_region_set_flags(vm, TEST_DATA_MEMSLOT, KVM_MEM_LOG_DIRTY_PAGES);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|351| <<test_migration_mode>> vm_mem_region_set_flags(vm, TEST_DATA_TWO_MEMSLOT, KVM_MEM_LOG_DIRTY_PAGES);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|362| <<test_migration_mode>> vm_mem_region_set_flags(vm, TEST_DATA_TWO_MEMSLOT, 0);
+ */
 void vm_mem_region_set_flags(struct kvm_vm *vm, uint32_t slot, uint32_t flags)
 {
 	int ret;
@@ -1573,6 +1636,32 @@ void vm_create_irqchip(struct kvm_vm *vm)
 	vm->has_irqchip = true;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|667| <<vcpu_run_loop>> ret = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|47| <<vcpu_worker>> ret = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|88| <<vcpu_worker>> ret = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|204| <<vcpu_worker>> ret = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1639| <<_vcpu_run>> int _vcpu_run(struct kvm_vcpu *vcpu)
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1658| <<vcpu_run>> int ret = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|47| <<vcpu_worker>> ret = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/s390x/sync_regs_test.c|83| <<test_read_invalid>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/s390x/sync_regs_test.c|90| <<test_read_invalid>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/s390x/sync_regs_test.c|104| <<test_set_invalid>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/s390x/sync_regs_test.c|111| <<test_set_invalid>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/s390x/sync_regs_test.c|127| <<test_req_and_verify_all_valid_regs>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/s390x/sync_regs_test.c|163| <<test_set_and_verify_various_reg_values>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/s390x/sync_regs_test.c|195| <<test_clear_kvm_dirty_regs_bits>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/sync_regs_test.c|231| <<main>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/sync_regs_test.c|238| <<main>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/sync_regs_test.c|246| <<main>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/sync_regs_test.c|253| <<main>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/sync_regs_test.c|262| <<main>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/sync_regs_test.c|281| <<main>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/sync_regs_test.c|305| <<main>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/sync_regs_test.c|320| <<main>> rv = _vcpu_run(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/sync_regs_test.c|337| <<main>> rv = _vcpu_run(vcpu);
+ */
 int _vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int rc;
@@ -1629,6 +1718,10 @@ struct kvm_reg_list *vcpu_get_reg_list(struct kvm_vcpu *vcpu)
 	return reg_list;
 }
 
+/*
+ * callled by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|592| <<dirty_ring_collect_dirty_pages>> count = dirty_ring_collect_one(vcpu_map_dirty_ring(vcpu),
+ */
 void *vcpu_map_dirty_ring(struct kvm_vcpu *vcpu)
 {
 	uint32_t page_size = getpagesize();
diff --git a/tools/testing/selftests/kvm/lib/memstress.c b/tools/testing/selftests/kvm/lib/memstress.c
index df457452d..d3cad35ba 100644
--- a/tools/testing/selftests/kvm/lib/memstress.c
+++ b/tools/testing/selftests/kvm/lib/memstress.c
@@ -11,12 +11,23 @@
 #include "memstress.h"
 #include "processor.h"
 
+/*
+ * 在其他文件很多地方用到
+ */
 struct memstress_args memstress_args;
 
 /*
  * Guest virtual memory offset of the testing memory slot.
  * Must not conflict with identity mapped test code.
  */
+/*
+ * 在以下使用guest_test_virt_mem:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|105| <<memstress_setup_vcpus>> vcpu_args->gva = guest_test_virt_mem +
+ *   - tools/testing/selftests/kvm/lib/memstress.c|111| <<memstress_setup_vcpus>> vcpu_args->gva = guest_test_virt_mem;
+ *   - tools/testing/selftests/kvm/lib/memstress.c|227| <<memstress_create_vm>> virt_map(vm, guest_test_virt_mem, args->gpa, guest_num_pages);
+ *
+ * 0xc0000000
+ */
 static uint64_t guest_test_virt_mem = DEFAULT_GUEST_TEST_MEM;
 
 struct vcpu_thread {
@@ -26,16 +37,39 @@ struct vcpu_thread {
 	/* The pthread backing the vCPU. */
 	pthread_t thread;
 
+	/*
+	 * 在以下使用memstress的vcpu_thread->running:
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|285| <<vcpu_thread_main>> WRITE_ONCE(vcpu->running, true);
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|314| <<memstress_start_vcpu_threads>> WRITE_ONCE(vcpu->running, false);
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|320| <<memstress_start_vcpu_threads>> while (!READ_ONCE(vcpu_threads[i].running))
+	 */
 	/* Set to true once the vCPU thread is up and running. */
 	bool running;
 };
 
+/*
+ * 在以下使用vcpu_threads[KVM_MAX_VCPUS]:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|311| <<memstress_start_vcpu_threads>> struct vcpu_thread *vcpu = &vcpu_threads[i];
+ *   - tools/testing/selftests/kvm/lib/memstress.c|320| <<memstress_start_vcpu_threads>> while (!READ_ONCE(vcpu_threads[i].running))
+ *   - tools/testing/selftests/kvm/lib/memstress.c|334| <<memstress_join_vcpu_threads>> pthread_join(vcpu_threads[i].thread, NULL);
+ */
 /* The vCPU threads involved in this test. */
 static struct vcpu_thread vcpu_threads[KVM_MAX_VCPUS];
 
+/*
+ * 在以下使用vcpu_thread_fn:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|296| <<vcpu_thread_main>> vcpu_thread_fn(&memstress_args.vcpu_args[vcpu_idx]);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|306| <<memstress_start_vcpu_threads>> vcpu_thread_fn = vcpu_fn;
+ */
 /* The function run by each vCPU thread, as provided by the test. */
 static void (*vcpu_thread_fn)(struct memstress_vcpu_args *);
 
+/*
+ * 在以下使用all_vcpu_threads_running:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|293| <<vcpu_thread_main>> while (!READ_ONCE(all_vcpu_threads_running))
+ *   - tools/testing/selftests/kvm/lib/memstress.c|307| <<memstress_start_vcpu_threads>> WRITE_ONCE(all_vcpu_threads_running, false);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|324| <<memstress_start_vcpu_threads>> WRITE_ONCE(all_vcpu_threads_running, true);
+ */
 /* Set to true once all vCPU threads are up and running. */
 static bool all_vcpu_threads_running;
 
@@ -45,9 +79,20 @@ static struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
  * Continuously write to the first 8 bytes of each page in the
  * specified region.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|181| <<memstress_create_vm>> vm = __vm_create_with_vcpus(mode, nr_vcpus, slot0_pages + guest_num_pages, memstress_guest_code, vcpus);
+ *   - tools/testing/selftests/kvm/lib/x86_64/memstress.c|20| <<memstress_l2_guest_code>> memstress_guest_code(vcpu_id);
+ *
+ * 在一个循环里读写内存, 然后GUEST_SYNC(1);
+ */
 void memstress_guest_code(uint32_t vcpu_idx)
 {
 	struct memstress_args *args = &memstress_args;
+	/*
+	 * struct memstress_args *args:
+	 * -> struct memstress_vcpu_args vcpu_args[KVM_MAX_VCPUS];
+	 */
 	struct memstress_vcpu_args *vcpu_args = &args->vcpu_args[vcpu_idx];
 	struct guest_random_state rand_state;
 	uint64_t gva;
@@ -58,6 +103,13 @@ void memstress_guest_code(uint32_t vcpu_idx)
 
 	rand_state = new_guest_random_state(args->random_seed + vcpu_idx);
 
+	/*
+	 * struct memstress_args *args:
+	 * -> struct memstress_vcpu_args vcpu_args[KVM_MAX_VCPUS];
+	 *    -> uint64_t gpa;
+	 *    -> uint64_t gva;
+	 *    -> uint64_t pages;
+	 */
 	gva = vcpu_args->gva;
 	pages = vcpu_args->pages;
 
@@ -86,6 +138,10 @@ void memstress_guest_code(uint32_t vcpu_idx)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|229| <<memstress_create_vm>> memstress_setup_vcpus(vm, nr_vcpus, vcpus, vcpu_memory_bytes, partition_vcpu_memory_access);
+ */
 void memstress_setup_vcpus(struct kvm_vm *vm, int nr_vcpus,
 			   struct kvm_vcpu *vcpus[],
 			   uint64_t vcpu_memory_bytes,
@@ -96,6 +152,12 @@ void memstress_setup_vcpus(struct kvm_vm *vm, int nr_vcpus,
 	int i;
 
 	for (i = 0; i < nr_vcpus; i++) {
+		/*
+		 * struct memstress_args *args = &memstress_args;
+		 * -> struct memstress_vcpu_args vcpu_args[KVM_MAX_VCPUS];
+		 *    -> struct kvm_vcpu *vcpu;
+		 *    -> int vcpu_idx;
+		 */
 		vcpu_args = &args->vcpu_args[i];
 
 		vcpu_args->vcpu = vcpus[i];
@@ -122,6 +184,15 @@ void memstress_setup_vcpus(struct kvm_vm *vm, int nr_vcpus,
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|309| <<run_test>> vm = memstress_create_vm(mode, nr_vcpus, params->vcpu_memory_bytes, 1, params->backing_src, !overlap_memory_access);
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|139| <<run_test>> vm = memstress_create_vm(mode, nr_vcpus, guest_percpu_mem_size, 1, p->src_type, p->partition_vcpu_memory_access);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|155| <<run_test>> vm = memstress_create_vm(mode, nr_vcpus, guest_percpu_mem_size, p->slots, p->backing_src, p->partition_vcpu_memory_access);
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|98| <<run_test>> vm = memstress_create_vm(mode, nr_vcpus, guest_percpu_mem_size, 1,
+ *                                                                                      VM_MEM_SRC_ANONYMOUS, p->partition_vcpu_memory_access);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|103| <<run_test>> vm = memstress_create_vm(mode, VCPUS, guest_percpu_mem_size, SLOTS, backing_src, false);
+ */
 struct kvm_vm *memstress_create_vm(enum vm_guest_mode mode, int nr_vcpus,
 				   uint64_t vcpu_memory_bytes, int slots,
 				   enum vm_mem_backing_src_type backing_src,
@@ -136,6 +207,13 @@ struct kvm_vm *memstress_create_vm(enum vm_guest_mode mode, int nr_vcpus,
 
 	pr_info("Testing guest mode: %s\n", vm_guest_mode_string(mode));
 
+	/*
+	 * 在以下使用memstress_args->write_percent:
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|131| <<memstress_guest_code>> if (guest_random_u32(&rand_state) % 100 < args->write_percent)
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|211| <<memstress_create_vm>> args->write_percent = 100;
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|312| <<memstress_set_write_percent>> memstress_args.write_percent = write_percent;
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|313| <<memstress_set_write_percent>> sync_global_to_guest(vm, memstress_args.write_percent);
+	 */
 	/* By default vCPUs will write to memory. */
 	args->write_percent = 100;
 
@@ -156,6 +234,12 @@ struct kvm_vm *memstress_create_vm(enum vm_guest_mode mode, int nr_vcpus,
 		    "Guest memory cannot be evenly divided into %d slots.",
 		    slots);
 
+	/*
+	 * weak: tools/testing/selftests/kvm/lib/memstress.c
+	 * arch: tools/testing/selftests/kvm/lib/x86_64/memstress.c
+	 * called by:
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|235| <<memstress_create_vm>> slot0_pages += memstress_nested_pages(nr_vcpus);
+	 */
 	/*
 	 * If using nested, allocate extra pages for the nested page tables and
 	 * in-memory data structures.
@@ -168,6 +252,9 @@ struct kvm_vm *memstress_create_vm(enum vm_guest_mode mode, int nr_vcpus,
 	 * The memory is also added to memslot 0, but that's a benign side
 	 * effect as KVM allows aliasing HVAs in meslots.
 	 */
+	/*
+	 * memstress_guest_code(): 在一个循环里读写内存, 然后GUEST_SYNC(1);
+	 */
 	vm = __vm_create_with_vcpus(mode, nr_vcpus, slot0_pages + guest_num_pages,
 				    memstress_guest_code, vcpus);
 
@@ -217,11 +304,20 @@ struct kvm_vm *memstress_create_vm(enum vm_guest_mode mode, int nr_vcpus,
 	/* Do mapping for the demand paging memory slot */
 	virt_map(vm, guest_test_virt_mem, args->gpa, guest_num_pages);
 
+	/*
+	 * 只在这里调用
+	 */
 	memstress_setup_vcpus(vm, nr_vcpus, vcpus, vcpu_memory_bytes,
 			      partition_vcpu_memory_access);
 
 	if (args->nested) {
 		pr_info("Configuring vCPUs to run in L2 (nested).\n");
+		/*
+		 * weak: tools/testing/selftests/kvm/lib/memstress.c
+		 * arch: tools/testing/selftests/kvm/lib/x86_64/memstress.c
+		 * called by:
+		 *   - tools/testing/selftests/kvm/lib/memstress.c|296| <<memstress_create_vm>> memstress_setup_nested(vm, nr_vcpus, vcpus);
+		 */
 		memstress_setup_nested(vm, nr_vcpus, vcpus);
 	}
 
@@ -231,45 +327,91 @@ struct kvm_vm *memstress_create_vm(enum vm_guest_mode mode, int nr_vcpus,
 	return vm;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|328| <<run_test>> memstress_destroy_vm(vm);
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|200| <<run_test>> memstress_destroy_vm(vm);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|294| <<run_test>> memstress_destroy_vm(vm);
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|113| <<run_test>> memstress_destroy_vm(vm);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|166| <<run_test>> memstress_destroy_vm(vm);
+ */
 void memstress_destroy_vm(struct kvm_vm *vm)
 {
 	kvm_vm_free(vm);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|285| <<access_memory>> memstress_set_write_percent(vm, (access == ACCESS_READ) ? 0 : 100);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|161| <<run_test>> memstress_set_write_percent(vm, p->write_percent);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|191| <<run_test>> memstress_set_write_percent(vm, 100);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|214| <<run_test>> memstress_set_write_percent(vm, p->write_percent);
+ */
 void memstress_set_write_percent(struct kvm_vm *vm, uint32_t write_percent)
 {
 	memstress_args.write_percent = write_percent;
 	sync_global_to_guest(vm, memstress_args.write_percent);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|160| <<run_test>> memstress_set_random_seed(vm, p->random_seed);
+ */
 void memstress_set_random_seed(struct kvm_vm *vm, uint32_t random_seed)
 {
 	memstress_args.random_seed = random_seed;
 	sync_global_to_guest(vm, memstress_args.random_seed);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|192| <<run_test>> memstress_set_random_access(vm, false);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|215| <<run_test>> memstress_set_random_access(vm, p->random_access);
+ */
 void memstress_set_random_access(struct kvm_vm *vm, bool random_access)
 {
 	memstress_args.random_access = random_access;
 	sync_global_to_guest(vm, memstress_args.random_access);
 }
 
+/*
+ * weak: tools/testing/selftests/kvm/lib/memstress.c
+ * arch: tools/testing/selftests/kvm/lib/x86_64/memstress.c
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|235| <<memstress_create_vm>> slot0_pages += memstress_nested_pages(nr_vcpus);
+ */
 uint64_t __weak memstress_nested_pages(int nr_vcpus)
 {
 	return 0;
 }
 
+/*
+ * weak: tools/testing/selftests/kvm/lib/memstress.c
+ * arch: tools/testing/selftests/kvm/lib/x86_64/memstress.c
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|296| <<memstress_create_vm>> memstress_setup_nested(vm, nr_vcpus, vcpus);
+ */
 void __weak memstress_setup_nested(struct kvm_vm *vm, int nr_vcpus, struct kvm_vcpu **vcpus)
 {
 	pr_info("%s() not support on this architecture, skipping.\n", __func__);
 	exit(KSFT_SKIP);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|439| <<memstress_start_vcpu_threads>> pthread_create(&vcpu->thread, NULL, vcpu_thread_main, vcpu);
+ */
 static void *vcpu_thread_main(void *data)
 {
 	struct vcpu_thread *vcpu = data;
 	int vcpu_idx = vcpu->vcpu_idx;
 
+	/*
+	 * 在以下使用memstress_args->vcpu_to_pcpu[KVM_MAX_VCPUS]:
+	 *   - tools/testing/selftests/kvm/demand_paging_test.c|288| <<main>> kvm_parse_vcpu_pinning(cpulist, memstress_args.vcpu_to_pcpu, nr_vcpus);
+	 *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|423| <<main>> kvm_parse_vcpu_pinning(pcpu_list, memstress_args.vcpu_to_pcpu, nr_vcpus);
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|406| <<vcpu_thread_main>> kvm_pin_this_task_to_pcpu(memstress_args.vcpu_to_pcpu[vcpu_idx]);
+	 */
 	if (memstress_args.pin_vcpus)
 		kvm_pin_this_task_to_pcpu(memstress_args.vcpu_to_pcpu[vcpu_idx]);
 
@@ -281,14 +423,33 @@ static void *vcpu_thread_main(void *data)
 	 * requires taking the mmap_sem in write mode) from interfering with the
 	 * guest faulting in its memory.
 	 */
+	/*
+	 * 在以下使用all_vcpu_threads_running:
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|293| <<vcpu_thread_main>> while (!READ_ONCE(all_vcpu_threads_running))
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|307| <<memstress_start_vcpu_threads>> WRITE_ONCE(all_vcpu_threads_running, false);
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|324| <<memstress_start_vcpu_threads>> WRITE_ONCE(all_vcpu_threads_running, true);
+	 */
 	while (!READ_ONCE(all_vcpu_threads_running))
 		;
 
+	/*
+	 * 在以下使用vcpu_thread_fn:
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|296| <<vcpu_thread_main>> vcpu_thread_fn(&memstress_args.vcpu_args[vcpu_idx]);
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|306| <<memstress_start_vcpu_threads>> vcpu_thread_fn = vcpu_fn;
+	 */
 	vcpu_thread_fn(&memstress_args.vcpu_args[vcpu_idx]);
 
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|312| <<run_test>> memstress_start_vcpu_threads(nr_vcpus, vcpu_thread_main);
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|181| <<run_test>> memstress_start_vcpu_threads(nr_vcpus, vcpu_worker);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|193| <<run_test>> memstress_start_vcpu_threads(nr_vcpus, vcpu_worker);
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|104| <<run_test>> memstress_start_vcpu_threads(nr_vcpus, vcpu_worker);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|124| <<run_test>> memstress_start_vcpu_threads(VCPUS, vcpu_worker);
+ */
 void memstress_start_vcpu_threads(int nr_vcpus,
 				  void (*vcpu_fn)(struct memstress_vcpu_args *))
 {
@@ -315,6 +476,13 @@ void memstress_start_vcpu_threads(int nr_vcpus,
 	WRITE_ONCE(all_vcpu_threads_running, true);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|184| <<run_test>> memstress_join_vcpu_threads(nr_vcpus);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|278| <<run_test>> memstress_join_vcpu_threads(nr_vcpus);
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|110| <<run_test>> memstress_join_vcpu_threads(nr_vcpus);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|163| <<run_test>> memstress_join_vcpu_threads(VCPUS);
+ */
 void memstress_join_vcpu_threads(int nr_vcpus)
 {
 	int i;
@@ -325,6 +493,11 @@ void memstress_join_vcpu_threads(int nr_vcpus)
 		pthread_join(vcpu_threads[i].thread, NULL);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|474| <<memstress_enable_dirty_logging>> toggle_dirty_logging(vm, slots, true);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|479| <<memstress_disable_dirty_logging>> toggle_dirty_logging(vm, slots, false);
+ */
 static void toggle_dirty_logging(struct kvm_vm *vm, int slots, bool enable)
 {
 	int i;
@@ -333,20 +506,45 @@ static void toggle_dirty_logging(struct kvm_vm *vm, int slots, bool enable)
 		int slot = MEMSTRESS_MEM_SLOT_INDEX + i;
 		int flags = enable ? KVM_MEM_LOG_DIRTY_PAGES : 0;
 
+		/*
+		 * called by:
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|378| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX, KVM_MEM_LOG_DIRTY_PAGES);
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|391| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX, 0);
+		 *   - tools/testing/selftests/kvm/lib/memstress.c|510| <<toggle_dirty_logging>> vm_mem_region_set_flags(vm, slot, flags);
+		 *   - tools/testing/selftests/kvm/s390x/cmma_test.c|179| <<enable_dirty_tracking>> vm_mem_region_set_flags(vm, 0, KVM_MEM_LOG_DIRTY_PAGES);
+		 *   - tools/testing/selftests/kvm/s390x/cmma_test.c|180| <<enable_dirty_tracking>> vm_mem_region_set_flags(vm, TEST_DATA_MEMSLOT, KVM_MEM_LOG_DIRTY_PAGES);
+		 *   - tools/testing/selftests/kvm/s390x/cmma_test.c|351| <<test_migration_mode>> vm_mem_region_set_flags(vm, TEST_DATA_TWO_MEMSLOT, KVM_MEM_LOG_DIRTY_PAGES);
+		 *   - tools/testing/selftests/kvm/s390x/cmma_test.c|362| <<test_migration_mode>> vm_mem_region_set_flags(vm, TEST_DATA_TWO_MEMSLOT, 0);
+		 */
 		vm_mem_region_set_flags(vm, slot, flags);
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|209| <<run_test>> memstress_enable_dirty_logging(vm, p->slots);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|130| <<run_test>> memstress_enable_dirty_logging(vm, SLOTS);
+ */
 void memstress_enable_dirty_logging(struct kvm_vm *vm, int slots)
 {
 	toggle_dirty_logging(vm, slots, true);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|267| <<run_test>> memstress_disable_dirty_logging(vm, p->slots);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|149| <<run_test>> memstress_disable_dirty_logging(vm, SLOTS);
+ */
 void memstress_disable_dirty_logging(struct kvm_vm *vm, int slots)
 {
 	toggle_dirty_logging(vm, slots, false);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|238| <<run_test>> memstress_get_dirty_log(vm, bitmaps, p->slots);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|139| <<run_test>> memstress_get_dirty_log(vm, bitmaps, SLOTS);
+ */
 void memstress_get_dirty_log(struct kvm_vm *vm, unsigned long *bitmaps[], int slots)
 {
 	int i;
@@ -354,10 +552,23 @@ void memstress_get_dirty_log(struct kvm_vm *vm, unsigned long *bitmaps[], int sl
 	for (i = 0; i < slots; i++) {
 		int slot = MEMSTRESS_MEM_SLOT_INDEX + i;
 
+		/*
+		 * called by:
+		 *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|461| <<check_write_in_dirty_log>> kvm_vm_get_dirty_log(vm, region->region.slot, bmap);
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|235| <<dirty_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|242| <<clear_log_collect_dirty_pages>> kvm_vm_get_dirty_log(vcpu->vm, slot, bitmap);
+		 *   - tools/testing/selftests/kvm/lib/memstress.c|531| <<memstress_get_dirty_log>> kvm_vm_get_dirty_log(vm, slot, bitmaps[i]);
+		 *   - tools/testing/selftests/kvm/x86_64/vmx_dirty_log_test.c|129| <<main>> kvm_vm_get_dirty_log(vm, TEST_MEM_SLOT_INDEX, bmap);
+		 */
 		kvm_vm_get_dirty_log(vm, slot, bitmaps[i]);
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|247| <<run_test>> memstress_clear_dirty_log(vm, bitmaps, p->slots, pages_per_slot);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|142| <<run_test>> memstress_clear_dirty_log(vm, bitmaps, SLOTS, pages_per_slot);
+ */
 void memstress_clear_dirty_log(struct kvm_vm *vm, unsigned long *bitmaps[],
 			       int slots, uint64_t pages_per_slot)
 {
@@ -366,10 +577,20 @@ void memstress_clear_dirty_log(struct kvm_vm *vm, unsigned long *bitmaps[],
 	for (i = 0; i < slots; i++) {
 		int slot = MEMSTRESS_MEM_SLOT_INDEX + i;
 
+		/*
+		 * called by:
+		 *   - tools/testing/selftests/kvm/dirty_log_test.c|243| <<clear_log_collect_dirty_pages>> kvm_vm_clear_dirty_log(vcpu->vm, slot, bitmap, 0, num_pages);
+		 *   - tools/testing/selftests/kvm/lib/memstress.c|543| <<memstress_clear_dirty_log>> kvm_vm_clear_dirty_log(vm, slot, bitmaps[i], 0, pages_per_slot);
+		 */
 		kvm_vm_clear_dirty_log(vm, slot, bitmaps[i], 0, pages_per_slot);
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|168| <<run_test>> bitmaps = memstress_alloc_bitmaps(p->slots, pages_per_slot);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|111| <<run_test>> bitmaps = memstress_alloc_bitmaps(SLOTS, pages_per_slot);
+ */
 unsigned long **memstress_alloc_bitmaps(int slots, uint64_t pages_per_slot)
 {
 	unsigned long **bitmaps;
@@ -386,6 +607,11 @@ unsigned long **memstress_alloc_bitmaps(int slots, uint64_t pages_per_slot)
 	return bitmaps;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|292| <<run_test>> memstress_free_bitmaps(bitmaps, p->slots);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|165| <<run_test>> memstress_free_bitmaps(bitmaps, SLOTS);
+ */
 void memstress_free_bitmaps(unsigned long *bitmaps[], int slots)
 {
 	int i;
diff --git a/tools/testing/selftests/kvm/lib/x86_64/memstress.c b/tools/testing/selftests/kvm/lib/x86_64/memstress.c
index d61e623af..72ad00cfd 100644
--- a/tools/testing/selftests/kvm/lib/x86_64/memstress.c
+++ b/tools/testing/selftests/kvm/lib/x86_64/memstress.c
@@ -49,6 +49,12 @@ static void memstress_l1_guest_code(struct vmx_pages *vmx, uint64_t vcpu_id)
 	GUEST_DONE();
 }
 
+/*
+ * weak: tools/testing/selftests/kvm/lib/memstress.c
+ * arch: tools/testing/selftests/kvm/lib/x86_64/memstress.c
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|235| <<memstress_create_vm>> slot0_pages += memstress_nested_pages(nr_vcpus);
+ */
 uint64_t memstress_nested_pages(int nr_vcpus)
 {
 	/*
@@ -77,6 +83,12 @@ void memstress_setup_ept(struct vmx_pages *vmx, struct kvm_vm *vm)
 	nested_identity_map_1g(vmx, vm, start, end - start);
 }
 
+/*
+ * weak: tools/testing/selftests/kvm/lib/memstress.c
+ * arch: tools/testing/selftests/kvm/lib/x86_64/memstress.c
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/memstress.c|296| <<memstress_create_vm>> memstress_setup_nested(vm, nr_vcpus, vcpus);
+ */
 void memstress_setup_nested(struct kvm_vm *vm, int nr_vcpus, struct kvm_vcpu *vcpus[])
 {
 	struct vmx_pages *vmx, *vmx0 = NULL;
diff --git a/tools/testing/selftests/kvm/lib/x86_64/processor.c b/tools/testing/selftests/kvm/lib/x86_64/processor.c
index d82883740..f56263beb 100644
--- a/tools/testing/selftests/kvm/lib/x86_64/processor.c
+++ b/tools/testing/selftests/kvm/lib/x86_64/processor.c
@@ -562,6 +562,10 @@ void kvm_arch_vm_post_create(struct kvm_vm *vm)
 	sync_global_to_guest(vm, host_cpu_is_amd);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/include/kvm_util_base.h|854| <<vm_vcpu_add>> return vm_arch_vcpu_add(vm, vcpu_id, guest_code);
+ */
 struct kvm_vcpu *vm_arch_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id,
 				  void *guest_code)
 {
@@ -1006,6 +1010,15 @@ struct kvm_x86_state *vcpu_save_state(struct kvm_vcpu *vcpu)
 	return state;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/amx_test.c|323| <<main>> vcpu_load_state(vcpu, state);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_evmcs.c|222| <<save_restore_vm>> vcpu_load_state(vcpu, state);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|212| <<main>> vcpu_load_state(vcpu, state);
+ *   - tools/testing/selftests/kvm/x86_64/state_test.c|289| <<main>> vcpu_load_state(vcpu, state);
+ *   - tools/testing/selftests/kvm/x86_64/sync_regs_test.c|196| <<race_sync_regs>> vcpu_load_state(vcpu, state);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_preemption_timer_test.c|234| <<main>> vcpu_load_state(vcpu, state);
+ */
 void vcpu_load_state(struct kvm_vcpu *vcpu, struct kvm_x86_state *state)
 {
 	vcpu_sregs_set(vcpu, &state->sregs);
diff --git a/tools/testing/selftests/kvm/lib/x86_64/vmx.c b/tools/testing/selftests/kvm/lib/x86_64/vmx.c
index 59d97531c..b4a4037d4 100644
--- a/tools/testing/selftests/kvm/lib/x86_64/vmx.c
+++ b/tools/testing/selftests/kvm/lib/x86_64/vmx.c
@@ -357,6 +357,22 @@ static inline void init_vmcs_guest_state(void *rip, void *rsp)
 	vmwrite(GUEST_SYSENTER_EIP, vmreadz(HOST_IA32_SYSENTER_EIP));
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/x86_64/memstress.c|45| <<memstress_l1_guest_code>> prepare_vmcs(vmx, memstress_l2_guest_entry, rsp);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_evmcs.c|104| <<guest_code>> prepare_vmcs(vmx_pages, l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ *   - tools/testing/selftests/kvm/x86_64/nested_exceptions_test.c|137| <<l1_vmx_code>> prepare_vmcs(vmx, NULL, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|130| <<guest_code>> prepare_vmcs(vmx_pages, l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ *   - tools/testing/selftests/kvm/x86_64/state_test.c|89| <<vmx_l1_guest_code>> prepare_vmcs(vmx_pages, vmx_l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ *   - tools/testing/selftests/kvm/x86_64/triple_fault_event_test.c|34| <<l1_guest_code_vmx>> prepare_vmcs(vmx, l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_apic_access_test.c|47| <<l1_guest_code>> prepare_vmcs(vmx_pages, l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_close_while_nested_test.c|41| <<l1_guest_code>> prepare_vmcs(vmx_pages, l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_dirty_log_test.c|57| <<l1_guest_code>> prepare_vmcs(vmx, l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_invalid_nested_guest_state.c|35| <<l1_guest_code>> prepare_vmcs(vmx_pages, l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_nested_tsc_scaling_test.c|94| <<l1_guest_code>> prepare_vmcs(vmx_pages, l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_preemption_timer_test.c|81| <<l1_guest_code>> prepare_vmcs(vmx_pages, l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c|90| <<l1_guest_code>> prepare_vmcs(vmx_pages, l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ */
 void prepare_vmcs(struct vmx_pages *vmx, void *guest_rip, void *guest_rsp)
 {
 	init_vmcs_control_fields(vmx);
diff --git a/tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c b/tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c
index 634c6bfcd..98cffef5b 100644
--- a/tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c
+++ b/tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c
@@ -26,9 +26,38 @@ static uint64_t guest_percpu_mem_size = DEFAULT_PER_VCPU_MEM_SIZE;
 
 static enum vm_mem_backing_src_type backing_src = VM_MEM_SRC_ANONYMOUS_HUGETLB;
 
+/*
+ * 在以下使用dirty_log_manual_caps:
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|160| <<run_test>> if (dirty_log_manual_caps)
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|162| <<run_test>> dirty_log_manual_caps);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|202| <<run_test>> if (dirty_log_manual_caps) {
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|242| <<run_test>> if (dirty_log_manual_caps) {
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|305| <<main>> dirty_log_manual_caps = 0;
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|308| <<main>> dirty_log_manual_caps =
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|311| <<main>> if (dirty_log_manual_caps) {
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|312| <<main>> dirty_log_manual_caps &= (KVM_DIRTY_LOG_MANUAL_PROTECT_ENABLE |
+ */
 static u64 dirty_log_manual_caps;
 static bool host_quit;
+/*
+ * 在以下使用iteration:
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|57| <<run_vcpu_iteration>> iteration++;
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|60| <<run_vcpu_iteration>> iteration)
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|71| <<vcpu_worker>> int current_iteration = READ_ONCE(iteration);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|80| <<vcpu_worker>> while (current_iteration == READ_ONCE(iteration) &&
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|81| <<vcpu_worker>> READ_ONCE(iteration) >= 0 &&
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|118| <<run_test>> iteration = -1;
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|134| <<run_test>> while (iteration < ITERATIONS) {
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|136| <<run_test>> get_page_stats(vm, &stats_dirty_pass[iteration - 1],
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|144| <<run_test>> get_page_stats(vm, &stats_clear_pass[iteration - 1], "clearing dirty log");
+ */
 static int iteration;
+/*
+ * 在以下使用vcpu_last_completed_iteration[KVM_MAX_VCPUS]:
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|59| <<run_vcpu_iteration>> while (READ_ONCE(vcpu_last_completed_iteration[i]) !=
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|77| <<vcpu_worker>> vcpu_last_completed_iteration[vcpu_idx] = current_iteration;
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|122| <<run_test>> vcpu_last_completed_iteration[i] = -1;
+ */
 static int vcpu_last_completed_iteration[KVM_MAX_VCPUS];
 
 struct kvm_page_stats {
@@ -50,12 +79,24 @@ static void get_page_stats(struct kvm_vm *vm, struct kvm_page_stats *stats, cons
 		 stats->hugepages);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|126| <<run_test>> run_vcpu_iteration(vm);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|135| <<run_test>> run_vcpu_iteration(vm);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|154| <<run_test>> run_vcpu_iteration(vm);
+ */
 static void run_vcpu_iteration(struct kvm_vm *vm)
 {
 	int i;
 
 	iteration++;
 	for (i = 0; i < VCPUS; i++) {
+		/*
+		 * 在以下使用vcpu_last_completed_iteration[KVM_MAX_VCPUS]:
+		 *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|59| <<run_vcpu_iteration>> while (READ_ONCE(vcpu_last_completed_iteration[i]) !=
+		 *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|77| <<vcpu_worker>> vcpu_last_completed_iteration[vcpu_idx] = current_iteration;
+		 *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|122| <<run_test>> vcpu_last_completed_iteration[i] = -1;
+		 */
 		while (READ_ONCE(vcpu_last_completed_iteration[i]) !=
 		       iteration)
 			;
@@ -74,6 +115,12 @@ static void vcpu_worker(struct memstress_vcpu_args *vcpu_args)
 
 		TEST_ASSERT_EQ(get_ucall(vcpu, NULL), UCALL_SYNC);
 
+		/*
+		 * 在以下使用vcpu_last_completed_iteration[KVM_MAX_VCPUS]:
+		 *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|59| <<run_vcpu_iteration>> while (READ_ONCE(vcpu_last_completed_iteration[i]) !=
+		 *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|77| <<vcpu_worker>> vcpu_last_completed_iteration[vcpu_idx] = current_iteration;
+		 *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|122| <<run_test>> vcpu_last_completed_iteration[i] = -1;
+		 */
 		vcpu_last_completed_iteration[vcpu_idx] = current_iteration;
 
 		/* Wait for the start of the next iteration to be signaled. */
@@ -95,17 +142,34 @@ static void run_test(enum vm_guest_mode mode, void *unused)
 	uint64_t total_4k_pages;
 	struct kvm_page_stats stats_populated;
 	struct kvm_page_stats stats_dirty_logging_enabled;
+	/*
+	 * ITERATIONS是2
+	 */
 	struct kvm_page_stats stats_dirty_pass[ITERATIONS];
 	struct kvm_page_stats stats_clear_pass[ITERATIONS];
 	struct kvm_page_stats stats_dirty_logging_disabled;
 	struct kvm_page_stats stats_repopulated;
 
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|309| <<run_test>> vm = memstress_create_vm(mode, nr_vcpus, params->vcpu_memory_bytes, 1, params->backing_src, !overlap_memory_access);
+	 *   - tools/testing/selftests/kvm/demand_paging_test.c|139| <<run_test>> vm = memstress_create_vm(mode, nr_vcpus, guest_percpu_mem_size, 1, p->src_type, p->partition_vcpu_memory_access);
+	 *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|155| <<run_test>> vm = memstress_create_vm(mode, nr_vcpus, guest_percpu_mem_size, p->slots, p->backing_src, p->partition_vcpu_memory_access);
+	 *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|98| <<run_test>> vm = memstress_create_vm(mode, nr_vcpus, guest_percpu_mem_size, 1,
+	 *                                                                                      VM_MEM_SRC_ANONYMOUS, p->partition_vcpu_memory_access);
+	 *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|103| <<run_test>> vm = memstress_create_vm(mode, VCPUS, guest_percpu_mem_size, SLOTS, backing_src, false);
+	 *
+	 * guest代码是memstress_guest_code()
+	 */
 	vm = memstress_create_vm(mode, VCPUS, guest_percpu_mem_size,
 				 SLOTS, backing_src, false);
 
 	guest_num_pages = (VCPUS * guest_percpu_mem_size) >> vm->page_shift;
 	guest_num_pages = vm_adjust_num_guest_pages(mode, guest_num_pages);
 	host_num_pages = vm_num_host_pages(mode, guest_num_pages);
+	/*
+	 * SLOTS是1
+	 */
 	pages_per_slot = host_num_pages / SLOTS;
 
 	bitmaps = memstress_alloc_bitmaps(SLOTS, pages_per_slot);
@@ -118,9 +182,23 @@ static void run_test(enum vm_guest_mode mode, void *unused)
 	iteration = -1;
 	host_quit = false;
 
+	/*
+	 * 在以下使用vcpu_last_completed_iteration[KVM_MAX_VCPUS]:
+	 *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|59| <<run_vcpu_iteration>> while (READ_ONCE(vcpu_last_completed_iteration[i]) !=
+	 *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|77| <<vcpu_worker>> vcpu_last_completed_iteration[vcpu_idx] = current_iteration;
+	 *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|122| <<run_test>> vcpu_last_completed_iteration[i] = -1;
+	 */
 	for (i = 0; i < VCPUS; i++)
 		vcpu_last_completed_iteration[i] = -1;
 
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|312| <<run_test>> memstress_start_vcpu_threads(nr_vcpus, vcpu_thread_main);
+	 *   - tools/testing/selftests/kvm/demand_paging_test.c|181| <<run_test>> memstress_start_vcpu_threads(nr_vcpus, vcpu_worker);
+	 *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|193| <<run_test>> memstress_start_vcpu_threads(nr_vcpus, vcpu_worker);
+	 *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|104| <<run_test>> memstress_start_vcpu_threads(nr_vcpus, vcpu_worker);
+	 *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|124| <<run_test>> memstress_start_vcpu_threads(VCPUS, vcpu_worker);
+	 */
 	memstress_start_vcpu_threads(VCPUS, vcpu_worker);
 
 	run_vcpu_iteration(vm);
diff --git a/tools/testing/selftests/kvm/x86_64/smm_test.c b/tools/testing/selftests/kvm/x86_64/smm_test.c
index e18b86666..47af798db 100644
--- a/tools/testing/selftests/kvm/x86_64/smm_test.c
+++ b/tools/testing/selftests/kvm/x86_64/smm_test.c
@@ -19,6 +19,15 @@
 #include "vmx.h"
 #include "svm_util.h"
 
+/*
+ * SMM模式通过调用SMI进入,进入之后,SMI就会disable,不过系统会
+ * 暂存一个且只有一个SMI,当SMM模式退出时,检测到这个SMI会再次进入SMM模式.
+ *
+ * 进入SMM模式后,系统切换到SMRAM这个独立的环境中.
+ *
+ * RSM会使得系统离开SMM模式,RSM只有在SMM中才可以执行.
+ */
+
 #define SMRAM_SIZE 65536
 #define SMRAM_MEMSLOT ((1 << 16) | 1)
 #define SMRAM_PAGES (SMRAM_SIZE / PAGE_SIZE)
@@ -37,20 +46,49 @@
  * independent subset of asm here.
  * SMI handler always report back fixed stage SMRAM_STAGE.
  */
+/*
+ * 在以下使用smi_handler[]:
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|158| <<main>> memcpy(addr_gpa2hva(vm, SMRAM_GPA) + 0x8000, smi_handler, sizeof(smi_handler));
+ */
 uint8_t smi_handler[] = {
 	0xb0, SMRAM_STAGE,    /* mov $SMRAM_STAGE, %al */
 	0xe4, SYNC_PORT,      /* in $SYNC_PORT, %al */
 	0x0f, 0xaa,           /* rsm */
 };
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|68| <<l2_guest_code>> sync_with_host(8);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|70| <<l2_guest_code>> sync_with_host(10);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|83| <<guest_code>> sync_with_host(1);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|87| <<guest_code>> sync_with_host(2);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|91| <<guest_code>> sync_with_host(4);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|104| <<guest_code>> sync_with_host(5);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|108| <<guest_code>> sync_with_host(7);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|119| <<guest_code>> sync_with_host(12);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|122| <<guest_code>> sync_with_host(DONE);
+ */
 static inline void sync_with_host(uint64_t phase)
 {
 	asm volatile("in $" XSTR(SYNC_PORT)", %%al \n"
 		     : "+a" (phase));
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|89| <<guest_code>> self_smi();
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|106| <<guest_code>> self_smi();
+ */
 static void self_smi(void)
 {
+	/*
+	 * 在以下使用APIC_DM_SMI:
+	 *   - arch/x86/include/asm/apicdef.h|86| <<global>> #define APIC_DM_SMI 0x00200
+	 *   - tools/testing/selftests/kvm/include/x86_64/apic.h|54| <<global>> #define APIC_DM_SMI 0x00200
+	 *   - arch/x86/kvm/lapic.c|1336| <<__apic_accept_irq>> case APIC_DM_SMI:
+	 *   - drivers/thermal/intel/therm_throt.c|747| <<intel_init_thermal>> if ((l & MSR_IA32_MISC_ENABLE_TM1) && (h & APIC_DM_SMI)) {
+	 *   - tools/testing/selftests/kvm/x86_64/smm_test.c|55| <<self_smi>> APIC_DEST_SELF | APIC_INT_ASSERT | APIC_DM_SMI);
+	 */
 	x2apic_write_reg(APIC_ICR,
 			 APIC_DEST_SELF | APIC_INT_ASSERT | APIC_DM_SMI);
 }
@@ -114,6 +152,11 @@ static void guest_code(void *arg)
 	sync_with_host(DONE);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|197| <<main (stage 8)>> inject_smi(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|206| <<main (stage 10)>> inject_smi(vcpu);
+ */
 void inject_smi(struct kvm_vcpu *vcpu)
 {
 	struct kvm_vcpu_events events;
@@ -141,12 +184,18 @@ int main(int argc, char *argv[])
 	/* Create VM */
 	vm = vm_create_with_one_vcpu(&vcpu, guest_code);
 
+	/*
+	 * #define SMRAM_GPA 0x1000000
+	 */
 	vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, SMRAM_GPA,
 				    SMRAM_MEMSLOT, SMRAM_PAGES, 0);
 	TEST_ASSERT(vm_phy_pages_alloc(vm, SMRAM_PAGES, SMRAM_GPA, SMRAM_MEMSLOT)
 		    == SMRAM_GPA, "could not allocate guest physical addresses?");
 
 	memset(addr_gpa2hva(vm, SMRAM_GPA), 0x0, SMRAM_SIZE);
+	/*
+	 * 根据smm虚拟化代码, 进入的ip是0x8000
+	 */
 	memcpy(addr_gpa2hva(vm, SMRAM_GPA) + 0x8000, smi_handler,
 	       sizeof(smi_handler));
 
@@ -171,11 +220,25 @@ int main(int argc, char *argv[])
 		memset(&regs, 0, sizeof(regs));
 		vcpu_regs_get(vcpu, &regs);
 
+		/*
+		 * 例子:
+		 * #define SMRAM_STAGE 0xfe
+		 *
+		 * mov $SMRAM_STAGE, %al
+		 *
+		 * 0xfe = 11111110
+		 * 0xff = 11111111
+		 * and后是11111110
+		 */
 		stage_reported = regs.rax & 0xff;
 
 		if (stage_reported == DONE)
 			goto done;
 
+		/*
+		 * 这里判断用到了SMRAM_STAGE,
+		 * 说明是smi handler来的
+		 */
 		TEST_ASSERT(stage_reported == stage ||
 			    stage_reported == SMRAM_STAGE,
 			    "Unexpected stage: #%x, got %x",
diff --git a/tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c b/tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c
index 41ea7028a..667e69c62 100644
--- a/tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c
+++ b/tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c
@@ -26,8 +26,19 @@
 
 bool have_evmcs;
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c|121| <<test_vmx_nested_state>> test_nested_state(vcpu, state);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c|142| <<test_vmx_nested_state>> test_nested_state(vcpu, state);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c|203| <<test_vmx_nested_state>> test_nested_state(vcpu, state);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c|211| <<test_vmx_nested_state>> test_nested_state(vcpu, state);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_set_nested_state_test.c|235| <<test_vmx_nested_state>> test_nested_state(vcpu, state);
+ */
 void test_nested_state(struct kvm_vcpu *vcpu, struct kvm_nested_state *state)
 {
+	/*
+	 * 调用KVM_SET_NESTED_STATE
+	 */
 	vcpu_nested_state_set(vcpu, state);
 }
 
diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c
index e033c79d5..682cea47a 100644
--- a/virt/kvm/async_pf.c
+++ b/virt/kvm/async_pf.c
@@ -19,6 +19,10 @@
 
 static struct kmem_cache *async_pf_cache;
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|6149| <<kvm_init>> r = kvm_async_pf_init();
+ */
 int kvm_async_pf_init(void)
 {
 	async_pf_cache = KMEM_CACHE(kvm_async_pf, 0);
@@ -29,19 +33,48 @@ int kvm_async_pf_init(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|6182| <<kvm_init>> kvm_async_pf_deinit();
+ *   - virt/kvm/kvm_main.c|6215| <<kvm_exit>> kvm_async_pf_deinit();
+ */
 void kvm_async_pf_deinit(void)
 {
 	kmem_cache_destroy(async_pf_cache);
 	async_pf_cache = NULL;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|492| <<kvm_vcpu_init>> kvm_async_pf_vcpu_init(vcpu);
+ */
 void kvm_async_pf_vcpu_init(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 *     struct {
+	 *         u32 queued;
+	 *         struct list_head queue;
+	 *         struct list_head done;
+	 *         spinlock_t lock;
+	 *     } async_pf;
+	 */
 	INIT_LIST_HEAD(&vcpu->async_pf.done);
 	INIT_LIST_HEAD(&vcpu->async_pf.queue);
 	spin_lock_init(&vcpu->async_pf.lock);
 }
 
+/*
+ * 在以下调用kvm_setup_async_pf():
+ *   - arch/s390/kvm/kvm-s390.c|4674| <<kvm_arch_setup_async_pf>> return kvm_setup_async_pf(vcpu, current->thread.gmap_addr, hva, &arch);
+ *   - arch/x86/kvm/mmu/mmu.c|5769| <<kvm_arch_setup_async_pf>> return kvm_setup_async_pf(vcpu, cr2_or_gpa, kvm_vcpu_gfn_to_hva(vcpu, gfn), &arch);
+ *
+ * 在以下使用async_pf_execute():
+ *   - virt/kvm/async_pf.c|191| <<kvm_setup_async_pf>> INIT_WORK(&work->work, async_pf_execute);
+ *
+ * async_pf_execute()是在work执行的
+ * 1. get_user_pages_remote()获得host内存页面
+ */
 static void async_pf_execute(struct work_struct *work)
 {
 	struct kvm_async_pf *apf =
@@ -61,19 +94,58 @@ static void async_pf_execute(struct work_struct *work)
 	 * access remotely.
 	 */
 	mmap_read_lock(mm);
+	/*
+	 * 重要的地方: 这里获得的页面!!!
+	 */
 	get_user_pages_remote(mm, addr, 1, FOLL_WRITE, NULL, &locked);
 	if (locked)
 		mmap_read_unlock(mm);
 
+	/*
+	 * 在以下使用CONFIG_KVM_ASYNC_PF_SYNC:
+	 *   - virt/kvm/async_pf.c|109| <<async_pf_execute>> if (IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
+	 *   - virt/kvm/async_pf.c|118| <<async_pf_execute>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+	 *   - virt/kvm/async_pf.c|166| <<kvm_clear_async_pf_completion_queue>> #ifdef CONFIG_KVM_ASYNC_PF_SYNC
+	 *   - virt/kvm/async_pf.c|229| <<kvm_check_async_pf_completion>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
+	 *   - virt/kvm/async_pf.c|311| <<kvm_async_pf_wakeup_all>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+	 *
+	 * kvm_arch_async_page_present():
+	 * 应该是插入中断, 告诉VM准备好了
+	 * vcpu->arch.apf.vec应该是用MSR_KVM_ASYNC_PF_INT->kvm_pv_enable_async_pf_int()设置的
+	 */
 	if (IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
 		kvm_arch_async_page_present(vcpu, apf);
 
 	spin_lock(&vcpu->async_pf.lock);
+	/*
+	 * 在以下使用async_pf.done:
+	 *   - arch/x86/kvm/x86.c|13319| <<kvm_vcpu_has_events>> if (!list_empty_careful(&vcpu->async_pf.done))
+	 *   - virt/kvm/async_pf.c|62| <<kvm_async_pf_vcpu_init>> INIT_LIST_HEAD(&vcpu->async_pf.done);
+	 *   - virt/kvm/async_pf.c|120| <<async_pf_execute>> first = list_empty(&vcpu->async_pf.done);
+	 *   - virt/kvm/async_pf.c|121| <<async_pf_execute>> list_add_tail(&apf->link, &vcpu->async_pf.done);
+	 *   - virt/kvm/async_pf.c|201| <<kvm_clear_async_pf_completion_queue>> while (!list_empty(&vcpu->async_pf.done)) {
+	 *   - virt/kvm/async_pf.c|203| <<kvm_clear_async_pf_completion_queue>> list_first_entry(&vcpu->async_pf.done,
+	 *   - virt/kvm/async_pf.c|231| <<kvm_check_async_pf_completion>> while (!list_empty_careful(&vcpu->async_pf.done) &&
+	 *   - virt/kvm/async_pf.c|234| <<kvm_check_async_pf_completion>> work = list_first_entry(&vcpu->async_pf.done, typeof(*work),
+	 *   - virt/kvm/async_pf.c|335| <<kvm_async_pf_wakeup_all>> if (!list_empty_careful(&vcpu->async_pf.done))
+	 *   - virt/kvm/async_pf.c|345| <<kvm_async_pf_wakeup_all>> spin_lock(&vcpu->async_pf.lock);
+	 *   - virt/kvm/async_pf.c|346| <<kvm_async_pf_wakeup_all>> first = list_empty(&vcpu->async_pf.done);
+	 *   - virt/kvm/async_pf.c|347| <<kvm_async_pf_wakeup_all>> list_add_tail(&work->link, &vcpu->async_pf.done);
+	 *   - virt/kvm/kvm_main.c|3812| <<vcpu_dy_runnable>> if (!list_empty_careful(&vcpu->async_pf.done))
+	 */
 	first = list_empty(&vcpu->async_pf.done);
 	list_add_tail(&apf->link, &vcpu->async_pf.done);
 	apf->vcpu = NULL;
 	spin_unlock(&vcpu->async_pf.lock);
 
+	/*
+	 * 在以下使用CONFIG_KVM_ASYNC_PF_SYNC:
+	 *   - virt/kvm/async_pf.c|109| <<async_pf_execute>> if (IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
+	 *   - virt/kvm/async_pf.c|118| <<async_pf_execute>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+	 *   - virt/kvm/async_pf.c|166| <<kvm_clear_async_pf_completion_queue>> #ifdef CONFIG_KVM_ASYNC_PF_SYNC
+	 *   - virt/kvm/async_pf.c|229| <<kvm_check_async_pf_completion>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
+	 *   - virt/kvm/async_pf.c|311| <<kvm_async_pf_wakeup_all>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+	 */
 	if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
 		kvm_arch_async_page_present_queued(vcpu);
 
@@ -90,6 +162,19 @@ static void async_pf_execute(struct work_struct *work)
 	kvm_put_kvm(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - arch/s390/kvm/interrupt.c|2701| <<flic_set_attr>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3440| <<kvm_arch_vcpu_destroy>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3947| <<kvm_arch_vcpu_create>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4176| <<kvm_arch_vcpu_ioctl_set_one_reg>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4207| <<kvm_arch_vcpu_ioctl_normal_reset>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4892| <<sync_regs_fmt2>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/x86/kvm/x86.c|971| <<kvm_post_set_cr0>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/x86/kvm/x86.c|3545| <<kvm_pv_enable_async_pf>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/x86/kvm/x86.c|12436| <<kvm_vcpu_reset>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/x86/kvm/x86.c|12759| <<kvm_unload_vcpu_mmus>> kvm_clear_async_pf_completion_queue(vcpu);
+ */
 void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 {
 	spin_lock(&vcpu->async_pf.lock);
@@ -109,6 +194,14 @@ void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 			continue;
 
 		spin_unlock(&vcpu->async_pf.lock);
+/*
+ * 在以下使用CONFIG_KVM_ASYNC_PF_SYNC:
+ *   - virt/kvm/async_pf.c|109| <<async_pf_execute>> if (IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
+ *   - virt/kvm/async_pf.c|118| <<async_pf_execute>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+ *   - virt/kvm/async_pf.c|166| <<kvm_clear_async_pf_completion_queue>> #ifdef CONFIG_KVM_ASYNC_PF_SYNC
+ *   - virt/kvm/async_pf.c|229| <<kvm_check_async_pf_completion>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
+ *   - virt/kvm/async_pf.c|311| <<kvm_async_pf_wakeup_all>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+ */
 #ifdef CONFIG_KVM_ASYNC_PF_SYNC
 		flush_work(&work->work);
 #else
@@ -133,6 +226,20 @@ void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 	vcpu->async_pf.queued = 0;
 }
 
+/*
+ * 在以下使用KVM_REQ_APF_READY:
+ *   - arch/x86/kvm/lapic.c|503| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+ *   - arch/x86/kvm/lapic.c|2573| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+ *   - arch/x86/kvm/x86.c|11001| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+ *   - arch/x86/kvm/x86.c|13617| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+ *
+ * 处理函数是kvm_check_async_pf_completion()
+ *
+ * called by:
+ *   - arch/s390/kvm/kvm-s390.c|4686| <<vcpu_pre_run>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|3999| <<kvm_set_msr_common(MSR_KVM_ASYNC_PF_ACK)>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|11002| <<vcpu_enter_guest(KVM_REQ_APF_READY)>> kvm_check_async_pf_completion(vcpu);
+ */
 void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
 {
 	struct kvm_async_pf *work;
@@ -145,7 +252,26 @@ void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
 		list_del(&work->link);
 		spin_unlock(&vcpu->async_pf.lock);
 
+		/*
+		 * 只在此处调用kvm_arch_async_page_ready()
+		 */
 		kvm_arch_async_page_ready(vcpu, work);
+		/*
+		 * 在以下使用CONFIG_KVM_ASYNC_PF_SYNC:
+		 *   - virt/kvm/async_pf.c|109| <<async_pf_execute>> if (IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
+		 *   - virt/kvm/async_pf.c|118| <<async_pf_execute>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+		 *   - virt/kvm/async_pf.c|166| <<kvm_clear_async_pf_completion_queue>> #ifdef CONFIG_KVM_ASYNC_PF_SYNC
+		 *   - virt/kvm/async_pf.c|229| <<kvm_check_async_pf_completion>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
+		 *   - virt/kvm/async_pf.c|311| <<kvm_async_pf_wakeup_all>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+		 *
+		 * 在以下使用MSR_KVM_ASYNC_PF_ACK:
+		 *   - arch/x86/kernel/kvm.c|302| <<DEFINE_IDTENTRY_SYSVEC(sysvec_kvm_asyncpf_interrupt)>> wrmsrl(MSR_KVM_ASYNC_PF_ACK, 1);
+		 *   - arch/x86/kvm/x86.c|4002| <<kvm_set_msr_common>> case MSR_KVM_ASYNC_PF_ACK:
+		 *   - arch/x86/kvm/x86.c|4385| <<kvm_get_msr_common>> case MSR_KVM_ASYNC_PF_ACK:
+		 *
+		 * 应该是插入中断, 告诉VM准备好了
+		 * vcpu->arch.apf.vec应该是用MSR_KVM_ASYNC_PF_INT->kvm_pv_enable_async_pf_int()设置的
+		 */
 		if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
 			kvm_arch_async_page_present(vcpu, work);
 
@@ -159,11 +285,30 @@ void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
  * Try to schedule a job to handle page fault asynchronously. Returns 'true' on
  * success, 'false' on failure (page fault has to be handled synchronously).
  */
+/*
+ * called by:
+ *   - arch/s390/kvm/kvm-s390.c|4674| <<kvm_arch_setup_async_pf>> return kvm_setup_async_pf(vcpu, current->thread.gmap_addr, hva, &arch);
+ *   - arch/x86/kvm/mmu/mmu.c|5769| <<kvm_arch_setup_async_pf>> return kvm_setup_async_pf(vcpu, cr2_or_gpa, kvm_vcpu_gfn_to_hva(vcpu, gfn), &arch);
+ *
+ * kvm_faultin_pfn()
+ * -> __kvm_faultin_pfn()
+ *    -> kvm_arch_setup_async_pf()
+ *       -> kvm_setup_async_pf()
+ */
 bool kvm_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			unsigned long hva, struct kvm_arch_async_pf *arch)
 {
 	struct kvm_async_pf *work;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 *     struct {
+	 *         u32 queued;
+	 *         struct list_head queue;
+	 *         struct list_head done;
+	 *         spinlock_t lock;
+	 *     } async_pf;
+	 */
 	if (vcpu->async_pf.queued >= ASYNC_PF_PER_VCPU)
 		return false;
 
@@ -190,8 +335,30 @@ bool kvm_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 
 	INIT_WORK(&work->work, async_pf_execute);
 
+	/*
+	 * 在以下使用async_pf.queue:
+	 *   - virt/kvm/async_pf.c|63| <<kvm_async_pf_vcpu_init>> INIT_LIST_HEAD(&vcpu->async_pf.queue);
+	 *   - virt/kvm/async_pf.c|167| <<kvm_clear_async_pf_completion_queue>> while (!list_empty(&vcpu->async_pf.queue)) {
+	 *   - virt/kvm/async_pf.c|169| <<kvm_clear_async_pf_completion_queue>> list_first_entry(&vcpu->async_pf.queue,
+	 *   - virt/kvm/async_pf.c|317| <<kvm_setup_async_pf>> list_add_tail(&work->queue, &vcpu->async_pf.queue);
+	 */
 	list_add_tail(&work->queue, &vcpu->async_pf.queue);
+	/*
+	 * 在以下使用async_pf.queued:
+	 *   - virt/kvm/async_pf.c|210| <<kvm_clear_async_pf_completion_queue>> vcpu->async_pf.queued = 0;
+	 *   - virt/kvm/async_pf.c|263| <<kvm_check_async_pf_completion>> vcpu->async_pf.queued--;
+	 *   - virt/kvm/async_pf.c|291| <<kvm_setup_async_pf>> if (vcpu->async_pf.queued >= ASYNC_PF_PER_VCPU)
+	 *   - virt/kvm/async_pf.c|318| <<kvm_setup_async_pf>> vcpu->async_pf.queued++;
+	 *   - virt/kvm/async_pf.c|361| <<kvm_async_pf_wakeup_all>> vcpu->async_pf.queued++;
+	 */
 	vcpu->async_pf.queued++;
+	/*
+	 * 在以下使用work->notpresent_injected:
+	 *   - arch/x86/kvm/x86.c|13780| <<kvm_arch_async_page_present>> if ((work->wakeup_all || work->notpresent_injected) &&
+	 *   - virt/kvm/async_pf.c|358| <<kvm_setup_async_pf>> work->notpresent_injected = kvm_arch_async_page_not_present(vcpu, work);
+	 *
+	 * 可能插入: kvm_inject_page_fault()
+	 */
 	work->notpresent_injected = kvm_arch_async_page_not_present(vcpu, work);
 
 	schedule_work(&work->work);
@@ -199,6 +366,10 @@ bool kvm_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3557| <<kvm_pv_enable_async_pf>> kvm_async_pf_wakeup_all(vcpu);
+ */
 int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu)
 {
 	struct kvm_async_pf *work;
@@ -219,6 +390,14 @@ int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu)
 	list_add_tail(&work->link, &vcpu->async_pf.done);
 	spin_unlock(&vcpu->async_pf.lock);
 
+	/*
+	 * 在以下使用CONFIG_KVM_ASYNC_PF_SYNC:
+	 *   - virt/kvm/async_pf.c|109| <<async_pf_execute>> if (IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
+	 *   - virt/kvm/async_pf.c|118| <<async_pf_execute>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+	 *   - virt/kvm/async_pf.c|166| <<kvm_clear_async_pf_completion_queue>> #ifdef CONFIG_KVM_ASYNC_PF_SYNC
+	 *   - virt/kvm/async_pf.c|229| <<kvm_check_async_pf_completion>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
+	 *   - virt/kvm/async_pf.c|311| <<kvm_async_pf_wakeup_all>> if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+	 */
 	if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
 		kvm_arch_async_page_present_queued(vcpu);
 
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 1e567d1f6..3b292af57 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -165,6 +165,16 @@ bool __weak kvm_arch_can_set_irq_routing(struct kvm *kvm)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|152| <<kvm_vgic_setup_default_irq_routing>> ret = kvm_set_irq_routing(kvm, entries, nr, 0);
+ *   - arch/powerpc/kvm/mpic.c|1649| <<mpic_set_default_irq_routing>> kvm_set_irq_routing(opp->kvm, routing, 0, 0);
+ *   - arch/riscv/kvm/vm.c|107| <<kvm_riscv_setup_default_irq_routing>> rc = kvm_set_irq_routing(kvm, ents, lines, 0);
+ *   - arch/s390/kvm/kvm-s390.c|3008| <<kvm_arch_vm_ioctl>> r = kvm_set_irq_routing(kvm, &routing, 0, 0);
+ *   - arch/x86/kvm/irq_comm.c|389| <<kvm_setup_default_irq_routing>> return kvm_set_irq_routing(kvm, default_routing, ARRAY_SIZE(default_routing), 0);
+ *   - arch/x86/kvm/irq_comm.c|397| <<kvm_setup_empty_irq_routing>> return kvm_set_irq_routing(kvm, empty_routing, 0, 0);
+ *   - virt/kvm/kvm_main.c|5072| <<kvm_vm_ioctl>> r = kvm_set_irq_routing(kvm, entries, routing.nr, routing.flags);
+ */
 int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *ue,
 			unsigned nr,
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 7db96875a..bf1c26f2d 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -343,6 +343,22 @@ bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req)
 }
 EXPORT_SYMBOL_GPL(kvm_make_all_cpus_request);
 
+/*
+ * called by:
+ *   - arch/loongarch/kvm/mmu.c|303| <<kvm_flush_range>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/loongarch/kvm/mmu.c|393| <<kvm_arch_commit_memory_region>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/loongarch/kvm/mmu.c|913| <<kvm_arch_flush_remote_tlbs_memslot>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/mips/kvm/mips.c|187| <<kvm_arch_flush_shadow_all>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/riscv/kvm/mmu.c|343| <<gstage_wp_memory_region>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|2950| <<kvm_mmu_remote_flush_or_zap>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|3257| <<kvm_mmu_find_shadow_page>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|3948| <<kvm_mmu_commit_zap_page>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/page_track.c|118| <<__kvm_write_track_add_gfn>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1045| <<tdp_mmu_iter_cond_resched>> kvm_flush_remote_tlbs(kvm);
+ *   - virt/kvm/kvm_main.c|377| <<kvm_flush_remote_tlbs_range>> kvm_flush_remote_tlbs(kvm);
+ *   - virt/kvm/kvm_main.c|643| <<__kvm_handle_hva_range>> kvm_flush_remote_tlbs(kvm);
+ *   - virt/kvm/kvm_main.c|4766| <<kvm_vm_ioctl_reset_dirty_pages>> kvm_flush_remote_tlbs(kvm);
+ */
 void kvm_flush_remote_tlbs(struct kvm *kvm)
 {
 	++kvm->stat.generic.remote_tlb_flush_requests;
@@ -377,6 +393,16 @@ void kvm_flush_remote_tlbs_range(struct kvm *kvm, gfn_t gfn, u64 nr_pages)
 	kvm_flush_remote_tlbs(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1147| <<kvm_mmu_wp_memory_region>> kvm_flush_remote_tlbs_memslot(kvm, memslot);
+ *   - arch/mips/kvm/mips.c|202| <<kvm_arch_flush_shadow_memslot>> kvm_flush_remote_tlbs_memslot(kvm, slot);
+ *   - arch/mips/kvm/mips.c|238| <<kvm_arch_commit_memory_region>> kvm_flush_remote_tlbs_memslot(kvm, new);
+ *   - arch/x86/kvm/mmu/mmu.c|8687| <<kvm_rmap_zap_collapsible_sptes>> kvm_flush_remote_tlbs_memslot(kvm, slot);
+ *   - arch/x86/kvm/x86.c|13423| <<kvm_mmu_slot_apply_flags>> kvm_flush_remote_tlbs_memslot(kvm, new);
+ *   - virt/kvm/kvm_main.c|2281| <<kvm_get_dirty_log_protect>> kvm_flush_remote_tlbs_memslot(kvm, memslot);
+ *   - virt/kvm/kvm_main.c|2409| <<kvm_clear_dirty_log_protect>> kvm_flush_remote_tlbs_memslot(kvm, memslot);
+ */
 void kvm_flush_remote_tlbs_memslot(struct kvm *kvm,
 				   const struct kvm_memory_slot *memslot)
 {
@@ -391,6 +417,11 @@ void kvm_flush_remote_tlbs_memslot(struct kvm *kvm,
 	kvm_flush_remote_tlbs_range(kvm, memslot->base_gfn, memslot->npages);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|939| <<kvm_mmu_notifier_release>> kvm_flush_shadow_all(kvm);
+ *   - virt/kvm/kvm_main.c|1369| <<kvm_destroy_vm>> kvm_flush_shadow_all(kvm);
+ */
 static void kvm_flush_shadow_all(struct kvm *kvm)
 {
 	kvm_arch_flush_shadow_all(kvm);
@@ -694,6 +725,10 @@ static __always_inline int kvm_handle_hva_range_no_flush(struct mmu_notifier *mn
 	return __kvm_handle_hva_range(kvm, &range);
 }
 
+/*
+ * 在以下使用kvm_change_spte_gfn():
+ *   - virt/kvm/kvm_main.c|757| <<kvm_mmu_notifier_change_pte>> kvm_handle_hva_range(mn, address, address + 1, arg, kvm_change_spte_gfn);
+ */
 static bool kvm_change_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	/*
@@ -711,6 +746,21 @@ static bool kvm_change_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 	return kvm_set_spte_gfn(kvm, range);
 }
 
+/*
+ * static const struct mmu_notifier_ops kvm_mmu_notifier_ops = {
+ *         .invalidate_range_start = kvm_mmu_notifier_invalidate_range_start,
+ *         .invalidate_range_end   = kvm_mmu_notifier_invalidate_range_end,
+ *         .clear_flush_young      = kvm_mmu_notifier_clear_flush_young,
+ *         .clear_young            = kvm_mmu_notifier_clear_young,
+ *         .test_young             = kvm_mmu_notifier_test_young,
+ *         .change_pte             = kvm_mmu_notifier_change_pte,
+ *         .release                = kvm_mmu_notifier_release,
+ * };
+ *
+ * 注释:
+ * change_pte is called in cases that pte mapping to page is changed:
+ * for example, when ksm remaps pte to point to a new shared page.
+ */
 static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,
 					struct mm_struct *mm,
 					unsigned long address,
@@ -765,6 +815,10 @@ void kvm_mmu_invalidate_begin(struct kvm *kvm, unsigned long start,
 	}
 }
 
+/*
+ * 只在以下使用:
+ * struct mmu_notifier_ops kvm_mmu_notifier_ops.invalidate_range_start = kvm_mmu_notifier_invalidate_range_start()
+ */
 static int kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
 					const struct mmu_notifier_range *range)
 {
@@ -829,6 +883,10 @@ void kvm_mmu_invalidate_end(struct kvm *kvm, unsigned long start,
 	kvm->mmu_invalidate_in_progress--;
 }
 
+/*
+ * 只在以下使用:
+ * struct mmu_notifier_ops kvm_mmu_notifier_ops.invalidate_range_end   = kvm_mmu_notifier_invalidate_range_end()
+ */
 static void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,
 					const struct mmu_notifier_range *range)
 {
@@ -916,6 +974,10 @@ static void kvm_mmu_notifier_release(struct mmu_notifier *mn,
 	srcu_read_unlock(&kvm->srcu, idx);
 }
 
+/*
+ * 在以下使用kvm_mmu_notifier_ops:
+ *   - virt/kvm/kvm_main.c|931| <<kvm_init_mmu_notifier>> kvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;
+ */
 static const struct mmu_notifier_ops kvm_mmu_notifier_ops = {
 	.invalidate_range_start	= kvm_mmu_notifier_invalidate_range_start,
 	.invalidate_range_end	= kvm_mmu_notifier_invalidate_range_end,
@@ -929,6 +991,16 @@ static const struct mmu_notifier_ops kvm_mmu_notifier_ops = {
 static int kvm_init_mmu_notifier(struct kvm *kvm)
 {
 	kvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;
+	/*
+	 * called by:
+	 *   - arch/s390/kvm/pv.c|605| <<kvm_s390_pv_init_vm>> mmu_notifier_register(&kvm->arch.pv.mmu_notifier, kvm->mm);
+	 *   - drivers/infiniband/hw/hfi1/mmu_rb.c|68| <<hfi1_mmu_rb_register>> ret = mmu_notifier_register(&h->mn, current->mm);
+	 *   - drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c|320| <<arm_smmu_mmu_notifier_get>> ret = mmu_notifier_register(&smmu_mn->mn, mm);
+	 *   - drivers/iommu/intel/svm.c|338| <<intel_svm_bind_mm>> ret = mmu_notifier_register(&svm->notifier, mm);
+	 *   - drivers/misc/ocxl/link.c|596| <<ocxl_link_add_pe>> mmu_notifier_register(&pe_data->mmu_notifier, mm);
+	 *   - mm/mmu_notifier.c|999| <<mmu_interval_notifier_insert>> ret = mmu_notifier_register(NULL, mm);
+	 *   - virt/kvm/kvm_main.c|932| <<kvm_init_mmu_notifier>> return mmu_notifier_register(&kvm->mmu_notifier, current->mm);
+	 */
 	return mmu_notifier_register(&kvm->mmu_notifier, current->mm);
 }
 
@@ -1394,6 +1466,10 @@ static int kvm_vm_release(struct inode *inode, struct file *filp)
  * Allocation size is twice as large as the actual dirty bitmap size.
  * See kvm_vm_ioctl_get_dirty_log() why this is needed.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1655| <<kvm_prepare_memory_region>> r = kvm_alloc_dirty_bitmap(new);
+ */
 static int kvm_alloc_dirty_bitmap(struct kvm_memory_slot *memslot)
 {
 	unsigned long dirty_bytes = kvm_dirty_bitmap_bytes(memslot);
@@ -2135,6 +2211,10 @@ EXPORT_SYMBOL_GPL(kvm_get_dirty_log);
  * exiting to userspace will be logged for the next call.
  *
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2258| <<kvm_vm_ioctl_get_dirty_log>> r = kvm_get_dirty_log_protect(kvm, log);
+ */
 static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 {
 	struct kvm_memslots *slots;
@@ -2149,6 +2229,16 @@ static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 	if (!kvm_use_dirty_bitmap(kvm))
 		return -ENXIO;
 
+	/*
+	 * struct kvm_dirty_log {
+	 *     __u32 slot;
+	 *     __u32 padding1;
+	 *     union {
+	 *         void __user *dirty_bitmap; // one bit per page
+	 *         __u64 padding2;
+	 *     };
+	 * };
+	 */
 	as_id = log->slot >> 16;
 	id = (u16)log->slot;
 	if (as_id >= KVM_ADDRESS_SPACE_NUM || id >= KVM_USER_MEM_SLOTS)
@@ -2165,6 +2255,12 @@ static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 
 	n = kvm_dirty_bitmap_bytes(memslot);
 	flush = false;
+	/*
+	 * 在以下使用kvm->manual_dirty_log_protect:
+	 *   - include/linux/kvm_host.h|921| <<kvm_dirty_log_manual_protect_and_init_set>> return !!(kvm->manual_dirty_log_protect & KVM_DIRTY_LOG_INITIALLY_SET);
+	 *   - virt/kvm/kvm_main.c|2190| <<kvm_get_dirty_log_protect>> if (kvm->manual_dirty_log_protect) {
+	 *   - virt/kvm/kvm_main.c|4761| <<kvm_vm_ioctl_enable_cap_generic(KVM_CAP_MANUAL_DIRTY_LOG_PROTECT2)>> kvm->manual_dirty_log_protect = cap->args[0];
+	 */
 	if (kvm->manual_dirty_log_protect) {
 		/*
 		 * Unlike kvm_get_dirty_log, we always return false in *flush,
@@ -2176,6 +2272,9 @@ static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 		 */
 		dirty_bitmap_buffer = dirty_bitmap;
 	} else {
+		/*
+		 * 当时分配的时候在memslot->dirty_bitmap应该是分配了两大块
+		 */
 		dirty_bitmap_buffer = kvm_second_dirty_bitmap(memslot);
 		memset(dirty_bitmap_buffer, 0, n);
 
@@ -2188,9 +2287,18 @@ static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 				continue;
 
 			flush = true;
+			/*
+			 * 返回旧的value
+			 */
 			mask = xchg(&dirty_bitmap[i], 0);
 			dirty_bitmap_buffer[i] = mask;
 
+			/*
+			 * called by:
+			 *   - virt/kvm/dirty_ring.c|70| <<kvm_reset_dirty_gfn>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+			 *   - virt/kvm/kvm_main.c|2209| <<kvm_get_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+			 *   - virt/kvm/kvm_main.c|2326| <<kvm_clear_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+			 */
 			offset = i * BITS_PER_LONG;
 			kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot,
 								offset, mask);
@@ -2226,6 +2334,11 @@ static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
  *   3. Copy the snapshot to the userspace.
  *   4. Flush TLB's if needed.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4988| <<kvm_vm_ioctl>> r = kvm_vm_ioctl_get_dirty_log(kvm, &log);
+ *   - virt/kvm/kvm_main.c|5222| <<kvm_vm_compat_ioctl>> r = kvm_vm_ioctl_get_dirty_log(kvm, &log);
+ */
 static int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 				      struct kvm_dirty_log *log)
 {
@@ -2245,6 +2358,17 @@ static int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
  * @kvm:	pointer to kvm instance
  * @log:	slot id and address from which to fetch the bitmap of dirty pages
  */
+/*
+ * struct kvm_clear_dirty_log {
+ *     __u32 slot;
+ *     __u32 num_pages;
+ *     __u64 first_page;
+ *     union {
+ *         void __user *dirty_bitmap; // one bit per page
+ *         __u64 padding2;
+ *     };
+ * };
+ */
 static int kvm_clear_dirty_log_protect(struct kvm *kvm,
 				       struct kvm_clear_dirty_log *log)
 {
@@ -2341,6 +2465,24 @@ struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(gfn_to_memslot);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|994| <<gfn_to_memslot_dirty_bitmap>> slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|1720| <<kvm_vcpu_write_protect_gfn>> slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|335| <<kvm_mmu_do_page_fault>> fault.slot = kvm_vcpu_gfn_to_memslot(vcpu, fault.gfn);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|400| <<FNAME(walk_addr_generic)>> slot = kvm_vcpu_gfn_to_memslot(vcpu, gpa_to_gfn(real_gpa));
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|809| <<FNAME(page_fault)>> fault->slot = kvm_vcpu_gfn_to_memslot(vcpu, fault->gfn);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|989| <<FNAME(sync_spte)>> slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+ *   - virt/kvm/kvm_main.c|2401| <<kvm_vcpu_is_visible_gfn>> struct kvm_memory_slot *memslot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+ *   - virt/kvm/kvm_main.c|2472| <<kvm_vcpu_gfn_to_hva>> return gfn_to_hva_many(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, NULL);
+ *   - virt/kvm/kvm_main.c|2504| <<kvm_vcpu_gfn_to_hva_prot>> struct kvm_memory_slot *slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+ *   - virt/kvm/kvm_main.c|2809| <<kvm_vcpu_gfn_to_pfn_atomic>> return gfn_to_pfn_memslot_atomic(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
+ *   - virt/kvm/kvm_main.c|2821| <<kvm_vcpu_gfn_to_pfn>> return gfn_to_pfn_memslot(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
+ *   - virt/kvm/kvm_main.c|3062| <<kvm_vcpu_read_guest_page>> struct kvm_memory_slot *slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+ *   - virt/kvm/kvm_main.c|3129| <<kvm_vcpu_read_guest_atomic>> struct kvm_memory_slot *slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+ *   - virt/kvm/kvm_main.c|3165| <<kvm_vcpu_write_guest_page>> struct kvm_memory_slot *slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+ *   - virt/kvm/kvm_main.c|3410| <<kvm_vcpu_mark_page_dirty>> memslot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+ */
 struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	struct kvm_memslots *slots = kvm_vcpu_memslots(vcpu);
@@ -2467,6 +2609,19 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_hva);
  * @writable: used to return the read/write attribute of the @slot if the hva
  * is valid and @writable is not NULL
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1683| <<kvm_handle_guest_abort>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+ *   - arch/loongarch/kvm/mmu.c|771| <<kvm_map_page>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writeable);
+ *   - arch/riscv/kvm/vcpu_exit.c|25| <<gstage_page_fault>> hva = gfn_to_hva_memslot_prot(memslot, gfn, &writable);
+ *   - arch/s390/kvm/gaccess.c|1006| <<access_guest_page_with_key>> hva = gfn_to_hva_memslot_prot(slot, gfn, &writable);
+ *   - arch/s390/kvm/gaccess.c|1184| <<cmpxchg_guest_abs_with_key>> hva = gfn_to_hva_memslot_prot(slot, gfn, &writable);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|413| <<FNAME(walk_addr_generic)>> host_addr = gfn_to_hva_memslot_prot(slot, gpa_to_gfn(real_gpa), &walker->pte_writable[walker->level - 1]);
+ *   - virt/kvm/kvm_main.c|2517| <<gfn_to_hva_prot>> return gfn_to_hva_memslot_prot(slot, gfn, writable);
+ *   - virt/kvm/kvm_main.c|2524| <<kvm_vcpu_gfn_to_hva_prot>> return gfn_to_hva_memslot_prot(slot, gfn, writable);
+ *   - virt/kvm/kvm_main.c|3059| <<__kvm_read_guest_page>> addr = gfn_to_hva_memslot_prot(slot, gfn, NULL);
+ *   - virt/kvm/kvm_main.c|3132| <<__kvm_read_guest_atomic>> addr = gfn_to_hva_memslot_prot(slot, gfn, NULL);
+ */
 unsigned long gfn_to_hva_memslot_prot(struct kvm_memory_slot *slot,
 				      gfn_t gfn, bool *writable)
 {
@@ -2946,6 +3101,33 @@ void kvm_release_page_clean(struct page *page)
 }
 EXPORT_SYMBOL_GPL(kvm_release_page_clean);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|1071| <<kvm_vm_ioctl_mte_copy_tags>> kvm_release_pfn_clean(pfn);
+ *   - arch/arm64/kvm/mmu.c|1593| <<user_mem_abort>> kvm_release_pfn_clean(pfn);
+ *   - arch/loongarch/kvm/mmu.c|817| <<kvm_map_page>> kvm_release_pfn_clean(pfn);
+ *   - arch/loongarch/kvm/mmu.c|880| <<kvm_map_page>> kvm_release_pfn_clean(pfn);
+ *   - arch/mips/kvm/mmu.c|647| <<kvm_mips_map_page>> kvm_release_pfn_clean(pfn);
+ *   - arch/mips/kvm/mmu.c|676| <<kvm_mips_map_page>> kvm_release_pfn_clean(pfn);
+ *   - arch/powerpc/kvm/book3s_32_mmu_host.c|235| <<kvmppc_mmu_map_page>> kvm_release_pfn_clean(hpaddr >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_32_mmu_host.c|253| <<kvmppc_mmu_map_page>> kvm_release_pfn_clean(hpaddr >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_64_mmu_host.c|206| <<kvmppc_mmu_map_page>> kvm_release_pfn_clean(pfn);
+ *   - arch/powerpc/kvm/book3s_hv_uvmem.c|914| <<kvmppc_share_page>> kvm_release_pfn_clean(pfn);
+ *   - arch/powerpc/kvm/book3s_hv_uvmem.c|923| <<kvmppc_share_page>> kvm_release_pfn_clean(pfn);
+ *   - arch/powerpc/kvm/book3s_hv_uvmem.c|1100| <<kvmppc_send_page_to_uv>> kvm_release_pfn_clean(pfn);
+ *   - arch/powerpc/kvm/e500_mmu_host.c|505| <<kvmppc_e500_shadow_map>> kvm_release_pfn_clean(pfn);
+ *   - arch/riscv/kvm/mmu.c|707| <<kvm_riscv_gstage_map>> kvm_release_pfn_clean(hfn);
+ *   - arch/x86/kvm/mmu/mmu.c|5984| <<direct_page_fault>> kvm_release_pfn_clean(fault->pfn);
+ *   - arch/x86/kvm/mmu/mmu.c|6106| <<kvm_tdp_mmu_page_fault>> kvm_release_pfn_clean(fault->pfn);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|652| <<FNAME(prefetch_gpte)>> kvm_release_pfn_clean(pfn);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1099| <<FNAME(page_fault)>> kvm_release_pfn_clean(fault->pfn);
+ *   - arch/x86/kvm/vmx/vmx.c|6975| <<vmx_set_apic_access_page_addr>> kvm_release_pfn_clean(pfn);
+ *   - arch/x86/kvm/x86.c|8980| <<reexecute_instruction>> kvm_release_pfn_clean(pfn);
+ *   - virt/kvm/kvm_main.c|2938| <<kvm_release_pfn>> kvm_release_pfn_clean(pfn);
+ *   - virt/kvm/pfncache.c|180| <<hva_to_pfn_retry>> kvm_release_pfn_clean(new_pfn);
+ *   - virt/kvm/pfncache.c|206| <<hva_to_pfn_retry>> kvm_release_pfn_clean(new_pfn);
+ *   - virt/kvm/pfncache.c|229| <<hva_to_pfn_retry>> kvm_release_pfn_clean(new_pfn);
+ */
 void kvm_release_pfn_clean(kvm_pfn_t pfn)
 {
 	struct page *page;
@@ -3175,6 +3357,21 @@ int kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,
 }
 EXPORT_SYMBOL_GPL(kvm_write_guest);
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_hv_nested.c|248| <<kvmhv_write_guest_state_and_regs>> return kvm_vcpu_write_guest(vcpu, hv_ptr, l2_hv, size) ||
+ *   - arch/powerpc/kvm/book3s_hv_nested.c|249| <<kvmhv_write_guest_state_and_regs>> kvm_vcpu_write_guest(vcpu, regs_ptr, l2_regs, sizeof(struct pt_regs));
+ *   - arch/powerpc/kvm/book3s_hv_nested.c|628| <<kvmhv_copy_tofrom_guest_nested>> rc = kvm_vcpu_write_guest(vcpu, gp_to, buf, n);
+ *   - arch/powerpc/kvm/powerpc.c|1246| <<kvmppc_complete_mmio_load>> kvm_vcpu_write_guest(vcpu, vcpu->arch.nested_io_gpr, &gpr, sizeof(gpr));
+ *   - arch/x86/kvm/hyperv.c|1377| <<kvm_hv_set_msr_pw>> if (kvm_vcpu_write_guest(vcpu, addr, instructions, i))
+ *   - arch/x86/kvm/smm.c|416| <<enter_smm>> if (kvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, &smram, sizeof(smram)))
+ *   - arch/x86/kvm/vmx/nested.c|1011| <<nested_vmx_store_msr>> if (kvm_vcpu_write_guest(vcpu, gpa + i * sizeof(e) + offsetof(struct vmx_msr_entry, value), &data, sizeof(data))) {
+ *   - arch/x86/kvm/vmx/nested.c|5319| <<handle_vmclear>> (void )kvm_vcpu_write_guest(vcpu, vmptr + offsetof(struct vmcs12, launch_state), &zero, sizeof(zero));
+ *   - arch/x86/kvm/x86.c|7587| <<kvm_write_guest_virt_helper>> ret = kvm_vcpu_write_guest(vcpu, gpa, data, towrite);
+ *   - arch/x86/kvm/x86.c|7712| <<emulator_write_phys>> ret = kvm_vcpu_write_guest(vcpu, gpa, val, bytes);
+ *   - arch/x86/kvm/xen.c|1127| <<kvm_xen_write_hypercall_page>> if (kvm_vcpu_write_guest(vcpu, page_addr + (i * sizeof(instructions)), instructions, sizeof(instructions)))
+ *   - arch/x86/kvm/xen.c|1153| <<kvm_xen_write_hypercall_page>> ret = kvm_vcpu_write_guest(vcpu, page_addr, page, PAGE_SIZE);
+ */
 int kvm_vcpu_write_guest(struct kvm_vcpu *vcpu, gpa_t gpa, const void *data,
 		         unsigned long len)
 {
@@ -4827,6 +5024,17 @@ static long kvm_vm_ioctl(struct file *filp,
 	}
 #ifdef CONFIG_KVM_GENERIC_DIRTYLOG_READ_PROTECT
 	case KVM_CLEAR_DIRTY_LOG: {
+		/*
+		 * struct kvm_clear_dirty_log {
+		 *     __u32 slot;
+		 *     __u32 num_pages;
+		 *     __u64 first_page;
+		 *     union {
+		 *         void __user *dirty_bitmap; // one bit per page
+		 *         __u64 padding2;
+		 *     };
+		 * };
+		 */
 		struct kvm_clear_dirty_log log;
 
 		r = -EFAULT;
-- 
2.34.1

