From 6f509f31b696c7e4dfd49d72797bf1886115aecf Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 30 Sep 2024 11:28:04 -0700
Subject: [PATCH 1/1] linux-v6.11

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/arm_pmuv3.h |   4 +
 arch/arm64/include/asm/kvm_host.h  |  85 +++
 arch/arm64/kernel/acpi.c           |   6 +
 arch/arm64/kvm/arm.c               |  72 +++
 arch/arm64/kvm/guest.c             |   4 +
 arch/arm64/kvm/pmu-emul.c          | 973 +++++++++++++++++++++++++++++
 arch/arm64/kvm/sys_regs.c          |  48 ++
 arch/arm64/kvm/vgic/vgic.c         |  13 +
 arch/x86/events/amd/core.c         |   5 +
 arch/x86/events/core.c             |   9 +
 arch/x86/events/perf_event.h       |  10 +
 arch/x86/kernel/nmi.c              |   7 +
 drivers/pci/bus.c                  |  46 ++
 drivers/pci/setup-bus.c            |   4 +
 drivers/pci/setup-res.c            |  30 +
 drivers/perf/arm_pmu.c             |  55 ++
 drivers/perf/arm_pmu_acpi.c        |   4 +
 drivers/perf/arm_pmuv3.c           | 193 +++++-
 fs/locks.c                         | 117 ++++
 fs/nfs/nfs4proc.c                  |   7 +
 fs/nfsd/nfs4state.c                |  46 ++
 include/kvm/arm_pmu.h              |  51 ++
 include/linux/filelock.h           |  26 +
 include/linux/perf_event.h         |  41 ++
 kernel/events/core.c               |  78 +++
 kernel/resource.c                  |  46 ++
 26 files changed, 1979 insertions(+), 1 deletion(-)

diff --git a/arch/arm64/include/asm/arm_pmuv3.h b/arch/arm64/include/asm/arm_pmuv3.h
index a4697a0b6..41daa4e6f 100644
--- a/arch/arm64/include/asm/arm_pmuv3.h
+++ b/arch/arm64/include/asm/arm_pmuv3.h
@@ -91,6 +91,10 @@ static inline void write_pmintenclr(u32 val)
 	write_sysreg(val, pmintenclr_el1);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|662| <<armv8pmu_write_event_type>> write_pmccfiltr(hwc->config_base);
+ */
 static inline void write_pmccfiltr(u64 val)
 {
 	write_sysreg(val, pmccfiltr_el0);
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index a33f5996c..d7804263b 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -44,10 +44,26 @@
 
 #define KVM_REQ_SLEEP \
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_IRQ_PENDING:
+ *   - arch/arm64/kvm/arm.c|1009| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/arm.c|1348| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|865| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|351| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|401| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|717| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|998| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ */
 #define KVM_REQ_IRQ_PENDING	KVM_ARCH_REQ(1)
 #define KVM_REQ_VCPU_RESET	KVM_ARCH_REQ(2)
 #define KVM_REQ_RECORD_STEAL	KVM_ARCH_REQ(3)
 #define KVM_REQ_RELOAD_GICv4	KVM_ARCH_REQ(4)
+/*
+ * 在以下使用KVM_REQ_RELOAD_PMU:
+ *   - arch/arm64/kvm/arm.c|1022| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu)) kvm_vcpu_reload_pmu(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|1413| <<kvm_arm_pmu_v3_enable>> kvm_make_request(KVM_REQ_RELOAD_PMU, vcpu);
+ */
 #define KVM_REQ_RELOAD_PMU	KVM_ARCH_REQ(5)
 #define KVM_REQ_SUSPEND		KVM_ARCH_REQ(6)
 #define KVM_REQ_RESYNC_PMU_EL0	KVM_ARCH_REQ(7)
@@ -328,6 +344,23 @@ struct kvm_arch {
 #define KVM_ARCH_FLAG_FGU_INITIALIZED			8
 	unsigned long flags;
 
+	/*
+	 * #define KVM_ARM_VCPU_POWER_OFF          0 // CPU is started in OFF state
+	 * #define KVM_ARM_VCPU_EL1_32BIT          1 // CPU running a 32bit VM
+	 * #define KVM_ARM_VCPU_PSCI_0_2           2 // CPU uses PSCI v0.2
+	 * #define KVM_ARM_VCPU_PMU_V3             3 // Support guest PMUv3
+	 * #define KVM_ARM_VCPU_SVE                4 // enable SVE for this CPU
+	 * #define KVM_ARM_VCPU_PTRAUTH_ADDRESS    5 // VCPU uses address authentication
+	 * #define KVM_ARM_VCPU_PTRAUTH_GENERIC    6 // VCPU uses generic authentication
+	 * #define KVM_ARM_VCPU_HAS_EL2            7 // Support nested virtualization
+	 *
+	 * 在以下使用kvm_arch->vcpu_features:
+	 *   - arch/arm64/include/asm/kvm_host.h|348| <<global>> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/include/asm/kvm_host.h|1427| <<__vcpu_has_feature>> return test_bit(feature, ka->vcpu_features);
+	 *   - arch/arm64/kvm/arm.c|212| <<kvm_arch_init_vm>> bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1480| <<kvm_vcpu_init_changed>> return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1516| <<__kvm_vcpu_set_target>> bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 */
 	/* VM-wide vCPU feature set */
 	DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
 
@@ -338,11 +371,42 @@ struct kvm_arch {
 	 * VM-wide PMU filter, implemented as a bitmap and big enough for
 	 * up to 2^10 events (ARMv8.0) or 2^16 events (ARMv8.1+).
 	 */
+	/*
+	 * 在以下使用kvm_arch->pmu_filter:
+	 *   - arch/arm64/kvm/arm.c|256| <<kvm_arch_destroy_vm>> bitmap_free(kvm->arch.pmu_filter);
+	 *   - arch/arm64/kvm/pmu-emul.c|1094| <<kvm_pmu_create_perf_event>> if (vcpu->kvm->arch.pmu_filter && !test_bit(eventsel, vcpu->kvm->arch.pmu_filter))
+	 *   - arch/arm64/kvm/pmu-emul.c|1095| <<kvm_pmu_create_perf_event>> !test_bit(eventsel, vcpu->kvm->arch.pmu_filter))
+	 *   - arch/arm64/kvm/pmu-emul.c|1234| <<kvm_pmu_get_pmceid>> unsigned long *bmap = vcpu->kvm->arch.pmu_filter;
+	 *   - arch/arm64/kvm/pmu-emul.c|1443| <<kvm_arm_pmu_v3_set_pmu>> (kvm->arch.pmu_filter && kvm->arch.arm_pmu != arm_pmu)) {
+	 *   - arch/arm64/kvm/pmu-emul.c|1522| <<kvm_arm_pmu_v3_set_attr>> if (!kvm->arch.pmu_filter) {
+	 *   - arch/arm64/kvm/pmu-emul.c|1523| <<kvm_arm_pmu_v3_set_attr>> kvm->arch.pmu_filter = bitmap_alloc(nr_events, GFP_KERNEL_ACCOUNT);
+	 *   - arch/arm64/kvm/pmu-emul.c|1524| <<kvm_arm_pmu_v3_set_attr>> if (!kvm->arch.pmu_filter)
+	 *   - arch/arm64/kvm/pmu-emul.c|1534| <<kvm_arm_pmu_v3_set_attr>> bitmap_zero(kvm->arch.pmu_filter, nr_events);
+	 *   - arch/arm64/kvm/pmu-emul.c|1536| <<kvm_arm_pmu_v3_set_attr>> bitmap_fill(kvm->arch.pmu_filter, nr_events);
+	 *   - arch/arm64/kvm/pmu-emul.c|1540| <<kvm_arm_pmu_v3_set_attr>> bitmap_set(kvm->arch.pmu_filter, filter.base_event, filter.nevents);
+	 *   - arch/arm64/kvm/pmu-emul.c|1542| <<kvm_arm_pmu_v3_set_attr>> bitmap_clear(kvm->arch.pmu_filter, filter.base_event, filter.nevents);
+	 */
 	unsigned long *pmu_filter;
 	struct arm_pmu *arm_pmu;
 
 	cpumask_var_t supported_cpus;
 
+	/*
+	 * 在以下使用kvm_arch->pmcr_n:
+	 *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+	 *   - arch/arm64/kvm/pmu-emul.c|1969| <<kvm_vcpu_read_pmcr>> return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
+	 *   - arch/arm64/kvm/sys_regs.c|862| <<reset_pmu_reg>> u8 n = vcpu->kvm->arch.pmcr_n;
+	 *   - arch/arm64/kvm/sys_regs.c|1321| <<set_pmcr>> kvm->arch.pmcr_n = new_n;
+	 *
+	 * PMCR寄存器:
+	 * PMCR_EL0, Performance Monitors Control Register
+	 *
+	 * Provides details of the Performance Monitors implementation, including the
+	 * number of counters implemented, and configures and controls the counters.
+	 *
+	 * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+	 * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+	 */
 	/* PMCR_EL0.N value for the guest */
 	u8 pmcr_n;
 
@@ -1375,8 +1439,29 @@ bool kvm_arm_vcpu_is_finalized(struct kvm_vcpu *vcpu);
 #define kvm_vm_has_ran_once(kvm)					\
 	(test_bit(KVM_ARCH_FLAG_HAS_RAN_ONCE, &(kvm)->arch.flags))
 
+/*
+ * #define KVM_ARM_VCPU_POWER_OFF          0 // CPU is started in OFF state
+ * #define KVM_ARM_VCPU_EL1_32BIT          1 // CPU running a 32bit VM
+ * #define KVM_ARM_VCPU_PSCI_0_2           2 // CPU uses PSCI v0.2
+ * #define KVM_ARM_VCPU_PMU_V3             3 // Support guest PMUv3
+ * #define KVM_ARM_VCPU_SVE                4 // enable SVE for this CPU
+ * #define KVM_ARM_VCPU_PTRAUTH_ADDRESS    5 // VCPU uses address authentication
+ * #define KVM_ARM_VCPU_PTRAUTH_GENERIC    6 // VCPU uses generic authentication
+ * #define KVM_ARM_VCPU_HAS_EL2            7 // Support nested virtualization
+ */
 static inline bool __vcpu_has_feature(const struct kvm_arch *ka, int feature)
 {
+	/*
+	 * 在以下使用kvm_arch->vcpu_features:
+	 *   - arch/arm64/include/asm/kvm_host.h|348| <<global>> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/include/asm/kvm_host.h|1427| <<__vcpu_has_feature>> return test_bit(feature, ka->vcpu_features);
+	 *   - arch/arm64/kvm/arm.c|212| <<kvm_arch_init_vm>> bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1480| <<kvm_vcpu_init_changed>> return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1516| <<__kvm_vcpu_set_target>> bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *
+	 * struct kvm_arch:
+	 * -> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES)
+	 */
 	return test_bit(feature, ka->vcpu_features);
 }
 
diff --git a/arch/arm64/kernel/acpi.c b/arch/arm64/kernel/acpi.c
index e6f66491f..25e992738 100644
--- a/arch/arm64/kernel/acpi.c
+++ b/arch/arm64/kernel/acpi.c
@@ -37,6 +37,12 @@
 #include <asm/smp_plat.h>
 
 int acpi_noirq = 1;		/* skip ACPI IRQ initialization */
+/*
+ * 在以下设置arm64的acpi_disabled:
+ *   - arch/arm64/kernel/acpi.c|40| <<global>> int acpi_disabled = 1;
+ *   - arch/arm64/include/asm/acpi.h|85| <<disable_acpi>> acpi_disabled = 1;
+ *   - arch/arm64/include/asm/acpi.h|92| <<enable_acpi>> acpi_disabled = 0;
+ */
 int acpi_disabled = 1;
 EXPORT_SYMBOL(acpi_disabled);
 
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index 9bef76383..3b11c0290 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -209,6 +209,23 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	kvm_arm_init_hypercalls(kvm);
 
+	/*
+	 * #define KVM_ARM_VCPU_POWER_OFF          0 // CPU is started in OFF state
+	 * #define KVM_ARM_VCPU_EL1_32BIT          1 // CPU running a 32bit VM
+	 * #define KVM_ARM_VCPU_PSCI_0_2           2 // CPU uses PSCI v0.2
+	 * #define KVM_ARM_VCPU_PMU_V3             3 // Support guest PMUv3
+	 * #define KVM_ARM_VCPU_SVE                4 // enable SVE for this CPU
+	 * #define KVM_ARM_VCPU_PTRAUTH_ADDRESS    5 // VCPU uses address authentication
+	 * #define KVM_ARM_VCPU_PTRAUTH_GENERIC    6 // VCPU uses generic authentication
+	 * #define KVM_ARM_VCPU_HAS_EL2            7 // Support nested virtualization
+	 *
+	 * 在以下使用kvm_arch->vcpu_features:
+	 *   - arch/arm64/include/asm/kvm_host.h|348| <<global>> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/include/asm/kvm_host.h|1427| <<__vcpu_has_feature>> return test_bit(feature, ka->vcpu_features);
+	 *   - arch/arm64/kvm/arm.c|212| <<kvm_arch_init_vm>> bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1480| <<kvm_vcpu_init_changed>> return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1516| <<__kvm_vcpu_set_target>> bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 */
 	bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
 
 	return 0;
@@ -1019,6 +1036,11 @@ static int check_vcpu_requests(struct kvm_vcpu *vcpu)
 			preempt_enable();
 		}
 
+		/*
+		 * 在以下使用KVM_REQ_RELOAD_PMU:
+		 *   - arch/arm64/kvm/arm.c|1022| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu)) kvm_vcpu_reload_pmu(vcpu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1413| <<kvm_arm_pmu_v3_enable>> kvm_make_request(KVM_REQ_RELOAD_PMU, vcpu);
+		 */
 		if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu))
 			kvm_vcpu_reload_pmu(vcpu);
 
@@ -1472,6 +1494,23 @@ static bool kvm_vcpu_init_changed(struct kvm_vcpu *vcpu,
 {
 	unsigned long features = init->features[0];
 
+	/*
+	 * #define KVM_ARM_VCPU_POWER_OFF          0 // CPU is started in OFF state
+	 * #define KVM_ARM_VCPU_EL1_32BIT          1 // CPU running a 32bit VM
+	 * #define KVM_ARM_VCPU_PSCI_0_2           2 // CPU uses PSCI v0.2
+	 * #define KVM_ARM_VCPU_PMU_V3             3 // Support guest PMUv3
+	 * #define KVM_ARM_VCPU_SVE                4 // enable SVE for this CPU
+	 * #define KVM_ARM_VCPU_PTRAUTH_ADDRESS    5 // VCPU uses address authentication
+	 * #define KVM_ARM_VCPU_PTRAUTH_GENERIC    6 // VCPU uses generic authentication
+	 * #define KVM_ARM_VCPU_HAS_EL2            7 // Support nested virtualization
+	 *
+	 * 在以下使用kvm_arch->vcpu_features:
+	 *   - arch/arm64/include/asm/kvm_host.h|348| <<global>> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/include/asm/kvm_host.h|1427| <<__vcpu_has_feature>> return test_bit(feature, ka->vcpu_features);
+	 *   - arch/arm64/kvm/arm.c|212| <<kvm_arch_init_vm>> bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1480| <<kvm_vcpu_init_changed>> return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1516| <<__kvm_vcpu_set_target>> bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 */
 	return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features,
 			     KVM_VCPU_MAX_FEATURES);
 }
@@ -1495,6 +1534,10 @@ static int kvm_setup_vcpu(struct kvm_vcpu *vcpu)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1547| <<kvm_vcpu_set_target>> return __kvm_vcpu_set_target(vcpu, init);
+ */
 static int __kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
 				 const struct kvm_vcpu_init *init)
 {
@@ -1508,6 +1551,23 @@ static int __kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
 	    kvm_vcpu_init_changed(vcpu, init))
 		goto out_unlock;
 
+	/*
+	 * #define KVM_ARM_VCPU_POWER_OFF          0 // CPU is started in OFF state
+	 * #define KVM_ARM_VCPU_EL1_32BIT          1 // CPU running a 32bit VM
+	 * #define KVM_ARM_VCPU_PSCI_0_2           2 // CPU uses PSCI v0.2
+	 * #define KVM_ARM_VCPU_PMU_V3             3 // Support guest PMUv3
+	 * #define KVM_ARM_VCPU_SVE                4 // enable SVE for this CPU
+	 * #define KVM_ARM_VCPU_PTRAUTH_ADDRESS    5 // VCPU uses address authentication
+	 * #define KVM_ARM_VCPU_PTRAUTH_GENERIC    6 // VCPU uses generic authentication
+	 * #define KVM_ARM_VCPU_HAS_EL2            7 // Support nested virtualization
+	 *
+	 * 在以下使用kvm_arch->vcpu_features:
+	 *   - arch/arm64/include/asm/kvm_host.h|348| <<global>> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/include/asm/kvm_host.h|1427| <<__vcpu_has_feature>> return test_bit(feature, ka->vcpu_features);
+	 *   - arch/arm64/kvm/arm.c|212| <<kvm_arch_init_vm>> bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1480| <<kvm_vcpu_init_changed>> return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1516| <<__kvm_vcpu_set_target>> bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 */
 	bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
 
 	ret = kvm_setup_vcpu(vcpu);
@@ -1525,6 +1585,10 @@ static int __kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * 处理KVM_ARM_VCPU_INIT:
+ *   - arch/arm64/kvm/arm.c|1572| <<kvm_arch_vcpu_ioctl_vcpu_init(KVM_ARM_VCPU_INIT)>> ret = kvm_vcpu_set_target(vcpu, init);
+ */
 static int kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
 			       const struct kvm_vcpu_init *init)
 {
@@ -1602,6 +1666,10 @@ static int kvm_arch_vcpu_ioctl_vcpu_init(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * 处理KVM_SET_DEVICE_ATTR:
+ *   - arch/arm64/kvm/arm.c|1753| <<kvm_arch_vcpu_ioctl(KVM_SET_DEVICE_ATTR)>> r = kvm_arm_vcpu_set_attr(vcpu, &attr);
+ */
 static int kvm_arm_vcpu_set_attr(struct kvm_vcpu *vcpu,
 				 struct kvm_device_attr *attr)
 {
@@ -1670,6 +1738,10 @@ static int kvm_arm_vcpu_set_events(struct kvm_vcpu *vcpu,
 	return __kvm_arm_vcpu_set_events(vcpu, events);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4654| <<kvm_vcpu_ioctl(default)>> r = kvm_arch_vcpu_ioctl(filp, ioctl, arg);
+ */
 long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg)
 {
diff --git a/arch/arm64/kvm/guest.c b/arch/arm64/kvm/guest.c
index 11098eb7e..f04eb67b9 100644
--- a/arch/arm64/kvm/guest.c
+++ b/arch/arm64/kvm/guest.c
@@ -944,6 +944,10 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1617| <<kvm_arm_vcpu_set_attr>> ret = kvm_arm_vcpu_arch_set_attr(vcpu, attr);
+ */
 int kvm_arm_vcpu_arch_set_attr(struct kvm_vcpu *vcpu,
 			       struct kvm_device_attr *attr)
 {
diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
index 82a2a0032..3bd71504f 100644
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@ -15,26 +15,116 @@
 #include <kvm/arm_pmu.h>
 #include <kvm/arm_vgic.h>
 
+/*
+ * 1134 #define read_sysreg(r) ({                                       \
+ * 1135         u64 __val;                                              \
+ * 1136         asm volatile("mrs %0, " __stringify(r) : "=r" (__val)); \
+ * 1137         __val;                                                  \
+ * 1138 })
+ * 1139
+ *
+ * 读用的是MRS:
+ * Move System Register to general-purpose register allows the PE to read an
+ * AArch64 System register into a general-purpose register.
+ *
+ *
+ * 1144 #define write_sysreg(v, r) do {                                 \
+ * 1145         u64 __val = (u64)(v);                                   \
+ * 1146         asm volatile("msr " __stringify(r) ", %x0"              \
+ * 1147                      : : "rZ" (__val));                         \
+ * 1148 } while (0)
+ *
+ * 写用的是MSR:
+ * Move general-purpose register to System Register allows the PE to write an
+ * AArch64 System register from a general-purpose register.
+ */
+
 #define PERF_ATTR_CFG1_COUNTER_64BIT	BIT(0)
 
+/*
+ * 在以下使用kvm_arm_pmu_available:
+ *   - arch/arm64/kernel/image-vars.h|114| <<global>> KVM_NVHE_ALIAS(kvm_arm_pmu_available);
+ *   - arch/arm64/kvm/pmu-emul.c|20| <<global>> DEFINE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
+ *   - include/kvm/arm_pmu.h|40| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
+ *   - arch/arm64/kvm/pmu-emul.c|714| <<kvm_host_pmu_init>> static_branch_enable(&kvm_arm_pmu_available);
+ *   - include/kvm/arm_pmu.h|44| <<kvm_arm_support_pmu_v3>> return static_branch_likely(&kvm_arm_pmu_available);
+ */
 DEFINE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
 
+/*
+ * 在以下使用LIST_HEAD(arm_pmus):
+ *   - arch/arm64/kvm/pmu-emul.c|22| <<global>> static LIST_HEAD(arm_pmus);
+ *   - arch/arm64/kvm/pmu-emul.c|711| <<kvm_host_pmu_init>> list_add_tail(&entry->entry, &arm_pmus);
+ *   - arch/arm64/kvm/pmu-emul.c|713| <<kvm_host_pmu_init>> if (list_is_singular(&arm_pmus))
+ *   - arch/arm64/kvm/pmu-emul.c|746| <<kvm_pmu_probe_armpmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+ *   - arch/arm64/kvm/pmu-emul.c|960| <<kvm_arm_pmu_v3_set_pmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+ */
 static LIST_HEAD(arm_pmus);
 static DEFINE_MUTEX(arm_pmus_lock);
 
 static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc);
 static void kvm_pmu_release_perf_event(struct kvm_pmc *pmc);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|84| <<kvm_pmc_is_64bit>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|92| <<kvm_pmc_has_64bit_overflow>> u64 val = kvm_vcpu_read_pmcr(kvm_pmc_to_vcpu(pmc));
+ *   - arch/arm64/kvm/pmu-emul.c|116| <<kvm_pmu_get_pmc_value>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|151| <<kvm_pmu_set_pmc_value>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|211| <<kvm_pmu_stop_counter>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|502| <<kvm_pmu_perf_overflow>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|587| <<kvm_pmu_counter_is_enabled>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|598| <<kvm_pmu_create_perf_event>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ */
 static struct kvm_vcpu *kvm_pmc_to_vcpu(const struct kvm_pmc *pmc)
 {
+	/*
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 *       -> struct irq_work overflow_work;
+	 *       -> struct kvm_pmu_events events;
+	 *       -> struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
+	 *       -> int irq_num;
+	 *       -> bool created;
+	 *       -> bool irq_level;
+	 */
 	return container_of(pmc, struct kvm_vcpu, arch.pmu.pmc[pmc->idx]);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|146| <<kvm_pmu_get_counter_value>> return kvm_pmu_get_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx));
+ *   - arch/arm64/kvm/pmu-emul.c|187| <<kvm_pmu_set_counter_value>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx), val, false);
+ *   - arch/arm64/kvm/pmu-emul.c|251| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
+ *   - arch/arm64/kvm/pmu-emul.c|264| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+ *   - arch/arm64/kvm/pmu-emul.c|300| <<kvm_pmu_enable_counter_mask>> pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+ *   - arch/arm64/kvm/pmu-emul.c|332| <<kvm_pmu_disable_counter_mask>> pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+ *   - arch/arm64/kvm/pmu-emul.c|452| <<kvm_pmu_counter_increment>> struct kvm_pmc *pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+ *   - arch/arm64/kvm/pmu-emul.c|580| <<kvm_pmu_handle_pmcr>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, i), 0, true);
+ *   - arch/arm64/kvm/pmu-emul.c|681| <<kvm_pmu_set_counter_event_type>> struct kvm_pmc *pmc = kvm_vcpu_idx_to_pmc(vcpu, select_idx);
+ */
 static struct kvm_pmc *kvm_vcpu_idx_to_pmc(struct kvm_vcpu *vcpu, int cnt_idx)
 {
+	/*
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 *       -> struct irq_work overflow_work;
+	 *       -> struct kvm_pmu_events events;
+	 *       -> struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
+	 *       -> int irq_num;
+	 *       -> bool created;
+	 *       -> bool irq_level;
+	 */
 	return &vcpu->arch.pmu.pmc[cnt_idx];
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|59| <<kvm_pmu_event_mask>> return __kvm_pmu_event_mask(pmuver);
+ *   - arch/arm64/kvm/pmu-emul.c|1028| <<kvm_arm_pmu_v3_set_attr>> nr_events = __kvm_pmu_event_mask(pmuver) + 1;
+ */
 static u32 __kvm_pmu_event_mask(unsigned int pmuver)
 {
 	switch (pmuver) {
@@ -51,6 +141,13 @@ static u32 __kvm_pmu_event_mask(unsigned int pmuver)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|65| <<kvm_pmu_evtyper_mask>> kvm_pmu_event_mask(kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|457| <<kvm_pmu_counter_increment>> type &= kvm_pmu_event_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|612| <<kvm_pmu_create_perf_event>> eventsel = data & kvm_pmu_event_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|789| <<kvm_pmu_get_pmceid>> nr_events = kvm_pmu_event_mask(vcpu->kvm) + 1;
+ */
 static u32 kvm_pmu_event_mask(struct kvm *kvm)
 {
 	u64 dfr0 = kvm_read_vm_id_reg(kvm, SYS_ID_AA64DFR0_EL1);
@@ -59,6 +156,11 @@ static u32 kvm_pmu_event_mask(struct kvm *kvm)
 	return __kvm_pmu_event_mask(pmuver);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|688| <<kvm_pmu_set_counter_event_type>> __vcpu_sys_reg(vcpu, reg) = data & kvm_pmu_evtyper_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/sys_regs.c|888| <<reset_pmevtyper>> __vcpu_sys_reg(vcpu, r->reg) &= kvm_pmu_evtyper_mask(vcpu->kvm);
+ */
 u64 kvm_pmu_evtyper_mask(struct kvm *kvm)
 {
 	u64 mask = ARMV8_PMU_EXCLUDE_EL1 | ARMV8_PMU_EXCLUDE_EL0 |
@@ -79,38 +181,111 @@ u64 kvm_pmu_evtyper_mask(struct kvm *kvm)
  * kvm_pmc_is_64bit - determine if counter is 64bit
  * @pmc: counter context
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|130| <<kvm_pmu_get_pmc_value>> if (!kvm_pmc_is_64bit(pmc))
+ *   - arch/arm64/kvm/pmu-emul.c|463| <<kvm_pmu_counter_increment>> if (!kvm_pmc_is_64bit(pmc))
+ *   - arch/arm64/kvm/pmu-emul.c|485| <<compute_period>> if (kvm_pmc_is_64bit(pmc) && kvm_pmc_has_64bit_overflow(pmc))
+ *   - arch/arm64/kvm/pmu-emul.c|651| <<kvm_pmu_create_perf_event>> if (kvm_pmc_is_64bit(pmc))
+ */
 static bool kvm_pmc_is_64bit(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
 
+	/*
+	 * 注释:
+	 * If FEAT_PMUv3p5 is implemented and the highest Exception level is
+	 * using AArch64, the event counters are 64-bit. If FEAT_PMUv3p5 is not
+	 * implemented, the event counters are 32-bit.
+	 */
 	return (pmc->idx == ARMV8_PMU_CYCLE_IDX ||
 		kvm_has_feat(vcpu->kvm, ID_AA64DFR0_EL1, PMUVer, V3P5));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|101| <<kvm_pmu_counter_can_chain>> return (!(pmc->idx & 1) && (pmc->idx + 1) < ARMV8_PMU_CYCLE_IDX && !kvm_pmc_has_64bit_overflow(pmc));
+ *   - arch/arm64/kvm/pmu-emul.c|468| <<kvm_pmu_counter_increment>> if (kvm_pmc_has_64bit_overflow(pmc) ? reg : lower_32_bits(reg))
+ *   - arch/arm64/kvm/pmu-emul.c|485| <<compute_period>> if (kvm_pmc_is_64bit(pmc) && kvm_pmc_has_64bit_overflow(pmc))
+ */
 static bool kvm_pmc_has_64bit_overflow(struct kvm_pmc *pmc)
 {
 	u64 val = kvm_vcpu_read_pmcr(kvm_pmc_to_vcpu(pmc));
 
+	/*
+	 * 207 //
+	 * 208 // Per-CPU PMCR: config reg
+	 * 209  //
+	 * 210 #define ARMV8_PMU_PMCR_E        (1 << 0) // Enable all counters
+	 * 211 #define ARMV8_PMU_PMCR_P        (1 << 1) // Reset all counters
+	 * 212 #define ARMV8_PMU_PMCR_C        (1 << 2) // Cycle counter reset
+	 * 213 #define ARMV8_PMU_PMCR_D        (1 << 3) // CCNT counts every 64th cpu cycle
+	 * 214 #define ARMV8_PMU_PMCR_X        (1 << 4) // Export to ETM
+	 * 215 #define ARMV8_PMU_PMCR_DP       (1 << 5) // Disable CCNT if non-invasive debug
+	 * 216 #define ARMV8_PMU_PMCR_LC       (1 << 6) // Overflow on 64 bit cycle counter
+	 * 217 #define ARMV8_PMU_PMCR_LP       (1 << 7) // Long event counter enable
+	 * 218 #define ARMV8_PMU_PMCR_N        GENMASK(15, 11) // Number of counters supported
+	 */
 	return (pmc->idx < ARMV8_PMU_CYCLE_IDX && (val & ARMV8_PMU_PMCR_LP)) ||
 	       (pmc->idx == ARMV8_PMU_CYCLE_IDX && (val & ARMV8_PMU_PMCR_LC));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|474| <<kvm_pmu_counter_increment>> if (kvm_pmu_counter_can_chain(pmc))
+ *   - arch/arm64/kvm/pmu-emul.c|520| <<kvm_pmu_perf_overflow>> if (kvm_pmu_counter_can_chain(pmc))
+ */
 static bool kvm_pmu_counter_can_chain(struct kvm_pmc *pmc)
 {
 	return (!(pmc->idx & 1) && (pmc->idx + 1) < ARMV8_PMU_CYCLE_IDX &&
 		!kvm_pmc_has_64bit_overflow(pmc));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|119| <<kvm_pmu_get_pmc_value>> reg = counter_index_to_reg(pmc->idx);
+ *   - arch/arm64/kvm/pmu-emul.c|156| <<kvm_pmu_set_pmc_value>> reg = counter_index_to_reg(pmc->idx);
+ *   - arch/arm64/kvm/pmu-emul.c|219| <<kvm_pmu_stop_counter>> reg = counter_index_to_reg(pmc->idx);
+ *   - arch/arm64/kvm/pmu-emul.c|462| <<kvm_pmu_counter_increment>> reg = __vcpu_sys_reg(vcpu, counter_index_to_reg(i)) + 1;
+ *   - arch/arm64/kvm/pmu-emul.c|465| <<kvm_pmu_counter_increment>> __vcpu_sys_reg(vcpu, counter_index_to_reg(i)) = reg;
+ */
 static u32 counter_index_to_reg(u64 idx)
 {
+	/*
+	 * 418         // Performance Monitors Registers
+	 * 419         PMCR_EL0,       // Control Register
+	 * 420         PMSELR_EL0,     // Event Counter Selection Register
+	 * 421         PMEVCNTR0_EL0,  // Event Counter Register (0-30)
+	 * 422         PMEVCNTR30_EL0 = PMEVCNTR0_EL0 + 30,
+	 * 423         PMCCNTR_EL0,    // Cycle Counter Register
+	 * 424         PMEVTYPER0_EL0, // Event Type Register (0-30)
+	 * 425         PMEVTYPER30_EL0 = PMEVTYPER0_EL0 + 30,
+	 * 426         PMCCFILTR_EL0,  // Cycle Count Filter Register
+	 * 427         PMCNTENSET_EL0, // Count Enable Set Register
+	 * 428         PMINTENSET_EL1, // Interrupt Enable Set Register
+	 * 429         PMOVSSET_EL0,   // Overflow Flag Status Set Register
+	 * 430         PMUSERENR_EL0,  // User Enable Register
+	 */
 	return (idx == ARMV8_PMU_CYCLE_IDX) ? PMCCNTR_EL0 : PMEVCNTR0_EL0 + idx;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|456| <<kvm_pmu_counter_increment>> type = __vcpu_sys_reg(vcpu, counter_index_to_evtreg(i));
+ *   - arch/arm64/kvm/pmu-emul.c|605| <<kvm_pmu_create_perf_event>> reg = counter_index_to_evtreg(pmc->idx);
+ *   - arch/arm64/kvm/pmu-emul.c|687| <<kvm_pmu_set_counter_event_type>> reg = counter_index_to_evtreg(pmc->idx);
+ */
 static u32 counter_index_to_evtreg(u64 idx)
 {
 	return (idx == ARMV8_PMU_CYCLE_IDX) ? PMCCFILTR_EL0 : PMEVTYPER0_EL0 + idx;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|146| <<kvm_pmu_get_counter_value>> return kvm_pmu_get_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx));
+ *   - arch/arm64/kvm/pmu-emul.c|217| <<kvm_pmu_stop_counter>> val = kvm_pmu_get_pmc_value(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|654| <<kvm_pmu_create_perf_event>> attr.sample_period = compute_period(pmc, kvm_pmu_get_pmc_value(pmc));
+ */
 static u64 kvm_pmu_get_pmc_value(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
@@ -123,10 +298,26 @@ static u64 kvm_pmu_get_pmc_value(struct kvm_pmc *pmc)
 	 * The real counter value is equal to the value of counter register plus
 	 * the value perf event counts.
 	 */
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|127| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled,
+	 *   - arch/riscv/kvm/vcpu_pmu.c|249| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|625| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event,
+	 *   - arch/x86/kvm/pmu.h|112| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event,
+	 *   - include/uapi/linux/bpf.h|5852| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 *   - tools/include/uapi/linux/bpf.h|5852| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 */
 	if (pmc->perf_event)
 		counter += perf_event_read_value(pmc->perf_event, &enabled,
 						 &running);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|130| <<kvm_pmu_get_pmc_value>> if (!kvm_pmc_is_64bit(pmc))
+	 *   - arch/arm64/kvm/pmu-emul.c|463| <<kvm_pmu_counter_increment>> if (!kvm_pmc_is_64bit(pmc))
+	 *   - arch/arm64/kvm/pmu-emul.c|485| <<compute_period>> if (kvm_pmc_is_64bit(pmc) && kvm_pmc_has_64bit_overflow(pmc))
+	 *   - arch/arm64/kvm/pmu-emul.c|651| <<kvm_pmu_create_perf_event>> if (kvm_pmc_is_64bit(pmc))
+	 */
 	if (!kvm_pmc_is_64bit(pmc))
 		counter = lower_32_bits(counter);
 
@@ -138,6 +329,11 @@ static u64 kvm_pmu_get_pmc_value(struct kvm_pmc *pmc)
  * @vcpu: The vcpu pointer
  * @select_idx: The counter index
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1040| <<get_pmu_evcntr>> *val = kvm_pmu_get_counter_value(vcpu, idx);
+ *   - arch/arm64/kvm/sys_regs.c|1091| <<access_pmu_evcntr>> p->regval = kvm_pmu_get_counter_value(vcpu, idx);
+ */
 u64 kvm_pmu_get_counter_value(struct kvm_vcpu *vcpu, u64 select_idx)
 {
 	if (!kvm_vcpu_has_pmu(vcpu))
@@ -146,11 +342,22 @@ u64 kvm_pmu_get_counter_value(struct kvm_vcpu *vcpu, u64 select_idx)
 	return kvm_pmu_get_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|187| <<kvm_pmu_set_counter_value>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx), val, false);
+ *   - arch/arm64/kvm/pmu-emul.c|580| <<kvm_pmu_handle_pmcr>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, i), 0, true);
+ */
 static void kvm_pmu_set_pmc_value(struct kvm_pmc *pmc, u64 val, bool force)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
 	u64 reg;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|154| <<kvm_pmu_set_pmc_value>> kvm_pmu_release_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|223| <<kvm_pmu_stop_counter>> kvm_pmu_release_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|264| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 */
 	kvm_pmu_release_perf_event(pmc);
 
 	reg = counter_index_to_reg(pmc->idx);
@@ -169,6 +376,13 @@ static void kvm_pmu_set_pmc_value(struct kvm_pmc *pmc, u64 val, bool force)
 
 	__vcpu_sys_reg(vcpu, reg) = val;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|25| <<global>> static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|173| <<kvm_pmu_set_pmc_value>> kvm_pmu_create_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|303| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|690| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(pmc);
+	 */
 	/* Recreate the perf event to reflect the updated sample_period */
 	kvm_pmu_create_perf_event(pmc);
 }
@@ -179,6 +393,11 @@ static void kvm_pmu_set_pmc_value(struct kvm_pmc *pmc, u64 val, bool force)
  * @select_idx: The counter index
  * @val: The counter value
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|574| <<kvm_pmu_handle_pmcr>> kvm_pmu_set_counter_value(vcpu, ARMV8_PMU_CYCLE_IDX, 0);
+ *   - arch/arm64/kvm/sys_regs.c|1089| <<access_pmu_evcntr>> kvm_pmu_set_counter_value(vcpu, idx, p->regval);
+ */
 void kvm_pmu_set_counter_value(struct kvm_vcpu *vcpu, u64 select_idx, u64 val)
 {
 	if (!kvm_vcpu_has_pmu(vcpu))
@@ -191,10 +410,31 @@ void kvm_pmu_set_counter_value(struct kvm_vcpu *vcpu, u64 select_idx, u64 val)
  * kvm_pmu_release_perf_event - remove the perf event
  * @pmc: The PMU counter pointer
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|154| <<kvm_pmu_set_pmc_value>> kvm_pmu_release_perf_event(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|223| <<kvm_pmu_stop_counter>> kvm_pmu_release_perf_event(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|264| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+ */
 static void kvm_pmu_release_perf_event(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
 		perf_event_disable(pmc->perf_event);
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/pmu-emul.c|386| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/riscv/kvm/vcpu_pmu.c|82| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1058| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1060| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+		 *   - arch/x86/kvm/pmu.c|281| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|198| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+		 *   - kernel/events/core.c|5487| <<perf_release>> perf_event_release_kernel(file->private_data);
+		 *   - kernel/events/hw_breakpoint.c|829| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+		 *   - kernel/watchdog_perf.c|208| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+		 *   - kernel/watchdog_perf.c|275| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+		 *
+		 * arm64只在这里调用release
+		 */
 		perf_event_release_kernel(pmc->perf_event);
 		pmc->perf_event = NULL;
 	}
@@ -206,6 +446,11 @@ static void kvm_pmu_release_perf_event(struct kvm_pmc *pmc)
  *
  * If this counter has been configured to monitor some event, release it here.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|439| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
+ *   - arch/arm64/kvm/pmu-emul.c|825| <<kvm_pmu_create_perf_event>> kvm_pmu_stop_counter(pmc);
+ */
 static void kvm_pmu_stop_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
@@ -220,6 +465,12 @@ static void kvm_pmu_stop_counter(struct kvm_pmc *pmc)
 
 	__vcpu_sys_reg(vcpu, reg) = val;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|154| <<kvm_pmu_set_pmc_value>> kvm_pmu_release_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|223| <<kvm_pmu_stop_counter>> kvm_pmu_release_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|264| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 */
 	kvm_pmu_release_perf_event(pmc);
 }
 
@@ -228,11 +479,26 @@ static void kvm_pmu_stop_counter(struct kvm_pmc *pmc)
  * @vcpu: The vcpu pointer
  *
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|475| <<kvm_arch_vcpu_create>> kvm_pmu_vcpu_init(vcpu);
+ */
 void kvm_pmu_vcpu_init(struct kvm_vcpu *vcpu)
 {
 	int i;
 	struct kvm_pmu *pmu = &vcpu->arch.pmu;
 
+	/*
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 *       -> struct irq_work overflow_work;
+	 *       -> struct kvm_pmu_events events;
+	 *       -> struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
+	 *       -> int irq_num;
+	 *       -> bool created;
+	 *       -> bool irq_level;
+	 */
 	for (i = 0; i < ARMV8_PMU_MAX_COUNTERS; i++)
 		pmu->pmc[i].idx = i;
 }
@@ -242,11 +508,20 @@ void kvm_pmu_vcpu_init(struct kvm_vcpu *vcpu)
  * @vcpu: The vcpu pointer
  *
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/reset.c|205| <<kvm_reset_vcpu>> kvm_pmu_vcpu_reset(vcpu);
+ */
 void kvm_pmu_vcpu_reset(struct kvm_vcpu *vcpu)
 {
 	unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
 	int i;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|439| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 *   - arch/arm64/kvm/pmu-emul.c|825| <<kvm_pmu_create_perf_event>> kvm_pmu_stop_counter(pmc);
+	 */
 	for_each_set_bit(i, &mask, 32)
 		kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
 }
@@ -256,15 +531,38 @@ void kvm_pmu_vcpu_reset(struct kvm_vcpu *vcpu)
  * @vcpu: The vcpu pointer
  *
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|508| <<kvm_arch_vcpu_destroy>> kvm_pmu_vcpu_destroy(vcpu);
+ */
 void kvm_pmu_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	int i;
 
 	for (i = 0; i < ARMV8_PMU_MAX_COUNTERS; i++)
 		kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+	/*
+	 * 在以下使用kvm_pmu->overflow_work:
+	 *   - arch/arm64/kvm/pmu-emul.c|453| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_perf_overflow_notify_vcpu>> vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|718| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|1092| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+	 */
 	irq_work_sync(&vcpu->arch.pmu.overflow_work);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|474| <<kvm_pmu_vcpu_reset>> unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|835| <<kvm_pmu_handle_pmcr>> unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|1076| <<kvm_vcpu_reload_pmu>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1146| <<set_pmreg>> val &= kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1169| <<get_pmreg>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1183| <<access_pmcnten>> mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1206| <<access_pminten>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1230| <<access_pmovs>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1260| <<access_pmswinc>> mask = kvm_pmu_valid_counter_mask(vcpu);
+ */
 u64 kvm_pmu_valid_counter_mask(struct kvm_vcpu *vcpu)
 {
 	u64 val = FIELD_GET(ARMV8_PMU_PMCR_N, kvm_vcpu_read_pmcr(vcpu));
@@ -282,6 +580,11 @@ u64 kvm_pmu_valid_counter_mask(struct kvm_vcpu *vcpu)
  *
  * Call perf_event_enable to start counting the perf event
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|821| <<kvm_pmu_handle_pmcr>> kvm_pmu_enable_counter_mask(vcpu,
+ *   - arch/arm64/kvm/sys_regs.c|1189| <<access_pmcnten>> kvm_pmu_enable_counter_mask(vcpu, val);
+ */
 void kvm_pmu_enable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 {
 	int i;
@@ -300,8 +603,25 @@ void kvm_pmu_enable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 		pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
 
 		if (!pmc->perf_event) {
+			/*
+			 * called by:
+			 *   - arch/arm64/kvm/pmu-emul.c|25| <<global>> static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc);
+			 *   - arch/arm64/kvm/pmu-emul.c|173| <<kvm_pmu_set_pmc_value>> kvm_pmu_create_perf_event(pmc);
+			 *   - arch/arm64/kvm/pmu-emul.c|303| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(pmc);
+			 *   - arch/arm64/kvm/pmu-emul.c|690| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(pmc);
+			 */
 			kvm_pmu_create_perf_event(pmc);
 		} else {
+			/*
+			 * called by:
+			 *   - arch/arm64/kvm/pmu-emul.c|541| <<kvm_pmu_enable_counter_mask>> perf_event_enable(pmc->perf_event);
+			 *   - arch/riscv/kvm/vcpu_pmu.c|336| <<kvm_pmu_create_perf_event>> perf_event_enable(pmc->perf_event);
+			 *   - arch/riscv/kvm/vcpu_pmu.c|553| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_enable(pmc->perf_event);
+			 *   - arch/x86/kvm/pmu.c|272| <<pmc_resume_counter>> perf_event_enable(pmc->perf_event);
+			 *   - kernel/events/hw_breakpoint.c|815| <<modify_user_hw_breakpoint>> perf_event_enable(bp);
+			 *   - kernel/watchdog_perf.c|169| <<watchdog_hardlockup_enable>> perf_event_enable(this_cpu_read(watchdog_ev));
+			 *   - kernel/watchdog_perf.c|251| <<hardlockup_detector_perf_restart>> perf_event_enable(event);
+			 */
 			perf_event_enable(pmc->perf_event);
 			if (pmc->perf_event->state != PERF_EVENT_STATE_ACTIVE)
 				kvm_debug("fail to enable perf event\n");
@@ -316,6 +636,11 @@ void kvm_pmu_enable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
  *
  * Call perf_event_disable to stop counting the perf event
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|824| <<kvm_pmu_handle_pmcr>> kvm_pmu_disable_counter_mask(vcpu, __vcpu_sys_reg(vcpu, PMCNTENSET_EL0));
+ *   - arch/arm64/kvm/sys_regs.c|1194| <<access_pmcnten>> kvm_pmu_disable_counter_mask(vcpu, val);
+ */
 void kvm_pmu_disable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 {
 	int i;
@@ -336,10 +661,61 @@ void kvm_pmu_disable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 	}
 }
 
+/*
+ * PMOVSSET_EL0寄存器:
+ * PMOVSSET_EL0, Performance Monitors Overflow Flag Status Set Register
+ *
+ * Sets the state of the overflow bit for the Cycle Count Register,
+ * AArch64-PMCCNTR_EL0, and each of the implemented event counters
+ * AArch64-PMEVCNTR<n>_EL0.
+ *
+ * 可写可读, 每个bit表示一个event的overflow status
+ *
+ * -----------
+ *      
+ * PMINTENSET_EL1寄存器:
+ * PMINTENSET_EL1, Performance Monitors Interrupt Enable Set Register
+ *
+ * Enables the generation of interrupt requests on overflows from the
+ * cycle counter, AArch64-PMCCNTR_EL0, and the event counters
+ * AArch64-PMEVCNTR<n>_EL0. Reading the register shows which overflow
+ * interrupt requests are enabled.
+ *
+ * 可读可写, 每个bit表示一个event overflow的时候中断吗
+ *
+ * -----------
+ *
+ * PMCNTENSET_EL0寄存器:
+ * PMCNTENSET_EL0, Performance Monitors Count Enable Set Register
+ *
+ * Enables the Cycle Count Register, AArch64-PMCCNTR_EL0, and any
+ * implemented event counters AArch64-PMEVCNTR<n>_EL0. Reading this
+ * register shows which counters are enabled.
+ *
+ * 可读可写, 激活对应的counter
+ *
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|596| <<kvm_pmu_update_state>> overflow = !!kvm_pmu_overflow_status(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|760| <<kvm_pmu_perf_overflow>> if (kvm_pmu_overflow_status(vcpu)) {
+ */
 static u64 kvm_pmu_overflow_status(struct kvm_vcpu *vcpu)
 {
 	u64 reg = 0;
 
+	/*
+	 * 207 //
+	 * 208 // Per-CPU PMCR: config reg
+	 * 209 //
+	 * 210 #define ARMV8_PMU_PMCR_E        (1 << 0) // Enable all counters
+	 * 211 #define ARMV8_PMU_PMCR_P        (1 << 1) // Reset all counters
+	 * 212 #define ARMV8_PMU_PMCR_C        (1 << 2) // Cycle counter reset
+	 * 213 #define ARMV8_PMU_PMCR_D        (1 << 3) // CCNT counts every 64th cpu cycle
+	 * 214 #define ARMV8_PMU_PMCR_X        (1 << 4) // Export to ETM
+	 * 215 #define ARMV8_PMU_PMCR_DP       (1 << 5) // Disable CCNT if non-invasive debug
+	 * 216 #define ARMV8_PMU_PMCR_LC       (1 << 6) // Overflow on 64 bit cycle counter
+	 * 217 #define ARMV8_PMU_PMCR_LP       (1 << 7) // Long event counter enable
+	 * 218 #define ARMV8_PMU_PMCR_N        GENMASK(15, 11) // Number of counters supported
+	 */
 	if ((kvm_vcpu_read_pmcr(vcpu) & ARMV8_PMU_PMCR_E)) {
 		reg = __vcpu_sys_reg(vcpu, PMOVSSET_EL0);
 		reg &= __vcpu_sys_reg(vcpu, PMCNTENSET_EL0);
@@ -349,6 +725,11 @@ static u64 kvm_pmu_overflow_status(struct kvm_vcpu *vcpu)
 	return reg;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|643| <<kvm_pmu_flush_hwstate>> kvm_pmu_update_state(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|655| <<kvm_pmu_sync_hwstate>> kvm_pmu_update_state(vcpu);
+ */
 static void kvm_pmu_update_state(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = &vcpu->arch.pmu;
@@ -364,12 +745,33 @@ static void kvm_pmu_update_state(struct kvm_vcpu *vcpu)
 	pmu->irq_level = overflow;
 
 	if (likely(irqchip_in_kernel(vcpu->kvm))) {
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 *
+		 * called by:
+		 *   - arch/arm64/kvm/arch_timer.c|455| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, timer_irq(timer_ctx), timer_ctx->irq.level, timer_ctx);
+		 *   - arch/arm64/kvm/arm.c|1393| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/arm.c|1401| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, NULL, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/pmu-emul.c|603| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, NULL, spi_id, level, NULL);
+		 */
 		int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu,
 					      pmu->irq_num, overflow, pmu);
 		WARN_ON(ret);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1076| <<kvm_vcpu_exit_request>> if (kvm_timer_should_notify_user(vcpu) || kvm_pmu_should_notify_user(vcpu)) {
+ */
 bool kvm_pmu_should_notify_user(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = &vcpu->arch.pmu;
@@ -414,6 +816,11 @@ void kvm_pmu_flush_hwstate(struct kvm_vcpu *vcpu)
  * Check if the PMU has overflowed while we were running in the guest, and
  * inject an interrupt if that was the case.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1195| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_sync_hwstate(vcpu);
+ *   - arch/arm64/kvm/arm.c|1228| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_sync_hwstate(vcpu);
+ */
 void kvm_pmu_sync_hwstate(struct kvm_vcpu *vcpu)
 {
 	kvm_pmu_update_state(vcpu);
@@ -424,19 +831,75 @@ void kvm_pmu_sync_hwstate(struct kvm_vcpu *vcpu)
  * to the event.
  * This is why we need a callback to do it once outside of the NMI context.
  */
+/*
+ * 在以下使用kvm_pmu_perf_overflow_notify_vcpu():
+ *   - arch/arm64/kvm/pmu-emul.c|1141| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+ */
 static void kvm_pmu_perf_overflow_notify_vcpu(struct irq_work *work)
 {
 	struct kvm_vcpu *vcpu;
 
+	/*
+	 * 在以下使用kvm_pmu->overflow_work:
+	 *   - arch/arm64/kvm/pmu-emul.c|453| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_perf_overflow_notify_vcpu>> vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|718| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|1092| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+	 */
 	vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
 	kvm_vcpu_kick(vcpu);
 }
 
+/*
+ * 一些event的例子
+ *
+ * 12 //
+ * 13 // Common architectural and microarchitectural event numbers.
+ * 14 //
+ * 15 #define ARMV8_PMUV3_PERFCTR_SW_INCR                             0x0000
+ * 16 #define ARMV8_PMUV3_PERFCTR_L1I_CACHE_REFILL                    0x0001
+ * 17 #define ARMV8_PMUV3_PERFCTR_L1I_TLB_REFILL                      0x0002
+ * 18 #define ARMV8_PMUV3_PERFCTR_L1D_CACHE_REFILL                    0x0003
+ * 19 #define ARMV8_PMUV3_PERFCTR_L1D_CACHE                           0x0004
+ * 20 #define ARMV8_PMUV3_PERFCTR_L1D_TLB_REFILL                      0x0005
+ * 21 #define ARMV8_PMUV3_PERFCTR_LD_RETIRED                          0x0006
+ * 22 #define ARMV8_PMUV3_PERFCTR_ST_RETIRED                          0x0007
+ * 23 #define ARMV8_PMUV3_PERFCTR_INST_RETIRED                        0x0008
+ * 24 #define ARMV8_PMUV3_PERFCTR_EXC_TAKEN                           0x0009
+ * 25 #define ARMV8_PMUV3_PERFCTR_EXC_RETURN                          0x000A
+ * 26 #define ARMV8_PMUV3_PERFCTR_CID_WRITE_RETIRED                   0x000B
+ * 27 #define ARMV8_PMUV3_PERFCTR_PC_WRITE_RETIRED                    0x000C
+ * 28 #define ARMV8_PMUV3_PERFCTR_BR_IMMED_RETIRED                    0x000D
+ * 29 #define ARMV8_PMUV3_PERFCTR_BR_RETURN_RETIRED                   0x000E
+ * 30 #define ARMV8_PMUV3_PERFCTR_UNALIGNED_LDST_RETIRED              0x000F
+ * 31 #define ARMV8_PMUV3_PERFCTR_BR_MIS_PRED                         0x0010
+ * 32 #define ARMV8_PMUV3_PERFCTR_CPU_CYCLES                          0x0011
+ * 33 #define ARMV8_PMUV3_PERFCTR_BR_PRED                             0x0012
+ * 34 #define ARMV8_PMUV3_PERFCTR_MEM_ACCESS                          0x0013
+ * 35 #define ARMV8_PMUV3_PERFCTR_L1I_CACHE                           0x0014
+ * 36 #define ARMV8_PMUV3_PERFCTR_L1D_CACHE_WB                        0x0015
+ * 37 #define ARMV8_PMUV3_PERFCTR_L2D_CACHE                           0x0016
+ * 38 #define ARMV8_PMUV3_PERFCTR_L2D_CACHE_REFILL                    0x0017
+ * 39 #define ARMV8_PMUV3_PERFCTR_L2D_CACHE_WB                        0x0018
+ * 40 #define ARMV8_PMUV3_PERFCTR_BUS_ACCESS                          0x0019
+ * 41 #define ARMV8_PMUV3_PERFCTR_MEMORY_ERROR                        0x001A
+ * 42 #define ARMV8_PMUV3_PERFCTR_INST_SPEC                           0x001B
+ * 43 #define ARMV8_PMUV3_PERFCTR_TTBR_WRITE_RETIRED                  0x001C
+ * 44 #define ARMV8_PMUV3_PERFCTR_BUS_CYCLES                          0x001D
+ * 45 #define ARMV8_PMUV3_PERFCTR_CHAIN                               0x001E
+ */
+
 /*
  * Perform an increment on any of the counters described in @mask,
  * generating the overflow if required, and propagate it as a chained
  * event if possible.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|711| <<kvm_pmu_counter_increment>> kvm_pmu_counter_increment(vcpu, BIT(i + 1), ARMV8_PMUV3_PERFCTR_CHAIN);
+ *   - arch/arm64/kvm/pmu-emul.c|757| <<kvm_pmu_perf_overflow>> kvm_pmu_counter_increment(vcpu, BIT(idx + 1), ARMV8_PMUV3_PERFCTR_CHAIN);
+ *   - arch/arm64/kvm/pmu-emul.c|779| <<kvm_pmu_software_increment>> kvm_pmu_counter_increment(vcpu, val, ARMV8_PMUV3_PERFCTR_SW_INCR);
+ */
 static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
 				      unsigned long mask, u32 event)
 {
@@ -445,6 +908,17 @@ static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
 	if (!(kvm_vcpu_read_pmcr(vcpu) & ARMV8_PMU_PMCR_E))
 		return;
 
+	/*
+	 *
+	 * PMCNTENSET_EL0寄存器:
+	 * PMCNTENSET_EL0, Performance Monitors Count Enable Set Register
+	 *
+	 * Enables the Cycle Count Register, AArch64-PMCCNTR_EL0, and any
+	 * implemented event counters AArch64-PMEVCNTR<n>_EL0. Reading this
+	 * register shows which counters are enabled.
+	 *
+	 * 可读可写, 激活对应的counter
+	 */
 	/* Weed out disabled counters */
 	mask &= __vcpu_sys_reg(vcpu, PMCNTENSET_EL0);
 
@@ -455,6 +929,9 @@ static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
 		/* Filter on event type */
 		type = __vcpu_sys_reg(vcpu, counter_index_to_evtreg(i));
 		type &= kvm_pmu_event_mask(vcpu->kvm);
+		/*
+		 * 只针对event
+		 */
 		if (type != event)
 			continue;
 
@@ -468,6 +945,16 @@ static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
 		if (kvm_pmc_has_64bit_overflow(pmc) ? reg : lower_32_bits(reg))
 			continue;
 
+		/*
+		 * PMOVSSET_EL0寄存器:
+		 * PMOVSSET_EL0, Performance Monitors Overflow Flag Status Set Register
+		 *
+		 * Sets the state of the overflow bit for the Cycle Count Register,
+		 * AArch64-PMCCNTR_EL0, and each of the implemented event counters
+		 * AArch64-PMEVCNTR<n>_EL0.
+		 *
+		 * 可写可读, 每个bit表示一个event的overflow status
+		 */
 		/* Mark overflow */
 		__vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= BIT(i);
 
@@ -477,6 +964,11 @@ static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|852| <<kvm_pmu_perf_overflow>> period = compute_period(pmc, local64_read(&perf_event->count));
+ *   - arch/arm64/kvm/pmu-emul.c|1030| <<kvm_pmu_create_perf_event>> attr.sample_period = compute_period(pmc, kvm_pmu_get_pmc_value(pmc));
+ */
 /* Compute the sample period for a given counter value */
 static u64 compute_period(struct kvm_pmc *pmc, u64 counter)
 {
@@ -493,6 +985,10 @@ static u64 compute_period(struct kvm_pmc *pmc, u64 counter)
 /*
  * When the perf event overflows, set the overflow status and inform the vcpu.
  */
+/*
+ * 在以下使用kvm_pmu_perf_overflow():
+ *   - arch/arm64/kvm/pmu-emul.c|1033| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+ */
 static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 				  struct perf_sample_data *data,
 				  struct pt_regs *regs)
@@ -503,6 +999,11 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 	int idx = pmc->idx;
 	u64 period;
 
+	/*
+	 * #define PERF_EF_START   0x01            // start the counter when adding
+	 * #define PERF_EF_RELOAD  0x02            // reload the counter when starting
+	 * #define PERF_EF_UPDATE  0x04            // update the counter when stopping
+	 */
 	cpu_pmu->pmu.stop(perf_event, PERF_EF_UPDATE);
 
 	/*
@@ -515,6 +1016,19 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 	perf_event->attr.sample_period = period;
 	perf_event->hw.sample_period = period;
 
+	/*
+	 * PMOVSSET_EL0寄存器:
+	 * PMOVSSET_EL0, Performance Monitors Overflow Flag Status Set Register
+	 *
+	 * Sets the state of the overflow bit for the Cycle Count Register,
+	 * AArch64-PMCCNTR_EL0, and each of the implemented event counters
+	 * AArch64-PMEVCNTR<n>_EL0.
+	 *      
+	 * 可写可读, 每个bit表示一个event的overflow status
+	 *
+	 *
+	 * PMOVSSET_EL0,   // Overflow Flag Status Set Register
+	 */
 	__vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= BIT(idx);
 
 	if (kvm_pmu_counter_can_chain(pmc))
@@ -522,14 +1036,37 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 					  ARMV8_PMUV3_PERFCTR_CHAIN);
 
 	if (kvm_pmu_overflow_status(vcpu)) {
+		/*
+		 * 在以下使用KVM_REQ_IRQ_PENDING:
+		 *   - arch/arm64/kvm/arm.c|1009| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/arm.c|1348| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/pmu-emul.c|865| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|351| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|401| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|717| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|998| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 */
 		kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
 
+		/*
+		 * 在以下使用kvm_pmu->overflow_work:
+		 *   - arch/arm64/kvm/pmu-emul.c|453| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+		 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_perf_overflow_notify_vcpu>> vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
+		 *   - arch/arm64/kvm/pmu-emul.c|718| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+		 *   - arch/arm64/kvm/pmu-emul.c|1092| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+		 */
 		if (!in_nmi())
 			kvm_vcpu_kick(vcpu);
 		else
 			irq_work_queue(&vcpu->arch.pmu.overflow_work);
 	}
 
+	/*
+	 * #define PERF_EF_START   0x01            // start the counter when adding
+	 * #define PERF_EF_RELOAD  0x02            // reload the counter when starting
+	 * #define PERF_EF_UPDATE  0x04            // update the counter when stopping 
+	 */
 	cpu_pmu->pmu.start(perf_event, PERF_EF_RELOAD);
 }
 
@@ -538,8 +1075,18 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
  * @vcpu: The vcpu pointer
  * @val: the value guest writes to PMSWINC register
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1261| <<access_pmswinc>> kvm_pmu_software_increment(vcpu, p->regval & mask);
+ */
 void kvm_pmu_software_increment(struct kvm_vcpu *vcpu, u64 val)
 {
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|711| <<kvm_pmu_counter_increment>> kvm_pmu_counter_increment(vcpu, BIT(i + 1), ARMV8_PMUV3_PERFCTR_CHAIN);
+	 *   - arch/arm64/kvm/pmu-emul.c|757| <<kvm_pmu_perf_overflow>> kvm_pmu_counter_increment(vcpu, BIT(idx + 1), ARMV8_PMUV3_PERFCTR_CHAIN);
+	 *   - arch/arm64/kvm/pmu-emul.c|779| <<kvm_pmu_software_increment>> kvm_pmu_counter_increment(vcpu, val, ARMV8_PMUV3_PERFCTR_SW_INCR);
+	 */
 	kvm_pmu_counter_increment(vcpu, val, ARMV8_PMUV3_PERFCTR_SW_INCR);
 }
 
@@ -548,6 +1095,20 @@ void kvm_pmu_software_increment(struct kvm_vcpu *vcpu, u64 val)
  * @vcpu: The vcpu pointer
  * @val: the value guest writes to PMCR register
  */
+/*
+ * PMCR寄存器:
+ * PMCR_EL0, Performance Monitors Control Register
+ *
+ * Provides details of the Performance Monitors implementation, including the
+ * number of counters implemented, and configures and controls the counters.
+ *
+ * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+ * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+ *
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|809| <<kvm_vcpu_reload_pmu>> kvm_pmu_handle_pmcr(vcpu, kvm_vcpu_read_pmcr(vcpu));
+ *   - arch/arm64/kvm/sys_regs.c|966| <<access_pmcr>> kvm_pmu_handle_pmcr(vcpu, val);
+ */
 void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 {
 	int i;
@@ -562,7 +1123,31 @@ void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 	/* The reset bits don't indicate any state, and shouldn't be saved. */
 	__vcpu_sys_reg(vcpu, PMCR_EL0) = val & ~(ARMV8_PMU_PMCR_C | ARMV8_PMU_PMCR_P);
 
+	/*
+	 * 207 //
+	 * 208 // Per-CPU PMCR: config reg
+	 * 209  //
+	 * 210 #define ARMV8_PMU_PMCR_E        (1 << 0) // Enable all counters
+	 * 211 #define ARMV8_PMU_PMCR_P        (1 << 1) // Reset all counters
+	 * 212 #define ARMV8_PMU_PMCR_C        (1 << 2) // Cycle counter reset
+	 * 213 #define ARMV8_PMU_PMCR_D        (1 << 3) // CCNT counts every 64th cpu cycle
+	 * 214 #define ARMV8_PMU_PMCR_X        (1 << 4) // Export to ETM
+	 * 215 #define ARMV8_PMU_PMCR_DP       (1 << 5) // Disable CCNT if non-invasive debug
+	 * 216 #define ARMV8_PMU_PMCR_LC       (1 << 6) // Overflow on 64 bit cycle counter
+	 * 217 #define ARMV8_PMU_PMCR_LP       (1 << 7) // Long event counter enable
+	 * 218 #define ARMV8_PMU_PMCR_N        GENMASK(15, 11) // Number of counters supported
+	 */
 	if (val & ARMV8_PMU_PMCR_E) {
+		/*
+		 * PMCNTENSET_EL0寄存器:
+		 * PMCNTENSET_EL0, Performance Monitors Count Enable Set Register
+		 *
+		 * Enables the Cycle Count Register, AArch64-PMCCNTR_EL0, and any
+		 * implemented event counters AArch64-PMEVCNTR<n>_EL0. Reading this
+		 * register shows which counters are enabled.
+		 *
+		 * 可读可写, 激活对应的counter
+		 */
 		kvm_pmu_enable_counter_mask(vcpu,
 		       __vcpu_sys_reg(vcpu, PMCNTENSET_EL0));
 	} else {
@@ -570,6 +1155,9 @@ void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 		       __vcpu_sys_reg(vcpu, PMCNTENSET_EL0));
 	}
 
+	/*
+	 * #define ARMV8_PMU_CYCLE_IDX             (ARMV8_PMU_MAX_COUNTERS - 1)
+	 */
 	if (val & ARMV8_PMU_PMCR_C)
 		kvm_pmu_set_counter_value(vcpu, ARMV8_PMU_CYCLE_IDX, 0);
 
@@ -582,9 +1170,20 @@ void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 	kvm_vcpu_pmu_restore_guest(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1090| <<kvm_pmu_create_perf_event>> attr.disabled = !kvm_pmu_counter_is_enabled(pmc);
+ */
 static bool kvm_pmu_counter_is_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+	/*
+	 * PMCNTENSET_EL0, // Count Enable Set Register
+	 *
+	 * 0-30位表示对应的counter是否激活
+	 *
+	 * bit=31是给cycles用的(PMCCNTR_EL0)
+	 */
 	return (kvm_vcpu_read_pmcr(vcpu) & ARMV8_PMU_PMCR_E) &&
 	       (__vcpu_sys_reg(vcpu, PMCNTENSET_EL0) & BIT(pmc->idx));
 }
@@ -593,6 +1192,12 @@ static bool kvm_pmu_counter_is_enabled(struct kvm_pmc *pmc)
  * kvm_pmu_create_perf_event - create a perf event for a counter
  * @pmc: Counter context
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|173| <<kvm_pmu_set_pmc_value>> kvm_pmu_create_perf_event(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|303| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|690| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(pmc);
+ */
 static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
@@ -605,6 +1210,11 @@ static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
 	reg = counter_index_to_evtreg(pmc->idx);
 	data = __vcpu_sys_reg(vcpu, reg);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|439| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 *   - arch/arm64/kvm/pmu-emul.c|825| <<kvm_pmu_create_perf_event>> kvm_pmu_stop_counter(pmc);
+	 */
 	kvm_pmu_stop_counter(pmc);
 	if (pmc->idx == ARMV8_PMU_CYCLE_IDX)
 		eventsel = ARMV8_PMUV3_PERFCTR_CPU_CYCLES;
@@ -619,6 +1229,21 @@ static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
 	    eventsel == ARMV8_PMUV3_PERFCTR_CHAIN)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->pmu_filter:
+	 *   - arch/arm64/kvm/arm.c|256| <<kvm_arch_destroy_vm>> bitmap_free(kvm->arch.pmu_filter);
+	 *   - arch/arm64/kvm/pmu-emul.c|1094| <<kvm_pmu_create_perf_event>> if (vcpu->kvm->arch.pmu_filter && !test_bit(eventsel, vcpu->kvm->arch.pmu_filter))
+	 *   - arch/arm64/kvm/pmu-emul.c|1095| <<kvm_pmu_create_perf_event>> !test_bit(eventsel, vcpu->kvm->arch.pmu_filter))
+	 *   - arch/arm64/kvm/pmu-emul.c|1234| <<kvm_pmu_get_pmceid>> unsigned long *bmap = vcpu->kvm->arch.pmu_filter;
+	 *   - arch/arm64/kvm/pmu-emul.c|1443| <<kvm_arm_pmu_v3_set_pmu>> (kvm->arch.pmu_filter && kvm->arch.arm_pmu != arm_pmu)) {
+	 *   - arch/arm64/kvm/pmu-emul.c|1522| <<kvm_arm_pmu_v3_set_attr>> if (!kvm->arch.pmu_filter) {
+	 *   - arch/arm64/kvm/pmu-emul.c|1523| <<kvm_arm_pmu_v3_set_attr>> kvm->arch.pmu_filter = bitmap_alloc(nr_events, GFP_KERNEL_ACCOUNT);
+	 *   - arch/arm64/kvm/pmu-emul.c|1524| <<kvm_arm_pmu_v3_set_attr>> if (!kvm->arch.pmu_filter)
+	 *   - arch/arm64/kvm/pmu-emul.c|1534| <<kvm_arm_pmu_v3_set_attr>> bitmap_zero(kvm->arch.pmu_filter, nr_events);
+	 *   - arch/arm64/kvm/pmu-emul.c|1536| <<kvm_arm_pmu_v3_set_attr>> bitmap_fill(kvm->arch.pmu_filter, nr_events);
+	 *   - arch/arm64/kvm/pmu-emul.c|1540| <<kvm_arm_pmu_v3_set_attr>> bitmap_set(kvm->arch.pmu_filter, filter.base_event, filter.nevents);
+	 *   - arch/arm64/kvm/pmu-emul.c|1542| <<kvm_arm_pmu_v3_set_attr>> bitmap_clear(kvm->arch.pmu_filter, filter.base_event, filter.nevents);
+	 */
 	/*
 	 * If we have a filter in place and that the event isn't allowed, do
 	 * not install a perf event either.
@@ -653,6 +1278,22 @@ static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
 
 	attr.sample_period = compute_period(pmc, kvm_pmu_get_pmc_value(pmc));
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|1124| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|328| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(attr, -1, current, kvm_riscv_pmu_overflow, pmc);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|969| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|974| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kvm/pmu.c|215| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|243| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+	 *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+	 *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+	 *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>> return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+	 *   - kernel/watchdog_perf.c|135| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 *   - kernel/watchdog_perf.c|140| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 *
+	 * arm64唯一调用的地方
+	 */
 	event = perf_event_create_kernel_counter(&attr, -1, current,
 						 kvm_pmu_perf_overflow, pmc);
 
@@ -675,21 +1316,63 @@ static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
  * event with given hardware event number. Here we call perf_event API to
  * emulate this action and create a kernel perf event for it.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1124| <<access_pmu_evtyper>> kvm_pmu_set_counter_event_type(vcpu, p->regval, idx);
+ */
 void kvm_pmu_set_counter_event_type(struct kvm_vcpu *vcpu, u64 data,
 				    u64 select_idx)
 {
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|146| <<kvm_pmu_get_counter_value>> return kvm_pmu_get_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx));
+	 *   - arch/arm64/kvm/pmu-emul.c|187| <<kvm_pmu_set_counter_value>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx), val, false);
+	 *   - arch/arm64/kvm/pmu-emul.c|251| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 *   - arch/arm64/kvm/pmu-emul.c|264| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 *   - arch/arm64/kvm/pmu-emul.c|300| <<kvm_pmu_enable_counter_mask>> pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+	 *   - arch/arm64/kvm/pmu-emul.c|332| <<kvm_pmu_disable_counter_mask>> pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+	 *   - arch/arm64/kvm/pmu-emul.c|452| <<kvm_pmu_counter_increment>> struct kvm_pmc *pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+	 *   - arch/arm64/kvm/pmu-emul.c|580| <<kvm_pmu_handle_pmcr>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, i), 0, true);
+	 *   - arch/arm64/kvm/pmu-emul.c|681| <<kvm_pmu_set_counter_event_type>> struct kvm_pmc *pmc = kvm_vcpu_idx_to_pmc(vcpu, select_idx);
+	 *
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 *       -> struct irq_work overflow_work;
+	 *       -> struct kvm_pmu_events events;
+	 *       -> struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
+	 *       -> int irq_num;
+	 *       -> bool created;
+	 *       -> bool irq_level;
+	 */
 	struct kvm_pmc *pmc = kvm_vcpu_idx_to_pmc(vcpu, select_idx);
 	u64 reg;
 
 	if (!kvm_vcpu_has_pmu(vcpu))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|456| <<kvm_pmu_counter_increment>> type = __vcpu_sys_reg(vcpu, counter_index_to_evtreg(i));
+	 *   - arch/arm64/kvm/pmu-emul.c|605| <<kvm_pmu_create_perf_event>> reg = counter_index_to_evtreg(pmc->idx);
+	 *   - arch/arm64/kvm/pmu-emul.c|687| <<kvm_pmu_set_counter_event_type>> reg = counter_index_to_evtreg(pmc->idx);
+	 */
 	reg = counter_index_to_evtreg(pmc->idx);
 	__vcpu_sys_reg(vcpu, reg) = data & kvm_pmu_evtyper_mask(vcpu->kvm);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|173| <<kvm_pmu_set_pmc_value>> kvm_pmu_create_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|303| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|690| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(pmc);
+	 */
 	kvm_pmu_create_perf_event(pmc);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmu.c|931| <<armpmu_register>> kvm_host_pmu_init(pmu);
+ */
 void kvm_host_pmu_init(struct arm_pmu *pmu)
 {
 	struct arm_pmu_entry *entry;
@@ -708,6 +1391,14 @@ void kvm_host_pmu_init(struct arm_pmu *pmu)
 		goto out_unlock;
 
 	entry->arm_pmu = pmu;
+	/*
+	 * 在以下使用LIST_HEAD(arm_pmus):
+	 *   - arch/arm64/kvm/pmu-emul.c|22| <<global>> static LIST_HEAD(arm_pmus);
+	 *   - arch/arm64/kvm/pmu-emul.c|711| <<kvm_host_pmu_init>> list_add_tail(&entry->entry, &arm_pmus);
+	 *   - arch/arm64/kvm/pmu-emul.c|713| <<kvm_host_pmu_init>> if (list_is_singular(&arm_pmus))
+	 *   - arch/arm64/kvm/pmu-emul.c|746| <<kvm_pmu_probe_armpmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+	 *   - arch/arm64/kvm/pmu-emul.c|960| <<kvm_arm_pmu_v3_set_pmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+	 */
 	list_add_tail(&entry->entry, &arm_pmus);
 
 	if (list_is_singular(&arm_pmus))
@@ -717,6 +1408,10 @@ void kvm_host_pmu_init(struct arm_pmu *pmu)
 	mutex_unlock(&arm_pmus_lock);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1450| <<kvm_arm_set_default_pmu>> struct arm_pmu *arm_pmu = kvm_pmu_probe_armpmu();
+ */
 static struct arm_pmu *kvm_pmu_probe_armpmu(void)
 {
 	struct arm_pmu *tmp, *pmu = NULL;
@@ -743,6 +1438,14 @@ static struct arm_pmu *kvm_pmu_probe_armpmu(void)
 	 * carried here.
 	 */
 	cpu = raw_smp_processor_id();
+	/*
+	 * 在以下使用LIST_HEAD(arm_pmus):
+	 *   - arch/arm64/kvm/pmu-emul.c|22| <<global>> static LIST_HEAD(arm_pmus);
+	 *   - arch/arm64/kvm/pmu-emul.c|711| <<kvm_host_pmu_init>> list_add_tail(&entry->entry, &arm_pmus);
+	 *   - arch/arm64/kvm/pmu-emul.c|713| <<kvm_host_pmu_init>> if (list_is_singular(&arm_pmus))
+	 *   - arch/arm64/kvm/pmu-emul.c|746| <<kvm_pmu_probe_armpmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+	 *   - arch/arm64/kvm/pmu-emul.c|960| <<kvm_arm_pmu_v3_set_pmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+	 */
 	list_for_each_entry(entry, &arm_pmus, entry) {
 		tmp = entry->arm_pmu;
 
@@ -757,6 +1460,24 @@ static struct arm_pmu *kvm_pmu_probe_armpmu(void)
 	return pmu;
 }
 
+/*
+ * PMCEID1_EL0, Performance Monitors Common Event Identification register 1
+ *
+ * Defines which Common architectural events and Common microarchitectural
+ * events are implemented, or counted, using PMU events in the ranges 0x0020 to
+ * 0x003F and 0x4020 to 0x403F.
+ *
+ * 比如:
+ * ID0 corresponds to common event (0x20) L2D_CACHE_ALLOCATE
+ * ID1 corresponds to common event (0x21) BR_RETIRED
+ * ... ...
+ * ID12 corresponds to common event (0x2c) Reserved
+ * ... ...
+ * ID31 corresponds to common event (0x3f) STALL_SLOT
+ *
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1005| <<access_pmceid>> pmceid = kvm_pmu_get_pmceid(vcpu, (p->Op2 & 1));
+ */
 u64 kvm_pmu_get_pmceid(struct kvm_vcpu *vcpu, bool pmceid1)
 {
 	unsigned long *bmap = vcpu->kvm->arch.pmu_filter;
@@ -802,22 +1523,72 @@ u64 kvm_pmu_get_pmceid(struct kvm_vcpu *vcpu, bool pmceid1)
 	return val & mask;
 }
 
+/*
+ * 在以下使用KVM_REQ_RELOAD_PMU:
+ *   - arch/arm64/kvm/arm.c|1022| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu)) kvm_vcpu_reload_pmu(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|1413| <<kvm_arm_pmu_v3_enable>> kvm_make_request(KVM_REQ_RELOAD_PMU, vcpu);
+ *
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1023| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu)) kvm_vcpu_reload_pmu(vcpu);
+ */
 void kvm_vcpu_reload_pmu(struct kvm_vcpu *vcpu)
 {
 	u64 mask = kvm_pmu_valid_counter_mask(vcpu);
 
 	kvm_pmu_handle_pmcr(vcpu, kvm_vcpu_read_pmcr(vcpu));
 
+	/*
+	 * PMOVSSET_EL0寄存器:
+	 * PMOVSSET_EL0, Performance Monitors Overflow Flag Status Set Register
+	 *
+	 * Sets the state of the overflow bit for the Cycle Count Register,
+	 * AArch64-PMCCNTR_EL0, and each of the implemented event counters
+	 * AArch64-PMEVCNTR<n>_EL0.
+	 *
+	 * 可写可读, 每个bit表示一个event的overflow status
+	 *
+	 * -----------
+	 *
+	 * PMINTENSET_EL1寄存器:
+	 * PMINTENSET_EL1, Performance Monitors Interrupt Enable Set Register
+	 *
+	 * Enables the generation of interrupt requests on overflows from the
+	 * cycle counter, AArch64-PMCCNTR_EL0, and the event counters
+	 * AArch64-PMEVCNTR<n>_EL0. Reading the register shows which overflow
+	 * interrupt requests are enabled.
+	 *
+	 * 可读可写, 每个bit表示一个event overflow的时候中断吗
+	 *
+	 * -----------
+	 *
+	 * PMCNTENSET_EL0寄存器:
+	 * PMCNTENSET_EL0, Performance Monitors Count Enable Set Register
+	 *
+	 * Enables the Cycle Count Register, AArch64-PMCCNTR_EL0, and any
+	 * implemented event counters AArch64-PMEVCNTR<n>_EL0. Reading this
+	 * register shows which counters are enabled.
+	 *
+	 * 可读可写, 激活对应的counter
+	 */
 	__vcpu_sys_reg(vcpu, PMOVSSET_EL0) &= mask;
 	__vcpu_sys_reg(vcpu, PMINTENSET_EL1) &= mask;
 	__vcpu_sys_reg(vcpu, PMCNTENSET_EL0) &= mask;
 }
 
+/*
+ * arch/arm64/kvm/arm.c|840| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_arm_pmu_v3_enable(vcpu);
+ */
 int kvm_arm_pmu_v3_enable(struct kvm_vcpu *vcpu)
 {
 	if (!kvm_vcpu_has_pmu(vcpu))
 		return 0;
 
+	/*
+	 * 在以下使用kvm_pmu->created:
+	 *   - arch/arm64/kvm/pmu-emul.c|1503| <<kvm_arm_pmu_v3_enable>> if (!vcpu->arch.pmu.created) return -EINVAL;
+	 *   - arch/arm64/kvm/pmu-emul.c|1568| <<kvm_arm_pmu_v3_init>> vcpu->arch.pmu.created = true;
+	 *   - arch/arm64/kvm/pmu-emul.c|1709| <<kvm_arm_pmu_v3_set_attr>> if (vcpu->arch.pmu.created) return -EBUSY;
+	 */
 	if (!vcpu->arch.pmu.created)
 		return -EINVAL;
 
@@ -827,6 +1598,16 @@ int kvm_arm_pmu_v3_enable(struct kvm_vcpu *vcpu)
 	 * irqchip, or to not have an in-kernel GIC and not set an IRQ.
 	 */
 	if (irqchip_in_kernel(vcpu->kvm)) {
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 */
 		int irq = vcpu->arch.pmu.irq_num;
 		/*
 		 * If we are using an in-kernel vgic, at this point we know
@@ -840,12 +1621,21 @@ int kvm_arm_pmu_v3_enable(struct kvm_vcpu *vcpu)
 		   return -EINVAL;
 	}
 
+	/*
+	 * 在以下使用KVM_REQ_RELOAD_PMU:
+	 *   - arch/arm64/kvm/arm.c|1022| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu)) kvm_vcpu_reload_pmu(vcpu);
+	 *   - arch/arm64/kvm/pmu-emul.c|1413| <<kvm_arm_pmu_v3_enable>> kvm_make_request(KVM_REQ_RELOAD_PMU, vcpu);
+	 */
 	/* One-off reload of the PMU on first run */
 	kvm_make_request(KVM_REQ_RELOAD_PMU, vcpu);
 
 	return 0;
 }
 
+/*
+ * 处理KVM_ARM_VCPU_PMU_V3_SET_PMU:
+ *   - arch/arm64/kvm/pmu-emul.c|1797| <<kvm_arm_pmu_v3_set_attr(KVM_ARM_VCPU_PMU_V3_SET_PMU)>> return kvm_arm_pmu_v3_init(vcpu);
+ */
 static int kvm_arm_pmu_v3_init(struct kvm_vcpu *vcpu)
 {
 	if (irqchip_in_kernel(vcpu->kvm)) {
@@ -862,15 +1652,42 @@ static int kvm_arm_pmu_v3_init(struct kvm_vcpu *vcpu)
 		if (!kvm_arm_pmu_irq_initialized(vcpu))
 			return -ENXIO;
 
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 *
+		 * called by:
+		 *   - arch/arm64/kvm/arch_timer.c|1465| <<timer_irqs_are_valid>> if (kvm_vgic_set_owner(vcpu, irq, ctx))
+		 *   - arch/arm64/kvm/pmu-emul.c|1552| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num, &vcpu->arch.pmu);
+		 */
 		ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
 					 &vcpu->arch.pmu);
 		if (ret)
 			return ret;
 	}
 
+	/*
+	 * 在以下使用kvm_pmu->overflow_work:
+	 *   - arch/arm64/kvm/pmu-emul.c|453| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_perf_overflow_notify_vcpu>> vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|718| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|1092| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+	 */
 	init_irq_work(&vcpu->arch.pmu.overflow_work,
 		      kvm_pmu_perf_overflow_notify_vcpu);
 
+	/*
+	 * 在以下使用kvm_pmu->created:
+	 *   - arch/arm64/kvm/pmu-emul.c|1503| <<kvm_arm_pmu_v3_enable>> if (!vcpu->arch.pmu.created) return -EINVAL;
+	 *   - arch/arm64/kvm/pmu-emul.c|1568| <<kvm_arm_pmu_v3_init>> vcpu->arch.pmu.created = true;
+	 *   - arch/arm64/kvm/pmu-emul.c|1709| <<kvm_arm_pmu_v3_set_attr>> if (vcpu->arch.pmu.created) return -EBUSY;
+	 */
 	vcpu->arch.pmu.created = true;
 	return 0;
 }
@@ -880,6 +1697,10 @@ static int kvm_arm_pmu_v3_init(struct kvm_vcpu *vcpu)
  * As a PPI, the interrupt number is the same for all vcpus,
  * while as an SPI it must be a separate number per vcpu.
  */
+/*
+ * 处理KVM_ARM_VCPU_PMU_V3_IRQ:
+ *   - arch/arm64/kvm/pmu-emul.c|1798| <<kvm_arm_pmu_v3_set_attr(KVM_ARM_VCPU_PMU_V3_IRQ)>> if (!pmu_irq_is_valid(kvm, irq))
+ */
 static bool pmu_irq_is_valid(struct kvm *kvm, int irq)
 {
 	unsigned long i;
@@ -889,6 +1710,16 @@ static bool pmu_irq_is_valid(struct kvm *kvm, int irq)
 		if (!kvm_arm_pmu_irq_initialized(vcpu))
 			continue;
 
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 */
 		if (irq_is_ppi(irq)) {
 			if (vcpu->arch.pmu.irq_num != irq)
 				return false;
@@ -905,6 +1736,11 @@ static bool pmu_irq_is_valid(struct kvm *kvm, int irq)
  * kvm_arm_pmu_get_max_counters - Return the max number of PMU counters.
  * @kvm: The kvm pointer
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+ *   - arch/arm64/kvm/sys_regs.c|1320| <<set_pmcr>> if (!kvm_vm_has_ran_once(kvm) && new_n <= kvm_arm_pmu_get_max_counters(kvm))
+ */
 u8 kvm_arm_pmu_get_max_counters(struct kvm *kvm)
 {
 	struct arm_pmu *arm_pmu = kvm->arch.arm_pmu;
@@ -916,11 +1752,32 @@ u8 kvm_arm_pmu_get_max_counters(struct kvm *kvm)
 	return arm_pmu->num_events - 1;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1526| <<kvm_arm_set_default_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+ *   - arch/arm64/kvm/pmu-emul.c|1549| <<kvm_arm_pmu_v3_set_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+ */
 static void kvm_arm_set_pmu(struct kvm *kvm, struct arm_pmu *arm_pmu)
 {
 	lockdep_assert_held(&kvm->arch.config_lock);
 
 	kvm->arch.arm_pmu = arm_pmu;
+	/*
+	 * PMCR寄存器:
+	 * PMCR_EL0, Performance Monitors Control Register
+	 *
+	 * Provides details of the Performance Monitors implementation, including the
+	 * number of counters implemented, and configures and controls the counters.
+	 *
+	 * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+	 * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+	 *
+	 * 在以下使用kvm_arch->pmcr_n:
+	 *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+	 *   - arch/arm64/kvm/pmu-emul.c|1969| <<kvm_vcpu_read_pmcr>> return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
+	 *   - arch/arm64/kvm/sys_regs.c|862| <<reset_pmu_reg>> u8 n = vcpu->kvm->arch.pmcr_n;
+	 *   - arch/arm64/kvm/sys_regs.c|1321| <<set_pmcr>> kvm->arch.pmcr_n = new_n;
+	 */
 	kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
 }
 
@@ -936,17 +1793,33 @@ static void kvm_arm_set_pmu(struct kvm *kvm, struct arm_pmu *arm_pmu)
  * where vCPUs can be scheduled on any core but the guest
  * counters could stop working.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1489| <<kvm_setup_vcpu>> ret = kvm_arm_set_default_pmu(kvm);
+ */
 int kvm_arm_set_default_pmu(struct kvm *kvm)
 {
+	/*
+	 * 只在此处调用
+	 */
 	struct arm_pmu *arm_pmu = kvm_pmu_probe_armpmu();
 
 	if (!arm_pmu)
 		return -ENODEV;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|1526| <<kvm_arm_set_default_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+	 *   - arch/arm64/kvm/pmu-emul.c|1549| <<kvm_arm_pmu_v3_set_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+	 */
 	kvm_arm_set_pmu(kvm, arm_pmu);
 	return 0;
 }
 
+/*
+ * 处理KVM_ARM_VCPU_PMU_V3_SET_PMU:
+ *   - arch/arm64/kvm/pmu-emul.c|1654| <<kvm_arm_pmu_v3_set_attr(KVM_ARM_VCPU_PMU_V3_SET_PMU)>> return kvm_arm_pmu_v3_set_pmu(vcpu, pmu_id);
+ */
 static int kvm_arm_pmu_v3_set_pmu(struct kvm_vcpu *vcpu, int pmu_id)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -966,6 +1839,11 @@ static int kvm_arm_pmu_v3_set_pmu(struct kvm_vcpu *vcpu, int pmu_id)
 				break;
 			}
 
+			/*
+			 * called by:
+			 *   - arch/arm64/kvm/pmu-emul.c|1526| <<kvm_arm_set_default_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+			 *   - arch/arm64/kvm/pmu-emul.c|1549| <<kvm_arm_pmu_v3_set_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+			 */
 			kvm_arm_set_pmu(kvm, arm_pmu);
 			cpumask_copy(kvm->arch.supported_cpus, &arm_pmu->supported_cpus);
 			ret = 0;
@@ -977,6 +1855,21 @@ static int kvm_arm_pmu_v3_set_pmu(struct kvm_vcpu *vcpu, int pmu_id)
 	return ret;
 }
 
+/*
+ * #define   KVM_ARM_VCPU_PMU_V3_IRQ       0
+ * #define   KVM_ARM_VCPU_PMU_V3_INIT      1
+ * #define   KVM_ARM_VCPU_PMU_V3_FILTER    2 --> QEMU-9.1不使用
+ * #define   KVM_ARM_VCPU_PMU_V3_SET_PMU   3 --> QEMU-9.1不使用
+ *
+ * 处理KVM_ARM_VCPU_PMU_V3_CTRL:
+ *   - arch/arm64/kvm/guest.c|955| <<kvm_arm_vcpu_arch_set_attr>> ret = kvm_arm_pmu_v3_set_attr(vcpu, attr);
+ *
+ * kvm_arm_pmu_v3_set_attr()
+ * kvm_arm_vcpu_arch_set_attr(KVM_ARM_VCPU_PMU_V3_CTRL)
+ * kvm_arm_vcpu_set_attr()
+ * kvm_arch_vcpu_ioctl(KVM_SET_DEVICE_ATTR)
+ * kvm_vcpu_ioctl(default)
+ */
 int kvm_arm_pmu_v3_set_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -986,6 +1879,12 @@ int kvm_arm_pmu_v3_set_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 	if (!kvm_vcpu_has_pmu(vcpu))
 		return -ENODEV;
 
+	/*
+	 * 在以下使用kvm_pmu->created:
+	 *   - arch/arm64/kvm/pmu-emul.c|1503| <<kvm_arm_pmu_v3_enable>> if (!vcpu->arch.pmu.created) return -EINVAL;
+	 *   - arch/arm64/kvm/pmu-emul.c|1568| <<kvm_arm_pmu_v3_init>> vcpu->arch.pmu.created = true;
+	 *   - arch/arm64/kvm/pmu-emul.c|1709| <<kvm_arm_pmu_v3_set_attr>> if (vcpu->arch.pmu.created) return -EBUSY;
+	 */
 	if (vcpu->arch.pmu.created)
 		return -EBUSY;
 
@@ -1010,6 +1909,16 @@ int kvm_arm_pmu_v3_set_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 		if (kvm_arm_pmu_irq_initialized(vcpu))
 			return -EBUSY;
 
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 */
 		kvm_debug("Set kvm ARM PMU irq: %d\n", irq);
 		vcpu->arch.pmu.irq_num = irq;
 		return 0;
@@ -1096,6 +2005,16 @@ int kvm_arm_pmu_v3_get_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 		if (!kvm_arm_pmu_irq_initialized(vcpu))
 			return -ENXIO;
 
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 */
 		irq = vcpu->arch.pmu.irq_num;
 		return put_user(irq, uaddr);
 	}
@@ -1118,10 +2037,20 @@ int kvm_arm_pmu_v3_has_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 	return -ENXIO;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1319| <<kvm_host_pmu_init>> if (!pmuv3_implemented(kvm_arm_pmu_get_pmuver_limit()))
+ *   - arch/arm64/kvm/pmu-emul.c|1853| <<kvm_arm_pmu_v3_set_attr>> u8 pmuver = kvm_arm_pmu_get_pmuver_limit();
+ *   - arch/arm64/kvm/sys_regs.c|1784| <<read_sanitised_id_aa64dfr0_el1>> kvm_arm_pmu_get_pmuver_limit());
+ *   - arch/arm64/kvm/sys_regs.c|1829| <<read_sanitised_id_dfr0_el1>> u8 perfmon = pmuver_to_perfmon(kvm_arm_pmu_get_pmuver_limit());
+ */
 u8 kvm_arm_pmu_get_pmuver_limit(void)
 {
 	u64 tmp;
 
+	/*
+	 * PMUVer是[11:8]
+	 */
 	tmp = read_sanitised_ftr_reg(SYS_ID_AA64DFR0_EL1);
 	tmp = cpuid_feature_cap_perfmon_field(tmp,
 					      ID_AA64DFR0_EL1_PMUVer_SHIFT,
@@ -1129,13 +2058,57 @@ u8 kvm_arm_pmu_get_pmuver_limit(void)
 	return FIELD_GET(ARM64_FEATURE_MASK(ID_AA64DFR0_EL1_PMUVer), tmp);
 }
 
+/*
+ * PMCR寄存器:
+ * PMCR_EL0, Performance Monitors Control Register
+ *
+ * Provides details of the Performance Monitors implementation, including the
+ * number of counters implemented, and configures and controls the counters.
+ *
+ * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+ * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+ */
 /**
  * kvm_vcpu_read_pmcr - Read PMCR_EL0 register for the vCPU
  * @vcpu: The vcpu pointer
  */
 u64 kvm_vcpu_read_pmcr(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 404 enum vcpu_sysreg {
+	 * ... ...
+	 * 417
+	 * 418         // Performance Monitors Registers
+	 * 419         PMCR_EL0,       // Control Register
+	 * 420         PMSELR_EL0,     // Event Counter Selection Register
+	 * 421         PMEVCNTR0_EL0,  // Event Counter Register (0-30)
+	 * 422         PMEVCNTR30_EL0 = PMEVCNTR0_EL0 + 30,
+	 * 423         PMCCNTR_EL0,    // Cycle Counter Register
+	 * 424         PMEVTYPER0_EL0, // Event Type Register (0-30) 
+	 * 425         PMEVTYPER30_EL0 = PMEVTYPER0_EL0 + 30,
+	 * 426         PMCCFILTR_EL0,  // Cycle Count Filter Register
+	 * 427         PMCNTENSET_EL0, // Count Enable Set Register
+	 * 428         PMINTENSET_EL1, // Interrupt Enable Set Register
+	 * 429         PMOVSSET_EL0,   // Overflow Flag Status Set Register
+	 * 430         PMUSERENR_EL0,  // User Enable Register
+	 */
 	u64 pmcr = __vcpu_sys_reg(vcpu, PMCR_EL0);
 
+	/*
+	 * PMCR寄存器:
+	 * PMCR_EL0, Performance Monitors Control Register
+	 *
+	 * Provides details of the Performance Monitors implementation, including the
+	 * number of counters implemented, and configures and controls the counters.
+	 *
+	 * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+	 * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+	 *
+	 * 在以下使用kvm_arch->pmcr_n:
+	 *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+	 *   - arch/arm64/kvm/pmu-emul.c|1969| <<kvm_vcpu_read_pmcr>> return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
+	 *   - arch/arm64/kvm/sys_regs.c|862| <<reset_pmu_reg>> u8 n = vcpu->kvm->arch.pmcr_n;
+	 *   - arch/arm64/kvm/sys_regs.c|1321| <<set_pmcr>> kvm->arch.pmcr_n = new_n;
+	 */
 	return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
 }
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index 31e49da86..75922168a 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -859,6 +859,13 @@ static unsigned int pmu_visibility(const struct kvm_vcpu *vcpu,
 static u64 reset_pmu_reg(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)
 {
 	u64 mask = BIT(ARMV8_PMU_CYCLE_IDX);
+	/*
+	 * 在以下使用kvm_arch->pmcr_n:
+	 *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+	 *   - arch/arm64/kvm/pmu-emul.c|1969| <<kvm_vcpu_read_pmcr>> return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
+	 *   - arch/arm64/kvm/sys_regs.c|862| <<reset_pmu_reg>> u8 n = vcpu->kvm->arch.pmcr_n;
+	 *   - arch/arm64/kvm/sys_regs.c|1321| <<set_pmcr>> kvm->arch.pmcr_n = new_n;
+	 */
 	u8 n = vcpu->kvm->arch.pmcr_n;
 
 	if (n)
@@ -1002,6 +1009,21 @@ static bool access_pmceid(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 
 	get_access_mask(r, &mask, &shift);
 
+	/*
+	 * PMCEID1_EL0, Performance Monitors Common Event Identification register 1
+	 *
+	 * Defines which Common architectural events and Common microarchitectural
+	 * events are implemented, or counted, using PMU events in the ranges 0x0020 to
+	 * 0x003F and 0x4020 to 0x403F.
+	 *
+	 * 比如:
+	 * ID0 corresponds to common event (0x20) L2D_CACHE_ALLOCATE
+	 * ID1 corresponds to common event (0x21) BR_RETIRED
+	 * ... ...
+	 * ID12 corresponds to common event (0x2c) Reserved
+	 * ... ...
+	 * ID31 corresponds to common event (0x3f) STALL_SLOT
+	 */
 	pmceid = kvm_pmu_get_pmceid(vcpu, (p->Op2 & 1));
 	pmceid &= mask;
 	pmceid >>= shift;
@@ -1094,6 +1116,15 @@ static bool access_pmu_evcntr(struct kvm_vcpu *vcpu,
 	return true;
 }
 
+/*
+ * 在以下使用access_pmu_evtyper():
+ *   - arch/arm64/kvm/sys_regs.c|2574| <<global>> .access = access_pmu_evtyper, .reset = NULL },
+ *   - arch/arm64/kvm/sys_regs.c|2740| <<global>> { PMU_SYS_REG(PMCCFILTR_EL0), .access = access_pmu_evtyper,
+ *   - arch/arm64/kvm/sys_regs.c|3425| <<global>> { CP15_PMU_SYS_REG(DIRECT, 0, 9, 13, 1), .access = access_pmu_evtyper },
+ *   - arch/arm64/kvm/sys_regs.c|3519| <<global>> { CP15_PMU_SYS_REG(DIRECT, 0, 14, 15, 7), .access = access_pmu_evtyper },
+ *   - arch/arm64/kvm/sys_regs.c|1345| <<PMU_PMEVTYPER_EL0>> .access = access_pmu_evtyper, .reg = (PMEVTYPER0_EL0 + n), }
+ *   - arch/arm64/kvm/sys_regs.c|3376| <<PMU_PMEVTYPER>> .access = access_pmu_evtyper }
+ */
 static bool access_pmu_evtyper(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 			       const struct sys_reg_desc *r)
 {
@@ -1279,6 +1310,16 @@ static int get_pmcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 	return 0;
 }
 
+/*
+ * PMCR寄存器:
+ * PMCR_EL0, Performance Monitors Control Register
+ *
+ * Provides details of the Performance Monitors implementation, including the
+ * number of counters implemented, and configures and controls the counters.
+ *
+ * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+ * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+ */
 static int set_pmcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 		    u64 val)
 {
@@ -1287,6 +1328,13 @@ static int set_pmcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 
 	mutex_lock(&kvm->arch.config_lock);
 
+	/*
+	 * 在以下使用kvm_arch->pmcr_n:
+	 *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+	 *   - arch/arm64/kvm/pmu-emul.c|1969| <<kvm_vcpu_read_pmcr>> return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
+	 *   - arch/arm64/kvm/sys_regs.c|862| <<reset_pmu_reg>> u8 n = vcpu->kvm->arch.pmcr_n;
+	 *   - arch/arm64/kvm/sys_regs.c|1321| <<set_pmcr>> kvm->arch.pmcr_n = new_n;
+	 */
 	/*
 	 * The vCPU can't have more counters than the PMU hardware
 	 * implements. Ignore this error to maintain compatibility
diff --git a/arch/arm64/kvm/vgic/vgic.c b/arch/arm64/kvm/vgic/vgic.c
index abe29c7d8..41d26eb71 100644
--- a/arch/arm64/kvm/vgic/vgic.c
+++ b/arch/arm64/kvm/vgic/vgic.c
@@ -421,6 +421,14 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
  * level-sensitive interrupts.  You can think of the level parameter as 1
  * being HIGH and 0 being LOW and all devices being active-HIGH.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|455| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, timer_irq(timer_ctx), timer_ctx->irq.level, timer_ctx);
+ *   - arch/arm64/kvm/arm.c|1393| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu, irq_num, level, NULL);
+ *   - arch/arm64/kvm/arm.c|1401| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, NULL, irq_num, level, NULL);
+ *   - arch/arm64/kvm/pmu-emul.c|603| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, NULL, spi_id, level, NULL);
+ */
 int kvm_vgic_inject_irq(struct kvm *kvm, struct kvm_vcpu *vcpu,
 			unsigned int intid, bool level, void *owner)
 {
@@ -583,6 +591,11 @@ int kvm_vgic_get_map(struct kvm_vcpu *vcpu, unsigned int vintid)
  * Returns 0 if intid is not already used by another in-kernel device and the
  * owner is set, otherwise returns an error code.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1465| <<timer_irqs_are_valid>> if (kvm_vgic_set_owner(vcpu, irq, ctx))
+ *   - arch/arm64/kvm/pmu-emul.c|1552| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num, &vcpu->arch.pmu);
+ */
 int kvm_vgic_set_owner(struct kvm_vcpu *vcpu, unsigned int intid, void *owner)
 {
 	struct vgic_irq *irq;
diff --git a/arch/x86/events/amd/core.c b/arch/x86/events/amd/core.c
index 920e3a640..b53b9b20c 100644
--- a/arch/x86/events/amd/core.c
+++ b/arch/x86/events/amd/core.c
@@ -745,6 +745,11 @@ static void amd_pmu_check_overflow(void)
 	}
 }
 
+/*
+ * 在以下使用amd_pmu_enable_event():
+ *   - arch/x86/events/amd/core.c|1309| <<global>> struct x86_pmu amd_pmu.enable = amd_pmu_enable_event,
+ *   - arch/x86/events/amd/core.c|765| <<amd_pmu_enable_all>> amd_pmu_enable_event(cpuc->events[idx]);
+ */
 static void amd_pmu_enable_event(struct perf_event *event)
 {
 	x86_pmu_enable_event(event);
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index be01823b1..a835ee503 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1422,6 +1422,11 @@ int x86_perf_event_set_period(struct perf_event *event)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|750| <<amd_pmu_enable_event>> x86_pmu_enable_event(event);
+ *   - arch/x86/events/intel/core.c|4286| <<core_pmu_enable_event>> x86_pmu_enable_event(event);
+ */
 void x86_pmu_enable_event(struct perf_event *event)
 {
 	if (__this_cpu_read(cpu_hw_events.enabled))
@@ -1665,6 +1670,10 @@ static void x86_pmu_del(struct perf_event *event, int flags)
 	static_call_cond(x86_pmu_del)(event);
 }
 
+/*
+ * 在以下用到了x86_pmu_handle_irq()关键字:
+ *   - 
+ */
 int x86_pmu_handle_irq(struct pt_regs *regs)
 {
 	struct perf_sample_data data;
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index ac1182141..27e64dff3 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -1187,6 +1187,16 @@ static inline bool is_counter_pair(struct hw_perf_event *hwc)
 	return hwc->flags & PERF_X86_EVENT_PAIR;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|781| <<amd_pmu_v2_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|750| <<x86_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|1428| <<x86_pmu_enable_event>> __x86_pmu_enable_event(&event->hw, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2437| <<intel_pmu_nhm_workaround>> __x86_pmu_enable_event(&event->hw, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2868| <<intel_pmu_enable_event>> __x86_pmu_enable_event(hwc, enable_mask);
+ *   - arch/x86/events/intel/core.c|4301| <<core_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/zhaoxin/core.c|347| <<zhaoxin_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ */
 static inline void __x86_pmu_enable_event(struct hw_perf_event *hwc,
 					  u64 enable_mask)
 {
diff --git a/arch/x86/kernel/nmi.c b/arch/x86/kernel/nmi.c
index ed163c8c8..bb6922a70 100644
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@ -129,6 +129,13 @@ static void nmi_check_duration(struct nmiaction *action, u64 duration)
 		action->handler, duration, decimal_msecs);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|231| <<pci_serr_error>> if (nmi_handle(NMI_SERR, regs))
+ *   - arch/x86/kernel/nmi.c|254| <<io_check_error>> if (nmi_handle(NMI_IO_CHECK, regs))
+ *   - arch/x86/kernel/nmi.c|299| <<unknown_nmi_error>> handled = nmi_handle(NMI_UNKNOWN, regs);
+ *   - arch/x86/kernel/nmi.c|351| <<default_do_nmi>> handled = nmi_handle(NMI_LOCAL, regs);
+ */
 static int nmi_handle(unsigned int type, struct pt_regs *regs)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
diff --git a/drivers/pci/bus.c b/drivers/pci/bus.c
index 55c853686..8bf93fc04 100644
--- a/drivers/pci/bus.c
+++ b/drivers/pci/bus.c
@@ -156,6 +156,10 @@ static struct pci_bus_region pci_high = {(pci_bus_addr_t) 0x100000000ULL,
  * addresses of resources we allocate, e.g., we may need a resource that
  * can be mapped by a 32-bit BAR.
  */
+/*
+ * called by:
+ *   - drivers/pci/bus.c|213| <<pci_bus_alloc_from_region>> pci_clip_resource_to_region(bus, &avail, region);
+ */
 static void pci_clip_resource_to_region(struct pci_bus *bus,
 					struct resource *res,
 					struct pci_bus_region *region)
@@ -174,6 +178,15 @@ static void pci_clip_resource_to_region(struct pci_bus *bus,
 		pcibios_bus_to_resource(bus, res, &r);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/bus.c|279| <<pci_bus_alloc_resource>> rc = pci_bus_alloc_from_region(bus, res, size, align, min,
+ *                type_mask, alignf, alignf_data, &pci_high);
+ *   - drivers/pci/bus.c|285| <<pci_bus_alloc_resource>> return pci_bus_alloc_from_region(bus, res, size, align, min,
+ *                type_mask, alignf, alignf_data, &pci_64_bit);
+ *   - drivers/pci/bus.c|291| <<pci_bus_alloc_resource>> return pci_bus_alloc_from_region(bus, res, size, align, min,
+ *                type_mask, alignf, alignf_data, &pci_32_bit);
+ */
 static int pci_bus_alloc_from_region(struct pci_bus *bus, struct resource *res,
 		resource_size_t size, resource_size_t align,
 		resource_size_t min, unsigned long type_mask,
@@ -204,6 +217,9 @@ static int pci_bus_alloc_from_region(struct pci_bus *bus, struct resource *res,
 			continue;
 
 		avail = *r;
+		/*
+		 * 只在此处调用
+		 */
 		pci_clip_resource_to_region(bus, &avail, region);
 
 		/*
@@ -245,6 +261,27 @@ static int pci_bus_alloc_from_region(struct pci_bus *bus, struct resource *res,
  * alignment and type, try to find an acceptable resource allocation
  * for a specific device resource.
  */
+/*
+ * called by:
+ *   - drivers/char/agp/intel-gtt.c|1008| <<intel_alloc_chipset_flush_resource>> ret = pci_bus_alloc_resource(intel_private.bridge_dev->bus,
+ *            &intel_private.ifp_resource, PAGE_SIZE, PAGE_SIZE, PCIBIOS_MIN_MEM, 0, pcibios_align_resource, intel_private.bridge_dev);
+ *   - drivers/gpu/drm/i915/soc/intel_gmch.c|62| <<intel_alloc_mchbar_resource>> ret = pci_bus_alloc_resource(i915->gmch.pdev->bus,
+ *            &i915->gmch.mch_res, MCHBAR_SIZE, MCHBAR_SIZE, PCIBIOS_MIN_MEM, 0, pcibios_align_resource, i915->gmch.pdev);
+ *   - drivers/pci/setup-res.c|279| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+ *            IORESOURCE_PREFETCH | IORESOURCE_MEM_64, pcibios_align_resource, dev);
+ *   - drivers/pci/setup-res.c|291| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+ *            IORESOURCE_PREFETCH, pcibios_align_resource, dev);
+ *   - drivers/pci/setup-res.c|305| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+ *            0, pcibios_align_resource, dev);
+ *   - drivers/pcmcia/rsrc_iodyn.c|70| <<__iodyn_find_io_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+ *            0, pcmcia_align, &data);
+ *   - drivers/pcmcia/rsrc_nonstatic.c|702| <<__nonstatic_find_io_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+ *            0, pcmcia_align, &data);
+ *   - drivers/pcmcia/rsrc_nonstatic.c|836| <<nonstatic_find_mem_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+ *            0, pcmcia_align, &data);
+ *   - drivers/staging/vme_user/vme_tsi148.c|754| <<tsi148_alloc_resource>> retval = pci_bus_alloc_resource(pdev->bus, &image->bus_resource,
+ *            size, 0x10000, PCIBIOS_MIN_MEM, 0, NULL, NULL);
+ */
 int pci_bus_alloc_resource(struct pci_bus *bus, struct resource *res,
 		resource_size_t size, resource_size_t align,
 		resource_size_t min, unsigned long type_mask,
@@ -254,6 +291,15 @@ int pci_bus_alloc_resource(struct pci_bus *bus, struct resource *res,
 #ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
 	int rc;
 
+	/*
+	 * called by:
+	 *   - drivers/pci/bus.c|279| <<pci_bus_alloc_resource>> rc = pci_bus_alloc_from_region(bus, res, size, align, min,
+	 *                type_mask, alignf, alignf_data, &pci_high);
+	 *   - drivers/pci/bus.c|285| <<pci_bus_alloc_resource>> return pci_bus_alloc_from_region(bus, res, size, align, min,
+	 *                type_mask, alignf, alignf_data, &pci_64_bit);
+	 *   - drivers/pci/bus.c|291| <<pci_bus_alloc_resource>> return pci_bus_alloc_from_region(bus, res, size, align, min,
+	 *                type_mask, alignf, alignf_data, &pci_32_bit);
+	 */
 	if (res->flags & IORESOURCE_MEM_64) {
 		rc = pci_bus_alloc_from_region(bus, res, size, align, min,
 					       type_mask, alignf, alignf_data,
diff --git a/drivers/pci/setup-bus.c b/drivers/pci/setup-bus.c
index 23082bc0c..394b7cd59 100644
--- a/drivers/pci/setup-bus.c
+++ b/drivers/pci/setup-bus.c
@@ -212,6 +212,10 @@ static inline void reset_resource(struct resource *res)
  * Walk through each element of the realloc_head and try to procure additional
  * resources for the element, provided the element is in the head list.
  */
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|481| <<__assign_resources_sorted>> reassign_resources_sorted(realloc_head, head);
+ */
 static void reassign_resources_sorted(struct list_head *realloc_head,
 				      struct list_head *head)
 {
diff --git a/drivers/pci/setup-res.c b/drivers/pci/setup-res.c
index c6d933ddf..ebd47e72e 100644
--- a/drivers/pci/setup-res.c
+++ b/drivers/pci/setup-res.c
@@ -269,6 +269,27 @@ static int __pci_assign_resource(struct pci_bus *bus, struct pci_dev *dev,
 
 	min = (res->flags & IORESOURCE_IO) ? PCIBIOS_MIN_IO : PCIBIOS_MIN_MEM;
 
+	/*
+	 * called by:
+	 *   - drivers/char/agp/intel-gtt.c|1008| <<intel_alloc_chipset_flush_resource>> ret = pci_bus_alloc_resource(intel_private.bridge_dev->bus,
+	 *            &intel_private.ifp_resource, PAGE_SIZE, PAGE_SIZE, PCIBIOS_MIN_MEM, 0, pcibios_align_resource, intel_private.bridge_dev);
+	 *   - drivers/gpu/drm/i915/soc/intel_gmch.c|62| <<intel_alloc_mchbar_resource>> ret = pci_bus_alloc_resource(i915->gmch.pdev->bus,
+	 *            &i915->gmch.mch_res, MCHBAR_SIZE, MCHBAR_SIZE, PCIBIOS_MIN_MEM, 0, pcibios_align_resource, i915->gmch.pdev);
+	 *   - drivers/pci/setup-res.c|279| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+	 *            IORESOURCE_PREFETCH | IORESOURCE_MEM_64, pcibios_align_resource, dev);
+	 *   - drivers/pci/setup-res.c|291| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+	 *            IORESOURCE_PREFETCH, pcibios_align_resource, dev);
+	 *   - drivers/pci/setup-res.c|305| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+	 *            0, pcibios_align_resource, dev);
+	 *   - drivers/pcmcia/rsrc_iodyn.c|70| <<__iodyn_find_io_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+	 *            0, pcmcia_align, &data);
+	 *   - drivers/pcmcia/rsrc_nonstatic.c|702| <<__nonstatic_find_io_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+	 *            0, pcmcia_align, &data);
+	 *   - drivers/pcmcia/rsrc_nonstatic.c|836| <<nonstatic_find_mem_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+	 *            0, pcmcia_align, &data);
+	 *   - drivers/staging/vme_user/vme_tsi148.c|754| <<tsi148_alloc_resource>> retval = pci_bus_alloc_resource(pdev->bus, &image->bus_resource,
+	 *            size, 0x10000, PCIBIOS_MIN_MEM, 0, NULL, NULL);
+	 */
 	/*
 	 * First, try exact prefetching match.  Even if a 64-bit
 	 * prefetchable bridge window is below 4GB, we can't put a 32-bit
@@ -345,6 +366,11 @@ int pci_assign_resource(struct pci_dev *dev, int resno)
 	size = resource_size(res);
 	ret = _pci_assign_resource(dev, resno, size, align);
 
+	/*
+	 * 例子:
+	 * [   76.294540] pci 0000:01:00.0: BAR 0 [io  size 0x0040]: can't assign; no space
+	 * [   76.295401] pci 0000:01:00.0: BAR 0 [io  size 0x0040]: failed to assign
+	 */
 	/*
 	 * If we failed to assign anything, let's try the address
 	 * where firmware left it.  That at least has a chance of
@@ -370,6 +396,10 @@ int pci_assign_resource(struct pci_dev *dev, int resno)
 }
 EXPORT_SYMBOL(pci_assign_resource);
 
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|256| <<reassign_resources_sorted>> if (pci_reassign_resource(add_res->dev, idx, add_size, align))
+ */
 int pci_reassign_resource(struct pci_dev *dev, int resno,
 			  resource_size_t addsize, resource_size_t min_align)
 {
diff --git a/drivers/perf/arm_pmu.c b/drivers/perf/arm_pmu.c
index 8458fe2ce..911e9628a 100644
--- a/drivers/perf/arm_pmu.c
+++ b/drivers/perf/arm_pmu.c
@@ -99,6 +99,17 @@ static const struct pmu_irq_ops percpu_pmunmi_ops = {
 	.free_pmuirq = armpmu_free_percpu_pmunmi
 };
 
+/*
+ * 在以下使用percpu的cpu_armpmu:
+ *   - drivers/perf/arm_pmu.c|102| <<global>> static DEFINE_PER_CPU(struct arm_pmu *, cpu_armpmu);
+ *   - drivers/perf/arm_pmu.c|629| <<armpmu_free_irq>> per_cpu(cpu_irq_ops, cpu)->free_pmuirq(irq, cpu, &cpu_armpmu);
+ *   - drivers/perf/arm_pmu.c|660| <<armpmu_request_irq>> err = request_nmi(irq, handler, irq_flags, "arm-pmu", per_cpu_ptr(&cpu_armpmu, cpu));
+ *   - drivers/perf/arm_pmu.c|665| <<armpmu_request_irq>> err = request_irq(irq, handler, irq_flags, "arm-pmu", per_cpu_ptr(&cpu_armpmu, cpu));
+ *   - drivers/perf/arm_pmu.c|672| <<armpmu_request_irq>> err = request_percpu_nmi(irq, handler, "arm-pmu", &cpu_armpmu);
+ *   - drivers/perf/arm_pmu.c|677| <<armpmu_request_irq>> err = request_percpu_irq(irq, handler, "arm-pmu", &cpu_armpmu);
+ *   - drivers/perf/arm_pmu.c|730| <<arm_perf_starting_cpu>> per_cpu(cpu_armpmu, cpu) = pmu;
+ *   - drivers/perf/arm_pmu.c|751| <<arm_perf_teardown_cpu>> per_cpu(cpu_armpmu, cpu) = NULL;
+ */
 static DEFINE_PER_CPU(struct arm_pmu *, cpu_armpmu);
 static DEFINE_PER_CPU(int, cpu_irq);
 static DEFINE_PER_CPU(const struct pmu_irq_ops *, cpu_irq_ops);
@@ -170,6 +181,24 @@ armpmu_map_raw_event(u32 raw_event_mask, u64 config)
 	return (int)(config & raw_event_mask);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1176| <<global>> return armpmu_map_event(event, extra_event_map, extra_cache_map, ARMV8_PMU_EVTYPE_EVENT);
+ *   - drivers/perf/apple_m1_cpu_pmu.c|493| <<m1_pmu_map_event>> return armpmu_map_event(event, &m1_pmu_perf_map, NULL, M1_PMU_CFG_EVENT);
+ *   - drivers/perf/apple_m1_cpu_pmu.c|504| <<m2_pmu_map_event>> return armpmu_map_event(event, &m1_pmu_perf_map, NULL, M1_PMU_CFG_EVENT);
+ *   - drivers/perf/arm_pmuv3.c|1113| <<__armv8_pmuv3_map_event_id>> return armpmu_map_event(event, &armv8_pmuv3_perf_map, &armv8_pmuv3_perf_cache_map, ARMV8_PMU_EVTYPE_EVENT);
+ *   - drivers/perf/arm_v6_pmu.c|378| <<armv6_map_event>> return armpmu_map_event(event, &armv6_perf_map, &armv6_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1096| <<armv7_a8_map_event>> return armpmu_map_event(event, &armv7_a8_perf_map, &armv7_a8_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1102| <<armv7_a9_map_event>> return armpmu_map_event(event, &armv7_a9_perf_map, &armv7_a9_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1108| <<armv7_a5_map_event>> return armpmu_map_event(event, &armv7_a5_perf_map, &armv7_a5_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1114| <<armv7_a15_map_event>> return armpmu_map_event(event, &armv7_a15_perf_map, &armv7_a15_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1120| <<armv7_a7_map_event>> return armpmu_map_event(event, &armv7_a7_perf_map, &armv7_a7_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1126| <<armv7_a12_map_event>> return armpmu_map_event(event, &armv7_a12_perf_map, &armv7_a12_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1132| <<krait_map_event>> return armpmu_map_event(event, &krait_perf_map, &krait_perf_cache_map, 0xFFFFF);
+ *   - drivers/perf/arm_v7_pmu.c|1138| <<krait_map_event_no_branch>> return armpmu_map_event(event, &krait_perf_map_no_branch, &krait_perf_cache_map, 0xFFFFF);
+ *   - drivers/perf/arm_v7_pmu.c|1144| <<scorpion_map_event>> return armpmu_map_event(event, &scorpion_perf_map, &scorpion_perf_cache_map, 0xFFFFF);
+ *   - drivers/perf/arm_xscale_pmu.c|350| <<xscale_map_event>> return armpmu_map_event(event, &xscale_perf_map, &xscale_perf_cache_map, 0xFF);
+ */
 int
 armpmu_map_event(struct perf_event *event,
 		 const unsigned (*event_map)[PERF_COUNT_HW_MAX],
@@ -197,6 +226,16 @@ armpmu_map_event(struct perf_event *event,
 	return -ENOENT;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/apple_m1_cpu_pmu.c|412| <<m1_pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ *   - drivers/perf/arm_pmu.c|348| <<armpmu_start>> armpmu_event_set_period(event);
+ *   - drivers/perf/arm_pmuv3.c|927| <<armv8pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ *   - drivers/perf/arm_v6_pmu.c|275| <<armv6pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ *   - drivers/perf/arm_v7_pmu.c|979| <<armv7pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ *   - drivers/perf/arm_xscale_pmu.c|184| <<xscale1pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ *   - drivers/perf/arm_xscale_pmu.c|516| <<xscale2pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ */
 int armpmu_event_set_period(struct perf_event *event)
 {
 	struct arm_pmu *armpmu = to_arm_pmu(event->pmu);
@@ -239,6 +278,17 @@ int armpmu_event_set_period(struct perf_event *event)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/apple_m1_cpu_pmu.c|410| <<m1_pmu_handle_irq>> armpmu_event_update(event);
+ *   - drivers/perf/arm_pmu.c|297| <<armpmu_read>> armpmu_event_update(event);
+ *   - drivers/perf/arm_pmu.c|312| <<armpmu_stop>> armpmu_event_update(event);
+ *   - drivers/perf/arm_pmuv3.c|898| <<armv8pmu_handle_irq>> armpmu_event_update(event);
+ *   - drivers/perf/arm_v6_pmu.c|273| <<armv6pmu_handle_irq>> armpmu_event_update(event);
+ *   - drivers/perf/arm_v7_pmu.c|977| <<armv7pmu_handle_irq>> armpmu_event_update(event);
+ *   - drivers/perf/arm_xscale_pmu.c|182| <<xscale1pmu_handle_irq>> armpmu_event_update(event);
+ *   - drivers/perf/arm_xscale_pmu.c|514| <<xscale2pmu_handle_irq>> armpmu_event_update(event);
+ */
 u64 armpmu_event_update(struct perf_event *event)
 {
 	struct arm_pmu *armpmu = to_arm_pmu(event->pmu);
@@ -909,6 +959,11 @@ void armpmu_free(struct arm_pmu *pmu)
 	kfree(pmu);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmu_acpi.c|422| <<arm_pmu_acpi_probe>> ret = armpmu_register(pmu);
+ *   - drivers/perf/arm_pmu_platform.c|232| <<arm_pmu_device_probe>> ret = armpmu_register(pmu);
+ */
 int armpmu_register(struct arm_pmu *pmu)
 {
 	int ret;
diff --git a/drivers/perf/arm_pmu_acpi.c b/drivers/perf/arm_pmu_acpi.c
index 05dda19c5..683ab7f2f 100644
--- a/drivers/perf/arm_pmu_acpi.c
+++ b/drivers/perf/arm_pmu_acpi.c
@@ -349,6 +349,10 @@ static void arm_pmu_acpi_probe_matching_cpus(struct arm_pmu *pmu,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1434| <<armv8_pmu_driver_init>> ret = arm_pmu_acpi_probe(armv8_pmuv3_pmu_init);
+ */
 int arm_pmu_acpi_probe(armpmu_init_fn init_fn)
 {
 	int pmu_idx = 0;
diff --git a/drivers/perf/arm_pmuv3.c b/drivers/perf/arm_pmuv3.c
index d24684079..448ac941c 100644
--- a/drivers/perf/arm_pmuv3.c
+++ b/drivers/perf/arm_pmuv3.c
@@ -326,6 +326,14 @@ GEN_PMU_FORMAT_ATTR(threshold);
 
 static int sysctl_perf_user_access __read_mostly;
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1217| <<global>> if (armv8pmu_event_is_64bit(event))
+ *   - drivers/perf/arm_pmuv3.c|1231| <<global>> if (armv8pmu_event_is_64bit(event) &&
+ *   - drivers/perf/arm_pmuv3.c|498| <<armv8pmu_event_is_chained>> armv8pmu_event_is_64bit(event) &&
+ *   - drivers/perf/arm_pmuv3.c|564| <<armv8pmu_event_needs_bias>> if (armv8pmu_event_is_64bit(event))
+ *   - drivers/perf/arm_pmuv3.c|1030| <<armv8pmu_get_event_idx>> else if (armv8pmu_event_is_64bit(event) &&
+ */
 static bool armv8pmu_event_is_64bit(struct perf_event *event)
 {
 	return ATTR_CFG_GET_FLD(&event->attr, long);
@@ -466,6 +474,16 @@ static const struct attribute_group armv8_pmuv3_caps_attr_group = {
  * On AArch32, long counters make no sense (you can't access the top
  * bits), so we only enable this on AArch64.
  */
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1195| <<global>> if (armv8pmu_event_is_64bit(event) && (hw_event_id != ARMV8_PMUV3_PERFCTR_CPU_CYCLES) && !armv8pmu_has_long_event(armpmu))
+ *   - drivers/perf/arm_pmuv3.c|491| <<armv8pmu_event_is_chained>> return !armv8pmu_event_has_user_read(event) && armv8pmu_event_is_64bit(event) && !armv8pmu_has_long_event(cpu_pmu) && (idx != ARMV8_IDX_CYCLE_COUNTER);
+ *   - drivers/perf/arm_pmuv3.c|559| <<armv8pmu_event_needs_bias>> if (armv8pmu_has_long_event(cpu_pmu) || idx == ARMV8_IDX_CYCLE_COUNTER)
+ *   - drivers/perf/arm_pmuv3.c|994| <<armv8pmu_get_event_idx>> else if (armv8pmu_event_is_64bit(event) && armv8pmu_event_want_user_access(event) && !armv8pmu_has_long_event(cpu_pmu))
+ *   - drivers/perf/arm_pmuv3.c|1121| <<armv8pmu_reset>> if (armv8pmu_has_long_event(cpu_pmu))
+ *
+ * A1的芯片的VM是long=0
+ */
 static bool armv8pmu_has_long_event(struct arm_pmu *cpu_pmu)
 {
 	return (IS_ENABLED(CONFIG_ARM64) && is_pmuv3p5(cpu_pmu->pmuver));
@@ -481,6 +499,15 @@ static bool armv8pmu_event_has_user_read(struct perf_event *event)
  * except when we have allocated the 64bit cycle counter (for CPU
  * cycles event) or when user space counter access is enabled.
  */
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|547| <<armv8pmu_read_hw_counter>> if (armv8pmu_event_is_chained(event))
+ *   - drivers/perf/arm_pmuv3.c|627| <<armv8pmu_write_hw_counter>> if (armv8pmu_event_is_chained(event)) {
+ *   - drivers/perf/arm_pmuv3.c|687| <<armv8pmu_write_event_type>> if (armv8pmu_event_is_chained(event)) {
+ *   - drivers/perf/arm_pmuv3.c|706| <<armv8pmu_event_cnten_mask>> if (armv8pmu_event_is_chained(event))
+ *   - drivers/perf/arm_pmuv3.c|1039| <<armv8pmu_get_event_idx>> if (armv8pmu_event_is_chained(event))
+ *   - drivers/perf/arm_pmuv3.c|1051| <<armv8pmu_clear_event_idx>> if (armv8pmu_event_is_chained(event))
+ */
 static bool armv8pmu_event_is_chained(struct perf_event *event)
 {
 	int idx = event->hw.idx;
@@ -502,11 +529,23 @@ static bool armv8pmu_event_is_chained(struct perf_event *event)
 #define	ARMV8_IDX_TO_COUNTER(x)	\
 	(((x) - ARMV8_IDX_COUNTER0) & ARMV8_PMU_COUNTER_MASK)
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|889| <<armv8pmu_start>> armv8pmu_pmcr_write(armv8pmu_pmcr_read() | ARMV8_PMU_PMCR_E);
+ *   - drivers/perf/arm_pmuv3.c|897| <<armv8pmu_stop>> armv8pmu_pmcr_write(armv8pmu_pmcr_read() & ~ARMV8_PMU_PMCR_E);
+ *   - drivers/perf/arm_pmuv3.c|1328| <<__armv8pmu_probe_pmu>> cpu_pmu->num_events = FIELD_GET(ARMV8_PMU_PMCR_N, armv8pmu_pmcr_read());
+ */
 static u64 armv8pmu_pmcr_read(void)
 {
 	return read_pmcr();
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|889| <<armv8pmu_start>> armv8pmu_pmcr_write(armv8pmu_pmcr_read() | ARMV8_PMU_PMCR_E);
+ *   - drivers/perf/arm_pmuv3.c|897| <<armv8pmu_stop>> armv8pmu_pmcr_write(armv8pmu_pmcr_read() & ~ARMV8_PMU_PMCR_E);
+ *   - drivers/perf/arm_pmuv3.c|1184| <<armv8pmu_reset>> armv8pmu_pmcr_write(pmcr);
+ */
 static void armv8pmu_pmcr_write(u64 val)
 {
 	val &= ARMV8_PMU_PMCR_MASK;
@@ -519,6 +558,10 @@ static int armv8pmu_has_overflowed(u32 pmovsr)
 	return pmovsr & ARMV8_PMU_OVERFLOWED_MASK;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|957| <<armv8pmu_handle_irq>> if (!armv8pmu_counter_has_overflowed(pmovsr, idx))
+ */
 static int armv8pmu_counter_has_overflowed(u32 pmnc, int idx)
 {
 	return pmnc & BIT(ARMV8_IDX_TO_COUNTER(idx));
@@ -593,6 +636,13 @@ static u64 armv8pmu_read_counter(struct perf_event *event)
 	return  armv8pmu_unbias_long_counter(event, value);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|617| <<armv8pmu_write_hw_counter>> armv8pmu_write_evcntr(idx, upper_32_bits(value));
+ *   - drivers/perf/arm_pmuv3.c|618| <<armv8pmu_write_hw_counter>> armv8pmu_write_evcntr(idx - 1, lower_32_bits(value));
+ *   - drivers/perf/arm_pmuv3.c|620| <<armv8pmu_write_hw_counter>> armv8pmu_write_evcntr(idx, value);
+ *   - drivers/perf/arm_pmuv3.c|811| <<armv8pmu_enable_user_access>> armv8pmu_write_evcntr(i, 0);
+ */
 static void armv8pmu_write_evcntr(int idx, u64 value)
 {
 	u32 counter = ARMV8_IDX_TO_COUNTER(idx);
@@ -600,6 +650,10 @@ static void armv8pmu_write_evcntr(int idx, u64 value)
 	write_pmevcntrn(counter, value);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|634| <<armv8pmu_write_counter>> armv8pmu_write_hw_counter(event, value);
+ */
 static void armv8pmu_write_hw_counter(struct perf_event *event,
 					     u64 value)
 {
@@ -613,6 +667,10 @@ static void armv8pmu_write_hw_counter(struct perf_event *event,
 	}
 }
 
+/*
+ * 在以下使用armv8pmu_write_counter():
+ *   - drivers/perf/arm_pmuv3.c|1388| <<global>> cpu_pmu->write_counter = armv8pmu_write_counter;
+ */
 static void armv8pmu_write_counter(struct perf_event *event, u64 value)
 {
 	struct hw_perf_event *hwc = &event->hw;
@@ -626,6 +684,12 @@ static void armv8pmu_write_counter(struct perf_event *event, u64 value)
 		armv8pmu_write_hw_counter(event, value);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|658| <<armv8pmu_write_event_type>> armv8pmu_write_evtype(idx - 1, hwc->config_base);
+ *   - drivers/perf/arm_pmuv3.c|659| <<armv8pmu_write_event_type>> armv8pmu_write_evtype(idx, chain_evt);
+ *   - drivers/perf/arm_pmuv3.c|664| <<armv8pmu_write_event_type>> armv8pmu_write_evtype(idx, hwc->config_base);
+ */
 static void armv8pmu_write_evtype(int idx, unsigned long val)
 {
 	u32 counter = ARMV8_IDX_TO_COUNTER(idx);
@@ -641,6 +705,10 @@ static void armv8pmu_write_evtype(int idx, unsigned long val)
 	write_pmevtypern(counter, val);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|806| <<armv8pmu_enable_event>> armv8pmu_write_event_type(event);
+ */
 static void armv8pmu_write_event_type(struct perf_event *event)
 {
 	struct hw_perf_event *hwc = &event->hw;
@@ -814,6 +882,11 @@ static void armv8pmu_disable_event(struct perf_event *event)
 	armv8pmu_disable_event_irq(event);
 }
 
+/*
+ * 在以下使用armv8pmu_start():
+ *   - drivers/perf/arm_pmuv3.c|1368| <<global>> cpu_pmu->start = armv8pmu_start;
+ *   - drivers/perf/arm_pmuv3.c|938| <<armv8pmu_handle_irq>> armv8pmu_start(cpu_pmu);
+ */
 static void armv8pmu_start(struct arm_pmu *cpu_pmu)
 {
 	struct perf_event_context *ctx;
@@ -840,10 +913,22 @@ static void armv8pmu_stop(struct arm_pmu *cpu_pmu)
 	armv8pmu_pmcr_write(armv8pmu_pmcr_read() & ~ARMV8_PMU_PMCR_E);
 }
 
+/*
+ * 在以下使用armv8pmu_handle_irq():
+ *   - drivers/perf/arm_pmuv3.c|1330| <<global>> cpu_pmu->handle_irq = armv8pmu_handle_irq;
+ */
 static irqreturn_t armv8pmu_handle_irq(struct arm_pmu *cpu_pmu)
 {
 	u32 pmovsr;
 	struct perf_sample_data data;
+	/*
+	 * struct pmu_hw_events {
+	 *     struct perf_event       *events[ARMPMU_MAX_HWEVENTS];
+	 *     DECLARE_BITMAP(used_mask, ARMPMU_MAX_HWEVENTS);
+	 *     struct arm_pmu          *percpu_pmu;
+	 *     int irq;
+	 * };
+	 */
 	struct pmu_hw_events *cpuc = this_cpu_ptr(cpu_pmu->hw_events);
 	struct pt_regs *regs;
 	int idx;
@@ -851,6 +936,10 @@ static irqreturn_t armv8pmu_handle_irq(struct arm_pmu *cpu_pmu)
 	/*
 	 * Get and reset the IRQ flags
 	 */
+	/*
+	 * 注释:
+	 * PMOVSR, Performance Monitors Overflow Flag Status Register, PMSA
+	 */
 	pmovsr = armv8pmu_getreset_flags();
 
 	/*
@@ -885,8 +974,29 @@ static irqreturn_t armv8pmu_handle_irq(struct arm_pmu *cpu_pmu)
 			continue;
 
 		hwc = &event->hw;
+		/*
+		 * called by:
+		 *   - drivers/perf/apple_m1_cpu_pmu.c|410| <<m1_pmu_handle_irq>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_pmu.c|297| <<armpmu_read>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_pmu.c|312| <<armpmu_stop>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_pmuv3.c|898| <<armv8pmu_handle_irq>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_v6_pmu.c|273| <<armv6pmu_handle_irq>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_v7_pmu.c|977| <<armv7pmu_handle_irq>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_xscale_pmu.c|182| <<xscale1pmu_handle_irq>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_xscale_pmu.c|514| <<xscale2pmu_handle_irq>> armpmu_event_update(event);
+		 */
 		armpmu_event_update(event);
 		perf_sample_data_init(&data, 0, hwc->last_period);
+		/*
+		 * called by:
+		 *   - drivers/perf/apple_m1_cpu_pmu.c|412| <<m1_pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 *   - drivers/perf/arm_pmu.c|348| <<armpmu_start>> armpmu_event_set_period(event);
+		 *   - drivers/perf/arm_pmuv3.c|927| <<armv8pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 *   - drivers/perf/arm_v6_pmu.c|275| <<armv6pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 *   - drivers/perf/arm_v7_pmu.c|979| <<armv7pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 *   - drivers/perf/arm_xscale_pmu.c|184| <<xscale1pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 *   - drivers/perf/arm_xscale_pmu.c|516| <<xscale2pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 */
 		if (!armpmu_event_set_period(event))
 			continue;
 
@@ -903,6 +1013,10 @@ static irqreturn_t armv8pmu_handle_irq(struct arm_pmu *cpu_pmu)
 	return IRQ_HANDLED;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|973| <<armv8pmu_get_event_idx>> return armv8pmu_get_single_idx(cpuc, cpu_pmu);
+ */
 static int armv8pmu_get_single_idx(struct pmu_hw_events *cpuc,
 				    struct arm_pmu *cpu_pmu)
 {
@@ -1076,6 +1190,9 @@ static void armv8pmu_reset(void *info)
 	 */
 	pmcr = ARMV8_PMU_PMCR_P | ARMV8_PMU_PMCR_C | ARMV8_PMU_PMCR_LC;
 
+	/*
+	 * A1的芯片的VM是long=0
+	 */
 	/* Enable long event counter support where available */
 	if (armv8pmu_has_long_event(cpu_pmu))
 		pmcr |= ARMV8_PMU_PMCR_LP;
@@ -1105,6 +1222,15 @@ static int __armv8_pmuv3_map_event_id(struct arm_pmu *armpmu,
 				ARMV8_PMU_EVTYPE_EVENT);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1163| <<armv8_pmuv3_map_event>> return __armv8_pmuv3_map_event(event, NULL, NULL);
+ *   - drivers/perf/arm_pmuv3.c|1168| <<armv8_a53_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_a53_perf_cache_map);
+ *   - drivers/perf/arm_pmuv3.c|1173| <<armv8_a57_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_a57_perf_cache_map);
+ *   - drivers/perf/arm_pmuv3.c|1178| <<armv8_a73_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_a73_perf_cache_map);
+ *   - drivers/perf/arm_pmuv3.c|1183| <<armv8_thunder_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_thunder_perf_cache_map);
+ *   - drivers/perf/arm_pmuv3.c|1189| <<armv8_vulcan_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_vulcan_perf_cache_map);
+ */
 static int __armv8_pmuv3_map_event(struct perf_event *event,
 				   const unsigned (*extra_event_map)
 						  [PERF_COUNT_HW_MAX],
@@ -1195,6 +1321,10 @@ struct armv8pmu_probe_info {
 	bool present;
 };
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1247| <<armv8pmu_probe_pmu>> ret = smp_call_function_any(&cpu_pmu->supported_cpus, __armv8pmu_probe_pmu, &probe, 1);
+ */
 static void __armv8pmu_probe_pmu(void *info)
 {
 	struct armv8pmu_probe_info *probe = info;
@@ -1235,6 +1365,10 @@ static void __armv8pmu_probe_pmu(void *info)
 		cpu_pmu->reg_pmmir = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1294| <<armv8_pmu_init>> int ret = armv8pmu_probe_pmu(cpu_pmu);
+ */
 static int armv8pmu_probe_pmu(struct arm_pmu *cpu_pmu)
 {
 	struct armv8pmu_probe_info probe = {
@@ -1288,10 +1422,15 @@ static void armv8_pmu_register_sysctl_table(void)
 		register_sysctl("kernel", armv8_pmu_sysctl_table);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1324| <<PMUV3_INIT_SIMPLE>> return armv8_pmu_init(cpu_pmu, #name, armv8_pmuv3_map_event); \
+ *   - drivers/perf/arm_pmuv3.c|1330| <<PMUV3_INIT_MAP_EVENT>> return armv8_pmu_init(cpu_pmu, #name, map_event); \
+ */
 static int armv8_pmu_init(struct arm_pmu *cpu_pmu, char *name,
 			  int (*map_event)(struct perf_event *event))
 {
-	int ret = armv8pmu_probe_pmu(cpu_pmu);
+	int ret = armv8pmu_probe_pmu(cpu_pm);
 	if (ret)
 		return ret;
 
@@ -1318,12 +1457,54 @@ static int armv8_pmu_init(struct arm_pmu *cpu_pmu, char *name,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1333| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_pmuv3)
+ *   - drivers/perf/arm_pmuv3.c|1335| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a34)
+ *   - drivers/perf/arm_pmuv3.c|1336| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a55)
+ *   - drivers/perf/arm_pmuv3.c|1337| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a65)
+ *   - drivers/perf/arm_pmuv3.c|1338| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a75)
+ *   - drivers/perf/arm_pmuv3.c|1339| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a76)
+ *   - drivers/perf/arm_pmuv3.c|1340| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a77)
+ *   - drivers/perf/arm_pmuv3.c|1341| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a78)
+ *   - drivers/perf/arm_pmuv3.c|1342| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a510)
+ *   - drivers/perf/arm_pmuv3.c|1343| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a520)
+ *   - drivers/perf/arm_pmuv3.c|1344| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a710)
+ *   - drivers/perf/arm_pmuv3.c|1345| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a715)
+ *   - drivers/perf/arm_pmuv3.c|1346| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a720)
+ *   - drivers/perf/arm_pmuv3.c|1347| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a725)
+ *   - drivers/perf/arm_pmuv3.c|1348| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_x1)
+ *   - drivers/perf/arm_pmuv3.c|1349| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_x2)
+ *   - drivers/perf/arm_pmuv3.c|1350| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_x3)
+ *   - drivers/perf/arm_pmuv3.c|1351| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_x4)
+ *   - drivers/perf/arm_pmuv3.c|1352| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_x925)
+ *   - drivers/perf/arm_pmuv3.c|1353| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_e1)
+ *   - drivers/perf/arm_pmuv3.c|1354| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_n1)
+ *   - drivers/perf/arm_pmuv3.c|1355| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_neoverse_n2)
+ *   - drivers/perf/arm_pmuv3.c|1356| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_neoverse_n3)
+ *   - drivers/perf/arm_pmuv3.c|1357| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_v1)
+ *   - drivers/perf/arm_pmuv3.c|1358| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_v2)
+ *   - drivers/perf/arm_pmuv3.c|1359| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_v3)
+ *   - drivers/perf/arm_pmuv3.c|1360| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_v3ae)
+ *   - drivers/perf/arm_pmuv3.c|1362| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_nvidia_carmel)
+ *   - drivers/perf/arm_pmuv3.c|1363| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_nvidia_denver)
+ */
 #define PMUV3_INIT_SIMPLE(name)						\
 static int name##_pmu_init(struct arm_pmu *cpu_pmu)			\
 {									\
 	return armv8_pmu_init(cpu_pmu, #name, armv8_pmuv3_map_event);	\
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1365| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_MAP_EVENT(armv8_cortex_a35, armv8_a53_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1366| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_cortex_a53, armv8_a53_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1367| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_cortex_a57, armv8_a57_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1368| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_cortex_a72, armv8_a57_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1369| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_cortex_a73, armv8_a73_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1370| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_cavium_thunder, armv8_thunder_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1371| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_brcm_vulcan, armv8_vulcan_map_event)
+ */
 #define PMUV3_INIT_MAP_EVENT(name, map_event)				\
 static int name##_pmu_init(struct arm_pmu *cpu_pmu)			\
 {									\
@@ -1428,6 +1609,12 @@ static int __init armv8_pmu_driver_init(void)
 {
 	int ret;
 
+	/*
+	 * 在以下设置arm64的acpi_disabled:
+	 *   - arch/arm64/kernel/acpi.c|40| <<global>> int acpi_disabled = 1;
+	 *   - arch/arm64/include/asm/acpi.h|85| <<disable_acpi>> acpi_disabled = 1;
+	 *   - arch/arm64/include/asm/acpi.h|92| <<enable_acpi>> acpi_disabled = 0;
+	 */
 	if (acpi_disabled)
 		ret = platform_driver_register(&armv8_pmu_driver);
 	else
@@ -1440,6 +1627,10 @@ static int __init armv8_pmu_driver_init(void)
 }
 device_initcall(armv8_pmu_driver_init)
 
+/*
+ * called by:
+ *   - kernel/events/core.c|6222| <<perf_event_update_userpage>> arch_perf_update_userpage(event, userpg, now);
+ */
 void arch_perf_update_userpage(struct perf_event *event,
 			       struct perf_event_mmap_page *userpg, u64 now)
 {
diff --git a/fs/locks.c b/fs/locks.c
index e45cad40f..2bd4799b5 100644
--- a/fs/locks.c
+++ b/fs/locks.c
@@ -632,6 +632,10 @@ static int posix_same_owner(struct file_lock_core *fl1, struct file_lock_core *f
 	return fl1->flc_owner == fl2->flc_owner;
 }
 
+/*
+ * called by:
+ *   - fs/locks.c|862| <<locks_insert_lock_ctx>> locks_insert_global_locks(fl);
+ */
 /* Must be called with the flc_lock held! */
 static void locks_insert_global_locks(struct file_lock_core *flc)
 {
@@ -855,6 +859,59 @@ static void locks_wake_up_blocks(struct file_lock_core *blocker)
 	spin_unlock(&blocked_lock_lock);
 }
 
+/*
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] do_lock_file_wait
+ * [0] fcntl_setlk
+ * [0] do_fcntl
+ * [0] __x64_sys_fcntl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * nfs的client.
+ *
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] locks_lock_inode_wait
+ * [0] nfs4_lock_done
+ * [0] rpc_exit_task
+ * [0] __rpc_execute
+ * [0] rpc_async_schedule
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * NFS的server.
+ *
+ * [0] nfsd4_lock
+ * [0] nfsd4_proc_compound
+ * [0] nfsd_dispatch
+ * [0] svc_process_common
+ * [0] svc_process
+ * [0] nfsd
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] nfsd4_lock
+ * [0] nfsd4_proc_compound
+ * [0] nfsd_dispatch
+ * [0] svc_process_common
+ * [0] svc_process
+ * [0] nfsd
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - fs/locks.c|1132| <<flock_lock_inode>> locks_insert_lock_ctx(&new_fl->c, &ctx->flc_flock);
+ *   - fs/locks.c|1317| <<posix_lock_inode>> locks_insert_lock_ctx(&request->c, &fl->c.flc_list);
+ *   - fs/locks.c|1348| <<posix_lock_inode>> locks_insert_lock_ctx(&new_fl->c, &fl->c.flc_list);
+ *   - fs/locks.c|1360| <<posix_lock_inode>> locks_insert_lock_ctx(&left->c, &fl->c.flc_list);
+ *   - fs/locks.c|1851| <<generic_add_lease>> locks_insert_lock_ctx(&lease->c, &ctx->flc_lease);
+ */
 static void
 locks_insert_lock_ctx(struct file_lock_core *fl, struct list_head *before)
 {
@@ -1071,6 +1128,11 @@ static bool posix_locks_deadlock(struct file_lock *caller_fl,
  * whether or not a lock was successfully freed by testing the return
  * value for -ENOENT.
  */
+/*
+ * called by:
+ *   - fs/locks.c|2080| <<flock_lock_inode_wait>> error = flock_lock_inode(inode, fl);
+ *   - fs/locks.c|2656| <<locks_remove_flock>> flock_lock_inode(inode, &fl);
+ */
 static int flock_lock_inode(struct inode *inode, struct file_lock *request)
 {
 	struct file_lock *new_fl = NULL;
@@ -1143,6 +1205,11 @@ static int flock_lock_inode(struct inode *inode, struct file_lock *request)
 	return error;
 }
 
+/*
+ * called by:
+ *   - fs/locks.c|1428| <<posix_lock_file>> return posix_lock_inode(file_inode(filp), fl, conflock);
+ *   - fs/locks.c|1444| <<posix_lock_inode_wait>> error = posix_lock_inode(inode, fl, NULL);
+ */
 static int posix_lock_inode(struct inode *inode, struct file_lock *request,
 			    struct file_lock *conflock)
 {
@@ -1396,6 +1463,16 @@ static int posix_lock_inode(struct inode *inode, struct file_lock *request,
  * whether or not a lock was successfully freed by testing the return
  * value for -ENOENT.
  */
+/*
+ * called by:
+ *   - fs/ceph/locks.c|278| <<ceph_lock>> posix_lock_file(file, fl, NULL);
+ *   - fs/ceph/locks.c|299| <<ceph_lock>> err = posix_lock_file(file, fl, NULL);
+ *   - fs/dlm/plock.c|262| <<dlm_plock_callback>> if (posix_lock_file(file, flc, NULL)) {
+ *   - fs/fuse/file.c|2714| <<fuse_file_lock>> err = posix_lock_file(file, fl, NULL);
+ *   - fs/locks.c|2395| <<vfs_lock_file>> return posix_lock_file(filp, fl, conf);
+ *   - fs/orangefs/file.c|545| <<orangefs_lock>> rc = posix_lock_file(filp, fl, NULL);
+ *   - fs/smb/client/file.c|1753| <<cifs_posix_lock_set>> rc = posix_lock_file(file, flock, NULL);
+ */
 int posix_lock_file(struct file *filp, struct file_lock *fl,
 			struct file_lock *conflock)
 {
@@ -2072,6 +2149,10 @@ int fcntl_setlease(unsigned int fd, struct file *filp, int arg)
  *
  * Apply a FLOCK style lock request to an inode.
  */
+/*
+ * called by:
+ *   - fs/locks.c|2107| <<locks_lock_inode_wait>> res = flock_lock_inode_wait(inode, fl);
+ */
 static int flock_lock_inode_wait(struct inode *inode, struct file_lock *fl)
 {
 	int error;
@@ -2096,6 +2177,15 @@ static int flock_lock_inode_wait(struct inode *inode, struct file_lock *fl)
  *
  * Apply a POSIX or FLOCK style lock request to an inode.
  */
+/*
+ * called by:
+ *   - fs/nfs/nfs4proc.c|7066| <<nfs4_locku_done>> locks_lock_inode_wait(calldata->lsp->ls_state->inode, &calldata->fl);
+ *   - fs/nfs/nfs4proc.c|7194| <<nfs4_proc_unlck>> if (locks_lock_inode_wait(inode, request) == -ENOENT) {
+ *   - fs/nfs/nfs4proc.c|7334| <<nfs4_lock_done>> if (locks_lock_inode_wait(lsp->ls_state->inode, &data->fl) < 0)
+ *   - fs/nfs/nfs4proc.c|7545| <<_nfs4_proc_setlk>> status = locks_lock_inode_wait(state->inode, request);
+ *   - fs/nfs/nfs4proc.c|7554| <<_nfs4_proc_setlk>> status = locks_lock_inode_wait(state->inode, request);
+ *   - include/linux/filelock.h|417| <<locks_lock_file_wait>> return locks_lock_inode_wait(file_inode(filp), fl);
+ */
 int locks_lock_inode_wait(struct inode *inode, struct file_lock *fl)
 {
 	int res = 0;
@@ -2347,6 +2437,22 @@ int fcntl_getlk(struct file *filp, unsigned int cmd, struct flock *flock)
  * ->lm_grant() before returning to the caller with a FILE_LOCK_DEFERRED
  * return code.
  */
+/*
+ * called by:
+ *   - fs/lockd/svclock.c|567| <<nlmsvc_lock>> error = vfs_lock_file(file->f_file[mode], F_SETLK, &lock->fl, NULL);
+ *   - fs/lockd/svclock.c|688| <<nlmsvc_unlock>> error = vfs_lock_file(lock->fl.c.flc_file, F_SETLK,
+ *   - fs/lockd/svclock.c|692| <<nlmsvc_unlock>> error |= vfs_lock_file(lock->fl.c.flc_file, F_SETLK,
+ *   - fs/lockd/svclock.c|872| <<nlmsvc_grant_blocked>> error = vfs_lock_file(file->f_file[mode], F_SETLK, &lock->fl, NULL);
+ *   - fs/lockd/svclock.c|999| <<nlmsvc_grant_reply>> error = vfs_lock_file(fl->c.flc_file, F_SETLK, fl, NULL);
+ *   - fs/lockd/svcsubs.c|192| <<nlm_unlock_files>> if (lock.c.flc_file && vfs_lock_file(lock.c.flc_file, F_SETLK, &lock, NULL))
+ *   - fs/lockd/svcsubs.c|195| <<nlm_unlock_files>> if (lock.c.flc_file && vfs_lock_file(lock.c.flc_file, F_SETLK, &lock, NULL))
+ *   - fs/locks.c|2414| <<do_lock_file_wait>> error = vfs_lock_file(filp, cmd, fl, NULL);
+ *   - fs/locks.c|2676| <<locks_remove_posix>> error = vfs_lock_file(filp, F_SETLK, &lock, NULL);
+ *   - fs/nfsd/nfs4state.c|8035| <<nfsd4_lock>> err = vfs_lock_file(nf->nf_file, F_SETLK, file_lock, conflock);
+ *   - fs/nfsd/nfs4state.c|8273| <<nfsd4_locku>> err = vfs_lock_file(nf->nf_file, F_SETLK, file_lock, NULL);
+ *   - fs/smb/server/smb2pdu.c|7394| <<smb2_lock>> rc = vfs_lock_file(filp, smb_lock->cmd, flock, NULL);
+ *   - fs/smb/server/smb2pdu.c|7507| <<smb2_lock>> rc = vfs_lock_file(filp, F_SETLK, rlock, NULL);
+ */
 int vfs_lock_file(struct file *filp, unsigned int cmd, struct file_lock *fl, struct file_lock *conf)
 {
 	WARN_ON_ONCE(filp != fl->c.flc_file);
@@ -2357,6 +2463,11 @@ int vfs_lock_file(struct file *filp, unsigned int cmd, struct file_lock *fl, str
 }
 EXPORT_SYMBOL_GPL(vfs_lock_file);
 
+/*
+ * called by:
+ *   - fs/locks.c|2478| <<fcntl_setlk>> error = do_lock_file_wait(filp, cmd, file_lock);
+ *   - fs/locks.c|2600| <<fcntl_setlk64>> error = do_lock_file_wait(filp, cmd, file_lock);
+ */
 static int do_lock_file_wait(struct file *filp, unsigned int cmd,
 			     struct file_lock *fl)
 {
@@ -2399,6 +2510,12 @@ check_fmode_for_setlk(struct file_lock *fl)
 /* Apply the lock described by l to an open file descriptor.
  * This implements both the F_SETLK and F_SETLKW commands of fcntl().
  */
+/*
+ * called by:
+ *   - fs/fcntl.c|398| <<do_fcntl>> err = fcntl_setlk(fd, filp, cmd, &flock);
+ *   - fs/fcntl.c|684| <<do_compat_fcntl64>> err = fcntl_setlk(fd, f.file, convert_fcntl_cmd(cmd), &flock);
+ *   - fs/fcntl.c|693| <<do_compat_fcntl64>> err = fcntl_setlk(fd, f.file, convert_fcntl_cmd(cmd), &flock);
+ */
 int fcntl_setlk(unsigned int fd, struct file *filp, unsigned int cmd,
 		struct flock *flock)
 {
diff --git a/fs/nfs/nfs4proc.c b/fs/nfs/nfs4proc.c
index b8ffbe52b..8d171392b 100644
--- a/fs/nfs/nfs4proc.c
+++ b/fs/nfs/nfs4proc.c
@@ -7411,6 +7411,13 @@ static void nfs4_handle_setlk_error(struct nfs_server *server, struct nfs4_lock_
 	}
 }
 
+/*
+ * called by:
+ *   - fs/nfs/nfs4proc.c|7480| <<nfs4_lock_reclaim>> err = _nfs4_do_setlk(state, F_SETLK, request, NFS_LOCK_RECLAIM);
+ *   - fs/nfs/nfs4proc.c|7506| <<nfs4_lock_expired>> err = _nfs4_do_setlk(state, F_SETLK, request, NFS_LOCK_EXPIRED);
+ *   - fs/nfs/nfs4proc.c|7561| <<_nfs4_proc_setlk>> status = _nfs4_do_setlk(state, cmd, request, NFS_LOCK_NEW);
+ *   - fs/nfs/nfs4proc.c|7785| <<nfs4_lock_delegation_recall>> err = _nfs4_do_setlk(state, F_SETLK, fl, NFS_LOCK_NEW);
+ */
 static int _nfs4_do_setlk(struct nfs4_state *state, int cmd, struct file_lock *fl, int recovery_type)
 {
 	struct nfs4_lockdata *data;
diff --git a/fs/nfsd/nfs4state.c b/fs/nfsd/nfs4state.c
index a366fb1c1..51f02a24e 100644
--- a/fs/nfsd/nfs4state.c
+++ b/fs/nfsd/nfs4state.c
@@ -7864,6 +7864,52 @@ lookup_or_create_lock_state(struct nfsd4_compound_state *cstate,
 /*
  *  LOCK operation 
  */
+/*
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] do_lock_file_wait
+ * [0] fcntl_setlk
+ * [0] do_fcntl
+ * [0] __x64_sys_fcntl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * nfs的client.
+ *
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] locks_lock_inode_wait
+ * [0] nfs4_lock_done
+ * [0] rpc_exit_task
+ * [0] __rpc_execute
+ * [0] rpc_async_schedule
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * NFS的server.
+ *
+ * [0] nfsd4_lock
+ * [0] nfsd4_proc_compound
+ * [0] nfsd_dispatch
+ * [0] svc_process_common
+ * [0] svc_process
+ * [0] nfsd
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] nfsd4_lock
+ * [0] nfsd4_proc_compound
+ * [0] nfsd_dispatch
+ * [0] svc_process_common
+ * [0] svc_process
+ * [0] nfsd
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 __be32
 nfsd4_lock(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,
 	   union nfsd4_op_u *u)
diff --git a/include/kvm/arm_pmu.h b/include/kvm/arm_pmu.h
index 35d4ca4f6..eb053d60f 100644
--- a/include/kvm/arm_pmu.h
+++ b/include/kvm/arm_pmu.h
@@ -24,10 +24,33 @@ struct kvm_pmu_events {
 };
 
 struct kvm_pmu {
+	/*
+	 * 在以下使用kvm_pmu->overflow_work:
+	 *   - arch/arm64/kvm/pmu-emul.c|453| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_perf_overflow_notify_vcpu>> vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|718| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|1092| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+	 */
 	struct irq_work overflow_work;
 	struct kvm_pmu_events events;
 	struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
+	/*
+	 * 在以下使用kvm_pmu->irq_num:
+	 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+	 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+	 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+	 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+	 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+	 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+	 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+	 */
 	int irq_num;
+	/*
+	 * 在以下使用kvm_pmu->created:
+	 *   - arch/arm64/kvm/pmu-emul.c|1503| <<kvm_arm_pmu_v3_enable>> if (!vcpu->arch.pmu.created) return -EINVAL;
+	 *   - arch/arm64/kvm/pmu-emul.c|1568| <<kvm_arm_pmu_v3_init>> vcpu->arch.pmu.created = true;
+	 *   - arch/arm64/kvm/pmu-emul.c|1709| <<kvm_arm_pmu_v3_set_attr>> if (vcpu->arch.pmu.created) return -EBUSY;
+	 */
 	bool created;
 	bool irq_level;
 };
@@ -44,6 +67,14 @@ static __always_inline bool kvm_arm_support_pmu_v3(void)
 	return static_branch_likely(&kvm_arm_pmu_available);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1555| <<kvm_arm_pmu_v3_enable>> } else if (kvm_arm_pmu_irq_initialized(vcpu)) {
+ *   - arch/arm64/kvm/pmu-emul.c|1587| <<kvm_arm_pmu_v3_init>> if (!kvm_arm_pmu_irq_initialized(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1645| <<pmu_irq_is_valid>> if (!kvm_arm_pmu_irq_initialized(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1835| <<kvm_arm_pmu_v3_set_attr>> if (kvm_arm_pmu_irq_initialized(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1931| <<kvm_arm_pmu_v3_get_attr>> if (!kvm_arm_pmu_irq_initialized(vcpu))
+ */
 #define kvm_arm_pmu_irq_initialized(v)	((v)->arch.pmu.irq_num >= VGIC_NR_SGIS)
 u64 kvm_pmu_get_counter_value(struct kvm_vcpu *vcpu, u64 select_idx);
 void kvm_pmu_set_counter_value(struct kvm_vcpu *vcpu, u64 select_idx, u64 val);
@@ -76,6 +107,26 @@ void kvm_vcpu_pmu_restore_guest(struct kvm_vcpu *vcpu);
 void kvm_vcpu_pmu_restore_host(struct kvm_vcpu *vcpu);
 void kvm_vcpu_pmu_resync_el0(void);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1493| <<kvm_setup_vcpu>> if (kvm_vcpu_has_pmu(vcpu) && !kvm->arch.arm_pmu)
+ *   - arch/arm64/kvm/pmu-emul.c|302| <<kvm_pmu_get_counter_value>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|366| <<kvm_pmu_set_counter_value>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|554| <<kvm_pmu_enable_counter_mask>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|611| <<kvm_pmu_disable_counter_mask>> if (!kvm_vcpu_has_pmu(vcpu) || !val)
+ *   - arch/arm64/kvm/pmu-emul.c|701| <<kvm_pmu_update_state>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1079| <<kvm_pmu_handle_pmcr>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1292| <<kvm_pmu_set_counter_event_type>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1422| <<kvm_pmu_get_pmceid>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1808| <<kvm_arm_pmu_v3_set_attr>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1931| <<kvm_arm_pmu_v3_get_attr>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1962| <<kvm_arm_pmu_v3_has_attr>> if (kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/sys_regs.c|853| <<pmu_visibility>> if (kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/sys_regs.c|891| <<reset_pmevtyper>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/sys_regs.c|1782| <<read_sanitised_id_aa64dfr0_el1>> if (kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/sys_regs.c|1833| <<read_sanitised_id_dfr0_el1>> if (kvm_vcpu_has_pmu(vcpu))
+ */
 #define kvm_vcpu_has_pmu(vcpu)					\
 	(vcpu_has_feature(vcpu, KVM_ARM_VCPU_PMU_V3))
 
diff --git a/include/linux/filelock.h b/include/linux/filelock.h
index daee999d0..76998be05 100644
--- a/include/linux/filelock.h
+++ b/include/linux/filelock.h
@@ -412,6 +412,32 @@ locks_inode_context(const struct inode *inode)
 /* for walking lists of file_locks linked by fl_list */
 #define for_each_file_lock(_fl, _head)	list_for_each_entry(_fl, _head, c.flc_list)
 
+/*
+ * called by:
+ *   - fs/9p/vfs_file.c|131| <<v9fs_file_do_lock>> res = locks_lock_file_wait(filp, fl);
+ *   - fs/9p/vfs_file.c|214| <<v9fs_file_do_lock>> locks_lock_file_wait(filp, fl);
+ *   - fs/afs/flock.c|609| <<afs_do_setlk>> ret = locks_lock_file_wait(file, fl);
+ *   - fs/afs/flock.c|714| <<afs_do_unlk>> ret = locks_lock_file_wait(file, fl);
+ *   - fs/ceph/locks.c|233| <<try_unlock_file>> err = locks_lock_file_wait(file, fl);
+ *   - fs/ceph/locks.c|338| <<ceph_flock>> locks_lock_file_wait(file, fl);
+ *   - fs/ceph/locks.c|361| <<ceph_flock>> err = locks_lock_file_wait(file, fl);
+ *   - fs/dlm/plock.c|225| <<dlm_posix_lock>> if (locks_lock_file_wait(file, fl) < 0)
+ *   - fs/dlm/plock.c|309| <<dlm_posix_unlock>> rv = locks_lock_file_wait(file, fl);
+ *   - fs/fuse/file.c|2728| <<fuse_file_flock>> err = locks_lock_file_wait(file, fl);
+ *   - fs/gfs2/file.c|1448| <<gfs2_lock>> locks_lock_file_wait(file, fl);
+ *   - fs/gfs2/file.c|1501| <<do_flock>> locks_lock_file_wait(file, &request);
+ *   - fs/gfs2/file.c|1527| <<do_flock>> error = locks_lock_file_wait(file, fl);
+ *   - fs/gfs2/file.c|1542| <<do_unflock>> locks_lock_file_wait(file, fl);
+ *   - fs/lockd/clntproc.c|498| <<do_vfs_lock>> return locks_lock_file_wait(fl->c.flc_file, fl);
+ *   - fs/locks.c|2177| <<SYSCALL_DEFINE2(flock)>> error = locks_lock_file_wait(f.file, &fl);
+ *   - fs/nfs/file.c|782| <<do_unlk>> status = locks_lock_file_wait(filp, fl);
+ *   - fs/nfs/file.c|807| <<do_setlk>> status = locks_lock_file_wait(filp, fl);
+ *   - fs/ocfs2/locks.c|58| <<ocfs2_do_flock>> locks_lock_file_wait(file, &request);
+ *   - fs/ocfs2/locks.c|72| <<ocfs2_do_flock>> ret = locks_lock_file_wait(file, fl);
+ *   - fs/ocfs2/locks.c|89| <<ocfs2_do_funlock>> ret = locks_lock_file_wait(file, fl);
+ *   - fs/ocfs2/locks.c|108| <<ocfs2_flock>> return locks_lock_file_wait(file, fl);
+ *   - fs/smb/client/file.c|2317| <<cifs_setlk>> rc = locks_lock_file_wait(file, flock);
+ */
 static inline int locks_lock_file_wait(struct file *filp, struct file_lock *fl)
 {
 	return locks_lock_inode_wait(file_inode(filp), fl);
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1a8942277..68ba59fa8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1210,6 +1210,47 @@ struct perf_sample_data {
 		    PERF_MEM_S(TLB, NA)   |\
 		    PERF_MEM_S(LVLNUM, NA))
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/perf_event.c|853| <<alpha_perf_event_irq_handler>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/arc/kernel/perf_event.c|601| <<arc_pmu_intr>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/csky/kernel/perf_event.c|1139| <<csky_pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/loongarch/kernel/perf_event.c|506| <<pmu_handle_irq>> perf_sample_data_init(&data, 0, 0);
+ *   - arch/mips/kernel/perf_event_mipsxx.c|1598| <<mipsxx_pmu_handle_shared_irq>> perf_sample_data_init(&data, 0, 0);
+ *   - arch/powerpc/perf/core-book3s.c|2299| <<record_and_restart>> perf_sample_data_init(&data, ~0ULL, event->hw.last_period);
+ *   - arch/powerpc/perf/core-fsl-emb.c|635| <<record_and_restart>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/s390/kernel/perf_cpum_cf.c|971| <<cfdiag_push_sample>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/s390/kernel/perf_cpum_sf.c|1120| <<perf_push_sample>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/s390/kernel/perf_pai_crypto.c|465| <<paicrypt_push_sample>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/s390/kernel/perf_pai_ext.c|492| <<paiext_push_sample>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/sparc/kernel/perf_event.c|1667| <<perf_event_nmi_handler>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/x86/events/amd/core.c|1003| <<amd_pmu_v2_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/x86/events/amd/ibs.c|1061| <<perf_ibs_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/x86/events/core.c|1715| <<x86_pmu_handle_irq>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/core.c|3008| <<x86_pmu_handle_guest_pebs>> perf_sample_data_init(data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/core.c|3115| <<handle_pmi_common>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/ds.c|877| <<intel_pmu_drain_bts_buffer>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/ds.c|1745| <<setup_pebs_fixed_sample_data>> perf_sample_data_init(data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/ds.c|1928| <<setup_pebs_adaptive_sample_data>> perf_sample_data_init(data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/knc.c|253| <<knc_pmu_handle_irq>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/p4.c|1068| <<p4_pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/x86/events/zhaoxin/core.c|395| <<zhaoxin_pmu_handle_irq>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/xtensa/kernel/perf_event.c|390| <<xtensa_pmu_irq_handler>> perf_sample_data_init(&data, 0, last_period);
+ *   - drivers/perf/apple_m1_cpu_pmu.c|411| <<m1_pmu_handle_irq>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - drivers/perf/arm_pmuv3.c|899| <<armv8pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - drivers/perf/arm_v6_pmu.c|274| <<armv6pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - drivers/perf/arm_v7_pmu.c|978| <<armv7pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - drivers/perf/arm_xscale_pmu.c|183| <<xscale1pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - drivers/perf/arm_xscale_pmu.c|515| <<xscale2pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - drivers/perf/riscv_pmu_sbi.c|1021| <<pmu_sbi_ovf_handler>> perf_sample_data_init(&data, 0, hw_evt->last_period);
+ *   - kernel/events/core.c|10044| <<___perf_sw_event>> perf_sample_data_init(&data, addr, 0);
+ *   - kernel/events/core.c|10404| <<perf_tp_event>> perf_sample_data_init(&data, 0, 0);
+ *   - kernel/events/core.c|10421| <<perf_tp_event>> perf_sample_data_init(&data, 0, 0);
+ *   - kernel/events/core.c|10700| <<perf_bp_event>> perf_sample_data_init(&sample, bp->attr.bp_addr, 0);
+ *   - kernel/events/core.c|11147| <<perf_swevent_hrtimer>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - kernel/trace/bpf_trace.c|690| <<BPF_CALL_5>> perf_sample_data_init(sd, 0, 0);
+ *   - kernel/trace/bpf_trace.c|751| <<bpf_event_output>> perf_sample_data_init(sd, 0, 0);
+ */
 static inline void perf_sample_data_init(struct perf_sample_data *data,
 					 u64 addr, u64 period)
 {
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 8a6c6bbcd..57dc2a4e5 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -3012,6 +3012,16 @@ static void _perf_event_enable(struct perf_event *event)
 /*
  * See perf_event_disable();
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|541| <<kvm_pmu_enable_counter_mask>> perf_event_enable(pmc->perf_event);
+ *   - arch/riscv/kvm/vcpu_pmu.c|336| <<kvm_pmu_create_perf_event>> perf_event_enable(pmc->perf_event);
+ *   - arch/riscv/kvm/vcpu_pmu.c|553| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_enable(pmc->perf_event);
+ *   - arch/x86/kvm/pmu.c|272| <<pmc_resume_counter>> perf_event_enable(pmc->perf_event);
+ *   - kernel/events/hw_breakpoint.c|815| <<modify_user_hw_breakpoint>> perf_event_enable(bp);
+ *   - kernel/watchdog_perf.c|169| <<watchdog_hardlockup_enable>> perf_event_enable(this_cpu_read(watchdog_ev));
+ *   - kernel/watchdog_perf.c|251| <<hardlockup_detector_perf_restart>> perf_event_enable(event);
+ */
 void perf_event_enable(struct perf_event *event)
 {
 	struct perf_event_context *ctx;
@@ -5357,6 +5367,19 @@ static void put_event(struct perf_event *event)
  * object, it will not preserve its functionality. Once the last 'user'
  * gives up the object, we'll destroy the thing.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|386| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/riscv/kvm/vcpu_pmu.c|82| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1058| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1060| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+ *   - arch/x86/kvm/pmu.c|281| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|198| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+ *   - kernel/events/core.c|5487| <<perf_release>> perf_event_release_kernel(file->private_data);
+ *   - kernel/events/hw_breakpoint.c|829| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+ *   - kernel/watchdog_perf.c|208| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+ *   - kernel/watchdog_perf.c|275| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+ */
 int perf_event_release_kernel(struct perf_event *event)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -5517,6 +5540,15 @@ static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *
 	return total;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|127| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled,
+ *   - arch/riscv/kvm/vcpu_pmu.c|249| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/riscv/kvm/vcpu_pmu.c|625| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event,
+ *   - arch/x86/kvm/pmu.h|112| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event,
+ *   - include/uapi/linux/bpf.h|5852| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+ *   - tools/include/uapi/linux/bpf.h|5852| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+ */
 u64 perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)
 {
 	struct perf_event_context *ctx;
@@ -9780,6 +9812,38 @@ static int __perf_event_overflow(struct perf_event *event,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/perf_event.c|856| <<alpha_perf_event_irq_handler>> if (perf_event_overflow(event, &data, regs)) {
+ *   - arch/arc/kernel/perf_event.c|603| <<arc_pmu_intr>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/csky/kernel/perf_event.c|1142| <<csky_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/loongarch/kernel/perf_event.c|482| <<handle_associated_event>> if (perf_event_overflow(event, data, regs))
+ *   - arch/mips/kernel/perf_event_mipsxx.c|794| <<handle_associated_event>> if (perf_event_overflow(event, data, regs))
+ *   - arch/powerpc/perf/core-book3s.c|2322| <<record_and_restart>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/powerpc/perf/core-fsl-emb.c|637| <<record_and_restart>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/s390/kernel/perf_cpum_cf.c|983| <<cfdiag_push_sample>> overflow = perf_event_overflow(event, &data, &regs);
+ *   - arch/s390/kernel/perf_cpum_sf.c|1171| <<perf_push_sample>> if (perf_event_overflow(event, &data, &regs)) {
+ *   - arch/s390/kernel/perf_pai_crypto.c|484| <<paicrypt_push_sample>> overflow = perf_event_overflow(event, &data, &regs);
+ *   - arch/s390/kernel/perf_pai_ext.c|509| <<paiext_push_sample>> overflow = perf_event_overflow(event, &data, &regs);
+ *   - arch/sparc/kernel/perf_event.c|1671| <<perf_event_nmi_handler>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/amd/core.c|1011| <<amd_pmu_v2_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/amd/ibs.c|1135| <<perf_ibs_handle_irq>> throttle = perf_event_overflow(event, &data, &regs);
+ *   - arch/x86/events/core.c|1720| <<x86_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/core.c|3009| <<x86_pmu_handle_guest_pebs>> if (perf_event_overflow(event, data, regs))
+ *   - arch/x86/events/intel/core.c|3120| <<handle_pmi_common>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/ds.c|2207| <<handle_pmi_common>> if (perf_event_overflow(event, data, regs))
+ *   - arch/x86/events/intel/knc.c|255| <<knc_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/p4.c|1074| <<p4_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/zhaoxin/core.c|400| <<zhaoxin_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/xtensa/kernel/perf_event.c|391| <<xtensa_pmu_irq_handler>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/apple_m1_cpu_pmu.c|415| <<m1_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/arm_pmuv3.c|935| <<armv8pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/arm_v6_pmu.c|278| <<armv6pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/arm_v7_pmu.c|982| <<armv7pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/arm_xscale_pmu.c|187| <<xscale1pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/arm_xscale_pmu.c|519| <<xscale2pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/riscv_pmu_sbi.c|1031| <<pmu_sbi_ovf_handler>> perf_event_overflow(event, &data, regs);
+ */
 int perf_event_overflow(struct perf_event *event,
 			struct perf_sample_data *data,
 			struct pt_regs *regs)
@@ -12874,6 +12938,20 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1124| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+ *   - arch/riscv/kvm/vcpu_pmu.c|328| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(attr, -1, current, kvm_riscv_pmu_overflow, pmc);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|969| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|974| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+ *   - arch/x86/kvm/pmu.c|215| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|243| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+ *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+ *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+ *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>> return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+ *   - kernel/watchdog_perf.c|135| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+ *   - kernel/watchdog_perf.c|140| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
diff --git a/kernel/resource.c b/kernel/resource.c
index a83040fde..f451fb57c 100644
--- a/kernel/resource.c
+++ b/kernel/resource.c
@@ -615,6 +615,11 @@ static void resource_clip(struct resource *res, resource_size_t min,
  * Find empty space in the resource tree with the given range and
  * alignment constraints
  */
+/*
+ * called by:
+ *   - kernel/resource.c|696| <<find_resource_space>> return __find_resource_space(root, NULL, new, size, constraint);
+ *   - kernel/resource.c|720| <<reallocate_resource>> if ((err = __find_resource_space(root, old, &new, newsize, constraint)))
+ */
 static int __find_resource_space(struct resource *root, struct resource *old,
 				 struct resource *new, resource_size_t size,
 				 struct resource_constraint *constraint)
@@ -689,6 +694,11 @@ next:		if (!this || this->end == root->end)
  * * %0		- if successful, @new members start, end, and flags are altered.
  * * %-EBUSY	- if no empty space was found.
  */
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|1015| <<pbus_upstream_space_available>> if (find_resource_space(r, &gap, size, &constraint) == 0) {
+ *   - kernel/resource.c|783| <<allocate_resource>> err = find_resource_space(root, new, size, &constraint);
+ */
 int find_resource_space(struct resource *root, struct resource *new,
 			resource_size_t size,
 			struct resource_constraint *constraint)
@@ -746,6 +756,36 @@ static int reallocate_resource(struct resource *root, struct resource *old,
 }
 
 
+/*
+ * called by:
+ *   - 2 arch/arm/mach-footbridge/dc21285.c|276| <<dc21285_setup>> allocate_resource(&iomem_resource, &res[1], 0x20000000,
+ *   - arch/arm/mach-footbridge/dc21285.c|278| <<dc21285_setup>> allocate_resource(&iomem_resource, &res[0], 0x40000000,
+ *   - arch/m68k/amiga/chipram.c|81| <<amiga_chip_alloc_res>> error = allocate_resource(&chipram_res, res, size, 0, UINT_MAX, 
+ *   - arch/m68k/atari/stram.c|169| <<atari_stram_alloc>> error = allocate_resource(&stram_pool, res, size, 0, UINT_MAX,
+ *   - arch/mips/txx9/generic/pci.c|160| <<txx9_alloc_pci_controller>> if (allocate_resource(&iomem_resource,
+ *   - arch/mips/txx9/generic/pci.c|182| <<txx9_alloc_pci_controller>> if (allocate_resource(&iomem_resource,
+ *   - arch/sparc/kernel/ioport.c|209| <<_sparc_ioremap>> if (allocate_resource(&sparc_iomap, res,
+ *   - arch/sparc/kernel/ioport.c|246| <<sparc_dma_alloc_resource>> if (allocate_resource(&_sparc_dvma, res, len, _sparc_dvma.start,
+ *   - drivers/gpu/drm/gma500/gtt.c|37| <<psb_gtt_allocate_resource>> ret = allocate_resource(root, res, size, start, end, align, NULL, NULL);
+ *   - 11 drivers/input/joystick/iforce/iforce-ff.c|25| <<make_magnitude_modifier>> if (allocate_resource(&(iforce->device_memory), mod_chunk, 2,
+ *   - drivers/input/joystick/iforce/iforce-ff.c|58| <<make_period_modifier>> if (allocate_resource(&(iforce->device_memory), mod_chunk, 0x0c,
+ *   - drivers/input/joystick/iforce/iforce-ff.c|98| <<make_envelope_modifier>> if (allocate_resource(&(iforce->device_memory), mod_chunk, 0x0e,
+ *   - drivers/input/joystick/iforce/iforce-ff.c|135| <<make_condition_modifier>> if (allocate_resource(&(iforce->device_memory), mod_chunk, 8,
+ *   - drivers/memory/omap-gpmc.c|1000| <<gpmc_cs_request>> r = allocate_resource(&gpmc_mem_root, res, size, 0, ~0,
+ *   - drivers/mtd/maps/scx200_docflash.c|143| <<init_scx200_docflash>> if (allocate_resource(&iomem_resource, &docmem,
+ *   - drivers/parisc/ccio-dma.c|1454| <<ccio_allocate_resource>> !allocate_resource(parent, res, size, min, max, align, NULL, NULL))
+ *   - drivers/parisc/ccio-dma.c|1458| <<ccio_allocate_resource>> !allocate_resource(parent + 1, res, size, min, max, align,
+ *   - 19 drivers/parisc/ccio-dma.c|1478| <<ccio_allocate_resource>> return allocate_resource(parent, res, size, min, max, align, NULL,NULL);
+ *   - drivers/parisc/iommu.h|49| <<ccio_allocate_resource>> allocate_resource(&iomem_resource, res, size, min, max, \
+ *   - drivers/pci/bus.c|231| <<pci_bus_alloc_from_region>> ret = allocate_resource(r, res, size, min_used, max,
+ *   - drivers/pcmcia/rsrc_iodyn.c|74| <<__iodyn_find_io_region>> ret = allocate_resource(&ioport_resource, res, num, min, ~0UL,
+ *   - drivers/pcmcia/rsrc_nonstatic.c|706| <<__nonstatic_find_io_region>> ret = allocate_resource(&ioport_resource, res, num, min, ~0UL,
+ *   - drivers/pcmcia/rsrc_nonstatic.c|842| <<nonstatic_find_mem_region>> ret = allocate_resource(&iomem_resource,
+ *   - drivers/pcmcia/yenta_socket.c|660| <<yenta_search_one_res>> if (allocate_resource(root, res, size, start, end, align,
+ *   - drivers/watchdog/via_wdt.c|175| <<wdt_probe>> if (allocate_resource(&iomem_resource, &wdt_res, VIA_WDT_MMIO_LEN,
+ *   - drivers/xen/balloon.c|248| <<additional_memory_resource>> ret = allocate_resource(&iomem_resource, res,
+ *   - drivers/xen/unpopulated-alloc.c|52| <<fill_list>> ret = allocate_resource(target_resource, res,
+ */
 /**
  * allocate_resource - allocate empty slot in the resource tree given range & alignment.
  * 	The resource will be reallocated with a new size if it was already allocated
@@ -1972,6 +2012,12 @@ struct resource *devm_request_free_mem_region(struct device *dev,
 }
 EXPORT_SYMBOL_GPL(devm_request_free_mem_region);
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_hv_uvmem.c|1177| <<kvmppc_uvmem_init>> res = request_free_mem_region(&iomem_resource, size, "kvmppc_uvmem");
+ *   - drivers/gpu/drm/nouveau/nouveau_dmem.c|243| <<nouveau_dmem_chunk_alloc>> res = request_free_mem_region(&iomem_resource, DMEM_CHUNK_SIZE,
+ *   - lib/test_hmm.c|510| <<dmirror_allocate_chunk>> res = request_free_mem_region(&iomem_resource, DEVMEM_CHUNK_SIZE,
+ */
 struct resource *request_free_mem_region(struct resource *base,
 		unsigned long size, const char *name)
 {
-- 
2.39.3 (Apple Git-146)

