From e3e37ac1ddefd80f927763918aa6f87f8d73abf2 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 29 Oct 2024 16:47:15 -0700
Subject: [PATCH 1/1] linux-v6.11

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/arm_pmuv3.h   |   4 +
 arch/arm64/include/asm/cputype.h     |   5 +
 arch/arm64/include/asm/kvm_host.h    | 101 +++
 arch/arm64/kernel/acpi.c             |   6 +
 arch/arm64/kvm/arm.c                 |  72 ++
 arch/arm64/kvm/guest.c               |   4 +
 arch/arm64/kvm/pmu-emul.c            | 973 +++++++++++++++++++++++++++
 arch/arm64/kvm/sys_regs.c            |  63 ++
 arch/arm64/kvm/vgic/vgic.c           |  13 +
 arch/x86/events/amd/core.c           |   5 +
 arch/x86/events/core.c               |   9 +
 arch/x86/events/perf_event.h         |  10 +
 arch/x86/include/asm/kvm_host.h      | 155 +++++
 arch/x86/include/uapi/asm/kvm.h      |  18 +
 arch/x86/include/uapi/asm/kvm_para.h |   4 +
 arch/x86/kernel/apic/vector.c        |  19 +
 arch/x86/kernel/kvm.c                |  15 +
 arch/x86/kernel/nmi.c                |   7 +
 arch/x86/kvm/pmu.c                   | 167 +++++
 arch/x86/kvm/pmu.h                   |  60 ++
 arch/x86/kvm/vmx/pmu_intel.c         |   4 +
 arch/x86/kvm/x86.c                   | 101 +++
 drivers/block/xen-blkfront.c         |   9 +
 drivers/iommu/iommufd/pages.c        |   6 +
 drivers/pci/bus.c                    |  46 ++
 drivers/pci/setup-bus.c              |   4 +
 drivers/pci/setup-res.c              |  30 +
 drivers/perf/arm_pmu.c               |  55 ++
 drivers/perf/arm_pmu_acpi.c          |   4 +
 drivers/perf/arm_pmuv3.c             | 193 +++++-
 drivers/vfio/vfio_iommu_type1.c      |  40 ++
 fs/locks.c                           | 117 ++++
 fs/nfs/nfs4proc.c                    |   7 +
 fs/nfsd/nfs4state.c                  |  46 ++
 include/kvm/arm_pmu.h                |  51 ++
 include/linux/filelock.h             |  26 +
 include/linux/perf_event.h           |  41 ++
 kernel/events/core.c                 |  82 +++
 kernel/irq/manage.c                  |   3 +
 kernel/irq/matrix.c                  | 191 ++++++
 kernel/irq/msi.c                     |   3 +
 kernel/resource.c                    |  46 ++
 mm/util.c                            |  28 +
 43 files changed, 2842 insertions(+), 1 deletion(-)

diff --git a/arch/arm64/include/asm/arm_pmuv3.h b/arch/arm64/include/asm/arm_pmuv3.h
index a4697a0b6..41daa4e6f 100644
--- a/arch/arm64/include/asm/arm_pmuv3.h
+++ b/arch/arm64/include/asm/arm_pmuv3.h
@@ -91,6 +91,10 @@ static inline void write_pmintenclr(u32 val)
 	write_sysreg(val, pmintenclr_el1);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|662| <<armv8pmu_write_event_type>> write_pmccfiltr(hwc->config_base);
+ */
 static inline void write_pmccfiltr(u64 val)
 {
 	write_sysreg(val, pmccfiltr_el0);
diff --git a/arch/arm64/include/asm/cputype.h b/arch/arm64/include/asm/cputype.h
index 5fd7caea4..7d9227655 100644
--- a/arch/arm64/include/asm/cputype.h
+++ b/arch/arm64/include/asm/cputype.h
@@ -15,6 +15,11 @@
 #define MPIDR_LEVEL_BITS	(1 << MPIDR_LEVEL_BITS_SHIFT)
 #define MPIDR_LEVEL_MASK	((1 << MPIDR_LEVEL_BITS) - 1)
 
+/*
+ * MPIDR_LEVEL_SHIFT(0) = (((1 << 0) >> 1) << 3) = 0
+ * MPIDR_LEVEL_SHIFT(1) = (((1 << 1) >> 1) << 3) = 8
+ * MPIDR_LEVEL_SHIFT(2) = (((1 << 2) >> 1) << 3) = 16 (0x10)
+ */
 #define MPIDR_LEVEL_SHIFT(level) \
 	(((1 << level) >> 1) << MPIDR_LEVEL_BITS_SHIFT)
 
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index a33f5996c..e50f11ff6 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -44,10 +44,26 @@
 
 #define KVM_REQ_SLEEP \
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_IRQ_PENDING:
+ *   - arch/arm64/kvm/arm.c|1009| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/arm.c|1348| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|865| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|351| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|401| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|717| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|998| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ */
 #define KVM_REQ_IRQ_PENDING	KVM_ARCH_REQ(1)
 #define KVM_REQ_VCPU_RESET	KVM_ARCH_REQ(2)
 #define KVM_REQ_RECORD_STEAL	KVM_ARCH_REQ(3)
 #define KVM_REQ_RELOAD_GICv4	KVM_ARCH_REQ(4)
+/*
+ * 在以下使用KVM_REQ_RELOAD_PMU:
+ *   - arch/arm64/kvm/arm.c|1022| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu)) kvm_vcpu_reload_pmu(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|1413| <<kvm_arm_pmu_v3_enable>> kvm_make_request(KVM_REQ_RELOAD_PMU, vcpu);
+ */
 #define KVM_REQ_RELOAD_PMU	KVM_ARCH_REQ(5)
 #define KVM_REQ_SUSPEND		KVM_ARCH_REQ(6)
 #define KVM_REQ_RESYNC_PMU_EL0	KVM_ARCH_REQ(7)
@@ -328,6 +344,23 @@ struct kvm_arch {
 #define KVM_ARCH_FLAG_FGU_INITIALIZED			8
 	unsigned long flags;
 
+	/*
+	 * #define KVM_ARM_VCPU_POWER_OFF          0 // CPU is started in OFF state
+	 * #define KVM_ARM_VCPU_EL1_32BIT          1 // CPU running a 32bit VM
+	 * #define KVM_ARM_VCPU_PSCI_0_2           2 // CPU uses PSCI v0.2
+	 * #define KVM_ARM_VCPU_PMU_V3             3 // Support guest PMUv3
+	 * #define KVM_ARM_VCPU_SVE                4 // enable SVE for this CPU
+	 * #define KVM_ARM_VCPU_PTRAUTH_ADDRESS    5 // VCPU uses address authentication
+	 * #define KVM_ARM_VCPU_PTRAUTH_GENERIC    6 // VCPU uses generic authentication
+	 * #define KVM_ARM_VCPU_HAS_EL2            7 // Support nested virtualization
+	 *
+	 * 在以下使用kvm_arch->vcpu_features:
+	 *   - arch/arm64/include/asm/kvm_host.h|348| <<global>> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/include/asm/kvm_host.h|1427| <<__vcpu_has_feature>> return test_bit(feature, ka->vcpu_features);
+	 *   - arch/arm64/kvm/arm.c|212| <<kvm_arch_init_vm>> bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1480| <<kvm_vcpu_init_changed>> return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1516| <<__kvm_vcpu_set_target>> bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 */
 	/* VM-wide vCPU feature set */
 	DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
 
@@ -338,11 +371,42 @@ struct kvm_arch {
 	 * VM-wide PMU filter, implemented as a bitmap and big enough for
 	 * up to 2^10 events (ARMv8.0) or 2^16 events (ARMv8.1+).
 	 */
+	/*
+	 * 在以下使用kvm_arch->pmu_filter:
+	 *   - arch/arm64/kvm/arm.c|256| <<kvm_arch_destroy_vm>> bitmap_free(kvm->arch.pmu_filter);
+	 *   - arch/arm64/kvm/pmu-emul.c|1094| <<kvm_pmu_create_perf_event>> if (vcpu->kvm->arch.pmu_filter && !test_bit(eventsel, vcpu->kvm->arch.pmu_filter))
+	 *   - arch/arm64/kvm/pmu-emul.c|1095| <<kvm_pmu_create_perf_event>> !test_bit(eventsel, vcpu->kvm->arch.pmu_filter))
+	 *   - arch/arm64/kvm/pmu-emul.c|1234| <<kvm_pmu_get_pmceid>> unsigned long *bmap = vcpu->kvm->arch.pmu_filter;
+	 *   - arch/arm64/kvm/pmu-emul.c|1443| <<kvm_arm_pmu_v3_set_pmu>> (kvm->arch.pmu_filter && kvm->arch.arm_pmu != arm_pmu)) {
+	 *   - arch/arm64/kvm/pmu-emul.c|1522| <<kvm_arm_pmu_v3_set_attr>> if (!kvm->arch.pmu_filter) {
+	 *   - arch/arm64/kvm/pmu-emul.c|1523| <<kvm_arm_pmu_v3_set_attr>> kvm->arch.pmu_filter = bitmap_alloc(nr_events, GFP_KERNEL_ACCOUNT);
+	 *   - arch/arm64/kvm/pmu-emul.c|1524| <<kvm_arm_pmu_v3_set_attr>> if (!kvm->arch.pmu_filter)
+	 *   - arch/arm64/kvm/pmu-emul.c|1534| <<kvm_arm_pmu_v3_set_attr>> bitmap_zero(kvm->arch.pmu_filter, nr_events);
+	 *   - arch/arm64/kvm/pmu-emul.c|1536| <<kvm_arm_pmu_v3_set_attr>> bitmap_fill(kvm->arch.pmu_filter, nr_events);
+	 *   - arch/arm64/kvm/pmu-emul.c|1540| <<kvm_arm_pmu_v3_set_attr>> bitmap_set(kvm->arch.pmu_filter, filter.base_event, filter.nevents);
+	 *   - arch/arm64/kvm/pmu-emul.c|1542| <<kvm_arm_pmu_v3_set_attr>> bitmap_clear(kvm->arch.pmu_filter, filter.base_event, filter.nevents);
+	 */
 	unsigned long *pmu_filter;
 	struct arm_pmu *arm_pmu;
 
 	cpumask_var_t supported_cpus;
 
+	/*
+	 * 在以下使用kvm_arch->pmcr_n:
+	 *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+	 *   - arch/arm64/kvm/pmu-emul.c|1969| <<kvm_vcpu_read_pmcr>> return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
+	 *   - arch/arm64/kvm/sys_regs.c|862| <<reset_pmu_reg>> u8 n = vcpu->kvm->arch.pmcr_n;
+	 *   - arch/arm64/kvm/sys_regs.c|1321| <<set_pmcr>> kvm->arch.pmcr_n = new_n;
+	 *
+	 * PMCR寄存器:
+	 * PMCR_EL0, Performance Monitors Control Register
+	 *
+	 * Provides details of the Performance Monitors implementation, including the
+	 * number of counters implemented, and configures and controls the counters.
+	 *
+	 * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+	 * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+	 */
 	/* PMCR_EL0.N value for the guest */
 	u8 pmcr_n;
 
@@ -1364,6 +1428,22 @@ bool kvm_arm_vcpu_is_finalized(struct kvm_vcpu *vcpu);
 
 #define kvm_arm_vcpu_sve_finalized(vcpu) vcpu_get_flag(vcpu, VCPU_SVE_FINALIZED)
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1482| <<kvm_vcpu_init_check_features>> if (kvm_has_mte(vcpu->kvm))
+ *   - arch/arm64/kvm/guest.c|1032| <<kvm_vm_ioctl_mte_copy_tags>> if (!kvm_has_mte(kvm))
+ *   - arch/arm64/kvm/hyp/exception.c|134| <<enter_exception64>> if (kvm_has_mte(kern_hyp_va(vcpu->kvm)))
+ *   - arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h|44| <<ctxt_has_mte>> return kvm_has_mte(kern_hyp_va(vcpu->kvm));
+ *   - arch/arm64/kvm/hyp/nvhe/sys_regs.c|102| <<get_pvm_id_aa64pfr1>> if (!kvm_has_mte(kvm))
+ *   - arch/arm64/kvm/mmu.c|1405| <<sanitise_mte_tags>> if (!kvm_has_mte(kvm))
+ *   - arch/arm64/kvm/mmu.c|1648| <<user_mem_abort>> if (!fault_is_perm && !device && kvm_has_mte(kvm)) {
+ *   - arch/arm64/kvm/mmu.c|2148| <<kvm_arch_prepare_memory_region>> if (kvm_has_mte(kvm) && !kvm_vma_mte_allowed(vma)) {
+ *   - arch/arm64/kvm/sys_regs.c|349| <<access_dcgsw>> if (!kvm_has_mte(vcpu->kvm)) {
+ *   - arch/arm64/kvm/sys_regs.c|1601| <<__kvm_read_sanitised_id_reg>> if (!kvm_has_mte(vcpu->kvm))
+ *   - arch/arm64/kvm/sys_regs.c|2037| <<reset_clidr>> if (kvm_has_mte(vcpu->kvm))
+ *   - arch/arm64/kvm/sys_regs.c|2090| <<mte_visibility>> if (kvm_has_mte(vcpu->kvm))
+ *   - arch/arm64/kvm/sys_regs.c|4599| <<vcpu_set_hcr>> if (kvm_has_mte(vcpu->kvm))
+ */
 #define kvm_has_mte(kvm)					\
 	(system_supports_mte() &&				\
 	 test_bit(KVM_ARCH_FLAG_MTE_ENABLED, &(kvm)->arch.flags))
@@ -1375,8 +1455,29 @@ bool kvm_arm_vcpu_is_finalized(struct kvm_vcpu *vcpu);
 #define kvm_vm_has_ran_once(kvm)					\
 	(test_bit(KVM_ARCH_FLAG_HAS_RAN_ONCE, &(kvm)->arch.flags))
 
+/*
+ * #define KVM_ARM_VCPU_POWER_OFF          0 // CPU is started in OFF state
+ * #define KVM_ARM_VCPU_EL1_32BIT          1 // CPU running a 32bit VM
+ * #define KVM_ARM_VCPU_PSCI_0_2           2 // CPU uses PSCI v0.2
+ * #define KVM_ARM_VCPU_PMU_V3             3 // Support guest PMUv3
+ * #define KVM_ARM_VCPU_SVE                4 // enable SVE for this CPU
+ * #define KVM_ARM_VCPU_PTRAUTH_ADDRESS    5 // VCPU uses address authentication
+ * #define KVM_ARM_VCPU_PTRAUTH_GENERIC    6 // VCPU uses generic authentication
+ * #define KVM_ARM_VCPU_HAS_EL2            7 // Support nested virtualization
+ */
 static inline bool __vcpu_has_feature(const struct kvm_arch *ka, int feature)
 {
+	/*
+	 * 在以下使用kvm_arch->vcpu_features:
+	 *   - arch/arm64/include/asm/kvm_host.h|348| <<global>> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/include/asm/kvm_host.h|1427| <<__vcpu_has_feature>> return test_bit(feature, ka->vcpu_features);
+	 *   - arch/arm64/kvm/arm.c|212| <<kvm_arch_init_vm>> bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1480| <<kvm_vcpu_init_changed>> return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1516| <<__kvm_vcpu_set_target>> bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *
+	 * struct kvm_arch:
+	 * -> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES)
+	 */
 	return test_bit(feature, ka->vcpu_features);
 }
 
diff --git a/arch/arm64/kernel/acpi.c b/arch/arm64/kernel/acpi.c
index e6f66491f..25e992738 100644
--- a/arch/arm64/kernel/acpi.c
+++ b/arch/arm64/kernel/acpi.c
@@ -37,6 +37,12 @@
 #include <asm/smp_plat.h>
 
 int acpi_noirq = 1;		/* skip ACPI IRQ initialization */
+/*
+ * 在以下设置arm64的acpi_disabled:
+ *   - arch/arm64/kernel/acpi.c|40| <<global>> int acpi_disabled = 1;
+ *   - arch/arm64/include/asm/acpi.h|85| <<disable_acpi>> acpi_disabled = 1;
+ *   - arch/arm64/include/asm/acpi.h|92| <<enable_acpi>> acpi_disabled = 0;
+ */
 int acpi_disabled = 1;
 EXPORT_SYMBOL(acpi_disabled);
 
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index 9bef76383..3b11c0290 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -209,6 +209,23 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	kvm_arm_init_hypercalls(kvm);
 
+	/*
+	 * #define KVM_ARM_VCPU_POWER_OFF          0 // CPU is started in OFF state
+	 * #define KVM_ARM_VCPU_EL1_32BIT          1 // CPU running a 32bit VM
+	 * #define KVM_ARM_VCPU_PSCI_0_2           2 // CPU uses PSCI v0.2
+	 * #define KVM_ARM_VCPU_PMU_V3             3 // Support guest PMUv3
+	 * #define KVM_ARM_VCPU_SVE                4 // enable SVE for this CPU
+	 * #define KVM_ARM_VCPU_PTRAUTH_ADDRESS    5 // VCPU uses address authentication
+	 * #define KVM_ARM_VCPU_PTRAUTH_GENERIC    6 // VCPU uses generic authentication
+	 * #define KVM_ARM_VCPU_HAS_EL2            7 // Support nested virtualization
+	 *
+	 * 在以下使用kvm_arch->vcpu_features:
+	 *   - arch/arm64/include/asm/kvm_host.h|348| <<global>> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/include/asm/kvm_host.h|1427| <<__vcpu_has_feature>> return test_bit(feature, ka->vcpu_features);
+	 *   - arch/arm64/kvm/arm.c|212| <<kvm_arch_init_vm>> bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1480| <<kvm_vcpu_init_changed>> return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1516| <<__kvm_vcpu_set_target>> bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 */
 	bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
 
 	return 0;
@@ -1019,6 +1036,11 @@ static int check_vcpu_requests(struct kvm_vcpu *vcpu)
 			preempt_enable();
 		}
 
+		/*
+		 * 在以下使用KVM_REQ_RELOAD_PMU:
+		 *   - arch/arm64/kvm/arm.c|1022| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu)) kvm_vcpu_reload_pmu(vcpu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1413| <<kvm_arm_pmu_v3_enable>> kvm_make_request(KVM_REQ_RELOAD_PMU, vcpu);
+		 */
 		if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu))
 			kvm_vcpu_reload_pmu(vcpu);
 
@@ -1472,6 +1494,23 @@ static bool kvm_vcpu_init_changed(struct kvm_vcpu *vcpu,
 {
 	unsigned long features = init->features[0];
 
+	/*
+	 * #define KVM_ARM_VCPU_POWER_OFF          0 // CPU is started in OFF state
+	 * #define KVM_ARM_VCPU_EL1_32BIT          1 // CPU running a 32bit VM
+	 * #define KVM_ARM_VCPU_PSCI_0_2           2 // CPU uses PSCI v0.2
+	 * #define KVM_ARM_VCPU_PMU_V3             3 // Support guest PMUv3
+	 * #define KVM_ARM_VCPU_SVE                4 // enable SVE for this CPU
+	 * #define KVM_ARM_VCPU_PTRAUTH_ADDRESS    5 // VCPU uses address authentication
+	 * #define KVM_ARM_VCPU_PTRAUTH_GENERIC    6 // VCPU uses generic authentication
+	 * #define KVM_ARM_VCPU_HAS_EL2            7 // Support nested virtualization
+	 *
+	 * 在以下使用kvm_arch->vcpu_features:
+	 *   - arch/arm64/include/asm/kvm_host.h|348| <<global>> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/include/asm/kvm_host.h|1427| <<__vcpu_has_feature>> return test_bit(feature, ka->vcpu_features);
+	 *   - arch/arm64/kvm/arm.c|212| <<kvm_arch_init_vm>> bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1480| <<kvm_vcpu_init_changed>> return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1516| <<__kvm_vcpu_set_target>> bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 */
 	return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features,
 			     KVM_VCPU_MAX_FEATURES);
 }
@@ -1495,6 +1534,10 @@ static int kvm_setup_vcpu(struct kvm_vcpu *vcpu)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1547| <<kvm_vcpu_set_target>> return __kvm_vcpu_set_target(vcpu, init);
+ */
 static int __kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
 				 const struct kvm_vcpu_init *init)
 {
@@ -1508,6 +1551,23 @@ static int __kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
 	    kvm_vcpu_init_changed(vcpu, init))
 		goto out_unlock;
 
+	/*
+	 * #define KVM_ARM_VCPU_POWER_OFF          0 // CPU is started in OFF state
+	 * #define KVM_ARM_VCPU_EL1_32BIT          1 // CPU running a 32bit VM
+	 * #define KVM_ARM_VCPU_PSCI_0_2           2 // CPU uses PSCI v0.2
+	 * #define KVM_ARM_VCPU_PMU_V3             3 // Support guest PMUv3
+	 * #define KVM_ARM_VCPU_SVE                4 // enable SVE for this CPU
+	 * #define KVM_ARM_VCPU_PTRAUTH_ADDRESS    5 // VCPU uses address authentication
+	 * #define KVM_ARM_VCPU_PTRAUTH_GENERIC    6 // VCPU uses generic authentication
+	 * #define KVM_ARM_VCPU_HAS_EL2            7 // Support nested virtualization
+	 *
+	 * 在以下使用kvm_arch->vcpu_features:
+	 *   - arch/arm64/include/asm/kvm_host.h|348| <<global>> DECLARE_BITMAP(vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/include/asm/kvm_host.h|1427| <<__vcpu_has_feature>> return test_bit(feature, ka->vcpu_features);
+	 *   - arch/arm64/kvm/arm.c|212| <<kvm_arch_init_vm>> bitmap_zero(kvm->arch.vcpu_features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1480| <<kvm_vcpu_init_changed>> return !bitmap_equal(vcpu->kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 *   - arch/arm64/kvm/arm.c|1516| <<__kvm_vcpu_set_target>> bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
+	 */
 	bitmap_copy(kvm->arch.vcpu_features, &features, KVM_VCPU_MAX_FEATURES);
 
 	ret = kvm_setup_vcpu(vcpu);
@@ -1525,6 +1585,10 @@ static int __kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * 处理KVM_ARM_VCPU_INIT:
+ *   - arch/arm64/kvm/arm.c|1572| <<kvm_arch_vcpu_ioctl_vcpu_init(KVM_ARM_VCPU_INIT)>> ret = kvm_vcpu_set_target(vcpu, init);
+ */
 static int kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
 			       const struct kvm_vcpu_init *init)
 {
@@ -1602,6 +1666,10 @@ static int kvm_arch_vcpu_ioctl_vcpu_init(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * 处理KVM_SET_DEVICE_ATTR:
+ *   - arch/arm64/kvm/arm.c|1753| <<kvm_arch_vcpu_ioctl(KVM_SET_DEVICE_ATTR)>> r = kvm_arm_vcpu_set_attr(vcpu, &attr);
+ */
 static int kvm_arm_vcpu_set_attr(struct kvm_vcpu *vcpu,
 				 struct kvm_device_attr *attr)
 {
@@ -1670,6 +1738,10 @@ static int kvm_arm_vcpu_set_events(struct kvm_vcpu *vcpu,
 	return __kvm_arm_vcpu_set_events(vcpu, events);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4654| <<kvm_vcpu_ioctl(default)>> r = kvm_arch_vcpu_ioctl(filp, ioctl, arg);
+ */
 long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg)
 {
diff --git a/arch/arm64/kvm/guest.c b/arch/arm64/kvm/guest.c
index 11098eb7e..f04eb67b9 100644
--- a/arch/arm64/kvm/guest.c
+++ b/arch/arm64/kvm/guest.c
@@ -944,6 +944,10 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1617| <<kvm_arm_vcpu_set_attr>> ret = kvm_arm_vcpu_arch_set_attr(vcpu, attr);
+ */
 int kvm_arm_vcpu_arch_set_attr(struct kvm_vcpu *vcpu,
 			       struct kvm_device_attr *attr)
 {
diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
index 82a2a0032..3bd71504f 100644
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@ -15,26 +15,116 @@
 #include <kvm/arm_pmu.h>
 #include <kvm/arm_vgic.h>
 
+/*
+ * 1134 #define read_sysreg(r) ({                                       \
+ * 1135         u64 __val;                                              \
+ * 1136         asm volatile("mrs %0, " __stringify(r) : "=r" (__val)); \
+ * 1137         __val;                                                  \
+ * 1138 })
+ * 1139
+ *
+ * 读用的是MRS:
+ * Move System Register to general-purpose register allows the PE to read an
+ * AArch64 System register into a general-purpose register.
+ *
+ *
+ * 1144 #define write_sysreg(v, r) do {                                 \
+ * 1145         u64 __val = (u64)(v);                                   \
+ * 1146         asm volatile("msr " __stringify(r) ", %x0"              \
+ * 1147                      : : "rZ" (__val));                         \
+ * 1148 } while (0)
+ *
+ * 写用的是MSR:
+ * Move general-purpose register to System Register allows the PE to write an
+ * AArch64 System register from a general-purpose register.
+ */
+
 #define PERF_ATTR_CFG1_COUNTER_64BIT	BIT(0)
 
+/*
+ * 在以下使用kvm_arm_pmu_available:
+ *   - arch/arm64/kernel/image-vars.h|114| <<global>> KVM_NVHE_ALIAS(kvm_arm_pmu_available);
+ *   - arch/arm64/kvm/pmu-emul.c|20| <<global>> DEFINE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
+ *   - include/kvm/arm_pmu.h|40| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
+ *   - arch/arm64/kvm/pmu-emul.c|714| <<kvm_host_pmu_init>> static_branch_enable(&kvm_arm_pmu_available);
+ *   - include/kvm/arm_pmu.h|44| <<kvm_arm_support_pmu_v3>> return static_branch_likely(&kvm_arm_pmu_available);
+ */
 DEFINE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
 
+/*
+ * 在以下使用LIST_HEAD(arm_pmus):
+ *   - arch/arm64/kvm/pmu-emul.c|22| <<global>> static LIST_HEAD(arm_pmus);
+ *   - arch/arm64/kvm/pmu-emul.c|711| <<kvm_host_pmu_init>> list_add_tail(&entry->entry, &arm_pmus);
+ *   - arch/arm64/kvm/pmu-emul.c|713| <<kvm_host_pmu_init>> if (list_is_singular(&arm_pmus))
+ *   - arch/arm64/kvm/pmu-emul.c|746| <<kvm_pmu_probe_armpmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+ *   - arch/arm64/kvm/pmu-emul.c|960| <<kvm_arm_pmu_v3_set_pmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+ */
 static LIST_HEAD(arm_pmus);
 static DEFINE_MUTEX(arm_pmus_lock);
 
 static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc);
 static void kvm_pmu_release_perf_event(struct kvm_pmc *pmc);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|84| <<kvm_pmc_is_64bit>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|92| <<kvm_pmc_has_64bit_overflow>> u64 val = kvm_vcpu_read_pmcr(kvm_pmc_to_vcpu(pmc));
+ *   - arch/arm64/kvm/pmu-emul.c|116| <<kvm_pmu_get_pmc_value>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|151| <<kvm_pmu_set_pmc_value>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|211| <<kvm_pmu_stop_counter>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|502| <<kvm_pmu_perf_overflow>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|587| <<kvm_pmu_counter_is_enabled>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|598| <<kvm_pmu_create_perf_event>> struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+ */
 static struct kvm_vcpu *kvm_pmc_to_vcpu(const struct kvm_pmc *pmc)
 {
+	/*
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 *       -> struct irq_work overflow_work;
+	 *       -> struct kvm_pmu_events events;
+	 *       -> struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
+	 *       -> int irq_num;
+	 *       -> bool created;
+	 *       -> bool irq_level;
+	 */
 	return container_of(pmc, struct kvm_vcpu, arch.pmu.pmc[pmc->idx]);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|146| <<kvm_pmu_get_counter_value>> return kvm_pmu_get_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx));
+ *   - arch/arm64/kvm/pmu-emul.c|187| <<kvm_pmu_set_counter_value>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx), val, false);
+ *   - arch/arm64/kvm/pmu-emul.c|251| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
+ *   - arch/arm64/kvm/pmu-emul.c|264| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+ *   - arch/arm64/kvm/pmu-emul.c|300| <<kvm_pmu_enable_counter_mask>> pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+ *   - arch/arm64/kvm/pmu-emul.c|332| <<kvm_pmu_disable_counter_mask>> pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+ *   - arch/arm64/kvm/pmu-emul.c|452| <<kvm_pmu_counter_increment>> struct kvm_pmc *pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+ *   - arch/arm64/kvm/pmu-emul.c|580| <<kvm_pmu_handle_pmcr>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, i), 0, true);
+ *   - arch/arm64/kvm/pmu-emul.c|681| <<kvm_pmu_set_counter_event_type>> struct kvm_pmc *pmc = kvm_vcpu_idx_to_pmc(vcpu, select_idx);
+ */
 static struct kvm_pmc *kvm_vcpu_idx_to_pmc(struct kvm_vcpu *vcpu, int cnt_idx)
 {
+	/*
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 *       -> struct irq_work overflow_work;
+	 *       -> struct kvm_pmu_events events;
+	 *       -> struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
+	 *       -> int irq_num;
+	 *       -> bool created;
+	 *       -> bool irq_level;
+	 */
 	return &vcpu->arch.pmu.pmc[cnt_idx];
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|59| <<kvm_pmu_event_mask>> return __kvm_pmu_event_mask(pmuver);
+ *   - arch/arm64/kvm/pmu-emul.c|1028| <<kvm_arm_pmu_v3_set_attr>> nr_events = __kvm_pmu_event_mask(pmuver) + 1;
+ */
 static u32 __kvm_pmu_event_mask(unsigned int pmuver)
 {
 	switch (pmuver) {
@@ -51,6 +141,13 @@ static u32 __kvm_pmu_event_mask(unsigned int pmuver)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|65| <<kvm_pmu_evtyper_mask>> kvm_pmu_event_mask(kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|457| <<kvm_pmu_counter_increment>> type &= kvm_pmu_event_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|612| <<kvm_pmu_create_perf_event>> eventsel = data & kvm_pmu_event_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|789| <<kvm_pmu_get_pmceid>> nr_events = kvm_pmu_event_mask(vcpu->kvm) + 1;
+ */
 static u32 kvm_pmu_event_mask(struct kvm *kvm)
 {
 	u64 dfr0 = kvm_read_vm_id_reg(kvm, SYS_ID_AA64DFR0_EL1);
@@ -59,6 +156,11 @@ static u32 kvm_pmu_event_mask(struct kvm *kvm)
 	return __kvm_pmu_event_mask(pmuver);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|688| <<kvm_pmu_set_counter_event_type>> __vcpu_sys_reg(vcpu, reg) = data & kvm_pmu_evtyper_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/sys_regs.c|888| <<reset_pmevtyper>> __vcpu_sys_reg(vcpu, r->reg) &= kvm_pmu_evtyper_mask(vcpu->kvm);
+ */
 u64 kvm_pmu_evtyper_mask(struct kvm *kvm)
 {
 	u64 mask = ARMV8_PMU_EXCLUDE_EL1 | ARMV8_PMU_EXCLUDE_EL0 |
@@ -79,38 +181,111 @@ u64 kvm_pmu_evtyper_mask(struct kvm *kvm)
  * kvm_pmc_is_64bit - determine if counter is 64bit
  * @pmc: counter context
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|130| <<kvm_pmu_get_pmc_value>> if (!kvm_pmc_is_64bit(pmc))
+ *   - arch/arm64/kvm/pmu-emul.c|463| <<kvm_pmu_counter_increment>> if (!kvm_pmc_is_64bit(pmc))
+ *   - arch/arm64/kvm/pmu-emul.c|485| <<compute_period>> if (kvm_pmc_is_64bit(pmc) && kvm_pmc_has_64bit_overflow(pmc))
+ *   - arch/arm64/kvm/pmu-emul.c|651| <<kvm_pmu_create_perf_event>> if (kvm_pmc_is_64bit(pmc))
+ */
 static bool kvm_pmc_is_64bit(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
 
+	/*
+	 * 注释:
+	 * If FEAT_PMUv3p5 is implemented and the highest Exception level is
+	 * using AArch64, the event counters are 64-bit. If FEAT_PMUv3p5 is not
+	 * implemented, the event counters are 32-bit.
+	 */
 	return (pmc->idx == ARMV8_PMU_CYCLE_IDX ||
 		kvm_has_feat(vcpu->kvm, ID_AA64DFR0_EL1, PMUVer, V3P5));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|101| <<kvm_pmu_counter_can_chain>> return (!(pmc->idx & 1) && (pmc->idx + 1) < ARMV8_PMU_CYCLE_IDX && !kvm_pmc_has_64bit_overflow(pmc));
+ *   - arch/arm64/kvm/pmu-emul.c|468| <<kvm_pmu_counter_increment>> if (kvm_pmc_has_64bit_overflow(pmc) ? reg : lower_32_bits(reg))
+ *   - arch/arm64/kvm/pmu-emul.c|485| <<compute_period>> if (kvm_pmc_is_64bit(pmc) && kvm_pmc_has_64bit_overflow(pmc))
+ */
 static bool kvm_pmc_has_64bit_overflow(struct kvm_pmc *pmc)
 {
 	u64 val = kvm_vcpu_read_pmcr(kvm_pmc_to_vcpu(pmc));
 
+	/*
+	 * 207 //
+	 * 208 // Per-CPU PMCR: config reg
+	 * 209  //
+	 * 210 #define ARMV8_PMU_PMCR_E        (1 << 0) // Enable all counters
+	 * 211 #define ARMV8_PMU_PMCR_P        (1 << 1) // Reset all counters
+	 * 212 #define ARMV8_PMU_PMCR_C        (1 << 2) // Cycle counter reset
+	 * 213 #define ARMV8_PMU_PMCR_D        (1 << 3) // CCNT counts every 64th cpu cycle
+	 * 214 #define ARMV8_PMU_PMCR_X        (1 << 4) // Export to ETM
+	 * 215 #define ARMV8_PMU_PMCR_DP       (1 << 5) // Disable CCNT if non-invasive debug
+	 * 216 #define ARMV8_PMU_PMCR_LC       (1 << 6) // Overflow on 64 bit cycle counter
+	 * 217 #define ARMV8_PMU_PMCR_LP       (1 << 7) // Long event counter enable
+	 * 218 #define ARMV8_PMU_PMCR_N        GENMASK(15, 11) // Number of counters supported
+	 */
 	return (pmc->idx < ARMV8_PMU_CYCLE_IDX && (val & ARMV8_PMU_PMCR_LP)) ||
 	       (pmc->idx == ARMV8_PMU_CYCLE_IDX && (val & ARMV8_PMU_PMCR_LC));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|474| <<kvm_pmu_counter_increment>> if (kvm_pmu_counter_can_chain(pmc))
+ *   - arch/arm64/kvm/pmu-emul.c|520| <<kvm_pmu_perf_overflow>> if (kvm_pmu_counter_can_chain(pmc))
+ */
 static bool kvm_pmu_counter_can_chain(struct kvm_pmc *pmc)
 {
 	return (!(pmc->idx & 1) && (pmc->idx + 1) < ARMV8_PMU_CYCLE_IDX &&
 		!kvm_pmc_has_64bit_overflow(pmc));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|119| <<kvm_pmu_get_pmc_value>> reg = counter_index_to_reg(pmc->idx);
+ *   - arch/arm64/kvm/pmu-emul.c|156| <<kvm_pmu_set_pmc_value>> reg = counter_index_to_reg(pmc->idx);
+ *   - arch/arm64/kvm/pmu-emul.c|219| <<kvm_pmu_stop_counter>> reg = counter_index_to_reg(pmc->idx);
+ *   - arch/arm64/kvm/pmu-emul.c|462| <<kvm_pmu_counter_increment>> reg = __vcpu_sys_reg(vcpu, counter_index_to_reg(i)) + 1;
+ *   - arch/arm64/kvm/pmu-emul.c|465| <<kvm_pmu_counter_increment>> __vcpu_sys_reg(vcpu, counter_index_to_reg(i)) = reg;
+ */
 static u32 counter_index_to_reg(u64 idx)
 {
+	/*
+	 * 418         // Performance Monitors Registers
+	 * 419         PMCR_EL0,       // Control Register
+	 * 420         PMSELR_EL0,     // Event Counter Selection Register
+	 * 421         PMEVCNTR0_EL0,  // Event Counter Register (0-30)
+	 * 422         PMEVCNTR30_EL0 = PMEVCNTR0_EL0 + 30,
+	 * 423         PMCCNTR_EL0,    // Cycle Counter Register
+	 * 424         PMEVTYPER0_EL0, // Event Type Register (0-30)
+	 * 425         PMEVTYPER30_EL0 = PMEVTYPER0_EL0 + 30,
+	 * 426         PMCCFILTR_EL0,  // Cycle Count Filter Register
+	 * 427         PMCNTENSET_EL0, // Count Enable Set Register
+	 * 428         PMINTENSET_EL1, // Interrupt Enable Set Register
+	 * 429         PMOVSSET_EL0,   // Overflow Flag Status Set Register
+	 * 430         PMUSERENR_EL0,  // User Enable Register
+	 */
 	return (idx == ARMV8_PMU_CYCLE_IDX) ? PMCCNTR_EL0 : PMEVCNTR0_EL0 + idx;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|456| <<kvm_pmu_counter_increment>> type = __vcpu_sys_reg(vcpu, counter_index_to_evtreg(i));
+ *   - arch/arm64/kvm/pmu-emul.c|605| <<kvm_pmu_create_perf_event>> reg = counter_index_to_evtreg(pmc->idx);
+ *   - arch/arm64/kvm/pmu-emul.c|687| <<kvm_pmu_set_counter_event_type>> reg = counter_index_to_evtreg(pmc->idx);
+ */
 static u32 counter_index_to_evtreg(u64 idx)
 {
 	return (idx == ARMV8_PMU_CYCLE_IDX) ? PMCCFILTR_EL0 : PMEVTYPER0_EL0 + idx;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|146| <<kvm_pmu_get_counter_value>> return kvm_pmu_get_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx));
+ *   - arch/arm64/kvm/pmu-emul.c|217| <<kvm_pmu_stop_counter>> val = kvm_pmu_get_pmc_value(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|654| <<kvm_pmu_create_perf_event>> attr.sample_period = compute_period(pmc, kvm_pmu_get_pmc_value(pmc));
+ */
 static u64 kvm_pmu_get_pmc_value(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
@@ -123,10 +298,26 @@ static u64 kvm_pmu_get_pmc_value(struct kvm_pmc *pmc)
 	 * The real counter value is equal to the value of counter register plus
 	 * the value perf event counts.
 	 */
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|127| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled,
+	 *   - arch/riscv/kvm/vcpu_pmu.c|249| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|625| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event,
+	 *   - arch/x86/kvm/pmu.h|112| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event,
+	 *   - include/uapi/linux/bpf.h|5852| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 *   - tools/include/uapi/linux/bpf.h|5852| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 */
 	if (pmc->perf_event)
 		counter += perf_event_read_value(pmc->perf_event, &enabled,
 						 &running);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|130| <<kvm_pmu_get_pmc_value>> if (!kvm_pmc_is_64bit(pmc))
+	 *   - arch/arm64/kvm/pmu-emul.c|463| <<kvm_pmu_counter_increment>> if (!kvm_pmc_is_64bit(pmc))
+	 *   - arch/arm64/kvm/pmu-emul.c|485| <<compute_period>> if (kvm_pmc_is_64bit(pmc) && kvm_pmc_has_64bit_overflow(pmc))
+	 *   - arch/arm64/kvm/pmu-emul.c|651| <<kvm_pmu_create_perf_event>> if (kvm_pmc_is_64bit(pmc))
+	 */
 	if (!kvm_pmc_is_64bit(pmc))
 		counter = lower_32_bits(counter);
 
@@ -138,6 +329,11 @@ static u64 kvm_pmu_get_pmc_value(struct kvm_pmc *pmc)
  * @vcpu: The vcpu pointer
  * @select_idx: The counter index
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1040| <<get_pmu_evcntr>> *val = kvm_pmu_get_counter_value(vcpu, idx);
+ *   - arch/arm64/kvm/sys_regs.c|1091| <<access_pmu_evcntr>> p->regval = kvm_pmu_get_counter_value(vcpu, idx);
+ */
 u64 kvm_pmu_get_counter_value(struct kvm_vcpu *vcpu, u64 select_idx)
 {
 	if (!kvm_vcpu_has_pmu(vcpu))
@@ -146,11 +342,22 @@ u64 kvm_pmu_get_counter_value(struct kvm_vcpu *vcpu, u64 select_idx)
 	return kvm_pmu_get_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|187| <<kvm_pmu_set_counter_value>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx), val, false);
+ *   - arch/arm64/kvm/pmu-emul.c|580| <<kvm_pmu_handle_pmcr>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, i), 0, true);
+ */
 static void kvm_pmu_set_pmc_value(struct kvm_pmc *pmc, u64 val, bool force)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
 	u64 reg;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|154| <<kvm_pmu_set_pmc_value>> kvm_pmu_release_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|223| <<kvm_pmu_stop_counter>> kvm_pmu_release_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|264| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 */
 	kvm_pmu_release_perf_event(pmc);
 
 	reg = counter_index_to_reg(pmc->idx);
@@ -169,6 +376,13 @@ static void kvm_pmu_set_pmc_value(struct kvm_pmc *pmc, u64 val, bool force)
 
 	__vcpu_sys_reg(vcpu, reg) = val;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|25| <<global>> static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|173| <<kvm_pmu_set_pmc_value>> kvm_pmu_create_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|303| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|690| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(pmc);
+	 */
 	/* Recreate the perf event to reflect the updated sample_period */
 	kvm_pmu_create_perf_event(pmc);
 }
@@ -179,6 +393,11 @@ static void kvm_pmu_set_pmc_value(struct kvm_pmc *pmc, u64 val, bool force)
  * @select_idx: The counter index
  * @val: The counter value
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|574| <<kvm_pmu_handle_pmcr>> kvm_pmu_set_counter_value(vcpu, ARMV8_PMU_CYCLE_IDX, 0);
+ *   - arch/arm64/kvm/sys_regs.c|1089| <<access_pmu_evcntr>> kvm_pmu_set_counter_value(vcpu, idx, p->regval);
+ */
 void kvm_pmu_set_counter_value(struct kvm_vcpu *vcpu, u64 select_idx, u64 val)
 {
 	if (!kvm_vcpu_has_pmu(vcpu))
@@ -191,10 +410,31 @@ void kvm_pmu_set_counter_value(struct kvm_vcpu *vcpu, u64 select_idx, u64 val)
  * kvm_pmu_release_perf_event - remove the perf event
  * @pmc: The PMU counter pointer
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|154| <<kvm_pmu_set_pmc_value>> kvm_pmu_release_perf_event(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|223| <<kvm_pmu_stop_counter>> kvm_pmu_release_perf_event(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|264| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+ */
 static void kvm_pmu_release_perf_event(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
 		perf_event_disable(pmc->perf_event);
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/pmu-emul.c|386| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/riscv/kvm/vcpu_pmu.c|82| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1058| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1060| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+		 *   - arch/x86/kvm/pmu.c|281| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|198| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+		 *   - kernel/events/core.c|5487| <<perf_release>> perf_event_release_kernel(file->private_data);
+		 *   - kernel/events/hw_breakpoint.c|829| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+		 *   - kernel/watchdog_perf.c|208| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+		 *   - kernel/watchdog_perf.c|275| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+		 *
+		 * arm64只在这里调用release
+		 */
 		perf_event_release_kernel(pmc->perf_event);
 		pmc->perf_event = NULL;
 	}
@@ -206,6 +446,11 @@ static void kvm_pmu_release_perf_event(struct kvm_pmc *pmc)
  *
  * If this counter has been configured to monitor some event, release it here.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|439| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
+ *   - arch/arm64/kvm/pmu-emul.c|825| <<kvm_pmu_create_perf_event>> kvm_pmu_stop_counter(pmc);
+ */
 static void kvm_pmu_stop_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
@@ -220,6 +465,12 @@ static void kvm_pmu_stop_counter(struct kvm_pmc *pmc)
 
 	__vcpu_sys_reg(vcpu, reg) = val;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|154| <<kvm_pmu_set_pmc_value>> kvm_pmu_release_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|223| <<kvm_pmu_stop_counter>> kvm_pmu_release_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|264| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 */
 	kvm_pmu_release_perf_event(pmc);
 }
 
@@ -228,11 +479,26 @@ static void kvm_pmu_stop_counter(struct kvm_pmc *pmc)
  * @vcpu: The vcpu pointer
  *
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|475| <<kvm_arch_vcpu_create>> kvm_pmu_vcpu_init(vcpu);
+ */
 void kvm_pmu_vcpu_init(struct kvm_vcpu *vcpu)
 {
 	int i;
 	struct kvm_pmu *pmu = &vcpu->arch.pmu;
 
+	/*
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 *       -> struct irq_work overflow_work;
+	 *       -> struct kvm_pmu_events events;
+	 *       -> struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
+	 *       -> int irq_num;
+	 *       -> bool created;
+	 *       -> bool irq_level;
+	 */
 	for (i = 0; i < ARMV8_PMU_MAX_COUNTERS; i++)
 		pmu->pmc[i].idx = i;
 }
@@ -242,11 +508,20 @@ void kvm_pmu_vcpu_init(struct kvm_vcpu *vcpu)
  * @vcpu: The vcpu pointer
  *
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/reset.c|205| <<kvm_reset_vcpu>> kvm_pmu_vcpu_reset(vcpu);
+ */
 void kvm_pmu_vcpu_reset(struct kvm_vcpu *vcpu)
 {
 	unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
 	int i;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|439| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 *   - arch/arm64/kvm/pmu-emul.c|825| <<kvm_pmu_create_perf_event>> kvm_pmu_stop_counter(pmc);
+	 */
 	for_each_set_bit(i, &mask, 32)
 		kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
 }
@@ -256,15 +531,38 @@ void kvm_pmu_vcpu_reset(struct kvm_vcpu *vcpu)
  * @vcpu: The vcpu pointer
  *
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|508| <<kvm_arch_vcpu_destroy>> kvm_pmu_vcpu_destroy(vcpu);
+ */
 void kvm_pmu_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	int i;
 
 	for (i = 0; i < ARMV8_PMU_MAX_COUNTERS; i++)
 		kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+	/*
+	 * 在以下使用kvm_pmu->overflow_work:
+	 *   - arch/arm64/kvm/pmu-emul.c|453| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_perf_overflow_notify_vcpu>> vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|718| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|1092| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+	 */
 	irq_work_sync(&vcpu->arch.pmu.overflow_work);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|474| <<kvm_pmu_vcpu_reset>> unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|835| <<kvm_pmu_handle_pmcr>> unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|1076| <<kvm_vcpu_reload_pmu>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1146| <<set_pmreg>> val &= kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1169| <<get_pmreg>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1183| <<access_pmcnten>> mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1206| <<access_pminten>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1230| <<access_pmovs>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|1260| <<access_pmswinc>> mask = kvm_pmu_valid_counter_mask(vcpu);
+ */
 u64 kvm_pmu_valid_counter_mask(struct kvm_vcpu *vcpu)
 {
 	u64 val = FIELD_GET(ARMV8_PMU_PMCR_N, kvm_vcpu_read_pmcr(vcpu));
@@ -282,6 +580,11 @@ u64 kvm_pmu_valid_counter_mask(struct kvm_vcpu *vcpu)
  *
  * Call perf_event_enable to start counting the perf event
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|821| <<kvm_pmu_handle_pmcr>> kvm_pmu_enable_counter_mask(vcpu,
+ *   - arch/arm64/kvm/sys_regs.c|1189| <<access_pmcnten>> kvm_pmu_enable_counter_mask(vcpu, val);
+ */
 void kvm_pmu_enable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 {
 	int i;
@@ -300,8 +603,25 @@ void kvm_pmu_enable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 		pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
 
 		if (!pmc->perf_event) {
+			/*
+			 * called by:
+			 *   - arch/arm64/kvm/pmu-emul.c|25| <<global>> static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc);
+			 *   - arch/arm64/kvm/pmu-emul.c|173| <<kvm_pmu_set_pmc_value>> kvm_pmu_create_perf_event(pmc);
+			 *   - arch/arm64/kvm/pmu-emul.c|303| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(pmc);
+			 *   - arch/arm64/kvm/pmu-emul.c|690| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(pmc);
+			 */
 			kvm_pmu_create_perf_event(pmc);
 		} else {
+			/*
+			 * called by:
+			 *   - arch/arm64/kvm/pmu-emul.c|541| <<kvm_pmu_enable_counter_mask>> perf_event_enable(pmc->perf_event);
+			 *   - arch/riscv/kvm/vcpu_pmu.c|336| <<kvm_pmu_create_perf_event>> perf_event_enable(pmc->perf_event);
+			 *   - arch/riscv/kvm/vcpu_pmu.c|553| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_enable(pmc->perf_event);
+			 *   - arch/x86/kvm/pmu.c|272| <<pmc_resume_counter>> perf_event_enable(pmc->perf_event);
+			 *   - kernel/events/hw_breakpoint.c|815| <<modify_user_hw_breakpoint>> perf_event_enable(bp);
+			 *   - kernel/watchdog_perf.c|169| <<watchdog_hardlockup_enable>> perf_event_enable(this_cpu_read(watchdog_ev));
+			 *   - kernel/watchdog_perf.c|251| <<hardlockup_detector_perf_restart>> perf_event_enable(event);
+			 */
 			perf_event_enable(pmc->perf_event);
 			if (pmc->perf_event->state != PERF_EVENT_STATE_ACTIVE)
 				kvm_debug("fail to enable perf event\n");
@@ -316,6 +636,11 @@ void kvm_pmu_enable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
  *
  * Call perf_event_disable to stop counting the perf event
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|824| <<kvm_pmu_handle_pmcr>> kvm_pmu_disable_counter_mask(vcpu, __vcpu_sys_reg(vcpu, PMCNTENSET_EL0));
+ *   - arch/arm64/kvm/sys_regs.c|1194| <<access_pmcnten>> kvm_pmu_disable_counter_mask(vcpu, val);
+ */
 void kvm_pmu_disable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 {
 	int i;
@@ -336,10 +661,61 @@ void kvm_pmu_disable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 	}
 }
 
+/*
+ * PMOVSSET_EL0寄存器:
+ * PMOVSSET_EL0, Performance Monitors Overflow Flag Status Set Register
+ *
+ * Sets the state of the overflow bit for the Cycle Count Register,
+ * AArch64-PMCCNTR_EL0, and each of the implemented event counters
+ * AArch64-PMEVCNTR<n>_EL0.
+ *
+ * 可写可读, 每个bit表示一个event的overflow status
+ *
+ * -----------
+ *      
+ * PMINTENSET_EL1寄存器:
+ * PMINTENSET_EL1, Performance Monitors Interrupt Enable Set Register
+ *
+ * Enables the generation of interrupt requests on overflows from the
+ * cycle counter, AArch64-PMCCNTR_EL0, and the event counters
+ * AArch64-PMEVCNTR<n>_EL0. Reading the register shows which overflow
+ * interrupt requests are enabled.
+ *
+ * 可读可写, 每个bit表示一个event overflow的时候中断吗
+ *
+ * -----------
+ *
+ * PMCNTENSET_EL0寄存器:
+ * PMCNTENSET_EL0, Performance Monitors Count Enable Set Register
+ *
+ * Enables the Cycle Count Register, AArch64-PMCCNTR_EL0, and any
+ * implemented event counters AArch64-PMEVCNTR<n>_EL0. Reading this
+ * register shows which counters are enabled.
+ *
+ * 可读可写, 激活对应的counter
+ *
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|596| <<kvm_pmu_update_state>> overflow = !!kvm_pmu_overflow_status(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|760| <<kvm_pmu_perf_overflow>> if (kvm_pmu_overflow_status(vcpu)) {
+ */
 static u64 kvm_pmu_overflow_status(struct kvm_vcpu *vcpu)
 {
 	u64 reg = 0;
 
+	/*
+	 * 207 //
+	 * 208 // Per-CPU PMCR: config reg
+	 * 209 //
+	 * 210 #define ARMV8_PMU_PMCR_E        (1 << 0) // Enable all counters
+	 * 211 #define ARMV8_PMU_PMCR_P        (1 << 1) // Reset all counters
+	 * 212 #define ARMV8_PMU_PMCR_C        (1 << 2) // Cycle counter reset
+	 * 213 #define ARMV8_PMU_PMCR_D        (1 << 3) // CCNT counts every 64th cpu cycle
+	 * 214 #define ARMV8_PMU_PMCR_X        (1 << 4) // Export to ETM
+	 * 215 #define ARMV8_PMU_PMCR_DP       (1 << 5) // Disable CCNT if non-invasive debug
+	 * 216 #define ARMV8_PMU_PMCR_LC       (1 << 6) // Overflow on 64 bit cycle counter
+	 * 217 #define ARMV8_PMU_PMCR_LP       (1 << 7) // Long event counter enable
+	 * 218 #define ARMV8_PMU_PMCR_N        GENMASK(15, 11) // Number of counters supported
+	 */
 	if ((kvm_vcpu_read_pmcr(vcpu) & ARMV8_PMU_PMCR_E)) {
 		reg = __vcpu_sys_reg(vcpu, PMOVSSET_EL0);
 		reg &= __vcpu_sys_reg(vcpu, PMCNTENSET_EL0);
@@ -349,6 +725,11 @@ static u64 kvm_pmu_overflow_status(struct kvm_vcpu *vcpu)
 	return reg;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|643| <<kvm_pmu_flush_hwstate>> kvm_pmu_update_state(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|655| <<kvm_pmu_sync_hwstate>> kvm_pmu_update_state(vcpu);
+ */
 static void kvm_pmu_update_state(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = &vcpu->arch.pmu;
@@ -364,12 +745,33 @@ static void kvm_pmu_update_state(struct kvm_vcpu *vcpu)
 	pmu->irq_level = overflow;
 
 	if (likely(irqchip_in_kernel(vcpu->kvm))) {
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 *
+		 * called by:
+		 *   - arch/arm64/kvm/arch_timer.c|455| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, timer_irq(timer_ctx), timer_ctx->irq.level, timer_ctx);
+		 *   - arch/arm64/kvm/arm.c|1393| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/arm.c|1401| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, NULL, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/pmu-emul.c|603| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, NULL, spi_id, level, NULL);
+		 */
 		int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu,
 					      pmu->irq_num, overflow, pmu);
 		WARN_ON(ret);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1076| <<kvm_vcpu_exit_request>> if (kvm_timer_should_notify_user(vcpu) || kvm_pmu_should_notify_user(vcpu)) {
+ */
 bool kvm_pmu_should_notify_user(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = &vcpu->arch.pmu;
@@ -414,6 +816,11 @@ void kvm_pmu_flush_hwstate(struct kvm_vcpu *vcpu)
  * Check if the PMU has overflowed while we were running in the guest, and
  * inject an interrupt if that was the case.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1195| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_sync_hwstate(vcpu);
+ *   - arch/arm64/kvm/arm.c|1228| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_sync_hwstate(vcpu);
+ */
 void kvm_pmu_sync_hwstate(struct kvm_vcpu *vcpu)
 {
 	kvm_pmu_update_state(vcpu);
@@ -424,19 +831,75 @@ void kvm_pmu_sync_hwstate(struct kvm_vcpu *vcpu)
  * to the event.
  * This is why we need a callback to do it once outside of the NMI context.
  */
+/*
+ * 在以下使用kvm_pmu_perf_overflow_notify_vcpu():
+ *   - arch/arm64/kvm/pmu-emul.c|1141| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+ */
 static void kvm_pmu_perf_overflow_notify_vcpu(struct irq_work *work)
 {
 	struct kvm_vcpu *vcpu;
 
+	/*
+	 * 在以下使用kvm_pmu->overflow_work:
+	 *   - arch/arm64/kvm/pmu-emul.c|453| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_perf_overflow_notify_vcpu>> vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|718| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|1092| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+	 */
 	vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
 	kvm_vcpu_kick(vcpu);
 }
 
+/*
+ * 一些event的例子
+ *
+ * 12 //
+ * 13 // Common architectural and microarchitectural event numbers.
+ * 14 //
+ * 15 #define ARMV8_PMUV3_PERFCTR_SW_INCR                             0x0000
+ * 16 #define ARMV8_PMUV3_PERFCTR_L1I_CACHE_REFILL                    0x0001
+ * 17 #define ARMV8_PMUV3_PERFCTR_L1I_TLB_REFILL                      0x0002
+ * 18 #define ARMV8_PMUV3_PERFCTR_L1D_CACHE_REFILL                    0x0003
+ * 19 #define ARMV8_PMUV3_PERFCTR_L1D_CACHE                           0x0004
+ * 20 #define ARMV8_PMUV3_PERFCTR_L1D_TLB_REFILL                      0x0005
+ * 21 #define ARMV8_PMUV3_PERFCTR_LD_RETIRED                          0x0006
+ * 22 #define ARMV8_PMUV3_PERFCTR_ST_RETIRED                          0x0007
+ * 23 #define ARMV8_PMUV3_PERFCTR_INST_RETIRED                        0x0008
+ * 24 #define ARMV8_PMUV3_PERFCTR_EXC_TAKEN                           0x0009
+ * 25 #define ARMV8_PMUV3_PERFCTR_EXC_RETURN                          0x000A
+ * 26 #define ARMV8_PMUV3_PERFCTR_CID_WRITE_RETIRED                   0x000B
+ * 27 #define ARMV8_PMUV3_PERFCTR_PC_WRITE_RETIRED                    0x000C
+ * 28 #define ARMV8_PMUV3_PERFCTR_BR_IMMED_RETIRED                    0x000D
+ * 29 #define ARMV8_PMUV3_PERFCTR_BR_RETURN_RETIRED                   0x000E
+ * 30 #define ARMV8_PMUV3_PERFCTR_UNALIGNED_LDST_RETIRED              0x000F
+ * 31 #define ARMV8_PMUV3_PERFCTR_BR_MIS_PRED                         0x0010
+ * 32 #define ARMV8_PMUV3_PERFCTR_CPU_CYCLES                          0x0011
+ * 33 #define ARMV8_PMUV3_PERFCTR_BR_PRED                             0x0012
+ * 34 #define ARMV8_PMUV3_PERFCTR_MEM_ACCESS                          0x0013
+ * 35 #define ARMV8_PMUV3_PERFCTR_L1I_CACHE                           0x0014
+ * 36 #define ARMV8_PMUV3_PERFCTR_L1D_CACHE_WB                        0x0015
+ * 37 #define ARMV8_PMUV3_PERFCTR_L2D_CACHE                           0x0016
+ * 38 #define ARMV8_PMUV3_PERFCTR_L2D_CACHE_REFILL                    0x0017
+ * 39 #define ARMV8_PMUV3_PERFCTR_L2D_CACHE_WB                        0x0018
+ * 40 #define ARMV8_PMUV3_PERFCTR_BUS_ACCESS                          0x0019
+ * 41 #define ARMV8_PMUV3_PERFCTR_MEMORY_ERROR                        0x001A
+ * 42 #define ARMV8_PMUV3_PERFCTR_INST_SPEC                           0x001B
+ * 43 #define ARMV8_PMUV3_PERFCTR_TTBR_WRITE_RETIRED                  0x001C
+ * 44 #define ARMV8_PMUV3_PERFCTR_BUS_CYCLES                          0x001D
+ * 45 #define ARMV8_PMUV3_PERFCTR_CHAIN                               0x001E
+ */
+
 /*
  * Perform an increment on any of the counters described in @mask,
  * generating the overflow if required, and propagate it as a chained
  * event if possible.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|711| <<kvm_pmu_counter_increment>> kvm_pmu_counter_increment(vcpu, BIT(i + 1), ARMV8_PMUV3_PERFCTR_CHAIN);
+ *   - arch/arm64/kvm/pmu-emul.c|757| <<kvm_pmu_perf_overflow>> kvm_pmu_counter_increment(vcpu, BIT(idx + 1), ARMV8_PMUV3_PERFCTR_CHAIN);
+ *   - arch/arm64/kvm/pmu-emul.c|779| <<kvm_pmu_software_increment>> kvm_pmu_counter_increment(vcpu, val, ARMV8_PMUV3_PERFCTR_SW_INCR);
+ */
 static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
 				      unsigned long mask, u32 event)
 {
@@ -445,6 +908,17 @@ static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
 	if (!(kvm_vcpu_read_pmcr(vcpu) & ARMV8_PMU_PMCR_E))
 		return;
 
+	/*
+	 *
+	 * PMCNTENSET_EL0寄存器:
+	 * PMCNTENSET_EL0, Performance Monitors Count Enable Set Register
+	 *
+	 * Enables the Cycle Count Register, AArch64-PMCCNTR_EL0, and any
+	 * implemented event counters AArch64-PMEVCNTR<n>_EL0. Reading this
+	 * register shows which counters are enabled.
+	 *
+	 * 可读可写, 激活对应的counter
+	 */
 	/* Weed out disabled counters */
 	mask &= __vcpu_sys_reg(vcpu, PMCNTENSET_EL0);
 
@@ -455,6 +929,9 @@ static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
 		/* Filter on event type */
 		type = __vcpu_sys_reg(vcpu, counter_index_to_evtreg(i));
 		type &= kvm_pmu_event_mask(vcpu->kvm);
+		/*
+		 * 只针对event
+		 */
 		if (type != event)
 			continue;
 
@@ -468,6 +945,16 @@ static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
 		if (kvm_pmc_has_64bit_overflow(pmc) ? reg : lower_32_bits(reg))
 			continue;
 
+		/*
+		 * PMOVSSET_EL0寄存器:
+		 * PMOVSSET_EL0, Performance Monitors Overflow Flag Status Set Register
+		 *
+		 * Sets the state of the overflow bit for the Cycle Count Register,
+		 * AArch64-PMCCNTR_EL0, and each of the implemented event counters
+		 * AArch64-PMEVCNTR<n>_EL0.
+		 *
+		 * 可写可读, 每个bit表示一个event的overflow status
+		 */
 		/* Mark overflow */
 		__vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= BIT(i);
 
@@ -477,6 +964,11 @@ static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|852| <<kvm_pmu_perf_overflow>> period = compute_period(pmc, local64_read(&perf_event->count));
+ *   - arch/arm64/kvm/pmu-emul.c|1030| <<kvm_pmu_create_perf_event>> attr.sample_period = compute_period(pmc, kvm_pmu_get_pmc_value(pmc));
+ */
 /* Compute the sample period for a given counter value */
 static u64 compute_period(struct kvm_pmc *pmc, u64 counter)
 {
@@ -493,6 +985,10 @@ static u64 compute_period(struct kvm_pmc *pmc, u64 counter)
 /*
  * When the perf event overflows, set the overflow status and inform the vcpu.
  */
+/*
+ * 在以下使用kvm_pmu_perf_overflow():
+ *   - arch/arm64/kvm/pmu-emul.c|1033| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+ */
 static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 				  struct perf_sample_data *data,
 				  struct pt_regs *regs)
@@ -503,6 +999,11 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 	int idx = pmc->idx;
 	u64 period;
 
+	/*
+	 * #define PERF_EF_START   0x01            // start the counter when adding
+	 * #define PERF_EF_RELOAD  0x02            // reload the counter when starting
+	 * #define PERF_EF_UPDATE  0x04            // update the counter when stopping
+	 */
 	cpu_pmu->pmu.stop(perf_event, PERF_EF_UPDATE);
 
 	/*
@@ -515,6 +1016,19 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 	perf_event->attr.sample_period = period;
 	perf_event->hw.sample_period = period;
 
+	/*
+	 * PMOVSSET_EL0寄存器:
+	 * PMOVSSET_EL0, Performance Monitors Overflow Flag Status Set Register
+	 *
+	 * Sets the state of the overflow bit for the Cycle Count Register,
+	 * AArch64-PMCCNTR_EL0, and each of the implemented event counters
+	 * AArch64-PMEVCNTR<n>_EL0.
+	 *      
+	 * 可写可读, 每个bit表示一个event的overflow status
+	 *
+	 *
+	 * PMOVSSET_EL0,   // Overflow Flag Status Set Register
+	 */
 	__vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= BIT(idx);
 
 	if (kvm_pmu_counter_can_chain(pmc))
@@ -522,14 +1036,37 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 					  ARMV8_PMUV3_PERFCTR_CHAIN);
 
 	if (kvm_pmu_overflow_status(vcpu)) {
+		/*
+		 * 在以下使用KVM_REQ_IRQ_PENDING:
+		 *   - arch/arm64/kvm/arm.c|1009| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/arm.c|1348| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/pmu-emul.c|865| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|351| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|401| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|717| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|998| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 */
 		kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
 
+		/*
+		 * 在以下使用kvm_pmu->overflow_work:
+		 *   - arch/arm64/kvm/pmu-emul.c|453| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+		 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_perf_overflow_notify_vcpu>> vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
+		 *   - arch/arm64/kvm/pmu-emul.c|718| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+		 *   - arch/arm64/kvm/pmu-emul.c|1092| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+		 */
 		if (!in_nmi())
 			kvm_vcpu_kick(vcpu);
 		else
 			irq_work_queue(&vcpu->arch.pmu.overflow_work);
 	}
 
+	/*
+	 * #define PERF_EF_START   0x01            // start the counter when adding
+	 * #define PERF_EF_RELOAD  0x02            // reload the counter when starting
+	 * #define PERF_EF_UPDATE  0x04            // update the counter when stopping 
+	 */
 	cpu_pmu->pmu.start(perf_event, PERF_EF_RELOAD);
 }
 
@@ -538,8 +1075,18 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
  * @vcpu: The vcpu pointer
  * @val: the value guest writes to PMSWINC register
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1261| <<access_pmswinc>> kvm_pmu_software_increment(vcpu, p->regval & mask);
+ */
 void kvm_pmu_software_increment(struct kvm_vcpu *vcpu, u64 val)
 {
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|711| <<kvm_pmu_counter_increment>> kvm_pmu_counter_increment(vcpu, BIT(i + 1), ARMV8_PMUV3_PERFCTR_CHAIN);
+	 *   - arch/arm64/kvm/pmu-emul.c|757| <<kvm_pmu_perf_overflow>> kvm_pmu_counter_increment(vcpu, BIT(idx + 1), ARMV8_PMUV3_PERFCTR_CHAIN);
+	 *   - arch/arm64/kvm/pmu-emul.c|779| <<kvm_pmu_software_increment>> kvm_pmu_counter_increment(vcpu, val, ARMV8_PMUV3_PERFCTR_SW_INCR);
+	 */
 	kvm_pmu_counter_increment(vcpu, val, ARMV8_PMUV3_PERFCTR_SW_INCR);
 }
 
@@ -548,6 +1095,20 @@ void kvm_pmu_software_increment(struct kvm_vcpu *vcpu, u64 val)
  * @vcpu: The vcpu pointer
  * @val: the value guest writes to PMCR register
  */
+/*
+ * PMCR寄存器:
+ * PMCR_EL0, Performance Monitors Control Register
+ *
+ * Provides details of the Performance Monitors implementation, including the
+ * number of counters implemented, and configures and controls the counters.
+ *
+ * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+ * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+ *
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|809| <<kvm_vcpu_reload_pmu>> kvm_pmu_handle_pmcr(vcpu, kvm_vcpu_read_pmcr(vcpu));
+ *   - arch/arm64/kvm/sys_regs.c|966| <<access_pmcr>> kvm_pmu_handle_pmcr(vcpu, val);
+ */
 void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 {
 	int i;
@@ -562,7 +1123,31 @@ void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 	/* The reset bits don't indicate any state, and shouldn't be saved. */
 	__vcpu_sys_reg(vcpu, PMCR_EL0) = val & ~(ARMV8_PMU_PMCR_C | ARMV8_PMU_PMCR_P);
 
+	/*
+	 * 207 //
+	 * 208 // Per-CPU PMCR: config reg
+	 * 209  //
+	 * 210 #define ARMV8_PMU_PMCR_E        (1 << 0) // Enable all counters
+	 * 211 #define ARMV8_PMU_PMCR_P        (1 << 1) // Reset all counters
+	 * 212 #define ARMV8_PMU_PMCR_C        (1 << 2) // Cycle counter reset
+	 * 213 #define ARMV8_PMU_PMCR_D        (1 << 3) // CCNT counts every 64th cpu cycle
+	 * 214 #define ARMV8_PMU_PMCR_X        (1 << 4) // Export to ETM
+	 * 215 #define ARMV8_PMU_PMCR_DP       (1 << 5) // Disable CCNT if non-invasive debug
+	 * 216 #define ARMV8_PMU_PMCR_LC       (1 << 6) // Overflow on 64 bit cycle counter
+	 * 217 #define ARMV8_PMU_PMCR_LP       (1 << 7) // Long event counter enable
+	 * 218 #define ARMV8_PMU_PMCR_N        GENMASK(15, 11) // Number of counters supported
+	 */
 	if (val & ARMV8_PMU_PMCR_E) {
+		/*
+		 * PMCNTENSET_EL0寄存器:
+		 * PMCNTENSET_EL0, Performance Monitors Count Enable Set Register
+		 *
+		 * Enables the Cycle Count Register, AArch64-PMCCNTR_EL0, and any
+		 * implemented event counters AArch64-PMEVCNTR<n>_EL0. Reading this
+		 * register shows which counters are enabled.
+		 *
+		 * 可读可写, 激活对应的counter
+		 */
 		kvm_pmu_enable_counter_mask(vcpu,
 		       __vcpu_sys_reg(vcpu, PMCNTENSET_EL0));
 	} else {
@@ -570,6 +1155,9 @@ void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 		       __vcpu_sys_reg(vcpu, PMCNTENSET_EL0));
 	}
 
+	/*
+	 * #define ARMV8_PMU_CYCLE_IDX             (ARMV8_PMU_MAX_COUNTERS - 1)
+	 */
 	if (val & ARMV8_PMU_PMCR_C)
 		kvm_pmu_set_counter_value(vcpu, ARMV8_PMU_CYCLE_IDX, 0);
 
@@ -582,9 +1170,20 @@ void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 	kvm_vcpu_pmu_restore_guest(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1090| <<kvm_pmu_create_perf_event>> attr.disabled = !kvm_pmu_counter_is_enabled(pmc);
+ */
 static bool kvm_pmu_counter_is_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
+	/*
+	 * PMCNTENSET_EL0, // Count Enable Set Register
+	 *
+	 * 0-30位表示对应的counter是否激活
+	 *
+	 * bit=31是给cycles用的(PMCCNTR_EL0)
+	 */
 	return (kvm_vcpu_read_pmcr(vcpu) & ARMV8_PMU_PMCR_E) &&
 	       (__vcpu_sys_reg(vcpu, PMCNTENSET_EL0) & BIT(pmc->idx));
 }
@@ -593,6 +1192,12 @@ static bool kvm_pmu_counter_is_enabled(struct kvm_pmc *pmc)
  * kvm_pmu_create_perf_event - create a perf event for a counter
  * @pmc: Counter context
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|173| <<kvm_pmu_set_pmc_value>> kvm_pmu_create_perf_event(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|303| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|690| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(pmc);
+ */
 static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
@@ -605,6 +1210,11 @@ static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
 	reg = counter_index_to_evtreg(pmc->idx);
 	data = __vcpu_sys_reg(vcpu, reg);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|439| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 *   - arch/arm64/kvm/pmu-emul.c|825| <<kvm_pmu_create_perf_event>> kvm_pmu_stop_counter(pmc);
+	 */
 	kvm_pmu_stop_counter(pmc);
 	if (pmc->idx == ARMV8_PMU_CYCLE_IDX)
 		eventsel = ARMV8_PMUV3_PERFCTR_CPU_CYCLES;
@@ -619,6 +1229,21 @@ static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
 	    eventsel == ARMV8_PMUV3_PERFCTR_CHAIN)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->pmu_filter:
+	 *   - arch/arm64/kvm/arm.c|256| <<kvm_arch_destroy_vm>> bitmap_free(kvm->arch.pmu_filter);
+	 *   - arch/arm64/kvm/pmu-emul.c|1094| <<kvm_pmu_create_perf_event>> if (vcpu->kvm->arch.pmu_filter && !test_bit(eventsel, vcpu->kvm->arch.pmu_filter))
+	 *   - arch/arm64/kvm/pmu-emul.c|1095| <<kvm_pmu_create_perf_event>> !test_bit(eventsel, vcpu->kvm->arch.pmu_filter))
+	 *   - arch/arm64/kvm/pmu-emul.c|1234| <<kvm_pmu_get_pmceid>> unsigned long *bmap = vcpu->kvm->arch.pmu_filter;
+	 *   - arch/arm64/kvm/pmu-emul.c|1443| <<kvm_arm_pmu_v3_set_pmu>> (kvm->arch.pmu_filter && kvm->arch.arm_pmu != arm_pmu)) {
+	 *   - arch/arm64/kvm/pmu-emul.c|1522| <<kvm_arm_pmu_v3_set_attr>> if (!kvm->arch.pmu_filter) {
+	 *   - arch/arm64/kvm/pmu-emul.c|1523| <<kvm_arm_pmu_v3_set_attr>> kvm->arch.pmu_filter = bitmap_alloc(nr_events, GFP_KERNEL_ACCOUNT);
+	 *   - arch/arm64/kvm/pmu-emul.c|1524| <<kvm_arm_pmu_v3_set_attr>> if (!kvm->arch.pmu_filter)
+	 *   - arch/arm64/kvm/pmu-emul.c|1534| <<kvm_arm_pmu_v3_set_attr>> bitmap_zero(kvm->arch.pmu_filter, nr_events);
+	 *   - arch/arm64/kvm/pmu-emul.c|1536| <<kvm_arm_pmu_v3_set_attr>> bitmap_fill(kvm->arch.pmu_filter, nr_events);
+	 *   - arch/arm64/kvm/pmu-emul.c|1540| <<kvm_arm_pmu_v3_set_attr>> bitmap_set(kvm->arch.pmu_filter, filter.base_event, filter.nevents);
+	 *   - arch/arm64/kvm/pmu-emul.c|1542| <<kvm_arm_pmu_v3_set_attr>> bitmap_clear(kvm->arch.pmu_filter, filter.base_event, filter.nevents);
+	 */
 	/*
 	 * If we have a filter in place and that the event isn't allowed, do
 	 * not install a perf event either.
@@ -653,6 +1278,22 @@ static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
 
 	attr.sample_period = compute_period(pmc, kvm_pmu_get_pmc_value(pmc));
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|1124| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|328| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(attr, -1, current, kvm_riscv_pmu_overflow, pmc);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|969| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|974| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kvm/pmu.c|215| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|243| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+	 *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+	 *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+	 *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>> return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+	 *   - kernel/watchdog_perf.c|135| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 *   - kernel/watchdog_perf.c|140| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 *
+	 * arm64唯一调用的地方
+	 */
 	event = perf_event_create_kernel_counter(&attr, -1, current,
 						 kvm_pmu_perf_overflow, pmc);
 
@@ -675,21 +1316,63 @@ static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
  * event with given hardware event number. Here we call perf_event API to
  * emulate this action and create a kernel perf event for it.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1124| <<access_pmu_evtyper>> kvm_pmu_set_counter_event_type(vcpu, p->regval, idx);
+ */
 void kvm_pmu_set_counter_event_type(struct kvm_vcpu *vcpu, u64 data,
 				    u64 select_idx)
 {
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|146| <<kvm_pmu_get_counter_value>> return kvm_pmu_get_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx));
+	 *   - arch/arm64/kvm/pmu-emul.c|187| <<kvm_pmu_set_counter_value>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, select_idx), val, false);
+	 *   - arch/arm64/kvm/pmu-emul.c|251| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 *   - arch/arm64/kvm/pmu-emul.c|264| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(kvm_vcpu_idx_to_pmc(vcpu, i));
+	 *   - arch/arm64/kvm/pmu-emul.c|300| <<kvm_pmu_enable_counter_mask>> pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+	 *   - arch/arm64/kvm/pmu-emul.c|332| <<kvm_pmu_disable_counter_mask>> pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+	 *   - arch/arm64/kvm/pmu-emul.c|452| <<kvm_pmu_counter_increment>> struct kvm_pmc *pmc = kvm_vcpu_idx_to_pmc(vcpu, i);
+	 *   - arch/arm64/kvm/pmu-emul.c|580| <<kvm_pmu_handle_pmcr>> kvm_pmu_set_pmc_value(kvm_vcpu_idx_to_pmc(vcpu, i), 0, true);
+	 *   - arch/arm64/kvm/pmu-emul.c|681| <<kvm_pmu_set_counter_event_type>> struct kvm_pmc *pmc = kvm_vcpu_idx_to_pmc(vcpu, select_idx);
+	 *
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 *       -> struct irq_work overflow_work;
+	 *       -> struct kvm_pmu_events events;
+	 *       -> struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
+	 *       -> int irq_num;
+	 *       -> bool created;
+	 *       -> bool irq_level;
+	 */
 	struct kvm_pmc *pmc = kvm_vcpu_idx_to_pmc(vcpu, select_idx);
 	u64 reg;
 
 	if (!kvm_vcpu_has_pmu(vcpu))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|456| <<kvm_pmu_counter_increment>> type = __vcpu_sys_reg(vcpu, counter_index_to_evtreg(i));
+	 *   - arch/arm64/kvm/pmu-emul.c|605| <<kvm_pmu_create_perf_event>> reg = counter_index_to_evtreg(pmc->idx);
+	 *   - arch/arm64/kvm/pmu-emul.c|687| <<kvm_pmu_set_counter_event_type>> reg = counter_index_to_evtreg(pmc->idx);
+	 */
 	reg = counter_index_to_evtreg(pmc->idx);
 	__vcpu_sys_reg(vcpu, reg) = data & kvm_pmu_evtyper_mask(vcpu->kvm);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|173| <<kvm_pmu_set_pmc_value>> kvm_pmu_create_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|303| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|690| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(pmc);
+	 */
 	kvm_pmu_create_perf_event(pmc);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmu.c|931| <<armpmu_register>> kvm_host_pmu_init(pmu);
+ */
 void kvm_host_pmu_init(struct arm_pmu *pmu)
 {
 	struct arm_pmu_entry *entry;
@@ -708,6 +1391,14 @@ void kvm_host_pmu_init(struct arm_pmu *pmu)
 		goto out_unlock;
 
 	entry->arm_pmu = pmu;
+	/*
+	 * 在以下使用LIST_HEAD(arm_pmus):
+	 *   - arch/arm64/kvm/pmu-emul.c|22| <<global>> static LIST_HEAD(arm_pmus);
+	 *   - arch/arm64/kvm/pmu-emul.c|711| <<kvm_host_pmu_init>> list_add_tail(&entry->entry, &arm_pmus);
+	 *   - arch/arm64/kvm/pmu-emul.c|713| <<kvm_host_pmu_init>> if (list_is_singular(&arm_pmus))
+	 *   - arch/arm64/kvm/pmu-emul.c|746| <<kvm_pmu_probe_armpmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+	 *   - arch/arm64/kvm/pmu-emul.c|960| <<kvm_arm_pmu_v3_set_pmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+	 */
 	list_add_tail(&entry->entry, &arm_pmus);
 
 	if (list_is_singular(&arm_pmus))
@@ -717,6 +1408,10 @@ void kvm_host_pmu_init(struct arm_pmu *pmu)
 	mutex_unlock(&arm_pmus_lock);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1450| <<kvm_arm_set_default_pmu>> struct arm_pmu *arm_pmu = kvm_pmu_probe_armpmu();
+ */
 static struct arm_pmu *kvm_pmu_probe_armpmu(void)
 {
 	struct arm_pmu *tmp, *pmu = NULL;
@@ -743,6 +1438,14 @@ static struct arm_pmu *kvm_pmu_probe_armpmu(void)
 	 * carried here.
 	 */
 	cpu = raw_smp_processor_id();
+	/*
+	 * 在以下使用LIST_HEAD(arm_pmus):
+	 *   - arch/arm64/kvm/pmu-emul.c|22| <<global>> static LIST_HEAD(arm_pmus);
+	 *   - arch/arm64/kvm/pmu-emul.c|711| <<kvm_host_pmu_init>> list_add_tail(&entry->entry, &arm_pmus);
+	 *   - arch/arm64/kvm/pmu-emul.c|713| <<kvm_host_pmu_init>> if (list_is_singular(&arm_pmus))
+	 *   - arch/arm64/kvm/pmu-emul.c|746| <<kvm_pmu_probe_armpmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+	 *   - arch/arm64/kvm/pmu-emul.c|960| <<kvm_arm_pmu_v3_set_pmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+	 */
 	list_for_each_entry(entry, &arm_pmus, entry) {
 		tmp = entry->arm_pmu;
 
@@ -757,6 +1460,24 @@ static struct arm_pmu *kvm_pmu_probe_armpmu(void)
 	return pmu;
 }
 
+/*
+ * PMCEID1_EL0, Performance Monitors Common Event Identification register 1
+ *
+ * Defines which Common architectural events and Common microarchitectural
+ * events are implemented, or counted, using PMU events in the ranges 0x0020 to
+ * 0x003F and 0x4020 to 0x403F.
+ *
+ * 比如:
+ * ID0 corresponds to common event (0x20) L2D_CACHE_ALLOCATE
+ * ID1 corresponds to common event (0x21) BR_RETIRED
+ * ... ...
+ * ID12 corresponds to common event (0x2c) Reserved
+ * ... ...
+ * ID31 corresponds to common event (0x3f) STALL_SLOT
+ *
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1005| <<access_pmceid>> pmceid = kvm_pmu_get_pmceid(vcpu, (p->Op2 & 1));
+ */
 u64 kvm_pmu_get_pmceid(struct kvm_vcpu *vcpu, bool pmceid1)
 {
 	unsigned long *bmap = vcpu->kvm->arch.pmu_filter;
@@ -802,22 +1523,72 @@ u64 kvm_pmu_get_pmceid(struct kvm_vcpu *vcpu, bool pmceid1)
 	return val & mask;
 }
 
+/*
+ * 在以下使用KVM_REQ_RELOAD_PMU:
+ *   - arch/arm64/kvm/arm.c|1022| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu)) kvm_vcpu_reload_pmu(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|1413| <<kvm_arm_pmu_v3_enable>> kvm_make_request(KVM_REQ_RELOAD_PMU, vcpu);
+ *
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1023| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu)) kvm_vcpu_reload_pmu(vcpu);
+ */
 void kvm_vcpu_reload_pmu(struct kvm_vcpu *vcpu)
 {
 	u64 mask = kvm_pmu_valid_counter_mask(vcpu);
 
 	kvm_pmu_handle_pmcr(vcpu, kvm_vcpu_read_pmcr(vcpu));
 
+	/*
+	 * PMOVSSET_EL0寄存器:
+	 * PMOVSSET_EL0, Performance Monitors Overflow Flag Status Set Register
+	 *
+	 * Sets the state of the overflow bit for the Cycle Count Register,
+	 * AArch64-PMCCNTR_EL0, and each of the implemented event counters
+	 * AArch64-PMEVCNTR<n>_EL0.
+	 *
+	 * 可写可读, 每个bit表示一个event的overflow status
+	 *
+	 * -----------
+	 *
+	 * PMINTENSET_EL1寄存器:
+	 * PMINTENSET_EL1, Performance Monitors Interrupt Enable Set Register
+	 *
+	 * Enables the generation of interrupt requests on overflows from the
+	 * cycle counter, AArch64-PMCCNTR_EL0, and the event counters
+	 * AArch64-PMEVCNTR<n>_EL0. Reading the register shows which overflow
+	 * interrupt requests are enabled.
+	 *
+	 * 可读可写, 每个bit表示一个event overflow的时候中断吗
+	 *
+	 * -----------
+	 *
+	 * PMCNTENSET_EL0寄存器:
+	 * PMCNTENSET_EL0, Performance Monitors Count Enable Set Register
+	 *
+	 * Enables the Cycle Count Register, AArch64-PMCCNTR_EL0, and any
+	 * implemented event counters AArch64-PMEVCNTR<n>_EL0. Reading this
+	 * register shows which counters are enabled.
+	 *
+	 * 可读可写, 激活对应的counter
+	 */
 	__vcpu_sys_reg(vcpu, PMOVSSET_EL0) &= mask;
 	__vcpu_sys_reg(vcpu, PMINTENSET_EL1) &= mask;
 	__vcpu_sys_reg(vcpu, PMCNTENSET_EL0) &= mask;
 }
 
+/*
+ * arch/arm64/kvm/arm.c|840| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_arm_pmu_v3_enable(vcpu);
+ */
 int kvm_arm_pmu_v3_enable(struct kvm_vcpu *vcpu)
 {
 	if (!kvm_vcpu_has_pmu(vcpu))
 		return 0;
 
+	/*
+	 * 在以下使用kvm_pmu->created:
+	 *   - arch/arm64/kvm/pmu-emul.c|1503| <<kvm_arm_pmu_v3_enable>> if (!vcpu->arch.pmu.created) return -EINVAL;
+	 *   - arch/arm64/kvm/pmu-emul.c|1568| <<kvm_arm_pmu_v3_init>> vcpu->arch.pmu.created = true;
+	 *   - arch/arm64/kvm/pmu-emul.c|1709| <<kvm_arm_pmu_v3_set_attr>> if (vcpu->arch.pmu.created) return -EBUSY;
+	 */
 	if (!vcpu->arch.pmu.created)
 		return -EINVAL;
 
@@ -827,6 +1598,16 @@ int kvm_arm_pmu_v3_enable(struct kvm_vcpu *vcpu)
 	 * irqchip, or to not have an in-kernel GIC and not set an IRQ.
 	 */
 	if (irqchip_in_kernel(vcpu->kvm)) {
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 */
 		int irq = vcpu->arch.pmu.irq_num;
 		/*
 		 * If we are using an in-kernel vgic, at this point we know
@@ -840,12 +1621,21 @@ int kvm_arm_pmu_v3_enable(struct kvm_vcpu *vcpu)
 		   return -EINVAL;
 	}
 
+	/*
+	 * 在以下使用KVM_REQ_RELOAD_PMU:
+	 *   - arch/arm64/kvm/arm.c|1022| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu)) kvm_vcpu_reload_pmu(vcpu);
+	 *   - arch/arm64/kvm/pmu-emul.c|1413| <<kvm_arm_pmu_v3_enable>> kvm_make_request(KVM_REQ_RELOAD_PMU, vcpu);
+	 */
 	/* One-off reload of the PMU on first run */
 	kvm_make_request(KVM_REQ_RELOAD_PMU, vcpu);
 
 	return 0;
 }
 
+/*
+ * 处理KVM_ARM_VCPU_PMU_V3_SET_PMU:
+ *   - arch/arm64/kvm/pmu-emul.c|1797| <<kvm_arm_pmu_v3_set_attr(KVM_ARM_VCPU_PMU_V3_SET_PMU)>> return kvm_arm_pmu_v3_init(vcpu);
+ */
 static int kvm_arm_pmu_v3_init(struct kvm_vcpu *vcpu)
 {
 	if (irqchip_in_kernel(vcpu->kvm)) {
@@ -862,15 +1652,42 @@ static int kvm_arm_pmu_v3_init(struct kvm_vcpu *vcpu)
 		if (!kvm_arm_pmu_irq_initialized(vcpu))
 			return -ENXIO;
 
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 *
+		 * called by:
+		 *   - arch/arm64/kvm/arch_timer.c|1465| <<timer_irqs_are_valid>> if (kvm_vgic_set_owner(vcpu, irq, ctx))
+		 *   - arch/arm64/kvm/pmu-emul.c|1552| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num, &vcpu->arch.pmu);
+		 */
 		ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
 					 &vcpu->arch.pmu);
 		if (ret)
 			return ret;
 	}
 
+	/*
+	 * 在以下使用kvm_pmu->overflow_work:
+	 *   - arch/arm64/kvm/pmu-emul.c|453| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_perf_overflow_notify_vcpu>> vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|718| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|1092| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+	 */
 	init_irq_work(&vcpu->arch.pmu.overflow_work,
 		      kvm_pmu_perf_overflow_notify_vcpu);
 
+	/*
+	 * 在以下使用kvm_pmu->created:
+	 *   - arch/arm64/kvm/pmu-emul.c|1503| <<kvm_arm_pmu_v3_enable>> if (!vcpu->arch.pmu.created) return -EINVAL;
+	 *   - arch/arm64/kvm/pmu-emul.c|1568| <<kvm_arm_pmu_v3_init>> vcpu->arch.pmu.created = true;
+	 *   - arch/arm64/kvm/pmu-emul.c|1709| <<kvm_arm_pmu_v3_set_attr>> if (vcpu->arch.pmu.created) return -EBUSY;
+	 */
 	vcpu->arch.pmu.created = true;
 	return 0;
 }
@@ -880,6 +1697,10 @@ static int kvm_arm_pmu_v3_init(struct kvm_vcpu *vcpu)
  * As a PPI, the interrupt number is the same for all vcpus,
  * while as an SPI it must be a separate number per vcpu.
  */
+/*
+ * 处理KVM_ARM_VCPU_PMU_V3_IRQ:
+ *   - arch/arm64/kvm/pmu-emul.c|1798| <<kvm_arm_pmu_v3_set_attr(KVM_ARM_VCPU_PMU_V3_IRQ)>> if (!pmu_irq_is_valid(kvm, irq))
+ */
 static bool pmu_irq_is_valid(struct kvm *kvm, int irq)
 {
 	unsigned long i;
@@ -889,6 +1710,16 @@ static bool pmu_irq_is_valid(struct kvm *kvm, int irq)
 		if (!kvm_arm_pmu_irq_initialized(vcpu))
 			continue;
 
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 */
 		if (irq_is_ppi(irq)) {
 			if (vcpu->arch.pmu.irq_num != irq)
 				return false;
@@ -905,6 +1736,11 @@ static bool pmu_irq_is_valid(struct kvm *kvm, int irq)
  * kvm_arm_pmu_get_max_counters - Return the max number of PMU counters.
  * @kvm: The kvm pointer
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+ *   - arch/arm64/kvm/sys_regs.c|1320| <<set_pmcr>> if (!kvm_vm_has_ran_once(kvm) && new_n <= kvm_arm_pmu_get_max_counters(kvm))
+ */
 u8 kvm_arm_pmu_get_max_counters(struct kvm *kvm)
 {
 	struct arm_pmu *arm_pmu = kvm->arch.arm_pmu;
@@ -916,11 +1752,32 @@ u8 kvm_arm_pmu_get_max_counters(struct kvm *kvm)
 	return arm_pmu->num_events - 1;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1526| <<kvm_arm_set_default_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+ *   - arch/arm64/kvm/pmu-emul.c|1549| <<kvm_arm_pmu_v3_set_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+ */
 static void kvm_arm_set_pmu(struct kvm *kvm, struct arm_pmu *arm_pmu)
 {
 	lockdep_assert_held(&kvm->arch.config_lock);
 
 	kvm->arch.arm_pmu = arm_pmu;
+	/*
+	 * PMCR寄存器:
+	 * PMCR_EL0, Performance Monitors Control Register
+	 *
+	 * Provides details of the Performance Monitors implementation, including the
+	 * number of counters implemented, and configures and controls the counters.
+	 *
+	 * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+	 * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+	 *
+	 * 在以下使用kvm_arch->pmcr_n:
+	 *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+	 *   - arch/arm64/kvm/pmu-emul.c|1969| <<kvm_vcpu_read_pmcr>> return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
+	 *   - arch/arm64/kvm/sys_regs.c|862| <<reset_pmu_reg>> u8 n = vcpu->kvm->arch.pmcr_n;
+	 *   - arch/arm64/kvm/sys_regs.c|1321| <<set_pmcr>> kvm->arch.pmcr_n = new_n;
+	 */
 	kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
 }
 
@@ -936,17 +1793,33 @@ static void kvm_arm_set_pmu(struct kvm *kvm, struct arm_pmu *arm_pmu)
  * where vCPUs can be scheduled on any core but the guest
  * counters could stop working.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1489| <<kvm_setup_vcpu>> ret = kvm_arm_set_default_pmu(kvm);
+ */
 int kvm_arm_set_default_pmu(struct kvm *kvm)
 {
+	/*
+	 * 只在此处调用
+	 */
 	struct arm_pmu *arm_pmu = kvm_pmu_probe_armpmu();
 
 	if (!arm_pmu)
 		return -ENODEV;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|1526| <<kvm_arm_set_default_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+	 *   - arch/arm64/kvm/pmu-emul.c|1549| <<kvm_arm_pmu_v3_set_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+	 */
 	kvm_arm_set_pmu(kvm, arm_pmu);
 	return 0;
 }
 
+/*
+ * 处理KVM_ARM_VCPU_PMU_V3_SET_PMU:
+ *   - arch/arm64/kvm/pmu-emul.c|1654| <<kvm_arm_pmu_v3_set_attr(KVM_ARM_VCPU_PMU_V3_SET_PMU)>> return kvm_arm_pmu_v3_set_pmu(vcpu, pmu_id);
+ */
 static int kvm_arm_pmu_v3_set_pmu(struct kvm_vcpu *vcpu, int pmu_id)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -966,6 +1839,11 @@ static int kvm_arm_pmu_v3_set_pmu(struct kvm_vcpu *vcpu, int pmu_id)
 				break;
 			}
 
+			/*
+			 * called by:
+			 *   - arch/arm64/kvm/pmu-emul.c|1526| <<kvm_arm_set_default_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+			 *   - arch/arm64/kvm/pmu-emul.c|1549| <<kvm_arm_pmu_v3_set_pmu>> kvm_arm_set_pmu(kvm, arm_pmu);
+			 */
 			kvm_arm_set_pmu(kvm, arm_pmu);
 			cpumask_copy(kvm->arch.supported_cpus, &arm_pmu->supported_cpus);
 			ret = 0;
@@ -977,6 +1855,21 @@ static int kvm_arm_pmu_v3_set_pmu(struct kvm_vcpu *vcpu, int pmu_id)
 	return ret;
 }
 
+/*
+ * #define   KVM_ARM_VCPU_PMU_V3_IRQ       0
+ * #define   KVM_ARM_VCPU_PMU_V3_INIT      1
+ * #define   KVM_ARM_VCPU_PMU_V3_FILTER    2 --> QEMU-9.1不使用
+ * #define   KVM_ARM_VCPU_PMU_V3_SET_PMU   3 --> QEMU-9.1不使用
+ *
+ * 处理KVM_ARM_VCPU_PMU_V3_CTRL:
+ *   - arch/arm64/kvm/guest.c|955| <<kvm_arm_vcpu_arch_set_attr>> ret = kvm_arm_pmu_v3_set_attr(vcpu, attr);
+ *
+ * kvm_arm_pmu_v3_set_attr()
+ * kvm_arm_vcpu_arch_set_attr(KVM_ARM_VCPU_PMU_V3_CTRL)
+ * kvm_arm_vcpu_set_attr()
+ * kvm_arch_vcpu_ioctl(KVM_SET_DEVICE_ATTR)
+ * kvm_vcpu_ioctl(default)
+ */
 int kvm_arm_pmu_v3_set_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -986,6 +1879,12 @@ int kvm_arm_pmu_v3_set_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 	if (!kvm_vcpu_has_pmu(vcpu))
 		return -ENODEV;
 
+	/*
+	 * 在以下使用kvm_pmu->created:
+	 *   - arch/arm64/kvm/pmu-emul.c|1503| <<kvm_arm_pmu_v3_enable>> if (!vcpu->arch.pmu.created) return -EINVAL;
+	 *   - arch/arm64/kvm/pmu-emul.c|1568| <<kvm_arm_pmu_v3_init>> vcpu->arch.pmu.created = true;
+	 *   - arch/arm64/kvm/pmu-emul.c|1709| <<kvm_arm_pmu_v3_set_attr>> if (vcpu->arch.pmu.created) return -EBUSY;
+	 */
 	if (vcpu->arch.pmu.created)
 		return -EBUSY;
 
@@ -1010,6 +1909,16 @@ int kvm_arm_pmu_v3_set_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 		if (kvm_arm_pmu_irq_initialized(vcpu))
 			return -EBUSY;
 
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 */
 		kvm_debug("Set kvm ARM PMU irq: %d\n", irq);
 		vcpu->arch.pmu.irq_num = irq;
 		return 0;
@@ -1096,6 +2005,16 @@ int kvm_arm_pmu_v3_get_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 		if (!kvm_arm_pmu_irq_initialized(vcpu))
 			return -ENXIO;
 
+		/*
+		 * 在以下使用kvm_pmu->irq_num:
+		 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+		 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+		 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+		 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+		 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+		 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+		 */
 		irq = vcpu->arch.pmu.irq_num;
 		return put_user(irq, uaddr);
 	}
@@ -1118,10 +2037,20 @@ int kvm_arm_pmu_v3_has_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 	return -ENXIO;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1319| <<kvm_host_pmu_init>> if (!pmuv3_implemented(kvm_arm_pmu_get_pmuver_limit()))
+ *   - arch/arm64/kvm/pmu-emul.c|1853| <<kvm_arm_pmu_v3_set_attr>> u8 pmuver = kvm_arm_pmu_get_pmuver_limit();
+ *   - arch/arm64/kvm/sys_regs.c|1784| <<read_sanitised_id_aa64dfr0_el1>> kvm_arm_pmu_get_pmuver_limit());
+ *   - arch/arm64/kvm/sys_regs.c|1829| <<read_sanitised_id_dfr0_el1>> u8 perfmon = pmuver_to_perfmon(kvm_arm_pmu_get_pmuver_limit());
+ */
 u8 kvm_arm_pmu_get_pmuver_limit(void)
 {
 	u64 tmp;
 
+	/*
+	 * PMUVer是[11:8]
+	 */
 	tmp = read_sanitised_ftr_reg(SYS_ID_AA64DFR0_EL1);
 	tmp = cpuid_feature_cap_perfmon_field(tmp,
 					      ID_AA64DFR0_EL1_PMUVer_SHIFT,
@@ -1129,13 +2058,57 @@ u8 kvm_arm_pmu_get_pmuver_limit(void)
 	return FIELD_GET(ARM64_FEATURE_MASK(ID_AA64DFR0_EL1_PMUVer), tmp);
 }
 
+/*
+ * PMCR寄存器:
+ * PMCR_EL0, Performance Monitors Control Register
+ *
+ * Provides details of the Performance Monitors implementation, including the
+ * number of counters implemented, and configures and controls the counters.
+ *
+ * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+ * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+ */
 /**
  * kvm_vcpu_read_pmcr - Read PMCR_EL0 register for the vCPU
  * @vcpu: The vcpu pointer
  */
 u64 kvm_vcpu_read_pmcr(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 404 enum vcpu_sysreg {
+	 * ... ...
+	 * 417
+	 * 418         // Performance Monitors Registers
+	 * 419         PMCR_EL0,       // Control Register
+	 * 420         PMSELR_EL0,     // Event Counter Selection Register
+	 * 421         PMEVCNTR0_EL0,  // Event Counter Register (0-30)
+	 * 422         PMEVCNTR30_EL0 = PMEVCNTR0_EL0 + 30,
+	 * 423         PMCCNTR_EL0,    // Cycle Counter Register
+	 * 424         PMEVTYPER0_EL0, // Event Type Register (0-30) 
+	 * 425         PMEVTYPER30_EL0 = PMEVTYPER0_EL0 + 30,
+	 * 426         PMCCFILTR_EL0,  // Cycle Count Filter Register
+	 * 427         PMCNTENSET_EL0, // Count Enable Set Register
+	 * 428         PMINTENSET_EL1, // Interrupt Enable Set Register
+	 * 429         PMOVSSET_EL0,   // Overflow Flag Status Set Register
+	 * 430         PMUSERENR_EL0,  // User Enable Register
+	 */
 	u64 pmcr = __vcpu_sys_reg(vcpu, PMCR_EL0);
 
+	/*
+	 * PMCR寄存器:
+	 * PMCR_EL0, Performance Monitors Control Register
+	 *
+	 * Provides details of the Performance Monitors implementation, including the
+	 * number of counters implemented, and configures and controls the counters.
+	 *
+	 * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+	 * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+	 *
+	 * 在以下使用kvm_arch->pmcr_n:
+	 *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+	 *   - arch/arm64/kvm/pmu-emul.c|1969| <<kvm_vcpu_read_pmcr>> return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
+	 *   - arch/arm64/kvm/sys_regs.c|862| <<reset_pmu_reg>> u8 n = vcpu->kvm->arch.pmcr_n;
+	 *   - arch/arm64/kvm/sys_regs.c|1321| <<set_pmcr>> kvm->arch.pmcr_n = new_n;
+	 */
 	return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
 }
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index 31e49da86..6f4e6a9d5 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -838,6 +838,16 @@ static u64 reset_mpidr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)
 	 * of the GICv3 to be able to address each CPU directly when
 	 * sending IPIs.
 	 */
+	/*
+	 * MPIDR_LEVEL_SHIFT(0) = (((1 << 0) >> 1) << 3) = 0
+	 * MPIDR_LEVEL_SHIFT(1) = (((1 << 1) >> 1) << 3) = 8
+	 * MPIDR_LEVEL_SHIFT(2) = (((1 << 2) >> 1) << 3) = 16 (0x10)
+	 *
+	 * 1. 取出vcpu_id最后4位[0:3], 当作mpidr的[0:7]
+	 * 2. 取出vcpu_id的[4:11]这8位, 当作mpidr的[8:15]
+	 * 3. 取出vcpu_id的[12:19]这8位, 当作mpidr的[16:23]
+	 * 4. mpidr[31](最高位)是1
+	 */
 	mpidr = (vcpu->vcpu_id & 0x0f) << MPIDR_LEVEL_SHIFT(0);
 	mpidr |= ((vcpu->vcpu_id >> 4) & 0xff) << MPIDR_LEVEL_SHIFT(1);
 	mpidr |= ((vcpu->vcpu_id >> 12) & 0xff) << MPIDR_LEVEL_SHIFT(2);
@@ -859,6 +869,13 @@ static unsigned int pmu_visibility(const struct kvm_vcpu *vcpu,
 static u64 reset_pmu_reg(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)
 {
 	u64 mask = BIT(ARMV8_PMU_CYCLE_IDX);
+	/*
+	 * 在以下使用kvm_arch->pmcr_n:
+	 *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+	 *   - arch/arm64/kvm/pmu-emul.c|1969| <<kvm_vcpu_read_pmcr>> return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
+	 *   - arch/arm64/kvm/sys_regs.c|862| <<reset_pmu_reg>> u8 n = vcpu->kvm->arch.pmcr_n;
+	 *   - arch/arm64/kvm/sys_regs.c|1321| <<set_pmcr>> kvm->arch.pmcr_n = new_n;
+	 */
 	u8 n = vcpu->kvm->arch.pmcr_n;
 
 	if (n)
@@ -1002,6 +1019,21 @@ static bool access_pmceid(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 
 	get_access_mask(r, &mask, &shift);
 
+	/*
+	 * PMCEID1_EL0, Performance Monitors Common Event Identification register 1
+	 *
+	 * Defines which Common architectural events and Common microarchitectural
+	 * events are implemented, or counted, using PMU events in the ranges 0x0020 to
+	 * 0x003F and 0x4020 to 0x403F.
+	 *
+	 * 比如:
+	 * ID0 corresponds to common event (0x20) L2D_CACHE_ALLOCATE
+	 * ID1 corresponds to common event (0x21) BR_RETIRED
+	 * ... ...
+	 * ID12 corresponds to common event (0x2c) Reserved
+	 * ... ...
+	 * ID31 corresponds to common event (0x3f) STALL_SLOT
+	 */
 	pmceid = kvm_pmu_get_pmceid(vcpu, (p->Op2 & 1));
 	pmceid &= mask;
 	pmceid >>= shift;
@@ -1094,6 +1126,15 @@ static bool access_pmu_evcntr(struct kvm_vcpu *vcpu,
 	return true;
 }
 
+/*
+ * 在以下使用access_pmu_evtyper():
+ *   - arch/arm64/kvm/sys_regs.c|2574| <<global>> .access = access_pmu_evtyper, .reset = NULL },
+ *   - arch/arm64/kvm/sys_regs.c|2740| <<global>> { PMU_SYS_REG(PMCCFILTR_EL0), .access = access_pmu_evtyper,
+ *   - arch/arm64/kvm/sys_regs.c|3425| <<global>> { CP15_PMU_SYS_REG(DIRECT, 0, 9, 13, 1), .access = access_pmu_evtyper },
+ *   - arch/arm64/kvm/sys_regs.c|3519| <<global>> { CP15_PMU_SYS_REG(DIRECT, 0, 14, 15, 7), .access = access_pmu_evtyper },
+ *   - arch/arm64/kvm/sys_regs.c|1345| <<PMU_PMEVTYPER_EL0>> .access = access_pmu_evtyper, .reg = (PMEVTYPER0_EL0 + n), }
+ *   - arch/arm64/kvm/sys_regs.c|3376| <<PMU_PMEVTYPER>> .access = access_pmu_evtyper }
+ */
 static bool access_pmu_evtyper(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 			       const struct sys_reg_desc *r)
 {
@@ -1279,6 +1320,16 @@ static int get_pmcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 	return 0;
 }
 
+/*
+ * PMCR寄存器:
+ * PMCR_EL0, Performance Monitors Control Register
+ *
+ * Provides details of the Performance Monitors implementation, including the
+ * number of counters implemented, and configures and controls the counters.
+ *
+ * 比方N是[15:11]: Indicates the number of event counters implemented. Reads of
+ * this field from EL1 and EL0 return the value of AArch64-MDCR_EL2.HPMN.
+ */
 static int set_pmcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 		    u64 val)
 {
@@ -1287,6 +1338,13 @@ static int set_pmcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 
 	mutex_lock(&kvm->arch.config_lock);
 
+	/*
+	 * 在以下使用kvm_arch->pmcr_n:
+	 *   - arch/arm64/kvm/pmu-emul.c|1682| <<kvm_arm_set_pmu>> kvm->arch.pmcr_n = kvm_arm_pmu_get_max_counters(kvm);
+	 *   - arch/arm64/kvm/pmu-emul.c|1969| <<kvm_vcpu_read_pmcr>> return u64_replace_bits(pmcr, vcpu->kvm->arch.pmcr_n, ARMV8_PMU_PMCR_N);
+	 *   - arch/arm64/kvm/sys_regs.c|862| <<reset_pmu_reg>> u8 n = vcpu->kvm->arch.pmcr_n;
+	 *   - arch/arm64/kvm/sys_regs.c|1321| <<set_pmcr>> kvm->arch.pmcr_n = new_n;
+	 */
 	/*
 	 * The vCPU can't have more counters than the PMU hardware
 	 * implements. Ignore this error to maintain compatibility
@@ -4312,6 +4370,11 @@ int kvm_arm_sys_reg_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg
 				    sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|4403| <<kvm_arm_sys_reg_set_reg>> return kvm_sys_reg_set_user(vcpu, reg, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|361| <<vgic_v3_cpu_sysregs_uaccess>> return kvm_sys_reg_set_user(vcpu, &reg, gic_v3_icc_reg_descs, ARRAY_SIZE(gic_v3_icc_reg_descs));
+ */
 int kvm_sys_reg_set_user(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg,
 			 const struct sys_reg_desc table[], unsigned int num)
 {
diff --git a/arch/arm64/kvm/vgic/vgic.c b/arch/arm64/kvm/vgic/vgic.c
index abe29c7d8..41d26eb71 100644
--- a/arch/arm64/kvm/vgic/vgic.c
+++ b/arch/arm64/kvm/vgic/vgic.c
@@ -421,6 +421,14 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
  * level-sensitive interrupts.  You can think of the level parameter as 1
  * being HIGH and 0 being LOW and all devices being active-HIGH.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|455| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, timer_irq(timer_ctx), timer_ctx->irq.level, timer_ctx);
+ *   - arch/arm64/kvm/arm.c|1393| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu, irq_num, level, NULL);
+ *   - arch/arm64/kvm/arm.c|1401| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, NULL, irq_num, level, NULL);
+ *   - arch/arm64/kvm/pmu-emul.c|603| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, NULL, spi_id, level, NULL);
+ */
 int kvm_vgic_inject_irq(struct kvm *kvm, struct kvm_vcpu *vcpu,
 			unsigned int intid, bool level, void *owner)
 {
@@ -583,6 +591,11 @@ int kvm_vgic_get_map(struct kvm_vcpu *vcpu, unsigned int vintid)
  * Returns 0 if intid is not already used by another in-kernel device and the
  * owner is set, otherwise returns an error code.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1465| <<timer_irqs_are_valid>> if (kvm_vgic_set_owner(vcpu, irq, ctx))
+ *   - arch/arm64/kvm/pmu-emul.c|1552| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num, &vcpu->arch.pmu);
+ */
 int kvm_vgic_set_owner(struct kvm_vcpu *vcpu, unsigned int intid, void *owner)
 {
 	struct vgic_irq *irq;
diff --git a/arch/x86/events/amd/core.c b/arch/x86/events/amd/core.c
index 920e3a640..b53b9b20c 100644
--- a/arch/x86/events/amd/core.c
+++ b/arch/x86/events/amd/core.c
@@ -745,6 +745,11 @@ static void amd_pmu_check_overflow(void)
 	}
 }
 
+/*
+ * 在以下使用amd_pmu_enable_event():
+ *   - arch/x86/events/amd/core.c|1309| <<global>> struct x86_pmu amd_pmu.enable = amd_pmu_enable_event,
+ *   - arch/x86/events/amd/core.c|765| <<amd_pmu_enable_all>> amd_pmu_enable_event(cpuc->events[idx]);
+ */
 static void amd_pmu_enable_event(struct perf_event *event)
 {
 	x86_pmu_enable_event(event);
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index be01823b1..a835ee503 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1422,6 +1422,11 @@ int x86_perf_event_set_period(struct perf_event *event)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|750| <<amd_pmu_enable_event>> x86_pmu_enable_event(event);
+ *   - arch/x86/events/intel/core.c|4286| <<core_pmu_enable_event>> x86_pmu_enable_event(event);
+ */
 void x86_pmu_enable_event(struct perf_event *event)
 {
 	if (__this_cpu_read(cpu_hw_events.enabled))
@@ -1665,6 +1670,10 @@ static void x86_pmu_del(struct perf_event *event, int flags)
 	static_call_cond(x86_pmu_del)(event);
 }
 
+/*
+ * 在以下用到了x86_pmu_handle_irq()关键字:
+ *   - 
+ */
 int x86_pmu_handle_irq(struct pt_regs *regs)
 {
 	struct perf_sample_data data;
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index ac1182141..27e64dff3 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -1187,6 +1187,16 @@ static inline bool is_counter_pair(struct hw_perf_event *hwc)
 	return hwc->flags & PERF_X86_EVENT_PAIR;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|781| <<amd_pmu_v2_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|750| <<x86_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|1428| <<x86_pmu_enable_event>> __x86_pmu_enable_event(&event->hw, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2437| <<intel_pmu_nhm_workaround>> __x86_pmu_enable_event(&event->hw, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2868| <<intel_pmu_enable_event>> __x86_pmu_enable_event(hwc, enable_mask);
+ *   - arch/x86/events/intel/core.c|4301| <<core_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/zhaoxin/core.c|347| <<zhaoxin_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ */
 static inline void __x86_pmu_enable_event(struct hw_perf_event *hwc,
 					  u64 enable_mask)
 {
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4a68cb3eb..546eef875 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -84,13 +84,98 @@
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
+/*
+ * 在以下使用KVM_REQ_EVENT:
+ *   - arch/x86/kvm/i8259.c|64| <<pic_unlock>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/lapic.c|983| <<apic_update_ppr>> kvm_make_request(KVM_REQ_EVENT, apic->vcpu);
+ *   - arch/x86/kvm/lapic.c|1346| <<__apic_accept_irq>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/lapic.c|1368| <<__apic_accept_irq>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/lapic.c|1379| <<__apic_accept_irq>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/lapic.c|1496| <<apic_set_eoi>> kvm_make_request(KVM_REQ_EVENT, apic->vcpu);
+ *   - arch/x86/kvm/lapic.c|1511| <<kvm_apic_set_eoi_accelerated>> kvm_make_request(KVM_REQ_EVENT, apic->vcpu);
+ *   - arch/x86/kvm/lapic.c|3062| <<kvm_apic_set_state>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/smm.c|122| <<kvm_smm_changed>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/smm.c|138| <<process_smi>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/svm/nested.c|659| <<nested_vmcb02_prepare_control>> kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
+ *   - arch/x86/kvm/svm/nested.c|1062| <<nested_svm_vmexit>> kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
+ *   - arch/x86/kvm/svm/sev.c|4344| <<sev_handle_vmgexit>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2102| <<db_interception>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2446| <<svm_set_gif>> kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2599| <<iret_interception>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/svm/svm.c|3227| <<interrupt_window_interception>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/svm/svm.c|3695| <<svm_complete_interrupt_delivery>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/svm/svm.c|4089| <<svm_complete_interrupts>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/svm/svm.c|4099| <<svm_complete_interrupts>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|2165| <<vmx_preemption_timer_fn>> kvm_make_request(KVM_REQ_EVENT, &vmx->vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3570| <<nested_vmx_enter_non_root_mode>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3706| <<nested_vmx_run>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5379| <<handle_vmxoff>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6819| <<vmx_set_nested_state>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|1807| <<vmx_update_emulated_instruction>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4229| <<vmx_deliver_nested_posted_interrupt>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4293| <<vmx_deliver_interrupt>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5652| <<handle_interrupt_window>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5852| <<handle_nmi_window>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5878| <<handle_invalid_guest_state>> if (kvm_test_request(KVM_REQ_EVENT, vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|6941| <<vmx_sync_pir_to_irr>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7117| <<__vmx_complete_interrupts>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|637| <<kvm_multiple_exception>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|5268| <<kvm_vcpu_ioctl_interrupt>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|5283| <<kvm_vcpu_ioctl_interrupt>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|5629| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|8779| <<toggle_interruptibility>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|9445| <<x86_emulate_instruction>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|10720| <<process_nmi>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|10770| <<__kvm_vcpu_update_apicv>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|11082| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win ||
+ *   - arch/x86/kvm/x86.c|11165| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|11307| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|11726| <<__set_regs>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|11885| <<kvm_arch_vcpu_ioctl_set_mpstate>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|12040| <<__set_sregs>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|12495| <<kvm_vcpu_reset>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ *   - arch/x86/kvm/x86.c|13332| <<kvm_arch_dy_runnable>> kvm_test_request(KVM_REQ_EVENT, vcpu))
+ *   - arch/x86/kvm/x86.c|13402| <<kvm_set_rflags>> kvm_make_request(KVM_REQ_EVENT, vcpu);
+ */
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
+/*
+ * 在以下使用KVM_REQ_NMI:
+ *   - arch/x86/kvm/x86.c|805| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|5582| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|11012| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu)) process_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|13272| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *   - arch/x86/kvm/x86.c|13328| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ */
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|142| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.c|1038| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|5084| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *   - arch/x86/kvm/x86.c|11004| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu)) kvm_pmu_handle_event(vcpu);
+ */
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|123| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8325| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|11006| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu)) kvm_pmu_deliver_pmi(vcpu);
+ *   - arch/x86/kvm/x86.c|13284| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu)) return true;
+ */
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #ifdef CONFIG_KVM_SMM
+/*
+ * 在以下使用KVM_REQ_SMI:
+ *   - arch/x86/kvm/smm.h|145| <<kvm_inject_smi>> kvm_make_request(KVM_REQ_SMI, vcpu);
+ *   - arch/x86/kvm/x86.c|5430| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> if (kvm_check_request(KVM_REQ_SMI, vcpu)) process_smi(vcpu);
+ *   - arch/x86/kvm/x86.c|11009| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_SMI, vcpu)) process_smi(vcpu);
+ *   - arch/x86/kvm/x86.c|13278| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_SMI, vcpu) ||
+ *   - arch/x86/kvm/x86.c|13330| <<kvm_arch_dy_runnable>> kvm_test_request(KVM_REQ_SMI, vcpu) ||
+ */
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
 #endif
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
@@ -552,6 +637,16 @@ struct kvm_pmu {
 	u64 fixed_ctr_ctrl;
 	u64 fixed_ctr_ctrl_rsvd;
 	u64 global_ctrl;
+	/*
+	 * 在以下使用kvm_pmu->global_status:
+	 *   - arch/x86/kvm/pmu.c|116| <<__kvm_perf_overflow>> skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT, (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/pmu.c|119| <<__kvm_perf_overflow>> __set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/pmu.c|729| <<kvm_pmu_get_msr(MSR_CORE_PERF_GLOBAL_STATUS或MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>> msr_info->data = pmu->global_status;
+	 *   - arch/x86/kvm/pmu.c|770| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_STATUS或MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>> pmu->global_status = data;
+	 *   - arch/x86/kvm/pmu.c|795| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL或MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR)>> pmu->global_status &= ~data;
+	 *   - arch/x86/kvm/pmu.c|839| <<kvm_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|8327| <<vmx_handle_intel_pt_intr>> __set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT, (unsigned long *)&vcpu->arch.pmu.global_status);
+	 */
 	u64 global_status;
 	u64 counter_bitmask[2];
 	u64 global_ctrl_rsvd;
@@ -567,10 +662,38 @@ struct kvm_pmu {
 	 * filter changes.
 	 */
 	union {
+		/*
+		 * 在以下使用kvm_pmu->reprogram_pmi:
+		 *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+		 *   - arch/x86/kvm/pmu.c|502| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+		 *   - arch/x86/kvm/pmu.c|520| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|740| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+		 *   - arch/x86/kvm/pmu.c|1042| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+		 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+		 *
+		 * 在以下使用kvm_pmu->__reprogram_pmi:
+		 *   - arch/x86/kvm/pmu.c|510| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|1043| <<kvm_vm_ioctl_set_pmu_event_filter>> sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+		 *   - arch/x86/kvm/pmu.c|1046| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+		 */
 		DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
 		atomic64_t __reprogram_pmi;
 	};
 	DECLARE_BITMAP(all_valid_pmc_idx, X86_PMC_IDX_MAX);
+	/*
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|635| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|814| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|823| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|53| <<reprogram_fixed_counters>> __set_bit(KVM_FIXED_PMC_BASE_IDX + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|239| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|252| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|285| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|289| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|691| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	DECLARE_BITMAP(pmc_in_use, X86_PMC_IDX_MAX);
 
 	u64 ds_area;
@@ -592,6 +715,13 @@ struct kvm_pmu {
 	 * The gate to release perf_events not marked in
 	 * pmc_in_use only once in a vcpu time slice.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|601| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|811| <<kvm_pmu_reset>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|909| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|5083| <<kvm_arch_vcpu_load>> pmu->need_cleanup = true;
+	 */
 	bool need_cleanup;
 
 	/*
@@ -901,6 +1031,21 @@ struct kvm_vcpu_arch {
 		struct gfn_to_hva_cache cache;
 	} st;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|682| <<nested_vmcb02_prepare_control>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(vcpu->arch.l1_tsc_offset, svm->nested.ctl.tsc_offset, svm->tsc_ratio_msr);
+	 *   - arch/x86/kvm/svm/nested.c|1094| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/svm/svm.c|1167| <<svm_write_tsc_offset>> svm->vmcb01.ptr->control.tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2642| <<prepare_vmcs02>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(vcpu->arch.l1_tsc_offset, vmx_get_l2_tsc_offset(vcpu), vmx_get_l2_tsc_multiplier(vcpu));
+	 *   - arch/x86/kvm/vmx/nested.c|4870| <<nested_vmx_vmexit>> vcpu->arch.tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2558| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(host_tsc, vcpu->arch.l1_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2591| <<kvm_vcpu_write_tsc_offset>>  trace_kvm_write_tsc_offset(vcpu->vcpu_id, vcpu->arch.l1_tsc_offset,l1_offset);
+	 *   - arch/x86/kvm/x86.c|2594| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = l1_offset;
+	 *   - arch/x86/kvm/x86.c|2766| <<adjust_tsc_offset_guest>> u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3958| <<kvm_set_msr_common(MSR_IA32_TSC)>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|4300| <<kvm_get_msr_common(MSR_IA32_TSC)>> offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|5749| <<kvm_arch_tsc_get_attr(KVM_VCPU_TSC_OFFSET)>> if (put_user(vcpu->arch.l1_tsc_offset, uaddr))
+	 */
 	u64 l1_tsc_offset;
 	u64 tsc_offset; /* current tsc offset */
 	u64 last_guest_tsc;
@@ -1363,6 +1508,16 @@ struct kvm_arch {
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|572| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3127| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3133| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3299| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3379| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7050| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|12698| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	s64 kvmclock_offset;
 
 	/*
diff --git a/arch/x86/include/uapi/asm/kvm.h b/arch/x86/include/uapi/asm/kvm.h
index bf57a824f..5cdbfc01f 100644
--- a/arch/x86/include/uapi/asm/kvm.h
+++ b/arch/x86/include/uapi/asm/kvm.h
@@ -911,8 +911,26 @@ struct kvm_hyperv_eventfd {
 #define KVM_PMU_MASKED_ENTRY_EXCLUDE		(_BITULL(55))
 #define KVM_PMU_MASKED_ENTRY_UMASK_MASK_SHIFT	(56)
 
+/*
+ * 在以下使用KVM_VCPU_TSC_CTRL:
+ *   - arch/x86/include/uapi/asm/kvm.h|915| <<global>> #define KVM_VCPU_TSC_CTRL 0
+ *   - tools/arch/x86/include/uapi/asm/kvm.h|915| <<global>> #define KVM_VCPU_TSC_CTRL 0
+ *   - arch/x86/kvm/x86.c|5827| <<kvm_vcpu_ioctl_device_attr>> if (attr.group != KVM_VCPU_TSC_CTRL)
+ *   - tools/testing/selftests/kvm/system_counter_offset_test.c|31| <<check_preconditions>> __TEST_REQUIRE(!__vcpu_has_device_attr(vcpu, KVM_VCPU_TSC_CTRL,
+ *   - tools/testing/selftests/kvm/system_counter_offset_test.c|38| <<setup_system_counter>> vcpu_device_attr_set(vcpu, KVM_VCPU_TSC_CTRL, KVM_VCPU_TSC_OFFSET,
+ */
 /* for KVM_{GET,SET,HAS}_DEVICE_ATTR */
 #define KVM_VCPU_TSC_CTRL 0 /* control group for the timestamp counter (TSC) */
+/*
+ * 在以下使用KVM_VCPU_TSC_OFFSET:
+ *   - arch/x86/include/uapi/asm/kvm.h|916| <<global>> #define KVM_VCPU_TSC_OFFSET 0
+ *   - tools/arch/x86/include/uapi/asm/kvm.h|916| <<global>> #define KVM_VCPU_TSC_OFFSET 0
+ *   - arch/x86/kvm/x86.c|5747| <<kvm_arch_tsc_has_attr>> case KVM_VCPU_TSC_OFFSET:
+ *   - arch/x86/kvm/x86.c|5764| <<kvm_arch_tsc_get_attr>> case KVM_VCPU_TSC_OFFSET:
+ *   - arch/x86/kvm/x86.c|5785| <<kvm_arch_tsc_set_attr>> case KVM_VCPU_TSC_OFFSET: {
+ *   - tools/testing/selftests/kvm/system_counter_offset_test.c|32| <<check_preconditions>> KVM_VCPU_TSC_OFFSET),
+ *   - tools/testing/selftests/kvm/system_counter_offset_test.c|38| <<setup_system_counter>> vcpu_device_attr_set(vcpu, KVM_VCPU_TSC_CTRL, KVM_VCPU_TSC_OFFSET,
+ */
 #define   KVM_VCPU_TSC_OFFSET 0 /* attribute for the TSC offset */
 
 /* x86-specific KVM_EXIT_HYPERCALL flags. */
diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index a1efa7907..bd8cf792b 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -33,6 +33,10 @@
 #define KVM_FEATURE_POLL_CONTROL	12
 #define KVM_FEATURE_PV_SCHED_YIELD	13
 #define KVM_FEATURE_ASYNC_PF_INT	14
+/*
+ * 在以下使用KVM_FEATURE_MSI_EXT_DEST_ID:
+ *   - arch/x86/kernel/kvm.c|924| <<kvm_msi_ext_dest_id>> return kvm_para_has_feature(KVM_FEATURE_MSI_EXT_DEST_ID);
+ */
 #define KVM_FEATURE_MSI_EXT_DEST_ID	15
 #define KVM_FEATURE_HC_MAP_GPA_RANGE	16
 #define KVM_FEATURE_MIGRATION_CONTROL	17
diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c
index 557318145..12b4aaf13 100644
--- a/arch/x86/kernel/apic/vector.c
+++ b/arch/x86/kernel/apic/vector.c
@@ -139,6 +139,11 @@ static void apic_update_irq_cfg(struct irq_data *irqd, unsigned int vector,
 			    apicd->hw_irq_cfg.dest_apicid);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|263| <<assign_vector_locked>> apic_update_vector(irqd, vector, cpu);
+ *   - arch/x86/kernel/apic/vector.c|345| <<assign_managed_vector>> apic_update_vector(irqd, vector, cpu);
+ */
 static void apic_update_vector(struct irq_data *irqd, unsigned int newvec,
 			       unsigned int newcpu)
 {
@@ -320,6 +325,11 @@ assign_irq_vector_policy(struct irq_data *irqd, struct irq_alloc_info *info)
 	return reserve_irq_vector(irqd);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|443| <<activate_managed>> ret = assign_managed_vector(irqd, vector_searchmask);
+ *   - arch/x86/kernel/apic/vector.c|884| <<apic_set_affinity>> err = assign_managed_vector(irqd, vector_searchmask);
+ */
 static int
 assign_managed_vector(struct irq_data *irqd, const struct cpumask *dest)
 {
@@ -870,6 +880,15 @@ void lapic_offline(void)
 	unlock_vector_lock();
 }
 
+/*
+ * 925 static struct irq_chip lapic_controller = {
+ * 926         .name                   = "APIC",
+ * 927         .irq_ack                = apic_ack_edge,
+ * 928         .irq_set_affinity       = apic_set_affinity,
+ * 929         .irq_compose_msi_msg    = x86_vector_msi_compose_msg,
+ * 930         .irq_retrigger          = apic_retrigger_irq,
+ * 931 };
+ */
 static int apic_set_affinity(struct irq_data *irqd,
 			     const struct cpumask *dest, bool force)
 {
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index 263f8aed4..2d5e3a975 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -919,6 +919,21 @@ static void __init kvm_apic_init(void)
 #endif
 }
 
+/*
+ * 1002 const __initconst struct hypervisor_x86 x86_hyper_kvm = {
+ * 1003         .name                           = "KVM",
+ * 1004         .detect                         = kvm_detect,
+ * 1005         .type                           = X86_HYPER_KVM,
+ * 1006         .init.guest_late_init           = kvm_guest_init,
+ * 1007         .init.x2apic_available          = kvm_para_available,
+ * 1008         .init.msi_ext_dest_id           = kvm_msi_ext_dest_id,
+ * 1009         .init.init_platform             = kvm_init_platform,
+ * 1010 #if defined(CONFIG_AMD_MEM_ENCRYPT)
+ * 1011         .runtime.sev_es_hcall_prepare   = kvm_sev_es_hcall_prepare,
+ * 1012         .runtime.sev_es_hcall_finish    = kvm_sev_es_hcall_finish,
+ * 1013 #endif
+ * 1014 };
+ */
 static bool __init kvm_msi_ext_dest_id(void)
 {
 	return kvm_para_has_feature(KVM_FEATURE_MSI_EXT_DEST_ID);
diff --git a/arch/x86/kernel/nmi.c b/arch/x86/kernel/nmi.c
index ed163c8c8..bb6922a70 100644
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@ -129,6 +129,13 @@ static void nmi_check_duration(struct nmiaction *action, u64 duration)
 		action->handler, duration, decimal_msecs);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|231| <<pci_serr_error>> if (nmi_handle(NMI_SERR, regs))
+ *   - arch/x86/kernel/nmi.c|254| <<io_check_error>> if (nmi_handle(NMI_IO_CHECK, regs))
+ *   - arch/x86/kernel/nmi.c|299| <<unknown_nmi_error>> handled = nmi_handle(NMI_UNKNOWN, regs);
+ *   - arch/x86/kernel/nmi.c|351| <<default_do_nmi>> handled = nmi_handle(NMI_LOCAL, regs);
+ */
 static int nmi_handle(unsigned int type, struct pt_regs *regs)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 47a46283c..baa73d339 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -134,6 +134,21 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 	 * to be reprogrammed, e.g. if a PMI for the previous event races with
 	 * KVM's handling of a related guest WRMSR.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|502| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|520| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|740| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1042| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|510| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1043| <<kvm_vm_ioctl_set_pmu_event_filter>> sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1046| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
 		return;
 
@@ -227,6 +242,10 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 	return 0;
 }
 
+/*
+ * called by:
+ *    - arch/x86/kvm/pmu.c|462| <<reprogram_counter>> emulate_overflow = pmc_pause_counter(pmc);
+ */
 static bool pmc_pause_counter(struct kvm_pmc *pmc)
 {
 	u64 counter = pmc->counter;
@@ -253,6 +272,10 @@ static bool pmc_pause_counter(struct kvm_pmc *pmc)
 	return pmc->counter < prev_counter;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|485| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+ */
 static bool pmc_resume_counter(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event)
@@ -275,6 +298,11 @@ static bool pmc_resume_counter(struct kvm_pmc *pmc)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|292| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+ *   - arch/x86/kvm/pmu.c|488| <<reprogram_counter>> pmc_release_perf_event(pmc);
+ */
 static void pmc_release_perf_event(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
@@ -285,6 +313,11 @@ static void pmc_release_perf_event(struct kvm_pmc *pmc)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|774| <<kvm_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/pmu.c|859| <<kvm_pmu_cleanup>> pmc_stop_counter(pmc);
+ */
 static void pmc_stop_counter(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
@@ -303,6 +336,12 @@ static void pmc_update_sample_period(struct kvm_pmc *pmc)
 			  get_sample_period(pmc, pmc->counter));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|159| <<amd_pmu_set_msr>> pmc_write_counter(pmc, data);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|393| <<intel_pmu_set_msr>> pmc_write_counter(pmc, data);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|396| <<intel_pmu_set_msr>> pmc_write_counter(pmc, data);
+ */
 void pmc_write_counter(struct kvm_pmc *pmc, u64 val)
 {
 	/*
@@ -447,6 +486,10 @@ static bool pmc_event_is_allowed(struct kvm_pmc *pmc)
 	       check_pmu_event_filter(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|531| <<kvm_pmu_handle_event>> if (reprogram_counter(pmc))
+ */
 static int reprogram_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -455,6 +498,9 @@ static int reprogram_counter(struct kvm_pmc *pmc)
 	bool emulate_overflow;
 	u8 fixed_ctr_ctrl;
 
+	/*
+	 * 只在此处调用
+	 */
 	emulate_overflow = pmc_pause_counter(pmc);
 
 	if (!pmc_event_is_allowed(pmc))
@@ -492,6 +538,18 @@ static int reprogram_counter(struct kvm_pmc *pmc)
 				     eventsel & ARCH_PERFMON_EVENTSEL_INT);
 }
 
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|142| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.c|1038| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|5084| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *   - arch/x86/kvm/x86.c|11004| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu)) kvm_pmu_handle_event(vcpu);
+ *
+ * 处理KVM_REQ_PMU:
+ *   - arch/x86/kvm/x86.c|11005| <<vcpu_enter_guest(KVM_REQ_PMU)>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	DECLARE_BITMAP(bitmap, X86_PMC_IDX_MAX);
@@ -499,6 +557,21 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 	struct kvm_pmc *pmc;
 	int bit;
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|502| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|520| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|740| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1042| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|510| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1043| <<kvm_vm_ioctl_set_pmu_event_filter>> sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1046| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
 
 	/*
@@ -604,10 +677,20 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * 处理KVM_REQ_PMI:
+ *   - arch/x86/kvm/x86.c|11007| <<vcpu_enter_guest>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
+		/*
+		 * 只有intel_pmu_deliver_pmi()
+		 */
 		kvm_pmu_call(deliver_pmi)(vcpu);
+		/*
+		 * 这个函数是核心
+		 */
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 	}
 }
@@ -626,11 +709,28 @@ bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 	       kvm_pmu_call(is_valid_msr)(vcpu, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|798| <<kvm_pmu_set_msr>> kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+ */
 static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 	struct kvm_pmc *pmc = kvm_pmu_call(msr_idx_to_pmc)(vcpu, msr);
 
+	/*
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|635| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|814| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|823| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|53| <<reprogram_fixed_counters>> __set_bit(KVM_FIXED_PMC_BASE_IDX + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|239| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|252| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|285| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|289| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|691| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	if (pmc)
 		__set_bit(pmc->idx, pmu->pmc_in_use);
 }
@@ -684,6 +784,16 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		if (data & pmu->global_status_rsvd)
 			return 1;
 
+		/*
+		 * 在以下使用kvm_pmu->global_status:
+		 *   - arch/x86/kvm/pmu.c|116| <<__kvm_perf_overflow>> skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT, (unsigned long *)&pmu->global_status);
+		 *   - arch/x86/kvm/pmu.c|119| <<__kvm_perf_overflow>> __set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
+		 *   - arch/x86/kvm/pmu.c|729| <<kvm_pmu_get_msr(MSR_CORE_PERF_GLOBAL_STATUS或MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>> msr_info->data = pmu->global_status;
+		 *   - arch/x86/kvm/pmu.c|770| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_STATUS或MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>> pmu->global_status = data;
+		 *   - arch/x86/kvm/pmu.c|795| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL或MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR)>> pmu->global_status &= ~data;
+		 *   - arch/x86/kvm/pmu.c|839| <<kvm_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+		 *   - arch/x86/kvm/vmx/vmx.c|8327| <<vmx_handle_intel_pt_intr>> __set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT, (unsigned long *)&vcpu->arch.pmu.global_status);
+		 */
 		pmu->global_status = data;
 		break;
 	case MSR_AMD64_PERF_CNTR_GLOBAL_CTL:
@@ -708,6 +818,16 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 			return 1;
 		fallthrough;
 	case MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR:
+		/*
+		 * 在以下使用kvm_pmu->global_status:
+		 *   - arch/x86/kvm/pmu.c|116| <<__kvm_perf_overflow>> skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT, (unsigned long *)&pmu->global_status);
+		 *   - arch/x86/kvm/pmu.c|119| <<__kvm_perf_overflow>> __set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
+		 *   - arch/x86/kvm/pmu.c|729| <<kvm_pmu_get_msr(MSR_CORE_PERF_GLOBAL_STATUS或MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>> msr_info->data = pmu->global_status;
+		 *   - arch/x86/kvm/pmu.c|770| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_STATUS或MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>> pmu->global_status = data;
+		 *   - arch/x86/kvm/pmu.c|795| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL或MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR)>> pmu->global_status &= ~data;
+		 *   - arch/x86/kvm/pmu.c|839| <<kvm_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+		 *   - arch/x86/kvm/vmx/vmx.c|8327| <<vmx_handle_intel_pt_intr>> __set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT, (unsigned long *)&vcpu->arch.pmu.global_status);
+		 */
 		if (!msr_info->host_initiated)
 			pmu->global_status &= ~data;
 		break;
@@ -727,6 +847,21 @@ static void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 
 	pmu->need_cleanup = false;
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|502| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|520| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|740| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1042| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|510| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1043| <<kvm_vm_ioctl_set_pmu_event_filter>> sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1046| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
 
 	kvm_for_each_pmc(pmu, pmc, i, pmu->all_valid_pmc_idx) {
@@ -800,6 +935,10 @@ void kvm_pmu_init(struct kvm_vcpu *vcpu)
 	kvm_pmu_refresh(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|602| <<kvm_pmu_handle_event>> kvm_pmu_cleanup(vcpu);
+ */
 /* Release perf_events for vPMCs that have been unused for a full time slice.  */
 void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
@@ -820,6 +959,19 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 
 	kvm_pmu_call(cleanup)(vcpu);
 
+	/*
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|635| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|814| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|823| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|53| <<reprogram_fixed_counters>> __set_bit(KVM_FIXED_PMC_BASE_IDX + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|239| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|252| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|285| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|289| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|691| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
 }
 
@@ -1029,6 +1181,21 @@ int kvm_vm_ioctl_set_pmu_event_filter(struct kvm *kvm, void __user *argp)
 	mutex_unlock(&kvm->lock);
 	synchronize_srcu_expedited(&kvm->srcu);
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|502| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|520| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|740| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1042| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|510| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1043| <<kvm_vm_ioctl_set_pmu_event_filter>> sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1046| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
 		     sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
 
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index ad89d0bd6..ec22ae0a8 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -227,12 +227,48 @@ static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 		perf_get_hw_event_config(PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|844| <<kvm_pmu_incr_counter>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|168| <<amd_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|58| <<reprogram_fixed_counters>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|408| <<intel_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+ */
 static inline void kvm_pmu_request_counter_reprogram(struct kvm_pmc *pmc)
 {
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|502| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|520| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|740| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1042| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|510| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1043| <<kvm_vm_ioctl_set_pmu_event_filter>> sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1046| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|142| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|1038| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|5084| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *   - arch/x86/kvm/x86.c|11004| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu)) kvm_pmu_handle_event(vcpu);
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|709| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_CTRL)>> reprogram_counters(pmu, diff);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|368| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> reprogram_counters(pmu, diff);
+ */
 static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 {
 	int bit;
@@ -240,8 +276,32 @@ static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 	if (!diff)
 		return;
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|502| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|520| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|740| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1042| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|510| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1043| <<kvm_vm_ioctl_set_pmu_event_filter>> sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1046| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	for_each_set_bit(bit, (unsigned long *)&diff, X86_PMC_IDX_MAX)
 		set_bit(bit, pmu->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|142| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|1038| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|5084| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *   - arch/x86/kvm/x86.c|11004| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu)) kvm_pmu_handle_event(vcpu);
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
 }
 
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index 83382a4d1..99340e9ae 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -34,6 +34,10 @@
 
 #define MSR_PMC_FULL_WIDTH_BIT      (MSR_IA32_PMC0 - MSR_IA32_PERFCTR0)
 
+/*
+ * 处理MSR_CORE_PERF_FIXED_CTR_CTRL:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|355| <<intel_pmu_set_msr>> reprogram_fixed_counters(pmu, data);
+ */
 static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 {
 	struct kvm_pmc *pmc;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index c983c8e43..0ae733d53 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1880,6 +1880,12 @@ static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 	return kvm_x86_call(set_msr)(vcpu, &msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1954| <<kvm_set_msr_with_filter>> return kvm_set_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|1965| <<kvm_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|2220| <<do_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, *data, true);
+ */
 static int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,
 				     u32 index, u64 data, bool host_initiated)
 {
@@ -2202,6 +2208,10 @@ static int do_get_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 	return kvm_get_msr_ignored_check(vcpu, index, data, true);
 }
 
+/*
+ * 处理KVM_SET_MSRS:
+ *   - arch/x86/kvm/x86.c|6051| <<kvm_arch_vcpu_ioctl>> r = msr_io(vcpu, argp, do_set_msr, 0);
+ */
 static int do_set_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 {
 	u64 val;
@@ -2358,6 +2368,35 @@ static uint32_t div_frac(uint32_t dividend, uint32_t divisor)
 	return dividend;
 }
 
+/*
+ * 注释:
+ *
+ * https://lore.kernel.org/all/20240522001817.619072-12-dwmw2@infradead.org/
+ *
+ * + * Calculate scaling factors to be applied with pvclock_scale_delta().
+ * + *
+ * + * The output of this function is a fixed-point factor which is used to
+ * + * scale a tick count at base_hz, to a tick count at scaled_hz, within
+ * + * the limitations of the Xen/KVM pvclock ABI.
+ * + *
+ * + * Mathematically, the factor is (*pmultiplier) >> (32 - *pshift).
+ * + *
+ * + * Working backwards, the div_frac() function divides (dividend << 32) by
+ * + * the given divisor, in other words giving dividend/divisor in the form
+ * + * of a 32-bit fixed-point fraction in the range 0 to 0x0.FFFFFFFF, which
+ * + * is (*pmultiplier >> 32).
+ * + *
+ * + * The rest of the function is shifting the scaled_hz and base_hz left or
+ * + * right as appropriate to ensure maximal precision within the constraints.
+ * + *
+ * + * The first constraint is that the result of the division *must* be less
+ * + * than 1, which means the dividend (derived from scaled_hz) must be greater
+ * + * than the divisor (derived from base_hz).
+ * + *
+ * + * The second constraint is that for optimal precision, the dividend (scaled)
+ * + * shouldn't be more than twice the divisor (base) — i.e. the top bit ought
+ * + * to be set in the resulting *pmultiplier.
+ */
 static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 			       s8 *pshift, u32 *pmultiplier)
 {
@@ -2534,6 +2573,15 @@ static inline u64 __scale_tsc(u64 ratio, u64 tsc)
 	return mul_u64_u64_shr(tsc, ratio, kvm_caps.tsc_scaling_ratio_frac_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2551| <<kvm_compute_l1_tsc_offset>> tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2559| <<kvm_read_l1_tsc>> kvm_scale_tsc(host_tsc, vcpu->arch.l1_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2774| <<adjust_tsc_offset_host>> adjustment = kvm_scale_tsc((u64) adjustment,
+ *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> tgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz,
+ *   - arch/x86/kvm/x86.c|4324| <<kvm_get_msr_common>> msr_info->data = kvm_scale_tsc(rdtsc(), ratio) + offset;
+ *   - arch/x86/kvm/x86.c|5800| <<kvm_arch_tsc_set_attr>> tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio) + offset;
+ */
 u64 kvm_scale_tsc(u64 tsc, u64 ratio)
 {
 	u64 _tsc = tsc;
@@ -2798,6 +2846,12 @@ static u64 read_tsc(void)
 	return last;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2850| <<do_kvmclock_base>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+ *   - arch/x86/kvm/x86.c|2873| <<do_monotonic>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+ *   - arch/x86/kvm/x86.c|2893| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+ */
 static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 			  int *mode)
 {
@@ -2860,8 +2914,23 @@ static int do_kvmclock_base(s64 *t, u64 *tsc_timestamp)
  * This calculates CLOCK_MONOTONIC at the time of the TSC snapshot, with
  * no boot time offset.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2928| <<kvm_get_monotonic_and_clockread>> return gtod_is_based_on_tsc(do_monotonic(kernel_ns, tsc_timestamp));
+ */
 static int do_monotonic(s64 *t, u64 *tsc_timestamp)
 {
+	/*
+	 * 2234 struct pvclock_gtod_data {
+	 * 2235         seqcount_t      seq;
+	 * 2236
+	 * 2237         struct pvclock_clock clock; // extract of a clocksource struct
+	 * 2238         struct pvclock_clock raw_clock; // extract of a clocksource struct
+	 * 2239
+	 * 2240         ktime_t         offs_boot;
+	 * 2241         u64             wall_time_sec;
+	 * 2242 };
+	 */
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 	unsigned long seq;
 	int mode;
@@ -2988,6 +3057,13 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3083| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|7027| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|9533| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|12701| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -3275,6 +3351,16 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	}
 
 	vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|572| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3127| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3133| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3299| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3379| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7050| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|12698| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
 	vcpu->last_guest_tsc = tsc_timestamp;
 
@@ -7002,7 +7088,13 @@ static int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)
 		return -EINVAL;
 
 	kvm_hv_request_tsc_page_update(kvm);
+	/*
+	 * 一些锁操作
+	 */
 	kvm_start_pvclock_update(kvm);
+	/*
+	 * 更新master clock!
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
 
 	/*
@@ -10909,6 +11001,15 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		}
 		if (kvm_check_request(KVM_REQ_STEAL_UPDATE, vcpu))
 			record_steal_time(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/kvm/pmu.c|142| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|1038| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|5084| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *   - arch/x86/kvm/x86.c|11004| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu)) kvm_pmu_handle_event(vcpu);
+		 */
 		if (kvm_check_request(KVM_REQ_PMU, vcpu))
 			kvm_pmu_handle_event(vcpu);
 		if (kvm_check_request(KVM_REQ_PMI, vcpu))
diff --git a/drivers/block/xen-blkfront.c b/drivers/block/xen-blkfront.c
index 59ce113b8..83382d7b5 100644
--- a/drivers/block/xen-blkfront.c
+++ b/drivers/block/xen-blkfront.c
@@ -1299,6 +1299,15 @@ static void blkif_free_ring(struct blkfront_ring_info *rinfo)
 	rinfo->evtchn = rinfo->irq = 0;
 }
 
+/*
+ * clled by:
+ *   - drivers/block/xen-blkfront.c|1711| <<setup_blkring>> blkif_free(info, 0);
+ *   - drivers/block/xen-blkfront.c|1891| <<talk_to_blkback>> blkif_free(info, 0);
+ *   - drivers/block/xen-blkfront.c|2108| <<blkfront_resume>> blkif_free(info, info->connected == BLKIF_STATE_CONNECTED);
+ *   - drivers/block/xen-blkfront.c|2381| <<blkfront_connect>> blkif_free(info, 0);
+ *   - drivers/block/xen-blkfront.c|2412| <<blkfront_connect>> blkif_free(info, 0);
+ *   - drivers/block/xen-blkfront.c|2484| <<blkfront_remove>> blkif_free(info, 0);
+ */
 static void blkif_free(struct blkfront_info *info, int suspend)
 {
 	unsigned int i;
diff --git a/drivers/iommu/iommufd/pages.c b/drivers/iommu/iommufd/pages.c
index 117f644a0..4647bf2b6 100644
--- a/drivers/iommu/iommufd/pages.c
+++ b/drivers/iommu/iommufd/pages.c
@@ -846,6 +846,12 @@ static int update_mm_locked_vm(struct iopt_pages *pages, unsigned long npages,
 	}
 
 	mmap_write_lock(pages->source_mm);
+	/*
+	 * called by:
+	 *   - drivers/iommu/iommufd/pages.c|849| <<update_mm_locked_vm>> rc = __account_locked_vm(pages->source_mm, npages, inc, pages->source_task, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|420| <<mm_lock_acct>> ret = __account_locked_vm(mm, abs(npage), npage > 0, task, lock_cap);
+	 *   - mm/util.c|567| <<account_locked_vm>> ret = __account_locked_vm(mm, pages, inc, current, capable(CAP_IPC_LOCK));
+	 */
 	rc = __account_locked_vm(pages->source_mm, npages, inc,
 				 pages->source_task, false);
 	mmap_write_unlock(pages->source_mm);
diff --git a/drivers/pci/bus.c b/drivers/pci/bus.c
index 55c853686..8bf93fc04 100644
--- a/drivers/pci/bus.c
+++ b/drivers/pci/bus.c
@@ -156,6 +156,10 @@ static struct pci_bus_region pci_high = {(pci_bus_addr_t) 0x100000000ULL,
  * addresses of resources we allocate, e.g., we may need a resource that
  * can be mapped by a 32-bit BAR.
  */
+/*
+ * called by:
+ *   - drivers/pci/bus.c|213| <<pci_bus_alloc_from_region>> pci_clip_resource_to_region(bus, &avail, region);
+ */
 static void pci_clip_resource_to_region(struct pci_bus *bus,
 					struct resource *res,
 					struct pci_bus_region *region)
@@ -174,6 +178,15 @@ static void pci_clip_resource_to_region(struct pci_bus *bus,
 		pcibios_bus_to_resource(bus, res, &r);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/bus.c|279| <<pci_bus_alloc_resource>> rc = pci_bus_alloc_from_region(bus, res, size, align, min,
+ *                type_mask, alignf, alignf_data, &pci_high);
+ *   - drivers/pci/bus.c|285| <<pci_bus_alloc_resource>> return pci_bus_alloc_from_region(bus, res, size, align, min,
+ *                type_mask, alignf, alignf_data, &pci_64_bit);
+ *   - drivers/pci/bus.c|291| <<pci_bus_alloc_resource>> return pci_bus_alloc_from_region(bus, res, size, align, min,
+ *                type_mask, alignf, alignf_data, &pci_32_bit);
+ */
 static int pci_bus_alloc_from_region(struct pci_bus *bus, struct resource *res,
 		resource_size_t size, resource_size_t align,
 		resource_size_t min, unsigned long type_mask,
@@ -204,6 +217,9 @@ static int pci_bus_alloc_from_region(struct pci_bus *bus, struct resource *res,
 			continue;
 
 		avail = *r;
+		/*
+		 * 只在此处调用
+		 */
 		pci_clip_resource_to_region(bus, &avail, region);
 
 		/*
@@ -245,6 +261,27 @@ static int pci_bus_alloc_from_region(struct pci_bus *bus, struct resource *res,
  * alignment and type, try to find an acceptable resource allocation
  * for a specific device resource.
  */
+/*
+ * called by:
+ *   - drivers/char/agp/intel-gtt.c|1008| <<intel_alloc_chipset_flush_resource>> ret = pci_bus_alloc_resource(intel_private.bridge_dev->bus,
+ *            &intel_private.ifp_resource, PAGE_SIZE, PAGE_SIZE, PCIBIOS_MIN_MEM, 0, pcibios_align_resource, intel_private.bridge_dev);
+ *   - drivers/gpu/drm/i915/soc/intel_gmch.c|62| <<intel_alloc_mchbar_resource>> ret = pci_bus_alloc_resource(i915->gmch.pdev->bus,
+ *            &i915->gmch.mch_res, MCHBAR_SIZE, MCHBAR_SIZE, PCIBIOS_MIN_MEM, 0, pcibios_align_resource, i915->gmch.pdev);
+ *   - drivers/pci/setup-res.c|279| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+ *            IORESOURCE_PREFETCH | IORESOURCE_MEM_64, pcibios_align_resource, dev);
+ *   - drivers/pci/setup-res.c|291| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+ *            IORESOURCE_PREFETCH, pcibios_align_resource, dev);
+ *   - drivers/pci/setup-res.c|305| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+ *            0, pcibios_align_resource, dev);
+ *   - drivers/pcmcia/rsrc_iodyn.c|70| <<__iodyn_find_io_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+ *            0, pcmcia_align, &data);
+ *   - drivers/pcmcia/rsrc_nonstatic.c|702| <<__nonstatic_find_io_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+ *            0, pcmcia_align, &data);
+ *   - drivers/pcmcia/rsrc_nonstatic.c|836| <<nonstatic_find_mem_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+ *            0, pcmcia_align, &data);
+ *   - drivers/staging/vme_user/vme_tsi148.c|754| <<tsi148_alloc_resource>> retval = pci_bus_alloc_resource(pdev->bus, &image->bus_resource,
+ *            size, 0x10000, PCIBIOS_MIN_MEM, 0, NULL, NULL);
+ */
 int pci_bus_alloc_resource(struct pci_bus *bus, struct resource *res,
 		resource_size_t size, resource_size_t align,
 		resource_size_t min, unsigned long type_mask,
@@ -254,6 +291,15 @@ int pci_bus_alloc_resource(struct pci_bus *bus, struct resource *res,
 #ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
 	int rc;
 
+	/*
+	 * called by:
+	 *   - drivers/pci/bus.c|279| <<pci_bus_alloc_resource>> rc = pci_bus_alloc_from_region(bus, res, size, align, min,
+	 *                type_mask, alignf, alignf_data, &pci_high);
+	 *   - drivers/pci/bus.c|285| <<pci_bus_alloc_resource>> return pci_bus_alloc_from_region(bus, res, size, align, min,
+	 *                type_mask, alignf, alignf_data, &pci_64_bit);
+	 *   - drivers/pci/bus.c|291| <<pci_bus_alloc_resource>> return pci_bus_alloc_from_region(bus, res, size, align, min,
+	 *                type_mask, alignf, alignf_data, &pci_32_bit);
+	 */
 	if (res->flags & IORESOURCE_MEM_64) {
 		rc = pci_bus_alloc_from_region(bus, res, size, align, min,
 					       type_mask, alignf, alignf_data,
diff --git a/drivers/pci/setup-bus.c b/drivers/pci/setup-bus.c
index 23082bc0c..394b7cd59 100644
--- a/drivers/pci/setup-bus.c
+++ b/drivers/pci/setup-bus.c
@@ -212,6 +212,10 @@ static inline void reset_resource(struct resource *res)
  * Walk through each element of the realloc_head and try to procure additional
  * resources for the element, provided the element is in the head list.
  */
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|481| <<__assign_resources_sorted>> reassign_resources_sorted(realloc_head, head);
+ */
 static void reassign_resources_sorted(struct list_head *realloc_head,
 				      struct list_head *head)
 {
diff --git a/drivers/pci/setup-res.c b/drivers/pci/setup-res.c
index c6d933ddf..ebd47e72e 100644
--- a/drivers/pci/setup-res.c
+++ b/drivers/pci/setup-res.c
@@ -269,6 +269,27 @@ static int __pci_assign_resource(struct pci_bus *bus, struct pci_dev *dev,
 
 	min = (res->flags & IORESOURCE_IO) ? PCIBIOS_MIN_IO : PCIBIOS_MIN_MEM;
 
+	/*
+	 * called by:
+	 *   - drivers/char/agp/intel-gtt.c|1008| <<intel_alloc_chipset_flush_resource>> ret = pci_bus_alloc_resource(intel_private.bridge_dev->bus,
+	 *            &intel_private.ifp_resource, PAGE_SIZE, PAGE_SIZE, PCIBIOS_MIN_MEM, 0, pcibios_align_resource, intel_private.bridge_dev);
+	 *   - drivers/gpu/drm/i915/soc/intel_gmch.c|62| <<intel_alloc_mchbar_resource>> ret = pci_bus_alloc_resource(i915->gmch.pdev->bus,
+	 *            &i915->gmch.mch_res, MCHBAR_SIZE, MCHBAR_SIZE, PCIBIOS_MIN_MEM, 0, pcibios_align_resource, i915->gmch.pdev);
+	 *   - drivers/pci/setup-res.c|279| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+	 *            IORESOURCE_PREFETCH | IORESOURCE_MEM_64, pcibios_align_resource, dev);
+	 *   - drivers/pci/setup-res.c|291| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+	 *            IORESOURCE_PREFETCH, pcibios_align_resource, dev);
+	 *   - drivers/pci/setup-res.c|305| <<__pci_assign_resource>> ret = pci_bus_alloc_resource(bus, res, size, align, min,
+	 *            0, pcibios_align_resource, dev);
+	 *   - drivers/pcmcia/rsrc_iodyn.c|70| <<__iodyn_find_io_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+	 *            0, pcmcia_align, &data);
+	 *   - drivers/pcmcia/rsrc_nonstatic.c|702| <<__nonstatic_find_io_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+	 *            0, pcmcia_align, &data);
+	 *   - drivers/pcmcia/rsrc_nonstatic.c|836| <<nonstatic_find_mem_region>> ret = pci_bus_alloc_resource(s->cb_dev->bus, res, num, 1, min,
+	 *            0, pcmcia_align, &data);
+	 *   - drivers/staging/vme_user/vme_tsi148.c|754| <<tsi148_alloc_resource>> retval = pci_bus_alloc_resource(pdev->bus, &image->bus_resource,
+	 *            size, 0x10000, PCIBIOS_MIN_MEM, 0, NULL, NULL);
+	 */
 	/*
 	 * First, try exact prefetching match.  Even if a 64-bit
 	 * prefetchable bridge window is below 4GB, we can't put a 32-bit
@@ -345,6 +366,11 @@ int pci_assign_resource(struct pci_dev *dev, int resno)
 	size = resource_size(res);
 	ret = _pci_assign_resource(dev, resno, size, align);
 
+	/*
+	 * 例子:
+	 * [   76.294540] pci 0000:01:00.0: BAR 0 [io  size 0x0040]: can't assign; no space
+	 * [   76.295401] pci 0000:01:00.0: BAR 0 [io  size 0x0040]: failed to assign
+	 */
 	/*
 	 * If we failed to assign anything, let's try the address
 	 * where firmware left it.  That at least has a chance of
@@ -370,6 +396,10 @@ int pci_assign_resource(struct pci_dev *dev, int resno)
 }
 EXPORT_SYMBOL(pci_assign_resource);
 
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|256| <<reassign_resources_sorted>> if (pci_reassign_resource(add_res->dev, idx, add_size, align))
+ */
 int pci_reassign_resource(struct pci_dev *dev, int resno,
 			  resource_size_t addsize, resource_size_t min_align)
 {
diff --git a/drivers/perf/arm_pmu.c b/drivers/perf/arm_pmu.c
index 8458fe2ce..911e9628a 100644
--- a/drivers/perf/arm_pmu.c
+++ b/drivers/perf/arm_pmu.c
@@ -99,6 +99,17 @@ static const struct pmu_irq_ops percpu_pmunmi_ops = {
 	.free_pmuirq = armpmu_free_percpu_pmunmi
 };
 
+/*
+ * 在以下使用percpu的cpu_armpmu:
+ *   - drivers/perf/arm_pmu.c|102| <<global>> static DEFINE_PER_CPU(struct arm_pmu *, cpu_armpmu);
+ *   - drivers/perf/arm_pmu.c|629| <<armpmu_free_irq>> per_cpu(cpu_irq_ops, cpu)->free_pmuirq(irq, cpu, &cpu_armpmu);
+ *   - drivers/perf/arm_pmu.c|660| <<armpmu_request_irq>> err = request_nmi(irq, handler, irq_flags, "arm-pmu", per_cpu_ptr(&cpu_armpmu, cpu));
+ *   - drivers/perf/arm_pmu.c|665| <<armpmu_request_irq>> err = request_irq(irq, handler, irq_flags, "arm-pmu", per_cpu_ptr(&cpu_armpmu, cpu));
+ *   - drivers/perf/arm_pmu.c|672| <<armpmu_request_irq>> err = request_percpu_nmi(irq, handler, "arm-pmu", &cpu_armpmu);
+ *   - drivers/perf/arm_pmu.c|677| <<armpmu_request_irq>> err = request_percpu_irq(irq, handler, "arm-pmu", &cpu_armpmu);
+ *   - drivers/perf/arm_pmu.c|730| <<arm_perf_starting_cpu>> per_cpu(cpu_armpmu, cpu) = pmu;
+ *   - drivers/perf/arm_pmu.c|751| <<arm_perf_teardown_cpu>> per_cpu(cpu_armpmu, cpu) = NULL;
+ */
 static DEFINE_PER_CPU(struct arm_pmu *, cpu_armpmu);
 static DEFINE_PER_CPU(int, cpu_irq);
 static DEFINE_PER_CPU(const struct pmu_irq_ops *, cpu_irq_ops);
@@ -170,6 +181,24 @@ armpmu_map_raw_event(u32 raw_event_mask, u64 config)
 	return (int)(config & raw_event_mask);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1176| <<global>> return armpmu_map_event(event, extra_event_map, extra_cache_map, ARMV8_PMU_EVTYPE_EVENT);
+ *   - drivers/perf/apple_m1_cpu_pmu.c|493| <<m1_pmu_map_event>> return armpmu_map_event(event, &m1_pmu_perf_map, NULL, M1_PMU_CFG_EVENT);
+ *   - drivers/perf/apple_m1_cpu_pmu.c|504| <<m2_pmu_map_event>> return armpmu_map_event(event, &m1_pmu_perf_map, NULL, M1_PMU_CFG_EVENT);
+ *   - drivers/perf/arm_pmuv3.c|1113| <<__armv8_pmuv3_map_event_id>> return armpmu_map_event(event, &armv8_pmuv3_perf_map, &armv8_pmuv3_perf_cache_map, ARMV8_PMU_EVTYPE_EVENT);
+ *   - drivers/perf/arm_v6_pmu.c|378| <<armv6_map_event>> return armpmu_map_event(event, &armv6_perf_map, &armv6_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1096| <<armv7_a8_map_event>> return armpmu_map_event(event, &armv7_a8_perf_map, &armv7_a8_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1102| <<armv7_a9_map_event>> return armpmu_map_event(event, &armv7_a9_perf_map, &armv7_a9_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1108| <<armv7_a5_map_event>> return armpmu_map_event(event, &armv7_a5_perf_map, &armv7_a5_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1114| <<armv7_a15_map_event>> return armpmu_map_event(event, &armv7_a15_perf_map, &armv7_a15_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1120| <<armv7_a7_map_event>> return armpmu_map_event(event, &armv7_a7_perf_map, &armv7_a7_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1126| <<armv7_a12_map_event>> return armpmu_map_event(event, &armv7_a12_perf_map, &armv7_a12_perf_cache_map, 0xFF);
+ *   - drivers/perf/arm_v7_pmu.c|1132| <<krait_map_event>> return armpmu_map_event(event, &krait_perf_map, &krait_perf_cache_map, 0xFFFFF);
+ *   - drivers/perf/arm_v7_pmu.c|1138| <<krait_map_event_no_branch>> return armpmu_map_event(event, &krait_perf_map_no_branch, &krait_perf_cache_map, 0xFFFFF);
+ *   - drivers/perf/arm_v7_pmu.c|1144| <<scorpion_map_event>> return armpmu_map_event(event, &scorpion_perf_map, &scorpion_perf_cache_map, 0xFFFFF);
+ *   - drivers/perf/arm_xscale_pmu.c|350| <<xscale_map_event>> return armpmu_map_event(event, &xscale_perf_map, &xscale_perf_cache_map, 0xFF);
+ */
 int
 armpmu_map_event(struct perf_event *event,
 		 const unsigned (*event_map)[PERF_COUNT_HW_MAX],
@@ -197,6 +226,16 @@ armpmu_map_event(struct perf_event *event,
 	return -ENOENT;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/apple_m1_cpu_pmu.c|412| <<m1_pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ *   - drivers/perf/arm_pmu.c|348| <<armpmu_start>> armpmu_event_set_period(event);
+ *   - drivers/perf/arm_pmuv3.c|927| <<armv8pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ *   - drivers/perf/arm_v6_pmu.c|275| <<armv6pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ *   - drivers/perf/arm_v7_pmu.c|979| <<armv7pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ *   - drivers/perf/arm_xscale_pmu.c|184| <<xscale1pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ *   - drivers/perf/arm_xscale_pmu.c|516| <<xscale2pmu_handle_irq>> if (!armpmu_event_set_period(event))
+ */
 int armpmu_event_set_period(struct perf_event *event)
 {
 	struct arm_pmu *armpmu = to_arm_pmu(event->pmu);
@@ -239,6 +278,17 @@ int armpmu_event_set_period(struct perf_event *event)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/apple_m1_cpu_pmu.c|410| <<m1_pmu_handle_irq>> armpmu_event_update(event);
+ *   - drivers/perf/arm_pmu.c|297| <<armpmu_read>> armpmu_event_update(event);
+ *   - drivers/perf/arm_pmu.c|312| <<armpmu_stop>> armpmu_event_update(event);
+ *   - drivers/perf/arm_pmuv3.c|898| <<armv8pmu_handle_irq>> armpmu_event_update(event);
+ *   - drivers/perf/arm_v6_pmu.c|273| <<armv6pmu_handle_irq>> armpmu_event_update(event);
+ *   - drivers/perf/arm_v7_pmu.c|977| <<armv7pmu_handle_irq>> armpmu_event_update(event);
+ *   - drivers/perf/arm_xscale_pmu.c|182| <<xscale1pmu_handle_irq>> armpmu_event_update(event);
+ *   - drivers/perf/arm_xscale_pmu.c|514| <<xscale2pmu_handle_irq>> armpmu_event_update(event);
+ */
 u64 armpmu_event_update(struct perf_event *event)
 {
 	struct arm_pmu *armpmu = to_arm_pmu(event->pmu);
@@ -909,6 +959,11 @@ void armpmu_free(struct arm_pmu *pmu)
 	kfree(pmu);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmu_acpi.c|422| <<arm_pmu_acpi_probe>> ret = armpmu_register(pmu);
+ *   - drivers/perf/arm_pmu_platform.c|232| <<arm_pmu_device_probe>> ret = armpmu_register(pmu);
+ */
 int armpmu_register(struct arm_pmu *pmu)
 {
 	int ret;
diff --git a/drivers/perf/arm_pmu_acpi.c b/drivers/perf/arm_pmu_acpi.c
index 05dda19c5..683ab7f2f 100644
--- a/drivers/perf/arm_pmu_acpi.c
+++ b/drivers/perf/arm_pmu_acpi.c
@@ -349,6 +349,10 @@ static void arm_pmu_acpi_probe_matching_cpus(struct arm_pmu *pmu,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1434| <<armv8_pmu_driver_init>> ret = arm_pmu_acpi_probe(armv8_pmuv3_pmu_init);
+ */
 int arm_pmu_acpi_probe(armpmu_init_fn init_fn)
 {
 	int pmu_idx = 0;
diff --git a/drivers/perf/arm_pmuv3.c b/drivers/perf/arm_pmuv3.c
index d24684079..448ac941c 100644
--- a/drivers/perf/arm_pmuv3.c
+++ b/drivers/perf/arm_pmuv3.c
@@ -326,6 +326,14 @@ GEN_PMU_FORMAT_ATTR(threshold);
 
 static int sysctl_perf_user_access __read_mostly;
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1217| <<global>> if (armv8pmu_event_is_64bit(event))
+ *   - drivers/perf/arm_pmuv3.c|1231| <<global>> if (armv8pmu_event_is_64bit(event) &&
+ *   - drivers/perf/arm_pmuv3.c|498| <<armv8pmu_event_is_chained>> armv8pmu_event_is_64bit(event) &&
+ *   - drivers/perf/arm_pmuv3.c|564| <<armv8pmu_event_needs_bias>> if (armv8pmu_event_is_64bit(event))
+ *   - drivers/perf/arm_pmuv3.c|1030| <<armv8pmu_get_event_idx>> else if (armv8pmu_event_is_64bit(event) &&
+ */
 static bool armv8pmu_event_is_64bit(struct perf_event *event)
 {
 	return ATTR_CFG_GET_FLD(&event->attr, long);
@@ -466,6 +474,16 @@ static const struct attribute_group armv8_pmuv3_caps_attr_group = {
  * On AArch32, long counters make no sense (you can't access the top
  * bits), so we only enable this on AArch64.
  */
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1195| <<global>> if (armv8pmu_event_is_64bit(event) && (hw_event_id != ARMV8_PMUV3_PERFCTR_CPU_CYCLES) && !armv8pmu_has_long_event(armpmu))
+ *   - drivers/perf/arm_pmuv3.c|491| <<armv8pmu_event_is_chained>> return !armv8pmu_event_has_user_read(event) && armv8pmu_event_is_64bit(event) && !armv8pmu_has_long_event(cpu_pmu) && (idx != ARMV8_IDX_CYCLE_COUNTER);
+ *   - drivers/perf/arm_pmuv3.c|559| <<armv8pmu_event_needs_bias>> if (armv8pmu_has_long_event(cpu_pmu) || idx == ARMV8_IDX_CYCLE_COUNTER)
+ *   - drivers/perf/arm_pmuv3.c|994| <<armv8pmu_get_event_idx>> else if (armv8pmu_event_is_64bit(event) && armv8pmu_event_want_user_access(event) && !armv8pmu_has_long_event(cpu_pmu))
+ *   - drivers/perf/arm_pmuv3.c|1121| <<armv8pmu_reset>> if (armv8pmu_has_long_event(cpu_pmu))
+ *
+ * A1的芯片的VM是long=0
+ */
 static bool armv8pmu_has_long_event(struct arm_pmu *cpu_pmu)
 {
 	return (IS_ENABLED(CONFIG_ARM64) && is_pmuv3p5(cpu_pmu->pmuver));
@@ -481,6 +499,15 @@ static bool armv8pmu_event_has_user_read(struct perf_event *event)
  * except when we have allocated the 64bit cycle counter (for CPU
  * cycles event) or when user space counter access is enabled.
  */
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|547| <<armv8pmu_read_hw_counter>> if (armv8pmu_event_is_chained(event))
+ *   - drivers/perf/arm_pmuv3.c|627| <<armv8pmu_write_hw_counter>> if (armv8pmu_event_is_chained(event)) {
+ *   - drivers/perf/arm_pmuv3.c|687| <<armv8pmu_write_event_type>> if (armv8pmu_event_is_chained(event)) {
+ *   - drivers/perf/arm_pmuv3.c|706| <<armv8pmu_event_cnten_mask>> if (armv8pmu_event_is_chained(event))
+ *   - drivers/perf/arm_pmuv3.c|1039| <<armv8pmu_get_event_idx>> if (armv8pmu_event_is_chained(event))
+ *   - drivers/perf/arm_pmuv3.c|1051| <<armv8pmu_clear_event_idx>> if (armv8pmu_event_is_chained(event))
+ */
 static bool armv8pmu_event_is_chained(struct perf_event *event)
 {
 	int idx = event->hw.idx;
@@ -502,11 +529,23 @@ static bool armv8pmu_event_is_chained(struct perf_event *event)
 #define	ARMV8_IDX_TO_COUNTER(x)	\
 	(((x) - ARMV8_IDX_COUNTER0) & ARMV8_PMU_COUNTER_MASK)
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|889| <<armv8pmu_start>> armv8pmu_pmcr_write(armv8pmu_pmcr_read() | ARMV8_PMU_PMCR_E);
+ *   - drivers/perf/arm_pmuv3.c|897| <<armv8pmu_stop>> armv8pmu_pmcr_write(armv8pmu_pmcr_read() & ~ARMV8_PMU_PMCR_E);
+ *   - drivers/perf/arm_pmuv3.c|1328| <<__armv8pmu_probe_pmu>> cpu_pmu->num_events = FIELD_GET(ARMV8_PMU_PMCR_N, armv8pmu_pmcr_read());
+ */
 static u64 armv8pmu_pmcr_read(void)
 {
 	return read_pmcr();
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|889| <<armv8pmu_start>> armv8pmu_pmcr_write(armv8pmu_pmcr_read() | ARMV8_PMU_PMCR_E);
+ *   - drivers/perf/arm_pmuv3.c|897| <<armv8pmu_stop>> armv8pmu_pmcr_write(armv8pmu_pmcr_read() & ~ARMV8_PMU_PMCR_E);
+ *   - drivers/perf/arm_pmuv3.c|1184| <<armv8pmu_reset>> armv8pmu_pmcr_write(pmcr);
+ */
 static void armv8pmu_pmcr_write(u64 val)
 {
 	val &= ARMV8_PMU_PMCR_MASK;
@@ -519,6 +558,10 @@ static int armv8pmu_has_overflowed(u32 pmovsr)
 	return pmovsr & ARMV8_PMU_OVERFLOWED_MASK;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|957| <<armv8pmu_handle_irq>> if (!armv8pmu_counter_has_overflowed(pmovsr, idx))
+ */
 static int armv8pmu_counter_has_overflowed(u32 pmnc, int idx)
 {
 	return pmnc & BIT(ARMV8_IDX_TO_COUNTER(idx));
@@ -593,6 +636,13 @@ static u64 armv8pmu_read_counter(struct perf_event *event)
 	return  armv8pmu_unbias_long_counter(event, value);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|617| <<armv8pmu_write_hw_counter>> armv8pmu_write_evcntr(idx, upper_32_bits(value));
+ *   - drivers/perf/arm_pmuv3.c|618| <<armv8pmu_write_hw_counter>> armv8pmu_write_evcntr(idx - 1, lower_32_bits(value));
+ *   - drivers/perf/arm_pmuv3.c|620| <<armv8pmu_write_hw_counter>> armv8pmu_write_evcntr(idx, value);
+ *   - drivers/perf/arm_pmuv3.c|811| <<armv8pmu_enable_user_access>> armv8pmu_write_evcntr(i, 0);
+ */
 static void armv8pmu_write_evcntr(int idx, u64 value)
 {
 	u32 counter = ARMV8_IDX_TO_COUNTER(idx);
@@ -600,6 +650,10 @@ static void armv8pmu_write_evcntr(int idx, u64 value)
 	write_pmevcntrn(counter, value);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|634| <<armv8pmu_write_counter>> armv8pmu_write_hw_counter(event, value);
+ */
 static void armv8pmu_write_hw_counter(struct perf_event *event,
 					     u64 value)
 {
@@ -613,6 +667,10 @@ static void armv8pmu_write_hw_counter(struct perf_event *event,
 	}
 }
 
+/*
+ * 在以下使用armv8pmu_write_counter():
+ *   - drivers/perf/arm_pmuv3.c|1388| <<global>> cpu_pmu->write_counter = armv8pmu_write_counter;
+ */
 static void armv8pmu_write_counter(struct perf_event *event, u64 value)
 {
 	struct hw_perf_event *hwc = &event->hw;
@@ -626,6 +684,12 @@ static void armv8pmu_write_counter(struct perf_event *event, u64 value)
 		armv8pmu_write_hw_counter(event, value);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|658| <<armv8pmu_write_event_type>> armv8pmu_write_evtype(idx - 1, hwc->config_base);
+ *   - drivers/perf/arm_pmuv3.c|659| <<armv8pmu_write_event_type>> armv8pmu_write_evtype(idx, chain_evt);
+ *   - drivers/perf/arm_pmuv3.c|664| <<armv8pmu_write_event_type>> armv8pmu_write_evtype(idx, hwc->config_base);
+ */
 static void armv8pmu_write_evtype(int idx, unsigned long val)
 {
 	u32 counter = ARMV8_IDX_TO_COUNTER(idx);
@@ -641,6 +705,10 @@ static void armv8pmu_write_evtype(int idx, unsigned long val)
 	write_pmevtypern(counter, val);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|806| <<armv8pmu_enable_event>> armv8pmu_write_event_type(event);
+ */
 static void armv8pmu_write_event_type(struct perf_event *event)
 {
 	struct hw_perf_event *hwc = &event->hw;
@@ -814,6 +882,11 @@ static void armv8pmu_disable_event(struct perf_event *event)
 	armv8pmu_disable_event_irq(event);
 }
 
+/*
+ * 在以下使用armv8pmu_start():
+ *   - drivers/perf/arm_pmuv3.c|1368| <<global>> cpu_pmu->start = armv8pmu_start;
+ *   - drivers/perf/arm_pmuv3.c|938| <<armv8pmu_handle_irq>> armv8pmu_start(cpu_pmu);
+ */
 static void armv8pmu_start(struct arm_pmu *cpu_pmu)
 {
 	struct perf_event_context *ctx;
@@ -840,10 +913,22 @@ static void armv8pmu_stop(struct arm_pmu *cpu_pmu)
 	armv8pmu_pmcr_write(armv8pmu_pmcr_read() & ~ARMV8_PMU_PMCR_E);
 }
 
+/*
+ * 在以下使用armv8pmu_handle_irq():
+ *   - drivers/perf/arm_pmuv3.c|1330| <<global>> cpu_pmu->handle_irq = armv8pmu_handle_irq;
+ */
 static irqreturn_t armv8pmu_handle_irq(struct arm_pmu *cpu_pmu)
 {
 	u32 pmovsr;
 	struct perf_sample_data data;
+	/*
+	 * struct pmu_hw_events {
+	 *     struct perf_event       *events[ARMPMU_MAX_HWEVENTS];
+	 *     DECLARE_BITMAP(used_mask, ARMPMU_MAX_HWEVENTS);
+	 *     struct arm_pmu          *percpu_pmu;
+	 *     int irq;
+	 * };
+	 */
 	struct pmu_hw_events *cpuc = this_cpu_ptr(cpu_pmu->hw_events);
 	struct pt_regs *regs;
 	int idx;
@@ -851,6 +936,10 @@ static irqreturn_t armv8pmu_handle_irq(struct arm_pmu *cpu_pmu)
 	/*
 	 * Get and reset the IRQ flags
 	 */
+	/*
+	 * 注释:
+	 * PMOVSR, Performance Monitors Overflow Flag Status Register, PMSA
+	 */
 	pmovsr = armv8pmu_getreset_flags();
 
 	/*
@@ -885,8 +974,29 @@ static irqreturn_t armv8pmu_handle_irq(struct arm_pmu *cpu_pmu)
 			continue;
 
 		hwc = &event->hw;
+		/*
+		 * called by:
+		 *   - drivers/perf/apple_m1_cpu_pmu.c|410| <<m1_pmu_handle_irq>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_pmu.c|297| <<armpmu_read>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_pmu.c|312| <<armpmu_stop>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_pmuv3.c|898| <<armv8pmu_handle_irq>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_v6_pmu.c|273| <<armv6pmu_handle_irq>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_v7_pmu.c|977| <<armv7pmu_handle_irq>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_xscale_pmu.c|182| <<xscale1pmu_handle_irq>> armpmu_event_update(event);
+		 *   - drivers/perf/arm_xscale_pmu.c|514| <<xscale2pmu_handle_irq>> armpmu_event_update(event);
+		 */
 		armpmu_event_update(event);
 		perf_sample_data_init(&data, 0, hwc->last_period);
+		/*
+		 * called by:
+		 *   - drivers/perf/apple_m1_cpu_pmu.c|412| <<m1_pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 *   - drivers/perf/arm_pmu.c|348| <<armpmu_start>> armpmu_event_set_period(event);
+		 *   - drivers/perf/arm_pmuv3.c|927| <<armv8pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 *   - drivers/perf/arm_v6_pmu.c|275| <<armv6pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 *   - drivers/perf/arm_v7_pmu.c|979| <<armv7pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 *   - drivers/perf/arm_xscale_pmu.c|184| <<xscale1pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 *   - drivers/perf/arm_xscale_pmu.c|516| <<xscale2pmu_handle_irq>> if (!armpmu_event_set_period(event))
+		 */
 		if (!armpmu_event_set_period(event))
 			continue;
 
@@ -903,6 +1013,10 @@ static irqreturn_t armv8pmu_handle_irq(struct arm_pmu *cpu_pmu)
 	return IRQ_HANDLED;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|973| <<armv8pmu_get_event_idx>> return armv8pmu_get_single_idx(cpuc, cpu_pmu);
+ */
 static int armv8pmu_get_single_idx(struct pmu_hw_events *cpuc,
 				    struct arm_pmu *cpu_pmu)
 {
@@ -1076,6 +1190,9 @@ static void armv8pmu_reset(void *info)
 	 */
 	pmcr = ARMV8_PMU_PMCR_P | ARMV8_PMU_PMCR_C | ARMV8_PMU_PMCR_LC;
 
+	/*
+	 * A1的芯片的VM是long=0
+	 */
 	/* Enable long event counter support where available */
 	if (armv8pmu_has_long_event(cpu_pmu))
 		pmcr |= ARMV8_PMU_PMCR_LP;
@@ -1105,6 +1222,15 @@ static int __armv8_pmuv3_map_event_id(struct arm_pmu *armpmu,
 				ARMV8_PMU_EVTYPE_EVENT);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1163| <<armv8_pmuv3_map_event>> return __armv8_pmuv3_map_event(event, NULL, NULL);
+ *   - drivers/perf/arm_pmuv3.c|1168| <<armv8_a53_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_a53_perf_cache_map);
+ *   - drivers/perf/arm_pmuv3.c|1173| <<armv8_a57_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_a57_perf_cache_map);
+ *   - drivers/perf/arm_pmuv3.c|1178| <<armv8_a73_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_a73_perf_cache_map);
+ *   - drivers/perf/arm_pmuv3.c|1183| <<armv8_thunder_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_thunder_perf_cache_map);
+ *   - drivers/perf/arm_pmuv3.c|1189| <<armv8_vulcan_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_vulcan_perf_cache_map);
+ */
 static int __armv8_pmuv3_map_event(struct perf_event *event,
 				   const unsigned (*extra_event_map)
 						  [PERF_COUNT_HW_MAX],
@@ -1195,6 +1321,10 @@ struct armv8pmu_probe_info {
 	bool present;
 };
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1247| <<armv8pmu_probe_pmu>> ret = smp_call_function_any(&cpu_pmu->supported_cpus, __armv8pmu_probe_pmu, &probe, 1);
+ */
 static void __armv8pmu_probe_pmu(void *info)
 {
 	struct armv8pmu_probe_info *probe = info;
@@ -1235,6 +1365,10 @@ static void __armv8pmu_probe_pmu(void *info)
 		cpu_pmu->reg_pmmir = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1294| <<armv8_pmu_init>> int ret = armv8pmu_probe_pmu(cpu_pmu);
+ */
 static int armv8pmu_probe_pmu(struct arm_pmu *cpu_pmu)
 {
 	struct armv8pmu_probe_info probe = {
@@ -1288,10 +1422,15 @@ static void armv8_pmu_register_sysctl_table(void)
 		register_sysctl("kernel", armv8_pmu_sysctl_table);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1324| <<PMUV3_INIT_SIMPLE>> return armv8_pmu_init(cpu_pmu, #name, armv8_pmuv3_map_event); \
+ *   - drivers/perf/arm_pmuv3.c|1330| <<PMUV3_INIT_MAP_EVENT>> return armv8_pmu_init(cpu_pmu, #name, map_event); \
+ */
 static int armv8_pmu_init(struct arm_pmu *cpu_pmu, char *name,
 			  int (*map_event)(struct perf_event *event))
 {
-	int ret = armv8pmu_probe_pmu(cpu_pmu);
+	int ret = armv8pmu_probe_pmu(cpu_pm);
 	if (ret)
 		return ret;
 
@@ -1318,12 +1457,54 @@ static int armv8_pmu_init(struct arm_pmu *cpu_pmu, char *name,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1333| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_pmuv3)
+ *   - drivers/perf/arm_pmuv3.c|1335| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a34)
+ *   - drivers/perf/arm_pmuv3.c|1336| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a55)
+ *   - drivers/perf/arm_pmuv3.c|1337| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a65)
+ *   - drivers/perf/arm_pmuv3.c|1338| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a75)
+ *   - drivers/perf/arm_pmuv3.c|1339| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a76)
+ *   - drivers/perf/arm_pmuv3.c|1340| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a77)
+ *   - drivers/perf/arm_pmuv3.c|1341| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_a78)
+ *   - drivers/perf/arm_pmuv3.c|1342| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a510)
+ *   - drivers/perf/arm_pmuv3.c|1343| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a520)
+ *   - drivers/perf/arm_pmuv3.c|1344| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a710)
+ *   - drivers/perf/arm_pmuv3.c|1345| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a715)
+ *   - drivers/perf/arm_pmuv3.c|1346| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a720)
+ *   - drivers/perf/arm_pmuv3.c|1347| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_a725)
+ *   - drivers/perf/arm_pmuv3.c|1348| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_cortex_x1)
+ *   - drivers/perf/arm_pmuv3.c|1349| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_x2)
+ *   - drivers/perf/arm_pmuv3.c|1350| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_x3)
+ *   - drivers/perf/arm_pmuv3.c|1351| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_x4)
+ *   - drivers/perf/arm_pmuv3.c|1352| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_cortex_x925)
+ *   - drivers/perf/arm_pmuv3.c|1353| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_e1)
+ *   - drivers/perf/arm_pmuv3.c|1354| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_n1)
+ *   - drivers/perf/arm_pmuv3.c|1355| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_neoverse_n2)
+ *   - drivers/perf/arm_pmuv3.c|1356| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv9_neoverse_n3)
+ *   - drivers/perf/arm_pmuv3.c|1357| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_v1)
+ *   - drivers/perf/arm_pmuv3.c|1358| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_v2)
+ *   - drivers/perf/arm_pmuv3.c|1359| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_v3)
+ *   - drivers/perf/arm_pmuv3.c|1360| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_neoverse_v3ae)
+ *   - drivers/perf/arm_pmuv3.c|1362| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_nvidia_carmel)
+ *   - drivers/perf/arm_pmuv3.c|1363| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_SIMPLE(armv8_nvidia_denver)
+ */
 #define PMUV3_INIT_SIMPLE(name)						\
 static int name##_pmu_init(struct arm_pmu *cpu_pmu)			\
 {									\
 	return armv8_pmu_init(cpu_pmu, #name, armv8_pmuv3_map_event);	\
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmuv3.c|1365| <<PMUV3_INIT_SIMPLE>> PMUV3_INIT_MAP_EVENT(armv8_cortex_a35, armv8_a53_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1366| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_cortex_a53, armv8_a53_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1367| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_cortex_a57, armv8_a57_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1368| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_cortex_a72, armv8_a57_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1369| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_cortex_a73, armv8_a73_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1370| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_cavium_thunder, armv8_thunder_map_event)
+ *   - drivers/perf/arm_pmuv3.c|1371| <<PMUV3_INIT_MAP_EVENT>> PMUV3_INIT_MAP_EVENT(armv8_brcm_vulcan, armv8_vulcan_map_event)
+ */
 #define PMUV3_INIT_MAP_EVENT(name, map_event)				\
 static int name##_pmu_init(struct arm_pmu *cpu_pmu)			\
 {									\
@@ -1428,6 +1609,12 @@ static int __init armv8_pmu_driver_init(void)
 {
 	int ret;
 
+	/*
+	 * 在以下设置arm64的acpi_disabled:
+	 *   - arch/arm64/kernel/acpi.c|40| <<global>> int acpi_disabled = 1;
+	 *   - arch/arm64/include/asm/acpi.h|85| <<disable_acpi>> acpi_disabled = 1;
+	 *   - arch/arm64/include/asm/acpi.h|92| <<enable_acpi>> acpi_disabled = 0;
+	 */
 	if (acpi_disabled)
 		ret = platform_driver_register(&armv8_pmu_driver);
 	else
@@ -1440,6 +1627,10 @@ static int __init armv8_pmu_driver_init(void)
 }
 device_initcall(armv8_pmu_driver_init)
 
+/*
+ * called by:
+ *   - kernel/events/core.c|6222| <<perf_event_update_userpage>> arch_perf_update_userpage(event, userpg, now);
+ */
 void arch_perf_update_userpage(struct perf_event *event,
 			       struct perf_event_mmap_page *userpg, u64 now)
 {
diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c
index 0960699e7..a2887daee 100644
--- a/drivers/vfio/vfio_iommu_type1.c
+++ b/drivers/vfio/vfio_iommu_type1.c
@@ -409,6 +409,12 @@ static int vfio_iova_put_vfio_pfn(struct vfio_dma *dma, struct vfio_pfn *vpfn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|437| <<vfio_lock_acct>> ret = mm_lock_acct(dma->task, mm, dma->lock_cap, npage);
+ *   - drivers/vfio/vfio_iommu_type1.c|1529| <<vfio_change_dma_owner>> ret = mm_lock_acct(task, mm, lock_cap, npage);
+ *   - drivers/vfio/vfio_iommu_type1.c|1534| <<vfio_change_dma_owner>> mm_lock_acct(dma->task, dma->mm, dma->lock_cap, -npage);
+ */
 static int mm_lock_acct(struct task_struct *task, struct mm_struct *mm,
 			bool lock_cap, long npage)
 {
@@ -417,11 +423,27 @@ static int mm_lock_acct(struct task_struct *task, struct mm_struct *mm,
 	if (ret)
 		return ret;
 
+	/*
+	 * called by:
+	 *   - drivers/iommu/iommufd/pages.c|849| <<update_mm_locked_vm>> rc = __account_locked_vm(pages->source_mm, npages, inc, pages->source_task, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|420| <<mm_lock_acct>> ret = __account_locked_vm(mm, abs(npage), npage > 0, task, lock_cap);
+	 *   - mm/util.c|567| <<account_locked_vm>> ret = __account_locked_vm(mm, pages, inc, current, capable(CAP_IPC_LOCK));
+	 */
 	ret = __account_locked_vm(mm, abs(npage), npage > 0, task, lock_cap);
 	mmap_write_unlock(mm);
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|690| <<vfio_pin_pages_remote>> ret = vfio_lock_acct(dma, lock_acct, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|728| <<vfio_unpin_pages_remote>> vfio_lock_acct(dma, locked - unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|751| <<vfio_pin_page_external>> ret = vfio_lock_acct(dma, 1, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|779| <<vfio_unpin_page_external>> vfio_lock_acct(dma, -unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|862| <<vfio_iommu_type1_pin_pages>> vfio_lock_acct(dma, -1, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|1103| <<vfio_unmap_unpin>> vfio_lock_acct(dma, -unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|2377| <<vfio_iommu_unmap_unpin_reaccount>> vfio_lock_acct(dma, locked - unlocked, true);
+ */
 static int vfio_lock_acct(struct vfio_dma *dma, long npage, bool async)
 {
 	struct mm_struct *mm;
@@ -434,6 +456,12 @@ static int vfio_lock_acct(struct vfio_dma *dma, long npage, bool async)
 	if (async && !mmget_not_zero(mm))
 		return -ESRCH; /* process exited */
 
+	/*
+	 * called by:
+	 *   - drivers/vfio/vfio_iommu_type1.c|437| <<vfio_lock_acct>> ret = mm_lock_acct(dma->task, mm, dma->lock_cap, npage);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1529| <<vfio_change_dma_owner>> ret = mm_lock_acct(task, mm, lock_cap, npage);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1534| <<vfio_change_dma_owner>> mm_lock_acct(dma->task, dma->mm, dma->lock_cap, -npage);
+	 */
 	ret = mm_lock_acct(dma->task, mm, dma->lock_cap, npage);
 	if (!ret)
 		dma->locked_vm += npage;
@@ -1442,6 +1470,10 @@ static int vfio_iommu_map(struct vfio_iommu *iommu, dma_addr_t iova,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1652| <<vfio_dma_do_map>> ret = vfio_pin_map_dma(iommu, dma, size);
+ */
 static int vfio_pin_map_dma(struct vfio_iommu *iommu, struct vfio_dma *dma,
 			    size_t map_size)
 {
@@ -1542,6 +1574,10 @@ static int vfio_change_dma_owner(struct vfio_dma *dma)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|2828| <<vfio_iommu_type1_map_dma>> return vfio_dma_do_map(iommu, &map);
+ */
 static int vfio_dma_do_map(struct vfio_iommu *iommu,
 			   struct vfio_iommu_type1_dma_map *map)
 {
@@ -2809,6 +2845,10 @@ static int vfio_iommu_type1_get_info(struct vfio_iommu *iommu,
 			-EFAULT : 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|2999| <<vfio_iommu_type1_ioctl(VFIO_IOMMU_MAP_DMA)>> return vfio_iommu_type1_map_dma(iommu, arg);
+ */
 static int vfio_iommu_type1_map_dma(struct vfio_iommu *iommu,
 				    unsigned long arg)
 {
diff --git a/fs/locks.c b/fs/locks.c
index e45cad40f..2bd4799b5 100644
--- a/fs/locks.c
+++ b/fs/locks.c
@@ -632,6 +632,10 @@ static int posix_same_owner(struct file_lock_core *fl1, struct file_lock_core *f
 	return fl1->flc_owner == fl2->flc_owner;
 }
 
+/*
+ * called by:
+ *   - fs/locks.c|862| <<locks_insert_lock_ctx>> locks_insert_global_locks(fl);
+ */
 /* Must be called with the flc_lock held! */
 static void locks_insert_global_locks(struct file_lock_core *flc)
 {
@@ -855,6 +859,59 @@ static void locks_wake_up_blocks(struct file_lock_core *blocker)
 	spin_unlock(&blocked_lock_lock);
 }
 
+/*
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] do_lock_file_wait
+ * [0] fcntl_setlk
+ * [0] do_fcntl
+ * [0] __x64_sys_fcntl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * nfs的client.
+ *
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] locks_lock_inode_wait
+ * [0] nfs4_lock_done
+ * [0] rpc_exit_task
+ * [0] __rpc_execute
+ * [0] rpc_async_schedule
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * NFS的server.
+ *
+ * [0] nfsd4_lock
+ * [0] nfsd4_proc_compound
+ * [0] nfsd_dispatch
+ * [0] svc_process_common
+ * [0] svc_process
+ * [0] nfsd
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] nfsd4_lock
+ * [0] nfsd4_proc_compound
+ * [0] nfsd_dispatch
+ * [0] svc_process_common
+ * [0] svc_process
+ * [0] nfsd
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - fs/locks.c|1132| <<flock_lock_inode>> locks_insert_lock_ctx(&new_fl->c, &ctx->flc_flock);
+ *   - fs/locks.c|1317| <<posix_lock_inode>> locks_insert_lock_ctx(&request->c, &fl->c.flc_list);
+ *   - fs/locks.c|1348| <<posix_lock_inode>> locks_insert_lock_ctx(&new_fl->c, &fl->c.flc_list);
+ *   - fs/locks.c|1360| <<posix_lock_inode>> locks_insert_lock_ctx(&left->c, &fl->c.flc_list);
+ *   - fs/locks.c|1851| <<generic_add_lease>> locks_insert_lock_ctx(&lease->c, &ctx->flc_lease);
+ */
 static void
 locks_insert_lock_ctx(struct file_lock_core *fl, struct list_head *before)
 {
@@ -1071,6 +1128,11 @@ static bool posix_locks_deadlock(struct file_lock *caller_fl,
  * whether or not a lock was successfully freed by testing the return
  * value for -ENOENT.
  */
+/*
+ * called by:
+ *   - fs/locks.c|2080| <<flock_lock_inode_wait>> error = flock_lock_inode(inode, fl);
+ *   - fs/locks.c|2656| <<locks_remove_flock>> flock_lock_inode(inode, &fl);
+ */
 static int flock_lock_inode(struct inode *inode, struct file_lock *request)
 {
 	struct file_lock *new_fl = NULL;
@@ -1143,6 +1205,11 @@ static int flock_lock_inode(struct inode *inode, struct file_lock *request)
 	return error;
 }
 
+/*
+ * called by:
+ *   - fs/locks.c|1428| <<posix_lock_file>> return posix_lock_inode(file_inode(filp), fl, conflock);
+ *   - fs/locks.c|1444| <<posix_lock_inode_wait>> error = posix_lock_inode(inode, fl, NULL);
+ */
 static int posix_lock_inode(struct inode *inode, struct file_lock *request,
 			    struct file_lock *conflock)
 {
@@ -1396,6 +1463,16 @@ static int posix_lock_inode(struct inode *inode, struct file_lock *request,
  * whether or not a lock was successfully freed by testing the return
  * value for -ENOENT.
  */
+/*
+ * called by:
+ *   - fs/ceph/locks.c|278| <<ceph_lock>> posix_lock_file(file, fl, NULL);
+ *   - fs/ceph/locks.c|299| <<ceph_lock>> err = posix_lock_file(file, fl, NULL);
+ *   - fs/dlm/plock.c|262| <<dlm_plock_callback>> if (posix_lock_file(file, flc, NULL)) {
+ *   - fs/fuse/file.c|2714| <<fuse_file_lock>> err = posix_lock_file(file, fl, NULL);
+ *   - fs/locks.c|2395| <<vfs_lock_file>> return posix_lock_file(filp, fl, conf);
+ *   - fs/orangefs/file.c|545| <<orangefs_lock>> rc = posix_lock_file(filp, fl, NULL);
+ *   - fs/smb/client/file.c|1753| <<cifs_posix_lock_set>> rc = posix_lock_file(file, flock, NULL);
+ */
 int posix_lock_file(struct file *filp, struct file_lock *fl,
 			struct file_lock *conflock)
 {
@@ -2072,6 +2149,10 @@ int fcntl_setlease(unsigned int fd, struct file *filp, int arg)
  *
  * Apply a FLOCK style lock request to an inode.
  */
+/*
+ * called by:
+ *   - fs/locks.c|2107| <<locks_lock_inode_wait>> res = flock_lock_inode_wait(inode, fl);
+ */
 static int flock_lock_inode_wait(struct inode *inode, struct file_lock *fl)
 {
 	int error;
@@ -2096,6 +2177,15 @@ static int flock_lock_inode_wait(struct inode *inode, struct file_lock *fl)
  *
  * Apply a POSIX or FLOCK style lock request to an inode.
  */
+/*
+ * called by:
+ *   - fs/nfs/nfs4proc.c|7066| <<nfs4_locku_done>> locks_lock_inode_wait(calldata->lsp->ls_state->inode, &calldata->fl);
+ *   - fs/nfs/nfs4proc.c|7194| <<nfs4_proc_unlck>> if (locks_lock_inode_wait(inode, request) == -ENOENT) {
+ *   - fs/nfs/nfs4proc.c|7334| <<nfs4_lock_done>> if (locks_lock_inode_wait(lsp->ls_state->inode, &data->fl) < 0)
+ *   - fs/nfs/nfs4proc.c|7545| <<_nfs4_proc_setlk>> status = locks_lock_inode_wait(state->inode, request);
+ *   - fs/nfs/nfs4proc.c|7554| <<_nfs4_proc_setlk>> status = locks_lock_inode_wait(state->inode, request);
+ *   - include/linux/filelock.h|417| <<locks_lock_file_wait>> return locks_lock_inode_wait(file_inode(filp), fl);
+ */
 int locks_lock_inode_wait(struct inode *inode, struct file_lock *fl)
 {
 	int res = 0;
@@ -2347,6 +2437,22 @@ int fcntl_getlk(struct file *filp, unsigned int cmd, struct flock *flock)
  * ->lm_grant() before returning to the caller with a FILE_LOCK_DEFERRED
  * return code.
  */
+/*
+ * called by:
+ *   - fs/lockd/svclock.c|567| <<nlmsvc_lock>> error = vfs_lock_file(file->f_file[mode], F_SETLK, &lock->fl, NULL);
+ *   - fs/lockd/svclock.c|688| <<nlmsvc_unlock>> error = vfs_lock_file(lock->fl.c.flc_file, F_SETLK,
+ *   - fs/lockd/svclock.c|692| <<nlmsvc_unlock>> error |= vfs_lock_file(lock->fl.c.flc_file, F_SETLK,
+ *   - fs/lockd/svclock.c|872| <<nlmsvc_grant_blocked>> error = vfs_lock_file(file->f_file[mode], F_SETLK, &lock->fl, NULL);
+ *   - fs/lockd/svclock.c|999| <<nlmsvc_grant_reply>> error = vfs_lock_file(fl->c.flc_file, F_SETLK, fl, NULL);
+ *   - fs/lockd/svcsubs.c|192| <<nlm_unlock_files>> if (lock.c.flc_file && vfs_lock_file(lock.c.flc_file, F_SETLK, &lock, NULL))
+ *   - fs/lockd/svcsubs.c|195| <<nlm_unlock_files>> if (lock.c.flc_file && vfs_lock_file(lock.c.flc_file, F_SETLK, &lock, NULL))
+ *   - fs/locks.c|2414| <<do_lock_file_wait>> error = vfs_lock_file(filp, cmd, fl, NULL);
+ *   - fs/locks.c|2676| <<locks_remove_posix>> error = vfs_lock_file(filp, F_SETLK, &lock, NULL);
+ *   - fs/nfsd/nfs4state.c|8035| <<nfsd4_lock>> err = vfs_lock_file(nf->nf_file, F_SETLK, file_lock, conflock);
+ *   - fs/nfsd/nfs4state.c|8273| <<nfsd4_locku>> err = vfs_lock_file(nf->nf_file, F_SETLK, file_lock, NULL);
+ *   - fs/smb/server/smb2pdu.c|7394| <<smb2_lock>> rc = vfs_lock_file(filp, smb_lock->cmd, flock, NULL);
+ *   - fs/smb/server/smb2pdu.c|7507| <<smb2_lock>> rc = vfs_lock_file(filp, F_SETLK, rlock, NULL);
+ */
 int vfs_lock_file(struct file *filp, unsigned int cmd, struct file_lock *fl, struct file_lock *conf)
 {
 	WARN_ON_ONCE(filp != fl->c.flc_file);
@@ -2357,6 +2463,11 @@ int vfs_lock_file(struct file *filp, unsigned int cmd, struct file_lock *fl, str
 }
 EXPORT_SYMBOL_GPL(vfs_lock_file);
 
+/*
+ * called by:
+ *   - fs/locks.c|2478| <<fcntl_setlk>> error = do_lock_file_wait(filp, cmd, file_lock);
+ *   - fs/locks.c|2600| <<fcntl_setlk64>> error = do_lock_file_wait(filp, cmd, file_lock);
+ */
 static int do_lock_file_wait(struct file *filp, unsigned int cmd,
 			     struct file_lock *fl)
 {
@@ -2399,6 +2510,12 @@ check_fmode_for_setlk(struct file_lock *fl)
 /* Apply the lock described by l to an open file descriptor.
  * This implements both the F_SETLK and F_SETLKW commands of fcntl().
  */
+/*
+ * called by:
+ *   - fs/fcntl.c|398| <<do_fcntl>> err = fcntl_setlk(fd, filp, cmd, &flock);
+ *   - fs/fcntl.c|684| <<do_compat_fcntl64>> err = fcntl_setlk(fd, f.file, convert_fcntl_cmd(cmd), &flock);
+ *   - fs/fcntl.c|693| <<do_compat_fcntl64>> err = fcntl_setlk(fd, f.file, convert_fcntl_cmd(cmd), &flock);
+ */
 int fcntl_setlk(unsigned int fd, struct file *filp, unsigned int cmd,
 		struct flock *flock)
 {
diff --git a/fs/nfs/nfs4proc.c b/fs/nfs/nfs4proc.c
index b8ffbe52b..8d171392b 100644
--- a/fs/nfs/nfs4proc.c
+++ b/fs/nfs/nfs4proc.c
@@ -7411,6 +7411,13 @@ static void nfs4_handle_setlk_error(struct nfs_server *server, struct nfs4_lock_
 	}
 }
 
+/*
+ * called by:
+ *   - fs/nfs/nfs4proc.c|7480| <<nfs4_lock_reclaim>> err = _nfs4_do_setlk(state, F_SETLK, request, NFS_LOCK_RECLAIM);
+ *   - fs/nfs/nfs4proc.c|7506| <<nfs4_lock_expired>> err = _nfs4_do_setlk(state, F_SETLK, request, NFS_LOCK_EXPIRED);
+ *   - fs/nfs/nfs4proc.c|7561| <<_nfs4_proc_setlk>> status = _nfs4_do_setlk(state, cmd, request, NFS_LOCK_NEW);
+ *   - fs/nfs/nfs4proc.c|7785| <<nfs4_lock_delegation_recall>> err = _nfs4_do_setlk(state, F_SETLK, fl, NFS_LOCK_NEW);
+ */
 static int _nfs4_do_setlk(struct nfs4_state *state, int cmd, struct file_lock *fl, int recovery_type)
 {
 	struct nfs4_lockdata *data;
diff --git a/fs/nfsd/nfs4state.c b/fs/nfsd/nfs4state.c
index a366fb1c1..51f02a24e 100644
--- a/fs/nfsd/nfs4state.c
+++ b/fs/nfsd/nfs4state.c
@@ -7864,6 +7864,52 @@ lookup_or_create_lock_state(struct nfsd4_compound_state *cstate,
 /*
  *  LOCK operation 
  */
+/*
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] do_lock_file_wait
+ * [0] fcntl_setlk
+ * [0] do_fcntl
+ * [0] __x64_sys_fcntl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * nfs的client.
+ *
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] locks_lock_inode_wait
+ * [0] nfs4_lock_done
+ * [0] rpc_exit_task
+ * [0] __rpc_execute
+ * [0] rpc_async_schedule
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * NFS的server.
+ *
+ * [0] nfsd4_lock
+ * [0] nfsd4_proc_compound
+ * [0] nfsd_dispatch
+ * [0] svc_process_common
+ * [0] svc_process
+ * [0] nfsd
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] locks_insert_lock_ctx
+ * [0] posix_lock_inode
+ * [0] nfsd4_lock
+ * [0] nfsd4_proc_compound
+ * [0] nfsd_dispatch
+ * [0] svc_process_common
+ * [0] svc_process
+ * [0] nfsd
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 __be32
 nfsd4_lock(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,
 	   union nfsd4_op_u *u)
diff --git a/include/kvm/arm_pmu.h b/include/kvm/arm_pmu.h
index 35d4ca4f6..eb053d60f 100644
--- a/include/kvm/arm_pmu.h
+++ b/include/kvm/arm_pmu.h
@@ -24,10 +24,33 @@ struct kvm_pmu_events {
 };
 
 struct kvm_pmu {
+	/*
+	 * 在以下使用kvm_pmu->overflow_work:
+	 *   - arch/arm64/kvm/pmu-emul.c|453| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_perf_overflow_notify_vcpu>> vcpu = container_of(work, struct kvm_vcpu, arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|718| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|1092| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+	 */
 	struct irq_work overflow_work;
 	struct kvm_pmu_events events;
 	struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
+	/*
+	 * 在以下使用kvm_pmu->irq_num:
+	 *   - arch/arm64/kvm/pmu-emul.c|720| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu, pmu->irq_num, overflow, pmu);
+	 *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> int irq = vcpu->arch.pmu.irq_num;
+	 *   - arch/arm64/kvm/pmu-emul.c|1567| <<kvm_arm_pmu_v3_init>> ret = kvm_vgic_set_owner(vcpu, vcpu->arch.pmu.irq_num,
+	 *   - arch/arm64/kvm/pmu-emul.c|1608| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num != irq)
+	 *   - arch/arm64/kvm/pmu-emul.c|1611| <<pmu_irq_is_valid>> if (vcpu->arch.pmu.irq_num == irq)
+	 *   - arch/arm64/kvm/pmu-emul.c|1761| <<kvm_arm_pmu_v3_set_attr>> vcpu->arch.pmu.irq_num = irq;
+	 *   - arch/arm64/kvm/pmu-emul.c|1846| <<kvm_arm_pmu_v3_get_attr>> irq = vcpu->arch.pmu.irq_num;
+	 */
 	int irq_num;
+	/*
+	 * 在以下使用kvm_pmu->created:
+	 *   - arch/arm64/kvm/pmu-emul.c|1503| <<kvm_arm_pmu_v3_enable>> if (!vcpu->arch.pmu.created) return -EINVAL;
+	 *   - arch/arm64/kvm/pmu-emul.c|1568| <<kvm_arm_pmu_v3_init>> vcpu->arch.pmu.created = true;
+	 *   - arch/arm64/kvm/pmu-emul.c|1709| <<kvm_arm_pmu_v3_set_attr>> if (vcpu->arch.pmu.created) return -EBUSY;
+	 */
 	bool created;
 	bool irq_level;
 };
@@ -44,6 +67,14 @@ static __always_inline bool kvm_arm_support_pmu_v3(void)
 	return static_branch_likely(&kvm_arm_pmu_available);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1555| <<kvm_arm_pmu_v3_enable>> } else if (kvm_arm_pmu_irq_initialized(vcpu)) {
+ *   - arch/arm64/kvm/pmu-emul.c|1587| <<kvm_arm_pmu_v3_init>> if (!kvm_arm_pmu_irq_initialized(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1645| <<pmu_irq_is_valid>> if (!kvm_arm_pmu_irq_initialized(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1835| <<kvm_arm_pmu_v3_set_attr>> if (kvm_arm_pmu_irq_initialized(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1931| <<kvm_arm_pmu_v3_get_attr>> if (!kvm_arm_pmu_irq_initialized(vcpu))
+ */
 #define kvm_arm_pmu_irq_initialized(v)	((v)->arch.pmu.irq_num >= VGIC_NR_SGIS)
 u64 kvm_pmu_get_counter_value(struct kvm_vcpu *vcpu, u64 select_idx);
 void kvm_pmu_set_counter_value(struct kvm_vcpu *vcpu, u64 select_idx, u64 val);
@@ -76,6 +107,26 @@ void kvm_vcpu_pmu_restore_guest(struct kvm_vcpu *vcpu);
 void kvm_vcpu_pmu_restore_host(struct kvm_vcpu *vcpu);
 void kvm_vcpu_pmu_resync_el0(void);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1493| <<kvm_setup_vcpu>> if (kvm_vcpu_has_pmu(vcpu) && !kvm->arch.arm_pmu)
+ *   - arch/arm64/kvm/pmu-emul.c|302| <<kvm_pmu_get_counter_value>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|366| <<kvm_pmu_set_counter_value>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|554| <<kvm_pmu_enable_counter_mask>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|611| <<kvm_pmu_disable_counter_mask>> if (!kvm_vcpu_has_pmu(vcpu) || !val)
+ *   - arch/arm64/kvm/pmu-emul.c|701| <<kvm_pmu_update_state>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1079| <<kvm_pmu_handle_pmcr>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1292| <<kvm_pmu_set_counter_event_type>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1422| <<kvm_pmu_get_pmceid>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1518| <<kvm_arm_pmu_v3_enable>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1808| <<kvm_arm_pmu_v3_set_attr>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1931| <<kvm_arm_pmu_v3_get_attr>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/pmu-emul.c|1962| <<kvm_arm_pmu_v3_has_attr>> if (kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/sys_regs.c|853| <<pmu_visibility>> if (kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/sys_regs.c|891| <<reset_pmevtyper>> if (!kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/sys_regs.c|1782| <<read_sanitised_id_aa64dfr0_el1>> if (kvm_vcpu_has_pmu(vcpu))
+ *   - arch/arm64/kvm/sys_regs.c|1833| <<read_sanitised_id_dfr0_el1>> if (kvm_vcpu_has_pmu(vcpu))
+ */
 #define kvm_vcpu_has_pmu(vcpu)					\
 	(vcpu_has_feature(vcpu, KVM_ARM_VCPU_PMU_V3))
 
diff --git a/include/linux/filelock.h b/include/linux/filelock.h
index daee999d0..76998be05 100644
--- a/include/linux/filelock.h
+++ b/include/linux/filelock.h
@@ -412,6 +412,32 @@ locks_inode_context(const struct inode *inode)
 /* for walking lists of file_locks linked by fl_list */
 #define for_each_file_lock(_fl, _head)	list_for_each_entry(_fl, _head, c.flc_list)
 
+/*
+ * called by:
+ *   - fs/9p/vfs_file.c|131| <<v9fs_file_do_lock>> res = locks_lock_file_wait(filp, fl);
+ *   - fs/9p/vfs_file.c|214| <<v9fs_file_do_lock>> locks_lock_file_wait(filp, fl);
+ *   - fs/afs/flock.c|609| <<afs_do_setlk>> ret = locks_lock_file_wait(file, fl);
+ *   - fs/afs/flock.c|714| <<afs_do_unlk>> ret = locks_lock_file_wait(file, fl);
+ *   - fs/ceph/locks.c|233| <<try_unlock_file>> err = locks_lock_file_wait(file, fl);
+ *   - fs/ceph/locks.c|338| <<ceph_flock>> locks_lock_file_wait(file, fl);
+ *   - fs/ceph/locks.c|361| <<ceph_flock>> err = locks_lock_file_wait(file, fl);
+ *   - fs/dlm/plock.c|225| <<dlm_posix_lock>> if (locks_lock_file_wait(file, fl) < 0)
+ *   - fs/dlm/plock.c|309| <<dlm_posix_unlock>> rv = locks_lock_file_wait(file, fl);
+ *   - fs/fuse/file.c|2728| <<fuse_file_flock>> err = locks_lock_file_wait(file, fl);
+ *   - fs/gfs2/file.c|1448| <<gfs2_lock>> locks_lock_file_wait(file, fl);
+ *   - fs/gfs2/file.c|1501| <<do_flock>> locks_lock_file_wait(file, &request);
+ *   - fs/gfs2/file.c|1527| <<do_flock>> error = locks_lock_file_wait(file, fl);
+ *   - fs/gfs2/file.c|1542| <<do_unflock>> locks_lock_file_wait(file, fl);
+ *   - fs/lockd/clntproc.c|498| <<do_vfs_lock>> return locks_lock_file_wait(fl->c.flc_file, fl);
+ *   - fs/locks.c|2177| <<SYSCALL_DEFINE2(flock)>> error = locks_lock_file_wait(f.file, &fl);
+ *   - fs/nfs/file.c|782| <<do_unlk>> status = locks_lock_file_wait(filp, fl);
+ *   - fs/nfs/file.c|807| <<do_setlk>> status = locks_lock_file_wait(filp, fl);
+ *   - fs/ocfs2/locks.c|58| <<ocfs2_do_flock>> locks_lock_file_wait(file, &request);
+ *   - fs/ocfs2/locks.c|72| <<ocfs2_do_flock>> ret = locks_lock_file_wait(file, fl);
+ *   - fs/ocfs2/locks.c|89| <<ocfs2_do_funlock>> ret = locks_lock_file_wait(file, fl);
+ *   - fs/ocfs2/locks.c|108| <<ocfs2_flock>> return locks_lock_file_wait(file, fl);
+ *   - fs/smb/client/file.c|2317| <<cifs_setlk>> rc = locks_lock_file_wait(file, flock);
+ */
 static inline int locks_lock_file_wait(struct file *filp, struct file_lock *fl)
 {
 	return locks_lock_inode_wait(file_inode(filp), fl);
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1a8942277..68ba59fa8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1210,6 +1210,47 @@ struct perf_sample_data {
 		    PERF_MEM_S(TLB, NA)   |\
 		    PERF_MEM_S(LVLNUM, NA))
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/perf_event.c|853| <<alpha_perf_event_irq_handler>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/arc/kernel/perf_event.c|601| <<arc_pmu_intr>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/csky/kernel/perf_event.c|1139| <<csky_pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/loongarch/kernel/perf_event.c|506| <<pmu_handle_irq>> perf_sample_data_init(&data, 0, 0);
+ *   - arch/mips/kernel/perf_event_mipsxx.c|1598| <<mipsxx_pmu_handle_shared_irq>> perf_sample_data_init(&data, 0, 0);
+ *   - arch/powerpc/perf/core-book3s.c|2299| <<record_and_restart>> perf_sample_data_init(&data, ~0ULL, event->hw.last_period);
+ *   - arch/powerpc/perf/core-fsl-emb.c|635| <<record_and_restart>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/s390/kernel/perf_cpum_cf.c|971| <<cfdiag_push_sample>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/s390/kernel/perf_cpum_sf.c|1120| <<perf_push_sample>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/s390/kernel/perf_pai_crypto.c|465| <<paicrypt_push_sample>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/s390/kernel/perf_pai_ext.c|492| <<paiext_push_sample>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/sparc/kernel/perf_event.c|1667| <<perf_event_nmi_handler>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/x86/events/amd/core.c|1003| <<amd_pmu_v2_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/x86/events/amd/ibs.c|1061| <<perf_ibs_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/x86/events/core.c|1715| <<x86_pmu_handle_irq>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/core.c|3008| <<x86_pmu_handle_guest_pebs>> perf_sample_data_init(data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/core.c|3115| <<handle_pmi_common>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/ds.c|877| <<intel_pmu_drain_bts_buffer>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/ds.c|1745| <<setup_pebs_fixed_sample_data>> perf_sample_data_init(data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/ds.c|1928| <<setup_pebs_adaptive_sample_data>> perf_sample_data_init(data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/knc.c|253| <<knc_pmu_handle_irq>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/x86/events/intel/p4.c|1068| <<p4_pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - arch/x86/events/zhaoxin/core.c|395| <<zhaoxin_pmu_handle_irq>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - arch/xtensa/kernel/perf_event.c|390| <<xtensa_pmu_irq_handler>> perf_sample_data_init(&data, 0, last_period);
+ *   - drivers/perf/apple_m1_cpu_pmu.c|411| <<m1_pmu_handle_irq>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - drivers/perf/arm_pmuv3.c|899| <<armv8pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - drivers/perf/arm_v6_pmu.c|274| <<armv6pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - drivers/perf/arm_v7_pmu.c|978| <<armv7pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - drivers/perf/arm_xscale_pmu.c|183| <<xscale1pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - drivers/perf/arm_xscale_pmu.c|515| <<xscale2pmu_handle_irq>> perf_sample_data_init(&data, 0, hwc->last_period);
+ *   - drivers/perf/riscv_pmu_sbi.c|1021| <<pmu_sbi_ovf_handler>> perf_sample_data_init(&data, 0, hw_evt->last_period);
+ *   - kernel/events/core.c|10044| <<___perf_sw_event>> perf_sample_data_init(&data, addr, 0);
+ *   - kernel/events/core.c|10404| <<perf_tp_event>> perf_sample_data_init(&data, 0, 0);
+ *   - kernel/events/core.c|10421| <<perf_tp_event>> perf_sample_data_init(&data, 0, 0);
+ *   - kernel/events/core.c|10700| <<perf_bp_event>> perf_sample_data_init(&sample, bp->attr.bp_addr, 0);
+ *   - kernel/events/core.c|11147| <<perf_swevent_hrtimer>> perf_sample_data_init(&data, 0, event->hw.last_period);
+ *   - kernel/trace/bpf_trace.c|690| <<BPF_CALL_5>> perf_sample_data_init(sd, 0, 0);
+ *   - kernel/trace/bpf_trace.c|751| <<bpf_event_output>> perf_sample_data_init(sd, 0, 0);
+ */
 static inline void perf_sample_data_init(struct perf_sample_data *data,
 					 u64 addr, u64 period)
 {
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 8a6c6bbcd..39f0dc5ff 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -3012,6 +3012,16 @@ static void _perf_event_enable(struct perf_event *event)
 /*
  * See perf_event_disable();
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|541| <<kvm_pmu_enable_counter_mask>> perf_event_enable(pmc->perf_event);
+ *   - arch/riscv/kvm/vcpu_pmu.c|336| <<kvm_pmu_create_perf_event>> perf_event_enable(pmc->perf_event);
+ *   - arch/riscv/kvm/vcpu_pmu.c|553| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_enable(pmc->perf_event);
+ *   - arch/x86/kvm/pmu.c|272| <<pmc_resume_counter>> perf_event_enable(pmc->perf_event);
+ *   - kernel/events/hw_breakpoint.c|815| <<modify_user_hw_breakpoint>> perf_event_enable(bp);
+ *   - kernel/watchdog_perf.c|169| <<watchdog_hardlockup_enable>> perf_event_enable(this_cpu_read(watchdog_ev));
+ *   - kernel/watchdog_perf.c|251| <<hardlockup_detector_perf_restart>> perf_event_enable(event);
+ */
 void perf_event_enable(struct perf_event *event)
 {
 	struct perf_event_context *ctx;
@@ -5357,6 +5367,19 @@ static void put_event(struct perf_event *event)
  * object, it will not preserve its functionality. Once the last 'user'
  * gives up the object, we'll destroy the thing.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|386| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/riscv/kvm/vcpu_pmu.c|82| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1058| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1060| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+ *   - arch/x86/kvm/pmu.c|281| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|198| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+ *   - kernel/events/core.c|5487| <<perf_release>> perf_event_release_kernel(file->private_data);
+ *   - kernel/events/hw_breakpoint.c|829| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+ *   - kernel/watchdog_perf.c|208| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+ *   - kernel/watchdog_perf.c|275| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+ */
 int perf_event_release_kernel(struct perf_event *event)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -5517,6 +5540,15 @@ static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *
 	return total;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|127| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled,
+ *   - arch/riscv/kvm/vcpu_pmu.c|249| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/riscv/kvm/vcpu_pmu.c|625| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event,
+ *   - arch/x86/kvm/pmu.h|112| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event,
+ *   - include/uapi/linux/bpf.h|5852| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+ *   - tools/include/uapi/linux/bpf.h|5852| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+ */
 u64 perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)
 {
 	struct perf_event_context *ctx;
@@ -5764,6 +5796,10 @@ static void _perf_event_reset(struct perf_event *event)
 	perf_event_update_userpage(event);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|237| <<pmc_pause_counter>> counter += perf_event_pause(pmc->perf_event, true);
+ */
 /* Assume it's not an event with inherit set. */
 u64 perf_event_pause(struct perf_event *event, bool reset)
 {
@@ -9780,6 +9816,38 @@ static int __perf_event_overflow(struct perf_event *event,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/perf_event.c|856| <<alpha_perf_event_irq_handler>> if (perf_event_overflow(event, &data, regs)) {
+ *   - arch/arc/kernel/perf_event.c|603| <<arc_pmu_intr>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/csky/kernel/perf_event.c|1142| <<csky_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/loongarch/kernel/perf_event.c|482| <<handle_associated_event>> if (perf_event_overflow(event, data, regs))
+ *   - arch/mips/kernel/perf_event_mipsxx.c|794| <<handle_associated_event>> if (perf_event_overflow(event, data, regs))
+ *   - arch/powerpc/perf/core-book3s.c|2322| <<record_and_restart>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/powerpc/perf/core-fsl-emb.c|637| <<record_and_restart>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/s390/kernel/perf_cpum_cf.c|983| <<cfdiag_push_sample>> overflow = perf_event_overflow(event, &data, &regs);
+ *   - arch/s390/kernel/perf_cpum_sf.c|1171| <<perf_push_sample>> if (perf_event_overflow(event, &data, &regs)) {
+ *   - arch/s390/kernel/perf_pai_crypto.c|484| <<paicrypt_push_sample>> overflow = perf_event_overflow(event, &data, &regs);
+ *   - arch/s390/kernel/perf_pai_ext.c|509| <<paiext_push_sample>> overflow = perf_event_overflow(event, &data, &regs);
+ *   - arch/sparc/kernel/perf_event.c|1671| <<perf_event_nmi_handler>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/amd/core.c|1011| <<amd_pmu_v2_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/amd/ibs.c|1135| <<perf_ibs_handle_irq>> throttle = perf_event_overflow(event, &data, &regs);
+ *   - arch/x86/events/core.c|1720| <<x86_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/core.c|3009| <<x86_pmu_handle_guest_pebs>> if (perf_event_overflow(event, data, regs))
+ *   - arch/x86/events/intel/core.c|3120| <<handle_pmi_common>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/ds.c|2207| <<handle_pmi_common>> if (perf_event_overflow(event, data, regs))
+ *   - arch/x86/events/intel/knc.c|255| <<knc_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/p4.c|1074| <<p4_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/zhaoxin/core.c|400| <<zhaoxin_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/xtensa/kernel/perf_event.c|391| <<xtensa_pmu_irq_handler>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/apple_m1_cpu_pmu.c|415| <<m1_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/arm_pmuv3.c|935| <<armv8pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/arm_v6_pmu.c|278| <<armv6pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/arm_v7_pmu.c|982| <<armv7pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/arm_xscale_pmu.c|187| <<xscale1pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/arm_xscale_pmu.c|519| <<xscale2pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/riscv_pmu_sbi.c|1031| <<pmu_sbi_ovf_handler>> perf_event_overflow(event, &data, regs);
+ */
 int perf_event_overflow(struct perf_event *event,
 			struct perf_sample_data *data,
 			struct pt_regs *regs)
@@ -12874,6 +12942,20 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1124| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+ *   - arch/riscv/kvm/vcpu_pmu.c|328| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(attr, -1, current, kvm_riscv_pmu_overflow, pmc);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|969| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|974| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+ *   - arch/x86/kvm/pmu.c|215| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|243| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+ *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+ *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+ *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>> return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+ *   - kernel/watchdog_perf.c|135| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+ *   - kernel/watchdog_perf.c|140| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index dd53298ef..95ed39247 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -273,6 +273,9 @@ int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,
 	 * case we do as we are told).
 	 */
 	cpumask_and(&tmp_mask, prog_mask, cpu_online_mask);
+	/*
+	 * msi_domain_set_affinity()
+	 */
 	if (!force && !cpumask_empty(&tmp_mask))
 		ret = chip->irq_set_affinity(data, &tmp_mask, force);
 	else if (force)
diff --git a/kernel/irq/matrix.c b/kernel/irq/matrix.c
index 8f222d1cc..7b15d37bf 100644
--- a/kernel/irq/matrix.c
+++ b/kernel/irq/matrix.c
@@ -9,17 +9,114 @@
 #include <linux/irq.h>
 
 struct cpumap {
+	/*
+	 * 在以下设置cpumap->available:
+	 *   - kernel/irq/matrix.c|91| <<irq_matrix_online>> cm->available = m->alloc_size;
+	 *   - kernel/irq/matrix.c|92| <<irq_matrix_online>> cm->available -= cm->managed + m->systembits_inalloc;
+	 *   - kernel/irq/matrix.c|229| <<irq_matrix_reserve_managed>> cm->available--;
+	 *   - kernel/irq/matrix.c|279| <<irq_matrix_remove_managed>> cm->available++;
+	 *   - kernel/irq/matrix.c|338| <<irq_matrix_assign>> cm->available--;
+	 *   - kernel/irq/matrix.c|405| <<irq_matrix_alloc>> cm->available--;
+	 *   - kernel/irq/matrix.c|443| <<irq_matrix_free>> cm->available++;
+	 * 在以下使用cpumap->available:
+	 *   - kernel/irq/matrix.c|95| <<irq_matrix_online>> m->global_available += cm->available;
+	 *   - kernel/irq/matrix.c|110| <<irq_matrix_offline>> m->global_available -= cm->available;
+	 *   - kernel/irq/matrix.c|146| <<matrix_find_best_cpu>> if (!cm->online || cm->available <= maxavl)
+	 *   - kernel/irq/matrix.c|150| <<matrix_find_best_cpu>> maxavl = cm->available;
+	 *   - kernel/irq/matrix.c|462| <<irq_matrix_available>> return m->global_available - cm->available;
+	 *   - kernel/irq/matrix.c|513| <<irq_matrix_debug_show>> seq_printf(sf, "%*s %4d  %4u  %4u  %4u %4u  %*pbl\n", ind, " ", cpu, cm->available, cm->managed,
+	 */
 	unsigned int		available;
+	/*
+	 * 在以下设置cpumap->allocated:
+	 *   - kernel/irq/matrix.c|223| <<irq_matrix_assign_system>> cm->allocated--;
+	 *   - kernel/irq/matrix.c|342| <<irq_matrix_alloc_managed>> cm->allocated++;
+	 *   - kernel/irq/matrix.c|365| <<irq_matrix_assign>> cm->allocated++;
+	 *   - kernel/irq/matrix.c|438| <<irq_matrix_alloc>> cm->allocated++;
+	 *   - kernel/irq/matrix.c|477| <<irq_matrix_free>> cm->allocated--;
+	 * 在以下使用cpumap->allocated:
+	 *   - kernel/irq/matrix.c|526| <<irq_matrix_allocated>> return cm->allocated - cm->managed_allocated;
+	 *   - kernel/irq/matrix.c|556| <<irq_matrix_debug_show>> seq_printf(sf, "%*s %4d  %4u  %4u  %4u %4u  %*pbl\n", ind, " ", ... cm->managed_allocated, cm->allocated,
+	 */
 	unsigned int		allocated;
+	/*
+	 * 在以下设置cpumap->managed:
+	 *   - kernel/irq/matrix.c|252| <<irq_matrix_reserve_managed>> cm->managed++;
+	 *   - kernel/irq/matrix.c|302| <<irq_matrix_remove_managed>> cm->managed--;
+	 * 在以下使用cpumap->managed:
+	 *   - kernel/irq/matrix.c|108| <<irq_matrix_online>> cm->available -= cm->managed + m->systembits_inalloc;
+	 *   - kernel/irq/matrix.c|290| <<irq_matrix_remove_managed>> if (WARN_ON_ONCE(!cm->managed))
+	 *   - kernel/irq/matrix.c|555| <<irq_matrix_debug_show>> seq_printf(sf, "%*s %4d  %4u  %4u  %4u %4u  %*pbl\n", ind, " ", cpu, cm->available, cm->managed,
+	 */
 	unsigned int		managed;
+	/*
+	 * 在以下设置cpumap->managed_allocated:
+	 * 在以下使用cpumap->managed_allocated:
+	 *   - kernel/irq/matrix.c|192| <<matrix_find_best_cpu_managed>> if (!cm->online || cm->managed_allocated > allocated)
+	 *   - kernel/irq/matrix.c|196| <<matrix_find_best_cpu_managed>> allocated = cm->managed_allocated;
+	 *   - kernel/irq/matrix.c|343| <<irq_matrix_alloc_managed>> cm->managed_allocated++;
+	 *   - kernel/irq/matrix.c|479| <<irq_matrix_free>> cm->managed_allocated--;
+	 *   - kernel/irq/matrix.c|526| <<irq_matrix_allocated>> return cm->allocated - cm->managed_allocated;
+	 *   - kernel/irq/matrix.c|556| <<irq_matrix_debug_show>> seq_printf(sf, "%*s %4d  %4u  %4u  %4u %4u  %*pbl\n", ind, " ", ... cm->managed_allocated, cm->allocated,
+	 */
 	unsigned int		managed_allocated;
+	/*
+	 * 在以下使用cpumap->initialized:
+	 *   - kernel/irq/matrix.c|106| <<irq_matrix_online>> if (!cm->initialized) {
+	 *   - kernel/irq/matrix.c|109| <<irq_matrix_online>> cm->initialized = true;
+	 */
 	bool			initialized;
+	/*
+	 * 在以下设置cpumap->online:
+	 *   - kernel/irq/matrix.c|112| <<irq_matrix_online>> cm->online = true;
+	 *   - kernel/irq/matrix.c|127| <<irq_matrix_offline>> cm->online = false;
+	 * 在以下使用cpumap->online:
+	 *   - kernel/irq/matrix.c|104| <<irq_matrix_online>> BUG_ON(cm->online);
+	 *   - kernel/irq/matrix.c|171| <<matrix_find_best_cpu>> if (!cm->online || cm->available <= maxavl)
+	 *   - kernel/irq/matrix.c|192| <<matrix_find_best_cpu_managed>> if (!cm->online || cm->managed_allocated > allocated)
+	 *   - kernel/irq/matrix.c|253| <<irq_matrix_reserve_managed>> if (cm->online) {
+	 *   - kernel/irq/matrix.c|303| <<irq_matrix_remove_managed>> if (cm->online) {
+	 *   - kernel/irq/matrix.c|481| <<irq_matrix_free>> if (cm->online)
+	 *   - kernel/irq/matrix.c|486| <<irq_matrix_free>> if (cm->online)
+	 */
 	bool			online;
+	/*
+	 * 在以下使用cpumap->managed_map:
+	 *   - kernel/irq/matrix.c|90| <<irq_alloc_matrix>> cm->managed_map = &cm->alloc_map[matrix_size];
+	 *   - kernel/irq/matrix.c|143| <<matrix_alloc_area>> bitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);
+	 *   - kernel/irq/matrix.c|149| <<matrix_alloc_area>> bitmap_set(cm->managed_map, area, num);
+	 *   - kernel/irq/matrix.c|294| <<irq_matrix_remove_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|300| <<irq_matrix_remove_managed>> clear_bit(bit, cm->managed_map);
+	 *   - kernel/irq/matrix.c|337| <<irq_matrix_alloc_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 */
 	unsigned long		*managed_map;
+	/*
+	 * 在以下使用cpumap->alloc_map[]:
+	 *   - kernel/irq/matrix.c|80| <<irq_alloc_matrix>> m->maps = __alloc_percpu(struct_size(m->maps, alloc_map, matrix_size * 2),
+	 *   - kernel/irq/matrix.c|90| <<irq_alloc_matrix>> cm->managed_map = &cm->alloc_map[matrix_size];
+	 *   - kernel/irq/matrix.c|144| <<matrix_alloc_area>> bitmap_or(m->scratch_map, m->scratch_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|151| <<matrix_alloc_area>> bitmap_set(cm->alloc_map, area, num);
+	 *   - kernel/irq/matrix.c|222| <<irq_matrix_assign_system>> BUG_ON(!test_and_clear_bit(bit, cm->alloc_map));
+	 *   - kernel/irq/matrix.c|294| <<irq_matrix_remove_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|337| <<irq_matrix_alloc_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|341| <<irq_matrix_alloc_managed>> set_bit(bit, cm->alloc_map);
+	 *   - kernel/irq/matrix.c|363| <<irq_matrix_assign>> if (WARN_ON_ONCE(test_and_set_bit(bit, cm->alloc_map)))
+	 *   - kernel/irq/matrix.c|474| <<irq_matrix_free>> if (WARN_ON_ONCE(!test_and_clear_bit(bit, cm->alloc_map)))
+	 *   - kernel/irq/matrix.c|557| <<irq_matrix_debug_show>> m->matrix_bits, cm->alloc_map);
+	 */
 	unsigned long		alloc_map[];
 };
 
 struct irq_matrix {
+	/*
+	 * 在以下设置irq_matrix->matrix_bits:
+	 *   - kernel/irq/matrix.c|76| <<irq_alloc_matrix>> m->matrix_bits = matrix_bits;
+	 * 在以下使用irq_matrix->matrix_bits:
+	 *   - kernel/irq/matrix.c|217| <<irq_matrix_assign_system>> BUG_ON(bit > m->matrix_bits);
+	 *   - kernel/irq/matrix.c|540| <<irq_matrix_debug_show>> unsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);
+	 *   - kernel/irq/matrix.c|547| <<irq_matrix_debug_show>> seq_printf(sf, "System: %u: %*pbl\n", nsys, m->matrix_bits,
+	 *   - kernel/irq/matrix.c|557| <<irq_matrix_debug_show>> m->matrix_bits, cm->alloc_map);
+	 */
 	unsigned int		matrix_bits;
 	unsigned int		alloc_start;
 	unsigned int		alloc_end;
@@ -28,9 +125,57 @@ struct irq_matrix {
 	unsigned int		global_reserved;
 	unsigned int		systembits_inalloc;
 	unsigned int		total_allocated;
+	/*
+	 * 在以下使用irq_matrix->online_maps:
+	 *   - kernel/irq/matrix.c|215| <<irq_matrix_online>> m->online_maps++;
+	 *   - kernel/irq/matrix.c|230| <<irq_matrix_offline>> m->online_maps--;
+	 *   - kernel/irq/matrix.c|324| <<irq_matrix_assign_system>> BUG_ON(m->online_maps > 1 || (m->online_maps && !replace));
+	 *   - kernel/irq/matrix.c|654| <<irq_matrix_debug_show>> seq_printf(sf, "Online bitmaps: %6u\n", m->online_maps);
+	 */
 	unsigned int		online_maps;
+	/*
+	 * 在以下使用irq_matrix->maps:
+	 *   - kernel/irq/matrix.c|176| <<irq_alloc_matrix>> m->maps = __alloc_percpu(struct_size(m->maps, alloc_map, matrix_size * 2),
+	 *   - kernel/irq/matrix.c|177| <<irq_alloc_matrix>> __alignof__(*m->maps)); 
+	 *   - kernel/irq/matrix.c|178| <<irq_alloc_matrix>> if (!m->maps) { 
+	 *   - kernel/irq/matrix.c|184| <<irq_alloc_matrix>> struct cpumap *cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|204| <<irq_matrix_online>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|225| <<irq_matrix_offline>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|271| <<matrix_find_best_cpu>> cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|296| <<matrix_find_best_cpu_managed>> cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|321| <<irq_matrix_assign_system>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|352| <<irq_matrix_reserve_managed>> struct cpumap *cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|393| <<irq_matrix_remove_managed>> struct cpumap *cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|440| <<irq_matrix_alloc_managed>> cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|465| <<irq_matrix_assign>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|540| <<irq_matrix_alloc>> cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|575| <<irq_matrix_free>> struct cpumap *cm = per_cpu_ptr(m->maps, cpu);
+	 *   - kernel/irq/matrix.c|606| <<irq_matrix_available>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|630| <<irq_matrix_allocated>> struct cpumap *cm = this_cpu_ptr(m->maps);
+	 *   - kernel/irq/matrix.c|663| <<irq_matrix_debug_show>> struct cpumap *cm = per_cpu_ptr(m->maps, cpu);
+	 */
 	struct cpumap __percpu	*maps;
+	/*
+	 * 在以下使用irq_matrix->system_map:
+	 *   - kernel/irq/matrix.c|170| <<irq_alloc_matrix>> m->system_map = &m->scratch_map[matrix_size];
+	 *   - kernel/irq/matrix.c|245| <<matrix_alloc_area>> bitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);
+	 *   - kernel/irq/matrix.c|326| <<irq_matrix_assign_system>> set_bit(bit, m->system_map);
+	 *   - kernel/irq/matrix.c|651| <<irq_matrix_debug_show>> unsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);
+	 *   - kernel/irq/matrix.c|659| <<irq_matrix_debug_show>> m->system_map);
+	 */
 	unsigned long		*system_map;
+	/*
+	 * 在以下使用irq_matrix->scratch_map[]:
+	 *   - kernel/irq/matrix.c|149| <<irq_alloc_matrix>> m = kzalloc(struct_size(m, scratch_map, matrix_size * 2), GFP_KERNEL);
+	 *   - kernel/irq/matrix.c|153| <<irq_alloc_matrix>> m->system_map = &m->scratch_map[matrix_size];
+	 *   - kernel/irq/matrix.c|222| <<matrix_alloc_area>> bitmap_or(m->scratch_map, cm->managed_map, m->system_map, end);
+	 *   - kernel/irq/matrix.c|223| <<matrix_alloc_area>> bitmap_or(m->scratch_map, m->scratch_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|224| <<matrix_alloc_area>> area = bitmap_find_next_zero_area(m->scratch_map, end, start, num, 0);
+	 *   - kernel/irq/matrix.c|373| <<irq_matrix_remove_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|375| <<irq_matrix_remove_managed>> bit = find_first_bit(m->scratch_map, end);
+	 *   - kernel/irq/matrix.c|416| <<irq_matrix_alloc_managed>> bitmap_andnot(m->scratch_map, cm->managed_map, cm->alloc_map, end);
+	 *   - kernel/irq/matrix.c|417| <<irq_matrix_alloc_managed>> bit = find_first_bit(m->scratch_map, end);
+	 */
 	unsigned long		scratch_map[];
 };
 
@@ -44,6 +189,11 @@ struct irq_matrix {
  * @alloc_end:		At which bit the allocation search ends, i.e first
  *			invalid bit
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|815| <<arch_early_irq_init>> vector_matrix = irq_alloc_matrix(NR_VECTORS, FIRST_EXTERNAL_VECTOR, FIRST_SYSTEM_VECTOR);
+ *   - drivers/irqchip/irq-riscv-imsic-state.c|499| <<imsic_matrix_init>> imsic->matrix = irq_alloc_matrix(global->nr_ids + 1, 0, global->nr_ids + 1);
+ */
 __init struct irq_matrix *irq_alloc_matrix(unsigned int matrix_bits,
 					   unsigned int alloc_start,
 					   unsigned int alloc_end)
@@ -81,6 +231,12 @@ __init struct irq_matrix *irq_alloc_matrix(unsigned int matrix_bits,
  * irq_matrix_online - Bring the local CPU matrix online
  * @m:		Matrix pointer
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|784| <<lapic_assign_system_vectors>> irq_matrix_online(vector_matrix);
+ *   - arch/x86/kernel/apic/vector.c|845| <<lapic_online>> irq_matrix_online(vector_matrix);
+ *   - drivers/irqchip/irq-riscv-imsic-state.c|473| <<imsic_state_online>> irq_matrix_online(imsic->matrix);
+ */
 void irq_matrix_online(struct irq_matrix *m)
 {
 	struct cpumap *cm = this_cpu_ptr(m->maps);
@@ -113,6 +269,11 @@ void irq_matrix_offline(struct irq_matrix *m)
 	trace_irq_matrix_offline(m);
 }
 
+/*
+ * called by:
+ *   - kernel/irq/matrix.c|224| <<irq_matrix_reserve_managed>> bit = matrix_alloc_area(m, cm, 1, true);
+ *   - kernel/irq/matrix.c|401| <<irq_matrix_alloc>> bit = matrix_alloc_area(m, cm, 1, false);
+ */
 static unsigned int matrix_alloc_area(struct irq_matrix *m, struct cpumap *cm,
 				      unsigned int num, bool managed)
 {
@@ -131,6 +292,10 @@ static unsigned int matrix_alloc_area(struct irq_matrix *m, struct cpumap *cm,
 	return area;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/matrix.c|396| <<irq_matrix_alloc>> cpu = matrix_find_best_cpu(m, msk);
+ */
 /* Find the best CPU which has the lowest vector allocation count */
 static unsigned int matrix_find_best_cpu(struct irq_matrix *m,
 					const struct cpumask *msk)
@@ -152,6 +317,10 @@ static unsigned int matrix_find_best_cpu(struct irq_matrix *m,
 	return best_cpu;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/matrix.c|409| <<irq_matrix_alloc_managed>> cpu = matrix_find_best_cpu_managed(m, msk);
+ */
 /* Find the best CPU which has the lowest number of managed IRQs allocated */
 static unsigned int matrix_find_best_cpu_managed(struct irq_matrix *m,
 						const struct cpumask *msk)
@@ -289,6 +458,10 @@ void irq_matrix_remove_managed(struct irq_matrix *m, const struct cpumask *msk)
  * @msk:	Which CPUs to search in
  * @mapped_cpu:	Pointer to store the CPU for which the irq was allocated
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|335| <<assign_managed_vector>> vector = irq_matrix_alloc_managed(vector_matrix, vector_searchmask, &cpu);
+ */
 int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,
 			     unsigned int *mapped_cpu)
 {
@@ -380,6 +553,11 @@ void irq_matrix_remove_reserved(struct irq_matrix *m)
  * @reserved:	Allocate previously reserved interrupts
  * @mapped_cpu: Pointer to store the CPU for which the irq was allocated
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|259| <<assign_vector_locked>> vector = irq_matrix_alloc(vector_matrix, dest, resvd, &cpu);
+ *   - drivers/irqchip/irq-riscv-imsic-state.c|379| <<imsic_vector_alloc>> local_id = irq_matrix_alloc(imsic->matrix, mask, false, &cpu);
+ */
 int irq_matrix_alloc(struct irq_matrix *m, const struct cpumask *msk,
 		     bool reserved, unsigned int *mapped_cpu)
 {
@@ -421,6 +599,14 @@ int irq_matrix_alloc(struct irq_matrix *m, const struct cpumask *msk,
  * @managed:	If true, the interrupt is managed and not accounted
  *		as available.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|177| <<apic_update_vector>> irq_matrix_free(vector_matrix, apicd->cpu, apicd->vector,
+ *   - arch/x86/kernel/apic/vector.c|360| <<clear_irq_vector>> irq_matrix_free(vector_matrix, apicd->cpu, vector, managed);
+ *   - arch/x86/kernel/apic/vector.c|369| <<clear_irq_vector>> irq_matrix_free(vector_matrix, apicd->prev_cpu, vector, managed);
+ *   - arch/x86/kernel/apic/vector.c|952| <<free_moved_vector>> irq_matrix_free(vector_matrix, cpu, vector, managed);
+ *   - drivers/irqchip/irq-riscv-imsic-state.c|399| <<imsic_vector_free>> irq_matrix_free(imsic->matrix, vec->cpu, vec->local_id, false);
+ */
 void irq_matrix_free(struct irq_matrix *m, unsigned int cpu,
 		     unsigned int bit, bool managed)
 {
@@ -493,6 +679,11 @@ unsigned int irq_matrix_allocated(struct irq_matrix *m)
  *
  * Note, this is a lockless snapshot.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|629| <<x86_vector_debug_show>> irq_matrix_debug_show(m, vector_matrix, ind);
+ *   - drivers/irqchip/irq-riscv-imsic-state.c|356| <<imsic_vector_debug_show_summary>> irq_matrix_debug_show(m, imsic->matrix, ind);
+ */
 void irq_matrix_debug_show(struct seq_file *sf, struct irq_matrix *m, int ind)
 {
 	unsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);
diff --git a/kernel/irq/msi.c b/kernel/irq/msi.c
index 5fa0547ec..81be21636 100644
--- a/kernel/irq/msi.c
+++ b/kernel/irq/msi.c
@@ -667,6 +667,9 @@ int msi_domain_set_affinity(struct irq_data *irq_data,
 	struct msi_msg msg[2] = { [1] = { }, };
 	int ret;
 
+	/*
+	 * apic_set_affinity()
+	 */
 	ret = parent->chip->irq_set_affinity(parent, mask, force);
 	if (ret >= 0 && ret != IRQ_SET_MASK_OK_DONE) {
 		BUG_ON(irq_chip_compose_msi_msg(irq_data, msg));
diff --git a/kernel/resource.c b/kernel/resource.c
index a83040fde..f451fb57c 100644
--- a/kernel/resource.c
+++ b/kernel/resource.c
@@ -615,6 +615,11 @@ static void resource_clip(struct resource *res, resource_size_t min,
  * Find empty space in the resource tree with the given range and
  * alignment constraints
  */
+/*
+ * called by:
+ *   - kernel/resource.c|696| <<find_resource_space>> return __find_resource_space(root, NULL, new, size, constraint);
+ *   - kernel/resource.c|720| <<reallocate_resource>> if ((err = __find_resource_space(root, old, &new, newsize, constraint)))
+ */
 static int __find_resource_space(struct resource *root, struct resource *old,
 				 struct resource *new, resource_size_t size,
 				 struct resource_constraint *constraint)
@@ -689,6 +694,11 @@ next:		if (!this || this->end == root->end)
  * * %0		- if successful, @new members start, end, and flags are altered.
  * * %-EBUSY	- if no empty space was found.
  */
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|1015| <<pbus_upstream_space_available>> if (find_resource_space(r, &gap, size, &constraint) == 0) {
+ *   - kernel/resource.c|783| <<allocate_resource>> err = find_resource_space(root, new, size, &constraint);
+ */
 int find_resource_space(struct resource *root, struct resource *new,
 			resource_size_t size,
 			struct resource_constraint *constraint)
@@ -746,6 +756,36 @@ static int reallocate_resource(struct resource *root, struct resource *old,
 }
 
 
+/*
+ * called by:
+ *   - 2 arch/arm/mach-footbridge/dc21285.c|276| <<dc21285_setup>> allocate_resource(&iomem_resource, &res[1], 0x20000000,
+ *   - arch/arm/mach-footbridge/dc21285.c|278| <<dc21285_setup>> allocate_resource(&iomem_resource, &res[0], 0x40000000,
+ *   - arch/m68k/amiga/chipram.c|81| <<amiga_chip_alloc_res>> error = allocate_resource(&chipram_res, res, size, 0, UINT_MAX, 
+ *   - arch/m68k/atari/stram.c|169| <<atari_stram_alloc>> error = allocate_resource(&stram_pool, res, size, 0, UINT_MAX,
+ *   - arch/mips/txx9/generic/pci.c|160| <<txx9_alloc_pci_controller>> if (allocate_resource(&iomem_resource,
+ *   - arch/mips/txx9/generic/pci.c|182| <<txx9_alloc_pci_controller>> if (allocate_resource(&iomem_resource,
+ *   - arch/sparc/kernel/ioport.c|209| <<_sparc_ioremap>> if (allocate_resource(&sparc_iomap, res,
+ *   - arch/sparc/kernel/ioport.c|246| <<sparc_dma_alloc_resource>> if (allocate_resource(&_sparc_dvma, res, len, _sparc_dvma.start,
+ *   - drivers/gpu/drm/gma500/gtt.c|37| <<psb_gtt_allocate_resource>> ret = allocate_resource(root, res, size, start, end, align, NULL, NULL);
+ *   - 11 drivers/input/joystick/iforce/iforce-ff.c|25| <<make_magnitude_modifier>> if (allocate_resource(&(iforce->device_memory), mod_chunk, 2,
+ *   - drivers/input/joystick/iforce/iforce-ff.c|58| <<make_period_modifier>> if (allocate_resource(&(iforce->device_memory), mod_chunk, 0x0c,
+ *   - drivers/input/joystick/iforce/iforce-ff.c|98| <<make_envelope_modifier>> if (allocate_resource(&(iforce->device_memory), mod_chunk, 0x0e,
+ *   - drivers/input/joystick/iforce/iforce-ff.c|135| <<make_condition_modifier>> if (allocate_resource(&(iforce->device_memory), mod_chunk, 8,
+ *   - drivers/memory/omap-gpmc.c|1000| <<gpmc_cs_request>> r = allocate_resource(&gpmc_mem_root, res, size, 0, ~0,
+ *   - drivers/mtd/maps/scx200_docflash.c|143| <<init_scx200_docflash>> if (allocate_resource(&iomem_resource, &docmem,
+ *   - drivers/parisc/ccio-dma.c|1454| <<ccio_allocate_resource>> !allocate_resource(parent, res, size, min, max, align, NULL, NULL))
+ *   - drivers/parisc/ccio-dma.c|1458| <<ccio_allocate_resource>> !allocate_resource(parent + 1, res, size, min, max, align,
+ *   - 19 drivers/parisc/ccio-dma.c|1478| <<ccio_allocate_resource>> return allocate_resource(parent, res, size, min, max, align, NULL,NULL);
+ *   - drivers/parisc/iommu.h|49| <<ccio_allocate_resource>> allocate_resource(&iomem_resource, res, size, min, max, \
+ *   - drivers/pci/bus.c|231| <<pci_bus_alloc_from_region>> ret = allocate_resource(r, res, size, min_used, max,
+ *   - drivers/pcmcia/rsrc_iodyn.c|74| <<__iodyn_find_io_region>> ret = allocate_resource(&ioport_resource, res, num, min, ~0UL,
+ *   - drivers/pcmcia/rsrc_nonstatic.c|706| <<__nonstatic_find_io_region>> ret = allocate_resource(&ioport_resource, res, num, min, ~0UL,
+ *   - drivers/pcmcia/rsrc_nonstatic.c|842| <<nonstatic_find_mem_region>> ret = allocate_resource(&iomem_resource,
+ *   - drivers/pcmcia/yenta_socket.c|660| <<yenta_search_one_res>> if (allocate_resource(root, res, size, start, end, align,
+ *   - drivers/watchdog/via_wdt.c|175| <<wdt_probe>> if (allocate_resource(&iomem_resource, &wdt_res, VIA_WDT_MMIO_LEN,
+ *   - drivers/xen/balloon.c|248| <<additional_memory_resource>> ret = allocate_resource(&iomem_resource, res,
+ *   - drivers/xen/unpopulated-alloc.c|52| <<fill_list>> ret = allocate_resource(target_resource, res,
+ */
 /**
  * allocate_resource - allocate empty slot in the resource tree given range & alignment.
  * 	The resource will be reallocated with a new size if it was already allocated
@@ -1972,6 +2012,12 @@ struct resource *devm_request_free_mem_region(struct device *dev,
 }
 EXPORT_SYMBOL_GPL(devm_request_free_mem_region);
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_hv_uvmem.c|1177| <<kvmppc_uvmem_init>> res = request_free_mem_region(&iomem_resource, size, "kvmppc_uvmem");
+ *   - drivers/gpu/drm/nouveau/nouveau_dmem.c|243| <<nouveau_dmem_chunk_alloc>> res = request_free_mem_region(&iomem_resource, DMEM_CHUNK_SIZE,
+ *   - lib/test_hmm.c|510| <<dmirror_allocate_chunk>> res = request_free_mem_region(&iomem_resource, DEVMEM_CHUNK_SIZE,
+ */
 struct resource *request_free_mem_region(struct resource *base,
 		unsigned long size, const char *name)
 {
diff --git a/mm/util.c b/mm/util.c
index bd283e213..dfd6eaed4 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -513,6 +513,12 @@ EXPORT_SYMBOL_IF_KUNIT(arch_pick_mmap_layout);
  * * 0       on success
  * * -ENOMEM if RLIMIT_MEMLOCK would be exceeded.
  */
+/*
+ * called by:
+ *   - drivers/iommu/iommufd/pages.c|849| <<update_mm_locked_vm>> rc = __account_locked_vm(pages->source_mm, npages, inc, pages->source_task, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|420| <<mm_lock_acct>> ret = __account_locked_vm(mm, abs(npage), npage > 0, task, lock_cap);
+ *   - mm/util.c|567| <<account_locked_vm>> ret = __account_locked_vm(mm, pages, inc, current, capable(CAP_IPC_LOCK));
+ */
 int __account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc,
 			struct task_struct *task, bool bypass_rlim)
 {
@@ -556,6 +562,22 @@ EXPORT_SYMBOL_GPL(__account_locked_vm);
  * * 0       on success, or if mm is NULL
  * * -ENOMEM if RLIMIT_MEMLOCK would be exceeded.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_vio.c|285| <<kvm_spapr_tce_release>> account_locked_vm(kvm->mm,
+ *   - arch/powerpc/kvm/book3s_64_vio.c|314| <<kvm_vm_ioctl_create_spapr_tce>> ret = account_locked_vm(mm, kvmppc_stt_pages(npages), true);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|359| <<kvm_vm_ioctl_create_spapr_tce>> account_locked_vm(mm, kvmppc_stt_pages(npages), false);
+ *   - arch/powerpc/mm/book3s64/iommu_api.c|66| <<mm_iommu_do_alloc>> ret = account_locked_vm(mm, entries, true);
+ *   - arch/powerpc/mm/book3s64/iommu_api.c|181| <<mm_iommu_do_alloc>> account_locked_vm(mm, locked_entries, false);
+ *   - arch/powerpc/mm/book3s64/iommu_api.c|282| <<mm_iommu_put>> account_locked_vm(mm, unlock_entries, false);
+ *   - drivers/fpga/dfl-afu-dma-region.c|41| <<afu_dma_pin_pages>> ret = account_locked_vm(current->mm, npages, true);
+ *   - drivers/fpga/dfl-afu-dma-region.c|70| <<afu_dma_pin_pages>> account_locked_vm(current->mm, npages, false);
+ *   - drivers/fpga/dfl-afu-dma-region.c|90| <<afu_dma_unpin_pages>> account_locked_vm(current->mm, npages, false);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|294| <<tce_iommu_enable>> ret = account_locked_vm(container->mm, locked, true);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|313| <<tce_iommu_disable>> account_locked_vm(container->mm, container->locked_pages, false);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|621| <<tce_iommu_create_table>> ret = account_locked_vm(container->mm, table_size >> PAGE_SHIFT, true);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|640| <<tce_iommu_free_table>> account_locked_vm(container->mm, pages, false);
+ */
 int account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc)
 {
 	int ret;
@@ -564,6 +586,12 @@ int account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc)
 		return 0;
 
 	mmap_write_lock(mm);
+	/*
+	 * called by:
+	 *   - drivers/iommu/iommufd/pages.c|849| <<update_mm_locked_vm>> rc = __account_locked_vm(pages->source_mm, npages, inc, pages->source_task, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|420| <<mm_lock_acct>> ret = __account_locked_vm(mm, abs(npage), npage > 0, task, lock_cap);
+	 *   - mm/util.c|567| <<account_locked_vm>> ret = __account_locked_vm(mm, pages, inc, current, capable(CAP_IPC_LOCK));
+	 */
 	ret = __account_locked_vm(mm, pages, inc, current,
 				  capable(CAP_IPC_LOCK));
 	mmap_write_unlock(mm);
-- 
2.39.3 (Apple Git-146)

