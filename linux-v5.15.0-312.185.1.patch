From 25050577e07dd60d9dcb269840549a2a4acabfa5 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 10 Nov 2025 09:28:50 -0800
Subject: [PATCH 1/1] linux v5.15.0-312.185.1

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/kvm/arm.c                     |  10 +
 arch/arm64/kvm/mmu.c                     |  13 +
 arch/x86/entry/vdso/vma.c                |  25 +
 arch/x86/include/asm/kvm_host.h          | 116 ++++
 arch/x86/include/asm/pvclock.h           |  39 ++
 arch/x86/include/asm/spec-ctrl.h         |   4 +
 arch/x86/include/asm/vdso/gettimeofday.h |  16 +
 arch/x86/kernel/kvmclock.c               |  27 +
 arch/x86/kernel/pvclock.c                |  75 +++
 arch/x86/kvm/hyperv.c                    |   9 +
 arch/x86/kvm/lapic.c                     |  78 +++
 arch/x86/kvm/mmu/mmu.c                   | 104 +++
 arch/x86/kvm/mmu/tdp_mmu.c               |  13 +
 arch/x86/kvm/svm/svm.c                   |  23 +
 arch/x86/kvm/vmx/nested.c                |  18 +
 arch/x86/kvm/vmx/vmx.c                   |  16 +
 arch/x86/kvm/x86.c                       | 792 +++++++++++++++++++++++
 arch/x86/kvm/x86.h                       |  14 +
 arch/x86/xen/time.c                      |   6 +
 drivers/infiniband/core/cq.c             |   6 +
 drivers/infiniband/core/device.c         |  19 +
 drivers/infiniband/core/nldev.c          |   5 +
 drivers/infiniband/sw/rxe/rxe.c          |  29 +
 drivers/infiniband/sw/rxe/rxe_comp.c     |   7 +
 drivers/infiniband/sw/rxe/rxe_cq.c       |  12 +
 drivers/infiniband/sw/rxe/rxe_mr.c       |  24 +
 drivers/infiniband/sw/rxe/rxe_net.c      | 115 ++++
 drivers/infiniband/sw/rxe/rxe_qp.c       |  16 +
 drivers/infiniband/sw/rxe/rxe_queue.c    |  16 +
 drivers/infiniband/sw/rxe/rxe_recv.c     |   5 +
 drivers/infiniband/sw/rxe/rxe_req.c      |  13 +
 drivers/infiniband/sw/rxe/rxe_resp.c     |  37 ++
 drivers/infiniband/sw/rxe/rxe_srq.c      |   8 +
 drivers/infiniband/sw/rxe/rxe_task.c     |  88 +++
 drivers/infiniband/sw/rxe/rxe_verbs.c    |  47 ++
 drivers/net/virtio_net.c                 |  72 +++
 drivers/pci/rom.c                        |   4 +
 drivers/ptp/ptp_kvm_x86.c                |   5 +
 drivers/scsi/virtio_scsi.c               | 166 +++++
 drivers/target/target_core_configfs.c    |   7 +
 drivers/vfio/pci/vfio_pci_core.c         |   3 +
 drivers/vhost/iotlb.c                    |  11 +
 drivers/vhost/net.c                      |  42 ++
 drivers/vhost/scsi.c                     | 142 ++++
 drivers/vhost/vhost.c                    | 516 +++++++++++++++
 drivers/vhost/vhost.h                    |  24 +
 drivers/vhost/vsock.c                    |  18 +
 drivers/virtio/virtio_ring.c             | 128 ++++
 include/linux/entry-kvm.h                |  21 +
 include/linux/kvm_host.h                 |  53 ++
 include/net/udp_tunnel.h                 |  15 +
 kernel/dma/swiotlb.c                     |   4 +
 kernel/entry/kvm.c                       |   9 +
 kernel/time/posix-timers.c               |  12 +
 kernel/time/time.c                       |  24 +
 kernel/time/timekeeping.c                |  17 +
 lib/vdso/gettimeofday.c                  |   5 +
 net/ipv4/ip_output.c                     |  17 +
 net/ipv4/udp.c                           |  16 +
 net/ipv4/udp_tunnel_core.c               |  16 +
 virt/kvm/eventfd.c                       |  32 +
 virt/kvm/kvm_main.c                      | 342 ++++++++++
 62 files changed, 3566 insertions(+)

diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index eb091274a4c0..952ec4cfcd16 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -750,6 +750,11 @@ static bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu, int *ret)
 		}
 	}
 
+	/*
+	 * 在以下使用xfer_to_guest_mode_work_pending():
+	 *   - arch/arm64/kvm/arm.c|755| <<kvm_vcpu_exit_request>> xfer_to_guest_mode_work_pending();
+	 *   - arch/x86/kvm/x86.c|2048| <<kvm_vcpu_exit_request>> xfer_to_guest_mode_work_pending();
+	 */
 	return kvm_request_pending(vcpu) ||
 			need_new_vmid_gen(&vcpu->arch.hw_mmu->vmid) ||
 			xfer_to_guest_mode_work_pending();
@@ -816,6 +821,11 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		/*
 		 * Check conditions before entering the guest
 		 */
+		/*
+		 * 在以下使用xfer_to_guest_mode_handle_work():
+		 *   - arch/arm64/kvm/arm.c|819| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+		 *   - arch/x86/kvm/x86.c|10944| <<vcpu_run>> r = xfer_to_guest_mode_handle_work(vcpu);
+		 */
 		ret = xfer_to_guest_mode_handle_work(vcpu);
 		if (!ret)
 			ret = 1;
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index db0486402d6a..91c2f57c448c 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -1176,6 +1176,19 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 	/* Mark the page dirty only if the fault is handled successfully */
 	if (writable && !ret) {
 		kvm_set_pfn_dirty(pfn);
+		/*
+		 * 在以下使用mark_page_dirty_in_slot():
+		 *   - arch/arm64/kvm/mmu.c|1179| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|267| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+		 *   - arch/x86/kvm/x86.c|3401| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+		 *   - arch/x86/kvm/x86.c|4596| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+		 *   - arch/x86/kvm/xen.c|204| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+		 *   - virt/kvm/kvm_main.c|2898| <<__kvm_unmap_gfn>> mark_page_dirty_in_slot(kvm, memslot, map->gfn);
+		 *   - virt/kvm/kvm_main.c|3113| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+		 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+		 *   - virt/kvm/kvm_main.c|3358| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+		 *   - virt/kvm/kvm_main.c|3367| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+		 */
 		mark_page_dirty_in_slot(kvm, memslot, gfn);
 	}
 
diff --git a/arch/x86/entry/vdso/vma.c b/arch/x86/entry/vdso/vma.c
index a380f7ecdd54..7bc77056ef20 100644
--- a/arch/x86/entry/vdso/vma.c
+++ b/arch/x86/entry/vdso/vma.c
@@ -56,6 +56,13 @@ void __init init_vdso_image(const struct vdso_image *image)
 static const struct vm_special_mapping vvar_mapping;
 struct linux_binprm;
 
+/*
+ * 236 static const struct vm_special_mapping vdso_mapping = {
+ * 237         .name = "[vdso]",
+ * 238         .fault = vdso_fault,
+ * 239         .mremap = vdso_mremap,
+ * 240 };
+ */
 static vm_fault_t vdso_fault(const struct vm_special_mapping *sm,
 		      struct vm_area_struct *vma, struct vm_fault *vmf)
 {
@@ -86,6 +93,13 @@ static void vdso_fix_landing(const struct vdso_image *image,
 #endif
 }
 
+/*
+ * 236 static const struct vm_special_mapping vdso_mapping = {
+ * 237         .name = "[vdso]",
+ * 238         .fault = vdso_fault,
+ * 239         .mremap = vdso_mremap,
+ * 240 };
+ */
 static int vdso_mremap(const struct vm_special_mapping *sm,
 		struct vm_area_struct *new_vma)
 {
@@ -147,6 +161,12 @@ static inline struct page *find_timens_vvar_page(struct vm_area_struct *vma)
 }
 #endif
 
+/*
+ * 255 static const struct vm_special_mapping vvar_mapping = {
+ * 256         .name = "[vvar]",
+ * 257         .fault = vvar_fault,
+ * 258 };
+ */
 static vm_fault_t vvar_fault(const struct vm_special_mapping *sm,
 		      struct vm_area_struct *vma, struct vm_fault *vmf)
 {
@@ -202,6 +222,11 @@ static vm_fault_t vvar_fault(const struct vm_special_mapping *sm,
 
 		return vmf_insert_pfn(vma, vmf->address, pfn);
 	} else if (sym_offset == image->sym_pvclock_page) {
+		/*
+		 * 在以下使用pvclock_get_pvti_cpu0_va():
+		 *   - arch/x86/entry/vdso/vma.c|206| <<vvar_fault>> pvclock_get_pvti_cpu0_va();
+		 *   - drivers/ptp/ptp_kvm_x86.c|48| <<kvm_arch_ptp_init>> if (!pvclock_get_pvti_cpu0_va()) {
+		 */
 		struct pvclock_vsyscall_time_info *pvti =
 			pvclock_get_pvti_cpu0_va();
 		if (pvti && vclock_was_used(VDSO_CLOCKMODE_PVCLOCK)) {
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 8b6149faa33f..94899347e664 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -70,6 +70,26 @@
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2881| <<kvm_gen_update_masterclock>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3030| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3122| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3131| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3515| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4547| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5107| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl(KVM_SET_CLOCK)>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *   - arch/x86/kvm/x86.c|8594| <<kvm_hyperv_tsc_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8655| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9990| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|10312| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11569| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|371| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|394| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *
+ * 处理的函数kvm_guest_time_update()
+ */
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
@@ -79,11 +99,32 @@
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1398| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2247| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2474| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9104| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10312| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|11956| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|58| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * 处理的函数kvm_gen_update_masterclock().
+ */
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
 #define KVM_REQ_MCLOCK_INPROGRESS \
 	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2239| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4575| <<kvm_arch_vcpu_load>> if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
+ *                                      kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9992| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * 处理的函数kvm_gen_kvmclock_update().
+ */
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
 #define KVM_REQ_APIC_PAGE_RELOAD \
 	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
@@ -747,9 +788,29 @@ struct kvm_vcpu_arch {
 
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;
+	/*
+	 * 在以下使用kvm_vcpu_arch->hw_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3253| <<kvm_guest_time_update>> if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3257| <<kvm_guest_time_update>> vcpu->hw_tsc_khz = tgt_tsc_khz;
+	 */
 	unsigned int hw_tsc_khz;
 	struct gfn_to_hva_cache pv_time;
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2242| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2249| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3088| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3286| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5108| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|6133| <<kvm_arch_suspend_notifier>> if (!vcpu->arch.pv_time_enabled)
+	 */
 	bool pv_time_enabled;
+	/*
+	 * 在以下使用kvm_vcou_arch->pvclock_set_guest_stopped_request:
+	 *   - arch/x86/kvm/x86.c|3033| <<kvm_setup_pvclock_page>> if (vcpu->pvclock_set_guest_stopped_request) {
+	 *   - arch/x86/kvm/x86.c|3035| <<kvm_setup_pvclock_page>> vcpu->pvclock_set_guest_stopped_request = false;
+	 *   - arch/x86/kvm/x86.c|5323| <<kvm_set_guest_paused>> vcpu->arch.pvclock_set_guest_stopped_request = true;
+	 */
 	/* set guest stopped flag in pvclock flags field */
 	bool pvclock_set_guest_stopped_request;
 
@@ -760,6 +821,20 @@ struct kvm_vcpu_arch {
 		struct gfn_to_hva_cache cache;
 	} st;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|619| <<nested_vmcb02_prepare_control>> vcpu->arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|972| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/svm/svm.c|1260| <<svm_write_tsc_offset>> svm->vmcb01.ptr->control.tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2556| <<prepare_vmcs02>> vcpu->arch.l1_tsc_offset,
+	 *   - arch/x86/kvm/vmx/nested.c|4602| <<nested_vmx_vmexit>> vcpu->arch.tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2550| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset +
+	 *   - arch/x86/kvm/x86.c|2583| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset,
+	 *   - arch/x86/kvm/x86.c|2586| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = l1_offset;
+	 *   - arch/x86/kvm/x86.c|2746| <<adjust_tsc_offset_guest>> u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3893| <<kvm_set_msr_common(MSR_IA32_TSC)>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|4222| <<kvm_get_msr_common(MSR_IA32_TSC)>> offset = vcpu->arch.l1_tsc_offset;
+	 */
 	u64 l1_tsc_offset;
 	u64 tsc_offset; /* current tsc offset */
 	u64 last_guest_tsc;
@@ -772,6 +847,24 @@ struct kvm_vcpu_arch {
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;
 	u32 virtual_tsc_mult;
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2446| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/hyperv.c|1684| <<kvm_hv_get_msr>> data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
+	 *   - arch/x86/kvm/lapic.c|1650| <<__wait_lapic_expire>> do_div(delay_ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1670| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1675| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1777| <<start_sw_tscdeadline>> unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/vmx/nested.c|2111| <<vmx_start_preemption_timer>> if (vcpu->arch.virtual_tsc_khz == 0)
+	 *   - arch/x86/kvm/vmx/nested.c|2116| <<vmx_start_preemption_timer>> do_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/vmx/nested.c|4043| <<vmx_get_preemption_timer_value>> value = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2672| <<kvm_synchronize_tsc>> if (vcpu->arch.virtual_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2683| <<kvm_synchronize_tsc>> u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
+	 *   - arch/x86/kvm/x86.c|2701| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2734| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|5910| <<kvm_arch_vcpu_ioctl(KVM_GET_TSC_KHZ)>> r = vcpu->arch.virtual_tsc_khz;
+	 */
 	u32 virtual_tsc_khz;
 	s64 ia32_tsc_adjust_msr;
 	u64 msr_ia32_power_ctl;
@@ -1185,11 +1278,34 @@ struct kvm_arch {
 	u64 cur_tsc_generation;
 	int nr_vcpus_matched_tsc;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_gtod_sync_lock:
+	 *   - kvm_synchronize_tsc()
+	 *   - kvm_gen_update_masterclock()
+	 *   - get_kvmclock_ns()
+	 *   - kvm_arch_vm_ioctl(KVM_SET_CLOCK)
+	 *   - kvm_hyperv_tsc_notifier()
+	 *   - kvm_arch_init_vm()
+	 */
 	raw_spinlock_t pvclock_gtod_sync_lock;
 	bool use_master_clock;
 	u64 master_kernel_ns;
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3136| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3152| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11789| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11838| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3233| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|11489| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|11913| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|11960| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	struct delayed_work kvmclock_sync_work;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
diff --git a/arch/x86/include/asm/pvclock.h b/arch/x86/include/asm/pvclock.h
index 19b695ff2c68..4bae4d7eab32 100644
--- a/arch/x86/include/asm/pvclock.h
+++ b/arch/x86/include/asm/pvclock.h
@@ -17,6 +17,13 @@ void pvclock_resume(void);
 
 void pvclock_touch_watchdogs(void);
 
+/*
+ * 在以下使用pvclock_read_begin():
+ *   - arch/x86/include/asm/vdso/gettimeofday.h|226| <<vread_pvclock>> version = pvclock_read_begin(pvti);
+ *   - arch/x86/kernel/pvclock.c|60| <<pvclock_read_flags>> version = pvclock_read_begin(src);
+ *   - arch/x86/kernel/pvclock.c|75| <<pvclock_clocksource_read>> version = pvclock_read_begin(src);
+ *   - drivers/ptp/ptp_kvm_x86.c|112| <<kvm_arch_ptp_get_crosststamp>> version = pvclock_read_begin(src);
+ */
 static __always_inline
 unsigned pvclock_read_begin(const struct pvclock_vcpu_time_info *src)
 {
@@ -26,6 +33,13 @@ unsigned pvclock_read_begin(const struct pvclock_vcpu_time_info *src)
 	return version;
 }
 
+/*
+ * 在以下使用pvclock_read_retry():
+ *   - arch/x86/include/asm/vdso/gettimeofday.h|232| <<vread_pvclock>> } while (pvclock_read_retry(pvti, version));
+ *   - arch/x86/kernel/pvclock.c|62| <<pvclock_read_flags>> } while (pvclock_read_retry(src, version));
+ *   - arch/x86/kernel/pvclock.c|78| <<pvclock_clocksource_read>> } while (pvclock_read_retry(src, version));
+ *   - drivers/ptp/ptp_kvm_x86.c|124| <<kvm_arch_ptp_get_crosststamp>> } while (pvclock_read_retry(src, version));
+ */
 static __always_inline
 bool pvclock_read_retry(const struct pvclock_vcpu_time_info *src,
 			unsigned version)
@@ -39,6 +53,15 @@ bool pvclock_read_retry(const struct pvclock_vcpu_time_info *src,
  * Scale a 64-bit delta by scaling and multiplying by a 32-bit fraction,
  * yielding a 64-bit result.
  */
+/*
+ * 在以下使用pvclock_scale_delta():
+ *   - arch/x86/include/asm/pvclock.h|85| <<__pvclock_read_cycles>> u64 offset = pvclock_scale_delta(delta,
+ *          src->tsc_to_system_mul, src->tsc_shift);
+ *   - arch/x86/kvm/x86.c|2375| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+ *          vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+ *   - arch/x86/kvm/x86.h|354| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+ *          vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+ */
 static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 {
 	u64 product;
@@ -78,10 +101,26 @@ static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 	return product;
 }
 
+/*
+ * 在以下使用__pvclock_read_cycles():
+ *   - arch/x86/include/asm/vdso/gettimeofday.h|231| <<vread_pvclock>> ret = __pvclock_read_cycles(pvti, rdtsc_ordered());
+ *   - arch/x86/kernel/pvclock.c|76| <<pvclock_clocksource_read>> ret = __pvclock_read_cycles(src, rdtsc_ordered());
+ *   - arch/x86/kvm/x86.c|2904| <<get_kvmclock_ns>> ret = __pvclock_read_cycles(&hv_clock, rdtsc());
+ *   - drivers/ptp/ptp_kvm_x86.c|123| <<kvm_arch_ptp_get_crosststamp>> *cycle = __pvclock_read_cycles(src, clock_pair->tsc);
+ */
 static __always_inline
 u64 __pvclock_read_cycles(const struct pvclock_vcpu_time_info *src, u64 tsc)
 {
 	u64 delta = tsc - src->tsc_timestamp;
+	/*
+	 * 在以下使用pvclock_scale_delta():
+	 *   - arch/x86/include/asm/pvclock.h|85| <<__pvclock_read_cycles>> u64 offset = pvclock_scale_delta(delta,
+	 *          src->tsc_to_system_mul, src->tsc_shift);
+	 *   - arch/x86/kvm/x86.c|2375| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *          vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|354| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+	 *          vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 */
 	u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
 					     src->tsc_shift);
 	return src->system_time + offset;
diff --git a/arch/x86/include/asm/spec-ctrl.h b/arch/x86/include/asm/spec-ctrl.h
index 5393babc0598..6db18f4d4dce 100644
--- a/arch/x86/include/asm/spec-ctrl.h
+++ b/arch/x86/include/asm/spec-ctrl.h
@@ -23,6 +23,10 @@ extern void x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bo
  *
  * Avoids writing to the MSR if the content/bits are the same
  */
+/*
+ * 在以下使用x86_spec_ctrl_set_guest():
+ *   - arch/x86/kvm/svm/svm.c|4170| <<svm_vcpu_run>> x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
+ */
 static inline
 void x86_spec_ctrl_set_guest(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
 {
diff --git a/arch/x86/include/asm/vdso/gettimeofday.h b/arch/x86/include/asm/vdso/gettimeofday.h
index 1936f21ed8cd..f903e6877340 100644
--- a/arch/x86/include/asm/vdso/gettimeofday.h
+++ b/arch/x86/include/asm/vdso/gettimeofday.h
@@ -194,6 +194,10 @@ long clock_getres32_fallback(clockid_t _clkid, struct old_timespec32 *_ts)
 #endif
 
 #ifdef CONFIG_PARAVIRT_CLOCK
+/*
+ * 在以下使用vread_pvclock():
+ *   - arch/x86/include/asm/vdso/gettimeofday.h|259| <<__arch_get_hw_counter>> return vread_pvclock();
+ */
 static u64 vread_pvclock(void)
 {
 	const struct pvclock_vcpu_time_info *pvti = &pvclock_page.pvti;
@@ -223,6 +227,13 @@ static u64 vread_pvclock(void)
 	 */
 
 	do {
+		/*
+		 * 在以下使用pvclock_read_begin():
+		 *   - arch/x86/include/asm/vdso/gettimeofday.h|226| <<vread_pvclock>> version = pvclock_read_begin(pvti);
+		 *   - arch/x86/kernel/pvclock.c|60| <<pvclock_read_flags>> version = pvclock_read_begin(src);
+		 *   - arch/x86/kernel/pvclock.c|75| <<pvclock_clocksource_read>> version = pvclock_read_begin(src);
+		 *   - drivers/ptp/ptp_kvm_x86.c|112| <<kvm_arch_ptp_get_crosststamp>> version = pvclock_read_begin(src);
+		 */
 		version = pvclock_read_begin(pvti);
 
 		if (unlikely(!(pvti->flags & PVCLOCK_TSC_STABLE_BIT)))
@@ -242,6 +253,11 @@ static u64 vread_hvclock(void)
 }
 #endif
 
+/*
+ * 在以下使用__arch_get_hw_counter():
+ *   - lib/vdso/gettimeofday.c|73| <<do_hres_timens>> cycles = __arch_get_hw_counter(vd->clock_mode, vd);
+ *   - lib/vdso/gettimeofday.c|144| <<do_hres>> cycles = __arch_get_hw_counter(vd->clock_mode, vd);
+ */
 static inline u64 __arch_get_hw_counter(s32 clock_mode,
 					const struct vdso_data *vd)
 {
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 8e1cd291aed3..94c008e698ba 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -46,6 +46,15 @@ early_param("no-kvmclock-vsyscall", parse_no_kvmclock_vsyscall);
 #define HVC_BOOT_ARRAY_SIZE \
 	(PAGE_SIZE / sizeof(struct pvclock_vsyscall_time_info))
 
+/*
+ * 在以下使用hv_clock_boot[HVC_BOOT_ARRAY_SIZE]:
+ *   - arch/x86/kernel/kvmclock.c|50| <<global>> hv_clock_boot[HVC_BOOT_ARRAY_SIZE] __bss_decrypted __aligned(PAGE_SIZE);
+ *   - arch/x86/kernel/kvmclock.c|267| <<kvm_setup_vsyscall_timeinfo>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+ *   - arch/x86/kernel/kvmclock.c|293| <<kvmclock_setup_percpu>> p = &hv_clock_boot[cpu];
+ *   - arch/x86/kernel/kvmclock.c|328| <<kvmclock_init>> this_cpu_write(hv_clock_per_cpu, &hv_clock_boot[0]);
+ *   - arch/x86/kernel/kvmclock.c|330| <<kvmclock_init>> pvclock_set_pvti_cpu0_va(hv_clock_boot);
+ *   - arch/x86/kernel/kvmclock.c|335| <<kvmclock_init>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+ */
 static struct pvclock_vsyscall_time_info
 			hv_clock_boot[HVC_BOOT_ARRAY_SIZE] __bss_decrypted __aligned(PAGE_SIZE);
 static struct pvclock_wall_clock wall_clock __bss_decrypted;
@@ -76,6 +85,12 @@ static u64 kvm_clock_read(void)
 	u64 ret;
 
 	preempt_disable_notrace();
+	/*
+	 * 在以下使用pvclock_clocksource_read():
+	 *   - arch/x86/kernel/kvmclock.c|79| <<kvm_clock_read>> ret = pvclock_clocksource_read(this_cpu_pvti());
+	 *   - arch/x86/kernel/pvclock.c|161| <<pvclock_read_wallclock>> delta = pvclock_clocksource_read(vcpu_time);
+	 *   - arch/x86/xen/time.c|53| <<xen_clocksource_read>> ret = pvclock_clocksource_read(src);
+	 */
 	ret = pvclock_clocksource_read(this_cpu_pvti());
 	preempt_enable_notrace();
 	return ret;
@@ -132,6 +147,13 @@ static void __init kvm_get_preset_lpj(void)
 	preset_lpj = lpj;
 }
 
+/*
+ * 在以下使用kvm_check_and_clear_guest_paused():
+ *   - kernel/rcu/tree_stall.h|723| <<check_cpu_stall>> if (kvm_check_and_clear_guest_paused())
+ *   - kernel/rcu/tree_stall.h|741| <<check_cpu_stall>> if (kvm_check_and_clear_guest_paused())
+ *   - kernel/watchdog.c|395| <<watchdog_timer_fn>> kvm_check_and_clear_guest_paused();
+ *   - kernel/workqueue.c|5903| <<wq_watchdog_timer_fn>> kvm_check_and_clear_guest_paused();
+ */
 bool kvm_check_and_clear_guest_paused(void)
 {
 	struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
@@ -142,6 +164,11 @@ bool kvm_check_and_clear_guest_paused(void)
 
 	if ((src->pvti.flags & PVCLOCK_GUEST_STOPPED) != 0) {
 		src->pvti.flags &= ~PVCLOCK_GUEST_STOPPED;
+		/*
+		 * 在以下使用pvclock_touch_watchdogs():
+		 *   - arch/x86/kernel/kvmclock.c|145| <<kvm_check_and_clear_guest_paused>> pvclock_touch_watchdogs();
+		 *   - arch/x86/kernel/pvclock.c|96| <<pvclock_clocksource_read>> pvclock_touch_watchdogs();
+		 */
 		pvclock_touch_watchdogs();
 		ret = true;
 	}
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index eda37df016f0..c58f7968c1f5 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -17,6 +17,11 @@
 #include <asm/vgtod.h>
 
 static u8 valid_flags __read_mostly = 0;
+/*
+ * 在以下使用pvti_cpu0_va:
+ *   - arch/x86/kernel/pvclock.c|192| <<pvclock_set_pvti_cpu0_va>> pvti_cpu0_va = pvti;
+ *   - arch/x86/kernel/pvclock.c|197| <<pvclock_get_pvti_cpu0_va>> return pvti_cpu0_va;
+ */
 static struct pvclock_vsyscall_time_info *pvti_cpu0_va __read_mostly;
 
 void pvclock_set_flags(u8 flags)
@@ -36,6 +41,11 @@ unsigned long pvclock_tsc_khz(struct pvclock_vcpu_time_info *src)
 	return pv_tsc_khz;
 }
 
+/*
+ * 在以下使用pvclock_touch_watchdogs():
+ *   - arch/x86/kernel/kvmclock.c|145| <<kvm_check_and_clear_guest_paused>> pvclock_touch_watchdogs();
+ *   - arch/x86/kernel/pvclock.c|96| <<pvclock_clocksource_read>> pvclock_touch_watchdogs();
+ */
 void pvclock_touch_watchdogs(void)
 {
 	touch_softlockup_watchdog_sync();
@@ -44,6 +54,13 @@ void pvclock_touch_watchdogs(void)
 	reset_hung_task_detector();
 }
 
+/*
+ * 在以下使用last_value:
+ *   - arch/x86/kernel/pvclock.c|52| <<global>> static atomic64_t last_value = ATOMIC64_INIT(0);
+ *   - arch/x86/kernel/pvclock.c|56| <<pvclock_resume>> atomic64_set(&last_value, 0);
+ *   - arch/x86/kernel/pvclock.c|133| <<pvclock_clocksource_read>> last = atomic64_read(&last_value);
+ *   - arch/x86/kernel/pvclock.c|137| <<pvclock_clocksource_read>> last = atomic64_cmpxchg(&last_value, last, ret);
+ */
 static atomic64_t last_value = ATOMIC64_INIT(0);
 
 void pvclock_resume(void)
@@ -57,6 +74,13 @@ u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src)
 	u8 flags;
 
 	do {
+		/*
+		 * 在以下使用pvclock_read_begin():
+		 *   - arch/x86/include/asm/vdso/gettimeofday.h|226| <<vread_pvclock>> version = pvclock_read_begin(pvti);
+		 *   - arch/x86/kernel/pvclock.c|60| <<pvclock_read_flags>> version = pvclock_read_begin(src);
+		 *   - arch/x86/kernel/pvclock.c|75| <<pvclock_clocksource_read>> version = pvclock_read_begin(src);
+		 *   - drivers/ptp/ptp_kvm_x86.c|112| <<kvm_arch_ptp_get_crosststamp>> version = pvclock_read_begin(src);
+		 */
 		version = pvclock_read_begin(src);
 		flags = src->flags;
 	} while (pvclock_read_retry(src, version));
@@ -64,6 +88,19 @@ u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src)
 	return flags & valid_flags;
 }
 
+/*
+ * [0] pvclock_clocksource_read
+ * [0] kvm_clock_get_cycles
+ * [0] ktime_get
+ * [0] tick_irq_enter
+ * [0] irq_enter_rcu
+ * [0] sysvec_apic_timer_interrupt
+ *
+ * 在以下使用pvclock_clocksource_read():
+ *   - arch/x86/kernel/kvmclock.c|79| <<kvm_clock_read>> ret = pvclock_clocksource_read(this_cpu_pvti());
+ *   - arch/x86/kernel/pvclock.c|161| <<pvclock_read_wallclock>> delta = pvclock_clocksource_read(vcpu_time);
+ *   - arch/x86/xen/time.c|53| <<xen_clocksource_read>> ret = pvclock_clocksource_read(src);
+ */
 u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 {
 	unsigned version;
@@ -72,6 +109,13 @@ u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 	u8 flags;
 
 	do {
+		/*
+		 * 在以下使用pvclock_read_begin():
+		 *   - arch/x86/include/asm/vdso/gettimeofday.h|226| <<vread_pvclock>> version = pvclock_read_begin(pvti);
+		 *   - arch/x86/kernel/pvclock.c|60| <<pvclock_read_flags>> version = pvclock_read_begin(src);
+		 *   - arch/x86/kernel/pvclock.c|75| <<pvclock_clocksource_read>> version = pvclock_read_begin(src);
+		 *   - drivers/ptp/ptp_kvm_x86.c|112| <<kvm_arch_ptp_get_crosststamp>> version = pvclock_read_begin(src);
+		 */
 		version = pvclock_read_begin(src);
 		ret = __pvclock_read_cycles(src, rdtsc_ordered());
 		flags = src->flags;
@@ -79,6 +123,11 @@ u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 
 	if (unlikely((flags & PVCLOCK_GUEST_STOPPED) != 0)) {
 		src->flags &= ~PVCLOCK_GUEST_STOPPED;
+		/*
+		 * 在以下使用pvclock_touch_watchdogs():
+		 *   - arch/x86/kernel/kvmclock.c|145| <<kvm_check_and_clear_guest_paused>> pvclock_touch_watchdogs();
+		 *   - arch/x86/kernel/pvclock.c|96| <<pvclock_clocksource_read>> pvclock_touch_watchdogs();
+		 */
 		pvclock_touch_watchdogs();
 	}
 
@@ -134,6 +183,12 @@ void pvclock_read_wallclock(struct pvclock_wall_clock *wall_clock,
 		rmb();		/* fetch time before checking version */
 	} while ((wall_clock->version & 1) || (version != wall_clock->version));
 
+	/*
+	 * 在以下使用pvclock_clocksource_read():
+	 *   - arch/x86/kernel/kvmclock.c|79| <<kvm_clock_read>> ret = pvclock_clocksource_read(this_cpu_pvti());
+	 *   - arch/x86/kernel/pvclock.c|161| <<pvclock_read_wallclock>> delta = pvclock_clocksource_read(vcpu_time);
+	 *   - arch/x86/xen/time.c|53| <<xen_clocksource_read>> ret = pvclock_clocksource_read(src);
+	 */
 	delta = pvclock_clocksource_read(vcpu_time);	/* time since system boot */
 	delta += now.tv_sec * NSEC_PER_SEC + now.tv_nsec;
 
@@ -143,14 +198,34 @@ void pvclock_read_wallclock(struct pvclock_wall_clock *wall_clock,
 	set_normalized_timespec64(ts, now.tv_sec, now.tv_nsec);
 }
 
+/*
+ * 在以下使用pvclock_set_pvti_cpu0_va():
+ *   - arch/x86/kernel/kvmclock.c|339| <<kvmclock_init>> pvclock_set_pvti_cpu0_va(hv_clock_boot);
+ *   - arch/x86/xen/time.c|478| <<xen_setup_vsyscall_time_info>> pvclock_set_pvti_cpu0_va(xen_clock);
+ */
 void pvclock_set_pvti_cpu0_va(struct pvclock_vsyscall_time_info *pvti)
 {
 	WARN_ON(vclock_was_used(VDSO_CLOCKMODE_PVCLOCK));
+	/*
+	 * 在以下使用pvti_cpu0_va:
+	 *   - arch/x86/kernel/pvclock.c|192| <<pvclock_set_pvti_cpu0_va>> pvti_cpu0_va = pvti;
+	 *   - arch/x86/kernel/pvclock.c|197| <<pvclock_get_pvti_cpu0_va>> return pvti_cpu0_va;
+	 */
 	pvti_cpu0_va = pvti;
 }
 
+/*
+ * 在以下使用pvclock_get_pvti_cpu0_va():
+ *   - arch/x86/entry/vdso/vma.c|206| <<vvar_fault>> pvclock_get_pvti_cpu0_va();
+ *   - drivers/ptp/ptp_kvm_x86.c|48| <<kvm_arch_ptp_init>> if (!pvclock_get_pvti_cpu0_va()) {
+ */
 struct pvclock_vsyscall_time_info *pvclock_get_pvti_cpu0_va(void)
 {
+	/*
+	 * 在以下使用pvti_cpu0_va:
+	 *   - arch/x86/kernel/pvclock.c|192| <<pvclock_set_pvti_cpu0_va>> pvti_cpu0_va = pvti;
+	 *   - arch/x86/kernel/pvclock.c|197| <<pvclock_get_pvti_cpu0_va>> return pvti_cpu0_va;
+	 */
 	return pvti_cpu0_va;
 }
 EXPORT_SYMBOL_GPL(pvclock_get_pvti_cpu0_va);
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index d849b67a3f13..f4e406e0e83e 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -562,6 +562,15 @@ static u64 get_time_ref_counter(struct kvm *kvm)
 	 * Fall back to get_kvmclock_ns() when TSC page hasn't been set up,
 	 * is broken, disabled or being updated.
 	 */
+	/*
+	 * 在以下使用get_kvmclock_ns():
+	 *   - arch/x86/kvm/hyperv.c|566| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+	 *   - arch/x86/kvm/x86.c|2208| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+	 *   - arch/x86/kvm/x86.c|6554| <<kvm_arch_vm_ioctl(KVM_GET_CLOCK)>> now_ns = get_kvmclock_ns(kvm);
+	 *   - arch/x86/kvm/xen.c|68| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+	 *   - arch/x86/kvm/xen.c|455| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+	 *   - arch/x86/kvm/xen.c|496| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+	 */
 	if (hv->hv_tsc_page_status != HV_TSC_PAGE_SET)
 		return div_u64(get_kvmclock_ns(kvm), 100);
 
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 3e25192d41d0..f1d55d1a2833 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -724,6 +724,15 @@ int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
 static int pv_eoi_put_user(struct kvm_vcpu *vcpu, u8 val)
 {
 
+	/*
+	 * 在以下使用kvm_write_guest_cached():
+	 *   - arch/x86/kvm/lapic.c|727| <<pv_eoi_put_user>> return kvm_write_guest_cached(vcpu->kvm,
+	 *          &vcpu->arch.pv_eoi.data, &val, sizeof(val));
+	 *   - arch/x86/kvm/lapic.c|2922| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm,
+	 *          &vcpu->arch.apic->vapic_cache, &data, sizeof(u32));
+	 *   - arch/x86/kvm/x86.c|12454| <<apf_put_user_notpresent>> return kvm_write_guest_cached(vcpu->kvm,
+	 *          &vcpu->arch.apf.data, &reason, sizeof(reason));
+	 */
 	return kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.pv_eoi.data, &val,
 				      sizeof(val));
 }
@@ -1765,6 +1774,24 @@ static void start_sw_tscdeadline(struct kvm_lapic *apic)
 	u64 ns = 0;
 	ktime_t expire;
 	struct kvm_vcpu *vcpu = apic->vcpu;
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2446| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/hyperv.c|1684| <<kvm_hv_get_msr>> data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
+	 *   - arch/x86/kvm/lapic.c|1650| <<__wait_lapic_expire>> do_div(delay_ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1670| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1675| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1777| <<start_sw_tscdeadline>> unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/vmx/nested.c|2111| <<vmx_start_preemption_timer>> if (vcpu->arch.virtual_tsc_khz == 0)
+	 *   - arch/x86/kvm/vmx/nested.c|2116| <<vmx_start_preemption_timer>> do_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/vmx/nested.c|4043| <<vmx_get_preemption_timer_value>> value = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2672| <<kvm_synchronize_tsc>> if (vcpu->arch.virtual_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2683| <<kvm_synchronize_tsc>> u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
+	 *   - arch/x86/kvm/x86.c|2701| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2734| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|5910| <<kvm_arch_vcpu_ioctl(KVM_GET_TSC_KHZ)>> r = vcpu->arch.virtual_tsc_khz;
+	 */
 	unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
 	unsigned long flags;
 	ktime_t now;
@@ -2919,6 +2946,15 @@ void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)
 		max_isr = 0;
 	data = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);
 
+	/*
+	 * 在以下使用kvm_write_guest_cached():
+	 *   - arch/x86/kvm/lapic.c|727| <<pv_eoi_put_user>> return kvm_write_guest_cached(vcpu->kvm,
+	 *          &vcpu->arch.pv_eoi.data, &val, sizeof(val));
+	 *   - arch/x86/kvm/lapic.c|2922| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm,
+	 *          &vcpu->arch.apic->vapic_cache, &data, sizeof(u32));
+	 *   - arch/x86/kvm/x86.c|12454| <<apf_put_user_notpresent>> return kvm_write_guest_cached(vcpu->kvm,
+	 *          &vcpu->arch.apf.data, &reason, sizeof(reason));
+	 */
 	kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
 				sizeof(u32));
 }
@@ -2926,6 +2962,27 @@ void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)
 int kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)
 {
 	if (vapic_addr) {
+		/*
+		 * 在以下使用kvm_gfn_to_hva_cache_init():
+		 *   - arch/x86/kvm/lapic.c|2947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm
+		 *            &vcpu->arch.apic->vapic_cache, vapic_addr, sizeof(u32)))
+		 *   - arch/x86/kvm/lapic.c|3063| <<kvm_lapic_enable_pv_eoi>> return kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            ghc, addr, new_len);
+		 *   - arch/x86/kvm/x86.c|2278| <<kvm_write_system_time>> if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            &vcpu->arch.pv_time, system_time & ~1ULL, sizeof(struct pvclock_vcpu_time_info)))
+		 *   - arch/x86/kvm/x86.c|3442| <<kvm_pv_enable_async_pf>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            &vcpu->arch.apf.data, gpa, sizeof(u64)))
+		 *   - arch/x86/kvm/x86.c|3561| <<record_steal_time>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            ghc, gpa, sizeof(*st)) ||
+		 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_update_runstate_guest>> kvm_gfn_to_hva_cache_init(v->kvm,
+		 *            ghc, ghc->gpa, ghc->len))
+		 *   - arch/x86/kvm/xen.c|365| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            &vcpu->arch.xen.vcpu_info_cache, data->u.gpa, sizeof(struct vcpu_info));
+		 *   - arch/x86/kvm/xen.c|388| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            &vcpu->arch.xen.vcpu_time_info_cache, data->u.gpa, sizeof(struct pvclock_vcpu_time_info));
+		 *   - arch/x86/kvm/xen.c|415| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            &vcpu->arch.xen.runstate_cache, data->u.gpa, sizeof(struct vcpu_runstate_info));
+		 */
 		if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
 					&vcpu->arch.apic->vapic_cache,
 					vapic_addr, sizeof(u32)))
@@ -3042,6 +3099,27 @@ int kvm_lapic_enable_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 	else
 		new_len = len;
 
+	/*
+	 * 在以下使用kvm_gfn_to_hva_cache_init():
+	 *   - arch/x86/kvm/lapic.c|2947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.apic->vapic_cache, vapic_addr, sizeof(u32)))
+	 *   - arch/x86/kvm/lapic.c|3063| <<kvm_lapic_enable_pv_eoi>> return kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            ghc, addr, new_len);
+	 *   - arch/x86/kvm/x86.c|2278| <<kvm_write_system_time>> if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.pv_time, system_time & ~1ULL, sizeof(struct pvclock_vcpu_time_info)))
+	 *   - arch/x86/kvm/x86.c|3442| <<kvm_pv_enable_async_pf>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.apf.data, gpa, sizeof(u64)))
+	 *   - arch/x86/kvm/x86.c|3561| <<record_steal_time>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            ghc, gpa, sizeof(*st)) ||
+	 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_update_runstate_guest>> kvm_gfn_to_hva_cache_init(v->kvm,
+	 *            ghc, ghc->gpa, ghc->len))
+	 *   - arch/x86/kvm/xen.c|365| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.xen.vcpu_info_cache, data->u.gpa, sizeof(struct vcpu_info));
+	 *   - arch/x86/kvm/xen.c|388| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.xen.vcpu_time_info_cache, data->u.gpa, sizeof(struct pvclock_vcpu_time_info));
+	 *   - arch/x86/kvm/xen.c|415| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.xen.runstate_cache, data->u.gpa, sizeof(struct vcpu_runstate_info));
+	 */
 	return kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, addr, new_len);
 }
 
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index a3563afb4e56..109e9aab9360 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -895,6 +895,15 @@ gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu, gfn_t gfn,
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
 	if (!slot || slot->flags & KVM_MEMSLOT_INVALID)
 		return NULL;
+	/*
+	 * 在以下使用kvm_dirty_log_pgtable():
+	 *   - arch/x86/kvm/mmu/mmu.c|899| <<gfn_to_memslot_dirty_bitmap>> !kvm_dirty_log_pgtable(vcpu->kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1388| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> !kvm_dirty_log_pgtable(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1401| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> if (kvm_dirty_log_pgtable(kvm) &&
+	 *   - arch/x86/kvm/x86.c|6114| <<kvm_arch_sync_dirty_log>> if (kvm_dirty_log_pgtable(kvm)) {
+	 *   - arch/x86/kvm/x86.c|12631| <<kvm_mmu_slot_apply_flags>> if (!kvm_dirty_log_pgtable(kvm))
+	 *   - virt/kvm/kvm_main.c|2235| <<kvm_clear_dirty_log_protect>> if (!kvm_dirty_log_pgtable(kvm))
+	 */
 	if (no_dirty_log && kvm_slot_dirty_track_enabled(slot) &&
             !kvm_dirty_log_pgtable(vcpu->kvm))
 		return NULL;
@@ -1384,6 +1393,15 @@ void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 	 * of memslot has no such restriction, so the range can cross two large
 	 * pages.
 	 */
+	/*
+	 * 在以下使用kvm_dirty_log_pgtable():
+	 *   - arch/x86/kvm/mmu/mmu.c|899| <<gfn_to_memslot_dirty_bitmap>> !kvm_dirty_log_pgtable(vcpu->kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1388| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> !kvm_dirty_log_pgtable(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1401| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> if (kvm_dirty_log_pgtable(kvm) &&
+	 *   - arch/x86/kvm/x86.c|6114| <<kvm_arch_sync_dirty_log>> if (kvm_dirty_log_pgtable(kvm)) {
+	 *   - arch/x86/kvm/x86.c|12631| <<kvm_mmu_slot_apply_flags>> if (!kvm_dirty_log_pgtable(kvm))
+	 *   - virt/kvm/kvm_main.c|2235| <<kvm_clear_dirty_log_protect>> if (!kvm_dirty_log_pgtable(kvm))
+	 */
 	if (kvm_dirty_log_manual_protect_and_init_set(kvm) &&
 	    !kvm_dirty_log_pgtable(kvm)) {
 		gfn_t start = slot->base_gfn + gfn_offset + __ffs(mask);
@@ -1398,6 +1416,15 @@ void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 						       PG_LEVEL_2M);
 	}
 
+	/*
+	 * 在以下使用kvm_dirty_log_pgtable():
+	 *   - arch/x86/kvm/mmu/mmu.c|899| <<gfn_to_memslot_dirty_bitmap>> !kvm_dirty_log_pgtable(vcpu->kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1388| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> !kvm_dirty_log_pgtable(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1401| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> if (kvm_dirty_log_pgtable(kvm) &&
+	 *   - arch/x86/kvm/x86.c|6114| <<kvm_arch_sync_dirty_log>> if (kvm_dirty_log_pgtable(kvm)) {
+	 *   - arch/x86/kvm/x86.c|12631| <<kvm_mmu_slot_apply_flags>> if (!kvm_dirty_log_pgtable(kvm))
+	 *   - virt/kvm/kvm_main.c|2235| <<kvm_clear_dirty_log_protect>> if (!kvm_dirty_log_pgtable(kvm))
+	 */
 	if (kvm_dirty_log_pgtable(kvm) &&
 	    !is_tdp_mmu_enabled(kvm)) {
 		return;
@@ -1657,6 +1684,19 @@ static void rmap_add(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 				vcpu->kvm, sp->gfn, KVM_PAGES_PER_HPAGE(sp->role.level));
 	}
 
+	/*
+	 * 在以下使用kvm_memory_slot->present_bitmap:
+	 *   - arch/x86/kvm/mmu/mmu.c|1661| <<rmap_add>> if (slot && slot->present_bitmap) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1662| <<rmap_add>> bitmap_set(slot->present_bitmap, gfn - slot->base_gfn,
+	 *   - arch/x86/kvm/mmu/mmu.c|5880| <<kvm_rmap_walk_present>> atomic_long_t *p = (atomic_long_t *) &slot->present_bitmap[i];
+	 *   - virt/kvm/kvm_main.c|928| <<kvm_destroy_present_bitmap>> if (!memslot->present_bitmap)
+	 *   - virt/kvm/kvm_main.c|931| <<kvm_destroy_present_bitmap>> kvfree(memslot->present_bitmap);
+	 *   - virt/kvm/kvm_main.c|932| <<kvm_destroy_present_bitmap>> memslot->present_bitmap = NULL;
+	 *   - virt/kvm/kvm_main.c|1349| <<kvm_alloc_present_bitmap>> memslot->present_bitmap = __vcalloc(2, present_bytes, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|1350| <<kvm_alloc_present_bitmap>> if (!memslot->present_bitmap)
+	 *   - virt/kvm/kvm_main.c|1979| <<__kvm_set_memory_region>> new->present_bitmap = old->present_bitmap;
+	 *   - virt/kvm/kvm_main.c|1980| <<__kvm_set_memory_region>> if (!new->present_bitmap) {
+	 */
 	/* Page is present in RMAP */
 	if (slot && slot->present_bitmap) {
 		bitmap_set(slot->present_bitmap, gfn - slot->base_gfn,
@@ -3111,6 +3151,11 @@ fast_pf_fix_direct_spte(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	if (cmpxchg64(sptep, old_spte, new_spte) != old_spte)
 		return false;
 
+	/*
+	 * 在以下使用mark_page_range_dirty_in_slot():
+	 *   - arch/x86/kvm/mmu/mmu.c|3115| <<fast_pf_fix_direct_spte>> mark_page_range_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn,
+	 *   - arch/x86/kvm/mmu/mmu.c|5912| <<rmap_test_dirty>> mark_page_range_dirty_in_slot(kvm, slot, pfn,
+	 */
 	if (is_writable_pte(new_spte) && !is_writable_pte(old_spte))
 		mark_page_range_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn,
 				KVM_PAGES_PER_HPAGE(sp->role.level));
@@ -5253,6 +5298,37 @@ static void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 	write_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * kvm mmio处理:
+ *
+ * 开始的时候mmio的内存都没有kvm_memory_slot, pte也都不存在,所以访问的时候会ept violation.
+ * handle_ept_violation()会调用kvm_mmu_page_fault().因为handle_ept_violation()不会为
+ * error_code设置PFERR_RSVD_MASK,所以kvm_mmu_page_fault()调用tdp_page_fault().
+ *
+ * tdp_page_fault()-->try_async_pf()-->__gfn_to_pfn_memslot()-->__gfn_to_hva_many().
+ * __gfn_to_hva_many()返回KVM_HVA_ERR_BAD=PAGE_OFFSET,所以__gfn_to_pfn_memslot()返回
+ * KVM_PFN_NOSLOT.              
+ *
+ * 然后tdp_page_fault()-->__direct_map(),通过mmu_set_spte()-->set_mmio_spte()设置对应
+ * 的pte为mmio类型,也就是:      
+ *
+ * 1. SPTE_SPECIAL_MASK(1往左移62位)是1
+ * 2. 结尾是110
+ * 3. 还有gen的信息             
+ *
+ * 最后进行RET_PF_EMULATE       
+ *
+ * 等到下一次page fault的时候,mmio的内存会触发handle_ept_misconfig()-->kvm_mmu_page_fault().
+ * 因为这次handle_ept_misconfig()设置error_code=PFERR_RSVD_MASK,kvm_mmu_page_fault()会调
+ * 用handle_mmio_page_fault(),返回进行emulate.
+ *
+ *
+ * 在以下使用kvm_mmu_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|4094| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+ *   - arch/x86/kvm/svm/svm.c|2086| <<npf_interception>> return kvm_mmu_page_fault(vcpu, fault_address, error_code,
+ *   - arch/x86/kvm/vmx/vmx.c|5502| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5523| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -5308,6 +5384,12 @@ int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 	if (!mmio_info_in_cache(vcpu, cr2_or_gpa, direct) && !is_guest_mode(vcpu))
 		emulation_type |= EMULTYPE_ALLOW_RETRY_PF;
 emulate:
+	/*
+	 * 在以下使用x86_emulate_instruction():
+	 *   - arch/x86/kvm/mmu/mmu.c|5356| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+	 *   - arch/x86/kvm/x86.c|8902| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+	 *   - arch/x86/kvm/x86.c|8909| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+	 */
 	return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
 				       insn_len);
 }
@@ -5865,6 +5947,10 @@ static bool kvm_rmap_walk_masked(struct kvm *kvm,
 	return flush;
 }
 
+/*
+ * 在以下使用kvm_rmap_walk_present():
+ *   - arch/x86/kvm/mmu/mmu.c|5927| <<kvm_mmu_slot_test_dirty>> flush = kvm_rmap_walk_present(kvm, memslot,
+ */
 static bool kvm_rmap_walk_present(struct kvm *kvm,
 				  const struct kvm_memory_slot *slot,
 				  int start_level, unsigned long dirty_mask,
@@ -5877,6 +5963,19 @@ static bool kvm_rmap_walk_present(struct kvm *kvm,
 	for (offset = 0, i = offset / BITS_PER_LONG,
 		 n = DIV_ROUND_UP(slot->npages, BITS_PER_LONG); n--;
 	     i++, offset += BITS_PER_LONG) {
+		/*
+		 * 在以下使用kvm_memory_slot->present_bitmap:
+		 *   - arch/x86/kvm/mmu/mmu.c|1661| <<rmap_add>> if (slot && slot->present_bitmap) {
+		 *   - arch/x86/kvm/mmu/mmu.c|1662| <<rmap_add>> bitmap_set(slot->present_bitmap, gfn - slot->base_gfn,
+		 *   - arch/x86/kvm/mmu/mmu.c|5880| <<kvm_rmap_walk_present>> atomic_long_t *p = (atomic_long_t *) &slot->present_bitmap[i];
+		 *   - virt/kvm/kvm_main.c|928| <<kvm_destroy_present_bitmap>> if (!memslot->present_bitmap)
+		 *   - virt/kvm/kvm_main.c|931| <<kvm_destroy_present_bitmap>> kvfree(memslot->present_bitmap);
+		 *   - virt/kvm/kvm_main.c|932| <<kvm_destroy_present_bitmap>> memslot->present_bitmap = NULL;
+		 *   - virt/kvm/kvm_main.c|1349| <<kvm_alloc_present_bitmap>> memslot->present_bitmap = __vcalloc(2, present_bytes, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|1350| <<kvm_alloc_present_bitmap>> if (!memslot->present_bitmap)
+		 *   - virt/kvm/kvm_main.c|1979| <<__kvm_set_memory_region>> new->present_bitmap = old->present_bitmap;
+		 *   - virt/kvm/kvm_main.c|1980| <<__kvm_set_memory_region>> if (!new->present_bitmap) {
+		 */
 		atomic_long_t *p = (atomic_long_t *) &slot->present_bitmap[i];
 		unsigned long mask = atomic_long_read(p);
 		if (!mask)
@@ -5909,6 +6008,11 @@ static bool rmap_test_dirty(struct kvm *kvm,
 		flush = true;
 		sp = sptep_to_sp(sptep);
 		pfn = kvm_mmu_page_get_gfn(sp, spte_index(sptep));
+		/*
+		 * 在以下使用mark_page_range_dirty_in_slot():
+		 *   - arch/x86/kvm/mmu/mmu.c|3115| <<fast_pf_fix_direct_spte>> mark_page_range_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn,
+		 *   - arch/x86/kvm/mmu/mmu.c|5912| <<rmap_test_dirty>> mark_page_range_dirty_in_slot(kvm, slot, pfn,
+		 */
 		mark_page_range_dirty_in_slot(kvm, slot, pfn,
 				KVM_PAGES_PER_HPAGE(sp->role.level));
 	}
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index 5416639d7ba1..e7d549f90898 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -264,6 +264,19 @@ static void handle_changed_spte_dirty_log(struct kvm *kvm, int as_id, gfn_t gfn,
 	if ((!is_writable_pte(old_spte) || pfn_changed) &&
 	    is_writable_pte(new_spte)) {
 		slot = __gfn_to_memslot(__kvm_memslots(kvm, as_id), gfn);
+		/*
+		 * 在以下使用mark_page_dirty_in_slot():
+		 *   - arch/arm64/kvm/mmu.c|1179| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|267| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+		 *   - arch/x86/kvm/x86.c|3401| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+		 *   - arch/x86/kvm/x86.c|4596| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+		 *   - arch/x86/kvm/xen.c|204| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+		 *   - virt/kvm/kvm_main.c|2898| <<__kvm_unmap_gfn>> mark_page_dirty_in_slot(kvm, memslot, map->gfn);
+		 *   - virt/kvm/kvm_main.c|3113| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+		 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+		 *   - virt/kvm/kvm_main.c|3358| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+		 *   - virt/kvm/kvm_main.c|3367| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+		 */
 		mark_page_dirty_in_slot(kvm, slot, gfn);
 	}
 }
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 180fb3bd32b8..f34df3de701c 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -3559,6 +3559,10 @@ static void reload_tss(struct kvm_vcpu *vcpu)
 	load_TR_desc();
 }
 
+/*
+ * 在以下使用pre_svm_run():
+ *   - arch/x86/kvm/svm/svm.c|4139| <<svm_vcpu_run>> pre_svm_run(vcpu);
+ */
 static void pre_svm_run(struct kvm_vcpu *vcpu)
 {
 	struct svm_cpu_data *sd = per_cpu(svm_data, vcpu->cpu);
@@ -3986,6 +3990,11 @@ static void svm_complete_soft_interrupt(struct kvm_vcpu *vcpu, u8 vector,
 		kvm_rip_write(vcpu, svm->soft_int_old_rip);
 }
 
+/*
+ * 在以下使用svm_complete_interrupts():
+ *   - arch/x86/kvm/svm/svm.c|4067| <<svm_cancel_injection>> svm_complete_interrupts(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|4250| <<svm_vcpu_run>> svm_complete_interrupts(vcpu);
+ */
 static void svm_complete_interrupts(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4067,6 +4076,10 @@ static void svm_cancel_injection(struct kvm_vcpu *vcpu)
 	svm_complete_interrupts(vcpu);
 }
 
+/*
+ * 只在一个地方使用svm_exit_handlers_fastpath():
+ *   - arch/x86/kvm/svm/svm.c|4255| <<svm_vcpu_run>> return svm_exit_handlers_fastpath(vcpu);
+ */
 static fastpath_t svm_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
 {
 	struct vmcb_control_area *control = &to_svm(vcpu)->vmcb->control;
@@ -4075,6 +4088,11 @@ static fastpath_t svm_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
 	 * Note, the next RIP must be provided as SRCU isn't held, i.e. KVM
 	 * can't read guest memory (dereference memslots) to decode the WRMSR.
 	 */
+	/*
+	 * 在以下使用handle_fastpath_set_msr_irqoff():
+	 *   - arch/x86/kvm/svm/svm.c|4080| <<svm_exit_handlers_fastpath>> return handle_fastpath_set_msr_irqoff(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6835| <<vmx_exit_handlers_fastpath>> return handle_fastpath_set_msr_irqoff(vcpu);
+	 */
 	if (control->exit_code == SVM_EXIT_MSR && control->exit_info_1 &&
 	    nrips && control->next_rip)
 		return handle_fastpath_set_msr_irqoff(vcpu);
@@ -4107,6 +4125,11 @@ static noinstr void svm_vcpu_enter_exit(struct kvm_vcpu *vcpu)
 		vmload(__sme_page_pa(sd->save_area));
 	}
 
+	/*
+	 * 在以下使用kvm_guest_exit_irqoff():
+	 *   - arch/x86/kvm/svm/svm.c|4110| <<svm_vcpu_enter_exit>> kvm_guest_exit_irqoff();
+	 *   - arch/x86/kvm/vmx/vmx.c|6873| <<vmx_vcpu_enter_exit>> kvm_guest_exit_irqoff();
+	 */
 	kvm_guest_exit_irqoff();
 }
 
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index ab2a35fe97f8..1e3af984cabf 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -2108,6 +2108,24 @@ static void vmx_start_preemption_timer(struct kvm_vcpu *vcpu,
 		return;
 	}
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2446| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/hyperv.c|1684| <<kvm_hv_get_msr>> data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
+	 *   - arch/x86/kvm/lapic.c|1650| <<__wait_lapic_expire>> do_div(delay_ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1670| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1675| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1777| <<start_sw_tscdeadline>> unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/vmx/nested.c|2111| <<vmx_start_preemption_timer>> if (vcpu->arch.virtual_tsc_khz == 0)
+	 *   - arch/x86/kvm/vmx/nested.c|2116| <<vmx_start_preemption_timer>> do_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/vmx/nested.c|4043| <<vmx_get_preemption_timer_value>> value = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2672| <<kvm_synchronize_tsc>> if (vcpu->arch.virtual_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2683| <<kvm_synchronize_tsc>> u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
+	 *   - arch/x86/kvm/x86.c|2701| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2734| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|5910| <<kvm_arch_vcpu_ioctl(KVM_GET_TSC_KHZ)>> r = vcpu->arch.virtual_tsc_khz;
+	 */
 	if (vcpu->arch.virtual_tsc_khz == 0)
 		return;
 
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index a9d7208fa311..aba6c2def690 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -5570,6 +5570,12 @@ static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
 		 * xfer_to_guest_mode() which will create a proper return
 		 * code.
 		 */
+		/*
+		 * 在以下使用__xfer_to_guest_mode_work_pending():
+		 *   - arch/x86/kvm/vmx/vmx.c|5573| <<handle_invalid_guest_state>> if (__xfer_to_guest_mode_work_pending())
+		 *   - arch/x86/kvm/x86.c|10942| <<vcpu_run>> if (__xfer_to_guest_mode_work_pending()) {
+		 *   - include/linux/entry-kvm.h|95| <<xfer_to_guest_mode_work_pending>> return __xfer_to_guest_mode_work_pending();
+		 */
 		if (__xfer_to_guest_mode_work_pending())
 			return 1;
 	}
@@ -6832,6 +6838,11 @@ static fastpath_t vmx_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
 {
 	switch (to_vmx(vcpu)->exit_reason.basic) {
 	case EXIT_REASON_MSR_WRITE:
+		/*
+		 * 在以下使用handle_fastpath_set_msr_irqoff():
+		 *   - arch/x86/kvm/svm/svm.c|4080| <<svm_exit_handlers_fastpath>> return handle_fastpath_set_msr_irqoff(vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|6835| <<vmx_exit_handlers_fastpath>> return handle_fastpath_set_msr_irqoff(vcpu);
+		 */
 		return handle_fastpath_set_msr_irqoff(vcpu);
 	case EXIT_REASON_PREEMPTION_TIMER:
 		return handle_fastpath_preemption_timer(vcpu);
@@ -6870,6 +6881,11 @@ static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
 
 	vmx_enable_fb_clear(vmx);
 
+	/*
+	 * 在以下使用kvm_guest_exit_irqoff():
+	 *   - arch/x86/kvm/svm/svm.c|4110| <<svm_vcpu_enter_exit>> kvm_guest_exit_irqoff();
+	 *   - arch/x86/kvm/vmx/vmx.c|6873| <<vmx_vcpu_enter_exit>> kvm_guest_exit_irqoff();
+	 */
 	kvm_guest_exit_irqoff();
 }
 
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 2b29a85631bb..a7297e99e932 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -163,6 +163,13 @@ bool __read_mostly kvm_has_bus_lock_exit;
 EXPORT_SYMBOL_GPL(kvm_has_bus_lock_exit);
 
 /* tsc tolerance in parts per million - default to 1/2 of the NTP threshold */
+/*
+ * 在以下使用tsc_tolerance_ppm:
+ *   - arch/x86/kvm/x86.c|166| <<global>> static u32 __read_mostly tsc_tolerance_ppm = 250;
+ *   - arch/x86/kvm/x86.c|167| <<global>> module_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/x86.c|2429| <<kvm_set_tsc_khz>> thresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);
+ *   - arch/x86/kvm/x86.c|2430| <<kvm_set_tsc_khz>> thresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);
+ */
 static u32 __read_mostly tsc_tolerance_ppm = 250;
 module_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);
 
@@ -2029,9 +2036,22 @@ int kvm_emulate_monitor(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_monitor);
 
+/*
+ * 在以下使用kvm_vcpu_exit_request():
+ *   - arch/x86/kvm/x86.c|10667| <<vcpu_enter_guest>> if (kvm_vcpu_exit_request(vcpu)) {
+ *   - arch/x86/kvm/x86.c|10719| <<vcpu_enter_guest>> if (unlikely(kvm_vcpu_exit_request(vcpu))) {
+ */
 static inline bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 只在这里调用
+	 */
 	xfer_to_guest_mode_prepare();
+	/*
+	 * 在以下使用xfer_to_guest_mode_work_pending():
+	 *   - arch/arm64/kvm/arm.c|755| <<kvm_vcpu_exit_request>> xfer_to_guest_mode_work_pending();
+	 *   - arch/x86/kvm/x86.c|2048| <<kvm_vcpu_exit_request>> xfer_to_guest_mode_work_pending();
+	 */
 	return vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||
 		xfer_to_guest_mode_work_pending();
 }
@@ -2066,6 +2086,11 @@ static int handle_fastpath_set_tscdeadline(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * 在以下使用handle_fastpath_set_msr_irqoff():
+ *   - arch/x86/kvm/svm/svm.c|4080| <<svm_exit_handlers_fastpath>> return handle_fastpath_set_msr_irqoff(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6835| <<vmx_exit_handlers_fastpath>> return handle_fastpath_set_msr_irqoff(vcpu);
+ */
 fastpath_t handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu)
 {
 	u32 msr = kvm_rcx_read(vcpu);
@@ -2205,6 +2230,15 @@ void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 	 * system time (updated by kvm_guest_time_update below) to the
 	 * wall clock specified here.  We do the reverse here.
 	 */
+	/*
+	 * 在以下使用get_kvmclock_ns():
+	 *   - arch/x86/kvm/hyperv.c|566| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+	 *   - arch/x86/kvm/x86.c|2208| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+	 *   - arch/x86/kvm/x86.c|6554| <<kvm_arch_vm_ioctl(KVM_GET_CLOCK)>> now_ns = get_kvmclock_ns(kvm);
+	 *   - arch/x86/kvm/xen.c|68| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+	 *   - arch/x86/kvm/xen.c|455| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+	 *   - arch/x86/kvm/xen.c|496| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+	 */
 	wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
 
 	wc.nsec = do_div(wall_nsec, 1000000000);
@@ -2223,12 +2257,29 @@ void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * 在以下使用kvm_write_system_time():
+ *   - arch/x86/kvm/x86.c|3589| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME_NEW)>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|3595| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME)>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 
 	if (vcpu->vcpu_id == 0 && !host_initiated) {
+		/*
+		 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+		 *   - arch/x86/kvm/hyperv.c|1398| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2247| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2474| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9104| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10312| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|11956| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|58| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+		 *
+		 * 处理的函数kvm_gen_update_masterclock().
+		 */
 		if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 
@@ -2236,13 +2287,52 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 	}
 
 	vcpu->arch.time = system_time;
+	/*
+	 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|2239| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|4575| <<kvm_arch_vcpu_load>> if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
+	 *                                      kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9992| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+	 *
+	 * 处理的函数kvm_gen_kvmclock_update().
+	 */
 	kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 
 	/* we verify if the enable bit is set... */
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2242| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2249| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3088| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3286| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5108| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|6133| <<kvm_arch_suspend_notifier>> if (!vcpu->arch.pv_time_enabled)
+	 */
 	vcpu->arch.pv_time_enabled = false;
 	if (!(system_time & 1))
 		return;
 
+	/*
+	 * 在以下使用kvm_gfn_to_hva_cache_init():
+	 *   - arch/x86/kvm/lapic.c|2947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.apic->vapic_cache, vapic_addr, sizeof(u32)))
+	 *   - arch/x86/kvm/lapic.c|3063| <<kvm_lapic_enable_pv_eoi>> return kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            ghc, addr, new_len);
+	 *   - arch/x86/kvm/x86.c|2278| <<kvm_write_system_time>> if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.pv_time, system_time & ~1ULL, sizeof(struct pvclock_vcpu_time_info)))
+	 *   - arch/x86/kvm/x86.c|3442| <<kvm_pv_enable_async_pf>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.apf.data, gpa, sizeof(u64)))
+	 *   - arch/x86/kvm/x86.c|3561| <<record_steal_time>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            ghc, gpa, sizeof(*st)) ||
+	 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_update_runstate_guest>> kvm_gfn_to_hva_cache_init(v->kvm,
+	 *            ghc, ghc->gpa, ghc->len))
+	 *   - arch/x86/kvm/xen.c|365| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.xen.vcpu_info_cache, data->u.gpa, sizeof(struct vcpu_info));
+	 *   - arch/x86/kvm/xen.c|388| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.xen.vcpu_time_info_cache, data->u.gpa, sizeof(struct pvclock_vcpu_time_info));
+	 *   - arch/x86/kvm/xen.c|415| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.xen.runstate_cache, data->u.gpa, sizeof(struct vcpu_runstate_info));
+	 */
 	if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
 				       &vcpu->arch.pv_time, system_time & ~1ULL,
 				       sizeof(struct pvclock_vcpu_time_info)))
@@ -2337,6 +2427,11 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_set_tsc_khz():
+ *   - arch/x86/kvm/x86.c|5904| <<kvm_arch_vcpu_ioctl>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|11936| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, max_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2353,6 +2448,24 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
 			   &vcpu->arch.virtual_tsc_shift,
 			   &vcpu->arch.virtual_tsc_mult);
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2446| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/hyperv.c|1684| <<kvm_hv_get_msr>> data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
+	 *   - arch/x86/kvm/lapic.c|1650| <<__wait_lapic_expire>> do_div(delay_ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1670| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1675| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1777| <<start_sw_tscdeadline>> unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/vmx/nested.c|2111| <<vmx_start_preemption_timer>> if (vcpu->arch.virtual_tsc_khz == 0)
+	 *   - arch/x86/kvm/vmx/nested.c|2116| <<vmx_start_preemption_timer>> do_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/vmx/nested.c|4043| <<vmx_get_preemption_timer_value>> value = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2672| <<kvm_synchronize_tsc>> if (vcpu->arch.virtual_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2683| <<kvm_synchronize_tsc>> u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
+	 *   - arch/x86/kvm/x86.c|2701| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2734| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|5910| <<kvm_arch_vcpu_ioctl(KVM_GET_TSC_KHZ)>> r = vcpu->arch.virtual_tsc_khz;
+	 */
 	vcpu->arch.virtual_tsc_khz = user_tsc_khz;
 
 	/*
@@ -2372,6 +2485,15 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 
 static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 {
+	/*
+	 * 在以下使用pvclock_scale_delta():
+	 *   - arch/x86/include/asm/pvclock.h|85| <<__pvclock_read_cycles>> u64 offset = pvclock_scale_delta(delta,
+	 *          src->tsc_to_system_mul, src->tsc_shift);
+	 *   - arch/x86/kvm/x86.c|2375| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *          vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|354| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+	 *          vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 */
 	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
 				      vcpu->arch.virtual_tsc_mult,
 				      vcpu->arch.virtual_tsc_shift);
@@ -2407,6 +2529,18 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu, bool new_generation)
 	 * enabled (compute_guest_tsc() requires the masterclock snapshot to be
 	 * taken _after_ the new generation is created).
 	 */
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   - arch/x86/kvm/hyperv.c|1398| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2247| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2474| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9104| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10312| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|11956| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|58| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 *
+	 * 处理的函数kvm_gen_update_masterclock().
+	 */
 	if ((ka->use_master_clock && new_generation) ||
 	    (ka->use_master_clock != use_master_clock))
 		kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -2484,6 +2618,12 @@ u64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)
 }
 EXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_multiplier);
 
+/*
+ * 在以下使用kvm_vcpu_write_tsc_offset():
+ *   - arch/x86/kvm/x86.c|2719| <<kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2747| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|4909| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 {
 	trace_kvm_write_tsc_offset(vcpu->vcpu_id,
@@ -2552,6 +2692,24 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	ns = get_kvmclock_base_ns();
 	elapsed = ns - kvm->arch.last_tsc_nsec;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2446| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/hyperv.c|1684| <<kvm_hv_get_msr>> data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
+	 *   - arch/x86/kvm/lapic.c|1650| <<__wait_lapic_expire>> do_div(delay_ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1670| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1675| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1777| <<start_sw_tscdeadline>> unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/vmx/nested.c|2111| <<vmx_start_preemption_timer>> if (vcpu->arch.virtual_tsc_khz == 0)
+	 *   - arch/x86/kvm/vmx/nested.c|2116| <<vmx_start_preemption_timer>> do_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/vmx/nested.c|4043| <<vmx_get_preemption_timer_value>> value = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2672| <<kvm_synchronize_tsc>> if (vcpu->arch.virtual_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2683| <<kvm_synchronize_tsc>> u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
+	 *   - arch/x86/kvm/x86.c|2701| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2734| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|5910| <<kvm_arch_vcpu_ioctl(KVM_GET_TSC_KHZ)>> r = vcpu->arch.virtual_tsc_khz;
+	 */
 	if (vcpu->arch.virtual_tsc_khz) {
 		if (data == 0) {
 			/*
@@ -2626,6 +2784,16 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	kvm_vcpu_write_tsc_offset(vcpu, offset);
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 
+	/*
+	 *  在以下使用kvm_arch->pvclock_gtod_sync_lock:
+	 *   - kvm_synchronize_tsc()
+	 *   - kvm_gen_update_masterclock()
+	 *   - get_kvmclock_ns()
+	 *   - kvm_guest_time_update()
+	 *   - kvm_arch_vm_ioctl(KVM_SET_CLOCK)
+	 *   - kvm_hyperv_tsc_notifier()
+	 *   - kvm_arch_init_vm()
+	 */
 	raw_spin_lock_irqsave(&kvm->arch.pvclock_gtod_sync_lock, flags);
 	if (!matched) {
 		kvm->arch.nr_vcpus_matched_tsc = 0;
@@ -2815,6 +2983,12 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * 在以下使用pvclock_update_vm_gtod_copy():
+ *   - arch/x86/kvm/x86.c|2987| <<kvm_gen_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|9018| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|12270| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2846,6 +3020,11 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 #endif
 }
 
+/*
+ * 在以下使用kvm_make_mclock_inprogress_request():
+ *   - arch/x86/kvm/x86.c|2979| <<kvm_gen_update_masterclock>> kvm_make_mclock_inprogress_request(kvm);
+ *   - arch/x86/kvm/x86.c|9027| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+ */
 void kvm_make_mclock_inprogress_request(struct kvm *kvm)
 {
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
@@ -2863,6 +3042,16 @@ static void kvm_gen_update_masterclock(struct kvm *kvm)
 
 	kvm_make_mclock_inprogress_request(kvm);
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_gtod_sync_lock:
+	 *   - kvm_synchronize_tsc()
+	 *   - kvm_gen_update_masterclock()
+	 *   - get_kvmclock_ns()
+	 *   - kvm_guest_time_update()
+	 *   - kvm_arch_vm_ioctl(KVM_SET_CLOCK)
+	 *   - kvm_hyperv_tsc_notifier()
+	 *   - kvm_arch_init_vm()
+	 */
 	/* no guest entries from this point */
 	raw_spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
 	pvclock_update_vm_gtod_copy(kvm);
@@ -2877,6 +3066,15 @@ static void kvm_gen_update_masterclock(struct kvm *kvm)
 #endif
 }
 
+/*
+ * 在以下使用get_kvmclock_ns():
+ *   - arch/x86/kvm/hyperv.c|566| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|2208| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/x86.c|6554| <<kvm_arch_vm_ioctl(KVM_GET_CLOCK)>> now_ns = get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|68| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|455| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|496| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2884,6 +3082,16 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	unsigned long flags;
 	u64 ret;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_gtod_sync_lock:
+	 *   - kvm_synchronize_tsc()
+	 *   - kvm_gen_update_masterclock()
+	 *   - get_kvmclock_ns()
+	 *   - kvm_guest_time_update()
+	 *   - kvm_arch_vm_ioctl(KVM_SET_CLOCK)
+	 *   - kvm_hyperv_tsc_notifier()
+	 *   - kvm_arch_init_vm()
+	 */
 	raw_spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
 	if (!ka->use_master_clock) {
 		raw_spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
@@ -2910,13 +3118,43 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	return ret;
 }
 
+/*
+ * 在以下使用kvm_setup_pvclock_page():
+ *   - arch/x86/kvm/x86.c|3057| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->pv_time, 0);
+ *   - arch/x86/kvm/x86.c|3059| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->xen.vcpu_info_cache,
+ *                offsetof(struct compat_vcpu_info, time));
+ *   - arch/x86/kvm/x86.c|3062| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->xen.vcpu_time_info_cache, 0);
+ */
 static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 				   struct gfn_to_hva_cache *cache,
 				   unsigned int offset)
 {
 	struct kvm_vcpu_arch *vcpu = &v->arch;
+	/*
+	 * struct pvclock_vcpu_time_info {
+	 *     u32   version;
+	 *     u32   pad0;
+	 *     u64   tsc_timestamp;
+	 *     u64   system_time;
+	 *     u32   tsc_to_system_mul;
+	 *     s8    tsc_shift;
+	 *     u8    flags;
+	 *     u8    pad[2];
+	 * } __attribute__((__packed__));
+	 */
 	struct pvclock_vcpu_time_info guest_hv_clock;
 
+	/*
+	 * 在以下使用kvm_read_guest_offset_cached():
+	 *   - arch/x86/kvm/x86.c|2989| <<kvm_setup_pvclock_page>> if (unlikely(kvm_read_guest_offset_cached(v->kvm,
+	 *          cache, &guest_hv_clock, offset, sizeof(guest_hv_clock))))
+	 *   - arch/x86/kvm/x86.c|12767| <<apf_pageready_slot_free>> if (kvm_read_guest_offset_cached(vcpu->kvm,
+	 *          &vcpu->arch.apf.data, &val, offset, sizeof(val)))
+	 *   - arch/x86/kvm/xen.c|259| <<__kvm_xen_has_interrupt>> kvm_read_guest_offset_cached(v->kvm,
+	 *          ghc, &rc, offset, sizeof(rc));
+	 *   - virt/kvm/kvm_main.c|3319| <<kvm_read_guest_cached>> return kvm_read_guest_offset_cached(kvm,
+	 *          ghc, data, 0, len);
+	 */
 	if (unlikely(kvm_read_guest_offset_cached(v->kvm, cache,
 		&guest_hv_clock, offset, sizeof(guest_hv_clock))))
 		return;
@@ -2941,6 +3179,9 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 		++guest_hv_clock.version;  /* first time write, random junk */
 
 	vcpu->hv_clock.version = guest_hv_clock.version + 1;
+	/*
+	 * 第一次
+	 */
 	kvm_write_guest_offset_cached(v->kvm, cache,
 				      &vcpu->hv_clock, offset,
 				      sizeof(vcpu->hv_clock.version));
@@ -2950,6 +3191,12 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 	/* retain PVCLOCK_GUEST_STOPPED if set in guest copy */
 	vcpu->hv_clock.flags |= (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);
 
+	/*
+	 * 在以下使用kvm_vcou_arch->pvclock_set_guest_stopped_request:
+	 *   - arch/x86/kvm/x86.c|3033| <<kvm_setup_pvclock_page>> if (vcpu->pvclock_set_guest_stopped_request) {
+	 *   - arch/x86/kvm/x86.c|3035| <<kvm_setup_pvclock_page>> vcpu->pvclock_set_guest_stopped_request = false;
+	 *   - arch/x86/kvm/x86.c|5323| <<kvm_set_guest_paused>> vcpu->arch.pvclock_set_guest_stopped_request = true;
+	 */
 	if (vcpu->pvclock_set_guest_stopped_request) {
 		vcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
 		vcpu->pvclock_set_guest_stopped_request = false;
@@ -2957,6 +3204,9 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 
 	trace_kvm_pvclock_update(v->vcpu_id, &vcpu->hv_clock);
 
+	/*
+	 * 第二次
+	 */
 	kvm_write_guest_offset_cached(v->kvm, cache,
 				      &vcpu->hv_clock, offset,
 				      sizeof(vcpu->hv_clock));
@@ -2964,11 +3214,36 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 	smp_wmb();
 
 	vcpu->hv_clock.version++;
+	/*
+	 * 第三次
+	 */
 	kvm_write_guest_offset_cached(v->kvm, cache,
 				     &vcpu->hv_clock, offset,
 				     sizeof(vcpu->hv_clock.version));
 }
 
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2881| <<kvm_gen_update_masterclock>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3030| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3122| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3131| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3515| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4547| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5107| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl(KVM_SET_CLOCK)>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *   - arch/x86/kvm/x86.c|8594| <<kvm_hyperv_tsc_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8655| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9990| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|10312| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11569| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|371| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|394| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *
+ *
+ * 处理KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|9991| <<vcpu_enter_guest(KVM_REQ_CLOCK_UPDATE)>> r = kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -2982,6 +3257,16 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	kernel_ns = 0;
 	host_tsc = 0;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_gtod_sync_lock:
+	 *   - kvm_synchronize_tsc()
+	 *   - kvm_gen_update_masterclock()
+	 *   - get_kvmclock_ns()
+	 *   - kvm_guest_time_update()
+	 *   - kvm_arch_vm_ioctl(KVM_SET_CLOCK)
+	 *   - kvm_hyperv_tsc_notifier()
+	 *   - kvm_arch_init_vm()
+	 */
 	/*
 	 * If the host uses TSC clock, then passthrough TSC as stable
 	 * to the guest.
@@ -3035,6 +3320,11 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		tgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz,
 					    v->arch.l1_tsc_scaling_ratio);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->hw_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3253| <<kvm_guest_time_update>> if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3257| <<kvm_guest_time_update>> vcpu->hw_tsc_khz = tgt_tsc_khz;
+	 */
 	if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
 		kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
 				   &vcpu->hv_clock.tsc_shift,
@@ -3053,6 +3343,15 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	vcpu->hv_clock.flags = pvclock_flags;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2242| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2249| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3088| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3286| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5108| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|6133| <<kvm_arch_suspend_notifier>> if (!vcpu->arch.pv_time_enabled)
+	 */
 	if (vcpu->pv_time_enabled)
 		kvm_setup_pvclock_page(v, &vcpu->pv_time, 0);
 	if (vcpu->xen.vcpu_info_set)
@@ -3079,8 +3378,24 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
  * by the delay we use to rate-limit the updates.
  */
 
+/*
+ * 在以下使用KVMCLOCK_UPDATE_DELAY:
+ *   - arch/x86/kvm/x86.c|3137| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+ *                                                     KVMCLOCK_UPDATE_DELAY);
+ */
 #define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)
 
+/*
+ * 在以下使用kvm_arch->kvmclock_update_work:
+ *   - arch/x86/kvm/x86.c|3136| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+ *   - arch/x86/kvm/x86.c|3152| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+ *   - arch/x86/kvm/x86.c|11789| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ *   - arch/x86/kvm/x86.c|11838| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+ *
+ *
+ * 在以下使用kvmclock_update_fn():
+ *   - arch/x86/kvm/x86.c|11789| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ */
 static void kvmclock_update_fn(struct work_struct *work)
 {
 	int i;
@@ -3091,22 +3406,101 @@ static void kvmclock_update_fn(struct work_struct *work)
 	struct kvm_vcpu *vcpu;
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/x86.c|2881| <<kvm_gen_update_masterclock>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3030| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3122| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3131| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3515| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|4547| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5107| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl(KVM_SET_CLOCK)>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+		 *   - arch/x86/kvm/x86.c|8594| <<kvm_hyperv_tsc_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|8655| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9990| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|10312| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11569| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|371| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|394| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *
+		 * 处理的函数kvm_guest_time_update()
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		kvm_vcpu_kick(vcpu);
 	}
 }
 
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2239| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4575| <<kvm_arch_vcpu_load>> if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
+ *                                      kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9992| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *       
+ * 处理的函数kvm_gen_kvmclock_update().
+ *
+ *
+ * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|9993| <<vcpu_enter_guest(KVM_REQ_GLOBAL_CLOCK_UPDATE)>> kvm_gen_kvmclock_update(vcpu);
+ *
+ * 核心思想就是让所有的vCPU都KVM_REQ_CLOCK_UPDATE
+ */
 static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 {
 	struct kvm *kvm = v->kvm;
 
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|2881| <<kvm_gen_update_masterclock>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3030| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3122| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3131| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3515| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|4547| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5107| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl(KVM_SET_CLOCK)>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+	 *   - arch/x86/kvm/x86.c|8594| <<kvm_hyperv_tsc_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|8655| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9990| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|10312| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11569| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|371| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|394| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *
+	 * 处理的函数kvm_guest_time_update()
+	 */
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3136| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3152| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11789| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11838| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 *
+	 * 调用kvmclock_update_fn()
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
 }
 
+/*
+ * 在以下使用KVMCLOCK_SYNC_PERIOD:
+ *   - arch/x86/kvm/x86.c|3154| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|11374| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ */
 #define KVMCLOCK_SYNC_PERIOD (300 * HZ)
 
+/*
+ * 在以下使用kvm_arch->kvmclock_sync_work:
+ *   - arch/x86/kvm/x86.c|3233| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|11489| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|11913| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+ *   - arch/x86/kvm/x86.c|11960| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+ *
+ * 在以下使用kvmclock_sync_fn():
+ *   - arch/x86/kvm/x86.c|11913| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+ */
 static void kvmclock_sync_fn(struct work_struct *work)
 {
 	struct delayed_work *dwork = to_delayed_work(work);
@@ -3117,7 +3511,21 @@ static void kvmclock_sync_fn(struct work_struct *work)
 	if (!kvmclock_periodic_sync)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3136| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3152| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11789| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11838| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3233| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|11489| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|11913| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|11960| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 					KVMCLOCK_SYNC_PERIOD);
 }
@@ -3221,6 +3629,27 @@ static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)
 		return 0;
 	}
 
+	/*
+	 * 在以下使用kvm_gfn_to_hva_cache_init():
+	 *   - arch/x86/kvm/lapic.c|2947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.apic->vapic_cache, vapic_addr, sizeof(u32)))
+	 *   - arch/x86/kvm/lapic.c|3063| <<kvm_lapic_enable_pv_eoi>> return kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            ghc, addr, new_len);
+	 *   - arch/x86/kvm/x86.c|2278| <<kvm_write_system_time>> if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.pv_time, system_time & ~1ULL, sizeof(struct pvclock_vcpu_time_info)))
+	 *   - arch/x86/kvm/x86.c|3442| <<kvm_pv_enable_async_pf>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.apf.data, gpa, sizeof(u64)))
+	 *   - arch/x86/kvm/x86.c|3561| <<record_steal_time>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            ghc, gpa, sizeof(*st)) ||
+	 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_update_runstate_guest>> kvm_gfn_to_hva_cache_init(v->kvm,
+	 *            ghc, ghc->gpa, ghc->len))
+	 *   - arch/x86/kvm/xen.c|365| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.xen.vcpu_info_cache, data->u.gpa, sizeof(struct vcpu_info));
+	 *   - arch/x86/kvm/xen.c|388| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.xen.vcpu_time_info_cache, data->u.gpa, sizeof(struct pvclock_vcpu_time_info));
+	 *   - arch/x86/kvm/xen.c|415| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *            &vcpu->arch.xen.runstate_cache, data->u.gpa, sizeof(struct vcpu_runstate_info));
+	 */
 	if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apf.data, gpa,
 					sizeof(u64)))
 		return 1;
@@ -3251,6 +3680,15 @@ static int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)
 
 static void kvmclock_reset(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2242| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2249| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3088| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3286| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5108| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|6133| <<kvm_arch_suspend_notifier>> if (!vcpu->arch.pv_time_enabled)
+	 */
 	vcpu->arch.pv_time_enabled = false;
 	vcpu->arch.time = 0;
 }
@@ -3331,6 +3769,27 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
 		/* We rely on the fact that it fits in a single page. */
 		BUILD_BUG_ON((sizeof(*st) - 1) & KVM_STEAL_VALID_BITS);
 
+		/*
+		 * 在以下使用kvm_gfn_to_hva_cache_init():
+		 *   - arch/x86/kvm/lapic.c|2947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            &vcpu->arch.apic->vapic_cache, vapic_addr, sizeof(u32)))
+		 *   - arch/x86/kvm/lapic.c|3063| <<kvm_lapic_enable_pv_eoi>> return kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            ghc, addr, new_len);
+		 *   - arch/x86/kvm/x86.c|2278| <<kvm_write_system_time>> if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            &vcpu->arch.pv_time, system_time & ~1ULL, sizeof(struct pvclock_vcpu_time_info)))
+		 *   - arch/x86/kvm/x86.c|3442| <<kvm_pv_enable_async_pf>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            &vcpu->arch.apf.data, gpa, sizeof(u64)))
+		 *   - arch/x86/kvm/x86.c|3561| <<record_steal_time>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            ghc, gpa, sizeof(*st)) ||
+		 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_update_runstate_guest>> kvm_gfn_to_hva_cache_init(v->kvm,
+		 *            ghc, ghc->gpa, ghc->len))
+		 *   - arch/x86/kvm/xen.c|365| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            &vcpu->arch.xen.vcpu_info_cache, data->u.gpa, sizeof(struct vcpu_info));
+		 *   - arch/x86/kvm/xen.c|388| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            &vcpu->arch.xen.vcpu_time_info_cache, data->u.gpa, sizeof(struct pvclock_vcpu_time_info));
+		 *   - arch/x86/kvm/xen.c|415| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *            &vcpu->arch.xen.runstate_cache, data->u.gpa, sizeof(struct vcpu_runstate_info));
+		 */
 		if (kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, gpa, sizeof(*st)) ||
 		    kvm_is_error_hva(ghc->hva) || !ghc->memslot)
 			return;
@@ -3398,6 +3857,19 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
  out:
 	user_access_end();
  dirty:
+	/*
+	 * 在以下使用mark_page_dirty_in_slot():
+	 *   - arch/arm64/kvm/mmu.c|1179| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|267| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+	 *   - arch/x86/kvm/x86.c|3401| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/x86.c|4596| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/xen.c|204| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|2898| <<__kvm_unmap_gfn>> mark_page_dirty_in_slot(kvm, memslot, map->gfn);
+	 *   - virt/kvm/kvm_main.c|3113| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|3358| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3367| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+	 */
 	mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
 }
 
@@ -4539,6 +5011,15 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 		 * On a host with synchronized TSC, there is no need to update
 		 * kvmclock on vcpu->cpu migration
 		 */
+		/*
+		 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/x86.c|2239| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|4575| <<kvm_arch_vcpu_load>> if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
+		 *                                      kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9992| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+		 *
+		 * 处理的函数kvm_gen_kvmclock_update().
+		 */
 		if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
 			kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 		if (vcpu->cpu != cpu)
@@ -4593,6 +5074,19 @@ static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)
 	if (!copy_to_user_nofault(&st->preempted, &preempted, sizeof(preempted)))
 		vcpu->arch.st.preempted = KVM_VCPU_PREEMPTED;
 
+	/*
+	 * 在以下使用mark_page_dirty_in_slot():
+	 *   - arch/arm64/kvm/mmu.c|1179| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|267| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+	 *   - arch/x86/kvm/x86.c|3401| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/x86.c|4596| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/xen.c|204| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|2898| <<__kvm_unmap_gfn>> mark_page_dirty_in_slot(kvm, memslot, map->gfn);
+	 *   - virt/kvm/kvm_main.c|3113| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|3358| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3367| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+	 */
 	mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
 }
 
@@ -5071,10 +5565,30 @@ static int kvm_vcpu_ioctl_x86_set_xcrs(struct kvm_vcpu *vcpu,
  * EINVAL is returned when the host attempts to set the flag for a guest that
  * does not support pv clocks.
  */
+/*
+ * 在以下使用kvm_set_guest_paused():
+ *   - arch/x86/kvm/x86.c|5686| <<kvm_arch_vcpu_ioctl>> r = kvm_set_guest_paused(vcpu);
+ *   - arch/x86/kvm/x86.c|6349| <<kvm_arch_suspend_notifier>> ret = kvm_set_guest_paused(vcpu);
+ */
 static int kvm_set_guest_paused(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2242| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2249| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3088| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3286| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5108| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|6133| <<kvm_arch_suspend_notifier>> if (!vcpu->arch.pv_time_enabled)
+	 */
 	if (!vcpu->arch.pv_time_enabled)
 		return -EINVAL;
+	/*
+	 * 在以下使用kvm_vcou_arch->pvclock_set_guest_stopped_request:
+	 *   - arch/x86/kvm/x86.c|3033| <<kvm_setup_pvclock_page>> if (vcpu->pvclock_set_guest_stopped_request) {
+	 *   - arch/x86/kvm/x86.c|3035| <<kvm_setup_pvclock_page>> vcpu->pvclock_set_guest_stopped_request = false;
+	 *   - arch/x86/kvm/x86.c|5323| <<kvm_set_guest_paused>> vcpu->arch.pvclock_set_guest_stopped_request = true;
+	 */
 	vcpu->arch.pvclock_set_guest_stopped_request = true;
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 	return 0;
@@ -5434,6 +5948,24 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		goto out;
 	}
 	case KVM_GET_TSC_KHZ: {
+		/*
+		 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+		 *   - arch/x86/kvm/x86.c|2446| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+		 * 在以下使用kvm_vcpu_arch->virtual_tsc_khz:
+		 *   - arch/x86/kvm/hyperv.c|1684| <<kvm_hv_get_msr>> data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
+		 *   - arch/x86/kvm/lapic.c|1650| <<__wait_lapic_expire>> do_div(delay_ns, vcpu->arch.virtual_tsc_khz);
+		 *   - arch/x86/kvm/lapic.c|1670| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+		 *   - arch/x86/kvm/lapic.c|1675| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+		 *   - arch/x86/kvm/lapic.c|1777| <<start_sw_tscdeadline>> unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
+		 *   - arch/x86/kvm/vmx/nested.c|2111| <<vmx_start_preemption_timer>> if (vcpu->arch.virtual_tsc_khz == 0)
+		 *   - arch/x86/kvm/vmx/nested.c|2116| <<vmx_start_preemption_timer>> do_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);
+		 *   - arch/x86/kvm/vmx/nested.c|4043| <<vmx_get_preemption_timer_value>> value = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;
+		 *   - arch/x86/kvm/x86.c|2672| <<kvm_synchronize_tsc>> if (vcpu->arch.virtual_tsc_khz) {
+		 *   - arch/x86/kvm/x86.c|2683| <<kvm_synchronize_tsc>> u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
+		 *   - arch/x86/kvm/x86.c|2701| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+		 *   - arch/x86/kvm/x86.c|2734| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+		 *   - arch/x86/kvm/x86.c|5910| <<kvm_arch_vcpu_ioctl(KVM_GET_TSC_KHZ)>> r = vcpu->arch.virtual_tsc_khz;
+		 */
 		r = vcpu->arch.virtual_tsc_khz;
 		goto out;
 	}
@@ -5751,6 +6283,15 @@ void kvm_arch_sync_dirty_log(struct kvm *kvm, struct kvm_memory_slot *memslot)
 	struct kvm_vcpu *vcpu;
 	int i;
 
+	/*
+	 * 在以下使用kvm_dirty_log_pgtable():
+	 *   - arch/x86/kvm/mmu/mmu.c|899| <<gfn_to_memslot_dirty_bitmap>> !kvm_dirty_log_pgtable(vcpu->kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1388| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> !kvm_dirty_log_pgtable(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1401| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> if (kvm_dirty_log_pgtable(kvm) &&
+	 *   - arch/x86/kvm/x86.c|6114| <<kvm_arch_sync_dirty_log>> if (kvm_dirty_log_pgtable(kvm)) {
+	 *   - arch/x86/kvm/x86.c|12631| <<kvm_mmu_slot_apply_flags>> if (!kvm_dirty_log_pgtable(kvm))
+	 *   - virt/kvm/kvm_main.c|2235| <<kvm_clear_dirty_log_protect>> if (!kvm_dirty_log_pgtable(kvm))
+	 */
 	if (kvm_dirty_log_pgtable(kvm)) {
 		kvm_mmu_slot_test_dirty(kvm, memslot, PG_LEVEL_4K);
 		return;
@@ -6397,6 +6938,16 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		 * is slightly ahead) here we risk going negative on unsigned
 		 * 'system_time' when 'user_ns.clock' is very small.
 		 */
+		/*
+		 *  在以下使用kvm_arch->pvclock_gtod_sync_lock:
+		 *   - kvm_synchronize_tsc()
+		 *   - kvm_gen_update_masterclock()
+		 *   - get_kvmclock_ns()
+		 *   - kvm_guest_time_update()
+		 *   - kvm_arch_vm_ioctl(KVM_SET_CLOCK)
+		 *   - kvm_hyperv_tsc_notifier()
+		 *   - kvm_arch_init_vm()
+		 */
 		raw_spin_lock_irq(&ka->pvclock_gtod_sync_lock);
 		if (kvm->arch.use_master_clock)
 			now_ns = ka->master_kernel_ns;
@@ -6409,9 +6960,25 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		break;
 	}
 	case KVM_GET_CLOCK: {
+		/*
+		 * struct kvm_clock_data {
+		 *     __u64 clock;
+		 *     __u32 flags;
+		 *     __u32 pad[9];
+		 * };
+		 */
 		struct kvm_clock_data user_ns;
 		u64 now_ns;
 
+		/*
+		 * 在以下使用get_kvmclock_ns():
+		 *   - arch/x86/kvm/hyperv.c|566| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+		 *   - arch/x86/kvm/x86.c|2208| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+		 *   - arch/x86/kvm/x86.c|6554| <<kvm_arch_vm_ioctl(KVM_GET_CLOCK)>> now_ns = get_kvmclock_ns(kvm);
+		 *   - arch/x86/kvm/xen.c|68| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+		 *   - arch/x86/kvm/xen.c|455| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+		 *   - arch/x86/kvm/xen.c|496| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+		 */
 		now_ns = get_kvmclock_ns(kvm);
 		user_ns.clock = now_ns;
 		user_ns.flags = kvm->arch.use_master_clock ? KVM_CLOCK_TSC_STABLE : 0;
@@ -8219,6 +8786,12 @@ int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 }
 EXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);
 
+/*
+ * 在以下使用x86_emulate_instruction():
+ *   - arch/x86/kvm/mmu/mmu.c|5356| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+ *   - arch/x86/kvm/x86.c|8902| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|8909| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len)
 {
@@ -8404,15 +8977,50 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	return r;
 }
 
+/*
+ * 在以下使用kvm_emulate_instruction():
+ *   - arch/x86/kvm/svm/avic.c|697| <<avic_unaccelerated_access_interception>> ret = kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|403| <<__skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+ *   - arch/x86/kvm/svm/svm.c|2256| <<io_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|2419| <<gp_interception>> return kvm_emulate_instruction(vcpu,
+ *   - arch/x86/kvm/svm/svm.c|2611| <<invlpg_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|2619| <<emulate_on_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|1633| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+ *   - arch/x86/kvm/vmx/vmx.c|4849| <<handle_rmode_exception>> if (kvm_emulate_instruction(vcpu, 0)) {
+ *   - arch/x86/kvm/vmx/vmx.c|4938| <<handle_exception_nmi>> return kvm_emulate_instruction(vcpu, EMULTYPE_VMWARE_GP);
+ *   - arch/x86/kvm/vmx/vmx.c|5076| <<handle_io>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5143| <<handle_desc>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5362| <<handle_apic_access>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5500| <<handle_ept_violation>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5554| <<handle_invalid_guest_state>> if (!kvm_emulate_instruction(vcpu, 0))
+ *   - arch/x86/kvm/x86.c|7358| <<handle_ud>> return kvm_emulate_instruction(vcpu, emul_type);
+ *   - arch/x86/kvm/x86.c|10961| <<complete_emulated_io>> r = kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE);
+ */
 int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type)
 {
+	/*
+	 * 在以下使用x86_emulate_instruction():
+	 *   - arch/x86/kvm/mmu/mmu.c|5356| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+	 *   - arch/x86/kvm/x86.c|8902| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+	 *   - arch/x86/kvm/x86.c|8909| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+	 */
 	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_instruction);
 
+/*
+ * 在以下使用kvm_emulate_instruction_from_buffer():
+ *   - arch/x86/kvm/svm/svm.c|2624| <<rsm_interception>> return kvm_emulate_instruction_from_buffer(vcpu, rsm_ins_bytes, 2);
+ */
 int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
 					void *insn, int insn_len)
 {
+	/*
+	 * 在以下使用x86_emulate_instruction():
+	 *   - arch/x86/kvm/mmu/mmu.c|5356| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+	 *   - arch/x86/kvm/x86.c|8902| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+	 *   - arch/x86/kvm/x86.c|8909| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+	 */
 	return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_instruction_from_buffer);
@@ -8558,6 +9166,16 @@ static void kvm_hyperv_tsc_notifier(void)
 	list_for_each_entry(kvm, &vm_list, vm_list) {
 		struct kvm_arch *ka = &kvm->arch;
 
+		/*
+		 *  在以下使用kvm_arch->pvclock_gtod_sync_lock:
+		 *   - kvm_synchronize_tsc()
+		 *   - kvm_gen_update_masterclock()
+		 *   - get_kvmclock_ns()
+		 *   - kvm_guest_time_update()
+		 *   - kvm_arch_vm_ioctl(KVM_SET_CLOCK)
+		 *   - kvm_hyperv_tsc_notifier()
+		 *   - kvm_arch_init_vm()
+		 */
 		raw_spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
 		pvclock_update_vm_gtod_copy(kvm);
 		raw_spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
@@ -8746,6 +9364,10 @@ static struct perf_guest_info_callbacks kvm_guest_cbs = {
 };
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用pvclock_gtod_update_fn():
+ *   - arch/x86/kvm/x86.c|9109| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ */
 static void pvclock_gtod_update_fn(struct work_struct *work)
 {
 	struct kvm *kvm;
@@ -8754,6 +9376,18 @@ static void pvclock_gtod_update_fn(struct work_struct *work)
 	int i;
 
 	mutex_lock(&kvm_lock);
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   - arch/x86/kvm/hyperv.c|1398| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2247| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2474| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9104| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10312| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|11956| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|58| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 *
+	 * 处理的函数kvm_gen_update_masterclock().
+	 */
 	list_for_each_entry(kvm, &vm_list, vm_list)
 		kvm_for_each_vcpu(i, vcpu, kvm)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -9955,6 +10589,27 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			kvm_mmu_unload(vcpu);
 		if (kvm_check_request(KVM_REQ_MIGRATE_TIMER, vcpu))
 			__kvm_migrate_timers(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+		 *   - arch/x86/kvm/hyperv.c|1398| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2247| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2474| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9104| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10312| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|11956| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|58| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+		 *
+		 * 处理的函数kvm_gen_update_masterclock().
+		 *
+		 *
+		 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/x86.c|2239| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|4575| <<kvm_arch_vcpu_load>> if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
+		 *                                      kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9992| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+		 *
+		 * 处理的函数kvm_gen_kvmclock_update().
+		 */
 		if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
 			kvm_gen_update_masterclock(vcpu->kvm);
 		if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
@@ -10179,6 +10834,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
 			     (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));
 
+		/*
+		 * svm_vcpu_run()
+		 */
 		exit_fastpath = static_call(kvm_x86_vcpu_run)(vcpu);
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
@@ -10232,6 +10890,23 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.xfd_no_write_intercept)
 		fpu_sync_guest_vmexit_xfd_state();
 
+	/*
+	 * ioeventfd_write
+	 * __kvm_io_bus_write
+	 * kvm_io_bus_write
+	 * handle_ept_misconfig
+	 * vmx_handle_exit
+	 * vcpu_enter_guest
+	 * vcpu_run
+	 * kvm_arch_vcpu_ioctl_run
+	 * kvm_vcpu_ioctl
+	 * __x64_sys_ioctl
+	 * do_syscall_64  
+	 * entry_SYSCALL_64_after_hwframe
+	 *
+	 * vmx_handle_exit_irqoff()
+	 * svm_handle_exit_irqoff()
+	 */
 	static_call(kvm_x86_handle_exit_irqoff)(vcpu);
 
 	if (vcpu->arch.guest_fpu.xfd_err)
@@ -10286,6 +10961,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.apic_attention)
 		kvm_lapic_sync_from_vapic(vcpu);
 
+	/*
+	 * vmx_handle_exit()
+	 * handle_exit()
+	 */
 	r = static_call(kvm_x86_handle_exit)(vcpu, exit_fastpath);
 	return r;
 
@@ -10396,8 +11075,19 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 			break;
 		}
 
+		/*
+		 * 在以下使用__xfer_to_guest_mode_work_pending():
+		 *   - arch/x86/kvm/vmx/vmx.c|5573| <<handle_invalid_guest_state>> if (__xfer_to_guest_mode_work_pending())
+		 *   - arch/x86/kvm/x86.c|10942| <<vcpu_run>> if (__xfer_to_guest_mode_work_pending()) {
+		 *   - include/linux/entry-kvm.h|95| <<xfer_to_guest_mode_work_pending>> return __xfer_to_guest_mode_work_pending();
+		 */
 		if (__xfer_to_guest_mode_work_pending()) {
 			srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);
+			/*
+			 * 在以下使用xfer_to_guest_mode_handle_work():
+			 *   - arch/arm64/kvm/arm.c|819| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+			 *   - arch/x86/kvm/x86.c|10944| <<vcpu_run>> r = xfer_to_guest_mode_handle_work(vcpu);
+			 */
 			r = xfer_to_guest_mode_handle_work(vcpu);
 			if (r)
 				return r;
@@ -10410,6 +11100,11 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * 在以下使用complete_emulated_io():
+ *   - arch/x86/kvm/x86.c|10970| <<complete_emulated_pio>> return complete_emulated_io(vcpu);
+ *   - arch/x86/kvm/x86.c|11023| <<complete_emulated_mmio>> return complete_emulated_io(vcpu);
+ */
 static inline int complete_emulated_io(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -10445,6 +11140,11 @@ static int complete_emulated_pio(struct kvm_vcpu *vcpu)
  *       copy data
  *       exit
  */
+/*
+ * 在以下使用complete_emulated_mmio():
+ *   - arch/x86/kvm/x86.c|8857| <<x86_emulate_instruction>> vcpu->arch.complete_userspace_io = complete_emulated_mmio;
+ *   - arch/x86/kvm/x86.c|11032| <<complete_emulated_mmio>> vcpu->arch.complete_userspace_io = complete_emulated_mmio;
+ */
 static int complete_emulated_mmio(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *run = vcpu->run;
@@ -11337,6 +12037,13 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 
 	mutex_unlock(&vcpu->mutex);
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3233| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|11489| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|11913| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|11960| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
 		schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 						KVMCLOCK_SYNC_PERIOD);
@@ -11592,6 +12299,18 @@ int kvm_arch_hardware_enable(void)
 			kvm_for_each_vcpu(i, vcpu, kvm) {
 				vcpu->arch.tsc_offset_adjustment += delta_cyc;
 				vcpu->arch.last_host_tsc = local_tsc;
+				/*
+				 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+				 *   - arch/x86/kvm/hyperv.c|1398| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|2247| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|2474| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|9104| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|10312| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+				 *   - arch/x86/kvm/x86.c|11956| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/xen.c|58| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+				 *
+				 * 处理的函数kvm_gen_update_masterclock().
+				 */
 				kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 			}
 
@@ -11742,6 +12461,16 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	raw_spin_lock_init(&kvm->arch.tsc_write_lock);
 	mutex_init(&kvm->arch.apic_map_lock);
+	/*
+	 *  在以下使用kvm_arch->pvclock_gtod_sync_lock:
+	 *   - kvm_synchronize_tsc()
+	 *   - kvm_gen_update_masterclock()
+	 *   - get_kvmclock_ns()
+	 *   - kvm_guest_time_update()
+	 *   - kvm_arch_vm_ioctl(KVM_SET_CLOCK)
+	 *   - kvm_hyperv_tsc_notifier()
+	 *   - kvm_arch_init_vm()
+	 */
 	raw_spin_lock_init(&kvm->arch.pvclock_gtod_sync_lock);
 
 	kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
@@ -11754,7 +12483,21 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.hv_root_tdp = INVALID_PAGE;
 #endif
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3136| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3152| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11789| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11838| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3233| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|11489| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|11913| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|11960| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
 
 	kvm_apicv_init(kvm);
@@ -11802,7 +12545,21 @@ static void kvm_free_vcpus(struct kvm *kvm)
 
 void kvm_arch_sync_events(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3233| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|11489| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|11913| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|11960| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3136| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3152| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11789| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11838| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
 	kvm_free_pit(kvm);
 }
@@ -12119,6 +12876,10 @@ static void kvm_mmu_update_cpu_dirty_logging(struct kvm *kvm, bool enable)
 	WARN_ON_ONCE(ka->cpu_dirty_logging_count < 0);
 }
 
+/*
+ * 在以下使用kvm_mmu_slot_apply_flags():
+ *   - arch/x86/kvm/x86.c|12759| <<kvm_arch_commit_memory_region>> kvm_mmu_slot_apply_flags(kvm, old, new, change);
+ */
 static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 				     struct kvm_memory_slot *old,
 				     const struct kvm_memory_slot *new,
@@ -12175,6 +12936,15 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 		 * which can be collapsed into a single large-page spte.  Later
 		 * page faults will create the large-page sptes.
 		 */
+		/*
+		 * 在以下使用kvm_dirty_log_pgtable():
+		 *   - arch/x86/kvm/mmu/mmu.c|899| <<gfn_to_memslot_dirty_bitmap>> !kvm_dirty_log_pgtable(vcpu->kvm))
+		 *   - arch/x86/kvm/mmu/mmu.c|1388| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> !kvm_dirty_log_pgtable(kvm)) {
+		 *   - arch/x86/kvm/mmu/mmu.c|1401| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> if (kvm_dirty_log_pgtable(kvm) &&
+		 *   - arch/x86/kvm/x86.c|6114| <<kvm_arch_sync_dirty_log>> if (kvm_dirty_log_pgtable(kvm)) {
+		 *   - arch/x86/kvm/x86.c|12631| <<kvm_mmu_slot_apply_flags>> if (!kvm_dirty_log_pgtable(kvm))
+		 *   - virt/kvm/kvm_main.c|2235| <<kvm_clear_dirty_log_protect>> if (!kvm_dirty_log_pgtable(kvm))
+		 */
 	        if (!kvm_dirty_log_pgtable(kvm))
 			kvm_mmu_zap_collapsible_sptes(kvm, new);
 	} else {
@@ -12194,6 +12964,10 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 	}
 }
 
+/*
+ * 在以下使用kvm_arch_commit_memory_region():
+ *   - virt/kvm/kvm_main.c|1639| <<kvm_commit_memory_region>> kvm_arch_commit_memory_region(kvm, old, new, change);
+ */
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *old,
 				const struct kvm_memory_slot *new,
@@ -12208,6 +12982,9 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 		kvm_mmu_change_mmu_pages(kvm, nr_mmu_pages);
 	}
 
+        /*
+	 * 只在这里调用
+	 */
 	kvm_mmu_slot_apply_flags(kvm, old, new, change);
 
 	/* Free the arrays associated with the old memslot. */
@@ -12308,6 +13085,12 @@ bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)
 
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_exiting_guest_mode():
+	 *   - arch/arm64/kvm/arm.c|68| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+	 *   - arch/x86/kvm/x86.c|12918| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+	 *   - virt/kvm/kvm_main.c|227| <<kvm_request_needs_ipi>> int mode = kvm_vcpu_exiting_guest_mode(vcpu);
+	 */
 	return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
 }
 
@@ -12451,6 +13234,15 @@ static inline int apf_put_user_notpresent(struct kvm_vcpu *vcpu)
 {
 	u32 reason = KVM_PV_REASON_PAGE_NOT_PRESENT;
 
+	/*
+	 * 在以下使用kvm_write_guest_cached():
+	 *   - arch/x86/kvm/lapic.c|727| <<pv_eoi_put_user>> return kvm_write_guest_cached(vcpu->kvm,
+	 *          &vcpu->arch.pv_eoi.data, &val, sizeof(val));
+	 *   - arch/x86/kvm/lapic.c|2922| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm,
+	 *          &vcpu->arch.apic->vapic_cache, &data, sizeof(u32));
+	 *   - arch/x86/kvm/x86.c|12454| <<apf_put_user_notpresent>> return kvm_write_guest_cached(vcpu->kvm,
+	 *          &vcpu->arch.apf.data, &reason, sizeof(reason));
+	 */
 	return kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apf.data, &reason,
 				      sizeof(reason));
 }
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index f7854e742e8c..eaa7f54de656 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -33,6 +33,11 @@ static __always_inline void kvm_guest_enter_irqoff(void)
 	lockdep_hardirqs_on(CALLER_ADDR0);
 }
 
+/*
+ * 在以下使用kvm_guest_exit_irqoff():
+ *   - arch/x86/kvm/svm/svm.c|4110| <<svm_vcpu_enter_exit>> kvm_guest_exit_irqoff();
+ *   - arch/x86/kvm/vmx/vmx.c|6873| <<vmx_vcpu_enter_exit>> kvm_guest_exit_irqoff();
+ */
 static __always_inline void kvm_guest_exit_irqoff(void)
 {
 	/*
@@ -351,6 +356,15 @@ extern bool report_ignored_msrs;
 
 static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 {
+	/*
+	 * 在以下使用pvclock_scale_delta():
+	 *   - arch/x86/include/asm/pvclock.h|85| <<__pvclock_read_cycles>> u64 offset = pvclock_scale_delta(delta,
+	 *          src->tsc_to_system_mul, src->tsc_shift);
+	 *   - arch/x86/kvm/x86.c|2375| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *          vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|354| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+	 *          vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 */
 	return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
 				   vcpu->arch.virtual_tsc_shift);
 }
diff --git a/arch/x86/xen/time.c b/arch/x86/xen/time.c
index 9ef0a5cca96e..f1e1a4bed402 100644
--- a/arch/x86/xen/time.c
+++ b/arch/x86/xen/time.c
@@ -50,6 +50,12 @@ static u64 xen_clocksource_read(void)
 
 	preempt_disable_notrace();
 	src = &__this_cpu_read(xen_vcpu)->time;
+	/*
+	 * 在以下使用pvclock_clocksource_read():
+	 *   - arch/x86/kernel/kvmclock.c|79| <<kvm_clock_read>> ret = pvclock_clocksource_read(this_cpu_pvti());
+	 *   - arch/x86/kernel/pvclock.c|161| <<pvclock_read_wallclock>> delta = pvclock_clocksource_read(vcpu_time);
+	 *   - arch/x86/xen/time.c|53| <<xen_clocksource_read>> ret = pvclock_clocksource_read(src);
+	 */
 	ret = pvclock_clocksource_read(src);
 	preempt_enable_notrace();
 	return ret;
diff --git a/drivers/infiniband/core/cq.c b/drivers/infiniband/core/cq.c
index 433b426729d4..adcae69c1e59 100644
--- a/drivers/infiniband/core/cq.c
+++ b/drivers/infiniband/core/cq.c
@@ -90,6 +90,12 @@ static int __poll_cq(struct ib_cq *cq, int num_entries, struct ib_wc *wc)
 	return rc;
 }
 
+/*
+ * 在以下使用__ib_process_cq():
+ *   - drivers/infiniband/core/cq.c|143| <<ib_process_cq_direct>> return __ib_process_cq(cq, budget, wcs, IB_POLL_BATCH_DIRECT);
+ *   - drivers/infiniband/core/cq.c|158| <<ib_poll_handler>> completed = __ib_process_cq(cq, budget, cq->wc, IB_POLL_BATCH);
+ *   - drivers/infiniband/core/cq.c|184| <<ib_cq_poll_work>> completed = __ib_process_cq(cq, IB_POLL_BUDGET_WORKQUEUE, cq->wc, IB_POLL_BATCH);
+ */
 static int __ib_process_cq(struct ib_cq *cq, int budget, struct ib_wc *wcs,
 			   int batch)
 {
diff --git a/drivers/infiniband/core/device.c b/drivers/infiniband/core/device.c
index 8e0158c816fa..0890f38ef351 100644
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@ -1377,6 +1377,25 @@ static void prevent_dealloc_device(struct ib_device *ib_dev)
  * asynchronously then the device pointer may become freed as soon as this
  * function returns.
  */
+/*
+ * 在以下使用ib_register_device():
+ *   - drivers/infiniband/hw/bnxt_re/main.c|594| <<bnxt_re_register_ib>> return ib_register_device(ibdev, "bnxt_re%d", &rdev->en_dev->pdev->dev);
+ *   - drivers/infiniband/hw/cxgb4/provider.c|550| <<c4iw_register_device>> ret = ib_register_device(&dev->ibdev, "cxgb4_%d",
+ *   - drivers/infiniband/hw/efa/efa_main.c|314| <<efa_ib_device_add>> err = ib_register_device(&dev->ibdev, "efa_%d", &pdev->dev); 
+ *   - drivers/infiniband/hw/hns/hns_roce_main.c|548| <<hns_roce_register_device>> ret = ib_register_device(ib_dev, "hns_%d", dev);
+ *   - drivers/infiniband/hw/irdma/verbs.c|4531| <<irdma_ib_register_device>> ret = ib_register_device(&iwdev->ibdev, "irdma%d", iwdev->rf->hw.device);
+ *   - drivers/infiniband/hw/mana/device.c|81| <<mana_ib_probe>> ret = ib_register_device(&dev->ib_dev, "mana_%d",
+ *   - drivers/infiniband/hw/mlx4/main.c|2936| <<mlx4_ib_probe>> err = ib_register_device(&ibdev->ib_dev, "mlx4_%d",
+ *   - drivers/infiniband/hw/mlx5/main.c|4258| <<mlx5_ib_stage_ib_reg_init>> return ib_register_device(&dev->ib_dev, name, &dev->mdev->pdev->dev);
+ *   - drivers/infiniband/hw/mthca/mthca_provider.c|1174| <<mthca_register_device>> ret = ib_register_device(&dev->ib_dev, "mthca%d", &dev->pdev->dev);
+ *   - drivers/infiniband/hw/ocrdma/ocrdma_main.c|228| <<ocrdma_register_device>> return ib_register_device(&dev->ibdev, "ocrdma%d",
+ *   - drivers/infiniband/hw/qedr/main.c|267| <<qedr_register_device>> return ib_register_device(&dev->ibdev, "qedr%d", &dev->pdev->dev);
+ *   - drivers/infiniband/hw/usnic/usnic_ib_main.c|410| <<usnic_ib_device_add>> if (ib_register_device(&us_ibdev->ib_dev, "usnic_%d", &dev->dev))
+ *   - drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c|246| <<pvrdma_register_device>> ret = ib_register_device(&dev->ib_dev, "vmw_pvrdma%d", &dev->pdev->dev);
+ *   - drivers/infiniband/sw/rdmavt/vt.c|556| <<rvt_register_device>> ret = ib_register_device(&rdi->ibdev, dev_name(&rdi->ibdev.dev), NULL);
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|1213| <<rxe_register_device>> err = ib_register_device(dev, ibdev_name, NULL);
+ *   - drivers/infiniband/sw/siw/siw_main.c|72| <<siw_device_register>> rv = ib_register_device(base_dev, name, NULL);
+ */
 int ib_register_device(struct ib_device *device, const char *name,
 		       struct device *dma_device)
 {
diff --git a/drivers/infiniband/core/nldev.c b/drivers/infiniband/core/nldev.c
index e557a7a70c40..8fe7042fa5cf 100644
--- a/drivers/infiniband/core/nldev.c
+++ b/drivers/infiniband/core/nldev.c
@@ -1657,6 +1657,11 @@ static const struct rdma_link_ops *link_ops_get(const char *type)
 	return ops;
 }
 
+/*
+ * 在以下使用rdma_link_register():
+ *   - drivers/infiniband/sw/rxe/rxe.c|294| <<rxe_module_init>> rdma_link_register(&rxe_link_ops);
+ *   - drivers/infiniband/sw/siw/siw_main.c|577| <<siw_init_module>> rdma_link_register(&siw_link_ops);
+ */
 void rdma_link_register(struct rdma_link_ops *ops)
 {
 	down_write(&link_ops_rwsem);
diff --git a/drivers/infiniband/sw/rxe/rxe.c b/drivers/infiniband/sw/rxe/rxe.c
index f6ef782ce75c..196b9095c1c4 100644
--- a/drivers/infiniband/sw/rxe/rxe.c
+++ b/drivers/infiniband/sw/rxe/rxe.c
@@ -13,6 +13,16 @@ MODULE_AUTHOR("Bob Pearson, Frank Zago, John Groves, Kamal Heib");
 MODULE_DESCRIPTION("Soft RDMA transport");
 MODULE_LICENSE("Dual BSD/GPL");
 
+/*
+ * tcpdump -i ens3 udp port 4791
+ */
+
+/*
+ * 在以下使用rxe_initialized:
+ *   - drivers/infiniband/sw/rxe/rxe.c|295| <<rxe_module_init>> rxe_initialized = true;
+ *   - drivers/infiniband/sw/rxe/rxe.c|306| <<rxe_module_exit>> rxe_initialized = false;
+ *   - drivers/infiniband/sw/rxe/rxe_sysfs.c|37| <<rxe_param_set_add>> if (!rxe_initialized) {
+ */
 bool rxe_initialized;
 
 /* free resources for a rxe device all objects created for this device must
@@ -220,6 +230,11 @@ static int rxe_init(struct rxe_dev *rxe)
 	return 0;
 }
 
+/*
+ * 在以下使用rxe_set_mtu():
+ *   - drivers/infiniband/sw/rxe/rxe.c|248| <<rxe_add>> rxe_set_mtu(rxe, mtu);
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|645| <<rxe_notify>> rxe_set_mtu(rxe, ndev->mtu);
+ */
 void rxe_set_mtu(struct rxe_dev *rxe, unsigned int ndev_mtu)
 {
 	struct rxe_port *port = &rxe->port;
@@ -237,6 +252,10 @@ void rxe_set_mtu(struct rxe_dev *rxe, unsigned int ndev_mtu)
 /* called by ifc layer to create new rxe device.
  * The caller should allocate memory for rxe by calling ib_alloc_device.
  */
+/*
+ * 在以下使用rxe_add():
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|569| <<rxe_net_add>> err = rxe_add(rxe, ndev->mtu, ibdev_name);
+ */
 int rxe_add(struct rxe_dev *rxe, unsigned int mtu, const char *ibdev_name)
 {
 	int err;
@@ -245,6 +264,11 @@ int rxe_add(struct rxe_dev *rxe, unsigned int mtu, const char *ibdev_name)
 	if (err)
 		return err;
 
+	/*
+	 * 在以下使用rxe_set_mtu():
+	 *   - drivers/infiniband/sw/rxe/rxe.c|248| <<rxe_add>> rxe_set_mtu(rxe, mtu);
+	 *   - drivers/infiniband/sw/rxe/rxe_net.c|645| <<rxe_notify>> rxe_set_mtu(rxe, ndev->mtu);
+	 */
 	rxe_set_mtu(rxe, mtu);
 
 	return rxe_register_device(rxe, ibdev_name);
@@ -291,6 +315,11 @@ static int __init rxe_module_init(void)
 	if (err)
 		return err;
 
+	/*
+	 * 在以下使用rdma_link_register():
+	 *   - drivers/infiniband/sw/rxe/rxe.c|294| <<rxe_module_init>> rdma_link_register(&rxe_link_ops);
+	 *   - drivers/infiniband/sw/siw/siw_main.c|577| <<siw_init_module>> rdma_link_register(&siw_link_ops);
+	 */
 	rdma_link_register(&rxe_link_ops);
 	rxe_initialized = true;
 	pr_info("loaded\n");
diff --git a/drivers/infiniband/sw/rxe/rxe_comp.c b/drivers/infiniband/sw/rxe/rxe_comp.c
index 250494306932..bf998cd1e669 100644
--- a/drivers/infiniband/sw/rxe/rxe_comp.c
+++ b/drivers/infiniband/sw/rxe/rxe_comp.c
@@ -133,6 +133,10 @@ void rxe_comp_queue_pkt(struct rxe_qp *qp, struct sk_buff *skb)
 	rxe_run_task(&qp->comp.task, must_sched);
 }
 
+/*
+ * 在以下使用get_wqe():
+ *   - drivers/infiniband/sw/rxe/rxe_comp.c|600| <<rxe_completer>> state = get_wqe(qp, pkt, &wqe);
+ */
 static inline enum comp_state get_wqe(struct rxe_qp *qp,
 				      struct rxe_pkt_info *pkt,
 				      struct rxe_send_wqe **wqe_p)
@@ -597,6 +601,9 @@ int rxe_completer(void *arg)
 			break;
 
 		case COMPST_GET_WQE:
+			/*
+			 * 只在这里调用
+			 */
 			state = get_wqe(qp, pkt, &wqe);
 			break;
 
diff --git a/drivers/infiniband/sw/rxe/rxe_cq.c b/drivers/infiniband/sw/rxe/rxe_cq.c
index f22f8e950bae..55e10e3c76d4 100644
--- a/drivers/infiniband/sw/rxe/rxe_cq.c
+++ b/drivers/infiniband/sw/rxe/rxe_cq.c
@@ -54,6 +54,10 @@ static void rxe_send_complete(struct tasklet_struct *t)
 	cq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context);
 }
 
+/*
+ * 在以下使用rxe_cq_from_init():
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|787| <<rxe_create_cq>> err = rxe_cq_from_init(rxe, cq, attr->cqe, attr->comp_vector, udata,
+ */
 int rxe_cq_from_init(struct rxe_dev *rxe, struct rxe_cq *cq, int cqe,
 		     int comp_vector, struct ib_udata *udata,
 		     struct rxe_create_cq_resp __user *uresp)
@@ -62,6 +66,14 @@ int rxe_cq_from_init(struct rxe_dev *rxe, struct rxe_cq *cq, int cqe,
 	enum queue_type type;
 
 	type = QUEUE_TYPE_TO_CLIENT;
+	/*
+	 * 在以下使用rxe_queue_init():
+	 *   - drivers/infiniband/sw/rxe/rxe_cq.c|65| <<rxe_cq_from_init>> cq->queue = rxe_queue_init(rxe, &cqe, sizeof(struct rxe_cqe), type);
+	 *   - drivers/infiniband/sw/rxe/rxe_qp.c|246| <<rxe_qp_init_req>> qp->sq.queue = rxe_queue_init(rxe, &qp->sq.max_wr, wqe_size, type);
+	 *   - drivers/infiniband/sw/rxe/rxe_qp.c|299| <<rxe_qp_init_resp>> qp->rq.queue = rxe_queue_init(rxe, &qp->rq.max_wr, wqe_size, type);
+	 *   - drivers/infiniband/sw/rxe/rxe_queue.c|158| <<rxe_queue_resize>> new_q = rxe_queue_init(q->rxe, &num_elem, elem_size, q->type);
+	 *   - drivers/infiniband/sw/rxe/rxe_srq.c|96| <<rxe_srq_from_init>> q = rxe_queue_init(rxe, &srq->rq.max_wr, srq_wqe_size, type);
+	 */
 	cq->queue = rxe_queue_init(rxe, &cqe,
 			sizeof(struct rxe_cqe), type);
 	if (!cq->queue) {
diff --git a/drivers/infiniband/sw/rxe/rxe_mr.c b/drivers/infiniband/sw/rxe/rxe_mr.c
index a408857a76c5..dc4db744a8ba 100644
--- a/drivers/infiniband/sw/rxe/rxe_mr.c
+++ b/drivers/infiniband/sw/rxe/rxe_mr.c
@@ -43,6 +43,12 @@ int mr_check_range(struct rxe_mr *mr, u64 iova, size_t length)
 				| IB_ACCESS_REMOTE_WRITE	\
 				| IB_ACCESS_REMOTE_ATOMIC)
 
+/*
+ * 在以下使用rxe_mr_init():
+ *   - drivers/infiniband/sw/rxe/rxe_mr.c|104| <<rxe_mr_init_dma>> rxe_mr_init(access, mr);
+ *   - drivers/infiniband/sw/rxe/rxe_mr.c|134| <<rxe_mr_init_user>> rxe_mr_init(access, mr);
+ *   - drivers/infiniband/sw/rxe/rxe_mr.c|201| <<rxe_mr_init_fast>> rxe_mr_init(IB_ACCESS_REMOTE, mr);
+ */
 static void rxe_mr_init(int access, struct rxe_mr *mr)
 {
 	u32 lkey = mr->pelem.index << 8 | rxe_get_next_key(-1);
@@ -101,6 +107,12 @@ static int rxe_mr_alloc(struct rxe_mr *mr, int num_buf)
 
 void rxe_mr_init_dma(struct rxe_pd *pd, int access, struct rxe_mr *mr)
 {
+	/*
+	 * 在以下使用rxe_mr_init():
+	 *   - drivers/infiniband/sw/rxe/rxe_mr.c|104| <<rxe_mr_init_dma>> rxe_mr_init(access, mr);
+	 *   - drivers/infiniband/sw/rxe/rxe_mr.c|134| <<rxe_mr_init_user>> rxe_mr_init(access, mr);
+	 *   - drivers/infiniband/sw/rxe/rxe_mr.c|201| <<rxe_mr_init_fast>> rxe_mr_init(IB_ACCESS_REMOTE, mr);
+	 */
 	rxe_mr_init(access, mr);
 
 	mr->ibmr.pd = &pd->ibpd;
@@ -131,6 +143,12 @@ int rxe_mr_init_user(struct rxe_pd *pd, u64 start, u64 length, u64 iova,
 
 	num_buf = ib_umem_num_pages(umem);
 
+	/*
+	 * 在以下使用rxe_mr_init():
+	 *   - drivers/infiniband/sw/rxe/rxe_mr.c|104| <<rxe_mr_init_dma>> rxe_mr_init(access, mr);
+	 *   - drivers/infiniband/sw/rxe/rxe_mr.c|134| <<rxe_mr_init_user>> rxe_mr_init(access, mr);
+	 *   - drivers/infiniband/sw/rxe/rxe_mr.c|201| <<rxe_mr_init_fast>> rxe_mr_init(IB_ACCESS_REMOTE, mr);
+	 */
 	rxe_mr_init(access, mr);
 
 	err = rxe_mr_alloc(mr, num_buf);
@@ -197,6 +215,12 @@ int rxe_mr_init_fast(struct rxe_pd *pd, int max_pages, struct rxe_mr *mr)
 {
 	int err;
 
+	/*
+	 * 在以下使用rxe_mr_init():
+	 *   - drivers/infiniband/sw/rxe/rxe_mr.c|104| <<rxe_mr_init_dma>> rxe_mr_init(access, mr);
+	 *   - drivers/infiniband/sw/rxe/rxe_mr.c|134| <<rxe_mr_init_user>> rxe_mr_init(access, mr);
+	 *   - drivers/infiniband/sw/rxe/rxe_mr.c|201| <<rxe_mr_init_fast>> rxe_mr_init(IB_ACCESS_REMOTE, mr);
+	 */
 	/* always allow remote access for FMRs */
 	rxe_mr_init(IB_ACCESS_REMOTE, mr);
 
diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index be86b879a0d5..08dd65c6ae20 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -42,6 +42,10 @@ int rxe_mcast_delete(struct rxe_dev *rxe, union ib_gid *mgid)
 	return err;
 }
 
+/*
+ * 在以下使用rxe_find_route4():
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|130| <<rxe_find_route>> dst = rxe_find_route4(ndev, saddr, daddr);
+ */
 static struct dst_entry *rxe_find_route4(struct net_device *ndev,
 				  struct in_addr *saddr,
 				  struct in_addr *daddr)
@@ -108,6 +112,11 @@ static struct dst_entry *rxe_find_route6(struct net_device *ndev,
 
 #endif
 
+/*
+ * 在以下使用rxe_find_route():
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|335| <<prepare4>> dst = rxe_find_route(skb->dev, qp, av);
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|359| <<prepare6>> dst = rxe_find_route(skb->dev, qp, av);
+ */
 static struct dst_entry *rxe_find_route(struct net_device *ndev,
 					struct rxe_qp *qp,
 					struct rxe_av *av)
@@ -127,6 +136,9 @@ static struct dst_entry *rxe_find_route(struct net_device *ndev,
 
 			saddr = &av->sgid_addr._sockaddr_in.sin_addr;
 			daddr = &av->dgid_addr._sockaddr_in.sin_addr;
+			/*
+			 * 只在这里调用
+			 */
 			dst = rxe_find_route4(ndev, saddr, daddr);
 		} else if (av->network_type == RXE_NETWORK_TYPE_IPV6) {
 			struct in6_addr *saddr6;
@@ -150,6 +162,10 @@ static struct dst_entry *rxe_find_route(struct net_device *ndev,
 	return dst;
 }
 
+/*
+ * 在以下使用rxe_udp_encap_recv():
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|217| <<rxe_setup_udp_tunnel>> tnl_cfg.encap_rcv = rxe_udp_encap_recv;
+ */
 static int rxe_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
 {
 	struct udphdr *udph;
@@ -191,6 +207,13 @@ static int rxe_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
 	return 0;
 }
 
+/*
+ * 在以下使用rxe_setup_udp_tunnel():
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|671| <<rxe_net_ipv4_init>> recv_sockets.sk4 =
+ *        rxe_setup_udp_tunnel(&init_net, htons(ROCE_V2_UDP_DPORT), false);
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|686| <<rxe_net_ipv6_init>> recv_sockets.sk6 =
+ *        rxe_setup_udp_tunnel(&init_net, htons(ROCE_V2_UDP_DPORT), true);
+ */
 static struct socket *rxe_setup_udp_tunnel(struct net *net, __be16 port,
 					   bool ipv6)
 {
@@ -208,6 +231,21 @@ static struct socket *rxe_setup_udp_tunnel(struct net *net, __be16 port,
 
 	udp_cfg.local_udp_port = port;
 
+	/*
+	 * 在以下使用udp_sock_create():
+	 *   - drivers/infiniband/sw/rxe/rxe_net.c|216| <<rxe_setup_udp_tunnel>> err = udp_sock_create(net, &udp_cfg, &sock);
+	 *   - drivers/net/bareudp.c|241| <<bareudp_create_sock>> err = udp_sock_create(net, &udp_conf, &sock);
+	 *   - drivers/net/geneve.c|493| <<geneve_create_sock>> err = udp_sock_create(net, &udp_conf, &sock);
+	 *   - drivers/net/vxlan/vxlan_core.c|3479| <<vxlan_create_sock>> err = udp_sock_create(net, &udp_conf, &sock);
+	 *   - drivers/net/wireguard/socket.c|387| <<wg_socket_init>> ret = udp_sock_create(net, &port4, &new4);
+	 *   - drivers/net/wireguard/socket.c|398| <<wg_socket_init>> ret = udp_sock_create(net, &port6, &new6);
+	 *   - net/ipv4/fou.c|582| <<fou_create>> err = udp_sock_create(net, &cfg->udp_config, &sock);
+	 *   - net/l2tp/l2tp_core.c|1324| <<l2tp_tunnel_sock_create>> err = udp_sock_create(net, &udp_conf, &sock);
+	 *   - net/rxrpc/local_object.c|132| <<rxrpc_open_socket>> ret = udp_sock_create(net, &udp_conf, &local->socket);
+	 *   - net/sctp/protocol.c|869| <<sctp_udp_sock_start>> err = udp_sock_create(net, &udp_conf, &sock);
+	 *   - net/sctp/protocol.c|889| <<sctp_udp_sock_start>> err = udp_sock_create(net, &udp_conf, &sock);
+	 *   - net/tipc/udp_media.c|774| <<tipc_udp_enable>> err = udp_sock_create(net, &udp_conf, &ub->ubsock);
+	 */
 	/* Create UDP socket */
 	err = udp_sock_create(net, &udp_cfg, &sock);
 	if (err < 0)
@@ -216,6 +254,22 @@ static struct socket *rxe_setup_udp_tunnel(struct net *net, __be16 port,
 	tnl_cfg.encap_type = 1;
 	tnl_cfg.encap_rcv = rxe_udp_encap_recv;
 
+	/*
+	 * 在以下使用setup_udp_tunnel_sock():
+	 *   - drivers/infiniband/sw/rxe/rxe_net.c|258| <<rxe_setup_udp_tunnel>> setup_udp_tunnel_sock(net, sock, &tnl_cfg);
+	 *   - drivers/net/bareudp.c|266| <<bareudp_socket_create>> setup_udp_tunnel_sock(bareudp->net, sock, &tunnel_cfg);
+	 *   - drivers/net/geneve.c|626| <<geneve_socket_create>> setup_udp_tunnel_sock(net, sock, &tunnel_cfg);
+	 *   - drivers/net/gtp.c|845| <<gtp_encap_enable_socket>> setup_udp_tunnel_sock(sock_net(sock->sk), sock, &tuncfg);
+	 *   - drivers/net/vxlan/vxlan_core.c|3533| <<vxlan_socket_create>> setup_udp_tunnel_sock(net, sock, &tunnel_cfg);
+	 *   - drivers/net/wireguard/socket.c|393| <<wg_socket_init>> setup_udp_tunnel_sock(net, new4, &cfg);
+	 *   - drivers/net/wireguard/socket.c|408| <<wg_socket_init>> setup_udp_tunnel_sock(net, new6, &cfg);
+	 *   - net/ipv4/fou.c|624| <<fou_create>> setup_udp_tunnel_sock(net, sock, &tunnel_cfg);
+	 *   - net/l2tp/l2tp_core.c|1512| <<l2tp_tunnel_register>> setup_udp_tunnel_sock(net, sock, &udp_cfg);
+	 *   - net/rxrpc/local_object.c|142| <<rxrpc_open_socket>> setup_udp_tunnel_sock(net, local->socket, &tuncfg);
+	 *   - net/sctp/protocol.c|878| <<sctp_udp_sock_start>> setup_udp_tunnel_sock(net, sock, &tuncfg);
+	 *   - net/sctp/protocol.c|900| <<sctp_udp_sock_start>> setup_udp_tunnel_sock(net, sock, &tuncfg);
+	 *   - net/tipc/udp_media.c|781| <<tipc_udp_enable>> setup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);
+	 */
 	/* Setup UDP tunnel */
 	setup_udp_tunnel_sock(net, sock, &tnl_cfg);
 
@@ -296,6 +350,10 @@ static void prepare_ipv6_hdr(struct dst_entry *dst, struct sk_buff *skb,
 	ip6h->payload_len = htons(skb->len - sizeof(*ip6h));
 }
 
+/*
+ * 在以下使用prepare4():
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|381| <<rxe_prepare>> err = prepare4(pkt, skb);
+ */
 static int prepare4(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 {
 	struct rxe_qp *qp = pkt->qp;
@@ -306,6 +364,11 @@ static int prepare4(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 	struct in_addr *saddr = &av->sgid_addr._sockaddr_in.sin_addr;
 	struct in_addr *daddr = &av->dgid_addr._sockaddr_in.sin_addr;
 
+	/*
+	 * 在以下使用rxe_find_route():
+	 *   - drivers/infiniband/sw/rxe/rxe_net.c|335| <<prepare4>> dst = rxe_find_route(skb->dev, qp, av);
+	 *   - drivers/infiniband/sw/rxe/rxe_net.c|359| <<prepare6>> dst = rxe_find_route(skb->dev, qp, av);
+	 */
 	dst = rxe_find_route(skb->dev, qp, av);
 	if (!dst) {
 		pr_err("Host not reachable\n");
@@ -330,6 +393,11 @@ static int prepare6(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 	struct in6_addr *saddr = &av->sgid_addr._sockaddr_in6.sin6_addr;
 	struct in6_addr *daddr = &av->dgid_addr._sockaddr_in6.sin6_addr;
 
+	/*
+	 * 在以下使用rxe_find_route():
+	 *   - drivers/infiniband/sw/rxe/rxe_net.c|335| <<prepare4>> dst = rxe_find_route(skb->dev, qp, av);
+	 *   - drivers/infiniband/sw/rxe/rxe_net.c|359| <<prepare6>> dst = rxe_find_route(skb->dev, qp, av);
+	 */
 	dst = rxe_find_route(skb->dev, qp, av);
 	if (!dst) {
 		pr_err("Host not reachable\n");
@@ -347,6 +415,11 @@ static int prepare6(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 	return 0;
 }
 
+/*
+ * 在以下使用rxe_prepare():
+ *   - drivers/infiniband/sw/rxe/rxe_req.c|461| <<finish_packet>> err = rxe_prepare(pkt, skb);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|654| <<prepare_ack_packet>> err = rxe_prepare(ack, skb);
+ */
 int rxe_prepare(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 {
 	int err = 0;
@@ -375,6 +448,10 @@ static void rxe_skb_tx_dtor(struct sk_buff *skb)
 	rxe_drop_ref(qp);
 }
 
+/*
+ * 在以下使用rxe_send():
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|451| <<rxe_xmit_packet>> err = rxe_send(skb, pkt);
+ */
 static int rxe_send(struct sk_buff *skb, struct rxe_pkt_info *pkt)
 {
 	int err;
@@ -386,6 +463,23 @@ static int rxe_send(struct sk_buff *skb, struct rxe_pkt_info *pkt)
 	atomic_inc(&pkt->qp->skb_out);
 
 	if (skb->protocol == htons(ETH_P_IP)) {
+		/*
+		 * 在以下使用ip_local_out():
+		 *   - net/ipv4/ip_output.c|130| <<global>> EXPORT_SYMBOL_GPL(ip_local_out);
+		 *   - drivers/infiniband/sw/rxe/rxe_net.c|450| <<rxe_send>> err = ip_local_out(dev_net(skb_dst(skb)->dev), skb->sk, skb);
+		 *   - drivers/net/ipvlan/ipvlan_core.c|443| <<ipvlan_process_v4_outbound>> err = ip_local_out(net, NULL, skb);
+		 *   - drivers/net/ppp/pptp.c|261| <<pptp_xmit>> ip_local_out(net, skb->sk, skb);
+		 *   - net/ipv4/igmp.c|427| <<igmpv3_sendpack>> return ip_local_out(dev_net(skb_dst(skb)->dev), skb->sk, skb);
+		 *   - net/ipv4/igmp.c|796| <<igmp_send_report>> return ip_local_out(net, skb->sk, skb);
+		 *   - net/ipv4/ip_output.c|190| <<ip_build_and_send_pkt>> return ip_local_out(net, skb->sk, skb);
+		 *   - net/ipv4/ip_output.c|532| <<__ip_queue_xmit>> res = ip_local_out(net, sk, skb);
+		 *   - net/ipv4/ip_output.c|1581| <<ip_send_skb>> err = ip_local_out(net, skb->sk, skb);
+		 *   - net/ipv4/ip_tunnel_core.c|82| <<iptunnel_xmit>> err = ip_local_out(net, sk, skb);
+		 *   - net/ipv4/netfilter/nf_dup_ipv4.c|89| <<nf_dup_ipv4>> ip_local_out(net, skb->sk, skb);
+		 *   - net/ipv4/netfilter/nf_reject_ipv4.c|303| <<nf_send_reset>> ip_local_out(net, nskb->sk, nskb);
+		 *   - net/netfilter/ipvs/ip_vs_xmit.c|1267| <<ip_vs_tunnel_xmit>> ip_local_out(net, skb->sk, skb);
+		 *   - net/netfilter/nf_synproxy_core.c|461| <<synproxy_send_tcp>> ip_local_out(net, nskb->sk, nskb);
+		 */
 		err = ip_local_out(dev_net(skb_dst(skb)->dev), skb->sk, skb);
 	} else if (skb->protocol == htons(ETH_P_IPV6)) {
 		err = ip6_local_out(dev_net(skb_dst(skb)->dev), skb->sk, skb);
@@ -408,6 +502,10 @@ static int rxe_send(struct sk_buff *skb, struct rxe_pkt_info *pkt)
 /* fix up a send packet to match the packets
  * received from UDP before looping them back
  */
+/*
+ * 在以下使用rxe_loopback():
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|449| <<rxe_xmit_packet>> err = rxe_loopback(skb, pkt);
+ */
 static int rxe_loopback(struct sk_buff *skb, struct rxe_pkt_info *pkt)
 {
 	memcpy(SKB_TO_PKT(skb), pkt, sizeof(*pkt));
@@ -430,6 +528,14 @@ static int rxe_loopback(struct sk_buff *skb, struct rxe_pkt_info *pkt)
 	return 0;
 }
 
+/*
+ * 在以下使用rxe_xmit_packet():
+ *   - drivers/infiniband/sw/rxe/rxe_req.c|742| <<rxe_requester>> ret = rxe_xmit_packet(qp, &pkt, skb);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|731| <<read_reply>> err = rxe_xmit_packet(qp, &ack_pkt, skb);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|958| <<send_ack>> err = rxe_xmit_packet(qp, &ack_pkt, skb);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|993| <<send_atomic_ack>> rc = rxe_xmit_packet(qp, &ack_pkt, skb);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|1122| <<duplicate_request>> rc = rxe_xmit_packet(qp, pkt, res->atomic.skb);
+ */
 int rxe_xmit_packet(struct rxe_qp *qp, struct rxe_pkt_info *pkt,
 		    struct sk_buff *skb)
 {
@@ -622,6 +728,11 @@ static int rxe_notify(struct notifier_block *not_blk,
 		break;
 	case NETDEV_CHANGEMTU:
 		pr_info("%s changed mtu to %d\n", ndev->name, ndev->mtu);
+		/*
+		 * 在以下使用rxe_set_mtu():
+		 *   - drivers/infiniband/sw/rxe/rxe.c|248| <<rxe_add>> rxe_set_mtu(rxe, mtu);
+		 *   - drivers/infiniband/sw/rxe/rxe_net.c|645| <<rxe_notify>> rxe_set_mtu(rxe, ndev->mtu);
+		 */
 		rxe_set_mtu(rxe, ndev->mtu);
 		break;
 	case NETDEV_CHANGE:
@@ -687,6 +798,10 @@ void rxe_net_exit(void)
 	unregister_netdevice_notifier(&rxe_net_notifier);
 }
 
+/*
+ * 在以下使用rxe_net_init():
+ *   - drivers/infiniband/sw/rxe/rxe.c|290| <<rxe_module_init>> err = rxe_net_init();
+ */
 int rxe_net_init(void)
 {
 	int err;
diff --git a/drivers/infiniband/sw/rxe/rxe_qp.c b/drivers/infiniband/sw/rxe/rxe_qp.c
index 13b237d93a61..f63a5d6c842a 100644
--- a/drivers/infiniband/sw/rxe/rxe_qp.c
+++ b/drivers/infiniband/sw/rxe/rxe_qp.c
@@ -243,6 +243,14 @@ static int rxe_qp_init_req(struct rxe_dev *rxe, struct rxe_qp *qp,
 	wqe_size += sizeof(struct rxe_send_wqe);
 
 	type = QUEUE_TYPE_FROM_CLIENT;
+	/*
+	 * 在以下使用rxe_queue_init():
+	 *   - drivers/infiniband/sw/rxe/rxe_cq.c|65| <<rxe_cq_from_init>> cq->queue = rxe_queue_init(rxe, &cqe, sizeof(struct rxe_cqe), type);
+	 *   - drivers/infiniband/sw/rxe/rxe_qp.c|246| <<rxe_qp_init_req>> qp->sq.queue = rxe_queue_init(rxe, &qp->sq.max_wr, wqe_size, type);
+	 *   - drivers/infiniband/sw/rxe/rxe_qp.c|299| <<rxe_qp_init_resp>> qp->rq.queue = rxe_queue_init(rxe, &qp->rq.max_wr, wqe_size, type);
+	 *   - drivers/infiniband/sw/rxe/rxe_queue.c|158| <<rxe_queue_resize>> new_q = rxe_queue_init(q->rxe, &num_elem, elem_size, q->type);
+	 *   - drivers/infiniband/sw/rxe/rxe_srq.c|96| <<rxe_srq_from_init>> q = rxe_queue_init(rxe, &srq->rq.max_wr, srq_wqe_size, type);
+	 */
 	qp->sq.queue = rxe_queue_init(rxe, &qp->sq.max_wr,
 				wqe_size, type);
 	if (!qp->sq.queue)
@@ -296,6 +304,14 @@ static int rxe_qp_init_resp(struct rxe_dev *rxe, struct rxe_qp *qp,
 			 qp_num(qp), qp->rq.max_wr, qp->rq.max_sge, wqe_size);
 
 		type = QUEUE_TYPE_FROM_CLIENT;
+		/*
+		 * 在以下使用rxe_queue_init():
+		 *   - drivers/infiniband/sw/rxe/rxe_cq.c|65| <<rxe_cq_from_init>> cq->queue = rxe_queue_init(rxe, &cqe, sizeof(struct rxe_cqe), type);
+		 *   - drivers/infiniband/sw/rxe/rxe_qp.c|246| <<rxe_qp_init_req>> qp->sq.queue = rxe_queue_init(rxe, &qp->sq.max_wr, wqe_size, type);
+		 *   - drivers/infiniband/sw/rxe/rxe_qp.c|299| <<rxe_qp_init_resp>> qp->rq.queue = rxe_queue_init(rxe, &qp->rq.max_wr, wqe_size, type);
+		 *   - drivers/infiniband/sw/rxe/rxe_queue.c|158| <<rxe_queue_resize>> new_q = rxe_queue_init(q->rxe, &num_elem, elem_size, q->type);
+		 *   - drivers/infiniband/sw/rxe/rxe_srq.c|96| <<rxe_srq_from_init>> q = rxe_queue_init(rxe, &srq->rq.max_wr, srq_wqe_size, type);
+		 */
 		qp->rq.queue = rxe_queue_init(rxe, &qp->rq.max_wr,
 					wqe_size, type);
 		if (!qp->rq.queue)
diff --git a/drivers/infiniband/sw/rxe/rxe_queue.c b/drivers/infiniband/sw/rxe/rxe_queue.c
index 03157de52f5f..41ee73681367 100644
--- a/drivers/infiniband/sw/rxe/rxe_queue.c
+++ b/drivers/infiniband/sw/rxe/rxe_queue.c
@@ -52,6 +52,14 @@ inline void rxe_queue_reset(struct rxe_queue *q)
 	memset(q->buf->data, 0, q->buf_size - sizeof(struct rxe_queue_buf));
 }
 
+/*
+ * 在以下使用rxe_queue_init():
+ *   - drivers/infiniband/sw/rxe/rxe_cq.c|65| <<rxe_cq_from_init>> cq->queue = rxe_queue_init(rxe, &cqe, sizeof(struct rxe_cqe), type);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|246| <<rxe_qp_init_req>> qp->sq.queue = rxe_queue_init(rxe, &qp->sq.max_wr, wqe_size, type);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|299| <<rxe_qp_init_resp>> qp->rq.queue = rxe_queue_init(rxe, &qp->rq.max_wr, wqe_size, type);
+ *   - drivers/infiniband/sw/rxe/rxe_queue.c|158| <<rxe_queue_resize>> new_q = rxe_queue_init(q->rxe, &num_elem, elem_size, q->type);
+ *   - drivers/infiniband/sw/rxe/rxe_srq.c|96| <<rxe_srq_from_init>> q = rxe_queue_init(rxe, &srq->rq.max_wr, srq_wqe_size, type);
+ */
 struct rxe_queue *rxe_queue_init(struct rxe_dev *rxe, int *num_elem,
 			unsigned int elem_size, enum queue_type type)
 {
@@ -155,6 +163,14 @@ int rxe_queue_resize(struct rxe_queue *q, unsigned int *num_elem_p,
 	int err;
 	unsigned long flags = 0, flags1;
 
+	/*
+	 * 在以下使用rxe_queue_init():
+	 *   - drivers/infiniband/sw/rxe/rxe_cq.c|65| <<rxe_cq_from_init>> cq->queue = rxe_queue_init(rxe, &cqe, sizeof(struct rxe_cqe), type);
+	 *   - drivers/infiniband/sw/rxe/rxe_qp.c|246| <<rxe_qp_init_req>> qp->sq.queue = rxe_queue_init(rxe, &qp->sq.max_wr, wqe_size, type);
+	 *   - drivers/infiniband/sw/rxe/rxe_qp.c|299| <<rxe_qp_init_resp>> qp->rq.queue = rxe_queue_init(rxe, &qp->rq.max_wr, wqe_size, type);
+	 *   - drivers/infiniband/sw/rxe/rxe_queue.c|158| <<rxe_queue_resize>> new_q = rxe_queue_init(q->rxe, &num_elem, elem_size, q->type);
+	 *   - drivers/infiniband/sw/rxe/rxe_srq.c|96| <<rxe_srq_from_init>> q = rxe_queue_init(rxe, &srq->rq.max_wr, srq_wqe_size, type);
+	 */
 	new_q = rxe_queue_init(q->rxe, &num_elem, elem_size, q->type);
 	if (!new_q)
 		return -ENOMEM;
diff --git a/drivers/infiniband/sw/rxe/rxe_recv.c b/drivers/infiniband/sw/rxe/rxe_recv.c
index 6a6cc1fa90e4..d5fa52f01656 100644
--- a/drivers/infiniband/sw/rxe/rxe_recv.c
+++ b/drivers/infiniband/sw/rxe/rxe_recv.c
@@ -355,6 +355,11 @@ static int rxe_chk_dgid(struct rxe_dev *rxe, struct sk_buff *skb)
 	return 0;
 }
 
+/*
+ * 在以下使用rxe_rcv():
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|185| <<rxe_udp_encap_recv>> rxe_rcv(skb);
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|428| <<rxe_loopback>> rxe_rcv(skb);
+ */
 /* rxe_rcv is called from the interface driver */
 void rxe_rcv(struct sk_buff *skb)
 {
diff --git a/drivers/infiniband/sw/rxe/rxe_req.c b/drivers/infiniband/sw/rxe/rxe_req.c
index 093808419207..a464c7da6c07 100644
--- a/drivers/infiniband/sw/rxe/rxe_req.c
+++ b/drivers/infiniband/sw/rxe/rxe_req.c
@@ -458,6 +458,11 @@ static int finish_packet(struct rxe_qp *qp, struct rxe_send_wqe *wqe,
 {
 	int err;
 
+	/*
+	 * 在以下使用rxe_prepare():
+	 *   - drivers/infiniband/sw/rxe/rxe_req.c|461| <<finish_packet>> err = rxe_prepare(pkt, skb);
+	 *   - drivers/infiniband/sw/rxe/rxe_resp.c|654| <<prepare_ack_packet>> err = rxe_prepare(ack, skb);
+	 */
 	err = rxe_prepare(pkt, skb);
 	if (err)
 		return err;
@@ -739,6 +744,14 @@ int rxe_requester(void *arg)
 	save_state(wqe, qp, &rollback_wqe, &rollback_psn);
 	update_wqe_state(qp, wqe, &pkt);
 	update_wqe_psn(qp, wqe, &pkt, payload);
+	/*
+	 * 在以下使用rxe_xmit_packet():
+	 *   - drivers/infiniband/sw/rxe/rxe_req.c|742| <<rxe_requester>> ret = rxe_xmit_packet(qp, &pkt, skb);
+	 *   - drivers/infiniband/sw/rxe/rxe_resp.c|731| <<read_reply>> err = rxe_xmit_packet(qp, &ack_pkt, skb);
+	 *   - drivers/infiniband/sw/rxe/rxe_resp.c|958| <<send_ack>> err = rxe_xmit_packet(qp, &ack_pkt, skb);
+	 *   - drivers/infiniband/sw/rxe/rxe_resp.c|993| <<send_atomic_ack>> rc = rxe_xmit_packet(qp, &ack_pkt, skb);
+	 *   - drivers/infiniband/sw/rxe/rxe_resp.c|1122| <<duplicate_request>> rc = rxe_xmit_packet(qp, pkt, res->atomic.skb);
+	 */
 	ret = rxe_xmit_packet(qp, &pkt, skb);
 	if (ret) {
 		qp->need_req_skb = 1;
diff --git a/drivers/infiniband/sw/rxe/rxe_resp.c b/drivers/infiniband/sw/rxe/rxe_resp.c
index e7dec8481061..05e6b405adb0 100644
--- a/drivers/infiniband/sw/rxe/rxe_resp.c
+++ b/drivers/infiniband/sw/rxe/rxe_resp.c
@@ -514,6 +514,12 @@ static enum resp_states check_rkey(struct rxe_qp *qp,
 	return state;
 }
 
+/*
+ * 在以下使用send_data_in():
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|781| <<execute>> err = send_data_in(qp, &hdr, sizeof(hdr));
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|783| <<execute>> err = send_data_in(qp, ipv6_hdr(skb), sizeof(hdr));
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|789| <<execute>> err = send_data_in(qp, payload_addr(pkt), payload_size(pkt));
+ */
 static enum resp_states send_data_in(struct rxe_qp *qp, void *data_addr,
 				     int data_len)
 {
@@ -528,6 +534,10 @@ static enum resp_states send_data_in(struct rxe_qp *qp, void *data_addr,
 	return RESPST_NONE;
 }
 
+/*
+ * 在以下使用write_data_in():
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|793| <<execute>> err = write_data_in(qp, pkt);
+ */
 static enum resp_states write_data_in(struct rxe_qp *qp,
 				      struct rxe_pkt_info *pkt)
 {
@@ -591,6 +601,15 @@ static enum resp_states process_atomic(struct rxe_qp *qp,
 	return ret;
 }
 
+/*
+ * 在以下使用prepare_ack_packet():
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|715| <<read_reply>> skb = prepare_ack_packet(qp, req_pkt,
+ *       &ack_pkt, opcode, payload, res->cur_psn, AETH_ACK_UNLIMITED);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|951| <<send_ack>> skb = prepare_ack_packet(qp, pkt,
+ *       &ack_pkt, IB_OPCODE_RC_ACKNOWLEDGE, 0, psn, syndrome);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|974| <<send_atomic_ack>> skb = prepare_ack_packet(qp, pkt,
+ *       &ack_pkt, IB_OPCODE_RC_ATOMIC_ACKNOWLEDGE, 0, pkt->psn, syndrome);
+ */
 static struct sk_buff *prepare_ack_packet(struct rxe_qp *qp,
 					  struct rxe_pkt_info *pkt,
 					  struct rxe_pkt_info *ack,
@@ -632,6 +651,11 @@ static struct sk_buff *prepare_ack_packet(struct rxe_qp *qp,
 	if (ack->mask & RXE_ATMACK_MASK)
 		atmack_set_orig(ack, qp->resp.atomic_orig);
 
+	/*
+	 * 在以下使用rxe_prepare():
+	 *   - drivers/infiniband/sw/rxe/rxe_req.c|461| <<finish_packet>> err = rxe_prepare(pkt, skb);
+	 *   - drivers/infiniband/sw/rxe/rxe_resp.c|654| <<prepare_ack_packet>> err = rxe_prepare(ack, skb);
+	 */
 	err = rxe_prepare(ack, skb);
 	if (err) {
 		kfree_skb(skb);
@@ -786,10 +810,19 @@ static enum resp_states execute(struct rxe_qp *qp, struct rxe_pkt_info *pkt)
 			if (err)
 				return err;
 		}
+		/*
+		 * 在以下使用send_data_in():
+		 *   - drivers/infiniband/sw/rxe/rxe_resp.c|781| <<execute>> err = send_data_in(qp, &hdr, sizeof(hdr));
+		 *   - drivers/infiniband/sw/rxe/rxe_resp.c|783| <<execute>> err = send_data_in(qp, ipv6_hdr(skb), sizeof(hdr));
+		 *   - drivers/infiniband/sw/rxe/rxe_resp.c|789| <<execute>> err = send_data_in(qp, payload_addr(pkt), payload_size(pkt));
+		 */
 		err = send_data_in(qp, payload_addr(pkt), payload_size(pkt));
 		if (err)
 			return err;
 	} else if (pkt->mask & RXE_WRITE_MASK) {
+		/*
+		 * 只在这里使用
+		 */
 		err = write_data_in(qp, pkt);
 		if (err)
 			return err;
@@ -818,6 +851,10 @@ static enum resp_states execute(struct rxe_qp *qp, struct rxe_pkt_info *pkt)
 		/* We successfully processed this new request. */
 		qp->resp.msn++;
 
+	/*
+	 * struct rxe_qp *qp:
+	 * -> struct rxe_resp_info resp;
+	 */
 	/* next expected psn, read handles this separately */
 	qp->resp.psn = (pkt->psn + 1) & BTH_PSN_MASK;
 	qp->resp.ack_psn = qp->resp.psn;
diff --git a/drivers/infiniband/sw/rxe/rxe_srq.c b/drivers/infiniband/sw/rxe/rxe_srq.c
index eb1c4c3b3a78..27b4cb77ac70 100644
--- a/drivers/infiniband/sw/rxe/rxe_srq.c
+++ b/drivers/infiniband/sw/rxe/rxe_srq.c
@@ -93,6 +93,14 @@ int rxe_srq_from_init(struct rxe_dev *rxe, struct rxe_srq *srq,
 	spin_lock_init(&srq->rq.consumer_lock);
 
 	type = QUEUE_TYPE_FROM_CLIENT;
+	/*
+	 * 在以下使用rxe_queue_init():
+	 *   - drivers/infiniband/sw/rxe/rxe_cq.c|65| <<rxe_cq_from_init>> cq->queue = rxe_queue_init(rxe, &cqe, sizeof(struct rxe_cqe), type);
+	 *   - drivers/infiniband/sw/rxe/rxe_qp.c|246| <<rxe_qp_init_req>> qp->sq.queue = rxe_queue_init(rxe, &qp->sq.max_wr, wqe_size, type);
+	 *   - drivers/infiniband/sw/rxe/rxe_qp.c|299| <<rxe_qp_init_resp>> qp->rq.queue = rxe_queue_init(rxe, &qp->rq.max_wr, wqe_size, type);
+	 *   - drivers/infiniband/sw/rxe/rxe_queue.c|158| <<rxe_queue_resize>> new_q = rxe_queue_init(q->rxe, &num_elem, elem_size, q->type);
+	 *   - drivers/infiniband/sw/rxe/rxe_srq.c|96| <<rxe_srq_from_init>> q = rxe_queue_init(rxe, &srq->rq.max_wr, srq_wqe_size, type);
+	 */
 	q = rxe_queue_init(rxe, &srq->rq.max_wr,
 			srq_wqe_size, type);
 	if (!q) {
diff --git a/drivers/infiniband/sw/rxe/rxe_task.c b/drivers/infiniband/sw/rxe/rxe_task.c
index 5aa69947a979..61a769bdf657 100644
--- a/drivers/infiniband/sw/rxe/rxe_task.c
+++ b/drivers/infiniband/sw/rxe/rxe_task.c
@@ -10,6 +10,19 @@
 
 #include "rxe.h"
 
+/*
+ * 在以下使用__rxe_do_task():
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|514| <<rxe_qp_reset>> __rxe_do_task(&qp->resp.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|517| <<rxe_qp_reset>> __rxe_do_task(&qp->comp.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|518| <<rxe_qp_reset>> __rxe_do_task(&qp->req.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|560| <<rxe_qp_drain>> __rxe_do_task(&qp->comp.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|579| <<rxe_qp_error>> __rxe_do_task(&qp->comp.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|793| <<rxe_qp_destroy>> __rxe_do_task(&qp->req.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|796| <<rxe_qp_destroy>> __rxe_do_task(&qp->comp.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|797| <<rxe_qp_destroy>> __rxe_do_task(&qp->req.task);
+ *   - drivers/infiniband/sw/rxe/rxe_req.c|713| <<rxe_requester>> __rxe_do_task(&qp->comp.task);
+ *   - drivers/infiniband/sw/rxe/rxe_req.c|768| <<rxe_requester>> __rxe_do_task(&qp->comp.task);
+ */
 int __rxe_do_task(struct rxe_task *task)
 
 {
@@ -28,6 +41,11 @@ int __rxe_do_task(struct rxe_task *task)
  * a second caller finds the task already running
  * but looks just after the last call to func
  */
+/*
+ * 在以下使用rxe_do_task():
+ *   - drivers/infiniband/sw/rxe/rxe_task.c|110| <<rxe_init_task>> tasklet_setup(&task->tasklet, rxe_do_task);
+ *   - drivers/infiniband/sw/rxe/rxe_task.c|146| <<rxe_run_task>> rxe_do_task(&task->tasklet);
+ */
 void rxe_do_task(struct tasklet_struct *t)
 {
 	int cont;
@@ -95,12 +113,28 @@ void rxe_do_task(struct tasklet_struct *t)
 	task->ret = ret;
 }
 
+/*
+ * 在以下使用rxe_init_task():
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|269| <<rxe_qp_init_req>> rxe_init_task(&qp->req.task, qp, rxe_requester);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|270| <<rxe_qp_init_req>> rxe_init_task(&qp->comp.task, qp, rxe_completer);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|315| <<rxe_qp_init_resp>> rxe_init_task(&qp->resp.task, qp, rxe_responder);
+ *
+ * 有下面几种:
+ * - comp --> rxe_completer()
+ * - req  --> rxe_requester()
+ * - resp --> rxe_responder()
+ */
 int rxe_init_task(struct rxe_task *task, void *arg, int (*func)(void *))
 {
 	task->arg	= arg;
 	task->func	= func;
 	task->destroyed	= false;
 
+	/*
+	 * 在以下使用rxe_do_task():
+	 *   - drivers/infiniband/sw/rxe/rxe_task.c|110| <<rxe_init_task>> tasklet_setup(&task->tasklet, rxe_do_task);
+	 *   - drivers/infiniband/sw/rxe/rxe_task.c|146| <<rxe_run_task>> rxe_do_task(&task->tasklet);
+	 */
 	tasklet_setup(&task->tasklet, rxe_do_task);
 
 	task->state = TASK_STATE_START;
@@ -109,6 +143,12 @@ int rxe_init_task(struct rxe_task *task, void *arg, int (*func)(void *))
 	return 0;
 }
 
+/*
+ * 在以下使用rxe_cleanup_task():
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|781| <<rxe_qp_destroy>> rxe_cleanup_task(&qp->resp.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|788| <<rxe_qp_destroy>> rxe_cleanup_task(&qp->req.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|789| <<rxe_qp_destroy>> rxe_cleanup_task(&qp->comp.task);
+ */
 void rxe_cleanup_task(struct rxe_task *task)
 {
 	unsigned long flags;
@@ -129,22 +169,70 @@ void rxe_cleanup_task(struct rxe_task *task)
 	tasklet_kill(&task->tasklet);
 }
 
+/*
+ * 在以下使用rxe_run_task():
+ *   - drivers/infiniband/sw/rxe/rxe_comp.c|119| <<retransmit_timer>> rxe_run_task(&qp->comp.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_comp.c|133| <<rxe_comp_queue_pkt>> rxe_run_task(&qp->comp.task, must_sched);
+ *   - drivers/infiniband/sw/rxe/rxe_comp.c|306| <<check_ack>> rxe_run_task(&qp->req.task, 0);
+ *   - drivers/infiniband/sw/rxe/rxe_comp.c|448| <<do_complete>> rxe_run_task(&qp->req.task, 0);
+ *   - drivers/infiniband/sw/rxe/rxe_comp.c|464| <<complete_ack>> rxe_run_task(&qp->req.task, 0);
+ *   - drivers/infiniband/sw/rxe/rxe_comp.c|510| <<complete_wqe>> rxe_run_task(&qp->req.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_comp.c|646| <<rxe_completer>> rxe_run_task(&qp->req.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_comp.c|717| <<rxe_completer>> rxe_run_task(&qp->req.task, 0);
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|430| <<rxe_skb_tx_dtor>> rxe_run_task(&qp->req.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|534| <<rxe_xmit_packet>> rxe_run_task(&qp->comp.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|558| <<rxe_qp_drain>> rxe_run_task(&qp->comp.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|561| <<rxe_qp_drain>> rxe_run_task(&qp->req.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|574| <<rxe_qp_error>> rxe_run_task(&qp->resp.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|577| <<rxe_qp_error>> rxe_run_task(&qp->comp.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|580| <<rxe_qp_error>> rxe_run_task(&qp->req.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_req.c|107| <<rnr_nak_timer>> rxe_run_task(&qp->req.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_req.c|615| <<rxe_do_local_ops>> rxe_run_task(&qp->comp.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_req.c|754| <<rxe_requester>> rxe_run_task(&qp->req.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|92| <<rxe_resp_queue_pkt>> rxe_run_task(&qp->resp.task, must_sched);
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|694| <<rxe_post_send_kernel>> rxe_run_task(&qp->req.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|696| <<rxe_post_send_kernel>> rxe_run_task(&qp->comp.task, 1);
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|718| <<rxe_post_send>> rxe_run_task(&qp->req.task, 0);
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|758| <<rxe_post_recv>> rxe_run_task(&qp->resp.task, 1);
+ *
+ * 有下面几种:
+ * - comp --> rxe_completer()
+ * - req  --> rxe_requester()
+ * - resp --> rxe_responder()
+ */
 void rxe_run_task(struct rxe_task *task, int sched)
 {
 	if (task->destroyed)
 		return;
 
+	/*
+	 * 在以下使用rxe_do_task():
+	 *   - drivers/infiniband/sw/rxe/rxe_task.c|110| <<rxe_init_task>> tasklet_setup(&task->tasklet, rxe_do_task);
+	 *   - drivers/infiniband/sw/rxe/rxe_task.c|146| <<rxe_run_task>> rxe_do_task(&task->tasklet);
+	 */
 	if (sched)
 		tasklet_schedule(&task->tasklet);
 	else
 		rxe_do_task(&task->tasklet);
 }
 
+/*
+ * 在以下使用rxe_disable_task():
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|498| <<rxe_qp_reset>> rxe_disable_task(&qp->resp.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|503| <<rxe_qp_reset>> rxe_disable_task(&qp->comp.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|504| <<rxe_qp_reset>> rxe_disable_task(&qp->req.task);
+ */
 void rxe_disable_task(struct rxe_task *task)
 {
 	tasklet_disable(&task->tasklet);
 }
 
+/*
+ * 在以下使用rxe_enable_task():
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|541| <<rxe_qp_reset>> rxe_enable_task(&qp->resp.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|545| <<rxe_qp_reset>> rxe_enable_task(&qp->comp.task);
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|547| <<rxe_qp_reset>> rxe_enable_task(&qp->req.task);
+ */
 void rxe_enable_task(struct rxe_task *task)
 {
 	tasklet_enable(&task->tasklet);
diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.c b/drivers/infiniband/sw/rxe/rxe_verbs.c
index 9b6445df5d62..7b7102134f51 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.c
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.c
@@ -582,6 +582,10 @@ static void copy_inline_data_to_wqe(struct rxe_send_wqe *wqe,
 	}
 }
 
+/*
+ * 在以下使用init_send_wqe():
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|646| <<post_one_send>> init_send_wqe(qp, ibwr, mask, length, send_wqe);
+ */
 static void init_send_wqe(struct rxe_qp *qp, const struct ib_send_wr *ibwr,
 			 unsigned int mask, unsigned int length,
 			 struct rxe_send_wqe *wqe)
@@ -620,6 +624,10 @@ static void init_send_wqe(struct rxe_qp *qp, const struct ib_send_wr *ibwr,
 	wqe->ssn		= atomic_add_return(1, &qp->ssn);
 }
 
+/*
+ * 在以下使用post_one_send():
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|685| <<rxe_post_send_kernel>> err = post_one_send(qp, wr, mask, length);
+ */
 static int post_one_send(struct rxe_qp *qp, const struct ib_send_wr *ibwr,
 			 unsigned int mask, u32 length)
 {
@@ -635,6 +643,11 @@ static int post_one_send(struct rxe_qp *qp, const struct ib_send_wr *ibwr,
 
 	spin_lock_irqsave(&qp->sq.sq_lock, flags);
 
+	/*
+	 * struct rxe_sq *sq:
+	 * -> struct rxe_queue *queue;
+	 *    -> struct rxe_queue_buf *buf;
+	 */
 	full = queue_full(sq->queue, QUEUE_TYPE_TO_DRIVER);
 
 	if (unlikely(full)) {
@@ -652,6 +665,10 @@ static int post_one_send(struct rxe_qp *qp, const struct ib_send_wr *ibwr,
 	return 0;
 }
 
+/*
+ * 在以下使用rxe_post_send_kernel():
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|721| <<rxe_post_send>> return rxe_post_send_kernel(qp, wr, bad_wr);
+ */
 static int rxe_post_send_kernel(struct rxe_qp *qp, const struct ib_send_wr *wr,
 				const struct ib_send_wr **bad_wr)
 {
@@ -698,6 +715,9 @@ static int rxe_post_send_kernel(struct rxe_qp *qp, const struct ib_send_wr *wr,
 	return err;
 }
 
+/*
+ * struct ib_device_ops rxe_dev_ops.post_send = rxe_post_send()
+ */
 static int rxe_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 			 const struct ib_send_wr **bad_wr)
 {
@@ -1117,6 +1137,10 @@ static int rxe_enable_driver(struct ib_device *ib_dev)
 	return 0;
 }
 
+/*
+ * 在以下使用rxe_dev_ops:
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|1208| <<rxe_register_device>> ib_set_device_ops(dev, &rxe_dev_ops);
+ */
 static const struct ib_device_ops rxe_dev_ops = {
 	.owner = THIS_MODULE,
 	.driver_id = RDMA_DRIVER_RXE,
@@ -1185,6 +1209,10 @@ static const struct ib_device_ops rxe_dev_ops = {
 	INIT_RDMA_OBJ_SIZE(ib_mw, rxe_mw, ibmw),
 };
 
+/*
+ * 在以下使用rxe_register_device():
+ *   - drivers/infiniband/sw/rxe/rxe.c|250| <<rxe_add>> return rxe_register_device(rxe, ibdev_name);
+ */
 int rxe_register_device(struct rxe_dev *rxe, const char *ibdev_name)
 {
 	int err;
@@ -1210,6 +1238,25 @@ int rxe_register_device(struct rxe_dev *rxe, const char *ibdev_name)
 	if (err)
 		return err;
 
+	/*
+	 * 在以下使用ib_register_device():
+	 *   - drivers/infiniband/hw/bnxt_re/main.c|594| <<bnxt_re_register_ib>> return ib_register_device(ibdev, "bnxt_re%d", &rdev->en_dev->pdev->dev);
+	 *   - drivers/infiniband/hw/cxgb4/provider.c|550| <<c4iw_register_device>> ret = ib_register_device(&dev->ibdev, "cxgb4_%d",
+	 *   - drivers/infiniband/hw/efa/efa_main.c|314| <<efa_ib_device_add>> err = ib_register_device(&dev->ibdev, "efa_%d", &pdev->dev);
+	 *   - drivers/infiniband/hw/hns/hns_roce_main.c|548| <<hns_roce_register_device>> ret = ib_register_device(ib_dev, "hns_%d", dev);
+	 *   - drivers/infiniband/hw/irdma/verbs.c|4531| <<irdma_ib_register_device>> ret = ib_register_device(&iwdev->ibdev, "irdma%d", iwdev->rf->hw.device);
+	 *   - drivers/infiniband/hw/mana/device.c|81| <<mana_ib_probe>> ret = ib_register_device(&dev->ib_dev, "mana_%d",
+	 *   - drivers/infiniband/hw/mlx4/main.c|2936| <<mlx4_ib_probe>> err = ib_register_device(&ibdev->ib_dev, "mlx4_%d",
+	 *   - drivers/infiniband/hw/mlx5/main.c|4258| <<mlx5_ib_stage_ib_reg_init>> return ib_register_device(&dev->ib_dev, name, &dev->mdev->pdev->dev);
+	 *   - drivers/infiniband/hw/mthca/mthca_provider.c|1174| <<mthca_register_device>> ret = ib_register_device(&dev->ib_dev, "mthca%d", &dev->pdev->dev);
+	 *   - drivers/infiniband/hw/ocrdma/ocrdma_main.c|228| <<ocrdma_register_device>> return ib_register_device(&dev->ibdev, "ocrdma%d",
+	 *   - drivers/infiniband/hw/qedr/main.c|267| <<qedr_register_device>> return ib_register_device(&dev->ibdev, "qedr%d", &dev->pdev->dev);
+	 *   - drivers/infiniband/hw/usnic/usnic_ib_main.c|410| <<usnic_ib_device_add>> if (ib_register_device(&us_ibdev->ib_dev, "usnic_%d", &dev->dev))
+	 *   - drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c|246| <<pvrdma_register_device>> ret = ib_register_device(&dev->ib_dev, "vmw_pvrdma%d", &dev->pdev->dev);
+	 *   - drivers/infiniband/sw/rdmavt/vt.c|556| <<rvt_register_device>> ret = ib_register_device(&rdi->ibdev, dev_name(&rdi->ibdev.dev), NULL);
+	 *   - drivers/infiniband/sw/rxe/rxe_verbs.c|1213| <<rxe_register_device>> err = ib_register_device(dev, ibdev_name, NULL);
+	 *   - drivers/infiniband/sw/siw/siw_main.c|72| <<siw_device_register>> rv = ib_register_device(base_dev, name, NULL);
+	 */
 	err = ib_register_device(dev, ibdev_name, NULL);
 	if (err)
 		pr_warn("%s failed with error %d\n", __func__, err);
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 40c8bbd8b680..cbc40508037d 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -191,6 +191,12 @@ struct virtnet_info {
 	/* Max # of queue pairs supported by the device */
 	u16 max_queue_pairs;
 
+	/*
+	 * 在以下修改virtnet_info->curr_queue_pairs:
+	 *   - drivers/net/virtio_net.c|2050| <<_virtnet_set_queues>> vi->curr_queue_pairs = queue_pairs;
+	 *   - drivers/net/virtio_net.c|3402| <<virtnet_probe>> vi->curr_queue_pairs = max_queue_pairs;
+	 *   - drivers/net/virtio_net.c|3404| <<virtnet_probe>> vi->curr_queue_pairs = num_online_cpus();
+	 */
 	/* # of queue pairs currently used by the driver */
 	u16 curr_queue_pairs;
 
@@ -1668,6 +1674,12 @@ static int virtnet_open(struct net_device *dev)
 	enable_delayed_refill(vi);
 
 	for (i = 0; i < vi->max_queue_pairs; i++) {
+		/*
+		 * 在以下修改virtnet_info->curr_queue_pairs:
+		 *   - drivers/net/virtio_net.c|2050| <<_virtnet_set_queues>> vi->curr_queue_pairs = queue_pairs;
+		 *   - drivers/net/virtio_net.c|3402| <<virtnet_probe>> vi->curr_queue_pairs = max_queue_pairs;
+		 *   - drivers/net/virtio_net.c|3404| <<virtnet_probe>> vi->curr_queue_pairs = num_online_cpus();
+		 */
 		if (i < vi->curr_queue_pairs)
 			/* Make sure we have some buffers: if oom use wq. */
 			if (!try_fill_recv(vi, &vi->rq[i], GFP_KERNEL))
@@ -2017,6 +2029,13 @@ static void virtnet_ack_link_announce(struct virtnet_info *vi)
 	rtnl_unlock();
 }
 
+/*
+ * 在以下使用_virtnet_set_queues():
+ *   - drivers/net/virtio_net.c|2057| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|2344| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|2674| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+ *   - drivers/net/virtio_net.c|3412| <<virtnet_probe>> _virtnet_set_queues(vi, vi->curr_queue_pairs);
+ */
 static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 {
 	struct scatterlist sg;
@@ -2025,6 +2044,12 @@ static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 	if (!vi->has_cvq || !virtio_has_feature(vi->vdev, VIRTIO_NET_F_MQ))
 		return 0;
 
+	/*
+	 * struct virtnet_info *vi:
+	 * -> struct control_buf *ctrl;
+	 *    -> struct virtio_net_ctrl_mq mq;
+	 *       -> __virtio16 virtqueue_pairs;
+	 */
 	vi->ctrl->mq.virtqueue_pairs = cpu_to_virtio16(vi->vdev, queue_pairs);
 	sg_init_one(&sg, &vi->ctrl->mq, sizeof(vi->ctrl->mq));
 
@@ -2034,6 +2059,12 @@ static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 			 queue_pairs);
 		return -EINVAL;
 	} else {
+		/*
+		 * 在以下修改virtnet_info->curr_queue_pairs:
+		 *   - drivers/net/virtio_net.c|2050| <<_virtnet_set_queues>> vi->curr_queue_pairs = queue_pairs;
+		 *   - drivers/net/virtio_net.c|3402| <<virtnet_probe>> vi->curr_queue_pairs = max_queue_pairs;
+		 *   - drivers/net/virtio_net.c|3404| <<virtnet_probe>> vi->curr_queue_pairs = num_online_cpus();
+		 */
 		vi->curr_queue_pairs = queue_pairs;
 		/* virtnet_open() will refill when device is going to up. */
 		if (dev->flags & IFF_UP)
@@ -2043,11 +2074,22 @@ static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 	return 0;
 }
 
+/*
+ * 在以下使用virtnet_set_queues():
+ *   - drivers/net/virtio_net.c|3509| <<virtnet_restore>> virtnet_set_queues(vi, vi->curr_queue_pairs);
+ */
 static int virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 {
 	int err;
 
 	rtnl_lock();
+	/*
+	 * 在以下使用_virtnet_set_queues():
+	 *   - drivers/net/virtio_net.c|2057| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+	 *   - drivers/net/virtio_net.c|2344| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+	 *   - drivers/net/virtio_net.c|2674| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+	 *   - drivers/net/virtio_net.c|3412| <<virtnet_probe>> _virtnet_set_queues(vi, vi->curr_queue_pairs);
+	 */
 	err = _virtnet_set_queues(vi, queue_pairs);
 	rtnl_unlock();
 	return err;
@@ -2310,6 +2352,9 @@ static void virtnet_get_drvinfo(struct net_device *dev,
 
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.set_channels = virtnet_set_channels()
+ */
 /* TODO: Eliminate OOO packets during switching */
 static int virtnet_set_channels(struct net_device *dev,
 				struct ethtool_channels *channels)
@@ -2335,6 +2380,13 @@ static int virtnet_set_channels(struct net_device *dev,
 		return -EINVAL;
 
 	cpus_read_lock();
+	/*
+	 * 在以下使用_virtnet_set_queues():
+	 *   - drivers/net/virtio_net.c|2057| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+	 *   - drivers/net/virtio_net.c|2344| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+	 *   - drivers/net/virtio_net.c|2674| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+	 *   - drivers/net/virtio_net.c|3412| <<virtnet_probe>> _virtnet_set_queues(vi, vi->curr_queue_pairs);
+	 */
 	err = _virtnet_set_queues(vi, queue_pairs);
 	if (err) {
 		cpus_read_unlock();
@@ -2665,6 +2717,13 @@ static int virtnet_xdp_set(struct net_device *dev, struct bpf_prog *prog,
 		synchronize_net();
 	}
 
+	/*
+	 * 在以下使用_virtnet_set_queues():
+	 *   - drivers/net/virtio_net.c|2057| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+	 *   - drivers/net/virtio_net.c|2344| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+	 *   - drivers/net/virtio_net.c|2674| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+	 *   - drivers/net/virtio_net.c|3412| <<virtnet_probe>> _virtnet_set_queues(vi, vi->curr_queue_pairs);
+	 */
 	err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
 	if (err)
 		goto err;
@@ -3359,6 +3418,12 @@ static int virtnet_probe(struct virtio_device *vdev)
 	if (vi->any_header_sg)
 		dev->needed_headroom = vi->hdr_len;
 
+	/*
+	 * 在以下修改virtnet_info->curr_queue_pairs:
+	 *   - drivers/net/virtio_net.c|2050| <<_virtnet_set_queues>> vi->curr_queue_pairs = queue_pairs;
+	 *   - drivers/net/virtio_net.c|3402| <<virtnet_probe>> vi->curr_queue_pairs = max_queue_pairs;
+	 *   - drivers/net/virtio_net.c|3404| <<virtnet_probe>> vi->curr_queue_pairs = num_online_cpus();
+	 */
 	/* Enable multiqueue by default */
 	if (num_online_cpus() >= max_queue_pairs)
 		vi->curr_queue_pairs = max_queue_pairs;
@@ -3403,6 +3468,13 @@ static int virtnet_probe(struct virtio_device *vdev)
 
 	virtio_device_ready(vdev);
 
+	/*
+	 * 在以下使用_virtnet_set_queues():
+	 *   - drivers/net/virtio_net.c|2057| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+	 *   - drivers/net/virtio_net.c|2344| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+	 *   - drivers/net/virtio_net.c|2674| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+	 *   - drivers/net/virtio_net.c|3412| <<virtnet_probe>> _virtnet_set_queues(vi, vi->curr_queue_pairs);
+	 */
 	_virtnet_set_queues(vi, vi->curr_queue_pairs);
 
 	/* Assume link up if device can't report link status,
diff --git a/drivers/pci/rom.c b/drivers/pci/rom.c
index e18d3a4383ba..ed8ec858352d 100644
--- a/drivers/pci/rom.c
+++ b/drivers/pci/rom.c
@@ -80,6 +80,10 @@ EXPORT_SYMBOL_GPL(pci_disable_rom);
  * The PCI window size could be much larger than the
  * actual image size.
  */
+/*
+ * 在以下使用pci_get_rom_size():
+ *   - drivers/pci/rom.c|164| <<pci_map_rom>> *size = pci_get_rom_size(pdev, rom, *size);
+ */
 static size_t pci_get_rom_size(struct pci_dev *pdev, void __iomem *rom,
 			       size_t size)
 {
diff --git a/drivers/ptp/ptp_kvm_x86.c b/drivers/ptp/ptp_kvm_x86.c
index 5e5b2ef78547..2184b7d2378b 100644
--- a/drivers/ptp/ptp_kvm_x86.c
+++ b/drivers/ptp/ptp_kvm_x86.c
@@ -45,6 +45,11 @@ int kvm_arch_ptp_init(void)
 	}
 
 	clock_pair_gpa = slow_virt_to_phys(clock_pair);
+	/*
+	 * 在以下使用pvclock_get_pvti_cpu0_va():
+	 *   - arch/x86/entry/vdso/vma.c|206| <<vvar_fault>> pvclock_get_pvti_cpu0_va();
+	 *   - drivers/ptp/ptp_kvm_x86.c|48| <<kvm_arch_ptp_init>> if (!pvclock_get_pvti_cpu0_va()) {
+	 */
 	if (!pvclock_get_pvti_cpu0_va()) {
 		ret = -EOPNOTSUPP;
 		goto err;
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 07d0250f17c3..7b3d853f2198 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -105,6 +105,13 @@ static void virtscsi_compute_resid(struct scsi_cmnd *sc, u32 resid)
  *
  * Called with vq_lock held.
  */
+/*
+ * 在以下使用virtscsi_complete_cmd():
+ *   - drivers/scsi/virtio_scsi.c|197| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|207| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i],
+ *                                                    virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|593| <<virtscsi_queuecommand>> virtscsi_complete_cmd(vscsi, cmd);
+ */
 static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
@@ -166,6 +173,13 @@ static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 	sc->scsi_done(sc);
 }
 
+/*
+ * 在以下使用virtscsi_vq_done():
+ *   - drivers/scsi/virtio_scsi.c|197| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|206| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i], virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|223| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+ *   - drivers/scsi/virtio_scsi.c|413| <<virtscsi_event_done>> virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
+ */
 static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 			     struct virtio_scsi_vq *virtscsi_vq,
 			     void (*fn)(struct virtio_scsi *vscsi, void *buf))
@@ -178,6 +192,9 @@ static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 	spin_lock_irqsave(&virtscsi_vq->vq_lock, flags);
 	do {
 		virtqueue_disable_cb(vq);
+		/*
+		 * 很多对virtqueue_get_buf()的调用
+		 */
 		while ((buf = virtqueue_get_buf(vq, &len)) != NULL)
 			fn(vscsi, buf);
 
@@ -187,6 +204,10 @@ static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 	spin_unlock_irqrestore(&virtscsi_vq->vq_lock, flags);
 }
 
+/*
+ * 在以下使用virtscsi_req_done():
+ *   - drivers/scsi/virtio_scsi.c|812| <<virtscsi_init>> callbacks[i] = virtscsi_req_done;
+ */
 static void virtscsi_req_done(struct virtqueue *vq)
 {
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
@@ -194,14 +215,38 @@ static void virtscsi_req_done(struct virtqueue *vq)
 	int index = vq->index - VIRTIO_SCSI_VQ_BASE;
 	struct virtio_scsi_vq *req_vq = &vscsi->req_vqs[index];
 
+	/*
+	 * 在以下使用virtscsi_complete_cmd():
+	 *   - drivers/scsi/virtio_scsi.c|197| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|207| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i],
+	 *                                                    virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|593| <<virtscsi_queuecommand>> virtscsi_complete_cmd(vscsi, cmd);
+	 */
 	virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
 };
 
+/*
+ * 在以下使用virtscsi_poll_requests():
+ *   - drivers/scsi/virtio_scsi.c|788| <<virtscsi_tmf>> virtscsi_poll_requests(vscsi);
+ */
 static void virtscsi_poll_requests(struct virtio_scsi *vscsi)
 {
 	int i, num_vqs;
 
 	num_vqs = vscsi->num_queues;
+	/*
+	 * 在以下使用virtscsi_vq_done():
+	 *   - drivers/scsi/virtio_scsi.c|197| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|206| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i], virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|223| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|413| <<virtscsi_event_done>> virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
+	 *
+	 * 在以下使用virtscsi_complete_cmd():
+	 *   - drivers/scsi/virtio_scsi.c|197| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|207| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i],
+	 *                                                    virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|593| <<virtscsi_queuecommand>> virtscsi_complete_cmd(vscsi, cmd);
+	 */
 	for (i = 0; i < num_vqs; i++)
 		virtscsi_vq_done(vscsi, &vscsi->req_vqs[i],
 				 virtscsi_complete_cmd);
@@ -220,6 +265,13 @@ static void virtscsi_ctrl_done(struct virtqueue *vq)
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
 	struct virtio_scsi *vscsi = shost_priv(sh);
 
+	/*
+	 * 在以下使用virtscsi_vq_done():
+	 *   - drivers/scsi/virtio_scsi.c|197| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|206| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i], virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|223| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|413| <<virtscsi_event_done>> virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
+	 */
 	virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
 };
 
@@ -410,9 +462,24 @@ static void virtscsi_event_done(struct virtqueue *vq)
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
 	struct virtio_scsi *vscsi = shost_priv(sh);
 
+	/*
+	 * 在以下使用virtscsi_vq_done():
+	 *   - drivers/scsi/virtio_scsi.c|197| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|206| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i], virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|223| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|413| <<virtscsi_event_done>> virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
+	 */
 	virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
 };
 
+/*
+ * 只在以下地方调用__virtscsi_add_cmd():
+ *   - drivers/scsi/virtio_scsi.c|492| <<virtscsi_add_cmd>> err = __virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size);
+ *
+ * 比如:
+ * req_size  = sizeof(cmd->req.cmd)
+ * resq_size = sizeof(cmd->resp.cmd)
+ */
 static int __virtscsi_add_cmd(struct virtqueue *vq,
 			    struct virtio_scsi_cmd *cmd,
 			    size_t req_size, size_t resp_size)
@@ -424,6 +491,17 @@ static int __virtscsi_add_cmd(struct virtqueue *vq,
 
 	out = in = NULL;
 
+	/*
+	 * struct virtio_scsi_cmd *cmd:
+	 * -> struct scsi_cmnd *sc;
+	 *    -> struct scsi_data_buffer sdb;
+	 *       -> struct sg_table table;
+	 *          -> struct scatterlist *sgl;
+	 *          -> unsigned int nents;
+	 *          -> unsigned int orig_nents;
+	 *       -> unsigned length;
+	 *    -> struct scsi_data_buffer *prot_sdb;
+	 */
 	if (sc && sc->sc_data_direction != DMA_NONE) {
 		if (sc->sc_data_direction != DMA_FROM_DEVICE)
 			out = &sc->sdb.table;
@@ -431,6 +509,11 @@ static int __virtscsi_add_cmd(struct virtqueue *vq,
 			in = &sc->sdb.table;
 	}
 
+	/*
+	 * 比如:
+	 * req_size  = sizeof(cmd->req.cmd)
+	 * resq_size = sizeof(cmd->resp.cmd)
+	 */
 	/* Request header.  */
 	sg_init_one(&req, &cmd->req, req_size);
 	sgs[out_num++] = &req;
@@ -455,6 +538,9 @@ static int __virtscsi_add_cmd(struct virtqueue *vq,
 		sgs[out_num + in_num++] = in->sgl;
 	}
 
+	/*
+	 * 很多调用
+	 */
 	return virtqueue_add_sgs(vq, sgs, out_num, in_num, cmd, GFP_ATOMIC);
 }
 
@@ -479,6 +565,30 @@ static void virtscsi_kick_vq(struct virtio_scsi_vq *vq)
  * @resp_size	: size of the response buffer
  * @kick	: whether to kick the virtqueue immediately
  */
+/*
+ * 在以下使用virtscsi_add_cmd():
+ *   - drivers/scsi/virtio_scsi.c|589| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq,
+ *                     cmd, req_size, sizeof(cmd->resp.cmd), kick);
+ *   - drivers/scsi/virtio_scsi.c|607| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq,
+ *                     cmd, sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+ *
+ * 40 struct virtio_scsi_cmd {     
+ * 41         struct scsi_cmnd *sc;
+ * 42         struct completion *comp;
+ * 43         union {
+ * 44                 struct virtio_scsi_cmd_req       cmd;
+ * 45                 struct virtio_scsi_cmd_req_pi    cmd_pi;
+ * 46                 struct virtio_scsi_ctrl_tmf_req  tmf;
+ * 47                 struct virtio_scsi_ctrl_an_req   an;
+ * 48         } req;
+ * 49         union {
+ * 50                 struct virtio_scsi_cmd_resp      cmd;
+ * 51                 struct virtio_scsi_ctrl_tmf_resp tmf;
+ * 52                 struct virtio_scsi_ctrl_an_resp  an;
+ * 53                 struct virtio_scsi_event         evt;
+ * 54         } resp;
+ * 55 } ____cacheline_aligned_in_smp;
+ */
 static int virtscsi_add_cmd(struct virtio_scsi_vq *vq,
 			     struct virtio_scsi_cmd *cmd,
 			     size_t req_size, size_t resp_size,
@@ -489,17 +599,35 @@ static int virtscsi_add_cmd(struct virtio_scsi_vq *vq,
 	bool needs_kick = false;
 
 	spin_lock_irqsave(&vq->vq_lock, flags);
+	/*
+	 * 比如:
+	 * req_size  = sizeof(cmd->req.cmd)
+	 * resq_size = sizeof(cmd->resp.cmd)
+	 *
+	 *
+	 * 只在这里调用
+	 */
 	err = __virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size);
 	if (!err && kick)
 		needs_kick = virtqueue_kick_prepare(vq->vq);
 
 	spin_unlock_irqrestore(&vq->vq_lock, flags);
 
+	/*
+	 * virtio-scsi在两个地方:
+	 *   - drivers/scsi/virtio_scsi.c|471| <<virtscsi_kick_vq>> virtqueue_notify(vq->vq);
+	 *   - drivers/scsi/virtio_scsi.c|499| <<virtscsi_add_cmd>> virtqueue_notify(vq->vq);
+	 */
 	if (needs_kick)
 		virtqueue_notify(vq->vq);
 	return err;
 }
 
+/*
+ * 在以下使用virtio_scsi_init_hdr():
+ *   - drivers/scsi/virtio_scsi.c|525| <<virtio_scsi_init_hdr_pi>> virtio_scsi_init_hdr(vdev, (struct virtio_scsi_cmd_req *)cmd_pi, sc);
+ *   - drivers/scsi/virtio_scsi.c|583| <<virtscsi_queuecommand>> virtio_scsi_init_hdr(vscsi->vdev, &cmd->req.cmd, sc);
+ */
 static void virtio_scsi_init_hdr(struct virtio_device *vdev,
 				 struct virtio_scsi_cmd_req *cmd,
 				 struct scsi_cmnd *sc)
@@ -580,16 +708,47 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	} else
 #endif
 	{
+		/*
+		 * struct virtio_scsi_cmd_req {
+		 *     __u8 lun[8];            // Logical Unit Number
+		 *     __virtio64 tag;         // Command identifier/
+		 *     __u8 task_attr;         // Task attribute
+		 *     __u8 prio;              // SAM command priority field
+		 *     __u8 crn;
+		 *     __u8 cdb[VIRTIO_SCSI_CDB_SIZE];
+		 * } __attribute__((packed));
+		 *
+		 *
+		 * 在以下使用virtio_scsi_init_hdr():
+		 *   - drivers/scsi/virtio_scsi.c|525| <<virtio_scsi_init_hdr_pi>> virtio_scsi_init_hdr(vdev, (struct virtio_scsi_cmd_req *)cmd_pi, sc);
+		 *   - drivers/scsi/virtio_scsi.c|583| <<virtscsi_queuecommand>> virtio_scsi_init_hdr(vscsi->vdev, &cmd->req.cmd, sc);
+		 *
+		 * 这里整个是在初始化virtio_scsi_cmd_req
+		 */
 		virtio_scsi_init_hdr(vscsi->vdev, &cmd->req.cmd, sc);
 		memcpy(cmd->req.cmd.cdb, sc->cmnd, sc->cmd_len);
 		req_size = sizeof(cmd->req.cmd);
 	}
 
 	kick = (sc->flags & SCMD_LAST) != 0;
+	/*
+	 * 在以下使用virtscsi_add_cmd():
+	 *   - drivers/scsi/virtio_scsi.c|589| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq,
+	 *                     cmd, req_size, sizeof(cmd->resp.cmd), kick);
+	 *   - drivers/scsi/virtio_scsi.c|607| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq,
+	 *                     cmd, sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+	 */
 	ret = virtscsi_add_cmd(req_vq, cmd, req_size, sizeof(cmd->resp.cmd), kick);
 	if (ret == -EIO) {
 		cmd->resp.cmd.response = VIRTIO_SCSI_S_BAD_TARGET;
 		spin_lock_irqsave(&req_vq->vq_lock, flags);
+		/*
+		 * 在以下使用virtscsi_complete_cmd():
+		 *   - drivers/scsi/virtio_scsi.c|197| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+		 *   - drivers/scsi/virtio_scsi.c|207| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i],
+		 *                                                    virtscsi_complete_cmd);
+		 *   - drivers/scsi/virtio_scsi.c|593| <<virtscsi_queuecommand>> virtscsi_complete_cmd(vscsi, cmd);
+		 */
 		virtscsi_complete_cmd(vscsi, cmd);
 		spin_unlock_irqrestore(&req_vq->vq_lock, flags);
 	} else if (ret != 0) {
@@ -604,6 +763,13 @@ static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 	int ret = FAILED;
 
 	cmd->comp = &comp;
+	/*
+	 * 在以下使用virtscsi_add_cmd():
+	 *   - drivers/scsi/virtio_scsi.c|589| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq,
+	 *                     cmd, req_size, sizeof(cmd->resp.cmd), kick);
+	 *   - drivers/scsi/virtio_scsi.c|607| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq,
+	 *                     cmd, sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+	 */
 	if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
 			      sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
 		goto out;
diff --git a/drivers/target/target_core_configfs.c b/drivers/target/target_core_configfs.c
index 30ce3451bc6b..3bbcb7c3692d 100644
--- a/drivers/target/target_core_configfs.c
+++ b/drivers/target/target_core_configfs.c
@@ -319,6 +319,13 @@ static struct configfs_subsystem target_core_fabrics = {
 	},
 };
 
+/*
+ * 在以下使用target_depend_item():
+ *   - drivers/target/target_core_pr.c|1415| <<core_scsi3_tpg_depend_item>> return target_depend_item(&tpg->tpg_group.cg_item);
+ *   - drivers/target/target_core_pr.c|1428| <<core_scsi3_nodeacl_depend_item>> return target_depend_item(&nacl->acl_group.cg_item);
+ *   - drivers/target/target_core_pr.c|1450| <<core_scsi3_lunacl_depend_item>> return target_depend_item(&lun_acl->se_lun_group.cg_item);
+ *   - drivers/vhost/scsi.c|2116| <<vhost_scsi_set_endpoint>> ret = target_depend_item(&se_tpg->tpg_group.cg_item);
+ */
 int target_depend_item(struct config_item *item)
 {
 	return configfs_depend_item(&target_core_fabrics, item);
diff --git a/drivers/vfio/pci/vfio_pci_core.c b/drivers/vfio/pci/vfio_pci_core.c
index f57ae4180471..560063146f7b 100644
--- a/drivers/vfio/pci/vfio_pci_core.c
+++ b/drivers/vfio/pci/vfio_pci_core.c
@@ -743,6 +743,9 @@ long vfio_pci_core_ioctl(struct vfio_device *core_vdev, unsigned int cmd,
 			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
 			info.flags = 0;
 
+			/*
+			 * struct pci_dev *pdev = vdev->pdev;
+			 */
 			/* Report the BAR size, not the ROM size */
 			info.size = pci_resource_len(pdev, info.index);
 			if (!info.size) {
diff --git a/drivers/vhost/iotlb.c b/drivers/vhost/iotlb.c
index ea61330a3431..cf08202b7974 100644
--- a/drivers/vhost/iotlb.c
+++ b/drivers/vhost/iotlb.c
@@ -100,6 +100,17 @@ int vhost_iotlb_add_range_ctx(struct vhost_iotlb *iotlb,
 }
 EXPORT_SYMBOL_GPL(vhost_iotlb_add_range_ctx);
 
+/*
+ * 在以下使用vhost_iotlb_add_range():
+ *   - drivers/vdpa/mlx5/core/mr.c|609| <<dup_iotlb>> err = vhost_iotlb_add_range(dst, start, last, start, VHOST_ACCESS_RW);
+ *   - drivers/vdpa/mlx5/core/mr.c|615| <<dup_iotlb>> err = vhost_iotlb_add_range(dst, map->start, map->last,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|157| <<vdpasim_do_reset>> vhost_iotlb_add_range(&vdpasim->iommu[i], 0, ULONG_MAX,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|266| <<vdpasim_create>> vhost_iotlb_add_range(&vdpasim->iommu[i], 0, ULONG_MAX, 0,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|649| <<vdpasim_set_map>> ret = vhost_iotlb_add_range(iommu, map->start,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|674| <<vdpasim_reset_map>> vhost_iotlb_add_range(&vdpasim->iommu[asid], 0, ULONG_MAX,
+ *   - drivers/vhost/vhost.c|1123| <<vhost_process_iotlb_msg>> if (vhost_iotlb_add_range(dev->iotlb, msg->iova,
+ *   - drivers/vhost/vhost.c|1481| <<vhost_set_memory>> if (vhost_iotlb_add_range(newumem,
+ */
 int vhost_iotlb_add_range(struct vhost_iotlb *iotlb,
 			  u64 start, u64 last,
 			  u64 addr, unsigned int perm)
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 51498f97a7d0..75e9283d8ea9 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -1340,6 +1340,19 @@ static int vhost_net_open(struct inode *inode, struct file *f)
 		n->vqs[i].rx_ring = NULL;
 		vhost_net_buf_init(&n->vqs[i].rxq);
 	}
+	/*
+	 * 在以下使用vhost_dev_init():
+	 *   - drivers/vhost/net.c|1343| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *         VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH, VHOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2436| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *         nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *         VHOST_TEST_VQ_MAX, UIO_MAXIOV, VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1432| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *         nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|704| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *         ARRAY_SIZE(vsock->vqs), UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,
 		       UIO_MAXIOV + VHOST_NET_BATCH,
 		       VHOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true,
@@ -1547,8 +1560,32 @@ static long vhost_net_set_backend(struct vhost_net *n, unsigned index, int fd)
 		}
 
 		vhost_net_disable_vq(n, vq);
+		/*
+		 * 在以下使用vhost_vq_set_backend():
+		 *   - drivers/vhost/net.c|1368| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/net.c|1550| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+		 *   - drivers/vhost/net.c|1599| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+		 *   - drivers/vhost/scsi.c|2147| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+		 *   - drivers/vhost/scsi.c|2260| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|137| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|202| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+		 *   - drivers/vhost/test.c|294| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|296| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+		 *   - drivers/vhost/vsock.c|603| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+		 *   - drivers/vhost/vsock.c|629| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/vsock.c|636| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/vsock.c|661| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+		 */
 		vhost_vq_set_backend(vq, sock);
 		vhost_net_buf_unproduce(nvq);
+		/*
+		 * 在以下使用vhost_vq_init_access():
+		 *   - drivers/vhost/net.c|1552| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/scsi.c|2126| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+		 *   - drivers/vhost/test.c|204| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+		 *   - drivers/vhost/test.c|297| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/vsock.c|604| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+		 */
 		r = vhost_vq_init_access(vq);
 		if (r)
 			goto err_used;
@@ -1687,6 +1724,11 @@ static long vhost_net_set_owner(struct vhost_net *n)
 	r = vhost_net_set_ubuf_info(n);
 	if (r)
 		goto out;
+	/*
+	 * 在以下使用vhost_dev_set_owner():
+	 *   - drivers/vhost/net.c|1714| <<vhost_net_set_owner>> r = vhost_dev_set_owner(&n->dev);
+	 *   - drivers/vhost/vhost.c|2011| <<vhost_dev_ioctl(VHOST_SET_OWNER)>> r = vhost_dev_set_owner(d);
+	 */
 	r = vhost_dev_set_owner(&n->dev);
 	if (r)
 		vhost_net_clear_ubuf_info(n);
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 7c713cf9c75d..144a6173820f 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -300,6 +300,11 @@ static void vhost_scsi_done_inflight(struct kref *kref)
 	complete(&inflight->comp);
 }
 
+/*
+ * 在以下使用vhost_scsi_init_inflight():
+ *   - drivers/vhost/scsi.c|1923| <<vhost_scsi_flush>> vhost_scsi_init_inflight(vs, vs->old_inflight);
+ *   - drivers/vhost/scsi.c|2434| <<vhost_scsi_open>> vhost_scsi_init_inflight(vs, NULL);
+ */
 static void vhost_scsi_init_inflight(struct vhost_scsi *vs,
 				    struct vhost_scsi_inflight *old_inflight[])
 {
@@ -1060,6 +1065,10 @@ static int vhost_scsi_to_tcm_attr(int attr)
 	return TCM_SIMPLE_TAG;
 }
 
+/*
+ * 在以下使用vhost_scsi_target_queue_cmd():
+ *   - drivers/vhost/scsi.c|1552| <<vhost_scsi_handle_vq>> vhost_scsi_target_queue_cmd(nexus, cmd, cdb, lun, task_attr,
+ */
 static void vhost_scsi_target_queue_cmd(struct vhost_scsi_nexus *nexus,
 					struct vhost_scsi_cmd *cmd,
 					unsigned char *cdb, u16 lun,
@@ -1168,6 +1177,25 @@ vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 	if (likely(log_num))
 		*log_num = 0;
 
+	/*
+	 * 在以下使用vhost_get_vq_desc():
+	 *   - drivers/vhost/net.c|579| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(tvq,
+	 *             tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|591| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(tvq,
+	 *             tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1067| <<get_rx_bufs>> r = vhost_get_vq_desc(vq,
+	 *             vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|616| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq,
+	 *             vq->iov, ARRAY_SIZE(vq->iov), &out, &in, vq_log, &log_num);
+	 *   - drivers/vhost/scsi.c|1171| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq,
+	 *             vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, log, log_num);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(vq,
+	 *             vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|131| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq,
+	 *             vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|524| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq,
+	 *             vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 */
 	vc->head = vhost_get_vq_desc(vq, vq->iov,
 				     ARRAY_SIZE(vq->iov), &vc->out, &vc->in,
 				     log, log_num);
@@ -1179,6 +1207,17 @@ vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 	if (unlikely(vc->head < 0))
 		goto done;
 
+	/*
+	 * struct vhost_scsi_ctx {
+	 *     int head;
+	 *     unsigned int out, in;
+	 *     size_t req_size, rsp_size;
+	 *     size_t out_size, in_size;
+	 *     u8 *target, *lunp;
+	 *     void *req;
+	 *     struct iov_iter out_iter;
+	 * };
+	 */
 	/* Nothing new?  Wait for eventfd to tell us they refilled. */
 	if (vc->head == vq->num) {
 		if (unlikely(vhost_enable_notify(&vs->dev, vq))) {
@@ -1303,6 +1342,10 @@ static u16 vhost_buf_to_lun(u8 *lun_buf)
 	return ((lun_buf[2] << 8) | lun_buf[3]) & 0x3FFF;
 }
 
+/*
+ * 在以下使用vhost_scsi_handle_vq():
+ *   - drivers/vhost/scsi.c|1912| <<vhost_scsi_handle_kick>> vhost_scsi_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -1864,6 +1907,10 @@ static void vhost_scsi_evt_handle_kick(struct vhost_work *work)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用vhost_scsi_handle_kick():
+ *   - drivers/vhost/scsi.c|2434| <<vhost_scsi_open>> svq->vq.handle_kick = vhost_scsi_handle_kick;
+ */
 static void vhost_scsi_handle_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1873,6 +1920,13 @@ static void vhost_scsi_handle_kick(struct vhost_work *work)
 	vhost_scsi_handle_vq(vs, vq);
 }
 
+/*
+ * 在以下使用vhost_scsi_flush():
+ *   - drivers/vhost/scsi.c|2146| <<vhost_scsi_set_endpoint>> vhost_scsi_flush(vs);
+ *   - drivers/vhost/scsi.c|2242| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+ *   - drivers/vhost/scsi.c|2253| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+ *   - drivers/vhost/scsi.c|2411| <<vhost_scsi_release>> vhost_scsi_flush(vs);
+ */
 /* Callers must hold dev mutex */
 static void vhost_scsi_flush(struct vhost_scsi *vs)
 {
@@ -1897,6 +1951,11 @@ static void vhost_scsi_flush(struct vhost_scsi *vs)
 		wait_for_completion(&vs->old_inflight[i]->comp);
 }
 
+/*
+ * 在以下使用vhost_scsi_destroy_vq_log():
+ *   - drivers/vhost/scsi.c|1967| <<vhost_scsi_destroy_vq_cmds>> vhost_scsi_destroy_vq_log(vq);
+ *   - drivers/vhost/scsi.c|2312| <<vhost_scsi_set_features>> vhost_scsi_destroy_vq_log(vq);
+ */
 static void vhost_scsi_destroy_vq_log(struct vhost_virtqueue *vq)
 {
 	struct vhost_scsi_virtqueue *svq = container_of(vq,
@@ -1915,6 +1974,12 @@ static void vhost_scsi_destroy_vq_log(struct vhost_virtqueue *vq)
 	}
 }
 
+/*
+ * 在以下使用vhost_scsi_destroy_vq_cmds():
+ *   - drivers/vhost/scsi.c|2025| <<vhost_scsi_setup_vq_cmds>> vhost_scsi_destroy_vq_cmds(vq);
+ *   - drivers/vhost/scsi.c|2154| <<vhost_scsi_set_endpoint>> vhost_scsi_destroy_vq_cmds(&vs->vqs[i].vq);
+ *   - drivers/vhost/scsi.c|2246| <<vhost_scsi_clear_endpoint>> vhost_scsi_destroy_vq_cmds(vq);
+ */
 static void vhost_scsi_destroy_vq_cmds(struct vhost_virtqueue *vq)
 {
 	struct vhost_scsi_virtqueue *svq = container_of(vq,
@@ -2061,6 +2126,13 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 			 * dependency now.
 			 */
 			se_tpg = &tpg->se_tpg;
+			/*
+			 * 在以下使用target_depend_item():
+			 *   - drivers/target/target_core_pr.c|1415| <<core_scsi3_tpg_depend_item>> return target_depend_item(&tpg->tpg_group.cg_item);
+			 *   - drivers/target/target_core_pr.c|1428| <<core_scsi3_nodeacl_depend_item>> return target_depend_item(&nacl->acl_group.cg_item);
+			 *   - drivers/target/target_core_pr.c|1450| <<core_scsi3_lunacl_depend_item>> return target_depend_item(&lun_acl->se_lun_group.cg_item);
+			 *   - drivers/vhost/scsi.c|2116| <<vhost_scsi_set_endpoint>> ret = target_depend_item(&se_tpg->tpg_group.cg_item);
+			 */
 			ret = target_depend_item(&se_tpg->tpg_group.cg_item);
 			if (ret) {
 				pr_warn("target_depend_item() failed: %d\n", ret);
@@ -2092,7 +2164,31 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 		for (i = 0; i < vs->dev.nvqs; i++) {
 			vq = &vs->vqs[i].vq;
 			mutex_lock(&vq->mutex);
+			/*
+			 * 在以下使用vhost_vq_set_backend():
+			 *   - drivers/vhost/net.c|1368| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/net.c|1550| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+			 *   - drivers/vhost/net.c|1599| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+			 *   - drivers/vhost/scsi.c|2147| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+			 *   - drivers/vhost/scsi.c|2260| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/test.c|137| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/test.c|202| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+			 *   - drivers/vhost/test.c|294| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/test.c|296| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+			 *   - drivers/vhost/vsock.c|603| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+			 *   - drivers/vhost/vsock.c|629| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/vsock.c|636| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/vsock.c|661| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+			 */
 			vhost_vq_set_backend(vq, vs_tpg);
+			/*
+			 * 在以下使用vhost_vq_init_access():
+			 *   - drivers/vhost/net.c|1552| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/scsi.c|2126| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+			 *   - drivers/vhost/test.c|204| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+			 *   - drivers/vhost/test.c|297| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/vsock.c|604| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+			 */
 			vhost_vq_init_access(vq);
 			mutex_unlock(&vq->mutex);
 		}
@@ -2105,6 +2201,13 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 	 * Act as synchronize_rcu to make sure access to
 	 * old vs->vs_tpg is finished.
 	 */
+	/*
+	 * 在以下使用vhost_scsi_flush():
+	 *   - drivers/vhost/scsi.c|2146| <<vhost_scsi_set_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2242| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2253| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2411| <<vhost_scsi_release>> vhost_scsi_flush(vs);
+	 */
 	vhost_scsi_flush(vs);
 	kfree(vs->vs_tpg);
 	vs->vs_tpg = vs_tpg;
@@ -2321,6 +2424,16 @@ static int vhost_scsi_open(struct inode *inode, struct file *f)
 	if (!vqs)
 		goto err_local_vqs;
 
+	/*
+	 * 在以下使用vhost_work_init():
+	 *   - drivers/vhost/scsi.c|2427| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+	 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 *   - drivers/vhost/scsi.c|2662| <<vhost_scsi_port_link>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 *   - drivers/vhost/vhost.c|203| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+	 *   - drivers/vhost/vhost.c|253| <<vhost_work_dev_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+	 *   - drivers/vhost/vhost.c|592| <<vhost_attach_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+	 *   - drivers/vhost/vsock.c|711| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+	 */
 	vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
 	vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
 
@@ -2338,6 +2451,19 @@ static int vhost_scsi_open(struct inode *inode, struct file *f)
 		svq->vs = vs;
 		svq->vq.handle_kick = vhost_scsi_handle_kick;
 	}
+	/*
+	 * 在以下使用vhost_dev_init():
+	 *   - drivers/vhost/net.c|1343| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *         VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH, VHOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2436| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *         nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *         VHOST_TEST_VQ_MAX, UIO_MAXIOV, VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1432| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *         nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|704| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *         ARRAY_SIZE(vsock->vqs), UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(&vs->dev, vqs, nvqs, UIO_MAXIOV,
 		       VHOST_SCSI_WEIGHT, 0, true, NULL);
 
@@ -2371,6 +2497,12 @@ static int vhost_scsi_release(struct inode *inode, struct file *f)
 	vhost_dev_cleanup(&vs->dev);
 	/* Jobs can re-queue themselves in evt kick handler. Do extra flush. */
 	vhost_scsi_flush(vs);
+	/*
+	 * struct vhost_scsi *vs:
+	 * -> struct vhost_dev dev;
+	 *    -> struct vhost_virtqueue **vqs;
+	 * -> struct vhost_scsi_virtqueue *vqs;
+	 */
 	kfree(vs->dev.vqs);
 	kfree(vs->vqs);
 	kfree(vs->old_inflight);
@@ -2537,6 +2669,16 @@ static int vhost_scsi_port_link(struct se_portal_group *se_tpg,
 	if (!tmf)
 		return -ENOMEM;
 	INIT_LIST_HEAD(&tmf->queue_entry);
+	/*
+	 * 在以下使用vhost_work_init():
+	 *   - drivers/vhost/scsi.c|2427| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+	 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 *   - drivers/vhost/scsi.c|2662| <<vhost_scsi_port_link>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 *   - drivers/vhost/vhost.c|203| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+	 *   - drivers/vhost/vhost.c|253| <<vhost_work_dev_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+	 *   - drivers/vhost/vhost.c|592| <<vhost_attach_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+	 *   - drivers/vhost/vsock.c|711| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+	 */
 	vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
 
 	mutex_lock(&vhost_scsi_mutex);
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index c3c3ae34a72b..87b2c87a7116 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -166,6 +166,11 @@ static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 	add_wait_queue(wqh, &poll->wait);
 }
 
+/*
+ * 在以下使用vhost_poll_wakeup():
+ *   - drivers/vhost/vhost.c|197| <<vhost_poll_init>> init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+ *   - drivers/vhost/vhost.c|218| <<vhost_poll_start>> vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
+ */
 static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 			     void *key)
 {
@@ -175,6 +180,22 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	if (!(key_to_poll(key) & poll->mask))
 		return 0;
 
+	/*
+	 * 在以下使用vhost_poll_queue():
+	 *   - drivers/vhost/net.c|406| <<vhost_zerocopy_callback>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|512| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|515| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|799| <<handle_tx_copy>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|893| <<handle_tx_zerocopy>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|1175| <<handle_rx>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|1251| <<handle_rx>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/vhost.c|181| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+	 *   - drivers/vhost/vhost.c|457| <<vhost_exceeds_weight>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/vhost.c|1201| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+	 *   - drivers/vhost/vhost.h|48| <<vhost_iotlb_notify_vq>> void vhost_poll_queue(struct vhost_poll *poll);
+	 *   - drivers/vhost/vsock.c|266| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+	 *   - drivers/vhost/vsock.c|346| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+	 */
 	if (!poll->dev->use_worker)
 		work->fn(work);
 	else
@@ -183,6 +204,16 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	return 0;
 }
 
+/*
+ * 在以下使用vhost_work_init():
+ *   - drivers/vhost/scsi.c|2427| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+ *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ *   - drivers/vhost/scsi.c|2662| <<vhost_scsi_port_link>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ *   - drivers/vhost/vhost.c|203| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+ *   - drivers/vhost/vhost.c|253| <<vhost_work_dev_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ *   - drivers/vhost/vhost.c|592| <<vhost_attach_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+ *   - drivers/vhost/vsock.c|711| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+ */
 void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 {
 	clear_bit(VHOST_WORK_QUEUED, &work->flags);
@@ -190,6 +221,12 @@ void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 }
 EXPORT_SYMBOL_GPL(vhost_work_init);
 
+/*
+ * 在以下使用vhost_poll_init():
+ *   - drivers/vhost/vhost.c|624| <<vhost_dev_init>> vhost_poll_init(&vq->poll, vq->handle_kick, EPOLLIN, dev);
+ *   - drivers/vhost/net.c|1361| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev);
+ *   - drivers/vhost/net.c|1362| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev);
+ */
 /* Init poll structure */
 void vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,
 		     __poll_t mask, struct vhost_dev *dev)
@@ -200,10 +237,25 @@ void vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,
 	poll->dev = dev;
 	poll->wqh = NULL;
 
+	/*
+	 * 在以下使用vhost_work_init():
+	 *   - drivers/vhost/scsi.c|2427| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+	 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 *   - drivers/vhost/scsi.c|2662| <<vhost_scsi_port_link>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 *   - drivers/vhost/vhost.c|203| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+	 *   - drivers/vhost/vhost.c|253| <<vhost_work_dev_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+	 *   - drivers/vhost/vhost.c|592| <<vhost_attach_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+	 *   - drivers/vhost/vsock.c|711| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+	 */
 	vhost_work_init(&poll->work, fn);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_init);
 
+/*
+ * 在以下使用vhost_poll_start():
+ *   - drivers/vhost/test.c|299| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *   - drivers/vhost/vhost.c|1971| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+ */
 /* Start polling a file. We add ourselves to file's wait queue. The caller must
  * keep a reference to a file until after vhost_poll_stop is called. */
 int vhost_poll_start(struct vhost_poll *poll, struct file *file)
@@ -214,6 +266,11 @@ int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 		return 0;
 
 	mask = vfs_poll(file, &poll->table);
+	/*
+	 * 在以下使用vhost_poll_wakeup():
+	 *   - drivers/vhost/vhost.c|197| <<vhost_poll_init>> init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+	 *   - drivers/vhost/vhost.c|218| <<vhost_poll_start>> vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
+	 */
 	if (mask)
 		vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
 	if (mask & EPOLLERR) {
@@ -236,12 +293,30 @@ void vhost_poll_stop(struct vhost_poll *poll)
 }
 EXPORT_SYMBOL_GPL(vhost_poll_stop);
 
+/*
+ * 在以下使用vhost_work_dev_flush():
+ *   - drivers/vhost/scsi.c|1934| <<vhost_scsi_flush>> vhost_work_dev_flush(&vs->dev);
+ *   - drivers/vhost/vhost.c|257| <<vhost_poll_flush>> vhost_work_dev_flush(poll->dev);
+ *   - drivers/vhost/vhost.c|557| <<vhost_attach_cgroups>> vhost_work_dev_flush(dev);
+ *   - drivers/vhost/vhost.h|49| <<vhost_attach_cgroups>> void vhost_work_dev_flush(struct vhost_dev *dev);
+ *   - drivers/vhost/vsock.c|726| <<vhost_vsock_flush>> vhost_work_dev_flush(&vsock->dev);
+ */
 void vhost_work_dev_flush(struct vhost_dev *dev)
 {
 	struct vhost_flush_struct flush;
 
 	if (dev->worker) {
 		init_completion(&flush.wait_event);
+		/*
+		 * 在以下使用vhost_work_init():
+		 *   - drivers/vhost/scsi.c|2427| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+		 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+		 *   - drivers/vhost/scsi.c|2662| <<vhost_scsi_port_link>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+		 *   - drivers/vhost/vhost.c|203| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+		 *   - drivers/vhost/vhost.c|253| <<vhost_work_dev_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+		 *   - drivers/vhost/vhost.c|592| <<vhost_attach_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+		 *   - drivers/vhost/vsock.c|711| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+		 */
 		vhost_work_init(&flush.work, vhost_flush_work);
 
 		vhost_work_queue(dev, &flush.work);
@@ -258,6 +333,17 @@ void vhost_poll_flush(struct vhost_poll *poll)
 }
 EXPORT_SYMBOL_GPL(vhost_poll_flush);
 
+/*
+ * 在以下使用vhost_work_queue():
+ *   - drivers/vhost/scsi.c|489| <<vhost_scsi_release_cmd>> vhost_work_queue(&tmf->vhost->dev, &tmf->vwork);
+ *   - drivers/vhost/scsi.c|499| <<vhost_scsi_release_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+ *   - drivers/vhost/scsi.c|1882| <<vhost_scsi_send_evt>> vhost_work_queue(&vs->dev, &vs->vs_event_work);
+ *   - drivers/vhost/vhost.c|247| <<vhost_work_dev_flush>> vhost_work_queue(dev, &flush.work);
+ *   - drivers/vhost/vhost.c|286| <<vhost_poll_queue>> vhost_work_queue(poll->dev, &poll->work);
+ *   - drivers/vhost/vhost.c|545| <<vhost_attach_cgroups>> vhost_work_queue(dev, &attach.work);
+ *   - drivers/vhost/vsock.c|303| <<vhost_transport_send_pkt>> vhost_work_queue(&vsock->dev, &vsock->send_pkt_work);
+ *   - drivers/vhost/vsock.c|623| <<vhost_vsock_start>> vhost_work_queue(&vsock->dev, &vsock->send_pkt_work);
+ */
 void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)
 {
 	if (!dev->worker)
@@ -268,6 +354,14 @@ void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)
 		 * sure it was not in the list.
 		 * test_and_set_bit() implies a memory barrier.
 		 */
+		/*
+		 * 在以下使用vhost_dev->work_list:
+		 *   - drivers/vhost/vhost.c|543| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|290| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+		 *   - drivers/vhost/vhost.c|299| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|401| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|813| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+		 */
 		llist_add(&work->node, &dev->work_list);
 		wake_up_process(dev->worker);
 	}
@@ -277,12 +371,47 @@ EXPORT_SYMBOL_GPL(vhost_work_queue);
 /* A lockless hint for busy polling code to exit the loop */
 bool vhost_has_work(struct vhost_dev *dev)
 {
+	/*
+	 * 在以下使用vhost_dev->work_list:
+	 *   - drivers/vhost/vhost.c|543| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|290| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+	 *   - drivers/vhost/vhost.c|299| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|401| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|813| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+	 */
 	return !llist_empty(&dev->work_list);
 }
 EXPORT_SYMBOL_GPL(vhost_has_work);
 
+/*
+ * 在以下使用vhost_poll_queue():
+ *   - drivers/vhost/net.c|406| <<vhost_zerocopy_callback>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|512| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|515| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|799| <<handle_tx_copy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|893| <<handle_tx_zerocopy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1175| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1251| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|181| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+ *   - drivers/vhost/vhost.c|457| <<vhost_exceeds_weight>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|1201| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+ *   - drivers/vhost/vhost.h|48| <<vhost_iotlb_notify_vq>> void vhost_poll_queue(struct vhost_poll *poll);
+ *   - drivers/vhost/vsock.c|266| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+ *   - drivers/vhost/vsock.c|346| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+ */
 void vhost_poll_queue(struct vhost_poll *poll)
 {
+	/*
+	 * 在以下使用vhost_work_queue():
+	 *   - drivers/vhost/scsi.c|489| <<vhost_scsi_release_cmd>> vhost_work_queue(&tmf->vhost->dev, &tmf->vwork);
+	 *   - drivers/vhost/scsi.c|499| <<vhost_scsi_release_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+	 *   - drivers/vhost/scsi.c|1882| <<vhost_scsi_send_evt>> vhost_work_queue(&vs->dev, &vs->vs_event_work);
+	 *   - drivers/vhost/vhost.c|247| <<vhost_work_dev_flush>> vhost_work_queue(dev, &flush.work);
+	 *   - drivers/vhost/vhost.c|286| <<vhost_poll_queue>> vhost_work_queue(poll->dev, &poll->work);
+	 *   - drivers/vhost/vhost.c|545| <<vhost_attach_cgroups>> vhost_work_queue(dev, &attach.work);
+	 *   - drivers/vhost/vsock.c|303| <<vhost_transport_send_pkt>> vhost_work_queue(&vsock->dev, &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|623| <<vhost_vsock_start>> vhost_work_queue(&vsock->dev, &vsock->send_pkt_work);
+	 */
 	vhost_work_queue(poll->dev, &poll->work);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_queue);
@@ -363,6 +492,14 @@ static int vhost_worker(void *data)
 			break;
 		}
 
+		/*
+		 * 在以下使用vhost_dev->work_list:
+		 *   - drivers/vhost/vhost.c|543| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|290| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+		 *   - drivers/vhost/vhost.c|299| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|401| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|813| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+		 */
 		node = llist_del_all(&dev->work_list);
 		if (!node)
 			schedule();
@@ -469,6 +606,19 @@ static size_t vhost_get_desc_size(struct vhost_virtqueue *vq,
 	return sizeof(*vq->desc) * num;
 }
 
+/*
+ * 在以下使用vhost_dev_init():
+ *   - drivers/vhost/net.c|1343| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+ *         VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH, VHOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+ *   - drivers/vhost/scsi.c|2436| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+ *         nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+ *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+ *         VHOST_TEST_VQ_MAX, UIO_MAXIOV, VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+ *   - drivers/vhost/vdpa.c|1432| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+ *         nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+ *   - drivers/vhost/vsock.c|704| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+ *         ARRAY_SIZE(vsock->vqs), UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+ */
 void vhost_dev_init(struct vhost_dev *dev,
 		    struct vhost_virtqueue **vqs, int nvqs,
 		    int iov_limit, int weight, int byte_weight,
@@ -507,6 +657,12 @@ void vhost_dev_init(struct vhost_dev *dev,
 		vq->dev = dev;
 		mutex_init(&vq->mutex);
 		vhost_vq_reset(dev, vq);
+		/*
+		 * 在以下使用vhost_poll_init():
+		 *   - drivers/vhost/vhost.c|624| <<vhost_dev_init>> vhost_poll_init(&vq->poll, vq->handle_kick, EPOLLIN, dev);
+		 *   - drivers/vhost/net.c|1361| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev);
+		 *   - drivers/vhost/net.c|1362| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev);
+		 */
 		if (vq->handle_kick)
 			vhost_poll_init(&vq->poll, vq->handle_kick,
 					EPOLLIN, dev);
@@ -541,6 +697,16 @@ static int vhost_attach_cgroups(struct vhost_dev *dev)
 	struct vhost_attach_cgroups_struct attach;
 
 	attach.owner = current;
+	/*
+	 * 在以下使用vhost_work_init():
+	 *   - drivers/vhost/scsi.c|2427| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+	 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 *   - drivers/vhost/scsi.c|2662| <<vhost_scsi_port_link>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 *   - drivers/vhost/vhost.c|203| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+	 *   - drivers/vhost/vhost.c|253| <<vhost_work_dev_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+	 *   - drivers/vhost/vhost.c|592| <<vhost_attach_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+	 *   - drivers/vhost/vsock.c|711| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+	 */
 	vhost_work_init(&attach.work, vhost_attach_cgroups_work);
 	vhost_work_queue(dev, &attach.work);
 	vhost_work_dev_flush(dev);
@@ -584,6 +750,11 @@ static void vhost_detach_mm(struct vhost_dev *dev)
 	dev->mm = NULL;
 }
 
+/*
+ * 在以下使用vhost_dev_set_owner():
+ *   - drivers/vhost/net.c|1714| <<vhost_net_set_owner>> r = vhost_dev_set_owner(&n->dev);
+ *   - drivers/vhost/vhost.c|2011| <<vhost_dev_ioctl(VHOST_SET_OWNER)>> r = vhost_dev_set_owner(d);
+ */
 /* Caller should have device mutex */
 long vhost_dev_set_owner(struct vhost_dev *dev)
 {
@@ -636,14 +807,31 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_set_owner);
 
+/*
+ * 在以下使用iotlb_alloc():
+ *   - drivers/vhost/vhost.c|647| <<vhost_dev_reset_owner_prepare>> return iotlb_alloc();
+ *   - drivers/vhost/vhost.c|1472| <<vhost_set_memory>> newumem = iotlb_alloc();
+ *   - drivers/vhost/vhost.c|1760| <<vhost_init_device_iotlb>> niotlb = iotlb_alloc();
+ */
 static struct vhost_iotlb *iotlb_alloc(void)
 {
 	return vhost_iotlb_alloc(max_iotlb_entries,
 				 VHOST_IOTLB_FLAG_RETIRE);
 }
 
+/*
+ * 在以下使用vhost_dev_reset_owner_prepare():
+ *   - drivers/vhost/net.c|1616| <<vhost_net_reset_owner>> umem = vhost_dev_reset_owner_prepare();
+ *   - drivers/vhost/test.c|234| <<vhost_test_reset_owner>> umem = vhost_dev_reset_owner_prepare();
+ */
 struct vhost_iotlb *vhost_dev_reset_owner_prepare(void)
 {
+	/*
+	 * 在以下使用iotlb_alloc():
+	 *   - drivers/vhost/vhost.c|647| <<vhost_dev_reset_owner_prepare>> return iotlb_alloc();
+	 *   - drivers/vhost/vhost.c|1472| <<vhost_set_memory>> newumem = iotlb_alloc();
+	 *   - drivers/vhost/vhost.c|1760| <<vhost_init_device_iotlb>> niotlb = iotlb_alloc();
+	 */
 	return iotlb_alloc();
 }
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner_prepare);
@@ -664,6 +852,16 @@ void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_iotlb *umem)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner);
 
+/*
+ * 在以下使用vhost_dev_stop():
+ *   - drivers/vhost/net.c|1413| <<vhost_net_release>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/net.c|1631| <<vhost_net_reset_owner>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/scsi.c|2408| <<vhost_scsi_release>> vhost_dev_stop(&vs->dev);
+ *   - drivers/vhost/test.c|164| <<vhost_test_release>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/test.c|241| <<vhost_test_reset_owner>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/vdpa.c|1469| <<vhost_vdpa_release>> vhost_dev_stop(&v->vdev);
+ *   - drivers/vhost/vsock.c|779| <<vhost_vsock_dev_release>> vhost_dev_stop(&vsock->dev);
+ */
 void vhost_dev_stop(struct vhost_dev *dev)
 {
 	int i;
@@ -697,6 +895,15 @@ void vhost_clear_msg(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_clear_msg);
 
+/*
+ * 在以下使用vhost_dev_cleanup():
+ *   - drivers/vhost/net.c|1414| <<vhost_net_release>> vhost_dev_cleanup(&n->dev);
+ *   - drivers/vhost/scsi.c|2431| <<vhost_scsi_release>> vhost_dev_cleanup(&vs->dev);
+ *   - drivers/vhost/test.c|165| <<vhost_test_release>> vhost_dev_cleanup(&n->dev);
+ *   - drivers/vhost/vdpa.c|1396| <<vhost_vdpa_cleanup>> vhost_dev_cleanup(&v->vdev);
+ *   - drivers/vhost/vhost.c|673| <<vhost_dev_reset_owner>> vhost_dev_cleanup(dev);
+ *   - drivers/vhost/vsock.c|792| <<vhost_vsock_dev_release>> vhost_dev_cleanup(&vsock->dev);
+ */
 void vhost_dev_cleanup(struct vhost_dev *dev)
 {
 	int i;
@@ -721,6 +928,14 @@ void vhost_dev_cleanup(struct vhost_dev *dev)
 	dev->iotlb = NULL;
 	vhost_clear_msg(dev);
 	wake_up_interruptible_poll(&dev->wait, EPOLLIN | EPOLLRDNORM);
+	/*
+	 * 在以下使用vhost_dev->work_list:
+	 *   - drivers/vhost/vhost.c|543| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|290| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+	 *   - drivers/vhost/vhost.c|299| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|401| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|813| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+	 */
 	WARN_ON(!llist_empty(&dev->work_list));
 	if (dev->worker) {
 		kthread_stop(dev->worker);
@@ -844,6 +1059,25 @@ static int vhost_copy_to_user(struct vhost_virtqueue *vq, void __user *to,
 		if (uaddr)
 			return __copy_to_user(uaddr, from, size);
 
+		/*
+		 * 在以下使用translate_desc():
+		 *   - drivers/vhost/vhost.c|847| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|882| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|907| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1950| <<log_used>> ret = translate_desc(vq,
+		 *       (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2166| <<get_indirect>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV, VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2207| <<get_indirect>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *       iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2347| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *       iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov,
 				     ARRAY_SIZE(vq->iotlb_iov),
 				     VHOST_ACCESS_WO);
@@ -879,6 +1113,25 @@ static int vhost_copy_from_user(struct vhost_virtqueue *vq, void *to,
 		if (uaddr)
 			return __copy_from_user(to, uaddr, size);
 
+		/*
+		 * 在以下使用translate_desc():
+		 *   - drivers/vhost/vhost.c|847| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|882| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|907| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1950| <<log_used>> ret = translate_desc(vq,
+		 *       (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2166| <<get_indirect>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV, VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2207| <<get_indirect>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *       iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2347| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *       iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov,
 				     ARRAY_SIZE(vq->iotlb_iov),
 				     VHOST_ACCESS_RO);
@@ -904,6 +1157,25 @@ static void __user *__vhost_get_user_slow(struct vhost_virtqueue *vq,
 {
 	int ret;
 
+	/*
+	 * 在以下使用translate_desc():
+	 *   - drivers/vhost/vhost.c|847| <<vhost_copy_to_user>> ret = translate_desc(vq,
+	 *       (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|882| <<vhost_copy_from_user>> ret = translate_desc(vq,
+	 *       (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|907| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+	 *       (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|1950| <<log_used>> ret = translate_desc(vq,
+	 *       (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|2166| <<get_indirect>> ret = translate_desc(vq,
+	 *       vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV, VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2207| <<get_indirect>> ret = translate_desc(vq,
+	 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *       iov + iov_count, iov_size - iov_count, access);
+	 *   - drivers/vhost/vhost.c|2347| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+	 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *       iov + iov_count, iov_size - iov_count, access);
+	 */
 	ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov,
 			     ARRAY_SIZE(vq->iotlb_iov),
 			     VHOST_ACCESS_RO);
@@ -1024,9 +1296,22 @@ static void vhost_dev_unlock_vqs(struct vhost_dev *d)
 		mutex_unlock(&d->vqs[i]->mutex);
 }
 
+/*
+ * 在以下使用vhost_get_avail_idx():
+ *   - drivers/vhost/vhost.c|2262| <<vhost_get_vq_desc>> if (unlikely(vhost_get_avail_idx(vq, &avail_idx))) {
+ *   - drivers/vhost/vhost.c|2551| <<vhost_vq_avail_empty>> r = vhost_get_avail_idx(vq, &avail_idx);
+ *   - drivers/vhost/vhost.c|2597| <<vhost_enable_notify>> r = vhost_get_avail_idx(vq, &avail_idx);
+ */
 static inline int vhost_get_avail_idx(struct vhost_virtqueue *vq,
 				      __virtio16 *idx)
 {
+	/*
+	 * struct vhost_virtqueue *vq:
+	 * -> vring_avail_t __user *avail;
+	 *    -> __virtio16 flags;
+	 *    -> __virtio16 idx;
+	 *    -> __virtio16 ring[];
+	 */
 	return vhost_get_avail(vq, *idx, &vq->avail->idx);
 }
 
@@ -1120,6 +1405,17 @@ static int vhost_process_iotlb_msg(struct vhost_dev *dev, u32 asid,
 			break;
 		}
 		vhost_vq_meta_reset(dev);
+		/*
+		 * 在以下使用vhost_iotlb_add_range():
+		 *   - drivers/vdpa/mlx5/core/mr.c|609| <<dup_iotlb>> err = vhost_iotlb_add_range(dst, start, last, start, VHOST_ACCESS_RW);
+		 *   - drivers/vdpa/mlx5/core/mr.c|615| <<dup_iotlb>> err = vhost_iotlb_add_range(dst, map->start, map->last,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|157| <<vdpasim_do_reset>> vhost_iotlb_add_range(&vdpasim->iommu[i], 0, ULONG_MAX,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|266| <<vdpasim_create>> vhost_iotlb_add_range(&vdpasim->iommu[i], 0, ULONG_MAX, 0,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|649| <<vdpasim_set_map>> ret = vhost_iotlb_add_range(iommu, map->start,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|674| <<vdpasim_reset_map>> vhost_iotlb_add_range(&vdpasim->iommu[asid], 0, ULONG_MAX,
+		 *   - drivers/vhost/vhost.c|1123| <<vhost_process_iotlb_msg>> if (vhost_iotlb_add_range(dev->iotlb, msg->iova,
+		 *   - drivers/vhost/vhost.c|1481| <<vhost_set_memory>> if (vhost_iotlb_add_range(newumem,
+		 */
 		if (vhost_iotlb_add_range(dev->iotlb, msg->iova,
 					  msg->iova + msg->size - 1,
 					  msg->uaddr, msg->perm)) {
@@ -1443,10 +1739,38 @@ bool vhost_vq_access_ok(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_access_ok);
 
+/*
+ * 处理VHOST_SET_MEM_TABLE:
+ *   - drivers/vhost/vhost.c|1803| <<vhost_dev_ioctl(VHOST_SET_MEM_TABLE)>> r = vhost_set_memory(d, argp);
+ */
 static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 {
+	/*
+	 * struct vhost_memory {
+	 *     __u32 nregions;
+	 *     __u32 padding;
+	 *     struct vhost_memory_region regions[0]
+	 * };
+	 */
 	struct vhost_memory mem, *newmem;
+	/*
+	 * struct vhost_memory_region {
+	 *     __u64 guest_phys_addr;
+	 *     __u64 memory_size; // bytes
+	 *     __u64 userspace_addr;
+	 *     __u64 flags_padding; // No flags are currently specified.
+	 * };
+	 */
 	struct vhost_memory_region *region;
+	/*
+	 * struct vhost_iotlb {
+	 *     struct rb_root_cached root;
+	 *     struct list_head list;
+	 *     unsigned int limit;
+	 *     unsigned int nmaps;
+	 *     unsigned int flags;
+	 * };
+	 */
 	struct vhost_iotlb *newumem, *oldumem;
 	unsigned long size = offsetof(struct vhost_memory, regions);
 	int i;
@@ -1457,6 +1781,9 @@ static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 		return -EOPNOTSUPP;
 	if (mem.nregions > max_mem_regions)
 		return -E2BIG;
+	/*
+	 * 这里newmem是vhost_memory类型
+	 */
 	newmem = kvzalloc(struct_size(newmem, regions, mem.nregions),
 			GFP_KERNEL);
 	if (!newmem)
@@ -1469,6 +1796,12 @@ static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 		return -EFAULT;
 	}
 
+	/*
+	 * 在以下使用iotlb_alloc():
+	 *   - drivers/vhost/vhost.c|647| <<vhost_dev_reset_owner_prepare>> return iotlb_alloc();
+	 *   - drivers/vhost/vhost.c|1472| <<vhost_set_memory>> newumem = iotlb_alloc();
+	 *   - drivers/vhost/vhost.c|1760| <<vhost_init_device_iotlb>> niotlb = iotlb_alloc();
+	 */
 	newumem = iotlb_alloc();
 	if (!newumem) {
 		kvfree(newmem);
@@ -1478,6 +1811,17 @@ static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 	for (region = newmem->regions;
 	     region < newmem->regions + mem.nregions;
 	     region++) {
+		/*
+		 * 在以下使用vhost_iotlb_add_range():
+		 *   - drivers/vdpa/mlx5/core/mr.c|609| <<dup_iotlb>> err = vhost_iotlb_add_range(dst, start, last, start, VHOST_ACCESS_RW);
+		 *   - drivers/vdpa/mlx5/core/mr.c|615| <<dup_iotlb>> err = vhost_iotlb_add_range(dst, map->start, map->last,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|157| <<vdpasim_do_reset>> vhost_iotlb_add_range(&vdpasim->iommu[i], 0, ULONG_MAX,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|266| <<vdpasim_create>> vhost_iotlb_add_range(&vdpasim->iommu[i], 0, ULONG_MAX, 0,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|649| <<vdpasim_set_map>> ret = vhost_iotlb_add_range(iommu, map->start,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|674| <<vdpasim_reset_map>> vhost_iotlb_add_range(&vdpasim->iommu[asid], 0, ULONG_MAX,
+		 *   - drivers/vhost/vhost.c|1123| <<vhost_process_iotlb_msg>> if (vhost_iotlb_add_range(dev->iotlb, msg->iova,
+		 *   - drivers/vhost/vhost.c|1481| <<vhost_set_memory>> if (vhost_iotlb_add_range(newumem,
+		 */
 		if (vhost_iotlb_add_range(newumem,
 					  region->guest_phys_addr,
 					  region->guest_phys_addr +
@@ -1490,6 +1834,11 @@ static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 	if (!memory_access_ok(d, newumem, 0))
 		goto err;
 
+	/*
+	 * struct vhost_dev *d:
+	 * -> struct vhost_iotlb *umem;
+	 * -> struct vhost_iotlb *iotlb;
+	 */
 	oldumem = d->umem;
 	d->umem = newumem;
 
@@ -1607,6 +1956,9 @@ static long vhost_vring_set_num_addr(struct vhost_dev *d,
 
 	return r;
 }
+/*
+ * 在以下使用vhost_vring_ioctl()
+ */
 long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 {
 	struct file *eventfp, *filep = NULL;
@@ -1752,11 +2104,22 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 }
 EXPORT_SYMBOL_GPL(vhost_vring_ioctl);
 
+/*
+ * 在以下使用vhost_init_device_iotlb():
+ *   - drivers/vhost/net.c|1659| <<vhost_net_set_features>> if (vhost_init_device_iotlb(&n->dev))
+ *   - drivers/vhost/vsock.c|842| <<vhost_vsock_set_features>> if (vhost_init_device_iotlb(&vsock->dev))
+ */
 int vhost_init_device_iotlb(struct vhost_dev *d)
 {
 	struct vhost_iotlb *niotlb, *oiotlb;
 	int i;
 
+	/*
+	 * 在以下使用iotlb_alloc():
+	 *   - drivers/vhost/vhost.c|647| <<vhost_dev_reset_owner_prepare>> return iotlb_alloc();
+	 *   - drivers/vhost/vhost.c|1472| <<vhost_set_memory>> newumem = iotlb_alloc();
+	 *   - drivers/vhost/vhost.c|1760| <<vhost_init_device_iotlb>> niotlb = iotlb_alloc();
+	 */
 	niotlb = iotlb_alloc();
 	if (!niotlb)
 		return -ENOMEM;
@@ -1789,6 +2152,11 @@ long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 
 	/* If you are not the owner, you can become one */
 	if (ioctl == VHOST_SET_OWNER) {
+		/*
+		 * 在以下使用vhost_dev_set_owner():
+		 *   - drivers/vhost/net.c|1714| <<vhost_net_set_owner>> r = vhost_dev_set_owner(&n->dev);
+		 *   - drivers/vhost/vhost.c|2011| <<vhost_dev_ioctl(VHOST_SET_OWNER)>> r = vhost_dev_set_owner(d);
+		 */
 		r = vhost_dev_set_owner(d);
 		goto done;
 	}
@@ -1947,6 +2315,25 @@ static int log_used(struct vhost_virtqueue *vq, u64 used_offset, u64 len)
 	if (!vq->iotlb)
 		return log_write(vq->log_base, vq->log_addr + used_offset, len);
 
+	/*
+	 * 在以下使用translate_desc():
+	 *   - drivers/vhost/vhost.c|847| <<vhost_copy_to_user>> ret = translate_desc(vq,
+	 *       (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|882| <<vhost_copy_from_user>> ret = translate_desc(vq,
+	 *       (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|907| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+	 *       (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|1950| <<log_used>> ret = translate_desc(vq,
+	 *       (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|2166| <<get_indirect>> ret = translate_desc(vq,
+	 *       vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV, VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2207| <<get_indirect>> ret = translate_desc(vq,
+	 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *       iov + iov_count, iov_size - iov_count, access);
+	 *   - drivers/vhost/vhost.c|2347| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+	 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *       iov + iov_count, iov_size - iov_count, access);
+	 */
 	ret = translate_desc(vq, (uintptr_t)vq->used + used_offset,
 			     len, iov, 64, VHOST_ACCESS_WO);
 	if (ret < 0)
@@ -2046,6 +2433,14 @@ static int vhost_update_avail_event(struct vhost_virtqueue *vq, u16 avail_event)
 	return 0;
 }
 
+/*
+ * 在以下使用vhost_vq_init_access():
+ *   - drivers/vhost/net.c|1552| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/scsi.c|2126| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+ *   - drivers/vhost/test.c|204| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+ *   - drivers/vhost/test.c|297| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/vsock.c|604| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+ */
 int vhost_vq_init_access(struct vhost_virtqueue *vq)
 {
 	__virtio16 last_used_idx;
@@ -2081,6 +2476,25 @@ int vhost_vq_init_access(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_init_access);
 
+/*
+ * 在以下使用translate_desc():
+ *   - drivers/vhost/vhost.c|847| <<vhost_copy_to_user>> ret = translate_desc(vq,
+ *       (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_WO);
+ *   - drivers/vhost/vhost.c|882| <<vhost_copy_from_user>> ret = translate_desc(vq,
+ *       (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|907| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+ *       (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|1950| <<log_used>> ret = translate_desc(vq,
+ *       (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+ *   - drivers/vhost/vhost.c|2166| <<get_indirect>> ret = translate_desc(vq,
+ *       vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV, VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|2207| <<get_indirect>> ret = translate_desc(vq,
+ *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+ *       iov + iov_count, iov_size - iov_count, access);
+ *   - drivers/vhost/vhost.c|2347| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+ *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+ *       iov + iov_count, iov_size - iov_count, access);
+ */
 static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			  struct iovec iov[], int iov_size, int access)
 {
@@ -2142,6 +2556,11 @@ static unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)
 	return next;
 }
 
+/*
+ * 在以下使用get_indirect():
+ *   - drivers/vhost/vhost.c|2331| <<vhost_get_vq_desc>> ret = get_indirect(vq,
+ *       iov, iov_size, out_num, in_num, log, log_num, &desc);
+ */
 static int get_indirect(struct vhost_virtqueue *vq,
 			struct iovec iov[], unsigned int iov_size,
 			unsigned int *out_num, unsigned int *in_num,
@@ -2163,6 +2582,25 @@ static int get_indirect(struct vhost_virtqueue *vq,
 		return -EINVAL;
 	}
 
+	/*
+	 * 在以下使用translate_desc():
+	 *   - drivers/vhost/vhost.c|847| <<vhost_copy_to_user>> ret = translate_desc(vq,
+	 *       (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|882| <<vhost_copy_from_user>> ret = translate_desc(vq,
+	 *       (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|907| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+	 *       (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|1950| <<log_used>> ret = translate_desc(vq,
+	 *       (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|2166| <<get_indirect>> ret = translate_desc(vq,
+	 *       vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV, VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2207| <<get_indirect>> ret = translate_desc(vq,
+	 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *       iov + iov_count, iov_size - iov_count, access);
+	 *   - drivers/vhost/vhost.c|2347| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+	 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *       iov + iov_count, iov_size - iov_count, access);
+	 */
 	ret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect,
 			     UIO_MAXIOV, VHOST_ACCESS_RO);
 	if (unlikely(ret < 0)) {
@@ -2204,6 +2642,25 @@ static int get_indirect(struct vhost_virtqueue *vq,
 		else
 			access = VHOST_ACCESS_RO;
 
+		/*
+		 * 在以下使用translate_desc():
+		 *   - drivers/vhost/vhost.c|847| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|882| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|907| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1950| <<log_used>> ret = translate_desc(vq,
+		 *       (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2166| <<get_indirect>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV, VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2207| <<get_indirect>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *       iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2347| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *       iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
 				     iov_size - iov_count, access);
@@ -2243,6 +2700,25 @@ static int get_indirect(struct vhost_virtqueue *vq,
  * This function returns the descriptor number found, or vq->num (which is
  * never a valid descriptor number) if none was found.  A negative code is
  * returned on error. */
+/*
+ * 在以下使用vhost_get_vq_desc():
+ *   - drivers/vhost/net.c|579| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(tvq,
+ *             tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|591| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(tvq,
+ *             tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|1067| <<get_rx_bufs>> r = vhost_get_vq_desc(vq,
+ *             vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+ *   - drivers/vhost/scsi.c|616| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq,
+ *             vq->iov, ARRAY_SIZE(vq->iov), &out, &in, vq_log, &log_num);
+ *   - drivers/vhost/scsi.c|1171| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq,
+ *             vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, log, log_num);
+ *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(vq,
+ *             vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|131| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq,
+ *             vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|524| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq,
+ *             vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ */
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
@@ -2259,6 +2735,12 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 	last_avail_idx = vq->last_avail_idx;
 
 	if (vq->avail_idx == vq->last_avail_idx) {
+		/*
+		 * 在以下使用vhost_get_avail_idx():
+		 *   - drivers/vhost/vhost.c|2262| <<vhost_get_vq_desc>> if (unlikely(vhost_get_avail_idx(vq, &avail_idx))) {
+		 *   - drivers/vhost/vhost.c|2551| <<vhost_vq_avail_empty>> r = vhost_get_avail_idx(vq, &avail_idx);
+		 *   - drivers/vhost/vhost.c|2597| <<vhost_enable_notify>> r = vhost_get_avail_idx(vq, &avail_idx);
+		 */
 		if (unlikely(vhost_get_avail_idx(vq, &avail_idx))) {
 			vq_err(vq, "Failed to access avail idx at %p\n",
 				&vq->avail->idx);
@@ -2328,6 +2810,9 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			return -EFAULT;
 		}
 		if (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT)) {
+			/*
+			 * 只在这一个地方调用get_indirect()
+			 */
 			ret = get_indirect(vq, iov, iov_size,
 					   out_num, in_num,
 					   log, log_num, &desc);
@@ -2344,6 +2829,25 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			access = VHOST_ACCESS_WO;
 		else
 			access = VHOST_ACCESS_RO;
+		/*
+		 * 在以下使用translate_desc():
+		 *   - drivers/vhost/vhost.c|847| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|882| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|907| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *       (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov), VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1950| <<log_used>> ret = translate_desc(vq,
+		 *       (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2166| <<get_indirect>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV, VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2207| <<get_indirect>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *       iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2347| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *       vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *       iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
 				     iov_size - iov_count, access);
@@ -2548,6 +3052,12 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 	if (vq->avail_idx != vq->last_avail_idx)
 		return false;
 
+	/*
+	 * 在以下使用vhost_get_avail_idx():
+	 *   - drivers/vhost/vhost.c|2262| <<vhost_get_vq_desc>> if (unlikely(vhost_get_avail_idx(vq, &avail_idx))) {
+	 *   - drivers/vhost/vhost.c|2551| <<vhost_vq_avail_empty>> r = vhost_get_avail_idx(vq, &avail_idx);
+	 *   - drivers/vhost/vhost.c|2597| <<vhost_enable_notify>> r = vhost_get_avail_idx(vq, &avail_idx);
+	 */
 	r = vhost_get_avail_idx(vq, &avail_idx);
 	if (unlikely(r))
 		return false;
@@ -2594,6 +3104,12 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 	/* They could have slipped one in as we were doing that: make
 	 * sure it's written, then check again. */
 	smp_mb();
+	/*
+	 * 在以下使用vhost_get_avail_idx():
+	 *   - drivers/vhost/vhost.c|2262| <<vhost_get_vq_desc>> if (unlikely(vhost_get_avail_idx(vq, &avail_idx))) {
+	 *   - drivers/vhost/vhost.c|2551| <<vhost_vq_avail_empty>> r = vhost_get_avail_idx(vq, &avail_idx);
+	 *   - drivers/vhost/vhost.c|2597| <<vhost_enable_notify>> r = vhost_get_avail_idx(vq, &avail_idx);
+	 */
 	r = vhost_get_avail_idx(vq, &avail_idx);
 	if (r) {
 		vq_err(vq, "Failed to check avail idx at %p: %d\n",
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index 50b909feeb8c..1b0e1f3211ce 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -152,6 +152,14 @@ struct vhost_dev {
 	struct vhost_virtqueue **vqs;
 	int nvqs;
 	struct eventfd_ctx *log_ctx;
+	/*
+	 * 在以下使用vhost_dev->work_list:
+	 *   - drivers/vhost/vhost.c|543| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|290| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+	 *   - drivers/vhost/vhost.c|299| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|401| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|813| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+	 */
 	struct llist_head work_list;
 	struct task_struct *worker;
 	struct vhost_iotlb *umem;
@@ -255,6 +263,22 @@ enum {
  *
  * Context: Need to call with vq->mutex acquired.
  */
+/*
+ * 在以下使用vhost_vq_set_backend():
+ *   - drivers/vhost/net.c|1368| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/net.c|1550| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+ *   - drivers/vhost/net.c|1599| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+ *   - drivers/vhost/scsi.c|2147| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+ *   - drivers/vhost/scsi.c|2260| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|137| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|202| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+ *   - drivers/vhost/test.c|294| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|296| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+ *   - drivers/vhost/vsock.c|603| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+ *   - drivers/vhost/vsock.c|629| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/vsock.c|636| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/vsock.c|661| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+ */
 static inline void vhost_vq_set_backend(struct vhost_virtqueue *vq,
 					void *private_data)
 {
diff --git a/drivers/vhost/vsock.c b/drivers/vhost/vsock.c
index d9b5bbd5f876..35d97be0142e 100644
--- a/drivers/vhost/vsock.c
+++ b/drivers/vhost/vsock.c
@@ -601,6 +601,14 @@ static int vhost_vsock_start(struct vhost_vsock *vsock)
 
 		if (!vhost_vq_get_backend(vq)) {
 			vhost_vq_set_backend(vq, vsock);
+			/*
+			 * 在以下使用vhost_vq_init_access():
+			 *   - drivers/vhost/net.c|1552| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/scsi.c|2126| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+			 *   - drivers/vhost/test.c|204| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+			 *   - drivers/vhost/test.c|297| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/vsock.c|604| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+			 */
 			ret = vhost_vq_init_access(vq);
 			if (ret)
 				goto err_vq;
@@ -700,6 +708,16 @@ static int vhost_vsock_dev_open(struct inode *inode, struct file *file)
 	file->private_data = vsock;
 	spin_lock_init(&vsock->send_pkt_list_lock);
 	INIT_LIST_HEAD(&vsock->send_pkt_list);
+	/*
+	 * 在以下使用vhost_work_init():
+	 *   - drivers/vhost/scsi.c|2427| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+	 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 *   - drivers/vhost/scsi.c|2662| <<vhost_scsi_port_link>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 *   - drivers/vhost/vhost.c|203| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+	 *   - drivers/vhost/vhost.c|253| <<vhost_work_dev_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+	 *   - drivers/vhost/vhost.c|592| <<vhost_attach_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+	 *   - drivers/vhost/vsock.c|711| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+	 */
 	vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
 	return 0;
 
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 3e5b14e97a43..ab19d63729e3 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -91,6 +91,11 @@ struct vring_virtqueue {
 	/* Is this a packed ring? */
 	bool packed_ring;
 
+	/*
+	 * 在以下使用vring_virtqueue->use_dma_api:
+	 *   - drivers/virtio/virtio_ring.c|1744| <<__vring_new_virtqueue>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 *   - drivers/virtio/virtio_ring.c|2229| <<vring_create_virtqueue_packed>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 */
 	/* Is DMA API used? */
 	bool use_dma_api;
 
@@ -331,6 +336,13 @@ static inline struct device *vring_dma_dev(const struct vring_virtqueue *vq)
 	return vq->dma_dev;
 }
 
+/*
+ * 在以下使用vring_map_one_sg():
+ *   - drivers/virtio/virtio_ring.c|551| <<virtqueue_add_split>> dma_addr_t addr = vring_map_one_sg(vq, sg, DMA_TO_DEVICE);
+ *   - drivers/virtio/virtio_ring.c|566| <<virtqueue_add_split>> dma_addr_t addr = vring_map_one_sg(vq, sg, DMA_FROM_DEVICE);
+ *   - drivers/virtio/virtio_ring.c|1100| <<virtqueue_add_indirect_packed>> addr = vring_map_one_sg(vq, sg, n < out_sgs ?
+ *   - drivers/virtio/virtio_ring.c|1247| <<virtqueue_add_packed>> dma_addr_t addr = vring_map_one_sg(vq, sg, n < out_sgs ?
+ */
 /* Map one sg entry. */
 static dma_addr_t vring_map_one_sg(const struct vring_virtqueue *vq,
 				   struct scatterlist *sg,
@@ -428,6 +440,10 @@ static unsigned int vring_unmap_one_split(const struct vring_virtqueue *vq,
 	return extra[i].next;
 }
 
+/*
+ * 在以下使用alloc_indirect_split():
+ *   - drivers/virtio/virtio_ring.c|516| <<virtqueue_add_split>> desc = alloc_indirect_split(_vq, total_sg, gfp);
+ */
 static struct vring_desc *alloc_indirect_split(struct virtqueue *_vq,
 					       unsigned int total_sg,
 					       gfp_t gfp)
@@ -451,6 +467,12 @@ static struct vring_desc *alloc_indirect_split(struct virtqueue *_vq,
 	return desc;
 }
 
+/*
+ * 在以下使用virtqueue_add_desc_split():
+ *   - drivers/virtio/virtio_ring.c|559| <<virtqueue_add_split>> i = virtqueue_add_desc_split(_vq, desc, i, addr, sg->length,
+ *   - drivers/virtio/virtio_ring.c|574| <<virtqueue_add_split>> i = virtqueue_add_desc_split(_vq, desc, i, addr,
+ *   - drivers/virtio/virtio_ring.c|595| <<virtqueue_add_split>> virtqueue_add_desc_split(_vq, vq->split.vring.desc,
+ */
 static inline unsigned int virtqueue_add_desc_split(struct virtqueue *vq,
 						    struct vring_desc *desc,
 						    unsigned int i,
@@ -480,6 +502,13 @@ static inline unsigned int virtqueue_add_desc_split(struct virtqueue *vq,
 	return next;
 }
 
+/*
+ * 在以下使用virtqueue_add_split():
+ *   - drivers/virtio/virtio_ring.c|1836| <<virtqueue_add>> virtqueue_add_split(_vq, sgs, total_sg,
+ *
+ * virtio-scsi的data是:
+ * struct virtio_scsi_cmd *cmd
+ */
 static inline int virtqueue_add_split(struct virtqueue *_vq,
 				      struct scatterlist *sgs[],
 				      unsigned int total_sg,
@@ -556,6 +585,12 @@ static inline int virtqueue_add_split(struct virtqueue *_vq,
 			/* Note that we trust indirect descriptor
 			 * table since it use stream DMA mapping.
 			 */
+			/*
+			 * 在以下使用virtqueue_add_desc_split():
+			 *   - drivers/virtio/virtio_ring.c|559| <<virtqueue_add_split>> i = virtqueue_add_desc_split(_vq, desc, i, addr, sg->length,
+			 *   - drivers/virtio/virtio_ring.c|574| <<virtqueue_add_split>> i = virtqueue_add_desc_split(_vq, desc, i, addr,
+			 *   - drivers/virtio/virtio_ring.c|595| <<virtqueue_add_split>> virtqueue_add_desc_split(_vq, vq->split.vring.desc,
+			 */
 			i = virtqueue_add_desc_split(_vq, desc, i, addr, sg->length,
 						     VRING_DESC_F_NEXT,
 						     indirect);
@@ -571,6 +606,12 @@ static inline int virtqueue_add_split(struct virtqueue *_vq,
 			/* Note that we trust indirect descriptor
 			 * table since it use stream DMA mapping.
 			 */
+			/*
+			 * 在以下使用virtqueue_add_desc_split():
+			 *   - drivers/virtio/virtio_ring.c|559| <<virtqueue_add_split>> i = virtqueue_add_desc_split(_vq, desc, i, addr, sg->length,
+			 *   - drivers/virtio/virtio_ring.c|574| <<virtqueue_add_split>> i = virtqueue_add_desc_split(_vq, desc, i, addr,
+			 *   - drivers/virtio/virtio_ring.c|595| <<virtqueue_add_split>> virtqueue_add_desc_split(_vq, vq->split.vring.desc,
+			 */
 			i = virtqueue_add_desc_split(_vq, desc, i, addr,
 						     sg->length,
 						     VRING_DESC_F_NEXT |
@@ -580,6 +621,11 @@ static inline int virtqueue_add_split(struct virtqueue *_vq,
 	}
 	/* Last one doesn't continue. */
 	desc[prev].flags &= cpu_to_virtio16(_vq->vdev, ~VRING_DESC_F_NEXT);
+	/*
+	 * 在以下使用vring_virtqueue->use_dma_api:
+	 *   - drivers/virtio/virtio_ring.c|1744| <<__vring_new_virtqueue>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 *   - drivers/virtio/virtio_ring.c|2229| <<vring_create_virtqueue_packed>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 */
 	if (!indirect && vq->use_dma_api)
 		vq->split.desc_extra[prev & (vq->split.vring.num - 1)].flags &=
 			~VRING_DESC_F_NEXT;
@@ -592,6 +638,14 @@ static inline int virtqueue_add_split(struct virtqueue *_vq,
 		if (vring_mapping_error(vq, addr))
 			goto unmap_release;
 
+		/*
+		 * 在以下使用virtqueue_add_desc_split():
+		 *   - drivers/virtio/virtio_ring.c|559| <<virtqueue_add_split>> i = virtqueue_add_desc_split(_vq, desc, i, addr, sg->length,
+		 *   - drivers/virtio/virtio_ring.c|574| <<virtqueue_add_split>> i = virtqueue_add_desc_split(_vq, desc, i, addr,
+		 *   - drivers/virtio/virtio_ring.c|595| <<virtqueue_add_split>> virtqueue_add_desc_split(_vq, vq->split.vring.desc,
+		 *
+		 * head是desc的index
+		 */
 		virtqueue_add_desc_split(_vq, vq->split.vring.desc,
 					 head, addr,
 					 total_sg * sizeof(struct vring_desc),
@@ -694,6 +748,11 @@ static bool virtqueue_kick_prepare_split(struct virtqueue *_vq)
 	return needs_kick;
 }
 
+/*
+ * 在以下使用detach_buf_split():
+ *   - drivers/virtio/virtio_ring.c|849| <<virtqueue_get_buf_ctx_split>> detach_buf_split(vq, i, ctx);
+ *   - drivers/virtio/virtio_ring.c|975| <<virtqueue_detach_unused_buf_split>> detach_buf_split(vq, i, NULL);
+ */
 static void detach_buf_split(struct vring_virtqueue *vq, unsigned int head,
 			     void **ctx)
 {
@@ -750,6 +809,9 @@ static inline bool more_used_split(const struct vring_virtqueue *vq)
 			vq->split.vring.used->idx);
 }
 
+/*
+ * virtio-scsi的done过来ctx似乎是NULL
+ */
 static void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,
 					 unsigned int *len,
 					 void **ctx)
@@ -792,6 +854,11 @@ static void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,
 
 	/* detach_buf_split clears data, so grab it now. */
 	ret = vq->split.desc_state[i].data;
+	/*
+	 * 在以下使用detach_buf_split():
+	 *   - drivers/virtio/virtio_ring.c|849| <<virtqueue_get_buf_ctx_split>> detach_buf_split(vq, i, ctx);
+	 *   - drivers/virtio/virtio_ring.c|975| <<virtqueue_detach_unused_buf_split>> detach_buf_split(vq, i, NULL);
+	 */
 	detach_buf_split(vq, i, ctx);
 	vq->last_used_idx++;
 	/* If we expect an interrupt for the next entry, tell host
@@ -901,6 +968,10 @@ static bool virtqueue_enable_cb_delayed_split(struct virtqueue *_vq)
 	return true;
 }
 
+/*
+ * 在以下使用virtqueue_detach_unused_buf_split():
+ *   - drivers/virtio/virtio_ring.c|2162| <<virtqueue_detach_unused_buf>> virtqueue_detach_unused_buf_split(_vq);
+ */
 static void *virtqueue_detach_unused_buf_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -1820,6 +1891,16 @@ static struct virtqueue *vring_create_virtqueue_packed(
  * Generic functions and exported symbols.
  */
 
+/*
+ * 在以下使用virtqueue_add():
+ *   - drivers/virtio/virtio_ring.c|1870| <<virtqueue_add_sgs>> return virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|1893| <<virtqueue_add_outbuf>> return virtqueue_add(vq, &sg, num, 1, 0, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|1915| <<virtqueue_add_inbuf>> return virtqueue_add(vq, &sg, num, 0, 1, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|1939| <<virtqueue_add_inbuf_ctx>> return virtqueue_add(vq, &sg, num, 0, 1, data, ctx, gfp);
+ *
+ * virtio-scsi的data是: 
+ * struct virtio_scsi_cmd *cmd
+ */
 static inline int virtqueue_add(struct virtqueue *_vq,
 				struct scatterlist *sgs[],
 				unsigned int total_sg,
@@ -1867,6 +1948,16 @@ int virtqueue_add_sgs(struct virtqueue *_vq,
 		for (sg = sgs[i]; sg; sg = sg_next(sg))
 			total_sg++;
 	}
+	/*
+	 * 在以下使用virtqueue_add():
+	 *   - drivers/virtio/virtio_ring.c|1870| <<virtqueue_add_sgs>> return virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1893| <<virtqueue_add_outbuf>> return virtqueue_add(vq, &sg, num, 1, 0, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1915| <<virtqueue_add_inbuf>> return virtqueue_add(vq, &sg, num, 0, 1, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1939| <<virtqueue_add_inbuf_ctx>> return virtqueue_add(vq, &sg, num, 0, 1, data, ctx, gfp);
+	 *
+	 * virtio-scsi的data是:
+	 * struct virtio_scsi_cmd *cmd
+	 */
 	return virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs,
 			     data, NULL, gfp);
 }
@@ -1890,6 +1981,13 @@ int virtqueue_add_outbuf(struct virtqueue *vq,
 			 void *data,
 			 gfp_t gfp)
 {
+	/*
+	 * 在以下使用virtqueue_add():
+	 *   - drivers/virtio/virtio_ring.c|1870| <<virtqueue_add_sgs>> return virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1893| <<virtqueue_add_outbuf>> return virtqueue_add(vq, &sg, num, 1, 0, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1915| <<virtqueue_add_inbuf>> return virtqueue_add(vq, &sg, num, 0, 1, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1939| <<virtqueue_add_inbuf_ctx>> return virtqueue_add(vq, &sg, num, 0, 1, data, ctx, gfp);
+	 */
 	return virtqueue_add(vq, &sg, num, 1, 0, data, NULL, gfp);
 }
 EXPORT_SYMBOL_GPL(virtqueue_add_outbuf);
@@ -1912,6 +2010,13 @@ int virtqueue_add_inbuf(struct virtqueue *vq,
 			void *data,
 			gfp_t gfp)
 {
+	/*
+	 * 在以下使用virtqueue_add():
+	 *   - drivers/virtio/virtio_ring.c|1870| <<virtqueue_add_sgs>> return virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1893| <<virtqueue_add_outbuf>> return virtqueue_add(vq, &sg, num, 1, 0, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1915| <<virtqueue_add_inbuf>> return virtqueue_add(vq, &sg, num, 0, 1, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1939| <<virtqueue_add_inbuf_ctx>> return virtqueue_add(vq, &sg, num, 0, 1, data, ctx, gfp);
+	 */
 	return virtqueue_add(vq, &sg, num, 0, 1, data, NULL, gfp);
 }
 EXPORT_SYMBOL_GPL(virtqueue_add_inbuf);
@@ -1936,6 +2041,13 @@ int virtqueue_add_inbuf_ctx(struct virtqueue *vq,
 			void *ctx,
 			gfp_t gfp)
 {
+	/*
+	 * 在以下使用virtqueue_add():
+	 *   - drivers/virtio/virtio_ring.c|1870| <<virtqueue_add_sgs>> return virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1893| <<virtqueue_add_outbuf>> return virtqueue_add(vq, &sg, num, 1, 0, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1915| <<virtqueue_add_inbuf>> return virtqueue_add(vq, &sg, num, 0, 1, data, NULL, gfp);
+	 *   - drivers/virtio/virtio_ring.c|1939| <<virtqueue_add_inbuf_ctx>> return virtqueue_add(vq, &sg, num, 0, 1, data, ctx, gfp);
+	 */
 	return virtqueue_add(vq, &sg, num, 0, 1, data, ctx, gfp);
 }
 EXPORT_SYMBOL_GPL(virtqueue_add_inbuf_ctx);
@@ -2031,6 +2143,9 @@ void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 }
 EXPORT_SYMBOL_GPL(virtqueue_get_buf_ctx);
 
+/*
+ * 很多调用
+ */
 void *virtqueue_get_buf(struct virtqueue *_vq, unsigned int *len)
 {
 	return virtqueue_get_buf_ctx(_vq, len, NULL);
@@ -2154,6 +2269,19 @@ EXPORT_SYMBOL_GPL(virtqueue_enable_cb_delayed);
  * This is not valid on an active queue; it is useful only for device
  * shutdown.
  */
+/*
+ * 在以下使用virtqueue_detach_unused_buf():
+ *   - drivers/bluetooth/virtio_bt.c|73| <<virtbt_close>> while ((skb = virtqueue_detach_unused_buf(vq)))
+ *   - drivers/char/virtio_console.c|1941| <<remove_vqs>> while ((buf = virtqueue_detach_unused_buf(vq)))
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|427| <<virtcrypto_free_unused_reqs>> while ((vc_req = virtqueue_detach_unused_buf(vq)) != NULL) {
+ *   - drivers/net/caif/caif_virtio.c|469| <<cfv_netdev_close>> while ((buf_info = virtqueue_detach_unused_buf(cfv->vq_tx)))
+ *   - drivers/net/virtio_net.c|2978| <<free_unused_bufs>> while ((buf = virtqueue_detach_unused_buf(vq)) != NULL)
+ *   - drivers/net/virtio_net.c|2985| <<free_unused_bufs>> while ((buf = virtqueue_detach_unused_buf(vq)) != NULL)
+ *   - drivers/net/wireless/mac80211_hwsim.c|4410| <<remove_vqs>> while ((skb = virtqueue_detach_unused_buf(vq)))
+ *   - drivers/virtio/virtio_input.c|351| <<virtinput_remove>> while ((buf = virtqueue_detach_unused_buf(vi->sts)) != NULL)
+ *   - net/vmw_vsock/virtio_transport.c|654| <<virtio_vsock_vqs_del>> while ((pkt = virtqueue_detach_unused_buf(vsock->vqs[VSOCK_VQ_RX])))
+ *   - net/vmw_vsock/virtio_transport.c|659| <<virtio_vsock_vqs_del>> while ((pkt = virtqueue_detach_unused_buf(vsock->vqs[VSOCK_VQ_TX])))
+ */
 void *virtqueue_detach_unused_buf(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
diff --git a/include/linux/entry-kvm.h b/include/linux/entry-kvm.h
index 07c878d6e323..db1b2f70b21b 100644
--- a/include/linux/entry-kvm.h
+++ b/include/linux/entry-kvm.h
@@ -59,6 +59,10 @@ int xfer_to_guest_mode_handle_work(struct kvm_vcpu *vcpu);
  * Has to be invoked with interrupts disabled before the last call
  * to xfer_to_guest_mode_work_pending().
  */
+/*
+ * 在以下使用xfer_to_guest_mode_prepare():
+ *   - arch/x86/kvm/x86.c|2046| <<kvm_vcpu_exit_request>> xfer_to_guest_mode_prepare();
+ */
 static inline void xfer_to_guest_mode_prepare(void)
 {
 	lockdep_assert_irqs_disabled();
@@ -73,6 +77,12 @@ static inline void xfer_to_guest_mode_prepare(void)
  * Bare variant of xfer_to_guest_mode_work_pending(). Can be called from
  * interrupt enabled code for racy quick checks with care.
  */
+/*
+ * 在以下使用__xfer_to_guest_mode_work_pending():
+ *   - arch/x86/kvm/vmx/vmx.c|5573| <<handle_invalid_guest_state>> if (__xfer_to_guest_mode_work_pending())
+ *   - arch/x86/kvm/x86.c|10942| <<vcpu_run>> if (__xfer_to_guest_mode_work_pending()) {
+ *   - include/linux/entry-kvm.h|95| <<xfer_to_guest_mode_work_pending>> return __xfer_to_guest_mode_work_pending();
+ */
 static inline bool __xfer_to_guest_mode_work_pending(void)
 {
 	unsigned long ti_work = read_thread_flags();
@@ -89,9 +99,20 @@ static inline bool __xfer_to_guest_mode_work_pending(void)
  * Has to be invoked with interrupts disabled before the transition to
  * guest mode.
  */
+/*
+ * 在以下使用xfer_to_guest_mode_work_pending():
+ *   - arch/arm64/kvm/arm.c|755| <<kvm_vcpu_exit_request>> xfer_to_guest_mode_work_pending();
+ *   - arch/x86/kvm/x86.c|2048| <<kvm_vcpu_exit_request>> xfer_to_guest_mode_work_pending();
+ */
 static inline bool xfer_to_guest_mode_work_pending(void)
 {
 	lockdep_assert_irqs_disabled();
+	/*
+	 * 在以下使用__xfer_to_guest_mode_work_pending():
+	 *   - arch/x86/kvm/vmx/vmx.c|5573| <<handle_invalid_guest_state>> if (__xfer_to_guest_mode_work_pending())
+	 *   - arch/x86/kvm/x86.c|10942| <<vcpu_run>> if (__xfer_to_guest_mode_work_pending()) {
+	 *   - include/linux/entry-kvm.h|95| <<xfer_to_guest_mode_work_pending>> return __xfer_to_guest_mode_work_pending();
+	 */
 	return __xfer_to_guest_mode_work_pending();
 }
 #endif /* CONFIG_KVM_XFER_TO_GUEST_WORK */
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1b4b732a5caa..58db0bb47a85 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -519,6 +519,12 @@ static __always_inline void guest_state_exit_irqoff(void)
 	instrumentation_end();
 }
 
+/*
+ * 在以下使用kvm_vcpu_exiting_guest_mode():
+ *   - arch/arm64/kvm/arm.c|68| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+ *   - arch/x86/kvm/x86.c|12918| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+ *   - virt/kvm/kvm_main.c|227| <<kvm_request_needs_ipi>> int mode = kvm_vcpu_exiting_guest_mode(vcpu);
+ */
 static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -559,6 +565,19 @@ struct kvm_memory_slot {
 	gfn_t base_gfn;
 	unsigned long npages;
 	unsigned long *dirty_bitmap;
+	/*
+	 * 在以下使用kvm_memory_slot->present_bitmap:
+	 *   - arch/x86/kvm/mmu/mmu.c|1661| <<rmap_add>> if (slot && slot->present_bitmap) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1662| <<rmap_add>> bitmap_set(slot->present_bitmap, gfn - slot->base_gfn,
+	 *   - arch/x86/kvm/mmu/mmu.c|5880| <<kvm_rmap_walk_present>> atomic_long_t *p = (atomic_long_t *) &slot->present_bitmap[i];
+	 *   - virt/kvm/kvm_main.c|928| <<kvm_destroy_present_bitmap>> if (!memslot->present_bitmap)
+	 *   - virt/kvm/kvm_main.c|931| <<kvm_destroy_present_bitmap>> kvfree(memslot->present_bitmap);
+	 *   - virt/kvm/kvm_main.c|932| <<kvm_destroy_present_bitmap>> memslot->present_bitmap = NULL;
+	 *   - virt/kvm/kvm_main.c|1349| <<kvm_alloc_present_bitmap>> memslot->present_bitmap = __vcalloc(2, present_bytes, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|1350| <<kvm_alloc_present_bitmap>> if (!memslot->present_bitmap)
+	 *   - virt/kvm/kvm_main.c|1979| <<__kvm_set_memory_region>> new->present_bitmap = old->present_bitmap;
+	 *   - virt/kvm/kvm_main.c|1980| <<__kvm_set_memory_region>> if (!new->present_bitmap) {
+	 */
 	unsigned long *present_bitmap;
 	struct kvm_arch_memory_slot arch;
 	unsigned long userspace_addr;
@@ -567,6 +586,13 @@ struct kvm_memory_slot {
 	u16 as_id;
 };
 
+/*
+ * 在以下使用kvm_slot_dirty_track_enabled():
+ *   - arch/x86/kvm/mmu/mmu.c|898| <<gfn_to_memslot_dirty_bitmap>> if (no_dirty_log && kvm_slot_dirty_track_enabled(slot) &&
+ *   - arch/x86/kvm/mmu/mmu.c|2917| <<kvm_mmu_hugepage_adjust>> if (kvm_slot_dirty_track_enabled(slot))
+ *   - virt/kvm/kvm_main.c|3435| <<mark_page_dirty_in_slot>> if (memslot && kvm_slot_dirty_track_enabled(memslot)) {
+ *   - virt/kvm/kvm_main.c|3453| <<mark_page_range_dirty_in_slot>> kvm_slot_dirty_track_enabled((struct kvm_memory_slot *)memslot)) {
+ */
 static inline bool kvm_slot_dirty_track_enabled(struct kvm_memory_slot *slot)
 {
 	return slot->flags & KVM_MEM_LOG_DIRTY_PAGES;
@@ -748,6 +774,11 @@ struct kvm {
 #endif
 	struct list_head devices;
 	u64 manual_dirty_log_protect;
+	/*
+	 * 在以下使用kvm->dirty_log_pgtable:
+	 *   - include/linux/kvm_host.h|832| <<kvm_dirty_log_pgtable>> return !!(kvm->dirty_log_pgtable & KVM_DIRTY_LOG_PGTABLE);
+	 *   - virt/kvm/kvm_main.c|4674| <<kvm_vm_ioctl_enable_cap_generic>> kvm->dirty_log_pgtable = cap->args[0];
+	 */
 	u64 dirty_log_pgtable;
 	struct dentry *debugfs_dentry;
 	struct kvm_stat_data **debugfs_stat_data;
@@ -755,6 +786,10 @@ struct kvm {
 	struct srcu_struct irq_srcu;
 	pid_t userspace_pid;
 	unsigned int max_halt_poll_ns;
+	/*
+	 * 在以下设置kvm->dirty_ring_size:
+	 *   - virt/kvm/kvm_main.c|4689| <<kvm_vm_ioctl_enable_dirty_log_ring>> kvm->dirty_ring_size = size;
+	 */
 	u32 dirty_ring_size;
 	bool vm_bugged;
 	bool vm_dead;
@@ -827,8 +862,22 @@ static inline bool kvm_dirty_log_manual_protect_and_init_set(struct kvm *kvm)
 	return !!(kvm->manual_dirty_log_protect & KVM_DIRTY_LOG_INITIALLY_SET);
 }
 
+/*
+ * 在以下使用kvm_dirty_log_pgtable():
+ *   - arch/x86/kvm/mmu/mmu.c|899| <<gfn_to_memslot_dirty_bitmap>> !kvm_dirty_log_pgtable(vcpu->kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1388| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> !kvm_dirty_log_pgtable(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|1401| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> if (kvm_dirty_log_pgtable(kvm) &&
+ *   - arch/x86/kvm/x86.c|6114| <<kvm_arch_sync_dirty_log>> if (kvm_dirty_log_pgtable(kvm)) {
+ *   - arch/x86/kvm/x86.c|12631| <<kvm_mmu_slot_apply_flags>> if (!kvm_dirty_log_pgtable(kvm))
+ *   - virt/kvm/kvm_main.c|2235| <<kvm_clear_dirty_log_protect>> if (!kvm_dirty_log_pgtable(kvm))
+ */
 static inline bool kvm_dirty_log_pgtable(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm->dirty_log_pgtable:
+	 *   - include/linux/kvm_host.h|832| <<kvm_dirty_log_pgtable>> return !!(kvm->dirty_log_pgtable & KVM_DIRTY_LOG_PGTABLE);
+	 *   - virt/kvm/kvm_main.c|4674| <<kvm_vm_ioctl_enable_cap_generic>> kvm->dirty_log_pgtable = cap->args[0];
+	 */
 	return !!(kvm->dirty_log_pgtable & KVM_DIRTY_LOG_PGTABLE);
 }
 
@@ -2095,6 +2144,10 @@ int kvm_vm_create_worker_thread(struct kvm *kvm, kvm_vm_thread_fn_t thread_fn,
 				struct task_struct **thread_ptr);
 
 #ifdef CONFIG_KVM_XFER_TO_GUEST_WORK
+/*
+ * 在以下使用kvm_handle_signal_exit():
+ *   - kernel/entry/kvm.c|15| <<xfer_to_guest_mode_work>> kvm_handle_signal_exit(vcpu);
+ */
 static inline void kvm_handle_signal_exit(struct kvm_vcpu *vcpu)
 {
 	vcpu->run->exit_reason = KVM_EXIT_INTR;
diff --git a/include/net/udp_tunnel.h b/include/net/udp_tunnel.h
index 05883e14c51d..57638c971647 100644
--- a/include/net/udp_tunnel.h
+++ b/include/net/udp_tunnel.h
@@ -51,6 +51,21 @@ static inline int udp_sock_create6(struct net *net, struct udp_port_cfg *cfg,
 }
 #endif
 
+/*
+ * 在以下使用udp_sock_create():
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|216| <<rxe_setup_udp_tunnel>> err = udp_sock_create(net, &udp_cfg, &sock);
+ *   - drivers/net/bareudp.c|241| <<bareudp_create_sock>> err = udp_sock_create(net, &udp_conf, &sock);
+ *   - drivers/net/geneve.c|493| <<geneve_create_sock>> err = udp_sock_create(net, &udp_conf, &sock);
+ *   - drivers/net/vxlan/vxlan_core.c|3479| <<vxlan_create_sock>> err = udp_sock_create(net, &udp_conf, &sock);
+ *   - drivers/net/wireguard/socket.c|387| <<wg_socket_init>> ret = udp_sock_create(net, &port4, &new4);
+ *   - drivers/net/wireguard/socket.c|398| <<wg_socket_init>> ret = udp_sock_create(net, &port6, &new6);
+ *   - net/ipv4/fou.c|582| <<fou_create>> err = udp_sock_create(net, &cfg->udp_config, &sock);
+ *   - net/l2tp/l2tp_core.c|1324| <<l2tp_tunnel_sock_create>> err = udp_sock_create(net, &udp_conf, &sock);
+ *   - net/rxrpc/local_object.c|132| <<rxrpc_open_socket>> ret = udp_sock_create(net, &udp_conf, &local->socket);
+ *   - net/sctp/protocol.c|869| <<sctp_udp_sock_start>> err = udp_sock_create(net, &udp_conf, &sock);
+ *   - net/sctp/protocol.c|889| <<sctp_udp_sock_start>> err = udp_sock_create(net, &udp_conf, &sock);
+ *   - net/tipc/udp_media.c|774| <<tipc_udp_enable>> err = udp_sock_create(net, &udp_conf, &ub->ubsock);
+ */
 static inline int udp_sock_create(struct net *net,
 				  struct udp_port_cfg *cfg,
 				  struct socket **sockp)
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index 1716899d8161..12be6c9a12ea 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -118,6 +118,10 @@ unsigned long swiotlb_size_or_default(void)
 	return default_nslabs << IO_TLB_SHIFT;
 }
 
+/*
+ * 在以下使用swiotlb_adjust_size():
+ *   - arch/x86/mm/mem_encrypt_amd.c|245| <<sev_setup_arch>> swiotlb_adjust_size(size);
+ */
 void __init swiotlb_adjust_size(unsigned long size)
 {
 	/*
diff --git a/kernel/entry/kvm.c b/kernel/entry/kvm.c
index 96d476e06c77..92e0136d18bd 100644
--- a/kernel/entry/kvm.c
+++ b/kernel/entry/kvm.c
@@ -3,6 +3,10 @@
 #include <linux/entry-kvm.h>
 #include <linux/kvm_host.h>
 
+/*
+ * 在以下使用xfer_to_guest_mode_work():
+ *   - kernel/entry/kvm.c|50| <<xfer_to_guest_mode_handle_work>> return xfer_to_guest_mode_work(vcpu, ti_work);
+ */
 static int xfer_to_guest_mode_work(struct kvm_vcpu *vcpu, unsigned long ti_work)
 {
 	do {
@@ -31,6 +35,11 @@ static int xfer_to_guest_mode_work(struct kvm_vcpu *vcpu, unsigned long ti_work)
 	return 0;
 }
 
+/*
+ * 在以下使用xfer_to_guest_mode_handle_work():
+ *   - arch/arm64/kvm/arm.c|819| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+ *   - arch/x86/kvm/x86.c|10944| <<vcpu_run>> r = xfer_to_guest_mode_handle_work(vcpu);
+ */
 int xfer_to_guest_mode_handle_work(struct kvm_vcpu *vcpu)
 {
 	unsigned long ti_work;
diff --git a/kernel/time/posix-timers.c b/kernel/time/posix-timers.c
index 57d765a84892..df663a9d98e5 100644
--- a/kernel/time/posix-timers.c
+++ b/kernel/time/posix-timers.c
@@ -194,6 +194,18 @@ static int posix_clock_realtime_set(const clockid_t which_clock,
 static int posix_clock_realtime_adj(const clockid_t which_clock,
 				    struct __kernel_timex *t)
 {
+	/*
+	 * 在以下使用do_adjtimex():
+	 *  - arch/alpha/kernel/osf_sys.c|1200| <<SYSCALL_DEFINE1(old_adjtimex)>> ret = do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|597| <<stp_clear_leap>> ret = do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|603| <<stp_clear_leap>> return do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|643| <<stp_check_leap>> ret = do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|652| <<stp_check_leap>> ret = do_adjtimex(&txc);
+	 *  - arch/sparc/kernel/sys_sparc_64.c|569| <<SYSCALL_DEFINE1(sparc_adjtimex)>> ret = do_adjtimex(&txc);
+	 *  - kernel/time/posix-timers.c|197| <<posix_clock_realtime_adj>> return do_adjtimex(t);
+	 *  - kernel/time/time.c|280| <<SYSCALL_DEFINE1(adjtimex)>> ret = do_adjtimex(&txc);
+	 *  - kernel/time/time.c|358| <<SYSCALL_DEFINE1(adjtimex_time32)>> ret = do_adjtimex(&txc);
+	 */
 	return do_adjtimex(t);
 }
 
diff --git a/kernel/time/time.c b/kernel/time/time.c
index a7fce68465a3..ad52206f2702 100644
--- a/kernel/time/time.c
+++ b/kernel/time/time.c
@@ -277,6 +277,18 @@ SYSCALL_DEFINE1(adjtimex, struct __kernel_timex __user *, txc_p)
 	 */
 	if (copy_from_user(&txc, txc_p, sizeof(struct __kernel_timex)))
 		return -EFAULT;
+	/*
+	 * 在以下使用do_adjtimex():
+	 *  - arch/alpha/kernel/osf_sys.c|1200| <<SYSCALL_DEFINE1(old_adjtimex)>> ret = do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|597| <<stp_clear_leap>> ret = do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|603| <<stp_clear_leap>> return do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|643| <<stp_check_leap>> ret = do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|652| <<stp_check_leap>> ret = do_adjtimex(&txc);
+	 *  - arch/sparc/kernel/sys_sparc_64.c|569| <<SYSCALL_DEFINE1(sparc_adjtimex)>> ret = do_adjtimex(&txc);
+	 *  - kernel/time/posix-timers.c|197| <<posix_clock_realtime_adj>> return do_adjtimex(t);
+	 *  - kernel/time/time.c|280| <<SYSCALL_DEFINE1(adjtimex)>> ret = do_adjtimex(&txc);
+	 *  - kernel/time/time.c|358| <<SYSCALL_DEFINE1(adjtimex_time32)>> ret = do_adjtimex(&txc);
+	 */
 	ret = do_adjtimex(&txc);
 	return copy_to_user(txc_p, &txc, sizeof(struct __kernel_timex)) ? -EFAULT : ret;
 }
@@ -355,6 +367,18 @@ SYSCALL_DEFINE1(adjtimex_time32, struct old_timex32 __user *, utp)
 	if (err)
 		return err;
 
+	/*
+	 * 在以下使用do_adjtimex():
+	 *  - arch/alpha/kernel/osf_sys.c|1200| <<SYSCALL_DEFINE1(old_adjtimex)>> ret = do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|597| <<stp_clear_leap>> ret = do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|603| <<stp_clear_leap>> return do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|643| <<stp_check_leap>> ret = do_adjtimex(&txc);
+	 *  - arch/s390/kernel/time.c|652| <<stp_check_leap>> ret = do_adjtimex(&txc);
+	 *  - arch/sparc/kernel/sys_sparc_64.c|569| <<SYSCALL_DEFINE1(sparc_adjtimex)>> ret = do_adjtimex(&txc);
+	 *  - kernel/time/posix-timers.c|197| <<posix_clock_realtime_adj>> return do_adjtimex(t);
+	 *  - kernel/time/time.c|280| <<SYSCALL_DEFINE1(adjtimex)>> ret = do_adjtimex(&txc);
+	 *  - kernel/time/time.c|358| <<SYSCALL_DEFINE1(adjtimex_time32)>> ret = do_adjtimex(&txc);
+	 */
 	ret = do_adjtimex(&txc);
 
 	err = put_old_timex32(utp, &txc);
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index 07c949c10de2..7d379c3010ee 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -2130,6 +2130,11 @@ static u64 logarithmic_accumulation(struct timekeeper *tk, u64 offset,
  * timekeeping_advance - Updates the timekeeper to the current time and
  * current NTP tick length
  */
+/*
+ * 在以下使用timekeeping_advance():
+ *   - kernel/time/timekeeping.c|2214| <<update_wall_time>> if (timekeeping_advance(TK_ADV_TICK))
+ *   - kernel/time/timekeeping.c|2459| <<do_adjtimex>> clock_set |= timekeeping_advance(TK_ADV_FREQ);
+ */
 static bool timekeeping_advance(enum timekeeping_adv_mode mode)
 {
 	struct timekeeper *real_tk = &tk_core.timekeeper;
@@ -2399,6 +2404,18 @@ unsigned long random_get_entropy_fallback(void)
 }
 EXPORT_SYMBOL_GPL(random_get_entropy_fallback);
 
+/*
+ * 在以下使用do_adjtimex():
+ *  - arch/alpha/kernel/osf_sys.c|1200| <<SYSCALL_DEFINE1(old_adjtimex)>> ret = do_adjtimex(&txc); 
+ *  - arch/s390/kernel/time.c|597| <<stp_clear_leap>> ret = do_adjtimex(&txc);
+ *  - arch/s390/kernel/time.c|603| <<stp_clear_leap>> return do_adjtimex(&txc);
+ *  - arch/s390/kernel/time.c|643| <<stp_check_leap>> ret = do_adjtimex(&txc);
+ *  - arch/s390/kernel/time.c|652| <<stp_check_leap>> ret = do_adjtimex(&txc);
+ *  - arch/sparc/kernel/sys_sparc_64.c|569| <<SYSCALL_DEFINE1(sparc_adjtimex)>> ret = do_adjtimex(&txc);
+ *  - kernel/time/posix-timers.c|197| <<posix_clock_realtime_adj>> return do_adjtimex(t);
+ *  - kernel/time/time.c|280| <<SYSCALL_DEFINE1(adjtimex)>> ret = do_adjtimex(&txc);
+ *  - kernel/time/time.c|358| <<SYSCALL_DEFINE1(adjtimex_time32)>> ret = do_adjtimex(&txc);
+ */
 /**
  * do_adjtimex() - Accessor function to NTP __do_adjtimex function
  */
diff --git a/lib/vdso/gettimeofday.c b/lib/vdso/gettimeofday.c
index ce2f69552003..0d8c6c10baba 100644
--- a/lib/vdso/gettimeofday.c
+++ b/lib/vdso/gettimeofday.c
@@ -107,6 +107,11 @@ static __always_inline int do_hres_timens(const struct vdso_data *vdns, clockid_
 }
 #endif
 
+/*
+ * 在以下使用do_hres():
+ *   - lib/vdso/gettimeofday.c|251| <<__cvdso_clock_gettime_common>> return do_hres(vd, clock, ts);
+ *   - lib/vdso/gettimeofday.c|306| <<__cvdso_gettimeofday_data>> if (do_hres(&vd[CS_HRES_COARSE], CLOCK_REALTIME, &ts))
+ */
 static __always_inline int do_hres(const struct vdso_data *vd, clockid_t clk,
 				   struct __kernel_timespec *ts)
 {
diff --git a/net/ipv4/ip_output.c b/net/ipv4/ip_output.c
index dacc84da1a63..19cd1ca50365 100644
--- a/net/ipv4/ip_output.c
+++ b/net/ipv4/ip_output.c
@@ -117,6 +117,23 @@ int __ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb)
 		       dst_output);
 }
 
+/*
+ * 在以下使用ip_local_out():
+ *   - net/ipv4/ip_output.c|130| <<global>> EXPORT_SYMBOL_GPL(ip_local_out);
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|450| <<rxe_send>> err = ip_local_out(dev_net(skb_dst(skb)->dev), skb->sk, skb);
+ *   - drivers/net/ipvlan/ipvlan_core.c|443| <<ipvlan_process_v4_outbound>> err = ip_local_out(net, NULL, skb);
+ *   - drivers/net/ppp/pptp.c|261| <<pptp_xmit>> ip_local_out(net, skb->sk, skb);
+ *   - net/ipv4/igmp.c|427| <<igmpv3_sendpack>> return ip_local_out(dev_net(skb_dst(skb)->dev), skb->sk, skb);
+ *   - net/ipv4/igmp.c|796| <<igmp_send_report>> return ip_local_out(net, skb->sk, skb);
+ *   - net/ipv4/ip_output.c|190| <<ip_build_and_send_pkt>> return ip_local_out(net, skb->sk, skb);
+ *   - net/ipv4/ip_output.c|532| <<__ip_queue_xmit>> res = ip_local_out(net, sk, skb);
+ *   - net/ipv4/ip_output.c|1581| <<ip_send_skb>> err = ip_local_out(net, skb->sk, skb);
+ *   - net/ipv4/ip_tunnel_core.c|82| <<iptunnel_xmit>> err = ip_local_out(net, sk, skb);
+ *   - net/ipv4/netfilter/nf_dup_ipv4.c|89| <<nf_dup_ipv4>> ip_local_out(net, skb->sk, skb);
+ *   - net/ipv4/netfilter/nf_reject_ipv4.c|303| <<nf_send_reset>> ip_local_out(net, nskb->sk, nskb);
+ *   - net/netfilter/ipvs/ip_vs_xmit.c|1267| <<ip_vs_tunnel_xmit>> ip_local_out(net, skb->sk, skb);
+ *   - net/netfilter/nf_synproxy_core.c|461| <<synproxy_send_tcp>> ip_local_out(net, nskb->sk, nskb);
+ */
 int ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb)
 {
 	int err;
diff --git a/net/ipv4/udp.c b/net/ipv4/udp.c
index abc3cee9e72b..97aea0e88dbb 100644
--- a/net/ipv4/udp.c
+++ b/net/ipv4/udp.c
@@ -2150,6 +2150,11 @@ static int __udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)
  * Note that in the success and error cases, the skb is assumed to
  * have either been requeued or freed.
  */
+/*
+ * 在以下使用udp_queue_rcv_one_skb():
+ *   - net/ipv4/udp.c|2268| <<udp_queue_rcv_skb>> return udp_queue_rcv_one_skb(sk, skb);
+ *   - net/ipv4/udp.c|2277| <<udp_queue_rcv_skb>> ret = udp_queue_rcv_one_skb(sk, skb);
+ */
 static int udp_queue_rcv_one_skb(struct sock *sk, struct sk_buff *skb)
 {
 	struct udp_sock *up = udp_sk(sk);
@@ -2253,6 +2258,12 @@ static int udp_queue_rcv_one_skb(struct sock *sk, struct sk_buff *skb)
 	return -1;
 }
 
+/*
+ * 在以下使用udp_queue_rcv_skb():
+ *   - net/ipv4/udp.c|2343| <<__udp4_lib_mcast_deliver>> if (udp_queue_rcv_skb(sk, nskb) > 0)
+ *   - net/ipv4/udp.c|2354| <<__udp4_lib_mcast_deliver>> if (udp_queue_rcv_skb(first, skb) > 0)
+ *   - net/ipv4/udp.c|2422| <<udp_unicast_rcv_skb>> ret = udp_queue_rcv_skb(sk, skb);
+ */
 static int udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)
 {
 	struct sk_buff *next, *segs;
@@ -2411,6 +2422,11 @@ static inline int udp4_csum_init(struct sk_buff *skb, struct udphdr *uh,
 /* wrapper for udp_queue_rcv_skb tacking care of csum conversion and
  * return code conversion for ip layer consumption
  */
+/*
+ * 在以下使用udp_unicast_rcv_skb():
+ *   - net/ipv4/udp.c|2426| <<skb_checksum_try_convert>> skb_checksum_try_convert(skb, IPPROTO_UDP, inet_compute_pseudo);
+ *   - net/ipv4/udp.c|2428| <<udp_queue_rcv_skb>> ret = udp_queue_rcv_skb(sk, skb);
+ */
 static int udp_unicast_rcv_skb(struct sock *sk, struct sk_buff *skb,
 			       struct udphdr *uh)
 {
diff --git a/net/ipv4/udp_tunnel_core.c b/net/ipv4/udp_tunnel_core.c
index 1ff5b8e30bb9..4b0d5283d69e 100644
--- a/net/ipv4/udp_tunnel_core.c
+++ b/net/ipv4/udp_tunnel_core.c
@@ -60,6 +60,22 @@ int udp_sock_create4(struct net *net, struct udp_port_cfg *cfg,
 }
 EXPORT_SYMBOL(udp_sock_create4);
 
+/*
+ * 在以下使用setup_udp_tunnel_sock():
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|258| <<rxe_setup_udp_tunnel>> setup_udp_tunnel_sock(net, sock, &tnl_cfg);
+ *   - drivers/net/bareudp.c|266| <<bareudp_socket_create>> setup_udp_tunnel_sock(bareudp->net, sock, &tunnel_cfg);
+ *   - drivers/net/geneve.c|626| <<geneve_socket_create>> setup_udp_tunnel_sock(net, sock, &tunnel_cfg);
+ *   - drivers/net/gtp.c|845| <<gtp_encap_enable_socket>> setup_udp_tunnel_sock(sock_net(sock->sk), sock, &tuncfg);
+ *   - drivers/net/vxlan/vxlan_core.c|3533| <<vxlan_socket_create>> setup_udp_tunnel_sock(net, sock, &tunnel_cfg);
+ *   - drivers/net/wireguard/socket.c|393| <<wg_socket_init>> setup_udp_tunnel_sock(net, new4, &cfg);
+ *   - drivers/net/wireguard/socket.c|408| <<wg_socket_init>> setup_udp_tunnel_sock(net, new6, &cfg);
+ *   - net/ipv4/fou.c|624| <<fou_create>> setup_udp_tunnel_sock(net, sock, &tunnel_cfg);
+ *   - net/l2tp/l2tp_core.c|1512| <<l2tp_tunnel_register>> setup_udp_tunnel_sock(net, sock, &udp_cfg);
+ *   - net/rxrpc/local_object.c|142| <<rxrpc_open_socket>> setup_udp_tunnel_sock(net, local->socket, &tuncfg);
+ *   - net/sctp/protocol.c|878| <<sctp_udp_sock_start>> setup_udp_tunnel_sock(net, sock, &tuncfg);
+ *   - net/sctp/protocol.c|900| <<sctp_udp_sock_start>> setup_udp_tunnel_sock(net, sock, &tuncfg);
+ *   - net/tipc/udp_media.c|781| <<tipc_udp_enable>> setup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);
+ */
 void setup_udp_tunnel_sock(struct net *net, struct socket *sock,
 			   struct udp_tunnel_sock_cfg *cfg)
 {
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 37e3a49f4e76..1a35a1958785 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -729,6 +729,38 @@ ioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)
 	return _val == p->datamatch;
 }
 
+/*
+ * ioeventfd_write
+ * __kvm_io_bus_write
+ * kvm_io_bus_write
+ * write_mmio
+ * emulator_read_write_onepage
+ * emulator_read_write
+ * x86_emulate_insn
+ * x86_emulate_instruction
+ * vmx_handle_exit
+ * vcpu_enter_guest
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ *
+ * ioeventfd_write
+ * __kvm_io_bus_write
+ * kvm_io_bus_write
+ * handle_ept_misconfig
+ * vmx_handle_exit
+ * vcpu_enter_guest
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 /* MMIO/PIO writes trigger an event if the addr/val match */
 static int
 ioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 04f8fcaa82c7..8cc89c675de8 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -224,6 +224,12 @@ EXPORT_SYMBOL_GPL(vcpu_put);
 /* TODO: merge with kvm_arch_vcpu_should_kick */
 static bool kvm_request_needs_ipi(struct kvm_vcpu *vcpu, unsigned req)
 {
+	/*
+	 * 在以下使用kvm_vcpu_exiting_guest_mode():
+	 *   - arch/arm64/kvm/arm.c|68| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+	 *   - arch/x86/kvm/x86.c|12918| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+	 *   - virt/kvm/kvm_main.c|227| <<kvm_request_needs_ipi>> int mode = kvm_vcpu_exiting_guest_mode(vcpu);
+	 */
 	int mode = kvm_vcpu_exiting_guest_mode(vcpu);
 
 	/*
@@ -925,6 +931,19 @@ static void kvm_destroy_dirty_bitmap(struct kvm_memory_slot *memslot)
 
 static void kvm_destroy_present_bitmap(struct kvm_memory_slot *memslot)
 {
+	/*
+	 * 在以下使用kvm_memory_slot->present_bitmap:
+	 *   - arch/x86/kvm/mmu/mmu.c|1661| <<rmap_add>> if (slot && slot->present_bitmap) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1662| <<rmap_add>> bitmap_set(slot->present_bitmap, gfn - slot->base_gfn,
+	 *   - arch/x86/kvm/mmu/mmu.c|5880| <<kvm_rmap_walk_present>> atomic_long_t *p = (atomic_long_t *) &slot->present_bitmap[i];
+	 *   - virt/kvm/kvm_main.c|928| <<kvm_destroy_present_bitmap>> if (!memslot->present_bitmap)
+	 *   - virt/kvm/kvm_main.c|931| <<kvm_destroy_present_bitmap>> kvfree(memslot->present_bitmap);
+	 *   - virt/kvm/kvm_main.c|932| <<kvm_destroy_present_bitmap>> memslot->present_bitmap = NULL;
+	 *   - virt/kvm/kvm_main.c|1349| <<kvm_alloc_present_bitmap>> memslot->present_bitmap = __vcalloc(2, present_bytes, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|1350| <<kvm_alloc_present_bitmap>> if (!memslot->present_bitmap)
+	 *   - virt/kvm/kvm_main.c|1979| <<__kvm_set_memory_region>> new->present_bitmap = old->present_bitmap;
+	 *   - virt/kvm/kvm_main.c|1980| <<__kvm_set_memory_region>> if (!new->present_bitmap) {
+	 */
 	if (!memslot->present_bitmap)
 		return;
 
@@ -1346,6 +1365,19 @@ static int kvm_alloc_present_bitmap(struct kvm_memory_slot *memslot)
 {
 	unsigned long present_bytes = kvm_dirty_bitmap_bytes(memslot);
 
+	/*
+	 * 在以下使用kvm_memory_slot->present_bitmap:
+	 *   - arch/x86/kvm/mmu/mmu.c|1661| <<rmap_add>> if (slot && slot->present_bitmap) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1662| <<rmap_add>> bitmap_set(slot->present_bitmap, gfn - slot->base_gfn,
+	 *   - arch/x86/kvm/mmu/mmu.c|5880| <<kvm_rmap_walk_present>> atomic_long_t *p = (atomic_long_t *) &slot->present_bitmap[i];
+	 *   - virt/kvm/kvm_main.c|928| <<kvm_destroy_present_bitmap>> if (!memslot->present_bitmap)
+	 *   - virt/kvm/kvm_main.c|931| <<kvm_destroy_present_bitmap>> kvfree(memslot->present_bitmap);
+	 *   - virt/kvm/kvm_main.c|932| <<kvm_destroy_present_bitmap>> memslot->present_bitmap = NULL;
+	 *   - virt/kvm/kvm_main.c|1349| <<kvm_alloc_present_bitmap>> memslot->present_bitmap = __vcalloc(2, present_bytes, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|1350| <<kvm_alloc_present_bitmap>> if (!memslot->present_bitmap)
+	 *   - virt/kvm/kvm_main.c|1979| <<__kvm_set_memory_region>> new->present_bitmap = old->present_bitmap;
+	 *   - virt/kvm/kvm_main.c|1980| <<__kvm_set_memory_region>> if (!new->present_bitmap) {
+	 */
 	memslot->present_bitmap = __vcalloc(2, present_bytes, GFP_KERNEL_ACCOUNT);
 	if (!memslot->present_bitmap)
 		return -ENOMEM;
@@ -1975,6 +2007,19 @@ int __kvm_set_memory_region(struct kvm *kvm,
 	new->npages = npages;
 	new->flags = mem->flags;
 	new->userspace_addr = mem->userspace_addr;
+	/*
+	 * 在以下使用kvm_memory_slot->present_bitmap:
+	 *   - arch/x86/kvm/mmu/mmu.c|1661| <<rmap_add>> if (slot && slot->present_bitmap) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1662| <<rmap_add>> bitmap_set(slot->present_bitmap, gfn - slot->base_gfn,
+	 *   - arch/x86/kvm/mmu/mmu.c|5880| <<kvm_rmap_walk_present>> atomic_long_t *p = (atomic_long_t *) &slot->present_bitmap[i];
+	 *   - virt/kvm/kvm_main.c|928| <<kvm_destroy_present_bitmap>> if (!memslot->present_bitmap)
+	 *   - virt/kvm/kvm_main.c|931| <<kvm_destroy_present_bitmap>> kvfree(memslot->present_bitmap);
+	 *   - virt/kvm/kvm_main.c|932| <<kvm_destroy_present_bitmap>> memslot->present_bitmap = NULL;
+	 *   - virt/kvm/kvm_main.c|1349| <<kvm_alloc_present_bitmap>> memslot->present_bitmap = __vcalloc(2, present_bytes, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|1350| <<kvm_alloc_present_bitmap>> if (!memslot->present_bitmap)
+	 *   - virt/kvm/kvm_main.c|1979| <<__kvm_set_memory_region>> new->present_bitmap = old->present_bitmap;
+	 *   - virt/kvm/kvm_main.c|1980| <<__kvm_set_memory_region>> if (!new->present_bitmap) {
+	 */
 	if (old)
 		new->present_bitmap = old->present_bitmap;
 	if (!new->present_bitmap) {
@@ -2232,6 +2277,15 @@ static int kvm_clear_dirty_log_protect(struct kvm *kvm,
 	    (log->num_pages < memslot->npages - log->first_page && (log->num_pages & 63)))
 	    return -EINVAL;
 
+	/*
+	 * 在以下使用kvm_dirty_log_pgtable():
+	 *   - arch/x86/kvm/mmu/mmu.c|899| <<gfn_to_memslot_dirty_bitmap>> !kvm_dirty_log_pgtable(vcpu->kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1388| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> !kvm_dirty_log_pgtable(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1401| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> if (kvm_dirty_log_pgtable(kvm) &&
+	 *   - arch/x86/kvm/x86.c|6114| <<kvm_arch_sync_dirty_log>> if (kvm_dirty_log_pgtable(kvm)) {
+	 *   - arch/x86/kvm/x86.c|12631| <<kvm_mmu_slot_apply_flags>> if (!kvm_dirty_log_pgtable(kvm))
+	 *   - virt/kvm/kvm_main.c|2235| <<kvm_clear_dirty_log_protect>> if (!kvm_dirty_log_pgtable(kvm))
+	 */
 	if (!kvm_dirty_log_pgtable(kvm))
 		kvm_arch_sync_dirty_log(kvm, memslot);
 
@@ -2384,6 +2438,14 @@ static unsigned long __gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,
 	return __gfn_to_hva_memslot(slot, gfn);
 }
 
+/*
+ * 在以下使用gfn_to_hva_many():
+ *   - virt/kvm/kvm_main.c|2444| <<gfn_to_hva_memslot>> return gfn_to_hva_many(slot, gfn, NULL);
+ *   - virt/kvm/kvm_main.c|2450| <<gfn_to_hva>> return gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);
+ *   - virt/kvm/kvm_main.c|2456| <<kvm_vcpu_gfn_to_hva>> return gfn_to_hva_many(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, NULL);
+ *   - virt/kvm/kvm_main.c|2791| <<gfn_to_page_many_atomic>> addr = gfn_to_hva_many(slot, gfn, &entry);
+ *   - virt/kvm/kvm_main.c|3285| <<__kvm_gfn_to_hva_cache_init>> ghc->hva = gfn_to_hva_many(ghc->memslot, start_gfn, &nr_pages_avail);
+ */
 static unsigned long gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,
 				     gfn_t *nr_pages)
 {
@@ -2393,18 +2455,42 @@ static unsigned long gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,
 unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot,
 					gfn_t gfn)
 {
+	/*
+	 * 在以下使用gfn_to_hva_many():
+	 *   - virt/kvm/kvm_main.c|2444| <<gfn_to_hva_memslot>> return gfn_to_hva_many(slot, gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2450| <<gfn_to_hva>> return gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2456| <<kvm_vcpu_gfn_to_hva>> return gfn_to_hva_many(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2791| <<gfn_to_page_many_atomic>> addr = gfn_to_hva_many(slot, gfn, &entry);
+	 *   - virt/kvm/kvm_main.c|3285| <<__kvm_gfn_to_hva_cache_init>> ghc->hva = gfn_to_hva_many(ghc->memslot, start_gfn, &nr_pages_avail);
+	 */
 	return gfn_to_hva_many(slot, gfn, NULL);
 }
 EXPORT_SYMBOL_GPL(gfn_to_hva_memslot);
 
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn)
 {
+	/*
+	 * 在以下使用gfn_to_hva_many():
+	 *   - virt/kvm/kvm_main.c|2444| <<gfn_to_hva_memslot>> return gfn_to_hva_many(slot, gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2450| <<gfn_to_hva>> return gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2456| <<kvm_vcpu_gfn_to_hva>> return gfn_to_hva_many(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2791| <<gfn_to_page_many_atomic>> addr = gfn_to_hva_many(slot, gfn, &entry);
+	 *   - virt/kvm/kvm_main.c|3285| <<__kvm_gfn_to_hva_cache_init>> ghc->hva = gfn_to_hva_many(ghc->memslot, start_gfn, &nr_pages_avail);
+	 */
 	return gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);
 }
 EXPORT_SYMBOL_GPL(gfn_to_hva);
 
 unsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
+	/*
+	 * 在以下使用gfn_to_hva_many():
+	 *   - virt/kvm/kvm_main.c|2444| <<gfn_to_hva_memslot>> return gfn_to_hva_many(slot, gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2450| <<gfn_to_hva>> return gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2456| <<kvm_vcpu_gfn_to_hva>> return gfn_to_hva_many(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2791| <<gfn_to_page_many_atomic>> addr = gfn_to_hva_many(slot, gfn, &entry);
+	 *   - virt/kvm/kvm_main.c|3285| <<__kvm_gfn_to_hva_cache_init>> ghc->hva = gfn_to_hva_many(ghc->memslot, start_gfn, &nr_pages_avail);
+	 */
 	return gfn_to_hva_many(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, NULL);
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_hva);
@@ -2740,6 +2826,14 @@ int gfn_to_page_many_atomic(struct kvm_memory_slot *slot, gfn_t gfn,
 	unsigned long addr;
 	gfn_t entry = 0;
 
+	/*
+	 * 在以下使用gfn_to_hva_many():
+	 *   - virt/kvm/kvm_main.c|2444| <<gfn_to_hva_memslot>> return gfn_to_hva_many(slot, gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2450| <<gfn_to_hva>> return gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2456| <<kvm_vcpu_gfn_to_hva>> return gfn_to_hva_many(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, NULL);
+	 *   - virt/kvm/kvm_main.c|2791| <<gfn_to_page_many_atomic>> addr = gfn_to_hva_many(slot, gfn, &entry);
+	 *   - virt/kvm/kvm_main.c|3285| <<__kvm_gfn_to_hva_cache_init>> ghc->hva = gfn_to_hva_many(ghc->memslot, start_gfn, &nr_pages_avail);
+	 */
 	addr = gfn_to_hva_many(slot, gfn, &entry);
 	if (kvm_is_error_hva(addr))
 		return -1;
@@ -2894,6 +2988,19 @@ static void __kvm_unmap_gfn(struct kvm *kvm,
 		WARN_ONCE(1, "Unexpected unmapping in atomic context");
 #endif
 
+	/*
+	 * 在以下使用mark_page_dirty_in_slot():
+	 *   - arch/arm64/kvm/mmu.c|1179| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|267| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+	 *   - arch/x86/kvm/x86.c|3401| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/x86.c|4596| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/xen.c|204| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|2898| <<__kvm_unmap_gfn>> mark_page_dirty_in_slot(kvm, memslot, map->gfn);
+	 *   - virt/kvm/kvm_main.c|3113| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|3358| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3367| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+	 */
 	if (dirty)
 		mark_page_dirty_in_slot(kvm, memslot, map->gfn);
 
@@ -3110,6 +3217,19 @@ static int __kvm_write_guest_page(struct kvm *kvm,
 	r = __copy_to_user((void __user *)addr + offset, data, len);
 	if (r)
 		return -EFAULT;
+	/*
+	 * 在以下使用mark_page_dirty_in_slot():
+	 *   - arch/arm64/kvm/mmu.c|1179| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|267| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+	 *   - arch/x86/kvm/x86.c|3401| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/x86.c|4596| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/xen.c|204| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|2898| <<__kvm_unmap_gfn>> mark_page_dirty_in_slot(kvm, memslot, map->gfn);
+	 *   - virt/kvm/kvm_main.c|3113| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|3358| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3367| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+	 */
 	mark_page_dirty_in_slot(kvm, memslot, gfn);
 	return 0;
 }
@@ -3132,6 +3252,10 @@ int kvm_vcpu_write_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn,
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_write_guest_page);
 
+/*
+ * 在以下使用kvm_write_guest():
+ *   - 
+ */
 int kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,
 		    unsigned long len)
 {
@@ -3174,6 +3298,20 @@ int kvm_vcpu_write_guest(struct kvm_vcpu *vcpu, gpa_t gpa, const void *data,
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_write_guest);
 
+/*
+ * 在以下使用__kvm_gfn_to_hva_cache_init():
+ *   - virt/kvm/kvm_main.c|3222| <<kvm_gfn_to_hva_cache_init>> return __kvm_gfn_to_hva_cache_init(slots, ghc, gpa, len);
+ *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> if (__kvm_gfn_to_hva_cache_init(slots, ghc, ghc->gpa, ghc->len))
+ *   - virt/kvm/kvm_main.c|3309| <<kvm_read_guest_offset_cached>> if (__kvm_gfn_to_hva_cache_init(slots, ghc, ghc->gpa, ghc->len))
+ *
+ * struct gfn_to_hva_cache { 
+ *     u64 generation;                     
+ *     gpa_t gpa;
+ *     unsigned long hva;
+ *     unsigned long len;               
+ *     struct kvm_memory_slot *memslot;
+ * };
+ */
 static int __kvm_gfn_to_hva_cache_init(struct kvm_memslots *slots,
 				       struct gfn_to_hva_cache *ghc,
 				       gpa_t gpa, unsigned long len)
@@ -3198,6 +3336,14 @@ static int __kvm_gfn_to_hva_cache_init(struct kvm_memslots *slots,
 	 */
 	for ( ; start_gfn <= end_gfn; start_gfn += nr_pages_avail) {
 		ghc->memslot = __gfn_to_memslot(slots, start_gfn);
+		/*
+		 * 在以下使用gfn_to_hva_many():
+		 *   - virt/kvm/kvm_main.c|2444| <<gfn_to_hva_memslot>> return gfn_to_hva_many(slot, gfn, NULL);
+		 *   - virt/kvm/kvm_main.c|2450| <<gfn_to_hva>> return gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);
+		 *   - virt/kvm/kvm_main.c|2456| <<kvm_vcpu_gfn_to_hva>> return gfn_to_hva_many(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, NULL);
+		 *   - virt/kvm/kvm_main.c|2791| <<gfn_to_page_many_atomic>> addr = gfn_to_hva_many(slot, gfn, &entry);
+		 *   - virt/kvm/kvm_main.c|3285| <<__kvm_gfn_to_hva_cache_init>> ghc->hva = gfn_to_hva_many(ghc->memslot, start_gfn, &nr_pages_avail);
+		 */
 		ghc->hva = gfn_to_hva_many(ghc->memslot, start_gfn,
 					   &nr_pages_avail);
 		if (kvm_is_error_hva(ghc->hva))
@@ -3215,14 +3361,62 @@ static int __kvm_gfn_to_hva_cache_init(struct kvm_memslots *slots,
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_gfn_to_hva_cache_init():
+ *   - arch/x86/kvm/lapic.c|2947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *            &vcpu->arch.apic->vapic_cache, vapic_addr, sizeof(u32)))
+ *   - arch/x86/kvm/lapic.c|3063| <<kvm_lapic_enable_pv_eoi>> return kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *            ghc, addr, new_len);
+ *   - arch/x86/kvm/x86.c|2278| <<kvm_write_system_time>> if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *            &vcpu->arch.pv_time, system_time & ~1ULL, sizeof(struct pvclock_vcpu_time_info)))
+ *   - arch/x86/kvm/x86.c|3442| <<kvm_pv_enable_async_pf>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *            &vcpu->arch.apf.data, gpa, sizeof(u64)))
+ *   - arch/x86/kvm/x86.c|3561| <<record_steal_time>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *            ghc, gpa, sizeof(*st)) ||
+ *   - arch/x86/kvm/xen.c|109| <<kvm_xen_update_runstate_guest>> kvm_gfn_to_hva_cache_init(v->kvm,
+ *            ghc, ghc->gpa, ghc->len))
+ *   - arch/x86/kvm/xen.c|365| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *            &vcpu->arch.xen.vcpu_info_cache, data->u.gpa, sizeof(struct vcpu_info));
+ *   - arch/x86/kvm/xen.c|388| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *            &vcpu->arch.xen.vcpu_time_info_cache, data->u.gpa, sizeof(struct pvclock_vcpu_time_info));
+ *   - arch/x86/kvm/xen.c|415| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *            &vcpu->arch.xen.runstate_cache, data->u.gpa, sizeof(struct vcpu_runstate_info));
+ */
 int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			      gpa_t gpa, unsigned long len)
 {
 	struct kvm_memslots *slots = kvm_memslots(kvm);
+	/*
+	 * 在以下使用__kvm_gfn_to_hva_cache_init():
+	 *   - virt/kvm/kvm_main.c|3222| <<kvm_gfn_to_hva_cache_init>> return __kvm_gfn_to_hva_cache_init(slots, ghc, gpa, len);
+	 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> if (__kvm_gfn_to_hva_cache_init(slots, ghc, ghc->gpa, ghc->len))
+	 *   - virt/kvm/kvm_main.c|3309| <<kvm_read_guest_offset_cached>> if (__kvm_gfn_to_hva_cache_init(slots, ghc, ghc->gpa, ghc->len))
+	 */
 	return __kvm_gfn_to_hva_cache_init(slots, ghc, gpa, len);
 }
 EXPORT_SYMBOL_GPL(kvm_gfn_to_hva_cache_init);
 
+/*
+ * 在以下使用kvm_write_guest_offset_cached():
+ *   - arch/x86/kvm/x86.c|2944| <<kvm_setup_pvclock_page>> kvm_write_guest_offset_cached(v->kvm,
+ *          cache, &vcpu->hv_clock, offset, sizeof(vcpu->hv_clock.version));
+ *   - arch/x86/kvm/x86.c|2960| <<kvm_setup_pvclock_page>> kvm_write_guest_offset_cached(v->kvm,
+ *          cache, &vcpu->hv_clock, offset, sizeof(vcpu->hv_clock));
+ *   - arch/x86/kvm/x86.c|2967| <<kvm_setup_pvclock_page>> kvm_write_guest_offset_cached(v->kvm,
+ *          cache, &vcpu->hv_clock, offset, sizeof(vcpu->hv_clock.version));
+ *   - arch/x86/kvm/x86.c|12462| <<apf_put_user_ready>> return kvm_write_guest_offset_cached(vcpu->kvm,
+ *          &vcpu->arch.apf.data, &token, offset, sizeof(token));
+ *   - virt/kvm/kvm_main.c|3260| <<kvm_write_guest_cached>> return kvm_write_guest_offset_cached(kvm,
+ *          ghc, data, 0, len);
+ *
+ * struct gfn_to_hva_cache {
+ *     u64 generation;
+ *     gpa_t gpa;
+ *     unsigned long hva;
+ *     unsigned long len;
+ *     struct kvm_memory_slot *memslot;
+ * };
+ */
 int kvm_write_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 				  void *data, unsigned int offset,
 				  unsigned long len)
@@ -3235,6 +3429,12 @@ int kvm_write_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 		return -EINVAL;
 
 	if (slots->generation != ghc->generation) {
+		/*
+		 * 在以下使用__kvm_gfn_to_hva_cache_init():
+		 *   - virt/kvm/kvm_main.c|3222| <<kvm_gfn_to_hva_cache_init>> return __kvm_gfn_to_hva_cache_init(slots, ghc, gpa, len);
+		 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> if (__kvm_gfn_to_hva_cache_init(slots, ghc, ghc->gpa, ghc->len))
+		 *   - virt/kvm/kvm_main.c|3309| <<kvm_read_guest_offset_cached>> if (__kvm_gfn_to_hva_cache_init(slots, ghc, ghc->gpa, ghc->len))
+		 */
 		if (__kvm_gfn_to_hva_cache_init(slots, ghc, ghc->gpa, ghc->len))
 			return -EFAULT;
 	}
@@ -3248,12 +3448,34 @@ int kvm_write_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 	r = __copy_to_user((void __user *)ghc->hva + offset, data, len);
 	if (r)
 		return -EFAULT;
+	/*
+	 * 在以下使用mark_page_dirty_in_slot():
+	 *   - arch/arm64/kvm/mmu.c|1179| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|267| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+	 *   - arch/x86/kvm/x86.c|3401| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/x86.c|4596| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/xen.c|204| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|2898| <<__kvm_unmap_gfn>> mark_page_dirty_in_slot(kvm, memslot, map->gfn);
+	 *   - virt/kvm/kvm_main.c|3113| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|3358| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3367| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+	 */
 	mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
 
 	return 0;
 }
 EXPORT_SYMBOL_GPL(kvm_write_guest_offset_cached);
 
+/*
+ * 在以下使用kvm_write_guest_cached():
+ *   - arch/x86/kvm/lapic.c|727| <<pv_eoi_put_user>> return kvm_write_guest_cached(vcpu->kvm,
+ *          &vcpu->arch.pv_eoi.data, &val, sizeof(val));
+ *   - arch/x86/kvm/lapic.c|2922| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm,
+ *          &vcpu->arch.apic->vapic_cache, &data, sizeof(u32));
+ *   - arch/x86/kvm/x86.c|12454| <<apf_put_user_notpresent>> return kvm_write_guest_cached(vcpu->kvm,
+ *          &vcpu->arch.apf.data, &reason, sizeof(reason));
+ */
 int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			   void *data, unsigned long len)
 {
@@ -3261,6 +3483,17 @@ int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 }
 EXPORT_SYMBOL_GPL(kvm_write_guest_cached);
 
+/*
+ * 在以下使用kvm_read_guest_offset_cached():
+ *   - arch/x86/kvm/x86.c|2989| <<kvm_setup_pvclock_page>> if (unlikely(kvm_read_guest_offset_cached(v->kvm,
+ *          cache, &guest_hv_clock, offset, sizeof(guest_hv_clock))))
+ *   - arch/x86/kvm/x86.c|12767| <<apf_pageready_slot_free>> if (kvm_read_guest_offset_cached(vcpu->kvm,
+ *          &vcpu->arch.apf.data, &val, offset, sizeof(val)))
+ *   - arch/x86/kvm/xen.c|259| <<__kvm_xen_has_interrupt>> kvm_read_guest_offset_cached(v->kvm,
+ *          ghc, &rc, offset, sizeof(rc));
+ *   - virt/kvm/kvm_main.c|3319| <<kvm_read_guest_cached>> return kvm_read_guest_offset_cached(kvm,
+ *          ghc, data, 0, len);
+ */
 int kvm_read_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 				 void *data, unsigned int offset,
 				 unsigned long len)
@@ -3273,6 +3506,12 @@ int kvm_read_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 		return -EINVAL;
 
 	if (slots->generation != ghc->generation) {
+		/*
+		 * 在以下使用__kvm_gfn_to_hva_cache_init():
+		 *   - virt/kvm/kvm_main.c|3222| <<kvm_gfn_to_hva_cache_init>> return __kvm_gfn_to_hva_cache_init(slots, ghc, gpa, len);
+		 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> if (__kvm_gfn_to_hva_cache_init(slots, ghc, ghc->gpa, ghc->len))
+		 *   - virt/kvm/kvm_main.c|3309| <<kvm_read_guest_offset_cached>> if (__kvm_gfn_to_hva_cache_init(slots, ghc, ghc->gpa, ghc->len))
+		 */
 		if (__kvm_gfn_to_hva_cache_init(slots, ghc, ghc->gpa, ghc->len))
 			return -EFAULT;
 	}
@@ -3294,6 +3533,17 @@ EXPORT_SYMBOL_GPL(kvm_read_guest_offset_cached);
 int kvm_read_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			  void *data, unsigned long len)
 {
+	/*
+	 * 在以下使用kvm_read_guest_offset_cached():
+	 *   - arch/x86/kvm/x86.c|2989| <<kvm_setup_pvclock_page>> if (unlikely(kvm_read_guest_offset_cached(v->kvm,
+	 *          cache, &guest_hv_clock, offset, sizeof(guest_hv_clock))))
+	 *   - arch/x86/kvm/x86.c|12767| <<apf_pageready_slot_free>> if (kvm_read_guest_offset_cached(vcpu->kvm,
+	 *          &vcpu->arch.apf.data, &val, offset, sizeof(val)))
+	 *   - arch/x86/kvm/xen.c|259| <<__kvm_xen_has_interrupt>> kvm_read_guest_offset_cached(v->kvm,
+	 *          ghc, &rc, offset, sizeof(rc));
+	 *   - virt/kvm/kvm_main.c|3319| <<kvm_read_guest_cached>> return kvm_read_guest_offset_cached(kvm,
+	 *          ghc, data, 0, len);
+	 */
 	return kvm_read_guest_offset_cached(kvm, ghc, data, 0, len);
 }
 EXPORT_SYMBOL_GPL(kvm_read_guest_cached);
@@ -3318,14 +3568,42 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 }
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
+/*
+ * 在以下使用mark_page_dirty_in_slot():
+ *   - arch/arm64/kvm/mmu.c|1179| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|267| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/x86/kvm/x86.c|3401| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/x86.c|4596| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/xen.c|204| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|2898| <<__kvm_unmap_gfn>> mark_page_dirty_in_slot(kvm, memslot, map->gfn);
+ *   - virt/kvm/kvm_main.c|3113| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3358| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3367| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+ */
 void mark_page_dirty_in_slot(struct kvm *kvm,
 			     struct kvm_memory_slot *memslot,
 		 	     gfn_t gfn)
 {
+	/*
+	 * 在以下使用kvm_slot_dirty_track_enabled():
+	 *   - arch/x86/kvm/mmu/mmu.c|898| <<gfn_to_memslot_dirty_bitmap>> if (no_dirty_log && kvm_slot_dirty_track_enabled(slot) &&
+	 *   - arch/x86/kvm/mmu/mmu.c|2917| <<kvm_mmu_hugepage_adjust>> if (kvm_slot_dirty_track_enabled(slot))
+	 *   - virt/kvm/kvm_main.c|3435| <<mark_page_dirty_in_slot>> if (memslot && kvm_slot_dirty_track_enabled(memslot)) {
+	 *   - virt/kvm/kvm_main.c|3453| <<mark_page_range_dirty_in_slot>> kvm_slot_dirty_track_enabled((struct kvm_memory_slot *)memslot)) {
+	 */
 	if (memslot && kvm_slot_dirty_track_enabled(memslot)) {
 		unsigned long rel_gfn = gfn - memslot->base_gfn;
 		u32 slot = (memslot->as_id << 16) | memslot->id;
 
+		/*
+		 * 在以下使用memslot->dirty_bitmap:
+		 *   - virt/kvm/kvm_main.c|923| <<kvm_destroy_dirty_bitmap>> memslot->dirty_bitmap = NULL;
+		 *   - virt/kvm/kvm_main.c|1351| <<kvm_alloc_dirty_bitmap>> memslot->dirty_bitmap = __vcalloc(2, dirty_bytes, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|1603| <<kvm_prepare_memory_region>> new->dirty_bitmap = NULL;
+		 *   - virt/kvm/kvm_main.c|1605| <<kvm_prepare_memory_region>> new->dirty_bitmap = old->dirty_bitmap;
+		 *   - virt/kvm/kvm_main.c|1696| <<kvm_copy_memslot>> dest->dirty_bitmap = src->dirty_bitmap;
+		 */
 		if (kvm->dirty_ring_size)
 			kvm_dirty_ring_push(kvm_dirty_ring_get(kvm),
 					    slot, rel_gfn);
@@ -3335,6 +3613,11 @@ void mark_page_dirty_in_slot(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(mark_page_dirty_in_slot);
 
+/*
+ * 在以下使用mark_page_range_dirty_in_slot():
+ *   - arch/x86/kvm/mmu/mmu.c|3115| <<fast_pf_fix_direct_spte>> mark_page_range_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn,
+ *   - arch/x86/kvm/mmu/mmu.c|5912| <<rmap_test_dirty>> mark_page_range_dirty_in_slot(kvm, slot, pfn,
+ */
 void mark_page_range_dirty_in_slot(struct kvm *kvm,
 				   const struct kvm_memory_slot *memslot,
 				   gfn_t gfn, unsigned long npages)
@@ -3350,20 +3633,70 @@ void mark_page_range_dirty_in_slot(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(mark_page_range_dirty_in_slot);
 
+/*
+ * 在以下使用mark_page_dirty():
+ *   - arch/mips/kvm/mmu.c|547| <<_kvm_mips_map_page_fast>> mark_page_dirty(kvm, gfn);
+ *   - arch/mips/kvm/mmu.c|661| <<kvm_mips_map_page>> mark_page_dirty(kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_32_mmu_host.c|200| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, orig_pte->raddr >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_64_mmu_host.c|128| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_hv.c|852| <<kvmppc_copy_guest>> mark_page_dirty(kvm, to >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_xive_native.c|909| <<kvmppc_xive_native_vcpu_eq_sync>> mark_page_dirty(vcpu->kvm, gpa_to_gfn(q->guest_qaddr));
+ *   - arch/s390/kvm/interrupt.c|2811| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->ind_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/interrupt.c|2817| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->summary_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/kvm-s390.c|635| <<kvm_arch_sync_dirty_log>> mark_page_dirty(kvm, cur_gfn + i);
+ *   - arch/s390/kvm/vsie.c|662| <<unpin_guest_page>> mark_page_dirty(kvm, gpa_to_gfn(gpa));
+ *   - include/linux/kvm_host.h|1225| <<__kvm_put_guest>> mark_page_dirty(kvm, gfn); \
+ */
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn)
 {
 	struct kvm_memory_slot *memslot;
 
 	memslot = gfn_to_memslot(kvm, gfn);
+	/*
+	 * 在以下使用mark_page_dirty_in_slot():
+	 *   - arch/arm64/kvm/mmu.c|1179| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|267| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+	 *   - arch/x86/kvm/x86.c|3401| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/x86.c|4596| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/xen.c|204| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|2898| <<__kvm_unmap_gfn>> mark_page_dirty_in_slot(kvm, memslot, map->gfn);
+	 *   - virt/kvm/kvm_main.c|3113| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|3358| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3367| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+	 */
 	mark_page_dirty_in_slot(kvm, memslot, gfn);
 }
 EXPORT_SYMBOL_GPL(mark_page_dirty);
 
+/*
+ * 在以下使用kvm_vcpu_mark_page_dirty():
+ *   - arch/x86/kvm/hyperv.c|1522| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|288| <<FNAME(update_accessed_dirty_bits)>> kvm_vcpu_mark_page_dirty(vcpu, table_gfn);
+ *   - arch/x86/kvm/mmu/spte.c|185| <<make_spte>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/vmx/nested.c|3799| <<nested_mark_vmcs12_pages_dirty>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/vmx/nested.c|3804| <<nested_mark_vmcs12_pages_dirty>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/vmx/vmx.c|5864| <<vmx_flush_pml_buffer>> kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/x86.c|7639| <<emulator_cmpxchg_emulated>> kvm_vcpu_mark_page_dirty(vcpu, gpa_to_gfn(gpa));
+ */
 void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	struct kvm_memory_slot *memslot;
 
 	memslot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+	/*
+	 * 在以下使用mark_page_dirty_in_slot():
+	 *   - arch/arm64/kvm/mmu.c|1179| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|267| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+	 *   - arch/x86/kvm/x86.c|3401| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/x86.c|4596| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+	 *   - arch/x86/kvm/xen.c|204| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|2898| <<__kvm_unmap_gfn>> mark_page_dirty_in_slot(kvm, memslot, map->gfn);
+	 *   - virt/kvm/kvm_main.c|3113| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3251| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+	 *   - virt/kvm/kvm_main.c|3358| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+	 *   - virt/kvm/kvm_main.c|3367| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+	 */
 	mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_mark_page_dirty);
@@ -4473,6 +4806,10 @@ static long kvm_vm_ioctl_check_extension_generic(struct kvm *kvm, long arg)
 	return kvm_vm_ioctl_check_extension(kvm, arg);
 }
 
+/*
+ * 在以下使用kvm_vm_ioctl_enable_dirty_log_ring():
+ *   - virt/kvm/kvm_main.c|4763| <<kvm_vm_ioctl_enable_cap_generic>> return kvm_vm_ioctl_enable_dirty_log_ring(kvm, cap->args[0]);
+ */
 static int kvm_vm_ioctl_enable_dirty_log_ring(struct kvm *kvm, u32 size)
 {
 	int r;
@@ -4561,6 +4898,11 @@ static int kvm_vm_ioctl_enable_cap_generic(struct kvm *kvm,
 
 		if (cap->flags || (cap->args[0] & ~allowed_options))
 			return -EINVAL;
+		/*
+		 * 在以下使用kvm->dirty_log_pgtable:
+		 *   - include/linux/kvm_host.h|832| <<kvm_dirty_log_pgtable>> return !!(kvm->dirty_log_pgtable & KVM_DIRTY_LOG_PGTABLE);
+		 *   - virt/kvm/kvm_main.c|4674| <<kvm_vm_ioctl_enable_cap_generic>> kvm->dirty_log_pgtable = cap->args[0];
+		 */
 		kvm->dirty_log_pgtable = cap->args[0];
 		return 0;
 	}
-- 
2.39.5 (Apple Git-154)

