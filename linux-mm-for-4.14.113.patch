From fc18c457622cb2fd2726703a7359a8e3acce3525 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Fri, 17 Jan 2020 08:44:47 -0800
Subject: [PATCH 1/1] linux-mm-for-4.14.113

Signed-off-by: Dongli Zhang <dongli.zhangi0129@gmail.com>
---
 include/linux/memcontrol.h |   14 +
 include/linux/mm.h         |    3 +
 include/linux/mm_types.h   |   34 +
 include/linux/slab.h       |   26 +
 include/linux/slub_def.h   |   55 ++
 mm/kasan/kasan.c           |    5 +
 mm/slab.h                  |   47 ++
 mm/slab_common.c           |   11 +
 mm/slub.c                  | 1545 ++++++++++++++++++++++++++++++++++++
 9 files changed, 1740 insertions(+)

diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index 69966c46..4df90172 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -1118,6 +1118,20 @@ void memcg_put_cache_ids(void);
 #define for_each_memcg_cache_index(_idx)	\
 	for ((_idx) = 0; (_idx) < memcg_nr_cache_ids; (_idx)++)
 
+/*
+ * called by:
+ *   - fs/pipe.c|148| <<anon_pipe_buf_steal>> if (memcg_kmem_enabled())
+ *   - mm/list_lru.c|70| <<mem_cgroup_from_kmem>> if (!memcg_kmem_enabled())
+ *   - mm/page_alloc.c|1049| <<free_pages_prepare>> if (memcg_kmem_enabled() && PageKmemcg(page))
+ *   - mm/page_alloc.c|4201| <<__alloc_pages_nodemask>> if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
+ *   - mm/slab.h|277| <<memcg_charge_slab>> if (!memcg_kmem_enabled())
+ *   - mm/slab.h|287| <<memcg_uncharge_slab>> if (!memcg_kmem_enabled())
+ *   - mm/slab.h|366| <<cache_from_obj>> if (!memcg_kmem_enabled() &&
+ *   - mm/slab.h|432| <<slab_pre_alloc_hook>> if (memcg_kmem_enabled() &&
+ *   - mm/slab.h|453| <<slab_post_alloc_hook>> if (memcg_kmem_enabled())
+ *   - mm/vmscan.c|468| <<shrink_slab>> if (memcg && (!memcg_kmem_enabled() || !mem_cgroup_online(memcg)))
+ *   - mm/vmscan.c|497| <<shrink_slab>> if (memcg_kmem_enabled() &&
+ */
 static inline bool memcg_kmem_enabled(void)
 {
 	return static_branch_unlikely(&memcg_kmem_enabled_key);
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 58f2263d..31fc7006 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2425,6 +2425,9 @@ extern void __kernel_map_pages(struct page *page, int numpages, int enable);
 
 static inline bool debug_pagealloc_enabled(void)
 {
+	/*
+	 * _debug_pagealloc_enabled定义在page_alloc.c
+	 */
 	return _debug_pagealloc_enabled;
 }
 
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index e41ef532..26adb790 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -92,6 +92,40 @@ struct page {
 
 				unsigned int active;		/* SLAB */
 				struct {			/* SLUB */
+					/*
+					 * 修改page->inuse的地方:
+					 *   - mm/slub.c|692| <<set_page_slub_counters>> page->inuse = tmp.inuse;
+					 *   - mm/slub.c|1509| <<on_freelist>> page->inuse = page->objects;
+					 *   - mm/slub.c|1807| <<alloc_debug_processing>> page->inuse = page->objects;
+					 *   - mm/slub.c|1534| <<on_freelist>> page->inuse = page->objects - nr;
+					 *   - mm/slub.c|2428| <<allocate_slab>> page->inuse = page->objects;
+					 *   - mm/slub.c|4455| <<early_kmem_cache_node_alloc>> page->inuse = 1;
+					 *   - mm/slub.c|2699| <<acquire_slab>> new.inuse = page->objects;
+					 *   - mm/slub.c|3001| <<deactivate_slab>> new.inuse--;
+					 *   - mm/slub.c|3035| <<deactivate_slab>> new.inuse--;
+					 *   - mm/slub.c|3463| <<get_freelist>> new.inuse = page->objects;
+					 *   - mm/slub.c|3877| <<__slab_free>> new.inuse -= cnt;
+					 *
+					 * 使用page->inuse的地方:
+					 *   - mm/slub.c|1072| <<print_page_info>> page, page->objects, page->inuse, page->freelist, page->flags);
+					 *   - mm/slub.c|1468| <<check_slab>> if (page->inuse > page->objects) {
+					 *   - mm/slub.c|1470| <<check_slab>> page->inuse, page->objects);
+					 *   - mm/slub.c|1531| <<on_freelist>> if (page->inuse != page->objects - nr) {
+					 *   - mm/slub.c|1533| <<on_freelist>> page->inuse, page->objects - nr);
+					 *   - mm/slub.c|1589| <<trace>> object, page->inuse,
+					 *   - mm/slub.c|3233| <<put_cpu_partial>> pobjects += page->objects - page->inuse;
+					 *   - mm/slub.c|3332| <<count_free>> return page->objects - page->inuse;
+					 *   - mm/slub.c|4798| <<free_partial>> if (!page->inuse) {
+					 *   - mm/slub.c|5059| <<__kmem_cache_shrink>> int free = page->objects - page->inuse;
+					 *   - mm/slub.c|5468| <<count_inuse>> return page->inuse;
+					 *   - mm/slub.c|5932| <<show_slab_objects>> x = page->inuse;
+					 *   - mm/slub.c|3043| <<deactivate_slab>> if (!new.inuse && n->nr_partial >= s->min_partial)
+					 *   - mm/slub.c|3163| <<unfreeze_partials>> if (unlikely(!new.inuse && n->nr_partial >= s->min_partial)) {
+					 *   - mm/slub.c|3878| <<__slab_free>> if ((!new.inuse || !prior) && !was_frozen) {
+					 *   - mm/slub.c|3930| <<__slab_free>> if (unlikely(!new.inuse && n->nr_partial >= s->min_partial))
+					 *
+					 * struct page结构体中inuse代表已经使用的obj数量
+					 */
 					unsigned inuse:16;
 					unsigned objects:15;
 					unsigned frozen:1;
diff --git a/include/linux/slab.h b/include/linux/slab.h
index ae5ed649..d70fb5b2 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -21,6 +21,20 @@
  * Flags to pass to kmem_cache_create().
  * The ones marked DEBUG are only valid if CONFIG_DEBUG_SLAB is set.
  */
+/*
+ * slub中使用SLAB_CONSISTENCY_CHECKS的例子 (当slub_debug设置了f的时候):
+ *   - mm/slub.c|313| <<DEBUG_DEFAULT_FLAGS>> #define DEBUG_DEFAULT_FLAGS (SLAB_CONSISTENCY_CHECKS | SLAB_RED_ZONE | \
+ *   - mm/slub.c|320| <<SLAB_NO_CMPXCHG>> #define SLAB_NO_CMPXCHG (SLAB_CONSISTENCY_CHECKS | SLAB_STORE_USER | \
+ *   - mm/slub.c|1786| <<alloc_debug_processing>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+ *   - mm/slub.c|1882| <<free_debug_processing>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+ *   - mm/slub.c|1890| <<free_debug_processing>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+ *   - mm/slub.c|1949| <<setup_slub_debug>> slub_debug |= SLAB_CONSISTENCY_CHECKS;
+ *   - mm/slub.c|2381| <<__free_slab>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+ *   - mm/slub.c|5988| <<sanity_checks_show>> return sprintf(buf, "%d\n", !!(s->flags & SLAB_CONSISTENCY_CHECKS));
+ *   - mm/slub.c|5994| <<sanity_checks_store>> s->flags &= ~SLAB_CONSISTENCY_CHECKS;
+ *   - mm/slub.c|5997| <<sanity_checks_store>> s->flags |= SLAB_CONSISTENCY_CHECKS;
+ *   - mm/slub.c|6532| <<create_unique_id>> if (s->flags & SLAB_CONSISTENCY_CHECKS)
+ */
 #define SLAB_CONSISTENCY_CHECKS	0x00000100UL	/* DEBUG: Perform (expensive) checks on alloc/free */
 #define SLAB_RED_ZONE		0x00000400UL	/* DEBUG: Red zone objs in a cache */
 #define SLAB_POISON		0x00000800UL	/* DEBUG: Poison objects */
@@ -67,6 +81,18 @@
  */
 #define SLAB_TYPESAFE_BY_RCU	0x00080000UL	/* Defer freeing slabs to RCU */
 #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
+/*
+ * 在以下使用SLAB_TRACE:
+ *   - mm/slab.h|136| <<SLAB_DEBUG_FLAGS>> SLAB_TRACE | SLAB_CONSISTENCY_CHECKS)
+ *   - mm/slab.h|160| <<SLAB_FLAGS_PERMITTED>> SLAB_TRACE | \
+ *   - mm/slab_common.c|43| <<SLAB_NEVER_MERGE>> SLAB_TRACE | SLAB_TYPESAFE_BY_RCU | SLAB_NOLEAKTRACE | \
+ *   - mm/slub.c|321| <<SLAB_NO_CMPXCHG>> SLAB_TRACE)
+ *   - mm/slub.c|1481| <<trace>> if (s->flags & SLAB_TRACE) {
+ *   - mm/slub.c|1751| <<setup_slub_debug>> slub_debug |= SLAB_TRACE;
+ *   - mm/slub.c|5795| <<trace_show>> return sprintf(buf, "%d\n", !!(s->flags & SLAB_TRACE));
+ *   - mm/slub.c|5809| <<trace_store>> s->flags &= ~SLAB_TRACE;
+ *   - mm/slub.c|5812| <<trace_store>> s->flags |= SLAB_TRACE;
+ */
 #define SLAB_TRACE		0x00200000UL	/* Trace allocations and frees */
 
 /* Flag to prevent checks on free */
diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index f8ced87a..c5cd22e8 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -43,6 +43,14 @@ struct kmem_cache_cpu {
 	unsigned long tid;	/* Globally unique transaction id */
 	struct page *page;	/* The slab from which we are allocating */
 #ifdef CONFIG_SLUB_CPU_PARTIAL
+	/*
+	 * kmem_cache_cpu->partial使用的例子:
+	 *   - include/linux/slub_def.h|54| <<slub_percpu_partial>> #define slub_percpu_partial(c) ((c)->partial)
+	 *   - mm/slub.c|3133| <<unfreeze_partials>> while ((page = c->partial)) {
+	 *   - mm/slub.c|3137| <<unfreeze_partials>> c->partial = page->next;
+	 *   - mm/slub.c|3212| <<put_cpu_partial>> oldpage = this_cpu_read(s->cpu_slab->partial);
+	 *   - mm/slub.c|3240| <<put_cpu_partial>> } while (this_cpu_cmpxchg(s->cpu_slab->partial, oldpage, page)
+	 */
 	struct page *partial;	/* Partially allocated frozen slabs */
 #endif
 #ifdef CONFIG_SLUB_STATS
@@ -51,13 +59,33 @@ struct kmem_cache_cpu {
 };
 
 #ifdef CONFIG_SLUB_CPU_PARTIAL
+/*
+ * 在以下调用slub_percpu_partial():
+ *   - include/linux/slub_def.h|62| <<slub_set_percpu_partial>> slub_percpu_partial(c) = (p)->next; \
+ *   - include/linux/slub_def.h|66| <<slub_percpu_partial_read_once>> #define slub_percpu_partial_read_once(c) READ_ONCE(slub_percpu_partial(c))
+ *   - mm/slub.c|3290| <<has_cpu_slab>> return c->page || slub_percpu_partial(c);
+ *   - mm/slub.c|3565| <<___slab_alloc>> if (slub_percpu_partial(c)) {
+ *   - mm/slub.c|3566| <<___slab_alloc>> page = c->page = slub_percpu_partial(c);
+ *   - mm/slub.c|6207| <<slabs_cpu_partial_show>> page = slub_percpu_partial(per_cpu_ptr(s->cpu_slab, cpu));
+ *   - mm/slub.c|6221| <<slabs_cpu_partial_show>> page = slub_percpu_partial(per_cpu_ptr(s->cpu_slab, cpu));
+ */
 #define slub_percpu_partial(c)		((c)->partial)
 
+/*
+ * called by:
+ *   - mm/slub.c|2608| <<___slab_alloc>> slub_set_percpu_partial(c, page);
+ */
 #define slub_set_percpu_partial(c, p)		\
 ({						\
 	slub_percpu_partial(c) = (p)->next;	\
 })
 
+/*
+ * called by:
+ *   - mm/slub.c|5977| <<show_slab_objects>> page = slub_percpu_partial_read_once(c);
+ *
+ * 如果c->partial存在
+ */
 #define slub_percpu_partial_read_once(c)     READ_ONCE(slub_percpu_partial(c))
 #else
 #define slub_percpu_partial(c)			NULL
@@ -86,6 +114,17 @@ struct kmem_cache {
 	unsigned long min_partial;
 	int size;		/* The size of an object including meta data */
 	int object_size;	/* The size of an object without meta data */
+	/*
+	 * 设置kmem_cache->offset的地方:
+	 *   - mm/slub.c|4905| <<calculate_sizes>> s->offset = size;
+	 *   - mm/slub.c|5000| <<kmem_cache_open>> s->offset = 0
+	 *
+	 * 就是在calculate_sizes()设置, 在以下条件设置:
+	 * 4898         if (((flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)) ||
+	 * 4899                 s->ctor)) {
+	 * 当slub_debug=ZUF(没有P)的时候, kmalloc-128的s->offset=0
+	 * 当slub_debug=PZUF(有P)的时候, kmalloc-128的s->offset=136
+	 */
 	int offset;		/* Free pointer offset. */
 #ifdef CONFIG_SLUB_CPU_PARTIAL
 	/* Number of per cpu partial objects to keep around */
@@ -99,6 +138,22 @@ struct kmem_cache {
 	gfp_t allocflags;	/* gfp flags to use on each alloc */
 	int refcount;		/* Refcount for slab cache destroy */
 	void (*ctor)(void *);
+	/*
+	 * 修改kmem_cache->inuse的地方:
+	 *   - mm/slub.c|5371| <<__kmem_cache_alias>> s->inuse = max_t(int , s->inuse, ALIGN(size, sizeof(void *)));
+	 *   - mm/slub.c|4607| <<calculate_sizes>> s->inuse = size;
+	 *   - mm/slub.c|1405| <<check_object>> s->inuse - s->object_size);
+	 *   - mm/slub.c|5375| <<__kmem_cache_alias>> c->inuse = max_t(int , c->inuse,
+	 * 使用kmem_cache->inuse的地方:
+	 *   - mm/slub.c|950| <<get_track>> p = object + s->inuse;
+	 *   - mm/slub.c|1143| <<print_trailer>> s->inuse - s->object_size);
+	 *   - mm/slub.c|1148| <<print_trailer>> off = s->inuse;
+	 *   - mm/slub.c|1206| <<init_object>> memset(p + s->object_size, val, s->inuse - s->object_size);
+	 *   - mm/slub.c|1298| <<check_pad_bytes>> unsigned long off = s->inuse;
+	 *   - mm/slub.c|1399| <<check_object>> endobject, val, s->inuse - s->object_size))
+	 *   - mm/slub.c|1402| <<check_object>> if ((s->flags & SLAB_POISON) && s->object_size < s->inuse) {
+	 *   - mm/slub.c|2697| <<acquire_slab>> *objects = new.objects - new.inuse;
+	 */
 	int inuse;		/* Offset to metadata */
 	int align;		/* Alignment */
 	int reserved;		/* Reserved bytes at the end of slabs */
diff --git a/mm/kasan/kasan.c b/mm/kasan/kasan.c
index 71a43192..23e52193 100644
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@ -453,6 +453,11 @@ static inline depot_stack_handle_t save_stack(gfp_t flags)
 	return depot_save_stack(&trace, flags);
 }
 
+/*
+ * called by:
+ *   - mm/kasan/kasan.c|524| <<kasan_slab_free>> set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
+ *   - mm/kasan/kasan.c|551| <<kasan_kmalloc>> set_track(&get_alloc_info(cache, object)->alloc_track, flags);
+ */
 static inline void set_track(struct kasan_track *track, gfp_t flags)
 {
 	track->pid = current->pid;
diff --git a/mm/slab.h b/mm/slab.h
index 485d9fbb..1998cb24 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -351,6 +351,13 @@ static inline void memcg_link_cache(struct kmem_cache *s)
 
 #endif /* CONFIG_MEMCG && !CONFIG_SLOB */
 
+/*
+ * called by:
+ *   - mm/slub.c|3033| <<kmem_cache_free>> s = cache_from_obj(s, x);
+ *   - mm/slub.c|3094| <<build_detached_freelist>> df->s = cache_from_obj(s, object);
+ *   - mm/slab.c|3750| <<kmem_cache_free>> cachep = cache_from_obj(cachep, objp);
+ *   - mm/slab.c|3777| <<kmem_cache_free_bulk>> s = cache_from_obj(orig_s, objp);
+ */
 static inline struct kmem_cache *cache_from_obj(struct kmem_cache *s, void *x)
 {
 	struct kmem_cache *cachep;
@@ -408,6 +415,14 @@ static inline size_t slab_ksize(const struct kmem_cache *s)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2649| <<slab_alloc_node>> s = slab_pre_alloc_hook(s, gfpflags);
+ *   - mm/slub.c|3117| <<kmem_cache_alloc_bulk>> s = slab_pre_alloc_hook(s, flags);
+ *   - mm/slab.c|3297| <<slab_alloc_node>> cachep = slab_pre_alloc_hook(cachep, flags);
+ *   - mm/slab.c|3376| <<slab_alloc>> cachep = slab_pre_alloc_hook(cachep, flags);
+ *   - mm/slab.c|3575| <<kmem_cache_alloc_bulk>> s = slab_pre_alloc_hook(s, flags);
+ */
 static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
 						     gfp_t flags)
 {
@@ -428,6 +443,16 @@ static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
 	return s;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2764| <<slab_alloc_node>> slab_post_alloc_hook(s, gfpflags, 1, &object);
+ *   - mm/slub.c|3200| <<kmem_cache_alloc_bulk>> slab_post_alloc_hook(s, flags, size, p);
+ *   - mm/slub.c|3204| <<kmem_cache_alloc_bulk>> slab_post_alloc_hook(s, flags, i, p);
+ *   - mm/slab.c|3333| <<slab_alloc_node>> slab_post_alloc_hook(cachep, flags, 1, &ptr);
+ *   - mm/slab.c|3390| <<slab_alloc>> slab_post_alloc_hook(cachep, flags, 1, &objp);
+ *   - mm/slab.c|3598| <<kmem_cache_alloc_bulk>> slab_post_alloc_hook(s, flags, size, p);
+ *   - mm/slab.c|3604| <<kmem_cache_alloc_bulk>> slab_post_alloc_hook(s, flags, i, p);
+ */
 static inline void slab_post_alloc_hook(struct kmem_cache *s, gfp_t flags,
 					size_t size, void **p)
 {
@@ -469,7 +494,29 @@ struct kmem_cache_node {
 #endif
 
 #ifdef CONFIG_SLUB
+	/*
+	 * 增加nr_partial的地方:
+	 *   - mm/slub.c|2588| <<__add_partial>> n->nr_partial++;
+	 * 减少nr_partial的地方:
+	 *   - mm/slub.c|2613| <<remove_partial>> n->nr_partial--;
+	 *   - mm/slub.c|4943| <<__kmem_cache_shrink>> n->nr_partial--;
+	 * 其他使用nr_partial的地方:
+	 *   - mm/slub.c|2687| <<get_partial_node>> if (!n || !n->nr_partial)
+	 *   - mm/slub.c|2764| <<get_any_partial>> n->nr_partial > s->min_partial) {
+	 *   - mm/slub.c|2961| <<deactivate_slab>> if (!new.inuse && n->nr_partial >= s->min_partial)
+	 *   - mm/slub.c|3075| <<unfreeze_partials>> if (unlikely(!new.inuse && n->nr_partial >= s->min_partial)) {
+	 *   - mm/slub.c|3807| <<__slab_free>> if (unlikely(!new.inuse && n->nr_partial >= s->min_partial))
+	 *   - mm/slub.c|4273| <<init_kmem_cache_node>> n->nr_partial = 0;
+	 *   - mm/slub.c|4698| <<__kmem_cache_shutdown>> if (n->nr_partial || slabs_node(s, node))
+	 *   - mm/slub.c|5413| <<validate_slab_node>> if (count != n->nr_partial)
+	 *   - mm/slub.c|5415| <<validate_slab_node>> s->name, count, n->nr_partial);
+	 *   - mm/slub.c|5857| <<show_slab_objects>> x = n->nr_partial;
+	 */
 	unsigned long nr_partial;
+	/*
+	 * 比如page->lru是在__add_partial()加入partial的
+	 * 在remove_partial()删除的
+	 */
 	struct list_head partial;
 #ifdef CONFIG_SLUB_DEBUG
 	atomic_long_t nr_slabs;
diff --git a/mm/slab_common.c b/mm/slab_common.c
index f6764cf1..cf60942c 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -290,6 +290,11 @@ int slab_unmergeable(struct kmem_cache *s)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - mm/slab.c|1896| <<__kmem_cache_alias>> cachep = find_mergeable(size, align, flags, name, ctor);
+ *   - mm/slub.c|5106| <<__kmem_cache_alias>> s = find_mergeable(size, align, flags, name, ctor);
+ */
 struct kmem_cache *find_mergeable(size_t size, size_t align,
 		unsigned long flags, const char *name, void (*ctor)(void *))
 {
@@ -1061,6 +1066,12 @@ void __init setup_kmalloc_cache_index_table(void)
 	}
 }
 
+/*
+ * called by:
+ *   - mm/slab_common.c|1081| <<create_kmalloc_caches>> new_kmalloc_cache(i, flags);
+ *   - mm/slab_common.c|1089| <<create_kmalloc_caches>> new_kmalloc_cache(1, flags);
+ *   - mm/slab_common.c|1091| <<create_kmalloc_caches>> new_kmalloc_cache(2, flags);
+ */
 static void __init new_kmalloc_cache(int idx, unsigned long flags)
 {
 	kmalloc_caches[idx] = create_kmalloc_cache(kmalloc_info[idx].name,
diff --git a/mm/slub.c b/mm/slub.c
index 220d42e5..c713c96c 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -40,6 +40,101 @@
 
 #include "internal.h"
 
+/*
+ * NOTES:
+ *
+ * 1. s->offset: Free pointer offset
+ * 就是在calculate_sizes()设置, 在以下条件设置:
+ * 4898         if (((flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)) ||
+ * 4899                 s->ctor)) {
+ * 当slub_debug=ZUF(没有P)的时候, kmalloc-128的s->offset=0
+ * 当slub_debug=PZUF(有P)的时候, kmalloc-128的s->offset=136
+ */
+
+/*
+ * 从high level考虑,SLUB就是利用特殊区域填充特殊的magic num,在每一次
+ * alloc/free的时候检查magic num是否被意外修改.只申请内存而不释放的
+ * 话,是没法检测的.我们只能借助slabinfo工具主动触发检测功能。所以,这
+ * 也是SLUB DEBUG的一个劣势,它不能做到动态监测.它的检测机制是被动的.
+ *
+ * 一共四个debug的选项:
+ * - SLAB_CONSISTENCY_CHECKS
+ * - SLAB_RED_ZONE
+ * - SLAB_POISON
+ * - SLAB_STORE_USER
+ *
+ * SLUB DEBUG检测oob问题原理也很简单,既然为了发现是否越界,那么
+ * 就在分配出去的内存尾部添加一段额外的内存,填充特殊数字(magic num).
+ * 我们只需要检测这块额外的内存的数据是否被修改就可以知道是否发生了
+ * oob情况.而这段额外的内存就叫做Redzone.
+ *
+ * SLUB DEBUG关闭的情况下,free pointer是内嵌在object之中的,但是SLUB DEBUG
+ * 打开之后,free pointer是在object之外,并且多了很多其他的内存,
+ * 例如red zone,trace和red_left_pad等.这里之所以将FP后移就是因为为了检测
+ * use-after-free问题,当free object时会在将object填充magic num(0x6b).
+ * 如果不后移的话,岂不是破坏了object之间的单链表关系.
+ *
+ * 在没有slub_debug的时候:
+ * [Object size(with FP at the beginning 8-byte)][Obj align]
+ *
+ * 在slub_debug=PZU的时候:
+ * [Object size][Red zone][FP][alloc/free track][padding][red_left_pad]
+ *
+ * 其实slub_debug=PZU的时候应该是:
+ * [red_left_pad][Object size][Red zone][FP][alloc/free track][padding]
+ *
+ * - Redzone (主要检测右边oob)
+ * 从图中我们可以看到在object后面紧接着就是Red zone区域,那么Red zone有什
+ * 么作用呢?既然紧随其后,自然是检测右边界越界访问
+ * (right out-of-bounds access).原理很简单,在Red zone区域填充magic num,
+ * 检查Red zone区域数据是否被修改即可知道是否发生right oob. 可能你会想到
+ * 如果越过Redzone,直接改写了FP,岂不是检测不到oob了,并且链表结构也被破坏
+ * 了.其实在check_object()函数中会调用check_valid_pointer()来检查FP是否
+ * valid,如果invalid,同样会print error syslog.
+ *
+ * - padding
+ * padding是sizeof(void *) bytes的填充区域,在分配slab缓存池时,会将所有的内
+ * 存填充0x5a.同样在free/alloc object的时候作为检测的一种途径.如果padding
+ * 区域的数据不是0x5a,就代表发生了"Object padding overwritten"问题.这也是
+ * 有可能,越界跨度很大.
+ *
+ * - red_left_pad (检测左边的oob)
+ * 在struct page结构中有一个freelist指针,freelist会指向第一个available object.
+ * 在构建object之间的单链表的时候,object首地址实际上都会加上一个red_left_pad的
+ * 偏移,这样实际的layout就如同下面转换之后的layout:
+ *
+ * 其实slub_debug=PZU的时候应该是:
+ * [red_left_pad][Object size][Red zone][FP][alloc/free track][padding]
+ *
+ * 填充的magic num和Redzone一样,差别只是检测的区域不一样而已.
+ */
+
+/*
+ * 关于填充
+ *
+ * 从high level考虑,SLUB就是利用特殊区域填充特殊的magic num,在
+ * 每一次alloc/free的时候检查magic num是否被意外修改.
+ *
+ * - SLUB_RED_INACTIVE
+ *
+ * - SLUB_RED_ACTIVE
+ *
+ * - POISON_INUSE
+ *
+ * - POISON_FREE
+ */
+
+/*
+ * To detect out-of-bound:
+ *
+ * To detect use-after-free:
+ */
+
+/*
+ * - https://blog.csdn.net/juS3Ve/article/details/79285745
+ * - http://www.wowotech.net/memory_management/426.html
+ */
+
 /*
  * Lock order:
  *   1. slab_mutex (Global Mutex)
@@ -116,6 +211,20 @@
  * 			the fast path and disables lockless freelists.
  */
 
+ /*
+  * called by:
+  *   - mm/slub.c|130| <<fixup_red_left>> if (kmem_cache_debug(s) && s->flags & SLAB_RED_ZONE)
+  *   - mm/slub.c|139| <<kmem_cache_has_cpu_partial>> return !kmem_cache_debug(s);
+  *   - mm/slub.c|2097| <<deactivate_slab>> if (kmem_cache_debug(s) && !lock) {
+  *   - mm/slub.c|2594| <<___slab_alloc>> if (likely(!kmem_cache_debug(s) && pfmemalloc_match(page, gfpflags)))
+  *   - mm/slub.c|2598| <<___slab_alloc>> if (kmem_cache_debug(s) &&
+  *   - mm/slub.c|2814| <<__slab_free>> if (kmem_cache_debug(s) &&
+  *   - mm/slub.c|2889| <<__slab_free>> if (kmem_cache_debug(s))
+  *   - mm/slub.c|3845| <<__check_heap_object>> if (kmem_cache_debug(s) && s->flags & SLAB_RED_ZONE) {
+  */
+ /*
+  * 当kmem_cache->flags设置了debug的任何flag的时候返回 true
+  */
 static inline int kmem_cache_debug(struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_DEBUG
@@ -125,6 +234,16 @@ static inline int kmem_cache_debug(struct kmem_cache *s)
 #endif
 }
 
+/*
+ * called by:
+ *   - include/linux/slub_def.h|183| <<nearest_obj>> result = fixup_red_left(cache, result);
+ *   - mm/slub.c|432| <<for_each_object>> for (__p = fixup_red_left(__s, __addr); \
+ *   - mm/slub.c|437| <<for_each_object_idx>> for (__p = fixup_red_left(__s, __addr), __idx = 1; \
+ *   - mm/slub.c|1743| <<shuffle_freelist>> start = fixup_red_left(s, page_address(page));
+ *   - mm/slub.c|1841| <<allocate_slab>> page->freelist = fixup_red_left(s, start);
+ *
+ * 如果redzone被使用了,object的地址要往前移动s->red_left_pad
+ */
 void *fixup_red_left(struct kmem_cache *s, void *p)
 {
 	if (kmem_cache_debug(s) && s->flags & SLAB_RED_ZONE)
@@ -133,9 +252,28 @@ void *fixup_red_left(struct kmem_cache *s, void *p)
 	return p;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1842| <<get_partial_node>> if (!kmem_cache_has_cpu_partial(s)
+ *   - mm/slub.c|2831| <<__slab_free>> if (kmem_cache_has_cpu_partial(s) && !prior) {
+ *   - mm/slub.c|2888| <<__slab_free>> if (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {
+ *   - mm/slub.c|3445| <<set_cpu_partial>> if (!kmem_cache_has_cpu_partial(s))
+ *   - mm/slub.c|4958| <<cpu_partial_store>> if (objects && !kmem_cache_has_cpu_partial(s))
+ *
+ * 使用了CONFIG_SLUB_CPU_PARTIAL的情况下, 当kmem_cache->flags设置了debug的任何flag的时候返回false
+ * 没有CONFIG_SLUB_CPU_PARTIAL直接返回false
+ * ol支持CONFIG_SLUB_CPU_PARTIAL
+ * 说明用debug的时候不支持kmem_cache_has_cpu_partial
+ */
 static inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)
 {
+/*
+ * 在ol上使用了CONFIG_SLUB_CPU_PARTIAL
+ */
 #ifdef CONFIG_SLUB_CPU_PARTIAL
+	/*
+	 * 当kmem_cache->flags设置了debug的任何flag的时候返回 true
+	 */
 	return !kmem_cache_debug(s);
 #else
 	return false;
@@ -160,6 +298,11 @@ static inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)
  * Mininum number of partial slabs. These will be left on the partial
  * lists even if they are empty. kmem_cache_shrink may reclaim them.
  */
+/*
+ * 在以下使用MIN_PARTIAL:
+ *   - mm/slub.c|3812| <<set_min_partial>> if (min < MIN_PARTIAL)
+ *   - mm/slub.c|3813| <<set_min_partial>> min = MIN_PARTIAL;
+ */
 #define MIN_PARTIAL 5
 
 /*
@@ -167,6 +310,11 @@ static inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)
  * The existence of more partial slabs makes kmem_cache_shrink
  * sort the partial list by the number of objects in use.
  */
+/*
+ * 在以下使用MAX_PARTIAL:
+ *   - mm/slub.c|3814| <<set_min_partial>> else if (min > MAX_PARTIAL)
+ *   - mm/slub.c|3815| <<set_min_partial>> min = MAX_PARTIAL;
+ */
 #define MAX_PARTIAL 10
 
 #define DEBUG_DEFAULT_FLAGS (SLAB_CONSISTENCY_CHECKS | SLAB_RED_ZONE | \
@@ -188,11 +336,31 @@ static inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)
 #define DEBUG_METADATA_FLAGS (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER)
 
 #define OO_SHIFT	16
+/*
+ * 低16位都是1
+ */
 #define OO_MASK		((1 << OO_SHIFT) - 1)
 #define MAX_OBJS_PER_PAGE	32767 /* since page.objects is u15 */
 
 /* Internal SLUB flags */
+/*
+ * 在以下使用__OBJECT_POISON:
+ *   - mm/slub.c|950| <<init_object>> if (s->flags & __OBJECT_POISON) {
+ *   - mm/slub.c|1116| <<check_object>> if (val != SLUB_RED_ACTIVE && (s->flags & __OBJECT_POISON) &&
+ *   - mm/slub.c|1324| <<setup_object_debug>> if (!(s->flags & (SLAB_STORE_USER|SLAB_RED_ZONE|__OBJECT_POISON)))
+ *   - mm/slub.c|3877| <<calculate_sizes>> s->flags |= __OBJECT_POISON;
+ *   - mm/slub.c|3879| <<calculate_sizes>> s->flags &= ~__OBJECT_POISON;
+ */
 #define __OBJECT_POISON		0x80000000UL /* Poison object */
+/*
+ * 在以下使用__CMPXCHG_DOUBLE:
+ *   - mm/slub.c|548| <<__cmpxchg_double_slab>> if (s->flags & __CMPXCHG_DOUBLE) {
+ *   - mm/slub.c|588| <<cmpxchg_double_slab>> if (s->flags & __CMPXCHG_DOUBLE) {
+ *   - mm/slub.c|4008| <<kmem_cache_open>> s->flags |= __CMPXCHG_DOUBLE;
+ *   - mm/slub.c|5543| <<sanity_checks_store>> s->flags &= ~__CMPXCHG_DOUBLE;
+ *   - mm/slub.c|5568| <<trace_store>> s->flags &= ~__CMPXCHG_DOUBLE;
+ *   - mm/slub.c|5628| <<store_user_store>> s->flags &= ~__CMPXCHG_DOUBLE;
+ */
 #define __CMPXCHG_DOUBLE	0x40000000UL /* Use cmpxchg_double */
 
 /*
@@ -209,6 +377,19 @@ struct track {
 	unsigned long when;	/* When did the operation occur */
 };
 
+/*
+ * 在以下使用TRACK_ALLOC:
+ *   - mm/slub.c|799| <<init_tracking>> set_track(s, object, TRACK_ALLOC, 0UL);
+ *   - mm/slub.c|836| <<print_tracking>> print_track("Allocated", get_track(s, object, TRACK_ALLOC));
+ *   - mm/slub.c|1368| <<alloc_debug_processing>> set_track(s, object, TRACK_ALLOC, addr);
+ *   - mm/slub.c|5659| <<alloc_calls_show>> return list_locations(s, buf, TRACK_ALLOC);
+ *
+ * 在以下使用TRACK_FREE:
+ *   - mm/slub.c|798| <<init_tracking>> set_track(s, object, TRACK_FREE, 0UL);
+ *   - mm/slub.c|837| <<print_tracking>> print_track("Freed", get_track(s, object, TRACK_FREE));
+ *   - mm/slub.c|1448| <<free_debug_processing>> set_track(s, object, TRACK_FREE, addr);
+ *   - mm/slub.c|5667| <<free_calls_show>> return list_locations(s, buf, TRACK_FREE);
+ */
 enum track_item { TRACK_ALLOC, TRACK_FREE };
 
 #ifdef CONFIG_SYSFS
@@ -224,6 +405,39 @@ static inline void memcg_propagate_slab_attrs(struct kmem_cache *s) { }
 static inline void sysfs_slab_remove(struct kmem_cache *s) { }
 #endif
 
+/*
+ * called by:
+ *   - mm/slub.c|568| <<__cmpxchg_double_slab>> stat(s, CMPXCHG_DOUBLE_FAIL);
+ *   - mm/slub.c|613| <<cmpxchg_double_slab>> stat(s, CMPXCHG_DOUBLE_FAIL);
+ *   - mm/slub.c|1859| <<allocate_slab>> stat(s, ORDER_FALLBACK);
+ *   - mm/slub.c|2120| <<get_partial_node>> stat(s, ALLOC_FROM_PARTIAL);
+ *   - mm/slub.c|2124| <<get_partial_node>> stat(s, CPU_PARTIAL_NODE);
+ *   - mm/slub.c|2280| <<note_cmpxchg_failure>> stat(s, CMPXCHG_DOUBLE_CPU_FAIL);
+ *   - mm/slub.c|2314| <<deactivate_slab>> stat(s, DEACTIVATE_REMOTE_FREES);
+ *   - mm/slub.c|2416| <<deactivate_slab>> stat(s, tail);
+ *   - mm/slub.c|2420| <<deactivate_slab>> stat(s, DEACTIVATE_FULL);
+ *   - mm/slub.c|2437| <<deactivate_slab>> stat(s, DEACTIVATE_EMPTY);
+ *   - mm/slub.c|2439| <<deactivate_slab>> stat(s, FREE_SLAB);
+ *   - mm/slub.c|2496| <<unfreeze_partials>> stat(s, FREE_ADD_PARTIAL);
+ *   - mm/slub.c|2507| <<unfreeze_partials>> stat(s, DEACTIVATE_EMPTY);
+ *   - mm/slub.c|2509| <<unfreeze_partials>> stat(s, FREE_SLAB);
+ *   - mm/slub.c|2551| <<put_cpu_partial>> stat(s, CPU_PARTIAL_DRAIN);
+ *   - mm/slub.c|2577| <<flush_slab>> stat(s, CPUSLAB_FLUSH);
+ *   - mm/slub.c|2746| <<new_slab_objects>> stat(s, ALLOC_SLAB);
+ *   - mm/slub.c|2842| <<___slab_alloc>> stat(s, ALLOC_NODE_MISMATCH);
+ *   - mm/slub.c|2867| <<___slab_alloc>> stat(s, DEACTIVATE_BYPASS);
+ *   - mm/slub.c|2871| <<___slab_alloc>> stat(s, ALLOC_REFILL);
+ *   - mm/slub.c|2890| <<___slab_alloc>> stat(s, CPU_PARTIAL_ALLOC);
+ *   - mm/slub.c|3010| <<slab_alloc_node>> stat(s, ALLOC_SLOWPATH);
+ *   - mm/slub.c|3045| <<slab_alloc_node>> stat(s, ALLOC_FASTPATH);
+ *   - mm/slub.c|3150| <<__slab_free>> stat(s, FREE_SLOWPATH);
+ *   - mm/slub.c|3212| <<__slab_free>> stat(s, CPU_PARTIAL_FREE);
+ *   - mm/slub.c|3219| <<__slab_free>> stat(s, FREE_FROZEN);
+ *   - mm/slub.c|3234| <<__slab_free>> stat(s, FREE_ADD_PARTIAL);
+ *   - mm/slub.c|3245| <<__slab_free>> stat(s, FREE_REMOVE_PARTIAL);
+ *   - mm/slub.c|3252| <<__slab_free>> stat(s, FREE_SLAB);
+ *   - mm/slub.c|3332| <<do_slab_free>> stat(s, FREE_FASTPATH);
+ */
 static inline void stat(const struct kmem_cache *s, enum stat_item si)
 {
 #ifdef CONFIG_SLUB_STATS
@@ -244,6 +458,9 @@ static inline void stat(const struct kmem_cache *s, enum stat_item si)
  * with an XOR of the address where the pointer is held and a per-cache
  * random number.
  */
+/*
+ * 没有CONFIG_SLAB_FREELIST_HARDENED(uek4不支持)就返回ptr
+ */
 static inline void *freelist_ptr(const struct kmem_cache *s, void *ptr,
 				 unsigned long ptr_addr)
 {
@@ -255,6 +472,13 @@ static inline void *freelist_ptr(const struct kmem_cache *s, void *ptr,
 }
 
 /* Returns the freelist pointer recorded at location ptr_addr. */
+/*
+ * called by:
+ *   - mm/slub.c|495| <<get_freepointer>> return freelist_dereference(s, object + s->offset);
+ *   - mm/slub.c|505| <<prefetch_freepointer>> prefetch(freelist_dereference(s, object + s->offset));
+ *
+ * 没有CONFIG_SLAB_FREELIST_HARDENED(uek4不支持)就返回ptr_addr
+ */
 static inline void *freelist_dereference(const struct kmem_cache *s,
 					 void *ptr_addr)
 {
@@ -262,30 +486,95 @@ static inline void *freelist_dereference(const struct kmem_cache *s,
 			    (unsigned long)ptr_addr);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|318| <<get_freepointer_safe>> return get_freepointer(s, object);
+ *   - mm/slub.c|519| <<get_map>> for (p = page->freelist; p; p = get_freepointer(s, p))
+ *   - mm/slub.c|728| <<print_trailer>> p, p - addr, get_freepointer(s, p));
+ *   - mm/slub.c|966| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+ *   - mm/slub.c|1041| <<on_freelist>> fp = get_freepointer(s, object);
+ *   - mm/slub.c|1267| <<free_debug_processing>> object = get_freepointer(s, object);
+ *   - mm/slub.c|1445| <<slab_free_hook>> freeptr = get_freepointer(s, x);
+ *   - mm/slub.c|2114| <<deactivate_slab>> while (freelist && (nextfree = get_freepointer(s, freelist))) {
+ *   - mm/slub.c|2665| <<___slab_alloc>> c->freelist = get_freepointer(s, freelist);
+ *   - mm/slub.c|2696| <<___slab_alloc>> deactivate_slab(s, page, get_freepointer(s, freelist), c);
+ *   - mm/slub.c|3316| <<kmem_cache_alloc_bulk>> c->freelist = get_freepointer(s, object);
+ *   - mm/slub.c|3525| <<early_kmem_cache_node_alloc>> page->freelist = get_freepointer(kmem_cache_node, n);
+ *
+ * 没有CONFIG_SLAB_FREELIST_HARDENED(uek4不支持)就返回object + s->offset
+ */
 static inline void *get_freepointer(struct kmem_cache *s, void *object)
 {
+	/*
+	 * s->offset: Free pointer offset
+	 * 就是在calculate_sizes()设置, 在以下条件设置:
+	 * 4898         if (((flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)) ||
+	 * 4899                 s->ctor)) {
+	 * 当slub_debug=ZUF(没有P)的时候, kmalloc-128的s->offset=0
+	 * 当slub_debug=PZUF(有P)的时候, kmalloc-128的s->offset=136
+	 */
 	return freelist_dereference(s, object + s->offset);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2757| <<slab_alloc_node>> prefetch_freepointer(s, next_object);
+ */
 static void prefetch_freepointer(const struct kmem_cache *s, void *object)
 {
+	/*
+	 * freelist_dereference():
+	 * 没有CONFIG_SLAB_FREELIST_HARDENED(uek4不支持)就返回object + s->offset
+	 */
 	if (object)
 		prefetch(freelist_dereference(s, object + s->offset));
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2733| <<slab_alloc_node>> void *next_object = get_freepointer_safe(s, object);
+ *
+ * 核心思想是没有CONFIG_SLAB_FREELIST_HARDENED(uek4不支持)就返回object + s->offset的值
+ */
 static inline void *get_freepointer_safe(struct kmem_cache *s, void *object)
 {
 	unsigned long freepointer_addr;
 	void *p;
 
+	/*
+	 * 如果没有debug
+	 *
+	 * object理论是c->freelist
+	 *
+	 * get_freepointer(s, object):
+	 * 没有CONFIG_SLAB_FREELIST_HARDENED(uek4不支持)就返回object + s->offset
+	 */
 	if (!debug_pagealloc_enabled())
 		return get_freepointer(s, object);
 
 	freepointer_addr = (unsigned long)object + s->offset;
 	probe_kernel_read(&p, (void **)freepointer_addr, sizeof(p));
+	/* 没有CONFIG_SLAB_FREELIST_HARDENED(uek4不支持)就返回ptr */
 	return freelist_ptr(s, p, freepointer_addr);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1249| <<check_object>> set_freepointer(s, p, NULL);
+ *   - mm/slub.c|1312| <<on_freelist>> set_freepointer(s, object, NULL);
+ *   - mm/slub.c|1906| <<shuffle_freelist>> set_freepointer(s, cur, next);
+ *   - mm/slub.c|1910| <<shuffle_freelist>> set_freepointer(s, cur, NULL);
+ *   - mm/slub.c|1989| <<allocate_slab>> set_freepointer(s, p, p + s->size);
+ *   - mm/slub.c|1991| <<allocate_slab>> set_freepointer(s, p, NULL);
+ *   - mm/slub.c|2439| <<deactivate_slab>> set_freepointer(s, freelist, prior);
+ *   - mm/slub.c|2476| <<deactivate_slab>> set_freepointer(s, freelist, old.freelist);
+ *   - mm/slub.c|3273| <<__slab_free>> set_freepointer(s, tail, prior);
+ *   - mm/slub.c|3423| <<do_slab_free>> set_freepointer(s, tail_obj, c->freelist);
+ *   - mm/slub.c|3558| <<build_detached_freelist>> set_freepointer(df->s, object, NULL);
+ *   - mm/slub.c|3572| <<build_detached_freelist>> set_freepointer(df->s, object, df->freelist);
+ *
+ * 核心思想是设置object + s->offset的值为fp
+ */
 static inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)
 {
 	unsigned long freeptr_addr = (unsigned long)object + s->offset;
@@ -298,17 +587,39 @@ static inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)
 }
 
 /* Loop over all objects in a slab */
+/*
+ * called by:
+ *   - mm/slub.c|2043| <<__free_slab>> for_each_object(p, s, page_address(page),
+ *   - mm/slub.c|4169| <<list_slab_objects>> for_each_object(p, s, addr, page->objects) {
+ *   - mm/slub.c|4892| <<validate_slab>> for_each_object(p, s, addr, page->objects) {
+ *   - mm/slub.c|4898| <<validate_slab>> for_each_object(p, s, addr, page->objects)
+ *   - mm/slub.c|5118| <<process_slab>> for_each_object(p, s, addr, page->objects)
+ *
+ * Loop over all objects in a slab
+ */
 #define for_each_object(__p, __s, __addr, __objects) \
 	for (__p = fixup_red_left(__s, __addr); \
 		__p < (__addr) + (__objects) * (__s)->size; \
 		__p += (__s)->size)
 
+/*
+ * called by:
+ *   - mm/slub.c|1986| <<allocate_slab>> for_each_object_idx(p, idx, s, start, page->objects) {
+ */
 #define for_each_object_idx(__p, __idx, __s, __addr, __objects) \
 	for (__p = fixup_red_left(__s, __addr), __idx = 1; \
 		__idx <= __objects; \
 		__p += (__s)->size, __idx++)
 
 /* Determine object index from a given position */
+/*
+ * called by:
+ *   - mm/slub.c|464| <<get_map>> set_bit(slab_index(p, s, addr), map);
+ *   - mm/slub.c|3662| <<list_slab_objects>> if (!test_bit(slab_index(p, s, addr), map)) {
+ *   - mm/slub.c|4362| <<validate_slab>> if (test_bit(slab_index(p, s, addr), map))
+ *   - mm/slub.c|4368| <<validate_slab>> if (!test_bit(slab_index(p, s, addr), map))
+ *   - mm/slub.c|4569| <<process_slab>> if (!test_bit(slab_index(p, s, addr), map))
+ */
 static inline int slab_index(void *p, struct kmem_cache *s, void *addr)
 {
 	return (p - addr) / s->size;
@@ -319,6 +630,11 @@ static inline int order_objects(int order, unsigned long size, int reserved)
 	return ((PAGE_SIZE << order) - reserved) / size;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4113| <<calculate_sizes>> s->oo = oo_make(order, size, s->reserved);
+ *   - mm/slub.c|4114| <<calculate_sizes>> s->min = oo_make(get_order(size), size, s->reserved);
+ */
 static inline struct kmem_cache_order_objects oo_make(int order,
 		unsigned long size, int reserved)
 {
@@ -342,18 +658,41 @@ static inline int oo_objects(struct kmem_cache_order_objects x)
 /*
  * Per slab locking using the pagelock
  */
+/*
+ * called by:
+ *   - mm/slub.c|698| <<__cmpxchg_double_slab>> slab_lock(page);
+ *   - mm/slub.c|741| <<cmpxchg_double_slab>> slab_lock(page);
+ *   - mm/slub.c|1574| <<free_debug_processing>> slab_lock(page);
+ *   - mm/slub.c|4202| <<list_slab_objects>> slab_lock(page);
+ *   - mm/slub.c|4949| <<validate_slab_slab>> slab_lock(page);
+ */
 static __always_inline void slab_lock(struct page *page)
 {
 	VM_BUG_ON_PAGE(PageTail(page), page);
 	bit_spin_lock(PG_locked, &page->flags);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|703| <<__cmpxchg_double_slab>> slab_unlock(page);
+ *   - mm/slub.c|706| <<__cmpxchg_double_slab>> slab_unlock(page);
+ *   - mm/slub.c|746| <<cmpxchg_double_slab>> slab_unlock(page);
+ *   - mm/slub.c|750| <<cmpxchg_double_slab>> slab_unlock(page);
+ *   - mm/slub.c|1607| <<free_debug_processing>> slab_unlock(page);
+ *   - mm/slub.c|4212| <<list_slab_objects>> slab_unlock(page);
+ *   - mm/slub.c|4951| <<validate_slab_slab>> slab_unlock(page);
+ */
 static __always_inline void slab_unlock(struct page *page)
 {
 	VM_BUG_ON_PAGE(PageTail(page), page);
 	__bit_spin_unlock(PG_locked, &page->flags);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|666| <<__cmpxchg_double_slab>> set_page_slub_counters(page, counters_new);
+ *   - mm/slub.c|709| <<cmpxchg_double_slab>> set_page_slub_counters(page, counters_new);
+ */
 static inline void set_page_slub_counters(struct page *page, unsigned long counters_new)
 {
 	struct page tmp;
@@ -370,6 +709,18 @@ static inline void set_page_slub_counters(struct page *page, unsigned long count
 }
 
 /* Interrupts must be disabled (for the fallback code to work right) */
+/*
+ * called by:
+ *   - mm/slub.c|1832| <<acquire_slab>> if (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2087| <<deactivate_slab>> } while (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2176| <<deactivate_slab>> if (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2235| <<unfreeze_partials>> } while (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2534| <<get_freelist>> } while (!__cmpxchg_double_slab(s, page,
+ *
+ * 我们期待__cmpxchg_double_slab()返回true, 不希望false
+ * 核心思想是判断page->freelist和page->counters是否和old的相等
+ * 如果相等,则吧page->freelist和page->counters都更新成新的
+ */
 static inline bool __cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
 		void *freelist_old, unsigned long counters_old,
 		void *freelist_new, unsigned long counters_new,
@@ -407,6 +758,14 @@ static inline bool __cmpxchg_double_slab(struct kmem_cache *s, struct page *page
 	return false;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2938| <<__slab_free>> } while (!cmpxchg_double_slab(s, page,
+ *
+ * 我们期待cmpxchg_double_slab()返回true, 不希望false
+ * 核心思想是判断page->freelist和page->counters是否和old的相等
+ * 如果相等,则吧page->freelist和page->counters都更新成新的
+ */
 static inline bool cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
 		void *freelist_old, unsigned long counters_old,
 		void *freelist_new, unsigned long counters_new,
@@ -455,6 +814,15 @@ static inline bool cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
  * Node listlock must be held to guarantee that the page does
  * not vanish from under us.
  */
+/*
+ * called by:
+ *   - mm/slub.c|4204| <<list_slab_objects>> get_map(s, page, map);
+ *   - mm/slub.c|4927| <<validate_slab>> get_map(s, page, map);
+ *   - mm/slub.c|5152| <<process_slab>> get_map(s, page, map);
+ *
+ * 把page中可用的object返回到map对应的bit
+ * Determine a map of object in use on a page.
+ */
 static void get_map(struct kmem_cache *s, struct page *page, unsigned long *map)
 {
 	void *p;
@@ -472,6 +840,10 @@ static inline int size_from_object(struct kmem_cache *s)
 	return s->size;
 }
 
+/*
+ * 如果kmem_cache->flags支持SLAB_RED_ZONE, p往左移动s->red_left_pad
+ * 否则直接返回p
+ */
 static inline void *restore_red_left(struct kmem_cache *s, void *p)
 {
 	if (s->flags & SLAB_RED_ZONE)
@@ -489,7 +861,20 @@ static int slub_debug = DEBUG_DEFAULT_FLAGS;
 static int slub_debug;
 #endif
 
+/*
+ * 在以下使用slub_debug_slabs:
+ *   - mm/slub.c|1675| <<setup_slub_debug>> slub_debug_slabs = str + 1;
+ *   - mm/slub.c|1694| <<kmem_cache_flags>> if (slub_debug && (!slub_debug_slabs || (name &&
+ *   - mm/slub.c|1695| <<kmem_cache_flags>> !strncmp(slub_debug_slabs, name, strlen(slub_debug_slabs)))))
+ */
 static char *slub_debug_slabs;
+/*
+ * 在以下使用:
+ *   - mm/slub.c|1728| <<global>> #define disable_higher_order_debug 0
+ *   - mm/slub.c|1665| <<setup_slub_debug>> disable_higher_order_debug = 1;
+ *   - mm/slub.c|4138| <<kmem_cache_open>> if (disable_higher_order_debug) {
+ *   - mm/slub.c|6279| <<sysfs_slab_add>> if (!unmergeable && disable_higher_order_debug &&
+ */
 static int disable_higher_order_debug;
 
 /*
@@ -498,6 +883,13 @@ static int disable_higher_order_debug;
  * be reported by kasan as a bounds error.  metadata_access_enable() is used
  * to tell kasan that these accesses are OK.
  */
+/*
+ * called by:
+ *   - mm/slub.c|857| <<print_section>> metadata_access_enable();
+ *   - mm/slub.c|910| <<set_track>> metadata_access_enable();
+ *   - mm/slub.c|1115| <<check_bytes_and_report>> metadata_access_enable();
+ *   - mm/slub.c|1212| <<slab_pad_check>> metadata_access_enable();
+ */
 static inline void metadata_access_enable(void)
 {
 	kasan_disable_current();
@@ -513,6 +905,15 @@ static inline void metadata_access_disable(void)
  */
 
 /* Verify that a pointer has an address that is valid within a slab page */
+/*
+ * called by:
+ *   - mm/slub.c|1121| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+ *   - mm/slub.c|1181| <<on_freelist>> if (!check_valid_pointer(s, page, fp)) {
+ *   - mm/slub.c|1317| <<alloc_consistency_checks>> if (!check_valid_pointer(s, page, object)) {
+ *   - mm/slub.c|1365| <<free_consistency_checks>> if (!check_valid_pointer(s, page, object)) {
+ *
+ * 核心思想是判断一个object是否在page范围内, 而且object的地址是align正确的地址
+ */
 static inline int check_valid_pointer(struct kmem_cache *s,
 				struct page *page, void *object)
 {
@@ -522,6 +923,10 @@ static inline int check_valid_pointer(struct kmem_cache *s,
 		return 1;
 
 	base = page_address(page);
+	/*
+	 * 如果kmem_cache->flags支持SLAB_RED_ZONE, p往左移动s->red_left_pad
+	 * 否则直接返回p
+	 */
 	object = restore_red_left(s, object);
 	if (object < base || object >= base + page->objects * s->size ||
 		(object - base) % s->size) {
@@ -531,6 +936,16 @@ static inline int check_valid_pointer(struct kmem_cache *s,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1035| <<print_trailer>> print_section(KERN_ERR, "Redzone ", p - s->red_left_pad,
+ *   - mm/slub.c|1038| <<print_trailer>> print_section(KERN_ERR, "Bytes b4 ", p - 16, 16);
+ *   - mm/slub.c|1040| <<print_trailer>> print_section(KERN_ERR, "Object ", p,
+ *   - mm/slub.c|1043| <<print_trailer>> print_section(KERN_ERR, "Redzone ", p + s->object_size,
+ *   - mm/slub.c|1058| <<print_trailer>> print_section(KERN_ERR, "Padding ", p + off,
+ *   - mm/slub.c|1221| <<slab_pad_check>> print_section(KERN_ERR, "Padding ", end - remainder, remainder);
+ *   - mm/slub.c|1393| <<trace>> print_section(KERN_INFO, "Object ", (void *)object,
+ */
 static void print_section(char *level, char *text, u8 *addr,
 			  unsigned int length)
 {
@@ -540,6 +955,13 @@ static void print_section(char *level, char *text, u8 *addr,
 	metadata_access_disable();
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|899| <<set_track>> struct track *p = get_track(s, object, alloc);
+ *   - mm/slub.c|978| <<print_tracking>> print_track("Allocated", get_track(s, object, TRACK_ALLOC));
+ *   - mm/slub.c|979| <<print_tracking>> print_track("Freed", get_track(s, object, TRACK_FREE));
+ *   - mm/slub.c|5156| <<process_slab>> add_location(t, s, get_track(s, p, alloc));
+ */
 static struct track *get_track(struct kmem_cache *s, void *object,
 	enum track_item alloc)
 {
@@ -553,6 +975,26 @@ static struct track *get_track(struct kmem_cache *s, void *object,
 	return p + alloc;
 }
 
+/*
+ * [0] set_track
+ * [0] alloc_debug_processing
+ * [0] ___slab_alloc
+ * [0] __slab_alloc
+ * [0] kmem_cache_alloc
+ * [0] testsys_store
+ * [0] kernfs_fop_write
+ * [0] __vfs_write
+ * [0] vfs_write
+ * [0] SyS_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - mm/slub.c|666| <<init_tracking>> set_track(s, object, TRACK_FREE, 0UL);
+ *   - mm/slub.c|667| <<init_tracking>> set_track(s, object, TRACK_ALLOC, 0UL);
+ *   - mm/slub.c|1195| <<alloc_debug_processing>> set_track(s, object, TRACK_ALLOC, addr);
+ *   - mm/slub.c|1275| <<free_debug_processing>> set_track(s, object, TRACK_FREE, addr);
+ */
 static void set_track(struct kmem_cache *s, void *object,
 			enum track_item alloc, unsigned long addr)
 {
@@ -587,15 +1029,28 @@ static void set_track(struct kmem_cache *s, void *object,
 		memset(p, 0, sizeof(struct track));
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1163| <<setup_object_debug>> init_tracking(s, object);
+ *   - mm/slub.c|3546| <<early_kmem_cache_node_alloc>> init_tracking(kmem_cache_node, n);
+ */
 static void init_tracking(struct kmem_cache *s, void *object)
 {
 	if (!(s->flags & SLAB_STORE_USER))
 		return;
 
+	/*
+	 * set_track()最后一个参数是0的时候, 直接把struct track给memset成0
+	 */
 	set_track(s, object, TRACK_FREE, 0UL);
 	set_track(s, object, TRACK_ALLOC, 0UL);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|711| <<print_tracking>> print_track("Allocated", get_track(s, object, TRACK_ALLOC));
+ *   - mm/slub.c|712| <<print_tracking>> print_track("Freed", get_track(s, object, TRACK_FREE));
+ */
 static void print_track(const char *s, struct track *t)
 {
 	if (!t->addr)
@@ -615,6 +1070,11 @@ static void print_track(const char *s, struct track *t)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|738| <<print_trailer>> print_tracking(s, p);
+ *   - mm/slub.c|3851| <<list_slab_objects>> print_tracking(s, p);
+ */
 static void print_tracking(struct kmem_cache *s, void *object)
 {
 	if (!(s->flags & SLAB_STORE_USER))
@@ -624,6 +1084,11 @@ static void print_tracking(struct kmem_cache *s, void *object)
 	print_track("Freed", get_track(s, object, TRACK_FREE));
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1102| <<print_trailer>> print_page_info(page);
+ *   - mm/slub.c|1154| <<slab_err>> print_page_info(page);
+ */
 static void print_page_info(struct page *page)
 {
 	pr_err("INFO: Slab 0x%p objects=%u used=%u fp=0x%p flags=0x%04lx\n",
@@ -631,6 +1096,9 @@ static void print_page_info(struct page *page)
 
 }
 
+/*
+ * 主要是打印信息
+ */
 static void slab_bug(struct kmem_cache *s, char *fmt, ...)
 {
 	struct va_format vaf;
@@ -647,6 +1115,15 @@ static void slab_bug(struct kmem_cache *s, char *fmt, ...)
 	va_end(args);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1177| <<restore_bytes>> slab_fix(s, "Restoring 0x%p-0x%p=0x%x\n", from, to - 1, data);
+ *   - mm/slub.c|1426| <<on_freelist>> slab_fix(s, "Freelist cleared");
+ *   - mm/slub.c|1444| <<on_freelist>> slab_fix(s, "Number of objects adjusted.");
+ *   - mm/slub.c|1450| <<on_freelist>> slab_fix(s, "Object count adjusted.");
+ *   - mm/slub.c|1595| <<alloc_debug_processing>> slab_fix(s, "Marking all objects used");
+ *   - mm/slub.c|1683| <<free_debug_processing>> slab_fix(s, "Object at 0x%p not freed", object);
+ */
 static void slab_fix(struct kmem_cache *s, char *fmt, ...)
 {
 	struct va_format vaf;
@@ -659,6 +1136,11 @@ static void slab_fix(struct kmem_cache *s, char *fmt, ...)
 	va_end(args);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|779| <<object_err>> print_trailer(s, page, object);
+ *   - mm/slub.c|839| <<check_bytes_and_report>> print_trailer(s, page, object);
+ */
 static void print_trailer(struct kmem_cache *s, struct page *page, u8 *p)
 {
 	unsigned int off;	/* Offset of last byte */
@@ -722,6 +1204,15 @@ static __printf(3, 4) void slab_err(struct kmem_cache *s, struct page *page,
 	dump_stack();
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1565| <<setup_object_debug>> init_object(s, object, SLUB_RED_INACTIVE);
+ *   - mm/slub.c|1608| <<alloc_debug_processing>> init_object(s, object, SLUB_RED_ACTIVE);
+ *   - mm/slub.c|1689| <<free_debug_processing>> init_object(s, object, SLUB_RED_INACTIVE);
+ *   - mm/slub.c|3990| <<early_kmem_cache_node_alloc>> init_object(kmem_cache_node, n, SLUB_RED_ACTIVE);
+ *
+ * 根据有没有SLAB_RED_ZONE和__OBJECT_POISON初始化embed的数据
+ */
 static void init_object(struct kmem_cache *s, void *object, u8 val)
 {
 	u8 *p = object;
@@ -738,6 +1229,11 @@ static void init_object(struct kmem_cache *s, void *object, u8 val)
 		memset(p + s->object_size, val, s->inuse - s->object_size);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1226| <<check_bytes_and_report>> restore_bytes(s, what, value, fault, end);
+ *   - mm/slub.c|1319| <<slab_pad_check>> restore_bytes(s, "slab padding", POISON_INUSE, end - remainder, end);
+ */
 static void restore_bytes(struct kmem_cache *s, char *message, u8 data,
 						void *from, void *to)
 {
@@ -745,6 +1241,17 @@ static void restore_bytes(struct kmem_cache *s, char *message, u8 data,
 	memset(from, data, to - from);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1285| <<check_pad_bytes>> return check_bytes_and_report(s, page, p, "Object padding",
+ *   - mm/slub.c|1338| <<check_object>> if (!check_bytes_and_report(s, page, object, "Redzone",
+ *   - mm/slub.c|1342| <<check_object>> if (!check_bytes_and_report(s, page, object, "Redzone",
+ *   - mm/slub.c|1347| <<check_object>> check_bytes_and_report(s, page, p, "Alignment padding",
+ *   - mm/slub.c|1355| <<check_object>> (!check_bytes_and_report(s, page, p, "Poison", p,
+ *   - mm/slub.c|1357| <<check_object>> !check_bytes_and_report(s, page, p, "Poison",
+ *
+ * 每一种情况(Redzone或者Poison)都会调用这个函数一次
+ */
 static int check_bytes_and_report(struct kmem_cache *s, struct page *page,
 			u8 *object, char *what,
 			u8 *start, unsigned int value, unsigned int bytes)
@@ -864,6 +1371,42 @@ static int slab_pad_check(struct kmem_cache *s, struct page *page)
 	return 0;
 }
 
+/*
+ * [0] check_slab
+ * [0] alloc_debug_processing
+ * [0] ___slab_alloc
+ * [0] __slab_alloc
+ * [0] kmem_cache_alloc
+ * [0] jbd2__journal_start
+ * [0] ext4_dirty_inode
+ * [0] __mark_inode_dirty
+ * [0] generic_update_time
+ * [0] file_update_time
+ * [0] __generic_file_write_iter
+ * [0] ext4_file_write_iter
+ * [0] __vfs_write
+ * [0] vfs_write
+ * [0] SyS_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] check_slab
+ * [0] free_debug_processing
+ * [0] __slab_free
+ * [0] kernfs_fop_write
+ * [0] __vfs_write
+ * [0] vfs_write
+ * [0] SyS_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - mm/slub.c|1322| <<alloc_consistency_checks>> if (!check_object(s, page, object, SLUB_RED_INACTIVE))
+ *   - mm/slub.c|1375| <<free_consistency_checks>> if (!check_object(s, page, object, SLUB_RED_ACTIVE))
+ *   - mm/slub.c|1914| <<__free_slab>> check_object(s, page, p, SLUB_RED_INACTIVE);
+ *   - mm/slub.c|4754| <<validate_slab>> if (!check_object(s, page, p, SLUB_RED_INACTIVE))
+ *   - mm/slub.c|4760| <<validate_slab>> if (!check_object(s, page, p, SLUB_RED_ACTIVE))
+ */
 static int check_object(struct kmem_cache *s, struct page *page,
 					void *object, u8 val)
 {
@@ -920,6 +1463,14 @@ static int check_object(struct kmem_cache *s, struct page *page,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1314| <<alloc_consistency_checks>> if (!check_slab(s, page))
+ *   - mm/slub.c|1410| <<free_debug_processing>> if (!check_slab(s, page))
+ *   - mm/slub.c|4744| <<validate_slab>> if (!check_slab(s, page) ||
+ *
+ * 检查某一个page上的object数量是否正确
+ */
 static int check_slab(struct kmem_cache *s, struct page *page)
 {
 	int maxobj;
@@ -951,6 +1502,11 @@ static int check_slab(struct kmem_cache *s, struct page *page)
  * Determine if a certain object on a page is on the freelist. Must hold the
  * slab lock to guarantee that the chains are in a consistent state.
  */
+/*
+ * called by:
+ *   - mm/slub.c|1190| <<free_consistency_checks>> if (on_freelist(s, page, object)) {
+ *   - mm/slub.c|4498| <<validate_slab>> !on_freelist(s, page, NULL))
+ */
 static int on_freelist(struct kmem_cache *s, struct page *page, void *search)
 {
 	int nr = 0;
@@ -962,6 +1518,9 @@ static int on_freelist(struct kmem_cache *s, struct page *page, void *search)
 	while (fp && nr <= page->objects) {
 		if (fp == search)
 			return 1;
+		/*
+		 * 核心思想是判断一个object是否在page范围内, 而且object的地址是align正确的地址
+		 */
 		if (!check_valid_pointer(s, page, fp)) {
 			if (object) {
 				object_err(s, page, object,
@@ -977,6 +1536,7 @@ static int on_freelist(struct kmem_cache *s, struct page *page, void *search)
 			break;
 		}
 		object = fp;
+		/* 没有CONFIG_SLAB_FREELIST_HARDENED(uek4不支持)就返回object + s->offset */
 		fp = get_freepointer(s, object);
 		nr++;
 	}
@@ -1000,9 +1560,51 @@ static int on_freelist(struct kmem_cache *s, struct page *page, void *search)
 	return search == NULL;
 }
 
+/*
+ * echo 1 > /sys/kernel/slab/kmalloc-128/trace
+ *
+ * [  165.344090] TRACE kmalloc-128 alloc 0xffff9ee275e02578 inuse=17 fp=0x          (null)
+ * [  165.344092] CPU: 1 PID: 15 Comm: kworker/1:0 Not tainted 4.14.113 #4
+ * [  165.344093] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-59-gc9ba5276e321-prebuilt.qemu.org 04/01/2014
+ * [  165.344096] Workqueue: events_freezable_power_ disk_events_workfn
+ * [  165.344098] Call Trace:
+ * [  165.344102]  dump_stack+0x46/0x64
+ * [  165.344104]  alloc_debug_processing+0xc0/0x127
+ * [  165.344107]  ___slab_alloc+0x476/0x4b0
+ * [  165.344110]  ? scsi_old_init_rq+0xa7/0x170
+ * [  165.344113]  ? bio_alloc_bioset+0x1d0/0x270
+ * [  165.344115]  ? alloc_request_size+0x48/0x60
+ * [  165.344118]  ? mempool_alloc+0x5a/0x150
+ * [  165.344120]  ? bio_alloc_bioset+0x1d0/0x270
+ * [  165.344122]  __slab_alloc+0x9/0xd
+ * [  165.344124]  __kmalloc+0x13b/0x190
+ * [  165.344127]  bio_alloc_bioset+0x1d0/0x270
+ * [  165.344129]  bio_copy_kern+0x62/0x150
+ * [  165.344132]  blk_rq_map_kern+0x63/0x120
+ * [  165.344134]  scsi_execute+0x13c/0x1e0
+ * [  165.344137]  sr_check_events+0xa7/0x290
+ * [  165.344140]  ? __switch_to_asm+0x30/0x60
+ * [  165.344141]  ? __switch_to_asm+0x24/0x60
+ * [  165.344144]  cdrom_check_events+0x15/0x30
+ * [  165.344146]  sr_block_check_events+0x78/0xb0
+ * [  165.344147]  disk_check_events+0x4e/0x130
+ * [  165.344150]  process_one_work+0x139/0x350
+ * [  165.344152]  worker_thread+0x3f/0x3b0
+ * [  165.344155]  kthread+0xfa/0x130
+ * [  165.344156]  ? process_one_work+0x350/0x350
+ * [  165.344158]  ? __kthread_parkme+0x90/0x90
+ * [  165.344160]  ret_from_fork+0x35/0x40
+ *
+ * called by:
+ *   - mm/slub.c|1607| <<alloc_debug_processing>> trace(s, page, object, 1);
+ *   - mm/slub.c|1687| <<free_debug_processing>> trace(s, page, object, 0);
+ */
 static void trace(struct kmem_cache *s, struct page *page, void *object,
 								int alloc)
 {
+	/*
+	 * Trace allocations and frees
+	 */
 	if (s->flags & SLAB_TRACE) {
 		pr_info("TRACE %s %s 0x%p inuse=%d fp=0x%p\n",
 			s->name,
@@ -1021,6 +1623,12 @@ static void trace(struct kmem_cache *s, struct page *page, void *object,
 /*
  * Tracking of fully allocated slabs for debugging purposes.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2763| <<deactivate_slab>> add_full(s, n, page);
+ *
+ * 如果s->flags设置了SLAB_STORE_USER, 则把page->lru加入到kmem_cache_node->full
+ */
 static void add_full(struct kmem_cache *s,
 	struct kmem_cache_node *n, struct page *page)
 {
@@ -1031,6 +1639,14 @@ static void add_full(struct kmem_cache *s,
 	list_add(&page->lru, &n->full);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2753| <<deactivate_slab>> remove_full(s, n, page);
+ *   - mm/slub.c|3574| <<__slab_free>> remove_full(s, n, page);
+ *   - mm/slub.c|3590| <<__slab_free>> remove_full(s, n, page);
+ *
+ * 如果s->flags设置了SLAB_STORE_USER, 则把page->lru从kmem_cache_node->full删除
+ */
 static void remove_full(struct kmem_cache *s, struct kmem_cache_node *n, struct page *page)
 {
 	if (!(s->flags & SLAB_STORE_USER))
@@ -1041,6 +1657,12 @@ static void remove_full(struct kmem_cache *s, struct kmem_cache_node *n, struct
 }
 
 /* Tracking of the number of slabs for debugging purposes */
+/*
+ * called by:
+ *   - mm/slub.c|4456| <<__kmem_cache_shutdown>> if (n->nr_partial || slabs_node(s, node))
+ *   - mm/slub.c|4719| <<__kmem_cache_shrink>> if (slabs_node(s, node))
+ *   - mm/slub.c|4800| <<slab_mem_offline_callback>> BUG_ON(slabs_node(s, offline_node));
+ */
 static inline unsigned long slabs_node(struct kmem_cache *s, int node)
 {
 	struct kmem_cache_node *n = get_node(s, node);
@@ -1048,13 +1670,28 @@ static inline unsigned long slabs_node(struct kmem_cache *s, int node)
 	return atomic_long_read(&n->nr_slabs);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|3050| <<slab_out_of_memory>> nr_slabs = node_nr_slabs(n);
+ *   - mm/slub.c|6652| <<get_slabinfo>> nr_slabs += node_nr_slabs(n);
+ */
 static inline unsigned long node_nr_slabs(struct kmem_cache_node *n)
 {
 	return atomic_long_read(&n->nr_slabs);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2246| <<allocate_slab>> inc_slabs_node(s, page_to_nid(page), page->objects);
+ *   - mm/slub.c|4100| <<early_kmem_cache_node_alloc>> inc_slabs_node(kmem_cache_node, node, page->objects);
+ */
 static inline void inc_slabs_node(struct kmem_cache *s, int node, int objects)
 {
+	/*
+	 * struct kmem_cache_node *node[MAX_NUMNODES];
+	 *
+	 * 返回s->node[node]
+	 */
 	struct kmem_cache_node *n = get_node(s, node);
 
 	/*
@@ -1068,6 +1705,10 @@ static inline void inc_slabs_node(struct kmem_cache *s, int node, int objects)
 		atomic_long_add(objects, &n->total_objects);
 	}
 }
+/*
+ * called by:
+ *   - mm/slub.c|2336| <<discard_slab>> dec_slabs_node(s, page_to_nid(page), page->objects);
+ */
 static inline void dec_slabs_node(struct kmem_cache *s, int node, int objects)
 {
 	struct kmem_cache_node *n = get_node(s, node);
@@ -1077,16 +1718,52 @@ static inline void dec_slabs_node(struct kmem_cache *s, int node, int objects)
 }
 
 /* Object debug checks for alloc/free paths */
+/*
+ * [0] setup_object_debug
+ * [0] new_slab
+ * [0] ___slab_alloc
+ * [0] __slab_alloc
+ * [0] kmem_cache_alloc
+ * [0] mempool_alloc
+ * [0] bvec_alloc
+ * [0] bio_alloc_bioset
+ * [0] ext4_bio_write_page
+ * [0] mpage_submit_page
+ * [0] mpage_process_page_bufs
+ * [0] mpage_prepare_extent_to_map
+ * [0] ext4_writepages
+ * [0] do_writepages
+ * [0] __filemap_fdatawrite_range
+ * [0] file_write_and_wait_range
+ * [0] ext4_sync_file
+ * [0] do_fsync
+ * [0] SyS_fsync
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - mm/slub.c|1521| <<setup_object>> setup_object_debug(s, page, object);
+ *
+ * 根据有没有SLAB_RED_ZONE和__OBJECT_POISON初始化embed的数据
+ * 再就是init_tracking(s, object);
+ */
 static void setup_object_debug(struct kmem_cache *s, struct page *page,
 								void *object)
 {
 	if (!(s->flags & (SLAB_STORE_USER|SLAB_RED_ZONE|__OBJECT_POISON)))
 		return;
 
+	/*
+	 * 根据有没有SLAB_RED_ZONE和__OBJECT_POISON初始化embed的数据
+	 */
 	init_object(s, object, SLUB_RED_INACTIVE);
 	init_tracking(s, object);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1337| <<alloc_debug_processing>> if (!alloc_consistency_checks(s, page, object, addr))
+ */
 static inline int alloc_consistency_checks(struct kmem_cache *s,
 					struct page *page,
 					void *object, unsigned long addr)
@@ -1105,6 +1782,27 @@ static inline int alloc_consistency_checks(struct kmem_cache *s,
 	return 1;
 }
 
+/*
+ * [0] alloc_debug_processing
+ * [0] ___slab_alloc
+ * [0] __slab_alloc
+ * [0] kmem_cache_alloc
+ * [0] jbd2__journal_start
+ * [0] ext4_dirty_inode
+ * [0] __mark_inode_dirty
+ * [0] generic_update_time
+ * [0] file_update_time
+ * [0] ext4_page_mkwrite
+ * [0] do_page_mkwrite
+ * [0] do_wp_page
+ * [0] __handle_mm_fault
+ * [0] handle_mm_fault
+ * [0] __do_page_fault
+ * [0] page_fault
+ *
+ * called by:
+ *   - mm/slub.c|2749| <<___slab_alloc>> !alloc_debug_processing(s, page, freelist, addr))
+ */
 static noinline int alloc_debug_processing(struct kmem_cache *s,
 					struct page *page,
 					void *object, unsigned long addr)
@@ -1168,6 +1866,24 @@ static inline int free_consistency_checks(struct kmem_cache *s,
 }
 
 /* Supports checking bulk free of a constructed freelist */
+/*
+ * free_debug_processing+0x5b/0x2c3
+ * [0] __slab_free+0x1bd/0x2c0
+ * [0] kmem_cache_free+0x122/0x130
+ * [0] jbd2_journal_stop+0x1ba/0x3c0
+ * [0] __ext4_journal_stop+0x32/0x90
+ * [0] __mark_inode_dirty+0x15a/0x360
+ * [0] generic_update_time+0x80/0xd0
+ * [0] file_update_time+0x96/0xf0
+ * [0] ext4_page_mkwrite+0x52/0x460
+ * [0] do_page_mkwrite+0x2c/0x70
+ * [0] __handle_mm_fault+0x6d6/0xaf0
+ * [0] handle_mm_fault+0x7c/0x100
+ * [0] __do_page_fault+0x1fd/0x4b0
+ * [0] page_fault+0x45/0x50
+ *
+ * 会检查每一个free的object
+ */
 static noinline int free_debug_processing(
 	struct kmem_cache *s, struct page *page,
 	void *head, void *tail, int bulk_cnt,
@@ -1190,6 +1906,20 @@ static noinline int free_debug_processing(
 next_object:
 	cnt++;
 
+	/*
+	 * slub中使用SLAB_CONSISTENCY_CHECKS的例子 (当slub_debug设置了f的时候):
+	 *   - mm/slub.c|313| <<DEBUG_DEFAULT_FLAGS>> #define DEBUG_DEFAULT_FLAGS (SLAB_CONSISTENCY_CHECKS | SLAB_RED_ZONE | \
+	 *   - mm/slub.c|320| <<SLAB_NO_CMPXCHG>> #define SLAB_NO_CMPXCHG (SLAB_CONSISTENCY_CHECKS | SLAB_STORE_USER | \
+	 *   - mm/slub.c|1786| <<alloc_debug_processing>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+	 *   - mm/slub.c|1882| <<free_debug_processing>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+	 *   - mm/slub.c|1890| <<free_debug_processing>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+	 *   - mm/slub.c|1949| <<setup_slub_debug>> slub_debug |= SLAB_CONSISTENCY_CHECKS;
+	 *   - mm/slub.c|2381| <<__free_slab>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+	 *   - mm/slub.c|5988| <<sanity_checks_show>> return sprintf(buf, "%d\n", !!(s->flags & SLAB_CONSISTENCY_CHECKS));
+	 *   - mm/slub.c|5994| <<sanity_checks_store>> s->flags &= ~SLAB_CONSISTENCY_CHECKS;
+	 *   - mm/slub.c|5997| <<sanity_checks_store>> s->flags |= SLAB_CONSISTENCY_CHECKS;
+	 *   - mm/slub.c|6532| <<create_unique_id>> if (s->flags & SLAB_CONSISTENCY_CHECKS)
+	 */
 	if (s->flags & SLAB_CONSISTENCY_CHECKS) {
 		if (!free_consistency_checks(s, page, object, addr))
 			goto out;
@@ -1199,6 +1929,9 @@ static noinline int free_debug_processing(
 		set_track(s, object, TRACK_FREE, addr);
 	trace(s, page, object, 0);
 	/* Freepointer not overwritten by init_object(), SLAB_POISON moved it */
+	/*
+	 * 根据有没有SLAB_RED_ZONE和__OBJECT_POISON初始化embed的数据
+	 */
 	init_object(s, object, SLUB_RED_INACTIVE);
 
 	/* Reached end of constructed freelist yet? */
@@ -1236,6 +1969,7 @@ static int __init setup_slub_debug(char *str)
 		 */
 		goto check_slabs;
 
+	/* 这里会reset!!!!! */
 	slub_debug = 0;
 	if (*str == '-')
 		/*
@@ -1288,6 +2022,11 @@ static int __init setup_slub_debug(char *str)
 
 __setup("slub_debug", setup_slub_debug);
 
+/*
+ * called by:
+ *   - mm/slab_common.c|307| <<find_mergeable>> flags = kmem_cache_flags(size, flags, name, NULL);
+ *   - mm/slub.c|3769| <<kmem_cache_open>> s->flags = kmem_cache_flags(s->size, flags, s->name, s->ctor);
+ */
 unsigned long kmem_cache_flags(unsigned long object_size,
 	unsigned long flags, const char *name,
 	void (*ctor)(void *))
@@ -1295,6 +2034,12 @@ unsigned long kmem_cache_flags(unsigned long object_size,
 	/*
 	 * Enable debugging if selected on the kernel commandline.
 	 */
+	/*
+	 * 在以下使用slub_debug_slabs:
+	 *   - mm/slub.c|1675| <<setup_slub_debug>> slub_debug_slabs = str + 1;
+	 *   - mm/slub.c|1694| <<kmem_cache_flags>> if (slub_debug && (!slub_debug_slabs || (name &&
+	 *   - mm/slub.c|1695| <<kmem_cache_flags>> !strncmp(slub_debug_slabs, name, strlen(slub_debug_slabs)))))
+	 */
 	if (slub_debug && (!slub_debug_slabs || (name &&
 		!strncmp(slub_debug_slabs, name, strlen(slub_debug_slabs)))))
 		flags |= slub_debug;
@@ -1346,18 +2091,31 @@ static inline void dec_slabs_node(struct kmem_cache *s, int node,
  * Hooks for other subsystems that check memory allocations. In a typical
  * production configuration these hooks all should produce no code at all.
  */
+/*
+ * called by:
+ *   - mm/slub.c|4641| <<kmalloc_large_node>> kmalloc_large_node_hook(ptr, size, flags);
+ */
 static inline void kmalloc_large_node_hook(void *ptr, size_t size, gfp_t flags)
 {
 	kmemleak_alloc(ptr, size, 1, flags);
 	kasan_kmalloc_large(ptr, size, flags);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|3893| <<build_detached_freelist>> kfree_hook(object);
+ *   - mm/slub.c|4757| <<kfree>> kfree_hook(x);
+ */
 static inline void kfree_hook(const void *x)
 {
 	kmemleak_free(x);
 	kasan_kfree_large(x);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2118| <<slab_free_freelist_hook>> freeptr = slab_free_hook(s, object);
+ */
 static inline void *slab_free_hook(struct kmem_cache *s, void *x)
 {
 	void *freeptr;
@@ -1390,6 +2148,10 @@ static inline void *slab_free_hook(struct kmem_cache *s, void *x)
 	return freeptr;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|3802| <<slab_free>> slab_free_freelist_hook(s, head, tail);
+ */
 static inline void slab_free_freelist_hook(struct kmem_cache *s,
 					   void *head, void *tail)
 {
@@ -1412,9 +2174,24 @@ static inline void slab_free_freelist_hook(struct kmem_cache *s,
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1640| <<shuffle_freelist>> setup_object(s, page, cur);
+ *   - mm/slub.c|1646| <<shuffle_freelist>> setup_object(s, page, cur);
+ *   - mm/slub.c|1724| <<allocate_slab>> setup_object(s, page, p);
+ *
+ * 核心思想有三部分
+ * 1. 根据有没有SLAB_RED_ZONE和__OBJECT_POISON初始化embed的数据
+ * 2. 再就是init_tracking(s, object);
+ * 3. 最后是kasan的部分
+ */
 static void setup_object(struct kmem_cache *s, struct page *page,
 				void *object)
 {
+	/*
+	 * 根据有没有SLAB_RED_ZONE和__OBJECT_POISON初始化embed的数据
+	 * 再就是init_tracking(s, object);
+	 */
 	setup_object_debug(s, page, object);
 	kasan_init_slab_obj(s, object);
 	if (unlikely(s->ctor)) {
@@ -1427,6 +2204,13 @@ static void setup_object(struct kmem_cache *s, struct page *page,
 /*
  * Slab allocation and freeing
  */
+/*
+ * called by:
+ *   - mm/slub.c|2302| <<allocate_slab>> page = alloc_slab_page(s, alloc_gfp, node, oo);
+ *   - mm/slub.c|2310| <<allocate_slab>> page = alloc_slab_page(s, alloc_gfp, node, oo);
+ *
+ * 核心思想是根据参数的oo(struct kmem_cache_order_objects)计算order, 然后分配page
+ */
 static inline struct page *alloc_slab_page(struct kmem_cache *s,
 		gfp_t flags, int node, struct kmem_cache_order_objects oo)
 {
@@ -1448,6 +2232,13 @@ static inline struct page *alloc_slab_page(struct kmem_cache *s,
 
 #ifdef CONFIG_SLAB_FREELIST_RANDOM
 /* Pre-initialize the random sequence cache */
+/*
+ * called by:
+ *   - mm/slub.c|2197| <<init_freelist_randomization>> init_cache_random_seq(s);
+ *   - mm/slub.c|4484| <<kmem_cache_open>> if (init_cache_random_seq(s))
+ *
+ * ol不支持CONFIG_SLAB_FREELIST_RANDOM
+ */
 static int init_cache_random_seq(struct kmem_cache *s)
 {
 	int err;
@@ -1473,6 +2264,9 @@ static int init_cache_random_seq(struct kmem_cache *s)
 }
 
 /* Initialize each random sequence freelist per cache */
+/*
+ * ol不支持CONFIG_SLAB_FREELIST_RANDOM
+ */
 static void __init init_freelist_randomization(void)
 {
 	struct kmem_cache *s;
@@ -1486,6 +2280,9 @@ static void __init init_freelist_randomization(void)
 }
 
 /* Get the next entry on the pre-computed freelist randomized */
+/*
+ * ol不支持CONFIG_SLAB_FREELIST_RANDOM
+ */
 static void *next_freelist_entry(struct kmem_cache *s, struct page *page,
 				unsigned long *pos, void *start,
 				unsigned long page_limit,
@@ -1508,6 +2305,12 @@ static void *next_freelist_entry(struct kmem_cache *s, struct page *page,
 }
 
 /* Shuffle the single linked freelist based on a random pre-computed sequence */
+/*
+ * called by:
+ *   - mm/slub.c|1720| <<allocate_slab>> shuffle = shuffle_freelist(s, page);
+ *
+ * ol不支持CONFIG_SLAB_FREELIST_RANDOM
+ */
 static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
 {
 	void *start;
@@ -1542,17 +2345,26 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
 	return true;
 }
 #else
+/* ol不支持CONFIG_SLAB_FREELIST_RANDOM */
 static inline int init_cache_random_seq(struct kmem_cache *s)
 {
 	return 0;
 }
+/* ol不支持CONFIG_SLAB_FREELIST_RANDOM */
 static inline void init_freelist_randomization(void) { }
+/* ol不支持CONFIG_SLAB_FREELIST_RANDOM */
 static inline bool shuffle_freelist(struct kmem_cache *s, struct page *page)
 {
 	return false;
 }
 #endif /* CONFIG_SLAB_FREELIST_RANDOM */
 
+/*
+ * called by:
+ *   - mm/slub.c|1695| <<new_slab>> return allocate_slab(s,
+ *
+ * 核心思想就是分配一个或者一组struct page, 然后制作好page的layout
+ */
 static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 {
 	struct page *page;
@@ -1577,6 +2389,11 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	if ((alloc_gfp & __GFP_DIRECT_RECLAIM) && oo_order(oo) > oo_order(s->min))
 		alloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) & ~(__GFP_RECLAIM|__GFP_NOFAIL);
 
+	/*
+	 * 核心思想是根据参数的oo(struct kmem_cache_order_objects)计算order, 然后分配page
+	 *
+	 * oo来自上面的s->oo
+	 */
 	page = alloc_slab_page(s, alloc_gfp, node, oo);
 	if (unlikely(!page)) {
 		oo = s->min;
@@ -1585,6 +2402,9 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 		 * Allocation may have failed due to fragmentation.
 		 * Try a lower order alloc if possible
 		 */
+		/*
+		 * 核心思想是根据参数的oo(struct kmem_cache_order_objects)计算order, 然后分配page
+		 */
 		page = alloc_slab_page(s, alloc_gfp, node, oo);
 		if (unlikely(!page))
 			goto out;
@@ -1606,19 +2426,33 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	kasan_poison_slab(page);
 
+	/* ol不支持CONFIG_SLAB_FREELIST_RANDOM */
 	shuffle = shuffle_freelist(s, page);
 
 	if (!shuffle) {
 		for_each_object_idx(p, idx, s, start, page->objects) {
+			/*
+			 * 核心思想有三部分
+			 * 1. 根据有没有SLAB_RED_ZONE和__OBJECT_POISON初始化embed的数据
+			 * 2. 再就是init_tracking(s, object);
+			 * 3. 最后是kasan的部分
+			 */
 			setup_object(s, page, p);
+			/*
+			 * set_freepointer()核心思想是设置object + s->offset的值为fp
+			 */
 			if (likely(idx < page->objects))
 				set_freepointer(s, p, p + s->size);
 			else
 				set_freepointer(s, p, NULL);
 		}
+		/*
+		 * 如果redzone被使用了,object的地址要往前移动s->red_left_pad
+		 */
 		page->freelist = fixup_red_left(s, start);
 	}
 
+	/* struct page结构体中inuse代表已经使用的obj数量 */
 	page->inuse = page->objects;
 	page->frozen = 1;
 
@@ -1638,6 +2472,13 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	return page;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2478| <<new_slab_objects>> page = new_slab(s, flags, node);
+ *   - mm/slub.c|3469| <<early_kmem_cache_node_alloc>> page = new_slab(kmem_cache_node, GFP_NOWAIT, node);
+ *
+ * 核心思想就是分配一个或者一组struct page, 然后制作好page的layout
+ */
 static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
 {
 	if (unlikely(flags & GFP_SLAB_BUG_MASK)) {
@@ -1648,15 +2489,37 @@ static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
 		dump_stack();
 	}
 
+	/* 核心思想就是分配一个或者一组struct page, 然后制作好page的layout */
 	return allocate_slab(s,
 		flags & (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK), node);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2507| <<rcu_free_slab>> __free_slab(page->slab_cache, page);
+ *   - mm/slub.c|2527| <<free_slab>> __free_slab(s, page);
+ *
+ * 核心思想是释放参数的page
+ */
 static void __free_slab(struct kmem_cache *s, struct page *page)
 {
 	int order = compound_order(page);
 	int pages = 1 << order;
 
+	/*
+	 * slub中使用SLAB_CONSISTENCY_CHECKS的例子 (当slub_debug设置了f的时候):
+	 *   - mm/slub.c|313| <<DEBUG_DEFAULT_FLAGS>> #define DEBUG_DEFAULT_FLAGS (SLAB_CONSISTENCY_CHECKS | SLAB_RED_ZONE | \
+	 *   - mm/slub.c|320| <<SLAB_NO_CMPXCHG>> #define SLAB_NO_CMPXCHG (SLAB_CONSISTENCY_CHECKS | SLAB_STORE_USER | \
+	 *   - mm/slub.c|1786| <<alloc_debug_processing>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+	 *   - mm/slub.c|1882| <<free_debug_processing>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+	 *   - mm/slub.c|1890| <<free_debug_processing>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+	 *   - mm/slub.c|1949| <<setup_slub_debug>> slub_debug |= SLAB_CONSISTENCY_CHECKS;
+	 *   - mm/slub.c|2381| <<__free_slab>> if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+	 *   - mm/slub.c|5988| <<sanity_checks_show>> return sprintf(buf, "%d\n", !!(s->flags & SLAB_CONSISTENCY_CHECKS));
+	 *   - mm/slub.c|5994| <<sanity_checks_store>> s->flags &= ~SLAB_CONSISTENCY_CHECKS;
+	 *   - mm/slub.c|5997| <<sanity_checks_store>> s->flags |= SLAB_CONSISTENCY_CHECKS;
+	 *   - mm/slub.c|6532| <<create_unique_id>> if (s->flags & SLAB_CONSISTENCY_CHECKS)
+	 */
 	if (s->flags & SLAB_CONSISTENCY_CHECKS) {
 		void *p;
 
@@ -1681,9 +2544,19 @@ static void __free_slab(struct kmem_cache *s, struct page *page)
 	__free_pages(page, order);
 }
 
+/*
+ * 在以下使用need_reserve_slab_rcu:
+ *   - mm/slub.c|2502| <<rcu_free_slab>> if (need_reserve_slab_rcu)
+ *   - mm/slub.c|2519| <<free_slab>> if (need_reserve_slab_rcu) {
+ *   - mm/slub.c|4533| <<kmem_cache_open>> if (need_reserve_slab_rcu && (s->flags & SLAB_TYPESAFE_BY_RCU))
+ */
 #define need_reserve_slab_rcu						\
 	(sizeof(((struct page *)NULL)->lru) < sizeof(struct rcu_head))
 
+/*
+ * 在以下使用rcu_free_slab:
+ *   - mm/slub.c|2525| <<free_slab>> call_rcu(head, rcu_free_slab);
+ */
 static void rcu_free_slab(struct rcu_head *h)
 {
 	struct page *page;
@@ -1696,6 +2569,12 @@ static void rcu_free_slab(struct rcu_head *h)
 	__free_slab(page->slab_cache, page);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2533| <<discard_slab>> free_slab(s, page);
+ *
+ * 核心思想是释放参数的page
+ */
 static void free_slab(struct kmem_cache *s, struct page *page)
 {
 	if (unlikely(s->flags & SLAB_TYPESAFE_BY_RCU)) {
@@ -1716,36 +2595,105 @@ static void free_slab(struct kmem_cache *s, struct page *page)
 		__free_slab(s, page);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2976| <<deactivate_slab>> discard_slab(s, page);
+ *   - mm/slub.c|3046| <<unfreeze_partials>> discard_slab(s, page);
+ *   - mm/slub.c|3791| <<__slab_free>> discard_slab(s, page);
+ *   - mm/slub.c|4637| <<free_partial>> discard_slab(s, page);
+ *   - mm/slub.c|4913| <<__kmem_cache_shrink>> discard_slab(s, page);
+ *
+ * 核心思想是释放参数的page
+ * 还要把page->objects从node的cache减去
+ */
 static void discard_slab(struct kmem_cache *s, struct page *page)
 {
 	dec_slabs_node(s, page_to_nid(page), page->objects);
+	/*
+	 * 核心思想是释放参数的page
+	 */
 	free_slab(s, page);
 }
 
 /*
  * Management of partially allocated slabs.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2559| <<add_partial>> __add_partial(n, page, tail);
+ *   - mm/slub.c|4302| <<early_kmem_cache_node_alloc>> __add_partial(n, page, DEACTIVATE_TO_HEAD);
+ *
+ * 根据参数是否是DEACTIVATE_TO_TAIL决定是否把page->lru加入到n->partial的head还是tail,
+ * 增加n->nr_partial++
+ */
 static inline void
 __add_partial(struct kmem_cache_node *n, struct page *page, int tail)
 {
+	/*
+	 * 增加nr_partial的地方:
+	 *   - mm/slub.c|2588| <<__add_partial>> n->nr_partial++;
+	 * 减少nr_partial的地方:
+	 *   - mm/slub.c|2613| <<remove_partial>> n->nr_partial--;
+	 *   - mm/slub.c|4943| <<__kmem_cache_shrink>> n->nr_partial--;
+	 * 其他使用nr_partial的地方:
+	 *   - mm/slub.c|2687| <<get_partial_node>> if (!n || !n->nr_partial)
+	 *   - mm/slub.c|2764| <<get_any_partial>> n->nr_partial > s->min_partial) {
+	 *   - mm/slub.c|2961| <<deactivate_slab>> if (!new.inuse && n->nr_partial >= s->min_partial)
+	 *   - mm/slub.c|3075| <<unfreeze_partials>> if (unlikely(!new.inuse && n->nr_partial >= s->min_partial)) {
+	 *   - mm/slub.c|3807| <<__slab_free>> if (unlikely(!new.inuse && n->nr_partial >= s->min_partial))
+	 *   - mm/slub.c|4273| <<init_kmem_cache_node>> n->nr_partial = 0;
+	 *   - mm/slub.c|4698| <<__kmem_cache_shutdown>> if (n->nr_partial || slabs_node(s, node))
+	 *   - mm/slub.c|5413| <<validate_slab_node>> if (count != n->nr_partial)
+	 *   - mm/slub.c|5415| <<validate_slab_node>> s->name, count, n->nr_partial);
+	 *   - mm/slub.c|5857| <<show_slab_objects>> x = n->nr_partial;
+	 */
 	n->nr_partial++;
+	/*
+	 * struct kmem_cache_node:
+	 *   unsigned long nr_partial;
+	 *   struct list_head partial;
+	 */
 	if (tail == DEACTIVATE_TO_TAIL)
 		list_add_tail(&page->lru, &n->partial);
 	else
 		list_add(&page->lru, &n->partial);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2164| <<deactivate_slab>> add_partial(n, page, tail);
+ *   - mm/slub.c|2244| <<unfreeze_partials>> add_partial(n, page, DEACTIVATE_TO_TAIL);
+ *   - mm/slub.c|2972| <<__slab_free>> add_partial(n, page, DEACTIVATE_TO_TAIL);
+ *
+ * 根据参数是否是DEACTIVATE_TO_TAIL决定是否把page->lru加入到n->partial的head还是tail,
+ * 增加n->nr_partial++
+ */
 static inline void add_partial(struct kmem_cache_node *n,
 				struct page *page, int tail)
 {
 	lockdep_assert_held(&n->list_lock);
+	/*
+	 * 根据参数是否是DEACTIVATE_TO_TAIL决定是否把page->lru加入到n->partial的head还是tail,
+	 * 增加n->nr_partial++
+	 */
 	__add_partial(n, page, tail);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2657| <<acquire_slab>> remove_partial(n, page);
+ *   - mm/slub.c|2991| <<deactivate_slab>> remove_partial(n, page);
+ *   - mm/slub.c|3828| <<__slab_free>> remove_partial(n, page);
+ *   - mm/slub.c|4673| <<free_partial>> remove_partial(n, page);
+ *
+ * 把page->lru从n->partial移除
+ * 减少n->nr_partial--
+ */
 static inline void remove_partial(struct kmem_cache_node *n,
 					struct page *page)
 {
 	lockdep_assert_held(&n->list_lock);
+	/* 把page->lru从n->partial删除 */
 	list_del(&page->lru);
 	n->nr_partial--;
 }
@@ -1756,6 +2704,15 @@ static inline void remove_partial(struct kmem_cache_node *n,
  *
  * Returns a list of objects or NULL if it fails.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2697| <<get_partial_node>> t = acquire_slab(s, n, page, object == NULL, &objects);
+ *
+ * Remove slab from the partial list, freeze it and
+ * return the pointer to the freelist.
+ * 最后会把page->lru从n->partial移除
+ * 减少n->nr_partial--
+ */
 static inline void *acquire_slab(struct kmem_cache *s,
 		struct kmem_cache_node *n, struct page *page,
 		int mode, int *objects)
@@ -1771,6 +2728,10 @@ static inline void *acquire_slab(struct kmem_cache *s,
 	 * The old freelist is the list of objects for the
 	 * per cpu allocation list.
 	 */
+	/*
+	 * struct page:
+	 *   void *freelist;
+	 */
 	freelist = page->freelist;
 	counters = page->counters;
 	new.counters = counters;
@@ -1785,12 +2746,21 @@ static inline void *acquire_slab(struct kmem_cache *s,
 	VM_BUG_ON(new.frozen);
 	new.frozen = 1;
 
+	/*
+	 * 我们期待__cmpxchg_double_slab()返回true, 不希望false
+	 * 核心思想是判断page->freelist和page->counters是否和old的相等
+	 * 如果相等,则吧page->freelist和page->counters都更新成新的
+	 */
 	if (!__cmpxchg_double_slab(s, page,
 			freelist, counters,
 			new.freelist, new.counters,
 			"acquire_slab"))
 		return NULL;
 
+	/*
+	 * 把page->lru从n->partial移除
+	 * 减少n->nr_partial--
+	 */
 	remove_partial(n, page);
 	WARN_ON(!freelist);
 	return freelist;
@@ -1802,6 +2772,13 @@ static inline bool pfmemalloc_match(struct page *page, gfp_t gfpflags);
 /*
  * Try to allocate a partial slab from a specific node.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2051| <<get_any_partial>> object = get_partial_node(s, n, c, flags);
+ *   - mm/slub.c|2083| <<get_partial>> object = get_partial_node(s, get_node(s, searchnode), c, flags);
+ *
+ * Try to allocate a partial slab from a specific node.
+ */
 static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
 				struct kmem_cache_cpu *c, gfp_t flags)
 {
@@ -1816,6 +2793,13 @@ static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
 	 * partial slab and there is none available then get_partials()
 	 * will return NULL.
 	 */
+	/*
+	 * 增加nr_partial的地方:
+	 *   - mm/slub.c|2588| <<__add_partial>> n->nr_partial++;
+	 * 减少nr_partial的地方:
+	 *   - mm/slub.c|2613| <<remove_partial>> n->nr_partial--;
+	 *   - mm/slub.c|4943| <<__kmem_cache_shrink>> n->nr_partial--;
+	 */
 	if (!n || !n->nr_partial)
 		return NULL;
 
@@ -1826,6 +2810,12 @@ static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
 		if (!pfmemalloc_match(page, flags))
 			continue;
 
+		/*
+		 * Remove slab from the partial list, freeze it and
+		 * return the pointer to the freelist.
+		 * 最后会把page->lru从n->partial移除
+		 * 减少n->nr_partial--
+		 */
 		t = acquire_slab(s, n, page, object == NULL, &objects);
 		if (!t)
 			break;
@@ -1839,6 +2829,12 @@ static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
 			put_cpu_partial(s, page, 0);
 			stat(s, CPU_PARTIAL_NODE);
 		}
+		/*
+		 * 使用了CONFIG_SLUB_CPU_PARTIAL的情况下, 当kmem_cache->flags设置了debug的任何flag的时候返回false
+		 * 没有CONFIG_SLUB_CPU_PARTIAL直接返回false
+		 * ol支持CONFIG_SLUB_CPU_PARTIAL
+		 * 说明用debug的时候不支持kmem_cache_has_cpu_partial
+		 */
 		if (!kmem_cache_has_cpu_partial(s)
 			|| available > slub_cpu_partial(s) / 2)
 			break;
@@ -1851,6 +2847,12 @@ static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
 /*
  * Get a page from somewhere. Search in increasing NUMA distances.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2918| <<get_partial>> return get_any_partial(s, flags, c);
+ *
+ * Get a page from somewhere. Search in increasing NUMA distances.
+ */
 static void *get_any_partial(struct kmem_cache *s, gfp_t flags,
 		struct kmem_cache_cpu *c)
 {
@@ -1915,6 +2917,10 @@ static void *get_any_partial(struct kmem_cache *s, gfp_t flags,
 /*
  * Get a partial page, lock it and return it.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2594| <<new_slab_objects>> freelist = get_partial(s, flags, node, c);
+ */
 static void *get_partial(struct kmem_cache *s, gfp_t flags, int node,
 		struct kmem_cache_cpu *c)
 {
@@ -1926,10 +2932,16 @@ static void *get_partial(struct kmem_cache *s, gfp_t flags, int node,
 	else if (!node_present_pages(node))
 		searchnode = node_to_mem_node(node);
 
+	/*
+	 * Try to allocate a partial slab from a specific node.
+	 */
 	object = get_partial_node(s, get_node(s, searchnode), c, flags);
 	if (object || node != NUMA_NO_NODE)
 		return object;
 
+	/*
+	 * Get a page from somewhere. Search in increasing NUMA distances.
+	 */
 	return get_any_partial(s, flags, c);
 }
 
@@ -1992,6 +3004,12 @@ static inline void note_cmpxchg_failure(const char *n,
 	stat(s, CMPXCHG_DOUBLE_CPU_FAIL);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4693| <<alloc_kmem_cache_cpus>> init_kmem_cache_cpus(s);
+ *
+ * 初始化kmem_cache->cpu_slab每一个cpu的tid
+ */
 static void init_kmem_cache_cpus(struct kmem_cache *s)
 {
 	int cpu;
@@ -2003,18 +3021,44 @@ static void init_kmem_cache_cpus(struct kmem_cache *s)
 /*
  * Remove the cpu slab
  */
+/*
+ * called by:
+ *   - mm/slub.c|2359| <<flush_slab>> deactivate_slab(s, c->page, c->freelist, c);
+ *   - mm/slub.c|2617| <<___slab_alloc>> deactivate_slab(s, page, c->freelist, c);
+ *   - mm/slub.c|2628| <<___slab_alloc>> deactivate_slab(s, page, c->freelist, c);
+ *   - mm/slub.c|2684| <<___slab_alloc>> deactivate_slab(s, page, get_freepointer(s, freelist), c);
+ *
+ * Remove the cpu slab
+ * 该函数会将CPU缓存对象的freelist中的对象,还给slab对象(page)
+ * 然后把这个page放入partial或者根据情况释放
+ * 关键最后会进行以下:
+ *   c->page = NULL;
+ *   c->freelist = NULL;
+ *
+ * 假设从___slab_alloc()第一次进入这里, page->freelist=NULL, c->page=page,
+ * freelist是第一个(要分配出去的)object
+ * 第3个参数是get_freepointer(s, freelist), 也就是分配出去的下一个
+ */
 static void deactivate_slab(struct kmem_cache *s, struct page *page,
 				void *freelist, struct kmem_cache_cpu *c)
 {
 	enum slab_modes { M_NONE, M_PARTIAL, M_FULL, M_FREE };
+	/* 返回kmem_cache->node[page_to_nid(page)] */
 	struct kmem_cache_node *n = get_node(s, page_to_nid(page));
 	int lock = 0;
 	enum slab_modes l = M_NONE, m = M_NONE;
 	void *nextfree;
+	/*
+	 * DEACTIVATE_TO_HEAD: Cpu slab was moved to the head of partials
+	 * DEACTIVATE_TO_TAIL: Cpu slab was moved to the tail of partials
+	 */
 	int tail = DEACTIVATE_TO_HEAD;
 	struct page new;
 	struct page old;
 
+	/*
+	 * 在slub_debug第一次分配的时候, page->freelist是NULL
+	 */
 	if (page->freelist) {
 		stat(s, DEACTIVATE_REMOTE_FREES);
 		tail = DEACTIVATE_TO_TAIL;
@@ -2028,18 +3072,32 @@ static void deactivate_slab(struct kmem_cache *s, struct page *page,
 	 * There is no need to take the list->lock because the page
 	 * is still frozen.
 	 */
+	/*
+	 * get_freepointer():
+	 * 没有CONFIG_SLAB_FREELIST_HARDENED(uek4不支持)就返回object + s->offset
+	 */
 	while (freelist && (nextfree = get_freepointer(s, freelist))) {
 		void *prior;
 		unsigned long counters;
 
+		/*
+		 * 这个大while循环就是把freelist的entry都归还到page->freelist
+		 */
+
 		do {
 			prior = page->freelist;
 			counters = page->counters;
+			/* 核心思想是设置object + s->offset的值为fp */
 			set_freepointer(s, freelist, prior);
 			new.counters = counters;
 			new.inuse--;
 			VM_BUG_ON(!new.frozen);
 
+			/*
+			 * 我们期待__cmpxchg_double_slab()返回true, 不希望false
+			 * 核心思想是判断page->freelist和page->counters是否和old的相等
+			 * 如果相等,则吧page->freelist和page->counters都更新成新的
+			 */
 		} while (!__cmpxchg_double_slab(s, page,
 			prior, counters,
 			freelist, new.counters,
@@ -2107,28 +3165,45 @@ static void deactivate_slab(struct kmem_cache *s, struct page *page,
 
 	if (l != m) {
 
+		/*
+		 * 把page->lru从n->partial移除
+		 * 减少n->nr_partial--
+		 */
 		if (l == M_PARTIAL)
 
 			remove_partial(n, page);
 
+		/*
+		 *  如果s->flags设置了SLAB_STORE_USER, 则把page->lru从kmem_cache_node->full删除
+		 */
 		else if (l == M_FULL)
 
 			remove_full(s, n, page);
 
 		if (m == M_PARTIAL) {
 
+			/*
+			 * 根据参数是否是DEACTIVATE_TO_TAIL决定是否把page->lru加入到n->partial的head还是tail,
+			 * 增加n->nr_partial++
+			 */
 			add_partial(n, page, tail);
 			stat(s, tail);
 
 		} else if (m == M_FULL) {
 
 			stat(s, DEACTIVATE_FULL);
+			/* 如果s->flags设置了SLAB_STORE_USER, 则把page->lru加入到kmem_cache_node->full */
 			add_full(s, n, page);
 
 		}
 	}
 
 	l = m;
+	/*
+	 * 我们期待__cmpxchg_double_slab()返回true, 不希望false
+	 * 核心思想是判断page->freelist和page->counters是否和old的相等
+	 * 如果相等,则吧page->freelist和page->counters都更新成新的
+	 */
 	if (!__cmpxchg_double_slab(s, page,
 				old.freelist, old.counters,
 				new.freelist, new.counters,
@@ -2140,6 +3215,10 @@ static void deactivate_slab(struct kmem_cache *s, struct page *page,
 
 	if (m == M_FREE) {
 		stat(s, DEACTIVATE_EMPTY);
+		/*
+		 * 核心思想是释放参数的page
+		 * 还要把page->objects从node的cache减去
+		 */
 		discard_slab(s, page);
 		stat(s, FREE_SLAB);
 	}
@@ -2155,6 +3234,17 @@ static void deactivate_slab(struct kmem_cache *s, struct page *page,
  * for the cpu using c (or some other guarantee must be there
  * to guarantee no concurrent accesses).
  */
+/*
+ * called by:
+ *   - mm/slub.c|3217| <<put_cpu_partial>> unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+ *   - mm/slub.c|3239| <<put_cpu_partial>> unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+ *   - mm/slub.c|3267| <<__flush_cpu_slab>> unfreeze_partials(s, c);
+ *
+ * Unfreeze all the cpu partial slabs.
+ * 把kmem_cache_cpu->partial的每个page要么释放(比如!page.inuse的情况)
+ * 要么根据参数是否是DEACTIVATE_TO_TAIL决定是否把page->lru加入到n->partial的head还是tail,
+ * 增加n->nr_partial++
+ */
 static void unfreeze_partials(struct kmem_cache *s,
 		struct kmem_cache_cpu *c)
 {
@@ -2188,15 +3278,27 @@ static void unfreeze_partials(struct kmem_cache *s,
 
 			new.frozen = 0;
 
+			/*
+			 * 我们期待__cmpxchg_double_slab()返回true, 不希望false
+			 * 核心思想是判断page->freelist和page->counters是否和old的相等
+			 * 如果相等,则吧page->freelist和page->counters都更新成新的
+			 */
 		} while (!__cmpxchg_double_slab(s, page,
 				old.freelist, old.counters,
 				new.freelist, new.counters,
 				"unfreezing slab"));
 
+		/*
+		 * page->inuse: struct page结构体中inuse代表已经使用的obj数量
+		 */
 		if (unlikely(!new.inuse && n->nr_partial >= s->min_partial)) {
 			page->next = discard_page;
 			discard_page = page;
 		} else {
+			/*
+			 * 根据参数是否是DEACTIVATE_TO_TAIL决定是否把page->lru加入到n->partial的head还是tail,
+			 * 增加n->nr_partial++
+			 */
 			add_partial(n, page, DEACTIVATE_TO_TAIL);
 			stat(s, FREE_ADD_PARTIAL);
 		}
@@ -2210,6 +3312,10 @@ static void unfreeze_partials(struct kmem_cache *s,
 		discard_page = discard_page->next;
 
 		stat(s, DEACTIVATE_EMPTY);
+		/*
+		 * 核心思想是释放参数的page
+		 * 还要把page->objects从node的cache减去
+		 */
 		discard_slab(s, page);
 		stat(s, FREE_SLAB);
 	}
@@ -2225,6 +3331,16 @@ static void unfreeze_partials(struct kmem_cache *s,
  * If we did not find a slot then simply move all the partials to the
  * per node partial list.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2765| <<get_partial_node>> put_cpu_partial(s, page, 0);
+ *   - mm/slub.c|3883| <<__slab_free>> put_cpu_partial(s, page, 1);
+ *
+ * Put a page that was just frozen (in __slab_free) into a partial page
+ * slot if available.
+ * 把一个page加入到cpu cache的partial中
+ * 这个page有可能是在__slab_free()中frozen的
+ */
 static void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)
 {
 #ifdef CONFIG_SLUB_CPU_PARTIAL
@@ -2241,6 +3357,9 @@ static void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)
 		if (oldpage) {
 			pobjects = oldpage->pobjects;
 			pages = oldpage->pages;
+			/*
+			 * Number of per cpu partial objects to keep around
+			 */
 			if (drain && pobjects > s->cpu_partial) {
 				unsigned long flags;
 				/*
@@ -2248,6 +3367,12 @@ static void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)
 				 * set to the per node partial list.
 				 */
 				local_irq_save(flags);
+				/*
+				 * Unfreeze all the cpu partial slabs.
+				 * 把kmem_cache_cpu->partial的每个page要么释放(比如!page.inuse的情况)
+				 * 要么根据参数是否是DEACTIVATE_TO_TAIL决定是否把page->lru加入到n->partial的head还是tail,
+				 * 增加n->nr_partial++
+				 */
 				unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
 				local_irq_restore(flags);
 				oldpage = NULL;
@@ -2270,6 +3395,12 @@ static void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)
 		unsigned long flags;
 
 		local_irq_save(flags);
+		/*
+		 * Unfreeze all the cpu partial slabs.
+		 * 把kmem_cache_cpu->partial的每个page要么释放(比如!page.inuse的情况)
+		 * 要么根据参数是否是DEACTIVATE_TO_TAIL决定是否把page->lru加入到n->partial的head还是tail,
+		 * 增加n->nr_partial++
+		 */
 		unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
 		local_irq_restore(flags);
 	}
@@ -2419,6 +3550,16 @@ slab_out_of_memory(struct kmem_cache *s, gfp_t gfpflags, int nid)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2676| <<___slab_alloc>> freelist = new_slab_objects(s, gfpflags, node, &c);
+ *
+ * 核心思想就是分配一个或者一组struct page, 然后制作好page的layout
+ * 如果分配成功, 返回page->freelist,
+ * 设置page->freelist = NULL
+ * c->page = page
+ * !!! 然而, get_partial()可以返回的时候直接从get_partial()返回就好了!!!
+ */
 static inline void *new_slab_objects(struct kmem_cache *s, gfp_t flags,
 			int node, struct kmem_cache_cpu **pc)
 {
@@ -2431,6 +3572,9 @@ static inline void *new_slab_objects(struct kmem_cache *s, gfp_t flags,
 	if (freelist)
 		return freelist;
 
+	/*
+	 * 核心思想就是分配一个或者一组struct page, 然后制作好page的layout
+	 */
 	page = new_slab(s, flags, node);
 	if (page) {
 		c = raw_cpu_ptr(s->cpu_slab);
@@ -2487,6 +3631,11 @@ static inline void *get_freelist(struct kmem_cache *s, struct page *page)
 		new.inuse = page->objects;
 		new.frozen = freelist != NULL;
 
+		/*
+		 * 我们期待__cmpxchg_double_slab()返回true, 不希望false
+		 * 核心思想是判断page->freelist和page->counters是否和old的相等
+		 * 如果相等,则吧page->freelist和page->counters都更新成新的
+		 */
 	} while (!__cmpxchg_double_slab(s, page,
 		freelist, counters,
 		NULL, new.counters,
@@ -2514,12 +3663,28 @@ static inline void *get_freelist(struct kmem_cache *s, struct page *page)
  * Version of __slab_alloc to use when we know that interrupts are
  * already disabled (which is the case for bulk allocation).
  */
+/*
+ * called by:
+ *   - mm/slub.c|2653| <<__slab_alloc>> p = ___slab_alloc(s, gfpflags, node, addr, c);
+ *   - mm/slub.c|3163| <<kmem_cache_alloc_bulk>> p[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,
+ */
 static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 			  unsigned long addr, struct kmem_cache_cpu *c)
 {
 	void *freelist;
 	struct page *page;
 
+	/*
+	 * deactivate_slab():
+	 * Remove the cpu slab
+	 * 该函数会将CPU缓存对象的freelist中的对象,还给slab对象(page)
+	 * 然后把这个page放入partial或者根据情况释放
+	 * 关键最后会进行以下:
+	 *   c->page = NULL;
+	 *   c->freelist = NULL;
+	 *
+	 * slub_debug=U的时候c->page总是NULL
+	 */
 	page = c->page;
 	if (!page)
 		goto new_slab;
@@ -2533,6 +3698,14 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 
 		if (unlikely(!node_match(page, searchnode))) {
 			stat(s, ALLOC_NODE_MISMATCH);
+			/*
+			 * Remove the cpu slab
+			 * 该函数会将CPU缓存对象的freelist中的对象,还给slab对象(page)
+			 * 然后把这个page放入partial或者根据情况释放
+			 * 关键最后会进行以下:
+			 *   c->page = NULL;
+			 *   c->freelist = NULL;
+			 */
 			deactivate_slab(s, page, c->freelist, c);
 			goto new_slab;
 		}
@@ -2544,6 +3717,14 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 	 * information when the page leaves the per-cpu allocator
 	 */
 	if (unlikely(!pfmemalloc_match(page, gfpflags))) {
+		/*
+		 * Remove the cpu slab
+		 * 该函数会将CPU缓存对象的freelist中的对象,还给slab对象(page)
+		 * 然后把这个page放入partial或者根据情况释放
+		 * 关键最后会进行以下:
+		 *   c->page = NULL;
+		 *   c->freelist = NULL;
+		 */
 		deactivate_slab(s, page, c->freelist, c);
 		goto new_slab;
 	}
@@ -2576,6 +3757,11 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 
 new_slab:
 
+	/* 如果c->partial存在 */
+	/*
+	 * 只设置slub_debug=U就不会执行下面的了
+	 * 看来只要激活了slub_debug就不会走percpu cache了
+	 */
 	if (slub_percpu_partial(c)) {
 		page = c->page = slub_percpu_partial(c);
 		slub_set_percpu_partial(c, page);
@@ -2583,6 +3769,15 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 		goto redo;
 	}
 
+	/*
+	 * freelist声明的时候是void * *
+	 *
+	 * 核心思想就是分配一个或者一组struct page, 然后制作好page的layout
+	 * 如果分配成功, 返回page->freelist,
+	 * 设置page->freelist = NULL
+	 * c->page = page
+	 * !!! 然而, get_partial()可以返回的时候直接从get_partial()返回就好了!!!
+	 */
 	freelist = new_slab_objects(s, gfpflags, node, &c);
 
 	if (unlikely(!freelist)) {
@@ -2591,6 +3786,9 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 	}
 
 	page = c->page;
+	/*
+	 * kmem_cache_debug(): 当kmem_cache->flags设置了debug的任何flag的时候返回 true
+	 */
 	if (likely(!kmem_cache_debug(s) && pfmemalloc_match(page, gfpflags)))
 		goto load_freelist;
 
@@ -2599,6 +3797,20 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 			!alloc_debug_processing(s, page, freelist, addr))
 		goto new_slab;	/* Slab failed checks. Next slab needed */
 
+	/*
+	 * 下面是slub_debug的分配路径的核心!!!!!!
+	 *
+	 * 假设从___slab_alloc()第一次进入这里, page->freelist=NULL, c->page=page,
+	 * freelist是第一个(要分配出去的)object
+	 * 第3个参数是get_freepointer(s, freelist), 也就是分配出去的下一个
+	 *
+	 * Remove the cpu slab
+	 * 该函数会将CPU缓存对象的freelist中的对象,还给slab对象(page)
+	 * 然后把这个page放入partial或者根据情况释放
+	 * 关键最后会进行以下:
+	 *   c->page = NULL;
+	 *   c->freelist = NULL;
+	 */
 	deactivate_slab(s, page, get_freepointer(s, freelist), c);
 	return freelist;
 }
@@ -2607,6 +3819,10 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
  * Another one that disabled interrupt and compensates for possible
  * cpu changes by refetching the per cpu area pointer.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2716| <<slab_alloc_node>> object = __slab_alloc(s, gfpflags, node, addr, c);
+ */
 static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 			  unsigned long addr, struct kmem_cache_cpu *c)
 {
@@ -2683,12 +3899,31 @@ static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 	 * linked list in between.
 	 */
 
+	/*
+	 * 下面的代码尝试在c->freelist分配一个
+	 * 如果c->freelist已经空了,就要用__slab_alloc()的slow path
+	 *
+	 * c->freelist是NULL说明分配已经到最后了
+	 */
+
+	/*
+	 * deactivate_slab():
+	 * Remove the cpu slab
+	 * 该函数会将CPU缓存对象的freelist中的对象,还给slab对象(page)
+	 * 然后把这个page放入partial或者根据情况释放
+	 * 关键最后会进行以下:
+	 *   c->page = NULL;
+	 *   c->freelist = NULL;
+	 */
 	object = c->freelist;
 	page = c->page;
 	if (unlikely(!object || !node_match(page, node))) {
 		object = __slab_alloc(s, gfpflags, node, addr, c);
 		stat(s, ALLOC_SLOWPATH);
 	} else {
+		/*
+		 * freelist_dereference(s, object + s->offset);
+		 */
 		void *next_object = get_freepointer_safe(s, object);
 
 		/*
@@ -2705,6 +3940,11 @@ static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 		 * against code executing on this cpu *not* from access by
 		 * other cpus.
 		 */
+		/*
+		 * 这里用atomic的形式:
+		 * 1. 把s->cpu_slab->freelist从object(c->freelist)换成next_object=get_freepointer_safe(s, object)
+		 * 2. 把s->cpu_slab->tid换成next_tid(tid)
+		 */
 		if (unlikely(!this_cpu_cmpxchg_double(
 				s->cpu_slab->freelist, s->cpu_slab->tid,
 				object, tid,
@@ -2725,6 +3965,13 @@ static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 	return object;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2736| <<kmem_cache_alloc>> void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+ *   - mm/slub.c|2748| <<kmem_cache_alloc_trace>> void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+ *   - mm/slub.c|3756| <<__kmalloc>> ret = slab_alloc(s, flags, _RET_IP_);
+ *   - mm/slub.c|4288| <<__kmalloc_track_caller>> ret = slab_alloc(s, gfpflags, caller);
+ */
 static __always_inline void *slab_alloc(struct kmem_cache *s,
 		gfp_t gfpflags, unsigned long addr)
 {
@@ -2743,6 +3990,11 @@ void *kmem_cache_alloc(struct kmem_cache *s, gfp_t gfpflags)
 EXPORT_SYMBOL(kmem_cache_alloc);
 
 #ifdef CONFIG_TRACING
+/*
+ * called by:
+ *   - include/linux/slab.h|404| <<kmem_cache_alloc_node_trace>> return kmem_cache_alloc_trace(s, gfpflags, size);
+ *   - include/linux/slab.h|514| <<kmalloc>> return kmem_cache_alloc_trace(kmalloc_caches[index],
+ */
 void *kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)
 {
 	void *ret = slab_alloc(s, gfpflags, _RET_IP_);
@@ -2754,6 +4006,28 @@ EXPORT_SYMBOL(kmem_cache_alloc_trace);
 #endif
 
 #ifdef CONFIG_NUMA
+/*
+ * called by:
+ *   - arch/powerpc/platforms/pseries/dtl.c|205| <<dtl_enable>> buf = kmem_cache_alloc_node(dtl_cache, GFP_KERNEL, cpu_to_node(dtl->cpu));
+ *   - arch/sparc/mm/tsb.c|424| <<tsb_grow>> new_tsb = kmem_cache_alloc_node(tsb_caches[new_cache_index],
+ *   - block/bfq-iosched.c|3956| <<bfq_get_queue>> bfqq = kmem_cache_alloc_node(bfq_pool,
+ *   - block/blk-core.c|706| <<alloc_request_simple>> return kmem_cache_alloc_node(request_cachep, gfp_mask, q->node);
+ *   - block/blk-core.c|833| <<blk_alloc_queue_node>> q = kmem_cache_alloc_node(blk_requestq_cachep,
+ *   - block/blk-ioc.c|271| <<create_task_io_context>> ioc = kmem_cache_alloc_node(iocontext_cachep, gfp_flags | __GFP_ZERO,
+ *   - block/blk-ioc.c|396| <<ioc_create_icq>> icq = kmem_cache_alloc_node(et->icq_cache, gfp_mask | __GFP_ZERO,
+ *   - block/cfq-iosched.c|3835| <<cfq_get_queue>> cfqq = kmem_cache_alloc_node(cfq_pool,
+ *   - drivers/scsi/scsi_lib.c|65| <<scsi_alloc_sense_buffer>> return kmem_cache_alloc_node(scsi_select_sense_cache(unchecked_isa_dma),
+ *   - include/linux/slab.h|423| <<kmem_cache_alloc_node_trace>> void *ret = kmem_cache_alloc_node(s, gfpflags, node);
+ *   - include/trace/events/kmem.h|109| <<__field>> DEFINE_EVENT(kmem_alloc_node, kmem_cache_alloc_node,
+ *   - kernel/fork.c|156| <<alloc_task_struct_node>> return kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node);
+ *   - kernel/fork.c|274| <<alloc_thread_stack_node>> return kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);
+ *   - kernel/workqueue.c|3559| <<alloc_unbound_pwq>> pwq = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL, pool->node);
+ *   - mm/slab.c|2380| <<alloc_slabmgmt>> freelist = kmem_cache_alloc_node(cachep->freelist_cache,
+ *   - mm/slub.c|4438| <<init_kmem_cache_nodes>> n = kmem_cache_alloc_node(kmem_cache_node,
+ *   - net/core/skbuff.c|193| <<__alloc_skb>> skb = kmem_cache_alloc_node(cache, gfp_mask & ~__GFP_DMA, node);
+ *   - net/openvswitch/flow.c|105| <<ovs_flow_stats_update>> kmem_cache_alloc_node(flow_stats_cache,
+ *   - net/openvswitch/flow_table.c|91| <<ovs_flow_alloc>> stats = kmem_cache_alloc_node(flow_stats_cache,
+ */
 void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags, int node)
 {
 	void *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_);
@@ -2790,6 +4064,16 @@ EXPORT_SYMBOL(kmem_cache_alloc_node_trace);
  * lock and free the item. If there is no additional partial page
  * handling required then we can return immediately.
  */
+/*
+ * called by:
+ *   - mm/slub.c|3006| <<do_slab_free>> __slab_free(s, page, head, tail_obj, cnt, addr);
+ *
+ *  kmem_cache_free()间接进来的时候:
+ *   - cnt是1
+ *   - page是virt_to_head_page(x)
+ *   - head是x
+ *   - tail是head=x
+ */
 static void __slab_free(struct kmem_cache *s, struct page *page,
 			void *head, void *tail, int cnt,
 			unsigned long addr)
@@ -2804,6 +4088,13 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 
 	stat(s, FREE_SLOWPATH);
 
+	/*
+	 * kmem_cache_debug():
+	 * 当kmem_cache->flags设置了debug的任何flag的时候返回 true
+	 *
+	 * free_debug_processing():
+	 * 会检查每一个free的object:
+	 */
 	if (kmem_cache_debug(s) &&
 	    !free_debug_processing(s, page, head, tail, cnt, addr))
 		return;
@@ -2813,14 +4104,30 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 			spin_unlock_irqrestore(&n->list_lock, flags);
 			n = NULL;
 		}
+		/*
+		 * struct page:
+		 *   -> void *freelist; // sl[aou]b first free object
+		 */
 		prior = page->freelist;
 		counters = page->counters;
 		set_freepointer(s, tail, prior);
 		new.counters = counters;
+		/*
+		 * struct page:
+		 *   unsigned inuse:16;
+		 *   unsigned objects:15;
+		 *   unsigned frozen:1;
+		 */
 		was_frozen = new.frozen;
 		new.inuse -= cnt;
 		if ((!new.inuse || !prior) && !was_frozen) {
 
+			/*
+			 * 使用了CONFIG_SLUB_CPU_PARTIAL的情况下, 当kmem_cache->flags设置了debug的任何flag的时候返回false
+			 * 没有CONFIG_SLUB_CPU_PARTIAL直接返回false
+			 * ol支持CONFIG_SLUB_CPU_PARTIAL
+			 * 说明用debug的时候不支持kmem_cache_has_cpu_partial
+			 */
 			if (kmem_cache_has_cpu_partial(s) && !prior) {
 
 				/*
@@ -2847,6 +4154,11 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 			}
 		}
 
+		/*
+		 * 我们期待cmpxchg_double_slab()返回true, 不希望false
+		 * 核心思想是判断page->freelist和page->counters是否和old的相等
+		 * 如果相等,则吧page->freelist和page->counters都更新成新的
+		 */
 	} while (!cmpxchg_double_slab(s, page,
 		prior, counters,
 		head, new.counters,
@@ -2859,6 +4171,12 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 		 * per cpu partial list.
 		 */
 		if (new.frozen && !was_frozen) {
+			/*
+			 * Put a page that was just frozen (in __slab_free) into a partial page
+			 * slot if available.
+			 * 核心思想把一个page加入到cpu cache的partial中
+			 * 这个page有可能是在__slab_free()中frozen的
+			 */
 			put_cpu_partial(s, page, 1);
 			stat(s, CPU_PARTIAL_FREE);
 		}
@@ -2881,6 +4199,10 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 	if (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {
 		if (kmem_cache_debug(s))
 			remove_full(s, n, page);
+		/*
+		 * 根据参数是否是DEACTIVATE_TO_TAIL决定是否把page->lru加入到n->partial的head还是tail,
+		 * 增加n->nr_partial++
+		 */
 		add_partial(n, page, DEACTIVATE_TO_TAIL);
 		stat(s, FREE_ADD_PARTIAL);
 	}
@@ -2892,15 +4214,26 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 		/*
 		 * Slab on the partial list.
 		 */
+		/*
+		 * 把page->lru从n->partial移除
+		 * 减少n->nr_partial--
+		 */
 		remove_partial(n, page);
 		stat(s, FREE_REMOVE_PARTIAL);
 	} else {
 		/* Slab must be on the full list */
+		/*
+		 * 如果s->flags设置了SLAB_STORE_USER, 则把page->lru从kmem_cache_node->full删除
+		 */
 		remove_full(s, n, page);
 	}
 
 	spin_unlock_irqrestore(&n->list_lock, flags);
 	stat(s, FREE_SLAB);
+	/*
+	 * 核心思想是释放参数的page
+	 * 还要把page->objects从node的cache减去
+	 */
 	discard_slab(s, page);
 }
 
@@ -2919,10 +4252,29 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
  * same page) possible by specifying head and tail ptr, plus objects
  * count (cnt). Bulk free indicated by tail pointer being set.
  */
+/*
+ * called by:
+ *   - mm/slub.c|3021| <<slab_free>> do_slab_free(s, page, head, tail, cnt, addr);
+ *   - mm/slub.c|3027| <<___cache_free>> do_slab_free(cache, virt_to_head_page(x), x, NULL, 1, addr);
+ *
+ * kmem_cache_free()间接进来的时候:
+ *   - cnt是1
+ *   - page是virt_to_head_page(x)
+ *   - head是x
+ *   - tail是NULL
+ */
 static __always_inline void do_slab_free(struct kmem_cache *s,
 				struct page *page, void *head, void *tail,
 				int cnt, unsigned long addr)
 {
+	/*
+	 * kmem_cache_free()间接进来的时候:
+	 *   - cnt是1
+	 *   - page是virt_to_head_page(x)
+	 *   - head是x
+	 *   - tail是NULL
+	 *   - tail_obj是head=x
+	 */
 	void *tail_obj = tail ? : head;
 	struct kmem_cache_cpu *c;
 	unsigned long tid;
@@ -2942,9 +4294,25 @@ static __always_inline void do_slab_free(struct kmem_cache *s,
 	/* Same with comment on barrier() in slab_alloc_node() */
 	barrier();
 
+	/*
+	 * 在slub_debug开启的情况下, 在deactivate_slab()中会:
+	 * Remove the cpu slab
+	 * 该函数会将CPU缓存对象的freelist中的对象,还给slab对象(page)
+	 * 然后把这个page放入partial或者根据情况释放
+	 * 关键最后会进行以下:
+	 *   c->page = NULL;
+	 *   c->freelist = NULL;
+	 *
+	 * (page == c->page)说明当前的freelist的page和要释放的object共享一个page
+	 */
 	if (likely(page == c->page)) {
 		set_freepointer(s, tail_obj, c->freelist);
 
+		/*
+		 * 用atomic的方式:
+		 * 1. 把s->cpu_slab->freelist从c->freelist换成head
+		 * 2. 把s->cpu_slab->tid从tid换成next_tid(tid)
+		 */
 		if (unlikely(!this_cpu_cmpxchg_double(
 				s->cpu_slab->freelist, s->cpu_slab->tid,
 				c->freelist, tid,
@@ -2959,6 +4327,12 @@ static __always_inline void do_slab_free(struct kmem_cache *s,
 
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|3036| <<kmem_cache_free>> slab_free(s, virt_to_head_page(x), x, NULL, 1, _RET_IP_);
+ *   - mm/slub.c|3145| <<kmem_cache_free_bulk>> slab_free(df.s, df.page, df.freelist, df.tail, df.cnt,_RET_IP_);
+ *   - mm/slub.c|3945| <<kfree>> slab_free(page->slab_cache, page, object, NULL, 1, _RET_IP_);
+ */
 static __always_inline void slab_free(struct kmem_cache *s, struct page *page,
 				      void *head, void *tail, int cnt,
 				      unsigned long addr)
@@ -2968,8 +4342,18 @@ static __always_inline void slab_free(struct kmem_cache *s, struct page *page,
 	 * slab_free_freelist_hook() could have put the items into quarantine.
 	 * If so, no need to free them.
 	 */
+	/*
+	 * 如果KASAN激活了,退出
+	 */
 	if (s->flags & SLAB_KASAN && !(s->flags & SLAB_TYPESAFE_BY_RCU))
 		return;
+	/*
+	 * kmem_cache_free()进来的时候:
+	 *   - cnt是1
+	 *   - page是virt_to_head_page(x)
+	 *   - head是x
+	 *   - tail是NULL
+	 */
 	do_slab_free(s, page, head, tail, cnt, addr);
 }
 
@@ -2980,11 +4364,20 @@ void ___cache_free(struct kmem_cache *cache, void *x, unsigned long addr)
 }
 #endif
 
+/*
+ * 调用的一个例子:
+ *   - drivers/scsi/scsi_lib.c|58| <<scsi_free_sense_buffer>> kmem_cache_free(scsi_select_sense_cache(unchecked_isa_dma),
+ */
 void kmem_cache_free(struct kmem_cache *s, void *x)
 {
+	/*
+	 * 有cgroup是否激活两种判断方式
+	 * 这里就是查看还给哪个slab
+	 */
 	s = cache_from_obj(s, x);
 	if (!s)
 		return;
+	/* virt_to_head_page(x)是一个struct page * */
 	slab_free(s, virt_to_head_page(x), x, NULL, 1, _RET_IP_);
 	trace_kmem_cache_free(_RET_IP_, x);
 }
@@ -3232,6 +4625,10 @@ static inline int slab_order(int size, int min_objects,
 	return order;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4660| <<calculate_sizes>> order = calculate_order(size, s->reserved);
+ */
 static inline int calculate_order(int size, int reserved)
 {
 	int order;
@@ -3295,6 +4692,10 @@ init_kmem_cache_node(struct kmem_cache_node *n)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|5056| <<kmem_cache_open>> if (alloc_kmem_cache_cpus(s))
+ */
 static inline int alloc_kmem_cache_cpus(struct kmem_cache *s)
 {
 	BUILD_BUG_ON(PERCPU_DYNAMIC_EARLY_SIZE <
@@ -3406,6 +4807,11 @@ static int init_kmem_cache_nodes(struct kmem_cache *s)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4015| <<kmem_cache_open>> set_min_partial(s, ilog2(s->size) / 2);
+ *   - mm/slub.c|5378| <<min_partial_store>> set_min_partial(s, min);
+ */
 static void set_min_partial(struct kmem_cache *s, unsigned long min)
 {
 	if (min < MIN_PARTIAL)
@@ -3435,6 +4841,12 @@ static void set_cpu_partial(struct kmem_cache *s)
 	 *    per node list when we run out of per cpu objects. We only fetch
 	 *    50% to keep some capacity around for frees.
 	 */
+	/*
+	 * 使用了CONFIG_SLUB_CPU_PARTIAL的情况下, 当kmem_cache->flags设置了debug的任何flag的时候返回false
+	 * 没有CONFIG_SLUB_CPU_PARTIAL直接返回false
+	 * ol支持CONFIG_SLUB_CPU_PARTIAL
+	 * 说明用debug的时候不支持kmem_cache_has_cpu_partial
+	 */
 	if (!kmem_cache_has_cpu_partial(s))
 		s->cpu_partial = 0;
 	else if (s->size >= PAGE_SIZE)
@@ -3452,9 +4864,32 @@ static void set_cpu_partial(struct kmem_cache *s)
  * calculate_sizes() determines the order and the distribution of data within
  * a slab object.
  */
+/*
+ * [0] calculate_sizes
+ * [0] __kmem_cache_create
+ * [0] create_boot_cache
+ * [0] create_kmalloc_cache
+ * [0] new_kmalloc_cache
+ * [0] create_kmalloc_caches
+ * [0] kmem_cache_init
+ * [0] start_kernel
+ * [0] secondary_startup_64
+ *
+ * called by:
+ *   - mm/slub.c|4701| <<kmem_cache_open>> if (!calculate_sizes(s, -1))
+ *   - mm/slub.c|4711| <<kmem_cache_open>> if (!calculate_sizes(s, -1))
+ *   - mm/slub.c|6068| <<order_store>> calculate_sizes(s, order);
+ *   - mm/slub.c|6305| <<red_zone_store>> calculate_sizes(s, -1);
+ *   - mm/slub.c|6325| <<poison_store>> calculate_sizes(s, -1);
+ *   - mm/slub.c|6346| <<store_user_store>> calculate_sizes(s, -1);
+ */
 static int calculate_sizes(struct kmem_cache *s, int forced_order)
 {
 	unsigned long flags = s->flags;
+	/*
+	 * kmem_cache->size: The size of an object including meta data
+	 * kmem_cache->object_size: The size of an object without meta data
+	 */
 	size_t size = s->object_size;
 	int order;
 
@@ -3503,6 +4938,10 @@ static int calculate_sizes(struct kmem_cache *s, int forced_order)
 		 * This is the case if we do RCU, have a constructor or
 		 * destructor or are poisoning the objects.
 		 */
+		/*
+		 * 当slub_debug=ZUF(没有P)的时候, kmalloc-128的s->offset=0
+		 * 当slub_debug=PZUF(有P)的时候, kmalloc-128的s->offset=136
+		 */
 		s->offset = size;
 		size += sizeof(void *);
 	}
@@ -3540,6 +4979,10 @@ static int calculate_sizes(struct kmem_cache *s, int forced_order)
 	 * each object to conform to the alignment.
 	 */
 	size = ALIGN(size, s->align);
+	/*
+	 * kmem_cache->size: The size of an object including meta data
+	 * kmem_cache->object_size: The size of an object without meta data
+	 */
 	s->size = size;
 	if (forced_order >= 0)
 		order = forced_order;
@@ -3570,6 +5013,21 @@ static int calculate_sizes(struct kmem_cache *s, int forced_order)
 	return !!oo_objects(s->oo);
 }
 
+/*
+ * 初始化kmalloc-128:
+ * [0] kmem_cach_open
+ * [0] __kmem_cache_create
+ * [0] create_boot_cache
+ * [0] create_kmalloc_cache
+ * [0] new_kmalloc_cache
+ * [0] create_kmalloc_caches
+ * [0] kmem_cache_init
+ * [0] start_kernel
+ * [0] secondary_startup_64
+ *
+ * called by:
+ *   - mm/slub.c|4537| <<__kmem_cache_create>> err = kmem_cache_open(s, flags);
+ */
 static int kmem_cache_open(struct kmem_cache *s, unsigned long flags)
 {
 	s->flags = kmem_cache_flags(s->size, flags, s->name, s->ctor);
@@ -3624,6 +5082,9 @@ static int kmem_cache_open(struct kmem_cache *s, unsigned long flags)
 	if (!init_kmem_cache_nodes(s))
 		goto error;
 
+	/*
+	 * 似乎成功了返回1
+	 */
 	if (alloc_kmem_cache_cpus(s))
 		return 0;
 
@@ -4167,8 +5628,21 @@ static struct kmem_cache * __init bootstrap(struct kmem_cache *static_cache)
 	return s;
 }
 
+/*
+ * called by:
+ *   - init/main.c|503| <<mm_init>> kmem_cache_init();
+ */
 void __init kmem_cache_init(void)
 {
+	/*
+	 * 在以下使用boot_kmem_cache:
+	 *   - mm/slub.c|4217| <<kmem_cache_init>> kmem_cache = &boot_kmem_cache;
+	 *   - mm/slub.c|4232| <<kmem_cache_init>> kmem_cache = bootstrap(&boot_kmem_cache);
+	 *
+	 * 在以下使用boot_kmem_cache_node:
+	 *   - mm/slub.c|4216| <<kmem_cache_init>> kmem_cache_node = &boot_kmem_cache_node;
+	 *   - mm/slub.c|4239| <<kmem_cache_init>> kmem_cache_node = bootstrap(&boot_kmem_cache_node);
+	 */
 	static __initdata struct kmem_cache boot_kmem_cache,
 		boot_kmem_cache_node;
 
@@ -4252,6 +5726,22 @@ __kmem_cache_alias(const char *name, size_t size, size_t align,
 	return s;
 }
 
+/*
+ * 初始化kmalloc-128:
+ * [0] kmem_cach_open
+ * [0] __kmem_cache_create
+ * [0] create_boot_cache
+ * [0] create_kmalloc_cache
+ * [0] new_kmalloc_cache
+ * [0] create_kmalloc_caches
+ * [0] kmem_cache_init
+ * [0] start_kernel
+ * [0] secondary_startup_64
+ *
+ * called by:
+ *   - mm/slab_common.c|390| <<create_cache>> err = __kmem_cache_create(s, flags);
+ *   - mm/slab_common.c|896| <<create_boot_cache>> err = __kmem_cache_create(s, flags);
+ */
 int __kmem_cache_create(struct kmem_cache *s, unsigned long flags)
 {
 	int err;
@@ -4337,6 +5827,15 @@ static int count_total(struct page *page)
 #endif
 
 #ifdef CONFIG_SLUB_DEBUG
+/*
+ * called by:
+ *   - mm/slub.c|4794| <<validate_slab_slab>> validate_slab(s, page, map);
+ *
+ * validate_slab_cache()
+ *  -> validate_slab_node()
+ *      -> validate_slab_slab()
+ *          -> validate_slab()
+ */
 static int validate_slab(struct kmem_cache *s, struct page *page,
 						unsigned long *map)
 {
@@ -4364,6 +5863,11 @@ static int validate_slab(struct kmem_cache *s, struct page *page,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4808| <<validate_slab_node>> validate_slab_slab(s, page, map);
+ *   - mm/slub.c|4819| <<validate_slab_node>> validate_slab_slab(s, page, map);
+ */
 static void validate_slab_slab(struct kmem_cache *s, struct page *page,
 						unsigned long *map)
 {
@@ -4372,6 +5876,10 @@ static void validate_slab_slab(struct kmem_cache *s, struct page *page,
 	slab_unlock(page);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4844| <<validate_slab_cache>> count += validate_slab_node(s, n, map);
+ */
 static int validate_slab_node(struct kmem_cache *s,
 		struct kmem_cache_node *n, unsigned long *map)
 {
@@ -4405,6 +5913,16 @@ static int validate_slab_node(struct kmem_cache *s,
 	return count;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|5091| <<resiliency_test>> validate_slab_cache(kmalloc_caches[4]);
+ *   - mm/slub.c|5100| <<resiliency_test>> validate_slab_cache(kmalloc_caches[5]);
+ *   - mm/slub.c|5107| <<resiliency_test>> validate_slab_cache(kmalloc_caches[6]);
+ *   - mm/slub.c|5114| <<resiliency_test>> validate_slab_cache(kmalloc_caches[7]);
+ *   - mm/slub.c|5120| <<resiliency_test>> validate_slab_cache(kmalloc_caches[8]);
+ *   - mm/slub.c|5126| <<resiliency_test>> validate_slab_cache(kmalloc_caches[9]);
+ *   - mm/slub.c|5624| <<validate_store>> ret = validate_slab_cache(s);
+ */
 static long validate_slab_cache(struct kmem_cache *s)
 {
 	int node;
@@ -4472,6 +5990,10 @@ static int alloc_loc_track(struct loc_track *t, unsigned long max, gfp_t flags)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|6076| <<process_slab>> add_location(t, s, get_track(s, p, alloc));
+ */
 static int add_location(struct loc_track *t, struct kmem_cache *s,
 				const struct track *track)
 {
@@ -4548,6 +6070,11 @@ static int add_location(struct loc_track *t, struct kmem_cache *s,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|6107| <<list_locations>> process_slab(&t, s, page, alloc, map);
+ *   - mm/slub.c|6109| <<list_locations>> process_slab(&t, s, page, alloc, map);
+ */
 static void process_slab(struct loc_track *t, struct kmem_cache *s,
 		struct page *page, enum track_item alloc,
 		unsigned long *map)
@@ -4563,6 +6090,11 @@ static void process_slab(struct loc_track *t, struct kmem_cache *s,
 			add_location(t, s, get_track(s, p, alloc));
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|6726| <<alloc_calls_show>> return list_locations(s, buf, TRACK_ALLOC);
+ *   - mm/slub.c|6734| <<free_calls_show>> return list_locations(s, buf, TRACK_FREE);
+ */
 static int list_locations(struct kmem_cache *s, char *buf,
 					enum track_item alloc)
 {
@@ -5664,6 +7196,11 @@ static void sysfs_slab_remove_workfn(struct work_struct *work)
 	kobject_put(&s->kobj);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4268| <<__kmem_cache_create>> err = sysfs_slab_add(s);
+ *   - mm/slub.c|5811| <<slab_sysfs_init>> err = sysfs_slab_add(s);
+ */
 static int sysfs_slab_add(struct kmem_cache *s)
 {
 	int err;
@@ -5837,6 +7374,11 @@ __initcall(slab_sysfs_init);
  * The /proc/slabinfo ABI
  */
 #ifdef CONFIG_SLABINFO
+/*
+ * called by:
+ *   - mm/slab_common.c|1247| <<memcg_accumulate_slabinfo>> get_slabinfo(c, &sinfo);
+ *   - mm/slab_common.c|1262| <<cache_show>> get_slabinfo(s, &sinfo);
+ */
 void get_slabinfo(struct kmem_cache *s, struct slabinfo *sinfo)
 {
 	unsigned long nr_slabs = 0;
@@ -5845,6 +7387,9 @@ void get_slabinfo(struct kmem_cache *s, struct slabinfo *sinfo)
 	int node;
 	struct kmem_cache_node *n;
 
+	/*
+	 * 对于s中的每一个struct kmem_cache_node *node[MAX_NUMNODES];
+	 */
 	for_each_kmem_cache_node(s, node, n) {
 		nr_slabs += node_nr_slabs(n);
 		nr_objs += node_nr_objs(n);
-- 
2.17.1

