From 5e3bc64dd300cb051b012c795469c1a7d98286bd Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Wed, 8 Jan 2020 06:24:28 -0800
Subject: [PATCH 1/1] linux-mm-for-4.14.113

Signed-off-by: Dongli Zhang <dongli.zhangi0129@gmail.com>
---
 include/linux/memcontrol.h |  14 +++
 include/linux/slub_def.h   |   5 +
 mm/kasan/kasan.c           |   5 +
 mm/slab.h                  |  25 ++++
 mm/slab_common.c           |   6 +
 mm/slub.c                  | 301 +++++++++++++++++++++++++++++++++++++++++++++
 6 files changed, 356 insertions(+)

diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index 69966c4..4df9017 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -1118,6 +1118,20 @@ void memcg_put_cache_ids(void);
 #define for_each_memcg_cache_index(_idx)	\
 	for ((_idx) = 0; (_idx) < memcg_nr_cache_ids; (_idx)++)
 
+/*
+ * called by:
+ *   - fs/pipe.c|148| <<anon_pipe_buf_steal>> if (memcg_kmem_enabled())
+ *   - mm/list_lru.c|70| <<mem_cgroup_from_kmem>> if (!memcg_kmem_enabled())
+ *   - mm/page_alloc.c|1049| <<free_pages_prepare>> if (memcg_kmem_enabled() && PageKmemcg(page))
+ *   - mm/page_alloc.c|4201| <<__alloc_pages_nodemask>> if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
+ *   - mm/slab.h|277| <<memcg_charge_slab>> if (!memcg_kmem_enabled())
+ *   - mm/slab.h|287| <<memcg_uncharge_slab>> if (!memcg_kmem_enabled())
+ *   - mm/slab.h|366| <<cache_from_obj>> if (!memcg_kmem_enabled() &&
+ *   - mm/slab.h|432| <<slab_pre_alloc_hook>> if (memcg_kmem_enabled() &&
+ *   - mm/slab.h|453| <<slab_post_alloc_hook>> if (memcg_kmem_enabled())
+ *   - mm/vmscan.c|468| <<shrink_slab>> if (memcg && (!memcg_kmem_enabled() || !mem_cgroup_online(memcg)))
+ *   - mm/vmscan.c|497| <<shrink_slab>> if (memcg_kmem_enabled() &&
+ */
 static inline bool memcg_kmem_enabled(void)
 {
 	return static_branch_unlikely(&memcg_kmem_enabled_key);
diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index f8ced87..10525cb 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -53,11 +53,16 @@ struct kmem_cache_cpu {
 #ifdef CONFIG_SLUB_CPU_PARTIAL
 #define slub_percpu_partial(c)		((c)->partial)
 
+/*
+ * called by:
+ *   - mm/slub.c|2608| <<___slab_alloc>> slub_set_percpu_partial(c, page);
+ */
 #define slub_set_percpu_partial(c, p)		\
 ({						\
 	slub_percpu_partial(c) = (p)->next;	\
 })
 
+/* 如果c->partial存在 */
 #define slub_percpu_partial_read_once(c)     READ_ONCE(slub_percpu_partial(c))
 #else
 #define slub_percpu_partial(c)			NULL
diff --git a/mm/kasan/kasan.c b/mm/kasan/kasan.c
index 71a4319..23e5219 100644
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@ -453,6 +453,11 @@ static inline depot_stack_handle_t save_stack(gfp_t flags)
 	return depot_save_stack(&trace, flags);
 }
 
+/*
+ * called by:
+ *   - mm/kasan/kasan.c|524| <<kasan_slab_free>> set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
+ *   - mm/kasan/kasan.c|551| <<kasan_kmalloc>> set_track(&get_alloc_info(cache, object)->alloc_track, flags);
+ */
 static inline void set_track(struct kasan_track *track, gfp_t flags)
 {
 	track->pid = current->pid;
diff --git a/mm/slab.h b/mm/slab.h
index 485d9fb..6c108b4 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -351,6 +351,13 @@ static inline void memcg_link_cache(struct kmem_cache *s)
 
 #endif /* CONFIG_MEMCG && !CONFIG_SLOB */
 
+/*
+ * called by:
+ *   - mm/slub.c|3033| <<kmem_cache_free>> s = cache_from_obj(s, x);
+ *   - mm/slub.c|3094| <<build_detached_freelist>> df->s = cache_from_obj(s, object);
+ *   - mm/slab.c|3750| <<kmem_cache_free>> cachep = cache_from_obj(cachep, objp);
+ *   - mm/slab.c|3777| <<kmem_cache_free_bulk>> s = cache_from_obj(orig_s, objp);
+ */
 static inline struct kmem_cache *cache_from_obj(struct kmem_cache *s, void *x)
 {
 	struct kmem_cache *cachep;
@@ -408,6 +415,14 @@ static inline size_t slab_ksize(const struct kmem_cache *s)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2649| <<slab_alloc_node>> s = slab_pre_alloc_hook(s, gfpflags);
+ *   - mm/slub.c|3117| <<kmem_cache_alloc_bulk>> s = slab_pre_alloc_hook(s, flags);
+ *   - mm/slab.c|3297| <<slab_alloc_node>> cachep = slab_pre_alloc_hook(cachep, flags);
+ *   - mm/slab.c|3376| <<slab_alloc>> cachep = slab_pre_alloc_hook(cachep, flags);
+ *   - mm/slab.c|3575| <<kmem_cache_alloc_bulk>> s = slab_pre_alloc_hook(s, flags);
+ */
 static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
 						     gfp_t flags)
 {
@@ -428,6 +443,16 @@ static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
 	return s;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2764| <<slab_alloc_node>> slab_post_alloc_hook(s, gfpflags, 1, &object);
+ *   - mm/slub.c|3200| <<kmem_cache_alloc_bulk>> slab_post_alloc_hook(s, flags, size, p);
+ *   - mm/slub.c|3204| <<kmem_cache_alloc_bulk>> slab_post_alloc_hook(s, flags, i, p);
+ *   - mm/slab.c|3333| <<slab_alloc_node>> slab_post_alloc_hook(cachep, flags, 1, &ptr);
+ *   - mm/slab.c|3390| <<slab_alloc>> slab_post_alloc_hook(cachep, flags, 1, &objp);
+ *   - mm/slab.c|3598| <<kmem_cache_alloc_bulk>> slab_post_alloc_hook(s, flags, size, p);
+ *   - mm/slab.c|3604| <<kmem_cache_alloc_bulk>> slab_post_alloc_hook(s, flags, i, p);
+ */
 static inline void slab_post_alloc_hook(struct kmem_cache *s, gfp_t flags,
 					size_t size, void **p)
 {
diff --git a/mm/slab_common.c b/mm/slab_common.c
index f6764cf..cd24f74 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1061,6 +1061,12 @@ void __init setup_kmalloc_cache_index_table(void)
 	}
 }
 
+/*
+ * called by:
+ *   - mm/slab_common.c|1081| <<create_kmalloc_caches>> new_kmalloc_cache(i, flags);
+ *   - mm/slab_common.c|1089| <<create_kmalloc_caches>> new_kmalloc_cache(1, flags);
+ *   - mm/slab_common.c|1091| <<create_kmalloc_caches>> new_kmalloc_cache(2, flags);
+ */
 static void __init new_kmalloc_cache(int idx, unsigned long flags)
 {
 	kmalloc_caches[idx] = create_kmalloc_cache(kmalloc_info[idx].name,
diff --git a/mm/slub.c b/mm/slub.c
index 220d42e..7fd2357 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -41,6 +41,10 @@
 #include "internal.h"
 
 /*
+ * 创建的入口是__kmem_cache_create()
+ */
+
+/*
  * Lock order:
  *   1. slab_mutex (Global Mutex)
  *   2. node->list_lock
@@ -116,6 +120,17 @@
  * 			the fast path and disables lockless freelists.
  */
 
+ /*
+  * called by:
+  *   - mm/slub.c|130| <<fixup_red_left>> if (kmem_cache_debug(s) && s->flags & SLAB_RED_ZONE)
+  *   - mm/slub.c|139| <<kmem_cache_has_cpu_partial>> return !kmem_cache_debug(s);
+  *   - mm/slub.c|2097| <<deactivate_slab>> if (kmem_cache_debug(s) && !lock) {
+  *   - mm/slub.c|2594| <<___slab_alloc>> if (likely(!kmem_cache_debug(s) && pfmemalloc_match(page, gfpflags)))
+  *   - mm/slub.c|2598| <<___slab_alloc>> if (kmem_cache_debug(s) &&
+  *   - mm/slub.c|2814| <<__slab_free>> if (kmem_cache_debug(s) &&
+  *   - mm/slub.c|2889| <<__slab_free>> if (kmem_cache_debug(s))
+  *   - mm/slub.c|3845| <<__check_heap_object>> if (kmem_cache_debug(s) && s->flags & SLAB_RED_ZONE) {
+  */
 static inline int kmem_cache_debug(struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_DEBUG
@@ -133,6 +148,14 @@ void *fixup_red_left(struct kmem_cache *s, void *p)
 	return p;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1842| <<get_partial_node>> if (!kmem_cache_has_cpu_partial(s)
+ *   - mm/slub.c|2831| <<__slab_free>> if (kmem_cache_has_cpu_partial(s) && !prior) {
+ *   - mm/slub.c|2888| <<__slab_free>> if (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {
+ *   - mm/slub.c|3445| <<set_cpu_partial>> if (!kmem_cache_has_cpu_partial(s))
+ *   - mm/slub.c|4958| <<cpu_partial_store>> if (objects && !kmem_cache_has_cpu_partial(s))
+ */
 static inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_CPU_PARTIAL
@@ -262,22 +285,50 @@ static inline void *freelist_dereference(const struct kmem_cache *s,
 			    (unsigned long)ptr_addr);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|318| <<get_freepointer_safe>> return get_freepointer(s, object);
+ *   - mm/slub.c|519| <<get_map>> for (p = page->freelist; p; p = get_freepointer(s, p))
+ *   - mm/slub.c|728| <<print_trailer>> p, p - addr, get_freepointer(s, p));
+ *   - mm/slub.c|966| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+ *   - mm/slub.c|1041| <<on_freelist>> fp = get_freepointer(s, object);
+ *   - mm/slub.c|1267| <<free_debug_processing>> object = get_freepointer(s, object);
+ *   - mm/slub.c|1445| <<slab_free_hook>> freeptr = get_freepointer(s, x);
+ *   - mm/slub.c|2114| <<deactivate_slab>> while (freelist && (nextfree = get_freepointer(s, freelist))) {
+ *   - mm/slub.c|2665| <<___slab_alloc>> c->freelist = get_freepointer(s, freelist);
+ *   - mm/slub.c|2696| <<___slab_alloc>> deactivate_slab(s, page, get_freepointer(s, freelist), c);
+ *   - mm/slub.c|3316| <<kmem_cache_alloc_bulk>> c->freelist = get_freepointer(s, object);
+ *   - mm/slub.c|3525| <<early_kmem_cache_node_alloc>> page->freelist = get_freepointer(kmem_cache_node, n);
+ */
 static inline void *get_freepointer(struct kmem_cache *s, void *object)
 {
 	return freelist_dereference(s, object + s->offset);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2757| <<slab_alloc_node>> prefetch_freepointer(s, next_object);
+ */
 static void prefetch_freepointer(const struct kmem_cache *s, void *object)
 {
 	if (object)
 		prefetch(freelist_dereference(s, object + s->offset));
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2733| <<slab_alloc_node>> void *next_object = get_freepointer_safe(s, object);
+ */
 static inline void *get_freepointer_safe(struct kmem_cache *s, void *object)
 {
 	unsigned long freepointer_addr;
 	void *p;
 
+	/*
+	 * 如果没有debug
+	 *
+	 * object理论是c->freelist
+	 */
 	if (!debug_pagealloc_enabled())
 		return get_freepointer(s, object);
 
@@ -309,6 +360,14 @@ static inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)
 		__p += (__s)->size, __idx++)
 
 /* Determine object index from a given position */
+/*
+ * called by:
+ *   - mm/slub.c|464| <<get_map>> set_bit(slab_index(p, s, addr), map);
+ *   - mm/slub.c|3662| <<list_slab_objects>> if (!test_bit(slab_index(p, s, addr), map)) {
+ *   - mm/slub.c|4362| <<validate_slab>> if (test_bit(slab_index(p, s, addr), map))
+ *   - mm/slub.c|4368| <<validate_slab>> if (!test_bit(slab_index(p, s, addr), map))
+ *   - mm/slub.c|4569| <<process_slab>> if (!test_bit(slab_index(p, s, addr), map))
+ */
 static inline int slab_index(void *p, struct kmem_cache *s, void *addr)
 {
 	return (p - addr) / s->size;
@@ -370,6 +429,14 @@ static inline void set_page_slub_counters(struct page *page, unsigned long count
 }
 
 /* Interrupts must be disabled (for the fallback code to work right) */
+/*
+ * called by:
+ *   - mm/slub.c|1832| <<acquire_slab>> if (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2087| <<deactivate_slab>> } while (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2176| <<deactivate_slab>> if (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2235| <<unfreeze_partials>> } while (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2534| <<get_freelist>> } while (!__cmpxchg_double_slab(s, page,
+ */
 static inline bool __cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
 		void *freelist_old, unsigned long counters_old,
 		void *freelist_new, unsigned long counters_new,
@@ -407,6 +474,10 @@ static inline bool __cmpxchg_double_slab(struct kmem_cache *s, struct page *page
 	return false;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2938| <<__slab_free>> } while (!cmpxchg_double_slab(s, page,
+ */
 static inline bool cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
 		void *freelist_old, unsigned long counters_old,
 		void *freelist_new, unsigned long counters_new,
@@ -553,6 +624,26 @@ static struct track *get_track(struct kmem_cache *s, void *object,
 	return p + alloc;
 }
 
+/*
+ * [0] set_track
+ * [0] alloc_debug_processing
+ * [0] ___slab_alloc
+ * [0] __slab_alloc
+ * [0] kmem_cache_alloc
+ * [0] testsys_store
+ * [0] kernfs_fop_write
+ * [0] __vfs_write
+ * [0] vfs_write
+ * [0] SyS_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - mm/slub.c|666| <<init_tracking>> set_track(s, object, TRACK_FREE, 0UL);
+ *   - mm/slub.c|667| <<init_tracking>> set_track(s, object, TRACK_ALLOC, 0UL);
+ *   - mm/slub.c|1195| <<alloc_debug_processing>> set_track(s, object, TRACK_ALLOC, addr);
+ *   - mm/slub.c|1275| <<free_debug_processing>> set_track(s, object, TRACK_FREE, addr);
+ */
 static void set_track(struct kmem_cache *s, void *object,
 			enum track_item alloc, unsigned long addr)
 {
@@ -587,6 +678,11 @@ static void set_track(struct kmem_cache *s, void *object,
 		memset(p, 0, sizeof(struct track));
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1163| <<setup_object_debug>> init_tracking(s, object);
+ *   - mm/slub.c|3546| <<early_kmem_cache_node_alloc>> init_tracking(kmem_cache_node, n);
+ */
 static void init_tracking(struct kmem_cache *s, void *object)
 {
 	if (!(s->flags & SLAB_STORE_USER))
@@ -596,6 +692,11 @@ static void init_tracking(struct kmem_cache *s, void *object)
 	set_track(s, object, TRACK_ALLOC, 0UL);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|711| <<print_tracking>> print_track("Allocated", get_track(s, object, TRACK_ALLOC));
+ *   - mm/slub.c|712| <<print_tracking>> print_track("Freed", get_track(s, object, TRACK_FREE));
+ */
 static void print_track(const char *s, struct track *t)
 {
 	if (!t->addr)
@@ -615,6 +716,11 @@ static void print_track(const char *s, struct track *t)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|738| <<print_trailer>> print_tracking(s, p);
+ *   - mm/slub.c|3851| <<list_slab_objects>> print_tracking(s, p);
+ */
 static void print_tracking(struct kmem_cache *s, void *object)
 {
 	if (!(s->flags & SLAB_STORE_USER))
@@ -659,6 +765,11 @@ static void slab_fix(struct kmem_cache *s, char *fmt, ...)
 	va_end(args);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|779| <<object_err>> print_trailer(s, page, object);
+ *   - mm/slub.c|839| <<check_bytes_and_report>> print_trailer(s, page, object);
+ */
 static void print_trailer(struct kmem_cache *s, struct page *page, u8 *p)
 {
 	unsigned int off;	/* Offset of last byte */
@@ -951,6 +1062,11 @@ static int check_slab(struct kmem_cache *s, struct page *page)
  * Determine if a certain object on a page is on the freelist. Must hold the
  * slab lock to guarantee that the chains are in a consistent state.
  */
+/*
+ * called by:
+ *   - mm/slub.c|1190| <<free_consistency_checks>> if (on_freelist(s, page, object)) {
+ *   - mm/slub.c|4498| <<validate_slab>> !on_freelist(s, page, NULL))
+ */
 static int on_freelist(struct kmem_cache *s, struct page *page, void *search)
 {
 	int nr = 0;
@@ -1077,6 +1193,10 @@ static inline void dec_slabs_node(struct kmem_cache *s, int node, int objects)
 }
 
 /* Object debug checks for alloc/free paths */
+/*
+ * called by:
+ *   - mm/slub.c|1521| <<setup_object>> setup_object_debug(s, page, object);
+ */
 static void setup_object_debug(struct kmem_cache *s, struct page *page,
 								void *object)
 {
@@ -1105,6 +1225,10 @@ static inline int alloc_consistency_checks(struct kmem_cache *s,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2749| <<___slab_alloc>> !alloc_debug_processing(s, page, freelist, addr))
+ */
 static noinline int alloc_debug_processing(struct kmem_cache *s,
 					struct page *page,
 					void *object, unsigned long addr)
@@ -1288,6 +1412,11 @@ static int __init setup_slub_debug(char *str)
 
 __setup("slub_debug", setup_slub_debug);
 
+/*
+ * called by:
+ *   - mm/slab_common.c|307| <<find_mergeable>> flags = kmem_cache_flags(size, flags, name, NULL);
+ *   - mm/slub.c|3769| <<kmem_cache_open>> s->flags = kmem_cache_flags(s->size, flags, s->name, s->ctor);
+ */
 unsigned long kmem_cache_flags(unsigned long object_size,
 	unsigned long flags, const char *name,
 	void (*ctor)(void *))
@@ -1412,6 +1541,12 @@ static inline void slab_free_freelist_hook(struct kmem_cache *s,
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1640| <<shuffle_freelist>> setup_object(s, page, cur);
+ *   - mm/slub.c|1646| <<shuffle_freelist>> setup_object(s, page, cur);
+ *   - mm/slub.c|1724| <<allocate_slab>> setup_object(s, page, p);
+ */
 static void setup_object(struct kmem_cache *s, struct page *page,
 				void *object)
 {
@@ -1508,6 +1643,10 @@ static void *next_freelist_entry(struct kmem_cache *s, struct page *page,
 }
 
 /* Shuffle the single linked freelist based on a random pre-computed sequence */
+/*
+ * called by:
+ *   - mm/slub.c|1720| <<allocate_slab>> shuffle = shuffle_freelist(s, page);
+ */
 static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
 {
 	void *start;
@@ -1553,6 +1692,10 @@ static inline bool shuffle_freelist(struct kmem_cache *s, struct page *page)
 }
 #endif /* CONFIG_SLAB_FREELIST_RANDOM */
 
+/*
+ * called by:
+ *   - mm/slub.c|1695| <<new_slab>> return allocate_slab(s,
+ */
 static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 {
 	struct page *page;
@@ -1638,6 +1781,11 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	return page;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2478| <<new_slab_objects>> page = new_slab(s, flags, node);
+ *   - mm/slub.c|3469| <<early_kmem_cache_node_alloc>> page = new_slab(kmem_cache_node, GFP_NOWAIT, node);
+ */
 static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
 {
 	if (unlikely(flags & GFP_SLAB_BUG_MASK)) {
@@ -1735,6 +1883,12 @@ __add_partial(struct kmem_cache_node *n, struct page *page, int tail)
 		list_add(&page->lru, &n->partial);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2164| <<deactivate_slab>> add_partial(n, page, tail);
+ *   - mm/slub.c|2244| <<unfreeze_partials>> add_partial(n, page, DEACTIVATE_TO_TAIL);
+ *   - mm/slub.c|2972| <<__slab_free>> add_partial(n, page, DEACTIVATE_TO_TAIL);
+ */
 static inline void add_partial(struct kmem_cache_node *n,
 				struct page *page, int tail)
 {
@@ -1802,6 +1956,11 @@ static inline bool pfmemalloc_match(struct page *page, gfp_t gfpflags);
 /*
  * Try to allocate a partial slab from a specific node.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2051| <<get_any_partial>> object = get_partial_node(s, n, c, flags);
+ *   - mm/slub.c|2083| <<get_partial>> object = get_partial_node(s, get_node(s, searchnode), c, flags);
+ */
 static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
 				struct kmem_cache_cpu *c, gfp_t flags)
 {
@@ -1915,6 +2074,10 @@ static void *get_any_partial(struct kmem_cache *s, gfp_t flags,
 /*
  * Get a partial page, lock it and return it.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2594| <<new_slab_objects>> freelist = get_partial(s, flags, node, c);
+ */
 static void *get_partial(struct kmem_cache *s, gfp_t flags, int node,
 		struct kmem_cache_cpu *c)
 {
@@ -2003,6 +2166,13 @@ static void init_kmem_cache_cpus(struct kmem_cache *s)
 /*
  * Remove the cpu slab
  */
+/*
+ * called by:
+ *   - mm/slub.c|2359| <<flush_slab>> deactivate_slab(s, c->page, c->freelist, c);
+ *   - mm/slub.c|2617| <<___slab_alloc>> deactivate_slab(s, page, c->freelist, c);
+ *   - mm/slub.c|2628| <<___slab_alloc>> deactivate_slab(s, page, c->freelist, c);
+ *   - mm/slub.c|2684| <<___slab_alloc>> deactivate_slab(s, page, get_freepointer(s, freelist), c);
+ */
 static void deactivate_slab(struct kmem_cache *s, struct page *page,
 				void *freelist, struct kmem_cache_cpu *c)
 {
@@ -2419,6 +2589,10 @@ slab_out_of_memory(struct kmem_cache *s, gfp_t gfpflags, int nid)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2676| <<___slab_alloc>> freelist = new_slab_objects(s, gfpflags, node, &c);
+ */
 static inline void *new_slab_objects(struct kmem_cache *s, gfp_t flags,
 			int node, struct kmem_cache_cpu **pc)
 {
@@ -2514,12 +2688,20 @@ static inline void *get_freelist(struct kmem_cache *s, struct page *page)
  * Version of __slab_alloc to use when we know that interrupts are
  * already disabled (which is the case for bulk allocation).
  */
+/*
+ * called by:
+ *   - mm/slub.c|2653| <<__slab_alloc>> p = ___slab_alloc(s, gfpflags, node, addr, c);
+ *   - mm/slub.c|3163| <<kmem_cache_alloc_bulk>> p[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,
+ */
 static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 			  unsigned long addr, struct kmem_cache_cpu *c)
 {
 	void *freelist;
 	struct page *page;
 
+	/*
+	 * slub_debug=U的时候c->page总是NULL
+	 */
 	page = c->page;
 	if (!page)
 		goto new_slab;
@@ -2576,6 +2758,7 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 
 new_slab:
 
+	/* 如果c->partial存在 */
 	if (slub_percpu_partial(c)) {
 		page = c->page = slub_percpu_partial(c);
 		slub_set_percpu_partial(c, page);
@@ -2583,6 +2766,7 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 		goto redo;
 	}
 
+	/* freelist声明的时候是void * */
 	freelist = new_slab_objects(s, gfpflags, node, &c);
 
 	if (unlikely(!freelist)) {
@@ -2607,6 +2791,10 @@ static void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
  * Another one that disabled interrupt and compensates for possible
  * cpu changes by refetching the per cpu area pointer.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2716| <<slab_alloc_node>> object = __slab_alloc(s, gfpflags, node, addr, c);
+ */
 static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 			  unsigned long addr, struct kmem_cache_cpu *c)
 {
@@ -2683,12 +2871,22 @@ static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 	 * linked list in between.
 	 */
 
+	/*
+	 * 下面的代码尝试在c->freelist分配一个
+	 * 如果c->freelist已经空了,就要用__slab_alloc()的slow path
+	 *
+	 * c->freelist是NULL说明分配已经到最后了
+	 */
+
 	object = c->freelist;
 	page = c->page;
 	if (unlikely(!object || !node_match(page, node))) {
 		object = __slab_alloc(s, gfpflags, node, addr, c);
 		stat(s, ALLOC_SLOWPATH);
 	} else {
+		/*
+		 * freelist_dereference(s, object + s->offset);
+		 */
 		void *next_object = get_freepointer_safe(s, object);
 
 		/*
@@ -2705,6 +2903,11 @@ static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 		 * against code executing on this cpu *not* from access by
 		 * other cpus.
 		 */
+		/*
+		 * 这里用atomic的形式:
+		 * 1. 把s->cpu_slab->freelist从object(c->freelist)换成next_object=get_freepointer_safe(s, object)
+		 * 2. 把s->cpu_slab->tid换成next_tid(tid)
+		 */
 		if (unlikely(!this_cpu_cmpxchg_double(
 				s->cpu_slab->freelist, s->cpu_slab->tid,
 				object, tid,
@@ -2725,6 +2928,13 @@ static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 	return object;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2736| <<kmem_cache_alloc>> void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+ *   - mm/slub.c|2748| <<kmem_cache_alloc_trace>> void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+ *   - mm/slub.c|3756| <<__kmalloc>> ret = slab_alloc(s, flags, _RET_IP_);
+ *   - mm/slub.c|4288| <<__kmalloc_track_caller>> ret = slab_alloc(s, gfpflags, caller);
+ */
 static __always_inline void *slab_alloc(struct kmem_cache *s,
 		gfp_t gfpflags, unsigned long addr)
 {
@@ -2790,6 +3000,16 @@ EXPORT_SYMBOL(kmem_cache_alloc_node_trace);
  * lock and free the item. If there is no additional partial page
  * handling required then we can return immediately.
  */
+/*
+ * called by:
+ *   - mm/slub.c|3006| <<do_slab_free>> __slab_free(s, page, head, tail_obj, cnt, addr);
+ *
+ *  kmem_cache_free()间接进来的时候:
+ *   - cnt是1
+ *   - page是virt_to_head_page(x)
+ *   - head是x
+ *   - tail是head=x
+ */
 static void __slab_free(struct kmem_cache *s, struct page *page,
 			void *head, void *tail, int cnt,
 			unsigned long addr)
@@ -2813,6 +3033,10 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 			spin_unlock_irqrestore(&n->list_lock, flags);
 			n = NULL;
 		}
+		/*
+		 * struct page:
+		 *   -> void *freelist; // sl[aou]b first free object
+		 */
 		prior = page->freelist;
 		counters = page->counters;
 		set_freepointer(s, tail, prior);
@@ -2919,10 +3143,29 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
  * same page) possible by specifying head and tail ptr, plus objects
  * count (cnt). Bulk free indicated by tail pointer being set.
  */
+/*
+ * called by:
+ *   - mm/slub.c|3021| <<slab_free>> do_slab_free(s, page, head, tail, cnt, addr);
+ *   - mm/slub.c|3027| <<___cache_free>> do_slab_free(cache, virt_to_head_page(x), x, NULL, 1, addr);
+ *
+ * kmem_cache_free()间接进来的时候:
+ *   - cnt是1
+ *   - page是virt_to_head_page(x)
+ *   - head是x
+ *   - tail是NULL
+ */
 static __always_inline void do_slab_free(struct kmem_cache *s,
 				struct page *page, void *head, void *tail,
 				int cnt, unsigned long addr)
 {
+	/*
+	 * kmem_cache_free()间接进来的时候:
+	 *   - cnt是1
+	 *   - page是virt_to_head_page(x)
+	 *   - head是x
+	 *   - tail是NULL
+	 *   - tail_obj是head=x
+	 */
 	void *tail_obj = tail ? : head;
 	struct kmem_cache_cpu *c;
 	unsigned long tid;
@@ -2942,9 +3185,17 @@ static __always_inline void do_slab_free(struct kmem_cache *s,
 	/* Same with comment on barrier() in slab_alloc_node() */
 	barrier();
 
+	/*
+	 * (page == c->page)说明当前的freelist的page和要释放的object共享一个page
+	 */
 	if (likely(page == c->page)) {
 		set_freepointer(s, tail_obj, c->freelist);
 
+		/*
+		 * 用atomic的方式:
+		 * 1. 把s->cpu_slab->freelist从c->freelist换成head
+		 * 2. 把s->cpu_slab->tid从tid换成next_tid(tid)
+		 */
 		if (unlikely(!this_cpu_cmpxchg_double(
 				s->cpu_slab->freelist, s->cpu_slab->tid,
 				c->freelist, tid,
@@ -2959,6 +3210,12 @@ static __always_inline void do_slab_free(struct kmem_cache *s,
 
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|3036| <<kmem_cache_free>> slab_free(s, virt_to_head_page(x), x, NULL, 1, _RET_IP_);
+ *   - mm/slub.c|3145| <<kmem_cache_free_bulk>> slab_free(df.s, df.page, df.freelist, df.tail, df.cnt,_RET_IP_);
+ *   - mm/slub.c|3945| <<kfree>> slab_free(page->slab_cache, page, object, NULL, 1, _RET_IP_);
+ */
 static __always_inline void slab_free(struct kmem_cache *s, struct page *page,
 				      void *head, void *tail, int cnt,
 				      unsigned long addr)
@@ -2970,6 +3227,13 @@ static __always_inline void slab_free(struct kmem_cache *s, struct page *page,
 	 */
 	if (s->flags & SLAB_KASAN && !(s->flags & SLAB_TYPESAFE_BY_RCU))
 		return;
+	/*
+	 * kmem_cache_free()进来的时候:
+	 *   - cnt是1
+	 *   - page是virt_to_head_page(x)
+	 *   - head是x
+	 *   - tail是NULL
+	 */
 	do_slab_free(s, page, head, tail, cnt, addr);
 }
 
@@ -2980,11 +3244,16 @@ void ___cache_free(struct kmem_cache *cache, void *x, unsigned long addr)
 }
 #endif
 
+/*
+ * 调用的一个例子:
+ *   - drivers/scsi/scsi_lib.c|58| <<scsi_free_sense_buffer>> kmem_cache_free(scsi_select_sense_cache(unchecked_isa_dma),
+ */
 void kmem_cache_free(struct kmem_cache *s, void *x)
 {
 	s = cache_from_obj(s, x);
 	if (!s)
 		return;
+	/* virt_to_head_page(x)是一个struct page * */
 	slab_free(s, virt_to_head_page(x), x, NULL, 1, _RET_IP_);
 	trace_kmem_cache_free(_RET_IP_, x);
 }
@@ -3570,6 +3839,10 @@ static int calculate_sizes(struct kmem_cache *s, int forced_order)
 	return !!oo_objects(s->oo);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4537| <<__kmem_cache_create>> err = kmem_cache_open(s, flags);
+ */
 static int kmem_cache_open(struct kmem_cache *s, unsigned long flags)
 {
 	s->flags = kmem_cache_flags(s->size, flags, s->name, s->ctor);
@@ -4167,8 +4440,21 @@ static struct kmem_cache * __init bootstrap(struct kmem_cache *static_cache)
 	return s;
 }
 
+/*
+ * called by:
+ *   - init/main.c|503| <<mm_init>> kmem_cache_init();
+ */
 void __init kmem_cache_init(void)
 {
+	/*
+	 * 在以下使用boot_kmem_cache:
+	 *   - mm/slub.c|4217| <<kmem_cache_init>> kmem_cache = &boot_kmem_cache;
+	 *   - mm/slub.c|4232| <<kmem_cache_init>> kmem_cache = bootstrap(&boot_kmem_cache);
+	 *
+	 * 在以下使用boot_kmem_cache_node:
+	 *   - mm/slub.c|4216| <<kmem_cache_init>> kmem_cache_node = &boot_kmem_cache_node;
+	 *   - mm/slub.c|4239| <<kmem_cache_init>> kmem_cache_node = bootstrap(&boot_kmem_cache_node);
+	 */
 	static __initdata struct kmem_cache boot_kmem_cache,
 		boot_kmem_cache_node;
 
@@ -4252,6 +4538,11 @@ __kmem_cache_alias(const char *name, size_t size, size_t align,
 	return s;
 }
 
+/*
+ * called by:
+ *   - mm/slab_common.c|390| <<create_cache>> err = __kmem_cache_create(s, flags);
+ *   - mm/slab_common.c|896| <<create_boot_cache>> err = __kmem_cache_create(s, flags);
+ */
 int __kmem_cache_create(struct kmem_cache *s, unsigned long flags)
 {
 	int err;
@@ -5664,6 +5955,11 @@ static void sysfs_slab_remove_workfn(struct work_struct *work)
 	kobject_put(&s->kobj);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4268| <<__kmem_cache_create>> err = sysfs_slab_add(s);
+ *   - mm/slub.c|5811| <<slab_sysfs_init>> err = sysfs_slab_add(s);
+ */
 static int sysfs_slab_add(struct kmem_cache *s)
 {
 	int err;
@@ -5837,6 +6133,11 @@ __initcall(slab_sysfs_init);
  * The /proc/slabinfo ABI
  */
 #ifdef CONFIG_SLABINFO
+/*
+ * called by:
+ *   - mm/slab_common.c|1247| <<memcg_accumulate_slabinfo>> get_slabinfo(c, &sinfo);
+ *   - mm/slab_common.c|1262| <<cache_show>> get_slabinfo(s, &sinfo);
+ */
 void get_slabinfo(struct kmem_cache *s, struct slabinfo *sinfo)
 {
 	unsigned long nr_slabs = 0;
-- 
2.7.4

