[PATCH 0/5] KVM: arm64: Accelerate lookup of vcpus by MPIDR values

... from Marc Zyngier <maz@kernel.org> ...

https://lore.kernel.org/all/20230907100931.1186690-1-maz@kernel.org/

--------------------------

当VM的cpu特别多的时候, SGI (ipi)性能会有问题.

因为寻找index比较大的vcpu比较慢: line 1097 and 1111.

Xu Zhao recently reported[1] that sending SGIs on large VMs was slower
than expected, specially if targeting vcpus that have a high vcpu
index. They root-caused it to the way we walk the vcpu xarray in the
search of the correct MPIDR, one vcpu at a time, which is of course
grossly inefficient.


1073 void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg, bool allow_group1)
1074 {
1075         struct kvm *kvm = vcpu->kvm;
1076         struct kvm_vcpu *c_vcpu;
1077         u16 target_cpus;
1078         u64 mpidr;
1079         int sgi;
1080         int vcpu_id = vcpu->vcpu_id;
1081         bool broadcast;
1082         unsigned long c, flags;
1083 
1084         sgi = (reg & ICC_SGI1R_SGI_ID_MASK) >> ICC_SGI1R_SGI_ID_SHIFT;
1085         broadcast = reg & BIT_ULL(ICC_SGI1R_IRQ_ROUTING_MODE_BIT);
1086         target_cpus = (reg & ICC_SGI1R_TARGET_LIST_MASK) >> ICC_SGI1R_TARGET_LIST_SHIFT;
1087         mpidr = SGI_AFFINITY_LEVEL(reg, 3);
1088         mpidr |= SGI_AFFINITY_LEVEL(reg, 2);
1089         mpidr |= SGI_AFFINITY_LEVEL(reg, 1);
1090 
1091         /*
1092          * We iterate over all VCPUs to find the MPIDRs matching the request.
1093          * If we have handled one CPU, we clear its bit to detect early
1094          * if we are already finished. This avoids iterating through all
1095          * VCPUs when most of the times we just signal a single VCPU.
1096          */
1097         kvm_for_each_vcpu(c, c_vcpu, kvm) {
1098                 struct vgic_irq *irq;
1099         
1100                 /* Exit early if we have dealt with all requested CPUs */
1101                 if (!broadcast && target_cpus == 0)
1102                         break;
1103              
1104                 /* Don't signal the calling VCPU */
1105                 if (broadcast && c == vcpu_id)
1106                         continue;
1107                                 
1108                 if (!broadcast) {
1109                         int level0;
1110           
1111                         level0 = match_mpidr(mpidr, target_cpus, c_vcpu);
1112                         if (level0 == -1)
1113                                 continue;
1114         
1115                         /* remove this matching VCPU from the mask */
1116                         target_cpus &= ~BIT(level0);
1117                 }
1118 
1119                 irq = vgic_get_irq(vcpu->kvm, c_vcpu, sgi);
1120 
1121                 raw_spin_lock_irqsave(&irq->irq_lock, flags);

bugfix的核心是使用hash table.

Performance wise, this is very significant: using the KUT micro-bench
test with the following patch (always IPI-ing the last vcpu of the VM)
and running it with large number of vcpus shows a large improvement
(from 3832ns to 2593ns for a 64 vcpu VM, a 32% reduction, measured on
an Ampere Altra). I expect that IPI-happy workloads could benefit from
this.


下面是kvm-unit-test测试的patch.

diff --git a/arm/micro-bench.c b/arm/micro-bench.c
index bfd181dc..f3ac3270 100644
--- a/arm/micro-bench.c
+++ b/arm/micro-bench.c
@@ -88,7 +88,7 @@ static bool test_init(void)

 	irq_ready = false;
 	gic_enable_defaults();
-	on_cpu_async(1, gic_secondary_entry, NULL);
+	on_cpu_async(nr_cpus - 1, gic_secondary_entry, NULL);

 	cntfrq = get_cntfrq();
 	printf("Timer Frequency %d Hz (Output in microseconds)\n", cntfrq);
@@ -157,7 +157,7 @@ static void ipi_exec(void)

 	irq_received = false;

-	gic_ipi_send_single(1, 1);
+	gic_ipi_send_single(1, nr_cpus - 1);

 	while (!irq_received && tries--)
 		cpu_relax();
