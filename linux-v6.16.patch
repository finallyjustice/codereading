From 50b7779d1dc73fc87a5e2b179ca9372f354cdcf1 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 28 Sep 2025 11:00:43 -0700
Subject: [PATCH 1/1] linux-v6.16

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/kvm/arch_timer.c                   |   46 +
 arch/arm64/kvm/hyp/vhe/timer-sr.c             |    4 +
 arch/arm64/kvm/mmu.c                          |    4 +
 arch/arm64/kvm/vgic/vgic-its.c                |   10 +
 arch/arm64/kvm/vgic/vgic.h                    |   12 +
 arch/x86/include/asm/kvm_host.h               |  196 ++++
 arch/x86/include/asm/nospec-branch.h          |   29 +
 arch/x86/kernel/alternative.c                 |   58 +
 arch/x86/kernel/cpu/bugs.c                    |  548 +++++++++
 arch/x86/kernel/cpu/common.c                  |   10 +
 arch/x86/kernel/pvclock.c                     |    5 +
 arch/x86/kvm/cpuid.c                          |   18 +
 arch/x86/kvm/hyperv.c                         |  134 +++
 arch/x86/kvm/lapic.c                          |   13 +
 arch/x86/kvm/x86.c                            | 1041 +++++++++++++++++
 arch/x86/kvm/xen.c                            |   12 +
 arch/x86/net/bpf_jit_comp.c                   |   33 +
 drivers/infiniband/core/device.c              |   23 +
 drivers/nvme/host/fabrics.c                   |    7 +
 drivers/nvme/host/fc.c                        |    7 +
 drivers/nvme/host/rdma.c                      |   53 +
 drivers/nvme/host/tcp.c                       |    7 +
 drivers/nvme/target/loop.c                    |    7 +
 drivers/nvme/target/rdma.c                    |   23 +
 drivers/ptp/ptp_vmclock.c                     |   22 +
 drivers/target/target_core_iblock.c           |   84 ++
 drivers/target/target_core_transport.c        |    7 +
 include/kvm/arm_arch_timer.h                  |   28 +
 include/kvm/arm_vgic.h                        |    6 +
 include/linux/kvm_host.h                      |   98 ++
 include/linux/seqlock.h                       |    5 +-
 include/linux/timekeeper_internal.h           |   10 +
 include/rdma/ib_verbs.h                       |   47 +
 kernel/sched/cputime.c                        |    6 +
 kernel/time/timekeeping.c                     |    4 +
 net/rds/ib.c                                  |   23 +
 tools/arch/x86/include/asm/pvclock.h          |    3 +
 .../selftests/kvm/x86/kvm_clock_test.c        |   23 +
 virt/kvm/dirty_ring.c                         |   25 +
 virt/kvm/kvm_main.c                           |  178 +++
 virt/kvm/pfncache.c                           |    5 +
 41 files changed, 2873 insertions(+), 1 deletion(-)

diff --git a/arch/arm64/kvm/arch_timer.c b/arch/arm64/kvm/arch_timer.c
index 701ea10a6..b64695f53 100644
--- a/arch/arm64/kvm/arch_timer.c
+++ b/arch/arm64/kvm/arch_timer.c
@@ -485,6 +485,12 @@ static void timer_emulate(struct arch_timer_context *ctx)
 	soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
 }
 
+/*
+ * 在以下使用set_cntvoff():
+ *   - arch/arm64/kvm/arch_timer.c|545| <<timer_save_state>> set_cntvoff(0);
+ *   - arch/arm64/kvm/arch_timer.c|632| <<timer_restore_state>> set_cntvoff(0);
+ *   - arch/arm64/kvm/arch_timer.c|635| <<timer_restore_state>> set_cntvoff(offset);
+ */
 static void set_cntvoff(u64 cntvoff)
 {
 	kvm_call_hyp(__kvm_timer_set_cntvoff, cntvoff);
@@ -542,6 +548,12 @@ static void timer_save_state(struct arch_timer_context *ctx)
 		 * Do it unconditionally, as this is either unavoidable
 		 * or dirt cheap.
 		 */
+		/*
+		 * 在以下使用set_cntvoff():
+		 *   - arch/arm64/kvm/arch_timer.c|545| <<timer_save_state>> set_cntvoff(0);
+		 *   - arch/arm64/kvm/arch_timer.c|632| <<timer_restore_state>> set_cntvoff(0);
+		 *   - arch/arm64/kvm/arch_timer.c|635| <<timer_restore_state>> set_cntvoff(offset);
+		 */
 		set_cntvoff(0);
 		break;
 	case TIMER_PTIMER:
@@ -607,6 +619,13 @@ static void kvm_timer_unblocking(struct kvm_vcpu *vcpu)
 	soft_timer_cancel(&timer->bg_timer);
 }
 
+/*
+ * 在以下使用timer_restore_state():
+ *   - arch/arm64/kvm/arch_timer.c|888| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|890| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1262| <<kvm_arm_timer_read_sysreg>> timer_restore_state(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1313| <<kvm_arm_timer_write_sysreg>> timer_restore_state(timer);
+ */
 static void timer_restore_state(struct arch_timer_context *ctx)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(ctx->vcpu);
@@ -632,6 +651,12 @@ static void timer_restore_state(struct arch_timer_context *ctx)
 			set_cntvoff(0);
 			cval += offset;
 		} else {
+			/*
+			 * 在以下使用set_cntvoff():
+			 *   - arch/arm64/kvm/arch_timer.c|545| <<timer_save_state>> set_cntvoff(0);
+			 *   - arch/arm64/kvm/arch_timer.c|632| <<timer_restore_state>> set_cntvoff(0);
+			 *   - arch/arm64/kvm/arch_timer.c|635| <<timer_restore_state>> set_cntvoff(offset);
+			 */
 			set_cntvoff(offset);
 		}
 		write_sysreg_el0(cval, SYS_CNTV_CVAL);
@@ -885,6 +910,13 @@ void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 
 	kvm_timer_unblocking(vcpu);
 
+	/*
+	 * 在以下使用timer_restore_state():
+	 *   - arch/arm64/kvm/arch_timer.c|888| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_vtimer);
+	 *   - arch/arm64/kvm/arch_timer.c|890| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_ptimer);
+	 *   - arch/arm64/kvm/arch_timer.c|1262| <<kvm_arm_timer_read_sysreg>> timer_restore_state(timer);
+	 *   - arch/arm64/kvm/arch_timer.c|1313| <<kvm_arm_timer_write_sysreg>> timer_restore_state(timer);
+	 */
 	timer_restore_state(map.direct_vtimer);
 	if (map.direct_ptimer)
 		timer_restore_state(map.direct_ptimer);
@@ -1259,6 +1291,13 @@ u64 kvm_arm_timer_read_sysreg(struct kvm_vcpu *vcpu,
 
 	val = kvm_arm_timer_read(vcpu, timer, treg);
 
+	/*
+	 * 在以下使用timer_restore_state():
+	 *   - arch/arm64/kvm/arch_timer.c|888| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_vtimer);
+	 *   - arch/arm64/kvm/arch_timer.c|890| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_ptimer);
+	 *   - arch/arm64/kvm/arch_timer.c|1262| <<kvm_arm_timer_read_sysreg>> timer_restore_state(timer);
+	 *   - arch/arm64/kvm/arch_timer.c|1313| <<kvm_arm_timer_write_sysreg>> timer_restore_state(timer);
+	 */
 	timer_restore_state(timer);
 	preempt_enable();
 
@@ -1310,6 +1349,13 @@ void kvm_arm_timer_write_sysreg(struct kvm_vcpu *vcpu,
 		preempt_disable();
 		timer_save_state(timer);
 		kvm_arm_timer_write(vcpu, timer, treg, val);
+		/*
+		 * 在以下使用timer_restore_state():
+		 *   - arch/arm64/kvm/arch_timer.c|888| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_vtimer);
+		 *   - arch/arm64/kvm/arch_timer.c|890| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_ptimer);
+		 *   - arch/arm64/kvm/arch_timer.c|1262| <<kvm_arm_timer_read_sysreg>> timer_restore_state(timer);
+		 *   - arch/arm64/kvm/arch_timer.c|1313| <<kvm_arm_timer_write_sysreg>> timer_restore_state(timer);
+		 */
 		timer_restore_state(timer);
 		preempt_enable();
 	}
diff --git a/arch/arm64/kvm/hyp/vhe/timer-sr.c b/arch/arm64/kvm/hyp/vhe/timer-sr.c
index 4cda674a8..1fb70110e 100644
--- a/arch/arm64/kvm/hyp/vhe/timer-sr.c
+++ b/arch/arm64/kvm/hyp/vhe/timer-sr.c
@@ -6,6 +6,10 @@
 
 #include <asm/kvm_hyp.h>
 
+/*
+ * 在以下使用__kvm_timer_set_cntvoff():
+ *   - arch/arm64/kvm/arch_timer.c|490| <<set_cntvoff>> kvm_call_hyp(__kvm_timer_set_cntvoff, cntvoff);
+ */
 void __kvm_timer_set_cntvoff(u64 cntvoff)
 {
 	write_sysreg(cntvoff, cntvoff_el2);
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index 2942ec92c..e3e28e024 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -1470,6 +1470,10 @@ static bool kvm_vma_mte_allowed(struct vm_area_struct *vma)
 	return vma->vm_flags & VM_MTE_ALLOWED;
 }
 
+/*
+ * 在以下使用user_mem_abort():
+ *   - arch/arm64/kvm/mmu.c|1956| <<kvm_handle_guest_abort>> ret = user_mem_abort(vcpu, fault_ipa, nested, memslot, hva,
+ */
 static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 			  struct kvm_s2_trans *nested,
 			  struct kvm_memory_slot *memslot, unsigned long hva,
diff --git a/arch/arm64/kvm/vgic/vgic-its.c b/arch/arm64/kvm/vgic/vgic-its.c
index 534049c7c..c0696796a 100644
--- a/arch/arm64/kvm/vgic/vgic-its.c
+++ b/arch/arm64/kvm/vgic/vgic-its.c
@@ -2713,10 +2713,20 @@ static int vgic_its_ctrl(struct kvm *kvm, struct vgic_its *its, u64 attr)
  * bitmap is used to track the dirty guest pages due to the missed running
  * VCPU in the period.
  */
+/*
+ * 在以下使用kvm_arch_allow_write_without_running_vcpu():
+ *   - virt/kvm/kvm_main.c|3594| <<mark_page_dirty_in_slot>> WARN_ON_ONCE(!vcpu && !kvm_arch_allow_write_without_running_vcpu(kvm));
+ */
 bool kvm_arch_allow_write_without_running_vcpu(struct kvm *kvm)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
 
+	/*
+	 * 在以下使用vgic_dist->table_write_in_progress:
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|2720| <<kvm_arch_allow_write_without_running_vcpu>> return dist->table_write_in_progress;
+	 *   - arch/arm64/kvm/vgic/vgic.h|142| <<vgic_write_guest_lock>> dist->table_write_in_progress = true;
+	 *   - arch/arm64/kvm/vgic/vgic.h|144| <<vgic_write_guest_lock>> dist->table_write_in_progress = false;
+	 */
 	return dist->table_write_in_progress;
 }
 
diff --git a/arch/arm64/kvm/vgic/vgic.h b/arch/arm64/kvm/vgic/vgic.h
index 4349084cb..c36306d2f 100644
--- a/arch/arm64/kvm/vgic/vgic.h
+++ b/arch/arm64/kvm/vgic/vgic.h
@@ -133,12 +133,24 @@ static inline bool vgic_irq_is_multi_sgi(struct vgic_irq *irq)
 	return vgic_irq_get_lr_count(irq) > 1;
 }
 
+/*
+ * 在以下使用vgic_write_guest_lock():
+ *   - arch/arm64/kvm/vgic/vgic-its.c|64| <<vgic_its_write_entry_lock>> __ret = vgic_write_guest_lock(__k, (g), \
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|355| <<vgic_v3_lpi_sync_pending_status>> ret = vgic_write_guest_lock(kvm, ptr, &val, 1);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|451| <<vgic_v3_save_pending_tables>> ret = vgic_write_guest_lock(kvm, ptr, &val, 1);
+ */
 static inline int vgic_write_guest_lock(struct kvm *kvm, gpa_t gpa,
 					const void *data, unsigned long len)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
 	int ret;
 
+	/*
+	 * 在以下使用vgic_dist->table_write_in_progress:
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|2720| <<kvm_arch_allow_write_without_running_vcpu>> return dist->table_write_in_progress;
+	 *   - arch/arm64/kvm/vgic/vgic.h|142| <<vgic_write_guest_lock>> dist->table_write_in_progress = true;
+	 *   - arch/arm64/kvm/vgic/vgic.h|144| <<vgic_write_guest_lock>> dist->table_write_in_progress = false;
+	 */
 	dist->table_write_in_progress = true;
 	ret = kvm_write_guest_lock(kvm, gpa, data, len);
 	dist->table_write_in_progress = false;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f7af967aa..5322e6c18 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -88,6 +88,26 @@
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/cpuid.c|2016| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|3314| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3579| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3826| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3835| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|4314| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5443| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|6148| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10089| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11324| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|11681| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|13189| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *
+ * 处理的函数kvm_guest_time_update()
+ */
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
@@ -930,6 +950,16 @@ struct kvm_vcpu_arch {
 	s8  pvclock_tsc_shift;
 	u32 pvclock_tsc_mul;
 	unsigned int hw_tsc_khz;
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2371| <<kvm_write_system_time>> kvm_gpc_activate(&vcpu->arch.pv_time, system_time & ~1ULL,
+	 *   - arch/x86/kvm/x86.c|2374| <<kvm_write_system_time>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|3704| <<kvm_guest_time_update>> if (vcpu->pv_time.active) {
+	 *   - arch/x86/kvm/x86.c|3723| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|4102| <<kvmclock_reset>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|6275| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|13065| <<kvm_arch_vcpu_create>> kvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm);
+	 */
 	struct gfn_to_pfn_cache pv_time;
 	/* set guest stopped flag in pvclock flags field */
 	bool pvclock_set_guest_stopped_request;
@@ -941,6 +971,27 @@ struct kvm_vcpu_arch {
 		struct gfn_to_hva_cache cache;
 	} st;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/vmx/tdx.c|695| <<tdx_vcpu_create>> vcpu->arch.l1_tsc_offset = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2634| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = l1_offset;
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|718| <<nested_vmcb02_prepare_control>> vcpu->arch.tsc_offset =
+	 *        kvm_calc_nested_tsc_offset(vcpu->arch.l1_tsc_offset, svm->nested.ctl.tsc_offset, svm->tsc_ratio_msr);
+	 *   - arch/x86/kvm/svm/nested.c|1134| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/svm/svm.c|1165| <<svm_write_tsc_offset>> svm->vmcb01.ptr->control.tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2693| <<prepare_vmcs02>> vcpu->arch.tsc_offset =
+	 *        kvm_calc_nested_tsc_offset(vcpu->arch.l1_tsc_offset, vmx_get_l2_tsc_offset(vcpu), vmx_get_l2_tsc_multiplier(vcpu));
+	 *   - arch/x86/kvm/vmx/nested.c|4985| <<__nested_vmx_vmexit>> vcpu->arch.tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2589| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(host_tsc,
+	 *        vcpu->arch.l1_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2631| <<kvm_vcpu_write_tsc_offset>> trace_kvm_write_tsc_offset(vcpu->vcpu_id, vcpu->arch.l1_tsc_offset,
+	 *        l1_offset);
+	 *   - arch/x86/kvm/x86.c|2849| <<adjust_tsc_offset_guest>> u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|4206| <<kvm_set_msr_common(MSR_IA32_TSC)>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|4536| <<kvm_get_msr_common(MSR_IA32_TSC)>> offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|6028| <<kvm_arch_tsc_get_attr(KVM_VCPU_TSC_OFFSET)>> if (put_user(vcpu->arch.l1_tsc_offset, uaddr))
+	 */
 	u64 l1_tsc_offset;
 	u64 tsc_offset; /* current tsc offset */
 	u64 last_guest_tsc;
@@ -949,6 +1000,12 @@ struct kvm_vcpu_arch {
 	u64 this_tsc_nsec;
 	u64 this_tsc_write;
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2432| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3446| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|5272| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;
@@ -956,6 +1013,26 @@ struct kvm_vcpu_arch {
 	u32 virtual_tsc_khz;
 	s64 ia32_tsc_adjust_msr;
 	u64 msr_ia32_power_ctl;
+	/*
+	 * 在以下设置kvm_vcpu_arch->l1_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/vmx/tdx.c|697| <<tdx_vcpu_create>> vcpu->arch.l1_tsc_scaling_ratio = kvm_tdx->tsc_multiplier;
+	 *   - arch/x86/kvm/x86.c|2654| <<kvm_vcpu_write_tsc_multiplier>> vcpu->arch.l1_tsc_scaling_ratio = l1_multiplier;
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/svm/nested.c|1141| <<nested_svm_vmexit>> vcpu->arch.tsc_scaling_ratio != vcpu->arch.l1_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|1142| <<nested_svm_vmexit>> vcpu->arch.tsc_scaling_ratio = vcpu->arch.l1_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/svm/nested.c|1577| <<nested_svm_update_tsc_ratio_msr>> kvm_calc_nested_tsc_multiplier(vcpu->arch.l1_tsc_scaling_ratio,
+	 *   - arch/x86/kvm/vmx/nested.c|2698| <<prepare_vmcs02>> vcpu->arch.l1_tsc_scaling_ratio,
+	 *   - arch/x86/kvm/vmx/nested.c|4987| <<__nested_vmx_vmexit>> vcpu->arch.tsc_scaling_ratio = vcpu->arch.l1_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8168| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/vmx/vmx.c|8171| <<vmx_set_hv_timer>> if...vcpu->arch.l1_tsc_scaling_ratio, &delta_tsc))
+	 *   - arch/x86/kvm/x86.c|2582| <<kvm_compute_l1_tsc_offset>> tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2590| <<kvm_read_l1_tsc>> kvm_scale_tsc(host_tsc, vcpu->arch.l1_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2861| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2864| <<adjust_tsc_offset_host>> adjustment = kvm_scale_tsc((u64) adjustment, vcpu->arch.l1_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|3497| <<kvm_guest_time_update>> tgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz, v->arch.l1_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|4537| <<kvm_get_msr_common(MSR_IA32_TSC)>> ratio = vcpu->arch.l1_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|6074| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>> tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio) + offset;
+	 */
 	u64 l1_tsc_scaling_ratio;
 	u64 tsc_scaling_ratio; /* current scaling ratio */
 
@@ -1162,6 +1239,22 @@ struct kvm_hv {
 	u64 hv_guest_os_id;
 	u64 hv_hypercall;
 	u64 hv_tsc_page;
+	/*
+	 * 在以下使用kvm_hv->hv_tsc_page_status:
+	 *   - arch/x86/kvm/hyperv.c|585| <<get_time_ref_counter>> if (hv->hv_tsc_page_status != HV_TSC_PAGE_SET)
+	 *   - arch/x86/kvm/hyperv.c|1160| <<tsc_page_update_unsafe>> return (hv->hv_tsc_page_status != HV_TSC_PAGE_GUEST_CHANGED) &&
+	 *   - arch/x86/kvm/hyperv.c|1180| <<kvm_hv_setup_tsc_page>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_BROKEN ||
+	 *   - arch/x86/kvm/hyperv.c|1181| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_SET ||
+	 *   - arch/x86/kvm/hyperv.c|1182| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_UNSET)
+	 *   - arch/x86/kvm/hyperv.c|1201| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1237| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1241| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_BROKEN;
+	 *   - arch/x86/kvm/hyperv.c|1252| <<kvm_hv_request_tsc_page_update>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_SET &&
+	 *   - arch/x86/kvm/hyperv.c|1254| <<kvm_hv_request_tsc_page_update>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1446| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_GUEST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1448| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1451| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_UNSET;
+	 */
 	enum hv_tsc_page_status hv_tsc_page_status;
 
 	/* Hyper-v based guest crash (NT kernel bugcheck) parameters */
@@ -1393,6 +1486,19 @@ struct kvm_arch {
 	bool apic_access_memslot_inhibited;
 
 	/* Protects apicv_inhibit_reasons */
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10951| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|11529| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|11554| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|11586| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|11628| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|11630| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|13085| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|13094| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	struct rw_semaphore apicv_update_lock;
 	unsigned long apicv_inhibit_reasons;
 
@@ -1404,30 +1510,120 @@ struct kvm_arch {
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|572| <<kvm_pmu_rdpmc_vmware>> ctr_val = ktime_get_boottime_ns() + vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3279| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3294| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3497| <<kvm_guest_time_update>> hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3611| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7340| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|13130| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	s64 kvmclock_offset;
 
 	/*
 	 * This also protects nr_vcpus_matched_tsc which is read from a
 	 * preemption-disabled region, so it must be a raw spinlock.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2659| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2716| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2772| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3017| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3049| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3068| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock)
+	 *   - arch/x86/kvm/x86.c|5799| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5809| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|12820| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spinlock_t tsc_write_lock;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2776| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2856| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|13575| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	u64 last_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->arch.last_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2777| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+	 *   - arch/x86/kvm/x86.c|2866| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+	 *   - arch/x86/kvm/x86.c|13576| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_write = 0;
+	 */
 	u64 last_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2778| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2892| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|6420| <<kvm_arch_tsc_set_attr>> kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 */
 	u32 last_tsc_khz;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2779| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|6421| <<kvm_arch_tsc_set_attr>> kvm->arch.last_tsc_offset == offset);
+	 */
 	u64 last_tsc_offset;
 	u64 cur_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2826| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_write = tsc;
+	 *   - arch/x86/kvm/x86.c|2836| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 */
 	u64 cur_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2827| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|2935| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+	 */
 	u64 cur_tsc_offset;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2824| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+	 *   - arch/x86/kvm/x86.c|2829| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+	 *   - arch/x86/kvm/x86.c|2834| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 */
 	u64 cur_tsc_generation;
+	/*
+	 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2576| <<kvm_track_tsc_matching>> bool use_master_clock = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2590| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2828| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2830| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|3209| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	int nr_vcpus_matched_tsc;
 
 	u32 default_tsc_khz;
 	bool user_set_tsc;
 	u64 apic_bus_cycle_ns;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_sc:
+	 *   - arch/x86/kvm/x86.c|3050| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3067| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3143| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3145| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3223| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3229| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3359| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3387| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|12822| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc,
+	 *        &kvm->arch.tsc_write_lock);
+	 */
 	seqcount_raw_spinlock_t pvclock_sc;
 	bool use_master_clock;
 	u64 master_kernel_ns;
+	/*
+	 * 在以下使用kvm_arch->master_cycle_now:
+	 *   - arch/x86/kvm/x86.c|3177| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+	 *                                           &ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3450| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3630| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3867| <<kvm_get_wall_clock_epoch>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 */
 	u64 master_cycle_now;
 	struct delayed_work kvmclock_update_work;
 	struct delayed_work kvmclock_sync_work;
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index 10f261678..8d953ccb5 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -328,10 +328,39 @@
 	__CLEAR_CPU_BUFFERS X86_FEATURE_CLEAR_CPU_BUF_VM
 
 #ifdef CONFIG_X86_64
+/*
+ * 使用CLEAR_BRANCH_HISTORY和CLEAR_BRANCH_HISTORY_VMEXIT的地方:
+ *   - arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+ *   - arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+ *   - arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+ *   - arch/x86/entry/entry_64.S:	CLEAR_BRANCH_HISTORY
+ *   - arch/x86/kvm/vmx/vmenter.S:	CLEAR_BRANCH_HISTORY_VMEXIT
+ *
+ * 在以下使用X86_FEATURE_CLEAR_BHB_LOOP:
+ *   - arch/x86/include/asm/nospec-branch.h|332| <<CLEAR_BRANCH_HISTORY>> ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
+ *   - arch/x86/kernel/cpu/bugs.c|2141| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+ *   - arch/x86/kernel/cpu/bugs.c|3307| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_LOOP))
+ *   - arch/x86/net/bpf_jit_comp.c|1516| <<emit_spectre_bhb_barrier>> if (cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP)) {
+ *   - arch/x86/net/bpf_jit_comp.c|1531| <<emit_spectre_bhb_barrier>> if ((cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP) &&
+ */
 .macro CLEAR_BRANCH_HISTORY
 	ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
 .endm
 
+/*
+ * 使用CLEAR_BRANCH_HISTORY和CLEAR_BRANCH_HISTORY_VMEXIT的地方:
+ *   - arch/x86/entry/entry_64_compat.S:        CLEAR_BRANCH_HISTORY
+ *   - arch/x86/entry/entry_64_compat.S:        CLEAR_BRANCH_HISTORY
+ *   - arch/x86/entry/entry_64_compat.S:        CLEAR_BRANCH_HISTORY
+ *   - arch/x86/entry/entry_64.S:       CLEAR_BRANCH_HISTORY
+ *   - arch/x86/kvm/vmx/vmenter.S:      CLEAR_BRANCH_HISTORY_VMEXIT
+ *
+ * 在以下使用X86_FEATURE_CLEAR_BHB_VMEXIT:
+ *   - arch/x86/include/asm/nospec-branch.h|336| <<CLEAR_BRANCH_HISTORY_VMEXIT>> ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_VMEXIT
+ *   - arch/x86/kernel/cpu/bugs.c|2136| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_VMEXIT);
+ *   - arch/x86/kernel/cpu/bugs.c|2142| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_VMEXIT);
+ *   - arch/x86/kernel/cpu/bugs.c|3313| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_VMEXIT))
+ */
 .macro CLEAR_BRANCH_HISTORY_VMEXIT
 	ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_VMEXIT
 .endm
diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c
index ea1d98416..d188f2bb1 100644
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@ -852,6 +852,20 @@ static bool cpu_wants_indirect_its_thunk_at(unsigned long addr, int reg)
  *
  * It also tries to inline spectre_v2=retpoline,lfence when size permits.
  */
+/*
+ * 在以下调用patch_retpoline():
+ *   - arch/x86/kernel/alternative.c|993| <<apply_retpolines>> len = patch_retpoline(addr, &insn, bytes);
+ *
+ * start_kernel()
+ * -> arch_cpu_finalize_init()
+ *    -> cpu_select_mitigations()
+ *    -> alternative_instructions()
+ *       -> __apply_fineibt(__retpoline_sites, __retpoline_sites_end,
+ *                          __cfi_sites, __cfi_sites_end, true);
+ *       -> apply_retpolines(__retpoline_sites, __retpoline_sites_end);
+ *          -> loop: patch_retpoline()
+ *       -> apply_returns(__return_sites, __return_sites_end);
+ */
 static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 {
 	retpoline_thunk_t *target;
@@ -867,6 +881,18 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
 	BUG_ON(reg == 4);
 
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE_LFENCE:
+	 *   - arch/x86/include/asm/nospec-branch.h|514| <<CALL_NOSPEC(32位)>> X86_FEATURE_RETPOLINE_LFENCE)
+	 *   - arch/x86/kernel/alternative.c|871| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *                     !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 *   - arch/x86/kernel/alternative.c|907| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2384| <<bhi_apply_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *                     !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2556| <<spectre_v2_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE_LFENCE);
+	 *   - arch/x86/kernel/cpu/bugs.c|3685| <<spectre_bhi_state>> !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|669| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
 	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
 		if (cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))
@@ -901,6 +927,18 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 		op = JMP32_INSN_OPCODE;
 	}
 
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE_LFENCE:
+	 *   - arch/x86/include/asm/nospec-branch.h|514| <<CALL_NOSPEC(32位)>> X86_FEATURE_RETPOLINE_LFENCE)
+	 *   - arch/x86/kernel/alternative.c|871| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *                     !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 *   - arch/x86/kernel/alternative.c|907| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2384| <<bhi_apply_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *                     !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2556| <<spectre_v2_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE_LFENCE);
+	 *   - arch/x86/kernel/cpu/bugs.c|3685| <<spectre_bhi_state>> !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|669| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 */
 	/*
 	 * For RETPOLINE_LFENCE: prepend the indirect CALL/JMP with an LFENCE.
 	 */
@@ -942,6 +980,22 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 /*
  * Generated by 'objtool --retpoline'.
  */
+/*
+ * start_kernel()
+ * -> arch_cpu_finalize_init()
+ *    -> cpu_select_mitigations()
+ *    -> alternative_instructions()
+ *       -> __apply_fineibt(__retpoline_sites, __retpoline_sites_end,
+ *                          __cfi_sites, __cfi_sites_end, true);
+ *       -> apply_retpolines(__retpoline_sites, __retpoline_sites_end);
+ *          -> loop: patch_retpoline()
+ *       -> apply_returns(__return_sites, __return_sites_end);
+ *
+ *
+ * 在以下使用:
+ *   - arch/x86/kernel/alternative.c|2367| <<alternative_instructions>> apply_retpolines(__retpoline_sites, __retpoline_sites_end);
+ *   - arch/x86/kernel/module.c|289| <<module_finalize>> apply_retpolines(rseg, rseg + retpolines->sh_size);
+ */
 void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
 {
 	s32 *s;
@@ -2324,6 +2378,10 @@ static noinline void __init alt_reloc_selftest(void)
 	);
 }
 
+/*
+ * 在以下使用alternative_instructions():
+ *   - arch/x86/kernel/cpu/common.c|2553| <<arch_cpu_finalize_init>> alternative_instructions();
+ */
 void __init alternative_instructions(void)
 {
 	u64 ibt;
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index f4d3abb12..683f7b94f 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -34,6 +34,61 @@
 
 #include "cpu.h"
 
+/*
+ * # SpectreV2原理:
+ *
+ * 利用CPU的分支目标预测器(BTB, Branch Target Buffer).
+ * 攻击者训练BTB,使得受害者进程中间接跳转(如 jmp *%rax)在投机执行阶
+ * 段跳到攻击者准备的gadget.
+ * 虽然真正跳转时CPU会发现错了,但投机执行期间的数据访问可泄露敏感
+ * 信息(通过cache timing等方式).
+ *
+ * # Retpoline的防御原理:
+ *
+ * 通过ret, 彻底避开了间接跳转预测的参与,使BTB被"绕过".
+ * BTB不再参与间接跳转的预测过程.
+ * CPU对ret指令的预测只能依赖于RSB(Return Stack Buffer),而RSB是按
+ * 调用-返回对维护的,攻击者很难污染.
+ * 投机跳转只能去trampoline,而非攻击者gadget.
+ * 所以:SpectreV2成立的关键是BTB被污染,Retpoline正好绕开了BTB.
+ *
+ * # 纯Retpoline为什么能防BHI?
+ *
+ * RRSBA是某些CPU的硬件行为特性,在RSB underflow时,不是停下来等待真正
+ * 返回地址,而是fallback到BTB/BHB来猜测返回地址.
+ *
+ * BHI利用的是BHB(Branch History Buffer),记录之前是否taken的分支路径.
+ *
+ * 在CPU 执行分支预测时,不仅仅依赖BTB,还会使用BHB中的历史信息做路径选择.
+ *
+ * 如果CPU的ret执行过程中发生fallback to BTB/BHB(RRSBA),那么BHB被攻击者
+ * 污染就可能影响投机路径.
+ * 在没有RRSBA行为的CPU上, ret 始终使用RSB预测跳转,而不是fallback到BTB/BHB.
+ * 此时, Retpoline构造的ret安全有效,不会受到BHB的污染影响.
+ * 因此在这些 CPU 上,Retpoline间接"顺带"也防住了BHI,即便不是设计目标.
+ * Again, Retpoline是一种"软件绕过硬件预测器"的技巧,不是阻止预测,而是从根本
+ * 不走那条路.
+ *
+ * # 假设CPU没有RRSBA, 为什么retpoline可以防BHI, 同时retpoline + lfence就不可以?
+ *
+ * 因为Retpoline+LFENCE在某些实现中,破坏了RSB的正常工作,使得CPU fallback
+ * 到其他预测路径(如BTB/BHB),从而导致BHI重新成为可能.
+ * 在没有RRSBA的CPU上,Retpoline单独使用时能保证RSB是有效的,所以防住了BHI.
+ * 加了LFENCE后,打乱了CPU对ret指令的正常处理方式(例如pipeline flush),
+ * 使得RSB不再被正确使用,可能导致fallback,反而引入了攻击面.
+ *
+ *
+ * start_kernel()
+ * -> arch_cpu_finalize_init()
+ *    -> cpu_select_mitigations()
+ *    -> alternative_instructions()
+ *       -> __apply_fineibt(__retpoline_sites, __retpoline_sites_end,
+ *                          __cfi_sites, __cfi_sites_end, true);
+ *       -> apply_retpolines(__retpoline_sites, __retpoline_sites_end);
+ *          -> loop: patch_retpoline()
+ *       -> apply_returns(__return_sites, __return_sites_end);
+ */
+
 /*
  * Speculation Vulnerability Handling
  *
@@ -97,10 +152,42 @@ static void __init its_apply_mitigation(void);
 static void __init tsa_select_mitigation(void);
 static void __init tsa_apply_mitigation(void);
 
+/*
+ * 在以下使用x86_spec_ctrl_base:
+ *   - arch/x86/kernel/cpu/bugs.c|201| <<cpu_select_mitigations>> rdmsrq(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|208| <<cpu_select_mitigations>> x86_spec_ctrl_base &= ~SPEC_CTRL_MITIGATIONS_MASK;
+ *   - arch/x86/kernel/cpu/bugs.c|1996| <<spec_ctrl_disable_kernel_rrsba>> x86_spec_ctrl_base |= SPEC_CTRL_RRSBA_DIS_S;
+ *   - arch/x86/kernel/cpu/bugs.c|1997| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2060| <<spec_ctrl_bhi_dis>> x86_spec_ctrl_base |= SPEC_CTRL_BHI_DIS_S;
+ *   - arch/x86/kernel/cpu/bugs.c|2061| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2225| <<spectre_v2_apply_mitigation>> x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
+ *   - arch/x86/kernel/cpu/bugs.c|2226| <<spectre_v2_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2296| <<update_stibp_msr>> u64 val = spec_ctrl_current() | (x86_spec_ctrl_base & SPEC_CTRL_STIBP);
+ *   - arch/x86/kernel/cpu/bugs.c|2303| <<update_stibp_strict>> u64 mask = x86_spec_ctrl_base & ~SPEC_CTRL_STIBP;
+ *   - arch/x86/kernel/cpu/bugs.c|2308| <<update_stibp_strict>> if (mask == x86_spec_ctrl_base)
+ *   - arch/x86/kernel/cpu/bugs.c|2313| <<update_stibp_strict>> x86_spec_ctrl_base = mask;
+ *   - arch/x86/kernel/cpu/bugs.c|2557| <<ssb_apply_mitigation>> x86_spec_ctrl_base |= SPEC_CTRL_SSBD;
+ *   - arch/x86/kernel/cpu/bugs.c|2558| <<ssb_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2802| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/process.c|624| <<__speculation_ctrl_update>> u64 msr = x86_spec_ctrl_base;
+ */
 /* The base value of the SPEC_CTRL MSR without task-specific bits set */
 u64 x86_spec_ctrl_base;
 EXPORT_SYMBOL_GPL(x86_spec_ctrl_base);
 
+/*
+ * 在以下使用x86_spec_ctrl_current:
+ *   - arch/x86/entry/calling.h|323| <<IBRS_ENTER>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+ *   - arch/x86/entry/calling.h|343| <<IBRS_EXIT>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+ *   - arch/x86/include/asm/nospec-branch.h|542| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+ *   - arch/x86/kernel/cpu/bugs.c|105| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+ *   - arch/x86/include/asm/spec-ctrl.h|86| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|127| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|137| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+ *   - arch/x86/kernel/cpu/bugs.c|140| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|152| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+ *   - arch/x86/kvm/vmx/vmx.c|7214| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+ */
 /* The current value of the SPEC_CTRL MSR with task-specific bits set */
 DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
 EXPORT_PER_CPU_SYMBOL_GPL(x86_spec_ctrl_current);
@@ -121,9 +208,34 @@ static void __init set_return_thunk(void *thunk)
 	x86_return_thunk = thunk;
 }
 
+/*
+ * 在以下调用update_spec_ctrl():
+ *   - arch/x86/kernel/cpu/bugs.c|1997| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2061| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2226| <<spectre_v2_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2297| <<update_stibp_msr>> update_spec_ctrl(val);
+ *   - arch/x86/kernel/cpu/bugs.c|2558| <<ssb_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2802| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+ *
+ * this_cpu_write(x86_spec_ctrl_current, val);
+ * wrmsrq(MSR_IA32_SPEC_CTRL, val);
+ */
 /* Update SPEC_CTRL MSR and its cached copy unconditionally */
 static void update_spec_ctrl(u64 val)
 {
+	/*
+	 * 在以下使用x86_spec_ctrl_current:
+	 *   - arch/x86/entry/calling.h|323| <<IBRS_ENTER>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/entry/calling.h|343| <<IBRS_EXIT>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/include/asm/nospec-branch.h|542| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|105| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/include/asm/spec-ctrl.h|86| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|127| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|137| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+	 *   - arch/x86/kernel/cpu/bugs.c|140| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|152| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+	 *   - arch/x86/kvm/vmx/vmx.c|7214| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+	 */
 	this_cpu_write(x86_spec_ctrl_current, val);
 	wrmsrq(MSR_IA32_SPEC_CTRL, val);
 }
@@ -1883,6 +1995,10 @@ static void __init spec_v2_print_cond(const char *reason, bool secure)
 		pr_info("%s selected on command line.\n", reason);
 }
 
+/*
+ * 在以下使用spectre_v2_parse_cmdline():
+ *   - arch/x86/kernel/cpu/bugs.c|2443| <<spectre_v2_select_mitigation>> spectre_v2_cmd = spectre_v2_parse_cmdline();
+ */
 static enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)
 {
 	enum spectre_v2_mitigation_cmd cmd;
@@ -1977,24 +2093,87 @@ static enum spectre_v2_mitigation __init spectre_v2_select_retpoline(void)
 	return SPECTRE_V2_RETPOLINE;
 }
 
+/*
+ * 在以下使用rrsba_disabled:
+ *   - arch/x86/kernel/cpu/bugs.c|2042| <<spec_ctrl_disable_kernel_rrsba>> if (rrsba_disabled)
+ *   - arch/x86/kernel/cpu/bugs.c|2046| <<spec_ctrl_disable_kernel_rrsba>> rrsba_disabled = true;
+ *   - arch/x86/kernel/cpu/bugs.c|2067| <<spec_ctrl_disable_kernel_rrsba>> rrsba_disabled = true;
+ *   - arch/x86/kernel/cpu/bugs.c|2370| <<bhi_apply_mitigation>> if (rrsba_disabled)
+ *   - arch/x86/kernel/cpu/bugs.c|3659| <<spectre_bhi_state>> rrsba_disabled)
+ */
 static bool __ro_after_init rrsba_disabled;
 
+/*
+ * 在以下使用spec_ctrl_disable_kernel_rrsba():
+ *   - arch/x86/kernel/cpu/bugs.c|2385| <<bhi_apply_mitigation>> spec_ctrl_disable_kernel_rrsba();
+ *   - arch/x86/kernel/cpu/bugs.c|2573| <<spectre_v2_apply_mitigation>> spec_ctrl_disable_kernel_rrsba();
+ */
 /* Disable in-kernel use of non-RSB RET predictors */
 static void __init spec_ctrl_disable_kernel_rrsba(void)
 {
+	/*
+	 * 在以下使用rrsba_disabled:
+	 *   - arch/x86/kernel/cpu/bugs.c|2042| <<spec_ctrl_disable_kernel_rrsba>> if (rrsba_disabled)
+	 *   - arch/x86/kernel/cpu/bugs.c|2046| <<spec_ctrl_disable_kernel_rrsba>> rrsba_disabled = true;
+	 *   - arch/x86/kernel/cpu/bugs.c|2067| <<spec_ctrl_disable_kernel_rrsba>> rrsba_disabled = true;
+	 *   - arch/x86/kernel/cpu/bugs.c|2370| <<bhi_apply_mitigation>> if (rrsba_disabled)
+	 *   - arch/x86/kernel/cpu/bugs.c|3659| <<spectre_bhi_state>> rrsba_disabled)
+	 *
+	 * 在这里rrsba_disabled是true
+	 */
 	if (rrsba_disabled)
 		return;
 
+	/*
+	 * 在这里rrsba_disabled是true
+	 *
+	 * 注释:
+	 * Indicates RET may use predictors
+	 * other than the RSB. With eIBRS
+	 * enabled predictions in kernel mode
+	 * are restricted to targets 
+	 * kernel.
+	 */
 	if (!(x86_arch_cap_msr & ARCH_CAP_RRSBA)) {
 		rrsba_disabled = true;
 		return;
 	}
 
+	/*
+	 * icelake的结果:
+	 * $ cpuid -1 -l 0x7 -s 2
+         * CPU:
+         *       PSFD: fast store forwarding pred disable = true
+         *       IPRED_CTRL: IBP disable                  = false
+         *       RRSBA_CTRL: IBP bottomless RSB disable   = false
+         *       DDPD_U: data dep prefetcher disable      = false
+         *       BHI_CTRL: IBP BHB-focused disable        = false
+         *       MCDT_NO: MCDT mitigation not needed      = false
+         *       UC-lock disable                          = false
+	 *
+	 *
+	 * 在这里rrsba_disabled不好说!!!
+	 */
 	if (!boot_cpu_has(X86_FEATURE_RRSBA_CTRL))
 		return;
 
 	x86_spec_ctrl_base |= SPEC_CTRL_RRSBA_DIS_S;
+	/*
+	 * 在以下调用update_spec_ctrl():
+	 *   - arch/x86/kernel/cpu/bugs.c|1997| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2061| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2226| <<spectre_v2_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2297| <<update_stibp_msr>> update_spec_ctrl(val);
+	 *   - arch/x86/kernel/cpu/bugs.c|2558| <<ssb_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2802| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *
+	 * this_cpu_write(x86_spec_ctrl_current, val);
+	 * wrmsrq(MSR_IA32_SPEC_CTRL, val);
+	 */
 	update_spec_ctrl(x86_spec_ctrl_base);
+	/*
+	 * 在这里rrsba_disabled是true
+	 */
 	rrsba_disabled = true;
 }
 
@@ -2052,13 +2231,46 @@ static void __init spectre_v2_select_rsb_mitigation(enum spectre_v2_mitigation m
  * Set BHI_DIS_S to prevent indirect branches in kernel to be influenced by
  * branch history in userspace. Not needed if BHI_NO is set.
  */
+/*
+ * 在以下使用spec_ctrl_bhi_dis():
+ *   - arch/x86/kernel/cpu/bugs.c|2131| <<bhi_apply_mitigation>> if (spec_ctrl_bhi_dis())
+ */
 static bool __init spec_ctrl_bhi_dis(void)
 {
+	/*
+	 * $ cpuid -1 -l 0x7 -s 2
+         * CPU:
+         *       PSFD: fast store forwarding pred disable = true
+         *       IPRED_CTRL: IBP disable                  = false
+         *       RRSBA_CTRL: IBP bottomless RSB disable   = false
+         *       DDPD_U: data dep prefetcher disable      = false
+         *       BHI_CTRL: IBP BHB-focused disable        = false
+         *       MCDT_NO: MCDT mitigation not needed      = false
+         *       UC-lock disable                          = false
+	 */
 	if (!boot_cpu_has(X86_FEATURE_BHI_CTRL))
 		return false;
 
 	x86_spec_ctrl_base |= SPEC_CTRL_BHI_DIS_S;
+	/*
+	 * 在以下调用update_spec_ctrl():
+	 *   - arch/x86/kernel/cpu/bugs.c|1997| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2061| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2226| <<spectre_v2_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2297| <<update_stibp_msr>> update_spec_ctrl(val);
+	 *   - arch/x86/kernel/cpu/bugs.c|2558| <<ssb_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2802| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *
+	 * this_cpu_write(x86_spec_ctrl_current, val);
+	 * wrmsrq(MSR_IA32_SPEC_CTRL, val);
+	 */
 	update_spec_ctrl(x86_spec_ctrl_base);
+	/*
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_HW:
+	 *   - arch/x86/kernel/cpu/bugs.c|2062| <<spec_ctrl_bhi_dis>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_HW);
+	 *   - arch/x86/kernel/cpu/bugs.c|3305| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_HW))
+	 *   - arch/x86/net/bpf_jit_comp.c|1533| <<emit_spectre_bhb_barrier>> cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_HW)) {
+	 */
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_HW);
 
 	return true;
@@ -2071,9 +2283,25 @@ enum bhi_mitigations {
 	BHI_MITIGATION_VMEXIT_ONLY,
 };
 
+/*
+ * 在以下使用bhi_mitigation:
+ *   - arch/x86/kernel/cpu/bugs.c|2158| <<spectre_bhi_parse_cmdline>> bhi_mitigation = BHI_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|2160| <<spectre_bhi_parse_cmdline>> bhi_mitigation = BHI_MITIGATION_ON;
+ *   - arch/x86/kernel/cpu/bugs.c|2162| <<spectre_bhi_parse_cmdline>> bhi_mitigation = BHI_MITIGATION_VMEXIT_ONLY;
+ *   - arch/x86/kernel/cpu/bugs.c|2179| <<bhi_select_mitigation>> bhi_mitigation = BHI_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|2181| <<bhi_select_mitigation>> if (bhi_mitigation == BHI_MITIGATION_AUTO)
+ *   - arch/x86/kernel/cpu/bugs.c|2182| <<bhi_select_mitigation>> bhi_mitigation = BHI_MITIGATION_ON;
+ *   - arch/x86/kernel/cpu/bugs.c|2188| <<bhi_update_mitigation>> bhi_mitigation = BHI_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|2192| <<bhi_update_mitigation>> bhi_mitigation = BHI_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|2197| <<bhi_apply_mitigation>> if (bhi_mitigation == BHI_MITIGATION_OFF)
+ *   - arch/x86/kernel/cpu/bugs.c|2218| <<bhi_apply_mitigation>> if (bhi_mitigation == BHI_MITIGATION_VMEXIT_ONLY) {
+ */
 static enum bhi_mitigations bhi_mitigation __ro_after_init =
 	IS_ENABLED(CONFIG_MITIGATION_SPECTRE_BHI) ? BHI_MITIGATION_AUTO : BHI_MITIGATION_OFF;
 
+/*
+ * 处理"spectre_bhi"
+ */
 static int __init spectre_bhi_parse_cmdline(char *str)
 {
 	if (!str)
@@ -2092,8 +2320,30 @@ static int __init spectre_bhi_parse_cmdline(char *str)
 }
 early_param("spectre_bhi", spectre_bhi_parse_cmdline);
 
+/*
+ * 在以下使用bhi_select_mitigation():
+ *   - arch/x86/kernel/cpu/bugs.c|286| <<cpu_select_mitigations>> bhi_select_mitigation();
+ */
 static void __init bhi_select_mitigation(void)
 {
+	/*
+	 * 在以下使用bhi_mitigation:
+	 *   - arch/x86/kernel/cpu/bugs.c|2158| <<spectre_bhi_parse_cmdline>> bhi_mitigation = BHI_MITIGATION_OFF;
+	 *   - arch/x86/kernel/cpu/bugs.c|2160| <<spectre_bhi_parse_cmdline>> bhi_mitigation = BHI_MITIGATION_ON;
+	 *   - arch/x86/kernel/cpu/bugs.c|2162| <<spectre_bhi_parse_cmdline>> bhi_mitigation = BHI_MITIGATION_VMEXIT_ONLY;
+	 *   - arch/x86/kernel/cpu/bugs.c|2179| <<bhi_select_mitigation>> bhi_mitigation = BHI_MITIGATION_OFF;
+	 *   - arch/x86/kernel/cpu/bugs.c|2181| <<bhi_select_mitigation>> if (bhi_mitigation == BHI_MITIGATION_AUTO)
+	 *   - arch/x86/kernel/cpu/bugs.c|2182| <<bhi_select_mitigation>> bhi_mitigation = BHI_MITIGATION_ON;
+	 *   - arch/x86/kernel/cpu/bugs.c|2188| <<bhi_update_mitigation>> bhi_mitigation = BHI_MITIGATION_OFF;
+	 *   - arch/x86/kernel/cpu/bugs.c|2192| <<bhi_update_mitigation>> bhi_mitigation = BHI_MITIGATION_OFF;
+	 *   - arch/x86/kernel/cpu/bugs.c|2197| <<bhi_apply_mitigation>> if (bhi_mitigation == BHI_MITIGATION_OFF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2218| <<bhi_apply_mitigation>> if (bhi_mitigation == BHI_MITIGATION_VMEXIT_ONLY) {
+	 *
+	 * 在以下使用X86_BUG_BHI:
+	 *   - arch/x86/kernel/cpu/bugs.c|2097| <<bhi_select_mitigation>> if (!boot_cpu_has(X86_BUG_BHI) || cpu_mitigations_off())
+	 *   - arch/x86/kernel/cpu/bugs.c|3303| <<spectre_bhi_state>> if (!boot_cpu_has_bug(X86_BUG_BHI))
+	 *   - arch/x86/kernel/cpu/common.c|1524| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_BHI);
+	 */
 	if (!boot_cpu_has(X86_BUG_BHI) || cpu_mitigations_off())
 		bhi_mitigation = BHI_MITIGATION_OFF;
 
@@ -2101,6 +2351,14 @@ static void __init bhi_select_mitigation(void)
 		bhi_mitigation = BHI_MITIGATION_ON;
 }
 
+/*
+ * 在以下调用bhi_update_mitigation():
+ *   - arch/x86/kernel/cpu/bugs.c|316| <<cpu_select_mitigations>> bhi_update_mitigation();
+ *
+ * cpu_select_mitigations()
+ * -> bhi_update_mitigation()
+ * -> bhi_apply_mitigation()
+ */
 static void __init bhi_update_mitigation(void)
 {
 	if (spectre_v2_cmd == SPECTRE_V2_CMD_NONE)
@@ -2111,15 +2369,192 @@ static void __init bhi_update_mitigation(void)
 		bhi_mitigation = BHI_MITIGATION_OFF;
 }
 
+/*
+ * commit 0cd01ac5dcb1e18eb18df0f0d05b5de76522a437
+ * Author: Josh Poimboeuf <jpoimboe@kernel.org>
+ * Date:   Fri Apr 5 11:14:13 2024 -0700
+ *
+ * x86/bugs: Change commas to semicolons in 'spectre_v2' sysfs file
+ *
+ *
+ * commit 7390db8aea0d64e9deb28b8e1ce716f5020c7ee5
+ * Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
+ * Date:   Mon Mar 11 08:56:58 2024 -0700
+ *
+ * x86/bhi: Add support for clearing branch history at syscall entry
+ *
+ *
+ * commit 0f4a837615ff925ba62648d280a861adf1582df7
+ * Author: Daniel Sneddon <daniel.sneddon@linux.intel.com>
+ * Date:   Wed Mar 13 09:47:57 2024 -0700
+ *
+ * x86/bhi: Define SPEC_CTRL_BHI_DIS_S
+ *
+ *
+ * commit be482ff9500999f56093738f9219bbabc729d163
+ * Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
+ * Date:   Mon Mar 11 08:57:03 2024 -0700
+ *
+ * x86/bhi: Enumerate Branch History Injection (BHI) bug
+ *
+ *
+ * commit ec9404e40e8f36421a2b66ecb76dc2209fe7f3ef
+ * Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
+ * Date:   Mon Mar 11 08:57:05 2024 -0700
+ *
+ * x86/bhi: Add BHI mitigation knob
+ *
+ *
+ * commit 95a6ccbdc7199a14b71ad8901cb788ba7fb5167b
+ * Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
+ * Date:   Mon Mar 11 08:57:09 2024 -0700
+ *
+ * x86/bhi: Mitigate KVM by default
+ *
+ *
+ * commit ed2e8d49b54d677f3123668a21a57822d679651f
+ * Author: Daniel Sneddon <daniel.sneddon@linux.intel.com>
+ * Date:   Wed Mar 13 09:49:17 2024 -0700
+ *
+ * KVM: x86: Add BHI_NO
+ *
+ *
+ * -----------------------
+ * commit ec9404e40e8f36421a2b66ecb76dc2209fe7f3ef
+ * Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
+ * Date:   Mon Mar 11 08:57:05 2024 -0700
+ *
+ * x86/bhi: Add BHI mitigation knob
+ *
+ * Branch history clearing software sequences and hardware control
+ * BHI_DIS_S were defined to mitigate Branch History Injection (BHI).
+ *
+ * Add cmdline spectre_bhi={on|off|auto} to control BHI mitigation:
+ *
+ *  auto - Deploy the hardware mitigation BHI_DIS_S, if available.
+ *  on   - Deploy the hardware mitigation BHI_DIS_S, if available,
+ *         otherwise deploy the software sequence at syscall entry and
+ *         VMexit.
+ *  off  - Turn off BHI mitigation.
+ *
+ * The default is auto mode which does not deploy the software sequence
+ * mitigation.  This is because of the hardening done in the syscall
+ * dispatch path, which is the likely target of BHI.
+ *
+ * Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
+ * Signed-off-by: Daniel Sneddon <daniel.sneddon@linux.intel.com>
+ * Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
+ * Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
+ * Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
+ *
+ *
+ * # SpectreV2原理:
+ *
+ * 利用CPU的分支目标预测器(BTB, Branch Target Buffer).
+ * 攻击者训练BTB,使得受害者进程中间接跳转(如 jmp *%rax)在投机执行阶
+ * 段跳到攻击者准备的gadget.
+ * 虽然真正跳转时CPU会发现错了,但投机执行期间的数据访问可泄露敏感
+ * 信息(通过cache timing等方式).
+ *
+ * # Retpoline的防御原理:
+ *
+ * 通过ret, 彻底避开了间接跳转预测的参与,使BTB被"绕过".
+ * BTB不再参与间接跳转的预测过程.
+ * CPU对ret指令的预测只能依赖于RSB(Return Stack Buffer),而RSB是按
+ * 调用-返回对维护的,攻击者很难污染.
+ * 投机跳转只能去trampoline,而非攻击者gadget.
+ * 所以:SpectreV2成立的关键是BTB被污染,Retpoline正好绕开了BTB.
+ *
+ * # 纯Retpoline为什么能防BHI?
+ *
+ * RRSBA是某些CPU的硬件行为特性,在RSB underflow时,不是停下来等待真正
+ * 返回地址,而是fallback到BTB/BHB来猜测返回地址.
+ *
+ * BHI利用的是BHB(Branch History Buffer),记录之前是否taken的分支路径.
+ *
+ * 在CPU 执行分支预测时,不仅仅依赖BTB,还会使用BHB中的历史信息做路径选择.
+ *
+ * 如果CPU的ret执行过程中发生fallback to BTB/BHB(RRSBA),那么BHB被攻击者
+ * 污染就可能影响投机路径.
+ * 在没有RRSBA行为的CPU上, ret 始终使用RSB预测跳转,而不是fallback到BTB/BHB.
+ * 此时, Retpoline构造的ret安全有效,不会受到BHB的污染影响.
+ * 因此在这些 CPU 上,Retpoline间接"顺带"也防住了BHI,即便不是设计目标.
+ * Again, Retpoline是一种"软件绕过硬件预测器"的技巧,不是阻止预测,而是从根本
+ * 不走那条路.
+ *
+ * # 假设CPU没有RRSBA, 为什么retpoline可以防BHI, 同时retpoline + lfence就不可以?
+ *
+ * 因为Retpoline+LFENCE在某些实现中,破坏了RSB的正常工作,使得CPU fallback
+ * 到其他预测路径(如BTB/BHB),从而导致BHI重新成为可能.
+ * 在没有RRSBA的CPU上,Retpoline单独使用时能保证RSB是有效的,所以防住了BHI.
+ * 加了LFENCE后,打乱了CPU对ret指令的正常处理方式(例如pipeline flush),
+ * 使得RSB不再被正确使用,可能导致fallback,反而引入了攻击面.
+ *
+ *
+ * 在以下调用bhi_apply_mitigation():
+ *   - arch/x86/kernel/cpu/bugs.c|331| <<cpu_select_mitigations>> bhi_apply_mitigation();
+ *
+ * start_kernel()
+ * -> arch_cpu_finalize_init()
+ *    -> cpu_select_mitigations()
+ *    -> alternative_instructions()
+ *       -> __apply_fineibt(__retpoline_sites, __retpoline_sites_end,
+ *                          __cfi_sites, __cfi_sites_end, true);
+ *       -> apply_retpolines(__retpoline_sites, __retpoline_sites_end);
+ *          -> loop: patch_retpoline()
+ *       -> apply_returns(__return_sites, __return_sites_end);
+ *
+ * cpu_select_mitigations()
+ * -> bhi_update_mitigation()
+ * -> bhi_apply_mitigation()
+ */
 static void __init bhi_apply_mitigation(void)
 {
+	/*
+	 * 在以下使用bhi_mitigation:
+	 *   - arch/x86/kernel/cpu/bugs.c|2158| <<spectre_bhi_parse_cmdline>> bhi_mitigation = BHI_MITIGATION_OFF;
+	 *   - arch/x86/kernel/cpu/bugs.c|2160| <<spectre_bhi_parse_cmdline>> bhi_mitigation = BHI_MITIGATION_ON;
+	 *   - arch/x86/kernel/cpu/bugs.c|2162| <<spectre_bhi_parse_cmdline>> bhi_mitigation = BHI_MITIGATION_VMEXIT_ONLY;
+	 *   - arch/x86/kernel/cpu/bugs.c|2179| <<bhi_select_mitigation>> bhi_mitigation = BHI_MITIGATION_OFF;
+	 *   - arch/x86/kernel/cpu/bugs.c|2181| <<bhi_select_mitigation>> if (bhi_mitigation == BHI_MITIGATION_AUTO)
+	 *   - arch/x86/kernel/cpu/bugs.c|2182| <<bhi_select_mitigation>> bhi_mitigation = BHI_MITIGATION_ON;
+	 *   - arch/x86/kernel/cpu/bugs.c|2188| <<bhi_update_mitigation>> bhi_mitigation = BHI_MITIGATION_OFF;
+	 *   - arch/x86/kernel/cpu/bugs.c|2192| <<bhi_update_mitigation>> bhi_mitigation = BHI_MITIGATION_OFF;
+	 *   - arch/x86/kernel/cpu/bugs.c|2197| <<bhi_apply_mitigation>> if (bhi_mitigation == BHI_MITIGATION_OFF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2218| <<bhi_apply_mitigation>> if (bhi_mitigation == BHI_MITIGATION_VMEXIT_ONLY) {
+	 */
 	if (bhi_mitigation == BHI_MITIGATION_OFF)
 		return;
 
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE_LFENCE:
+	 *   - arch/x86/include/asm/nospec-branch.h|514| <<CALL_NOSPEC(32位)>> X86_FEATURE_RETPOLINE_LFENCE)
+	 *   - arch/x86/kernel/alternative.c|871| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *                     !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 *   - arch/x86/kernel/alternative.c|907| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2384| <<bhi_apply_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *                     !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2556| <<spectre_v2_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE_LFENCE);
+	 *   - arch/x86/kernel/cpu/bugs.c|3685| <<spectre_bhi_state>> !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|669| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+	 */
 	/* Retpoline mitigates against BHI unless the CPU has RRSBA behavior */
 	if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
 	    !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE)) {
+		/*
+		 * 在以下使用spec_ctrl_disable_kernel_rrsba():
+		 *   - arch/x86/kernel/cpu/bugs.c|2385| <<bhi_apply_mitigation>> spec_ctrl_disable_kernel_rrsba();
+		 *   - arch/x86/kernel/cpu/bugs.c|2573| <<spectre_v2_apply_mitigation>> spec_ctrl_disable_kernel_rrsba();
+		 */
 		spec_ctrl_disable_kernel_rrsba();
+		/*
+		 * 在以下使用rrsba_disabled:
+		 *   - arch/x86/kernel/cpu/bugs.c|2042| <<spec_ctrl_disable_kernel_rrsba>> if (rrsba_disabled)
+		 *   - arch/x86/kernel/cpu/bugs.c|2046| <<spec_ctrl_disable_kernel_rrsba>> rrsba_disabled = true;
+		 *   - arch/x86/kernel/cpu/bugs.c|2067| <<spec_ctrl_disable_kernel_rrsba>> rrsba_disabled = true;
+		 *   - arch/x86/kernel/cpu/bugs.c|2370| <<bhi_apply_mitigation>> if (rrsba_disabled)
+		 *   - arch/x86/kernel/cpu/bugs.c|3659| <<spectre_bhi_state>> rrsba_disabled)
+		 */
 		if (rrsba_disabled)
 			return;
 	}
@@ -2127,18 +2562,43 @@ static void __init bhi_apply_mitigation(void)
 	if (!IS_ENABLED(CONFIG_X86_64))
 		return;
 
+	/*
+	 * 只在这里使用spec_ctrl_bhi_dis()
+	 */
 	/* Mitigate in hardware if supported */
 	if (spec_ctrl_bhi_dis())
 		return;
 
 	if (bhi_mitigation == BHI_MITIGATION_VMEXIT_ONLY) {
 		pr_info("Spectre BHI mitigation: SW BHB clearing on VM exit only\n");
+		/*
+		 * 在以下使用X86_FEATURE_CLEAR_BHB_VMEXIT:
+		 *   - arch/x86/include/asm/nospec-branch.h|336| <<CLEAR_BRANCH_HISTORY_VMEXIT>> ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_VMEXIT
+		 *   - arch/x86/kernel/cpu/bugs.c|2136| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_VMEXIT);
+		 *   - arch/x86/kernel/cpu/bugs.c|2142| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_VMEXIT);
+		 *   - arch/x86/kernel/cpu/bugs.c|3313| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_VMEXIT))
+		 */
 		setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_VMEXIT);
 		return;
 	}
 
 	pr_info("Spectre BHI mitigation: SW BHB clearing on syscall and VM exit\n");
+	/*
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_LOOP:
+	 *   - arch/x86/include/asm/nospec-branch.h|332| <<CLEAR_BRANCH_HISTORY>> ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
+	 *   - arch/x86/kernel/cpu/bugs.c|2141| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+	 *   - arch/x86/kernel/cpu/bugs.c|3307| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_LOOP))
+	 *   - arch/x86/net/bpf_jit_comp.c|1516| <<emit_spectre_bhb_barrier>> if (cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP)) {
+	 *   - arch/x86/net/bpf_jit_comp.c|1531| <<emit_spectre_bhb_barrier>> if ((cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP) &&
+	 */
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+	/*
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_VMEXIT:
+	 *   - arch/x86/include/asm/nospec-branch.h|336| <<CLEAR_BRANCH_HISTORY_VMEXIT>> ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_VMEXIT
+	 *   - arch/x86/kernel/cpu/bugs.c|2136| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_VMEXIT);
+	 *   - arch/x86/kernel/cpu/bugs.c|2142| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_VMEXIT);
+	 *   - arch/x86/kernel/cpu/bugs.c|3313| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_VMEXIT))
+	 */
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_VMEXIT);
 }
 
@@ -2222,6 +2682,18 @@ static void __init spectre_v2_apply_mitigation(void)
 		if (boot_cpu_has(X86_FEATURE_AUTOIBRS)) {
 			msr_set_bit(MSR_EFER, _EFER_AUTOIBRS);
 		} else {
+			/*
+			 * 在以下调用update_spec_ctrl():
+			 *   - arch/x86/kernel/cpu/bugs.c|1997| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2061| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2226| <<spectre_v2_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2297| <<update_stibp_msr>> update_spec_ctrl(val);
+			 *   - arch/x86/kernel/cpu/bugs.c|2558| <<ssb_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2802| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *
+			 * this_cpu_write(x86_spec_ctrl_current, val);
+			 * wrmsrq(MSR_IA32_SPEC_CTRL, val);
+			 */
 			x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
 			update_spec_ctrl(x86_spec_ctrl_base);
 		}
@@ -2242,6 +2714,18 @@ static void __init spectre_v2_apply_mitigation(void)
 
 	case SPECTRE_V2_LFENCE:
 	case SPECTRE_V2_EIBRS_LFENCE:
+		/*
+		 * 在以下使用X86_FEATURE_RETPOLINE_LFENCE:
+		 *   - arch/x86/include/asm/nospec-branch.h|514| <<CALL_NOSPEC(32位)>> X86_FEATURE_RETPOLINE_LFENCE)
+		 *   - arch/x86/kernel/alternative.c|871| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+		 *                     !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		 *   - arch/x86/kernel/alternative.c|907| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2384| <<bhi_apply_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *                     !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2556| <<spectre_v2_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE_LFENCE);
+		 *   - arch/x86/kernel/cpu/bugs.c|3685| <<spectre_bhi_state>> !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE) &&
+		 *   - arch/x86/net/bpf_jit_comp.c|669| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		 */
 		setup_force_cpu_cap(X86_FEATURE_RETPOLINE_LFENCE);
 		fallthrough;
 
@@ -2294,6 +2778,18 @@ static void __init spectre_v2_apply_mitigation(void)
 static void update_stibp_msr(void * __unused)
 {
 	u64 val = spec_ctrl_current() | (x86_spec_ctrl_base & SPEC_CTRL_STIBP);
+	/*
+	 * 在以下调用update_spec_ctrl():
+	 *   - arch/x86/kernel/cpu/bugs.c|1997| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2061| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2226| <<spectre_v2_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2297| <<update_stibp_msr>> update_spec_ctrl(val);
+	 *   - arch/x86/kernel/cpu/bugs.c|2558| <<ssb_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2802| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *
+	 * this_cpu_write(x86_spec_ctrl_current, val);
+	 * wrmsrq(MSR_IA32_SPEC_CTRL, val);
+	 */
 	update_spec_ctrl(val);
 }
 
@@ -2555,6 +3051,18 @@ static void __init ssb_apply_mitigation(void)
 			x86_amd_ssb_disable();
 		} else {
 			x86_spec_ctrl_base |= SPEC_CTRL_SSBD;
+			/*
+			 * 在以下调用update_spec_ctrl():
+			 *   - arch/x86/kernel/cpu/bugs.c|1997| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2061| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2226| <<spectre_v2_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2297| <<update_stibp_msr>> update_spec_ctrl(val);
+			 *   - arch/x86/kernel/cpu/bugs.c|2558| <<ssb_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2802| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *
+			 * this_cpu_write(x86_spec_ctrl_current, val);
+			 * wrmsrq(MSR_IA32_SPEC_CTRL, val);
+			 */
 			update_spec_ctrl(x86_spec_ctrl_base);
 		}
 	}
@@ -2798,6 +3306,18 @@ int arch_prctl_spec_ctrl_get(struct task_struct *task, unsigned long which)
 
 void x86_spec_ctrl_setup_ap(void)
 {
+	/*
+	 * 在以下调用update_spec_ctrl():
+	 *   - arch/x86/kernel/cpu/bugs.c|1997| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2061| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2226| <<spectre_v2_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2297| <<update_stibp_msr>> update_spec_ctrl(val);
+	 *   - arch/x86/kernel/cpu/bugs.c|2558| <<ssb_apply_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *   - arch/x86/kernel/cpu/bugs.c|2802| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+	 *
+	 * this_cpu_write(x86_spec_ctrl_current, val);
+	 * wrmsrq(MSR_IA32_SPEC_CTRL, val);
+	 */
 	if (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
 		update_spec_ctrl(x86_spec_ctrl_base);
 
@@ -3298,8 +3818,36 @@ static char *pbrsb_eibrs_state(void)
 	}
 }
 
+/*
+ * 在以下使用spectre_bhi_state():
+ *   - arch/x86/kernel/cpu/bugs.c|3338| <<spectre_v2_show_state>> spectre_bhi_state(),
+ */
 static const char *spectre_bhi_state(void)
 {
+	/*
+	 * 在以下使用X86_BUG_BHI:
+	 *   - arch/x86/kernel/cpu/bugs.c|2097| <<bhi_select_mitigation>> if (!boot_cpu_has(X86_BUG_BHI) || cpu_mitigations_off())
+	 *   - arch/x86/kernel/cpu/bugs.c|3303| <<spectre_bhi_state>> if (!boot_cpu_has_bug(X86_BUG_BHI))
+	 *   - arch/x86/kernel/cpu/common.c|1524| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_BHI);
+	 *
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_HW:
+	 *   - arch/x86/kernel/cpu/bugs.c|2062| <<spec_ctrl_bhi_dis>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_HW);
+	 *   - arch/x86/kernel/cpu/bugs.c|3305| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_HW))
+	 *   - arch/x86/net/bpf_jit_comp.c|1533| <<emit_spectre_bhb_barrier>> cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_HW)) {
+	 *
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_LOOP:
+	 *   - arch/x86/include/asm/nospec-branch.h|332| <<CLEAR_BRANCH_HISTORY>> ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
+	 *   - arch/x86/kernel/cpu/bugs.c|2141| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+	 *   - arch/x86/kernel/cpu/bugs.c|3307| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_LOOP))
+	 *   - arch/x86/net/bpf_jit_comp.c|1516| <<emit_spectre_bhb_barrier>> if (cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP)) {
+	 *   - arch/x86/net/bpf_jit_comp.c|1531| <<emit_spectre_bhb_barrier>> if ((cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP) &&
+	 *
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_VMEXIT:
+	 *   - arch/x86/include/asm/nospec-branch.h|336| <<CLEAR_BRANCH_HISTORY_VMEXIT>> ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_VMEXIT
+	 *   - arch/x86/kernel/cpu/bugs.c|2136| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_VMEXIT);
+	 *   - arch/x86/kernel/cpu/bugs.c|2142| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_VMEXIT);
+	 *   - arch/x86/kernel/cpu/bugs.c|3313| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_VMEXIT))
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_BHI))
 		return "; BHI: Not affected";
 	else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_HW))
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index fb50c1dd5..4d146b10c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1518,6 +1518,12 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	 * BHI_NO still need to use the BHI mitigation to prevent Intra-mode
 	 * attacks.  When virtualized, eIBRS could be hidden, assume vulnerable.
 	 */
+	/*
+	 * 在以下使用X86_BUG_BHI:
+	 *   - arch/x86/kernel/cpu/bugs.c|2097| <<bhi_select_mitigation>> if (!boot_cpu_has(X86_BUG_BHI) || cpu_mitigations_off())
+	 *   - arch/x86/kernel/cpu/bugs.c|3303| <<spectre_bhi_state>> if (!boot_cpu_has_bug(X86_BUG_BHI))
+	 *   - arch/x86/kernel/cpu/common.c|1524| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_BHI);
+	 */
 	if (!cpu_matches(cpu_vuln_whitelist, NO_BHI) &&
 	    (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED) ||
 	     boot_cpu_has(X86_FEATURE_HYPERVISOR)))
@@ -2495,6 +2501,10 @@ void arch_smt_update(void)
 	apic_smt_update();
 }
 
+/*
+ * 在以下使用arch_cpu_finalize_init():
+ *   - init/main.c|1067| <<start_kernel>> arch_cpu_finalize_init();
+ */
 void __init arch_cpu_finalize_init(void)
 {
 	struct cpuinfo_x86 *c = this_cpu_ptr(&cpu_info);
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index b3f81379c..ef71c5b8d 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -120,6 +120,11 @@ noinstr u64 pvclock_clocksource_read_nowd(struct pvclock_vcpu_time_info *src)
 	return __pvclock_clocksource_read(src, false);
 }
 
+/*
+ * 在以下使用pvclock_read_wallclock():
+ *   - arch/x86/kernel/kvmclock.c|65| <<kvm_get_wallclock>> pvclock_read_wallclock(&wall_clock, this_cpu_pvti(), now);
+ *   - arch/x86/xen/time.c|83| <<xen_read_wallclock>> pvclock_read_wallclock(wall_clock, vcpu_time, ts);
+ */
 void pvclock_read_wallclock(struct pvclock_wall_clock *wall_clock,
 			    struct pvclock_vcpu_time_info *vcpu_time,
 			    struct timespec64 *ts)
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index f84bc0569..d5c2d65bf 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -2013,6 +2013,24 @@ bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 			 * Ignore failures, there is no sane value that can be
 			 * provided if KVM can't get the TSC frequency.
 			 */
+			/*
+			 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+			 *   - arch/x86/kvm/cpuid.c|2016| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+			 *   - arch/x86/kvm/x86.c|3314| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|3579| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+			 *   - arch/x86/kvm/x86.c|3826| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|3835| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+			 *   - arch/x86/kvm/x86.c|4314| <<kvm_set_msr_common(KVM_REQ_CLOCK_UPDATE)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|5443| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|6148| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|10089| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|11324| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+			 *   - arch/x86/kvm/x86.c|11681| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|13189| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+			 */
 			if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
 				kvm_guest_time_update(vcpu);
 
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index ee27064dd..b52ca30f3 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -565,6 +565,13 @@ static void synic_init(struct kvm_vcpu_hv_synic *synic)
 	}
 }
 
+/*
+ * 在以下使用get_time_ref_counter():
+ *   - arch/x86/kvm/hyperv.c|635| <<stimer_start>> time_now = get_time_ref_counter(hv_stimer_to_vcpu(stimer)->kvm);
+ *   - arch/x86/kvm/hyperv.c|826| <<stimer_send_msg>> payload->delivery_time = get_time_ref_counter(vcpu->kvm);
+ *   - arch/x86/kvm/hyperv.c|881| <<kvm_hv_process_stimers>> get_time_ref_counter(vcpu->kvm);
+ *   - arch/x86/kvm/hyperv.c|1647| <<kvm_hv_get_msr_pw>> data = get_time_ref_counter(kvm);
+ */
 static u64 get_time_ref_counter(struct kvm *kvm)
 {
 	struct kvm_hv *hv = to_kvm_hv(kvm);
@@ -575,6 +582,22 @@ static u64 get_time_ref_counter(struct kvm *kvm)
 	 * Fall back to get_kvmclock_ns() when TSC page hasn't been set up,
 	 * is broken, disabled or being updated.
 	 */
+	/*
+	 * 在以下使用kvm_hv->hv_tsc_page_status:
+	 *   - arch/x86/kvm/hyperv.c|585| <<get_time_ref_counter>> if (hv->hv_tsc_page_status != HV_TSC_PAGE_SET)
+	 *   - arch/x86/kvm/hyperv.c|1160| <<tsc_page_update_unsafe>> return (hv->hv_tsc_page_status != HV_TSC_PAGE_GUEST_CHANGED) &&
+	 *   - arch/x86/kvm/hyperv.c|1180| <<kvm_hv_setup_tsc_page>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_BROKEN ||
+	 *   - arch/x86/kvm/hyperv.c|1181| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_SET |
+	 *   - arch/x86/kvm/hyperv.c|1182| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_UNSET)
+	 *   - arch/x86/kvm/hyperv.c|1201| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1237| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1241| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_BROKEN;
+	 *   - arch/x86/kvm/hyperv.c|1252| <<kvm_hv_request_tsc_page_update>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_SET &&
+	 *   - arch/x86/kvm/hyperv.c|1254| <<kvm_hv_request_tsc_page_update>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1446| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_GUEST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1448| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1451| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_UNSET;
+	 */
 	if (hv->hv_tsc_page_status != HV_TSC_PAGE_SET)
 		return div_u64(get_kvmclock_ns(kvm), 100);
 
@@ -1105,6 +1128,10 @@ static int kvm_hv_msr_set_crash_data(struct kvm *kvm, u32 index, u64 data)
  *
  * These two equivalencies are implemented in this function.
  */
+/*
+ * 只在一个地方调用compute_tsc_page_parameters():
+ *   - arch/x86/kvm/hyperv.c|1262| <<kvm_hv_setup_tsc_page>> if (!compute_tsc_page_parameters(hv_clock, &hv->tsc_ref))
+ */
 static bool compute_tsc_page_parameters(struct pvclock_vcpu_time_info *hv_clock,
 					struct ms_hyperv_tsc_page *tsc_ref)
 {
@@ -1148,12 +1175,37 @@ static bool compute_tsc_page_parameters(struct pvclock_vcpu_time_info *hv_clock,
  * frequency and guest visible TSC value across migration (and prevent it when
  * TSC scaling is unsupported).
  */
+/*
+ * 在以下使用tsc_page_update_unsafe():
+ *   - arch/x86/kvm/hyperv.c|1245| <<kvm_hv_setup_tsc_page>> if (tsc_seq && tsc_page_update_unsafe(hv)) {
+ *   - arch/x86/kvm/hyperv.c|1338| <<kvm_hv_request_tsc_page_update>> !tsc_page_update_unsafe(hv))
+ */
 static inline bool tsc_page_update_unsafe(struct kvm_hv *hv)
 {
+	/*
+	 * 在以下使用kvm_hv->hv_tsc_page_status:
+	 *   - arch/x86/kvm/hyperv.c|585| <<get_time_ref_counter>> if (hv->hv_tsc_page_status != HV_TSC_PAGE_SET)
+	 *   - arch/x86/kvm/hyperv.c|1160| <<tsc_page_update_unsafe>> return (hv->hv_tsc_page_status != HV_TSC_PAGE_GUEST_CHANGED) &&
+	 *   - arch/x86/kvm/hyperv.c|1180| <<kvm_hv_setup_tsc_page>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_BROKEN ||
+	 *   - arch/x86/kvm/hyperv.c|1181| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_SET |
+	 *   - arch/x86/kvm/hyperv.c|1182| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_UNSET)
+	 *   - arch/x86/kvm/hyperv.c|1201| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1237| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1241| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_BROKEN;
+	 *   - arch/x86/kvm/hyperv.c|1252| <<kvm_hv_request_tsc_page_update>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_SET &&
+	 *   - arch/x86/kvm/hyperv.c|1254| <<kvm_hv_request_tsc_page_update>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1446| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_GUEST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1448| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1451| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_UNSET;
+	 */
 	return (hv->hv_tsc_page_status != HV_TSC_PAGE_GUEST_CHANGED) &&
 		hv->hv_tsc_emulation_control;
 }
 
+/*
+ * 在以下使用kvm_hv_setup_tsc_page():
+ *   - arch/x86/kvm/x86.c|3593| <<kvm_guest_time_update>> kvm_hv_setup_tsc_page(v->kvm, &hv_clock);
+ */
 void kvm_hv_setup_tsc_page(struct kvm *kvm,
 			   struct pvclock_vcpu_time_info *hv_clock)
 {
@@ -1166,6 +1218,22 @@ void kvm_hv_setup_tsc_page(struct kvm *kvm,
 
 	mutex_lock(&hv->hv_lock);
 
+	/*
+	 * 在以下使用kvm_hv->hv_tsc_page_status:
+	 *   - arch/x86/kvm/hyperv.c|585| <<get_time_ref_counter>> if (hv->hv_tsc_page_status != HV_TSC_PAGE_SET)
+	 *   - arch/x86/kvm/hyperv.c|1160| <<tsc_page_update_unsafe>> return (hv->hv_tsc_page_status != HV_TSC_PAGE_GUEST_CHANGED) &&
+	 *   - arch/x86/kvm/hyperv.c|1180| <<kvm_hv_setup_tsc_page>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_BROKEN ||
+	 *   - arch/x86/kvm/hyperv.c|1181| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_SET |
+	 *   - arch/x86/kvm/hyperv.c|1182| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_UNSET)
+	 *   - arch/x86/kvm/hyperv.c|1201| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1237| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1241| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_BROKEN;
+	 *   - arch/x86/kvm/hyperv.c|1252| <<kvm_hv_request_tsc_page_update>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_SET &&
+	 *   - arch/x86/kvm/hyperv.c|1254| <<kvm_hv_request_tsc_page_update>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1446| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_GUEST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1448| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1451| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_UNSET;
+	 */
 	if (hv->hv_tsc_page_status == HV_TSC_PAGE_BROKEN ||
 	    hv->hv_tsc_page_status == HV_TSC_PAGE_SET ||
 	    hv->hv_tsc_page_status == HV_TSC_PAGE_UNSET)
@@ -1183,6 +1251,11 @@ void kvm_hv_setup_tsc_page(struct kvm *kvm,
 				    &tsc_seq, sizeof(tsc_seq))))
 		goto out_err;
 
+	/*
+	 * 在以下使用tsc_page_update_unsafe():
+	 *   - arch/x86/kvm/hyperv.c|1245| <<kvm_hv_setup_tsc_page>> if (tsc_seq && tsc_page_update_unsafe(hv)) {
+	 *   - arch/x86/kvm/hyperv.c|1338| <<kvm_hv_request_tsc_page_update>> !tsc_page_update_unsafe(hv))
+	 */
 	if (tsc_seq && tsc_page_update_unsafe(hv)) {
 		if (kvm_read_guest(kvm, gfn_to_gpa(gfn), &hv->tsc_ref, sizeof(hv->tsc_ref)))
 			goto out_err;
@@ -1200,6 +1273,9 @@ void kvm_hv_setup_tsc_page(struct kvm *kvm,
 			    &hv->tsc_ref, sizeof(hv->tsc_ref.tsc_sequence)))
 		goto out_err;
 
+	/*
+	 * 只在这里调用
+	 */
 	if (!compute_tsc_page_parameters(hv_clock, &hv->tsc_ref))
 		goto out_err;
 
@@ -1223,6 +1299,22 @@ void kvm_hv_setup_tsc_page(struct kvm *kvm,
 			    &hv->tsc_ref, sizeof(hv->tsc_ref.tsc_sequence)))
 		goto out_err;
 
+	/*
+	 * 在以下使用kvm_hv->hv_tsc_page_status:
+	 *   - arch/x86/kvm/hyperv.c|585| <<get_time_ref_counter>> if (hv->hv_tsc_page_status != HV_TSC_PAGE_SET)
+	 *   - arch/x86/kvm/hyperv.c|1160| <<tsc_page_update_unsafe>> return (hv->hv_tsc_page_status != HV_TSC_PAGE_GUEST_CHANGED) &&
+	 *   - arch/x86/kvm/hyperv.c|1180| <<kvm_hv_setup_tsc_page>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_BROKEN ||
+	 *   - arch/x86/kvm/hyperv.c|1181| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_SET |
+	 *   - arch/x86/kvm/hyperv.c|1182| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_UNSET)
+	 *   - arch/x86/kvm/hyperv.c|1201| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1237| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1241| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_BROKEN;
+	 *   - arch/x86/kvm/hyperv.c|1252| <<kvm_hv_request_tsc_page_update>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_SET &&
+	 *   - arch/x86/kvm/hyperv.c|1254| <<kvm_hv_request_tsc_page_update>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1446| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_GUEST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1448| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1451| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_UNSET;
+	 */
 	hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
 	goto out_unlock;
 
@@ -1232,12 +1324,38 @@ void kvm_hv_setup_tsc_page(struct kvm *kvm,
 	mutex_unlock(&hv->hv_lock);
 }
 
+/*
+ * 在以下使用kvm_hv_request_tsc_page_update():
+ *   - arch/x86/kvm/x86.c|3274| <<kvm_update_masterclock>> kvm_hv_request_tsc_page_update(kvm);
+ *   - arch/x86/kvm/x86.c|7402| <<kvm_vm_ioctl_set_clock>> kvm_hv_request_tsc_page_update(kvm);
+ */
 void kvm_hv_request_tsc_page_update(struct kvm *kvm)
 {
 	struct kvm_hv *hv = to_kvm_hv(kvm);
 
 	mutex_lock(&hv->hv_lock);
 
+	/*
+	 * 在以下使用kvm_hv->hv_tsc_page_status:
+	 *   - arch/x86/kvm/hyperv.c|585| <<get_time_ref_counter>> if (hv->hv_tsc_page_status != HV_TSC_PAGE_SET)
+	 *   - arch/x86/kvm/hyperv.c|1160| <<tsc_page_update_unsafe>> return (hv->hv_tsc_page_status != HV_TSC_PAGE_GUEST_CHANGED) &&
+	 *   - arch/x86/kvm/hyperv.c|1180| <<kvm_hv_setup_tsc_page>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_BROKEN ||
+	 *   - arch/x86/kvm/hyperv.c|1181| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_SET |
+	 *   - arch/x86/kvm/hyperv.c|1182| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_UNSET)
+	 *   - arch/x86/kvm/hyperv.c|1201| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1237| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+	 *   - arch/x86/kvm/hyperv.c|1241| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_BROKEN;
+	 *   - arch/x86/kvm/hyperv.c|1252| <<kvm_hv_request_tsc_page_update>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_SET &&
+	 *   - arch/x86/kvm/hyperv.c|1254| <<kvm_hv_request_tsc_page_update>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1446| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_GUEST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1448| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+	 *   - arch/x86/kvm/hyperv.c|1451| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_UNSET;
+	 *
+	 *
+	 * 在以下使用tsc_page_update_unsafe():
+	 *   - arch/x86/kvm/hyperv.c|1245| <<kvm_hv_setup_tsc_page>> if (tsc_seq && tsc_page_update_unsafe(hv)) {
+	 *   - arch/x86/kvm/hyperv.c|1338| <<kvm_hv_request_tsc_page_update>> !tsc_page_update_unsafe(hv))
+	 */
 	if (hv->hv_tsc_page_status == HV_TSC_PAGE_SET &&
 	    !tsc_page_update_unsafe(hv))
 		hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
@@ -1431,6 +1549,22 @@ static int kvm_hv_set_msr_pw(struct kvm_vcpu *vcpu, u32 msr, u64 data,
 	case HV_X64_MSR_REFERENCE_TSC:
 		hv->hv_tsc_page = data;
 		if (hv->hv_tsc_page & HV_X64_MSR_TSC_REFERENCE_ENABLE) {
+			/*
+			 * 在以下使用kvm_hv->hv_tsc_page_status:
+			 *   - arch/x86/kvm/hyperv.c|585| <<get_time_ref_counter>> if (hv->hv_tsc_page_status != HV_TSC_PAGE_SET)
+			 *   - arch/x86/kvm/hyperv.c|1160| <<tsc_page_update_unsafe>> return (hv->hv_tsc_page_status != HV_TSC_PAGE_GUEST_CHANGED) &&
+			 *   - arch/x86/kvm/hyperv.c|1180| <<kvm_hv_setup_tsc_page>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_BROKEN ||
+			 *   - arch/x86/kvm/hyperv.c|1181| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_SET |
+			 *   - arch/x86/kvm/hyperv.c|1182| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status == HV_TSC_PAGE_UNSET)
+			 *   - arch/x86/kvm/hyperv.c|1201| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+			 *   - arch/x86/kvm/hyperv.c|1237| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_SET;
+			 *   - arch/x86/kvm/hyperv.c|1241| <<kvm_hv_setup_tsc_page>> hv->hv_tsc_page_status = HV_TSC_PAGE_BROKEN;
+			 *   - arch/x86/kvm/hyperv.c|1252| <<kvm_hv_request_tsc_page_update>> if (hv->hv_tsc_page_status == HV_TSC_PAGE_SET &&
+			 *   - arch/x86/kvm/hyperv.c|1254| <<kvm_hv_request_tsc_page_update>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+			 *   - arch/x86/kvm/hyperv.c|1446| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_GUEST_CHANGED;
+			 *   - arch/x86/kvm/hyperv.c|1448| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+			 *   - arch/x86/kvm/hyperv.c|1451| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> hv->hv_tsc_page_status = HV_TSC_PAGE_UNSET;
+			 */
 			if (!host)
 				hv->hv_tsc_page_status = HV_TSC_PAGE_GUEST_CHANGED;
 			else
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 73418dc0e..7cc5bbcc3 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -221,6 +221,10 @@ static inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,
 	}
 }
 
+/*
+ * 在以下使用kvm_recalculate_phys_map():
+ *   - arch/x86/kvm/lapic.c|438| <<kvm_recalculate_apic_map>> r = kvm_recalculate_phys_map(new, vcpu, &xapic_id_mismatch);
+ */
 static int kvm_recalculate_phys_map(struct kvm_apic_map *new,
 				    struct kvm_vcpu *vcpu,
 				    bool *xapic_id_mismatch)
@@ -2597,6 +2601,11 @@ u64 kvm_lapic_get_cr8(struct kvm_vcpu *vcpu)
 	return (tpr & 0xf0) >> 4;
 }
 
+/*
+ * 在以下使用__kvm_apic_set_base():
+ *   - arch/x86/kvm/lapic.c|2668| <<kvm_apic_set_base>> __kvm_apic_set_base(vcpu, value);
+ *   - arch/x86/kvm/lapic.c|2779| <<kvm_lapic_reset>> __kvm_apic_set_base(vcpu, msr_val);
+ */
 static void __kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value)
 {
 	u64 old_value = vcpu->arch.apic_base;
@@ -2757,6 +2766,10 @@ void kvm_inhibit_apic_access_page(struct kvm_vcpu *vcpu)
 	kvm_vcpu_srcu_read_lock(vcpu);
 }
 
+/*
+ * 在以下使用kvm_lapic_reset():
+ *   - arch/x86/kvm/x86.c|12507| <<kvm_vcpu_reset>> kvm_lapic_reset(vcpu, init_event);
+ */
 void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 93636f77c..4f7e530dd 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -2278,6 +2278,15 @@ static void update_pvclock_gtod(struct timekeeper *tk)
 	write_seqcount_end(&vdata->seq);
 }
 
+/*
+ * 在以下使用get_kvmclock_base_ns():
+ *   - arch/x86/kvm/x86.c|2718| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|3131| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+ *   - arch/x86/kvm/x86.c|3241| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|5806| <<kvm_arch_tsc_set_attr>> ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|7071| <<kvm_vm_ioctl_set_clock>> now_raw_ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|12823| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+ */
 static s64 get_kvmclock_base_ns(void)
 {
 	/* Count up from boot time, but with the frequency of the raw clock.  */
@@ -2291,6 +2300,11 @@ static s64 get_kvmclock_base_ns(void)
 }
 #endif
 
+/*
+ * 在以下使用kvm_write_wall_clock():
+ *   - arch/x86/kvm/x86.c|4484| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK_NEW)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/x86.c|4491| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ */
 static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 {
 	int version;
@@ -2314,6 +2328,11 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 	if (kvm_write_guest(kvm, wall_clock, &version, sizeof(version)))
 		return;
 
+	/*
+	 * 在以下使用kvm_get_wall_clock_epoch():
+	 *   - arch/x86/kvm/x86.c|2326| <<kvm_write_wall_clock>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+	 *   - arch/x86/kvm/xen.c|63| <<kvm_xen_shared_info_init>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+	 */
 	wall_nsec = kvm_get_wall_clock_epoch(kvm);
 
 	wc.nsec = do_div(wall_nsec, NSEC_PER_SEC);
@@ -2347,6 +2366,16 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 	vcpu->arch.time = system_time;
 	kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2371| <<kvm_write_system_time>> kvm_gpc_activate(&vcpu->arch.pv_time, system_time & ~1ULL,
+	 *   - arch/x86/kvm/x86.c|2374| <<kvm_write_system_time>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|3704| <<kvm_guest_time_update>> if (vcpu->pv_time.active) {
+	 *   - arch/x86/kvm/x86.c|3723| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|4102| <<kvmclock_reset>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|6275| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|13065| <<kvm_arch_vcpu_create>> kvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm);
+	 */
 	/* we verify if the enable bit is set... */
 	if (system_time & 1)
 		kvm_gpc_activate(&vcpu->arch.pv_time, system_time & ~1ULL,
@@ -2407,12 +2436,22 @@ static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier);
 
+/*
+ * 在以下使用set_tsc_khz():
+ *   - arch/x86/kvm/x86.c|2492| <<kvm_set_tsc_khz>> return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
+ */
 static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 {
 	u64 ratio;
 
 	/* Guest TSC same frequency as host TSC? */
 	if (!scale) {
+		/*
+		 * 在以下使用kvm_vcpu_write_tsc_multiplier():
+		 *   - arch/x86/kvm/x86.c|2425| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+		 *   - arch/x86/kvm/x86.c|2457| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, ratio);
+		 *   - arch/x86/kvm/x86.c|2469| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+		 */
 		kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
 		return 0;
 	}
@@ -2420,6 +2459,12 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	/* TSC scaling supported? */
 	if (!kvm_caps.has_tsc_control) {
 		if (user_tsc_khz > tsc_khz) {
+			/*
+			 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+			 *   - arch/x86/kvm/x86.c|2432| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+			 *   - arch/x86/kvm/x86.c|3446| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+			 *   - arch/x86/kvm/x86.c|5272| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+			 */
 			vcpu->arch.tsc_catchup = 1;
 			vcpu->arch.tsc_always_catchup = 1;
 			return 0;
@@ -2439,10 +2484,21 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 		return -1;
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_write_tsc_multiplier():
+	 *   - arch/x86/kvm/x86.c|2425| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2457| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, ratio);
+	 *   - arch/x86/kvm/x86.c|2469| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 */
 	kvm_vcpu_write_tsc_multiplier(vcpu, ratio);
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_set_tsc_khz():
+ *   - arch/x86/kvm/x86.c|6494| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|12721| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2450,6 +2506,12 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 
 	/* tsc_khz can be zero if TSC calibration fails */
 	if (user_tsc_khz == 0) {
+		/*
+		 * 在以下使用kvm_vcpu_write_tsc_multiplier():
+		 *   - arch/x86/kvm/x86.c|2425| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+		 *   - arch/x86/kvm/x86.c|2457| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, ratio);
+		 *   - arch/x86/kvm/x86.c|2469| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+		 */
 		/* set tsc_scaling_ratio to a safe value */
 		kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
 		return -1;
@@ -2474,9 +2536,16 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 			 user_tsc_khz, thresh_lo, thresh_hi);
 		use_scaling = 1;
 	}
+	/*
+	 * 只在这里调用
+	 */
 	return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
 }
 
+/*
+ * 只在vcpu->tsc_catchup的时候调用compute_guest_tsc():
+ *   - arch/x86/kvm/x86.c|3447| <<kvm_guest_time_update>> u64 tsc = compute_guest_tsc(v, kernel_ns);
+ */
 static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 {
 	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
@@ -2493,6 +2562,10 @@ static inline bool gtod_is_based_on_tsc(int mode)
 }
 #endif
 
+/*
+ * 在以下使用kvm_track_tsc_matching():
+ *   - arch/x86/kvm/x86.c|2838| <<__kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu, !matched);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu, bool new_generation)
 {
 #ifdef CONFIG_X86_64
@@ -2504,6 +2577,14 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu, bool new_generation)
 	 * and all vCPUs must have matching TSCs.  Note, the count for matching
 	 * vCPUs doesn't include the reference vCPU, hence "+1".
 	 */
+	/*
+	 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2576| <<kvm_track_tsc_matching>> bool use_master_clock = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2590| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2828| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2830| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|3209| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	bool use_master_clock = (ka->nr_vcpus_matched_tsc + 1 ==
 				 atomic_read(&vcpu->kvm->online_vcpus)) &&
 				gtod_is_based_on_tsc(gtod->clock.vclock_mode);
@@ -2549,6 +2630,13 @@ u64 kvm_scale_tsc(u64 tsc, u64 ratio)
 	return _tsc;
 }
 
+/*
+ * 在以下使用kvm_compute_l1_tsc_offset():
+ *   - arch/x86/kvm/x86.c|2772| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|2830| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|4180| <<kvm_set_msr_common>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+ *   - arch/x86/kvm/x86.c|5278| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+ */
 static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
@@ -2560,6 +2648,11 @@ static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/vmx/tdx.c|695| <<tdx_vcpu_create>> vcpu->arch.l1_tsc_offset = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2634| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = l1_offset;
+	 */
 	return vcpu->arch.l1_tsc_offset +
 		kvm_scale_tsc(host_tsc, vcpu->arch.l1_tsc_scaling_ratio);
 }
@@ -2590,6 +2683,12 @@ u64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)
 }
 EXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_multiplier);
 
+/*
+ * 在以下使用kvm_vcpu_write_tsc_offset():
+ *   - arch/x86/kvm/x86.c|2699| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2821| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|5231| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 {
 	if (vcpu->arch.guest_tsc_protected)
@@ -2617,6 +2716,12 @@ static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 	kvm_x86_call(write_tsc_offset)(vcpu);
 }
 
+/*
+ * 在以下使用kvm_vcpu_write_tsc_multiplier():
+ *   - arch/x86/kvm/x86.c|2425| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2457| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, ratio);
+ *   - arch/x86/kvm/x86.c|2469| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+ */
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier)
 {
 	vcpu->arch.l1_tsc_scaling_ratio = l1_multiplier;
@@ -2651,11 +2756,28 @@ static inline bool kvm_check_tsc_unstable(void)
  * offset for the vcpu and tracks the TSC matching generation that the vcpu
  * participates in.
  */
+/*
+ * 在以下使用__kvm_synchronize_tsc():
+ *   - arch/x86/kvm/x86.c|2944| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched, !!user_value);
+ *   - arch/x86/kvm/x86.c|6518| <<kvm_arch_tsc_set_attr>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched, true);
+ */
 static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 				  u64 ns, bool matched, bool user_set_tsc)
 {
 	struct kvm *kvm = vcpu->kvm;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2659| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2716| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2772| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3017| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3049| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3068| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock)
+	 *   - arch/x86/kvm/x86.c|5799| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5809| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|12820| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	lockdep_assert_held(&kvm->arch.tsc_write_lock);
 
 	if (vcpu->arch.guest_tsc_protected)
@@ -2668,13 +2790,42 @@ static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 	 * We also track th most recent recorded KHZ, write and time to
 	 * allow the matching interval to be extended at each write.
 	 */
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2776| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2856| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|13575| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	kvm->arch.last_tsc_nsec = ns;
+	/*
+	 * 在以下使用kvm_arch->arch.last_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2777| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+	 *   - arch/x86/kvm/x86.c|2866| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+	 *   - arch/x86/kvm/x86.c|13576| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_write = 0;
+	 */
 	kvm->arch.last_tsc_write = tsc;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2778| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2892| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|6420| <<kvm_arch_tsc_set_attr>> kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 */
 	kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2779| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|6421| <<kvm_arch_tsc_set_attr>> kvm->arch.last_tsc_offset == offset);
+	 */
 	kvm->arch.last_tsc_offset = offset;
 
 	vcpu->arch.last_guest_tsc = tsc;
 
+	/*
+	 * 在以下使用kvm_vcpu_write_tsc_offset():
+	 *   - arch/x86/kvm/x86.c|2699| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 *   - arch/x86/kvm/x86.c|2821| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+	 *   - arch/x86/kvm/x86.c|5231| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 */
 	kvm_vcpu_write_tsc_offset(vcpu, offset);
 
 	if (!matched) {
@@ -2687,23 +2838,57 @@ static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 		 *
 		 * These values are tracked in kvm->arch.cur_xxx variables.
 		 */
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_generation:
+		 *   - arch/x86/kvm/x86.c|2824| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+		 *   - arch/x86/kvm/x86.c|2829| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+		 *   - arch/x86/kvm/x86.c|2834| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+		 */
 		kvm->arch.cur_tsc_generation++;
 		kvm->arch.cur_tsc_nsec = ns;
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_write:
+		 *   - arch/x86/kvm/x86.c|2826| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_write = tsc;
+		 *   - arch/x86/kvm/x86.c|2836| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+		 */
 		kvm->arch.cur_tsc_write = tsc;
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_offset:
+		 *   - arch/x86/kvm/x86.c|2827| <<__kvm_synchronize_tsc(MSR_IA32_TSC)>> kvm->arch.cur_tsc_offset = offset;
+		 *   - arch/x86/kvm/x86.c|2935| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+		 */
 		kvm->arch.cur_tsc_offset = offset;
 		kvm->arch.nr_vcpus_matched_tsc = 0;
 	} else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+		/*
+		 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+		 *   - arch/x86/kvm/x86.c|2576| <<kvm_track_tsc_matching>> bool use_master_clock = (ka->nr_vcpus_matched_tsc + 1 ==
+		 *   - arch/x86/kvm/x86.c|2590| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+		 *   - arch/x86/kvm/x86.c|2828| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+		 *   - arch/x86/kvm/x86.c|2830| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+		 *   - arch/x86/kvm/x86.c|3209| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+		 */
 		kvm->arch.nr_vcpus_matched_tsc++;
 	}
 
 	/* Keep track of which generation this VCPU has synchronized to */
 	vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
 	vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2826| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_write = tsc;
+	 *   - arch/x86/kvm/x86.c|2836| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 */
 	vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
 
 	kvm_track_tsc_matching(vcpu, !matched);
 }
 
+/*
+ * 在以下使用kvm_synchronize_tsc():
+ *   - arch/x86/kvm/x86.c|4586| <<kvm_set_msr_common>> kvm_synchronize_tsc(vcpu, &data);
+ *   - arch/x86/kvm/x86.c|13303| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, NULL);
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 *user_value)
 {
 	u64 data = user_value ? *user_value : 0;
@@ -2713,9 +2898,41 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 *user_value)
 	bool matched = false;
 	bool synchronizing = false;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2659| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2716| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2772| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3017| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3049| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3068| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock)
+	 *   - arch/x86/kvm/x86.c|5799| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5809| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|12820| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
 	offset = kvm_compute_l1_tsc_offset(vcpu, data);
+	/*
+	 * 在以下使用get_kvmclock_base_ns():
+	 *   - arch/x86/kvm/x86.c|2718| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|3131| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3241| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|5806| <<kvm_arch_tsc_set_attr>> ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|7071| <<kvm_vm_ioctl_set_clock>> now_raw_ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|12823| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 *
+	 * 现在的nanoseconds
+	 */
 	ns = get_kvmclock_base_ns();
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2776| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2856| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|13575| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_nsec = 0;
+	 *
+	 * 上一次进来kvm_synchronize_tsc()的时候
+	 * 的get_kvmclock_base_ns()
+	 */
 	elapsed = ns - kvm->arch.last_tsc_nsec;
 
 	if (vcpu->arch.virtual_tsc_khz) {
@@ -2726,6 +2943,15 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 *user_value)
 			 */
 			synchronizing = true;
 		} else if (kvm->arch.user_set_tsc) {
+			/*
+			 * 在以下使用kvm_arch->arch.last_tsc_write:
+			 *   - arch/x86/kvm/x86.c|2777| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+			 *   - arch/x86/kvm/x86.c|2866| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+			 *   - arch/x86/kvm/x86.c|13576| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_write = 0;
+			 *
+			 * 上一次进来kvm_synchronize_tsc()的时候
+			 * 传进来的data
+			 */
 			u64 tsc_exp = kvm->arch.last_tsc_write +
 						nsec_to_cycles(vcpu, elapsed);
 			u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
@@ -2756,9 +2982,20 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 *user_value)
 	 * compensation code attempt to catch up if we fall behind, but
 	 * it's better to try to match offsets from the beginning.
          */
+	/*
+	 * 在以下使用kvm_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2778| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2892| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|6420| <<kvm_arch_tsc_set_attr>> kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 */
 	if (synchronizing &&
 	    vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
 		if (!kvm_check_tsc_unstable()) {
+			/*
+			 * 在以下使用kvm_arch->cur_tsc_offset:
+			 *   - arch/x86/kvm/x86.c|2827| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+			 *   - arch/x86/kvm/x86.c|2935| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+			 */
 			offset = kvm->arch.cur_tsc_offset;
 		} else {
 			u64 delta = nsec_to_cycles(vcpu, elapsed);
@@ -2768,6 +3005,11 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 *user_value)
 		matched = true;
 	}
 
+	/*
+	 * 在以下使用__kvm_synchronize_tsc():
+	 *   - arch/x86/kvm/x86.c|2944| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched, !!user_value);
+	 *   - arch/x86/kvm/x86.c|6518| <<kvm_arch_tsc_set_attr>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched, true);
+	 */
 	__kvm_synchronize_tsc(vcpu, offset, data, ns, matched, !!user_value);
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 }
@@ -2776,6 +3018,12 @@ static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 					   s64 adjustment)
 {
 	u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	/*
+	 * 在以下使用kvm_vcpu_write_tsc_offset():
+	 *   - arch/x86/kvm/x86.c|2699| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 *   - arch/x86/kvm/x86.c|2821| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+	 *   - arch/x86/kvm/x86.c|5231| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 */
 	kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
 }
 
@@ -3000,6 +3248,13 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * 在以下使用pvclock_update_vm_gtod_copy():
+ *   - arch/x86/kvm/x86.c|3074| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|7042| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|9520| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|12819| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -3007,7 +3262,27 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	int vclock_mode;
 	bool host_tsc_clocksource, vcpus_matched;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2659| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2716| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2772| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3017| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3049| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3068| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock)
+	 *   - arch/x86/kvm/x86.c|5799| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5809| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|12820| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	/*
+	 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2576| <<kvm_track_tsc_matching>> bool use_master_clock = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2590| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2828| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2830| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|3209| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			atomic_read(&kvm->online_vcpus));
 
@@ -3015,6 +3290,14 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	 * If the host uses TSC clock, then passthrough TSC as stable
 	 * to the guest.
 	 */
+	/*
+	 * 在以下使用kvm_arch->master_cycle_now:
+	 *   - arch/x86/kvm/x86.c|3177| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+	 *                                           &ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3450| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3630| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3867| <<kvm_get_wall_clock_epoch>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 */
 	host_tsc_clocksource = kvm_get_time_and_clockread(
 					&ka->master_kernel_ns,
 					&ka->master_cycle_now);
@@ -3032,46 +3315,192 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 #endif
 }
 
+/*
+ * 在以下使用kvm_make_mclock_inprogress_request():
+ *   - arch/x86/kvm/x86.c|3220| <<kvm_start_pvclock_update>> kvm_make_mclock_inprogress_request(kvm);
+ *   - arch/x86/kvm/x86.c|9923| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+ */
 static void kvm_make_mclock_inprogress_request(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用KVM_REQ_MCLOCK_INPROGRESS:
+	 *   - arch/x86/kvm/x86.c|3174| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+	 *   - arch/x86/kvm/x86.c|3275| <<kvm_end_pvclock_update>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
 }
 
+/*
+ * 在以下使用__kvm_start_pvclock_update():
+ *   - arch/x86/kvm/x86.c|3160| <<kvm_start_pvclock_update>> __kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9755| <<kvm_hyperv_tsc_notifier>> __kvm_start_pvclock_update(kvm);
+ *
+ * 写者在修改数据时,必须通过这个raw_spinlock.
+ * 读者则依赖seqcount 的机制进行无锁访问.
+ */
 static void __kvm_start_pvclock_update(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2659| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2716| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2772| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3017| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3049| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3068| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock)
+	 *   - arch/x86/kvm/x86.c|5799| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5809| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|12820| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	/*
+	 * 在以下使用kvm_arch->pvclock_sc:
+	 *   - arch/x86/kvm/x86.c|3050| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3067| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3143| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3145| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3223| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3229| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3359| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3387| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|12822| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc,
+	 *        &kvm->arch.tsc_write_lock);
+	 */
 	write_seqcount_begin(&kvm->arch.pvclock_sc);
 }
 
+/*
+ * commit 869b44211adc878be7149cc4ae57207f924f7390
+ * Author: Paolo Bonzini <pbonzini@redhat.com>
+ * Date:   Thu Sep 16 18:15:36 2021 +0000
+ *
+ * kvm: x86: protect masterclock with a seqcount
+ *
+ * Protect the reference point for kvmclock with a seqcount, so that
+ * kvmclock updates for all vCPUs can proceed in parallel.  Xen runstate
+ * updates will also run in parallel and not bounce the kvmclock cacheline.
+ *
+ * Of the variables that were protected by pvclock_gtod_sync_lock,
+ * nr_vcpus_matched_tsc is different because it is updated outside
+ * pvclock_update_vm_gtod_copy and read inside it.  Therefore, we
+ * need to keep it protected by a spinlock.  In fact it must now
+ * be a raw spinlock, because pvclock_update_vm_gtod_copy, being the
+ * write-side of a seqcount, is non-preemptible.  Since we alread
+ * have tsc_write_lock which is a raw spinlock, we can just use
+ * tsc_write_lock as the lock that protects the write-side of the
+ * seqcount.
+ *
+ * Co-developed-by: Oliver Upton <oupton@google.com>
+ * Message-Id: <20210916181538.968978-6-oupton@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ */
+/*
+ * 在以下使用kvm_start_pvclock_update():
+ *   - arch/x86/kvm/x86.c|3184| <<kvm_update_masterclock>> kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|7239| <<kvm_vm_ioctl_set_clock>> kvm_start_pvclock_update(kvm);
+ */
 static void kvm_start_pvclock_update(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_make_mclock_inprogress_request():
+	 *   - arch/x86/kvm/x86.c|3220| <<kvm_start_pvclock_update>> kvm_make_mclock_inprogress_request(kvm);
+	 *   - arch/x86/kvm/x86.c|9923| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+	 */
 	kvm_make_mclock_inprogress_request(kvm);
 
+	/*
+	 * 在以下使用__kvm_start_pvclock_update():
+	 *   - arch/x86/kvm/x86.c|3160| <<kvm_start_pvclock_update>> __kvm_start_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|9755| <<kvm_hyperv_tsc_notifier>> __kvm_start_pvclock_update(kvm);
+	 */
 	/* no guest entries from this point */
 	__kvm_start_pvclock_update(kvm);
 }
 
+/*
+ * 在以下使用kvm_end_pvclock_update():
+ *   - arch/x86/kvm/x86.c|3277| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|7437| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9897| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+ */
 static void kvm_end_pvclock_update(struct kvm *kvm)
 {
 	struct kvm_arch *ka = &kvm->arch;
 	struct kvm_vcpu *vcpu;
 	unsigned long i;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_sc:
+	 *   - arch/x86/kvm/x86.c|3050| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3067| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3143| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3145| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3223| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3229| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3359| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3387| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|12822| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc,
+	 *        &kvm->arch.tsc_write_lock);
+	 */
 	write_seqcount_end(&ka->pvclock_sc);
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2659| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2716| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2772| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3017| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3049| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3068| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock)
+	 *   - arch/x86/kvm/x86.c|5799| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5809| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|12820| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spin_unlock_irq(&ka->tsc_write_lock);
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
+	/*
+	 * 在以下使用KVM_REQ_MCLOCK_INPROGRESS:
+	 *   - arch/x86/kvm/x86.c|3174| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+	 *   - arch/x86/kvm/x86.c|3275| <<kvm_end_pvclock_update>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+	 */
 	/* guest entries allowed */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
 }
 
+/*
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|11189| <<vcpu_enter_guest>> kvm_update_masterclock(vcpu->kvm);
+ */
 static void kvm_update_masterclock(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_hv_request_tsc_page_update():
+	 *   - arch/x86/kvm/x86.c|3274| <<kvm_update_masterclock>> kvm_hv_request_tsc_page_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7402| <<kvm_vm_ioctl_set_clock>> kvm_hv_request_tsc_page_update(kvm);
+	 */
 	kvm_hv_request_tsc_page_update(kvm);
+	/*
+	 * 在以下使用kvm_start_pvclock_update():
+	 *   - arch/x86/kvm/x86.c|3184| <<kvm_update_masterclock>> kvm_start_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7239| <<kvm_vm_ioctl_set_clock>> kvm_start_pvclock_update(kvm);
+	 */
 	kvm_start_pvclock_update(kvm);
+	/*
+	 * 在以下使用pvclock_update_vm_gtod_copy():
+	 *   - arch/x86/kvm/x86.c|3074| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|7042| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|9520| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|12819| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
+	/*
+	 * 在以下使用kvm_end_pvclock_update():
+	 *   - arch/x86/kvm/x86.c|3277| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7437| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|9897| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+	 */
 	kvm_end_pvclock_update(kvm);
 }
 
@@ -3091,10 +3520,40 @@ static unsigned long get_cpu_tsc_khz(void)
 		return __this_cpu_read(cpu_tsc_khz);
 }
 
+/*
+ * 在以下使用__get_kvmclock():
+ *   - arch/x86/kvm/x86.c|3320| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ *
+ * // Do not use 1, KVM_CHECK_EXTENSION returned it before we had flags.
+ * #define KVM_CLOCK_TSC_STABLE            2
+ * #define KVM_CLOCK_REALTIME              (1 << 2)
+ * #define KVM_CLOCK_HOST_TSC              (1 << 3)
+ *
+ * struct kvm_clock_data {
+ *     __u64 clock;
+ *     __u32 flags;
+ *     __u32 pad0;
+ *     __u64 realtime;
+ *     __u64 host_tsc;
+ *     __u32 pad[4];
+ * };
+ */
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
+	/*
+	 * struct pvclock_vcpu_time_info {
+	 *     u32   version;
+	 *     u32   pad0;
+	 *     u64   tsc_timestamp;
+	 *     u64   system_time;
+	 *     u32   tsc_to_system_mul;
+	 *     s8    tsc_shift;
+	 *     u8    flags;
+	 *     u8    pad[2];
+	 * } __attribute__((__packed__));
+	 */
 	struct pvclock_vcpu_time_info hv_clock;
 
 	/* both __this_cpu_read() and rdtsc() should be on the same cpu */
@@ -3114,6 +3573,14 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 		data->host_tsc = rdtsc();
 
 		data->flags |= KVM_CLOCK_TSC_STABLE;
+		/*
+		 * 在以下使用kvm_arch->master_cycle_now:
+		 *   - arch/x86/kvm/x86.c|3177| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+		 *                                           &ka->master_kernel_ns, &ka->master_cycle_now);
+		 *   - arch/x86/kvm/x86.c|3450| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+		 *   - arch/x86/kvm/x86.c|3630| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+		 *   - arch/x86/kvm/x86.c|3867| <<kvm_get_wall_clock_epoch>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+		 */
 		hv_clock.tsc_timestamp = ka->master_cycle_now;
 		hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
 		kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
@@ -3121,31 +3588,96 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 				   &hv_clock.tsc_to_system_mul);
 		data->clock = __pvclock_read_cycles(&hv_clock, data->host_tsc);
 	} else {
+		/*
+		 * 在以下使用get_kvmclock_base_ns():
+		 *   - arch/x86/kvm/x86.c|2718| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|3131| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3241| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|5806| <<kvm_arch_tsc_set_attr>> ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|7071| <<kvm_vm_ioctl_set_clock>> now_raw_ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|12823| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 */
 		data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
 	}
 
 	put_cpu();
 }
 
+/*
+ * 在以下使用get_kvmclock():
+ *   - arch/x86/kvm/x86.c|3337| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+ *   - arch/x86/kvm/x86.c|7283| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+ */
 static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
 	unsigned seq;
 
 	do {
+		/*
+		 * 在以下使用kvm_arch->pvclock_sc:
+		 *   - arch/x86/kvm/x86.c|3050| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3067| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3143| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3145| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|3223| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3229| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|3359| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3387| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|12822| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc,
+		 *        &kvm->arch.tsc_write_lock);
+		 */
 		seq = read_seqcount_begin(&ka->pvclock_sc);
 		__get_kvmclock(kvm, data);
 	} while (read_seqcount_retry(&ka->pvclock_sc, seq));
 }
 
+/*
+ * 在以下使用get_kvmclock_ns():
+ *   - arch/x86/kvm/hyperv.c|579| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|3403| <<kvm_get_wall_clock_epoch>> return ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|272| <<kvm_xen_start_timer>> guest_now = get_kvmclock_ns(vcpu->kvm);
+ *   - arch/x86/kvm/xen.c|590| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|1039| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|1080| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_clock_data data;
 
+	/*
+	 * 在以下使用get_kvmclock():
+	 *   - arch/x86/kvm/x86.c|3337| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+	 *   - arch/x86/kvm/x86.c|7283| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+	 */
 	get_kvmclock(kvm, &data);
 	return data.clock;
 }
 
+/*
+ * 在以下使用kvm_setup_guest_pvclock():
+ *   - arch/x86/kvm/x86.c|3570| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock,
+ *          v, &vcpu->pv_time, 0);
+ *   - arch/x86/kvm/x86.c|3590| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock,
+ *          v, &vcpu->xen.vcpu_info_cache, offsetof(struct compat_vcpu_info, time));
+ *   - arch/x86/kvm/x86.c|3593| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock,
+ *          v, &vcpu->xen.vcpu_time_info_cache, 0);
+ *
+ * struct gfn_to_pfn_cache {
+ *     u64 generation;
+ *     gpa_t gpa;
+ *     unsigned long uhva; 
+ *     struct kvm_memory_slot *memslot;
+ *     struct kvm *kvm;
+ *     struct list_head list;
+ *     rwlock_t lock;
+ *     struct mutex refresh_lock;
+ *     void *khva;
+ *     kvm_pfn_t pfn;
+ *     bool active;
+ *     bool valid;
+ * };
+ */
 static void kvm_setup_guest_pvclock(struct pvclock_vcpu_time_info *ref_hv_clock,
 				    struct kvm_vcpu *vcpu,
 				    struct gfn_to_pfn_cache *gpc,
@@ -3213,9 +3745,30 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 	 * to the guest.
 	 */
 	do {
+		/*
+		 * 在以下使用kvm_arch->pvclock_sc:
+		 *   - arch/x86/kvm/x86.c|3050| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3067| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3143| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3145| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|3223| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3229| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|3359| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3387| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|12822| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc,
+		 *        &kvm->arch.tsc_write_lock);
+		 */
 		seq = read_seqcount_begin(&ka->pvclock_sc);
 		use_master_clock = ka->use_master_clock;
 		if (use_master_clock) {
+			/*
+			 * 在以下使用kvm_arch->master_cycle_now:
+			 *   - arch/x86/kvm/x86.c|3177| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+			 *                                           &ka->master_kernel_ns, &ka->master_cycle_now);
+			 *   - arch/x86/kvm/x86.c|3450| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+			 *   - arch/x86/kvm/x86.c|3630| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+			 *   - arch/x86/kvm/x86.c|3867| <<kvm_get_wall_clock_epoch>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+			 */
 			host_tsc = ka->master_cycle_now;
 			kernel_ns = ka->master_kernel_ns;
 		}
@@ -3226,11 +3779,38 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 	tgt_tsc_khz = get_cpu_tsc_khz();
 	if (unlikely(tgt_tsc_khz == 0)) {
 		local_irq_restore(flags);
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/cpuid.c|2016| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|3314| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3579| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3826| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3835| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|4314| <<kvm_set_msr_common(KVM_REQ_CLOCK_UPDATE)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5443| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|6148| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10089| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11324| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|11681| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|13189| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
 		return 1;
 	}
 	if (!use_master_clock) {
 		host_tsc = rdtsc();
+		/*
+		 * 在以下使用get_kvmclock_base_ns():
+		 *   - arch/x86/kvm/x86.c|2718| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|3131| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3241| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|5806| <<kvm_arch_tsc_set_attr>> ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|7071| <<kvm_vm_ioctl_set_clock>> now_raw_ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|12823| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 */
 		kernel_ns = get_kvmclock_base_ns();
 	}
 
@@ -3246,6 +3826,12 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 	 *      time to disappear, and the guest to stand still or run
 	 *	very slowly.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2432| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3446| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|5272| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	if (vcpu->tsc_catchup) {
 		u64 tsc = compute_guest_tsc(v, kernel_ns);
 		if (tsc > tsc_timestamp) {
@@ -3282,6 +3868,16 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 	if (use_master_clock)
 		hv_clock.flags |= PVCLOCK_TSC_STABLE_BIT;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2371| <<kvm_write_system_time>> kvm_gpc_activate(&vcpu->arch.pv_time, system_time & ~1ULL,
+	 *   - arch/x86/kvm/x86.c|2374| <<kvm_write_system_time>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|3704| <<kvm_guest_time_update>> if (vcpu->pv_time.active) {
+	 *   - arch/x86/kvm/x86.c|3723| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|4102| <<kvmclock_reset>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|6275| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|13065| <<kvm_arch_vcpu_create>> kvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm);
+	 */
 	if (vcpu->pv_time.active) {
 		/*
 		 * GUEST_STOPPED is only supported by kvmclock, and KVM's
@@ -3292,6 +3888,15 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 			hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
 			vcpu->pvclock_set_guest_stopped_request = false;
 		}
+		/*
+		 * 在以下使用kvm_setup_guest_pvclock():
+		 *   - arch/x86/kvm/x86.c|3570| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock,
+		 *          v, &vcpu->pv_time, 0);
+		 *   - arch/x86/kvm/x86.c|3590| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock,
+		 *          v, &vcpu->xen.vcpu_info_cache, offsetof(struct compat_vcpu_info, time));
+		 *   - arch/x86/kvm/x86.c|3593| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock,
+		 *          v, &vcpu->xen.vcpu_time_info_cache, 0);
+		 */
 		kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->pv_time, 0);
 
 		hv_clock.flags &= ~PVCLOCK_GUEST_STOPPED;
@@ -3311,6 +3916,15 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 	if (ka->xen.hvm_config.flags & KVM_XEN_HVM_CONFIG_PVCLOCK_TSC_UNSTABLE)
 		hv_clock.flags &= ~PVCLOCK_TSC_STABLE_BIT;
 
+	/*
+	 * 在以下使用kvm_setup_guest_pvclock():
+	 *   - arch/x86/kvm/x86.c|3570| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock,
+	 *          v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|3590| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock,
+	 *          v, &vcpu->xen.vcpu_info_cache, offsetof(struct compat_vcpu_info, time));
+	 *   - arch/x86/kvm/x86.c|3593| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock,
+	 *          v, &vcpu->xen.vcpu_time_info_cache, 0);
+	 */
 	if (vcpu->xen.vcpu_info_cache.active)
 		kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->xen.vcpu_info_cache,
 					offsetof(struct compat_vcpu_info, time));
@@ -3339,6 +3953,11 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
  * Fall back to using their values at slightly different moments by
  * calling ktime_get_real_ns() and get_kvmclock_ns() separately.
  */
+/*
+ * 在以下使用kvm_get_wall_clock_epoch():
+ *   - arch/x86/kvm/x86.c|2326| <<kvm_write_wall_clock>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+ *   - arch/x86/kvm/xen.c|63| <<kvm_xen_shared_info_init>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+ */
 uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -3349,6 +3968,19 @@ uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 	uint64_t host_tsc;
 
 	do {
+		/*
+		 * 在以下使用kvm_arch->pvclock_sc:
+		 *   - arch/x86/kvm/x86.c|3050| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3067| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3143| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3145| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|3223| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3229| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|3359| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3387| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|12822| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc,
+		 *        &kvm->arch.tsc_write_lock);
+		 */
 		seq = read_seqcount_begin(&ka->pvclock_sc);
 
 		local_tsc_khz = 0;
@@ -3374,6 +4006,14 @@ uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 		 * After that, it's just mathematics which can happen on any
 		 * CPU at any time.
 		 */
+		/*
+		 * 在以下使用kvm_arch->master_cycle_now:
+		 *   - arch/x86/kvm/x86.c|3177| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+		 *                                           &ka->master_kernel_ns, &ka->master_cycle_now);
+		 *   - arch/x86/kvm/x86.c|3450| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+		 *   - arch/x86/kvm/x86.c|3630| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+		 *   - arch/x86/kvm/x86.c|3867| <<kvm_get_wall_clock_epoch>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+		 */
 		hv_clock.tsc_timestamp = ka->master_cycle_now;
 		hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
 
@@ -3422,6 +4062,24 @@ static void kvmclock_update_fn(struct work_struct *work)
 	struct kvm_vcpu *vcpu;
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/cpuid.c|2016| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|3314| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3579| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3826| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3835| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|4314| <<kvm_set_msr_common(KVM_REQ_CLOCK_UPDATE)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5443| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|6148| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10089| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11324| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|11681| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|13189| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		kvm_vcpu_kick(vcpu);
 	}
@@ -3431,6 +4089,24 @@ static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 {
 	struct kvm *kvm = v->kvm;
 
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/cpuid.c|2016| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|3314| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3579| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3826| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3835| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|4314| <<kvm_set_msr_common(KVM_REQ_CLOCK_UPDATE)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5443| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|6148| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10089| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11324| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|11681| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|13189| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+	 */
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
@@ -3606,8 +4282,23 @@ static int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * 在以下使用kvmclock_reset():
+ *   - arch/x86/kvm/x86.c|13180| <<kvm_arch_vcpu_destroy>> kvmclock_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|13255| <<kvm_vcpu_reset>> kvmclock_reset(vcpu);
+ */
 static void kvmclock_reset(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2371| <<kvm_write_system_time>> kvm_gpc_activate(&vcpu->arch.pv_time, system_time & ~1ULL,
+	 *   - arch/x86/kvm/x86.c|2374| <<kvm_write_system_time>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|3704| <<kvm_guest_time_update>> if (vcpu->pv_time.active) {
+	 *   - arch/x86/kvm/x86.c|3723| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|4102| <<kvmclock_reset>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|6275| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|13065| <<kvm_arch_vcpu_create>> kvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm);
+	 */
 	kvm_gpc_deactivate(&vcpu->arch.pv_time);
 	vcpu->arch.time = 0;
 }
@@ -3910,6 +4601,26 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 				/* Before back to guest, tsc_timestamp must be adjusted
 				 * as well, otherwise guest's percpu pvclock time could jump.
 				 */
+				/*
+				 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+				 *   - arch/x86/kvm/cpuid.c|2016| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+				 *   - arch/x86/kvm/x86.c|3314| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|3579| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+				 *   - arch/x86/kvm/x86.c|3826| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|3835| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+				 *   - arch/x86/kvm/x86.c|4314| <<kvm_set_msr_common(KVM_REQ_CLOCK_UPDATE)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|5443| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|6148| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|10089| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|11324| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+				 *   - arch/x86/kvm/x86.c|11681| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|13189| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+				 *
+				 * 处理的函数kvm_guest_time_update()
+				 */
 				kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 			}
 			vcpu->arch.ia32_tsc_adjust_msr = data;
@@ -5039,6 +5750,26 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	if (unlikely(vcpu->arch.tsc_offset_adjustment)) {
 		adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
 		vcpu->arch.tsc_offset_adjustment = 0;
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/cpuid.c|2016| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|3314| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3579| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3826| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3835| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|4314| <<kvm_set_msr_common(KVM_REQ_CLOCK_UPDATE)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5443| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|6148| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10089| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11324| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|11681| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|13189| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+		 *
+		 * 处理的函数kvm_guest_time_update()
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 	}
 
@@ -5051,7 +5782,19 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 		if (kvm_check_tsc_unstable()) {
 			u64 offset = kvm_compute_l1_tsc_offset(vcpu,
 						vcpu->arch.last_guest_tsc);
+			/*
+			 * 在以下使用kvm_vcpu_write_tsc_offset():
+			 *   - arch/x86/kvm/x86.c|2699| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+			 *   - arch/x86/kvm/x86.c|2821| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+			 *   - arch/x86/kvm/x86.c|5231| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+			 */
 			kvm_vcpu_write_tsc_offset(vcpu, offset);
+			/*
+			 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+			 *   - arch/x86/kvm/x86.c|2432| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+			 *   - arch/x86/kvm/x86.c|3446| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+			 *   - arch/x86/kvm/x86.c|5272| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+			 */
 			if (!vcpu->arch.guest_tsc_protected)
 				vcpu->arch.tsc_catchup = 1;
 		}
@@ -5732,6 +6475,26 @@ static int kvm_set_guest_paused(struct kvm_vcpu *vcpu)
 	if (!vcpu->arch.pv_time.active)
 		return -EINVAL;
 	vcpu->arch.pvclock_set_guest_stopped_request = true;
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/cpuid.c|2016| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|3314| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3579| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3826| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3835| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|4314| <<kvm_set_msr_common(KVM_REQ_CLOCK_UPDATE)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5443| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|6148| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10089| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11324| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|11681| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|13189| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+	 *
+	 * 处理的函数kvm_guest_time_update()
+	 */
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 	return 0;
 }
@@ -5752,6 +6515,14 @@ static int kvm_arch_tsc_has_attr(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+/*
+ * struct kvm_device_attr {
+ *     __u32   flags;          // no flags currently defined
+ *     __u32   group;          // device-defined
+ *     __u64   attr;           // group-defined
+ *     __u64   addr;           // userspace address of attr data
+ * };
+ */
 static int kvm_arch_tsc_get_attr(struct kvm_vcpu *vcpu,
 				 struct kvm_device_attr *attr)
 {
@@ -5761,6 +6532,11 @@ static int kvm_arch_tsc_get_attr(struct kvm_vcpu *vcpu,
 	switch (attr->attr) {
 	case KVM_VCPU_TSC_OFFSET:
 		r = -EFAULT;
+		/*
+		 * 在以下设置kvm_vcpu_arch->l1_tsc_offset:
+		 *   - arch/x86/kvm/vmx/tdx.c|695| <<tdx_vcpu_create>> vcpu->arch.l1_tsc_offset = vcpu->arch.tsc_offset;
+		 *   - arch/x86/kvm/x86.c|2634| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = l1_offset;
+		 */
 		if (put_user(vcpu->arch.l1_tsc_offset, uaddr))
 			break;
 		r = 0;
@@ -5786,18 +6562,57 @@ static int kvm_arch_tsc_set_attr(struct kvm_vcpu *vcpu,
 		bool matched;
 
 		r = -EFAULT;
+		/*
+		 * 读出来offset!!!
+		 */
 		if (get_user(offset, uaddr))
 			break;
 
+		/*
+		 * 在以下使用kvm_arch->tsc_write_lock:
+		 *   - arch/x86/kvm/x86.c|2659| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+		 *   - arch/x86/kvm/x86.c|2716| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+		 *   - arch/x86/kvm/x86.c|2772| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+		 *   - arch/x86/kvm/x86.c|3017| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+		 *   - arch/x86/kvm/x86.c|3049| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+		 *   - arch/x86/kvm/x86.c|3068| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock)
+		 *   - arch/x86/kvm/x86.c|5799| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+		 *   - arch/x86/kvm/x86.c|5809| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+		 *   - arch/x86/kvm/x86.c|12820| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+		 */
 		raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
 
+		/*
+		 * 在以下使用kvm_arch->last_tsc_khz:
+		 *   - arch/x86/kvm/x86.c|2778| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+		 *   - arch/x86/kvm/x86.c|2892| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+		 *   - arch/x86/kvm/x86.c|6420| <<kvm_arch_tsc_set_attr>> kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+		 *
+		 * 在以下使用kvm_arch->last_tsc_offset:
+		 *   - arch/x86/kvm/x86.c|2779| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_offset = offset;
+		 *   - arch/x86/kvm/x86.c|6421| <<kvm_arch_tsc_set_attr>> kvm->arch.last_tsc_offset == offset);
+		 */
 		matched = (vcpu->arch.virtual_tsc_khz &&
 			   kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
 			   kvm->arch.last_tsc_offset == offset);
 
 		tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio) + offset;
+		/*
+		 * 在以下使用get_kvmclock_base_ns():
+		 *   - arch/x86/kvm/x86.c|2718| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|3131| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3241| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|5806| <<kvm_arch_tsc_set_attr>> ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|7071| <<kvm_vm_ioctl_set_clock>> now_raw_ns = get_kvmclock_base_ns();
+		 *   - arch/x86/kvm/x86.c|12823| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 */
 		ns = get_kvmclock_base_ns();
 
+		/*
+		 * 在以下使用__kvm_synchronize_tsc():
+		 *   - arch/x86/kvm/x86.c|2944| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched, !!user_value);
+		 *   - arch/x86/kvm/x86.c|6518| <<kvm_arch_tsc_set_attr>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched, true);
+		 */
 		__kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched, true);
 		raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 
@@ -7010,10 +7825,19 @@ int kvm_arch_pm_notifier(struct kvm *kvm, unsigned long state)
 }
 #endif /* CONFIG_HAVE_KVM_PM_NOTIFIER */
 
+/*
+ * 处理KVM_GET_CLOCK:
+ *   - arch/x86/kvm/x86.c|7604| <<kvm_arch_vm_ioctl>> r = kvm_vm_ioctl_get_clock(kvm, argp);
+ */
 static int kvm_vm_ioctl_get_clock(struct kvm *kvm, void __user *argp)
 {
 	struct kvm_clock_data data = { 0 };
 
+	/*
+	 * 在以下使用get_kvmclock():
+	 *   - arch/x86/kvm/x86.c|3337| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+	 *   - arch/x86/kvm/x86.c|7283| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+	 */
 	get_kvmclock(kvm, &data);
 	if (copy_to_user(argp, &data, sizeof(data)))
 		return -EFAULT;
@@ -7037,8 +7861,25 @@ static int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)
 	if (data.flags & ~KVM_CLOCK_VALID_FLAGS)
 		return -EINVAL;
 
+	/*
+	 * 在以下使用kvm_hv_request_tsc_page_update():
+	 *   - arch/x86/kvm/x86.c|3274| <<kvm_update_masterclock>> kvm_hv_request_tsc_page_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7402| <<kvm_vm_ioctl_set_clock>> kvm_hv_request_tsc_page_update(kvm);
+	 */
 	kvm_hv_request_tsc_page_update(kvm);
+	/*
+	 * 在以下使用kvm_start_pvclock_update():
+	 *   - arch/x86/kvm/x86.c|3184| <<kvm_update_masterclock>> kvm_start_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7239| <<kvm_vm_ioctl_set_clock>> kvm_start_pvclock_update(kvm);
+	 */
 	kvm_start_pvclock_update(kvm);
+	/*
+	 * 在以下使用pvclock_update_vm_gtod_copy():
+	 *   - arch/x86/kvm/x86.c|3074| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|7042| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|9520| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|12819| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
 
 	/*
@@ -7049,6 +7890,10 @@ static int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)
 	 * 'system_time' when 'data.clock' is very small.
 	 */
 	if (data.flags & KVM_CLOCK_REALTIME) {
+		/*
+		 * 系统实时时钟(wall clock)的当前时间,以纳秒(nanoseconds)为单位,类型是u64.
+		 * 这个时间从UNIX Epoch(1970-01-01 00:00:00 UTC)开始计算.
+		 */
 		u64 now_real_ns = ktime_get_real_ns();
 
 		/*
@@ -7058,11 +7903,36 @@ static int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)
 			data.clock += now_real_ns - data.realtime;
 	}
 
+	/*
+	 * 在以下使用get_kvmclock_base_ns():
+	 *   - arch/x86/kvm/x86.c|2718| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|3131| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3241| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|5806| <<kvm_arch_tsc_set_attr>> ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|7071| <<kvm_vm_ioctl_set_clock>> now_raw_ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|12823| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	if (ka->use_master_clock)
 		now_raw_ns = ka->master_kernel_ns;
 	else
 		now_raw_ns = get_kvmclock_base_ns();
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|572| <<kvm_pmu_rdpmc_vmware>> ctr_val = ktime_get_boottime_ns() + vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3279| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3294| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3497| <<kvm_guest_time_update>> hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3611| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7340| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|13130| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	ka->kvmclock_offset = data.clock - now_raw_ns;
+	/*
+	 * 在以下使用kvm_end_pvclock_update():
+	 *   - arch/x86/kvm/x86.c|3277| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7437| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|9897| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+	 */
 	kvm_end_pvclock_update(kvm);
 	return 0;
 }
@@ -9502,6 +10372,11 @@ static void kvm_hyperv_tsc_notifier(void)
 	int cpu;
 
 	mutex_lock(&kvm_lock);
+	/*
+	 * 在以下使用kvm_make_mclock_inprogress_request():
+	 *   - arch/x86/kvm/x86.c|3220| <<kvm_start_pvclock_update>> kvm_make_mclock_inprogress_request(kvm);
+	 *   - arch/x86/kvm/x86.c|9923| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+	 */
 	list_for_each_entry(kvm, &vm_list, vm_list)
 		kvm_make_mclock_inprogress_request(kvm);
 
@@ -9516,8 +10391,26 @@ static void kvm_hyperv_tsc_notifier(void)
 	kvm_caps.max_guest_tsc_khz = tsc_khz;
 
 	list_for_each_entry(kvm, &vm_list, vm_list) {
+		/*
+		 * 在以下使用__kvm_start_pvclock_update():
+		 *   - arch/x86/kvm/x86.c|3160| <<kvm_start_pvclock_update>> __kvm_start_pvclock_update(kvm);
+		 *   - arch/x86/kvm/x86.c|9755| <<kvm_hyperv_tsc_notifier>> __kvm_start_pvclock_update(kvm);
+		 */
 		__kvm_start_pvclock_update(kvm);
+		/*
+		 * 在以下使用pvclock_update_vm_gtod_copy():
+		 *   - arch/x86/kvm/x86.c|3074| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+		 *   - arch/x86/kvm/x86.c|7042| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+		 *   - arch/x86/kvm/x86.c|9520| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+		 *   - arch/x86/kvm/x86.c|12819| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+		 */
 		pvclock_update_vm_gtod_copy(kvm);
+		/*
+		 * 在以下使用kvm_end_pvclock_update():
+		 *   - arch/x86/kvm/x86.c|3277| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+		 *   - arch/x86/kvm/x86.c|7437| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+		 *   - arch/x86/kvm/x86.c|9897| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+		 */
 		kvm_end_pvclock_update(kvm);
 	}
 
@@ -9578,6 +10471,26 @@ static void __kvmclock_cpufreq_notifier(struct cpufreq_freqs *freq, int cpu)
 		kvm_for_each_vcpu(i, vcpu, kvm) {
 			if (vcpu->cpu != cpu)
 				continue;
+			/*
+			 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+			 *   - arch/x86/kvm/cpuid.c|2016| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+			 *   - arch/x86/kvm/x86.c|3314| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|3579| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+			 *   - arch/x86/kvm/x86.c|3826| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|3835| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+			 *   - arch/x86/kvm/x86.c|4314| <<kvm_set_msr_common(KVM_REQ_CLOCK_UPDATE)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|5443| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|6148| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|10089| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|11324| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+			 *   - arch/x86/kvm/x86.c|11681| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|13189| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+			 *
+			 * 处理的函数kvm_guest_time_update()
+			 */
 			kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 			if (vcpu->cpu != raw_smp_processor_id())
 				send_ipi = 1;
@@ -10605,6 +11518,11 @@ void kvm_make_scan_ioapic_request(struct kvm *kvm)
 	kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
 }
 
+/*
+ * 在以下使用__kvm_vcpu_update_apicv():
+ *   - arch/x86/kvm/svm/nested.c|1201| <<nested_svm_vmexit>> __kvm_vcpu_update_apicv(vcpu);
+ *   - arch/x86/kvm/x86.c|11578| <<kvm_vcpu_update_apicv>> __kvm_vcpu_update_apicv(vcpu);
+ */
 void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -10613,6 +11531,19 @@ void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	if (!lapic_in_kernel(vcpu))
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10951| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|11529| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|11554| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|11586| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|11628| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|11630| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|13085| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|13094| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_read(&vcpu->kvm->arch.apicv_update_lock);
 	preempt_disable();
 
@@ -10807,6 +11738,26 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		}
 		if (kvm_check_request(KVM_REQ_MMU_FREE_OBSOLETE_ROOTS, vcpu))
 			kvm_mmu_free_obsolete_roots(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/cpuid.c|2016| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|3314| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3579| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3826| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3835| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|4314| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5443| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|6148| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10089| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11324| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|11681| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|13189| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+		 *
+		 * 处理的函数kvm_guest_time_update()
+		 */
 		if (kvm_check_request(KVM_REQ_MIGRATE_TIMER, vcpu))
 			__kvm_migrate_timers(vcpu);
 		if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
@@ -11169,6 +12120,26 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		profile_hit(KVM_PROFILING, (void *)rip);
 	}
 
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/cpuid.c|2016| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|3314| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3579| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3826| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3835| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|4314| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5443| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|6148| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10089| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11324| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|11681| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|13189| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+	 *
+	 * 处理的函数kvm_guest_time_update()
+	 */
 	if (unlikely(vcpu->arch.tsc_always_catchup))
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
@@ -12340,6 +13311,16 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	vcpu->arch.regs_avail = ~0;
 	vcpu->arch.regs_dirty = ~0;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2371| <<kvm_write_system_time>> kvm_gpc_activate(&vcpu->arch.pv_time, system_time & ~1ULL,
+	 *   - arch/x86/kvm/x86.c|2374| <<kvm_write_system_time>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|3704| <<kvm_guest_time_update>> if (vcpu->pv_time.active) {
+	 *   - arch/x86/kvm/x86.c|3723| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|4102| <<kvmclock_reset>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|6275| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|13065| <<kvm_arch_vcpu_create>> kvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm);
+	 */
 	kvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm);
 
 	if (!irqchip_in_kernel(vcpu->kvm) || kvm_vcpu_is_reset_bsp(vcpu))
@@ -12479,6 +13460,13 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	kvfree(vcpu->arch.cpuid_entries);
 }
 
+/*
+ * 在以下使用kvm_vcpu_reset():
+ *   - arch/x86/kvm/lapic.c|3426| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/svm/svm.c|2264| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/x86.c|11644| <<vcpu_enter_guest>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/x86.c|13131| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+ */
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_cpuid_entry2 *cpuid_0x1;
@@ -12741,7 +13729,19 @@ int kvm_arch_enable_virtualization_cpu(void)
 			 * you may have some problem.  Solving this issue is
 			 * left as an exercise to the reader.
 			 */
+			/*
+			 * 在以下使用kvm_arch->last_tsc_nsec:
+			 *   - arch/x86/kvm/x86.c|2776| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+			 *   - arch/x86/kvm/x86.c|2856| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+			 *   - arch/x86/kvm/x86.c|13575| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_nsec = 0;
+			 */
 			kvm->arch.last_tsc_nsec = 0;
+			/*
+			 * 在以下使用kvm_arch->arch.last_tsc_write:
+			 *   - arch/x86/kvm/x86.c|2777| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+			 *   - arch/x86/kvm/x86.c|2866| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+			 *   - arch/x86/kvm/x86.c|13576| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_write = 0;
+			 */
 			kvm->arch.last_tsc_write = 0;
 		}
 
@@ -12810,12 +13810,53 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	set_bit(KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
 		&kvm->arch.irq_sources_bitmap);
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2659| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2716| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2772| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3017| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3049| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3068| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock)
+	 *   - arch/x86/kvm/x86.c|5799| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5809| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|12820| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spin_lock_init(&kvm->arch.tsc_write_lock);
 	mutex_init(&kvm->arch.apic_map_lock);
+	/*
+	 * 在以下使用kvm_arch->pvclock_sc:
+	 *   - arch/x86/kvm/x86.c|3050| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3067| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3143| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3145| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3223| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3229| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3359| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3387| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|12822| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc,
+	 *        &kvm->arch.tsc_write_lock);
+	 */
 	seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	/*
+	 * 在以下使用get_kvmclock_base_ns():
+	 *   - arch/x86/kvm/x86.c|2718| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|3131| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3241| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|5806| <<kvm_arch_tsc_set_attr>> ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|7071| <<kvm_vm_ioctl_set_clock>> now_raw_ns = get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|12823| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
 
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	/*
+	 * 在以下使用pvclock_update_vm_gtod_copy():
+	 *   - arch/x86/kvm/x86.c|3074| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|7042| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|9520| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|12819| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 
diff --git a/arch/x86/kvm/xen.c b/arch/x86/kvm/xen.c
index d6b2a665b..165393114 100644
--- a/arch/x86/kvm/xen.c
+++ b/arch/x86/kvm/xen.c
@@ -60,6 +60,11 @@ static int kvm_xen_shared_info_init(struct kvm *kvm)
 	 * This code mirrors kvm_write_wall_clock() except that it writes
 	 * directly through the pfn cache and doesn't mark the page dirty.
 	 */
+	/*
+	 *  在以下使用kvm_get_wall_clock_epoch():
+	 *   - arch/x86/kvm/x86.c|2326| <<kvm_write_wall_clock>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+	 *   - arch/x86/kvm/xen.c|63| <<kvm_xen_shared_info_init>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+	 */
 	wall_nsec = kvm_get_wall_clock_epoch(kvm);
 
 	/* Paranoia checks on the 32-bit struct layout */
@@ -584,6 +589,13 @@ static void kvm_xen_update_runstate_guest(struct kvm_vcpu *v, bool atomic)
 	read_unlock_irqrestore(&gpc1->lock, flags);
 }
 
+/*
+ * 在以下使用():
+ *   - arch/x86/kvm/xen.c|1018| <<kvm_xen_vcpu_set_attr>> kvm_xen_update_runstate(vcpu, data->u.runstate.state);
+ *   - arch/x86/kvm/xen.c|1099| <<kvm_xen_vcpu_set_attr>> kvm_xen_update_runstate(vcpu, data->u.runstate.state);
+ *   - arch/x86/kvm/xen.h|197| <<kvm_xen_runstate_set_running>> kvm_xen_update_runstate(vcpu, RUNSTATE_running);
+ *   - arch/x86/kvm/xen.h|212| <<kvm_xen_runstate_set_preempted>> kvm_xen_update_runstate(vcpu, RUNSTATE_runnable);
+ */
 void kvm_xen_update_runstate(struct kvm_vcpu *v, int state)
 {
 	struct kvm_vcpu_xen *vx = &v->arch.xen;
diff --git a/arch/x86/net/bpf_jit_comp.c b/arch/x86/net/bpf_jit_comp.c
index 15672cb92..973544e81 100644
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@ -667,6 +667,18 @@ static void emit_indirect_jump(u8 **pprog, int reg, u8 *ip)
 		OPTIMIZER_HIDE_VAR(reg);
 		emit_jump(&prog, its_static_thunk(reg), ip);
 	} else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		/*
+		 * 在以下使用X86_FEATURE_RETPOLINE_LFENCE:
+		 *   - arch/x86/include/asm/nospec-branch.h|514| <<CALL_NOSPEC(32位)>> X86_FEATURE_RETPOLINE_LFENCE)
+		 *   - arch/x86/kernel/alternative.c|871| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+		 *                     !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		 *   - arch/x86/kernel/alternative.c|907| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2384| <<bhi_apply_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *                     !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2556| <<spectre_v2_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE_LFENCE);
+		 *   - arch/x86/kernel/cpu/bugs.c|3685| <<spectre_bhi_state>> !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE) &&
+		 *   - arch/x86/net/bpf_jit_comp.c|669| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		 */
 		EMIT_LFENCE();
 		EMIT2(0xFF, 0xE0 + reg);
 	} else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
@@ -1513,6 +1525,14 @@ static int emit_spectre_bhb_barrier(u8 **pprog, u8 *ip,
 	u8 *prog = *pprog;
 	u8 *func;
 
+	/*
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_LOOP:
+	 *   - arch/x86/include/asm/nospec-branch.h|332| <<CLEAR_BRANCH_HISTORY>> ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
+	 *   - arch/x86/kernel/cpu/bugs.c|2141| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+	 *   - arch/x86/kernel/cpu/bugs.c|3307| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_LOOP))
+	 *   - arch/x86/net/bpf_jit_comp.c|1516| <<emit_spectre_bhb_barrier>> if (cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP)) {
+	 *   - arch/x86/net/bpf_jit_comp.c|1531| <<emit_spectre_bhb_barrier>> if ((cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP) &&
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP)) {
 		/* The clearing sequence clobbers eax and ecx. */
 		EMIT1(0x50); /* push rax */
@@ -1527,6 +1547,19 @@ static int emit_spectre_bhb_barrier(u8 **pprog, u8 *ip,
 		EMIT1(0x59); /* pop rcx */
 		EMIT1(0x58); /* pop rax */
 	}
+	/*
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_LOOP:
+	 *   - arch/x86/include/asm/nospec-branch.h|332| <<CLEAR_BRANCH_HISTORY>> ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
+	 *   - arch/x86/kernel/cpu/bugs.c|2141| <<bhi_apply_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+	 *   - arch/x86/kernel/cpu/bugs.c|3307| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_LOOP))
+	 *   - arch/x86/net/bpf_jit_comp.c|1516| <<emit_spectre_bhb_barrier>> if (cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP)) {
+	 *   - arch/x86/net/bpf_jit_comp.c|1531| <<emit_spectre_bhb_barrier>> if ((cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP) &&
+	 *
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_HW:
+	 *   - arch/x86/kernel/cpu/bugs.c|2062| <<spec_ctrl_bhi_dis>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_HW);
+	 *   - arch/x86/kernel/cpu/bugs.c|3305| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_HW))
+	 *   - arch/x86/net/bpf_jit_comp.c|1533| <<emit_spectre_bhb_barrier>> cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_HW)) {
+	 */
 	/* Insert IBHF instruction */
 	if ((cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP) &&
 	     cpu_feature_enabled(X86_FEATURE_HYPERVISOR)) ||
diff --git a/drivers/infiniband/core/device.c b/drivers/infiniband/core/device.c
index d42633858..7a1fecc2f 100644
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@ -1813,6 +1813,29 @@ static void remove_client_id(struct ib_client *client)
  * ib_register_client() is called, the client will receive an add
  * callback for all devices already registered.
  */
+/*
+ * 在以下使用ib_register_client():
+ *   - drivers/infiniband/core/cm.c|4492| <<ib_cm_init>> ret = ib_register_client(&cm_client);
+ *   - drivers/infiniband/core/cma.c|5474| <<cma_init>> ret = ib_register_client(&cma_client);
+ *   - drivers/infiniband/core/mad.c|3151| <<ib_mad_init>> if (ib_register_client(&mad_client)) {
+ *   - drivers/infiniband/core/multicast.c|890| <<mcast_init>> ret = ib_register_client(&mcast_client);
+ *   - drivers/infiniband/core/sa_query.c|2261| <<ib_sa_init>> ret = ib_register_client(&sa_client);
+ *   - drivers/infiniband/core/ucma.c|1866| <<ucma_init>> ret = ib_register_client(&rdma_cma_client);
+ *   - drivers/infiniband/core/user_mad.c|1477| <<ib_umad_init>> ret = ib_register_client(&umad_client);
+ *   - drivers/infiniband/core/user_mad.c|1481| <<ib_umad_init>> ret = ib_register_client(&issm_client);
+ *   - drivers/infiniband/core/uverbs_main.c|1318| <<ib_uverbs_init>> ret = ib_register_client(&uverbs_client);
+ *   - drivers/infiniband/ulp/ipoib/ipoib_main.c|2736| <<ipoib_init_module>> ret = ib_register_client(&ipoib_client);
+ *   - drivers/infiniband/ulp/opa_vnic/opa_vnic_vema.c|1040| <<opa_vnic_init>> rc = ib_register_client(&opa_vnic_client);
+ *   - drivers/infiniband/ulp/rtrs/rtrs-srv.c|2202| <<rtrs_srv_open>> err = ib_register_client(&rtrs_srv_client);
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|4190| <<srp_init_module>> ret = ib_register_client(&srp_client);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3965| <<srpt_init_module>> ret = ib_register_client(&srpt_client);
+ *   - drivers/nvme/host/rdma.c|2401| <<nvme_rdma_init_module>> ret = ib_register_client(&nvme_rdma_ib_client);
+ *   - drivers/nvme/target/rdma.c|2102| <<nvmet_rdma_init>> ret = ib_register_client(&nvmet_rdma_ib_client);
+ *   - fs/smb/server/transport_rdma.c|2195| <<ksmbd_rdma_init>> ret = ib_register_client(&smb_direct_ib_client);
+ *   - net/rds/ib.c|576| <<rds_ib_init>> ret = ib_register_client(&rds_ib_client);
+ *   - net/smc/smc_ib.c|1012| <<smc_ib_register_client>> return ib_register_client(&smc_ib_client);
+ *   - net/sunrpc/xprtrdma/ib_client.c|183| <<rpcrdma_ib_client_register>> return ib_register_client(&rpcrdma_ib_client);
+ */
 int ib_register_client(struct ib_client *client)
 {
 	struct ib_device *device;
diff --git a/drivers/nvme/host/fabrics.c b/drivers/nvme/host/fabrics.c
index 2e58a7ce1..dc40431d9 100644
--- a/drivers/nvme/host/fabrics.c
+++ b/drivers/nvme/host/fabrics.c
@@ -612,6 +612,13 @@ EXPORT_SYMBOL_GPL(nvmf_should_reconnect);
  * being implemented to the common NVMe fabrics library. Part of
  * the overall init sequence of starting up a fabrics driver.
  */
+/*
+ * 在以下使用nvmf_register_transport():
+ *   - drivers/nvme/host/fc.c|3904| <<nvme_fc_init_module>> ret = nvmf_register_transport(&nvme_fc_transport);
+ *   - drivers/nvme/host/rdma.c|2405| <<nvme_rdma_init_module>> ret = nvmf_register_transport(&nvme_rdma_transport);
+ *   - drivers/nvme/host/tcp.c|3038| <<nvme_tcp_init_module>> nvmf_register_transport(&nvme_tcp_transport);
+ *   - drivers/nvme/target/loop.c|697| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+ */
 int nvmf_register_transport(struct nvmf_transport_ops *ops)
 {
 	if (!ops->create_ctrl)
diff --git a/drivers/nvme/host/fc.c b/drivers/nvme/host/fc.c
index 014b387f1..481aa6709 100644
--- a/drivers/nvme/host/fc.c
+++ b/drivers/nvme/host/fc.c
@@ -3901,6 +3901,13 @@ static int __init nvme_fc_init_module(void)
 		goto out_destroy_class;
 	}
 
+	/*
+	 * 在以下使用nvmf_register_transport():
+	 *   - drivers/nvme/host/fc.c|3904| <<nvme_fc_init_module>> ret = nvmf_register_transport(&nvme_fc_transport);
+	 *   - drivers/nvme/host/rdma.c|2405| <<nvme_rdma_init_module>> ret = nvmf_register_transport(&nvme_rdma_transport);
+	 *   - drivers/nvme/host/tcp.c|3038| <<nvme_tcp_init_module>> nvmf_register_transport(&nvme_tcp_transport);
+	 *   - drivers/nvme/target/loop.c|697| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+	 */
 	ret = nvmf_register_transport(&nvme_fc_transport);
 	if (ret)
 		goto out_destroy_device;
diff --git a/drivers/nvme/host/rdma.c b/drivers/nvme/host/rdma.c
index 9bd364656..a74abf61e 100644
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@ -1154,6 +1154,12 @@ static void nvme_rdma_error_recovery(struct nvme_rdma_ctrl *ctrl)
 	queue_work(nvme_reset_wq, &ctrl->err_work);
 }
 
+/*
+ * 在以下使用nvme_rdma_end_request():
+ *   - drivers/nvme/host/rdma.c|1195| <<nvme_rdma_inv_rkey_done>> nvme_rdma_end_request(req);
+ *   - drivers/nvme/host/rdma.c|1588| <<nvme_rdma_send_done>> nvme_rdma_end_request(req);
+ *   - drivers/nvme/host/rdma.c|1731| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_end_request(req);
+ */
 static void nvme_rdma_end_request(struct nvme_rdma_request *req)
 {
 	struct request *rq = blk_mq_rq_from_pdu(req);
@@ -1619,6 +1625,11 @@ static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * 在以下使用nvme_rdma_post_recv():
+ *   - drivers/nvme/host/rdma.c|1771| <<nvme_rdma_recv_done>> nvme_rdma_post_recv(queue, qe);
+ *   - drivers/nvme/host/rdma.c|1779| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+ */
 static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe)
 {
@@ -1687,6 +1698,10 @@ static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 	WARN_ON_ONCE(ret);
 }
 
+/*
+ * 在以下使用nvme_rdma_process_nvme_rsp():
+ *   - drivers/nvme/host/rdma.c|1768| <<nvme_rdma_recv_done>> nvme_rdma_process_nvme_rsp(queue, cqe, wc);
+ */
 static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		struct nvme_completion *cqe, struct ib_wc *wc)
 {
@@ -1731,6 +1746,10 @@ static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 	nvme_rdma_end_request(req);
 }
 
+/*
+ * 在以下使用nvme_rdma_recv_done():
+ *   - drivers/nvme/host/rdma.c|1633| <<nvme_rdma_post_recv>> qe->cqe.done = nvme_rdma_recv_done;
+ */
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1771,6 +1790,10 @@ static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 	nvme_rdma_post_recv(queue, qe);
 }
 
+/*
+ * 在以下使用nvme_rdma_conn_established():
+ *   - drivers/nvme/host/rdma.c|1904| <<nvme_rdma_cm_handler>> queue->cm_error = nvme_rdma_conn_established(queue);
+ */
 static int nvme_rdma_conn_established(struct nvme_rdma_queue *queue)
 {
 	int ret, i;
@@ -2398,10 +2421,40 @@ static int __init nvme_rdma_init_module(void)
 {
 	int ret;
 
+	/*
+	 * 在以下使用ib_register_client():
+	 *   - drivers/infiniband/core/cm.c|4492| <<ib_cm_init>> ret = ib_register_client(&cm_client);
+	 *   - drivers/infiniband/core/cma.c|5474| <<cma_init>> ret = ib_register_client(&cma_client);
+	 *   - drivers/infiniband/core/mad.c|3151| <<ib_mad_init>> if (ib_register_client(&mad_client)) {
+	 *   - drivers/infiniband/core/multicast.c|890| <<mcast_init>> ret = ib_register_client(&mcast_client);
+	 *   - drivers/infiniband/core/sa_query.c|2261| <<ib_sa_init>> ret = ib_register_client(&sa_client);
+	 *   - drivers/infiniband/core/ucma.c|1866| <<ucma_init>> ret = ib_register_client(&rdma_cma_client);
+	 *   - drivers/infiniband/core/user_mad.c|1477| <<ib_umad_init>> ret = ib_register_client(&umad_client);
+	 *   - drivers/infiniband/core/user_mad.c|1481| <<ib_umad_init>> ret = ib_register_client(&issm_client);
+	 *   - drivers/infiniband/core/uverbs_main.c|1318| <<ib_uverbs_init>> ret = ib_register_client(&uverbs_client);
+	 *   - drivers/infiniband/ulp/ipoib/ipoib_main.c|2736| <<ipoib_init_module>> ret = ib_register_client(&ipoib_client);
+	 *   - drivers/infiniband/ulp/opa_vnic/opa_vnic_vema.c|1040| <<opa_vnic_init>> rc = ib_register_client(&opa_vnic_client);
+	 *   - drivers/infiniband/ulp/rtrs/rtrs-srv.c|2202| <<rtrs_srv_open>> err = ib_register_client(&rtrs_srv_client);
+	 *   - drivers/infiniband/ulp/srp/ib_srp.c|4190| <<srp_init_module>> ret = ib_register_client(&srp_client);
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3965| <<srpt_init_module>> ret = ib_register_client(&srpt_client);
+	 *   - drivers/nvme/host/rdma.c|2401| <<nvme_rdma_init_module>> ret = ib_register_client(&nvme_rdma_ib_client);
+	 *   - drivers/nvme/target/rdma.c|2102| <<nvmet_rdma_init>> ret = ib_register_client(&nvmet_rdma_ib_client);
+	 *   - fs/smb/server/transport_rdma.c|2195| <<ksmbd_rdma_init>> ret = ib_register_client(&smb_direct_ib_client);
+	 *   - net/rds/ib.c|576| <<rds_ib_init>> ret = ib_register_client(&rds_ib_client);
+	 *   - net/smc/smc_ib.c|1012| <<smc_ib_register_client>> return ib_register_client(&smc_ib_client);
+	 *   - net/sunrpc/xprtrdma/ib_client.c|183| <<rpcrdma_ib_client_register>> return ib_register_client(&rpcrdma_ib_client);
+	 */
 	ret = ib_register_client(&nvme_rdma_ib_client);
 	if (ret)
 		return ret;
 
+	/*
+	 * 在以下使用nvmf_register_transport():
+	 *   - drivers/nvme/host/fc.c|3904| <<nvme_fc_init_module>> ret = nvmf_register_transport(&nvme_fc_transport);
+	 *   - drivers/nvme/host/rdma.c|2405| <<nvme_rdma_init_module>> ret = nvmf_register_transport(&nvme_rdma_transport);
+	 *   - drivers/nvme/host/tcp.c|3038| <<nvme_tcp_init_module>> nvmf_register_transport(&nvme_tcp_transport);
+	 *   - drivers/nvme/target/loop.c|697| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+	 */
 	ret = nvmf_register_transport(&nvme_rdma_transport);
 	if (ret)
 		goto err_unreg_client;
diff --git a/drivers/nvme/host/tcp.c b/drivers/nvme/host/tcp.c
index d924008c3..2392664c8 100644
--- a/drivers/nvme/host/tcp.c
+++ b/drivers/nvme/host/tcp.c
@@ -3035,6 +3035,13 @@ static int __init nvme_tcp_init_module(void)
 	for_each_possible_cpu(cpu)
 		atomic_set(&nvme_tcp_cpu_queues[cpu], 0);
 
+	/*
+	 * 在以下使用nvmf_register_transport():
+	 *   - drivers/nvme/host/fc.c|3904| <<nvme_fc_init_module>> ret = nvmf_register_transport(&nvme_fc_transport);
+	 *   - drivers/nvme/host/rdma.c|2405| <<nvme_rdma_init_module>> ret = nvmf_register_transport(&nvme_rdma_transport);
+	 *   - drivers/nvme/host/tcp.c|3038| <<nvme_tcp_init_module>> nvmf_register_transport(&nvme_tcp_transport);
+	 *   - drivers/nvme/target/loop.c|697| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+	 */
 	nvmf_register_transport(&nvme_tcp_transport);
 	return 0;
 }
diff --git a/drivers/nvme/target/loop.c b/drivers/nvme/target/loop.c
index f85a8441b..dff5ec24c 100644
--- a/drivers/nvme/target/loop.c
+++ b/drivers/nvme/target/loop.c
@@ -694,6 +694,13 @@ static int __init nvme_loop_init_module(void)
 	if (ret)
 		return ret;
 
+	/*
+	 * 在以下使用nvmf_register_transport():
+	 *   - drivers/nvme/host/fc.c|3904| <<nvme_fc_init_module>> ret = nvmf_register_transport(&nvme_fc_transport);
+	 *   - drivers/nvme/host/rdma.c|2405| <<nvme_rdma_init_module>> ret = nvmf_register_transport(&nvme_rdma_transport);
+	 *   - drivers/nvme/host/tcp.c|3038| <<nvme_tcp_init_module>> nvmf_register_transport(&nvme_tcp_transport);
+	 *   - drivers/nvme/target/loop.c|697| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+	 */
 	ret = nvmf_register_transport(&nvme_loop_transport);
 	if (ret)
 		nvmet_unregister_transport(&nvme_loop_ops);
diff --git a/drivers/nvme/target/rdma.c b/drivers/nvme/target/rdma.c
index 67f61c67c..1f5d3fccd 100644
--- a/drivers/nvme/target/rdma.c
+++ b/drivers/nvme/target/rdma.c
@@ -2099,6 +2099,29 @@ static int __init nvmet_rdma_init(void)
 {
 	int ret;
 
+	/*
+	 * 在以下使用ib_register_client():
+	 *   - drivers/infiniband/core/cm.c|4492| <<ib_cm_init>> ret = ib_register_client(&cm_client);
+	 *   - drivers/infiniband/core/cma.c|5474| <<cma_init>> ret = ib_register_client(&cma_client);
+	 *   - drivers/infiniband/core/mad.c|3151| <<ib_mad_init>> if (ib_register_client(&mad_client)) {
+	 *   - drivers/infiniband/core/multicast.c|890| <<mcast_init>> ret = ib_register_client(&mcast_client);
+	 *   - drivers/infiniband/core/sa_query.c|2261| <<ib_sa_init>> ret = ib_register_client(&sa_client);
+	 *   - drivers/infiniband/core/ucma.c|1866| <<ucma_init>> ret = ib_register_client(&rdma_cma_client);
+	 *   - drivers/infiniband/core/user_mad.c|1477| <<ib_umad_init>> ret = ib_register_client(&umad_client);
+	 *   - drivers/infiniband/core/user_mad.c|1481| <<ib_umad_init>> ret = ib_register_client(&issm_client);
+	 *   - drivers/infiniband/core/uverbs_main.c|1318| <<ib_uverbs_init>> ret = ib_register_client(&uverbs_client);
+	 *   - drivers/infiniband/ulp/ipoib/ipoib_main.c|2736| <<ipoib_init_module>> ret = ib_register_client(&ipoib_client);
+	 *   - drivers/infiniband/ulp/opa_vnic/opa_vnic_vema.c|1040| <<opa_vnic_init>> rc = ib_register_client(&opa_vnic_client);
+	 *   - drivers/infiniband/ulp/rtrs/rtrs-srv.c|2202| <<rtrs_srv_open>> err = ib_register_client(&rtrs_srv_client);
+	 *   - drivers/infiniband/ulp/srp/ib_srp.c|4190| <<srp_init_module>> ret = ib_register_client(&srp_client);
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3965| <<srpt_init_module>> ret = ib_register_client(&srpt_client);
+	 *   - drivers/nvme/host/rdma.c|2401| <<nvme_rdma_init_module>> ret = ib_register_client(&nvme_rdma_ib_client);
+	 *   - drivers/nvme/target/rdma.c|2102| <<nvmet_rdma_init>> ret = ib_register_client(&nvmet_rdma_ib_client);
+	 *   - fs/smb/server/transport_rdma.c|2195| <<ksmbd_rdma_init>> ret = ib_register_client(&smb_direct_ib_client);
+	 *   - net/rds/ib.c|576| <<rds_ib_init>> ret = ib_register_client(&rds_ib_client);
+	 *   - net/smc/smc_ib.c|1012| <<smc_ib_register_client>> return ib_register_client(&smc_ib_client);
+	 *   - net/sunrpc/xprtrdma/ib_client.c|183| <<rpcrdma_ib_client_register>> return ib_register_client(&rpcrdma_ib_client);
+	 */
 	ret = ib_register_client(&nvmet_rdma_ib_client);
 	if (ret)
 		return ret;
diff --git a/drivers/ptp/ptp_vmclock.c b/drivers/ptp/ptp_vmclock.c
index b3a83b03d..3bf91011d 100644
--- a/drivers/ptp/ptp_vmclock.c
+++ b/drivers/ptp/ptp_vmclock.c
@@ -88,6 +88,12 @@ static bool tai_adjust(struct vmclock_abi *clk, uint64_t *sec)
 	return false;
 }
 
+/*
+ * 在以下使用vmclock_get_crosststamp():
+ *   - drivers/ptp/ptp_vmclock.c|205| <<vmclock_get_crosststamp_kvmclock>> ret = vmclock_get_crosststamp(st, sts, system_counter, tspec);
+ *   - drivers/ptp/ptp_vmclock.c|244| <<ptp_vmclock_get_time_fn>> ret = vmclock_get_crosststamp(st, NULL, system_counter, &tspec);
+ *   - drivers/ptp/ptp_vmclock.c|307| <<ptp_vmclock_gettimex>> return vmclock_get_crosststamp(st, sts, NULL, ts);
+ */
 static int vmclock_get_crosststamp(struct vmclock_state *st,
 				   struct ptp_system_timestamp *sts,
 				   struct system_counterval_t *system_counter,
@@ -327,6 +333,10 @@ static const struct ptp_clock_info ptp_vmclock_info = {
 	.getcrosststamp = ptp_vmclock_getcrosststamp,
 };
 
+/*
+ * 在以下使用vmclock_ptp_register():
+ *   - drivers/ptp/ptp_vmclock.c|570| <<vmclock_probe>> st->ptp_clock = vmclock_ptp_register(dev, st);
+ */
 static struct ptp_clock *vmclock_ptp_register(struct device *dev,
 					      struct vmclock_state *st)
 {
@@ -514,6 +524,18 @@ static int vmclock_probe(struct platform_device *pdev)
 			 resource_size(&st->res));
 		return -EINVAL;
 	}
+	/*
+	 * struct vmclock_state {
+	 *     struct resource res;
+	 *     struct vmclock_abi *clk;
+	 *     struct miscdevice miscdev;
+	 *     struct ptp_clock_info ptp_clock_info;
+	 *     struct ptp_clock *ptp_clock;
+	 *     enum clocksource_ids cs_id, sys_cs_id;
+	 *     int index;
+	 *     char *name;
+	 * };
+	 */
 	st->clk = devm_memremap(dev, st->res.start, resource_size(&st->res),
 				MEMREMAP_WB | MEMREMAP_DEC);
 	if (IS_ERR(st->clk)) {
diff --git a/drivers/target/target_core_iblock.c b/drivers/target/target_core_iblock.c
index 73564efd1..318820b45 100644
--- a/drivers/target/target_core_iblock.c
+++ b/drivers/target/target_core_iblock.c
@@ -306,11 +306,20 @@ static sector_t iblock_get_blocks(struct se_device *dev)
 	return blocks_long;
 }
 
+/*
+ * 在以下使用iblock_complete_cmd():
+ *   - drivers/target/target_core_iblock.c|345| <<iblock_bio_done>> iblock_complete_cmd(cmd, blk_status);
+ *   - drivers/target/target_core_iblock.c|765| <<iblock_execute_rw>> iblock_complete_cmd(cmd, BLK_STS_OK);
+ *   - drivers/target/target_core_iblock.c|823| <<iblock_execute_rw>> iblock_complete_cmd(cmd, BLK_STS_OK);
+ */
 static void iblock_complete_cmd(struct se_cmd *cmd, blk_status_t blk_status)
 {
 	struct iblock_req *ibr = cmd->priv;
 	u8 status;
 
+	/*
+	 * 这里减少iblock_req->pending.
+	 */
 	if (!refcount_dec_and_test(&ibr->pending))
 		return;
 
@@ -325,6 +334,10 @@ static void iblock_complete_cmd(struct se_cmd *cmd, blk_status_t blk_status)
 	kfree(ibr);
 }
 
+/*
+ * 在以下使用iblock_bio_done():
+ *   - drivers/target/target_core_iblock.c|366| <<iblock_get_bio>> bio->bi_end_io = &iblock_bio_done;
+ */
 static void iblock_bio_done(struct bio *bio)
 {
 	struct se_cmd *cmd = bio->bi_private;
@@ -342,9 +355,22 @@ static void iblock_bio_done(struct bio *bio)
 
 	bio_put(bio);
 
+	/*
+	 * 在以下使用iblock_complete_cmd():
+	 *   - drivers/target/target_core_iblock.c|345| <<iblock_bio_done>> iblock_complete_cmd(cmd, blk_status);
+	 *   - drivers/target/target_core_iblock.c|765| <<iblock_execute_rw>> iblock_complete_cmd(cmd, BLK_STS_OK);
+	 *   - drivers/target/target_core_iblock.c|823| <<iblock_execute_rw>> iblock_complete_cmd(cmd, BLK_STS_OK);
+	 */
 	iblock_complete_cmd(cmd, blk_status);
 }
 
+/*
+ * 在以下使用iblock_get_bio():
+ *   - drivers/target/target_core_iblock.c|524| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+ *   - drivers/target/target_core_iblock.c|537| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+ *   - drivers/target/target_core_iblock.c|769| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sgl_nents, opf);
+ *   - drivers/target/target_core_iblock.c|802| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sg_num, opf);
+ */
 static struct bio *iblock_get_bio(struct se_cmd *cmd, sector_t lba, u32 sg_num,
 				  blk_opf_t opf)
 {
@@ -369,6 +395,12 @@ static struct bio *iblock_get_bio(struct se_cmd *cmd, sector_t lba, u32 sg_num,
 	return bio;
 }
 
+/*
+ * 在以下使用iblock_submit_bios():
+ *   - drivers/target/target_core_iblock.c|550| <<iblock_execute_write_same>> iblock_submit_bios(&list);
+ *   - drivers/target/target_core_iblock.c|798| <<iblock_execute_rw>> iblock_submit_bios(&list);
+ *   - drivers/target/target_core_iblock.c|822| <<iblock_execute_rw>> iblock_submit_bios(&list);
+ */
 static void iblock_submit_bios(struct bio_list *list)
 {
 	struct blk_plug plug;
@@ -521,6 +553,13 @@ iblock_execute_write_same(struct se_cmd *cmd)
 		goto fail;
 	cmd->priv = ibr;
 
+	/*
+	 * 在以下使用iblock_get_bio():
+	 *   - drivers/target/target_core_iblock.c|524| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+	 *   - drivers/target/target_core_iblock.c|537| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+	 *   - drivers/target/target_core_iblock.c|769| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sgl_nents, opf);
+	 *   - drivers/target/target_core_iblock.c|802| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sg_num, opf);
+	 */
 	bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
 	if (!bio)
 		goto fail_free_ibr;
@@ -534,6 +573,13 @@ iblock_execute_write_same(struct se_cmd *cmd)
 		while (bio_add_page(bio, sg_page(sg), sg->length, sg->offset)
 				!= sg->length) {
 
+			/*
+			 * 在以下使用iblock_get_bio():
+			 *   - drivers/target/target_core_iblock.c|524| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+			 *   - drivers/target/target_core_iblock.c|537| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+			 *   - drivers/target/target_core_iblock.c|769| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sgl_nents, opf);
+			 *   - drivers/target/target_core_iblock.c|802| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sg_num, opf);
+			 */
 			bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
 			if (!bio)
 				goto fail_put_bios;
@@ -547,6 +593,12 @@ iblock_execute_write_same(struct se_cmd *cmd)
 		sectors -= sg->length >> SECTOR_SHIFT;
 	}
 
+	/*
+	 * 在以下使用iblock_submit_bios():
+	 *   - drivers/target/target_core_iblock.c|550| <<iblock_execute_write_same>> iblock_submit_bios(&list);
+	 *   - drivers/target/target_core_iblock.c|798| <<iblock_execute_rw>> iblock_submit_bios(&list);
+	 *   - drivers/target/target_core_iblock.c|822| <<iblock_execute_rw>> iblock_submit_bios(&list);
+	 */
 	iblock_submit_bios(&list);
 	return 0;
 
@@ -766,6 +818,13 @@ iblock_execute_rw(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,
 		return 0;
 	}
 
+	/*
+	 * 在以下使用iblock_get_bio():
+	 *   - drivers/target/target_core_iblock.c|524| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+	 *   - drivers/target/target_core_iblock.c|537| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+	 *   - drivers/target/target_core_iblock.c|769| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sgl_nents, opf);
+	 *   - drivers/target/target_core_iblock.c|802| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sg_num, opf);
+	 */
 	bio = iblock_get_bio(cmd, block_lba, sgl_nents, opf);
 	if (!bio)
 		goto fail_free_ibr;
@@ -795,10 +854,23 @@ iblock_execute_rw(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,
 			}
 
 			if (bio_cnt >= IBLOCK_MAX_BIO_PER_TASK) {
+				/*
+				 * 在以下使用iblock_submit_bios():
+				 *   - drivers/target/target_core_iblock.c|550| <<iblock_execute_write_same>> iblock_submit_bios(&list);
+				 *   - drivers/target/target_core_iblock.c|798| <<iblock_execute_rw>> iblock_submit_bios(&list);
+				 *   - drivers/target/target_core_iblock.c|822| <<iblock_execute_rw>> iblock_submit_bios(&list);
+				 */
 				iblock_submit_bios(&list);
 				bio_cnt = 0;
 			}
 
+			/*
+			 * 在以下使用iblock_get_bio():
+			 *   - drivers/target/target_core_iblock.c|524| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+			 *   - drivers/target/target_core_iblock.c|537| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+			 *   - drivers/target/target_core_iblock.c|769| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sgl_nents, opf);
+			 *   - drivers/target/target_core_iblock.c|802| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sg_num, opf);
+			 */
 			bio = iblock_get_bio(cmd, block_lba, sg_num, opf);
 			if (!bio)
 				goto fail_put_bios;
@@ -819,7 +891,19 @@ iblock_execute_rw(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,
 			goto fail_put_bios;
 	}
 
+	/*
+	 * 在以下使用iblock_submit_bios():
+	 *   - drivers/target/target_core_iblock.c|550| <<iblock_execute_write_same>> iblock_submit_bios(&list);
+	 *   - drivers/target/target_core_iblock.c|798| <<iblock_execute_rw>> iblock_submit_bios(&list);
+	 *   - drivers/target/target_core_iblock.c|822| <<iblock_execute_rw>> iblock_submit_bios(&list);
+	 */
 	iblock_submit_bios(&list);
+	/*
+	 * 在以下使用iblock_complete_cmd():
+	 *   - drivers/target/target_core_iblock.c|345| <<iblock_bio_done>> iblock_complete_cmd(cmd, blk_status);
+	 *   - drivers/target/target_core_iblock.c|765| <<iblock_execute_rw>> iblock_complete_cmd(cmd, BLK_STS_OK);
+	 *   - drivers/target/target_core_iblock.c|823| <<iblock_execute_rw>> iblock_complete_cmd(cmd, BLK_STS_OK);
+	 */
 	iblock_complete_cmd(cmd, BLK_STS_OK);
 	return 0;
 
diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c
index 0a76bdfe5..8d1400b19 100644
--- a/drivers/target/target_core_transport.c
+++ b/drivers/target/target_core_transport.c
@@ -902,6 +902,13 @@ static bool target_cmd_interrupted(struct se_cmd *cmd)
 	return false;
 }
 
+/*
+ * 在以下使用target_complete_cmd_with_sense():
+ *   - drivers/target/target_core_transport.c|950| <<target_complete_cmd>> target_complete_cmd_with_sense(cmd,
+ *             scsi_status, scsi_status ? TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE : TCM_NO_SENSE);
+ *   - drivers/target/target_core_xcopy.c|780| <<target_xcopy_do_work>> target_complete_cmd_with_sense(ec_cmd,
+ *             SAM_STAT_CHECK_CONDITION, sense_rc);
+ */
 /* May be called from interrupt context so must not sleep. */
 void target_complete_cmd_with_sense(struct se_cmd *cmd, u8 scsi_status,
 				    sense_reason_t sense_reason)
diff --git a/include/kvm/arm_arch_timer.h b/include/kvm/arm_arch_timer.h
index 681cf0c8b..2345b6edb 100644
--- a/include/kvm/arm_arch_timer.h
+++ b/include/kvm/arm_arch_timer.h
@@ -32,11 +32,34 @@ struct arch_timer_offset {
 	 * If set, pointer to one of the offsets in the kvm's offset
 	 * structure. If NULL, assume a zero offset.
 	 */
+	/*
+	 * 在以下使用arch_timer_offset->vm_offset(指针?):
+	 *   - arch/arm64/kvm/arch_timer.c|151| <<timer_set_offset>> if (!ctxt->offset.vm_offset) {
+  3 arch/arm64/kvm/arch_timer.c|156| <<timer_set_offset>> WRITE_ONCE(*ctxt->offset.vm_offset, offset);
+  4 arch/arm64/kvm/arch_timer.c|1040| <<kvm_timer_vcpu_reset>> offs->vm_offset = &vcpu->kvm->arch.timer_data.poffset;
+  5 arch/arm64/kvm/arch_timer.c|1069| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.voffset;
+  6 arch/arm64/kvm/arch_timer.c|1071| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.poffset;
+  7 arch/arm64/kvm/hyp/include/hyp/switch.h|761| <<hyp_timer_get_offset>> if (ctxt->offset.vm_offset)
+  8 arch/arm64/kvm/hyp/include/hyp/switch.h|762| <<hyp_timer_get_offset>> offset += *kern_hyp_va(ctxt->offset.vm_offset);
+  9 include/kvm/arm_arch_timer.h|173| <<timer_get_offset>> if (ctxt->offset.vm_offset)
+ 10 include/kvm/arm_arch_timer.h|174| <<timer_get_offset>> offset += *ctxt->offset.vm_offset;
+	 */
 	u64	*vm_offset;
 	/*
 	 * If set, pointer to one of the offsets in the vcpu's sysreg
 	 * array. If NULL, assume a zero offset.
 	 */
+	/*
+	 * 在以下设置arch_timer_offset->vcpu_offset(指针?):
+	 *   - arch/arm64/kvm/arch_timer.c|1039| <<kvm_timer_vcpu_reset>> offs->vcpu_offset = __ctxt_sys_reg(&vcpu->arch.ctxt, CNTVOFF_EL2);
+	 *   - arch/arm64/kvm/arch_timer.c|1287| <<kvm_arm_timer_write(TIMER_REG_VOFF)>> *timer->offset.vcpu_offset = val;
+	 * 在以下使用arch_timer_offset->vcpu_offset(指针?):
+	 *   - arch/arm64/kvm/arch_timer.c|1233| <<kvm_arm_timer_read(TIMER_REG_VOFF)>> val = *timer->offset.vcpu_offset;
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|763| <<hyp_timer_get_offset>> if (ctxt->offset.vcpu_offset)
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|764| <<hyp_timer_get_offset>> offset += *kern_hyp_va(ctxt->offset.vcpu_offset);
+	 *   - include/kvm/arm_arch_timer.h|175| <<timer_get_offset>> if (ctxt->offset.vcpu_offset)
+	 *   - include/kvm/arm_arch_timer.h|176| <<timer_get_offset>> offset += *ctxt->offset.vcpu_offset;
+	 */
 	u64	*vcpu_offset;
 };
 
@@ -163,6 +186,11 @@ static inline bool has_cntpoff(void)
 	return (has_vhe() && cpus_have_final_cap(ARM64_HAS_ECV_CNTPOFF));
 }
 
+/*
+ * 在以下使用timer_get_offset():
+ *   - arch/arm64/kvm/arch_timer.c|1275| <<kvm_arm_timer_write(TIMER_REG_CVAL)>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|276| <<kvm_hyp_handle_timer(SYS_CNTP_CVAL_EL0)>> val -= timer_get_offset(vcpu_hptimer(vcpu));
+ */
 static inline u64 timer_get_offset(struct arch_timer_context *ctxt)
 {
 	u64 offset = 0;
diff --git a/include/kvm/arm_vgic.h b/include/kvm/arm_vgic.h
index 4a34f7f0a..620ef0fd2 100644
--- a/include/kvm/arm_vgic.h
+++ b/include/kvm/arm_vgic.h
@@ -272,6 +272,12 @@ struct vgic_dist {
 	struct vgic_io_device	dist_iodev;
 
 	bool			has_its;
+	/*
+	 * 在以下使用vgic_dist->table_write_in_progress:
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|2720| <<kvm_arch_allow_write_without_running_vcpu>> return dist->table_write_in_progress;
+	 *   - arch/arm64/kvm/vgic/vgic.h|142| <<vgic_write_guest_lock>> dist->table_write_in_progress = true;
+	 *   - arch/arm64/kvm/vgic/vgic.h|144| <<vgic_write_guest_lock>> dist->table_write_in_progress = false;
+	 */
 	bool			table_write_in_progress;
 
 	/*
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3bde4fb5c..588b282f5 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -619,6 +619,18 @@ static inline bool kvm_slot_can_be_private(const struct kvm_memory_slot *slot)
 	return slot && (slot->flags & KVM_MEM_GUEST_MEMFD);
 }
 
+/*
+ * 在以下使用kvm_slot_dirty_track_enabled():
+ *   - arch/loongarch/kvm/mmu.c|583| <<kvm_map_page_fast>> if (kvm_slot_dirty_track_enabled(slot)) {
+ *   - arch/loongarch/kvm/mmu.c|614| <<fault_supports_huge_mapping>> if (kvm_slot_dirty_track_enabled(memslot) && write)
+ *   - arch/x86/kvm/mmu/mmu.c|847| <<gfn_to_memslot_dirty_bitmap>> if (no_dirty_log && kvm_slot_dirty_track_enabled(slot))
+ *   - arch/x86/kvm/mmu/mmu.c|3307| <<kvm_mmu_hugepage_adjust>> if (kvm_slot_dirty_track_enabled(slot))
+ *   - arch/x86/kvm/mmu/mmu.c|3663| <<fast_page_fault>> kvm_slot_dirty_track_enabled(fault->slot))
+ *   - arch/x86/kvm/mmu/mmu.c|7591| <<kvm_recover_nx_huge_pages>> if (slot && kvm_slot_dirty_track_enabled(slot))
+ *   - arch/x86/kvm/mmu/spte.c|257| <<make_spte>> if ((spte & PT_WRITABLE_MASK) && kvm_slot_dirty_track_enabled(slot)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1782| <<recover_huge_pages_range>> if (WARN_ON_ONCE(kvm_slot_dirty_track_enabled(slot)))
+ *   - virt/kvm/kvm_main.c|3571| <<mark_page_dirty_in_slot>> if (memslot && kvm_slot_dirty_track_enabled(memslot)) {
+ */
 static inline bool kvm_slot_dirty_track_enabled(const struct kvm_memory_slot *slot)
 {
 	return slot->flags & KVM_MEM_LOG_DIRTY_PAGES;
@@ -776,6 +788,17 @@ struct kvm {
 	struct kvm_memslots __memslots[KVM_MAX_NR_ADDRESS_SPACES][2];
 	/* The current active memslot set for each address space */
 	struct kvm_memslots __rcu *memslots[KVM_MAX_NR_ADDRESS_SPACES];
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|991| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|996| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3995| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4214| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4249| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	struct xarray vcpu_array;
 	/*
 	 * Protected by slots_lock, but can be read outside if an
@@ -851,6 +874,11 @@ struct kvm {
 	bool override_halt_poll_ns;
 	unsigned int max_halt_poll_ns;
 	u32 dirty_ring_size;
+	/*
+	 * 在以下使用dirty_ring_with_bitmap (KVM_CAP_DIRTY_LOG_RING_WITH_BITMAP):
+	 *   - virt/kvm/dirty_ring.c|28| <<kvm_use_dirty_bitmap>> return !kvm->dirty_ring_size || kvm->dirty_ring_with_bitmap;
+	 *   - virt/kvm/kvm_main.c|5161| <<kvm_vm_ioctl_enable_cap_generic>> kvm->dirty_ring_with_bitmap = true;
+	 */
 	bool dirty_ring_with_bitmap;
 	bool vm_bugged;
 	bool vm_dead;
@@ -972,6 +1000,41 @@ static inline struct kvm_io_bus *kvm_get_bus(struct kvm *kvm, enum kvm_bus idx)
 				      !refcount_read(&kvm->users_count));
 }
 
+/*
+ * 在以下使用kvm_get_vcpu():
+ *   - arch/arm64/kvm/arm.c|2715| <<kvm_mpidr_to_vcpu>> vcpu = kvm_get_vcpu(kvm, data->cmpidr_to_idx[idx]);
+ *   - arch/arm64/kvm/vgic/vgic-debug.c|284| <<vgic_debug_show>> vcpu = kvm_get_vcpu(kvm, iter->vcpu_id);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|184| <<kvm_vgic_dist_init>> struct kvm_vcpu *vcpu0 = kvm_get_vcpu(kvm, 0);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|501| <<vgic_v3_parse_attr>> reg_attr->vcpu = kvm_get_vcpu(dev->kvm, 0);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|201| <<vgic_mmio_write_target>> irq->target_vcpu = kvm_get_vcpu(vcpu->kvm, target);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|836| <<vgic_register_all_redist_iodevs>> vcpu = kvm_get_vcpu(kvm, i);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|372| <<unmap_all_vpes>> free_irq(dist->its_vm.vpes[i]->irq, kvm_get_vcpu(kvm, i));
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|381| <<map_all_vpes>> WARN_ON(vgic_v4_request_vpe_irq(kvm_get_vcpu(kvm, i),
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|326| <<vgic_v4_teardown>> struct kvm_vcpu *vcpu = kvm_get_vcpu(kvm, i);
+ *   - arch/loongarch/kvm/intc/eiointc.c|50| <<eiointc_update_irq>> vcpu = kvm_get_vcpu(s->kvm, cpu);
+ *   - arch/loongarch/kvm/intc/ipi.c|321| <<kvm_ipi_regs_access>> vcpu = kvm_get_vcpu(dev->kvm, cpu);
+ *   - arch/mips/kvm/loongson_ipi.c|125| <<loongson_vipi_write>> kvm_vcpu_ioctl_interrupt(kvm_get_vcpu(kvm, id), &irq);
+ *   - arch/mips/kvm/loongson_ipi.c|133| <<loongson_vipi_write>> kvm_vcpu_ioctl_interrupt(kvm_get_vcpu(kvm, id), &irq);
+ *   - arch/mips/kvm/mips.c|495| <<kvm_vcpu_ioctl_interrupt>> dvcpu = kvm_get_vcpu(vcpu->kvm, irq->cpu);
+ *   - arch/powerpc/kvm/book3s_pr.c|1952| <<kvm_vm_ioctl_get_smmu_info_pr>> vcpu = kvm_get_vcpu(kvm, 0);
+ *   - arch/riscv/kvm/aia_device.c|166| <<aia_imsic_addr>> vcpu = kvm_get_vcpu(kvm, vcpu_idx);
+ *   - arch/riscv/kvm/aia_device.c|287| <<aia_init>> vcpu = kvm_get_vcpu(kvm, i);
+ *   - arch/s390/kvm/interrupt.c|1960| <<__floating_irq_kick>> } while (is_vcpu_stopped(kvm_get_vcpu(kvm, sigcpu)));
+ *   - arch/s390/kvm/interrupt.c|1962| <<__floating_irq_kick>> dst_vcpu = kvm_get_vcpu(kvm, sigcpu);
+ *   - arch/s390/kvm/interrupt.c|3076| <<__airqs_kick_single_vcpu>> vcpu = kvm_get_vcpu(kvm, vcpu_idx);
+ *   - arch/s390/kvm/kvm-s390.c|5481| <<kvm_s390_vcpu_start>> if (!is_vcpu_stopped(kvm_get_vcpu(vcpu->kvm, i)))
+ *   - arch/s390/kvm/kvm-s390.c|5548| <<kvm_s390_vcpu_stop>> struct kvm_vcpu *tmp = kvm_get_vcpu(vcpu->kvm, i);
+ *   - arch/x86/kvm/hyperv.c|198| <<get_vcpu_by_vpidx>> vcpu = kvm_get_vcpu(kvm, vpidx);
+ *   - arch/x86/kvm/hyperv.c|581| <<get_time_ref_counter>> vcpu = kvm_get_vcpu(kvm, 0);
+ *   - arch/x86/kvm/hyperv.c|2149| <<kvm_hv_flush_tlb>> v = kvm_get_vcpu(kvm, i);
+ *   - arch/x86/kvm/irq_comm.c|95| <<kvm_irq_delivery_to_apic>> lowest = kvm_get_vcpu(kvm, idx);
+ *   - arch/x86/kvm/svm/sev.c|1949| <<sev_migrate_from>> src_vcpu = kvm_get_vcpu(src_kvm, i);
+ *   - arch/x86/kvm/xen.c|1805| <<kvm_xen_set_evtchn_fast>> vcpu = kvm_get_vcpu(kvm, vcpu_idx);
+ *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu_by_id>> vcpu = kvm_get_vcpu(kvm, id);
+ *   - virt/kvm/kvm_main.c|258| <<kvm_make_vcpus_request_mask>> vcpu = kvm_get_vcpu(kvm, i);
+ *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+ *   - virt/kvm/kvm_main.c|4394| <<kvm_wait_for_vcpu_online>> if (WARN_ON_ONCE(!kvm_get_vcpu(kvm, vcpu->vcpu_idx)))
+ */
 static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 {
 	int num_vcpus = atomic_read(&kvm->online_vcpus);
@@ -986,16 +1049,44 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 
 	i = array_index_nospec(i, num_vcpus);
 
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|991| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|996| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3995| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4214| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4249| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	/* Pairs with smp_wmb() in kvm_vm_ioctl_create_vcpu.  */
 	smp_rmb();
 	return xa_load(&kvm->vcpu_array, i);
 }
 
+/*
+ * 在以下使用kvm->vcpu_array:
+ *   - include/linux/kvm_host.h|991| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+ *   - include/linux/kvm_host.h|996| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+ *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+ *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+ *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+ *   - virt/kvm/kvm_main.c|3995| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+ *   - virt/kvm/kvm_main.c|4214| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+ *   - virt/kvm/kvm_main.c|4249| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+ *
+ *
+ * 特别特别多的调用
+ */
 #define kvm_for_each_vcpu(idx, vcpup, kvm)				\
 	if (atomic_read(&kvm->online_vcpus))				\
 		xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0,	\
 				  (atomic_read(&kvm->online_vcpus) - 1))
 
+/*
+ * 很多的调用
+ */
 static inline struct kvm_vcpu *kvm_get_vcpu_by_id(struct kvm *kvm, int id)
 {
 	struct kvm_vcpu *vcpu = NULL;
@@ -1915,6 +2006,13 @@ static inline bool kvm_is_gpa_in_memslot(struct kvm *kvm, gpa_t gpa)
 	return !kvm_is_error_hva(hva);
 }
 
+/*
+ * 在以下使用kvm_gpc_mark_dirty_in_slot():
+ *   - arch/x86/kvm/x86.c|3438| <<kvm_setup_guest_pvclock>> kvm_gpc_mark_dirty_in_slot(gpc);
+ *   - arch/x86/kvm/xen.c|579| <<kvm_xen_update_runstate_guest>> kvm_gpc_mark_dirty_in_slot(gpc2);
+ *   - arch/x86/kvm/xen.c|583| <<kvm_xen_update_runstate_guest>> kvm_gpc_mark_dirty_in_slot(gpc1);
+ *   - arch/x86/kvm/xen.c|696| <<kvm_xen_inject_pending_events>> kvm_gpc_mark_dirty_in_slot(gpc);
+ */
 static inline void kvm_gpc_mark_dirty_in_slot(struct gfn_to_pfn_cache *gpc)
 {
 	lockdep_assert_held(&gpc->lock);
diff --git a/include/linux/seqlock.h b/include/linux/seqlock.h
index 5ce48eab7..5e3a92ffc 100644
--- a/include/linux/seqlock.h
+++ b/include/linux/seqlock.h
@@ -126,7 +126,10 @@ static inline void seqcount_lockdep_reader_access(const seqcount_t *s)
 		seqcount_init(&____s->seqcount);			\
 		__SEQ_LOCK(____s->lock = (_lock));			\
 	} while (0)
-
+/*
+ * 写者在修改数据时,必须通过这个raw_spinlock.
+ * 读者则依赖seqcount 的机制进行无锁访问.
+ */
 #define seqcount_raw_spinlock_init(s, lock)	seqcount_LOCKNAME_init(s, lock, raw_spinlock)
 #define seqcount_spinlock_init(s, lock)		seqcount_LOCKNAME_init(s, lock, spinlock)
 #define seqcount_rwlock_init(s, lock)		seqcount_LOCKNAME_init(s, lock, rwlock)
diff --git a/include/linux/timekeeper_internal.h b/include/linux/timekeeper_internal.h
index 785048a3b..9f25c70c8 100644
--- a/include/linux/timekeeper_internal.h
+++ b/include/linux/timekeeper_internal.h
@@ -145,6 +145,16 @@ struct timekeeper {
 	u32			ntp_error_shift;
 	u32			ntp_err_mult;
 	u32			skip_second_overflow;
+	/*
+	 * 在以下使用timekeeper->tai_offset:
+	 *   - kernel/time/timekeeping.c|220| <<tk_set_wall_to_mono>> WRITE_ONCE(tk->offs_tai, ktime_add(tk->offs_real, ktime_set(tk->tai_offset, 0)));
+	 *   - kernel/time/timekeeping.c|1487| <<__timekeeping_set_tai_offset>> static void __timekeeping_set_tai_offset(struct timekeeper *tk, s32 tai_offset)
+	 *   - kernel/time/timekeeping.c|1489| <<__timekeeping_set_tai_offset>> tk->tai_offset = tai_offset;
+	 *   - kernel/time/timekeeping.c|1490| <<__timekeeping_set_tai_offset>> tk->offs_tai = ktime_add(tk->offs_real, ktime_set(tai_offset, 0));
+	 *   - kernel/time/timekeeping.c|2129| <<accumulate_nsecs_to_secs>> __timekeeping_set_tai_offset(tk, tk->tai_offset - leap);
+	 *   - kernel/time/timekeeping.c|2571| <<do_adjtimex>> orig_tai = tai = tks->tai_offset;
+	 *   - kernel/time/vsyscall.c|74| <<update_vdso_time_data>> vdso_ts->sec = tk->xtime_sec + (s64)tk->tai_offset;
+	 */
 	s32			tai_offset;
 };
 
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index af43a8d2a..54b421162 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4134,6 +4134,53 @@ static inline int ib_dma_mapping_error(struct ib_device *dev, u64 dma_addr)
 	return dma_mapping_error(dev->dma_device, dma_addr);
 }
 
+/*
+ * 在以下调用ib_dma_map_single():
+ *   - drivers/infiniband/core/mad.c|1010| <<ib_send_mad>> sge[0].addr = ib_dma_map_single(mad_agent->device,
+ *   - drivers/infiniband/core/mad.c|1019| <<ib_send_mad>> sge[1].addr = ib_dma_map_single(mad_agent->device,
+ *   - drivers/infiniband/core/mad.c|2700| <<ib_mad_post_receive_mads>> sg_list.addr = ib_dma_map_single(qp_info->port_priv->device,
+ *   - drivers/infiniband/hw/mlx4/mad.c|1642| <<mlx4_ib_alloc_pv_bufs>> tun_qp->ring[i].map = ib_dma_map_single(ctx->ib_dev,
+ *   - drivers/infiniband/hw/mlx4/mad.c|1658| <<mlx4_ib_alloc_pv_bufs>> ib_dma_map_single(ctx->ib_dev,
+ *   - drivers/infiniband/hw/mlx4/qp.c|486| <<alloc_proxy_bufs>> ib_dma_map_single(dev, qp->sqp_proxy_rcv[i].addr,
+ *   - drivers/infiniband/ulp/ipoib/ipoib_cm.c|161| <<ipoib_cm_alloc_rx_skb>> mapping[0] = ib_dma_map_single(priv->ca, skb->data, IPOIB_CM_HEAD_SIZE,
+ *   - drivers/infiniband/ulp/ipoib/ipoib_ib.c|143| <<ipoib_alloc_rx_skb>> mapping[0] = ib_dma_map_single(priv->ca, skb->data, buf_size,
+ *   - drivers/infiniband/ulp/ipoib/ipoib_ib.c|284| <<ipoib_dma_map_tx>> mapping[0] = ib_dma_map_single(ca, skb->data, skb_headlen(skb),
+ *   - drivers/infiniband/ulp/iser/iscsi_iser.c|206| <<iser_initialize_task_headers>> dma_addr = ib_dma_map_single(device->ib_device, (void *)tx_desc,
+ *   - drivers/infiniband/ulp/iser/iser_initiator.c|191| <<iser_alloc_login_buf>> desc->req_dma = ib_dma_map_single(device->ib_device, desc->req,
+ *   - drivers/infiniband/ulp/iser/iser_initiator.c|202| <<iser_alloc_login_buf>> desc->rsp_dma = ib_dma_map_single(device->ib_device, desc->rsp,
+ *   - drivers/infiniband/ulp/iser/iser_initiator.c|252| <<iser_alloc_rx_descriptors>> dma_addr = ib_dma_map_single(device->ib_device, (void *)rx_desc,
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|164| <<isert_alloc_rx_descriptors>> dma_addr = ib_dma_map_single(ib_dev, rx_desc->buf,
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|341| <<isert_alloc_login_buf>> isert_conn->login_desc->dma_addr = ib_dma_map_single(ib_dev,
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|357| <<isert_alloc_login_buf>> isert_conn->login_rsp_dma = ib_dma_map_single(ib_dev,
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|843| <<isert_init_tx_hdrs>> dma_addr = ib_dma_map_single(ib_dev, (void *)tx_desc,
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|1789| <<isert_put_response>> isert_cmd->pdu_buf_dma = ib_dma_map_single(ib_dev,
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|1918| <<isert_put_reject>> isert_cmd->pdu_buf_dma = ib_dma_map_single(ib_dev,
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|1961| <<isert_put_text_rsp>> isert_cmd->pdu_buf_dma = ib_dma_map_single(ib_dev,
+ *   - drivers/infiniband/ulp/rtrs/rtrs.c|39| <<isert_put_text_rsp>> iu->dma_addr = ib_dma_map_single(dma_dev, iu->buf, size, dir);
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|235| <<srp_alloc_iu>> iu->dma = ib_dma_map_single(host->srp_dev->dev, iu->buf, size,
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|1000| <<srp_init_cmd_priv>> dma_addr = ib_dma_map_single(ibdev, req->indirect_desc,
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|735| <<srpt_alloc_ioctx>> ioctx->dma = ib_dma_map_single(sdev->device, ioctx->buf,
+ *   - drivers/nvme/host/rdma.c|190| <<nvme_rdma_alloc_qe>> qe->dma = ib_dma_map_single(ibdev, qe->data, capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|2013| <<nvme_rdma_queue_rq>> req->sqe.dma = ib_dma_map_single(dev, req->sqe.data,
+ *   - drivers/nvme/target/rdma.c|324| <<nvmet_rdma_alloc_cmd>> c->sge[0].addr = ib_dma_map_single(ndev->device, c->nvme_cmd,
+ *   - drivers/nvme/target/rdma.c|408| <<nvmet_rdma_alloc_rsp>> r->send_sge.addr = ib_dma_map_single(ndev->device, r->req.cqe,
+ *   - fs/smb/client/smbdirect.c|717| <<smbd_post_send_negotiate_req>> request->sge[0].addr = ib_dma_map_single(
+ *   - fs/smb/client/smbdirect.c|961| <<smbd_post_send_iter>> request->sge[0].addr = ib_dma_map_single(sc->ib.dev,
+ *   - fs/smb/client/smbdirect.c|1052| <<smbd_post_recv>> response->sge.addr = ib_dma_map_single(
+ *   - fs/smb/server/transport_rdma.c|652| <<smb_direct_post_recv>> recvmsg->sge.addr = ib_dma_map_single(t->cm_id->device,
+ *   - fs/smb/server/transport_rdma.c|1068| <<smb_direct_create_header>> sendmsg->sge[0].addr = ib_dma_map_single(t->cm_id->device,
+ *   - fs/smb/server/transport_rdma.c|1618| <<smb_direct_send_negotiate_response>> sendmsg->sge[0].addr = ib_dma_map_single(t->cm_id->device,
+ *   - net/9p/trans_rdma.c|390| <<post_recv>> c->busa = ib_dma_map_single(rdma->cm_id->device,
+ *   - net/9p/trans_rdma.c|485| <<rdma_request>> c->busa = ib_dma_map_single(rdma->cm_id->device,
+ *   - net/rds/ib_cm.c|425| <<rds_dma_hdr_alloc>> *dma_addr = ib_dma_map_single(dev, hdr, sizeof(*hdr),
+ *   - net/smc/smc_wr.c|872| <<smc_wr_create_link>> lnk->wr_rx_dma_addr = ib_dma_map_single(
+ *   - net/smc/smc_wr.c|883| <<smc_wr_create_link>> ib_dma_map_single(ibdev, lnk->lgr->wr_rx_buf_v2,
+ *   - net/smc/smc_wr.c|891| <<smc_wr_create_link>> lnk->wr_tx_v2_dma_addr = ib_dma_map_single(ibdev,
+ *   - net/smc/smc_wr.c|900| <<smc_wr_create_link>> lnk->wr_tx_dma_addr = ib_dma_map_single(
+ *   - net/sunrpc/xprtrdma/svc_rdma_recvfrom.c|136| <<svc_rdma_recv_ctxt_alloc>> addr = ib_dma_map_single(rdma->sc_pd->device, buffer,
+ *   - net/sunrpc/xprtrdma/svc_rdma_sendto.c|139| <<svc_rdma_send_ctxt_alloc>> addr = ib_dma_map_single(rdma->sc_pd->device, buffer,
+ *   - net/sunrpc/xprtrdma/verbs.c|1309| <<__rpcrdma_regbuf_dma_map>> rb->rg_iov.addr = ib_dma_map_single(device, rdmab_data(rb),
+ */
 /**
  * ib_dma_map_single - Map a kernel virtual address to DMA address
  * @dev: The device for which the dma_addr is to be created
diff --git a/kernel/sched/cputime.c b/kernel/sched/cputime.c
index 6dab4854c..208c9eff4 100644
--- a/kernel/sched/cputime.c
+++ b/kernel/sched/cputime.c
@@ -248,6 +248,12 @@ void __account_forceidle_time(struct task_struct *p, u64 delta)
  * ticks are not redelivered later. Due to that, this function may on
  * occasion account more time than the calling functions think elapsed.
  */
+/*
+ * 在以下使用steal_account_process_time():
+ *   - kernel/sched/cputime.c|278| <<account_other_time>> accounted = steal_account_process_time(max);
+ *   - kernel/sched/cputime.c|485| <<account_process_tick>> steal = steal_account_process_time(ULONG_MAX);
+ *   - kernel/sched/cputime.c|514| <<account_idle_ticks>> steal = steal_account_process_time(ULONG_MAX);
+ */
 static __always_inline u64 steal_account_process_time(u64 maxtime)
 {
 #ifdef CONFIG_PARAVIRT
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index 83c65f3af..28246affe 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -1676,6 +1676,10 @@ static bool persistent_clock_exists;
 /*
  * timekeeping_init - Initializes the clocksource and common timekeeping values
  */
+/*
+ * 在以下使用timekeeping_init():
+ *   - init/main.c|1011| <<start_kernel>> timekeeping_init();
+ */
 void __init timekeeping_init(void)
 {
 	struct timespec64 wall_time, boot_offset, wall_to_mono;
diff --git a/net/rds/ib.c b/net/rds/ib.c
index 9826fe7f9..df13e0249 100644
--- a/net/rds/ib.c
+++ b/net/rds/ib.c
@@ -573,6 +573,29 @@ int rds_ib_init(void)
 	if (ret)
 		goto out;
 
+	/*
+	 * 在以下使用ib_register_client():
+	 *   - drivers/infiniband/core/cm.c|4492| <<ib_cm_init>> ret = ib_register_client(&cm_client);
+	 *   - drivers/infiniband/core/cma.c|5474| <<cma_init>> ret = ib_register_client(&cma_client);
+	 *   - drivers/infiniband/core/mad.c|3151| <<ib_mad_init>> if (ib_register_client(&mad_client)) {
+	 *   - drivers/infiniband/core/multicast.c|890| <<mcast_init>> ret = ib_register_client(&mcast_client);
+	 *   - drivers/infiniband/core/sa_query.c|2261| <<ib_sa_init>> ret = ib_register_client(&sa_client);
+	 *   - drivers/infiniband/core/ucma.c|1866| <<ucma_init>> ret = ib_register_client(&rdma_cma_client);
+	 *   - drivers/infiniband/core/user_mad.c|1477| <<ib_umad_init>> ret = ib_register_client(&umad_client);
+	 *   - drivers/infiniband/core/user_mad.c|1481| <<ib_umad_init>> ret = ib_register_client(&issm_client);
+	 *   - drivers/infiniband/core/uverbs_main.c|1318| <<ib_uverbs_init>> ret = ib_register_client(&uverbs_client);
+	 *   - drivers/infiniband/ulp/ipoib/ipoib_main.c|2736| <<ipoib_init_module>> ret = ib_register_client(&ipoib_client);
+	 *   - drivers/infiniband/ulp/opa_vnic/opa_vnic_vema.c|1040| <<opa_vnic_init>> rc = ib_register_client(&opa_vnic_client);
+	 *   - drivers/infiniband/ulp/rtrs/rtrs-srv.c|2202| <<rtrs_srv_open>> err = ib_register_client(&rtrs_srv_client);
+	 *   - drivers/infiniband/ulp/srp/ib_srp.c|4190| <<srp_init_module>> ret = ib_register_client(&srp_client);
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3965| <<srpt_init_module>> ret = ib_register_client(&srpt_client);
+	 *   - drivers/nvme/host/rdma.c|2401| <<nvme_rdma_init_module>> ret = ib_register_client(&nvme_rdma_ib_client);
+	 *   - drivers/nvme/target/rdma.c|2102| <<nvmet_rdma_init>> ret = ib_register_client(&nvmet_rdma_ib_client);
+	 *   - fs/smb/server/transport_rdma.c|2195| <<ksmbd_rdma_init>> ret = ib_register_client(&smb_direct_ib_client);
+	 *   - net/rds/ib.c|576| <<rds_ib_init>> ret = ib_register_client(&rds_ib_client);
+	 *   - net/smc/smc_ib.c|1012| <<smc_ib_register_client>> return ib_register_client(&smc_ib_client);
+	 *   - net/sunrpc/xprtrdma/ib_client.c|183| <<rpcrdma_ib_client_register>> return ib_register_client(&rpcrdma_ib_client);
+	 */
 	ret = ib_register_client(&rds_ib_client);
 	if (ret)
 		goto out_mr_exit;
diff --git a/tools/arch/x86/include/asm/pvclock.h b/tools/arch/x86/include/asm/pvclock.h
index 2628f9a63..943c23f93 100644
--- a/tools/arch/x86/include/asm/pvclock.h
+++ b/tools/arch/x86/include/asm/pvclock.h
@@ -75,6 +75,9 @@ static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 	return product;
 }
 
+/*
+ * 在以下使用__pvclock_read_cycles()
+ */
 static __always_inline
 u64 __pvclock_read_cycles(const struct pvclock_vcpu_time_info *src, u64 tsc)
 {
diff --git a/tools/testing/selftests/kvm/x86/kvm_clock_test.c b/tools/testing/selftests/kvm/x86/kvm_clock_test.c
index 5bc12222d..3ef6163fd 100644
--- a/tools/testing/selftests/kvm/x86/kvm_clock_test.c
+++ b/tools/testing/selftests/kvm/x86/kvm_clock_test.c
@@ -49,6 +49,16 @@ static inline void assert_flags(struct kvm_clock_data *data)
 		    data->flags, EXPECTED_FLAGS);
 }
 
+/*
+ * struct kvm_clock_data {
+ *     __u64 clock;
+ *     __u32 flags;
+ *     __u32 pad0;
+ *     __u64 realtime;
+ *     __u64 host_tsc;
+ *     __u32 pad[4];
+ * };
+ */
 static void handle_sync(struct ucall *uc, struct kvm_clock_data *start,
 			struct kvm_clock_data *end)
 {
@@ -110,6 +120,19 @@ static void enter_guest(struct kvm_vcpu *vcpu)
 	int i;
 
 	for (i = 0; i < ARRAY_SIZE(test_cases); i++) {
+		/*
+		 * 19 struct test_case {
+		 * 20         uint64_t kvmclock_base;
+		 * 21         int64_t realtime_offset;
+		 * 22 };
+		 * 23
+		 * 24 static struct test_case test_cases[] = {
+		 * 25         { .kvmclock_base = 0 },
+		 * 26         { .kvmclock_base = 180 * NSEC_PER_SEC },
+		 * 27         { .kvmclock_base = 0, .realtime_offset = -180 * NSEC_PER_SEC },
+		 * 28         { .kvmclock_base = 0, .realtime_offset = 180 * NSEC_PER_SEC },
+		 * 29 };
+		 */
 		setup_clock(vm, &test_cases[i]);
 
 		vm_ioctl(vm, KVM_GET_CLOCK, &start);
diff --git a/virt/kvm/dirty_ring.c b/virt/kvm/dirty_ring.c
index d14ffc751..c84ceb060 100644
--- a/virt/kvm/dirty_ring.c
+++ b/virt/kvm/dirty_ring.c
@@ -21,6 +21,13 @@ u32 kvm_dirty_ring_get_rsvd_entries(struct kvm *kvm)
 	return KVM_DIRTY_RING_RSVD_ENTRIES + kvm_cpu_dirty_log_size(kvm);
 }
 
+/*
+ * 在以下使用kvm_use_dirty_bitmap():
+ *   - virt/kvm/kvm_main.c|1723| <<kvm_prepare_memory_region>> else if (kvm_use_dirty_bitmap(kvm)) {
+ *   - virt/kvm/kvm_main.c|2198| <<kvm_get_dirty_log>> if (!kvm_use_dirty_bitmap(kvm))
+ *   - virt/kvm/kvm_main.c|2263| <<kvm_get_dirty_log_protect>> if (!kvm_use_dirty_bitmap(kvm))
+ *   - virt/kvm/kvm_main.c|2375| <<kvm_clear_dirty_log_protect>> if (!kvm_use_dirty_bitmap(kvm))
+ */
 bool kvm_use_dirty_bitmap(struct kvm *kvm)
 {
 	lockdep_assert_held(&kvm->slots_lock);
@@ -29,6 +36,10 @@ bool kvm_use_dirty_bitmap(struct kvm *kvm)
 }
 
 #ifndef CONFIG_NEED_KVM_DIRTY_RING_WITH_BITMAP
+/*
+ * 在以下使用kvm_arch_allow_write_without_running_vcpu():
+ *   - virt/kvm/kvm_main.c|3594| <<mark_page_dirty_in_slot>> WARN_ON_ONCE(!vcpu && !kvm_arch_allow_write_without_running_vcpu(kvm));
+ */
 bool kvm_arch_allow_write_without_running_vcpu(struct kvm *kvm)
 {
 	return false;
@@ -170,8 +181,22 @@ int kvm_dirty_ring_reset(struct kvm *kvm, struct kvm_dirty_ring *ring)
 	return count;
 }
 
+/*
+ * 在以下使用kvm_dirty_ring_push():
+ *   - virt/kvm/kvm_main.c|3576| <<mark_page_dirty_in_slot>> kvm_dirty_ring_push(vcpu, slot, rel_gfn);
+ */
 void kvm_dirty_ring_push(struct kvm_vcpu *vcpu, u32 slot, u64 offset)
 {
+	/*
+	 * struct kvm_dirty_ring {
+	 *     u32 dirty_index;
+	 *     u32 reset_index;
+	 *     u32 size;
+	 *     u32 soft_limit;
+	 *     struct kvm_dirty_gfn *dirty_gfns;
+	 *     int index;
+	 * };
+	 */
 	struct kvm_dirty_ring *ring = &vcpu->dirty_ring;
 	struct kvm_dirty_gfn *entry;
 
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 222f0e894..18efef9b2 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -113,6 +113,17 @@ LIST_HEAD(vm_list);
 static struct kmem_cache *kvm_vcpu_cache;
 
 static __read_mostly struct preempt_ops kvm_preempt_ops;
+/*
+ * 在以下使用percpu的kvm_running_vcpu:
+ *   - virt/kvm/kvm_main.c|116| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+ *   - virt/kvm/kvm_main.c|168| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|180| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|3846| <<__kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+ *   - virt/kvm/kvm_main.c|6403| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|6421| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|6438| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+ *   - virt/kvm/kvm_main.c|6450| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+ */
 static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
 
 static struct dentry *kvm_debugfs_dir;
@@ -165,6 +176,17 @@ void vcpu_load(struct kvm_vcpu *vcpu)
 {
 	int cpu = get_cpu();
 
+	/*
+	 * 在以下使用percpu的kvm_running_vcpu:
+	 *   - virt/kvm/kvm_main.c|116| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|168| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|180| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|3846| <<__kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+	 *   - virt/kvm/kvm_main.c|6403| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|6421| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|6438| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|6450| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+	 */
 	__this_cpu_write(kvm_running_vcpu, vcpu);
 	preempt_notifier_register(&vcpu->preempt_notifier);
 	kvm_arch_vcpu_load(vcpu, cpu);
@@ -177,6 +199,17 @@ void vcpu_put(struct kvm_vcpu *vcpu)
 	preempt_disable();
 	kvm_arch_vcpu_put(vcpu);
 	preempt_notifier_unregister(&vcpu->preempt_notifier);
+	/*
+	 * 在以下使用percpu的kvm_running_vcpu:
+	 *   - virt/kvm/kvm_main.c|116| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|168| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|180| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|3846| <<__kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+	 *   - virt/kvm/kvm_main.c|6403| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|6421| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|6438| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|6450| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+	 */
 	__this_cpu_write(kvm_running_vcpu, NULL);
 	preempt_enable();
 }
@@ -486,6 +519,17 @@ void kvm_destroy_vcpus(struct kvm *kvm)
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
 		kvm_vcpu_destroy(vcpu);
+		/*
+		 * 在以下使用kvm->vcpu_array:
+		 *   - include/linux/kvm_host.h|991| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+		 *   - include/linux/kvm_host.h|996| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+		 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+		 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+		 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+		 *   - virt/kvm/kvm_main.c|3995| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+		 *   - virt/kvm/kvm_main.c|4214| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|4249| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+		 */
 		xa_erase(&kvm->vcpu_array, i);
 
 		/*
@@ -1122,6 +1166,17 @@ static struct kvm *kvm_create_vm(unsigned long type, const char *fdname)
 	mutex_init(&kvm->slots_arch_lock);
 	spin_lock_init(&kvm->mn_invalidate_lock);
 	rcuwait_init(&kvm->mn_memslots_update_rcuwait);
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|991| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|996| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3995| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4214| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4249| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	xa_init(&kvm->vcpu_array);
 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
 	xa_init(&kvm->mem_attr_array);
@@ -1431,6 +1486,10 @@ EXPORT_SYMBOL_GPL(kvm_unlock_all_vcpus);
  * Allocation size is twice as large as the actual dirty bitmap size.
  * See kvm_vm_ioctl_get_dirty_log() why this is needed.
  */
+/*
+ * 在以下使用kvm_alloc_dirty_bitmap():
+ *   - virt/kvm/kvm_main.c|1746| <<kvm_prepare_memory_region>> r = kvm_alloc_dirty_bitmap(new);
+ */
 static int kvm_alloc_dirty_bitmap(struct kvm_memory_slot *memslot)
 {
 	unsigned long dirty_bytes = kvm_dirty_bitmap_bytes(memslot);
@@ -3505,6 +3564,23 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 }
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
+/*
+ * 在以下使用mark_page_dirty_in_slot():
+ *   - arch/arm64/kvm/mmu.c|1762| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - arch/loongarch/kvm/mmu.c|908| <<kvm_map_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - arch/loongarch/kvm/vcpu.c|196| <<kvm_update_stolen_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/s390/kvm/gaccess.c|872| <<access_guest_page_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/s390/kvm/gaccess.c|1096| <<cmpxchg_guest_abs_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|3544| <<fast_pf_fix_direct_spte>> mark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);
+ *   - arch/x86/kvm/mmu/spte.c|260| <<make_spte>> mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
+ *   - arch/x86/kvm/x86.c|4059| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/x86.c|5425| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - include/linux/kvm_host.h|1999| <<kvm_gpc_mark_dirty_in_slot>> mark_page_dirty_in_slot(gpc->kvm, gpc->memslot, gpa_to_gfn(gpc->gpa));
+ *   - virt/kvm/kvm_main.c|3322| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3460| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3560| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn)
+ *   - virt/kvm/kvm_main.c|3569| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+ */
 void mark_page_dirty_in_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *memslot,
 		 	     gfn_t gfn)
@@ -3522,6 +3598,9 @@ void mark_page_dirty_in_slot(struct kvm *kvm,
 		unsigned long rel_gfn = gfn - memslot->base_gfn;
 		u32 slot = (memslot->as_id << 16) | memslot->id;
 
+		/*
+		 * 只在这里调用kvm_dirty_ring_push()
+		 */
 		if (kvm->dirty_ring_size && vcpu)
 			kvm_dirty_ring_push(vcpu, slot, rel_gfn);
 		else if (memslot->dirty_bitmap)
@@ -3634,6 +3713,12 @@ static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
  * pending.  This is mostly used when halting a vCPU, but may also be used
  * directly for other vCPU non-runnable states, e.g. x86's Wait-For-SIPI.
  */
+/*
+ * 在以下使用kvm_vcpu_block():
+ *   - arch/x86/kvm/x86.c|11511| <<vcpu_block>> kvm_vcpu_block(vcpu);
+ *   - arch/x86/kvm/x86.c|11826| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_block(vcpu);
+ *   - virt/kvm/kvm_main.c|3763| <<kvm_vcpu_halt>> waited = kvm_vcpu_block(vcpu);
+ */
 bool kvm_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	struct rcuwait *wait = kvm_arch_vcpu_get_wait(vcpu);
@@ -3815,6 +3900,17 @@ void __kvm_vcpu_kick(struct kvm_vcpu *vcpu, bool wait)
 	 * kick" check does not need atomic operations if kvm_vcpu_kick is used
 	 * within the vCPU thread itself.
 	 */
+	/*
+	 * 在以下使用percpu的kvm_running_vcpu:
+	 *   - virt/kvm/kvm_main.c|116| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|168| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|180| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|3846| <<__kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+	 *   - virt/kvm/kvm_main.c|6403| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|6421| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|6438| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|6450| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+	 */
 	if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
 		if (vcpu->mode == IN_GUEST_MODE)
 			WRITE_ONCE(vcpu->mode, EXITING_GUEST_MODE);
@@ -3951,6 +4047,16 @@ bool __weak kvm_arch_dy_has_pending_interrupt(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * 在以下使用kvm_vcpu_on_spin():
+ *   -  arch/arm64/kvm/handle_exit.c|162| <<kvm_handle_wfx>> kvm_vcpu_on_spin(vcpu, vcpu_mode_priv(vcpu));
+ *   - arch/riscv/kvm/vcpu_insn.c|212| <<wrs_insn>> kvm_vcpu_on_spin(vcpu, vcpu->arch.guest_context.sstatus & SR_SPP);
+ *   - arch/s390/kvm/diag.c|169| <<__diag_time_slice_end>> kvm_vcpu_on_spin(vcpu, true);
+ *   - arch/x86/kvm/hyperv.c|2600| <<kvm_hv_hypercall>> kvm_vcpu_on_spin(vcpu, true);
+ *   - arch/x86/kvm/svm/svm.c|3299| <<pause_interception>> kvm_vcpu_on_spin(vcpu, in_kernel);
+ *   - arch/x86/kvm/vmx/vmx.c|5914| <<handle_pause>> kvm_vcpu_on_spin(vcpu, true);
+ *   - arch/x86/kvm/xen.c|1590| <<kvm_xen_hcall_sched_op>> kvm_vcpu_on_spin(vcpu, true);
+ */
 void kvm_vcpu_on_spin(struct kvm_vcpu *me, bool yield_to_kernel_mode)
 {
 	int nr_vcpus, start, i, idx, yielded;
@@ -3992,6 +4098,17 @@ void kvm_vcpu_on_spin(struct kvm_vcpu *me, bool yield_to_kernel_mode)
 		if (idx == me->vcpu_idx)
 			continue;
 
+		/*
+		 * 在以下使用kvm->vcpu_array:
+		 *   - include/linux/kvm_host.h|991| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+		 *   - include/linux/kvm_host.h|996| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+		 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+		 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+		 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+		 *   - virt/kvm/kvm_main.c|3995| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+		 *   - virt/kvm/kvm_main.c|4214| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|4249| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+		 */
 		vcpu = xa_load(&kvm->vcpu_array, idx);
 		if (!READ_ONCE(vcpu->ready))
 			continue;
@@ -4211,6 +4328,17 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, unsigned long id)
 	}
 
 	vcpu->vcpu_idx = atomic_read(&kvm->online_vcpus);
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|991| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|996| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3995| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4214| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4249| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
 	WARN_ON_ONCE(r == -EBUSY);
 	if (r)
@@ -6340,6 +6468,17 @@ static void kvm_sched_in(struct preempt_notifier *pn, int cpu)
 	WRITE_ONCE(vcpu->preempted, false);
 	WRITE_ONCE(vcpu->ready, false);
 
+	/*
+	 * 在以下使用percpu的kvm_running_vcpu:
+	 *   - virt/kvm/kvm_main.c|116| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|168| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|180| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|3846| <<__kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+	 *   - virt/kvm/kvm_main.c|6403| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|6421| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|6438| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|6450| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+	 */
 	__this_cpu_write(kvm_running_vcpu, vcpu);
 	kvm_arch_vcpu_load(vcpu, cpu);
 
@@ -6358,6 +6497,17 @@ static void kvm_sched_out(struct preempt_notifier *pn,
 		WRITE_ONCE(vcpu->ready, true);
 	}
 	kvm_arch_vcpu_put(vcpu);
+	/*
+	 * 在以下使用percpu的kvm_running_vcpu:
+	 *   - virt/kvm/kvm_main.c|116| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|168| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|180| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|3846| <<__kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+	 *   - virt/kvm/kvm_main.c|6403| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|6421| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|6438| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|6450| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+	 */
 	__this_cpu_write(kvm_running_vcpu, NULL);
 }
 
@@ -6370,11 +6520,39 @@ static void kvm_sched_out(struct preempt_notifier *pn,
  * the per-CPU value later will give us the same value as we update the
  * per-CPU variable in the preempt notifier handlers.
  */
+/*
+ * 在以下使用kvm_get_running_vcpu():
+ *   - arch/arm64/kvm/arch_timer.c|1595| <<kvm_arch_timer_get_input_level>> struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|23| <<enter_vmid_context>> struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
+ *   - arch/arm64/kvm/pmu.c|186| <<kvm_set_pmuserenr>> vcpu = kvm_get_running_vcpu();
+ *   - arch/arm64/kvm/pmu.c|206| <<kvm_vcpu_pmu_resync_el0>> vcpu = kvm_get_running_vcpu();
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|474| <<vgic_access_active_prepare>> vcpu != kvm_get_running_vcpu()) ||
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|483| <<vgic_access_active_finish>> vcpu != kvm_get_running_vcpu()) ||
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|550| <<vgic_mmio_change_active>> struct kvm_vcpu *requester_vcpu = kvm_get_running_vcpu();
+ *   - arch/x86/kvm/lapic.c|1938| <<apic_timer_expired>> WARN_ON(kvm_get_running_vcpu() != vcpu);
+ *   - arch/x86/kvm/vmx/common.h|144| <<kvm_vcpu_trigger_posted_interrupt>> if (vcpu != kvm_get_running_vcpu())
+ *   - arch/x86/kvm/vmx/vmx.c|8362| <<vmx_handle_intel_pt_intr>> struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
+ *   - arch/x86/kvm/vmx/vmx_onhyperv.h|112| <<evmcs_load>> if (KVM_BUG_ON(!vp_ap, kvm_get_running_vcpu()->kvm))
+ *   - virt/kvm/kvm_main.c|3534| <<mark_page_dirty_in_slot>> struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
+ *   - virt/kvm/kvm_main.c|6456| <<kvm_guest_state>> struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
+ *   - virt/kvm/kvm_main.c|6471| <<kvm_guest_get_ip>> struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
+ */
 struct kvm_vcpu *kvm_get_running_vcpu(void)
 {
 	struct kvm_vcpu *vcpu;
 
 	preempt_disable();
+	/*
+	 * 在以下使用percpu的kvm_running_vcpu:
+	 *   - virt/kvm/kvm_main.c|116| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|168| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|180| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|3846| <<__kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+	 *   - virt/kvm/kvm_main.c|6403| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|6421| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|6438| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|6450| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+	 */
 	vcpu = __this_cpu_read(kvm_running_vcpu);
 	preempt_enable();
 
diff --git a/virt/kvm/pfncache.c b/virt/kvm/pfncache.c
index 728d2c1b4..6498af348 100644
--- a/virt/kvm/pfncache.c
+++ b/virt/kvm/pfncache.c
@@ -394,6 +394,11 @@ void kvm_gpc_init(struct gfn_to_pfn_cache *gpc, struct kvm *kvm)
 	gpc->active = gpc->valid = false;
 }
 
+/*
+ * 在以下使用__kvm_gpc_activate():
+ *   - virt/kvm/pfncache.c|436| <<kvm_gpc_activate>> return __kvm_gpc_activate(gpc, gpa, KVM_HVA_ERR_BAD, len);
+ *   - virt/kvm/pfncache.c|444| <<kvm_gpc_activate_hva>> return __kvm_gpc_activate(gpc, INVALID_GPA, uhva, len);
+ */
 static int __kvm_gpc_activate(struct gfn_to_pfn_cache *gpc, gpa_t gpa, unsigned long uhva,
 			      unsigned long len)
 {
-- 
2.39.5 (Apple Git-154)

