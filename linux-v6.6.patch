From 18fd404e0809dc005423951bda54e1d5bf85f1e3 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 28 Nov 2023 06:20:58 -0800
Subject: [PATCH 1/1] linux-v6.6

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/kvm/pmu-emul.c          |   9 ++
 arch/x86/events/amd/core.c         |  29 ++++++
 arch/x86/events/core.c             |  70 +++++++++++++
 arch/x86/events/perf_event.h       |  11 ++
 arch/x86/include/asm/kvm_host.h    |  65 ++++++++++++
 arch/x86/kernel/nmi.c              |  13 +++
 arch/x86/kvm/pmu.c                 | 129 ++++++++++++++++++++++++
 arch/x86/kvm/pmu.h                 | 155 +++++++++++++++++++++++++++++
 arch/x86/kvm/svm/pmu.c             | 123 +++++++++++++++++++++++
 arch/x86/kvm/x86.c                 |  23 +++++
 drivers/acpi/bus.c                 |   4 +
 drivers/acpi/osl.c                 |  17 ++++
 drivers/acpi/scan.c                |   4 +
 drivers/pci/hotplug/acpiphp_glue.c |  35 +++++++
 drivers/pci/probe.c                |  14 +++
 drivers/pci/setup-bus.c            |  69 +++++++++++++
 drivers/scsi/scsi_scan.c           |   4 +
 drivers/scsi/virtio_scsi.c         |  78 +++++++++++++++
 drivers/vhost/scsi.c               |  39 ++++++++
 drivers/vhost/vhost.c              |  79 +++++++++++++++
 drivers/vhost/vhost.h              |  56 +++++++++++
 drivers/virtio/virtio_ring.c       |  55 ++++++++++
 kernel/events/core.c               |  55 ++++++++++
 kernel/watchdog_perf.c             |  12 +++
 24 files changed, 1148 insertions(+)

diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
index 6b066e04d..e34f10a81 100644
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@ -102,6 +102,15 @@ static u64 kvm_pmu_get_pmc_value(struct kvm_pmc *pmc)
 	reg = counter_index_to_reg(pmc->idx);
 	counter = __vcpu_sys_reg(vcpu, reg);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|110| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|213| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|437| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/x86/kvm/pmu.h|71| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 *   - tools/include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 */
 	/*
 	 * The real counter value is equal to the value of counter register plus
 	 * the value perf event counts.
diff --git a/arch/x86/events/amd/core.c b/arch/x86/events/amd/core.c
index e24976593..bb6ab9487 100644
--- a/arch/x86/events/amd/core.c
+++ b/arch/x86/events/amd/core.c
@@ -832,6 +832,19 @@ static void amd_pmu_del_event(struct perf_event *event)
  * handled a counter. When an un-handled NMI is received, it will be claimed
  * only if arriving within that window.
  */
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ */
 static inline int amd_pmu_adjust_nmi_window(int handled)
 {
 	/*
@@ -850,6 +863,19 @@ static inline int amd_pmu_adjust_nmi_window(int handled)
 	return NMI_HANDLED;
 }
 
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ */
 static int amd_pmu_handle_irq(struct pt_regs *regs)
 {
 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
@@ -869,6 +895,9 @@ static int amd_pmu_handle_irq(struct pt_regs *regs)
 	if (cpuc->lbr_users)
 		amd_brs_drain();
 
+	/*
+	 * 比如x86_perf_event_update()
+	 */
 	/* Process any counter overflows */
 	handled = x86_pmu_handle_irq(regs);
 
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index bba3650c5..b91024874 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -112,6 +112,30 @@ u64 __read_mostly hw_cache_extra_regs
  * Can only be executed on the CPU where the event is active.
  * Returns the delta events processed.
  */
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ *
+ * 在以下使用x86_perf_event_update():
+ *   - arch/x86/events/intel/core.c|2721| <<global>> DEFINE_STATIC_CALL(intel_pmu_update_topdown_event, x86_perf_event_update);
+ *   - arch/x86/events/amd/core.c|957| <<amd_pmu_v2_handle_irq>> x86_perf_event_update(event);
+ *   - arch/x86/events/core.c|2175| <<init_hw_perf_events>> x86_pmu.update = x86_perf_event_update;
+ *   - arch/x86/events/intel/core.c|2744| <<intel_pmu_read_event>> x86_perf_event_update(event);
+ *   - arch/x86/events/intel/core.c|2872| <<intel_pmu_update>> return x86_perf_event_update(event);
+ *   - arch/x86/events/intel/p4.c|1061| <<p4_pmu_handle_irq>> val = x86_perf_event_update(event);
+ *   - arch/x86/events/zhaoxin/core.c|394| <<zhaoxin_pmu_handle_irq>> x86_perf_event_update(event);
+ *
+ * 核心思想是比较旧的count和新的count
+ */
 u64 x86_perf_event_update(struct perf_event *event)
 {
 	struct hw_perf_event *hwc = &event->hw;
@@ -1661,6 +1685,19 @@ static void x86_pmu_del(struct perf_event *event, int flags)
 	static_call_cond(x86_pmu_del)(event);
 }
 
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ */
 int x86_pmu_handle_irq(struct pt_regs *regs)
 {
 	struct perf_sample_data data;
@@ -1687,7 +1724,25 @@ int x86_pmu_handle_irq(struct pt_regs *regs)
 
 		event = cpuc->events[idx];
 
+		/*
+		 * 在以下使用x86_pmu_update:
+		 *   - arch/x86/events/core.c|76| <<global>> DEFINE_STATIC_CALL_NULL(x86_pmu_update, *x86_pmu.update);
+		 *   - arch/x86/events/perf_event.h|1054| <<global>> DECLARE_STATIC_CALL(x86_pmu_update, *x86_pmu.update);
+		 *   - arch/x86/events/core.c|1600| <<x86_pmu_stop>> static_call(x86_pmu_update)(event);
+		 *   - arch/x86/events/core.c|1690| <<x86_pmu_handle_irq>> val = static_call(x86_pmu_update)(event);
+		 *   - arch/x86/events/core.c|2019| <<x86_pmu_static_call_update>> static_call_update(x86_pmu_update, x86_pmu.update);
+		 *   - arch/x86/events/core.c|2042| <<_x86_pmu_read>> static_call(x86_pmu_update)(event);
+		 *   - arch/x86/events/intel/core.c|2351| <<intel_pmu_nhm_workaround>> static_call(x86_pmu_update)(event);
+		 *   - arch/x86/events/intel/core.c|2844| <<intel_pmu_save_and_restart>> static_call(x86_pmu_update)(event);
+		 *
+		 * 比如x86_perf_event_update()
+		 *
+		 * val返回的是counter的最新值
+		 */
 		val = static_call(x86_pmu_update)(event);
+		/*
+		 * 这里是判断counter最高位有没有1, 也就是有没有溢出
+		 */
 		if (val & (1ULL << (x86_pmu.cntval_bits - 1)))
 			continue;
 
@@ -1726,6 +1781,18 @@ void perf_events_lapic_init(void)
 }
 
 /*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ *
  * called by:
  *   - arch/sparc/kernel/perf_event.c|1683| <<global>> .notifier_call = perf_event_nmi_handler,
  *   - arch/x86/events/core.c|2102| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
@@ -1745,6 +1812,9 @@ perf_event_nmi_handler(unsigned int cmd, struct pt_regs *regs)
 		return NMI_DONE;
 
 	start_clock = sched_clock();
+	/*
+	 * amd_pmu_handle_irq()
+	 */
 	ret = static_call(x86_pmu_handle_irq)(regs);
 	finish_clock = sched_clock();
 
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index c8ba2be75..edb00dc37 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -1051,6 +1051,17 @@ struct pmu *x86_get_pmu(unsigned int cpu);
 extern struct x86_pmu x86_pmu __read_mostly;
 
 DECLARE_STATIC_CALL(x86_pmu_set_period, *x86_pmu.set_period);
+/*
+ * 在以下使用x86_pmu_update:
+ *   - arch/x86/events/core.c|76| <<global>> DEFINE_STATIC_CALL_NULL(x86_pmu_update, *x86_pmu.update);
+ *   - arch/x86/events/perf_event.h|1054| <<global>> DECLARE_STATIC_CALL(x86_pmu_update, *x86_pmu.update);
+ *   - arch/x86/events/core.c|1600| <<x86_pmu_stop>> static_call(x86_pmu_update)(event);
+ *   - arch/x86/events/core.c|1690| <<x86_pmu_handle_irq>> val = static_call(x86_pmu_update)(event);
+ *   - arch/x86/events/core.c|2019| <<x86_pmu_static_call_update>> static_call_update(x86_pmu_update, x86_pmu.update);
+ *   - arch/x86/events/core.c|2042| <<_x86_pmu_read>> static_call(x86_pmu_update)(event);
+ *   - arch/x86/events/intel/core.c|2351| <<intel_pmu_nhm_workaround>> static_call(x86_pmu_update)(event);
+ *   - arch/x86/events/intel/core.c|2844| <<intel_pmu_save_and_restart>> static_call(x86_pmu_update)(event);
+ */
 DECLARE_STATIC_CALL(x86_pmu_update,     *x86_pmu.update);
 
 static __always_inline struct x86_perf_task_context_opt *task_context_opt(void *ctx)
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 6d9a74051..dac26f2b8 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -87,6 +87,8 @@
  *   - arch/x86/kvm/x86.c|12866| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
  *   - arch/x86/kvm/x86.c|12919| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
  *
+ * 应该是PMI插入NMI(PMI)的事件
+ *
  * process_nmi()
  */
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
@@ -99,6 +101,8 @@
  *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
  *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
  *
+ * 应该是重新program PMU的事件
+ *
  * kvm_pmu_handle_event()
  */
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
@@ -520,6 +524,15 @@ enum pmc_type {
 struct kvm_pmc {
 	enum pmc_type type;
 	u8 idx;
+	/*
+	 * 在以下使用kvm_pmc->is_paused:
+	 *   - arch/x86/kvm/pmu.c|255| <<pmc_reprogram_counter>> pmc->is_paused = false;
+	 *   - arch/x86/kvm/pmu.c|268| <<pmc_pause_counter>> if (!pmc->perf_event || pmc->is_paused)
+	 *   - arch/x86/kvm/pmu.c|274| <<pmc_pause_counter>> pmc->is_paused = true;
+	 *   - arch/x86/kvm/pmu.c|294| <<pmc_resume_counter>> pmc->is_paused = false;
+	 *   - arch/x86/kvm/pmu.h|88| <<pmc_read_counter>> if (pmc->perf_event && !pmc->is_paused)
+	 *   - arch/x86/kvm/pmu.h|184| <<pmc_update_sample_period>> if (!pmc->perf_event || pmc->is_paused ||
+	 */
 	bool is_paused;
 	bool intr;
 	u64 counter;
@@ -534,6 +547,11 @@ struct kvm_pmc {
 	 */
 	u64 prev_counter;
 	u64 eventsel;
+	/*
+	 * 在以下使用kvm_pmu->perf_event:
+	 *   - arch/x86/kvm/pmu.c|253| <<pmc_reprogram_counter>> pmc->perf_event = event;
+	 *   - arch/x86/kvm/pmu.h|111| <<pmc_release_perf_event>> pmc->perf_event = NULL;
+	 */
 	struct perf_event *perf_event;
 	struct kvm_vcpu *vcpu;
 	/*
@@ -553,6 +571,15 @@ struct kvm_pmc {
 #define KVM_AMD_PMC_MAX_GENERIC	6
 struct kvm_pmu {
 	u8 version;
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|196| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = ebx.split.num_core_pmc;
+	 *   - arch/x86/kvm/svm/pmu.c|198| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+	 *   - arch/x86/kvm/svm/pmu.c|200| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+	 *   - arch/x86/kvm/svm/pmu.c|203| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(unsigned int , pmu->nr_arch_gp_counters, kvm_pmu_cap.num_counters_gp);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|496| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|529| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+	 */
 	unsigned nr_arch_gp_counters;
 	unsigned nr_arch_fixed_counters;
 	unsigned available_event_types;
@@ -561,7 +588,26 @@ struct kvm_pmu {
 	u64 global_ctrl;
 	u64 global_status;
 	u64 counter_bitmask[2];
+	/*
+	 * 在以下使用kvm_pmu->global_ctrl_mask:
+	 *   - arch/x86/kvm/pmu.c|698| <<kvm_pmu_set_msr>> data &= ~pmu->global_ctrl_mask;
+	 *   - arch/x86/kvm/pmu.h|138| <<kvm_valid_perf_global_ctrl>> return !(pmu->global_ctrl_mask & data);
+	 *   - arch/x86/kvm/svm/pmu.c|262| <<amd_pmu_refresh>> pmu->global_ctrl_mask = ~((1ull << pmu->nr_arch_gp_counters) - 1);
+	 *   - arch/x86/kvm/svm/pmu.c|263| <<amd_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|503| <<intel_pmu_refresh>> pmu->global_ctrl_mask = ~0ull;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|555| <<intel_pmu_refresh>> pmu->global_ctrl_mask = counter_mask;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|562| <<intel_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask & ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF | MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
+	 */
 	u64 global_ctrl_mask;
+	/*
+	 * 在以下使用kvm_pmu->global_status_mask:
+	 *   - arch/x86/kvm/pmu.c|692| <<kvm_pmu_set_msr>> if (data & pmu->global_status_mask)
+	 *   - arch/x86/kvm/pmu.c|715| <<kvm_pmu_set_msr>> if (data & pmu->global_status_mask)
+	 *   - arch/x86/kvm/svm/pmu.c|263| <<amd_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|504| <<intel_pmu_refresh>> pmu->global_status_mask = ~0ull;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|562| <<intel_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|566| <<intel_pmu_refresh>> pmu->global_status_mask &= ~MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI;
+	 */
 	u64 global_status_mask;
 	u64 reserved_bits;
 	u64 raw_event_mask;
@@ -574,6 +620,16 @@ struct kvm_pmu {
 	 * filter changes.
 	 */
 	union {
+		/*
+		 * 在以下使用kvm_pmu->reprogram_pmi:
+		 *   - arch/x86/kvm/pmu.c|160| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+		 *   - arch/x86/kvm/pmu.c|486| <<reprogram_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|499| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+		 *   - arch/x86/kvm/pmu.c|503| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|1012| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) > sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+		 *   - arch/x86/kvm/pmu.h|249| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.h|261| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+		 */
 		DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
 		atomic64_t __reprogram_pmi;
 	};
@@ -612,6 +668,15 @@ struct kvm_pmu {
 	 * The total number of programmed perf_events and it helps to avoid
 	 * redundant check before cleanup if guest don't use vPMU at all.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->event_count:
+	 *   - rch/x86/kvm/pmu.c|287| <<pmc_reprogram_counter>> pmc_to_pmu(pmc)->event_count++;
+	 *   - arch/x86/kvm/pmu.c|848| <<kvm_pmu_init>> pmu->event_count = 0;
+	 *   - arch/x86/kvm/pmu.h|118| <<pmc_release_perf_event>> pmc_to_pmu(pmc)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|250| <<intel_pmu_release_guest_lbr_event>> vcpu_to_pmu(vcpu)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|301| <<intel_pmu_create_guest_lbr_event>> pmu->event_count++;
+	 *   - arch/x86/kvm/x86.c|12434| <<kvm_arch_sched_in>> if (pmu->version && unlikely(pmu->event_count)) {
+	 */
 	u8 event_count;
 };
 
diff --git a/arch/x86/kernel/nmi.c b/arch/x86/kernel/nmi.c
index a0c551846..d503ba3cb 100644
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@ -127,6 +127,19 @@ static void nmi_check_duration(struct nmiaction *action, u64 duration)
 		action->handler, duration, decimal_msecs);
 }
 
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ */
 static int nmi_handle(unsigned int type, struct pt_regs *regs)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index c3cbc737d..cf4af89ad 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -175,6 +175,8 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 	 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
 	 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
 	 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * kvm_pmu_handle_event()
 	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
@@ -201,6 +203,21 @@ static u64 pmc_get_pebs_precise_level(struct kvm_pmc *pmc)
 	return 1;
 }
 
+/*
+ * 应该是重新program PMU的事件
+ * vcpu_enter_guest(KVM_REQ_PMU)
+ * -> kvm_pmu_handle_event()
+ *    -> reprogram_counter()
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|478| <<reprogram_counter>> if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ *
+ * 495         if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ * 496                                   (eventsel & pmu->raw_event_mask),
+ * 497                                   !(eventsel & ARCH_PERFMON_EVENTSEL_USR),
+ * 498                                   !(eventsel & ARCH_PERFMON_EVENTSEL_OS),
+ * 499                                   eventsel & ARCH_PERFMON_EVENTSEL_INT))
+ */
 static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 				 bool exclude_user, bool exclude_kernel,
 				 bool intr)
@@ -240,6 +257,19 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 		attr.precise_ip = pmc_get_pebs_precise_level(pmc);
 	}
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|634| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|250| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(attr, -1, current, NULL, pmc);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|968| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|973| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kvm/pmu.c|230| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|293| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+	 *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+	 *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+	 *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>> return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+	 *   - kernel/watchdog_perf.c|157| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 */
 	event = perf_event_create_kernel_counter(&attr, -1, current,
 						 kvm_perf_overflow, pmc);
 	if (IS_ERR(event)) {
@@ -248,6 +278,11 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 		return PTR_ERR(event);
 	}
 
+	/*
+	 * 在以下使用kvm_pmu->perf_event:
+	 *   - arch/x86/kvm/pmu.c|253| <<pmc_reprogram_counter>> pmc->perf_event = event;
+	 *   - arch/x86/kvm/pmu.h|111| <<pmc_release_perf_event>> pmc->perf_event = NULL;
+	 */
 	pmc->perf_event = event;
 	pmc_to_pmu(pmc)->event_count++;
 	pmc->is_paused = false;
@@ -272,11 +307,26 @@ static void pmc_pause_counter(struct kvm_pmc *pmc)
 	pmc->is_paused = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|513| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+ */
 static bool pmc_resume_counter(struct kvm_pmc *pmc)
 {
+	/*
+	 * 在以下使用kvm_pmu->perf_event:
+	 *   - arch/x86/kvm/pmu.c|253| <<pmc_reprogram_counter>> pmc->perf_event = event;
+	 *   - arch/x86/kvm/pmu.h|111| <<pmc_release_perf_event>> pmc->perf_event = NULL;
+	 */
 	if (!pmc->perf_event)
 		return false;
 
+	/*
+	 * called by:
+	 *   - arch/riscv/kvm/vcpu_pmu.c|380| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_period(pmc->perf_event, kvm_pmu_get_sample_period(pmc));
+	 *   - arch/x86/kvm/pmu.c|317| <<pmc_resume_counter>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+	 *   - arch/x86/kvm/pmu.h|200| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter));
+	 */
 	/* recalibrate sample period and check if it's accepted by perf core */
 	if (is_sampling_event(pmc->perf_event) &&
 	    perf_event_period(pmc->perf_event,
@@ -417,6 +467,13 @@ static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 
 static bool pmc_event_is_allowed(struct kvm_pmc *pmc)
 {
+	/*
+	 * 一共四个条件:
+	 * - pmc_is_globally_enabled(pmc)
+	 * - pmc_speculative_in_use(pmc)
+	 * - static_call(kvm_x86_pmu_hw_event_available)(pmc)
+	 * - check_pmu_event_filter(pmc)
+	 */
 	return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
 	       static_call(kvm_x86_pmu_hw_event_available)(pmc) &&
 	       check_pmu_event_filter(pmc);
@@ -437,11 +494,19 @@ static void reprogram_counter(struct kvm_pmc *pmc)
 	u64 new_config = eventsel;
 	u8 fixed_ctr_ctrl;
 
+	/*
+	 * 只在此处调用
+	 */
 	pmc_pause_counter(pmc);
 
 	if (!pmc_event_is_allowed(pmc))
 		goto reprogram_complete;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+	 *   - arch/x86/kvm/pmu.c|403| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+	 */
 	if (pmc->counter < pmc->prev_counter)
 		__kvm_perf_overflow(pmc, false);
 
@@ -463,6 +528,11 @@ static void reprogram_counter(struct kvm_pmc *pmc)
 	if (pmc->current_config == new_config && pmc_resume_counter(pmc))
 		goto reprogram_complete;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|468| <<reprogram_counter>> pmc_release_perf_event(pmc);
+	 *   - arch/x86/kvm/pmu.h|121| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+	 */
 	pmc_release_perf_event(pmc);
 
 	pmc->current_config = new_config;
@@ -481,6 +551,16 @@ static void reprogram_counter(struct kvm_pmc *pmc)
 		return;
 
 reprogram_complete:
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|160| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|486| <<reprogram_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|499| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/pmu.c|503| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1012| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) > sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.h|249| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|261| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 */
 	clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
 	pmc->prev_counter = 0;
 }
@@ -494,6 +574,16 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 	int bit;
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|160| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|486| <<reprogram_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|499| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/pmu.c|503| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1012| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) > sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.h|249| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|261| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 */
 	for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
 		struct kvm_pmc *pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, bit);
 
@@ -502,9 +592,25 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 			continue;
 		}
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/pmu.c|458| <<kvm_pmu_handle_event>> reprogram_counter(pmc);
+		 *
+		 * 应该是重新program PMU的事件
+		 * vcpu_enter_guest(KVM_REQ_PMU)
+		 * -> kvm_pmu_handle_event()
+		 *    -> reprogram_counter()
+		 */
 		reprogram_counter(pmc);
 	}
 
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|488| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|723| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|735| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12399| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 */
 	/*
 	 * Unused perf_events are only released if the corresponding MSRs
 	 * weren't accessed during the last vCPU time slice. kvm_arch_sched_in
@@ -627,6 +733,10 @@ bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 		static_call(kvm_x86_pmu_is_valid_msr)(vcpu, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|830| <<kvm_pmu_set_msr>> kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+ */
 static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -718,7 +828,16 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 			pmu->global_status &= ~data;
 		break;
 	default:
+		/*
+		 * 只在这里调用
+		 */
 		kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+		/*
+		 * intel_pmu_set_msr()
+		 * amd_pmu_set_msr()
+		 *
+		 * 只在这里调用kvm_x86_pmu_set_msr
+		 */
 		return static_call(kvm_x86_pmu_set_msr)(vcpu, msr_info);
 	}
 
@@ -776,6 +895,10 @@ void kvm_pmu_init(struct kvm_vcpu *vcpu)
 	kvm_pmu_refresh(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|516| <<kvm_pmu_handle_event>> kvm_pmu_cleanup(vcpu);
+ */
 /* Release perf_events for vPMCs that have been unused for a full time slice.  */
 void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
@@ -792,6 +915,12 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 	for_each_set_bit(i, bitmask, X86_PMC_IDX_MAX) {
 		pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, i);
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/pmu.c|477| <<pmc_event_is_allowed>> return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
+		 *   - arch/x86/kvm/pmu.c|905| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|777| <<intel_pmu_cross_mapped_check>> if (!pmc || !pmc_speculative_in_use(pmc) ||
+		 */
 		if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
 			pmc_stop_counter(pmc);
 	}
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 5d378ddcd..83b1e45dc 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -62,11 +62,29 @@ static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 	return pmu->counter_bitmask[pmc->type];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|579| <<kvm_pmu_rdpmc>> *data = pmc_read_counter(pmc) & mask;
+ *   - arch/x86/kvm/pmu.h|85| <<pmc_write_counter>> pmc->counter += val - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/pmu.h|102| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|140| <<amd_pmu_get_msr>> msr_info->data = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|370| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|375| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ */
 static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 {
 	u64 counter, enabled, running;
 
 	counter = pmc->counter;
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|110| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|213| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|437| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/x86/kvm/pmu.h|71| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 *   - tools/include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 */
 	if (pmc->perf_event && !pmc->is_paused)
 		counter += perf_event_read_value(pmc->perf_event,
 						 &enabled, &running);
@@ -86,20 +104,64 @@ static inline void pmc_write_counter(struct kvm_pmc *pmc, u64 val)
 	pmc->counter &= pmc_bitmask(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|468| <<reprogram_counter>> pmc_release_perf_event(pmc);
+ *   - arch/x86/kvm/pmu.h|121| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+ */
 static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/pmu-emul.c|190| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/riscv/kvm/vcpu_pmu.c|81| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1057| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1059| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+		 *   - arch/x86/kvm/pmu.h|115| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|248| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+		 *   - kernel/events/core.c|5395| <<perf_release>> perf_event_release_kernel(file->private_data);
+		 *   - kernel/events/hw_breakpoint.c|829| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+		 *   - kernel/watchdog_perf.c|330| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+		 *   - kernel/watchdog_perf.c|406| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+		 */
 		perf_event_release_kernel(pmc->perf_event);
+		/*
+		 * 在以下使用kvm_pmu->perf_event:
+		 *   - arch/x86/kvm/pmu.c|253| <<pmc_reprogram_counter>> pmc->perf_event = event;
+		 *   - arch/x86/kvm/pmu.h|111| <<pmc_release_perf_event>> pmc->perf_event = NULL;
+		 */
 		pmc->perf_event = NULL;
 		pmc->current_config = 0;
+		/*
+		 * 在以下使用kvm_pmu->event_count:
+		 *   - arch/x86/kvm/pmu.c|287| <<pmc_reprogram_counter>> pmc_to_pmu(pmc)->event_count++;
+		 *   - arch/x86/kvm/pmu.c|848| <<kvm_pmu_init>> pmu->event_count = 0;
+		 *   - arch/x86/kvm/pmu.h|118| <<pmc_release_perf_event>> pmc_to_pmu(pmc)->event_count--;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|250| <<intel_pmu_release_guest_lbr_event>> vcpu_to_pmu(vcpu)->event_count--;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|301| <<intel_pmu_create_guest_lbr_event>> pmu->event_count++;
+		 *   - arch/x86/kvm/x86.c|12434| <<kvm_arch_sched_in>> if (pmu->version && unlikely(pmu->event_count)) {
+		 */
 		pmc_to_pmu(pmc)->event_count--;
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|798| <<kvm_pmu_cleanup>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|344| <<amd_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|642| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|649| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ */
 static inline void pmc_stop_counter(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
 		pmc->counter = pmc_read_counter(pmc);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/pmu.c|468| <<reprogram_counter>> pmc_release_perf_event(pmc);
+		 *   - arch/x86/kvm/pmu.h|121| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+		 */
 		pmc_release_perf_event(pmc);
 	}
 }
@@ -124,6 +186,21 @@ static inline bool kvm_valid_perf_global_ctrl(struct kvm_pmu *pmu,
  * used for both PERFCTRn and EVNTSELn; that is why it accepts base as a
  * parameter to tell them apart.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|95| <<intel_pmc_idx_to_pmc>> return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|179| <<get_fw_gp_pmc>> return get_gp_pmc(pmu, msr, MSR_IA32_PMC0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|221| <<intel_is_valid_msr>> ret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|222| <<intel_is_valid_msr>> get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|237| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|238| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|368| <<intel_pmu_get_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|369| <<intel_pmu_get_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|379| <<intel_pmu_get_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|430| <<intel_pmu_set_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|431| <<intel_pmu_set_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|446| <<intel_pmu_set_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ */
 static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 					 u32 base)
 {
@@ -137,6 +214,15 @@ static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|85| <<reprogram_fixed_counters>> pmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|100| <<intel_pmc_idx_to_pmc>> return get_fixed_pmc(pmu, idx + MSR_CORE_PERF_FIXED_CTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|223| <<intel_is_valid_msr>> get_fixed_pmc(pmu, msr) || get_fw_gp_pmc(pmu, msr) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|236| <<intel_msr_idx_to_pmc>> pmc = get_fixed_pmc(pmu, msr);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|374| <<intel_pmu_get_msr>> } else if ((pmc = get_fixed_pmc(pmu, msr))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|442| <<intel_pmu_set_msr>> } else if ((pmc = get_fixed_pmc(pmu, msr))) {
+ */
 /* returns fixed PMC with the specified MSR */
 static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 {
@@ -152,6 +238,12 @@ static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 	return NULL;
 }
 
+/*
+ * 在以下使用get_sample_period():
+ *   - arch/x86/kvm/pmu.c|239| <<pmc_reprogram_counter>> attr.sample_period = get_sample_period(pmc, pmc->counter);
+ *   - arch/x86/kvm/pmu.c|333| <<pmc_resume_counter>> get_sample_period(pmc, pmc->counter)))
+ *   - arch/x86/kvm/pmu.h|225| <<pmc_update_sample_period>> get_sample_period(pmc, pmc->counter));
+ */
 static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 {
 	u64 sample_period = (-counter_value) & pmc_bitmask(pmc);
@@ -161,16 +253,34 @@ static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 	return sample_period;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|229| <<amd_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|440| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|444| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ */
 static inline void pmc_update_sample_period(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event || pmc->is_paused ||
 	    !is_sampling_event(pmc->perf_event))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/riscv/kvm/vcpu_pmu.c|380| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_period(pmc->perf_event, kvm_pmu_get_sample_period(pmc));
+	 *   - arch/x86/kvm/pmu.c|317| <<pmc_resume_counter>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+	 *   - arch/x86/kvm/pmu.h|200| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter));
+	 */
 	perf_event_period(pmc->perf_event,
 			  get_sample_period(pmc, pmc->counter));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|477| <<pmc_event_is_allowed>> return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
+ *   - arch/x86/kvm/pmu.c|905| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|777| <<intel_pmu_cross_mapped_check>> if (!pmc || !pmc_speculative_in_use(pmc) ||
+ */
 static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -184,6 +294,10 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 
 extern struct x86_pmu_capability kvm_pmu_cap;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9570| <<__kvm_x86_vendor_init>> kvm_init_pmu_capability(ops->pmu_ops);
+ */
 static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
@@ -226,12 +340,35 @@ static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 					     KVM_PMC_MAX_FIXED);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|828| <<kvm_pmu_incr_counter>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|225| <<amd_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|88| <<reprogram_fixed_counters>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|456| <<intel_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+ */
 static inline void kvm_pmu_request_counter_reprogram(struct kvm_pmc *pmc)
 {
 	set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|796| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_CTRL)>> reprogram_counters(pmu, diff);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|414| <<intel_pmu_set_msr>> reprogram_counters(pmu, diff);
+ */
 static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 {
 	int bit;
@@ -241,6 +378,19 @@ static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 
 	for_each_set_bit(bit, (unsigned long *)&diff, X86_PMC_IDX_MAX)
 		set_bit(bit, pmu->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * 应该是重新program PMU的事件
+	 *
+	 * kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
 }
 
@@ -249,6 +399,11 @@ static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
  *
  * If the vPMU doesn't have global_ctrl MSR, all vPMCs are enabled.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|462| <<pmc_event_is_allowed>> return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
+ *   - arch/x86/kvm/vmx/pmu_intel.c|778| <<intel_pmu_cross_mapped_check>> !pmc_is_globally_enabled(pmc) || !pmc->perf_event)
+ */
 static inline bool pmc_is_globally_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
diff --git a/arch/x86/kvm/svm/pmu.c b/arch/x86/kvm/svm/pmu.c
index 373ff6a66..5635deea6 100644
--- a/arch/x86/kvm/svm/pmu.c
+++ b/arch/x86/kvm/svm/pmu.c
@@ -25,16 +25,47 @@ enum pmu_type {
 	PMU_TYPE_EVNTSEL,
 };
 
+/*
+ * 在以下使用amd_pmc_idx_to_pmc():
+ *   - arch/x86/kvm/svm/pmu.c|253| <<global>> struct kvm_pmu_ops amd_pmu_ops.pmc_idx_to_pmc = amd_pmc_idx_to_pmc,
+ *   - arch/x86/kvm/svm/pmu.c|73| <<get_gp_pmc_amd>> return amd_pmc_idx_to_pmc(pmu, idx);
+ *   - arch/x86/kvm/svm/pmu.c|94| <<amd_rdpmc_ecx_to_pmc>> return amd_pmc_idx_to_pmc(vcpu_to_pmu(vcpu), idx & ~(3u << 30));
+ *
+ * struct kvm_pmu_ops amd_pmu_ops.pmc_idx_to_pmc = amd_pmc_idx_to_pmc()
+ */
 static struct kvm_pmc *amd_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|196| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = ebx.split.num_core_pmc;
+	 *   - arch/x86/kvm/svm/pmu.c|198| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+	 *   - arch/x86/kvm/svm/pmu.c|200| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+	 *   - arch/x86/kvm/svm/pmu.c|203| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(unsigned int , pmu->nr_arch_gp_counters, kvm_pmu_cap.num_counters_gp);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|496| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|529| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+	 */
 	unsigned int num_counters = pmu->nr_arch_gp_counters;
 
 	if (pmc_idx >= num_counters)
 		return NULL;
 
+	/*
+	 * struct kvm_pmu *pmu:
+	 * -> struct kvm_pmc gp_counters[KVM_INTEL_PMC_MAX_GENERIC];
+	 * -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+	 */
 	return &pmu->gp_counters[array_index_nospec(pmc_idx, num_counters)];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|102| <<amd_msr_idx_to_pmc>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|103| <<amd_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|138| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|144| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|161| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|168| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ */
 static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 					     enum pmu_type type)
 {
@@ -73,11 +104,17 @@ static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 	return amd_pmc_idx_to_pmc(pmu, idx);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.hw_event_available = amd_hw_event_available()
+ */
 static bool amd_hw_event_available(struct kvm_pmc *pmc)
 {
 	return true;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.is_valid_rdpmc_ecx = amd_is_valid_rdpmc_ecx()
+ */
 static bool amd_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -88,12 +125,18 @@ static bool amd_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 }
 
 /* idx is the ECX register of RDPMC instruction */
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.rdpmc_ecx_to_pmc = amd_rdpmc_ecx_to_pmc()
+ */
 static struct kvm_pmc *amd_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	unsigned int idx, u64 *mask)
 {
 	return amd_pmc_idx_to_pmc(vcpu_to_pmu(vcpu), idx & ~(3u << 30));
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.msr_idx_to_pmc = amd_msr_idx_to_pmc()
+ */
 static struct kvm_pmc *amd_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -105,6 +148,9 @@ static struct kvm_pmc *amd_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 	return pmc;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.is_valid_msr = amd_is_valid_msr()
+ */
 static bool amd_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -128,6 +174,9 @@ static bool amd_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 	return amd_msr_idx_to_pmc(vcpu, msr);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.get_msr = amd_pmu_get_msr()
+ */
 static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -137,12 +186,25 @@ static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	/* MSR_PERFCTRn */
 	pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
 	if (pmc) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/pmu.c|579| <<kvm_pmu_rdpmc>> *data = pmc_read_counter(pmc) & mask;
+		 *   - arch/x86/kvm/pmu.h|85| <<pmc_write_counter>> pmc->counter += val - pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/pmu.h|102| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/svm/pmu.c|140| <<amd_pmu_get_msr>> msr_info->data = pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|370| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|375| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+		 */
 		msr_info->data = pmc_read_counter(pmc);
 		return 0;
 	}
 	/* MSR_EVNTSELn */
 	pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
 	if (pmc) {
+		/*
+		 * struct kvm_pmc *pmc;
+		 * -> u64 eventsel;
+		 */
 		msr_info->data = pmc->eventsel;
 		return 0;
 	}
@@ -150,6 +212,9 @@ static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.set_msr = amd_pmu_set_msr()
+ */
 static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -170,6 +235,13 @@ static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		data &= ~pmu->reserved_bits;
 		if (data != pmc->eventsel) {
 			pmc->eventsel = data;
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/pmu.c|828| <<kvm_pmu_incr_counter>> kvm_pmu_request_counter_reprogram(pmc);
+			 *   - arch/x86/kvm/svm/pmu.c|225| <<amd_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+			 *   - arch/x86/kvm/vmx/pmu_intel.c|88| <<reprogram_fixed_counters>> kvm_pmu_request_counter_reprogram(pmc);
+			 *   - arch/x86/kvm/vmx/pmu_intel.c|456| <<intel_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+			 */
 			kvm_pmu_request_counter_reprogram(pmc);
 		}
 		return 0;
@@ -178,6 +250,9 @@ static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.refresh = amd_pmu_refresh()
+ */
 static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -193,6 +268,15 @@ static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 		BUILD_BUG_ON(x86_feature_cpuid(X86_FEATURE_PERFMON_V2).function != 0x80000022 ||
 			     x86_feature_cpuid(X86_FEATURE_PERFMON_V2).index);
 		ebx.full = kvm_find_cpuid_entry_index(vcpu, 0x80000022, 0)->ebx;
+		/*
+		 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+		 *   - arch/x86/kvm/svm/pmu.c|196| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = ebx.split.num_core_pmc;
+		 *   - arch/x86/kvm/svm/pmu.c|198| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+		 *   - arch/x86/kvm/svm/pmu.c|200| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+		 *   - arch/x86/kvm/svm/pmu.c|203| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(unsigned int , pmu->nr_arch_gp_counters, kvm_pmu_cap.num_counters_gp);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|496| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|529| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+		 */
 		pmu->nr_arch_gp_counters = ebx.split.num_core_pmc;
 	} else if (guest_cpuid_has(vcpu, X86_FEATURE_PERFCTR_CORE)) {
 		pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
@@ -204,7 +288,27 @@ static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 					 kvm_pmu_cap.num_counters_gp);
 
 	if (pmu->version > 1) {
+		/*
+		 * 在以下使用kvm_pmu->global_ctrl_mask:
+		 *   - arch/x86/kvm/pmu.c|698| <<kvm_pmu_set_msr>> data &= ~pmu->global_ctrl_mask;
+		 *   - arch/x86/kvm/pmu.h|138| <<kvm_valid_perf_global_ctrl>> return !(pmu->global_ctrl_mask & data);
+		 *   - arch/x86/kvm/svm/pmu.c|262| <<amd_pmu_refresh>> pmu->global_ctrl_mask = ~((1ull << pmu->nr_arch_gp_counters) - 1);
+		 *   - arch/x86/kvm/svm/pmu.c|263| <<amd_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|503| <<intel_pmu_refresh>> pmu->global_ctrl_mask = ~0ull;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|555| <<intel_pmu_refresh>> pmu->global_ctrl_mask = counter_mask;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|562| <<intel_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask &
+		 *                                                  ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF | MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
+		 */
 		pmu->global_ctrl_mask = ~((1ull << pmu->nr_arch_gp_counters) - 1);
+		/*
+		 * 在以下使用kvm_pmu->global_status_mask:
+		 *   - arch/x86/kvm/pmu.c|692| <<kvm_pmu_set_msr>> if (data & pmu->global_status_mask)
+		 *   - arch/x86/kvm/pmu.c|715| <<kvm_pmu_set_msr>> if (data & pmu->global_status_mask)
+		 *   - arch/x86/kvm/svm/pmu.c|263| <<amd_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|504| <<intel_pmu_refresh>> pmu->global_status_mask = ~0ull;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|562| <<intel_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|566| <<intel_pmu_refresh>> pmu->global_status_mask &= ~MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI;
+		 */
 		pmu->global_status_mask = pmu->global_ctrl_mask;
 	}
 
@@ -217,6 +321,9 @@ static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 	bitmap_set(pmu->all_valid_pmc_idx, 0, pmu->nr_arch_gp_counters);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.init = amd_pmu_init()
+ */
 static void amd_pmu_init(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -226,6 +333,11 @@ static void amd_pmu_init(struct kvm_vcpu *vcpu)
 	BUILD_BUG_ON(KVM_AMD_PMC_MAX_GENERIC > INTEL_PMC_MAX_GENERIC);
 
 	for (i = 0; i < KVM_AMD_PMC_MAX_GENERIC ; i++) {
+		/*
+		 * struct kvm_pmu *pmu:
+		 * -> struct kvm_pmc gp_counters[KVM_INTEL_PMC_MAX_GENERIC];
+		 * -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+		 */
 		pmu->gp_counters[i].type = KVM_PMC_GP;
 		pmu->gp_counters[i].vcpu = vcpu;
 		pmu->gp_counters[i].idx = i;
@@ -233,12 +345,20 @@ static void amd_pmu_init(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.reset = amd_pmu_reset()
+ */
 static void amd_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 	int i;
 
 	for (i = 0; i < KVM_AMD_PMC_MAX_GENERIC; i++) {
+		/*
+		 * struct kvm_pmu *pmu:
+		 * -> struct kvm_pmc gp_counters[KVM_INTEL_PMC_MAX_GENERIC];
+		 * -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+		 */
 		struct kvm_pmc *pmc = &pmu->gp_counters[i];
 
 		pmc_stop_counter(pmc);
@@ -248,6 +368,9 @@ static void amd_pmu_reset(struct kvm_vcpu *vcpu)
 	pmu->global_ctrl = pmu->global_status = 0;
 }
 
+/*
+ * struct kvm_x86_init_ops svm_init_ops.pmu_ops = &amd_pmu_ops
+ */
 struct kvm_pmu_ops amd_pmu_ops __initdata = {
 	.hw_event_available = amd_hw_event_available,
 	.pmc_idx_to_pmc = amd_pmc_idx_to_pmc,
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index e6a6a13b0..d8e68eafb 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -12431,7 +12431,26 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 
 	vcpu->arch.l1tf_flush_l1d = true;
+	/*
+	 * 在以下使用kvm_pmu->event_count:
+	 *   - rch/x86/kvm/pmu.c|287| <<pmc_reprogram_counter>> pmc_to_pmu(pmc)->event_count++;
+	 *   - arch/x86/kvm/pmu.c|848| <<kvm_pmu_init>> pmu->event_count = 0;
+	 *   - arch/x86/kvm/pmu.h|118| <<pmc_release_perf_event>> pmc_to_pmu(pmc)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|250| <<intel_pmu_release_guest_lbr_event>> vcpu_to_pmu(vcpu)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|301| <<intel_pmu_create_guest_lbr_event>> pmu->event_count++;
+	 *   - arch/x86/kvm/x86.c|12434| <<kvm_arch_sched_in>> if (pmu->version && unlikely(pmu->event_count)) {
+	 *
+	 * The total number of programmed perf_events and it helps to avoid
+	 * redundant check before cleanup if guest don't use vPMU at all.
+	 */
 	if (pmu->version && unlikely(pmu->event_count)) {
+		/*
+		 * 在以下使用kvm_pmu->need_cleanup:
+		 *   - arch/x86/kvm/pmu.c|488| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+		 *   - arch/x86/kvm/pmu.c|723| <<kvm_pmu_init>> pmu->need_cleanup = false;
+		 *   - arch/x86/kvm/pmu.c|735| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+		 *   - arch/x86/kvm/x86.c|12399| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+		 */
 		pmu->need_cleanup = true;
 		/*
 		 * 在以下使用KVM_REQ_PMU:
@@ -12441,6 +12460,10 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 		 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
 		 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
 		 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *
+		 * 应该是重新program PMU的事件
+		 *               
+		 * kvm_pmu_handle_event()
 		 */
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
diff --git a/drivers/acpi/bus.c b/drivers/acpi/bus.c
index a4aa53b7e..38785ed62 100644
--- a/drivers/acpi/bus.c
+++ b/drivers/acpi/bus.c
@@ -464,6 +464,10 @@ static void acpi_bus_osc_negotiate_usb_control(void)
  *
  * This only handles notifications related to device hotplug.
  */
+/*
+ * 在以下使用acpi_bus_notify():
+ *   - drivers/acpi/bus.c|1365| <<acpi_bus_init>> status = acpi_install_notify_handler(ACPI_ROOT_OBJECT, ACPI_SYSTEM_NOTIFY, &acpi_bus_notify, NULL);
+ */
 static void acpi_bus_notify(acpi_handle handle, u32 type, void *data)
 {
 	struct acpi_device *adev;
diff --git a/drivers/acpi/osl.c b/drivers/acpi/osl.c
index f725813d0..52a3f43d8 100644
--- a/drivers/acpi/osl.c
+++ b/drivers/acpi/osl.c
@@ -65,6 +65,14 @@ static acpi_osd_handler acpi_irq_handler;
 static void *acpi_irq_context;
 static struct workqueue_struct *kacpid_wq;
 static struct workqueue_struct *kacpi_notify_wq;
+/*
+ * 在以下使用kacpi_hotplug_wq:
+ *   - drivers/acpi/osl.c|1187| <<acpi_hotplug_schedule>> if (!queue_work(kacpi_hotplug_wq, &hpw->work)) {
+ *   - drivers/acpi/osl.c|1196| <<acpi_queue_hotplug_work>> return queue_work(kacpi_hotplug_wq, work);
+ *   - drivers/acpi/osl.c|1674| <<acpi_os_initialize1>> kacpi_hotplug_wq = alloc_ordered_workqueue("kacpi_hotplug", 0);
+ *   - drivers/acpi/osl.c|1677| <<acpi_os_initialize1>> BUG_ON(!kacpi_hotplug_wq);
+ *   - drivers/acpi/osl.c|1702| <<acpi_os_terminate>> destroy_workqueue(kacpi_hotplug_wq);
+ */
 static struct workqueue_struct *kacpi_hotplug_wq;
 static bool acpi_os_initialized;
 unsigned int acpi_sci_irq = INVALID_ACPI_IRQ;
@@ -1154,6 +1162,10 @@ struct acpi_hp_work {
 	u32 src;
 };
 
+/*
+ * 在以下使用acpi_hotplug_work_fn():
+ *   - drivers/acpi/osl.c|1178| <<acpi_hotplug_schedule>> INIT_WORK(&hpw->work, acpi_hotplug_work_fn);
+ */
 static void acpi_hotplug_work_fn(struct work_struct *work)
 {
 	struct acpi_hp_work *hpw = container_of(work, struct acpi_hp_work, work);
@@ -1163,6 +1175,11 @@ static void acpi_hotplug_work_fn(struct work_struct *work)
 	kfree(hpw);
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/bus.c|514| <<acpi_bus_notify>> if (adev && ACPI_SUCCESS(acpi_hotplug_schedule(adev, type)))
+ *   - drivers/acpi/device_sysfs.c|387| <<eject_store>> status = acpi_hotplug_schedule(acpi_device, ACPI_OST_EC_OSPM_EJECT);
+ */
 acpi_status acpi_hotplug_schedule(struct acpi_device *adev, u32 src)
 {
 	struct acpi_hp_work *hpw;
diff --git a/drivers/acpi/scan.c b/drivers/acpi/scan.c
index 691d4b768..1625d10c1 100644
--- a/drivers/acpi/scan.c
+++ b/drivers/acpi/scan.c
@@ -373,6 +373,10 @@ static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/osl.c|1174| <<acpi_hotplug_work_fn>> acpi_device_hotplug(hpw->adev, hpw->src);
+ */
 void acpi_device_hotplug(struct acpi_device *adev, u32 src)
 {
 	u32 ost_code = ACPI_OST_SC_NON_SPECIFIC_FAILURE;
diff --git a/drivers/pci/hotplug/acpiphp_glue.c b/drivers/pci/hotplug/acpiphp_glue.c
index 601129772..6e9266b57 100644
--- a/drivers/pci/hotplug/acpiphp_glue.c
+++ b/drivers/pci/hotplug/acpiphp_glue.c
@@ -479,6 +479,21 @@ static void acpiphp_native_scan_bridge(struct pci_dev *bridge)
  * This function should be called per *physical slot*,
  * not per each slot object in ACPI namespace.
  */
+/*
+ * CPU: 1 PID: 11 Comm: kworker/u16:0 Not tainted 6.6.0-dirty #13
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] pci_assign_unassigned_bridge_resources
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ */
 static void enable_slot(struct acpiphp_slot *slot, bool bridge)
 {
 	struct pci_dev *dev;
@@ -517,6 +532,11 @@ static void enable_slot(struct acpiphp_slot *slot, bool bridge)
 				}
 			}
 		}
+		/*
+		 * 注释
+		 * Returns true if the PCI bus is root (behind host-PCI bridge),
+		 * false otherwise
+		 */
 		if (pci_is_root_bus(bus))
 			__pci_bus_assign_resources(bus, &add_list, NULL);
 		else
@@ -697,6 +717,13 @@ static void trim_stale_devices(struct pci_dev *dev)
  * Iterate over all slots under this bridge and make sure that if a
  * card is present they are enabled, and if not they are disabled.
  */
+/*
+ * called by:
+ *   - drivers/pci/hotplug/acpiphp_glue.c|777| <<acpiphp_check_host_bridge>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|807| <<hotplug_event>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|817| <<hotplug_event>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|824| <<hotplug_event>> acpiphp_check_bridge(func->parent);
+ */
 static void acpiphp_check_bridge(struct acpiphp_bridge *bridge)
 {
 	struct acpiphp_slot *slot;
@@ -708,6 +735,10 @@ static void acpiphp_check_bridge(struct acpiphp_bridge *bridge)
 	if (bridge->pci_dev)
 		pm_runtime_get_sync(&bridge->pci_dev->dev);
 
+	/*
+	 * struct acpiphp_bridge *bridge:
+	 * -> struct list_head slots;
+	 */
 	list_for_each_entry(slot, &bridge->slots, node) {
 		struct pci_bus *bus = slot->bus;
 		struct pci_dev *dev, *tmp;
@@ -837,6 +868,10 @@ static void hotplug_event(u32 type, struct acpiphp_context *context)
 		put_bridge(bridge);
 }
 
+/*
+ * 在以下使用acpiphp_hotplug_notify():
+ *   - drivers/pci/hotplug/acpiphp_glue.c|68| <<acpiphp_init_context>> context->hp.notify = acpiphp_hotplug_notify;
+ */
 static int acpiphp_hotplug_notify(struct acpi_device *adev, u32 type)
 {
 	struct acpiphp_context *context;
diff --git a/drivers/pci/probe.c b/drivers/pci/probe.c
index 795534589..404406f1e 100644
--- a/drivers/pci/probe.c
+++ b/drivers/pci/probe.c
@@ -510,6 +510,20 @@ static void pci_read_bridge_mmio_pref(struct pci_bus *child)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/pci.c|260| <<pcibios_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - arch/ia64/pci/pci.c|369| <<pcibios_fixup_bus>> pci_read_bridge_bases(b);
+ *   - arch/mips/pci/pci-generic.c|47| <<pcibios_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - arch/mips/pci/pci-legacy.c|295| <<pcibios_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - arch/powerpc/kernel/pci-common.c|1107| <<pcibios_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - arch/sparc/kernel/pci.c|479| <<of_scan_pci_bridge>> pci_read_bridge_bases(bus);
+ *   - arch/x86/pci/common.c|168| <<pcibios_fixup_bus>> pci_read_bridge_bases(b);
+ *   - arch/xtensa/kernel/pci.c|65| <<pcibios_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - drivers/parisc/dino.c|613| <<dino_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - drivers/parisc/lba_pci.c|730| <<lba_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - drivers/pci/setup-bus.c|1478| <<pci_bus_allocate_resources>> pci_read_bridge_bases(b);
+ */
 void pci_read_bridge_bases(struct pci_bus *child)
 {
 	struct pci_dev *dev = child->self;
diff --git a/drivers/pci/setup-bus.c b/drivers/pci/setup-bus.c
index dae490f25..bad2503c1 100644
--- a/drivers/pci/setup-bus.c
+++ b/drivers/pci/setup-bus.c
@@ -659,6 +659,26 @@ static void pci_setup_bridge_mmio_pref(struct pci_dev *bridge)
 	pci_write_config_dword(bridge, PCI_PREF_LIMIT_UPPER32, lu);
 }
 
+/*
+ * [0] CPU: 3 PID: 11 Comm: kworker/u16:0 Not tainted 6.6.0-dirty #4
+ * [0] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] pci_setup_bridge
+ * [0] pci_assign_unassigned_bridge_resources
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - drivers/pci/setup-bus.c|691| <<pci_setup_bridge>> __pci_setup_bridge(bus, type);
+ *   - drivers/pci/setup-bus.c|1569| <<pci_bridge_release_resources>> __pci_setup_bridge(bus, type);
+ */
 static void __pci_setup_bridge(struct pci_bus *bus, unsigned long type)
 {
 	struct pci_dev *bridge = bus->self;
@@ -682,6 +702,13 @@ void __weak pcibios_setup_bridge(struct pci_bus *bus, unsigned long type)
 {
 }
 
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|1393| <<__pci_bus_assign_resources>> pci_setup_bridge(b);
+ *   - drivers/pci/setup-bus.c|1498| <<__pci_bridge_assign_resources>> pci_setup_bridge(b);
+ *   - drivers/pci/setup-bus.c|2285| <<pci_reassign_bridge_resources>> pci_setup_bridge(bridge->subordinate);
+ *   - drivers/pci/setup-bus.c|2315| <<pci_reassign_bridge_resources>> pci_setup_bridge(bridge->subordinate);
+ */
 void pci_setup_bridge(struct pci_bus *bus)
 {
 	unsigned long type = IORESOURCE_IO | IORESOURCE_MEM |
@@ -1369,6 +1396,15 @@ static void pdev_assign_fixed_resources(struct pci_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/acpiphp_glue.c|521| <<enable_slot>> __pci_bus_assign_resources(bus, &add_list, NULL);
+ *   - drivers/pci/setup-bus.c|1415| <<__pci_bus_assign_resources>> __pci_bus_assign_resources(b, realloc_head, fail_head);
+ *   - drivers/pci/setup-bus.c|1437| <<pci_bus_assign_resources>> __pci_bus_assign_resources(bus, NULL, NULL);
+ *   - drivers/pci/setup-bus.c|1526| <<__pci_bridge_assign_resources>> __pci_bus_assign_resources(b, add_head, fail_head);
+ *   - drivers/pci/setup-bus.c|2104| <<pci_assign_unassigned_root_bus_resources>> __pci_bus_assign_resources(bus, add_list, &fail_head);
+ *   - drivers/pci/setup-bus.c|2375| <<pci_assign_unassigned_bus_resources>> __pci_bus_assign_resources(bus, &add_list, NULL);
+ */
 void __pci_bus_assign_resources(const struct pci_bus *bus,
 				struct list_head *realloc_head,
 				struct list_head *fail_head)
@@ -1453,6 +1489,11 @@ static void pci_bus_allocate_dev_resources(struct pci_bus *b)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|1483| <<pci_bus_allocate_resources>> pci_bus_allocate_resources(child);
+ *   - drivers/pci/setup-bus.c|1488| <<pci_bus_claim_resources>> pci_bus_allocate_resources(b);
+ */
 static void pci_bus_allocate_resources(struct pci_bus *b)
 {
 	struct pci_bus *child;
@@ -1478,6 +1519,11 @@ void pci_bus_claim_resources(struct pci_bus *b)
 }
 EXPORT_SYMBOL(pci_bus_claim_resources);
 
+/*
+ * called by;
+ *   - drivers/pci/setup-bus.c|2230| <<pci_assign_unassigned_bridge_resources>> __pci_bridge_assign_resources(bridge, &add_list, &fail_head);
+ *   - drivers/pci/setup-bus.c|2335| <<pci_reassign_bridge_resources>> __pci_bridge_assign_resources(bridge, &added, &failed);
+ */
 static void __pci_bridge_assign_resources(const struct pci_dev *bridge,
 					  struct list_head *add_head,
 					  struct list_head *fail_head)
@@ -2142,6 +2188,29 @@ void __init pci_assign_unassigned_resources(void)
 	}
 }
 
+/*
+ * CPU: 1 PID: 11 Comm: kworker/u16:0 Not tainted 6.6.0-dirty #13
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] pci_assign_unassigned_bridge_resources
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - arch/powerpc/kernel/pci-common.c|1495| <<pcibios_finish_adding_to_bus>> pci_assign_unassigned_bridge_resources(bus->self);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|523| <<enable_slot>> pci_assign_unassigned_bridge_resources(bus->self);
+ *   - drivers/pci/hotplug/cpci_hotplug_pci.c|276| <<cpci_configure_slot>> pci_assign_unassigned_bridge_resources(parent->self);
+ *   - drivers/pci/hotplug/pciehp_pci.c|64| <<pciehp_configure_device>> pci_assign_unassigned_bridge_resources(bridge);
+ *   - drivers/pci/hotplug/shpchp_pci.c|55| <<shpchp_configure_device>> pci_assign_unassigned_bridge_resources(bridge);
+ *   - drivers/pci/probe.c|3275| <<pci_rescan_bus_bridge_resize>> pci_assign_unassigned_bridge_resources(bridge);
+ */
 void pci_assign_unassigned_bridge_resources(struct pci_dev *bridge)
 {
 	struct pci_bus *parent = bridge->subordinate;
diff --git a/drivers/scsi/scsi_scan.c b/drivers/scsi/scsi_scan.c
index 44680f65e..5229655d5 100644
--- a/drivers/scsi/scsi_scan.c
+++ b/drivers/scsi/scsi_scan.c
@@ -1951,6 +1951,10 @@ static void do_scsi_scan_host(struct Scsi_Host *shost)
 	}
 }
 
+/*
+ * 在以下使用do_scan_async:
+ *   - drivers/scsi/scsi_scan.c|1987| <<scsi_scan_host>> async_schedule(do_scan_async, data);
+ */
 static void do_scan_async(void *_data, async_cookie_t c)
 {
 	struct async_scan_data *data = _data;
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 9d1bdcdc1..0caa7ac73 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -33,6 +33,74 @@
 
 #include "sd.h"
 
+/*
+ * CPU: 1 PID: 11 Comm: kworker/u16:0 Not tainted 6.6.0-dirty #13
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] pci_assign_unassigned_bridge_resources
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] blk_execute_rq
+ * [0] scsi_execute_cmd
+ * [0] scsi_probe_lun
+ * [0] scsi_probe_and_add_lun
+ * [0] __scsi_scan_target
+ * [0] scsi_scan_channel
+ * [0] scsi_scan_host_selected
+ * [0] do_scsi_scan_host
+ * [0] do_scan_async
+ * [0] async_run_entry_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] CPU: 2 PID: 10 Comm: kworker/u16:0 Not tainted 6.6.0-dirty #10
+ * [0] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] do_scsi_scan_host
+ * [0] scsi_scan_host
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __device_attach_driver
+ * [0] bus_for_each_drv
+ * [0] __device_attach
+ * [0] bus_probe_device
+ * [0] device_add
+ * [0] register_virtio_device
+ * [0] virtio_pci_probe
+ * [0] local_pci_probe
+ * [0] pci_device_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __device_attach_driver
+ * [0] bus_for_each_drv
+ * [0] __device_attach
+ * [0] pci_bus_add_device
+ * [0] pci_bus_add_devices
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ */
+
 #define VIRTIO_SCSI_MEMPOOL_SZ 64
 #define VIRTIO_SCSI_EVENT_LEN 8
 #define VIRTIO_SCSI_VQ_BASE 2
@@ -167,6 +235,13 @@ static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 	scsi_done(sc);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|198| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|207| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i], virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|224| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+ *   - drivers/scsi/virtio_scsi.c|422| <<virtscsi_event_done>> virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
+ */
 static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 			     struct virtio_scsi_vq *virtscsi_vq,
 			     void (*fn)(struct virtio_scsi *vscsi, void *buf))
@@ -179,6 +254,9 @@ static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 	spin_lock_irqsave(&virtscsi_vq->vq_lock, flags);
 	do {
 		virtqueue_disable_cb(vq);
+		/*
+		 * 比如: virtscsi_complete_cmd()
+		 */
 		while ((buf = virtqueue_get_buf(vq, &len)) != NULL)
 			fn(vscsi, buf);
 
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index abef0619c..1d2ef5815 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -463,6 +463,17 @@ vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 
 again:
 	vhost_disable_notify(&vs->dev, vq);
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|581| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|593| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1069| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|466| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/scsi.c|940| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|126| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|495| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 */
 	head = vhost_get_vq_desc(vq, vq->iov,
 			ARRAY_SIZE(vq->iov), &out, &in,
 			NULL, NULL);
@@ -591,6 +602,10 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 		vhost_signal(&svq->vs->dev, &svq->vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1195| <<vhost_scsi_handle_vq>> cmd = vhost_scsi_get_cmd(vq, tpg, cdb, tag, lun, task_attr, exp_data_len + prot_bytes, data_direction);
+ */
 static struct vhost_scsi_cmd *
 vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		   unsigned char *cdb, u64 scsi_tag, u16 lun, u8 task_attr,
@@ -931,12 +946,28 @@ vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_cmd_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1069| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ *   - drivers/vhost/scsi.c|1394| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ */
 static int
 vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		    struct vhost_scsi_ctx *vc)
 {
 	int ret = -ENXIO;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|581| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|593| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1069| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|466| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/scsi.c|940| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|126| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|495| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 */
 	vc->head = vhost_get_vq_desc(vq, vq->iov,
 				     ARRAY_SIZE(vq->iov), &vc->out, &vc->in,
 				     NULL, NULL);
@@ -1034,6 +1065,10 @@ static u16 vhost_buf_to_lun(u8 *lun_buf)
 	return ((lun_buf[2] << 8) | lun_buf[3]) & 0x3FFF;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1538| <<vhost_scsi_handle_kick>> vhost_scsi_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -1529,6 +1564,10 @@ static void vhost_scsi_evt_handle_kick(struct vhost_work *work)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用vhost_scsi_handle_kick():
+ *   - drivers/vhost/scsi.c|1972| <<vhost_scsi_open>> svq->vq.handle_kick = vhost_scsi_handle_kick;
+ */
 static void vhost_scsi_handle_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index e0c181ad1..dbfa1ebdd 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -46,6 +46,46 @@ enum {
 	VHOST_MEMORY_F_LOG = 0x1,
 };
 
+/*
+ * 在virtio端的定义:
+ *
+ * #define vring_used_event(vr) ((vr)->avail->ring[(vr)->num])
+ * #define vring_avail_event(vr) (*(__virtio16 *)&(vr)->used->ring[(vr)->num])
+ *
+ * 在vhost端的定义:
+ *
+ * #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
+ * #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
+ *
+ * -----------------------------
+ *
+ * #define vring_used_event(vr) ((vr)->avail->ring[(vr)->num])
+ * #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
+ *
+ * 在VM side存放当前的vq->last_used_idx (也就是下一个要用到的vq->last_used_idx)
+ *
+ *
+ * #define vring_avail_event(vr) (*(__virtio16 *)&(vr)->used->ring[(vr)->num])
+ * #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
+ *
+ * 在Hypervisor side存放当前的vq->avail_idx.
+ *
+ * vhost_scsi_handle_vq()
+ * -> vhost_scsi_get_desc()
+ *    -> vhost_enable_notify()
+ *       -> vhost_update_avail_event()
+ *          -> vhost_avail_event()
+ *
+ * 对vhost-scsi仍然有疑虑.
+ */
+
+/*
+ * vhost_scsi_handle_vq()
+ * -> vhost_scsi_get_desc()
+ *    -> vhost_enable_notify()
+ *       -> vhost_update_avail_event()
+ *          -> vhost_avail_event()
+ */
 #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
 #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
 
@@ -1224,6 +1264,10 @@ static inline void __user *__vhost_get_user(struct vhost_virtqueue *vq,
 	ret; \
 })
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2281| <<vhost_update_avail_event>> if (vhost_put_avail_event(vq))
+ */
 static inline int vhost_put_avail_event(struct vhost_virtqueue *vq)
 {
 	return vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
@@ -2276,6 +2320,16 @@ static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * vhost_scsi_handle_vq()
+ * -> vhost_scsi_get_desc()
+ *    -> vhost_enable_notify()
+ *       -> vhost_update_avail_event()
+ *          -> vhost_avail_event()
+ *
+ * called by:
+ *   - drivers/vhost/vhost.c|2836| <<vhost_enable_notify>> r = vhost_update_avail_event(vq);
+ */
 static int vhost_update_avail_event(struct vhost_virtqueue *vq)
 {
 	if (vhost_put_avail_event(vq))
@@ -2491,6 +2545,17 @@ static int get_indirect(struct vhost_virtqueue *vq,
  * This function returns the descriptor number found, or vq->num (which is
  * never a valid descriptor number) if none was found.  A negative code is
  * returned on error. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|581| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|593| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|1069| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+ *   - drivers/vhost/scsi.c|466| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/scsi.c|940| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+ *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|126| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|495| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ */
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
@@ -2805,6 +2870,20 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_avail_empty);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|515| <<vhost_net_busy_poll_try_queue>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|567| <<vhost_net_busy_poll>> vhost_enable_notify(&net->dev, rvq);
+ *   - drivers/vhost/net.c|802| <<handle_tx_copy>> } else if (unlikely(vhost_enable_notify(&net->dev,
+ *   - drivers/vhost/net.c|896| <<handle_tx_zerocopy>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|1178| <<handle_rx>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/scsi.c|485| <<vhost_scsi_do_evt_work>> if (vhost_enable_notify(&vs->dev, vq))
+ *   - drivers/vhost/scsi.c|984| <<vhost_scsi_get_desc>> if (unlikely(vhost_enable_notify(&vs->dev, vq))) {
+ *   - drivers/vhost/test.c|70| <<handle_vq>> if (unlikely(vhost_enable_notify(&n->dev, vq))) {
+ *   - drivers/vhost/vsock.c|122| <<vhost_transport_do_send_pkt>> vhost_enable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|138| <<vhost_transport_do_send_pkt>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ *   - drivers/vhost/vsock.c|501| <<vhost_vsock_handle_tx_kick>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ */
 /* OK, now we need to know about added descriptors. */
 bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index f60d5f7be..6a31c8b24 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -100,19 +100,75 @@ struct vhost_virtqueue {
 	/* Last available index we saw.
 	 * Values are limited to 0x7fff, and the high bit is used as
 	 * a wrap counter when using VIRTIO_F_RING_PACKED. */
+	/*
+	 * vhost.c在以下使用vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|370| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1911| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num & 0xffff;
+	 *   - drivers/vhost/vhost.c|1918| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|1921| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|1926| <<vhost_vring_ioctl>> s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
+	 *   - drivers/vhost/vhost.c|1928| <<vhost_vring_ioctl>> s.num = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2507| <<vhost_get_vq_desc>> last_avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2509| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2526| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2626| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2638| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 *   - drivers/vhost/vhost.c|2735| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2796| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2804| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2843| <<vhost_enable_notify>> return vq->avail_idx != vq->last_avail_idx;
+	 */
 	u16 last_avail_idx;
 
 	/* Caches available index value from user. */
+	/*
+	 * vhost.c在以下使用vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|371| <<vhost_vq_reset>> vq->avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1229| <<vhost_put_avail_event>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
+	 *   - drivers/vhost/vhost.c|1921| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2509| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2515| <<vhost_get_vq_desc>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2517| <<vhost_get_vq_desc>> if (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {
+	 *   - drivers/vhost/vhost.c|2519| <<vhost_get_vq_desc>> last_avail_idx, vq->avail_idx);
+	 *   - drivers/vhost/vhost.c|2526| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2735| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2796| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2802| <<vhost_vq_avail_empty>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2804| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2843| <<vhost_enable_notify>> return vq->avail_idx != vq->last_avail_idx;
+	 */
 	u16 avail_idx;
 
 	/* Last index we used.
 	 * Values are limited to 0x7fff, and the high bit is used as
 	 * a wrap counter when using VIRTIO_F_RING_PACKED. */
+	/*
+	 * vhost.c在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|372| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1251| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|1912| <<vhost_vring_ioctl>> vq->last_used_idx = (s.num >> 16) & 0xffff;
+	 *   - drivers/vhost/vhost.c|1926| <<vhost_vring_ioctl>> s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
+	 *   - drivers/vhost/vhost.c|2299| <<vhost_vq_init_access>> __virtio16 last_used_idx;
+	 *   - drivers/vhost/vhost.c|2317| <<vhost_vq_init_access>> r = vhost_get_used_idx(vq, &last_used_idx);
+	 *   - drivers/vhost/vhost.c|2323| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2663| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2676| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2677| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|2694| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2748| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 last_used_idx;
 
 	/* Used flags */
 	u16 used_flags;
 
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|373| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2682| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2746| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2748| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	/* Last used index value we have signalled on */
 	u16 signalled_used;
 
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index dd17da094..9778d3544 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -97,6 +97,19 @@ struct vring_virtqueue_split {
 	 * Last written value to avail->idx in
 	 * guest byte order.
 	 */
+	/*
+	 * 在以下使用vring_virtqueue_split->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|687| <<virtqueue_add_split>> avail = vq->split.avail_idx_shadow & (vq->split.vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|693| <<virtqueue_add_split>> vq->split.avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|695| <<virtqueue_add_split>> vq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->split.avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|745| <<virtqueue_kick_prepare_split>> old = vq->split.avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|746| <<virtqueue_kick_prepare_split>> new = vq->split.avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|958| <<virtqueue_enable_cb_delayed_split>> bufs = (u16)(vq->split.avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|988| <<virtqueue_detach_unused_buf_split>> vq->split.avail_idx_shadow--;
+	 *   - drivers/virtio/virtio_ring.c|990| <<virtqueue_detach_unused_buf_split>> vq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->split.avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1009| <<virtqueue_vring_init_split>> vring_split->avail_idx_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|2972| <<vring_notification_data>> next = vq->split.avail_idx_shadow;
+	 */
 	u16 avail_idx_shadow;
 
 	/* Per-descriptor state. */
@@ -682,6 +695,19 @@ static inline int virtqueue_add_split(struct virtqueue *_vq,
 	else
 		vq->split.desc_state[head].indir_desc = ctx;
 
+	/*
+	 * 在以下使用vring_virtqueue_split->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|687| <<virtqueue_add_split>> avail = vq->split.avail_idx_shadow & (vq->split.vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|693| <<virtqueue_add_split>> vq->split.avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|695| <<virtqueue_add_split>> vq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->split.avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|745| <<virtqueue_kick_prepare_split>> old = vq->split.avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|746| <<virtqueue_kick_prepare_split>> new = vq->split.avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|958| <<virtqueue_enable_cb_delayed_split>> bufs = (u16)(vq->split.avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|988| <<virtqueue_detach_unused_buf_split>> vq->split.avail_idx_shadow--;
+	 *   - drivers/virtio/virtio_ring.c|990| <<virtqueue_detach_unused_buf_split>> vq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->split.avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1009| <<virtqueue_vring_init_split>> vring_split->avail_idx_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|2972| <<vring_notification_data>> next = vq->split.avail_idx_shadow;
+	 */
 	/* Put entry in available array (but don't update avail->idx until they
 	 * do sync). */
 	avail = vq->split.avail_idx_shadow & (vq->split.vring.num - 1);
@@ -762,6 +788,11 @@ static bool virtqueue_kick_prepare_split(struct virtqueue *_vq)
 	return needs_kick;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|895| <<virtqueue_get_buf_ctx_split>> detach_buf_split(vq, i, ctx);
+ *   - drivers/virtio/virtio_ring.c|1017| <<virtqueue_detach_unused_buf_split>> detach_buf_split(vq, i, NULL);
+ */
 static void detach_buf_split(struct vring_virtqueue *vq, unsigned int head,
 			     void **ctx)
 {
@@ -820,6 +851,10 @@ static bool more_used_split(const struct vring_virtqueue *vq)
 			vq->split.vring.used->idx);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2421| <<virtqueue_get_buf_ctx>> virtqueue_get_buf_ctx_split(_vq, len, ctx);
+ */
 static void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,
 					 unsigned int *len,
 					 void **ctx)
@@ -862,6 +897,9 @@ static void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,
 
 	/* detach_buf_split clears data, so grab it now. */
 	ret = vq->split.desc_state[i].data;
+	/*
+	 * 这里是核心!!!!
+	 */
 	detach_buf_split(vq, i, ctx);
 	vq->last_used_idx++;
 	/* If we expect an interrupt for the next entry, tell host
@@ -2412,6 +2450,11 @@ EXPORT_SYMBOL_GPL(virtqueue_kick);
  * Returns NULL if there are no used buffers, or the "data" token
  * handed to virtqueue_add_*().
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|638| <<virtnet_rq_get_buf>> buf = virtqueue_get_buf_ctx(rq->vq, len, ctx);
+ *   - drivers/virtio/virtio_ring.c|2427| <<virtqueue_get_buf>> return virtqueue_get_buf_ctx(_vq, len, NULL);
+ */
 void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 			    void **ctx)
 {
@@ -2422,6 +2465,18 @@ void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 }
 EXPORT_SYMBOL_GPL(virtqueue_get_buf_ctx);
 
+/*
+ * 太多了, 选择一些例子:
+ *   - drivers/block/virtio_blk.c|398| <<virtblk_done>> while ((vbr = virtqueue_get_buf(vblk->vqs[qid].vq, &len)) != NULL) {
+ *   - drivers/block/virtio_blk.c|1309| <<virtblk_poll>> while ((vbr = virtqueue_get_buf(vq->vq, &len)) != NULL) {
+ *   - drivers/net/virtio_net.c|762| <<free_old_xmit_skbs>> while ((ptr = virtqueue_get_buf(sq->vq, &len)) != NULL) {
+ *   - drivers/net/virtio_net.c|953| <<virtnet_xdp_xmit>> while ((ptr = virtqueue_get_buf(sq->vq, &len)) != NULL) {
+ *   - drivers/net/virtio_net.c|2541| <<virtnet_send_command>> while (!virtqueue_get_buf(vi->cvq, &tmp) &&
+ *   - drivers/scsi/virtio_scsi.c|182| <<virtscsi_vq_done>> while ((buf = virtqueue_get_buf(vq, &len)) != NULL)
+ *   - drivers/virtio/virtio_mem.c|1389| <<virtio_mem_send_request>> wait_event(vm->host_resp, virtqueue_get_buf(vm->vq, &len));
+ *   - fs/fuse/virtio_fs.c|344| <<virtio_fs_hiprio_done_work>> while ((req = virtqueue_get_buf(vq, &len)) != NULL) {
+ *   - fs/fuse/virtio_fs.c|625| <<virtio_fs_requests_done_work>> while ((req = virtqueue_get_buf(vq, &len)) != NULL) {
+ */
 void *virtqueue_get_buf(struct virtqueue *_vq, unsigned int *len)
 {
 	return virtqueue_get_buf_ctx(_vq, len, NULL);
diff --git a/kernel/events/core.c b/kernel/events/core.c
index eecbbd80f..c4967242d 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -5278,6 +5278,19 @@ static void put_event(struct perf_event *event)
  * object, it will not preserve its functionality. Once the last 'user'
  * gives up the object, we'll destroy the thing.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|190| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/riscv/kvm/vcpu_pmu.c|81| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1057| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1059| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+ *   - arch/x86/kvm/pmu.h|115| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|248| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+ *   - kernel/events/core.c|5395| <<perf_release>> perf_event_release_kernel(file->private_data);
+ *   - kernel/events/hw_breakpoint.c|829| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+ *   - kernel/watchdog_perf.c|330| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+ *   - kernel/watchdog_perf.c|406| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+ */
 int perf_event_release_kernel(struct perf_event *event)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -5396,6 +5409,11 @@ static int perf_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5434| <<perf_event_read_value>> count = __perf_event_read_value(event, enabled, running);
+ *   - kernel/events/core.c|5569| <<perf_read_one>> values[n++] = __perf_event_read_value(event, &enabled, &running);
+ */
 static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)
 {
 	struct perf_event *child;
@@ -5425,6 +5443,15 @@ static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *
 	return total;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|110| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/riscv/kvm/vcpu_pmu.c|213| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/riscv/kvm/vcpu_pmu.c|437| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/x86/kvm/pmu.h|71| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+ *   - tools/include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+ */
 u64 perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)
 {
 	struct perf_event_context *ctx;
@@ -5559,6 +5586,10 @@ static int perf_read_group(struct perf_event *event,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5622| <<__perf_read>> ret = perf_read_one(event, read_format, buf);
+ */
 static int perf_read_one(struct perf_event *event,
 				 u64 read_format, char __user *buf)
 {
@@ -5598,6 +5629,10 @@ static bool is_event_hup(struct perf_event *event)
 /*
  * Read the performance event - simple non blocking version for now
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|5639| <<perf_read>> ret = __perf_read(event, buf, count);
+ */
 static ssize_t
 __perf_read(struct perf_event *event, char __user *buf, size_t count)
 {
@@ -5624,6 +5659,9 @@ __perf_read(struct perf_event *event, char __user *buf, size_t count)
 	return ret;
 }
 
+/*
+ * kernel/events/core.c|6647| <<global>> struct file_operations perf_fops.read = perf_read()
+ */
 static ssize_t
 perf_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)
 {
@@ -5672,6 +5710,10 @@ static void _perf_event_reset(struct perf_event *event)
 	perf_event_update_userpage(event);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|272| <<pmc_pause_counter>> counter += perf_event_pause(pmc->perf_event, true);
+ */
 /* Assume it's not an event with inherit set. */
 u64 perf_event_pause(struct perf_event *event, bool reset)
 {
@@ -5789,6 +5831,12 @@ static int _perf_event_period(struct perf_event *event, u64 value)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/riscv/kvm/vcpu_pmu.c|380| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_period(pmc->perf_event, kvm_pmu_get_sample_period(pmc));
+ *   - arch/x86/kvm/pmu.c|317| <<pmc_resume_counter>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+ *   - arch/x86/kvm/pmu.h|200| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter));
+ */
 int perf_event_period(struct perf_event *event, u64 value)
 {
 	struct perf_event_context *ctx;
@@ -6619,6 +6667,13 @@ static int perf_fasync(int fd, struct file *filp, int on)
 	return 0;
 }
 
+/*
+ * 在以下使用perf_fops:
+ *   - kernel/events/core.c|5835| <<perf_fget_light>> if (f.file->f_op != &perf_fops) {
+ *   - kernel/events/core.c|12661| <<SYSCALL_DEFINE5(perf_event_open)>> event_file = anon_inode_getfile("[perf_event]", &perf_fops, event, f_flags);
+ *   - kernel/events/core.c|13237| <<perf_event_get>> if (file->f_op != &perf_fops) {
+ *   - kernel/events/core.c|13247| <<perf_get_event>> if (file->f_op != &perf_fops)
+ */
 static const struct file_operations perf_fops = {
 	.llseek			= no_llseek,
 	.release		= perf_release,
diff --git a/kernel/watchdog_perf.c b/kernel/watchdog_perf.c
index 49722118a..42f84e632 100644
--- a/kernel/watchdog_perf.c
+++ b/kernel/watchdog_perf.c
@@ -144,6 +144,18 @@ static struct perf_event_attr wd_hw_attr = {
 };
 
 /*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ *
  * called by:
  *   - kernel/watchdog_perf.c|124| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
  */
-- 
2.34.1

