From 2975d7b4befd2f020fc00a3e16cfe58f4b580618 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Fri, 17 Nov 2023 11:15:10 -0800
Subject: [PATCH 1/1] linux-v6.6

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/arch_timer.h           |   3 +
 arch/arm64/include/asm/kvm_emulate.h          |   6 +
 arch/arm64/include/asm/kvm_host.h             |  20 ++
 arch/arm64/include/asm/kvm_mmu.h              |  33 ++
 arch/arm64/kernel/process.c                   |  12 +
 arch/arm64/kvm/arch_timer.c                   | 289 ++++++++++++++++++
 arch/arm64/kvm/arm.c                          | 132 ++++++++
 arch/arm64/kvm/guest.c                        |   4 +
 arch/arm64/kvm/handle_exit.c                  |  29 ++
 arch/arm64/kvm/hyp/include/hyp/switch.h       |  44 +++
 arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h    |  10 +
 arch/arm64/kvm/hyp/include/nvhe/mem_protect.h |   7 +
 arch/arm64/kvm/hyp/nvhe/timer-sr.c            |  18 ++
 arch/arm64/kvm/hyp/vgic-v3-sr.c               |  10 +
 arch/arm64/kvm/hyp/vhe/switch.c               |  61 ++++
 arch/arm64/kvm/hyp/vhe/sysreg-sr.c            |  32 ++
 arch/arm64/kvm/hyp/vhe/tlb.c                  |  13 +
 arch/arm64/kvm/mmio.c                         |   9 +
 arch/arm64/kvm/mmu.c                          |  22 ++
 arch/arm64/kvm/sys_regs.c                     |  22 ++
 arch/arm64/kvm/vgic/vgic-irqfd.c              |  27 ++
 arch/arm64/kvm/vgic/vgic-its.c                | 125 ++++++++
 arch/arm64/kvm/vgic/vgic-mmio-v3.c            |   4 +
 arch/arm64/kvm/vgic/vgic-v3.c                 |   8 +
 arch/arm64/kvm/vgic/vgic-v4.c                 |   4 +
 arch/arm64/kvm/vgic/vgic.c                    | 132 ++++++++
 arch/x86/events/core.c                        |   5 +
 arch/x86/include/asm/kvm_host.h               | 130 ++++++++
 arch/x86/include/asm/nmi.h                    |  22 ++
 arch/x86/kernel/apic/hw_nmi.c                 |  27 ++
 arch/x86/kvm/cpuid.c                          |   4 +
 arch/x86/kvm/irq.c                            |   4 +
 arch/x86/kvm/irq_comm.c                       |   4 +
 arch/x86/kvm/lapic.c                          |  52 ++++
 arch/x86/kvm/lapic.h                          |   4 +
 arch/x86/kvm/pmu.c                            | 128 ++++++++
 arch/x86/kvm/pmu.h                            |   6 +
 arch/x86/kvm/svm/avic.c                       |   4 +
 arch/x86/kvm/svm/nested.c                     |   5 +
 arch/x86/kvm/svm/svm.c                        |  12 +
 arch/x86/kvm/x86.c                            | 147 +++++++++
 arch/x86/kvm/x86.h                            |   7 +
 arch/x86/kvm/xen.c                            |  82 +++++
 drivers/block/virtio_blk.c                    |  35 +++
 drivers/clocksource/arm_arch_timer.c          |  45 +++
 drivers/irqchip/irq-gic-v3.c                  |  16 +
 drivers/irqchip/irq-gic-v4.c                  |   5 +
 drivers/net/virtio_net.c                      |  53 ++++
 drivers/vhost/net.c                           |   4 +
 drivers/virtio/virtio_ring.c                  |  35 +++
 include/kvm/arm_vgic.h                        |  59 ++++
 include/linux/kvm_host.h                      |   6 +
 include/linux/kvm_irqfd.h                     |   7 +
 kernel/entry/kvm.c                            |   6 +
 kernel/events/core.c                          |  13 +
 kernel/panic.c                                |  13 +
 kernel/stop_machine.c                         |  12 +
 kernel/watchdog.c                             | 176 ++++++++++-
 kernel/watchdog_perf.c                        | 137 +++++++++
 .../kvm/memslot_modification_stress_test.c    |   4 +
 virt/kvm/eventfd.c                            |  32 ++
 virt/kvm/irqchip.c                            |   3 +
 virt/kvm/kvm_main.c                           |  31 ++
 63 files changed, 2410 insertions(+), 1 deletion(-)

diff --git a/arch/arm64/include/asm/arch_timer.h b/arch/arm64/include/asm/arch_timer.h
index 934c658ee..090dcc5ec 100644
--- a/arch/arm64/include/asm/arch_timer.h
+++ b/arch/arm64/include/asm/arch_timer.h
@@ -201,6 +201,9 @@ static __always_inline u64 __arch_counter_get_cntvct(void)
 {
 	u64 cnt;
 
+	/*
+	 * 虚拟化应该是PhysicalCountInt() - CNTVOFF_EL2
+	 */
 	asm volatile(ALTERNATIVE("isb\n mrs %0, cntvct_el0",
 				 "nop\n" __mrs_s("%0", SYS_CNTVCTSS_EL0),
 				 ARM64_HAS_ECV)
diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h
index 3d6725ff0..5a2f7d4e9 100644
--- a/arch/arm64/include/asm/kvm_emulate.h
+++ b/arch/arm64/include/asm/kvm_emulate.h
@@ -294,6 +294,12 @@ static inline bool vcpu_mode_priv(const struct kvm_vcpu *vcpu)
 
 static __always_inline u64 kvm_vcpu_get_esr(const struct kvm_vcpu *vcpu)
 {
+	/*
+	 * ESR_EL2, Exception Syndrome Register (EL2)
+	 * EC, bits [31:26]
+	 *   Exception Class. Indicates the reason for the exception
+	 *   that this register holds information about.
+	 */
 	return vcpu->arch.fault.esr_el2;
 }
 
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index af06ccb7e..868271186 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -43,6 +43,17 @@
 
 #define KVM_REQ_SLEEP \
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_IRQ_PENDING:
+ *   - arch/arm64/kvm/arm.c|806| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/arm.c|1169| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|509| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|388| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|437| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|755| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|1112| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ */
 #define KVM_REQ_IRQ_PENDING	KVM_ARCH_REQ(1)
 #define KVM_REQ_VCPU_RESET	KVM_ARCH_REQ(2)
 #define KVM_REQ_RECORD_STEAL	KVM_ARCH_REQ(3)
@@ -155,6 +166,15 @@ struct kvm_s2_mmu {
 	 * for vEL1/EL0 with vHCR_EL2.VM == 0.  In that case, we use the
 	 * canonical stage-2 page tables.
 	 */
+	/*
+	 * 在以下使用kvm_s2_mmu->pgd_phys:
+	 *   - arch/arm64/include/asm/kvm_mmu.h|289| <<kvm_get_vttbr>> baddr = mmu->pgd_phys;
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|157| <<kvm_host_prepare_stage2>> mmu->pgd_phys = __hyp_pa(host_mmu.pgt.pgd);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|264| <<kvm_guest_prepare_stage2>> vm->kvm.arch.mmu.pgd_phys = __hyp_pa(vm->pgt.pgd);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|276| <<reclaim_guest_pages>> vm->kvm.arch.mmu.pgd_phys = 0ULL;
+	 *   - arch/arm64/kvm/mmu.c|925| <<kvm_init_stage2_mmu>> mmu->pgd_phys = __pa(pgt->pgd);
+	 *   - arch/arm64/kvm/mmu.c|1017| <<kvm_free_stage2_pgd>> mmu->pgd_phys = 0;
+	 */
 	phys_addr_t	pgd_phys;
 	struct kvm_pgtable *pgt;
 
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index 96a80e8f6..ea3a65d15 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -280,12 +280,21 @@ static inline int kvm_write_guest_lock(struct kvm *kvm, gpa_t gpa,
  * path, we rely on a previously issued DSB so that page table updates
  * and VMID reads are correctly ordered.
  */
+/*
+ * called by:
+ *   - arch/arm64/include/asm/kvm_mmu.h|303| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|297| <<__pkvm_prot_finalize>> params->vttbr = kvm_get_vttbr(mmu);
+ */
 static __always_inline u64 kvm_get_vttbr(struct kvm_s2_mmu *mmu)
 {
 	struct kvm_vmid *vmid = &mmu->vmid;
 	u64 vmid_field, baddr;
 	u64 cnp = system_supports_cnp() ? VTTBR_CNP_BIT : 0;
 
+	/*
+	 * struct kvm_s2_mmu *mmu:
+	 * -> phys_addr_t pgd_phys;
+	 */
 	baddr = mmu->pgd_phys;
 	vmid_field = atomic64_read(&vmid->id) << VTTBR_VMID_SHIFT;
 	vmid_field &= VTTBR_VMID_MASK(kvm_arm_vmid_bits);
@@ -296,10 +305,34 @@ static __always_inline u64 kvm_get_vttbr(struct kvm_s2_mmu *mmu)
  * Must be called from hyp code running at EL2 with an updated VTTBR
  * and interrupts disabled.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|89| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|310| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|306| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|65| <<__tlb_switch_to_guest>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|227| <<__kvm_vcpu_run_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|56| <<__tlb_switch_to_guest>> __load_stage2(mmu, mmu->arch);
+ */
 static __always_inline void __load_stage2(struct kvm_s2_mmu *mmu,
 					  struct kvm_arch *arch)
 {
+	/*
+	 * struct kvm_arch *arch:
+	 * -> u64 vtcr
+	 */
 	write_sysreg(arch->vtcr, vtcr_el2);
+	/*
+	 * called by:
+	 *   - arch/arm64/include/asm/kvm_mmu.h|303| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|297| <<__pkvm_prot_finalize>> params->vttbr = kvm_get_vttbr(mmu);
+	 *
+	 * 在以下使用vttbr_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|113| <<global>> .macro __init_el2_stage2 msr vttbr_el2, xzr
+	 *   - arch/arm64/include/asm/kvm_mmu.h|303| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|91| <<__load_host_stage2>> write_sysreg(0, vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|69| <<__tlb_switch_to_host>> write_sysreg(0, vttbr_el2);
+	 */
 	write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
 
 	/*
diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 0fcc4eb1a..cef5caffc 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -155,6 +155,10 @@ static const char *const btypes[] = {
 };
 #undef bstr
 
+/*
+ * called by:
+ *   - arch/arm64/kernel/process.c|216| <<__show_regs>> print_pstate(regs);
+ */
 static void print_pstate(struct pt_regs *regs)
 {
 	u64 pstate = regs->pstate;
@@ -197,6 +201,14 @@ static void print_pstate(struct pt_regs *regs)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kernel/entry-common.c|297| <<__panic_unhandled>> __show_regs(regs);
+ *   - arch/arm64/kernel/process.c|245| <<show_regs>> __show_regs(regs);
+ *   - arch/arm64/kernel/traps.c|263| <<arm64_show_signal>> __show_regs(regs);
+ *   - arch/arm64/kernel/traps.c|943| <<panic_bad_stack>> __show_regs(regs);
+ *   - arch/arm64/kernel/traps.c|961| <<arm64_serror_panic>> __show_regs(regs);
+ */
 void __show_regs(struct pt_regs *regs)
 {
 	int i, top_reg;
diff --git a/arch/arm64/kvm/arch_timer.c b/arch/arm64/kvm/arch_timer.c
index a1e24228a..ee68e4c31 100644
--- a/arch/arm64/kvm/arch_timer.c
+++ b/arch/arm64/kvm/arch_timer.c
@@ -24,13 +24,25 @@
 #include "trace.h"
 
 static struct timecounter *timecounter;
+/*
+ * 在以下设置host_vtimer_irq:
+ *   - arch/arm64/kvm/arch_timer.c|1332| <<kvm_irq_init>> host_vtimer_irq = info->virtual_irq;
+ */
 static unsigned int host_vtimer_irq;
+/*
+ * 在以下设置host_ptimer_irq:
+ *   - arch/arm64/kvm/arch_timer.c|1359| <<kvm_irq_init>> host_ptimer_irq = info->physical_irq;
+ */
 static unsigned int host_ptimer_irq;
 static u32 host_vtimer_irq_flags;
 static u32 host_ptimer_irq_flags;
 
 static DEFINE_STATIC_KEY_FALSE(has_gic_active_state);
 
+/*
+ * 在以下使用default_ppi[]:
+ *   - arch/arm64/kvm/arch_timer.c|1034| <<kvm_timer_init_vm>> kvm->arch.timer_data.ppi[i] = default_ppi[i];
+ */
 static const u8 default_ppi[] = {
 	[TIMER_PTIMER]  = 30,
 	[TIMER_VTIMER]  = 27,
@@ -63,6 +75,13 @@ static int nr_timers(struct kvm_vcpu *vcpu)
 	return NR_KVM_TIMERS;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|283| <<kvm_timer_irq_can_fire>> ((timer_get_ctl(timer_ctx) &
+ *   - arch/arm64/kvm/arch_timer.c|626| <<timer_restore_state>> write_sysreg_el0(timer_get_ctl(ctx), SYS_CNTV_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|636| <<timer_restore_state>> write_sysreg_el0(timer_get_ctl(ctx), SYS_CNTP_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|1102| <<read_timer_ctl>> u32 ctl = timer_get_ctl(timer);
+ */
 u32 timer_get_ctl(struct arch_timer_context *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -82,6 +101,15 @@ u32 timer_get_ctl(struct arch_timer_context *ctxt)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|276| <<kvm_timer_compute_delta>> return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
+ *   - arch/arm64/kvm/arch_timer.c|417| <<kvm_timer_should_fire>> cval = timer_get_cval(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|624| <<timer_restore_state>> write_sysreg_el0(timer_get_cval(ctx), SYS_CNTV_CVAL);
+ *   - arch/arm64/kvm/arch_timer.c|630| <<timer_restore_state>> cval = timer_get_cval(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|1143| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1152| <<kvm_arm_timer_read>> val = timer_get_cval(timer);
+ */
 u64 timer_get_cval(struct arch_timer_context *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -101,6 +129,18 @@ u64 timer_get_cval(struct arch_timer_context *ctxt)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|259| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|418| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|546| <<timer_save_state>> cval -= timer_get_offset(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|623| <<timer_restore_state>> set_cntvoff(timer_get_offset(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|631| <<timer_restore_state>> offset = timer_get_offset(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|794| <<timer_set_traps>> if (!has_cntpoff() && timer_get_offset(map->direct_ptimer))
+ *   - arch/arm64/kvm/arch_timer.c|1143| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1156| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1202| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ */
 static u64 timer_get_offset(struct arch_timer_context *ctxt)
 {
 	u64 offset = 0;
@@ -116,6 +156,13 @@ static u64 timer_get_offset(struct arch_timer_context *ctxt)
 	return offset;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|651| <<timer_save_state>> timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTV_CTL));
+ *   - arch/arm64/kvm/arch_timer.c|677| <<timer_save_state>> timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTP_CTL));
+ *   - arch/arm64/kvm/arch_timer.c|1114| <<kvm_timer_vcpu_reset>> timer_set_ctl(vcpu_get_timer(vcpu, i), 0);
+ *   - arch/arm64/kvm/arch_timer.c|1399| <<kvm_arm_timer_write>> timer_set_ctl(timer, val & ~ARCH_TIMER_CTRL_IT_STAT);
+ */
 static void timer_set_ctl(struct arch_timer_context *ctxt, u32 ctl)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -160,6 +207,13 @@ static void timer_set_cval(struct arch_timer_context *ctxt, u64 cval)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1023| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_vtimer(vcpu), kvm_phys_timer_read());
+ *   - arch/arm64/kvm/arch_timer.c|1024| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_ptimer(vcpu), 0);
+ *   - arch/arm64/kvm/arch_timer.c|1064| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1079| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ */
 static void timer_set_offset(struct arch_timer_context *ctxt, u64 offset)
 {
 	if (!ctxt->offset.vm_offset) {
@@ -167,14 +221,48 @@ static void timer_set_offset(struct arch_timer_context *ctxt, u64 offset)
 		return;
 	}
 
+	/*
+	 * struct arch_timer_context *ctxt:
+	 * -> struct arch_timer_offset offset;
+	 *    -> u64     *vm_offset;
+	 *    -> u64     *vcpu_offset;
+	 */
 	WRITE_ONCE(*ctxt->offset.vm_offset, offset);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|259| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|418| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|1023| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_vtimer(vcpu), kvm_phys_timer_read());
+ *   - arch/arm64/kvm/arch_timer.c|1064| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1079| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1143| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1156| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1202| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ */
 u64 kvm_phys_timer_read(void)
 {
+	/*
+	 * static struct timecounter *timecounter;
+	 * -> const struct cyclecounter *cc;
+	 */
 	return timecounter->cc->read(timecounter->cc);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|302| <<kvm_arch_timer_handler>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|667| <<kvm_timer_blocking>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|936| <<kvm_timer_vcpu_load>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|987| <<kvm_timer_vcpu_put>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1045| <<kvm_timer_vcpu_reset>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1298| <<kvm_arm_timer_read_sysreg>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1350| <<kvm_arm_timer_write_sysreg>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1670| <<kvm_timer_enable>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|54| <<__activate_traps>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|124| <<__deactivate_traps>> get_timer_map(vcpu, &map);
+ */
 void get_timer_map(struct kvm_vcpu *vcpu, struct timer_map *map)
 {
 	if (vcpu_has_nv(vcpu)) {
@@ -190,7 +278,13 @@ void get_timer_map(struct kvm_vcpu *vcpu, struct timer_map *map)
 			map->emul_ptimer = vcpu_hptimer(vcpu);
 		}
 	} else if (has_vhe()) {
+		/*
+		 * (&(v)->arch.timer_cpu.timers[TIMER_VTIMER])
+		 */
 		map->direct_vtimer = vcpu_vtimer(vcpu);
+		/*
+		 * (&(v)->arch.timer_cpu.timers[TIMER_PTIMER])
+		 */
 		map->direct_ptimer = vcpu_ptimer(vcpu);
 		map->emul_vtimer = NULL;
 		map->emul_ptimer = NULL;
@@ -210,6 +304,11 @@ static inline bool userspace_irqchip(struct kvm *kvm)
 		unlikely(!irqchip_in_kernel(kvm));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|574| <<timer_emulate>> soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|684| <<kvm_timer_blocking>> soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
+ */
 static void soft_timer_start(struct hrtimer *hrt, u64 ns)
 {
 	hrtimer_start(hrt, ktime_add_ns(ktime_get(), ns),
@@ -221,6 +320,11 @@ static void soft_timer_cancel(struct hrtimer *hrt)
 	hrtimer_cancel(hrt);
 }
 
+/*
+ * 在以下使用kvm_arch_timer_handler():
+ *   - arch/arm64/kvm/arch_timer.c|1389| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_vtimer_irq, kvm_arch_timer_handler, "kvm guest vtimer", kvm_get_running_vcpus());
+ *   - arch/arm64/kvm/arch_timer.c|1413| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_ptimer_irq, kvm_arch_timer_handler, "kvm guest ptimer", kvm_get_running_vcpus());
+ */
 static irqreturn_t kvm_arch_timer_handler(int irq, void *dev_id)
 {
 	struct kvm_vcpu *vcpu = *(struct kvm_vcpu **)dev_id;
@@ -328,6 +432,10 @@ static u64 kvm_timer_earliest_exp(struct kvm_vcpu *vcpu)
 	return min_delta;
 }
 
+/*
+ * bg_timer的到期执行函数,当需要调用kvm_vcpu_block让vcpu睡眠时,
+ * 需要先启动bg_timer,bg_timer到期时再将vcpu唤醒;
+ */
 static enum hrtimer_restart kvm_bg_timer_expire(struct hrtimer *hrt)
 {
 	struct arch_timer_cpu *timer;
@@ -352,6 +460,15 @@ static enum hrtimer_restart kvm_bg_timer_expire(struct hrtimer *hrt)
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+ * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+ * 然后再次进入guest时,就有中断了.
+ *
+ * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+ * timer到期的时候(还在host mode)唤醒guest并inject irq.
+ * KVM注册了一个hrtimer来处理这种情况.
+ */
 static enum hrtimer_restart kvm_hrtimer_expire(struct hrtimer *hrt)
 {
 	struct arch_timer_context *ctx;
@@ -443,6 +560,16 @@ void kvm_timer_update_run(struct kvm_vcpu *vcpu)
 		regs->device_irq_level |= KVM_ARM_DEV_EL1_PTIMER;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|247| <<kvm_arch_timer_handler>> kvm_timer_update_irq(vcpu, true, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|377| <<kvm_hrtimer_expire>> kvm_timer_update_irq(vcpu, true, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|472| <<timer_emulate>> kvm_timer_update_irq(ctx->vcpu, should_fire, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|667| <<kvm_timer_vcpu_load_gic>> kvm_timer_update_irq(ctx->vcpu, kvm_timer_should_fire(ctx), ctx);
+ *   - arch/arm64/kvm/arch_timer.c|687| <<kvm_timer_vcpu_load_nogic>> kvm_timer_update_irq(vcpu, kvm_timer_should_fire(vtimer), vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|920| <<unmask_vtimer_irq_user>> kvm_timer_update_irq(vcpu, false, vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|969| <<kvm_timer_vcpu_reset>> kvm_timer_update_irq(vcpu, false,
+ */
 static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 				 struct arch_timer_context *timer_ctx)
 {
@@ -453,6 +580,14 @@ static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 				   timer_ctx->irq.level);
 
 	if (!userspace_irqchip(vcpu->kvm)) {
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/arch_timer.c|456| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+		 *   - arch/arm64/kvm/arm.c|1195| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/arm.c|1203| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, 0, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/pmu-emul.c|351| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, 0, spi_id, level, NULL);
+		 */
 		ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
 					  timer_irq(timer_ctx),
 					  timer_ctx->irq.level,
@@ -461,6 +596,12 @@ static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 	}
 }
 
+/*
+ * 似乎一般的硬件虚拟化是不用的:
+ *   - arch/arm64/kvm/arch_timer.c|955| <<kvm_timer_vcpu_load>> timer_emulate(map.emul_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|957| <<kvm_timer_vcpu_load>> timer_emulate(map.emul_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1355| <<kvm_arm_timer_write_sysreg>> timer_emulate(timer);
+ */
 /* Only called for a fully emulated timer */
 static void timer_emulate(struct arch_timer_context *ctx)
 {
@@ -601,6 +742,13 @@ static void kvm_timer_unblocking(struct kvm_vcpu *vcpu)
 	soft_timer_cancel(&timer->bg_timer);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|850| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|852| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1189| <<kvm_arm_timer_read_sysreg>> timer_restore_state(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1240| <<kvm_arm_timer_write_sysreg>> timer_restore_state(timer);
+ */
 static void timer_restore_state(struct arch_timer_context *ctx)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(ctx->vcpu);
@@ -762,6 +910,10 @@ static void kvm_timer_vcpu_load_nested_switch(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1003| <<kvm_timer_vcpu_load>> timer_set_traps(vcpu, &map);
+ */
 static void timer_set_traps(struct kvm_vcpu *vcpu, struct timer_map *map)
 {
 	bool tpt, tpc;
@@ -817,13 +969,62 @@ static void timer_set_traps(struct kvm_vcpu *vcpu, struct timer_map *map)
 	set = 0;
 	clr = 0;
 
+	/*
+	 * - EL0PCTEN, bit [0]
+	 *
+	 * When HCR_EL2.TGE is 0, this control does not cause any instructions to be
+	 * trapped.
+	 * When HCR_EL2.TGE is 1, traps EL0 accesses to the frequency register and
+	 * physical counter register to EL2, e.g., CNTPCT_EL0.
+	 *
+	 * - EL0VCTEN, bit [1]
+	 *
+	 * When HCR_EL2.TGE is 0, this control does not cause any instructions to be
+	 * trapped.
+	 * When HCR_EL2.TGE is 1, traps EL0 accesses to the frequency register and virtual
+	 * counter register to EL2, e.g., CNTVCT_EL0.
+	 */
 	assign_clear_set_bit(tpt, CNTHCTL_EL1PCEN << 10, set, clr);
 	assign_clear_set_bit(tpc, CNTHCTL_EL1PCTEN << 10, set, clr);
 
+	/*
+	 * 在以下使用cnthctl_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|56| <<global>> msr cnthctl_el2, x0
+	 *   - arch/arm64/kvm/arch_timer.c|1007| <<timer_set_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 *   - arch/arm64/kvm/arch_timer.c|1817| <<kvm_timer_init_vhe>> sysreg_clear_set(cnthctl_el2, 0, CNTHCTL_ECV);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|31| <<__timer_disable_traps>> val = read_sysreg(cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|33| <<__timer_disable_traps>> write_sysreg(val, cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|61| <<__timer_enable_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 *
+	 * - EL1NVVCT, bit [16]
+	 * Traps EL1 accesses to the specified EL1 virtual timer registers
+	 * using the EL02 descriptors to EL2, when EL2 is enabled for the
+	 * current Security state.
+	 *
+	 * - EL1NVPCT, bit [15]
+	 * Traps EL1 accesses to the specified EL1 physical timer registers
+	 * using the EL02 descriptors to EL2, when EL2 is enabled for the
+	 * current Security state.
+	 *
+	 *
+	 * - EL1TVT: 如果是0就不会trap任何
+	 *
+	 * - EL1PTEN: 如果是1或者HCR_EL2.TGE=1, 就不会trap任何
+	 *            如果是0(没配置), 就会trap一些寄存器, 比如CNTP_CTL_EL0
+	 */
+
+	/*
+	 * Modify bits in a sysreg. Bits in the clear mask are zeroed, then bits in the
+	 * set mask are set. Other bits are left as-is.
+	 */
 	/* This only happens on VHE, so use the CNTHCTL_EL2 accessor. */
 	sysreg_clear_set(cnthctl_el2, clr, set);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|465| <<kvm_arch_vcpu_load>> kvm_timer_vcpu_load(vcpu);
+ */
 void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1048,6 +1249,10 @@ void kvm_timer_cpu_down(void)
 		disable_percpu_irq(host_ptimer_irq);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|639| <<set_timer_reg>> return kvm_arm_timer_set_reg(vcpu, reg->id, val);
+ */
 int kvm_arm_timer_set_reg(struct kvm_vcpu *vcpu, u64 regid, u64 value)
 {
 	struct arch_timer_context *timer;
@@ -1091,6 +1296,10 @@ int kvm_arm_timer_set_reg(struct kvm_vcpu *vcpu, u64 regid, u64 value)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1308| <<kvm_arm_timer_read>> val = read_timer_ctl(timer);
+ */
 static u64 read_timer_ctl(struct arch_timer_context *timer)
 {
 	/*
@@ -1132,6 +1341,25 @@ u64 kvm_arm_timer_get_reg(struct kvm_vcpu *vcpu, u64 regid)
 	return (u64)-1;
 }
 
+/*
+ * enum kvm_arch_timer_regs {
+ *     TIMER_REG_CNT,
+ *     TIMER_REG_CVAL,
+ *     TIMER_REG_TVAL,
+ *     TIMER_REG_CTL, 
+ *     TIMER_REG_VOFF,
+ * };
+ *
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1114| <<kvm_arm_timer_get_reg(KVM_REG_ARM_TIMER_CTL)>> return kvm_arm_timer_read(vcpu, vcpu_vtimer(vcpu), TIMER_REG_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|1117| <<kvm_arm_timer_get_reg(KVM_REG_ARM_TIMER_CNT)>> return kvm_arm_timer_read(vcpu, vcpu_vtimer(vcpu), TIMER_REG_CNT);
+ *   - arch/arm64/kvm/arch_timer.c|1120| <<kvm_arm_timer_get_reg(KVM_REG_ARM_TIMER_CVAL)>> return kvm_arm_timer_read(vcpu, vcpu_vtimer(vcpu), TIMER_REG_CVAL);
+ *   - arch/arm64/kvm/arch_timer.c|1123| <<kvm_arm_timer_get_reg(KVM_REG_ARM_PTIMER_CTL)>> return kvm_arm_timer_read(vcpu, vcpu_ptimer(vcpu), TIMER_REG_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|1126| <<kvm_arm_timer_get_reg(KVM_REG_ARM_PTIMER_CNT)>> return kvm_arm_timer_read(vcpu, vcpu_ptimer(vcpu), TIMER_REG_CNT);
+ *   - arch/arm64/kvm/arch_timer.c|1129| <<kvm_arm_timer_get_reg(KVM_REG_ARM_PTIMER_CVAL)>> return kvm_arm_timer_read(vcpu, vcpu_ptimer(vcpu), TIMER_REG_CVAL);
+ *   - arch/arm64/kvm/arch_timer.c|1182| <<kvm_arm_timer_read_sysreg>> return kvm_arm_timer_read(vcpu, timer, treg);
+ *   - arch/arm64/kvm/arch_timer.c|1187| <<kvm_arm_timer_read_sysreg>> val = kvm_arm_timer_read(vcpu, timer, treg);
+ */
 static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 			      struct arch_timer_context *timer,
 			      enum kvm_arch_timer_regs treg)
@@ -1192,6 +1420,15 @@ u64 kvm_arm_timer_read_sysreg(struct kvm_vcpu *vcpu,
 	return val;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1199| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CTL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1210| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CVAL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1214| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CTL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1225| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CVAL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1394| <<kvm_arm_timer_write_sysreg>> kvm_arm_timer_write(vcpu, timer, treg, val);
+ *   - arch/arm64/kvm/arch_timer.c|1399| <<kvm_arm_timer_write_sysreg>> kvm_arm_timer_write(vcpu, timer, treg, val);
+ */
 static void kvm_arm_timer_write(struct kvm_vcpu *vcpu,
 				struct arch_timer_context *timer,
 				enum kvm_arch_timer_regs treg,
@@ -1219,6 +1456,10 @@ static void kvm_arm_timer_write(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1204| <<access_arch_timer>> kvm_arm_timer_write_sysreg(vcpu, tmr, treg, p->regval);
+ */
 void kvm_arm_timer_write_sysreg(struct kvm_vcpu *vcpu,
 				enum kvm_arch_timers tmr,
 				enum kvm_arch_timer_regs treg,
@@ -1319,6 +1560,10 @@ static void kvm_irq_fixup_flags(unsigned int virq, u32 *flags)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1383| <<kvm_timer_hyp_init>> err = kvm_irq_init(info);
+ */
 static int kvm_irq_init(struct arch_timer_kvm_info *info)
 {
 	struct irq_domain *domain = NULL;
@@ -1367,12 +1612,27 @@ static int kvm_irq_init(struct arch_timer_kvm_info *info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2059| <<init_subsystems>> err = kvm_timer_hyp_init(vgic_present);
+ */
 int __init kvm_timer_hyp_init(bool has_gic)
 {
 	struct arch_timer_kvm_info *info;
 	int err;
 
+	/*
+	 * struct arch_timer_kvm_info {
+	 *     struct timecounter timecounter;
+	 *     int virtual_irq;
+	 *     int physical_irq;
+	 * };
+	 */
 	info = arch_timer_get_kvm_info();
+	/*
+	 * 这个文件的静态变量
+	 * static struct timecounter *timecounter;
+	 */
 	timecounter = &info->timecounter;
 
 	if (!timecounter->cc) {
@@ -1445,6 +1705,10 @@ int __init kvm_timer_hyp_init(bool has_gic)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|416| <<kvm_arch_vcpu_destroy>> kvm_timer_vcpu_terminate(vcpu);
+ */
 void kvm_timer_vcpu_terminate(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1506,6 +1770,10 @@ static bool kvm_arch_timer_get_input_level(int vintid)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|632| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_timer_enable(vcpu);
+ */
 int kvm_timer_enable(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1552,9 +1820,26 @@ int kvm_timer_enable(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1869| <<cpu_hyp_init_features>> kvm_timer_init_vhe();
+ */
 /* If we have CNTPOFF, permanently set ECV to enable it */
 void kvm_timer_init_vhe(void)
 {
+	/*
+	 * 在以下使用cnthctl_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|56| <<global>> msr cnthctl_el2, x0
+	 *   - arch/arm64/kvm/arch_timer.c|1007| <<timer_set_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 *   - arch/arm64/kvm/arch_timer.c|1817| <<kvm_timer_init_vhe>> sysreg_clear_set(cnthctl_el2, 0, CNTHCTL_ECV);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|31| <<__timer_disable_traps>> val = read_sysreg(cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|33| <<__timer_disable_traps>> write_sysreg(val, cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|61| <<__timer_enable_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 */
+	/*
+	 * Modify bits in a sysreg. Bits in the clear mask are zeroed, then bits in the
+	 * set mask are set. Other bits are left as-is.
+	 */
 	if (cpus_have_final_cap(ARM64_HAS_ECV_CNTPOFF))
 		sysreg_clear_set(cnthctl_el2, 0, CNTHCTL_ECV);
 }
@@ -1611,6 +1896,10 @@ int kvm_arm_timer_set_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 	return ret;
 }
 
+/*
+ * 处理KVM_ARM_SET_COUNTER_OFFSET:
+ *   - arch/arm64/kvm/arm.c|1634| <<kvm_arch_vm_ioctl>> return kvm_vm_ioctl_set_counter_offset(kvm, &offset);
+ */
 int kvm_arm_timer_get_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 {
 	int __user *uaddr = (int __user *)(long)attr->addr;
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index 4866b3f7b..425deeedd 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -133,6 +133,10 @@ static int kvm_arm_default_max_vcpus(void)
  * kvm_arch_init_vm - initializes a VM data structure
  * @kvm:	pointer to the KVM struct
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1222| <<kvm_create_vm>> r = kvm_arch_init_vm(kvm, type);
+ */
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
 	int ret;
@@ -161,6 +165,10 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	}
 	cpumask_copy(kvm->arch.supported_cpus, cpu_possible_mask);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arm.c|164| <<kvm_arch_init_vm>> ret = kvm_init_stage2_mmu(kvm, &kvm->arch.mmu, type);
+	 */
 	ret = kvm_init_stage2_mmu(kvm, &kvm->arch.mmu, type);
 	if (ret)
 		goto err_free_cpumask;
@@ -421,6 +429,14 @@ void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu)
 
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/emulate-nested.c|1947| <<kvm_emulate_nested_eret>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - arch/arm64/kvm/emulate-nested.c|2028| <<kvm_inject_nested>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - arch/arm64/kvm/reset.c|300| <<kvm_reset_vcpu>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - virt/kvm/kvm_main.c|216| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5963| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct kvm_s2_mmu *mmu;
@@ -705,6 +721,12 @@ static void kvm_vcpu_sleep(struct kvm_vcpu *vcpu)
  * the vCPU is runnable.  The vCPU may or may not be scheduled out, depending
  * on when a wake event arrives, e.g. there may already be a pending wake event.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|756| <<kvm_vcpu_suspend>> kvm_vcpu_wfi(vcpu);
+ *   - arch/arm64/kvm/handle_exit.c|147| <<kvm_handle_wfx>> kvm_vcpu_wfi(vcpu);
+ *   - arch/arm64/kvm/psci.c|49| <<kvm_psci_vcpu_suspend>> kvm_vcpu_wfi(vcpu);
+ */
 void kvm_vcpu_wfi(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -774,6 +796,10 @@ static int kvm_vcpu_suspend(struct kvm_vcpu *vcpu)
  *	   < 0 if we should exit to userspace, where the return value indicates
  *	   an error
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|975| <<kvm_arch_vcpu_ioctl_run>> ret = check_vcpu_requests(vcpu);
+ */
 static int check_vcpu_requests(struct kvm_vcpu *vcpu)
 {
 	if (kvm_request_pending(vcpu)) {
@@ -783,6 +809,17 @@ static int check_vcpu_requests(struct kvm_vcpu *vcpu)
 		if (kvm_check_request(KVM_REQ_VCPU_RESET, vcpu))
 			kvm_reset_vcpu(vcpu);
 
+		/*
+		 * 在以下使用KVM_REQ_IRQ_PENDING:
+		 *   - arch/arm64/kvm/arm.c|806| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/arm.c|1169| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/pmu-emul.c|509| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|388| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|437| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|755| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|1112| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 */
 		/*
 		 * Clear IRQ_PENDING requests that were made to guarantee
 		 * that a VCPU sees new virtual interrupts.
@@ -842,6 +879,10 @@ static bool vcpu_mode_is_bad_32bit(struct kvm_vcpu *vcpu)
  * true. For an exit to preemptible + interruptible kernel context (i.e. check
  * for pending work and re-enter), return true without writing to ret.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1000| <<kvm_arch_vcpu_ioctl_run>> if (ret <= 0 || kvm_vcpu_exit_request(vcpu, &ret)) {
+ */
 static bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu, int *ret)
 {
 	struct kvm_run *run = vcpu->run;
@@ -870,6 +911,15 @@ static bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu, int *ret)
 		return true;
 	}
 
+	/*
+	 * xfer_to_guest_mode_work_pending - Check if work is pending which needs to be
+	 *                                   handled before returning to guest mode
+	 *
+	 * Returns: True if work pending, False otherwise.
+	 *
+	 * Has to be invoked with interrupts disabled before the transition to
+	 * guest mode.
+	 */
 	return kvm_request_pending(vcpu) ||
 			xfer_to_guest_mode_work_pending();
 }
@@ -881,6 +931,10 @@ static bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu, int *ret)
  * This must be noinstr as instrumentation may make use of RCU, and this is not
  * safe during the EQS.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1008| <<kvm_arch_vcpu_ioctl_run>> ret = kvm_arm_vcpu_enter_exit(vcpu);
+ */
 static int noinstr kvm_arm_vcpu_enter_exit(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -902,17 +956,54 @@ static int noinstr kvm_arm_vcpu_enter_exit(struct kvm_vcpu *vcpu)
  * return with return value 0 and with the kvm_run structure filled in with the
  * required data for the requested emulation.
  */
+/*
+ * kvm_arch_vcpu_ioctl_run()
+ * -> handle_exit()
+ *    -> handle_trap_exceptions(vcpu)
+ *       -> kvm_get_exit_handler()
+ *          -> arm_exit_handlers[esr_ec]
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|4175| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *run = vcpu->run;
 	int ret;
 
+	/*
+	 * 设置run->exit_reason为KVM_EXIT_MMIO
+	 *   - arch/arm64/kvm/mmio.c|199| <<io_mem_abort>> run->exit_reason = KVM_EXIT_MMIO;
+	 */
 	if (run->exit_reason == KVM_EXIT_MMIO) {
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/arm.c|935| <<kvm_arch_vcpu_ioctl_run>> ret = kvm_handle_mmio_return(vcpu);
+		 *   - arch/arm64/kvm/mmio.c|187| <<io_mem_abort>> kvm_handle_mmio_return(vcpu);
+		 *
+		 * kvm_handle_mmio_return -- Handle MMIO loads after user space emulation
+		 *                           or in-kernel IO emulatio
+		 */
 		ret = kvm_handle_mmio_return(vcpu);
 		if (ret)
 			return ret;
 	}
 
+	/*
+	 * 关于arm64的调用:
+	 *   - 只在这里!!!
+	 *
+	 * 关于kvm_arch_vcpu_load()的调用:
+	 *   - arch/arm64/kvm/emulate-nested.c|1947| <<kvm_emulate_nested_eret>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+	 *   - arch/arm64/kvm/emulate-nested.c|2028| <<kvm_inject_nested>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+	 *   - arch/arm64/kvm/reset.c|300| <<kvm_reset_vcpu>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+	 *   - virt/kvm/kvm_main.c|216| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+	 *   - virt/kvm/kvm_main.c|5963| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+	 *
+	 * vcpu_load()
+	 * -> kvm_arch_vcpu_load()
+	 *    -> kvm_vcpu_load_sysregs_vhe()
+	 */
 	vcpu_load(vcpu);
 
 	if (run->immediate_exit) {
@@ -920,6 +1011,15 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		goto out;
 	}
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arm.c|939| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/mips/kvm/mips.c|431| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/powerpc/kvm/powerpc.c|1859| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/riscv/kvm/vcpu.c|664| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/s390/kvm/kvm-s390.c|5065| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/x86/kvm/x86.c|11207| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 */
 	kvm_sigset_activate(vcpu);
 
 	ret = 1;
@@ -933,6 +1033,15 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		if (!ret)
 			ret = 1;
 
+		/*
+		 * check_vcpu_requests - check and handle pending vCPU requests
+		 * @vcpu:       the VCPU pointer
+		 *
+		 * Return: 1 if we should enter the guest
+		 *         0 if we should exit to userspace
+		 *         < 0 if we should exit to userspace, where the return value indicates
+		 *         an error
+		 */
 		if (ret > 0)
 			ret = check_vcpu_requests(vcpu);
 
@@ -956,6 +1065,17 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 
 		local_irq_disable();
 
+		/*
+		 * 这里是关于中断的!!!!
+		 *
+		 * called by:
+		 *   - arch/arm64/kvm/arm.c|988| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_flush_hwstate(vcpu);
+		 *
+		 * 对于gicv3核心思想分为两步. (还有gicv4的)
+		 *
+		 * 1. 对于gicv3, 写入vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+		 * 2. 对于gicv3, 把vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr]写入寄存器
+		 */
 		kvm_vgic_flush_hwstate(vcpu);
 
 		kvm_pmu_update_vcpu_events(vcpu);
@@ -989,6 +1109,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		trace_kvm_entry(*vcpu_pc(vcpu));
 		guest_timing_enter_irqoff();
 
+		/*
+		 * __kvm_vcpu_run()
+		 */
 		ret = kvm_arm_vcpu_enter_exit(vcpu);
 
 		vcpu->mode = OUTSIDE_GUEST_MODE;
@@ -1069,6 +1192,15 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 			ret = ARM_EXCEPTION_IL;
 		}
 
+		/*
+		 * kvm_arch_vcpu_ioctl_run()
+		 * -> handle_exit()
+		 *    -> handle_trap_exceptions(vcpu)
+		 *       -> kvm_get_exit_handler()
+		 *          -> arm_exit_handlers[esr_ec]
+		 *
+		 * 只有ret > 0, while循环才会进行下去
+		 */
 		ret = handle_exit(vcpu, ret);
 	}
 
diff --git a/arch/arm64/kvm/guest.c b/arch/arm64/kvm/guest.c
index 95f6945c4..eec4883ef 100644
--- a/arch/arm64/kvm/guest.c
+++ b/arch/arm64/kvm/guest.c
@@ -626,6 +626,10 @@ static int copy_timer_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|797| <<kvm_arm_set_reg>> return set_timer_reg(vcpu, reg);
+ */
 static int set_timer_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)
 {
 	void __user *uaddr = (void __user *)(long)reg->addr;
diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 617ae6dea..6f9f331f1 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -252,6 +252,10 @@ static int handle_svc(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用arm_exit_handlers[]:
+ *   - arch/arm64/kvm/handle_exit.c|288| <<kvm_get_exit_handler>> return arm_exit_handlers[esr_ec];
+ */
 static exit_handle_fn arm_exit_handlers[] = {
 	[0 ... ESR_ELx_EC_MAX]	= kvm_handle_unknown_ec,
 	[ESR_ELx_EC_WFx]	= kvm_handle_wfx,
@@ -280,11 +284,18 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_PAC]	= kvm_handle_ptrauth,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|311| <<handle_trap_exceptions>> exit_handler = kvm_get_exit_handler(vcpu);
+ */
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 {
 	u64 esr = kvm_vcpu_get_esr(vcpu);
 	u8 esr_ec = ESR_ELx_EC(esr);
 
+	/*
+	 * 只在此处使用
+	 */
 	return arm_exit_handlers[esr_ec];
 }
 
@@ -294,6 +305,10 @@ static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
  * KVM_EXIT_DEBUG, otherwise userspace needs to complete its
  * emulation first.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|342| <<handle_exit>> return handle_trap_exceptions(vcpu);
+ */
 static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
 {
 	int handled;
@@ -319,6 +334,16 @@ static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
  * Return > 0 to return to guest, < 0 on error, 0 (and set exit_reason) on
  * proper exit to userspace.
  */
+/*
+ * kvm_arch_vcpu_ioctl_run()
+ * -> handle_exit()
+ *    -> handle_trap_exceptions(vcpu)
+ *       -> kvm_get_exit_handler()
+ *          -> arm_exit_handlers[esr_ec]
+ *
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1140| <<kvm_arch_vcpu_ioctl_run>> ret = handle_exit(vcpu, ret);
+ */
 int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 {
 	struct kvm_run *run = vcpu->run;
@@ -362,6 +387,10 @@ int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1117| <<kvm_arch_vcpu_ioctl_run>> handle_exit_early(vcpu, ret);
+ */
 /* For exit types that need handling before we can be preempted */
 void handle_exit_early(struct kvm_vcpu *vcpu, int exception_index)
 {
diff --git a/arch/arm64/kvm/hyp/include/hyp/switch.h b/arch/arm64/kvm/hyp/include/hyp/switch.h
index 9cfe6bd1d..3fa79da4f 100644
--- a/arch/arm64/kvm/hyp/include/hyp/switch.h
+++ b/arch/arm64/kvm/hyp/include/hyp/switch.h
@@ -154,6 +154,10 @@ static inline void __activate_traps_hfgxtr(struct kvm_vcpu *vcpu)
 	write_sysreg_s(w_val, SYS_HDFGWTR_EL2);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|233| <<__deactivate_traps_common>> __deactivate_traps_hfgxtr(vcpu);
+ */
 static inline void __deactivate_traps_hfgxtr(struct kvm_vcpu *vcpu)
 {
 	struct kvm_cpu_context *hctxt = &this_cpu_ptr(&kvm_host_data)->host_ctxt;
@@ -214,6 +218,11 @@ static inline void __activate_traps_common(struct kvm_vcpu *vcpu)
 	__activate_traps_hfgxtr(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|107| <<__deactivate_traps>> __deactivate_traps_common(vcpu);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|179| <<deactivate_traps_vhe_put>> __deactivate_traps_common(vcpu);
+ */
 static inline void __deactivate_traps_common(struct kvm_vcpu *vcpu)
 {
 	write_sysreg(vcpu->arch.mdcr_el2_host, mdcr_el2);
@@ -233,8 +242,20 @@ static inline void __deactivate_traps_common(struct kvm_vcpu *vcpu)
 	__deactivate_traps_hfgxtr(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|43| <<__activate_traps>> ___activate_traps(vcpu);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|40| <<__activate_traps>> ___activate_traps(vcpu);
+ */
 static inline void ___activate_traps(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> u64 hcr_el2;
+	 *    -> u64 mdcr_el2;
+	 *    -> u64 cptr_el2;
+	 */
 	u64 hcr = vcpu->arch.hcr_el2;
 
 	if (cpus_have_final_cap(ARM64_WORKAROUND_CAVIUM_TX2_219_TVM))
@@ -522,6 +543,17 @@ static bool handle_ampere1_tcr(struct kvm_vcpu *vcpu)
 	return true;
 }
 
+/*
+ * 处理ESR_ELx_EC_SYS64.
+ * EC == 0b011000
+ * When AArch64 is supported: Trapped MSR, MRS or System instruction execution
+ * in AArch64 state, that is not reported using EC 0b000000, 0b000001 or
+ * 0b000111.
+ * This includes all instructions that cause exceptions that are part of the
+ * encoding space defined in System instruction class encoding overview on page
+ * C5-731, except for those exceptions reported using EC values 0b000000,
+ * 0b000001, or 0b000111.
+ */
 static bool kvm_hyp_handle_sysreg(struct kvm_vcpu *vcpu, u64 *exit_code)
 {
 	if (cpus_have_final_cap(ARM64_WORKAROUND_CAVIUM_TX2_219_TVM) &&
@@ -642,6 +674,11 @@ static inline void synchronize_vcpu_pstate(struct kvm_vcpu *vcpu, u64 *exit_code
  * the guest, false when we should restore the host state and return to the
  * main run loop.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|319| <<__kvm_vcpu_run>> } while (fixup_guest_exit(vcpu, &exit_code));
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|297| <<__kvm_vcpu_run_vhe>> } while (fixup_guest_exit(vcpu, &exit_code));
+ */
 static inline bool fixup_guest_exit(struct kvm_vcpu *vcpu, u64 *exit_code)
 {
 	/*
@@ -702,6 +739,13 @@ static inline void __kvm_unexpected_el2_exception(void)
 	extern char __guest_exit_panic[];
 	unsigned long addr, fixup;
 	struct kvm_exception_table_entry *entry, *end;
+	/*
+	 * c在以下使用elr_el2:
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|737| <<__kvm_unexpected_el2_exception>> unsigned long elr_el2 = read_sysreg(elr_el2);
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|746| <<__kvm_unexpected_el2_exception>> if (addr != elr_el2) {
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|751| <<__kvm_unexpected_el2_exception>> write_sysreg(fixup, elr_el2);
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|756| <<__kvm_unexpected_el2_exception>> write_sysreg(__guest_exit_panic, elr_el2);
+	 */
 	unsigned long elr_el2 = read_sysreg(elr_el2);
 
 	entry = &__start___kvm_ex_table;
diff --git a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
index bb6b571ec..731bc676e 100644
--- a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
+++ b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
@@ -37,6 +37,11 @@ static inline bool ctxt_has_mte(struct kvm_cpu_context *ctxt)
 	return kvm_has_mte(kern_hyp_va(vcpu->kvm));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/sysreg-sr.c|23| <<__sysreg_save_state_nvhe>> __sysreg_save_el1_state(ctxt);
+ *   - arch/arm64/kvm/hyp/vhe/sysreg-sr.c|118| <<kvm_vcpu_put_sysregs_vhe>> __sysreg_save_el1_state(guest_ctxt);
+ */
 static inline void __sysreg_save_el1_state(struct kvm_cpu_context *ctxt)
 {
 	ctxt_sys_reg(ctxt, SCTLR_EL1)	= read_sysreg_el1(SYS_SCTLR);
@@ -97,6 +102,11 @@ static inline void __sysreg_restore_user_state(struct kvm_cpu_context *ctxt)
 	write_sysreg(ctxt_sys_reg(ctxt, TPIDRRO_EL0),	tpidrro_el0);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/sysreg-sr.c|31| <<__sysreg_restore_state_nvhe>> __sysreg_restore_el1_state(ctxt);
+ *   - arch/arm64/kvm/hyp/vhe/sysreg-sr.c|92| <<kvm_vcpu_load_sysregs_vhe>> __sysreg_restore_el1_state(guest_ctxt);
+ */
 static inline void __sysreg_restore_el1_state(struct kvm_cpu_context *ctxt)
 {
 	write_sysreg(ctxt_sys_reg(ctxt, MPIDR_EL1),	vmpidr_el2);
diff --git a/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h b/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
index 0972faccc..083ef237e 100644
--- a/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
+++ b/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
@@ -85,6 +85,13 @@ int refill_memcache(struct kvm_hyp_memcache *mc, unsigned long min_pages,
 
 static __always_inline void __load_host_stage2(void)
 {
+	/*
+	 * 在以下使用vttbr_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|113| <<global>> .macro __init_el2_stage2 msr vttbr_el2, xzr
+	 *   - arch/arm64/include/asm/kvm_mmu.h|303| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|91| <<__load_host_stage2>> write_sysreg(0, vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|69| <<__tlb_switch_to_host>> write_sysreg(0, vttbr_el2);
+	 */
 	if (static_branch_likely(&kvm_protected_mode_initialized))
 		__load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
 	else
diff --git a/arch/arm64/kvm/hyp/nvhe/timer-sr.c b/arch/arm64/kvm/hyp/nvhe/timer-sr.c
index 3aaab20ae..8236e15d1 100644
--- a/arch/arm64/kvm/hyp/nvhe/timer-sr.c
+++ b/arch/arm64/kvm/hyp/nvhe/timer-sr.c
@@ -27,6 +27,15 @@ void __timer_disable_traps(struct kvm_vcpu *vcpu)
 	if (has_hvhe())
 		shift = 10;
 
+	/*
+	 * 在以下使用cnthctl_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|56| <<global>> msr cnthctl_el2, x0
+	 *   - arch/arm64/kvm/arch_timer.c|1007| <<timer_set_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 *   - arch/arm64/kvm/arch_timer.c|1817| <<kvm_timer_init_vhe>> sysreg_clear_set(cnthctl_el2, 0, CNTHCTL_ECV);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|31| <<__timer_disable_traps>> val = read_sysreg(cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|33| <<__timer_disable_traps>> write_sysreg(val, cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|61| <<__timer_enable_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 */
 	/* Allow physical timer/counter access for the host */
 	val = read_sysreg(cnthctl_el2);
 	val |= (CNTHCTL_EL1PCTEN | CNTHCTL_EL1PCEN) << shift;
@@ -58,5 +67,14 @@ void __timer_enable_traps(struct kvm_vcpu *vcpu)
 		set <<= 10;
 	}
 
+	/*
+	 * 在以下使用cnthctl_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|56| <<global>> msr cnthctl_el2, x0
+	 *   - arch/arm64/kvm/arch_timer.c|1007| <<timer_set_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 *   - arch/arm64/kvm/arch_timer.c|1817| <<kvm_timer_init_vhe>> sysreg_clear_set(cnthctl_el2, 0, CNTHCTL_ECV);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|31| <<__timer_disable_traps>> val = read_sysreg(cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|33| <<__timer_disable_traps>> write_sysreg(val, cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|61| <<__timer_enable_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 */
 	sysreg_clear_set(cnthctl_el2, clr, set);
 }
diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
index 6cb638b18..83426a83b 100644
--- a/arch/arm64/kvm/hyp/vgic-v3-sr.c
+++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
@@ -231,6 +231,11 @@ void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|129| <<__hyp_vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ *   - arch/arm64/kvm/vgic/vgic.c|971| <<vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ */
 void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 used_lrs = cpu_if->used_lrs;
@@ -257,6 +262,11 @@ void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|128| <<__hyp_vgic_restore_state>> __vgic_v3_activate_traps(&vcpu->arch.vgic_cpu.vgic_v3);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|735| <<vgic_v3_load>> __vgic_v3_activate_traps(cpu_if);
+ */
 void __vgic_v3_activate_traps(struct vgic_v3_cpu_if *cpu_if)
 {
 	/*
diff --git a/arch/arm64/kvm/hyp/vhe/switch.c b/arch/arm64/kvm/hyp/vhe/switch.c
index 448b17080..35c115605 100644
--- a/arch/arm64/kvm/hyp/vhe/switch.c
+++ b/arch/arm64/kvm/hyp/vhe/switch.c
@@ -31,12 +31,32 @@
 /* VHE specific context */
 DEFINE_PER_CPU(struct kvm_host_data, kvm_host_data);
 DEFINE_PER_CPU(struct kvm_cpu_context, kvm_hyp_ctxt);
+/*
+ * 在以下使用percpu的kvm_hyp_vector:
+ *   - arch/arm64/include/asm/kvm_hyp.h|16| <<global>> DECLARE_PER_CPU(unsigned long , kvm_hyp_vector);
+ *   - arch/arm64/kvm/arm.c|50| <<global>> DECLARE_KVM_HYP_PER_CPU(unsigned long , kvm_hyp_vector);
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|35| <<global>> DEFINE_PER_CPU(unsigned long , kvm_hyp_vector);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|34| <<global>> DEFINE_PER_CPU(unsigned long , kvm_hyp_vector);
+ *   - arch/arm64/kvm/arm.c|1850| <<cpu_set_hyp_vector>> *this_cpu_ptr_hyp_sym(kvm_hyp_vector) = (unsigned long )vector;
+ *   - arch/arm64/kvm/hyp/nvhe/mm.c|205| <<pkvm_cpu_set_vector>> *this_cpu_ptr(&kvm_hyp_vector) = (unsigned long )vector;
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|67| <<__activate_traps>> write_sysreg(__this_cpu_read(kvm_hyp_vector), vbar_el2);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|97| <<__activate_traps>> write_sysreg(__this_cpu_read(kvm_hyp_vector), vbar_el1);
+ */
 DEFINE_PER_CPU(unsigned long, kvm_hyp_vector);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|228| <<__kvm_vcpu_run_vhe>> __activate_traps(vcpu);
+ */
 static void __activate_traps(struct kvm_vcpu *vcpu)
 {
 	u64 val;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/hyp/nvhe/switch.c|43| <<__activate_traps>> ___activate_traps(vcpu);
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|40| <<__activate_traps>> ___activate_traps(vcpu);
+	 */
 	___activate_traps(vcpu);
 
 	if (has_cntpoff()) {
@@ -89,12 +109,23 @@ static void __activate_traps(struct kvm_vcpu *vcpu)
 }
 NOKPROBE_SYMBOL(__activate_traps);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|249| <<__kvm_vcpu_run_vhe>> __deactivate_traps(vcpu);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|304| <<__hyp_call_panic>> __deactivate_traps(vcpu);
+ */
 static void __deactivate_traps(struct kvm_vcpu *vcpu)
 {
 	const char *host_vectors = vectors;
 
 	___deactivate_traps(vcpu);
 
+	/*
+	 * 在以下使用HCR_HOST_VHE_FLAGS:
+	 *   - arch/arm64/include/asm/kvm_arm.h|103| <<global>> #define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|98| <<__deactivate_traps>> write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|77| <<__tlb_switch_to_host>> write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
+	 */
 	write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
 
 	if (has_cntpoff()) {
@@ -207,8 +238,18 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 	struct kvm_cpu_context *guest_ctxt;
 	u64 exit_code;
 
+	/*
+	 * struct kvm_host_data {
+	 *     struct kvm_cpu_context host_ctxt;
+	 * };
+	 */
 	host_ctxt = &this_cpu_ptr(&kvm_host_data)->host_ctxt;
 	host_ctxt->__hyp_running_vcpu = vcpu;
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_cpu_context ctxt;
+	 */
 	guest_ctxt = &vcpu->arch.ctxt;
 
 	sysreg_save_host_state_vhe(host_ctxt);
@@ -224,7 +265,15 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 	 * __load_stage2 configures stage 2 translation, and
 	 * __activate_traps clear HCR_EL2.TGE (among other things).
 	 */
+	/*
+	 * 按照上面的erratum, 之前:
+	 * kvm_arch_vcpu_load()
+	 * -> kvm_vcpu_load_sysregs_vhe()
+	 */
 	__load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+	/*
+	 * 只在这里调用
+	 */
 	__activate_traps(vcpu);
 
 	__kvm_adjust_pc(vcpu);
@@ -238,6 +287,9 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 		vcpu_clear_flag(vcpu, VCPU_HYP_CONTEXT);
 
 	do {
+		/*
+		 * arch/arm64/kvm/hyp/entry.S
+		 */
 		/* Jump in the fire! */
 		exit_code = __guest_enter(vcpu);
 
@@ -246,6 +298,11 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 
 	sysreg_save_guest_state_vhe(guest_ctxt);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|249| <<__kvm_vcpu_run_vhe>> __deactivate_traps(vcpu);
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|304| <<__hyp_call_panic>> __deactivate_traps(vcpu);
+	 */
 	__deactivate_traps(vcpu);
 
 	sysreg_restore_host_state_vhe(host_ctxt);
@@ -259,6 +316,10 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 }
 NOKPROBE_SYMBOL(__kvm_vcpu_run_vhe);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|905| <<kvm_arm_vcpu_enter_exit>> ret = kvm_call_hyp_ret(__kvm_vcpu_run, vcpu);
+ */
 int __kvm_vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int ret;
diff --git a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
index b35a178e7..f6dbdb17e 100644
--- a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
+++ b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
@@ -25,6 +25,10 @@
  * classes are handled as part of kvm_arch_vcpu_load and kvm_arch_vcpu_put.
  */
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|245| <<__kvm_vcpu_run_vhe>> sysreg_save_host_state_vhe(host_ctxt);
+ */
 void sysreg_save_host_state_vhe(struct kvm_cpu_context *ctxt)
 {
 	__sysreg_save_common_state(ctxt);
@@ -44,6 +48,10 @@ void sysreg_restore_host_state_vhe(struct kvm_cpu_context *ctxt)
 }
 NOKPROBE_SYMBOL(sysreg_restore_host_state_vhe);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|271| <<__kvm_vcpu_run_vhe>> sysreg_restore_guest_state_vhe(guest_ctxt);
+ */
 void sysreg_restore_guest_state_vhe(struct kvm_cpu_context *ctxt)
 {
 	__sysreg_restore_common_state(ctxt);
@@ -62,8 +70,28 @@ NOKPROBE_SYMBOL(sysreg_restore_guest_state_vhe);
  * and loading system register state early avoids having to load them on
  * every entry to the VM.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|459| <<kvm_arch_vcpu_load>> kvm_vcpu_load_sysregs_vhe(vcpu);
+ */
 void kvm_vcpu_load_sysregs_vhe(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_cpu_context {
+	 *     struct user_pt_regs regs;       // sp = sp_el0
+	 *
+	 *     u64     spsr_abt;
+	 *     u64     spsr_und;
+	 *     u64     spsr_irq;
+	 *     u64     spsr_fiq;
+	 *     
+	 *     struct user_fpsimd_state fp_regs;
+	 *
+	 *     u64 sys_regs[NR_SYS_REGS];
+	 *
+	 *     struct kvm_vcpu *__hyp_running_vcpu;
+	 * };
+	 */
 	struct kvm_cpu_context *guest_ctxt = &vcpu->arch.ctxt;
 	struct kvm_cpu_context *host_ctxt;
 
@@ -107,6 +135,10 @@ void kvm_vcpu_load_sysregs_vhe(struct kvm_vcpu *vcpu)
  * and deferring saving system register state until we're no longer running the
  * VCPU avoids having to save them on every exit from the VM.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|483| <<kvm_arch_vcpu_put>> kvm_vcpu_put_sysregs_vhe(vcpu);
+ */
 void kvm_vcpu_put_sysregs_vhe(struct kvm_vcpu *vcpu)
 {
 	struct kvm_cpu_context *guest_ctxt = &vcpu->arch.ctxt;
diff --git a/arch/arm64/kvm/hyp/vhe/tlb.c b/arch/arm64/kvm/hyp/vhe/tlb.c
index 46bd43f61..3de722ffa 100644
--- a/arch/arm64/kvm/hyp/vhe/tlb.c
+++ b/arch/arm64/kvm/hyp/vhe/tlb.c
@@ -66,7 +66,20 @@ static void __tlb_switch_to_host(struct tlb_inv_context *cxt)
 	 * We're done with the TLB operation, let's restore the host's
 	 * view of HCR_EL2.
 	 */
+	/*
+	 * 在以下使用vttbr_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|113| <<global>> .macro __init_el2_stage2 msr vttbr_el2, xzr
+	 *   - arch/arm64/include/asm/kvm_mmu.h|303| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|91| <<__load_host_stage2>> write_sysreg(0, vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|69| <<__tlb_switch_to_host>> write_sysreg(0, vttbr_el2);
+	 */
 	write_sysreg(0, vttbr_el2);
+	/*
+	 * 在以下使用HCR_HOST_VHE_FLAGS:
+	 *   - arch/arm64/include/asm/kvm_arm.h|103| <<global>> #define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|98| <<__deactivate_traps>> write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|77| <<__tlb_switch_to_host>> write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
+	 */
 	write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
 	isb();
 
diff --git a/arch/arm64/kvm/mmio.c b/arch/arm64/kvm/mmio.c
index 3dd38a151..942abc7b6 100644
--- a/arch/arm64/kvm/mmio.c
+++ b/arch/arm64/kvm/mmio.c
@@ -78,6 +78,11 @@ unsigned long kvm_mmio_read_buf(const void *buf, unsigned int len)
  *
  * @vcpu: The VCPU pointer
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|935| <<kvm_arch_vcpu_ioctl_run>> ret = kvm_handle_mmio_return(vcpu);
+ *   - arch/arm64/kvm/mmio.c|187| <<io_mem_abort>> kvm_handle_mmio_return(vcpu);
+ */
 int kvm_handle_mmio_return(struct kvm_vcpu *vcpu)
 {
 	unsigned long data;
@@ -120,6 +125,10 @@ int kvm_handle_mmio_return(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1762| <<kvm_handle_guest_abort>> ret = io_mem_abort(vcpu, fault_ipa);
+ */
 int io_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
 {
 	struct kvm_run *run = vcpu->run;
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index 482280fe2..e07fc80e5 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -863,6 +863,10 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
  * Note we don't need locking here as this is only called when the VM is
  * created, which can only be done once.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|164| <<kvm_arch_init_vm>> ret = kvm_init_stage2_mmu(kvm, &kvm->arch.mmu, type);
+ */
 int kvm_init_stage2_mmu(struct kvm *kvm, struct kvm_s2_mmu *mmu, unsigned long type)
 {
 	u32 kvm_ipa_limit = get_kvm_ipa_limit();
@@ -899,6 +903,10 @@ int kvm_init_stage2_mmu(struct kvm *kvm, struct kvm_s2_mmu *mmu, unsigned long t
 		return -EINVAL;
 	}
 
+	/*
+	 * struct kvm_pgtable *pgt;
+	 * -> kvm_pteref_t pgd;
+	 */
 	pgt = kzalloc(sizeof(*pgt), GFP_KERNEL_ACCOUNT);
 	if (!pgt)
 		return -ENOMEM;
@@ -922,6 +930,15 @@ int kvm_init_stage2_mmu(struct kvm *kvm, struct kvm_s2_mmu *mmu, unsigned long t
 	mmu->split_page_cache.gfp_zero = __GFP_ZERO;
 
 	mmu->pgt = pgt;
+	/*
+	 * 在以下使用kvm_s2_mmu->pgd_phys:
+	 *   - arch/arm64/include/asm/kvm_mmu.h|289| <<kvm_get_vttbr>> baddr = mmu->pgd_phys;
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|157| <<kvm_host_prepare_stage2>> mmu->pgd_phys = __hyp_pa(host_mmu.pgt.pgd);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|264| <<kvm_guest_prepare_stage2>> vm->kvm.arch.mmu.pgd_phys = __hyp_pa(vm->pgt.pgd);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|276| <<reclaim_guest_pages>> vm->kvm.arch.mmu.pgd_phys = 0ULL;
+	 *   - arch/arm64/kvm/mmu.c|925| <<kvm_init_stage2_mmu>> mmu->pgd_phys = __pa(pgt->pgd);
+	 *   - arch/arm64/kvm/mmu.c|1017| <<kvm_free_stage2_pgd>> mmu->pgd_phys = 0;
+	 */
 	mmu->pgd_phys = __pa(pgt->pgd);
 	return 0;
 
@@ -1635,6 +1652,11 @@ static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
  * space. The distinction is based on the IPA causing the fault and whether this
  * memory region has been registered as standard RAM by user space.
  */
+/*
+ * called by (static exit_handle_fn arm_exit_handlers[]):
+ *   - arch/arm64/kvm/handle_exit.c|272| <<global>> [ESR_ELx_EC_IABT_LOW] = kvm_handle_guest_abort,
+ *   - arch/arm64/kvm/handle_exit.c|273| <<global>> [ESR_ELx_EC_DABT_LOW] = kvm_handle_guest_abort,
+ */
 int kvm_handle_guest_abort(struct kvm_vcpu *vcpu)
 {
 	unsigned long fault_status;
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index 0afd6136e..df62d5f1f 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -295,6 +295,15 @@ static bool access_actlr(struct kvm_vcpu *vcpu,
  * The cp15_64 code makes sure this automatically works
  * for both AArch64 and AArch32 accesses.
  */
+/*
+ * 在以下使用access_gic_sgi():
+ *   - arch/arm64/kvm/sys_regs.c|2156| <<global>> { SYS_DESC(SYS_ICC_SGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2157| <<global>> { SYS_DESC(SYS_ICC_ASGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2158| <<global>> { SYS_DESC(SYS_ICC_SGI0R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2758| <<global>> { Op1( 0), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2761| <<global>> { Op1( 1), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2762| <<global>> { Op1( 2), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ */
 static bool access_gic_sgi(struct kvm_vcpu *vcpu,
 			   struct sys_reg_params *p,
 			   const struct sys_reg_desc *r)
@@ -1164,6 +1173,19 @@ static unsigned int ptrauth_visibility(const struct kvm_vcpu *vcpu,
 	__PTRAUTH_KEY(k ## KEYLO_EL1),					\
 	__PTRAUTH_KEY(k ## KEYHI_EL1)
 
+/*
+ * 在以下使用access_arch_timer():
+ *   - arch/arm64/kvm/sys_regs.c|2285| <<global>> { SYS_DESC(SYS_CNTPCT_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2286| <<global>> { SYS_DESC(SYS_CNTPCTSS_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2287| <<global>> { SYS_DESC(SYS_CNTP_TVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2288| <<global>> { SYS_DESC(SYS_CNTP_CTL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2289| <<global>> { SYS_DESC(SYS_CNTP_CVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2663| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_TVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2664| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CTL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2746| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCT), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2750| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2751| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCTSS), access_arch_timer },
+ */
 static bool access_arch_timer(struct kvm_vcpu *vcpu,
 			      struct sys_reg_params *p,
 			      const struct sys_reg_desc *r)
diff --git a/arch/arm64/kvm/vgic/vgic-irqfd.c b/arch/arm64/kvm/vgic/vgic-irqfd.c
index 475059bac..680bb13a4 100644
--- a/arch/arm64/kvm/vgic/vgic-irqfd.c
+++ b/arch/arm64/kvm/vgic/vgic-irqfd.c
@@ -66,6 +66,11 @@ int kvm_set_routing_entry(struct kvm *kvm,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|97| <<kvm_set_msi>> kvm_populate_msi(e, &msi);
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|118| <<kvm_arch_set_irq_inatomic>> kvm_populate_msi(e, &msi);
+ */
 static void kvm_populate_msi(struct kvm_kernel_irq_routing_entry *e,
 			     struct kvm_msi *msi)
 {
@@ -82,6 +87,11 @@ static void kvm_populate_msi(struct kvm_kernel_irq_routing_entry *e,
  * This is the entry point for irqfd MSI injection
  * and userspace MSI injection.
  */
+/*
+ * 在以下使用kvm_set_msi():
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|54| <<kvm_set_routing_entry>> e->set = kvm_set_msi;
+ *   - virt/kvm/irqchip.c|61| <<kvm_send_userspace_msi>> return kvm_set_msi(&route, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false);
+ */
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 		struct kvm *kvm, int irq_source_id,
 		int level, bool line_status)
@@ -95,12 +105,19 @@ int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 		return -1;
 
 	kvm_populate_msi(e, &msi);
+	/*
+	 * 只在这里调用
+	 */
 	return vgic_its_inject_msi(kvm, &msi);
 }
 
 /**
  * kvm_arch_set_irq_inatomic: fast-path for irqfd injection
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|247| <<irqfd_wakeup>> if (kvm_arch_set_irq_inatomic(&irq, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false) == -EWOULDBLOCK)
+ */
 int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 			      struct kvm *kvm, int irq_source_id, int level,
 			      bool line_status)
@@ -115,7 +132,17 @@ int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 		if (!vgic_has_its(kvm))
 			break;
 
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|97| <<kvm_set_msi>> kvm_populate_msi(e, &msi);
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|118| <<kvm_arch_set_irq_inatomic>> kvm_populate_msi(e, &msi);
+		 */
 		kvm_populate_msi(e, &msi);
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|119| <<kvm_arch_set_irq_inatomic>> return vgic_its_inject_cached_translation(kvm, &msi);
+		 *   - arch/arm64/kvm/vgic/vgic-its.c|781| <<vgic_its_inject_msi>> if (!vgic_its_inject_cached_translation(kvm, msi))
+		 */
 		return vgic_its_inject_cached_translation(kvm, &msi);
 	}
 
diff --git a/arch/arm64/kvm/vgic/vgic-its.c b/arch/arm64/kvm/vgic/vgic-its.c
index 5fe2365a6..47e284440 100644
--- a/arch/arm64/kvm/vgic/vgic-its.c
+++ b/arch/arm64/kvm/vgic/vgic-its.c
@@ -545,12 +545,31 @@ static unsigned long vgic_mmio_read_its_idregs(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|587| <<vgic_its_check_cache>> irq = __vgic_its_check_cache(dist, db, devid, eventid);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|617| <<vgic_its_cache_translation>> if (__vgic_its_check_cache(dist, db, devid, eventid))
+ */
 static struct vgic_irq *__vgic_its_check_cache(struct vgic_dist *dist,
 					       phys_addr_t db,
 					       u32 devid, u32 eventid)
 {
 	struct vgic_translation_cache_entry *cte;
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1894| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1908| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1920| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 */
 	list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
 		/*
 		 * If we hit a NULL entry, there is nothing after this
@@ -576,6 +595,10 @@ static struct vgic_irq *__vgic_its_check_cache(struct vgic_dist *dist,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|759| <<vgic_its_inject_cached_translation>> irq = vgic_its_check_cache(kvm, db, msi->devid, msi->data);
+ */
 static struct vgic_irq *vgic_its_check_cache(struct kvm *kvm, phys_addr_t db,
 					     u32 devid, u32 eventid)
 {
@@ -590,6 +613,10 @@ static struct vgic_irq *vgic_its_check_cache(struct kvm *kvm, phys_addr_t db,
 	return irq;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|712| <<vgic_its_resolve_lpi>> vgic_its_cache_translation(kvm, its, devid, eventid, ite->irq);
+ */
 static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 				       u32 devid, u32 eventid,
 				       struct vgic_irq *irq)
@@ -617,6 +644,20 @@ static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 	if (__vgic_its_check_cache(dist, db, devid, eventid))
 		goto out;
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1894| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1908| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1920| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 */
 	/* Always reuse the last entry (LRU policy) */
 	cte = list_last_entry(&dist->lpi_translation_cache,
 			      typeof(*cte), entry);
@@ -631,6 +672,9 @@ static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 
 	vgic_get_irq_kref(irq);
 
+	/*
+	 * struct vgic_translation_cache_entry *cte;
+	 */
 	cte->db		= db;
 	cte->devid	= devid;
 	cte->eventid	= eventid;
@@ -643,6 +687,17 @@ static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 	raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|893| <<vgic_its_cmd_handle_discard>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|930| <<vgic_its_cmd_handle_movi>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1189| <<vgic_its_free_device>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1296| <<vgic_its_cmd_handle_mapc>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1469| <<vgic_its_cmd_handle_movall>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1822| <<vgic_mmio_write_its_ctlr>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1964| <<vgic_lpi_translation_cache_destroy>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|280| <<vgic_mmio_write_v3r_ctlr>> vgic_its_invalidate_cache(vcpu->kvm);
+ */
 void vgic_its_invalidate_cache(struct kvm *kvm)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
@@ -651,6 +706,9 @@ void vgic_its_invalidate_cache(struct kvm *kvm)
 
 	raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
 
+	/*
+	 * struct vgic_translation_cache_entry *cte;
+	 */
 	list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
 		/*
 		 * If we hit a NULL entry, there is nothing after this
@@ -666,6 +724,12 @@ void vgic_its_invalidate_cache(struct kvm *kvm)
 	raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|760| <<vgic_its_trigger_msi>> err = vgic_its_resolve_lpi(kvm, its, devid, eventid, &irq);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|438| <<kvm_vgic_v4_set_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|510| <<kvm_vgic_v4_unset_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+ */
 int vgic_its_resolve_lpi(struct kvm *kvm, struct vgic_its *its,
 			 u32 devid, u32 eventid, struct vgic_irq **irq)
 {
@@ -686,12 +750,20 @@ int vgic_its_resolve_lpi(struct kvm *kvm, struct vgic_its *its,
 	if (!vgic_lpis_enabled(vcpu))
 		return -EBUSY;
 
+	/*
+	 * 只在这里调用
+	 */
 	vgic_its_cache_translation(kvm, its, devid, eventid, ite->irq);
 
 	*irq = ite->irq;
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|822| <<vgic_its_inject_msi>> its = vgic_msi_to_its(kvm, msi);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|408| <<vgic_get_its>> return vgic_msi_to_its(kvm, &msi);
+ */
 struct vgic_its *vgic_msi_to_its(struct kvm *kvm, struct kvm_msi *msi)
 {
 	u64 address;
@@ -727,6 +799,11 @@ struct vgic_its *vgic_msi_to_its(struct kvm *kvm, struct kvm_msi *msi)
  * Returns 0 on success, a positive error value for any ITS mapping
  * related errors and negative error values for generic errors.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|827| <<vgic_its_inject_msi>> ret = vgic_its_trigger_msi(kvm, its, msi->devid, msi->data);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1485| <<vgic_its_cmd_handle_int>> return vgic_its_trigger_msi(kvm, its, msi_devid, msi_data);
+ */
 static int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,
 				u32 devid, u32 eventid)
 {
@@ -734,6 +811,12 @@ static int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,
 	unsigned long flags;
 	int err;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|760| <<vgic_its_trigger_msi>> err = vgic_its_resolve_lpi(kvm, its, devid, eventid, &irq);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|438| <<kvm_vgic_v4_set_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|510| <<kvm_vgic_v4_unset_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+	 */
 	err = vgic_its_resolve_lpi(kvm, its, devid, eventid, &irq);
 	if (err)
 		return err;
@@ -744,11 +827,19 @@ static int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,
 
 	raw_spin_lock_irqsave(&irq->irq_lock, flags);
 	irq->pending_latch = true;
+	/*
+	 * 会被特别多调用
+	 */
 	vgic_queue_irq_unlock(kvm, irq, flags);
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|119| <<kvm_arch_set_irq_inatomic>> return vgic_its_inject_cached_translation(kvm, &msi);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|781| <<vgic_its_inject_msi>> if (!vgic_its_inject_cached_translation(kvm, msi))
+ */
 int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct vgic_irq *irq;
@@ -756,12 +847,18 @@ int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
 	phys_addr_t db;
 
 	db = (u64)msi->address_hi << 32 | msi->address_lo;
+	/*
+	 * 只在这里调用
+	 */
 	irq = vgic_its_check_cache(kvm, db, msi->devid, msi->data);
 	if (!irq)
 		return -EWOULDBLOCK;
 
 	raw_spin_lock_irqsave(&irq->irq_lock, flags);
 	irq->pending_latch = true;
+	/*
+	 * 被很多的调用
+	 */
 	vgic_queue_irq_unlock(kvm, irq, flags);
 
 	return 0;
@@ -773,19 +870,38 @@ int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
  * We then call vgic_its_trigger_msi() with the decoded data.
  * According to the KVM_SIGNAL_MSI API description returns 1 on success.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|98| <<kvm_set_msi>> return vgic_its_inject_msi(kvm, &msi);
+ */
 int vgic_its_inject_msi(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct vgic_its *its;
 	int ret;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|119| <<kvm_arch_set_irq_inatomic>> return vgic_its_inject_cached_translation(kvm, &msi);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|781| <<vgic_its_inject_msi>> if (!vgic_its_inject_cached_translation(kvm, msi))
+	 */
 	if (!vgic_its_inject_cached_translation(kvm, msi))
 		return 1;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|822| <<vgic_its_inject_msi>> its = vgic_msi_to_its(kvm, msi);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|408| <<vgic_get_its>> return vgic_msi_to_its(kvm, &msi);
+	 */
 	its = vgic_msi_to_its(kvm, msi);
 	if (IS_ERR(its))
 		return PTR_ERR(its);
 
 	mutex_lock(&its->its_lock);
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|827| <<vgic_its_inject_msi>> ret = vgic_its_trigger_msi(kvm, its, msi->devid, msi->data);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1485| <<vgic_its_cmd_handle_int>> return vgic_its_trigger_msi(kvm, its, msi_devid, msi_data);
+	 */
 	ret = vgic_its_trigger_msi(kvm, its, msi->devid, msi->data);
 	mutex_unlock(&its->its_lock);
 
@@ -1885,6 +2001,11 @@ static int vgic_register_its_iodev(struct kvm *kvm, struct vgic_its *its,
 /* Default is 16 cached LPIs per vcpu */
 #define LPI_DEFAULT_PCPU_CACHE_SIZE	16
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|309| <<vgic_init>> vgic_lpi_translation_cache_init(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1994| <<vgic_its_create>> vgic_lpi_translation_cache_init(dev->kvm);
+ */
 void vgic_lpi_translation_cache_init(struct kvm *kvm)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
@@ -1909,6 +2030,10 @@ void vgic_lpi_translation_cache_init(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|365| <<kvm_vgic_dist_destroy>> vgic_lpi_translation_cache_destroy(kvm);
+ */
 void vgic_lpi_translation_cache_destroy(struct kvm *kvm)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
diff --git a/arch/arm64/kvm/vgic/vgic-mmio-v3.c b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
index 188d2187e..36eeeb17d 100644
--- a/arch/arm64/kvm/vgic/vgic-mmio-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
@@ -1066,6 +1066,10 @@ static int match_mpidr(u64 sgi_aff, u16 sgi_cpu_mask, struct kvm_vcpu *vcpu)
  * check for matching ones. If this bit is set, we signal all, but not the
  * calling VCPU.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|338| <<access_gic_sgi>> vgic_v3_dispatch_sgi(vcpu, p->regval, g1);
+ */
 void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg, bool allow_group1)
 {
 	struct kvm *kvm = vcpu->kvm;
diff --git a/arch/arm64/kvm/vgic/vgic-v3.c b/arch/arm64/kvm/vgic/vgic-v3.c
index 3dfc8b84e..cdb1f8fc1 100644
--- a/arch/arm64/kvm/vgic/vgic-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-v3.c
@@ -32,6 +32,10 @@ static bool lr_signals_eoi_mi(u64 lr_val)
 	       !(lr_val & ICH_LR_HW);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|770| <<vgic_fold_lr_state>> vgic_v3_fold_lr_state(vcpu);
+ */
 void vgic_v3_fold_lr_state(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
@@ -103,6 +107,10 @@ void vgic_v3_fold_lr_state(struct kvm_vcpu *vcpu)
 	cpuif->used_lrs = 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|782| <<vgic_populate_lr>> vgic_v3_populate_lr(vcpu, irq, lr);
+ */
 /* Requires the irq to be locked already */
 void vgic_v3_populate_lr(struct kvm_vcpu *vcpu, struct vgic_irq *irq, int lr)
 {
diff --git a/arch/arm64/kvm/vgic/vgic-v4.c b/arch/arm64/kvm/vgic/vgic-v4.c
index 339a55194..867d6da82 100644
--- a/arch/arm64/kvm/vgic/vgic-v4.c
+++ b/arch/arm64/kvm/vgic/vgic-v4.c
@@ -408,6 +408,10 @@ static struct vgic_its *vgic_get_its(struct kvm *kvm,
 	return vgic_msi_to_its(kvm, &msi);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2396| <<kvm_arch_irq_bypass_add_producer>> return kvm_vgic_v4_set_forwarding(irqfd->kvm, prod->irq,
+ */
 int kvm_vgic_v4_set_forwarding(struct kvm *kvm, int virq,
 			       struct kvm_kernel_irq_routing_entry *irq_entry)
 {
diff --git a/arch/arm64/kvm/vgic/vgic.c b/arch/arm64/kvm/vgic/vgic.c
index 8be4c1ebd..3b369b00d 100644
--- a/arch/arm64/kvm/vgic/vgic.c
+++ b/arch/arm64/kvm/vgic/vgic.c
@@ -333,6 +333,27 @@ static bool vgic_validate_injection(struct vgic_irq *irq, bool level, void *owne
  * Needs to be entered with the IRQ lock already held, but will return
  * with all locks dropped.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|301| <<update_lpi_config>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|467| <<its_sync_lpi_pending_table>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|747| <<vgic_its_trigger_msi>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|765| <<vgic_its_inject_cached_translation>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|157| <<vgic_mmio_write_sgir>> vgic_queue_irq_unlock(source_vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|264| <<vgic_mmio_write_sgipends>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|375| <<vgic_v3_uaccess_write_pending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1131| <<vgic_v3_dispatch_sgi>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|85| <<vgic_mmio_write_group>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|159| <<vgic_mmio_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|200| <<vgic_uaccess_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|341| <<vgic_mmio_write_spending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|368| <<vgic_uaccess_write_spending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|603| <<vgic_mmio_change_active>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|828| <<vgic_write_irq_line_level_info>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|338| <<vgic_v3_lpi_sync_pending_status>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|480| <<kvm_vgic_v4_set_forwarding>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic.c|483| <<kvm_vgic_inject_irq>> vgic_queue_irq_unlock(kvm, irq, flags);
+ */
 bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 			   unsigned long flags)
 {
@@ -402,6 +423,28 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 		goto retry;
 	}
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|160| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|305| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|410| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|653| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|725| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|792| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|824| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|850| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|890| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|929| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|935| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|999| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *
+	 * List of IRQs that this VCPU should consider because they are either
+	 * Active or Pending (hence the name; AP list), or because they recently
+	 * were one of the two and need to be migrated off this list to another
+	 * VCPU.
+	 */
 	/*
 	 * Grab a reference to the irq to reflect the fact that it is
 	 * now in the ap_list.
@@ -413,6 +456,17 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 	raw_spin_unlock(&irq->irq_lock);
 	raw_spin_unlock_irqrestore(&vcpu->arch.vgic_cpu.ap_list_lock, flags);
 
+	/*
+	 * 在以下使用KVM_REQ_IRQ_PENDING:
+	 *   - arch/arm64/kvm/arm.c|806| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/arm.c|1169| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/pmu-emul.c|509| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|388| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|437| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|755| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|1112| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 */
 	kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
 	kvm_vcpu_kick(vcpu);
 
@@ -436,6 +490,14 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
  * level-sensitive interrupts.  You can think of the level parameter as 1
  * being HIGH and 0 being LOW and all devices being active-HIGH.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|456| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/arm.c|1195| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level, NULL);
+ *   - arch/arm64/kvm/arm.c|1203| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, 0, irq_num, level, NULL);
+ *   - arch/arm64/kvm/pmu-emul.c|351| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, 0, spi_id, level, NULL);
+ */
 int kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int intid,
 			bool level, void *owner)
 {
@@ -733,6 +795,10 @@ static void vgic_prune_ap_list(struct kvm_vcpu *vcpu)
 	raw_spin_unlock(&vgic_cpu->ap_list_lock);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|927| <<kvm_vgic_sync_hwstate>> vgic_fold_lr_state(vcpu);
+ */
 static inline void vgic_fold_lr_state(struct kvm_vcpu *vcpu)
 {
 	if (kvm_vgic_global_state.type == VGIC_V2)
@@ -741,6 +807,10 @@ static inline void vgic_fold_lr_state(struct kvm_vcpu *vcpu)
 		vgic_v3_fold_lr_state(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|865| <<vgic_flush_lr_state>> vgic_populate_lr(vcpu, irq, count++);
+ */
 /* Requires the irq_lock to be held. */
 static inline void vgic_populate_lr(struct kvm_vcpu *vcpu,
 				    struct vgic_irq *irq, int lr)
@@ -796,6 +866,10 @@ static int compute_ap_list_depth(struct kvm_vcpu *vcpu,
 }
 
 /* Requires the VCPU's ap_list_lock to be held. */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|937| <<kvm_vgic_flush_hwstate>> vgic_flush_lr_state(vcpu);
+ */
 static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
@@ -813,6 +887,28 @@ static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 
 	count = 0;
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|160| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|305| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|410| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|653| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|725| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|792| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|824| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|850| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|890| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|929| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|935| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|999| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *
+	 * List of IRQs that this VCPU should consider because they are either
+	 * Active or Pending (hence the name; AP list), or because they recently
+	 * were one of the two and need to be migrated off this list to another
+	 * VCPU.
+	 */
 	list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
 		raw_spin_lock(&irq->irq_lock);
 
@@ -855,6 +951,11 @@ static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 		vcpu->arch.vgic_cpu.vgic_v3.used_lrs = count;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|953| <<kvm_vgic_sync_hwstate>> if (can_access_vgic_from_kernel())
+ *   - arch/arm64/kvm/vgic/vgic.c|1009| <<kvm_vgic_flush_hwstate>> if (can_access_vgic_from_kernel())
+ */
 static inline bool can_access_vgic_from_kernel(void)
 {
 	/*
@@ -873,6 +974,11 @@ static inline void vgic_save_state(struct kvm_vcpu *vcpu)
 		__vgic_v3_save_state(&vcpu->arch.vgic_cpu.vgic_v3);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1019| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_sync_hwstate(vcpu);
+ *   - arch/arm64/kvm/arm.c|1056| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_sync_hwstate(vcpu);
+ */
 /* Sync back the hardware VGIC state into our emulation after a guest's run. */
 void kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)
 {
@@ -895,6 +1001,10 @@ void kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)
 	vgic_prune_ap_list(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|1010| <<kvm_vgic_flush_hwstate>> vgic_restore_state(vcpu);
+ */
 static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 {
 	if (!static_branch_unlikely(&kvm_vgic_global_state.gicv3_cpuif))
@@ -903,6 +1013,15 @@ static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 		__vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|988| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_flush_hwstate(vcpu);
+ *
+ * 对于gicv3核心思想分为两步. (还有gicv4的)
+ *
+ * 1. 对于gicv3, 写入vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+ * 2. 对于gicv3, 把vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr]写入寄存器
+ */
 /* Flush our emulation state into the GIC hardware before entering the guest. */
 void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 {
@@ -926,10 +1045,19 @@ void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 
 	if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
 		raw_spin_lock(&vcpu->arch.vgic_cpu.ap_list_lock);
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/vgic/vgic.c|937| <<kvm_vgic_flush_hwstate>> vgic_flush_lr_state(vcpu);
+		 *
+		 * 对于gicv3, 写入vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+		 */
 		vgic_flush_lr_state(vcpu);
 		raw_spin_unlock(&vcpu->arch.vgic_cpu.ap_list_lock);
 	}
 
+	/*
+	 * 对于gicv3, 把vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr]写入寄存器
+	 */
 	if (can_access_vgic_from_kernel())
 		vgic_restore_state(vcpu);
 
@@ -937,6 +1065,10 @@ void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 		vgic_v4_commit(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|464| <<kvm_arch_vcpu_load>> kvm_vgic_load(vcpu);
+ */
 void kvm_vgic_load(struct kvm_vcpu *vcpu)
 {
 	if (unlikely(!vgic_initialized(vcpu->kvm)))
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 185f902e5..bba3650c5 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1725,6 +1725,11 @@ void perf_events_lapic_init(void)
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
 }
 
+/*
+ * called by:
+ *   - arch/sparc/kernel/perf_event.c|1683| <<global>> .notifier_call = perf_event_nmi_handler,
+ *   - arch/x86/events/core.c|2102| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ */
 static int
 perf_event_nmi_handler(unsigned int cmd, struct pt_regs *regs)
 {
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 70d139406..6d9a74051 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -79,8 +79,38 @@
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
+/*
+ * 在以下使用KVM_REQ_NMI:
+ *   - arch/x86/kvm/x86.c|821| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|5301| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10615| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+ *   - arch/x86/kvm/x86.c|12866| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *   - arch/x86/kvm/x86.c|12919| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *
+ * process_nmi()
+ */
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *
+ * kvm_pmu_handle_event()
+ */
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|120| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8362| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10619| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *   - arch/x86/kvm/x86.c|12878| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+ *
+ * kvm_pmu_deliver_pmi()
+ */
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #ifdef CONFIG_KVM_SMM
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
@@ -493,6 +523,15 @@ struct kvm_pmc {
 	bool is_paused;
 	bool intr;
 	u64 counter;
+	/*
+	 * 在以下使用kvm_pmu->prev_counter:
+	 *   - arch/x86/kvm/pmu.c|424| <<reprogram_counter>> if (pmc->counter < pmc->prev_counter)
+	 *   - arch/x86/kvm/pmu.c|464| <<reprogram_counter>> pmc->prev_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|759| <<kvm_pmu_incr_counter>> pmc->prev_counter = pmc->counter;
+	 *   - arch/x86/kvm/svm/pmu.c|245| <<amd_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|643| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|650| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = 0;
+	 */
 	u64 prev_counter;
 	u64 eventsel;
 	struct perf_event *perf_event;
@@ -560,6 +599,13 @@ struct kvm_pmu {
 	 * The gate to release perf_events not marked in
 	 * pmc_in_use only once in a vcpu time slice.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|488| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|723| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|735| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12399| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 */
 	bool need_cleanup;
 
 	/*
@@ -695,6 +741,16 @@ struct kvm_vcpu_xen {
 	u32 vcpu_id; /* The Xen / ACPI vCPU ID */
 	u32 timer_virq;
 	u64 timer_expires; /* In guest epoch */
+	/*
+	 * 在以下使用kvm_vcpu_xen->timer_pending:
+	 *   - arch/x86/kvm/xen.c|139| <<kvm_xen_inject_timer_irqs>> if (atomic_read(&vcpu->arch.xen.timer_pending) > 0) {
+	 *   - arch/x86/kvm/xen.c|150| <<kvm_xen_inject_timer_irqs>> atomic_set(&vcpu->arch.xen.timer_pending, 0);
+	 *   - arch/x86/kvm/xen.c|158| <<xen_timer_callback>> if (atomic_read(&vcpu->arch.xen.timer_pending))
+	 *   - arch/x86/kvm/xen.c|161| <<xen_timer_callback>> atomic_inc(&vcpu->arch.xen.timer_pending);
+	 *   - arch/x86/kvm/xen.c|170| <<kvm_xen_start_timer>> atomic_set(&vcpu->arch.xen.timer_pending, 0);
+	 *   - arch/x86/kvm/xen.c|187| <<kvm_xen_stop_timer>> atomic_set(&vcpu->arch.xen.timer_pending, 0);
+	 *   - arch/x86/kvm/xen.h|76| <<kvm_xen_has_pending_timer>> return atomic_read(&vcpu->arch.xen.timer_pending);
+	 */
 	atomic_t timer_pending;
 	struct hrtimer timer;
 	int poll_evtchn;
@@ -880,7 +936,19 @@ struct kvm_vcpu_arch {
 	u64 this_tsc_nsec;
 	u64 this_tsc_write;
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2460| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3245| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4892| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2461| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|9746| <<kvm_pv_clock_pairing>> if (vcpu->arch.tsc_always_catchup)
+	 *   - arch/x86/kvm/x86.c|10997| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;
 	u32 virtual_tsc_mult;
@@ -890,7 +958,37 @@ struct kvm_vcpu_arch {
 	u64 l1_tsc_scaling_ratio;
 	u64 tsc_scaling_ratio; /* current scaling ratio */
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|825| <<kvm_inject_nmi>> atomic_inc(&vcpu->arch.nmi_queued);
+	 *   - arch/x86/kvm/x86.c|5315| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
+	 *   - arch/x86/kvm/x86.c|10312| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+	 *   - arch/x86/kvm/x86.c|12110| <<kvm_vcpu_reset>> atomic_set(&vcpu->arch.nmi_queued, 0);
+	 */
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
+	/*
+	 * 在以下使用kvm_vcpu_arch->nmi_pending:
+	 *   - arch/x86/kvm/svm/nested.c|671| <<nested_vmcb02_prepare_control>> svm->vcpu.arch.nmi_pending++;
+	 *   - arch/x86/kvm/svm/nested.c|1092| <<nested_svm_vmexit>> if (vcpu->arch.nmi_pending) {
+	 *   - arch/x86/kvm/svm/nested.c|1093| <<nested_svm_vmexit>> vcpu->arch.nmi_pending--;
+	 *   - arch/x86/kvm/svm/nested.c|1487| <<svm_check_nested_events>> if (vcpu->arch.nmi_pending && !svm_nmi_blocked(vcpu)) {
+	 *   - arch/x86/kvm/svm/svm.c|2438| <<svm_set_gif>> svm->vcpu.arch.nmi_pending ||
+	 *   - arch/x86/kvm/vmx/nested.c|4162| <<vmx_check_nested_events>> if (vcpu->arch.nmi_pending && !vmx_nmi_blocked(vcpu)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4175| <<vmx_check_nested_events>> vcpu->arch.nmi_pending = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6528| <<__vmx_handle_exit>> vcpu->arch.nmi_pending) {
+	 *   - arch/x86/kvm/x86.c|5314| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.nmi_pending = 0;
+	 *   - arch/x86/kvm/x86.c|10222| <<kvm_check_and_inject_events>> if (vcpu->arch.nmi_pending) {
+	 *   - arch/x86/kvm/x86.c|10227| <<kvm_check_and_inject_events>> --vcpu->arch.nmi_pending;
+	 *   - arch/x86/kvm/x86.c|10233| <<kvm_check_and_inject_events>> if (vcpu->arch.nmi_pending)
+	 *   - arch/x86/kvm/x86.c|10309| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+	 *   - arch/x86/kvm/x86.c|10310| <<process_nmi>> vcpu->arch.nmi_pending = min(vcpu->arch.nmi_pending, limit);
+	 *   - arch/x86/kvm/x86.c|10312| <<process_nmi>> if (vcpu->arch.nmi_pending &&
+	 *   - arch/x86/kvm/x86.c|10314| <<process_nmi>> vcpu->arch.nmi_pending--;
+	 *   - arch/x86/kvm/x86.c|10316| <<process_nmi>> if (vcpu->arch.nmi_pending)
+	 *   - arch/x86/kvm/x86.c|10323| <<kvm_get_nr_pending_nmis>> return vcpu->arch.nmi_pending +
+	 *   - arch/x86/kvm/x86.c|12098| <<kvm_vcpu_reset>> vcpu->arch.nmi_pending = 0;
+	 *   - arch/x86/kvm/x86.c|12907| <<kvm_vcpu_has_events>> (vcpu->arch.nmi_pending &&
+	 */
 	/* Number of NMIs pending injection, not including hardware vNMIs. */
 	unsigned int nmi_pending;
 	bool nmi_injected;    /* Trying to inject an NMI this entry */
@@ -1131,8 +1229,35 @@ struct kvm_xen {
 	u32 xen_version;
 	bool long_mode;
 	bool runstate_update_flag;
+	/*
+	 * 在以下使用kvm_xen->upcall_vector:
+	 *   - arch/x86/kvm/irq.c|122| <<kvm_cpu_get_extint>> return v->kvm->arch.xen.upcall_vector;
+	 *   - arch/x86/kvm/xen.c|632| <<kvm_xen_hvm_set_attr>> kvm->arch.xen.upcall_vector = data->u.vector;
+	 *   - arch/x86/kvm/xen.c|688| <<kvm_xen_hvm_get_attr>> data->u.vector = kvm->arch.xen.upcall_vector;
+	 *   - arch/x86/kvm/xen.h|56| <<kvm_xen_has_interrupt>> vcpu->kvm->arch.xen.upcall_vector)
+	 *
+	 * 在以下使用kvm_vcpu_xen->upcall_vector:
+	 *   - arch/x86/kvm/xen.c|480| <<kvm_xen_inject_vcpu_vector>> irq.vector = v->arch.xen.upcall_vector;
+	 *   - arch/x86/kvm/xen.c|549| <<kvm_xen_inject_pending_events>> if (v->arch.xen.upcall_vector)
+	 *   - arch/x86/kvm/xen.c|936| <<kvm_xen_vcpu_set_attr>> vcpu->arch.xen.upcall_vector = data->u.vector;
+	 *   - arch/x86/kvm/xen.c|1029| <<kvm_xen_vcpu_get_attr>> data->u.vector = vcpu->arch.xen.upcall_vector;
+	 *   - arch/x86/kvm/xen.c|1643| <<kvm_xen_set_evtchn_fast>> if (kick_vcpu && vcpu->arch.xen.upcall_vector) {
+	 */
 	u8 upcall_vector;
 	struct gfn_to_pfn_cache shinfo_cache;
+	/*
+	 * 在以下使用kvm_xen->evtchn_ports:
+	 *   - arch/x86/kvm/xen.c|1828| <<kvm_xen_eventfd_update>> evtchnfd = idr_find(&kvm->arch.xen.evtchn_ports, port);
+	 *   - arch/x86/kvm/xen.c|1922| <<kvm_xen_eventfd_assign>> ret = idr_alloc(&kvm->arch.xen.evtchn_ports, evtchnfd, port, port + 1,
+	 *   - arch/x86/kvm/xen.c|1943| <<kvm_xen_eventfd_deassign>> evtchnfd = idr_remove(&kvm->arch.xen.evtchn_ports, port);
+	 *   - arch/x86/kvm/xen.c|1969| <<kvm_xen_eventfd_reset>> idr_for_each_entry(&kvm->arch.xen.evtchn_ports, evtchnfd, i)
+	 *   - arch/x86/kvm/xen.c|1979| <<kvm_xen_eventfd_reset>> idr_for_each_entry(&kvm->arch.xen.evtchn_ports, evtchnfd, i) {
+	 *   - arch/x86/kvm/xen.c|1981| <<kvm_xen_eventfd_reset>> idr_remove(&kvm->arch.xen.evtchn_ports, evtchnfd->send_port);
+	 *   - arch/x86/kvm/xen.c|2036| <<kvm_xen_hcall_evtchn_send>> evtchnfd = idr_find(&vcpu->kvm->arch.xen.evtchn_ports, send.port);
+	 *   - arch/x86/kvm/xen.c|2109| <<kvm_xen_init_vm>> idr_init(&kvm->arch.xen.evtchn_ports);
+	 *   - arch/x86/kvm/xen.c|2120| <<kvm_xen_destroy_vm>> idr_for_each_entry(&kvm->arch.xen.evtchn_ports, evtchnfd, i) {
+	 *   - arch/x86/kvm/xen.c|2125| <<kvm_xen_destroy_vm>> idr_destroy(&kvm->arch.xen.evtchn_ports);
+	 */
 	struct idr evtchn_ports;
 	unsigned long poll_mask[BITS_TO_LONGS(KVM_MAX_VCPUS)];
 };
@@ -1359,6 +1484,11 @@ struct kvm_arch {
 	bool triple_fault_event;
 
 	bool bus_lock_detection_enabled;
+	/*
+	 * 在以下设置kvm_arch->enable_pmu:
+	 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap>> kvm->arch.enable_pmu = !(cap->args[0] & KVM_PMU_CAP_DISABLE);
+	 *   - arch/x86/kvm/x86.c|12492| <<kvm_arch_init_vm>> kvm->arch.enable_pmu = enable_pmu;
+	 */
 	bool enable_pmu;
 
 	u32 notify_window;
diff --git a/arch/x86/include/asm/nmi.h b/arch/x86/include/asm/nmi.h
index 5c5f1e56c..4f5176308 100644
--- a/arch/x86/include/asm/nmi.h
+++ b/arch/x86/include/asm/nmi.h
@@ -44,6 +44,28 @@ struct nmiaction {
 	const char		*name;
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|1246| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2102| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|56| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|773| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/cpu/mshyperv.c|487| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/reboot.c|912| <<nmi_shootdown_cpus>> if (register_nmi_handler(NMI_LOCAL, crash_nmi_callback,
+ *   - arch/x86/kernel/smp.c|145| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/platform/uv/uv_nmi.c|1022| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - arch/x86/platform/uv/uv_nmi.c|1025| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1199| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ *   - drivers/watchdog/hpwdt.c|249| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|252| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|255| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
 #define register_nmi_handler(t, fn, fg, n, init...)	\
 ({							\
 	static struct nmiaction init fn##_na = {	\
diff --git a/arch/x86/kernel/apic/hw_nmi.c b/arch/x86/kernel/apic/hw_nmi.c
index 45af535c4..851ce3270 100644
--- a/arch/x86/kernel/apic/hw_nmi.c
+++ b/arch/x86/kernel/apic/hw_nmi.c
@@ -51,6 +51,10 @@ static int nmi_cpu_backtrace_handler(unsigned int cmd, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(nmi_cpu_backtrace_handler);
 
+/*
+ * 在以下使用register_nmi_cpu_backtrace_handler():
+ *   - arch/x86/kernel/apic/hw_nmi.c|60| <<global>> early_initcall(register_nmi_cpu_backtrace_handler);
+ */
 static int __init register_nmi_cpu_backtrace_handler(void)
 {
 	register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
@@ -59,3 +63,26 @@ static int __init register_nmi_cpu_backtrace_handler(void)
 }
 early_initcall(register_nmi_cpu_backtrace_handler);
 #endif
+
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|1246| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2102| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|56| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|773| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/cpu/mshyperv.c|487| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/reboot.c|912| <<nmi_shootdown_cpus>> if (register_nmi_handler(NMI_LOCAL, crash_nmi_callback,
+ *   - arch/x86/kernel/smp.c|145| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/platform/uv/uv_nmi.c|1022| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - arch/x86/platform/uv/uv_nmi.c|1025| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1199| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ *   - drivers/watchdog/hpwdt.c|249| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|252| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|255| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index 773132c3b..2cf59ccf9 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -321,6 +321,10 @@ static bool kvm_cpuid_has_hyperv(struct kvm_cpuid_entry2 *entries, int nent)
 	return entry && entry->eax == HYPERV_CPUID_SIGNATURE_EAX;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|452| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index b2c397dd2..f79a0e2a1 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -143,6 +143,10 @@ int kvm_cpu_get_interrupt(struct kvm_vcpu *v)
 }
 EXPORT_SYMBOL_GPL(kvm_cpu_get_interrupt);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11087| <<vcpu_run>> kvm_inject_pending_timer_irqs(vcpu);
+ */
 void kvm_inject_pending_timer_irqs(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu))
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 16d076a1b..608c2a38d 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -155,6 +155,10 @@ static int kvm_hv_set_sint(struct kvm_kernel_irq_routing_entry *e,
 	return kvm_hv_synic_set_irq(kvm, e->hv_sint.vcpu, e->hv_sint.sint);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|247| <<irqfd_wakeup>> if (kvm_arch_set_irq_inatomic(&irq, kvm, * KVM_USERSPACE_IRQ_SOURCE_ID, 1, false) == -EWOULDBLOCK)
+ */
 int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 			      struct kvm *kvm, int irq_source_id, int level,
 			      bool line_status)
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 3e977dbbf..e17cd891c 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -817,6 +817,17 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|839| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|2161| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq_comm.c|77| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq_comm.c|99| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|842| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1221| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|1234| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/x86.c|13185| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
@@ -1202,6 +1213,12 @@ static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|55| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+ *   - arch/x86/kvm/irq_comm.c|176| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+ *   - arch/x86/kvm/xen.c|532| <<kvm_xen_inject_vcpu_vector>> WARN_ON_ONCE(!kvm_irq_delivery_to_apic_fast(v->kvm, NULL, &irq, &r, NULL));
+ */
 bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)
 {
@@ -1285,6 +1302,11 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
  * Add a pending IRQ into lapic.
  * Return 1 if successfully added and 0 if discarded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|825| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector, irq->level, irq->trig_mode, dest_map);
+ *   - arch/x86/kvm/lapic.c|2769| <<kvm_apic_local_deliver>> r = __apic_accept_irq(apic, mode, vector, 1, trig_mode, NULL);
+ */
 static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map)
@@ -1322,6 +1344,10 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 						       apic->regs + APIC_TMR);
 		}
 
+		/*
+		 * vmx_deliver_interrupt()
+		 * svm_deliver_interrupt()
+		 */
 		static_call(kvm_x86_deliver_interrupt)(apic, delivery_mode,
 						       trig_mode, vector);
 		break;
@@ -1342,6 +1368,11 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 
 	case APIC_DM_NMI:
 		result = 1;
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/lapic.c|1345| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+		 *   - arch/x86/kvm/x86.c|5009| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+		 */
 		kvm_inject_nmi(vcpu);
 		kvm_vcpu_kick(vcpu);
 		break;
@@ -1790,6 +1821,10 @@ static bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1885| <<__kvm_wait_lapic_expire>> __wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);
+ */
 static inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)
 {
 	u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
@@ -1864,6 +1899,11 @@ static void __kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 		__wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4178| <<svm_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7361| <<vmx_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+ */
 void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu) &&
@@ -2755,6 +2795,13 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1881| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2782| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|531| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ *   - arch/x86/kvm/x86.c|5084| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -2766,6 +2813,11 @@ int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 		mode = reg & APIC_MODE_MASK;
 		trig_mode = reg & APIC_LVT_LEVEL_TRIGGER;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/lapic.c|825| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector, irq->level, irq->trig_mode, dest_map);
+		 *   - arch/x86/kvm/lapic.c|2769| <<kvm_apic_local_deliver>> r = __apic_accept_irq(apic, mode, vector, 1, trig_mode, NULL);
+		 */
 		r = __apic_accept_irq(apic, mode, vector, 1, trig_mode, NULL);
 		if (r && lvt_type == APIC_LVTPC)
 			kvm_lapic_set_reg(apic, APIC_LVTPC, reg | APIC_LVT_MASKED);
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 0a0ea4b5d..8ee5aaf70 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -17,6 +17,10 @@
 #define APIC_DEST_MASK			0x800
 
 #define APIC_BUS_CYCLE_NS       1
+/*
+ * 只在下面使用APIC_BUS_FREQUENCY:
+ *   - arch/x86/kvm/hyperv.c|1688| <<kvm_hv_get_msr>> data = APIC_BUS_FREQUENCY;
+ */
 #define APIC_BUS_FREQUENCY      (1000000000ULL / APIC_BUS_CYCLE_NS)
 
 #define APIC_BROADCAST			0xFF
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 9ae07db6f..c3cbc737d 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -80,6 +80,10 @@ static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;
 #define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
 #include <asm/kvm-x86-pmu-ops.h>
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9476| <<kvm_ops_update>> kvm_pmu_ops_update(ops->pmu_ops);
+ */
 void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 {
 	memcpy(&kvm_pmu_ops, pmu_ops, sizeof(kvm_pmu_ops));
@@ -93,6 +97,15 @@ void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 #undef __KVM_X86_PMU_OP
 }
 
+/*
+ * vcpu_enter_guest(KVM_REQ_PMU)
+ * -> kvm_pmu_handle_event()
+ *    -> reprogram_counter()
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|403| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -116,10 +129,23 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		__set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
 	}
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|120| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8362| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10619| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|12878| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * kvm_pmu_deliver_pmi()
+	 */
 	if (pmc->intr && !skip_pmi)
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
 }
 
+/*
+ * 在以下使用kvm_perf_overflow():
+ *   - arch/x86/kvm/pmu.c|213| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -134,8 +160,22 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 	if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+	 *   - arch/x86/kvm/pmu.c|403| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+	 */
 	__kvm_perf_overflow(pmc, true);
 
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
@@ -215,6 +255,10 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|419| <<reprogram_counter>> pmc_pause_counter(pmc);
+ */
 static void pmc_pause_counter(struct kvm_pmc *pmc)
 {
 	u64 counter = pmc->counter;
@@ -378,6 +422,14 @@ static bool pmc_event_is_allowed(struct kvm_pmc *pmc)
 	       check_pmu_event_filter(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|458| <<kvm_pmu_handle_event>> reprogram_counter(pmc);
+ *
+ * vcpu_enter_guest(KVM_REQ_PMU)
+ * -> kvm_pmu_handle_event()
+ *    -> reprogram_counter()
+ */
 static void reprogram_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -433,6 +485,10 @@ static void reprogram_counter(struct kvm_pmc *pmc)
 	pmc->prev_counter = 0;
 }
 
+/*
+ * called by(KVM_REQ_PMU):
+ *   - arch/x86/kvm/x86.c|10717| <<vcpu_enter_guest>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -524,10 +580,35 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|120| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8362| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10619| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *   - arch/x86/kvm/x86.c|12878| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|10620| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
+		/*
+		 * svm似乎没有intel_pmu_deliver_pmi()
+		 */
 		static_call_cond(kvm_x86_pmu_deliver_pmi)(vcpu);
+		/*
+		 * LVT Performance Counter Register (FEE0 0340H) -
+		 * Specifies interrupt delivery when a performance counter
+		 * generates an interrupt on overflow (see Section 19.6.3.5.8,
+		 * "Generating an Interrupt on Overflow") or when Intel PT
+		 * signals a ToPA PMI (see Section 32.2.7.2). This LVT entry
+		 * is implementation specific, not architectural. If
+		 * implemented, it is not guaranteed to be at base address
+		 * FEE0 0340H.
+		 *
+		 * APIC_DM_NMI
+		 */
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 	}
 }
@@ -580,6 +661,11 @@ int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3949| <<kvm_set_msr_common>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|4030| <<kvm_set_msr_common>> return kvm_pmu_set_msr(vcpu, msr_info);
+ */
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -643,8 +729,21 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  * settings are changed (such as changes of PMU CPUID by guest VMs), which
  * should rarely happen.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|368| <<kvm_vcpu_after_set_cpuid>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/pmu.c|724| <<kvm_pmu_init>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/x86.c|3686| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> kvm_pmu_refresh(vcpu);
+ */
 void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/cpuid.c|427| <<kvm_set_cpuid>> if (kvm_vcpu_has_run(vcpu)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5444| <<kvm_mmu_after_set_cpuid>> KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm);
+	 *   - arch/x86/kvm/pmu.c|704| <<kvm_pmu_refresh>> if (KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|2250| <<do_set_msr>> if (kvm_vcpu_has_run(vcpu) && kvm_is_immutable_feature_msr(index)) {
+	 */
 	if (KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm))
 		return;
 
@@ -652,11 +751,20 @@ void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 	static_call(kvm_x86_pmu_refresh)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|754| <<kvm_pmu_destroy>> kvm_pmu_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|12190| <<kvm_vcpu_reset>> kvm_pmu_reset(vcpu);
+ */
 void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	static_call(kvm_x86_pmu_reset)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12059| <<kvm_arch_vcpu_create>> kvm_pmu_init(vcpu);
+ */
 void kvm_pmu_init(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -698,8 +806,21 @@ void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
 	kvm_pmu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|804| <<kvm_pmu_trigger_event>> kvm_pmu_incr_counter(pmc);
+ */
 static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 {
+	/*
+	 * 在以下使用kvm_pmu->prev_counter:
+	 *   - arch/x86/kvm/pmu.c|424| <<reprogram_counter>> if (pmc->counter < pmc->prev_counter)
+	 *   - arch/x86/kvm/pmu.c|464| <<reprogram_counter>> pmc->prev_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|759| <<kvm_pmu_incr_counter>> pmc->prev_counter = pmc->counter;
+	 *   - arch/x86/kvm/svm/pmu.c|245| <<amd_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|643| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|650| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = 0;
+	 */
 	pmc->prev_counter = pmc->counter;
 	pmc->counter = (pmc->counter + 1) & pmc_bitmask(pmc);
 	kvm_pmu_request_counter_reprogram(pmc);
@@ -731,6 +852,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3567| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8752| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|9059| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|9061| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 1d64113de..5d378ddcd 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -74,6 +74,12 @@ static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 	return counter & pmc_bitmask(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|163| <<amd_pmu_set_msr>> pmc_write_counter(pmc, data);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|439| <<intel_pmu_set_msr>> pmc_write_counter(pmc, data);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|443| <<intel_pmu_set_msr>> pmc_write_counter(pmc, data);
+ */
 static inline void pmc_write_counter(struct kvm_pmc *pmc, u64 val)
 {
 	pmc->counter += val - pmc_read_counter(pmc);
diff --git a/arch/x86/kvm/svm/avic.c b/arch/x86/kvm/svm/avic.c
index 4b74ea91f..342ba2a46 100644
--- a/arch/x86/kvm/svm/avic.c
+++ b/arch/x86/kvm/svm/avic.c
@@ -318,6 +318,10 @@ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3666| <<svm_complete_interrupt_delivery>> avic_ring_doorbell(vcpu);
+ */
 void avic_ring_doorbell(struct kvm_vcpu *vcpu)
 {
 	/*
diff --git a/arch/x86/kvm/svm/nested.c b/arch/x86/kvm/svm/nested.c
index 3fea8c476..0bb8ff041 100644
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@ -795,6 +795,11 @@ static void nested_svm_copy_common_state(struct vmcb *from_vmcb, struct vmcb *to
 	to_vmcb->save.spec_ctrl = from_vmcb->save.spec_ctrl;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|919| <<nested_svm_vmrun>> if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
+ *   - arch/x86/kvm/svm/svm.c|4702| <<svm_leave_smm>> ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
+ */
 int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 			 struct vmcb *vmcb12, bool from_vmrun)
 {
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index beea99c8e..dd6783648 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -4108,6 +4108,10 @@ static fastpath_t svm_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
 	return EXIT_FASTPATH_NONE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4189| <<svm_vcpu_run>> svm_vcpu_enter_exit(vcpu, spec_ctrl_intercepted);
+ */
 static noinstr void svm_vcpu_enter_exit(struct kvm_vcpu *vcpu, bool spec_ctrl_intercepted)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4116,6 +4120,9 @@ static noinstr void svm_vcpu_enter_exit(struct kvm_vcpu *vcpu, bool spec_ctrl_in
 
 	amd_clear_divider();
 
+	/*
+	 * arch/x86/kvm/svm/vmenter.S
+	 */
 	if (sev_es_guest(vcpu->kvm))
 		__svm_sev_es_vcpu_run(svm, spec_ctrl_intercepted);
 	else
@@ -4175,6 +4182,11 @@ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu)
 	clgi();
 	kvm_load_guest_xsave_state(vcpu);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/svm/svm.c|4178| <<svm_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7361| <<vmx_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+	 */
 	kvm_wait_lapic_expire(vcpu);
 
 	/*
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 41cce5031..e6a6a13b0 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -815,9 +815,24 @@ void kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
 }
 EXPORT_SYMBOL_GPL(kvm_inject_emulated_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1345| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|5009| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+ */
 void kvm_inject_nmi(struct kvm_vcpu *vcpu)
 {
 	atomic_inc(&vcpu->arch.nmi_queued);
+	/*
+	 * 在以下使用KVM_REQ_NMI:
+	 *   - arch/x86/kvm/x86.c|821| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|5301| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10615| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|12866| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *   - arch/x86/kvm/x86.c|12919| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *
+	 * process_nmi()
+	 */
 	kvm_make_request(KVM_REQ_NMI, vcpu);
 }
 
@@ -3039,6 +3054,10 @@ static unsigned long get_cpu_tsc_khz(void)
 		return __this_cpu_read(cpu_tsc_khz);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3100| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ */
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
@@ -3075,6 +3094,11 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	put_cpu();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3108| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+ *   - arch/x86/kvm/x86.c|6737| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+ */
 static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -3086,14 +3110,37 @@ static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	} while (read_seqcount_retry(&ka->pvclock_sc, seq));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|579| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|2354| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|79| <<kvm_xen_shared_info_init>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|480| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|881| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|922| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|974| <<kvm_xen_vcpu_set_attr>> get_kvmclock_ns(vcpu->kvm));
+ *   - arch/x86/kvm/xen.c|1436| <<kvm_xen_hcall_vcpu_op>> delta = oneshot.timeout_abs_ns - get_kvmclock_ns(vcpu->kvm);
+ *   - arch/x86/kvm/xen.c|1466| <<kvm_xen_hcall_set_timer_op>> uint64_t guest_now = get_kvmclock_ns(vcpu->kvm);
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_clock_data data;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|3108| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+	 *   - arch/x86/kvm/x86.c|6737| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+	 */
 	get_kvmclock(kvm, &data);
 	return data.clock;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3275| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
+ *   - arch/x86/kvm/x86.c|3277| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_info_cache,
+ *   - arch/x86/kvm/x86.c|3280| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_time_info_cache, 0);
+ */
 static void kvm_setup_guest_pvclock(struct kvm_vcpu *v,
 				    struct gfn_to_pfn_cache *gpc,
 				    unsigned int offset)
@@ -4801,6 +4848,14 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/emulate-nested.c|1947| <<kvm_emulate_nested_eret>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - arch/arm64/kvm/emulate-nested.c|2028| <<kvm_inject_nested>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - arch/arm64/kvm/reset.c|300| <<kvm_reset_vcpu>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - virt/kvm/kvm_main.c|216| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5963| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Address WBINVD may be executed by guest */
@@ -10018,6 +10073,9 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
 				vcpu->arch.exception.error_code,
 				vcpu->arch.exception.injected);
 
+	/*
+	 * vmx_inject_exception()
+	 */
 	static_call(kvm_x86_inject_exception)(vcpu);
 }
 
@@ -10060,6 +10118,10 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
  * ordering between that side effect, the instruction completing, _and_ the
  * delivery of the asynchronous event.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10736| <<vcpu_enter_guest(KVM_REQ_EVENT)>> r = kvm_check_and_inject_events(vcpu, &req_immediate_exit);
+ */
 static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 				       bool *req_immediate_exit)
 {
@@ -10211,6 +10273,10 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 		if (r) {
 			--vcpu->arch.nmi_pending;
 			vcpu->arch.nmi_injected = true;
+			/*
+			 * vmx_inject_nmi
+			 * svm_inject_nmi
+			 */
 			static_call(kvm_x86_inject_nmi)(vcpu);
 			can_inject = false;
 			WARN_ON(static_call(kvm_x86_nmi_allowed)(vcpu, true) < 0);
@@ -10265,6 +10331,20 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+/*
+ * 在以下使用KVM_REQ_NMI:
+ *   - arch/x86/kvm/x86.c|821| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|5301| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10615| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+ *   - arch/x86/kvm/x86.c|12866| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *   - arch/x86/kvm/x86.c|12919| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|5162| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> process_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|5283| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> process_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|10644| <<vcpu_enter_guest(KVM_REQ_NMI)>> process_nmi(vcpu);
+ */
 static void process_nmi(struct kvm_vcpu *vcpu)
 {
 	unsigned int limit;
@@ -10492,6 +10572,27 @@ void __kvm_request_immediate_exit(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);
 
+/*
+ * 4.14上handle_external_intr()的例子.
+ *
+ * [0] bnxt_rx_pages
+ * [0] bnxt_rx_pkt
+ * [0] __bnxt_poll_work
+ * [0] bnxt_poll
+ * [0] net_rx_action
+ * [0] __softirqentry_text_start
+ * [0] irq_exit
+ * [0] do_IRQ
+ * --- <IRQ stack> ---
+ * [0] ret_from_intr
+ * [0] vmx_handle_external_intr
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] __dta_kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 /*
  * Called within kvm->srcu read side.
  * Returns 1 to let vcpu_run() continue the guest execution loop without
@@ -10591,10 +10692,36 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		if (kvm_check_request(KVM_REQ_SMI, vcpu))
 			process_smi(vcpu);
 #endif
+		/*
+		 * 在以下使用KVM_REQ_NMI:
+		 *   - arch/x86/kvm/x86.c|821| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|5301| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|10615| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+		 *   - arch/x86/kvm/x86.c|12866| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *   - arch/x86/kvm/x86.c|12919| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *
+		 * process_nmi()
+		 */
 		if (kvm_check_request(KVM_REQ_NMI, vcpu))
 			process_nmi(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 */
 		if (kvm_check_request(KVM_REQ_PMU, vcpu))
 			kvm_pmu_handle_event(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMI:
+		 *   - arch/x86/kvm/pmu.c|120| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|8362| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|10619| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+		 *   - arch/x86/kvm/x86.c|12878| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+		 */
 		if (kvm_check_request(KVM_REQ_PMI, vcpu))
 			kvm_pmu_deliver_pmi(vcpu);
 		if (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {
@@ -10655,6 +10782,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			static_call(kvm_x86_update_cpu_dirty_logging)(vcpu);
 	}
 
+	/*
+	 * 非常非常重要的KVM_REQ_EVENT!!!
+	 */
 	if (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win ||
 	    kvm_xen_has_interrupt(vcpu)) {
 		++vcpu->stat.req_event;
@@ -10771,6 +10901,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
 			     (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));
 
+		/*
+		 * vmx_vcpu_run()
+		 * svm_vcpu_run()
+		 */
 		exit_fastpath = static_call(kvm_x86_vcpu_run)(vcpu);
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
@@ -11099,6 +11233,10 @@ static void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 	trace_kvm_fpu(0);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4161| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_queued_exception *ex = &vcpu->arch.exception;
@@ -12295,6 +12433,15 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 	vcpu->arch.l1tf_flush_l1d = true;
 	if (pmu->version && unlikely(pmu->event_count)) {
 		pmu->need_cleanup = true;
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 */
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
 	static_call(kvm_x86_sched_in)(vcpu, cpu);
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 1e7be1f6a..af054a41b 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -92,6 +92,13 @@ static inline unsigned int __shrink_ple_window(unsigned int val,
 void kvm_service_local_tlb_flush_requests(struct kvm_vcpu *vcpu);
 int kvm_check_nested_events(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|427| <<kvm_set_cpuid>> if (kvm_vcpu_has_run(vcpu)) {
+ *   - arch/x86/kvm/mmu/mmu.c|5444| <<kvm_mmu_after_set_cpuid>> KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm);
+ *   - arch/x86/kvm/pmu.c|704| <<kvm_pmu_refresh>> if (KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|2250| <<do_set_msr>> if (kvm_vcpu_has_run(vcpu) && kvm_is_immutable_feature_msr(index)) {
+ */
 static inline bool kvm_vcpu_has_run(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.last_vmentry_cpu != -1;
diff --git a/arch/x86/kvm/xen.c b/arch/x86/kvm/xen.c
index 40edf4d19..53e70c700 100644
--- a/arch/x86/kvm/xen.c
+++ b/arch/x86/kvm/xen.c
@@ -32,8 +32,25 @@ static int kvm_xen_set_evtchn(struct kvm_xen_evtchn *xe, struct kvm *kvm);
 static int kvm_xen_setattr_evtchn(struct kvm *kvm, struct kvm_xen_hvm_attr *data);
 static bool kvm_xen_hcall_evtchn_send(struct kvm_vcpu *vcpu, u64 param, u64 *r);
 
+/*
+ * 在以下使用kvm_xen_enabled:
+ *   - arch/x86/kvm/xen.c|35| <<global>> DEFINE_STATIC_KEY_DEFERRED_FALSE(kvm_xen_enabled, HZ);
+ *   - arch/x86/kvm/x86.c|9621| <<kvm_x86_vendor_exit>> static_key_deferred_flush(&kvm_xen_enabled);
+ *   - arch/x86/kvm/x86.c|9622| <<kvm_x86_vendor_exit>> WARN_ON(static_branch_unlikely(&kvm_xen_enabled.key));
+ *   - arch/x86/kvm/xen.c|1133| <<kvm_xen_hvm_config>> static_branch_inc(&kvm_xen_enabled.key);
+ *   - arch/x86/kvm/xen.c|1135| <<kvm_xen_hvm_config>> static_branch_slow_dec_deferred(&kvm_xen_enabled);
+ *   - arch/x86/kvm/xen.c|2128| <<kvm_xen_destroy_vm>> static_branch_slow_dec_deferred(&kvm_xen_enabled);
+ *   - arch/x86/kvm/xen.h|41| <<kvm_xen_msr_enabled>> return static_branch_unlikely(&kvm_xen_enabled.key) &&
+ *   - arch/x86/kvm/xen.h|47| <<kvm_xen_hypercall_enabled>> return static_branch_unlikely(&kvm_xen_enabled.key) &&
+ *   - arch/x86/kvm/xen.h|54| <<kvm_xen_has_interrupt>> if (static_branch_unlikely(&kvm_xen_enabled.key) &&
+ *   - arch/x86/kvm/xen.h|64| <<kvm_xen_has_pending_events>> return static_branch_unlikely(&kvm_xen_enabled.key) &&
+ */
 DEFINE_STATIC_KEY_DEFERRED_FALSE(kvm_xen_enabled, HZ);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|623| <<kvm_xen_hvm_set_attr>> r = kvm_xen_shared_info_init(kvm, data->u.shared_info.gfn);
+ */
 static int kvm_xen_shared_info_init(struct kvm *kvm, gfn_t gfn)
 {
 	struct gfn_to_pfn_cache *gpc = &kvm->arch.xen.shinfo_cache;
@@ -113,6 +130,10 @@ static int kvm_xen_shared_info_init(struct kvm *kvm, gfn_t gfn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|151| <<kvm_inject_pending_timer_irqs>> kvm_xen_inject_timer_irqs(vcpu);
+ */
 void kvm_xen_inject_timer_irqs(struct kvm_vcpu *vcpu)
 {
 	if (atomic_read(&vcpu->arch.xen.timer_pending) > 0) {
@@ -144,6 +165,12 @@ static enum hrtimer_restart xen_timer_callback(struct hrtimer *timer)
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|972| <<kvm_xen_vcpu_set_attr>> kvm_xen_start_timer(vcpu, data->u.timer.expires_ns,
+ *   - arch/x86/kvm/xen.c|1442| <<kvm_xen_hcall_vcpu_op>> kvm_xen_start_timer(vcpu, oneshot.timeout_abs_ns, delta);
+ *   - arch/x86/kvm/xen.c|1484| <<kvm_xen_hcall_set_timer_op>> kvm_xen_start_timer(vcpu, timeout, delta);
+ */
 static void kvm_xen_start_timer(struct kvm_vcpu *vcpu, u64 guest_abs, s64 delta_ns)
 {
 	atomic_set(&vcpu->arch.xen.timer_pending, 0);
@@ -173,6 +200,12 @@ static void kvm_xen_init_timer(struct kvm_vcpu *vcpu)
 	vcpu->arch.xen.timer.function = xen_timer_callback;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|471| <<kvm_xen_update_runstate>> kvm_xen_update_runstate_guest(v, state == RUNSTATE_runnable);
+ *   - arch/x86/kvm/xen.c|800| <<kvm_xen_vcpu_set_attr>> kvm_xen_update_runstate_guest(vcpu, false);
+ *   - arch/x86/kvm/xen.c|896| <<kvm_xen_vcpu_set_attr>> kvm_xen_update_runstate_guest(vcpu, false);
+ */
 static void kvm_xen_update_runstate_guest(struct kvm_vcpu *v, bool atomic)
 {
 	struct kvm_vcpu_xen *vx = &v->arch.xen;
@@ -440,6 +473,13 @@ static void kvm_xen_update_runstate_guest(struct kvm_vcpu *v, bool atomic)
 		mark_page_dirty_in_slot(v->kvm, gpc2->memslot, gpc2->gpa >> PAGE_SHIFT);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|813| <<kvm_xen_vcpu_set_attr>> kvm_xen_update_runstate(vcpu, data->u.runstate.state);
+ *   - arch/x86/kvm/xen.c|894| <<kvm_xen_vcpu_set_attr>> kvm_xen_update_runstate(vcpu, data->u.runstate.state);
+ *   - arch/x86/kvm/xen.h|157| <<kvm_xen_runstate_set_running>> kvm_xen_update_runstate(vcpu, RUNSTATE_running);
+ *   - arch/x86/kvm/xen.h|172| <<kvm_xen_runstate_set_preempted>> kvm_xen_update_runstate(vcpu, RUNSTATE_runnable);
+ */
 void kvm_xen_update_runstate(struct kvm_vcpu *v, int state)
 {
 	struct kvm_vcpu_xen *vx = &v->arch.xen;
@@ -471,6 +511,11 @@ void kvm_xen_update_runstate(struct kvm_vcpu *v, int state)
 		kvm_xen_update_runstate_guest(v, state == RUNSTATE_runnable);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|550| <<kvm_xen_inject_pending_events>> kvm_xen_inject_vcpu_vector(v);
+ *   - arch/x86/kvm/xen.c|1644| <<kvm_xen_set_evtchn_fast>> kvm_xen_inject_vcpu_vector(vcpu);
+ */
 static void kvm_xen_inject_vcpu_vector(struct kvm_vcpu *v)
 {
 	struct kvm_lapic_irq irq = { };
@@ -601,6 +646,10 @@ int __kvm_xen_has_interrupt(struct kvm_vcpu *v)
 	return rc;
 }
 
+/*
+ * 处理KVM_XEN_HVM_SET_ATTR:
+ *   - arch/x86/kvm/x86.c|7003| <<kvm_arch_vm_ioctl(KVM_XEN_HVM_SET_ATTR)>> r = kvm_xen_hvm_set_attr(kvm, &xha);
+ */
 int kvm_xen_hvm_set_attr(struct kvm *kvm, struct kvm_xen_hvm_attr *data)
 {
 	int r = -ENOENT;
@@ -711,6 +760,10 @@ int kvm_xen_hvm_get_attr(struct kvm *kvm, struct kvm_xen_hvm_attr *data)
 	return r;
 }
 
+/*
+ * 处理KVM_XEN_VCPU_SET_ATTR:
+ *   - arch/x86/kvm/x86.c|6044| <<kvm_arch_vcpu_ioctl(KVM_XEN_VCPU_SET_ATTR)>> r = kvm_xen_vcpu_set_attr(vcpu, &xva);
+ */
 int kvm_xen_vcpu_set_attr(struct kvm_vcpu *vcpu, struct kvm_xen_vcpu_attr *data)
 {
 	int idx, r = -ENOENT;
@@ -1038,6 +1091,10 @@ int kvm_xen_vcpu_get_attr(struct kvm_vcpu *vcpu, struct kvm_xen_vcpu_attr *data)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3635| <<kvm_set_msr_common>> return kvm_xen_write_hypercall_page(vcpu, data);
+ */
 int kvm_xen_write_hypercall_page(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -1109,6 +1166,10 @@ int kvm_xen_write_hypercall_page(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6983| <<kvm_arch_vm_ioctl(KVM_XEN_HVM_CONFIG)>> r = kvm_xen_hvm_config(kvm, &xhc);
+ */
 int kvm_xen_hvm_config(struct kvm *kvm, struct kvm_xen_hvm_config *xhc)
 {
 	/* Only some feature flags need to be *enabled* by userspace */
@@ -1146,6 +1207,10 @@ static int kvm_xen_hypercall_set_result(struct kvm_vcpu *vcpu, u64 result)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * 在以下使用kvm_xen_hypercall_complete_userspace():
+ *   - vcpu->arch.complete_userspace_io = kvm_xen_hypercall_complete_userspace;
+ */
 static int kvm_xen_hypercall_complete_userspace(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *run = vcpu->run;
@@ -1374,6 +1439,11 @@ static bool kvm_xen_hcall_vcpu_op(struct kvm_vcpu *vcpu, bool longmode, int cmd,
 			return true;
 		}
 
+		/*
+		 * struct vcpu_set_singleshot_timer oneshot;
+		 * -> uint64_t timeout_abs_ns;
+		 * -> uint32_t flags;
+		 */
 		delta = oneshot.timeout_abs_ns - get_kvmclock_ns(vcpu->kvm);
 		if ((oneshot.flags & VCPU_SSHOTTMR_future) && delta < 0) {
 			*r = -ETIME;
@@ -1431,6 +1501,10 @@ static bool kvm_xen_hcall_set_timer_op(struct kvm_vcpu *vcpu, uint64_t timeout,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9823| <<kvm_emulate_hypercall>> return kvm_xen_hypercall(vcpu);
+ */
 int kvm_xen_hypercall(struct kvm_vcpu *vcpu)
 {
 	bool longmode;
@@ -2103,6 +2177,10 @@ void kvm_xen_update_tsc_info(struct kvm_vcpu *vcpu)
 		entry->eax = vcpu->arch.hw_tsc_khz;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12361| <<kvm_arch_init_vm>> kvm_xen_init_vm(kvm);
+ */
 void kvm_xen_init_vm(struct kvm *kvm)
 {
 	mutex_init(&kvm->arch.xen.xen_lock);
@@ -2110,6 +2188,10 @@ void kvm_xen_init_vm(struct kvm *kvm)
 	kvm_gpc_init(&kvm->arch.xen.shinfo_cache, kvm, NULL, KVM_HOST_USES_PFN);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12508| <<kvm_arch_destroy_vm>> kvm_xen_destroy_vm(kvm);
+ */
 void kvm_xen_destroy_vm(struct kvm *kvm)
 {
 	struct evtchnfd *evtchnfd;
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index 1fe011676..af6a14d8d 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -348,6 +348,41 @@ static inline void virtblk_request_done(struct request *req)
 	blk_mq_end_request(req, status);
 }
 
+/*
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_fasteoi_irq
+ * [0] generic_handle_domain_irq
+ * [0] gic_handle_irq
+ * [0] call_on_irq_stack
+ * [0] do_interrupt_handler
+ * [0] el1_interrupt
+ * [0] el1h_64_irq_handler
+ * [0] el1h_64_irq
+ * [0] __folio_start_writeback
+ * [0] ext4_bio_write_folio
+ * [0] mpage_submit_folio
+ * [0] mpage_process_page_bufs
+ * [0] mpage_prepare_extent_to_map
+ * [0] ext4_do_writepages
+ * [0] ext4_writepages
+ * [0] do_writepages
+ * [0] filemap_fdatawrite_wbc
+ * [0] __filemap_fdatawrite_range
+ * [0] file_write_and_wait_range
+ * [0] ext4_sync_file
+ * [0] vfs_fsync_range
+ * [0] do_fsync
+ * [0] __arm64_sys_fsync
+ * [0] invoke_syscall
+ * [0] el0_svc_common.constprop
+ * [0] do_el0_svc
+ * [0] el0_svc
+ * [0] el0t_64_sync_handler
+ * [0] el0t_64_sync
+ */
 static void virtblk_done(struct virtqueue *vq)
 {
 	struct virtio_blk *vblk = vq->vdev->priv;
diff --git a/drivers/clocksource/arm_arch_timer.c b/drivers/clocksource/arm_arch_timer.c
index 7dd2c615b..0f54afcab 100644
--- a/drivers/clocksource/arm_arch_timer.c
+++ b/drivers/clocksource/arm_arch_timer.c
@@ -59,6 +59,12 @@
  */
 #define MIN_ROLLOVER_SECS	(40ULL * 365 * 24 * 3600)
 
+/*
+ * 在以下修改arch_timers_present:
+ *   - drivers/clocksource/arm_arch_timer.c|1409| <<arch_timer_of_init>> arch_timers_present |= ARCH_TIMER_TYPE_CP15;
+ *   - drivers/clocksource/arm_arch_timer.c|1566| <<arch_timer_mem_frame_register>> arch_timers_present |= ARCH_TIMER_TYPE_MEM;
+ *   - drivers/clocksource/arm_arch_timer.c|1734| <<arch_timer_acpi_init>> arch_timers_present |= ARCH_TIMER_TYPE_CP15
+ */
 static unsigned arch_timers_present __initdata;
 
 struct arch_timer {
@@ -83,6 +89,17 @@ static const char *arch_timer_ppi_names[ARCH_TIMER_MAX_TIMER_PPI] = {
 
 static struct clock_event_device __percpu *arch_timer_evt;
 
+/*
+ * 5.4的例子.
+ *
+ * crash> arch_timer_uses_ppi
+ * arch_timer_uses_ppi = $1 = ARCH_TIMER_HYP_PPI
+ *
+ * 在以下使用arch_timer_uses_ppi:
+ *   - drivers/clocksource/arm_arch_timer.c|1438| <<arch_timer_of_init>> arch_timer_uses_ppi = ARCH_TIMER_PHYS_SECURE_PPI;
+ *   - drivers/clocksource/arm_arch_timer.c|1440| <<arch_timer_of_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+ *   - drivers/clocksource/arm_arch_timer.c|1762| <<arch_timer_acpi_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+ */
 static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
 static bool arch_timer_c3stop __ro_after_init;
 static bool arch_timer_mem_use_virtual __ro_after_init;
@@ -413,6 +430,13 @@ EXPORT_SYMBOL_GPL(timer_unstable_counter_workaround);
 
 static atomic_t timer_unstable_counter_workaround_in_use = ATOMIC_INIT(0);
 
+/*
+ * 有两种方式可以配置和使用一个timer:
+ * 1. CVAL(comparatoer)寄存器,通过设置比较器的值,当System Count >= CVAL时满足触发条件
+ * 2. TVAL寄存器,设置TVAL寄存器值后,比较器的值CVAL = TVAL + System Counter,当System Count >= CVAL时满足触发条件,
+ * TVAL是一个有符号数,当递减到0时还会继续递减,因此可以记录timer是在多久之前触发的
+ */
+
 /*
  * Force the inlining of this function so that the register accesses
  * can be themselves correctly inlined.
@@ -662,6 +686,13 @@ static bool arch_timer_counter_has_wa(void)
 #define arch_timer_counter_has_wa()			({false;})
 #endif /* CONFIG_ARM_ARCH_TIMER_OOL_WORKAROUND */
 
+/*
+ * called by:
+ *   - drivers/clocksource/arm_arch_timer.c|685| <<arch_timer_handler_virt>> return timer_handler(ARCH_TIMER_VIRT_ACCESS, evt);
+ *   - drivers/clocksource/arm_arch_timer.c|692| <<arch_timer_handler_phys>> return timer_handler(ARCH_TIMER_PHYS_ACCESS, evt);
+ *   - drivers/clocksource/arm_arch_timer.c|699| <<arch_timer_handler_phys_mem>> return timer_handler(ARCH_TIMER_MEM_PHYS_ACCESS, evt);
+ *   - drivers/clocksource/arm_arch_timer.c|706| <<arch_timer_handler_virt_mem>> return timer_handler(ARCH_TIMER_MEM_VIRT_ACCESS, evt);
+ */
 static __always_inline irqreturn_t timer_handler(const int access,
 					struct clock_event_device *evt)
 {
@@ -738,6 +769,11 @@ static int arch_timer_shutdown_phys_mem(struct clock_event_device *clk)
 	return arch_timer_shutdown(ARCH_TIMER_MEM_PHYS_ACCESS, clk);
 }
 
+/*
+ * 在以下使用set_next_event():
+ *   - drivers/clocksource/arm_arch_timer.c|787| <<arch_timer_set_next_event_virt>> set_next_event(ARCH_TIMER_VIRT_ACCESS, evt, clk);
+ *   - drivers/clocksource/arm_arch_timer.c|794| <<arch_timer_set_next_event_phys>> set_next_event(ARCH_TIMER_PHYS_ACCESS, evt, clk);
+ */
 static __always_inline void set_next_event(const int access, unsigned long evt,
 					   struct clock_event_device *clk)
 {
@@ -1097,6 +1133,10 @@ struct arch_timer_kvm_info *arch_timer_get_kvm_info(void)
 	return &arch_timer_kvm_info;
 }
 
+/*
+ * called by:
+ *   - drivers/clocksource/arm_arch_timer.c|1355| <<arch_timer_common_init>> arch_counter_register(arch_timers_present);
+ */
 static void __init arch_counter_register(unsigned type)
 {
 	u64 (*scr)(void);
@@ -1225,6 +1265,11 @@ static int __init arch_timer_register(void)
 	}
 
 	ppi = arch_timer_ppi[arch_timer_uses_ppi];
+	/*
+	 * 5.4的KVM的例子.
+	 * crash> arch_timer_uses_ppi
+	 * arch_timer_uses_ppi = $4 = ARCH_TIMER_HYP_PPI
+	 */
 	switch (arch_timer_uses_ppi) {
 	case ARCH_TIMER_VIRT_PPI:
 		err = request_percpu_irq(ppi, arch_timer_handler_virt,
diff --git a/drivers/irqchip/irq-gic-v3.c b/drivers/irqchip/irq-gic-v3.c
index f59ac9586..ac843a425 100644
--- a/drivers/irqchip/irq-gic-v3.c
+++ b/drivers/irqchip/irq-gic-v3.c
@@ -243,6 +243,11 @@ static inline void __iomem *gic_dist_base(struct irq_data *d)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|264| <<gic_dist_wait_for_rwp>> gic_do_wait_for_rwp(gic_data.dist_base, GICD_CTLR_RWP);
+ *   - drivers/irqchip/irq-gic-v3.c|270| <<gic_redist_wait_for_rwp>> gic_do_wait_for_rwp(gic_data_rdist_rd_base(), GICR_CTLR_RWP);
+ */
 static void gic_do_wait_for_rwp(void __iomem *base, u32 bit)
 {
 	u32 count = 1000000;	/* 1s! */
@@ -259,12 +264,23 @@ static void gic_do_wait_for_rwp(void __iomem *base, u32 bit)
 }
 
 /* Wait for completion of a distributor change */
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|412| <<gic_mask_irq>> gic_dist_wait_for_rwp();
+ *   - drivers/irqchip/irq-gic-v3.c|916| <<gic_dist_init>> gic_dist_wait_for_rwp();
+ *   - drivers/irqchip/irq-gic-v3.c|953| <<gic_dist_init>> gic_dist_wait_for_rwp();
+ */
 static void gic_dist_wait_for_rwp(void)
 {
 	gic_do_wait_for_rwp(gic_data.dist_base, GICD_CTLR_RWP);
 }
 
 /* Wait for completion of a redistributor change */
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|410| <<gic_mask_irq>> gic_redist_wait_for_rwp();
+ *   - drivers/irqchip/irq-gic-v3.c|1285| <<gic_cpu_init>> gic_cpu_config(rbase, gic_data.ppi_nr + 16, gic_redist_wait_for_rwp);
+ */
 static void gic_redist_wait_for_rwp(void)
 {
 	gic_do_wait_for_rwp(gic_data_rdist_rd_base(), GICR_CTLR_RWP);
diff --git a/drivers/irqchip/irq-gic-v4.c b/drivers/irqchip/irq-gic-v4.c
index 94d56a03b..5910bb128 100644
--- a/drivers/irqchip/irq-gic-v4.c
+++ b/drivers/irqchip/irq-gic-v4.c
@@ -306,6 +306,11 @@ int its_invall_vpe(struct its_vpe *vpe)
 	return its_send_vpe_cmd(vpe, &info);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|375| <<update_affinity>> ret = its_map_vlpi(irq->host_irq, &map);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|455| <<kvm_vgic_v4_set_forwarding>> ret = its_map_vlpi(virq, &map);
+ */
 int its_map_vlpi(int irq, struct its_vlpi_map *map)
 {
 	struct its_cmd_info info = {
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index d67f742fb..7ede5eaf2 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -422,6 +422,14 @@ static void disable_delayed_refill(struct virtnet_info *vi)
 	spin_unlock_bh(&vi->refill_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|442| <<virtqueue_napi_complete>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|457| <<skb_xmit_done>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|816| <<check_sq_full_and_disable>> virtqueue_napi_schedule(&sq->napi, sq->vq);
+ *   - drivers/net/virtio_net.c|2000| <<skb_recv_done>> virtqueue_napi_schedule(&rq->napi, rvq);
+ *   - drivers/net/virtio_net.c|2012| <<virtnet_napi_enable>> virtqueue_napi_schedule(napi, vq);
+ */
 static void virtqueue_napi_schedule(struct napi_struct *napi,
 				    struct virtqueue *vq)
 {
@@ -1963,6 +1971,13 @@ static int add_recvbuf_mergeable(struct virtnet_info *vi,
  * before we're receiving packets, or from refill_work which is
  * careful to disable receiving (using napi_disable).
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2097| <<refill_work>> still_empty = !try_fill_recv(vi, rq, GFP_KERNEL);
+ *   - drivers/net/virtio_net.c|2134| <<virtnet_receive>> if (!try_fill_recv(vi, rq, GFP_ATOMIC)) {
+ *   - drivers/net/virtio_net.c|2258| <<virtnet_open>> if (!try_fill_recv(vi, &vi->rq[i], GFP_KERNEL))
+ *   - drivers/net/virtio_net.c|2442| <<virtnet_rx_resize>> if (!try_fill_recv(vi, rq, GFP_KERNEL))
+ */
 static bool try_fill_recv(struct virtnet_info *vi, struct receive_queue *rq,
 			  gfp_t gfp)
 {
@@ -1992,6 +2007,44 @@ static bool try_fill_recv(struct virtnet_info *vi, struct receive_queue *rq,
 	return !oom;
 }
 
+/*
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_fasteoi_irq
+ * [0] generic_handle_domain_irq
+ * [0] gic_handle_irq
+ * [0] call_on_irq_stack
+ * [0] do_interrupt_handler
+ * [0] el1_interrupt
+ * [0] el1h_64_irq_handler
+ * [0] el1h_64_irq
+ * [0] __folio_start_writeback
+ * [0] ext4_bio_write_folio
+ * [0] mpage_submit_folio
+ * [0] mpage_process_page_bufs
+ * [0] mpage_prepare_extent_to_map
+ * [0] ext4_do_writepages
+ * [0] ext4_writepages
+ * [0] do_writepages
+ * [0] filemap_fdatawrite_wbc
+ * [0] __filemap_fdatawrite_range
+ * [0] file_write_and_wait_range
+ * [0] ext4_sync_file
+ * [0] vfs_fsync_range
+ * [0] do_fsync
+ * [0] __arm64_sys_fsync
+ * [0] invoke_syscall
+ * [0] el0_svc_common.constprop
+ * [0] do_el0_svc
+ * [0] el0_svc
+ * [0] el0t_64_sync_handler
+ * [0] el0t_64_sync
+ *
+ * called by:
+ *   - drivers/net/virtio_net.c|4099| <<virtnet_find_vqs>> callbacks[rxq2vq(i)] = skb_recv_done;
+ */
 static void skb_recv_done(struct virtqueue *rvq)
 {
 	struct virtnet_info *vi = rvq->vdev->priv;
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index f2ed7167c..908a0f267 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -1258,6 +1258,10 @@ static void handle_rx(struct vhost_net *net)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用handle_tx_kick():
+ *   - drivers/vhost/net.c|1332| <<vhost_net_open>> n->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;
+ */
 static void handle_tx_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 51d8f3299..dd17da094 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -2567,6 +2567,41 @@ static inline bool more_used(const struct vring_virtqueue *vq)
  * Calls the callback function of @_vq to process the virtqueue
  * notification.
  */
+/*
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_fasteoi_irq
+ * [0] generic_handle_domain_irq
+ * [0] gic_handle_irq
+ * [0] call_on_irq_stack
+ * [0] do_interrupt_handler
+ * [0] el1_interrupt
+ * [0] el1h_64_irq_handler
+ * [0] el1h_64_irq
+ * [0] __folio_start_writeback
+ * [0] ext4_bio_write_folio
+ * [0] mpage_submit_folio
+ * [0] mpage_process_page_bufs
+ * [0] mpage_prepare_extent_to_map
+ * [0] ext4_do_writepages
+ * [0] ext4_writepages
+ * [0] do_writepages
+ * [0] filemap_fdatawrite_wbc
+ * [0] __filemap_fdatawrite_range
+ * [0] file_write_and_wait_range
+ * [0] ext4_sync_file
+ * [0] vfs_fsync_range
+ * [0] do_fsync
+ * [0] __arm64_sys_fsync
+ * [0] invoke_syscall
+ * [0] el0_svc_common.constprop
+ * [0] do_el0_svc
+ * [0] el0_svc
+ * [0] el0t_64_sync_handler
+ * [0] el0t_64_sync
+ */
 irqreturn_t vring_interrupt(int irq, void *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
diff --git a/include/kvm/arm_vgic.h b/include/kvm/arm_vgic.h
index 5b27f94d4..0b4a9bc56 100644
--- a/include/kvm/arm_vgic.h
+++ b/include/kvm/arm_vgic.h
@@ -274,10 +274,42 @@ struct vgic_dist {
 	u64			propbaser;
 
 	/* Protects the lpi_list and the count value below. */
+	/*
+	 * 在以下使用vgic_dist->lpi_list_lock:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|58| <<kvm_vgic_early_init>> raw_spin_lock_init(&dist->lpi_list_lock);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|65| <<vgic_add_lpi>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|93| <<vgic_add_lpi>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|339| <<vgic_copy_lpi_list>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|348| <<vgic_copy_lpi_list>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|609| <<vgic_its_check_cache>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|611| <<vgic_its_check_cache>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|629| <<vgic_its_cache_translation>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|666| <<vgic_its_cache_translation>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|675| <<vgic_its_invalidate_cache>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|689| <<vgic_its_invalidate_cache>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic.c|66| <<vgic_get_lpi>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic.c|82| <<vgic_get_lpi>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic.c|147| <<vgic_put_irq>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic.c|149| <<vgic_put_irq>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 */
 	raw_spinlock_t		lpi_list_lock;
 	struct list_head	lpi_list_head;
 	int			lpi_list_count;
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1894| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1908| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1920| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 */
 	/* LPI translation cache */
 	struct list_head	lpi_translation_cache;
 
@@ -309,6 +341,16 @@ struct vgic_v3_cpu_if {
 	u32		vgic_sre;	/* Restored only, change ignored */
 	u32		vgic_ap0r[4];
 	u32		vgic_ap1r[4];
+	/*
+	 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+	 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|243| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|186| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|191| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+	 */
 	u64		vgic_lr[VGIC_V3_MAX_LRS];
 
 	/*
@@ -339,6 +381,23 @@ struct vgic_cpu {
 	 * were one of the two and need to be migrated off this list to another
 	 * VCPU.
 	 */
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|160| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|305| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|410| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|653| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|725| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|792| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|824| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|850| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|890| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|929| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|935| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|999| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 */
 	struct list_head ap_list_head;
 
 	/*
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index fb6c6109f..67c976152 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -2067,6 +2067,12 @@ static inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 
 void kvm_arch_irq_routing_update(struct kvm *kvm);
 
+/*
+ * called by:
+ *   - arch/s390/kvm/kvm-s390.c|4044| <<kvm_s390_sync_request>> __kvm_make_request(req, vcpu);
+ *   - include/linux/kvm_host.h|2090| <<kvm_make_request>> __kvm_make_request(req, vcpu);
+ *   - virt/kvm/kvm_main.c|268| <<kvm_make_vcpu_request>> __kvm_make_request(req, vcpu);
+ */
 static inline void __kvm_make_request(int req, struct kvm_vcpu *vcpu)
 {
 	/*
diff --git a/include/linux/kvm_irqfd.h b/include/linux/kvm_irqfd.h
index 8ad43692e..adadd756b 100644
--- a/include/linux/kvm_irqfd.h
+++ b/include/linux/kvm_irqfd.h
@@ -45,6 +45,13 @@ struct kvm_kernel_irqfd {
 	seqcount_spinlock_t irq_entry_sc;
 	/* Used for level IRQ fast-path */
 	int gsi;
+	/*
+	 * 在以下使用kvm_kernel_irqfd->inject:
+	 *   - virt/kvm/eventfd.c|141| <<irqfd_shutdown>> flush_work(&irqfd->inject);
+	 *   - virt/kvm/eventfd.c|218| <<irqfd_wakeup>> schedule_work(&irqfd->inject);
+	 *   - virt/kvm/eventfd.c|325| <<kvm_irqfd_assign>> INIT_WORK(&irqfd->inject, irqfd_inject);
+	 *   - virt/kvm/eventfd.c|425| <<kvm_irqfd_assign>> schedule_work(&irqfd->inject);
+	 */
 	struct work_struct inject;
 	/* The resampler used by this irqfd (resampler-only) */
 	struct kvm_kernel_irqfd_resampler *resampler;
diff --git a/kernel/entry/kvm.c b/kernel/entry/kvm.c
index 2e0f75bcb..d21330e6c 100644
--- a/kernel/entry/kvm.c
+++ b/kernel/entry/kvm.c
@@ -28,6 +28,12 @@ static int xfer_to_guest_mode_work(struct kvm_vcpu *vcpu, unsigned long ti_work)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1023| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|670| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+ *   - arch/x86/kvm/x86.c|11135| <<vcpu_run>> r = xfer_to_guest_mode_handle_work(vcpu);
+ */
 int xfer_to_guest_mode_handle_work(struct kvm_vcpu *vcpu)
 {
 	unsigned long ti_work;
diff --git a/kernel/events/core.c b/kernel/events/core.c
index a2f2a9525..eecbbd80f 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -12751,6 +12751,19 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|634| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+ *   - arch/riscv/kvm/vcpu_pmu.c|250| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(attr, -1, current, NULL, pmc);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|968| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|973| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+ *   - arch/x86/kvm/pmu.c|230| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|293| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+ *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+ *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+ *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>> return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+ *   - kernel/watchdog_perf.c|157| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
diff --git a/kernel/panic.c b/kernel/panic.c
index ffa037fa7..ea48c9f9f 100644
--- a/kernel/panic.c
+++ b/kernel/panic.c
@@ -190,6 +190,19 @@ atomic_t panic_cpu = ATOMIC_INIT(PANIC_CPU_INVALID);
  * nmi_panic_self_stop() which can provide architecture dependent code such
  * as saving register state for crash dump.
  */
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ */
 void nmi_panic(struct pt_regs *regs, const char *msg)
 {
 	int old_cpu, cpu;
diff --git a/kernel/stop_machine.c b/kernel/stop_machine.c
index cedb17ba1..bd7f8a9df 100644
--- a/kernel/stop_machine.c
+++ b/kernel/stop_machine.c
@@ -381,6 +381,18 @@ int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *
  * true if cpu_stop_work was queued successfully and @fn will be called,
  * false otherwise.
  */
+/*
+ * called by:
+ *   - kernel/sched/core.c|2668| <<migration_cpu_stop>> stop_one_cpu_nowait(task_cpu(p), migration_cpu_stop,
+ *   - kernel/sched/core.c|2992| <<affine_move_task>> stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
+ *   - kernel/sched/core.c|3063| <<affine_move_task>> stop_one_cpu_nowait(cpu_of(rq), migration_cpu_stop,
+ *   - kernel/sched/core.c|9509| <<balance_push>> stop_one_cpu_nowait(rq->cpu, __balance_push_cpu_stop, push_task,
+ *   - kernel/sched/deadline.c|2453| <<pull_dl_task>> stop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,
+ *   - kernel/sched/fair.c|11257| <<load_balance>> stop_one_cpu_nowait(cpu_of(busiest),
+ *   - kernel/sched/rt.c|2113| <<push_rt_task>> stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
+ *   - kernel/sched/rt.c|2452| <<pull_rt_task>> stop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,
+ *   - kernel/watchdog.c|453| <<watchdog_timer_fn>> stop_one_cpu_nowait(smp_processor_id(),
+ */
 bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,
 			struct cpu_stop_work *work_buf)
 {
diff --git a/kernel/watchdog.c b/kernel/watchdog.c
index d145305d9..6af10dcc4 100644
--- a/kernel/watchdog.c
+++ b/kernel/watchdog.c
@@ -1,4 +1,4 @@
-// SPDX-License-Identifier: GPL-2.0
+// spdx-license-identifier: gpl-2.0
 /*
  * Detect hard and soft lockups on a system
  *
@@ -35,11 +35,32 @@ static DEFINE_MUTEX(watchdog_mutex);
 # define WATCHDOG_HARDLOCKUP_DEFAULT	0
 #endif
 
+/*
+ * WATCHDOG_HARDLOCKUP_ENABLED
+ * WATCHDOG_SOFTLOCKUP_ENABLED
+ */
 unsigned long __read_mostly watchdog_enabled;
 int __read_mostly watchdog_user_enabled = 1;
+/*
+ * 在以下使用watchdog_hardlockup_user_enabled:
+ *   - kernel/watchdog.c|44| <<global>> static int __read_mostly watchdog_hardlockup_user_enabled = WATCHDOG_HARDLOCKUP_DEFAULT;
+ *   - kernel/watchdog.c|1006| <<global>> .data = &watchdog_hardlockup_user_enabled,
+ *   - kernel/watchdog.c|81| <<hardlockup_detector_disable>> watchdog_hardlockup_user_enabled = 0;
+ *   - kernel/watchdog.c|91| <<hardlockup_panic_setup>> watchdog_hardlockup_user_enabled = 0;
+ *   - kernel/watchdog.c|93| <<hardlockup_panic_setup>> watchdog_hardlockup_user_enabled = 1;
+ *   - kernel/watchdog.c|318| <<lockup_detector_update_enable>> if (watchdog_hardlockup_available && watchdog_hardlockup_user_enabled)
+ */
 static int __read_mostly watchdog_hardlockup_user_enabled = WATCHDOG_HARDLOCKUP_DEFAULT;
 static int __read_mostly watchdog_softlockup_user_enabled = 1;
 int __read_mostly watchdog_thresh = 10;
+/*
+ * 在以下使用watchdog_hardlockup_available:
+ *   - kernel/watchdog.c|300| <<lockup_detector_update_enable>> if (watchdog_hardlockup_available && watchdog_hardlockup_user_enabled)
+ *   - kernel/watchdog.c|844| <<proc_nmi_watchdog>> if (!watchdog_hardlockup_available && write)
+ *   - kernel/watchdog.c|1002| <<watchdog_sysctl_init>> if (watchdog_hardlockup_available)
+ *   - kernel/watchdog.c|1030| <<lockup_detector_delay_init>> watchdog_hardlockup_available = true;
+ *   - kernel/watchdog.c|1082| <<lockup_detector_init>> watchdog_hardlockup_available = true;
+ */
 static int __read_mostly watchdog_hardlockup_available;
 
 struct cpumask watchdog_cpumask __read_mostly;
@@ -64,6 +85,13 @@ unsigned int __read_mostly hardlockup_panic =
  * kernel command line parameters are parsed, because otherwise it is not
  * possible to override this in hardlockup_panic_setup().
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/setup_64.c|922| <<disable_hardlockup_detector>> hardlockup_detector_disable();
+ *   - arch/powerpc/kernel/setup_64.c|926| <<disable_hardlockup_detector>> hardlockup_detector_disable();
+ *   - arch/x86/kernel/cpu/mshyperv.c|584| <<ms_hyperv_init_platform>> hardlockup_detector_disable();
+ *   - arch/x86/kernel/kvm.c|866| <<kvm_guest_init>> hardlockup_detector_disable();
+ */
 void __init hardlockup_detector_disable(void)
 {
 	watchdog_hardlockup_user_enabled = 0;
@@ -87,7 +115,21 @@ __setup("nmi_watchdog=", hardlockup_panic_setup);
 
 #if defined(CONFIG_HARDLOCKUP_DETECTOR_COUNTS_HRTIMER)
 
+/*
+ * 在以下使用percpu的hrtimer_interrupts:
+ *   - kernel/watchdog.c|90| <<global>> static DEFINE_PER_CPU(atomic_t, hrtimer_interrupts);
+ *   - kernel/watchdog.c|116| <<is_hardlockup>> int hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));
+ *   - kernel/watchdog.c|135| <<watchdog_hardlockup_kick>> new_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));
+ *   - kernel/watchdog_buddy.c|88| <<watchdog_buddy_check_hardlockup>> void watchdog_buddy_check_hardlockup(int hrtimer_interrupts)
+ *   - kernel/watchdog_buddy.c|97| <<watchdog_buddy_check_hardlockup>> if (hrtimer_interrupts % 3 != 0)
+ */
 static DEFINE_PER_CPU(atomic_t, hrtimer_interrupts);
+/*
+ * 在以下使用percpu的hrtimer_interrupts_saved:
+ *   - kernel/watchdog.c|99| <<global>> static DEFINE_PER_CPU(int , hrtimer_interrupts_saved);
+ *   - kernel/watchdog.c|134| <<is_hardlockup>> if (per_cpu(hrtimer_interrupts_saved, cpu) == hrint)
+ *   - kernel/watchdog.c|142| <<is_hardlockup>> per_cpu(hrtimer_interrupts_saved, cpu) = hrint;
+ */
 static DEFINE_PER_CPU(int, hrtimer_interrupts_saved);
 static DEFINE_PER_CPU(bool, watchdog_hardlockup_warned);
 static DEFINE_PER_CPU(bool, watchdog_hardlockup_touched);
@@ -111,10 +153,28 @@ void watchdog_hardlockup_touch_cpu(unsigned int cpu)
 	per_cpu(watchdog_hardlockup_touched, cpu) = true;
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|176| <<watchdog_hardlockup_check>> if (is_hardlockup(cpu)) {
+ */
 static bool is_hardlockup(unsigned int cpu)
 {
+	/*
+	 * 在以下使用percpu的hrtimer_interrupts:
+	 *   - kernel/watchdog.c|90| <<global>> static DEFINE_PER_CPU(atomic_t, hrtimer_interrupts);
+	 *   - kernel/watchdog.c|116| <<is_hardlockup>> int hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));
+	 *   - kernel/watchdog.c|135| <<watchdog_hardlockup_kick>> new_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));
+	 *   - kernel/watchdog_buddy.c|88| <<watchdog_buddy_check_hardlockup>> void watchdog_buddy_check_hardlockup(int hrtimer_interrupts)
+	 *   - kernel/watchdog_buddy.c|97| <<watchdog_buddy_check_hardlockup>> if (hrtimer_interrupts % 3 != 0)
+	 */
 	int hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));
 
+	/*
+	 * 在以下使用percpu的hrtimer_interrupts_saved:
+	 *   - kernel/watchdog.c|99| <<global>> static DEFINE_PER_CPU(int , hrtimer_interrupts_saved);
+	 *   - kernel/watchdog.c|134| <<is_hardlockup>> if (per_cpu(hrtimer_interrupts_saved, cpu) == hrint)
+	 *   - kernel/watchdog.c|142| <<is_hardlockup>> per_cpu(hrtimer_interrupts_saved, cpu) = hrint;
+	 */
 	if (per_cpu(hrtimer_interrupts_saved, cpu) == hrint)
 		return true;
 
@@ -128,14 +188,31 @@ static bool is_hardlockup(unsigned int cpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|472| <<watchdog_timer_fn>> watchdog_hardlockup_kick();
+ */
 static void watchdog_hardlockup_kick(void)
 {
 	int new_interrupts;
 
+	/*
+	 * 在以下使用percpu的hrtimer_interrupts:
+	 *   - kernel/watchdog.c|90| <<global>> static DEFINE_PER_CPU(atomic_t, hrtimer_interrupts);
+	 *   - kernel/watchdog.c|116| <<is_hardlockup>> int hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));
+	 *   - kernel/watchdog.c|135| <<watchdog_hardlockup_kick>> new_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));
+	 *   - kernel/watchdog_buddy.c|88| <<watchdog_buddy_check_hardlockup>> void watchdog_buddy_check_hardlockup(int hrtimer_interrupts)
+	 *   - kernel/watchdog_buddy.c|97| <<watchdog_buddy_check_hardlockup>> if (hrtimer_interrupts % 3 != 0)
+	 */
 	new_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));
 	watchdog_buddy_check_hardlockup(new_interrupts);
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog_buddy.c|112| <<watchdog_buddy_check_hardlockup>> watchdog_hardlockup_check(next_cpu, NULL);
+ *   - kernel/watchdog_perf.c|104| <<watchdog_overflow_callback>> watchdog_hardlockup_check(smp_processor_id(), regs);
+ */
 void watchdog_hardlockup_check(unsigned int cpu, struct pt_regs *regs)
 {
 	if (per_cpu(watchdog_hardlockup_touched, cpu)) {
@@ -243,6 +320,12 @@ void __weak watchdog_hardlockup_start(void) { }
  * Caller needs to make sure that the hard watchdogs are off, so this
  * can't race with watchdog_hardlockup_disable().
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|692| <<__lockup_detector_reconfigure>> lockup_detector_update_enable();
+ *   - kernel/watchdog.c|721| <<lockup_detector_setup>> lockup_detector_update_enable();
+ *   - kernel/watchdog.c|738| <<__lockup_detector_reconfigure>> lockup_detector_update_enable();
+ */
 static void lockup_detector_update_enable(void)
 {
 	watchdog_enabled = 0;
@@ -328,6 +411,10 @@ static unsigned long get_timestamp(void)
 	return running_clock() >> 30LL;  /* 2^30 ~= 10^9 */
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|683| <<__lockup_detector_reconfigure>> set_sample_period();
+ */
 static void set_sample_period(void)
 {
 	/*
@@ -434,6 +521,10 @@ static int softlockup_fn(void *data)
 	return 0;
 }
 
+/*
+ * 在以下使用watchdog_timer_fn():
+ *   - kernel/watchdog.c|601| <<watchdog_enable>> hrtimer->function = watchdog_timer_fn;
+ */
 /* watchdog kicker functions */
 static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)
 {
@@ -556,6 +647,11 @@ static void watchdog_enable(unsigned int cpu)
 		watchdog_hardlockup_enable(cpu);
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|626| <<softlockup_stop_fn>> watchdog_disable(smp_processor_id());
+ *   - kernel/watchdog.c|668| <<lockup_detector_offline_cpu>> watchdog_disable(cpu);
+ */
 static void watchdog_disable(unsigned int cpu)
 {
 	struct hrtimer *hrtimer = this_cpu_ptr(&watchdog_hrtimer);
@@ -591,12 +687,37 @@ static void softlockup_stop_all(void)
 	cpumask_clear(&watchdog_allowed_mask);
 }
 
+/*
+ * 在以下使用softlockup_start_fn():
+ *   - kernel/watchdog.c|702| <<softlockup_start_all>> smp_call_on_cpu(cpu, softlockup_start_fn, NULL, false);
+ */
 static int softlockup_start_fn(void *data)
 {
 	watchdog_enable(smp_processor_id());
 	return 0;
 }
 
+/*
+ * [0] softlockup_start_all
+ * [0] __lockup_detector_reconfigure
+ * [0] lockup_detector_init
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] softlockup_start_all
+ * [0] __lockup_detector_reconfigure
+ * [0] proc_watchdog_common
+ * [0] proc_sys_call_handler
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用softlockup_start_all():
+ *   - kernel/watchdog.c|728| <<__lockup_detector_reconfigure>> softlockup_start_all();
+ */
 static void softlockup_start_all(void)
 {
 	int cpu;
@@ -620,6 +741,19 @@ int lockup_detector_offline_cpu(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * [0] __lockup_detector_reconfigure
+ * [0] lockup_detector_init
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - kernel/watchdog.c|763| <<lockup_detector_reconfigure>> __lockup_detector_reconfigure();
+ *   - kernel/watchdog.c|783| <<lockup_detector_setup>> __lockup_detector_reconfigure();
+ *   - kernel/watchdog.c|849| <<proc_watchdog_update>> __lockup_detector_reconfigure();
+ */
 static void __lockup_detector_reconfigure(void)
 {
 	cpus_read_lock();
@@ -631,6 +765,10 @@ static void __lockup_detector_reconfigure(void)
 	if (watchdog_enabled && watchdog_thresh)
 		softlockup_start_all();
 
+	/*
+	 * 似乎是这里启动
+	 * 好吧, 不是 :)
+	 */
 	watchdog_hardlockup_start();
 	cpus_read_unlock();
 	/*
@@ -650,6 +788,11 @@ void lockup_detector_reconfigure(void)
 /*
  * Create the watchdog infrastructure and configure the detector(s).
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|1109| <<lockup_detector_delay_init>> lockup_detector_setup();
+ *   - kernel/watchdog.c|1168| <<lockup_detector_init>> lockup_detector_setup();
+ */
 static __init void lockup_detector_setup(void)
 {
 	/*
@@ -718,6 +861,12 @@ void lockup_detector_soft_poweroff(void)
 
 #ifdef CONFIG_SYSCTL
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|863| <<proc_watchdog_common>> proc_watchdog_update();
+ *   - kernel/watchdog.c|916| <<proc_watchdog_thresh>> proc_watchdog_update();
+ *   - kernel/watchdog.c|937| <<proc_watchdog_cpumask>> proc_watchdog_update();
+ */
 /* Propagate any changes to the watchdog infrastructure */
 static void proc_watchdog_update(void)
 {
@@ -738,6 +887,12 @@ static void proc_watchdog_update(void)
  * -------------------|----------------------------------|-------------------------------
  * proc_soft_watchdog | watchdog_softlockup_user_enabled | WATCHDOG_SOFTOCKUP_ENABLED
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|875| <<proc_watchdog>> return proc_watchdog_common(WATCHDOG_HARDLOCKUP_ENABLED |
+ *   - kernel/watchdog.c|888| <<proc_nmi_watchdog>> return proc_watchdog_common(WATCHDOG_HARDLOCKUP_ENABLED,
+ *   - kernel/watchdog.c|898| <<proc_soft_watchdog>> return proc_watchdog_common(WATCHDOG_SOFTOCKUP_ENABLED,
+ */
 static int proc_watchdog_common(int which, struct ctl_table *table, int write,
 				void *buffer, size_t *lenp, loff_t *ppos)
 {
@@ -949,9 +1104,20 @@ static void __init watchdog_sysctl_init(void)
 static void __init lockup_detector_delay_init(struct work_struct *work);
 static bool allow_lockup_detector_init_retry __initdata;
 
+/*
+ * 在以下使用detector_work:
+ *   - kernel/watchdog.c|1032| <<global>> static struct work_struct detector_work __initdata =
+ *   - kernel/watchdog.c|1033| <<global>> __WORK_INITIALIZER(detector_work, lockup_detector_delay_init);
+ *   - kernel/watchdog.c|1065| <<lockup_detector_retry_init>> schedule_work(&detector_work);
+ *   - kernel/watchdog.c|1078| <<lockup_detector_check>> flush_work(&detector_work);
+ */
 static struct work_struct detector_work __initdata =
 		__WORK_INITIALIZER(detector_work, lockup_detector_delay_init);
 
+/*
+ * 在以下使用lockup_detector_delay_init():
+ *   - kernel/watchdog.c|1033| <<global>> __WORK_INITIALIZER(detector_work, lockup_detector_delay_init);
+ */
 static void __init lockup_detector_delay_init(struct work_struct *work)
 {
 	int ret;
@@ -989,6 +1155,10 @@ void __init lockup_detector_retry_init(void)
  * Ensure that optional delayed hardlockup init is proceed before
  * the init code and memory is freed.
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|1085| <<global>> late_initcall_sync(lockup_detector_check);
+ */
 static int __init lockup_detector_check(void)
 {
 	/* Prevent any later retry. */
@@ -1004,6 +1174,10 @@ static int __init lockup_detector_check(void)
 }
 late_initcall_sync(lockup_detector_check);
 
+/*
+ * called by:
+ *   - init/main.c|1538| <<kernel_init_freeable>> lockup_detector_init();
+ */
 void __init lockup_detector_init(void)
 {
 	if (tick_nohz_full_enabled())
diff --git a/kernel/watchdog_perf.c b/kernel/watchdog_perf.c
index 8ea00c4a2..49722118a 100644
--- a/kernel/watchdog_perf.c
+++ b/kernel/watchdog_perf.c
@@ -20,17 +20,62 @@
 #include <asm/irq_regs.h>
 #include <linux/perf_event.h>
 
+/*
+ * 在以下使用percpu的watchdog_ev:
+ *   - kernel/watchdog_perf.c|23| <<global>> static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
+ *   - kernel/watchdog_perf.c|163| <<hardlockup_detector_event_create>> this_cpu_write(watchdog_ev, evt);
+ *   - kernel/watchdog_perf.c|187| <<watchdog_hardlockup_enable>> perf_event_enable(this_cpu_read(watchdog_ev));
+ *   - kernel/watchdog_perf.c|197| <<watchdog_hardlockup_disable>> struct perf_event *event = this_cpu_read(watchdog_ev);
+ *   - kernel/watchdog_perf.c|203| <<watchdog_hardlockup_disable>> this_cpu_write(watchdog_ev, NULL);
+ *   - kernel/watchdog_perf.c|245| <<hardlockup_detector_perf_stop>> struct perf_event *event = per_cpu(watchdog_ev, cpu);
+ *   - kernel/watchdog_perf.c|271| <<hardlockup_detector_perf_restart>> struct perf_event *event = per_cpu(watchdog_ev, cpu);
+ *   - kernel/watchdog_perf.c|303| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+ *   - kernel/watchdog_perf.c|304| <<watchdog_hardlockup_probe>> this_cpu_write(watchdog_ev, NULL);
+ */
 static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
+/*
+ * 在以下使用percpu的dead_event:
+ *   - kernel/watchdog_perf.c|24| <<global>> static DEFINE_PER_CPU(struct perf_event *, dead_event);
+ *   - kernel/watchdog_perf.c|189| <<watchdog_hardlockup_disable>> this_cpu_write(dead_event, event);
+ *   - kernel/watchdog_perf.c|205| <<hardlockup_detector_perf_cleanup>> struct perf_event *event = per_cpu(dead_event, cpu);
+ *   - kernel/watchdog_perf.c|213| <<hardlockup_detector_perf_cleanup>> per_cpu(dead_event, cpu) = NULL;
+ */
 static DEFINE_PER_CPU(struct perf_event *, dead_event);
+/*
+ * 在以下使用dead_events_mask:
+ *   - kernel/watchdog_perf.c|205| <<watchdog_hardlockup_disable>> cpumask_set_cpu(smp_processor_id(), &dead_events_mask);
+ *   - kernel/watchdog_perf.c|219| <<hardlockup_detector_perf_cleanup>> for_each_cpu(cpu, &dead_events_mask) {
+ *   - kernel/watchdog_perf.c|230| <<hardlockup_detector_perf_cleanup>> cpumask_clear(&dead_events_mask);
+ */
 static struct cpumask dead_events_mask;
 
+/*
+ * 在以下使用watchdog_cpus:
+ *   - kernel/watchdog_perf.c|184| <<watchdog_hardlockup_enable>> if (!atomic_fetch_inc(&watchdog_cpus))
+ *   - kernel/watchdog_perf.c|206| <<watchdog_hardlockup_disable>> atomic_dec(&watchdog_cpus);
+ */
 static atomic_t watchdog_cpus = ATOMIC_INIT(0);
 
 #ifdef CONFIG_HARDLOCKUP_CHECK_TIMESTAMP
 static DEFINE_PER_CPU(ktime_t, last_timestamp);
+/*
+ * 在以下使用percpu的nmi_rearmed:
+ *   - kernel/watchdog_perf.c|61| <<global>> static DEFINE_PER_CPU(unsigned int , nmi_rearmed);
+ *   - kernel/watchdog_perf.c|114| <<watchdog_check_timestamp>> if (__this_cpu_inc_return(nmi_rearmed) < 10)
+ *   - kernel/watchdog_perf.c|117| <<watchdog_check_timestamp>> __this_cpu_write(nmi_rearmed, 0);
+ */
 static DEFINE_PER_CPU(unsigned int, nmi_rearmed);
+/*
+ * 在以下使用watchdog_hrtimer_sample_threshold:
+ *   - kernel/watchdog_perf.c|57| <<watchdog_update_hrtimer_threshold>> watchdog_hrtimer_sample_threshold = period * 2;
+ *   - kernel/watchdog_perf.c|70| <<watchdog_check_timestamp>> if (delta < watchdog_hrtimer_sample_threshold) {
+ */
 static ktime_t watchdog_hrtimer_sample_threshold __read_mostly;
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|390| <<set_sample_period>> watchdog_update_hrtimer_threshold(sample_period);
+ */
 void watchdog_update_hrtimer_threshold(u64 period)
 {
 	/*
@@ -57,6 +102,10 @@ void watchdog_update_hrtimer_threshold(u64 period)
 	watchdog_hrtimer_sample_threshold = period * 2;
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog_perf.c|105| <<watchdog_overflow_callback>> if (!watchdog_check_timestamp())
+ */
 static bool watchdog_check_timestamp(void)
 {
 	ktime_t delta, now = ktime_get_mono_fast_ns();
@@ -82,6 +131,10 @@ static inline bool watchdog_check_timestamp(void)
 }
 #endif
 
+/*
+ * 在以下使用wd_hw_attr:
+ *   - kernel/watchdog_perf.c|123| <<hardlockup_detector_event_create>> wd_attr = &wd_hw_attr;
+ */
 static struct perf_event_attr wd_hw_attr = {
 	.type		= PERF_TYPE_HARDWARE,
 	.config		= PERF_COUNT_HW_CPU_CYCLES,
@@ -90,6 +143,10 @@ static struct perf_event_attr wd_hw_attr = {
 	.disabled	= 1,
 };
 
+/*
+ * called by:
+ *   - kernel/watchdog_perf.c|124| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+ */
 /* Callback function for perf event subsystem */
 static void watchdog_overflow_callback(struct perf_event *event,
 				       struct perf_sample_data *data,
@@ -104,6 +161,11 @@ static void watchdog_overflow_callback(struct perf_event *event,
 	watchdog_hardlockup_check(smp_processor_id(), regs);
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog_perf.c|147| <<watchdog_hardlockup_enable>> if (hardlockup_detector_event_create())
+ *   - kernel/watchdog_perf.c|256| <<watchdog_hardlockup_probe>> ret = hardlockup_detector_event_create();
+ */
 static int hardlockup_detector_event_create(void)
 {
 	unsigned int cpu;
@@ -136,6 +198,50 @@ static int hardlockup_detector_event_create(void)
  *
  * @cpu: The CPU to enable hard lockup on.
  */
+/*
+ * BM启动的时候CPU=0
+ * [0] watchdog_hardlockup_enable
+ * [0] softlockup_start_fn
+ * [0] smp_call_on_cpu_callback
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * BM启动的时候其他的CPU
+ * [0] watchdog_hardlockup_enable
+ * [0] lockup_detector_online_cpu
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * 动态调整echo 1 > /proc/sys/kernel/nmi_watchdog
+ * [0] watchdog_hardlockup_enable
+ * [0] softlockup_start_fn
+ * [0] smp_call_on_cpu_callback
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * cpu hotplug onle的时候
+ * [0] watchdog_hardlockup_enable
+ * [0] lockup_detector_online_cpu
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - kernel/watchdog.c|605| <<watchdog_enable>> watchdog_hardlockup_enable(cpu);
+ */
 void watchdog_hardlockup_enable(unsigned int cpu)
 {
 	WARN_ON_ONCE(cpu != smp_processor_id());
@@ -147,6 +253,18 @@ void watchdog_hardlockup_enable(unsigned int cpu)
 	if (!atomic_fetch_inc(&watchdog_cpus))
 		pr_info("Enabled. Permanently consumes one hw-PMU counter.\n");
 
+	/*
+	 * 在以下使用percpu的watchdog_ev:
+	 *   - kernel/watchdog_perf.c|23| <<global>> static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
+	 *   - kernel/watchdog_perf.c|163| <<hardlockup_detector_event_create>> this_cpu_write(watchdog_ev, evt);
+	 *   - kernel/watchdog_perf.c|187| <<watchdog_hardlockup_enable>> perf_event_enable(this_cpu_read(watchdog_ev));
+	 *   - kernel/watchdog_perf.c|197| <<watchdog_hardlockup_disable>> struct perf_event *event = this_cpu_read(watchdog_ev);
+	 *   - kernel/watchdog_perf.c|203| <<watchdog_hardlockup_disable>> this_cpu_write(watchdog_ev, NULL);
+	 *   - kernel/watchdog_perf.c|245| <<hardlockup_detector_perf_stop>> struct perf_event *event = per_cpu(watchdog_ev, cpu);
+	 *   - kernel/watchdog_perf.c|271| <<hardlockup_detector_perf_restart>> struct perf_event *event = per_cpu(watchdog_ev, cpu);
+	 *   - kernel/watchdog_perf.c|303| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+	 *   - kernel/watchdog_perf.c|304| <<watchdog_hardlockup_probe>> this_cpu_write(watchdog_ev, NULL);
+	 */
 	perf_event_enable(this_cpu_read(watchdog_ev));
 }
 
@@ -175,10 +293,20 @@ void watchdog_hardlockup_disable(unsigned int cpu)
  *
  * Called from lockup_detector_cleanup(). Serialized by the caller.
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|755| <<__lockup_detector_cleanup>> hardlockup_detector_perf_cleanup();
+ */
 void hardlockup_detector_perf_cleanup(void)
 {
 	int cpu;
 
+	/*
+	 * 在以下使用dead_events_mask:
+	 *   - kernel/watchdog_perf.c|205| <<watchdog_hardlockup_disable>> cpumask_set_cpu(smp_processor_id(), &dead_events_mask);
+	 *   - kernel/watchdog_perf.c|219| <<hardlockup_detector_perf_cleanup>> for_each_cpu(cpu, &dead_events_mask) {
+	 *   - kernel/watchdog_perf.c|230| <<hardlockup_detector_perf_cleanup>> cpumask_clear(&dead_events_mask);
+	 */
 	for_each_cpu(cpu, &dead_events_mask) {
 		struct perf_event *event = per_cpu(dead_event, cpu);
 
@@ -217,6 +345,10 @@ void __init hardlockup_detector_perf_stop(void)
  *
  * Special interface for x86 to handle the perf HT bug.
  */
+/*
+ * called by:
+ *   - arch/x86/events/intel/core.c|6890| <<fixup_ht_bug>> hardlockup_detector_perf_restart();
+ */
 void __init hardlockup_detector_perf_restart(void)
 {
 	int cpu;
@@ -242,6 +374,11 @@ bool __weak __init arch_perf_nmi_is_available(void)
 /**
  * watchdog_hardlockup_probe - Probe whether NMI event is available at all
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|1017| <<lockup_detector_delay_init>> ret = watchdog_hardlockup_probe();
+ *   - kernel/watchdog.c|1073| <<lockup_detector_init>> if (!watchdog_hardlockup_probe())
+ */
 int __init watchdog_hardlockup_probe(void)
 {
 	int ret;
diff --git a/tools/testing/selftests/kvm/memslot_modification_stress_test.c b/tools/testing/selftests/kvm/memslot_modification_stress_test.c
index 9855c41ca..7e1dfb69d 100644
--- a/tools/testing/selftests/kvm/memslot_modification_stress_test.c
+++ b/tools/testing/selftests/kvm/memslot_modification_stress_test.c
@@ -62,6 +62,10 @@ struct memslot_antagonist_args {
 	uint64_t nr_modifications;
 };
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|108| <<run_test>> add_remove_memslot(vm, p->delay, p->nr_iterations);
+ */
 static void add_remove_memslot(struct kvm_vm *vm, useconds_t delay,
 			       uint64_t nr_modifications)
 {
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 89912a17f..7aba347ad 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -38,6 +38,16 @@ kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
 	return true;
 }
 
+/*
+ * 在以下使用kvm_kernel_irqfd->inject:
+ *   - virt/kvm/eventfd.c|141| <<irqfd_shutdown>> flush_work(&irqfd->inject);
+ *   - virt/kvm/eventfd.c|218| <<irqfd_wakeup>> schedule_work(&irqfd->inject);
+ *   - virt/kvm/eventfd.c|325| <<kvm_irqfd_assign>> INIT_WORK(&irqfd->inject, irqfd_inject);
+ *   - virt/kvm/eventfd.c|425| <<kvm_irqfd_assign>> schedule_work(&irqfd->inject);
+ *
+ * 在以下使用irqfd_inject():
+ *   - virt/kvm/eventfd.c|325| <<kvm_irqfd_assign>> INIT_WORK(&irqfd->inject, irqfd_inject);
+ */
 static void
 irqfd_inject(struct work_struct *work)
 {
@@ -190,6 +200,28 @@ int __attribute__((weak)) kvm_arch_set_irq_inatomic(
 /*
  * Called with wqh->lock held and interrupts disabled
  */
+/*
+ * 旧的callstack
+ * kvm_apic_set_irq
+ * kvm_arch_set_irq_inatomic
+ * irqfd_wakeup
+ * __wake_up_common
+ * __wake_up_locked_key
+ * eventfd_signal
+ * vhost_signal
+ * vhost_add_used_and_signal_n
+ * vhost_net_signal_used
+ * vhost_tx_batch.isra.23
+ * handle_tx_copy
+ * handle_tx
+ * handle_tx_kick
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ *
+ * 在以下使用irqfd_wakeup():
+ *   - virt/kvm/eventfd.c|396| <<kvm_irqfd_assign>> init_waitqueue_func_entry(&irqfd->wait, irqfd_wakeup);
+ */
 static int
 irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 {
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 1e567d1f6..3b24e782f 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -85,6 +85,9 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 
 	while (i--) {
 		int r;
+		/*
+		 * kvm_set_msi()
+		 */
 		r = irq_set[i].set(&irq_set[i], kvm, irq_source_id, level,
 				   line_status);
 		if (r < 0)
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 486800a70..c75dc840d 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -2923,6 +2923,11 @@ void kvm_vcpu_unmap(struct kvm_vcpu *vcpu, struct kvm_host_map *map, bool dirty)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_unmap);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2937| <<kvm_set_page_dirty>> if (kvm_is_ad_tracked_page(page))
+ *   - virt/kvm/kvm_main.c|2943| <<kvm_set_page_accessed>> if (kvm_is_ad_tracked_page(page))
+ */
 static bool kvm_is_ad_tracked_page(struct page *page)
 {
 	/*
@@ -3390,6 +3395,15 @@ void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_mark_page_dirty);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|939| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/mips/kvm/mips.c|431| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|1859| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|664| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|5065| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/x86/kvm/x86.c|11207| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ */
 void kvm_sigset_activate(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu->sigset_active)
@@ -3476,6 +3490,10 @@ static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
  * pending.  This is mostly used when halting a vCPU, but may also be used
  * directly for other vCPU non-runnable states, e.g. x86's Wait-For-SIPI.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3597| <<kvm_vcpu_halt>> waited = kvm_vcpu_block(vcpu);
+ */
 bool kvm_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	struct rcuwait *wait = kvm_arch_vcpu_get_wait(vcpu);
@@ -3554,6 +3572,19 @@ static unsigned int kvm_vcpu_max_halt_poll_ns(struct kvm_vcpu *vcpu)
  * expensive block+unblock sequence if a wake event arrives soon after the vCPU
  * is halted.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|742| <<kvm_vcpu_wfi>> kvm_vcpu_halt(vcpu);
+ *   - arch/mips/kvm/emulate.c|955| <<kvm_mips_emul_wait>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr.c|501| <<kvmppc_set_msr_pr>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr_papr.c|395| <<kvmppc_h_pr>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/booke.c|724| <<kvmppc_core_prepare_to_enter>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|240| <<kvmppc_kvm_pv>> kvm_vcpu_halt(vcpu);
+ *   - arch/riscv/kvm/vcpu_insn.c|192| <<kvm_riscv_vcpu_wfi>> kvm_vcpu_halt(vcpu);
+ *   - arch/s390/kvm/interrupt.c|1334| <<kvm_s390_handle_wait>> kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/x86.c|11039| <<vcpu_block>> kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/xen.c|1352| <<kvm_xen_schedop_poll>> kvm_vcpu_halt(vcpu);
+ */
 void kvm_vcpu_halt(struct kvm_vcpu *vcpu)
 {
 	unsigned int max_halt_poll_ns = kvm_vcpu_max_halt_poll_ns(vcpu);
-- 
2.34.1

