From 9a0efca89521c4fb8b572b8080e3df8aac3df49e Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 9 Jan 2024 23:50:34 -0800
Subject: [PATCH 1/1] linux-v6.6

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/arch_timer.h           |   3 +
 arch/arm64/include/asm/kvm_emulate.h          |   6 +
 arch/arm64/include/asm/kvm_host.h             |  20 ++
 arch/arm64/include/asm/kvm_mmu.h              |  33 ++
 arch/arm64/kernel/process.c                   |  12 +
 arch/arm64/kvm/arch_timer.c                   | 289 +++++++++++++++++
 arch/arm64/kvm/arm.c                          | 136 ++++++++
 arch/arm64/kvm/guest.c                        |   4 +
 arch/arm64/kvm/handle_exit.c                  |  29 ++
 arch/arm64/kvm/hyp/include/hyp/switch.h       |  44 +++
 arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h    |  10 +
 arch/arm64/kvm/hyp/include/nvhe/mem_protect.h |   7 +
 arch/arm64/kvm/hyp/nvhe/timer-sr.c            |  18 +
 arch/arm64/kvm/hyp/vgic-v3-sr.c               |  10 +
 arch/arm64/kvm/hyp/vhe/switch.c               |  61 ++++
 arch/arm64/kvm/hyp/vhe/sysreg-sr.c            |  32 ++
 arch/arm64/kvm/hyp/vhe/tlb.c                  |  13 +
 arch/arm64/kvm/mmio.c                         |   9 +
 arch/arm64/kvm/mmu.c                          |  22 ++
 arch/arm64/kvm/pmu-emul.c                     |   9 +
 arch/arm64/kvm/sys_regs.c                     |  22 ++
 arch/arm64/kvm/vgic/vgic-irqfd.c              |  27 ++
 arch/arm64/kvm/vgic/vgic-its.c                | 125 +++++++
 arch/arm64/kvm/vgic/vgic-mmio-v3.c            |   4 +
 arch/arm64/kvm/vgic/vgic-v3.c                 |   8 +
 arch/arm64/kvm/vgic/vgic-v4.c                 |   4 +
 arch/arm64/kvm/vgic/vgic.c                    | 132 ++++++++
 arch/x86/events/amd/core.c                    |  29 ++
 arch/x86/events/core.c                        |  79 +++++
 arch/x86/events/perf_event.h                  |  11 +
 arch/x86/include/asm/kvm_host.h               | 230 +++++++++++++
 arch/x86/include/asm/nmi.h                    |  22 ++
 arch/x86/kernel/apic/hw_nmi.c                 |  27 ++
 arch/x86/kernel/kvm.c                         |   8 +
 arch/x86/kernel/nmi.c                         |  13 +
 arch/x86/kvm/cpuid.c                          |   4 +
 arch/x86/kvm/emulate.c                        |   3 +
 arch/x86/kvm/irq.c                            |   4 +
 arch/x86/kvm/irq_comm.c                       |   4 +
 arch/x86/kvm/lapic.c                          |  56 ++++
 arch/x86/kvm/lapic.h                          |   4 +
 arch/x86/kvm/mmu/mmu.c                        |   7 +
 arch/x86/kvm/pmu.c                            | 257 +++++++++++++++
 arch/x86/kvm/pmu.h                            | 161 +++++++++
 arch/x86/kvm/smm.c                            |  51 +++
 arch/x86/kvm/svm/avic.c                       |   4 +
 arch/x86/kvm/svm/nested.c                     |   5 +
 arch/x86/kvm/svm/pmu.c                        | 123 +++++++
 arch/x86/kvm/svm/svm.c                        |  12 +
 arch/x86/kvm/vmx/vmx.c                        |  19 ++
 arch/x86/kvm/vmx/vmx.h                        |  15 +
 arch/x86/kvm/x86.c                            | 190 +++++++++++
 arch/x86/kvm/x86.h                            |  41 +++
 arch/x86/kvm/xen.c                            |  82 +++++
 block/bdev.c                                  |  34 ++
 block/genhd.c                                 |  13 +
 block/partitions/core.c                       |  26 ++
 drivers/acpi/bus.c                            |   4 +
 drivers/acpi/osl.c                            | 151 +++++++++
 drivers/acpi/scan.c                           |   4 +
 drivers/block/virtio_blk.c                    |  35 ++
 drivers/clocksource/arm_arch_timer.c          |  45 +++
 drivers/cpuidle/cpuidle.c                     |   7 +
 drivers/cpuidle/poll_state.c                  |  10 +
 drivers/irqchip/irq-gic-v3.c                  |  16 +
 drivers/irqchip/irq-gic-v4.c                  |   5 +
 drivers/net/virtio_net.c                      |  53 +++
 drivers/nvme/host/core.c                      | 188 +++++++++++
 drivers/nvme/host/nvme.h                      |  28 ++
 drivers/nvme/host/pci.c                       | 234 +++++++++++++
 drivers/pci/hotplug/acpiphp_glue.c            | 114 +++++++
 drivers/pci/pci.c                             |  18 +
 drivers/pci/probe.c                           |  14 +
 drivers/pci/setup-bus.c                       | 300 +++++++++++++++++
 drivers/pci/setup-res.c                       | 125 +++++++
 drivers/scsi/scsi_common.c                    |  13 +
 drivers/scsi/scsi_scan.c                      |   4 +
 drivers/scsi/virtio_scsi.c                    |  91 ++++++
 drivers/vhost/net.c                           |   4 +
 drivers/vhost/scsi.c                          |  39 +++
 drivers/vhost/vhost.c                         | 100 ++++++
 drivers/vhost/vhost.h                         |  56 ++++
 drivers/virtio/virtio_ring.c                  |  90 +++++
 include/kvm/arm_vgic.h                        |  59 ++++
 include/linux/blk-mq.h                        |  31 ++
 include/linux/kvm_host.h                      |  19 ++
 include/linux/kvm_irqfd.h                     |   7 +
 include/linux/perf_event.h                    |   7 +
 include/uapi/linux/pci_regs.h                 |  15 +
 kernel/async.c                                |  58 ++++
 kernel/entry/kvm.c                            |   6 +
 kernel/events/core.c                          |  84 +++++
 kernel/panic.c                                |  13 +
 kernel/sched/idle.c                           |  24 ++
 kernel/stop_machine.c                         |  12 +
 kernel/watchdog.c                             | 176 +++++++++-
 kernel/watchdog_perf.c                        | 149 +++++++++
 mm/filemap.c                                  |  11 +
 .../selftests/kvm/include/guest_modes.h       |  18 +
 .../selftests/kvm/include/kvm_util_base.h     | 210 ++++++++++++
 .../selftests/kvm/kvm_page_table_test.c       | 166 ++++++++++
 tools/testing/selftests/kvm/lib/elf.c         |  39 +++
 tools/testing/selftests/kvm/lib/guest_modes.c |  90 +++++
 tools/testing/selftests/kvm/lib/io.c          |   7 +
 tools/testing/selftests/kvm/lib/kvm_util.c    | 307 ++++++++++++++++++
 tools/testing/selftests/kvm/lib/sparsebit.c   |   9 +
 .../testing/selftests/kvm/lib/ucall_common.c  |  73 +++++
 .../selftests/kvm/lib/x86_64/processor.c      |  58 ++++
 .../testing/selftests/kvm/lib/x86_64/ucall.c  |   8 +
 .../kvm/memslot_modification_stress_test.c    |   4 +
 .../selftests/kvm/set_memory_region_test.c    |  16 +
 tools/testing/selftests/kvm/steal_time.c      | 119 +++++++
 .../selftests/kvm/x86_64/kvm_clock_test.c     |  10 +
 .../selftests/kvm/x86_64/nx_huge_pages_test.c |  64 ++++
 virt/kvm/eventfd.c                            |  70 ++++
 virt/kvm/irqchip.c                            |   3 +
 virt/kvm/kvm_main.c                           |  62 ++++
 117 files changed, 6374 insertions(+), 1 deletion(-)

diff --git a/arch/arm64/include/asm/arch_timer.h b/arch/arm64/include/asm/arch_timer.h
index 934c658ee..090dcc5ec 100644
--- a/arch/arm64/include/asm/arch_timer.h
+++ b/arch/arm64/include/asm/arch_timer.h
@@ -201,6 +201,9 @@ static __always_inline u64 __arch_counter_get_cntvct(void)
 {
 	u64 cnt;
 
+	/*
+	 * 虚拟化应该是PhysicalCountInt() - CNTVOFF_EL2
+	 */
 	asm volatile(ALTERNATIVE("isb\n mrs %0, cntvct_el0",
 				 "nop\n" __mrs_s("%0", SYS_CNTVCTSS_EL0),
 				 ARM64_HAS_ECV)
diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h
index 3d6725ff0..5a2f7d4e9 100644
--- a/arch/arm64/include/asm/kvm_emulate.h
+++ b/arch/arm64/include/asm/kvm_emulate.h
@@ -294,6 +294,12 @@ static inline bool vcpu_mode_priv(const struct kvm_vcpu *vcpu)
 
 static __always_inline u64 kvm_vcpu_get_esr(const struct kvm_vcpu *vcpu)
 {
+	/*
+	 * ESR_EL2, Exception Syndrome Register (EL2)
+	 * EC, bits [31:26]
+	 *   Exception Class. Indicates the reason for the exception
+	 *   that this register holds information about.
+	 */
 	return vcpu->arch.fault.esr_el2;
 }
 
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index af06ccb7e..868271186 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -43,6 +43,17 @@
 
 #define KVM_REQ_SLEEP \
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_IRQ_PENDING:
+ *   - arch/arm64/kvm/arm.c|806| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/arm.c|1169| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|509| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|388| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|437| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|755| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|1112| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ */
 #define KVM_REQ_IRQ_PENDING	KVM_ARCH_REQ(1)
 #define KVM_REQ_VCPU_RESET	KVM_ARCH_REQ(2)
 #define KVM_REQ_RECORD_STEAL	KVM_ARCH_REQ(3)
@@ -155,6 +166,15 @@ struct kvm_s2_mmu {
 	 * for vEL1/EL0 with vHCR_EL2.VM == 0.  In that case, we use the
 	 * canonical stage-2 page tables.
 	 */
+	/*
+	 * 在以下使用kvm_s2_mmu->pgd_phys:
+	 *   - arch/arm64/include/asm/kvm_mmu.h|289| <<kvm_get_vttbr>> baddr = mmu->pgd_phys;
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|157| <<kvm_host_prepare_stage2>> mmu->pgd_phys = __hyp_pa(host_mmu.pgt.pgd);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|264| <<kvm_guest_prepare_stage2>> vm->kvm.arch.mmu.pgd_phys = __hyp_pa(vm->pgt.pgd);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|276| <<reclaim_guest_pages>> vm->kvm.arch.mmu.pgd_phys = 0ULL;
+	 *   - arch/arm64/kvm/mmu.c|925| <<kvm_init_stage2_mmu>> mmu->pgd_phys = __pa(pgt->pgd);
+	 *   - arch/arm64/kvm/mmu.c|1017| <<kvm_free_stage2_pgd>> mmu->pgd_phys = 0;
+	 */
 	phys_addr_t	pgd_phys;
 	struct kvm_pgtable *pgt;
 
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index 96a80e8f6..ea3a65d15 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -280,12 +280,21 @@ static inline int kvm_write_guest_lock(struct kvm *kvm, gpa_t gpa,
  * path, we rely on a previously issued DSB so that page table updates
  * and VMID reads are correctly ordered.
  */
+/*
+ * called by:
+ *   - arch/arm64/include/asm/kvm_mmu.h|303| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|297| <<__pkvm_prot_finalize>> params->vttbr = kvm_get_vttbr(mmu);
+ */
 static __always_inline u64 kvm_get_vttbr(struct kvm_s2_mmu *mmu)
 {
 	struct kvm_vmid *vmid = &mmu->vmid;
 	u64 vmid_field, baddr;
 	u64 cnp = system_supports_cnp() ? VTTBR_CNP_BIT : 0;
 
+	/*
+	 * struct kvm_s2_mmu *mmu:
+	 * -> phys_addr_t pgd_phys;
+	 */
 	baddr = mmu->pgd_phys;
 	vmid_field = atomic64_read(&vmid->id) << VTTBR_VMID_SHIFT;
 	vmid_field &= VTTBR_VMID_MASK(kvm_arm_vmid_bits);
@@ -296,10 +305,34 @@ static __always_inline u64 kvm_get_vttbr(struct kvm_s2_mmu *mmu)
  * Must be called from hyp code running at EL2 with an updated VTTBR
  * and interrupts disabled.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|89| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|310| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|306| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|65| <<__tlb_switch_to_guest>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|227| <<__kvm_vcpu_run_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|56| <<__tlb_switch_to_guest>> __load_stage2(mmu, mmu->arch);
+ */
 static __always_inline void __load_stage2(struct kvm_s2_mmu *mmu,
 					  struct kvm_arch *arch)
 {
+	/*
+	 * struct kvm_arch *arch:
+	 * -> u64 vtcr
+	 */
 	write_sysreg(arch->vtcr, vtcr_el2);
+	/*
+	 * called by:
+	 *   - arch/arm64/include/asm/kvm_mmu.h|303| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|297| <<__pkvm_prot_finalize>> params->vttbr = kvm_get_vttbr(mmu);
+	 *
+	 * 在以下使用vttbr_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|113| <<global>> .macro __init_el2_stage2 msr vttbr_el2, xzr
+	 *   - arch/arm64/include/asm/kvm_mmu.h|303| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|91| <<__load_host_stage2>> write_sysreg(0, vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|69| <<__tlb_switch_to_host>> write_sysreg(0, vttbr_el2);
+	 */
 	write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
 
 	/*
diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 0fcc4eb1a..cef5caffc 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -155,6 +155,10 @@ static const char *const btypes[] = {
 };
 #undef bstr
 
+/*
+ * called by:
+ *   - arch/arm64/kernel/process.c|216| <<__show_regs>> print_pstate(regs);
+ */
 static void print_pstate(struct pt_regs *regs)
 {
 	u64 pstate = regs->pstate;
@@ -197,6 +201,14 @@ static void print_pstate(struct pt_regs *regs)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kernel/entry-common.c|297| <<__panic_unhandled>> __show_regs(regs);
+ *   - arch/arm64/kernel/process.c|245| <<show_regs>> __show_regs(regs);
+ *   - arch/arm64/kernel/traps.c|263| <<arm64_show_signal>> __show_regs(regs);
+ *   - arch/arm64/kernel/traps.c|943| <<panic_bad_stack>> __show_regs(regs);
+ *   - arch/arm64/kernel/traps.c|961| <<arm64_serror_panic>> __show_regs(regs);
+ */
 void __show_regs(struct pt_regs *regs)
 {
 	int i, top_reg;
diff --git a/arch/arm64/kvm/arch_timer.c b/arch/arm64/kvm/arch_timer.c
index a1e24228a..ee68e4c31 100644
--- a/arch/arm64/kvm/arch_timer.c
+++ b/arch/arm64/kvm/arch_timer.c
@@ -24,13 +24,25 @@
 #include "trace.h"
 
 static struct timecounter *timecounter;
+/*
+ * 在以下设置host_vtimer_irq:
+ *   - arch/arm64/kvm/arch_timer.c|1332| <<kvm_irq_init>> host_vtimer_irq = info->virtual_irq;
+ */
 static unsigned int host_vtimer_irq;
+/*
+ * 在以下设置host_ptimer_irq:
+ *   - arch/arm64/kvm/arch_timer.c|1359| <<kvm_irq_init>> host_ptimer_irq = info->physical_irq;
+ */
 static unsigned int host_ptimer_irq;
 static u32 host_vtimer_irq_flags;
 static u32 host_ptimer_irq_flags;
 
 static DEFINE_STATIC_KEY_FALSE(has_gic_active_state);
 
+/*
+ * 在以下使用default_ppi[]:
+ *   - arch/arm64/kvm/arch_timer.c|1034| <<kvm_timer_init_vm>> kvm->arch.timer_data.ppi[i] = default_ppi[i];
+ */
 static const u8 default_ppi[] = {
 	[TIMER_PTIMER]  = 30,
 	[TIMER_VTIMER]  = 27,
@@ -63,6 +75,13 @@ static int nr_timers(struct kvm_vcpu *vcpu)
 	return NR_KVM_TIMERS;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|283| <<kvm_timer_irq_can_fire>> ((timer_get_ctl(timer_ctx) &
+ *   - arch/arm64/kvm/arch_timer.c|626| <<timer_restore_state>> write_sysreg_el0(timer_get_ctl(ctx), SYS_CNTV_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|636| <<timer_restore_state>> write_sysreg_el0(timer_get_ctl(ctx), SYS_CNTP_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|1102| <<read_timer_ctl>> u32 ctl = timer_get_ctl(timer);
+ */
 u32 timer_get_ctl(struct arch_timer_context *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -82,6 +101,15 @@ u32 timer_get_ctl(struct arch_timer_context *ctxt)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|276| <<kvm_timer_compute_delta>> return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
+ *   - arch/arm64/kvm/arch_timer.c|417| <<kvm_timer_should_fire>> cval = timer_get_cval(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|624| <<timer_restore_state>> write_sysreg_el0(timer_get_cval(ctx), SYS_CNTV_CVAL);
+ *   - arch/arm64/kvm/arch_timer.c|630| <<timer_restore_state>> cval = timer_get_cval(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|1143| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1152| <<kvm_arm_timer_read>> val = timer_get_cval(timer);
+ */
 u64 timer_get_cval(struct arch_timer_context *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -101,6 +129,18 @@ u64 timer_get_cval(struct arch_timer_context *ctxt)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|259| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|418| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|546| <<timer_save_state>> cval -= timer_get_offset(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|623| <<timer_restore_state>> set_cntvoff(timer_get_offset(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|631| <<timer_restore_state>> offset = timer_get_offset(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|794| <<timer_set_traps>> if (!has_cntpoff() && timer_get_offset(map->direct_ptimer))
+ *   - arch/arm64/kvm/arch_timer.c|1143| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1156| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1202| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ */
 static u64 timer_get_offset(struct arch_timer_context *ctxt)
 {
 	u64 offset = 0;
@@ -116,6 +156,13 @@ static u64 timer_get_offset(struct arch_timer_context *ctxt)
 	return offset;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|651| <<timer_save_state>> timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTV_CTL));
+ *   - arch/arm64/kvm/arch_timer.c|677| <<timer_save_state>> timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTP_CTL));
+ *   - arch/arm64/kvm/arch_timer.c|1114| <<kvm_timer_vcpu_reset>> timer_set_ctl(vcpu_get_timer(vcpu, i), 0);
+ *   - arch/arm64/kvm/arch_timer.c|1399| <<kvm_arm_timer_write>> timer_set_ctl(timer, val & ~ARCH_TIMER_CTRL_IT_STAT);
+ */
 static void timer_set_ctl(struct arch_timer_context *ctxt, u32 ctl)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -160,6 +207,13 @@ static void timer_set_cval(struct arch_timer_context *ctxt, u64 cval)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1023| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_vtimer(vcpu), kvm_phys_timer_read());
+ *   - arch/arm64/kvm/arch_timer.c|1024| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_ptimer(vcpu), 0);
+ *   - arch/arm64/kvm/arch_timer.c|1064| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1079| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ */
 static void timer_set_offset(struct arch_timer_context *ctxt, u64 offset)
 {
 	if (!ctxt->offset.vm_offset) {
@@ -167,14 +221,48 @@ static void timer_set_offset(struct arch_timer_context *ctxt, u64 offset)
 		return;
 	}
 
+	/*
+	 * struct arch_timer_context *ctxt:
+	 * -> struct arch_timer_offset offset;
+	 *    -> u64     *vm_offset;
+	 *    -> u64     *vcpu_offset;
+	 */
 	WRITE_ONCE(*ctxt->offset.vm_offset, offset);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|259| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|418| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|1023| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_vtimer(vcpu), kvm_phys_timer_read());
+ *   - arch/arm64/kvm/arch_timer.c|1064| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1079| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1143| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1156| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1202| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ */
 u64 kvm_phys_timer_read(void)
 {
+	/*
+	 * static struct timecounter *timecounter;
+	 * -> const struct cyclecounter *cc;
+	 */
 	return timecounter->cc->read(timecounter->cc);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|302| <<kvm_arch_timer_handler>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|667| <<kvm_timer_blocking>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|936| <<kvm_timer_vcpu_load>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|987| <<kvm_timer_vcpu_put>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1045| <<kvm_timer_vcpu_reset>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1298| <<kvm_arm_timer_read_sysreg>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1350| <<kvm_arm_timer_write_sysreg>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1670| <<kvm_timer_enable>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|54| <<__activate_traps>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|124| <<__deactivate_traps>> get_timer_map(vcpu, &map);
+ */
 void get_timer_map(struct kvm_vcpu *vcpu, struct timer_map *map)
 {
 	if (vcpu_has_nv(vcpu)) {
@@ -190,7 +278,13 @@ void get_timer_map(struct kvm_vcpu *vcpu, struct timer_map *map)
 			map->emul_ptimer = vcpu_hptimer(vcpu);
 		}
 	} else if (has_vhe()) {
+		/*
+		 * (&(v)->arch.timer_cpu.timers[TIMER_VTIMER])
+		 */
 		map->direct_vtimer = vcpu_vtimer(vcpu);
+		/*
+		 * (&(v)->arch.timer_cpu.timers[TIMER_PTIMER])
+		 */
 		map->direct_ptimer = vcpu_ptimer(vcpu);
 		map->emul_vtimer = NULL;
 		map->emul_ptimer = NULL;
@@ -210,6 +304,11 @@ static inline bool userspace_irqchip(struct kvm *kvm)
 		unlikely(!irqchip_in_kernel(kvm));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|574| <<timer_emulate>> soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|684| <<kvm_timer_blocking>> soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
+ */
 static void soft_timer_start(struct hrtimer *hrt, u64 ns)
 {
 	hrtimer_start(hrt, ktime_add_ns(ktime_get(), ns),
@@ -221,6 +320,11 @@ static void soft_timer_cancel(struct hrtimer *hrt)
 	hrtimer_cancel(hrt);
 }
 
+/*
+ * 在以下使用kvm_arch_timer_handler():
+ *   - arch/arm64/kvm/arch_timer.c|1389| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_vtimer_irq, kvm_arch_timer_handler, "kvm guest vtimer", kvm_get_running_vcpus());
+ *   - arch/arm64/kvm/arch_timer.c|1413| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_ptimer_irq, kvm_arch_timer_handler, "kvm guest ptimer", kvm_get_running_vcpus());
+ */
 static irqreturn_t kvm_arch_timer_handler(int irq, void *dev_id)
 {
 	struct kvm_vcpu *vcpu = *(struct kvm_vcpu **)dev_id;
@@ -328,6 +432,10 @@ static u64 kvm_timer_earliest_exp(struct kvm_vcpu *vcpu)
 	return min_delta;
 }
 
+/*
+ * bg_timer的到期执行函数,当需要调用kvm_vcpu_block让vcpu睡眠时,
+ * 需要先启动bg_timer,bg_timer到期时再将vcpu唤醒;
+ */
 static enum hrtimer_restart kvm_bg_timer_expire(struct hrtimer *hrt)
 {
 	struct arch_timer_cpu *timer;
@@ -352,6 +460,15 @@ static enum hrtimer_restart kvm_bg_timer_expire(struct hrtimer *hrt)
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+ * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+ * 然后再次进入guest时,就有中断了.
+ *
+ * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+ * timer到期的时候(还在host mode)唤醒guest并inject irq.
+ * KVM注册了一个hrtimer来处理这种情况.
+ */
 static enum hrtimer_restart kvm_hrtimer_expire(struct hrtimer *hrt)
 {
 	struct arch_timer_context *ctx;
@@ -443,6 +560,16 @@ void kvm_timer_update_run(struct kvm_vcpu *vcpu)
 		regs->device_irq_level |= KVM_ARM_DEV_EL1_PTIMER;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|247| <<kvm_arch_timer_handler>> kvm_timer_update_irq(vcpu, true, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|377| <<kvm_hrtimer_expire>> kvm_timer_update_irq(vcpu, true, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|472| <<timer_emulate>> kvm_timer_update_irq(ctx->vcpu, should_fire, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|667| <<kvm_timer_vcpu_load_gic>> kvm_timer_update_irq(ctx->vcpu, kvm_timer_should_fire(ctx), ctx);
+ *   - arch/arm64/kvm/arch_timer.c|687| <<kvm_timer_vcpu_load_nogic>> kvm_timer_update_irq(vcpu, kvm_timer_should_fire(vtimer), vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|920| <<unmask_vtimer_irq_user>> kvm_timer_update_irq(vcpu, false, vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|969| <<kvm_timer_vcpu_reset>> kvm_timer_update_irq(vcpu, false,
+ */
 static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 				 struct arch_timer_context *timer_ctx)
 {
@@ -453,6 +580,14 @@ static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 				   timer_ctx->irq.level);
 
 	if (!userspace_irqchip(vcpu->kvm)) {
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/arch_timer.c|456| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+		 *   - arch/arm64/kvm/arm.c|1195| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/arm.c|1203| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, 0, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/pmu-emul.c|351| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, 0, spi_id, level, NULL);
+		 */
 		ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
 					  timer_irq(timer_ctx),
 					  timer_ctx->irq.level,
@@ -461,6 +596,12 @@ static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 	}
 }
 
+/*
+ * 似乎一般的硬件虚拟化是不用的:
+ *   - arch/arm64/kvm/arch_timer.c|955| <<kvm_timer_vcpu_load>> timer_emulate(map.emul_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|957| <<kvm_timer_vcpu_load>> timer_emulate(map.emul_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1355| <<kvm_arm_timer_write_sysreg>> timer_emulate(timer);
+ */
 /* Only called for a fully emulated timer */
 static void timer_emulate(struct arch_timer_context *ctx)
 {
@@ -601,6 +742,13 @@ static void kvm_timer_unblocking(struct kvm_vcpu *vcpu)
 	soft_timer_cancel(&timer->bg_timer);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|850| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|852| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1189| <<kvm_arm_timer_read_sysreg>> timer_restore_state(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1240| <<kvm_arm_timer_write_sysreg>> timer_restore_state(timer);
+ */
 static void timer_restore_state(struct arch_timer_context *ctx)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(ctx->vcpu);
@@ -762,6 +910,10 @@ static void kvm_timer_vcpu_load_nested_switch(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1003| <<kvm_timer_vcpu_load>> timer_set_traps(vcpu, &map);
+ */
 static void timer_set_traps(struct kvm_vcpu *vcpu, struct timer_map *map)
 {
 	bool tpt, tpc;
@@ -817,13 +969,62 @@ static void timer_set_traps(struct kvm_vcpu *vcpu, struct timer_map *map)
 	set = 0;
 	clr = 0;
 
+	/*
+	 * - EL0PCTEN, bit [0]
+	 *
+	 * When HCR_EL2.TGE is 0, this control does not cause any instructions to be
+	 * trapped.
+	 * When HCR_EL2.TGE is 1, traps EL0 accesses to the frequency register and
+	 * physical counter register to EL2, e.g., CNTPCT_EL0.
+	 *
+	 * - EL0VCTEN, bit [1]
+	 *
+	 * When HCR_EL2.TGE is 0, this control does not cause any instructions to be
+	 * trapped.
+	 * When HCR_EL2.TGE is 1, traps EL0 accesses to the frequency register and virtual
+	 * counter register to EL2, e.g., CNTVCT_EL0.
+	 */
 	assign_clear_set_bit(tpt, CNTHCTL_EL1PCEN << 10, set, clr);
 	assign_clear_set_bit(tpc, CNTHCTL_EL1PCTEN << 10, set, clr);
 
+	/*
+	 * 在以下使用cnthctl_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|56| <<global>> msr cnthctl_el2, x0
+	 *   - arch/arm64/kvm/arch_timer.c|1007| <<timer_set_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 *   - arch/arm64/kvm/arch_timer.c|1817| <<kvm_timer_init_vhe>> sysreg_clear_set(cnthctl_el2, 0, CNTHCTL_ECV);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|31| <<__timer_disable_traps>> val = read_sysreg(cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|33| <<__timer_disable_traps>> write_sysreg(val, cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|61| <<__timer_enable_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 *
+	 * - EL1NVVCT, bit [16]
+	 * Traps EL1 accesses to the specified EL1 virtual timer registers
+	 * using the EL02 descriptors to EL2, when EL2 is enabled for the
+	 * current Security state.
+	 *
+	 * - EL1NVPCT, bit [15]
+	 * Traps EL1 accesses to the specified EL1 physical timer registers
+	 * using the EL02 descriptors to EL2, when EL2 is enabled for the
+	 * current Security state.
+	 *
+	 *
+	 * - EL1TVT: 如果是0就不会trap任何
+	 *
+	 * - EL1PTEN: 如果是1或者HCR_EL2.TGE=1, 就不会trap任何
+	 *            如果是0(没配置), 就会trap一些寄存器, 比如CNTP_CTL_EL0
+	 */
+
+	/*
+	 * Modify bits in a sysreg. Bits in the clear mask are zeroed, then bits in the
+	 * set mask are set. Other bits are left as-is.
+	 */
 	/* This only happens on VHE, so use the CNTHCTL_EL2 accessor. */
 	sysreg_clear_set(cnthctl_el2, clr, set);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|465| <<kvm_arch_vcpu_load>> kvm_timer_vcpu_load(vcpu);
+ */
 void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1048,6 +1249,10 @@ void kvm_timer_cpu_down(void)
 		disable_percpu_irq(host_ptimer_irq);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|639| <<set_timer_reg>> return kvm_arm_timer_set_reg(vcpu, reg->id, val);
+ */
 int kvm_arm_timer_set_reg(struct kvm_vcpu *vcpu, u64 regid, u64 value)
 {
 	struct arch_timer_context *timer;
@@ -1091,6 +1296,10 @@ int kvm_arm_timer_set_reg(struct kvm_vcpu *vcpu, u64 regid, u64 value)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1308| <<kvm_arm_timer_read>> val = read_timer_ctl(timer);
+ */
 static u64 read_timer_ctl(struct arch_timer_context *timer)
 {
 	/*
@@ -1132,6 +1341,25 @@ u64 kvm_arm_timer_get_reg(struct kvm_vcpu *vcpu, u64 regid)
 	return (u64)-1;
 }
 
+/*
+ * enum kvm_arch_timer_regs {
+ *     TIMER_REG_CNT,
+ *     TIMER_REG_CVAL,
+ *     TIMER_REG_TVAL,
+ *     TIMER_REG_CTL, 
+ *     TIMER_REG_VOFF,
+ * };
+ *
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1114| <<kvm_arm_timer_get_reg(KVM_REG_ARM_TIMER_CTL)>> return kvm_arm_timer_read(vcpu, vcpu_vtimer(vcpu), TIMER_REG_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|1117| <<kvm_arm_timer_get_reg(KVM_REG_ARM_TIMER_CNT)>> return kvm_arm_timer_read(vcpu, vcpu_vtimer(vcpu), TIMER_REG_CNT);
+ *   - arch/arm64/kvm/arch_timer.c|1120| <<kvm_arm_timer_get_reg(KVM_REG_ARM_TIMER_CVAL)>> return kvm_arm_timer_read(vcpu, vcpu_vtimer(vcpu), TIMER_REG_CVAL);
+ *   - arch/arm64/kvm/arch_timer.c|1123| <<kvm_arm_timer_get_reg(KVM_REG_ARM_PTIMER_CTL)>> return kvm_arm_timer_read(vcpu, vcpu_ptimer(vcpu), TIMER_REG_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|1126| <<kvm_arm_timer_get_reg(KVM_REG_ARM_PTIMER_CNT)>> return kvm_arm_timer_read(vcpu, vcpu_ptimer(vcpu), TIMER_REG_CNT);
+ *   - arch/arm64/kvm/arch_timer.c|1129| <<kvm_arm_timer_get_reg(KVM_REG_ARM_PTIMER_CVAL)>> return kvm_arm_timer_read(vcpu, vcpu_ptimer(vcpu), TIMER_REG_CVAL);
+ *   - arch/arm64/kvm/arch_timer.c|1182| <<kvm_arm_timer_read_sysreg>> return kvm_arm_timer_read(vcpu, timer, treg);
+ *   - arch/arm64/kvm/arch_timer.c|1187| <<kvm_arm_timer_read_sysreg>> val = kvm_arm_timer_read(vcpu, timer, treg);
+ */
 static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 			      struct arch_timer_context *timer,
 			      enum kvm_arch_timer_regs treg)
@@ -1192,6 +1420,15 @@ u64 kvm_arm_timer_read_sysreg(struct kvm_vcpu *vcpu,
 	return val;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1199| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CTL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1210| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CVAL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1214| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CTL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1225| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CVAL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1394| <<kvm_arm_timer_write_sysreg>> kvm_arm_timer_write(vcpu, timer, treg, val);
+ *   - arch/arm64/kvm/arch_timer.c|1399| <<kvm_arm_timer_write_sysreg>> kvm_arm_timer_write(vcpu, timer, treg, val);
+ */
 static void kvm_arm_timer_write(struct kvm_vcpu *vcpu,
 				struct arch_timer_context *timer,
 				enum kvm_arch_timer_regs treg,
@@ -1219,6 +1456,10 @@ static void kvm_arm_timer_write(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1204| <<access_arch_timer>> kvm_arm_timer_write_sysreg(vcpu, tmr, treg, p->regval);
+ */
 void kvm_arm_timer_write_sysreg(struct kvm_vcpu *vcpu,
 				enum kvm_arch_timers tmr,
 				enum kvm_arch_timer_regs treg,
@@ -1319,6 +1560,10 @@ static void kvm_irq_fixup_flags(unsigned int virq, u32 *flags)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1383| <<kvm_timer_hyp_init>> err = kvm_irq_init(info);
+ */
 static int kvm_irq_init(struct arch_timer_kvm_info *info)
 {
 	struct irq_domain *domain = NULL;
@@ -1367,12 +1612,27 @@ static int kvm_irq_init(struct arch_timer_kvm_info *info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2059| <<init_subsystems>> err = kvm_timer_hyp_init(vgic_present);
+ */
 int __init kvm_timer_hyp_init(bool has_gic)
 {
 	struct arch_timer_kvm_info *info;
 	int err;
 
+	/*
+	 * struct arch_timer_kvm_info {
+	 *     struct timecounter timecounter;
+	 *     int virtual_irq;
+	 *     int physical_irq;
+	 * };
+	 */
 	info = arch_timer_get_kvm_info();
+	/*
+	 * 这个文件的静态变量
+	 * static struct timecounter *timecounter;
+	 */
 	timecounter = &info->timecounter;
 
 	if (!timecounter->cc) {
@@ -1445,6 +1705,10 @@ int __init kvm_timer_hyp_init(bool has_gic)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|416| <<kvm_arch_vcpu_destroy>> kvm_timer_vcpu_terminate(vcpu);
+ */
 void kvm_timer_vcpu_terminate(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1506,6 +1770,10 @@ static bool kvm_arch_timer_get_input_level(int vintid)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|632| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_timer_enable(vcpu);
+ */
 int kvm_timer_enable(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1552,9 +1820,26 @@ int kvm_timer_enable(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1869| <<cpu_hyp_init_features>> kvm_timer_init_vhe();
+ */
 /* If we have CNTPOFF, permanently set ECV to enable it */
 void kvm_timer_init_vhe(void)
 {
+	/*
+	 * 在以下使用cnthctl_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|56| <<global>> msr cnthctl_el2, x0
+	 *   - arch/arm64/kvm/arch_timer.c|1007| <<timer_set_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 *   - arch/arm64/kvm/arch_timer.c|1817| <<kvm_timer_init_vhe>> sysreg_clear_set(cnthctl_el2, 0, CNTHCTL_ECV);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|31| <<__timer_disable_traps>> val = read_sysreg(cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|33| <<__timer_disable_traps>> write_sysreg(val, cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|61| <<__timer_enable_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 */
+	/*
+	 * Modify bits in a sysreg. Bits in the clear mask are zeroed, then bits in the
+	 * set mask are set. Other bits are left as-is.
+	 */
 	if (cpus_have_final_cap(ARM64_HAS_ECV_CNTPOFF))
 		sysreg_clear_set(cnthctl_el2, 0, CNTHCTL_ECV);
 }
@@ -1611,6 +1896,10 @@ int kvm_arm_timer_set_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 	return ret;
 }
 
+/*
+ * 处理KVM_ARM_SET_COUNTER_OFFSET:
+ *   - arch/arm64/kvm/arm.c|1634| <<kvm_arch_vm_ioctl>> return kvm_vm_ioctl_set_counter_offset(kvm, &offset);
+ */
 int kvm_arm_timer_get_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 {
 	int __user *uaddr = (int __user *)(long)attr->addr;
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index 4866b3f7b..8383bd564 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -133,6 +133,10 @@ static int kvm_arm_default_max_vcpus(void)
  * kvm_arch_init_vm - initializes a VM data structure
  * @kvm:	pointer to the KVM struct
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1222| <<kvm_create_vm>> r = kvm_arch_init_vm(kvm, type);
+ */
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
 	int ret;
@@ -161,6 +165,10 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	}
 	cpumask_copy(kvm->arch.supported_cpus, cpu_possible_mask);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arm.c|164| <<kvm_arch_init_vm>> ret = kvm_init_stage2_mmu(kvm, &kvm->arch.mmu, type);
+	 */
 	ret = kvm_init_stage2_mmu(kvm, &kvm->arch.mmu, type);
 	if (ret)
 		goto err_free_cpumask;
@@ -421,6 +429,14 @@ void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu)
 
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/emulate-nested.c|1947| <<kvm_emulate_nested_eret>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - arch/arm64/kvm/emulate-nested.c|2028| <<kvm_inject_nested>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - arch/arm64/kvm/reset.c|300| <<kvm_reset_vcpu>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - virt/kvm/kvm_main.c|216| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5963| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct kvm_s2_mmu *mmu;
@@ -705,6 +721,12 @@ static void kvm_vcpu_sleep(struct kvm_vcpu *vcpu)
  * the vCPU is runnable.  The vCPU may or may not be scheduled out, depending
  * on when a wake event arrives, e.g. there may already be a pending wake event.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|756| <<kvm_vcpu_suspend>> kvm_vcpu_wfi(vcpu);
+ *   - arch/arm64/kvm/handle_exit.c|147| <<kvm_handle_wfx>> kvm_vcpu_wfi(vcpu);
+ *   - arch/arm64/kvm/psci.c|49| <<kvm_psci_vcpu_suspend>> kvm_vcpu_wfi(vcpu);
+ */
 void kvm_vcpu_wfi(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -774,6 +796,10 @@ static int kvm_vcpu_suspend(struct kvm_vcpu *vcpu)
  *	   < 0 if we should exit to userspace, where the return value indicates
  *	   an error
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|975| <<kvm_arch_vcpu_ioctl_run>> ret = check_vcpu_requests(vcpu);
+ */
 static int check_vcpu_requests(struct kvm_vcpu *vcpu)
 {
 	if (kvm_request_pending(vcpu)) {
@@ -783,6 +809,17 @@ static int check_vcpu_requests(struct kvm_vcpu *vcpu)
 		if (kvm_check_request(KVM_REQ_VCPU_RESET, vcpu))
 			kvm_reset_vcpu(vcpu);
 
+		/*
+		 * 在以下使用KVM_REQ_IRQ_PENDING:
+		 *   - arch/arm64/kvm/arm.c|806| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/arm.c|1169| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/pmu-emul.c|509| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|388| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|437| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|755| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|1112| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 */
 		/*
 		 * Clear IRQ_PENDING requests that were made to guarantee
 		 * that a VCPU sees new virtual interrupts.
@@ -842,6 +879,10 @@ static bool vcpu_mode_is_bad_32bit(struct kvm_vcpu *vcpu)
  * true. For an exit to preemptible + interruptible kernel context (i.e. check
  * for pending work and re-enter), return true without writing to ret.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1000| <<kvm_arch_vcpu_ioctl_run>> if (ret <= 0 || kvm_vcpu_exit_request(vcpu, &ret)) {
+ */
 static bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu, int *ret)
 {
 	struct kvm_run *run = vcpu->run;
@@ -870,6 +911,15 @@ static bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu, int *ret)
 		return true;
 	}
 
+	/*
+	 * xfer_to_guest_mode_work_pending - Check if work is pending which needs to be
+	 *                                   handled before returning to guest mode
+	 *
+	 * Returns: True if work pending, False otherwise.
+	 *
+	 * Has to be invoked with interrupts disabled before the transition to
+	 * guest mode.
+	 */
 	return kvm_request_pending(vcpu) ||
 			xfer_to_guest_mode_work_pending();
 }
@@ -881,6 +931,10 @@ static bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu, int *ret)
  * This must be noinstr as instrumentation may make use of RCU, and this is not
  * safe during the EQS.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1008| <<kvm_arch_vcpu_ioctl_run>> ret = kvm_arm_vcpu_enter_exit(vcpu);
+ */
 static int noinstr kvm_arm_vcpu_enter_exit(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -902,17 +956,54 @@ static int noinstr kvm_arm_vcpu_enter_exit(struct kvm_vcpu *vcpu)
  * return with return value 0 and with the kvm_run structure filled in with the
  * required data for the requested emulation.
  */
+/*
+ * kvm_arch_vcpu_ioctl_run()
+ * -> handle_exit()
+ *    -> handle_trap_exceptions(vcpu)
+ *       -> kvm_get_exit_handler()
+ *          -> arm_exit_handlers[esr_ec]
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|4175| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *run = vcpu->run;
 	int ret;
 
+	/*
+	 * 设置run->exit_reason为KVM_EXIT_MMIO
+	 *   - arch/arm64/kvm/mmio.c|199| <<io_mem_abort>> run->exit_reason = KVM_EXIT_MMIO;
+	 */
 	if (run->exit_reason == KVM_EXIT_MMIO) {
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/arm.c|935| <<kvm_arch_vcpu_ioctl_run>> ret = kvm_handle_mmio_return(vcpu);
+		 *   - arch/arm64/kvm/mmio.c|187| <<io_mem_abort>> kvm_handle_mmio_return(vcpu);
+		 *
+		 * kvm_handle_mmio_return -- Handle MMIO loads after user space emulation
+		 *                           or in-kernel IO emulatio
+		 */
 		ret = kvm_handle_mmio_return(vcpu);
 		if (ret)
 			return ret;
 	}
 
+	/*
+	 * 关于arm64的调用:
+	 *   - 只在这里!!!
+	 *
+	 * 关于kvm_arch_vcpu_load()的调用:
+	 *   - arch/arm64/kvm/emulate-nested.c|1947| <<kvm_emulate_nested_eret>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+	 *   - arch/arm64/kvm/emulate-nested.c|2028| <<kvm_inject_nested>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+	 *   - arch/arm64/kvm/reset.c|300| <<kvm_reset_vcpu>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+	 *   - virt/kvm/kvm_main.c|216| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+	 *   - virt/kvm/kvm_main.c|5963| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+	 *
+	 * vcpu_load()
+	 * -> kvm_arch_vcpu_load()
+	 *    -> kvm_vcpu_load_sysregs_vhe()
+	 */
 	vcpu_load(vcpu);
 
 	if (run->immediate_exit) {
@@ -920,6 +1011,15 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		goto out;
 	}
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arm.c|939| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/mips/kvm/mips.c|431| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/powerpc/kvm/powerpc.c|1859| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/riscv/kvm/vcpu.c|664| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/s390/kvm/kvm-s390.c|5065| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/x86/kvm/x86.c|11207| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 */
 	kvm_sigset_activate(vcpu);
 
 	ret = 1;
@@ -933,6 +1033,15 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		if (!ret)
 			ret = 1;
 
+		/*
+		 * check_vcpu_requests - check and handle pending vCPU requests
+		 * @vcpu:       the VCPU pointer
+		 *
+		 * Return: 1 if we should enter the guest
+		 *         0 if we should exit to userspace
+		 *         < 0 if we should exit to userspace, where the return value indicates
+		 *         an error
+		 */
 		if (ret > 0)
 			ret = check_vcpu_requests(vcpu);
 
@@ -956,6 +1065,17 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 
 		local_irq_disable();
 
+		/*
+		 * 这里是关于中断的!!!!
+		 *
+		 * called by:
+		 *   - arch/arm64/kvm/arm.c|988| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_flush_hwstate(vcpu);
+		 *
+		 * 对于gicv3核心思想分为两步. (还有gicv4的)
+		 *
+		 * 1. 对于gicv3, 写入vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+		 * 2. 对于gicv3, 把vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr]写入寄存器
+		 */
 		kvm_vgic_flush_hwstate(vcpu);
 
 		kvm_pmu_update_vcpu_events(vcpu);
@@ -989,6 +1109,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		trace_kvm_entry(*vcpu_pc(vcpu));
 		guest_timing_enter_irqoff();
 
+		/*
+		 * __kvm_vcpu_run()
+		 */
 		ret = kvm_arm_vcpu_enter_exit(vcpu);
 
 		vcpu->mode = OUTSIDE_GUEST_MODE;
@@ -1069,6 +1192,15 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 			ret = ARM_EXCEPTION_IL;
 		}
 
+		/*
+		 * kvm_arch_vcpu_ioctl_run()
+		 * -> handle_exit()
+		 *    -> handle_trap_exceptions(vcpu)
+		 *       -> kvm_get_exit_handler()
+		 *          -> arm_exit_handlers[esr_ec]
+		 *
+		 * 只有ret > 0, while循环才会进行下去
+		 */
 		ret = handle_exit(vcpu, ret);
 	}
 
@@ -1879,6 +2011,10 @@ static void cpu_hyp_uninit(void *discard)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5214| <<__hardware_enable_nolock>> if (kvm_arch_hardware_enable()) {
+ */
 int kvm_arch_hardware_enable(void)
 {
 	/*
diff --git a/arch/arm64/kvm/guest.c b/arch/arm64/kvm/guest.c
index 95f6945c4..eec4883ef 100644
--- a/arch/arm64/kvm/guest.c
+++ b/arch/arm64/kvm/guest.c
@@ -626,6 +626,10 @@ static int copy_timer_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|797| <<kvm_arm_set_reg>> return set_timer_reg(vcpu, reg);
+ */
 static int set_timer_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)
 {
 	void __user *uaddr = (void __user *)(long)reg->addr;
diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 617ae6dea..6f9f331f1 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -252,6 +252,10 @@ static int handle_svc(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用arm_exit_handlers[]:
+ *   - arch/arm64/kvm/handle_exit.c|288| <<kvm_get_exit_handler>> return arm_exit_handlers[esr_ec];
+ */
 static exit_handle_fn arm_exit_handlers[] = {
 	[0 ... ESR_ELx_EC_MAX]	= kvm_handle_unknown_ec,
 	[ESR_ELx_EC_WFx]	= kvm_handle_wfx,
@@ -280,11 +284,18 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_PAC]	= kvm_handle_ptrauth,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|311| <<handle_trap_exceptions>> exit_handler = kvm_get_exit_handler(vcpu);
+ */
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 {
 	u64 esr = kvm_vcpu_get_esr(vcpu);
 	u8 esr_ec = ESR_ELx_EC(esr);
 
+	/*
+	 * 只在此处使用
+	 */
 	return arm_exit_handlers[esr_ec];
 }
 
@@ -294,6 +305,10 @@ static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
  * KVM_EXIT_DEBUG, otherwise userspace needs to complete its
  * emulation first.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|342| <<handle_exit>> return handle_trap_exceptions(vcpu);
+ */
 static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
 {
 	int handled;
@@ -319,6 +334,16 @@ static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
  * Return > 0 to return to guest, < 0 on error, 0 (and set exit_reason) on
  * proper exit to userspace.
  */
+/*
+ * kvm_arch_vcpu_ioctl_run()
+ * -> handle_exit()
+ *    -> handle_trap_exceptions(vcpu)
+ *       -> kvm_get_exit_handler()
+ *          -> arm_exit_handlers[esr_ec]
+ *
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1140| <<kvm_arch_vcpu_ioctl_run>> ret = handle_exit(vcpu, ret);
+ */
 int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 {
 	struct kvm_run *run = vcpu->run;
@@ -362,6 +387,10 @@ int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1117| <<kvm_arch_vcpu_ioctl_run>> handle_exit_early(vcpu, ret);
+ */
 /* For exit types that need handling before we can be preempted */
 void handle_exit_early(struct kvm_vcpu *vcpu, int exception_index)
 {
diff --git a/arch/arm64/kvm/hyp/include/hyp/switch.h b/arch/arm64/kvm/hyp/include/hyp/switch.h
index 9cfe6bd1d..3fa79da4f 100644
--- a/arch/arm64/kvm/hyp/include/hyp/switch.h
+++ b/arch/arm64/kvm/hyp/include/hyp/switch.h
@@ -154,6 +154,10 @@ static inline void __activate_traps_hfgxtr(struct kvm_vcpu *vcpu)
 	write_sysreg_s(w_val, SYS_HDFGWTR_EL2);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|233| <<__deactivate_traps_common>> __deactivate_traps_hfgxtr(vcpu);
+ */
 static inline void __deactivate_traps_hfgxtr(struct kvm_vcpu *vcpu)
 {
 	struct kvm_cpu_context *hctxt = &this_cpu_ptr(&kvm_host_data)->host_ctxt;
@@ -214,6 +218,11 @@ static inline void __activate_traps_common(struct kvm_vcpu *vcpu)
 	__activate_traps_hfgxtr(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|107| <<__deactivate_traps>> __deactivate_traps_common(vcpu);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|179| <<deactivate_traps_vhe_put>> __deactivate_traps_common(vcpu);
+ */
 static inline void __deactivate_traps_common(struct kvm_vcpu *vcpu)
 {
 	write_sysreg(vcpu->arch.mdcr_el2_host, mdcr_el2);
@@ -233,8 +242,20 @@ static inline void __deactivate_traps_common(struct kvm_vcpu *vcpu)
 	__deactivate_traps_hfgxtr(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|43| <<__activate_traps>> ___activate_traps(vcpu);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|40| <<__activate_traps>> ___activate_traps(vcpu);
+ */
 static inline void ___activate_traps(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> u64 hcr_el2;
+	 *    -> u64 mdcr_el2;
+	 *    -> u64 cptr_el2;
+	 */
 	u64 hcr = vcpu->arch.hcr_el2;
 
 	if (cpus_have_final_cap(ARM64_WORKAROUND_CAVIUM_TX2_219_TVM))
@@ -522,6 +543,17 @@ static bool handle_ampere1_tcr(struct kvm_vcpu *vcpu)
 	return true;
 }
 
+/*
+ * 处理ESR_ELx_EC_SYS64.
+ * EC == 0b011000
+ * When AArch64 is supported: Trapped MSR, MRS or System instruction execution
+ * in AArch64 state, that is not reported using EC 0b000000, 0b000001 or
+ * 0b000111.
+ * This includes all instructions that cause exceptions that are part of the
+ * encoding space defined in System instruction class encoding overview on page
+ * C5-731, except for those exceptions reported using EC values 0b000000,
+ * 0b000001, or 0b000111.
+ */
 static bool kvm_hyp_handle_sysreg(struct kvm_vcpu *vcpu, u64 *exit_code)
 {
 	if (cpus_have_final_cap(ARM64_WORKAROUND_CAVIUM_TX2_219_TVM) &&
@@ -642,6 +674,11 @@ static inline void synchronize_vcpu_pstate(struct kvm_vcpu *vcpu, u64 *exit_code
  * the guest, false when we should restore the host state and return to the
  * main run loop.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|319| <<__kvm_vcpu_run>> } while (fixup_guest_exit(vcpu, &exit_code));
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|297| <<__kvm_vcpu_run_vhe>> } while (fixup_guest_exit(vcpu, &exit_code));
+ */
 static inline bool fixup_guest_exit(struct kvm_vcpu *vcpu, u64 *exit_code)
 {
 	/*
@@ -702,6 +739,13 @@ static inline void __kvm_unexpected_el2_exception(void)
 	extern char __guest_exit_panic[];
 	unsigned long addr, fixup;
 	struct kvm_exception_table_entry *entry, *end;
+	/*
+	 * c在以下使用elr_el2:
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|737| <<__kvm_unexpected_el2_exception>> unsigned long elr_el2 = read_sysreg(elr_el2);
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|746| <<__kvm_unexpected_el2_exception>> if (addr != elr_el2) {
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|751| <<__kvm_unexpected_el2_exception>> write_sysreg(fixup, elr_el2);
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|756| <<__kvm_unexpected_el2_exception>> write_sysreg(__guest_exit_panic, elr_el2);
+	 */
 	unsigned long elr_el2 = read_sysreg(elr_el2);
 
 	entry = &__start___kvm_ex_table;
diff --git a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
index bb6b571ec..731bc676e 100644
--- a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
+++ b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
@@ -37,6 +37,11 @@ static inline bool ctxt_has_mte(struct kvm_cpu_context *ctxt)
 	return kvm_has_mte(kern_hyp_va(vcpu->kvm));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/sysreg-sr.c|23| <<__sysreg_save_state_nvhe>> __sysreg_save_el1_state(ctxt);
+ *   - arch/arm64/kvm/hyp/vhe/sysreg-sr.c|118| <<kvm_vcpu_put_sysregs_vhe>> __sysreg_save_el1_state(guest_ctxt);
+ */
 static inline void __sysreg_save_el1_state(struct kvm_cpu_context *ctxt)
 {
 	ctxt_sys_reg(ctxt, SCTLR_EL1)	= read_sysreg_el1(SYS_SCTLR);
@@ -97,6 +102,11 @@ static inline void __sysreg_restore_user_state(struct kvm_cpu_context *ctxt)
 	write_sysreg(ctxt_sys_reg(ctxt, TPIDRRO_EL0),	tpidrro_el0);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/sysreg-sr.c|31| <<__sysreg_restore_state_nvhe>> __sysreg_restore_el1_state(ctxt);
+ *   - arch/arm64/kvm/hyp/vhe/sysreg-sr.c|92| <<kvm_vcpu_load_sysregs_vhe>> __sysreg_restore_el1_state(guest_ctxt);
+ */
 static inline void __sysreg_restore_el1_state(struct kvm_cpu_context *ctxt)
 {
 	write_sysreg(ctxt_sys_reg(ctxt, MPIDR_EL1),	vmpidr_el2);
diff --git a/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h b/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
index 0972faccc..083ef237e 100644
--- a/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
+++ b/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
@@ -85,6 +85,13 @@ int refill_memcache(struct kvm_hyp_memcache *mc, unsigned long min_pages,
 
 static __always_inline void __load_host_stage2(void)
 {
+	/*
+	 * 在以下使用vttbr_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|113| <<global>> .macro __init_el2_stage2 msr vttbr_el2, xzr
+	 *   - arch/arm64/include/asm/kvm_mmu.h|303| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|91| <<__load_host_stage2>> write_sysreg(0, vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|69| <<__tlb_switch_to_host>> write_sysreg(0, vttbr_el2);
+	 */
 	if (static_branch_likely(&kvm_protected_mode_initialized))
 		__load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
 	else
diff --git a/arch/arm64/kvm/hyp/nvhe/timer-sr.c b/arch/arm64/kvm/hyp/nvhe/timer-sr.c
index 3aaab20ae..8236e15d1 100644
--- a/arch/arm64/kvm/hyp/nvhe/timer-sr.c
+++ b/arch/arm64/kvm/hyp/nvhe/timer-sr.c
@@ -27,6 +27,15 @@ void __timer_disable_traps(struct kvm_vcpu *vcpu)
 	if (has_hvhe())
 		shift = 10;
 
+	/*
+	 * 在以下使用cnthctl_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|56| <<global>> msr cnthctl_el2, x0
+	 *   - arch/arm64/kvm/arch_timer.c|1007| <<timer_set_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 *   - arch/arm64/kvm/arch_timer.c|1817| <<kvm_timer_init_vhe>> sysreg_clear_set(cnthctl_el2, 0, CNTHCTL_ECV);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|31| <<__timer_disable_traps>> val = read_sysreg(cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|33| <<__timer_disable_traps>> write_sysreg(val, cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|61| <<__timer_enable_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 */
 	/* Allow physical timer/counter access for the host */
 	val = read_sysreg(cnthctl_el2);
 	val |= (CNTHCTL_EL1PCTEN | CNTHCTL_EL1PCEN) << shift;
@@ -58,5 +67,14 @@ void __timer_enable_traps(struct kvm_vcpu *vcpu)
 		set <<= 10;
 	}
 
+	/*
+	 * 在以下使用cnthctl_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|56| <<global>> msr cnthctl_el2, x0
+	 *   - arch/arm64/kvm/arch_timer.c|1007| <<timer_set_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 *   - arch/arm64/kvm/arch_timer.c|1817| <<kvm_timer_init_vhe>> sysreg_clear_set(cnthctl_el2, 0, CNTHCTL_ECV);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|31| <<__timer_disable_traps>> val = read_sysreg(cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|33| <<__timer_disable_traps>> write_sysreg(val, cnthctl_el2);
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|61| <<__timer_enable_traps>> sysreg_clear_set(cnthctl_el2, clr, set);
+	 */
 	sysreg_clear_set(cnthctl_el2, clr, set);
 }
diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
index 6cb638b18..83426a83b 100644
--- a/arch/arm64/kvm/hyp/vgic-v3-sr.c
+++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
@@ -231,6 +231,11 @@ void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|129| <<__hyp_vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ *   - arch/arm64/kvm/vgic/vgic.c|971| <<vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ */
 void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 used_lrs = cpu_if->used_lrs;
@@ -257,6 +262,11 @@ void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|128| <<__hyp_vgic_restore_state>> __vgic_v3_activate_traps(&vcpu->arch.vgic_cpu.vgic_v3);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|735| <<vgic_v3_load>> __vgic_v3_activate_traps(cpu_if);
+ */
 void __vgic_v3_activate_traps(struct vgic_v3_cpu_if *cpu_if)
 {
 	/*
diff --git a/arch/arm64/kvm/hyp/vhe/switch.c b/arch/arm64/kvm/hyp/vhe/switch.c
index 448b17080..35c115605 100644
--- a/arch/arm64/kvm/hyp/vhe/switch.c
+++ b/arch/arm64/kvm/hyp/vhe/switch.c
@@ -31,12 +31,32 @@
 /* VHE specific context */
 DEFINE_PER_CPU(struct kvm_host_data, kvm_host_data);
 DEFINE_PER_CPU(struct kvm_cpu_context, kvm_hyp_ctxt);
+/*
+ * 在以下使用percpu的kvm_hyp_vector:
+ *   - arch/arm64/include/asm/kvm_hyp.h|16| <<global>> DECLARE_PER_CPU(unsigned long , kvm_hyp_vector);
+ *   - arch/arm64/kvm/arm.c|50| <<global>> DECLARE_KVM_HYP_PER_CPU(unsigned long , kvm_hyp_vector);
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|35| <<global>> DEFINE_PER_CPU(unsigned long , kvm_hyp_vector);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|34| <<global>> DEFINE_PER_CPU(unsigned long , kvm_hyp_vector);
+ *   - arch/arm64/kvm/arm.c|1850| <<cpu_set_hyp_vector>> *this_cpu_ptr_hyp_sym(kvm_hyp_vector) = (unsigned long )vector;
+ *   - arch/arm64/kvm/hyp/nvhe/mm.c|205| <<pkvm_cpu_set_vector>> *this_cpu_ptr(&kvm_hyp_vector) = (unsigned long )vector;
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|67| <<__activate_traps>> write_sysreg(__this_cpu_read(kvm_hyp_vector), vbar_el2);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|97| <<__activate_traps>> write_sysreg(__this_cpu_read(kvm_hyp_vector), vbar_el1);
+ */
 DEFINE_PER_CPU(unsigned long, kvm_hyp_vector);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|228| <<__kvm_vcpu_run_vhe>> __activate_traps(vcpu);
+ */
 static void __activate_traps(struct kvm_vcpu *vcpu)
 {
 	u64 val;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/hyp/nvhe/switch.c|43| <<__activate_traps>> ___activate_traps(vcpu);
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|40| <<__activate_traps>> ___activate_traps(vcpu);
+	 */
 	___activate_traps(vcpu);
 
 	if (has_cntpoff()) {
@@ -89,12 +109,23 @@ static void __activate_traps(struct kvm_vcpu *vcpu)
 }
 NOKPROBE_SYMBOL(__activate_traps);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|249| <<__kvm_vcpu_run_vhe>> __deactivate_traps(vcpu);
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|304| <<__hyp_call_panic>> __deactivate_traps(vcpu);
+ */
 static void __deactivate_traps(struct kvm_vcpu *vcpu)
 {
 	const char *host_vectors = vectors;
 
 	___deactivate_traps(vcpu);
 
+	/*
+	 * 在以下使用HCR_HOST_VHE_FLAGS:
+	 *   - arch/arm64/include/asm/kvm_arm.h|103| <<global>> #define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|98| <<__deactivate_traps>> write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|77| <<__tlb_switch_to_host>> write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
+	 */
 	write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
 
 	if (has_cntpoff()) {
@@ -207,8 +238,18 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 	struct kvm_cpu_context *guest_ctxt;
 	u64 exit_code;
 
+	/*
+	 * struct kvm_host_data {
+	 *     struct kvm_cpu_context host_ctxt;
+	 * };
+	 */
 	host_ctxt = &this_cpu_ptr(&kvm_host_data)->host_ctxt;
 	host_ctxt->__hyp_running_vcpu = vcpu;
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_cpu_context ctxt;
+	 */
 	guest_ctxt = &vcpu->arch.ctxt;
 
 	sysreg_save_host_state_vhe(host_ctxt);
@@ -224,7 +265,15 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 	 * __load_stage2 configures stage 2 translation, and
 	 * __activate_traps clear HCR_EL2.TGE (among other things).
 	 */
+	/*
+	 * 按照上面的erratum, 之前:
+	 * kvm_arch_vcpu_load()
+	 * -> kvm_vcpu_load_sysregs_vhe()
+	 */
 	__load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+	/*
+	 * 只在这里调用
+	 */
 	__activate_traps(vcpu);
 
 	__kvm_adjust_pc(vcpu);
@@ -238,6 +287,9 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 		vcpu_clear_flag(vcpu, VCPU_HYP_CONTEXT);
 
 	do {
+		/*
+		 * arch/arm64/kvm/hyp/entry.S
+		 */
 		/* Jump in the fire! */
 		exit_code = __guest_enter(vcpu);
 
@@ -246,6 +298,11 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 
 	sysreg_save_guest_state_vhe(guest_ctxt);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|249| <<__kvm_vcpu_run_vhe>> __deactivate_traps(vcpu);
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|304| <<__hyp_call_panic>> __deactivate_traps(vcpu);
+	 */
 	__deactivate_traps(vcpu);
 
 	sysreg_restore_host_state_vhe(host_ctxt);
@@ -259,6 +316,10 @@ static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 }
 NOKPROBE_SYMBOL(__kvm_vcpu_run_vhe);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|905| <<kvm_arm_vcpu_enter_exit>> ret = kvm_call_hyp_ret(__kvm_vcpu_run, vcpu);
+ */
 int __kvm_vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int ret;
diff --git a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
index b35a178e7..f6dbdb17e 100644
--- a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
+++ b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
@@ -25,6 +25,10 @@
  * classes are handled as part of kvm_arch_vcpu_load and kvm_arch_vcpu_put.
  */
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|245| <<__kvm_vcpu_run_vhe>> sysreg_save_host_state_vhe(host_ctxt);
+ */
 void sysreg_save_host_state_vhe(struct kvm_cpu_context *ctxt)
 {
 	__sysreg_save_common_state(ctxt);
@@ -44,6 +48,10 @@ void sysreg_restore_host_state_vhe(struct kvm_cpu_context *ctxt)
 }
 NOKPROBE_SYMBOL(sysreg_restore_host_state_vhe);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|271| <<__kvm_vcpu_run_vhe>> sysreg_restore_guest_state_vhe(guest_ctxt);
+ */
 void sysreg_restore_guest_state_vhe(struct kvm_cpu_context *ctxt)
 {
 	__sysreg_restore_common_state(ctxt);
@@ -62,8 +70,28 @@ NOKPROBE_SYMBOL(sysreg_restore_guest_state_vhe);
  * and loading system register state early avoids having to load them on
  * every entry to the VM.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|459| <<kvm_arch_vcpu_load>> kvm_vcpu_load_sysregs_vhe(vcpu);
+ */
 void kvm_vcpu_load_sysregs_vhe(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_cpu_context {
+	 *     struct user_pt_regs regs;       // sp = sp_el0
+	 *
+	 *     u64     spsr_abt;
+	 *     u64     spsr_und;
+	 *     u64     spsr_irq;
+	 *     u64     spsr_fiq;
+	 *     
+	 *     struct user_fpsimd_state fp_regs;
+	 *
+	 *     u64 sys_regs[NR_SYS_REGS];
+	 *
+	 *     struct kvm_vcpu *__hyp_running_vcpu;
+	 * };
+	 */
 	struct kvm_cpu_context *guest_ctxt = &vcpu->arch.ctxt;
 	struct kvm_cpu_context *host_ctxt;
 
@@ -107,6 +135,10 @@ void kvm_vcpu_load_sysregs_vhe(struct kvm_vcpu *vcpu)
  * and deferring saving system register state until we're no longer running the
  * VCPU avoids having to save them on every exit from the VM.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|483| <<kvm_arch_vcpu_put>> kvm_vcpu_put_sysregs_vhe(vcpu);
+ */
 void kvm_vcpu_put_sysregs_vhe(struct kvm_vcpu *vcpu)
 {
 	struct kvm_cpu_context *guest_ctxt = &vcpu->arch.ctxt;
diff --git a/arch/arm64/kvm/hyp/vhe/tlb.c b/arch/arm64/kvm/hyp/vhe/tlb.c
index 46bd43f61..3de722ffa 100644
--- a/arch/arm64/kvm/hyp/vhe/tlb.c
+++ b/arch/arm64/kvm/hyp/vhe/tlb.c
@@ -66,7 +66,20 @@ static void __tlb_switch_to_host(struct tlb_inv_context *cxt)
 	 * We're done with the TLB operation, let's restore the host's
 	 * view of HCR_EL2.
 	 */
+	/*
+	 * 在以下使用vttbr_el2:
+	 *   - arch/arm64/include/asm/el2_setup.h|113| <<global>> .macro __init_el2_stage2 msr vttbr_el2, xzr
+	 *   - arch/arm64/include/asm/kvm_mmu.h|303| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|91| <<__load_host_stage2>> write_sysreg(0, vttbr_el2);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|69| <<__tlb_switch_to_host>> write_sysreg(0, vttbr_el2);
+	 */
 	write_sysreg(0, vttbr_el2);
+	/*
+	 * 在以下使用HCR_HOST_VHE_FLAGS:
+	 *   - arch/arm64/include/asm/kvm_arm.h|103| <<global>> #define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|98| <<__deactivate_traps>> write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|77| <<__tlb_switch_to_host>> write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
+	 */
 	write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
 	isb();
 
diff --git a/arch/arm64/kvm/mmio.c b/arch/arm64/kvm/mmio.c
index 3dd38a151..942abc7b6 100644
--- a/arch/arm64/kvm/mmio.c
+++ b/arch/arm64/kvm/mmio.c
@@ -78,6 +78,11 @@ unsigned long kvm_mmio_read_buf(const void *buf, unsigned int len)
  *
  * @vcpu: The VCPU pointer
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|935| <<kvm_arch_vcpu_ioctl_run>> ret = kvm_handle_mmio_return(vcpu);
+ *   - arch/arm64/kvm/mmio.c|187| <<io_mem_abort>> kvm_handle_mmio_return(vcpu);
+ */
 int kvm_handle_mmio_return(struct kvm_vcpu *vcpu)
 {
 	unsigned long data;
@@ -120,6 +125,10 @@ int kvm_handle_mmio_return(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1762| <<kvm_handle_guest_abort>> ret = io_mem_abort(vcpu, fault_ipa);
+ */
 int io_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
 {
 	struct kvm_run *run = vcpu->run;
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index 482280fe2..e07fc80e5 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -863,6 +863,10 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
  * Note we don't need locking here as this is only called when the VM is
  * created, which can only be done once.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|164| <<kvm_arch_init_vm>> ret = kvm_init_stage2_mmu(kvm, &kvm->arch.mmu, type);
+ */
 int kvm_init_stage2_mmu(struct kvm *kvm, struct kvm_s2_mmu *mmu, unsigned long type)
 {
 	u32 kvm_ipa_limit = get_kvm_ipa_limit();
@@ -899,6 +903,10 @@ int kvm_init_stage2_mmu(struct kvm *kvm, struct kvm_s2_mmu *mmu, unsigned long t
 		return -EINVAL;
 	}
 
+	/*
+	 * struct kvm_pgtable *pgt;
+	 * -> kvm_pteref_t pgd;
+	 */
 	pgt = kzalloc(sizeof(*pgt), GFP_KERNEL_ACCOUNT);
 	if (!pgt)
 		return -ENOMEM;
@@ -922,6 +930,15 @@ int kvm_init_stage2_mmu(struct kvm *kvm, struct kvm_s2_mmu *mmu, unsigned long t
 	mmu->split_page_cache.gfp_zero = __GFP_ZERO;
 
 	mmu->pgt = pgt;
+	/*
+	 * 在以下使用kvm_s2_mmu->pgd_phys:
+	 *   - arch/arm64/include/asm/kvm_mmu.h|289| <<kvm_get_vttbr>> baddr = mmu->pgd_phys;
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|157| <<kvm_host_prepare_stage2>> mmu->pgd_phys = __hyp_pa(host_mmu.pgt.pgd);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|264| <<kvm_guest_prepare_stage2>> vm->kvm.arch.mmu.pgd_phys = __hyp_pa(vm->pgt.pgd);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|276| <<reclaim_guest_pages>> vm->kvm.arch.mmu.pgd_phys = 0ULL;
+	 *   - arch/arm64/kvm/mmu.c|925| <<kvm_init_stage2_mmu>> mmu->pgd_phys = __pa(pgt->pgd);
+	 *   - arch/arm64/kvm/mmu.c|1017| <<kvm_free_stage2_pgd>> mmu->pgd_phys = 0;
+	 */
 	mmu->pgd_phys = __pa(pgt->pgd);
 	return 0;
 
@@ -1635,6 +1652,11 @@ static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
  * space. The distinction is based on the IPA causing the fault and whether this
  * memory region has been registered as standard RAM by user space.
  */
+/*
+ * called by (static exit_handle_fn arm_exit_handlers[]):
+ *   - arch/arm64/kvm/handle_exit.c|272| <<global>> [ESR_ELx_EC_IABT_LOW] = kvm_handle_guest_abort,
+ *   - arch/arm64/kvm/handle_exit.c|273| <<global>> [ESR_ELx_EC_DABT_LOW] = kvm_handle_guest_abort,
+ */
 int kvm_handle_guest_abort(struct kvm_vcpu *vcpu)
 {
 	unsigned long fault_status;
diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
index 6b066e04d..e34f10a81 100644
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@ -102,6 +102,15 @@ static u64 kvm_pmu_get_pmc_value(struct kvm_pmc *pmc)
 	reg = counter_index_to_reg(pmc->idx);
 	counter = __vcpu_sys_reg(vcpu, reg);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|110| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|213| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|437| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/x86/kvm/pmu.h|71| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 *   - tools/include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 */
 	/*
 	 * The real counter value is equal to the value of counter register plus
 	 * the value perf event counts.
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index 0afd6136e..df62d5f1f 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -295,6 +295,15 @@ static bool access_actlr(struct kvm_vcpu *vcpu,
  * The cp15_64 code makes sure this automatically works
  * for both AArch64 and AArch32 accesses.
  */
+/*
+ * 在以下使用access_gic_sgi():
+ *   - arch/arm64/kvm/sys_regs.c|2156| <<global>> { SYS_DESC(SYS_ICC_SGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2157| <<global>> { SYS_DESC(SYS_ICC_ASGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2158| <<global>> { SYS_DESC(SYS_ICC_SGI0R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2758| <<global>> { Op1( 0), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2761| <<global>> { Op1( 1), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2762| <<global>> { Op1( 2), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ */
 static bool access_gic_sgi(struct kvm_vcpu *vcpu,
 			   struct sys_reg_params *p,
 			   const struct sys_reg_desc *r)
@@ -1164,6 +1173,19 @@ static unsigned int ptrauth_visibility(const struct kvm_vcpu *vcpu,
 	__PTRAUTH_KEY(k ## KEYLO_EL1),					\
 	__PTRAUTH_KEY(k ## KEYHI_EL1)
 
+/*
+ * 在以下使用access_arch_timer():
+ *   - arch/arm64/kvm/sys_regs.c|2285| <<global>> { SYS_DESC(SYS_CNTPCT_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2286| <<global>> { SYS_DESC(SYS_CNTPCTSS_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2287| <<global>> { SYS_DESC(SYS_CNTP_TVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2288| <<global>> { SYS_DESC(SYS_CNTP_CTL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2289| <<global>> { SYS_DESC(SYS_CNTP_CVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2663| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_TVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2664| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CTL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2746| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCT), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2750| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2751| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCTSS), access_arch_timer },
+ */
 static bool access_arch_timer(struct kvm_vcpu *vcpu,
 			      struct sys_reg_params *p,
 			      const struct sys_reg_desc *r)
diff --git a/arch/arm64/kvm/vgic/vgic-irqfd.c b/arch/arm64/kvm/vgic/vgic-irqfd.c
index 475059bac..680bb13a4 100644
--- a/arch/arm64/kvm/vgic/vgic-irqfd.c
+++ b/arch/arm64/kvm/vgic/vgic-irqfd.c
@@ -66,6 +66,11 @@ int kvm_set_routing_entry(struct kvm *kvm,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|97| <<kvm_set_msi>> kvm_populate_msi(e, &msi);
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|118| <<kvm_arch_set_irq_inatomic>> kvm_populate_msi(e, &msi);
+ */
 static void kvm_populate_msi(struct kvm_kernel_irq_routing_entry *e,
 			     struct kvm_msi *msi)
 {
@@ -82,6 +87,11 @@ static void kvm_populate_msi(struct kvm_kernel_irq_routing_entry *e,
  * This is the entry point for irqfd MSI injection
  * and userspace MSI injection.
  */
+/*
+ * 在以下使用kvm_set_msi():
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|54| <<kvm_set_routing_entry>> e->set = kvm_set_msi;
+ *   - virt/kvm/irqchip.c|61| <<kvm_send_userspace_msi>> return kvm_set_msi(&route, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false);
+ */
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 		struct kvm *kvm, int irq_source_id,
 		int level, bool line_status)
@@ -95,12 +105,19 @@ int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 		return -1;
 
 	kvm_populate_msi(e, &msi);
+	/*
+	 * 只在这里调用
+	 */
 	return vgic_its_inject_msi(kvm, &msi);
 }
 
 /**
  * kvm_arch_set_irq_inatomic: fast-path for irqfd injection
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|247| <<irqfd_wakeup>> if (kvm_arch_set_irq_inatomic(&irq, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false) == -EWOULDBLOCK)
+ */
 int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 			      struct kvm *kvm, int irq_source_id, int level,
 			      bool line_status)
@@ -115,7 +132,17 @@ int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 		if (!vgic_has_its(kvm))
 			break;
 
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|97| <<kvm_set_msi>> kvm_populate_msi(e, &msi);
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|118| <<kvm_arch_set_irq_inatomic>> kvm_populate_msi(e, &msi);
+		 */
 		kvm_populate_msi(e, &msi);
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|119| <<kvm_arch_set_irq_inatomic>> return vgic_its_inject_cached_translation(kvm, &msi);
+		 *   - arch/arm64/kvm/vgic/vgic-its.c|781| <<vgic_its_inject_msi>> if (!vgic_its_inject_cached_translation(kvm, msi))
+		 */
 		return vgic_its_inject_cached_translation(kvm, &msi);
 	}
 
diff --git a/arch/arm64/kvm/vgic/vgic-its.c b/arch/arm64/kvm/vgic/vgic-its.c
index 5fe2365a6..47e284440 100644
--- a/arch/arm64/kvm/vgic/vgic-its.c
+++ b/arch/arm64/kvm/vgic/vgic-its.c
@@ -545,12 +545,31 @@ static unsigned long vgic_mmio_read_its_idregs(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|587| <<vgic_its_check_cache>> irq = __vgic_its_check_cache(dist, db, devid, eventid);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|617| <<vgic_its_cache_translation>> if (__vgic_its_check_cache(dist, db, devid, eventid))
+ */
 static struct vgic_irq *__vgic_its_check_cache(struct vgic_dist *dist,
 					       phys_addr_t db,
 					       u32 devid, u32 eventid)
 {
 	struct vgic_translation_cache_entry *cte;
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1894| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1908| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1920| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 */
 	list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
 		/*
 		 * If we hit a NULL entry, there is nothing after this
@@ -576,6 +595,10 @@ static struct vgic_irq *__vgic_its_check_cache(struct vgic_dist *dist,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|759| <<vgic_its_inject_cached_translation>> irq = vgic_its_check_cache(kvm, db, msi->devid, msi->data);
+ */
 static struct vgic_irq *vgic_its_check_cache(struct kvm *kvm, phys_addr_t db,
 					     u32 devid, u32 eventid)
 {
@@ -590,6 +613,10 @@ static struct vgic_irq *vgic_its_check_cache(struct kvm *kvm, phys_addr_t db,
 	return irq;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|712| <<vgic_its_resolve_lpi>> vgic_its_cache_translation(kvm, its, devid, eventid, ite->irq);
+ */
 static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 				       u32 devid, u32 eventid,
 				       struct vgic_irq *irq)
@@ -617,6 +644,20 @@ static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 	if (__vgic_its_check_cache(dist, db, devid, eventid))
 		goto out;
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1894| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1908| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1920| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 */
 	/* Always reuse the last entry (LRU policy) */
 	cte = list_last_entry(&dist->lpi_translation_cache,
 			      typeof(*cte), entry);
@@ -631,6 +672,9 @@ static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 
 	vgic_get_irq_kref(irq);
 
+	/*
+	 * struct vgic_translation_cache_entry *cte;
+	 */
 	cte->db		= db;
 	cte->devid	= devid;
 	cte->eventid	= eventid;
@@ -643,6 +687,17 @@ static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 	raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|893| <<vgic_its_cmd_handle_discard>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|930| <<vgic_its_cmd_handle_movi>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1189| <<vgic_its_free_device>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1296| <<vgic_its_cmd_handle_mapc>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1469| <<vgic_its_cmd_handle_movall>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1822| <<vgic_mmio_write_its_ctlr>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1964| <<vgic_lpi_translation_cache_destroy>> vgic_its_invalidate_cache(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|280| <<vgic_mmio_write_v3r_ctlr>> vgic_its_invalidate_cache(vcpu->kvm);
+ */
 void vgic_its_invalidate_cache(struct kvm *kvm)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
@@ -651,6 +706,9 @@ void vgic_its_invalidate_cache(struct kvm *kvm)
 
 	raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
 
+	/*
+	 * struct vgic_translation_cache_entry *cte;
+	 */
 	list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
 		/*
 		 * If we hit a NULL entry, there is nothing after this
@@ -666,6 +724,12 @@ void vgic_its_invalidate_cache(struct kvm *kvm)
 	raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|760| <<vgic_its_trigger_msi>> err = vgic_its_resolve_lpi(kvm, its, devid, eventid, &irq);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|438| <<kvm_vgic_v4_set_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|510| <<kvm_vgic_v4_unset_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+ */
 int vgic_its_resolve_lpi(struct kvm *kvm, struct vgic_its *its,
 			 u32 devid, u32 eventid, struct vgic_irq **irq)
 {
@@ -686,12 +750,20 @@ int vgic_its_resolve_lpi(struct kvm *kvm, struct vgic_its *its,
 	if (!vgic_lpis_enabled(vcpu))
 		return -EBUSY;
 
+	/*
+	 * 只在这里调用
+	 */
 	vgic_its_cache_translation(kvm, its, devid, eventid, ite->irq);
 
 	*irq = ite->irq;
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|822| <<vgic_its_inject_msi>> its = vgic_msi_to_its(kvm, msi);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|408| <<vgic_get_its>> return vgic_msi_to_its(kvm, &msi);
+ */
 struct vgic_its *vgic_msi_to_its(struct kvm *kvm, struct kvm_msi *msi)
 {
 	u64 address;
@@ -727,6 +799,11 @@ struct vgic_its *vgic_msi_to_its(struct kvm *kvm, struct kvm_msi *msi)
  * Returns 0 on success, a positive error value for any ITS mapping
  * related errors and negative error values for generic errors.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|827| <<vgic_its_inject_msi>> ret = vgic_its_trigger_msi(kvm, its, msi->devid, msi->data);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1485| <<vgic_its_cmd_handle_int>> return vgic_its_trigger_msi(kvm, its, msi_devid, msi_data);
+ */
 static int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,
 				u32 devid, u32 eventid)
 {
@@ -734,6 +811,12 @@ static int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,
 	unsigned long flags;
 	int err;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|760| <<vgic_its_trigger_msi>> err = vgic_its_resolve_lpi(kvm, its, devid, eventid, &irq);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|438| <<kvm_vgic_v4_set_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|510| <<kvm_vgic_v4_unset_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+	 */
 	err = vgic_its_resolve_lpi(kvm, its, devid, eventid, &irq);
 	if (err)
 		return err;
@@ -744,11 +827,19 @@ static int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,
 
 	raw_spin_lock_irqsave(&irq->irq_lock, flags);
 	irq->pending_latch = true;
+	/*
+	 * 会被特别多调用
+	 */
 	vgic_queue_irq_unlock(kvm, irq, flags);
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|119| <<kvm_arch_set_irq_inatomic>> return vgic_its_inject_cached_translation(kvm, &msi);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|781| <<vgic_its_inject_msi>> if (!vgic_its_inject_cached_translation(kvm, msi))
+ */
 int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct vgic_irq *irq;
@@ -756,12 +847,18 @@ int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
 	phys_addr_t db;
 
 	db = (u64)msi->address_hi << 32 | msi->address_lo;
+	/*
+	 * 只在这里调用
+	 */
 	irq = vgic_its_check_cache(kvm, db, msi->devid, msi->data);
 	if (!irq)
 		return -EWOULDBLOCK;
 
 	raw_spin_lock_irqsave(&irq->irq_lock, flags);
 	irq->pending_latch = true;
+	/*
+	 * 被很多的调用
+	 */
 	vgic_queue_irq_unlock(kvm, irq, flags);
 
 	return 0;
@@ -773,19 +870,38 @@ int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
  * We then call vgic_its_trigger_msi() with the decoded data.
  * According to the KVM_SIGNAL_MSI API description returns 1 on success.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|98| <<kvm_set_msi>> return vgic_its_inject_msi(kvm, &msi);
+ */
 int vgic_its_inject_msi(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct vgic_its *its;
 	int ret;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|119| <<kvm_arch_set_irq_inatomic>> return vgic_its_inject_cached_translation(kvm, &msi);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|781| <<vgic_its_inject_msi>> if (!vgic_its_inject_cached_translation(kvm, msi))
+	 */
 	if (!vgic_its_inject_cached_translation(kvm, msi))
 		return 1;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|822| <<vgic_its_inject_msi>> its = vgic_msi_to_its(kvm, msi);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|408| <<vgic_get_its>> return vgic_msi_to_its(kvm, &msi);
+	 */
 	its = vgic_msi_to_its(kvm, msi);
 	if (IS_ERR(its))
 		return PTR_ERR(its);
 
 	mutex_lock(&its->its_lock);
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|827| <<vgic_its_inject_msi>> ret = vgic_its_trigger_msi(kvm, its, msi->devid, msi->data);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1485| <<vgic_its_cmd_handle_int>> return vgic_its_trigger_msi(kvm, its, msi_devid, msi_data);
+	 */
 	ret = vgic_its_trigger_msi(kvm, its, msi->devid, msi->data);
 	mutex_unlock(&its->its_lock);
 
@@ -1885,6 +2001,11 @@ static int vgic_register_its_iodev(struct kvm *kvm, struct vgic_its *its,
 /* Default is 16 cached LPIs per vcpu */
 #define LPI_DEFAULT_PCPU_CACHE_SIZE	16
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|309| <<vgic_init>> vgic_lpi_translation_cache_init(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1994| <<vgic_its_create>> vgic_lpi_translation_cache_init(dev->kvm);
+ */
 void vgic_lpi_translation_cache_init(struct kvm *kvm)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
@@ -1909,6 +2030,10 @@ void vgic_lpi_translation_cache_init(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|365| <<kvm_vgic_dist_destroy>> vgic_lpi_translation_cache_destroy(kvm);
+ */
 void vgic_lpi_translation_cache_destroy(struct kvm *kvm)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
diff --git a/arch/arm64/kvm/vgic/vgic-mmio-v3.c b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
index 188d2187e..36eeeb17d 100644
--- a/arch/arm64/kvm/vgic/vgic-mmio-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
@@ -1066,6 +1066,10 @@ static int match_mpidr(u64 sgi_aff, u16 sgi_cpu_mask, struct kvm_vcpu *vcpu)
  * check for matching ones. If this bit is set, we signal all, but not the
  * calling VCPU.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|338| <<access_gic_sgi>> vgic_v3_dispatch_sgi(vcpu, p->regval, g1);
+ */
 void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg, bool allow_group1)
 {
 	struct kvm *kvm = vcpu->kvm;
diff --git a/arch/arm64/kvm/vgic/vgic-v3.c b/arch/arm64/kvm/vgic/vgic-v3.c
index 3dfc8b84e..cdb1f8fc1 100644
--- a/arch/arm64/kvm/vgic/vgic-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-v3.c
@@ -32,6 +32,10 @@ static bool lr_signals_eoi_mi(u64 lr_val)
 	       !(lr_val & ICH_LR_HW);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|770| <<vgic_fold_lr_state>> vgic_v3_fold_lr_state(vcpu);
+ */
 void vgic_v3_fold_lr_state(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
@@ -103,6 +107,10 @@ void vgic_v3_fold_lr_state(struct kvm_vcpu *vcpu)
 	cpuif->used_lrs = 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|782| <<vgic_populate_lr>> vgic_v3_populate_lr(vcpu, irq, lr);
+ */
 /* Requires the irq to be locked already */
 void vgic_v3_populate_lr(struct kvm_vcpu *vcpu, struct vgic_irq *irq, int lr)
 {
diff --git a/arch/arm64/kvm/vgic/vgic-v4.c b/arch/arm64/kvm/vgic/vgic-v4.c
index 339a55194..867d6da82 100644
--- a/arch/arm64/kvm/vgic/vgic-v4.c
+++ b/arch/arm64/kvm/vgic/vgic-v4.c
@@ -408,6 +408,10 @@ static struct vgic_its *vgic_get_its(struct kvm *kvm,
 	return vgic_msi_to_its(kvm, &msi);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2396| <<kvm_arch_irq_bypass_add_producer>> return kvm_vgic_v4_set_forwarding(irqfd->kvm, prod->irq,
+ */
 int kvm_vgic_v4_set_forwarding(struct kvm *kvm, int virq,
 			       struct kvm_kernel_irq_routing_entry *irq_entry)
 {
diff --git a/arch/arm64/kvm/vgic/vgic.c b/arch/arm64/kvm/vgic/vgic.c
index 8be4c1ebd..3b369b00d 100644
--- a/arch/arm64/kvm/vgic/vgic.c
+++ b/arch/arm64/kvm/vgic/vgic.c
@@ -333,6 +333,27 @@ static bool vgic_validate_injection(struct vgic_irq *irq, bool level, void *owne
  * Needs to be entered with the IRQ lock already held, but will return
  * with all locks dropped.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|301| <<update_lpi_config>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|467| <<its_sync_lpi_pending_table>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|747| <<vgic_its_trigger_msi>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|765| <<vgic_its_inject_cached_translation>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|157| <<vgic_mmio_write_sgir>> vgic_queue_irq_unlock(source_vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|264| <<vgic_mmio_write_sgipends>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|375| <<vgic_v3_uaccess_write_pending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1131| <<vgic_v3_dispatch_sgi>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|85| <<vgic_mmio_write_group>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|159| <<vgic_mmio_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|200| <<vgic_uaccess_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|341| <<vgic_mmio_write_spending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|368| <<vgic_uaccess_write_spending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|603| <<vgic_mmio_change_active>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|828| <<vgic_write_irq_line_level_info>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|338| <<vgic_v3_lpi_sync_pending_status>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|480| <<kvm_vgic_v4_set_forwarding>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic.c|483| <<kvm_vgic_inject_irq>> vgic_queue_irq_unlock(kvm, irq, flags);
+ */
 bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 			   unsigned long flags)
 {
@@ -402,6 +423,28 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 		goto retry;
 	}
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|160| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|305| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|410| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|653| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|725| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|792| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|824| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|850| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|890| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|929| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|935| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|999| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *
+	 * List of IRQs that this VCPU should consider because they are either
+	 * Active or Pending (hence the name; AP list), or because they recently
+	 * were one of the two and need to be migrated off this list to another
+	 * VCPU.
+	 */
 	/*
 	 * Grab a reference to the irq to reflect the fact that it is
 	 * now in the ap_list.
@@ -413,6 +456,17 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 	raw_spin_unlock(&irq->irq_lock);
 	raw_spin_unlock_irqrestore(&vcpu->arch.vgic_cpu.ap_list_lock, flags);
 
+	/*
+	 * 在以下使用KVM_REQ_IRQ_PENDING:
+	 *   - arch/arm64/kvm/arm.c|806| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/arm.c|1169| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/pmu-emul.c|509| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|388| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|437| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|755| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|1112| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 */
 	kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
 	kvm_vcpu_kick(vcpu);
 
@@ -436,6 +490,14 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
  * level-sensitive interrupts.  You can think of the level parameter as 1
  * being HIGH and 0 being LOW and all devices being active-HIGH.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|456| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/arm.c|1195| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level, NULL);
+ *   - arch/arm64/kvm/arm.c|1203| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, 0, irq_num, level, NULL);
+ *   - arch/arm64/kvm/pmu-emul.c|351| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, 0, spi_id, level, NULL);
+ */
 int kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int intid,
 			bool level, void *owner)
 {
@@ -733,6 +795,10 @@ static void vgic_prune_ap_list(struct kvm_vcpu *vcpu)
 	raw_spin_unlock(&vgic_cpu->ap_list_lock);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|927| <<kvm_vgic_sync_hwstate>> vgic_fold_lr_state(vcpu);
+ */
 static inline void vgic_fold_lr_state(struct kvm_vcpu *vcpu)
 {
 	if (kvm_vgic_global_state.type == VGIC_V2)
@@ -741,6 +807,10 @@ static inline void vgic_fold_lr_state(struct kvm_vcpu *vcpu)
 		vgic_v3_fold_lr_state(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|865| <<vgic_flush_lr_state>> vgic_populate_lr(vcpu, irq, count++);
+ */
 /* Requires the irq_lock to be held. */
 static inline void vgic_populate_lr(struct kvm_vcpu *vcpu,
 				    struct vgic_irq *irq, int lr)
@@ -796,6 +866,10 @@ static int compute_ap_list_depth(struct kvm_vcpu *vcpu,
 }
 
 /* Requires the VCPU's ap_list_lock to be held. */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|937| <<kvm_vgic_flush_hwstate>> vgic_flush_lr_state(vcpu);
+ */
 static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
@@ -813,6 +887,28 @@ static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 
 	count = 0;
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|160| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|305| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|410| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|653| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|725| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|792| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|824| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|850| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|890| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|929| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|935| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|999| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *
+	 * List of IRQs that this VCPU should consider because they are either
+	 * Active or Pending (hence the name; AP list), or because they recently
+	 * were one of the two and need to be migrated off this list to another
+	 * VCPU.
+	 */
 	list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
 		raw_spin_lock(&irq->irq_lock);
 
@@ -855,6 +951,11 @@ static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 		vcpu->arch.vgic_cpu.vgic_v3.used_lrs = count;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|953| <<kvm_vgic_sync_hwstate>> if (can_access_vgic_from_kernel())
+ *   - arch/arm64/kvm/vgic/vgic.c|1009| <<kvm_vgic_flush_hwstate>> if (can_access_vgic_from_kernel())
+ */
 static inline bool can_access_vgic_from_kernel(void)
 {
 	/*
@@ -873,6 +974,11 @@ static inline void vgic_save_state(struct kvm_vcpu *vcpu)
 		__vgic_v3_save_state(&vcpu->arch.vgic_cpu.vgic_v3);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1019| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_sync_hwstate(vcpu);
+ *   - arch/arm64/kvm/arm.c|1056| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_sync_hwstate(vcpu);
+ */
 /* Sync back the hardware VGIC state into our emulation after a guest's run. */
 void kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)
 {
@@ -895,6 +1001,10 @@ void kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)
 	vgic_prune_ap_list(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|1010| <<kvm_vgic_flush_hwstate>> vgic_restore_state(vcpu);
+ */
 static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 {
 	if (!static_branch_unlikely(&kvm_vgic_global_state.gicv3_cpuif))
@@ -903,6 +1013,15 @@ static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 		__vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|988| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_flush_hwstate(vcpu);
+ *
+ * 对于gicv3核心思想分为两步. (还有gicv4的)
+ *
+ * 1. 对于gicv3, 写入vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+ * 2. 对于gicv3, 把vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr]写入寄存器
+ */
 /* Flush our emulation state into the GIC hardware before entering the guest. */
 void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 {
@@ -926,10 +1045,19 @@ void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 
 	if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
 		raw_spin_lock(&vcpu->arch.vgic_cpu.ap_list_lock);
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/vgic/vgic.c|937| <<kvm_vgic_flush_hwstate>> vgic_flush_lr_state(vcpu);
+		 *
+		 * 对于gicv3, 写入vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+		 */
 		vgic_flush_lr_state(vcpu);
 		raw_spin_unlock(&vcpu->arch.vgic_cpu.ap_list_lock);
 	}
 
+	/*
+	 * 对于gicv3, 把vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr]写入寄存器
+	 */
 	if (can_access_vgic_from_kernel())
 		vgic_restore_state(vcpu);
 
@@ -937,6 +1065,10 @@ void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 		vgic_v4_commit(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|464| <<kvm_arch_vcpu_load>> kvm_vgic_load(vcpu);
+ */
 void kvm_vgic_load(struct kvm_vcpu *vcpu)
 {
 	if (unlikely(!vgic_initialized(vcpu->kvm)))
diff --git a/arch/x86/events/amd/core.c b/arch/x86/events/amd/core.c
index e24976593..bb6ab9487 100644
--- a/arch/x86/events/amd/core.c
+++ b/arch/x86/events/amd/core.c
@@ -832,6 +832,19 @@ static void amd_pmu_del_event(struct perf_event *event)
  * handled a counter. When an un-handled NMI is received, it will be claimed
  * only if arriving within that window.
  */
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ */
 static inline int amd_pmu_adjust_nmi_window(int handled)
 {
 	/*
@@ -850,6 +863,19 @@ static inline int amd_pmu_adjust_nmi_window(int handled)
 	return NMI_HANDLED;
 }
 
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ */
 static int amd_pmu_handle_irq(struct pt_regs *regs)
 {
 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
@@ -869,6 +895,9 @@ static int amd_pmu_handle_irq(struct pt_regs *regs)
 	if (cpuc->lbr_users)
 		amd_brs_drain();
 
+	/*
+	 * 比如x86_perf_event_update()
+	 */
 	/* Process any counter overflows */
 	handled = x86_pmu_handle_irq(regs);
 
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 185f902e5..f21cfe33d 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -112,6 +112,30 @@ u64 __read_mostly hw_cache_extra_regs
  * Can only be executed on the CPU where the event is active.
  * Returns the delta events processed.
  */
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ *
+ * 在以下使用x86_perf_event_update():
+ *   - arch/x86/events/intel/core.c|2721| <<global>> DEFINE_STATIC_CALL(intel_pmu_update_topdown_event, x86_perf_event_update);
+ *   - arch/x86/events/amd/core.c|957| <<amd_pmu_v2_handle_irq>> x86_perf_event_update(event);
+ *   - arch/x86/events/core.c|2175| <<init_hw_perf_events>> x86_pmu.update = x86_perf_event_update;
+ *   - arch/x86/events/intel/core.c|2744| <<intel_pmu_read_event>> x86_perf_event_update(event);
+ *   - arch/x86/events/intel/core.c|2872| <<intel_pmu_update>> return x86_perf_event_update(event);
+ *   - arch/x86/events/intel/p4.c|1061| <<p4_pmu_handle_irq>> val = x86_perf_event_update(event);
+ *   - arch/x86/events/zhaoxin/core.c|394| <<zhaoxin_pmu_handle_irq>> x86_perf_event_update(event);
+ *
+ * 核心思想是比较旧的count和新的count
+ */
 u64 x86_perf_event_update(struct perf_event *event)
 {
 	struct hw_perf_event *hwc = &event->hw;
@@ -1661,6 +1685,19 @@ static void x86_pmu_del(struct perf_event *event, int flags)
 	static_call_cond(x86_pmu_del)(event);
 }
 
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ */
 int x86_pmu_handle_irq(struct pt_regs *regs)
 {
 	struct perf_sample_data data;
@@ -1687,7 +1724,25 @@ int x86_pmu_handle_irq(struct pt_regs *regs)
 
 		event = cpuc->events[idx];
 
+		/*
+		 * 在以下使用x86_pmu_update:
+		 *   - arch/x86/events/core.c|76| <<global>> DEFINE_STATIC_CALL_NULL(x86_pmu_update, *x86_pmu.update);
+		 *   - arch/x86/events/perf_event.h|1054| <<global>> DECLARE_STATIC_CALL(x86_pmu_update, *x86_pmu.update);
+		 *   - arch/x86/events/core.c|1600| <<x86_pmu_stop>> static_call(x86_pmu_update)(event);
+		 *   - arch/x86/events/core.c|1690| <<x86_pmu_handle_irq>> val = static_call(x86_pmu_update)(event);
+		 *   - arch/x86/events/core.c|2019| <<x86_pmu_static_call_update>> static_call_update(x86_pmu_update, x86_pmu.update);
+		 *   - arch/x86/events/core.c|2042| <<_x86_pmu_read>> static_call(x86_pmu_update)(event);
+		 *   - arch/x86/events/intel/core.c|2351| <<intel_pmu_nhm_workaround>> static_call(x86_pmu_update)(event);
+		 *   - arch/x86/events/intel/core.c|2844| <<intel_pmu_save_and_restart>> static_call(x86_pmu_update)(event);
+		 *
+		 * 比如x86_perf_event_update()
+		 *
+		 * val返回的是counter的最新值
+		 */
 		val = static_call(x86_pmu_update)(event);
+		/*
+		 * 这里是判断counter最高位有没有1, 也就是有没有溢出
+		 */
 		if (val & (1ULL << (x86_pmu.cntval_bits - 1)))
 			continue;
 
@@ -1725,6 +1780,23 @@ void perf_events_lapic_init(void)
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
 }
 
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ *
+ * called by:
+ *   - arch/sparc/kernel/perf_event.c|1683| <<global>> .notifier_call = perf_event_nmi_handler,
+ *   - arch/x86/events/core.c|2102| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ */
 static int
 perf_event_nmi_handler(unsigned int cmd, struct pt_regs *regs)
 {
@@ -1740,6 +1812,9 @@ perf_event_nmi_handler(unsigned int cmd, struct pt_regs *regs)
 		return NMI_DONE;
 
 	start_clock = sched_clock();
+	/*
+	 * amd_pmu_handle_irq()
+	 */
 	ret = static_call(x86_pmu_handle_irq)(regs);
 	finish_clock = sched_clock();
 
@@ -2936,6 +3011,10 @@ static unsigned long code_segment_base(struct pt_regs *regs)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|7680| <<perf_prepare_sample>> data->ip = perf_instruction_pointer(regs);
+ */
 unsigned long perf_instruction_pointer(struct pt_regs *regs)
 {
 	if (perf_guest_state())
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index c8ba2be75..edb00dc37 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -1051,6 +1051,17 @@ struct pmu *x86_get_pmu(unsigned int cpu);
 extern struct x86_pmu x86_pmu __read_mostly;
 
 DECLARE_STATIC_CALL(x86_pmu_set_period, *x86_pmu.set_period);
+/*
+ * 在以下使用x86_pmu_update:
+ *   - arch/x86/events/core.c|76| <<global>> DEFINE_STATIC_CALL_NULL(x86_pmu_update, *x86_pmu.update);
+ *   - arch/x86/events/perf_event.h|1054| <<global>> DECLARE_STATIC_CALL(x86_pmu_update, *x86_pmu.update);
+ *   - arch/x86/events/core.c|1600| <<x86_pmu_stop>> static_call(x86_pmu_update)(event);
+ *   - arch/x86/events/core.c|1690| <<x86_pmu_handle_irq>> val = static_call(x86_pmu_update)(event);
+ *   - arch/x86/events/core.c|2019| <<x86_pmu_static_call_update>> static_call_update(x86_pmu_update, x86_pmu.update);
+ *   - arch/x86/events/core.c|2042| <<_x86_pmu_read>> static_call(x86_pmu_update)(event);
+ *   - arch/x86/events/intel/core.c|2351| <<intel_pmu_nhm_workaround>> static_call(x86_pmu_update)(event);
+ *   - arch/x86/events/intel/core.c|2844| <<intel_pmu_save_and_restart>> static_call(x86_pmu_update)(event);
+ */
 DECLARE_STATIC_CALL(x86_pmu_update,     *x86_pmu.update);
 
 static __always_inline struct x86_perf_task_context_opt *task_context_opt(void *ctx)
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 70d139406..1f71e3245 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -79,8 +79,42 @@
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
+/*
+ * 在以下使用KVM_REQ_NMI:
+ *   - arch/x86/kvm/x86.c|821| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|5301| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10615| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+ *   - arch/x86/kvm/x86.c|12866| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *   - arch/x86/kvm/x86.c|12919| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *
+ * 应该是PMI插入NMI(PMI)的事件
+ *
+ * process_nmi()
+ */
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *
+ * 应该是重新program PMU的事件
+ *
+ * kvm_pmu_handle_event()
+ */
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|120| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8362| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10619| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *   - arch/x86/kvm/x86.c|12878| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+ *
+ * kvm_pmu_deliver_pmi()
+ */
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #ifdef CONFIG_KVM_SMM
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
@@ -490,11 +524,34 @@ enum pmc_type {
 struct kvm_pmc {
 	enum pmc_type type;
 	u8 idx;
+	/*
+	 * 在以下使用kvm_pmc->is_paused:
+	 *   - arch/x86/kvm/pmu.c|255| <<pmc_reprogram_counter>> pmc->is_paused = false;
+	 *   - arch/x86/kvm/pmu.c|268| <<pmc_pause_counter>> if (!pmc->perf_event || pmc->is_paused)
+	 *   - arch/x86/kvm/pmu.c|274| <<pmc_pause_counter>> pmc->is_paused = true;
+	 *   - arch/x86/kvm/pmu.c|294| <<pmc_resume_counter>> pmc->is_paused = false;
+	 *   - arch/x86/kvm/pmu.h|88| <<pmc_read_counter>> if (pmc->perf_event && !pmc->is_paused)
+	 *   - arch/x86/kvm/pmu.h|184| <<pmc_update_sample_period>> if (!pmc->perf_event || pmc->is_paused ||
+	 */
 	bool is_paused;
 	bool intr;
 	u64 counter;
+	/*
+	 * 在以下使用kvm_pmu->prev_counter:
+	 *   - arch/x86/kvm/pmu.c|424| <<reprogram_counter>> if (pmc->counter < pmc->prev_counter)
+	 *   - arch/x86/kvm/pmu.c|464| <<reprogram_counter>> pmc->prev_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|759| <<kvm_pmu_incr_counter>> pmc->prev_counter = pmc->counter;
+	 *   - arch/x86/kvm/svm/pmu.c|245| <<amd_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|643| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|650| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = 0;
+	 */
 	u64 prev_counter;
 	u64 eventsel;
+	/*
+	 * 在以下使用kvm_pmu->perf_event:
+	 *   - arch/x86/kvm/pmu.c|253| <<pmc_reprogram_counter>> pmc->perf_event = event;
+	 *   - arch/x86/kvm/pmu.h|111| <<pmc_release_perf_event>> pmc->perf_event = NULL;
+	 */
 	struct perf_event *perf_event;
 	struct kvm_vcpu *vcpu;
 	/*
@@ -514,6 +571,15 @@ struct kvm_pmc {
 #define KVM_AMD_PMC_MAX_GENERIC	6
 struct kvm_pmu {
 	u8 version;
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|196| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = ebx.split.num_core_pmc;
+	 *   - arch/x86/kvm/svm/pmu.c|198| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+	 *   - arch/x86/kvm/svm/pmu.c|200| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+	 *   - arch/x86/kvm/svm/pmu.c|203| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(unsigned int , pmu->nr_arch_gp_counters, kvm_pmu_cap.num_counters_gp);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|496| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|529| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+	 */
 	unsigned nr_arch_gp_counters;
 	unsigned nr_arch_fixed_counters;
 	unsigned available_event_types;
@@ -522,7 +588,26 @@ struct kvm_pmu {
 	u64 global_ctrl;
 	u64 global_status;
 	u64 counter_bitmask[2];
+	/*
+	 * 在以下使用kvm_pmu->global_ctrl_mask:
+	 *   - arch/x86/kvm/pmu.c|698| <<kvm_pmu_set_msr>> data &= ~pmu->global_ctrl_mask;
+	 *   - arch/x86/kvm/pmu.h|138| <<kvm_valid_perf_global_ctrl>> return !(pmu->global_ctrl_mask & data);
+	 *   - arch/x86/kvm/svm/pmu.c|262| <<amd_pmu_refresh>> pmu->global_ctrl_mask = ~((1ull << pmu->nr_arch_gp_counters) - 1);
+	 *   - arch/x86/kvm/svm/pmu.c|263| <<amd_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|503| <<intel_pmu_refresh>> pmu->global_ctrl_mask = ~0ull;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|555| <<intel_pmu_refresh>> pmu->global_ctrl_mask = counter_mask;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|562| <<intel_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask & ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF | MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
+	 */
 	u64 global_ctrl_mask;
+	/*
+	 * 在以下使用kvm_pmu->global_status_mask:
+	 *   - arch/x86/kvm/pmu.c|692| <<kvm_pmu_set_msr>> if (data & pmu->global_status_mask)
+	 *   - arch/x86/kvm/pmu.c|715| <<kvm_pmu_set_msr>> if (data & pmu->global_status_mask)
+	 *   - arch/x86/kvm/svm/pmu.c|263| <<amd_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|504| <<intel_pmu_refresh>> pmu->global_status_mask = ~0ull;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|562| <<intel_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|566| <<intel_pmu_refresh>> pmu->global_status_mask &= ~MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI;
+	 */
 	u64 global_status_mask;
 	u64 reserved_bits;
 	u64 raw_event_mask;
@@ -535,6 +620,16 @@ struct kvm_pmu {
 	 * filter changes.
 	 */
 	union {
+		/*
+		 * 在以下使用kvm_pmu->reprogram_pmi:
+		 *   - arch/x86/kvm/pmu.c|160| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+		 *   - arch/x86/kvm/pmu.c|486| <<reprogram_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|499| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+		 *   - arch/x86/kvm/pmu.c|503| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|1012| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) > sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+		 *   - arch/x86/kvm/pmu.h|249| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.h|261| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+		 */
 		DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
 		atomic64_t __reprogram_pmi;
 	};
@@ -560,12 +655,28 @@ struct kvm_pmu {
 	 * The gate to release perf_events not marked in
 	 * pmc_in_use only once in a vcpu time slice.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|488| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|723| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|735| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12399| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 */
 	bool need_cleanup;
 
 	/*
 	 * The total number of programmed perf_events and it helps to avoid
 	 * redundant check before cleanup if guest don't use vPMU at all.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->event_count:
+	 *   - rch/x86/kvm/pmu.c|287| <<pmc_reprogram_counter>> pmc_to_pmu(pmc)->event_count++;
+	 *   - arch/x86/kvm/pmu.c|848| <<kvm_pmu_init>> pmu->event_count = 0;
+	 *   - arch/x86/kvm/pmu.h|118| <<pmc_release_perf_event>> pmc_to_pmu(pmc)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|250| <<intel_pmu_release_guest_lbr_event>> vcpu_to_pmu(vcpu)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|301| <<intel_pmu_create_guest_lbr_event>> pmu->event_count++;
+	 *   - arch/x86/kvm/x86.c|12434| <<kvm_arch_sched_in>> if (pmu->version && unlikely(pmu->event_count)) {
+	 */
 	u8 event_count;
 };
 
@@ -695,6 +806,16 @@ struct kvm_vcpu_xen {
 	u32 vcpu_id; /* The Xen / ACPI vCPU ID */
 	u32 timer_virq;
 	u64 timer_expires; /* In guest epoch */
+	/*
+	 * 在以下使用kvm_vcpu_xen->timer_pending:
+	 *   - arch/x86/kvm/xen.c|139| <<kvm_xen_inject_timer_irqs>> if (atomic_read(&vcpu->arch.xen.timer_pending) > 0) {
+	 *   - arch/x86/kvm/xen.c|150| <<kvm_xen_inject_timer_irqs>> atomic_set(&vcpu->arch.xen.timer_pending, 0);
+	 *   - arch/x86/kvm/xen.c|158| <<xen_timer_callback>> if (atomic_read(&vcpu->arch.xen.timer_pending))
+	 *   - arch/x86/kvm/xen.c|161| <<xen_timer_callback>> atomic_inc(&vcpu->arch.xen.timer_pending);
+	 *   - arch/x86/kvm/xen.c|170| <<kvm_xen_start_timer>> atomic_set(&vcpu->arch.xen.timer_pending, 0);
+	 *   - arch/x86/kvm/xen.c|187| <<kvm_xen_stop_timer>> atomic_set(&vcpu->arch.xen.timer_pending, 0);
+	 *   - arch/x86/kvm/xen.h|76| <<kvm_xen_has_pending_timer>> return atomic_read(&vcpu->arch.xen.timer_pending);
+	 */
 	atomic_t timer_pending;
 	struct hrtimer timer;
 	int poll_evtchn;
@@ -741,6 +862,23 @@ struct kvm_vcpu_arch {
 	int32_t apic_arb_prio;
 	int mp_state;
 	u64 ia32_misc_enable_msr;
+	/*
+	 * 在以下使用kvm_arch_vcpu->smbase:
+	 *   - arch/x86/kvm/smm.c|120| <<kvm_smm_changed>> trace_kvm_smm_transition(vcpu->vcpu_id, vcpu->arch.smbase, entering_smm);
+	 *   - arch/x86/kvm/smm.c|230| <<enter_smm_save_state_32>> smram->smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|259| <<enter_smm_save_state_64>> smram->smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|326| <<enter_smm>> if (kvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, &smram, sizeof(smram)))
+	 *   - arch/x86/kvm/smm.c|352| <<enter_smm>> cs.selector = (vcpu->arch.smbase >> 4) & 0xffff;
+	 *   - arch/x86/kvm/smm.c|353| <<enter_smm>> cs.base = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|523| <<rsm_load_state_32>> vcpu->arch.smbase = smstate->smbase;
+	 *   - arch/x86/kvm/smm.c|556| <<rsm_load_state_64>> vcpu->arch.smbase = smstate->smbase;
+	 *   - arch/x86/kvm/smm.c|602| <<emulator_leave_smm>> u64 smbase;
+	 *   - arch/x86/kvm/smm.c|605| <<emulator_leave_smm>> smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|607| <<emulator_leave_smm>> ret = kvm_vcpu_read_guest(vcpu, smbase + 0xfe00, smram.bytes, sizeof(smram));
+	 *   - arch/x86/kvm/x86.c|3820| <<kvm_set_msr_common(MSR_IA32_SMBASE)>> vcpu->arch.smbase = data;
+	 *   - arch/x86/kvm/x86.c|4219| <<kvm_get_msr_common(MSR_IA32_SMBASE)>> msr_info->data = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/x86.c|12237| <<kvm_vcpu_reset>> vcpu->arch.smbase = 0x30000;
+	 */
 	u64 smbase;
 	u64 smi_count;
 	bool at_instruction_boundary;
@@ -880,7 +1018,19 @@ struct kvm_vcpu_arch {
 	u64 this_tsc_nsec;
 	u64 this_tsc_write;
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2460| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3245| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4892| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2461| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|9746| <<kvm_pv_clock_pairing>> if (vcpu->arch.tsc_always_catchup)
+	 *   - arch/x86/kvm/x86.c|10997| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;
 	u32 virtual_tsc_mult;
@@ -890,11 +1040,48 @@ struct kvm_vcpu_arch {
 	u64 l1_tsc_scaling_ratio;
 	u64 tsc_scaling_ratio; /* current scaling ratio */
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|825| <<kvm_inject_nmi>> atomic_inc(&vcpu->arch.nmi_queued);
+	 *   - arch/x86/kvm/x86.c|5315| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
+	 *   - arch/x86/kvm/x86.c|10312| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+	 *   - arch/x86/kvm/x86.c|12110| <<kvm_vcpu_reset>> atomic_set(&vcpu->arch.nmi_queued, 0);
+	 */
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
+	/*
+	 * 在以下使用kvm_vcpu_arch->nmi_pending:
+	 *   - arch/x86/kvm/svm/nested.c|671| <<nested_vmcb02_prepare_control>> svm->vcpu.arch.nmi_pending++;
+	 *   - arch/x86/kvm/svm/nested.c|1092| <<nested_svm_vmexit>> if (vcpu->arch.nmi_pending) {
+	 *   - arch/x86/kvm/svm/nested.c|1093| <<nested_svm_vmexit>> vcpu->arch.nmi_pending--;
+	 *   - arch/x86/kvm/svm/nested.c|1487| <<svm_check_nested_events>> if (vcpu->arch.nmi_pending && !svm_nmi_blocked(vcpu)) {
+	 *   - arch/x86/kvm/svm/svm.c|2438| <<svm_set_gif>> svm->vcpu.arch.nmi_pending ||
+	 *   - arch/x86/kvm/vmx/nested.c|4162| <<vmx_check_nested_events>> if (vcpu->arch.nmi_pending && !vmx_nmi_blocked(vcpu)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4175| <<vmx_check_nested_events>> vcpu->arch.nmi_pending = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6528| <<__vmx_handle_exit>> vcpu->arch.nmi_pending) {
+	 *   - arch/x86/kvm/x86.c|5314| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.nmi_pending = 0;
+	 *   - arch/x86/kvm/x86.c|10222| <<kvm_check_and_inject_events>> if (vcpu->arch.nmi_pending) {
+	 *   - arch/x86/kvm/x86.c|10227| <<kvm_check_and_inject_events>> --vcpu->arch.nmi_pending;
+	 *   - arch/x86/kvm/x86.c|10233| <<kvm_check_and_inject_events>> if (vcpu->arch.nmi_pending)
+	 *   - arch/x86/kvm/x86.c|10309| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+	 *   - arch/x86/kvm/x86.c|10310| <<process_nmi>> vcpu->arch.nmi_pending = min(vcpu->arch.nmi_pending, limit);
+	 *   - arch/x86/kvm/x86.c|10312| <<process_nmi>> if (vcpu->arch.nmi_pending &&
+	 *   - arch/x86/kvm/x86.c|10314| <<process_nmi>> vcpu->arch.nmi_pending--;
+	 *   - arch/x86/kvm/x86.c|10316| <<process_nmi>> if (vcpu->arch.nmi_pending)
+	 *   - arch/x86/kvm/x86.c|10323| <<kvm_get_nr_pending_nmis>> return vcpu->arch.nmi_pending +
+	 *   - arch/x86/kvm/x86.c|12098| <<kvm_vcpu_reset>> vcpu->arch.nmi_pending = 0;
+	 *   - arch/x86/kvm/x86.c|12907| <<kvm_vcpu_has_events>> (vcpu->arch.nmi_pending &&
+	 */
 	/* Number of NMIs pending injection, not including hardware vNMIs. */
 	unsigned int nmi_pending;
 	bool nmi_injected;    /* Trying to inject an NMI this entry */
 	bool smi_pending;    /* SMI queued after currently running handler */
+	/*
+	 * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+	 *   - arch/x86/include/asm/kvm_host.h|2040| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+	 *   - arch/x86/kvm/x86.h|449| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+	 *   - arch/x86/kvm/x86.h|454| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+	 *   - arch/x86/kvm/x86.h|459| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+	 */
 	u8 handling_intr_from_guest;
 
 	struct kvm_mtrr mtrr_state;
@@ -1131,8 +1318,35 @@ struct kvm_xen {
 	u32 xen_version;
 	bool long_mode;
 	bool runstate_update_flag;
+	/*
+	 * 在以下使用kvm_xen->upcall_vector:
+	 *   - arch/x86/kvm/irq.c|122| <<kvm_cpu_get_extint>> return v->kvm->arch.xen.upcall_vector;
+	 *   - arch/x86/kvm/xen.c|632| <<kvm_xen_hvm_set_attr>> kvm->arch.xen.upcall_vector = data->u.vector;
+	 *   - arch/x86/kvm/xen.c|688| <<kvm_xen_hvm_get_attr>> data->u.vector = kvm->arch.xen.upcall_vector;
+	 *   - arch/x86/kvm/xen.h|56| <<kvm_xen_has_interrupt>> vcpu->kvm->arch.xen.upcall_vector)
+	 *
+	 * 在以下使用kvm_vcpu_xen->upcall_vector:
+	 *   - arch/x86/kvm/xen.c|480| <<kvm_xen_inject_vcpu_vector>> irq.vector = v->arch.xen.upcall_vector;
+	 *   - arch/x86/kvm/xen.c|549| <<kvm_xen_inject_pending_events>> if (v->arch.xen.upcall_vector)
+	 *   - arch/x86/kvm/xen.c|936| <<kvm_xen_vcpu_set_attr>> vcpu->arch.xen.upcall_vector = data->u.vector;
+	 *   - arch/x86/kvm/xen.c|1029| <<kvm_xen_vcpu_get_attr>> data->u.vector = vcpu->arch.xen.upcall_vector;
+	 *   - arch/x86/kvm/xen.c|1643| <<kvm_xen_set_evtchn_fast>> if (kick_vcpu && vcpu->arch.xen.upcall_vector) {
+	 */
 	u8 upcall_vector;
 	struct gfn_to_pfn_cache shinfo_cache;
+	/*
+	 * 在以下使用kvm_xen->evtchn_ports:
+	 *   - arch/x86/kvm/xen.c|1828| <<kvm_xen_eventfd_update>> evtchnfd = idr_find(&kvm->arch.xen.evtchn_ports, port);
+	 *   - arch/x86/kvm/xen.c|1922| <<kvm_xen_eventfd_assign>> ret = idr_alloc(&kvm->arch.xen.evtchn_ports, evtchnfd, port, port + 1,
+	 *   - arch/x86/kvm/xen.c|1943| <<kvm_xen_eventfd_deassign>> evtchnfd = idr_remove(&kvm->arch.xen.evtchn_ports, port);
+	 *   - arch/x86/kvm/xen.c|1969| <<kvm_xen_eventfd_reset>> idr_for_each_entry(&kvm->arch.xen.evtchn_ports, evtchnfd, i)
+	 *   - arch/x86/kvm/xen.c|1979| <<kvm_xen_eventfd_reset>> idr_for_each_entry(&kvm->arch.xen.evtchn_ports, evtchnfd, i) {
+	 *   - arch/x86/kvm/xen.c|1981| <<kvm_xen_eventfd_reset>> idr_remove(&kvm->arch.xen.evtchn_ports, evtchnfd->send_port);
+	 *   - arch/x86/kvm/xen.c|2036| <<kvm_xen_hcall_evtchn_send>> evtchnfd = idr_find(&vcpu->kvm->arch.xen.evtchn_ports, send.port);
+	 *   - arch/x86/kvm/xen.c|2109| <<kvm_xen_init_vm>> idr_init(&kvm->arch.xen.evtchn_ports);
+	 *   - arch/x86/kvm/xen.c|2120| <<kvm_xen_destroy_vm>> idr_for_each_entry(&kvm->arch.xen.evtchn_ports, evtchnfd, i) {
+	 *   - arch/x86/kvm/xen.c|2125| <<kvm_xen_destroy_vm>> idr_destroy(&kvm->arch.xen.evtchn_ports);
+	 */
 	struct idr evtchn_ports;
 	unsigned long poll_mask[BITS_TO_LONGS(KVM_MAX_VCPUS)];
 };
@@ -1359,6 +1573,11 @@ struct kvm_arch {
 	bool triple_fault_event;
 
 	bool bus_lock_detection_enabled;
+	/*
+	 * 在以下设置kvm_arch->enable_pmu:
+	 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap>> kvm->arch.enable_pmu = !(cap->args[0] & KVM_PMU_CAP_DISABLE);
+	 *   - arch/x86/kvm/x86.c|12492| <<kvm_arch_init_vm>> kvm->arch.enable_pmu = enable_pmu;
+	 */
 	bool enable_pmu;
 
 	u32 notify_window;
@@ -1824,6 +2043,17 @@ static inline int kvm_arch_flush_remote_tlbs(struct kvm *kvm)
 
 #define __KVM_HAVE_ARCH_FLUSH_REMOTE_TLBS_RANGE
 
+/*
+ * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+ *   - arch/x86/include/asm/kvm_host.h|2040| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+ *   - arch/x86/kvm/x86.h|449| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+ *   - arch/x86/kvm/x86.h|454| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+ *   - arch/x86/kvm/x86.h|459| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|6059| <<kvm_guest_state>> if (!kvm_arch_pmi_in_guest(vcpu))
+ *   - virt/kvm/kvm_main.c|6074| <<kvm_guest_get_ip>> if (WARN_ON_ONCE(!kvm_arch_pmi_in_guest(vcpu)))
+ */
 #define kvm_arch_pmi_in_guest(vcpu) \
 	((vcpu) && (vcpu)->arch.handling_intr_from_guest)
 
diff --git a/arch/x86/include/asm/nmi.h b/arch/x86/include/asm/nmi.h
index 5c5f1e56c..4f5176308 100644
--- a/arch/x86/include/asm/nmi.h
+++ b/arch/x86/include/asm/nmi.h
@@ -44,6 +44,28 @@ struct nmiaction {
 	const char		*name;
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|1246| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2102| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|56| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|773| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/cpu/mshyperv.c|487| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/reboot.c|912| <<nmi_shootdown_cpus>> if (register_nmi_handler(NMI_LOCAL, crash_nmi_callback,
+ *   - arch/x86/kernel/smp.c|145| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/platform/uv/uv_nmi.c|1022| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - arch/x86/platform/uv/uv_nmi.c|1025| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1199| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ *   - drivers/watchdog/hpwdt.c|249| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|252| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|255| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
 #define register_nmi_handler(t, fn, fg, n, init...)	\
 ({							\
 	static struct nmiaction init fn##_na = {	\
diff --git a/arch/x86/kernel/apic/hw_nmi.c b/arch/x86/kernel/apic/hw_nmi.c
index 45af535c4..851ce3270 100644
--- a/arch/x86/kernel/apic/hw_nmi.c
+++ b/arch/x86/kernel/apic/hw_nmi.c
@@ -51,6 +51,10 @@ static int nmi_cpu_backtrace_handler(unsigned int cmd, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(nmi_cpu_backtrace_handler);
 
+/*
+ * 在以下使用register_nmi_cpu_backtrace_handler():
+ *   - arch/x86/kernel/apic/hw_nmi.c|60| <<global>> early_initcall(register_nmi_cpu_backtrace_handler);
+ */
 static int __init register_nmi_cpu_backtrace_handler(void)
 {
 	register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
@@ -59,3 +63,26 @@ static int __init register_nmi_cpu_backtrace_handler(void)
 }
 early_initcall(register_nmi_cpu_backtrace_handler);
 #endif
+
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|1246| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2102| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|56| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|773| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/cpu/mshyperv.c|487| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/reboot.c|912| <<nmi_shootdown_cpus>> if (register_nmi_handler(NMI_LOCAL, crash_nmi_callback,
+ *   - arch/x86/kernel/smp.c|145| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/platform/uv/uv_nmi.c|1022| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - arch/x86/platform/uv/uv_nmi.c|1025| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1199| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ *   - drivers/watchdog/hpwdt.c|249| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|252| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|255| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index b8ab9ee58..25673459b 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -66,6 +66,14 @@ static int __init parse_no_stealacc(char *arg)
 early_param("no-steal-acc", parse_no_stealacc);
 
 static DEFINE_PER_CPU_DECRYPTED(struct kvm_vcpu_pv_apf_data, apf_reason) __aligned(64);
+/*
+ * 在以下使用percpu的steal_time:
+ *   - arch/x86/kernel/kvm.c|323| <<kvm_register_steal_time>> struct kvm_steal_time *st = &per_cpu(steal_time, cpu);
+ *   - arch/x86/kernel/kvm.c|409| <<kvm_steal_clock>> src = &per_cpu(steal_time, cpu);
+ *   - arch/x86/kernel/kvm.c|442| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(steal_time, cpu), sizeof(steal_time));
+ *   - arch/x86/kernel/kvm.c|666| <<kvm_flush_tlb_multi>> src = &per_cpu(steal_time, cpu);
+ *   - arch/x86/kernel/kvm.c|785| <<__kvm_vcpu_is_preempted>> struct kvm_steal_time *src = &per_cpu(steal_time, cpu);
+ */
 DEFINE_PER_CPU_DECRYPTED(struct kvm_steal_time, steal_time) __aligned(64) __visible;
 static int has_steal_clock = 0;
 
diff --git a/arch/x86/kernel/nmi.c b/arch/x86/kernel/nmi.c
index a0c551846..d503ba3cb 100644
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@ -127,6 +127,19 @@ static void nmi_check_duration(struct nmiaction *action, u64 duration)
 		action->handler, duration, decimal_msecs);
 }
 
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ */
 static int nmi_handle(unsigned int type, struct pt_regs *regs)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index 773132c3b..2cf59ccf9 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -321,6 +321,10 @@ static bool kvm_cpuid_has_hyperv(struct kvm_cpuid_entry2 *entries, int nent)
 	return entry && entry->eax == HYPERV_CPUID_SIGNATURE_EAX;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|452| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index 2673cd5c4..10985e4e7 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -2319,6 +2319,9 @@ static int em_rsm(struct x86_emulate_ctxt *ctxt)
 	if (!ctxt->ops->is_smm(ctxt))
 		return emulate_ud(ctxt);
 
+	/*
+	 * emulator_leave_smm()
+	 */
 	if (ctxt->ops->leave_smm(ctxt))
 		ctxt->ops->triple_fault(ctxt);
 
diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index b2c397dd2..f79a0e2a1 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -143,6 +143,10 @@ int kvm_cpu_get_interrupt(struct kvm_vcpu *v)
 }
 EXPORT_SYMBOL_GPL(kvm_cpu_get_interrupt);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11087| <<vcpu_run>> kvm_inject_pending_timer_irqs(vcpu);
+ */
 void kvm_inject_pending_timer_irqs(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu))
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 16d076a1b..608c2a38d 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -155,6 +155,10 @@ static int kvm_hv_set_sint(struct kvm_kernel_irq_routing_entry *e,
 	return kvm_hv_synic_set_irq(kvm, e->hv_sint.vcpu, e->hv_sint.sint);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|247| <<irqfd_wakeup>> if (kvm_arch_set_irq_inatomic(&irq, kvm, * KVM_USERSPACE_IRQ_SOURCE_ID, 1, false) == -EWOULDBLOCK)
+ */
 int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 			      struct kvm *kvm, int irq_source_id, int level,
 			      bool line_status)
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 3e977dbbf..b5b6f42c9 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -817,6 +817,17 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|839| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|2161| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq_comm.c|77| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq_comm.c|99| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|842| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1221| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|1234| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/x86.c|13185| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
@@ -1202,6 +1213,12 @@ static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|55| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+ *   - arch/x86/kvm/irq_comm.c|176| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+ *   - arch/x86/kvm/xen.c|532| <<kvm_xen_inject_vcpu_vector>> WARN_ON_ONCE(!kvm_irq_delivery_to_apic_fast(v->kvm, NULL, &irq, &r, NULL));
+ */
 bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)
 {
@@ -1285,6 +1302,11 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
  * Add a pending IRQ into lapic.
  * Return 1 if successfully added and 0 if discarded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|825| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector, irq->level, irq->trig_mode, dest_map);
+ *   - arch/x86/kvm/lapic.c|2769| <<kvm_apic_local_deliver>> r = __apic_accept_irq(apic, mode, vector, 1, trig_mode, NULL);
+ */
 static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map)
@@ -1322,6 +1344,10 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 						       apic->regs + APIC_TMR);
 		}
 
+		/*
+		 * vmx_deliver_interrupt()
+		 * svm_deliver_interrupt()
+		 */
 		static_call(kvm_x86_deliver_interrupt)(apic, delivery_mode,
 						       trig_mode, vector);
 		break;
@@ -1342,6 +1368,11 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 
 	case APIC_DM_NMI:
 		result = 1;
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/lapic.c|1345| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+		 *   - arch/x86/kvm/x86.c|5009| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+		 */
 		kvm_inject_nmi(vcpu);
 		kvm_vcpu_kick(vcpu);
 		break;
@@ -1790,6 +1821,10 @@ static bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1885| <<__kvm_wait_lapic_expire>> __wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);
+ */
 static inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)
 {
 	u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
@@ -1864,6 +1899,11 @@ static void __kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 		__wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4178| <<svm_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7361| <<vmx_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+ */
 void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu) &&
@@ -2745,6 +2785,10 @@ static bool lapic_is_periodic(struct kvm_lapic *apic)
 	return apic_lvtt_period(apic);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|29| <<kvm_cpu_has_pending_timer>> r = apic_has_pending_timer(vcpu);
+ */
 int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2755,6 +2799,13 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1881| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2782| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|531| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ *   - arch/x86/kvm/x86.c|5084| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -2766,6 +2817,11 @@ int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 		mode = reg & APIC_MODE_MASK;
 		trig_mode = reg & APIC_LVT_LEVEL_TRIGGER;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/lapic.c|825| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector, irq->level, irq->trig_mode, dest_map);
+		 *   - arch/x86/kvm/lapic.c|2769| <<kvm_apic_local_deliver>> r = __apic_accept_irq(apic, mode, vector, 1, trig_mode, NULL);
+		 */
 		r = __apic_accept_irq(apic, mode, vector, 1, trig_mode, NULL);
 		if (r && lvt_type == APIC_LVTPC)
 			kvm_lapic_set_reg(apic, APIC_LVTPC, reg | APIC_LVT_MASKED);
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 0a0ea4b5d..8ee5aaf70 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -17,6 +17,10 @@
 #define APIC_DEST_MASK			0x800
 
 #define APIC_BUS_CYCLE_NS       1
+/*
+ * 只在下面使用APIC_BUS_FREQUENCY:
+ *   - arch/x86/kvm/hyperv.c|1688| <<kvm_hv_get_msr>> data = APIC_BUS_FREQUENCY;
+ */
 #define APIC_BUS_FREQUENCY      (1000000000ULL / APIC_BUS_CYCLE_NS)
 
 #define APIC_BROADCAST			0xFF
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index f7901cb4d..b7baea328 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -5484,6 +5484,13 @@ int kvm_mmu_load(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5449| <<kvm_mmu_reset_context>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|6940| <<kvm_mmu_destroy>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|1180| <<kvm_post_set_cr4>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|12560| <<kvm_unload_vcpu_mmu>> kvm_mmu_unload(vcpu);
+ */
 void kvm_mmu_unload(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 9ae07db6f..cf4af89ad 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -80,6 +80,10 @@ static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;
 #define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
 #include <asm/kvm-x86-pmu-ops.h>
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9476| <<kvm_ops_update>> kvm_pmu_ops_update(ops->pmu_ops);
+ */
 void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 {
 	memcpy(&kvm_pmu_ops, pmu_ops, sizeof(kvm_pmu_ops));
@@ -93,6 +97,15 @@ void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 #undef __KVM_X86_PMU_OP
 }
 
+/*
+ * vcpu_enter_guest(KVM_REQ_PMU)
+ * -> kvm_pmu_handle_event()
+ *    -> reprogram_counter()
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|403| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -116,10 +129,23 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		__set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
 	}
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|120| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8362| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10619| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|12878| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * kvm_pmu_deliver_pmi()
+	 */
 	if (pmc->intr && !skip_pmi)
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
 }
 
+/*
+ * 在以下使用kvm_perf_overflow():
+ *   - arch/x86/kvm/pmu.c|213| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -134,8 +160,24 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 	if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+	 *   - arch/x86/kvm/pmu.c|403| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+	 */
 	__kvm_perf_overflow(pmc, true);
 
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
@@ -161,6 +203,21 @@ static u64 pmc_get_pebs_precise_level(struct kvm_pmc *pmc)
 	return 1;
 }
 
+/*
+ * 应该是重新program PMU的事件
+ * vcpu_enter_guest(KVM_REQ_PMU)
+ * -> kvm_pmu_handle_event()
+ *    -> reprogram_counter()
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|478| <<reprogram_counter>> if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ *
+ * 495         if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ * 496                                   (eventsel & pmu->raw_event_mask),
+ * 497                                   !(eventsel & ARCH_PERFMON_EVENTSEL_USR),
+ * 498                                   !(eventsel & ARCH_PERFMON_EVENTSEL_OS),
+ * 499                                   eventsel & ARCH_PERFMON_EVENTSEL_INT))
+ */
 static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 				 bool exclude_user, bool exclude_kernel,
 				 bool intr)
@@ -200,6 +257,19 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 		attr.precise_ip = pmc_get_pebs_precise_level(pmc);
 	}
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|634| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|250| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(attr, -1, current, NULL, pmc);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|968| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|973| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kvm/pmu.c|230| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|293| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+	 *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+	 *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+	 *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>> return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+	 *   - kernel/watchdog_perf.c|157| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 */
 	event = perf_event_create_kernel_counter(&attr, -1, current,
 						 kvm_perf_overflow, pmc);
 	if (IS_ERR(event)) {
@@ -208,6 +278,11 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 		return PTR_ERR(event);
 	}
 
+	/*
+	 * 在以下使用kvm_pmu->perf_event:
+	 *   - arch/x86/kvm/pmu.c|253| <<pmc_reprogram_counter>> pmc->perf_event = event;
+	 *   - arch/x86/kvm/pmu.h|111| <<pmc_release_perf_event>> pmc->perf_event = NULL;
+	 */
 	pmc->perf_event = event;
 	pmc_to_pmu(pmc)->event_count++;
 	pmc->is_paused = false;
@@ -215,6 +290,10 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|419| <<reprogram_counter>> pmc_pause_counter(pmc);
+ */
 static void pmc_pause_counter(struct kvm_pmc *pmc)
 {
 	u64 counter = pmc->counter;
@@ -228,11 +307,26 @@ static void pmc_pause_counter(struct kvm_pmc *pmc)
 	pmc->is_paused = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|513| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+ */
 static bool pmc_resume_counter(struct kvm_pmc *pmc)
 {
+	/*
+	 * 在以下使用kvm_pmu->perf_event:
+	 *   - arch/x86/kvm/pmu.c|253| <<pmc_reprogram_counter>> pmc->perf_event = event;
+	 *   - arch/x86/kvm/pmu.h|111| <<pmc_release_perf_event>> pmc->perf_event = NULL;
+	 */
 	if (!pmc->perf_event)
 		return false;
 
+	/*
+	 * called by:
+	 *   - arch/riscv/kvm/vcpu_pmu.c|380| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_period(pmc->perf_event, kvm_pmu_get_sample_period(pmc));
+	 *   - arch/x86/kvm/pmu.c|317| <<pmc_resume_counter>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+	 *   - arch/x86/kvm/pmu.h|200| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter));
+	 */
 	/* recalibrate sample period and check if it's accepted by perf core */
 	if (is_sampling_event(pmc->perf_event) &&
 	    perf_event_period(pmc->perf_event,
@@ -373,11 +467,26 @@ static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 
 static bool pmc_event_is_allowed(struct kvm_pmc *pmc)
 {
+	/*
+	 * 一共四个条件:
+	 * - pmc_is_globally_enabled(pmc)
+	 * - pmc_speculative_in_use(pmc)
+	 * - static_call(kvm_x86_pmu_hw_event_available)(pmc)
+	 * - check_pmu_event_filter(pmc)
+	 */
 	return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
 	       static_call(kvm_x86_pmu_hw_event_available)(pmc) &&
 	       check_pmu_event_filter(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|458| <<kvm_pmu_handle_event>> reprogram_counter(pmc);
+ *
+ * vcpu_enter_guest(KVM_REQ_PMU)
+ * -> kvm_pmu_handle_event()
+ *    -> reprogram_counter()
+ */
 static void reprogram_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -385,11 +494,19 @@ static void reprogram_counter(struct kvm_pmc *pmc)
 	u64 new_config = eventsel;
 	u8 fixed_ctr_ctrl;
 
+	/*
+	 * 只在此处调用
+	 */
 	pmc_pause_counter(pmc);
 
 	if (!pmc_event_is_allowed(pmc))
 		goto reprogram_complete;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|137| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+	 *   - arch/x86/kvm/pmu.c|403| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+	 */
 	if (pmc->counter < pmc->prev_counter)
 		__kvm_perf_overflow(pmc, false);
 
@@ -411,6 +528,11 @@ static void reprogram_counter(struct kvm_pmc *pmc)
 	if (pmc->current_config == new_config && pmc_resume_counter(pmc))
 		goto reprogram_complete;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|468| <<reprogram_counter>> pmc_release_perf_event(pmc);
+	 *   - arch/x86/kvm/pmu.h|121| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+	 */
 	pmc_release_perf_event(pmc);
 
 	pmc->current_config = new_config;
@@ -429,15 +551,39 @@ static void reprogram_counter(struct kvm_pmc *pmc)
 		return;
 
 reprogram_complete:
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|160| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|486| <<reprogram_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|499| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/pmu.c|503| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1012| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) > sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.h|249| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|261| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 */
 	clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
 	pmc->prev_counter = 0;
 }
 
+/*
+ * called by(KVM_REQ_PMU):
+ *   - arch/x86/kvm/x86.c|10717| <<vcpu_enter_guest>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 	int bit;
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|160| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|486| <<reprogram_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|499| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/pmu.c|503| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1012| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) > sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.h|249| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|261| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 */
 	for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
 		struct kvm_pmc *pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, bit);
 
@@ -446,9 +592,25 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 			continue;
 		}
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/pmu.c|458| <<kvm_pmu_handle_event>> reprogram_counter(pmc);
+		 *
+		 * 应该是重新program PMU的事件
+		 * vcpu_enter_guest(KVM_REQ_PMU)
+		 * -> kvm_pmu_handle_event()
+		 *    -> reprogram_counter()
+		 */
 		reprogram_counter(pmc);
 	}
 
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|488| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|723| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|735| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12399| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 */
 	/*
 	 * Unused perf_events are only released if the corresponding MSRs
 	 * weren't accessed during the last vCPU time slice. kvm_arch_sched_in
@@ -524,10 +686,35 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|120| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8362| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10619| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *   - arch/x86/kvm/x86.c|12878| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|10620| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
+		/*
+		 * svm似乎没有intel_pmu_deliver_pmi()
+		 */
 		static_call_cond(kvm_x86_pmu_deliver_pmi)(vcpu);
+		/*
+		 * LVT Performance Counter Register (FEE0 0340H) -
+		 * Specifies interrupt delivery when a performance counter
+		 * generates an interrupt on overflow (see Section 19.6.3.5.8,
+		 * "Generating an Interrupt on Overflow") or when Intel PT
+		 * signals a ToPA PMI (see Section 32.2.7.2). This LVT entry
+		 * is implementation specific, not architectural. If
+		 * implemented, it is not guaranteed to be at base address
+		 * FEE0 0340H.
+		 *
+		 * APIC_DM_NMI
+		 */
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 	}
 }
@@ -546,6 +733,10 @@ bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 		static_call(kvm_x86_pmu_is_valid_msr)(vcpu, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|830| <<kvm_pmu_set_msr>> kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+ */
 static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -580,6 +771,11 @@ int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3949| <<kvm_set_msr_common>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|4030| <<kvm_set_msr_common>> return kvm_pmu_set_msr(vcpu, msr_info);
+ */
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -632,7 +828,16 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 			pmu->global_status &= ~data;
 		break;
 	default:
+		/*
+		 * 只在这里调用
+		 */
 		kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+		/*
+		 * intel_pmu_set_msr()
+		 * amd_pmu_set_msr()
+		 *
+		 * 只在这里调用kvm_x86_pmu_set_msr
+		 */
 		return static_call(kvm_x86_pmu_set_msr)(vcpu, msr_info);
 	}
 
@@ -643,8 +848,21 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  * settings are changed (such as changes of PMU CPUID by guest VMs), which
  * should rarely happen.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|368| <<kvm_vcpu_after_set_cpuid>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/pmu.c|724| <<kvm_pmu_init>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/x86.c|3686| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> kvm_pmu_refresh(vcpu);
+ */
 void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/cpuid.c|427| <<kvm_set_cpuid>> if (kvm_vcpu_has_run(vcpu)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5444| <<kvm_mmu_after_set_cpuid>> KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm);
+	 *   - arch/x86/kvm/pmu.c|704| <<kvm_pmu_refresh>> if (KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|2250| <<do_set_msr>> if (kvm_vcpu_has_run(vcpu) && kvm_is_immutable_feature_msr(index)) {
+	 */
 	if (KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm))
 		return;
 
@@ -652,11 +870,20 @@ void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 	static_call(kvm_x86_pmu_refresh)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|754| <<kvm_pmu_destroy>> kvm_pmu_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|12190| <<kvm_vcpu_reset>> kvm_pmu_reset(vcpu);
+ */
 void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	static_call(kvm_x86_pmu_reset)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12059| <<kvm_arch_vcpu_create>> kvm_pmu_init(vcpu);
+ */
 void kvm_pmu_init(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -668,6 +895,10 @@ void kvm_pmu_init(struct kvm_vcpu *vcpu)
 	kvm_pmu_refresh(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|516| <<kvm_pmu_handle_event>> kvm_pmu_cleanup(vcpu);
+ */
 /* Release perf_events for vPMCs that have been unused for a full time slice.  */
 void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
@@ -684,6 +915,12 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 	for_each_set_bit(i, bitmask, X86_PMC_IDX_MAX) {
 		pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, i);
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/pmu.c|477| <<pmc_event_is_allowed>> return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
+		 *   - arch/x86/kvm/pmu.c|905| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|777| <<intel_pmu_cross_mapped_check>> if (!pmc || !pmc_speculative_in_use(pmc) ||
+		 */
 		if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
 			pmc_stop_counter(pmc);
 	}
@@ -698,8 +935,21 @@ void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
 	kvm_pmu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|804| <<kvm_pmu_trigger_event>> kvm_pmu_incr_counter(pmc);
+ */
 static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 {
+	/*
+	 * 在以下使用kvm_pmu->prev_counter:
+	 *   - arch/x86/kvm/pmu.c|424| <<reprogram_counter>> if (pmc->counter < pmc->prev_counter)
+	 *   - arch/x86/kvm/pmu.c|464| <<reprogram_counter>> pmc->prev_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|759| <<kvm_pmu_incr_counter>> pmc->prev_counter = pmc->counter;
+	 *   - arch/x86/kvm/svm/pmu.c|245| <<amd_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|643| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|650| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = 0;
+	 */
 	pmc->prev_counter = pmc->counter;
 	pmc->counter = (pmc->counter + 1) & pmc_bitmask(pmc);
 	kvm_pmu_request_counter_reprogram(pmc);
@@ -731,6 +981,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3567| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8752| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|9059| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|9061| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 1d64113de..83b1e45dc 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -62,11 +62,29 @@ static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 	return pmu->counter_bitmask[pmc->type];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|579| <<kvm_pmu_rdpmc>> *data = pmc_read_counter(pmc) & mask;
+ *   - arch/x86/kvm/pmu.h|85| <<pmc_write_counter>> pmc->counter += val - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/pmu.h|102| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|140| <<amd_pmu_get_msr>> msr_info->data = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|370| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|375| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ */
 static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 {
 	u64 counter, enabled, running;
 
 	counter = pmc->counter;
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|110| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|213| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|437| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/x86/kvm/pmu.h|71| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 *   - tools/include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+	 */
 	if (pmc->perf_event && !pmc->is_paused)
 		counter += perf_event_read_value(pmc->perf_event,
 						 &enabled, &running);
@@ -74,26 +92,76 @@ static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 	return counter & pmc_bitmask(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|163| <<amd_pmu_set_msr>> pmc_write_counter(pmc, data);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|439| <<intel_pmu_set_msr>> pmc_write_counter(pmc, data);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|443| <<intel_pmu_set_msr>> pmc_write_counter(pmc, data);
+ */
 static inline void pmc_write_counter(struct kvm_pmc *pmc, u64 val)
 {
 	pmc->counter += val - pmc_read_counter(pmc);
 	pmc->counter &= pmc_bitmask(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|468| <<reprogram_counter>> pmc_release_perf_event(pmc);
+ *   - arch/x86/kvm/pmu.h|121| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+ */
 static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/pmu-emul.c|190| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/riscv/kvm/vcpu_pmu.c|81| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1057| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1059| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+		 *   - arch/x86/kvm/pmu.h|115| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|248| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+		 *   - kernel/events/core.c|5395| <<perf_release>> perf_event_release_kernel(file->private_data);
+		 *   - kernel/events/hw_breakpoint.c|829| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+		 *   - kernel/watchdog_perf.c|330| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+		 *   - kernel/watchdog_perf.c|406| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+		 */
 		perf_event_release_kernel(pmc->perf_event);
+		/*
+		 * 在以下使用kvm_pmu->perf_event:
+		 *   - arch/x86/kvm/pmu.c|253| <<pmc_reprogram_counter>> pmc->perf_event = event;
+		 *   - arch/x86/kvm/pmu.h|111| <<pmc_release_perf_event>> pmc->perf_event = NULL;
+		 */
 		pmc->perf_event = NULL;
 		pmc->current_config = 0;
+		/*
+		 * 在以下使用kvm_pmu->event_count:
+		 *   - arch/x86/kvm/pmu.c|287| <<pmc_reprogram_counter>> pmc_to_pmu(pmc)->event_count++;
+		 *   - arch/x86/kvm/pmu.c|848| <<kvm_pmu_init>> pmu->event_count = 0;
+		 *   - arch/x86/kvm/pmu.h|118| <<pmc_release_perf_event>> pmc_to_pmu(pmc)->event_count--;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|250| <<intel_pmu_release_guest_lbr_event>> vcpu_to_pmu(vcpu)->event_count--;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|301| <<intel_pmu_create_guest_lbr_event>> pmu->event_count++;
+		 *   - arch/x86/kvm/x86.c|12434| <<kvm_arch_sched_in>> if (pmu->version && unlikely(pmu->event_count)) {
+		 */
 		pmc_to_pmu(pmc)->event_count--;
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|798| <<kvm_pmu_cleanup>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|344| <<amd_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|642| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|649| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ */
 static inline void pmc_stop_counter(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
 		pmc->counter = pmc_read_counter(pmc);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/pmu.c|468| <<reprogram_counter>> pmc_release_perf_event(pmc);
+		 *   - arch/x86/kvm/pmu.h|121| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+		 */
 		pmc_release_perf_event(pmc);
 	}
 }
@@ -118,6 +186,21 @@ static inline bool kvm_valid_perf_global_ctrl(struct kvm_pmu *pmu,
  * used for both PERFCTRn and EVNTSELn; that is why it accepts base as a
  * parameter to tell them apart.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|95| <<intel_pmc_idx_to_pmc>> return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|179| <<get_fw_gp_pmc>> return get_gp_pmc(pmu, msr, MSR_IA32_PMC0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|221| <<intel_is_valid_msr>> ret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|222| <<intel_is_valid_msr>> get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|237| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|238| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|368| <<intel_pmu_get_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|369| <<intel_pmu_get_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|379| <<intel_pmu_get_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|430| <<intel_pmu_set_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|431| <<intel_pmu_set_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|446| <<intel_pmu_set_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ */
 static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 					 u32 base)
 {
@@ -131,6 +214,15 @@ static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|85| <<reprogram_fixed_counters>> pmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|100| <<intel_pmc_idx_to_pmc>> return get_fixed_pmc(pmu, idx + MSR_CORE_PERF_FIXED_CTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|223| <<intel_is_valid_msr>> get_fixed_pmc(pmu, msr) || get_fw_gp_pmc(pmu, msr) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|236| <<intel_msr_idx_to_pmc>> pmc = get_fixed_pmc(pmu, msr);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|374| <<intel_pmu_get_msr>> } else if ((pmc = get_fixed_pmc(pmu, msr))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|442| <<intel_pmu_set_msr>> } else if ((pmc = get_fixed_pmc(pmu, msr))) {
+ */
 /* returns fixed PMC with the specified MSR */
 static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 {
@@ -146,6 +238,12 @@ static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 	return NULL;
 }
 
+/*
+ * 在以下使用get_sample_period():
+ *   - arch/x86/kvm/pmu.c|239| <<pmc_reprogram_counter>> attr.sample_period = get_sample_period(pmc, pmc->counter);
+ *   - arch/x86/kvm/pmu.c|333| <<pmc_resume_counter>> get_sample_period(pmc, pmc->counter)))
+ *   - arch/x86/kvm/pmu.h|225| <<pmc_update_sample_period>> get_sample_period(pmc, pmc->counter));
+ */
 static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 {
 	u64 sample_period = (-counter_value) & pmc_bitmask(pmc);
@@ -155,16 +253,34 @@ static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 	return sample_period;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|229| <<amd_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|440| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|444| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ */
 static inline void pmc_update_sample_period(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event || pmc->is_paused ||
 	    !is_sampling_event(pmc->perf_event))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/riscv/kvm/vcpu_pmu.c|380| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_period(pmc->perf_event, kvm_pmu_get_sample_period(pmc));
+	 *   - arch/x86/kvm/pmu.c|317| <<pmc_resume_counter>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+	 *   - arch/x86/kvm/pmu.h|200| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter));
+	 */
 	perf_event_period(pmc->perf_event,
 			  get_sample_period(pmc, pmc->counter));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|477| <<pmc_event_is_allowed>> return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
+ *   - arch/x86/kvm/pmu.c|905| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|777| <<intel_pmu_cross_mapped_check>> if (!pmc || !pmc_speculative_in_use(pmc) ||
+ */
 static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -178,6 +294,10 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 
 extern struct x86_pmu_capability kvm_pmu_cap;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9570| <<__kvm_x86_vendor_init>> kvm_init_pmu_capability(ops->pmu_ops);
+ */
 static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
@@ -220,12 +340,35 @@ static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 					     KVM_PMC_MAX_FIXED);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|828| <<kvm_pmu_incr_counter>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|225| <<amd_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|88| <<reprogram_fixed_counters>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|456| <<intel_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+ */
 static inline void kvm_pmu_request_counter_reprogram(struct kvm_pmc *pmc)
 {
 	set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|796| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_CTRL)>> reprogram_counters(pmu, diff);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|414| <<intel_pmu_set_msr>> reprogram_counters(pmu, diff);
+ */
 static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 {
 	int bit;
@@ -235,6 +378,19 @@ static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 
 	for_each_set_bit(bit, (unsigned long *)&diff, X86_PMC_IDX_MAX)
 		set_bit(bit, pmu->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * 应该是重新program PMU的事件
+	 *
+	 * kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
 }
 
@@ -243,6 +399,11 @@ static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
  *
  * If the vPMU doesn't have global_ctrl MSR, all vPMCs are enabled.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|462| <<pmc_event_is_allowed>> return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
+ *   - arch/x86/kvm/vmx/pmu_intel.c|778| <<intel_pmu_cross_mapped_check>> !pmc_is_globally_enabled(pmc) || !pmc->perf_event)
+ */
 static inline bool pmc_is_globally_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
diff --git a/arch/x86/kvm/smm.c b/arch/x86/kvm/smm.c
index b42111a24..a856bbd11 100644
--- a/arch/x86/kvm/smm.c
+++ b/arch/x86/kvm/smm.c
@@ -109,6 +109,12 @@ static void check_smram_offsets(void)
 #undef CHECK_SMRAM32_OFFSET
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|310| <<enter_smm>> kvm_smm_changed(vcpu, true);
+ *   - arch/x86/kvm/smm.c|589| <<emulator_leave_smm>> kvm_smm_changed(vcpu, false);
+ *   - arch/x86/kvm/x86.c|5368| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_smm_changed(vcpu, events->smi.smm);
+ */
 void kvm_smm_changed(struct kvm_vcpu *vcpu, bool entering_smm)
 {
 	trace_kvm_smm_transition(vcpu->vcpu_id, vcpu->arch.smbase, entering_smm);
@@ -278,6 +284,10 @@ static void enter_smm_save_state_64(struct kvm_vcpu *vcpu,
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10262| <<kvm_check_and_inject_events>> enter_smm(vcpu);
+ */
 void enter_smm(struct kvm_vcpu *vcpu)
 {
 	struct kvm_segment cs, ds;
@@ -304,11 +314,32 @@ void enter_smm(struct kvm_vcpu *vcpu)
 	 * Kill the VM in the unlikely case of failure, because the VM
 	 * can be in undefined state in this case.
 	 */
+	/*
+	 * vmx_enter_smm()
+	 * svm_enter_smm()
+	 */
 	if (static_call(kvm_x86_enter_smm)(vcpu, &smram))
 		goto error;
 
 	kvm_smm_changed(vcpu, true);
 
+	/*
+	 * 在以下使用kvm_arch_vcpu->smbase:
+	 *   - arch/x86/kvm/smm.c|120| <<kvm_smm_changed>> trace_kvm_smm_transition(vcpu->vcpu_id, vcpu->arch.smbase, entering_smm);
+	 *   - arch/x86/kvm/smm.c|230| <<enter_smm_save_state_32>> smram->smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|259| <<enter_smm_save_state_64>> smram->smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|326| <<enter_smm>> if (kvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, &smram, sizeof(smram)))
+	 *   - arch/x86/kvm/smm.c|352| <<enter_smm>> cs.selector = (vcpu->arch.smbase >> 4) & 0xffff;
+	 *   - arch/x86/kvm/smm.c|353| <<enter_smm>> cs.base = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|523| <<rsm_load_state_32>> vcpu->arch.smbase = smstate->smbase;
+	 *   - arch/x86/kvm/smm.c|556| <<rsm_load_state_64>> vcpu->arch.smbase = smstate->smbase;
+	 *   - arch/x86/kvm/smm.c|602| <<emulator_leave_smm>> u64 smbase;
+	 *   - arch/x86/kvm/smm.c|605| <<emulator_leave_smm>> smbase = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/smm.c|607| <<emulator_leave_smm>> ret = kvm_vcpu_read_guest(vcpu, smbase + 0xfe00, smram.bytes, sizeof(smram));
+	 *   - arch/x86/kvm/x86.c|3820| <<kvm_set_msr_common(MSR_IA32_SMBASE)>> vcpu->arch.smbase = data;
+	 *   - arch/x86/kvm/x86.c|4219| <<kvm_get_msr_common(MSR_IA32_SMBASE)>> msr_info->data = vcpu->arch.smbase;
+	 *   - arch/x86/kvm/x86.c|12237| <<kvm_vcpu_reset>> vcpu->arch.smbase = 0x30000;
+	 */
 	if (kvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, &smram, sizeof(smram)))
 		goto error;
 
@@ -320,6 +351,9 @@ void enter_smm(struct kvm_vcpu *vcpu)
 	kvm_set_rflags(vcpu, X86_EFLAGS_FIXED);
 	kvm_rip_write(vcpu, 0x8000);
 
+	/*
+	 * vmx_set_interrupt_shadow()
+	 */
 	static_call(kvm_x86_set_interrupt_shadow)(vcpu, 0);
 
 	cr0 = vcpu->arch.cr0 & ~(X86_CR0_PE | X86_CR0_EM | X86_CR0_TS | X86_CR0_PG);
@@ -419,6 +453,11 @@ static int rsm_load_seg_64(struct kvm_vcpu *vcpu,
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/smm.c|506| <<rsm_load_state_32>> r = rsm_enter_protected_mode(vcpu, smstate->cr0,
+ *   - arch/x86/kvm/smm.c|554| <<rsm_load_state_64>> r = rsm_enter_protected_mode(vcpu, smstate->cr0, smstate->cr3, smstate->cr4);
+ */
 static int rsm_enter_protected_mode(struct kvm_vcpu *vcpu,
 				    u64 cr0, u64 cr3, u64 cr4)
 {
@@ -569,6 +608,12 @@ static int rsm_load_state_64(struct x86_emulate_ctxt *ctxt,
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|2322| <<em_rsm>> if (ctxt->ops->leave_smm(ctxt))
+ *
+ * struct x86_emulate_ops emulate_op.leave_smm = emulator_leave_smm()
+ */
 int emulator_leave_smm(struct x86_emulate_ctxt *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -577,6 +622,12 @@ int emulator_leave_smm(struct x86_emulate_ctxt *ctxt)
 	u64 smbase;
 	int ret;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> u64 smbase;
+	 *    -> u64 smi_count;
+	 */
 	smbase = vcpu->arch.smbase;
 
 	ret = kvm_vcpu_read_guest(vcpu, smbase + 0xfe00, smram.bytes, sizeof(smram));
diff --git a/arch/x86/kvm/svm/avic.c b/arch/x86/kvm/svm/avic.c
index 4b74ea91f..342ba2a46 100644
--- a/arch/x86/kvm/svm/avic.c
+++ b/arch/x86/kvm/svm/avic.c
@@ -318,6 +318,10 @@ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3666| <<svm_complete_interrupt_delivery>> avic_ring_doorbell(vcpu);
+ */
 void avic_ring_doorbell(struct kvm_vcpu *vcpu)
 {
 	/*
diff --git a/arch/x86/kvm/svm/nested.c b/arch/x86/kvm/svm/nested.c
index 3fea8c476..0bb8ff041 100644
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@ -795,6 +795,11 @@ static void nested_svm_copy_common_state(struct vmcb *from_vmcb, struct vmcb *to
 	to_vmcb->save.spec_ctrl = from_vmcb->save.spec_ctrl;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|919| <<nested_svm_vmrun>> if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
+ *   - arch/x86/kvm/svm/svm.c|4702| <<svm_leave_smm>> ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
+ */
 int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 			 struct vmcb *vmcb12, bool from_vmrun)
 {
diff --git a/arch/x86/kvm/svm/pmu.c b/arch/x86/kvm/svm/pmu.c
index 373ff6a66..5635deea6 100644
--- a/arch/x86/kvm/svm/pmu.c
+++ b/arch/x86/kvm/svm/pmu.c
@@ -25,16 +25,47 @@ enum pmu_type {
 	PMU_TYPE_EVNTSEL,
 };
 
+/*
+ * 在以下使用amd_pmc_idx_to_pmc():
+ *   - arch/x86/kvm/svm/pmu.c|253| <<global>> struct kvm_pmu_ops amd_pmu_ops.pmc_idx_to_pmc = amd_pmc_idx_to_pmc,
+ *   - arch/x86/kvm/svm/pmu.c|73| <<get_gp_pmc_amd>> return amd_pmc_idx_to_pmc(pmu, idx);
+ *   - arch/x86/kvm/svm/pmu.c|94| <<amd_rdpmc_ecx_to_pmc>> return amd_pmc_idx_to_pmc(vcpu_to_pmu(vcpu), idx & ~(3u << 30));
+ *
+ * struct kvm_pmu_ops amd_pmu_ops.pmc_idx_to_pmc = amd_pmc_idx_to_pmc()
+ */
 static struct kvm_pmc *amd_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|196| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = ebx.split.num_core_pmc;
+	 *   - arch/x86/kvm/svm/pmu.c|198| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+	 *   - arch/x86/kvm/svm/pmu.c|200| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+	 *   - arch/x86/kvm/svm/pmu.c|203| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(unsigned int , pmu->nr_arch_gp_counters, kvm_pmu_cap.num_counters_gp);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|496| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|529| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+	 */
 	unsigned int num_counters = pmu->nr_arch_gp_counters;
 
 	if (pmc_idx >= num_counters)
 		return NULL;
 
+	/*
+	 * struct kvm_pmu *pmu:
+	 * -> struct kvm_pmc gp_counters[KVM_INTEL_PMC_MAX_GENERIC];
+	 * -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+	 */
 	return &pmu->gp_counters[array_index_nospec(pmc_idx, num_counters)];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|102| <<amd_msr_idx_to_pmc>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|103| <<amd_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|138| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|144| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|161| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|168| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ */
 static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 					     enum pmu_type type)
 {
@@ -73,11 +104,17 @@ static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 	return amd_pmc_idx_to_pmc(pmu, idx);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.hw_event_available = amd_hw_event_available()
+ */
 static bool amd_hw_event_available(struct kvm_pmc *pmc)
 {
 	return true;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.is_valid_rdpmc_ecx = amd_is_valid_rdpmc_ecx()
+ */
 static bool amd_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -88,12 +125,18 @@ static bool amd_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 }
 
 /* idx is the ECX register of RDPMC instruction */
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.rdpmc_ecx_to_pmc = amd_rdpmc_ecx_to_pmc()
+ */
 static struct kvm_pmc *amd_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	unsigned int idx, u64 *mask)
 {
 	return amd_pmc_idx_to_pmc(vcpu_to_pmu(vcpu), idx & ~(3u << 30));
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.msr_idx_to_pmc = amd_msr_idx_to_pmc()
+ */
 static struct kvm_pmc *amd_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -105,6 +148,9 @@ static struct kvm_pmc *amd_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 	return pmc;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.is_valid_msr = amd_is_valid_msr()
+ */
 static bool amd_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -128,6 +174,9 @@ static bool amd_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 	return amd_msr_idx_to_pmc(vcpu, msr);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.get_msr = amd_pmu_get_msr()
+ */
 static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -137,12 +186,25 @@ static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	/* MSR_PERFCTRn */
 	pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
 	if (pmc) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/pmu.c|579| <<kvm_pmu_rdpmc>> *data = pmc_read_counter(pmc) & mask;
+		 *   - arch/x86/kvm/pmu.h|85| <<pmc_write_counter>> pmc->counter += val - pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/pmu.h|102| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/svm/pmu.c|140| <<amd_pmu_get_msr>> msr_info->data = pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|370| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|375| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+		 */
 		msr_info->data = pmc_read_counter(pmc);
 		return 0;
 	}
 	/* MSR_EVNTSELn */
 	pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
 	if (pmc) {
+		/*
+		 * struct kvm_pmc *pmc;
+		 * -> u64 eventsel;
+		 */
 		msr_info->data = pmc->eventsel;
 		return 0;
 	}
@@ -150,6 +212,9 @@ static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.set_msr = amd_pmu_set_msr()
+ */
 static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -170,6 +235,13 @@ static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		data &= ~pmu->reserved_bits;
 		if (data != pmc->eventsel) {
 			pmc->eventsel = data;
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/pmu.c|828| <<kvm_pmu_incr_counter>> kvm_pmu_request_counter_reprogram(pmc);
+			 *   - arch/x86/kvm/svm/pmu.c|225| <<amd_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+			 *   - arch/x86/kvm/vmx/pmu_intel.c|88| <<reprogram_fixed_counters>> kvm_pmu_request_counter_reprogram(pmc);
+			 *   - arch/x86/kvm/vmx/pmu_intel.c|456| <<intel_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+			 */
 			kvm_pmu_request_counter_reprogram(pmc);
 		}
 		return 0;
@@ -178,6 +250,9 @@ static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.refresh = amd_pmu_refresh()
+ */
 static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -193,6 +268,15 @@ static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 		BUILD_BUG_ON(x86_feature_cpuid(X86_FEATURE_PERFMON_V2).function != 0x80000022 ||
 			     x86_feature_cpuid(X86_FEATURE_PERFMON_V2).index);
 		ebx.full = kvm_find_cpuid_entry_index(vcpu, 0x80000022, 0)->ebx;
+		/*
+		 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+		 *   - arch/x86/kvm/svm/pmu.c|196| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = ebx.split.num_core_pmc;
+		 *   - arch/x86/kvm/svm/pmu.c|198| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+		 *   - arch/x86/kvm/svm/pmu.c|200| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+		 *   - arch/x86/kvm/svm/pmu.c|203| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(unsigned int , pmu->nr_arch_gp_counters, kvm_pmu_cap.num_counters_gp);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|496| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|529| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+		 */
 		pmu->nr_arch_gp_counters = ebx.split.num_core_pmc;
 	} else if (guest_cpuid_has(vcpu, X86_FEATURE_PERFCTR_CORE)) {
 		pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
@@ -204,7 +288,27 @@ static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 					 kvm_pmu_cap.num_counters_gp);
 
 	if (pmu->version > 1) {
+		/*
+		 * 在以下使用kvm_pmu->global_ctrl_mask:
+		 *   - arch/x86/kvm/pmu.c|698| <<kvm_pmu_set_msr>> data &= ~pmu->global_ctrl_mask;
+		 *   - arch/x86/kvm/pmu.h|138| <<kvm_valid_perf_global_ctrl>> return !(pmu->global_ctrl_mask & data);
+		 *   - arch/x86/kvm/svm/pmu.c|262| <<amd_pmu_refresh>> pmu->global_ctrl_mask = ~((1ull << pmu->nr_arch_gp_counters) - 1);
+		 *   - arch/x86/kvm/svm/pmu.c|263| <<amd_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|503| <<intel_pmu_refresh>> pmu->global_ctrl_mask = ~0ull;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|555| <<intel_pmu_refresh>> pmu->global_ctrl_mask = counter_mask;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|562| <<intel_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask &
+		 *                                                  ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF | MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
+		 */
 		pmu->global_ctrl_mask = ~((1ull << pmu->nr_arch_gp_counters) - 1);
+		/*
+		 * 在以下使用kvm_pmu->global_status_mask:
+		 *   - arch/x86/kvm/pmu.c|692| <<kvm_pmu_set_msr>> if (data & pmu->global_status_mask)
+		 *   - arch/x86/kvm/pmu.c|715| <<kvm_pmu_set_msr>> if (data & pmu->global_status_mask)
+		 *   - arch/x86/kvm/svm/pmu.c|263| <<amd_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|504| <<intel_pmu_refresh>> pmu->global_status_mask = ~0ull;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|562| <<intel_pmu_refresh>> pmu->global_status_mask = pmu->global_ctrl_mask
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|566| <<intel_pmu_refresh>> pmu->global_status_mask &= ~MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI;
+		 */
 		pmu->global_status_mask = pmu->global_ctrl_mask;
 	}
 
@@ -217,6 +321,9 @@ static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 	bitmap_set(pmu->all_valid_pmc_idx, 0, pmu->nr_arch_gp_counters);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.init = amd_pmu_init()
+ */
 static void amd_pmu_init(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -226,6 +333,11 @@ static void amd_pmu_init(struct kvm_vcpu *vcpu)
 	BUILD_BUG_ON(KVM_AMD_PMC_MAX_GENERIC > INTEL_PMC_MAX_GENERIC);
 
 	for (i = 0; i < KVM_AMD_PMC_MAX_GENERIC ; i++) {
+		/*
+		 * struct kvm_pmu *pmu:
+		 * -> struct kvm_pmc gp_counters[KVM_INTEL_PMC_MAX_GENERIC];
+		 * -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+		 */
 		pmu->gp_counters[i].type = KVM_PMC_GP;
 		pmu->gp_counters[i].vcpu = vcpu;
 		pmu->gp_counters[i].idx = i;
@@ -233,12 +345,20 @@ static void amd_pmu_init(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.reset = amd_pmu_reset()
+ */
 static void amd_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 	int i;
 
 	for (i = 0; i < KVM_AMD_PMC_MAX_GENERIC; i++) {
+		/*
+		 * struct kvm_pmu *pmu:
+		 * -> struct kvm_pmc gp_counters[KVM_INTEL_PMC_MAX_GENERIC];
+		 * -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+		 */
 		struct kvm_pmc *pmc = &pmu->gp_counters[i];
 
 		pmc_stop_counter(pmc);
@@ -248,6 +368,9 @@ static void amd_pmu_reset(struct kvm_vcpu *vcpu)
 	pmu->global_ctrl = pmu->global_status = 0;
 }
 
+/*
+ * struct kvm_x86_init_ops svm_init_ops.pmu_ops = &amd_pmu_ops
+ */
 struct kvm_pmu_ops amd_pmu_ops __initdata = {
 	.hw_event_available = amd_hw_event_available,
 	.pmc_idx_to_pmc = amd_pmc_idx_to_pmc,
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index beea99c8e..dd6783648 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -4108,6 +4108,10 @@ static fastpath_t svm_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
 	return EXIT_FASTPATH_NONE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4189| <<svm_vcpu_run>> svm_vcpu_enter_exit(vcpu, spec_ctrl_intercepted);
+ */
 static noinstr void svm_vcpu_enter_exit(struct kvm_vcpu *vcpu, bool spec_ctrl_intercepted)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4116,6 +4120,9 @@ static noinstr void svm_vcpu_enter_exit(struct kvm_vcpu *vcpu, bool spec_ctrl_in
 
 	amd_clear_divider();
 
+	/*
+	 * arch/x86/kvm/svm/vmenter.S
+	 */
 	if (sev_es_guest(vcpu->kvm))
 		__svm_sev_es_vcpu_run(svm, spec_ctrl_intercepted);
 	else
@@ -4175,6 +4182,11 @@ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu)
 	clgi();
 	kvm_load_guest_xsave_state(vcpu);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/svm/svm.c|4178| <<svm_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7361| <<vmx_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+	 */
 	kvm_wait_lapic_expire(vcpu);
 
 	/*
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 72e3943f3..5630043ea 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -6958,6 +6958,10 @@ static void handle_exception_irqoff(struct vcpu_vmx *vmx)
 		kvm_machine_check();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6986| <<vmx_handle_exit_irqoff>> handle_external_interrupt_irqoff(vcpu);
+ */
 static void handle_external_interrupt_irqoff(struct kvm_vcpu *vcpu)
 {
 	u32 intr_info = vmx_get_intr_info(vcpu);
@@ -7280,6 +7284,21 @@ static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
 		     vmx->loaded_vmcs->soft_vnmi_blocked))
 		vmx->loaded_vmcs->entry_time = ktime_get();
 
+	/*
+	 * 在以下设置vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx/nested.c|4587| <<load_vmcs12_host_state>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1551| <<vmx_set_rflags>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3357| <<vmx_set_cr0>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3608| <<vmx_set_segment>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 * 在以下使用vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx/vmx.c|1826| <<vmx_inject_exception>> WARN_ON_ONCE(vmx->emulation_required);
+	 *   - arch/x86/kvm/vmx/vmx.c|5828| <<vmx_emulation_required_with_pending_exception>> return vmx->emulation_required && !vmx->rmode.vm86_active &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5841| <<handle_invalid_guest_state>> while (vmx->emulation_required && count-- != 0) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6462| <<__vmx_handle_exit>> if (vmx->emulation_required) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6472| <<__vmx_handle_exit>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx/vmx.c|6982| <<vmx_handle_exit_irqoff>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx/vmx.c|7288| <<vmx_vcpu_run>> if (unlikely(vmx->emulation_required)) {
+	 */
 	/*
 	 * Don't enter VMX if guest state is invalid, let the exit handler
 	 * start emulation until we arrive back to a valid state.  Synthesize a
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index c2130d2c8..1bfb6b06b 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -313,6 +313,21 @@ struct vcpu_vmx {
 		} seg[8];
 	} segment_cache;
 	int vpid;
+	/*
+	 * 在以下设置vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx/nested.c|4587| <<load_vmcs12_host_state>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1551| <<vmx_set_rflags>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3357| <<vmx_set_cr0>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3608| <<vmx_set_segment>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 * 在以下使用vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx/vmx.c|1826| <<vmx_inject_exception>> WARN_ON_ONCE(vmx->emulation_required);
+	 *   - arch/x86/kvm/vmx/vmx.c|5828| <<vmx_emulation_required_with_pending_exception>> return vmx->emulation_required && !vmx->rmode.vm86_active &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5841| <<handle_invalid_guest_state>> while (vmx->emulation_required && count-- != 0) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6462| <<__vmx_handle_exit>> if (vmx->emulation_required) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6472| <<__vmx_handle_exit>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx/vmx.c|6982| <<vmx_handle_exit_irqoff>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx/vmx.c|7288| <<vmx_vcpu_run>> if (unlikely(vmx->emulation_required)) {
+	 */
 	bool emulation_required;
 
 	union vmx_exit_reason exit_reason;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 41cce5031..f23e2b573 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -815,9 +815,24 @@ void kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
 }
 EXPORT_SYMBOL_GPL(kvm_inject_emulated_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1345| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|5009| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+ */
 void kvm_inject_nmi(struct kvm_vcpu *vcpu)
 {
 	atomic_inc(&vcpu->arch.nmi_queued);
+	/*
+	 * 在以下使用KVM_REQ_NMI:
+	 *   - arch/x86/kvm/x86.c|821| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|5301| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10615| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|12866| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *   - arch/x86/kvm/x86.c|12919| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *
+	 * process_nmi()
+	 */
 	kvm_make_request(KVM_REQ_NMI, vcpu);
 }
 
@@ -3039,6 +3054,10 @@ static unsigned long get_cpu_tsc_khz(void)
 		return __this_cpu_read(cpu_tsc_khz);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3100| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ */
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
@@ -3075,6 +3094,11 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	put_cpu();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3108| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+ *   - arch/x86/kvm/x86.c|6737| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+ */
 static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -3086,14 +3110,37 @@ static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	} while (read_seqcount_retry(&ka->pvclock_sc, seq));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|579| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|2354| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|79| <<kvm_xen_shared_info_init>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|480| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|881| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|922| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|974| <<kvm_xen_vcpu_set_attr>> get_kvmclock_ns(vcpu->kvm));
+ *   - arch/x86/kvm/xen.c|1436| <<kvm_xen_hcall_vcpu_op>> delta = oneshot.timeout_abs_ns - get_kvmclock_ns(vcpu->kvm);
+ *   - arch/x86/kvm/xen.c|1466| <<kvm_xen_hcall_set_timer_op>> uint64_t guest_now = get_kvmclock_ns(vcpu->kvm);
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_clock_data data;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|3108| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+	 *   - arch/x86/kvm/x86.c|6737| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+	 */
 	get_kvmclock(kvm, &data);
 	return data.clock;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3275| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
+ *   - arch/x86/kvm/x86.c|3277| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_info_cache,
+ *   - arch/x86/kvm/x86.c|3280| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_time_info_cache, 0);
+ */
 static void kvm_setup_guest_pvclock(struct kvm_vcpu *v,
 				    struct gfn_to_pfn_cache *gpc,
 				    unsigned int offset)
@@ -4801,6 +4848,14 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/emulate-nested.c|1947| <<kvm_emulate_nested_eret>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - arch/arm64/kvm/emulate-nested.c|2028| <<kvm_inject_nested>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - arch/arm64/kvm/reset.c|300| <<kvm_reset_vcpu>> kvm_arch_vcpu_load(vcpu, smp_processor_id());
+ *   - virt/kvm/kvm_main.c|216| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5963| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Address WBINVD may be executed by guest */
@@ -10018,6 +10073,9 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
 				vcpu->arch.exception.error_code,
 				vcpu->arch.exception.injected);
 
+	/*
+	 * vmx_inject_exception()
+	 */
 	static_call(kvm_x86_inject_exception)(vcpu);
 }
 
@@ -10060,6 +10118,10 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
  * ordering between that side effect, the instruction completing, _and_ the
  * delivery of the asynchronous event.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10736| <<vcpu_enter_guest(KVM_REQ_EVENT)>> r = kvm_check_and_inject_events(vcpu, &req_immediate_exit);
+ */
 static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 				       bool *req_immediate_exit)
 {
@@ -10201,6 +10263,10 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 			can_inject = false;
 		} else
 			static_call(kvm_x86_enable_smi_window)(vcpu);
+		/*
+		 * vmx_enable_smi_window()
+		 * svm_enable_smi_window()
+		 */
 	}
 #endif
 
@@ -10211,6 +10277,10 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 		if (r) {
 			--vcpu->arch.nmi_pending;
 			vcpu->arch.nmi_injected = true;
+			/*
+			 * vmx_inject_nmi
+			 * svm_inject_nmi
+			 */
 			static_call(kvm_x86_inject_nmi)(vcpu);
 			can_inject = false;
 			WARN_ON(static_call(kvm_x86_nmi_allowed)(vcpu, true) < 0);
@@ -10265,6 +10335,20 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+/*
+ * 在以下使用KVM_REQ_NMI:
+ *   - arch/x86/kvm/x86.c|821| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|5301| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10615| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+ *   - arch/x86/kvm/x86.c|12866| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *   - arch/x86/kvm/x86.c|12919| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|5162| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> process_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|5283| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> process_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|10644| <<vcpu_enter_guest(KVM_REQ_NMI)>> process_nmi(vcpu);
+ */
 static void process_nmi(struct kvm_vcpu *vcpu)
 {
 	unsigned int limit;
@@ -10492,6 +10576,27 @@ void __kvm_request_immediate_exit(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);
 
+/*
+ * 4.14上handle_external_intr()的例子.
+ *
+ * [0] bnxt_rx_pages
+ * [0] bnxt_rx_pkt
+ * [0] __bnxt_poll_work
+ * [0] bnxt_poll
+ * [0] net_rx_action
+ * [0] __softirqentry_text_start
+ * [0] irq_exit
+ * [0] do_IRQ
+ * --- <IRQ stack> ---
+ * [0] ret_from_intr
+ * [0] vmx_handle_external_intr
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] __dta_kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 /*
  * Called within kvm->srcu read side.
  * Returns 1 to let vcpu_run() continue the guest execution loop without
@@ -10591,10 +10696,36 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		if (kvm_check_request(KVM_REQ_SMI, vcpu))
 			process_smi(vcpu);
 #endif
+		/*
+		 * 在以下使用KVM_REQ_NMI:
+		 *   - arch/x86/kvm/x86.c|821| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|5301| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|10615| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+		 *   - arch/x86/kvm/x86.c|12866| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *   - arch/x86/kvm/x86.c|12919| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *
+		 * process_nmi()
+		 */
 		if (kvm_check_request(KVM_REQ_NMI, vcpu))
 			process_nmi(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 */
 		if (kvm_check_request(KVM_REQ_PMU, vcpu))
 			kvm_pmu_handle_event(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMI:
+		 *   - arch/x86/kvm/pmu.c|120| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|8362| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|10619| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+		 *   - arch/x86/kvm/x86.c|12878| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+		 */
 		if (kvm_check_request(KVM_REQ_PMI, vcpu))
 			kvm_pmu_deliver_pmi(vcpu);
 		if (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {
@@ -10655,6 +10786,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			static_call(kvm_x86_update_cpu_dirty_logging)(vcpu);
 	}
 
+	/*
+	 * 非常非常重要的KVM_REQ_EVENT!!!
+	 */
 	if (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win ||
 	    kvm_xen_has_interrupt(vcpu)) {
 		++vcpu->stat.req_event;
@@ -10771,6 +10905,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
 			     (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));
 
+		/*
+		 * vmx_vcpu_run()
+		 * svm_vcpu_run()
+		 */
 		exit_fastpath = static_call(kvm_x86_vcpu_run)(vcpu);
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
@@ -10824,6 +10962,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.xfd_no_write_intercept)
 		fpu_sync_guest_vmexit_xfd_state();
 
+	/*
+	 * vmx_handle_exit_irqoff()
+	 */
 	static_call(kvm_x86_handle_exit_irqoff)(vcpu);
 
 	if (vcpu->arch.guest_fpu.xfd_err)
@@ -10870,6 +11011,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.apic_attention)
 		kvm_lapic_sync_from_vapic(vcpu);
 
+	/*
+	 * vmx_handle_exit()
+	 */
 	r = static_call(kvm_x86_handle_exit)(vcpu, exit_fastpath);
 	return r;
 
@@ -11099,6 +11243,10 @@ static void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 	trace_kvm_fpu(0);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4161| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_queued_exception *ex = &vcpu->arch.exception;
@@ -12173,6 +12321,10 @@ void kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_deliver_sipi_vector);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5214| <<__hardware_enable_nolock>> if (kvm_arch_hardware_enable()) {
+ */
 int kvm_arch_hardware_enable(void)
 {
 	struct kvm *kvm;
@@ -12293,8 +12445,40 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 
 	vcpu->arch.l1tf_flush_l1d = true;
+	/*
+	 * 在以下使用kvm_pmu->event_count:
+	 *   - rch/x86/kvm/pmu.c|287| <<pmc_reprogram_counter>> pmc_to_pmu(pmc)->event_count++;
+	 *   - arch/x86/kvm/pmu.c|848| <<kvm_pmu_init>> pmu->event_count = 0;
+	 *   - arch/x86/kvm/pmu.h|118| <<pmc_release_perf_event>> pmc_to_pmu(pmc)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|250| <<intel_pmu_release_guest_lbr_event>> vcpu_to_pmu(vcpu)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|301| <<intel_pmu_create_guest_lbr_event>> pmu->event_count++;
+	 *   - arch/x86/kvm/x86.c|12434| <<kvm_arch_sched_in>> if (pmu->version && unlikely(pmu->event_count)) {
+	 *
+	 * The total number of programmed perf_events and it helps to avoid
+	 * redundant check before cleanup if guest don't use vPMU at all.
+	 */
 	if (pmu->version && unlikely(pmu->event_count)) {
+		/*
+		 * 在以下使用kvm_pmu->need_cleanup:
+		 *   - arch/x86/kvm/pmu.c|488| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+		 *   - arch/x86/kvm/pmu.c|723| <<kvm_pmu_init>> pmu->need_cleanup = false;
+		 *   - arch/x86/kvm/pmu.c|735| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+		 *   - arch/x86/kvm/x86.c|12399| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+		 */
 		pmu->need_cleanup = true;
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/kvm/pmu.c|139| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|888| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|226| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|238| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|10617| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *   - arch/x86/kvm/x86.c|12319| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *
+		 * 应该是重新program PMU的事件
+		 *               
+		 * kvm_pmu_handle_event()
+		 */
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
 	static_call(kvm_x86_sched_in)(vcpu, cpu);
@@ -12958,6 +13142,12 @@ unsigned long kvm_get_rflags(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_get_rflags);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9098| <<x86_emulate_instruction>> __kvm_set_rflags(vcpu, ctxt->eflags);
+ *   - arch/x86/kvm/x86.c|10220| <<kvm_check_and_inject_events>> __kvm_set_rflags(vcpu, kvm_get_rflags(vcpu) |
+ *   - arch/x86/kvm/x86.c|13151| <<kvm_set_rflags>> __kvm_set_rflags(vcpu, rflags);
+ */
 static void __kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 {
 	if (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP &&
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 1e7be1f6a..8a9b7185c 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -92,6 +92,13 @@ static inline unsigned int __shrink_ple_window(unsigned int val,
 void kvm_service_local_tlb_flush_requests(struct kvm_vcpu *vcpu);
 int kvm_check_nested_events(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|427| <<kvm_set_cpuid>> if (kvm_vcpu_has_run(vcpu)) {
+ *   - arch/x86/kvm/mmu/mmu.c|5444| <<kvm_mmu_after_set_cpuid>> KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm);
+ *   - arch/x86/kvm/pmu.c|704| <<kvm_pmu_refresh>> if (KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|2250| <<do_set_msr>> if (kvm_vcpu_has_run(vcpu) && kvm_is_immutable_feature_msr(index)) {
+ */
 static inline bool kvm_vcpu_has_run(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.last_vmentry_cpu != -1;
@@ -436,19 +443,53 @@ enum kvm_intr_type {
 	KVM_HANDLING_NMI,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4215| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|6971| <<handle_external_interrupt_irqoff>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+ *   - arch/x86/kvm/vmx/vmx.c|7264| <<vmx_vcpu_enter_exit>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+ *   - arch/x86/kvm/x86.c|10980| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+ *
+ * 标记开始处理interrupt(从VM trap exit出来)
+ */
 static __always_inline void kvm_before_interrupt(struct kvm_vcpu *vcpu,
 						 enum kvm_intr_type intr)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+	 *   - arch/x86/include/asm/kvm_host.h|2040| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+	 *   - arch/x86/kvm/x86.h|449| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+	 *   - arch/x86/kvm/x86.h|454| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+	 *   - arch/x86/kvm/x86.h|459| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+	 */
 	WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
 }
 
 static __always_inline void kvm_after_interrupt(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+	 *   - arch/x86/include/asm/kvm_host.h|2040| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+	 *   - arch/x86/kvm/x86.h|449| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+	 *   - arch/x86/kvm/x86.h|454| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+	 *   - arch/x86/kvm/x86.h|459| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+	 */
 	WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8374| <<vmx_handle_intel_pt_intr>> if (!vcpu || !kvm_handling_nmi_from_guest(vcpu))
+ */
 static inline bool kvm_handling_nmi_from_guest(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+	 *   - arch/x86/include/asm/kvm_host.h|2040| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+	 *   - arch/x86/kvm/x86.h|449| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+	 *   - arch/x86/kvm/x86.h|454| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+	 *   - arch/x86/kvm/x86.h|459| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+	 */
 	return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
 }
 
diff --git a/arch/x86/kvm/xen.c b/arch/x86/kvm/xen.c
index 40edf4d19..53e70c700 100644
--- a/arch/x86/kvm/xen.c
+++ b/arch/x86/kvm/xen.c
@@ -32,8 +32,25 @@ static int kvm_xen_set_evtchn(struct kvm_xen_evtchn *xe, struct kvm *kvm);
 static int kvm_xen_setattr_evtchn(struct kvm *kvm, struct kvm_xen_hvm_attr *data);
 static bool kvm_xen_hcall_evtchn_send(struct kvm_vcpu *vcpu, u64 param, u64 *r);
 
+/*
+ * 在以下使用kvm_xen_enabled:
+ *   - arch/x86/kvm/xen.c|35| <<global>> DEFINE_STATIC_KEY_DEFERRED_FALSE(kvm_xen_enabled, HZ);
+ *   - arch/x86/kvm/x86.c|9621| <<kvm_x86_vendor_exit>> static_key_deferred_flush(&kvm_xen_enabled);
+ *   - arch/x86/kvm/x86.c|9622| <<kvm_x86_vendor_exit>> WARN_ON(static_branch_unlikely(&kvm_xen_enabled.key));
+ *   - arch/x86/kvm/xen.c|1133| <<kvm_xen_hvm_config>> static_branch_inc(&kvm_xen_enabled.key);
+ *   - arch/x86/kvm/xen.c|1135| <<kvm_xen_hvm_config>> static_branch_slow_dec_deferred(&kvm_xen_enabled);
+ *   - arch/x86/kvm/xen.c|2128| <<kvm_xen_destroy_vm>> static_branch_slow_dec_deferred(&kvm_xen_enabled);
+ *   - arch/x86/kvm/xen.h|41| <<kvm_xen_msr_enabled>> return static_branch_unlikely(&kvm_xen_enabled.key) &&
+ *   - arch/x86/kvm/xen.h|47| <<kvm_xen_hypercall_enabled>> return static_branch_unlikely(&kvm_xen_enabled.key) &&
+ *   - arch/x86/kvm/xen.h|54| <<kvm_xen_has_interrupt>> if (static_branch_unlikely(&kvm_xen_enabled.key) &&
+ *   - arch/x86/kvm/xen.h|64| <<kvm_xen_has_pending_events>> return static_branch_unlikely(&kvm_xen_enabled.key) &&
+ */
 DEFINE_STATIC_KEY_DEFERRED_FALSE(kvm_xen_enabled, HZ);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|623| <<kvm_xen_hvm_set_attr>> r = kvm_xen_shared_info_init(kvm, data->u.shared_info.gfn);
+ */
 static int kvm_xen_shared_info_init(struct kvm *kvm, gfn_t gfn)
 {
 	struct gfn_to_pfn_cache *gpc = &kvm->arch.xen.shinfo_cache;
@@ -113,6 +130,10 @@ static int kvm_xen_shared_info_init(struct kvm *kvm, gfn_t gfn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|151| <<kvm_inject_pending_timer_irqs>> kvm_xen_inject_timer_irqs(vcpu);
+ */
 void kvm_xen_inject_timer_irqs(struct kvm_vcpu *vcpu)
 {
 	if (atomic_read(&vcpu->arch.xen.timer_pending) > 0) {
@@ -144,6 +165,12 @@ static enum hrtimer_restart xen_timer_callback(struct hrtimer *timer)
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|972| <<kvm_xen_vcpu_set_attr>> kvm_xen_start_timer(vcpu, data->u.timer.expires_ns,
+ *   - arch/x86/kvm/xen.c|1442| <<kvm_xen_hcall_vcpu_op>> kvm_xen_start_timer(vcpu, oneshot.timeout_abs_ns, delta);
+ *   - arch/x86/kvm/xen.c|1484| <<kvm_xen_hcall_set_timer_op>> kvm_xen_start_timer(vcpu, timeout, delta);
+ */
 static void kvm_xen_start_timer(struct kvm_vcpu *vcpu, u64 guest_abs, s64 delta_ns)
 {
 	atomic_set(&vcpu->arch.xen.timer_pending, 0);
@@ -173,6 +200,12 @@ static void kvm_xen_init_timer(struct kvm_vcpu *vcpu)
 	vcpu->arch.xen.timer.function = xen_timer_callback;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|471| <<kvm_xen_update_runstate>> kvm_xen_update_runstate_guest(v, state == RUNSTATE_runnable);
+ *   - arch/x86/kvm/xen.c|800| <<kvm_xen_vcpu_set_attr>> kvm_xen_update_runstate_guest(vcpu, false);
+ *   - arch/x86/kvm/xen.c|896| <<kvm_xen_vcpu_set_attr>> kvm_xen_update_runstate_guest(vcpu, false);
+ */
 static void kvm_xen_update_runstate_guest(struct kvm_vcpu *v, bool atomic)
 {
 	struct kvm_vcpu_xen *vx = &v->arch.xen;
@@ -440,6 +473,13 @@ static void kvm_xen_update_runstate_guest(struct kvm_vcpu *v, bool atomic)
 		mark_page_dirty_in_slot(v->kvm, gpc2->memslot, gpc2->gpa >> PAGE_SHIFT);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|813| <<kvm_xen_vcpu_set_attr>> kvm_xen_update_runstate(vcpu, data->u.runstate.state);
+ *   - arch/x86/kvm/xen.c|894| <<kvm_xen_vcpu_set_attr>> kvm_xen_update_runstate(vcpu, data->u.runstate.state);
+ *   - arch/x86/kvm/xen.h|157| <<kvm_xen_runstate_set_running>> kvm_xen_update_runstate(vcpu, RUNSTATE_running);
+ *   - arch/x86/kvm/xen.h|172| <<kvm_xen_runstate_set_preempted>> kvm_xen_update_runstate(vcpu, RUNSTATE_runnable);
+ */
 void kvm_xen_update_runstate(struct kvm_vcpu *v, int state)
 {
 	struct kvm_vcpu_xen *vx = &v->arch.xen;
@@ -471,6 +511,11 @@ void kvm_xen_update_runstate(struct kvm_vcpu *v, int state)
 		kvm_xen_update_runstate_guest(v, state == RUNSTATE_runnable);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|550| <<kvm_xen_inject_pending_events>> kvm_xen_inject_vcpu_vector(v);
+ *   - arch/x86/kvm/xen.c|1644| <<kvm_xen_set_evtchn_fast>> kvm_xen_inject_vcpu_vector(vcpu);
+ */
 static void kvm_xen_inject_vcpu_vector(struct kvm_vcpu *v)
 {
 	struct kvm_lapic_irq irq = { };
@@ -601,6 +646,10 @@ int __kvm_xen_has_interrupt(struct kvm_vcpu *v)
 	return rc;
 }
 
+/*
+ * 处理KVM_XEN_HVM_SET_ATTR:
+ *   - arch/x86/kvm/x86.c|7003| <<kvm_arch_vm_ioctl(KVM_XEN_HVM_SET_ATTR)>> r = kvm_xen_hvm_set_attr(kvm, &xha);
+ */
 int kvm_xen_hvm_set_attr(struct kvm *kvm, struct kvm_xen_hvm_attr *data)
 {
 	int r = -ENOENT;
@@ -711,6 +760,10 @@ int kvm_xen_hvm_get_attr(struct kvm *kvm, struct kvm_xen_hvm_attr *data)
 	return r;
 }
 
+/*
+ * 处理KVM_XEN_VCPU_SET_ATTR:
+ *   - arch/x86/kvm/x86.c|6044| <<kvm_arch_vcpu_ioctl(KVM_XEN_VCPU_SET_ATTR)>> r = kvm_xen_vcpu_set_attr(vcpu, &xva);
+ */
 int kvm_xen_vcpu_set_attr(struct kvm_vcpu *vcpu, struct kvm_xen_vcpu_attr *data)
 {
 	int idx, r = -ENOENT;
@@ -1038,6 +1091,10 @@ int kvm_xen_vcpu_get_attr(struct kvm_vcpu *vcpu, struct kvm_xen_vcpu_attr *data)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3635| <<kvm_set_msr_common>> return kvm_xen_write_hypercall_page(vcpu, data);
+ */
 int kvm_xen_write_hypercall_page(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -1109,6 +1166,10 @@ int kvm_xen_write_hypercall_page(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6983| <<kvm_arch_vm_ioctl(KVM_XEN_HVM_CONFIG)>> r = kvm_xen_hvm_config(kvm, &xhc);
+ */
 int kvm_xen_hvm_config(struct kvm *kvm, struct kvm_xen_hvm_config *xhc)
 {
 	/* Only some feature flags need to be *enabled* by userspace */
@@ -1146,6 +1207,10 @@ static int kvm_xen_hypercall_set_result(struct kvm_vcpu *vcpu, u64 result)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * 在以下使用kvm_xen_hypercall_complete_userspace():
+ *   - vcpu->arch.complete_userspace_io = kvm_xen_hypercall_complete_userspace;
+ */
 static int kvm_xen_hypercall_complete_userspace(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *run = vcpu->run;
@@ -1374,6 +1439,11 @@ static bool kvm_xen_hcall_vcpu_op(struct kvm_vcpu *vcpu, bool longmode, int cmd,
 			return true;
 		}
 
+		/*
+		 * struct vcpu_set_singleshot_timer oneshot;
+		 * -> uint64_t timeout_abs_ns;
+		 * -> uint32_t flags;
+		 */
 		delta = oneshot.timeout_abs_ns - get_kvmclock_ns(vcpu->kvm);
 		if ((oneshot.flags & VCPU_SSHOTTMR_future) && delta < 0) {
 			*r = -ETIME;
@@ -1431,6 +1501,10 @@ static bool kvm_xen_hcall_set_timer_op(struct kvm_vcpu *vcpu, uint64_t timeout,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9823| <<kvm_emulate_hypercall>> return kvm_xen_hypercall(vcpu);
+ */
 int kvm_xen_hypercall(struct kvm_vcpu *vcpu)
 {
 	bool longmode;
@@ -2103,6 +2177,10 @@ void kvm_xen_update_tsc_info(struct kvm_vcpu *vcpu)
 		entry->eax = vcpu->arch.hw_tsc_khz;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12361| <<kvm_arch_init_vm>> kvm_xen_init_vm(kvm);
+ */
 void kvm_xen_init_vm(struct kvm *kvm)
 {
 	mutex_init(&kvm->arch.xen.xen_lock);
@@ -2110,6 +2188,10 @@ void kvm_xen_init_vm(struct kvm *kvm)
 	kvm_gpc_init(&kvm->arch.xen.shinfo_cache, kvm, NULL, KVM_HOST_USES_PFN);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12508| <<kvm_arch_destroy_vm>> kvm_xen_destroy_vm(kvm);
+ */
 void kvm_xen_destroy_vm(struct kvm *kvm)
 {
 	struct evtchnfd *evtchnfd;
diff --git a/block/bdev.c b/block/bdev.c
index f3b13aa1b..da628fc55 100644
--- a/block/bdev.c
+++ b/block/bdev.c
@@ -631,6 +631,11 @@ static void blkdev_flush_mapping(struct block_device *bdev)
 	bdev_write_inode(bdev);
 }
 
+/*
+ * called by:
+ *   - block/bdev.c|671| <<blkdev_get_part>> ret = blkdev_get_whole(bdev_whole(part), mode);
+ *   - block/bdev.c|795| <<blkdev_get_by_dev>> ret = blkdev_get_whole(bdev, mode);
+ */
 static int blkdev_get_whole(struct block_device *bdev, blk_mode_t mode)
 {
 	struct gendisk *disk = bdev->bd_disk;
@@ -663,6 +668,10 @@ static void blkdev_put_whole(struct block_device *bdev)
 		bdev->bd_disk->fops->release(bdev->bd_disk);
 }
 
+/*
+ * called by:
+ *   - block/bdev.c|793| <<blkdev_get_by_dev>> ret = blkdev_get_part(bdev, mode);
+ */
 static int blkdev_get_part(struct block_device *part, blk_mode_t mode)
 {
 	struct gendisk *disk = part->bd_disk;
@@ -749,6 +758,31 @@ void blkdev_put_no_open(struct block_device *bdev)
  * RETURNS:
  * Reference to the block_device on success, ERR_PTR(-errno) on failure.
  */
+/*
+ * called by:
+ *   - block/bdev.c|860| <<blkdev_get_by_path>> bdev = blkdev_get_by_dev(dev, mode, holder, hops);
+ *   - block/fops.c|589| <<blkdev_open>> bdev = blkdev_get_by_dev(inode->i_rdev, file_to_blk_mode(filp),
+ *   - block/genhd.c|369| <<disk_scan_partitions>> bdev = blkdev_get_by_dev(disk_devt(disk), mode & ~BLK_OPEN_EXCL, NULL,
+ *   - block/ioctl.c|481| <<blkdev_bszset>> if (IS_ERR(blkdev_get_by_dev(bdev->bd_dev, mode, &bdev, NULL)))
+ *   - drivers/block/pktcdvd.c|2170| <<pkt_open_dev>> bdev = blkdev_get_by_dev(pd->bdev->bd_dev, BLK_OPEN_READ, pd, NULL);
+ *   - drivers/block/pktcdvd.c|2516| <<pkt_new_dev>> bdev = blkdev_get_by_dev(dev, BLK_OPEN_READ | BLK_OPEN_NDELAY, NULL,
+ *   - drivers/block/xen-blkback/xenbus.c|494| <<xen_vbd_create>> bdev = blkdev_get_by_dev(vbd->pdevice, vbd->readonly ?
+ *   - drivers/block/zram/zram_drv.c|510| <<backing_dev_store>> bdev = blkdev_get_by_dev(inode->i_rdev, BLK_OPEN_READ | BLK_OPEN_WRITE,
+ *   - drivers/md/bcache/super.c|2570| <<register_bcache>> bdev2 = blkdev_get_by_dev(bdev->bd_dev, BLK_OPEN_READ | BLK_OPEN_WRITE,
+ *   - drivers/md/dm.c|736| <<open_table_device>> bdev = blkdev_get_by_dev(dev, mode, _dm_claim_ptr, NULL);
+ *   - drivers/md/md.c|3658| <<md_import_device>> rdev->bdev = blkdev_get_by_dev(newdev, BLK_OPEN_READ | BLK_OPEN_WRITE,
+ *   - drivers/mtd/devices/block2mtd.c|252| <<mdtblock_early_get_bdev>> bdev = blkdev_get_by_dev(devt, mode, dev, NULL);
+ *   - drivers/s390/block/dasd_genhd.c|133| <<dasd_scan_partitions>> bdev = blkdev_get_by_dev(disk_devt(block->gdp), BLK_OPEN_READ, NULL,
+ *   - fs/ext4/super.c|5859| <<ext4_get_journal_blkdev>> bdev = blkdev_get_by_dev(j_dev, BLK_OPEN_READ | BLK_OPEN_WRITE, sb,
+ *   - fs/jfs/jfs_logmgr.c|1103| <<lmLogOpen>> bdev = blkdev_get_by_dev(sbi->logdev, BLK_OPEN_READ | BLK_OPEN_WRITE,
+ *   - fs/nfs/blocklayout/dev.c|246| <<bl_parse_simple>> bdev = blkdev_get_by_dev(dev, BLK_OPEN_READ | BLK_OPEN_WRITE, NULL,
+ *   - fs/ocfs2/cluster/heartbeat.c|1788| <<o2hb_region_dev_store>> reg->hr_bdev = blkdev_get_by_dev(f.file->f_mapping->host->i_rdev,
+ *   - fs/reiserfs/journal.c|2626| <<journal_init_dev>> journal->j_dev_bd = blkdev_get_by_dev(jdev, blkdev_mode, holder,
+ *   - fs/super.c|1484| <<setup_bdev_super>> bdev = blkdev_get_by_dev(sb->s_dev, mode, sb, &fs_holder_ops);
+ *   - kernel/power/swap.c|359| <<swsusp_swap_check>> hib_resume_bdev = blkdev_get_by_dev(swsusp_resume_device,
+ *   - kernel/power/swap.c|1525| <<swsusp_check>> hib_resume_bdev = blkdev_get_by_dev(swsusp_resume_device, BLK_OPEN_READ,
+ *   - mm/swapfile.c|2767| <<claim_swapfile>> p->bdev = blkdev_get_by_dev(inode->i_rdev,
+ */
 struct block_device *blkdev_get_by_dev(dev_t dev, blk_mode_t mode, void *holder,
 		const struct blk_holder_ops *hops)
 {
diff --git a/block/genhd.c b/block/genhd.c
index cc32a0c70..228df1619 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -340,6 +340,13 @@ void disk_uevent(struct gendisk *disk, enum kobject_action action)
 }
 EXPORT_SYMBOL_GPL(disk_uevent);
 
+/*
+ * called by:
+ *   - block/genhd.c|362| <<disk_scan_partitions>> ret = bd_prepare_to_claim(disk->part0, disk_scan_partitions,
+ *   - block/genhd.c|383| <<disk_scan_partitions>> bd_abort_claiming(disk->part0, disk_scan_partitions);
+ *   - block/genhd.c|510| <<device_add_disk>> disk_scan_partitions(disk, BLK_OPEN_READ);
+ *   - block/ioctl.c|555| <<blkdev_common_ioctl>> return disk_scan_partitions(bdev->bd_disk, mode);
+ */
 int disk_scan_partitions(struct gendisk *disk, blk_mode_t mode)
 {
 	struct block_device *bdev;
@@ -489,6 +496,12 @@ int __must_check device_add_disk(struct device *parent, struct gendisk *disk,
 	if (ret)
 		goto out_put_slave_dir;
 
+	/*
+	 * 注释:
+	 * ``GENHD_FL_HIDDEN``: the block device is hidden; it doesn't produce events,
+	 * doesn't appear in sysfs, and can't be opened from userspace or using
+	 * blkdev_get*. Used for the underlying components of multipath devices.
+	 */
 	if (!(disk->flags & GENHD_FL_HIDDEN)) {
 		ret = bdi_register(disk->bdi, "%u:%u",
 				   disk->major, disk->first_minor);
diff --git a/block/partitions/core.c b/block/partitions/core.c
index e137a87f4..b584719e9 100644
--- a/block/partitions/core.c
+++ b/block/partitions/core.c
@@ -12,6 +12,12 @@
 #include <linux/raid/detect.h>
 #include "check.h"
 
+/*
+ * 在以下使用check_part数组:
+ *   - block/partitions/core.c|15| <<global>> static int (*const check_part[])(struct parsed_partitions *) = {
+ *   - block/partitions/core.c|140| <<check_partition>> while (!res && check_part[i]) {
+ *   - block/partitions/core.c|142| <<check_partition>> res = check_part[i++](state);
+ */
 static int (*const check_part[])(struct parsed_partitions *) = {
 	/*
 	 * Probe partition formats with tables at disk address 0
@@ -111,6 +117,10 @@ static void free_partitions(struct parsed_partitions *state)
 	kfree(state);
 }
 
+/*
+ * called by:
+ *   - block/partitions/core.c|596| <<blk_add_partitions>> state = check_partition(disk);
+ */
 static struct parsed_partitions *check_partition(struct gendisk *hd)
 {
 	struct parsed_partitions *state;
@@ -133,6 +143,9 @@ static struct parsed_partitions *check_partition(struct gendisk *hd)
 		sprintf(state->name, "p");
 
 	i = res = err = 0;
+	/*
+	 * efi_partition
+	 */
 	while (!res && check_part[i]) {
 		memset(state->parts, 0, state->limit * sizeof(state->parts[0]));
 		res = check_part[i++](state);
@@ -582,6 +595,10 @@ static bool blk_add_partition(struct gendisk *disk,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/partitions/core.c|694| <<bdev_disk_changed>> ret = blk_add_partitions(disk);
+ */
 static int blk_add_partitions(struct gendisk *disk)
 {
 	struct parsed_partitions *state;
@@ -646,6 +663,15 @@ static int blk_add_partitions(struct gendisk *disk)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/bdev.c|645| <<blkdev_get_whole>> bdev_disk_changed(disk, true);
+ *   - block/bdev.c|653| <<blkdev_get_whole>> bdev_disk_changed(disk, false);
+ *   - drivers/block/loop.c|518| <<loop_reread_partitions>> rc = bdev_disk_changed(lo->lo_disk, false);
+ *   - drivers/block/loop.c|1189| <<__loop_clr_fd>> err = bdev_disk_changed(lo->lo_disk, false);
+ *   - drivers/s390/block/dasd_genhd.c|143| <<dasd_scan_partitions>> rc = bdev_disk_changed(block->gdp, false);
+ *   - drivers/s390/block/dasd_genhd.c|179| <<dasd_destroy_partitions>> bdev_disk_changed(bdev->bd_disk, true);
+ */
 int bdev_disk_changed(struct gendisk *disk, bool invalidate)
 {
 	struct block_device *part;
diff --git a/drivers/acpi/bus.c b/drivers/acpi/bus.c
index a4aa53b7e..38785ed62 100644
--- a/drivers/acpi/bus.c
+++ b/drivers/acpi/bus.c
@@ -464,6 +464,10 @@ static void acpi_bus_osc_negotiate_usb_control(void)
  *
  * This only handles notifications related to device hotplug.
  */
+/*
+ * 在以下使用acpi_bus_notify():
+ *   - drivers/acpi/bus.c|1365| <<acpi_bus_init>> status = acpi_install_notify_handler(ACPI_ROOT_OBJECT, ACPI_SYSTEM_NOTIFY, &acpi_bus_notify, NULL);
+ */
 static void acpi_bus_notify(acpi_handle handle, u32 type, void *data)
 {
 	struct acpi_device *adev;
diff --git a/drivers/acpi/osl.c b/drivers/acpi/osl.c
index f725813d0..600cb60f3 100644
--- a/drivers/acpi/osl.c
+++ b/drivers/acpi/osl.c
@@ -42,6 +42,52 @@
 #define _COMPONENT		ACPI_OS_SERVICES
 ACPI_MODULE_NAME("osl");
 
+/*
+ * [0] ACPI: OSL: debug: acpi_os_execute() type=2
+ * [0] CPU: 1 PID: 0 Comm: swapper/1 Not tainted 6.6.0 #3
+ * [0] acpi_os_execute
+ * [0] acpi_ev_gpe_dispatch
+ * [0] acpi_ev_detect_gpe
+ * [0] acpi_ev_gpe_detect
+ * [0] acpi_ev_sci_xrupt_handler
+ * [0] acpi_irq
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_fasteoi_irq
+ * [0] __common_interrupt
+ * [0] common_interrupt
+ *
+ * [0] ACPI: OSL: debug: acpi_os_execute() type=1
+ * [0] CPU: 0 PID: 11 Comm: kworker/0:1 Not tainted 6.6.0 #3
+ * [0] Workqueue: kacpid acpi_os_execute_deferred
+ * [0] acpi_os_execute
+ * [0] acpi_ev_queue_notify_request
+ * [0] acpi_ds_exec_end_op
+ * [0] acpi_ps_parse_loop
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] ACPI: OSL: debug: acpi_os_execute() type=1
+ * [0] CPU: 0 PID: 11 Comm: kworker/0:1 Not tainted 6.6.0 #3
+ * [0] Workqueue: kacpid acpi_os_execute_deferred
+ * [0] acpi_os_execute
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ */
+
 struct acpi_os_dpc {
 	acpi_osd_exec_callback function;
 	void *context;
@@ -65,6 +111,14 @@ static acpi_osd_handler acpi_irq_handler;
 static void *acpi_irq_context;
 static struct workqueue_struct *kacpid_wq;
 static struct workqueue_struct *kacpi_notify_wq;
+/*
+ * 在以下使用kacpi_hotplug_wq:
+ *   - drivers/acpi/osl.c|1187| <<acpi_hotplug_schedule>> if (!queue_work(kacpi_hotplug_wq, &hpw->work)) {
+ *   - drivers/acpi/osl.c|1196| <<acpi_queue_hotplug_work>> return queue_work(kacpi_hotplug_wq, work);
+ *   - drivers/acpi/osl.c|1674| <<acpi_os_initialize1>> kacpi_hotplug_wq = alloc_ordered_workqueue("kacpi_hotplug", 0);
+ *   - drivers/acpi/osl.c|1677| <<acpi_os_initialize1>> BUG_ON(!kacpi_hotplug_wq);
+ *   - drivers/acpi/osl.c|1702| <<acpi_os_terminate>> destroy_workqueue(kacpi_hotplug_wq);
+ */
 static struct workqueue_struct *kacpi_hotplug_wq;
 static bool acpi_os_initialized;
 unsigned int acpi_sci_irq = INVALID_ACPI_IRQ;
@@ -843,6 +897,24 @@ acpi_os_write_pci_configuration(struct acpi_pci_id * pci_id, u32 reg,
 }
 #endif
 
+/*
+ * [0] ACPI: OSL: debug: acpi_hotplug_schedule() src=1 --> 是ACPI_NOTIFY_DEVICE_CHECK
+ * [0] CPU: 0 PID: 103 Comm: kworker/0:2 Not tainted 6.6.0 #2
+ * [0] Workqueue: kacpi_notify acpi_os_execute_deferred
+ * [0] acpi_hotplug_schedule
+ * [0] acpi_bus_notify
+ * [0] acpi_ev_notify_dispatch
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * 在以下使用acpi_os_execute_deferred():
+ *   - drivers/acpi/osl.c|1114| <<acpi_os_execute>> INIT_WORK(&dpc->work, acpi_os_execute_deferred);
+ *   - drivers/acpi/osl.c|1117| <<acpi_os_execute>> INIT_WORK(&dpc->work, acpi_os_execute_deferred);
+ */
 static void acpi_os_execute_deferred(struct work_struct *work)
 {
 	struct acpi_os_dpc *dpc = container_of(work, struct acpi_os_dpc, work);
@@ -1060,6 +1132,63 @@ int __init acpi_debugger_init(void)
  *
  ******************************************************************************/
 
+/*
+ * [0] ACPI: OSL: debug: acpi_os_execute() type=2
+ * [0] CPU: 1 PID: 0 Comm: swapper/1 Not tainted 6.6.0 #3
+ * [0] acpi_os_execute
+ * [0] acpi_ev_gpe_dispatch
+ * [0] acpi_ev_detect_gpe
+ * [0] acpi_ev_gpe_detect
+ * [0] acpi_ev_sci_xrupt_handler
+ * [0] acpi_irq
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_fasteoi_irq
+ * [0] __common_interrupt
+ * [0] common_interrupt
+ *
+ * [0] ACPI: OSL: debug: acpi_os_execute() type=1
+ * [0] CPU: 0 PID: 11 Comm: kworker/0:1 Not tainted 6.6.0 #3
+ * [0] Workqueue: kacpid acpi_os_execute_deferred
+ * [0] acpi_os_execute
+ * [0] acpi_ev_queue_notify_request
+ * [0] acpi_ds_exec_end_op
+ * [0] acpi_ps_parse_loop
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] ACPI: OSL: debug: acpi_os_execute() type=1
+ * [0] CPU: 0 PID: 11 Comm: kworker/0:1 Not tainted 6.6.0 #3
+ * [0] Workqueue: kacpid acpi_os_execute_deferred
+ * [0] acpi_os_execute
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - drivers/acpi/acpica/dbexec.c|695| <<acpi_db_create_execution_thread>> status = acpi_os_execute(OSL_DEBUGGER_EXEC_THREAD, acpi_db_single_execution_thread, &acpi_gbl_db_method_info);
+ *   - drivers/acpi/acpica/dbexec.c|849| <<acpi_db_create_execution_threads>> acpi_os_execute(OSL_DEBUGGER_EXEC_THREAD, acpi_db_method_thread, &acpi_gbl_db_method_info);
+ *   - drivers/acpi/acpica/dbxface.c|448| <<acpi_initialize_debugger>> status = acpi_os_execute(OSL_DEBUGGER_MAIN_THREAD, acpi_db_execute_thread, NULL);
+ *   - drivers/acpi/acpica/evgpe.c|526| <<acpi_ev_asynch_execute_gpe_method>> status = acpi_os_execute(OSL_NOTIFY_HANDLER, acpi_ev_asynch_enable_gpe, gpe_event_info);
+ *   - drivers/acpi/acpica/evgpe.c|823| <<acpi_ev_gpe_dispatch>> status = acpi_os_execute(OSL_GPE_HANDLER, acpi_ev_asynch_execute_gpe_method, gpe_event_info);
+ *   - drivers/acpi/acpica/evmisc.c|139| <<acpi_ev_queue_notify_request>> status = acpi_os_execute(OSL_NOTIFY_HANDLER, acpi_ev_notify_dispatch, info);
+ *   - drivers/acpi/button.c|467| <<acpi_button_event>> acpi_os_execute(OSL_NOTIFY_HANDLER, acpi_button_notify_run, data);
+ *   - drivers/acpi/sbshc.c|232| <<smbus_alarm>> acpi_os_execute(OSL_NOTIFY_HANDLER, acpi_smbus_callback, hc);
+ *   - drivers/acpi/tiny-power-button.c|34| <<acpi_tiny_power_button_event>> acpi_os_execute(OSL_NOTIFY_HANDLER, acpi_tiny_power_button_notify_run, NULL);
+ *   - drivers/platform/x86/dell/dell-rbtn.c|278| <<rbtn_resume>> status = acpi_os_execute(OSL_NOTIFY_HANDLER, rbtn_clear_suspended_flag, rbtn_data);
+ */
 acpi_status acpi_os_execute(acpi_execute_type type,
 			    acpi_osd_exec_callback function, void *context)
 {
@@ -1154,6 +1283,10 @@ struct acpi_hp_work {
 	u32 src;
 };
 
+/*
+ * 在以下使用acpi_hotplug_work_fn():
+ *   - drivers/acpi/osl.c|1178| <<acpi_hotplug_schedule>> INIT_WORK(&hpw->work, acpi_hotplug_work_fn);
+ */
 static void acpi_hotplug_work_fn(struct work_struct *work)
 {
 	struct acpi_hp_work *hpw = container_of(work, struct acpi_hp_work, work);
@@ -1163,6 +1296,24 @@ static void acpi_hotplug_work_fn(struct work_struct *work)
 	kfree(hpw);
 }
 
+/*
+ * [0] ACPI: OSL: debug: acpi_hotplug_schedule() src=1 --> 是ACPI_NOTIFY_DEVICE_CHECK
+ * [0] CPU: 0 PID: 103 Comm: kworker/0:2 Not tainted 6.6.0 #2
+ * [0] Workqueue: kacpi_notify acpi_os_execute_deferred
+ * [0] acpi_hotplug_schedule
+ * [0] acpi_bus_notify
+ * [0] acpi_ev_notify_dispatch
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - drivers/acpi/bus.c|514| <<acpi_bus_notify>> if (adev && ACPI_SUCCESS(acpi_hotplug_schedule(adev, type)))
+ *   - drivers/acpi/device_sysfs.c|387| <<eject_store>> status = acpi_hotplug_schedule(acpi_device, ACPI_OST_EC_OSPM_EJECT);
+ */
 acpi_status acpi_hotplug_schedule(struct acpi_device *adev, u32 src)
 {
 	struct acpi_hp_work *hpw;
diff --git a/drivers/acpi/scan.c b/drivers/acpi/scan.c
index 691d4b768..1625d10c1 100644
--- a/drivers/acpi/scan.c
+++ b/drivers/acpi/scan.c
@@ -373,6 +373,10 @@ static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/osl.c|1174| <<acpi_hotplug_work_fn>> acpi_device_hotplug(hpw->adev, hpw->src);
+ */
 void acpi_device_hotplug(struct acpi_device *adev, u32 src)
 {
 	u32 ost_code = ACPI_OST_SC_NON_SPECIFIC_FAILURE;
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index 1fe011676..af6a14d8d 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -348,6 +348,41 @@ static inline void virtblk_request_done(struct request *req)
 	blk_mq_end_request(req, status);
 }
 
+/*
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_fasteoi_irq
+ * [0] generic_handle_domain_irq
+ * [0] gic_handle_irq
+ * [0] call_on_irq_stack
+ * [0] do_interrupt_handler
+ * [0] el1_interrupt
+ * [0] el1h_64_irq_handler
+ * [0] el1h_64_irq
+ * [0] __folio_start_writeback
+ * [0] ext4_bio_write_folio
+ * [0] mpage_submit_folio
+ * [0] mpage_process_page_bufs
+ * [0] mpage_prepare_extent_to_map
+ * [0] ext4_do_writepages
+ * [0] ext4_writepages
+ * [0] do_writepages
+ * [0] filemap_fdatawrite_wbc
+ * [0] __filemap_fdatawrite_range
+ * [0] file_write_and_wait_range
+ * [0] ext4_sync_file
+ * [0] vfs_fsync_range
+ * [0] do_fsync
+ * [0] __arm64_sys_fsync
+ * [0] invoke_syscall
+ * [0] el0_svc_common.constprop
+ * [0] do_el0_svc
+ * [0] el0_svc
+ * [0] el0t_64_sync_handler
+ * [0] el0t_64_sync
+ */
 static void virtblk_done(struct virtqueue *vq)
 {
 	struct virtio_blk *vblk = vq->vdev->priv;
diff --git a/drivers/clocksource/arm_arch_timer.c b/drivers/clocksource/arm_arch_timer.c
index 7dd2c615b..0f54afcab 100644
--- a/drivers/clocksource/arm_arch_timer.c
+++ b/drivers/clocksource/arm_arch_timer.c
@@ -59,6 +59,12 @@
  */
 #define MIN_ROLLOVER_SECS	(40ULL * 365 * 24 * 3600)
 
+/*
+ * 在以下修改arch_timers_present:
+ *   - drivers/clocksource/arm_arch_timer.c|1409| <<arch_timer_of_init>> arch_timers_present |= ARCH_TIMER_TYPE_CP15;
+ *   - drivers/clocksource/arm_arch_timer.c|1566| <<arch_timer_mem_frame_register>> arch_timers_present |= ARCH_TIMER_TYPE_MEM;
+ *   - drivers/clocksource/arm_arch_timer.c|1734| <<arch_timer_acpi_init>> arch_timers_present |= ARCH_TIMER_TYPE_CP15
+ */
 static unsigned arch_timers_present __initdata;
 
 struct arch_timer {
@@ -83,6 +89,17 @@ static const char *arch_timer_ppi_names[ARCH_TIMER_MAX_TIMER_PPI] = {
 
 static struct clock_event_device __percpu *arch_timer_evt;
 
+/*
+ * 5.4的例子.
+ *
+ * crash> arch_timer_uses_ppi
+ * arch_timer_uses_ppi = $1 = ARCH_TIMER_HYP_PPI
+ *
+ * 在以下使用arch_timer_uses_ppi:
+ *   - drivers/clocksource/arm_arch_timer.c|1438| <<arch_timer_of_init>> arch_timer_uses_ppi = ARCH_TIMER_PHYS_SECURE_PPI;
+ *   - drivers/clocksource/arm_arch_timer.c|1440| <<arch_timer_of_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+ *   - drivers/clocksource/arm_arch_timer.c|1762| <<arch_timer_acpi_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+ */
 static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
 static bool arch_timer_c3stop __ro_after_init;
 static bool arch_timer_mem_use_virtual __ro_after_init;
@@ -413,6 +430,13 @@ EXPORT_SYMBOL_GPL(timer_unstable_counter_workaround);
 
 static atomic_t timer_unstable_counter_workaround_in_use = ATOMIC_INIT(0);
 
+/*
+ * 有两种方式可以配置和使用一个timer:
+ * 1. CVAL(comparatoer)寄存器,通过设置比较器的值,当System Count >= CVAL时满足触发条件
+ * 2. TVAL寄存器,设置TVAL寄存器值后,比较器的值CVAL = TVAL + System Counter,当System Count >= CVAL时满足触发条件,
+ * TVAL是一个有符号数,当递减到0时还会继续递减,因此可以记录timer是在多久之前触发的
+ */
+
 /*
  * Force the inlining of this function so that the register accesses
  * can be themselves correctly inlined.
@@ -662,6 +686,13 @@ static bool arch_timer_counter_has_wa(void)
 #define arch_timer_counter_has_wa()			({false;})
 #endif /* CONFIG_ARM_ARCH_TIMER_OOL_WORKAROUND */
 
+/*
+ * called by:
+ *   - drivers/clocksource/arm_arch_timer.c|685| <<arch_timer_handler_virt>> return timer_handler(ARCH_TIMER_VIRT_ACCESS, evt);
+ *   - drivers/clocksource/arm_arch_timer.c|692| <<arch_timer_handler_phys>> return timer_handler(ARCH_TIMER_PHYS_ACCESS, evt);
+ *   - drivers/clocksource/arm_arch_timer.c|699| <<arch_timer_handler_phys_mem>> return timer_handler(ARCH_TIMER_MEM_PHYS_ACCESS, evt);
+ *   - drivers/clocksource/arm_arch_timer.c|706| <<arch_timer_handler_virt_mem>> return timer_handler(ARCH_TIMER_MEM_VIRT_ACCESS, evt);
+ */
 static __always_inline irqreturn_t timer_handler(const int access,
 					struct clock_event_device *evt)
 {
@@ -738,6 +769,11 @@ static int arch_timer_shutdown_phys_mem(struct clock_event_device *clk)
 	return arch_timer_shutdown(ARCH_TIMER_MEM_PHYS_ACCESS, clk);
 }
 
+/*
+ * 在以下使用set_next_event():
+ *   - drivers/clocksource/arm_arch_timer.c|787| <<arch_timer_set_next_event_virt>> set_next_event(ARCH_TIMER_VIRT_ACCESS, evt, clk);
+ *   - drivers/clocksource/arm_arch_timer.c|794| <<arch_timer_set_next_event_phys>> set_next_event(ARCH_TIMER_PHYS_ACCESS, evt, clk);
+ */
 static __always_inline void set_next_event(const int access, unsigned long evt,
 					   struct clock_event_device *clk)
 {
@@ -1097,6 +1133,10 @@ struct arch_timer_kvm_info *arch_timer_get_kvm_info(void)
 	return &arch_timer_kvm_info;
 }
 
+/*
+ * called by:
+ *   - drivers/clocksource/arm_arch_timer.c|1355| <<arch_timer_common_init>> arch_counter_register(arch_timers_present);
+ */
 static void __init arch_counter_register(unsigned type)
 {
 	u64 (*scr)(void);
@@ -1225,6 +1265,11 @@ static int __init arch_timer_register(void)
 	}
 
 	ppi = arch_timer_ppi[arch_timer_uses_ppi];
+	/*
+	 * 5.4的KVM的例子.
+	 * crash> arch_timer_uses_ppi
+	 * arch_timer_uses_ppi = $4 = ARCH_TIMER_HYP_PPI
+	 */
 	switch (arch_timer_uses_ppi) {
 	case ARCH_TIMER_VIRT_PPI:
 		err = request_percpu_irq(ppi, arch_timer_handler_virt,
diff --git a/drivers/cpuidle/cpuidle.c b/drivers/cpuidle/cpuidle.c
index 737a026ef..17f7a7c6d 100644
--- a/drivers/cpuidle/cpuidle.c
+++ b/drivers/cpuidle/cpuidle.c
@@ -208,6 +208,13 @@ int cpuidle_enter_s2idle(struct cpuidle_driver *drv, struct cpuidle_device *dev)
  * @drv: cpuidle driver for this cpu
  * @index: index into the states table in @drv of the state to enter
  */
+/*
+ * called by:
+ *   - drivers/cpuidle/coupled.c|486| <<cpuidle_enter_state_coupled>> entered_state = cpuidle_enter_state(dev, drv,
+ *   - drivers/cpuidle/coupled.c|534| <<cpuidle_enter_state_coupled>> entered_state = cpuidle_enter_state(dev, drv,
+ *   - drivers/cpuidle/coupled.c|595| <<cpuidle_enter_state_coupled>> entered_state = cpuidle_enter_state(dev, drv, next_state);
+ *   - drivers/cpuidle/cpuidle.c|388| <<cpuidle_enter>> ret = cpuidle_enter_state(dev, drv, index);
+ */
 noinstr int cpuidle_enter_state(struct cpuidle_device *dev,
 				 struct cpuidle_driver *drv,
 				 int index)
diff --git a/drivers/cpuidle/poll_state.c b/drivers/cpuidle/poll_state.c
index 9b6d90a72..1f44b8b1f 100644
--- a/drivers/cpuidle/poll_state.c
+++ b/drivers/cpuidle/poll_state.c
@@ -10,6 +10,16 @@
 
 #define POLL_IDLE_RELAX_COUNT	200
 
+/*
+ * [0] CPU: 1 PID: 0 Comm: swapper/1 Not tainted 6.6.0 #5
+ * [0] poll_idle
+ * [0] cpuidle_enter_state
+ * [0] cpuidle_enter
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64_no_verify
+ */
 static int __cpuidle poll_idle(struct cpuidle_device *dev,
 			       struct cpuidle_driver *drv, int index)
 {
diff --git a/drivers/irqchip/irq-gic-v3.c b/drivers/irqchip/irq-gic-v3.c
index f59ac9586..ac843a425 100644
--- a/drivers/irqchip/irq-gic-v3.c
+++ b/drivers/irqchip/irq-gic-v3.c
@@ -243,6 +243,11 @@ static inline void __iomem *gic_dist_base(struct irq_data *d)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|264| <<gic_dist_wait_for_rwp>> gic_do_wait_for_rwp(gic_data.dist_base, GICD_CTLR_RWP);
+ *   - drivers/irqchip/irq-gic-v3.c|270| <<gic_redist_wait_for_rwp>> gic_do_wait_for_rwp(gic_data_rdist_rd_base(), GICR_CTLR_RWP);
+ */
 static void gic_do_wait_for_rwp(void __iomem *base, u32 bit)
 {
 	u32 count = 1000000;	/* 1s! */
@@ -259,12 +264,23 @@ static void gic_do_wait_for_rwp(void __iomem *base, u32 bit)
 }
 
 /* Wait for completion of a distributor change */
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|412| <<gic_mask_irq>> gic_dist_wait_for_rwp();
+ *   - drivers/irqchip/irq-gic-v3.c|916| <<gic_dist_init>> gic_dist_wait_for_rwp();
+ *   - drivers/irqchip/irq-gic-v3.c|953| <<gic_dist_init>> gic_dist_wait_for_rwp();
+ */
 static void gic_dist_wait_for_rwp(void)
 {
 	gic_do_wait_for_rwp(gic_data.dist_base, GICD_CTLR_RWP);
 }
 
 /* Wait for completion of a redistributor change */
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|410| <<gic_mask_irq>> gic_redist_wait_for_rwp();
+ *   - drivers/irqchip/irq-gic-v3.c|1285| <<gic_cpu_init>> gic_cpu_config(rbase, gic_data.ppi_nr + 16, gic_redist_wait_for_rwp);
+ */
 static void gic_redist_wait_for_rwp(void)
 {
 	gic_do_wait_for_rwp(gic_data_rdist_rd_base(), GICR_CTLR_RWP);
diff --git a/drivers/irqchip/irq-gic-v4.c b/drivers/irqchip/irq-gic-v4.c
index 94d56a03b..5910bb128 100644
--- a/drivers/irqchip/irq-gic-v4.c
+++ b/drivers/irqchip/irq-gic-v4.c
@@ -306,6 +306,11 @@ int its_invall_vpe(struct its_vpe *vpe)
 	return its_send_vpe_cmd(vpe, &info);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|375| <<update_affinity>> ret = its_map_vlpi(irq->host_irq, &map);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|455| <<kvm_vgic_v4_set_forwarding>> ret = its_map_vlpi(virq, &map);
+ */
 int its_map_vlpi(int irq, struct its_vlpi_map *map)
 {
 	struct its_cmd_info info = {
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index d67f742fb..7ede5eaf2 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -422,6 +422,14 @@ static void disable_delayed_refill(struct virtnet_info *vi)
 	spin_unlock_bh(&vi->refill_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|442| <<virtqueue_napi_complete>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|457| <<skb_xmit_done>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|816| <<check_sq_full_and_disable>> virtqueue_napi_schedule(&sq->napi, sq->vq);
+ *   - drivers/net/virtio_net.c|2000| <<skb_recv_done>> virtqueue_napi_schedule(&rq->napi, rvq);
+ *   - drivers/net/virtio_net.c|2012| <<virtnet_napi_enable>> virtqueue_napi_schedule(napi, vq);
+ */
 static void virtqueue_napi_schedule(struct napi_struct *napi,
 				    struct virtqueue *vq)
 {
@@ -1963,6 +1971,13 @@ static int add_recvbuf_mergeable(struct virtnet_info *vi,
  * before we're receiving packets, or from refill_work which is
  * careful to disable receiving (using napi_disable).
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2097| <<refill_work>> still_empty = !try_fill_recv(vi, rq, GFP_KERNEL);
+ *   - drivers/net/virtio_net.c|2134| <<virtnet_receive>> if (!try_fill_recv(vi, rq, GFP_ATOMIC)) {
+ *   - drivers/net/virtio_net.c|2258| <<virtnet_open>> if (!try_fill_recv(vi, &vi->rq[i], GFP_KERNEL))
+ *   - drivers/net/virtio_net.c|2442| <<virtnet_rx_resize>> if (!try_fill_recv(vi, rq, GFP_KERNEL))
+ */
 static bool try_fill_recv(struct virtnet_info *vi, struct receive_queue *rq,
 			  gfp_t gfp)
 {
@@ -1992,6 +2007,44 @@ static bool try_fill_recv(struct virtnet_info *vi, struct receive_queue *rq,
 	return !oom;
 }
 
+/*
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_fasteoi_irq
+ * [0] generic_handle_domain_irq
+ * [0] gic_handle_irq
+ * [0] call_on_irq_stack
+ * [0] do_interrupt_handler
+ * [0] el1_interrupt
+ * [0] el1h_64_irq_handler
+ * [0] el1h_64_irq
+ * [0] __folio_start_writeback
+ * [0] ext4_bio_write_folio
+ * [0] mpage_submit_folio
+ * [0] mpage_process_page_bufs
+ * [0] mpage_prepare_extent_to_map
+ * [0] ext4_do_writepages
+ * [0] ext4_writepages
+ * [0] do_writepages
+ * [0] filemap_fdatawrite_wbc
+ * [0] __filemap_fdatawrite_range
+ * [0] file_write_and_wait_range
+ * [0] ext4_sync_file
+ * [0] vfs_fsync_range
+ * [0] do_fsync
+ * [0] __arm64_sys_fsync
+ * [0] invoke_syscall
+ * [0] el0_svc_common.constprop
+ * [0] do_el0_svc
+ * [0] el0_svc
+ * [0] el0t_64_sync_handler
+ * [0] el0t_64_sync
+ *
+ * called by:
+ *   - drivers/net/virtio_net.c|4099| <<virtnet_find_vqs>> callbacks[rxq2vq(i)] = skb_recv_done;
+ */
 static void skb_recv_done(struct virtqueue *rvq)
 {
 	struct virtnet_info *vi = rvq->vdev->priv;
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index 21783aa2e..221252d19 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -126,8 +126,26 @@ static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 static void nvme_update_keep_alive(struct nvme_ctrl *ctrl,
 				   struct nvme_command *cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1144| <<nvme_passthru_end>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|4160| <<nvme_handle_aen_notice>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|4396| <<nvme_start_ctrl>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/ioctl.c|976| <<nvme_dev_ioctl>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/sysfs.c|33| <<nvme_sysfs_rescan>> nvme_queue_scan(ctrl);
+ */
 void nvme_queue_scan(struct nvme_ctrl *ctrl)
 {
+	/*
+	 * 在以下使用nvme_ctrl->scan_work:
+	 *   - drivers/nvme/host/apple.c|1234| <<apple_nvme_async_probe>> flush_work(&anv->ctrl.scan_work);
+	 *   - drivers/nvme/host/core.c|135| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|1145| <<nvme_passthru_end>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|3920| <<nvme_scan_work>> container_of(work, struct nvme_ctrl, scan_work);
+	 *   - drivers/nvme/host/core.c|3986| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|4488| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+	 *   - drivers/nvme/host/pci.c|3125| <<nvme_probe>> flush_work(&dev->ctrl.scan_work);
+	 */
 	/*
 	 * Only new queue scan work when admin and IO queues are both alive
 	 */
@@ -612,6 +630,11 @@ static bool nvme_state_terminal(struct nvme_ctrl *ctrl)
  * Waits for the controller state to be resetting, or returns false if it is
  * not possible to ever transition to that state.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/nvme.h|645| <<nvme_reset_subsystem>> if (!nvme_wait_reset(ctrl))
+ *   - drivers/nvme/host/pci.c|2786| <<nvme_disable_prepare_reset>> if (!nvme_wait_reset(&dev->ctrl))
+ */
 bool nvme_wait_reset(struct nvme_ctrl *ctrl)
 {
 	wait_event(ctrl->state_wq,
@@ -701,6 +724,15 @@ EXPORT_SYMBOL_GPL(nvme_init_request);
  * Note: commands used to initialize the controller will be marked for failfast.
  * Note: nvme cli/ioctl commands are marked for failfast.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/apple.c|754| <<apple_nvme_queue_rq>> return nvme_fail_nonready_command(&anv->ctrl, req);
+ *   - drivers/nvme/host/fc.c|2814| <<nvme_fc_queue_rq>> return nvme_fail_nonready_command(&queue->ctrl->ctrl, rq);
+ *   - drivers/nvme/host/pci.c|891| <<nvme_queue_rq>> return nvme_fail_nonready_command(&dev->ctrl, req);
+ *   - drivers/nvme/host/rdma.c|1990| <<nvme_rdma_queue_rq>> return nvme_fail_nonready_command(&queue->ctrl->ctrl, rq);
+ *   - drivers/nvme/host/tcp.c|2396| <<nvme_tcp_queue_rq>> return nvme_fail_nonready_command(&queue->ctrl->ctrl, rq);
+ *   - drivers/nvme/target/loop.c|142| <<nvme_loop_queue_rq>> return nvme_fail_nonready_command(&queue->ctrl->ctrl, req);
+ */
 blk_status_t nvme_fail_nonready_command(struct nvme_ctrl *ctrl,
 		struct request *rq)
 {
@@ -1316,6 +1348,11 @@ static bool nvme_ctrl_limited_cns(struct nvme_ctrl *ctrl)
 	return ctrl->vs < NVME_VS(1, 1, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3011| <<nvme_init_identify>> ret = nvme_identify_ctrl(ctrl, &id);
+ *   - drivers/nvme/host/core.c|3881| <<nvme_scan_ns_sequential>> if (nvme_identify_ctrl(ctrl, &id))
+ */
 static int nvme_identify_ctrl(struct nvme_ctrl *dev, struct nvme_id_ctrl **id)
 {
 	struct nvme_command c = { };
@@ -2015,6 +2052,11 @@ static int nvme_update_ns_info_generic(struct nvme_ns *ns,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2100| <<nvme_update_ns_info>> return nvme_update_ns_info_block(ns, info);
+ *   - drivers/nvme/host/core.c|2102| <<nvme_update_ns_info>> return nvme_update_ns_info_block(ns, info);
+ */
 static int nvme_update_ns_info_block(struct nvme_ns *ns,
 		struct nvme_ns_info *info)
 {
@@ -2743,6 +2785,10 @@ static bool nvme_validate_cntlid(struct nvme_subsystem *subsys,
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3068| <<nvme_init_identify>> ret = nvme_init_subsystem(ctrl, id);
+ */
 static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 {
 	struct nvme_subsystem *subsys, *found;
@@ -2996,6 +3042,10 @@ static int nvme_init_effects(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3208| <<nvme_init_ctrl_finish>> ret = nvme_init_identify(ctrl);
+ */
 static int nvme_init_identify(struct nvme_ctrl *ctrl)
 {
 	struct nvme_id_ctrl *id;
@@ -3153,6 +3203,16 @@ static int nvme_init_identify(struct nvme_ctrl *ctrl)
  * register in our nvme_ctrl structure.  This should be called as soon as
  * the admin queue is fully up and running.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/apple.c|1119| <<apple_nvme_reset_work>> ret = nvme_init_ctrl_finish(&anv->ctrl, false);
+ *   - drivers/nvme/host/fc.c|3134| <<nvme_fc_create_association>> ret = nvme_init_ctrl_finish(&ctrl->ctrl, false);
+ *   - drivers/nvme/host/pci.c|2895| <<nvme_reset_work>> result = nvme_init_ctrl_finish(&dev->ctrl, was_suspend);
+ *   - drivers/nvme/host/pci.c|3231| <<nvme_probe>> result = nvme_init_ctrl_finish(&dev->ctrl, false);
+ *   - drivers/nvme/host/rdma.c|838| <<nvme_rdma_configure_admin_queue>> error = nvme_init_ctrl_finish(&ctrl->ctrl, false);
+ *   - drivers/nvme/host/tcp.c|1947| <<nvme_tcp_configure_admin_queue>> error = nvme_init_ctrl_finish(ctrl, false);
+ *   - drivers/nvme/target/loop.c|380| <<nvme_loop_configure_admin_queue>> error = nvme_init_ctrl_finish(&ctrl->ctrl, false);
+ */
 int nvme_init_ctrl_finish(struct nvme_ctrl *ctrl, bool was_suspended)
 {
 	int ret;
@@ -3570,6 +3630,10 @@ static void nvme_ns_add_to_ctrl_list(struct nvme_ns *ns)
 	list_add(&ns->list, &ns->ctrl->namespaces);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3784| <<nvme_scan_ns>> nvme_alloc_ns(ctrl, &info);
+ */
 static void nvme_alloc_ns(struct nvme_ctrl *ctrl, struct nvme_ns_info *info)
 {
 	struct nvme_ns *ns;
@@ -3634,6 +3698,13 @@ static void nvme_alloc_ns(struct nvme_ctrl *ctrl, struct nvme_ns_info *info)
 	up_write(&ctrl->namespaces_rwsem);
 	nvme_get_ctrl(ctrl);
 
+	/*
+	 * nvme_scan_work()
+	 * -> nvme_scan_ns_sequential()
+	 *    -> nvme_scan_ns()
+	 *       -> nvme_alloc_ns()
+	 *          -> device_add_disk()
+	 */
 	if (device_add_disk(ctrl->device, ns->disk, nvme_ns_id_attr_groups))
 		goto out_cleanup_ns_from_list;
 
@@ -3718,6 +3789,10 @@ static void nvme_ns_remove_by_nsid(struct nvme_ctrl *ctrl, u32 nsid)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3781| <<nvme_scan_ns>> nvme_validate_ns(ns, &info);
+ */
 static void nvme_validate_ns(struct nvme_ns *ns, struct nvme_ns_info *info)
 {
 	int ret = NVME_SC_INVALID_NS | NVME_SC_DNR;
@@ -3740,6 +3815,11 @@ static void nvme_validate_ns(struct nvme_ns *ns, struct nvme_ns_info *info)
 		nvme_ns_remove(ns);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3837| <<nvme_scan_ns_list>> nvme_scan_ns(ctrl, nsid);
+ *   - drivers/nvme/host/core.c|3860| <<nvme_scan_ns_sequential>> nvme_scan_ns(ctrl, i);
+ */
 static void nvme_scan_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 {
 	struct nvme_ns_info info = { .nsid = nsid };
@@ -3781,6 +3861,9 @@ static void nvme_scan_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 		nvme_validate_ns(ns, &info);
 		nvme_put_ns(ns);
 	} else {
+		/*
+		 * 只在这里调用
+		 */
 		nvme_alloc_ns(ctrl, &info);
 	}
 }
@@ -3803,6 +3886,10 @@ static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3946| <<nvme_scan_work>> ret = nvme_scan_ns_list(ctrl);
+ */
 static int nvme_scan_ns_list(struct nvme_ctrl *ctrl)
 {
 	const int nr_entries = NVME_IDENTIFY_DATA_SIZE / sizeof(__le32);
@@ -3846,11 +3933,21 @@ static int nvme_scan_ns_list(struct nvme_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3939| <<nvme_scan_work>> nvme_scan_ns_sequential(ctrl);
+ *   - drivers/nvme/host/core.c|3948| <<nvme_scan_work>> nvme_scan_ns_sequential(ctrl);
+ */
 static void nvme_scan_ns_sequential(struct nvme_ctrl *ctrl)
 {
 	struct nvme_id_ctrl *id;
 	u32 nn, i;
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/core.c|3011| <<nvme_init_identify>> ret = nvme_identify_ctrl(ctrl, &id);
+	 *   - drivers/nvme/host/core.c|3881| <<nvme_scan_ns_sequential>> if (nvme_identify_ctrl(ctrl, &id))
+	 */
 	if (nvme_identify_ctrl(ctrl, &id))
 		return;
 	nn = le32_to_cpu(id->nn);
@@ -3887,6 +3984,46 @@ static void nvme_clear_changed_ns_log(struct nvme_ctrl *ctrl)
 	kfree(log);
 }
 
+/*
+ * [0] CPU: 0 PID: 63 Comm: kworker/u8:3 Not tainted 6.6.0 #2
+ * [0] Workqueue: nvme-wq nvme_scan_work
+ * [0] blkdev_read_folio
+ * [0] filemap_read_folio
+ * [0] do_read_cache_folio
+ * [0] read_part_sector
+ * [0] read_lba
+ * [0] efi_partition
+ * [0] bdev_disk_changed
+ * [0] blkdev_get_whole
+ * [0] blkdev_get_by_dev
+ * [0] disk_scan_partitions
+ * [0] device_add_disk
+ * [0] nvme_scan_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * 在以下使用nvme_ctrl->scan_work:
+ *   - drivers/nvme/host/apple.c|1234| <<apple_nvme_async_probe>> flush_work(&anv->ctrl.scan_work);
+ *   - drivers/nvme/host/core.c|135| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+ *   - drivers/nvme/host/core.c|1145| <<nvme_passthru_end>> flush_work(&ctrl->scan_work);
+ *   - drivers/nvme/host/core.c|3920| <<nvme_scan_work>> container_of(work, struct nvme_ctrl, scan_work);
+ *   - drivers/nvme/host/core.c|3986| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+ *   - drivers/nvme/host/core.c|4488| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+ *   - drivers/nvme/host/pci.c|3125| <<nvme_probe>> flush_work(&dev->ctrl.scan_work);
+ *
+ * 在以下使用nvme_scan_work():
+ *   - drivers/nvme/host/core.c|4488| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+ *
+ * nvme_scan_work()
+ * -> nvme_scan_ns_sequential()
+ *    -> nvme_scan_ns()
+ *       -> nvme_alloc_ns()
+ *          -> device_add_disk()
+ */
 static void nvme_scan_work(struct work_struct *work)
 {
 	struct nvme_ctrl *ctrl =
@@ -3955,6 +4092,16 @@ void nvme_remove_namespaces(struct nvme_ctrl *ctrl)
 	 */
 	nvme_unquiesce_io_queues(ctrl);
 
+	/*
+	 * 在以下使用nvme_ctrl->scan_work:
+	 *   - drivers/nvme/host/apple.c|1234| <<apple_nvme_async_probe>> flush_work(&anv->ctrl.scan_work);
+	 *   - drivers/nvme/host/core.c|135| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|1145| <<nvme_passthru_end>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|3920| <<nvme_scan_work>> container_of(work, struct nvme_ctrl, scan_work);
+	 *   - drivers/nvme/host/core.c|3986| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|4488| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+	 *   - drivers/nvme/host/pci.c|3125| <<nvme_probe>> flush_work(&dev->ctrl.scan_work);
+	 */
 	/* prevent racing with ns scanning */
 	flush_work(&ctrl->scan_work);
 
@@ -4206,6 +4353,14 @@ void nvme_complete_async_event(struct nvme_ctrl *ctrl, __le16 status,
 }
 EXPORT_SYMBOL_GPL(nvme_complete_async_event);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3560| <<nvme_fc_init_ctrl>> ret = nvme_alloc_admin_tag_set(&ctrl->ctrl, &ctrl->admin_tag_set,
+ *   - drivers/nvme/host/pci.c|3010| <<nvme_probe>> result = nvme_alloc_admin_tag_set(&dev->ctrl, &dev->admin_tagset,
+ *   - drivers/nvme/host/rdma.c|812| <<nvme_rdma_configure_admin_queue>> error = nvme_alloc_admin_tag_set(&ctrl->ctrl,
+ *   - drivers/nvme/host/tcp.c|1929| <<nvme_tcp_configure_admin_queue>> error = nvme_alloc_admin_tag_set(ctrl,
+ *   - drivers/nvme/target/loop.c|355| <<nvme_loop_configure_admin_queue>> error = nvme_alloc_admin_tag_set(&ctrl->ctrl, &ctrl->admin_tag_set,
+ */
 int nvme_alloc_admin_tag_set(struct nvme_ctrl *ctrl, struct blk_mq_tag_set *set,
 		const struct blk_mq_ops *ops, unsigned int cmd_size)
 {
@@ -4341,6 +4496,17 @@ void nvme_stop_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_stop_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/apple.c|1156| <<apple_nvme_reset_work>> nvme_start_ctrl(&anv->ctrl);
+ *   - drivers/nvme/host/fc.c|3196| <<nvme_fc_create_association>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|2799| <<nvme_reset_work>> nvme_start_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3123| <<nvme_probe>> nvme_start_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1069| <<nvme_rdma_setup_ctrl>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|2066| <<nvme_tcp_setup_ctrl>> nvme_start_ctrl(ctrl);
+ *   - drivers/nvme/target/loop.c|462| <<nvme_loop_reset_ctrl_work>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|598| <<nvme_loop_create_ctrl>> nvme_start_ctrl(&ctrl->ctrl);
+ */
 void nvme_start_ctrl(struct nvme_ctrl *ctrl)
 {
 	nvme_start_keep_alive(ctrl);
@@ -4425,6 +4591,15 @@ static void nvme_free_ctrl(struct device *dev)
  * earliest initialization so that we have the initialized structured around
  * during probing.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/apple.c|1511| <<apple_nvme_probe>> ret = nvme_init_ctrl(&anv->ctrl, anv->dev, &nvme_ctrl_ops, NVME_QUIRK_SKIP_CID_GEN | NVME_QUIRK_IDENTIFY_CNS);
+ *   - drivers/nvme/host/fc.c|3554| <<nvme_fc_init_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_fc_ctrl_ops, 0);
+ *   - drivers/nvme/host/pci.c|2945| <<nvme_pci_alloc_dev>> ret = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops, quirks);
+ *   - drivers/nvme/host/rdma.c|2276| <<nvme_rdma_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops, 0);
+ *   - drivers/nvme/host/tcp.c|2580| <<nvme_tcp_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_tcp_ctrl_ops, 0);
+ *   - drivers/nvme/target/loop.c|547| <<nvme_loop_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_loop_ctrl_ops, 0);
+ */
 int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
 		const struct nvme_ctrl_ops *ops, unsigned long quirks)
 {
@@ -4441,6 +4616,16 @@ int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
 	ctrl->ops = ops;
 	ctrl->quirks = quirks;
 	ctrl->numa_node = NUMA_NO_NODE;
+	/*
+	 * 在以下使用nvme_ctrl->scan_work:
+	 *   - drivers/nvme/host/apple.c|1234| <<apple_nvme_async_probe>> flush_work(&anv->ctrl.scan_work);
+	 *   - drivers/nvme/host/core.c|135| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|1145| <<nvme_passthru_end>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|3920| <<nvme_scan_work>> container_of(work, struct nvme_ctrl, scan_work);
+	 *   - drivers/nvme/host/core.c|3986| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|4488| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+	 *   - drivers/nvme/host/pci.c|3125| <<nvme_probe>> flush_work(&dev->ctrl.scan_work);
+	 */
 	INIT_WORK(&ctrl->scan_work, nvme_scan_work);
 	INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
 	INIT_WORK(&ctrl->fw_act_work, nvme_fw_act_work);
@@ -4484,6 +4669,9 @@ int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
 	nvme_get_ctrl(ctrl);
 	cdev_init(&ctrl->cdev, &nvme_dev_fops);
 	ctrl->cdev.owner = ops->module;
+	/*
+	 * 这里加入cdev
+	 */
 	ret = cdev_device_add(&ctrl->cdev, ctrl->device);
 	if (ret)
 		goto out_free_name;
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index f35647c47..97629b74c 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -327,6 +327,16 @@ struct nvme_ctrl {
 	struct nvme_id_power_state psd[32];
 	struct nvme_effects_log *effects;
 	struct xarray cels;
+	/*
+	 * 在以下使用nvme_ctrl->scan_work:
+	 *   - drivers/nvme/host/apple.c|1234| <<apple_nvme_async_probe>> flush_work(&anv->ctrl.scan_work);
+	 *   - drivers/nvme/host/core.c|135| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|1145| <<nvme_passthru_end>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|3920| <<nvme_scan_work>> container_of(work, struct nvme_ctrl, scan_work);
+	 *   - drivers/nvme/host/core.c|3986| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|4488| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+	 *   - drivers/nvme/host/pci.c|3125| <<nvme_probe>> flush_work(&dev->ctrl.scan_work);
+	 */
 	struct work_struct scan_work;
 	struct work_struct async_event_work;
 	struct delayed_work ka_work;
@@ -626,6 +636,10 @@ static inline void nvme_should_fail(struct request *req) {}
 bool nvme_wait_reset(struct nvme_ctrl *ctrl);
 int nvme_try_sched_reset(struct nvme_ctrl *ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/ioctl.c|972| <<nvme_dev_ioctl>> return nvme_reset_subsystem(ctrl);
+ */
 static inline int nvme_reset_subsystem(struct nvme_ctrl *ctrl)
 {
 	int ret;
@@ -710,6 +724,10 @@ static inline bool nvme_try_complete_req(struct request *req, __le16 status,
 
 static inline void nvme_get_ctrl(struct nvme_ctrl *ctrl)
 {
+	/*
+	 * struct nvme_ctrl *ctrl:
+	 * -> struct device *dev;
+	 */
 	get_device(ctrl->device);
 }
 
@@ -792,6 +810,16 @@ blk_status_t nvme_fail_nonready_command(struct nvme_ctrl *ctrl,
 bool __nvme_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
 		bool queue_live);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/apple.c|753| <<apple_nvme_queue_rq>> if (!nvme_check_ready(&anv->ctrl, req, true))
+ *   - drivers/nvme/host/fc.c|2813| <<nvme_fc_queue_rq>> !nvme_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+ *   - drivers/nvme/host/pci.c|890| <<nvme_queue_rq>> if (unlikely(!nvme_check_ready(&dev->ctrl, req, true)))
+ *   - drivers/nvme/host/pci.c|924| <<nvme_prep_rq_batch>> if (unlikely(!nvme_check_ready(&nvmeq->dev->ctrl, req, true)))
+ *   - drivers/nvme/host/rdma.c|1989| <<nvme_rdma_queue_rq>> if (!nvme_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+ *   - drivers/nvme/host/tcp.c|2395| <<nvme_tcp_queue_rq>> if (!nvme_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+ *   - drivers/nvme/target/loop.c|141| <<nvme_loop_queue_rq>> if (!nvme_check_ready(&queue->ctrl->ctrl, req, queue_ready))
+ */
 static inline bool nvme_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
 		bool queue_live)
 {
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index 3f0c9ee09..98911ff0c 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -128,6 +128,12 @@ struct nvme_dev {
 	unsigned io_queues[HCTX_MAX_TYPES];
 	unsigned int num_vecs;
 	u32 q_depth;
+	/*
+	 * 在以下使用nvme_dev->io_sqes:
+	 *   - drivers/nvme/host/pci.c|1623| <<nvme_alloc_queue>> nvmeq->sqes = qid ? dev->io_sqes : NVME_ADM_SQES;
+	 *   - drivers/nvme/host/pci.c|2617| <<nvme_pci_enable>> dev->io_sqes = 7;
+	 *   - drivers/nvme/host/pci.c|2619| <<nvme_pci_enable>> dev->io_sqes = NVME_NVM_IOSQES;
+	 */
 	int io_sqes;
 	u32 db_stride;
 	void __iomem *bar;
@@ -189,20 +195,93 @@ static inline struct nvme_dev *to_nvme_dev(struct nvme_ctrl *ctrl)
 struct nvme_queue {
 	struct nvme_dev *dev;
 	spinlock_t sq_lock;
+	/*
+	 * 在以下使用nvme_queue->sq_cmds:
+	 *   - drivers/nvme/host/pci.c|530| <<nvme_sq_copy_cmd>> memcpy(nvmeq->sq_cmds + (nvmeq->sq_tail << nvmeq->sqes),
+	 *   - drivers/nvme/host/pci.c|1488| <<nvme_free_queue>> if (!nvmeq->sq_cmds)
+	 *   - drivers/nvme/host/pci.c|1493| <<nvme_free_queue>> pci_free_p2pmem(to_pci_dev(nvmeq->dev->dev), nvmeq->sq_cmds, SQ_SIZE(nvmeq));
+	 *   - drivers/nvme/host/pci.c|1496| <<nvme_free_queue>> dma_free_coherent(nvmeq->dev->dev, SQ_SIZE(nvmeq), nvmeq->sq_cmds, nvmeq->sq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1583| <<nvme_alloc_sq_cmds>> nvmeq->sq_cmds = pci_alloc_p2pmem(pdev, SQ_SIZE(nvmeq));
+	 *   - drivers/nvme/host/pci.c|1584| <<nvme_alloc_sq_cmds>> if (nvmeq->sq_cmds) {
+	 *   - drivers/nvme/host/pci.c|1586| <<nvme_alloc_sq_cmds>> dma_free_coherent(nvmeq->dev->dev, CQ_SIZE(nvmeq), nvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev, nvmeq->sq_cmds);
+	 *   - drivers/nvme/host/pci.c|1592| <<nvme_alloc_sq_cmds>> pci_free_p2pmem(pdev, nvmeq->sq_cmds, SQ_SIZE(nvmeq));
+	 *   - drivers/nvme/host/pci.c|1596| <<nvme_alloc_sq_cmds>> nvmeq->sq_cmds = dma_alloc_coherent(dev->dev, SQ_SIZE(nvmeq), &nvmeq->sq_dma_addr, GFP_KERNEL);
+	 *   - drivers/nvme/host/pci.c|1598| <<nvme_alloc_sq_cmds>> if (!nvmeq->sq_cmds)
+	 */
 	void *sq_cmds;
 	 /* only used for poll queues: */
 	spinlock_t cq_poll_lock ____cacheline_aligned_in_smp;
+	/*
+	 * 在以下使用nvme_queue->cqes:
+	 *   - drivers/nvme/host/pci.c|1061| <<nvme_cqe_pending>> struct nvme_completion *hcqe = &nvmeq->cqes[nvmeq->cq_head];
+	 *   - drivers/nvme/host/pci.c|1089| <<nvme_handle_cqe>> struct nvme_completion *cqe = &nvmeq->cqes[idx];
+	 *   - drivers/nvme/host/pci.c|1500| <<nvme_free_queue>> (void *)nvmeq->cqes, nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1625| <<nvme_alloc_queue>> nvmeq->cqes = dma_alloc_coherent(dev->dev, CQ_SIZE(nvmeq),
+	 *   - drivers/nvme/host/pci.c|1627| <<nvme_alloc_queue>> if (!nvmeq->cqes)
+	 *   - drivers/nvme/host/pci.c|1645| <<nvme_alloc_queue>> dma_free_coherent(dev->dev, CQ_SIZE(nvmeq), (void *)nvmeq->cqes,
+	 *   - drivers/nvme/host/pci.c|1674| <<nvme_init_queue>> memset((void *)nvmeq->cqes, 0, CQ_SIZE(nvmeq));
+	 */
 	struct nvme_completion *cqes;
 	dma_addr_t sq_dma_addr;
 	dma_addr_t cq_dma_addr;
 	u32 __iomem *q_db;
 	u32 q_depth;
 	u16 cq_vector;
+	/*
+	 * 在以下使用nvme_queue->sq_tail:
+	 *   - drivers/nvme/host/pci.c|474| <<nvme_write_sq_db>> u16 next_tail = nvmeq->sq_tail + 1;
+	 *   - drivers/nvme/host/pci.c|482| <<nvme_write_sq_db>> if (nvme_dbbuf_update_and_check_event(nvmeq->sq_tail,
+	 *   - drivers/nvme/host/pci.c|484| <<nvme_write_sq_db>> writel(nvmeq->sq_tail, nvmeq->q_db);
+	 *   - drivers/nvme/host/pci.c|485| <<nvme_write_sq_db>> nvmeq->last_sq_tail = nvmeq->sq_tail;
+	 *   - drivers/nvme/host/pci.c|491| <<nvme_sq_copy_cmd>> memcpy(nvmeq->sq_cmds + (nvmeq->sq_tail << nvmeq->sqes),
+	 *   - drivers/nvme/host/pci.c|493| <<nvme_sq_copy_cmd>> if (++nvmeq->sq_tail == nvmeq->q_depth)
+	 *   - drivers/nvme/host/pci.c|494| <<nvme_sq_copy_cmd>> nvmeq->sq_tail = 0;
+	 *   - drivers/nvme/host/pci.c|502| <<nvme_commit_rqs>> if (nvmeq->sq_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|1039| <<nvme_handle_cqe>> trace_nvme_sq(req, cqe->sq_head, nvmeq->sq_tail);
+	 *   - drivers/nvme/host/pci.c|1572| <<nvme_init_queue>> nvmeq->sq_tail = 0;
+	 */
 	u16 sq_tail;
+	/*
+	 * 在以下使用nvme_queue->last_sq_tail:
+	 *   - drivers/nvme/host/pci.c|478| <<nvme_write_sq_db>> if (next_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|485| <<nvme_write_sq_db>> nvmeq->last_sq_tail = nvmeq->sq_tail;
+	 *   - drivers/nvme/host/pci.c|502| <<nvme_commit_rqs>> if (nvmeq->sq_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|1573| <<nvme_init_queue>> nvmeq->last_sq_tail = 0;
+	 */
 	u16 last_sq_tail;
+	/*
+	 * 在以下设置nvme_queue->cq_head:
+	 *   - drivers/nvme/host/pci.c|1051| <<nvme_update_cq_head>> nvmeq->cq_head = 0;
+	 *   - drivers/nvme/host/pci.c|1054| <<nvme_update_cq_head>> nvmeq->cq_head = tmp;
+	 *   - drivers/nvme/host/pci.c|1539| <<nvme_alloc_queue>> nvmeq->cq_head = 0; 
+	 *   - drivers/nvme/host/pci.c|1574| <<nvme_init_queue>> nvmeq->cq_head = 0;
+	 * 在以下使用nvme_queue->cq_head:
+	 *   - drivers/nvme/host/pci.c|991| <<nvme_cqe_pending>> struct nvme_completion *hcqe = &nvmeq->cqes[nvmeq->cq_head];
+	 *   - drivers/nvme/host/pci.c|998| <<nvme_ring_cq_doorbell>> u16 head = nvmeq->cq_head;
+	 *   - drivers/nvme/host/pci.c|1048| <<nvme_update_cq_head>> u32 tmp = nvmeq->cq_head + 1;
+	 *   - drivers/nvme/host/pci.c|1070| <<nvme_poll_cq>> nvme_handle_cqe(nvmeq, iob, nvmeq->cq_head);
+	 */
 	u16 cq_head;
 	u16 qid;
+	/*
+	 * 在以下使用nvme_queue->cq_phase:
+	 *   - drivers/nvme/host/pci.c|993| <<nvme_cqe_pending>> return (le16_to_cpu(READ_ONCE(hcqe->status)) & 1) == nvmeq->cq_phase;
+	 *   - drivers/nvme/host/pci.c|1052| <<nvme_update_cq_head>> nvmeq->cq_phase ^= 1;
+	 *   - drivers/nvme/host/pci.c|1540| <<nvme_alloc_queue>> nvmeq->cq_phase = 1;
+	 *   - drivers/nvme/host/pci.c|1575| <<nvme_init_queue>> nvmeq->cq_phase = 1;
+	 */
 	u8 cq_phase;
+	/*
+	 * 在以下使用nvme_dev->io_sqes:
+	 *   - drivers/nvme/host/pci.c|1623| <<nvme_alloc_queue>> nvmeq->sqes = qid ? dev->io_sqes : NVME_ADM_SQES;
+	 *   - drivers/nvme/host/pci.c|2617| <<nvme_pci_enable>> dev->io_sqes = 7;
+	 *   - drivers/nvme/host/pci.c|2619| <<nvme_pci_enable>> dev->io_sqes = NVME_NVM_IOSQES;
+	 *
+	 * 在以下使用nvme_queue->sqes:
+	 *   - drivers/nvme/host/pci.c|35| <<SQ_SIZE>> #define SQ_SIZE(q) ((q)->q_depth << (q)->sqes)
+	 *   - drivers/nvme/host/pci.c|543| <<nvme_sq_copy_cmd>> memcpy(nvmeq->sq_cmds + (nvmeq->sq_tail << nvmeq->sqes),
+	 *   - drivers/nvme/host/pci.c|1623| <<nvme_alloc_queue>> nvmeq->sqes = qid ? dev->io_sqes : NVME_ADM_SQES;
+	 */
 	u8 sqes;
 	unsigned long flags;
 #define NVMEQ_ENABLED		0
@@ -468,6 +547,13 @@ static void nvme_pci_map_queues(struct blk_mq_tag_set *set)
 /*
  * Write sq tail if we are asked to, or if the next command would wrap.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|582| <<nvme_commit_rqs>> nvme_write_sq_db(nvmeq, true);
+ *   - drivers/nvme/host/pci.c|983| <<nvme_queue_rq>> nvme_write_sq_db(nvmeq, bd->last);
+ *   - drivers/nvme/host/pci.c|997| <<nvme_submit_cmds>> nvme_write_sq_db(nvmeq, true);
+ *   - drivers/nvme/host/pci.c|1266| <<nvme_pci_submit_async_event>> nvme_write_sq_db(nvmeq, true);
+ */
 static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
 {
 	if (!write_sq) {
@@ -482,9 +568,18 @@ static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
 	if (nvme_dbbuf_update_and_check_event(nvmeq->sq_tail,
 			nvmeq->dbbuf_sq_db, nvmeq->dbbuf_sq_ei))
 		writel(nvmeq->sq_tail, nvmeq->q_db);
+	/*
+	 * 感觉last_sq_tail应该是最后一次写doorbell的
+	 */
 	nvmeq->last_sq_tail = nvmeq->sq_tail;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|982| <<nvme_queue_rq>> nvme_sq_copy_cmd(nvmeq, &iod->cmd);
+ *   - drivers/nvme/host/pci.c|995| <<nvme_submit_cmds>> nvme_sq_copy_cmd(nvmeq, &iod->cmd);
+ *   - drivers/nvme/host/pci.c|1265| <<nvme_pci_submit_async_event>> nvme_sq_copy_cmd(nvmeq, &c);
+ */
 static inline void nvme_sq_copy_cmd(struct nvme_queue *nvmeq,
 				    struct nvme_command *cmd)
 {
@@ -875,6 +970,12 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 			 const struct blk_mq_queue_data *bd)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
+	/*
+	 * struct nvme_dev:
+	 * -> struct nvme_queue *queues;
+	 * -> struct blk_mq_tag_set tagset;
+	 * -> struct blk_mq_tag_set admin_tagset;
+	 */
 	struct nvme_dev *dev = nvmeq->dev;
 	struct request *req = bd->rq;
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -988,6 +1089,18 @@ static void nvme_pci_complete_batch(struct io_comp_batch *iob)
 /* We read the CQE phase first to check if the rest of the entry is valid */
 static inline bool nvme_cqe_pending(struct nvme_queue *nvmeq)
 {
+	/*
+	 * struct nvme_queue *nvmeq:
+	 * -> void *sq_cmds;
+	 * -> struct nvme_completion *cqes;
+	 * -> dma_addr_t sq_dma_addr;
+	 * -> dma_addr_t cq_dma_addr;
+	 * -> u16 sq_tail;
+	 * -> u16 last_sq_tail;
+	 * -> u16 cq_head;
+	 * -> u8 cq_phase; 
+	 * -> u8 sqes;
+	 */
 	struct nvme_completion *hcqe = &nvmeq->cqes[nvmeq->cq_head];
 
 	return (le16_to_cpu(READ_ONCE(hcqe->status)) & 1) == nvmeq->cq_phase;
@@ -1009,6 +1122,10 @@ static inline struct blk_mq_tags *nvme_queue_tagset(struct nvme_queue *nvmeq)
 	return nvmeq->dev->tagset.tags[nvmeq->qid - 1];
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1070| <<nvme_poll_cq>> nvme_handle_cqe(nvmeq, iob, nvmeq->cq_head);
+ */
 static inline void nvme_handle_cqe(struct nvme_queue *nvmeq,
 				   struct io_comp_batch *iob, u16 idx)
 {
@@ -1043,10 +1160,26 @@ static inline void nvme_handle_cqe(struct nvme_queue *nvmeq,
 		nvme_pci_complete_rq(req);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1071| <<nvme_poll_cq>> nvme_update_cq_head(nvmeq);
+ */
 static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
 {
 	u32 tmp = nvmeq->cq_head + 1;
 
+	/*
+	 * 在以下设置nvme_queue->cq_head:
+	 *   - drivers/nvme/host/pci.c|1051| <<nvme_update_cq_head>> nvmeq->cq_head = 0;
+	 *   - drivers/nvme/host/pci.c|1054| <<nvme_update_cq_head>> nvmeq->cq_head = tmp;
+	 *   - drivers/nvme/host/pci.c|1539| <<nvme_alloc_queue>> nvmeq->cq_head = 0;
+	 *   - drivers/nvme/host/pci.c|1574| <<nvme_init_queue>> nvmeq->cq_head = 0;
+	 * 在以下使用nvme_queue->cq_head:
+	 *   - drivers/nvme/host/pci.c|991| <<nvme_cqe_pending>> struct nvme_completion *hcqe = &nvmeq->cqes[nvmeq->cq_head];
+	 *   - drivers/nvme/host/pci.c|998| <<nvme_ring_cq_doorbell>> u16 head = nvmeq->cq_head;
+	 *   - drivers/nvme/host/pci.c|1048| <<nvme_update_cq_head>> u32 tmp = nvmeq->cq_head + 1;
+	 *   - drivers/nvme/host/pci.c|1070| <<nvme_poll_cq>> nvme_handle_cqe(nvmeq, iob, nvmeq->cq_head);
+	 */
 	if (tmp == nvmeq->q_depth) {
 		nvmeq->cq_head = 0;
 		nvmeq->cq_phase ^= 1;
@@ -1055,6 +1188,13 @@ static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1084| <<nvme_irq>> if (nvme_poll_cq(nvmeq, &iob)) {
+ *   - drivers/nvme/host/pci.c|1112| <<nvme_poll_irqdisable>> nvme_poll_cq(nvmeq, NULL);
+ *   - drivers/nvme/host/pci.c|1125| <<nvme_poll>> found = nvme_poll_cq(nvmeq, iob);
+ *   - drivers/nvme/host/pci.c|1463| <<nvme_reap_pending_cqes>> nvme_poll_cq(&dev->queues[i], NULL);
+ */
 static inline int nvme_poll_cq(struct nvme_queue *nvmeq,
 			       struct io_comp_batch *iob)
 {
@@ -1516,6 +1656,11 @@ static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1894| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/pci.c|1930| <<nvme_create_io_queues>> if (nvme_alloc_queue(dev, i, dev->q_depth)) {
+ */
 static int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)
 {
 	struct nvme_queue *nvmeq = &dev->queues[qid];
@@ -1692,6 +1837,12 @@ static unsigned long db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)
 	return NVME_REG_DBS + ((nr_io_queues + 1) * 8 * dev->db_stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1722| <<nvme_pci_configure_admin_queue>> result = nvme_remap_bar(dev, db_bar_size(dev, 0));
+ *   - drivers/nvme/host/pci.c|2306| <<nvme_setup_io_queues>> result = nvme_remap_bar(dev, size);
+ *   - drivers/nvme/host/pci.c|2859| <<nvme_dev_map>> if (nvme_remap_bar(dev, NVME_REG_DBS + 4096))
+ */
 static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -1702,6 +1853,10 @@ static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
 		return -ENOMEM;
 	if (dev->bar)
 		iounmap(dev->bar);
+	/*
+	 * struct nvme_dev *dev:
+	 * -> void __iomem *bar;
+	 */
 	dev->bar = ioremap(pci_resource_start(pdev, 0), size);
 	if (!dev->bar) {
 		dev->bar_mapped_size = 0;
@@ -2465,6 +2620,11 @@ static void nvme_pci_update_nr_queues(struct nvme_dev *dev)
 	nvme_free_queues(dev, dev->online_queues);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2710| <<nvme_reset_work>> result = nvme_pci_enable(dev);
+ *   - drivers/nvme/host/pci.c|3024| <<nvme_probe>> result = nvme_pci_enable(dev);
+ */
 static int nvme_pci_enable(struct nvme_dev *dev)
 {
 	int result = -ENOMEM;
@@ -2629,6 +2789,10 @@ static int nvme_disable_prepare_reset(struct nvme_dev *dev, bool shutdown)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3014| <<nvme_probe>> result = nvme_setup_prp_pools(dev);
+ */
 static int nvme_setup_prp_pools(struct nvme_dev *dev)
 {
 	dev->prp_page_pool = dma_pool_create("prp list page", dev->dev,
@@ -2653,6 +2817,10 @@ static void nvme_release_prp_pools(struct nvme_dev *dev)
 	dma_pool_destroy(dev->prp_small_pool);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3018| <<nvme_probe>> result = nvme_pci_alloc_iod_mempool(dev);
+ */
 static int nvme_pci_alloc_iod_mempool(struct nvme_dev *dev)
 {
 	size_t alloc_size = sizeof(struct scatterlist) * NVME_MAX_SEGS;
@@ -2707,6 +2875,11 @@ static void nvme_reset_work(struct work_struct *work)
 	nvme_sync_queues(&dev->ctrl);
 
 	mutex_lock(&dev->shutdown_lock);
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|2710| <<nvme_reset_work>> result = nvme_pci_enable(dev);
+	 *   - drivers/nvme/host/pci.c|3024| <<nvme_probe>> result = nvme_pci_enable(dev);
+	 */
 	result = nvme_pci_enable(dev);
 	if (result)
 		goto out_unlock;
@@ -2849,10 +3022,17 @@ static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
 	.supports_pci_p2pdma	= nvme_pci_supports_pci_p2pdma,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3010| <<nvme_probe>> result = nvme_dev_map(dev);
+ */
 static int nvme_dev_map(struct nvme_dev *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 
+	/*
+	 * PCI注释: Reserve selected PCI I/O and memory resources
+	 */
 	if (pci_request_mem_regions(pdev, "nvme"))
 		return -ENODEV;
 
@@ -2908,6 +3088,10 @@ static unsigned long check_vendor_combination_bug(struct pci_dev *pdev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2988| <<nvme_probe>> dev = nvme_pci_alloc_dev(pdev, id);
+ */
 static struct nvme_dev *nvme_pci_alloc_dev(struct pci_dev *pdev,
 		const struct pci_device_id *id)
 {
@@ -2925,6 +3109,12 @@ static struct nvme_dev *nvme_pci_alloc_dev(struct pci_dev *pdev,
 	dev->nr_write_queues = write_queues;
 	dev->nr_poll_queues = poll_queues;
 	dev->nr_allocated_queues = nvme_max_io_queues(dev) + 1;
+	/*
+	 * struct nvme_dev *dev;
+	 * -> struct nvme_queue *queues;
+	 * -> struct blk_mq_tag_set tagset;
+	 * -> struct blk_mq_tag_set admin_tagset;
+	 */
 	dev->queues = kcalloc_node(dev->nr_allocated_queues,
 			sizeof(struct nvme_queue), GFP_KERNEL, node);
 	if (!dev->queues)
@@ -2942,6 +3132,10 @@ static struct nvme_dev *nvme_pci_alloc_dev(struct pci_dev *pdev,
 			 "platform quirk: setting simple suspend\n");
 		quirks |= NVME_QUIRK_SIMPLE_SUSPEND;
 	}
+	/*
+	 * struct nvme_dev *dev;
+	 * -> struct nvme_ctrl ctrl;
+	 */
 	ret = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
 			     quirks);
 	if (ret)
@@ -2954,6 +3148,10 @@ static struct nvme_dev *nvme_pci_alloc_dev(struct pci_dev *pdev,
 	dma_set_min_align_mask(&pdev->dev, NVME_CTRL_PAGE_SIZE - 1);
 	dma_set_max_seg_size(&pdev->dev, 0xffffffff);
 
+	/*
+	 * struct nvme_dev *dev;
+	 * -> struct nvme_ctrl ctrl;
+	 */
 	/*
 	 * Limit the max command size to prevent iod->sg allocations going
 	 * over a single page.
@@ -2977,15 +3175,25 @@ static struct nvme_dev *nvme_pci_alloc_dev(struct pci_dev *pdev,
 	return ERR_PTR(ret);
 }
 
+/*
+ * struct pci_driver nvme_driver.probe = nvme_probe()
+ */
 static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 {
 	struct nvme_dev *dev;
 	int result = -ENOMEM;
 
+	/*
+	 * 分配初始化nvme_dev
+	 * 初始化nvme_dev->ctrl (struct nvme_ctrl ctrl;)
+	 */
 	dev = nvme_pci_alloc_dev(pdev, id);
 	if (IS_ERR(dev))
 		return PTR_ERR(dev);
 
+	/*
+	 * 分配pci io的资源, 比如初始化nvme_dev->bar (void __iomem *bar;)
+	 */
 	result = nvme_dev_map(dev);
 	if (result)
 		goto out_uninit_ctrl;
@@ -3000,6 +3208,11 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 
 	dev_info(dev->ctrl.device, "pci function %s\n", dev_name(&pdev->dev));
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|2710| <<nvme_reset_work>> result = nvme_pci_enable(dev);
+	 *   - drivers/nvme/host/pci.c|3024| <<nvme_probe>> result = nvme_pci_enable(dev);
+	 */
 	result = nvme_pci_enable(dev);
 	if (result)
 		goto out_release_iod_mempool;
@@ -3052,8 +3265,29 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 
 	pci_set_drvdata(pdev, dev);
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/apple.c|1156| <<apple_nvme_reset_work>> nvme_start_ctrl(&anv->ctrl);
+	 *   - drivers/nvme/host/fc.c|3196| <<nvme_fc_create_association>> nvme_start_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/pci.c|2799| <<nvme_reset_work>> nvme_start_ctrl(&dev->ctrl);
+	 *   - drivers/nvme/host/pci.c|3123| <<nvme_probe>> nvme_start_ctrl(&dev->ctrl);
+	 *   - drivers/nvme/host/rdma.c|1069| <<nvme_rdma_setup_ctrl>> nvme_start_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/tcp.c|2066| <<nvme_tcp_setup_ctrl>> nvme_start_ctrl(ctrl);
+	 *   - drivers/nvme/target/loop.c|462| <<nvme_loop_reset_ctrl_work>> nvme_start_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/target/loop.c|598| <<nvme_loop_create_ctrl>> nvme_start_ctrl(&ctrl->ctrl);
+	 */
 	nvme_start_ctrl(&dev->ctrl);
 	nvme_put_ctrl(&dev->ctrl);
+	/*
+	 * 在以下使用nvme_ctrl->scan_work:
+	 *   - drivers/nvme/host/apple.c|1234| <<apple_nvme_async_probe>> flush_work(&anv->ctrl.scan_work);
+	 *   - drivers/nvme/host/core.c|135| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|1145| <<nvme_passthru_end>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|3920| <<nvme_scan_work>> container_of(work, struct nvme_ctrl, scan_work);
+	 *   - drivers/nvme/host/core.c|3986| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|4488| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+	 *   - drivers/nvme/host/pci.c|3125| <<nvme_probe>> flush_work(&dev->ctrl.scan_work);
+	 */
 	flush_work(&dev->ctrl.scan_work);
 	return 0;
 
diff --git a/drivers/pci/hotplug/acpiphp_glue.c b/drivers/pci/hotplug/acpiphp_glue.c
index 601129772..bf1788e09 100644
--- a/drivers/pci/hotplug/acpiphp_glue.c
+++ b/drivers/pci/hotplug/acpiphp_glue.c
@@ -41,6 +41,52 @@
 #include "../pci.h"
 #include "acpiphp.h"
 
+/*
+ * [0] ACPI: OSL: debug: acpi_os_execute() type=2
+ * [0] CPU: 1 PID: 0 Comm: swapper/1 Not tainted 6.6.0 #3
+ * [0] acpi_os_execute
+ * [0] acpi_ev_gpe_dispatch
+ * [0] acpi_ev_detect_gpe
+ * [0] acpi_ev_gpe_detect
+ * [0] acpi_ev_sci_xrupt_handler
+ * [0] acpi_irq
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_fasteoi_irq
+ * [0] __common_interrupt
+ * [0] common_interrupt
+ *
+ * [0] ACPI: OSL: debug: acpi_os_execute() type=1
+ * [0] CPU: 0 PID: 11 Comm: kworker/0:1 Not tainted 6.6.0 #3
+ * [0] Workqueue: kacpid acpi_os_execute_deferred
+ * [0] acpi_os_execute
+ * [0] acpi_ev_queue_notify_request
+ * [0] acpi_ds_exec_end_op
+ * [0] acpi_ps_parse_loop
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] ACPI: OSL: debug: acpi_os_execute() type=1
+ * [0] CPU: 0 PID: 11 Comm: kworker/0:1 Not tainted 6.6.0 #3
+ * [0] Workqueue: kacpid acpi_os_execute_deferred
+ * [0] acpi_os_execute
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ */
+
 static LIST_HEAD(bridge_list);
 static DEFINE_MUTEX(bridge_mutex);
 
@@ -479,9 +525,49 @@ static void acpiphp_native_scan_bridge(struct pci_dev *bridge)
  * This function should be called per *physical slot*,
  * not per each slot object in ACPI namespace.
  */
+/*
+ * CPU: 1 PID: 11 Comm: kworker/u16:0 Not tainted 6.6.0-dirty #13
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] pci_assign_unassigned_bridge_resources
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * struct acpiphp_bridge *bridge:
+ * -> struct list_head slots;
+ */
 static void enable_slot(struct acpiphp_slot *slot, bool bridge)
 {
 	struct pci_dev *dev;
+	/*
+	 * struct pci_dev  *self;          // Bridge device as seen by parent
+	 * -> struct pci_bus  *bus;           // Bus this device is on
+	 * -> struct pci_bus  *subordinate;   // Bus this device bridges to
+	 * -> unsigned int    devfn;          // Encoded device & function index
+	 * -> unsigned short  vendor;
+	 * -> unsigned short  device;
+	 *
+	 * struct acpiphp_slot *slot:
+	 * -> struct pci_bus *bus;
+	 *    -> struct pci_bus  *parent;        // Parent bus this bridge is on
+	 *    -> struct list_head children;      // List of child buses
+	 *    -> struct list_head devices;       // List of devices on this bus 
+	 *    -> struct pci_dev  *self;          // Bridge device as seen by parent
+	 *    -> struct list_head slots;         // List of slots on this bus; protected by pci_slot_mutex
+	 *    -> struct resource *resource[PCI_BRIDGE_RESOURCE_NUM];
+	 *    -> struct list_head resources;     // Address space routed to this bus
+	 *    -> struct resource busn_res;       // Bus numbers routed to this bus
+	 *    -> unsigned char   number;         // Bus number
+	 *    -> unsigned char   primary;        // Number of primary bridge
+	 *    -> char            name[48];
+	 */
 	struct pci_bus *bus = slot->bus;
 	struct acpiphp_func *func;
 
@@ -517,6 +603,11 @@ static void enable_slot(struct acpiphp_slot *slot, bool bridge)
 				}
 			}
 		}
+		/*
+		 * 注释
+		 * Returns true if the PCI bus is root (behind host-PCI bridge),
+		 * false otherwise
+		 */
 		if (pci_is_root_bus(bus))
 			__pci_bus_assign_resources(bus, &add_list, NULL);
 		else
@@ -697,6 +788,13 @@ static void trim_stale_devices(struct pci_dev *dev)
  * Iterate over all slots under this bridge and make sure that if a
  * card is present they are enabled, and if not they are disabled.
  */
+/*
+ * called by:
+ *   - drivers/pci/hotplug/acpiphp_glue.c|777| <<acpiphp_check_host_bridge>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|807| <<hotplug_event>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|817| <<hotplug_event>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|824| <<hotplug_event>> acpiphp_check_bridge(func->parent);
+ */
 static void acpiphp_check_bridge(struct acpiphp_bridge *bridge)
 {
 	struct acpiphp_slot *slot;
@@ -708,6 +806,12 @@ static void acpiphp_check_bridge(struct acpiphp_bridge *bridge)
 	if (bridge->pci_dev)
 		pm_runtime_get_sync(&bridge->pci_dev->dev);
 
+	/*
+	 * struct acpiphp_bridge *bridge:
+	 * -> struct list_head slots;
+	 *
+	 * struct acpiphp_slot *slot;
+	 */
 	list_for_each_entry(slot, &bridge->slots, node) {
 		struct pci_bus *bus = slot->bus;
 		struct pci_dev *dev, *tmp;
@@ -721,9 +825,15 @@ static void acpiphp_check_bridge(struct acpiphp_bridge *bridge)
 				if (PCI_SLOT(dev->devfn) == slot->device)
 					trim_stale_devices(dev);
 
+			/*
+			 * struct acpiphp_slot *slot;
+			 */
 			/* configure all functions */
 			enable_slot(slot, true);
 		} else {
+			/*
+			 * struct acpiphp_slot *slot;
+			 */
 			disable_slot(slot);
 		}
 	}
@@ -837,6 +947,10 @@ static void hotplug_event(u32 type, struct acpiphp_context *context)
 		put_bridge(bridge);
 }
 
+/*
+ * 在以下使用acpiphp_hotplug_notify():
+ *   - drivers/pci/hotplug/acpiphp_glue.c|68| <<acpiphp_init_context>> context->hp.notify = acpiphp_hotplug_notify;
+ */
 static int acpiphp_hotplug_notify(struct acpi_device *adev, u32 type)
 {
 	struct acpiphp_context *context;
diff --git a/drivers/pci/pci.c b/drivers/pci/pci.c
index 59c01d68c..a8968dbbf 100644
--- a/drivers/pci/pci.c
+++ b/drivers/pci/pci.c
@@ -1935,6 +1935,11 @@ int __weak pcibios_enable_device(struct pci_dev *dev, int bars)
 	return pci_enable_resources(dev, bars);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/pci.c|1982| <<pci_reenable_device>> return do_pci_enable_device(dev, (1 << PCI_NUM_RESOURCES) - 1);
+ *   - drivers/pci/pci.c|2038| <<pci_enable_device_flags>> err = do_pci_enable_device(dev, bars);
+ */
 static int do_pci_enable_device(struct pci_dev *dev, int bars)
 {
 	int err;
@@ -1976,6 +1981,15 @@ static int do_pci_enable_device(struct pci_dev *dev, int bars)
  * NOTE: This function is a backend of pci_default_resume() and is not supposed
  * to be called by normal code, write proper resume handler and use it instead.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/eeh.c|1432| <<eeh_pe_reenable_devices>> ret = pci_reenable_device(pdev);
+ *   - drivers/ata/ata_piix.c|1042| <<piix_pci_device_resume>> rc = pci_reenable_device(pdev);
+ *   - drivers/net/ethernet/intel/fm10k/fm10k_pci.c|2419| <<fm10k_io_slot_reset>> if (pci_reenable_device(pdev)) {
+ *   - drivers/net/wireless/intersil/p54/p54pci.c|687| <<p54p_resume>> err = pci_reenable_device(pdev);
+ *   - drivers/pci/pci-driver.c|624| <<pci_pm_reenable_device>> retval = pci_reenable_device(pci_dev);
+ *   - drivers/pci/setup-bus.c|2370| <<pci_assign_unassigned_bridge_resources>> retval = pci_reenable_device(bridge);
+ */
 int pci_reenable_device(struct pci_dev *dev)
 {
 	if (pci_is_enabled(dev))
@@ -6753,6 +6767,10 @@ static void pci_request_resource_alignment(struct pci_dev *dev, int bar,
  * Later on, the kernel will assign page-aligned memory resource back
  * to the device.
  */
+/*
+ * called by:
+ *   - drivers/pci/probe.c|2575| <<pci_device_add>> pci_reassigndev_resource_alignment(dev);
+ */
 void pci_reassigndev_resource_alignment(struct pci_dev *dev)
 {
 	int i;
diff --git a/drivers/pci/probe.c b/drivers/pci/probe.c
index 795534589..404406f1e 100644
--- a/drivers/pci/probe.c
+++ b/drivers/pci/probe.c
@@ -510,6 +510,20 @@ static void pci_read_bridge_mmio_pref(struct pci_bus *child)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/pci.c|260| <<pcibios_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - arch/ia64/pci/pci.c|369| <<pcibios_fixup_bus>> pci_read_bridge_bases(b);
+ *   - arch/mips/pci/pci-generic.c|47| <<pcibios_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - arch/mips/pci/pci-legacy.c|295| <<pcibios_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - arch/powerpc/kernel/pci-common.c|1107| <<pcibios_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - arch/sparc/kernel/pci.c|479| <<of_scan_pci_bridge>> pci_read_bridge_bases(bus);
+ *   - arch/x86/pci/common.c|168| <<pcibios_fixup_bus>> pci_read_bridge_bases(b);
+ *   - arch/xtensa/kernel/pci.c|65| <<pcibios_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - drivers/parisc/dino.c|613| <<dino_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - drivers/parisc/lba_pci.c|730| <<lba_fixup_bus>> pci_read_bridge_bases(bus);
+ *   - drivers/pci/setup-bus.c|1478| <<pci_bus_allocate_resources>> pci_read_bridge_bases(b);
+ */
 void pci_read_bridge_bases(struct pci_bus *child)
 {
 	struct pci_dev *dev = child->self;
diff --git a/drivers/pci/setup-bus.c b/drivers/pci/setup-bus.c
index dae490f25..1068bbd7d 100644
--- a/drivers/pci/setup-bus.c
+++ b/drivers/pci/setup-bus.c
@@ -568,6 +568,20 @@ EXPORT_SYMBOL(pci_setup_cardbus);
  * have some undesirable address (e.g. 0) after the first write.  Ditto
  * 64-bit prefetchable MMIO.
  */
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|739| <<__pci_setup_bridge>> pci_setup_bridge_io(bridge);
+ *   - drivers/pci/setup-bus.c|826| <<pci_claim_bridge_resource>> pci_setup_bridge_io(bridge);
+ *
+ * struct pci_dev:
+ * -> struct list_head bus_list;      // Node in per-bus list
+ * -> struct pci_bus  *bus;           // Bus this device is on
+ * -> struct pci_bus  *subordinate;   // Bus this device bridges to
+ * -> struct pci_slot *slot;          // Physical slot this device is in
+ * -> unsigned int    devfn;          // Encoded device & function index
+ * -> unsigned short  vendor;
+ * -> unsigned short  device;
+ */
 static void pci_setup_bridge_io(struct pci_dev *bridge)
 {
 	struct resource *res;
@@ -624,6 +638,40 @@ static void pci_setup_bridge_mmio(struct pci_dev *bridge)
 	pci_write_config_dword(bridge, PCI_MEMORY_BASE, l);
 }
 
+/*
+ * [0] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 6.6.0 #5
+ * [0] pci_setup_bridge_mmio_pref
+ * [0] pci_setup_bridge
+ * [0] __pci_bus_assign_resources
+ * [0] pci_assign_unassigned_root_bus_resources
+ * [0] pci_assign_unassigned_resources
+ * [0] pcibios_assign_resources
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] CPU: 1 PID: 11 Comm: kworker/u16:0 Not tainted 6.6.0 #5
+ * [0] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] pci_setup_bridge_mmio_pref
+ * [0] pci_setup_bridge
+ * [0] pci_assign_unassigned_bridge_resources
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - drivers/pci/setup-bus.c|696| <<__pci_setup_bridge>> pci_setup_bridge_mmio_pref(bridge);
+ *   - drivers/pci/setup-bus.c|744| <<pci_claim_bridge_resource>> pci_setup_bridge_mmio_pref(bridge);
+ */
 static void pci_setup_bridge_mmio_pref(struct pci_dev *bridge)
 {
 	struct resource *res;
@@ -654,24 +702,108 @@ static void pci_setup_bridge_mmio_pref(struct pci_dev *bridge)
 	}
 	pci_write_config_dword(bridge, PCI_PREF_MEMORY_BASE, l);
 
+	/*
+	 * 在以下使用PCI_PREF_BASE_UPPER32:
+	 *   - drivers/pci/pci-bridge-emul.c|125| <<global>> [PCI_PREF_BASE_UPPER32 / 4] = {
+	 *   - arch/powerpc/platforms/powermac/pci.c|1231| <<fixup_u4_pcie>> pci_write_config_dword(dev, PCI_PREF_BASE_UPPER32, 0);
+	 *   - drivers/pci/controller/vmd.c|560| <<vmd_domain_reset>> writel(0xffffffff, base + PCI_PREF_BASE_UPPER32);
+	 *   - drivers/pci/hotplug/ibmphp_pci.c|977| <<configure_bridge>> pci_bus_write_config_dword(ibmphp_pci_bus, devfn, PCI_PREF_BASE_UPPER32, 0x00000000);
+	 *   - drivers/pci/hotplug/ibmphp_res.c|2060| <<update_bridge_ranges>> pci_bus_read_config_dword(ibmphp_pci_bus, devfn, PCI_PREF_BASE_UPPER32, &upper_start);
+	 *   - drivers/pci/probe.c|388| <<pci_read_bridge_windows>> pci_read_config_dword(bridge, PCI_PREF_BASE_UPPER32, &pmem);
+	 *   - drivers/pci/probe.c|389| <<pci_read_bridge_windows>> pci_write_config_dword(bridge, PCI_PREF_BASE_UPPER32,
+	 *   - drivers/pci/probe.c|391| <<pci_read_bridge_windows>> pci_read_config_dword(bridge, PCI_PREF_BASE_UPPER32, &tmp);
+	 *   - drivers/pci/probe.c|392| <<pci_read_bridge_windows>> pci_write_config_dword(bridge, PCI_PREF_BASE_UPPER32, pmem);
+	 *   - drivers/pci/probe.c|478| <<pci_read_bridge_mmio_pref>> pci_read_config_dword(dev, PCI_PREF_BASE_UPPER32, &mem_base_hi);
+	 *   - drivers/pci/setup-bus.c|658| <<pci_setup_bridge_mmio_pref>> pci_write_config_dword(bridge, PCI_PREF_BASE_UPPER32, bu);
+	 *   - drivers/pci/setup-res.c|182| <<pci_disable_bridge_window>> pci_write_config_dword(dev, PCI_PREF_BASE_UPPER32, 0xffffffff);
+	 */
 	/* Set the upper 32 bits of PREF base & limit */
 	pci_write_config_dword(bridge, PCI_PREF_BASE_UPPER32, bu);
 	pci_write_config_dword(bridge, PCI_PREF_LIMIT_UPPER32, lu);
 }
 
+/*
+ * [0] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 6.6.0 #5
+ * [0] pci_setup_bridge_mmio_pref
+ * [0] pci_setup_bridge
+ * [0] __pci_bus_assign_resources
+ * [0] pci_assign_unassigned_root_bus_resources
+ * [0] pci_assign_unassigned_resources
+ * [0] pcibios_assign_resources
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] CPU: 1 PID: 11 Comm: kworker/u16:0 Not tainted 6.6.0 #5
+ * [0] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] pci_setup_bridge_mmio_pref
+ * [0] pci_setup_bridge
+ * [0] pci_assign_unassigned_bridge_resources
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] CPU: 3 PID: 11 Comm: kworker/u16:0 Not tainted 6.6.0-dirty #4
+ * [0] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] pci_setup_bridge
+ * [0] pci_assign_unassigned_bridge_resources
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - drivers/pci/setup-bus.c|691| <<pci_setup_bridge>> __pci_setup_bridge(bus, type);
+ *   - drivers/pci/setup-bus.c|1569| <<pci_bridge_release_resources>> __pci_setup_bridge(bus, type);
+ */
 static void __pci_setup_bridge(struct pci_bus *bus, unsigned long type)
 {
 	struct pci_dev *bridge = bus->self;
 
+	/*
+	 * 4行log, 对应下面4部分
+	 * [   39.470453] pci 0000:00:04.0: PCI bridge to [bus 01]
+	 * [   39.470468] pci 0000:00:04.0:   bridge window [io  0xc000-0xcfff]
+	 * [   39.471153] pci 0000:00:04.0:   bridge window [mem 0xc1000000-0xc11fffff]
+	 * [   44.474537] pci 0000:00:04.0:   bridge window [mem 0x800100000-0x8001fffff 64bit pref]
+	 */
+
+	/*
+	 * [   39.470453] pci 0000:00:04.0: PCI bridge to [bus 01]
+	 */
 	pci_info(bridge, "PCI bridge to %pR\n",
 		 &bus->busn_res);
 
+	/*
+	 * [   39.470468] pci 0000:00:04.0:   bridge window [io  0xc000-0xcfff]
+	 */
 	if (type & IORESOURCE_IO)
 		pci_setup_bridge_io(bridge);
 
+	/*
+	 * [   39.471153] pci 0000:00:04.0:   bridge window [mem 0xc1000000-0xc11fffff]
+	 */
 	if (type & IORESOURCE_MEM)
 		pci_setup_bridge_mmio(bridge);
 
+	/*
+	 * [   44.474537] pci 0000:00:04.0:   bridge window [mem 0x800100000-0x8001fffff 64bit pref]
+	 */
 	if (type & IORESOURCE_PREFETCH)
 		pci_setup_bridge_mmio_pref(bridge);
 
@@ -682,16 +814,67 @@ void __weak pcibios_setup_bridge(struct pci_bus *bus, unsigned long type)
 {
 }
 
+/*
+ * [0] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 6.6.0 #5
+ * [0] pci_setup_bridge_mmio_pref
+ * [0] pci_setup_bridge
+ * [0] __pci_bus_assign_resources
+ * [0] pci_assign_unassigned_root_bus_resources
+ * [0] pci_assign_unassigned_resources
+ * [0] pcibios_assign_resources
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] CPU: 1 PID: 11 Comm: kworker/u16:0 Not tainted 6.6.0 #5
+ * [0] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] pci_setup_bridge_mmio_pref
+ * [0] pci_setup_bridge
+ * [0] pci_assign_unassigned_bridge_resources
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - drivers/pci/setup-bus.c|1393| <<__pci_bus_assign_resources>> pci_setup_bridge(b);
+ *   - drivers/pci/setup-bus.c|1498| <<__pci_bridge_assign_resources>> pci_setup_bridge(b);
+ *   - drivers/pci/setup-bus.c|2285| <<pci_reassign_bridge_resources>> pci_setup_bridge(bridge->subordinate);
+ *   - drivers/pci/setup-bus.c|2315| <<pci_reassign_bridge_resources>> pci_setup_bridge(bridge->subordinate);
+ */
 void pci_setup_bridge(struct pci_bus *bus)
 {
 	unsigned long type = IORESOURCE_IO | IORESOURCE_MEM |
 				  IORESOURCE_PREFETCH;
 
 	pcibios_setup_bridge(bus, type);
+	/*
+	 * called by:
+	 *   - drivers/pci/setup-bus.c|691| <<pci_setup_bridge>> __pci_setup_bridge(bus, type);
+	 *   - drivers/pci/setup-bus.c|1569| <<pci_bridge_release_resources>> __pci_setup_bridge(bus, type);
+	 */
 	__pci_setup_bridge(bus, type);
 }
 
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/pci.c|302| <<pcibios_claim_one_bus>> pci_claim_bridge_resource(dev, i);
+ *   - arch/ia64/pci/pci.c|357| <<pcibios_fixup_bridge_resources>> pci_claim_bridge_resource(dev, idx);
+ *   - arch/powerpc/kernel/pci-common.c|1269| <<pcibios_allocate_bus_resources>> pci_claim_bridge_resource(dev,
+ *   - arch/powerpc/kernel/pci-common.c|1469| <<pcibios_claim_one_bus>> pci_claim_bridge_resource(dev, i);
+ *   - arch/x86/pci/i386.c|220| <<pcibios_allocate_bridge_resources>> if (!r->start || pci_claim_bridge_resource(dev, idx) < 0) {
+ *   - drivers/parisc/lba_pci.c|678| <<pcibios_allocate_bridge_resources>> if (!r->start || pci_claim_bridge_resource(dev, idx) < 0) {
+ *   - drivers/pci/setup-bus.c|1474| <<pci_claim_bridge_resources>> pci_claim_bridge_resource(dev, i);
+ */
 int pci_claim_bridge_resource(struct pci_dev *bridge, int i)
 {
 	if (i < PCI_BRIDGE_RESOURCES || i > PCI_BRIDGE_RESOURCE_END)
@@ -1369,6 +1552,15 @@ static void pdev_assign_fixed_resources(struct pci_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/acpiphp_glue.c|521| <<enable_slot>> __pci_bus_assign_resources(bus, &add_list, NULL);
+ *   - drivers/pci/setup-bus.c|1415| <<__pci_bus_assign_resources>> __pci_bus_assign_resources(b, realloc_head, fail_head);
+ *   - drivers/pci/setup-bus.c|1437| <<pci_bus_assign_resources>> __pci_bus_assign_resources(bus, NULL, NULL);
+ *   - drivers/pci/setup-bus.c|1526| <<__pci_bridge_assign_resources>> __pci_bus_assign_resources(b, add_head, fail_head);
+ *   - drivers/pci/setup-bus.c|2104| <<pci_assign_unassigned_root_bus_resources>> __pci_bus_assign_resources(bus, add_list, &fail_head);
+ *   - drivers/pci/setup-bus.c|2375| <<pci_assign_unassigned_bus_resources>> __pci_bus_assign_resources(bus, &add_list, NULL);
+ */
 void __pci_bus_assign_resources(const struct pci_bus *bus,
 				struct list_head *realloc_head,
 				struct list_head *fail_head)
@@ -1381,6 +1573,9 @@ void __pci_bus_assign_resources(const struct pci_bus *bus,
 	list_for_each_entry(dev, &bus->devices, bus_list) {
 		pdev_assign_fixed_resources(dev);
 
+		/*
+		 * struct pci_bus *b;
+		 */
 		b = dev->subordinate;
 		if (!b)
 			continue;
@@ -1425,6 +1620,10 @@ static void pci_claim_device_resources(struct pci_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|1508| <<pci_bus_allocate_resources>> pci_claim_bridge_resources(b->self);
+ */
 static void pci_claim_bridge_resources(struct pci_dev *dev)
 {
 	int i;
@@ -1453,6 +1652,11 @@ static void pci_bus_allocate_dev_resources(struct pci_bus *b)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|1483| <<pci_bus_allocate_resources>> pci_bus_allocate_resources(child);
+ *   - drivers/pci/setup-bus.c|1488| <<pci_bus_claim_resources>> pci_bus_allocate_resources(b);
+ */
 static void pci_bus_allocate_resources(struct pci_bus *b)
 {
 	struct pci_bus *child;
@@ -1471,6 +1675,14 @@ static void pci_bus_allocate_resources(struct pci_bus *b)
 		pci_bus_allocate_resources(child);
 }
 
+/*
+ * called by:
+ *   - arch/arm/kernel/bios32.c|515| <<pci_common_init_dev>> pci_bus_claim_resources(bus);
+ *   - arch/arm64/kernel/pci.c|209| <<pci_acpi_scan_root>> pci_bus_claim_resources(bus);
+ *   - arch/mips/pci/pci-legacy.c|124| <<pcibios_scanbus>> pci_bus_claim_resources(bus);
+ *   - arch/mips/pci/pci-xtalk-bridge.c|721| <<bridge_probe>> pci_bus_claim_resources(host->bus);
+ *   - drivers/pci/probe.c|3103| <<pci_host_probe>> pci_bus_claim_resources(bus);
+ */
 void pci_bus_claim_resources(struct pci_bus *b)
 {
 	pci_bus_allocate_resources(b);
@@ -1478,12 +1690,21 @@ void pci_bus_claim_resources(struct pci_bus *b)
 }
 EXPORT_SYMBOL(pci_bus_claim_resources);
 
+/*
+ * called by;
+ *   - drivers/pci/setup-bus.c|2230| <<pci_assign_unassigned_bridge_resources>> __pci_bridge_assign_resources(bridge, &add_list, &fail_head);
+ *   - drivers/pci/setup-bus.c|2335| <<pci_reassign_bridge_resources>> __pci_bridge_assign_resources(bridge, &added, &failed);
+ */
 static void __pci_bridge_assign_resources(const struct pci_dev *bridge,
 					  struct list_head *add_head,
 					  struct list_head *fail_head)
 {
 	struct pci_bus *b;
 
+	/*
+	 * 似乎是下面的打印(哪怕是为了bridge上别的slot打印):
+	 * [   16.390598] pci 0000:00:04.0: BAR 15: assigned [mem 0x800100000-0x8001fffff 64bit pref]
+	 */
 	pdev_assign_resources_sorted((struct pci_dev *)bridge,
 					 add_head, fail_head);
 
@@ -1491,10 +1712,33 @@ static void __pci_bridge_assign_resources(const struct pci_dev *bridge,
 	if (!b)
 		return;
 
+	/*
+	 * called by:
+	 *   - drivers/pci/hotplug/acpiphp_glue.c|521| <<enable_slot>> __pci_bus_assign_resources(bus, &add_list, NULL);
+	 *   - drivers/pci/setup-bus.c|1415| <<__pci_bus_assign_resources>> __pci_bus_assign_resources(b, realloc_head, fail_head);
+	 *   - drivers/pci/setup-bus.c|1437| <<pci_bus_assign_resources>> __pci_bus_assign_resources(bus, NULL, NULL);
+	 *   - drivers/pci/setup-bus.c|1526| <<__pci_bridge_assign_resources>> __pci_bus_assign_resources(b, add_head, fail_head);
+	 *   - drivers/pci/setup-bus.c|2104| <<pci_assign_unassigned_root_bus_resources>> __pci_bus_assign_resources(bus, add_list, &fail_head);
+	 *   - drivers/pci/setup-bus.c|2375| <<pci_assign_unassigned_bus_resources>> __pci_bus_assign_resources(bus, &add_list, NULL);
+	 *
+	 * 似乎是下面的打印(哪怕是为了bridge上别的slot打印):
+	 * [   16.391236] pci 0000:01:08.0: BAR 4: assigned [mem 0x800100000-0x800103fff 64bit pref]
+	 * [   16.391334] pci 0000:01:08.0: BAR 1: assigned [mem 0xc1000000-0xc1000fff]
+	 * [   16.391357] pci 0000:01:08.0: BAR 0: assigned [io  0xc000-0xc03f]
+	 */
 	__pci_bus_assign_resources(b, add_head, fail_head);
 
 	switch (bridge->class >> 8) {
 	case PCI_CLASS_BRIDGE_PCI:
+		/*
+		 * called by:
+		 *   - drivers/pci/setup-bus.c|1393| <<__pci_bus_assign_resources>> pci_setup_bridge(b);
+		 *   - drivers/pci/setup-bus.c|1498| <<__pci_bridge_assign_resources>> pci_setup_bridge(b);
+		 *   - drivers/pci/setup-bus.c|2285| <<pci_reassign_bridge_resources>> pci_setup_bridge(bridge->subordinate);
+		 *   - drivers/pci/setup-bus.c|2315| <<pci_reassign_bridge_resources>> pci_setup_bridge(bridge->subordinate);
+		 *
+		 * 就是这里很重要!!!!!
+		 */
 		pci_setup_bridge(b);
 		break;
 
@@ -1513,6 +1757,10 @@ static void __pci_bridge_assign_resources(const struct pci_dev *bridge,
 	(IORESOURCE_IO | IORESOURCE_MEM | IORESOURCE_PREFETCH |\
 	 IORESOURCE_MEM_64)
 
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|1686| <<pci_bus_release_bridge_resources>> pci_bridge_release_resources(bus, type);
+ */
 static void pci_bridge_release_resources(struct pci_bus *bus,
 					 unsigned long type)
 {
@@ -1581,6 +1829,12 @@ enum release_type {
  * Try to release PCI bridge resources from leaf bridge, so we can allocate
  * a larger window later.
  */
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|1675| <<pci_bus_release_bridge_resources>> pci_bus_release_bridge_resources(b, type, whole_subtree);
+ *   - drivers/pci/setup-bus.c|2176| <<pci_assign_unassigned_root_bus_resources>> pci_bus_release_bridge_resources(fail_res->dev->bus, fail_res->flags & PCI_RES_TYPE_MASK, rel_type);
+ *   - drivers/pci/setup-bus.c|2291| <<pci_assign_unassigned_bridge_resources>> pci_bus_release_bridge_resources(fail_res->dev->bus, fail_res->flags & PCI_RES_TYPE_MASK, whole_subtree);
+ */
 static void pci_bus_release_bridge_resources(struct pci_bus *bus,
 					     unsigned long type,
 					     enum release_type rel_type)
@@ -2142,8 +2396,34 @@ void __init pci_assign_unassigned_resources(void)
 	}
 }
 
+/*
+ * CPU: 1 PID: 11 Comm: kworker/u16:0 Not tainted 6.6.0-dirty #13
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] pci_assign_unassigned_bridge_resources
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - arch/powerpc/kernel/pci-common.c|1495| <<pcibios_finish_adding_to_bus>> pci_assign_unassigned_bridge_resources(bus->self);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|523| <<enable_slot>> pci_assign_unassigned_bridge_resources(bus->self);
+ *   - drivers/pci/hotplug/cpci_hotplug_pci.c|276| <<cpci_configure_slot>> pci_assign_unassigned_bridge_resources(parent->self);
+ *   - drivers/pci/hotplug/pciehp_pci.c|64| <<pciehp_configure_device>> pci_assign_unassigned_bridge_resources(bridge);
+ *   - drivers/pci/hotplug/shpchp_pci.c|55| <<shpchp_configure_device>> pci_assign_unassigned_bridge_resources(bridge);
+ *   - drivers/pci/probe.c|3275| <<pci_rescan_bus_bridge_resize>> pci_assign_unassigned_bridge_resources(bridge);
+ */
 void pci_assign_unassigned_bridge_resources(struct pci_dev *bridge)
 {
+	/*
+	 * Bus this device bridges to
+	 */
 	struct pci_bus *parent = bridge->subordinate;
 	/* List of resources that want additional resources */
 	LIST_HEAD(add_list);
@@ -2163,6 +2443,13 @@ void pci_assign_unassigned_bridge_resources(struct pci_dev *bridge)
 	 */
 	pci_bridge_distribute_available_resources(bridge, &add_list);
 
+	/*
+	 * called by;
+	 *   - drivers/pci/setup-bus.c|2230| <<pci_assign_unassigned_bridge_resources>> __pci_bridge_assign_resources(bridge, &add_list, &fail_head);
+	 *   - drivers/pci/setup-bus.c|2335| <<pci_reassign_bridge_resources>> __pci_bridge_assign_resources(bridge, &added, &failed);
+	 *
+	 * 这里!!!
+	 */
 	__pci_bridge_assign_resources(bridge, &add_list, &fail_head);
 	BUG_ON(!list_empty(&add_list));
 	tried_times++;
@@ -2216,6 +2503,10 @@ void pci_assign_unassigned_bridge_resources(struct pci_dev *bridge)
 }
 EXPORT_SYMBOL_GPL(pci_assign_unassigned_bridge_resources);
 
+/*
+ * called by:
+ *   - drivers/pci/setup-res.c|496| <<pci_resize_resource>> ret = pci_reassign_bridge_resources(dev->bus->self, res->flags);
+ */
 int pci_reassign_bridge_resources(struct pci_dev *bridge, unsigned long type)
 {
 	struct pci_dev_resource *dev_res;
@@ -2320,6 +2611,15 @@ int pci_reassign_bridge_resources(struct pci_dev *bridge, unsigned long type)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kernel/pci-common.c|1497| <<pcibios_finish_adding_to_bus>> pci_assign_unassigned_bus_resources(bus);
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_device.c|1160| <<amdgpu_device_resize_fb_bar>> pci_assign_unassigned_bus_resources(adev->pdev->bus);
+ *   - drivers/gpu/drm/i915/gt/intel_region_lmem.c|121| <<i915_resize_lmem_bar>> pci_assign_unassigned_bus_resources(pdev->bus);
+ *   - drivers/pci/controller/vmd.c|953| <<vmd_enable_domain>> pci_assign_unassigned_bus_resources(vmd->bus);
+ *   - drivers/pci/pci-sysfs.c|1454| <<pci_dev_resource_resize_attr>> pci_assign_unassigned_bus_resources(pdev->bus); \
+ *   - drivers/pci/probe.c|3310| <<pci_rescan_bus>> pci_assign_unassigned_bus_resources(bus);
+ */
 void pci_assign_unassigned_bus_resources(struct pci_bus *bus)
 {
 	struct pci_dev *dev;
diff --git a/drivers/pci/setup-res.c b/drivers/pci/setup-res.c
index ceaa69491..26767d9f5 100644
--- a/drivers/pci/setup-res.c
+++ b/drivers/pci/setup-res.c
@@ -22,6 +22,10 @@
 #include <linux/slab.h>
 #include "pci.h"
 
+/*
+ * called by:
+ *   - drivers/pci/setup-res.c|128| <<pci_update_resource>> pci_std_update_resource(dev, resno);
+ */
 static void pci_std_update_resource(struct pci_dev *dev, int resno)
 {
 	struct pci_bus_region region;
@@ -88,6 +92,9 @@ static void pci_std_update_resource(struct pci_dev *dev, int resno)
 	} else
 		return;
 
+	/*
+	 * 第一步: enable
+	 */
 	/*
 	 * We can't update a 64-bit BAR atomically, so when possible,
 	 * disable decoding so that a half-updated BAR won't conflict
@@ -100,6 +107,9 @@ static void pci_std_update_resource(struct pci_dev *dev, int resno)
 				      cmd & ~PCI_COMMAND_MEMORY);
 	}
 
+	/*
+	 * 第二步: enable
+	 */
 	pci_write_config_dword(dev, reg, new);
 	pci_read_config_dword(dev, reg, &check);
 
@@ -118,10 +128,21 @@ static void pci_std_update_resource(struct pci_dev *dev, int resno)
 		}
 	}
 
+	/*
+	 * 第三步: enable
+	 */
 	if (disable)
 		pci_write_config_word(dev, PCI_COMMAND, cmd);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/platforms/powernv/pci-sriov.c|577| <<pnv_pci_vf_resource_shift>> pci_update_resource(dev, i + PCI_IOV_RESOURCES);
+ *   - drivers/pci/iov.c|881| <<sriov_restore_state>> pci_update_resource(dev, i + PCI_IOV_RESOURCES);
+ *   - drivers/pci/pci.c|1019| <<pci_restore_bars>> pci_update_resource(dev, i);
+ *   - drivers/pci/setup-res.c|364| <<pci_assign_resource>> pci_update_resource(dev, resno);
+ *   - drivers/pci/setup-res.c|404| <<pci_reassign_resource>> pci_update_resource(dev, resno);
+ */
 void pci_update_resource(struct pci_dev *dev, int resno)
 {
 	if (resno <= PCI_ROM_RESOURCE)
@@ -132,6 +153,24 @@ void pci_update_resource(struct pci_dev *dev, int resno)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/pci.c|299| <<pcibios_claim_one_bus>> if (pci_claim_resource(dev, i) == 0)
+ *   - arch/ia64/pci/pci.c|339| <<pcibios_fixup_device_resources>> pci_claim_resource(dev, idx); 
+ *   - arch/powerpc/kernel/pci-common.c|1466| <<pcibios_claim_one_bus>> if (pci_claim_resource(dev, i) == 0)
+ *   - arch/s390/pci/pci.c|592| <<pcibios_device_add>> pci_claim_resource(pdev, i);
+ *   - arch/sparc/kernel/pci.c|678| <<pci_claim_bus_resources>> pci_claim_resource(dev, i);
+ *   - arch/x86/pci/i386.c|278| <<pcibios_allocate_dev_resources>> if (pci_claim_resource(dev, idx) < 0) {
+ *   - arch/x86/pci/i386.c|336| <<pcibios_allocate_dev_rom_resource>> if (pci_claim_resource(dev, PCI_ROM_RESOURCE) < 0) {
+ *   - drivers/parisc/lba_pci.c|813| <<lba_fixup_bus>> pci_claim_resource(dev, i);
+ *   - drivers/pci/quirks.c|673| <<quirk_io_region>> if (!pci_claim_resource(dev, nr))
+ *   - drivers/pci/setup-bus.c|883| <<pci_claim_bridge_resource>> if (pci_claim_resource(bridge, i) == 0)
+ *   - drivers/pci/setup-bus.c|906| <<pci_claim_bridge_resource>> if (pci_claim_resource(bridge, i) == 0)
+ *   - drivers/pci/setup-bus.c|1616| <<pci_claim_device_resources>> pci_claim_resource(dev, i);
+ *   - drivers/pci/setup-bus.c|2572| <<pci_reassign_bridge_resources>> pci_claim_resource(bridge, i);
+ *   - drivers/pci/xen-pcifront.c|396| <<pcifront_claim_resource>> if (pci_claim_resource(dev, i)) {
+ *   - drivers/pcmcia/yenta_socket.c|716| <<yenta_allocate_res>> if (pci_claim_resource(dev, nr) == 0)
+ */
 int pci_claim_resource(struct pci_dev *dev, int resource)
 {
 	struct resource *res = &dev->resource[resource];
@@ -159,6 +198,13 @@ int pci_claim_resource(struct pci_dev *dev, int resource)
 		return -EINVAL;
 	}
 
+	/*
+	 * request_resource_conflict - request and reserve an I/O or memory resource
+	 * @root: root resource descriptor
+	 * @new: resource descriptor desired by caller
+	 *
+	 * Returns 0 for success, conflict resource on error.
+	 */
 	conflict = request_resource_conflict(root, res);
 	if (conflict) {
 		pci_info(dev, "can't claim BAR %d %pR: address conflict with %s %pR\n",
@@ -171,6 +217,10 @@ int pci_claim_resource(struct pci_dev *dev, int resource)
 }
 EXPORT_SYMBOL(pci_claim_resource);
 
+/*
+ * called by:
+ *   - drivers/pci/pci.c|6805| <<pci_reassigndev_resource_alignment>> pci_disable_bridge_window(dev);
+ */
 void pci_disable_bridge_window(struct pci_dev *dev)
 {
 	/* MMIO Base/Limit */
@@ -179,6 +229,21 @@ void pci_disable_bridge_window(struct pci_dev *dev)
 	/* Prefetchable MMIO Base/Limit */
 	pci_write_config_dword(dev, PCI_PREF_LIMIT_UPPER32, 0);
 	pci_write_config_dword(dev, PCI_PREF_MEMORY_BASE, 0x0000fff0);
+	/*
+	 * 在以下使用PCI_PREF_BASE_UPPER32:
+	 *   - drivers/pci/pci-bridge-emul.c|125| <<global>> [PCI_PREF_BASE_UPPER32 / 4] = {
+	 *   - arch/powerpc/platforms/powermac/pci.c|1231| <<fixup_u4_pcie>> pci_write_config_dword(dev, PCI_PREF_BASE_UPPER32, 0);
+	 *   - drivers/pci/controller/vmd.c|560| <<vmd_domain_reset>> writel(0xffffffff, base + PCI_PREF_BASE_UPPER32);
+	 *   - drivers/pci/hotplug/ibmphp_pci.c|977| <<configure_bridge>> pci_bus_write_config_dword(ibmphp_pci_bus, devfn, PCI_PREF_BASE_UPPER32, 0x00000000);
+	 *   - drivers/pci/hotplug/ibmphp_res.c|2060| <<update_bridge_ranges>> pci_bus_read_config_dword(ibmphp_pci_bus, devfn, PCI_PREF_BASE_UPPER32, &upper_start);
+	 *   - drivers/pci/probe.c|388| <<pci_read_bridge_windows>> pci_read_config_dword(bridge, PCI_PREF_BASE_UPPER32, &pmem);
+	 *   - drivers/pci/probe.c|389| <<pci_read_bridge_windows>> pci_write_config_dword(bridge, PCI_PREF_BASE_UPPER32,
+	 *   - drivers/pci/probe.c|391| <<pci_read_bridge_windows>> pci_read_config_dword(bridge, PCI_PREF_BASE_UPPER32, &tmp);
+	 *   - drivers/pci/probe.c|392| <<pci_read_bridge_windows>> pci_write_config_dword(bridge, PCI_PREF_BASE_UPPER32, pmem);
+	 *   - drivers/pci/probe.c|478| <<pci_read_bridge_mmio_pref>> pci_read_config_dword(dev, PCI_PREF_BASE_UPPER32, &mem_base_hi);
+	 *   - drivers/pci/setup-bus.c|658| <<pci_setup_bridge_mmio_pref>> pci_write_config_dword(bridge, PCI_PREF_BASE_UPPER32, bu);
+	 *   - drivers/pci/setup-res.c|182| <<pci_disable_bridge_window>> pci_write_config_dword(dev, PCI_PREF_BASE_UPPER32, 0xffffffff);
+	 */
 	pci_write_config_dword(dev, PCI_PREF_BASE_UPPER32, 0xffffffff);
 }
 
@@ -196,6 +261,10 @@ resource_size_t __weak pcibios_retrieve_fw_addr(struct pci_dev *dev, int idx)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/pci/setup-res.c|383| <<pci_assign_resource>> ret = pci_revert_fw_address(res, dev, resno, size);
+ */
 static int pci_revert_fw_address(struct resource *res, struct pci_dev *dev,
 		int resno, resource_size_t size)
 {
@@ -258,6 +327,10 @@ resource_size_t __weak pcibios_align_resource(void *data,
        return res->start;
 }
 
+/*
+ * called by:
+ *   - drivers/pci/setup-res.c|347| <<_pci_assign_resource>> while ((ret = __pci_assign_resource(bus, dev, resno, size, min_align))) {
+ */
 static int __pci_assign_resource(struct pci_bus *bus, struct pci_dev *dev,
 		int resno, resource_size_t size, resource_size_t align)
 {
@@ -306,6 +379,11 @@ static int __pci_assign_resource(struct pci_bus *bus, struct pci_dev *dev,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/pci/setup-res.c|374| <<pci_assign_resource>> ret = _pci_assign_resource(dev, resno, size, align);
+ *   - drivers/pci/setup-res.c|422| <<pci_reassign_resource>> ret = _pci_assign_resource(dev, resno, new_size, min_align);
+ */
 static int _pci_assign_resource(struct pci_dev *dev, int resno,
 				resource_size_t size, resource_size_t min_align)
 {
@@ -322,6 +400,23 @@ static int _pci_assign_resource(struct pci_dev *dev, int resno,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/bcma/driver_pci_host.c|571| <<bcma_core_pci_fixup_addresses>> err = pci_assign_resource(dev, pos);
+ *   - drivers/bus/mhi/host/pci_generic.c|753| <<mhi_pci_claim>> err = pci_assign_resource(pdev, bar_num);
+ *   - drivers/char/agp/efficeon-agp.c|389| <<agp_efficeon_probe>> if (pci_assign_resource(pdev, 0)) {
+ *   - drivers/char/agp/intel-agp.c|778| <<agp_intel_probe>> if (pci_assign_resource(pdev, 0)) {
+ *   - drivers/mtd/maps/l440gx.c|112| <<init_l440gx>> if (pci_assign_resource(pm_dev, PIIXE_IOBASE_RESOURCE) != 0) {
+ *   - drivers/net/wireless/ath/ath11k/pci.c|511| <<ath11k_pci_claim>> ret = pci_assign_resource(pdev, ATH11K_PCI_BAR_NUM);
+ *   - drivers/net/wireless/ath/ath12k/pci.c|725| <<ath12k_pci_claim>> ret = pci_assign_resource(pdev, ATH12K_PCI_BAR_NUM);
+ *   - drivers/parisc/dino.c|636| <<dino_fixup_bus>> WARN_ON(pci_assign_resource(bus->self, i));
+ *   - drivers/pci/rom.c|143| <<pci_map_rom>> if (res->parent == NULL && pci_assign_resource(pdev, PCI_ROM_RESOURCE))
+ *   - drivers/pci/setup-bus.c|245| <<reassign_resources_sorted>> if (pci_assign_resource(add_res->dev, idx))
+ *   - drivers/pci/setup-bus.c|283| <<assign_requested_resources_sorted>> pci_assign_resource(dev_res->dev, idx)) {
+ *
+ * 先分配resource的range
+ * 对于bar的那几个,还要调用pci_update_resource()生效
+ */
 int pci_assign_resource(struct pci_dev *dev, int resno)
 {
 	struct resource *res = dev->resource + resno;
@@ -367,6 +462,10 @@ int pci_assign_resource(struct pci_dev *dev, int resno)
 }
 EXPORT_SYMBOL(pci_assign_resource);
 
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|250| <<reassign_resources_sorted>> if (pci_reassign_resource(add_res->dev, idx, add_size, align))
+ */
 int pci_reassign_resource(struct pci_dev *dev, int resno, resource_size_t addsize,
 			resource_size_t min_align)
 {
@@ -379,6 +478,9 @@ int pci_reassign_resource(struct pci_dev *dev, int resno, resource_size_t addsiz
 		return 0;
 
 	flags = res->flags;
+	/*
+	 * 似乎这个重要!!
+	 */
 	res->flags |= IORESOURCE_UNSET;
 	if (!res->parent) {
 		pci_info(dev, "BAR %d: can't reassign an unassigned resource %pR\n",
@@ -406,6 +508,13 @@ int pci_reassign_resource(struct pci_dev *dev, int resno, resource_size_t addsiz
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_device.c|1150| <<amdgpu_device_resize_fb_bar>> pci_release_resource(adev->pdev, 2);
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_device.c|1152| <<amdgpu_device_resize_fb_bar>> pci_release_resource(adev->pdev, 0);
+ *   - drivers/gpu/drm/i915/gt/intel_region_lmem.c|27| <<_release_bars>> pci_release_resource(pdev, resno);
+ *   - drivers/pci/pci-sysfs.c|1449| <<pci_dev_resource_resize_attr>> pci_release_resource(pdev, i); \
+ */
 void pci_release_resource(struct pci_dev *dev, int resno)
 {
 	struct resource *res = dev->resource + resno;
@@ -422,6 +531,12 @@ void pci_release_resource(struct pci_dev *dev, int resno)
 }
 EXPORT_SYMBOL(pci_release_resource);
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_device.c|1154| <<amdgpu_device_resize_fb_bar>> r = pci_resize_resource(adev->pdev, 0, rbar_size);
+ *   - drivers/gpu/drm/i915/gt/intel_region_lmem.c|40| <<_resize_bar>> ret = pci_resize_resource(pdev, resno, bar_size);
+ *   - drivers/pci/pci-sysfs.c|1452| <<pci_dev_resource_resize_attr>> ret = pci_resize_resource(pdev, n, size); \
+ */
 int pci_resize_resource(struct pci_dev *dev, int resno, int size)
 {
 	struct resource *res = dev->resource + resno;
@@ -475,6 +590,16 @@ int pci_resize_resource(struct pci_dev *dev, int resno, int size)
 }
 EXPORT_SYMBOL(pci_resize_resource);
 
+/*
+ * called by:
+ *   - arch/ia64/pci/pci.c|396| <<pcibios_enable_device>> ret = pci_enable_resources(dev, mask);
+ *   - arch/parisc/kernel/pci.c|232| <<pcibios_enable_device>> err = pci_enable_resources(dev, mask);
+ *   - arch/powerpc/kernel/pci-common.c|1513| <<pcibios_enable_device>> return pci_enable_resources(dev, mask);
+ *   - arch/s390/pci/pci.c|613| <<pcibios_enable_device>> return pci_enable_resources(pdev, mask);
+ *   - arch/x86/pci/common.c|695| <<pcibios_enable_device>> if ((err = pci_enable_resources(dev, mask)) < 0)
+ *   - drivers/pci/iov.c|643| <<sriov_enable>> if (pci_enable_resources(dev, bars)) {
+ *   - drivers/pci/pci.c|1935| <<pcibios_enable_device>> return pci_enable_resources(dev, bars);
+ */
 int pci_enable_resources(struct pci_dev *dev, int mask)
 {
 	u16 cmd, old_cmd;
diff --git a/drivers/scsi/scsi_common.c b/drivers/scsi/scsi_common.c
index 9c14fdf61..317ac76c2 100644
--- a/drivers/scsi/scsi_common.c
+++ b/drivers/scsi/scsi_common.c
@@ -52,6 +52,19 @@ static const char *const scsi_device_types[] = {
  * scsi_device_type - Return 17-char string indicating device type.
  * @type: type number to look up
  */
+/*
+ * called by:
+ *   - drivers/scsi/hpsa.c|845| <<path_info_show>> scsi_device_type(hdev->devtype));
+ *   - drivers/scsi/hpsa.c|1274| <<hpsa_show_dev_msg>> scsi_device_type(dev->devtype),
+ *   - drivers/scsi/hpsa.c|3989| <<hpsa_update_device_info>> scsi_device_type(this_device->devtype),
+ *   - drivers/scsi/megaraid.c|2300| <<mega_print_inquiry>> seq_printf(m, " Type: %s ", scsi_device_type(i));
+ *   - drivers/scsi/scsi_proc.c|311| <<proc_print_scsidevice>> seq_printf(s, " Type: %s ", scsi_device_type(sdev->type));
+ *   - drivers/scsi/scsi_scan.c|963| <<scsi_add_lun>> sdev_printk(KERN_NOTICE, sdev, "%s %.8s %.16s %.4s PQ: %d " "ANSI: %d%s\n", scsi_device_type(sdev->type),
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|1984| <<pqi_device_type>> return scsi_device_type(device->devtype);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|7185| <<pqi_path_info_show>> scsi_device_type(device->devtype));
+ *   - drivers/target/target_core_device.c|685| <<scsi_dump_inquiry>> pr_debug(" Type: %s ", scsi_device_type(device_type));
+ *   - drivers/target/target_core_pscsi.c|415| <<pscsi_create_type_nondisk>> phv->phv_host_id, scsi_device_type(sd->type), sh->host_no,
+ */
 const char *scsi_device_type(unsigned type)
 {
 	if (type == 0x1e)
diff --git a/drivers/scsi/scsi_scan.c b/drivers/scsi/scsi_scan.c
index 44680f65e..5229655d5 100644
--- a/drivers/scsi/scsi_scan.c
+++ b/drivers/scsi/scsi_scan.c
@@ -1951,6 +1951,10 @@ static void do_scsi_scan_host(struct Scsi_Host *shost)
 	}
 }
 
+/*
+ * 在以下使用do_scan_async:
+ *   - drivers/scsi/scsi_scan.c|1987| <<scsi_scan_host>> async_schedule(do_scan_async, data);
+ */
 static void do_scan_async(void *_data, async_cookie_t c)
 {
 	struct async_scan_data *data = _data;
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 9d1bdcdc1..cec394935 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -33,6 +33,87 @@
 
 #include "sd.h"
 
+/*
+ * [0] ACPI: OSL: debug: acpi_hotplug_schedule() src=1 --> 是ACPI_NOTIFY_DEVICE_CHECK
+ * [0] CPU: 0 PID: 103 Comm: kworker/0:2 Not tainted 6.6.0 #2
+ * [0] Workqueue: kacpi_notify acpi_os_execute_deferred
+ * [0] acpi_hotplug_schedule
+ * [0] acpi_bus_notify
+ * [0] acpi_ev_notify_dispatch
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * CPU: 1 PID: 11 Comm: kworker/u16:0 Not tainted 6.6.0-dirty #13
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] pci_assign_unassigned_bridge_resources
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] blk_execute_rq
+ * [0] scsi_execute_cmd
+ * [0] scsi_probe_lun
+ * [0] scsi_probe_and_add_lun
+ * [0] __scsi_scan_target
+ * [0] scsi_scan_channel
+ * [0] scsi_scan_host_selected
+ * [0] do_scsi_scan_host
+ * [0] do_scan_async
+ * [0] async_run_entry_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] CPU: 2 PID: 10 Comm: kworker/u16:0 Not tainted 6.6.0-dirty #10
+ * [0] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] do_scsi_scan_host
+ * [0] scsi_scan_host
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __device_attach_driver
+ * [0] bus_for_each_drv
+ * [0] __device_attach
+ * [0] bus_probe_device
+ * [0] device_add
+ * [0] register_virtio_device
+ * [0] virtio_pci_probe
+ * [0] local_pci_probe
+ * [0] pci_device_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __device_attach_driver
+ * [0] bus_for_each_drv
+ * [0] __device_attach
+ * [0] pci_bus_add_device
+ * [0] pci_bus_add_devices
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ */
+
 #define VIRTIO_SCSI_MEMPOOL_SZ 64
 #define VIRTIO_SCSI_EVENT_LEN 8
 #define VIRTIO_SCSI_VQ_BASE 2
@@ -167,6 +248,13 @@ static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 	scsi_done(sc);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|198| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|207| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i], virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|224| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+ *   - drivers/scsi/virtio_scsi.c|422| <<virtscsi_event_done>> virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
+ */
 static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 			     struct virtio_scsi_vq *virtscsi_vq,
 			     void (*fn)(struct virtio_scsi *vscsi, void *buf))
@@ -179,6 +267,9 @@ static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 	spin_lock_irqsave(&virtscsi_vq->vq_lock, flags);
 	do {
 		virtqueue_disable_cb(vq);
+		/*
+		 * 比如: virtscsi_complete_cmd()
+		 */
 		while ((buf = virtqueue_get_buf(vq, &len)) != NULL)
 			fn(vscsi, buf);
 
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index f2ed7167c..908a0f267 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -1258,6 +1258,10 @@ static void handle_rx(struct vhost_net *net)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用handle_tx_kick():
+ *   - drivers/vhost/net.c|1332| <<vhost_net_open>> n->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;
+ */
 static void handle_tx_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index abef0619c..1d2ef5815 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -463,6 +463,17 @@ vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 
 again:
 	vhost_disable_notify(&vs->dev, vq);
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|581| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|593| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1069| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|466| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/scsi.c|940| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|126| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|495| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 */
 	head = vhost_get_vq_desc(vq, vq->iov,
 			ARRAY_SIZE(vq->iov), &out, &in,
 			NULL, NULL);
@@ -591,6 +602,10 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 		vhost_signal(&svq->vs->dev, &svq->vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1195| <<vhost_scsi_handle_vq>> cmd = vhost_scsi_get_cmd(vq, tpg, cdb, tag, lun, task_attr, exp_data_len + prot_bytes, data_direction);
+ */
 static struct vhost_scsi_cmd *
 vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		   unsigned char *cdb, u64 scsi_tag, u16 lun, u8 task_attr,
@@ -931,12 +946,28 @@ vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_cmd_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1069| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ *   - drivers/vhost/scsi.c|1394| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ */
 static int
 vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		    struct vhost_scsi_ctx *vc)
 {
 	int ret = -ENXIO;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|581| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|593| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1069| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|466| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/scsi.c|940| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|126| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|495| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 */
 	vc->head = vhost_get_vq_desc(vq, vq->iov,
 				     ARRAY_SIZE(vq->iov), &vc->out, &vc->in,
 				     NULL, NULL);
@@ -1034,6 +1065,10 @@ static u16 vhost_buf_to_lun(u8 *lun_buf)
 	return ((lun_buf[2] << 8) | lun_buf[3]) & 0x3FFF;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1538| <<vhost_scsi_handle_kick>> vhost_scsi_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -1529,6 +1564,10 @@ static void vhost_scsi_evt_handle_kick(struct vhost_work *work)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用vhost_scsi_handle_kick():
+ *   - drivers/vhost/scsi.c|1972| <<vhost_scsi_open>> svq->vq.handle_kick = vhost_scsi_handle_kick;
+ */
 static void vhost_scsi_handle_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index e0c181ad1..523a7ee03 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -46,6 +46,46 @@ enum {
 	VHOST_MEMORY_F_LOG = 0x1,
 };
 
+/*
+ * 在virtio端的定义:
+ *
+ * #define vring_used_event(vr) ((vr)->avail->ring[(vr)->num])
+ * #define vring_avail_event(vr) (*(__virtio16 *)&(vr)->used->ring[(vr)->num])
+ *
+ * 在vhost端的定义:
+ *
+ * #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
+ * #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
+ *
+ * -----------------------------
+ *
+ * #define vring_used_event(vr) ((vr)->avail->ring[(vr)->num])
+ * #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
+ *
+ * 在VM side存放当前的vq->last_used_idx (也就是下一个要用到的vq->last_used_idx)
+ *
+ *
+ * #define vring_avail_event(vr) (*(__virtio16 *)&(vr)->used->ring[(vr)->num])
+ * #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
+ *
+ * 在Hypervisor side存放当前的vq->avail_idx.
+ *
+ * vhost_scsi_handle_vq()
+ * -> vhost_scsi_get_desc()
+ *    -> vhost_enable_notify()
+ *       -> vhost_update_avail_event()
+ *          -> vhost_avail_event()
+ *
+ * 对vhost-scsi仍然有疑虑.
+ */
+
+/*
+ * vhost_scsi_handle_vq()
+ * -> vhost_scsi_get_desc()
+ *    -> vhost_enable_notify()
+ *       -> vhost_update_avail_event()
+ *          -> vhost_avail_event()
+ */
 #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
 #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
 
@@ -392,6 +432,10 @@ static void vhost_vq_reset(struct vhost_dev *dev,
 	__vhost_vq_meta_reset(vq);
 }
 
+/*
+ * 在以下使用vhost_worker():
+ *   - drivers/vhost/vhost.c|676| <<vhost_worker_create>> vtsk = vhost_task_create(vhost_worker, worker, name);
+ */
 static bool vhost_worker(void *data)
 {
 	struct vhost_worker *worker = data;
@@ -619,6 +663,11 @@ static void vhost_workers_free(struct vhost_dev *dev)
 	xa_destroy(&dev->worker_xa);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|777| <<vhost_new_worker>> worker = vhost_worker_create(dev);
+ *   - drivers/vhost/vhost.c|929| <<vhost_dev_set_owner>> worker = vhost_worker_create(dev);
+ */
 static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
@@ -1062,6 +1111,11 @@ static inline void __user *vhost_vq_meta_fetch(struct vhost_virtqueue *vq,
 
 /* Can we switch to this memory table? */
 /* Caller should have device mutex but not vq mutex */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1717| <<vhost_log_access_ok>> return memory_access_ok(dev, dev->umem, 1);
+ *   - drivers/vhost/vhost.c|1800| <<vhost_set_memory>> if (!memory_access_ok(d, newumem, 0))
+ */
 static bool memory_access_ok(struct vhost_dev *d, struct vhost_iotlb *umem,
 			     int log_all)
 {
@@ -1224,6 +1278,10 @@ static inline void __user *__vhost_get_user(struct vhost_virtqueue *vq,
 	ret; \
 })
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2281| <<vhost_update_avail_event>> if (vhost_put_avail_event(vq))
+ */
 static inline int vhost_put_avail_event(struct vhost_virtqueue *vq)
 {
 	return vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
@@ -1723,6 +1781,13 @@ static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 		return -EOPNOTSUPP;
 	if (mem.nregions > max_mem_regions)
 		return -E2BIG;
+	/*
+	 * struct vhost_memory {
+	 *     __u32 nregions;
+	 *     __u32 padding;
+	 *     struct vhost_memory_region regions[];
+	 * };
+	 */
 	newmem = kvzalloc(struct_size(newmem, regions, mem.nregions),
 			GFP_KERNEL);
 	if (!newmem)
@@ -2276,6 +2341,16 @@ static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * vhost_scsi_handle_vq()
+ * -> vhost_scsi_get_desc()
+ *    -> vhost_enable_notify()
+ *       -> vhost_update_avail_event()
+ *          -> vhost_avail_event()
+ *
+ * called by:
+ *   - drivers/vhost/vhost.c|2836| <<vhost_enable_notify>> r = vhost_update_avail_event(vq);
+ */
 static int vhost_update_avail_event(struct vhost_virtqueue *vq)
 {
 	if (vhost_put_avail_event(vq))
@@ -2491,6 +2566,17 @@ static int get_indirect(struct vhost_virtqueue *vq,
  * This function returns the descriptor number found, or vq->num (which is
  * never a valid descriptor number) if none was found.  A negative code is
  * returned on error. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|581| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|593| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|1069| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+ *   - drivers/vhost/scsi.c|466| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/scsi.c|940| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+ *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|126| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|495| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ */
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
@@ -2805,6 +2891,20 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_avail_empty);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|515| <<vhost_net_busy_poll_try_queue>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|567| <<vhost_net_busy_poll>> vhost_enable_notify(&net->dev, rvq);
+ *   - drivers/vhost/net.c|802| <<handle_tx_copy>> } else if (unlikely(vhost_enable_notify(&net->dev,
+ *   - drivers/vhost/net.c|896| <<handle_tx_zerocopy>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|1178| <<handle_rx>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/scsi.c|485| <<vhost_scsi_do_evt_work>> if (vhost_enable_notify(&vs->dev, vq))
+ *   - drivers/vhost/scsi.c|984| <<vhost_scsi_get_desc>> if (unlikely(vhost_enable_notify(&vs->dev, vq))) {
+ *   - drivers/vhost/test.c|70| <<handle_vq>> if (unlikely(vhost_enable_notify(&n->dev, vq))) {
+ *   - drivers/vhost/vsock.c|122| <<vhost_transport_do_send_pkt>> vhost_enable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|138| <<vhost_transport_do_send_pkt>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ *   - drivers/vhost/vsock.c|501| <<vhost_vsock_handle_tx_kick>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ */
 /* OK, now we need to know about added descriptors. */
 bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index f60d5f7be..6a31c8b24 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -100,19 +100,75 @@ struct vhost_virtqueue {
 	/* Last available index we saw.
 	 * Values are limited to 0x7fff, and the high bit is used as
 	 * a wrap counter when using VIRTIO_F_RING_PACKED. */
+	/*
+	 * vhost.c在以下使用vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|370| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1911| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num & 0xffff;
+	 *   - drivers/vhost/vhost.c|1918| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|1921| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|1926| <<vhost_vring_ioctl>> s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
+	 *   - drivers/vhost/vhost.c|1928| <<vhost_vring_ioctl>> s.num = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2507| <<vhost_get_vq_desc>> last_avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2509| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2526| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2626| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2638| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 *   - drivers/vhost/vhost.c|2735| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2796| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2804| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2843| <<vhost_enable_notify>> return vq->avail_idx != vq->last_avail_idx;
+	 */
 	u16 last_avail_idx;
 
 	/* Caches available index value from user. */
+	/*
+	 * vhost.c在以下使用vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|371| <<vhost_vq_reset>> vq->avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1229| <<vhost_put_avail_event>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
+	 *   - drivers/vhost/vhost.c|1921| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2509| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2515| <<vhost_get_vq_desc>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2517| <<vhost_get_vq_desc>> if (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {
+	 *   - drivers/vhost/vhost.c|2519| <<vhost_get_vq_desc>> last_avail_idx, vq->avail_idx);
+	 *   - drivers/vhost/vhost.c|2526| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2735| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2796| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2802| <<vhost_vq_avail_empty>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2804| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2843| <<vhost_enable_notify>> return vq->avail_idx != vq->last_avail_idx;
+	 */
 	u16 avail_idx;
 
 	/* Last index we used.
 	 * Values are limited to 0x7fff, and the high bit is used as
 	 * a wrap counter when using VIRTIO_F_RING_PACKED. */
+	/*
+	 * vhost.c在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|372| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1251| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|1912| <<vhost_vring_ioctl>> vq->last_used_idx = (s.num >> 16) & 0xffff;
+	 *   - drivers/vhost/vhost.c|1926| <<vhost_vring_ioctl>> s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
+	 *   - drivers/vhost/vhost.c|2299| <<vhost_vq_init_access>> __virtio16 last_used_idx;
+	 *   - drivers/vhost/vhost.c|2317| <<vhost_vq_init_access>> r = vhost_get_used_idx(vq, &last_used_idx);
+	 *   - drivers/vhost/vhost.c|2323| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2663| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2676| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2677| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|2694| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2748| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 last_used_idx;
 
 	/* Used flags */
 	u16 used_flags;
 
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|373| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2682| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2746| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2748| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	/* Last used index value we have signalled on */
 	u16 signalled_used;
 
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 51d8f3299..9778d3544 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -97,6 +97,19 @@ struct vring_virtqueue_split {
 	 * Last written value to avail->idx in
 	 * guest byte order.
 	 */
+	/*
+	 * 在以下使用vring_virtqueue_split->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|687| <<virtqueue_add_split>> avail = vq->split.avail_idx_shadow & (vq->split.vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|693| <<virtqueue_add_split>> vq->split.avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|695| <<virtqueue_add_split>> vq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->split.avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|745| <<virtqueue_kick_prepare_split>> old = vq->split.avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|746| <<virtqueue_kick_prepare_split>> new = vq->split.avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|958| <<virtqueue_enable_cb_delayed_split>> bufs = (u16)(vq->split.avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|988| <<virtqueue_detach_unused_buf_split>> vq->split.avail_idx_shadow--;
+	 *   - drivers/virtio/virtio_ring.c|990| <<virtqueue_detach_unused_buf_split>> vq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->split.avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1009| <<virtqueue_vring_init_split>> vring_split->avail_idx_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|2972| <<vring_notification_data>> next = vq->split.avail_idx_shadow;
+	 */
 	u16 avail_idx_shadow;
 
 	/* Per-descriptor state. */
@@ -682,6 +695,19 @@ static inline int virtqueue_add_split(struct virtqueue *_vq,
 	else
 		vq->split.desc_state[head].indir_desc = ctx;
 
+	/*
+	 * 在以下使用vring_virtqueue_split->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|687| <<virtqueue_add_split>> avail = vq->split.avail_idx_shadow & (vq->split.vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|693| <<virtqueue_add_split>> vq->split.avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|695| <<virtqueue_add_split>> vq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->split.avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|745| <<virtqueue_kick_prepare_split>> old = vq->split.avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|746| <<virtqueue_kick_prepare_split>> new = vq->split.avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|958| <<virtqueue_enable_cb_delayed_split>> bufs = (u16)(vq->split.avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|988| <<virtqueue_detach_unused_buf_split>> vq->split.avail_idx_shadow--;
+	 *   - drivers/virtio/virtio_ring.c|990| <<virtqueue_detach_unused_buf_split>> vq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->split.avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1009| <<virtqueue_vring_init_split>> vring_split->avail_idx_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|2972| <<vring_notification_data>> next = vq->split.avail_idx_shadow;
+	 */
 	/* Put entry in available array (but don't update avail->idx until they
 	 * do sync). */
 	avail = vq->split.avail_idx_shadow & (vq->split.vring.num - 1);
@@ -762,6 +788,11 @@ static bool virtqueue_kick_prepare_split(struct virtqueue *_vq)
 	return needs_kick;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|895| <<virtqueue_get_buf_ctx_split>> detach_buf_split(vq, i, ctx);
+ *   - drivers/virtio/virtio_ring.c|1017| <<virtqueue_detach_unused_buf_split>> detach_buf_split(vq, i, NULL);
+ */
 static void detach_buf_split(struct vring_virtqueue *vq, unsigned int head,
 			     void **ctx)
 {
@@ -820,6 +851,10 @@ static bool more_used_split(const struct vring_virtqueue *vq)
 			vq->split.vring.used->idx);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2421| <<virtqueue_get_buf_ctx>> virtqueue_get_buf_ctx_split(_vq, len, ctx);
+ */
 static void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,
 					 unsigned int *len,
 					 void **ctx)
@@ -862,6 +897,9 @@ static void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,
 
 	/* detach_buf_split clears data, so grab it now. */
 	ret = vq->split.desc_state[i].data;
+	/*
+	 * 这里是核心!!!!
+	 */
 	detach_buf_split(vq, i, ctx);
 	vq->last_used_idx++;
 	/* If we expect an interrupt for the next entry, tell host
@@ -2412,6 +2450,11 @@ EXPORT_SYMBOL_GPL(virtqueue_kick);
  * Returns NULL if there are no used buffers, or the "data" token
  * handed to virtqueue_add_*().
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|638| <<virtnet_rq_get_buf>> buf = virtqueue_get_buf_ctx(rq->vq, len, ctx);
+ *   - drivers/virtio/virtio_ring.c|2427| <<virtqueue_get_buf>> return virtqueue_get_buf_ctx(_vq, len, NULL);
+ */
 void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 			    void **ctx)
 {
@@ -2422,6 +2465,18 @@ void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 }
 EXPORT_SYMBOL_GPL(virtqueue_get_buf_ctx);
 
+/*
+ * 太多了, 选择一些例子:
+ *   - drivers/block/virtio_blk.c|398| <<virtblk_done>> while ((vbr = virtqueue_get_buf(vblk->vqs[qid].vq, &len)) != NULL) {
+ *   - drivers/block/virtio_blk.c|1309| <<virtblk_poll>> while ((vbr = virtqueue_get_buf(vq->vq, &len)) != NULL) {
+ *   - drivers/net/virtio_net.c|762| <<free_old_xmit_skbs>> while ((ptr = virtqueue_get_buf(sq->vq, &len)) != NULL) {
+ *   - drivers/net/virtio_net.c|953| <<virtnet_xdp_xmit>> while ((ptr = virtqueue_get_buf(sq->vq, &len)) != NULL) {
+ *   - drivers/net/virtio_net.c|2541| <<virtnet_send_command>> while (!virtqueue_get_buf(vi->cvq, &tmp) &&
+ *   - drivers/scsi/virtio_scsi.c|182| <<virtscsi_vq_done>> while ((buf = virtqueue_get_buf(vq, &len)) != NULL)
+ *   - drivers/virtio/virtio_mem.c|1389| <<virtio_mem_send_request>> wait_event(vm->host_resp, virtqueue_get_buf(vm->vq, &len));
+ *   - fs/fuse/virtio_fs.c|344| <<virtio_fs_hiprio_done_work>> while ((req = virtqueue_get_buf(vq, &len)) != NULL) {
+ *   - fs/fuse/virtio_fs.c|625| <<virtio_fs_requests_done_work>> while ((req = virtqueue_get_buf(vq, &len)) != NULL) {
+ */
 void *virtqueue_get_buf(struct virtqueue *_vq, unsigned int *len)
 {
 	return virtqueue_get_buf_ctx(_vq, len, NULL);
@@ -2567,6 +2622,41 @@ static inline bool more_used(const struct vring_virtqueue *vq)
  * Calls the callback function of @_vq to process the virtqueue
  * notification.
  */
+/*
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_fasteoi_irq
+ * [0] generic_handle_domain_irq
+ * [0] gic_handle_irq
+ * [0] call_on_irq_stack
+ * [0] do_interrupt_handler
+ * [0] el1_interrupt
+ * [0] el1h_64_irq_handler
+ * [0] el1h_64_irq
+ * [0] __folio_start_writeback
+ * [0] ext4_bio_write_folio
+ * [0] mpage_submit_folio
+ * [0] mpage_process_page_bufs
+ * [0] mpage_prepare_extent_to_map
+ * [0] ext4_do_writepages
+ * [0] ext4_writepages
+ * [0] do_writepages
+ * [0] filemap_fdatawrite_wbc
+ * [0] __filemap_fdatawrite_range
+ * [0] file_write_and_wait_range
+ * [0] ext4_sync_file
+ * [0] vfs_fsync_range
+ * [0] do_fsync
+ * [0] __arm64_sys_fsync
+ * [0] invoke_syscall
+ * [0] el0_svc_common.constprop
+ * [0] do_el0_svc
+ * [0] el0_svc
+ * [0] el0t_64_sync_handler
+ * [0] el0t_64_sync
+ */
 irqreturn_t vring_interrupt(int irq, void *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
diff --git a/include/kvm/arm_vgic.h b/include/kvm/arm_vgic.h
index 5b27f94d4..0b4a9bc56 100644
--- a/include/kvm/arm_vgic.h
+++ b/include/kvm/arm_vgic.h
@@ -274,10 +274,42 @@ struct vgic_dist {
 	u64			propbaser;
 
 	/* Protects the lpi_list and the count value below. */
+	/*
+	 * 在以下使用vgic_dist->lpi_list_lock:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|58| <<kvm_vgic_early_init>> raw_spin_lock_init(&dist->lpi_list_lock);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|65| <<vgic_add_lpi>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|93| <<vgic_add_lpi>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|339| <<vgic_copy_lpi_list>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|348| <<vgic_copy_lpi_list>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|609| <<vgic_its_check_cache>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|611| <<vgic_its_check_cache>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|629| <<vgic_its_cache_translation>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|666| <<vgic_its_cache_translation>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|675| <<vgic_its_invalidate_cache>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|689| <<vgic_its_invalidate_cache>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic.c|66| <<vgic_get_lpi>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic.c|82| <<vgic_get_lpi>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic.c|147| <<vgic_put_irq>> raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
+	 *   - arch/arm64/kvm/vgic/vgic.c|149| <<vgic_put_irq>> raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
+	 */
 	raw_spinlock_t		lpi_list_lock;
 	struct list_head	lpi_list_head;
 	int			lpi_list_count;
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1894| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1908| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1920| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 */
 	/* LPI translation cache */
 	struct list_head	lpi_translation_cache;
 
@@ -309,6 +341,16 @@ struct vgic_v3_cpu_if {
 	u32		vgic_sre;	/* Restored only, change ignored */
 	u32		vgic_ap0r[4];
 	u32		vgic_ap1r[4];
+	/*
+	 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+	 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|243| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|186| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|191| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+	 */
 	u64		vgic_lr[VGIC_V3_MAX_LRS];
 
 	/*
@@ -339,6 +381,23 @@ struct vgic_cpu {
 	 * were one of the two and need to be migrated off this list to another
 	 * VCPU.
 	 */
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|160| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|305| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|410| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|653| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|725| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|792| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|824| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|850| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|890| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|929| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|935| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|999| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 */
 	struct list_head ap_list_head;
 
 	/*
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 958ed7e89..e017140ce 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -689,6 +689,37 @@ enum {
 
 struct gendisk *__blk_mq_alloc_disk(struct blk_mq_tag_set *set, void *queuedata,
 		struct lock_class_key *lkclass);
+/*
+ * called by:
+ *   - arch/um/drivers/ubd_kern.c|910| <<ubd_add>> disk = blk_mq_alloc_disk(&ubd_dev->tag_set, ubd_dev);
+ *   - drivers/block/amiflop.c|1782| <<fd_alloc_disk>> disk = blk_mq_alloc_disk(&unit[drive].tag_set, NULL);
+ *   - drivers/block/aoe/aoeblk.c|373| <<aoeblk_gdalloc>> gd = blk_mq_alloc_disk(set, d);
+ *   - drivers/block/ataflop.c|1995| <<ataflop_alloc_disk>> disk = blk_mq_alloc_disk(&unit[drive].tag_set, NULL);
+ *   - drivers/block/floppy.c|4516| <<floppy_alloc_disk>> disk = blk_mq_alloc_disk(&tag_sets[drive], NULL);
+ *   - drivers/block/loop.c|2034| <<loop_add>> disk = lo->lo_disk = blk_mq_alloc_disk(&lo->tag_set, lo);
+ *   - drivers/block/mtip32xx/mtip32xx.c|3434| <<mtip_block_initialize>> dd->disk = blk_mq_alloc_disk(&dd->tags, dd);
+ *   - drivers/block/nbd.c|1788| <<nbd_dev_add>> disk = blk_mq_alloc_disk(&nbd->tag_set, NULL);
+ *   - drivers/block/null_blk/main.c|2119| <<null_add_dev>> nullb->disk = blk_mq_alloc_disk(nullb->tag_set, nullb);
+ *   - drivers/block/ps3disk.c|434| <<ps3disk_probe>> gendisk = blk_mq_alloc_disk(&priv->tag_set, dev);
+ *   - drivers/block/rbd.c|4963| <<rbd_init_disk>> disk = blk_mq_alloc_disk(&rbd_dev->tag_set, rbd_dev);
+ *   - drivers/block/rnbd/rnbd-clt.c|1409| <<rnbd_client_setup_device>> dev->gd = blk_mq_alloc_disk(&dev->sess->tag_set, dev);
+ *   - drivers/block/sunvdc.c|827| <<probe_disk>> g = blk_mq_alloc_disk(&port->tag_set, port);
+ *   - drivers/block/swim.c|823| <<swim_floppy_init>> blk_mq_alloc_disk(&swd->unit[drive].tag_set,
+ *   - drivers/block/swim3.c|1213| <<swim3_attach>> disk = blk_mq_alloc_disk(&fs->tag_set, fs);
+ *   - drivers/block/ublk_drv.c|2163| <<ublk_ctrl_start_dev>> disk = blk_mq_alloc_disk(&ub->tag_set, NULL);
+ *   - drivers/block/virtio_blk.c|1420| <<virtblk_probe>> vblk->disk = blk_mq_alloc_disk(&vblk->tag_set, vblk);
+ *   - drivers/block/xen-blkfront.c|1139| <<xlvbd_alloc_gendisk>> gd = blk_mq_alloc_disk(&info->tag_set, info);
+ *   - drivers/block/z2ram.c|321| <<z2ram_register_disk>> disk = blk_mq_alloc_disk(&tag_set, NULL);
+ *   - drivers/cdrom/gdrom.c|781| <<probe_gdrom>> gd.disk = blk_mq_alloc_disk(&gd.tag_set, NULL);
+ *   - drivers/memstick/core/ms_block.c|2096| <<msb_init_disk>> msb->disk = blk_mq_alloc_disk(&msb->tag_set, card);
+ *   - drivers/memstick/core/mspro_block.c|1141| <<mspro_block_init_disk>> msb->disk = blk_mq_alloc_disk(&msb->tag_set, card);
+ *   - drivers/mmc/core/queue.c|454| <<mmc_init_queue>> disk = blk_mq_alloc_disk(&mq->tag_set, mq);
+ *   - drivers/mtd/mtd_blkdevs.c|336| <<add_mtd_blktrans_dev>> gd = blk_mq_alloc_disk(new->tag_set, new);
+ *   - drivers/mtd/ubi/block.c|396| <<ubiblock_create>> gd = blk_mq_alloc_disk(&dev->tag_set, dev);
+ *   - drivers/nvme/host/core.c|3592| <<nvme_alloc_ns>> disk = blk_mq_alloc_disk(ctrl->tagset, ns);
+ *   - drivers/s390/block/dasd_genhd.c|61| <<dasd_gendisk_alloc>> gdp = blk_mq_alloc_disk(&block->tag_set, block);
+ *   - drivers/s390/block/scm_blk.c|464| <<scm_blk_dev_setup>> bdev->gendisk = blk_mq_alloc_disk(&bdev->tag_set, scmdev);
+ */
 #define blk_mq_alloc_disk(set, queuedata)				\
 ({									\
 	static struct lock_class_key __key;				\
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index fb6c6109f..83facd139 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -207,6 +207,12 @@ struct kvm_io_range {
 
 struct kvm_io_bus {
 	int dev_count;
+	/*
+	 * 在以下使用kvm_io_bus->ioeventfd_count:
+	 *   - virt/kvm/eventfd.c|915| <<kvm_assign_ioeventfd_idx>> kvm_get_bus(kvm, bus_idx)->ioeventfd_count++;
+	 *   - virt/kvm/eventfd.c|964| <<kvm_deassign_ioeventfd_idx>> bus->ioeventfd_count--;
+	 *   - virt/kvm/kvm_main.c|5596| <<kvm_io_bus_register_dev>> if (bus->dev_count - bus->ioeventfd_count > NR_IOBUS_DEVS - 1)
+	 */
 	int ioeventfd_count;
 	struct kvm_io_range range[];
 };
@@ -761,6 +767,13 @@ struct kvm {
 		struct list_head  resampler_list;
 		struct mutex      resampler_lock;
 	} irqfds;
+	/*
+	 * 在以下使用kvm->ioeventfds:
+	 *   - virt/kvm/eventfd.c|572| <<kvm_eventfd_init>> INIT_LIST_HEAD(&kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|848| <<ioeventfd_check_collision>> list_for_each_entry(_p, &kvm->ioeventfds, list)
+	 *   - virt/kvm/eventfd.c|916| <<kvm_assign_ioeventfd_idx>> list_add_tail(&p->list, &kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|950| <<kvm_deassign_ioeventfd_idx>> list_for_each_entry(p, &kvm->ioeventfds, list) {
+	 */
 	struct list_head ioeventfds;
 #endif
 	struct kvm_vm_stat stat;
@@ -2067,6 +2080,12 @@ static inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 
 void kvm_arch_irq_routing_update(struct kvm *kvm);
 
+/*
+ * called by:
+ *   - arch/s390/kvm/kvm-s390.c|4044| <<kvm_s390_sync_request>> __kvm_make_request(req, vcpu);
+ *   - include/linux/kvm_host.h|2090| <<kvm_make_request>> __kvm_make_request(req, vcpu);
+ *   - virt/kvm/kvm_main.c|268| <<kvm_make_vcpu_request>> __kvm_make_request(req, vcpu);
+ */
 static inline void __kvm_make_request(int req, struct kvm_vcpu *vcpu)
 {
 	/*
diff --git a/include/linux/kvm_irqfd.h b/include/linux/kvm_irqfd.h
index 8ad43692e..adadd756b 100644
--- a/include/linux/kvm_irqfd.h
+++ b/include/linux/kvm_irqfd.h
@@ -45,6 +45,13 @@ struct kvm_kernel_irqfd {
 	seqcount_spinlock_t irq_entry_sc;
 	/* Used for level IRQ fast-path */
 	int gsi;
+	/*
+	 * 在以下使用kvm_kernel_irqfd->inject:
+	 *   - virt/kvm/eventfd.c|141| <<irqfd_shutdown>> flush_work(&irqfd->inject);
+	 *   - virt/kvm/eventfd.c|218| <<irqfd_wakeup>> schedule_work(&irqfd->inject);
+	 *   - virt/kvm/eventfd.c|325| <<kvm_irqfd_assign>> INIT_WORK(&irqfd->inject, irqfd_inject);
+	 *   - virt/kvm/eventfd.c|425| <<kvm_irqfd_assign>> schedule_work(&irqfd->inject);
+	 */
 	struct work_struct inject;
 	/* The resampler used by this irqfd (resampler-only) */
 	struct kvm_kernel_irqfd_resampler *resampler;
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7b5406e32..d778d8a0e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1212,6 +1212,13 @@ static inline void perf_sample_data_init(struct perf_sample_data *data,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|1130| <<perf_ibs_handle_irq>> perf_sample_save_callchain(&data, event, iregs);
+ *   - arch/x86/events/intel/ds.c|1658| <<setup_pebs_fixed_sample_data>> perf_sample_save_callchain(data, event, iregs);
+ *   - arch/x86/events/intel/ds.c|1826| <<setup_pebs_adaptive_sample_data>> perf_sample_save_callchain(data, event, iregs);
+ *   - kernel/events/core.c|7685| <<perf_prepare_sample>> perf_sample_save_callchain(data, event, regs);
+ */
 static inline void perf_sample_save_callchain(struct perf_sample_data *data,
 					      struct perf_event *event,
 					      struct pt_regs *regs)
diff --git a/include/uapi/linux/pci_regs.h b/include/uapi/linux/pci_regs.h
index e5f558d96..4e3248b5c 100644
--- a/include/uapi/linux/pci_regs.h
+++ b/include/uapi/linux/pci_regs.h
@@ -149,6 +149,21 @@
 #define  PCI_PREF_RANGE_TYPE_32	0x00
 #define  PCI_PREF_RANGE_TYPE_64	0x01
 #define  PCI_PREF_RANGE_MASK	(~0x0fUL)
+/*
+ * 在以下使用PCI_PREF_BASE_UPPER32:
+ *   - drivers/pci/pci-bridge-emul.c|125| <<global>> [PCI_PREF_BASE_UPPER32 / 4] = {
+ *   - arch/powerpc/platforms/powermac/pci.c|1231| <<fixup_u4_pcie>> pci_write_config_dword(dev, PCI_PREF_BASE_UPPER32, 0);
+ *   - drivers/pci/controller/vmd.c|560| <<vmd_domain_reset>> writel(0xffffffff, base + PCI_PREF_BASE_UPPER32);
+ *   - drivers/pci/hotplug/ibmphp_pci.c|977| <<configure_bridge>> pci_bus_write_config_dword(ibmphp_pci_bus, devfn, PCI_PREF_BASE_UPPER32, 0x00000000);
+ *   - drivers/pci/hotplug/ibmphp_res.c|2060| <<update_bridge_ranges>> pci_bus_read_config_dword(ibmphp_pci_bus, devfn, PCI_PREF_BASE_UPPER32, &upper_start);
+ *   - drivers/pci/probe.c|388| <<pci_read_bridge_windows>> pci_read_config_dword(bridge, PCI_PREF_BASE_UPPER32, &pmem);
+ *   - drivers/pci/probe.c|389| <<pci_read_bridge_windows>> pci_write_config_dword(bridge, PCI_PREF_BASE_UPPER32,
+ *   - drivers/pci/probe.c|391| <<pci_read_bridge_windows>> pci_read_config_dword(bridge, PCI_PREF_BASE_UPPER32, &tmp);
+ *   - drivers/pci/probe.c|392| <<pci_read_bridge_windows>> pci_write_config_dword(bridge, PCI_PREF_BASE_UPPER32, pmem);
+ *   - drivers/pci/probe.c|478| <<pci_read_bridge_mmio_pref>> pci_read_config_dword(dev, PCI_PREF_BASE_UPPER32, &mem_base_hi);
+ *   - drivers/pci/setup-bus.c|658| <<pci_setup_bridge_mmio_pref>> pci_write_config_dword(bridge, PCI_PREF_BASE_UPPER32, bu);
+ *   - drivers/pci/setup-res.c|182| <<pci_disable_bridge_window>> pci_write_config_dword(dev, PCI_PREF_BASE_UPPER32, 0xffffffff);
+ */
 #define PCI_PREF_BASE_UPPER32	0x28	/* Upper half of prefetchable memory range */
 #define PCI_PREF_LIMIT_UPPER32	0x2c
 #define PCI_IO_BASE_UPPER16	0x30	/* Upper half of I/O addresses */
diff --git a/kernel/async.c b/kernel/async.c
index b2c4ba568..b9adebc39 100644
--- a/kernel/async.c
+++ b/kernel/async.c
@@ -74,6 +74,12 @@ struct async_entry {
 	struct async_domain	*domain;
 };
 
+/*
+ * 在以下使用async_done:
+ *   - kernel/async.c|77| <<global>> static DECLARE_WAIT_QUEUE_HEAD(async_done);
+ *   - kernel/async.c|145| <<async_run_entry_fn>> wake_up(&async_done);
+ *   - kernel/async.c|274| <<async_synchronize_cookie_domain>> wait_event(async_done, lowest_in_progress(domain) >= cookie);
+ */
 static DECLARE_WAIT_QUEUE_HEAD(async_done);
 
 static atomic_t entry_count;
@@ -162,6 +168,12 @@ static void async_run_entry_fn(struct work_struct *work)
  * has no CPUs associated with it then the work is distributed among all
  * available CPUs.
  */
+/*
+ * called by:
+ *   - include/linux/async.h|72| <<async_schedule_domain>> return async_schedule_node_domain(func, data, NUMA_NO_NODE, domain);
+ *   - include/linux/async.h|112| <<async_schedule_dev_domain>> return async_schedule_node_domain(func, dev, dev_to_node(dev), domain);
+ *   - kernel/async.c|230| <<async_schedule_node>> return async_schedule_node_domain(func, data, node, &async_dfl_domain);
+ */
 async_cookie_t async_schedule_node_domain(async_func_t func, void *data,
 					  int node, struct async_domain *domain)
 {
@@ -231,6 +243,25 @@ async_cookie_t async_schedule_node(async_func_t func, void *data, int node)
 }
 EXPORT_SYMBOL_GPL(async_schedule_node);
 
+/*
+ * called by:
+ *   - arch/sh/drivers/pci/pcie-sh7786.c|604| <<sh7786_pcie_init>> async_synchronize_full();
+ *   - drivers/ata/pata_legacy.c|1030| <<legacy_init_one>> async_synchronize_full();
+ *   - drivers/base/dd.c|775| <<wait_for_device_probe>> async_synchronize_full();
+ *   - drivers/base/dd.c|1344| <<driver_detach>> async_synchronize_full();
+ *   - drivers/base/power/main.c|733| <<dpm_noirq_resume_devices>> async_synchronize_full();
+ *   - drivers/base/power/main.c|874| <<dpm_resume_early>> async_synchronize_full();
+ *   - drivers/base/power/main.c|1042| <<dpm_resume>> async_synchronize_full();
+ *   - drivers/base/power/main.c|1329| <<dpm_noirq_suspend_devices>> async_synchronize_full();
+ *   - drivers/base/power/main.c|1516| <<dpm_suspend_late>> async_synchronize_full();
+ *   - drivers/base/power/main.c|1796| <<dpm_suspend>> async_synchronize_full();
+ *   - init/do_mounts.c|429| <<wait_for_root>> async_synchronize_full();
+ *   - init/main.c|1439| <<kernel_init>> async_synchronize_full();
+ *   - kernel/irq/autoprobe.c|39| <<probe_irq_on>> async_synchronize_full();
+ *   - kernel/module/main.c|761| <<SYSCALL_DEFINE2>> async_synchronize_full();
+ *   - kernel/module/main.c|2559| <<do_init_module>> async_synchronize_full();
+ *   - kernel/power/hibernate.c|932| <<find_resume_device>> async_synchronize_full();
+ */
 /**
  * async_synchronize_full - synchronize all asynchronous function calls
  *
@@ -264,6 +295,21 @@ EXPORT_SYMBOL_GPL(async_synchronize_full_domain);
  * synchronization domain specified by @domain submitted prior to @cookie
  * have been done.
  */
+/*
+ * [0] async_synchronize_cookie_domain
+ * [0] async_synchronize_full
+ * [0] do_init_module
+ * [0] load_module
+ * [0] __do_sys_init_module
+ * [0] __x64_sys_init_module
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - init/initramfs.c|742| <<wait_for_initramfs>> async_synchronize_cookie_domain(initramfs_cookie + 1, &initramfs_domain);
+ *   - kernel/async.c|254| <<async_synchronize_full_domain>> async_synchronize_cookie_domain(ASYNC_COOKIE_MAX, domain);
+ *   - kernel/async.c|290| <<async_synchronize_cookie>> async_synchronize_cookie_domain(cookie, &async_dfl_domain);
+ */
 void async_synchronize_cookie_domain(async_cookie_t cookie, struct async_domain *domain)
 {
 	ktime_t starttime;
@@ -271,6 +317,18 @@ void async_synchronize_cookie_domain(async_cookie_t cookie, struct async_domain
 	pr_debug("async_waiting @ %i\n", task_pid_nr(current));
 	starttime = ktime_get();
 
+	/*
+	 * wait_event - sleep until a condition gets true
+	 * @wq_head: the waitqueue to wait on
+	 * @condition: a C expression for the event to wait for
+	 *
+	 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
+	 * @condition evaluates to true. The @condition is checked each time
+	 * the waitqueue @wq_head is woken up.
+	 *
+	 * wake_up() has to be called after changing any variable that could
+	 * change the result of the wait condition.
+	 */
 	wait_event(async_done, lowest_in_progress(domain) >= cookie);
 
 	pr_debug("async_continuing @ %i after %lli usec\n", task_pid_nr(current),
diff --git a/kernel/entry/kvm.c b/kernel/entry/kvm.c
index 2e0f75bcb..d21330e6c 100644
--- a/kernel/entry/kvm.c
+++ b/kernel/entry/kvm.c
@@ -28,6 +28,12 @@ static int xfer_to_guest_mode_work(struct kvm_vcpu *vcpu, unsigned long ti_work)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1023| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|670| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+ *   - arch/x86/kvm/x86.c|11135| <<vcpu_run>> r = xfer_to_guest_mode_handle_work(vcpu);
+ */
 int xfer_to_guest_mode_handle_work(struct kvm_vcpu *vcpu)
 {
 	unsigned long ti_work;
diff --git a/kernel/events/core.c b/kernel/events/core.c
index a2f2a9525..457b78ead 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -5278,6 +5278,19 @@ static void put_event(struct perf_event *event)
  * object, it will not preserve its functionality. Once the last 'user'
  * gives up the object, we'll destroy the thing.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|190| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/riscv/kvm/vcpu_pmu.c|81| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1057| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1059| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+ *   - arch/x86/kvm/pmu.h|115| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|248| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+ *   - kernel/events/core.c|5395| <<perf_release>> perf_event_release_kernel(file->private_data);
+ *   - kernel/events/hw_breakpoint.c|829| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+ *   - kernel/watchdog_perf.c|330| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+ *   - kernel/watchdog_perf.c|406| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+ */
 int perf_event_release_kernel(struct perf_event *event)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -5396,6 +5409,11 @@ static int perf_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5434| <<perf_event_read_value>> count = __perf_event_read_value(event, enabled, running);
+ *   - kernel/events/core.c|5569| <<perf_read_one>> values[n++] = __perf_event_read_value(event, &enabled, &running);
+ */
 static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)
 {
 	struct perf_event *child;
@@ -5425,6 +5443,15 @@ static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *
 	return total;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|110| <<kvm_pmu_get_pmc_value>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/riscv/kvm/vcpu_pmu.c|213| <<pmu_ctr_read>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/riscv/kvm/vcpu_pmu.c|437| <<kvm_riscv_vcpu_pmu_ctr_stop>> pmc->counter_val += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/x86/kvm/pmu.h|71| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+ *   - tools/include/uapi/linux/bpf.h|5702| <<___BPF_FUNC_MAPPER>> FN(perf_event_read_value, 55, ##ctx) \
+ */
 u64 perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)
 {
 	struct perf_event_context *ctx;
@@ -5559,6 +5586,10 @@ static int perf_read_group(struct perf_event *event,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5622| <<__perf_read>> ret = perf_read_one(event, read_format, buf);
+ */
 static int perf_read_one(struct perf_event *event,
 				 u64 read_format, char __user *buf)
 {
@@ -5598,6 +5629,10 @@ static bool is_event_hup(struct perf_event *event)
 /*
  * Read the performance event - simple non blocking version for now
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|5639| <<perf_read>> ret = __perf_read(event, buf, count);
+ */
 static ssize_t
 __perf_read(struct perf_event *event, char __user *buf, size_t count)
 {
@@ -5624,6 +5659,9 @@ __perf_read(struct perf_event *event, char __user *buf, size_t count)
 	return ret;
 }
 
+/*
+ * kernel/events/core.c|6647| <<global>> struct file_operations perf_fops.read = perf_read()
+ */
 static ssize_t
 perf_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)
 {
@@ -5672,6 +5710,10 @@ static void _perf_event_reset(struct perf_event *event)
 	perf_event_update_userpage(event);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|272| <<pmc_pause_counter>> counter += perf_event_pause(pmc->perf_event, true);
+ */
 /* Assume it's not an event with inherit set. */
 u64 perf_event_pause(struct perf_event *event, bool reset)
 {
@@ -5789,6 +5831,12 @@ static int _perf_event_period(struct perf_event *event, u64 value)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/riscv/kvm/vcpu_pmu.c|380| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_period(pmc->perf_event, kvm_pmu_get_sample_period(pmc));
+ *   - arch/x86/kvm/pmu.c|317| <<pmc_resume_counter>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+ *   - arch/x86/kvm/pmu.h|200| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter));
+ */
 int perf_event_period(struct perf_event *event, u64 value)
 {
 	struct perf_event_context *ctx;
@@ -6619,6 +6667,13 @@ static int perf_fasync(int fd, struct file *filp, int on)
 	return 0;
 }
 
+/*
+ * 在以下使用perf_fops:
+ *   - kernel/events/core.c|5835| <<perf_fget_light>> if (f.file->f_op != &perf_fops) {
+ *   - kernel/events/core.c|12661| <<SYSCALL_DEFINE5(perf_event_open)>> event_file = anon_inode_getfile("[perf_event]", &perf_fops, event, f_flags);
+ *   - kernel/events/core.c|13237| <<perf_event_get>> if (file->f_op != &perf_fops) {
+ *   - kernel/events/core.c|13247| <<perf_get_event>> if (file->f_op != &perf_fops)
+ */
 static const struct file_operations perf_fops = {
 	.llseek			= no_llseek,
 	.release		= perf_release,
@@ -6787,6 +6842,11 @@ DEFINE_STATIC_CALL_RET0(__perf_guest_state, *perf_guest_cbs->state);
 DEFINE_STATIC_CALL_RET0(__perf_guest_get_ip, *perf_guest_cbs->get_ip);
 DEFINE_STATIC_CALL_RET0(__perf_guest_handle_intel_pt_intr, *perf_guest_cbs->handle_intel_pt_intr);
 
+/*
+ * called by:
+ *   - arch/x86/xen/pmu.c|556| <<xen_pmu_init>> perf_register_guest_info_callbacks(&xen_guest_cbs);
+ *   - virt/kvm/kvm_main.c|6095| <<kvm_register_perf_callbacks>> perf_register_guest_info_callbacks(&kvm_guest_cbs);
+ */
 void perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *cbs)
 {
 	if (WARN_ON_ONCE(rcu_access_pointer(perf_guest_cbs)))
@@ -7570,6 +7630,10 @@ static u64 perf_get_page_size(unsigned long addr)
 
 static struct perf_callchain_entry __empty_callchain = { .nr = 0, };
 
+/*
+ * called by:
+ *   - include/linux/perf_event.h|1221| <<perf_sample_save_callchain>> data->callchain = perf_callchain(event, regs);
+ */
 struct perf_callchain_entry *
 perf_callchain(struct perf_event *event, struct pt_regs *regs)
 {
@@ -7593,6 +7657,13 @@ static __always_inline u64 __cond_set(u64 flags, u64 s, u64 d)
 	return d * !!(flags & s);
 }
 
+/*
+ * called by:
+ *   - arch/s390/kernel/perf_cpum_sf.c|716| <<cpumsf_output_event_pid>> perf_prepare_sample(data, event, regs);
+ *   - arch/x86/events/intel/ds.c|817| <<intel_pmu_drain_bts_buffer>> perf_prepare_sample(&data, event, &regs);
+ *   - kernel/events/core.c|7873| <<__perf_event_output>> perf_prepare_sample(data, event, regs);
+ *   - kernel/events/core.c|10447| <<bpf_overflow_handler>> perf_prepare_sample(data, event, regs);
+ */
 void perf_prepare_sample(struct perf_sample_data *data,
 			 struct perf_event *event,
 			 struct pt_regs *regs)
@@ -12751,6 +12822,19 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|634| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+ *   - arch/riscv/kvm/vcpu_pmu.c|250| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(attr, -1, current, NULL, pmc);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|968| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|973| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+ *   - arch/x86/kvm/pmu.c|230| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|293| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+ *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+ *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+ *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>> return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+ *   - kernel/watchdog_perf.c|157| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
diff --git a/kernel/panic.c b/kernel/panic.c
index ffa037fa7..ea48c9f9f 100644
--- a/kernel/panic.c
+++ b/kernel/panic.c
@@ -190,6 +190,19 @@ atomic_t panic_cpu = ATOMIC_INIT(PANIC_CPU_INVALID);
  * nmi_panic_self_stop() which can provide architecture dependent code such
  * as saving register state for crash dump.
  */
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ */
 void nmi_panic(struct pt_regs *regs, const char *msg)
 {
 	int old_cpu, cpu;
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 5007b25c5..74695e6bc 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -371,6 +371,30 @@ void play_idle_precise(u64 duration_ns, u64 latency_ns)
 }
 EXPORT_SYMBOL_GPL(play_idle_precise);
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/smp.c|169| <<smp_callin>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/arc/kernel/smp.c|191| <<start_kernel_secondary>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/arm/kernel/smp.c|478| <<secondary_start_kernel>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/arm64/kernel/smp.c|264| <<secondary_start_kernel>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/csky/kernel/smp.c|277| <<csky_start_secondary>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/hexagon/kernel/smp.c|168| <<start_secondary>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/ia64/kernel/smpboot.c|446| <<start_secondary>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/loongarch/kernel/smp.c|540| <<start_secondary>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/mips/kernel/smp.c|401| <<start_secondary>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/openrisc/kernel/smp.c|149| <<secondary_start_kernel>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/parisc/kernel/smp.c|324| <<smp_callin>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/powerpc/kernel/smp.c|1680| <<start_secondary>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/riscv/kernel/smpboot.c|267| <<smp_callin>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/s390/kernel/smp.c|916| <<smp_start_secondary>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/sh/kernel/smp.c|200| <<start_secondary>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/sparc/kernel/smp_32.c|358| <<sparc_start_secondary>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/sparc/kernel/smp_64.c|143| <<smp_callin>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/x86/kernel/smpboot.c|326| <<start_secondary>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/x86/xen/smp_pv.c|96| <<cpu_bringup_and_idle>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - arch/xtensa/kernel/smp.c|165| <<secondary_start_kernel>> cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
+ *   - init/main.c|726| <<rest_init>> cpu_startup_entry(CPUHP_ONLINE);
+ */
 void cpu_startup_entry(enum cpuhp_state state)
 {
 	current->flags |= PF_IDLE;
diff --git a/kernel/stop_machine.c b/kernel/stop_machine.c
index cedb17ba1..bd7f8a9df 100644
--- a/kernel/stop_machine.c
+++ b/kernel/stop_machine.c
@@ -381,6 +381,18 @@ int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *
  * true if cpu_stop_work was queued successfully and @fn will be called,
  * false otherwise.
  */
+/*
+ * called by:
+ *   - kernel/sched/core.c|2668| <<migration_cpu_stop>> stop_one_cpu_nowait(task_cpu(p), migration_cpu_stop,
+ *   - kernel/sched/core.c|2992| <<affine_move_task>> stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
+ *   - kernel/sched/core.c|3063| <<affine_move_task>> stop_one_cpu_nowait(cpu_of(rq), migration_cpu_stop,
+ *   - kernel/sched/core.c|9509| <<balance_push>> stop_one_cpu_nowait(rq->cpu, __balance_push_cpu_stop, push_task,
+ *   - kernel/sched/deadline.c|2453| <<pull_dl_task>> stop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,
+ *   - kernel/sched/fair.c|11257| <<load_balance>> stop_one_cpu_nowait(cpu_of(busiest),
+ *   - kernel/sched/rt.c|2113| <<push_rt_task>> stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
+ *   - kernel/sched/rt.c|2452| <<pull_rt_task>> stop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,
+ *   - kernel/watchdog.c|453| <<watchdog_timer_fn>> stop_one_cpu_nowait(smp_processor_id(),
+ */
 bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,
 			struct cpu_stop_work *work_buf)
 {
diff --git a/kernel/watchdog.c b/kernel/watchdog.c
index d145305d9..6af10dcc4 100644
--- a/kernel/watchdog.c
+++ b/kernel/watchdog.c
@@ -1,4 +1,4 @@
-// SPDX-License-Identifier: GPL-2.0
+// spdx-license-identifier: gpl-2.0
 /*
  * Detect hard and soft lockups on a system
  *
@@ -35,11 +35,32 @@ static DEFINE_MUTEX(watchdog_mutex);
 # define WATCHDOG_HARDLOCKUP_DEFAULT	0
 #endif
 
+/*
+ * WATCHDOG_HARDLOCKUP_ENABLED
+ * WATCHDOG_SOFTLOCKUP_ENABLED
+ */
 unsigned long __read_mostly watchdog_enabled;
 int __read_mostly watchdog_user_enabled = 1;
+/*
+ * 在以下使用watchdog_hardlockup_user_enabled:
+ *   - kernel/watchdog.c|44| <<global>> static int __read_mostly watchdog_hardlockup_user_enabled = WATCHDOG_HARDLOCKUP_DEFAULT;
+ *   - kernel/watchdog.c|1006| <<global>> .data = &watchdog_hardlockup_user_enabled,
+ *   - kernel/watchdog.c|81| <<hardlockup_detector_disable>> watchdog_hardlockup_user_enabled = 0;
+ *   - kernel/watchdog.c|91| <<hardlockup_panic_setup>> watchdog_hardlockup_user_enabled = 0;
+ *   - kernel/watchdog.c|93| <<hardlockup_panic_setup>> watchdog_hardlockup_user_enabled = 1;
+ *   - kernel/watchdog.c|318| <<lockup_detector_update_enable>> if (watchdog_hardlockup_available && watchdog_hardlockup_user_enabled)
+ */
 static int __read_mostly watchdog_hardlockup_user_enabled = WATCHDOG_HARDLOCKUP_DEFAULT;
 static int __read_mostly watchdog_softlockup_user_enabled = 1;
 int __read_mostly watchdog_thresh = 10;
+/*
+ * 在以下使用watchdog_hardlockup_available:
+ *   - kernel/watchdog.c|300| <<lockup_detector_update_enable>> if (watchdog_hardlockup_available && watchdog_hardlockup_user_enabled)
+ *   - kernel/watchdog.c|844| <<proc_nmi_watchdog>> if (!watchdog_hardlockup_available && write)
+ *   - kernel/watchdog.c|1002| <<watchdog_sysctl_init>> if (watchdog_hardlockup_available)
+ *   - kernel/watchdog.c|1030| <<lockup_detector_delay_init>> watchdog_hardlockup_available = true;
+ *   - kernel/watchdog.c|1082| <<lockup_detector_init>> watchdog_hardlockup_available = true;
+ */
 static int __read_mostly watchdog_hardlockup_available;
 
 struct cpumask watchdog_cpumask __read_mostly;
@@ -64,6 +85,13 @@ unsigned int __read_mostly hardlockup_panic =
  * kernel command line parameters are parsed, because otherwise it is not
  * possible to override this in hardlockup_panic_setup().
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/setup_64.c|922| <<disable_hardlockup_detector>> hardlockup_detector_disable();
+ *   - arch/powerpc/kernel/setup_64.c|926| <<disable_hardlockup_detector>> hardlockup_detector_disable();
+ *   - arch/x86/kernel/cpu/mshyperv.c|584| <<ms_hyperv_init_platform>> hardlockup_detector_disable();
+ *   - arch/x86/kernel/kvm.c|866| <<kvm_guest_init>> hardlockup_detector_disable();
+ */
 void __init hardlockup_detector_disable(void)
 {
 	watchdog_hardlockup_user_enabled = 0;
@@ -87,7 +115,21 @@ __setup("nmi_watchdog=", hardlockup_panic_setup);
 
 #if defined(CONFIG_HARDLOCKUP_DETECTOR_COUNTS_HRTIMER)
 
+/*
+ * 在以下使用percpu的hrtimer_interrupts:
+ *   - kernel/watchdog.c|90| <<global>> static DEFINE_PER_CPU(atomic_t, hrtimer_interrupts);
+ *   - kernel/watchdog.c|116| <<is_hardlockup>> int hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));
+ *   - kernel/watchdog.c|135| <<watchdog_hardlockup_kick>> new_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));
+ *   - kernel/watchdog_buddy.c|88| <<watchdog_buddy_check_hardlockup>> void watchdog_buddy_check_hardlockup(int hrtimer_interrupts)
+ *   - kernel/watchdog_buddy.c|97| <<watchdog_buddy_check_hardlockup>> if (hrtimer_interrupts % 3 != 0)
+ */
 static DEFINE_PER_CPU(atomic_t, hrtimer_interrupts);
+/*
+ * 在以下使用percpu的hrtimer_interrupts_saved:
+ *   - kernel/watchdog.c|99| <<global>> static DEFINE_PER_CPU(int , hrtimer_interrupts_saved);
+ *   - kernel/watchdog.c|134| <<is_hardlockup>> if (per_cpu(hrtimer_interrupts_saved, cpu) == hrint)
+ *   - kernel/watchdog.c|142| <<is_hardlockup>> per_cpu(hrtimer_interrupts_saved, cpu) = hrint;
+ */
 static DEFINE_PER_CPU(int, hrtimer_interrupts_saved);
 static DEFINE_PER_CPU(bool, watchdog_hardlockup_warned);
 static DEFINE_PER_CPU(bool, watchdog_hardlockup_touched);
@@ -111,10 +153,28 @@ void watchdog_hardlockup_touch_cpu(unsigned int cpu)
 	per_cpu(watchdog_hardlockup_touched, cpu) = true;
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|176| <<watchdog_hardlockup_check>> if (is_hardlockup(cpu)) {
+ */
 static bool is_hardlockup(unsigned int cpu)
 {
+	/*
+	 * 在以下使用percpu的hrtimer_interrupts:
+	 *   - kernel/watchdog.c|90| <<global>> static DEFINE_PER_CPU(atomic_t, hrtimer_interrupts);
+	 *   - kernel/watchdog.c|116| <<is_hardlockup>> int hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));
+	 *   - kernel/watchdog.c|135| <<watchdog_hardlockup_kick>> new_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));
+	 *   - kernel/watchdog_buddy.c|88| <<watchdog_buddy_check_hardlockup>> void watchdog_buddy_check_hardlockup(int hrtimer_interrupts)
+	 *   - kernel/watchdog_buddy.c|97| <<watchdog_buddy_check_hardlockup>> if (hrtimer_interrupts % 3 != 0)
+	 */
 	int hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));
 
+	/*
+	 * 在以下使用percpu的hrtimer_interrupts_saved:
+	 *   - kernel/watchdog.c|99| <<global>> static DEFINE_PER_CPU(int , hrtimer_interrupts_saved);
+	 *   - kernel/watchdog.c|134| <<is_hardlockup>> if (per_cpu(hrtimer_interrupts_saved, cpu) == hrint)
+	 *   - kernel/watchdog.c|142| <<is_hardlockup>> per_cpu(hrtimer_interrupts_saved, cpu) = hrint;
+	 */
 	if (per_cpu(hrtimer_interrupts_saved, cpu) == hrint)
 		return true;
 
@@ -128,14 +188,31 @@ static bool is_hardlockup(unsigned int cpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|472| <<watchdog_timer_fn>> watchdog_hardlockup_kick();
+ */
 static void watchdog_hardlockup_kick(void)
 {
 	int new_interrupts;
 
+	/*
+	 * 在以下使用percpu的hrtimer_interrupts:
+	 *   - kernel/watchdog.c|90| <<global>> static DEFINE_PER_CPU(atomic_t, hrtimer_interrupts);
+	 *   - kernel/watchdog.c|116| <<is_hardlockup>> int hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));
+	 *   - kernel/watchdog.c|135| <<watchdog_hardlockup_kick>> new_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));
+	 *   - kernel/watchdog_buddy.c|88| <<watchdog_buddy_check_hardlockup>> void watchdog_buddy_check_hardlockup(int hrtimer_interrupts)
+	 *   - kernel/watchdog_buddy.c|97| <<watchdog_buddy_check_hardlockup>> if (hrtimer_interrupts % 3 != 0)
+	 */
 	new_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));
 	watchdog_buddy_check_hardlockup(new_interrupts);
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog_buddy.c|112| <<watchdog_buddy_check_hardlockup>> watchdog_hardlockup_check(next_cpu, NULL);
+ *   - kernel/watchdog_perf.c|104| <<watchdog_overflow_callback>> watchdog_hardlockup_check(smp_processor_id(), regs);
+ */
 void watchdog_hardlockup_check(unsigned int cpu, struct pt_regs *regs)
 {
 	if (per_cpu(watchdog_hardlockup_touched, cpu)) {
@@ -243,6 +320,12 @@ void __weak watchdog_hardlockup_start(void) { }
  * Caller needs to make sure that the hard watchdogs are off, so this
  * can't race with watchdog_hardlockup_disable().
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|692| <<__lockup_detector_reconfigure>> lockup_detector_update_enable();
+ *   - kernel/watchdog.c|721| <<lockup_detector_setup>> lockup_detector_update_enable();
+ *   - kernel/watchdog.c|738| <<__lockup_detector_reconfigure>> lockup_detector_update_enable();
+ */
 static void lockup_detector_update_enable(void)
 {
 	watchdog_enabled = 0;
@@ -328,6 +411,10 @@ static unsigned long get_timestamp(void)
 	return running_clock() >> 30LL;  /* 2^30 ~= 10^9 */
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|683| <<__lockup_detector_reconfigure>> set_sample_period();
+ */
 static void set_sample_period(void)
 {
 	/*
@@ -434,6 +521,10 @@ static int softlockup_fn(void *data)
 	return 0;
 }
 
+/*
+ * 在以下使用watchdog_timer_fn():
+ *   - kernel/watchdog.c|601| <<watchdog_enable>> hrtimer->function = watchdog_timer_fn;
+ */
 /* watchdog kicker functions */
 static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)
 {
@@ -556,6 +647,11 @@ static void watchdog_enable(unsigned int cpu)
 		watchdog_hardlockup_enable(cpu);
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|626| <<softlockup_stop_fn>> watchdog_disable(smp_processor_id());
+ *   - kernel/watchdog.c|668| <<lockup_detector_offline_cpu>> watchdog_disable(cpu);
+ */
 static void watchdog_disable(unsigned int cpu)
 {
 	struct hrtimer *hrtimer = this_cpu_ptr(&watchdog_hrtimer);
@@ -591,12 +687,37 @@ static void softlockup_stop_all(void)
 	cpumask_clear(&watchdog_allowed_mask);
 }
 
+/*
+ * 在以下使用softlockup_start_fn():
+ *   - kernel/watchdog.c|702| <<softlockup_start_all>> smp_call_on_cpu(cpu, softlockup_start_fn, NULL, false);
+ */
 static int softlockup_start_fn(void *data)
 {
 	watchdog_enable(smp_processor_id());
 	return 0;
 }
 
+/*
+ * [0] softlockup_start_all
+ * [0] __lockup_detector_reconfigure
+ * [0] lockup_detector_init
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * [0] softlockup_start_all
+ * [0] __lockup_detector_reconfigure
+ * [0] proc_watchdog_common
+ * [0] proc_sys_call_handler
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用softlockup_start_all():
+ *   - kernel/watchdog.c|728| <<__lockup_detector_reconfigure>> softlockup_start_all();
+ */
 static void softlockup_start_all(void)
 {
 	int cpu;
@@ -620,6 +741,19 @@ int lockup_detector_offline_cpu(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * [0] __lockup_detector_reconfigure
+ * [0] lockup_detector_init
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - kernel/watchdog.c|763| <<lockup_detector_reconfigure>> __lockup_detector_reconfigure();
+ *   - kernel/watchdog.c|783| <<lockup_detector_setup>> __lockup_detector_reconfigure();
+ *   - kernel/watchdog.c|849| <<proc_watchdog_update>> __lockup_detector_reconfigure();
+ */
 static void __lockup_detector_reconfigure(void)
 {
 	cpus_read_lock();
@@ -631,6 +765,10 @@ static void __lockup_detector_reconfigure(void)
 	if (watchdog_enabled && watchdog_thresh)
 		softlockup_start_all();
 
+	/*
+	 * 似乎是这里启动
+	 * 好吧, 不是 :)
+	 */
 	watchdog_hardlockup_start();
 	cpus_read_unlock();
 	/*
@@ -650,6 +788,11 @@ void lockup_detector_reconfigure(void)
 /*
  * Create the watchdog infrastructure and configure the detector(s).
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|1109| <<lockup_detector_delay_init>> lockup_detector_setup();
+ *   - kernel/watchdog.c|1168| <<lockup_detector_init>> lockup_detector_setup();
+ */
 static __init void lockup_detector_setup(void)
 {
 	/*
@@ -718,6 +861,12 @@ void lockup_detector_soft_poweroff(void)
 
 #ifdef CONFIG_SYSCTL
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|863| <<proc_watchdog_common>> proc_watchdog_update();
+ *   - kernel/watchdog.c|916| <<proc_watchdog_thresh>> proc_watchdog_update();
+ *   - kernel/watchdog.c|937| <<proc_watchdog_cpumask>> proc_watchdog_update();
+ */
 /* Propagate any changes to the watchdog infrastructure */
 static void proc_watchdog_update(void)
 {
@@ -738,6 +887,12 @@ static void proc_watchdog_update(void)
  * -------------------|----------------------------------|-------------------------------
  * proc_soft_watchdog | watchdog_softlockup_user_enabled | WATCHDOG_SOFTOCKUP_ENABLED
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|875| <<proc_watchdog>> return proc_watchdog_common(WATCHDOG_HARDLOCKUP_ENABLED |
+ *   - kernel/watchdog.c|888| <<proc_nmi_watchdog>> return proc_watchdog_common(WATCHDOG_HARDLOCKUP_ENABLED,
+ *   - kernel/watchdog.c|898| <<proc_soft_watchdog>> return proc_watchdog_common(WATCHDOG_SOFTOCKUP_ENABLED,
+ */
 static int proc_watchdog_common(int which, struct ctl_table *table, int write,
 				void *buffer, size_t *lenp, loff_t *ppos)
 {
@@ -949,9 +1104,20 @@ static void __init watchdog_sysctl_init(void)
 static void __init lockup_detector_delay_init(struct work_struct *work);
 static bool allow_lockup_detector_init_retry __initdata;
 
+/*
+ * 在以下使用detector_work:
+ *   - kernel/watchdog.c|1032| <<global>> static struct work_struct detector_work __initdata =
+ *   - kernel/watchdog.c|1033| <<global>> __WORK_INITIALIZER(detector_work, lockup_detector_delay_init);
+ *   - kernel/watchdog.c|1065| <<lockup_detector_retry_init>> schedule_work(&detector_work);
+ *   - kernel/watchdog.c|1078| <<lockup_detector_check>> flush_work(&detector_work);
+ */
 static struct work_struct detector_work __initdata =
 		__WORK_INITIALIZER(detector_work, lockup_detector_delay_init);
 
+/*
+ * 在以下使用lockup_detector_delay_init():
+ *   - kernel/watchdog.c|1033| <<global>> __WORK_INITIALIZER(detector_work, lockup_detector_delay_init);
+ */
 static void __init lockup_detector_delay_init(struct work_struct *work)
 {
 	int ret;
@@ -989,6 +1155,10 @@ void __init lockup_detector_retry_init(void)
  * Ensure that optional delayed hardlockup init is proceed before
  * the init code and memory is freed.
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|1085| <<global>> late_initcall_sync(lockup_detector_check);
+ */
 static int __init lockup_detector_check(void)
 {
 	/* Prevent any later retry. */
@@ -1004,6 +1174,10 @@ static int __init lockup_detector_check(void)
 }
 late_initcall_sync(lockup_detector_check);
 
+/*
+ * called by:
+ *   - init/main.c|1538| <<kernel_init_freeable>> lockup_detector_init();
+ */
 void __init lockup_detector_init(void)
 {
 	if (tick_nohz_full_enabled())
diff --git a/kernel/watchdog_perf.c b/kernel/watchdog_perf.c
index 8ea00c4a2..42f84e632 100644
--- a/kernel/watchdog_perf.c
+++ b/kernel/watchdog_perf.c
@@ -20,17 +20,62 @@
 #include <asm/irq_regs.h>
 #include <linux/perf_event.h>
 
+/*
+ * 在以下使用percpu的watchdog_ev:
+ *   - kernel/watchdog_perf.c|23| <<global>> static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
+ *   - kernel/watchdog_perf.c|163| <<hardlockup_detector_event_create>> this_cpu_write(watchdog_ev, evt);
+ *   - kernel/watchdog_perf.c|187| <<watchdog_hardlockup_enable>> perf_event_enable(this_cpu_read(watchdog_ev));
+ *   - kernel/watchdog_perf.c|197| <<watchdog_hardlockup_disable>> struct perf_event *event = this_cpu_read(watchdog_ev);
+ *   - kernel/watchdog_perf.c|203| <<watchdog_hardlockup_disable>> this_cpu_write(watchdog_ev, NULL);
+ *   - kernel/watchdog_perf.c|245| <<hardlockup_detector_perf_stop>> struct perf_event *event = per_cpu(watchdog_ev, cpu);
+ *   - kernel/watchdog_perf.c|271| <<hardlockup_detector_perf_restart>> struct perf_event *event = per_cpu(watchdog_ev, cpu);
+ *   - kernel/watchdog_perf.c|303| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+ *   - kernel/watchdog_perf.c|304| <<watchdog_hardlockup_probe>> this_cpu_write(watchdog_ev, NULL);
+ */
 static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
+/*
+ * 在以下使用percpu的dead_event:
+ *   - kernel/watchdog_perf.c|24| <<global>> static DEFINE_PER_CPU(struct perf_event *, dead_event);
+ *   - kernel/watchdog_perf.c|189| <<watchdog_hardlockup_disable>> this_cpu_write(dead_event, event);
+ *   - kernel/watchdog_perf.c|205| <<hardlockup_detector_perf_cleanup>> struct perf_event *event = per_cpu(dead_event, cpu);
+ *   - kernel/watchdog_perf.c|213| <<hardlockup_detector_perf_cleanup>> per_cpu(dead_event, cpu) = NULL;
+ */
 static DEFINE_PER_CPU(struct perf_event *, dead_event);
+/*
+ * 在以下使用dead_events_mask:
+ *   - kernel/watchdog_perf.c|205| <<watchdog_hardlockup_disable>> cpumask_set_cpu(smp_processor_id(), &dead_events_mask);
+ *   - kernel/watchdog_perf.c|219| <<hardlockup_detector_perf_cleanup>> for_each_cpu(cpu, &dead_events_mask) {
+ *   - kernel/watchdog_perf.c|230| <<hardlockup_detector_perf_cleanup>> cpumask_clear(&dead_events_mask);
+ */
 static struct cpumask dead_events_mask;
 
+/*
+ * 在以下使用watchdog_cpus:
+ *   - kernel/watchdog_perf.c|184| <<watchdog_hardlockup_enable>> if (!atomic_fetch_inc(&watchdog_cpus))
+ *   - kernel/watchdog_perf.c|206| <<watchdog_hardlockup_disable>> atomic_dec(&watchdog_cpus);
+ */
 static atomic_t watchdog_cpus = ATOMIC_INIT(0);
 
 #ifdef CONFIG_HARDLOCKUP_CHECK_TIMESTAMP
 static DEFINE_PER_CPU(ktime_t, last_timestamp);
+/*
+ * 在以下使用percpu的nmi_rearmed:
+ *   - kernel/watchdog_perf.c|61| <<global>> static DEFINE_PER_CPU(unsigned int , nmi_rearmed);
+ *   - kernel/watchdog_perf.c|114| <<watchdog_check_timestamp>> if (__this_cpu_inc_return(nmi_rearmed) < 10)
+ *   - kernel/watchdog_perf.c|117| <<watchdog_check_timestamp>> __this_cpu_write(nmi_rearmed, 0);
+ */
 static DEFINE_PER_CPU(unsigned int, nmi_rearmed);
+/*
+ * 在以下使用watchdog_hrtimer_sample_threshold:
+ *   - kernel/watchdog_perf.c|57| <<watchdog_update_hrtimer_threshold>> watchdog_hrtimer_sample_threshold = period * 2;
+ *   - kernel/watchdog_perf.c|70| <<watchdog_check_timestamp>> if (delta < watchdog_hrtimer_sample_threshold) {
+ */
 static ktime_t watchdog_hrtimer_sample_threshold __read_mostly;
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|390| <<set_sample_period>> watchdog_update_hrtimer_threshold(sample_period);
+ */
 void watchdog_update_hrtimer_threshold(u64 period)
 {
 	/*
@@ -57,6 +102,10 @@ void watchdog_update_hrtimer_threshold(u64 period)
 	watchdog_hrtimer_sample_threshold = period * 2;
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog_perf.c|105| <<watchdog_overflow_callback>> if (!watchdog_check_timestamp())
+ */
 static bool watchdog_check_timestamp(void)
 {
 	ktime_t delta, now = ktime_get_mono_fast_ns();
@@ -82,6 +131,10 @@ static inline bool watchdog_check_timestamp(void)
 }
 #endif
 
+/*
+ * 在以下使用wd_hw_attr:
+ *   - kernel/watchdog_perf.c|123| <<hardlockup_detector_event_create>> wd_attr = &wd_hw_attr;
+ */
 static struct perf_event_attr wd_hw_attr = {
 	.type		= PERF_TYPE_HARDWARE,
 	.config		= PERF_COUNT_HW_CPU_CYCLES,
@@ -90,6 +143,22 @@ static struct perf_event_attr wd_hw_attr = {
 	.disabled	= 1,
 };
 
+/*
+ * [0] nmi_panic
+ * [0] watchdog_overflow_callback
+ * [0] __perf_event_overflow
+ * [0] perf_event_overflow
+ * [0] x86_pmu_handle_irq
+ * [0] amd_pmu_handle_irq
+ * [0] perf_event_nmi_handler
+ * [0] nmi_handle
+ * [0] default_do_nmi
+ * [0] do_nmi
+ * [0] end_repeat_nmi
+ *
+ * called by:
+ *   - kernel/watchdog_perf.c|124| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+ */
 /* Callback function for perf event subsystem */
 static void watchdog_overflow_callback(struct perf_event *event,
 				       struct perf_sample_data *data,
@@ -104,6 +173,11 @@ static void watchdog_overflow_callback(struct perf_event *event,
 	watchdog_hardlockup_check(smp_processor_id(), regs);
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog_perf.c|147| <<watchdog_hardlockup_enable>> if (hardlockup_detector_event_create())
+ *   - kernel/watchdog_perf.c|256| <<watchdog_hardlockup_probe>> ret = hardlockup_detector_event_create();
+ */
 static int hardlockup_detector_event_create(void)
 {
 	unsigned int cpu;
@@ -136,6 +210,50 @@ static int hardlockup_detector_event_create(void)
  *
  * @cpu: The CPU to enable hard lockup on.
  */
+/*
+ * BM启动的时候CPU=0
+ * [0] watchdog_hardlockup_enable
+ * [0] softlockup_start_fn
+ * [0] smp_call_on_cpu_callback
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * BM启动的时候其他的CPU
+ * [0] watchdog_hardlockup_enable
+ * [0] lockup_detector_online_cpu
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * 动态调整echo 1 > /proc/sys/kernel/nmi_watchdog
+ * [0] watchdog_hardlockup_enable
+ * [0] softlockup_start_fn
+ * [0] smp_call_on_cpu_callback
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * cpu hotplug onle的时候
+ * [0] watchdog_hardlockup_enable
+ * [0] lockup_detector_online_cpu
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * called by:
+ *   - kernel/watchdog.c|605| <<watchdog_enable>> watchdog_hardlockup_enable(cpu);
+ */
 void watchdog_hardlockup_enable(unsigned int cpu)
 {
 	WARN_ON_ONCE(cpu != smp_processor_id());
@@ -147,6 +265,18 @@ void watchdog_hardlockup_enable(unsigned int cpu)
 	if (!atomic_fetch_inc(&watchdog_cpus))
 		pr_info("Enabled. Permanently consumes one hw-PMU counter.\n");
 
+	/*
+	 * 在以下使用percpu的watchdog_ev:
+	 *   - kernel/watchdog_perf.c|23| <<global>> static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
+	 *   - kernel/watchdog_perf.c|163| <<hardlockup_detector_event_create>> this_cpu_write(watchdog_ev, evt);
+	 *   - kernel/watchdog_perf.c|187| <<watchdog_hardlockup_enable>> perf_event_enable(this_cpu_read(watchdog_ev));
+	 *   - kernel/watchdog_perf.c|197| <<watchdog_hardlockup_disable>> struct perf_event *event = this_cpu_read(watchdog_ev);
+	 *   - kernel/watchdog_perf.c|203| <<watchdog_hardlockup_disable>> this_cpu_write(watchdog_ev, NULL);
+	 *   - kernel/watchdog_perf.c|245| <<hardlockup_detector_perf_stop>> struct perf_event *event = per_cpu(watchdog_ev, cpu);
+	 *   - kernel/watchdog_perf.c|271| <<hardlockup_detector_perf_restart>> struct perf_event *event = per_cpu(watchdog_ev, cpu);
+	 *   - kernel/watchdog_perf.c|303| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+	 *   - kernel/watchdog_perf.c|304| <<watchdog_hardlockup_probe>> this_cpu_write(watchdog_ev, NULL);
+	 */
 	perf_event_enable(this_cpu_read(watchdog_ev));
 }
 
@@ -175,10 +305,20 @@ void watchdog_hardlockup_disable(unsigned int cpu)
  *
  * Called from lockup_detector_cleanup(). Serialized by the caller.
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|755| <<__lockup_detector_cleanup>> hardlockup_detector_perf_cleanup();
+ */
 void hardlockup_detector_perf_cleanup(void)
 {
 	int cpu;
 
+	/*
+	 * 在以下使用dead_events_mask:
+	 *   - kernel/watchdog_perf.c|205| <<watchdog_hardlockup_disable>> cpumask_set_cpu(smp_processor_id(), &dead_events_mask);
+	 *   - kernel/watchdog_perf.c|219| <<hardlockup_detector_perf_cleanup>> for_each_cpu(cpu, &dead_events_mask) {
+	 *   - kernel/watchdog_perf.c|230| <<hardlockup_detector_perf_cleanup>> cpumask_clear(&dead_events_mask);
+	 */
 	for_each_cpu(cpu, &dead_events_mask) {
 		struct perf_event *event = per_cpu(dead_event, cpu);
 
@@ -217,6 +357,10 @@ void __init hardlockup_detector_perf_stop(void)
  *
  * Special interface for x86 to handle the perf HT bug.
  */
+/*
+ * called by:
+ *   - arch/x86/events/intel/core.c|6890| <<fixup_ht_bug>> hardlockup_detector_perf_restart();
+ */
 void __init hardlockup_detector_perf_restart(void)
 {
 	int cpu;
@@ -242,6 +386,11 @@ bool __weak __init arch_perf_nmi_is_available(void)
 /**
  * watchdog_hardlockup_probe - Probe whether NMI event is available at all
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|1017| <<lockup_detector_delay_init>> ret = watchdog_hardlockup_probe();
+ *   - kernel/watchdog.c|1073| <<lockup_detector_init>> if (!watchdog_hardlockup_probe())
+ */
 int __init watchdog_hardlockup_probe(void)
 {
 	int ret;
diff --git a/mm/filemap.c b/mm/filemap.c
index f0a15ce1b..7369f34ea 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -3698,6 +3698,9 @@ static struct folio *do_read_cache_folio(struct address_space *mapping,
 	struct folio *folio;
 	int err;
 
+	/*
+	 * 比如blkdev_read_folio()
+	 */
 	if (!filler)
 		filler = mapping->a_ops->read_folio;
 repeat:
@@ -3811,6 +3814,14 @@ static struct page *do_read_cache_page(struct address_space *mapping,
 	return folio_file_page(folio, index);
 }
 
+/*
+ * called by:
+ *   - fs/f2fs/file.c|4032| <<redirty_blocks>> page = read_cache_page(mapping, page_idx, NULL, NULL);
+ *   - fs/gfs2/aops.c|493| <<gfs2_internal_read>> page = read_cache_page(mapping, index, gfs2_read_folio, NULL);
+ *   - fs/jffs2/gc.c|1329| <<jffs2_garbage_collect_dnode>> page = read_cache_page(inode->i_mapping, start >> PAGE_SHIFT,
+ *   - fs/nfs/symlink.c|69| <<nfs_get_link>> page = read_cache_page(&inode->i_data, 0, nfs_symlink_filler,
+ *   - include/linux/pagemap.h|860| <<read_mapping_page>> return read_cache_page(mapping, index, NULL, file);
+ */
 struct page *read_cache_page(struct address_space *mapping,
 			pgoff_t index, filler_t *filler, struct file *file)
 {
diff --git a/tools/testing/selftests/kvm/include/guest_modes.h b/tools/testing/selftests/kvm/include/guest_modes.h
index b691df33e..102c0c970 100644
--- a/tools/testing/selftests/kvm/include/guest_modes.h
+++ b/tools/testing/selftests/kvm/include/guest_modes.h
@@ -11,6 +11,24 @@ struct guest_mode {
 
 extern struct guest_mode guest_modes[NUM_VM_MODES];
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|17| <<guest_modes_append_default>> guest_mode_append(VM_MODE_DEFAULT, true, true);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|29| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P52V48_64K, ps64k, ps64k);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|31| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P48V48_4K, ps4k, ps4k);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|32| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P48V48_16K, ps16k, ps16k);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|33| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P48V48_64K, ps64k, ps64k);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|36| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P40V48_4K, ps4k, ps4k);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|37| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P40V48_16K, ps16k, ps16k);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|38| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P40V48_64K, ps64k, ps64k);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|43| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P36V48_4K, ps4k, ps4k);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|44| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P36V48_16K, ps16k, ps16k);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|45| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P36V48_64K, ps64k, ps64k);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|46| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P36V47_16K, ps16k, ps16k);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|75| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P47V64_4K, true, true);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|83| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P52V48_4K, true, true);
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|85| <<guest_modes_append_default>> guest_mode_append(VM_MODE_P48V48_4K, true, true);
+ */
 #define guest_mode_append(mode, supported, enabled) ({ \
 	guest_modes[mode] = (struct guest_mode){ supported, enabled }; \
 })
diff --git a/tools/testing/selftests/kvm/include/kvm_util_base.h b/tools/testing/selftests/kvm/include/kvm_util_base.h
index a18db6a7b..ec06def26 100644
--- a/tools/testing/selftests/kvm/include/kvm_util_base.h
+++ b/tools/testing/selftests/kvm/include/kvm_util_base.h
@@ -45,6 +45,17 @@ typedef uint64_t vm_vaddr_t; /* Virtual Machine (Guest) virtual address */
 
 struct userspace_mem_region {
 	struct kvm_userspace_memory_region region;
+	/*
+	 * 在以下使用unused_phy_pages:
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|819| <<__vm_mem_region_delete>> sparsebit_free(&region->unused_phy_pages);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1175| <<vm_userspace_mem_region_add>> region->unused_phy_pages = sparsebit_alloc();
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1176| <<vm_userspace_mem_region_add>> sparsebit_set_num(region->unused_phy_pages, guest_paddr >> vm->page_shift, npages);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|2043| <<vm_dump>> sparsebit_dump(stream, region->unused_phy_pages, 0);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|2190| <<vm_phy_pages_alloc>> if (!sparsebit_is_set(region->unused_phy_pages, pg)) {
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|2195| <<vm_phy_pages_alloc>> base = pg = sparsebit_next_set(region->unused_phy_pages, pg);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|2211| <<vm_phy_pages_alloc>> sparsebit_clear(region->unused_phy_pages, pg);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/vmx.c|509| <<nested_map_memslot>> i = sparsebit_next_clear(region->unused_phy_pages, i);
+	 */
 	struct sparsebit *unused_phy_pages;
 	int fd;
 	off_t offset;
@@ -79,6 +90,16 @@ struct userspace_mem_regions {
 	DECLARE_HASHTABLE(slot_hash, 9);
 };
 
+/*
+ * 在以下使用MEM_REGION_PT:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|370| <<setup_uffd>> setup_uffd_args(vm_get_mem_region(vm, MEM_REGION_PT), &pt_args);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|475| <<handle_cmd>> pt_region = vm_get_mem_region(vm, MEM_REGION_PT);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|607| <<setup_memslots>> vm->memslots[MEM_REGION_PT] = PAGE_TABLE_MEMSLOT;
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|104| <<virt_arch_pgd_alloc>> vm->memslots[MEM_REGION_PT]);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|2212| <<vm_alloc_page_table>> vm->memslots[MEM_REGION_PT]);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|65| <<virt_arch_pgd_alloc>> vm->memslots[MEM_REGION_PT]);
+ *   - tools/testing/selftests/kvm/lib/s390x/processor.c|25| <<virt_arch_pgd_alloc>> vm->memslots[MEM_REGION_PT]);
+ */
 enum kvm_mem_region_type {
 	MEM_REGION_CODE,
 	MEM_REGION_DATA,
@@ -97,6 +118,29 @@ struct kvm_vm {
 	unsigned int page_shift;
 	unsigned int pa_bits;
 	unsigned int va_bits;
+	/*
+	 * 在以下使用kvm_vm->max_gfn:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|752| <<run_test>> guest_test_phys_mem = (vm->max_gfn - guest_num_pages) * guest_page_size;
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|359| <<pre_init_before_test>> guest_test_phys_mem = (vm->max_gfn - guest_num_pages) * guest_page_size;
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|123| <<_virt_pg_map>> TEST_ASSERT((paddr >> vm->page_shift) <= vm->max_gfn,
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|126| <<_virt_pg_map>> TEST_ASSERT( ... paddr, vm->max_gfn, vm->page_size);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|337| <<____vm_create>> vm->max_gfn = vm_compute_max_gfn(vm);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1060| <<vm_userspace_mem_region_add>> TEST_ASSERT( ... <= vm->max_gfn, "Physical range beyond maximum "
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1064| <<vm_userspace_mem_region_add>> TEST_ASSERT( ... guest_paddr, npages, vm->max_gfn, vm->page_size);
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|177| <<memstress_create_vm>> region_end_gfn = vm->max_gfn + 1;
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|83| <<virt_arch_pg_map>> TEST_ASSERT((paddr >> vm->page_shift) <= vm->max_gfn,
+	 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|86| <<virt_arch_pg_map>> TEST_ASSERT( ... paddr, vm->max_gfn, vm->page_size);
+	 *   - tools/testing/selftests/kvm/lib/s390x/processor.c|67| <<virt_arch_pg_map>> TEST_ASSERT((gpa >> vm->page_shift) <= vm->max_gfn,
+	 *   - tools/testing/selftests/kvm/lib/s390x/processor.c|70| <<virt_arch_pg_map>> TEST_ASSERT( ... gva, vm->max_gfn, vm->page_size);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|199| <<__virt_pg_map>> TEST_ASSERT((paddr >> vm->page_shift) <= vm->max_gfn,
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|202| <<__virt_pg_map>> TEST_ASSERT( ... paddr, vm->max_gfn, vm->page_size);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/vmx.c|416| <<__nested_pg_map>> TEST_ASSERT((nested_paddr >> vm->page_shift) <= vm->max_gfn,
+	 *   - tools/testing/selftests/kvm/lib/x86_64/vmx.c|419| <<__nested_pg_map>> TEST_ASSERT( ... paddr, vm->max_gfn, vm->page_size);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/vmx.c|424| <<__nested_pg_map>> TEST_ASSERT((paddr >> vm->page_shift) <= vm->max_gfn,
+	 *   - tools/testing/selftests/kvm/lib/x86_64/vmx.c|427| <<__nested_pg_map>> TEST_ASSERT( ... paddr, vm->max_gfn, vm->page_size);
+	 *   - tools/testing/selftests/kvm/max_guest_memory_test.c|219| <<main>> max_gpa = vm->max_gfn << vm->page_shift;
+	 *   - tools/testing/selftests/kvm/x86_64/vmx_apic_access_test.c|87| <<main>> high_gpa = (vm->max_gfn - 1) << vm->page_shift;
+	 */
 	uint64_t max_gfn;
 	struct list_head vcpus;
 	struct userspace_mem_regions regions;
@@ -105,6 +149,24 @@ struct kvm_vm {
 	bool has_irqchip;
 	bool pgd_created;
 	vm_paddr_t ucall_mmio_addr;
+	/*
+	 * 有很多使用, aarch64和x86_64在以下使用kvm_vm->pgd:
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|102| <<virt_arch_pgd_alloc>> vm->pgd = vm_phy_pages_alloc(vm, nr_pages,
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|128| <<_virt_pg_map>> ptep = addr_gpa2hva(vm, vm->pgd) + pgd_index(vm, vaddr) * 8;
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|167| <<virt_get_pte_hva>> ptep = addr_gpa2hva(vm, vm->pgd) + pgd_index(vm, gva) * 8;
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|227| <<virt_arch_dump>> uint64_t pgd, *ptep;
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|232| <<virt_arch_dump>> for (pgd = vm->pgd; pgd < vm->pgd + ptrs_per_pgd(vm) * 8; pgd += 8) {
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|233| <<virt_arch_dump>> ptep = addr_gpa2hva(vm, pgd);
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|236| <<virt_arch_dump>> fprintf(stream, "%*spgd: %lx: %lx at %p\n", indent, "", pgd, *ptep, ptep);
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|296| <<aarch64_vcpu_setup>> ttbr0_el1 = vm->pgd & GENMASK(47, vm->page_shift);
+	 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|302| <<aarch64_vcpu_setup>> ttbr0_el1 |= FIELD_GET(GENMASK(51, 48), vm->pgd) << 2;
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|132| <<virt_arch_pgd_alloc>> vm->pgd = vm_alloc_page_table(vm);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|153| <<virt_get_pte>> TEST_ASSERT((*parent_pte & PTE_PRESENT_MASK) || parent_pte == &vm->pgd,
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|257| <<__virt_pg_map>> pml4e = virt_create_upper_pte(vm, &vm->pgd, vaddr, paddr, PG_LEVEL_512G, level);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|338| <<__vm_get_page_table_entry>> pml4e = virt_get_pte(vm, &vm->pgd, vaddr, PG_LEVEL_512G);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|375| <<virt_arch_dump>> pml4e_start = (uint64_t *) addr_gpa2hva(vm, vm->pgd);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|608| <<vcpu_setup>> sregs.cr3 = vm->pgd;
+	 */
 	vm_paddr_t pgd;
 	vm_vaddr_t gdt;
 	vm_vaddr_t tss;
@@ -717,13 +779,51 @@ struct kvm_vm *____vm_create(enum vm_guest_mode mode);
 struct kvm_vm *__vm_create(enum vm_guest_mode mode, uint32_t nr_runnable_vcpus,
 			   uint64_t nr_extra_pages);
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/smccc_filter.c|247| <<kvm_supports_smccc_filter>> struct kvm_vm *vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/aarch64/vcpu_width_config.c|28| <<add_init_2vcpus>> vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/aarch64/vcpu_width_config.c|54| <<add_2vcpus_init_2vcpus>> vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/aarch64/vcpu_width_config.c|88| <<main>> vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/get-reg-list.c|173| <<run_test>> vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/kvm_binary_stats_test.c|229| <<main>> vms[i] = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/kvm_create_max_vcpus.c|31| <<test_vcpu_creation>> vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|318| <<test_zero_memory_regions>> vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|354| <<test_add_max_memory_regions>> vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/x86_64/max_vcpuid_cap_test.c|19| <<main>> vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/x86_64/set_sregs_test.c|89| <<main>> vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/x86_64/sev_migrate_tests.c|56| <<sev_vm_create>> vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/x86_64/sev_migrate_tests.c|73| <<aux_vm_create>> vm = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/x86_64/sev_migrate_tests.c|171| <<test_sev_migrate_parameters>> vm_no_vcpu = vm_create_barebones();
+ *   - tools/testing/selftests/kvm/x86_64/sev_migrate_tests.c|183| <<test_sev_migrate_parameters>> sev_es_vm_no_vmsa = vm_create_barebones();
+ */
 static inline struct kvm_vm *vm_create_barebones(void)
 {
 	return ____vm_create(VM_MODE_DEFAULT);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/psci_test.c|81| <<setup_vm>> vm = vm_create(2);
+ *   - tools/testing/selftests/kvm/aarch64/smccc_filter.c|66| <<setup_vm>> vm = vm_create(1);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|420| <<test_v3_typer_accesses>> v.vm = vm_create(NR_VCPUS);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|480| <<vm_gic_v3_create_with_vcpuids>> v.vm = vm_create(nr_vcpus);
+ *   - tools/testing/selftests/kvm/hardware_disable_test.c|101| <<run_test>> vm = vm_create(VCPU_NUM);
+ *   - tools/testing/selftests/kvm/s390x/resets.c|209| <<create_vm>> vm = vm_create(1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|117| <<run_test>> vm = vm_create(1);
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|387| <<test_pmu_config_disable>> vm = vm_create(1);
+ *   - tools/testing/selftests/kvm/x86_64/set_boot_cpu_id.c|82| <<create_vm>> vm = vm_create(nr_vcpus);
+ *   - tools/testing/selftests/kvm/x86_64/tsc_scaling_sync.c|90| <<main>> vm = vm_create(NR_TEST_VCPUS);
+ */
 static inline struct kvm_vm *vm_create(uint32_t nr_runnable_vcpus)
 {
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/dirty_log_test.c|702| <<create_vm>> vm = __vm_create(mode, 1, extra_mem_pages);
+	 *   - tools/testing/selftests/kvm/include/kvm_util_base.h|727| <<vm_create>> return __vm_create(VM_MODE_DEFAULT, nr_runnable_vcpus, 0);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|412| <<__vm_create_with_vcpus>> vm = __vm_create(mode, nr_vcpus, extra_mem_pages);
+	 *   - tools/testing/selftests/kvm/x86_64/ucna_injection_test.c|274| <<main>> vm = __vm_create(VM_MODE_DEFAULT, 3, 0);
+	 */
 	return __vm_create(VM_MODE_DEFAULT, nr_runnable_vcpus, 0);
 }
 
@@ -731,10 +831,27 @@ struct kvm_vm *__vm_create_with_vcpus(enum vm_guest_mode mode, uint32_t nr_vcpus
 				      uint64_t extra_mem_pages,
 				      void *guest_code, struct kvm_vcpu *vcpus[]);
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/arch_timer.c|378| <<test_vm_create>> vm = vm_create_with_vcpus(nr_vcpus, guest_code, vcpus);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|81| <<vm_gic_create_with_vcpus>> v.vm = vm_create_with_vcpus(nr_vcpus, guest_code, vcpus);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|643| <<test_kvm_device>> v.vm = vm_create_with_vcpus(NR_VCPUS, guest_code, vcpus);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|217| <<main>> vm = vm_create_with_vcpus(nr_vcpus, guest_code, vcpus);
+ *   - tools/testing/selftests/kvm/steal_time.c|265| <<main>> vm = vm_create_with_vcpus(NR_VCPUS, guest_code, vcpus);
+ *   - tools/testing/selftests/kvm/x86_64/recalc_apic_map_test.c|51| <<main>> vm = vm_create_with_vcpus(KVM_MAX_VCPUS, NULL, vcpus);
+ *   - tools/testing/selftests/kvm/x86_64/xapic_state_test.c|169| <<test_apic_id>> vm = vm_create_with_vcpus(NR_VCPUS, NULL, vcpus);
+ */
 static inline struct kvm_vm *vm_create_with_vcpus(uint32_t nr_vcpus,
 						  void *guest_code,
 						  struct kvm_vcpu *vcpus[])
 {
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/include/kvm_util_base.h|738| <<vm_create_with_vcpus>> return __vm_create_with_vcpus(VM_MODE_DEFAULT, nr_vcpus, 0, guest_code, vcpus);
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|354| <<pre_init_before_test>> vm = __vm_create_with_vcpus(mode, nr_vcpus, guest_num_pages, guest_code, test_args.vcpus);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|503| <<__vm_create_with_one_vcpu>> vm = __vm_create_with_vcpus(VM_MODE_DEFAULT, 1, extra_mem_pages, guest_code, vcpus);
+	 *   - tools/testing/selftests/kvm/lib/memstress.c|171| <<memstress_create_vm>> vm = __vm_create_with_vcpus(mode, nr_vcpus, slot0_pages + guest_num_pages, memstress_guest_code, vcpus);
+	 */
 	return __vm_create_with_vcpus(VM_MODE_DEFAULT, nr_vcpus, 0,
 				      guest_code, vcpus);
 }
@@ -780,11 +897,50 @@ struct kvm_userspace_memory_region *
 kvm_userspace_memory_region_find(struct kvm_vm *vm, uint64_t start,
 				 uint64_t end);
 
+/*
+ * called by;
+ *   - tools/testing/selftests/kvm/aarch64/arch_timer.c|364| <<test_init_timer_irq>> sync_global_to_guest(vm, ptimer_irq);
+ *   - tools/testing/selftests/kvm/aarch64/arch_timer.c|365| <<test_init_timer_irq>> sync_global_to_guest(vm, vtimer_irq);
+ *   - tools/testing/selftests/kvm/aarch64/arch_timer.c|398| <<test_vm_create>> sync_global_to_guest(vm, test_args);
+ *   - tools/testing/selftests/kvm/aarch64/hypercalls.c|253| <<test_guest_stage>> sync_global_to_guest(*vm, stage);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|783| <<run_test>> sync_global_to_guest(vm, host_page_size);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|784| <<run_test>> sync_global_to_guest(vm, guest_page_size);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|785| <<run_test>> sync_global_to_guest(vm, guest_test_virt_mem);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|786| <<run_test>> sync_global_to_guest(vm, guest_num_pages);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|790| <<run_test>> sync_global_to_guest(vm, iteration);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|825| <<run_test>> sync_global_to_guest(vm, iteration);
+ *   - tools/testing/selftests/kvm/guest_print_test.c|59| <<BUILD_TYPE_STRINGS_AND_HELPER>> sync_global_to_guest(vcpu->vm, vals); \
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|398| <<pre_init_before_test>> sync_global_to_guest(vm, test_args);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|229| <<memstress_create_vm>> sync_global_to_guest(vm, memstress_args);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|242| <<memstress_set_write_percent>> sync_global_to_guest(vm, memstress_args.write_percent);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|248| <<memstress_set_random_seed>> sync_global_to_guest(vm, memstress_args.random_seed);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|254| <<memstress_set_random_access>> sync_global_to_guest(vm, memstress_args.random_access);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|561| <<kvm_arch_vm_post_create>> sync_global_to_guest(vm, host_cpu_is_intel);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|562| <<kvm_arch_vm_post_create>> sync_global_to_guest(vm, host_cpu_is_amd);
+ *   - tools/testing/selftests/kvm/steal_time.c|72| <<steal_time_init>> sync_global_to_guest(vcpu->vm, st_gva[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|183| <<steal_time_init>> sync_global_to_guest(vm, st_gva[i]);
+ *   - tools/testing/selftests/kvm/x86_64/fix_hypercall_test.c|122| <<test_fix_hypercall>> sync_global_to_guest(vm, quirk_disabled);
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|222| <<run_vcpu_and_sync_pmc_results>> sync_global_to_guest(vcpu->vm, pmc_results);
+ *
+ * 从gva拷贝到转换后的hva
+ * 难道是类似:从当前的space拷贝到qemu的space?
+ */
 #define sync_global_to_guest(vm, g) ({				\
 	typeof(g) *_p = addr_gva2hva(vm, (vm_vaddr_t)&(g));	\
 	memcpy(_p, &(g), sizeof(g));				\
 })
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/arch_timer.c|245| <<test_vcpu_run>> sync_global_from_guest(vm, *shared_data);
+ *   - tools/testing/selftests/kvm/steal_time.c|283| <<main>> sync_global_from_guest(vm, guest_stolen_time[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|304| <<main>> sync_global_from_guest(vm, guest_stolen_time[i]);
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|227| <<run_vcpu_and_sync_pmc_results>> sync_global_from_guest(vcpu->vm, pmc_results);
+ *
+ * 从转换后的hva拷贝到gva
+ * 难道是类似:从qemu的space拷贝到当前的space?
+ * 然后才能在test program访问?
+ */
 #define sync_global_from_guest(vm, g) ({			\
 	typeof(g) *_p = addr_gva2hva(vm, (vm_vaddr_t)&(g));	\
 	memcpy(&(g), _p, sizeof(g));				\
@@ -796,6 +952,13 @@ kvm_userspace_memory_region_find(struct kvm_vm *vm, uint64_t start,
  * data into their own region of physical memory), but can be used anytime it's
  * undesirable to change the host's copy of the global.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/aarch64/ucall.c|19| <<ucall_arch_init>> write_guest_global(vm, ucall_exit_mmio_addr, (vm_vaddr_t *)mmio_gva);
+ *   - tools/testing/selftests/kvm/lib/ucall_common.c|41| <<ucall_init>> write_guest_global(vm, ucall_pool, (struct ucall_header *)vaddr);
+ *
+ * 从参数的val写入到hva-of-g
+ */
 #define write_guest_global(vm, g, val) ({			\
 	typeof(g) *_p = addr_gva2hva(vm, (vm_vaddr_t)&(g));	\
 	typeof(g) _val = val;					\
@@ -825,6 +988,37 @@ static inline void vcpu_dump(FILE *stream, struct kvm_vcpu *vcpu,
 struct kvm_vcpu *vm_arch_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id,
 				  void *guest_code);
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|712| <<run_test>> vcpu = vm_vcpu_add(vm, 0, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|335| <<test_vgic_then_vcpus>> vcpus[i] = vm_vcpu_add(v.vm, i, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|421| <<test_v3_typer_accesses>> (void )vm_vcpu_add(v.vm, 0, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|425| <<test_v3_typer_accesses>> (void )vm_vcpu_add(v.vm, 3, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|430| <<test_v3_typer_accesses>> (void )vm_vcpu_add(v.vm, 1, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|435| <<test_v3_typer_accesses>> (void )vm_vcpu_add(v.vm, 2, guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|482| <<vm_gic_v3_create_with_vcpuids>> vm_vcpu_add(v.vm, vcpuids[i], guest_code);
+ *   - tools/testing/selftests/kvm/aarch64/vgic_init.c|578| <<test_v3_redist_ipa_range_check_at_vcpu_run>> vcpus[i] = vm_vcpu_add(v.vm, i, guest_code);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|705| <<create_vm>> *vcpu = vm_vcpu_add(vm, 0, guest_code);
+ *   - tools/testing/selftests/kvm/hardware_disable_test.c|105| <<run_test>> vcpu = vm_vcpu_add(vm, i, guest_code);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|516| <<__vm_create_with_vcpus>> vcpus[i] = vm_vcpu_add(vm, i, guest_code);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|244| <<test_get_cmma_basic>> vcpu = vm_vcpu_add(vm, 1, guest_do_one_essa);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|295| <<test_migration_mode>> vcpu = vm_vcpu_add(vm, 1, guest_do_one_essa);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|458| <<test_get_inital_dirty>> vcpu = vm_vcpu_add(vm, 1, guest_do_one_essa);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|517| <<test_get_skip_holes>> vcpu = vm_vcpu_add(vm, 1, guest_dirty_test_data);
+ *   - tools/testing/selftests/kvm/s390x/resets.c|211| <<create_vm>> *vcpu = vm_vcpu_add(vm, ARBITRARY_NON_ZERO_VCPU_ID, guest_code_initial);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_ipi.c|259| <<main>> vcpu[1] = vm_vcpu_add(vm, RECEIVER_VCPU_ID_1, receiver_code);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_ipi.c|265| <<main>> vcpu[2] = vm_vcpu_add(vm, RECEIVER_VCPU_ID_2, receiver_code);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_tlb_flush.c|636| <<main>> vcpu[1] = vm_vcpu_add(vm, WORKER_VCPU_ID_1, worker_guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_tlb_flush.c|641| <<main>> vcpu[2] = vm_vcpu_add(vm, WORKER_VCPU_ID_2, worker_guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|130| <<run_test>> vcpu = vm_vcpu_add(vm, 0, guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|391| <<test_pmu_config_disable>> vcpu = vm_vcpu_add(vm, 0, guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|949| <<main>> vcpu2 = vm_vcpu_add(vm, 2, intel_masked_events_guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|951| <<main>> vcpu2 = vm_vcpu_add(vm, 2, amd_masked_events_guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/set_boot_cpu_id.c|87| <<create_vm>> vcpus[i] = vm_vcpu_add(vm, i, i == bsp_vcpu_id ? guest_bsp_vcpu :
+ *   - tools/testing/selftests/kvm/x86_64/tsc_scaling_sync.c|53| <<run_vcpu>> vcpu = vm_vcpu_add(vm, vcpu_id, guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/ucna_injection_test.c|259| <<create_vcpu_with_mce_cap>> struct kvm_vcpu *vcpu = vm_vcpu_add(vm, vcpuid, guest_code);
+ *   - tools/testing/selftests/kvm/x86_64/xapic_ipi_test.c|419| <<main>> params[1].vcpu = vm_vcpu_add(vm, 1, sender_guest_code);
+ */
 static inline struct kvm_vcpu *vm_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id,
 					   void *guest_code)
 {
@@ -844,6 +1038,10 @@ void vcpu_arch_free(struct kvm_vcpu *vcpu);
 
 void virt_arch_pgd_alloc(struct kvm_vm *vm);
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1547| <<__vm_vaddr_alloc>> virt_pgd_alloc(vm);
+ */
 static inline void virt_pgd_alloc(struct kvm_vm *vm)
 {
 	virt_arch_pgd_alloc(vm);
@@ -867,6 +1065,18 @@ static inline void virt_pgd_alloc(struct kvm_vm *vm)
  */
 void virt_arch_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr);
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|560| <<setup_gva_maps>> virt_pg_map(vm, TEST_GVA, region->region.guest_phys_addr);
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|563| <<setup_gva_maps>> virt_pg_map(vm, TEST_PTE_GVA, pte_gpa);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1481| <<__vm_vaddr_alloc>> virt_pg_map(vm, vaddr, paddr);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1582| <<virt_map>> virt_pg_map(vm, vaddr, paddr);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|249| <<main>> virt_pg_map(vm, gpa + i, gpa + i);
+ *   - tools/testing/selftests/kvm/x86_64/fix_hypercall_test.c|124| <<test_fix_hypercall>> virt_pg_map(vm, APIC_DEFAULT_GPA, APIC_DEFAULT_GPA);
+ *   - tools/testing/selftests/kvm/x86_64/ucna_injection_test.c|295| <<main>> virt_pg_map(vm, APIC_DEFAULT_GPA, APIC_DEFAULT_GPA);
+ *   - tools/testing/selftests/kvm/x86_64/xapic_ipi_test.c|417| <<main>> virt_pg_map(vm, APIC_DEFAULT_GPA, APIC_DEFAULT_GPA);
+ *   - tools/testing/selftests/kvm/x86_64/xapic_state_test.c|210| <<main>> virt_pg_map(vm, APIC_DEFAULT_GPA, APIC_DEFAULT_GPA);
+ */
 static inline void virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr)
 {
 	virt_arch_pg_map(vm, vaddr, paddr);
diff --git a/tools/testing/selftests/kvm/kvm_page_table_test.c b/tools/testing/selftests/kvm/kvm_page_table_test.c
index 69f26d80c..84aa812cb 100644
--- a/tools/testing/selftests/kvm/kvm_page_table_test.c
+++ b/tools/testing/selftests/kvm/kvm_page_table_test.c
@@ -50,9 +50,32 @@ struct test_args {
 	struct kvm_vm *vm;
 	uint64_t guest_test_virt_mem;
 	uint64_t host_page_size;
+	/*
+	 * 在以下使用test_args->host_num_pages:
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|177| <<guest_code>> for (i = 0; i < p->host_num_pages; i++) {
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|213| <<guest_code>> for (i = 0; i < p->host_num_pages; i++) {
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|345| <<pre_init_before_test>> test_args.host_num_pages = test_mem_size / host_page_size;
+	 */
 	uint64_t host_num_pages;
+	/*
+	 * 在以下使用test_args->large_page_size:
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|122| <<guest_code>> addr += p->large_page_size;
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|155| <<guest_code>> addr += p->large_page_size / 2;
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|276| <<pre_init_before_test>> test_args.large_page_size = large_page_size;
+	 */
 	uint64_t large_page_size;
+	/*
+	 * 在以下使用test_args->large_num_pages:
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|155| <<guest_code>> for (i = 0; i < p->large_num_pages; i++) {
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|184| <<guest_code>> for (i = 0; i < p->large_num_pages; i++) {
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|347| <<pre_init_before_test>> test_args.large_num_pages = test_mem_size / large_page_size;
+	 */
 	uint64_t large_num_pages;
+	/*
+	 * 在以下使用test_args->host_pages_per_lpage:
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|198| <<guest_code>> for (j = 0; j < p->host_pages_per_lpage / 2; j++) {
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|348| <<pre_init_before_test>> test_args.host_pages_per_lpage = large_page_size / host_page_size;
+	 */
 	uint64_t host_pages_per_lpage;
 	enum vm_mem_backing_src_type src_type;
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
@@ -62,14 +85,48 @@ struct test_args {
  * Guest variables. Use addr_gva2hva() if these variables need
  * to be changed in host.
  */
+/*
+ * 在以下使用guest_test_stage:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|71| <<global>> static enum test_stage guest_test_stage;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|132| <<guest_code>> enum test_stage *current_stage = &guest_test_stage;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|379| <<pre_init_before_test>> current_stage = addr_gva2hva(vm, (vm_vaddr_t)(&guest_test_stage));
+ *
+ * 似乎guest和host的variable要分开.
+ * Guest variables. Use addr_gva2hva() if these variables need
+ * to be changed in host.
+ */
 static enum test_stage guest_test_stage;
 
 /* Host variables */
 static uint32_t nr_vcpus = 1;
 static struct test_args test_args;
+/*
+ * 在以下使用current_stage:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|93| <<guest_code>> enum test_stage *current_stage = &guest_test_stage;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|100| <<guest_code>> switch (READ_ONCE(*current_stage)) {
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|213| <<vcpu_worker>> stage = READ_ONCE(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|300| <<pre_init_before_test>> current_stage = addr_gva2hva(vm, (vm_vaddr_t)(&guest_test_stage));
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|301| <<pre_init_before_test>> *current_stage = NUM_TEST_STAGES;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|358| <<run_test>> *current_stage = KVM_BEFORE_MAPPINGS;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|364| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|368| <<run_test>> *current_stage = KVM_CREATE_MAPPINGS;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|371| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|381| <<run_test>> *current_stage = KVM_UPDATE_MAPPINGS;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|384| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|393| <<run_test>> *current_stage = KVM_ADJUST_MAPPINGS;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|396| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ */
 static enum test_stage *current_stage;
 static bool host_quit;
 
+/*
+ * 在以下使用test_stage_updated:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|197| <<vcpu_worker>> ret = sem_wait(&test_stage_updated);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|294| <<pre_init_before_test>> ret = sem_init(&test_stage_updated, 0, 0);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|325| <<vcpus_complete_new_stage>> ret = sem_post(&test_stage_updated);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|405| <<run_test>> ret = sem_post(&test_stage_updated);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|412| <<run_test>> ret = sem_destroy(&test_stage_updated);
+ */
 /* Whether the test stage is updated, or completed */
 static sem_t test_stage_updated;
 static sem_t test_stage_completed;
@@ -90,6 +147,15 @@ static uint64_t guest_test_virt_mem = DEFAULT_GUEST_TEST_MEM;
 static void guest_code(bool do_write)
 {
 	struct test_args *p = &test_args;
+	/*
+	 * enum test_stage {
+	 *     KVM_BEFORE_MAPPINGS,
+	 *     KVM_CREATE_MAPPINGS,
+	 *     KVM_UPDATE_MAPPINGS,
+	 *     KVM_ADJUST_MAPPINGS,
+	 *     NUM_TEST_STAGES,
+	 * };
+	 */
 	enum test_stage *current_stage = &guest_test_stage;
 	uint64_t addr;
 	int i, j;
@@ -131,6 +197,9 @@ static void guest_code(bool do_write)
 		 * is THP or HUGETLB.
 		 */
 		case KVM_UPDATE_MAPPINGS:
+			/*
+			 * 如果是VM_MEM_SRC_ANONYMOUS一定提前break
+			 */
 			if (p->src_type == VM_MEM_SRC_ANONYMOUS) {
 				for (i = 0; i < p->host_num_pages; i++) {
 					*(uint64_t *)addr = 0x0123456789ABCDEF;
@@ -185,15 +254,43 @@ static void guest_code(bool do_write)
 static void *vcpu_worker(void *data)
 {
 	struct kvm_vcpu *vcpu = data;
+	/*
+	 * 0 : true
+	 * 1 : false
+	 * 2 : true
+	 * 3 : false
+	 */
 	bool do_write = !(vcpu->id % 2);
 	struct timespec start;
 	struct timespec ts_diff;
 	enum test_stage stage;
 	int ret;
 
+	/*
+	 * // for KVM_GET_REGS and KVM_SET_REGS
+	 * struct kvm_regs {
+	 *     __u64 rax, rbx, rcx, rdx;
+	 *     __u64 rsi, rdi, rsp, rbp;
+	 *     __u64 r8,  r9,  r10, r11;
+	 *     __u64 r12, r13, r14, r15;
+	 *     __u64 rip, rflags;
+	 * };
+	 *
+	 * void vcpu_args_set(struct kvm_vcpu *vcpu, unsigned int num, ...)
+	 *
+	 * 就设置一个参数, 给rdi
+	 */
 	vcpu_args_set(vcpu, 1, do_write);
 
 	while (!READ_ONCE(host_quit)) {
+		/*
+		 * 在以下使用test_stage_updated:
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|197| <<vcpu_worker>> ret = sem_wait(&test_stage_updated);
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|294| <<pre_init_before_test>> ret = sem_init(&test_stage_updated, 0, 0);
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|325| <<vcpus_complete_new_stage>> ret = sem_post(&test_stage_updated);
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|405| <<run_test>> ret = sem_post(&test_stage_updated);
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|412| <<run_test>> ret = sem_destroy(&test_stage_updated);
+		 */
 		ret = sem_wait(&test_stage_updated);
 		TEST_ASSERT(ret == 0, "Error in sem_wait");
 
@@ -278,6 +375,15 @@ static struct kvm_vm *pre_init_before_test(enum vm_guest_mode mode, void *arg)
 	test_args.host_pages_per_lpage = large_page_size / host_page_size;
 	test_args.src_type = src_type;
 
+	/*
+	 * 在test里使用KVM_SET_USER_MEMORY_REGION的地方:
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|504| <<kvm_vm_restart>> int ret = ioctl(vmp->fd, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|737| <<__vm_mem_region_delete>> vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|937| <<__vm_set_user_memory_region>> return ioctl(vm->fd, KVM_SET_USER_MEMORY_REGION, &region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1094| <<vm_userspace_mem_region_add>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1177| <<vm_mem_region_set_flags>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1207| <<vm_mem_region_move>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 */
 	/* Add an extra memory slot with specified backing src type */
 	vm_userspace_mem_region_add(vm, src_type, guest_test_phys_mem,
 				    TEST_MEM_SLOT_INDEX, guest_num_pages, 0);
@@ -315,6 +421,13 @@ static struct kvm_vm *pre_init_before_test(enum vm_guest_mode mode, void *arg)
 	return vm;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|426| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|433| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|446| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|458| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ */
 static void vcpus_complete_new_stage(enum test_stage stage)
 {
 	int ret;
@@ -322,6 +435,14 @@ static void vcpus_complete_new_stage(enum test_stage stage)
 
 	/* Wake up all the vcpus to run new test stage */
 	for (vcpus = 0; vcpus < nr_vcpus; vcpus++) {
+		/*
+		 * 在以下使用test_stage_updated:
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|197| <<vcpu_worker>> ret = sem_wait(&test_stage_updated);
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|294| <<pre_init_before_test>> ret = sem_init(&test_stage_updated, 0, 0);
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|325| <<vcpus_complete_new_stage>> ret = sem_post(&test_stage_updated);
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|405| <<run_test>> ret = sem_post(&test_stage_updated);
+		 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|412| <<run_test>> ret = sem_destroy(&test_stage_updated);
+		 */
 		ret = sem_post(&test_stage_updated);
 		TEST_ASSERT(ret == 0, "Error in sem_post");
 	}
@@ -340,6 +461,26 @@ static void vcpus_complete_new_stage(enum test_stage stage)
 		 test_stage_string[stage]);
 }
 
+/*
+ * enum vm_guest_mode {
+ *     VM_MODE_P52V48_4K,
+ *     VM_MODE_P52V48_64K,
+ *     VM_MODE_P48V48_4K,
+ *     VM_MODE_P48V48_16K,
+ *     VM_MODE_P48V48_64K,
+ *     VM_MODE_P40V48_4K,
+ *     VM_MODE_P40V48_16K,
+ *     VM_MODE_P40V48_64K,
+ *     VM_MODE_PXXV48_4K,      // For 48bits VA but ANY bits PA
+ *     VM_MODE_P47V64_4K,
+ *     VM_MODE_P44V64_4K,
+ *     VM_MODE_P36V48_4K,
+ *     VM_MODE_P36V48_16K,
+ *     VM_MODE_P36V48_64K,
+ *     VM_MODE_P36V47_16K,
+ *     NUM_VM_MODES,
+ * };
+ */
 static void run_test(enum vm_guest_mode mode, void *arg)
 {
 	pthread_t *vcpu_threads;
@@ -364,6 +505,9 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 	vcpus_complete_new_stage(*current_stage);
 	pr_info("Started all vCPUs successfully\n");
 
+	/*
+	 * 创建mapping
+	 */
 	/* Test the stage of KVM creating mappings */
 	*current_stage = KVM_CREATE_MAPPINGS;
 
@@ -374,6 +518,9 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 	pr_info("KVM_CREATE_MAPPINGS: total execution time: %ld.%.9lds\n\n",
 		ts_diff.tv_sec, ts_diff.tv_nsec);
 
+	/*
+	 * 设置dirty track
+	 */
 	/* Test the stage of KVM updating mappings */
 	vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX,
 				KVM_MEM_LOG_DIRTY_PAGES);
@@ -387,6 +534,9 @@ static void run_test(enum vm_guest_mode mode, void *arg)
 	pr_info("KVM_UPDATE_MAPPINGS: total execution time: %ld.%.9lds\n\n",
 		ts_diff.tv_sec, ts_diff.tv_nsec);
 
+	/*
+	 * 取消dirty track
+	 */
 	/* Test the stage of KVM adjusting mappings */
 	vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX, 0);
 
@@ -438,13 +588,29 @@ static void help(char *name)
 
 int main(int argc, char *argv[])
 {
+	/*
+	 * 比方x86是KVM_MAX_VCPUS
+	 */
 	int max_vcpus = kvm_check_cap(KVM_CAP_MAX_VCPUS);
+	/*
+	 * struct test_params {
+	 *     uint64_t phys_offset;
+	 *     uint64_t test_mem_size;
+	 *     enum vm_mem_backing_src_type src_type;
+	 * };
+	 *
+	 * 1 << 30 = 1G
+	 */
 	struct test_params p = {
 		.test_mem_size = DEFAULT_TEST_MEM_SIZE,
 		.src_type = DEFAULT_VM_MEM_SRC,
 	};
 	int opt;
 
+	/*
+	 * 对于x86, 似乎就一行:
+	 * guest_mode_append(VM_MODE_DEFAULT, true, true);
+	 */
 	guest_modes_append_default();
 
 	while ((opt = getopt(argc, argv, "hp:m:b:v:s:")) != -1) {
diff --git a/tools/testing/selftests/kvm/lib/elf.c b/tools/testing/selftests/kvm/lib/elf.c
index 266f3876e..f9e5f41e6 100644
--- a/tools/testing/selftests/kvm/lib/elf.c
+++ b/tools/testing/selftests/kvm/lib/elf.c
@@ -111,6 +111,20 @@ static void elfhdr_get(const char *filename, Elf64_Ehdr *hdrp)
  * by the image and it needs to have sufficient available physical pages, to
  * back the virtual pages used to load the image.
  */
+/*
+ * Linux平台上ASLR分为0，1，2三级,用户可以通过内核参数randomize_va_space
+ * 进行等级控制,不同级别的含义如下:
+ *
+ * 0 = 关
+ * 1 = 半随机;共享库,栈,mmap()以及VDSO将被随机化
+ * 2 = 全随机;除了1中所述,还会随机化heap
+ * 注:系统默认开启2全随机模式,PIE会影响heap的随机化
+ *
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|710| <<run_test>> kvm_vm_elf_load(vm, program_invocation_name);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|444| <<__vm_create>> kvm_vm_elf_load(vm, program_invocation_name);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|148| <<finish_vm_setup>> kvm_vm_elf_load(vm, program_invocation_name);
+ */
 void kvm_vm_elf_load(struct kvm_vm *vm, const char *filename)
 {
 	off_t offset, offset_rv;
@@ -144,6 +158,18 @@ void kvm_vm_elf_load(struct kvm_vm *vm, const char *filename)
 			"  rv: %jd errno: %i",
 			n1, filename, (intmax_t) offset_rv, errno);
 
+		/*
+		 * typedef struct elf64_phdr {
+		 *     Elf64_Word p_type;
+		 *     Elf64_Word p_flags;
+		 *     Elf64_Off p_offset;           // Segment file offset
+		 *     Elf64_Addr p_vaddr;           // Segment virtual address
+		 *     Elf64_Addr p_paddr;           // Segment physical address
+		 *     Elf64_Xword p_filesz;         // Segment size in file
+		 *     Elf64_Xword p_memsz;          // Segment size in memory
+		 *     Elf64_Xword p_align;          // Segment alignment, file & memory
+		 * } Elf64_Phdr;
+		 */
 		/* Read in the program header. */
 		Elf64_Phdr phdr;
 		test_read(fd, &phdr, sizeof(phdr));
@@ -162,6 +188,19 @@ void kvm_vm_elf_load(struct kvm_vm *vm, const char *filename)
 		seg_vend |= vm->page_size - 1;
 		size_t seg_size = seg_vend - seg_vstart + 1;
 
+		/*
+		 * called by:
+		 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|356| <<aarch64_vcpu_add>> stack_vaddr = __vm_vaddr_alloc(vm, stack_size, DEFAULT_ARM64_GUEST_STACK_VADDR_MIN, MEM_REGION_DATA);
+		 *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|463| <<vm_init_descriptor_tables>> vm->handlers = __vm_vaddr_alloc(vm, sizeof(struct handlers), vm->page_size, MEM_REGION_DATA);
+		 *   - tools/testing/selftests/kvm/lib/elf.c|165| <<kvm_vm_elf_load>> vm_vaddr_t vaddr = __vm_vaddr_alloc(vm, seg_size, seg_vstart, MEM_REGION_CODE);
+		 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1494| <<vm_vaddr_alloc>> return __vm_vaddr_alloc(vm, sz, vaddr_min, MEM_REGION_TEST_DATA);
+		 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1518| <<__vm_vaddr_alloc_page>> return __vm_vaddr_alloc(vm, getpagesize(), KVM_UTIL_MIN_VADDR, type);
+		 *   - tools/testing/selftests/kvm/lib/riscv/processor.c|292| <<vm_arch_vcpu_add>> stack_vaddr = __vm_vaddr_alloc(vm, stack_size, DEFAULT_RISCV_GUEST_STACK_VADDR_MIN, MEM_REGION_DATA);
+		 *   - tools/testing/selftests/kvm/lib/s390x/processor.c|171| <<vm_arch_vcpu_add>> stack_vaddr = __vm_vaddr_alloc(vm, stack_size, DEFAULT_GUEST_STACK_VADDR_MIN, MEM_REGION_DATA);
+		 *   - tools/testing/selftests/kvm/lib/ucall_common.c|32| <<ucall_init>> vaddr = __vm_vaddr_alloc(vm, sizeof(*hdr), KVM_UTIL_MIN_VADDR, MEM_REGION_DATA);
+		 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|573| <<vm_arch_vcpu_add>> stack_vaddr = __vm_vaddr_alloc(vm, DEFAULT_STACK_PGS * getpagesize(),
+		 *                                                                     DEFAULT_GUEST_STACK_VADDR_MIN, MEM_REGION_DATA);
+		 */
 		vm_vaddr_t vaddr = __vm_vaddr_alloc(vm, seg_size, seg_vstart,
 						    MEM_REGION_CODE);
 		TEST_ASSERT(vaddr == seg_vstart, "Unable to allocate "
diff --git a/tools/testing/selftests/kvm/lib/guest_modes.c b/tools/testing/selftests/kvm/lib/guest_modes.c
index 1df3ce4b1..3e89c0bfe 100644
--- a/tools/testing/selftests/kvm/lib/guest_modes.c
+++ b/tools/testing/selftests/kvm/lib/guest_modes.c
@@ -9,13 +9,59 @@
 enum vm_guest_mode vm_mode_default;
 #endif
 
+/*
+ * enum vm_guest_mode {
+ *     VM_MODE_P52V48_4K,
+ *     VM_MODE_P52V48_64K,
+ *     VM_MODE_P48V48_4K,
+ *     VM_MODE_P48V48_16K,
+ *     VM_MODE_P48V48_64K,
+ *     VM_MODE_P40V48_4K,
+ *     VM_MODE_P40V48_16K,
+ *     VM_MODE_P40V48_64K,
+ *     VM_MODE_PXXV48_4K,      // For 48bits VA but ANY bits PA
+ *     VM_MODE_P47V64_4K,
+ *     VM_MODE_P44V64_4K,
+ *     VM_MODE_P36V48_4K,
+ *     VM_MODE_P36V48_16K,
+ *     VM_MODE_P36V48_64K,
+ *     VM_MODE_P36V47_16K,
+ *     NUM_VM_MODES,
+ * };
+ *
+ * 在以下使用guest_modes[NUM_VM_MODES]:
+ *   - tools/testing/selftests/kvm/include/guest_modes.h|15| <<guest_mode_append>> guest_modes[mode] = (struct guest_mode){ supported, enabled }; \
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|54| <<guest_modes_append_default>> if (guest_modes[i].supported && guest_modes[i].enabled)
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|95| <<for_each_guest_mode>> if (!guest_modes[i].enabled)
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|97| <<for_each_guest_mode>> TEST_ASSERT(guest_modes[i].supported,
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|114| <<guest_modes_help>> guest_modes[i].supported ? " (supported)" : "");
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|126| <<guest_modes_cmdline>> guest_modes[i].enabled = false;
+ *   - tools/testing/selftests/kvm/lib/guest_modes.c|132| <<guest_modes_cmdline>> guest_modes[mode].enabled = true;
+ */
 struct guest_mode guest_modes[NUM_VM_MODES];
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|360| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|240| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|359| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|877| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|448| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|575| <<kvm_selftest_arch_init>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|151| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|242| <<main>> guest_modes_append_default();
+ *
+ * 对于x86, 似乎就一行:
+ * guest_mode_append(VM_MODE_DEFAULT, true, true);
+ */
 void guest_modes_append_default(void)
 {
 #ifndef __aarch64__
 	guest_mode_append(VM_MODE_DEFAULT, true, true);
 #else
+	/*
+	 * 下面的是aarch64的
+	 */
 	{
 		unsigned int limit = kvm_check_cap(KVM_CAP_ARM_VM_IPA_SIZE);
 		bool ps4k, ps16k, ps64k;
@@ -87,10 +133,43 @@ void guest_modes_append_default(void)
 #endif
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1105| <<for_each_test_and_guest_mode>> for_each_guest_mode(run_test, &p);
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|391| <<main>> for_each_guest_mode(run_test, &params);
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|293| <<main>> for_each_guest_mode(run_test, &p);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|432| <<main>> for_each_guest_mode(run_test, &p);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|941| <<main>> for_each_guest_mode(run_test, &p);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|945| <<main>> for_each_guest_mode(run_test, &p);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|476| <<main>> for_each_guest_mode(run_test, &p);
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|183| <<main>> for_each_guest_mode(run_test, &p);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|245| <<main>> for_each_guest_mode(run_test, NULL);
+ *   - tools/testing/selftests/kvm/x86_64/dirty_log_page_splitting_test.c|253| <<main>> for_each_guest_mode(run_test, NULL);
+ */
 void for_each_guest_mode(void (*func)(enum vm_guest_mode, void *), void *arg)
 {
 	int i;
 
+	/*
+	 * enum vm_guest_mode {
+	 *     VM_MODE_P52V48_4K,
+	 *     VM_MODE_P52V48_64K,
+	 *     VM_MODE_P48V48_4K,
+	 *     VM_MODE_P48V48_16K,
+	 *     VM_MODE_P48V48_64K,
+	 *     VM_MODE_P40V48_4K,
+	 *     VM_MODE_P40V48_16K,
+	 *     VM_MODE_P40V48_64K,
+	 *     VM_MODE_PXXV48_4K,      // For 48bits VA but ANY bits PA
+	 *     VM_MODE_P47V64_4K,
+	 *     VM_MODE_P44V64_4K,
+	 *     VM_MODE_P36V48_4K,
+	 *     VM_MODE_P36V48_16K,
+	 *     VM_MODE_P36V48_64K,
+	 *     VM_MODE_P36V47_16K,
+	 *     NUM_VM_MODES,
+	 * };
+	 */
 	for (i = 0; i < NUM_VM_MODES; ++i) {
 		if (!guest_modes[i].enabled)
 			continue;
@@ -115,6 +194,17 @@ void guest_modes_help(void)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|1119| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|365| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|245| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|386| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|894| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/include/guest_modes.h|39| <<main>> void guest_modes_cmdline(const char *arg);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|609| <<main>> guest_modes_cmdline(optarg);
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|156| <<main>> guest_modes_cmdline(optarg);
+ */
 void guest_modes_cmdline(const char *arg)
 {
 	static bool mode_selected;
diff --git a/tools/testing/selftests/kvm/lib/io.c b/tools/testing/selftests/kvm/lib/io.c
index fedb2a741..173949bd6 100644
--- a/tools/testing/selftests/kvm/lib/io.c
+++ b/tools/testing/selftests/kvm/lib/io.c
@@ -115,6 +115,13 @@ ssize_t test_write(int fd, const void *buf, size_t count)
  *  On success, number of bytes read.
  *  On failure, a TEST_ASSERT failure is caused.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/elf.c|34| <<elfhdr_get>> test_read(fd, ident, sizeof(ident));
+ *   - tools/testing/selftests/kvm/lib/elf.c|82| <<elfhdr_get>> test_read(fd, hdrp, sizeof(*hdrp));
+ *   - tools/testing/selftests/kvm/lib/elf.c|155| <<kvm_vm_elf_load>> test_read(fd, &phdr, sizeof(phdr));
+ *   - tools/testing/selftests/kvm/lib/elf.c|209| <<kvm_vm_elf_load>> test_read(fd, addr_gva2hva(vm, phdr.p_vaddr), phdr.p_filesz);
+ */
 ssize_t test_read(int fd, void *buf, size_t count)
 {
 	ssize_t rc;
diff --git a/tools/testing/selftests/kvm/lib/kvm_util.c b/tools/testing/selftests/kvm/lib/kvm_util.c
index 7a8af1821..e217e8df7 100644
--- a/tools/testing/selftests/kvm/lib/kvm_util.c
+++ b/tools/testing/selftests/kvm/lib/kvm_util.c
@@ -134,6 +134,11 @@ void vm_enable_dirty_ring(struct kvm_vm *vm, uint32_t ring_size)
 	vm->dirty_ring_size = ring_size;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|296| <<____vm_create>> vm_open(vm);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|451| <<kvm_vm_restart>> vm_open(vmp);
+ */
 static void vm_open(struct kvm_vm *vm)
 {
 	vm->kvm_fd = _open_kvm_dev_path_or_exit(O_RDWR);
@@ -200,6 +205,10 @@ _Static_assert(sizeof(vm_guest_mode_params)/sizeof(struct vm_guest_mode_params)
  * based on the MSB of the VA. On architectures with this behavior
  * the VA region spans [0, 2^(va_bits - 1)), [-(2^(va_bits - 1), -1].
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|330| <<____vm_create>> vm_vaddr_populate_bitmap(vm);
+ */
 __weak void vm_vaddr_populate_bitmap(struct kvm_vm *vm)
 {
 	sparsebit_set_num(vm->vpages_valid,
@@ -209,6 +218,19 @@ __weak void vm_vaddr_populate_bitmap(struct kvm_vm *vm)
 		(1ULL << (vm->va_bits - 1)) >> vm->page_shift);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|708| <<run_test>> vm = ____vm_create(mode);
+ *   - tools/testing/selftests/kvm/include/kvm_util_base.h|722| <<vm_create_barebones>> return ____vm_create(VM_MODE_DEFAULT);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|362| <<__vm_create>> vm = ____vm_create(mode);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|99| <<create_vm>> return ____vm_create(VM_MODE_DEFAULT);
+ *
+ * 注释:
+ * ____vm_create() does KVM_CREATE_VM and little else.  __vm_create() also
+ * loads the test binary into guest memory and creates an IRQ chip (x86 only).
+ * __vm_create() does NOT create vCPUs, @nr_runnable_vcpus is used purely to
+ * calculate the amount of memory needed for per-vCPU data, e.g. stacks.
+ */
 struct kvm_vm *____vm_create(enum vm_guest_mode mode)
 {
 	struct kvm_vm *vm;
@@ -217,6 +239,13 @@ struct kvm_vm *____vm_create(enum vm_guest_mode mode)
 	TEST_ASSERT(vm != NULL, "Insufficient Memory");
 
 	INIT_LIST_HEAD(&vm->vcpus);
+	/*
+	 * struct kvm_vm *vm:
+	 * -> struct userspace_mem_regions regions;
+	 *    -> struct rb_root gpa_tree;
+	 *    -> struct rb_root hva_tree;
+	 *    -> DECLARE_HASHTABLE(slot_hash, 9);
+	 */
 	vm->regions.gpa_tree = RB_ROOT;
 	vm->regions.hva_tree = RB_ROOT;
 	hash_init(vm->regions.slot_hash);
@@ -295,6 +324,11 @@ struct kvm_vm *____vm_create(enum vm_guest_mode mode)
 
 	vm_open(vm);
 
+	/*
+	 * struct kvm_vm *vm:
+	 * -> struct sparsebit *vpages_valid;
+	 * -> struct sparsebit *vpages_mapped;
+	 */
 	/* Limit to VA-bit canonical virtual addresses. */
 	vm->vpages_valid = sparsebit_alloc();
 	vm_vaddr_populate_bitmap(vm);
@@ -308,6 +342,10 @@ struct kvm_vm *____vm_create(enum vm_guest_mode mode)
 	return vm;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|394| <<__vm_create>> uint64_t nr_pages = vm_nr_pages_required(mode, nr_runnable_vcpus, nr_extra_pages);
+ */
 static uint64_t vm_nr_pages_required(enum vm_guest_mode mode,
 				     uint32_t nr_runnable_vcpus,
 				     uint64_t extra_mem_pages)
@@ -347,6 +385,13 @@ static uint64_t vm_nr_pages_required(enum vm_guest_mode mode,
 	return vm_adjust_num_guest_pages(mode, nr_pages);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|702| <<create_vm>> vm = __vm_create(mode, 1, extra_mem_pages);
+ *   - tools/testing/selftests/kvm/include/kvm_util_base.h|727| <<vm_create>> return __vm_create(VM_MODE_DEFAULT, nr_runnable_vcpus, 0);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|412| <<__vm_create_with_vcpus>> vm = __vm_create(mode, nr_vcpus, extra_mem_pages);
+ *   - tools/testing/selftests/kvm/x86_64/ucna_injection_test.c|274| <<main>> vm = __vm_create(VM_MODE_DEFAULT, 3, 0);
+ */
 struct kvm_vm *__vm_create(enum vm_guest_mode mode, uint32_t nr_runnable_vcpus,
 			   uint64_t nr_extra_pages)
 {
@@ -359,12 +404,56 @@ struct kvm_vm *__vm_create(enum vm_guest_mode mode, uint32_t nr_runnable_vcpus,
 	pr_debug("%s: mode='%s' pages='%ld'\n", __func__,
 		 vm_guest_mode_string(mode), nr_pages);
 
+	/*
+	 * 注释:
+	 * ____vm_create() does KVM_CREATE_VM and little else.  __vm_create() also
+	 * loads the test binary into guest memory and creates an IRQ chip (x86 only).
+	 * __vm_create() does NOT create vCPUs, @nr_runnable_vcpus is used purely to
+	 * calculate the amount of memory needed for per-vCPU data, e.g. stacks.
+	 */
 	vm = ____vm_create(mode);
 
+	/*
+	 * 在test里使用KVM_SET_USER_MEMORY_REGION的地方:
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|504| <<kvm_vm_restart>> int ret = ioctl(vmp->fd, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|737| <<__vm_mem_region_delete>> vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|937| <<__vm_set_user_memory_region>> return ioctl(vm->fd, KVM_SET_USER_MEMORY_REGION, &region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1094| <<vm_userspace_mem_region_add>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1177| <<vm_mem_region_set_flags>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1207| <<vm_mem_region_move>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *
+	 * 注释:
+	 * Allocates a memory area of the number of pages specified by npages
+	 * and maps it to the VM specified by vm, at a starting physical address
+	 * given by guest_paddr.  The region is created with a KVM region slot
+	 * given by slot, which must be unique and < KVM_MEM_SLOTS_NUM.  The
+	 * region is created with the flags given by flags.
+	 */
 	vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, 0, 0, nr_pages, 0);
+	/*
+	 * struct kvm_vm *vm;
+	 * -> uint32_t memslots[NR_MEM_REGIONS];
+	 */
 	for (i = 0; i < NR_MEM_REGIONS; i++)
 		vm->memslots[i] = 0;
 
+	/*
+	 * 注释:
+	 *
+	 * Linux平台上ASLR分为0，1，2三级,用户可以通过内核参数randomize_va_space
+	 * 进行等级控制,不同级别的含义如下:
+	 *
+	 * 0 = 关
+	 * 1 = 半随机;共享库,栈,mmap()以及VDSO将被随机化
+	 * 2 = 全随机;除了1中所述,还会随机化heap
+	 * 注:系统默认开启2全随机模式,PIE会影响heap的随机化
+	 *
+	 * Loads the program image of the ELF file specified by filename,
+	 * into the virtual address space of the VM pointed to by vm.  On entry
+	 * the VM needs to not be using any of the virtual address space used
+	 * by the image and it needs to have sufficient available physical pages, to
+	 * back the virtual pages used to load the image.
+	 */
 	kvm_vm_elf_load(vm, program_invocation_name);
 
 	/*
@@ -376,6 +465,12 @@ struct kvm_vm *__vm_create(enum vm_guest_mode mode, uint32_t nr_runnable_vcpus,
 	slot0 = memslot2region(vm, 0);
 	ucall_init(vm, slot0->region.guest_phys_addr + slot0->region.memory_size);
 
+	/*
+	 * 只有x86有代码
+	 * vm_create_irqchip(vm);
+	 * sync_global_to_guest(vm, host_cpu_is_intel);
+	 * sync_global_to_guest(vm, host_cpu_is_amd);
+	 */
 	kvm_arch_vm_post_create(vm);
 
 	return vm;
@@ -400,6 +495,18 @@ struct kvm_vm *__vm_create(enum vm_guest_mode mode, uint32_t nr_runnable_vcpus,
  * extra_mem_pages is only used to calculate the maximum page table size,
  * no real memory allocation for non-slot0 memory in this function.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/include/kvm_util_base.h|738| <<vm_create_with_vcpus>> return __vm_create_with_vcpus(VM_MODE_DEFAULT, nr_vcpus, 0, guest_code, vcpus);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|354| <<pre_init_before_test>> vm = __vm_create_with_vcpus(mode, nr_vcpus, guest_num_pages, guest_code, test_args.vcpus);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|503| <<__vm_create_with_one_vcpu>> vm = __vm_create_with_vcpus(VM_MODE_DEFAULT, 1, extra_mem_pages, guest_code, vcpus);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|171| <<memstress_create_vm>> vm = __vm_create_with_vcpus(mode, nr_vcpus, slot0_pages + guest_num_pages, memstress_guest_code, vcpus);
+ *
+ * 注释:
+ * Creates a VM with the mode specified by mode (e.g. VM_MODE_P52V48_4K).
+ * extra_mem_pages is only used to calculate the maximum page table size,
+ * no real memory allocation for non-slot0 memory in this function.
+ */
 struct kvm_vm *__vm_create_with_vcpus(enum vm_guest_mode mode, uint32_t nr_vcpus,
 				      uint64_t extra_mem_pages,
 				      void *guest_code, struct kvm_vcpu *vcpus[])
@@ -567,6 +674,13 @@ void kvm_parse_vcpu_pinning(const char *pcpus_string, uint32_t vcpu_to_pcpu[],
  * of the regions is returned.  Null is returned only when no overlapping
  * region exists.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|663| <<kvm_userspace_memory_region_find>> region = userspace_mem_region_find(vm, start, end);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1000| <<vm_userspace_mem_region_add>> region = (struct userspace_mem_region *) userspace_mem_region_find(
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1530| <<addr_gpa2hva>> region = userspace_mem_region_find(vm, gpa, gpa);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1605| <<addr_gpa2alias>> region = userspace_mem_region_find(vm, gpa, gpa);
+ */
 static struct userspace_mem_region *
 userspace_mem_region_find(struct kvm_vm *vm, uint64_t start, uint64_t end)
 {
@@ -658,6 +772,15 @@ static void vm_vcpu_rm(struct kvm_vm *vm, struct kvm_vcpu *vcpu)
 	free(vcpu);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|809| <<kvm_vm_free>> kvm_vm_release(vmp);
+ *   - tools/testing/selftests/kvm/x86_64/amx_test.c|319| <<main>> kvm_vm_release(vm);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_evmcs.c|216| <<save_restore_vm>> kvm_vm_release(vm);
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|201| <<main>> kvm_vm_release(vm);
+ *   - tools/testing/selftests/kvm/x86_64/state_test.c|285| <<main>> kvm_vm_release(vm);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_preemption_timer_test.c|230| <<main>> kvm_vm_release(vm);
+ */
 void kvm_vm_release(struct kvm_vm *vmp)
 {
 	struct kvm_vcpu *vcpu, *tmp;
@@ -673,6 +796,11 @@ void kvm_vm_release(struct kvm_vm *vmp)
 	TEST_ASSERT(!ret,  __KVM_SYSCALL_ERROR("close()", ret));
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|803| <<kvm_vm_free>> __vm_mem_region_delete(vmp, region, false);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1304| <<vm_mem_region_delete>> __vm_mem_region_delete(vm, memslot2region(vm, slot), true);
+ */
 static void __vm_mem_region_delete(struct kvm_vm *vm,
 				   struct userspace_mem_region *region,
 				   bool unlink)
@@ -875,6 +1003,11 @@ static void vm_userspace_mem_region_hva_insert(struct rb_root *hva_tree,
 }
 
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1023| <<vm_set_user_memory_region>> int ret = __vm_set_user_memory_region(vm, slot, flags, gpa, size, hva);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|377| <<test_add_max_memory_regions>> ret = __vm_set_user_memory_region(vm, max_mem_slots, 0,
+ */
 int __vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags,
 				uint64_t gpa, uint64_t size, void *hva)
 {
@@ -889,6 +1022,12 @@ int __vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags
 	return ioctl(vm->fd, KVM_SET_USER_MEMORY_REGION, &region);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|241| <<main>> vm_set_user_memory_region(vm, slot, 0, gpa, slot_size, mem);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|282| <<main>> vm_set_user_memory_region(vm, slot, 0, 0, 0, NULL);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|367| <<test_add_max_memory_regions>> vm_set_user_memory_region(vm, slot, 0,
+ */
 void vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags,
 			       uint64_t gpa, uint64_t size, void *hva)
 {
@@ -920,6 +1059,15 @@ void vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags,
  * given by slot, which must be unique and < KVM_MEM_SLOTS_NUM.  The
  * region is created with the flags given by flags.
  */
+/*
+ * 在test里使用KVM_SET_USER_MEMORY_REGION的地方:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|504| <<kvm_vm_restart>> int ret = ioctl(vmp->fd, KVM_SET_USER_MEMORY_REGION, &region->region);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|737| <<__vm_mem_region_delete>> vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|937| <<__vm_set_user_memory_region>> return ioctl(vm->fd, KVM_SET_USER_MEMORY_REGION, &region);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1094| <<vm_userspace_mem_region_add>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1177| <<vm_mem_region_set_flags>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1207| <<vm_mem_region_move>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+ */
 void vm_userspace_mem_region_add(struct kvm_vm *vm,
 	enum vm_mem_backing_src_type src_type,
 	uint64_t guest_paddr, uint32_t slot, uint64_t npages,
@@ -1036,6 +1184,17 @@ void vm_userspace_mem_region_add(struct kvm_vm *vm,
 
 	region->backing_src_type = src_type;
 	region->unused_phy_pages = sparsebit_alloc();
+	/*
+	 * 在以下使用unused_phy_pages:
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|819| <<__vm_mem_region_delete>> sparsebit_free(&region->unused_phy_pages);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1175| <<vm_userspace_mem_region_add>> region->unused_phy_pages = sparsebit_alloc();
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1176| <<vm_userspace_mem_region_add>> sparsebit_set_num(region->unused_phy_pages, guest_paddr >> vm->page_shift, npages);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|2043| <<vm_dump>> sparsebit_dump(stream, region->unused_phy_pages, 0);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|2190| <<vm_phy_pages_alloc>> if (!sparsebit_is_set(region->unused_phy_pages, pg)) {
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|2195| <<vm_phy_pages_alloc>> base = pg = sparsebit_next_set(region->unused_phy_pages, pg);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|2211| <<vm_phy_pages_alloc>> sparsebit_clear(region->unused_phy_pages, pg);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/vmx.c|509| <<nested_map_memslot>> i = sparsebit_next_clear(region->unused_phy_pages, i);
+	 */
 	sparsebit_set_num(region->unused_phy_pages,
 		guest_paddr >> vm->page_shift, npages);
 	region->region.slot = slot;
@@ -1043,6 +1202,31 @@ void vm_userspace_mem_region_add(struct kvm_vm *vm,
 	region->region.guest_phys_addr = guest_paddr;
 	region->region.memory_size = npages * vm->page_size;
 	region->region.userspace_addr = (uintptr_t) region->host_mem;
+	/*
+	 * 在test里使用KVM_SET_USER_MEMORY_REGION的地方:
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|504| <<kvm_vm_restart>> int ret = ioctl(vmp->fd, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|737| <<__vm_mem_region_delete>> vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|937| <<__vm_set_user_memory_region>> return ioctl(vm->fd, KVM_SET_USER_MEMORY_REGION, &region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1094| <<vm_userspace_mem_region_add>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1177| <<vm_mem_region_set_flags>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1207| <<vm_mem_region_move>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *
+	 * 在这里注册给KVM
+	 *
+	 * struct userspace_mem_region *region;
+	 * -> struct kvm_userspace_memory_region region;
+	 *    -> __u32 slot;
+	 *    -> __u32 flags;
+	 *    -> __u64 guest_phys_addr;
+	 *    -> __u64 memory_size; // bytes
+	 *    -> __u64 userspace_addr; // start of the userspace allocated memory
+	 *
+	 * struct kvm_vm *vm:
+	 * -> struct userspace_mem_regions regions;
+	 *    -> struct rb_root gpa_tree;
+	 *    -> struct rb_root hva_tree;
+	 *    -> DECLARE_HASHTABLE(slot_hash, 9);
+	 */
 	ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
 	TEST_ASSERT(ret == 0, "KVM_SET_USER_MEMORY_REGION IOCTL failed,\n"
 		"  rc: %i errno: %i\n"
@@ -1085,6 +1269,17 @@ void vm_userspace_mem_region_add(struct kvm_vm *vm,
  *   on error (e.g. currently no memory region using memslot as a KVM
  *   memory slot ID).
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/include/kvm_util_base.h|162| <<vm_get_mem_region>> return memslot2region(vm, vm->memslots[type]);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|448| <<__vm_create>> slot0 = memslot2region(vm, 0);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1248| <<vm_mem_region_set_flags>> region = memslot2region(vm, slot);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1278| <<vm_mem_region_move>> region = memslot2region(vm, slot);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1304| <<vm_mem_region_delete>> __vm_mem_region_delete(vm, memslot2region(vm, slot), true);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|2109| <<vm_phy_pages_alloc>> region = memslot2region(vm, memslot);
+ *   - tools/testing/selftests/kvm/lib/x86_64/vmx.c|504| <<nested_map_memslot>> memslot2region(vm, memslot);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|150| <<finish_vm_setup>> slot0 = memslot2region(vm, 0);
+ */
 struct userspace_mem_region *
 memslot2region(struct kvm_vm *vm, uint32_t memslot)
 {
@@ -1117,6 +1312,16 @@ memslot2region(struct kvm_vm *vm, uint32_t memslot)
  * Sets the flags of the memory region specified by the value of slot,
  * to the values given by flags.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|519| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX,
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|532| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX, 0);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|336| <<toggle_dirty_logging>> vm_mem_region_set_flags(vm, slot, flags);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|179| <<enable_dirty_tracking>> vm_mem_region_set_flags(vm, 0, KVM_MEM_LOG_DIRTY_PAGES);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|180| <<enable_dirty_tracking>> vm_mem_region_set_flags(vm, TEST_DATA_MEMSLOT, KVM_MEM_LOG_DIRTY_PAGES);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|351| <<test_migration_mode>> vm_mem_region_set_flags(vm, TEST_DATA_TWO_MEMSLOT, KVM_MEM_LOG_DIRTY_PAGES);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|362| <<test_migration_mode>> vm_mem_region_set_flags(vm, TEST_DATA_TWO_MEMSLOT, 0);
+ */
 void vm_mem_region_set_flags(struct kvm_vm *vm, uint32_t slot, uint32_t flags)
 {
 	int ret;
@@ -1147,6 +1352,13 @@ void vm_mem_region_set_flags(struct kvm_vm *vm, uint32_t slot, uint32_t flags)
  *
  * Change the gpa of a memory region.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/memslot_perf_test.c|624| <<test_memslot_move_loop>> vm_mem_region_move(data->vm, data->nslots - 1 + 1,
+ *   - tools/testing/selftests/kvm/memslot_perf_test.c|626| <<test_memslot_move_loop>> vm_mem_region_move(data->vm, data->nslots - 1 + 1, movesrcgpa);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|206| <<test_move_memory_region>> vm_mem_region_move(vm, MEM_REGION_SLOT, MEM_REGION_GPA - 4096);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|223| <<test_move_memory_region>> vm_mem_region_move(vm, MEM_REGION_SLOT, MEM_REGION_GPA);
+ */
 void vm_mem_region_move(struct kvm_vm *vm, uint32_t slot, uint64_t new_gpa)
 {
 	struct userspace_mem_region *region;
@@ -1154,6 +1366,15 @@ void vm_mem_region_move(struct kvm_vm *vm, uint32_t slot, uint64_t new_gpa)
 
 	region = memslot2region(vm, slot);
 
+	/*
+	 * struct userspace_mem_region *region;
+	 * -> struct kvm_userspace_memory_region region;
+	 *    -> __u32 slot;
+	 *    -> __u32 flags;
+	 *    -> __u64 guest_phys_addr;
+	 *    -> __u64 memory_size; // bytes
+	 *    -> __u64 userspace_addr; // start of the userspace allocated memory
+	 */
 	region->region.guest_phys_addr = new_gpa;
 
 	ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
@@ -1176,6 +1397,13 @@ void vm_mem_region_move(struct kvm_vm *vm, uint32_t slot, uint64_t new_gpa)
  *
  * Delete a memory region.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|87| <<add_remove_memslot>> vm_mem_region_delete(vm, DUMMY_MEMSLOT_INDEX);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|269| <<test_delete_memory_region>> vm_mem_region_delete(vm, MEM_REGION_SLOT);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|279| <<test_delete_memory_region>> vm_mem_region_delete(vm, MEM_REGION_SLOT);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|286| <<test_delete_memory_region>> vm_mem_region_delete(vm, 0);
+ */
 void vm_mem_region_delete(struct kvm_vm *vm, uint32_t slot)
 {
 	__vm_mem_region_delete(vm, memslot2region(vm, slot), true);
@@ -1237,6 +1465,10 @@ struct kvm_vcpu *__vm_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id)
 	TEST_ASSERT(vcpu->run != MAP_FAILED,
 		    __KVM_SYSCALL_ERROR("mmap()", (int)(unsigned long)MAP_FAILED));
 
+	/*
+	 * struct kvm_vm *vm:
+	 * -> struct list_head vcpus;
+	 */
 	/* Add to linked-list of VCPUs. */
 	list_add(&vcpu->list, &vm->vcpus);
 
@@ -1329,6 +1561,18 @@ vm_vaddr_t vm_vaddr_unused_gap(struct kvm_vm *vm, size_t sz,
 	return pgidx_start * vm->page_size;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|356| <<aarch64_vcpu_add>> stack_vaddr = __vm_vaddr_alloc(vm, stack_size, DEFAULT_ARM64_GUEST_STACK_VADDR_MIN, MEM_REGION_DATA);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|463| <<vm_init_descriptor_tables>> vm->handlers = __vm_vaddr_alloc(vm, sizeof(struct handlers), vm->page_size, MEM_REGION_DATA);
+ *   - tools/testing/selftests/kvm/lib/elf.c|165| <<kvm_vm_elf_load>> vm_vaddr_t vaddr = __vm_vaddr_alloc(vm, seg_size, seg_vstart, MEM_REGION_CODE);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1494| <<vm_vaddr_alloc>> return __vm_vaddr_alloc(vm, sz, vaddr_min, MEM_REGION_TEST_DATA);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1518| <<__vm_vaddr_alloc_page>> return __vm_vaddr_alloc(vm, getpagesize(), KVM_UTIL_MIN_VADDR, type);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|292| <<vm_arch_vcpu_add>> stack_vaddr = __vm_vaddr_alloc(vm, stack_size, DEFAULT_RISCV_GUEST_STACK_VADDR_MIN, MEM_REGION_DATA);
+ *   - tools/testing/selftests/kvm/lib/s390x/processor.c|171| <<vm_arch_vcpu_add>> stack_vaddr = __vm_vaddr_alloc(vm, stack_size, DEFAULT_GUEST_STACK_VADDR_MIN, MEM_REGION_DATA);
+ *   - tools/testing/selftests/kvm/lib/ucall_common.c|32| <<ucall_init>> vaddr = __vm_vaddr_alloc(vm, sizeof(*hdr), KVM_UTIL_MIN_VADDR, MEM_REGION_DATA);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|573| <<vm_arch_vcpu_add>> stack_vaddr = __vm_vaddr_alloc(vm, DEFAULT_STACK_PGS * getpagesize(), DEFAULT_GUEST_STACK_VADDR_MIN, MEM_REGION_DATA);
+ */
 vm_vaddr_t __vm_vaddr_alloc(struct kvm_vm *vm, size_t sz, vm_vaddr_t vaddr_min,
 			    enum kvm_mem_region_type type)
 {
@@ -1440,6 +1684,23 @@ vm_vaddr_t vm_vaddr_alloc_page(struct kvm_vm *vm)
  * Within the VM given by @vm, creates a virtual translation for
  * @npages starting at @vaddr to the page range starting at @paddr.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|777| <<run_test>> virt_map(vm, guest_test_virt_mem, guest_test_phys_mem, guest_num_pages);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|356| <<pre_init_before_test>> virt_map(vm, guest_test_virt_mem, guest_test_phys_mem, guest_num_pages);
+ *   - tools/testing/selftests/kvm/lib/aarch64/ucall.c|15| <<ucall_arch_init>> virt_map(vm, mmio_gva, mmio_gpa, 1);
+ *   - tools/testing/selftests/kvm/lib/aarch64/vgic.c|66| <<vgic_v3_setup>> virt_map(vm, gicd_base_gpa, gicd_base_gpa, nr_gic_pages);
+ *   - tools/testing/selftests/kvm/lib/aarch64/vgic.c|74| <<vgic_v3_setup>> virt_map(vm, gicr_base_gpa, gicr_base_gpa, nr_gic_pages);
+ *   - tools/testing/selftests/kvm/lib/memstress.c|218| <<memstress_create_vm>> virt_map(vm, guest_test_virt_mem, args->gpa, guest_num_pages);
+ *   - tools/testing/selftests/kvm/memslot_perf_test.c|347| <<prepare_vm>> virt_map(data->vm, MEM_GPA, MEM_GPA, data->npages);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|131| <<spawn_vm>> virt_map(vm, MEM_REGION_GPA, MEM_REGION_GPA, 2);
+ *   - tools/testing/selftests/kvm/steal_time.c|268| <<main>> virt_map(vm, ST_GPA_BASE, ST_GPA_BASE, gpages);
+ *   - tools/testing/selftests/kvm/x86_64/exit_on_emulation_failure_test.c|33| <<main>> virt_map(vm, MMIO_GVA, MMIO_GPA, 1);
+ *   - tools/testing/selftests/kvm/x86_64/smaller_maxphyaddr_emulation_test.c|78| <<main>> virt_map(vm, MEM_REGION_GVA, MEM_REGION_GPA, 1);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_dirty_log_test.c|98| <<main>> virt_map(vm, GUEST_TEST_MEM, GUEST_TEST_MEM, TEST_MEM_PAGES);
+ *   - tools/testing/selftests/kvm/x86_64/xen_shinfo_test.c|453| <<main>> virt_map(vm, SHINFO_REGION_GVA, SHINFO_REGION_GPA, 3);
+ *   - tools/testing/selftests/kvm/x86_64/xen_vmcall_test.c|102| <<main>> virt_map(vm, HCALL_REGION_GPA, HCALL_REGION_GPA, 2);
+ */
 void virt_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr,
 	      unsigned int npages)
 {
@@ -1807,6 +2068,11 @@ void kvm_gsi_routing_write(struct kvm_vm *vm, struct kvm_irq_routing *routing)
  * Dumps the current state of the VM given by vm, to the FILE stream
  * given by stream.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1274| <<memslot2region>> vm_dump(stderr, vm, 2);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|2212| <<vm_phy_pages_alloc>> vm_dump(stderr, vm, 2);
+ */
 void vm_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent)
 {
 	int ctr;
@@ -1937,6 +2203,19 @@ const char *exit_reason_str(unsigned int exit_reason)
  * and their base address is returned. A TEST_ASSERT failure occurs if
  * not enough pages are available at or above paddr_min.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|102| <<virt_arch_pgd_alloc>> vm->pgd = vm_phy_pages_alloc(vm, nr_pages,
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1531| <<__vm_vaddr_alloc>> vm_paddr_t paddr = vm_phy_pages_alloc(vm, pages,
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|2193| <<vm_phy_page_alloc>> return vm_phy_pages_alloc(vm, 1, paddr_min, memslot);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|63| <<virt_arch_pgd_alloc>> vm->pgd = vm_phy_pages_alloc(vm, nr_pages,
+ *   - tools/testing/selftests/kvm/lib/s390x/processor.c|23| <<virt_arch_pgd_alloc>> paddr = vm_phy_pages_alloc(vm, PAGES_PER_REGION,
+ *   - tools/testing/selftests/kvm/lib/s390x/processor.c|41| <<virt_alloc_region>> taddr = vm_phy_pages_alloc(vm, ri < 4 ? PAGES_PER_REGION : 1,
+ *   - tools/testing/selftests/kvm/memslot_perf_test.c|337| <<prepare_vm>> gpa = vm_phy_pages_alloc(data->vm, npages, guest_addr, slot);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|128| <<spawn_vm>> gpa = vm_phy_pages_alloc(vm, 2, MEM_REGION_GPA, MEM_REGION_SLOT);
+ *   - tools/testing/selftests/kvm/x86_64/smaller_maxphyaddr_emulation_test.c|75| <<main>> gpa = vm_phy_pages_alloc(vm, MEM_REGION_SIZE / PAGE_SIZE,
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|146| <<main>> TEST_ASSERT(vm_phy_pages_alloc(vm, SMRAM_PAGES, SMRAM_GPA, SMRAM_MEMSLOT)
+ */
 vm_paddr_t vm_phy_pages_alloc(struct kvm_vm *vm, size_t num,
 			      vm_paddr_t paddr_min, uint32_t memslot)
 {
@@ -1950,12 +2229,25 @@ vm_paddr_t vm_phy_pages_alloc(struct kvm_vm *vm, size_t num,
 		"  paddr_min: 0x%lx page_size: 0x%x",
 		paddr_min, vm->page_size);
 
+	/*
+	 * struct userspace_mem_region *region;
+	 * -> struct kvm_userspace_memory_region region;
+	 *    -> __u32 slot;
+	 *    -> __u32 flags;
+	 *    -> __u64 guest_phys_addr;
+	 *    -> __u64 memory_size; // bytes
+	 *    -> __u64 userspace_addr; // start of the userspace allocated memory
+	 */
 	region = memslot2region(vm, memslot);
 	base = pg = paddr_min >> vm->page_shift;
 
 	do {
 		for (; pg < base + num; ++pg) {
 			if (!sparsebit_is_set(region->unused_phy_pages, pg)) {
+				/*
+				 * 如果设置了说明un-used
+				 * 如果没设置, 说明used
+				 */
 				base = pg = sparsebit_next_set(region->unused_phy_pages, pg);
 				break;
 			}
@@ -1977,12 +2269,27 @@ vm_paddr_t vm_phy_pages_alloc(struct kvm_vm *vm, size_t num,
 	return base * vm->page_size;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|2224| <<vm_alloc_page_table>> return vm_phy_page_alloc(vm, KVM_GUEST_PAGE_TABLE_MIN_PADDR, vm->memslots[MEM_REGION_PT]);
+ */
 vm_paddr_t vm_phy_page_alloc(struct kvm_vm *vm, vm_paddr_t paddr_min,
 			     uint32_t memslot)
 {
 	return vm_phy_pages_alloc(vm, 1, paddr_min, memslot);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|130| <<_virt_pg_map>> *ptep = addr_pte(vm, vm_alloc_page_table(vm), 3);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|136| <<_virt_pg_map>> *ptep = addr_pte(vm, vm_alloc_page_table(vm), 3);
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|141| <<_virt_pg_map>> *ptep = addr_pte(vm, vm_alloc_page_table(vm), 3);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|90| <<virt_arch_pg_map>> next_ppn = vm_alloc_page_table(vm) >> PGTBL_PAGE_SIZE_SHIFT;
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|100| <<virt_arch_pg_map>> next_ppn = vm_alloc_page_table(vm) >> PGTBL_PAGE_SIZE_SHIFT;
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|132| <<virt_arch_pgd_alloc>> vm->pgd = vm_alloc_page_table(vm);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|180| <<virt_create_upper_pte>> *pte |= vm_alloc_page_table(vm) & PHYSICAL_PAGE_MASK;
+ *   - tools/testing/selftests/kvm/lib/x86_64/vmx.c|382| <<nested_create_pte>> pte->address = vm_alloc_page_table(vm) >> vm->page_shift;
+ */
 vm_paddr_t vm_alloc_page_table(struct kvm_vm *vm)
 {
 	return vm_phy_page_alloc(vm, KVM_GUEST_PAGE_TABLE_MIN_PADDR,
diff --git a/tools/testing/selftests/kvm/lib/sparsebit.c b/tools/testing/selftests/kvm/lib/sparsebit.c
index 88cb6b84e..aa0f44692 100644
--- a/tools/testing/selftests/kvm/lib/sparsebit.c
+++ b/tools/testing/selftests/kvm/lib/sparsebit.c
@@ -1583,6 +1583,15 @@ static size_t display_range(FILE *stream, sparsebit_idx_t low,
  * contiguous bits.  This is done because '-' is used to specify command-line
  * options, and sometimes ranges are specified as command-line arguments.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|2043| <<vm_dump>> sparsebit_dump(stream, region->unused_phy_pages, 0);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|2046| <<vm_dump>> sparsebit_dump(stream, vm->vpages_mapped, indent + 2);
+ *
+ * struct sparsebit *s:
+ * -> struct node *root;
+ * -> sparsebit_num_t num_set;
+ */
 void sparsebit_dump(FILE *stream, struct sparsebit *s,
 	unsigned int indent)
 {
diff --git a/tools/testing/selftests/kvm/lib/ucall_common.c b/tools/testing/selftests/kvm/lib/ucall_common.c
index 816a3fa10..7ac7a6a81 100644
--- a/tools/testing/selftests/kvm/lib/ucall_common.c
+++ b/tools/testing/selftests/kvm/lib/ucall_common.c
@@ -11,6 +11,10 @@ struct ucall_header {
 	struct ucall ucalls[KVM_MAX_VCPUS];
 };
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|379| <<vm_nr_pages_required>> nr_pages += ucall_nr_pages_required(page_size);
+ */
 int ucall_nr_pages_required(uint64_t page_size)
 {
 	return align_up(sizeof(struct ucall_header), page_size) / page_size;
@@ -22,6 +26,12 @@ int ucall_nr_pages_required(uint64_t page_size)
  */
 static struct ucall_header *ucall_pool;
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/aarch64/page_fault_test.c|619| <<setup_ucall>> ucall_init(vm, region->region.guest_phys_addr + region->region.memory_size);
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|462| <<__vm_create>> ucall_init(vm, slot0->region.guest_phys_addr + slot0->region.memory_size);
+ *   - tools/testing/selftests/kvm/s390x/cmma_test.c|151| <<finish_vm_setup>> ucall_init(vm, slot0->region.guest_phys_addr + slot0->region.memory_size);
+ */
 void ucall_init(struct kvm_vm *vm, vm_paddr_t mmio_gpa)
 {
 	struct ucall_header *hdr;
@@ -30,19 +40,51 @@ void ucall_init(struct kvm_vm *vm, vm_paddr_t mmio_gpa)
 	int i;
 
 	vaddr = __vm_vaddr_alloc(vm, sizeof(*hdr), KVM_UTIL_MIN_VADDR, MEM_REGION_DATA);
+	/*
+	 * struct ucall_header {
+	 *     DECLARE_BITMAP(in_use, KVM_MAX_VCPUS);
+	 *     struct ucall ucalls[KVM_MAX_VCPUS]; --> 1024
+	 * };
+	 */
 	hdr = (struct ucall_header *)addr_gva2hva(vm, vaddr);
 	memset(hdr, 0, sizeof(*hdr));
 
 	for (i = 0; i < KVM_MAX_VCPUS; ++i) {
+		/*
+		 * struct ucall {
+		 *     uint64_t cmd;
+		 *     uint64_t args[UCALL_MAX_ARGS]; --> 7
+		 *     char buffer[UCALL_BUFFER_LEN]; --> 1024
+		 *
+		 *     // Host virtual address of this struct.
+		 *     struct ucall *hva;
+		 * };
+		 */
 		uc = &hdr->ucalls[i];
 		uc->hva = uc;
 	}
 
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/lib/aarch64/ucall.c|19| <<ucall_arch_init>> write_guest_global(vm, ucall_exit_mmio_addr, (vm_vaddr_t *)mmio_gva);
+	 *   - tools/testing/selftests/kvm/lib/ucall_common.c|41| <<ucall_init>> write_guest_global(vm, ucall_pool, (struct ucall_header *)vaddr);
+	 *
+	 * 从参数的val写入到hva-of-g
+	 */
 	write_guest_global(vm, ucall_pool, (struct ucall_header *)vaddr);
 
+	/*
+	 * x86没有, aarch64有实现
+	 */
 	ucall_arch_init(vm, mmio_gpa);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/ucall_common.c|84| <<ucall_assert>> uc = ucall_alloc();
+ *   - tools/testing/selftests/kvm/lib/ucall_common.c|105| <<ucall_fmt>> uc = ucall_alloc();
+ *   - tools/testing/selftests/kvm/lib/ucall_common.c|123| <<ucall>> uc = ucall_alloc();
+ */
 static struct ucall *ucall_alloc(void)
 {
 	struct ucall *uc;
@@ -75,6 +117,12 @@ static void ucall_free(struct ucall *uc)
 	clear_bit(uc - ucall_pool->ucalls, ucall_pool->in_use);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/include/ucall_common.h|70| <<____GUEST_ASSERT>> ucall_assert(UCALL_ABORT, _exp, __FILE__, __LINE__, _fmt, ##_args); \
+ *   - tools/testing/selftests/kvm/include/ucall_common.h|80| <<GUEST_FAIL>> ucall_assert(UCALL_ABORT, "Unconditional guest failure", \
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|1090| <<route_exception>> ucall_assert(UCALL_UNHANDLED,
+ */
 void ucall_assert(uint64_t cmd, const char *exp, const char *file,
 		  unsigned int line, const char *fmt, ...)
 {
@@ -97,6 +145,10 @@ void ucall_assert(uint64_t cmd, const char *exp, const char *file,
 	ucall_free(uc);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/include/ucall_common.h|55| <<GUEST_PRINTF>> #define GUEST_PRINTF(_fmt, _args...) ucall_fmt(UCALL_PRINTF, _fmt, ##_args)
+ */
 void ucall_fmt(uint64_t cmd, const char *fmt, ...)
 {
 	struct ucall *uc;
@@ -114,8 +166,26 @@ void ucall_fmt(uint64_t cmd, const char *fmt, ...)
 	ucall_free(uc);
 }
 
+/*
+ * 50 #define GUEST_UCALL_NONE()      ucall_arch_do_ucall((vm_vaddr_t)NULL)
+ * 51
+ * 52 #define GUEST_SYNC_ARGS(stage, arg1, arg2, arg3, arg4)  \
+ * 53                                 ucall(UCALL_SYNC, 6, "hello", stage, arg1, arg2, arg3, arg4)
+ * 54 #define GUEST_SYNC(stage)       ucall(UCALL_SYNC, 2, "hello", stage)
+ * 55 #define GUEST_PRINTF(_fmt, _args...) ucall_fmt(UCALL_PRINTF, _fmt, ##_args)
+ * 56 #define GUEST_DONE()            ucall(UCALL_DONE, 0)
+ */
 void ucall(uint64_t cmd, int nargs, ...)
 {
+	/*
+	 * struct ucall {
+	 *     uint64_t cmd;
+	 *     uint64_t args[UCALL_MAX_ARGS]; --> 7
+	 *     char buffer[UCALL_BUFFER_LEN]; --> 1024
+	 *
+	 *     // Host virtual address of this struct.
+	 *     struct ucall *hva;
+	 */
 	struct ucall *uc;
 	va_list va;
 	int i;
@@ -136,6 +206,9 @@ void ucall(uint64_t cmd, int nargs, ...)
 	ucall_free(uc);
 }
 
+/*
+ * 特别多调用
+ */
 uint64_t get_ucall(struct kvm_vcpu *vcpu, struct ucall *uc)
 {
 	struct ucall ucall;
diff --git a/tools/testing/selftests/kvm/lib/x86_64/processor.c b/tools/testing/selftests/kvm/lib/x86_64/processor.c
index d82883740..23fe38acb 100644
--- a/tools/testing/selftests/kvm/lib/x86_64/processor.c
+++ b/tools/testing/selftests/kvm/lib/x86_64/processor.c
@@ -122,6 +122,10 @@ bool kvm_is_tdp_enabled(void)
 		return get_kvm_amd_param_bool("npt");
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/include/kvm_util_base.h|1007| <<virt_pgd_alloc>> virt_arch_pgd_alloc(vm);
+ */
 void virt_arch_pgd_alloc(struct kvm_vm *vm)
 {
 	TEST_ASSERT(vm->mode == VM_MODE_PXXV48_4K, "Attempt to use "
@@ -134,6 +138,15 @@ void virt_arch_pgd_alloc(struct kvm_vm *vm)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|158| <<virt_create_upper_pte>> uint64_t *pte = virt_get_pte(vm, parent_pte, vaddr, current_level);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|221| <<__virt_pg_map>> pte = virt_get_pte(vm, pde, vaddr, PG_LEVEL_4K);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|284| <<__vm_get_page_table_entry>> pml4e = virt_get_pte(vm, &vm->pgd, vaddr, PG_LEVEL_512G);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|288| <<__vm_get_page_table_entry>> pdpe = virt_get_pte(vm, pml4e, vaddr, PG_LEVEL_1G);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|292| <<__vm_get_page_table_entry>> pde = virt_get_pte(vm, pdpe, vaddr, PG_LEVEL_2M);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|296| <<__vm_get_page_table_entry>> return virt_get_pte(vm, pde, vaddr, PG_LEVEL_4K);
+ */
 static void *virt_get_pte(struct kvm_vm *vm, uint64_t *parent_pte,
 			  uint64_t vaddr, int level)
 {
@@ -148,6 +161,21 @@ static void *virt_get_pte(struct kvm_vm *vm, uint64_t *parent_pte,
 	return &page_table[index];
 }
 
+/*
+ * enum pg_level {
+ *     PG_LEVEL_NONE,
+ *     PG_LEVEL_4K,
+ *     PG_LEVEL_2M,
+ *     PG_LEVEL_1G,
+ *     PG_LEVEL_512G,
+ *     PG_LEVEL_NUM
+ * };
+ *
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|208| <<__virt_pg_map>> pml4e = virt_create_upper_pte(vm, &vm->pgd, vaddr, paddr, PG_LEVEL_512G, level);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|212| <<__virt_pg_map>> pdpe = virt_create_upper_pte(vm, pml4e, vaddr, paddr, PG_LEVEL_1G, level);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|216| <<__virt_pg_map>> pde = virt_create_upper_pte(vm, pdpe, vaddr, paddr, PG_LEVEL_2M, level);
+ */
 static uint64_t *virt_create_upper_pte(struct kvm_vm *vm,
 				       uint64_t *parent_pte,
 				       uint64_t vaddr,
@@ -179,6 +207,13 @@ static uint64_t *virt_create_upper_pte(struct kvm_vm *vm,
 	return pte;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|229| <<virt_arch_pg_map>> __virt_pg_map(vm, vaddr, paddr, PG_LEVEL_4K);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|244| <<virt_map_level>> __virt_pg_map(vm, vaddr, paddr, level);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|246| <<main>> __virt_pg_map(vm, gpa + i, gpa + i, PG_LEVEL_1G);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_tlb_flush.c|624| <<main>> __virt_pg_map(vm, gva + PAGE_SIZE * i, gpa & PAGE_MASK, PG_LEVEL_4K);
+ */
 void __virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr, int level)
 {
 	const uint64_t pg_size = PG_LEVEL_SIZE(level);
@@ -201,6 +236,24 @@ void __virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr, int level)
 		    "  paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x",
 		    paddr, vm->max_gfn, vm->page_size);
 
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|208| <<__virt_pg_map>> pml4e = virt_create_upper_pte(vm, &vm->pgd, vaddr, paddr, PG_LEVEL_512G, level);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|212| <<__virt_pg_map>> pdpe = virt_create_upper_pte(vm, pml4e, vaddr, paddr, PG_LEVEL_1G, level);
+	 *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|216| <<__virt_pg_map>> pde = virt_create_upper_pte(vm, pdpe, vaddr, paddr, PG_LEVEL_2M, level);
+	 *
+	 * enum pg_level {
+	 *     PG_LEVEL_NONE,
+	 *     PG_LEVEL_4K,
+	 *     PG_LEVEL_2M,
+	 *     PG_LEVEL_1G,
+	 *     PG_LEVEL_512G,
+	 *     PG_LEVEL_NUM
+	 * };
+	 *
+	 * struct kvm_vm *vm:
+	 * -> vm_paddr_t pgd;
+	 */
 	/*
 	 * Allocate upper level page tables, if not already present.  Return
 	 * early if a hugepage was created.
@@ -229,6 +282,11 @@ void virt_arch_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr)
 	__virt_pg_map(vm, vaddr, paddr, PG_LEVEL_4K);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|179| <<run_test>> virt_map_level(vm, HPAGE_GVA, HPAGE_GPA, nr_bytes, PG_LEVEL_4K);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|181| <<run_test>> virt_map_level(vm, HPAGE_GVA, HPAGE_GPA, nr_bytes, PG_LEVEL_2M);
+ */
 void virt_map_level(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr,
 		    uint64_t nr_bytes, int level)
 {
diff --git a/tools/testing/selftests/kvm/lib/x86_64/ucall.c b/tools/testing/selftests/kvm/lib/x86_64/ucall.c
index 1265cecc7..5b49e0cbf 100644
--- a/tools/testing/selftests/kvm/lib/x86_64/ucall.c
+++ b/tools/testing/selftests/kvm/lib/x86_64/ucall.c
@@ -8,6 +8,14 @@
 
 #define UCALL_PIO_PORT ((uint16_t)0x1000)
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/include/ucall_common.h|50| <<GUEST_UCALL_NONE>> #define GUEST_UCALL_NONE() ucall_arch_do_ucall((vm_vaddr_t)NULL)
+ *   - tools/testing/selftests/kvm/lib/ucall_common.c|110| <<ucall_alloc>> ucall_arch_do_ucall(GUEST_UCALL_FAILED);
+ *   - tools/testing/selftests/kvm/lib/ucall_common.c|143| <<ucall_assert>> ucall_arch_do_ucall((vm_vaddr_t)uc->hva);
+ *   - tools/testing/selftests/kvm/lib/ucall_common.c|164| <<ucall_fmt>> ucall_arch_do_ucall((vm_vaddr_t)uc->hva);
+ *   - tools/testing/selftests/kvm/lib/ucall_common.c|204| <<ucall>> ucall_arch_do_ucall((vm_vaddr_t)uc->hva);
+ */
 void ucall_arch_do_ucall(vm_vaddr_t uc)
 {
 	/*
diff --git a/tools/testing/selftests/kvm/memslot_modification_stress_test.c b/tools/testing/selftests/kvm/memslot_modification_stress_test.c
index 9855c41ca..7e1dfb69d 100644
--- a/tools/testing/selftests/kvm/memslot_modification_stress_test.c
+++ b/tools/testing/selftests/kvm/memslot_modification_stress_test.c
@@ -62,6 +62,10 @@ struct memslot_antagonist_args {
 	uint64_t nr_modifications;
 };
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|108| <<run_test>> add_remove_memslot(vm, p->delay, p->nr_iterations);
+ */
 static void add_remove_memslot(struct kvm_vm *vm, useconds_t delay,
 			       uint64_t nr_modifications)
 {
diff --git a/tools/testing/selftests/kvm/set_memory_region_test.c b/tools/testing/selftests/kvm/set_memory_region_test.c
index b32960189..4ce00a3c3 100644
--- a/tools/testing/selftests/kvm/set_memory_region_test.c
+++ b/tools/testing/selftests/kvm/set_memory_region_test.c
@@ -38,6 +38,17 @@ extern const uint64_t final_rip_end;
 
 static sem_t vcpu_ready;
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|158| <<guest_code_move_memory_region>> val = guest_spin_on_val(0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|163| <<guest_code_move_memory_region>> val = guest_spin_on_val(MMIO_VAL);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|168| <<guest_code_move_memory_region>> val = guest_spin_on_val(0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|173| <<guest_code_move_memory_region>> val = guest_spin_on_val(MMIO_VAL);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|229| <<guest_code_delete_memory_region>> val = guest_spin_on_val(0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|233| <<guest_code_delete_memory_region>> val = guest_spin_on_val(MMIO_VAL);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|237| <<guest_code_delete_memory_region>> val = guest_spin_on_val(0);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|247| <<guest_code_delete_memory_region>> guest_spin_on_val(MMIO_VAL);
+ */
 static inline uint64_t guest_spin_on_val(uint64_t spin_val)
 {
 	uint64_t val;
@@ -108,6 +119,11 @@ static void wait_for_vcpu(void)
 	usleep(100000);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|197| <<test_move_memory_region>> vm = spawn_vm(&vcpu, &vcpu_thread, guest_code_move_memory_region);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|277| <<test_delete_memory_region>> vm = spawn_vm(&vcpu, &vcpu_thread, guest_code_delete_memory_region);
+ */
 static struct kvm_vm *spawn_vm(struct kvm_vcpu **vcpu, pthread_t *vcpu_thread,
 			       void *guest_code)
 {
diff --git a/tools/testing/selftests/kvm/steal_time.c b/tools/testing/selftests/kvm/steal_time.c
index 171adfb2a..e574c05bc 100644
--- a/tools/testing/selftests/kvm/steal_time.c
+++ b/tools/testing/selftests/kvm/steal_time.c
@@ -17,10 +17,56 @@
 #include "kvm_util.h"
 #include "processor.h"
 
+/*
+ * Linux平台上ASLR分为0，1，2三级,用户可以通过内核参数randomize_va_space
+ * 进行等级控制,不同级别的含义如下:
+ *
+ * 0 = 关
+ * 1 = 半随机;共享库,栈,mmap()以及VDSO将被随机化
+ * 2 = 全随机;除了1中所述,还会随机化heap
+ * 注:系统默认开启2全随机模式,PIE会影响heap的随机化
+ */
+
 #define NR_VCPUS		4
+/*
+ * 在以下使用ST_GPA_BASE:
+ *   - tools/testing/selftests/kvm/steal_time.c|118| <<steal_time_init>> st_gva[i] = (void *)(ST_GPA_BASE + i * STEAL_TIME_SIZE);
+ *   - tools/testing/selftests/kvm/steal_time.c|237| <<steal_time_init>> st_gva[i] = (void *)(ST_GPA_BASE + i * STEAL_TIME_SIZE);
+ *   - tools/testing/selftests/kvm/steal_time.c|339| <<main>> vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, ST_GPA_BASE, 1, gpages, 0);
+ *   - tools/testing/selftests/kvm/steal_time.c|340| <<main>> virt_map(vm, ST_GPA_BASE, ST_GPA_BASE, gpages);
+ */
 #define ST_GPA_BASE		(1 << 30)
 
+/*
+ * 在以下使用st_gva[NR_VCPUS]:
+ *   - tools/testing/selftests/kvm/steal_time.c|33| <<global>> static void *st_gva[NR_VCPUS];
+ *   - tools/testing/selftests/kvm/steal_time.c|78| <<guest_code>> struct kvm_steal_time *st = st_gva[cpu];
+ *   - tools/testing/selftests/kvm/steal_time.c|81| <<guest_code>> GUEST_ASSERT_EQ(rdmsr(MSR_KVM_STEAL_TIME), ((uint64_t)st_gva[cpu] | KVM_MSR_ENABLED));
+ *   - tools/testing/selftests/kvm/steal_time.c|118| <<steal_time_init>> st_gva[i] = (void *)(ST_GPA_BASE + i * STEAL_TIME_SIZE);
+ *   - tools/testing/selftests/kvm/steal_time.c|119| <<steal_time_init>> sync_global_to_guest(vcpu->vm, st_gva[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|122| <<steal_time_init>> (ulong)st_gva[i] | KVM_STEAL_RESERVED_MASK);
+ *   - tools/testing/selftests/kvm/steal_time.c|125| <<steal_time_init>> vcpu_set_msr(vcpu, MSR_KVM_STEAL_TIME, (ulong)st_gva[i] | KVM_MSR_ENABLED);
+ *   - tools/testing/selftests/kvm/steal_time.c|130| <<steal_time_dump>> struct kvm_steal_time *st = addr_gva2hva(vm, (ulong)st_gva[vcpu_idx]);
+ *   - tools/testing/selftests/kvm/steal_time.c|193| <<guest_code>> GUEST_ASSERT_EQ(status, (ulong)st_gva[cpu]);
+ *   - tools/testing/selftests/kvm/steal_time.c|237| <<steal_time_init>> st_gva[i] = (void *)(ST_GPA_BASE + i * STEAL_TIME_SIZE);
+ *   - tools/testing/selftests/kvm/steal_time.c|238| <<steal_time_init>> sync_global_to_guest(vm, st_gva[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|240| <<steal_time_init>> st_ipa = (ulong)st_gva[i] | 1;
+ *   - tools/testing/selftests/kvm/steal_time.c|244| <<steal_time_init>> st_ipa = (ulong)st_gva[i];
+ *   - tools/testing/selftests/kvm/steal_time.c|253| <<steal_time_dump>> struct st_time *st = addr_gva2hva(vm, (ulong)st_gva[vcpu_idx]);
+ */
 static void *st_gva[NR_VCPUS];
+/*
+ * 在以下使用guest_stolen_time[NR_VCPUS]:
+ *   - tools/testing/selftests/kvm/steal_time.c|49| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->steal);
+ *   - tools/testing/selftests/kvm/steal_time.c|56| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->steal);
+ *   - tools/testing/selftests/kvm/steal_time.c|149| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->st_time);
+ *   - tools/testing/selftests/kvm/steal_time.c|153| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->st_time);
+ *   - tools/testing/selftests/kvm/steal_time.c|283| <<main>> sync_global_from_guest(vm, guest_stolen_time[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|284| <<main>> stolen_time = guest_stolen_time[i];
+ *   - tools/testing/selftests/kvm/steal_time.c|304| <<main>> sync_global_from_guest(vm, guest_stolen_time[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|305| <<main>> stolen_time = guest_stolen_time[i] - stolen_time;
+ *   - tools/testing/selftests/kvm/steal_time.c|312| <<main>> guest_stolen_time[i], stolen_time);
+ */
 static uint64_t guest_stolen_time[NR_VCPUS];
 
 #if defined(__x86_64__)
@@ -28,6 +74,22 @@ static uint64_t guest_stolen_time[NR_VCPUS];
 /* steal_time must have 64-byte alignment */
 #define STEAL_TIME_SIZE		((sizeof(struct kvm_steal_time) + 63) & ~63)
 
+/*
+ * struct kvm_steal_time {
+ *     __u64 steal;
+ *     __u32 version;
+ *     __u32 flags;
+ *     __u8  preempted;
+ *     __u8  u8_pad[3];
+ *     __u32 pad[11];
+ * };
+ *
+ * called by:
+ *   - tools/testing/selftests/kvm/steal_time.c|60| <<guest_code>> check_status(st);
+ *   - tools/testing/selftests/kvm/steal_time.c|63| <<guest_code>> check_status(st);
+ *   - tools/testing/selftests/kvm/steal_time.c|66| <<guest_code>> check_status(st);
+ *   - tools/testing/selftests/kvm/steal_time.c|69| <<guest_code>> check_status(st);
+ */
 static void check_status(struct kvm_steal_time *st)
 {
 	GUEST_ASSERT(!(READ_ONCE(st->version) & 1));
@@ -63,10 +125,19 @@ static bool is_steal_time_supported(struct kvm_vcpu *vcpu)
 	return kvm_cpu_has(X86_FEATURE_KVM_STEAL_TIME);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/steal_time.c|342| <<main>> steal_time_init(vcpus[i], i);
+ */
 static void steal_time_init(struct kvm_vcpu *vcpu, uint32_t i)
 {
 	int ret;
 
+	/*
+	 * sync_global_to_guest():
+	 * 从gva拷贝到转换后的hva
+	 * 难道是类似:从当前的space拷贝到qemu的space?
+	 */
 	/* ST_GPA_BASE is identity mapped */
 	st_gva[i] = (void *)(ST_GPA_BASE + i * STEAL_TIME_SIZE);
 	sync_global_to_guest(vcpu->vm, st_gva[i]);
@@ -98,6 +169,9 @@ static void steal_time_dump(struct kvm_vm *vm, uint32_t vcpu_idx)
 }
 
 #elif defined(__aarch64__)
+/*
+ * 下面是aarch64
+ */
 
 /* PV_TIME_ST must have 64-byte alignment */
 #define STEAL_TIME_SIZE		((sizeof(struct st_time) + 63) & ~63)
@@ -178,6 +252,11 @@ static void steal_time_init(struct kvm_vcpu *vcpu, uint32_t i)
 
 	vcpu_ioctl(vcpu, KVM_HAS_DEVICE_ATTR, &dev);
 
+	/*
+	 * sync_global_to_guest():
+	 * 从gva拷贝到转换后的hva
+	 * 难道是类似:从当前的space拷贝到qemu的space?
+	 */
 	/* ST_GPA_BASE is identity mapped */
 	st_gva[i] = (void *)(ST_GPA_BASE + i * STEAL_TIME_SIZE);
 	sync_global_to_guest(vm, st_gva[i]);
@@ -203,6 +282,9 @@ static void steal_time_dump(struct kvm_vm *vm, uint32_t vcpu_idx)
 	pr_info("    st_time: %ld\n", st->st_time);
 }
 
+/*
+ * 上面是aarch64
+ */
 #endif
 
 static void *do_steal_time(void *arg)
@@ -261,9 +343,23 @@ int main(int ac, char **av)
 	pthread_attr_setaffinity_np(&attr, sizeof(cpu_set_t), &cpuset);
 	pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);
 
+	/*
+	 * 4个vcpu
+	 */
 	/* Create a VM and an identity mapped memslot for the steal time structure */
 	vm = vm_create_with_vcpus(NR_VCPUS, guest_code, vcpus);
 	gpages = vm_calc_num_guest_pages(VM_MODE_DEFAULT, STEAL_TIME_SIZE * NR_VCPUS);
+	/*
+	 * 在test里使用KVM_SET_USER_MEMORY_REGION的地方:
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|504| <<kvm_vm_restart>> int ret = ioctl(vmp->fd, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|737| <<__vm_mem_region_delete>> vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|937| <<__vm_set_user_memory_region>> return ioctl(vm->fd, KVM_SET_USER_MEMORY_REGION, &region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1094| <<vm_userspace_mem_region_add>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1177| <<vm_mem_region_set_flags>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *   - tools/testing/selftests/kvm/lib/kvm_util.c|1207| <<vm_mem_region_move>> ret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);
+	 *
+	 * 1 << 30是1G
+	 */
 	vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, ST_GPA_BASE, 1, gpages, 0);
 	virt_map(vm, ST_GPA_BASE, ST_GPA_BASE, gpages);
 
@@ -280,6 +376,23 @@ int main(int ac, char **av)
 
 		/* Second VCPU run, expect guest stolen time to be <= run_delay */
 		run_vcpu(vcpus[i]);
+		/*
+		 * 在以下使用guest_stolen_time[NR_VCPUS]:
+		 *   - tools/testing/selftests/kvm/steal_time.c|49| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->steal);
+		 *   - tools/testing/selftests/kvm/steal_time.c|56| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->steal);
+		 *   - tools/testing/selftests/kvm/steal_time.c|149| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->st_time);
+		 *   - tools/testing/selftests/kvm/steal_time.c|153| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->st_time);
+		 *   - tools/testing/selftests/kvm/steal_time.c|283| <<main>> sync_global_from_guest(vm, guest_stolen_time[i]);
+		 *   - tools/testing/selftests/kvm/steal_time.c|284| <<main>> stolen_time = guest_stolen_time[i];
+		 *   - tools/testing/selftests/kvm/steal_time.c|304| <<main>> sync_global_from_guest(vm, guest_stolen_time[i]);
+		 *   - tools/testing/selftests/kvm/steal_time.c|305| <<main>> stolen_time = guest_stolen_time[i] - stolen_time;
+		 *   - tools/testing/selftests/kvm/steal_time.c|312| <<main>> guest_stolen_time[i], stolen_time);
+		 *
+		 * sync_global_from_guest()
+		 * 从转换后的hva拷贝到gva
+		 * 难道是类似:从qemu的space拷贝到当前的space?
+		 * 然后才能在test program访问?
+		 */
 		sync_global_from_guest(vm, guest_stolen_time[i]);
 		stolen_time = guest_stolen_time[i];
 		run_delay = get_run_delay();
@@ -301,6 +414,12 @@ int main(int ac, char **av)
 
 		/* Run VCPU again to confirm stolen time is consistent with run_delay */
 		run_vcpu(vcpus[i]);
+		/*
+		 * sync_global_from_guest()
+		 * 从转换后的hva拷贝到gva
+		 * 难道是类似:从qemu的space拷贝到当前的space?
+		 * 然后才能在test program访问?
+		 */
 		sync_global_from_guest(vm, guest_stolen_time[i]);
 		stolen_time = guest_stolen_time[i] - stolen_time;
 		TEST_ASSERT(stolen_time >= run_delay,
diff --git a/tools/testing/selftests/kvm/x86_64/kvm_clock_test.c b/tools/testing/selftests/kvm/x86_64/kvm_clock_test.c
index 177870436..9f286d3c7 100644
--- a/tools/testing/selftests/kvm/x86_64/kvm_clock_test.c
+++ b/tools/testing/selftests/kvm/x86_64/kvm_clock_test.c
@@ -104,6 +104,16 @@ static void setup_clock(struct kvm_vm *vm, struct test_case *test_case)
 
 static void enter_guest(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_clock_data {
+	 *     __u64 clock;
+	 *     __u32 flags;
+	 *     __u32 pad0;
+	 *     __u64 realtime;
+	 *     __u64 host_tsc;
+	 *     __u32 pad[4];
+	 * };
+	 */
 	struct kvm_clock_data start, end;
 	struct kvm_vm *vm = vcpu->vm;
 	struct ucall uc;
diff --git a/tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c b/tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c
index 18ac5c195..92bfc802b 100644
--- a/tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c
+++ b/tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c
@@ -34,6 +34,11 @@
  */
 #define RETURN_OPCODE 0xC3
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|62| <<guest_code>> guest_do_CALL(hpage_1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|65| <<guest_code>> guest_do_CALL(hpage_3);
+ */
 /* Call the specified memory address. */
 static void guest_do_CALL(uint64_t target)
 {
@@ -72,6 +77,17 @@ void guest_code(void)
 	GUEST_SYNC(6);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|155| <<run_test>> check_2m_page_count(vm, 0);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|163| <<run_test>> check_2m_page_count(vm, 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|171| <<run_test>> check_2m_page_count(vm, 2);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|181| <<run_test>> check_2m_page_count(vm, disable_nx_huge_pages ? 2 : 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|191| <<run_test>> check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|196| <<run_test>> check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|208| <<run_test>> check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|216| <<run_test>> check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 2);
+ */
 static void check_2m_page_count(struct kvm_vm *vm, int expected_pages_2m)
 {
 	int actual_pages_2m;
@@ -83,6 +99,17 @@ static void check_2m_page_count(struct kvm_vm *vm, int expected_pages_2m)
 		    expected_pages_2m, actual_pages_2m);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|156| <<run_test>> check_split_count(vm, 0);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|164| <<run_test>> check_split_count(vm, 0);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|172| <<run_test>> check_split_count(vm, 0);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|182| <<run_test>> check_split_count(vm, disable_nx_huge_pages ? 0 : 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|192| <<run_test>> check_split_count(vm, disable_nx_huge_pages ? 0 : 2);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|197| <<run_test>> check_split_count(vm, disable_nx_huge_pages ? 0 : 2);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|209| <<run_test>> check_split_count(vm, 0);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|217| <<run_test>> check_split_count(vm, 0);
+ */
 static void check_split_count(struct kvm_vm *vm, int expected_splits)
 {
 	int actual_splits;
@@ -94,6 +121,10 @@ static void check_split_count(struct kvm_vm *vm, int expected_splits)
 		    expected_splits, actual_splits);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|200| <<run_test>> wait_for_reclaim(reclaim_period_ms);
+ */
 static void wait_for_reclaim(int reclaim_period_ms)
 {
 	long reclaim_wait_ms;
@@ -152,9 +183,17 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	hva = addr_gpa2hva(vm, HPAGE_GPA);
 	memset(hva, RETURN_OPCODE, nr_bytes);
 
+	/*
+	 * 一开始的时候都是0
+	 */
 	check_2m_page_count(vm, 0);
 	check_split_count(vm, 0);
 
+	/*
+	 * 第1次的:
+	 * READ_ONCE(*(uint64_t *)hpage_1);
+	 * GUEST_SYNC(1);
+	 */
 	/*
 	 * The guest code will first read from the first hugepage, resulting
 	 * in a huge page mapping being created.
@@ -163,6 +202,11 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	check_2m_page_count(vm, 1);
 	check_split_count(vm, 0);
 
+	/*
+	 * 第2次的:
+	 * READ_ONCE(*(uint64_t *)hpage_2);
+	 * GUEST_SYNC(2);
+	 */
 	/*
 	 * Then the guest code will read from the second hugepage, resulting
 	 * in another huge page mapping being created.
@@ -171,6 +215,11 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	check_2m_page_count(vm, 2);
 	check_split_count(vm, 0);
 
+	/*
+	 * 第3次的:
+	 * guest_do_CALL(hpage_1);
+	 * GUEST_SYNC(3);
+	 */
 	/*
 	 * Next, the guest will execute from the first huge page, causing it
 	 * to be remapped at 4k.
@@ -181,6 +230,11 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	check_2m_page_count(vm, disable_nx_huge_pages ? 2 : 1);
 	check_split_count(vm, disable_nx_huge_pages ? 0 : 1);
 
+	/*
+	 * 第4次的:
+	 * guest_do_CALL(hpage_3);
+	 * GUEST_SYNC(4);
+	 */
 	/*
 	 * Executing from the third huge page (previously unaccessed) will
 	 * cause part to be mapped at 4k.
@@ -191,6 +245,11 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 1);
 	check_split_count(vm, disable_nx_huge_pages ? 0 : 2);
 
+	/*
+	 * 第5次的:
+	 * READ_ONCE(*(uint64_t *)hpage_1);
+	 * GUEST_SYNC(5);
+	 */
 	/* Reading from the first huge page again should have no effect. */
 	vcpu_run(vcpu);
 	check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 1);
@@ -208,6 +267,11 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 1);
 	check_split_count(vm, 0);
 
+	/*
+	 * 第6次的:
+	 * READ_ONCE(*(uint64_t *)hpage_3);
+	 * GUEST_SYNC(6);
+	 */
 	/*
 	 * The 4k mapping on hpage 3 should have been removed, so check that
 	 * reading from it causes a huge page mapping to be installed.
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 89912a17f..e15c8affa 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -38,6 +38,16 @@ kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
 	return true;
 }
 
+/*
+ * 在以下使用kvm_kernel_irqfd->inject:
+ *   - virt/kvm/eventfd.c|141| <<irqfd_shutdown>> flush_work(&irqfd->inject);
+ *   - virt/kvm/eventfd.c|218| <<irqfd_wakeup>> schedule_work(&irqfd->inject);
+ *   - virt/kvm/eventfd.c|325| <<kvm_irqfd_assign>> INIT_WORK(&irqfd->inject, irqfd_inject);
+ *   - virt/kvm/eventfd.c|425| <<kvm_irqfd_assign>> schedule_work(&irqfd->inject);
+ *
+ * 在以下使用irqfd_inject():
+ *   - virt/kvm/eventfd.c|325| <<kvm_irqfd_assign>> INIT_WORK(&irqfd->inject, irqfd_inject);
+ */
 static void
 irqfd_inject(struct work_struct *work)
 {
@@ -190,6 +200,28 @@ int __attribute__((weak)) kvm_arch_set_irq_inatomic(
 /*
  * Called with wqh->lock held and interrupts disabled
  */
+/*
+ * 旧的callstack
+ * kvm_apic_set_irq
+ * kvm_arch_set_irq_inatomic
+ * irqfd_wakeup
+ * __wake_up_common
+ * __wake_up_locked_key
+ * eventfd_signal
+ * vhost_signal
+ * vhost_add_used_and_signal_n
+ * vhost_net_signal_used
+ * vhost_tx_batch.isra.23
+ * handle_tx_copy
+ * handle_tx
+ * handle_tx_kick
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ *
+ * 在以下使用irqfd_wakeup():
+ *   - virt/kvm/eventfd.c|396| <<kvm_irqfd_assign>> init_waitqueue_func_entry(&irqfd->wait, irqfd_wakeup);
+ */
 static int
 irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 {
@@ -802,11 +834,19 @@ ioeventfd_destructor(struct kvm_io_device *this)
 	ioeventfd_release(p);
 }
 
+/*
+ * 在以下使用ioeventfd_ops:
+ *   - virt/kvm/eventfd.c|908| <<kvm_assign_ioeventfd_idx>> kvm_iodevice_init(&p->dev, &ioeventfd_ops);
+ */
 static const struct kvm_io_device_ops ioeventfd_ops = {
 	.write      = ioeventfd_write,
 	.destructor = ioeventfd_destructor,
 };
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|903| <<kvm_assign_ioeventfd_idx>> if (ioeventfd_check_collision(kvm, p)) {
+ */
 /* assumes kvm->slots_lock held */
 static bool
 ioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)
@@ -825,6 +865,11 @@ ioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)
 	return false;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|978| <<kvm_deassign_ioeventfd>> enum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);
+ *   - virt/kvm/eventfd.c|993| <<kvm_assign_ioeventfd>> bus_idx = ioeventfd_bus_from_flags(args->flags);
+ */
 static enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)
 {
 	if (flags & KVM_IOEVENTFD_FLAG_PIO)
@@ -880,6 +925,12 @@ static int kvm_assign_ioeventfd_idx(struct kvm *kvm,
 	if (ret < 0)
 		goto unlock_fail;
 
+	/*
+	 * 在以下使用kvm_io_bus->ioeventfd_count:
+	 *   - virt/kvm/eventfd.c|915| <<kvm_assign_ioeventfd_idx>> kvm_get_bus(kvm, bus_idx)->ioeventfd_count++;
+	 *   - virt/kvm/eventfd.c|964| <<kvm_deassign_ioeventfd_idx>> bus->ioeventfd_count--;
+	 *   - virt/kvm/kvm_main.c|5596| <<kvm_io_bus_register_dev>> if (bus->dev_count - bus->ioeventfd_count > NR_IOBUS_DEVS - 1)
+	 */
 	kvm_get_bus(kvm, bus_idx)->ioeventfd_count++;
 	list_add_tail(&p->list, &kvm->ioeventfds);
 
@@ -928,6 +979,12 @@ kvm_deassign_ioeventfd_idx(struct kvm *kvm, enum kvm_bus bus_idx,
 
 		kvm_io_bus_unregister_dev(kvm, bus_idx, &p->dev);
 		bus = kvm_get_bus(kvm, bus_idx);
+		/*
+		 * 在以下使用kvm_io_bus->ioeventfd_count:
+		 *   - virt/kvm/eventfd.c|915| <<kvm_assign_ioeventfd_idx>> kvm_get_bus(kvm, bus_idx)->ioeventfd_count++;
+		 *   - virt/kvm/eventfd.c|964| <<kvm_deassign_ioeventfd_idx>> bus->ioeventfd_count--;
+		 *   - virt/kvm/kvm_main.c|5596| <<kvm_io_bus_register_dev>> if (bus->dev_count - bus->ioeventfd_count > NR_IOBUS_DEVS - 1)
+		 */
 		if (bus)
 			bus->ioeventfd_count--;
 		ret = 0;
@@ -958,6 +1015,15 @@ kvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 	enum kvm_bus              bus_idx;
 	int ret;
 
+	/*
+	 * enum kvm_bus {
+	 *     KVM_MMIO_BUS,
+	 *     KVM_PIO_BUS,
+	 *     KVM_VIRTIO_CCW_NOTIFY_BUS,
+	 *     KVM_FAST_MMIO_BUS,
+	 *     KVM_NR_BUSES
+	 * };
+	 */
 	bus_idx = ioeventfd_bus_from_flags(args->flags);
 	/* must be natural-word sized, or 0 to ignore length */
 	switch (args->len) {
@@ -1004,6 +1070,10 @@ kvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4910| <<kvm_vm_ioctl(KVM_IOEVENTFD)>> r = kvm_ioeventfd(kvm, &data);
+ */
 int
 kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 1e567d1f6..3b24e782f 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -85,6 +85,9 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 
 	while (i--) {
 		int r;
+		/*
+		 * kvm_set_msi()
+		 */
 		r = irq_set[i].set(&irq_set[i], kvm, irq_source_id, level,
 				   line_status);
 		if (r < 0)
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 486800a70..433cc1825 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -2923,6 +2923,11 @@ void kvm_vcpu_unmap(struct kvm_vcpu *vcpu, struct kvm_host_map *map, bool dirty)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_unmap);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2937| <<kvm_set_page_dirty>> if (kvm_is_ad_tracked_page(page))
+ *   - virt/kvm/kvm_main.c|2943| <<kvm_set_page_accessed>> if (kvm_is_ad_tracked_page(page))
+ */
 static bool kvm_is_ad_tracked_page(struct page *page)
 {
 	/*
@@ -3390,6 +3395,15 @@ void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_mark_page_dirty);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|939| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/mips/kvm/mips.c|431| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|1859| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|664| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|5065| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/x86/kvm/x86.c|11207| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ */
 void kvm_sigset_activate(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu->sigset_active)
@@ -3476,6 +3490,10 @@ static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
  * pending.  This is mostly used when halting a vCPU, but may also be used
  * directly for other vCPU non-runnable states, e.g. x86's Wait-For-SIPI.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3597| <<kvm_vcpu_halt>> waited = kvm_vcpu_block(vcpu);
+ */
 bool kvm_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	struct rcuwait *wait = kvm_arch_vcpu_get_wait(vcpu);
@@ -3554,6 +3572,19 @@ static unsigned int kvm_vcpu_max_halt_poll_ns(struct kvm_vcpu *vcpu)
  * expensive block+unblock sequence if a wake event arrives soon after the vCPU
  * is halted.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|742| <<kvm_vcpu_wfi>> kvm_vcpu_halt(vcpu);
+ *   - arch/mips/kvm/emulate.c|955| <<kvm_mips_emul_wait>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr.c|501| <<kvmppc_set_msr_pr>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr_papr.c|395| <<kvmppc_h_pr>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/booke.c|724| <<kvmppc_core_prepare_to_enter>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|240| <<kvmppc_kvm_pv>> kvm_vcpu_halt(vcpu);
+ *   - arch/riscv/kvm/vcpu_insn.c|192| <<kvm_riscv_vcpu_wfi>> kvm_vcpu_halt(vcpu);
+ *   - arch/s390/kvm/interrupt.c|1334| <<kvm_s390_handle_wait>> kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/x86.c|11039| <<vcpu_block>> kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/xen.c|1352| <<kvm_xen_schedop_poll>> kvm_vcpu_halt(vcpu);
+ */
 void kvm_vcpu_halt(struct kvm_vcpu *vcpu)
 {
 	unsigned int max_halt_poll_ns = kvm_vcpu_max_halt_poll_ns(vcpu);
@@ -5253,6 +5284,10 @@ static void hardware_disable_all(void)
 	cpus_read_unlock();
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1226| <<kvm_create_vm>> r = hardware_enable_all();
+ */
 static int hardware_enable_all(void)
 {
 	atomic_t failed = ATOMIC_INIT(0);
@@ -5557,6 +5592,12 @@ int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 	if (!bus)
 		return -ENOMEM;
 
+	/*
+	 * 在以下使用kvm_io_bus->ioeventfd_count:
+	 *   - virt/kvm/eventfd.c|915| <<kvm_assign_ioeventfd_idx>> kvm_get_bus(kvm, bus_idx)->ioeventfd_count++;
+	 *   - virt/kvm/eventfd.c|964| <<kvm_deassign_ioeventfd_idx>> bus->ioeventfd_count--;
+	 *   - virt/kvm/kvm_main.c|5596| <<kvm_io_bus_register_dev>> if (bus->dev_count - bus->ioeventfd_count > NR_IOBUS_DEVS - 1)
+	 */
 	/* exclude ioeventfd which is limited by maximum fd */
 	if (bus->dev_count - bus->ioeventfd_count > NR_IOBUS_DEVS - 1)
 		return -ENOSPC;
@@ -6000,6 +6041,16 @@ EXPORT_SYMBOL_GPL(kvm_get_running_vcpu);
 /**
  * kvm_get_running_vcpus - get the per-CPU array of currently running vcpus.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1650| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_vtimer_irq, kvm_arch_timer_handler, "kvm guest vtimer", kvm_get_running_vcpus());
+ *   - arch/arm64/kvm/arch_timer.c|1659| <<kvm_timer_hyp_init>> err = irq_set_vcpu_affinity(host_vtimer_irq, kvm_get_running_vcpus());
+ *   - arch/arm64/kvm/arch_timer.c|1674| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_ptimer_irq, kvm_arch_timer_handler, "kvm guest ptimer", kvm_get_running_vcpus());
+ *   - arch/arm64/kvm/arch_timer.c|1683| <<kvm_timer_hyp_init>> err = irq_set_vcpu_affinity(host_ptimer_irq, kvm_get_running_vcpus());
+ *   - arch/arm64/kvm/arch_timer.c|1702| <<kvm_timer_hyp_init>> free_percpu_irq(host_ptimer_irq, kvm_get_running_vcpus());
+ *   - arch/arm64/kvm/arch_timer.c|1704| <<kvm_timer_hyp_init>> free_percpu_irq(host_vtimer_irq, kvm_get_running_vcpus());
+ *   - arch/arm64/kvm/vgic/vgic-init.c|606| <<kvm_vgic_hyp_init>> ret = request_percpu_irq(kvm_vgic_global_state.maint_irq, vgic_maintenance_handler, "vgic", kvm_get_running_vcpus());
+ */
 struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void)
 {
         return &kvm_running_vcpu;
@@ -6032,12 +6083,23 @@ static unsigned long kvm_guest_get_ip(void)
 	return kvm_arch_vcpu_get_ip(vcpu);
 }
 
+/*
+ * 在以下使用kvm_guest_cbs:
+ *   - virt/kvm/kvm_main.c|6078| <<kvm_register_perf_callbacks>> kvm_guest_cbs.handle_intel_pt_intr = pt_intr_handler;
+ *   - virt/kvm/kvm_main.c|6079| <<kvm_register_perf_callbacks>> perf_register_guest_info_callbacks(&kvm_guest_cbs);
+ *   - virt/kvm/kvm_main.c|6083| <<kvm_unregister_perf_callbacks>> perf_unregister_guest_info_callbacks(&kvm_guest_cbs);
+ */
 static struct perf_guest_info_callbacks kvm_guest_cbs = {
 	.state			= kvm_guest_state,
 	.get_ip			= kvm_guest_get_ip,
 	.handle_intel_pt_intr	= NULL,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2183| <<init_subsystems>> kvm_register_perf_callbacks(NULL);
+ *   - arch/x86/kvm/x86.c|9603| <<__kvm_x86_vendor_init>> kvm_register_perf_callbacks(ops->handle_intel_pt_intr);
+ */
 void kvm_register_perf_callbacks(unsigned int (*pt_intr_handler)(void))
 {
 	kvm_guest_cbs.handle_intel_pt_intr = pt_intr_handler;
-- 
2.34.1

