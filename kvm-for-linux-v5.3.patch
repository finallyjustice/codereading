From 8d573aabad90924488a1cf8a859aacd35fd17343 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Thu, 14 Nov 2019 05:57:30 +0800
Subject: [PATCH 1/1] kvm for linux v5.3

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/kvm_host.h      |  229 ++++
 arch/x86/include/asm/processor.h     |    7 +
 arch/x86/include/uapi/asm/kvm_para.h |  108 ++
 arch/x86/kernel/apic/apic.c          |    3 +
 arch/x86/kernel/kvm.c                |  574 ++++++++++
 arch/x86/kernel/kvmclock.c           |   74 ++
 arch/x86/kvm/lapic.c                 |   14 +
 arch/x86/kvm/mmu.c                   | 2078 ++++++++++++++++++++++++++++++++++
 arch/x86/kvm/mtrr.c                  |    4 +
 arch/x86/kvm/page_track.c            |    4 +
 arch/x86/kvm/vmx/ops.h               |    4 +
 arch/x86/kvm/vmx/vmenter.S           |   11 +
 arch/x86/kvm/vmx/vmx.c               |   62 +
 arch/x86/kvm/x86.c                   |  141 +++
 drivers/vhost/vhost.c                |   13 +
 drivers/vhost/vsock.c                |   36 +
 include/linux/kvm_host.h             |  102 ++
 include/linux/kvm_para.h             |   22 +
 include/uapi/linux/kvm_para.h        |   11 +
 virt/kvm/async_pf.c                  |   20 +
 virt/kvm/kvm_main.c                  |   97 ++
 21 files changed, 3614 insertions(+)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index bdc16b0..ce9c3dc 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -109,14 +109,55 @@ enum {
 	/* set max level to the biggest one */
 	PT_MAX_HUGEPAGE_LEVEL = PT_PDPE_LEVEL,
 };
+/* 3 - 1 + 1 = 3 */
 #define KVM_NR_PAGE_SIZES	(PT_MAX_HUGEPAGE_LEVEL - \
 				 PT_PAGE_TABLE_LEVEL + 1)
+/*
+ * x = 1: KVM_HPAGE_GFN_SHIFT(1) = 0   none
+ * x = 2: KVM_HPAGE_GFN_SHIFT(2) = 9   2M
+ * x = 3: KVM_HPAGE_GFN_SHIFT(3) = 18  1G
+ */
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+/*
+ * KVM_HPAGE_SHIFT(1) = 12 +  0 = 12
+ * KVM_HPAGE_SHIFT(2) = 12 +  9 = 21
+ * KVM_HPAGE_SHIFT(3) = 12 + 18 = 30
+ */
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
+/*
+ * KVM_HPAGE_SIZE(1) = 1 << 12 = 4K
+ * KVM_HPAGE_SIZE(2) = 1 << 21 = 2M
+ * KVM_HPAGE_SIZE(3) = 1 << 30 = 1G
+ */
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
+/*
+ * KVM_HPAGE_MASK(1) : mask了4K
+ * KVM_HPAGE_MASK(2) : mask了2M
+ * KVM_HPAGE_MASK(3) : mask了1G
+ */
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
+/*
+ * KVM_PAGES_PER_HPAGE(1) : 4K有多少4K的page
+ * KVM_PAGES_PER_HPAGE(2) : 2M有多少4K的page
+ * KVM_PAGES_PER_HPAGE(3) : 1G有多少4K的page
+ */
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1142| <<lpage_info_slot>> idx = gfn_to_index(gfn, slot->base_gfn, level);
+ *   - arch/x86/kvm/mmu.c|1411| <<__gfn_to_rmap>> idx = gfn_to_index(gfn, slot->base_gfn, level);
+ *   - arch/x86/kvm/page_track.c|68| <<update_gfn_track>> index = gfn_to_index(gfn, slot->base_gfn, PT_PAGE_TABLE_LEVEL);
+ *   - arch/x86/kvm/page_track.c|158| <<kvm_page_track_is_active>> index = gfn_to_index(gfn, slot->base_gfn, PT_PAGE_TABLE_LEVEL);
+ *   - arch/x86/kvm/x86.c|9510| <<kvm_arch_create_memslot>> lpages = gfn_to_index(slot->base_gfn + npages - 1,
+ *
+ * base_gfn是基于4k开始的gfn
+ * gfn是基于4k结束的gfn
+ * 计算从开始到结束需要用到几个hugepage (或者普通page)
+ *   level是1的时候hugepage大小是4K
+ *   level是2的时候hugepage大小是2M
+ *   level是3的时候hugepage大小是1G
+ */
 static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 {
 	/* KVM_HPAGE_GFN_SHIFT(PT_PAGE_TABLE_LEVEL) must be 0. */
@@ -209,6 +250,12 @@ enum {
 #define PFERR_PRESENT_MASK (1U << PFERR_PRESENT_BIT)
 #define PFERR_WRITE_MASK (1U << PFERR_WRITE_BIT)
 #define PFERR_USER_MASK (1U << PFERR_USER_BIT)
+/*
+ * 设置PFERR_RSVD_MASK的地方:
+ *   - arch/x86/kvm/paging_tmpl.h|404| <<FNAME(walk_addr_generic)>> errcode = PFERR_RSVD_MASK | PFERR_PRESENT_MASK;
+ *   - arch/x86/kvm/paging_tmpl.h|776| <<FNAME(page_fault)>> error_code &= ~PFERR_RSVD_MASK;
+ *   - arch/x86/kvm/vmx/vmx.c|5181| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 #define PFERR_RSVD_MASK (1U << PFERR_RSVD_BIT)
 #define PFERR_FETCH_MASK (1U << PFERR_FETCH_BIT)
 #define PFERR_PK_MASK (1U << PFERR_PK_BIT)
@@ -258,9 +305,23 @@ struct kvm_mmu_memory_cache {
 union kvm_mmu_page_role {
 	u32 word;
 	struct {
+		/*
+		 * The level in the shadow paging hierarchy that this shadow page belongs to.
+		 * 1=4k sptes, 2=2M sptes, 3=1G sptes, etc.
+		 */
 		unsigned level:4;
 		unsigned gpte_is_8_bytes:1;
 		unsigned quadrant:2;
+		/*
+		 * If set, leaf sptes reachable from this page are for a linear range.
+		 * Examples include real mode translation, large guest pages backed by small
+		 * host pages, and gpa->hpa translations when NPT or EPT is active.
+		 * The linear range starts at (gfn << PAGE_SHIFT) and its size is determined
+		 * by role.level (2MB for first level, 1GB for second level, 0.5TB for third
+		 * level, 256TB for fourth level)
+		 * If clear, this page corresponds to a guest page table denoted by the gfn
+		 * field.
+		 */
 		unsigned direct:1;
 		unsigned access:3;
 		unsigned invalid:1;
@@ -268,6 +329,11 @@ union kvm_mmu_page_role {
 		unsigned cr0_wp:1;
 		unsigned smep_andnot_wp:1;
 		unsigned smap_andnot_wp:1;
+		/*
+		 * Is 1 if the MMU instance cannot use A/D bits.  EPT did not have A/D
+		 * bits before Haswell; shadow EPT page tables also cannot use A/D bits
+		 * if the L1 hypervisor does not enable them.
+		 */
 		unsigned ad_disabled:1;
 		unsigned guest_mode:1;
 		unsigned :6;
@@ -313,13 +379,28 @@ union kvm_mmu_role {
 };
 
 struct kvm_rmap_head {
+	/*
+	 * 注释:
+	 *
+	 * If the bit zero of rmap_head->val is clear, then it points to the only spte
+	 * in this rmap chain. Otherwise, (rmap_head->val & ~1) points to a struct
+	 * pte_list_desc containing more mappings.
+	 */
 	unsigned long val;
 };
 
+/*
+ * 每个页表页(MMU page)对应一个数据结构kvm_mmu_page
+ */
 struct kvm_mmu_page {
 	struct list_head link;
 	struct hlist_node hash_link;
 	bool unsync;
+	/*
+	 * 在以下使用mmio_cached:
+	 *   - arch/x86/kvm/mmu.c|952| <<mark_mmio_spte>> page_header(__pa(sptep))->mmio_cached = true;
+	 *   - arch/x86/kvm/mmu.c|6916| <<__kvm_mmu_zap_all>> if (mmio_only && !sp->mmio_cached)
+	 */
 	bool mmio_cached;
 
 	/*
@@ -327,14 +408,71 @@ struct kvm_mmu_page {
 	 * hash table.
 	 */
 	union kvm_mmu_page_role role;
+	/*
+	 * 设置sp->gfn的地方:
+	 *   - arch/x86/kvm/mmu.c|4173| <<kvm_mmu_get_page>> sp->gfn = gfn;
+	 *
+	 * 其他使用sp->gfn的地方:
+	 *   - arch/x86/kvm/mmu.c|2292| <<kvm_mmu_page_get_gfn>> return sp->gfn + (index << ((sp->role.level - 1) * PT64_LEVEL_BITS));
+	 *   - arch/x86/kvm/mmu.c|2311| <<kvm_mmu_page_set_gfn>> sp->gfn,
+	 *   - arch/x86/kvm/mmu.c|2418| <<account_shadowed>> gfn = sp->gfn;
+	 *   - arch/x86/kvm/mmu.c|2443| <<unaccount_shadowed>> gfn = sp->gfn;
+	 *   - arch/x86/kvm/mmu.c|3099| <<drop_large_spte>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, sp->gfn,
+	 *   - arch/x86/kvm/mmu.c|3601| <<rmap_recycle>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, sp->gfn,
+	 *   - arch/x86/kvm/mmu.c|4068| <<mmu_sync_children>> protected |= rmap_write_protect(vcpu, sp->gfn);
+	 *   - arch/x86/kvm/mmu.c|4139| <<kvm_mmu_get_page>> if (sp->gfn != gfn) {
+	 *   - arch/x86/kvm/mmu.c|7767| <<kvm_mmu_zap_collapsible_spte>> kvm_flush_remote_tlbs_with_address(kvm, sp->gfn,
+	 *   - arch/x86/kvm/mmu_audit.c|145| <<inspect_spte_has_rmap>> (long int )(sptep - rev_sp->spt), rev_sp->gfn);
+	 *   - arch/x86/kvm/mmu_audit.c|202| <<audit_write_protection>> slot = __gfn_to_memslot(slots, sp->gfn);
+	 *   - arch/x86/kvm/mmu_audit.c|203| <<audit_write_protection>> rmap_head = __gfn_to_rmap(sp->gfn, PT_PAGE_TABLE_LEVEL, slot);
+	 *   - arch/x86/kvm/mmu_audit.c|209| <<audit_write_protection>> sp->gfn, sp->role.word);
+	 *   - arch/x86/kvm/paging_tmpl.h|872| <<FNAME(get_level1_sp_gpa)>> return gfn_to_gpa(sp->gfn) + offset * sizeof(pt_element_t);
+	 *   - arch/x86/kvm/paging_tmpl.h|913| <<FNAME(invlpg)>> sp->gfn, KVM_PAGES_PER_HPAGE(sp->role.level));
+	 */
 	gfn_t gfn;
 
+	/*
+	 * 在kvm_mmu_alloc_page()分配的真正的页表的4K page
+	 */
 	u64 *spt;
 	/* hold the gfn of each spte inside spt */
+	/*
+	 * 设置sp->gfns的地方:
+	 *   - arch/x86/kvm/mmu.c|2304| <<kvm_mmu_page_set_gfn>> sp->gfns[index] = gfn;
+	 * 
+	 * 使用和分配sp->gfns的地方:
+	 *   - arch/x86/kvm/mmu.c|2283| <<kvm_mmu_page_get_gfn>> return sp->gfns[index];
+	 *   - arch/x86/kvm/mmu.c|3672| <<kvm_mmu_free_page>> free_page((unsigned long )sp->gfns);
+	 *   - arch/x86/kvm/mmu.c|3734| <<kvm_mmu_alloc_page>> sp->gfns = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_cache);
+	 *   - arch/x86/kvm/paging_tmpl.h|1030| <<FNAME(sync_page)>> if (gfn != sp->gfns[i]) {
+	 *
+	 * 似乎tdp不用, shadow才用
+	 */
 	gfn_t *gfns;
 	int root_count;          /* Currently serving as active root */
 	unsigned int unsync_children;
 	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
+	/*
+	 * Zapping all pages (page generation count)
+	 *
+	 * For the large memory guests, walking and zapping all pages is really slow
+	 * (because there are a lot of pages), and also blocks memory accesses of
+	 * all VCPUs because it needs to hold the MMU lock.
+	 *
+	 * To make it be more scalable, kvm maintains a global generation number
+	 * which is stored in kvm->arch.mmu_valid_gen.  Every shadow page stores
+	 * the current global generation-number into sp->mmu_valid_gen when it
+	 * is created.  Pages with a mismatching generation number are "obsolete".
+	 *
+	 * When KVM need zap all shadow pages sptes, it just simply increases the global
+	 * generation-number then reload root shadow pages on all vcpus.  As the VCPUs
+	 * create new shadow page tables, the old pages are not used because of the
+	 * mismatching generation number.
+	 *
+	 * KVM then walks through all pages and zaps obsolete pages.  While the zap
+	 * operation needs to take the MMU lock, the lock can be released periodically
+	 * so that the VCPUs can make progress.
+	 */
 	unsigned long mmu_valid_gen;
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
@@ -347,6 +485,12 @@ struct kvm_mmu_page {
 #endif
 
 	/* Number of writes since the last time traversal visited this page.  */
+	/*
+	 * 修改和使用write_flooding_count的地方:
+	 *   - arch/x86/kvm/mmu.c|3559| <<__clear_sp_write_flooding_count>> atomic_set(&sp->write_flooding_count, 0);
+	 *   - arch/x86/kvm/mmu.c|6392| <<detect_write_flooding>> atomic_inc(&sp->write_flooding_count);
+	 *   - arch/x86/kvm/mmu.c|6393| <<detect_write_flooding>> return atomic_read(&sp->write_flooding_count) >= 3;
+	 */
 	atomic_t write_flooding_count;
 };
 
@@ -403,6 +547,14 @@ struct kvm_mmu {
 	u8 root_level;
 	u8 shadow_root_level;
 	u8 ept_ad;
+	/*
+	 * 在以下修改direct_map:
+	 *   - arch/x86/kvm/mmu.c|5833| <<nonpaging_init_context>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|6416| <<paging64_init_context_common>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|6445| <<paging32_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|6549| <<init_kvm_tdp_mmu>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|6700| <<kvm_init_shadow_ept_mmu>> context->direct_map = false;
+	 */
 	bool direct_map;
 	struct kvm_mmu_root_info prev_roots[KVM_MMU_NUM_PREV_ROOTS];
 
@@ -778,6 +930,12 @@ struct kvm_vcpu_arch {
 	int pending_external_vector;
 
 	/* GPA available */
+	/*
+	 * 在以下设置使用gpa_available:
+	 *   - arch/x86/kvm/mmu.c|7117| <<kvm_mmu_page_fault>> vcpu->arch.gpa_available = true;
+	 *   - arch/x86/kvm/x86.c|5498| <<emulator_read_write_onepage>> if (vcpu->arch.gpa_available &&
+	 *   - arch/x86/kvm/x86.c|8106| <<vcpu_enter_guest>> vcpu->arch.gpa_available = false;
+	 */
 	bool gpa_available;
 	gpa_t gpa_val;
 
@@ -792,12 +950,39 @@ struct kvm_vcpu_arch {
 };
 
 struct kvm_lpage_info {
+	/*
+	 * 在以下修改disallow_lpage:
+	 *   - arch/x86/kvm/mmu.c|1154| <<update_gfn_disallow_lpage_count>> linfo->disallow_lpage += count;
+	 *   - arch/x86/kvm/x86.c|9528| <<kvm_arch_create_memslot>> linfo[0].disallow_lpage = 1;
+	 *   - arch/x86/kvm/x86.c|9530| <<kvm_arch_create_memslot>> linfo[lpages - 1].disallow_lpage = 1;
+	 *   - arch/x86/kvm/x86.c|9542| <<kvm_arch_create_memslot>> linfo[j].disallow_lpage = 1;
+	 *
+	 * 在以下使用disallow_lpage:
+	 *   - arch/x86/kvm/mmu.c|2112| <<update_gfn_disallow_lpage_count>> WARN_ON(linfo->disallow_lpage < 0);
+	 *   - arch/x86/kvm/mmu.c|2177| <<__mmu_gfn_lpage_is_disallowed>> return !!linfo->disallow_lpage;
+	 */
 	int disallow_lpage;
 };
 
 struct kvm_arch_memory_slot {
+	/*
+	 * About rmap_head encoding:
+	 *
+	 * If the bit zero of rmap_head->val is clear, then it points to the only spte
+	 * in this rmap chain. Otherwise, (rmap_head->val & ~1) points to a struct
+	 * pte_list_desc containing more mappings.
+	 *
+	 * rmap和lpage_info的第二维都是在kvm_arch_create_memslot()分配
+	 * 数量是对应level的page的数量
+	 * level越大(大页)数量越少
+	 *
+	 * KVM_NR_PAGE_SIZES = 3 !!!!!
+	 */
 	struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+	/*
+	 * 应该是每个slot中page的数量吧
+	 */
 	unsigned short *gfn_track[KVM_PAGE_TRACK_MAX];
 };
 
@@ -857,7 +1042,31 @@ struct kvm_arch {
 	unsigned long n_requested_mmu_pages;
 	unsigned long n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
+	/*
+	 * Zapping all pages (page generation count)
+	 *
+	 * For the large memory guests, walking and zapping all pages is really slow
+	 * (because there are a lot of pages), and also blocks memory accesses of
+	 * all VCPUs because it needs to hold the MMU lock.
+	 *
+	 * To make it be more scalable, kvm maintains a global generation number
+	 * which is stored in kvm->arch.mmu_valid_gen.  Every shadow page stores
+	 * the current global generation-number into sp->mmu_valid_gen when it
+	 * is created.  Pages with a mismatching generation number are "obsolete".
+	 *
+	 * When KVM need zap all shadow pages sptes, it just simply increases the global
+	 * generation-number then reload root shadow pages on all vcpus.  As the VCPUs
+	 * create new shadow page tables, the old pages are not used because of the
+	 * mismatching generation number.
+	 *
+	 * KVM then walks through all pages and zaps obsolete pages.  While the zap
+	 * operation needs to take the MMU lock, the lock can be released periodically
+	 * so that the VCPUs can make progress.
+	 */
 	unsigned long mmu_valid_gen;
+	/*
+	 * 通过hash可以快速获得一个gfn对应的也表页面
+	 */
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	/*
 	 * Hash table of struct kvm_mmu_page.
@@ -972,6 +1181,10 @@ struct kvm_vcpu_stat {
 	u64 halt_poll_invalid;
 	u64 halt_wakeup;
 	u64 request_irq_exits;
+	/*
+	 * 在以下增加irq_exits:
+	 *   - arch/x86/kvm/vmx/vmx.c|4589| <<handle_external_interrupt>> ++vcpu->stat.irq_exits;
+	 */
 	u64 irq_exits;
 	u64 host_state_reload;
 	u64 fpu_reload;
@@ -1002,6 +1215,10 @@ struct kvm_lapic_irq {
 	bool msi_redir_hint;
 };
 
+/*
+ * intel : vmx_x86_ops
+ * amd   : svm_x86_ops
+ */
 struct kvm_x86_ops {
 	int (*cpu_has_kvm_support)(void);          /* __init */
 	int (*disabled_by_bios)(void);             /* __init */
@@ -1226,6 +1443,10 @@ extern struct kmem_cache *x86_fpu_cache;
 #define __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)
 {
+	/*
+	 * intel: vmx_vm_alloc()
+	 * amd  : svm_vm_alloc()
+	 */
 	return kvm_x86_ops->vm_alloc();
 }
 
@@ -1434,8 +1655,16 @@ static inline gpa_t translate_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 	return gpa;
 }
 
+/*
+ * shadow_page可能是一个指向某个pte的地址 (shadow_page保存的这个pte的地址而不是内容)
+ * 获得包含这个地址的页表页对应的kvm_mmu_page
+ */
 static inline struct kvm_mmu_page *page_header(hpa_t shadow_page)
 {
+	/*
+	 * shadow_page可能是一个指向某个pte的地址 (shadow_page保存的这个pte的地址而不是内容)
+	 * shadow_page向右移动PAGE_SHIFT是这个pte所在的页表页的基地址的pfn
+	 */
 	struct page *page = pfn_to_page(shadow_page >> PAGE_SHIFT);
 
 	return (struct kvm_mmu_page *)page_private(page);
diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 6e0a3b4..04dd0e0 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -941,6 +941,13 @@ static inline u16 amd_get_nb_id(int cpu)		{ return 0; }
 static inline u32 amd_get_nodes_per_socket(void)	{ return 0; }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/xen/hypervisor.h|43| <<xen_cpuid_base>> return hypervisor_cpuid_base("XenVMMXenVMM", 2);
+ *   - arch/x86/kernel/cpu/acrn.c|21| <<acrn_detect>> return hypervisor_cpuid_base("ACRNACRNACRN\0\0", 0);
+ *   - arch/x86/kernel/jailhouse.c|33| <<jailhouse_cpuid_base>> return hypervisor_cpuid_base("Jailhouse\0\0\0", 0);
+ *   - arch/x86/kernel/kvm.c|919| <<__kvm_cpuid_base>> return hypervisor_cpuid_base("KVMKVMKVM\0\0\0", 0);
+ */
 static inline uint32_t hypervisor_cpuid_base(const char *sig, uint32_t leaves)
 {
 	uint32_t base, eax, signature[3];
diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index 2a8e0b6..4998dac 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -15,23 +15,104 @@
  * in edx.
  */
 #define KVM_CPUID_FEATURES	0x40000001
+/*
+ * 在以下使用KVM_FEATURE_CLOCKSOURCE:
+ *   - arch/x86/kernel/kvmclock.c|392| <<kvmclock_init>> } else if (!kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE)) {
+ *   - arch/x86/kvm/cpuid.c|687| <<__do_cpuid_func>> entry->eax = (1 << KVM_FEATURE_CLOCKSOURCE) |
+ */
 #define KVM_FEATURE_CLOCKSOURCE		0
+/*
+ * 在以下使用KVM_FEATURE_NOP_IO_DELAY:
+ *   - arch/x86/kernel/kvm.c|472| <<paravirt_ops_setup>> if (kvm_para_has_feature(KVM_FEATURE_NOP_IO_DELAY))
+ *   - arch/x86/kvm/cpuid.c|688| <<__do_cpuid_func>> (1 << KVM_FEATURE_NOP_IO_DELAY) |
+ */
 #define KVM_FEATURE_NOP_IO_DELAY	1
+/*
+ * 没人使用
+ */
 #define KVM_FEATURE_MMU_OP		2
 /* This indicates that the new set of kvmclock msrs
  * are available. The use of 0x11 and 0x12 is deprecated
  */
+/*
+ * 在以下使用KVM_FEATURE_CLOCKSOURCE2:
+ *   - arch/x86/kernel/kvmclock.c|389| <<kvmclock_init>> if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE2)) {
+ *   - arch/x86/kvm/cpuid.c|689| <<__do_cpuid_func>> (1 << KVM_FEATURE_CLOCKSOURCE2) |
+ */
 #define KVM_FEATURE_CLOCKSOURCE2        3
+/*
+ * 在以下使用KVM_FEATURE_ASYNC_PF:
+ *   - arch/x86/kernel/kvm.c|437| <<kvm_guest_cpu_init>> if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF) && kvmapf) {
+ *   - arch/x86/kernel/kvm.c|866| <<kvm_guest_init>> if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF))
+ *   - arch/x86/kvm/cpuid.c|690| <<__do_cpuid_func>> (1 << KVM_FEATURE_ASYNC_PF) |
+ */
 #define KVM_FEATURE_ASYNC_PF		4
+/*
+ * 在以下使用KVM_FEATURE_STEAL_TIME:
+ *   - arch/x86/kernel/kvm.c|1065| <<kvm_guest_init>> if (kvm_para_has_feature(KVM_FEATURE_STEAL_TIME)) {
+ *   - arch/x86/kernel/kvm.c|1072| <<kvm_guest_init>> kvm_para_has_feature(KVM_FEATURE_STEAL_TIME)) {
+ *   - arch/x86/kernel/kvm.c|1085| <<kvm_guest_init>> kvm_para_has_feature(KVM_FEATURE_STEAL_TIME)) {
+ *   - arch/x86/kernel/kvm.c|1243| <<kvm_setup_pv_tlb_flush>> kvm_para_has_feature(KVM_FEATURE_STEAL_TIME)) {
+ *   - arch/x86/kernel/kvm.c|1367| <<kvm_spinlock_init>> if (kvm_para_has_feature(KVM_FEATURE_STEAL_TIME)) {
+ *   - arch/x86/kvm/cpuid.c|701| <<__do_cpuid_func>> entry->eax |= (1 << KVM_FEATURE_STEAL_TIME);
+ */
 #define KVM_FEATURE_STEAL_TIME		5
+/*
+ * 在以下使用KVM_FEATURE_PV_EOI:
+ *   - arch/x86/kernel/kvm.c|618| <<kvm_guest_cpu_init>> if (kvm_para_has_feature(KVM_FEATURE_PV_EOI)) {
+ *   - arch/x86/kernel/kvm.c|668| <<kvm_pv_guest_cpu_reboot>> if (kvm_para_has_feature(KVM_FEATURE_PV_EOI))
+ *   - arch/x86/kernel/kvm.c|975| <<kvm_guest_cpu_offline>> if (kvm_para_has_feature(KVM_FEATURE_PV_EOI))
+ *   - arch/x86/kernel/kvm.c|1077| <<kvm_guest_init>> if (kvm_para_has_feature(KVM_FEATURE_PV_EOI))
+ *   - arch/x86/kvm/cpuid.c|691| <<__do_cpuid_func>> (1 << KVM_FEATURE_PV_EOI) |
+ */
 #define KVM_FEATURE_PV_EOI		6
+/*
+ * 在以下使用KVM_FEATURE_PV_UNHALT:
+ *   - arch/x86/kernel/kvm.c|1309| <<kvm_spinlock_init>> if (!kvm_para_has_feature(KVM_FEATURE_PV_UNHALT))
+ *   - arch/x86/kvm/cpuid.c|134| <<kvm_update_cpuid>> (best->eax & (1 << KVM_FEATURE_PV_UNHALT)))
+ *   - arch/x86/kvm/cpuid.c|135| <<kvm_update_cpuid>> best->eax &= ~(1 << KVM_FEATURE_PV_UNHALT);
+ *   - arch/x86/kvm/cpuid.c|693| <<__do_cpuid_func>> (1 << KVM_FEATURE_PV_UNHALT) |
+ */
 #define KVM_FEATURE_PV_UNHALT		7
+/*
+ * 在以下使用KVM_FEATURE_PV_TLB_FLUSH:
+ *   - arch/x86/kernel/kvm.c|1029| <<kvm_guest_init>> if (kvm_para_has_feature(KVM_FEATURE_PV_TLB_FLUSH) &&
+ *   - arch/x86/kernel/kvm.c|1200| <<kvm_setup_pv_tlb_flush>> if (kvm_para_has_feature(KVM_FEATURE_PV_TLB_FLUSH) &&
+ *   - arch/x86/kvm/cpuid.c|694| <<__do_cpuid_func>> (1 << KVM_FEATURE_PV_TLB_FLUSH) |
+ */
 #define KVM_FEATURE_PV_TLB_FLUSH	9
+/*
+ * 在以下使用KVM_FEATURE_ASYNC_PF_VMEXIT:
+ *   - arch/x86/kernel/kvm.c|569| <<kvm_guest_cpu_init>> if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_VMEXIT))
+ *   - arch/x86/kvm/cpuid.c|695| <<__do_cpuid_func>> (1 << KVM_FEATURE_ASYNC_PF_VMEXIT) |
+ */
 #define KVM_FEATURE_ASYNC_PF_VMEXIT	10
+/*
+ * 在以下使用KVM_FEATURE_PV_SEND_IPI:
+ *   - arch/x86/kernel/kvm.c|1151| <<kvm_apic_init>> if (kvm_para_has_feature(KVM_FEATURE_PV_SEND_IPI))
+ *   - arch/x86/kvm/cpuid.c|696| <<__do_cpuid_func>> (1 << KVM_FEATURE_PV_SEND_IPI) |
+ */
 #define KVM_FEATURE_PV_SEND_IPI	11
+/*
+ * 在以下使用KVM_FEATURE_POLL_CONTROL:
+ *   - arch/x86/kvm/cpuid.c|697| <<__do_cpuid_func>> (1 << KVM_FEATURE_POLL_CONTROL) |
+ */
 #define KVM_FEATURE_POLL_CONTROL	12
+/*
+ * 在以下使用KVM_FEATURE_PV_SCHED_YIELD:
+ *   - arch/x86/kernel/kvm.c|1042| <<kvm_guest_init>> if (kvm_para_has_feature(KVM_FEATURE_PV_SCHED_YIELD) &&
+ *   - arch/x86/kvm/cpuid.c|698| <<__do_cpuid_func>> (1 << KVM_FEATURE_PV_SCHED_YIELD);
+ */
 #define KVM_FEATURE_PV_SCHED_YIELD	13
 
+/*
+ * 在以下使用KVM_HINTS_REALTIME:
+ *   - arch/x86/kernel/kvm.c|933| <<kvm_smp_prepare_cpus>> if (kvm_para_has_hint(KVM_HINTS_REALTIME))
+ *   - arch/x86/kernel/kvm.c|1061| <<kvm_guest_init>> !kvm_para_has_hint(KVM_HINTS_REALTIME) &&
+ *   - arch/x86/kernel/kvm.c|1074| <<kvm_guest_init>> !kvm_para_has_hint(KVM_HINTS_REALTIME) &&
+ *   - arch/x86/kernel/kvm.c|1232| <<kvm_setup_pv_tlb_flush>> !kvm_para_has_hint(KVM_HINTS_REALTIME) &&
+ *   - arch/x86/kernel/kvm.c|1343| <<kvm_spinlock_init>> if (kvm_para_has_hint(KVM_HINTS_REALTIME))
+ */
 #define KVM_HINTS_REALTIME      0
 
 /* The last 8 bits are used to indicate how to interpret the flags field
@@ -46,6 +127,14 @@
 /* Custom MSRs falls in the range 0x4b564d00-0x4b564dff */
 #define MSR_KVM_WALL_CLOCK_NEW  0x4b564d00
 #define MSR_KVM_SYSTEM_TIME_NEW 0x4b564d01
+/*
+ * 在以下使用MSR_KVM_ASYNC_PF_EN:
+ *   - arch/x86/kvm/x86.c|1164| <<global>> MSR_KVM_ASYNC_PF_EN, MSR_KVM_STEAL_TIME,
+ *   - arch/x86/kernel/kvm.c|448| <<kvm_guest_cpu_init>> wrmsrl(MSR_KVM_ASYNC_PF_EN, pa);
+ *   - arch/x86/kernel/kvm.c|478| <<kvm_pv_disable_apf>> wrmsrl(MSR_KVM_ASYNC_PF_EN, 0);
+ *   - arch/x86/kvm/x86.c|2624| <<kvm_set_msr_common>> case MSR_KVM_ASYNC_PF_EN:
+ *   - arch/x86/kvm/x86.c|2905| <<kvm_get_msr_common>> case MSR_KVM_ASYNC_PF_EN:
+ */
 #define MSR_KVM_ASYNC_PF_EN 0x4b564d02
 #define MSR_KVM_STEAL_TIME  0x4b564d03
 #define MSR_KVM_PV_EOI_EN      0x4b564d04
@@ -60,6 +149,12 @@ struct kvm_steal_time {
 	__u32 pad[11];
 };
 
+/*
+ * 在以下使用KVM_VCPU_PREEMPTED:
+ *   - arch/x86/kernel/kvm.c|1042| <<kvm_flush_tlb_others>> if ((state & KVM_VCPU_PREEMPTED)) {
+ *   - arch/x86/kernel/kvm.c|1309| <<__kvm_vcpu_is_preempted>> return !!(src->preempted & KVM_VCPU_PREEMPTED);
+ *   - arch/x86/kvm/x86.c|3373| <<kvm_steal_time_set_preempted>> vcpu->arch.st.steal.preempted = KVM_VCPU_PREEMPTED;
+ */
 #define KVM_VCPU_PREEMPTED          (1 << 0)
 #define KVM_VCPU_FLUSH_TLB          (1 << 1)
 
@@ -108,7 +203,20 @@ struct kvm_mmu_op_release_pt {
 	__u64 pt_phys;
 };
 
+/*
+ * 在以下使用KVM_PV_REASON_PAGE_NOT_PRESENT:
+ *   - arch/x86/kernel/kvm.c|353| <<do_async_page_fault>> case KVM_PV_REASON_PAGE_NOT_PRESENT:
+ *   - arch/x86/kvm/mmu.c|5947| <<kvm_handle_page_fault>> case KVM_PV_REASON_PAGE_NOT_PRESENT:
+ *   - arch/x86/kvm/x86.c|10025| <<kvm_arch_async_page_not_present>> !apf_put_user(vcpu, KVM_PV_REASON_PAGE_NOT_PRESENT)) {
+ *   - arch/x86/kvm/x86.c|10060| <<kvm_arch_async_page_present>> if (val == KVM_PV_REASON_PAGE_NOT_PRESENT &&
+ */
 #define KVM_PV_REASON_PAGE_NOT_PRESENT 1
+/*
+ * 在以下使用KVM_PV_REASON_PAGE_READY:
+ *   - arch/x86/kernel/kvm.c|359| <<do_async_page_fault>> case KVM_PV_REASON_PAGE_READY:
+ *   - arch/x86/kvm/mmu.c|5953| <<kvm_handle_page_fault>> case KVM_PV_REASON_PAGE_READY:
+ *   - arch/x86/kvm/x86.c|10071| <<kvm_arch_async_page_present>> } else if (!apf_put_user(vcpu, KVM_PV_REASON_PAGE_READY)) {
+ */
 #define KVM_PV_REASON_PAGE_READY 2
 
 struct kvm_vcpu_pv_apf_data {
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index 08fb79f..92f27c7 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -1118,6 +1118,9 @@ static void local_apic_timer_interrupt(void)
  * [ if a single-CPU system runs an SMP kernel then we call the local
  *   interrupt as well. Thus we cannot inline the local irq ... ]
  */
+/*
+ *  arch/x86/entry/entry_64.S: 用作LOCAL_TIMER_VECTOR (0xec)的处理函数
+ */
 __visible void __irq_entry smp_apic_timer_interrupt(struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index 4ab377c9..8f13863 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -34,6 +34,61 @@
 #include <asm/hypervisor.h>
 #include <asm/tlb.h>
 
+/*
+ * Documentation/virt/kvm/msr.txt
+ * kvm的pv选项:
+ *   - apf
+ *   - steal clock
+ *   - pv-eoi
+ *   - pv-ipi
+ */
+/*
+ * apf
+ *
+ * 1. 进程访问被swap出去的内存页从而触发page fault,kvm mmu尝试apf方式.
+ * 2. 将具体的处理逻辑交给apf worker,然后通过注入page not present的异常通知guest.
+ * 3. guest进入异常处理逻辑,将该进程block住,然后reschedule运行其他进程.
+ * 4. apf work完成page换进工作后通知guest,page页已经准备好.guest重新将block的进程调度进来,使该进程正常进行.
+ *
+ * https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2019/03/24/kvm-async-page-fault
+ * https://www.linux-kvm.org/images/a/ac/2010-forum-Async-page-faults.pdf
+ * https://www.kernelnote.com/entry/kvmguestswap
+ */
+/*
+ * pv-eoi (减少vmexit的数量)
+ *
+ * x86 PC体系架构中的中断控制器,早先是8259A,现在更普遍使用的是APIC,他们处理中断的流程遵循如下流程:
+ *
+ * 1. 外部设备产生一个中断,如果该中断没有被屏蔽掉,中断控制器将IRR寄存器中相应的位置1,表示收到中断,但是还未提交给CPU处理.
+ * 2. 中断控制器将该中断提交给CPU,CPU收到中断请求后,会应答中断控制器.
+ * 3. 中断控制器收到CPU的中断应答后,将IRR寄存器中相应的位清0,并将ISR寄存器相应的位置1,表示CPU正在处理该中断.
+ * 4. 当该中断的处理程序结束以前,需要将中断控制器的EOI寄存器对应的位置1,表示CPU完成了对该中断的处理.
+ * 5. 中断控制器收到EOI后,ISR寄存器中相应的位清0，允许下次中断.
+ * 6. 在虚拟化场景中,该流程至少会导致两次VM Exit: 第一次是VMM截获到设备中断的时候,通知客户机退出,将这个中断注入到客户机中;
+ *    另外一次是当客户机操作系统处理完该中断后,写中断控制器的EOI寄存器,这是个MMIO操作,也会导致客户机退出.
+ *    在一个外部IO比较频繁的场景中,外部中断会导致大量的VM Exit,影响客户机的整体性能.
+ *
+ * PV-EOI其实就是通过半虚拟化的办法来优化上述的VM Exit影响,virtio也是使用这个思想来优化网络和磁盘;就EOI的优化来说,其思想本质上很简单:
+ *
+ * 1. 客户机和VMM协商,首先确定双方是否都能支持PV-EOI特性,如果成功,则进一步协商一块2 bytes的内存区间作为双方处理EOI的共享缓存;
+ * 2. 在VMM向客户机注入中断之前,会把缓存的最低位置1,表示客户机不需要通过写EOI寄存器;
+ * 3. 客户机在写EOI之前,如果发现该位被设置,则将该位清0;VMM轮询这个标志位,当检查到清0后,会更新模拟中断控制器中的EOI寄存器;
+ *    如果客户机发现该位未被设置,则继续使用MMIO或者MSR写EOI寄存器;
+ *
+ * 需要注意的是,为了保证客户机和VMM同时处理共享内存的性能和可靠性,目前KVM的PV-EOF方案采用了如下的优化措施:
+ *
+ * 1. VMM保障仅会在客户机VCPU的上下文中更改共享内存中的最低位,从而避免了客户机采用任何锁机制来与VMM进行同步;
+ * 2. 客户机必须使用原子的test_and_clear操作来更改共享内存中的最低位,这是因为VMM在任何时候都有可能设置或者清除该位;
+ *
+ * https://blog.csdn.net/luo_brian/article/details/8744025?utm_source=tuicool&utm_medium=referral
+ * https://fedoraproject.org/wiki/QA:Testcase_Virtualization_PV_EOI
+ */
+
+/*
+ * 在以下使用kvmapf:
+ *   - arch/x86/kernel/kvm.c|41| <<parse_no_kvmapf>> kvmapf = 0;
+ *   - arch/x86/kernel/kvm.c|311| <<kvm_guest_cpu_init>> if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF) && kvmapf) {
+ */
 static int kvmapf = 1;
 
 static int __init parse_no_kvmapf(char *arg)
@@ -44,6 +99,11 @@ static int __init parse_no_kvmapf(char *arg)
 
 early_param("no-kvmapf", parse_no_kvmapf);
 
+/*
+ * 在以下使用steal_acc:
+ *   - arch/x86/kernel/kvm.c|50| <<parse_no_stealacc>> steal_acc = 0;
+ *   - arch/x86/kernel/kvm.c|741| <<activate_jump_labels>> if (steal_acc)
+ */
 static int steal_acc = 1;
 static int __init parse_no_stealacc(char *arg)
 {
@@ -53,18 +113,67 @@ static int __init parse_no_stealacc(char *arg)
 
 early_param("no-steal-acc", parse_no_stealacc);
 
+/*
+ * 在以下使用apf_reason:
+ *   - arch/x86/kernel/kvm.c|234| <<kvm_read_and_reset_pf_reason>> if (__this_cpu_read(apf_reason.enabled)) {
+ *   - arch/x86/kernel/kvm.c|235| <<kvm_read_and_reset_pf_reason>> reason = __this_cpu_read(apf_reason.reason);
+ *   - arch/x86/kernel/kvm.c|236| <<kvm_read_and_reset_pf_reason>> __this_cpu_write(apf_reason.reason, 0);
+ *   - arch/x86/kernel/kvm.c|312| <<kvm_guest_cpu_init>> u64 pa = slow_virt_to_phys(this_cpu_ptr(&apf_reason));
+ *   - arch/x86/kernel/kvm.c|323| <<kvm_guest_cpu_init>> __this_cpu_write(apf_reason.enabled, 1);
+ *   - arch/x86/kernel/kvm.c|344| <<kvm_pv_disable_apf>> if (!__this_cpu_read(apf_reason.enabled))
+ *   - arch/x86/kernel/kvm.c|348| <<kvm_pv_disable_apf>> __this_cpu_write(apf_reason.enabled, 0);
+ *   - arch/x86/kernel/kvm.c|425| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(apf_reason, cpu), sizeof(apf_reason));
+ */
 static DEFINE_PER_CPU_DECRYPTED(struct kvm_vcpu_pv_apf_data, apf_reason) __aligned(64);
+/*
+ * 在以下使用steal_time:
+ *   - arch/x86/kernel/kvm.c|283| <<kvm_register_steal_time>> struct kvm_steal_time *st = &per_cpu(steal_time, cpu);
+ *   - arch/x86/kernel/kvm.c|385| <<kvm_steal_clock>> src = &per_cpu(steal_time, cpu);
+ *   - arch/x86/kernel/kvm.c|426| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(steal_time, cpu), sizeof(steal_time));
+ *   - arch/x86/kernel/kvm.c|609| <<kvm_flush_tlb_others>> src = &per_cpu(steal_time, cpu);
+ *   - arch/x86/kernel/kvm.c|810| <<__kvm_vcpu_is_preempted>> struct kvm_steal_time *src = &per_cpu(steal_time, cpu);
+ */
 DEFINE_PER_CPU_DECRYPTED(struct kvm_steal_time, steal_time) __aligned(64) __visible;
+/*
+ * 在以下使用has_steal_clock:
+ *   - arch/x86/kernel/kvm.c|285| <<kvm_register_steal_time>> if (!has_steal_clock)
+ *   - arch/x86/kernel/kvm.c|338| <<kvm_guest_cpu_init>> if (has_steal_clock)
+ *   - arch/x86/kernel/kvm.c|398| <<kvm_disable_steal_time>> if (!has_steal_clock)
+ *   - arch/x86/kernel/kvm.c|633| <<kvm_guest_init>> has_steal_clock = 1;
+ *   - arch/x86/kernel/kvm.c|739| <<activate_jump_labels>> if (has_steal_clock) {
+ */
 static int has_steal_clock = 0;
 
 /*
  * No need for any "IO delay" on KVM
  */
+/*
+ * called by:
+ *   - arch/x86/include/asm/paravirt.h|42| <<slow_down_io>> pv_ops.cpu.io_delay();
+ *   - arch/x86/include/asm/paravirt.h|44| <<slow_down_io>> pv_ops.cpu.io_delay();
+ *   - arch/x86/include/asm/paravirt.h|45| <<slow_down_io>> pv_ops.cpu.io_delay();
+ *   - arch/x86/include/asm/paravirt.h|46| <<slow_down_io>> pv_ops.cpu.io_delay();
+ *
+ * used by:
+ *   - arch/x86/kernel/kvm.c|328| <<paravirt_ops_setup>> pv_ops.cpu.io_delay = kvm_io_delay;
+ */
 static void kvm_io_delay(void)
 {
 }
 
+/*
+ * 在以下使用KVM_TASK_SLEEP_HASHBITS:
+ *   - arch/x86/kernel/kvm.c|105| <<KVM_TASK_SLEEP_HASHSIZE>> #define KVM_TASK_SLEEP_HASHSIZE (1<<KVM_TASK_SLEEP_HASHBITS)
+ *   - arch/x86/kernel/kvm.c|141| <<kvm_async_pf_task_wait>> u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
+ *   - arch/x86/kernel/kvm.c|229| <<kvm_async_pf_task_wake>> u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
+ */
 #define KVM_TASK_SLEEP_HASHBITS 8
+/*
+ * 在以下使用KVM_TASK_SLEEP_HASHSIZE:
+ *   - arch/x86/kernel/kvm.c|118| <<global>> } async_pf_sleepers[KVM_TASK_SLEEP_HASHSIZE];
+ *   - arch/x86/kernel/kvm.c|213| <<apf_task_wake_all>> for (i = 0; i < KVM_TASK_SLEEP_HASHSIZE; i++) {
+ *   - arch/x86/kernel/kvm.c|711| <<kvm_guest_init>> for (i = 0; i < KVM_TASK_SLEEP_HASHSIZE; i++)
+ */
 #define KVM_TASK_SLEEP_HASHSIZE (1<<KVM_TASK_SLEEP_HASHBITS)
 
 struct kvm_task_sleep_node {
@@ -75,11 +184,26 @@ struct kvm_task_sleep_node {
 	bool halted;
 };
 
+/*
+ * 在以下使用async_pf_sleepers[]:
+ *   - arch/x86/kernel/kvm.c|168| <<kvm_async_pf_task_wait>> struct kvm_task_sleep_head *b = &async_pf_sleepers[key];
+ *   - arch/x86/kernel/kvm.c|251| <<apf_task_wake_all>> struct kvm_task_sleep_head *b = &async_pf_sleepers[i];
+ *   - arch/x86/kernel/kvm.c|271| <<kvm_async_pf_task_wake>> struct kvm_task_sleep_head *b = &async_pf_sleepers[key];
+ *   - arch/x86/kernel/kvm.c|878| <<kvm_guest_init>> raw_spin_lock_init(&async_pf_sleepers[i].lock);
+ */
 static struct kvm_task_sleep_head {
 	raw_spinlock_t lock;
 	struct hlist_head list;
 } async_pf_sleepers[KVM_TASK_SLEEP_HASHSIZE];
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|149| <<kvm_async_pf_task_wait>> e = _find_apf_task(b, token);
+ *   - arch/x86/kernel/kvm.c|240| <<kvm_async_pf_task_wake>> n = _find_apf_task(b, token);
+ *
+ * 遍历kvm_task_sleep_head->list中每一个kvm_task_sleep_node,
+ * 返回和参数token相等的kvm_task_sleep_node->token的kvm_task_sleep_node
+ */
 static struct kvm_task_sleep_node *_find_apf_task(struct kvm_task_sleep_head *b,
 						  u32 token)
 {
@@ -99,6 +223,11 @@ static struct kvm_task_sleep_node *_find_apf_task(struct kvm_task_sleep_head *b,
  * @interrupt_kernel: Is this called from a routine which interrupts the kernel
  * 		      (other than user space)?
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|311| <<do_async_page_fault>> kvm_async_pf_task_wait((u32)address, !user_mode(regs));
+ *   - arch/x86/kvm/mmu.c|5950| <<kvm_handle_page_fault>> kvm_async_pf_task_wait(fault_address, 0);
+ */
 void kvm_async_pf_task_wait(u32 token, int interrupt_kernel)
 {
 	u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
@@ -109,6 +238,10 @@ void kvm_async_pf_task_wait(u32 token, int interrupt_kernel)
 	rcu_irq_enter();
 
 	raw_spin_lock(&b->lock);
+	/*
+	 * 遍历kvm_task_sleep_head->list中每一个kvm_task_sleep_node,
+	 * 返回和参数token相等的kvm_task_sleep_node->token的kvm_task_sleep_node
+	 */
 	e = _find_apf_task(b, token);
 	if (e) {
 		/* dummy entry exist -> wake up was delivered ahead of PF */
@@ -160,6 +293,11 @@ void kvm_async_pf_task_wait(u32 token, int interrupt_kernel)
 }
 EXPORT_SYMBOL_GPL(kvm_async_pf_task_wait);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|221| <<apf_task_wake_all>> apf_task_wake_one(n);
+ *   - arch/x86/kernel/kvm.c|261| <<kvm_async_pf_task_wake>> apf_task_wake_one(n);
+ */
 static void apf_task_wake_one(struct kvm_task_sleep_node *n)
 {
 	hlist_del_init(&n->link);
@@ -169,6 +307,11 @@ static void apf_task_wake_one(struct kvm_task_sleep_node *n)
 		swake_up_one(&n->wq);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|234| <<kvm_async_pf_task_wake>> apf_task_wake_all();
+ *   - arch/x86/kernel/kvm.c|637| <<kvm_guest_cpu_offline>> apf_task_wake_all();
+ */
 static void apf_task_wake_all(void)
 {
 	int i;
@@ -187,6 +330,11 @@ static void apf_task_wake_all(void)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|316| <<do_async_page_fault>> kvm_async_pf_task_wake((u32)address);
+ *   - arch/x86/kvm/mmu.c|5956| <<kvm_handle_page_fault>> kvm_async_pf_task_wake(fault_address);
+ */
 void kvm_async_pf_task_wake(u32 token)
 {
 	u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
@@ -227,10 +375,29 @@ void kvm_async_pf_task_wake(u32 token)
 }
 EXPORT_SYMBOL_GPL(kvm_async_pf_task_wake);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|286| <<do_async_page_fault>> switch (kvm_read_and_reset_pf_reason()) {
+ *   - arch/x86/kvm/svm.c|5834| <<svm_vcpu_run>> svm->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();
+ *   - arch/x86/kvm/vmx/vmx.c|6189| <<handle_exception_nmi_irqoff>> vmx->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();
+ *
+ * 获取这个cpu的apf_reason.enabled, 并设置为0
+ */
 u32 kvm_read_and_reset_pf_reason(void)
 {
 	u32 reason = 0;
 
+	/*
+	 * 在以下使用apf_reason:
+	 *   - arch/x86/kernel/kvm.c|234| <<kvm_read_and_reset_pf_reason>> if (__this_cpu_read(apf_reason.enabled)) {
+	 *   - arch/x86/kernel/kvm.c|235| <<kvm_read_and_reset_pf_reason>> reason = __this_cpu_read(apf_reason.reason);
+	 *   - arch/x86/kernel/kvm.c|236| <<kvm_read_and_reset_pf_reason>> __this_cpu_write(apf_reason.reason, 0);
+	 *   - arch/x86/kernel/kvm.c|312| <<kvm_guest_cpu_init>> u64 pa = slow_virt_to_phys(this_cpu_ptr(&apf_reason));
+	 *   - arch/x86/kernel/kvm.c|323| <<kvm_guest_cpu_init>> __this_cpu_write(apf_reason.enabled, 1);
+	 *   - arch/x86/kernel/kvm.c|344| <<kvm_pv_disable_apf>> if (!__this_cpu_read(apf_reason.enabled))
+	 *   - arch/x86/kernel/kvm.c|348| <<kvm_pv_disable_apf>> __this_cpu_write(apf_reason.enabled, 0);
+	 *   - arch/x86/kernel/kvm.c|425| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(apf_reason, cpu), sizeof(apf_reason));
+	 */
 	if (__this_cpu_read(apf_reason.enabled)) {
 		reason = __this_cpu_read(apf_reason.reason);
 		__this_cpu_write(apf_reason.reason, 0);
@@ -241,22 +408,77 @@ u32 kvm_read_and_reset_pf_reason(void)
 EXPORT_SYMBOL_GPL(kvm_read_and_reset_pf_reason);
 NOKPROBE_SYMBOL(kvm_read_and_reset_pf_reason);
 
+/*
+ * 在arch/x86/entry/entry_64.S:
+ *   idtentry async_page_fault	do_async_page_fault
+ *
+ * When apf reason is 'KVM_PV_REASON_PAGE_NOT_PRESENT' it calls
+ * 'kvm_async_pf_task_wait' adds current process to a sleep list and
+ * reschedule.
+ *
+ * When the guest receive 'KVM_PV_REASON_PAGE_READY' it calls
+ * 'kvm_async_pf_task_wake' to wakeup the sleep process.
+ */
 dotraplinkage void
 do_async_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned long address)
 {
 	enum ctx_state prev_state;
 
+	/*
+	 * kvm_read_and_reset_pf_reason():
+	 * 获取这个cpu的apf_reason.enabled, 并设置为0
+	 */
 	switch (kvm_read_and_reset_pf_reason()) {
 	default:
+		/* x86下do_page_fault()在arch/x86/mm/fault.c */
 		do_page_fault(regs, error_code, address);
 		break;
 	case KVM_PV_REASON_PAGE_NOT_PRESENT:
+		/*
+		 * kvm插入的代码:
+		 * tdp_page_fault()
+		 *  -> try_async_pf()
+		 *      -> kvm_setup_async_pf()
+		 *          -> kvm_arch_async_page_not_present()
+		 *              -> apf_put_user(vcpu, KVM_PV_REASON_PAGE_NOT_PRESENT)
+		 *
+		 * 在以下使用KVM_PV_REASON_PAGE_NOT_PRESENT:
+		 *   - arch/x86/kernel/kvm.c|353| <<do_async_page_fault>> case KVM_PV_REASON_PAGE_NOT_PRESENT:
+		 *   - arch/x86/kvm/mmu.c|5947| <<kvm_handle_page_fault>> case KVM_PV_REASON_PAGE_NOT_PRESENT:
+		 *   - arch/x86/kvm/x86.c|10025| <<kvm_arch_async_page_not_present>> !apf_put_user(vcpu, KVM_PV_REASON_PAGE_NOT_PRESENT)) {
+		 *   - arch/x86/kvm/x86.c|10060| <<kvm_arch_async_page_present>> if (val == KVM_PV_REASON_PAGE_NOT_PRESENT &&
+		 */
 		/* page is swapped out by the host. */
 		prev_state = exception_enter();
 		kvm_async_pf_task_wait((u32)address, !user_mode(regs));
 		exception_exit(prev_state);
 		break;
 	case KVM_PV_REASON_PAGE_READY:
+		/*
+		 * 一共有两处可能执行到这里
+		 *
+		 * vcpu_run()
+		 *  -> kvm_check_async_pf_completion()
+		 *      -> kvm_async_page_present_async()
+		 *          -> kvm_arch_async_page_present()
+		 *              -> apf_put_user(vcpu, KVM_PV_REASON_PAGE_READY)
+		 *
+		 * tdp_page_fault()
+		 *  -> try_async_pf()
+		 *      -> kvm_arch_setup_async_pf()
+		 *          -> kvm_setup_async_pf()
+		 *              -> INIT_WORK(&work->work, async_pf_execute)
+		 *              -> schedule_work(&work->work))
+		 *                 async_pf_execute()
+		 *                  -> kvm_async_page_present_sync()
+		 *                      -> kvm_arch_async_page_present()
+		 *                          -> apf_put_user(vcpu, KVM_PV_REASON_PAGE_READY)
+		 *
+		 * 在以下使用KVM_PV_REASON_PAGE_READY:
+		 *   - arch/x86/kernel/kvm.c|359| <<do_async_page_fault>> case KVM_PV_REASON_PAGE_READY:
+		 *   - arch/x86/kvm/mmu.c|5953| <<kvm_handle_page_fault>> case KVM_PV_REASON_PAGE_READY:
+		 *   - arch/x86/kvm/x86.c|10071| <<kvm_arch_async_page_present>> } else if (!apf_put_user(vcpu, KVM_PV_REASON_PAGE_READY)) {
+		 */
 		rcu_irq_enter();
 		kvm_async_pf_task_wake((u32)address);
 		rcu_irq_exit();
@@ -265,10 +487,25 @@ do_async_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned lon
 }
 NOKPROBE_SYMBOL(do_async_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|709| <<kvm_guest_init>> paravirt_ops_setup();
+ */
 static void __init paravirt_ops_setup(void)
 {
 	pv_info.name = "KVM";
 
+	/*
+	 * 在以下使用KVM_FEATURE_NOP_IO_DELAY:
+	 *   - arch/x86/kernel/kvm.c|472| <<paravirt_ops_setup>> if (kvm_para_has_feature(KVM_FEATURE_NOP_IO_DELAY))
+	 *   - arch/x86/kvm/cpuid.c|688| <<__do_cpuid_func>> (1 << KVM_FEATURE_NOP_IO_DELAY) |
+	 *
+	 * called by:
+	 *   - arch/x86/include/asm/paravirt.h|42| <<slow_down_io>> pv_ops.cpu.io_delay();
+	 *   - arch/x86/include/asm/paravirt.h|44| <<slow_down_io>> pv_ops.cpu.io_delay();
+	 *   - arch/x86/include/asm/paravirt.h|45| <<slow_down_io>> pv_ops.cpu.io_delay();
+	 *   - arch/x86/include/asm/paravirt.h|46| <<slow_down_io>> pv_ops.cpu.io_delay();
+	 */
 	if (kvm_para_has_feature(KVM_FEATURE_NOP_IO_DELAY))
 		pv_ops.cpu.io_delay = kvm_io_delay;
 
@@ -277,9 +514,21 @@ static void __init paravirt_ops_setup(void)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|394| <<kvm_guest_cpu_init>> kvm_register_steal_time();
+ */
 static void kvm_register_steal_time(void)
 {
 	int cpu = smp_processor_id();
+	/*
+	 * 在以下使用steal_time:
+	 *   - arch/x86/kernel/kvm.c|283| <<kvm_register_steal_time>> struct kvm_steal_time *st = &per_cpu(steal_time, cpu);
+	 *   - arch/x86/kernel/kvm.c|385| <<kvm_steal_clock>> src = &per_cpu(steal_time, cpu);
+	 *   - arch/x86/kernel/kvm.c|426| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(steal_time, cpu), sizeof(steal_time));
+	 *   - arch/x86/kernel/kvm.c|609| <<kvm_flush_tlb_others>> src = &per_cpu(steal_time, cpu);
+	 *   - arch/x86/kernel/kvm.c|810| <<__kvm_vcpu_is_preempted>> struct kvm_steal_time *src = &per_cpu(steal_time, cpu);
+	 */
 	struct kvm_steal_time *st = &per_cpu(steal_time, cpu);
 
 	if (!has_steal_clock)
@@ -290,8 +539,20 @@ static void kvm_register_steal_time(void)
 		cpu, (unsigned long long) slow_virt_to_phys(st));
 }
 
+/*
+ * 在以下使用kvm_apic_eoi:
+ *   - arch/x86/kernel/kvm.c|359| <<kvm_guest_apic_eoi_write>> if (__test_and_clear_bit(KVM_PV_EOI_BIT, this_cpu_ptr(&kvm_apic_eoi)))
+ *   - arch/x86/kernel/kvm.c|386| <<kvm_guest_cpu_init>> BUILD_BUG_ON(__alignof__(kvm_apic_eoi) < 4);
+ *   - arch/x86/kernel/kvm.c|387| <<kvm_guest_cpu_init>> __this_cpu_write(kvm_apic_eoi, 0);
+ *   - arch/x86/kernel/kvm.c|388| <<kvm_guest_cpu_init>> pa = slow_virt_to_phys(this_cpu_ptr(&kvm_apic_eoi))
+ *   - arch/x86/kernel/kvm.c|488| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(kvm_apic_eoi, cpu), sizeof(kvm_apic_eoi));
+ */
 static DEFINE_PER_CPU_DECRYPTED(unsigned long, kvm_apic_eoi) = KVM_PV_EOI_DISABLED;
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|729| <<kvm_guest_init>> apic_set_eoi_write(kvm_guest_apic_eoi_write);
+ */
 static notrace void kvm_guest_apic_eoi_write(u32 reg, u32 val)
 {
 	/**
@@ -306,9 +567,31 @@ static notrace void kvm_guest_apic_eoi_write(u32 reg, u32 val)
 	apic->native_eoi_write(APIC_EOI, APIC_EOI_ACK);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|622| <<kvm_smp_prepare_boot_cpu>> kvm_guest_cpu_init();
+ *   - arch/x86/kernel/kvm.c|647| <<kvm_cpu_online>> kvm_guest_cpu_init();
+ *   - arch/x86/kernel/kvm.c|745| <<kvm_guest_init>> kvm_guest_cpu_init();
+ */
 static void kvm_guest_cpu_init(void)
 {
+	/*
+	 * 在以下使用kvmapf:
+	 *   - arch/x86/kernel/kvm.c|41| <<parse_no_kvmapf>> kvmapf = 0;
+	 *   - arch/x86/kernel/kvm.c|311| <<kvm_guest_cpu_init>> if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF) && kvmapf) {
+	 */
 	if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF) && kvmapf) {
+		/*
+		 * 在以下使用apf_reason:
+		 *   - arch/x86/kernel/kvm.c|234| <<kvm_read_and_reset_pf_reason>> if (__this_cpu_read(apf_reason.enabled)) {
+		 *   - arch/x86/kernel/kvm.c|235| <<kvm_read_and_reset_pf_reason>> reason = __this_cpu_read(apf_reason.reason);
+		 *   - arch/x86/kernel/kvm.c|236| <<kvm_read_and_reset_pf_reason>> __this_cpu_write(apf_reason.reason, 0);
+		 *   - arch/x86/kernel/kvm.c|312| <<kvm_guest_cpu_init>> u64 pa = slow_virt_to_phys(this_cpu_ptr(&apf_reason));
+		 *   - arch/x86/kernel/kvm.c|323| <<kvm_guest_cpu_init>> __this_cpu_write(apf_reason.enabled, 1);
+		 *   - arch/x86/kernel/kvm.c|344| <<kvm_pv_disable_apf>> if (!__this_cpu_read(apf_reason.enabled))
+		 *   - arch/x86/kernel/kvm.c|348| <<kvm_pv_disable_apf>> __this_cpu_write(apf_reason.enabled, 0);
+		 *   - arch/x86/kernel/kvm.c|425| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(apf_reason, cpu), sizeof(apf_reason));
+		 */
 		u64 pa = slow_virt_to_phys(this_cpu_ptr(&apf_reason));
 
 #ifdef CONFIG_PREEMPT
@@ -319,6 +602,14 @@ static void kvm_guest_cpu_init(void)
 		if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_VMEXIT))
 			pa |= KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;
 
+		/*
+		 * 在以下使用MSR_KVM_ASYNC_PF_EN:
+		 *   - arch/x86/kvm/x86.c|1164| <<global>> MSR_KVM_ASYNC_PF_EN, MSR_KVM_STEAL_TIME,
+		 *   - arch/x86/kernel/kvm.c|448| <<kvm_guest_cpu_init>> wrmsrl(MSR_KVM_ASYNC_PF_EN, pa);
+		 *   - arch/x86/kernel/kvm.c|478| <<kvm_pv_disable_apf>> wrmsrl(MSR_KVM_ASYNC_PF_EN, 0);
+		 *   - arch/x86/kvm/x86.c|2624| <<kvm_set_msr_common>> case MSR_KVM_ASYNC_PF_EN:
+		 *   - arch/x86/kvm/x86.c|2905| <<kvm_get_msr_common>> case MSR_KVM_ASYNC_PF_EN:
+		 */
 		wrmsrl(MSR_KVM_ASYNC_PF_EN, pa);
 		__this_cpu_write(apf_reason.enabled, 1);
 		printk(KERN_INFO"KVM setup async PF for cpu %d\n",
@@ -339,11 +630,24 @@ static void kvm_guest_cpu_init(void)
 		kvm_register_steal_time();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|418| <<kvm_pv_guest_cpu_reboot>> kvm_pv_disable_apf();
+ *   - arch/x86/kernel/kvm.c|636| <<kvm_guest_cpu_offline>> kvm_pv_disable_apf();
+ */
 static void kvm_pv_disable_apf(void)
 {
 	if (!__this_cpu_read(apf_reason.enabled))
 		return;
 
+	/*
+	 * 在以下使用MSR_KVM_ASYNC_PF_EN:
+	 *   - arch/x86/kvm/x86.c|1164| <<global>> MSR_KVM_ASYNC_PF_EN, MSR_KVM_STEAL_TIME,
+	 *   - arch/x86/kernel/kvm.c|448| <<kvm_guest_cpu_init>> wrmsrl(MSR_KVM_ASYNC_PF_EN, pa);
+	 *   - arch/x86/kernel/kvm.c|478| <<kvm_pv_disable_apf>> wrmsrl(MSR_KVM_ASYNC_PF_EN, 0);
+	 *   - arch/x86/kvm/x86.c|2624| <<kvm_set_msr_common>> case MSR_KVM_ASYNC_PF_EN:
+	 *   - arch/x86/kvm/x86.c|2905| <<kvm_get_msr_common>> case MSR_KVM_ASYNC_PF_EN:
+	 */
 	wrmsrl(MSR_KVM_ASYNC_PF_EN, 0);
 	__this_cpu_write(apf_reason.enabled, 0);
 
@@ -351,6 +655,10 @@ static void kvm_pv_disable_apf(void)
 	       smp_processor_id());
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|426| <<kvm_pv_reboot_notify>> on_each_cpu(kvm_pv_guest_cpu_reboot, NULL, 1);
+ */
 static void kvm_pv_guest_cpu_reboot(void *unused)
 {
 	/*
@@ -364,6 +672,9 @@ static void kvm_pv_guest_cpu_reboot(void *unused)
 	kvm_disable_steal_time();
 }
 
+/*
+ * struct notifier_block kvm_pv_reboot_nb.notifier_call = kvm_pv_reboot_notify()
+ */
 static int kvm_pv_reboot_notify(struct notifier_block *nb,
 				unsigned long code, void *unused)
 {
@@ -372,16 +683,35 @@ static int kvm_pv_reboot_notify(struct notifier_block *nb,
 	return NOTIFY_DONE;
 }
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|710| <<kvm_guest_init>> register_reboot_notifier(&kvm_pv_reboot_nb);
+ */
 static struct notifier_block kvm_pv_reboot_nb = {
 	.notifier_call = kvm_pv_reboot_notify,
 };
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/paravirt.h|36| <<paravirt_steal_clock>> return PVOP_CALL1(u64, time.steal_clock, cpu);
+ *
+ * used by:
+ *   - arch/x86/kernel/kvm.c|718| <<kvm_guest_init>> pv_ops.time.steal_clock = kvm_steal_clock;
+ */
 static u64 kvm_steal_clock(int cpu)
 {
 	u64 steal;
 	struct kvm_steal_time *src;
 	int version;
 
+	/*
+	 * 在以下使用steal_time:
+	 *   - arch/x86/kernel/kvm.c|283| <<kvm_register_steal_time>> struct kvm_steal_time *st = &per_cpu(steal_time, cpu);
+	 *   - arch/x86/kernel/kvm.c|385| <<kvm_steal_clock>> src = &per_cpu(steal_time, cpu);
+	 *   - arch/x86/kernel/kvm.c|426| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(steal_time, cpu), sizeof(steal_time));
+	 *   - arch/x86/kernel/kvm.c|609| <<kvm_flush_tlb_others>> src = &per_cpu(steal_time, cpu);
+	 *   - arch/x86/kernel/kvm.c|810| <<__kvm_vcpu_is_preempted>> struct kvm_steal_time *src = &per_cpu(steal_time, cpu);
+	 */
 	src = &per_cpu(steal_time, cpu);
 	do {
 		version = src->version;
@@ -393,6 +723,13 @@ static u64 kvm_steal_clock(int cpu)
 	return steal;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|419| <<kvm_pv_guest_cpu_reboot>> kvm_disable_steal_time();
+ *   - arch/x86/kernel/kvm.c|633| <<kvm_guest_cpu_offline>> kvm_disable_steal_time();
+ *   - arch/x86/kernel/kvmclock.c|282| <<kvm_crash_shutdown>> kvm_disable_steal_time();
+ *   - arch/x86/kernel/kvmclock.c|290| <<kvm_shutdown>> kvm_disable_steal_time();
+ */
 void kvm_disable_steal_time(void)
 {
 	if (!has_steal_clock)
@@ -401,6 +738,12 @@ void kvm_disable_steal_time(void)
 	wrmsr(MSR_KVM_STEAL_TIME, 0, 0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|462| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(apf_reason, cpu), sizeof(apf_reason));
+ *   - arch/x86/kernel/kvm.c|463| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(steal_time, cpu), sizeof(steal_time));
+ *   - arch/x86/kernel/kvm.c|464| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(kvm_apic_eoi, cpu), sizeof(kvm_apic_eoi));
+ */
 static inline void __set_percpu_decrypted(void *ptr, unsigned long size)
 {
 	early_set_memory_decrypted((unsigned long) ptr, size);
@@ -414,6 +757,11 @@ static inline void __set_percpu_decrypted(void *ptr, unsigned long size)
  * hotplugged will have their per-cpu variable already mapped as
  * decrypted.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|620| <<kvm_smp_prepare_boot_cpu>> sev_map_percpu_data();
+ *   - arch/x86/kernel/kvm.c|744| <<kvm_guest_init>> sev_map_percpu_data();
+ */
 static void __init sev_map_percpu_data(void)
 {
 	int cpu;
@@ -429,8 +777,30 @@ static void __init sev_map_percpu_data(void)
 }
 
 #ifdef CONFIG_SMP
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|525| <<__send_ipi_mask>> } else if (apic_id < min && max - apic_id < KVM_IPI_CLUSTER_SIZE) {
+ *   - arch/x86/kernel/kvm.c|528| <<__send_ipi_mask>> } else if (apic_id < min + KVM_IPI_CLUSTER_SIZE) {
+ */
 #define KVM_IPI_CLUSTER_SIZE	(2 * BITS_PER_LONG)
 
+/*
+ * commit aaffcfd1e82d3378538408d0310b7424b98d8f81
+ * Author: Wanpeng Li <wanpengli@tencent.com>
+ * Date:   Mon Jul 23 14:39:52 2018 +0800
+ *
+ * KVM: X86: Implement PV IPIs in linux guest
+ *
+ * Implement paravirtual apic hooks to enable PV IPIs for KVM if the "send IPI"
+ * hypercall is available.  The hypercall lets a guest send IPIs, with
+ * at most 128 destinations per hypercall in 64-bit mode and 64 vCPUs per
+ * hypercall in 32-bit mode
+ *
+ * called by:
+ *   - arch/x86/kernel/kvm.c|551| <<kvm_send_ipi_mask>> __send_ipi_mask(mask, vector);
+ *   - arch/x86/kernel/kvm.c|563| <<kvm_send_ipi_mask_allbutself>> __send_ipi_mask(local_mask, vector);
+ *   - arch/x86/kernel/kvm.c|573| <<kvm_send_ipi_all>> __send_ipi_mask(cpu_online_mask, vector);
+ */
 static void __send_ipi_mask(const struct cpumask *mask, int vector)
 {
 	unsigned long flags;
@@ -477,6 +847,12 @@ static void __send_ipi_mask(const struct cpumask *mask, int vector)
 	}
 
 	if (ipi_bitmap) {
+		/*
+		 * 在以下使用KVM_HC_SEND_IPI:
+		 *   - arch/x86/kernel/kvm.c|827| <<__send_ipi_mask>> ret = kvm_hypercall4(KVM_HC_SEND_IPI, (unsigned long )ipi_bitmap,
+		 *   - arch/x86/kernel/kvm.c|837| <<__send_ipi_mask>> ret = kvm_hypercall4(KVM_HC_SEND_IPI, (unsigned long )ipi_bitmap,
+		 *   - arch/x86/kvm/x86.c|7297| <<kvm_emulate_hypercall>> case KVM_HC_SEND_IPI:
+		 */
 		ret = kvm_hypercall4(KVM_HC_SEND_IPI, (unsigned long)ipi_bitmap,
 			(unsigned long)(ipi_bitmap >> BITS_PER_LONG), min, icr);
 		WARN_ONCE(ret < 0, "KVM: failed to send PV IPI: %ld", ret);
@@ -485,11 +861,22 @@ static void __send_ipi_mask(const struct cpumask *mask, int vector)
 	local_irq_restore(flags);
 }
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|581| <<kvm_setup_pv_ipi>> apic->send_IPI_mask = kvm_send_ipi_mask;
+ */
 static void kvm_send_ipi_mask(const struct cpumask *mask, int vector)
 {
 	__send_ipi_mask(mask, vector);
 }
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|582| <<kvm_setup_pv_ipi>> apic->send_IPI_mask_allbutself = kvm_send_ipi_mask_allbutself;
+ *
+ * called by:
+ *   - arch/x86/kernel/kvm.c|568| <<kvm_send_ipi_allbutself>> kvm_send_ipi_mask_allbutself(cpu_online_mask, vector);
+ */
 static void kvm_send_ipi_mask_allbutself(const struct cpumask *mask, int vector)
 {
 	unsigned int this_cpu = smp_processor_id();
@@ -502,11 +889,19 @@ static void kvm_send_ipi_mask_allbutself(const struct cpumask *mask, int vector)
 	__send_ipi_mask(local_mask, vector);
 }
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|583| <<kvm_setup_pv_ipi>> apic->send_IPI_allbutself = kvm_send_ipi_allbutself;
+ */
 static void kvm_send_ipi_allbutself(int vector)
 {
 	kvm_send_ipi_mask_allbutself(cpu_online_mask, vector);
 }
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|584| <<kvm_setup_pv_ipi>> apic->send_IPI_all = kvm_send_ipi_all;
+ */
 static void kvm_send_ipi_all(int vector)
 {
 	__send_ipi_mask(cpu_online_mask, vector);
@@ -515,6 +910,10 @@ static void kvm_send_ipi_all(int vector)
 /*
  * Set the IPI entry points
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|832| <<kvm_apic_init>> kvm_setup_pv_ipi();
+ */
 static void kvm_setup_pv_ipi(void)
 {
 	apic->send_IPI_mask = kvm_send_ipi_mask;
@@ -524,6 +923,13 @@ static void kvm_setup_pv_ipi(void)
 	pr_info("KVM setup pv IPIs\n");
 }
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/smp.h|127| <<arch_send_call_function_ipi_mask>> smp_ops.send_call_func_ipi(mask);
+ *
+ * used by:
+ *   - arch/x86/kernel/kvm.c|737| <<kvm_guest_init>> smp_ops.send_call_func_ipi = kvm_smp_send_call_func_ipi;
+ */
 static void kvm_smp_send_call_func_ipi(const struct cpumask *mask)
 {
 	int cpu;
@@ -539,6 +945,10 @@ static void kvm_smp_send_call_func_ipi(const struct cpumask *mask)
 	}
 }
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|732| <<kvm_guest_init>> smp_ops.smp_prepare_cpus = kvm_smp_prepare_cpus;
+ */
 static void __init kvm_smp_prepare_cpus(unsigned int max_cpus)
 {
 	native_smp_prepare_cpus(max_cpus);
@@ -546,6 +956,14 @@ static void __init kvm_smp_prepare_cpus(unsigned int max_cpus)
 		static_branch_disable(&virt_spin_lock_key);
 }
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|710| <<kvm_guest_init>> smp_ops.smp_prepare_boot_cpu = kvm_smp_prepare_boot_cpu;
+ *
+ * called by:
+ *   - arch/x86/include/asm/smp.h|82| <<smp_prepare_boot_cpu>> smp_ops.smp_prepare_boot_cpu();
+ *   - init/main.c|601| <<start_kernel>> smp_prepare_boot_cpu();
+ */
 static void __init kvm_smp_prepare_boot_cpu(void)
 {
 	/*
@@ -559,6 +977,10 @@ static void __init kvm_smp_prepare_boot_cpu(void)
 	kvm_spinlock_init();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|643| <<kvm_cpu_down_prepare>> kvm_guest_cpu_offline();
+ */
 static void kvm_guest_cpu_offline(void)
 {
 	kvm_disable_steal_time();
@@ -568,6 +990,10 @@ static void kvm_guest_cpu_offline(void)
 	apf_task_wake_all();
 }
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|718| <<kvm_guest_init>> kvm_cpu_online, kvm_cpu_down_prepare) < 0)
+ */
 static int kvm_cpu_online(unsigned int cpu)
 {
 	local_irq_disable();
@@ -576,6 +1002,10 @@ static int kvm_cpu_online(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|741| <<kvm_guest_init>> kvm_cpu_online, kvm_cpu_down_prepare) < 0)
+ */
 static int kvm_cpu_down_prepare(unsigned int cpu)
 {
 	local_irq_disable();
@@ -585,13 +1015,54 @@ static int kvm_cpu_down_prepare(unsigned int cpu)
 }
 #endif
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|710| <<kvm_guest_init>> x86_init.irqs.trap_init = kvm_apf_trap_init;
+ */
 static void __init kvm_apf_trap_init(void)
 {
+	/*
+	 * 在arch/x86/entry/entry_64.S:
+	 *   idtentry async_page_fault  do_async_page_fault
+	 */
 	update_intr_gate(X86_TRAP_PF, async_page_fault);
 }
 
+/*
+ * 在以下使用__pv_tlb_mask:
+ *   - arch/x86/kernel/kvm.c|1032| <<kvm_flush_tlb_others>> struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_tlb_mask);
+ *   - arch/x86/kernel/kvm.c|1246| <<kvm_setup_pv_tlb_flush>> zalloc_cpumask_var_node(per_cpu_ptr(&__pv_tlb_mask, cpu),
+ */
 static DEFINE_PER_CPU(cpumask_var_t, __pv_tlb_mask);
 
+/*
+ * commit 858a43aae23672d46fe802a41f4748f322965182
+ * Author: Wanpeng Li <wanpeng.li@hotmail.com>
+ * Date:   Tue Dec 12 17:33:02 2017 -0800
+ *
+ * KVM: X86: use paravirtualized TLB Shootdown
+ *   
+ * Remote TLB flush does a busy wait which is fine in bare-metal
+ * scenario. But with-in the guest, the vcpus might have been pre-empted or
+ * blocked. In this scenario, the initator vcpu would end up busy-waiting
+ * for a long amount of time; it also consumes CPU unnecessarily to wake
+ * up the target of the shootdown.
+ *
+ * This patch set adds support for KVM's new paravirtualized TLB flush;
+ * remote TLB flush does not wait for vcpus that are sleeping, instead
+ * KVM will flush the TLB as soon as the vCPU starts running again.
+ *
+ * The improvement is clearly visible when the host is overcommitted; in this
+ * case, the PV TLB flush (in addition to avoiding the wait on the main CPU)
+ * prevents preempted vCPUs from stealing precious execution time from the
+ * running ones.
+ *
+ * Testing on a Xeon Gold 6142 2.6GHz 2 sockets, 32 cores, 64 threads,
+ * so 64 pCPUs, and each VM is 64 vCPUs.
+ *
+ * used by:
+ *   - arch/x86/kernel/kvm.c|716| <<kvm_guest_init>> pv_ops.mmu.flush_tlb_others = kvm_flush_tlb_others;
+ */
 static void kvm_flush_tlb_others(const struct cpumask *cpumask,
 			const struct flush_tlb_info *info)
 {
@@ -618,6 +1089,9 @@ static void kvm_flush_tlb_others(const struct cpumask *cpumask,
 	native_flush_tlb_others(flushmask, info);
 }
 
+/*
+ * struct hypervisor_x86 x86_hyper_kvm.init.guest_late_init = kvm_guest_init()
+ */
 static void __init kvm_guest_init(void)
 {
 	int i;
@@ -669,6 +1143,10 @@ static void __init kvm_guest_init(void)
 	hardlockup_detector_disable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|764| <<kvm_cpuid_base>> kvm_cpuid_base = __kvm_cpuid_base();
+ */
 static noinline uint32_t __kvm_cpuid_base(void)
 {
 	if (boot_cpu_data.cpuid_level < 0)
@@ -680,6 +1158,16 @@ static noinline uint32_t __kvm_cpuid_base(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|764| <<kvm_cpuid_base>> kvm_cpuid_base = __kvm_cpuid_base();
+ *   - arch/x86/kernel/kvm.c|771| <<kvm_para_available>> return kvm_cpuid_base() != 0;
+ *   - arch/x86/kernel/kvm.c|777| <<kvm_arch_para_features>> return cpuid_eax(kvm_cpuid_base() | KVM_CPUID_FEATURES);
+ *   - arch/x86/kernel/kvm.c|786| <<kvm_arch_para_hints>> return cpuid_edx(kvm_cpuid_base() | KVM_CPUID_FEATURES);
+ *   - arch/x86/kernel/kvm.c|794| <<kvm_detect>> return kvm_cpuid_base();
+ *
+ * 返回kvm_cpuid_base(结果来自于__kvm_cpuid_base())
+ */
 static inline uint32_t kvm_cpuid_base(void)
 {
 	static int kvm_cpuid_base = -1;
@@ -690,41 +1178,81 @@ static inline uint32_t kvm_cpuid_base(void)
 	return kvm_cpuid_base;
 }
 
+/*
+ * struct hypervisor_x86 x86_hyper_kvm.init_x2apic_available = kvm_para_available()
+ */
 bool kvm_para_available(void)
 {
 	return kvm_cpuid_base() != 0;
 }
 EXPORT_SYMBOL_GPL(kvm_para_available);
 
+/*
+ * called by:
+ *   - include/linux/kvm_para.h|10| <<kvm_para_has_feature>> return !!(kvm_arch_para_features() & (1UL << feature));
+ */
 unsigned int kvm_arch_para_features(void)
 {
+	/*
+	 * kvm_cpuid_base():
+	 * 返回kvm_cpuid_base(结果来自于__kvm_cpuid_base())
+	 *
+	 * KVM_CPUID_FEATURES:
+	 * This CPUID returns two feature bitmaps in eax, edx. Before enabling
+	 * a particular paravirtualization, the appropriate feature bit should
+	 * be checked in eax. The performance hint feature bit should be checked
+	 * in edx.
+	 */
 	return cpuid_eax(kvm_cpuid_base() | KVM_CPUID_FEATURES);
 }
 
+/*
+ * called by:
+ *   - include/linux/kvm_para.h|15| <<kvm_para_has_hint>> return !!(kvm_arch_para_hints() & (1UL << feature));
+ */
 unsigned int kvm_arch_para_hints(void)
 {
 	return cpuid_edx(kvm_cpuid_base() | KVM_CPUID_FEATURES);
 }
 
+/*
+ * struct hypervisor_x86 x86_hyper_kvm.detect = kvm_detect()
+ */
 static uint32_t __init kvm_detect(void)
 {
 	return kvm_cpuid_base();
 }
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|798| <<kvm_init_platform>> x86_platform.apic_post_init = kvm_apic_init;
+ */
 static void __init kvm_apic_init(void)
 {
 #if defined(CONFIG_SMP)
+	/*
+	 * 在以下使用KVM_FEATURE_PV_SEND_IPI:
+	 *   - arch/x86/kernel/kvm.c|1151| <<kvm_apic_init>> if (kvm_para_has_feature(KVM_FEATURE_PV_SEND_IPI))
+	 *   - arch/x86/kvm/cpuid.c|696| <<__do_cpuid_func>> (1 << KVM_FEATURE_PV_SEND_IPI) |
+	 */
 	if (kvm_para_has_feature(KVM_FEATURE_PV_SEND_IPI))
 		kvm_setup_pv_ipi();
 #endif
 }
 
+/*
+ * struct hypervisor_x86 x86_hyper_kvm.init.init_platform()
+ */
 static void __init kvm_init_platform(void)
 {
 	kvmclock_init();
 	x86_platform.apic_post_init = kvm_apic_init;
 }
 
+/*
+ * 在以下使用:
+ *   - arch/x86/kernel/cpu/hypervisor.c|40| <<global>> &x86_hyper_kvm,
+ */
 const __initconst struct hypervisor_x86 x86_hyper_kvm = {
 	.name			= "KVM",
 	.detect			= kvm_detect,
@@ -734,6 +1262,9 @@ const __initconst struct hypervisor_x86 x86_hyper_kvm = {
 	.init.init_platform	= kvm_init_platform,
 };
 
+/*
+ * 用arch_initcall()初始化的函数
+ */
 static __init int activate_jump_labels(void)
 {
 	if (has_steal_clock) {
@@ -746,6 +1277,9 @@ static __init int activate_jump_labels(void)
 }
 arch_initcall(activate_jump_labels);
 
+/*
+ * 用arch_initcall()初始化的函数
+ */
 static __init int kvm_setup_pv_tlb_flush(void)
 {
 	int cpu;
@@ -767,6 +1301,10 @@ arch_initcall(kvm_setup_pv_tlb_flush);
 #ifdef CONFIG_PARAVIRT_SPINLOCKS
 
 /* Kick a cpu by its apicid. Used to wake up a halted vcpu */
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|926| <<kvm_spinlock_init>> pv_ops.lock.kick = kvm_kick_cpu;
+ */
 static void kvm_kick_cpu(int cpu)
 {
 	int apicid;
@@ -778,6 +1316,10 @@ static void kvm_kick_cpu(int cpu)
 
 #include <asm/qspinlock.h>
 
+/*
+ * used by:
+ *   - arch/x86/kernel/kvm.c|925| <<kvm_spinlock_init>> pv_ops.lock.wait = kvm_wait;
+ */
 static void kvm_wait(u8 *ptr, u8 val)
 {
 	unsigned long flags;
@@ -823,6 +1365,10 @@ extern bool __raw_callee_save___kvm_vcpu_is_preempted(long);
  * Hand-optimize version for x86-64 to avoid 8 64-bit register saving and
  * restoring to/from the stack.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|930| <<kvm_spinlock_init>> PV_CALLEE_SAVE(__kvm_vcpu_is_preempted);
+ */
 asm(
 ".pushsection .text;"
 ".global __raw_callee_save___kvm_vcpu_is_preempted;"
@@ -840,12 +1386,27 @@ asm(
 /*
  * Setup pv_lock_ops to exploit KVM_FEATURE_PV_UNHALT if present.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|596| <<kvm_smp_prepare_boot_cpu>> kvm_spinlock_init();
+ */
 void __init kvm_spinlock_init(void)
 {
 	/* Does host kernel support KVM_FEATURE_PV_UNHALT? */
+	/*
+	 * 在以下使用KVM_FEATURE_PV_UNHALT:
+	 *   - arch/x86/kernel/kvm.c|1309| <<kvm_spinlock_init>> if (!kvm_para_has_feature(KVM_FEATURE_PV_UNHALT))
+	 *   - arch/x86/kvm/cpuid.c|134| <<kvm_update_cpuid>> (best->eax & (1 << KVM_FEATURE_PV_UNHALT)))
+	 *   - arch/x86/kvm/cpuid.c|135| <<kvm_update_cpuid>> best->eax &= ~(1 << KVM_FEATURE_PV_UNHALT);
+	 *   - arch/x86/kvm/cpuid.c|693| <<__do_cpuid_func>> (1 << KVM_FEATURE_PV_UNHALT) |
+	 */
 	if (!kvm_para_has_feature(KVM_FEATURE_PV_UNHALT))
 		return;
 
+	/*
+	 * guest checks this feature bit to determine that vCPUs are never
+	 * preempted for an unlimited time allowing optimizations
+	 */
 	if (kvm_para_has_hint(KVM_HINTS_REALTIME))
 		return;
 
@@ -853,11 +1414,24 @@ void __init kvm_spinlock_init(void)
 	if (num_possible_cpus() == 1)
 		return;
 
+	/*
+	 * Allocate memory for the PV qspinlock hash buckets
+	 *
+	 * This function should be called from the paravirt spinlock initialization
+	 * routine.
+	 */
 	__pv_init_lock_hash();
 	pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
 	pv_ops.lock.queued_spin_unlock =
 		PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+	/* 下面两个更和kvm相关 */
+	/*
+	 * 就是调用halt()或者safe_halt()
+	 */
 	pv_ops.lock.wait = kvm_wait;
+	/*
+	 * 就是用KVM_HC_KICK_CPU这个hypercall来kick一下cpu
+	 */
 	pv_ops.lock.kick = kvm_kick_cpu;
 
 	if (kvm_para_has_feature(KVM_FEATURE_STEAL_TIME)) {
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 904494b..8780918 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -23,6 +23,76 @@
 #include <asm/reboot.h>
 #include <asm/kvmclock.h>
 
+/*
+ * FROM SUSE!!!
+ * When using kvm-clock, it is not recommended to use NTP in the VM Guest, as
+ * well. Using NTP on the VM Host Server, however, is still recommended.
+ */
+
+/*
+ * Clocksource is a device that can give a timestamp whenever you need it. In
+ * other words, Clocksource is any ticking counter that allows you to get its
+ * value.
+ *
+ * Clockevent device is an alarm clock—you ask the device to signal a time in
+ * the future (e.g., "wake me up in 1ms") and when the alarm is triggered, you
+ * get the signal.
+ *
+ * sched_clock() function is similar to clocksource, but this particular one
+ * should be "cheap" to read (meaning that one can get its value fast), as
+ * sched_clock() is used for task-scheduling purposes and scheduling happens
+ * often. We're ready to sacrifice accuracy and other characteristics for
+ * speed.
+ *
+ * > CLOCK_REALTIME clock gives the time passed since January 1, 1970. This
+ *   clock is affected by NTP adjustments and can jump forward and backward when
+ *   a system administrator adjusts system time.
+ *
+ * > CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually
+ *   since you booted the system. This clock is affected by NTP, but it can't
+ *   jump backward.
+ *
+ * > CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this
+ *   clock is not affected by NTP adjustments.
+ *
+ * > CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but
+ *   less-accurate variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ *
+ *
+ * Hardware extensions for virtualizing TSC
+ *
+ * Since the early days of hardware-assisted virtualization, Intel was
+ * supplying an option to do TSC offsetting for virtual guests in hardware,
+ * which would mean that a guest's rdtsc reading will return a host's TSC value
+ * + offset. Unfortunately, this wasn't enough to support migration between
+ * different hosts because TSC frequency may differ, so pvclock and TSC page
+ * protocol were introduced. In late 2015, Intel introduced the TSC scaling
+ * feature (which was already present in AMD processors for several years) and,
+ * in theory, this is a game changer making pvclock and TSC page protocols
+ * redundant. However, an immediate switch to using plain TSC as a clocksource
+ * for virtualized guests seems impractical; one must be sure that all
+ * potential migration recipient hosts support the feature, but it is not yet
+ * widely available. Extensive testing also must be performed to make sure
+ * there are no drawbacks to switching from paravirtualized protocols.
+ */
+
+/*
+ * To get the current TSC reading, guests must do the following math:
+ *
+ * PerCPUTime = ((RDTSC() - tsc_timestamp) >> tsc_shift) * tsc_to_system_mul + system_time 
+ *
+ *
+ *
+ * kvmclock or KVM pvclock lets guests read the host's wall clock time. It's
+ * really very simple: the guest sets aside a page of its RAM and asks the host
+ * to write time into that page (using an MSR). The host writes a structure
+ * containing the current time to this page - in theory the host updates this
+ * page constantly, but in reality that would be wasteful and the structure is
+ * only updated just before reentering the guest after some VM event.
+ * host更新clock的时间的函数在kvm_guest_time_update(), 只被vcpu_enter_guest()调用
+ */
+
 static int kvmclock __initdata = 1;
 static int kvmclock_vsyscall __initdata = 1;
 static int msr_kvm_system_time __ro_after_init = MSR_KVM_SYSTEM_TIME;
@@ -305,6 +375,10 @@ static int kvmclock_setup_percpu(unsigned int cpu)
 	return p ? 0 : -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|724| <<kvm_init_platform>> kvmclock_init();
+ */
 void __init kvmclock_init(void)
 {
 	u8 flags;
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index e904ff0..cafc1d2 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -2278,6 +2278,20 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 		return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9241| <<kvm_arch_vcpu_init>> r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ *
+ * QEMU和KVM都实现了对中断芯片的模拟,这是由于历史原因造成的.早在KVM诞生之前,
+ * QEMU就提供了一整套对设备的模拟,包括中断芯片.而KVM诞生之后,为了进一步提高
+ * 中断性能,因此又在KVM中实现了一套中断芯片.我们可以通过QEMU的启动参数
+ * kernel-irqchip来决定使用谁的中断芯片(irq chip)/
+ *   on: KVM模拟全部
+ *   split: QEMU模拟IOAPIC和PIC,KVM模拟LAPIC
+ *   off: QEMU模拟全部
+ *
+ * 分配并且初始化vcpu的apic (在内核中模拟)
+ */
 int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 {
 	struct kvm_lapic *apic;
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index a63964e..3d2ad88 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -48,15 +48,41 @@
 #include "trace.h"
 
 /*
+ * 当QEMU通过ioctl创建vcpu时,调用kvm_mmu_create初始化mmu相关信息,
+ * 为页表项结构分配slab cache.
+ *
+ * 当KVM要进入Guest前,vcpu_enter_guest => kvm_mmu_reload会将根级
+ * 页表地址加载到VMCS,让Guest使用该页表.
+ *
+ * 当EPT Violation发生时,VMEXIT到KVM中.如果是缺页,则拿到对应的GPA,
+ * 根据GPA算出gfn,根据gfn找到对应的memory slot,得到对应的HVA.然后
+ * 根据HVA找到对应的pfn,确保该page位于内存.在把缺的页填上后,需要更
+ * 新EPT,完善其中缺少的页表项.于是从L4开始,逐层补全页表,对于在某层
+ * 上缺少的页表页,会从slab中分配后将新页的HPA填入到上一级页表中.
+ *
+ * 除了建立上级页表到下级页表的关联外,KVM 还会建立反向映射,可以直
+ * 接根据GPA找到gfn相关的页表项,而无需再次走EPT查询.
+ */
+
+/*
  * When setting this variable to true it enables Two-Dimensional-Paging
  * where the hardware walks 2 page tables:
  * 1. the guest-virtual to guest-physical
  * 2. while doing 1. it walks guest-physical to host-physical
  * If the hardware supports that we don't need to do shadow paging.
  */
+/*
+ * 设置tdp_enabled的地方, 默认是false:
+ *   - arch/x86/kvm/mmu.c|5531| <<kvm_enable_tdp>> tdp_enabled = true;
+ *   - arch/x86/kvm/mmu.c|5537| <<kvm_disable_tdp>> tdp_enabled = false;
+ */
 bool tdp_enabled = false;
 
 enum {
+	/*
+	 * 只在以下使用AUDIT_PRE_PAGE_FAULT:
+	 *   - arch/x86/kvm/paging_tmpl.h|848| <<FNAME(page_fault)>> kvm_mmu_audit(vcpu, AUDIT_PRE_PAGE_FAULT);
+	 */
 	AUDIT_PRE_PAGE_FAULT,
 	AUDIT_POST_PAGE_FAULT,
 	AUDIT_PRE_PTE_WRITE,
@@ -80,16 +106,63 @@ module_param(dbg, bool, 0644);
 #define MMU_WARN_ON(x) do { } while (0)
 #endif
 
+/*
+ * 在以下使用PTE_PREFETCH_NUM:
+ *   - arch/x86/kvm/paging_tmpl.h|89| <<global>> pt_element_t prefetch_ptes[PTE_PREFETCH_NUM];
+ *   - arch/x86/kvm/mmu.c|1054| <<mmu_topup_memory_caches>> pte_list_desc_cache, 8 + PTE_PREFETCH_NUM);
+ *   - arch/x86/kvm/mmu.c|3135| <<direct_pte_prefetch_many>> struct page *pages[PTE_PREFETCH_NUM];
+ *   - arch/x86/kvm/mmu.c|3167| <<__direct_pte_prefetch>> i = (sptep - sp->spt) & ~(PTE_PREFETCH_NUM - 1);
+ *   - arch/x86/kvm/mmu.c|3170| <<__direct_pte_prefetch>> for (i = 0; i < PTE_PREFETCH_NUM; i++, spte++) {
+ *   - arch/x86/kvm/paging_tmpl.h|564| <<FNAME>> mask = PTE_PREFETCH_NUM * sizeof(pt_element_t) - 1;
+ *   - arch/x86/kvm/paging_tmpl.h|594| <<FNAME>> i = (sptep - sp->spt) & ~(PTE_PREFETCH_NUM - 1);
+ *   - arch/x86/kvm/paging_tmpl.h|597| <<FNAME>> for (i = 0; i < PTE_PREFETCH_NUM; i++, spte++) {
+ */
 #define PTE_PREFETCH_NUM		8
 
+/*
+ * 在以下使用PT_FIRST_AVAIL_BITS_SHIFT:
+ *   - arch/x86/kvm/mmu.c|184| <<SPTE_HOST_WRITEABLE>> #define SPTE_HOST_WRITEABLE (1ULL << PT_FIRST_AVAIL_BITS_SHIFT)
+ *   - arch/x86/kvm/mmu.c|185| <<SPTE_MMU_WRITEABLE>> #define SPTE_MMU_WRITEABLE (1ULL << (PT_FIRST_AVAIL_BITS_SHIFT + 1))
+ */
 #define PT_FIRST_AVAIL_BITS_SHIFT 10
+/*
+ * 在以下使用:
+ *   - arch/x86/kvm/mmu.c|277| <<global>> static const u64 shadow_acc_track_saved_bits_shift = PT64_SECOND_AVAIL_BITS_SHIFT;
+ */
 #define PT64_SECOND_AVAIL_BITS_SHIFT 52
 
+/*
+ * 在以下使用PT64_LEVEL_BITS:
+ *   - arch/x86/kvm/mmu.c|129| <<PT64_LEVEL_SHIFT>> (PAGE_SHIFT + (level - 1) * PT64_LEVEL_BITS)
+ *   - arch/x86/kvm/mmu.c|135| <<PT64_INDEX>> (((address) >> PT64_LEVEL_SHIFT(level)) & ((1 << PT64_LEVEL_BITS) - 1))
+ *   - arch/x86/kvm/mmu.c|158| <<PT64_LVL_ADDR_MASK>> * PT64_LEVEL_BITS))) - 1))
+ *   - arch/x86/kvm/mmu.c|161| <<PT64_LVL_OFFSET_MASK>> * PT64_LEVEL_BITS))) - 1))
+ *   - arch/x86/kvm/mmu.c|1140| <<kvm_mmu_page_get_gfn>> return sp->gfn + (index << ((sp->role.level - 1) * PT64_LEVEL_BITS));
+ *   - arch/x86/kvm/paging_tmpl.h|31| <<PT_LEVEL_BITS>> #define PT_LEVEL_BITS PT64_LEVEL_BITS
+ *   - arch/x86/kvm/paging_tmpl.h|64| <<PT_LEVEL_BITS>> #define PT_LEVEL_BITS PT64_LEVEL_BITS
+ *   - arch/x86/kvm/paging_tmpl.h|870| <<FNAME(get_level1_sp_gpa)>> offset = sp->role.quadrant << PT64_LEVEL_BITS;
+ */
 #define PT64_LEVEL_BITS 9
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|160| <<PT64_INDEX>> (((address) >> PT64_LEVEL_SHIFT(level)) & ((1 << PT64_LEVEL_BITS) - 1))
+ *
+ * PT64_LEVEL_SHIFT(1) = 12 + (1 - 1) * 9 = 12
+ * PT64_LEVEL_SHIFT(2) = 12 + (2 - 1) * 9 = 21
+ * PT64_LEVEL_SHIFT(3) = 12 + (3 - 1) * 9 = 30
+ */
 #define PT64_LEVEL_SHIFT(level) \
 		(PAGE_SHIFT + (level - 1) * PT64_LEVEL_BITS)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|187| <<SHADOW_PT_INDEX>> #define SHADOW_PT_INDEX(addr, level) PT64_INDEX(addr, level)
+ *   - arch/x86/kvm/paging_tmpl.h|30| <<PT_INDEX>> #define PT_INDEX(addr, level) PT64_INDEX(addr, level)
+ *   - arch/x86/kvm/paging_tmpl.h|63| <<PT_INDEX>> #define PT_INDEX(addr, level) PT64_INDEX(addr, level)
+ *
+ * 先把adress根据level右移若干位, 然后取出最后9位作为index
+ */
 #define PT64_INDEX(address, level)\
 	(((address) >> PT64_LEVEL_SHIFT(level)) & ((1 << PT64_LEVEL_BITS) - 1))
 
@@ -110,11 +183,28 @@ module_param(dbg, bool, 0644);
 #ifdef CONFIG_DYNAMIC_PHYSICAL_MASK
 #define PT64_BASE_ADDR_MASK (physical_mask & ~(u64)(PAGE_SIZE-1))
 #else
+/*
+ * ((1ULL << 52) - 1)  = 0x000fffffffffffff
+ * ~(u64)(PAGE_SIZE-1) = 是64位中就后面12位是0
+ *
+ * 在普通模式下相当于表示的51位最大物理地址mask了最后12位 = 0x000ffffffffff000
+ * 在CONFIG_DYNAMIC_PHYSICAL_MASK下不是52位, 是physical_mask
+ */
 #define PT64_BASE_ADDR_MASK (((1ULL << 52) - 1) & ~(u64)(PAGE_SIZE-1))
 #endif
+/*
+ * 在以下使用PT64_LVL_ADDR_MASK()
+ *   - arch/x86/kvm/paging_tmpl.h|28| <<PT_LVL_ADDR_MASK>> #define PT_LVL_ADDR_MASK(lvl) PT64_LVL_ADDR_MASK(lvl)
+ *   - arch/x86/kvm/paging_tmpl.h|61| <<PT_LVL_ADDR_MASK>> #define PT_LVL_ADDR_MASK(lvl) PT64_LVL_ADDR_MASK(lvl)
+ */
 #define PT64_LVL_ADDR_MASK(level) \
 	(PT64_BASE_ADDR_MASK & ~((1ULL << (PAGE_SHIFT + (((level) - 1) \
 						* PT64_LEVEL_BITS))) - 1))
+/*
+ * 在以下使用PT64_LVL_OFFSET_MASK:
+ *   - arch/x86/kvm/paging_tmpl.h|29| <<PT_LVL_OFFSET_MASK>> #define PT_LVL_OFFSET_MASK(lvl) PT64_LVL_OFFSET_MASK(lvl)
+ *   - arch/x86/kvm/paging_tmpl.h|62| <<PT_LVL_OFFSET_MASK>> #define PT_LVL_OFFSET_MASK(lvl) PT64_LVL_OFFSET_MASK(lvl)
+ */
 #define PT64_LVL_OFFSET_MASK(level) \
 	(PT64_BASE_ADDR_MASK & ((1ULL << (PAGE_SHIFT + (((level) - 1) \
 						* PT64_LEVEL_BITS))) - 1))
@@ -126,26 +216,81 @@ module_param(dbg, bool, 0644);
 	(PAGE_MASK & ~((1ULL << (PAGE_SHIFT + (((level) - 1) \
 					    * PT32_LEVEL_BITS))) - 1))
 
+/*
+ * 在以下使用:
+ *   - arch/x86/kvm/mmu.c|5216| <<need_remote_flush>> return (old & ~new & PT64_PERM_MASK) != 0;
+ */
 #define PT64_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \
 			| shadow_x_mask | shadow_nx_mask | shadow_me_mask)
 
 #define ACC_EXEC_MASK    1
+/* 1 << 1 */
 #define ACC_WRITE_MASK   PT_WRITABLE_MASK
+/* 1 << 2 */
 #define ACC_USER_MASK    PT_USER_MASK
+/*
+ * 最后三位111全部设置
+ */
 #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
 
 /* The mask for the R/X bits in EPT PTEs */
+/*
+ * 在以下使用PT64_EPT_READABLE_MASK:
+ *   - arch/x86/kvm/mmu.c|321| <<global>> static const u64 shadow_acc_track_saved_bits_mask = PT64_EPT_READABLE_MASK |
+ */
 #define PT64_EPT_READABLE_MASK			0x1ull
 #define PT64_EPT_EXECUTABLE_MASK		0x4ull
 
 #include <trace/events/kvm.h>
 
+/*
+ * 在以下使用SPTE_HOST_WRITEABLE:
+ *   - arch/x86/kvm/mmu.c|1519| <<spte_can_locklessly_be_made_writable>> return (spte & (SPTE_HOST_WRITEABLE | SPTE_MMU_WRITEABLE)) ==
+ *   - arch/x86/kvm/mmu.c|1520| <<spte_can_locklessly_be_made_writable>> (SPTE_HOST_WRITEABLE | SPTE_MMU_WRITEABLE);
+ *   - arch/x86/kvm/mmu.c|2669| <<kvm_set_pte_rmapp>> new_spte &= ~SPTE_HOST_WRITEABLE;
+ *   - arch/x86/kvm/mmu.c|3845| <<set_spte>> spte |= SPTE_HOST_WRITEABLE;
+ *   - arch/x86/kvm/paging_tmpl.h|1043| <<FNAME(sync_page)>> host_writable = sp->spt[i] & SPTE_HOST_WRITEABLE;
+ *
+ * 1往左移10位
+ *
+ * SPTE_HOST_WRITEABLE means the gfn is writable on host.
+ */
 #define SPTE_HOST_WRITEABLE	(1ULL << PT_FIRST_AVAIL_BITS_SHIFT)
+/*
+ * 在以下使用SPTE_MMU_WRITEABLE:
+ *   - arch/x86/kvm/mmu.c|1476| <<spte_can_locklessly_be_made_writable>> return (spte & (SPTE_HOST_WRITEABLE | SPTE_MMU_WRITEABLE)) ==
+ *   - arch/x86/kvm/mmu.c|1477| <<spte_can_locklessly_be_made_writable>> (SPTE_HOST_WRITEABLE | SPTE_MMU_WRITEABLE);
+ *   - arch/x86/kvm/mmu.c|2366| <<spte_write_protect>> spte &= ~SPTE_MMU_WRITEABLE;
+ *   - arch/x86/kvm/mmu.c|3813| <<set_spte>> spte |= PT_WRITABLE_MASK | SPTE_MMU_WRITEABLE;
+ *   - arch/x86/kvm/mmu.c|3829| <<set_spte>> spte &= ~(PT_WRITABLE_MASK | SPTE_MMU_WRITEABLE);
+ *
+ * 1往左移11位
+ *
+ * SPTE_MMU_WRITEABLE means the gfn is writable on mmu. The bit is set when
+ * the gfn is writable on guest mmu and it is not write-protected by shadow
+ * page write-protection.
+ */
 #define SPTE_MMU_WRITEABLE	(1ULL << (PT_FIRST_AVAIL_BITS_SHIFT + 1))
 
+/*
+ * 使用SHADOW_PT_INDEX()的地方:
+ *   - arch/x86/kvm/mmu.c|2701| <<shadow_walk_okay>> iterator->index = SHADOW_PT_INDEX(iterator->addr, iterator->level);
+ *
+ * 先把adress根据level右移若干位, 然后取出最后9位作为index
+ */
 #define SHADOW_PT_INDEX(addr, level) PT64_INDEX(addr, level)
 
 /* make pte_list_desc fit well in cache line */
+/*
+ * 使用PTE_LIST_EXT的地方:
+ *   - arch/x86/kvm/mmu.c|253| <<global>> u64 *sptes[PTE_LIST_EXT];
+ *   - arch/x86/kvm/mmu.c|1415| <<pte_list_add>> while (desc->sptes[PTE_LIST_EXT-1] && desc->more) {
+ *   - arch/x86/kvm/mmu.c|1417| <<pte_list_add>> count += PTE_LIST_EXT;
+ *   - arch/x86/kvm/mmu.c|1419| <<pte_list_add>> if (desc->sptes[PTE_LIST_EXT-1]) {
+ *   - arch/x86/kvm/mmu.c|1437| <<pte_list_desc_remove_entry>> for (j = PTE_LIST_EXT - 1; !desc->sptes[j] && j > i; --j)
+ *   - arch/x86/kvm/mmu.c|1474| <<__pte_list_remove>> for (i = 0; i < PTE_LIST_EXT && desc->sptes[i]; ++i) {
+ *   - arch/x86/kvm/mmu.c|1595| <<rmap_get_next>> if (iter->pos < PTE_LIST_EXT - 1) {
+ */
 #define PTE_LIST_EXT 3
 
 /*
@@ -168,13 +313,26 @@ struct pte_list_desc {
 };
 
 struct kvm_shadow_walk_iterator {
+	/* 发生page fault的GPA,迭代过程就是要把GPA所涉及的页表项都填上 */
 	u64 addr;
+	/* 当前页表项的 HPA，在 shadow_walk_init 中设置为 vcpu->arch.mmu.root_hpa */
 	hpa_t shadow_addr;
+	/* 指向当前页表项, 在shadow_walk_okay()中更新 */
 	u64 *sptep;
+	/* 当前层级,在shadow_walk_okay()中设置为4 (x86_64 PT64_ROOT_LEVEL), 在shadow_walk_next()中减1 */
 	int level;
+	/* 在当前level页表中的索引,在shadow_walk_okey中更新 */
 	unsigned index;
 };
 
+/*
+ * 在以下使用mmu_base_role_mask:
+ *   - arch/x86/kvm/mmu.c|5125| <<init_kvm_tdp_mmu>> new_role.base.word &= mmu_base_role_mask.word;
+ *   - arch/x86/kvm/mmu.c|5197| <<kvm_init_shadow_mmu>> new_role.base.word &= mmu_base_role_mask.word;
+ *   - arch/x86/kvm/mmu.c|5254| <<kvm_init_shadow_ept_mmu>> new_role.base.word &= mmu_base_role_mask.word;
+ *   - arch/x86/kvm/mmu.c|5295| <<init_kvm_nested_mmu>> new_role.base.word &= mmu_base_role_mask.word;
+ *   - arch/x86/kvm/mmu.c|5590| <<kvm_mmu_pte_write>> & mmu_base_role_mask.word) && rmap_can_add(vcpu))
+ */
 static const union kvm_mmu_page_role mmu_base_role_mask = {
 	.cr0_wp = 1,
 	.gpte_is_8_bytes = 1,
@@ -186,17 +344,34 @@ static const union kvm_mmu_page_role mmu_base_role_mask = {
 	.ad_disabled = 1,
 };
 
+/*
+ * 在以下调用for_each_shadow_entry_using_root():
+ *   - arch/x86/kvm/paging_tmpl.h|896| <<FNAME(invlpg)>> for_each_shadow_entry_using_root(vcpu, root_hpa, gva, iterator) {
+ */
 #define for_each_shadow_entry_using_root(_vcpu, _root, _addr, _walker)     \
 	for (shadow_walk_init_using_root(&(_walker), (_vcpu),              \
 					 (_root), (_addr));                \
 	     shadow_walk_okay(&(_walker));			           \
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * 在以下调用for_each_shadow_entry():
+ *   - arch/x86/kvm/mmu.c|3433| <<__direct_map>> for_each_shadow_entry(vcpu, gpa, it) {
+ *
+ * 对于一个gpa的每一级(level)的pte
+ */
 #define for_each_shadow_entry(_vcpu, _addr, _walker)            \
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);	\
 	     shadow_walk_okay(&(_walker));			\
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * 在以下调用for_each_shadow_entry_lockless():
+ *   - arch/x86/kvm/mmu.c|3641| <<fast_page_fault>> for_each_shadow_entry_lockless(vcpu, gva, iterator, spte)
+ *   - arch/x86/kvm/mmu.c|4230| <<shadow_page_table_clear_flood>> for_each_shadow_entry_lockless(vcpu, addr, iterator, spte) {
+ *
+ * 对于一个gpa的每一级(level)的pte
+ */
 #define for_each_shadow_entry_lockless(_vcpu, _addr, _walker, spte)	\
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);		\
 	     shadow_walk_okay(&(_walker)) &&				\
@@ -207,14 +382,121 @@ static struct kmem_cache *pte_list_desc_cache;
 static struct kmem_cache *mmu_page_header_cache;
 static struct percpu_counter kvm_total_used_mmu_pages;
 
+/*
+ * 更新mask们的核心函数:
+ *  - kvm_mmu_set_mask_ptes()
+ *  - kvm_mmu_reset_all_pte_masks()
+ *  - kvm_mmu_set_mmio_spte_mask()
+ *
+ * vmx_init()
+ *  -> kvm_init()
+ *      -> kvm_arch_init()
+ *          -> kvm_mmu_module_init()
+ *              -> kvm_mmu_reset_all_pte_masks()
+ *              -> kvm_set_mmio_spte_mask()
+ *                   -> kvm_mmu_set_mmio_spte_mask() --> 第一次
+ *          -> kvm_mmu_set_mask_ptes() --> 第一次
+ *      -> kvm_arch_hardware_setup()
+ *          -> kvm_x86_ops->hardware_setup = hardware_setup()
+ *              -> vmx_enable_tdp()
+ *                  -> kvm_mmu_set_mask_ptes() --> 第二次
+ *                  -> ept_set_mmio_spte_mask()
+ *                      -> kvm_mmu_set_mmio_spte_mask() --> 第二次
+ */
+
+/*
+ * 在以下设置shadow_nx_mask:
+ *   - arch/x86/kvm/mmu.c|645| <<kvm_mmu_set_mask_ptes>> shadow_nx_mask = nx_mask;
+ *   - arch/x86/kvm/mmu.c|679| <<kvm_mmu_reset_all_pte_masks>> shadow_nx_mask = 0;
+ *
+ * 在new machine上是0x0
+ */
 static u64 __read_mostly shadow_nx_mask;
+/*
+ * 在以下设置shadow_x_mask:
+ *   - arch/x86/kvm/mmu.c|646| <<kvm_mmu_set_mask_ptes>> shadow_x_mask = x_mask;
+ *   - arch/x86/kvm/mmu.c|680| <<kvm_mmu_reset_all_pte_masks>> shadow_x_mask = 0;
+ *
+ * 在new machine上是VMX_EPT_EXECUTABLE_MASK
+ */
 static u64 __read_mostly shadow_x_mask;	/* mutual exclusive with nx_mask */
+/*
+ * 在以下设置:
+ *   - arch/x86/kvm/mmu.c|642| <<kvm_mmu_set_mask_ptes>> shadow_user_mask = user_mask;
+ *   - arch/x86/kvm/mmu.c|676| <<kvm_mmu_reset_all_pte_masks>> shadow_user_mask = 0;
+ *
+ * 在new machine上是VMX_EPT_READABLE_MASK
+ */
 static u64 __read_mostly shadow_user_mask;
+/*
+ * 在以下设置shadow_accessed_mask:
+ *   - arch/x86/kvm/mmu.c|643| <<kvm_mmu_set_mask_ptes>> shadow_accessed_mask = accessed_mask;
+ *   - arch/x86/kvm/mmu.c|677| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = 0;
+ *
+ * 在new machine上从vmx_enable_tdp()第二次进来的结果:
+ *   shadow_accessed_mask = enable_ept_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+ *   也就是0x0000000000000100
+ */
 static u64 __read_mostly shadow_accessed_mask;
+/*
+ * 在以下设置shadow_dirty_mask:
+ *   - arch/x86/kvm/mmu.c|644| <<kvm_mmu_set_mask_ptes>> shadow_dirty_mask = dirty_mask;
+ *   - arch/x86/kvm/mmu.c|678| <<kvm_mmu_reset_all_pte_masks>> shadow_dirty_mask = 0;
+ *
+ * 在new machine上从vmx_enable_tdp()第二次进来的结果:
+ *   shadow_dirty_mask = enable_ept_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull,
+ *   也就是0x0000000000000200
+ */
 static u64 __read_mostly shadow_dirty_mask;
+/*
+ * 在以下设置shadow_mmio_mask:
+ *   - arch/x86/kvm/mmu.c|482| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_mask = mmio_mask | SPTE_SPECIAL_MASK;
+ *
+ * 从vmx_enable_tdp()-->ept_set_mmio_spte_mask()第二次进入的例子:
+ *   shadow_mmio_value = VMX_EPT_MISCONFIG_WX_VALUE | SPTE_SPECIAL_MASK;
+ *   shadow_mmio_mask = VMX_EPT_RWX_MASK | SPTE_SPECIAL_MASK;
+ *
+ * 第二次从vmx_enable_tdp()-->ept_set_mmio_spte_mask()进入的结果:
+ *   mmio_value = 0x0000000000000006
+ *   mmio_mask  = 0x0000000000000007
+ *   shadow_mmio_mask  = 0x4000000000000007
+ *   shadow_mmio_value = 0x4000000000000006
+ *
+ * 设置RWX并且还有1往左62位 (SPTE_SPECIAL_MASK)
+ */
 static u64 __read_mostly shadow_mmio_mask;
+/*
+ * 在以下设置shadow_mmio_value:
+ *   - arch/x86/kvm/mmu.c|481| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_value = mmio_value | SPTE_SPECIAL_MASK;
+ *
+ * 从vmx_enable_tdp()-->ept_set_mmio_spte_mask()第二次进入的例子:
+ *   shadow_mmio_value = VMX_EPT_MISCONFIG_WX_VALUE | SPTE_SPECIAL_MASK;
+ *   shadow_mmio_mask = VMX_EPT_RWX_MASK | SPTE_SPECIAL_MASK;
+ *
+ * 第二次从vmx_enable_tdp()-->ept_set_mmio_spte_mask()进入的结果:
+ *   mmio_value = 0x0000000000000006
+ *   mmio_mask  = 0x0000000000000007
+ *   shadow_mmio_mask  = 0x4000000000000007
+ *   shadow_mmio_value = 0x4000000000000006
+ *
+ * 设置WX并且还有1往左62位 (SPTE_SPECIAL_MASK)
+ */
 static u64 __read_mostly shadow_mmio_value;
+/*
+ * 在以下设置shadow_present_mask:
+ *   - arch/x86/kvm/mmu.c|647| <<kvm_mmu_set_mask_ptes>> shadow_present_mask = p_mask;
+ *   - arch/x86/kvm/mmu.c|682| <<kvm_mmu_reset_all_pte_masks>> shadow_present_mask = 0;
+ *
+ * 在new machine上是0ull
+ * cpu_has_vmx_ept_execute_only() ? 0ull : VMX_EPT_READABLE_MASK;
+ */
 static u64 __read_mostly shadow_present_mask;
+/*
+ * 在以下设置shadow_me_mask:
+ *   - arch/x86/kvm/mmu.c|649| <<kvm_mmu_set_mask_ptes>> shadow_me_mask = me_mask;
+ *
+ * 在new machine上是0ull
+ */
 static u64 __read_mostly shadow_me_mask;
 
 /*
@@ -222,7 +504,34 @@ static u64 __read_mostly shadow_me_mask;
  * Non-present SPTEs with shadow_acc_track_value set are in place for access
  * tracking.
  */
+/*
+ * 设置shadow_acc_track_mask的地方:
+ *   - arch/x86/kvm/mmu.c|578| <<kvm_mmu_set_mask_ptes>> shadow_acc_track_mask = acc_track_mask;
+ *   - arch/x86/kvm/mmu.c|613| <<kvm_mmu_reset_all_pte_masks>> shadow_acc_track_mask = 0;
+ *
+ * 在new machine上从vmx_enable_tdp()第二次进来的结果:
+ *   shadow_acc_track_mask = VMX_EPT_RWX_MASK;
+ *   也就是0x0000000000000007
+ */
 static u64 __read_mostly shadow_acc_track_mask;
+/*
+ * SPTE_SPECIAL_MASK的意义:
+ * The mask used to denote special SPTEs, which can be either MMIO SPTEs or
+ * Access Tracking SPTEs. We use bit 62 instead of bit 63 to avoid conflicting
+ * with the SVE bit in EPT PTEs.
+ *
+ * 只在这里修改shadow_acc_track_value, 使用的地方:
+ *   - arch/x86/kvm/mmu.c|424| <<spte_ad_enabled>> return !(spte & shadow_acc_track_value);
+ *   - arch/x86/kvm/mmu.c|570| <<kvm_mmu_set_mask_ptes>> BUG_ON(acc_track_mask & shadow_acc_track_value);
+ *   - arch/x86/kvm/mmu.c|2734| <<link_shadow_page>> spte |= shadow_acc_track_value;
+ *   - arch/x86/kvm/mmu.c|3075| <<set_spte>> spte |= shadow_acc_track_value;
+ *
+ * 1向左移动62位, 也就是SPTE_SPECIAL_MASK
+ * SPTE_SPECIAL_MASK的意义:
+ * The mask used to denote special SPTEs, which can be either MMIO SPTEs or
+ * Access Tracking SPTEs. We use bit 62 instead of bit 63 to avoid conflicting
+ * with the SVE bit in EPT PTEs.
+ */
 static const u64 shadow_acc_track_value = SPTE_SPECIAL_MASK;
 
 /*
@@ -231,19 +540,48 @@ static const u64 shadow_acc_track_value = SPTE_SPECIAL_MASK;
  * PTEs being access tracked also need to be dirty tracked, so the W bit will be
  * restored only when a write is attempted to the page.
  */
+/*
+ * 只在这里修改, 使用shadow_acc_track_saved_bits_mask的地方:
+ *   - arch/x86/kvm/mmu.c|1023| <<mark_spte_for_access_track>> WARN_ONCE(spte & (shadow_acc_track_saved_bits_mask <<
+ *   - arch/x86/kvm/mmu.c|1027| <<mark_spte_for_access_track>> spte |= (spte & shadow_acc_track_saved_bits_mask) <<
+ *   - arch/x86/kvm/mmu.c|1039| <<restore_acc_track_spte>> & shadow_acc_track_saved_bits_mask;
+ *   - arch/x86/kvm/mmu.c|1045| <<restore_acc_track_spte>> new_spte &= ~(shadow_acc_track_saved_bits_mask <<
+ *
+ * 第0位和第2位 (read and exec)是1
+ */
 static const u64 shadow_acc_track_saved_bits_mask = PT64_EPT_READABLE_MASK |
 						    PT64_EPT_EXECUTABLE_MASK;
+/*
+ * 只在这里修改, 使用shadow_acc_track_saved_bits_shift的地方:
+ *   - arch/x86/kvm/mmu.c|1024| <<mark_spte_for_access_track>> shadow_acc_track_saved_bits_shift),
+ *   - arch/x86/kvm/mmu.c|1028| <<mark_spte_for_access_track>> shadow_acc_track_saved_bits_shift;
+ *   - arch/x86/kvm/mmu.c|1038| <<restore_acc_track_spte>> u64 saved_bits = (spte >> shadow_acc_track_saved_bits_shift)
+ *   - arch/x86/kvm/mmu.c|1046| <<restore_acc_track_spte>> shadow_acc_track_saved_bits_shift);
+ *
+ * 52
+ */
 static const u64 shadow_acc_track_saved_bits_shift = PT64_SECOND_AVAIL_BITS_SHIFT;
 
 /*
  * This mask must be set on all non-zero Non-Present or Reserved SPTEs in order
  * to guard against L1TF attacks.
  */
+/*
+ * 在以下修改shadow_nonpresent_or_rsvd_mask:
+ *   - arch/x86/kvm/mmu.c|627| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_mask = 0;
+ *   - arch/x86/kvm/mmu.c|631| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_mask =
+ *                                                                       rsvd_bits(boot_cpu_data.x86_cache_bits -
+ *                                                                                 shadow_nonpresent_or_rsvd_mask_len,
+ *                                                                                 boot_cpu_data.x86_cache_bits - 1);
+ */
 static u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
 
 /*
  * The number of high-order 1 bits to use in the mask above.
  */
+/*
+ * 只在这一处修改shadow_nonpresent_or_rsvd_mask_len
+ */
 static const u64 shadow_nonpresent_or_rsvd_mask_len = 5;
 
 /*
@@ -254,12 +592,25 @@ static const u64 shadow_nonpresent_or_rsvd_mask_len = 5;
  * left into the reserved bits, i.e. the GFN in the SPTE will be split into
  * high and low parts.  This mask covers the lower bits of the GFN.
  */
+/*
+ * 只在一处设置shadow_nonpresent_or_rsvd_lower_gfn_mask:
+ *   - arch/x86/kvm/mmu.c|621| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_lower_gfn_mask =
+ *                                                                           rsvd_bits(boot_cpu_data.x86_cache_bits -
+ *                                                                           shadow_nonpresent_or_rsvd_mask_len,
+ *                                                                           boot_cpu_data.x86_cache_bits - 1);
+ */
 static u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
 /*
  * The number of non-reserved physical address bits irrespective of features
  * that repurpose legal bits, e.g. MKTME.
  */
+/*
+ * 只在一处设置shadow_phys_bits:
+ *   - arch/x86/kvm/mmu.c|597| <<kvm_mmu_reset_all_pte_masks>> shadow_phys_bits = kvm_get_shadow_phys_bits();
+ *
+ * 在new machine上是39
+ */
 static u8 __read_mostly shadow_phys_bits;
 
 static void mmu_spte_set(u64 *sptep, u64 spte);
@@ -271,11 +622,20 @@ kvm_mmu_calc_root_page_role(struct kvm_vcpu *vcpu);
 #include "mmutrace.h"
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1830| <<kvm_set_pte_rmapp>> if (need_flush && kvm_available_flush_tlb_with_range()) {
+ *   - arch/x86/kvm/mmu.c|5873| <<kvm_mmu_zap_collapsible_spte>> if (kvm_available_flush_tlb_with_range())
+ */
 static inline bool kvm_available_flush_tlb_with_range(void)
 {
 	return kvm_x86_ops->tlb_remote_flush_with_range;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|559| <<kvm_flush_remote_tlbs_with_address>> kvm_flush_remote_tlbs_with_range(kvm, &range);
+ */
 static void kvm_flush_remote_tlbs_with_range(struct kvm *kvm,
 		struct kvm_tlb_range *range)
 {
@@ -288,6 +648,23 @@ static void kvm_flush_remote_tlbs_with_range(struct kvm *kvm,
 		kvm_flush_remote_tlbs(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1825| <<drop_large_spte>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, sp->gfn,
+ *   - arch/x86/kvm/mmu.c|2114| <<kvm_set_pte_rmapp>> kvm_flush_remote_tlbs_with_address(kvm, gfn, 1);
+ *   - arch/x86/kvm/mmu.c|2297| <<rmap_recycle>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, sp->gfn,
+ *   - arch/x86/kvm/mmu.c|2822| <<kvm_mmu_get_page>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, gfn, 1);
+ *   - arch/x86/kvm/mmu.c|2942| <<validate_direct_spte>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, child->gfn, 1);
+ *   - arch/x86/kvm/mmu.c|3380| <<mmu_set_spte>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, gfn,
+ *   - arch/x86/kvm/mmu.c|5878| <<slot_handle_level_range>> kvm_flush_remote_tlbs_with_address(kvm,
+ *   - arch/x86/kvm/mmu.c|5888| <<slot_handle_level_range>> kvm_flush_remote_tlbs_with_address(kvm, start_gfn,
+ *   - arch/x86/kvm/mmu.c|6150| <<kvm_mmu_slot_remove_write_access>> kvm_flush_remote_tlbs_with_address(kvm, memslot->base_gfn,
+ *   - arch/x86/kvm/mmu.c|6181| <<kvm_mmu_zap_collapsible_spte>> kvm_flush_remote_tlbs_with_address(kvm, sp->gfn,
+ *   - arch/x86/kvm/mmu.c|6221| <<kvm_mmu_slot_leaf_clear_dirty>> kvm_flush_remote_tlbs_with_address(kvm, memslot->base_gfn,
+ *   - arch/x86/kvm/mmu.c|6240| <<kvm_mmu_slot_largepage_remove_write_access>> kvm_flush_remote_tlbs_with_address(kvm, memslot->base_gfn,
+ *   - arch/x86/kvm/mmu.c|6258| <<kvm_mmu_slot_set_dirty>> kvm_flush_remote_tlbs_with_address(kvm, memslot->base_gfn,
+ *   - arch/x86/kvm/paging_tmpl.h|912| <<FNAME>> kvm_flush_remote_tlbs_with_address(vcpu->kvm,
+ */
 static void kvm_flush_remote_tlbs_with_address(struct kvm *kvm,
 		u64 start_gfn, u64 pages)
 {
@@ -299,39 +676,166 @@ static void kvm_flush_remote_tlbs_with_address(struct kvm *kvm,
 	kvm_flush_remote_tlbs_with_range(kvm, &range);
 }
 
+/*
+ * 第一次进入:
+ * [0] kvm_mmu_set_mmio_spte_mask
+ * [0] kvm_mmu_module_init [kvm]
+ * [0] kvm_arch_init [kvm]
+ * [0] kvm_init [kvm]
+ * [0] vmx_init [kvm_intel]
+ * [0] do_one_initcall
+ * [0] do_init_module
+ * [0] load_module
+ * [0] __do_sys_finit_module
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * 第二次进入:
+ * [0] kvm_mmu_set_mmio_spte_mask
+ * [0] hardware_setup [kvm_intel]
+ * [0] kvm_arch_hardware_setup [kvm]
+ * [0] kvm_init [kvm]
+ * [0] vmx_init [kvm_intel]
+ * [0] do_one_initcall
+ * [0] do_init_module
+ * [0] load_module
+ * [0] __do_sys_finit_module
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu.c|6324| <<kvm_set_mmio_spte_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask);
+ *   - arch/x86/kvm/vmx/vmx.c|4035| <<ept_set_mmio_spte_mask>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_RWX_MASK,
+ */
 void kvm_mmu_set_mmio_spte_mask(u64 mmio_mask, u64 mmio_value)
 {
+	/*
+	 * 第一次从kvm_mmu_module_init()-->kvm_set_mmio_spte_mask()进入的结果:
+	 *   mmio_value = 0x0008000000000001
+	 *   mmio_mask  = 0x0008000000000001
+	 *   shadow_mmio_mask  = 0x4008000000000001
+	 *   shadow_mmio_value = 0x4008000000000001
+	 *
+	 * 第二次从vmx_enable_tdp()-->ept_set_mmio_spte_mask()进入的结果:
+	 *   mmio_value = 0x0000000000000006
+	 *   mmio_mask  = 0x0000000000000007
+	 *   shadow_mmio_mask  = 0x4000000000000007
+	 *   shadow_mmio_value = 0x4000000000000006
+	 */
+	/*
+	 * 从vmx_enable_tdp()-->ept_set_mmio_spte_mask()第二次进入的例子:
+	 *   shadow_mmio_value = VMX_EPT_MISCONFIG_WX_VALUE | SPTE_SPECIAL_MASK;
+	 *   shadow_mmio_mask = VMX_EPT_RWX_MASK | SPTE_SPECIAL_MASK;
+	 */
 	BUG_ON((mmio_mask & mmio_value) != mmio_value);
 	shadow_mmio_value = mmio_value | SPTE_SPECIAL_MASK;
 	shadow_mmio_mask = mmio_mask | SPTE_SPECIAL_MASK;
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mmio_spte_mask);
 
+/*
+ * 应该是用来判断硬件ept是否支持ad bit (access & dirty)
+ */
 static inline bool sp_ad_disabled(struct kvm_mmu_page *sp)
 {
+	/*
+	 * 设置ad_disabled的地方:
+	 *   - 在struct kvm_mmu_page_role中默认是1
+	 *   - union kvm_mmu_page_role mmu_base_role_mask中默认是1 (.ad_disabled = 1)
+	 *   - arch/x86/kvm/mmu.c|5343| <<kvm_calc_tdp_mmu_root_page_role>> role.base.ad_disabled = (shadow_accessed_mask == 0);
+	 *   - arch/x86/kvm/mmu.c|5459| <<kvm_calc_shadow_ept_root_page_role>> role.base.ad_disabled = !accessed_dirty;
+	 *   - arch/x86/kvm/vmx/nested.c|5009| <<nested_vmx_eptp_switching>> mmu->mmu_role.base.ad_disabled = !accessed_dirty;
+	 */
 	return sp->role.ad_disabled;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|542| <<spte_shadow_accessed_mask>> return spte_ad_enabled(spte) ? shadow_accessed_mask : 0;
+ *   - arch/x86/kvm/mmu.c|548| <<spte_shadow_dirty_mask>> return spte_ad_enabled(spte) ? shadow_dirty_mask : 0;
+ *   - arch/x86/kvm/mmu.c|553| <<is_access_track_spte>> return !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;
+ *   - arch/x86/kvm/mmu.c|959| <<spte_has_volatile_bits>> if (spte_ad_enabled(spte)) {
+ *   - arch/x86/kvm/mmu.c|1120| <<mark_spte_for_access_track>> if (spte_ad_enabled(spte))
+ *   - arch/x86/kvm/mmu.c|1153| <<restore_acc_track_spte>> WARN_ON_ONCE(spte_ad_enabled(spte));
+ *   - arch/x86/kvm/mmu.c|1172| <<mmu_spte_age>> if (spte_ad_enabled(spte)) {
+ *   - arch/x86/kvm/mmu.c|1842| <<__rmap_clear_dirty>> if (spte_ad_enabled(*sptep))
+ *   - arch/x86/kvm/mmu.c|1868| <<__rmap_set_dirty>> if (spte_ad_enabled(*sptep))
+ *
+ * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了, 说明ad不支持, 没enabled, 返回false
+ */
 static inline bool spte_ad_enabled(u64 spte)
 {
 	MMU_WARN_ON((spte & shadow_mmio_mask) == shadow_mmio_value);
+	/*
+	 * 1向左移动62位, 也就是SPTE_SPECIAL_MASK
+	 * SPTE_SPECIAL_MASK的意义:
+	 * The mask used to denote special SPTEs, which can be either MMIO SPTEs or
+	 * Access Tracking SPTEs. We use bit 62 instead of bit 63 to avoid conflicting
+	 * with the SVE bit in EPT PTEs.
+	 */
 	return !(spte & shadow_acc_track_value);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1536| <<is_accessed_spte>> u64 accessed_mask = spte_shadow_accessed_mask(spte);
+ *   - arch/x86/kvm/mmu.c|3810| <<set_spte>> spte |= spte_shadow_accessed_mask(spte);
+ *
+ * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了, 说明ad不支持, 返回0
+ * 如果没设置, 说明pte支持ad bit, 所以返回shadow_accessed_mask
+ */
 static inline u64 spte_shadow_accessed_mask(u64 spte)
 {
 	MMU_WARN_ON((spte & shadow_mmio_mask) == shadow_mmio_value);
+	/*
+	 * 关于shadow_accessed_mask
+	 * 在new machine上从vmx_enable_tdp()第二次进来的结果:
+	 *   shadow_accessed_mask = enable_ept_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+	 *   也就是0x0000000000000100
+	 */
 	return spte_ad_enabled(spte) ? shadow_accessed_mask : 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1544| <<is_dirty_spte>> u64 dirty_mask = spte_shadow_dirty_mask(spte);
+ *   - arch/x86/kvm/mmu.c|3870| <<set_spte>> spte |= spte_shadow_dirty_mask(spte);
+ *
+ * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了, 说明ad不支持, 返回0
+ * 如果没设置, 说明pte支持ad bit, 所以返回shadow_dirty_mask
+ */
 static inline u64 spte_shadow_dirty_mask(u64 spte)
 {
 	MMU_WARN_ON((spte & shadow_mmio_mask) == shadow_mmio_value);
+	/*
+	 * 关于shadow_dirty_mask
+	 * 在new machine上从vmx_enable_tdp()第二次进来的结果:
+	 *   shadow_dirty_mask = enable_ept_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull,
+	 *   也就是0x0000000000000200
+	 */
 	return spte_ad_enabled(spte) ? shadow_dirty_mask : 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1225| <<spte_has_volatile_bits>> is_access_track_spte(spte))
+ *   - arch/x86/kvm/mmu.c|1242| <<is_accessed_spte>> : !is_access_track_spte(spte);
+ *   - arch/x86/kvm/mmu.c|1392| <<mark_spte_for_access_track>> if (is_access_track_spte(spte))
+ *   - arch/x86/kvm/mmu.c|1423| <<restore_acc_track_spte>> WARN_ON_ONCE(!is_access_track_spte(spte));
+ *   - arch/x86/kvm/mmu.c|3936| <<fast_page_fault>> if (is_access_track_spte(spte))
+ *
+ * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了(说明ad不支持),
+ * 并且spte在shadow_acc_track_mask中的bit都没设置
+ * 则返回true (如果返回true说明ad bit不支持)
+ */
 static inline bool is_access_track_spte(u64 spte)
 {
+	/*
+	 * 关于shadow_acc_track_mask
+	 * 在new machine上从vmx_enable_tdp()第二次进来的结果:
+	 *   shadow_acc_track_mask = VMX_EPT_RWX_MASK;
+	 *   也就是0x0000000000000007
+	 */
 	return !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;
 }
 
@@ -360,52 +864,165 @@ static inline bool is_access_track_spte(u64 spte)
 #define MMIO_SPTE_GEN_HIGH_END		61
 #define MMIO_SPTE_GEN_HIGH_MASK		GENMASK_ULL(MMIO_SPTE_GEN_HIGH_END, \
 						    MMIO_SPTE_GEN_HIGH_START)
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|848| <<mark_mmio_spte>> u64 mask = generation_mmio_spte_mask(gen);
+ *   - arch/x86/kvm/mmu.c|888| <<get_mmio_spte_access>> u64 mask = generation_mmio_spte_mask(MMIO_SPTE_GEN_MASK) | shadow_mmio_mask;
+ *
+ * spte bits 3-11 are used as bits 0-8 of the generation number,
+ * the bits 52-61 are used as bits 9-18 of the generation number.
+ *
+ * 把gen的第0-8位取出来左移2位, 变成spte的3-11位
+ * 把gen的第9-18位取出来左移52位, 变成spte的52-61位
+ * 把两个拼在一起, 成为了spte的gen mask
+ */
 static u64 generation_mmio_spte_mask(u64 gen)
 {
 	u64 mask;
 
+	/*
+	 * ~MMIO_SPTE_GEN_MASK除了0-18位都是1
+	 * 说明要求参数gen必须只是gen
+	 */
 	WARN_ON(gen & ~MMIO_SPTE_GEN_MASK);
 
+	/*
+	 * spte bits 3-11 are used as bits 1-9 of the generation number,
+	 * the bits 52-61 are used as bits 10-19 of the generation number.
+	 *
+	 * 把gen的第0-8位取出来左移2位, 变成spte的3-11位
+	 * 把gen的第9-18位取出来左移52位, 变成spte的52-61位
+	 * 把两个拼在一起, 成为了spte的gen mask
+	 */
 	mask = (gen << MMIO_SPTE_GEN_LOW_START) & MMIO_SPTE_GEN_LOW_MASK;
 	mask |= (gen << MMIO_SPTE_GEN_HIGH_START) & MMIO_SPTE_GEN_HIGH_MASK;
 	return mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|912| <<check_mmio_spte>> spte_gen = get_mmio_spte_generation(spte);
+ *
+ * spte bits 3-11 are used as bits 0-8 of the generation number,
+ * the bits 52-61 are used as bits 9-18 of the generation number.
+ *
+ * 给定一个spte, 获得其中的mmio generation
+ * 把spte的3-11位和52-61位取出来拼在一起, 组成gen
+ */
 static u64 get_mmio_spte_generation(u64 spte)
 {
 	u64 gen;
 
+	/* 把spte的最后3位清0 */
 	spte &= ~shadow_mmio_mask;
 
+	/*
+	 * spte bits 3-11 are used as bits 0-8 of the generation number,
+	 * the bits 52-61 are used as bits 9-18 of the generation number.
+	 *
+	 * 把spte的3-11位和52-61位取出来拼在一起, 组成gen
+	 */
 	gen = (spte & MMIO_SPTE_GEN_LOW_MASK) >> MMIO_SPTE_GEN_LOW_START;
 	gen |= (spte & MMIO_SPTE_GEN_HIGH_MASK) >> MMIO_SPTE_GEN_HIGH_START;
 	return gen;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|708| <<set_mmio_spte>> mark_mmio_spte(vcpu, sptep, gfn, access);
+ *   - arch/x86/kvm/mmu.c|4605| <<sync_mmio_spte>> mark_mmio_spte(vcpu, sptep, gfn, access);
+ *
+ * 根据gfn为sptep设置entry, 最后3位是110, 用来作为mmio的entry
+ * 似乎是设置完mmio之后:
+ * 1. SPTE_SPECIAL_MASK(1往左移62位)是1
+ * 2. 结尾是110
+ * 3. 还有gen的信息
+ */
 static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 			   unsigned access)
 {
+	/* 返回kvm_vcpu_memslots(vcpu)->generation中的0-18位 */
 	u64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;
+	/*
+	 * spte bits 3-11 are used as bits 0-8 of the generation number,
+	 * the bits 52-61 are used as bits 9-18 of the generation number.
+	 *
+	 * 把gen的第0-8位取出来左移2位, 变成spte的3-11位
+	 * 把gen的第9-18位取出来左移52位, 变成spte的52-61位
+	 * 把两个拼在一起, 成为了spte的gen mask
+	 */
 	u64 mask = generation_mmio_spte_mask(gen);
 	u64 gpa = gfn << PAGE_SHIFT;
 
+	/* 获取access的第1和第2位 (从第0位开始) */
 	access &= ACC_WRITE_MASK | ACC_USER_MASK;
+	/*
+	 * shadow_mmio_value是110 (还有SPTE_SPECIAL_MASK是1往左移62位)
+	 */
 	mask |= shadow_mmio_value | access;
 	mask |= gpa | shadow_nonpresent_or_rsvd_mask;
 	mask |= (gpa & shadow_nonpresent_or_rsvd_mask)
 		<< shadow_nonpresent_or_rsvd_mask_len;
 
+	/*
+	 * page_header():
+	 * __pa(sptep)可能是一个指向某个pte的地址 (__pa(sptep)保存的这个pte的地址而不是内容)
+	 * 获得包含这个地址的页表页对应的kvm_mmu_page
+	 *
+	 * 在以下使用mmio_cached:
+	 *   - arch/x86/kvm/mmu.c|952| <<mark_mmio_spte>> page_header(__pa(sptep))->mmio_cached = true;
+	 *   - arch/x86/kvm/mmu.c|6916| <<__kvm_mmu_zap_all>> if (mmio_only && !sp->mmio_cached)
+	 */
 	page_header(__pa(sptep))->mmio_cached = true;
 
 	trace_mark_mmio_spte(sptep, gfn, access, gen);
+	/* 核心就是用WRITE_ONCE(*sptep, spte)更新spte */
 	mmu_spte_set(sptep, mask);
+	
+	/*
+	 * 似乎是设置完mmio之后:
+	 * 1. SPTE_SPECIAL_MASK(1往左移62位)是1
+	 * 2. 结尾是110
+	 * 3. 还有gen的信息
+	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|833| <<is_shadow_present_pte>> return (pte != 0) && !is_mmio_spte(pte);
+ *   - arch/x86/kvm/mmu.c|2965| <<mmu_page_zap_pte>> if (is_mmio_spte(pte))
+ *   - arch/x86/kvm/mmu.c|3383| <<mmu_set_spte>> if (unlikely(is_mmio_spte(*sptep)))
+ *   - arch/x86/kvm/mmu.c|4245| <<handle_mmio_page_fault>> if (is_mmio_spte(spte)) {
+ *   - arch/x86/kvm/mmu.c|4598| <<sync_mmio_spte>> if (unlikely(is_mmio_spte(*sptep))) {
+ *
+ * 如果最后3位的最后1位没设置(EW=110)并且还有SPTE_SPECIAL_MASK是1往左移62位,
+ * 就是mmio的spte
+ */
 static bool is_mmio_spte(u64 spte)
 {
+	/*
+	 * 从vmx_enable_tdp()-->ept_set_mmio_spte_mask()第二次进入的例子:
+	 *   shadow_mmio_value = VMX_EPT_MISCONFIG_WX_VALUE | SPTE_SPECIAL_MASK;
+	 *   shadow_mmio_mask = VMX_EPT_RWX_MASK | SPTE_SPECIAL_MASK;
+	 *
+	 * 第二次从vmx_enable_tdp()-->ept_set_mmio_spte_mask()进入的结果:
+	 *   mmio_value = 0x0000000000000006
+	 *   mmio_mask  = 0x0000000000000007
+	 *   shadow_mmio_mask  = 0x4000000000000007
+	 *   shadow_mmio_value = 0x4000000000000006
+	 *
+	 * 设置RWX并且还有1往左62位 (SPTE_SPECIAL_MASK)
+	 */
 	return (spte & shadow_mmio_mask) == shadow_mmio_value;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4600| <<handle_mmio_page_fault>> gfn_t gfn = get_mmio_spte_gfn(spte);
+ *   - arch/x86/kvm/mmu.c|4953| <<sync_mmio_spte>> if (gfn != get_mmio_spte_gfn(*sptep)) {
+ *
+ * ept页表是gpa-to-hpa, 这里核心是把spte右移动PAGE_SHIFT(12)
+ */
 static gfn_t get_mmio_spte_gfn(u64 spte)
 {
 	u64 gpa = spte & shadow_nonpresent_or_rsvd_lower_gfn_mask;
@@ -416,16 +1033,62 @@ static gfn_t get_mmio_spte_gfn(u64 spte)
 	return gpa >> PAGE_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4601| <<handle_mmio_page_fault>> unsigned access = get_mmio_spte_access(spte);
+ *
+ * spte bits 3-11 are used as bits 0-8 of the generation number,
+ * the bits 52-61 are used as bits 9-18 of the generation number.
+ *
+ * 把spte中0-11位和52-61位都清0并返回
+ * (包括gen的3-11位和52-61位, 以及最后3位, 还有0-11位的PAGE_MASK)
+ * 还有62位的SPTE_SPECIAL_MASK也情况
+ * 剩下的就是access?
+ */
 static unsigned get_mmio_spte_access(u64 spte)
 {
+	/*
+	 * generation_mmio_spte_mask():
+	 *     spte bits 3-11 are used as bits 0-8 of the generation number,
+	 *     the bits 52-61 are used as bits 9-18 of the generation number.
+	 *
+	 *     把gen的第0-8位取出来左移2位, 变成spte的3-11位
+	 *     把gen的第9-18位取出来左移52位, 变成spte的52-61位
+	 *     把两个拼在一起, 成为了spte的gen mask
+	 *
+	 * 总之把一个spte中对应gen的3-11位和52-61位, 以及最后3位做成mask
+	 */
 	u64 mask = generation_mmio_spte_mask(MMIO_SPTE_GEN_MASK) | shadow_mmio_mask;
+	/*
+	 * 把spte中0-11位和52-61位都清0并返回
+	 * (包括gen的3-11位和52-61位, 以及最后3位, 还有0-11位的PAGE_MASK)
+	 * 还有62位的SPTE_SPECIAL_MASK也情况
+	 * 剩下的就是access?
+	 */
 	return (spte & ~mask) & ~PAGE_MASK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3602| <<set_spte>> if (set_mmio_spte(vcpu, sptep, gfn, pfn, pte_access))
+ *
+ * 根据gfn为sptep设置entry, 最后3位是110, 用来作为mmio的entry
+ * 似乎是设置完mmio之后:
+ * 1. SPTE_SPECIAL_MASK(1往左移62位)是1
+ * 2. 结尾是110
+ * 3. 还有gen的信息
+ */
 static bool set_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,
 			  kvm_pfn_t pfn, unsigned access)
 {
 	if (unlikely(is_noslot_pfn(pfn))) {
+		/*
+		 * 根据gfn为sptep设置entry, 最后3位是110, 用来作为mmio的entry
+		 * 似乎是设置完mmio之后:
+		 * 1. SPTE_SPECIAL_MASK(1往左移62位)是1
+		 * 2. 结尾是110
+		 * 3. 还有gen的信息
+		 */
 		mark_mmio_spte(vcpu, sptep, gfn, access);
 		return true;
 	}
@@ -433,15 +1096,35 @@ static bool set_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,
 	return false;
 }
 
+/*
+ * 判断spte中的gen (把spte的3-11位和52-61位取出来拼在一起, 组成gen)
+ * 是否和kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK(0-18位)相等
+ */
 static bool check_mmio_spte(struct kvm_vcpu *vcpu, u64 spte)
 {
 	u64 kvm_gen, spte_gen, gen;
 
 	gen = kvm_vcpu_memslots(vcpu)->generation;
+	/*
+	 * 在以下使用KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS:
+	 *   - arch/x86/kvm/mmu.c|958| <<check_mmio_spte>> if (unlikely(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS))
+	 *   - arch/x86/kvm/mmu.c|6668| <<kvm_mmu_invalidate_mmio_sptes>> WARN_ON(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS);
+	 *   - arch/x86/kvm/x86.h|191| <<vcpu_cache_mmio_info>> if (unlikely(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS))
+	 *   - virt/kvm/kvm_main.c|891| <<install_new_memslots>> WARN_ON(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS);
+	 *   - virt/kvm/kvm_main.c|892| <<install_new_memslots>> slots->generation = gen | KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS;
+	 *   - virt/kvm/kvm_main.c|903| <<install_new_memslots>> gen = slots->generation & ~KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS;
+	 */
 	if (unlikely(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS))
 		return false;
 
 	kvm_gen = gen & MMIO_SPTE_GEN_MASK;
+	/*
+	 * spte bits 3-11 are used as bits 0-8 of the generation number,
+	 * the bits 52-61 are used as bits 9-18 of the generation number.
+	 *
+	 * 给定一个spte, 获得其中的mmio generation
+	 * 把spte的3-11位和52-61位取出来拼在一起, 组成gen
+	 */
 	spte_gen = get_mmio_spte_generation(spte);
 
 	trace_check_mmio_spte(spte, kvm_gen, spte_gen);
@@ -455,6 +1138,36 @@ static bool check_mmio_spte(struct kvm_vcpu *vcpu, u64 spte)
  *  - Setting either @accessed_mask or @dirty_mask requires setting both
  *  - At least one of @accessed_mask or @acc_track_mask must be set
  */
+/*
+ * 第一次:
+ * [0] kvm_mmu_set_mask_ptes
+ * [0] kvm_arch_init [kvm]
+ * [0] kvm_init [kvm]
+ * [0] vmx_init [kvm_intel]
+ * [0] do_one_initcall
+ * [0] do_init_module
+ * [0] load_module
+ * [0] __do_sys_finit_module
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * 第二次:
+ * [0] kvm_mmu_set_mask_ptes
+ * [0] hardware_setup [kvm_intel]
+ * [0] kvm_arch_hardware_setup [kvm]
+ * [0] kvm_init [kvm]
+ * [0] vmx_init [kvm_intel]
+ * [0] do_one_initcall
+ * [0] do_init_module
+ * [0] load_module
+ * [0] __do_sys_finit_module
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5289| <<vmx_enable_tdp>> kvm_mmu_set_mask_ptes(VMX_EPT_READABLE_MASK,
+ *   - arch/x86/kvm/x86.c|7070| <<kvm_arch_init>> kvm_mmu_set_mask_ptes(PT_USER_MASK, PT_ACCESSED_MASK,
+ */
 void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 		u64 dirty_mask, u64 nx_mask, u64 x_mask, u64 p_mask,
 		u64 acc_track_mask, u64 me_mask)
@@ -463,6 +1176,50 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 	BUG_ON(!accessed_mask && !acc_track_mask);
 	BUG_ON(acc_track_mask & shadow_acc_track_value);
 
+	/*
+	 * 第一次从kvm_arch_init()进入new machine的结果:
+	 *   shadow_user_mask = 0x0000000000000004
+	 *   shadow_accessed_mask = 0x0000000000000020
+	 *   shadow_dirty_mask = 0x0000000000000040
+	 *   shadow_nx_mask = 0x8000000000000000
+	 *   shadow_x_mask = 0x0000000000000000
+	 *   shadow_present_mask = 0x0000000000000001
+	 *   shadow_acc_track_mask = 0x0000000000000000
+	 *   shadow_me_mask = 0x0000000000000000
+	 *
+	 * 第二次从vmx_enable_tdp()进入new machine的结果:
+	 *   shadow_user_mask = 0x0000000000000001
+	 *   shadow_accessed_mask = 0x0000000000000100
+	 *   shadow_dirty_mask = 0x0000000000000200
+	 *   shadow_nx_mask = 0x0000000000000000
+	 *   shadow_x_mask = 0x0000000000000004
+	 *   shadow_present_mask = 0x0000000000000000
+	 *   shadow_acc_track_mask = 0x0000000000000007
+	 *   shadow_me_mask = 0x0000000000000000
+	 */
+
+	/*
+	 * 从kvm_arch_init()进来的结果:
+	 *   shadow_user_mask = PT_USER_MASK;
+	 *   shadow_accessed_mask = PT_ACCESSED_MASK;
+	 *   shadow_dirty_mask = PT_DIRTY_MASK;
+	 *   shadow_nx_mask = PT64_NX_MASK;
+	 *   shadow_x_mask = 0;
+	 *   shadow_present_mask = PT_PRESENT_MASK;
+	 *   shadow_acc_track_mask = 0;
+	 *   shadow_me_mask = sme_me_mask;
+	 *
+	 * 从vmx_enable_tdp()进来的结果:
+	 *   shadow_user_mask = VMX_EPT_READABLE_MASK;
+	 *   shadow_accessed_mask = enable_ept_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+	 *   shadow_dirty_mask = enable_ept_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull,
+	 *   shadow_nx_mask = 0ull;
+	 *   shadow_x_mask = VMX_EPT_EXECUTABLE_MASK;
+	 *   shadow_present_mask = cpu_has_vmx_ept_execute_only() ? 0ull : VMX_EPT_READABLE_MASK;
+	 *   shadow_acc_track_mask = VMX_EPT_RWX_MASK;
+	 *   shadow_me_mask = 0ull;
+	 */
+
 	shadow_user_mask = user_mask;
 	shadow_accessed_mask = accessed_mask;
 	shadow_dirty_mask = dirty_mask;
@@ -474,6 +1231,12 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mask_ptes);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|597| <<kvm_mmu_reset_all_pte_masks>> shadow_phys_bits = kvm_get_shadow_phys_bits();
+ *
+ * 在new machine上kvm_get_shadow_phys_bits()返回39
+ */
 static u8 kvm_get_shadow_phys_bits(void)
 {
 	/*
@@ -489,6 +1252,19 @@ static u8 kvm_get_shadow_phys_bits(void)
 	return cpuid_eax(0x80000008) & 0xff;
 }
 
+/*
+ * [0] kvm_mmu_reset_all_pte_masks
+ * [0] kvm_mmu_module_init [kvm]
+ * [0] kvm_arch_init [kvm]
+ * [0] kvm_init [kvm]
+ * [0] vmx_init [kvm_intel]
+ * [0] do_one_initcall
+ * [0] do_init_module
+ * [0] load_module
+ * [0] __do_sys_finit_module
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void kvm_mmu_reset_all_pte_masks(void)
 {
 	u8 low_phys_bits;
@@ -515,6 +1291,11 @@ static void kvm_mmu_reset_all_pte_masks(void)
 	 * the most significant bits of legal physical address space.
 	 */
 	shadow_nonpresent_or_rsvd_mask = 0;
+	/*
+	 * 在new machine上:
+	 *   shadow_phys_bits = 39
+	 *   low_phys_bits = 44
+	 */
 	low_phys_bits = boot_cpu_data.x86_cache_bits;
 	if (boot_cpu_data.x86_cache_bits <
 	    52 - shadow_nonpresent_or_rsvd_mask_len) {
@@ -540,30 +1321,66 @@ static int is_nx(struct kvm_vcpu *vcpu)
 	return vcpu->arch.efer & EFER_NX;
 }
 
+/*
+ * 如果pte不为0并且pte的最后3位不是110并且62位(SPTE_SPECIAL_MASK)也没设置, 返回true
+ * 如果spte是0x0,或者spte的最后3位是110(62位也设置了), 函数返回false
+ * 或者说, 如果spte是0或者是mmio, is_shadow_present_pte()返回false
+ */
 static int is_shadow_present_pte(u64 pte)
 {
+	/*
+	 * 如果最后3位的最后1位没设置(110)并且还有SPTE_SPECIAL_MASK是1往左移62位,
+	 * 就是mmio的spte
+	 */
 	return (pte != 0) && !is_mmio_spte(pte);
 }
 
+/*
+ * 查看pte的从0开始数第7位, 判断是否支持大页
+ */
 static int is_large_pte(u64 pte)
 {
+	/*
+	 * 1后面跟着7个0
+	 * 从0开始数是第7位
+	 */
 	return pte & PT_PAGE_SIZE_MASK;
 }
 
+/*
+ * 判断是否指向最后一级页表了(4K/2M/1G), 而不是下一级页表
+ */
 static int is_last_spte(u64 pte, int level)
 {
 	if (level == PT_PAGE_TABLE_LEVEL)
 		return 1;
+	/* 查看pte的从0开始数第7位, 判断是否支持大页 */
 	if (is_large_pte(pte))
 		return 1;
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4238| <<is_access_allowed>> return is_executable_pte(spte);
+ *
+ * 判断spte的从0开始的第3位(exec)是否设置了
+ */
 static bool is_executable_pte(u64 spte)
 {
+	/*
+	 * hadow_nx_mask在vmx下是0x0
+	 * shadow_x_mask在vmx下是VMX_EPT_EXECUTABLE_MASK
+	 *
+	 * 相当于在vmx上是:
+	 *   return (spte & (0x4 | 0x1)) == 0x1
+	 */
 	return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
 }
 
+/*
+ * 根据pte中的内容获得gfn
+ */
 static kvm_pfn_t spte_to_pfn(u64 pte)
 {
 	return (pte & PT64_BASE_ADDR_MASK) >> PAGE_SHIFT;
@@ -577,26 +1394,36 @@ static gfn_t pse36_gfn_delta(u32 gpte)
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1058| <<mmu_spte_set>> __set_spte(sptep, new_spte);
+ *
+ * 用WRITE_ONCE(*sptep, spte)更新spte
+ */
 static void __set_spte(u64 *sptep, u64 spte)
 {
 	WRITE_ONCE(*sptep, spte);
 }
 
+/* 用WRITE_ONCE(*sptep, spte)更新spte */
 static void __update_clear_spte_fast(u64 *sptep, u64 spte)
 {
 	WRITE_ONCE(*sptep, spte);
 }
 
+/* 用xchg(sptep, spte)更新spte */
 static u64 __update_clear_spte_slow(u64 *sptep, u64 spte)
 {
 	return xchg(sptep, spte);
 }
 
+/* 用READ_ONCE(*sptep)读取spte的内容 */
 static u64 __get_spte_lockless(u64 *sptep)
 {
 	return READ_ONCE(*sptep);
 }
 #else
+/* 不是CONFIG_X86_64使用 */
 union split_spte {
 	struct {
 		u32 spte_low;
@@ -605,6 +1432,7 @@ union split_spte {
 	u64 spte;
 };
 
+/* 不是CONFIG_X86_64使用 */
 static void count_spte_clear(u64 *sptep, u64 spte)
 {
 	struct kvm_mmu_page *sp =  page_header(__pa(sptep));
@@ -617,6 +1445,7 @@ static void count_spte_clear(u64 *sptep, u64 spte)
 	sp->clear_spte_count++;
 }
 
+/* 不是CONFIG_X86_64使用 */
 static void __set_spte(u64 *sptep, u64 spte)
 {
 	union split_spte *ssptep, sspte;
@@ -636,6 +1465,7 @@ static void __set_spte(u64 *sptep, u64 spte)
 	WRITE_ONCE(ssptep->spte_low, sspte.spte_low);
 }
 
+/* 不是CONFIG_X86_64使用 */
 static void __update_clear_spte_fast(u64 *sptep, u64 spte)
 {
 	union split_spte *ssptep, sspte;
@@ -655,6 +1485,7 @@ static void __update_clear_spte_fast(u64 *sptep, u64 spte)
 	count_spte_clear(sptep, spte);
 }
 
+/* 不是CONFIG_X86_64使用 */
 static u64 __update_clear_spte_slow(u64 *sptep, u64 spte)
 {
 	union split_spte *ssptep, sspte, orig;
@@ -689,6 +1520,7 @@ static u64 __update_clear_spte_slow(u64 *sptep, u64 spte)
  * present->non-present updates: if it changed while reading the spte,
  * we might have hit the race.  This is done using clear_spte_count.
  */
+/* 不是CONFIG_X86_64使用 */
 static u64 __get_spte_lockless(u64 *sptep)
 {
 	struct kvm_mmu_page *sp =  page_header(__pa(sptep));
@@ -713,14 +1545,40 @@ static u64 __get_spte_lockless(u64 *sptep)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1475| <<spte_has_volatile_bits>> if (spte_can_locklessly_be_made_writable(spte) ||
+ *   - arch/x86/kvm/mmu.c|1575| <<mmu_spte_update>> if (spte_can_locklessly_be_made_writable(old_spte) &&
+ *   - arch/x86/kvm/mmu.c|1663| <<mark_spte_for_access_track>> !spte_can_locklessly_be_made_writable(spte),
+ *   - arch/x86/kvm/mmu.c|2313| <<spte_write_protect>> !(pt_protect && spte_can_locklessly_be_made_writable(spte)))
+ *   - arch/x86/kvm/mmu.c|4207| <<fast_page_fault>> spte_can_locklessly_be_made_writable(spte))
+ *
+ * 测试spte的第11位和第12位是否是1
+ *
+ * SPTE_HOST_WRITEABLE means the gfn is writable on host.
+ *
+ * SPTE_MMU_WRITEABLE means the gfn is writable on mmu. The bit is set when
+ * the gfn is writable on guest mmu and it is not write-protected by shadow
+ * page write-protection.
+ */
 static bool spte_can_locklessly_be_made_writable(u64 spte)
 {
 	return (spte & (SPTE_HOST_WRITEABLE | SPTE_MMU_WRITEABLE)) ==
 		(SPTE_HOST_WRITEABLE | SPTE_MMU_WRITEABLE);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1557| <<mmu_spte_update_no_track>> if (!spte_has_volatile_bits(old_spte))
+ *   - arch/x86/kvm/mmu.c|1624| <<mmu_spte_clear_track_bits>> if (!spte_has_volatile_bits(old_spte))
+ */
 static bool spte_has_volatile_bits(u64 spte)
 {
+	/*
+	 * 如果pte不为0并且pte的最后3位不是110并且62位(SPTE_SPECIAL_MASK)也没设置, 返回true
+	 * 如果spte是0x0,或者spte的最后3位是110(62位也设置了), 函数返回false
+	 * 或者说, 如果spte是0或者是mmio, is_shadow_present_pte()返回false
+	 */
 	if (!is_shadow_present_pte(spte))
 		return false;
 
@@ -730,10 +1588,23 @@ static bool spte_has_volatile_bits(u64 spte)
 	 * also, it can help us to get a stable is_writable_pte()
 	 * to ensure tlb flush is not missed.
 	 */
+	/*
+	 * spte_can_locklessly_be_made_writable():
+	 * 测试spte的第11位和第12位是否是1 (SPTE_HOST_WRITEABLE和SPTE_MMU_WRITEABLE)
+	 *
+	 * is_access_track_spte():
+	 * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了(说明ad不支持),
+	 * 并且spte在shadow_acc_track_mask中的bit都没设置
+	 * 则返回true (如果返回true说明ad bit不支持)
+	 */
 	if (spte_can_locklessly_be_made_writable(spte) ||
 	    is_access_track_spte(spte))
 		return true;
 
+	/*
+	 * spte_ad_enabled():
+	 * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了, 说明ad不支持, 没enabled, 返回false
+	 */
 	if (spte_ad_enabled(spte)) {
 		if ((spte & shadow_accessed_mask) == 0 ||
 	    	    (is_writable_pte(spte) && (spte & shadow_dirty_mask) == 0))
@@ -743,16 +1614,48 @@ static bool spte_has_volatile_bits(u64 spte)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1709| <<mmu_spte_update>> if (is_accessed_spte(old_spte) && !is_accessed_spte(new_spte)) {
+ *   - arch/x86/kvm/mmu.c|1750| <<mmu_spte_clear_track_bits>> if (is_accessed_spte(old_spte))
+ *   - arch/x86/kvm/mmu.c|1835| <<mmu_spte_age>> if (!is_accessed_spte(spte))
+ *   - arch/x86/kvm/mmu.c|2888| <<kvm_test_age_rmapp>> if (is_accessed_spte(*sptep))
+ *
+ * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了, 说明ad不支持, 返回!is_access_track_spte(spte)
+ * 支持ad bit的时候返回access bit是否设置
+ */
 static bool is_accessed_spte(u64 spte)
 {
+	/*
+	 * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了, 说明ad不支持, 返回0
+	 * 如果没设置, 说明pte支持ad bit, 所以返回shadow_accessed_mask
+	 */
 	u64 accessed_mask = spte_shadow_accessed_mask(spte);
 
+	/*
+	 * is_access_track_spte():
+	 * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了(说明ad不支持),
+	 * 并且spte在shadow_acc_track_mask中的bit都没设置
+	 * 则返回true (如果返回true说明ad bit不支持)
+	 */
 	return accessed_mask ? spte & accessed_mask
 			     : !is_access_track_spte(spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1714| <<mmu_spte_update>> if (is_dirty_spte(old_spte) && !is_dirty_spte(new_spte)) {
+ *   - arch/x86/kvm/mmu.c|1753| <<mmu_spte_clear_track_bits>> if (is_dirty_spte(old_spte))
+ *
+ * 如果支持ad, 返回spte & shadow_dirty_mask
+ * 如果不支持ad, 返回spte & PT_WRITABLE_MASK, 也就是spte的write bit
+ */
 static bool is_dirty_spte(u64 spte)
 {
+	/*
+	 * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了, 说明ad不支持, 返回0
+	 * 如果没设置, 说明pte支持ad bit, 所以返回shadow_dirty_mask
+	 */
 	u64 dirty_mask = spte_shadow_dirty_mask(spte);
 
 	return dirty_mask ? spte & dirty_mask : spte & PT_WRITABLE_MASK;
@@ -764,9 +1667,20 @@ static bool is_dirty_spte(u64 spte)
  * or in a state where the hardware will not attempt to update
  * the spte.
  */
+/*
+ * 核心就是用WRITE_ONCE(*sptep, spte)更新spte
+ */
 static void mmu_spte_set(u64 *sptep, u64 new_spte)
 {
+	/*
+	 * 如果pte不为0并且pte的最后3位不是110并且62位(SPTE_SPECIAL_MASK)也没设置, 返回true
+	 * 如果spte是0x0,或者spte的最后3位是110(62位也设置了), 函数返回false
+	 * 或者说, 如果spte是0或者是mmio, is_shadow_present_pte()返回false
+	 */
 	WARN_ON(is_shadow_present_pte(*sptep));
+	/*
+	 * 用WRITE_ONCE(*sptep, spte)更新spte
+	 */
 	__set_spte(sptep, new_spte);
 }
 
@@ -774,17 +1688,45 @@ static void mmu_spte_set(u64 *sptep, u64 new_spte)
  * Update the SPTE (excluding the PFN), but do not track changes in its
  * accessed/dirty status.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1690| <<mmu_spte_update>> u64 old_spte = mmu_spte_update_no_track(sptep, new_spte);
+ *   - arch/x86/kvm/mmu.c|1850| <<mmu_spte_age>> mmu_spte_update_no_track(sptep, spte);
+ *
+ * Update the SPTE (excluding the PFN), but do not track changes in its
+ * accessed/dirty status.
+ * 根据是否有volatile bits用WRITE_ONCE(*sptep, spte)或者xchg(sptep, spte)更新spte
+ * 核心思想就是把new_spte拷贝到sptep, 返回旧的pte
+ */
 static u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)
 {
 	u64 old_spte = *sptep;
 
+	/*
+	 * 如果pte不为0并且pte的最后3位不是110并且62位(SPTE_SPECIAL_MASK)也没设置, 返回true
+	 * 如果spte是0x0,或者spte的最后3位是110(62位也设置了), 函数返回false
+	 * 或者说, 如果spte是0或者是mmio, is_shadow_present_pte()返回false
+	 */
 	WARN_ON(!is_shadow_present_pte(new_spte));
 
+	/*
+	 * 如果pte不为0并且pte的最后3位不是110并且62位(SPTE_SPECIAL_MASK)也没设置, 返回true
+	 * 如果spte是0x0,或者spte的最后3位是110(62位也设置了), 函数返回false
+	 * 或者说, 如果spte是0或者是mmio, is_shadow_present_pte()返回false
+	 */
 	if (!is_shadow_present_pte(old_spte)) {
+		/* 核心就是用WRITE_ONCE(*sptep, spte)更新spte */
 		mmu_spte_set(sptep, new_spte);
 		return old_spte;
 	}
 
+	/*
+	 * __update_clear_spte_fast():
+	 * 用WRITE_ONCE(*sptep, spte)更新spte
+	 *
+	 * __update_clear_spte_slow():
+	 * 用xchg(sptep, spte)更新spte
+	 */
 	if (!spte_has_volatile_bits(old_spte))
 		__update_clear_spte_fast(sptep, new_spte);
 	else
@@ -806,11 +1748,32 @@ static u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)
  *
  * Returns true if the TLB needs to be flushed
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2508| <<spte_write_protect>> return mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu.c|2533| <<spte_clear_dirty>> return mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu.c|2575| <<spte_set_dirty>> return mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu.c|3981| <<set_spte>> if (mmu_spte_update(sptep, spte))
+ *
+ * update the SPTE (excluding the PFN), but do not track changes in its
+ * accessed/dirty status.
+ * 根据是否有volatile bits用WRITE_ONCE(*sptep, spte)或者xchg(sptep, spte)更新spte
+ * 然后再根据情况(access和dirty)决定是否flush并返回决定的结果true/false
+ */
 static bool mmu_spte_update(u64 *sptep, u64 new_spte)
 {
 	bool flush = false;
+	/*
+	 * Update the SPTE (excluding the PFN), but do not track changes in its
+	 * accessed/dirty status.
+	 */
 	u64 old_spte = mmu_spte_update_no_track(sptep, new_spte);
 
+	/*
+	 * 如果pte不为0并且pte的最后3位不是110并且62位(SPTE_SPECIAL_MASK)也没设置, 返回true
+	 * 如果spte是0x0,或者spte的最后3位是110(62位也设置了), 函数返回false
+	 * 或者说, 如果spte是0或者是mmio, is_shadow_present_pte()返回false
+	 */
 	if (!is_shadow_present_pte(old_spte))
 		return false;
 
@@ -847,19 +1810,42 @@ static bool mmu_spte_update(u64 *sptep, u64 new_spte)
  * state bits, it is used to clear the last level sptep.
  * Returns non-zero if the PTE was previously valid.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2320| <<pte_list_remove>> mmu_spte_clear_track_bits(sptep);
+ *   - arch/x86/kvm/mmu.c|2453| <<drop_spte>> if (mmu_spte_clear_track_bits(sptep))
+ *   - arch/x86/kvm/mmu.c|2759| <<kvm_set_pte_rmapp>> mmu_spte_clear_track_bits(sptep);
+ *
+ * 根据是否有volatile bits用WRITE_ONCE(*sptep, spte)或者xchg(sptep, spte)更新spte为0!
+ * 如果之前如果spte是0或者是mmio, 则退出返回0
+ * 否则根据情况把pfn在host的struct page设置accessed或者dirty返回1
+ */
 static int mmu_spte_clear_track_bits(u64 *sptep)
 {
 	kvm_pfn_t pfn;
 	u64 old_spte = *sptep;
 
+	/*
+	 * __update_clear_spte_fast():
+	 * 用WRITE_ONCE(*sptep, spte)更新spte
+	 *
+	 * __update_clear_spte_slow()
+	 * 用xchg(sptep, spte)更新spte
+	 */
 	if (!spte_has_volatile_bits(old_spte))
 		__update_clear_spte_fast(sptep, 0ull);
 	else
 		old_spte = __update_clear_spte_slow(sptep, 0ull);
 
+	/*
+	 * 如果pte不为0并且pte的最后3位不是110并且62位(SPTE_SPECIAL_MASK)也没设置, 返回true
+	 * 如果spte是0x0,或者spte的最后3位是110(62位也设置了), 函数返回false
+	 * 或者说, 如果spte是0或者是mmio, is_shadow_present_pte()返回false
+	 */
 	if (!is_shadow_present_pte(old_spte))
 		return 0;
 
+	/* 根据pte中的内容获得gfn */
 	pfn = spte_to_pfn(old_spte);
 
 	/*
@@ -883,21 +1869,58 @@ static int mmu_spte_clear_track_bits(u64 *sptep)
  * Directly clear spte without caring the state bits of sptep,
  * it is used to set the upper level spte.
  */
+/*
+ * 用WRITE_ONCE(*sptep, spte)更新spte为0ull
+ */
 static void mmu_spte_clear_no_track(u64 *sptep)
 {
+	/* 用WRITE_ONCE(*sptep, spte)更新spte */
 	__update_clear_spte_fast(sptep, 0ull);
 }
 
+/*
+ * 用READ_ONCE(*sptep)读取spte的内容
+ */
 static u64 mmu_spte_get_lockless(u64 *sptep)
 {
+	/* 用READ_ONCE(*sptep)读取spte的内容 */
 	return __get_spte_lockless(sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1753| <<mmu_spte_age>> spte = mark_spte_for_access_track(spte);
+ *   - arch/x86/kvm/mmu.c|2618| <<kvm_set_pte_rmapp>> new_spte = mark_spte_for_access_track(new_spte);
+ *   - arch/x86/kvm/mmu.c|3839| <<set_spte>> spte = mark_spte_for_access_track(spte);
+ *
+ * 如果ept支持ad bit, 直接清空spte的access bit就可以了
+ * 否则把第0位和第2位 (r+x) 保存在52位之后, 清除最后的3位
+ */
 static u64 mark_spte_for_access_track(u64 spte)
 {
+	/*
+	 * spte_ad_enabled():
+	 * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了, 说明ad不支持, 没enabled, 返回false
+	 *
+	 * shadow_accessed_mask
+	 * 在new machine上从vmx_enable_tdp()第二次进来的结果:
+	 *   shadow_accessed_mask = enable_ept_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+	 *   也就是0x0000000000000100
+	 *
+	 * 如果ept支持ad bit, 直接清空spte的access bit就可以了
+	 */
 	if (spte_ad_enabled(spte))
 		return spte & ~shadow_accessed_mask;
 
+	/*
+	 * 下面应该就是ept不支持ad bit的情况了
+	 */
+
+	/*
+	 * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了(说明ad不支持),
+	 * 并且spte在shadow_acc_track_mask中的bit都没设置
+	 * 则返回true (如果返回true说明ad bit不支持)
+	 */
 	if (is_access_track_spte(spte))
 		return spte;
 
@@ -914,39 +1937,88 @@ static u64 mark_spte_for_access_track(u64 spte)
 			  shadow_acc_track_saved_bits_shift),
 		  "kvm: Access Tracking saved bit locations are not zero\n");
 
+	/* 把第0位和第2位 (r+x) 保存在52位之后 */
 	spte |= (spte & shadow_acc_track_saved_bits_mask) <<
 		shadow_acc_track_saved_bits_shift;
+	/*
+	 * 清除最后的3位
+	 * 上面已经把第0位和第2位 (r+x) 保存在52位之后
+	 */
 	spte &= ~shadow_acc_track_mask;
 
 	return spte;
 }
 
 /* Restore an acc-track PTE back to a regular PTE */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4246| <<fast_page_fault>> new_spte = restore_acc_track_spte(new_spte);
+ *
+ * Restore an acc-track PTE back to a regular PTE
+ * 把52位之后临时保存的的r+x清空
+ * 把52位之后临时保存的的r+x设置回来
+ */
 static u64 restore_acc_track_spte(u64 spte)
 {
 	u64 new_spte = spte;
+	/*
+	 * shadow_acc_track_saved_bits_shift是52
+	 *
+	 * shadow_acc_track_saved_bits_mask是第0位和第2位 (read and exec)是1
+	 *
+	 * 把保存在52位开始的第0位和第2位保存到saved_bits
+	 */
 	u64 saved_bits = (spte >> shadow_acc_track_saved_bits_shift)
 			 & shadow_acc_track_saved_bits_mask;
 
 	WARN_ON_ONCE(spte_ad_enabled(spte));
 	WARN_ON_ONCE(!is_access_track_spte(spte));
 
+	/*
+	 * 在new machine上shadow_acc_track_mask是111, 也就是7
+	 * 这里是清空最后3位
+	 */
 	new_spte &= ~shadow_acc_track_mask;
+	/*
+	 * 这里清空52位开始的第0位和第2位
+	 */
 	new_spte &= ~(shadow_acc_track_saved_bits_mask <<
 		      shadow_acc_track_saved_bits_shift);
+	/*
+	 * 把52位开始的第0位和第2位放入pte的第0位和第2位
+	 */
 	new_spte |= saved_bits;
 
 	return new_spte;
 }
 
 /* Returns the Accessed status of the PTE and resets it at the same time. */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3176| <<kvm_age_rmapp>> young |= mmu_spte_age(sptep);
+ *
+ * 清空pte的access
+ * 如果支持硬件a/d直接把对应的bit清空就可以了
+ * 否则把第0位和第2位 (r+x) 保存在52位之后, 清除最后的3位
+ */
 static bool mmu_spte_age(u64 *sptep)
 {
+	/*
+	 * 用READ_ONCE(*sptep)读取spte的内容
+	 */
 	u64 spte = mmu_spte_get_lockless(sptep);
 
+	/*
+	 * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了, 说明ad不支持, 返回!is_access_track_spte(spte)
+	 * 支持ad bit的时候返回access bit是否设置
+	 */
 	if (!is_accessed_spte(spte))
 		return false;
 
+	/*
+	 * spte_ad_enabled():
+	 * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了, 说明ad不支持, 没enabled, 返回false
+	 */
 	if (spte_ad_enabled(spte)) {
 		clear_bit((ffs(shadow_accessed_mask) - 1),
 			  (unsigned long *)sptep);
@@ -958,13 +2030,31 @@ static bool mmu_spte_age(u64 *sptep)
 		if (is_writable_pte(spte))
 			kvm_set_pfn_dirty(spte_to_pfn(spte));
 
+		/*
+		 * 如果ept支持ad bit, 直接清空spte的access bit就可以了
+		 * 否则把第0位和第2位 (r+x) 保存在52位之后, 清除最后的3位
+		 */
 		spte = mark_spte_for_access_track(spte);
+		/*
+		 * Update the SPTE (excluding the PFN), but do not track changes in its
+		 * accessed/dirty status.
+		 * 根据是否有volatile bits用WRITE_ONCE(*sptep, spte)或者xchg(sptep, spte)更新spte
+		 * 核心思想就是把new_spte拷贝到sptep, 返回旧的pte
+		 */
 		mmu_spte_update_no_track(sptep, spte);
 	}
 
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4353| <<fast_page_fault>> walk_shadow_page_lockless_begin(vcpu);
+ *   - arch/x86/kvm/mmu.c|4850| <<walk_shadow_page_get_mmio_spte>> walk_shadow_page_lockless_begin(vcpu);
+ *   - arch/x86/kvm/mmu.c|4950| <<shadow_page_table_clear_flood>> walk_shadow_page_lockless_begin(vcpu);
+ *
+ * 关闭中断, 把READING_SHADOW_PAGE_TABLES存入vcpu->mode
+ */
 static void walk_shadow_page_lockless_begin(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -977,9 +2067,20 @@ static void walk_shadow_page_lockless_begin(struct kvm_vcpu *vcpu)
 	 * Make sure a following spte read is not reordered ahead of the write
 	 * to vcpu->mode.
 	 */
+	/*
+	 * x86只在这里使用READING_SHADOW_PAGE_TABLES
+	 */
 	smp_store_mb(vcpu->mode, READING_SHADOW_PAGE_TABLES);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4438| <<fast_page_fault>> walk_shadow_page_lockless_end(vcpu);
+ *   - arch/x86/kvm/mmu.c|4868| <<walk_shadow_page_get_mmio_spte>> walk_shadow_page_lockless_end(vcpu);
+ *   - arch/x86/kvm/mmu.c|4956| <<shadow_page_table_clear_flood>> walk_shadow_page_lockless_end(vcpu);
+ *
+ * 把OUTSIDE_GUEST_MODE存入vcpu->mode, 开启中断
+ */
 static void walk_shadow_page_lockless_end(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -987,10 +2088,24 @@ static void walk_shadow_page_lockless_end(struct kvm_vcpu *vcpu)
 	 * reads to sptes.  If it does, kvm_mmu_commit_zap_page() can see us
 	 * OUTSIDE_GUEST_MODE and proceed to free the shadow page table.
 	 */
+	/*
+	 * x86下使用OUTSIDE_GUEST_MODE的地方:
+	 *   - arch/x86/kvm/mmu.c|1963| <<walk_shadow_page_lockless_end>> smp_store_release(&vcpu->mode, OUTSIDE_GUEST_MODE);
+	 *   - arch/x86/kvm/x86.c|8001| <<vcpu_enter_guest>> vcpu->mode = OUTSIDE_GUEST_MODE;
+	 *   - arch/x86/kvm/x86.c|8060| <<vcpu_enter_guest>> vcpu->mode = OUTSIDE_GUEST_MODE;
+	 *   - virt/kvm/kvm_main.c|202| <<kvm_request_needs_ipi>> return mode != OUTSIDE_GUEST_MODE;
+	 */
 	smp_store_release(&vcpu->mode, OUTSIDE_GUEST_MODE);
 	local_irq_enable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2021| <<mmu_topup_memory_caches>> r = mmu_topup_memory_cache(&vcpu->arch.mmu_pte_list_desc_cache,
+ *   - arch/x86/kvm/mmu.c|2028| <<mmu_topup_memory_caches>> r = mmu_topup_memory_cache(&vcpu->arch.mmu_page_header_cache,
+ *
+ * 核心就是分配, 然后放入cache->objects[cache->nobjs++] = obj
+ */
 static int mmu_topup_memory_cache(struct kvm_mmu_memory_cache *cache,
 				  struct kmem_cache *base_cache, int min)
 {
@@ -1007,11 +2122,22 @@ static int mmu_topup_memory_cache(struct kvm_mmu_memory_cache *cache,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2878| <<rmap_can_add>> return mmu_memory_cache_free_objects(cache);
+ *
+ * 返回cache->nobjs表示还有多少free的元素
+ */
 static int mmu_memory_cache_free_objects(struct kvm_mmu_memory_cache *cache)
 {
 	return cache->nobjs;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2187| <<mmu_free_memory_caches>> mmu_free_memory_cache(&vcpu->arch.mmu_pte_list_desc_cache,
+ *   - arch/x86/kvm/mmu.c|2190| <<mmu_free_memory_caches>> mmu_free_memory_cache(&vcpu->arch.mmu_page_header_cache,
+ */
 static void mmu_free_memory_cache(struct kvm_mmu_memory_cache *mc,
 				  struct kmem_cache *cache)
 {
@@ -1019,6 +2145,12 @@ static void mmu_free_memory_cache(struct kvm_mmu_memory_cache *mc,
 		kmem_cache_free(cache, mc->objects[--mc->nobjs]);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2176| <<mmu_topup_memory_caches>> r = mmu_topup_memory_cache_page(&vcpu->arch.mmu_page_cache, 8);
+ *
+ * cache->nobjs (free的数目) 不够了, 多分配一些
+ */
 static int mmu_topup_memory_cache_page(struct kvm_mmu_memory_cache *cache,
 				       int min)
 {
@@ -1035,29 +2167,49 @@ static int mmu_topup_memory_cache_page(struct kvm_mmu_memory_cache *cache,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2189| <<mmu_free_memory_caches>> mmu_free_memory_cache_page(&vcpu->arch.mmu_page_cache);
+ */
 static void mmu_free_memory_cache_page(struct kvm_mmu_memory_cache *mc)
 {
 	while (mc->nobjs)
 		free_page((unsigned long)mc->objects[--mc->nobjs]);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5666| <<nonpaging_page_fault>> r = mmu_topup_memory_caches(vcpu);
+ *   - arch/x86/kvm/mmu.c|5782| <<tdp_page_fault>> r = mmu_topup_memory_caches(vcpu);
+ *   - arch/x86/kvm/mmu.c|6829| <<kvm_mmu_load>> r = mmu_topup_memory_caches(vcpu);
+ *   - arch/x86/kvm/mmu.c|7007| <<kvm_mmu_pte_write>> mmu_topup_memory_caches(vcpu);
+ *   - arch/x86/kvm/paging_tmpl.h|768| <<FNAME(page_fault)>> r = mmu_topup_memory_caches(vcpu);
+ *   - arch/x86/kvm/paging_tmpl.h|888| <<FNAME(invlpg)>> mmu_topup_memory_caches(vcpu);
+ */
 static int mmu_topup_memory_caches(struct kvm_vcpu *vcpu)
 {
 	int r;
 
+	/* 核心就是分配, 然后放入cache->objects[cache->nobjs++] = obj */
 	r = mmu_topup_memory_cache(&vcpu->arch.mmu_pte_list_desc_cache,
 				   pte_list_desc_cache, 8 + PTE_PREFETCH_NUM);
 	if (r)
 		goto out;
+	/* cache->nobjs (free的数目) 不够了, 多分配一些 */
 	r = mmu_topup_memory_cache_page(&vcpu->arch.mmu_page_cache, 8);
 	if (r)
 		goto out;
+	/* 核心就是分配, 然后放入cache->objects[cache->nobjs++] = obj */
 	r = mmu_topup_memory_cache(&vcpu->arch.mmu_page_header_cache,
 				   mmu_page_header_cache, 4);
 out:
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|8097| <<kvm_mmu_destroy>> mmu_free_memory_caches(vcpu);
+ */
 static void mmu_free_memory_caches(struct kvm_vcpu *vcpu)
 {
 	mmu_free_memory_cache(&vcpu->arch.mmu_pte_list_desc_cache,
@@ -1067,6 +2219,16 @@ static void mmu_free_memory_caches(struct kvm_vcpu *vcpu)
 				mmu_page_header_cache);
 }
 
+/*
+ * x86下的调用:
+ *   - arch/x86/kvm/mmu.c|2059| <<mmu_alloc_pte_list_desc>> return mmu_memory_cache_alloc(&vcpu->arch.mmu_pte_list_desc_cache);
+ *   - arch/x86/kvm/mmu.c|3083| <<kvm_mmu_alloc_page>> sp = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_header_cache);
+ *   - arch/x86/kvm/mmu.c|3084| <<kvm_mmu_alloc_page>> sp->spt = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_cache);
+ *   - arch/x86/kvm/mmu.c|3086| <<kvm_mmu_alloc_page>> sp->gfns = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_cache);
+ *
+ * 获得p = mc->objects[--mc->nobjs]并返回
+ * mc->nobjs是可用的数目
+ */
 static void *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)
 {
 	void *p;
@@ -1076,24 +2238,98 @@ static void *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)
 	return p;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2236| <<pte_list_add>> desc = mmu_alloc_pte_list_desc(vcpu);
+ *   - arch/x86/kvm/mmu.c|2249| <<pte_list_add>> desc->more = mmu_alloc_pte_list_desc(vcpu);
+ *
+ * 分配一个pte_list_desc
+ */
 static struct pte_list_desc *mmu_alloc_pte_list_desc(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 获得p = mc->objects[--mc->nobjs]并返回
+	 * mc->nobjs是可用的数目
+	 */
 	return mmu_memory_cache_alloc(&vcpu->arch.mmu_pte_list_desc_cache);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2326| <<pte_list_desc_remove_entry>> mmu_free_pte_list_desc(desc);
+ *
+ * 回收一个pte_list_desc
+ */
 static void mmu_free_pte_list_desc(struct pte_list_desc *pte_list_desc)
 {
 	kmem_cache_free(pte_list_desc_cache, pte_list_desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2219| <<kvm_mmu_page_set_gfn>> if (WARN_ON(gfn != kvm_mmu_page_get_gfn(sp, index)))
+ *   - arch/x86/kvm/mmu.c|2223| <<kvm_mmu_page_set_gfn>> kvm_mmu_page_get_gfn(sp, index), gfn);
+ *   - arch/x86/kvm/mmu.c|2629| <<rmap_remove>> gfn = kvm_mmu_page_get_gfn(sp, spte - sp->spt);
+ *   - arch/x86/kvm/mmu.c|4352| <<direct_pte_prefetch_many>> gfn = kvm_mmu_page_get_gfn(sp, start - sp->spt);
+ *   - arch/x86/kvm/mmu.c|4591| <<fast_pf_fix_direct_spte>> gfn = kvm_mmu_page_get_gfn(sp, sptep - sp->spt);
+ *   - arch/x86/kvm/mmu_audit.c|113| <<audit_mappings>> gfn = kvm_mmu_page_get_gfn(sp, sptep - sp->spt);
+ *   - arch/x86/kvm/mmu_audit.c|136| <<inspect_spte_has_rmap>> gfn = kvm_mmu_page_get_gfn(rev_sp, sptep - rev_sp->spt);
+ *
+ * 因为可能是大页, 根据index返回对应的gfn (每个entry表示一个gfn)
+ * 每个页表页(MMU page)对应一个数据结构kvm_mmu_page
+ * 这里是页表页, 也就是说kvm_mmu_page表示的page里全是entry!!!
+ */
 static gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index)
 {
+	/*
+	 * 设置sp->gfns的地方:
+	 *   - arch/x86/kvm/mmu.c|2304| <<kvm_mmu_page_set_gfn>> sp->gfns[index] = gfn;
+	 * 
+	 * 使用和分配sp->gfns的地方:
+	 *   - arch/x86/kvm/mmu.c|2283| <<kvm_mmu_page_get_gfn>> return sp->gfns[index];
+	 *   - arch/x86/kvm/mmu.c|3672| <<kvm_mmu_free_page>> free_page((unsigned long )sp->gfns);
+	 *   - arch/x86/kvm/mmu.c|3734| <<kvm_mmu_alloc_page>> sp->gfns = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_cache);
+	 *   - arch/x86/kvm/paging_tmpl.h|1030| <<FNAME(sync_page)>> if (gfn != sp->gfns[i]) {
+	 */
 	if (!sp->role.direct)
 		return sp->gfns[index];
 
+	/*
+	 * 设置sp->gfn的地方:
+	 *   - arch/x86/kvm/mmu.c|4173| <<kvm_mmu_get_page>> sp->gfn = gfn;
+	 *
+	 * 其他使用sp->gfn的地方:
+	 *   - arch/x86/kvm/mmu.c|2292| <<kvm_mmu_page_get_gfn>> return sp->gfn + (index << ((sp->role.level - 1) * PT64_LEVEL_BITS));
+	 *   - arch/x86/kvm/mmu.c|2311| <<kvm_mmu_page_set_gfn>> sp->gfn,
+	 *   - arch/x86/kvm/mmu.c|2418| <<account_shadowed>> gfn = sp->gfn;
+	 *   - arch/x86/kvm/mmu.c|2443| <<unaccount_shadowed>> gfn = sp->gfn;
+	 *   - arch/x86/kvm/mmu.c|3099| <<drop_large_spte>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, sp->gfn,
+	 *   - arch/x86/kvm/mmu.c|3601| <<rmap_recycle>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, sp->gfn,
+	 *   - arch/x86/kvm/mmu.c|4068| <<mmu_sync_children>> protected |= rmap_write_protect(vcpu, sp->gfn);
+	 *   - arch/x86/kvm/mmu.c|4139| <<kvm_mmu_get_page>> if (sp->gfn != gfn) {
+	 *   - arch/x86/kvm/mmu.c|7767| <<kvm_mmu_zap_collapsible_spte>> kvm_flush_remote_tlbs_with_address(kvm, sp->gfn,
+	 *   - arch/x86/kvm/mmu_audit.c|145| <<inspect_spte_has_rmap>> (long int )(sptep - rev_sp->spt), rev_sp->gfn);
+	 *   - arch/x86/kvm/mmu_audit.c|202| <<audit_write_protection>> slot = __gfn_to_memslot(slots, sp->gfn);
+	 *   - arch/x86/kvm/mmu_audit.c|203| <<audit_write_protection>> rmap_head = __gfn_to_rmap(sp->gfn, PT_PAGE_TABLE_LEVEL, slot);
+	 *   - arch/x86/kvm/mmu_audit.c|209| <<audit_write_protection>> sp->gfn, sp->role.word);
+	 *   - arch/x86/kvm/paging_tmpl.h|872| <<FNAME(get_level1_sp_gpa)>> return gfn_to_gpa(sp->gfn) + offset * sizeof(pt_element_t);
+	 *   - arch/x86/kvm/paging_tmpl.h|913| <<FNAME(invlpg)>> sp->gfn, KVM_PAGES_PER_HPAGE(sp->role.level));
+	 *
+	 * 1往左移动9位是512!!!!!
+	 *
+	 * ((1 - 1) * PT64_LEVEL_BITS) = 0
+	 * ((2 - 1) * PT64_LEVEL_BITS) = 9
+	 * ((3 - 1) * PT64_LEVEL_BITS) = 18
+	 */
 	return sp->gfn + (index << ((sp->role.level - 1) * PT64_LEVEL_BITS));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2617| <<rmap_add>> kvm_mmu_page_set_gfn(sp, spte - sp->spt, gfn);
+ *
+ * direct==true什么也不做
+ */
 static void kvm_mmu_page_set_gfn(struct kvm_mmu_page *sp, int index, gfn_t gfn)
 {
 	if (!sp->role.direct) {
@@ -1101,6 +2337,12 @@ static void kvm_mmu_page_set_gfn(struct kvm_mmu_page *sp, int index, gfn_t gfn)
 		return;
 	}
 
+	/*
+	 * 关于kvm_mmu_page_get_gfn():
+	 * 因为可能是大页, 根据index返回对应的gfn (每个entry表示一个gfn)
+	 * 每个页表页(MMU page)对应一个数据结构kvm_mmu_page
+	 * 这里是页表页, 也就是说kvm_mmu_page表示的page里全是entry!!!
+	 */
 	if (WARN_ON(gfn != kvm_mmu_page_get_gfn(sp, index)))
 		pr_err_ratelimited("gfn mismatch under direct page %llx "
 				   "(expected %llx, got %llx)\n",
@@ -1112,39 +2354,99 @@ static void kvm_mmu_page_set_gfn(struct kvm_mmu_page *sp, int index, gfn_t gfn)
  * Return the pointer to the large page information for a given gfn,
  * handling slots that are not large page aligned.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2110| <<update_gfn_disallow_lpage_count>> linfo = lpage_info_slot(gfn, slot, i);
+ *   - arch/x86/kvm/mmu.c|2176| <<__mmu_gfn_lpage_is_disallowed>> linfo = lpage_info_slot(gfn, slot, level);
+ *
+ * lpage_info的第二维的数量是对应level的page的数量, level越大(大页)数量越少
+ * 这里根据gfn返回对应的第二维的struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]
+ * 所以参数的level必须>=2
+ */
 static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
 					      struct kvm_memory_slot *slot,
 					      int level)
 {
 	unsigned long idx;
 
+	/*
+	 * base_gfn是基于4k开始的gfn
+	 * gfn是基于4k结束的gfn
+	 * 计算从开始到结束需要用到几个hugepage (或者普通page)
+	 *   level是1的时候hugepage大小是4K
+	 *   level是2的时候hugepage大小是2M
+	 *   level是3的时候hugepage大小是1G
+	 */
 	idx = gfn_to_index(gfn, slot->base_gfn, level);
+	/*
+	 * rmap和lpage_info的第二维都是在kvm_arch_create_memslot()分配
+	 * 数量是对应level的page的数量
+	 * level越大(大页)数量越少
+	 */
 	return &slot->arch.lpage_info[level - 2][idx];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2118| <<kvm_mmu_gfn_disallow_lpage>> update_gfn_disallow_lpage_count(slot, gfn, 1);
+ *   - arch/x86/kvm/mmu.c|2123| <<kvm_mmu_gfn_allow_lpage>> update_gfn_disallow_lpage_count(slot, gfn, -1);
+ *
+ * 根据gfn,找到其每一个2-3level上的二维struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]
+ * 然后linfo->disallow_lpage += count
+ */
 static void update_gfn_disallow_lpage_count(struct kvm_memory_slot *slot,
 					    gfn_t gfn, int count)
 {
 	struct kvm_lpage_info *linfo;
 	int i;
 
+	/*
+	 * PT_DIRECTORY_LEVEL    = 2
+	 * PT_MAX_HUGEPAGE_LEVEL = 3
+	 */
 	for (i = PT_DIRECTORY_LEVEL; i <= PT_MAX_HUGEPAGE_LEVEL; ++i) {
+		/*
+		 * lpage_info的第二维的数量是对应level的page的数量, level越大(大页)数量越少
+		 * 这里根据gfn返回对应的第二维的struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]
+		 */
 		linfo = lpage_info_slot(gfn, slot, i);
 		linfo->disallow_lpage += count;
 		WARN_ON(linfo->disallow_lpage < 0);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2217| <<account_shadowed>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ *   - arch/x86/kvm/page_track.c|104| <<kvm_slot_page_track_add_page>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ *
+ * 根据gfn,找到其每一个2-3level上的二维struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]
+ * 然后linfo->disallow_lpage增加1
+ */
 void kvm_mmu_gfn_disallow_lpage(struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2238| <<unaccount_shadowed>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ *   - arch/x86/kvm/page_track.c|138| <<kvm_slot_page_track_remove_page>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ *
+ * 根据gfn,找到其每一个2-3level上的二维struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]
+ * 然后linfo->disallow_lpage减少1
+ */
 void kvm_mmu_gfn_allow_lpage(struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, -1);
 }
 
+/*
+ * called by:
+ *   arch/x86/kvm/mmu.c|3300| <<kvm_mmu_get_page>> account_shadowed(vcpu->kvm, sp);
+ *
+ * 在caller中不会在(direct == 0)的情况下使用
+ */
 static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -1161,9 +2463,19 @@ static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 		return kvm_slot_page_track_add_page(kvm, slot, gfn,
 						    KVM_PAGE_TRACK_WRITE);
 
+	/*
+	 * 根据gfn,找到其每一个2-3level上的二维struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]
+	 * 然后linfo->disallow_lpage增加1
+	 */
 	kvm_mmu_gfn_disallow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3511| <<__kvm_mmu_prepare_zap_page>> unaccount_shadowed(kvm, sp);
+ *
+ * 在caller中不会在(direct == 0)的情况下使用
+ */
 static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -1178,15 +2490,31 @@ static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 		return kvm_slot_page_track_remove_page(kvm, slot, gfn,
 						       KVM_PAGE_TRACK_WRITE);
 
+	/*
+	 * 根据gfn,找到其每一个2-3level上的二维struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]
+	 * 然后linfo->disallow_lpage减少1
+	 */
 	kvm_mmu_gfn_allow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2362| <<mmu_gfn_lpage_is_disallowed>> return __mmu_gfn_lpage_is_disallowed(gfn, level, slot);
+ *   - arch/x86/kvm/mmu.c|2467| <<mapping_level>> if (__mmu_gfn_lpage_is_disallowed(large_gfn, level, slot))
+ *
+ * 如果(slot == NULL)直接返回true
+ * 否则根据gfn返回对应的第二维的struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]的disallow_lpage作为true/false
+ */
 static bool __mmu_gfn_lpage_is_disallowed(gfn_t gfn, int level,
 					  struct kvm_memory_slot *slot)
 {
 	struct kvm_lpage_info *linfo;
 
 	if (slot) {
+		/*
+		 * lpage_info的第二维的数量是对应level的page的数量, level越大(大页)数量越少
+		 * 这里根据gfn返回对应的第二维的struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]
+		 */
 		linfo = lpage_info_slot(gfn, slot, level);
 		return !!linfo->disallow_lpage;
 	}
@@ -1194,23 +2522,75 @@ static bool __mmu_gfn_lpage_is_disallowed(gfn_t gfn, int level,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4226| <<set_spte>> mmu_gfn_lpage_is_disallowed(vcpu, gfn, level))
+ *   - arch/x86/kvm/mmu.c|4488| <<transparent_hugepage_adjust>> !mmu_gfn_lpage_is_disallowed(vcpu, gfn, PT_DIRECTORY_LEVEL)) {
+ *
+ * 根据gfn找到对应的slot (struct kvm_memory_slot)
+ * 如果(slot == NULL)直接返回true
+ * 否则根据gfn返回对应的第二维的struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]的disallow_lpage作为true/false
+ */
 static bool mmu_gfn_lpage_is_disallowed(struct kvm_vcpu *vcpu, gfn_t gfn,
 					int level)
 {
 	struct kvm_memory_slot *slot;
 
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+	/*
+	 * 如果(slot == NULL)直接返回true
+	 * 否则根据gfn返回对应的第二维的struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]的disallow_lpage作为true/false
+	 */
 	return __mmu_gfn_lpage_is_disallowed(gfn, level, slot);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2318| <<mapping_level>> host_level = host_mapping_level(vcpu->kvm, large_gfn);
+ *
+ * 根据gfn获取其对于hva在host中的page size (4K,2M还是1G)
+ *     根据gfn获取在qemu中对应的hva, 把hva转化为vm_area_struct
+ *     然后就知道vm_area_struct的page size了
+ * 然后根据这个page size获取其在host上的mapping level
+ *
+ * 如果gfn对应的hva在host上的page size是4K
+ *   host_mapping_level()返回1
+ * 如果gfn对应的hva在host上的page size是2M
+ *   host_mapping_level()返回2
+ * 如果gfn对应的hva在host上的page size是1G
+ *   host_mapping_level()返回3
+ */
 static int host_mapping_level(struct kvm *kvm, gfn_t gfn)
 {
 	unsigned long page_size;
 	int i, ret = 0;
 
+	/*
+	 * 根据gfn获取其对于hva在host中的page size (4K,2M还是1G)
+	 *     根据gfn获取在qemu中对应的hva, 把hva转化为vm_area_struct
+	 *     然后就知道vm_area_struct的page size了
+	 */
 	page_size = kvm_host_page_size(kvm, gfn);
 
+	/*
+	 * PT_PAGE_TABLE_LEVEL   = 1
+	 * PT_MAX_HUGEPAGE_LEVEL = 3
+	 */
 	for (i = PT_PAGE_TABLE_LEVEL; i <= PT_MAX_HUGEPAGE_LEVEL; ++i) {
+		/*
+		 * KVM_HPAGE_SIZE(1) = 1 << 12 = 4K
+		 * KVM_HPAGE_SIZE(2) = 1 << 21 = 2M
+		 * KVM_HPAGE_SIZE(3) = 1 << 30 = 1G
+		 *
+		 * 假设上面返回的page_size是4K
+		 * 下面函数最终的返回值就是1
+		 *
+		 * 假设上面返回的page_size是2M
+		 * 下面函数最终的返回值就是2
+		 *
+		 * 假设上面返回的page_size是1G
+		 * 下面函数最终的返回值就是3
+		 */
 		if (page_size >= KVM_HPAGE_SIZE(i))
 			ret = i;
 		else
@@ -1220,30 +2600,81 @@ static int host_mapping_level(struct kvm *kvm, gfn_t gfn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2518| <<gfn_to_memslot_dirty_bitmap>> if (!memslot_valid_for_gpte(slot, no_dirty_log))
+ *   - arch/x86/kvm/mmu.c|2534| <<mapping_level>> *force_pt_level = !memslot_valid_for_gpte(slot, true);
+ *
+ * 以下情况返回false:
+ *   - (!slot || slot->flags & KVM_MEMSLOT_INVALID)
+ *   - (no_dirty_log && slot->dirty_bitmap)
+ */
 static inline bool memslot_valid_for_gpte(struct kvm_memory_slot *slot,
 					  bool no_dirty_log)
 {
+	/*
+	 * x86下使用KVM_MEMSLOT_INVALID的例子:
+	 *   - arch/x86/kvm/mmu.c|1868| <<memslot_valid_for_gpte>> if (!slot || slot->flags & KVM_MEMSLOT_INVALID)
+	 *   - virt/kvm/kvm_main.c|1048| <<__kvm_set_memory_region>> slot->flags |= KVM_MEMSLOT_INVALID;
+	 *   - virt/kvm/kvm_main.c|1349| <<kvm_is_visible_gfn>> memslot->flags & KVM_MEMSLOT_INVALID)
+	 *   - virt/kvm/kvm_main.c|1388| <<__gfn_to_hva_many>> if (!slot || slot->flags & KVM_MEMSLOT_INVALID)
+	 */
 	if (!slot || slot->flags & KVM_MEMSLOT_INVALID)
 		return false;
+	/*
+	 * 一个slot有许多客户机虚拟页面组成,通过dirty_bitmap
+	 * 标记每一个页是否可用,一个页面对应一个位
+	 *
+	 * 比如在如下分配:
+	 *   - virt/kvm/kvm_main.c|804| <<kvm_create_dirty_bitmap>> memslot->dirty_bitmap = kvzalloc(dirty_bytes, GFP_KERNEL_ACCOUNT);
+	 */
 	if (no_dirty_log && slot->dirty_bitmap)
 		return false;
 
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4414| <<pte_prefetch_gfn_to_pfn>> slot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, no_dirty_log);
+ *   - arch/x86/kvm/mmu.c|4432| <<direct_pte_prefetch_many>> slot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, access & ACC_WRITE_MASK);
+ *
+ * 根据gfn得到slot (struct kvm_memory_slot)
+ * 如果slot满足以下情况把返回NULL:
+ *   - (!slot || slot->flags & KVM_MEMSLOT_INVALID)
+ *   - (no_dirty_log && slot->dirty_bitmap)
+ * 否则返回slot
+ */
 static struct kvm_memory_slot *
 gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu, gfn_t gfn,
 			    bool no_dirty_log)
 {
 	struct kvm_memory_slot *slot;
 
+	/*
+	 * truct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+	 * struct kvm_memslots中有:
+	 *       struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+	 *       short id_to_index[KVM_MEM_SLOTS_NUM];
+	 */
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+	/*
+	 * 以下情况返回false:
+	 *   - (!slot || slot->flags & KVM_MEMSLOT_INVALID)
+	 *   - (no_dirty_log && slot->dirty_bitmap)
+	 */
 	if (!memslot_valid_for_gpte(slot, no_dirty_log))
 		slot = NULL;
 
 	return slot;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4813| <<nonpaging_map>> level = mapping_level(vcpu, gfn, &force_pt_level);
+ *   - arch/x86/kvm/mmu.c|5448| <<tdp_page_fault>> level = mapping_level(vcpu, gfn, &force_pt_level);
+ *   - arch/x86/kvm/paging_tmpl.h|805| <<FNAME(page_fault)>> level = mapping_level(vcpu, walker.gfn, &force_pt_level);
+ */
 static int mapping_level(struct kvm_vcpu *vcpu, gfn_t large_gfn,
 			 bool *force_pt_level)
 {
@@ -1253,19 +2684,68 @@ static int mapping_level(struct kvm_vcpu *vcpu, gfn_t large_gfn,
 	if (unlikely(*force_pt_level))
 		return PT_PAGE_TABLE_LEVEL;
 
+	/*
+	 * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+	 * struct kvm_memslots中有:
+	 *       struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+	 *       short id_to_index[KVM_MEM_SLOTS_NUM];
+	 */
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, large_gfn);
+	/*
+	 * 以下情况返回false:
+	 *   - (!slot || slot->flags & KVM_MEMSLOT_INVALID)
+	 *   - (no_dirty_log && slot->dirty_bitmap)
+	 */
 	*force_pt_level = !memslot_valid_for_gpte(slot, true);
 	if (unlikely(*force_pt_level))
 		return PT_PAGE_TABLE_LEVEL;
 
+	/*
+	 * 根据gfn获取其对于hva在host中的page size (4K,2M还是1G)
+	 *     根据gfn获取在qemu中对应的hva, 把hva转化为vm_area_struct
+	 *     然后就知道vm_area_struct的page size了
+	 * 然后根据这个page size获取其在host上的mapping level
+	 *
+	 * 如果gfn对应的hva在host上的page size是4K
+	 *   host_mapping_level()返回1
+	 * 如果gfn对应的hva在host上的page size是2M
+	 *   host_mapping_level()返回2
+	 * 如果gfn对应的hva在host上的page size是1G
+	 *   host_mapping_level()返回3
+	 */
 	host_level = host_mapping_level(vcpu->kvm, large_gfn);
 
+	/*
+	 * 如果hva在host上是4K的粒度,直接返回PT_PAGE_TABLE_LEVEL就可以了
+	 */
 	if (host_level == PT_PAGE_TABLE_LEVEL)
 		return host_level;
 
+	/*
+	 * vmx_get_lpage_level()
+	 * svm_get_lpage_level()
+	 *
+	 * vmx_get_lpage_level()根据配置返回2或者3, large page不是2M就是1G
+	 *
+	 * 注意, 这里是min()!!!!!!
+	 */
 	max_level = min(kvm_x86_ops->get_lpage_level(), host_level);
 
 	for (level = PT_DIRECTORY_LEVEL; level <= max_level; ++level)
+		/*
+		 * 在以下修改disallow_lpage:
+		 *   - arch/x86/kvm/mmu.c|1154| <<update_gfn_disallow_lpage_count>> linfo->disallow_lpage += count;
+		 *   - arch/x86/kvm/x86.c|9528| <<kvm_arch_create_memslot>> linfo[0].disallow_lpage = 1;
+		 *   - arch/x86/kvm/x86.c|9530| <<kvm_arch_create_memslot>> linfo[lpages - 1].disallow_lpage = 1;
+		 *   - arch/x86/kvm/x86.c|9542| <<kvm_arch_create_memslot>> linfo[j].disallow_lpage = 1;
+		 *
+		 * 在以下使用disallow_lpage:
+		 *   - arch/x86/kvm/mmu.c|2112| <<update_gfn_disallow_lpage_count>> WARN_ON(linfo->disallow_lpage < 0);
+		 *   - arch/x86/kvm/mmu.c|2177| <<__mmu_gfn_lpage_is_disallowed>> return !!linfo->disallow_lpage;
+		 *
+		 * 如果(slot == NULL)直接返回true
+		 * 否则根据gfn返回对应的第二维的struct kvm_lpage_info: slot->arch.lpage_info[level - 2][idx]的disallow_lpage作为true/false
+		 */
 		if (__mmu_gfn_lpage_is_disallowed(large_gfn, level, slot))
 			break;
 
@@ -1283,20 +2763,40 @@ static int mapping_level(struct kvm_vcpu *vcpu, gfn_t large_gfn,
 /*
  * Returns the number of pointers in the rmap chain, not counting the new one.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2803| <<rmap_add>> return pte_list_add(vcpu, spte, rmap_head);
+ *   - arch/x86/kvm/mmu.c|3455| <<mmu_page_add_parent_pte>> pte_list_add(vcpu, parent_pte, &sp->parent_ptes);
+ */
 static int pte_list_add(struct kvm_vcpu *vcpu, u64 *spte,
 			struct kvm_rmap_head *rmap_head)
 {
 	struct pte_list_desc *desc;
 	int i, count = 0;
 
+	/*
+	 * About rmap_head encoding:
+	 *
+	 * If the bit zero of rmap_head->val is clear, then it points to the only spte
+	 * in this rmap chain. Otherwise, (rmap_head->val & ~1) points to a struct
+	 * pte_list_desc containing more mappings
+	 */
 	if (!rmap_head->val) {
 		rmap_printk("pte_list_add: %p %llx 0->1\n", spte, *spte);
 		rmap_head->val = (unsigned long)spte;
 	} else if (!(rmap_head->val & 1)) {
 		rmap_printk("pte_list_add: %p %llx 1->many\n", spte, *spte);
+		/*
+		 * 分配一个pte_list_desc
+		 *     u64 *sptes[PTE_LIST_EXT];
+		 *     struct pte_list_desc *more;
+		 */
 		desc = mmu_alloc_pte_list_desc(vcpu);
 		desc->sptes[0] = (u64 *)rmap_head->val;
 		desc->sptes[1] = spte;
+		/*
+		 * 最后一位设置成1
+		 */
 		rmap_head->val = (unsigned long)desc | 1;
 		++count;
 	} else {
@@ -1317,6 +2817,10 @@ static int pte_list_add(struct kvm_vcpu *vcpu, u64 *spte,
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2748| <<__pte_list_remove>> pte_list_desc_remove_entry(rmap_head,
+ */
 static void
 pte_list_desc_remove_entry(struct kvm_rmap_head *rmap_head,
 			   struct pte_list_desc *desc, int i,
@@ -1340,6 +2844,12 @@ pte_list_desc_remove_entry(struct kvm_rmap_head *rmap_head,
 	mmu_free_pte_list_desc(desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2764| <<pte_list_remove>> __pte_list_remove(sptep, rmap_head);
+ *   - arch/x86/kvm/mmu.c|2815| <<rmap_remove>> __pte_list_remove(spte, rmap_head);
+ *   - arch/x86/kvm/mmu.c|3461| <<mmu_page_remove_parent_pte>> __pte_list_remove(parent_pte, &sp->parent_ptes);
+ */
 static void __pte_list_remove(u64 *spte, struct kvm_rmap_head *rmap_head)
 {
 	struct pte_list_desc *desc;
@@ -1376,21 +2886,77 @@ static void __pte_list_remove(u64 *spte, struct kvm_rmap_head *rmap_head)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3155| <<kvm_zap_rmapp>> pte_list_remove(rmap_head, sptep);
+ *   - arch/x86/kvm/mmu.c|3191| <<kvm_set_pte_rmapp>> pte_list_remove(rmap_head, sptep);
+ *   - arch/x86/kvm/mmu.c|7381| <<kvm_mmu_zap_collapsible_spte>> pte_list_remove(rmap_head, sptep);
+ */
 static void pte_list_remove(struct kvm_rmap_head *rmap_head, u64 *sptep)
 {
+	/*
+	 * 根据是否有volatile bits用WRITE_ONCE(*sptep, spte)或者xchg(sptep, spte)更新spte为0!
+	 * 如果之前如果spte是0或者是mmio, 则退出返回0
+	 * 否则根据情况把pfn在host的struct page设置accessed或者dirty返回1
+	 */
 	mmu_spte_clear_track_bits(sptep);
 	__pte_list_remove(sptep, rmap_head);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2805| <<gfn_to_rmap>> return __gfn_to_rmap(gfn, sp->role.level, slot);
+ *   - arch/x86/kvm/mmu.c|3088| <<kvm_mmu_write_protect_pt_masked>> rmap_head = __gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
+ *   - arch/x86/kvm/mmu.c|3114| <<kvm_mmu_clear_dirty_pt_masked>> rmap_head = __gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
+ *   - arch/x86/kvm/mmu.c|3168| <<kvm_mmu_slot_gfn_write_protect>> rmap_head = __gfn_to_rmap(gfn, i, slot);
+ *   - arch/x86/kvm/mmu.c|3274| <<rmap_walk_init_level>> iterator->rmap = __gfn_to_rmap(iterator->gfn, level, iterator->slot);
+ *   - arch/x86/kvm/mmu.c|3275| <<rmap_walk_init_level>> iterator->end_rmap = __gfn_to_rmap(iterator->end_gfn, level,
+ *   - arch/x86/kvm/mmu_audit.c|150| <<inspect_spte_has_rmap>> rmap_head = __gfn_to_rmap(gfn, rev_sp->role.level, slot);
+ *   - arch/x86/kvm/mmu_audit.c|203| <<audit_write_protection>> rmap_head = __gfn_to_rmap(sp->gfn, PT_PAGE_TABLE_LEVEL, slot);
+ *
+ * 根据gfn和level找到其在slot(kvm_memory_slot)中的index
+ * 然后返回对应的kvm_rmap_head: slot->arch.rmap[level - PT_PAGE_TABLE_LEVEL][idx]
+ */
 static struct kvm_rmap_head *__gfn_to_rmap(gfn_t gfn, int level,
 					   struct kvm_memory_slot *slot)
 {
 	unsigned long idx;
 
+	/*
+	 * base_gfn是基于4k开始的gfn
+	 * gfn是基于4k结束的gfn
+	 * 计算从开始到结束需要用到几个hugepage (或者普通page)
+	 *   level是1的时候hugepage大小是4K
+	 *   level是2的时候hugepage大小是2M
+	 *   level是3的时候hugepage大小是1G
+	 */
 	idx = gfn_to_index(gfn, slot->base_gfn, level);
+	/*
+	 * About rmap_head encoding:
+	 *
+	 * If the bit zero of rmap_head->val is clear, then it points to the only spte
+	 * in this rmap chain. Otherwise, (rmap_head->val & ~1) points to a struct
+	 * pte_list_desc containing more mappings.
+	 *
+	 * rmap和lpage_info的第二维都是在kvm_arch_create_memslot()分配
+	 * 数量是对应level的page的数量
+	 * level越大(大页)数量越少
+	 *
+	 * struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
+	 */
 	return &slot->arch.rmap[level - PT_PAGE_TABLE_LEVEL][idx];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2823| <<rmap_add>> rmap_head = gfn_to_rmap(vcpu->kvm, gfn, sp);
+ *   - arch/x86/kvm/mmu.c|2835| <<rmap_remove>> rmap_head = gfn_to_rmap(kvm, gfn, sp);
+ *   - arch/x86/kvm/mmu.c|3425| <<rmap_recycle>> rmap_head = gfn_to_rmap(vcpu->kvm, gfn, sp);
+ *
+ * 根据gfn找到slot (struct kvm_memory_slot)
+ * 然后根据gfn和sp->role.level找到其在slot(kvm_memory_slot)中的index
+ * 然后返回对应的kvm_rmap_head: slot->arch.rmap[level - PT_PAGE_TABLE_LEVEL][idx]
+ */
 static struct kvm_rmap_head *gfn_to_rmap(struct kvm *kvm, gfn_t gfn,
 					 struct kvm_mmu_page *sp)
 {
@@ -1399,9 +2965,20 @@ static struct kvm_rmap_head *gfn_to_rmap(struct kvm *kvm, gfn_t gfn,
 
 	slots = kvm_memslots_for_spte_role(kvm, sp->role);
 	slot = __gfn_to_memslot(slots, gfn);
+	/*
+	 * 根据gfn和level找到其在slot(kvm_memory_slot)中的index
+	 * 然后返回对应的kvm_rmap_head: slot->arch.rmap[level - PT_PAGE_TABLE_LEVEL][idx]
+	 */
 	return __gfn_to_rmap(gfn, sp->role.level, slot);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|6838| <<kvm_mmu_pte_write>> & mmu_base_role_mask.word) && rmap_can_add(vcpu))
+ *   - arch/x86/kvm/paging_tmpl.h|915| <<FNAME(invlpg)>> if (!rmap_can_add(vcpu))
+ *
+ * 检查是否还能分配pte_list_desc
+ */
 static bool rmap_can_add(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu_memory_cache *cache;
@@ -1410,25 +2987,81 @@ static bool rmap_can_add(struct kvm_vcpu *vcpu)
 	return mmu_memory_cache_free_objects(cache);
 }
 
+/*
+ * 在KVM中,逆向映射机制的作用是类似的,但是完成的却
+ * 不是从HPA到对应的EPT页表项的定位,而是从gfn到对应的
+ * 页表项的定位.
+ *
+ * 理论上讲根据gfn一步步遍历EPT也未尝不可,但是效率较低;
+ * 况且在EPT所维护的页面不同于host的页表,理论上讲是虚
+ * 拟机之间是禁止主动的共享内存的,为了提高效率,就有了
+ * 当前的逆向映射机制.
+ *
+ * 问题:
+ * 从gfn找到页表项最多只要查询访问4次(4级页表),为啥还要用反向映射,
+ * 况且建立rmap还得有一定内存开销,并没有提高太多效率吧?
+ */
+
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4542| <<mmu_set_spte>> rmap_count = rmap_add(vcpu, sptep, gfn);
+ *
+ * 根据gfn找到kvm_rmap_head
+ * 把一个spte加入到kvm_rmap_head
+ */
 static int rmap_add(struct kvm_vcpu *vcpu, u64 *spte, gfn_t gfn)
 {
 	struct kvm_mmu_page *sp;
 	struct kvm_rmap_head *rmap_head;
 
+	/*
+	 * page_header():
+	 * __pa(spte)可能是一个指向某个pte的地址 (__pa(spte)保存的这个pte的地址而不是内容)
+	 * 获得包含这个地址的页表页对应的kvm_mmu_page
+	 *
+	 * 根据spte找到kvm_mmu_page
+	 */
 	sp = page_header(__pa(spte));
+	/*
+	 * irect==true什么也不做
+	 */
 	kvm_mmu_page_set_gfn(sp, spte - sp->spt, gfn);
+	/*
+	 * 根据gfn找到slot (struct kvm_memory_slot)
+	 * 然后根据gfn和sp->role.level找到其在slot(kvm_memory_slot)中的index
+	 * 然后返回对应的kvm_rmap_head: slot->arch.rmap[level - PT_PAGE_TABLE_LEVEL][idx]
+	 */
 	rmap_head = gfn_to_rmap(vcpu->kvm, gfn, sp);
+	/* 把一个spte加入到kvm_rmap_head */
 	return pte_list_add(vcpu, spte, rmap_head);
 }
 
+/*
+ * called only by:
+ *   - arch/x86/kvm/mmu.c|2934| <<drop_spte>> rmap_remove(kvm, sptep);
+ */
 static void rmap_remove(struct kvm *kvm, u64 *spte)
 {
 	struct kvm_mmu_page *sp;
 	gfn_t gfn;
 	struct kvm_rmap_head *rmap_head;
 
+	/*
+	 * shadow_page可能是一个指向某个pte的地址 (shadow_page保存的这个pte的地址而不是内容)
+	 * 获得包含这个地址的页表页对应的kvm_mmu_page
+	 */
 	sp = page_header(__pa(spte));
+	/*
+	 * 因为可能是大页, 根据index返回对应的gfn (每个entry表示一个gfn)
+	 * 每个页表页(MMU page)对应一个数据结构kvm_mmu_page
+	 * 这里是页表页, 也就是说kvm_mmu_page表示的page里全是entry!!!
+	 */
 	gfn = kvm_mmu_page_get_gfn(sp, spte - sp->spt);
+	/*
+	 * 根据gfn找到slot (struct kvm_memory_slot)
+	 * 然后根据gfn和sp->role.level找到其在slot(kvm_memory_slot)中的index
+	 * 然后返回对应的kvm_rmap_head: slot->arch.rmap[level - PT_PAGE_TABLE_LEVEL][idx]
+	 */
 	rmap_head = gfn_to_rmap(kvm, gfn, sp);
 	__pte_list_remove(spte, rmap_head);
 }
@@ -1450,6 +3083,12 @@ struct rmap_iterator {
  *
  * Returns sptep if found, NULL otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2928| <<for_each_rmap_spte>> for (_spte_ = rmap_get_first(_rmap_head_, _iter_); \
+ *   - arch/x86/kvm/mmu.c|3189| <<kvm_zap_rmapp>> while ((sptep = rmap_get_first(rmap_head, &iter))) {
+ *   - arch/x86/kvm/mmu.c|4134| <<kvm_mmu_unlink_parents>> while ((sptep = rmap_get_first(&sp->parent_ptes, &iter)))
+ */
 static u64 *rmap_get_first(struct kvm_rmap_head *rmap_head,
 			   struct rmap_iterator *iter)
 {
@@ -1477,6 +3116,10 @@ static u64 *rmap_get_first(struct kvm_rmap_head *rmap_head,
  *
  * Returns sptep if found, NULL otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2892| <<for_each_rmap_spte>> _spte_; _spte_ = rmap_get_next(_iter_))
+ */
 static u64 *rmap_get_next(struct rmap_iterator *iter)
 {
 	u64 *sptep;
@@ -1505,6 +3148,20 @@ static u64 *rmap_get_next(struct rmap_iterator *iter)
 	return sptep;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|2962| <<__rmap_write_protect>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu.c|3001| <<__rmap_clear_dirty>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu.c|3027| <<__rmap_set_dirty>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu.c|3184| <<kvm_set_pte_rmapp>> for_each_rmap_spte(rmap_head, &iter, sptep) {
+ *   - arch/x86/kvm/mmu.c|3359| <<kvm_age_rmapp>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu.c|3373| <<kvm_test_age_rmapp>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu.c|3501| <<kvm_mmu_mark_parents_unsync>> for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
+ *   - arch/x86/kvm/mmu.c|7367| <<kvm_mmu_zap_collapsible_spte>> for_each_rmap_spte(rmap_head, &iter, sptep) {
+ *   - arch/x86/kvm/mmu_audit.c|205| <<audit_write_protection>> for_each_rmap_spte(rmap_head, &iter, sptep) {
+ *
+ * 通过struct rmap_iterator遍历kvm_rmap_head中的每一个pte
+ */
 #define for_each_rmap_spte(_rmap_head_, _iter_, _spte_)			\
 	for (_spte_ = rmap_get_first(_rmap_head_, _iter_);		\
 	     _spte_; _spte_ = rmap_get_next(_iter_))
@@ -1552,6 +3209,10 @@ static void drop_large_spte(struct kvm_vcpu *vcpu, u64 *sptep)
  *
  * Return true if tlb need be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3182| <<__rmap_write_protect>> flush |= spte_write_protect(sptep, pt_protect);
+ */
 static bool spte_write_protect(u64 *sptep, bool pt_protect)
 {
 	u64 spte = *sptep;
@@ -1659,6 +3320,12 @@ static bool __rmap_set_dirty(struct kvm *kvm, struct kvm_rmap_head *rmap_head)
  * Used when we do not need to care about huge page mappings: e.g. during dirty
  * logging we do not have any such mappings.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3142| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);
+ *   - virt/kvm/arm/mmu.c|1559| <<kvm_mmu_write_protect_pt_masked>> static void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
+ *   - virt/kvm/arm/mmu.c|1581| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);
+ */
 static void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
 				     struct kvm_memory_slot *slot,
 				     gfn_t gfn_offset, unsigned long mask)
@@ -1761,6 +3428,11 @@ static bool rmap_write_protect(struct kvm_vcpu *vcpu, u64 gfn)
 	return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3391| <<kvm_unmap_rmapp>> return kvm_zap_rmapp(kvm, rmap_head);
+ *   - arch/x86/kvm/mmu.c|7722| <<kvm_zap_gfn_range>> slot_handle_level_range(kvm, memslot, kvm_zap_rmapp,
+ */
 static bool kvm_zap_rmapp(struct kvm *kvm, struct kvm_rmap_head *rmap_head)
 {
 	u64 *sptep;
@@ -1784,6 +3456,10 @@ static int kvm_unmap_rmapp(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 	return kvm_zap_rmapp(kvm, rmap_head);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3385| <<kvm_set_spte_hva>> return kvm_handle_hva(kvm, hva, (unsigned long )&pte, kvm_set_pte_rmapp);
+ */
 static int kvm_set_pte_rmapp(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 			     struct kvm_memory_slot *slot, gfn_t gfn, int level,
 			     unsigned long data)
@@ -1891,6 +3567,11 @@ static void slot_rmap_walk_next(struct slot_rmap_walk_iterator *iterator)
 	rmap_walk_init_level(iterator, iterator->level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3471| <<kvm_handle_hva_range>> for_each_slot_rmap_range(memslot, PT_PAGE_TABLE_LEVEL,
+ *   - arch/x86/kvm/mmu.c|7226| <<slot_handle_level_range>> for_each_slot_rmap_range(memslot, start_level, end_level, start_gfn,
+ */
 #define for_each_slot_rmap_range(_slot_, _start_level_, _end_level_,	\
 	   _start_gfn, _end_gfn, _iter_)				\
 	for (slot_rmap_walk_init(_iter_, _slot_, _start_level_,		\
@@ -1898,6 +3579,12 @@ static void slot_rmap_walk_next(struct slot_rmap_walk_iterator *iterator)
 	     slot_rmap_walk_okay(_iter_);				\
 	     slot_rmap_walk_next(_iter_))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3491| <<kvm_handle_hva>> return kvm_handle_hva_range(kvm, hva, hva + 1, data, handler);
+ *   - arch/x86/kvm/mmu.c|3496| <<kvm_unmap_hva_range>> return kvm_handle_hva_range(kvm, start, end, 0, kvm_unmap_rmapp);
+ *   - arch/x86/kvm/mmu.c|3550| <<kvm_age_hva>> return kvm_handle_hva_range(kvm, start, end, 0, kvm_age_rmapp);
+ */
 static int kvm_handle_hva_range(struct kvm *kvm,
 				unsigned long start,
 				unsigned long end,
@@ -1945,6 +3632,11 @@ static int kvm_handle_hva_range(struct kvm *kvm,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3512| <<kvm_set_spte_hva>> return kvm_handle_hva(kvm, hva, (unsigned long )&pte, kvm_set_pte_rmapp);
+ *   - arch/x86/kvm/mmu.c|3566| <<kvm_test_age_hva>> return kvm_handle_hva(kvm, hva, 0, kvm_test_age_rmapp);
+ */
 static int kvm_handle_hva(struct kvm *kvm, unsigned long hva,
 			  unsigned long data,
 			  int (*handler)(struct kvm *kvm,
@@ -1996,6 +3688,10 @@ static int kvm_test_age_rmapp(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 
 #define RMAP_RECYCLE_THRESHOLD 1000
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4716| <<mmu_set_spte>> rmap_recycle(vcpu, sptep, gfn);
+ */
 static void rmap_recycle(struct kvm_vcpu *vcpu, u64 *spte, gfn_t gfn)
 {
 	struct kvm_rmap_head *rmap_head;
@@ -2010,17 +3706,30 @@ static void rmap_recycle(struct kvm_vcpu *vcpu, u64 *spte, gfn_t gfn)
 			KVM_PAGES_PER_HPAGE(sp->role.level));
 }
 
+/*
+ * x86下调用:
+ *   - virt/kvm/kvm_main.c|462| <<kvm_mmu_notifier_clear_flush_young>> young = kvm_age_hva(kvm, start, end);
+ *   - virt/kvm/kvm_main.c|498| <<kvm_mmu_notifier_clear_young>> young = kvm_age_hva(kvm, start, end);
+ */
 int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end)
 {
 	return kvm_handle_hva_range(kvm, start, end, 0, kvm_age_rmapp);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|517| <<kvm_mmu_notifier_test_young>> young = kvm_test_age_hva(kvm, address);
+ */
 int kvm_test_age_hva(struct kvm *kvm, unsigned long hva)
 {
 	return kvm_handle_hva(kvm, hva, 0, kvm_test_age_rmapp);
 }
 
 #ifdef MMU_DEBUG
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3588| <<kvm_mmu_free_page>> MMU_WARN_ON(!is_empty_shadow_page(sp->spt));
+ */
 static int is_empty_shadow_page(u64 *spt)
 {
 	u64 *pos;
@@ -2042,12 +3751,21 @@ static int is_empty_shadow_page(u64 *spt)
  * aggregate version in order to make the slab shrinker
  * faster
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3644| <<kvm_mmu_alloc_page>> kvm_mod_used_mmu_pages(vcpu->kvm, +1);
+ *   - arch/x86/kvm/mmu.c|4303| <<__kvm_mmu_prepare_zap_page>> kvm_mod_used_mmu_pages(kvm, -1);
+ */
 static inline void kvm_mod_used_mmu_pages(struct kvm *kvm, unsigned long nr)
 {
 	kvm->arch.n_used_mmu_pages += nr;
 	percpu_counter_add(&kvm_total_used_mmu_pages, nr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4345| <<kvm_mmu_commit_zap_page>> kvm_mmu_free_page(sp);
+ */
 static void kvm_mmu_free_page(struct kvm_mmu_page *sp)
 {
 	MMU_WARN_ON(!is_empty_shadow_page(sp->spt));
@@ -2059,11 +3777,20 @@ static void kvm_mmu_free_page(struct kvm_mmu_page *sp)
 	kmem_cache_free(mmu_page_header_cache, sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3791| <<for_each_valid_sp>> &(_kvm)->arch.mmu_page_hash[kvm_page_table_hashfn(_gfn)], hash_link) \
+ *   - arch/x86/kvm/mmu.c|4077| <<kvm_mmu_get_page>> &vcpu->kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)]);
+ */
 static unsigned kvm_page_table_hashfn(gfn_t gfn)
 {
 	return hash_64(gfn, KVM_MMU_HASH_SHIFT);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4183| <<link_shadow_page>> mmu_page_add_parent_pte(vcpu, sp, sptep);
+ */
 static void mmu_page_add_parent_pte(struct kvm_vcpu *vcpu,
 				    struct kvm_mmu_page *sp, u64 *parent_pte)
 {
@@ -2073,12 +3800,23 @@ static void mmu_page_add_parent_pte(struct kvm_vcpu *vcpu,
 	pte_list_add(vcpu, parent_pte, &sp->parent_ptes);
 }
 
+/*
+ * called by only:
+ *   - arch/x86/kvm/mmu.c|3620| <<drop_parent_pte>> mmu_page_remove_parent_pte(sp, parent_pte);
+ */
 static void mmu_page_remove_parent_pte(struct kvm_mmu_page *sp,
 				       u64 *parent_pte)
 {
 	__pte_list_remove(parent_pte, &sp->parent_ptes);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4206| <<validate_direct_spte>> drop_parent_pte(child, sptep);
+ *   - arch/x86/kvm/mmu.c|4225| <<mmu_page_zap_pte>> drop_parent_pte(child, spte);
+ *   - arch/x86/kvm/mmu.c|4251| <<kvm_mmu_unlink_parents>> drop_parent_pte(sp, sptep);
+ *   - arch/x86/kvm/mmu.c|4637| <<mmu_set_spte>> drop_parent_pte(child, sptep);
+ */
 static void drop_parent_pte(struct kvm_mmu_page *sp,
 			    u64 *parent_pte)
 {
@@ -2086,6 +3824,10 @@ static void drop_parent_pte(struct kvm_mmu_page *sp,
 	mmu_spte_clear_no_track(parent_pte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3458| <<kvm_mmu_get_page>> sp = kvm_mmu_alloc_page(vcpu, direct);
+ */
 static struct kvm_mmu_page *kvm_mmu_alloc_page(struct kvm_vcpu *vcpu, int direct)
 {
 	struct kvm_mmu_page *sp;
@@ -2320,6 +4062,10 @@ static bool kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 }
 
 /* @gfn should be write-protected at the call site */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|3647| <<kvm_mmu_get_page>> flush |= kvm_sync_pages(vcpu, gfn, &invalid_list);
+ */
 static bool kvm_sync_pages(struct kvm_vcpu *vcpu, gfn_t gfn,
 			   struct list_head *invalid_list)
 {
@@ -2456,6 +4202,16 @@ static void clear_sp_write_flooding_count(u64 *spte)
 	__clear_sp_write_flooding_count(sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4328| <<__direct_map>> sp = kvm_mmu_get_page(vcpu, base_gfn, it.addr,
+ *   - arch/x86/kvm/mmu.c|4756| <<mmu_alloc_direct_roots>> sp = kvm_mmu_get_page(vcpu, 0, 0,
+ *   - arch/x86/kvm/mmu.c|4771| <<mmu_alloc_direct_roots>> sp = kvm_mmu_get_page(vcpu, i << (30 - PAGE_SHIFT),
+ *   - arch/x86/kvm/mmu.c|4813| <<mmu_alloc_shadow_roots>> sp = kvm_mmu_get_page(vcpu, root_gfn, 0,
+ *   - arch/x86/kvm/mmu.c|4850| <<mmu_alloc_shadow_roots>> sp = kvm_mmu_get_page(vcpu, root_gfn, i << 30, PT32_ROOT_LEVEL,
+ *   - arch/x86/kvm/paging_tmpl.h|653| <<FNAME(fetch)>> sp = kvm_mmu_get_page(vcpu, table_gfn, addr, it.level-1,
+ *   - arch/x86/kvm/paging_tmpl.h|683| <<FNAME(fetch)>> sp = kvm_mmu_get_page(vcpu, base_gfn, addr,
+ */
 static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 					     gfn_t gfn,
 					     gva_t gaddr,
@@ -3045,6 +4801,13 @@ static int set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|4684| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, start, access, 0, sp->role.level, gfn,
+ *   - arch/x86/kvm/mmu.c|4763| <<__direct_map>> ret = mmu_set_spte(vcpu, it.sptep, ACC_ALL,
+ *   - arch/x86/kvm/paging_tmpl.h|540| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, spte, pte_access, 0, PT_PAGE_TABLE_LEVEL, gfn, pfn,
+ *   - arch/x86/kvm/paging_tmpl.h|689| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, it.sptep, gw->pte_access, write_fault,
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep, unsigned pte_access,
 			int write_fault, int level, gfn_t gfn, kvm_pfn_t pfn,
 		       	bool speculative, bool host_writable)
@@ -3058,6 +4821,11 @@ static int mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep, unsigned pte_access,
 	pgprintk("%s: spte %llx write_fault %d gfn %llx\n", __func__,
 		 *sptep, write_fault, gfn);
 
+	/*
+	 * 如果pte不为0并且pte的最后3位不是110并且62位(SPTE_SPECIAL_MASK)也没设置, 返回true
+	 * 如果spte是0x0,或者spte的最后3位是110(62位也设置了), 函数返回false
+	 * 或者说, 如果spte是0或者是mmio, is_shadow_present_pte()返回false
+	 */
 	if (is_shadow_present_pte(*sptep)) {
 		/*
 		 * If we overwrite a PTE page pointer with a 2MB PMD, unlink
@@ -3194,6 +4962,11 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 	__direct_pte_prefetch(vcpu, sp, sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5089| <<nonpaging_map>> r = __direct_map(vcpu, v, write, map_writable, level, pfn, prefault);
+ *   - arch/x86/kvm/mmu.c|5719| <<tdp_page_fault>> r = __direct_map(vcpu, gpa, write, map_writable, level, pfn, prefault);
+ */
 static int __direct_map(struct kvm_vcpu *vcpu, gpa_t gpa, int write,
 			int map_writable, int level, kvm_pfn_t pfn,
 			bool prefault)
@@ -3208,12 +4981,20 @@ static int __direct_map(struct kvm_vcpu *vcpu, gpa_t gpa, int write,
 		return RET_PF_RETRY;
 
 	trace_kvm_mmu_spte_requested(gpa, level, pfn);
+	/*
+	 * 对于一个gpa的每一级(level)的pte
+	 */
 	for_each_shadow_entry(vcpu, gpa, it) {
 		base_gfn = gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
 		if (it.level == level)
 			break;
 
 		drop_large_spte(vcpu, it.sptep);
+		/*
+		 * 如果pte不为0并且pte的最后3位不是110并且62位(SPTE_SPECIAL_MASK)也没设置, 返回true
+		 * 如果spte是0x0,或者spte的最后3位是110(62位也设置了), 函数返回false
+		 * 或者说, 如果spte是0或者是mmio, is_shadow_present_pte()返回false
+		 */
 		if (!is_shadow_present_pte(*it.sptep)) {
 			sp = kvm_mmu_get_page(vcpu, base_gfn, it.addr,
 					      it.level - 1, true, ACC_ALL);
@@ -3307,6 +5088,10 @@ static bool handle_abnormal_pfn(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5060| <<fast_page_fault>> if (!page_fault_can_be_fast(error_code))
+ */
 static bool page_fault_can_be_fast(u32 error_code)
 {
 	/*
@@ -3344,6 +5129,10 @@ static bool page_fault_can_be_fast(u32 error_code)
  * Returns true if the SPTE was fixed successfully. Otherwise,
  * someone else modified the SPTE from its original value.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5132| <<fast_page_fault>> fault_handled = fast_pf_fix_direct_spte(vcpu, sp,
+ */
 static bool
 fast_pf_fix_direct_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 			u64 *sptep, u64 old_spte, u64 new_spte)
@@ -3396,6 +5185,11 @@ static bool is_access_allowed(u32 fault_err_code, u64 spte)
  * - true: let the vcpu to access on the same address again.
  * - false: let the real page fault path to fix it.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5166| <<nonpaging_map>> if (fast_page_fault(vcpu, v, level, error_code))
+ *   - arch/x86/kvm/mmu.c|5796| <<tdp_page_fault>> if (fast_page_fault(vcpu, gpa, level, error_code))
+ */
 static bool fast_page_fault(struct kvm_vcpu *vcpu, gva_t gva, int level,
 			    u32 error_code)
 {
@@ -3411,11 +5205,17 @@ static bool fast_page_fault(struct kvm_vcpu *vcpu, gva_t gva, int level,
 	if (!page_fault_can_be_fast(error_code))
 		return false;
 
+	/*
+	 * 关闭中断, 把READING_SHADOW_PAGE_TABLES存入vcpu->mode
+	 */
 	walk_shadow_page_lockless_begin(vcpu);
 
 	do {
 		u64 new_spte;
 
+		/*
+		 * 对于一个gpa的每一级(level)的pte
+		 */
 		for_each_shadow_entry_lockless(vcpu, gva, iterator, spte)
 			if (!is_shadow_present_pte(spte) ||
 			    iterator.level < level)
@@ -3442,6 +5242,17 @@ static bool fast_page_fault(struct kvm_vcpu *vcpu, gva_t gva, int level,
 
 		new_spte = spte;
 
+		/*
+		 * is_access_track_spte():
+		 * 如果spte的SPTE_SPECIAL_MASK=1<<62设置了(说明ad不支持),
+		 * 并且spte在shadow_acc_track_mask中的bit都没设置
+		 * 则返回true (如果返回true说明ad bit不支持)
+		 *
+		 * restore_acc_track_spte():
+		 * Restore an acc-track PTE back to a regular PTE
+		 * 把52位之后临时保存的的r+x清空
+		 * 把52位之后临时保存的的r+x设置回来
+		 */
 		if (is_access_track_spte(spte))
 			new_spte = restore_acc_track_spte(new_spte);
 
@@ -3496,6 +5307,7 @@ static bool fast_page_fault(struct kvm_vcpu *vcpu, gva_t gva, int level,
 
 	trace_fast_page_fault(vcpu, gva, error_code, iterator.sptep,
 			      spte, fault_handled);
+	/* 把OUTSIDE_GUEST_MODE存入vcpu->mode, 开启中断 */
 	walk_shadow_page_lockless_end(vcpu);
 
 	return fault_handled;
@@ -3778,6 +5590,10 @@ static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|6832| <<kvm_mmu_load>> r = mmu_alloc_roots(vcpu);
+ */
 static int mmu_alloc_roots(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->arch.mmu->direct_map)
@@ -3786,6 +5602,13 @@ static int mmu_alloc_roots(struct kvm_vcpu *vcpu)
 		return mmu_alloc_shadow_roots(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|6833| <<kvm_mmu_load>> kvm_mmu_sync_roots(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5433| <<handle_invpcid>> kvm_mmu_sync_roots(vcpu);
+ *   - arch/x86/kvm/x86.c|958| <<kvm_set_cr3>> kvm_mmu_sync_roots(vcpu);
+ *   - arch/x86/kvm/x86.c|7841| <<vcpu_enter_guest>> kvm_mmu_sync_roots(vcpu);
+ */
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -3942,6 +5765,10 @@ walk_shadow_page_get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 	return reserved;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|6337| <<kvm_mmu_page_fault>> r = handle_mmio_page_fault(vcpu, cr2, direct);
+ */
 static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 {
 	u64 spte;
@@ -3976,6 +5803,12 @@ static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 	return RET_PF_RETRY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5663| <<nonpaging_page_fault>> if (page_fault_handle_page_track(vcpu, error_code, gfn))
+ *   - arch/x86/kvm/mmu.c|5779| <<tdp_page_fault>> if (page_fault_handle_page_track(vcpu, error_code, gfn))
+ *   - arch/x86/kvm/paging_tmpl.h|794| <<FNAME(page_fault)>> if (page_fault_handle_page_track(vcpu, error_code, walker.gfn)) {
+ */
 static bool page_fault_handle_page_track(struct kvm_vcpu *vcpu,
 					 u32 error_code, gfn_t gfn)
 {
@@ -4035,6 +5868,10 @@ static int nonpaging_page_fault(struct kvm_vcpu *vcpu, gva_t gva,
 			     error_code, gfn, prefault);
 }
 
+/*
+ * x86下调用的地方:
+ *   - arch/x86/kvm/mmu.c|5715| <<try_async_pf>> } else if (kvm_arch_setup_async_pf(vcpu, gva, gfn))
+ */
 static int kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn)
 {
 	struct kvm_arch_async_pf arch;
@@ -4047,6 +5884,20 @@ static int kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn)
 	return kvm_setup_async_pf(vcpu, gva, kvm_vcpu_gfn_to_hva(vcpu, gfn), &arch);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5172| <<nonpaging_map>> if (try_async_pf(vcpu, prefault, gfn, v, &pfn, write, &map_writable))
+ *   - arch/x86/kvm/mmu.c|5802| <<tdp_page_fault>> if (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, write, &map_writable))
+ *   - arch/x86/kvm/paging_tmpl.h|816| <<FNAME(page_fault)>> if (try_async_pf(vcpu, prefault, walker.gfn, addr, &pfn, write_fault,
+ *
+ * 1. 根据gfn找到对应的memslot
+ * 2. 用memslot的起始hva(userspace_addr)+(gfn-slot中的起始gfn(base_gfn))*页大小(PAGE_SIZE)
+ *    得到gfn对应的起始hva
+ * 3. 为该hva分配一个物理页,有hva_to_pfn_fast()和hva_to_pfn_slow()两种,hva_to_pfn_fast()
+ *    实际上是调用__get_user_pages_fast(),会尝试去pin该page,即确保该地址所在的物理页在内存中.
+ *    如果失败,退化到hva_to_pfn_slow(),会先去拿mm->mmap_sem的锁然后调用__get_user_pages()来pin
+ * 4. 如果分配成功,对其返回的struct page调用page_to_pfn()得到对应的pfn
+ */
 static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 			 gva_t gva, kvm_pfn_t *pfn, bool write, bool *writable)
 {
@@ -4061,8 +5912,17 @@ static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 		return false;
 	}
 
+	/*
+	 * 找到gfn对应的slot
+	 * 对于mmio, slot应该为NULL
+	 */
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
 	async = false;
+	/*
+	 * 找到gfn对应的pfn
+	 *
+	 * mmio则返回KVM_PFN_NOSLOT
+	 */
 	*pfn = __gfn_to_pfn_memslot(slot, gfn, false, &async, write, writable);
 	if (!async)
 		return false; /* *pfn has correct page already */
@@ -4081,6 +5941,11 @@ static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm.c|2698| <<pf_interception>> return kvm_handle_page_fault(&svm->vcpu, error_code, fault_address,
+ *   - arch/x86/kvm/vmx/vmx.c|4551| <<handle_exception_nmi>> return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
+ */
 int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 				u64 fault_address, char *insn, int insn_len)
 {
@@ -4113,6 +5978,11 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 }
 EXPORT_SYMBOL_GPL(kvm_handle_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5786| <<tdp_page_fault>> force_pt_level = !check_hugepage_cache_consistency(vcpu, gfn,
+ *   - arch/x86/kvm/mmu.c|5791| <<tdp_page_fault>> !check_hugepage_cache_consistency(vcpu, gfn, level))
+ */
 static bool
 check_hugepage_cache_consistency(struct kvm_vcpu *vcpu, gfn_t gfn, int level)
 {
@@ -4151,6 +6021,11 @@ static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 		if (level > PT_DIRECTORY_LEVEL &&
 		    !check_hugepage_cache_consistency(vcpu, gfn, level))
 			level = PT_DIRECTORY_LEVEL;
+		/*
+		 * KVM_PAGES_PER_HPAGE(1) : 4K有多少4K的page
+		 * KVM_PAGES_PER_HPAGE(2) : 2M有多少4K的page
+		 * KVM_PAGES_PER_HPAGE(3) : 1G有多少4K的page
+		 */
 		gfn &= ~(KVM_PAGES_PER_HPAGE(level) - 1);
 	}
 
@@ -4286,6 +6161,11 @@ static void __kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3,
 				   KVM_MMU_ROOT_CURRENT);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|976| <<nested_vmx_load_cr3>> kvm_mmu_new_cr3(vcpu, cr3, false);
+ *   - arch/x86/kvm/x86.c|971| <<kvm_set_cr3>> kvm_mmu_new_cr3(vcpu, cr3, skip_tlb_flush);
+ */
 void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3, bool skip_tlb_flush)
 {
 	__kvm_mmu_new_cr3(vcpu, new_cr3, kvm_mmu_calc_root_page_role(vcpu),
@@ -4807,6 +6687,11 @@ static void paging32E_init_context(struct kvm_vcpu *vcpu,
 	paging64_init_context_common(vcpu, context, PT32E_ROOT_LEVEL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5682| <<kvm_calc_mmu_role_common>> role.ext = kvm_calc_mmu_role_ext(vcpu);
+ *   - arch/x86/kvm/mmu.c|5847| <<kvm_calc_shadow_ept_root_page_role>> role.ext = kvm_calc_mmu_role_ext(vcpu);
+ */
 static union kvm_mmu_extended_role kvm_calc_mmu_role_ext(struct kvm_vcpu *vcpu)
 {
 	union kvm_mmu_extended_role ext = {0};
@@ -4825,6 +6710,12 @@ static union kvm_mmu_extended_role kvm_calc_mmu_role_ext(struct kvm_vcpu *vcpu)
 	return ext;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5695| <<kvm_calc_tdp_mmu_root_page_role>> union kvm_mmu_role role = kvm_calc_mmu_role_common(vcpu, base_only);
+ *   - arch/x86/kvm/mmu.c|5776| <<kvm_calc_shadow_mmu_root_page_role>> union kvm_mmu_role role = kvm_calc_mmu_role_common(vcpu, base_only);
+ *   - arch/x86/kvm/mmu.c|5909| <<init_kvm_nested_mmu>> union kvm_mmu_role new_role = kvm_calc_mmu_role_common(vcpu, false);
+ */
 static union kvm_mmu_role kvm_calc_mmu_role_common(struct kvm_vcpu *vcpu,
 						   bool base_only)
 {
@@ -4844,11 +6735,25 @@ static union kvm_mmu_role kvm_calc_mmu_role_common(struct kvm_vcpu *vcpu,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5355| <<init_kvm_tdp_mmu>> kvm_calc_tdp_mmu_root_page_role(vcpu, false);
+ *   - arch/x86/kvm/mmu.c|5597| <<kvm_mmu_calc_root_page_role>> role = kvm_calc_tdp_mmu_root_page_role(vcpu, true);
+ */
 static union kvm_mmu_role
 kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu, bool base_only)
 {
 	union kvm_mmu_role role = kvm_calc_mmu_role_common(vcpu, base_only);
 
+	/*
+	 * shadow_accessed_mask:
+	 * 在new machine上从vmx_enable_tdp()第二次进来的结果:
+	 *   shadow_accessed_mask = enable_ept_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+	 *   也就是0x0000000000000100
+	 *
+	 * 这里shadow_accessed_mask不为0, 所以role.base.ad_disabled = false
+	 * 也就是ad没有disable
+	 */
 	role.base.ad_disabled = (shadow_accessed_mask == 0);
 	role.base.level = kvm_x86_ops->get_tdp_level(vcpu);
 	role.base.direct = true;
@@ -4857,6 +6762,10 @@ kvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu, bool base_only)
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5913| <<kvm_init_mmu>> init_kvm_tdp_mmu(vcpu);
+ */
 static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *context = vcpu->arch.mmu;
@@ -4907,6 +6816,11 @@ static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
 	reset_tdp_shadow_zero_bits_mask(vcpu, context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5427| <<kvm_init_shadow_mmu>> kvm_calc_shadow_mmu_root_page_role(vcpu, false);
+ *   - arch/x86/kvm/mmu.c|5599| <<kvm_mmu_calc_root_page_role>> role = kvm_calc_shadow_mmu_root_page_role(vcpu, true);
+ */
 static union kvm_mmu_role
 kvm_calc_shadow_mmu_root_page_role(struct kvm_vcpu *vcpu, bool base_only)
 {
@@ -4929,6 +6843,11 @@ kvm_calc_shadow_mmu_root_page_role(struct kvm_vcpu *vcpu, bool base_only)
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5843| <<init_kvm_softmmu>> kvm_init_shadow_mmu(vcpu);
+ *   - arch/x86/kvm/svm.c|2980| <<nested_svm_init_mmu_context>> kvm_init_shadow_mmu(vcpu);
+ */
 void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *context = vcpu->arch.mmu;
@@ -4953,6 +6872,10 @@ void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_init_shadow_mmu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5862| <<kvm_init_shadow_ept_mmu>> kvm_calc_shadow_ept_root_page_role(vcpu, accessed_dirty,
+ */
 static union kvm_mmu_role
 kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty,
 				   bool execonly)
@@ -4982,6 +6905,10 @@ kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|333| <<nested_ept_init_mmu_context>> kvm_init_shadow_ept_mmu(vcpu,
+ */
 void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 			     bool accessed_dirty, gpa_t new_eptp)
 {
@@ -5017,6 +6944,10 @@ void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
 }
 EXPORT_SYMBOL_GPL(kvm_init_shadow_ept_mmu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5915| <<kvm_init_mmu>> init_kvm_softmmu(vcpu);
+ */
 static void init_kvm_softmmu(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *context = vcpu->arch.mmu;
@@ -5077,6 +7008,12 @@ static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu)
 	update_last_nonleaf_level(vcpu, g_context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5935| <<kvm_mmu_reset_context>> kvm_init_mmu(vcpu, true);
+ *   - arch/x86/kvm/vmx/nested.c|981| <<nested_vmx_load_cr3>> kvm_init_mmu(vcpu, false);
+ *   - arch/x86/kvm/x86.c|8940| <<kvm_arch_vcpu_setup>> kvm_init_mmu(vcpu, false);
+ */
 void kvm_init_mmu(struct kvm_vcpu *vcpu, bool reset_roots)
 {
 	if (reset_roots) {
@@ -5097,6 +7034,10 @@ void kvm_init_mmu(struct kvm_vcpu *vcpu, bool reset_roots)
 }
 EXPORT_SYMBOL_GPL(kvm_init_mmu);
 
+/*
+ * used by only:
+ *   - arch/x86/kvm/mmu.c|5091| <<kvm_mmu_new_cr3>> __kvm_mmu_new_cr3(vcpu, new_cr3, kvm_mmu_calc_root_page_role(vcpu),
+ */
 static union kvm_mmu_page_role
 kvm_mmu_calc_root_page_role(struct kvm_vcpu *vcpu)
 {
@@ -5376,20 +7317,69 @@ static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * kvm mmio处理:
+ *
+ * 开始的时候mmio的内存都没有kvm_memory_slot, pte也都不存在,所以访问的时候会ept violation.
+ * handle_ept_violation()会调用kvm_mmu_page_fault().因为handle_ept_violation()不会为
+ * error_code设置PFERR_RSVD_MASK,所以kvm_mmu_page_fault()调用tdp_page_fault().
+ *
+ * tdp_page_fault()-->try_async_pf()-->__gfn_to_pfn_memslot()-->__gfn_to_hva_many().
+ * __gfn_to_hva_many()返回KVM_HVA_ERR_BAD=PAGE_OFFSET,所以__gfn_to_pfn_memslot()返回
+ * KVM_PFN_NOSLOT.
+ *
+ * 然后tdp_page_fault()-->__direct_map(),通过mmu_set_spte()-->set_mmio_spte()设置对应
+ * 的pte为mmio类型,也就是:
+ *
+ * 1. SPTE_SPECIAL_MASK(1往左移62位)是1
+ * 2. 结尾是110
+ * 3. 还有gen的信息
+ *
+ * 最后进行RET_PF_EMULATE
+ *
+ * 等到下一次page fault的时候,mmio的内存会触发handle_ept_misconfig()-->kvm_mmu_page_fault().
+ * 因为这次handle_ept_misconfig()设置error_code=PFERR_RSVD_MASK,kvm_mmu_page_fault()会调
+ * 用handle_mmio_page_fault(),返回进行emulate.
+ *
+ * called by:
+ *   - arch/x86/kvm/svm.c|2710| <<npf_interception>> return kvm_mmu_page_fault(&svm->vcpu, fault_address, error_code,
+ *   - arch/x86/kvm/vmx/vmx.c|5149| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5181| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t cr2, u64 error_code,
 		       void *insn, int insn_len)
 {
 	int r, emulation_type = 0;
 	enum emulation_result er;
+	/*
+	 * 在以下修改direct_map:
+	 *   - arch/x86/kvm/mmu.c|5833| <<nonpaging_init_context>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|6416| <<paging64_init_context_common>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|6445| <<paging32_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu.c|6549| <<init_kvm_tdp_mmu>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu.c|6700| <<kvm_init_shadow_ept_mmu>> context->direct_map = false;
+	 */
 	bool direct = vcpu->arch.mmu->direct_map;
 
 	/* With shadow page tables, fault_address contains a GVA or nGPA.  */
 	if (vcpu->arch.mmu->direct_map) {
+		/*
+		 * 在以下设置使用gpa_available:
+		 *   - arch/x86/kvm/mmu.c|7117| <<kvm_mmu_page_fault>> vcpu->arch.gpa_available = true;
+		 *   - arch/x86/kvm/x86.c|5498| <<emulator_read_write_onepage>> if (vcpu->arch.gpa_available &&
+		 *   - arch/x86/kvm/x86.c|8106| <<vcpu_enter_guest>> vcpu->arch.gpa_available = false;
+		 */
 		vcpu->arch.gpa_available = true;
 		vcpu->arch.gpa_val = cr2;
 	}
 
 	r = RET_PF_INVALID;
+	/*
+	 * 设置PFERR_RSVD_MASK的地方:
+	 *   - arch/x86/kvm/paging_tmpl.h|404| <<FNAME(walk_addr_generic)>> errcode = PFERR_RSVD_MASK | PFERR_PRESENT_MASK;
+	 *   - arch/x86/kvm/paging_tmpl.h|776| <<FNAME(page_fault)>> error_code &= ~PFERR_RSVD_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|5181| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	if (unlikely(error_code & PFERR_RSVD_MASK)) {
 		r = handle_mmio_page_fault(vcpu, cr2, direct);
 		if (r == RET_PF_EMULATE)
@@ -5397,6 +7387,14 @@ int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t cr2, u64 error_code,
 	}
 
 	if (r == RET_PF_INVALID) {
+		/*
+		 * 设置page_fault的地方:
+		 *   - arch/x86/kvm/mmu.c|5826| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+		 *   - arch/x86/kvm/mmu.c|6410| <<paging64_init_context_common>> context->page_fault = paging64_page_fault;
+		 *   - arch/x86/kvm/mmu.c|6439| <<paging32_init_context>> context->page_fault = paging32_page_fault;
+		 *   - arch/x86/kvm/mmu.c|6544| <<init_kvm_tdp_mmu>> context->page_fault = tdp_page_fault;
+		 *   - arch/x86/kvm/mmu.c|6694| <<kvm_init_shadow_ept_mmu>> context->page_fault = ept_page_fault;
+		 */
 		r = vcpu->arch.mmu->page_fault(vcpu, cr2,
 					       lower_32_bits(error_code),
 					       false);
@@ -5526,6 +7524,11 @@ void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_invpcid_gva);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm.c|1367| <<svm_hardware_setup>> kvm_enable_tdp();
+ *   - arch/x86/kvm/vmx/vmx.c|5286| <<vmx_enable_tdp>> kvm_enable_tdp();
+ */
 void kvm_enable_tdp(void)
 {
 	tdp_enabled = true;
@@ -5645,6 +7648,10 @@ static int alloc_mmu_pages(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9239| <<kvm_arch_vcpu_init>> r = kvm_mmu_create(vcpu);
+ */
 int kvm_mmu_create(struct kvm_vcpu *vcpu)
 {
 	uint i;
@@ -5669,6 +7676,10 @@ int kvm_mmu_create(struct kvm_vcpu *vcpu)
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|6958| <<kvm_mmu_zap_all_fast>> kvm_zap_obsolete_pages(kvm);
+ */
 static void kvm_zap_obsolete_pages(struct kvm *kvm)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -5753,6 +7764,10 @@ static void kvm_mmu_invalidate_zap_pages_in_memslot(struct kvm *kvm,
 	kvm_mmu_zap_all_fast(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9347| <<kvm_arch_init_vm>> kvm_mmu_init_vm(kvm);
+ */
 void kvm_mmu_init_vm(struct kvm *kvm)
 {
 	struct kvm_page_track_notifier_node *node = &kvm->arch.mmu_sp_tracker;
@@ -5801,6 +7816,11 @@ static bool slot_rmap_write_protect(struct kvm *kvm,
 	return __rmap_write_protect(kvm, rmap_head, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9597| <<kvm_mmu_slot_apply_flags>> kvm_mmu_slot_remove_write_access(kvm, new);
+ *   - arch/x86/kvm/x86.c|9635| <<kvm_mmu_slot_apply_flags>> kvm_mmu_slot_remove_write_access(kvm, new);
+ */
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
 				      struct kvm_memory_slot *memslot)
 {
@@ -5834,6 +7854,10 @@ void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
 			memslot->npages);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|6699| <<kvm_mmu_zap_collapsible_sptes>> kvm_mmu_zap_collapsible_spte, true);
+ */
 static bool kvm_mmu_zap_collapsible_spte(struct kvm *kvm,
 					 struct kvm_rmap_head *rmap_head)
 {
@@ -5925,6 +7949,10 @@ void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_slot_largepage_remove_write_access);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7182| <<vmx_slot_disable_log_dirty>> kvm_mmu_slot_set_dirty(kvm, slot);
+ */
 void kvm_mmu_slot_set_dirty(struct kvm *kvm,
 			    struct kvm_memory_slot *memslot)
 {
@@ -5943,6 +7971,11 @@ void kvm_mmu_slot_set_dirty(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_slot_set_dirty);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|6790| <<kvm_mmu_zap_all>> return __kvm_mmu_zap_all(kvm, false);
+ *   - arch/x86/kvm/mmu.c|6814| <<kvm_mmu_invalidate_mmio_sptes>> __kvm_mmu_zap_all(kvm, true);
+ */
 static void __kvm_mmu_zap_all(struct kvm *kvm, bool mmio_only)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -5968,11 +8001,19 @@ static void __kvm_mmu_zap_all(struct kvm *kvm, bool mmio_only)
 	spin_unlock(&kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9685| <<kvm_arch_flush_shadow_all>> kvm_mmu_zap_all(kvm);
+ */
 void kvm_mmu_zap_all(struct kvm *kvm)
 {
 	return __kvm_mmu_zap_all(kvm, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9581| <<kvm_arch_memslots_updated>> kvm_mmu_invalidate_mmio_sptes(kvm, gen);
+ */
 void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, u64 gen)
 {
 	WARN_ON(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS);
@@ -5998,6 +8039,9 @@ void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, u64 gen)
 	}
 }
 
+/*
+ * struct shrinker mmu_shrinker.scan_objects = mmu_shrink_scan()
+ */
 static unsigned long
 mmu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)
 {
@@ -6051,24 +8095,41 @@ mmu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)
 	return freed;
 }
 
+/*
+ * struct shrinker mmu_shrinker.count_objects = mmu_shrink_count()
+ */
 static unsigned long
 mmu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)
 {
 	return percpu_counter_read_positive(&kvm_total_used_mmu_pages);
 }
 
+/*
+ * 在以下使用mmu_shrinker:
+ *   - arch/x86/kvm/mmu.c|7382| <<kvm_mmu_module_init>> ret = register_shrinker(&mmu_shrinker);
+ *   - arch/x86/kvm/mmu.c|7441| <<kvm_mmu_module_exit>> unregister_shrinker(&mmu_shrinker);
+ */
 static struct shrinker mmu_shrinker = {
 	.count_objects = mmu_shrink_count,
 	.scan_objects = mmu_shrink_scan,
 	.seeks = DEFAULT_SEEKS * 10,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|6965| <<kvm_mmu_module_init>> mmu_destroy_caches();
+ *   - arch/x86/kvm/mmu.c|7002| <<kvm_mmu_module_exit>> mmu_destroy_caches();
+ */
 static void mmu_destroy_caches(void)
 {
 	kmem_cache_destroy(pte_list_desc_cache);
 	kmem_cache_destroy(mmu_page_header_cache);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|6437| <<kvm_mmu_module_init>> kvm_set_mmio_spte_mask();
+ */
 static void kvm_set_mmio_spte_mask(void)
 {
 	u64 mask;
@@ -6097,6 +8158,10 @@ static void kvm_set_mmio_spte_mask(void)
 	kvm_mmu_set_mmio_spte_mask(mask, mask);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_init>> r = kvm_mmu_module_init();
+ */
 int kvm_mmu_module_init(void)
 {
 	int ret = -ENOMEM;
@@ -6144,6 +8209,10 @@ int kvm_mmu_module_init(void)
 /*
  * Calculate mmu pages needed for kvm.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9650| <<kvm_arch_commit_memory_region>> kvm_mmu_calculate_default_mmu_pages(kvm));
+ */
 unsigned long kvm_mmu_calculate_default_mmu_pages(struct kvm *kvm)
 {
 	unsigned long nr_mmu_pages;
@@ -6165,6 +8234,11 @@ unsigned long kvm_mmu_calculate_default_mmu_pages(struct kvm *kvm)
 	return nr_mmu_pages;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9288| <<kvm_arch_vcpu_init>> kvm_mmu_destroy(vcpu);
+ *   - arch/x86/kvm/x86.c|9304| <<kvm_arch_vcpu_uninit>> kvm_mmu_destroy(vcpu);
+ */
 void kvm_mmu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_unload(vcpu);
@@ -6172,6 +8246,10 @@ void kvm_mmu_destroy(struct kvm_vcpu *vcpu)
 	mmu_free_memory_caches(vcpu);
 }
 
+/*
+ * called by only:
+ *   - arch/x86/kvm/x86.c|7121| <<kvm_arch_exit>> kvm_mmu_module_exit();
+ */
 void kvm_mmu_module_exit(void)
 {
 	mmu_destroy_caches();
diff --git a/arch/x86/kvm/mtrr.c b/arch/x86/kvm/mtrr.c
index 25ce3ed..5afd5a2 100644
--- a/arch/x86/kvm/mtrr.c
+++ b/arch/x86/kvm/mtrr.c
@@ -688,6 +688,10 @@ u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_mtrr_get_guest_memory_type);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5813| <<check_hugepage_cache_consistency>> return kvm_mtrr_check_gfn_range_consistency(vcpu, gfn, page_num);
+ */
 bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
 					  int page_num)
 {
diff --git a/arch/x86/kvm/page_track.c b/arch/x86/kvm/page_track.c
index 3521e2d..e72324f 100644
--- a/arch/x86/kvm/page_track.c
+++ b/arch/x86/kvm/page_track.c
@@ -32,6 +32,10 @@ void kvm_page_track_free_memslot(struct kvm_memory_slot *free,
 		}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9563| <<kvm_arch_create_memslot>> if (kvm_page_track_create_memslot(slot, npages))
+ */
 int kvm_page_track_create_memslot(struct kvm_memory_slot *slot,
 				  unsigned long npages)
 {
diff --git a/arch/x86/kvm/vmx/ops.h b/arch/x86/kvm/vmx/ops.h
index 2200fb6..17bc868 100644
--- a/arch/x86/kvm/vmx/ops.h
+++ b/arch/x86/kvm/vmx/ops.h
@@ -232,6 +232,10 @@ static inline void __invept(unsigned long ext, u64 eptp, gpa_t gpa)
 	BUG_ON(error);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2735| <<vmx_flush_tlb_gva>> if (!vpid_sync_vcpu_addr(vpid, addr))
+ */
 static inline bool vpid_sync_vcpu_addr(int vpid, gva_t addr)
 {
 	if (vpid == 0)
diff --git a/arch/x86/kvm/vmx/vmenter.S b/arch/x86/kvm/vmx/vmenter.S
index 4010d51..ef2c75d 100644
--- a/arch/x86/kvm/vmx/vmenter.S
+++ b/arch/x86/kvm/vmx/vmenter.S
@@ -43,6 +43,9 @@
  * they VM-Fail, whereas a successful VM-Enter + VM-Exit will jump
  * to vmx_vmexit.
  */
+/**
+ * 在下面同样是汇编的__vmx_vcpu_run()调用
+ */
 ENTRY(vmx_vmenter)
 	/* EFLAGS.ZF is set if VMCS.LAUNCHED == 0 */
 	je 2f
@@ -77,6 +80,10 @@ ENDPROC(vmx_vmenter)
  * here after hardware loads the host's state, i.e. this is the destination
  * referred to by VMCS.HOST_RIP.
  */
+/**
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|3805| <<vmx_set_constant_host_state>> vmcs_writel(HOST_RIP, (unsigned long )vmx_vmexit);
+ */
 ENTRY(vmx_vmexit)
 #ifdef CONFIG_RETPOLINE
 	ALTERNATIVE "jmp .Lvmexit_skip_rsb", "", X86_FEATURE_RETPOLINE
@@ -101,6 +108,10 @@ ENDPROC(vmx_vmexit)
  * Returns:
  *	0 on VM-Exit, 1 on VM-Fail
  */
+/**
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6499| <<vmx_vcpu_run>> vmx->fail = __vmx_vcpu_run(vmx, (unsigned long *)&vcpu->arch.regs,
+ */
 ENTRY(__vmx_vcpu_run)
 	push %_ASM_BP
 	mov  %_ASM_SP, %_ASM_BP
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index c030c96..f2df7d0 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -79,6 +79,10 @@ module_param_named(vnmi, enable_vnmi, bool, S_IRUGO);
 bool __read_mostly flexpriority_enabled = 1;
 module_param_named(flexpriority, flexpriority_enabled, bool, S_IRUGO);
 
+/*
+ * 只有可能在下面修改成0:
+ *   - arch/x86/kvm/vmx/vmx.c|7554| <<hardware_setup>> enable_ept = 0;
+ */
 bool __read_mostly enable_ept = 1;
 module_param_named(ept, enable_ept, bool, S_IRUGO);
 
@@ -86,6 +90,11 @@ bool __read_mostly enable_unrestricted_guest = 1;
 module_param_named(unrestricted_guest,
 			enable_unrestricted_guest, bool, S_IRUGO);
 
+/*
+ * 在以下设置enable_ept_ad_bits:
+ *   - arch/x86/kvm/vmx/vmx.c|90| <<global>> module_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/vmx.c|7539| <<hardware_setup>> enable_ept_ad_bits = 0;
+ */
 bool __read_mostly enable_ept_ad_bits = 1;
 module_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);
 
@@ -1804,6 +1813,13 @@ static int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|352| <<kvm_pmu_set_msr>> return kvm_x86_ops->pmu_ops->set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|1385| <<kvm_set_msr>> return kvm_x86_ops->set_msr(vcpu, msr)
+ *
+ * struct kvm_x86_ops vmx_x86_ops.set_msr = vmx_set_msr()
+ */
 static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -2728,6 +2744,13 @@ static void exit_lmode(struct kvm_vcpu *vcpu)
 
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5497| <<kvm_mmu_invlpg>> kvm_x86_ops->tlb_flush_gva(vcpu, gva);
+ *   - arch/x86/kvm/mmu.c|5522| <<kvm_mmu_invpcid_gva>> kvm_x86_ops->tlb_flush_gva(vcpu, gva);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.tlb_flush_gva = vmx_flush_tlb_gva()
+ */
 static void vmx_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t addr)
 {
 	int vpid = to_vmx(vcpu)->vpid;
@@ -4019,6 +4042,10 @@ static void vmx_compute_secondary_exec_control(struct vcpu_vmx *vmx)
 	vmx->secondary_exec_control = exec_control;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5310| <<vmx_enable_tdp>> ept_set_mmio_spte_mask();
+ */
 static void ept_set_mmio_spte_mask(void)
 {
 	/*
@@ -4874,6 +4901,10 @@ static int handle_rdmsr(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * used by:
+ *   - arch/x86/kvm/vmx/vmx.c|5547| <<global>> [EXIT_REASON_MSR_WRITE] = handle_wrmsr,
+ */
 static int handle_wrmsr(struct kvm_vcpu *vcpu)
 {
 	struct msr_data msr;
@@ -5076,6 +5107,15 @@ static int handle_task_switch(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 当Guest第一次访问某个页面时,由于没有GVA到GPA的映射,触发Guest OS的page fault.
+ * 于是Guest OS会建立对应的pte并修复好各级页表,最后访问对应的GPA.由于没有建立
+ * GPA到HVA的映射,于是触发EPT Violation,VMEXIT到KVM.  KVM在vmx_handle_exit中执
+ * 行kvm_vmx_exit_handlers[exit_reason],发现exit_reason是
+ * EXIT_REASON_EPT_VIOLATION,因此调用 handle_ept_violation.
+ *
+ * kvm_vmx_exit_handlers[EXIT_REASON_EPT_VIOLATION]
+ */
 static int handle_ept_violation(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
@@ -5117,6 +5157,12 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 	       PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;
 
 	vcpu->arch.exit_qualification = exit_qualification;
+	/*
+	 * kvm_mmu_page_fault()被以下调用:
+	 *   - arch/x86/kvm/svm.c|2710| <<npf_interception>> return kvm_mmu_page_fault(&svm->vcpu, fault_address, error_code,
+	 *   - arch/x86/kvm/vmx/vmx.c|5149| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|5181| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 }
 
@@ -5273,6 +5319,10 @@ static void wakeup_handler(void)
 	spin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7572| <<hardware_setup>> vmx_enable_tdp();
+ */
 static void vmx_enable_tdp(void)
 {
 	kvm_mmu_set_mask_ptes(VMX_EPT_READABLE_MASK,
@@ -6409,6 +6459,12 @@ void vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp)
 
 bool __vmx_vcpu_run(struct vcpu_vmx *vmx, unsigned long *regs, bool launched);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8023| <<vcpu_enter_guest>> kvm_x86_ops->run(vcpu);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.run = vmx_vcpu_run()
+ */
 static void vmx_vcpu_run(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7474,6 +7530,12 @@ static bool vmx_need_emulation_on_page_fault(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9149| <<kvm_arch_hardware_setup>> r = kvm_x86_ops->hardware_setup();
+ *
+ * struct kvm_x86_ops vmx_x86_ops.hardware_setup = hardware_setup()
+ */
 static __init int hardware_setup(void)
 {
 	unsigned long host_bndcfgs;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 91602d3..c083abc 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1355,6 +1355,15 @@ EXPORT_SYMBOL_GPL(kvm_enable_efer_bits);
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm.c|4450| <<wrmsr_interception>> if (kvm_set_msr(&svm->vcpu, &msr)) {
+ *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_load_msr>> if (kvm_set_msr(vcpu, &msr)) {
+ *   - arch/x86/kvm/vmx/nested.c|3984| <<nested_vmx_restore_host_state>> if (kvm_set_msr(vcpu, &msr)) {
+ *   - arch/x86/kvm/vmx/vmx.c|4906| <<handle_wrmsr>> if (kvm_set_msr(vcpu, &msr) != 0) {
+ *   - arch/x86/kvm/x86.c|1414| <<do_set_msr>> return kvm_set_msr(vcpu, &msr);
+ *   - arch/x86/kvm/x86.c|5995| <<emulator_set_msr>> return kvm_set_msr(emul_to_vcpu(ctxt), &msr);
+ */
 int kvm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 {
 	switch (msr->index) {
@@ -2480,6 +2489,15 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
 		&vcpu->arch.st.steal, sizeof(struct kvm_steal_time));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm.c|4434| <<svm_set_msr>> return kvm_set_msr_common(vcpu, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|1827| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|1862| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|1945| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|1948| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2072| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ */
 int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	bool pr = false;
@@ -3347,6 +3365,10 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	kvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3402| <<kvm_arch_vcpu_put>> kvm_steal_time_set_preempted(vcpu);
+ */
 static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)
 {
 	if (!(vcpu->arch.st.msr_val & KVM_MSR_ENABLED))
@@ -6478,6 +6500,12 @@ static bool is_vmware_backdoor_opcode(struct x86_emulate_ctxt *ctxt)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|7179| <<kvm_mmu_page_fault>> er = x86_emulate_instruction(vcpu, cr2, emulation_type, insn, insn_len);
+ *   - arch/x86/kvm/x86.c|6628| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|6635| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu,
 			    unsigned long cr2,
 			    int emulation_type,
@@ -7009,6 +7037,10 @@ static struct notifier_block pvclock_gtod_notifier = {
 };
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4280| <<kvm_init>> r = kvm_arch_init(opaque);
+ */
 int kvm_arch_init(void *opaque)
 {
 	int r;
@@ -7020,6 +7052,10 @@ int kvm_arch_init(void *opaque)
 		goto out;
 	}
 
+	/*
+	 * vmx: cpu_has_kvm_support()
+	 * svm: has_svm()
+	 */
 	if (!ops->cpu_has_kvm_support()) {
 		printk(KERN_ERR "kvm: no hardware support\n");
 		r = -EOPNOTSUPP;
@@ -7800,6 +7836,10 @@ EXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8163| <<vcpu_run>> r = vcpu_enter_guest(vcpu);
+ */
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -8146,6 +8186,10 @@ static inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
 		!vcpu->arch.apf.halted);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8371| <<kvm_arch_vcpu_ioctl_run>> r = vcpu_run(vcpu);
+ */
 static int vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -8308,6 +8352,10 @@ static void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 	trace_kvm_fpu(0);
 }
 
+/*
+ * called by (处理KVM_RUN):
+ *   - virt/kvm/kvm_main.c|2765| <<kvm_vcpu_ioctl>> r = kvm_arch_vcpu_ioctl_run(vcpu, vcpu->run);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 {
 	int r;
@@ -9142,6 +9190,10 @@ void kvm_arch_hardware_disable(void)
 	drop_user_return_notifiers();
 }
 
+/*
+ * x86下的调用:
+ *   - virt/kvm/kvm_main.c|4270| <<kvm_init>> r = kvm_arch_hardware_setup();
+ */
 int kvm_arch_hardware_setup(void)
 {
 	int r;
@@ -9365,6 +9417,13 @@ void kvm_arch_sync_events(struct kvm *kvm)
 	kvm_free_pit(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm.c|1680| <<avic_init_access_page>> ret = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|3393| <<init_rmode_identity_map>> r = __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/vmx/vmx.c|3444| <<alloc_apic_access_page>> r = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|9449| <<x86_set_memory_region>> r = __x86_set_memory_region(kvm, id, gpa, size);
+ */
 int __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size)
 {
 	int i, r;
@@ -9476,20 +9535,49 @@ void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 	kvm_page_track_free_memslot(free, dont);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1055| <<__kvm_set_memory_region>> if (kvm_arch_create_memslot(kvm, &new, npages))
+ *
+ * 从level=0到level=3分配slot->arch.rmap[level]和slot->arch.lpage_info[level]
+ * 根据情况初始化linfo[].disallow_lpage
+ */
 int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 			    unsigned long npages)
 {
 	int i;
 
+	/*
+	 * KVM_NR_PAGE_SIZES是3
+	 *
+	 * i = 0 表示 level = 1
+	 * i = 1 表示 level = 2
+	 * i = 2 表示 level = 3
+	 */
 	for (i = 0; i < KVM_NR_PAGE_SIZES; ++i) {
 		struct kvm_lpage_info *linfo;
 		unsigned long ugfn;
 		int lpages;
 		int level = i + 1;
 
+		/*
+		 * base_gfn是基于4k开始的gfn
+		 * gfn是基于4k结束的gfn
+		 * 计算从开始到结束需要用到几个hugepage (或者普通page)
+		 *   level是1的时候hugepage大小是4K
+		 *   level是2的时候hugepage大小是2M
+		 *   level是3的时候hugepage大小是1G
+		 */
 		lpages = gfn_to_index(slot->base_gfn + npages - 1,
 				      slot->base_gfn, level) + 1;
 
+		/*
+		 *
+		 * struct kvm_memory_slot
+		 *   struct kvm_arch_memory_slot arch;
+		 *     struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
+		 *     struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+		 */
 		slot->arch.rmap[i] =
 			kvcalloc(lpages, sizeof(*slot->arch.rmap[i]),
 				 GFP_KERNEL_ACCOUNT);
@@ -9504,6 +9592,24 @@ int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 
 		slot->arch.lpage_info[i - 1] = linfo;
 
+		/*
+		 * 在以下修改disallow_lpage:
+		 *   - arch/x86/kvm/mmu.c|1154| <<update_gfn_disallow_lpage_count>> linfo->disallow_lpage += count;
+		 *   - arch/x86/kvm/x86.c|9528| <<kvm_arch_create_memslot>> linfo[0].disallow_lpage = 1;
+		 *   - arch/x86/kvm/x86.c|9530| <<kvm_arch_create_memslot>> linfo[lpages - 1].disallow_lpage = 1;
+		 *   - arch/x86/kvm/x86.c|9542| <<kvm_arch_create_memslot>> linfo[j].disallow_lpage = 1;
+		 *
+		 * 在以下使用disallow_lpage:
+		 *   - arch/x86/kvm/mmu.c|2112| <<update_gfn_disallow_lpage_count>> WARN_ON(linfo->disallow_lpage < 0);
+		 *   - arch/x86/kvm/mmu.c|2177| <<__mmu_gfn_lpage_is_disallowed>> return !!linfo->disallow_lpage;
+		 *
+		 * KVM_PAGES_PER_HPAGE(1) : 4K有多少4K的page
+		 * KVM_PAGES_PER_HPAGE(2) : 2M有多少4K的page 
+		 * KVM_PAGES_PER_HPAGE(3) : 1G有多少4K的page
+		 *
+		 *
+		 * 这里相当于判断第一个和最后一个linfo[]是否符合alignment
+		 */
 		if (slot->base_gfn & (KVM_PAGES_PER_HPAGE(level) - 1))
 			linfo[0].disallow_lpage = 1;
 		if ((slot->base_gfn + npages) & (KVM_PAGES_PER_HPAGE(level) - 1))
@@ -9541,6 +9647,10 @@ int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|938| <<install_new_memslots>> kvm_arch_memslots_updated(kvm, gen);
+ */
 void kvm_arch_memslots_updated(struct kvm *kvm, u64 gen)
 {
 	/*
@@ -9649,6 +9759,11 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 		kvm_mmu_slot_apply_flags(kvm, (struct kvm_memory_slot *) new);
 }
 
+/*
+ * x86下的调用:
+ *   - virt/kvm/kvm_main.c|510| <<kvm_mmu_notifier_release>> kvm_arch_flush_shadow_all(kvm);
+ *   - virt/kvm/kvm_main.c|758| <<kvm_destroy_vm>> kvm_arch_flush_shadow_all(kvm);
+ */
 void kvm_arch_flush_shadow_all(struct kvm *kvm)
 {
 	kvm_mmu_zap_all(kvm);
@@ -9902,6 +10017,10 @@ bool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)
 	return kvm_x86_ops->interrupt_allowed(vcpu);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|206| <<kvm_setup_async_pf>> kvm_arch_async_page_not_present(vcpu, work);
+ */
 void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 				     struct kvm_async_pf *work)
 {
@@ -9932,6 +10051,28 @@ void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|24| <<kvm_async_page_present_sync>> kvm_arch_async_page_present(vcpu, work);
+ *   - virt/kvm/async_pf.c|31| <<kvm_async_page_present_async>> kvm_arch_async_page_present(vcpu, work);
+ *
+ * 一共有两处可能执行到这里
+ *
+ * vcpu_run()
+ *  -> kvm_check_async_pf_completion()
+ *      -> kvm_async_page_present_async()
+ *          -> kvm_arch_async_page_present()
+ *
+ * tdp_page_fault()
+ *  -> try_async_pf()
+ *      -> kvm_arch_setup_async_pf()
+ *          -> kvm_setup_async_pf()
+ *              -> INIT_WORK(&work->work, async_pf_execute)
+ *              -> schedule_work(&work->work))
+ *                 async_pf_execute()
+ *                  -> kvm_async_page_present_sync()
+ *                      -> kvm_arch_async_page_present()
+ */
 void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work)
 {
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 36ca2cf..8411814 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -185,6 +185,12 @@ void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 EXPORT_SYMBOL_GPL(vhost_work_init);
 
 /* Init poll structure */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1329| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev);
+ *   - drivers/vhost/net.c|1330| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev);
+ *   - drivers/vhost/vhost.c|489| <<vhost_dev_init>> vhost_poll_init(&vq->poll, vq->handle_kick,
+ */
 void vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,
 		     __poll_t mask, struct vhost_dev *dev)
 {
@@ -452,6 +458,13 @@ static size_t vhost_get_desc_size(struct vhost_virtqueue *vq,
 	return sizeof(*vq->desc) * num;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1325| <<vhost_net_open>> vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,
+ *   - drivers/vhost/scsi.c|1630| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs, VHOST_SCSI_MAX_VQ, UIO_MAXIOV,
+ *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs, VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+ *   - drivers/vhost/vsock.c|556| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs),
+ */
 void vhost_dev_init(struct vhost_dev *dev,
 		    struct vhost_virtqueue **vqs, int nvqs,
 		    int iov_limit, int weight, int byte_weight)
diff --git a/drivers/vhost/vsock.c b/drivers/vhost/vsock.c
index 6a50e1d..b356158 100644
--- a/drivers/vhost/vsock.c
+++ b/drivers/vhost/vsock.c
@@ -35,6 +35,11 @@ enum {
 
 /* Used to track all the vhost_vsock instances on the system. */
 static DEFINE_MUTEX(vhost_vsock_mutex);
+/*
+ * vhost_vsock_hash在以下使用:
+ *   - drivers/vhost/vsock.c|68| <<vhost_vsock_get>> hash_for_each_possible_rcu(vhost_vsock_hash, vsock, hash, guest_cid) {
+ *   - drivers/vhost/vsock.c|658| <<vhost_vsock_set_cid>> hash_add_rcu(vhost_vsock_hash, &vsock->hash, vsock->guest_cid);
+ */
 static DEFINE_READ_MOSTLY_HASHTABLE(vhost_vsock_hash, 8);
 
 struct vhost_vsock {
@@ -206,6 +211,9 @@ static void vhost_transport_send_pkt_work(struct vhost_work *work)
 	vhost_transport_do_send_pkt(vsock, vq);
 }
 
+/*
+ * struct virtio_transport vhost_transport.send_pkt = vhost_transport_send_pkt()
+ */
 static int
 vhost_transport_send_pkt(struct virtio_vsock_pkt *pkt)
 {
@@ -434,6 +442,10 @@ static void vhost_vsock_handle_rx_kick(struct vhost_work *work)
 	vhost_transport_do_send_pkt(vsock, vq);
 }
 
+/*
+ * 处理VHOST_VSOCK_SET_RUNNING:
+ *   - drivers/vhost/vsock.c|725| <<vhost_vsock_dev_ioctl>> return vhost_vsock_start(vsock);
+ */
 static int vhost_vsock_start(struct vhost_vsock *vsock)
 {
 	struct vhost_virtqueue *vq;
@@ -514,6 +526,9 @@ static void vhost_vsock_free(struct vhost_vsock *vsock)
 	kvfree(vsock);
 }
 
+/*
+ * struct file_operations vhost_vsock_fops.open = vhost_vsock_dev_open()
+ */
 static int vhost_vsock_dev_open(struct inode *inode, struct file *file)
 {
 	struct vhost_virtqueue **vqs;
@@ -527,6 +542,7 @@ static int vhost_vsock_dev_open(struct inode *inode, struct file *file)
 	if (!vsock)
 		return -ENOMEM;
 
+	/* vsock->vqs类型是"struct vhost_virtqueue vqs[2];" */
 	vqs = kmalloc_array(ARRAY_SIZE(vsock->vqs), sizeof(*vqs), GFP_KERNEL);
 	if (!vqs) {
 		ret = -ENOMEM;
@@ -593,6 +609,9 @@ static void vhost_vsock_reset_orphans(struct sock *sk)
 	sk->sk_error_report(sk);
 }
 
+/*
+ * struct file_operations vhost_vsock_fops.vhost_vsock_dev_release()
+ */
 static int vhost_vsock_dev_release(struct inode *inode, struct file *file)
 {
 	struct vhost_vsock *vsock = file->private_data;
@@ -630,6 +649,10 @@ static int vhost_vsock_dev_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * 处理VHOST_VSOCK_SET_GUEST_CID:
+ *   - drivers/vhost/vsock.c|720| <<vhost_vsock_dev_ioctl>> return vhost_vsock_set_cid(vsock, guest_cid);
+ */
 static int vhost_vsock_set_cid(struct vhost_vsock *vsock, u64 guest_cid)
 {
 	struct vhost_vsock *other;
@@ -661,6 +684,10 @@ static int vhost_vsock_set_cid(struct vhost_vsock *vsock, u64 guest_cid)
 	return 0;
 }
 
+/*
+ * 处理VHOST_SET_FEATURES:
+ *   - drivers/vhost/vsock.c|736| <<vhost_vsock_dev_ioctl>> return vhost_vsock_set_features(vsock, features);
+ */
 static int vhost_vsock_set_features(struct vhost_vsock *vsock, u64 features)
 {
 	struct vhost_virtqueue *vq;
@@ -686,6 +713,9 @@ static int vhost_vsock_set_features(struct vhost_vsock *vsock, u64 features)
 	return 0;
 }
 
+/*
+ * struct file_operations vhost_vsock_fops.unlocked_ioctl = vhost_vsock_dev_ioctl()
+ */
 static long vhost_vsock_dev_ioctl(struct file *f, unsigned int ioctl,
 				  unsigned long arg)
 {
@@ -730,6 +760,9 @@ static long vhost_vsock_dev_ioctl(struct file *f, unsigned int ioctl,
 }
 
 #ifdef CONFIG_COMPAT
+/*
+ * struct file_operations vhost_vsock_fops.compat_ioctl = vhost_vsock_dev_compat_ioctl()
+ */
 static long vhost_vsock_dev_compat_ioctl(struct file *f, unsigned int ioctl,
 					 unsigned long arg)
 {
@@ -755,6 +788,9 @@ static struct miscdevice vhost_vsock_misc = {
 };
 
 static struct virtio_transport vhost_transport = {
+	/*
+	 * transport的类型是"struct vsock_transport transport;"
+	 */
 	.transport = {
 		.get_local_cid            = vhost_transport_get_local_cid,
 
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index fcb46b3..11bc4a7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -44,6 +44,13 @@
  * in kvm, other bits are visible for userspace which are defined in
  * include/linux/kvm_h.
  */
+/*
+ * x86下使用的例子:
+ *   - arch/x86/kvm/mmu.c|1868| <<memslot_valid_for_gpte>> if (!slot || slot->flags & KVM_MEMSLOT_INVALID)
+ *   - virt/kvm/kvm_main.c|1048| <<__kvm_set_memory_region>> slot->flags |= KVM_MEMSLOT_INVALID;
+ *   - virt/kvm/kvm_main.c|1349| <<kvm_is_visible_gfn>> memslot->flags & KVM_MEMSLOT_INVALID)
+ *   - virt/kvm/kvm_main.c|1388| <<__gfn_to_hva_many>> if (!slot || slot->flags & KVM_MEMSLOT_INVALID)
+ */
 #define KVM_MEMSLOT_INVALID	(1UL << 16)
 
 /*
@@ -65,9 +72,23 @@
  * the actual generation number against accesses that were inserted into the
  * cache *before* the memslots were updated.
  */
+/*
+ * 在以下使用KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS:
+ *   - arch/x86/kvm/mmu.c|958| <<check_mmio_spte>> if (unlikely(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS))
+ *   - arch/x86/kvm/mmu.c|6668| <<kvm_mmu_invalidate_mmio_sptes>> WARN_ON(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS);
+ *   - arch/x86/kvm/x86.h|191| <<vcpu_cache_mmio_info>> if (unlikely(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS))
+ *   - virt/kvm/kvm_main.c|891| <<install_new_memslots>> WARN_ON(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS);
+ *   - virt/kvm/kvm_main.c|892| <<install_new_memslots>> slots->generation = gen | KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS;
+ *   - virt/kvm/kvm_main.c|903| <<install_new_memslots>> gen = slots->generation & ~KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS;
+ */
 #define KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS	BIT_ULL(63)
 
 /* Two fragments for cross MMIO pages. */
+/*
+ * 在以下使用KVM_MAX_MMIO_FRAGMENTS:
+ *   - include/linux/kvm_host.h|296| <<global>> struct kvm_mmio_fragment mmio_fragments[KVM_MAX_MMIO_FRAGMENTS];
+ *   - arch/x86/kvm/x86.c|5523| <<emulator_read_write_onepage>> WARN_ON(vcpu->mmio_nr_fragments >= KVM_MAX_MMIO_FRAGMENTS);
+ */
 #define KVM_MAX_MMIO_FRAGMENTS	2
 
 #ifndef KVM_ADDRESS_SPACE_NUM
@@ -107,6 +128,12 @@ static inline bool is_error_noslot_pfn(kvm_pfn_t pfn)
 }
 
 /* noslot pfn indicates that the gfn is not in slot. */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|1058| <<set_mmio_spte>> if (unlikely(is_noslot_pfn(pfn))) {
+ *   - arch/x86/kvm/mmu.c|4157| <<handle_abnormal_pfn>> if (unlikely(is_noslot_pfn(pfn)))
+ *   - arch/x86/kvm/paging_tmpl.h|829| <<FNAME(page_fault)>> !is_noslot_pfn(pfn)) {
+ */
 static inline bool is_noslot_pfn(kvm_pfn_t pfn)
 {
 	return pfn == KVM_PFN_NOSLOT;
@@ -143,6 +170,11 @@ static inline bool is_error_page(struct page *page)
  * Bits 4-7 are reserved for more arch-independent bits.
  */
 #define KVM_REQ_TLB_FLUSH         (0 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * x86上在以下使用KVM_REQ_MMU_RELOAD:
+ *   - arch/x86/kvm/x86.c|7827| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MMU_RELOAD, vcpu))
+ *   - virt/kvm/kvm_main.c|299| <<kvm_reload_remote_mmus>> kvm_make_all_cpus_request(kvm, KVM_REQ_MMU_RELOAD);
+ */
 #define KVM_REQ_MMU_RELOAD        (1 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_PENDING_TIMER     2
 #define KVM_REQ_UNHALT            3
@@ -218,9 +250,20 @@ int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu);
 #endif
 
 enum {
+	/*
+	 * x86下使用OUTSIDE_GUEST_MODE的地方:
+	 *   - arch/x86/kvm/mmu.c|1963| <<walk_shadow_page_lockless_end>> smp_store_release(&vcpu->mode, OUTSIDE_GUEST_MODE);
+	 *   - arch/x86/kvm/x86.c|8001| <<vcpu_enter_guest>> vcpu->mode = OUTSIDE_GUEST_MODE;
+	 *   - arch/x86/kvm/x86.c|8060| <<vcpu_enter_guest>> vcpu->mode = OUTSIDE_GUEST_MODE;
+	 *   - virt/kvm/kvm_main.c|202| <<kvm_request_needs_ipi>> return mode != OUTSIDE_GUEST_MODE;
+	 */
 	OUTSIDE_GUEST_MODE,
 	IN_GUEST_MODE,
 	EXITING_GUEST_MODE,
+	/*
+	 * x86在以下使用READING_SHADOW_PAGE_TABLES:
+	 *    - arch/x86/kvm/mmu.c|1947| <<walk_shadow_page_lockless_begin>> smp_store_mb(vcpu->mode, READING_SHADOW_PAGE_TABLES)
+	 */
 	READING_SHADOW_PAGE_TABLES,
 };
 
@@ -343,6 +386,13 @@ static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
 struct kvm_memory_slot {
 	gfn_t base_gfn;
 	unsigned long npages;
+	/*
+	 * 一个slot有许多客户机虚拟页面组成,通过dirty_bitmap
+	 * 标记每一个页是否可用,一个页面对应一个位
+	 *
+	 * 比如在如下分配:
+	 *   - virt/kvm/kvm_main.c|804| <<kvm_create_dirty_bitmap>> memslot->dirty_bitmap = kvzalloc(dirty_bytes, GFP_KERNEL_ACCOUNT);
+	 */
 	unsigned long *dirty_bitmap;
 	struct kvm_arch_memory_slot arch;
 	unsigned long userspace_addr;
@@ -435,6 +485,15 @@ struct kvm_memslots {
 	u64 generation;
 	struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
 	/* The mapping table from slot id to the index in memslots[]. */
+	/*
+	 * 更新和使用id_to_index的地方:
+	 *   - virt/kvm/kvm_main.c|549| <<kvm_alloc_memslots>> slots->id_to_index[i] = slots->memslots[i].id = i;
+	 *   - virt/kvm/kvm_main.c|844| <<update_memslots>> slots->id_to_index[mslots[i].id] = i;
+	 *   - virt/kvm/kvm_main.c|861| <<update_memslots>> slots->id_to_index[mslots[i].id] = i;
+	 *   - virt/kvm/kvm_main.c|868| <<update_memslots>> slots->id_to_index[mslots[i].id] = i;
+	 *   - include/linux/kvm_host.h|684| <<id_to_memslot>> int index = slots->id_to_index[id];
+	 *   - virt/kvm/kvm_main.c|822| <<update_memslots>> int i = slots->id_to_index[id];
+	 */
 	short id_to_index[KVM_MEM_SLOTS_NUM];
 	atomic_t lru_slot;
 	int used_slots;
@@ -444,6 +503,9 @@ struct kvm {
 	spinlock_t mmu_lock;
 	struct mutex slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
+	/*
+	 * KVM_ADDRESS_SPACE_NUM是1或者2
+	 */
 	struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
 
@@ -623,6 +685,10 @@ void kvm_exit(void);
 void kvm_get_kvm(struct kvm *kvm);
 void kvm_put_kvm(struct kvm *kvm);
 
+/*
+ * 利用srcu返回kvm->memslots[as_id]
+ * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+ */
 static inline struct kvm_memslots *__kvm_memslots(struct kvm *kvm, int as_id)
 {
 	as_id = array_index_nospec(as_id, KVM_ADDRESS_SPACE_NUM);
@@ -643,9 +709,26 @@ static inline struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu)
 	return __kvm_memslots(vcpu->kvm, as_id);
 }
 
+/*
+ * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+ * struct kvm_memslots有以下两个
+ *     struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+ *     short id_to_index[KVM_MEM_SLOTS_NUM];
+ * 这个函数先把id通过id_to_index[]转换成index
+ * 然后用来索引并返回slots->memslots[index]
+ */
 static inline struct kvm_memory_slot *
 id_to_memslot(struct kvm_memslots *slots, int id)
 {
+	/*
+	 * 更新和使用id_to_index的地方:
+	 *   - virt/kvm/kvm_main.c|549| <<kvm_alloc_memslots>> slots->id_to_index[i] = slots->memslots[i].id = i;
+	 *   - virt/kvm/kvm_main.c|844| <<update_memslots>> slots->id_to_index[mslots[i].id] = i;
+	 *   - virt/kvm/kvm_main.c|861| <<update_memslots>> slots->id_to_index[mslots[i].id] = i;
+	 *   - virt/kvm/kvm_main.c|868| <<update_memslots>> slots->id_to_index[mslots[i].id] = i;
+	 *   - include/linux/kvm_host.h|684| <<id_to_memslot>> int index = slots->id_to_index[id];
+	 *   - virt/kvm/kvm_main.c|822| <<update_memslots>> int i = slots->id_to_index[id];
+	 */
 	int index = slots->id_to_index[id];
 	struct kvm_memory_slot *slot;
 
@@ -1001,6 +1084,10 @@ bool kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args);
  * gfn_to_memslot() itself isn't here as an inline because that would
  * bloat other code too much.
  */
+/*
+ * called by:
+ *   - include/linux/kvm_host.h|1102| <<__gfn_to_memslot>> return search_memslots(slots, gfn);
+ */
 static inline struct kvm_memory_slot *
 search_memslots(struct kvm_memslots *slots, gfn_t gfn)
 {
@@ -1030,12 +1117,27 @@ search_memslots(struct kvm_memslots *slots, gfn_t gfn)
 	return NULL;
 }
 
+/*
+ * x86下调用的例子:
+ *   - arch/x86/kvm/mmu.c|1799| <<account_shadowed>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu.c|1818| <<unaccount_shadowed>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu.c|2043| <<gfn_to_rmap>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu_audit.c|139| <<inspect_spte_has_rmap>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu_audit.c|202| <<audit_write_protection>> slot = __gfn_to_memslot(slots, sp->gfn);
+ *   - virt/kvm/kvm_main.c|1335| <<gfn_to_memslot>> return __gfn_to_memslot(kvm_memslots(kvm), gfn);
+ *   - virt/kvm/kvm_main.c|1341| <<kvm_vcpu_gfn_to_memslot>> return __gfn_to_memslot(kvm_vcpu_memslots(vcpu), gfn);
+ *   - virt/kvm/kvm_main.c|2120| <<__kvm_gfn_to_hva_cache_init>> ghc->memslot = __gfn_to_memslot(slots, start_gfn);
+ */
 static inline struct kvm_memory_slot *
 __gfn_to_memslot(struct kvm_memslots *slots, gfn_t gfn)
 {
 	return search_memslots(slots, gfn);
 }
 
+/*
+ * x86下调用的例子:
+ *   - virt/kvm/kvm_main.c|1397| <<__gfn_to_hva_many>> return __gfn_to_hva_memslot(slot, gfn);
+ */
 static inline unsigned long
 __gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn)
 {
diff --git a/include/linux/kvm_para.h b/include/linux/kvm_para.h
index f23b90b..df64a3c 100644
--- a/include/linux/kvm_para.h
+++ b/include/linux/kvm_para.h
@@ -10,6 +10,28 @@ static inline bool kvm_para_has_feature(unsigned int feature)
 	return !!(kvm_arch_para_features() & (1UL << feature));
 }
 
+/*
+ * commit a4429e53c9b3082b05e51224c3d58dbdd39306c5
+ * Author: Wanpeng Li <wanpengli@tencent.com>
+ * Date:   Tue Feb 13 09:05:40 2018 +0800
+ *
+ * KVM: Introduce paravirtualization hints and KVM_HINTS_DEDICATED
+ *
+ * This patch introduces kvm_para_has_hint() to query for hints about
+ * the configuration of the guests.  The first hint KVM_HINTS_DEDICATED,
+ * is set if the guest has dedicated physical CPUs for each vCPU (i.e.
+ * pinning and no over-commitment).  This allows optimizing spinlocks
+ * and tells the guest to avoid PV TLB flush.
+ *
+ * Cc: Paolo Bonzini <pbonzini@redhat.com>
+ * Cc: Radim Krčmář <rkrcmar@redhat.com>
+ * Cc: Eduardo Habkost <ehabkost@redhat.com>
+ * Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ * Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
+ *
+ * 名字改成了KVM_HINTS_REALTIME
+ */
 static inline bool kvm_para_has_hint(unsigned int feature)
 {
 	return !!(kvm_arch_para_hints() & (1UL << feature));
diff --git a/include/uapi/linux/kvm_para.h b/include/uapi/linux/kvm_para.h
index 8b86609..1d5f5afe 100644
--- a/include/uapi/linux/kvm_para.h
+++ b/include/uapi/linux/kvm_para.h
@@ -27,7 +27,18 @@
 #define KVM_HC_MIPS_EXIT_VM		7
 #define KVM_HC_MIPS_CONSOLE_OUTPUT	8
 #define KVM_HC_CLOCK_PAIRING		9
+/*
+ * 在以下使用KVM_HC_SEND_IPI:
+ *   - arch/x86/kernel/kvm.c|827| <<__send_ipi_mask>> ret = kvm_hypercall4(KVM_HC_SEND_IPI, (unsigned long )ipi_bitmap,
+ *   - arch/x86/kernel/kvm.c|837| <<__send_ipi_mask>> ret = kvm_hypercall4(KVM_HC_SEND_IPI, (unsigned long )ipi_bitmap,
+ *   - arch/x86/kvm/x86.c|7297| <<kvm_emulate_hypercall>> case KVM_HC_SEND_IPI:
+ */
 #define KVM_HC_SEND_IPI		10
+/*
+ * 在以下使用KVM_HC_SCHED_YIELD:
+ *   - arch/x86/kernel/kvm.c|920| <<kvm_smp_send_call_func_ipi>> kvm_hypercall1(KVM_HC_SCHED_YIELD, per_cpu(x86_cpu_to_apicid, cpu));
+ *   - arch/x86/kvm/x86.c|7300| <<kvm_emulate_hypercall>> case KVM_HC_SCHED_YIELD:
+ */
 #define KVM_HC_SCHED_YIELD		11
 
 /*
diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c
index 35305d6..80bcef5 100644
--- a/virt/kvm/async_pf.c
+++ b/virt/kvm/async_pf.c
@@ -17,6 +17,10 @@
 #include "async_pf.h"
 #include <trace/events/kvm.h>
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|83| <<async_pf_execute>> kvm_async_page_present_sync(vcpu, apf);
+ */
 static inline void kvm_async_page_present_sync(struct kvm_vcpu *vcpu,
 					       struct kvm_async_pf *work)
 {
@@ -24,6 +28,10 @@ static inline void kvm_async_page_present_sync(struct kvm_vcpu *vcpu,
 	kvm_arch_async_page_present(vcpu, work);
 #endif
 }
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|160| <<kvm_check_async_pf_completion>> kvm_async_page_present_async(vcpu, work);
+ */
 static inline void kvm_async_page_present_async(struct kvm_vcpu *vcpu,
 						struct kvm_async_pf *work)
 {
@@ -57,6 +65,10 @@ void kvm_async_pf_vcpu_init(struct kvm_vcpu *vcpu)
 	spin_lock_init(&vcpu->async_pf.lock);
 }
 
+/*
+ * used by:
+ *   - virt/kvm/async_pf.c|200| <<kvm_setup_async_pf>> INIT_WORK(&work->work, async_pf_execute);
+ */
 static void async_pf_execute(struct work_struct *work)
 {
 	struct kvm_async_pf *apf =
@@ -144,6 +156,10 @@ void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 	vcpu->async_pf.queued = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8219| <<vcpu_run>> kvm_check_async_pf_completion(vcpu);
+ */
 void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
 {
 	struct kvm_async_pf *work;
@@ -165,6 +181,10 @@ void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5884| <<kvm_arch_setup_async_pf>> return kvm_setup_async_pf(vcpu, gva, kvm_vcpu_gfn_to_hva(vcpu, gfn), &arch);
+ */
 int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, unsigned long hva,
 		       struct kvm_arch_async_pf *arch)
 {
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index c6a91b0..18eaf1a 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -99,6 +99,17 @@ DEFINE_MUTEX(kvm_lock);
 static DEFINE_RAW_SPINLOCK(kvm_count_lock);
 LIST_HEAD(vm_list);
 
+/*
+ * 在以下使用cpus_hardware_enabled:
+ *   - virt/kvm/kvm_main.c|3534| <<hardware_enable_nolock>> if (cpumask_test_cpu(cpu, cpus_hardware_enabled))
+ *   - virt/kvm/kvm_main.c|3537| <<hardware_enable_nolock>> cpumask_set_cpu(cpu, cpus_hardware_enabled);
+ *   - virt/kvm/kvm_main.c|3542| <<hardware_enable_nolock>> cpumask_clear_cpu(cpu, cpus_hardware_enabled);
+ *   - virt/kvm/kvm_main.c|3561| <<hardware_disable_nolock>> if (!cpumask_test_cpu(cpu, cpus_hardware_enabled))
+ *   - virt/kvm/kvm_main.c|3563| <<hardware_disable_nolock>> cpumask_clear_cpu(cpu, cpus_hardware_enabled);
+ *   - virt/kvm/kvm_main.c|4271| <<kvm_init>> if (!zalloc_cpumask_var(&cpus_hardware_enabled, GFP_KERNEL)) {
+ *   - virt/kvm/kvm_main.c|4343| <<kvm_init>> free_cpumask_var(cpus_hardware_enabled);
+ *   - virt/kvm/kvm_main.c|4366| <<kvm_exit>> free_cpumask_var(cpus_hardware_enabled);
+ */
 static cpumask_var_t cpus_hardware_enabled;
 static int kvm_usage_count;
 static atomic_t hardware_enable_failed;
@@ -347,6 +358,12 @@ static inline struct kvm *mmu_notifier_to_kvm(struct mmu_notifier *mn)
 	return container_of(mn, struct kvm, mmu_notifier);
 }
 
+/*
+ * called by:
+ *   - mm/mmu_notifier.c|163| <<__mmu_notifier_change_pte>> mn->ops->change_pte(mn, mm, address, pte);
+ *
+ * struct mmu_notifier_ops kvm_mmu_notifier_ops.change_pte = kvm_mmu_notifier_change_pte()
+ */
 static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,
 					struct mm_struct *mm,
 					unsigned long address,
@@ -366,6 +383,9 @@ static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,
 	srcu_read_unlock(&kvm->srcu, idx);
 }
 
+/*
+ * struct mmu_notifier_ops kvm_mmu_notifier_ops.invalicate_range_start = kvm_mmu_notifier_invalidate_range_start()
+ */
 static int kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
 					const struct mmu_notifier_range *range)
 {
@@ -398,6 +418,9 @@ static int kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
 	return ret;
 }
 
+/*
+ * struct mmu_notifier_ops kvm_mmu_notifier_ops.invalidate_range_end = kvm_mmu_notifier_invalidate_range_end()
+ */
 static void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,
 					const struct mmu_notifier_range *range)
 {
@@ -422,6 +445,9 @@ static void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,
 	BUG_ON(kvm->mmu_notifier_count < 0);
 }
 
+/*
+ * struct mmu_notifier_ops kvm_mmu_notifier_ops.clear_flush_young = kvm_mmu_notifier_clear_flush_young()
+ */
 static int kvm_mmu_notifier_clear_flush_young(struct mmu_notifier *mn,
 					      struct mm_struct *mm,
 					      unsigned long start,
@@ -443,6 +469,9 @@ static int kvm_mmu_notifier_clear_flush_young(struct mmu_notifier *mn,
 	return young;
 }
 
+/*
+ * struct mmu_notifier_ops kvm_mmu_notifier_ops.clear_young = kvm_mmu_notifier_clear_young()
+ */
 static int kvm_mmu_notifier_clear_young(struct mmu_notifier *mn,
 					struct mm_struct *mm,
 					unsigned long start,
@@ -473,6 +502,9 @@ static int kvm_mmu_notifier_clear_young(struct mmu_notifier *mn,
 	return young;
 }
 
+/*
+ * struct mmu_notifier_ops kvm_mmu_notifier_ops.test_young = kvm_mmu_notifier_test_young()
+ */
 static int kvm_mmu_notifier_test_young(struct mmu_notifier *mn,
 				       struct mm_struct *mm,
 				       unsigned long address)
@@ -489,6 +521,9 @@ static int kvm_mmu_notifier_test_young(struct mmu_notifier *mn,
 	return young;
 }
 
+/*
+ * struct mmu_notifier_ops kvm_mmu_notifier_ops.release = kvm_mmu_notifier_release()
+ */
 static void kvm_mmu_notifier_release(struct mmu_notifier *mn,
 				     struct mm_struct *mm)
 {
@@ -915,6 +950,11 @@ static struct kvm_memslots *install_new_memslots(struct kvm *kvm,
  *
  * Must be called holding kvm->slots_lock for write.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9432| <<__x86_set_memory_region>> r = __kvm_set_memory_region(kvm, &m);
+ *   - virt/kvm/kvm_main.c|1098| <<kvm_set_memory_region>> r = __kvm_set_memory_region(kvm, mem);
+ */
 int __kvm_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region *mem)
 {
@@ -1060,6 +1100,12 @@ int __kvm_set_memory_region(struct kvm *kvm,
 		memset(&new.arch, 0, sizeof(new.arch));
 	}
 
+	/*
+	 * Insert memslot and re-sort memslots based on their GFN,
+	 * so binary search could be used to lookup GFN.
+	 * Sorting algorithm takes advantage of having initially
+	 * sorted array and known changed memslot position.
+	 */
 	update_memslots(slots, &new, change);
 	old_memslots = install_new_memslots(kvm, as_id, slots);
 
@@ -1078,6 +1124,10 @@ int __kvm_set_memory_region(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(__kvm_set_memory_region);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1110| <<kvm_vm_ioctl_set_memory_region>> return kvm_set_memory_region(kvm, mem);
+ */
 int kvm_set_memory_region(struct kvm *kvm,
 			  const struct kvm_userspace_memory_region *mem)
 {
@@ -1310,6 +1360,12 @@ struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(gfn_to_memslot);
 
+/*
+ * struct kvm中有struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+ * struct kvm_memslots中有:
+ *       struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+ *       short id_to_index[KVM_MEM_SLOTS_NUM];
+ */
 struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	return __gfn_to_memslot(kvm_vcpu_memslots(vcpu), gfn);
@@ -1327,6 +1383,11 @@ bool kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_is_visible_gfn);
 
+/*
+ * 根据gfn获取其对于hva在host中的page size (4K,2M还是1G)
+ *     根据gfn获取在qemu中对应的hva, 把hva转化为vm_area_struct
+ *     然后就知道vm_area_struct的page size了
+ */
 unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)
 {
 	struct vm_area_struct *vma;
@@ -1489,6 +1550,14 @@ static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,
 	if (async)
 		flags |= FOLL_NOWAIT;
 
+	/*
+	 * called by:
+	 *   - drivers/media/pci/ivtv/ivtv-udma.c|115| <<ivtv_udma_setup>> err = get_user_pages_unlocked(user_dma.uaddr, user_dma.page_count,
+	 *   - drivers/media/pci/ivtv/ivtv-yuv.c|66| <<ivtv_yuv_prep_user_dma>> y_pages = get_user_pages_unlocked(y_dma.uaddr,
+	 *   - drivers/media/pci/ivtv/ivtv-yuv.c|70| <<ivtv_yuv_prep_user_dma>> uv_pages = get_user_pages_unlocked(uv_dma.uaddr,
+	 *   - mm/gup.c|2380| <<__gup_longterm_unlocked>> ret = get_user_pages_unlocked(start, nr_pages,
+	 *   - virt/kvm/kvm_main.c|1553| <<hva_to_pfn_slow>> npages = get_user_pages_unlocked(addr, 1, &page, flags);
+	 */
 	npages = get_user_pages_unlocked(addr, 1, &page, flags);
 	if (npages != 1)
 		return npages;
@@ -1581,6 +1650,10 @@ static int hva_to_pfn_remapped(struct vm_area_struct *vma,
  * 2): @write_fault = false && @writable, @writable will tell the caller
  *     whether the mapping is writable.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1717| <<__gfn_to_pfn_memslot>> return hva_to_pfn(addr, atomic, async, write_fault,
+ */
 static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,
 			bool write_fault, bool *writable)
 {
@@ -1860,6 +1933,11 @@ void kvm_set_pfn_dirty(kvm_pfn_t pfn)
 }
 EXPORT_SYMBOL_GPL(kvm_set_pfn_dirty);
 
+/*
+ * x86下的使用:
+ *   - arch/x86/kvm/mmu.c|1761| <<mmu_spte_update>> kvm_set_pfn_accessed(spte_to_pfn(old_spte));
+ *   - arch/x86/kvm/mmu.c|1807| <<mmu_spte_clear_track_bits>> kvm_set_pfn_accessed(pfn);
+ */
 void kvm_set_pfn_accessed(kvm_pfn_t pfn)
 {
 	if (!kvm_is_reserved_pfn(pfn))
@@ -2203,6 +2281,13 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 }
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2009| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(memslot, gfn);
+ *   - virt/kvm/kvm_main.c|2141| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(ghc->memslot, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|2221| <<mark_page_dirty>> mark_page_dirty_in_slot(memslot, gfn);
+ *   - virt/kvm/kvm_main.c|2230| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(memslot, gfn);
+ */
 static void mark_page_dirty_in_slot(struct kvm_memory_slot *memslot,
 				    gfn_t gfn)
 {
@@ -2716,6 +2801,12 @@ static int kvm_vcpu_ioctl_set_sigmask(struct kvm_vcpu *vcpu, sigset_t *sigset)
 	return 0;
 }
 
+/*
+ * 还有一处调用:
+ *   - virt/kvm/kvm_main.c|2961| <<kvm_vcpu_compat_ioctl>> r = kvm_vcpu_ioctl(filp, ioctl, arg);
+ *
+ * struct file_operations kvm_vcpu_fops.unlocked_ioctl = kvm_vcpu_ioctl()
+ */
 static long kvm_vcpu_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -4241,6 +4332,12 @@ static void check_processor_compat(void *rtn)
 	*(int *)rtn = kvm_arch_check_processor_compat();
 }
 
+/*
+ * x86和arm的例子:
+ *   - arch/x86/kvm/svm.c|7336| <<svm_init>> return kvm_init(&svm_x86_ops, sizeof(struct vcpu_svm),
+ *   - arch/x86/kvm/vmx/vmx.c|7882| <<vmx_init>> r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
+ *   - virt/kvm/arm/arm.c|1729| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
-- 
2.7.4

