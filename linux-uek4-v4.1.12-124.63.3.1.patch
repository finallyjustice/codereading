From 57bc8a856b84af6560bed465491b1f5a3eb1c652 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 1 Aug 2023 09:14:32 -0700
Subject: [PATCH 1/1] linux-uek4-v4.1.12-124.63.3.1

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 drivers/block/xen-blkback/blkback.c | 459 ++++++++++++++++++++++++++++
 drivers/block/xen-blkback/common.h  |  86 ++++++
 drivers/block/xen-blkback/xenbus.c  |  25 ++
 drivers/xen/grant-table.c           |  47 +++
 4 files changed, 617 insertions(+)

diff --git a/drivers/block/xen-blkback/blkback.c b/drivers/block/xen-blkback/blkback.c
index 003cf6b2a19f..be53047d9dc8 100644
--- a/drivers/block/xen-blkback/blkback.c
+++ b/drivers/block/xen-blkback/blkback.c
@@ -63,6 +63,12 @@
  * IO workloads.
  */
 
+/*
+ * 在以下使用xen_blkif_max_buffer_pages:
+ *   - drivers/block/xen-blkback/blkback.c|66| <<global>> static int xen_blkif_max_buffer_pages = 1024;
+ *   - drivers/block/xen-blkback/blkback.c|67| <<global>> module_param_named(max_buffer_pages, xen_blkif_max_buffer_pages, int , 0644);
+ *   - drivers/block/xen-blkback/blkback.c|700| <<xen_blkif_schedule>> shrink_free_pagepool(ring, xen_blkif_max_buffer_pages);
+ */
 static int xen_blkif_max_buffer_pages = 1024;
 module_param_named(max_buffer_pages, xen_blkif_max_buffer_pages, int, 0644);
 MODULE_PARM_DESC(max_buffer_pages,
@@ -79,6 +85,19 @@ MODULE_PARM_DESC(max_buffer_pages,
  * algorithm.
  */
 
+/*
+ * 在以下使用xen_blkif_max_pgrants:
+ *   - drivers/block/xen-blkback/blkback.c|82| <<global>> static int xen_blkif_max_pgrants = 1056;
+ *   - drivers/block/xen-blkback/blkback.c|83| <<global>> module_param_named(max_persistent_grants, xen_blkif_max_pgrants, int , 0644);
+ *   - drivers/block/xen-blkback/blkback.c|219| <<add_persistent_gnt>> if (ring->persistent_gnt_c >= xen_blkif_max_pgrants) {
+ *   - drivers/block/xen-blkback/blkback.c|378| <<purge_persistent_gnt>> if (ring->persistent_gnt_c < xen_blkif_max_pgrants ||
+ *   - drivers/block/xen-blkback/blkback.c|379| <<purge_persistent_gnt>> (ring->persistent_gnt_c == xen_blkif_max_pgrants &&
+ *   - drivers/block/xen-blkback/blkback.c|389| <<purge_persistent_gnt>> num_clean = (xen_blkif_max_pgrants / 100) * LRU_PERCENT_CLEAN;
+ *   - drivers/block/xen-blkback/blkback.c|390| <<purge_persistent_gnt>> num_clean = ring->persistent_gnt_c - xen_blkif_max_pgrants + num_clean;
+ *   - drivers/block/xen-blkback/blkback.c|632| <<print_stats>> xen_blkif_max_pgrants);
+ *   - drivers/block/xen-blkback/blkback.c|928| <<xen_blkbk_map>> if (use_persistent_gnts && ring->persistent_gnt_c < xen_blkif_max_pgrants) {
+ *   - drivers/block/xen-blkback/blkback.c|955| <<xen_blkbk_map>> pr_debug("grant %u added to the tree of persistent grants, using %u/%u\n", persistent_gnt->gnt, ring->persistent_gnt_c, xen_blkif_max_pgrants);
+ */
 static int xen_blkif_max_pgrants = 1056;
 module_param_named(max_persistent_grants, xen_blkif_max_pgrants, int, 0644);
 MODULE_PARM_DESC(max_persistent_grants,
@@ -88,6 +107,16 @@ MODULE_PARM_DESC(max_persistent_grants,
  * Maximum number of rings/queues blkback supports, allow as many queues as there
  * are CPUs if user has not specified a value.
  */
+/*
+ * 在以下使用xenblk_max_queues:
+ *   - drivers/block/xen-blkback/blkback.c|91| <<global>> unsigned int xenblk_max_queues;
+ *   - drivers/block/xen-blkback/blkback.c|92| <<global>> module_param_named(max_queues, xenblk_max_queues, uint, 0644);
+ *   - drivers/block/xen-blkback/blkback.c|1580| <<xen_blkif_init>> if (xenblk_max_queues == 0)
+ *   - drivers/block/xen-blkback/blkback.c|1581| <<xen_blkif_init>> xenblk_max_queues = num_online_cpus();
+ *   - drivers/block/xen-blkback/xenbus.c|680| <<xen_blkbk_probe>> "multi-queue-max-queues", "%u", xenblk_max_queues);
+ *   - drivers/block/xen-blkback/xenbus.c|1248| <<connect_ring>> if (requested_num_queues > xenblk_max_queues
+ *   - drivers/block/xen-blkback/xenbus.c|1253| <<connect_ring>> requested_num_queues, xenblk_max_queues);
+ */
 unsigned int xenblk_max_queues;
 module_param_named(max_queues, xenblk_max_queues, uint, 0644);
 MODULE_PARM_DESC(max_queues,
@@ -98,6 +127,17 @@ MODULE_PARM_DESC(max_queues,
  * Maximum order of pages to be used for the shared ring between front and
  * backend, 4KB page granularity is used.
  */
+/*
+ * 在以下使用xen_blkif_max_ring_order:
+ *   - drivers/block/xen-blkback/blkback.c|101| <<global>> unsigned int xen_blkif_max_ring_order = XENBUS_MAX_RING_GRANT_ORDER;
+ *   - drivers/block/xen-blkback/blkback.c|102| <<global>> module_param_named(max_ring_page_order, xen_blkif_max_ring_order, int , S_IRUGO);
+ *   - drivers/block/xen-blkback/blkback.c|1574| <<xen_blkif_init>> if (xen_blkif_max_ring_order > XENBUS_MAX_RING_GRANT_ORDER) {
+ *   - drivers/block/xen-blkback/blkback.c|1576| <<xen_blkif_init>> xen_blkif_max_ring_order, XENBUS_MAX_RING_GRANT_ORDER);
+ *   - drivers/block/xen-blkback/blkback.c|1577| <<xen_blkif_init>> xen_blkif_max_ring_order = XENBUS_MAX_RING_GRANT_ORDER;
+ *   - drivers/block/xen-blkback/xenbus.c|699| <<xen_blkbk_probe>> xen_blkif_max_ring_order);
+ *   - drivers/block/xen-blkback/xenbus.c|1157| <<read_per_ring_refs>> if (ring_page_order > xen_blkif_max_ring_order) {
+ *   - drivers/block/xen-blkback/xenbus.c|1161| <<read_per_ring_refs>> xen_blkif_max_ring_order);
+ */
 unsigned int xen_blkif_max_ring_order = XENBUS_MAX_RING_GRANT_ORDER;
 module_param_named(max_ring_page_order, xen_blkif_max_ring_order, int, S_IRUGO);
 MODULE_PARM_DESC(max_ring_page_order, "Maximum order of pages to be used for the shared ring");
@@ -124,6 +164,10 @@ module_param(log_stats, int, 0644);
 /* Number of free pages to remove on each call to gnttab_free_pages */
 #define NUM_BATCH_FREE_PAGES 10
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|884| <<xen_blkbk_map>> if (get_free_page(ring, &pages[i].page)) {
+ */
 static inline int get_free_page(struct xen_blkif_ring *ring, struct page **page)
 {
 	unsigned long flags;
@@ -132,17 +176,41 @@ static inline int get_free_page(struct xen_blkif_ring *ring, struct page **page)
 	if (list_empty(&ring->free_pages)) {
 		BUG_ON(ring->free_pages_num != 0);
 		spin_unlock_irqrestore(&ring->free_pages_lock, flags);
+		/*
+		 * called by:
+		 *   - drivers/block/xen-blkback/blkback.c|179| <<get_free_page>> return gnttab_alloc_pages(1, page);
+		 *   - drivers/net/xen-netback/grant.c|72| <<xenvif_new_grant>> err = gnttab_alloc_pages(1, &page);
+		 *   - drivers/net/xen-netback/interface.c|572| <<xenvif_init_queue>> err = gnttab_alloc_pages(MAX_PENDING_REQS, queue->mmap_pages);
+		 *   - drivers/xen/gntdev.c|156| <<gntdev_alloc_map>> if (gnttab_alloc_pages(count, add->pages))
+		 *   - drivers/xen/xen-scsiback.c|245| <<get_free_page>> return gnttab_alloc_pages(1, page);
+		 */
 		return gnttab_alloc_pages(1, page);
 	}
 	BUG_ON(ring->free_pages_num == 0);
 	page[0] = list_first_entry(&ring->free_pages, struct page, lru);
 	list_del(&page[0]->lru);
+	/*
+	 * 在以下修改xen_blkif_ring->free_pages_num:
+	 *   - drivers/block/xen-blkback/blkback.c|155| <<put_free_pages>> ring->free_pages_num += num;
+	 *   - drivers/block/xen-blkback/blkback.c|140| <<get_free_page>> ring->free_pages_num--;
+	 *   - drivers/block/xen-blkback/blkback.c|172| <<shrink_free_pagepool>> ring->free_pages_num--;
+	 */
 	ring->free_pages_num--;
 	spin_unlock_irqrestore(&ring->free_pages_lock, flags);
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|318| <<free_persistent_gnts>> put_free_pages(ring, pages, segs_to_unmap);
+ *   - drivers/block/xen-blkback/blkback.c|358| <<xen_blkbk_unmap_purged_grants>> put_free_pages(ring, pages, segs_to_unmap);
+ *   - drivers/block/xen-blkback/blkback.c|366| <<xen_blkbk_unmap_purged_grants>> put_free_pages(ring, pages, segs_to_unmap);
+ *   - drivers/block/xen-blkback/blkback.c|770| <<xen_blkbk_unmap_and_respond_callback>> put_free_pages(ring, data->pages, data->count);
+ *   - drivers/block/xen-blkback/blkback.c|837| <<xen_blkbk_unmap>> put_free_pages(ring, unmap_pages, invcount);
+ *   - drivers/block/xen-blkback/blkback.c|885| <<xen_blkbk_map>> put_free_pages(ring, pages_to_gnt, segs_to_map);
+ *   - drivers/block/xen-blkback/blkback.c|918| <<xen_blkbk_map>> put_free_pages(ring, &pages[seg_idx].page, 1);
+ */
 static inline void put_free_pages(struct xen_blkif_ring *ring, struct page **page,
                                   int num)
 {
@@ -152,10 +220,21 @@ static inline void put_free_pages(struct xen_blkif_ring *ring, struct page **pag
 	spin_lock_irqsave(&ring->free_pages_lock, flags);
 	for (i = 0; i < num; i++)
 		list_add(&page[i]->lru, &ring->free_pages);
+	/*
+	 * 在以下修改xen_blkif_ring->free_pages_num:
+	 *   - drivers/block/xen-blkback/blkback.c|155| <<put_free_pages>> ring->free_pages_num += num;
+	 *   - drivers/block/xen-blkback/blkback.c|140| <<get_free_page>> ring->free_pages_num--;
+	 *   - drivers/block/xen-blkback/blkback.c|172| <<shrink_free_pagepool>> ring->free_pages_num--;
+	 */
 	ring->free_pages_num += num;
 	spin_unlock_irqrestore(&ring->free_pages_lock, flags);
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|700| <<xen_blkif_schedule>> shrink_free_pagepool(ring, xen_blkif_max_buffer_pages);
+ *   - drivers/block/xen-blkback/blkback.c|731| <<xen_blkbk_free_caches>> shrink_free_pagepool(ring, 0 );
+ */
 static inline void shrink_free_pagepool(struct xen_blkif_ring *ring, int num)
 {
 	/* Remove requested pages in batches of NUM_BATCH_FREE_PAGES */
@@ -164,6 +243,12 @@ static inline void shrink_free_pagepool(struct xen_blkif_ring *ring, int num)
 	unsigned long flags;
 
 	spin_lock_irqsave(&ring->free_pages_lock, flags);
+	/*
+	 * 在以下修改xen_blkif_ring->free_pages_num:
+	 *   - drivers/block/xen-blkback/blkback.c|155| <<put_free_pages>> ring->free_pages_num += num;
+	 *   - drivers/block/xen-blkback/blkback.c|140| <<get_free_page>> ring->free_pages_num--;
+	 *   - drivers/block/xen-blkback/blkback.c|172| <<shrink_free_pagepool>> ring->free_pages_num--;
+	 */
 	while (ring->free_pages_num > num) {
 		BUG_ON(list_empty(&ring->free_pages));
 		page[num_pages] = list_first_entry(&ring->free_pages,
@@ -191,6 +276,11 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 static void make_response(struct xen_blkif_ring *ring, u64 id,
 			  unsigned short op, int st);
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|366| <<free_persistent_gnts>> foreach_grant_safe(persistent_gnt, n, root, node) {
+ *   - drivers/block/xen-blkback/blkback.c|481| <<purge_persistent_gnt>> foreach_grant_safe(persistent_gnt, n, root, node) {
+ */
 #define foreach_grant_safe(pos, n, rbtree, node) \
 	for ((pos) = container_of(rb_first((rbtree)), typeof(*(pos)), node), \
 	     (n) = (&(pos)->node != NULL) ? rb_next(&(pos)->node) : NULL; \
@@ -209,6 +299,10 @@ static void make_response(struct xen_blkif_ring *ring, u64 id,
  * bit operations to modify the flags of a persistent grant and to count
  * the number of used grants.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1019| <<xen_blkbk_map>> if (add_persistent_gnt(ring, persistent_gnt)) {
+ */
 static int add_persistent_gnt(struct xen_blkif_ring *ring,
 			       struct persistent_gnt *persistent_gnt)
 {
@@ -247,6 +341,10 @@ static int add_persistent_gnt(struct xen_blkif_ring *ring,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|944| <<xen_blkbk_map>> persistent_gnt = get_persistent_gnt(ring, pages[i].gref);
+ */
 static struct persistent_gnt *get_persistent_gnt(struct xen_blkif_ring *ring,
 						 grant_ref_t gref)
 {
@@ -274,6 +372,10 @@ static struct persistent_gnt *get_persistent_gnt(struct xen_blkif_ring *ring,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|818| <<xen_blkbk_unmap_prepare>> put_persistent_gnt(ring, pages[i].persistent_gnt);
+ */
 static void put_persistent_gnt(struct xen_blkif_ring *ring,
                                struct persistent_gnt *persistent_gnt)
 {
@@ -284,6 +386,10 @@ static void put_persistent_gnt(struct xen_blkif_ring *ring,
 	atomic_dec(&ring->persistent_gnt_in_use);
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|797| <<xen_blkbk_free_caches>> free_persistent_gnts(ring, &ring->persistent_gnts,
+ */
 static void free_persistent_gnts(struct xen_blkif_ring *ring, struct rb_root *root,
                                  unsigned int num)
 {
@@ -326,6 +432,16 @@ static void free_persistent_gnts(struct xen_blkif_ring *ring, struct rb_root *ro
 	BUG_ON(num != 0);
 }
 
+/*
+ * 在以下使用xen_blkif_ring->persistent_purge_work:
+ *   - drivers/block/xen-blkback/blkback.c|484| <<purge_persistent_gnt>> if (work_busy(&ring->persistent_purge_work)) {
+ *   - drivers/block/xen-blkback/blkback.c|554| <<purge_persistent_gnt>> schedule_work(&ring->persistent_purge_work);
+ *   - drivers/block/xen-blkback/blkback.c|858| <<xen_blkif_schedule>> flush_work(&ring->persistent_purge_work);
+ *   - drivers/block/xen-blkback/xenbus.c|166| <<xen_blkif_alloc_rings>> INIT_WORK(&ring->persistent_purge_work, xen_blkbk_unmap_purged_grants);
+ *
+ * 在以下使用xen_blkbk_unmap_purged_grants():
+ *   - drivers/block/xen-blkback/xenbus.c|166| <<xen_blkif_alloc_rings>> INIT_WORK(&ring->persistent_purge_work, xen_blkbk_unmap_purged_grants);
+ */
 void xen_blkbk_unmap_purged_grants(struct work_struct *work)
 {
 	struct gnttab_unmap_grant_ref unmap[BLKIF_MAX_SEGMENTS_PER_REQUEST];
@@ -367,6 +483,10 @@ void xen_blkbk_unmap_purged_grants(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|695| <<xen_blkif_schedule>> purge_persistent_gnt(ring);
+ */
 static void purge_persistent_gnt(struct xen_blkif_ring *ring)
 {
 	struct persistent_gnt *persistent_gnt;
@@ -462,11 +582,36 @@ out:
  * For direct requests, retrieve a free entry from list 'pending_free'.
  * For indirect, allocate with the appropriate number of segements.
  */
+/*
+ * struct pending_req {
+ *     struct xen_blkif_ring   *ring;
+ *     u64                     id;
+ *     int                     nr_segs;
+ *     atomic_t                pendcnt;
+ *     unsigned short          operation;
+ *     int                     status;
+ *     struct list_head        free_list;
+ *     struct grant_page       *segments;
+ *     struct grant_page       *indirect_pages;
+ *     struct seg_buf          *seg;
+ *     struct bio              **biolist;
+ *     struct gnttab_unmap_grant_ref *unmap;
+ *     struct page             **unmap_pages;
+ *     struct gntab_unmap_queue_data gnttab_unmap_data;
+ * };
+ *
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1268| <<__do_block_io_op>> pending_req = alloc_req(ring, nsegs);
+ */
 static struct pending_req *alloc_req(struct xen_blkif_ring *ring, unsigned int nsegs)
 {
 	struct pending_req *req = NULL;
 	unsigned long flags;
 
+	/*
+	 * 如果是BLKIF_OP_INDIRECT : nsegs是req.u.indirect.nr_segments
+	 * 否则                    : nsegs是req.u.rw.nr_segments
+	 */
 	if (nsegs <= BLKIF_MAX_SEGMENTS_PER_REQUEST) {
 		spin_lock_irqsave(&ring->pending_free_lock, flags);
 		if (!list_empty(&ring->pending_free)) {
@@ -480,6 +625,10 @@ static struct pending_req *alloc_req(struct xen_blkif_ring *ring, unsigned int n
 
 		ring->st_req_direct++;
 	} else {
+		/*
+		 * drivers/block/xen-blkback/blkback.c|556| <<alloc_req>> req = xen_blkbk_alloc_req(nsegs, true );
+		 *   - drivers/block/xen-blkback/xenbus.c|1183| <<read_per_ring_refs>> req = xen_blkbk_alloc_req(BLKIF_MAX_SEGMENTS_PER_REQUEST, false );
+		 */
 		req = xen_blkbk_alloc_req(nsegs, true /* indirect req */);
 		if (!req)
 			ring->st_oo_req_indirect++;
@@ -493,6 +642,11 @@ static struct pending_req *alloc_req(struct xen_blkif_ring *ring, unsigned int n
  * Return the 'pending_req' structure back to the freepool. We also
  * wake up the thread if it was waiting for a free page.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|846| <<xen_blkbk_unmap_and_respond_callback>> free_req(ring, pending_req);
+ *   - drivers/block/xen-blkback/blkback.c|1581| <<dispatch_rw_block_io>> free_req(ring, pending_req);
+ */
 static void free_req(struct xen_blkif_ring *ring, struct pending_req *req)
 {
 	unsigned long flags;
@@ -514,6 +668,11 @@ static void free_req(struct xen_blkif_ring *ring, struct pending_req *req)
 /*
  * Routines for managing virtual block devices (vbds).
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1138| <<dispatch_discard_io>> err = xen_vbd_translate(&preq, blkif, WRITE);
+ *   - drivers/block/xen-blkback/blkback.c|1480| <<dispatch_rw_block_io>> if (xen_vbd_translate(&preq, ring->blkif, operation) != 0) {
+ */
 static int xen_vbd_translate(struct phys_req *req, struct xen_blkif *blkif,
 			     int operation)
 {
@@ -544,6 +703,10 @@ static int xen_vbd_translate(struct phys_req *req, struct xen_blkif *blkif,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|730| <<xen_blkif_schedule>> xen_vbd_resize(blkif);
+ */
 static void xen_vbd_resize(struct xen_blkif *blkif)
 {
 	struct xen_vbd *vbd;
@@ -603,12 +766,20 @@ abort:
 /*
  * Notification from the guest OS.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|736| <<xen_blkif_be_int>> blkif_notify_work(dev_id);
+ */
 static void blkif_notify_work(struct xen_blkif_ring *ring)
 {
 	ring->waiting_reqs = 1;
 	wake_up(&ring->wq);
 }
 
+/*
+ * 在以下使用xen_blkif_be_int():
+ *   - drivers/block/xen-blkback/xenbus.c|245| <<xen_blkif_map>> err = bind_interdomain_evtchn_to_irqhandler_lateeoi(blkif->domid, evtchn, xen_blkif_be_int, 0, "blkif-backend", ring);
+ */
 irqreturn_t xen_blkif_be_int(int irq, void *dev_id)
 {
 	blkif_notify_work(dev_id);
@@ -717,6 +888,10 @@ purge_gnt_list:
 /*
  * Remove persistent grants and empty the pool of free pages
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/xenbus.c|295| <<xen_blkif_disconnect>> xen_blkbk_free_caches(ring);
+ */
 void xen_blkbk_free_caches(struct xen_blkif_ring *ring)
 {
 	/* Free all persistent grant pages */
@@ -731,6 +906,11 @@ void xen_blkbk_free_caches(struct xen_blkif_ring *ring)
 	shrink_free_pagepool(ring, 0 /* All */);
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|872| <<xen_blkbk_unmap_and_respond>> invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs, req->unmap, req->unmap_pages);
+ *   - drivers/block/xen-blkback/blkback.c|905| <<xen_blkbk_unmap>> invcount = xen_blkbk_unmap_prepare(ring, pages, batch, unmap, unmap_pages);
+ */
 static unsigned int xen_blkbk_unmap_prepare(
 	struct xen_blkif_ring *ring,
 	struct grant_page *pages,
@@ -751,12 +931,30 @@ static unsigned int xen_blkbk_unmap_prepare(
 		gnttab_set_unmap_op(&unmap_ops[invcount], vaddr(pages[i].page),
 				    GNTMAP_host_map, pages[i].handle);
 		pages[i].handle = BLKBACK_INVALID_HANDLE;
+		/*
+		 * invcount只在这里修改
+		 * 只对不是persistent的增加(真的需要unmap)
+		 */
 		invcount++;
        }
 
        return invcount;
 }
 
+/*
+ * 1020 invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs,
+ * 1021                                            req->unmap, req->unmap_pages);
+ * 1022
+ * 1023         work->data = req;
+ * 1024         work->done = xen_blkbk_unmap_and_respond_callback;
+ * 1025         work->unmap_ops = req->unmap;
+ * 1026         work->kunmap_ops = NULL;
+ * 1027         work->pages = req->unmap_pages;
+ * 1028         work->count = invcount;
+ *
+ * 在以下使用xen_blkbk_unmap_and_respond_callback():
+ *   - drivers/block/xen-blkback/blkback.c|876| <<xen_blkbk_unmap_and_respond>> work->done = xen_blkbk_unmap_and_respond_callback;
+ */
 static void xen_blkbk_unmap_and_respond_callback(int result, struct gntab_unmap_queue_data *data)
 {
 	struct pending_req *pending_req = (struct pending_req *)(data->data);
@@ -770,6 +968,11 @@ static void xen_blkbk_unmap_and_respond_callback(int result, struct gntab_unmap_
 	put_free_pages(ring, data->pages, data->count);
 	make_response(ring, pending_req->id,
 		      pending_req->operation, pending_req->status);
+	/*
+	 * called by:
+	 *   - drivers/block/xen-blkback/blkback.c|846| <<xen_blkbk_unmap_and_respond_callback>> free_req(ring, pending_req);
+	 *   - drivers/block/xen-blkback/blkback.c|1581| <<dispatch_rw_block_io>> free_req(ring, pending_req);
+	 */
 	free_req(ring, pending_req);
 	/*
 	 * Make sure the request is freed before releasing blkif,
@@ -789,13 +992,46 @@ static void xen_blkbk_unmap_and_respond_callback(int result, struct gntab_unmap_
 	xen_blkif_put(blkif);
 }
 
+/*
+ * struct pending_req {
+ *     struct xen_blkif_ring   *ring;
+ *     u64                     id;
+ *     int                     nr_segs;
+ *     atomic_t                pendcnt;
+ *     unsigned short          operation;
+ *     int                     status;
+ *     struct list_head        free_list;
+ *     struct grant_page       *segments;
+ *     struct grant_page       *indirect_pages;
+ *     struct seg_buf          *seg;
+ *     struct bio              **biolist;
+ *     struct gnttab_unmap_grant_ref *unmap;
+ *     struct page             **unmap_pages;
+ *     struct gntab_unmap_queue_data gnttab_unmap_data;
+ *
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1220| <<__end_block_io_op>> xen_blkbk_unmap_and_respond(pending_req);
+ */
 static void xen_blkbk_unmap_and_respond(struct pending_req *req)
 {
+	/*
+	 * 在以下使用pending_req->gnttab_unmap_data:
+	 *   - drivers/block/xen-blkback/blkback.c|982| <<xen_blkbk_unmap_and_respond>> struct gntab_unmap_queue_data* work = &req->gnttab_unmap_data;
+	 *   - drivers/block/xen-blkback/blkback.c|997| <<xen_blkbk_unmap_and_respond>> gnttab_unmap_refs_async(&req->gnttab_unmap_data);
+	 */
 	struct gntab_unmap_queue_data* work = &req->gnttab_unmap_data;
 	struct xen_blkif_ring *ring = req->ring;
 	struct grant_page *pages = req->segments;
 	unsigned int invcount;
 
+	/*
+	 * 在以下使用pending_req->unmap_pages:
+	 *   - drivers/block/xen-blkback/blkback.c|988| <<xen_blkbk_unmap_and_respond>> invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs, req->unmap, req->unmap_pages);
+	 *   - drivers/block/xen-blkback/blkback.c|994| <<xen_blkbk_unmap_and_respond>> work->pages = req->unmap_pages;
+	 *   - drivers/block/xen-blkback/xenbus.c|1088| <<xen_blkbk_free_req>> kfree(req->unmap_pages);
+	 *   - drivers/block/xen-blkback/xenbus.c|1115| <<xen_blkbk_alloc_req>> req->unmap_pages = kzalloc(nseg * sizeof(*req->unmap_pages), GFP_KERNEL);
+	 *   - drivers/block/xen-blkback/xenbus.c|1136| <<xen_blkbk_alloc_req>> !req->unmap_pages || (indirect && !req->indirect_pages)) {
+	 */
 	invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs,
 					   req->unmap, req->unmap_pages);
 
@@ -817,6 +1053,11 @@ static void xen_blkbk_unmap_and_respond(struct pending_req *req)
  * of hypercalls, but since this is only used in error paths there's
  * no real need.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1119| <<xen_blkbk_parse_indirect>> xen_blkbk_unmap(ring, pages, indirect_grefs);
+ *   - drivers/block/xen-blkback/blkback.c|1576| <<dispatch_rw_block_io>> xen_blkbk_unmap(ring, pending_req->segments,
+ */
 static void xen_blkbk_unmap(struct xen_blkif_ring *ring,
 			    struct grant_page *pages,
                             int num)
@@ -827,8 +1068,18 @@ static void xen_blkbk_unmap(struct xen_blkif_ring *ring,
 	int ret;
 
 	while (num) {
+		/*
+		 * 一次最多unmap 11个
+		 */
 		unsigned int batch = min(num, BLKIF_MAX_SEGMENTS_PER_REQUEST);
 
+		/*
+		 * called by:
+		 *   - drivers/block/xen-blkback/blkback.c|872| <<xen_blkbk_unmap_and_respond>> invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs, req->unmap, req->unmap_pages);
+		 *   - drivers/block/xen-blkback/blkback.c|905| <<xen_blkbk_unmap>> invcount = xen_blkbk_unmap_prepare(ring, pages, batch, unmap, unmap_pages);
+		 *
+		 * invcount只对不是persistent的增加(真的需要unmap)
+		 */
 		invcount = xen_blkbk_unmap_prepare(ring, pages, batch,
 						   unmap, unmap_pages);
 		if (invcount) {
@@ -836,11 +1087,20 @@ static void xen_blkbk_unmap(struct xen_blkif_ring *ring,
 			BUG_ON(ret);
 			put_free_pages(ring, unmap_pages, invcount);
 		}
+		/*
+		 * pages来自函数的参数
+		 * struct grant_page *pages,
+		 */
 		pages += batch;
 		num -= batch;
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1063| <<xen_blkbk_map_seg>> rc = xen_blkbk_map(pending_req->ring, pending_req->segments, pending_req->nr_segs, (pending_req->operation != BLKIF_OP_READ));
+ *   - drivers/block/xen-blkback/blkback.c|1087| <<xen_blkbk_parse_indirect>> rc = xen_blkbk_map(ring, pages, indirect_grefs, true);
+ */
 static int xen_blkbk_map(struct xen_blkif_ring *ring,
 			 struct grant_page *pages,
 			 int num, bool ro)
@@ -868,6 +1128,9 @@ again:
 		uint32_t flags;
 
 		if (use_persistent_gnts) {
+			/*
+			 * struct persistent_gnt *persistent_gnt = NULL;
+			 */
 			persistent_gnt = get_persistent_gnt(
 				ring,
 				pages[i].gref);
@@ -878,7 +1141,25 @@ again:
 			 * We are using persistent grants and
 			 * the grant is already mapped
 			 */
+			/*
+			 * struct grant_page {
+			 *     struct page             *page;
+			 *     struct persistent_gnt   *persistent_gnt;
+			 *     grant_handle_t          handle;
+			 *     grant_ref_t             gref;
+			 * };
+			 */
 			pages[i].page = persistent_gnt->page;
+			/*
+			 * 在以下使用grant_page->persistent_gnt:
+			 *   - drivers/block/xen-blkback/blkback.c|904| <<xen_blkbk_unmap_prepare>> if (pages[i].persistent_gnt != NULL) {
+			 *   - drivers/block/xen-blkback/blkback.c|905| <<xen_blkbk_unmap_prepare>> put_persistent_gnt(ring, pages[i].persistent_gnt);
+			 *   - drivers/block/xen-blkback/blkback.c|1060| <<xen_blkbk_map>> pages[i].persistent_gnt = persistent_gnt;
+			 *   - drivers/block/xen-blkback/blkback.c|1069| <<xen_blkbk_map>> pages[i].persistent_gnt = NULL;
+			 *   - drivers/block/xen-blkback/blkback.c|1091| <<xen_blkbk_map>> if (!pages[seg_idx].persistent_gnt) {
+			 *   - drivers/block/xen-blkback/blkback.c|1130| <<xen_blkbk_map>> pages[seg_idx].persistent_gnt = persistent_gnt;
+			 *   - drivers/block/xen-blkback/blkback.c|1157| <<xen_blkbk_map>> pages[i].persistent_gnt = NULL;
+			 */
 			pages[i].persistent_gnt = persistent_gnt;
 		} else {
 			if (get_free_page(ring, &pages[i].page)) {
@@ -886,12 +1167,28 @@ again:
 				ret = -ENOMEM;
 				goto out;
 			}
+			/*
+			 * addr是上面get_free_page()分配到的
+			 */
 			addr = vaddr(pages[i].page);
 			pages_to_gnt[segs_to_map] = pages[i].page;
+			/*
+			 * 在以下使用grant_page->persistent_gnt:
+			 *   - drivers/block/xen-blkback/blkback.c|904| <<xen_blkbk_unmap_prepare>> if (pages[i].persistent_gnt != NULL) {
+			 *   - drivers/block/xen-blkback/blkback.c|905| <<xen_blkbk_unmap_prepare>> put_persistent_gnt(ring, pages[i].persistent_gnt);
+			 *   - drivers/block/xen-blkback/blkback.c|1060| <<xen_blkbk_map>> pages[i].persistent_gnt = persistent_gnt;
+			 *   - drivers/block/xen-blkback/blkback.c|1069| <<xen_blkbk_map>> pages[i].persistent_gnt = NULL;
+			 *   - drivers/block/xen-blkback/blkback.c|1091| <<xen_blkbk_map>> if (!pages[seg_idx].persistent_gnt) {
+			 *   - drivers/block/xen-blkback/blkback.c|1130| <<xen_blkbk_map>> pages[seg_idx].persistent_gnt = persistent_gnt;
+			 *   - drivers/block/xen-blkback/blkback.c|1157| <<xen_blkbk_map>> pages[i].persistent_gnt = NULL;
+			 */
 			pages[i].persistent_gnt = NULL;
 			flags = GNTMAP_host_map;
 			if (!use_persistent_gnts && ro)
 				flags |= GNTMAP_readonly;
+			/*
+			 * struct gnttab_map_grant_ref map[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+			 */
 			gnttab_set_map_op(&map[segs_to_map++], addr,
 					  flags, pages[i].gref,
 					  blkif->domid);
@@ -943,6 +1240,10 @@ again:
 			persistent_gnt->gnt = map[new_map_idx].ref;
 			persistent_gnt->handle = map[new_map_idx].handle;
 			persistent_gnt->page = pages[seg_idx].page;
+			/*
+			 * called by:
+			 *   - drivers/block/xen-blkback/blkback.c|1019| <<xen_blkbk_map>> if (add_persistent_gnt(ring, persistent_gnt)) {
+			 */
 			if (add_persistent_gnt(ring,
 			                       persistent_gnt)) {
 				kfree(persistent_gnt);
@@ -969,6 +1270,9 @@ next:
 	}
 	segs_to_map = 0;
 	last_map = map_until;
+	/*
+	 * 如果没有map完, 再回去!!!
+	 */
 	if (!ret && map_until != num)
 		goto again;
 
@@ -983,10 +1287,35 @@ out:
 	return ret;
 }
 
+/*
+ * struct pending_req {
+ *     struct xen_blkif_ring   *ring;
+ *     u64                     id;
+ *     int                     nr_segs;
+ *     atomic_t                pendcnt;
+ *     unsigned short          operation;
+ *     int                     status;
+ *     struct list_head        free_list;
+ *     struct grant_page       *segments;
+ *     struct grant_page       *indirect_pages;
+ *     struct seg_buf          *seg;
+ *     struct bio              **biolist;
+ *     struct gnttab_unmap_grant_ref *unmap;
+ *     struct page             **unmap_pages;
+ *     struct gntab_unmap_queue_data gnttab_unmap_data;
+ *
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1606| <<dispatch_rw_block_io>> if (xen_blkbk_map_seg(pending_req))
+ */
 static int xen_blkbk_map_seg(struct pending_req *pending_req)
 {
 	int rc;
 
+	/*
+	 * called by:
+	 *   - drivers/block/xen-blkback/blkback.c|1063| <<xen_blkbk_map_seg>> rc = xen_blkbk_map(pending_req->ring, pending_req->segments, pending_req->nr_segs, (pending_req->operation != BLKIF_OP_READ));
+	 *   - drivers/block/xen-blkback/blkback.c|1087| <<xen_blkbk_parse_indirect>> rc = xen_blkbk_map(ring, pages, indirect_grefs, true);
+	 */
 	rc = xen_blkbk_map(pending_req->ring, pending_req->segments,
 			   pending_req->nr_segs,
 	                   (pending_req->operation != BLKIF_OP_READ));
@@ -994,6 +1323,10 @@ static int xen_blkbk_map_seg(struct pending_req *pending_req)
 	return rc;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1568| <<dispatch_rw_block_io>> if (xen_blkbk_parse_indirect(req, pending_req, seg, &preq))
+ */
 static int xen_blkbk_parse_indirect(struct blkif_request *req,
 				    struct pending_req *pending_req,
 				    struct seg_buf *seg,
@@ -1008,6 +1341,9 @@ static int xen_blkbk_parse_indirect(struct blkif_request *req,
 	indirect_grefs = INDIRECT_PAGES(nseg);
 	BUG_ON(indirect_grefs > BLKIF_MAX_INDIRECT_PAGES_PER_REQUEST);
 
+	/*
+	 * struct grant_page *pages = pending_req->indirect_pages;
+	 */
 	for (i = 0; i < indirect_grefs; i++)
 		pages[i].gref = req->u.indirect.indirect_grefs[i];
 
@@ -1047,6 +1383,10 @@ unmap:
 	return rc;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1458| <<__do_block_io_op>> if (dispatch_discard_io(ring, &req))
+ */
 static int dispatch_discard_io(struct xen_blkif_ring *ring,
 				struct blkif_request *req)
 {
@@ -1085,19 +1425,42 @@ fail_response:
 	} else if (err)
 		status = BLKIF_RSP_ERROR;
 
+	/*
+	 * called by:
+	 *   - drivers/block/xen-blkback/blkback.c|906| <<xen_blkbk_unmap_and_respond_callback>> make_response(ring, pending_req->id, pending_req->operation, pending_req->status);
+	 *   - drivers/block/xen-blkback/blkback.c|1237| <<dispatch_discard_io>> make_response(ring, req->u.discard.id, req->operation, status);
+	 *   - drivers/block/xen-blkback/blkback.c|1245| <<dispatch_other_io>> make_response(ring, req->u.other.id, req->operation, BLKIF_RSP_EOPNOTSUPP);
+	 *   - drivers/block/xen-blkback/blkback.c|1656| <<dispatch_rw_block_io>> make_response(ring, req->u.rw.id, req_operation, BLKIF_RSP_ERROR);
+	 */
 	make_response(ring, req->u.discard.id, req->operation, status);
 	xen_blkif_put(blkif);
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1444| <<__do_block_io_op>> if (dispatch_other_io(ring, &req))
+ *   - drivers/block/xen-blkback/blkback.c|1462| <<__do_block_io_op>> if (dispatch_other_io(ring, &req))
+ */
 static int dispatch_other_io(struct xen_blkif_ring *ring,
 			     struct blkif_request *req)
 {
+	/*
+	 * called by:
+	 *   - drivers/block/xen-blkback/blkback.c|906| <<xen_blkbk_unmap_and_respond_callback>> make_response(ring, pending_req->id, pending_req->operation, pending_req->status);
+	 *   - drivers/block/xen-blkback/blkback.c|1237| <<dispatch_discard_io>> make_response(ring, req->u.discard.id, req->operation, status);
+	 *   - drivers/block/xen-blkback/blkback.c|1245| <<dispatch_other_io>> make_response(ring, req->u.other.id, req->operation, BLKIF_RSP_EOPNOTSUPP);
+	 *   - drivers/block/xen-blkback/blkback.c|1656| <<dispatch_rw_block_io>> make_response(ring, req->u.rw.id, req_operation, BLKIF_RSP_ERROR);
+	 */
 	make_response(ring, req->u.other.id, req->operation,
 		      BLKIF_RSP_EOPNOTSUPP);
 	return -EIO;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1598| <<dispatch_rw_block_io>> xen_blk_drain_io(pending_req->ring);
+ */
 static void xen_blk_drain_io(struct xen_blkif_ring *ring)
 {
 	struct xen_blkif *blkif = ring->blkif;
@@ -1119,6 +1482,11 @@ static void xen_blk_drain_io(struct xen_blkif_ring *ring)
  * Completion callback on the bio's. Called as bh->b_end_io()
  */
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1304| <<end_block_io_op>> __end_block_io_op(bio->bi_private, error);
+ *   - drivers/block/xen-blkback/blkback.c|1681| <<dispatch_rw_block_io>> __end_block_io_op(pending_req, -EINVAL);
+ */
 static void __end_block_io_op(struct pending_req *pending_req, int error)
 {
 	/* An error fails the entire request. */
@@ -1143,6 +1511,12 @@ static void __end_block_io_op(struct pending_req *pending_req, int error)
 	 * the grant references associated with 'request' and provide
 	 * the proper response on the ring.
 	 */
+	/*
+	 * 在以下使用pending_req->pendcnt:
+	 *   - drivers/block/xen-blkback/blkback.c|1436| <<__end_block_io_op>> if (atomic_dec_and_test(&pending_req->pendcnt))
+	 *   - drivers/block/xen-blkback/blkback.c|1845| <<dispatch_rw_block_io>> atomic_set(&pending_req->pendcnt, nbio);
+	 *   - drivers/block/xen-blkback/blkback.c|1874| <<dispatch_rw_block_io>> atomic_set(&pending_req->pendcnt, 1);
+	 */
 	if (atomic_dec_and_test(&pending_req->pendcnt))
 		xen_blkbk_unmap_and_respond(pending_req);
 }
@@ -1150,12 +1524,21 @@ static void __end_block_io_op(struct pending_req *pending_req, int error)
 /*
  * bio callback.
  */
+/*
+ * 在以下使用end_block_io_op():
+ *   - drivers/block/xen-blkback/blkback.c|1630| <<dispatch_rw_block_io>> bio->bi_end_io = end_block_io_op;
+ *   - drivers/block/xen-blkback/blkback.c|1648| <<dispatch_rw_block_io>> bio->bi_end_io = end_block_io_op;
+ */
 static void end_block_io_op(struct bio *bio, int error)
 {
 	__end_block_io_op(bio->bi_private, error);
 	bio_put(bio);
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1414| <<__do_block_io_op>> valid_req = validate_io_op(&req, nsegs);
+ */
 static bool validate_io_op(const struct blkif_request *req, unsigned int nseg)
 {
 	unsigned short req_operation = req->operation == BLKIF_OP_INDIRECT ?
@@ -1210,6 +1593,10 @@ fail:
  * (which has the sectors we want, number of them, grant references, etc),
  * and transmute  it to the block API to hand it over to the proper block disk.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1473| <<do_block_io_op>> more_to_do = __do_block_io_op(ring, eoi_flags);
+ */
 static int
 __do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)
 {
@@ -1258,6 +1645,10 @@ __do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)
 			BUG();
 		}
 
+		/*
+		 * 如果是BLKIF_OP_INDIRECT : nsegs是req.u.indirect.nr_segments
+		 * 否则                    : nsegs是req.u.rw.nr_segments
+		 */
 		nsegs = req.operation == BLKIF_OP_INDIRECT ?
 			req.u.indirect.nr_segments : req.u.rw.nr_segments;
 
@@ -1265,6 +1656,9 @@ __do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)
 		valid_req = validate_io_op(&req, nsegs);
 
 		if (valid_req && req.operation != BLKIF_OP_DISCARD) {
+			/*
+			 * 只在此处调用alloc_req()
+			 */
 			pending_req = alloc_req(ring, nsegs);
 			if (pending_req == NULL) {
 				/* Get out before updating req_cons */
@@ -1314,6 +1708,10 @@ done:
 	return more_to_do;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|802| <<xen_blkif_schedule>> ret = do_block_io_op(ring, &eoi_flags);
+ */
 static int
 do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)
 {
@@ -1334,6 +1732,26 @@ do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)
  * Transmutation of the 'struct blkif_request' to a proper 'struct bio'
  * and call the 'submit_bio' to pass it to the underlying storage.
  */
+/*
+ * struct pending_req {
+ *     struct xen_blkif_ring   *ring;
+ *     u64                     id;
+ *     int                     nr_segs;
+ *     atomic_t                pendcnt;
+ *     unsigned short          operation;
+ *     int                     status;
+ *     struct list_head        free_list;
+ *     struct grant_page       *segments;
+ *     struct grant_page       *indirect_pages;
+ *     struct seg_buf          *seg;
+ *     struct bio              **biolist;
+ *     struct gnttab_unmap_grant_ref *unmap;
+ *     struct page             **unmap_pages;
+ *     struct gntab_unmap_queue_data gnttab_unmap_data;
+ *
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1446| <<__do_block_io_op>> if (dispatch_rw_block_io(ring, &req, pending_req))
+ */
 static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 				struct blkif_request *req,
 				struct pending_req *pending_req)
@@ -1375,6 +1793,20 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 	nseg = req->operation == BLKIF_OP_INDIRECT ?
 	       req->u.indirect.nr_segments : req->u.rw.nr_segments;
 
+	/*
+	 * 在以下设置phys_req->nr_sects:
+	 *   - drivers/block/xen-blkback/blkback.c|1253| <<dispatch_discard_io>> preq.nr_sects = req->u.discard.nr_sectors;
+	 *   - drivers/block/xen-blkback/blkback.c|1226| <<xen_blkbk_parse_indirect>> preq->nr_sects += seg[n].nsec;
+	 *   - drivers/block/xen-blkback/blkback.c|1624| <<dispatch_rw_block_io>> preq.nr_sects = 0;
+	 *   - drivers/block/xen-blkback/blkback.c|1644| <<dispatch_rw_block_io>> preq.nr_sects += seg[i].nsec;
+	 * 在以下使用phys_req->nr_sects:
+	 *   - drivers/block/xen-blkback/blkback.c|665| <<xen_vbd_translate>> if (likely(req->nr_sects)) {
+	 *   - drivers/block/xen-blkback/blkback.c|666| <<xen_vbd_translate>> blkif_sector_t end = req->sector_number + req->nr_sects;
+	 *   - drivers/block/xen-blkback/blkback.c|1259| <<dispatch_discard_io>> preq.sector_number + preq.nr_sects, blkif->vbd.pdevice);
+	 *   - drivers/block/xen-blkback/blkback.c|1657| <<dispatch_rw_block_io>> preq.sector_number + preq.nr_sects,
+	 *   - drivers/block/xen-blkback/blkback.c|1742| <<dispatch_rw_block_io>> ring->st_rd_sect += preq.nr_sects;
+	 *   - drivers/block/xen-blkback/blkback.c|1744| <<dispatch_rw_block_io>> ring->st_wr_sect += preq.nr_sects;
+	 */
 	preq.nr_sects      = 0;
 
 	pending_req->ring      = ring;
@@ -1387,7 +1819,13 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 		preq.dev               = req->u.rw.handle;
 		preq.sector_number     = req->u.rw.sector_number;
 		for (i = 0; i < nseg; i++) {
+			/*
+			 * struct grant_page *pages = pending_req->segments;
+			 */
 			pages[i].gref = req->u.rw.seg[i].gref;
+			/*
+			 * struct seg_buf *seg = pending_req->seg;
+			 */
 			seg[i].nsec = req->u.rw.seg[i].last_sect -
 				req->u.rw.seg[i].first_sect + 1;
 			seg[i].offset = (req->u.rw.seg[i].first_sect << 9);
@@ -1400,6 +1838,9 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 	} else {
 		preq.dev               = req->u.indirect.handle;
 		preq.sector_number     = req->u.indirect.sector_number;
+		/*
+		 * struct seg_buf *seg = pending_req->seg;
+		 */
 		if (xen_blkbk_parse_indirect(req, pending_req, seg, &preq))
 			goto fail_response;
 	}
@@ -1449,6 +1890,11 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 	atomic_inc(&ring->inflight);
 
 	for (i = 0; i < nseg; i++) {
+		/*
+		 * struct grant_page *pages = pending_req->segments;
+		 *
+		 * struct seg_buf *seg = pending_req->seg;
+		 */
 		while ((bio == NULL) ||
 		       (bio_add_page(bio, pages[i].page,
 				     seg[i].nsec << 9,
@@ -1523,6 +1969,13 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 /*
  * Put a response on the ring on how the operation fared.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|906| <<xen_blkbk_unmap_and_respond_callback>> make_response(ring, pending_req->id, pending_req->operation, pending_req->status);
+ *   - drivers/block/xen-blkback/blkback.c|1237| <<dispatch_discard_io>> make_response(ring, req->u.discard.id, req->operation, status);
+ *   - drivers/block/xen-blkback/blkback.c|1245| <<dispatch_other_io>> make_response(ring, req->u.other.id, req->operation, BLKIF_RSP_EOPNOTSUPP);
+ *   - drivers/block/xen-blkback/blkback.c|1656| <<dispatch_rw_block_io>> make_response(ring, req->u.rw.id, req_operation, BLKIF_RSP_ERROR);
+ */
 static void make_response(struct xen_blkif_ring *ring, u64 id,
 			  unsigned short op, int st)
 {
@@ -1533,6 +1986,12 @@ static void make_response(struct xen_blkif_ring *ring, u64 id,
 
 	up_read(&ring->blkif->vbd_lock);
 
+	/*
+	 * 在以下使用xen_blkif_ring->blk_ring_lock:
+	 *   - drivers/block/xen-blkback/blkback.c|1708| <<make_response>> spin_lock_irqsave(&ring->blk_ring_lock, flags);
+	 *   - drivers/block/xen-blkback/blkback.c|1734| <<make_response>> spin_unlock_irqrestore(&ring->blk_ring_lock, flags);
+	 *   - drivers/block/xen-blkback/xenbus.c|162| <<xen_blkif_alloc_rings>> spin_lock_init(&ring->blk_ring_lock);
+	 */
 	spin_lock_irqsave(&ring->blk_ring_lock, flags);
 	blk_rings = &ring->blk_rings;
 	/* Place on the response ring for the relevant domain. */
diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 7a7eac63a3d1..ae2017e41ad7 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -260,6 +260,12 @@ struct xen_blkif_ring {
 	union blkif_back_rings	blk_rings;
 	void			*blk_ring;
 	/* Private fields. */
+	/*
+	 * 在以下使用xen_blkif_ring->blk_ring_lock:
+	 *   - drivers/block/xen-blkback/blkback.c|1708| <<make_response>> spin_lock_irqsave(&ring->blk_ring_lock, flags);
+	 *   - drivers/block/xen-blkback/blkback.c|1734| <<make_response>> spin_unlock_irqrestore(&ring->blk_ring_lock, flags);
+	 *   - drivers/block/xen-blkback/xenbus.c|162| <<xen_blkif_alloc_rings>> spin_lock_init(&ring->blk_ring_lock);
+	 */
 	spinlock_t		blk_ring_lock;
 
 	wait_queue_head_t	wq;
@@ -297,10 +303,28 @@ struct xen_blkif_ring {
 
 	/* Used by the kworker that offload work from the persistent purge. */
 	struct list_head	persistent_purge_list;
+	/*
+	 * 在以下使用xen_blkif_ring->persistent_purge_work:
+	 *   - drivers/block/xen-blkback/blkback.c|484| <<purge_persistent_gnt>> if (work_busy(&ring->persistent_purge_work)) {
+	 *   - drivers/block/xen-blkback/blkback.c|554| <<purge_persistent_gnt>> schedule_work(&ring->persistent_purge_work);
+	 *   - drivers/block/xen-blkback/blkback.c|858| <<xen_blkif_schedule>> flush_work(&ring->persistent_purge_work);
+	 *   - drivers/block/xen-blkback/xenbus.c|166| <<xen_blkif_alloc_rings>> INIT_WORK(&ring->persistent_purge_work, xen_blkbk_unmap_purged_grants);
+	 */
 	struct work_struct	persistent_purge_work;
 
 	/* Buffer of free pages to map grant refs. */
 	spinlock_t		free_pages_lock;
+	/*
+	 * 在以下修改xen_blkif_ring->free_pages_num:
+	 *   - drivers/block/xen-blkback/blkback.c|155| <<put_free_pages>> ring->free_pages_num += num;
+	 *   - drivers/block/xen-blkback/blkback.c|140| <<get_free_page>> ring->free_pages_num--;
+	 *   - drivers/block/xen-blkback/blkback.c|172| <<shrink_free_pagepool>> ring->free_pages_num--;
+	 * 在以下使用xen_blkif_ring->free_pages_num:
+	 *   - drivers/block/xen-blkback/blkback.c|133| <<get_free_page>> BUG_ON(ring->free_pages_num != 0);
+	 *   - drivers/block/xen-blkback/blkback.c|137| <<get_free_page>> BUG_ON(ring->free_pages_num == 0);
+	 *   - drivers/block/xen-blkback/blkback.c|167| <<shrink_free_pagepool>> while (ring->free_pages_num > num) {
+	 *   - drivers/block/xen-blkback/xenbus.c|309| <<xen_blkif_disconnect>> BUG_ON(ring->free_pages_num != 0);
+	 */
 	int			free_pages_num;
 	struct list_head	free_pages;
 
@@ -356,6 +380,16 @@ struct seg_buf {
 
 struct grant_page {
 	struct page 		*page;
+	/*
+	 * 在以下使用grant_page->persistent_gnt:
+	 *   - drivers/block/xen-blkback/blkback.c|904| <<xen_blkbk_unmap_prepare>> if (pages[i].persistent_gnt != NULL) {
+	 *   - drivers/block/xen-blkback/blkback.c|905| <<xen_blkbk_unmap_prepare>> put_persistent_gnt(ring, pages[i].persistent_gnt);
+	 *   - drivers/block/xen-blkback/blkback.c|1060| <<xen_blkbk_map>> pages[i].persistent_gnt = persistent_gnt;
+	 *   - drivers/block/xen-blkback/blkback.c|1069| <<xen_blkbk_map>> pages[i].persistent_gnt = NULL;
+	 *   - drivers/block/xen-blkback/blkback.c|1091| <<xen_blkbk_map>> if (!pages[seg_idx].persistent_gnt) {
+	 *   - drivers/block/xen-blkback/blkback.c|1130| <<xen_blkbk_map>> pages[seg_idx].persistent_gnt = persistent_gnt;
+	 *   - drivers/block/xen-blkback/blkback.c|1157| <<xen_blkbk_map>> pages[i].persistent_gnt = NULL;
+	 */
 	struct persistent_gnt	*persistent_gnt;
 	grant_handle_t		handle;
 	grant_ref_t		gref;
@@ -370,18 +404,56 @@ struct grant_page {
 struct pending_req {
 	struct xen_blkif_ring   *ring;
 	u64			id;
+	/*
+	 * 在以下修改pending_req->nr_segs:
+	 *   - drivers/block/xen-blkback/blkback.c|1767| <<dispatch_rw_block_io>> pending_req->nr_segs = nseg;
+	 * 在以下使用pending_req->nr_segs:
+	 *   - drivers/block/xen-blkback/blkback.c|655| <<free_req>> if (likely(req->nr_segs <= BLKIF_MAX_SEGMENTS_PER_REQUEST)) {
+	 *   - drivers/block/xen-blkback/blkback.c|1000| <<xen_blkbk_unmap_and_respond>> invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs, req->unmap, req->unmap_pages);
+	 *   - drivers/block/xen-blkback/blkback.c|1271| <<xen_blkbk_map_seg>> rc = xen_blkbk_map(pending_req->ring, pending_req->segments, pending_req->nr_segs, (pending_req->operation != BLKIF_OP_READ));
+	 *   - drivers/block/xen-blkback/blkback.c|1291| <<xen_blkbk_parse_indirect>> nseg = pending_req->nr_segs;
+	 *   - drivers/block/xen-blkback/blkback.c|1901| <<dispatch_rw_block_io>> xen_blkbk_unmap(ring, pending_req->segments, pending_req->nr_segs);
+	 */
 	int			nr_segs;
+	/*
+	 * 在以下使用pending_req->pendcnt:
+	 *   - drivers/block/xen-blkback/blkback.c|1436| <<__end_block_io_op>> if (atomic_dec_and_test(&pending_req->pendcnt))
+	 *   - drivers/block/xen-blkback/blkback.c|1845| <<dispatch_rw_block_io>> atomic_set(&pending_req->pendcnt, nbio);
+	 *   - drivers/block/xen-blkback/blkback.c|1874| <<dispatch_rw_block_io>> atomic_set(&pending_req->pendcnt, 1);
+	 */
 	atomic_t		pendcnt;
 	unsigned short		operation;
 	int			status;
 	struct list_head	free_list;
 	struct grant_page	*segments;
 	/* Indirect descriptors */
+	/*
+	 * 在以下使用pending_req->indirect_pages:
+	 *   - drivers/block/xen-blkback/blkback.c|1151| <<xen_blkbk_parse_indirect>> struct grant_page *pages = pending_req->indirect_pages;
+	 *   - drivers/block/xen-blkback/xenbus.c|1079| <<xen_blkbk_free_req>> kfree(req->indirect_pages);
+	 *   - drivers/block/xen-blkback/xenbus.c|1112| <<xen_blkbk_alloc_req>> req->indirect_pages = kzalloc(INDIRECT_PAGES(nseg) *
+	 *   - drivers/block/xen-blkback/xenbus.c|1113| <<xen_blkbk_alloc_req>> sizeof(*req->indirect_pages),
+	 *   - drivers/block/xen-blkback/xenbus.c|1116| <<xen_blkbk_alloc_req>> req->indirect_pages = NULL;
+	 *   - drivers/block/xen-blkback/xenbus.c|1120| <<xen_blkbk_alloc_req>> !req->unmap_pages || (indirect && !req->indirect_pages)) {
+	 */
 	struct grant_page	*indirect_pages;
 	struct seg_buf		*seg;
 	struct bio		**biolist;
 	struct gnttab_unmap_grant_ref *unmap;
+	/*
+	 * 在以下使用pending_req->unmap_pages:
+	 *   - drivers/block/xen-blkback/blkback.c|988| <<xen_blkbk_unmap_and_respond>> invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs, req->unmap, req->unmap_pages);
+	 *   - drivers/block/xen-blkback/blkback.c|994| <<xen_blkbk_unmap_and_respond>> work->pages = req->unmap_pages;
+	 *   - drivers/block/xen-blkback/xenbus.c|1088| <<xen_blkbk_free_req>> kfree(req->unmap_pages);
+	 *   - drivers/block/xen-blkback/xenbus.c|1115| <<xen_blkbk_alloc_req>> req->unmap_pages = kzalloc(nseg * sizeof(*req->unmap_pages), GFP_KERNEL);
+	 *   - drivers/block/xen-blkback/xenbus.c|1136| <<xen_blkbk_alloc_req>> !req->unmap_pages || (indirect && !req->indirect_pages)) {
+	 */
 	struct page		**unmap_pages;
+	/*
+	 * 在以下使用pending_req->gnttab_unmap_data:
+	 *   - drivers/block/xen-blkback/blkback.c|982| <<xen_blkbk_unmap_and_respond>> struct gntab_unmap_queue_data* work = &req->gnttab_unmap_data;
+	 *   - drivers/block/xen-blkback/blkback.c|997| <<xen_blkbk_unmap_and_respond>> gnttab_unmap_refs_async(&req->gnttab_unmap_data);
+	 */
 	struct gntab_unmap_queue_data gnttab_unmap_data;
 };
 
@@ -399,6 +471,20 @@ struct pending_req {
 
 struct phys_req {
 	unsigned short		dev;
+	/*
+	 * 在以下设置phys_req->nr_sects:
+	 *   - drivers/block/xen-blkback/blkback.c|1253| <<dispatch_discard_io>> preq.nr_sects = req->u.discard.nr_sectors;
+	 *   - drivers/block/xen-blkback/blkback.c|1226| <<xen_blkbk_parse_indirect>> preq->nr_sects += seg[n].nsec;
+	 *   - drivers/block/xen-blkback/blkback.c|1624| <<dispatch_rw_block_io>> preq.nr_sects = 0;
+	 *   - drivers/block/xen-blkback/blkback.c|1644| <<dispatch_rw_block_io>> preq.nr_sects += seg[i].nsec;
+	 * 在以下使用phys_req->nr_sects:
+	 *   - drivers/block/xen-blkback/blkback.c|665| <<xen_vbd_translate>> if (likely(req->nr_sects)) {
+	 *   - drivers/block/xen-blkback/blkback.c|666| <<xen_vbd_translate>> blkif_sector_t end = req->sector_number + req->nr_sects;
+	 *   - drivers/block/xen-blkback/blkback.c|1259| <<dispatch_discard_io>> preq.sector_number + preq.nr_sects, blkif->vbd.pdevice);
+	 *   - drivers/block/xen-blkback/blkback.c|1657| <<dispatch_rw_block_io>> preq.sector_number + preq.nr_sects,
+	 *   - drivers/block/xen-blkback/blkback.c|1742| <<dispatch_rw_block_io>> ring->st_rd_sect += preq.nr_sects;
+	 *   - drivers/block/xen-blkback/blkback.c|1744| <<dispatch_rw_block_io>> ring->st_wr_sect += preq.nr_sects;
+	 */
 	blkif_sector_t		nr_sects;
 	struct block_device	*bdev;
 	blkif_sector_t		sector_number;
diff --git a/drivers/block/xen-blkback/xenbus.c b/drivers/block/xen-blkback/xenbus.c
index d88d08cdc756..5f42f7cd1373 100644
--- a/drivers/block/xen-blkback/xenbus.c
+++ b/drivers/block/xen-blkback/xenbus.c
@@ -1074,6 +1074,13 @@ again:
 	up_read(&be->blkif->vbd_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|644| <<free_req>> xen_blkbk_free_req(req);
+ *   - drivers/block/xen-blkback/xenbus.c|301| <<xen_blkif_disconnect>> xen_blkbk_free_req(req);
+ *   - drivers/block/xen-blkback/xenbus.c|1131| <<xen_blkbk_alloc_req>> xen_blkbk_free_req(req);
+ *   - drivers/block/xen-blkback/xenbus.c|1217| <<read_per_ring_refs>> xen_blkbk_free_req(req);
+ */
 void xen_blkbk_free_req(struct pending_req *req)
 {
 	kfree(req->indirect_pages);
@@ -1087,6 +1094,11 @@ void xen_blkbk_free_req(struct pending_req *req)
 	return;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|556| <<alloc_req>> req = xen_blkbk_alloc_req(nsegs, true );
+ *   - drivers/block/xen-blkback/xenbus.c|1183| <<read_per_ring_refs>> req = xen_blkbk_alloc_req(BLKIF_MAX_SEGMENTS_PER_REQUEST, false );
+ */
 struct pending_req *xen_blkbk_alloc_req(unsigned int nseg, bool indirect)
 {
 	struct pending_req *req;
@@ -1097,6 +1109,10 @@ struct pending_req *xen_blkbk_alloc_req(unsigned int nseg, bool indirect)
 	if (!req)
 		return NULL;
 
+	/*
+	 * kzalloc() ---> allocate memory. The memory is set to zero.
+	 * 都是0!
+	 */
 	req->seg = kzalloc(nseg * sizeof(*req->seg), GFP_KERNEL);
 	req->segments = kzalloc(nseg * sizeof(*req->segments), GFP_KERNEL);
 	req->biolist = kzalloc(nseg * sizeof(*req->biolist), GFP_KERNEL);
@@ -1104,6 +1120,15 @@ struct pending_req *xen_blkbk_alloc_req(unsigned int nseg, bool indirect)
 	req->unmap = kzalloc(nseg * sizeof(*req->unmap), GFP_KERNEL);
 
 	if (indirect) {
+		/*
+		 * 在以下使用pending_req->indirect_pages:
+		 *   - drivers/block/xen-blkback/blkback.c|1151| <<xen_blkbk_parse_indirect>> struct grant_page *pages = pending_req->indirect_pages;
+		 *   - drivers/block/xen-blkback/xenbus.c|1079| <<xen_blkbk_free_req>> kfree(req->indirect_pages);
+		 *   - drivers/block/xen-blkback/xenbus.c|1112| <<xen_blkbk_alloc_req>> req->indirect_pages = kzalloc(INDIRECT_PAGES(nseg) *
+		 *   - drivers/block/xen-blkback/xenbus.c|1113| <<xen_blkbk_alloc_req>> sizeof(*req->indirect_pages),
+		 *   - drivers/block/xen-blkback/xenbus.c|1116| <<xen_blkbk_alloc_req>> req->indirect_pages = NULL;
+		 *   - drivers/block/xen-blkback/xenbus.c|1120| <<xen_blkbk_alloc_req>> !req->unmap_pages || (indirect && !req->indirect_pages)) {
+		 */
 		req->indirect_pages = kzalloc(INDIRECT_PAGES(nseg) *
 					      sizeof(*req->indirect_pages),
 					      GFP_KERNEL);
diff --git a/drivers/xen/grant-table.c b/drivers/xen/grant-table.c
index 7086fb237b50..85a46365a7b5 100644
--- a/drivers/xen/grant-table.c
+++ b/drivers/xen/grant-table.c
@@ -683,6 +683,14 @@ EXPORT_SYMBOL_GPL(gnttab_free_auto_xlat_frames);
  * @nr_pages: number of pages to alloc
  * @pages: returns the pages
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|179| <<get_free_page>> return gnttab_alloc_pages(1, page);
+ *   - drivers/net/xen-netback/grant.c|72| <<xenvif_new_grant>> err = gnttab_alloc_pages(1, &page);
+ *   - drivers/net/xen-netback/interface.c|572| <<xenvif_init_queue>> err = gnttab_alloc_pages(MAX_PENDING_REQS, queue->mmap_pages);
+ *   - drivers/xen/gntdev.c|156| <<gntdev_alloc_map>> if (gnttab_alloc_pages(count, add->pages))
+ *   - drivers/xen/xen-scsiback.c|245| <<get_free_page>> return gnttab_alloc_pages(1, page);
+ */
 int gnttab_alloc_pages(int nr_pages, struct page **pages)
 {
 	int i;
@@ -715,6 +723,18 @@ EXPORT_SYMBOL(gnttab_alloc_pages);
  * @nr_pages; number of pages to free
  * @pages: the pages
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|246| <<shrink_free_pagepool>> gnttab_free_pages(num_pages, page);
+ *   - drivers/block/xen-blkback/blkback.c|253| <<shrink_free_pagepool>> gnttab_free_pages(num_pages, page);
+ *   - drivers/net/xen-netback/grant.c|101| <<xenvif_new_grant>> gnttab_free_pages(1, &page);
+ *   - drivers/net/xen-netback/grant.c|138| <<xenvif_remove_grant>> gnttab_free_pages(1, &entry->page);
+ *   - drivers/net/xen-netback/interface.c|757| <<xenvif_deinit_queue>> gnttab_free_pages(MAX_PENDING_REQS, queue->mmap_pages);
+ *   - drivers/xen/gntdev.c|123| <<gntdev_free_map>> gnttab_free_pages(map->count, map->pages);
+ *   - drivers/xen/grant-table.c|701| <<gnttab_alloc_pages>> gnttab_free_pages(nr_pages, pages);
+ *   - drivers/xen/xen-scsiback.c|228| <<put_free_pages>> gnttab_free_pages(n, page + num - n);
+ *   - drivers/xen/xen-scsiback.c|2078| <<scsiback_exit>> gnttab_free_pages(1, &page);
+ */
 void gnttab_free_pages(int nr_pages, struct page **pages)
 {
 	int i;
@@ -893,6 +913,17 @@ static void __gnttab_unmap_refs_async(struct gntab_unmap_queue_data* item)
 	int ret;
 	int pc;
 
+	/*
+	 * 1020 invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs,
+	 * 1021                                            req->unmap, req->unmap_pages);
+	 * 1022
+	 * 1023         work->data = req;
+	 * 1024         work->done = xen_blkbk_unmap_and_respond_callback;
+	 * 1025         work->unmap_ops = req->unmap;
+	 * 1026         work->kunmap_ops = NULL;
+	 * 1027         work->pages = req->unmap_pages;
+	 * 1028         work->count = invcount;
+	 */
 	for (pc = 0; pc < item->count; pc++) {
 		if (page_count(item->pages[pc]) > 1) {
 			unsigned long delay = GNTTAB_UNMAP_REFS_DELAY * (item->age + 1);
@@ -907,8 +938,24 @@ static void __gnttab_unmap_refs_async(struct gntab_unmap_queue_data* item)
 	item->done(ret, item);
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1010| <<xen_blkbk_unmap_and_respond>> gnttab_unmap_refs_async(&req->gnttab_unmap_data);
+ *   - drivers/xen/grant-table.c|955| <<gnttab_unmap_refs_sync>> gnttab_unmap_refs_async(item);
+ */
 void gnttab_unmap_refs_async(struct gntab_unmap_queue_data* item)
 {
+	/*
+	 * 1020 invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs,
+	 * 1021                                            req->unmap, req->unmap_pages);
+	 * 1022
+	 * 1023         work->data = req;
+	 * 1024         work->done = xen_blkbk_unmap_and_respond_callback;
+	 * 1025         work->unmap_ops = req->unmap;
+	 * 1026         work->kunmap_ops = NULL;
+	 * 1027         work->pages = req->unmap_pages;
+	 * 1028         work->count = invcount;
+	 */
 	INIT_DELAYED_WORK(&item->gnttab_work, gnttab_unmap_work);
 	item->age = 0;
 
-- 
2.34.1

