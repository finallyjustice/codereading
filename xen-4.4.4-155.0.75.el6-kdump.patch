From a593b201dba9aa45b421666f4d0f56375984a835 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Wed, 3 Jun 2020 10:05:46 -0700
Subject: [PATCH 1/1] xen-4.4.4-155.0.75.el6-kdump

xen-4.4.4-155.0.75

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 xen/arch/x86/acpi/power.c                |   8 +
 xen/arch/x86/crash.c                     |  25 ++
 xen/arch/x86/hvm/intercept.c             |   6 +
 xen/arch/x86/hvm/io.c                    |   4 +
 xen/arch/x86/hvm/vmsi.c                  |  40 +++
 xen/arch/x86/irq.c                       |   5 +
 xen/arch/x86/mm/hap/hap.c                |   4 +
 xen/arch/x86/mm/p2m-pt.c                 |  15 ++
 xen/arch/x86/mm/p2m.c                    |   6 +
 xen/arch/x86/mm/paging.c                 |   5 +
 xen/arch/x86/msi.c                       |  15 ++
 xen/arch/x86/physdev.c                   |   4 +
 xen/common/domain.c                      |  11 +
 xen/common/kexec.c                       |  25 ++
 xen/common/shutdown.c                    |   4 +
 xen/drivers/char/console.c               |  32 +++
 xen/drivers/passthrough/amd/iommu_init.c |   4 +
 xen/drivers/passthrough/ats.h            |   7 +
 xen/drivers/passthrough/io.c             |   4 +
 xen/drivers/passthrough/iommu.c          | 212 +++++++++++++++
 xen/drivers/passthrough/pci.c            |  60 +++++
 xen/drivers/passthrough/vtd/dmar.c       |  26 ++
 xen/drivers/passthrough/vtd/dmar.h       |  19 ++
 xen/drivers/passthrough/vtd/intremap.c   |  67 +++++
 xen/drivers/passthrough/vtd/iommu.c      | 330 +++++++++++++++++++++++
 xen/drivers/passthrough/vtd/iommu.h      |  36 +++
 xen/drivers/passthrough/vtd/qinval.c     |  94 +++++++
 xen/drivers/passthrough/vtd/quirks.c     |  15 ++
 xen/drivers/passthrough/vtd/utils.c      |   9 +
 xen/drivers/passthrough/vtd/x86/ats.c    |  15 ++
 xen/drivers/passthrough/vtd/x86/vtd.c    |  32 +++
 xen/drivers/passthrough/x86/ats.c        |  30 +++
 xen/drivers/pci/pci.c                    |  32 +++
 xen/include/xen/iommu.h                  |  15 ++
 34 files changed, 1216 insertions(+)

diff --git a/xen/arch/x86/acpi/power.c b/xen/arch/x86/acpi/power.c
index f41f0de1bc..3a1b9fecb9 100644
--- a/xen/arch/x86/acpi/power.c
+++ b/xen/arch/x86/acpi/power.c
@@ -43,6 +43,10 @@ struct acpi_sleep_info acpi_sinfo;
 
 void do_suspend_lowlevel(void);
 
+/*
+ * called by:
+ *   - arch/x86/acpi/power.c|168| <<enter_state>> if ( (error = device_power_down()) )
+ */
 static int device_power_down(void)
 {
     console_suspend();
@@ -126,6 +130,10 @@ static void acpi_sleep_prepare(u32 state)
 static void acpi_sleep_post(u32 state) {}
 
 /* Main interface to do xen specific suspend/resume */
+/*
+ * called by:
+ *   - arch/x86/acpi/power.c|236| <<enter_state_helper>> return enter_state(sinfo->sleep_state);
+ */
 static int enter_state(u32 state)
 {
     unsigned long flags;
diff --git a/xen/arch/x86/crash.c b/xen/arch/x86/crash.c
index c5332a5ea2..84f23548f0 100644
--- a/xen/arch/x86/crash.c
+++ b/xen/arch/x86/crash.c
@@ -36,6 +36,20 @@ static unsigned int crashing_cpu;
 static DEFINE_PER_CPU_READ_MOSTLY(bool_t, crash_save_done);
 
 /* This becomes the NMI handler for non-crashing CPUs, when Xen is crashing. */
+/*
+ * 在以下使用do_nmi_crash():
+ *   - arch/x86/crash.c|148| <<nmi_shootdown_cpus>> (unsigned long )&do_nmi_crash);
+ *
+ * //
+ * // Ideally would be:
+ * //   exception_table[TRAP_nmi] = &do_nmi_crash;
+ * //
+ * // but the exception_table is read only.  Access it via its directmap
+ * // mappings.
+ *
+ * write_atomic((unsigned long *)__va(__pa(&exception_table[TRAP_nmi])),
+ *              (unsigned long)&do_nmi_crash);
+ */
 void __attribute__((noreturn)) do_nmi_crash(struct cpu_user_regs *regs)
 {
     unsigned int cpu = smp_processor_id();
@@ -113,6 +127,10 @@ void __attribute__((noreturn)) do_nmi_crash(struct cpu_user_regs *regs)
         halt();
 }
 
+/*
+ * called by:
+ *   - arch/x86/crash.c|187| <<machine_crash_shutdown>> nmi_shootdown_cpus();
+ */
 static void nmi_shootdown_cpus(void)
 {
     unsigned long msecs;
@@ -180,10 +198,17 @@ static void nmi_shootdown_cpus(void)
     hpet_disable();
 }
 
+/*
+ * called by:
+ *   - common/kexec.c|368| <<kexec_crash>> machine_crash_shutdown();
+ */
 void machine_crash_shutdown(void)
 {
     crash_xen_info_t *info;
 
+    /*
+     * 打印: Shot down all CPUs
+     */
     nmi_shootdown_cpus();
 
     info = kexec_crash_save_info();
diff --git a/xen/arch/x86/hvm/intercept.c b/xen/arch/x86/hvm/intercept.c
index 6c4470d7bc..21c9ea23f8 100644
--- a/xen/arch/x86/hvm/intercept.c
+++ b/xen/arch/x86/hvm/intercept.c
@@ -322,6 +322,12 @@ static int process_portio_intercept(portio_action_t action, ioreq_t *p)
  * Check if the request is handled inside xen
  * return value: 0 --not handled; 1 --handled
  */
+/*
+ * called by:
+ *   - include/asm-x86/hvm/io.h|76| <<hvm_io_intercept>> int hvm_io_intercept(ioreq_t *p, int type);
+ *   - include/asm-x86/hvm/io.h|86| <<hvm_portio_intercept>> return hvm_io_intercept(p, HVM_PORTIO);
+ *   - include/asm-x86/hvm/io.h|91| <<hvm_buffered_io_intercept>> return hvm_io_intercept(p, HVM_BUFFERED_IO);
+ */
 int hvm_io_intercept(ioreq_t *p, int type)
 {
     struct vcpu *v = current;
diff --git a/xen/arch/x86/hvm/io.c b/xen/arch/x86/hvm/io.c
index 143f4ac092..8686c958cd 100644
--- a/xen/arch/x86/hvm/io.c
+++ b/xen/arch/x86/hvm/io.c
@@ -449,6 +449,10 @@ static int dpci_ioport_write(uint32_t mport, ioreq_t *p)
     return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/intercept.c|334| <<hvm_io_intercept>> int rc = dpci_ioport_intercept(p);
+ */
 int dpci_ioport_intercept(ioreq_t *p)
 {
     struct domain *d = current->domain;
diff --git a/xen/arch/x86/hvm/vmsi.c b/xen/arch/x86/hvm/vmsi.c
index c4f1840829..f04e86d877 100644
--- a/xen/arch/x86/hvm/vmsi.c
+++ b/xen/arch/x86/hvm/vmsi.c
@@ -261,6 +261,35 @@ out:
     return r;
 }
 
+/*
+ * 在xen 4.10截取的例子:
+ * [<ffff82d0802f2ea0>] vmsi.c#msixtbl_write+0x160/0x240
+ * [<ffff82d0802e587a>] hvm_process_io_intercept+0x6a/0x270
+ * [<ffff82d0802e55c7>] intercept.c#hvm_mmio_accept+0x37/0xe0
+ * [<ffff82d0802f2c5f>] vmsi.c#msixtbl_range+0x6f/0x150
+ * [<ffff82d0802e5aa4>] hvm_io_intercept+0x24/0x40
+ * [<ffff82d0802d7407>] emulate.c#hvmemul_do_io+0x1e7/0x5b0
+ * [<ffff82d0802f2c5f>] vmsi.c#msixtbl_range+0x6f/0x150
+ * [<ffff82d0802d7f4e>] emulate.c#hvmemul_do_io_buffer+0x2e/0x70
+ * [<ffff82d0802d8c5b>] emulate.c#hvmemul_linear_mmio_access+0x23b/0x530
+ * [<ffff82d0802d96cc>] emulate.c#hvmemul_write+0x46c/0x500
+ * [<ffff82d0802d91cf>] hvmemul_insn_fetch+0x3f/0xa0
+ * [<ffff82d0802a16c1>] x86_emulate.c#x86_decode+0x11b1/0x1e40
+ * [<ffff82d0802a4427>] x86_emulate+0x20d7/0x1d610
+ * [<ffff82d080311073>] __get_gfn_type_access+0xf3/0x280
+ * [<ffff82d0802da26a>] emulate.c#_hvm_emulate_one+0x4a/0x1c0
+ * [<ffff82d0802da088>] hvm_emulate_init_once+0x78/0xb0
+ * [<ffff82d0802e5efb>] hvm_emulate_one_insn+0x3b/0x120
+ * [<ffff82d0802e0177>] hvm.c#__hvm_copy+0xd7/0x350
+ * [<ffff82d0802bfa60>] x86_insn_is_mem_access+0/0xc0
+ * [<ffff82d0802de977>] hvm_hap_nested_page_fault+0x117/0x6b0
+ * [<ffff82d080234635>] schedule.c#schedule+0x245/0x620
+ * [<ffff82d0803076e9>] vmx_vmexit_handler+0x969/0x1b20
+ * [<ffff82d08030a161>] nvmx_switch_guest+0x81/0x19c0
+ * [<ffff82d080308989>] vmx_vmenter_helper+0xe9/0x3a0
+ * [<ffff82d08023a6bd>] tasklet.c#tasklet_softirq_action+0x3d/0x70
+ * [<ffff82d08030db4c>] vmx_asm_vmexit_handler+0x3c/0x110
+ */
 static int msixtbl_write(struct vcpu *v, unsigned long address,
                          unsigned long len, unsigned long val)
 {
@@ -378,6 +407,9 @@ static int msixtbl_range(struct vcpu *v, unsigned long addr)
     return !!virt;
 }
 
+/*
+ * 在hvm_mmio_handlers[HVM_MMIO_HANDLER_NR]中
+ */
 const struct hvm_mmio_handler msixtbl_mmio_handler = {
     .check_handler = msixtbl_range,
     .read_handler = msixtbl_read,
@@ -420,6 +452,10 @@ static void del_msixtbl_entry(struct msixtbl_entry *entry)
     call_rcu(&entry->rcu, free_msixtbl_entry);
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/io.c|158| <<pt_irq_create_bind>> rc = msixtbl_pt_register(d, info, pt_irq_bind->u.msi.gtable);
+ */
 int msixtbl_pt_register(struct domain *d, struct pirq *pirq, uint64_t gtable)
 {
     struct irq_desc *irq_desc;
@@ -537,6 +573,10 @@ void msixtbl_pt_cleanup(struct domain *d)
     local_irq_restore(flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/io.c|311| <<hvm_io_assist>> msix_write_completion(curr);
+ */
 void msix_write_completion(struct vcpu *v)
 {
     unsigned long ctrl_address = v->arch.hvm_vcpu.hvm_io.msix_unmask_address;
diff --git a/xen/arch/x86/irq.c b/xen/arch/x86/irq.c
index 67cf9b9f71..579e905bf4 100644
--- a/xen/arch/x86/irq.c
+++ b/xen/arch/x86/irq.c
@@ -1893,6 +1893,11 @@ int get_free_pirqs(struct domain *d, unsigned int nr)
     return -ENOSPC;
 }
 
+/*
+ * called by:
+ *   - arch/x86/io_apic.c|2419| <<ioapic_guest_write>> ret = map_domain_pirq(dom0, pirq, irq, MAP_PIRQ_TYPE_GSI, NULL);
+ *   - arch/x86/physdev.c|233| <<physdev_map_pirq>> ret = map_domain_pirq(d, pirq, irq, type, map_data);
+ */
 int map_domain_pirq(
     struct domain *d, int pirq, int irq, int type, void *data)
 {
diff --git a/xen/arch/x86/mm/hap/hap.c b/xen/arch/x86/mm/hap/hap.c
index 8d2441e84b..d859a9d704 100644
--- a/xen/arch/x86/mm/hap/hap.c
+++ b/xen/arch/x86/mm/hap/hap.c
@@ -433,6 +433,10 @@ void hap_domain_init(struct domain *d)
 }
 
 /* return 0 for success, -errno for failure */
+/*
+ * called by:
+ *   - arch/x86/mm/paging.c|826| <<paging_enable>> return hap_enable(d, mode | PG_HAP_enable);
+ */
 int hap_enable(struct domain *d, u32 mode)
 {
     unsigned int old_pages;
diff --git a/xen/arch/x86/mm/p2m-pt.c b/xen/arch/x86/mm/p2m-pt.c
index b033e8daff..dd40347422 100644
--- a/xen/arch/x86/mm/p2m-pt.c
+++ b/xen/arch/x86/mm/p2m-pt.c
@@ -423,6 +423,21 @@ p2m_pt_set_entry(struct p2m_domain *p2m, unsigned long gfn, mfn_t mfn,
 
     if ( iommu_enabled && need_iommu(p2m->domain) && old_mfn != mfn_x(mfn) )
     {
+        /*
+	 * 在以下调用iommu_use_hap_pt():
+	 *   - arch/x86/mm/p2m-pt.c|426| <<p2m_pt_set_entry>> if ( iommu_use_hap_pt(p2m->domain) )
+	 *   - drivers/passthrough/amd/iommu_map.c|662| <<amd_iommu_map_page>> if ( iommu_use_hap_pt(d) )
+	 *   - drivers/passthrough/amd/iommu_map.c|750| <<amd_iommu_unmap_page>> if ( iommu_use_hap_pt(d) )
+	 *   - drivers/passthrough/amd/iommu_map.c|823| <<amd_iommu_share_p2m>> if ( !iommu_use_hap_pt(d) )
+	 *   - drivers/passthrough/amd/pci_amd_iommu.c|464| <<deallocate_iommu_page_tables>> if ( iommu_use_hap_pt(d) )
+	 *   - drivers/passthrough/iommu.c|376| <<assign_device>> if ( !iommu_use_hap_pt(d) )
+	 *   - drivers/passthrough/iommu.c|1068| <<iommu_dump_p2m_table>> if ( iommu_use_hap_pt(d) )
+	 *   - drivers/passthrough/vtd/iommu.c|1911| <<iommu_domain_teardown>> if ( iommu_use_hap_pt(d) )
+	 *   - drivers/passthrough/vtd/iommu.c|1937| <<intel_iommu_map_page>> if ( iommu_use_hap_pt(d) )
+	 *   - drivers/passthrough/vtd/iommu.c|2044| <<iommu_set_pgd>> if ( !iommu_use_hap_pt(d) )
+	 *
+	 * 应该大部分都是true
+	 */
         if ( iommu_use_hap_pt(p2m->domain) )
         {
             if ( old_mfn != INVALID_MFN )
diff --git a/xen/arch/x86/mm/p2m.c b/xen/arch/x86/mm/p2m.c
index b2d1e42abe..fb4c6dd0f9 100644
--- a/xen/arch/x86/mm/p2m.c
+++ b/xen/arch/x86/mm/p2m.c
@@ -383,6 +383,12 @@ void p2m_free_ptp(struct p2m_domain *p2m, struct page_info *pg)
 //
 // Returns 0 for success or -errno.
 //
+/*
+ * called by:
+ *   - arch/x86/mm/hap/hap.c|474| <<hap_enable>> rv = p2m_alloc_table(p2m_get_hostp2m(d));
+ *   - arch/x86/mm/hap/hap.c|480| <<hap_enable>> rv = p2m_alloc_table(d->arch.nested_p2m[i]);
+ *   - arch/x86/mm/shadow/common.c|3007| <<shadow_enable>> rv = p2m_alloc_table(p2m);
+ */
 int p2m_alloc_table(struct p2m_domain *p2m)
 {
     mfn_t mfn = _mfn(INVALID_MFN);
diff --git a/xen/arch/x86/mm/paging.c b/xen/arch/x86/mm/paging.c
index 198ed8e7c7..da4923329a 100644
--- a/xen/arch/x86/mm/paging.c
+++ b/xen/arch/x86/mm/paging.c
@@ -811,6 +811,11 @@ void paging_final_teardown(struct domain *d)
 
 /* Enable an arbitrary paging-assistance mode.  Call once at domain
  * creation. */
+/*
+ * called  by:
+ *   - arch/x86/domain_build.c|1185| <<construct_dom0>> if ( paging_enable(d, PG_SH_enable) == 0 )
+ *   - arch/x86/hvm/hvm.c|605| <<hvm_domain_initialise>> rc = paging_enable(d, PG_refcounts|PG_translate|PG_external);
+ */
 int paging_enable(struct domain *d, u32 mode)
 {
     switch ( mode & (PG_external | PG_translate) )
diff --git a/xen/arch/x86/msi.c b/xen/arch/x86/msi.c
index a651070301..bae20e29fd 100644
--- a/xen/arch/x86/msi.c
+++ b/xen/arch/x86/msi.c
@@ -211,6 +211,12 @@ static void read_msi_msg(struct msi_desc *entry, struct msi_msg *msg)
         iommu_read_msi_from_ire(entry, msg);
 }
 
+/*
+ * called by:
+ *   - arch/x86/msi.c|298| <<set_msi_affinity>> write_msi_msg(msi_desc, &msg);
+ *   - arch/x86/msi.c|496| <<__setup_msi_irq>> ret = write_msi_msg(msidesc, &msg);
+ *   - arch/x86/msi.c|1161| <<pci_restore_msi_state>> write_msi_msg(entry, &msg);
+ */
 static int write_msi_msg(struct msi_desc *entry, struct msi_msg *msg)
 {
     entry->msg = *msg;
@@ -477,6 +483,10 @@ static struct msi_desc *alloc_msi_entry(unsigned int nr)
     return entry;
 }
 
+/*
+ * called by:
+ *   - arch/x86/irq.c|1997| <<map_domain_pirq>> while ( !(ret = setup_msi_irq(desc, msi_desc + nr)) )
+ */
 int setup_msi_irq(struct irq_desc *desc, struct msi_desc *msidesc)
 {
     return __setup_msi_irq(desc, msidesc,
@@ -484,6 +494,11 @@ int setup_msi_irq(struct irq_desc *desc, struct msi_desc *msidesc)
                                                      : &pci_msi_nonmaskable);
 }
 
+/*
+ * called by:
+ *   - arch/x86/msi.c|488| <<setup_msi_irq>> return __setup_msi_irq(desc, msidesc,
+ *   - drivers/passthrough/amd/iommu_init.c|816| <<set_iommu_interrupt_handler>> ret = __setup_msi_irq(irq_to_desc(irq), &iommu->msi, handler);
+ */
 int __setup_msi_irq(struct irq_desc *desc, struct msi_desc *msidesc,
                     hw_irq_controller *handler)
 {
diff --git a/xen/arch/x86/physdev.c b/xen/arch/x86/physdev.c
index 7af4b82a7c..fa5be22bf6 100644
--- a/xen/arch/x86/physdev.c
+++ b/xen/arch/x86/physdev.c
@@ -86,6 +86,10 @@ static int physdev_hvm_map_pirq(
     return ret;
 }
 
+/*
+ * 处理PHYSDEVOP_map_pirq:
+ *   - arch/x86/physdev.c|449| <<do_phys_dev_op(PHYSDEVOP_map_pirq)>> ret = physdev_map_pirq(map.domid, map.type, &map.index, &map.pirq,
+ */
 int physdev_map_pirq(domid_t domid, int type, int *index, int *pirq_p,
                      struct msi_info *msi)
 {
diff --git a/xen/common/domain.c b/xen/common/domain.c
index fcb13abf33..01df319807 100644
--- a/xen/common/domain.c
+++ b/xen/common/domain.c
@@ -694,6 +694,17 @@ void __domain_crash_synchronous(void)
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/hvm/hvm.c|1504| <<hvm_vcpu_down>> domain_shutdown(d, SHUTDOWN_poweroff);
+ *   - arch/x86/hvm/hvm.c|1567| <<hvm_triple_fault>> domain_shutdown(d, reason);
+ *   - common/domain.c|682| <<__domain_crash>> domain_shutdown(d, SHUTDOWN_crash);
+ *   - common/page_alloc.c|1125| <<offline_page>> domain_shutdown(owner, SHUTDOWN_crash);
+ *   - common/schedule.c|807| <<domain_watchdog_timeout>> domain_shutdown(d, SHUTDOWN_watchdog);
+ *   - common/schedule.c|893| <<do_sched_op_compat>> domain_shutdown(current->domain, (u8)arg);
+ *   - common/schedule.c|938| <<do_sched_op(SCHEDOP_shutdown)>> domain_shutdown(current->domain, (u8)sched_shutdown.reason);
+ *   - common/schedule.c|998| <<do_sched_op(SCHEDOP_remote_shutdown)>> domain_shutdown(d, (u8)sched_remote_shutdown.reason);
+ */
 void domain_shutdown(struct domain *d, u8 reason)
 {
     struct vcpu *v;
diff --git a/xen/common/kexec.c b/xen/common/kexec.c
index 44ae95d52f..d7ade29d6e 100644
--- a/xen/common/kexec.c
+++ b/xen/common/kexec.c
@@ -242,6 +242,10 @@ void __init set_kexec_crash_area_size(u64 system_ram)
  * This is noinline to make it obvious in stack traces which cpus have lost
  * the race (as opposed to being somewhere in kexec_common_shutdown())
  */
+/*
+ * called by:
+ *   - common/kexec.c|325| <<kexec_common_shutdown>> ret = one_cpu_only();
+ */
 static int noinline one_cpu_only(void)
 {
     static unsigned int crashing_cpu = -1;
@@ -271,6 +275,11 @@ static int noinline one_cpu_only(void)
 }
 
 /* Save the registers in the per-cpu crash note buffer. */
+/*
+ * called by:
+ *   - arch/x86/crash.c|65| <<do_nmi_crash>> kexec_crash_save_cpu();
+ *   - common/kexec.c|367| <<kexec_crash>> kexec_crash_save_cpu();
+ */
 void kexec_crash_save_cpu(void)
 {
     int cpu = smp_processor_id();
@@ -318,6 +327,11 @@ crash_xen_info_t *kexec_crash_save_info(void)
     return out;
 }
 
+/*
+ * called by:
+ *   - common/kexec.c|347| <<kexec_crash>> if ( kexec_common_shutdown() != 0 )
+ *   - common/kexec.c|363| <<kexec_reboot>> kexec_common_shutdown();
+ */
 static int kexec_common_shutdown(void)
 {
     int ret;
@@ -334,6 +348,14 @@ static int kexec_common_shutdown(void)
     return 0;
 }
 
+/*
+ * called by:
+ *   - common/kexec.c|373| <<do_crashdump_trigger>> kexec_crash();
+ *   - common/kexec.c|809| <<kexec_exec(KEXEC_TYPE_CRASH)>> kexec_crash();
+ *   - common/shutdown.c|51| <<dom0_shutdown>> kexec_crash();
+ *   - common/shutdown.c|68| <<dom0_shutdown>> kexec_crash();
+ *   - drivers/char/console.c|1072| <<panic>> kexec_crash();
+ */
 void kexec_crash(void)
 {
     int pos;
@@ -348,6 +370,9 @@ void kexec_crash(void)
         return;
 
     kexec_crash_save_cpu();
+    /*
+     * 打印: Shot down all CPUs
+     */
     machine_crash_shutdown();
     machine_kexec(kexec_image[KEXEC_IMAGE_CRASH_BASE + pos]);
 
diff --git a/xen/common/shutdown.c b/xen/common/shutdown.c
index 90ee384212..162a7e59e1 100644
--- a/xen/common/shutdown.c
+++ b/xen/common/shutdown.c
@@ -32,6 +32,10 @@ static void maybe_reboot(void)
     }
 }
 
+/*
+ * called by:
+ *   - common/domain.c|708| <<domain_shutdown>> dom0_shutdown(reason);
+ */
 void dom0_shutdown(u8 reason)
 {
     switch ( reason )
diff --git a/xen/drivers/char/console.c b/xen/drivers/char/console.c
index 9d1460474c..ebe022c458 100644
--- a/xen/drivers/char/console.c
+++ b/xen/drivers/char/console.c
@@ -65,6 +65,24 @@ static uint32_t conringc, conringp;
 
 static int __read_mostly sercon_handle = -1;
 
+/*
+ * 在以下使用console_lock:
+ *   - drivers/char/console.c|183| <<conring_puts>> ASSERT(spin_is_locked(&console_lock));
+ *   - drivers/char/console.c|221| <<read_console_ring>> spin_lock_irq(&console_lock);
+ *   - drivers/char/console.c|226| <<read_console_ring>> spin_unlock_irq(&console_lock);
+ *   - drivers/char/console.c|396| <<guest_console_write>> spin_lock_irq(&console_lock);
+ *   - drivers/char/console.c|407| <<guest_console_write>> spin_unlock_irq(&console_lock);
+ *   - drivers/char/console.c|505| <<__putstr>> ASSERT(spin_is_locked(&console_lock));
+ *   - drivers/char/console.c|579| <<vprintk_common>> spin_lock_recursive(&console_lock);
+ *   - drivers/char/console.c|614| <<vprintk_common>> spin_unlock_recursive(&console_lock);
+ *   - drivers/char/console.c|670| <<console_init_preirq>> spin_lock(&console_lock);
+ *   - drivers/char/console.c|672| <<console_init_preirq>> spin_unlock(&console_lock);
+ *   - drivers/char/console.c|706| <<console_init_postirq>> spin_lock_irq(&console_lock);
+ *   - drivers/char/console.c|712| <<console_init_postirq>> spin_unlock_irq(&console_lock);
+ *   - drivers/char/console.c|800| <<console_force_unlock>> spin_lock_init(&console_lock);
+ *   - drivers/char/console.c|853| <<__printk_ratelimit>> spin_lock_recursive(&console_lock);
+ *   - drivers/char/console.c|858| <<__printk_ratelimit>> spin_unlock_recursive(&console_lock);
+ */
 static DEFINE_SPINLOCK(console_lock);
 
 /*
@@ -566,6 +584,11 @@ static void printk_start_of_line(const char *prefix)
     __putstr(tstr);
 }
 
+/*
+ * called by:
+ *   - drivers/char/console.c|622| <<printk>> vprintk_common("(XEN) ", fmt, args);
+ *   - drivers/char/console.c|634| <<guest_printk>> vprintk_common(prefix, fmt, args);
+ */
 static void vprintk_common(const char *prefix, const char *fmt, va_list args)
 {
     static char   buf[1024];
@@ -794,6 +817,15 @@ void console_end_log_everything(void)
     atomic_dec(&print_everything);
 }
 
+/*
+ * called by:
+ *   - arch/x86/cpu/mcheck/mce.c|1542| <<mc_panic>> console_force_unlock();
+ *   - arch/x86/nmi.c|479| <<nmi_watchdog_tick>> console_force_unlock();
+ *   - arch/x86/traps.c|3296| <<pci_serr_error>> console_force_unlock();
+ *   - arch/x86/traps.c|3311| <<io_check_error>> console_force_unlock();
+ *   - arch/x86/traps.c|3330| <<unknown_nmi_error>> console_force_unlock();
+ *   - arch/x86/x86_64/traps.c|229| <<do_double_fault>> console_force_unlock();
+ */
 void console_force_unlock(void)
 {
     watchdog_disable();
diff --git a/xen/drivers/passthrough/amd/iommu_init.c b/xen/drivers/passthrough/amd/iommu_init.c
index 4686813a8b..9691f9ae5b 100644
--- a/xen/drivers/passthrough/amd/iommu_init.c
+++ b/xen/drivers/passthrough/amd/iommu_init.c
@@ -1036,6 +1036,10 @@ error_out:
     return -ENODEV;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/amd/iommu_init.c|1266| <<amd_iommu_init>> amd_iommu_init_cleanup();
+ */
 static void __init amd_iommu_init_cleanup(void)
 {
     struct amd_iommu *iommu, *next;
diff --git a/xen/drivers/passthrough/ats.h b/xen/drivers/passthrough/ats.h
index 000e76db95..20d0379bba 100644
--- a/xen/drivers/passthrough/ats.h
+++ b/xen/drivers/passthrough/ats.h
@@ -18,6 +18,13 @@
 
 #include <xen/pci_regs.h>
 
+/*
+ * Address Translation Service: ATS makes the PCI Endpoint be able to request
+ * the DMA address translation from the IOMMU and cache the translation in the
+ * Endpoint, thus alleviate IOMMU pressure and improve the hardware performance
+ * in the I/O virtualization environment.
+ */
+
 struct pci_ats_dev {
     struct list_head list;
     u16 seg;
diff --git a/xen/drivers/passthrough/io.c b/xen/drivers/passthrough/io.c
index cd386dfc58..45f8654189 100644
--- a/xen/drivers/passthrough/io.c
+++ b/xen/drivers/passthrough/io.c
@@ -93,6 +93,10 @@ void free_hvm_irq_dpci(struct hvm_irq_dpci *dpci)
     xfree(dpci);
 }
 
+/*
+ * called by:
+ *   - arch/x86/domctl.c|750| <<arch_do_domctl(XEN_DOMCTL_bind_pt_irq)>> ret = pt_irq_create_bind(d, bind);
+ */
 int pt_irq_create_bind(
     struct domain *d, xen_domctl_bind_pt_irq_t *pt_irq_bind)
 {
diff --git a/xen/drivers/passthrough/iommu.c b/xen/drivers/passthrough/iommu.c
index c86b237a32..93805b6b88 100644
--- a/xen/drivers/passthrough/iommu.c
+++ b/xen/drivers/passthrough/iommu.c
@@ -24,6 +24,85 @@
 #include <xsm/xsm.h>
 #include <xen/mm.h>
 
+/*
+ * (XEN) Intel VT-d iommu 0 supported page sizes: 4kB, 2MB, 1GB.
+ * (XEN) Intel VT-d iommu 1 supported page sizes: 4kB, 2MB, 1GB.
+ * (XEN) Intel VT-d Snoop Control enabled.
+ * (XEN) Intel VT-d Dom0 DMA Passthrough not enabled.
+ * (XEN) Intel VT-d Queued Invalidation enabled.
+ * (XEN) Intel VT-d Interrupt Remapping enabled.
+ * (XEN) Intel VT-d Shared EPT tables enabled.
+ * (XEN) I/O virtualisation enabled
+ * (XEN)  - Dom0 mode: Relaxed
+ * (XEN) Interrupt remapping enabled
+ */
+
+/*
+ * hvm_domain_initialise()
+ * -> hap_enable()
+ *    -> p2m_alloc_table()
+ *       -> iommu_share_p2m_table()
+ *          -> ops->share_p2m = iommu_set_pgd()
+ *             -> pgd_mfn = pagetable_get_mfn(p2m_get_pagetable(p2m_get_hostp2m(d)));
+ *             -> hd->pgd_maddr = pagetable_get_paddr(pagetable_from_mfn(pgd_mfn));
+ *
+ * iommu_do_domctl(XEN_DOMCTL_assign_device)
+ * -> hd->platform_ops->assign_device = intel_iommu_assign_device()
+ *    -> reassign_device_ownership()
+ *       -> domain_context_mapping()
+ *          -> domain_context_mapping_one()
+ *             -> pgd_maddr = hd->pgd_maddr;
+ *                -> context_set_address_root(*context, pgd_maddr);
+ */
+
+/*
+ * 下面是从xen-4.10的注释抄的:
+ * 在arch/x86/x86_64/entry.S,
+ * autogen_entrypoints生成vector table, 每一个vector指向一个处理函数
+ * 函数都差不多 (每个cpu支持256个vector):
+ *
+ * 先"movb  $vec,4(%rsp)", 再"jmp   common_interrupt"
+ *
+ * 在arch/x86/x86_64/entry.S:
+ * 392 ENTRY(common_interrupt)
+ * 393         SAVE_ALL CLAC
+ * 394         CR4_PV32_RESTORE
+ * 395         movq %rsp,%rdi
+ * 396         callq do_IRQ
+ * 397         jmp ret_from_intr
+ *
+ * do_IRQ()如何把vector发送到guest?
+ *
+ * 1. 先通过vector查找percpu的vector_irq[vector]找到irq:
+ * int irq = __get_cpu_var(vector_irq[vector]);
+ *
+ * 2. 再通过irq找到对应的struct irq_desc, x86下就是&irq_desc[irq]:
+ * desc = irq_to_desc(irq);
+ *
+ * 3. 很可能desc->status & IRQ_GUEST是true, 就要调用__do_IRQ_guest(irq)了
+ *
+ * 4. 如果是dom0, 调用send_guest_pirq()往dom0插入event
+ *
+ *
+ * 重要的函数或者数据结构:
+ *
+ * - vector_irq[vector]: 把cpu的vector转换成irq, 索引&irq_desc[irq]
+ * - domain_irq_to_pirq(d, irq)负责将xen的irq转换成某个domain d的pirq
+ * - pirq_info(d, pirq)负责将某个domain d的pirq转换成'struct pirq'
+ *
+ * struct pirq {
+ *   int pirq;
+ *   u16 evtchn;
+ *   bool_t masked;
+ *   struct rcu_head rcu_head;
+ *   struct arch_pirq arch;
+ * };
+ *
+ * dom0 linux的核心函数感觉是__startup_pirq()
+ *
+ * xen有自己的irq, linux有自己的irq, pirq是domain相关的, linux和xen都认识
+ */
+
 static void parse_iommu_param(char *s);
 static int iommu_populate_page_table(struct domain *d);
 static void iommu_dump_p2m_table(unsigned char key);
@@ -43,16 +122,62 @@ static void iommu_dump_p2m_table(unsigned char key);
  *   no-intremap                Disable VT-d Interrupt Remapping
  */
 custom_param("iommu", parse_iommu_param);
+/*
+ * 在以下修改iommu_enable:
+ *   - drivers/passthrough/amd/iommu_init.c|1066| <<amd_iommu_init_cleanup>> iommu_enabled = 0;
+ *   - drivers/passthrough/iommu.c|615| <<iommu_setup>> iommu_enabled = (rc == 0);
+ */
 bool_t __initdata iommu_enable = 1;
 bool_t __read_mostly iommu_enabled;
 bool_t __read_mostly force_iommu;
 bool_t __initdata iommu_dom0_strict;
 bool_t __read_mostly iommu_verbose;
 bool_t __read_mostly iommu_workaround_bios_bug;
+/*
+ * (XEN) Intel VT-d iommu 0 supported page sizes: 4kB, 2MB, 1GB.
+ * (XEN) Intel VT-d iommu 1 supported page sizes: 4kB, 2MB, 1GB.
+ * (XEN) Intel VT-d Snoop Control enabled.
+ * (XEN) Intel VT-d Dom0 DMA Passthrough not enabled.
+ * (XEN) Intel VT-d Queued Invalidation enabled.
+ * (XEN) Intel VT-d Interrupt Remapping enabled.
+ * (XEN) Intel VT-d Shared EPT tables enabled.
+ * (XEN) I/O virtualisation enabled
+ * (XEN)  - Dom0 mode: Relaxed
+ * (XEN) Interrupt remapping enabled
+ *
+ * 在以下设置iommu_passthrough:
+ *   - drivers/passthrough/amd/iommu_init.c|1071| <<amd_iommu_init_cleanup>> iommu_passthrough = 0;
+ *   - drivers/passthrough/iommu.c|114| <<parse_iommu_param>> iommu_passthrough = val;
+ *   - drivers/passthrough/iommu.c|655| <<iommu_setup>> iommu_passthrough = 0;
+ *   - drivers/passthrough/iommu.c|673| <<iommu_setup>> iommu_passthrough = 0;
+ *   - drivers/passthrough/vtd/iommu.c|2358| <<intel_vtd_setup>> iommu_passthrough = 0;
+ *   - drivers/passthrough/vtd/iommu.c|2407| <<intel_vtd_setup>> iommu_passthrough = 0;
+ *
+ * drivers/passthrough/iommu.c|114| <<parse_iommu_param>> iommu_passthrough = val;
+ *  118         else if ( !strcmp(s, "dom0-passthrough") )
+ *  119             iommu_passthrough = val;
+ *
+ * Control whether to disable DMA remapping for Dom0.
+ */
 bool_t __read_mostly iommu_passthrough;
 bool_t __read_mostly iommu_snoop = 1;
 bool_t __read_mostly iommu_qinval = 1;
 bool_t __read_mostly iommu_intremap = 1;
+/*
+ * 在以下使用iommu_hap_pt_share:
+ *   - arch/x86/mm/p2m-ept.c|445| <<ept_set_entry>> if ( iommu_hap_pt_share )
+ *   - arch/x86/mm/p2m-pt.c|156| <<p2m_add_iommu_flags>> if ( iommu_hap_pt_share )
+ *   - drivers/passthrough/iommu.c|133| <<parse_iommu_param>> iommu_hap_pt_share = val;
+ *   - drivers/passthrough/vtd/iommu.c|2449| <<intel_vtd_setup>> iommu_hap_pt_share = 0;
+ *   - drivers/passthrough/vtd/iommu.c|2473| <<intel_vtd_setup>> P(iommu_hap_pt_share, "Shared EPT tables");
+ *   - include/xen/iommu.h|38| <<iommu_use_hap_pt>> #define iommu_use_hap_pt(d) (hap_enabled(d) && iommu_hap_pt_share)
+ *
+ * Shared Virtual Memory: For devices supporting appropriate PCI-Express1 capabilities, OS may
+ * use the DMA remapping hardware capabilities to share virtual address space of application
+ * processes with I/O devices. Shared virtual memory along with support for I/O page-faults enable
+ * application programs to freely pass arbitrary data-structures to devices such as graphics
+ * processors or accelerators, without the overheads of pinning and marshalling of data.
+ */
 bool_t __read_mostly iommu_hap_pt_share = 1;
 bool_t __read_mostly iommu_debug;
 bool_t __read_mostly amd_iommu_perdev_intremap = 1;
@@ -116,6 +241,10 @@ static void __init parse_iommu_param(char *s)
     } while ( ss );
 }
 
+/*
+ * called by:
+ *   - arch/x86/domain.c|632| <<arch_domain_create>> if ( (rc = iommu_domain_init(d)) != 0 )
+ */
 int iommu_domain_init(struct domain *d)
 {
     struct hvm_iommu *hd = domain_hvm_iommu(d);
@@ -177,6 +306,10 @@ void __init iommu_dom0_init(struct domain *d)
     return hd->platform_ops->dom0_init(d);
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/pci.c|735| <<pci_add_device>> ret = iommu_add_device(pdev);
+ */
 int iommu_add_device(struct pci_dev *pdev)
 {
     struct hvm_iommu *hd;
@@ -208,6 +341,10 @@ int iommu_add_device(struct pci_dev *pdev)
     }
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/pci.c|745| <<pci_add_device>> iommu_enable_device(pdev);
+ */
 int iommu_enable_device(struct pci_dev *pdev)
 {
     struct hvm_iommu *hd;
@@ -265,6 +402,11 @@ static void iommu_teardown(struct domain *d)
     tasklet_schedule(&iommu_pt_cleanup_tasklet);
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/iommu.c|291| <<device_assigned>> return device_assigned_to_domain(dom0, seg, bus, devfn) ? 0 : -EBUSY;
+ *   - drivers/passthrough/iommu.c|296| <<device_hidden>> return device_assigned_to_domain(dom_xen, seg, bus, devfn) ? -EBUSY : 0;
+ */
 static int device_assigned_to_domain(struct domain *d, u16 seg, u8 bus, u8 devfn)
 {
     int rc = 0;
@@ -291,6 +433,11 @@ static int device_hidden(u16 seg, u8 bus, u8 devfn)
     return device_assigned_to_domain(dom_xen, seg, bus, devfn) ? -EBUSY : 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/iommu.c|879| <<iommu_do_domctl(XEN_DOMCTL_assign_device)>> ret = assign_device(d, seg, bus, devfn);
+ *   - drivers/passthrough/iommu.c|971| <<iommu_do_domctl(XEN_DOMCTL_hide_device)>> ret = assign_device(dom_xen, seg, bus, devfn);
+ */
 static int assign_device(struct domain *d, u16 seg, u8 bus, u8 devfn)
 {
     struct hvm_iommu *hd = domain_hvm_iommu(d);
@@ -328,6 +475,11 @@ static int assign_device(struct domain *d, u16 seg, u8 bus, u8 devfn)
 
     if ( need_iommu(d) <= 0 )
     {
+        /*
+	 * Does this domain have a P2M table we can use as its IOMMU pagetable?
+	 *
+	 * 应该大部分都是true
+	 */
         if ( !iommu_use_hap_pt(d) )
         {
             rc = iommu_populate_page_table(d);
@@ -349,6 +501,9 @@ static int assign_device(struct domain *d, u16 seg, u8 bus, u8 devfn)
 
     pdev->fault.count = 0;
 
+    /*
+     * intel_iommu_assign_device()
+     */
     if ( (rc = hd->platform_ops->assign_device(d, devfn, pdev)) )
         goto done;
 
@@ -372,6 +527,10 @@ static int assign_device(struct domain *d, u16 seg, u8 bus, u8 devfn)
     return rc;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/iommu.c|355| <<assign_device>> rc = iommu_populate_page_table(d);
+ */
 static int iommu_populate_page_table(struct domain *d)
 {
     struct hvm_iommu *hd = domain_hvm_iommu(d);
@@ -456,6 +615,18 @@ void iommu_domain_destroy(struct domain *d)
     }
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm.c|2590| <<__get_page_type>> iommu_map_page(d, mfn_to_gmfn(d, page_to_mfn(page)),
+ *   - arch/x86/mm/p2m-ept.c|453| <<ept_set_entry>> iommu_map_page(d, gfn + i, mfn_x(mfn) + i, flags);
+ *   - arch/x86/mm/p2m-pt.c|437| <<p2m_pt_set_entry>> iommu_map_page(p2m->domain, gfn+i, mfn_x(mfn)+i, flags);
+ *   - arch/x86/mm/p2m.c|576| <<guest_physmap_add_entry>> rc = iommu_map_page(
+ *   - arch/x86/x86_64/mm.c|1465| <<memory_add>> if ( iommu_map_page(dom0, i, i, IOMMUF_readable|IOMMUF_writable) )
+ *   - common/grant_table.c|992| <<__gnttab_map_grant_ref>> err = iommu_map_page(ld, frame, frame,
+ *   - common/grant_table.c|998| <<__gnttab_map_grant_ref>> err = iommu_map_page(ld, frame, frame, IOMMUF_readable);
+ *   - common/grant_table.c|1255| <<__gnttab_unmap_common>> err = iommu_map_page(ld, op->frame, op->frame, IOMMUF_readable);
+ *   - drivers/passthrough/vtd/x86/vtd.c|146| <<iommu_set_dom0_mapping>> iommu_map_page(d, pfn * tmp + j, pfn * tmp + j,
+ */
 int iommu_map_page(struct domain *d, unsigned long gfn, unsigned long mfn,
                    unsigned int flags)
 {
@@ -465,6 +636,9 @@ int iommu_map_page(struct domain *d, unsigned long gfn, unsigned long mfn,
     if ( !iommu_enabled || !hd->platform_ops )
         return 0;
 
+    /*
+     * intel_iommu_map_page()
+     */
     rc = hd->platform_ops->map_page(d, gfn, mfn, flags);
     if ( unlikely(rc) )
     {
@@ -641,6 +815,10 @@ int __init iommu_setup(void)
     return rc;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/iommu.c|824| <<iommu_do_domctl(XEN_DOMCTL_get_device_group)>> ret = iommu_get_device_group(d, seg, bus, devfn, sdevs, max_sdevs);
+ */
 static int iommu_get_device_group(
     struct domain *d, u16 seg, u8 bus, u8 devfn,
     XEN_GUEST_HANDLE_64(uint32) buf, int max_sdevs)
@@ -694,6 +872,13 @@ void iommu_update_ire_from_apic(
     ops->update_ire_from_apic(apic, reg, value);
 }
 
+/*
+ * called by:
+ *   - arch/x86/hpet.c|263| <<hpet_msi_write>> int rc = iommu_update_ire_from_msi(&ch->msi, msg);
+ *   - arch/x86/hpet.c|364| <<hpet_setup_msi_irq>> iommu_update_ire_from_msi(&ch->msi, NULL);
+ *   - arch/x86/msi.c|223| <<write_msi_msg>> rc = iommu_update_ire_from_msi(entry, msg);
+ *   - arch/x86/msi.c|518| <<msi_free_irq>> iommu_update_ire_from_msi(entry + nr, NULL);
+ */
 int iommu_update_ire_from_msi(
     struct msi_desc *msi_desc, struct msi_msg *msg)
 {
@@ -701,6 +886,11 @@ int iommu_update_ire_from_msi(
     return iommu_intremap ? ops->update_ire_from_msi(msi_desc, msg) : 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hpet.c|282| <<hpet_msi_read>> iommu_read_msi_from_ire(&ch->msi, msg);
+ *   - arch/x86/msi.c|211| <<read_msi_msg>> iommu_read_msi_from_ire(entry, msg);
+ */
 void iommu_read_msi_from_ire(
     struct msi_desc *msi_desc, struct msi_msg *msg)
 {
@@ -728,6 +918,10 @@ void iommu_resume()
         ops->resume();
 }
 
+/*
+ * called by:
+ *   - arch/x86/acpi/power.c|56| <<device_power_down>> iommu_suspend();
+ */
 void iommu_suspend()
 {
     const struct iommu_ops *ops = iommu_get_ops();
@@ -735,6 +929,10 @@ void iommu_suspend()
         ops->suspend();
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/p2m.c|415| <<p2m_alloc_table>> iommu_share_p2m_table(d);
+ */
 void iommu_share_p2m_table(struct domain* d)
 {
     const struct iommu_ops *ops = iommu_get_ops();
@@ -743,9 +941,18 @@ void iommu_share_p2m_table(struct domain* d)
         ops->share_p2m(d);
 }
 
+/*
+ * called by:
+ *   - arch/x86/crash.c|188| <<nmi_shootdown_cpus>> iommu_crash_shutdown();
+ */
 void iommu_crash_shutdown(void)
 {
+    /* 返回intel_iommu_ops或者amd_iommu_ops */
     const struct iommu_ops *ops = iommu_get_ops();
+    /*
+     * intel: vtd_crash_shutdown()
+     * amd  : amd_iommu_suspend()
+     */
     if ( iommu_enabled )
         ops->crash_shutdown();
     iommu_enabled = iommu_intremap = 0;
@@ -955,6 +1162,11 @@ int iommu_do_domctl(
     return ret;
 }
 
+/*
+ * 在支持shared p2m的机器上的结果:
+ * # xm debug-keys o
+ * (XEN) [2020-06-02 16:11:55] domain14 IOMMU p2m table shared with MMU:
+ */
 static void iommu_dump_p2m_table(unsigned char key)
 {
     struct domain *d;
diff --git a/xen/drivers/passthrough/pci.c b/xen/drivers/passthrough/pci.c
index 8514b1a437..506cc3cff8 100644
--- a/xen/drivers/passthrough/pci.c
+++ b/xen/drivers/passthrough/pci.c
@@ -47,6 +47,17 @@ struct pci_seg {
 };
 
 spinlock_t pcidevs_lock = SPIN_LOCK_UNLOCKED;
+/*
+ * called by:
+ *   - drivers/passthrough/pci.c|54| <<get_pseg>> return radix_tree_lookup(&pci_segments, seg);
+ *   - drivers/passthrough/pci.c|77| <<alloc_pseg>> if ( radix_tree_insert(&pci_segments, seg, pseg) )
+ *   - drivers/passthrough/pci.c|95| <<pci_segments_iterate>> if ( !radix_tree_gang_lookup(&pci_segments, (void **)&pseg, seg, 1) )
+ *   - drivers/passthrough/pci.c|106| <<pt_pci_init>> radix_tree_init(&pci_segments);
+ *   - drivers/passthrough/pci.c|465| <<pci_get_pdev>> radix_tree_gang_lookup(&pci_segments, (void **)&pseg, 0, 1);
+ *   - drivers/passthrough/pci.c|475| <<pci_get_pdev>> } while ( radix_tree_gang_lookup(&pci_segments, (void **)&pseg,
+ *   - drivers/passthrough/pci.c|514| <<pci_get_pdev_by_domain>> radix_tree_gang_lookup(&pci_segments, (void **)&pseg, 0, 1);
+ *   - drivers/passthrough/pci.c|525| <<pci_get_pdev_by_domain>> } while ( radix_tree_gang_lookup(&pci_segments, (void **)&pseg,
+ */
 static struct radix_tree_root pci_segments;
 
 static inline struct pci_seg *get_pseg(u16 seg)
@@ -59,6 +70,13 @@ bool_t pci_known_segment(u16 seg)
     return get_pseg(seg) != NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/pci.c|107| <<pt_pci_init>> if ( !alloc_pseg(0) )
+ *   - drivers/passthrough/pci.c|113| <<pci_add_segment>> return alloc_pseg(seg) ? 0 : -ENOMEM;
+ *   - drivers/passthrough/pci.c|427| <<pci_ro_device>> struct pci_seg *pseg = alloc_pseg(seg);
+ *   - drivers/passthrough/pci.c|606| <<pci_add_device>> pseg = alloc_pseg(seg);
+ */
 static struct pci_seg *alloc_pseg(u16 seg)
 {
     struct pci_seg *pseg = get_pseg(seg);
@@ -101,6 +119,10 @@ static int pci_segments_iterate(
     return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/setup.c|1428| <<__start_xen>> pt_pci_init();
+ */
 void __init pt_pci_init(void)
 {
     radix_tree_init(&pci_segments);
@@ -263,6 +285,13 @@ static void check_pdev(const struct pci_dev *pdev)
 #undef PCI_STATUS_CHECK
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/pci.c|414| <<pci_hide_device>> pdev = alloc_pdev(get_pseg(0), bus, devfn);
+ *   - drivers/passthrough/pci.c|432| <<pci_ro_device>> pdev = alloc_pdev(pseg, bus, devfn);
+ *   - drivers/passthrough/pci.c|609| <<pci_add_device>> pdev = alloc_pdev(pseg, bus, devfn);
+ *   - drivers/passthrough/pci.c|965| <<_scan_pci_devices>> pdev = alloc_pdev(pseg, bus, PCI_DEVFN(dev, func));
+ */
 static struct pci_dev *alloc_pdev(struct pci_seg *pseg, u8 bus, u8 devfn)
 {
     struct pci_dev *pdev;
@@ -499,6 +528,17 @@ struct pci_dev *pci_get_real_pdev(int seg, int bus, int devfn)
     return pdev;
 }
 
+/*
+ * called by:
+ *   - arch/x86/irq.c|1968| <<map_domain_pirq>> pdev = pci_get_pdev_by_domain(d, msi->seg, msi->bus, msi->devfn);
+ *   - drivers/passthrough/iommu.c|278| <<device_assigned_to_domain>> if ( pci_get_pdev_by_domain(d, seg, bus, devfn) )
+ *   - drivers/passthrough/iommu.c|348| <<assign_device>> pdev = pci_get_pdev_by_domain(dom0, seg, bus, devfn);
+ *   - drivers/passthrough/iommu.c|573| <<deassign_device>> pdev = pci_get_pdev_by_domain(d, seg, bus, devfn);
+ *   - arch/x86/irq.c|1968| <<map_domain_pirq>> pdev = pci_get_pdev_by_domain(d, msi->seg, msi->bus, msi->devfn);
+ *   - drivers/passthrough/iommu.c|278| <<device_assigned_to_domain>> if ( pci_get_pdev_by_domain(d, seg, bus, devfn) )
+ *   - drivers/passthrough/iommu.c|348| <<assign_device>> pdev = pci_get_pdev_by_domain(dom0, seg, bus, devfn);
+ *   - drivers/passthrough/iommu.c|573| <<deassign_device>> pdev = pci_get_pdev_by_domain(d, seg, bus, devfn);
+ */
 struct pci_dev *pci_get_pdev_by_domain(
     struct domain *d, int seg, int bus, int devfn)
 {
@@ -565,6 +605,13 @@ static void pci_enable_acs(struct pci_dev *pdev)
     pci_conf_write16(seg, bus, dev, func, pos + PCI_ACS_CTRL, ctrl);
 }
 
+/*
+ * called by:
+ *   - arch/x86/physdev.c|569| <<do_physdev_op(PHYSDEVOP_manage_pci_add)>> ret = pci_add_device(0, manage_pci.bus, manage_pci.devfn, NULL, -1);
+ *   - arch/x86/physdev.c|599| <<do_physdev_op(PHYSDEVOP_manage_pci_add_ext)>> ret = pci_add_device(0, manage_pci_ext.bus,
+ *   - arch/x86/physdev.c|635| <<do_physdev_op(PHYSDEVOP_pci_device_add)>> ret = pci_add_device(add.seg, add.bus, add.devfn, &pdev_info, node);
+ *   - drivers/passthrough/pci.c|590| <<pci_add_device>> pci_add_device(seg, info->physfn.bus, info->physfn.devfn, NULL, node);
+ */
 int pci_add_device(u16 seg, u8 bus, u8 devfn, const struct pci_dev_info *info, int node)
 {
     struct pci_seg *pseg;
@@ -936,6 +983,10 @@ void pci_check_disable_device(u16 seg, u8 bus, u8 devfn)
  * scan pci devices to add all existed PCI devices to alldevs_list,
  * and setup pci hierarchy in array bus2bridge.
  */
+/*
+ * 在以下使用_scan_pci_devices():
+ *   - drivers/passthrough/pci.c|979| <<scan_pci_devices>> ret = pci_segments_iterate(_scan_pci_devices, NULL);
+ */
 static int __init _scan_pci_devices(struct pci_seg *pseg, void *arg)
 {
     struct pci_dev *pdev;
@@ -971,6 +1022,11 @@ static int __init _scan_pci_devices(struct pci_seg *pseg, void *arg)
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/amd/pci_amd_iommu.c|223| <<amd_iov_detect>> return scan_pci_devices();
+ *   - drivers/passthrough/vtd/iommu.c|2315| <<intel_vtd_setup>> scan_pci_devices();
+ */
 int __init scan_pci_devices(void)
 {
     int ret;
@@ -1008,6 +1064,10 @@ static void setup_one_dom0_device(const struct setup_dom0 *ctxt,
               PCI_SLOT(devfn) == PCI_SLOT(pdev->devfn) );
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/pci.c|1067| <<setup_dom0_pci_devices>> pci_segments_iterate(_setup_dom0_pci_devices, &ctxt);
+ */
 static int __init _setup_dom0_pci_devices(struct pci_seg *pseg, void *arg)
 {
     struct setup_dom0 *ctxt = arg;
diff --git a/xen/drivers/passthrough/vtd/dmar.c b/xen/drivers/passthrough/vtd/dmar.c
index f5b85f2ad3..f7727a1713 100644
--- a/xen/drivers/passthrough/vtd/dmar.c
+++ b/xen/drivers/passthrough/vtd/dmar.c
@@ -41,6 +41,24 @@
 #define MIN_SCOPE_LEN (sizeof(struct acpi_dmar_device_scope) + \
                        sizeof(struct acpi_dmar_pci_path))
 
+/*
+ * 在以下使用acpi_drhd_units:
+ *   - drivers/passthrough/vtd/dmar.c|71| <<acpi_register_drhd_unit>> list_add_tail(&drhd->list, &acpi_drhd_units);
+ *   - drivers/passthrough/vtd/dmar.c|73| <<acpi_register_drhd_unit>> list_add(&drhd->list, &acpi_drhd_units);
+ *   - drivers/passthrough/vtd/dmar.c|99| <<disable_all_dmar_units>> list_for_each_entry_safe ( drhd, _drhd, &acpi_drhd_units, list )
+ *   - drivers/passthrough/vtd/dmar.c|133| <<ioapic_to_drhd>> list_for_each_entry( drhd, &acpi_drhd_units, list )
+ *   - drivers/passthrough/vtd/dmar.c|146| <<iommu_to_drhd>> list_for_each_entry( drhd, &acpi_drhd_units, list )
+ *   - drivers/passthrough/vtd/dmar.c|157| <<ioapic_to_iommu>> list_for_each_entry( drhd, &acpi_drhd_units, list )
+ *   - drivers/passthrough/vtd/dmar.c|178| <<hpet_to_drhd>> list_for_each_entry( drhd, &acpi_drhd_units, list )
+ *   - drivers/passthrough/vtd/dmar.c|230| <<acpi_find_matched_drhd_unit>> list_for_each_entry ( drhd, &acpi_drhd_units, list )
+ *   - drivers/passthrough/vtd/dmar.h|95| <<for_each_drhd_unit>> list_for_each_entry(drhd, &acpi_drhd_units, list)
+ *   - drivers/passthrough/vtd/intremap.c|152| <<iommu_supports_eim>> if ( !iommu_qinval || !iommu_intremap || list_empty(&acpi_drhd_units) )
+ *   - drivers/passthrough/vtd/iommu.c|1020| <<do_iommu_page_fault>> if ( list_empty(&acpi_drhd_units) )
+ *   - drivers/passthrough/vtd/iommu.c|1768| <<iommu_domain_teardown>> if ( list_empty(&acpi_drhd_units) )
+ *   - drivers/passthrough/vtd/iommu.c|2238| <<intel_vtd_setup>> if ( list_empty(&acpi_drhd_units) )
+ *   - drivers/passthrough/vtd/iommu.c|2342| <<intel_iommu_assign_device>> if ( list_empty(&acpi_drhd_units) )
+ *   - drivers/passthrough/vtd/iommu.c|2571| <<vtd_dump_p2m_table>> if ( list_empty(&acpi_drhd_units) )
+ */
 LIST_HEAD_READ_MOSTLY(acpi_drhd_units);
 LIST_HEAD_READ_MOSTLY(acpi_rmrr_units);
 static LIST_HEAD_READ_MOSTLY(acpi_atsr_units);
@@ -455,6 +473,10 @@ static int __init acpi_dmar_check_length(
     return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/dmar.c|842| <<acpi_parse_dmar>> ret = acpi_parse_one_drhd(entry_header);
+ */
 static int __init
 acpi_parse_one_drhd(struct acpi_dmar_header *header)
 {
@@ -869,6 +891,10 @@ out:
 /* SINIT saved in SinitMleData in TXT heap (which is DMA protected) */
 #define parse_dmar_table(h) tboot_parse_dmar_table(h)
 
+/*
+ * called by:
+ *   - arch/x86/acpi/boot.c|747| <<acpi_boot_init>> acpi_dmar_init();
+ */
 int __init acpi_dmar_init(void)
 {
     acpi_physical_address dmar_addr;
diff --git a/xen/drivers/passthrough/vtd/dmar.h b/xen/drivers/passthrough/vtd/dmar.h
index 479dbeff60..b2b64cf204 100644
--- a/xen/drivers/passthrough/vtd/dmar.h
+++ b/xen/drivers/passthrough/vtd/dmar.h
@@ -118,6 +118,25 @@ struct acpi_atsr_unit *acpi_find_matched_atsr_unit(const struct pci_dev *);
           __FILE__, __LINE__, __func__);
 #endif
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/intremap.c|768| <<enable_intremap>> IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
+ *   - drivers/passthrough/vtd/intremap.c|780| <<enable_intremap>> IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
+ *   - drivers/passthrough/vtd/intremap.c|803| <<disable_intremap>> IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
+ *   - drivers/passthrough/vtd/intremap.c|822| <<disable_intremap>> IOMMU_WAIT_OP(iommu, DMAR_IRTA_REG, dmar_readl,
+ *   - drivers/passthrough/vtd/iommu.c|333| <<iommu_flush_write_buffer>> IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
+ *   - drivers/passthrough/vtd/iommu.c|385| <<flush_context_reg>> IOMMU_WAIT_OP(iommu, DMAR_CCMD_REG, dmar_readq,
+ *   - drivers/passthrough/vtd/iommu.c|475| <<flush_iotlb_reg>> IOMMU_WAIT_OP(iommu, (tlb_offset + 8), dmar_readq,
+ *   - drivers/passthrough/vtd/iommu.c|728| <<iommu_set_root_entry>> IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
+ *   - drivers/passthrough/vtd/iommu.c|764| <<iommu_enable_translation>> IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
+ *   - drivers/passthrough/vtd/iommu.c|788| <<iommu_disable_translation>> IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
+ *   - drivers/passthrough/vtd/qinval.c|485| <<enable_qinval>> IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
+ *   - drivers/passthrough/vtd/qinval.c|508| <<disable_qinval>> IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
+ *   - drivers/passthrough/vtd/utils.c|52| <<disable_pmr>> IOMMU_WAIT_OP(iommu, DMAR_PMEN_REG, dmar_readl,
+ *
+ * 根据代码分析, 在timeout的条件下, 要么panic, 要么break退出
+ * 不会hang!!!
+ */
 #define IOMMU_WAIT_OP(iommu, offset, op, cond, sts) \
 do {                                                \
     s_time_t start_time = NOW();                    \
diff --git a/xen/drivers/passthrough/vtd/intremap.c b/xen/drivers/passthrough/vtd/intremap.c
index 2f8b9ebc23..977082e9b4 100644
--- a/xen/drivers/passthrough/vtd/intremap.c
+++ b/xen/drivers/passthrough/vtd/intremap.c
@@ -59,6 +59,10 @@
 /* apic_pin_2_ir_idx[apicid][pin] = interrupt remapping table index */
 static int **apic_pin_2_ir_idx;
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/intremap.c|811| <<enable_intremap>> return init_apic_pin_2_ir_idx();
+ */
 static int init_apic_pin_2_ir_idx(void)
 {
     int *_apic_pin_2_ir_idx;
@@ -197,6 +201,12 @@ static void free_remap_entry(struct iommu *iommu, int index)
  * Look for a free intr remap entry (or a contiguous set thereof).
  * Need hold iremap_lock, and setup returned entry before releasing lock.
  */
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/intremap.c|298| <<ioapic_rte_to_remap_entry>> index = alloc_remap_entry(iommu, 1);
+ *   - drivers/passthrough/vtd/intremap.c|584| <<msi_msg_to_remap_entry>> index = alloc_remap_entry(iommu, nr);
+ *   - drivers/passthrough/vtd/intremap.c|697| <<intel_setup_hpet_msi>> msi_desc->remap_index = alloc_remap_entry(iommu, 1);
+ */
 static unsigned int alloc_remap_entry(struct iommu *iommu, unsigned int nr)
 {
     struct iremap_entry *iremap_entries = NULL;
@@ -487,6 +497,10 @@ static void set_msi_source_id(struct pci_dev *pdev, struct iremap_entry *ire)
    }
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/intremap.c|662| <<msi_msg_read_remap_rte>> remap_entry_to_msi_msg(drhd->iommu, msg,
+ */
 static int remap_entry_to_msi_msg(
     struct iommu *iommu, struct msi_msg *msg, unsigned int index)
 {
@@ -550,6 +564,13 @@ static int remap_entry_to_msi_msg(
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/iommu.c|838| <<iommu_update_ire_from_msi>> return iommu_intremap ? ops->update_ire_from_msi(msi_desc, msg) : 0;
+ *
+ * called by:
+ *    - drivers/passthrough/vtd/intremap.c|682| <<msi_msg_write_remap_rte>> return drhd ? msi_msg_to_remap_entry(drhd->iommu, pdev, msi_desc, msg)
+ */
 static int msi_msg_to_remap_entry(
     struct iommu *iommu, struct pci_dev *pdev,
     struct msi_desc *msi_desc, struct msi_msg *msg)
@@ -596,6 +617,13 @@ static int msi_msg_to_remap_entry(
         return -EFAULT;
     }
 
+    /*
+     * Get the intr remap entry:
+     * maddr   - machine addr of the table
+     * index   - index of the entry
+     * entries - return addr of the page holding this entry, need unmap it
+     * entry   - return required entry
+     */
     GET_IREMAP_ENTRY(ir_ctrl->iremap_maddr, index,
                      iremap_entries, iremap_entry);
 
@@ -650,6 +678,9 @@ static int msi_msg_to_remap_entry(
     return 0;
 }
 
+/*
+ * struct iommu_ops intel_iommu_ops.read_msi_from_ire = msi_msg_read_remap_rte()
+ */
 void msi_msg_read_remap_rte(
     struct msi_desc *msi_desc, struct msi_msg *msg)
 {
@@ -664,6 +695,9 @@ void msi_msg_read_remap_rte(
                                ? msi_desc->msi_attrib.entry_nr : 0);
 }
 
+/*
+ * struct iommu_ops intel_iommu_ops.update_ire_from_msi = msi_msg_write_remap_rte()
+ */
 int msi_msg_write_remap_rte(
     struct msi_desc *msi_desc, struct msi_msg *msg)
 {
@@ -702,6 +736,11 @@ int __init intel_setup_hpet_msi(struct msi_desc *msi_desc)
     return rc;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/intremap.c|922| <<iommu_enable_x2apic_IR>> if ( enable_intremap(iommu, 1) )
+ *   - drivers/passthrough/vtd/iommu.c|2389| <<init_vtd_hw>> if ( enable_intremap(iommu, 0) != 0 )
+ */
 int enable_intremap(struct iommu *iommu, int eim)
 {
     struct acpi_drhd_unit *drhd;
@@ -758,6 +797,10 @@ int enable_intremap(struct iommu *iommu, int eim)
 
     /* set size of the interrupt remapping table */
     ir_ctrl->iremap_maddr |= IRTA_REG_TABLE_SIZE;
+    /*
+     * 在以下设置ir_ctrl:
+     *   - drivers/passthrough/vtd/intremap.c|771| <<enable_intremap>> ir_ctrl->iremap_maddr = alloc_pgtable_maddr(drhd, IREMAP_ARCH_PAGE_NR);
+     */
     dmar_writeq(iommu->reg, DMAR_IRTA_REG, ir_ctrl->iremap_maddr);
 
     /* set SIRTP */
@@ -784,6 +827,13 @@ int enable_intremap(struct iommu *iommu, int eim)
     return init_apic_pin_2_ir_idx();
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/intremap.c|855| <<iommu_enable_x2apic_IR>> disable_intremap(iommu);
+ *   - drivers/passthrough/vtd/intremap.c|898| <<iommu_disable_x2apic_IR>> disable_intremap(drhd->iommu);
+ *   - drivers/passthrough/vtd/iommu.c|2165| <<init_vtd_hw>> disable_intremap(drhd->iommu);
+ *   - drivers/passthrough/vtd/iommu.c|2429| <<vtd_crash_shutdown>> disable_intremap(drhd->iommu);
+ */
 void disable_intremap(struct iommu *iommu)
 {
     u32 sts;
@@ -800,6 +850,10 @@ void disable_intremap(struct iommu *iommu)
 
     dmar_writel(iommu->reg, DMAR_GCMD_REG, sts & (~DMA_GCMD_IRE));
 
+    /*
+     * 根据代码分析, 在timeout的条件下, 要么panic, 要么break退出
+     * 不会hang!!!
+     */
     IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
                   !(sts & DMA_GSTS_IRES), sts);
 
@@ -819,6 +873,10 @@ void disable_intremap(struct iommu *iommu)
         goto out;
 
     dmar_writel(iommu->reg, DMAR_IRTA_REG, irta & ~IRTA_EIME);
+    /*
+     * 根据代码分析, 在timeout的条件下, 要么panic, 要么break退出
+     * 不会hang!!!
+     */
     IOMMU_WAIT_OP(iommu, DMAR_IRTA_REG, dmar_readl,
                   !(irta & IRTA_EIME), irta);
 
@@ -830,6 +888,11 @@ out:
  * This function is used to enable Interrupt remapping when
  * enable x2apic
  */
+/*
+ * called by:
+ *   - arch/x86/apic.c|526| <<resume_x2apic>> iommu_enable_x2apic_IR();
+ *   - arch/x86/apic.c|969| <<x2apic_bsp_setup>> if ( iommu_enable_x2apic_IR() )
+ */
 int iommu_enable_x2apic_IR(void)
 {
     struct acpi_drhd_unit *drhd;
@@ -887,6 +950,10 @@ int iommu_enable_x2apic_IR(void)
  * This function is used to disable Interrutp remapping when
  * suspend local apic
  */
+/*
+ * called by:
+ *   - arch/x86/apic.c|744| <<lapic_suspend>> iommu_disable_x2apic_IR();
+ */
 void iommu_disable_x2apic_IR(void)
 {
     struct acpi_drhd_unit *drhd;
diff --git a/xen/drivers/passthrough/vtd/iommu.c b/xen/drivers/passthrough/vtd/iommu.c
index 2036d56301..e4e7b6371b 100644
--- a/xen/drivers/passthrough/vtd/iommu.c
+++ b/xen/drivers/passthrough/vtd/iommu.c
@@ -52,13 +52,31 @@ struct mapped_rmrr {
 /* Possible unfiltered LAPIC/MSI messages from untrusted sources? */
 bool_t __read_mostly untrusted_msi;
 
+/*
+ * 在以下使用nr_iommus:
+ *   - drivers/passthrough/vtd/iommu.c|1188| <<iommu_alloc>> if ( nr_iommus > MAX_IOMMUS )
+ *   - drivers/passthrough/vtd/iommu.c|1191| <<iommu_alloc>> "IOMMU: nr_iommus %d > MAX_IOMMUS\n", nr_iommus);
+ *   - drivers/passthrough/vtd/iommu.c|1216| <<iommu_alloc>> iommu->index = nr_iommus++;
+ */
 int nr_iommus;
 
+/*
+ * 在以下使用vtd_fault_tasklet:
+ *   - drivers/passthrough/vtd/iommu.c|1056| <<iommu_page_fault>> tasklet_schedule(&vtd_fault_tasklet);
+ *   - drivers/passthrough/vtd/iommu.c|2335| <<intel_vtd_setup>> softirq_tasklet_init(&vtd_fault_tasklet, do_iommu_page_fault, 0);
+ */
 static struct tasklet vtd_fault_tasklet;
 
 static int setup_dom0_device(u8 devfn, struct pci_dev *);
 static void setup_dom0_rmrr(struct domain *d);
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|641| <<__intel_iommu_iotlb_flush>> iommu_domid= domain_iommu_domid(d, iommu);
+ *   - drivers/passthrough/vtd/iommu.c|1634| <<domain_context_unmap_one>> iommu_domid= domain_iommu_domid(domain, iommu);
+ *   - drivers/passthrough/vtd/iommu.c|1761| <<domain_context_unmap>> iommu_domid = domain_iommu_domid(domain, iommu);
+ *   - drivers/passthrough/vtd/iommu.c|1914| <<iommu_pte_flush>> iommu_domid= domain_iommu_domid(d, iommu);
+ */
 static int domain_iommu_domid(struct domain *d,
                               struct iommu *iommu)
 {
@@ -119,6 +137,10 @@ static int context_set_domain_id(struct context_entry *context,
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1428| <<domain_context_mapping_one>> cdomain = context_get_domain_id(context, iommu);
+ */
 static int context_get_domain_id(struct context_entry *context,
                                  struct iommu *iommu)
 {
@@ -186,6 +208,15 @@ void iommu_flush_cache_page(void *addr, unsigned long npages)
 }
 
 /* Allocate page table, return its machine address */
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/intremap.c|751| <<enable_intremap>> ir_ctrl->iremap_maddr = alloc_pgtable_maddr(drhd, IREMAP_ARCH_PAGE_NR);
+ *   - drivers/passthrough/vtd/iommu.c|265| <<bus_to_context_maddr>> maddr = alloc_pgtable_maddr(drhd, 1);
+ *   - drivers/passthrough/vtd/iommu.c|307| <<addr_to_dma_page_maddr>> if ( !alloc || ((hd->pgd_maddr = alloc_pgtable_maddr(drhd, 1)) == 0) )
+ *   - drivers/passthrough/vtd/iommu.c|325| <<addr_to_dma_page_maddr>> pte_maddr = alloc_pgtable_maddr(drhd, 1);
+ *   - drivers/passthrough/vtd/iommu.c|1259| <<iommu_alloc>> if ( !(iommu->root_maddr = alloc_pgtable_maddr(drhd, 1)) )
+ *   - drivers/passthrough/vtd/qinval.c|470| <<enable_qinval>> qi_ctrl->qinval_maddr = alloc_pgtable_maddr(drhd, QINVAL_ARCH_PAGE_NR);
+ */
 u64 alloc_pgtable_maddr(struct acpi_drhd_unit *drhd, unsigned long npages)
 {
     struct acpi_rhsa_unit *rhsa;
@@ -223,6 +254,11 @@ void free_pgtable_maddr(u64 maddr)
 }
 
 /* context entry handling */
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1403| <<domain_context_mapping_one>> maddr = bus_to_context_maddr(iommu, bus);
+ *   - drivers/passthrough/vtd/iommu.c|1634| <<domain_context_unmap_one>> maddr = bus_to_context_maddr(iommu, bus);
+ */
 static u64 bus_to_context_maddr(struct iommu *iommu, u8 bus)
 {
     struct acpi_drhd_unit *drhd;
@@ -250,6 +286,12 @@ static u64 bus_to_context_maddr(struct iommu *iommu, u8 bus)
     return maddr;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|699| <<dma_pte_clear_one>> pg_maddr = addr_to_dma_page_maddr(domain, addr, 0);
+ *   - drivers/passthrough/vtd/iommu.c|1518| <<domain_context_mapping_one>> addr_to_dma_page_maddr(domain, 0, 1);
+ *   - drivers/passthrough/vtd/iommu.c|1919| <<intel_iommu_map_page>> pg_maddr = addr_to_dma_page_maddr(d, (paddr_t)gfn << PAGE_SHIFT_4K, 1);
+ */
 static u64 addr_to_dma_page_maddr(struct domain *domain, u64 addr, int alloc)
 {
     struct acpi_drhd_unit *drhd;
@@ -257,6 +299,10 @@ static u64 addr_to_dma_page_maddr(struct domain *domain, u64 addr, int alloc)
     struct hvm_iommu *hd = domain_hvm_iommu(domain);
     int addr_width = agaw_to_width(hd->agaw);
     struct dma_pte *parent, *pte = NULL;
+    /*
+     * struct hvm_iommu *hd:
+     *   -> int agaw; // adjusted guest address width, 0 is level 2 30-bit/
+     */
     int level = agaw_to_level(hd->agaw);
     int offset;
     u64 pte_maddr = 0;
@@ -382,6 +428,10 @@ static int flush_context_reg(
     dmar_writeq(iommu->reg, DMAR_CCMD_REG, val);
 
     /* Make sure hardware complete it */
+    /*
+     * 根据代码分析, 在timeout的条件下, 要么panic, 要么break退出
+     * 不会hang!!!
+     */
     IOMMU_WAIT_OP(iommu, DMAR_CCMD_REG, dmar_readq,
                   !(val & DMA_CCMD_ICC), val);
 
@@ -390,10 +440,21 @@ static int flush_context_reg(
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|556| <<iommu_flush_all>> iommu_flush_context_global(iommu, 0);
+ */
 static int iommu_flush_context_global(
     struct iommu *iommu, int flush_non_present_entry)
 {
     struct iommu_flush *flush = iommu_get_flush(iommu);
+    /*
+     * 在以下设置context:
+     *   - drivers/passthrough/vtd/iommu.c|2120| <<init_vtd_hw>> flush->context = flush_context_reg;
+     *   - drivers/passthrough/vtd/qinval.c|463| <<enable_qinval>> flush->context = flush_context_qi;
+     *
+     * 更大的可能是flush_context_qi()
+     */
     return flush->context(iommu, 0, 0, 0, DMA_CCMD_GLOBAL_INVL,
                                  flush_non_present_entry);
 }
@@ -475,6 +536,10 @@ static int flush_iotlb_reg(void *_iommu, u16 did,
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|558| <<iommu_flush_all>> iommu_flush_iotlb_global(iommu, 0, flush_dev_iotlb);
+ */
 static int iommu_flush_iotlb_global(struct iommu *iommu,
     int flush_non_present_entry, int flush_dev_iotlb)
 {
@@ -484,6 +549,13 @@ static int iommu_flush_iotlb_global(struct iommu *iommu,
     /* apply platform specific errata workarounds */
     vtd_ops_preamble_quirk(iommu);
 
+    /*
+     * 在以下设置iotlb:
+     *   - drivers/passthrough/vtd/iommu.c|2113| <<init_vtd_hw>> flush->iotlb = flush_iotlb_reg;
+     *   - drivers/passthrough/vtd/qinval.c|464| <<enable_qinval>> flush->iotlb = flush_iotlb_qi;
+     *
+     * 更大的可能是flush_iotlb_qi()
+     */
     status = flush->iotlb(iommu, 0, 0, 0, DMA_TLB_GLOBAL_FLUSH,
                         flush_non_present_entry, flush_dev_iotlb);
 
@@ -543,18 +615,39 @@ static int iommu_flush_iotlb_psi(
     return status;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1269| <<intel_iommu_dom0_init>> iommu_flush_all();
+ *   - drivers/passthrough/vtd/iommu.c|2155| <<init_vtd_hw>> iommu_flush_all();
+ *   - drivers/passthrough/vtd/iommu.c|2344| <<vtd_suspend>> iommu_flush_all();
+ *   - drivers/passthrough/vtd/iommu.c|2383| <<vtd_crash_shutdown>> iommu_flush_all();
+ */
 static void iommu_flush_all(void)
 {
     struct acpi_drhd_unit *drhd;
     struct iommu *iommu;
     int flush_dev_iotlb;
 
+    /*
+     * 在以下调用flush_all_cache():
+     *   - - drivers/passthrough/vtd/iommu.c|588| <<iommu_flush_all>> flush_all_cache();
+     *
+     * 就是一个wbinvd();
+     */
     flush_all_cache();
     for_each_drhd_unit ( drhd )
     {
         iommu = drhd->iommu;
+	/*
+	 * 在以下调用iommu_flush_context_global():
+	 *   - drivers/passthrough/vtd/iommu.c|556| <<iommu_flush_all>> iommu_flush_context_global(iommu, 0);
+	 */
         iommu_flush_context_global(iommu, 0);
         flush_dev_iotlb = find_ats_dev_drhd(iommu) ? 1 : 0;
+	/*
+	 * 在以下调用iommu_flush_iotlb_global():
+	 *   - drivers/passthrough/vtd/iommu.c|558| <<iommu_flush_all>> iommu_flush_iotlb_global(iommu, 0, flush_dev_iotlb);
+	 */
         iommu_flush_iotlb_global(iommu, 0, flush_dev_iotlb);
     }
 }
@@ -605,6 +698,9 @@ static void intel_iommu_iotlb_flush(struct domain *d, unsigned long gfn, unsigne
     __intel_iommu_iotlb_flush(d, gfn, 1, page_count);
 }
 
+/*
+ * iommu_ops intel_iommu_ops.iotlb_flush_all = intel_iommu_iotlb_flush_all()
+ */
 static void intel_iommu_iotlb_flush_all(struct domain *d)
 {
     __intel_iommu_iotlb_flush(d, 0, 0, 0);
@@ -685,6 +781,10 @@ static void iommu_free_page_table(struct page_info *pg)
     free_pgtable_maddr(pt_maddr);
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|2251| <<init_vtd_hw>> ret = iommu_set_root_entry(iommu);
+ */
 static int iommu_set_root_entry(struct iommu *iommu)
 {
     u32 sts;
@@ -704,6 +804,11 @@ static int iommu_set_root_entry(struct iommu *iommu)
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1358| <<intel_iommu_dom0_init>> iommu_enable_translation(drhd);
+ *   - drivers/passthrough/vtd/iommu.c|2575| <<vtd_resume>> iommu_enable_translation(drhd);
+ */
 static void iommu_enable_translation(struct acpi_drhd_unit *drhd)
 {
     u32 sts;
@@ -744,6 +849,11 @@ static void iommu_enable_translation(struct acpi_drhd_unit *drhd)
     disable_pmr(iommu);
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|2392| <<vtd_suspend>> iommu_disable_translation(iommu);
+ *   - drivers/passthrough/vtd/iommu.c|2428| <<vtd_crash_shutdown>> iommu_disable_translation(iommu);
+ */
 static void iommu_disable_translation(struct iommu *iommu)
 {
     u32 sts;
@@ -757,6 +867,10 @@ static void iommu_disable_translation(struct iommu *iommu)
     dmar_writel(iommu->reg, DMAR_GCMD_REG, sts & (~DMA_GCMD_TE));
 
     /* Make sure hardware complete it */
+    /*
+     * 根据代码分析, 在timeout的条件下, 要么panic, 要么break退出
+     * 不会hang!!!
+     */
     IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
                   !(sts & DMA_GSTS_TES), sts);
     spin_unlock_irqrestore(&iommu->register_lock, flags);
@@ -821,6 +935,10 @@ static const char *iommu_get_fault_reason(u8 fault_reason,
     }
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|995| <<__do_iommu_page_fault>> iommu_page_fault_do_one(iommu, type, fault_reason,
+ */
 static int iommu_page_fault_do_one(struct iommu *iommu, int type,
                                    u8 fault_reason, u16 source_id, u64 addr)
 {
@@ -887,6 +1005,10 @@ static void iommu_fault_status(u32 fault_status)
 }
 
 #define PRIMARY_FAULT_REG_LEN (16)
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1033| <<do_iommu_page_fault>> __do_iommu_page_fault(drhd->iommu);
+ */
 static void __do_iommu_page_fault(struct iommu *iommu)
 {
     int reg, fault_index;
@@ -957,6 +1079,10 @@ clear_overflow:
     }
 }
 
+/*
+ * 在以下使用do_iommu_page_fault():
+ *   - drivers/passthrough/vtd/iommu.c|2298| <<intel_vtd_setup>> softirq_tasklet_init(&vtd_fault_tasklet, do_iommu_page_fault, 0);
+ */
 static void do_iommu_page_fault(unsigned long data)
 {
     struct acpi_drhd_unit *drhd;
@@ -1111,6 +1237,10 @@ static int __init iommu_set_interrupt(struct acpi_drhd_unit *drhd)
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/dmar.c|505| <<acpi_parse_one_drhd>> ret = iommu_alloc(dmaru);
+ */
 int __init iommu_alloc(struct acpi_drhd_unit *drhd)
 {
     struct iommu *iommu;
@@ -1277,11 +1407,27 @@ static void __init intel_iommu_dom0_init(struct domain *d)
     }
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1557| <<domain_context_mapping>> ret = domain_context_mapping_one(domain, drhd->iommu, bus, devfn,
+ *   - drivers/passthrough/vtd/iommu.c|1570| <<domain_context_mapping>> ret = domain_context_mapping_one(domain, drhd->iommu, bus, devfn,
+ *   - drivers/passthrough/vtd/iommu.c|1578| <<domain_context_mapping>> ret = domain_context_mapping_one(domain, drhd->iommu, bus, devfn,
+ *   - drivers/passthrough/vtd/iommu.c|1588| <<domain_context_mapping>> ret = domain_context_mapping_one(domain, drhd->iommu, secbus, 0,
+ *   - drivers/passthrough/vtd/quirks.c|328| <<map_me_phantom_function>> domain_context_mapping_one(domain, drhd->iommu, 0,
+ */
 int domain_context_mapping_one(
     struct domain *domain,
     struct iommu *iommu,
     u8 bus, u8 devfn, const struct pci_dev *pdev)
 {
+    /*
+     * struct domain *domain:
+     *  -> struct arch_domain arch;
+     *      -> struct hvm_domain hvm_domain;
+     *          -> struct hvm_iommu hvm_iommu;
+     *
+     * 返回&d->arch.hvm_domain.hvm_iommu
+     */
     struct hvm_iommu *hd = domain_hvm_iommu(domain);
     struct context_entry *context, *context_entries;
     u64 maddr, pgd_maddr;
@@ -1291,9 +1437,18 @@ int domain_context_mapping_one(
     ASSERT(spin_is_locked(&pcidevs_lock));
     spin_lock(&iommu->lock);
     maddr = bus_to_context_maddr(iommu, bus);
+    /*
+     * struct context_entry *context, *context_entries;
+     */
     context_entries = (struct context_entry *)map_vtd_domain_page(maddr);
+    /*
+     * struct context_entry *context
+     */
     context = &context_entries[devfn];
 
+    /*
+     * 下面的if语句中, 怎么样都是错的
+     */
     if ( context_present(*context) )
     {
         int res = 0;
@@ -1302,6 +1457,11 @@ int domain_context_mapping_one(
          * not available, try to read it from the context itself. */
         if ( pdev )
         {
+            /*
+	     * 参数传进来的
+	     * struct pci_dev *pdev:
+	     *   -> struct domain *domain;
+	     */
             if ( pdev->domain != domain )
             {
                 printk(XENLOG_G_INFO VTDPREFIX
@@ -1336,11 +1496,29 @@ int domain_context_mapping_one(
             }
         }
 
+	/*
+	 * struct context_entry *context, *context_entries;
+	 */
         unmap_vtd_domain_page(context_entries);
         spin_unlock(&iommu->lock);
         return res;
     }
 
+    /*
+     * 在以下设置iommu_passthrough:
+     *   - drivers/passthrough/amd/iommu_init.c|1071| <<amd_iommu_init_cleanup>> iommu_passthrough = 0;
+     *   - drivers/passthrough/iommu.c|114| <<parse_iommu_param>> iommu_passthrough = val;
+     *   - drivers/passthrough/iommu.c|655| <<iommu_setup>> iommu_passthrough = 0;
+     *   - drivers/passthrough/iommu.c|673| <<iommu_setup>> iommu_passthrough = 0;
+     *   - drivers/passthrough/vtd/iommu.c|2358| <<intel_vtd_setup>> iommu_passthrough = 0;
+     *   - drivers/passthrough/vtd/iommu.c|2407| <<intel_vtd_setup>> iommu_passthrough = 0;
+     *
+     * drivers/passthrough/iommu.c|114| <<parse_iommu_param>> iommu_passthrough = val;
+     *  118         else if ( !strcmp(s, "dom0-passthrough") )
+     *  119             iommu_passthrough = val;
+     *
+     * Control whether to disable DMA remapping for Dom0.
+     */
     if ( iommu_passthrough && (domain->domain_id == 0) )
     {
         context_set_translation_type(*context, CONTEXT_TT_PASS_THRU);
@@ -1348,6 +1526,9 @@ int domain_context_mapping_one(
     }
     else
     {
+        /*
+	 * struct hvm_iommu *hd = domain_hvm_iommu(domain);
+	 */
         spin_lock(&hd->mapping_lock);
 
         /* Ensure we have pagetables allocated down to leaf PTE. */
@@ -1411,6 +1592,9 @@ int domain_context_mapping_one(
 
     set_bit(iommu->index, &hd->iommu_bitmap);
 
+    /*
+     * struct context_entry *context, *context_entries;
+     */
     unmap_vtd_domain_page(context_entries);
 
     if ( !seg )
@@ -1419,6 +1603,12 @@ int domain_context_mapping_one(
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1762| <<reassign_device_ownership>> ret = domain_context_mapping(target, devfn, pdev);
+ *   - drivers/passthrough/vtd/iommu.c|1984| <<intel_iommu_add_device>> ret = domain_context_mapping(pdev->domain, devfn, pdev);
+ *   - drivers/passthrough/vtd/iommu.c|2093| <<setup_dom0_device>> return domain_context_mapping(pdev->domain, devfn, pdev);
+ */
 static int domain_context_mapping(
     struct domain *domain, u8 devfn, const struct pci_dev *pdev)
 {
@@ -1672,6 +1862,14 @@ out:
     return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|2561| <<intel_iommu_assign_device>> ret = reassign_device_ownership(dom0, d, devfn, pdev);
+ *   - drivers/passthrough/iommu.c|685| <<deassign_device>> ret = hd->platform_ops->reassign_device(d, dom0, devfn, pdev);
+ *   - drivers/passthrough/iommu.c|695| <<deassign_device>> ret = hd->platform_ops->reassign_device(d, dom0, devfn, pdev);
+ *
+ * struct iommu_ops intel_iommu_ops.reassign_device = reassign_device_ownership()
+ */
 static int reassign_device_ownership(
     struct domain *source,
     struct domain *target,
@@ -1727,6 +1925,14 @@ void iommu_domain_teardown(struct domain *d)
     spin_unlock(&hd->mapping_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/iommu.c|195| <<iommu_dom0_init>> hd->platform_ops->map_page(d, gfn, mfn, mapping);
+ *   - drivers/passthrough/iommu.c|443| <<iommu_populate_page_table>> rc = hd->platform_ops->map_page(
+ *   - drivers/passthrough/iommu.c|528| <<iommu_map_page>> rc = hd->platform_ops->map_page(d, gfn, mfn, flags);
+ *
+ * struct iommu_ops intel_iommu_ops.map_page = intel_iommu_map_page()
+ */
 static int intel_iommu_map_page(
     struct domain *d, unsigned long gfn, unsigned long mfn,
     unsigned int flags)
@@ -1736,6 +1942,22 @@ static int intel_iommu_map_page(
     u64 pg_maddr;
 
     /* Do nothing if VT-d shares EPT page table */
+    /*
+     * 在以下调用iommu_use_hap_pt():
+     *   - arch/x86/mm/p2m-pt.c|426| <<p2m_pt_set_entry>> if ( iommu_use_hap_pt(p2m->domain) )
+     *   - drivers/passthrough/amd/iommu_map.c|662| <<amd_iommu_map_page>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/amd/iommu_map.c|750| <<amd_iommu_unmap_page>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/amd/iommu_map.c|823| <<amd_iommu_share_p2m>> if ( !iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/amd/pci_amd_iommu.c|464| <<deallocate_iommu_page_tables>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/iommu.c|376| <<assign_device>> if ( !iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/iommu.c|1068| <<iommu_dump_p2m_table>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/vtd/iommu.c|1911| <<iommu_domain_teardown>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/vtd/iommu.c|1937| <<intel_iommu_map_page>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/vtd/iommu.c|2044| <<iommu_set_pgd>> if ( !iommu_use_hap_pt(d) )
+     *
+     * 应该大部分都是true
+     * !! Do nothing if VT-d shares EPT page table !!
+     */
     if ( iommu_use_hap_pt(d) )
         return 0;
 
@@ -1836,6 +2058,12 @@ static int vtd_ept_page_compatible(struct iommu *iommu)
 /*
  * set VT-d page table directory to EPT table if allowed
  */
+/*
+ * called by:
+ *   - drivers/passthrough/iommu.c|871| <<iommu_share_p2m_table>> ops->share_p2m(d);
+ *
+ * struct iommu_ops intel_iommu_ops.share_p2m = iommu_set_pgd()
+ */
 void iommu_set_pgd(struct domain *d)
 {
     struct hvm_iommu *hd  = domain_hvm_iommu(d);
@@ -1843,6 +2071,22 @@ void iommu_set_pgd(struct domain *d)
 
     ASSERT( is_hvm_domain(d) && d->arch.hvm_domain.hap_enabled );
 
+    /*
+     * 在以下调用iommu_use_hap_pt():
+     *   - arch/x86/mm/p2m-pt.c|426| <<p2m_pt_set_entry>> if ( iommu_use_hap_pt(p2m->domain) )
+     *   - drivers/passthrough/amd/iommu_map.c|662| <<amd_iommu_map_page>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/amd/iommu_map.c|750| <<amd_iommu_unmap_page>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/amd/iommu_map.c|823| <<amd_iommu_share_p2m>> if ( !iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/amd/pci_amd_iommu.c|464| <<deallocate_iommu_page_tables>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/iommu.c|376| <<assign_device>> if ( !iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/iommu.c|1068| <<iommu_dump_p2m_table>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/vtd/iommu.c|1911| <<iommu_domain_teardown>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/vtd/iommu.c|1937| <<intel_iommu_map_page>> if ( iommu_use_hap_pt(d) )
+     *   - drivers/passthrough/vtd/iommu.c|2044| <<iommu_set_pgd>> if ( !iommu_use_hap_pt(d) )
+     *
+     * Does this domain have a P2M table we can use as its IOMMU pagetable?
+     * 应该大部分都是true
+     */
     if ( !iommu_use_hap_pt(d) )
         return;
 
@@ -1899,6 +2143,13 @@ static int rmrr_identity_mapping(struct domain *d,
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/iommu.c|262| <<iommu_add_device>> rc = hd->platform_ops->add_device(pdev->devfn, pdev);
+ *   - drivers/passthrough/iommu.c|271| <<iommu_add_device>> rc = hd->platform_ops->add_device(devfn, pdev);
+ *
+ * struct iommu_ops intel_iommu_ops.add_device = intel_iommu_add_device()
+ */
 static int intel_iommu_add_device(u8 devfn, struct pci_dev *pdev)
 {
     struct acpi_rmrr_unit *rmrr;
@@ -1934,6 +2185,9 @@ static int intel_iommu_add_device(u8 devfn, struct pci_dev *pdev)
     return ret;
 }
 
+/*
+ * struct iommu_ops intel_iommu_ops.enable_device = intel_iommu_enable_device()
+ */
 static int intel_iommu_enable_device(struct pci_dev *pdev)
 {
     struct acpi_drhd_unit *drhd = acpi_find_matched_drhd_unit(pdev);
@@ -2010,6 +2264,10 @@ static int intel_iommu_remove_device(u8 devfn, struct pci_dev *pdev)
     return domain_context_unmap(pdev->domain, devfn, pdev);
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1334| <<intel_iommu_dom0_init>> setup_dom0_pci_devices(d, setup_dom0_device);
+ */
 static int __init setup_dom0_device(u8 devfn, struct pci_dev *pdev)
 {
     return domain_context_mapping(pdev->domain, devfn, pdev);
@@ -2040,6 +2298,11 @@ static void adjust_irq_affinity(struct acpi_drhd_unit *drhd)
     dma_msi_set_affinity(irq_to_desc(drhd->iommu->msi.irq), cpumask);
 }
 
+/*
+ * called by:
+ *   - arch/x86/acpi/power.c|225| <<enter_state>> adjust_vtd_irq_affinities();
+ *   - drivers/passthrough/vtd/iommu.c|2123| <<global>> __initcall(adjust_vtd_irq_affinities);
+ */
 int adjust_vtd_irq_affinities(void)
 {
     struct acpi_drhd_unit *drhd;
@@ -2173,6 +2436,10 @@ static void __init setup_dom0_rmrr(struct domain *d)
     spin_unlock(&pcidevs_lock);
 }
 
+/*
+ * called by:
+ *   - include/asm-x86/hvm/iommu.h|32| <<iommu_hardware_setup>> return intel_vtd_setup();
+ */
 int __init intel_vtd_setup(void)
 {
     struct acpi_drhd_unit *drhd;
@@ -2253,6 +2520,13 @@ int __init intel_vtd_setup(void)
     P(iommu_passthrough, "Dom0 DMA Passthrough");
     P(iommu_qinval, "Queued Invalidation");
     P(iommu_intremap, "Interrupt Remapping");
+    /*
+     * Shared Virtual Memory: For devices supporting appropriate PCI-Express1 capabilities, OS may
+     * use the DMA remapping hardware capabilities to share virtual address space of application
+     * processes with I/O devices. Shared virtual memory along with support for I/O page-faults enable
+     * application programs to freely pass arbitrary data-structures to devices such as graphics
+     * processors or accelerators, without the overheads of pinning and marshalling of data.
+     */
     P(iommu_hap_pt_share, "Shared EPT tables");
 #undef P
 
@@ -2275,6 +2549,13 @@ int __init intel_vtd_setup(void)
     return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/iommu.c|441| <<assign_device>> if ( (rc = hd->platform_ops->assign_device(d, devfn, pdev)) )
+ *   - drivers/passthrough/iommu.c|449| <<assign_device>> rc = hd->platform_ops->assign_device(d, devfn, pdev);
+ *
+ * struct iommu_ops intel_iommu_ops.assign_device = intel_iommu_assign_device()
+ */
 static int intel_iommu_assign_device(
     struct domain *d, u8 devfn, struct pci_dev *pdev)
 {
@@ -2372,21 +2653,63 @@ static void vtd_suspend(void)
     }
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/iommu.c|759| <<iommu_crash_shutdown>> ops->crash_shutdown();
+ *
+ * struct iommu_ops intel_iommu_ops.crash_shutdown = vtd_crash_shutdown()
+ */
 static void vtd_crash_shutdown(void)
 {
     struct acpi_drhd_unit *drhd;
     struct iommu *iommu;
 
+    /*
+     * 在以下修改iommu_enable:
+     *   - drivers/passthrough/amd/iommu_init.c|1066| <<amd_iommu_init_cleanup>> iommu_enabled = 0;
+     *   - drivers/passthrough/iommu.c|615| <<iommu_setup>> iommu_enabled = (rc == 0)
+     */
     if ( !iommu_enabled )
         return;
 
+    /*
+     * 在以下调用iommu_flush_all():
+     *   - drivers/passthrough/vtd/iommu.c|1269| <<intel_iommu_dom0_init>> iommu_flush_all();
+     *   - drivers/passthrough/vtd/iommu.c|2155| <<init_vtd_hw>> iommu_flush_all();
+     *   - drivers/passthrough/vtd/iommu.c|2344| <<vtd_suspend>> iommu_flush_all();
+     *   - drivers/passthrough/vtd/iommu.c|2383| <<vtd_crash_shutdown>> iommu_flush_all();
+     *
+     * upstream是这样的:
+     * 2599     if ( iommu_flush_all() )
+     * 2600         printk(XENLOG_WARNING VTDPREFIX
+     * 2601                " crash shutdown: IOMMU flush all failed\n");
+     */
     iommu_flush_all();
 
     for_each_drhd_unit ( drhd )
     {
         iommu = drhd->iommu;
+	/*
+	 * 在以下调用iommu_disable_translation():
+	 *   - drivers/passthrough/vtd/iommu.c|2392| <<vtd_suspend>> iommu_disable_translation(iommu);
+	 *   - drivers/passthrough/vtd/iommu.c|2428| <<vtd_crash_shutdown>> iommu_disable_translation(iommu);
+	 */
         iommu_disable_translation(iommu);
+	/*
+	 * 在以下调用disable_intremap():
+	 *   - drivers/passthrough/vtd/intremap.c|855| <<iommu_enable_x2apic_IR>> disable_intremap(iommu);
+	 *   - drivers/passthrough/vtd/intremap.c|898| <<iommu_disable_x2apic_IR>> disable_intremap(drhd->iommu);
+	 *   - drivers/passthrough/vtd/iommu.c|2165| <<init_vtd_hw>> disable_intremap(drhd->iommu);
+	 *   - drivers/passthrough/vtd/iommu.c|2429| <<vtd_crash_shutdown>> disable_intremap(drhd->iommu);
+	 */
         disable_intremap(drhd->iommu);
+	/*
+	 * 在以下调用disable_qinval():
+	 *   - drivers/passthrough/vtd/intremap.c|856| <<iommu_enable_x2apic_IR>> disable_qinval(iommu);
+	 *   - drivers/passthrough/vtd/intremap.c|901| <<iommu_disable_x2apic_IR>> disable_qinval(drhd->iommu);
+	 *   - drivers/passthrough/vtd/iommu.c|2399| <<vtd_suspend>> disable_qinval(iommu);
+	 *   - drivers/passthrough/vtd/iommu.c|2430| <<vtd_crash_shutdown>> disable_qinval(drhd->iommu);
+	 */
         disable_qinval(drhd->iommu);
     }
 }
@@ -2466,6 +2789,9 @@ static void vtd_dump_p2m_table_level(paddr_t pt_maddr, int level, paddr_t gpa,
     unmap_vtd_domain_page(pt_vaddr);
 }
 
+/*
+ * struct iommu_ops intel_iommu_ops.dump_p2m_table()
+ */
 static void vtd_dump_p2m_table(struct domain *d)
 {
     struct hvm_iommu *hd;
@@ -2478,6 +2804,10 @@ static void vtd_dump_p2m_table(struct domain *d)
     vtd_dump_p2m_table_level(hd->pgd_maddr, agaw_to_level(hd->agaw), 0, 0);
 }
 
+/*
+ * called by:
+ *   - include/asm-x86/hvm/iommu.h|17| <<iommu_get_ops>> return &intel_iommu_ops;
+ */
 const struct iommu_ops intel_iommu_ops = {
     .init = intel_iommu_domain_init,
     .dom0_init = intel_iommu_dom0_init,
diff --git a/xen/drivers/passthrough/vtd/iommu.h b/xen/drivers/passthrough/vtd/iommu.h
index 821565453b..2ff19d878f 100644
--- a/xen/drivers/passthrough/vtd/iommu.h
+++ b/xen/drivers/passthrough/vtd/iommu.h
@@ -217,9 +217,22 @@ struct context_entry {
 #define context_translation_type(c) (((c).lo >> 2) & 3)
 #define context_address_root(c) ((c).lo & PAGE_MASK_4K)
 #define context_address_width(c) ((c).hi &  7)
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|150| <<context_get_domain_id>> dom_index = context_domain_id(*context);
+ *   - drivers/passthrough/vtd/x86/ats.c|91| <<device_in_domain>> if ( context_domain_id(ctxt_entry[pdev->devfn]) != did )
+ */
 #define context_domain_id(c) (((c).hi >> 8) & ((1 << 16) - 1))
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1508| <<domain_context_mapping_one>> context_set_present(*context);
+ */
 #define context_set_present(c) do {(c).lo |= 1;} while(0)
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1645| <<domain_context_unmap_one>> context_clear_present(*context);
+ */
 #define context_clear_present(c) do {(c).lo &= ~1;} while(0)
 #define context_set_fault_enable(c) \
     do {(c).lo &= (((u64)-1) << 2) | 1;} while(0)
@@ -232,6 +245,10 @@ struct context_entry {
 #define CONTEXT_TT_DEV_IOTLB   1
 #define CONTEXT_TT_PASS_THRU   2
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1542| <<domain_context_mapping_one>> context_set_address_root(*context, pgd_maddr);
+ */
 #define context_set_address_root(c, val) \
     do {(c).lo &= 0xfff; (c).lo |= (val) & PAGE_MASK_4K ;} while(0)
 #define context_set_address_width(c, val) \
@@ -247,6 +264,11 @@ struct context_entry {
 #define agaw_to_width(val) (30 + val * LEVEL_STRIDE)
 #define width_to_agaw(w)   ((w - 30)/LEVEL_STRIDE)
 #define level_to_offset_bits(l) (12 + (l - 1) * LEVEL_STRIDE)
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|314| <<addr_to_dma_page_maddr>> offset = address_level_offset(addr, level);
+ *   - drivers/passthrough/vtd/iommu.c|713| <<dma_pte_clear_one>> pte = page + address_level_offset(addr, 1);
+ */
 #define address_level_offset(addr, level) \
             ((addr >> level_to_offset_bits(level)) & LEVEL_MASK)
 #define offset_level_address(offset, level) \
@@ -343,6 +365,16 @@ struct iremap_entry {
  * entries - return addr of the page holding this entry, need unmap it
  * entry   - return required entry
  */
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/intremap.c|185| <<free_remap_entry>> GET_IREMAP_ENTRY(ir_ctrl->iremap_maddr, index,
+ *   - drivers/passthrough/vtd/intremap.c|217| <<alloc_remap_entry>> GET_IREMAP_ENTRY(ir_ctrl->iremap_maddr, i,
+ *   - drivers/passthrough/vtd/intremap.c|254| <<remap_entry_to_ioapic_rte>> GET_IREMAP_ENTRY(ir_ctrl->iremap_maddr, index,
+ *   - drivers/passthrough/vtd/intremap.c|313| <<ioapic_rte_to_remap_entry>> GET_IREMAP_ENTRY(ir_ctrl->iremap_maddr, index,
+ *   - drivers/passthrough/vtd/intremap.c|516| <<remap_entry_to_msi_msg>> GET_IREMAP_ENTRY(ir_ctrl->iremap_maddr, index,
+ *   - drivers/passthrough/vtd/intremap.c|603| <<msi_msg_to_remap_entry>> GET_IREMAP_ENTRY(ir_ctrl->iremap_maddr, index,
+ *   - drivers/passthrough/vtd/utils.c|244| <<dump_iommu_info>> GET_IREMAP_ENTRY(iremap_maddr, i,
+ */
 #define GET_IREMAP_ENTRY(maddr, index, entries, entry)                        \
 do {                                                                          \
     entries = (struct iremap_entry *)map_vtd_domain_page(                     \
@@ -496,6 +528,10 @@ struct qi_ctrl {
 };
 
 struct ir_ctrl {
+    /*
+     * 在以下设置ir_ctrl:
+     *   - drivers/passthrough/vtd/intremap.c|771| <<enable_intremap>> ir_ctrl->iremap_maddr = alloc_pgtable_maddr(drhd, IREMAP_ARCH_PAGE_NR);
+     */
     u64 iremap_maddr;            /* interrupt remap table machine address */
     int iremap_num;              /* total num of used interrupt remap entry */
     spinlock_t iremap_lock;      /* lock for irq remappping table */
diff --git a/xen/drivers/passthrough/vtd/qinval.c b/xen/drivers/passthrough/vtd/qinval.c
index fe29e826c1..034a8764a0 100644
--- a/xen/drivers/passthrough/vtd/qinval.c
+++ b/xen/drivers/passthrough/vtd/qinval.c
@@ -29,6 +29,19 @@
 #include "vtd.h"
 #include "extern.h"
 
+/*
+ * VT-d Queued Invalidation
+ *
+ * The queued invalidation provides an advanced interface for software to submit invalidation requests
+ * to hardware and to synchronize invalidation completions with hardware. Hardware implementations
+ * report queued invalidation support through the Extended Capability Register.
+ * Queued-invalidation supports invalidation of first-level, second-level and nested translations. Queued
+ */
+
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/qinval.c|235| <<queue_invalidate_wait>> print_qi_regs(iommu);
+ */
 static void print_qi_regs(struct iommu *iommu)
 {
     u64 val;
@@ -43,6 +56,14 @@ static void print_qi_regs(struct iommu *iommu)
     printk("DMAR_IQT_REG = %"PRIx64"\n", val);
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/qinval.c|117| <<queue_invalidate_context>> index = qinval_next_index(iommu);
+ *   - drivers/passthrough/vtd/qinval.c|168| <<queue_invalidate_iotlb>> index = qinval_next_index(iommu);
+ *   - drivers/passthrough/vtd/qinval.c|218| <<queue_invalidate_wait>> index = qinval_next_index(iommu);
+ *   - drivers/passthrough/vtd/qinval.c|303| <<qinval_device_iotlb>> index = qinval_next_index(iommu);
+ *   - drivers/passthrough/vtd/qinval.c|347| <<queue_invalidate_iec>> index = qinval_next_index(iommu);
+ */
 static int qinval_next_index(struct iommu *iommu)
 {
     u64 tail;
@@ -58,6 +79,14 @@ static int qinval_next_index(struct iommu *iommu)
     return tail;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/qinval.c|122| <<queue_invalidate_context>> ret |= qinval_update_qtail(iommu, index);
+ *   - drivers/passthrough/vtd/qinval.c|173| <<queue_invalidate_iotlb>> ret |= qinval_update_qtail(iommu, index);
+ *   - drivers/passthrough/vtd/qinval.c|223| <<queue_invalidate_wait>> ret |= qinval_update_qtail(iommu, index);
+ *   - drivers/passthrough/vtd/qinval.c|308| <<qinval_device_iotlb>> ret |= qinval_update_qtail(iommu, index);
+ *   - drivers/passthrough/vtd/qinval.c|351| <<queue_invalidate_iec>> ret |= qinval_update_qtail(iommu, index);
+ */
 static int qinval_update_qtail(struct iommu *iommu, int index)
 {
     u64 val;
@@ -69,6 +98,10 @@ static int qinval_update_qtail(struct iommu *iommu, int index)
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/qinval.c|120| <<queue_invalidate_context>> ret = gen_cc_inv_dsc(iommu, index, did, source_id,
+ */
 static int gen_cc_inv_dsc(struct iommu *iommu, int index,
     u16 did, u16 source_id, u8 function_mask, u8 granu)
 {
@@ -97,6 +130,10 @@ static int gen_cc_inv_dsc(struct iommu *iommu, int index,
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/qinval.c|404| <<flush_context_qi>> ret = queue_invalidate_context(iommu, did, sid, fm,
+ */
 int queue_invalidate_context(struct iommu *iommu,
     u16 did, u16 source_id, u8 function_mask, u8 granu)
 {
@@ -115,6 +152,10 @@ int queue_invalidate_context(struct iommu *iommu,
     return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/qinval.c|199| <<queue_invalidate_iotlb>> ret = gen_iotlb_inv_dsc(iommu, index, granu, dr, dw, did,
+ */
 static int gen_iotlb_inv_dsc(struct iommu *iommu, int index,
     u8 granu, u8 dr, u8 dw, u16 did, u8 am, u8 ih, u64 addr)
 {
@@ -147,6 +188,10 @@ static int gen_iotlb_inv_dsc(struct iommu *iommu, int index,
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/qinval.c|471| <<flush_iotlb_qi>> ret = queue_invalidate_iotlb(iommu,
+ */
 int queue_invalidate_iotlb(struct iommu *iommu,
     u8 granu, u8 dr, u8 dw, u16 did, u8 am, u8 ih, u64 addr)
 {
@@ -166,6 +211,10 @@ int queue_invalidate_iotlb(struct iommu *iommu,
     return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/qinval.c|478| <<flush_iotlb_qi>> ret = queue_invalidate_iotlb(iommu,
+ */
 static int gen_wait_dsc(struct iommu *iommu, int index,
     u8 iflag, u8 sw, u8 fn, u32 sdata, volatile u32 *saddr)
 {
@@ -192,6 +241,10 @@ static int gen_wait_dsc(struct iommu *iommu, int index,
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/qinval.c|246| <<invalidate_sync>> ret = queue_invalidate_wait(iommu, 0, 1, 1);
+ */
 static int queue_invalidate_wait(struct iommu *iommu,
     u8 iflag, u8 sw, u8 fn)
 {
@@ -228,6 +281,14 @@ static int queue_invalidate_wait(struct iommu *iommu,
     return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/intremap.c|361| <<ioapic_rte_to_remap_entry>> invalidate_sync(iommu);
+ *   - drivers/passthrough/vtd/intremap.c|646| <<msi_msg_to_remap_entry>> invalidate_sync(iommu);
+ *   - drivers/passthrough/vtd/qinval.c|339| <<__iommu_flush_iec>> ret |= invalidate_sync(iommu);
+ *   - drivers/passthrough/vtd/qinval.c|385| <<flush_context_qi>> ret |= invalidate_sync(iommu);
+ *   - drivers/passthrough/vtd/qinval.c|427| <<flush_iotlb_qi>> ret |= invalidate_sync(iommu);
+ */
 int invalidate_sync(struct iommu *iommu)
 {
     int ret = -1;
@@ -241,6 +302,10 @@ int invalidate_sync(struct iommu *iommu)
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/qinval.c|334| <<qinval_device_iotlb>> ret = gen_dev_iotlb_inv_dsc(iommu, index, max_invs_pend,
+ */
 static int gen_dev_iotlb_inv_dsc(struct iommu *iommu, int index,
     u32 max_invs_pend, u16 sid, u16 size, u64 addr)
 {
@@ -271,6 +336,11 @@ static int gen_dev_iotlb_inv_dsc(struct iommu *iommu, int index,
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/x86/ats.c|138| <<dev_invalidate_iotlb>> rc = qinval_device_iotlb(iommu, pdev->ats_queue_depth,
+ *   - drivers/passthrough/vtd/x86/ats.c|158| <<dev_invalidate_iotlb>> rc = qinval_device_iotlb(iommu, pdev->ats_queue_depth,
+ */
 int qinval_device_iotlb(struct iommu *iommu,
     u32 max_invs_pend, u16 sid, u16 size, u64 addr)
 {
@@ -356,6 +426,10 @@ int iommu_flush_iec_index(struct iommu *iommu, u8 im, u16 iidx)
    return __iommu_flush_iec(iommu, IEC_INDEX_INVL, im, iidx);
 }
 
+/*
+ * 在以下使用flush_context_qi():
+ *   - drivers/passthrough/vtd/qinval.c|542| <<enable_qinval>> flush->context = flush_context_qi;
+ */
 static int flush_context_qi(
     void *_iommu, u16 did, u16 sid, u8 fm, u64 type,
     int flush_non_present_entry)
@@ -387,6 +461,10 @@ static int flush_context_qi(
     return ret;
 }
 
+/*
+ * 在以下使用flush_iotlb_qi():
+ *   - drivers/passthrough/vtd/qinval.c|518| <<enable_qinval>> flush->iotlb = flush_iotlb_qi;
+ */
 static int flush_iotlb_qi(
     void *_iommu, u16 did,
     u64 addr, unsigned int size_order, u64 type,
@@ -429,6 +507,11 @@ static int flush_iotlb_qi(
     return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/intremap.c|923| <<iommu_enable_x2apic_IR>> if ( enable_qinval(iommu) != 0 )
+ *   - drivers/passthrough/vtd/iommu.c|2357| <<init_vtd_hw>> if ( enable_qinval(iommu) != 0 )
+ */
 int enable_qinval(struct iommu *iommu)
 {
     struct acpi_drhd_unit *drhd;
@@ -489,6 +572,13 @@ int enable_qinval(struct iommu *iommu)
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/intremap.c|856| <<iommu_enable_x2apic_IR>> disable_qinval(iommu);
+ *   - drivers/passthrough/vtd/intremap.c|901| <<iommu_disable_x2apic_IR>> disable_qinval(drhd->iommu);
+ *   - drivers/passthrough/vtd/iommu.c|2399| <<vtd_suspend>> disable_qinval(iommu);
+ *   - drivers/passthrough/vtd/iommu.c|2430| <<vtd_crash_shutdown>> disable_qinval(drhd->iommu);
+ */
 void disable_qinval(struct iommu *iommu)
 {
     u32 sts;
@@ -505,6 +595,10 @@ void disable_qinval(struct iommu *iommu)
     dmar_writel(iommu->reg, DMAR_GCMD_REG, sts & (~DMA_GCMD_QIE));
 
     /* Make sure hardware complete it */
+    /*
+     * 根据代码分析, 在timeout的条件下, 要么panic, 要么break退出
+     * 不会hang!!!
+     */
     IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, dmar_readl,
                   !(sts & DMA_GSTS_QIES), sts);
 out:
diff --git a/xen/drivers/passthrough/vtd/quirks.c b/xen/drivers/passthrough/vtd/quirks.c
index feeeea2895..ae85533578 100644
--- a/xen/drivers/passthrough/vtd/quirks.c
+++ b/xen/drivers/passthrough/vtd/quirks.c
@@ -216,6 +216,12 @@ static void snb_vtd_ops_postamble(struct iommu* iommu)
  * call before VT-d translation enable and IOTLB flush operations.
  */
 
+/*
+ * 在以下使用snb_igd_quirk:
+ *   - drivers/passthrough/vtd/quirks.c|220| <<global>> boolean_param("snb_igd_quirk", snb_igd_quirk);
+ *   - drivers/passthrough/vtd/quirks.c|225| <<vtd_ops_preamble_quirk>> if ( snb_igd_quirk )
+ *   - drivers/passthrough/vtd/quirks.c|239| <<vtd_ops_postamble_quirk>> if ( snb_igd_quirk )
+ */
 static int snb_igd_quirk;
 boolean_param("snb_igd_quirk", snb_igd_quirk);
 
@@ -269,6 +275,10 @@ static void __init tylersburg_intremap_quirk(void)
 }
 
 /* initialize platform identification flags */
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|2250| <<intel_vtd_setup>> platform_quirks_init();
+ */
 void __init platform_quirks_init(void)
 {
     ioh_id = pci_conf_read32(0, 0, IOH_DEV, 0, 0);
@@ -379,6 +389,11 @@ void me_wifi_quirk(struct domain *domain, u8 bus, u8 devfn, int map)
     }
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1557| <<domain_context_mapping>> pci_vtd_quirk(pdev);
+ *   - drivers/passthrough/vtd/iommu.c|1998| <<intel_iommu_enable_device>> pci_vtd_quirk(pdev);
+ */
 void pci_vtd_quirk(const struct pci_dev *pdev)
 {
     int seg = pdev->seg;
diff --git a/xen/drivers/passthrough/vtd/utils.c b/xen/drivers/passthrough/vtd/utils.c
index bd14c02827..5c9b37c992 100644
--- a/xen/drivers/passthrough/vtd/utils.c
+++ b/xen/drivers/passthrough/vtd/utils.c
@@ -57,6 +57,11 @@ void disable_pmr(struct iommu *iommu)
             "Disabled protected memory registers\n");
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1225| <<iommu_alloc>> print_iommu_regs(drhd);
+ *   - drivers/passthrough/vtd/iommu.c|1238| <<iommu_alloc>> print_iommu_regs(drhd);
+ */
 void print_iommu_regs(struct acpi_drhd_unit *drhd)
 {
     struct iommu *iommu = drhd->iommu;
@@ -95,6 +100,10 @@ static u32 get_level_index(unsigned long gmfn, int level)
     return gmfn & LEVEL_MASK;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|921| <<iommu_page_fault_do_one>> print_vtd_entries(iommu, PCI_BUS(source_id), PCI_DEVFN2(source_id),
+ */
 void print_vtd_entries(struct iommu *iommu, int bus, int devfn, u64 gmfn)
 {
     struct context_entry *ctxt_entry;
diff --git a/xen/drivers/passthrough/vtd/x86/ats.c b/xen/drivers/passthrough/vtd/x86/ats.c
index 6b0632b392..8c8c5dd7ba 100644
--- a/xen/drivers/passthrough/vtd/x86/ats.c
+++ b/xen/drivers/passthrough/vtd/x86/ats.c
@@ -29,6 +29,15 @@
 #include "../extern.h"
 #include "../../ats.h"
 
+/*
+ * Address Translation Service
+ */
+
+/*
+ * 在以下使用ats_dev_drhd_units:
+ *   - drivers/passthrough/vtd/x86/ats.c|37| <<find_ats_dev_drhd>> list_for_each_entry ( drhd, &ats_dev_drhd_units, list )
+ *   - drivers/passthrough/vtd/x86/ats.c|70| <<ats_device>> list_add_tail(&ats_drhd->list, &ats_dev_drhd_units);
+ */
 static LIST_HEAD(ats_dev_drhd_units);
 
 struct acpi_drhd_unit * find_ats_dev_drhd(struct iommu *iommu)
@@ -42,6 +51,12 @@ struct acpi_drhd_unit * find_ats_dev_drhd(struct iommu *iommu)
     return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|1648| <<domain_context_mapping>> if ( !ret && devfn == pdev->devfn && ats_device(pdev, drhd) > 0 )
+ *   - drivers/passthrough/vtd/iommu.c|1786| <<domain_context_unmap>> if ( !ret && devfn == pdev->devfn && ats_device(pdev, drhd) > 0 )
+ *   - drivers/passthrough/vtd/iommu.c|2194| <<intel_iommu_enable_device>> int ret = drhd ? ats_device(pdev, drhd) : -ENODEV;
+ */
 int ats_device(const struct pci_dev *pdev, const struct acpi_drhd_unit *drhd)
 {
     struct acpi_drhd_unit *ats_drhd;
diff --git a/xen/drivers/passthrough/vtd/x86/vtd.c b/xen/drivers/passthrough/vtd/x86/vtd.c
index ca17cb12ca..33f96e9512 100644
--- a/xen/drivers/passthrough/vtd/x86/vtd.c
+++ b/xen/drivers/passthrough/vtd/x86/vtd.c
@@ -39,6 +39,30 @@
 static bool_t __initdata iommu_inclusive_mapping = 1;
 boolean_param("iommu_inclusive_mapping", iommu_inclusive_mapping);
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|251| <<bus_to_context_maddr>> root_entries = (struct root_entry *)map_vtd_domain_page(iommu->root_maddr);
+ *   - drivers/passthrough/vtd/iommu.c|296| <<addr_to_dma_page_maddr>> parent = (struct dma_pte *)map_vtd_domain_page(hd->pgd_maddr);
+ *   - drivers/passthrough/vtd/iommu.c|329| <<addr_to_dma_page_maddr>> parent = map_vtd_domain_page(pte_maddr);
+ *   - drivers/passthrough/vtd/iommu.c|697| <<dma_pte_clear_one>> page = (struct dma_pte *)map_vtd_domain_page(pg_maddr);
+ *   - drivers/passthrough/vtd/iommu.c|737| <<iommu_free_page_table>> pt_vaddr = (struct dma_pte *)map_vtd_domain_page(pt_maddr);
+ *   - drivers/passthrough/vtd/iommu.c|1404| <<domain_context_mapping_one>> context_entries = (struct context_entry *)map_vtd_domain_page(maddr);
+ *   - drivers/passthrough/vtd/iommu.c|1483| <<domain_context_mapping_one>> struct dma_pte *p = map_vtd_domain_page(pgd_maddr);
+ *   - drivers/passthrough/vtd/iommu.c|1635| <<domain_context_unmap_one>> context_entries = (struct context_entry *)map_vtd_domain_page(maddr);
+ *   - drivers/passthrough/vtd/iommu.c|1870| <<intel_iommu_map_page>> page = (struct dma_pte *)map_vtd_domain_page(pg_maddr);
+ *   - drivers/passthrough/vtd/iommu.c|2618| <<vtd_dump_p2m_table_level>> pt_vaddr = map_vtd_domain_page(pt_maddr);
+ *   - drivers/passthrough/vtd/iommu.h|348| <<GET_IREMAP_ENTRY>> entries = (struct iremap_entry *)map_vtd_domain_page( \
+ *   - drivers/passthrough/vtd/qinval.c|87| <<gen_cc_inv_dsc>> (struct qinval_entry *)map_vtd_domain_page(entry_base);
+ *   - drivers/passthrough/vtd/qinval.c|134| <<gen_iotlb_inv_dsc>> (struct qinval_entry *)map_vtd_domain_page(entry_base);
+ *   - drivers/passthrough/vtd/qinval.c|184| <<gen_wait_dsc>> (struct qinval_entry *)map_vtd_domain_page(entry_base);
+ *   - drivers/passthrough/vtd/qinval.c|272| <<gen_dev_iotlb_inv_dsc>> (struct qinval_entry *)map_vtd_domain_page(entry_base);
+ *   - drivers/passthrough/vtd/qinval.c|320| <<gen_iec_inv_dsc>> (struct qinval_entry *)map_vtd_domain_page(entry_base);
+ *   - drivers/passthrough/vtd/utils.c|125| <<print_vtd_entries>> root_entry = (struct root_entry *)map_vtd_domain_page(iommu->root_maddr);
+ *   - drivers/passthrough/vtd/utils.c|143| <<print_vtd_entries>> ctxt_entry = map_vtd_domain_page(val);
+ *   - drivers/passthrough/vtd/utils.c|172| <<print_vtd_entries>> l = map_vtd_domain_page(val);
+ *   - drivers/passthrough/vtd/x86/ats.c|81| <<device_in_domain>> root_entry = (struct root_entry *) map_vtd_domain_page(iommu->root_maddr);
+ *   - drivers/passthrough/vtd/x86/ats.c|86| <<device_in_domain>> map_vtd_domain_page(root_entry[pdev->bus].val);
+ */
 void *map_vtd_domain_page(u64 maddr)
 {
     return map_domain_page(maddr >> PAGE_SHIFT_4K);
@@ -54,11 +78,19 @@ unsigned int get_cache_line_size(void)
     return ((cpuid_ebx(1) >> 8) & 0xff) * 8;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/vtd/iommu.c|197| <<__iommu_flush_cache>> cacheline_flush((char *)addr + i);
+ */
 void cacheline_flush(char * addr)
 {
     clflush(addr);
 }
 
+/*
+ * called by 
+ *   - drivers/passthrough/vtd/iommu.c|588| <<iommu_flush_all>> 
+ */
 void flush_all_cache()
 {
     wbinvd();
diff --git a/xen/drivers/passthrough/x86/ats.c b/xen/drivers/passthrough/x86/ats.c
index 436eadac61..109f1a989e 100644
--- a/xen/drivers/passthrough/x86/ats.c
+++ b/xen/drivers/passthrough/x86/ats.c
@@ -18,11 +18,32 @@
 #include <xen/pci_regs.h>
 #include "../ats.h"
 
+/*
+ * Address Translation Service: ATS makes the PCI Endpoint be able to request
+ * the DMA address translation from the IOMMU and cache the translation in the
+ * Endpoint, thus alleviate IOMMU pressure and improve the hardware performance
+ * in the I/O virtualization environment.
+ */
+
+/*
+ * 在以下使用ats_devices:
+ *   - drivers/passthrough/vtd/x86/ats.c|118| <<dev_invalidate_iotlb>> list_for_each_entry( pdev, &ats_devices, list )
+ *   - drivers/passthrough/x86/ats.c|43| <<enable_ats_device>> list_for_each_entry ( pdev, &ats_devices, list )
+ *   - drivers/passthrough/x86/ats.c|74| <<enable_ats_device>> list_add(&pdev->list, &ats_devices);
+ *   - drivers/passthrough/x86/ats.c|100| <<disable_ats_device>> list_for_each_entry ( pdev, &ats_devices, list )
+ *   - drivers/passthrough/x86/ats.c|122| <<get_ats_device>> list_for_each_entry ( pdev, &ats_devices, list )
+ */
 LIST_HEAD(ats_devices);
 
 bool_t __read_mostly ats_enabled = 0;
 boolean_param("ats", ats_enabled);
 
+/*
+ * called by:
+ *   - drivers/passthrough/amd/pci_amd_iommu.c|167| <<amd_iommu_setup_domain_device>> enable_ats_device(iommu->seg, bus, devfn, iommu);
+ *   - drivers/passthrough/vtd/iommu.c|1515| <<domain_context_mapping>> enable_ats_device(seg, bus, devfn, drhd->iommu);
+ *   - drivers/passthrough/vtd/iommu.c|2003| <<intel_iommu_enable_device>> ret = enable_ats_device(pdev->seg, pdev->bus, pdev->devfn, drhd->iommu);
+ */
 int enable_ats_device(int seg, int bus, int devfn, const void *iommu)
 {
     struct pci_ats_dev *pdev = NULL;
@@ -82,6 +103,11 @@ int enable_ats_device(int seg, int bus, int devfn, const void *iommu)
     return pos;
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/amd/pci_amd_iommu.c|354| <<amd_iommu_disable_domain_device>> disable_ats_device(iommu->seg, bus, devfn);
+ *   - drivers/passthrough/vtd/iommu.c|1653| <<domain_context_unmap>> disable_ats_device(seg, bus, devfn);
+ */
 void disable_ats_device(int seg, int bus, int devfn)
 {
     struct pci_ats_dev *pdev;
@@ -112,6 +138,10 @@ void disable_ats_device(int seg, int bus, int devfn)
                 seg, bus, PCI_SLOT(devfn), PCI_FUNC(devfn));
 }
 
+/*
+ * called by:
+ *   - drivers/passthrough/amd/iommu_cmd.c|299| <<amd_iommu_flush_iotlb>> ats_pdev = get_ats_device(pdev->seg, pdev->bus, pdev->devfn);
+ */
 struct pci_ats_dev *get_ats_device(int seg, int bus, int devfn)
 {
     struct pci_ats_dev *pdev;
diff --git a/xen/drivers/pci/pci.c b/xen/drivers/pci/pci.c
index ca07ed03b8..4c833fb1c5 100644
--- a/xen/drivers/pci/pci.c
+++ b/xen/drivers/pci/pci.c
@@ -8,6 +8,24 @@
 #include <xen/pci.h>
 #include <xen/pci_regs.h>
 
+/*
+ * called by:
+ *   - arch/x86/msi.c|319| <<msi_set_enable>> pos = pci_find_cap_offset(seg, bus, slot, func, PCI_CAP_ID_MSI);
+ *   - arch/x86/msi.c|332| <<msix_set_enable>> pos = pci_find_cap_offset(seg, bus, slot, func, PCI_CAP_ID_MSIX);
+ *   - arch/x86/msi.c|568| <<msi_capability_init>> pos = pci_find_cap_offset(seg, bus, slot, func, PCI_CAP_ID_MSI);
+ *   - arch/x86/msi.c|724| <<msix_capability_init>> pos = pci_find_cap_offset(seg, bus, slot, func, PCI_CAP_ID_MSIX);
+ *   - arch/x86/msi.c|957| <<__pci_enable_msix>> pos = pci_find_cap_offset(msi->seg, msi->bus, slot, func, PCI_CAP_ID_MSIX);
+ *   - arch/x86/msi.c|1013| <<__pci_disable_msix>> pos = pci_find_cap_offset(seg, bus, slot, func, PCI_CAP_ID_MSIX);
+ *   - arch/x86/msi.c|1031| <<pci_prepare_msix>> unsigned int pos = pci_find_cap_offset(seg, bus, slot, func,
+ *   - arch/x86/msi.c|1211| <<pci_msix_get_table_len>> pos = pci_find_cap_offset(seg, bus, slot, func, PCI_CAP_ID_MSIX);
+ *   - drivers/char/ehci-dbgp.c|691| <<__find_dbgp>> return pci_find_cap_offset(0, bus, slot, func, PCI_CAP_ID_EHCI_DEBUG);
+ *   - drivers/passthrough/amd/iommu_detect.c|35| <<get_iommu_msi_capabilities>> pos = pci_find_cap_offset(seg, bus, dev, func, PCI_CAP_ID_MSI);
+ *   - drivers/passthrough/pci.c|284| <<alloc_pdev>> if ( pci_find_cap_offset(pseg->nr, bus, PCI_SLOT(devfn), PCI_FUNC(devfn),
+ *   - drivers/passthrough/pci.c|325| <<alloc_pdev>> pos = pci_find_cap_offset(pseg->nr, bus, PCI_SLOT(devfn),
+ *   - drivers/passthrough/pci.c|822| <<pdev_type>> int pos = pci_find_cap_offset(seg, bus, d, f, PCI_CAP_ID_EXP);
+ *   - drivers/passthrough/pci.c|1087| <<hest_match_type>> unsigned int pos = pci_find_cap_offset(pdev->seg, pdev->bus,
+ *   - drivers/passthrough/pci.c|1164| <<pcie_aer_get_firmware_first>> return pci_find_cap_offset(pdev->seg, pdev->bus, PCI_SLOT(pdev->devfn),
+ */
 int pci_find_cap_offset(u16 seg, u8 bus, u8 dev, u8 func, u8 cap)
 {
     u8 id;
@@ -39,6 +57,9 @@ int pci_find_cap_offset(u16 seg, u8 bus, u8 dev, u8 func, u8 cap)
     return 0;
 }
 
+/*
+ * 没人调用
+ */
 int pci_find_next_cap(u16 seg, u8 bus, unsigned int devfn, u8 pos, int cap)
 {
     u8 id;
@@ -115,6 +136,17 @@ int pci_find_next_ext_capability(int seg, int bus, int devfn, int start, int cap
     return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/char/ehci-dbgp.c|1485| <<ehci_dbgp_init>> e = parse_pci(opt_dbgp + 8, NULL, &bus, &slot, &func);
+ *   - drivers/char/ns16550.c|690| <<ns16550_parse_port_config>> conf = parse_pci(conf, NULL, &uart->ps_bdf[0],
+ *   - drivers/char/ns16550.c|699| <<ns16550_parse_port_config>> if ( !parse_pci(conf, NULL, &uart->pb_bdf[0],
+ *   - drivers/passthrough/amd/iommu_acpi.c|650| <<parse_ivrs_ioapic>> s = parse_pci(s + 1, &seg, &bus, &dev, &func);
+ *   - drivers/passthrough/amd/iommu_acpi.c|671| <<parse_ivrs_hpet>> s = parse_pci(s + 1, &seg, &bus, &dev, &func);
+ *   - drivers/passthrough/pci.c|137| <<parse_phantom_dev>> s = parse_pci(s, &seg, &bus, &slot, NULL);
+ *
+ * 似乎没什么调用
+ */
 const char *__init parse_pci(const char *s, unsigned int *seg_p,
                              unsigned int *bus_p, unsigned int *dev_p,
                              unsigned int *func_p)
diff --git a/xen/include/xen/iommu.h b/xen/include/xen/iommu.h
index 8bb0a1d1aa..99825bc8fe 100644
--- a/xen/include/xen/iommu.h
+++ b/xen/include/xen/iommu.h
@@ -35,6 +35,21 @@ extern bool_t iommu_debug;
 extern bool_t amd_iommu_perdev_intremap;
 
 /* Does this domain have a P2M table we can use as its IOMMU pagetable? */
+/*
+ * 在以下调用iommu_use_hap_pt():
+ *   - arch/x86/mm/p2m-pt.c|426| <<p2m_pt_set_entry>> if ( iommu_use_hap_pt(p2m->domain) )
+ *   - drivers/passthrough/amd/iommu_map.c|662| <<amd_iommu_map_page>> if ( iommu_use_hap_pt(d) )
+ *   - drivers/passthrough/amd/iommu_map.c|750| <<amd_iommu_unmap_page>> if ( iommu_use_hap_pt(d) )
+ *   - drivers/passthrough/amd/iommu_map.c|823| <<amd_iommu_share_p2m>> if ( !iommu_use_hap_pt(d) )
+ *   - drivers/passthrough/amd/pci_amd_iommu.c|464| <<deallocate_iommu_page_tables>> if ( iommu_use_hap_pt(d) )
+ *   - drivers/passthrough/iommu.c|376| <<assign_device>> if ( !iommu_use_hap_pt(d) )
+ *   - drivers/passthrough/iommu.c|1068| <<iommu_dump_p2m_table>> if ( iommu_use_hap_pt(d) )
+ *   - drivers/passthrough/vtd/iommu.c|1911| <<iommu_domain_teardown>> if ( iommu_use_hap_pt(d) )
+ *   - drivers/passthrough/vtd/iommu.c|1937| <<intel_iommu_map_page>> if ( iommu_use_hap_pt(d) )
+ *   - drivers/passthrough/vtd/iommu.c|2044| <<iommu_set_pgd>> if ( !iommu_use_hap_pt(d) )
+ *
+ * 应该大部分都是true
+ */
 #define iommu_use_hap_pt(d) (hap_enabled(d) && iommu_hap_pt_share)
 
 #define domain_hvm_iommu(d)     (&d->arch.hvm_domain.hvm_iommu)
-- 
2.17.1

