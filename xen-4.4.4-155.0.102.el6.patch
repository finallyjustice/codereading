From fdec8b0012eb6bbfac0d2e23b4c5ef57b2cbc729 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 12 Apr 2022 23:19:34 -0700
Subject: [PATCH 1/1] xen-4.4.4-155.0.102.el6

xen-4.4.4-155.0.102

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 xen/arch/x86/domain.c      |   4 +
 xen/arch/x86/setup.c       |   4 +
 xen/common/cpupool.c       |   6 +
 xen/common/domain.c        |  32 +++
 xen/common/sched_credit.c  | 449 +++++++++++++++++++++++++++++++++++++
 xen/common/schedule.c      | 225 +++++++++++++++++++
 xen/include/xen/sched-if.h |   7 +
 xen/include/xen/sched.h    |  63 ++++++
 8 files changed, 790 insertions(+)

diff --git a/xen/arch/x86/domain.c b/xen/arch/x86/domain.c
index a1139bb047..674916e736 100644
--- a/xen/arch/x86/domain.c
+++ b/xen/arch/x86/domain.c
@@ -1631,6 +1631,10 @@ static void __context_switch(void)
 }
 
 
+/*
+ * called by:
+ *   - common/schedule.c|1325| <<schedule>> context_switch(prev, next);
+ */
 void context_switch(struct vcpu *prev, struct vcpu *next)
 {
     unsigned int cpu = smp_processor_id();
diff --git a/xen/arch/x86/setup.c b/xen/arch/x86/setup.c
index a7e00391a5..d45e898467 100644
--- a/xen/arch/x86/setup.c
+++ b/xen/arch/x86/setup.c
@@ -252,6 +252,10 @@ static void free_xen_data(char *s, char *e)
 
 extern char __init_begin[], __init_end[], __bss_start[];
 
+/*
+ * called by:
+ *   - arch/x86/setup.c|1414| <<__start_xen>> init_idle_domain();
+ */
 static void __init init_idle_domain(void)
 {
     scheduler_init();
diff --git a/xen/common/cpupool.c b/xen/common/cpupool.c
index 8461442f43..ee6d0aa386 100644
--- a/xen/common/cpupool.c
+++ b/xen/common/cpupool.c
@@ -227,6 +227,12 @@ static int cpupool_destroy(struct cpupool *c)
 /*
  * Move domain to another cpupool
  */
+/*
+ * called by:
+ *   - common/cpupool.c|249| <<cpupool_move_domain>> ret = cpupool_move_domain_locked(d, c);
+ *   - common/cpupool.c|370| <<cpupool_unassign_cpu>> ret = cpupool_move_domain_locked(d, cpupool0);
+ *   - common/cpupool.c|688| <<cpupool_do_sysctl>> ret = cpupool_move_domain_locked(d, c);
+ */
 static int cpupool_move_domain_locked(struct domain *d, struct cpupool *c)
 {
     int ret;
diff --git a/xen/common/domain.c b/xen/common/domain.c
index cff5d3ac5b..3c3ba192de 100644
--- a/xen/common/domain.c
+++ b/xen/common/domain.c
@@ -190,6 +190,16 @@ static void vcpu_info_reset(struct vcpu *v)
     v->vcpu_info_mfn = INVALID_MFN;
 }
 
+/*
+ * called by:
+ *   - arch/arm/domain_build.c|65| <<alloc_dom0_vcpu0>> return alloc_vcpu(dom0, 0, 0);
+ *   - arch/arm/domain_build.c|1087| <<construct_dom0>> if ( alloc_vcpu(d, i, cpu) == NULL )
+ *   - arch/x86/domain_build.c|235| <<alloc_dom0_vcpu0>> return alloc_vcpu(dom0, 0, cpu);
+ *   - arch/x86/domain_build.c|890| <<construct_dom0>> (void )alloc_vcpu(d, i, cpu);
+ *   - common/domctl.c|702| <<XEN_GUEST_HANDLE_PARAM(XEN_DOMCTL_max_vcpus)>> if ( alloc_vcpu(d, i, cpu) == NULL )
+ *   - common/schedule.c|1390| <<cpu_schedule_up>> alloc_vcpu(idle_vcpu[0]->domain, cpu, cpu);
+ *   - common/schedule.c|1492| <<scheduler_init>> if ( alloc_vcpu(idle_domain, 0, 0) == NULL )
+ */
 struct vcpu *alloc_vcpu(
     struct domain *d, unsigned int vcpu_id, unsigned int cpu_id)
 {
@@ -435,6 +445,16 @@ struct domain *domain_create(
 }
 
 
+/*
+ * called by:
+ *   - common/cpupool.c|295| <<cpupool_assign_cpu_locked>> domain_update_node_affinity(d);
+ *   - common/domain.c|274| <<alloc_vcpu>> domain_update_node_affinity(d);
+ *   - common/domain.c|537| <<domain_set_node_affinity>> domain_update_node_affinity(d);
+ *   - common/schedule.c|334| <<sched_move_domain>> domain_update_node_affinity(d);
+ *   - common/schedule.c|625| <<restore_vcpu_affinity>> domain_update_node_affinity(d);
+ *   - common/schedule.c|687| <<cpu_disable_scheduler>> domain_update_node_affinity(d);
+ *   - common/schedule.c|716| <<vcpu_set_affinity>> domain_update_node_affinity(v->domain);
+ */
 void domain_update_node_affinity(struct domain *d)
 {
     cpumask_var_t dom_cpumask, dom_cpumask_soft;
@@ -451,6 +471,9 @@ void domain_update_node_affinity(struct domain *d)
         return;
     }
 
+    /*
+     * 默认都是有d->cpupool的
+     */
     online = cpupool_online_cpumask(d->cpupool);
 
     spin_lock(&d->node_affinity_lock);
@@ -460,6 +483,15 @@ void domain_update_node_affinity(struct domain *d)
      * node-affinity and update d->node_affinity accordingly. if false,
      * just leave d->auto_node_affinity alone.
      */
+    /*
+     * 在以下使用domain->auto_node_affinity:
+     *   - common/domain.c|322| <<domain_create>> d->auto_node_affinity = 1;
+     *   - common/domain.c|483| <<domain_update_node_affinity>> if ( d->auto_node_affinity )
+     *   - common/domain.c|537| <<domain_set_node_affinity>> d->auto_node_affinity = 1;
+     *   - common/domain.c|541| <<domain_set_node_affinity>> d->auto_node_affinity = 0;
+     *
+     * 测试的例子是1 (dom0, domU, idle)
+     */
     if ( d->auto_node_affinity )
     {
         /*
diff --git a/xen/common/sched_credit.c b/xen/common/sched_credit.c
index e3d13c276b..832f431f14 100644
--- a/xen/common/sched_credit.c
+++ b/xen/common/sched_credit.c
@@ -24,6 +24,31 @@
 #include <xen/keyhandler.h>
 #include <xen/trace.h>
 
+/*
+ * Master_timer vs. PCPU->timer vs. s_timer->scheduler
+ *
+ * 主ticker的master_timer由csched_priv持有,在csched_start_tickers里初始化,
+ * 它每3个tick周期通过csched_acct来计算所有domain的vcpu的credit值,主要是根
+ * 据当前credit总值,每个domain占用的权重weight和比例上限cap,计算出每个domain
+ * 分得的credit值,然后再平均分配到每个vcpu上,vcpu的credit值就等于原有的
+ * credit值和新分到的credit之和.根据vcpu最新的credit值来确定vcpu的调度优先级
+ * 是over还是under,如果是over且cap有设置,则执行sleep,把自己调度出来,等待下次
+ * 判断和调度.
+ *
+ * 每个pcpu时会设置它的tick的timer,每个tick周期(10ms)会执行一次csched_tick,来
+ * 更新该pcpu上运行的当前vcpu的credit值,减去credit_per_tick,该值是100.当活动
+ * vcpu列表为空时说明所有vcpu的credit都为负,则重新计算credit值.否则通过pick后
+ * pcpu是否变化来检查是否需要进行vcpu的迁移,如果需要则触发软中断,执行schedule算法.
+ *
+ * schedule()会在每3个tick周期(30ms,即一个vcpu得到调度后运行的时间片)执行一次,
+ * 该过程的结果为该pcpu找出合适的vcpu和时间片,用于该vcpu在此pcpu上的运行,然后进
+ * 行上下文的切换,这个schedule过程是根据选择的调度算法(credit_schedule())来确定的.
+ * credit算法在执行schedule时,会首先获得该pcpu的就绪队列runq上队首vcpu,即snext,检查
+ * 其credit值有没有用完,如果没有用完直接返回.如果用完表明该pcpu的就绪队列上没有under
+ * 或boost状态的vcpu,则进行负载均衡,查找其它pcpu上优先级较高的vcpu,执行run_steal把
+ * 它偷过来运行.如果还没有找到合适的,就返回之前得到的snext.
+ */
+
 
 /*
  * Basic constants
@@ -38,6 +63,31 @@
 /*
  * Priorities
  */
+/*
+ * 在以下使用CSCHED_PRI_TS_UNDER:
+ *   - common/sched_credit.c|42| <<global>> #define CSCHED_PRI_TS_UNDER -1
+ *   - common/sched_credit.c|975| <<csched_vcpu_acct>> svc->pri = CSCHED_PRI_TS_UNDER;
+ *   - common/sched_credit.c|1022| <<csched_alloc_vdata>> CSCHED_PRI_IDLE : CSCHED_PRI_TS_UNDER;
+ *   - common/sched_credit.c|1157| <<csched_vcpu_wake>> if ( svc->pri == CSCHED_PRI_TS_UNDER &&
+ *   - common/sched_credit.c|1364| <<csched_runq_sort>> if ( svc_elem->pri >= CSCHED_PRI_TS_UNDER )
+ *   - common/sched_credit.c|1545| <<csched_acct>> svc->pri = CSCHED_PRI_TS_UNDER;
+ *
+ * 在以下使用CSCHED_PRI_TS_OVER:
+ *   - common/sched_credit.c|43| <<global>> #define CSCHED_PRI_TS_OVER -2
+ *   - common/sched_credit.c|1524| <<csched_acct>> svc->pri = CSCHED_PRI_TS_OVER;
+ *   - common/sched_credit.c|1738| <<csched_load_balance>> else if ( snext->pri == CSCHED_PRI_TS_OVER )
+ *   - common/sched_credit.c|1942| <<csched_schedule>> if ( snext->pri > CSCHED_PRI_TS_OVER )
+ *
+ * 在以下使用CSCHED_PRI_IDLE:
+ *   - common/sched_credit.c|59| <<global>> #define CSCHED_PRI_IDLE -64
+ *   - common/sched_credit.c|388| <<__runq_insert>> && __runq_elem(iter)->pri > CSCHED_PRI_IDLE )
+ *   - common/sched_credit.c|530| <<__runq_tickle>> if ( cur->pri == CSCHED_PRI_IDLE
+ *   - common/sched_credit.c|533| <<__runq_tickle>> if ( cur->pri != CSCHED_PRI_IDLE )
+ *   - common/sched_credit.c|1058| <<csched_alloc_vdata>> CSCHED_PRI_IDLE : CSCHED_PRI_TS_UNDER;
+ *   - common/sched_credit.c|1776| <<csched_load_balance>> if ( snext->pri == CSCHED_PRI_IDLE )
+ *   - common/sched_credit.c|1915| <<csched_schedule>> scurr->pri = CSCHED_PRI_IDLE;
+ *   - common/sched_credit.c|1991| <<csched_schedule>> if ( snext->pri == CSCHED_PRI_IDLE )
+ */
 #define CSCHED_PRI_TS_BOOST      0      /* time-share waking up */
 #define CSCHED_PRI_TS_UNDER     -1      /* time-share w/ credits */
 #define CSCHED_PRI_TS_OVER      -2      /* time-share w/o credits */
@@ -47,7 +97,20 @@
 /*
  * Flags
  */
+/*
+ * 在以下使用CSCHED_FLAG_VCPU_PARKED:
+ *   - common/sched_credit.c|1020| <<csched_vcpu_remove>> if ( test_and_clear_bit(CSCHED_FLAG_VCPU_PARKED, &svc->flags) )
+ *   - common/sched_credit.c|1106| <<csched_vcpu_wake>> !test_bit(CSCHED_FLAG_VCPU_PARKED, &svc->flags) )
+ *   - common/sched_credit.c|1477| <<csched_acct>> !test_and_set_bit(CSCHED_FLAG_VCPU_PARKED, &svc->flags) )
+ *   - common/sched_credit.c|1496| <<csched_acct>> if ( test_and_clear_bit(CSCHED_FLAG_VCPU_PARKED, &svc->flags) )
+ */
 #define CSCHED_FLAG_VCPU_PARKED    0x0  /* VCPU over capped credits */
+/*
+ * 在以下使用CSCHED_FLAG_VCPU_YIELD:
+ *   - common/sched_credit.c|266| <<__runq_insert>> if ( test_bit(CSCHED_FLAG_VCPU_YIELD, &svc->flags)
+ *   - common/sched_credit.c|1041| <<csched_vcpu_yield>> set_bit(CSCHED_FLAG_VCPU_YIELD, &svc->flags);
+ *   - common/sched_credit.c|1774| <<csched_schedule>> clear_bit(CSCHED_FLAG_VCPU_YIELD, &scurr->flags);
+ */
 #define CSCHED_FLAG_VCPU_YIELD     0x1  /* VCPU yielding */
 
 
@@ -62,6 +125,10 @@
 #define CSCHED_DOM(_dom)    ((struct csched_dom *) (_dom)->sched_priv)
 #define RUNQ(_cpu)          (&(CSCHED_PCPU(_cpu)->runq))
 /* Is the first element of _cpu's runq its idle vcpu? */
+/*
+ * called by:
+ *   - common/sched_credit.c|772| <<_csched_cpu_pick>> if ( vc->processor == cpu && IS_RUNQ_IDLE(cpu) )
+ */
 #define IS_RUNQ_IDLE(_cpu)  (list_empty(RUNQ(_cpu)) || \
                              is_idle_vcpu(__runq_elem(RUNQ(_cpu)->next)->vcpu))
 
@@ -103,6 +170,10 @@
  * Credit tracing events ("only" 512 available!). Check
  * include/public/trace.h for more details.
  */
+/*
+ * 在以下使用TRC_CSCHED_SCHED_TASKLET:
+ *   - common/sched_credit.c|1875| <<csched_schedule>> TRACE_0D(TRC_CSCHED_SCHED_TASKLET);
+ */
 #define TRC_CSCHED_SCHED_TASKLET TRC_SCHED_CLASS_EVT(CSCHED, 1)
 #define TRC_CSCHED_ACCOUNT_START TRC_SCHED_CLASS_EVT(CSCHED, 2)
 #define TRC_CSCHED_ACCOUNT_STOP  TRC_SCHED_CLASS_EVT(CSCHED, 3)
@@ -128,12 +199,26 @@
  * does not find any suitable and free pcpu, we fall back checking the hard
  * affinity.
  */
+/*
+ * CSCHED_BALANCE_SOFT_AFFINITY: each vcpu has some pcpus that it prefers
+ * CSCHED_BALANCE_HARD_AFFINITY: some that it does not prefer but is OK with
+ *                             : and some that it cannot run on at all
+ */
 #define CSCHED_BALANCE_SOFT_AFFINITY    0
 #define CSCHED_BALANCE_HARD_AFFINITY    1
 
 /*
  * Boot parameters
  */
+/*
+ * 在以下使用sched_credit_tslice_ms:
+ *   - common/sched_credit.c|151| <<global>> integer_param("sched_credit_tslice_ms", sched_credit_tslice_ms);
+ *   - common/sched_credit.c|2095| <<csched_init>> if ( sched_credit_tslice_ms > XEN_SYSCTL_CSCHED_TSLICE_MAX
+ *   - common/sched_credit.c|2096| <<csched_init>> || sched_credit_tslice_ms < XEN_SYSCTL_CSCHED_TSLICE_MIN )
+ *   - common/sched_credit.c|2103| <<csched_init>> sched_credit_tslice_ms = CSCHED_DEFAULT_TSLICE_MS;
+ *   - common/sched_credit.c|2106| <<csched_init>> __csched_set_tslice(prv, sched_credit_tslice_ms);
+ *   - common/sched_credit.c|2108| <<csched_init>> if ( MICROSECS(sched_ratelimit_us) > MILLISECS(sched_credit_tslice_ms) )
+ */
 static int __read_mostly sched_credit_tslice_ms = CSCHED_DEFAULT_TSLICE_MS;
 integer_param("sched_credit_tslice_ms", sched_credit_tslice_ms);
 
@@ -147,6 +232,20 @@ struct csched_pcpu {
     unsigned int tick;
     unsigned int idle_bias;
     /* Store this here to avoid having too many cpumask_var_t-s on stack */
+    /*
+     * 在以下使用csched_balance_mask():
+     *   - common/sched_credit.c|473| <<__runq_tickle>> csched_balance_mask(cpu));
+     *   - common/sched_credit.c|474| <<__runq_tickle>> cpumask_and(csched_balance_mask(cpu),
+     *   - common/sched_credit.c|475| <<__runq_tickle>> csched_balance_mask(cpu), &idle_mask);
+     *   - common/sched_credit.c|476| <<__runq_tickle>> new_idlers_empty = cpumask_empty(csched_balance_mask(cpu));
+     *   - common/sched_credit.c|1634| <<csched_runq_steal>> csched_balance_cpumask(vc, balance_step, csched_balance_mask(cpu));
+     *   - common/sched_credit.c|1636| <<csched_runq_steal>> csched_balance_mask(cpu)) )
+     *
+     * 在以下使用csched_pcpu->balance_mask:
+     *   - common/sched_credit.c|173| <<csched_balance_mask>> #define csched_balance_mask(c) (CSCHED_PCPU(c)->balance_mask)
+     *   - common/sched_credit.c|571| <<csched_free_pdata>> free_cpumask_var(spc->balance_mask);
+     *   - common/sched_credit.c|590| <<csched_alloc_pdata>> if ( !alloc_cpumask_var(&spc->balance_mask) )
+     */
     cpumask_var_t balance_mask;
 };
 
@@ -157,15 +256,47 @@ struct csched_pcpu {
  * as there is a csched_pcpu for each PCPU, and we always hold the
  * runqueue lock for the proper PCPU when using this.
  */
+/*
+ * 在以下使用csched_balance_mask():
+ *   - common/sched_credit.c|473| <<__runq_tickle>> csched_balance_mask(cpu));
+ *   - common/sched_credit.c|474| <<__runq_tickle>> cpumask_and(csched_balance_mask(cpu),
+ *   - common/sched_credit.c|475| <<__runq_tickle>> csched_balance_mask(cpu), &idle_mask);
+ *   - common/sched_credit.c|476| <<__runq_tickle>> new_idlers_empty = cpumask_empty(csched_balance_mask(cpu));
+ *   - common/sched_credit.c|1634| <<csched_runq_steal>> csched_balance_cpumask(vc, balance_step, csched_balance_mask(cpu));
+ *   - common/sched_credit.c|1636| <<csched_runq_steal>> csched_balance_mask(cpu)) )
+ */
 #define csched_balance_mask(c) (CSCHED_PCPU(c)->balance_mask)
 
 /*
  * Virtual CPU
  */
 struct csched_vcpu {
+    /*
+     * 在以下使用csched_vcpu->runq_elem:
+     *   - common/sched_credit.c|232| <<__vcpu_on_runq>> return !list_empty(&svc->runq_elem);
+     *   - common/sched_credit.c|275| <<__runq_insert>> list_add_tail(&svc->runq_elem, iter);
+     *   - common/sched_credit.c|282| <<__runq_remove>> list_del_init(&svc->runq_elem);
+     *   - common/sched_credit.c|899| <<csched_alloc_vdata>> INIT_LIST_HEAD(&svc->runq_elem);
+     *   - common/sched_credit.c|930| <<csched_free_vdata>> BUG_ON( !list_empty(&svc->runq_elem) );
+     *   - common/sched_credit.c|962| <<csched_vcpu_remove>> BUG_ON( !list_empty(&svc->runq_elem) );
+     */
     struct list_head runq_elem;
+    /*
+     * 在以下使用csched_vcpu->active_vcpu_elem:
+     *   - common/sched_credit.c|800| <<__csched_vcpu_acct_start>> if ( list_empty(&svc->active_vcpu_elem) )
+     *   - common/sched_credit.c|806| <<__csched_vcpu_acct_start>> list_add(&svc->active_vcpu_elem, &sdom->active_vcpu);
+     *   - common/sched_credit.c|827| <<__csched_vcpu_acct_stop_locked>> BUG_ON( list_empty(&svc->active_vcpu_elem) );
+     *   - common/sched_credit.c|834| <<__csched_vcpu_acct_stop_locked>> list_del_init(&svc->active_vcpu_elem);
+     *   - common/sched_credit.c|876| <<csched_vcpu_acct>> if ( list_empty(&svc->active_vcpu_elem) )
+     *   - common/sched_credit.c|900| <<csched_alloc_vdata>> INIT_LIST_HEAD(&svc->active_vcpu_elem);
+     *   - common/sched_credit.c|956| <<csched_vcpu_remove>> if ( !list_empty(&svc->active_vcpu_elem) )
+     */
     struct list_head active_vcpu_elem;
     struct csched_dom *sdom;
+    /*
+     * 只在这里修改csched_vcpu->vcpu:
+     *   - common/sched_credit.c|889| <<csched_alloc_vdata>> svc->vcpu = vc;
+     */
     struct vcpu *vcpu;
     atomic_t credit;
     unsigned int residual;
@@ -189,9 +320,37 @@ struct csched_vcpu {
  * Domain
  */
 struct csched_dom {
+    /*
+     * 在以下使用csched_dom->active_vcpu:
+     *   - common/sched_credit.c|806| <<__csched_vcpu_acct_start>> list_add(&svc->active_vcpu_elem, &sdom->active_vcpu);
+     *   - common/sched_credit.c|836| <<__csched_vcpu_acct_stop_locked>> if ( list_empty(&sdom->active_vcpu) )
+     *   - common/sched_credit.c|1139| <<csched_alloc_domdata>> INIT_LIST_HEAD(&sdom->active_vcpu);
+     *   - common/sched_credit.c|1355| <<csched_acct>> list_for_each_safe( iter_vcpu, next_vcpu, &sdom->active_vcpu )
+     *   - common/sched_credit.c|1942| <<csched_dump>> list_for_each( iter_svc, &sdom->active_vcpu )
+     */
     struct list_head active_vcpu;
     struct list_head active_sdom_elem;
     struct domain *dom;
+    /*
+     * 在以下修改csched_dom->active_vcpu_count:
+     *   - common/sched_credit.c|966| <<__csched_vcpu_acct_start>> sdom->active_vcpu_count++;
+     *   - common/sched_credit.c|994| <<__csched_vcpu_acct_stop_locked>> sdom->active_vcpu_count--;
+     * 在以下使用csched_dom->active_vcpu_count:
+     *   - common/sched_credit.c|977| <<__csched_vcpu_acct_start>> svc->vcpu->vcpu_id, sdom->active_vcpu_count);
+     *   - common/sched_credit.c|1003| <<__csched_vcpu_acct_stop_locked>> svc->vcpu->vcpu_id, sdom->active_vcpu_count);
+     *   - common/sched_credit.c|1260| <<csched_dom_cntl>> prv->weight -= sdom->weight * sdom->active_vcpu_count;
+     *   - common/sched_credit.c|1261| <<csched_dom_cntl>> prv->weight += op->u.credit.weight * sdom->active_vcpu_count;
+     *   - common/sched_credit.c|1483| <<csched_acct>> BUG_ON( sdom->active_vcpu_count == 0 );
+     *   - common/sched_credit.c|1485| <<csched_acct>> BUG_ON( (sdom->weight * sdom->active_vcpu_count) > weight_left );
+     *   - common/sched_credit.c|1487| <<csched_acct>> weight_left -= ( sdom->weight * sdom->active_vcpu_count );
+     *   - common/sched_credit.c|1497| <<csched_acct>> credit_peak = sdom->active_vcpu_count * prv->credits_per_tslice;
+     *   - common/sched_credit.c|1502| <<csched_acct>> * sdom->active_vcpu_count) +
+     *   - common/sched_credit.c|1514| <<csched_acct>> credit_cap = ( credit_cap + ( sdom->active_vcpu_count - 1 )
+     *   - common/sched_credit.c|1515| <<csched_acct>> ) / sdom->active_vcpu_count;
+     *   - common/sched_credit.c|1520| <<csched_acct>> * sdom->active_vcpu_count )
+     *   - common/sched_credit.c|1555| <<csched_acct>> credit_fair = ( credit_fair + ( sdom->active_vcpu_count - 1 )
+     *   - common/sched_credit.c|1556| <<csched_acct>> ) / sdom->active_vcpu_count;
+     */
     uint16_t active_vcpu_count;
     uint16_t weight;
     uint16_t cap;
@@ -234,6 +393,12 @@ __runq_elem(struct list_head *elem)
     return list_entry(elem, struct csched_vcpu, runq_elem);
 }
 
+/*
+ * called by:
+ *   - common/sched_credit.c|901| <<csched_vcpu_insert>> __runq_insert(vc->processor, svc);
+ *   - common/sched_credit.c|1012| <<csched_vcpu_wake>> __runq_insert(cpu, svc);
+ *   - common/sched_credit.c|1687| <<csched_schedule>> __runq_insert(cpu, scurr);
+ */
 static inline void
 __runq_insert(unsigned int cpu, struct csched_vcpu *svc)
 {
@@ -241,6 +406,21 @@ __runq_insert(unsigned int cpu, struct csched_vcpu *svc)
     struct list_head *iter;
 
     BUG_ON( __vcpu_on_runq(svc) );
+    /*
+     * 这里稍后被删除了
+     * commit e59321d1544518cd322a4dc09a4bff482c724f1c
+     * Author: Harmandeep Kaur <write.harmandeep@gmail.com>
+     * Date:   Wed Nov 4 17:46:46 2015 +0100
+     *
+     * credit: remove cpu argument to __runq_insert()
+     *
+     * __runq_insert() takes two arguments, cpu and svc. However,
+     * the cpu argument is redundant because we can get all the
+     * information we need about cpu from svc.
+     *
+     * Signed-off-by: Harmandeep Kaur <write.harmandeep@gmail.com>
+     * Acked-by: Dario Faggioli <dario.faggioli@citrix.com>
+     */
     BUG_ON( cpu != svc->vcpu->processor );
 
     list_for_each( iter, runq )
@@ -253,6 +433,12 @@ __runq_insert(unsigned int cpu, struct csched_vcpu *svc)
     /* If the vcpu yielded, try to put it behind one lower-priority
      * runnable vcpu if we can.  The next runq_sort will bring it forward
      * within 30ms if the queue too long. */
+    /*
+     * 在以下使用CSCHED_FLAG_VCPU_YIELD:
+     *   - common/sched_credit.c|266| <<__runq_insert>> if ( test_bit(CSCHED_FLAG_VCPU_YIELD, &svc->flags)
+     *   - common/sched_credit.c|1041| <<csched_vcpu_yield>> set_bit(CSCHED_FLAG_VCPU_YIELD, &svc->flags);
+     *   - common/sched_credit.c|1774| <<csched_schedule>> clear_bit(CSCHED_FLAG_VCPU_YIELD, &scurr->flags);
+     */
     if ( test_bit(CSCHED_FLAG_VCPU_YIELD, &svc->flags)
          && __runq_elem(iter)->pri > CSCHED_PRI_IDLE )
     {
@@ -265,6 +451,14 @@ __runq_insert(unsigned int cpu, struct csched_vcpu *svc)
     list_add_tail(&svc->runq_elem, iter);
 }
 
+/*
+ * called by:
+ *   - common/sched_credit.c|952| <<csched_vcpu_remove>> __runq_remove(svc);
+ *   - common/sched_credit.c|977| <<csched_vcpu_sleep>> __runq_remove(svc);
+ *   - common/sched_credit.c|1535| <<csched_runq_steal>> __runq_remove(speer);
+ *   - common/sched_credit.c|1664| <<csched_load_balance>> __runq_remove(snext);
+ *   - common/sched_credit.c|1785| <<csched_schedule>> __runq_remove(snext);
+ */
 static inline void
 __runq_remove(struct csched_vcpu *svc)
 {
@@ -273,6 +467,11 @@ __runq_remove(struct csched_vcpu *svc)
 }
 
 
+/*
+ * CSCHED_BALANCE_SOFT_AFFINITY: each vcpu has some pcpus that it prefers
+ * CSCHED_BALANCE_HARD_AFFINITY: some that it does not prefer but is OK with
+ *                             : and some that it cannot run on at all
+ */
 #define for_each_csched_balance_step(step) \
     for ( (step) = 0; (step) <= CSCHED_BALANCE_HARD_AFFINITY; (step)++ )
 
@@ -292,6 +491,10 @@ __runq_remove(struct csched_vcpu *svc)
 static inline int __vcpu_has_soft_affinity(const struct vcpu *vc,
                                            const cpumask_t *mask)
 {
+    /*
+     * // Bitmask of CPUs on which this VCPU prefers to run.
+     * cpumask_var_t    cpu_soft_affinity;
+     */
     if ( cpumask_full(vc->cpu_soft_affinity)
          || !cpumask_intersects(vc->cpu_soft_affinity, mask) )
         return 0;
@@ -306,6 +509,12 @@ static inline int __vcpu_has_soft_affinity(const struct vcpu *vc,
  * filtered out from the result, to avoid running a vcpu where it would
  * like, but is not allowed to!
  */
+/*
+ * called by:
+ *   - common/sched_credit.c|563| <<__runq_tickle>> csched_balance_cpumask(new->vcpu, balance_step,
+ *   - common/sched_credit.c|837| <<_csched_cpu_pick>> csched_balance_cpumask(vc, balance_step, &cpus);
+ *   - common/sched_credit.c|1733| <<csched_runq_steal>> csched_balance_cpumask(vc, balance_step, csched_balance_mask(cpu));
+ */
 static void
 csched_balance_cpumask(const struct vcpu *vc, int step, cpumask_t *mask)
 {
@@ -320,6 +529,11 @@ csched_balance_cpumask(const struct vcpu *vc, int step, cpumask_t *mask)
         cpumask_copy(mask, vc->cpu_hard_affinity);
 }
 
+/*
+ * called by:
+ *   - common/sched_credit.c|866| <<csched_vcpu_acct>> burn_credits(svc, NOW());
+ *   - common/sched_credit.c|1711| <<csched_schedule>> burn_credits(scurr, now);
+ */
 static void burn_credits(struct csched_vcpu *svc, s_time_t now)
 {
     s_time_t delta;
@@ -343,8 +557,22 @@ static void burn_credits(struct csched_vcpu *svc, s_time_t now)
 static bool_t __read_mostly opt_tickle_one_idle = 1;
 boolean_param("tickle_one_idle_cpu", opt_tickle_one_idle);
 
+/*
+ * 在以下使用percpu的last_tickle_cpu:
+ *   - common/sched_credit.c|406| <<global>> DEFINE_PER_CPU(unsigned int , last_tickle_cpu);
+ *   - common/sched_credit.c|499| <<__runq_tickle>> this_cpu(last_tickle_cpu) =
+ *   - common/sched_credit.c|500| <<__runq_tickle>> cpumask_cycle(this_cpu(last_tickle_cpu), &idle_mask);
+ *   - common/sched_credit.c|501| <<__runq_tickle>> __cpumask_set_cpu(this_cpu(last_tickle_cpu), &mask);
+ */
 DEFINE_PER_CPU(unsigned int, last_tickle_cpu);
 
+/*
+ * called by:
+ *   - common/sched_credit.c|1211| <<csched_vcpu_wake>> __runq_tickle(cpu, svc);
+ *
+ * 核心思想是计算要触发schedule的mask,
+ * 然后cpumask_raise_softirq(&mask, SCHEDULE_SOFTIRQ);
+ */
 static inline void
 __runq_tickle(unsigned int cpu, struct csched_vcpu *new)
 {
@@ -464,6 +692,9 @@ __runq_tickle(unsigned int cpu, struct csched_vcpu *new)
     }
 }
 
+/*
+ * struct scheduler sched_credit_def.free_pdata = csched_free_pdata()
+ */
 static void
 csched_free_pdata(const struct scheduler *ops, void *pcpu, int cpu)
 {
@@ -495,6 +726,9 @@ csched_free_pdata(const struct scheduler *ops, void *pcpu, int cpu)
     xfree(spc);
 }
 
+/*
+ * struct scheduler sched_credit_def.alloc_pdata = csched_alloc_pdata()
+ */
 static void *
 csched_alloc_pdata(const struct scheduler *ops, int cpu)
 {
@@ -577,9 +811,22 @@ __csched_vcpu_check(struct vcpu *vc)
  * implicit overheads such as cache-warming. 1ms (1000) has been measured
  * as a good value.
  */
+/*
+ * 在以下使用vcpu_migration_delay:
+ *   - common/sched_credit.c|751| <<global>> static unsigned int vcpu_migration_delay;
+ *   - common/sched_credit.c|752| <<global>> integer_param("vcpu_migration_delay", vcpu_migration_delay);
+ *   - common/sched_credit.c|756| <<set_vcpu_migration_delay>> vcpu_migration_delay = delay;
+ *   - common/sched_credit.c|761| <<get_vcpu_migration_delay>> return vcpu_migration_delay;
+ *   - common/sched_credit.c|768| <<__csched_vcpu_is_cache_hot>> ((uint64_t)vcpu_migration_delay * 1000u));
+ *   - common/sched_credit.c|2144| <<csched_dump>> vcpu_migration_delay);
+ */
 static unsigned int vcpu_migration_delay;
 integer_param("vcpu_migration_delay", vcpu_migration_delay);
 
+/*
+ * called by:
+ *   - drivers/acpi/pmstat.c|456| <<do_pm_op(XEN_SYSCTL_pm_op_set_vcpu_migration_delay)>> set_vcpu_migration_delay(op->u.set_vcpu_migration_delay);
+ */
 void set_vcpu_migration_delay(unsigned int delay)
 {
     vcpu_migration_delay = delay;
@@ -590,6 +837,10 @@ unsigned int get_vcpu_migration_delay(void)
     return vcpu_migration_delay;
 }
 
+/*
+ * called by:
+ *   - common/sched_credit.c|789| <<__csched_vcpu_is_migrateable>> !__csched_vcpu_is_cache_hot(vc) &&
+ */
 static inline int
 __csched_vcpu_is_cache_hot(struct vcpu *v)
 {
@@ -602,6 +853,10 @@ __csched_vcpu_is_cache_hot(struct vcpu *v)
     return hot;
 }
 
+/*
+ * called by:
+ *   - common/sched_credit.c|1499| <<csched_runq_steal>> if ( __csched_vcpu_is_migrateable(vc, cpu,
+ */
 static inline int
 __csched_vcpu_is_migrateable(struct vcpu *vc, int dest_cpu, cpumask_t *mask)
 {
@@ -615,6 +870,13 @@ __csched_vcpu_is_migrateable(struct vcpu *vc, int dest_cpu, cpumask_t *mask)
            cpumask_test_cpu(dest_cpu, mask);
 }
 
+/*
+ * called by:
+ *   - common/sched_credit.c|784| <<csched_cpu_pick>> return _csched_cpu_pick(ops, vc, 1);
+ *   - common/sched_credit.c|875| <<csched_vcpu_acct>> else if ( _csched_cpu_pick(ops, current, 0) != cpu )
+ *
+ * 核心思想是为vcpu选择一个pcpu
+ */
 static int
 _csched_cpu_pick(const struct scheduler *ops, struct vcpu *vc, bool_t commit)
 {
@@ -761,12 +1023,22 @@ _csched_cpu_pick(const struct scheduler *ops, struct vcpu *vc, bool_t commit)
     return cpu;
 }
 
+/*
+ * called by:
+ *   - common/schedule.c|487| <<vcpu_migrate>> new_cpu = SCHED_OP(VCPU2OP(v), pick_cpu, v);
+ *
+ * struct scheduler sched_credit_def.pick_cpu = csched_cpu_pick()
+ */
 static int
 csched_cpu_pick(const struct scheduler *ops, struct vcpu *vc)
 {
     return _csched_cpu_pick(ops, vc, 1);
 }
 
+/*
+ * called by:
+ *   - common/sched_credit.c|1039| <<csched_vcpu_acct>> __csched_vcpu_acct_start(prv, svc);
+ */
 static inline void
 __csched_vcpu_acct_start(struct csched_private *prv, struct csched_vcpu *svc)
 {
@@ -775,12 +1047,30 @@ __csched_vcpu_acct_start(struct csched_private *prv, struct csched_vcpu *svc)
 
     spin_lock_irqsave(&prv->lock, flags);
 
+    /*
+     * 在以下使用csched_vcpu->active_vcpu_elem:
+     *   - common/sched_credit.c|800| <<__csched_vcpu_acct_start>> if ( list_empty(&svc->active_vcpu_elem) )
+     *   - common/sched_credit.c|806| <<__csched_vcpu_acct_start>> list_add(&svc->active_vcpu_elem, &sdom->active_vcpu);
+     *   - common/sched_credit.c|827| <<__csched_vcpu_acct_stop_locked>> BUG_ON( list_empty(&svc->active_vcpu_elem) );
+     *   - common/sched_credit.c|834| <<__csched_vcpu_acct_stop_locked>> list_del_init(&svc->active_vcpu_elem);
+     *   - common/sched_credit.c|876| <<csched_vcpu_acct>> if ( list_empty(&svc->active_vcpu_elem) )
+     *   - common/sched_credit.c|900| <<csched_alloc_vdata>> INIT_LIST_HEAD(&svc->active_vcpu_elem);
+     *   - common/sched_credit.c|956| <<csched_vcpu_remove>> if ( !list_empty(&svc->active_vcpu_elem) )
+     */
     if ( list_empty(&svc->active_vcpu_elem) )
     {
         SCHED_VCPU_STAT_CRANK(svc, state_active);
         SCHED_STAT_CRANK(acct_vcpu_active);
 
         sdom->active_vcpu_count++;
+	/*
+         * 在以下使用csched_dom->active_vcpu:
+         *   - common/sched_credit.c|806| <<__csched_vcpu_acct_start>> list_add(&svc->active_vcpu_elem, &sdom->active_vcpu);
+         *   - common/sched_credit.c|836| <<__csched_vcpu_acct_stop_locked>> if ( list_empty(&sdom->active_vcpu) )
+         *   - common/sched_credit.c|1139| <<csched_alloc_domdata>> INIT_LIST_HEAD(&sdom->active_vcpu);
+         *   - common/sched_credit.c|1355| <<csched_acct>> list_for_each_safe( iter_vcpu, next_vcpu, &sdom->active_vcpu )
+         *   - common/sched_credit.c|1942| <<csched_dump>> list_for_each( iter_svc, &sdom->active_vcpu )
+	 */
         list_add(&svc->active_vcpu_elem, &sdom->active_vcpu);
         /* Make weight per-vcpu */
         prv->weight += sdom->weight;
@@ -820,6 +1110,10 @@ __csched_vcpu_acct_stop_locked(struct csched_private *prv,
              svc->vcpu->vcpu_id, sdom->active_vcpu_count);
 }
 
+/*
+ * called by:
+ *   - common/sched_credit.c|1655| <<csched_tick>> csched_vcpu_acct(prv, cpu);
+ */
 static void
 csched_vcpu_acct(struct csched_private *prv, unsigned int cpu)
 {
@@ -864,6 +1158,9 @@ csched_vcpu_acct(struct csched_private *prv, unsigned int cpu)
     }
 }
 
+/*
+ * struct scheduler sched_credit_def.alloc_vdata = csched_alloc_vdata()
+ */
 static void *
 csched_alloc_vdata(const struct scheduler *ops, struct vcpu *vc, void *dd)
 {
@@ -885,6 +1182,13 @@ csched_alloc_vdata(const struct scheduler *ops, struct vcpu *vc, void *dd)
     return svc;
 }
 
+/*
+ * called by:
+ *   - common/schedule.c|236| <<sched_init_vcpu>> SCHED_OP(DOM2OP(d), insert_vcpu, v);
+ *   - common/schedule.c|324| <<sched_move_domain>> SCHED_OP(c->sched, insert_vcpu, v);
+ *
+ * struct scheduler sched_credit_def.insert_vcpu = csched_vcpu_insert()
+ */
 static void
 csched_vcpu_insert(const struct scheduler *ops, struct vcpu *vc)
 {
@@ -900,6 +1204,9 @@ csched_vcpu_insert(const struct scheduler *ops, struct vcpu *vc)
     vcpu_schedule_unlock_irqrestore(lock, flags, vc);
 }
 
+/*
+ * struct scheduler sched_credit_def.free_vdata = csched_free_vdata()
+ */
 static void
 csched_free_vdata(const struct scheduler *ops, void *priv)
 {
@@ -910,6 +1217,9 @@ csched_free_vdata(const struct scheduler *ops, void *priv)
     xfree(svc);
 }
 
+/*
+ * struct scheduler sched_credit_def.remove_vcpu = csched_vcpu_remove()
+ */
 static void
 csched_vcpu_remove(const struct scheduler *ops, struct vcpu *vc)
 {
@@ -940,6 +1250,9 @@ csched_vcpu_remove(const struct scheduler *ops, struct vcpu *vc)
     BUG_ON( !list_empty(&svc->runq_elem) );
 }
 
+/*
+ * struct scheduler sched_credit_def.sleep = csched_vcpu_sleep()
+ */
 static void
 csched_vcpu_sleep(const struct scheduler *ops, struct vcpu *vc)
 {
@@ -955,6 +1268,12 @@ csched_vcpu_sleep(const struct scheduler *ops, struct vcpu *vc)
         __runq_remove(svc);
 }
 
+/*
+ * called by:
+ *   - common/schedule.c|405| <<vcpu_wake>> SCHED_OP(VCPU2OP(v), wake, v);
+ *
+ * struct scheduler sched_credit_def.wake = csched_vcpu_wake()
+ */
 static void
 csched_vcpu_wake(const struct scheduler *ops, struct vcpu *vc)
 {
@@ -1010,6 +1329,13 @@ csched_vcpu_wake(const struct scheduler *ops, struct vcpu *vc)
     __runq_tickle(cpu, svc);
 }
 
+/*
+ * do_yield()被以下调用:
+ *   - common/schedule.c|895| <<do_sched_op_compat(SCHEDOP_yield)>> ret = do_yield();
+ *   - common/schedule.c|932| <<do_sched_op(SCHEDOP_yield)>> ret = do_yield();
+ *
+ * struct scheduler sched_credit_def.yield = csched_vcpu_yield()
+ */
 static void
 csched_vcpu_yield(const struct scheduler *ops, struct vcpu *vc)
 {
@@ -1019,6 +1345,9 @@ csched_vcpu_yield(const struct scheduler *ops, struct vcpu *vc)
     set_bit(CSCHED_FLAG_VCPU_YIELD, &svc->flags);
 }
 
+/*
+ * struct scheduler sched_credit_def.adjust = csched_dom_cntl()
+ */
 static int
 csched_dom_cntl(
     const struct scheduler *ops,
@@ -1073,6 +1402,9 @@ __csched_set_tslice(struct csched_private *prv, unsigned timeslice)
     prv->credits_per_tslice = CSCHED_CREDITS_PER_MSEC * prv->tslice_ms;
 }
 
+/*
+ * struct scheduler sched_credit_def.adjust_global = csched_sys_cntl()
+ */
 static int
 csched_sys_cntl(const struct scheduler *ops,
                         struct xen_sysctl_scheduler_op *sc)
@@ -1104,6 +1436,9 @@ csched_sys_cntl(const struct scheduler *ops,
     return rc;
 }
 
+/*
+ * struct scheduler sched_credit_def.alloc_domdata = csched_alloc_domdata()
+ */
 static void *
 csched_alloc_domdata(const struct scheduler *ops, struct domain *dom)
 {
@@ -1122,6 +1457,9 @@ csched_alloc_domdata(const struct scheduler *ops, struct domain *dom)
     return (void *)sdom;
 }
 
+/*
+ * struct scheduler sched_credit_def.init_domain = csched_domain_init()
+ */
 static int
 csched_dom_init(const struct scheduler *ops, struct domain *dom)
 {
@@ -1139,12 +1477,18 @@ csched_dom_init(const struct scheduler *ops, struct domain *dom)
     return 0;
 }
 
+/*
+ * struct scheduler sched_credit_def.free_domdata = csched_free_domdata()
+ */
 static void
 csched_free_domdata(const struct scheduler *ops, void *data)
 {
     xfree(data);
 }
 
+/*
+ * struct scheduler sched_credit_def.destroy_domain = csched_dom_destroy()
+ */
 static void
 csched_dom_destroy(const struct scheduler *ops, struct domain *dom)
 {
@@ -1202,6 +1546,10 @@ csched_runq_sort(struct csched_private *prv, unsigned int cpu)
     pcpu_schedule_unlock_irqrestore(lock, flags, cpu);
 }
 
+/*
+ * 在以下使用csched_acct():
+ *   - common/sched_credit.c|696| <<csched_alloc_pdata>> init_timer(&prv->master_ticker, csched_acct, prv, cpu);
+ */
 static void
 csched_acct(void* dummy)
 {
@@ -1409,6 +1757,10 @@ out:
                NOW() + MILLISECS(prv->tslice_ms));
 }
 
+/*
+ * 在以下使用csched_tick():
+ *   - common/sched_credit.c|610| <<csched_alloc_pdata>> init_timer(&spc->ticker, csched_tick, (void *)(unsigned long )cpu, cpu);
+ */
 static void
 csched_tick(void *_cpu)
 {
@@ -1436,9 +1788,24 @@ csched_tick(void *_cpu)
     set_timer(&spc->ticker, NOW() + MICROSECS(prv->tick_period_us) );
 }
 
+/*
+ * called by:
+ *   - common/sched_credit.c|1598| <<csched_load_balance>> csched_runq_steal(peer_cpu, cpu, snext->pri, bstep) : NULL;
+ */
 static struct csched_vcpu *
 csched_runq_steal(int peer_cpu, int cpu, int pri, int balance_step)
 {
+	/*
+	 * struct csched_pcpu {
+	 *     struct list_head runq;
+	 *     uint32_t runq_sort_last;
+	 *     struct timer ticker;
+	 *     unsigned int tick;
+	 *     unsigned int idle_bias;
+	 *     // Store this here to avoid having too many cpumask_var_t-s on stack
+	 *     cpumask_var_t balance_mask;
+	 * };
+	 */
     const struct csched_pcpu * const peer_pcpu = CSCHED_PCPU(peer_cpu);
     const struct vcpu * const peer_vcpu = curr_on_cpu(peer_cpu);
     struct csched_vcpu *speer;
@@ -1463,6 +1830,9 @@ csched_runq_steal(int peer_cpu, int cpu, int pri, int balance_step)
                 break;
 
             /* Is this VCPU runnable on our PCPU? */
+	    /*
+	     * struct vcpu *vc;
+	     */
             vc = speer->vcpu;
             BUG_ON( is_idle_vcpu(vc) );
 
@@ -1493,6 +1863,13 @@ csched_runq_steal(int peer_cpu, int cpu, int pri, int balance_step)
                 SCHED_STAT_CRANK(migrate_queued);
                 WARN_ON(vc->is_urgent);
                 __runq_remove(speer);
+		/*
+		 * 在以下修改vcpu->processor(credit scheduler不支持migrate):
+		 *   - common/sched_credit.c|1509| <<csched_runq_steal>> vc->processor = cpu;
+		 *   - common/schedule.c|195| <<sched_init_vcpu>> v->processor = processor;
+		 *   - common/schedule.c|294| <<sched_move_domain>> v->processor = new_p;
+		 *   - common/schedule.c|531| <<vcpu_migrate>> v->processor = new_cpu;
+		 */
                 vc->processor = cpu;
                 return speer;
             }
@@ -1503,6 +1880,10 @@ csched_runq_steal(int peer_cpu, int cpu, int pri, int balance_step)
     return NULL;
 }
 
+/*
+ * called by:
+ *   - common/sched_credit.c|1705| <<csched_schedule>> snext = csched_load_balance(prv, cpu, snext, &ret.migrated);
+ */
 static struct csched_vcpu *
 csched_load_balance(struct csched_private *prv, int cpu,
     struct csched_vcpu *snext, bool_t *stolen)
@@ -1538,6 +1919,14 @@ csched_load_balance(struct csched_private *prv, int cpu,
      *  1. any "soft-affine work" to steal first,
      *  2. if not finding anything, any "hard-affine work" to steal.
      */
+    /*
+     * 声明
+     * int peer_cpu, peer_node, bstep;
+     * 
+     * 从0-1 ...
+     * #define CSCHED_BALANCE_SOFT_AFFINITY    0
+     * #define CSCHED_BALANCE_HARD_AFFINITY    1
+     */
     for_each_csched_balance_step( bstep )
     {
         /*
@@ -1577,6 +1966,9 @@ csched_load_balance(struct csched_private *prv, int cpu,
                 }
 
                 /* Any work over there to steal? */
+		/*
+		 * csched_runq_steal()只在这里调用
+		 */
                 speer = cpumask_test_cpu(peer_cpu, online) ?
                     csched_runq_steal(peer_cpu, cpu, snext->pri, bstep) : NULL;
                 pcpu_schedule_unlock(lock, peer_cpu);
@@ -1607,11 +1999,28 @@ csched_load_balance(struct csched_private *prv, int cpu,
  * This function is in the critical path. It is designed to be simple and
  * fast for the common case.
  */
+/*
+ * called by:
+ *   - common/schedule.c|1186| <<schedule>> next_slice = sched->do_schedule(sched, now, tasklet_work_scheduled);
+ *
+ * struct scheduler sched_credit_def.do_schedule = csched_schedule()
+ */
 static struct task_slice
 csched_schedule(
     const struct scheduler *ops, s_time_t now, bool_t tasklet_work_scheduled)
 {
     const int cpu = smp_processor_id();
+    /*
+     * struct csched_pcpu {
+     *     struct list_head runq;
+     *     uint32_t runq_sort_last;
+     *     struct timer ticker;
+     *     unsigned int tick;
+     *     unsigned int idle_bias;
+     *     // Store this here to avoid having too many cpumask_var_t-s on stack
+     *     cpumask_var_t balance_mask;
+     * };
+     */
     struct list_head * const runq = RUNQ(cpu);
     struct csched_vcpu * const scurr = CSCHED_VCPU(current);
     struct csched_private *prv = CSCHED_PRIV(ops);
@@ -1675,6 +2084,9 @@ csched_schedule(
     else
         BUG_ON( is_idle_vcpu(current) || list_empty(runq) );
 
+    /*
+     * struct list_head * const runq = RUNQ(cpu);
+     */
     snext = __runq_elem(runq->next);
     ret.migrated = 0;
 
@@ -1727,6 +2139,13 @@ out:
      */
     ret.time = (is_idle_vcpu(snext->vcpu) ?
                 -1 : tslice);
+    /*
+     * struct task_slice {
+     *     struct vcpu *task;
+     *     s_time_t     time;
+     *     bool_t       migrated;
+     * };
+     */
     ret.task = snext->vcpu;
 
     CSCHED_VCPU_CHECK(ret.task);
@@ -1764,6 +2183,9 @@ csched_dump_vcpu(struct csched_vcpu *svc)
     printk("\n");
 }
 
+/*
+ * struct scheduler sched_credit_def.dump_cpu_state = csched_dump_pcpu()
+ */
 static void
 csched_dump_pcpu(const struct scheduler *ops, int cpu)
 {
@@ -1802,6 +2224,9 @@ csched_dump_pcpu(const struct scheduler *ops, int cpu)
 #undef cpustr
 }
 
+/*
+ * struct scheduler sched_credit_def.dump_settings = csched_dump()
+ */
 static void
 csched_dump(const struct scheduler *ops)
 {
@@ -1864,6 +2289,9 @@ csched_dump(const struct scheduler *ops)
     spin_unlock_irqrestore(&(prv->lock), flags);
 }
 
+/*
+ * struct scheduler sched_credit_def.init = csched_init()
+ */
 static int
 csched_init(struct scheduler *ops)
 {
@@ -1910,6 +2338,9 @@ csched_init(struct scheduler *ops)
     return 0;
 }
 
+/*
+ * struct scheduler sched_credit_def.deinit = csched_deinit()
+ */
 static void
 csched_deinit(const struct scheduler *ops)
 {
@@ -1924,6 +2355,9 @@ csched_deinit(const struct scheduler *ops)
     }
 }
 
+/*
+ * struct scheduler sched_credit_def.tick_suspend = csched_tick_suspend()
+ */
 static void csched_tick_suspend(const struct scheduler *ops, unsigned int cpu)
 {
     struct csched_pcpu *spc;
@@ -1933,6 +2367,9 @@ static void csched_tick_suspend(const struct scheduler *ops, unsigned int cpu)
     stop_timer(&spc->ticker);
 }
 
+/*
+ * struct scheduler sched_credit_def.tick_resume = csched_tick_resume()
+ */
 static void csched_tick_resume(const struct scheduler *ops, unsigned int cpu)
 {
     struct csched_private *prv;
@@ -1947,8 +2384,20 @@ static void csched_tick_resume(const struct scheduler *ops, unsigned int cpu)
             - now % MICROSECS(prv->tick_period_us) );
 }
 
+/*
+ * struct scheduler sched_credit_def.sched_data = &_csched_priv;
+ */
 static struct csched_private _csched_priv;
 
+/*
+ * 在xen/common/schedule.c使用:
+ * static const struct scheduler *schedulers[] = {
+ *     &sched_sedf_def,
+ *     &sched_credit_def,
+ *     &sched_credit2_def,
+ *     &sched_arinc653_def,
+ * };
+ */
 const struct scheduler sched_credit_def = {
     .name           = "SMP Credit Scheduler",
     .opt_name       = "credit",
diff --git a/xen/common/schedule.c b/xen/common/schedule.c
index 3fce990fac..90686b19c4 100644
--- a/xen/common/schedule.c
+++ b/xen/common/schedule.c
@@ -137,6 +137,14 @@ static inline void vcpu_urgent_count_update(struct vcpu *v)
     }
 }
 
+/*
+ * called by:
+ *   - common/schedule.c|365| <<vcpu_sleep_nosync>> vcpu_runstate_change(v, RUNSTATE_offline, NOW());
+ *   - common/schedule.c|393| <<vcpu_wake>> vcpu_runstate_change(v, RUNSTATE_runnable, NOW());
+ *   - common/schedule.c|399| <<vcpu_wake>> vcpu_runstate_change(v, RUNSTATE_offline, NOW());
+ *   - common/schedule.c|1268| <<schedule>> vcpu_runstate_change(
+ *   - common/schedule.c|1276| <<schedule>> vcpu_runstate_change(next, RUNSTATE_running, now);
+ */
 static inline void vcpu_runstate_change(
     struct vcpu *v, int new_state, s_time_t new_entry_time)
 {
@@ -184,6 +192,10 @@ uint64_t get_cpu_idle_time(unsigned int cpu)
     return state.time[RUNSTATE_running];
 }
 
+/*
+ * called by:
+ *   - common/domain.c|232| <<alloc_vcpu>> if ( sched_init_vcpu(v, cpu_id) != 0 )
+ */
 int sched_init_vcpu(struct vcpu *v, unsigned int processor) 
 {
     struct domain *d = v->domain;
@@ -221,11 +233,18 @@ int sched_init_vcpu(struct vcpu *v, unsigned int processor)
     if ( v->sched_priv == NULL )
         return 1;
 
+    /*
+     * csched_vcpu_insert()
+     */
     SCHED_OP(DOM2OP(d), insert_vcpu, v);
 
     return 0;
 }
 
+/*
+ * called by:
+ *   - common/cpupool.c|235| <<cpupool_move_domain_locked>> ret = sched_move_domain(d, c);
+ */
 int sched_move_domain(struct domain *d, struct cpupool *c)
 {
     struct vcpu *v;
@@ -346,16 +365,43 @@ void sched_destroy_domain(struct domain *d)
     SCHED_OP(DOM2OP(d), destroy_domain, d);
 }
 
+/*
+ * called by:
+ *   - arch/arm/vpsci.c|82| <<do_psci_cpu_off>> vcpu_sleep_nosync(v);
+ *   - arch/x86/hvm/hvm.c|1494| <<hvm_vcpu_down>> vcpu_sleep_nosync(v);
+ *   - common/domain.c|940| <<vcpu_pause_nosync>> vcpu_sleep_nosync(v);
+ *   - common/domain.c|968| <<domain_pause_nosync>> vcpu_sleep_nosync(v);
+ *   - common/domain.c|1254| <<do_vcpu_op(VCPUOP_down)>> vcpu_sleep_nosync(v);
+ *   - common/hvm/save.c|223| <<hvm_load>> vcpu_sleep_nosync(v);
+ *   - common/schedule.c|391| <<vcpu_sleep_sync>> vcpu_sleep_nosync(v);
+ *   - common/schedule.c|601| <<vcpu_force_reschedule>> vcpu_sleep_nosync(v);
+ *   - common/schedule.c|625| <<restore_vcpu_affinity>> vcpu_sleep_nosync(v);
+ *   - common/schedule.c|681| <<cpu_disable_scheduler>> vcpu_sleep_nosync(v);
+ *   - common/schedule.c|738| <<vcpu_set_affinity>> vcpu_sleep_nosync(v);
+ */
 void vcpu_sleep_nosync(struct vcpu *v)
 {
     unsigned long flags;
+    /*
+     * called by:
+     *   - common/sched_credit.c|1199| <<csched_vcpu_insert>> lock = vcpu_schedule_lock_irqsave(vc, &flags);
+     *   - common/schedule.c|371| <<vcpu_sleep_nosync>> spinlock_t *lock = vcpu_schedule_lock_irqsave(v, &flags);
+     *   - common/schedule.c|414| <<vcpu_wake>> spinlock_t *lock = vcpu_schedule_lock_irqsave(v, &flags);
+     *   - common/schedule.c|658| <<cpu_disable_scheduler>> spinlock_t *lock = vcpu_schedule_lock_irqsave(v, &flags);
+     */
     spinlock_t *lock = vcpu_schedule_lock_irqsave(v, &flags);
 
+    /*
+     * 设置了_VPF_migrating的话v->pause_flags就不是0, vcpu_runnable()就会返回false
+     */
     if ( likely(!vcpu_runnable(v)) )
     {
         if ( v->runstate.state == RUNSTATE_runnable )
             vcpu_runstate_change(v, RUNSTATE_offline, NOW());
 
+        /*
+	 * csched_vcpu_sleep()
+	 */
         SCHED_OP(VCPU2OP(v), sleep, v);
     }
 
@@ -364,6 +410,12 @@ void vcpu_sleep_nosync(struct vcpu *v)
     TRACE_2D(TRC_SCHED_SLEEP, v->domain->domain_id, v->vcpu_id);
 }
 
+/*
+ * called by:
+ *   - common/domain.c|934| <<vcpu_pause>> vcpu_sleep_sync(v);
+ *   - common/domain.c|958| <<domain_pause>> vcpu_sleep_sync(v);
+ *   - common/domain.c|1448| <<continue_hypercall_tasklet_handler>> vcpu_sleep_sync(v);
+ */
 void vcpu_sleep_sync(struct vcpu *v)
 {
     vcpu_sleep_nosync(v);
@@ -374,6 +426,18 @@ void vcpu_sleep_sync(struct vcpu *v)
     sync_vcpu_execstate(v);
 }
 
+/*
+ * called by:
+ *   - arch/arm/vpsci.c|73| <<do_psci_cpu_on>> vcpu_wake(v);
+ *   - arch/x86/hvm/hvm.c|1033| <<hvm_load_cpu_ctxt>> vcpu_wake(v);
+ *   - arch/x86/hvm/vlapic.c|337| <<vlapic_accept_irq>> vcpu_wake(v);
+ *   - common/domain.c|924| <<vcpu_unpause>> vcpu_wake(v);
+ *   - common/domain.c|955| <<domain_unpause>> vcpu_wake(v);
+ *   - common/domain.c|1226| <<do_vcpu_op(VCPUOP_up)>> vcpu_wake(v);
+ *   - common/event_channel.c|69| <<default_xen_notification_fn>> vcpu_wake(v);
+ *   - common/schedule.c|436| <<vcpu_unblock>> vcpu_wake(v);
+ *   - common/schedule.c|561| <<vcpu_migrate>> vcpu_wake(v);
+ */
 void vcpu_wake(struct vcpu *v)
 {
     unsigned long flags;
@@ -383,6 +447,9 @@ void vcpu_wake(struct vcpu *v)
     {
         if ( v->runstate.state >= RUNSTATE_blocked )
             vcpu_runstate_change(v, RUNSTATE_runnable, NOW());
+        /*
+	 * csched_vcpu_wake()
+	 */
         SCHED_OP(VCPU2OP(v), wake, v);
     }
     else if ( !test_bit(_VPF_blocked, &v->pause_flags) )
@@ -417,6 +484,14 @@ void vcpu_unblock(struct vcpu *v)
     vcpu_wake(v);
 }
 
+/*
+ * called by:
+ *   - common/schedule.c|554| <<vcpu_force_reschedule>> vcpu_migrate(v);
+ *   - common/schedule.c|578| <<restore_vcpu_affinity>> vcpu_migrate(v);
+ *   - common/schedule.c|634| <<cpu_disable_scheduler>> vcpu_migrate(v);
+ *   - common/schedule.c|682| <<vcpu_set_affinity>> vcpu_migrate(v);
+ *   - common/schedule.c|1287| <<context_saved>> vcpu_migrate(prev);
+ */
 static void vcpu_migrate(struct vcpu *v)
 {
     unsigned long flags;
@@ -468,6 +543,9 @@ static void vcpu_migrate(struct vcpu *v)
                 break;
 
             /* Select a new CPU. */
+            /*
+	     * csched_cpu_pick()
+	     */
             new_cpu = SCHED_OP(VCPU2OP(v), pick_cpu, v);
             if ( (new_lock == per_cpu(schedule_data, new_cpu).schedule_lock) &&
                  cpumask_test_cpu(new_cpu, v->domain->cpupool->cpu_valid) )
@@ -493,6 +571,23 @@ static void vcpu_migrate(struct vcpu *v)
      * because they both happen in (different) spinlock regions, and those
      * regions are strictly serialised.
      */
+    /*
+     * 在以下使用_VPF_migrating:
+     *   - common/sched_credit.c|658| <<__runq_tickle>> set_bit(_VPF_migrating, &cur->vcpu->pause_flags);
+     *   - common/sched_credit.c|1156| <<csched_vcpu_acct>> set_bit(_VPF_migrating, &current->pause_flags);
+     *   - common/sched_credit2.c|1210| <<migrate>> set_bit(_VPF_migrating, &svc->vcpu->pause_flags);
+     *   - common/schedule.c|536| <<vcpu_migrate>> !test_and_clear_bit(_VPF_migrating, &v->pause_flags) )
+     *   - common/schedule.c|587| <<vcpu_force_reschedule>> set_bit(_VPF_migrating, &v->pause_flags);
+     *   - common/schedule.c|590| <<vcpu_force_reschedule>> if ( test_bit(_VPF_migrating, &v->pause_flags) )
+     *   - common/schedule.c|614| <<restore_vcpu_affinity>> set_bit(_VPF_migrating, &v->pause_flags);
+     *   - common/schedule.c|670| <<cpu_disable_scheduler>> set_bit(_VPF_migrating, &v->pause_flags);
+     *   - common/schedule.c|712| <<vcpu_set_affinity>> set_bit(_VPF_migrating, &v->pause_flags);
+     *   - common/schedule.c|718| <<vcpu_set_affinity>> if ( test_bit(_VPF_migrating, &v->pause_flags) )
+     *   - common/schedule.c|1358| <<context_saved>> if ( unlikely(test_bit(_VPF_migrating, &prev->pause_flags)) )
+     *   - include/xen/sched.h|783| <<VPF_migrating>> #define VPF_migrating (1UL<<_VPF_migrating)
+     *
+     * test_and_clear_bit - Clear a bit and return its old value
+     */
     if ( v->is_running ||
          !test_and_clear_bit(_VPF_migrating, &v->pause_flags) )
     {
@@ -517,6 +612,9 @@ static void vcpu_migrate(struct vcpu *v)
      * Switch to new CPU, then unlock new and old CPU.  This is safe because
      * the lock pointer cant' change while the current lock is held.
      */
+    /*
+     * credit1 不支持 migrate
+     */
     if ( VCPU2OP(v)->migrate )
         SCHED_OP(VCPU2OP(v), migrate, v, new_cpu);
     else
@@ -540,6 +638,11 @@ static void vcpu_migrate(struct vcpu *v)
  * most periodic-timer state need only be touched from within the scheduler
  * which can thus be done without need for synchronisation.
  */
+/*
+ * called by:
+ *   - common/domain.c|1284| <<do_vcpu_op(VCPUOP_set_periodic_timer)>> vcpu_force_reschedule(v);
+ *   - common/domain.c|1291| <<do_vcpu_op(VCPUOP_stop_periodic_timer)>> vcpu_force_reschedule(v);
+ */
 void vcpu_force_reschedule(struct vcpu *v)
 {
     spinlock_t *lock = vcpu_schedule_lock_irq(v);
@@ -555,6 +658,10 @@ void vcpu_force_reschedule(struct vcpu *v)
     }
 }
 
+/*
+ * called by:
+ *   - arch/x86/acpi/power.c|101| <<thaw_domains>> restore_vcpu_affinity(d);
+ */
 void restore_vcpu_affinity(struct domain *d)
 {
     struct vcpu *v;
@@ -651,6 +758,34 @@ int cpu_disable_scheduler(unsigned int cpu)
     return ret;
 }
 
+/*
+ * 在以下使用vcpu_schedule_lock_irq():
+ *   - common/sched_credit2.c|891| <<csched_vcpu_insert>> lock = vcpu_schedule_lock_irq(vc);
+ *   - common/sched_credit2.c|927| <<csched_vcpu_remove>> lock = vcpu_schedule_lock_irq(vc);
+ *   - common/sched_credit2.c|1018| <<csched_context_saved>> spinlock_t *lock = vcpu_schedule_lock_irq(vc);
+ *   - common/schedule.c|172| <<vcpu_runstate_get>> spinlock_t *lock = likely(v == current) ? NULL : vcpu_schedule_lock_irq(v);
+ *   - common/schedule.c|312| <<sched_move_domain>> lock = vcpu_schedule_lock_irq(v);
+ *   - common/schedule.c|620| <<vcpu_force_reschedule>> spinlock_t *lock = vcpu_schedule_lock_irq(v);
+ *   - common/schedule.c|639| <<restore_vcpu_affinity>> spinlock_t *lock = vcpu_schedule_lock_irq(v);
+ *   - common/schedule.c|751| <<vcpu_set_affinity>> lock = vcpu_schedule_lock_irq(v);
+ *   - common/schedule.c|878| <<do_yield>> spinlock_t *lock = vcpu_schedule_lock_irq(v);
+ *
+ * 在以下使用vcpu_schedule_lock_irqsave():
+ *   - common/sched_credit.c|1199| <<csched_vcpu_insert>> lock = vcpu_schedule_lock_irqsave(vc, &flags);
+ *   - common/schedule.c|392| <<vcpu_sleep_nosync>> spinlock_t *lock = vcpu_schedule_lock_irqsave(v, &flags);
+ *   - common/schedule.c|441| <<vcpu_wake>> spinlock_t *lock = vcpu_schedule_lock_irqsave(v, &flags);
+ *   - common/schedule.c|685| <<cpu_disable_scheduler>> spinlock_t *lock = vcpu_schedule_lock_irqsave(v, &flags);
+ */
+
+/*
+ * called by:
+ *   - arch/x86/traps.c|3232| <<nmi_mce_softirq>> vcpu_set_affinity(st->vcpu, cpumask_of(st->processor));
+ *   - arch/x86/traps.c|3261| <<async_exception_cleanup>> vcpu_set_affinity(curr, curr->cpu_hard_affinity_tmp);
+ *   - common/domctl.c|787| <<do_domctl(XEN_DOMCTL_setvcpuaffinity|XEN_DOMCTL_getvcpuaffinity)>> ret = vcpu_set_affinity(v, new_affinity);
+ *   - common/wait.c|137| <<__prepare_to_wait>> if ( vcpu_set_affinity(curr, cpumask_of(wqv->wakeup_cpu)) )
+ *   - common/wait.c|180| <<__finish_wait>> (void )vcpu_set_affinity(current, &wqv->saved_affinity);
+ *   - common/wait.c|198| <<check_wakeup_from_wait>> if ( vcpu_set_affinity(curr, cpumask_of(wqv->wakeup_cpu)) )
+ */
 int vcpu_set_affinity(struct vcpu *v, const cpumask_t *affinity)
 {
     cpumask_t online_affinity;
@@ -664,6 +799,9 @@ int vcpu_set_affinity(struct vcpu *v, const cpumask_t *affinity)
     if ( cpumask_empty(&online_affinity) )
         return -EINVAL;
 
+    /*
+     * 相当于lock per_cpu(schedule_data, v->processor).schedule_lock
+     */
     lock = vcpu_schedule_lock_irq(v);
 
     cpumask_copy(v->cpu_hard_affinity, affinity);
@@ -783,11 +921,19 @@ static long do_poll(struct sched_poll *sched_poll)
 }
 
 /* Voluntarily yield the processor for this allocation. */
+/*
+ * called by:
+ *   - common/schedule.c|895| <<do_sched_op_compat>> ret = do_yield();
+ *   - common/schedule.c|932| <<do_sched_op(SCHEDOP_yield)>> ret = do_yield();
+ */
 static long do_yield(void)
 {
     struct vcpu * v=current;
     spinlock_t *lock = vcpu_schedule_lock_irq(v);
 
+    /*
+     * csched_vcpu_yield()
+     */
     SCHED_OP(VCPU2OP(v), yield, v);
     vcpu_schedule_unlock_irq(lock, v);
 
@@ -1142,6 +1288,11 @@ static void vcpu_periodic_timer_work(struct vcpu *v)
  * - deschedule the current domain (scheduler independent).
  * - pick a new domain (scheduler dependent).
  */
+/*
+ * 在以下使用schedule():
+ *   - common/schedule.c|1411| <<scheduler_init>> open_softirq(SCHEDULE_SOFTIRQ, schedule);
+ *   - common/schedule.c|1584| <<wait>> schedule();
+ */
 static void schedule(void)
 {
     struct vcpu          *prev = current, *next = NULL;
@@ -1151,6 +1302,15 @@ static void schedule(void)
     bool_t                tasklet_work_scheduled = 0;
     struct schedule_data *sd;
     spinlock_t           *lock;
+    /*
+     * 更像是linux kernel的sched entity
+     *
+     * struct task_slice {
+     *     struct vcpu *task;
+     *     s_time_t     time;
+     *     bool_t       migrated;
+     * };
+     */
     struct task_slice     next_slice;
     int cpu = smp_processor_id();
 
@@ -1158,6 +1318,16 @@ static void schedule(void)
 
     SCHED_STAT_CRANK(sched_run);
 
+    /*
+     * struct schedule_data {
+     *     spinlock_t         *schedule_lock,
+     *                        _lock;
+     *     struct vcpu        *curr;           // current task
+     *     void               *sched_priv;
+     *     struct timer        s_timer;        // scheduling timer
+     *     atomic_t            urgent_count;   // how many urgent vcpus
+     * };
+     */
     sd = &this_cpu(schedule_data);
 
     /* Update tasklet scheduling status. */
@@ -1179,16 +1349,36 @@ static void schedule(void)
 
     lock = pcpu_schedule_lock_irq(cpu);
 
+    /*
+     * 在以下使用schedule_data->s_timer:
+     *   - common/schedule.c|1227| <<schedule>> stop_timer(&sd->s_timer);
+     *   - common/schedule.c|1244| <<schedule>> set_timer(&sd->s_timer, now + next_slice.time);
+     *   - common/schedule.c|1354| <<cpu_schedule_up>> init_timer(&sd->s_timer, s_timer_fn, NULL, cpu);
+     *   - common/schedule.c|1381| <<cpu_schedule_down>> kill_timer(&sd->s_timer);
+     */
     stop_timer(&sd->s_timer);
     
     /* get policy-specific decision on scheduling... */
+    /*
+     * struct scheduler     *sched;
+     */
     sched = this_cpu(scheduler);
+    /*
+     * struct scheduler sched_credit_def.do_schedule = csched_schedule()
+     */
     next_slice = sched->do_schedule(sched, now, tasklet_work_scheduled);
 
     next = next_slice.task;
 
     sd->curr = next;
 
+    /*
+     * 在以下使用schedule_data->s_timer:
+     *   - common/schedule.c|1227| <<schedule>> stop_timer(&sd->s_timer);
+     *   - common/schedule.c|1244| <<schedule>> set_timer(&sd->s_timer, now + next_slice.time);
+     *   - common/schedule.c|1354| <<cpu_schedule_up>> init_timer(&sd->s_timer, s_timer_fn, NULL, cpu);
+     *   - common/schedule.c|1381| <<cpu_schedule_down>> kill_timer(&sd->s_timer);
+     */
     if ( next_slice.time >= 0 ) /* -ve means no limit */
         set_timer(&sd->s_timer, now + next_slice.time);
 
@@ -1230,6 +1420,12 @@ static void schedule(void)
      */
 
     ASSERT(!next->is_running);
+    /*
+     * 在以下设置vcpu->is_running:
+     *   - common/schedule.c|219| <<sched_init_vcpu>> v->is_running = 1;
+     *   - common/schedule.c|1284| <<schedule>> next->is_running = 1;
+     *   - common/schedule.c|1305| <<context_saved>> prev->is_running = 0;
+     */
     next->is_running = 1;
 
     pcpu_schedule_unlock_irq(lock, cpu);
@@ -1246,6 +1442,11 @@ static void schedule(void)
     context_switch(prev, next);
 }
 
+/*
+ * called by:
+ *   - arch/arm/domain.c|234| <<schedule_tail>> context_saved(prev);
+ *   - arch/x86/domain.c|1735| <<context_switch>> context_saved(prev);
+ */
 void context_saved(struct vcpu *prev)
 {
     /* Clear running flag /after/ writing context to memory. */
@@ -1298,6 +1499,11 @@ static int cpu_schedule_up(unsigned int cpu)
 
     per_cpu(scheduler, cpu) = &ops;
     spin_lock_init(&sd->_lock);
+    /*
+     * struct schedule_data *sd = &per_cpu(schedule_data, cpu):
+     * -> spinlock_t         *schedule_lock,
+     * -> spinlock_t         _lock;
+     */
     sd->schedule_lock = &sd->_lock;
     sd->curr = idle_vcpu[cpu];
     init_timer(&sd->s_timer, s_timer_fn, NULL, cpu);
@@ -1307,6 +1513,11 @@ static int cpu_schedule_up(unsigned int cpu)
     if ( cpu == 0 )
         return 0;
 
+    /*
+     * vcpu是指针数组
+     * idle_domain->vcpu = idle_vcpu;
+     * 所以修改idle_domain->vcpu[]就是修改idle_vcpu
+     */
     if ( idle_vcpu[cpu] == NULL )
         alloc_vcpu(idle_vcpu[0]->domain, cpu, cpu);
     if ( idle_vcpu[cpu] == NULL )
@@ -1357,6 +1568,11 @@ static struct notifier_block cpu_schedule_nfb = {
 };
 
 /* Initialise the data structures. */
+/*
+ * called by:
+ *   - arch/arm/setup.c|58| <<init_idle_domain>> scheduler_init();
+ *   - arch/x86/setup.c|257| <<init_idle_domain>> scheduler_init();
+ */
 void __init scheduler_init(void)
 {
     struct domain *idle_domain;
@@ -1408,6 +1624,10 @@ void __init scheduler_init(void)
     idle_domain = domain_create(DOMID_IDLE, 0, 0);
     BUG_ON(IS_ERR(idle_domain));
     BUG_ON(nr_cpu_ids > ARRAY_SIZE(idle_vcpu));
+    /*
+     * struct domain *idle_domain:
+     * -> struct vcpu    **vcpu;
+     */
     idle_domain->vcpu = idle_vcpu;
     idle_domain->max_vcpus = nr_cpu_ids;
     if ( alloc_vcpu(idle_domain, 0, 0) == NULL )
@@ -1417,6 +1637,11 @@ void __init scheduler_init(void)
         BUG();
 }
 
+/*
+ * called by:
+ *   - common/cpupool.c|276| <<cpupool_assign_cpu_locked>> ret = schedule_cpu_switch(cpu, c);
+ *   - common/cpupool.c|315| <<cpupool_unassign_cpu_helper>> ret = schedule_cpu_switch(cpu, NULL);
+ */
 int schedule_cpu_switch(unsigned int cpu, struct cpupool *c)
 {
     struct vcpu *idle;
diff --git a/xen/include/xen/sched-if.h b/xen/include/xen/sched-if.h
index 4164dffb3e..6f97345ad6 100644
--- a/xen/include/xen/sched-if.h
+++ b/xen/include/xen/sched-if.h
@@ -37,6 +37,13 @@ struct schedule_data {
                        _lock;
     struct vcpu        *curr;           /* current task                    */
     void               *sched_priv;
+    /*
+     * 在以下使用schedule_data->s_timer:
+     *   - common/schedule.c|1227| <<schedule>> stop_timer(&sd->s_timer);
+     *   - common/schedule.c|1244| <<schedule>> set_timer(&sd->s_timer, now + next_slice.time);
+     *   - common/schedule.c|1354| <<cpu_schedule_up>> init_timer(&sd->s_timer, s_timer_fn, NULL, cpu);
+     *   - common/schedule.c|1381| <<cpu_schedule_down>> kill_timer(&sd->s_timer);
+     */
     struct timer        s_timer;        /* scheduling timer                */
     atomic_t            urgent_count;   /* how many urgent vcpus           */
 };
diff --git a/xen/include/xen/sched.h b/xen/include/xen/sched.h
index e8245a545b..58843604e0 100644
--- a/xen/include/xen/sched.h
+++ b/xen/include/xen/sched.h
@@ -117,6 +117,13 @@ struct vcpu
 {
     int              vcpu_id;
 
+    /*
+     * 在以下修改vcpu->processor(credit scheduler不支持migrate):
+     *   - common/sched_credit.c|1509| <<csched_runq_steal>> vc->processor = cpu;
+     *   - common/schedule.c|195| <<sched_init_vcpu>> v->processor = processor;
+     *   - common/schedule.c|294| <<sched_move_domain>> v->processor = new_p;
+     *   - common/schedule.c|531| <<vcpu_migrate>> v->processor = new_cpu;
+     */
     int              processor;
 
     vcpu_info_t     *vcpu_info;
@@ -156,6 +163,12 @@ struct vcpu
     /* Initialization completed for this VCPU? */
     bool_t           is_initialised;
     /* Currently running on a CPU? */
+    /*
+     * 在以下设置vcpu->is_running:
+     *   - common/schedule.c|219| <<sched_init_vcpu>> v->is_running = 1;
+     *   - common/schedule.c|1284| <<schedule>> next->is_running = 1;
+     *   - common/schedule.c|1305| <<context_saved>> prev->is_running = 0;
+     */
     bool_t           is_running;
     /* VCPU should wake fast (do not deep sleep the CPU). */
     bool_t           is_urgent;
@@ -211,6 +224,19 @@ struct vcpu
     cpumask_var_t    cpu_hard_affinity_saved;
 
     /* Bitmask of CPUs on which this VCPU prefers to run. */
+    /*
+     * 在以下使用domain->cpu_soft_affinity:
+     *   - common/domain.c|225| <<alloc_vcpu>> !zalloc_cpumask_var(&v->cpu_soft_affinity) ||
+     *   - common/domain.c|254| <<alloc_vcpu>> free_cpumask_var(v->cpu_soft_affinity);
+     *   - common/domain.c|508| <<domain_update_node_affinity>> v->cpu_soft_affinity);
+     *   - common/domain.c|876| <<complete_domain_destroy>> free_cpumask_var(v->cpu_soft_affinity);
+     *   - common/keyhandler.c|307| <<dump_domains>> cpuset_print(tmpstr, sizeof(tmpstr), v->cpu_soft_affinity);
+     *   - common/sched_credit.c|498| <<__vcpu_has_soft_affinity>> if ( cpumask_full(vc->cpu_soft_affinity)
+     *   - common/sched_credit.c|499| <<__vcpu_has_soft_affinity>> || !cpumask_intersects(vc->cpu_soft_affinity, mask) )
+     *   - common/sched_credit.c|523| <<csched_balance_cpumask>> cpumask_and(mask, vc->cpu_soft_affinity, vc->cpu_hard_affinity);
+     *   - common/schedule.c|213| <<sched_init_vcpu>> cpumask_setall(v->cpu_soft_affinity);
+     *   - common/schedule.c|310| <<sched_move_domain>> cpumask_setall(v->cpu_soft_affinity);
+     */
     cpumask_var_t    cpu_soft_affinity;
 
     /* Bitmask of CPUs which are holding onto this VCPU's state. */
@@ -353,6 +379,15 @@ struct domain
     s8               need_iommu;
 #endif
     /* is node-affinity automatically computed? */
+    /*
+     * 在以下使用domain->auto_node_affinity:
+     *   - common/domain.c|322| <<domain_create>> d->auto_node_affinity = 1;
+     *   - common/domain.c|483| <<domain_update_node_affinity>> if ( d->auto_node_affinity )
+     *   - common/domain.c|537| <<domain_set_node_affinity>> d->auto_node_affinity = 1;
+     *   - common/domain.c|541| <<domain_set_node_affinity>> d->auto_node_affinity = 0;
+     *
+     * 测试的例子是1 (dom0, domU, idle)
+     */
     bool_t           auto_node_affinity;
     /* Is this guest fully privileged (aka dom0)? */
     bool_t           is_privileged;
@@ -447,6 +482,16 @@ struct domain
      * Can be specified by the user. If that is not the case, it is
      * computed from the union of all the vcpu cpu-affinity masks.
      */
+    /*
+     * 在以下使用domain->node_affinity:
+     *   - common/domain.c|321| <<domain_create>> d->node_affinity = NODE_MASK_ALL;
+     *   - common/domain.c|511| <<domain_update_node_affinity>> nodes_clear(d->node_affinity);
+     *   - common/domain.c|513| <<domain_update_node_affinity>> node_set(cpu_to_node(cpu), d->node_affinity);
+     *   - common/domain.c|542| <<domain_set_node_affinity>> d->node_affinity = *affinity;
+     *   - common/domctl.c|762| <<do_domctl(XEN_DOMCTL_getnodeaffinity)>> &d->node_affinity);
+     *   - common/keyhandler.c|287| <<dump_domains>> nodeset_print(tmpstr, sizeof(tmpstr), &d->node_affinity);
+     *   - common/page_alloc.c|614| <<alloc_heap_pages>> nodemask_t nodemask = (d != NULL ) ? d->node_affinity : node_online_map;
+     */
     nodemask_t node_affinity;
     unsigned int last_alloc_node;
     spinlock_t node_affinity_lock;
@@ -766,6 +811,21 @@ static inline struct domain *next_domain_in_cpupool(
 #define _VPF_blocked_in_xen  2
 #define VPF_blocked_in_xen   (1UL<<_VPF_blocked_in_xen)
  /* VCPU affinity has changed: migrating to a new CPU. */
+/*
+ * 在以下使用_VPF_migrating:
+ *   - common/sched_credit.c|658| <<__runq_tickle>> set_bit(_VPF_migrating, &cur->vcpu->pause_flags);
+ *   - common/sched_credit.c|1156| <<csched_vcpu_acct>> set_bit(_VPF_migrating, &current->pause_flags);
+ *   - common/sched_credit2.c|1210| <<migrate>> set_bit(_VPF_migrating, &svc->vcpu->pause_flags);
+ *   - common/schedule.c|536| <<vcpu_migrate>> !test_and_clear_bit(_VPF_migrating, &v->pause_flags) )
+ *   - common/schedule.c|587| <<vcpu_force_reschedule>> set_bit(_VPF_migrating, &v->pause_flags);
+ *   - common/schedule.c|590| <<vcpu_force_reschedule>> if ( test_bit(_VPF_migrating, &v->pause_flags) )
+ *   - common/schedule.c|614| <<restore_vcpu_affinity>> set_bit(_VPF_migrating, &v->pause_flags);
+ *   - common/schedule.c|670| <<cpu_disable_scheduler>> set_bit(_VPF_migrating, &v->pause_flags);
+ *   - common/schedule.c|712| <<vcpu_set_affinity>> set_bit(_VPF_migrating, &v->pause_flags);
+ *   - common/schedule.c|718| <<vcpu_set_affinity>> if ( test_bit(_VPF_migrating, &v->pause_flags) )
+ *   - common/schedule.c|1358| <<context_saved>> if ( unlikely(test_bit(_VPF_migrating, &prev->pause_flags)) )
+ *   - include/xen/sched.h|783| <<VPF_migrating>> #define VPF_migrating (1UL<<_VPF_migrating)
+ */
 #define _VPF_migrating       3
 #define VPF_migrating        (1UL<<_VPF_migrating)
  /* VCPU is blocked due to missing mem_paging ring. */
@@ -783,6 +843,9 @@ static inline struct domain *next_domain_in_cpupool(
 
 static inline int vcpu_runnable(struct vcpu *v)
 {
+    /*
+     * 设置了_VPF_migrating的话v->pause_flags就不是0, vcpu_runnable()就会返回false
+     */
     return !(v->pause_flags |
              atomic_read(&v->pause_count) |
              atomic_read(&v->domain->pause_count));
-- 
2.17.1

