From 1d0253638644743508f450d5401c8a1d110052e7 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 25 May 2025 20:14:01 -0700
Subject: [PATCH 1/1] qemu for v10.0.0

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 accel/kvm/kvm-accel-ops.c       |  29 ++
 accel/kvm/kvm-all.c             | 100 ++++++
 backends/cryptodev-vhost.c      |  27 ++
 backends/vhost-user.c           |  27 ++
 block/accounting.c              | 103 ++++++
 block/aio_task.c                |   5 +
 block/block-backend.c           | 168 ++++++++++
 chardev/char.c                  |  53 +++
 cpu-common.c                    |  43 +++
 hw/acpi/utils.c                 |  51 +++
 hw/arm/virt-acpi-build.c        |  54 +++
 hw/block/vhost-user-blk.c       |  34 ++
 hw/block/virtio-blk-common.c    |   7 +
 hw/block/virtio-blk.c           | 466 +++++++++++++++++++++++++
 hw/char/serial.c                | 151 +++++++++
 hw/i386/acpi-build.c            |  65 ++++
 hw/i386/acpi-microvm.c          |  18 +
 hw/ide/core.c                   |  14 +
 hw/net/vhost_net.c              |  32 ++
 hw/nvme/ctrl.c                  |  14 +
 hw/nvram/fw_cfg.c               |  92 +++++
 hw/pci/pci.c                    |  38 +++
 hw/riscv/virt-acpi-build.c      |  18 +
 hw/scsi/scsi-disk.c             |  56 ++++
 hw/scsi/vhost-scsi.c            | 298 ++++++++++++++++
 hw/scsi/vhost-user-scsi.c       |  27 ++
 hw/scsi/virtio-scsi-dataplane.c |  51 +++
 hw/scsi/virtio-scsi.c           |  11 +
 hw/virtio/iothread-vq-mapping.c |  10 +
 hw/virtio/vdpa-dev.c            |  27 ++
 hw/virtio/vhost-user-base.c     |  27 ++
 hw/virtio/vhost-user-fs.c       |  27 ++
 hw/virtio/vhost-user-scmi.c     |  27 ++
 hw/virtio/vhost-user-vsock.c    |  27 ++
 hw/virtio/vhost-vsock.c         |  27 ++
 hw/virtio/vhost.c               |  27 ++
 hw/virtio/virtio-bus.c          |  19 ++
 hw/virtio/virtio.c              |  81 +++++
 include/exec/ram_addr.h         |   4 +
 include/exec/ramblock.h         |  33 ++
 include/exec/ramlist.h          |  59 ++++
 include/hw/acpi/aml-build.h     |  18 +
 include/hw/char/serial.h        |  34 ++
 include/hw/virtio/virtio-blk.h  |  54 +++
 include/hw/virtio/virtio-scsi.h |  10 +
 include/hw/virtio/virtio.h      |   7 +
 include/qemu/timer.h            |  13 +
 linux-headers/linux/kvm.h       |   8 +
 migration/cpr-transfer.c        |   4 +
 migration/cpr.c                 |   4 +
 migration/dirtyrate.c           |  80 +++++
 migration/migration.c           |  14 +
 migration/ram.c                 | 117 +++++++
 migration/ram.h                 |  40 +++
 net/slirp.c                     |   8 +
 system/cpus.c                   |  47 +++
 system/dirtylimit.c             |   7 +
 system/main.c                   |  30 ++
 system/memory.c                 | 162 +++++++++
 system/physmem.c                | 149 ++++++++
 system/runstate.c               |  15 +
 system/vl.c                     |   5 +
 util/async.c                    |  32 ++
 util/coroutine-ucontext.c       |   5 +
 util/main-loop.c                | 165 +++++++++
 util/osdep.c                    |   7 +
 util/qemu-coroutine.c           | 578 ++++++++++++++++++++++++++++++++
 util/qemu-timer.c               |  24 ++
 util/rcu.c                      | 166 +++++++++
 69 files changed, 4250 insertions(+)

diff --git a/accel/kvm/kvm-accel-ops.c b/accel/kvm/kvm-accel-ops.c
index 54ea60909..abe9cac4b 100644
--- a/accel/kvm/kvm-accel-ops.c
+++ b/accel/kvm/kvm-accel-ops.c
@@ -27,6 +27,11 @@
 #include <linux/kvm.h>
 #include "kvm-cpus.h"
 
+/*
+ * 在以下使用kvm_vcpu_thread_fn():
+ *   - accel/kvm/kvm-accel-ops.c|72| <<kvm_start_vcpu_thread>> qemu_thread_create(cpu->thread, thread_name,
+ *            kvm_vcpu_thread_fn, cpu, QEMU_THREAD_JOINABLE);
+ */
 static void *kvm_vcpu_thread_fn(void *arg)
 {
     CPUState *cpu = arg;
@@ -39,6 +44,9 @@ static void *kvm_vcpu_thread_fn(void *arg)
     cpu->thread_id = qemu_get_thread_id();
     current_cpu = cpu;
 
+    /*
+     * 只在此处调用kvm_init_vcpu()
+     */
     r = kvm_init_vcpu(cpu, &error_fatal);
     kvm_init_cpu_signals(cpu);
 
@@ -47,12 +55,33 @@ static void *kvm_vcpu_thread_fn(void *arg)
     qemu_guest_random_seed_thread_part2(cpu->random_seed);
 
     do {
+        /*
+	 * can run主要看能不能stop
+	 */
         if (cpu_can_run(cpu)) {
+            /*
+	     * 返回的ret
+	     * #define EXCP_INTERRUPT  0x10000 // async interruption
+	     * #define EXCP_HLT        0x10001 // hlt instruction reached
+	     * #define EXCP_DEBUG      0x10002 // cpu stopped after a breakpoint or singlestep
+	     * #define EXCP_HALTED     0x10003 // cpu is halted (waiting for external event)
+	     * #define EXCP_YIELD      0x10004 // cpu wants to yield timeslice to another
+	     * #define EXCP_ATOMIC     0x10005 // stop-the-world and emulate atomic
+	     *
+	     * 只在以下调用kvm_cpu_exec()
+	     */
             r = kvm_cpu_exec(cpu);
             if (r == EXCP_DEBUG) {
                 cpu_handle_guest_debug(cpu);
             }
         }
+        /*
+	 * 在以下使用qemu_wait_io_event():
+	 *   - accel/dummy-cpus.c|59| <<dummy_cpu_thread_fn>> qemu_wait_io_event(cpu);
+	 *   - accel/hvf/hvf-accel-ops.c|453| <<hvf_cpu_thread_fn>> qemu_wait_io_event(cpu);
+	 *   - accel/kvm/kvm-accel-ops.c|56| <<kvm_vcpu_thread_fn>> qemu_wait_io_event(cpu);
+	 *   - accel/tcg/tcg-accel-ops-mttcg.c|117| <<mttcg_cpu_thread_fn>> qemu_wait_io_event(cpu);
+	 */
         qemu_wait_io_event(cpu);
     } while (!cpu->unplug || cpu_can_run(cpu));
 
diff --git a/accel/kvm/kvm-all.c b/accel/kvm/kvm-all.c
index f89568bfa..8a1e80a0a 100644
--- a/accel/kvm/kvm-all.c
+++ b/accel/kvm/kvm-all.c
@@ -532,6 +532,10 @@ void kvm_destroy_vcpu(CPUState *cpu)
     }
 }
 
+/*
+ * 在以下使用kvm_init_vcpu():
+ *   - accel/kvm/kvm-accel-ops.c|42| <<kvm_vcpu_thread_fn>> r = kvm_init_vcpu(cpu, &error_fatal);
+ */
 int kvm_init_vcpu(CPUState *cpu, Error **errp)
 {
     KVMState *s = kvm_state;
@@ -599,12 +603,25 @@ err:
  * dirty pages logging control
  */
 
+/*
+ * 在以下使用kvm_mem_flags():
+ *   - accel/kvm/kvm-all.c|624| <<kvm_slot_update_flags>> mem->flags = kvm_mem_flags(mr);
+ *   - accel/kvm/kvm-all.c|1543| <<kvm_set_phys_mem>> mem->flags = kvm_mem_flags(mr);
+ */
 static int kvm_mem_flags(MemoryRegion *mr)
 {
     bool readonly = mr->readonly || memory_region_is_romd(mr);
     int flags = 0;
 
     if (memory_region_get_dirty_log_mask(mr) != 0) {
+        /*
+	 *  在以下使用KVM_MEM_LOG_DIRTY_PAGES:
+	 *   - linux-headers/linux/kvm.h|51| <<global>> #define KVM_MEM_LOG_DIRTY_PAGES (1UL << 0)
+	 *   - accel/kvm/kvm-all.c|608| <<kvm_mem_flags>> flags |= KVM_MEM_LOG_DIRTY_PAGES;
+	 *   - accel/kvm/kvm-all.c|720| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+	 *   - accel/kvm/kvm-all.c|1490| <<kvm_set_phys_mem>> if (mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
+	 *   - accel/kvm/kvm-all.c|1797| <<kvm_log_sync_global>> if (mem->memory_size && mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
+	 */
         flags |= KVM_MEM_LOG_DIRTY_PAGES;
     }
     if (readonly && kvm_readonly_mem_allowed) {
@@ -621,6 +638,11 @@ static int kvm_mem_flags(MemoryRegion *mr)
 static int kvm_slot_update_flags(KVMMemoryListener *kml, KVMSlot *mem,
                                  MemoryRegion *mr)
 {
+    /*
+     * 在以下使用kvm_mem_flags():
+     *   - accel/kvm/kvm-all.c|624| <<kvm_slot_update_flags>> mem->flags = kvm_mem_flags(mr);
+     *   - accel/kvm/kvm-all.c|1543| <<kvm_set_phys_mem>> mem->flags = kvm_mem_flags(mr);
+     */
     mem->flags = kvm_mem_flags(mr);
 
     /* If nothing changed effectively, no need to issue ioctl */
@@ -664,6 +686,24 @@ out:
     return ret;
 }
 
+/*
+ * (gdb) bt
+ * #0  kvm_log_start (listener=0x5555576ac8b8, section=0x7ffec5bf6360, old=0, new=4) at ../accel/kvm/kvm-all.c:671
+ * #1  0x0000555555ebb2ec in address_space_update_topology_pass (as=0x555557384080 <address_space_memory>,
+ *     old_view=0x7ffed4005a70, new_view=0x7ffeb40506a0, adding=true) at ../system/memory.c:1019
+ * #2  0x0000555555ebb787 in address_space_set_flatview (as=0x555557384080 <address_space_memory>) at ../system/memory.c:1112
+ * #3  0x0000555555ebb964 in memory_region_transaction_commit () at ../system/memory.c:1164
+ * #4  0x0000555555ec0ef4 in memory_global_dirty_log_start (flags=1, errp=0x7ffec5bf6628) at ../system/memory.c:2981
+ * #5  0x0000555555edacd2 in ram_init_bitmaps (rs=0x7ffeb4008290, errp=0x7ffec5bf6628) at ../migration/ram.c:2729
+ * #6  0x0000555555edaddc in ram_init_all (rsp=0x555557384360 <ram_state>, errp=0x7ffec5bf6628) at ../migration/ram.c:2763
+ * #7  0x0000555555edb46a in ram_save_setup (f=0x55555779d910, opaque=0x555557384360 <ram_state>, errp=0x7ffec5bf6628)
+ *     at ../migration/ram.c:2957
+ * #8  0x0000555555c818ec in qemu_savevm_state_setup (f=0x55555779d910, errp=0x7ffec5bf6628) at ../migration/savevm.c:1380
+ * #9  0x0000555555c6933f in migration_thread (opaque=0x55555745c750) at ../migration/migration.c:3690
+ * #10 0x00005555561ad4c6 in qemu_thread_start (args=0x55555824c5b0) at ../util/qemu-thread-posix.c:541
+ * #11 0x00007ffff68a91da in start_thread () from /lib64/libpthread.so.0
+ * #12 0x00007ffff4bda8d3 in clone () from /lib64/libc.so.6
+ */
 static void kvm_log_start(MemoryListener *listener,
                           MemoryRegionSection *section,
                           int old, int new)
@@ -681,6 +721,33 @@ static void kvm_log_start(MemoryListener *listener,
     }
 }
 
+/*
+ * (gdb) bt
+#0  kvm_log_stop (listener=0x5555576ac8b8, section=0x7fffffffd5b0, old=4, new=0) at ../accel/kvm/kvm-all.c:688
+#1  0x0000555555ebb3c0 in address_space_update_topology_pass (as=0x555557384080 <address_space_memory>, old_view=0x7ffe8c050df0, new_view=0x5555578709b0,
+    adding=true) at ../system/memory.c:1024
+#2  0x0000555555ebb787 in address_space_set_flatview (as=0x555557384080 <address_space_memory>) at ../system/memory.c:1112
+#3  0x0000555555ebb964 in memory_region_transaction_commit () at ../system/memory.c:1164
+#4  0x0000555555ec0fb9 in memory_global_dirty_log_do_stop (flags=1) at ../system/memory.c:2997
+#5  0x0000555555ec112c in memory_global_dirty_log_stop (flags=1) at ../system/memory.c:3044
+#6  0x0000555555ed9f8e in ram_save_cleanup (opaque=0x555557384360 <ram_state>) at ../migration/ram.c:2331
+#7  0x0000555555c82737 in qemu_savevm_state_cleanup () at ../migration/savevm.c:1737
+#8  0x0000555555c6462d in migration_cleanup (s=0x55555745c750) at ../migration/migration.c:1476
+#9  0x0000555555c6484b in migration_cleanup_bh (opaque=0x55555745c750) at ../migration/migration.c:1528
+#10 0x0000555555c61d92 in migration_bh_dispatch_bh (opaque=0x7ffe8c0bf4e0) at ../migration/migration.c:340
+#11 0x00005555561c8b2b in aio_bh_call (bh=0x55555777a8d0) at ../util/async.c:172
+#12 0x00005555561c8c79 in aio_bh_poll (ctx=0x55555745e9c0) at ../util/async.c:219
+#13 0x00005555561a773d in aio_dispatch (ctx=0x55555745e9c0) at ../util/aio-posix.c:436
+#14 0x00005555561c9148 in aio_ctx_dispatch (source=0x55555745e9c0, callback=0x0, user_data=0x0) at ../util/async.c:361
+#15 0x00007ffff6fd394b in g_main_dispatch (context=0x55555745edd0) at ../glib/gmain.c:3325
+#16 g_main_context_dispatch (context=0x55555745edd0) at ../glib/gmain.c:4043
+#17 0x00005555561ca939 in glib_pollfds_poll () at ../util/main-loop.c:287
+#18 0x00005555561ca9c7 in os_host_main_loop_wait (timeout=785834740) at ../util/main-loop.c:310
+#19 0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+#20 0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+#21 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+#22 0x00005555560d296f in main (argc=20, argv=0x7fffffffdb78) at ../system/main.c:80
+ */
 static void kvm_log_stop(MemoryListener *listener,
                           MemoryRegionSection *section,
                           int old, int new)
@@ -717,6 +784,14 @@ static void kvm_slot_reset_dirty_pages(KVMSlot *slot)
 /* Allocate the dirty bitmap for a slot  */
 static void kvm_slot_init_dirty_bitmap(KVMSlot *mem)
 {
+    /*
+     * 在以下使用KVM_MEM_LOG_DIRTY_PAGES:
+     *   - linux-headers/linux/kvm.h|51| <<global>> #define KVM_MEM_LOG_DIRTY_PAGES (1UL << 0)
+     *   - accel/kvm/kvm-all.c|608| <<kvm_mem_flags>> flags |= KVM_MEM_LOG_DIRTY_PAGES;
+     *   - accel/kvm/kvm-all.c|720| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+     *   - accel/kvm/kvm-all.c|1490| <<kvm_set_phys_mem>> if (mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
+     *   - accel/kvm/kvm-all.c|1797| <<kvm_log_sync_global>> if (mem->memory_size && mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
+     */
     if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
         return;
     }
@@ -1487,6 +1562,14 @@ static void kvm_set_phys_mem(KVMMemoryListener *kml,
             if (!mem) {
                 return;
             }
+            /*
+	     * 在以下使用KVM_MEM_LOG_DIRTY_PAGES:
+	     *   - linux-headers/linux/kvm.h|51| <<global>> #define KVM_MEM_LOG_DIRTY_PAGES (1UL << 0)
+	     *   - accel/kvm/kvm-all.c|608| <<kvm_mem_flags>> flags |= KVM_MEM_LOG_DIRTY_PAGES;
+	     *   - accel/kvm/kvm-all.c|720| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+	     *   - accel/kvm/kvm-all.c|1490| <<kvm_set_phys_mem>> if (mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
+	     *   - accel/kvm/kvm-all.c|1797| <<kvm_log_sync_global>> if (mem->memory_size && mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
+	     */
             if (mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
                 /*
                  * NOTE: We should be aware of the fact that here we're only
@@ -1540,6 +1623,11 @@ static void kvm_set_phys_mem(KVMMemoryListener *kml,
         mem->start_addr = start_addr;
         mem->ram_start_offset = ram_start_offset;
         mem->ram = ram;
+	/*
+	 * 在以下使用kvm_mem_flags():
+	 *   - accel/kvm/kvm-all.c|624| <<kvm_slot_update_flags>> mem->flags = kvm_mem_flags(mr);
+	 *   - accel/kvm/kvm-all.c|1543| <<kvm_set_phys_mem>> mem->flags = kvm_mem_flags(mr);
+	 */
         mem->flags = kvm_mem_flags(mr);
         mem->guest_memfd = mr->ram_block->guest_memfd;
         mem->guest_memfd_offset = (uint8_t*)ram - mr->ram_block->host;
@@ -1794,6 +1882,14 @@ static void kvm_log_sync_global(MemoryListener *l, bool last_stage)
     kvm_slots_lock();
     for (i = 0; i < kml->nr_slots_allocated; i++) {
         mem = &kml->slots[i];
+        /*
+	 * 在以下使用KVM_MEM_LOG_DIRTY_PAGES:
+	 *   - linux-headers/linux/kvm.h|51| <<global>> #define KVM_MEM_LOG_DIRTY_PAGES (1UL << 0)
+	 *   - accel/kvm/kvm-all.c|608| <<kvm_mem_flags>> flags |= KVM_MEM_LOG_DIRTY_PAGES;
+	 *   - accel/kvm/kvm-all.c|720| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+	 *   - accel/kvm/kvm-all.c|1490| <<kvm_set_phys_mem>> if (mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
+	 *   - accel/kvm/kvm-all.c|1797| <<kvm_log_sync_global>> if (mem->memory_size && mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
+	 */
         if (mem->memory_size && mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
             kvm_slot_sync_dirty_pages(mem);
 
@@ -3091,6 +3187,10 @@ out_unref:
     return ret;
 }
 
+/*
+ * 只在以下调用kvm_cpu_exec():
+ *   - accel/kvm/kvm-accel-ops.c|51| <<kvm_vcpu_thread_fn>> r = kvm_cpu_exec(cpu);
+ */
 int kvm_cpu_exec(CPUState *cpu)
 {
     struct kvm_run *run = cpu->kvm_run;
diff --git a/backends/cryptodev-vhost.c b/backends/cryptodev-vhost.c
index 943680a23..ae7c0e82c 100644
--- a/backends/cryptodev-vhost.c
+++ b/backends/cryptodev-vhost.c
@@ -66,6 +66,33 @@ cryptodev_vhost_init(
     /* vhost-user needs vq_index to initiate a specific queue pair */
     crypto->dev.vq_index = crypto->cc->queue_index * crypto->dev.nvqs;
 
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     r = vhost_dev_init(&crypto->dev, options->opaque, options->backend_type, 0,
                        &local_err);
     if (r < 0) {
diff --git a/backends/vhost-user.c b/backends/vhost-user.c
index d0e4d71a6..662ce7034 100644
--- a/backends/vhost-user.c
+++ b/backends/vhost-user.c
@@ -36,6 +36,33 @@ vhost_user_backend_dev_init(VhostUserBackend *b, VirtIODevice *vdev,
     b->dev.nvqs = nvqs;
     b->dev.vqs = g_new0(struct vhost_virtqueue, nvqs);
 
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     ret = vhost_dev_init(&b->dev, &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0,
                          errp);
     if (ret < 0) {
diff --git a/block/accounting.c b/block/accounting.c
index 3e4615956..cf5b09a41 100644
--- a/block/accounting.c
+++ b/block/accounting.c
@@ -102,6 +102,50 @@ BlockAcctTimedStats *block_acct_interval_next(BlockAcctStats *stats,
     }
 }
 
+/*
+ * called by:
+ *   - hw/block/dataplane/xen-block.c|375| <<xen_block_do_aio>> block_acct_start(blk_get_stats(dataplane->blk), &request->acct,
+ *   - hw/block/dataplane/xen-block.c|388| <<xen_block_do_aio>> block_acct_start(blk_get_stats(dataplane->blk), &request->acct,
+ *   - hw/block/virtio-blk.c|342| <<virtio_blk_handle_flush>> block_acct_start(blk_get_stats(s->blk), &req->acct, 0,
+ *   - hw/block/virtio-blk.c|425| <<virtio_blk_handle_discard_write_zeroes>> block_acct_start(blk_get_stats(s->blk), &req->acct, bytes,
+ *   - hw/block/virtio-blk.c|778| <<virtio_blk_handle_zone_append>> block_acct_start(blk_get_stats(s->blk), &req->acct, len,
+ *   - hw/block/virtio-blk.c|858| <<virtio_blk_handle_request>> block_acct_start(blk_get_stats(s->blk), &req->acct, req->qiov.size,
+ *   - hw/ide/atapi.c|95| <<cd_read_sector_sync>> block_acct_start(blk_get_stats(s->blk), &s->acct,
+ *   - hw/ide/atapi.c|167| <<cd_read_sector>> block_acct_start(blk_get_stats(s->blk), &s->acct,
+ *   - hw/ide/atapi.c|312| <<ide_atapi_cmd_reply>> block_acct_start(blk_get_stats(s->blk), &s->acct, size,
+ *   - hw/ide/atapi.c|434| <<ide_atapi_cmd_read_dma>> block_acct_start(blk_get_stats(s->blk), &s->acct, s->packet_transfer_size,
+ *   - hw/ide/core.c|498| <<ide_issue_trim_cb>> block_acct_start(blk_get_stats(s->blk), &s->acct,
+ *   - hw/ide/core.c|824| <<ide_sector_read>> block_acct_start(blk_get_stats(s->blk), &s->acct,
+ *   - hw/ide/core.c|995| <<ide_sector_start_dma>> block_acct_start(blk_get_stats(s->blk), &s->acct,
+ *   - hw/ide/core.c|999| <<ide_sector_start_dma>> block_acct_start(blk_get_stats(s->blk), &s->acct,
+ *   - hw/ide/core.c|1098| <<ide_sector_write>> block_acct_start(blk_get_stats(s->blk), &s->acct,
+ *   - hw/ide/core.c|1134| <<ide_flush_cache>> block_acct_start(blk_get_stats(s->blk), &s->acct, 0, BLOCK_ACCT_FLUSH);
+ *   - hw/ide/macio.c|221| <<pmac_ide_transfer>> block_acct_start(blk_get_stats(s->blk), &s->acct, io->len,
+ *   - hw/ide/macio.c|230| <<pmac_ide_transfer>> block_acct_start(blk_get_stats(s->blk), &s->acct, io->len,
+ *   - hw/ide/macio.c|234| <<pmac_ide_transfer>> block_acct_start(blk_get_stats(s->blk), &s->acct, io->len,
+ *   - hw/nvme/ctrl.c|2716| <<nvme_verify>> block_acct_start(blk_get_stats(blk), &req->acct, ctx->data.iov.size,
+ *   - hw/nvme/ctrl.c|3070| <<nvme_copy_in_completed_cb>> block_acct_start(blk_get_stats(dns->blkconf.blk), &iocb->acct.write, 0,
+ *   - hw/nvme/ctrl.c|3307| <<nvme_do_copy>> block_acct_start(blk_get_stats(sns->blkconf.blk), &iocb->acct.read, 0,
+ *   - hw/nvme/ctrl.c|3469| <<nvme_compare>> block_acct_start(blk_get_stats(blk), &req->acct, data_len,
+ *   - hw/nvme/ctrl.c|3667| <<nvme_read>> block_acct_start(blk_get_stats(blk), &req->acct, data_size,
+ *   - hw/nvme/ctrl.c|3837| <<nvme_do_write>> block_acct_start(blk_get_stats(blk), &req->acct, data_size,
+ *   - hw/nvme/dif.c|647| <<nvme_dif_rw>> block_acct_start(blk_get_stats(blk), &req->acct, ctx->data.iov.size,
+ *   - hw/nvme/dif.c|693| <<nvme_dif_rw>> block_acct_start(blk_get_stats(blk), &req->acct, ctx->data.iov.size,
+ *   - hw/scsi/scsi-disk.c|400| <<scsi_write_do_fua>> block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct, 0,
+ *   - hw/scsi/scsi-disk.c|509| <<scsi_do_read>> block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct,
+ *   - hw/scsi/scsi-disk.c|568| <<scsi_read_data>> block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct, 0,
+ *   - hw/scsi/scsi-disk.c|667| <<scsi_write_data>> block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct,
+ *   - hw/scsi/scsi-disk.c|1723| <<scsi_disk_emulate_mode_select>> block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct, 0,
+ *   - hw/scsi/scsi-disk.c|1789| <<scsi_unmap_complete_noio>> block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct,
+ *   - hw/scsi/scsi-disk.c|1906| <<scsi_write_same_complete>> block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct,
+ *   - hw/scsi/scsi-disk.c|1955| <<scsi_disk_emulate_write_same>> block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct,
+ *   - hw/scsi/scsi-disk.c|1981| <<scsi_disk_emulate_write_same>> block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct,
+ *   - hw/scsi/scsi-disk.c|2250| <<scsi_disk_emulate_command>> block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct, 0,
+ *   - qemu-io-cmds.c|1541| <<aio_read_f>> block_acct_start(blk_get_stats(blk), &ctx->acct, ctx->qiov.size,
+ *   - qemu-io-cmds.c|1696| <<aio_write_f>> block_acct_start(blk_get_stats(blk), &ctx->acct, ctx->qiov.size,
+ *   - qemu-io-cmds.c|1709| <<aio_flush_f>> block_acct_start(blk_get_stats(blk), &cookie, 0, BLOCK_ACCT_FLUSH);
+ *   - system/dma-helpers.c|320| <<dma_acct_start>> block_acct_start(blk_get_stats(blk), cookie, sg->size, type);
+ */
 void block_acct_start(BlockAcctStats *stats, BlockAcctCookie *cookie,
                       int64_t bytes, enum BlockAcctType type)
 {
@@ -241,11 +285,70 @@ static void block_account_one_io(BlockAcctStats *stats, BlockAcctCookie *cookie,
     cookie->type = BLOCK_ACCT_NONE;
 }
 
+/*
+ * 在以下使用block_acct_done():
+ *   - hw/block/dataplane/xen-block.c|304| <<xen_block_complete_aio>> block_acct_done(blk_get_stats(dataplane->blk), &request->acct);
+ *   - hw/block/virtio-blk.c|139| <<virtio_blk_rw_complete>> block_acct_done(blk_get_stats(s->blk), &req->acct);
+ *   - hw/block/virtio-blk.c|154| <<virtio_blk_flush_complete>> block_acct_done(blk_get_stats(s->blk), &req->acct);
+ *   - hw/block/virtio-blk.c|171| <<virtio_blk_discard_write_zeroes_complete>> block_acct_done(blk_get_stats(s->blk), &req->acct);
+ *   - hw/ide/ahci.c|1007| <<ncq_finish>> block_acct_done(blk_get_stats(ncq_tfs->drive->port.ifs[0].blk),
+ *   - hw/ide/atapi.c|120| <<cd_read_sector_sync>> block_acct_done(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/atapi.c|140| <<cd_read_sector_cb>> block_acct_done(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/atapi.c|417| <<ide_atapi_cmd_read_dma_cb>> block_acct_done(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/core.c|470| <<ide_issue_trim_cb>> block_acct_done(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/core.c|779| <<ide_sector_read_cb>> block_acct_done(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/core.c|982| <<ide_dma_cb>> block_acct_done(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/core.c|1040| <<ide_sector_write_cb>> block_acct_done(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/core.c|1118| <<ide_flush_cb>> block_acct_done(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/macio.c|125| <<pmac_ide_atapi_transfer_cb>> block_acct_done(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/macio.c|205| <<pmac_ide_transfer_cb>> block_acct_done(blk_get_stats(s->blk), &s->acct);
+ *   - hw/nvme/ctrl.c|2174| <<nvme_rw_complete_cb>> block_acct_done(stats, acct);
+ *   - hw/nvme/ctrl.c|2264| <<nvme_verify_cb>> block_acct_done(stats, acct);
+ *   - hw/nvme/ctrl.c|2413| <<nvme_compare_mdata_cb>> block_acct_done(stats, acct)
+ *   - hw/nvme/ctrl.c|2482| <<nvme_compare_data_cb>> block_acct_done(stats, acct);
+ *   - hw/nvme/ctrl.c|2785| <<nvme_copy_done>> block_acct_done(stats, &iocb->acct.read);
+ *   - hw/nvme/ctrl.c|2786| <<nvme_copy_done>> block_acct_done(stats, &iocb->acct.write);
+ *   - hw/scsi/scsi-disk.c|359| <<scsi_aio_complete>> block_acct_done(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - hw/scsi/scsi-disk.c|442| <<scsi_dma_complete>> block_acct_done(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - hw/scsi/scsi-disk.c|480| <<scsi_read_complete>> block_acct_done(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - hw/scsi/scsi-disk.c|530| <<scsi_do_read_cb>> block_acct_done(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - hw/scsi/scsi-disk.c|616| <<scsi_write_complete>> block_acct_done(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - hw/scsi/scsi-disk.c|1822| <<scsi_unmap_complete>> block_acct_done(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - hw/scsi/scsi-disk.c|1899| <<scsi_write_same_complete>> block_acct_done(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - qemu-io-cmds.c|1375| <<aio_write_done>> block_acct_done(blk_get_stats(ctx->blk), &ctx->acct);
+ *   - qemu-io-cmds.c|1418| <<aio_read_done>> block_acct_done(blk_get_stats(ctx->blk), &ctx->acct);
+ *   - qemu-io-cmds.c|1711| <<aio_flush_f>> block_acct_done(blk_get_stats(blk), &cookie);
+ */
 void block_acct_done(BlockAcctStats *stats, BlockAcctCookie *cookie)
 {
     block_account_one_io(stats, cookie, false);
 }
 
+/*
+ * 在以下使用block_acct_failed():
+ *   - hw/block/dataplane/xen-block.c|306| <<xen_block_complete_aio>> block_acct_failed(blk_get_stats(dataplane->blk), &request->acct);
+ *   - hw/block/virtio-blk.c|90| <<virtio_blk_handle_rw_error>> block_acct_failed(blk_get_stats(s->blk), &req->acct);
+ *   - hw/ide/atapi.c|118| <<cd_read_sector_sync>> block_acct_failed(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/atapi.c|135| <<cd_read_sector_cb>> block_acct_failed(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/atapi.c|415| <<ide_atapi_cmd_read_dma_cb>> block_acct_failed(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/core.c|472| <<ide_issue_trim_cb>> block_acct_failed(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/core.c|866| <<ide_handle_rw_error>> block_acct_failed(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/macio.c|123| <<pmac_ide_atapi_transfer_cb>> block_acct_failed(blk_get_stats(s->blk), &s->acct);
+ *   - hw/ide/macio.c|203| <<pmac_ide_transfer_cb>> block_acct_failed(blk_get_stats(s->blk), &s->acct);
+ *   - hw/nvme/ctrl.c|2151| <<nvme_rw_complete_cb>> block_acct_failed(stats, acct);
+ *   - hw/nvme/ctrl.c|2256| <<nvme_verify_cb>> block_acct_failed(stats, acct);
+ *   - hw/nvme/ctrl.c|2358| <<nvme_compare_mdata_cb>> block_acct_failed(stats, acct);
+ *   - hw/nvme/ctrl.c|2443| <<nvme_compare_data_cb>> block_acct_failed(stats, acct);
+ *   - hw/nvme/ctrl.c|2782| <<nvme_copy_done>> block_acct_failed(stats, &iocb->acct.read);
+ *   - hw/nvme/ctrl.c|2783| <<nvme_copy_done>> block_acct_failed(stats, &iocb->acct.write);
+ *   - hw/scsi/scsi-disk.c|304| <<scsi_handle_rw_error>> block_acct_failed(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - hw/scsi/scsi-disk.c|440| <<scsi_dma_complete>> block_acct_failed(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - hw/scsi/scsi-disk.c|478| <<scsi_read_complete>> block_acct_failed(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - hw/scsi/scsi-disk.c|528| <<scsi_do_read_cb>> block_acct_failed(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - hw/scsi/scsi-disk.c|614| <<scsi_write_complete>> block_acct_failed(blk_get_stats(s->qdev.conf.blk), &r->acct);
+ *   - qemu-io-cmds.c|1371| <<aio_write_done>> block_acct_failed(blk_get_stats(ctx->blk), &ctx->acct);
+ *   - qemu-io-cmds.c|1403| <<aio_read_done>> block_acct_failed(blk_get_stats(ctx->blk), &ctx->acct);
+ */
 void block_acct_failed(BlockAcctStats *stats, BlockAcctCookie *cookie)
 {
     block_account_one_io(stats, cookie, true);
diff --git a/block/aio_task.c b/block/aio_task.c
index bb5c05f45..24ef6f783 100644
--- a/block/aio_task.c
+++ b/block/aio_task.c
@@ -86,6 +86,11 @@ void coroutine_fn aio_task_pool_wait_all(AioTaskPool *pool)
     }
 }
 
+/*
+ * 在以下使用aio_task_pool_start_task():
+ *   - block/block-copy.c|483| <<block_copy_task_run>> aio_task_pool_start_task(pool, &task->task);
+ *   - block/qcow2.c|2324| <<qcow2_add_task>> aio_task_pool_start_task(pool, &task->task);
+ */
 void coroutine_fn aio_task_pool_start_task(AioTaskPool *pool, AioTask *task)
 {
     aio_task_pool_wait_slot(pool);
diff --git a/block/block-backend.c b/block/block-backend.c
index a402db13f..2141e9630 100644
--- a/block/block-backend.c
+++ b/block/block-backend.c
@@ -1560,6 +1560,18 @@ static const AIOCBInfo blk_aio_em_aiocb_info = {
     .aiocb_size         = sizeof(BlkAioEmAIOCB),
 };
 
+/*
+ * 在以下使用blk_aio_complete():
+ *   - block/block-backend.c|1576| <<blk_aio_complete_bh>> blk_aio_complete(acb);
+ *   - block/block-backend.c|1630| <<blk_aio_read_entry>> blk_aio_complete(acb);
+ *   - block/block-backend.c|1642| <<blk_aio_write_entry>> blk_aio_complete(acb);
+ *   - block/block-backend.c|1804| <<blk_aio_ioctl_entry>> blk_aio_complete(acb);
+ *   - block/block-backend.c|1838| <<blk_aio_pdiscard_entry>> blk_aio_complete(acb);
+ *   - block/block-backend.c|1883| <<blk_aio_flush_entry>> blk_aio_complete(acb);
+ *   - block/block-backend.c|1913| <<blk_aio_zone_report_entry>> blk_aio_complete(acb);
+ *   - block/block-backend.c|1956| <<blk_aio_zone_mgmt_entry>> blk_aio_complete(acb);
+ *   - block/block-backend.c|1996| <<blk_aio_zone_append_entry>> blk_aio_complete(acb);
+ */
 static void blk_aio_complete(BlkAioEmAIOCB *acb)
 {
     if (acb->has_returned) {
@@ -1569,6 +1581,17 @@ static void blk_aio_complete(BlkAioEmAIOCB *acb)
     }
 }
 
+/*
+ * 在以下使用blk_aio_complete_bh():
+ *   - block/block-backend.c|1615| <<blk_aio_prwv>>
+ *      replay_bh_schedule_oneshot_event(qemu_get_current_aio_context(), blk_aio_complete_bh, acb);
+ *   - block/block-backend.c|1942| <<blk_aio_zone_report>>
+ *      replay_bh_schedule_oneshot_event(qemu_get_current_aio_context(), blk_aio_complete_bh, acb);
+ *   - block/block-backend.c|1983| <<blk_aio_zone_mgmt>>
+ *      replay_bh_schedule_oneshot_event(qemu_get_current_aio_context(), blk_aio_complete_bh, acb);
+ *   - block/block-backend.c|2022| <<blk_aio_zone_append>>
+ *      replay_bh_schedule_oneshot_event(qemu_get_current_aio_context(), blk_aio_complete_bh, acb);
+ */
 static void blk_aio_complete_bh(void *opaque)
 {
     BlkAioEmAIOCB *acb = opaque;
@@ -1576,6 +1599,46 @@ static void blk_aio_complete_bh(void *opaque)
     blk_aio_complete(acb);
 }
 
+/*
+ * 没有iothread应该就是mainloop
+ * (gdb) bt
+ * #0  coroutine_pool_get_local () at ../util/qemu-coroutine.c:116
+ * #1  0x00005555561cb785 in coroutine_pool_get () at ../util/qemu-coroutine.c:181
+ * #2  0x00005555561cb8c7 in qemu_coroutine_create (entry=0x5555560034f4 <blk_aio_write_entry>, opaque=0x5555582fd4d0)
+ *     at ../util/qemu-coroutine.c:222
+ * #3  0x00005555560033cc in blk_aio_prwv (blk=0x55555846f060, offset=540709888, bytes=1024, iobuf=0x5555577f63e8,
+ *     co_entry=0x5555560034f4 <blk_aio_write_entry>, flags=BDRV_REQ_REGISTERED_BUF, 
+ *     cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350) at ../block/block-backend.c:1600
+ * #4  0x0000555556003927 in blk_aio_pwritev (blk=0x55555846f060, offset=540709888, qiov=0x5555577f63e8,
+ *     flags=BDRV_REQ_REGISTERED_BUF, cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350)
+ *     at ../block/block-backend.c:1722
+ * #5  0x0000555555e2c003 in submit_requests (s=0x5555584688c0, mrb=0x7fffffffd640, start=0, num_reqs=1, niov=-1) at ../hw/block/virtio-blk.c:259
+ * #6  0x0000555555e2c12f in virtio_blk_submit_multireq (s=0x5555584688c0, mrb=0x7fffffffd640) at ../hw/block/virtio-blk.c:294
+ * #7  0x0000555555e2e065 in virtio_blk_handle_vq (s=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1008
+ * #8  0x0000555555e2e0f8 in virtio_blk_handle_output (vdev=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1028
+ * #9  0x0000555555e89d02 in virtio_queue_notify_vq (vq=0x5555584b0960) at ../hw/virtio/virtio.c:2484
+ * #10 0x0000555555e8d53c in virtio_queue_host_notifier_read (n=0x5555584b09d4) at ../hw/virtio/virtio.c:3869
+ * #11 0x00005555561a74d8 in aio_dispatch_handler (ctx=0x55555745ee50, node=0x7fffe43dfba0) at ../util/aio-posix.c:376
+ * #12 0x00005555561a76ed in aio_dispatch_handlers (ctx=0x55555745ee50) at ../util/aio-posix.c:427
+ * #13 0x00005555561a7749 in aio_dispatch (ctx=0x55555745ee50) at ../util/aio-posix.c:437
+ * #14 0x00005555561c9148 in aio_ctx_dispatch (source=0x55555745ee50, callback=0x0, user_data=0x0) at ../util/async.c:361
+ * #15 0x00007ffff6fd394b in g_main_dispatch (context=0x55555745f260) at ../glib/gmain.c:3325
+ * #16 g_main_context_dispatch (context=0x55555745f260) at ../glib/gmain.c:4043
+ * #17 0x00005555561ca939 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #18 0x00005555561ca9c7 in os_host_main_loop_wait (timeout=657614) at ../util/main-loop.c:310
+ * #19 0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #20 0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #21 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #22 0x00005555560d296f in main (argc=25, argv=0x7fffffffdb28) at ../system/main.c:80
+ *
+ * called by:
+ *   - block/block-backend.c|1641| <<blk_aio_pwrite_zeroes>> return blk_aio_prwv(blk, offset, bytes, NULL, blk_aio_write_entry,
+ *   - block/block-backend.c|1712| <<blk_aio_preadv>> return blk_aio_prwv(blk, offset, qiov->size, qiov,
+ *   - block/block-backend.c|1722| <<blk_aio_pwritev>> return blk_aio_prwv(blk, offset, qiov->size, qiov,
+ *   - block/block-backend.c|1781| <<blk_aio_ioctl>> return blk_aio_prwv(blk, req, 0, buf, blk_aio_ioctl_entry, 0, cb, opaque);
+ *   - block/block-backend.c|1816| <<blk_aio_pdiscard>> return blk_aio_prwv(blk, offset, bytes, NULL, blk_aio_pdiscard_entry, 0,
+ *   - block/block-backend.c|1860| <<blk_aio_flush>> return blk_aio_prwv(blk, 0, 0, NULL, blk_aio_flush_entry, 0, cb, opaque);
+ */
 static BlockAIOCB *blk_aio_prwv(BlockBackend *blk, int64_t offset,
                                 int64_t bytes,
                                 void *iobuf, CoroutineEntry co_entry,
@@ -1602,6 +1665,17 @@ static BlockAIOCB *blk_aio_prwv(BlockBackend *blk, int64_t offset,
 
     acb->has_returned = true;
     if (acb->rwco.ret != NOT_DONE) {
+        /*
+	 * 在以下使用blk_aio_complete_bh():
+	 *   - block/block-backend.c|1615| <<blk_aio_prwv>>
+	 *      replay_bh_schedule_oneshot_event(qemu_get_current_aio_context(), blk_aio_complete_bh, acb);
+	 *   - block/block-backend.c|1942| <<blk_aio_zone_report>>
+	 *      replay_bh_schedule_oneshot_event(qemu_get_current_aio_context(), blk_aio_complete_bh, acb);
+	 *   - block/block-backend.c|1983| <<blk_aio_zone_mgmt>>
+	 *      replay_bh_schedule_oneshot_event(qemu_get_current_aio_context(), blk_aio_complete_bh, acb);
+	 *   - block/block-backend.c|2022| <<blk_aio_zone_append>>
+	 *      replay_bh_schedule_oneshot_event(qemu_get_current_aio_context(), blk_aio_complete_bh, acb);
+	 */
         replay_bh_schedule_oneshot_event(qemu_get_current_aio_context(),
                                          blk_aio_complete_bh, acb);
     }
@@ -1621,6 +1695,13 @@ static void coroutine_fn blk_aio_read_entry(void *opaque)
     blk_aio_complete(acb);
 }
 
+/*
+ * 在以下使用blk_aio_write_entry():
+ *   - block/block-backend.c|1650| <<blk_aio_pwrite_zeroes>> return blk_aio_prwv(blk,
+ *      offset, bytes, NULL, blk_aio_write_entry, flags | BDRV_REQ_ZERO_WRITE, cb, opaque);
+ *   - block/block-backend.c|1753| <<blk_aio_pwritev>> return blk_aio_prwv(blk,
+ *      offset, qiov->size, qiov, blk_aio_write_entry, flags, cb, opaque);
+ */
 static void coroutine_fn blk_aio_write_entry(void *opaque)
 {
     BlkAioEmAIOCB *acb = opaque;
@@ -1638,6 +1719,13 @@ BlockAIOCB *blk_aio_pwrite_zeroes(BlockBackend *blk, int64_t offset,
                                   BlockCompletionFunc *cb, void *opaque)
 {
     IO_CODE();
+    /*
+     * 在以下使用blk_aio_write_entry():
+     *   - block/block-backend.c|1650| <<blk_aio_pwrite_zeroes>> return blk_aio_prwv(blk,
+     *      offset, bytes, NULL, blk_aio_write_entry, flags | BDRV_REQ_ZERO_WRITE, cb, opaque);
+     *   - block/block-backend.c|1753| <<blk_aio_pwritev>> return blk_aio_prwv(blk,
+     *      offset, qiov->size, qiov, blk_aio_write_entry, flags, cb, opaque);
+     */
     return blk_aio_prwv(blk, offset, bytes, NULL, blk_aio_write_entry,
                         flags | BDRV_REQ_ZERO_WRITE, cb, opaque);
 }
@@ -1713,12 +1801,71 @@ BlockAIOCB *blk_aio_preadv(BlockBackend *blk, int64_t offset,
                         blk_aio_read_entry, flags, cb, opaque);
 }
 
+/*
+ * 没有iothread应该就是mainloop
+ * (gdb) bt
+ * #0  coroutine_pool_get_local () at ../util/qemu-coroutine.c:116
+ * #1  0x00005555561cb785 in coroutine_pool_get () at ../util/qemu-coroutine.c:181
+ * #2  0x00005555561cb8c7 in qemu_coroutine_create (entry=0x5555560034f4 <blk_aio_write_entry>, opaque=0x5555582fd4d0)
+ *     at ../util/qemu-coroutine.c:222
+ * #3  0x00005555560033cc in blk_aio_prwv (blk=0x55555846f060, offset=540709888, bytes=1024, iobuf=0x5555577f63e8,
+ *     co_entry=0x5555560034f4 <blk_aio_write_entry>, flags=BDRV_REQ_REGISTERED_BUF, 
+ *     cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350) at ../block/block-backend.c:1600
+ * #4  0x0000555556003927 in blk_aio_pwritev (blk=0x55555846f060, offset=540709888, qiov=0x5555577f63e8,
+ *     flags=BDRV_REQ_REGISTERED_BUF, cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350)
+ *     at ../block/block-backend.c:1722
+ * #5  0x0000555555e2c003 in submit_requests (s=0x5555584688c0, mrb=0x7fffffffd640, start=0, num_reqs=1, niov=-1) at ../hw/block/virtio-blk.c:259
+ * #6  0x0000555555e2c12f in virtio_blk_submit_multireq (s=0x5555584688c0, mrb=0x7fffffffd640) at ../hw/block/virtio-blk.c:294
+ * #7  0x0000555555e2e065 in virtio_blk_handle_vq (s=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1008
+ * #8  0x0000555555e2e0f8 in virtio_blk_handle_output (vdev=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1028
+ * #9  0x0000555555e89d02 in virtio_queue_notify_vq (vq=0x5555584b0960) at ../hw/virtio/virtio.c:2484
+ * #10 0x0000555555e8d53c in virtio_queue_host_notifier_read (n=0x5555584b09d4) at ../hw/virtio/virtio.c:3869
+ * #11 0x00005555561a74d8 in aio_dispatch_handler (ctx=0x55555745ee50, node=0x7fffe43dfba0) at ../util/aio-posix.c:376
+ * #12 0x00005555561a76ed in aio_dispatch_handlers (ctx=0x55555745ee50) at ../util/aio-posix.c:427
+ * #13 0x00005555561a7749 in aio_dispatch (ctx=0x55555745ee50) at ../util/aio-posix.c:437
+ * #14 0x00005555561c9148 in aio_ctx_dispatch (source=0x55555745ee50, callback=0x0, user_data=0x0) at ../util/async.c:361
+ * #15 0x00007ffff6fd394b in g_main_dispatch (context=0x55555745f260) at ../glib/gmain.c:3325
+ * #16 g_main_context_dispatch (context=0x55555745f260) at ../glib/gmain.c:4043
+ * #17 0x00005555561ca939 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #18 0x00005555561ca9c7 in os_host_main_loop_wait (timeout=657614) at ../util/main-loop.c:310
+ * #19 0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #20 0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #21 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #22 0x00005555560d296f in main (argc=25, argv=0x7fffffffdb28) at ../system/main.c:80
+ *
+ * called by:
+ *   - hw/block/dataplane/xen-block.c|393| <<xen_block_do_aio>> blk_aio_pwritev(dataplane->blk, request->start, &request->v, 0,
+ *   - hw/block/m25p80.c|581| <<flash_sync_page>> blk_aio_pwritev(s->blk, page * s->pi->page_size, iov, 0,
+ *   - hw/block/m25p80.c|597| <<flash_sync_area>> blk_aio_pwritev(s->blk, off, iov, 0, blk_sync_complete, iov);
+ *   - hw/block/virtio-blk.c|259| <<submit_requests>> blk_aio_pwritev(blk, sector_num << BDRV_SECTOR_BITS, qiov,
+ *   - hw/ide/core.c|1100| <<ide_sector_write>> s->pio_aiocb = blk_aio_pwritev(s->blk, sector_num << BDRV_SECTOR_BITS,
+ *   - hw/nvme/ctrl.c|1475| <<nvme_blk_write>> req->aiocb = blk_aio_pwritev(blk, offset, &req->sg.iov, 0, cb, req);
+ *   - hw/nvme/ctrl.c|2964| <<nvme_copy_out_cb>> iocb->aiocb = blk_aio_pwritev(dns->blkconf.blk, nvme_moff(dns, iocb->slba),
+ *   - hw/nvme/ctrl.c|3073| <<nvme_copy_in_completed_cb>> iocb->aiocb = blk_aio_pwritev(dns->blkconf.blk, nvme_l2b(dns, iocb->slba),
+ *   - hw/nvme/dif.c|530| <<nvme_dif_rw_mdata_out_cb>> req->aiocb = blk_aio_pwritev(blk, offset, &ctx->mdata.iov, 0,
+ *   - hw/nvme/dif.c|696| <<nvme_dif_rw>> req->aiocb = blk_aio_pwritev(ns->blkconf.blk, offset, &ctx->data.iov, 0,
+ *   - hw/scsi/scsi-disk.c|1911| <<scsi_write_same_complete>> r->req.aiocb = blk_aio_pwritev(s->qdev.conf.blk,
+ *   - hw/scsi/scsi-disk.c|1983| <<scsi_disk_emulate_write_same>> r->req.aiocb = blk_aio_pwritev(s->qdev.conf.blk,
+ *   - hw/scsi/scsi-disk.c|3140| <<scsi_dma_writev>> return blk_aio_pwritev(s->qdev.conf.blk, offset, iov, 0, cb, cb_opaque);
+ *   - qemu-img.c|4497| <<bench_cb>> acb = blk_aio_pwritev(b->blk, offset, b->qiov, 0, bench_cb, b);
+ *   - qemu-io-cmds.c|663| <<do_aio_writev>> blk_aio_pwritev(blk, offset, qiov, flags, aio_rw_done, &async_ret);
+ *   - qemu-io-cmds.c|1699| <<aio_write_f>> blk_aio_pwritev(blk, ctx->offset, &ctx->qiov, ctx->flags,
+ *   - system/dma-helpers.c|265| <<dma_blk_write_io_func>> return blk_aio_pwritev(blk, offset, iov, 0, cb, cb_opaque);
+ *   - tests/unit/test-replication.c|113| <<test_blk_write>> blk_aio_pwritev(blk, offset, &qiov, 0, blk_rw_done, &async_ret);
+ */
 BlockAIOCB *blk_aio_pwritev(BlockBackend *blk, int64_t offset,
                             QEMUIOVector *qiov, BdrvRequestFlags flags,
                             BlockCompletionFunc *cb, void *opaque)
 {
     IO_CODE();
     assert((uint64_t)qiov->size <= INT64_MAX);
+    /*
+     * 在以下使用blk_aio_write_entry():
+     *   - block/block-backend.c|1650| <<blk_aio_pwrite_zeroes>> return blk_aio_prwv(blk,
+     *      offset, bytes, NULL, blk_aio_write_entry, flags | BDRV_REQ_ZERO_WRITE, cb, opaque);
+     *   - block/block-backend.c|1753| <<blk_aio_pwritev>> return blk_aio_prwv(blk,
+     *      offset, qiov->size, qiov, blk_aio_write_entry, flags, cb, opaque);
+     */
     return blk_aio_prwv(blk, offset, qiov->size, qiov,
                         blk_aio_write_entry, flags, cb, opaque);
 }
@@ -1853,6 +2000,20 @@ static void coroutine_fn blk_aio_flush_entry(void *opaque)
     blk_aio_complete(acb);
 }
 
+/*
+ * 在以下使用blk_aio_flush():
+ *   - hw/block/dataplane/xen-block.c|367| <<xen_block_do_aio>> blk_aio_flush(request->dataplane->blk, xen_block_complete_aio,
+ *   - hw/block/virtio-blk.c|497| <<virtio_blk_handle_flush>> blk_aio_flush(s->blk, virtio_blk_flush_complete, req);
+ *   - hw/ide/core.c|1135| <<ide_flush_cache>> s->pio_aiocb = blk_aio_flush(s->blk, ide_flush_cb, s);
+ *   - hw/nvme/ctrl.c|3524| <<nvme_flush_ns_cb>> iocb->aiocb = blk_aio_flush(ns->blkconf.blk, nvme_flush_ns_cb, iocb);
+ *   - hw/scsi/scsi-disk.c|402| <<scsi_write_do_fua>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+ *   - hw/scsi/scsi-disk.c|570| <<scsi_read_data>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_do_read_cb, r);
+ *   - hw/scsi/scsi-disk.c|1725| <<scsi_disk_emulate_mode_select>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+ *   - hw/scsi/scsi-disk.c|2252| <<scsi_disk_emulate_command>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+ *   - qemu-img.c|4471| <<bench_cb>> acb = blk_aio_flush(b->blk, cb, b);
+ *   - tests/unit/test-block-backend.c|46| <<test_drain_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+ *   - tests/unit/test-block-backend.c|63| <<test_drain_all_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+ */
 BlockAIOCB *blk_aio_flush(BlockBackend *blk,
                           BlockCompletionFunc *cb, void *opaque)
 {
@@ -2120,6 +2281,13 @@ BlockdevOnError blk_get_on_error(BlockBackend *blk, bool is_read)
     return is_read ? blk->on_read_error : blk->on_write_error;
 }
 
+/*
+ * 在以下使用blk_get_error_action():
+ *   - hw/block/virtio-blk.c|76| <<virtio_blk_handle_rw_error>> BlockErrorAction action = blk_get_error_action(s->blk, is_read, error);
+ *   - hw/ide/ahci.c|1022| <<ncq_cb>> BlockErrorAction action = blk_get_error_action(ide_state->blk, is_read, -ret);
+ *   - hw/ide/core.c|860| <<ide_handle_rw_error>> BlockErrorAction action = blk_get_error_action(s->blk, is_read, error);
+ *   - hw/scsi/scsi-disk.c|297| <<scsi_handle_rw_error>> action = blk_get_error_action(s->qdev.conf.blk, is_read, error);
+ */
 BlockErrorAction blk_get_error_action(BlockBackend *blk, bool is_read,
                                       int error)
 {
diff --git a/chardev/char.c b/chardev/char.c
index 5a9e9762a..2cd39e154 100644
--- a/chardev/char.c
+++ b/chardev/char.c
@@ -108,6 +108,42 @@ static void qemu_chr_write_log(Chardev *s, const uint8_t *buf, size_t len)
     }
 }
 
+/*
+ * (gdb) bt
+ * #0  qemu_chr_write_buffer (s=0x55555750d400, buf=0x55555759aa0c "[", len=1, offset=0x7fffed23e1c0, write_all=false)
+ *     at ../chardev/char.c:115
+ * #1  0x00005555560c7c5e in qemu_chr_write (s=0x55555750d400, buf=0x55555759aa0c "[", len=1, write_all=false)
+ *     at ../chardev/char.c:186
+ * #2  0x00005555560bc2e5 in qemu_chr_fe_write (be=0x55555759aa28, buf=0x55555759aa0c "[", len=1)
+ *     at ../chardev/char-fe.c:41
+ * #3  0x0000555555965c20 in serial_xmit (s=0x55555759a970) at ../hw/char/serial.c:259
+ * #4  0x00005555559660b5 in serial_ioport_write (opaque=0x55555759a970, addr=0, val=91, size=1)
+ *     at ../hw/char/serial.c:359
+ * #5  0x0000555555eb8e96 in memory_region_write_accessor (mr=0x55555759aae0, addr=0, value=0x7fffed23e3a8, size=1,
+ *     shift=0, mask=255, attrs=...) at ../system/memory.c:497
+ * #6  0x0000555555eb91df in access_with_adjusted_size (addr=0, value=0x7fffed23e3a8, size=1, access_size_min=1,
+ *     access_size_max=1, access_fn=0x555555eb8da0 <memory_region_write_accessor>, mr=0x55555759aae0, attrs=...)
+ *     at ../system/memory.c:573
+ * #7  0x0000555555ebc9a2 in memory_region_dispatch_write (mr=0x55555759aae0, addr=0, data=91, op=MO_8, attrs=...)
+ *     at ../system/memory.c:1553
+ * #8  0x0000555555eccb5a in flatview_write_continue_step (attrs=..., buf=0x7ffff7fee000 "[", len=1, mr_addr=0,
+ *     l=0x7fffed23e490, mr=0x55555759aae0) at ../system/physmem.c:2951
+ * #9  0x0000555555eccc2f in flatview_write_continue (fv=0x7ffbd801c2c0, addr=1016, attrs=..., ptr=0x7ffff7fee000,
+ *     len=1, mr_addr=0, l=1, mr=0x55555759aae0) at ../system/physmem.c:2981
+ * #10 0x0000555555eccd5c in flatview_write (fv=0x7ffbd801c2c0, addr=1016, attrs=..., buf=0x7ffff7fee000, len=1)
+ *     at ../system/physmem.c:3012
+ * #11 0x0000555555ecd1e9 in address_space_write (as=0x555557383fc0 <address_space_io>, addr=1016, attrs=...,
+ *     buf=0x7ffff7fee000, len=1) at ../system/physmem.c:3132
+ * #12 0x0000555555ecd263 in address_space_rw (as=0x555557383fc0 <address_space_io>, addr=1016, attrs=...,
+ *     buf=0x7ffff7fee000, len=1, is_write=true) at ../system/physmem.c:3142
+ * #13 0x0000555555f0b7dd in kvm_handle_io (port=1016, attrs=..., data=0x7ffff7fee000, direction=1, size=1, count=1)
+ *     at ../accel/kvm/kvm-all.c:2800
+ * #14 0x0000555555f0c755 in kvm_cpu_exec (cpu=0x555557784610) at ../accel/kvm/kvm-all.c:3186
+ * #15 0x0000555555f0fecd in kvm_vcpu_thread_fn (arg=0x555557784610) at ../accel/kvm/kvm-accel-ops.c:51
+ * #16 0x00005555561ad4c6 in qemu_thread_start (args=0x55555778dd20) at ../util/qemu-thread-posix.c:541
+ * #17 0x00007ffff68a91da in start_thread () from /lib64/libpthread.so.0
+ * #18 0x00007ffff4bda8d3 in clone () from /lib64/libc.so.6
+ */
 static int qemu_chr_write_buffer(Chardev *s,
                                  const uint8_t *buf, int len,
                                  int *offset, bool write_all)
@@ -195,6 +231,23 @@ int qemu_chr_write(Chardev *s, const uint8_t *buf, int len, bool write_all)
     return offset;
 }
 
+/*
+ * (gdb) bt
+ * #0  serial_can_receive (s=0x55555759a970) at ../hw/char/serial.c:557
+ * #1  0x00005555559669af in serial_can_receive1 (opaque=0x55555759a970) at ../hw/char/serial.c:597
+ * #2  0x00005555560c7d08 in qemu_chr_be_can_write (s=0x55555750d400) at ../chardev/char.c:206
+ * #3  0x00005555560cae4c in fd_chr_read_poll (opaque=0x55555750d400) at ../chardev/char-fd.c:83
+ * #4  0x00005555560bd37f in io_watch_poll_prepare (source=0x555558342690, timeout=0x7fffffffda04)
+ *     at ../chardev/char-io.c:48
+ * #5  0x00007ffff6fd30f7 in g_main_context_prepare (context=0x55555745e6a0, priority=0x5555573a2588 <max_priority>)
+ *     at ../glib/gmain.c:3645
+ * #6  0x00005555561ca7e4 in glib_pollfds_fill (cur_timeout=0x7fffffffdac8) at ../util/main-loop.c:259
+ * #7  0x00005555561ca97a in os_host_main_loop_wait (timeout=271798) at ../util/main-loop.c:300
+ * #8  0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #9  0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #10 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #11 0x00005555560d296f in main (argc=21, argv=0x7fffffffdc88) at ../system/main.c:80
+ */
 int qemu_chr_be_can_write(Chardev *s)
 {
     CharBackend *be = s->be;
diff --git a/cpu-common.c b/cpu-common.c
index ef5757d23..30241549c 100644
--- a/cpu-common.c
+++ b/cpu-common.c
@@ -285,6 +285,36 @@ void cpu_exec_start(CPUState *cpu)
     }
 }
 
+/*
+ * 在以下调用cpu_exec_end():
+ *   - accel/kvm/kvm-all.c|3336| <<kvm_cpu_exec>> cpu_exec_end(cpu);
+ *   - accel/tcg/tcg-accel-ops.c|81| <<tcg_cpu_exec>> cpu_exec_end(cpu);
+ *   - bsd-user/aarch64/target_arch_cpu.h|56| <<target_cpu_loop>> cpu_exec_end(cs);
+ *   - bsd-user/arm/target_arch_cpu.h|48| <<target_cpu_loop>> cpu_exec_end(cs);
+ *   - bsd-user/i386/target_arch_cpu.h|115| <<target_cpu_loop>> cpu_exec_end(cs);
+ *   - bsd-user/riscv/target_arch_cpu.h|51| <<target_cpu_loop>> cpu_exec_end(cs);
+ *   - bsd-user/x86_64/target_arch_cpu.h|123| <<target_cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/aarch64/cpu_loop.c|40| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/alpha/cpu_loop.c|37| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/arm/cpu_loop.c|296| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/hexagon/cpu_loop.c|38| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/hppa/cpu_loop.c|121| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/i386/cpu_loop.c|216| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/loongarch64/cpu_loop.c|23| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/m68k/cpu_loop.c|35| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/microblaze/cpu_loop.c|34| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/mips/cpu_loop.c|76| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/openrisc/cpu_loop.c|35| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/ppc/cpu_loop.c|79| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/riscv/cpu_loop.c|38| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/s390x/cpu_loop.c|66| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/sh4/cpu_loop.c|36| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/sparc/cpu_loop.c|222| <<cpu_loop>> cpu_exec_end(cs);
+ *   - linux-user/xtensa/cpu_loop.c|135| <<cpu_loop>> cpu_exec_end(cs);
+ *   - target/i386/nvmm/nvmm-all.c|816| <<nvmm_vcpu_loop>> cpu_exec_end(cpu);
+ *   - target/i386/tcg/system/excp_helper.c|113| <<ptw_setl_slow>> cpu_exec_end(cpu);
+ *   - target/i386/whpx/whpx-all.c|2050| <<whpx_vcpu_run>> cpu_exec_end(cpu);
+ */
 /* Mark cpu as not executing, and release pending exclusive ops.  */
 void cpu_exec_end(CPUState *cpu)
 {
@@ -345,6 +375,19 @@ void free_queued_cpu_work(CPUState *cpu)
     }
 }
 
+/*
+ * QEMU-9.2的callstack
+ * (gdb) bt
+ * #0  kvm_arch_put_registers (cpu=0x555557721960, level=3, errp=0x7fffedb02580) at ../target/i386/kvm/kvm.c:5237
+ * #1  0x0000555555edb672 in do_kvm_cpu_synchronize_post_init (cpu=0x555557721960, arg=...) at ../accel/kvm/kvm-all.c:2905
+ * #2  0x00005555558841ea in process_queued_cpu_work (cpu=0x555557721960) at ../cpu-common.c:375
+ * #3  0x0000555555bd2d4f in qemu_wait_io_event_common (cpu=0x555557721960) at ../system/cpus.c:456
+ * #4  0x0000555555bd2de8 in qemu_wait_io_event (cpu=0x555557721960) at ../system/cpus.c:474
+ * #5  0x0000555555edf7e1 in kvm_vcpu_thread_fn (arg=0x555557721960) at ../accel/kvm/kvm-accel-ops.c:55
+ * #6  0x000055555617e61f in qemu_thread_start (args=0x55555772bbe0) at ../util/qemu-thread-posix.c:541
+ * #7  0x00007ffff68a91da in start_thread () from /lib64/libpthread.so.0
+ * #8  0x00007ffff4bda8d3 in clone () from /lib64/libc.so.6
+ */
 void process_queued_cpu_work(CPUState *cpu)
 {
     struct qemu_work_item *wi;
diff --git a/hw/acpi/utils.c b/hw/acpi/utils.c
index 0c486ea29..b8017cf58 100644
--- a/hw/acpi/utils.c
+++ b/hw/acpi/utils.c
@@ -26,6 +26,39 @@
 #include "hw/acpi/utils.h"
 #include "hw/loader.h"
 
+/*
+ * 在以下调用acpi_add_rom_blob():
+ *   - hw/arm/virt-acpi-build.c|1137| <<virt_acpi_setup>> build_state->table_mr =
+ *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.table_data, ACPI_BUILD_TABLE_FILE);
+ *   - hw/arm/virt-acpi-build.c|1142| <<virt_acpi_setup>> build_state->linker_mr =
+ *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+ *   - hw/arm/virt-acpi-build.c|1157| <<virt_acpi_setup>> build_state->rsdp_mr =
+ *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.rsdp, ACPI_BUILD_RSDP_FILE);
+ *   - hw/i386/acpi-build.c|2767| <<acpi_setup>> build_state->table_mr =
+ *       acpi_add_rom_blob(acpi_build_update, build_state, tables.table_data, ACPI_BUILD_TABLE_FILE);
+ *   - hw/i386/acpi-build.c|2773| <<acpi_setup>> build_state->linker_mr =
+ *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+ *   - hw/i386/acpi-build.c|2798| <<acpi_setup>> build_state->rsdp_mr =
+ *       acpi_add_rom_blob(acpi_build_update, build_state, tables.rsdp, ACPI_BUILD_RSDP_FILE);
+ *   - hw/i386/acpi-microvm.c|260| <<acpi_setup_microvm>> acpi_add_rom_blob(acpi_build_no_update,
+ *       NULL, tables.table_data, ACPI_BUILD_TABLE_FILE);
+ *   - hw/i386/acpi-microvm.c|262| <<acpi_setup_microvm>> acpi_add_rom_blob(acpi_build_no_update,
+ *       NULL, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+ *   - hw/i386/acpi-microvm.c|264| <<acpi_setup_microvm>> acpi_add_rom_blob(acpi_build_no_update,
+ *       NULL, tables.rsdp, ACPI_BUILD_RSDP_FILE);
+ *   - hw/loongarch/virt-acpi-build.c|717| <<virt_acpi_setup>> build_state->table_mr =
+ *       acpi_add_rom_blob(acpi_build_update, build_state, tables.table_data, ACPI_BUILD_TABLE_FILE);
+ *   - hw/loongarch/virt-acpi-build.c|723| <<virt_acpi_setup>> build_state->linker_mr =
+ *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+ *   - hw/loongarch/virt-acpi-build.c|726| <<virt_acpi_setup>> build_state->rsdp_mr =
+ *       acpi_add_rom_blob(acpi_build_update, build_state, tables.rsdp, ACPI_BUILD_RSDP_FILE);
+ *   - hw/riscv/virt-acpi-build.c|802| <<virt_acpi_setup>> build_state->table_mr =
+ *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.table_data, ACPI_BUILD_TABLE_FILE);
+ *   - hw/riscv/virt-acpi-build.c|807| <<virt_acpi_setup>> build_state->linker_mr =
+ *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+ *   - hw/riscv/virt-acpi-build.c|812| <<virt_acpi_setup>> build_state->rsdp_mr =
+ *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.rsdp, ACPI_BUILD_RSDP_FILE);
+ */
 MemoryRegion *acpi_add_rom_blob(FWCfgCallback update, void *opaque,
                                 GArray *blob, const char *name)
 {
@@ -35,6 +68,24 @@ MemoryRegion *acpi_add_rom_blob(FWCfgCallback update, void *opaque,
     if (!strcmp(name, ACPI_BUILD_TABLE_FILE)) {
         max_size = 0x200000;
     } else if (!strcmp(name, ACPI_BUILD_LOADER_FILE)) {
+        /*
+	 * 在以下使用ACPI_BUILD_LOADER_FILE:
+         *   - include/hw/acpi/aml-build.h|13| <<global>> #define ACPI_BUILD_LOADER_FILE "etc/table-loader"
+	 *   - hw/acpi/utils.c|37| <<acpi_add_rom_blob>> } else if (!strcmp(name, ACPI_BUILD_LOADER_FILE)) {
+         *   - hw/arm/virt-acpi-build.c|1145| <<virt_acpi_setup>> build_state->linker_mr =
+         *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+         *   - hw/i386/acpi-build.c|2774| <<acpi_setup>> build_state->linker_mr =
+         *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+         *   - hw/i386/acpi-microvm.c|263| <<acpi_setup_microvm>> acpi_add_rom_blob(acpi_build_no_update, NULL,
+         *       tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+         *   - hw/loongarch/virt-acpi-build.c|724| <<virt_acpi_setup>> build_state->linker_mr =
+         *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+         *   - hw/nvram/fw_cfg.c|654| <<fw_cfg_acpi_mr_restore_post_load>>
+         *       } else if (!strcmp(s->files->f[i].name, ACPI_BUILD_LOADER_FILE)) {
+         *   - hw/nvram/fw_cfg.c|894| <<fw_cfg_acpi_mr_save>> } else if (!strcmp(filename, ACPI_BUILD_LOADER_FILE)) {
+         *   - hw/riscv/virt-acpi-build.c|810| <<virt_acpi_setup>> build_state->linker_mr =
+         *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+	 */
         max_size = 0x10000;
     } else if (!strcmp(name, ACPI_BUILD_RSDP_FILE)) {
         max_size = 0x1000;
diff --git a/hw/arm/virt-acpi-build.c b/hw/arm/virt-acpi-build.c
index 3ac8f8e17..8d1aa1666 100644
--- a/hw/arm/virt-acpi-build.c
+++ b/hw/arm/virt-acpi-build.c
@@ -1026,18 +1026,43 @@ void virt_acpi_build(VirtMachineState *vms, AcpiBuildTables *tables)
     g_array_free(table_offsets, true);
 }
 
+/*
+ * 在以下调用arm的acpi_ram_update():
+ *   - hw/arm/virt-acpi-build.c|1064| <<virt_acpi_build_update>> acpi_ram_update(build_state->table_mr, tables.table_data);
+ *   - hw/arm/virt-acpi-build.c|1065| <<virt_acpi_build_update>> acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
+ *   - hw/arm/virt-acpi-build.c|1066| <<virt_acpi_build_update>> acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
+ *
+ * 在以下调用i386的acpi_ram_update():
+ *   - hw/i386/acpi-build.c|2671| <<acpi_build_update>> acpi_ram_update(build_state->table_mr, tables.table_data);
+ *   - hw/i386/acpi-build.c|2673| <<acpi_build_update>> acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
+ *   - hw/i386/acpi-build.c|2675| <<acpi_build_update>> acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
+ */
 static void acpi_ram_update(MemoryRegion *mr, GArray *data)
 {
     uint32_t size = acpi_data_len(data);
 
     /* Make sure RAM size is correct - in case it got changed
      * e.g. by migration */
+    /*
+     * 在以下调用memory_region_ram_resize():
+     *   - hw/arm/virt-acpi-build.c|1035| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/i386/acpi-build.c|2642| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/loongarch/virt-acpi-build.c|644| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/nvram/fw_cfg.c|631| <<fw_cfg_update_mr>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/riscv/virt-acpi-build.c|746| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     */
     memory_region_ram_resize(mr, size, &error_abort);
 
     memcpy(memory_region_get_ram_ptr(mr), data->data, size);
     memory_region_set_dirty(mr, 0, size);
 }
 
+/*
+ * 在以下使用virt_acpi_build_update():
+ *   - hw/arm/virt-acpi-build.c|1109| <<virt_acpi_setup>> build_state->table_mr = acpi_add_rom_blob(virt_acpi_build_update,
+ *   - hw/arm/virt-acpi-build.c|1114| <<virt_acpi_setup>> build_state->linker_mr = acpi_add_rom_blob(virt_acpi_build_update,
+ *   - hw/arm/virt-acpi-build.c|1129| <<virt_acpi_setup>> build_state->rsdp_mr = acpi_add_rom_blob(virt_acpi_build_update,
+ */
 static void virt_acpi_build_update(void *build_opaque)
 {
     AcpiBuildState *build_state = build_opaque;
@@ -1053,6 +1078,17 @@ static void virt_acpi_build_update(void *build_opaque)
 
     virt_acpi_build(VIRT_MACHINE(qdev_get_machine()), &tables);
 
+    /*
+     * 在以下调用arm的acpi_ram_update():
+     *   - hw/arm/virt-acpi-build.c|1064| <<virt_acpi_build_update>> acpi_ram_update(build_state->table_mr, tables.table_data);
+     *   - hw/arm/virt-acpi-build.c|1065| <<virt_acpi_build_update>> acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
+     *   - hw/arm/virt-acpi-build.c|1066| <<virt_acpi_build_update>> acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
+     *
+     * 在以下调用i386的acpi_ram_update():
+     *   - hw/i386/acpi-build.c|2671| <<acpi_build_update>> acpi_ram_update(build_state->table_mr, tables.table_data);
+     *   - hw/i386/acpi-build.c|2673| <<acpi_build_update>> acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
+     *   - hw/i386/acpi-build.c|2675| <<acpi_build_update>> acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
+     */
     acpi_ram_update(build_state->table_mr, tables.table_data);
     acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
     acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
@@ -1103,6 +1139,24 @@ void virt_acpi_setup(VirtMachineState *vms)
                                               ACPI_BUILD_TABLE_FILE);
     assert(build_state->table_mr != NULL);
 
+    /*
+     * 在以下使用ACPI_BUILD_LOADER_FILE:
+     *   - include/hw/acpi/aml-build.h|13| <<global>> #define ACPI_BUILD_LOADER_FILE "etc/table-loader"
+     *   - hw/acpi/utils.c|37| <<acpi_add_rom_blob>> } else if (!strcmp(name, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/arm/virt-acpi-build.c|1145| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/i386/acpi-build.c|2774| <<acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/i386/acpi-microvm.c|263| <<acpi_setup_microvm>> acpi_add_rom_blob(acpi_build_no_update, NULL,
+     *       tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/loongarch/virt-acpi-build.c|724| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/nvram/fw_cfg.c|654| <<fw_cfg_acpi_mr_restore_post_load>>
+     *       } else if (!strcmp(s->files->f[i].name, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/nvram/fw_cfg.c|894| <<fw_cfg_acpi_mr_save>> } else if (!strcmp(filename, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/riscv/virt-acpi-build.c|810| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     */
     build_state->linker_mr = acpi_add_rom_blob(virt_acpi_build_update,
                                                build_state,
                                                tables.linker->cmd_blob,
diff --git a/hw/block/vhost-user-blk.c b/hw/block/vhost-user-blk.c
index ae42327cf..1cfc2a36c 100644
--- a/hw/block/vhost-user-blk.c
+++ b/hw/block/vhost-user-blk.c
@@ -345,6 +345,33 @@ static int vhost_user_blk_connect(DeviceState *dev, Error **errp)
     vhost_dev_set_config_notifier(&s->dev, &blk_ops);
 
     s->vhost_user.supports_config = true;
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     ret = vhost_dev_init(&s->dev, &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0,
                          errp);
     if (ret < 0) {
@@ -475,6 +502,13 @@ static void vhost_user_blk_device_realize(DeviceState *dev, Error **errp)
         return;
     }
 
+    /*
+     * 在以下使用virtio_blk_cfg_size_params:
+     *   - hw/block/vhost-user-blk.c|478| <<vhost_user_blk_device_realize>>
+     *        config_size = virtio_get_config_size(&virtio_blk_cfg_size_params,
+     *   - hw/block/virtio-blk.c|1815| <<virtio_blk_device_realize>>
+     *        s->config_size = virtio_get_config_size(&virtio_blk_cfg_size_params,
+     */
     config_size = virtio_get_config_size(&virtio_blk_cfg_size_params,
                                          vdev->host_features);
     virtio_init(vdev, VIRTIO_ID_BLOCK, config_size);
diff --git a/hw/block/virtio-blk-common.c b/hw/block/virtio-blk-common.c
index e2f8e2f6d..475776b7f 100644
--- a/hw/block/virtio-blk-common.c
+++ b/hw/block/virtio-blk-common.c
@@ -34,6 +34,13 @@ static const VirtIOFeature feature_sizes[] = {
     {}
 };
 
+/*
+ * 在以下使用virtio_blk_cfg_size_params:
+ *   - hw/block/vhost-user-blk.c|478| <<vhost_user_blk_device_realize>>
+ *        config_size = virtio_get_config_size(&virtio_blk_cfg_size_params,
+ *   - hw/block/virtio-blk.c|1815| <<virtio_blk_device_realize>>
+ *        s->config_size = virtio_get_config_size(&virtio_blk_cfg_size_params,
+ */
 const VirtIOConfigSizeParams virtio_blk_cfg_size_params = {
     .min_size = VIRTIO_BLK_CFG_SIZE,
     .max_size = sizeof(struct virtio_blk_config),
diff --git a/hw/block/virtio-blk.c b/hw/block/virtio-blk.c
index 5077793e5..c248a1aba 100644
--- a/hw/block/virtio-blk.c
+++ b/hw/block/virtio-blk.c
@@ -38,8 +38,46 @@
 #include "hw/virtio/virtio-blk-common.h"
 #include "qemu/coroutine.h"
 
+/*
+ * 没有iothread应该就是mainloop
+ * (gdb) bt
+ * #0  coroutine_pool_get_local () at ../util/qemu-coroutine.c:116
+ * #1  0x00005555561cb785 in coroutine_pool_get () at ../util/qemu-coroutine.c:181
+ * #2  0x00005555561cb8c7 in qemu_coroutine_create (entry=0x5555560034f4 <blk_aio_write_entry>, opaque=0x5555582fd4d0)
+ *     at ../util/qemu-coroutine.c:222
+ * #3  0x00005555560033cc in blk_aio_prwv (blk=0x55555846f060, offset=540709888, bytes=1024, iobuf=0x5555577f63e8,
+ *     co_entry=0x5555560034f4 <blk_aio_write_entry>, flags=BDRV_REQ_REGISTERED_BUF,
+ *     cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350) at ../block/block-backend.c:1600
+ * #4  0x0000555556003927 in blk_aio_pwritev (blk=0x55555846f060, offset=540709888, qiov=0x5555577f63e8,
+ *     flags=BDRV_REQ_REGISTERED_BUF, cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350)
+ *     at ../block/block-backend.c:1722
+ * #5  0x0000555555e2c003 in submit_requests (s=0x5555584688c0, mrb=0x7fffffffd640, start=0, num_reqs=1, niov=-1) at ../hw/block/virtio-blk.c:259
+ * #6  0x0000555555e2c12f in virtio_blk_submit_multireq (s=0x5555584688c0, mrb=0x7fffffffd640) at ../hw/block/virtio-blk.c:294
+ * #7  0x0000555555e2e065 in virtio_blk_handle_vq (s=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1008
+ * #8  0x0000555555e2e0f8 in virtio_blk_handle_output (vdev=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1028
+ * #9  0x0000555555e89d02 in virtio_queue_notify_vq (vq=0x5555584b0960) at ../hw/virtio/virtio.c:2484
+ * #10 0x0000555555e8d53c in virtio_queue_host_notifier_read (n=0x5555584b09d4) at ../hw/virtio/virtio.c:3869
+ * #11 0x00005555561a74d8 in aio_dispatch_handler (ctx=0x55555745ee50, node=0x7fffe43dfba0) at ../util/aio-posix.c:376
+ * #12 0x00005555561a76ed in aio_dispatch_handlers (ctx=0x55555745ee50) at ../util/aio-posix.c:427
+ * #13 0x00005555561a7749 in aio_dispatch (ctx=0x55555745ee50) at ../util/aio-posix.c:437
+ * #14 0x00005555561c9148 in aio_ctx_dispatch (source=0x55555745ee50, callback=0x0, user_data=0x0) at ../util/async.c:361
+ * #15 0x00007ffff6fd394b in g_main_dispatch (context=0x55555745f260) at ../glib/gmain.c:3325
+ * #16 g_main_context_dispatch (context=0x55555745f260) at ../glib/gmain.c:4043
+ * #17 0x00005555561ca939 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #18 0x00005555561ca9c7 in os_host_main_loop_wait (timeout=657614) at ../util/main-loop.c:310
+ * #19 0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #20 0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #21 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #22 0x00005555560d296f in main (argc=25, argv=0x7fffffffdb28) at ../system/main.c:80
+ */
+
 static void virtio_blk_ioeventfd_attach(VirtIOBlock *s);
 
+/*
+ * 在以下使用virtio_blk_init_request():
+ *   - hw/block/virtio-blk.c|181| <<virtio_blk_get_request>> virtio_blk_init_request(s, vq, req);
+ *   - hw/block/virtio-blk.c|1395| <<virtio_blk_load_device>> virtio_blk_init_request(s, virtio_get_queue(vdev, vq_idx), req);
+ */
 static void virtio_blk_init_request(VirtIOBlock *s, VirtQueue *vq,
                                     VirtIOBlockReq *req)
 {
@@ -48,9 +86,36 @@ static void virtio_blk_init_request(VirtIOBlock *s, VirtQueue *vq,
     req->qiov.size = 0;
     req->in_len = 0;
     req->next = NULL;
+    /*
+     * 在以下使用VirtIOBlockReq->mr_next:
+     *   - hw/block/virtio-blk.c|51| <<virtio_blk_init_request>> req->mr_next = NULL;
+     *   - hw/block/virtio-blk.c|81| <<virtio_blk_handle_rw_error>> req->mr_next = NULL;
+     *   - hw/block/virtio-blk.c|112| <<virtio_blk_rw_complete>> next = req->mr_next;
+     *   - hw/block/virtio-blk.c|253| <<submit_requests>> mrb->reqs[i - 1]->mr_next = mrb->reqs[i];
+     */
     req->mr_next = NULL;
 }
 
+/*
+ * 在以下使用virtio_blk_req_complete():
+ *   - hw/block/virtio-blk.c|88| <<virtio_blk_handle_rw_error>> virtio_blk_req_complete(req, VIRTIO_BLK_S_IOERR);
+ *   - hw/block/virtio-blk.c|138| <<virtio_blk_rw_complete>> virtio_blk_req_complete(req, VIRTIO_BLK_S_OK);
+ *   - hw/block/virtio-blk.c|153| <<virtio_blk_flush_complete>> virtio_blk_req_complete(req, VIRTIO_BLK_S_OK);
+ *   - hw/block/virtio-blk.c|169| <<virtio_blk_discard_write_zeroes_complete>> virtio_blk_req_complete(req, VIRTIO_BLK_S_OK);
+ *   - hw/block/virtio-blk.c|217| <<virtio_blk_handle_scsi>> virtio_blk_req_complete(req, status);
+ *   - hw/block/virtio-blk.c|645| <<virtio_blk_zone_report_complete>> virtio_blk_req_complete(req, err_status);
+ *   - hw/block/virtio-blk.c|694| <<virtio_blk_handle_zone_report>> virtio_blk_req_complete(req, err_status);
+ *   - hw/block/virtio-blk.c|710| <<virtio_blk_zone_mgmt_complete>> virtio_blk_req_complete(req, err_status);
+ *   - hw/block/virtio-blk.c|752| <<virtio_blk_handle_zone_mgmt>> virtio_blk_req_complete(req, err_status);
+ *   - hw/block/virtio-blk.c|783| <<virtio_blk_zone_append_complete>> virtio_blk_req_complete(req, err_status);
+ *   - hw/block/virtio-blk.c|821| <<virtio_blk_handle_zone_append>> virtio_blk_req_complete(req, err_status);
+ *   - hw/block/virtio-blk.c|891| <<virtio_blk_handle_request>> virtio_blk_req_complete(req, VIRTIO_BLK_S_IOERR);
+ *   - hw/block/virtio-blk.c|949| <<virtio_blk_handle_request>> virtio_blk_req_complete(req, VIRTIO_BLK_S_OK);
+ *   - hw/block/virtio-blk.c|981| <<virtio_blk_handle_request>> virtio_blk_req_complete(req, VIRTIO_BLK_S_UNSUPP);
+ *   - hw/block/virtio-blk.c|998| <<virtio_blk_handle_request>> virtio_blk_req_complete(req, err_status);
+ *   - hw/block/virtio-blk.c|1013| <<virtio_blk_handle_request>> virtio_blk_req_complete(req, VIRTIO_BLK_S_UNSUPP);
+ *   - hw/vmapple/virtio-blk.c|57| <<vmapple_virtio_blk_handle_unknown_request>> virtio_blk_req_complete(req, VIRTIO_BLK_S_OK);
+ */
 void virtio_blk_req_complete(VirtIOBlockReq *req, unsigned char status)
 {
     VirtIOBlock *s = req->dev;
@@ -58,8 +123,33 @@ void virtio_blk_req_complete(VirtIOBlockReq *req, unsigned char status)
 
     trace_virtio_blk_req_complete(vdev, req, status);
 
+    /*
+     * 根据Linux kernel的注释:
+     * The status byte is always the last byte of the virtblk request
+     * in-header. This helper fetches its value for all in-header formats
+     * that are currently defined.
+     *
+     * VirtIOBlockReq *req:
+     * -> struct virtio_blk_inhdr *in;
+     * -> struct virtio_blk_outhdr out;
+     */
     stb_p(&req->in->status, status);
+    /*
+     * 在以下使用VirtIOBlockReq->inhdr_undo:
+     *   - hw/block/virtio-blk.c|62| <<virtio_blk_req_complete>> iov_discard_undo(&req->inhdr_undo);
+     *   - hw/block/virtio-blk.c|867| <<virtio_blk_handle_request>> iov_discard_back_undoable(in_iov,
+     *                          &in_num, sizeof(struct virtio_blk_inhdr), &req->inhdr_undo);
+     *   - hw/block/virtio-blk.c|988| <<virtio_blk_handle_request>> iov_discard_undo(&req->inhdr_undo);
+     */
     iov_discard_undo(&req->inhdr_undo);
+    /*
+     * 在以下使用VirtIOBlockReq->outhdr_undo:
+     *   - hw/block/virtio-blk.c|112| <<virtio_blk_req_complete>> iov_discard_undo(&req->outhdr_undo);
+     *   - hw/block/virtio-blk.c|964| <<virtio_blk_handle_request>> iov_discard_front_undoable(&out_iov,
+     *                           &out_num, sizeof(req->out), &req->outhdr_undo);
+     *   - hw/block/virtio-blk.c|968| <<virtio_blk_handle_request>> iov_discard_undo(&req->outhdr_undo);
+     *   - hw/block/virtio-blk.c|1100| <<virtio_blk_handle_request>> iov_discard_undo(&req->outhdr_undo);
+     */
     iov_discard_undo(&req->outhdr_undo);
     virtqueue_push(req->vq, &req->elem, req->in_len);
     if (qemu_in_iothread()) {
@@ -69,6 +159,12 @@ void virtio_blk_req_complete(VirtIOBlockReq *req, unsigned char status)
     }
 }
 
+/*
+ * 在以下使用virtio_blk_handle_rw_error():
+ *   - hw/block/virtio-blk.c|133| <<virtio_blk_rw_complete>> if (virtio_blk_handle_rw_error(req, -ret, is_read, true)) {
+ *   - hw/block/virtio-blk.c|149| <<virtio_blk_flush_complete>> if (ret && virtio_blk_handle_rw_error(req, -ret, 0, true)) {
+ *   - hw/block/virtio-blk.c|165| <<virtio_blk_discard_write_zeroes_complete>> if (ret && virtio_blk_handle_rw_error(req, -ret, false, is_write_zeroes)) {
+ */
 static int virtio_blk_handle_rw_error(VirtIOBlockReq *req, int error,
     bool is_read, bool acct_failed)
 {
@@ -76,6 +172,13 @@ static int virtio_blk_handle_rw_error(VirtIOBlockReq *req, int error,
     BlockErrorAction action = blk_get_error_action(s->blk, is_read, error);
 
     if (action == BLOCK_ERROR_ACTION_STOP) {
+        /*
+	 * 在以下使用VirtIOBlockReq->mr_next:
+	 *   - hw/block/virtio-blk.c|51| <<virtio_blk_init_request>> req->mr_next = NULL;
+	 *   - hw/block/virtio-blk.c|81| <<virtio_blk_handle_rw_error>> req->mr_next = NULL;
+	 *   - hw/block/virtio-blk.c|112| <<virtio_blk_rw_complete>> next = req->mr_next;
+	 *   - hw/block/virtio-blk.c|253| <<submit_requests>> mrb->reqs[i - 1]->mr_next = mrb->reqs[i];
+	 */
         /* Break the link as the next request is going to be parsed from the
          * ring again. Otherwise we may end up doing a double completion! */
         req->mr_next = NULL;
@@ -96,6 +199,13 @@ static int virtio_blk_handle_rw_error(VirtIOBlockReq *req, int error,
     return action != BLOCK_ERROR_ACTION_IGNORE;
 }
 
+/*
+ * 在以下使用virtio_blk_rw_complete():
+ *   - hw/block/virtio-blk.c|266| <<submit_requests>> blk_aio_pwritev(blk, sector_num << BDRV_SECTOR_BITS, qiov,
+ *        flags, virtio_blk_rw_complete, mrb->reqs[start]);
+ *   - hw/block/virtio-blk.c|270| <<submit_requests>> blk_aio_preadv(blk, sector_num << BDRV_SECTOR_BITS, qiov,
+ *        flags, virtio_blk_rw_complete, mrb->reqs[start]);
+ */
 static void virtio_blk_rw_complete(void *opaque, int ret)
 {
     VirtIOBlockReq *next = opaque;
@@ -104,9 +214,20 @@ static void virtio_blk_rw_complete(void *opaque, int ret)
 
     while (next) {
         VirtIOBlockReq *req = next;
+        /*
+	 * 在以下使用VirtIOBlockReq->mr_next:
+	 *   - hw/block/virtio-blk.c|51| <<virtio_blk_init_request>> req->mr_next = NULL;
+	 *   - hw/block/virtio-blk.c|81| <<virtio_blk_handle_rw_error>> req->mr_next = NULL;
+	 *   - hw/block/virtio-blk.c|112| <<virtio_blk_rw_complete>> next = req->mr_next;
+	 *   - hw/block/virtio-blk.c|253| <<submit_requests>> mrb->reqs[i - 1]->mr_next = mrb->reqs[i];
+	 */
         next = req->mr_next;
         trace_virtio_blk_rw_complete(vdev, req, ret);
 
+	/*
+	 * VirtIOBlockReq *req:
+	 * -> QEMUIOVector qiov;
+	 */
         if (req->qiov.nalloc != -1) {
             /* If nalloc is != -1 req->qiov is a local copy of the original
              * external iovec. It was allocated in submit_requests to be
@@ -125,6 +246,13 @@ static void virtio_blk_rw_complete(void *opaque, int ret)
              * the memory until the request is completed (which will
              * happen on the other side of the migration).
              */
+            /*
+	     * 在以下使用virtio_blk_handle_rw_error():
+	     *   - hw/block/virtio-blk.c|133| <<virtio_blk_rw_complete>> if (virtio_blk_handle_rw_error(req, -ret, is_read, true)) {
+	     *   - hw/block/virtio-blk.c|149| <<virtio_blk_flush_complete>> if (ret && virtio_blk_handle_rw_error(req, -ret, 0, true)) {
+	     *   - hw/block/virtio-blk.c|165| <<virtio_blk_discard_write_zeroes_complete>> if (ret && virtio_blk_handle_rw_error(req,
+	     *        -ret, false, is_write_zeroes)) {
+	     */
             if (virtio_blk_handle_rw_error(req, -ret, is_read, true)) {
                 continue;
             }
@@ -141,6 +269,13 @@ static void virtio_blk_flush_complete(void *opaque, int ret)
     VirtIOBlockReq *req = opaque;
     VirtIOBlock *s = req->dev;
 
+    /*
+     * 在以下使用virtio_blk_handle_rw_error():
+     *   - hw/block/virtio-blk.c|133| <<virtio_blk_rw_complete>> if (virtio_blk_handle_rw_error(req, -ret, is_read, true)) {
+     *   - hw/block/virtio-blk.c|149| <<virtio_blk_flush_complete>> if (ret && virtio_blk_handle_rw_error(req, -ret, 0, true)) {
+     *   - hw/block/virtio-blk.c|165| <<virtio_blk_discard_write_zeroes_complete>> if (ret && virtio_blk_handle_rw_error(req,
+     *        -ret, false, is_write_zeroes)) {
+     */
     if (ret && virtio_blk_handle_rw_error(req, -ret, 0, true)) {
         return;
     }
@@ -157,6 +292,13 @@ static void virtio_blk_discard_write_zeroes_complete(void *opaque, int ret)
     bool is_write_zeroes = (virtio_ldl_p(VIRTIO_DEVICE(s), &req->out.type) &
                             ~VIRTIO_BLK_T_BARRIER) == VIRTIO_BLK_T_WRITE_ZEROES;
 
+    /*
+     * 在以下使用virtio_blk_handle_rw_error():
+     *   - hw/block/virtio-blk.c|133| <<virtio_blk_rw_complete>> if (virtio_blk_handle_rw_error(req, -ret, is_read, true)) {
+     *   - hw/block/virtio-blk.c|149| <<virtio_blk_flush_complete>> if (ret && virtio_blk_handle_rw_error(req, -ret, 0, true)) {
+     *   - hw/block/virtio-blk.c|165| <<virtio_blk_discard_write_zeroes_complete>> if (ret && virtio_blk_handle_rw_error(req,
+     *        -ret, false, is_write_zeroes)) {
+     */
     if (ret && virtio_blk_handle_rw_error(req, -ret, false, is_write_zeroes)) {
         return;
     }
@@ -168,8 +310,16 @@ static void virtio_blk_discard_write_zeroes_complete(void *opaque, int ret)
     g_free(req);
 }
 
+/*
+ * 在以下使用virtio_blk_get_request():
+ *   - hw/block/virtio-blk.c|1034| <<virtio_blk_handle_vq>> while ((req = virtio_blk_get_request(s, vq))) {
+ */
 static VirtIOBlockReq *virtio_blk_get_request(VirtIOBlock *s, VirtQueue *vq)
 {
+    /*
+     * 89 typedef struct VirtIOBlockReq {
+     * 90     VirtQueueElement elem;
+     */
     VirtIOBlockReq *req = virtqueue_pop(vq, sizeof(VirtIOBlockReq));
 
     if (req) {
@@ -213,6 +363,43 @@ fail:
     g_free(req);
 }
 
+/*
+ * 没有iothread应该就是mainloop
+ * (gdb) bt
+ * #0  coroutine_pool_get_local () at ../util/qemu-coroutine.c:116
+ * #1  0x00005555561cb785 in coroutine_pool_get () at ../util/qemu-coroutine.c:181
+ * #2  0x00005555561cb8c7 in qemu_coroutine_create (entry=0x5555560034f4 <blk_aio_write_entry>, opaque=0x5555582fd4d0)
+ *     at ../util/qemu-coroutine.c:222
+ * #3  0x00005555560033cc in blk_aio_prwv (blk=0x55555846f060, offset=540709888, bytes=1024, iobuf=0x5555577f63e8,
+ *     co_entry=0x5555560034f4 <blk_aio_write_entry>, flags=BDRV_REQ_REGISTERED_BUF, 
+ *     cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350) at ../block/block-backend.c:1600
+ * #4  0x0000555556003927 in blk_aio_pwritev (blk=0x55555846f060, offset=540709888, qiov=0x5555577f63e8,
+ *     flags=BDRV_REQ_REGISTERED_BUF, cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350)
+ *     at ../block/block-backend.c:1722
+ * #5  0x0000555555e2c003 in submit_requests (s=0x5555584688c0, mrb=0x7fffffffd640, start=0, num_reqs=1, niov=-1) at ../hw/block/virtio-blk.c:259
+ * #6  0x0000555555e2c12f in virtio_blk_submit_multireq (s=0x5555584688c0, mrb=0x7fffffffd640) at ../hw/block/virtio-blk.c:294
+ * #7  0x0000555555e2e065 in virtio_blk_handle_vq (s=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1008
+ * #8  0x0000555555e2e0f8 in virtio_blk_handle_output (vdev=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1028
+ * #9  0x0000555555e89d02 in virtio_queue_notify_vq (vq=0x5555584b0960) at ../hw/virtio/virtio.c:2484
+ * #10 0x0000555555e8d53c in virtio_queue_host_notifier_read (n=0x5555584b09d4) at ../hw/virtio/virtio.c:3869
+ * #11 0x00005555561a74d8 in aio_dispatch_handler (ctx=0x55555745ee50, node=0x7fffe43dfba0) at ../util/aio-posix.c:376
+ * #12 0x00005555561a76ed in aio_dispatch_handlers (ctx=0x55555745ee50) at ../util/aio-posix.c:427
+ * #13 0x00005555561a7749 in aio_dispatch (ctx=0x55555745ee50) at ../util/aio-posix.c:437
+ * #14 0x00005555561c9148 in aio_ctx_dispatch (source=0x55555745ee50, callback=0x0, user_data=0x0) at ../util/async.c:361
+ * #15 0x00007ffff6fd394b in g_main_dispatch (context=0x55555745f260) at ../glib/gmain.c:3325
+ * #16 g_main_context_dispatch (context=0x55555745f260) at ../glib/gmain.c:4043
+ * #17 0x00005555561ca939 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #18 0x00005555561ca9c7 in os_host_main_loop_wait (timeout=657614) at ../util/main-loop.c:310
+ * #19 0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #20 0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #21 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #22 0x00005555560d296f in main (argc=25, argv=0x7fffffffdb28) at ../system/main.c:80
+ *
+ * called by:
+ *   - hw/block/virtio-blk.c|294| <<virtio_blk_submit_multireq>> submit_requests(s, mrb, 0, 1, -1);
+ *   - hw/block/virtio-blk.c|318| <<virtio_blk_submit_multireq>> submit_requests(s, mrb, start, num_reqs, niov);
+ *   - hw/block/virtio-blk.c|334| <<virtio_blk_submit_multireq>> submit_requests(s, mrb, start, num_reqs, niov);
+ */
 static inline void submit_requests(VirtIOBlock *s, MultiReqBuffer *mrb,
                                    int start, int num_reqs, int niov)
 {
@@ -239,6 +426,13 @@ static inline void submit_requests(VirtIOBlock *s, MultiReqBuffer *mrb,
         for (i = start + 1; i < start + num_reqs; i++) {
             qemu_iovec_concat(qiov, &mrb->reqs[i]->qiov, 0,
                               mrb->reqs[i]->qiov.size);
+	    /*
+	     * 在以下使用VirtIOBlockReq->mr_next:
+	     *   - hw/block/virtio-blk.c|51| <<virtio_blk_init_request>> req->mr_next = NULL;
+	     *   - hw/block/virtio-blk.c|81| <<virtio_blk_handle_rw_error>> req->mr_next = NULL;
+	     *   - hw/block/virtio-blk.c|112| <<virtio_blk_rw_complete>> next = req->mr_next;
+	     *   - hw/block/virtio-blk.c|253| <<submit_requests>> mrb->reqs[i - 1]->mr_next = mrb->reqs[i];
+	     */
             mrb->reqs[i - 1]->mr_next = mrb->reqs[i];
         }
 
@@ -284,6 +478,13 @@ static int multireq_compare(const void *a, const void *b)
     }
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|349| <<virtio_blk_handle_flush>> virtio_blk_submit_multireq(s, mrb);
+ *   - hw/block/virtio-blk.c|866| <<virtio_blk_handle_request>> virtio_blk_submit_multireq(s, mrb);
+ *   - hw/block/virtio-blk.c|1008| <<virtio_blk_handle_vq>> virtio_blk_submit_multireq(s, &mrb);
+ *   - hw/block/virtio-blk.c|1056| <<virtio_blk_dma_restart_bh>> virtio_blk_submit_multireq(s, &mrb);
+ */
 static void virtio_blk_submit_multireq(VirtIOBlock *s, MultiReqBuffer *mrb)
 {
     int i = 0, start = 0, num_reqs = 0, niov = 0, nb_sectors = 0;
@@ -302,6 +503,23 @@ static void virtio_blk_submit_multireq(VirtIOBlock *s, MultiReqBuffer *mrb)
           &multireq_compare);
 
     for (i = 0; i < mrb->num_reqs; i++) {
+        /*
+	 * typedef struct VirtIOBlockReq {
+	 *     VirtQueueElement elem;
+	 *     int64_t sector_num;
+	 *     VirtIOBlock *dev;
+	 *     VirtQueue *vq;
+	 *     IOVDiscardUndo inhdr_undo;
+	 *     IOVDiscardUndo outhdr_undo;
+	 *     struct virtio_blk_inhdr *in;
+	 *     struct virtio_blk_outhdr out;
+	 *     QEMUIOVector qiov;
+	 *     size_t in_len;
+	 *     struct VirtIOBlockReq *next;
+	 *     struct VirtIOBlockReq *mr_next;
+	 *     BlockAcctCookie acct;
+	 * } VirtIOBlockReq;
+	 */
         VirtIOBlockReq *req = mrb->reqs[i];
         if (num_reqs > 0) {
             /*
@@ -348,9 +566,28 @@ static void virtio_blk_handle_flush(VirtIOBlockReq *req, MultiReqBuffer *mrb)
     if (mrb->is_write && mrb->num_reqs > 0) {
         virtio_blk_submit_multireq(s, mrb);
     }
+    /*
+     * 在以下使用blk_aio_flush():
+     *   - hw/block/dataplane/xen-block.c|367| <<xen_block_do_aio>> blk_aio_flush(request->dataplane->blk, xen_block_complete_aio,
+     *   - hw/block/virtio-blk.c|497| <<virtio_blk_handle_flush>> blk_aio_flush(s->blk, virtio_blk_flush_complete, req);
+     *   - hw/ide/core.c|1135| <<ide_flush_cache>> s->pio_aiocb = blk_aio_flush(s->blk, ide_flush_cb, s);
+     *   - hw/nvme/ctrl.c|3524| <<nvme_flush_ns_cb>> iocb->aiocb = blk_aio_flush(ns->blkconf.blk, nvme_flush_ns_cb, iocb);
+     *   - hw/scsi/scsi-disk.c|402| <<scsi_write_do_fua>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+     *   - hw/scsi/scsi-disk.c|570| <<scsi_read_data>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_do_read_cb, r);
+     *   - hw/scsi/scsi-disk.c|1725| <<scsi_disk_emulate_mode_select>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+     *   - hw/scsi/scsi-disk.c|2252| <<scsi_disk_emulate_command>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+     *   - qemu-img.c|4471| <<bench_cb>> acb = blk_aio_flush(b->blk, cb, b);
+     *   - tests/unit/test-block-backend.c|46| <<test_drain_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+     *   - tests/unit/test-block-backend.c|63| <<test_drain_all_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+     */
     blk_aio_flush(s->blk, virtio_blk_flush_complete, req);
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|550| <<virtio_blk_handle_discard_write_zeroes>> if (unlikely(!virtio_blk_sect_range_ok(s, sector, bytes))) {
+ *   - hw/block/virtio-blk.c|1011| <<virtio_blk_handle_request>> if (!virtio_blk_sect_range_ok(s, req->sector_num, req->qiov.size)) {
+ */
 static bool virtio_blk_sect_range_ok(VirtIOBlock *dev,
                                      uint64_t sector, size_t size)
 {
@@ -788,9 +1025,20 @@ out:
     return err_status;
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|995| <<virtio_blk_handle_vq>> if (virtio_blk_handle_request(req, &mrb)) {
+ *   - hw/block/virtio-blk.c|1040| <<virtio_blk_dma_restart_bh>> if (virtio_blk_handle_request(req, &mrb)) {
+ */
 static int virtio_blk_handle_request(VirtIOBlockReq *req, MultiReqBuffer *mrb)
 {
     uint32_t type;
+    /*
+     * VirtIOBlockReq *req:
+     * -> VirtQueueElement elem;
+     * -> struct virtio_blk_inhdr *in;
+     * -> struct virtio_blk_outhdr out;
+     */
     struct iovec *in_iov = req->elem.in_sg;
     struct iovec *out_iov = req->elem.out_sg;
     unsigned in_num = req->elem.in_num;
@@ -809,11 +1057,30 @@ static int virtio_blk_handle_request(VirtIOBlockReq *req, MultiReqBuffer *mrb)
         return -1;
     }
 
+    /*
+     * 在以下使用VirtIOBlockReq->outhdr_undo:
+     *   - hw/block/virtio-blk.c|112| <<virtio_blk_req_complete>> iov_discard_undo(&req->outhdr_undo);
+     *   - hw/block/virtio-blk.c|964| <<virtio_blk_handle_request>> iov_discard_front_undoable(&out_iov,
+     *                           &out_num, sizeof(req->out), &req->outhdr_undo);
+     *   - hw/block/virtio-blk.c|968| <<virtio_blk_handle_request>> iov_discard_undo(&req->outhdr_undo);
+     *   - hw/block/virtio-blk.c|1100| <<virtio_blk_handle_request>> iov_discard_undo(&req->outhdr_undo);
+     *
+     * 似乎是从out_iov中把header的部分去掉?
+     * 更新了out_iov
+     */
     iov_discard_front_undoable(&out_iov, &out_num, sizeof(req->out),
                                &req->outhdr_undo);
 
     if (in_iov[in_num - 1].iov_len < sizeof(struct virtio_blk_inhdr)) {
         virtio_error(vdev, "virtio-blk request inhdr too short");
+        /*
+	 * 在以下使用VirtIOBlockReq->outhdr_undo:
+	 *   - hw/block/virtio-blk.c|112| <<virtio_blk_req_complete>> iov_discard_undo(&req->outhdr_undo);
+	 *   - hw/block/virtio-blk.c|964| <<virtio_blk_handle_request>> iov_discard_front_undoable(&out_iov,
+	 *                           &out_num, sizeof(req->out), &req->outhdr_undo);
+	 *   - hw/block/virtio-blk.c|968| <<virtio_blk_handle_request>> iov_discard_undo(&req->outhdr_undo);
+	 *   - hw/block/virtio-blk.c|1100| <<virtio_blk_handle_request>> iov_discard_undo(&req->outhdr_undo);
+	 */
         iov_discard_undo(&req->outhdr_undo);
         return -1;
     }
@@ -826,6 +1093,12 @@ static int virtio_blk_handle_request(VirtIOBlockReq *req, MultiReqBuffer *mrb)
     iov_discard_back_undoable(in_iov, &in_num, sizeof(struct virtio_blk_inhdr),
                               &req->inhdr_undo);
 
+    /*
+     * VirtIOBlockReq *req:
+     * -> VirtQueueElement elem;
+     * -> struct virtio_blk_inhdr *in;
+     * -> struct virtio_blk_outhdr out;
+     */
     type = virtio_ldl_p(vdev, &req->out.type);
 
     /* VIRTIO_BLK_T_OUT defines the command direction. VIRTIO_BLK_T_BARRIER
@@ -837,6 +1110,10 @@ static int virtio_blk_handle_request(VirtIOBlockReq *req, MultiReqBuffer *mrb)
         bool is_write = type & VIRTIO_BLK_T_OUT;
         req->sector_num = virtio_ldq_p(vdev, &req->out.sector);
 
+        /*
+	 * VirtIOBlockReq *req:
+	 * -> QEMUIOVector qiov;
+	 */
         if (is_write) {
             qemu_iovec_init_external(&req->qiov, out_iov, out_num);
             trace_virtio_blk_handle_write(vdev, req, req->sector_num,
@@ -858,14 +1135,33 @@ static int virtio_blk_handle_request(VirtIOBlockReq *req, MultiReqBuffer *mrb)
         block_acct_start(blk_get_stats(s->blk), &req->acct, req->qiov.size,
                          is_write ? BLOCK_ACCT_WRITE : BLOCK_ACCT_READ);
 
+        /*
+	 * 在以下使用VirtIOBlkConf->request_merging:
+	 *   - hw/block/virtio-blk.c|2054| <<global>> DEFINE_PROP_BIT("request-merging", VirtIOBlock, conf.request_merging, 0,
+	 *   - hw/block/virtio-blk.c|1026| <<virtio_blk_handle_request>> if (...!s->conf.request_merging)) {
+	 */
         /* merge would exceed maximum number of requests or IO direction
          * changes */
         if (mrb->num_reqs > 0 && (mrb->num_reqs == VIRTIO_BLK_MAX_MERGE_REQS ||
                                   is_write != mrb->is_write ||
                                   !s->conf.request_merging)) {
+            /*
+	     * called by:
+	     *   - hw/block/virtio-blk.c|349| <<virtio_blk_handle_flush>> virtio_blk_submit_multireq(s, mrb);
+	     *   - hw/block/virtio-blk.c|866| <<virtio_blk_handle_request>> virtio_blk_submit_multireq(s, mrb);
+	     *   - hw/block/virtio-blk.c|1008| <<virtio_blk_handle_vq>> virtio_blk_submit_multireq(s, &mrb);
+	     *   - hw/block/virtio-blk.c|1056| <<virtio_blk_dma_restart_bh>> virtio_blk_submit_multireq(s, &mrb);
+	     */
             virtio_blk_submit_multireq(s, mrb);
         }
 
+        /*
+	 * typedef struct MultiReqBuffer {
+	 *     VirtIOBlockReq *reqs[VIRTIO_BLK_MAX_MERGE_REQS];
+	 *     unsigned int num_reqs;
+	 *     bool is_write;
+	 * } MultiReqBuffer;
+	 */
         assert(mrb->num_reqs < VIRTIO_BLK_MAX_MERGE_REQS);
         mrb->reqs[mrb->num_reqs++] = req;
         mrb->is_write = is_write;
@@ -946,6 +1242,14 @@ static int virtio_blk_handle_request(VirtIOBlockReq *req, MultiReqBuffer *mrb)
         if (unlikely(iov_to_buf(out_iov, out_num, 0, &dwz_hdr,
                                 sizeof(dwz_hdr)) != sizeof(dwz_hdr))) {
             iov_discard_undo(&req->inhdr_undo);
+	    /*
+	     * 在以下使用VirtIOBlockReq->outhdr_undo:
+	     *   - hw/block/virtio-blk.c|112| <<virtio_blk_req_complete>> iov_discard_undo(&req->outhdr_undo);
+	     *   - hw/block/virtio-blk.c|964| <<virtio_blk_handle_request>> iov_discard_front_undoable(&out_iov,
+	     *                           &out_num, sizeof(req->out), &req->outhdr_undo);
+	     *   - hw/block/virtio-blk.c|968| <<virtio_blk_handle_request>> iov_discard_undo(&req->outhdr_undo);
+	     *   - hw/block/virtio-blk.c|1100| <<virtio_blk_handle_request>> iov_discard_undo(&req->outhdr_undo);
+	     */
             iov_discard_undo(&req->outhdr_undo);
             virtio_error(vdev, "virtio-blk discard/write_zeroes header"
                          " too short");
@@ -968,6 +1272,10 @@ static int virtio_blk_handle_request(VirtIOBlockReq *req, MultiReqBuffer *mrb)
          * class lookup is not in the hot path.
          */
         VirtIOBlkClass *vbk = VIRTIO_BLK_GET_CLASS(s);
+        /*
+	 * 只有hw/vmapple/virtio-blk.c实现了:
+	 * vbk->handle_unknown_request = vmapple_virtio_blk_handle_unknown_request;
+	 */
         if (!vbk->handle_unknown_request ||
             !vbk->handle_unknown_request(req, mrb, type)) {
             virtio_blk_req_complete(req, VIRTIO_BLK_S_UNSUPP);
@@ -978,6 +1286,41 @@ static int virtio_blk_handle_request(VirtIOBlockReq *req, MultiReqBuffer *mrb)
     return 0;
 }
 
+/*
+ * 没有iothread应该就是mainloop
+ * (gdb) bt
+ * #0  coroutine_pool_get_local () at ../util/qemu-coroutine.c:116
+ * #1  0x00005555561cb785 in coroutine_pool_get () at ../util/qemu-coroutine.c:181
+ * #2  0x00005555561cb8c7 in qemu_coroutine_create (entry=0x5555560034f4 <blk_aio_write_entry>, opaque=0x5555582fd4d0)
+ *     at ../util/qemu-coroutine.c:222
+ * #3  0x00005555560033cc in blk_aio_prwv (blk=0x55555846f060, offset=540709888, bytes=1024, iobuf=0x5555577f63e8,
+ *     co_entry=0x5555560034f4 <blk_aio_write_entry>, flags=BDRV_REQ_REGISTERED_BUF, 
+ *     cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350) at ../block/block-backend.c:1600
+ * #4  0x0000555556003927 in blk_aio_pwritev (blk=0x55555846f060, offset=540709888, qiov=0x5555577f63e8,
+ *     flags=BDRV_REQ_REGISTERED_BUF, cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350)
+ *     at ../block/block-backend.c:1722
+ * #5  0x0000555555e2c003 in submit_requests (s=0x5555584688c0, mrb=0x7fffffffd640, start=0, num_reqs=1, niov=-1) at ../hw/block/virtio-blk.c:259
+ * #6  0x0000555555e2c12f in virtio_blk_submit_multireq (s=0x5555584688c0, mrb=0x7fffffffd640) at ../hw/block/virtio-blk.c:294
+ * #7  0x0000555555e2e065 in virtio_blk_handle_vq (s=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1008
+ * #8  0x0000555555e2e0f8 in virtio_blk_handle_output (vdev=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1028
+ * #9  0x0000555555e89d02 in virtio_queue_notify_vq (vq=0x5555584b0960) at ../hw/virtio/virtio.c:2484
+ * #10 0x0000555555e8d53c in virtio_queue_host_notifier_read (n=0x5555584b09d4) at ../hw/virtio/virtio.c:3869
+ * #11 0x00005555561a74d8 in aio_dispatch_handler (ctx=0x55555745ee50, node=0x7fffe43dfba0) at ../util/aio-posix.c:376
+ * #12 0x00005555561a76ed in aio_dispatch_handlers (ctx=0x55555745ee50) at ../util/aio-posix.c:427
+ * #13 0x00005555561a7749 in aio_dispatch (ctx=0x55555745ee50) at ../util/aio-posix.c:437
+ * #14 0x00005555561c9148 in aio_ctx_dispatch (source=0x55555745ee50, callback=0x0, user_data=0x0) at ../util/async.c:361
+ * #15 0x00007ffff6fd394b in g_main_dispatch (context=0x55555745f260) at ../glib/gmain.c:3325
+ * #16 g_main_context_dispatch (context=0x55555745f260) at ../glib/gmain.c:4043
+ * #17 0x00005555561ca939 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #18 0x00005555561ca9c7 in os_host_main_loop_wait (timeout=657614) at ../util/main-loop.c:310
+ * #19 0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #20 0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #21 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #22 0x00005555560d296f in main (argc=25, argv=0x7fffffffdb28) at ../system/main.c:80
+ *
+ * 在以下使用virtio_blk_handle_vq():
+ *   - hw/block/virtio-blk.c|1068| <<virtio_blk_handle_output>> virtio_blk_handle_vq(s, vq);
+ */
 void virtio_blk_handle_vq(VirtIOBlock *s, VirtQueue *vq)
 {
     VirtIOBlockReq *req;
@@ -992,6 +1335,11 @@ void virtio_blk_handle_vq(VirtIOBlock *s, VirtQueue *vq)
         }
 
         while ((req = virtio_blk_get_request(s, vq))) {
+            /*
+	     * called by:
+	     *   - hw/block/virtio-blk.c|995| <<virtio_blk_handle_vq>> if (virtio_blk_handle_request(req, &mrb)) {
+	     *   - hw/block/virtio-blk.c|1040| <<virtio_blk_dma_restart_bh>> if (virtio_blk_handle_request(req, &mrb)) {
+	     */
             if (virtio_blk_handle_request(req, &mrb)) {
                 virtqueue_detach_element(req->vq, &req->elem, 0);
                 g_free(req);
@@ -1011,6 +1359,10 @@ void virtio_blk_handle_vq(VirtIOBlock *s, VirtQueue *vq)
     defer_call_end();
 }
 
+/*
+ * 在以下使用virtio_blk_handle_output():
+ *   - hw/block/virtio-blk.c|1826| <<virtio_blk_device_realize>> virtio_add_queue(vdev, conf->queue_size, virtio_blk_handle_output);
+ */
 static void virtio_blk_handle_output(VirtIODevice *vdev, VirtQueue *vq)
 {
     VirtIOBlock *s = (VirtIOBlock *)vdev;
@@ -1025,6 +1377,9 @@ static void virtio_blk_handle_output(VirtIODevice *vdev, VirtQueue *vq)
         }
     }
 
+    /*
+     * 只在此处调用
+     */
     virtio_blk_handle_vq(s, vq);
 }
 
@@ -1060,6 +1415,11 @@ static void virtio_blk_dma_restart_bh(void *opaque)
     blk_dec_in_flight(s->conf.conf.blk);
 }
 
+/*
+ * 在以下使用virtio_blk_dma_restart_cb():
+ *   - hw/block/virtio-blk.c|1805| <<virtio_blk_device_realize>>
+ *       s->change = qdev_add_vm_change_state_handler(dev, virtio_blk_dma_restart_cb, s);
+ */
 static void virtio_blk_dma_restart_cb(void *opaque, bool running,
                                       RunState state)
 {
@@ -1100,6 +1460,23 @@ static void virtio_blk_dma_restart_cb(void *opaque, bool running,
         /* Paired with dec in virtio_blk_dma_restart_bh() */
         blk_inc_in_flight(s->conf.conf.blk);
 
+        /*
+	 * 在以下使用VirtIOBlock->vq_aio_context (二维数组):
+	 *   - hw/block/virtio-blk.c|1368| <<virtio_blk_dma_restart_cb>> aio_bh_schedule_oneshot(s->vq_aio_context[i], virtio_blk_dma_restart_bh, vq_rq[i]);
+	 *   - hw/block/virtio-blk.c|1652| <<virtio_blk_ioeventfd_detach>> virtio_queue_aio_detach_host_notifier(vq, s->vq_aio_context[i]);
+	 *   - hw/block/virtio-blk.c|1662| <<virtio_blk_ioeventfd_attach>> virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+	 *   - hw/block/virtio-blk.c|1721| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = g_new(AioContext *, conf->num_queues);
+	 *   - hw/block/virtio-blk.c|1725| <<virtio_blk_vq_aio_context_init>> if (!iothread_vq_mapping_apply(conf->iothread_vq_mapping_list,
+	 *        s->vq_aio_context, conf->num_queues, errp)) {
+	 *   - hw/block/virtio-blk.c|1728| <<virtio_blk_vq_aio_context_init>> g_free(s->vq_aio_context);
+	 *   - hw/block/virtio-blk.c|1729| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = NULL;
+	 *   - hw/block/virtio-blk.c|1735| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx;
+	 *   - hw/block/virtio-blk.c|1743| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx;
+	 *   - hw/block/virtio-blk.c|1765| <<virtio_blk_vq_aio_context_cleanup>> g_free(s->vq_aio_context);
+	 *   - hw/block/virtio-blk.c|1766| <<virtio_blk_vq_aio_context_cleanup>> s->vq_aio_context = NULL;
+	 *   - hw/block/virtio-blk.c|1831| <<virtio_blk_start_ioeventfd>> r = blk_set_aio_context(s->conf.conf.blk, s->vq_aio_context[0], &local_err);
+	 *   - hw/block/virtio-blk.c|1910| <<virtio_blk_stop_ioeventfd>> AioContext *ctx = s->vq_aio_context[i];
+	 */
         aio_bh_schedule_oneshot(s->vq_aio_context[i],
                                 virtio_blk_dma_restart_bh,
                                 vq_rq[i]);
@@ -1388,12 +1765,28 @@ static void virtio_blk_ioeventfd_detach(VirtIOBlock *s)
     }
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|1682| <<virtio_blk_drained_end>> virtio_blk_ioeventfd_attach(s);
+ *   - hw/block/virtio-blk.c|1856| <<virtio_blk_start_ioeventfd>> virtio_blk_ioeventfd_attach(s);
+ */
 static void virtio_blk_ioeventfd_attach(VirtIOBlock *s)
 {
     VirtIODevice *vdev = VIRTIO_DEVICE(s);
 
     for (uint16_t i = 0; i < s->conf.num_queues; i++) {
         VirtQueue *vq = virtio_get_queue(vdev, i);
+        /*
+	 * 在以下使用virtio_queue_aio_attach_host_notifier():
+	 *   - hw/block/virtio-blk.c|1662| <<virtio_blk_ioeventfd_attach>>
+	 *        virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+	 *   - hw/scsi/virtio-scsi-dataplane.c|203| <<virtio_scsi_dataplane_start>>
+	 *        virtio_queue_aio_attach_host_notifier(vs->ctrl_vq, s->vq_aio_context[0]);
+	 *   - hw/scsi/virtio-scsi-dataplane.c|210| <<virtio_scsi_dataplane_start>>
+	 *        virtio_queue_aio_attach_host_notifier(vs->cmd_vqs[i], ctx);
+	 *   - hw/scsi/virtio-scsi.c|1250| <<virtio_scsi_drained_end>>
+	 *        virtio_queue_aio_attach_host_notifier(vq, ctx);
+	 */
         virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
     }
 }
@@ -1453,9 +1846,36 @@ static bool virtio_blk_vq_aio_context_init(VirtIOBlock *s, Error **errp)
         }
     }
 
+    /*
+     * 在以下使用VirtIOBlock->vq_aio_context (二维数组):
+     *   - hw/block/virtio-blk.c|1368| <<virtio_blk_dma_restart_cb>> aio_bh_schedule_oneshot(s->vq_aio_context[i], virtio_blk_dma_restart_bh, vq_rq[i]);
+     *   - hw/block/virtio-blk.c|1652| <<virtio_blk_ioeventfd_detach>> virtio_queue_aio_detach_host_notifier(vq, s->vq_aio_context[i]);
+     *   - hw/block/virtio-blk.c|1662| <<virtio_blk_ioeventfd_attach>> virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+     *   - hw/block/virtio-blk.c|1721| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = g_new(AioContext *, conf->num_queues);
+     *   - hw/block/virtio-blk.c|1725| <<virtio_blk_vq_aio_context_init>> if (!iothread_vq_mapping_apply(conf->iothread_vq_mapping_list,
+     *        s->vq_aio_context, conf->num_queues, errp)) {
+     *   - hw/block/virtio-blk.c|1728| <<virtio_blk_vq_aio_context_init>> g_free(s->vq_aio_context);
+     *   - hw/block/virtio-blk.c|1729| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = NULL;
+     *   - hw/block/virtio-blk.c|1735| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx;
+     *   - hw/block/virtio-blk.c|1743| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx;
+     *   - hw/block/virtio-blk.c|1765| <<virtio_blk_vq_aio_context_cleanup>> g_free(s->vq_aio_context);
+     *   - hw/block/virtio-blk.c|1766| <<virtio_blk_vq_aio_context_cleanup>> s->vq_aio_context = NULL;
+     *   - hw/block/virtio-blk.c|1831| <<virtio_blk_start_ioeventfd>> r = blk_set_aio_context(s->conf.conf.blk, s->vq_aio_context[0], &local_err);
+     *   - hw/block/virtio-blk.c|1910| <<virtio_blk_stop_ioeventfd>> AioContext *ctx = s->vq_aio_context[i];
+     */
     s->vq_aio_context = g_new(AioContext *, conf->num_queues);
 
     if (conf->iothread_vq_mapping_list) {
+        /*
+	 * 在以下调用iothread_vq_mapping_apply():
+	 *   - hw/block/virtio-blk.c|1740| <<virtio_blk_vq_aio_context_init>>
+	 *       if (!iothread_vq_mapping_apply(conf->iothread_vq_mapping_list,
+	 *                 s->vq_aio_context, conf->num_queues, errp)) {
+	 *   - hw/scsi/virtio-scsi-dataplane.c|68| <<virtio_scsi_dataplane_setup>>
+	 *       if (!iothread_vq_mapping_apply(vs->conf.iothread_vq_mapping_list,
+	 *                 &s->vq_aio_context[VIRTIO_SCSI_VQ_NUM_FIXED],
+	 *                 vs->conf.num_queues, errp)) {
+	 */
         if (!iothread_vq_mapping_apply(conf->iothread_vq_mapping_list,
                                        s->vq_aio_context,
                                        conf->num_queues,
@@ -1497,10 +1917,37 @@ static void virtio_blk_vq_aio_context_cleanup(VirtIOBlock *s)
         object_unref(OBJECT(conf->iothread));
     }
 
+    /*
+     * 在以下使用VirtIOBlock->vq_aio_context (二维数组):
+     *   - hw/block/virtio-blk.c|1368| <<virtio_blk_dma_restart_cb>> aio_bh_schedule_oneshot(s->vq_aio_context[i], virtio_blk_dma_restart_bh, vq_rq[i]);
+     *   - hw/block/virtio-blk.c|1652| <<virtio_blk_ioeventfd_detach>> virtio_queue_aio_detach_host_notifier(vq, s->vq_aio_context[i]);
+     *   - hw/block/virtio-blk.c|1662| <<virtio_blk_ioeventfd_attach>> virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+     *   - hw/block/virtio-blk.c|1721| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = g_new(AioContext *, conf->num_queues);
+     *   - hw/block/virtio-blk.c|1725| <<virtio_blk_vq_aio_context_init>> if (!iothread_vq_mapping_apply(conf->iothread_vq_mapping_list,
+     *        s->vq_aio_context, conf->num_queues, errp)) {
+     *   - hw/block/virtio-blk.c|1728| <<virtio_blk_vq_aio_context_init>> g_free(s->vq_aio_context);
+     *   - hw/block/virtio-blk.c|1729| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = NULL;
+     *   - hw/block/virtio-blk.c|1735| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx;
+     *   - hw/block/virtio-blk.c|1743| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx;
+     *   - hw/block/virtio-blk.c|1765| <<virtio_blk_vq_aio_context_cleanup>> g_free(s->vq_aio_context);
+     *   - hw/block/virtio-blk.c|1766| <<virtio_blk_vq_aio_context_cleanup>> s->vq_aio_context = NULL;
+     *   - hw/block/virtio-blk.c|1831| <<virtio_blk_start_ioeventfd>> r = blk_set_aio_context(s->conf.conf.blk, s->vq_aio_context[0], &local_err);
+     *   - hw/block/virtio-blk.c|1910| <<virtio_blk_stop_ioeventfd>> AioContext *ctx = s->vq_aio_context[i];
+     */
     g_free(s->vq_aio_context);
     s->vq_aio_context = NULL;
 }
 
+/*
+ * 在以下使用VirtioDeviceClass->start_ioeventfd:
+ *   - hw/block/virtio-blk.c|2178| <<virtio_blk_class_init>> vdc->start_ioeventfd = virtio_blk_start_ioeventfd;
+ *   - hw/scsi/virtio-scsi.c|1421| <<virtio_scsi_class_init>> vdc->start_ioeventfd = virtio_scsi_dataplane_start;
+ *   - hw/virtio/virtio-bus.c|236| <<virtio_bus_start_ioeventfd>> r = vdc->start_ioeventfd(vdev);
+ *   - hw/virtio/virtio.c|4153| <<virtio_device_class_init>> vdc->start_ioeventfd = virtio_device_start_ioeventfd_impl;
+ *
+ * 在以下使用virtio_blk_start_ioeventfd():
+ *   - hw/block/virtio-blk.c|2178| <<virtio_blk_class_init>> vdc->start_ioeventfd = virtio_blk_start_ioeventfd;
+ */
 /* Context: BQL held */
 static int virtio_blk_start_ioeventfd(VirtIODevice *vdev)
 {
@@ -1611,6 +2058,18 @@ static void virtio_blk_ioeventfd_stop_vq_bh(void *opaque)
 
     virtio_queue_aio_detach_host_notifier(vq, qemu_get_current_aio_context());
 
+    /*
+     * 在以下使用virtio_queue_host_notifier_read():
+     *   - hw/block/virtio-blk.c|2065| <<virtio_blk_ioeventfd_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+     *   - hw/scsi/virtio-scsi-dataplane.c|149| <<virtio_scsi_dataplane_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+     *   - hw/virtio/virtio-bus.c|328| <<virtio_bus_cleanup_host_notifier>> virtio_queue_host_notifier_read(notifier);
+     *   - hw/virtio/virtio.c|3829| <<virtio_queue_aio_attach_host_notifier>> aio_set_event_notifier(ctx, &vq->host_notifier,
+     *        virtio_queue_host_notifier_read, virtio_queue_host_notifier_aio_poll, virtio_queue_host_notifier_aio_poll_ready);
+     *   - hw/virtio/virtio.c|3858| <<virtio_queue_aio_attach_host_notifier_no_poll>> aio_set_event_notifier(ctx,
+     *        &vq->host_notifier, virtio_queue_host_notifier_read, NULL, NULL);
+     *   - hw/virtio/virtio.c|4077| <<virtio_device_start_ioeventfd_impl>> event_notifier_set_handler(&vq->host_notifier,
+     *        virtio_queue_host_notifier_read);
+     */
     /*
      * Test and clear notifier after disabling event, in case poll callback
      * didn't have time to run.
@@ -1767,6 +2226,13 @@ static void virtio_blk_device_realize(DeviceState *dev, Error **errp)
         return;
     }
 
+    /*
+     * 在以下使用virtio_blk_cfg_size_params:
+     *   - hw/block/vhost-user-blk.c|478| <<vhost_user_blk_device_realize>>
+     *        config_size = virtio_get_config_size(&virtio_blk_cfg_size_params,
+     *   - hw/block/virtio-blk.c|1815| <<virtio_blk_device_realize>>
+     *        s->config_size = virtio_get_config_size(&virtio_blk_cfg_size_params,
+     */
     s->config_size = virtio_get_config_size(&virtio_blk_cfg_size_params,
                                             s->host_features);
     virtio_init(vdev, VIRTIO_ID_BLOCK, s->config_size);
diff --git a/hw/char/serial.c b/hw/char/serial.c
index 70044e14a..472dd5e24 100644
--- a/hw/char/serial.c
+++ b/hw/char/serial.c
@@ -105,8 +105,22 @@
 static void serial_receive1(void *opaque, const uint8_t *buf, int size);
 static void serial_xmit(SerialState *s);
 
+/*
+ * called by:
+ *   - hw/char/serial.c|580| <<serial_receive_break>> recv_fifo_put(s, '\0');
+ *   - hw/char/serial.c|610| <<serial_receive1>> recv_fifo_put(s, buf[i]);
+ */
 static inline void recv_fifo_put(SerialState *s, uint8_t chr)
 {
+    /*
+     * typedef struct {
+     *     // All fields are private
+     *     uint8_t *data;
+     *     uint32_t capacity;
+     *     uint32_t head;
+     *     uint32_t num;
+     * } Fifo8;
+     */
     /* Receive overruns do not overwrite FIFO contents. */
     if (!fifo8_is_full(&s->recv_fifo)) {
         fifo8_push(&s->recv_fifo, chr);
@@ -115,6 +129,21 @@ static inline void recv_fifo_put(SerialState *s, uint8_t chr)
     }
 }
 
+/*
+ * called by:
+ *   - hw/char/serial.c|211| <<serial_update_msl>> serial_update_irq(s);
+ *   - hw/char/serial.c|251| <<serial_xmit>> serial_update_irq(s);
+ *   - hw/char/serial.c|357| <<serial_ioport_write>> serial_update_irq(s);
+ *   - hw/char/serial.c|403| <<serial_ioport_write>> serial_update_irq(s);
+ *   - hw/char/serial.c|429| <<serial_ioport_write>> serial_update_irq(s);
+ *   - hw/char/serial.c|494| <<serial_ioport_read>> serial_update_irq(s);
+ *   - hw/char/serial.c|512| <<serial_ioport_read>> serial_update_irq(s);
+ *   - hw/char/serial.c|526| <<serial_ioport_read>> serial_update_irq(s);
+ *   - hw/char/serial.c|543| <<serial_ioport_read>> serial_update_irq(s);
+ *   - hw/char/serial.c|582| <<serial_receive_break>> serial_update_irq(s);
+ *   - hw/char/serial.c|590| <<fifo_timeout_int>> serial_update_irq(s);
+ *   - hw/char/serial.c|621| <<serial_receive1>> serial_update_irq(s);
+ */
 static void serial_update_irq(SerialState *s)
 {
     uint8_t tmp_iir = UART_IIR_NO_INT;
@@ -145,6 +174,14 @@ static void serial_update_irq(SerialState *s)
     }
 }
 
+/*
+ * 在以下调用serial_update_parameters():
+ *   - hw/char/serial.c|344| <<serial_ioport_write>> serial_update_parameters(s);
+ *   - hw/char/serial.c|366| <<serial_ioport_write>> serial_update_parameters(s);
+ *   - hw/char/serial.c|435| <<serial_ioport_write>> serial_update_parameters(s);
+ *   - hw/char/serial.c|685| <<serial_post_load>> serial_update_parameters(s);
+ *   - hw/char/serial.c|901| <<serial_be_change>> serial_update_parameters(s);
+ */
 static void serial_update_parameters(SerialState *s)
 {
     float speed;
@@ -329,6 +366,42 @@ static void serial_update_tiocm(SerialState *s)
     qemu_chr_fe_ioctl(&s->chr, CHR_IOCTL_SERIAL_SET_TIOCM, &flags);
 }
 
+/*
+ * (gdb) bt
+ * #0  qemu_chr_write_buffer (s=0x55555750d400, buf=0x55555759aa0c "[", len=1, offset=0x7fffed23e1c0, write_all=false)
+ *     at ../chardev/char.c:115
+ * #1  0x00005555560c7c5e in qemu_chr_write (s=0x55555750d400, buf=0x55555759aa0c "[", len=1, write_all=false)
+ *     at ../chardev/char.c:186
+ * #2  0x00005555560bc2e5 in qemu_chr_fe_write (be=0x55555759aa28, buf=0x55555759aa0c "[", len=1)
+ *     at ../chardev/char-fe.c:41
+ * #3  0x0000555555965c20 in serial_xmit (s=0x55555759a970) at ../hw/char/serial.c:259
+ * #4  0x00005555559660b5 in serial_ioport_write (opaque=0x55555759a970, addr=0, val=91, size=1)
+ *     at ../hw/char/serial.c:359
+ * #5  0x0000555555eb8e96 in memory_region_write_accessor (mr=0x55555759aae0, addr=0, value=0x7fffed23e3a8, size=1,
+ *     shift=0, mask=255, attrs=...) at ../system/memory.c:497
+ * #6  0x0000555555eb91df in access_with_adjusted_size (addr=0, value=0x7fffed23e3a8, size=1, access_size_min=1,
+ *     access_size_max=1, access_fn=0x555555eb8da0 <memory_region_write_accessor>, mr=0x55555759aae0, attrs=...)
+ *     at ../system/memory.c:573
+ * #7  0x0000555555ebc9a2 in memory_region_dispatch_write (mr=0x55555759aae0, addr=0, data=91, op=MO_8, attrs=...)
+ *     at ../system/memory.c:1553
+ * #8  0x0000555555eccb5a in flatview_write_continue_step (attrs=..., buf=0x7ffff7fee000 "[", len=1, mr_addr=0,
+ *     l=0x7fffed23e490, mr=0x55555759aae0) at ../system/physmem.c:2951
+ * #9  0x0000555555eccc2f in flatview_write_continue (fv=0x7ffbd801c2c0, addr=1016, attrs=..., ptr=0x7ffff7fee000,
+ *     len=1, mr_addr=0, l=1, mr=0x55555759aae0) at ../system/physmem.c:2981
+ * #10 0x0000555555eccd5c in flatview_write (fv=0x7ffbd801c2c0, addr=1016, attrs=..., buf=0x7ffff7fee000, len=1)
+ *     at ../system/physmem.c:3012
+ * #11 0x0000555555ecd1e9 in address_space_write (as=0x555557383fc0 <address_space_io>, addr=1016, attrs=...,
+ *     buf=0x7ffff7fee000, len=1) at ../system/physmem.c:3132
+ * #12 0x0000555555ecd263 in address_space_rw (as=0x555557383fc0 <address_space_io>, addr=1016, attrs=...,
+ *     buf=0x7ffff7fee000, len=1, is_write=true) at ../system/physmem.c:3142
+ * #13 0x0000555555f0b7dd in kvm_handle_io (port=1016, attrs=..., data=0x7ffff7fee000, direction=1, size=1, count=1)
+ *     at ../accel/kvm/kvm-all.c:2800
+ * #14 0x0000555555f0c755 in kvm_cpu_exec (cpu=0x555557784610) at ../accel/kvm/kvm-all.c:3186
+ * #15 0x0000555555f0fecd in kvm_vcpu_thread_fn (arg=0x555557784610) at ../accel/kvm/kvm-accel-ops.c:51
+ * #16 0x00005555561ad4c6 in qemu_thread_start (args=0x55555778dd20) at ../util/qemu-thread-posix.c:541
+ * #17 0x00007ffff68a91da in start_thread () from /lib64/libpthread.so.0
+ * #18 0x00007ffff4bda8d3 in clone () from /lib64/libc.so.6
+ */
 static void serial_ioport_write(void *opaque, hwaddr addr, uint64_t val,
                                 unsigned size)
 {
@@ -466,6 +539,35 @@ static void serial_ioport_write(void *opaque, hwaddr addr, uint64_t val,
     }
 }
 
+/*
+ * (gdb) bt
+ * #0  serial_ioport_read (opaque=0x55555759a970, addr=1, size=1) at ../hw/char/serial.c:471
+ * #1  0x0000555555eb8b85 in memory_region_read_accessor (mr=0x55555759aae0, addr=1, value=0x7ffbdfdfe418, size=1, shift=0, mask=255,
+ *     attrs=...) at ../system/memory.c:445
+ * #2  0x0000555555eb91df in access_with_adjusted_size (addr=1, value=0x7ffbdfdfe418, size=1, access_size_min=1, access_size_max=1,
+ *     access_fn=0x555555eb8b3a <memory_region_read_accessor>, mr=0x55555759aae0, attrs=...) at ../system/memory.c:573
+ * #3  0x0000555555ebc539 in memory_region_dispatch_read1 (mr=0x55555759aae0, addr=1, pval=0x7ffbdfdfe418, size=1, attrs=...)
+ *     at ../system/memory.c:1458
+ * #4  0x0000555555ebc66c in memory_region_dispatch_read (mr=0x55555759aae0, addr=1, pval=0x7ffbdfdfe418, op=MO_8, attrs=...)
+ *     at ../system/memory.c:1491
+ * #5  0x0000555555ecce6e in flatview_read_continue_step (attrs=..., buf=0x7fffec239000 "", len=1, mr_addr=1, l=0x7ffbdfdfe490,
+ *     mr=0x55555759aae0) at ../system/physmem.c:3032
+ * #6  0x0000555555eccf6f in flatview_read_continue (fv=0x7ffbd83622b0, addr=1017, attrs=..., ptr=0x7fffec239000, len=1, mr_addr=1, l=1,
+ *     mr=0x55555759aae0) at ../system/physmem.c:3073
+ * #7  0x0000555555ecd09c in flatview_read (fv=0x7ffbd83622b0, addr=1017, attrs=..., buf=0x7fffec239000, len=1)
+ *     at ../system/physmem.c:3103
+ * #8  0x0000555555ecd13d in address_space_read_full (as=0x555557383fc0 <address_space_io>, addr=1017, attrs=..., buf=0x7fffec239000,
+ *     len=1) at ../system/physmem.c:3116
+ * #9  0x0000555555ecd284 in address_space_rw (as=0x555557383fc0 <address_space_io>, addr=1017, attrs=..., buf=0x7fffec239000, len=1,
+ *     is_write=false) at ../system/physmem.c:3144
+ * #10 0x0000555555f0b7dd in kvm_handle_io (port=1017, attrs=..., data=0x7fffec239000, direction=0, size=1, count=1)
+ *     at ../accel/kvm/kvm-all.c:2800
+ * #11 0x0000555555f0c755 in kvm_cpu_exec (cpu=0x555557798850) at ../accel/kvm/kvm-all.c:3186
+ * #12 0x0000555555f0fecd in kvm_vcpu_thread_fn (arg=0x555557798850) at ../accel/kvm/kvm-accel-ops.c:51
+ * #13 0x00005555561ad4c6 in qemu_thread_start (args=0x5555577a1de0) at ../util/qemu-thread-posix.c:541
+ * #14 0x00007ffff68a91da in start_thread () from /lib64/libpthread.so.0
+ * #15 0x00007ffff4bda8d3 in clone () from /lib64/libc.so.6
+ */
 static uint64_t serial_ioport_read(void *opaque, hwaddr addr, unsigned size)
 {
     SerialState *s = opaque;
@@ -552,6 +654,23 @@ static uint64_t serial_ioport_read(void *opaque, hwaddr addr, unsigned size)
     return ret;
 }
 
+/*
+ * (gdb) bt
+ * #0  serial_can_receive (s=0x55555759a970) at ../hw/char/serial.c:557
+ * #1  0x00005555559669af in serial_can_receive1 (opaque=0x55555759a970) at ../hw/char/serial.c:597
+ * #2  0x00005555560c7d08 in qemu_chr_be_can_write (s=0x55555750d400) at ../chardev/char.c:206
+ * #3  0x00005555560cae4c in fd_chr_read_poll (opaque=0x55555750d400) at ../chardev/char-fd.c:83
+ * #4  0x00005555560bd37f in io_watch_poll_prepare (source=0x555558342690, timeout=0x7fffffffda04)
+ *     at ../chardev/char-io.c:48
+ * #5  0x00007ffff6fd30f7 in g_main_context_prepare (context=0x55555745e6a0, priority=0x5555573a2588 <max_priority>)
+ *     at ../glib/gmain.c:3645
+ * #6  0x00005555561ca7e4 in glib_pollfds_fill (cur_timeout=0x7fffffffdac8) at ../util/main-loop.c:259
+ * #7  0x00005555561ca97a in os_host_main_loop_wait (timeout=271798) at ../util/main-loop.c:300
+ * #8  0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #9  0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #10 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #11 0x00005555560d296f in main (argc=21, argv=0x7fffffffdc88) at ../system/main.c:80
+ */
 static int serial_can_receive(SerialState *s)
 {
     if(s->fcr & UART_FCR_FE) {
@@ -597,6 +716,26 @@ static int serial_can_receive1(void *opaque)
     return serial_can_receive(s);
 }
 
+/*
+ * (gdb) bt
+ * #0  serial_receive1 (opaque=0x55555759a970, buf=0x7fffffffc9d0 "k", size=1) at ../hw/char/serial.c:602
+ * #1  0x00005555560c7d61 in qemu_chr_be_write_impl (s=0x55555750d400, buf=0x7fffffffc9d0 "k", len=1)
+ *     at ../chardev/char.c:214
+ * #2  0x00005555560c7dd2 in qemu_chr_be_write (s=0x55555750d400, buf=0x7fffffffc9d0 "k", len=1)
+ *     at ../chardev/char.c:226
+ * #3  0x00005555560cadf1 in fd_chr_read (chan=0x55555745f730, cond=G_IO_IN, opaque=0x55555750d400)
+ *     at ../chardev/char-fd.c:72
+ * #4  0x0000555555f8f8d0 in qio_channel_fd_source_dispatch (source=0x5555582ecd80,
+ *     callback=0x5555560cac95 <fd_chr_read>, user_data=0x55555750d400) at ../io/channel-watch.c:84
+ * #5  0x00007ffff6fd3854 in g_main_dispatch (context=0x55555745e6a0) at ../glib/gmain.c:3325
+ * #6  g_main_context_dispatch (context=0x55555745e6a0) at ../glib/gmain.c:4043
+ * #7  0x00005555561ca939 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #8  0x00005555561ca9c7 in os_host_main_loop_wait (timeout=962480) at ../util/main-loop.c:310
+ * #9  0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #10 0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #11 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #12 0x00005555560d296f in main (argc=21, argv=0x7fffffffdc88) at ../system/main.c:80
+ */
 static void serial_receive1(void *opaque, const uint8_t *buf, int size)
 {
     SerialState *s = opaque;
@@ -930,6 +1069,9 @@ static void serial_realize(DeviceState *dev, Error **errp)
 
     qemu_chr_fe_set_handlers(&s->chr, serial_can_receive1, serial_receive1,
                              serial_event, serial_be_change, s, NULL, true);
+    /*
+     * #define UART_FIFO_LENGTH 16
+     */
     fifo8_create(&s->recv_fifo, UART_FIFO_LENGTH);
     fifo8_create(&s->xmit_fifo, UART_FIFO_LENGTH);
     serial_reset(s);
@@ -951,6 +1093,15 @@ static void serial_unrealize(DeviceState *dev)
     qemu_unregister_reset(serial_reset, s);
 }
 
+/*
+ * 在以下使用serial_io_ops:
+ *   - hw/char/diva-gsp.c|141| <<diva_pci_realize>> memory_region_init_io(&s->io, OBJECT(pci), &serial_io_ops, s,
+ *   - hw/char/serial-isa.c|83| <<serial_isa_realizefn>> memory_region_init_io(&s->io, OBJECT(isa), &serial_io_ops, s, "serial", 8);
+ *   - hw/char/serial-mm.c|36| <<serial_mm_read>> return serial_io_ops.read(&s->serial, addr >> s->regshift, 1);
+ *   - hw/char/serial-mm.c|44| <<serial_mm_write>> serial_io_ops.write(&s->serial, addr >> s->regshift, value, 1);
+ *   - hw/char/serial-pci-multi.c|113| <<multi_serial_pci_realize>> memory_region_init_io(&s->io, OBJECT(pci), &serial_io_ops, s,
+ *   - hw/char/serial-pci.c|60| <<serial_pci_realize>> memory_region_init_io(&s->io, OBJECT(pci), &serial_io_ops, s, "serial", 8);
+ */
 const MemoryRegionOps serial_io_ops = {
     .read = serial_ioport_read,
     .write = serial_ioport_write,
diff --git a/hw/i386/acpi-build.c b/hw/i386/acpi-build.c
index 3fffa4a33..c71d06ceb 100644
--- a/hw/i386/acpi-build.c
+++ b/hw/i386/acpi-build.c
@@ -2634,10 +2634,29 @@ void acpi_build(AcpiBuildTables *tables, MachineState *machine)
     g_free(slic_oem.table_id);
 }
 
+/*
+ * 在以下调用arm的acpi_ram_update():
+ *   - hw/arm/virt-acpi-build.c|1064| <<virt_acpi_build_update>> acpi_ram_update(build_state->table_mr, tables.table_data);
+ *   - hw/arm/virt-acpi-build.c|1065| <<virt_acpi_build_update>> acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
+ *   - hw/arm/virt-acpi-build.c|1066| <<virt_acpi_build_update>> acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
+ *
+ * 在以下调用i386的acpi_ram_update():
+ *   - hw/i386/acpi-build.c|2671| <<acpi_build_update>> acpi_ram_update(build_state->table_mr, tables.table_data);
+ *   - hw/i386/acpi-build.c|2673| <<acpi_build_update>> acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
+ *   - hw/i386/acpi-build.c|2675| <<acpi_build_update>> acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
+ */
 static void acpi_ram_update(MemoryRegion *mr, GArray *data)
 {
     uint32_t size = acpi_data_len(data);
 
+    /*
+     * 在以下调用memory_region_ram_resize():
+     *   - hw/arm/virt-acpi-build.c|1035| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/i386/acpi-build.c|2642| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/loongarch/virt-acpi-build.c|644| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/nvram/fw_cfg.c|631| <<fw_cfg_update_mr>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/riscv/virt-acpi-build.c|746| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     */
     /* Make sure RAM size is correct - in case it got changed e.g. by migration */
     memory_region_ram_resize(mr, size, &error_abort);
 
@@ -2645,6 +2664,12 @@ static void acpi_ram_update(MemoryRegion *mr, GArray *data)
     memory_region_set_dirty(mr, 0, size);
 }
 
+/*
+ * 在以下使用acpi_build_update():
+ *   - hw/i386/acpi-build.c|2728| <<acpi_setup>> build_state->table_mr = acpi_add_rom_blob(acpi_build_update,
+ *   - hw/i386/acpi-build.c|2734| <<acpi_setup>> acpi_add_rom_blob(acpi_build_update, build_state,
+ *   - hw/i386/acpi-build.c|2759| <<acpi_setup>> build_state->rsdp_mr = acpi_add_rom_blob(acpi_build_update,
+ */
 static void acpi_build_update(void *build_opaque)
 {
     AcpiBuildState *build_state = build_opaque;
@@ -2660,10 +2685,32 @@ static void acpi_build_update(void *build_opaque)
 
     acpi_build(&tables, MACHINE(qdev_get_machine()));
 
+    /*
+     * 在以下调用arm的acpi_ram_update():
+     *   - hw/arm/virt-acpi-build.c|1064| <<virt_acpi_build_update>> acpi_ram_update(build_state->table_mr, tables.table_data);
+     *   - hw/arm/virt-acpi-build.c|1065| <<virt_acpi_build_update>> acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
+     *   - hw/arm/virt-acpi-build.c|1066| <<virt_acpi_build_update>> acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
+     *
+     * 在以下调用i386的acpi_ram_update():
+     *   - hw/i386/acpi-build.c|2671| <<acpi_build_update>> acpi_ram_update(build_state->table_mr, tables.table_data);
+     *   - hw/i386/acpi-build.c|2673| <<acpi_build_update>> acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
+     *   - hw/i386/acpi-build.c|2675| <<acpi_build_update>> acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
+     */
     acpi_ram_update(build_state->table_mr, tables.table_data);
 
     acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
 
+    /*
+     * 在以下调用arm的acpi_ram_update():
+     *   - hw/arm/virt-acpi-build.c|1064| <<virt_acpi_build_update>> acpi_ram_update(build_state->table_mr, tables.table_data);
+     *   - hw/arm/virt-acpi-build.c|1065| <<virt_acpi_build_update>> acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
+     *   - hw/arm/virt-acpi-build.c|1066| <<virt_acpi_build_update>> acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
+     *
+     * 在以下调用i386的acpi_ram_update():
+     *   - hw/i386/acpi-build.c|2671| <<acpi_build_update>> acpi_ram_update(build_state->table_mr, tables.table_data);
+     *   - hw/i386/acpi-build.c|2673| <<acpi_build_update>> acpi_ram_update(build_state->rsdp_mr, tables.rsdp);
+     *   - hw/i386/acpi-build.c|2675| <<acpi_build_update>> acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
+     */
     acpi_ram_update(build_state->linker_mr, tables.linker->cmd_blob);
     acpi_build_tables_cleanup(&tables, true);
 }
@@ -2722,6 +2769,24 @@ void acpi_setup(void)
                                               ACPI_BUILD_TABLE_FILE);
     assert(build_state->table_mr != NULL);
 
+    /*
+     * 在以下使用ACPI_BUILD_LOADER_FILE:
+     *   - include/hw/acpi/aml-build.h|13| <<global>> #define ACPI_BUILD_LOADER_FILE "etc/table-loader"
+     *   - hw/acpi/utils.c|37| <<acpi_add_rom_blob>> } else if (!strcmp(name, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/arm/virt-acpi-build.c|1145| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/i386/acpi-build.c|2774| <<acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/i386/acpi-microvm.c|263| <<acpi_setup_microvm>> acpi_add_rom_blob(acpi_build_no_update, NULL,
+     *       tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/loongarch/virt-acpi-build.c|724| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/nvram/fw_cfg.c|654| <<fw_cfg_acpi_mr_restore_post_load>>
+     *       } else if (!strcmp(s->files->f[i].name, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/nvram/fw_cfg.c|894| <<fw_cfg_acpi_mr_save>> } else if (!strcmp(filename, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/riscv/virt-acpi-build.c|810| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     */
     build_state->linker_mr =
         acpi_add_rom_blob(acpi_build_update, build_state,
                           tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
diff --git a/hw/i386/acpi-microvm.c b/hw/i386/acpi-microvm.c
index 279da6b4a..eb6361519 100644
--- a/hw/i386/acpi-microvm.c
+++ b/hw/i386/acpi-microvm.c
@@ -256,6 +256,24 @@ void acpi_setup_microvm(MicrovmMachineState *mms)
     acpi_build_tables_init(&tables);
     acpi_build_microvm(&tables, mms);
 
+    /*
+     * 在以下使用ACPI_BUILD_LOADER_FILE:
+     *   - include/hw/acpi/aml-build.h|13| <<global>> #define ACPI_BUILD_LOADER_FILE "etc/table-loader"
+     *   - hw/acpi/utils.c|37| <<acpi_add_rom_blob>> } else if (!strcmp(name, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/arm/virt-acpi-build.c|1145| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/i386/acpi-build.c|2774| <<acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/i386/acpi-microvm.c|263| <<acpi_setup_microvm>> acpi_add_rom_blob(acpi_build_no_update, NULL,
+     *       tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/loongarch/virt-acpi-build.c|724| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/nvram/fw_cfg.c|654| <<fw_cfg_acpi_mr_restore_post_load>>
+     *       } else if (!strcmp(s->files->f[i].name, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/nvram/fw_cfg.c|894| <<fw_cfg_acpi_mr_save>> } else if (!strcmp(filename, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/riscv/virt-acpi-build.c|810| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     */
     /* Now expose it all to Guest */
     acpi_add_rom_blob(acpi_build_no_update, NULL, tables.table_data,
                       ACPI_BUILD_TABLE_FILE);
diff --git a/hw/ide/core.c b/hw/ide/core.c
index b14983ec5..641c4bedd 100644
--- a/hw/ide/core.c
+++ b/hw/ide/core.c
@@ -1132,6 +1132,20 @@ static void ide_flush_cache(IDEState *s)
     s->status |= BUSY_STAT;
     ide_set_retry(s);
     block_acct_start(blk_get_stats(s->blk), &s->acct, 0, BLOCK_ACCT_FLUSH);
+    /*
+     * 在以下使用blk_aio_flush():
+     *   - hw/block/dataplane/xen-block.c|367| <<xen_block_do_aio>> blk_aio_flush(request->dataplane->blk, xen_block_complete_aio,
+     *   - hw/block/virtio-blk.c|497| <<virtio_blk_handle_flush>> blk_aio_flush(s->blk, virtio_blk_flush_complete, req);
+     *   - hw/ide/core.c|1135| <<ide_flush_cache>> s->pio_aiocb = blk_aio_flush(s->blk, ide_flush_cb, s);
+     *   - hw/nvme/ctrl.c|3524| <<nvme_flush_ns_cb>> iocb->aiocb = blk_aio_flush(ns->blkconf.blk, nvme_flush_ns_cb, iocb);
+     *   - hw/scsi/scsi-disk.c|402| <<scsi_write_do_fua>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+     *   - hw/scsi/scsi-disk.c|570| <<scsi_read_data>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_do_read_cb, r);
+     *   - hw/scsi/scsi-disk.c|1725| <<scsi_disk_emulate_mode_select>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+     *   - hw/scsi/scsi-disk.c|2252| <<scsi_disk_emulate_command>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+     *   - qemu-img.c|4471| <<bench_cb>> acb = blk_aio_flush(b->blk, cb, b);
+     *   - tests/unit/test-block-backend.c|46| <<test_drain_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+     *   - tests/unit/test-block-backend.c|63| <<test_drain_all_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+     */
     s->pio_aiocb = blk_aio_flush(s->blk, ide_flush_cb, s);
 }
 
diff --git a/hw/net/vhost_net.c b/hw/net/vhost_net.c
index 891f235a0..03d350128 100644
--- a/hw/net/vhost_net.c
+++ b/hw/net/vhost_net.c
@@ -351,6 +351,33 @@ struct vhost_net *vhost_net_init(VhostNetOptions *options)
         net->dev.vq_index = net->nc->queue_index * net->dev.nvqs;
     }
 
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     r = vhost_dev_init(&net->dev, options->opaque,
                        options->backend_type, options->busyloop_timeout,
                        &local_err);
@@ -583,6 +610,11 @@ err:
     return r;
 }
 
+/*
+ * 在以下使用vhost_net_stop():
+ *   - hw/net/virtio-net.c|318| <<virtio_net_vhost_status>> vhost_net_stop(vdev, n->nic->ncs, queue_pairs, cvq);
+ *   - net/vhost-vdpa.c|340| <<vhost_vdpa_net_log_global_enable>> vhost_net_stop(vdev, n->nic->ncs, data_queue_pairs, cvq);
+ */
 void vhost_net_stop(VirtIODevice *dev, NetClientState *ncs,
                     int data_queue_pairs, int cvq)
 {
diff --git a/hw/nvme/ctrl.c b/hw/nvme/ctrl.c
index d6b77d4fb..b0eb2d462 100644
--- a/hw/nvme/ctrl.c
+++ b/hw/nvme/ctrl.c
@@ -3521,6 +3521,20 @@ static void nvme_flush_ns_cb(void *opaque, int ret)
         trace_pci_nvme_flush_ns(iocb->nsid);
 
         iocb->ns = NULL;
+        /*
+	 * 在以下使用blk_aio_flush():
+	 *   - hw/block/dataplane/xen-block.c|367| <<xen_block_do_aio>> blk_aio_flush(request->dataplane->blk, xen_block_complete_aio,
+	 *   - hw/block/virtio-blk.c|497| <<virtio_blk_handle_flush>> blk_aio_flush(s->blk, virtio_blk_flush_complete, req);
+	 *   - hw/ide/core.c|1135| <<ide_flush_cache>> s->pio_aiocb = blk_aio_flush(s->blk, ide_flush_cb, s);
+	 *   - hw/nvme/ctrl.c|3524| <<nvme_flush_ns_cb>> iocb->aiocb = blk_aio_flush(ns->blkconf.blk, nvme_flush_ns_cb, iocb);
+	 *   - hw/scsi/scsi-disk.c|402| <<scsi_write_do_fua>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - hw/scsi/scsi-disk.c|570| <<scsi_read_data>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_do_read_cb, r);
+	 *   - hw/scsi/scsi-disk.c|1725| <<scsi_disk_emulate_mode_select>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - hw/scsi/scsi-disk.c|2252| <<scsi_disk_emulate_command>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - qemu-img.c|4471| <<bench_cb>> acb = blk_aio_flush(b->blk, cb, b);
+	 *   - tests/unit/test-block-backend.c|46| <<test_drain_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+	 *   - tests/unit/test-block-backend.c|63| <<test_drain_all_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+	 */
         iocb->aiocb = blk_aio_flush(ns->blkconf.blk, nvme_flush_ns_cb, iocb);
         return;
     }
diff --git a/hw/nvram/fw_cfg.c b/hw/nvram/fw_cfg.c
index a757939cf..55c7e56bc 100644
--- a/hw/nvram/fw_cfg.c
+++ b/hw/nvram/fw_cfg.c
@@ -615,6 +615,28 @@ static bool fw_cfg_acpi_mr_restore(void *opaque)
     return s->acpi_mr_restore && !mr_aligned;
 }
 
+/*
+ * (gdb) bt
+ * #0  qemu_ram_resize (block=0x5555582c8820, newsize=4096, errp=0x5555573a2118 <error_abort>) at ../system/physmem.c:1720
+ * #1  0x0000555555ebf3e9 in memory_region_ram_resize (mr=0x5555582a00e0, newsize=4096, errp=0x5555573a2118 <error_abort>) at ../system/memory.c:2476
+ * #2  0x0000555555ac2db8 in fw_cfg_update_mr (s=0x555557c2ac80, key=41, size=4096) at ../hw/nvram/fw_cfg.c:631
+ * #3  0x0000555555ac2f08 in fw_cfg_acpi_mr_restore_post_load (opaque=0x555557c2ac80, version_id=1) at ../hw/nvram/fw_cfg.c:647
+ * #4  0x0000555555f83d72 in vmstate_load_state (f=0x555557a5bb00, vmsd=0x5555571d0ce0 <vmstate_fw_cfg_acpi_mr>, opaque=0x555557c2ac80, version_id=1)
+ *     at ../migration/vmstate.c:234
+ * #5  0x0000555555f84d35 in vmstate_subsection_load (f=0x555557a5bb00, vmsd=0x5555571d0d60 <vmstate_fw_cfg>, opaque=0x555557c2ac80)
+ *     at ../migration/vmstate.c:608
+ * #6  0x0000555555f83d30 in vmstate_load_state (f=0x555557a5bb00, vmsd=0x5555571d0d60 <vmstate_fw_cfg>, opaque=0x555557c2ac80, version_id=2)
+ *     at ../migration/vmstate.c:228
+ * #7  0x0000555555c807aa in vmstate_load (f=0x555557a5bb00, se=0x555557c2d810) at ../migration/savevm.c:972
+ * #8  0x0000555555c8442a in qemu_loadvm_section_start_full (f=0x555557a5bb00, type=4 '\004') at ../migration/savevm.c:2701
+ * #9  0x0000555555c84fe1 in qemu_loadvm_state_main (f=0x555557a5bb00, mis=0x55555745ce40) at ../migration/savevm.c:3007
+ * #10 0x0000555555c851b0 in qemu_loadvm_state (f=0x555557a5bb00) at ../migration/savevm.c:3094
+ * #11 0x0000555555c6310c in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:866
+ * #12 0x00005555561cda9d in coroutine_trampoline (i0=1466944864, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #13 0x00007ffff4bc5120 in ?? () from /lib64/libc.so.6
+ * #14 0x00007fffffffc7c0 in ?? ()
+ * #15 0x0000000000000000 in ?? ()
+ */
 static void fw_cfg_update_mr(FWCfgState *s, uint16_t key, size_t size)
 {
     MemoryRegion *mr;
@@ -625,12 +647,46 @@ static void fw_cfg_update_mr(FWCfgState *s, uint16_t key, size_t size)
     key &= FW_CFG_ENTRY_MASK;
     assert(key < fw_cfg_max_entry(s));
 
+    /*
+     * FWCfgState *s:
+     * -> FWCfgEntry *entries[2];
+     */
     ptr = s->entries[arch][key].data;
     mr = memory_region_from_host(ptr, &offset);
 
+    /*
+     * 在以下调用memory_region_ram_resize():
+     *   - hw/arm/virt-acpi-build.c|1035| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/i386/acpi-build.c|2642| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/loongarch/virt-acpi-build.c|644| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/nvram/fw_cfg.c|631| <<fw_cfg_update_mr>> memory_region_ram_resize(mr, size, &error_abort);
+     *   - hw/riscv/virt-acpi-build.c|746| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+     */
     memory_region_ram_resize(mr, size, &error_abort);
 }
 
+/*
+ * (gdb) bt
+ * #0  qemu_ram_resize (block=0x5555582c8820, newsize=4096, errp=0x5555573a2118 <error_abort>) at ../system/physmem.c:1720
+ * #1  0x0000555555ebf3e9 in memory_region_ram_resize (mr=0x5555582a00e0, newsize=4096, errp=0x5555573a2118 <error_abort>) at ../system/memory.c:2476
+ * #2  0x0000555555ac2db8 in fw_cfg_update_mr (s=0x555557c2ac80, key=41, size=4096) at ../hw/nvram/fw_cfg.c:631
+ * #3  0x0000555555ac2f08 in fw_cfg_acpi_mr_restore_post_load (opaque=0x555557c2ac80, version_id=1) at ../hw/nvram/fw_cfg.c:647
+ * #4  0x0000555555f83d72 in vmstate_load_state (f=0x555557a5bb00, vmsd=0x5555571d0ce0 <vmstate_fw_cfg_acpi_mr>, opaque=0x555557c2ac80, version_id=1)
+ *     at ../migration/vmstate.c:234
+ * #5  0x0000555555f84d35 in vmstate_subsection_load (f=0x555557a5bb00, vmsd=0x5555571d0d60 <vmstate_fw_cfg>, opaque=0x555557c2ac80)
+ *     at ../migration/vmstate.c:608
+ * #6  0x0000555555f83d30 in vmstate_load_state (f=0x555557a5bb00, vmsd=0x5555571d0d60 <vmstate_fw_cfg>, opaque=0x555557c2ac80, version_id=2)
+ *     at ../migration/vmstate.c:228
+ * #7  0x0000555555c807aa in vmstate_load (f=0x555557a5bb00, se=0x555557c2d810) at ../migration/savevm.c:972
+ * #8  0x0000555555c8442a in qemu_loadvm_section_start_full (f=0x555557a5bb00, type=4 '\004') at ../migration/savevm.c:2701
+ * #9  0x0000555555c84fe1 in qemu_loadvm_state_main (f=0x555557a5bb00, mis=0x55555745ce40) at ../migration/savevm.c:3007
+ * #10 0x0000555555c851b0 in qemu_loadvm_state (f=0x555557a5bb00) at ../migration/savevm.c:3094
+ * #11 0x0000555555c6310c in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:866
+ * #12 0x00005555561cda9d in coroutine_trampoline (i0=1466944864, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #13 0x00007ffff4bc5120 in ?? () from /lib64/libc.so.6
+ * #14 0x00007fffffffc7c0 in ?? ()
+ * #15 0x0000000000000000 in ?? ()
+ */
 static int fw_cfg_acpi_mr_restore_post_load(void *opaque, int version_id)
 {
     FWCfgState *s = opaque;
@@ -644,6 +700,24 @@ static int fw_cfg_acpi_mr_restore_post_load(void *opaque, int version_id)
         if (!strcmp(s->files->f[i].name, ACPI_BUILD_TABLE_FILE)) {
             fw_cfg_update_mr(s, FW_CFG_FILE_FIRST + i, s->table_mr_size);
         } else if (!strcmp(s->files->f[i].name, ACPI_BUILD_LOADER_FILE)) {
+            /*
+	     * 在以下使用ACPI_BUILD_LOADER_FILE:
+             *   - include/hw/acpi/aml-build.h|13| <<global>> #define ACPI_BUILD_LOADER_FILE "etc/table-loader"
+             *   - hw/acpi/utils.c|37| <<acpi_add_rom_blob>> } else if (!strcmp(name, ACPI_BUILD_LOADER_FILE)) {
+             *   - hw/arm/virt-acpi-build.c|1145| <<virt_acpi_setup>> build_state->linker_mr =
+             *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+             *   - hw/i386/acpi-build.c|2774| <<acpi_setup>> build_state->linker_mr =
+             *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+             *   - hw/i386/acpi-microvm.c|263| <<acpi_setup_microvm>> acpi_add_rom_blob(acpi_build_no_update, NULL,
+             *       tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+             *   - hw/loongarch/virt-acpi-build.c|724| <<virt_acpi_setup>> build_state->linker_mr =
+             *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+             *   - hw/nvram/fw_cfg.c|654| <<fw_cfg_acpi_mr_restore_post_load>>
+	     *       } else if (!strcmp(s->files->f[i].name, ACPI_BUILD_LOADER_FILE)) {
+             *   - hw/nvram/fw_cfg.c|894| <<fw_cfg_acpi_mr_save>> } else if (!strcmp(filename, ACPI_BUILD_LOADER_FILE)) {
+             *   - hw/riscv/virt-acpi-build.c|810| <<virt_acpi_setup>> build_state->linker_mr =
+             *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+	     */
             fw_cfg_update_mr(s, FW_CFG_FILE_FIRST + i, s->linker_mr_size);
         } else if (!strcmp(s->files->f[i].name, ACPI_BUILD_RSDP_FILE)) {
             fw_cfg_update_mr(s, FW_CFG_FILE_FIRST + i, s->rsdp_mr_size);
@@ -884,6 +958,24 @@ static void fw_cfg_acpi_mr_save(FWCfgState *s, const char *filename, size_t len)
     if (!strcmp(filename, ACPI_BUILD_TABLE_FILE)) {
         s->table_mr_size = len;
     } else if (!strcmp(filename, ACPI_BUILD_LOADER_FILE)) {
+        /*
+	 * 在以下使用ACPI_BUILD_LOADER_FILE:
+	 *   - include/hw/acpi/aml-build.h|13| <<global>> #define ACPI_BUILD_LOADER_FILE "etc/table-loader"
+	 *   - hw/acpi/utils.c|37| <<acpi_add_rom_blob>> } else if (!strcmp(name, ACPI_BUILD_LOADER_FILE)) {
+	 *   - hw/arm/virt-acpi-build.c|1145| <<virt_acpi_setup>> build_state->linker_mr =
+	 *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+	 *   - hw/i386/acpi-build.c|2774| <<acpi_setup>> build_state->linker_mr =
+	 *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+	 *   - hw/i386/acpi-microvm.c|263| <<acpi_setup_microvm>> acpi_add_rom_blob(acpi_build_no_update, NULL,
+         *       tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+         *   - hw/loongarch/virt-acpi-build.c|724| <<virt_acpi_setup>> build_state->linker_mr =
+         *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+         *   - hw/nvram/fw_cfg.c|654| <<fw_cfg_acpi_mr_restore_post_load>>
+         *       } else if (!strcmp(s->files->f[i].name, ACPI_BUILD_LOADER_FILE)) {
+         *   - hw/nvram/fw_cfg.c|894| <<fw_cfg_acpi_mr_save>> } else if (!strcmp(filename, ACPI_BUILD_LOADER_FILE)) {
+         *   - hw/riscv/virt-acpi-build.c|810| <<virt_acpi_setup>> build_state->linker_mr =
+         *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+	 */
         s->linker_mr_size = len;
     } else if (!strcmp(filename, ACPI_BUILD_RSDP_FILE)) {
         s->rsdp_mr_size = len;
diff --git a/hw/pci/pci.c b/hw/pci/pci.c
index 2844ec555..3b0cb443b 100644
--- a/hw/pci/pci.c
+++ b/hw/pci/pci.c
@@ -793,6 +793,25 @@ static int get_pci_config_device(QEMUFile *f, void *pv, size_t size,
 
     qemu_get_buffer(f, config, size);
     for (i = 0; i < size; ++i) {
+        /*
+	 * (config[i] ^ s->config[i])先异或一下
+	 * 表示只关注那些迁移了的或者没迁移的
+	 *
+	 * 再&一下子cmask.
+	 * Used to enable checks on load. Note that writable bits are
+	 * never checked even if set in cmask.
+	 *
+	 * 再把那些write的bit移除
+	 *
+	 * 再把那些write-1-to-clear的移除
+	 *
+	 * 比如:
+	 * config[i] = 5 (101b)
+	 * s->config[i] = 3 (011b)
+	 * s->cmask[i] = = 0xff
+	 * s->wmask[i] = 0
+	 * s->w1cmask[i] = 0
+	 */
         if ((config[i] ^ s->config[i]) &
             s->cmask[i] & ~s->wmask[i] & ~s->w1cmask[i]) {
             error_report("%s: Bad config data: i=0x%x read: %x device: %x "
@@ -1165,6 +1184,10 @@ static void pci_config_alloc(PCIDevice *pci_dev)
     int config_size = pci_config_size(pci_dev);
 
     pci_dev->config = g_malloc0(config_size);
+    /*
+     * Used to enable config checks on load. Note that writable bits are
+     * never checked even if set in cmask.
+     */
     pci_dev->cmask = g_malloc0(config_size);
     pci_dev->wmask = g_malloc0(config_size);
     pci_dev->w1cmask = g_malloc0(config_size);
@@ -1763,9 +1786,24 @@ void pci_default_write_config(PCIDevice *d, uint32_t addr, uint32_t val_in, int
 
     for (i = 0; i < l; val >>= 8, ++i) {
         uint8_t wmask = d->wmask[addr + i];
+        /*
+	 * 注释Used to implement RW1C(Write 1 to Clear) bytes
+	 */
         uint8_t w1cmask = d->w1cmask[addr + i];
+        /*
+	 * assert的必须返回true
+	 * 所以(wmask & w1cmask)必须是false
+	 * write的bit和write clear不能重合?
+	 */
         assert(!(wmask & w1cmask));
+        /*
+	 * 把已经有的那些write的bit清空
+	 * 再和新的val里write的bit们组合
+	 */
         d->config[addr + i] = (d->config[addr + i] & ~wmask) | (val & wmask);
+        /*
+	 * 组合后的结果中的那些write-to-clear的bit们也要清空
+	 */
         d->config[addr + i] &= ~(val & w1cmask); /* W1C: Write 1 to Clear */
     }
 
diff --git a/hw/riscv/virt-acpi-build.c b/hw/riscv/virt-acpi-build.c
index 1ad680050..9f83559ce 100644
--- a/hw/riscv/virt-acpi-build.c
+++ b/hw/riscv/virt-acpi-build.c
@@ -804,6 +804,24 @@ void virt_acpi_setup(RISCVVirtState *s)
                                               ACPI_BUILD_TABLE_FILE);
     assert(build_state->table_mr != NULL);
 
+    /*
+     * 在以下使用ACPI_BUILD_LOADER_FILE:
+     *   - include/hw/acpi/aml-build.h|13| <<global>> #define ACPI_BUILD_LOADER_FILE "etc/table-loader"
+     *   - hw/acpi/utils.c|37| <<acpi_add_rom_blob>> } else if (!strcmp(name, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/arm/virt-acpi-build.c|1145| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/i386/acpi-build.c|2774| <<acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/i386/acpi-microvm.c|263| <<acpi_setup_microvm>> acpi_add_rom_blob(acpi_build_no_update, NULL,
+     *       tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/loongarch/virt-acpi-build.c|724| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     *   - hw/nvram/fw_cfg.c|654| <<fw_cfg_acpi_mr_restore_post_load>>
+     *       } else if (!strcmp(s->files->f[i].name, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/nvram/fw_cfg.c|894| <<fw_cfg_acpi_mr_save>> } else if (!strcmp(filename, ACPI_BUILD_LOADER_FILE)) {
+     *   - hw/riscv/virt-acpi-build.c|810| <<virt_acpi_setup>> build_state->linker_mr =
+     *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+     */
     build_state->linker_mr = acpi_add_rom_blob(virt_acpi_build_update,
                                                build_state,
                                                tables.linker->cmd_blob,
diff --git a/hw/scsi/scsi-disk.c b/hw/scsi/scsi-disk.c
index e59632e9b..816cc7289 100644
--- a/hw/scsi/scsi-disk.c
+++ b/hw/scsi/scsi-disk.c
@@ -399,6 +399,20 @@ static void scsi_write_do_fua(SCSIDiskReq *r)
     if (r->need_fua_emulation) {
         block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct, 0,
                          BLOCK_ACCT_FLUSH);
+        /*
+	 * 在以下使用blk_aio_flush():
+	 *   - hw/block/dataplane/xen-block.c|367| <<xen_block_do_aio>> blk_aio_flush(request->dataplane->blk, xen_block_complete_aio,
+	 *   - hw/block/virtio-blk.c|497| <<virtio_blk_handle_flush>> blk_aio_flush(s->blk, virtio_blk_flush_complete, req);
+	 *   - hw/ide/core.c|1135| <<ide_flush_cache>> s->pio_aiocb = blk_aio_flush(s->blk, ide_flush_cb, s);
+	 *   - hw/nvme/ctrl.c|3524| <<nvme_flush_ns_cb>> iocb->aiocb = blk_aio_flush(ns->blkconf.blk, nvme_flush_ns_cb, iocb);
+	 *   - hw/scsi/scsi-disk.c|402| <<scsi_write_do_fua>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - hw/scsi/scsi-disk.c|570| <<scsi_read_data>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_do_read_cb, r);
+	 *   - hw/scsi/scsi-disk.c|1725| <<scsi_disk_emulate_mode_select>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - hw/scsi/scsi-disk.c|2252| <<scsi_disk_emulate_command>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - qemu-img.c|4471| <<bench_cb>> acb = blk_aio_flush(b->blk, cb, b);
+	 *   - tests/unit/test-block-backend.c|46| <<test_drain_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+	 *   - tests/unit/test-block-backend.c|63| <<test_drain_all_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+	 */
         r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
         return;
     }
@@ -567,6 +581,20 @@ static void scsi_read_data(SCSIRequest *req)
     if (first && r->need_fua_emulation) {
         block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct, 0,
                          BLOCK_ACCT_FLUSH);
+        /*
+	 * 在以下使用blk_aio_flush():
+	 *   - hw/block/dataplane/xen-block.c|367| <<xen_block_do_aio>> blk_aio_flush(request->dataplane->blk, xen_block_complete_aio,
+	 *   - hw/block/virtio-blk.c|497| <<virtio_blk_handle_flush>> blk_aio_flush(s->blk, virtio_blk_flush_complete, req);
+	 *   - hw/ide/core.c|1135| <<ide_flush_cache>> s->pio_aiocb = blk_aio_flush(s->blk, ide_flush_cb, s);
+	 *   - hw/nvme/ctrl.c|3524| <<nvme_flush_ns_cb>> iocb->aiocb = blk_aio_flush(ns->blkconf.blk, nvme_flush_ns_cb, iocb);
+	 *   - hw/scsi/scsi-disk.c|402| <<scsi_write_do_fua>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - hw/scsi/scsi-disk.c|570| <<scsi_read_data>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_do_read_cb, r);
+	 *   - hw/scsi/scsi-disk.c|1725| <<scsi_disk_emulate_mode_select>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - hw/scsi/scsi-disk.c|2252| <<scsi_disk_emulate_command>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - qemu-img.c|4471| <<bench_cb>> acb = blk_aio_flush(b->blk, cb, b);
+	 *   - tests/unit/test-block-backend.c|46| <<test_drain_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+	 *   - tests/unit/test-block-backend.c|63| <<test_drain_all_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+	 */
         r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_do_read_cb, r);
     } else {
         scsi_do_read(r, 0);
@@ -1722,6 +1750,20 @@ static void scsi_disk_emulate_mode_select(SCSIDiskReq *r, uint8_t *inbuf)
         scsi_req_ref(&r->req);
         block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct, 0,
                          BLOCK_ACCT_FLUSH);
+        /*
+	 * 在以下使用blk_aio_flush():
+	 *   - hw/block/dataplane/xen-block.c|367| <<xen_block_do_aio>> blk_aio_flush(request->dataplane->blk, xen_block_complete_aio,
+	 *   - hw/block/virtio-blk.c|497| <<virtio_blk_handle_flush>> blk_aio_flush(s->blk, virtio_blk_flush_complete, req);
+	 *   - hw/ide/core.c|1135| <<ide_flush_cache>> s->pio_aiocb = blk_aio_flush(s->blk, ide_flush_cb, s);
+	 *   - hw/nvme/ctrl.c|3524| <<nvme_flush_ns_cb>> iocb->aiocb = blk_aio_flush(ns->blkconf.blk, nvme_flush_ns_cb, iocb);
+	 *   - hw/scsi/scsi-disk.c|402| <<scsi_write_do_fua>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - hw/scsi/scsi-disk.c|570| <<scsi_read_data>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_do_read_cb, r);
+	 *   - hw/scsi/scsi-disk.c|1725| <<scsi_disk_emulate_mode_select>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - hw/scsi/scsi-disk.c|2252| <<scsi_disk_emulate_command>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - qemu-img.c|4471| <<bench_cb>> acb = blk_aio_flush(b->blk, cb, b);
+	 *   - tests/unit/test-block-backend.c|46| <<test_drain_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+	 *   - tests/unit/test-block-backend.c|63| <<test_drain_all_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+	 */
         r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
         return;
     }
@@ -2249,6 +2291,20 @@ static int32_t scsi_disk_emulate_command(SCSIRequest *req, uint8_t *buf)
         scsi_req_ref(&r->req);
         block_acct_start(blk_get_stats(s->qdev.conf.blk), &r->acct, 0,
                          BLOCK_ACCT_FLUSH);
+        /*
+	 * 在以下使用blk_aio_flush():
+	 *   - hw/block/dataplane/xen-block.c|367| <<xen_block_do_aio>> blk_aio_flush(request->dataplane->blk, xen_block_complete_aio,
+	 *   - hw/block/virtio-blk.c|497| <<virtio_blk_handle_flush>> blk_aio_flush(s->blk, virtio_blk_flush_complete, req);
+	 *   - hw/ide/core.c|1135| <<ide_flush_cache>> s->pio_aiocb = blk_aio_flush(s->blk, ide_flush_cb, s);
+	 *   - hw/nvme/ctrl.c|3524| <<nvme_flush_ns_cb>> iocb->aiocb = blk_aio_flush(ns->blkconf.blk, nvme_flush_ns_cb, iocb);
+	 *   - hw/scsi/scsi-disk.c|402| <<scsi_write_do_fua>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - hw/scsi/scsi-disk.c|570| <<scsi_read_data>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_do_read_cb, r);
+	 *   - hw/scsi/scsi-disk.c|1725| <<scsi_disk_emulate_mode_select>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - hw/scsi/scsi-disk.c|2252| <<scsi_disk_emulate_command>> r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
+	 *   - qemu-img.c|4471| <<bench_cb>> acb = blk_aio_flush(b->blk, cb, b);
+	 *   - tests/unit/test-block-backend.c|46| <<test_drain_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+	 *   - tests/unit/test-block-backend.c|63| <<test_drain_all_aio_error>> acb = blk_aio_flush(blk, test_drain_aio_error_flush_cb, &completed);
+	 */
         r->req.aiocb = blk_aio_flush(s->qdev.conf.blk, scsi_aio_complete, r);
         return 0;
     case SEEK_10:
diff --git a/hw/scsi/vhost-scsi.c b/hw/scsi/vhost-scsi.c
index 8039d13fd..3076d5cd0 100644
--- a/hw/scsi/vhost-scsi.c
+++ b/hw/scsi/vhost-scsi.c
@@ -31,6 +31,104 @@
 #include "qemu/cutils.h"
 #include "system/system.h"
 
+/*
+ * Legacy的方式.
+ *
+ * VHOST_SET_OWNER
+ * -> vhost_dev_set_owner()
+ *    -> vhost_worker_create()
+ *    -> for (i = 0; i < dev->nvqs; i++)
+ *         __vhost_vq_attach_worker(dev->vqs[i], worker)
+ *
+ * 新的multiqueue/worker的方式.
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_NEW_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_NEW_WORKER
+ *          -> vhost_new_worker()
+ *             -> vhost_worker_create()
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_ATTACH_VRING_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_ATTACH_VRING_WORKER
+ *          -> vhost_vq_attach_worker()
+ *             -> __vhost_vq_attach_worker()
+ */
+/*
+ * vhost-scsi的stack.
+ *
+ * (gdb) bt
+ * #0  virtio_scsi_get_config (vdev=0x555558406a90, config=0x5555584313b0 "") at ../hw/scsi/virtio-scsi.c:888
+ * #1  0x0000555555e3d3e0 in virtio_config_modern_readl (vdev=0x555558406a90, addr=0) at ../hw/virtio/virtio-config-io.c:148
+ * #2  0x0000555555b7ba61 in virtio_pci_device_read (opaque=0x5555583fe4d0, addr=0, size=4) at ../hw/virtio/virtio-pci.c:1778
+ * #3  0x0000555555e65ce2 in memory_region_read_accessor (mr=0x5555583ff2f0, addr=0, value=0x7fffeb0d5458, size=4, shift=0,
+ *     mask=4294967295, attrs=...) at ../system/memory.c:445
+ * #4  0x0000555555e6633a in access_with_adjusted_size (addr=0, value=0x7fffeb0d5458, size=4, access_size_min=1,
+ *     access_size_max=4, access_fn=0x555555e65c97 <memory_region_read_accessor>, mr=0x5555583ff2f0, attrs=...)
+ *     at ../system/memory.c:573
+ * #5  0x0000555555e6969d in memory_region_dispatch_read1 (mr=0x5555583ff2f0, addr=0, pval=0x7fffeb0d5458, size=4, attrs=...)
+ *     at ../system/memory.c:1458
+ * #6  0x0000555555e697cc in memory_region_dispatch_read (mr=0x5555583ff2f0, addr=0, pval=0x7fffeb0d5458, op=MO_32, attrs=...)
+ *     at ../system/memory.c:1491
+ * #7  0x0000555555e796f6 in flatview_read_continue_step (attrs=..., buf=0x7ffff7ff0028 "\v", len=4, mr_addr=0, l=0x7fffeb0d54d0,
+ *     mr=0x5555583ff2f0) at ../system/physmem.c:2867
+ * #8  0x0000555555e797f4 in flatview_read_continue (fv=0x7ffbd4000fc0, addr=61572651163648, attrs=..., ptr=0x7ffff7ff0028,
+ *     len=4, mr_addr=0, l=4, mr=0x5555583ff2f0) at ../system/physmem.c:2908
+ * #9  0x0000555555e7991c in flatview_read (fv=0x7ffbd4000fc0, addr=61572651163648, attrs=..., buf=0x7ffff7ff0028, len=4)
+ *     at ../system/physmem.c:2938
+ * #10 0x0000555555e799bb in address_space_read_full (as=0x555557334980 <address_space_memory>, addr=61572651163648, attrs=...,
+ *     buf=0x7ffff7ff0028, len=4) at ../system/physmem.c:2951
+ * #11 0x0000555555e79afd in address_space_rw (as=0x555557334980 <address_space_memory>, addr=61572651163648, attrs=...,
+ *     buf=0x7ffff7ff0028, len=4, is_write=false) at ../system/physmem.c:2979
+ * #12 0x0000555555ed996b in kvm_cpu_exec (cpu=0x5555576f0660) at ../accel/kvm/kvm-all.c:3184
+ * #13 0x0000555555edd092 in kvm_vcpu_thread_fn (arg=0x5555576f0660) at ../accel/kvm/kvm-accel-ops.c:50
+ * #14 0x0000555556172954 in qemu_thread_start (args=0x5555576fa4e0) at ../util/qemu-thread-posix.c:541
+ * #15 0x00007ffff68a91da in start_thread () from /lib64/libpthread.so.0
+ * #16 0x00007ffff52488d3 in clone () from /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  virtio_pci_modern_regions_init (proxy=0x5555583fe4d0, vdev_name=0x55555637e0e5 "virtio-scsi") at ../hw/virtio/virtio-pci.c:1812
+ * #1  0x0000555555b7c3b4 in virtio_pci_device_plugged (d=0x5555583fe4d0, errp=0x7fffffffd198) at ../hw/virtio/virtio-pci.c:2050
+ * #2  0x0000555555b75f0f in virtio_bus_device_plugged (vdev=0x555558406a90, errp=0x7fffffffd1f0) at ../hw/virtio/virtio-bus.c:74
+ * #3  0x0000555555e3b66e in virtio_device_realize (dev=0x555558406a90, errp=0x7fffffffd250) at ../hw/virtio/virtio.c:3961
+ * #4  0x0000555555ef1d70 in device_set_realized (obj=0x555558406a90, value=true, errp=0x7fffffffd510) at ../hw/core/qdev.c:495
+ * #5  0x0000555555efccbf in property_set_bool (obj=0x555558406a90, v=0x55555840c050, name=0x5555563a55d1 "realized", opaque=0x55555740cf10,
+ *     errp=0x7fffffffd510) at ../qom/object.c:2348
+ * #6  0x0000555555efa803 in object_property_set (obj=0x555558406a90, name=0x5555563a55d1 "realized", v=0x55555840c050, errp=0x7fffffffd510)
+ *     at ../qom/object.c:1455
+ * #7  0x0000555555eff53a in object_property_set_qobject (obj=0x555558406a90, name=0x5555563a55d1 "realized", value=0x5555584312f0, errp=0x7fffffffd510)
+ *     at ../qom/qom-qobject.c:28
+ * #8  0x0000555555efaba8 in object_property_set_bool (obj=0x555558406a90, name=0x5555563a55d1 "realized", value=true, errp=0x7fffffffd510)
+ *     at ../qom/object.c:1525
+ * #9  0x0000555555ef145d in qdev_realize (dev=0x555558406a90, bus=0x555558406a10, errp=0x7fffffffd510) at ../hw/core/qdev.c:276
+ * #10 0x0000555555e5925b in vhost_scsi_pci_realize (vpci_dev=0x5555583fe4d0, errp=0x7fffffffd510) at ../hw/virtio/vhost-scsi-pci.c:62
+ * #11 0x0000555555b7cdb6 in virtio_pci_realize (pci_dev=0x5555583fe4d0, errp=0x7fffffffd510) at ../hw/virtio/virtio-pci.c:2268
+ * #12 0x0000555555a97958 in pci_qdev_realize (qdev=0x5555583fe4d0, errp=0x7fffffffd5d0) at ../hw/pci/pci.c:2114
+ * #13 0x0000555555b7d2d3 in virtio_pci_dc_realize (qdev=0x5555583fe4d0, errp=0x7fffffffd5d0) at ../hw/virtio/virtio-pci.c:2395
+ * #14 0x0000555555ef1d70 in device_set_realized (obj=0x5555583fe4d0, value=true, errp=0x7fffffffd840) at ../hw/core/qdev.c:495
+ * #15 0x0000555555efccbf in property_set_bool (obj=0x5555583fe4d0, v=0x5555584304e0, name=0x5555563a55d1 "realized", opaque=0x55555740cf10,
+ *     errp=0x7fffffffd840) at ../qom/object.c:2348
+ * #16 0x0000555555efa803 in object_property_set (obj=0x5555583fe4d0, name=0x5555563a55d1 "realized", v=0x5555584304e0, errp=0x7fffffffd840)
+ *     at ../qom/object.c:1455
+ * #17 0x0000555555eff53a in object_property_set_qobject (obj=0x5555583fe4d0, name=0x5555563a55d1 "realized", value=0x555558430470,
+ *     errp=0x7fffffffd840) at ../qom/qom-qobject.c:28
+ * #18 0x0000555555efaba8 in object_property_set_bool (obj=0x5555583fe4d0, name=0x5555563a55d1 "realized", value=true, errp=0x7fffffffd840)
+ *     at ../qom/object.c:1525
+ * #19 0x0000555555ef145d in qdev_realize (dev=0x5555583fe4d0, bus=0x555557779c60, errp=0x7fffffffd840) at ../hw/core/qdev.c:276
+ * #20 0x0000555555bd8ea6 in qdev_device_add_from_qdict (opts=0x55555842c7b0, from_json=false, errp=0x7fffffffd840)
+ *     at ../system/qdev-monitor.c:726
+ * #21 0x0000555555bd8f5a in qdev_device_add (opts=0x55555740a160, errp=0x555557350860 <error_fatal>) at ../system/qdev-monitor.c:745
+ * #22 0x0000555555be38b7 in device_init_func (opaque=0x0, opts=0x55555740a160, errp=0x555557350860 <error_fatal>) at ../system/vl.c:1215
+ * #23 0x000055555617e55f in qemu_opts_foreach (list=0x55555722dfe0 <qemu_device_opts>, func=0x555555be3888 <device_init_func>, opaque=0x0,
+ *     errp=0x555557350860 <error_fatal>) at ../util/qemu-option.c:1135
+ * #24 0x0000555555be77ac in qemu_create_cli_devices () at ../system/vl.c:2656
+ * #25 0x0000555555be7a30 in qmp_x_exit_preconfig (errp=0x555557350860 <error_fatal>) at ../system/vl.c:2719
+ * #26 0x0000555555bea569 in qemu_init (argc=21, argv=0x7fffffffdc58) at ../system/vl.c:3753
+ * #27 0x000055555609a086 in main (argc=21, argv=0x7fffffffdc58) at ../system/main.c:47
+ */
+
 /* Features supported by host kernel. */
 static const int kernel_feature_bits[] = {
     VIRTIO_F_NOTIFY_ON_EMPTY,
@@ -43,6 +141,151 @@ static const int kernel_feature_bits[] = {
     VHOST_INVALID_FEATURE_BIT
 };
 
+/*
+ * 9.1的.
+ *
+ * Bootup on source:
+ *
+ * (gdb) bt
+ * #0  vhost_kernel_scsi_set_endpoint (dev=0x5555585519b0, target=0x7fffeae130f0) at ../hw/virtio/vhost-backend.c:81
+ * #1  0x0000555555afe88a in vhost_scsi_set_endpoint (s=0x555558551710) at ../hw/scsi/vhost-scsi.c:56
+ * #2  0x0000555555afeacd in vhost_scsi_start (s=0x555558551710) at ../hw/scsi/vhost-scsi.c:100
+ * #3  0x0000555555afebd8 in vhost_scsi_set_status (vdev=0x555558551710, val=15 '\017') at ../hw/scsi/vhost-scsi.c:134
+ * #4  0x0000555555e33c08 in virtio_set_status (vdev=0x555558551710, val=15 '\017') at ../hw/virtio/virtio.c:2242
+ * #5  0x0000555555b7b531 in virtio_pci_common_write (opaque=0x555558549290, addr=20, val=15, size=1) at ../hw/virtio/virtio-pci.c:1608
+ * #6  0x0000555555e63c32 in memory_region_write_accessor (mr=0x555558549e80, addr=20, value=0x7fffeae133f8, size=1, shift=0,
+ *     mask=255, attrs=...) at ../system/memory.c:497
+ * #7  0x0000555555e63f79 in access_with_adjusted_size (addr=20, value=0x7fffeae133f8, size=1, access_size_min=1, access_size_max=4,
+ *     access_fn=0x555555e63b3c <memory_region_write_accessor>, mr=0x555558549e80, attrs=...) at ../system/memory.c:573
+ * #8  0x0000555555e674ae in memory_region_dispatch_write (mr=0x555558549e80, addr=20, data=15, op=MO_8, attrs=...) at ../system/memory.c:1521
+ * #9  0x0000555555e76bdc in flatview_write_continue_step (attrs=..., buf=0x7ffff7e0c028 "\017", len=1, mr_addr=20, l=0x7fffeae134e0,
+ *     mr=0x555558549e80) at ../system/physmem.c:2803
+ * #10 0x0000555555e76cae in flatview_write_continue (fv=0x7ffbdc32a800, addr=4261429268, attrs=..., ptr=0x7ffff7e0c028,
+ *     len=1, mr_addr=20, l=1, mr=0x555558549e80) at ../system/physmem.c:2833
+ * #11 0x0000555555e76dd6 in flatview_write (fv=0x7ffbdc32a800, addr=4261429268, attrs=..., buf=0x7ffff7e0c028, len=1) at ../system/physmem.c:2864
+ * #12 0x0000555555e77250 in address_space_write (as=0x55555732a200 <address_space_memory>, addr=4261429268, attrs=...,
+ *     buf=0x7ffff7e0c028, len=1) at ../system/physmem.c:2984
+ * #13 0x0000555555e772c8 in address_space_rw (as=0x55555732a200 <address_space_memory>, addr=4261429268, attrs=...,
+ *     buf=0x7ffff7e0c028, len=1, is_write=true) at ../system/physmem.c:2994
+ * #14 0x0000555555ed4d63 in kvm_cpu_exec (cpu=0x5555576e1b00) at ../accel/kvm/kvm-all.c:3075
+ * #15 0x0000555555ed8469 in kvm_vcpu_thread_fn (arg=0x5555576e1b00) at ../accel/kvm/kvm-accel-ops.c:50
+ * #16 0x000055555616b040 in qemu_thread_start (args=0x5555576ec4a0) at ../util/qemu-thread-posix.c:541
+ * #17 0x00007ffff68aa1da in start_thread () from /lib/../lib64/libpthread.so.0
+ * #18 0x00007ffff525ae73 in clone () from /lib/../lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  vhost_kernel_scsi_clear_endpoint (dev=0x5555585519b0, target=0x7fffeae13110) at ../hw/virtio/vhost-backend.c:87
+ * #1  0x0000555555afe9b0 in vhost_scsi_clear_endpoint (s=0x555558551710) at ../hw/scsi/vhost-scsi.c:72
+ * #2  0x0000555555afeb3f in vhost_scsi_stop (s=0x555558551710) at ../hw/scsi/vhost-scsi.c:113
+ * #3  0x0000555555afec1a in vhost_scsi_set_status (vdev=0x555558551710, val=0 '\000') at ../hw/scsi/vhost-scsi.c:140
+ * #4  0x0000555555e33c08 in virtio_set_status (vdev=0x555558551710, val=0 '\000') at ../hw/virtio/virtio.c:2242
+ * #5  0x0000555555b7b531 in virtio_pci_common_write (opaque=0x555558549290, addr=20, val=0, size=1) at ../hw/virtio/virtio-pci.c:1608
+ * #6  0x0000555555e63c32 in memory_region_write_accessor (mr=0x555558549e80, addr=20, value=0x7fffeae133f8, size=1, shift=0,
+ *     mask=255, attrs=...) at ../system/memory.c:497
+ * #7  0x0000555555e63f79 in access_with_adjusted_size (addr=20, value=0x7fffeae133f8, size=1, access_size_min=1, access_size_max=4,
+ *     access_fn=0x555555e63b3c <memory_region_write_accessor>, mr=0x555558549e80, attrs=...) at ../system/memory.c:573
+ * #8  0x0000555555e674ae in memory_region_dispatch_write (mr=0x555558549e80, addr=20, data=0, op=MO_8, attrs=...) at ../system/memory.c:1521
+ * #9  0x0000555555e76bdc in flatview_write_continue_step (attrs=..., buf=0x7ffff7e0c028 "", len=1, mr_addr=20,
+ *     l=0x7fffeae134e0, mr=0x555558549e80) at ../system/physmem.c:2803
+ * #10 0x0000555555e76cae in flatview_write_continue (fv=0x7ffbd8003990, addr=4261429268, attrs=..., ptr=0x7ffff7e0c028, len=1,
+ *     mr_addr=20, l=1, mr=0x555558549e80) at ../system/physmem.c:2833
+ * #11 0x0000555555e76dd6 in flatview_write (fv=0x7ffbd8003990, addr=4261429268, attrs=..., buf=0x7ffff7e0c028, len=1)
+ *     at ../system/physmem.c:2864
+ * #12 0x0000555555e77250 in address_space_write (as=0x55555732a200 <address_space_memory>, addr=4261429268, attrs=...,
+ *     buf=0x7ffff7e0c028, len=1) at ../system/physmem.c:2984
+ * #13 0x0000555555e772c8 in address_space_rw (as=0x55555732a200 <address_space_memory>, addr=4261429268, attrs=...,
+ *     buf=0x7ffff7e0c028, len=1, is_write=true) at ../system/physmem.c:2994
+ * #14 0x0000555555ed4d63 in kvm_cpu_exec (cpu=0x5555576e1b00) at ../accel/kvm/kvm-all.c:3075
+ * #15 0x0000555555ed8469 in kvm_vcpu_thread_fn (arg=0x5555576e1b00) at ../accel/kvm/kvm-accel-ops.c:50
+ * #16 0x000055555616b040 in qemu_thread_start (args=0x5555576ec4a0) at ../util/qemu-thread-posix.c:541
+ * #17 0x00007ffff68aa1da in start_thread () from /lib/../lib64/libpthread.so.0
+ * #18 0x00007ffff525ae73 in clone () from /lib/../lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  vhost_kernel_scsi_set_endpoint (dev=0x5555585519b0, target=0x7fffeae130f0) at ../hw/virtio/vhost-backend.c:81
+ * #1  0x0000555555afe88a in vhost_scsi_set_endpoint (s=0x555558551710) at ../hw/scsi/vhost-scsi.c:56
+ * #2  0x0000555555afeacd in vhost_scsi_start (s=0x555558551710) at ../hw/scsi/vhost-scsi.c:100
+ * #3  0x0000555555afebd8 in vhost_scsi_set_status (vdev=0x555558551710, val=15 '\017') at ../hw/scsi/vhost-scsi.c:134
+ * #4  0x0000555555e33c08 in virtio_set_status (vdev=0x555558551710, val=15 '\017') at ../hw/virtio/virtio.c:2242
+ * #5  0x0000555555b7b531 in virtio_pci_common_write (opaque=0x555558549290, addr=20, val=15, size=1) at ../hw/virtio/virtio-pci.c:1608
+ * #6  0x0000555555e63c32 in memory_region_write_accessor (mr=0x555558549e80, addr=20, value=0x7fffeae133f8, size=1, shift=0, mask=255,
+ *     attrs=...) at ../system/memory.c:497
+ * #7  0x0000555555e63f79 in access_with_adjusted_size (addr=20, value=0x7fffeae133f8, size=1, access_size_min=1,
+ *     access_size_max=4, access_fn=0x555555e63b3c <memory_region_write_accessor>, mr=0x555558549e80, attrs=...) at ../system/memory.c:573
+ * #8  0x0000555555e674ae in memory_region_dispatch_write (mr=0x555558549e80, addr=20, data=15, op=MO_8, attrs=...) at ../system/memory.c:1521
+ * #9  0x0000555555e76bdc in flatview_write_continue_step (attrs=..., buf=0x7ffff7e0c028 "\017\020", len=1, mr_addr=20,
+ *     l=0x7fffeae134e0, mr=0x555558549e80) at ../system/physmem.c:2803
+ * #10 0x0000555555e76cae in flatview_write_continue (fv=0x7ffbd8003990, addr=4261429268, attrs=..., ptr=0x7ffff7e0c028,
+ *     len=1, mr_addr=20, l=1, mr=0x555558549e80) at ../system/physmem.c:2833
+ * #11 0x0000555555e76dd6 in flatview_write (fv=0x7ffbd8003990, addr=4261429268, attrs=..., buf=0x7ffff7e0c028, len=1)
+ *     at ../system/physmem.c:2864
+ * #12 0x0000555555e77250 in address_space_write (as=0x55555732a200 <address_space_memory>, addr=4261429268, attrs=...,
+ *     buf=0x7ffff7e0c028, len=1) at ../system/physmem.c:2984
+ * #13 0x0000555555e772c8 in address_space_rw (as=0x55555732a200 <address_space_memory>, addr=4261429268, attrs=...,
+ *     buf=0x7ffff7e0c028, len=1, is_write=true) at ../system/physmem.c:2994
+ * #14 0x0000555555ed4d63 in kvm_cpu_exec (cpu=0x5555576e1b00) at ../accel/kvm/kvm-all.c:3075
+ * #15 0x0000555555ed8469 in kvm_vcpu_thread_fn (arg=0x5555576e1b00) at ../accel/kvm/kvm-accel-ops.c:50
+ * #16 0x000055555616b040 in qemu_thread_start (args=0x5555576ec4a0) at ../util/qemu-thread-posix.c:541
+ * #17 0x00007ffff68aa1da in start_thread () from /lib/../lib64/libpthread.so.0
+ * #18 0x00007ffff525ae73 in clone () from /lib/../lib64/libc.so.6
+ *
+ * --------------------------
+ *
+ * Migration from source:
+ *
+ * (gdb) bt
+ * #0  vhost_kernel_scsi_clear_endpoint (dev=0x5555585519b0, target=0x7ffbb95d5280) at ../hw/virtio/vhost-backend.c:87
+ * #1  0x0000555555afe9b0 in vhost_scsi_clear_endpoint (s=0x555558551710) at ../hw/scsi/vhost-scsi.c:72
+ * #2  0x0000555555afeb3f in vhost_scsi_stop (s=0x555558551710) at ../hw/scsi/vhost-scsi.c:113
+ * #3  0x0000555555afec1a in vhost_scsi_set_status (vdev=0x555558551710, val=15 '\017') at ../hw/scsi/vhost-scsi.c:140
+ * #4  0x0000555555e33c08 in virtio_set_status (vdev=0x555558551710, val=15 '\017') at ../hw/virtio/virtio.c:2242
+ * #5  0x0000555555e36d84 in virtio_vmstate_change (opaque=0x555558551710, running=false, state=RUN_STATE_FINISH_MIGRATE)
+ *     at ../hw/virtio/virtio.c:3428
+ * #6  0x0000555555be03d7 in vm_state_notify (running=false, state=RUN_STATE_FINISH_MIGRATE) at ../system/runstate.c:405
+ * #7  0x0000555555bd0b84 in do_vm_stop (state=RUN_STATE_FINISH_MIGRATE, send_stop=true) at ../system/cpus.c:301
+ * #8  0x0000555555bd1b08 in vm_stop (state=RUN_STATE_FINISH_MIGRATE) at ../system/cpus.c:712
+ * #9  0x0000555555bd1c83 in vm_stop_force_state (state=RUN_STATE_FINISH_MIGRATE) at ../system/cpus.c:781
+ * #10 0x0000555555c11e3b in migration_stop_vm (s=0x555557400150, state=RUN_STATE_FINISH_MIGRATE) at ../migration/migration.c:225
+ * #11 0x0000555555c17bde in migration_completion_precopy (s=0x555557400150, current_active_state=0x7ffbb95d5588)
+ *     at ../migration/migration.c:2757
+ * #12 0x0000555555c17e13 in migration_completion (s=0x555557400150) at ../migration/migration.c:2839
+ * #13 0x0000555555c18c5a in migration_iteration_run (s=0x555557400150) at ../migration/migration.c:3265
+ * #14 0x0000555555c193b8 in migration_thread (opaque=0x555557400150) at ../migration/migration.c:3531
+ * #15 0x000055555616b040 in qemu_thread_start (args=0x555557e01e50) at ../util/qemu-thread-posix.c:541
+ * #16 0x00007ffff68aa1da in start_thread () from /lib/../lib64/libpthread.so.0
+ * #17 0x00007ffff525ae73 in clone () from /lib/../lib64/libc.so.6
+ *
+ * Migration to target:
+ *
+ * (gdb) bt
+ * #0  vhost_kernel_scsi_set_endpoint (dev=0x555558552430, target=0x7fffffffd5c0) at ../hw/virtio/vhost-backend.c:81
+ * #1  0x0000555555afe88a in vhost_scsi_set_endpoint (s=0x555558552190) at ../hw/scsi/vhost-scsi.c:56
+ * #2  0x0000555555afeacd in vhost_scsi_start (s=0x555558552190) at ../hw/scsi/vhost-scsi.c:100
+ * #3  0x0000555555afebd8 in vhost_scsi_set_status (vdev=0x555558552190, val=15 '\017') at ../hw/scsi/vhost-scsi.c:134
+ * #4  0x0000555555e33c08 in virtio_set_status (vdev=0x555558552190, val=15 '\017') at ../hw/virtio/virtio.c:2242
+ * #5  0x0000555555e36d2f in virtio_vmstate_change (opaque=0x555558552190, running=true, state=RUN_STATE_RUNNING)
+ *     at ../hw/virtio/virtio.c:3420
+ * #6  0x0000555555be0320 in vm_state_notify (running=true, state=RUN_STATE_RUNNING) at ../system/runstate.c:395
+ * #7  0x0000555555bd1be6 in vm_prepare_start (step_pending=false) at ../system/cpus.c:755
+ * #8  0x0000555555bd1c1d in vm_start () at ../system/cpus.c:762
+ * #9  0x0000555555c13324 in process_incoming_migration_bh (opaque=0x5555573fd9d0) at ../migration/migration.c:757
+ * #10 0x0000555555c120bb in migration_bh_dispatch_bh (opaque=0x555557cc41e0) at ../migration/migration.c:284
+ * #11 0x0000555556186520 in aio_bh_call (bh=0x5555573fedc0) at ../util/async.c:171
+ * #12 0x000055555618666e in aio_bh_poll (ctx=0x5555573ff890) at ../util/async.c:218
+ * #13 0x0000555556165595 in aio_dispatch (ctx=0x5555573ff890) at ../util/aio-posix.c:423
+ * #14 0x0000555556186b3d in aio_ctx_dispatch (source=0x5555573ff890, callback=0x0, user_data=0x0) at ../util/async.c:360
+ * #15 0x00007ffff6fd494b in g_main_dispatch (context=0x5555573ffd80) at ../glib/gmain.c:3325
+ * #16 g_main_context_dispatch (context=0x5555573ffd80) at ../glib/gmain.c:4043
+ * #17 0x0000555556188215 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #18 0x00005555561882a3 in os_host_main_loop_wait (timeout=0) at ../util/main-loop.c:310
+ * #19 0x00005555561883d2 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #20 0x0000555555be0f17 in qemu_main_loop () at ../system/runstate.c:826
+ * #21 0x0000555556093994 in qemu_default_main () at ../system/main.c:37
+ * #22 0x00005555560939d1 in main (argc=22, argv=0x7fffffffdbd8) at ../system/main.c:48
+ *
+ * 在以下调用vhost_scsi_set_endpoint():
+ *   - hw/scsi/vhost-scsi.c|100| <<vhost_scsi_start>> ret = vhost_scsi_set_endpoint(s);
+ */
 static int vhost_scsi_set_endpoint(VHostSCSI *s)
 {
     VirtIOSCSICommon *vs = VIRTIO_SCSI_COMMON(s);
@@ -167,6 +410,34 @@ static const VMStateDescription vmstate_virtio_vhost_scsi = {
     .pre_save = vhost_scsi_pre_save,
 };
 
+/*
+ * Legacy的方式.
+ *
+ * VHOST_SET_OWNER
+ * -> vhost_dev_set_owner()
+ *    -> vhost_worker_create()
+ *    -> for (i = 0; i < dev->nvqs; i++)
+ *         __vhost_vq_attach_worker(dev->vqs[i], worker)
+ *
+ * 新的multiqueue/worker的方式.
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_NEW_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_NEW_WORKER
+ *          -> vhost_new_worker()
+ *             -> vhost_worker_create()
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_ATTACH_VRING_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_ATTACH_VRING_WORKER
+ *          -> vhost_vq_attach_worker()
+ *             -> __vhost_vq_attach_worker()
+ *
+ * 在以下使用vhost_scsi_set_workers():
+ *   - hw/scsi/vhost-scsi.c|534| <<vhost_scsi_realize>> ret = vhost_scsi_set_workers(vsc, vs->conf.worker_per_virtqueue);
+ */
 static int vhost_scsi_set_workers(VHostSCSICommon *vsc, bool per_virtqueue)
 {
     struct vhost_dev *dev = &vsc->dev;
@@ -277,6 +548,33 @@ static void vhost_scsi_realize(DeviceState *dev, Error **errp)
     vsc->dev.vq_index = 0;
     vsc->dev.backend_features = 0;
 
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     ret = vhost_dev_init(&vsc->dev, (void *)(uintptr_t)vhostfd,
                          VHOST_BACKEND_TYPE_KERNEL, 0, errp);
     if (ret < 0) {
diff --git a/hw/scsi/vhost-user-scsi.c b/hw/scsi/vhost-user-scsi.c
index adb41b981..7efa4ae28 100644
--- a/hw/scsi/vhost-user-scsi.c
+++ b/hw/scsi/vhost-user-scsi.c
@@ -157,6 +157,33 @@ static int vhost_user_scsi_connect(DeviceState *dev, Error **errp)
     vsc->dev.vq_index = 0;
     vsc->dev.backend_features = 0;
 
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     ret = vhost_dev_init(&vsc->dev, &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0,
                          errp);
     if (ret < 0) {
diff --git a/hw/scsi/virtio-scsi-dataplane.c b/hw/scsi/virtio-scsi-dataplane.c
index 95f13fb7c..0c929f52e 100644
--- a/hw/scsi/virtio-scsi-dataplane.c
+++ b/hw/scsi/virtio-scsi-dataplane.c
@@ -65,6 +65,16 @@ void virtio_scsi_dataplane_setup(VirtIOSCSI *s, Error **errp)
     s->vq_aio_context[1] = qemu_get_aio_context();
 
     if (vs->conf.iothread_vq_mapping_list) {
+        /*
+	 * 在以下调用iothread_vq_mapping_apply():
+	 *   - hw/block/virtio-blk.c|1740| <<virtio_blk_vq_aio_context_init>>
+	 *       if (!iothread_vq_mapping_apply(conf->iothread_vq_mapping_list,
+	 *                 s->vq_aio_context, conf->num_queues, errp)) {
+	 *   - hw/scsi/virtio-scsi-dataplane.c|68| <<virtio_scsi_dataplane_setup>>
+	 *       if (!iothread_vq_mapping_apply(vs->conf.iothread_vq_mapping_list,
+	 *                 &s->vq_aio_context[VIRTIO_SCSI_VQ_NUM_FIXED],
+	 *                 vs->conf.num_queues, errp)) {
+	 */
         if (!iothread_vq_mapping_apply(vs->conf.iothread_vq_mapping_list,
                     &s->vq_aio_context[VIRTIO_SCSI_VQ_NUM_FIXED],
                     vs->conf.num_queues, errp)) {
@@ -132,6 +142,18 @@ static void virtio_scsi_dataplane_stop_vq_bh(void *opaque)
     virtio_queue_aio_detach_host_notifier(vq, ctx);
     host_notifier = virtio_queue_get_host_notifier(vq);
 
+    /*
+     * 在以下使用virtio_queue_host_notifier_read():
+     *   - hw/block/virtio-blk.c|2065| <<virtio_blk_ioeventfd_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+     *   - hw/scsi/virtio-scsi-dataplane.c|149| <<virtio_scsi_dataplane_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+     *   - hw/virtio/virtio-bus.c|328| <<virtio_bus_cleanup_host_notifier>> virtio_queue_host_notifier_read(notifier);
+     *   - hw/virtio/virtio.c|3829| <<virtio_queue_aio_attach_host_notifier>> aio_set_event_notifier(ctx, &vq->host_notifier,
+     *        virtio_queue_host_notifier_read, virtio_queue_host_notifier_aio_poll, virtio_queue_host_notifier_aio_poll_ready);
+     *   - hw/virtio/virtio.c|3858| <<virtio_queue_aio_attach_host_notifier_no_poll>> aio_set_event_notifier(ctx,
+     *        &vq->host_notifier, virtio_queue_host_notifier_read, NULL, NULL);
+     *   - hw/virtio/virtio.c|4077| <<virtio_device_start_ioeventfd_impl>> event_notifier_set_handler(&vq->host_notifier,
+     *        virtio_queue_host_notifier_read);
+     */
     /*
      * Test and clear notifier after disabling event, in case poll callback
      * didn't have time to run.
@@ -139,6 +161,13 @@ static void virtio_scsi_dataplane_stop_vq_bh(void *opaque)
     virtio_queue_host_notifier_read(host_notifier);
 }
 
+/*
+ * 在以下使用VirtioDeviceClass->start_ioeventfd:
+ *   - hw/block/virtio-blk.c|2178| <<virtio_blk_class_init>> vdc->start_ioeventfd = virtio_blk_start_ioeventfd;
+ *   - hw/scsi/virtio-scsi.c|1421| <<virtio_scsi_class_init>> vdc->start_ioeventfd = virtio_scsi_dataplane_start;
+ *   - hw/virtio/virtio-bus.c|236| <<virtio_bus_start_ioeventfd>> r = vdc->start_ioeventfd(vdev);
+ *   - hw/virtio/virtio.c|4153| <<virtio_device_class_init>> vdc->start_ioeventfd = virtio_device_start_ioeventfd_impl;
+ */
 /* Context: BQL held */
 int virtio_scsi_dataplane_start(VirtIODevice *vdev)
 {
@@ -200,6 +229,17 @@ int virtio_scsi_dataplane_start(VirtIODevice *vdev)
     smp_wmb(); /* paired with aio_notify_accept() */
 
     if (s->bus.drain_count == 0) {
+        /*
+	 * 在以下使用virtio_queue_aio_attach_host_notifier():
+	 *   - hw/block/virtio-blk.c|1662| <<virtio_blk_ioeventfd_attach>>
+	 *        virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+	 *   - hw/scsi/virtio-scsi-dataplane.c|203| <<virtio_scsi_dataplane_start>>
+	 *        virtio_queue_aio_attach_host_notifier(vs->ctrl_vq, s->vq_aio_context[0]);
+	 *   - hw/scsi/virtio-scsi-dataplane.c|210| <<virtio_scsi_dataplane_start>>
+	 *        virtio_queue_aio_attach_host_notifier(vs->cmd_vqs[i], ctx);
+	 *   - hw/scsi/virtio-scsi.c|1250| <<virtio_scsi_drained_end>>
+	 *        virtio_queue_aio_attach_host_notifier(vq, ctx);
+	 */
         virtio_queue_aio_attach_host_notifier(vs->ctrl_vq,
                                               s->vq_aio_context[0]);
         virtio_queue_aio_attach_host_notifier_no_poll(vs->event_vq,
@@ -207,6 +247,17 @@ int virtio_scsi_dataplane_start(VirtIODevice *vdev)
 
         for (i = 0; i < vs->conf.num_queues; i++) {
             AioContext *ctx = s->vq_aio_context[VIRTIO_SCSI_VQ_NUM_FIXED + i];
+            /*
+	     * 在以下使用virtio_queue_aio_attach_host_notifier():
+	     *   - hw/block/virtio-blk.c|1662| <<virtio_blk_ioeventfd_attach>>
+	     *        virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+	     *   - hw/scsi/virtio-scsi-dataplane.c|203| <<virtio_scsi_dataplane_start>>
+	     *        virtio_queue_aio_attach_host_notifier(vs->ctrl_vq, s->vq_aio_context[0]);
+	     *   - hw/scsi/virtio-scsi-dataplane.c|210| <<virtio_scsi_dataplane_start>>
+	     *        virtio_queue_aio_attach_host_notifier(vs->cmd_vqs[i], ctx);
+	     *   - hw/scsi/virtio-scsi.c|1250| <<virtio_scsi_drained_end>>
+	     *        virtio_queue_aio_attach_host_notifier(vq, ctx);
+	     */
             virtio_queue_aio_attach_host_notifier(vs->cmd_vqs[i], ctx);
         }
     }
diff --git a/hw/scsi/virtio-scsi.c b/hw/scsi/virtio-scsi.c
index f5a3aa236..e52ebe999 100644
--- a/hw/scsi/virtio-scsi.c
+++ b/hw/scsi/virtio-scsi.c
@@ -1247,6 +1247,17 @@ static void virtio_scsi_drained_end(SCSIBus *bus)
         if (vq == vs->event_vq) {
             virtio_queue_aio_attach_host_notifier_no_poll(vq, ctx);
         } else {
+            /*
+	     * 在以下使用virtio_queue_aio_attach_host_notifier():
+	     *   - hw/block/virtio-blk.c|1662| <<virtio_blk_ioeventfd_attach>>
+	     *        virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+	     *   - hw/scsi/virtio-scsi-dataplane.c|203| <<virtio_scsi_dataplane_start>>
+	     *        virtio_queue_aio_attach_host_notifier(vs->ctrl_vq, s->vq_aio_context[0]);
+	     *   - hw/scsi/virtio-scsi-dataplane.c|210| <<virtio_scsi_dataplane_start>>
+	     *        virtio_queue_aio_attach_host_notifier(vs->cmd_vqs[i], ctx);
+	     *   - hw/scsi/virtio-scsi.c|1250| <<virtio_scsi_drained_end>>
+	     *        virtio_queue_aio_attach_host_notifier(vq, ctx);
+	     */
             virtio_queue_aio_attach_host_notifier(vq, ctx);
         }
     }
diff --git a/hw/virtio/iothread-vq-mapping.c b/hw/virtio/iothread-vq-mapping.c
index 15909eb93..b1f415e27 100644
--- a/hw/virtio/iothread-vq-mapping.c
+++ b/hw/virtio/iothread-vq-mapping.c
@@ -72,6 +72,16 @@ iothread_vq_mapping_validate(IOThreadVirtQueueMappingList *list, uint16_t
     return true;
 }
 
+/*
+ * 在以下调用iothread_vq_mapping_apply():
+ *   - hw/block/virtio-blk.c|1740| <<virtio_blk_vq_aio_context_init>>
+ *       if (!iothread_vq_mapping_apply(conf->iothread_vq_mapping_list,
+ *                 s->vq_aio_context, conf->num_queues, errp)) {
+ *   - hw/scsi/virtio-scsi-dataplane.c|68| <<virtio_scsi_dataplane_setup>>
+ *       if (!iothread_vq_mapping_apply(vs->conf.iothread_vq_mapping_list,
+ *                 &s->vq_aio_context[VIRTIO_SCSI_VQ_NUM_FIXED],
+ *                 vs->conf.num_queues, errp)) {
+ */
 bool iothread_vq_mapping_apply(
         IOThreadVirtQueueMappingList *list,
         AioContext **vq_aio_context,
diff --git a/hw/virtio/vdpa-dev.c b/hw/virtio/vdpa-dev.c
index a7e73b1c9..704350f28 100644
--- a/hw/virtio/vdpa-dev.c
+++ b/hw/virtio/vdpa-dev.c
@@ -117,6 +117,33 @@ static void vhost_vdpa_device_realize(DeviceState *dev, Error **errp)
     v->vdpa.shared->device_fd = v->vhostfd;
     v->vdpa.shared->iova_range = iova_range;
 
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     ret = vhost_dev_init(&v->dev, &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
     if (ret < 0) {
         error_setg(errp, "vhost-vdpa-device: vhost initialization failed: %s",
diff --git a/hw/virtio/vhost-user-base.c b/hw/virtio/vhost-user-base.c
index 2bc342332..58079bf5f 100644
--- a/hw/virtio/vhost-user-base.c
+++ b/hw/virtio/vhost-user-base.c
@@ -323,6 +323,33 @@ static void vub_device_realize(DeviceState *dev, Error **errp)
     vub->vhost_dev.nvqs = vub->num_vqs;
     vub->vhost_dev.vqs = g_new0(struct vhost_virtqueue, vub->vhost_dev.nvqs);
 
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     /* connect to backend */
     ret = vhost_dev_init(&vub->vhost_dev, &vub->vhost_user,
                          VHOST_BACKEND_TYPE_USER, 0, errp);
diff --git a/hw/virtio/vhost-user-fs.c b/hw/virtio/vhost-user-fs.c
index 3f00d79ed..998f8ee8c 100644
--- a/hw/virtio/vhost-user-fs.c
+++ b/hw/virtio/vhost-user-fs.c
@@ -250,6 +250,33 @@ static void vuf_device_realize(DeviceState *dev, Error **errp)
     /* 1 high prio queue, plus the number configured */
     fs->vhost_dev.nvqs = 1 + fs->conf.num_request_queues;
     fs->vhost_dev.vqs = g_new0(struct vhost_virtqueue, fs->vhost_dev.nvqs);
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     ret = vhost_dev_init(&fs->vhost_dev, &fs->vhost_user,
                          VHOST_BACKEND_TYPE_USER, 0, errp);
     if (ret < 0) {
diff --git a/hw/virtio/vhost-user-scmi.c b/hw/virtio/vhost-user-scmi.c
index 410a936ca..b62630081 100644
--- a/hw/virtio/vhost-user-scmi.c
+++ b/hw/virtio/vhost-user-scmi.c
@@ -247,6 +247,33 @@ static void vu_scmi_device_realize(DeviceState *dev, Error **errp)
     scmi->vhost_dev.nvqs = 2;
     scmi->vhost_dev.vqs = g_new0(struct vhost_virtqueue, scmi->vhost_dev.nvqs);
 
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     ret = vhost_dev_init(&scmi->vhost_dev, &scmi->vhost_user,
                          VHOST_BACKEND_TYPE_USER, 0, errp);
     if (ret < 0) {
diff --git a/hw/virtio/vhost-user-vsock.c b/hw/virtio/vhost-user-vsock.c
index 293273080..bbb4d59f0 100644
--- a/hw/virtio/vhost-user-vsock.c
+++ b/hw/virtio/vhost-user-vsock.c
@@ -109,6 +109,33 @@ static void vuv_device_realize(DeviceState *dev, Error **errp)
 
     vhost_dev_set_config_notifier(&vvc->vhost_dev, &vsock_ops);
 
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     ret = vhost_dev_init(&vvc->vhost_dev, &vsock->vhost_user,
                          VHOST_BACKEND_TYPE_USER, 0, errp);
     if (ret < 0) {
diff --git a/hw/virtio/vhost-vsock.c b/hw/virtio/vhost-vsock.c
index 940b30fa2..dd2258365 100644
--- a/hw/virtio/vhost-vsock.c
+++ b/hw/virtio/vhost-vsock.c
@@ -168,6 +168,33 @@ static void vhost_vsock_device_realize(DeviceState *dev, Error **errp)
 
     vhost_vsock_common_realize(vdev);
 
+    /*
+     * 在以下调用vhost_dev_init():
+     *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+     *        options->opaque, options->backend_type, 0, &local_err);
+     *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+     *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+     *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+     *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+     *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+     *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+     *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+     *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+     *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+     *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+     *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+     *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+     */
     ret = vhost_dev_init(&vvc->vhost_dev, (void *)(uintptr_t)vhostfd,
                          VHOST_BACKEND_TYPE_KERNEL, 0, errp);
     if (ret < 0) {
diff --git a/hw/virtio/vhost.c b/hw/virtio/vhost.c
index 6aa72fd43..776191d4d 100644
--- a/hw/virtio/vhost.c
+++ b/hw/virtio/vhost.c
@@ -1506,6 +1506,33 @@ static void vhost_virtqueue_cleanup(struct vhost_virtqueue *vq)
     }
 }
 
+/*
+ * 在以下调用vhost_dev_init():
+ *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev,
+ *        options->opaque, options->backend_type, 0, &local_err);
+ *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev,
+ *        &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev,
+ *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev,
+ *        options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+ *   - hw/scsi/vhost-scsi.c|523| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev,
+ *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+ *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev,
+ *        &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev,
+ *        &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+ *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev,
+ *        &vub->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev,
+ *        &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev,
+ *        &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+ *        &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev,
+ *        (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+ */
 int vhost_dev_init(struct vhost_dev *hdev, void *opaque,
                    VhostBackendType backend_type, uint32_t busyloop_timeout,
                    Error **errp)
diff --git a/hw/virtio/virtio-bus.c b/hw/virtio/virtio-bus.c
index 896feb37a..0e1bfec28 100644
--- a/hw/virtio/virtio-bus.c
+++ b/hw/virtio/virtio-bus.c
@@ -233,6 +233,13 @@ int virtio_bus_start_ioeventfd(VirtioBusState *bus)
 
     /* Only set our notifier if we have ownership.  */
     if (!bus->ioeventfd_grabbed) {
+        /*
+	 * 在以下使用VirtioDeviceClass->start_ioeventfd:
+	 *   - hw/block/virtio-blk.c|2178| <<virtio_blk_class_init>> vdc->start_ioeventfd = virtio_blk_start_ioeventfd;
+         *   - hw/scsi/virtio-scsi.c|1421| <<virtio_scsi_class_init>> vdc->start_ioeventfd = virtio_scsi_dataplane_start;
+         *   - hw/virtio/virtio-bus.c|236| <<virtio_bus_start_ioeventfd>> r = vdc->start_ioeventfd(vdev);
+         *   - hw/virtio/virtio.c|4153| <<virtio_device_class_init>> vdc->start_ioeventfd = virtio_device_start_ioeventfd_impl;
+	 */
         r = vdc->start_ioeventfd(vdev);
         if (r < 0) {
             error_report("%s: failed. Fallback to userspace (slower).", __func__);
@@ -315,6 +322,18 @@ void virtio_bus_cleanup_host_notifier(VirtioBusState *bus, int n)
     VirtQueue *vq = virtio_get_queue(vdev, n);
     EventNotifier *notifier = virtio_queue_get_host_notifier(vq);
 
+    /*
+     * 在以下使用virtio_queue_host_notifier_read():
+     *   - hw/block/virtio-blk.c|2065| <<virtio_blk_ioeventfd_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+     *   - hw/scsi/virtio-scsi-dataplane.c|149| <<virtio_scsi_dataplane_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+     *   - hw/virtio/virtio-bus.c|328| <<virtio_bus_cleanup_host_notifier>> virtio_queue_host_notifier_read(notifier);
+     *   - hw/virtio/virtio.c|3829| <<virtio_queue_aio_attach_host_notifier>> aio_set_event_notifier(ctx, &vq->host_notifier,
+     *        virtio_queue_host_notifier_read, virtio_queue_host_notifier_aio_poll, virtio_queue_host_notifier_aio_poll_ready);
+     *   - hw/virtio/virtio.c|3858| <<virtio_queue_aio_attach_host_notifier_no_poll>> aio_set_event_notifier(ctx,
+     *        &vq->host_notifier, virtio_queue_host_notifier_read, NULL, NULL);
+     *   - hw/virtio/virtio.c|4077| <<virtio_device_start_ioeventfd_impl>> event_notifier_set_handler(&vq->host_notifier,
+     *        virtio_queue_host_notifier_read);
+     */
     /* Test and clear notifier after disabling event,
      * in case poll callback didn't have time to run.
      */
diff --git a/hw/virtio/virtio.c b/hw/virtio/virtio.c
index 85110bce3..85d518574 100644
--- a/hw/virtio/virtio.c
+++ b/hw/virtio/virtio.c
@@ -2471,6 +2471,11 @@ void virtio_queue_set_shadow_avail_idx(VirtQueue *vq, uint16_t shadow_avail_idx)
     }
 }
 
+/*
+ * 在以下使用virtio_queue_notify_vq():
+ *   - hw/virtio/virtio.c|3783| <<virtio_queue_host_notifier_aio_poll_ready>> virtio_queue_notify_vq(vq);
+ *   - hw/virtio/virtio.c|3869| <<virtio_queue_host_notifier_read>> virtio_queue_notify_vq(vq);
+ */
 static void virtio_queue_notify_vq(VirtQueue *vq)
 {
     if (vq->vring.desc && vq->handle_output) {
@@ -3780,6 +3785,11 @@ static void virtio_queue_host_notifier_aio_poll_ready(EventNotifier *n)
 {
     VirtQueue *vq = container_of(n, VirtQueue, host_notifier);
 
+    /*
+     * 在以下使用virtio_queue_notify_vq():
+     *   - hw/virtio/virtio.c|3783| <<virtio_queue_host_notifier_aio_poll_ready>> virtio_queue_notify_vq(vq);
+     *   - hw/virtio/virtio.c|3869| <<virtio_queue_host_notifier_read>> virtio_queue_notify_vq(vq);
+     */
     virtio_queue_notify_vq(vq);
 }
 
@@ -3791,6 +3801,17 @@ static void virtio_queue_host_notifier_aio_poll_end(EventNotifier *n)
     virtio_queue_set_notification(vq, 1);
 }
 
+/*
+ * 在以下使用virtio_queue_aio_attach_host_notifier():
+ *   - hw/block/virtio-blk.c|1662| <<virtio_blk_ioeventfd_attach>>
+ *        virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+ *   - hw/scsi/virtio-scsi-dataplane.c|203| <<virtio_scsi_dataplane_start>>
+ *        virtio_queue_aio_attach_host_notifier(vs->ctrl_vq, s->vq_aio_context[0]);
+ *   - hw/scsi/virtio-scsi-dataplane.c|210| <<virtio_scsi_dataplane_start>>
+ *        virtio_queue_aio_attach_host_notifier(vs->cmd_vqs[i], ctx);
+ *   - hw/scsi/virtio-scsi.c|1250| <<virtio_scsi_drained_end>>
+ *        virtio_queue_aio_attach_host_notifier(vq, ctx);
+ */
 void virtio_queue_aio_attach_host_notifier(VirtQueue *vq, AioContext *ctx)
 {
     /*
@@ -3804,6 +3825,18 @@ void virtio_queue_aio_attach_host_notifier(VirtQueue *vq, AioContext *ctx)
         virtio_queue_set_notification(vq, 1);
     }
 
+    /*
+     * 在以下使用virtio_queue_host_notifier_read():
+     *   - hw/block/virtio-blk.c|2065| <<virtio_blk_ioeventfd_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+     *   - hw/scsi/virtio-scsi-dataplane.c|149| <<virtio_scsi_dataplane_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+     *   - hw/virtio/virtio-bus.c|328| <<virtio_bus_cleanup_host_notifier>> virtio_queue_host_notifier_read(notifier);
+     *   - hw/virtio/virtio.c|3829| <<virtio_queue_aio_attach_host_notifier>> aio_set_event_notifier(ctx, &vq->host_notifier,
+     *        virtio_queue_host_notifier_read, virtio_queue_host_notifier_aio_poll, virtio_queue_host_notifier_aio_poll_ready);
+     *   - hw/virtio/virtio.c|3858| <<virtio_queue_aio_attach_host_notifier_no_poll>> aio_set_event_notifier(ctx,
+     *        &vq->host_notifier, virtio_queue_host_notifier_read, NULL, NULL);
+     *   - hw/virtio/virtio.c|4077| <<virtio_device_start_ioeventfd_impl>> event_notifier_set_handler(&vq->host_notifier,
+     *        virtio_queue_host_notifier_read);
+     */
     aio_set_event_notifier(ctx, &vq->host_notifier,
                            virtio_queue_host_notifier_read,
                            virtio_queue_host_notifier_aio_poll,
@@ -3833,6 +3866,18 @@ void virtio_queue_aio_attach_host_notifier_no_poll(VirtQueue *vq, AioContext *ct
         virtio_queue_set_notification(vq, 1);
     }
 
+    /*
+     * 在以下使用virtio_queue_host_notifier_read():
+     *   - hw/block/virtio-blk.c|2065| <<virtio_blk_ioeventfd_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+     *   - hw/scsi/virtio-scsi-dataplane.c|149| <<virtio_scsi_dataplane_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+     *   - hw/virtio/virtio-bus.c|328| <<virtio_bus_cleanup_host_notifier>> virtio_queue_host_notifier_read(notifier);
+     *   - hw/virtio/virtio.c|3829| <<virtio_queue_aio_attach_host_notifier>> aio_set_event_notifier(ctx, &vq->host_notifier,
+     *        virtio_queue_host_notifier_read, virtio_queue_host_notifier_aio_poll, virtio_queue_host_notifier_aio_poll_ready);
+     *   - hw/virtio/virtio.c|3858| <<virtio_queue_aio_attach_host_notifier_no_poll>> aio_set_event_notifier(ctx,
+     *        &vq->host_notifier, virtio_queue_host_notifier_read, NULL, NULL);
+     *   - hw/virtio/virtio.c|4077| <<virtio_device_start_ioeventfd_impl>> event_notifier_set_handler(&vq->host_notifier,
+     *        virtio_queue_host_notifier_read);
+     */
     aio_set_event_notifier(ctx, &vq->host_notifier,
                            virtio_queue_host_notifier_read,
                            NULL, NULL);
@@ -3862,10 +3907,27 @@ void virtio_queue_aio_detach_host_notifier(VirtQueue *vq, AioContext *ctx)
      */
 }
 
+/*
+ * 在以下使用virtio_queue_host_notifier_read():
+ *   - hw/block/virtio-blk.c|2065| <<virtio_blk_ioeventfd_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+ *   - hw/scsi/virtio-scsi-dataplane.c|149| <<virtio_scsi_dataplane_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+ *   - hw/virtio/virtio-bus.c|328| <<virtio_bus_cleanup_host_notifier>> virtio_queue_host_notifier_read(notifier);
+ *   - hw/virtio/virtio.c|3829| <<virtio_queue_aio_attach_host_notifier>> aio_set_event_notifier(ctx, &vq->host_notifier,
+ *        virtio_queue_host_notifier_read, virtio_queue_host_notifier_aio_poll, virtio_queue_host_notifier_aio_poll_ready);
+ *   - hw/virtio/virtio.c|3858| <<virtio_queue_aio_attach_host_notifier_no_poll>> aio_set_event_notifier(ctx,
+ *        &vq->host_notifier, virtio_queue_host_notifier_read, NULL, NULL);
+ *   - hw/virtio/virtio.c|4077| <<virtio_device_start_ioeventfd_impl>> event_notifier_set_handler(&vq->host_notifier,
+ *        virtio_queue_host_notifier_read);
+ */
 void virtio_queue_host_notifier_read(EventNotifier *n)
 {
     VirtQueue *vq = container_of(n, VirtQueue, host_notifier);
     if (event_notifier_test_and_clear(n)) {
+        /*
+	 * 在以下使用virtio_queue_notify_vq():
+	 *   - hw/virtio/virtio.c|3783| <<virtio_queue_host_notifier_aio_poll_ready>> virtio_queue_notify_vq(vq);
+	 *   - hw/virtio/virtio.c|3869| <<virtio_queue_host_notifier_read>> virtio_queue_notify_vq(vq);
+	 */
         virtio_queue_notify_vq(vq);
     }
 }
@@ -4020,6 +4082,13 @@ static const Property virtio_properties[] = {
                      disable_legacy_check, false),
 };
 
+/*
+ * 在以下使用VirtioDeviceClass->start_ioeventfd:
+ *   - hw/block/virtio-blk.c|2178| <<virtio_blk_class_init>> vdc->start_ioeventfd = virtio_blk_start_ioeventfd;
+ *   - hw/scsi/virtio-scsi.c|1421| <<virtio_scsi_class_init>> vdc->start_ioeventfd = virtio_scsi_dataplane_start;
+ *   - hw/virtio/virtio-bus.c|236| <<virtio_bus_start_ioeventfd>> r = vdc->start_ioeventfd(vdev);
+ *   - hw/virtio/virtio.c|4153| <<virtio_device_class_init>> vdc->start_ioeventfd = virtio_device_start_ioeventfd_impl;
+ */
 static int virtio_device_start_ioeventfd_impl(VirtIODevice *vdev)
 {
     VirtioBusState *qbus = VIRTIO_BUS(qdev_get_parent_bus(DEVICE(vdev)));
@@ -4040,6 +4109,18 @@ static int virtio_device_start_ioeventfd_impl(VirtIODevice *vdev)
             err = r;
             goto assign_error;
         }
+        /*
+	 * 在以下使用virtio_queue_host_notifier_read():
+         *   - hw/block/virtio-blk.c|2065| <<virtio_blk_ioeventfd_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+         *   - hw/scsi/virtio-scsi-dataplane.c|149| <<virtio_scsi_dataplane_stop_vq_bh>> virtio_queue_host_notifier_read(host_notifier);
+         *   - hw/virtio/virtio-bus.c|328| <<virtio_bus_cleanup_host_notifier>> virtio_queue_host_notifier_read(notifier);
+         *   - hw/virtio/virtio.c|3829| <<virtio_queue_aio_attach_host_notifier>> aio_set_event_notifier(ctx, &vq->host_notifier,
+         *        virtio_queue_host_notifier_read, virtio_queue_host_notifier_aio_poll, virtio_queue_host_notifier_aio_poll_ready);
+	 *   - hw/virtio/virtio.c|3858| <<virtio_queue_aio_attach_host_notifier_no_poll>> aio_set_event_notifier(ctx,
+	 *        &vq->host_notifier, virtio_queue_host_notifier_read, NULL, NULL);
+	 *   - hw/virtio/virtio.c|4077| <<virtio_device_start_ioeventfd_impl>> event_notifier_set_handler(&vq->host_notifier,
+	 *        virtio_queue_host_notifier_read);
+	 */
         event_notifier_set_handler(&vq->host_notifier,
                                    virtio_queue_host_notifier_read);
     }
diff --git a/include/exec/ram_addr.h b/include/exec/ram_addr.h
index e4c28fbec..b3f09b3fc 100644
--- a/include/exec/ram_addr.h
+++ b/include/exec/ram_addr.h
@@ -479,6 +479,10 @@ static inline void cpu_physical_memory_clear_dirty_range(ram_addr_t start,
 }
 
 
+/*
+ * 在以下调用cpu_physical_memory_sync_dirty_bitmap():
+ *   - migration/ram.c|893| <<ramblock_sync_dirty_bitmap>> cpu_physical_memory_sync_dirty_bitmap(rb, 0, rb->used_length);
+ */
 /* Called with RCU critical section */
 static inline
 uint64_t cpu_physical_memory_sync_dirty_bitmap(RAMBlock *rb,
diff --git a/include/exec/ramblock.h b/include/exec/ramblock.h
index 64484cd82..ba1b02e66 100644
--- a/include/exec/ramblock.h
+++ b/include/exec/ramblock.h
@@ -31,6 +31,11 @@ struct RAMBlock {
     uint8_t *colo_cache; /* For colo, VM's ram cache */
     ram_addr_t offset;
     ram_addr_t used_length;
+    /*
+     * 在以下设置RAMBlock->max_length:
+     *   - system/physmem.c|2111| <<qemu_ram_alloc_from_fd>> new_block->max_length = max_size;
+     *   - system/physmem.c|2284| <<qemu_ram_alloc_internal>> new_block->max_length = max_size;
+     */
     ram_addr_t max_length;
     void (*resized)(const char*, uint64_t length, void *host);
     uint32_t flags;
@@ -44,6 +49,34 @@ struct RAMBlock {
     uint64_t fd_offset;
     int guest_memfd;
     size_t page_size;
+    /*
+     * 在以下使用RAMBlock->bmap:
+     *   - include/exec/ram_addr.h|491| <<cpu_physical_memory_sync_dirty_bitmap>> unsigned long *dest = rb->bmap;
+     *   - migration/ram.c|690| <<pss_find_next_dirty>> unsigned long *bitmap = rb->bmap;
+     *   - migration/ram.c|771| <<colo_bitmap_find_dirty>> unsigned long *bitmap = rb->bmap;
+     *   - migration/ram.c|806| <<migration_bitmap_clear_dirty>> ret = test_and_clear_bit(page, rb->bmap);
+     *   - migration/ram.c|832| <<dirty_bitmap_clear_section>> *cleared_bits += bitmap_count_one_with_offset(rb->bmap, start, npages);
+     *   - migration/ram.c|833| <<dirty_bitmap_clear_section>> bitmap_clear(rb->bmap, start, npages);
+     *   - migration/ram.c|853| <<ramblock_dirty_bitmap_clear_discarded_pages>> if (rb->mr && rb->bmap && memory_region_has_ram_discard_manager(rb->mr)) {
+     *   - migration/ram.c|1778| <<get_queued_page>> dirty = test_bit(page, block->bmap);
+     *   - migration/ram.c|2309| <<ram_bitmaps_destroy>> g_free(block->bmap);
+     *   - migration/ram.c|2310| <<ram_bitmaps_destroy>> block->bmap = NULL;
+     *   - migration/ram.c|2365| <<ram_postcopy_migrated_memory_release>> unsigned long *bitmap = block->bmap;
+     *   - migration/ram.c|2392| <<postcopy_send_discard_bm_ram>> unsigned long *bitmap = block->bmap;
+     *   - migration/ram.c|2468| <<postcopy_chunk_hostpages_pass>> unsigned long *bitmap = block->bmap;
+     *   - migration/ram.c|2695| <<ram_list_init_bitmaps>> block->bmap = bitmap_new(pages);
+     *   - migration/ram.c|2696| <<ram_list_init_bitmaps>> bitmap_set(block->bmap, 0, pages);
+     *   - migration/ram.c|2782| <<ram_state_resume_prepare>> pages += bitmap_count_one(block->bmap,
+     *   - migration/ram.c|2844| <<qemu_guest_free_page_hint>> bitmap_count_one_with_offset(block->bmap, start, npages);
+     *   - migration/ram.c|2845| <<qemu_guest_free_page_hint>> bitmap_clear(block->bmap, start, npages);
+     *   - migration/ram.c|3428| <<colo_record_bitmap>> block->bmap);
+     *   - migration/ram.c|3522| <<colo_init_ram_cache>> block->bmap = bitmap_new(pages);
+     *   - migration/ram.c|3545| <<colo_incoming_start_dirty_log>> bitmap_zero(block->bmap, block->max_length >> TARGET_PAGE_BITS);
+     *   - migration/ram.c|3564| <<colo_release_ram_cache>> g_free(block->bmap);
+     *   - migration/ram.c|3565| <<colo_release_ram_cache>> block->bmap = NULL;
+     *   - migration/ram.c|4392| <<ram_dirty_bitmap_reload>> bitmap_from_le(block->bmap, le_bitmap, nbits);
+     *   - migration/ram.c|4398| <<ram_dirty_bitmap_reload>> bitmap_complement(block->bmap, block->bmap, nbits);
+     */
     /* dirty bitmap used during migration */
     unsigned long *bmap;
 
diff --git a/include/exec/ramlist.h b/include/exec/ramlist.h
index d9cfe530b..89010b28a 100644
--- a/include/exec/ramlist.h
+++ b/include/exec/ramlist.h
@@ -54,11 +54,70 @@ typedef struct RAMList {
     uint32_t version;
     QLIST_HEAD(, RAMBlockNotifier) ramblock_notifiers;
 } RAMList;
+/*
+ * 在以下使用ram_list:
+ *   - system/physmem.c|96| <<global>> RAMList ram_list = { .blocks = QLIST_HEAD_INITIALIZER(ram_list.blocks) };
+ *   - system/physmem.c|956| <<global>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - hw/core/numa.c|846| <<ram_block_notifier_add>> QLIST_INSERT_HEAD(&ram_list.ramblock_notifiers, n, next);
+ *   - hw/core/numa.c|868| <<ram_block_notify_add>> QLIST_FOREACH_SAFE(notifier, &ram_list.ramblock_notifiers, next, next) {
+ *   - hw/core/numa.c|880| <<ram_block_notify_remove>> QLIST_FOREACH_SAFE(notifier, &ram_list.ramblock_notifiers, next, next) {
+ *   - hw/core/numa.c|892| <<ram_block_notify_resize>> QLIST_FOREACH_SAFE(notifier, &ram_list.ramblock_notifiers, next, next) {
+ *   - include/exec/ram_addr.h|175| <<cpu_physical_memory_get_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - include/exec/ram_addr.h|216| <<cpu_physical_memory_all_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - include/exec/ram_addr.h|289| <<cpu_physical_memory_set_dirty_flag>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - include/exec/ram_addr.h|312| <<cpu_physical_memory_set_dirty_range>> blocks[i] = qatomic_rcu_read(&ram_list.dirty_memory[i]);
+ *   - include/exec/ram_addr.h|382| <<cpu_physical_memory_set_dirty_lebitmap>> qatomic_rcu_read(&ram_list.dirty_memory[i])->blocks;
+ *   - include/exec/ram_addr.h|506| <<cpu_physical_memory_sync_dirty_bitmap>> &ram_list.dirty_memory[DIRTY_MEMORY_MIGRATION])->blocks;
+ *   - include/exec/ramlist.h|61| <<INTERNAL_RAMBLOCK_FOREACH>> QLIST_FOREACH_RCU(block, &ram_list.blocks, next)
+ *   - migration/ram.c|1309| <<find_dirty_block>> pss->block = QLIST_FIRST_RCU(&ram_list.blocks);
+ *   - migration/ram.c|2205| <<ram_find_and_save_block>> rs->last_seen_block = QLIST_FIRST_RCU(&ram_list.blocks);
+ *   - migration/ram.c|2352| <<ram_state_reset>> rs->last_version = ram_list.version;
+ *   - migration/ram.c|3107| <<ram_save_iterate>> if (ram_list.version != rs->last_version) {
+ *   - migration/ram.c|3822| <<colo_flush_ram_cache>> block = QLIST_FIRST_RCU(&ram_list.blocks);
+ *   - system/physmem.c|826| <<qemu_get_ram_block>> block = qatomic_rcu_read(&ram_list.mru_block);
+ *   - system/physmem.c|856| <<qemu_get_ram_block>> ram_list.mru_block = block;
+ *   - system/physmem.c|900| <<cpu_physical_memory_test_and_clear_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - system/physmem.c|1155| <<qemu_mutex_lock_ramlist>> qemu_mutex_lock(&ram_list.mutex);
+ *   - system/physmem.c|1160| <<qemu_mutex_unlock_ramlist>> qemu_mutex_unlock(&ram_list.mutex);
+ *   - system/physmem.c|1499| <<find_ram_offset>> if (QLIST_EMPTY_RCU(&ram_list.blocks)) {
+ *   - system/physmem.c|1889| <<dirty_memory_extend>> unsigned int old_num_blocks = ram_list.num_dirty_blocks;
+ *   - system/physmem.c|1904| <<dirty_memory_extend>> old_blocks = qatomic_rcu_read(&ram_list.dirty_memory[i]);
+ *   - system/physmem.c|1917| <<dirty_memory_extend>> qatomic_rcu_set(&ram_list.dirty_memory[i], new_blocks);
+ *   - system/physmem.c|1924| <<dirty_memory_extend>> ram_list.num_dirty_blocks = new_num_blocks;
+ *   - system/physmem.c|2021| <<ram_block_add>> QLIST_INSERT_HEAD_RCU(&ram_list.blocks, new_block, next);
+ *   - system/physmem.c|2023| <<ram_block_add>> ram_list.mru_block = NULL;
+ *   - system/physmem.c|2027| <<ram_block_add>> ram_list.version++;
+ *   - system/physmem.c|2379| <<qemu_ram_free>> ram_list.mru_block = NULL;
+ *   - system/physmem.c|2382| <<qemu_ram_free>> ram_list.version++;
+ *   - system/physmem.c|2566| <<qemu_ram_block_from_host>> block = qatomic_rcu_read(&ram_list.mru_block);
+ *   - system/physmem.c|3381| <<cpu_exec_init_all>> qemu_mutex_init(&ram_list.mutex);
+ */
 extern RAMList ram_list;
 
+/*
+ * 在以下使用INTERNAL_RAMBLOCK_FOREACH():
+ *   - include/exec/ramlist.h|63| <<RAMBLOCK_FOREACH>> #define RAMBLOCK_FOREACH(block) INTERNAL_RAMBLOCK_FOREACH(block)
+ *   - migration/ram.h|68| <<RAMBLOCK_FOREACH_NOT_IGNORED>> INTERNAL_RAMBLOCK_FOREACH(block) \
+ *   - migration/ram.h|72| <<RAMBLOCK_FOREACH_MIGRATABLE>> INTERNAL_RAMBLOCK_FOREACH(block) \
+ */
 /* Should be holding either ram_list.mutex, or the RCU lock. */
 #define  INTERNAL_RAMBLOCK_FOREACH(block)  \
     QLIST_FOREACH_RCU(block, &ram_list.blocks, next)
+/*
+ * 在以下使用RAMBLOCK_FOREACH():
+ *   - migration/postcopy-ram.c|418| <<postcopy_ram_supported_by_host>> RAMBLOCK_FOREACH(block) {
+ *   - system/physmem.c|830| <<qemu_get_ram_block>> RAMBLOCK_FOREACH(block) {
+ *   - system/physmem.c|1174| <<ram_block_format>> RAMBLOCK_FOREACH(block) {
+ *   - system/physmem.c|1503| <<find_ram_offset>> RAMBLOCK_FOREACH(block) {
+ *   - system/physmem.c|1515| <<find_ram_offset>> RAMBLOCK_FOREACH(next_block) {
+ *   - system/physmem.c|1648| <<qemu_ram_set_idstr>> RAMBLOCK_FOREACH(block) {
+ *   - system/physmem.c|1693| <<qemu_ram_pagesize_largest>> RAMBLOCK_FOREACH(block) {
+ *   - system/physmem.c|2010| <<ram_block_add>> RAMBLOCK_FOREACH(block) {
+ *   - system/physmem.c|2425| <<qemu_ram_remap>> RAMBLOCK_FOREACH(block) {
+ *   - system/physmem.c|2571| <<qemu_ram_block_from_host>> RAMBLOCK_FOREACH(block) {
+ *   - system/physmem.c|2602| <<qemu_ram_block_by_name>> RAMBLOCK_FOREACH(block) {
+ *   - system/physmem.c|3859| <<qemu_ram_foreach_block>> RAMBLOCK_FOREACH(block) {
+ */
 /* Never use the INTERNAL_ version except for defining other macros */
 #define RAMBLOCK_FOREACH(block) INTERNAL_RAMBLOCK_FOREACH(block)
 
diff --git a/include/hw/acpi/aml-build.h b/include/hw/acpi/aml-build.h
index c18f68134..3197ca859 100644
--- a/include/hw/acpi/aml-build.h
+++ b/include/hw/acpi/aml-build.h
@@ -10,6 +10,24 @@
 #define ACPI_BUILD_TABLE_FILE "etc/acpi/tables"
 #define ACPI_BUILD_RSDP_FILE "etc/acpi/rsdp"
 #define ACPI_BUILD_TPMLOG_FILE "etc/tpm/log"
+/*
+ * 在以下使用ACPI_BUILD_LOADER_FILE:
+ *   - include/hw/acpi/aml-build.h|13| <<global>> #define ACPI_BUILD_LOADER_FILE "etc/table-loader"
+ *   - hw/acpi/utils.c|37| <<acpi_add_rom_blob>> } else if (!strcmp(name, ACPI_BUILD_LOADER_FILE)) {
+ *   - hw/arm/virt-acpi-build.c|1145| <<virt_acpi_setup>> build_state->linker_mr =
+ *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+ *   - hw/i386/acpi-build.c|2774| <<acpi_setup>> build_state->linker_mr =
+ *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+ *   - hw/i386/acpi-microvm.c|263| <<acpi_setup_microvm>> acpi_add_rom_blob(acpi_build_no_update, NULL,
+ *       tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+ *   - hw/loongarch/virt-acpi-build.c|724| <<virt_acpi_setup>> build_state->linker_mr =
+ *       acpi_add_rom_blob(acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+ *   - hw/nvram/fw_cfg.c|654| <<fw_cfg_acpi_mr_restore_post_load>>
+ *       } else if (!strcmp(s->files->f[i].name, ACPI_BUILD_LOADER_FILE)) {
+ *   - hw/nvram/fw_cfg.c|894| <<fw_cfg_acpi_mr_save>> } else if (!strcmp(filename, ACPI_BUILD_LOADER_FILE)) {
+ *   - hw/riscv/virt-acpi-build.c|810| <<virt_acpi_setup>> build_state->linker_mr =
+ *       acpi_add_rom_blob(virt_acpi_build_update, build_state, tables.linker->cmd_blob, ACPI_BUILD_LOADER_FILE);
+ */
 #define ACPI_BUILD_LOADER_FILE "etc/table-loader"
 
 #define AML_NOTIFY_METHOD "NTFY"
diff --git a/include/hw/char/serial.h b/include/hw/char/serial.h
index 942b372df..bd321cedc 100644
--- a/include/hw/char/serial.h
+++ b/include/hw/char/serial.h
@@ -63,7 +63,41 @@ struct SerialState {
 
     /* Time when the last byte was successfully sent out of the tsr */
     uint64_t last_xmit_ts;
+    /*
+     * 在以下使用SerialState->recv_fifo:
+     *   - hw/char/serial.c|748| <<global>> VMSTATE_STRUCT(recv_fifo, SerialState, 1, vmstate_fifo8, Fifo8),
+     *   - hw/char/serial.c|111| <<recv_fifo_put>> if (!fifo8_is_full(&s->recv_fifo)) {
+     *   - hw/char/serial.c|112| <<recv_fifo_put>> fifo8_push(&s->recv_fifo, chr);
+     *   - hw/char/serial.c|131| <<serial_update_irq>> s->recv_fifo.num >= s->recv_fifo_itl)) {
+     *   - hw/char/serial.c|419| <<serial_ioport_write>> fifo8_reset(&s->recv_fifo);
+     *   - hw/char/serial.c|482| <<serial_ioport_read>> ret = fifo8_is_empty(&s->recv_fifo) ?
+     *   - hw/char/serial.c|483| <<serial_ioport_read>> 0 : fifo8_pop(&s->recv_fifo);
+     *   - hw/char/serial.c|484| <<serial_ioport_read>> if (s->recv_fifo.num == 0) {
+     *   - hw/char/serial.c|558| <<serial_can_receive>> if (s->recv_fifo.num < UART_FIFO_LENGTH) {
+     *   - hw/char/serial.c|566| <<serial_can_receive>> return (s->recv_fifo.num <= s->recv_fifo_itl) ?
+     *   - hw/char/serial.c|567| <<serial_can_receive>> s->recv_fifo_itl - s->recv_fifo.num : 1;
+     *   - hw/char/serial.c|588| <<fifo_timeout_int>> if (s->recv_fifo.num) {
+     *   - hw/char/serial.c|738| <<serial_recv_fifo_needed>> return !fifo8_is_empty(&s->recv_fifo);
+     *   - hw/char/serial.c|881| <<serial_reset>> fifo8_reset(&s->recv_fifo);
+     *   - hw/char/serial.c|933| <<serial_realize>> fifo8_create(&s->recv_fifo, UART_FIFO_LENGTH);
+     *   - hw/char/serial.c|948| <<serial_unrealize>> fifo8_destroy(&s->recv_fifo);
+     */
     Fifo8 recv_fifo;
+    /*
+     * 在以下使用SerialState->xmit_fifo:
+     *   - hw/char/serial.c|765| <<global>> VMSTATE_STRUCT(xmit_fifo, SerialState, 1, vmstate_fifo8, Fifo8),
+     *   - hw/char/serial.c|240| <<serial_xmit>> assert(!fifo8_is_empty(&s->xmit_fifo));
+     *   - hw/char/serial.c|241| <<serial_xmit>> s->tsr = fifo8_pop(&s->xmit_fifo);
+     *   - hw/char/serial.c|242| <<serial_xmit>> if (!s->xmit_fifo.num) {
+     *   - hw/char/serial.c|349| <<serial_ioport_write>> if (fifo8_is_full(&s->xmit_fifo)) {
+     *   - hw/char/serial.c|350| <<serial_ioport_write>> fifo8_pop(&s->xmit_fifo);
+     *   - hw/char/serial.c|352| <<serial_ioport_write>> fifo8_push(&s->xmit_fifo, s->thr);
+     *   - hw/char/serial.c|425| <<serial_ioport_write>> fifo8_reset(&s->xmit_fifo);
+     *   - hw/char/serial.c|756| <<serial_xmit_fifo_needed>> return !fifo8_is_empty(&s->xmit_fifo);
+     *   - hw/char/serial.c|882| <<serial_reset>> fifo8_reset(&s->xmit_fifo);
+     *   - hw/char/serial.c|934| <<serial_realize>> fifo8_create(&s->xmit_fifo, UART_FIFO_LENGTH);
+     *   - hw/char/serial.c|949| <<serial_unrealize>> fifo8_destroy(&s->xmit_fifo);
+     */
     Fifo8 xmit_fifo;
     /* Interrupt trigger level for recv_fifo */
     uint8_t recv_fifo_itl;
diff --git a/include/hw/virtio/virtio-blk.h b/include/hw/virtio/virtio-blk.h
index 3d8dee7ec..5142b8757 100644
--- a/include/hw/virtio/virtio-blk.h
+++ b/include/hw/virtio/virtio-blk.h
@@ -38,8 +38,23 @@ struct VirtIOBlkConf
 {
     BlockConf conf;
     IOThread *iothread;
+    /*
+     * 在以下使用VirtIOBlkConf->iothread_vq_mapping_list:
+     *   - hw/block/virtio-blk.c|1919| <<global>> conf.iothread_vq_mapping_list),
+     *   - hw/block/virtio-blk.c|1481| <<virtio_blk_vq_aio_context_init>> if (conf->iothread && conf->iothread_vq_mapping_list) {
+     *   - hw/block/virtio-blk.c|1488| <<virtio_blk_vq_aio_context_init>> if (conf->iothread || conf->iothread_vq_mapping_list) {
+     *   - hw/block/virtio-blk.c|1503| <<virtio_blk_vq_aio_context_init>> if (conf->iothread_vq_mapping_list) {
+     *   - hw/block/virtio-blk.c|1504| <<virtio_blk_vq_aio_context_init>> if (!iothread_vq_mapping_apply(conf->iothread_vq_mapping_list,
+     *   - hw/block/virtio-blk.c|1537| <<virtio_blk_vq_aio_context_cleanup>> if (conf->iothread_vq_mapping_list) {
+     *   - hw/block/virtio-blk.c|1538| <<virtio_blk_vq_aio_context_cleanup>> iothread_vq_mapping_cleanup(conf->iothread_vq_mapping_list);
+     */
     IOThreadVirtQueueMappingList *iothread_vq_mapping_list;
     char *serial;
+    /*
+     * 在以下使用VirtIOBlkConf->request_merging:
+     *   - hw/block/virtio-blk.c|2054| <<global>> DEFINE_PROP_BIT("request-merging", VirtIOBlock, conf.request_merging, 0,
+     *   - hw/block/virtio-blk.c|1026| <<virtio_blk_handle_request>> if (...!s->conf.request_merging)) {
+     */
     uint32_t request_merging;
     uint16_t num_queues;
     uint16_t queue_size;
@@ -65,6 +80,23 @@ struct VirtIOBlock {
     bool ioeventfd_starting;
     bool ioeventfd_stopping;
 
+    /*
+     * 在以下使用VirtIOBlock->vq_aio_context (二维数组):
+     *   - hw/block/virtio-blk.c|1368| <<virtio_blk_dma_restart_cb>> aio_bh_schedule_oneshot(s->vq_aio_context[i], virtio_blk_dma_restart_bh, vq_rq[i]);
+     *   - hw/block/virtio-blk.c|1652| <<virtio_blk_ioeventfd_detach>> virtio_queue_aio_detach_host_notifier(vq, s->vq_aio_context[i]);
+     *   - hw/block/virtio-blk.c|1662| <<virtio_blk_ioeventfd_attach>> virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+     *   - hw/block/virtio-blk.c|1721| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = g_new(AioContext *, conf->num_queues);
+     *   - hw/block/virtio-blk.c|1725| <<virtio_blk_vq_aio_context_init>> if (!iothread_vq_mapping_apply(conf->iothread_vq_mapping_list,
+     *        s->vq_aio_context, conf->num_queues, errp)) {
+     *   - hw/block/virtio-blk.c|1728| <<virtio_blk_vq_aio_context_init>> g_free(s->vq_aio_context);
+     *   - hw/block/virtio-blk.c|1729| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = NULL; 
+     *   - hw/block/virtio-blk.c|1735| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx; 
+     *   - hw/block/virtio-blk.c|1743| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx;
+     *   - hw/block/virtio-blk.c|1765| <<virtio_blk_vq_aio_context_cleanup>> g_free(s->vq_aio_context);
+     *   - hw/block/virtio-blk.c|1766| <<virtio_blk_vq_aio_context_cleanup>> s->vq_aio_context = NULL;
+     *   - hw/block/virtio-blk.c|1831| <<virtio_blk_start_ioeventfd>> r = blk_set_aio_context(s->conf.conf.blk, s->vq_aio_context[0], &local_err);
+     *   - hw/block/virtio-blk.c|1910| <<virtio_blk_stop_ioeventfd>> AioContext *ctx = s->vq_aio_context[i];
+     */
     /*
      * The AioContext for each virtqueue. The BlockDriverState will use the
      * first element as its AioContext.
@@ -81,13 +113,35 @@ typedef struct VirtIOBlockReq {
     int64_t sector_num;
     VirtIOBlock *dev;
     VirtQueue *vq;
+    /*
+     * 在以下使用VirtIOBlockReq->inhdr_undo:
+     *   - hw/block/virtio-blk.c|62| <<virtio_blk_req_complete>> iov_discard_undo(&req->inhdr_undo);
+     *   - hw/block/virtio-blk.c|867| <<virtio_blk_handle_request>> iov_discard_back_undoable(in_iov,
+     *                          &in_num, sizeof(struct virtio_blk_inhdr), &req->inhdr_undo);
+     *   - hw/block/virtio-blk.c|988| <<virtio_blk_handle_request>> iov_discard_undo(&req->inhdr_undo);
+     */
     IOVDiscardUndo inhdr_undo;
+    /*
+     * 在以下使用VirtIOBlockReq->outhdr_undo:
+     *   - hw/block/virtio-blk.c|112| <<virtio_blk_req_complete>> iov_discard_undo(&req->outhdr_undo);
+     *   - hw/block/virtio-blk.c|964| <<virtio_blk_handle_request>> iov_discard_front_undoable(&out_iov,
+     *                           &out_num, sizeof(req->out), &req->outhdr_undo);
+     *   - hw/block/virtio-blk.c|968| <<virtio_blk_handle_request>> iov_discard_undo(&req->outhdr_undo);
+     *   - hw/block/virtio-blk.c|1100| <<virtio_blk_handle_request>> iov_discard_undo(&req->outhdr_undo);
+     */
     IOVDiscardUndo outhdr_undo;
     struct virtio_blk_inhdr *in;
     struct virtio_blk_outhdr out;
     QEMUIOVector qiov;
     size_t in_len;
     struct VirtIOBlockReq *next;
+    /*
+     * 在以下使用VirtIOBlockReq->mr_next:
+     *   - hw/block/virtio-blk.c|51| <<virtio_blk_init_request>> req->mr_next = NULL;
+     *   - hw/block/virtio-blk.c|81| <<virtio_blk_handle_rw_error>> req->mr_next = NULL;
+     *   - hw/block/virtio-blk.c|112| <<virtio_blk_rw_complete>> next = req->mr_next;
+     *   - hw/block/virtio-blk.c|253| <<submit_requests>> mrb->reqs[i - 1]->mr_next = mrb->reqs[i];
+     */
     struct VirtIOBlockReq *mr_next;
     BlockAcctCookie acct;
 } VirtIOBlockReq;
diff --git a/include/hw/virtio/virtio-scsi.h b/include/hw/virtio/virtio-scsi.h
index 31e852ed6..da9798ef3 100644
--- a/include/hw/virtio/virtio-scsi.h
+++ b/include/hw/virtio/virtio-scsi.h
@@ -61,6 +61,16 @@ struct VirtIOSCSIConf {
     CharBackend chardev;
     uint32_t boot_tpgt;
     IOThread *iothread;
+    /*
+     * 在以下使用VirtIOSCSIConf->iothread_vq_mapping_list:
+     *   - hw/scsi/virtio-scsi.c|1385| <<global>> parent_obj.conf.iothread_vq_mapping_list),
+     *   - hw/scsi/virtio-scsi-dataplane.c|32| <<virtio_scsi_dataplane_setup>> if (vs->conf.iothread && vs->conf.iothread_vq_mapping_list) {
+     *   - hw/scsi/virtio-scsi-dataplane.c|39| <<virtio_scsi_dataplane_setup>> if (vs->conf.iothread || vs->conf.iothread_vq_mapping_list) {
+     *   - hw/scsi/virtio-scsi-dataplane.c|67| <<virtio_scsi_dataplane_setup>> if (vs->conf.iothread_vq_mapping_list) {
+     *   - hw/scsi/virtio-scsi-dataplane.c|68| <<virtio_scsi_dataplane_setup>> if (!iothread_vq_mapping_apply(vs->conf.iothread_vq_mapping_list,
+     *   - hw/scsi/virtio-scsi-dataplane.c|96| <<virtio_scsi_dataplane_cleanup>> if (vs->conf.iothread_vq_mapping_list) {
+     *   - hw/scsi/virtio-scsi-dataplane.c|97| <<virtio_scsi_dataplane_cleanup>> iothread_vq_mapping_cleanup(vs->conf.iothread_vq_mapping_list);
+     */
     IOThreadVirtQueueMappingList *iothread_vq_mapping_list;
 };
 
diff --git a/include/hw/virtio/virtio.h b/include/hw/virtio/virtio.h
index 638691028..dcd696cdd 100644
--- a/include/hw/virtio/virtio.h
+++ b/include/hw/virtio/virtio.h
@@ -208,6 +208,13 @@ struct VirtioDeviceClass {
      * must mask in frontend instead.
      */
     void (*guest_notifier_mask)(VirtIODevice *vdev, int n, bool mask);
+    /*
+     * 在以下使用VirtioDeviceClass->start_ioeventfd:
+     *   - hw/block/virtio-blk.c|2178| <<virtio_blk_class_init>> vdc->start_ioeventfd = virtio_blk_start_ioeventfd;
+     *   - hw/scsi/virtio-scsi.c|1421| <<virtio_scsi_class_init>> vdc->start_ioeventfd = virtio_scsi_dataplane_start;
+     *   - hw/virtio/virtio-bus.c|236| <<virtio_bus_start_ioeventfd>> r = vdc->start_ioeventfd(vdev);
+     *   - hw/virtio/virtio.c|4153| <<virtio_device_class_init>> vdc->start_ioeventfd = virtio_device_start_ioeventfd_impl;
+     */
     int (*start_ioeventfd)(VirtIODevice *vdev);
     void (*stop_ioeventfd)(VirtIODevice *vdev);
     /* Called before loading queues. Useful to add queues before loading. */
diff --git a/include/qemu/timer.h b/include/qemu/timer.h
index abd2204f3..9631ffa48 100644
--- a/include/qemu/timer.h
+++ b/include/qemu/timer.h
@@ -776,6 +776,19 @@ int qemu_poll_ns(GPollFD *fds, guint nfds, int64_t timeout);
  *
  * Returns: soonest timeout value in nanoseconds (or -1 for infinite)
  */
+/*
+ * 在以下使用qemu_soonest_timeout():
+ *   - accel/tcg/tcg-accel-ops-icount.c|49| <<icount_get_limit>> deadline = qemu_soonest_timeout(deadline,
+ *   - util/aio-posix.c|560| <<run_poll_handlers>> max_ns = qemu_soonest_timeout(*timeout, max_ns);
+ *   - util/aio-posix.c|605| <<try_poll_mode>> max_ns = qemu_soonest_timeout(*timeout, max_ns);
+ *   - util/async.c|298| <<aio_compute_timeout>> return qemu_soonest_timeout(timeout, deadline);
+ *   - util/main-loop.c|278| <<glib_pollfds_fill>> *cur_timeout = qemu_soonest_timeout(timeout_ns, *cur_timeout);
+ *   - util/main-loop.c|515| <<os_host_main_loop_wait>> poll_timeout_ns = qemu_soonest_timeout(poll_timeout_ns, timeout);
+ *   - util/main-loop.c|642| <<main_loop_wait>> timeout_ns = qemu_soonest_timeout(timeout_ns,
+ *   - util/qemu-timer.c|284| <<qemu_clock_deadline_ns_all>> deadline = qemu_soonest_timeout(deadline, delta);
+ *   - util/qemu-timer.c|612| <<timerlistgroup_deadline_ns>> deadline = qemu_soonest_timeout(deadline,
+ *   - util/qemu-timer.c|678| <<qemu_clock_advance_virtual_time>> int64_t warp = qemu_soonest_timeout(dest - clock, deadline);
+ */
 static inline int64_t qemu_soonest_timeout(int64_t timeout1, int64_t timeout2)
 {
     /* we can abuse the fact that -1 (which means infinite) is a maximal
diff --git a/linux-headers/linux/kvm.h b/linux-headers/linux/kvm.h
index 27181b3dd..2d86c4944 100644
--- a/linux-headers/linux/kvm.h
+++ b/linux-headers/linux/kvm.h
@@ -43,6 +43,14 @@ struct kvm_userspace_memory_region2 {
 	__u64 pad2[14];
 };
 
+/*
+ * 在以下使用KVM_MEM_LOG_DIRTY_PAGES:
+ *   - linux-headers/linux/kvm.h|51| <<global>> #define KVM_MEM_LOG_DIRTY_PAGES (1UL << 0)
+ *   - accel/kvm/kvm-all.c|608| <<kvm_mem_flags>> flags |= KVM_MEM_LOG_DIRTY_PAGES;
+ *   - accel/kvm/kvm-all.c|720| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+ *   - accel/kvm/kvm-all.c|1490| <<kvm_set_phys_mem>> if (mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
+ *   - accel/kvm/kvm-all.c|1797| <<kvm_log_sync_global>> if (mem->memory_size && mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
+ */
 /*
  * The bit 0 ~ bit 15 of kvm_userspace_memory_region::flags are visible for
  * userspace, other bits are reserved for kvm internal use which are defined
diff --git a/migration/cpr-transfer.c b/migration/cpr-transfer.c
index e1f140359..3eeb5ef03 100644
--- a/migration/cpr-transfer.c
+++ b/migration/cpr-transfer.c
@@ -17,6 +17,10 @@
 #include "migration/vmstate.h"
 #include "trace.h"
 
+/*
+ * 在以下调用cpr_transfer_output():
+ *   - migration/cpr.c|146| <<cpr_state_save>> f = cpr_transfer_output(channel, errp);
+ */
 QEMUFile *cpr_transfer_output(MigrationChannel *channel, Error **errp)
 {
     MigrationAddress *addr = channel->addr;
diff --git a/migration/cpr.c b/migration/cpr.c
index 42c46563e..9c2188d2c 100644
--- a/migration/cpr.c
+++ b/migration/cpr.c
@@ -133,6 +133,10 @@ bool cpr_is_incoming(void)
     return incoming_mode != MIG_MODE_NONE;
 }
 
+/*
+ * 在以下使用cpr_state_save():
+ *   - migration/migration.c|2226| <<qmp_migrate>> if (cpr_state_save(cpr_channel, &local_err)) {
+ */
 int cpr_state_save(MigrationChannel *channel, Error **errp)
 {
     int ret;
diff --git a/migration/dirtyrate.c b/migration/dirtyrate.c
index 4cd14779d..3aa959d64 100644
--- a/migration/dirtyrate.c
+++ b/migration/dirtyrate.c
@@ -100,6 +100,14 @@ void global_dirty_log_change(unsigned int flag, bool start)
             error_report_err(local_err);
         }
     } else {
+        /*
+	 * 在以下使用memory_global_dirty_log_stop():
+         *   - hw/i386/xen/xen-hvm.c|704| <<qmp_xen_set_global_dirty_log>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+         *   - migration/dirtyrate.c|103| <<global_dirty_log_change>> memory_global_dirty_log_stop(flag);
+         *   - migration/dirtyrate.c|118| <<global_dirty_log_sync>> memory_global_dirty_log_stop(flag);
+         *   - migration/ram.c|2354| <<ram_save_cleanup>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+         *   - migration/ram.c|3622| <<colo_release_ram_cache>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+	 */
         memory_global_dirty_log_stop(flag);
     }
     bql_unlock();
@@ -110,11 +118,37 @@ void global_dirty_log_change(unsigned int flag, bool start)
  * 1. sync dirty log from kvm
  * 2. stop dirty tracking if needed.
  */
+/*
+ * 在以下使用global_dirty_log_sync():
+ *   - migration/dirtyrate.c|171| <<vcpu_calculate_dirtyrate>> global_dirty_log_sync(flag, one_shot);
+ *   - migration/dirtyrate.c|654| <<calculate_dirtyrate_dirty_bitmap>> global_dirty_log_sync(GLOBAL_DIRTY_DIRTY_RATE, true);
+ *
+ * 注释:
+ * global_dirty_log_sync
+ * 1. sync dirty log from kvm
+ * 2. stop dirty tracking if needed.
+ */
 static void global_dirty_log_sync(unsigned int flag, bool one_shot)
 {
     bql_lock();
+    /*
+     * 在以下使用memory_global_dirty_log_sync():
+     *   - migration/dirtyrate.c|116| <<global_dirty_log_sync>> memory_global_dirty_log_sync(false);
+     *   - migration/dirtyrate.c|632| <<calculate_dirtyrate_dirty_bitmap>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|1041| <<migration_bitmap_sync>> memory_global_dirty_log_sync(last_stage);
+     *   - migration/ram.c|3600| <<colo_incoming_start_dirty_log>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|3872| <<colo_flush_ram_cache>> memory_global_dirty_log_sync(false);
+     */
     memory_global_dirty_log_sync(false);
     if (one_shot) {
+        /*
+	 * 在以下使用memory_global_dirty_log_stop():
+	 *   - hw/i386/xen/xen-hvm.c|704| <<qmp_xen_set_global_dirty_log>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+	 *   - migration/dirtyrate.c|103| <<global_dirty_log_change>> memory_global_dirty_log_stop(flag);
+	 *   - migration/dirtyrate.c|118| <<global_dirty_log_sync>> memory_global_dirty_log_stop(flag);
+	 *   - migration/ram.c|2354| <<ram_save_cleanup>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+	 *   - migration/ram.c|3622| <<colo_release_ram_cache>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+	 */
         memory_global_dirty_log_stop(flag);
     }
     bql_unlock();
@@ -145,6 +179,13 @@ static void vcpu_dirty_stat_collect(DirtyPageRecord *records,
     }
 }
 
+/*
+ * 在以下使用vcpu_calculate_dirtyrate():
+ *   - migration/dirtyrate.c|674| <<calculate_dirtyrate_dirty_ring>> DirtyStat.calc_time_ms =
+ *           vcpu_calculate_dirtyrate(config.calc_time_ms, &DirtyStat.dirty_ring, GLOBAL_DIRTY_DIRTY_RATE, true);
+ *   - system/dirtylimit.c|88| <<vcpu_dirty_rate_stat_collect>> vcpu_calculate_dirtyrate(period,
+ *           &stat, GLOBAL_DIRTY_LIMIT, false); 
+ */
 int64_t vcpu_calculate_dirtyrate(int64_t calc_time_ms,
                                  VcpuStat *stat,
                                  unsigned int flag,
@@ -168,6 +209,16 @@ retry:
 
     duration = dirty_stat_wait(calc_time_ms, init_time_ms);
 
+    /*
+     * 在以下使用global_dirty_log_sync():
+     *   - migration/dirtyrate.c|171| <<vcpu_calculate_dirtyrate>> global_dirty_log_sync(flag, one_shot);
+     *   - migration/dirtyrate.c|654| <<calculate_dirtyrate_dirty_bitmap>> global_dirty_log_sync(GLOBAL_DIRTY_DIRTY_RATE, true);
+     *
+     * 注释:
+     * global_dirty_log_sync
+     * 1. sync dirty log from kvm
+     * 2. stop dirty tracking if needed.
+     */
     global_dirty_log_sync(flag, one_shot);
 
     WITH_QEMU_LOCK_GUARD(&qemu_cpu_list_lock) {
@@ -629,6 +680,14 @@ static void calculate_dirtyrate_dirty_bitmap(struct DirtyRateConfig config)
      * skip it unconditionally and start dirty tracking
      * from 2'round of log sync
      */
+    /*
+     * 在以下使用memory_global_dirty_log_sync():
+     *   - migration/dirtyrate.c|116| <<global_dirty_log_sync>> memory_global_dirty_log_sync(false);
+     *   - migration/dirtyrate.c|632| <<calculate_dirtyrate_dirty_bitmap>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|1041| <<migration_bitmap_sync>> memory_global_dirty_log_sync(last_stage);
+     *   - migration/ram.c|3600| <<colo_incoming_start_dirty_log>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|3872| <<colo_flush_ram_cache>> memory_global_dirty_log_sync(false);
+     */
     memory_global_dirty_log_sync(false);
 
     /*
@@ -651,6 +710,16 @@ static void calculate_dirtyrate_dirty_bitmap(struct DirtyRateConfig config)
      * 1. fetch dirty bitmap from kvm
      * 2. stop dirty tracking
      */
+    /*
+     * 在以下使用global_dirty_log_sync():
+     *   - migration/dirtyrate.c|171| <<vcpu_calculate_dirtyrate>> global_dirty_log_sync(flag, one_shot);
+     *   - migration/dirtyrate.c|654| <<calculate_dirtyrate_dirty_bitmap>> global_dirty_log_sync(GLOBAL_DIRTY_DIRTY_RATE, true);
+     *
+     * 注释:
+     * global_dirty_log_sync
+     * 1. sync dirty log from kvm
+     * 2. stop dirty tracking if needed.
+     */
     global_dirty_log_sync(GLOBAL_DIRTY_DIRTY_RATE, true);
 
     record_dirtypages_bitmap(&dirty_pages, false);
@@ -659,6 +728,10 @@ static void calculate_dirtyrate_dirty_bitmap(struct DirtyRateConfig config)
                                                   DirtyStat.calc_time_ms);
 }
 
+/*
+ * 在以下使用calculate_dirtyrate_dirty_ring():
+ *   - migration/dirtyrate.c|790| <<calculate_dirtyrate>> calculate_dirtyrate_dirty_ring(config);
+ */
 static void calculate_dirtyrate_dirty_ring(struct DirtyRateConfig config)
 {
     uint64_t dirtyrate = 0;
@@ -670,6 +743,13 @@ static void calculate_dirtyrate_dirty_ring(struct DirtyRateConfig config)
 
     DirtyStat.start_time = qemu_clock_get_ms(QEMU_CLOCK_HOST) / 1000;
 
+    /*
+     * 在以下使用vcpu_calculate_dirtyrate():
+     *   - migration/dirtyrate.c|674| <<calculate_dirtyrate_dirty_ring>> DirtyStat.calc_time_ms =
+     *           vcpu_calculate_dirtyrate(config.calc_time_ms, &DirtyStat.dirty_ring, GLOBAL_DIRTY_DIRTY_RATE, true);
+     *   - system/dirtylimit.c|88| <<vcpu_dirty_rate_stat_collect>> vcpu_calculate_dirtyrate(period,
+     *           &stat, GLOBAL_DIRTY_LIMIT, false);
+     */
     /* calculate vcpu dirtyrate */
     DirtyStat.calc_time_ms = vcpu_calculate_dirtyrate(config.calc_time_ms,
                                                       &DirtyStat.dirty_ring,
diff --git a/migration/migration.c b/migration/migration.c
index d46e776e2..5344d5a6a 100644
--- a/migration/migration.c
+++ b/migration/migration.c
@@ -362,6 +362,10 @@ void migration_bh_schedule(QEMUBHFunc *cb, void *opaque)
     qemu_bh_schedule(bh);
 }
 
+/*
+ * 在以下使用migration_shutdown():
+ *   - system/runstate.c|899| <<qemu_cleanup>> migration_shutdown();
+ */
 void migration_shutdown(void)
 {
     /*
@@ -1584,6 +1588,11 @@ static void migration_connect_set_error(MigrationState *s, const Error *error)
     migrate_set_error(s, error);
 }
 
+/*
+ * 在以下使用migration_cancel():
+ *   - migration/migration.c|2314| <<qmp_migrate_cancel>> migration_cancel();
+ *   - migration/ram.c|4544| <<ram_mig_ram_block_resized>> migration_cancel();
+ */
 void migration_cancel(void)
 {
     MigrationState *s = migrate_get_current();
@@ -2311,6 +2320,11 @@ void qmp_migrate_cancel(Error **errp)
         return;
     }
 
+    /*
+     * 在以下使用migration_cancel():
+     *   - migration/migration.c|2314| <<qmp_migrate_cancel>> migration_cancel();
+     *   - migration/ram.c|4544| <<ram_mig_ram_block_resized>> migration_cancel();
+     */
     migration_cancel();
 }
 
diff --git a/migration/ram.c b/migration/ram.c
index 424df6d9f..a53b81bdf 100644
--- a/migration/ram.c
+++ b/migration/ram.c
@@ -886,6 +886,12 @@ bool ramblock_page_is_discarded(RAMBlock *rb, ram_addr_t start)
     return false;
 }
 
+/*
+ * 在以下调用ramblock_sync_dirty_bitmap():
+ *   - migration/ram.c|1035| <<migration_bitmap_sync>> ramblock_sync_dirty_bitmap(rs, block);
+ *   - migration/ram.c|3543| <<colo_incoming_start_dirty_log>> ramblock_sync_dirty_bitmap(ram_state, block);
+ *   - migration/ram.c|3816| <<colo_flush_ram_cache>> ramblock_sync_dirty_bitmap(ram_state, block);
+ */
 /* Called with RCU critical section */
 static void ramblock_sync_dirty_bitmap(RAMState *rs, RAMBlock *rb)
 {
@@ -1015,6 +1021,11 @@ static void migration_trigger_throttle(RAMState *rs)
     }
 }
 
+/*
+ * 在以下调用migration_bitmap_sync():
+ *   - migration/ram.c|1079| <<migration_bitmap_sync_precopy>> migration_bitmap_sync(ram_state, last_stage);
+ *   - migration/ram.c|2539| <<ram_postcopy_send_discard_bitmap>> migration_bitmap_sync(rs, false);
+ */
 static void migration_bitmap_sync(RAMState *rs, bool last_stage)
 {
     RAMBlock *block;
@@ -1027,6 +1038,14 @@ static void migration_bitmap_sync(RAMState *rs, bool last_stage)
     }
 
     trace_migration_bitmap_sync_start();
+    /*
+     * 在以下使用memory_global_dirty_log_sync():
+     *   - migration/dirtyrate.c|116| <<global_dirty_log_sync>> memory_global_dirty_log_sync(false);
+     *   - migration/dirtyrate.c|632| <<calculate_dirtyrate_dirty_bitmap>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|1041| <<migration_bitmap_sync>> memory_global_dirty_log_sync(last_stage);
+     *   - migration/ram.c|3600| <<colo_incoming_start_dirty_log>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|3872| <<colo_flush_ram_cache>> memory_global_dirty_log_sync(false);
+     */
     memory_global_dirty_log_sync(last_stage);
 
     WITH_QEMU_LOCK_GUARD(&rs->bitmap_mutex) {
@@ -1062,6 +1081,13 @@ static void migration_bitmap_sync(RAMState *rs, bool last_stage)
     }
 }
 
+/*
+ * 在以下调用migration_bitmap_sync_precopy():
+ *   - migration/cpu-throttle.c|151| <<cpu_throttle_dirty_sync_timer_tick>> migration_bitmap_sync_precopy(false);
+ *   - migration/ram.c|2733| <<ram_init_bitmaps>> migration_bitmap_sync_precopy(false);
+ *   - migration/ram.c|3212| <<ram_save_complete>> migration_bitmap_sync_precopy(true);
+ *   - migration/ram.c|3300| <<ram_state_pending_exact>> migration_bitmap_sync_precopy(false);
+ */
 void migration_bitmap_sync_precopy(bool last_stage)
 {
     Error *local_err = NULL;
@@ -2184,6 +2210,11 @@ static int ram_save_host_page(RAMState *rs, PageSearchStatus *pss)
  * On systems where host-page-size > target-page-size it will send all the
  * pages in a host page that are dirty.
  */
+/*
+ * 在以下调用ram_find_and_save_block():
+ *   - migration/ram.c|3130| <<ram_save_iterate>> pages = ram_find_and_save_block(rs);
+ *   - migration/ram.c|3228| <<ram_save_complete>> pages = ram_find_and_save_block(rs);
+ */
 static int ram_find_and_save_block(RAMState *rs)
 {
     PageSearchStatus *pss = &rs->pss[RAM_CHANNEL_PRECOPY];
@@ -2328,6 +2359,14 @@ static void ram_save_cleanup(void *opaque)
              * memory_global_dirty_log_stop will assert that
              * memory_global_dirty_log_start/stop used in pairs
              */
+            /*
+	     * 在以下使用memory_global_dirty_log_stop():
+	     *   - hw/i386/xen/xen-hvm.c|704| <<qmp_xen_set_global_dirty_log>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+	     *   - migration/dirtyrate.c|103| <<global_dirty_log_change>> memory_global_dirty_log_stop(flag);
+	     *   - migration/dirtyrate.c|118| <<global_dirty_log_sync>> memory_global_dirty_log_stop(flag);
+	     *   - migration/ram.c|2354| <<ram_save_cleanup>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+	     *   - migration/ram.c|3622| <<colo_release_ram_cache>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+	     */
             memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
         }
     }
@@ -2730,6 +2769,13 @@ static bool ram_init_bitmaps(RAMState *rs, Error **errp)
             if (!ret) {
                 goto out_unlock;
             }
+            /*
+	     * 在以下调用migration_bitmap_sync_precopy():
+             *   - migration/cpu-throttle.c|151| <<cpu_throttle_dirty_sync_timer_tick>> migration_bitmap_sync_precopy(false);
+             *   - migration/ram.c|2733| <<ram_init_bitmaps>> migration_bitmap_sync_precopy(false);
+	     *   - migration/ram.c|3212| <<ram_save_complete>> migration_bitmap_sync_precopy(true);
+             *   - migration/ram.c|3300| <<ram_state_pending_exact>> migration_bitmap_sync_precopy(false);
+	     */
             migration_bitmap_sync_precopy(false);
         }
     }
@@ -3189,6 +3235,22 @@ out:
     return done;
 }
 
+/*
+ * 4480 static SaveVMHandlers savevm_ram_handlers = {
+ * 4481     .save_setup = ram_save_setup,
+ * 4482     .save_live_iterate = ram_save_iterate,
+ * 4483     .save_live_complete_postcopy = ram_save_complete,
+ * 4484     .save_live_complete_precopy = ram_save_complete,
+ * 4485     .has_postcopy = ram_has_postcopy,
+ * 4486     .state_pending_exact = ram_state_pending_exact,
+ * 4487     .state_pending_estimate = ram_state_pending_estimate,
+ * 4488     .load_state = ram_load,
+ * 4489     .save_cleanup = ram_save_cleanup,
+ * 4490     .load_setup = ram_load_setup,
+ * 4491     .load_cleanup = ram_load_cleanup,
+ * 4492     .resume_prepare = ram_resume_prepare,
+ * 4493 };
+ */
 /**
  * ram_save_complete: function called to send the remaining amount of ram
  *
@@ -3209,6 +3271,13 @@ static int ram_save_complete(QEMUFile *f, void *opaque)
 
     WITH_RCU_READ_LOCK_GUARD() {
         if (!migration_in_postcopy()) {
+            /*
+	     * 在以下调用migration_bitmap_sync_precopy():
+             *   - migration/cpu-throttle.c|151| <<cpu_throttle_dirty_sync_timer_tick>> migration_bitmap_sync_precopy(false);
+             *   - migration/ram.c|2733| <<ram_init_bitmaps>> migration_bitmap_sync_precopy(false);
+             *   - migration/ram.c|3212| <<ram_save_complete>> migration_bitmap_sync_precopy(true);
+             *   - migration/ram.c|3300| <<ram_state_pending_exact>> migration_bitmap_sync_precopy(false);
+	     */
             migration_bitmap_sync_precopy(true);
         }
 
@@ -3297,6 +3366,13 @@ static void ram_state_pending_exact(void *opaque, uint64_t *must_precopy,
     if (!migration_in_postcopy()) {
         bql_lock();
         WITH_RCU_READ_LOCK_GUARD() {
+            /*
+	     * 在以下调用migration_bitmap_sync_precopy():
+             *   - migration/cpu-throttle.c|151| <<cpu_throttle_dirty_sync_timer_tick>> migration_bitmap_sync_precopy(false);
+             *   - migration/ram.c|2733| <<ram_init_bitmaps>> migration_bitmap_sync_precopy(false);
+             *   - migration/ram.c|3212| <<ram_save_complete>> migration_bitmap_sync_precopy(true);
+             *   - migration/ram.c|3300| <<ram_state_pending_exact>> migration_bitmap_sync_precopy(false);
+	     */
             migration_bitmap_sync_precopy(false);
         }
         bql_unlock();
@@ -3537,6 +3613,14 @@ void colo_incoming_start_dirty_log(void)
     bql_lock();
     qemu_mutex_lock_ramlist();
 
+    /*
+     * 在以下使用memory_global_dirty_log_sync():
+     *   - migration/dirtyrate.c|116| <<global_dirty_log_sync>> memory_global_dirty_log_sync(false);
+     *   - migration/dirtyrate.c|632| <<calculate_dirtyrate_dirty_bitmap>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|1041| <<migration_bitmap_sync>> memory_global_dirty_log_sync(last_stage);
+     *   - migration/ram.c|3600| <<colo_incoming_start_dirty_log>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|3872| <<colo_flush_ram_cache>> memory_global_dirty_log_sync(false);
+     */
     memory_global_dirty_log_sync(false);
     WITH_RCU_READ_LOCK_GUARD() {
         RAMBLOCK_FOREACH_NOT_IGNORED(block) {
@@ -3559,6 +3643,14 @@ void colo_release_ram_cache(void)
 {
     RAMBlock *block;
 
+    /*
+     * 在以下使用memory_global_dirty_log_stop():
+     *   - hw/i386/xen/xen-hvm.c|704| <<qmp_xen_set_global_dirty_log>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+     *   - migration/dirtyrate.c|103| <<global_dirty_log_change>> memory_global_dirty_log_stop(flag);
+     *   - migration/dirtyrate.c|118| <<global_dirty_log_sync>> memory_global_dirty_log_stop(flag);
+     *   - migration/ram.c|2354| <<ram_save_cleanup>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+     *   - migration/ram.c|3622| <<colo_release_ram_cache>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+     */
     memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
     RAMBLOCK_FOREACH_NOT_IGNORED(block) {
         g_free(block->bmap);
@@ -3809,6 +3901,14 @@ void colo_flush_ram_cache(void)
     void *src_host;
     unsigned long offset = 0;
 
+    /*
+     * 在以下使用memory_global_dirty_log_sync():
+     *   - migration/dirtyrate.c|116| <<global_dirty_log_sync>> memory_global_dirty_log_sync(false);
+     *   - migration/dirtyrate.c|632| <<calculate_dirtyrate_dirty_bitmap>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|1041| <<migration_bitmap_sync>> memory_global_dirty_log_sync(last_stage);
+     *   - migration/ram.c|3600| <<colo_incoming_start_dirty_log>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|3872| <<colo_flush_ram_cache>> memory_global_dirty_log_sync(false);
+     */
     memory_global_dirty_log_sync(false);
     qemu_mutex_lock(&ram_state->bitmap_mutex);
     WITH_RCU_READ_LOCK_GUARD() {
@@ -3992,6 +4092,11 @@ static int parse_ramblock(QEMUFile *f, RAMBlock *block, ram_addr_t length)
     }
 
     if (length != block->used_length) {
+        /*
+	 * called by:
+	 *   - migration/ram.c|3995| <<parse_ramblock>> ret = qemu_ram_resize(block, length, &local_err);
+	 *   - system/memory.c|2476| <<memory_region_ram_resize>> qemu_ram_resize(mr->ram_block, newsize, errp);
+	 */
         ret = qemu_ram_resize(block, length, &local_err);
         if (local_err) {
             error_report_err(local_err);
@@ -4073,6 +4178,10 @@ static int parse_ramblocks(QEMUFile *f, ram_addr_t total_ram_bytes)
  *
  * @f: QEMUFile where to send the data
  */
+/*
+ * 在以下调用ram_load_precopy():
+ *   - migration/ram.c|4271| <<ram_load>> ret = ram_load_precopy(f);
+ */
 static int ram_load_precopy(QEMUFile *f)
 {
     MigrationIncomingState *mis = migration_incoming_get_current();
@@ -4263,6 +4372,9 @@ static int ram_load(QEMUFile *f, void *opaque, int version_id)
              */
             ret = ram_load_postcopy(f, RAM_CHANNEL_PRECOPY);
         } else {
+            /*
+	     * 只在此处调用ram_load_precopy()
+	     */
             ret = ram_load_precopy(f);
         }
     }
@@ -4469,6 +4581,11 @@ static void ram_mig_ram_block_resized(RAMBlockNotifier *n, void *host,
         migrate_set_error(migrate_get_current(), err);
         error_free(err);
 
+        /*
+	 * 在以下使用migration_cancel():
+	 *   - migration/migration.c|2314| <<qmp_migrate_cancel>> migration_cancel();
+	 *   - migration/ram.c|4544| <<ram_mig_ram_block_resized>> migration_cancel();
+	 */
         migration_cancel();
     }
 
diff --git a/migration/ram.h b/migration/ram.h
index 921c39a2c..ee2e81c12 100644
--- a/migration/ram.h
+++ b/migration/ram.h
@@ -63,11 +63,51 @@
 
 extern XBZRLECacheStats xbzrle_counters;
 
+/*
+ * 在以下使用RAMBLOCK_FOREACH_NOT_IGNORED():
+ *   - migration/ram.c|214| <<foreach_not_ignored_block>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|227| <<ramblock_recv_map_init>> RAMBLOCK_FOREACH_NOT_IGNORED(rb) {
+ *   - migration/ram.c|913| <<ram_pagesize_summary>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1034| <<migration_bitmap_sync>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1466| <<ram_write_tracking_compatible>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1565| <<ram_write_tracking_prepare>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1636| <<ram_write_tracking_start>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1664| <<ram_write_tracking_start>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1689| <<ram_write_tracking_stop>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2258| <<ram_bytes_total>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2306| <<ram_bitmaps_destroy>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2364| <<ram_postcopy_migrated_memory_release>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2431| <<postcopy_each_ram_send_discard>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2684| <<ram_list_init_bitmaps>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2713| <<migration_bitmap_clear_discarded_pages>> RAMBLOCK_FOREACH_NOT_IGNORED(rb) {
+ *   - migration/ram.c|2781| <<ram_state_resume_prepare>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3492| <<colo_init_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3499| <<colo_init_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3520| <<colo_init_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3542| <<colo_incoming_start_dirty_log>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3563| <<colo_release_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3569| <<colo_release_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3600| <<ram_load_cleanup>> RAMBLOCK_FOREACH_NOT_IGNORED(rb) {
+ *   - migration/ram.c|3606| <<ram_load_cleanup>> RAMBLOCK_FOREACH_NOT_IGNORED(rb) {
+ *   - migration/ram.c|3815| <<colo_flush_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|4282| <<ram_has_postcopy>> RAMBLOCK_FOREACH_NOT_IGNORED(rb) {
+ *   - migration/ram.c|4302| <<ram_dirty_bitmap_sync_all>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ */
 /* Should be holding either ram_list.mutex, or the RCU lock. */
 #define RAMBLOCK_FOREACH_NOT_IGNORED(block)            \
     INTERNAL_RAMBLOCK_FOREACH(block)                   \
         if (migrate_ram_is_ignored(block)) {} else
 
+/*
+ * 在以下使用RAMBLOCK_FOREACH_MIGRATABLE():
+ *   - migration/dirtyrate.c|494| <<record_ramblock_hash_info>> RAMBLOCK_FOREACH_MIGRATABLE(block) {
+ *   - migration/dirtyrate.c|506| <<record_ramblock_hash_info>> RAMBLOCK_FOREACH_MIGRATABLE(block) {
+ *   - migration/dirtyrate.c|574| <<compare_page_hash_info>> RAMBLOCK_FOREACH_MIGRATABLE(block) {
+ *   - migration/dirtyrate.c|608| <<dirtyrate_manual_reset_protect>> RAMBLOCK_FOREACH_MIGRATABLE(block) {
+ *   - migration/ram.c|2245| <<ram_bytes_total_with_ignored>> RAMBLOCK_FOREACH_MIGRATABLE(block) {
+ *   - migration/ram.c|2973| <<ram_save_setup>> RAMBLOCK_FOREACH_MIGRATABLE(block) {
+ *   - migration/ram.c|3054| <<ram_save_file_bmap>> RAMBLOCK_FOREACH_MIGRATABLE(block) {
+ */
 #define RAMBLOCK_FOREACH_MIGRATABLE(block)             \
     INTERNAL_RAMBLOCK_FOREACH(block)                   \
         if (!qemu_ram_is_migratable(block)) {} else
diff --git a/net/slirp.c b/net/slirp.c
index 9657e86a8..94153ffef 100644
--- a/net/slirp.c
+++ b/net/slirp.c
@@ -423,6 +423,10 @@ static SaveVMHandlers savevm_slirp_state = {
     .load_state = net_slirp_state_load,
 };
 
+/*
+ * 在以下调用net_slirp_init():
+ *   - net/slirp.c|1223| <<net_init_slirp>> ret = net_slirp_init(peer, "user", name, user->q_restrict,
+ */
 static int net_slirp_init(NetClientState *peer, const char *model,
                           const char *name, int restricted,
                           bool ipv4, const char *vnetwork, const char *vhost,
@@ -672,6 +676,10 @@ static int net_slirp_init(NetClientState *peer, const char *model,
     register_savevm_live("slirp", VMSTATE_INSTANCE_ID_ANY,
                          slirp_state_version(), &savevm_slirp_state, s->slirp);
 
+    /*
+     * 只在以下调用main_loop_poll_add_notifier():
+     *   - net/slirp.c|676| <<net_slirp_init>> main_loop_poll_add_notifier(&s->poll_notifier);
+     */
     s->poll_notifier.notify = net_slirp_poll_notify;
     main_loop_poll_add_notifier(&s->poll_notifier);
 
diff --git a/system/cpus.c b/system/cpus.c
index 2cc5f887a..0640b0802 100644
--- a/system/cpus.c
+++ b/system/cpus.c
@@ -331,6 +331,15 @@ bool cpu_can_run(CPUState *cpu)
     return true;
 }
 
+/*
+ * 在以下使用cpu_handle_guest_debug():
+ *   - accel/hvf/hvf-accel-ops.c|450| <<hvf_cpu_thread_fn>> cpu_handle_guest_debug(cpu);
+ *   - accel/kvm/kvm-accel-ops.c|75| <<kvm_vcpu_thread_fn>> cpu_handle_guest_debug(cpu);
+ *   - accel/tcg/tcg-accel-ops-mttcg.c|98| <<mttcg_cpu_thread_fn>> cpu_handle_guest_debug(cpu);
+ *   - accel/tcg/tcg-accel-ops-rr.c|268| <<rr_cpu_thread_fn>> cpu_handle_guest_debug(cpu);
+ *   - target/i386/nvmm/nvmm-accel-ops.c|48| <<qemu_nvmm_cpu_thread_fn>> cpu_handle_guest_debug(cpu);
+ *   - target/i386/whpx/whpx-accel-ops.c|48| <<whpx_cpu_thread_fn>> cpu_handle_guest_debug(cpu);
+ */
 void cpu_handle_guest_debug(CPUState *cpu)
 {
     if (replay_running_debug()) {
@@ -442,15 +451,45 @@ static void qemu_cpu_stop(CPUState *cpu, bool exit)
     qemu_cond_broadcast(&qemu_pause_cond);
 }
 
+/*
+ * QEMU-9.2的callstack.
+ * (gdb) bt
+ * #0  kvm_arch_put_registers (cpu=0x555557721960, level=3, errp=0x7fffedb02580) at ../target/i386/kvm/kvm.c:5237
+ * #1  0x0000555555edb672 in do_kvm_cpu_synchronize_post_init (cpu=0x555557721960, arg=...) at ../accel/kvm/kvm-all.c:2905
+ * #2  0x00005555558841ea in process_queued_cpu_work (cpu=0x555557721960) at ../cpu-common.c:375
+ * #3  0x0000555555bd2d4f in qemu_wait_io_event_common (cpu=0x555557721960) at ../system/cpus.c:456
+ * #4  0x0000555555bd2de8 in qemu_wait_io_event (cpu=0x555557721960) at ../system/cpus.c:474
+ * #5  0x0000555555edf7e1 in kvm_vcpu_thread_fn (arg=0x555557721960) at ../accel/kvm/kvm-accel-ops.c:55
+ * #6  0x000055555617e61f in qemu_thread_start (args=0x55555772bbe0) at ../util/qemu-thread-posix.c:541
+ * #7  0x00007ffff68a91da in start_thread () from /lib64/libpthread.so.0
+ * #8  0x00007ffff4bda8d3 in clone () from /lib64/libc.so.6
+ *
+ * 在以下调用qemu_wait_io_event_common():
+ *   - accel/tcg/tcg-accel-ops-rr.c|120| <<rr_wait_io_event>> qemu_wait_io_event_common(cpu);
+ *   - accel/tcg/tcg-accel-ops-rr.c|206| <<rr_cpu_thread_fn>> qemu_wait_io_event_common(cpu);
+ *   - system/cpus.c|469| <<qemu_wait_io_event>> qemu_wait_io_event_common(cpu);
+ *   - target/i386/nvmm/nvmm-accel-ops.c|54| <<qemu_nvmm_cpu_thread_fn>> qemu_wait_io_event_common(cpu);
+ *   - target/i386/whpx/whpx-accel-ops.c|54| <<whpx_cpu_thread_fn>> qemu_wait_io_event_common(cpu);
+ */
 void qemu_wait_io_event_common(CPUState *cpu)
 {
     qatomic_set_mb(&cpu->thread_kicked, false);
     if (cpu->stop) {
         qemu_cpu_stop(cpu, false);
     }
+    /*
+     * 非常重要的函数
+     */
     process_queued_cpu_work(cpu);
 }
 
+/*
+ * 在以下使用qemu_wait_io_event():
+ *   - accel/dummy-cpus.c|59| <<dummy_cpu_thread_fn>> qemu_wait_io_event(cpu);
+ *   - accel/hvf/hvf-accel-ops.c|453| <<hvf_cpu_thread_fn>> qemu_wait_io_event(cpu);
+ *   - accel/kvm/kvm-accel-ops.c|56| <<kvm_vcpu_thread_fn>> qemu_wait_io_event(cpu);
+ *   - accel/tcg/tcg-accel-ops-mttcg.c|117| <<mttcg_cpu_thread_fn>> qemu_wait_io_event(cpu);
+ */
 void qemu_wait_io_event(CPUState *cpu)
 {
     bool slept = false;
@@ -466,6 +505,14 @@ void qemu_wait_io_event(CPUState *cpu)
         qemu_plugin_vcpu_resume_cb(cpu);
     }
 
+    /*
+     * 在以下调用qemu_wait_io_event_common():
+     *   - accel/tcg/tcg-accel-ops-rr.c|120| <<rr_wait_io_event>> qemu_wait_io_event_common(cpu);
+     *   - accel/tcg/tcg-accel-ops-rr.c|206| <<rr_cpu_thread_fn>> qemu_wait_io_event_common(cpu);
+     *   - system/cpus.c|469| <<qemu_wait_io_event>> qemu_wait_io_event_common(cpu);
+     *   - target/i386/nvmm/nvmm-accel-ops.c|54| <<qemu_nvmm_cpu_thread_fn>> qemu_wait_io_event_common(cpu);
+     *   - target/i386/whpx/whpx-accel-ops.c|54| <<whpx_cpu_thread_fn>> qemu_wait_io_event_common(cpu);
+     */
     qemu_wait_io_event_common(cpu);
 }
 
diff --git a/system/dirtylimit.c b/system/dirtylimit.c
index 7dedef8dd..e2850e71e 100644
--- a/system/dirtylimit.c
+++ b/system/dirtylimit.c
@@ -84,6 +84,13 @@ static void vcpu_dirty_rate_stat_collect(void)
         period = migrate_vcpu_dirty_limit_period();
     }
 
+    /*
+     * 在以下使用vcpu_calculate_dirtyrate():
+     *   - migration/dirtyrate.c|674| <<calculate_dirtyrate_dirty_ring>> DirtyStat.calc_time_ms =
+     *           vcpu_calculate_dirtyrate(config.calc_time_ms, &DirtyStat.dirty_ring, GLOBAL_DIRTY_DIRTY_RATE, true);
+     *   - system/dirtylimit.c|88| <<vcpu_dirty_rate_stat_collect>> vcpu_calculate_dirtyrate(period,
+     *           &stat, GLOBAL_DIRTY_LIMIT, false);
+     */
     /* calculate vcpu dirtyrate */
     vcpu_calculate_dirtyrate(period,
                               &stat,
diff --git a/system/main.c b/system/main.c
index 1c0220673..96d37aa3a 100644
--- a/system/main.c
+++ b/system/main.c
@@ -41,12 +41,21 @@
 #include <CoreFoundation/CoreFoundation.h>
 #endif
 
+/*
+ * 在以下使用qemu_default_main():
+ *   - system/main.c|77| <<main>> qemu_thread_create(&main_loop_thread,
+ *      "qemu_main", qemu_default_main, NULL, QEMU_THREAD_DETACHED);
+ *   - system/main.c|80| <<main>> qemu_default_main(NULL);
+ */
 static void *qemu_default_main(void *opaque)
 {
     int status;
 
     replay_mutex_lock();
     bql_lock();
+    /*
+     * 只在此处调用qemu_main_loop()
+     */
     status = qemu_main_loop();
     qemu_cleanup(status);
     bql_unlock();
@@ -55,6 +64,14 @@ static void *qemu_default_main(void *opaque)
     exit(status);
 }
 
+/*
+ * 在以下使用qemu_main函数指针:
+ *   - system/main.c|66| <<global>> int (*qemu_main)(void ) = os_darwin_cfrunloop_main;
+ *   - system/main.c|74| <<main>> if (qemu_main) {
+ *   - system/main.c|78| <<main>> return qemu_main();
+ *   - ui/gtk.c|2490| <<gtk_display_init>> qemu_main = NULL;
+ *   - ui/sdl2.c|945| <<sdl2_display_init>> qemu_main = NULL;
+ */
 int (*qemu_main)(void);
 
 #ifdef CONFIG_DARWIN
@@ -68,9 +85,22 @@ int (*qemu_main)(void) = os_darwin_cfrunloop_main;
 
 int main(int argc, char **argv)
 {
+    /*
+     * 在以下调用qemu_init():
+     *   - system/main.c|71| <<main>> qemu_init(argc, argv);
+     *   - tests/qtest/fuzz/fuzz.c|228| <<LLVMFuzzerInitialize>> qemu_init(result.we_wordc, result.we_wordv);
+     */
     qemu_init(argc, argv);
     bql_unlock();
     replay_mutex_unlock();
+    /*
+     * 在以下使用qemu_main函数指针:
+     *   - system/main.c|66| <<global>> int (*qemu_main)(void ) = os_darwin_cfrunloop_main;
+     *   - system/main.c|74| <<main>> if (qemu_main) {
+     *   - system/main.c|78| <<main>> return qemu_main();
+     *   - ui/gtk.c|2490| <<gtk_display_init>> qemu_main = NULL;
+     *   - ui/sdl2.c|945| <<sdl2_display_init>> qemu_main = NULL;
+     */
     if (qemu_main) {
         QemuThread main_loop_thread;
         qemu_thread_create(&main_loop_thread, "qemu_main",
diff --git a/system/memory.c b/system/memory.c
index 4c829793a..adce705ce 100644
--- a/system/memory.c
+++ b/system/memory.c
@@ -37,9 +37,62 @@
 
 //#define DEBUG_UNASSIGNED
 
+/*
+ * 在以下使用memory_region_transaction_depth:
+ *   - system/memory.c|1166| <<memory_region_transaction_begin>> ++memory_region_transaction_depth;
+ *   - system/memory.c|1173| <<memory_region_transaction_commit>> assert(memory_region_transaction_depth);
+ *   - system/memory.c|1176| <<memory_region_transaction_commit>> --memory_region_transaction_depth;
+ *   - system/memory.c|1177| <<memory_region_transaction_commit>> if (!memory_region_transaction_depth) {
+ */
 static unsigned memory_region_transaction_depth;
+/*
+ * 在以下修改memory_region_update_pending:
+ *   - system/memory.c|1187| <<memory_region_transaction_commit>> memory_region_update_pending = false;
+ *   - system/memory.c|2294| <<memory_region_set_log>> memory_region_update_pending |= mr->enabled;
+ *   - system/memory.c|2424| <<memory_region_set_readonly>> memory_region_update_pending |= mr->enabled;
+ *   - system/memory.c|2434| <<memory_region_set_nonvolatile>> memory_region_update_pending |= mr->enabled;
+ *   - system/memory.c|2444| <<memory_region_rom_device_set_romd>> memory_region_update_pending |= mr->enabled;
+ *   - system/memory.c|2693| <<memory_region_update_container_subregions>> memory_region_update_pending |= mr->enabled && subregion->enabled;
+ *   - system/memory.c|2743| <<memory_region_del_subregion>> memory_region_update_pending |= mr->enabled && subregion->enabled;
+ *   - system/memory.c|2754| <<memory_region_set_enabled>> memory_region_update_pending = true;
+ *   - system/memory.c|2770| <<memory_region_set_size>> memory_region_update_pending = true;
+ *   - system/memory.c|2806| <<memory_region_set_alias_offset>> memory_region_update_pending |= mr->enabled;
+ *   - system/memory.c|2818| <<memory_region_set_unmergeable>> memory_region_update_pending |= mr->enabled;
+ *   - system/memory.c|3017| <<memory_global_dirty_log_start>> memory_region_update_pending = true;
+ *   - system/memory.c|3033| <<memory_global_dirty_log_do_stop>> memory_region_update_pending = true;
+ * 在以下使用memory_region_update_pending:
+ *   - system/memory.c|1178| <<memory_region_transaction_commit>> if (memory_region_update_pending) {
+ */
 static bool memory_region_update_pending;
+/*
+ * 在以下使用ioeventfd_update_pending:
+ *   - system/memory.c|1188| <<memory_region_transaction_commit>> ioeventfd_update_pending = false;
+ *   - system/memory.c|1190| <<memory_region_transaction_commit>> } else if (ioeventfd_update_pending) {
+ *   - system/memory.c|1194| <<memory_region_transaction_commit>> ioeventfd_update_pending = false;
+ *   - system/memory.c|2638| <<memory_region_add_eventfd>> ioeventfd_update_pending |= mr->enabled;
+ *   - system/memory.c|2673| <<memory_region_del_eventfd>> ioeventfd_update_pending |= mr->enabled;
+ */
 static bool ioeventfd_update_pending;
+/*
+ * 在以下使用global_dirty_tracking:
+ *   - include/exec/ram_addr.h|392| <<cpu_physical_memory_set_dirty_lebitmap>> if (global_dirty_tracking) {
+ *   - include/exec/ram_addr.h|397| <<cpu_physical_memory_set_dirty_lebitmap>> global_dirty_tracking & GLOBAL_DIRTY_DIRTY_RATE)) {
+ *   - include/exec/ram_addr.h|421| <<cpu_physical_memory_set_dirty_lebitmap>> if (!global_dirty_tracking) {
+ *   - include/exec/ram_addr.h|433| <<cpu_physical_memory_set_dirty_lebitmap>> if (unlikely(global_dirty_tracking & GLOBAL_DIRTY_DIRTY_RATE)) {
+ *   - migration/ram.c|2348| <<ram_save_cleanup>> if (global_dirty_tracking & GLOBAL_DIRTY_MIGRATION) { 
+ *   - system/memory.c|1899| <<memory_region_get_dirty_log_mask>> if (global_dirty_tracking && ((rb && qemu_ram_is_migratable(rb)) ||
+ *   - system/memory.c|2980| <<memory_global_dirty_log_start>> flags &= ~global_dirty_tracking;
+ *   - system/memory.c|2985| <<memory_global_dirty_log_start>> old_flags = global_dirty_tracking;
+ *   - system/memory.c|2986| <<memory_global_dirty_log_start>> global_dirty_tracking |= flags;
+ *   - system/memory.c|2987| <<memory_global_dirty_log_start>> trace_global_dirty_changed(global_dirty_tracking);
+ *   - system/memory.c|2991| <<memory_global_dirty_log_start>> global_dirty_tracking &= ~flags;
+ *   - system/memory.c|2992| <<memory_global_dirty_log_start>> trace_global_dirty_changed(global_dirty_tracking);
+ *   - system/memory.c|3006| <<memory_global_dirty_log_do_stop>> assert((global_dirty_tracking & flags) == flags);
+ *   - system/memory.c|3007| <<memory_global_dirty_log_do_stop>> global_dirty_tracking &= ~flags;
+ *   - system/memory.c|3009| <<memory_global_dirty_log_do_stop>> trace_global_dirty_changed(global_dirty_tracking);
+ *   - system/memory.c|3011| <<memory_global_dirty_log_do_stop>> if (!global_dirty_tracking) {
+ *   - system/memory.c|3075| <<listener_add_address_space>> if (global_dirty_tracking) {
+ */
 unsigned int global_dirty_tracking;
 
 static QTAILQ_HEAD(, MemoryListener) memory_listeners
@@ -839,6 +892,12 @@ FlatView *address_space_get_flatview(AddressSpace *as)
     return view;
 }
 
+/*
+ * 在以下使用address_space_update_ioeventfds():
+ *   - system/memory.c|1185| <<memory_region_transaction_commit>> address_space_update_ioeventfds(as);
+ *   - system/memory.c|1192| <<memory_region_transaction_commit>> address_space_update_ioeventfds(as);
+ *   - system/memory.c|3277| <<address_space_init>> address_space_update_ioeventfds(as);
+ */
 static void address_space_update_ioeventfds(AddressSpace *as)
 {
     FlatView *view;
@@ -973,6 +1032,11 @@ flat_range_coalesced_io_notify_listener_add_del(FlatRange *fr,
     }
 }
 
+/*
+ * 在以下使用address_space_update_topology_pass():
+ *   - system/memory.c|1170| <<address_space_set_flatview>> address_space_update_topology_pass(as, old_view2, new_view, false);
+ *   - system/memory.c|1171| <<address_space_set_flatview>> address_space_update_topology_pass(as, old_view2, new_view, true);
+ */
 static void address_space_update_topology_pass(AddressSpace *as,
                                                const FlatView *old_view,
                                                const FlatView *new_view,
@@ -1084,6 +1148,11 @@ static void flatviews_reset(void)
     }
 }
 
+/*
+ * 在以下使用address_space_set_flatview():
+ *   - system/memory.c|1199| <<address_space_update_topology>> address_space_set_flatview(as);
+ *   - system/memory.c|1237| <<memory_region_transaction_commit>> address_space_set_flatview(as);
+ */
 static void address_space_set_flatview(AddressSpace *as)
 {
     FlatView *old_view = address_space_to_flatview(as);
@@ -1143,9 +1212,28 @@ static void address_space_update_topology(AddressSpace *as)
 void memory_region_transaction_begin(void)
 {
     qemu_flush_coalesced_mmio_buffer();
+    /*
+     * 在以下使用memory_region_transaction_depth:
+     *   - system/memory.c|1166| <<memory_region_transaction_begin>> ++memory_region_transaction_depth;
+     *   - system/memory.c|1173| <<memory_region_transaction_commit>> assert(memory_region_transaction_depth);
+     *   - system/memory.c|1176| <<memory_region_transaction_commit>> --memory_region_transaction_depth;
+     *   - system/memory.c|1177| <<memory_region_transaction_commit>> if (!memory_region_transaction_depth) {
+     */
     ++memory_region_transaction_depth;
 }
 
+/*
+ * (gdb) bt
+ * #0  memory_global_dirty_log_do_stop (flags=2) at ../system/memory.c:2995
+ * #1  0x0000555555cbbc10 in memory_global_dirty_log_stop (flags=<optimized out>) at ../system/memory.c:3054
+ * #2  0x0000555555b20a57 in global_dirty_log_sync (flag=2, one_shot=<optimized out>) at ../migration/dirtyrate.c:119
+ * #3  0x0000555555b219b0 in calculate_dirtyrate_dirty_bitmap (config=..., config=...) at ../migration/dirtyrate.c:656
+ * #4  calculate_dirtyrate (config=...) at ../migration/dirtyrate.c:723
+ * #5  get_dirtyrate_thread (arg=arg@entry=0x555556f4d6f0 <config>) at ../migration/dirtyrate.c:746
+ * #6  0x0000555555ec4839 in qemu_thread_start (args=<optimized out>) at ../util/qemu-thread-posix.c:541
+ * #7  0x00007ffff69201da in start_thread () from /lib64/libpthread.so.0
+ * #8  0x00007ffff657a8d3 in clone () from /lib64/libc.so.6
+ */
 void memory_region_transaction_commit(void)
 {
     AddressSpace *as;
@@ -1153,6 +1241,13 @@ void memory_region_transaction_commit(void)
     assert(memory_region_transaction_depth);
     assert(bql_locked());
 
+    /*
+     * 在以下使用memory_region_transaction_depth:
+     *   - system/memory.c|1166| <<memory_region_transaction_begin>> ++memory_region_transaction_depth;
+     *   - system/memory.c|1173| <<memory_region_transaction_commit>> assert(memory_region_transaction_depth);
+     *   - system/memory.c|1176| <<memory_region_transaction_commit>> --memory_region_transaction_depth;
+     *   - system/memory.c|1177| <<memory_region_transaction_commit>> if (!memory_region_transaction_depth) {
+     */
     --memory_region_transaction_depth;
     if (!memory_region_transaction_depth) {
         if (memory_region_update_pending) {
@@ -1162,6 +1257,12 @@ void memory_region_transaction_commit(void)
 
             QTAILQ_FOREACH(as, &address_spaces, address_spaces_link) {
                 address_space_set_flatview(as);
+                /*
+		 * 在以下使用address_space_update_ioeventfds():
+                 *   - system/memory.c|1185| <<memory_region_transaction_commit>> address_space_update_ioeventfds(as);
+		 *   - system/memory.c|1192| <<memory_region_transaction_commit>> address_space_update_ioeventfds(as);
+		 *   - system/memory.c|3277| <<address_space_init>> address_space_update_ioeventfds(as);
+		 */
                 address_space_update_ioeventfds(as);
             }
             memory_region_update_pending = false;
@@ -1610,6 +1711,10 @@ bool memory_region_init_ram_flags_nomigrate(MemoryRegion *mr,
     return true;
 }
 
+/*
+ * 在以下使用memory_region_init_resizeable_ram():
+ *   - hw/core/loader.c|1025| <<rom_set_mr>> memory_region_init_resizeable_ram(rom->mr, owner, name,
+ */
 bool memory_region_init_resizeable_ram(MemoryRegion *mr,
                                        Object *owner,
                                        const char *name,
@@ -2469,10 +2574,23 @@ ram_addr_t memory_region_get_ram_addr(MemoryRegion *mr)
     return mr->ram_block ? mr->ram_block->offset : RAM_ADDR_INVALID;
 }
 
+/*
+ * 在以下调用memory_region_ram_resize():
+ *   - hw/arm/virt-acpi-build.c|1035| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+ *   - hw/i386/acpi-build.c|2642| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+ *   - hw/loongarch/virt-acpi-build.c|644| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+ *   - hw/nvram/fw_cfg.c|631| <<fw_cfg_update_mr>> memory_region_ram_resize(mr, size, &error_abort);
+ *   - hw/riscv/virt-acpi-build.c|746| <<acpi_ram_update>> memory_region_ram_resize(mr, size, &error_abort);
+ */
 void memory_region_ram_resize(MemoryRegion *mr, ram_addr_t newsize, Error **errp)
 {
     assert(mr->ram_block);
 
+    /*
+     * called by:
+     *   - migration/ram.c|3995| <<parse_ramblock>> ret = qemu_ram_resize(block, length, &local_err);
+     *   - system/memory.c|2476| <<memory_region_ram_resize>> qemu_ram_resize(mr->ram_block, newsize, errp);
+     */
     qemu_ram_resize(mr->ram_block, newsize, errp);
 }
 
@@ -2598,6 +2716,14 @@ void memory_region_add_eventfd(MemoryRegion *mr,
     memmove(&mr->ioeventfds[i+1], &mr->ioeventfds[i],
             sizeof(*mr->ioeventfds) * (mr->ioeventfd_nb-1 - i));
     mr->ioeventfds[i] = mrfd;
+    /*
+     * 在以下使用ioeventfd_update_pending:
+     *   - system/memory.c|1188| <<memory_region_transaction_commit>> ioeventfd_update_pending = false;
+     *   - system/memory.c|1190| <<memory_region_transaction_commit>> } else if (ioeventfd_update_pending) {
+     *   - system/memory.c|1194| <<memory_region_transaction_commit>> ioeventfd_update_pending = false;
+     *   - system/memory.c|2638| <<memory_region_add_eventfd>> ioeventfd_update_pending |= mr->enabled;
+     *   - system/memory.c|2673| <<memory_region_del_eventfd>> ioeventfd_update_pending |= mr->enabled;
+     */
     ioeventfd_update_pending |= mr->enabled;
     memory_region_transaction_commit();
 }
@@ -2633,6 +2759,14 @@ void memory_region_del_eventfd(MemoryRegion *mr,
     --mr->ioeventfd_nb;
     mr->ioeventfds = g_realloc(mr->ioeventfds,
                                   sizeof(*mr->ioeventfds)*mr->ioeventfd_nb + 1);
+    /*
+     * 在以下使用ioeventfd_update_pending:
+     *   - system/memory.c|1188| <<memory_region_transaction_commit>> ioeventfd_update_pending = false;
+     *   - system/memory.c|1190| <<memory_region_transaction_commit>> } else if (ioeventfd_update_pending) {
+     *   - system/memory.c|1194| <<memory_region_transaction_commit>> ioeventfd_update_pending = false;
+     *   - system/memory.c|2638| <<memory_region_add_eventfd>> ioeventfd_update_pending |= mr->enabled;
+     *   - system/memory.c|2673| <<memory_region_del_eventfd>> ioeventfd_update_pending |= mr->enabled;
+     */
     ioeventfd_update_pending |= mr->enabled;
     memory_region_transaction_commit();
 }
@@ -2907,6 +3041,14 @@ bool memory_region_present(MemoryRegion *container, hwaddr addr)
     return mr && mr != container;
 }
 
+/*
+ * 在以下使用memory_global_dirty_log_sync():
+ *   - migration/dirtyrate.c|116| <<global_dirty_log_sync>> memory_global_dirty_log_sync(false);
+ *   - migration/dirtyrate.c|632| <<calculate_dirtyrate_dirty_bitmap>> memory_global_dirty_log_sync(false);
+ *   - migration/ram.c|1041| <<migration_bitmap_sync>> memory_global_dirty_log_sync(last_stage);
+ *   - migration/ram.c|3600| <<colo_incoming_start_dirty_log>> memory_global_dirty_log_sync(false);
+ *   - migration/ram.c|3872| <<colo_flush_ram_cache>> memory_global_dirty_log_sync(false);
+ */
 void memory_global_dirty_log_sync(bool last_stage)
 {
     memory_region_sync_dirty_bitmap(NULL, last_stage);
@@ -2983,6 +3125,18 @@ bool memory_global_dirty_log_start(unsigned int flags, Error **errp)
     return true;
 }
 
+/*
+ * (gdb) bt
+ * #0  memory_global_dirty_log_do_stop (flags=2) at ../system/memory.c:2995
+ * #1  0x0000555555cbbc10 in memory_global_dirty_log_stop (flags=<optimized out>) at ../system/memory.c:3054
+ * #2  0x0000555555b20a57 in global_dirty_log_sync (flag=2, one_shot=<optimized out>) at ../migration/dirtyrate.c:119
+ * #3  0x0000555555b219b0 in calculate_dirtyrate_dirty_bitmap (config=..., config=...) at ../migration/dirtyrate.c:656
+ * #4  calculate_dirtyrate (config=...) at ../migration/dirtyrate.c:723
+ * #5  get_dirtyrate_thread (arg=arg@entry=0x555556f4d6f0 <config>) at ../migration/dirtyrate.c:746
+ * #6  0x0000555555ec4839 in qemu_thread_start (args=<optimized out>) at ../util/qemu-thread-posix.c:541
+ * #7  0x00007ffff69201da in start_thread () from /lib64/libpthread.so.0
+ * #8  0x00007ffff657a8d3 in clone () from /lib64/libc.so.6
+ */
 static void memory_global_dirty_log_do_stop(unsigned int flags)
 {
     assert(flags && !(flags & (~GLOBAL_DIRTY_MASK)));
@@ -3026,6 +3180,14 @@ static void memory_vm_change_state_handler(void *opaque, bool running,
     }
 }
 
+/*
+ * 在以下使用memory_global_dirty_log_stop():
+ *   - hw/i386/xen/xen-hvm.c|704| <<qmp_xen_set_global_dirty_log>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+ *   - migration/dirtyrate.c|103| <<global_dirty_log_change>> memory_global_dirty_log_stop(flag);
+ *   - migration/dirtyrate.c|118| <<global_dirty_log_sync>> memory_global_dirty_log_stop(flag);
+ *   - migration/ram.c|2354| <<ram_save_cleanup>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+ *   - migration/ram.c|3622| <<colo_release_ram_cache>> memory_global_dirty_log_stop(GLOBAL_DIRTY_MIGRATION);
+ */
 void memory_global_dirty_log_stop(unsigned int flags)
 {
     if (!runstate_is_running()) {
diff --git a/system/physmem.c b/system/physmem.c
index 333a5eb94..3997d3300 100644
--- a/system/physmem.c
+++ b/system/physmem.c
@@ -90,6 +90,44 @@
 
 //#define DEBUG_SUBPAGE
 
+/*
+ * 在以下使用ram_list:
+ *   - system/physmem.c|96| <<global>> RAMList ram_list = { .blocks = QLIST_HEAD_INITIALIZER(ram_list.blocks) };
+ *   - system/physmem.c|956| <<global>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - hw/core/numa.c|846| <<ram_block_notifier_add>> QLIST_INSERT_HEAD(&ram_list.ramblock_notifiers, n, next);
+ *   - hw/core/numa.c|868| <<ram_block_notify_add>> QLIST_FOREACH_SAFE(notifier, &ram_list.ramblock_notifiers, next, next) {
+ *   - hw/core/numa.c|880| <<ram_block_notify_remove>> QLIST_FOREACH_SAFE(notifier, &ram_list.ramblock_notifiers, next, next) {
+ *   - hw/core/numa.c|892| <<ram_block_notify_resize>> QLIST_FOREACH_SAFE(notifier, &ram_list.ramblock_notifiers, next, next) {
+ *   - include/exec/ram_addr.h|175| <<cpu_physical_memory_get_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - include/exec/ram_addr.h|216| <<cpu_physical_memory_all_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - include/exec/ram_addr.h|289| <<cpu_physical_memory_set_dirty_flag>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - include/exec/ram_addr.h|312| <<cpu_physical_memory_set_dirty_range>> blocks[i] = qatomic_rcu_read(&ram_list.dirty_memory[i]);
+ *   - include/exec/ram_addr.h|382| <<cpu_physical_memory_set_dirty_lebitmap>> qatomic_rcu_read(&ram_list.dirty_memory[i])->blocks;
+ *   - include/exec/ram_addr.h|506| <<cpu_physical_memory_sync_dirty_bitmap>> &ram_list.dirty_memory[DIRTY_MEMORY_MIGRATION])->blocks;
+ *   - include/exec/ramlist.h|61| <<INTERNAL_RAMBLOCK_FOREACH>> QLIST_FOREACH_RCU(block, &ram_list.blocks, next)
+ *   - migration/ram.c|1309| <<find_dirty_block>> pss->block = QLIST_FIRST_RCU(&ram_list.blocks);
+ *   - migration/ram.c|2205| <<ram_find_and_save_block>> rs->last_seen_block = QLIST_FIRST_RCU(&ram_list.blocks);
+ *   - migration/ram.c|2352| <<ram_state_reset>> rs->last_version = ram_list.version;
+ *   - migration/ram.c|3107| <<ram_save_iterate>> if (ram_list.version != rs->last_version) {
+ *   - migration/ram.c|3822| <<colo_flush_ram_cache>> block = QLIST_FIRST_RCU(&ram_list.blocks);
+ *   - system/physmem.c|826| <<qemu_get_ram_block>> block = qatomic_rcu_read(&ram_list.mru_block);
+ *   - system/physmem.c|856| <<qemu_get_ram_block>> ram_list.mru_block = block;
+ *   - system/physmem.c|900| <<cpu_physical_memory_test_and_clear_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - system/physmem.c|1155| <<qemu_mutex_lock_ramlist>> qemu_mutex_lock(&ram_list.mutex);
+ *   - system/physmem.c|1160| <<qemu_mutex_unlock_ramlist>> qemu_mutex_unlock(&ram_list.mutex);
+ *   - system/physmem.c|1499| <<find_ram_offset>> if (QLIST_EMPTY_RCU(&ram_list.blocks)) {
+ *   - system/physmem.c|1889| <<dirty_memory_extend>> unsigned int old_num_blocks = ram_list.num_dirty_blocks;
+ *   - system/physmem.c|1904| <<dirty_memory_extend>> old_blocks = qatomic_rcu_read(&ram_list.dirty_memory[i]);
+ *   - system/physmem.c|1917| <<dirty_memory_extend>> qatomic_rcu_set(&ram_list.dirty_memory[i], new_blocks);
+ *   - system/physmem.c|1924| <<dirty_memory_extend>> ram_list.num_dirty_blocks = new_num_blocks;
+ *   - system/physmem.c|2021| <<ram_block_add>> QLIST_INSERT_HEAD_RCU(&ram_list.blocks, new_block, next);
+ *   - system/physmem.c|2023| <<ram_block_add>> ram_list.mru_block = NULL;
+ *   - system/physmem.c|2027| <<ram_block_add>> ram_list.version++;
+ *   - system/physmem.c|2379| <<qemu_ram_free>> ram_list.mru_block = NULL;
+ *   - system/physmem.c|2382| <<qemu_ram_free>> ram_list.version++;
+ *   - system/physmem.c|2566| <<qemu_ram_block_from_host>> block = qatomic_rcu_read(&ram_list.mru_block);
+ *   - system/physmem.c|3381| <<cpu_exec_init_all>> qemu_mutex_init(&ram_list.mutex);
+ */
 /* ram_list is read under rcu_read_lock()/rcu_read_unlock().  Writes
  * are protected by the ramlist lock.
  */
@@ -1171,6 +1209,21 @@ GString *ram_block_format(void)
                            "Block Name", "PSize", "Offset", "Used", "Total",
                            "HVA", "RO");
 
+    /*
+     * 在以下使用RAMBLOCK_FOREACH():
+     *   - migration/postcopy-ram.c|418| <<postcopy_ram_supported_by_host>> RAMBLOCK_FOREACH(block) {
+     *   - system/physmem.c|830| <<qemu_get_ram_block>> RAMBLOCK_FOREACH(block) {
+     *   - system/physmem.c|1174| <<ram_block_format>> RAMBLOCK_FOREACH(block) {
+     *   - system/physmem.c|1503| <<find_ram_offset>> RAMBLOCK_FOREACH(block) {
+     *   - system/physmem.c|1515| <<find_ram_offset>> RAMBLOCK_FOREACH(next_block) {
+     *   - system/physmem.c|1648| <<qemu_ram_set_idstr>> RAMBLOCK_FOREACH(block) {
+     *   - system/physmem.c|1693| <<qemu_ram_pagesize_largest>> RAMBLOCK_FOREACH(block) {
+     *   - system/physmem.c|2010| <<ram_block_add>> RAMBLOCK_FOREACH(block) {
+     *   - system/physmem.c|2425| <<qemu_ram_remap>> RAMBLOCK_FOREACH(block) {
+     *   - system/physmem.c|2571| <<qemu_ram_block_from_host>> RAMBLOCK_FOREACH(block) {
+     *   - system/physmem.c|2602| <<qemu_ram_block_by_name>> RAMBLOCK_FOREACH(block) {
+     *   - system/physmem.c|3859| <<qemu_ram_foreach_block>> RAMBLOCK_FOREACH(block) {
+     */
     RAMBLOCK_FOREACH(block) {
         psize = size_to_str(block->page_size);
         g_string_append_printf(buf, "%24s %8s  0x%016" PRIx64 " 0x%016" PRIx64
@@ -1602,6 +1655,14 @@ void qemu_ram_set_uf_zeroable(RAMBlock *rb)
     rb->flags |= RAM_UF_ZEROPAGE;
 }
 
+/*
+ * 在以下使用qemu_ram_is_migratable():
+ *   - migration/ram.c|199| <<migrate_ram_is_ignored>> return !qemu_ram_is_migratable(block) ||
+ *   - migration/ram.c|3989| <<parse_ramblock>> if (!qemu_ram_is_migratable(block)) {
+ *   - migration/ram.h|73| <<RAMBLOCK_FOREACH_MIGRATABLE>> if (!qemu_ram_is_migratable(block)) {} else
+ *   - system/memory.c|1899| <<memory_region_get_dirty_log_mask>> if (global_dirty_tracking && ((rb && qemu_ram_is_migratable(rb)) ||
+ *   - system/physmem.c|4240| <<ram_block_add_cpr_blocker>> assert(qemu_ram_is_migratable(rb));
+ */
 bool qemu_ram_is_migratable(RAMBlock *rb)
 {
     return rb->flags & RAM_MIGRATABLE;
@@ -1715,6 +1776,69 @@ static int memory_try_enable_merging(void *addr, size_t len)
  * resize callback to update device state and/or add assertions to detect
  * misuse, if necessary.
  */
+/*
+ * (gdb) bt
+ * #0  qemu_ram_resize (block=0x555557fd1400, newsize=4096, errp=0x5555573a2118 <error_abort>) at ../system/physmem.c:1720
+ * #1  0x0000555555ebf3e9 in memory_region_ram_resize (mr=0x5555583a8ae0, newsize=4096, errp=0x5555573a2118 <error_abort>)
+ *     at ../system/memory.c:2476
+ * #2  0x0000555555d89e92 in acpi_ram_update (mr=0x5555583a8ae0, data=0x5555576eaf90) at ../hw/i386/acpi-build.c:2642
+ * #3  0x0000555555d89fca in acpi_build_update (build_opaque=0x5555576d78c0) at ../hw/i386/acpi-build.c:2667
+ * #4  0x0000555555ac20fd in fw_cfg_select (s=0x555557c08fa0, key=42) at ../hw/nvram/fw_cfg.c:285
+ * #5  0x0000555555ac2460 in fw_cfg_dma_transfer (s=0x555557c08fa0) at ../hw/nvram/fw_cfg.c:359
+ * #6  0x0000555555ac28cb in fw_cfg_dma_mem_write (opaque=0x555557c08fa0, addr=4, value=27712, size=4)
+ *     at ../hw/nvram/fw_cfg.c:460
+ * #7  0x0000555555eb8e96 in memory_region_write_accessor (mr=0x555557c09330, addr=4, value=0x7fffeda3f3a8, size=4, shift=0,
+ *     mask=4294967295, attrs=...) at ../system/memory.c:497
+ * #8  0x0000555555eb9184 in access_with_adjusted_size (addr=4, value=0x7fffeda3f3a8, size=4, access_size_min=1,
+ *     access_size_max=8, access_fn=0x555555eb8da0 <memory_region_write_accessor>, mr=0x555557c09330, attrs=...)
+ *     at ../system/memory.c:568
+ * #9  0x0000555555ebc9a2 in memory_region_dispatch_write (mr=0x555557c09330, addr=4, data=27712, op=MO_32, attrs=...)
+ *     at ../system/memory.c:1553
+ * #10 0x0000555555eccb79 in flatview_write_continue_step (attrs=..., buf=0x7ffff7ff1000 "", len=4, mr_addr=4, l=0x7fffeda3f490,
+ *     mr=0x555557c09330) at ../system/physmem.c:2953
+ * #11 0x0000555555eccc4e in flatview_write_continue (fv=0x7fffe4202160, addr=1304, attrs=..., ptr=0x7ffff7ff1000, len=4,
+ *     mr_addr=4, l=4, mr=0x555557c09330) at ../system/physmem.c:2983
+ * #12 0x0000555555eccd7b in flatview_write (fv=0x7fffe4202160, addr=1304, attrs=..., buf=0x7ffff7ff1000, len=4)
+ *     at ../system/physmem.c:3014
+ * #13 0x0000555555ecd208 in address_space_write (as=0x555557383fc0 <address_space_io>, addr=1304, attrs=...,
+ *     buf=0x7ffff7ff1000, len=4) at ../system/physmem.c:3134
+ * #14 0x0000555555ecd282 in address_space_rw (as=0x555557383fc0 <address_space_io>, addr=1304, attrs=...,
+ *     buf=0x7ffff7ff1000, len=4, is_write=true) at ../system/physmem.c:3144
+ * #15 0x0000555555f0b7fc in kvm_handle_io (port=1304, attrs=..., data=0x7ffff7ff1000, direction=1, size=4, count=1) at
+ *     ../accel/kvm/kvm-all.c:2800
+ * #16 0x0000555555f0c774 in kvm_cpu_exec (cpu=0x5555577504d0) at ../accel/kvm/kvm-all.c:3186
+ * #17 0x0000555555f0feec in kvm_vcpu_thread_fn (arg=0x5555577504d0) at ../accel/kvm/kvm-accel-ops.c:51
+ * #18 0x00005555561ad4e5 in qemu_thread_start (args=0x55555775a960) at ../util/qemu-thread-posix.c:541
+ * #19 0x00007ffff68a91da in start_thread () from /lib64/libpthread.so.0
+ * #20 0x00007ffff4bda8d3 in clone () from /lib64/libc.so.6
+ *
+ *
+ * (gdb) bt
+ * #0  qemu_ram_resize (block=0x5555582c8820, newsize=4096, errp=0x5555573a2118 <error_abort>) at ../system/physmem.c:1720
+ * #1  0x0000555555ebf3e9 in memory_region_ram_resize (mr=0x5555582a00e0, newsize=4096, errp=0x5555573a2118 <error_abort>) at ../system/memory.c:2476
+ * #2  0x0000555555ac2db8 in fw_cfg_update_mr (s=0x555557c2ac80, key=41, size=4096) at ../hw/nvram/fw_cfg.c:631
+ * #3  0x0000555555ac2f08 in fw_cfg_acpi_mr_restore_post_load (opaque=0x555557c2ac80, version_id=1) at ../hw/nvram/fw_cfg.c:647
+ * #4  0x0000555555f83d72 in vmstate_load_state (f=0x555557a5bb00, vmsd=0x5555571d0ce0 <vmstate_fw_cfg_acpi_mr>, opaque=0x555557c2ac80, version_id=1)
+ *     at ../migration/vmstate.c:234
+ * #5  0x0000555555f84d35 in vmstate_subsection_load (f=0x555557a5bb00, vmsd=0x5555571d0d60 <vmstate_fw_cfg>, opaque=0x555557c2ac80)
+ *     at ../migration/vmstate.c:608
+ * #6  0x0000555555f83d30 in vmstate_load_state (f=0x555557a5bb00, vmsd=0x5555571d0d60 <vmstate_fw_cfg>, opaque=0x555557c2ac80, version_id=2)
+ *     at ../migration/vmstate.c:228
+ * #7  0x0000555555c807aa in vmstate_load (f=0x555557a5bb00, se=0x555557c2d810) at ../migration/savevm.c:972
+ * #8  0x0000555555c8442a in qemu_loadvm_section_start_full (f=0x555557a5bb00, type=4 '\004') at ../migration/savevm.c:2701
+ * #9  0x0000555555c84fe1 in qemu_loadvm_state_main (f=0x555557a5bb00, mis=0x55555745ce40) at ../migration/savevm.c:3007
+ * #10 0x0000555555c851b0 in qemu_loadvm_state (f=0x555557a5bb00) at ../migration/savevm.c:3094
+ * #11 0x0000555555c6310c in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:866
+ * #12 0x00005555561cda9d in coroutine_trampoline (i0=1466944864, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #13 0x00007ffff4bc5120 in ?? () from /lib64/libc.so.6
+ * #14 0x00007fffffffc7c0 in ?? ()
+ * #15 0x0000000000000000 in ?? ()
+ *
+ *
+ * called by:
+ *   - migration/ram.c|3995| <<parse_ramblock>> ret = qemu_ram_resize(block, length, &local_err);
+ *   - system/memory.c|2476| <<memory_region_ram_resize>> qemu_ram_resize(mr->ram_block, newsize, errp);
+ */
 int qemu_ram_resize(RAMBlock *block, ram_addr_t newsize, Error **errp)
 {
     const ram_addr_t oldsize = block->used_length;
@@ -1723,6 +1847,13 @@ int qemu_ram_resize(RAMBlock *block, ram_addr_t newsize, Error **errp)
     assert(block);
 
     newsize = TARGET_PAGE_ALIGN(newsize);
+    /*
+     * 实现:
+     * 747 static inline uintptr_t qemu_real_host_page_size(void)
+     * 748 {
+     * 749     return getpagesize();
+     * 750 }
+     */
     newsize = REAL_HOST_PAGE_ALIGN(newsize);
 
     if (block->used_length == newsize) {
@@ -1747,6 +1878,14 @@ int qemu_ram_resize(RAMBlock *block, ram_addr_t newsize, Error **errp)
         return -EINVAL;
     }
 
+    /*
+     * RAMBlock *block:
+     * -> char idstr[256];
+     *
+     * 比如说:
+     * newsize = 0x10000
+     * block->max_length = 0x1000
+     */
     if (block->max_length < newsize) {
         error_setg_errno(errp, EINVAL,
                          "Size too large: %s: 0x" RAM_ADDR_FMT
@@ -2138,6 +2277,12 @@ static int qemu_ram_get_shared_fd(const char *name, bool *reused, Error **errp)
 }
 #endif
 
+/*
+ * 在以下调用qemu_ram_alloc_internal():
+ *   - system/physmem.c|2302| <<qemu_ram_alloc_from_ptr>> return qemu_ram_alloc_internal(size, size, NULL, host, RAM_PREALLOC, mr,
+ *   - system/physmem.c|2311| <<qemu_ram_alloc>> return qemu_ram_alloc_internal(size, size, NULL, NULL, ram_flags, mr, errp);
+ *   - system/physmem.c|2318| <<qemu_ram_alloc_resizeable>> return qemu_ram_alloc_internal(size, maxsz, resized, NULL,
+ */
 static
 RAMBlock *qemu_ram_alloc_internal(ram_addr_t size, ram_addr_t max_size,
                                   qemu_ram_resize_cb resized,
@@ -2240,6 +2385,10 @@ RAMBlock *qemu_ram_alloc(ram_addr_t size, uint32_t ram_flags,
     return qemu_ram_alloc_internal(size, size, NULL, NULL, ram_flags, mr, errp);
 }
 
+/*
+ * 在以下调用qemu_ram_alloc_resizeable():
+ *   - system/memory.c|1628| <<memory_region_init_resizeable_ram>> mr->ram_block = qemu_ram_alloc_resizeable(size, max_size, resized,
+ */
 RAMBlock *qemu_ram_alloc_resizeable(ram_addr_t size, ram_addr_t maxsz,
                                     qemu_ram_resize_cb resized,
                                     MemoryRegion *mr, Error **errp)
diff --git a/system/runstate.c b/system/runstate.c
index 272801d30..3b1cf482f 100644
--- a/system/runstate.c
+++ b/system/runstate.c
@@ -768,6 +768,10 @@ void qemu_system_debug_request(void)
     qemu_notify_event();
 }
 
+/*
+ * 在以下使用main_loop_should_exit():
+ *   - system/runstate.c|834| <<qemu_main_loop>> while (!main_loop_should_exit(&status)) {
+ */
 static bool main_loop_should_exit(int *status)
 {
     RunState r;
@@ -827,10 +831,17 @@ static bool main_loop_should_exit(int *status)
     return false;
 }
 
+/*
+ * 在以下使用qemu_main_loop():
+ *   - system/main.c|50| <<qemu_default_main>> status = qemu_main_loop();
+ */
 int qemu_main_loop(void)
 {
     int status = EXIT_SUCCESS;
 
+    /*
+     * 只在此处调用main_loop_should_exit()
+     */
     while (!main_loop_should_exit(&status)) {
         main_loop_wait(false);
     }
@@ -888,6 +899,10 @@ void qemu_init_subsystems(void)
 }
 
 
+/*
+ * 在以下使用qemu_cleanup():
+ *   - system/main.c|51| <<qemu_default_main>> qemu_cleanup(status);
+ */
 void qemu_cleanup(int status)
 {
     gdb_exit(status);
diff --git a/system/vl.c b/system/vl.c
index ec93988a0..d1f1fd522 100644
--- a/system/vl.c
+++ b/system/vl.c
@@ -2837,6 +2837,11 @@ void qmp_x_exit_preconfig(Error **errp)
     }
 }
 
+/*
+ * 在以下调用qemu_init():
+ *   - system/main.c|71| <<main>> qemu_init(argc, argv);
+ *   - tests/qtest/fuzz/fuzz.c|228| <<LLVMFuzzerInitialize>> qemu_init(result.we_wordc, result.we_wordv);
+ */
 void qemu_init(int argc, char **argv)
 {
     QemuOpts *opts;
diff --git a/util/async.c b/util/async.c
index 863416dee..b61ae00b8 100644
--- a/util/async.c
+++ b/util/async.c
@@ -299,6 +299,14 @@ aio_compute_timeout(AioContext *ctx)
     }
 }
 
+/*
+ * 424 static GSourceFuncs aio_source_funcs = {
+ * 425     aio_ctx_prepare,
+ * 426     aio_ctx_check,
+ * 427     aio_ctx_dispatch,
+ * 428     aio_ctx_finalize
+ * 429 };
+ */
 static gboolean
 aio_ctx_prepare(GSource *source, gint    *timeout)
 {
@@ -323,6 +331,14 @@ aio_ctx_prepare(GSource *source, gint    *timeout)
     return *timeout == 0;
 }
 
+/*
+ * 424 static GSourceFuncs aio_source_funcs = {
+ * 425     aio_ctx_prepare,
+ * 426     aio_ctx_check,
+ * 427     aio_ctx_dispatch,
+ * 428     aio_ctx_finalize
+ * 429 };
+ */
 static gboolean
 aio_ctx_check(GSource *source)
 {
@@ -350,6 +366,14 @@ aio_ctx_check(GSource *source)
     return aio_pending(ctx) || (timerlistgroup_deadline_ns(&ctx->tlg) == 0);
 }
 
+/*
+ * 424 static GSourceFuncs aio_source_funcs = {
+ * 425     aio_ctx_prepare,
+ * 426     aio_ctx_check,
+ * 427     aio_ctx_dispatch,
+ * 428     aio_ctx_finalize
+ * 429 };
+ */
 static gboolean
 aio_ctx_dispatch(GSource     *source,
                  GSourceFunc  callback,
@@ -362,6 +386,14 @@ aio_ctx_dispatch(GSource     *source,
     return true;
 }
 
+/*
+ * 424 static GSourceFuncs aio_source_funcs = {
+ * 425     aio_ctx_prepare,
+ * 426     aio_ctx_check,
+ * 427     aio_ctx_dispatch,
+ * 428     aio_ctx_finalize
+ * 429 };
+ */
 static void
 aio_ctx_finalize(GSource     *source)
 {
diff --git a/util/coroutine-ucontext.c b/util/coroutine-ucontext.c
index 8ef603d08..5e08f2cce 100644
--- a/util/coroutine-ucontext.c
+++ b/util/coroutine-ucontext.c
@@ -278,6 +278,11 @@ static void coroutine_fn terminate_asan(void *opaque)
 }
 #endif
 
+/*
+ * 在以下调用qemu_coroutine_delete():
+ *   -  util/qemu-coroutine.c|288| <<coroutine_pool_batch_delete>> qemu_coroutine_delete(co);
+ *   - util/qemu-coroutine.c|699| <<coroutine_delete>> qemu_coroutine_delete(co);
+ */
 void qemu_coroutine_delete(Coroutine *co_)
 {
     CoroutineUContext *co = DO_UPCAST(CoroutineUContext, base, co_);
diff --git a/util/main-loop.c b/util/main-loop.c
index acad8c2e6..6ec65b89f 100644
--- a/util/main-loop.c
+++ b/util/main-loop.c
@@ -290,6 +290,58 @@ static void glib_pollfds_poll(void)
 
 #define MAX_MAIN_LOOP_SPIN (1000)
 
+/*
+ * (gdb) bt
+ * #0  virtio_scsi_get_config (vdev=0x555558406a90, config=0x5555584313b0 "") at ../hw/scsi/virtio-scsi.c:888
+ * #1  0x0000555555e3d3e0 in virtio_config_modern_readl (vdev=0x555558406a90, addr=0) at ../hw/virtio/virtio-config-io.c:148
+ * #2  0x0000555555b7ba61 in virtio_pci_device_read (opaque=0x5555583fe4d0, addr=0, size=4) at ../hw/virtio/virtio-pci.c:1778
+ * #3  0x0000555555e65ce2 in memory_region_read_accessor (mr=0x5555583ff2f0, addr=0, value=0x7fffeb0d5458, size=4, shift=0,
+ * #4  0x0000555555e6633a in access_with_adjusted_size (addr=0, value=0x7fffeb0d5458, size=4, access_size_min=1,
+ * #5  0x0000555555e6969d in memory_region_dispatch_read1 (mr=0x5555583ff2f0, addr=0, pval=0x7fffeb0d5458, size=4, attrs=...)
+ * #6  0x0000555555e697cc in memory_region_dispatch_read (mr=0x5555583ff2f0, addr=0, pval=0x7fffeb0d5458, op=MO_32, attrs=...)
+ * #7  0x0000555555e796f6 in flatview_read_continue_step (attrs=..., buf=0x7ffff7ff0028 "\v", len=4, mr_addr=0, l=0x7fffeb0d54d0,
+ * #8  0x0000555555e797f4 in flatview_read_continue (fv=0x7ffbd4000fc0, addr=61572651163648, attrs=..., ptr=0x7ffff7ff0028,
+ * #9  0x0000555555e7991c in flatview_read (fv=0x7ffbd4000fc0, addr=61572651163648, attrs=..., buf=0x7ffff7ff0028, len=4)
+ * #10 0x0000555555e799bb in address_space_read_full (as=0x555557334980 <address_space_memory>, addr=61572651163648, attrs=...,
+ * #11 0x0000555555e79afd in address_space_rw (as=0x555557334980 <address_space_memory>, addr=61572651163648, attrs=...,
+ * #12 0x0000555555ed996b in kvm_cpu_exec (cpu=0x5555576f0660) at ../accel/kvm/kvm-all.c:3184
+ * #13 0x0000555555edd092 in kvm_vcpu_thread_fn (arg=0x5555576f0660) at ../accel/kvm/kvm-accel-ops.c:50
+ * #14 0x0000555556172954 in qemu_thread_start (args=0x5555576fa4e0) at ../util/qemu-thread-posix.c:541
+ * #15 0x00007ffff68a91da in start_thread () from /lib64/libpthread.so.0
+ * #16 0x00007ffff52488d3 in clone () from /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  coroutine_pool_get_local () at ../util/qemu-coroutine.c:116
+ * #1  0x00005555561cb785 in coroutine_pool_get () at ../util/qemu-coroutine.c:181
+ * #2  0x00005555561cb8c7 in qemu_coroutine_create (entry=0x5555560034f4 <blk_aio_write_entry>, opaque=0x5555582fd4d0)
+ *     at ../util/qemu-coroutine.c:222
+ * #3  0x00005555560033cc in blk_aio_prwv (blk=0x55555846f060, offset=540709888, bytes=1024, iobuf=0x5555577f63e8,
+ *     co_entry=0x5555560034f4 <blk_aio_write_entry>, flags=BDRV_REQ_REGISTERED_BUF,
+ *     cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350) at ../block/block-backend.c:1600
+ * #4  0x0000555556003927 in blk_aio_pwritev (blk=0x55555846f060, offset=540709888, qiov=0x5555577f63e8,
+ *     flags=BDRV_REQ_REGISTERED_BUF, cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350)
+ *     at ../block/block-backend.c:1722
+ * #5  0x0000555555e2c003 in submit_requests (s=0x5555584688c0, mrb=0x7fffffffd640, start=0, num_reqs=1, niov=-1) at ../hw/block/virtio-blk.c:259
+ * #6  0x0000555555e2c12f in virtio_blk_submit_multireq (s=0x5555584688c0, mrb=0x7fffffffd640) at ../hw/block/virtio-blk.c:294
+ * #7  0x0000555555e2e065 in virtio_blk_handle_vq (s=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1008
+ * #8  0x0000555555e2e0f8 in virtio_blk_handle_output (vdev=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1028
+ * #9  0x0000555555e89d02 in virtio_queue_notify_vq (vq=0x5555584b0960) at ../hw/virtio/virtio.c:2484
+ * #10 0x0000555555e8d53c in virtio_queue_host_notifier_read (n=0x5555584b09d4) at ../hw/virtio/virtio.c:3869
+ * #11 0x00005555561a74d8 in aio_dispatch_handler (ctx=0x55555745ee50, node=0x7fffe43dfba0) at ../util/aio-posix.c:376
+ * #12 0x00005555561a76ed in aio_dispatch_handlers (ctx=0x55555745ee50) at ../util/aio-posix.c:427
+ * #13 0x00005555561a7749 in aio_dispatch (ctx=0x55555745ee50) at ../util/aio-posix.c:437
+ * #14 0x00005555561c9148 in aio_ctx_dispatch (source=0x55555745ee50, callback=0x0, user_data=0x0) at ../util/async.c:361
+ * #15 0x00007ffff6fd394b in g_main_dispatch (context=0x55555745f260) at ../glib/gmain.c:3325
+ * #16 g_main_context_dispatch (context=0x55555745f260) at ../glib/gmain.c:4043
+ * #17 0x00005555561ca939 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #18 0x00005555561ca9c7 in os_host_main_loop_wait (timeout=657614) at ../util/main-loop.c:310
+ * #19 0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #20 0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #21 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #22 0x00005555560d296f in main (argc=25, argv=0x7fffffffdb28) at ../system/main.c:80
+ *
+ * 这个是Linux的
+ */
 static int os_host_main_loop_wait(int64_t timeout)
 {
     GMainContext *context = g_main_context_default();
@@ -302,11 +354,17 @@ static int os_host_main_loop_wait(int64_t timeout)
     bql_unlock();
     replay_mutex_unlock();
 
+    /*
+     * 如果想测试时间, 应该是这个函数的后面到前面
+     */
     ret = qemu_poll_ns((GPollFD *)gpollfds->data, gpollfds->len, timeout);
 
     replay_mutex_lock();
     bql_lock();
 
+    /*
+     * 这里是handler!
+     */
     glib_pollfds_poll();
 
     g_main_context_release(context);
@@ -456,6 +514,9 @@ static void pollfds_poll(GArray *pollfds, int nfds, fd_set *rfds,
     }
 }
 
+/*
+ * 这个是Windows的
+ */
 static int os_host_main_loop_wait(int64_t timeout)
 {
     GMainContext *context = g_main_context_default();
@@ -544,11 +605,31 @@ static int os_host_main_loop_wait(int64_t timeout)
 }
 #endif
 
+/*
+ * 在以下使用main_loop_poll_notifiers:
+ *   - util/main-loop.c|547| <<global>> static NotifierList main_loop_poll_notifiers =
+ *             NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
+ *   - util/main-loop.c|552| <<main_loop_poll_add_notifier>> notifier_list_add(&main_loop_poll_notifiers, notify);
+ *   - util/main-loop.c|577| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+ *   - util/main-loop.c|591| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+ */
 static NotifierList main_loop_poll_notifiers =
     NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
 
+/*
+ * 只在以下调用main_loop_poll_add_notifier():
+ *   - net/slirp.c|676| <<net_slirp_init>> main_loop_poll_add_notifier(&s->poll_notifier);
+ */
 void main_loop_poll_add_notifier(Notifier *notify)
 {
+    /*
+     * 在以下使用main_loop_poll_notifiers:
+     *   - util/main-loop.c|547| <<global>> static NotifierList main_loop_poll_notifiers =
+     *             NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
+     *   - util/main-loop.c|552| <<main_loop_poll_add_notifier>> notifier_list_add(&main_loop_poll_notifiers, notify);
+     *   - util/main-loop.c|577| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+     *   - util/main-loop.c|591| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+     */
     notifier_list_add(&main_loop_poll_notifiers, notify);
 }
 
@@ -557,6 +638,51 @@ void main_loop_poll_remove_notifier(Notifier *notify)
     notifier_remove(notify);
 }
 
+/*
+ * (gdb) bt
+ * #0  virtio_scsi_get_config (vdev=0x555558406a90, config=0x5555584313b0 "") at ../hw/scsi/virtio-scsi.c:888
+ * #1  0x0000555555e3d3e0 in virtio_config_modern_readl (vdev=0x555558406a90, addr=0) at ../hw/virtio/virtio-config-io.c:148
+ * #2  0x0000555555b7ba61 in virtio_pci_device_read (opaque=0x5555583fe4d0, addr=0, size=4) at ../hw/virtio/virtio-pci.c:1778
+ * #3  0x0000555555e65ce2 in memory_region_read_accessor (mr=0x5555583ff2f0, addr=0, value=0x7fffeb0d5458, size=4, shift=0,
+ * #4  0x0000555555e6633a in access_with_adjusted_size (addr=0, value=0x7fffeb0d5458, size=4, access_size_min=1,
+ * #5  0x0000555555e6969d in memory_region_dispatch_read1 (mr=0x5555583ff2f0, addr=0, pval=0x7fffeb0d5458, size=4, attrs=...)
+ * #6  0x0000555555e697cc in memory_region_dispatch_read (mr=0x5555583ff2f0, addr=0, pval=0x7fffeb0d5458, op=MO_32, attrs=...)
+ * #7  0x0000555555e796f6 in flatview_read_continue_step (attrs=..., buf=0x7ffff7ff0028 "\v", len=4, mr_addr=0, l=0x7fffeb0d54d0,
+ * #8  0x0000555555e797f4 in flatview_read_continue (fv=0x7ffbd4000fc0, addr=61572651163648, attrs=..., ptr=0x7ffff7ff0028,
+ * #9  0x0000555555e7991c in flatview_read (fv=0x7ffbd4000fc0, addr=61572651163648, attrs=..., buf=0x7ffff7ff0028, len=4)
+ * #10 0x0000555555e799bb in address_space_read_full (as=0x555557334980 <address_space_memory>, addr=61572651163648, attrs=...,
+ * #11 0x0000555555e79afd in address_space_rw (as=0x555557334980 <address_space_memory>, addr=61572651163648, attrs=...,
+ * #12 0x0000555555ed996b in kvm_cpu_exec (cpu=0x5555576f0660) at ../accel/kvm/kvm-all.c:3184
+ * #13 0x0000555555edd092 in kvm_vcpu_thread_fn (arg=0x5555576f0660) at ../accel/kvm/kvm-accel-ops.c:50
+ * #14 0x0000555556172954 in qemu_thread_start (args=0x5555576fa4e0) at ../util/qemu-thread-posix.c:541
+ * #15 0x00007ffff68a91da in start_thread () from /lib64/libpthread.so.0
+ * #16 0x00007ffff52488d3 in clone () from /lib64/libc.so.6
+ *
+ * 在以下调用main_loop_wait():
+ *   - qemu-img.c|2144| <<convert_do_copy>> main_loop_wait(false);
+ *   - qemu-img.c|4743| <<img_bench>> main_loop_wait(false);
+ *   - qemu-io-cmds.c|651| <<do_aio_readv>> main_loop_wait(false);
+ *   - qemu-io-cmds.c|665| <<do_aio_writev>> main_loop_wait(false);
+ *   - qemu-io-cmds.c|1930| <<do_aio_zone_append>> main_loop_wait(false);
+ *   - qemu-io-cmds.c|2678| <<sleep_f>> main_loop_wait(false);
+ *   - qemu-io.c|445| <<command_loop>> main_loop_wait(false);
+ *   - qemu-nbd.c|1230| <<main>> main_loop_wait(false);
+ *   - scsi/qemu-pr-helper.c|1069| <<main>> main_loop_wait(false);
+ *   - storage-daemon/qemu-storage-daemon.c|434| <<main>> main_loop_wait(false);
+ *   - system/runstate.c|835| <<qemu_main_loop>> main_loop_wait(false);
+ *   - tests/qtest/fuzz/fuzz.c|51| <<flush_events>> main_loop_wait(false);
+ *   - tests/qtest/fuzz/fuzz.c|58| <<fuzz_reset>> main_loop_wait(true);
+ *   - tests/unit/test-char.c|35| <<main_loop>> main_loop_wait(false);
+ *   - tests/unit/test-char.c|1256| <<char_socket_server_test>> main_loop_wait(false);
+ *   - tests/unit/test-char.c|1279| <<char_socket_server_test>> main_loop_wait(false);
+ *   - tests/unit/test-char.c|1463| <<char_socket_client_test>> main_loop_wait(false);
+ *   - tests/unit/test-char.c|1486| <<char_socket_client_test>> main_loop_wait(false);
+ *   - tests/unit/test-replication.c|78| <<test_blk_read>> main_loop_wait(false);
+ *   - tests/unit/test-replication.c|115| <<test_blk_write>> main_loop_wait(false);
+ *   - tests/unit/test-util-filemonitor.c|85| <<qemu_file_monitor_test_event_loop>> main_loop_wait(true);
+ *   - tools/i386/qemu-vmsr-helper.c|532| <<main>> main_loop_wait(false);
+ *   - ui/gtk-clipboard.c|55| <<gd_clipboard_get_data>> main_loop_wait(false);
+ */
 void main_loop_wait(int nonblocking)
 {
     MainLoopPoll mlpoll = {
@@ -567,12 +693,26 @@ void main_loop_wait(int nonblocking)
     int ret;
     int64_t timeout_ns;
 
+    /*
+     * qemu-system-x86_64来的nonblocking=false
+     */
     if (nonblocking) {
         mlpoll.timeout = 0;
     }
 
     /* poll any events */
     g_array_set_size(gpollfds, 0); /* reset for new iteration */
+    /*
+     * 只在以下调用main_loop_poll_add_notifier():
+     *   - net/slirp.c|676| <<net_slirp_init>> main_loop_poll_add_notifier(&s->poll_notifier);
+     *
+     * 在以下使用main_loop_poll_notifiers:
+     *   - util/main-loop.c|547| <<global>> static NotifierList main_loop_poll_notifiers =
+     *             NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
+     *   - util/main-loop.c|552| <<main_loop_poll_add_notifier>> notifier_list_add(&main_loop_poll_notifiers, notify);
+     *   - util/main-loop.c|577| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+     *   - util/main-loop.c|591| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+     */
     /* XXX: separate device handlers from system ones */
     notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
 
@@ -582,12 +722,34 @@ void main_loop_wait(int nonblocking)
         timeout_ns = (uint64_t)mlpoll.timeout * (int64_t)(SCALE_MS);
     }
 
+    /*
+     * 注释:
+     * qemu_soonest_timeout:
+     * @timeout1: first timeout in nanoseconds (or -1 for infinite)
+     * @timeout2: second timeout in nanoseconds (or -1 for infinite)
+     *
+     * Calculates the soonest of two timeout values. -1 means infinite, which
+     * is later than any other value.
+     *
+     * Returns: soonest timeout value in nanoseconds (or -1 for infinite)
+     */
     timeout_ns = qemu_soonest_timeout(timeout_ns,
                                       timerlistgroup_deadline_ns(
                                           &main_loop_tlg));
 
     ret = os_host_main_loop_wait(timeout_ns);
     mlpoll.state = ret < 0 ? MAIN_LOOP_POLL_ERR : MAIN_LOOP_POLL_OK;
+    /*
+     * 只在以下调用main_loop_poll_add_notifier():
+     *   - net/slirp.c|676| <<net_slirp_init>> main_loop_poll_add_notifier(&s->poll_notifier);
+     *
+     * 在以下使用main_loop_poll_notifiers:
+     *   - util/main-loop.c|547| <<global>> static NotifierList main_loop_poll_notifiers =
+     *             NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
+     *   - util/main-loop.c|552| <<main_loop_poll_add_notifier>> notifier_list_add(&main_loop_poll_notifiers, notify);
+     *   - util/main-loop.c|577| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+     *   - util/main-loop.c|591| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+     */
     notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
 
     if (icount_enabled()) {
@@ -597,6 +759,9 @@ void main_loop_wait(int nonblocking)
          */
         icount_start_warp_timer();
     }
+    /*
+     * 只在以下调用qemu_clock_run_all_timers()
+     */
     qemu_clock_run_all_timers();
 }
 
diff --git a/util/osdep.c b/util/osdep.c
index 770369831..8676bce08 100644
--- a/util/osdep.c
+++ b/util/osdep.c
@@ -605,6 +605,13 @@ writev(int fd, const struct iovec *iov, int iov_cnt)
  * Unfortunately even in 2009 many operating systems do not support
  * fdatasync and have to fall back to fsync.
  */
+/*
+ * 在以下使用qemu_fdatasync():
+ *   - block/file-posix.c|1636| <<handle_aiocb_flush>> ret = qemu_fdatasync(aiocb->aio_fildes);
+ *   - hw/9pfs/9p-local.c|1179| <<local_fsync>> return qemu_fdatasync(fd);
+ *   - hw/ppc/spapr_nvdimm.c|471| <<flush_worker_cb>> if (qemu_fdatasync(backend_fd) < 0)
+ *   - util/oslib-win32.c|845| <<qemu_msync>> return qemu_fdatasync(fd);
+ */
 int qemu_fdatasync(int fd)
 {
 #ifdef CONFIG_FDATASYNC
diff --git a/util/qemu-coroutine.c b/util/qemu-coroutine.c
index 64d6264fc..5df0261ec 100644
--- a/util/qemu-coroutine.c
+++ b/util/qemu-coroutine.c
@@ -21,7 +21,83 @@
 #include "qemu/cutils.h"
 #include "block/aio.h"
 
+/*
+ * commit 86a637e48104ae74d8be53bed6441ce32be33433
+ * Author: Stefan Hajnoczi <stefanha@redhat.com>
+ * Date:   Mon Mar 18 14:34:29 2024 -0400
+ *
+ * coroutine: cap per-thread local pool size
+ *
+ * The coroutine pool implementation can hit the Linux vm.max_map_count
+ * limit, causing QEMU to abort with "failed to allocate memory for stack"
+ * or "failed to set up stack guard page" during coroutine creation.
+ *
+ * This happens because per-thread pools can grow to tens of thousands of
+ * coroutines. Each coroutine causes 2 virtual memory areas to be created.
+ * Eventually vm.max_map_count is reached and memory-related syscalls fail.
+ * The per-thread pool sizes are non-uniform and depend on past coroutine
+ * usage in each thread, so it's possible for one thread to have a large
+ * pool while another thread's pool is empty.
+ *
+ * Switch to a new coroutine pool implementation with a global pool that
+ * grows to a maximum number of coroutines and per-thread local pools that
+ * are capped at hardcoded small number of coroutines.
+ *
+ * This approach does not leave large numbers of coroutines pooled in a
+ * thread that may not use them again. In order to perform well it
+ * amortizes the cost of global pool accesses by working in batches of
+ * coroutines instead of individual coroutines.
+ *
+ * The global pool is a list. Threads donate batches of coroutines to when
+ * they have too many and take batches from when they have too few:
+ *
+ * .-----------------------------------.
+ * | Batch 1 | Batch 2 | Batch 3 | ... | global_pool
+ * `-----------------------------------'
+ *
+ * Each thread has up to 2 batches of coroutines:
+ *
+ * .-------------------.
+ * | Batch 1 | Batch 2 | per-thread local_pool (maximum 2 batches)
+ * `-------------------'
+ *
+ * The goal of this change is to reduce the excessive number of pooled
+ * coroutines that cause QEMU to abort when vm.max_map_count is reached
+ * without losing the performance of an adequately sized coroutine pool.
+ *
+ * Here are virtio-blk disk I/O benchmark results:
+ *
+ * RW BLKSIZE IODEPTH    OLD    NEW CHANGE
+ * randread      4k       1 113725 117451 +3.3%
+ * randread      4k       8 192968 198510 +2.9%
+ * randread      4k      16 207138 209429 +1.1%
+ * randread      4k      32 212399 215145 +1.3%
+ * randread      4k      64 218319 221277 +1.4%
+ * randread    128k       1  17587  17535 -0.3%
+ * randread    128k       8  17614  17616 +0.0%
+ * randread    128k      16  17608  17609 +0.0%
+ * randread    128k      32  17552  17553 +0.0%
+ * randread    128k      64  17484  17484 +0.0%
+ *
+ * See files/{fio.sh,test.xml.j2} for the benchmark configuration:
+ * https://gitlab.com/stefanha/virt-playbooks/-/tree/coroutine-pool-fix-sizing
+ *
+ * Buglink: https://issues.redhat.com/browse/RHEL-28947
+ * Reported-by: Sanjay Rao <srao@redhat.com>
+ * Reported-by: Boaz Ben Shabat <bbenshab@redhat.com>
+ * Reported-by: Joe Mario <jmario@redhat.com>
+ * Reviewed-by: Kevin Wolf <kwolf@redhat.com>
+ * Signed-off-by: Stefan Hajnoczi <stefanha@redhat.com>
+ * Message-ID: <20240318183429.1039340-1-stefanha@redhat.com>
+ */
+
 enum {
+    /*
+     * 在以下使用COROUTINE_POOL_BATCH_MAX_SIZE:
+     *   - util/qemu-coroutine.c|25| <<global>> COROUTINE_POOL_BATCH_MAX_SIZE = 128,
+     *   - util/qemu-coroutine.c|65| <<QSLIST_HEAD>> static unsigned int global_pool_max_size = COROUTINE_POOL_BATCH_MAX_SIZE;
+     *   - util/qemu-coroutine.c|204| <<coroutine_pool_put>> if (unlikely(batch->size >= COROUTINE_POOL_BATCH_MAX_SIZE)) {
+     */
     COROUTINE_POOL_BATCH_MAX_SIZE = 128,
 };
 
@@ -56,17 +132,127 @@ typedef struct CoroutinePoolBatch {
 
 typedef QSLIST_HEAD(, CoroutinePoolBatch) CoroutinePool;
 
+/*
+ * 在以下使用global_pool_hard_max_size:
+ *   - util/qemu-coroutine.c|136| <<QSLIST_HEAD>> static unsigned int global_pool_hard_max_size;
+ *   - util/qemu-coroutine.c|296| <<coroutine_pool_put_global>> unsigned int max = MIN(global_pool_max_size, global_pool_hard_max_size);
+ *   - util/qemu-coroutine.c|600| <<qemu_coroutine_init>> global_pool_hard_max_size = get_global_pool_hard_max_size();
+ */
 /* Host operating system limit on number of pooled coroutines */
 static unsigned int global_pool_hard_max_size;
 
+/*
+ * 在以下使用global_pool_lock:
+ *   - util/qemu-coroutine.c|138| <<QSLIST_HEAD>> static QemuMutex global_pool_lock;
+ *   - util/qemu-coroutine.c|276| <<coroutine_pool_refill_local>> WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+ *   - util/qemu-coroutine.c|294| <<coroutine_pool_put_global>> WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+ *   - util/qemu-coroutine.c|554| <<qemu_coroutine_inc_pool_size>> QEMU_LOCK_GUARD(&global_pool_lock);
+ *   - util/qemu-coroutine.c|560| <<qemu_coroutine_dec_pool_size>> QEMU_LOCK_GUARD(&global_pool_lock);
+ *   - util/qemu-coroutine.c|599| <<qemu_coroutine_init>> qemu_mutex_init(&global_pool_lock);
+ */
 static QemuMutex global_pool_lock; /* protects the following variables */
+/*
+ * 在以下使用global_pool:
+ *   - util/qemu-coroutine.c|139| <<QSLIST_HEAD>> static CoroutinePool global_pool = QSLIST_HEAD_INITIALIZER(global_pool);
+ *   - util/qemu-coroutine.c|277| <<coroutine_pool_refill_local>> batch = QSLIST_FIRST(&global_pool);
+ *   - util/qemu-coroutine.c|280| <<coroutine_pool_refill_local>> QSLIST_REMOVE_HEAD(&global_pool, next);
+ *   - util/qemu-coroutine.c|299| <<coroutine_pool_put_global>> QSLIST_INSERT_HEAD(&global_pool, batch, next);
+ */
 static CoroutinePool global_pool = QSLIST_HEAD_INITIALIZER(global_pool);
+/*
+ * 在以下使用global_pool_size:
+ *   - util/qemu-coroutine.c|140| <<QSLIST_HEAD>> static unsigned int global_pool_size;
+ *   - util/qemu-coroutine.c|281| <<coroutine_pool_refill_local>> global_pool_size -= batch->size;
+ *   - util/qemu-coroutine.c|298| <<coroutine_pool_put_global>> if (global_pool_size < max) {
+ *   - util/qemu-coroutine.c|302| <<coroutine_pool_put_global>> global_pool_size += batch->size;
+ */
 static unsigned int global_pool_size;
+/*
+ * 在以下使用COROUTINE_POOL_BATCH_MAX_SIZE:
+ *   - util/qemu-coroutine.c|25| <<global>> COROUTINE_POOL_BATCH_MAX_SIZE = 128,
+ *   - util/qemu-coroutine.c|65| <<QSLIST_HEAD>> static unsigned int global_pool_max_size = COROUTINE_POOL_BATCH_MAX_SIZE;
+ *   - util/qemu-coroutine.c|204| <<coroutine_pool_put>> if (unlikely(batch->size >= COROUTINE_POOL_BATCH_MAX_SIZE)) {
+ *
+ * 在以下使用global_pool_max_size:
+ *   - util/qemu-coroutine.c|389| <<coroutine_pool_put_global>> unsigned int max = MIN(global_pool_max_size,
+ *   - util/qemu-coroutine.c|664| <<qemu_coroutine_inc_pool_size>> global_pool_max_size += additional_pool_size;
+ *   - util/qemu-coroutine.c|670| <<qemu_coroutine_dec_pool_size>> global_pool_max_size -= removing_pool_size;
+ */
 static unsigned int global_pool_max_size = COROUTINE_POOL_BATCH_MAX_SIZE;
 
+/*
+ * (gdb) info threads 
+ *   Id   Target Id                                            Frame 
+ * * 1    Thread 0x7fffef5b5f00 (LWP 291721) "qemu-system-x86" 0x00007ffff4cd3c36 in ppoll () from /lib64/libc.so.6
+ *   2    Thread 0x7fffef5b4700 (LWP 291725) "qemu-system-x86" 0x00007ffff4bda41d in syscall () from /lib64/libc.so.6
+ *   4    Thread 0x7fffeda40700 (LWP 291729) "CPU 0/KVM"       0x00007ffff4bda22b in ioctl () from /lib64/libc.so.6
+ *   5    Thread 0x7fffed23f700 (LWP 291730) "CPU 1/KVM"       0x00007ffff4bda22b in ioctl () from /lib64/libc.so.6
+ *   6    Thread 0x7fffeca3e700 (LWP 291731) "CPU 2/KVM"       0x00007ffff4bda22b in ioctl () from /lib64/libc.so.6
+ *   7    Thread 0x7ffbdfdff700 (LWP 291732) "CPU 3/KVM"       0x00007ffff4bda22b in ioctl () from /lib64/libc.so.6
+ *   8    Thread 0x7ffbdd3ff700 (LWP 291734) "vnc_worker"      0x00007ffff68af4dc in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
+ *
+ * (gdb) p co_tls_local_pool
+ * $6 = {slh_first = 0x555557793a60}
+ *
+ * (gdb) thread 2
+ * (gdb) p co_tls_local_pool
+ * $7 = {slh_first = 0x0}
+ *
+ * (gdb) thread 4
+ * (gdb) p co_tls_local_pool
+ * $8 = {slh_first = 0x7fffe4045780}
+ *
+ * (gdb) thread 5
+ * (gdb) p co_tls_local_pool
+ * $9 = {slh_first = 0x5555577893f0}
+ *
+ * (gdb) thread 6
+ * (gdb) p co_tls_local_pool
+ * $10 = {slh_first = 0x7ffbd0018b40}
+ *
+ * (gdb) thread 7
+ * (gdb) p co_tls_local_pool
+ * $11 = {slh_first = 0x55555779d8f0}
+ *
+ * (gdb) thread 8
+ * (gdb) p co_tls_local_pool
+ * $12 = {slh_first = 0x0}
+ *
+ * 定义:
+ * 153 #define QEMU_DEFINE_STATIC_CO_TLS(type, var)                                 \
+ * 154     static __thread type co_tls_##var;                                       \
+ * 155     static __attribute__((noinline, unused))                                 \
+ * 156     type get_##var(void)                                                     \
+ * 157     { asm volatile(""); return co_tls_##var; }                               \
+ * 158     static __attribute__((noinline, unused))                                 \
+ * 159     void set_##var(type v)                                                   \
+ * 160     { asm volatile(""); co_tls_##var = v; }                                  \
+ * 161     static __attribute__((noinline, unused))                                 \
+ * 162     type *get_ptr_##var(void)                                                \
+ * 163     { type *ptr = &co_tls_##var; asm volatile("" : "+rm" (ptr)); return ptr; }
+ *
+ * get_local_pool()
+ * set_local_pool()
+ * get_ptr_local_pool()
+ *
+ * 在以下定义get_ptr_local_pool():
+ *   - QEMU_DEFINE_STATIC_CO_TLS(CoroutinePool, local_pool);
+ * 在以下使用get_ptr_local_pool():
+ *   - util/qemu-coroutine.c|93| <<local_pool_cleanup>> CoroutinePool *local_pool = get_ptr_local_pool();
+ *   - util/qemu-coroutine.c|116| <<coroutine_pool_get_local>> CoroutinePool *local_pool = get_ptr_local_pool();
+ *   - util/qemu-coroutine.c|138| <<coroutine_pool_refill_local>> CoroutinePool *local_pool = get_ptr_local_pool();
+ *   - util/qemu-coroutine.c|195| <<coroutine_pool_put>> CoroutinePool *local_pool = get_ptr_local_pool();
+ *
+ * 每个thread一个local_pool指针
+ */
 QEMU_DEFINE_STATIC_CO_TLS(CoroutinePool, local_pool);
 QEMU_DEFINE_STATIC_CO_TLS(Notifier, local_pool_cleanup_notifier);
 
+/*
+ * called by:
+ *   - util/qemu-coroutine.c|378| <<coroutine_pool_put>> batch = coroutine_pool_batch_new();
+ *   - util/qemu-coroutine.c|398| <<coroutine_pool_put>> batch = coroutine_pool_batch_new();
+ */
 static CoroutinePoolBatch *coroutine_pool_batch_new(void)
 {
     CoroutinePoolBatch *batch = g_new(CoroutinePoolBatch, 1);
@@ -76,20 +262,56 @@ static CoroutinePoolBatch *coroutine_pool_batch_new(void)
     return batch;
 }
 
+/*
+ * called by:
+ *   - util/qemu-coroutine.c|245| <<local_pool_cleanup>> coroutine_pool_batch_delete(batch);
+ *   - util/qemu-coroutine.c|285| <<coroutine_pool_get_local>> coroutine_pool_batch_delete(batch);
+ *   - util/qemu-coroutine.c|343| <<coroutine_pool_put_global>> coroutine_pool_batch_delete(batch);
+ *
+ * 如果从coroutine_pool_get_local()来的话理论上batch上没有co了.
+ */
 static void coroutine_pool_batch_delete(CoroutinePoolBatch *batch)
 {
     Coroutine *co;
     Coroutine *tmp;
 
+    /*
+     * 124 typedef struct CoroutinePoolBatch {
+     * 125     // Batches are kept in a list
+     * 126     QSLIST_ENTRY(CoroutinePoolBatch) next;
+     * 127
+     * 128     // This batch holds up to @COROUTINE_POOL_BATCH_MAX_SIZE coroutines
+     * 129     QSLIST_HEAD(, Coroutine) list;
+     * 130     unsigned int size;
+     * 131 } CoroutinePoolBatch;
+     */
     QSLIST_FOREACH_SAFE(co, &batch->list, pool_next, tmp) {
         QSLIST_REMOVE_HEAD(&batch->list, pool_next);
+        /*
+	 * 在以下调用qemu_coroutine_delete():
+	 *   - util/qemu-coroutine.c|288| <<coroutine_pool_batch_delete>> qemu_coroutine_delete(co);
+	 *   - util/qemu-coroutine.c|699| <<coroutine_delete>> qemu_coroutine_delete(co);
+	 */
         qemu_coroutine_delete(co);
     }
     g_free(batch);
 }
 
+/*
+ * 在以下使用local_pool_cleanup():
+ *   - util/qemu-coroutine.c|254| <<local_pool_cleanup_init_once>> notifier->notify = local_pool_cleanup;
+ */
 static void local_pool_cleanup(Notifier *n, void *value)
 {
+    /*
+     * 在以下定义get_ptr_local_pool():
+     *   - QEMU_DEFINE_STATIC_CO_TLS(CoroutinePool, local_pool);
+     * 在以下使用get_ptr_local_pool():
+     *   - util/qemu-coroutine.c|93| <<local_pool_cleanup>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|116| <<coroutine_pool_get_local>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|138| <<coroutine_pool_refill_local>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|195| <<coroutine_pool_put>> CoroutinePool *local_pool = get_ptr_local_pool();
+     */
     CoroutinePool *local_pool = get_ptr_local_pool();
     CoroutinePoolBatch *batch;
     CoroutinePoolBatch *tmp;
@@ -100,6 +322,11 @@ static void local_pool_cleanup(Notifier *n, void *value)
     }
 }
 
+/*
+ * called by:
+ *   - util/qemu-coroutine.c|316| <<coroutine_pool_refill_local>> local_pool_cleanup_init_once();
+ *   - util/qemu-coroutine.c|380| <<coroutine_pool_put>> local_pool_cleanup_init_once();
+ */
 /* Ensure the atexit notifier is registered */
 static void local_pool_cleanup_init_once(void)
 {
@@ -110,10 +337,58 @@ static void local_pool_cleanup_init_once(void)
     }
 }
 
+/*
+ * 没有iothread应该就是mainloop
+ * (gdb) bt
+ * #0  coroutine_pool_get_local () at ../util/qemu-coroutine.c:116
+ * #1  0x00005555561cb785 in coroutine_pool_get () at ../util/qemu-coroutine.c:181
+ * #2  0x00005555561cb8c7 in qemu_coroutine_create (entry=0x5555560034f4 <blk_aio_write_entry>, opaque=0x5555582fd4d0)
+ *     at ../util/qemu-coroutine.c:222
+ * #3  0x00005555560033cc in blk_aio_prwv (blk=0x55555846f060, offset=540709888, bytes=1024, iobuf=0x5555577f63e8,
+ *     co_entry=0x5555560034f4 <blk_aio_write_entry>, flags=BDRV_REQ_REGISTERED_BUF, 
+ *     cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350) at ../block/block-backend.c:1600
+ * #4  0x0000555556003927 in blk_aio_pwritev (blk=0x55555846f060, offset=540709888, qiov=0x5555577f63e8,
+ *     flags=BDRV_REQ_REGISTERED_BUF, cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350)
+ *     at ../block/block-backend.c:1722
+ * #5  0x0000555555e2c003 in submit_requests (s=0x5555584688c0, mrb=0x7fffffffd640, start=0, num_reqs=1, niov=-1) at ../hw/block/virtio-blk.c:259
+ * #6  0x0000555555e2c12f in virtio_blk_submit_multireq (s=0x5555584688c0, mrb=0x7fffffffd640) at ../hw/block/virtio-blk.c:294
+ * #7  0x0000555555e2e065 in virtio_blk_handle_vq (s=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1008
+ * #8  0x0000555555e2e0f8 in virtio_blk_handle_output (vdev=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1028
+ * #9  0x0000555555e89d02 in virtio_queue_notify_vq (vq=0x5555584b0960) at ../hw/virtio/virtio.c:2484
+ * #10 0x0000555555e8d53c in virtio_queue_host_notifier_read (n=0x5555584b09d4) at ../hw/virtio/virtio.c:3869
+ * #11 0x00005555561a74d8 in aio_dispatch_handler (ctx=0x55555745ee50, node=0x7fffe43dfba0) at ../util/aio-posix.c:376
+ * #12 0x00005555561a76ed in aio_dispatch_handlers (ctx=0x55555745ee50) at ../util/aio-posix.c:427
+ * #13 0x00005555561a7749 in aio_dispatch (ctx=0x55555745ee50) at ../util/aio-posix.c:437
+ * #14 0x00005555561c9148 in aio_ctx_dispatch (source=0x55555745ee50, callback=0x0, user_data=0x0) at ../util/async.c:361
+ * #15 0x00007ffff6fd394b in g_main_dispatch (context=0x55555745f260) at ../glib/gmain.c:3325
+ * #16 g_main_context_dispatch (context=0x55555745f260) at ../glib/gmain.c:4043
+ * #17 0x00005555561ca939 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #18 0x00005555561ca9c7 in os_host_main_loop_wait (timeout=657614) at ../util/main-loop.c:310
+ * #19 0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #20 0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #21 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #22 0x00005555560d296f in main (argc=25, argv=0x7fffffffdb28) at ../system/main.c:80
+ *
+ * 在以下调用coroutine_pool_get_local():
+ *   - util/qemu-coroutine.c|355| <<coroutine_pool_get>> co = coroutine_pool_get_local();
+ *   - util/qemu-coroutine.c|358| <<coroutine_pool_get>> co = coroutine_pool_get_local();
+ */
 /* Helper to get the next unused coroutine from the local pool */
 static Coroutine *coroutine_pool_get_local(void)
 {
+    /*
+     * 在以下定义get_ptr_local_pool():
+     *   - QEMU_DEFINE_STATIC_CO_TLS(CoroutinePool, local_pool);
+     * 在以下使用get_ptr_local_pool():
+     *   - util/qemu-coroutine.c|93| <<local_pool_cleanup>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|116| <<coroutine_pool_get_local>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|138| <<coroutine_pool_refill_local>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|195| <<coroutine_pool_put>> CoroutinePool *local_pool = get_ptr_local_pool();
+     */
     CoroutinePool *local_pool = get_ptr_local_pool();
+    /*
+     * 每个local应该最多两个batch
+     */
     CoroutinePoolBatch *batch = QSLIST_FIRST(local_pool);
     Coroutine *co;
 
@@ -127,22 +402,66 @@ static Coroutine *coroutine_pool_get_local(void)
 
     if (batch->size == 0) {
         QSLIST_REMOVE_HEAD(local_pool, next);
+        /*
+	 * called by:
+	 *   - util/qemu-coroutine.c|245| <<local_pool_cleanup>> coroutine_pool_batch_delete(batch);
+	 *   - util/qemu-coroutine.c|285| <<coroutine_pool_get_local>> coroutine_pool_batch_delete(batch);
+	 *   - util/qemu-coroutine.c|343| <<coroutine_pool_put_global>> coroutine_pool_batch_delete(batch);
+	 *
+	 * 理论上这时候batch上没有co了
+	 */
         coroutine_pool_batch_delete(batch);
     }
     return co;
 }
 
+/*
+ * 在以下使用coroutine_pool_refill_local():
+ *   - util/qemu-coroutine.c|357| <<coroutine_pool_get>> coroutine_pool_refill_local();
+ */
 /* Get the next batch from the global pool */
 static void coroutine_pool_refill_local(void)
 {
+    /*
+     * 在以下定义get_ptr_local_pool():
+     *   - QEMU_DEFINE_STATIC_CO_TLS(CoroutinePool, local_pool);
+     * 在以下使用get_ptr_local_pool():
+     *   - util/qemu-coroutine.c|93| <<local_pool_cleanup>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|116| <<coroutine_pool_get_local>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|138| <<coroutine_pool_refill_local>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|195| <<coroutine_pool_put>> CoroutinePool *local_pool = get_ptr_local_pool();
+     */
     CoroutinePool *local_pool = get_ptr_local_pool();
     CoroutinePoolBatch *batch = NULL;
 
+    /*
+     * 在以下使用global_pool_lock:
+     *   - util/qemu-coroutine.c|138| <<QSLIST_HEAD>> static QemuMutex global_pool_lock;
+     *   - util/qemu-coroutine.c|276| <<coroutine_pool_refill_local>> WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+     *   - util/qemu-coroutine.c|294| <<coroutine_pool_put_global>> WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+     *   - util/qemu-coroutine.c|554| <<qemu_coroutine_inc_pool_size>> QEMU_LOCK_GUARD(&global_pool_lock);
+     *   - util/qemu-coroutine.c|560| <<qemu_coroutine_dec_pool_size>> QEMU_LOCK_GUARD(&global_pool_lock);
+     *   - util/qemu-coroutine.c|599| <<qemu_coroutine_init>> qemu_mutex_init(&global_pool_lock);
+     */
     WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+        /*
+	 * 在以下使用global_pool:
+	 *   - util/qemu-coroutine.c|139| <<QSLIST_HEAD>> static CoroutinePool global_pool = QSLIST_HEAD_INITIALIZER(global_pool);
+	 *   - util/qemu-coroutine.c|277| <<coroutine_pool_refill_local>> batch = QSLIST_FIRST(&global_pool);
+	 *   - util/qemu-coroutine.c|280| <<coroutine_pool_refill_local>> QSLIST_REMOVE_HEAD(&global_pool, next);
+	 *   - util/qemu-coroutine.c|299| <<coroutine_pool_put_global>> QSLIST_INSERT_HEAD(&global_pool, batch, next);
+	 */
         batch = QSLIST_FIRST(&global_pool);
 
         if (batch) {
             QSLIST_REMOVE_HEAD(&global_pool, next);
+            /*
+	     * 在以下使用global_pool_size:
+	     *   - util/qemu-coroutine.c|140| <<QSLIST_HEAD>> static unsigned int global_pool_size;
+	     *   - util/qemu-coroutine.c|281| <<coroutine_pool_refill_local>> global_pool_size -= batch->size;
+	     *   - util/qemu-coroutine.c|298| <<coroutine_pool_put_global>> if (global_pool_size < max) {
+	     *   - util/qemu-coroutine.c|302| <<coroutine_pool_put_global>> global_pool_size += batch->size;
+	     */
             global_pool_size -= batch->size;
         }
     }
@@ -153,14 +472,46 @@ static void coroutine_pool_refill_local(void)
     }
 }
 
+/*
+ * 在以下调用coroutine_pool_put_global():
+ *   - util/qemu-coroutine.c|469| <<coroutine_pool_put>> coroutine_pool_put_global(batch);
+ */
 /* Add a batch of coroutines to the global pool */
 static void coroutine_pool_put_global(CoroutinePoolBatch *batch)
 {
+    /*
+     * 在以下使用global_pool_lock:
+     *   - util/qemu-coroutine.c|138| <<QSLIST_HEAD>> static QemuMutex global_pool_lock;
+     *   - util/qemu-coroutine.c|276| <<coroutine_pool_refill_local>> WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+     *   - util/qemu-coroutine.c|294| <<coroutine_pool_put_global>> WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+     *   - util/qemu-coroutine.c|554| <<qemu_coroutine_inc_pool_size>> QEMU_LOCK_GUARD(&global_pool_lock);
+     *   - util/qemu-coroutine.c|560| <<qemu_coroutine_dec_pool_size>> QEMU_LOCK_GUARD(&global_pool_lock);
+     *   - util/qemu-coroutine.c|599| <<qemu_coroutine_init>> qemu_mutex_init(&global_pool_lock);
+     */
     WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+        /*
+	 * 在以下使用global_pool_hard_max_size:
+	 *   - util/qemu-coroutine.c|136| <<QSLIST_HEAD>> static unsigned int global_pool_hard_max_size;
+	 *   - util/qemu-coroutine.c|296| <<coroutine_pool_put_global>> unsigned int max = MIN(global_pool_max_size, global_pool_hard_max_size);
+	 *   - util/qemu-coroutine.c|600| <<qemu_coroutine_init>> global_pool_hard_max_size = get_global_pool_hard_max_size();
+	 *
+	 * 在以下使用global_pool_size:
+	 *   - util/qemu-coroutine.c|140| <<QSLIST_HEAD>> static unsigned int global_pool_size;
+	 *   - util/qemu-coroutine.c|281| <<coroutine_pool_refill_local>> global_pool_size -= batch->size;
+	 *   - util/qemu-coroutine.c|298| <<coroutine_pool_put_global>> if (global_pool_size < max) {
+	 *   - util/qemu-coroutine.c|302| <<coroutine_pool_put_global>> global_pool_size += batch->size;
+	 */
         unsigned int max = MIN(global_pool_max_size,
                                global_pool_hard_max_size);
 
         if (global_pool_size < max) {
+            /*
+	     * 在以下使用global_pool:
+	     *   - util/qemu-coroutine.c|139| <<QSLIST_HEAD>> static CoroutinePool global_pool = QSLIST_HEAD_INITIALIZER(global_pool);
+	     *   - util/qemu-coroutine.c|277| <<coroutine_pool_refill_local>> batch = QSLIST_FIRST(&global_pool);
+	     *   - util/qemu-coroutine.c|280| <<coroutine_pool_refill_local>> QSLIST_REMOVE_HEAD(&global_pool, next);
+	     *   - util/qemu-coroutine.c|299| <<coroutine_pool_put_global>> QSLIST_INSERT_HEAD(&global_pool, batch, next);
+	     */
             QSLIST_INSERT_HEAD(&global_pool, batch, next);
 
             /* Overshooting the max pool size is allowed */
@@ -173,21 +524,77 @@ static void coroutine_pool_put_global(CoroutinePoolBatch *batch)
     coroutine_pool_batch_delete(batch);
 }
 
+/*
+ * 没有iothread应该就是mainloop
+ * (gdb) bt
+ * #0  coroutine_pool_get_local () at ../util/qemu-coroutine.c:116
+ * #1  0x00005555561cb785 in coroutine_pool_get () at ../util/qemu-coroutine.c:181
+ * #2  0x00005555561cb8c7 in qemu_coroutine_create (entry=0x5555560034f4 <blk_aio_write_entry>, opaque=0x5555582fd4d0)
+ *     at ../util/qemu-coroutine.c:222
+ * #3  0x00005555560033cc in blk_aio_prwv (blk=0x55555846f060, offset=540709888, bytes=1024, iobuf=0x5555577f63e8,
+ *     co_entry=0x5555560034f4 <blk_aio_write_entry>, flags=BDRV_REQ_REGISTERED_BUF, 
+ *     cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350) at ../block/block-backend.c:1600
+ * #4  0x0000555556003927 in blk_aio_pwritev (blk=0x55555846f060, offset=540709888, qiov=0x5555577f63e8,
+ *     flags=BDRV_REQ_REGISTERED_BUF, cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350)
+ *     at ../block/block-backend.c:1722
+ * #5  0x0000555555e2c003 in submit_requests (s=0x5555584688c0, mrb=0x7fffffffd640, start=0, num_reqs=1, niov=-1) at ../hw/block/virtio-blk.c:259
+ * #6  0x0000555555e2c12f in virtio_blk_submit_multireq (s=0x5555584688c0, mrb=0x7fffffffd640) at ../hw/block/virtio-blk.c:294
+ * #7  0x0000555555e2e065 in virtio_blk_handle_vq (s=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1008
+ * #8  0x0000555555e2e0f8 in virtio_blk_handle_output (vdev=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1028
+ * #9  0x0000555555e89d02 in virtio_queue_notify_vq (vq=0x5555584b0960) at ../hw/virtio/virtio.c:2484
+ * #10 0x0000555555e8d53c in virtio_queue_host_notifier_read (n=0x5555584b09d4) at ../hw/virtio/virtio.c:3869
+ * #11 0x00005555561a74d8 in aio_dispatch_handler (ctx=0x55555745ee50, node=0x7fffe43dfba0) at ../util/aio-posix.c:376
+ * #12 0x00005555561a76ed in aio_dispatch_handlers (ctx=0x55555745ee50) at ../util/aio-posix.c:427
+ * #13 0x00005555561a7749 in aio_dispatch (ctx=0x55555745ee50) at ../util/aio-posix.c:437
+ * #14 0x00005555561c9148 in aio_ctx_dispatch (source=0x55555745ee50, callback=0x0, user_data=0x0) at ../util/async.c:361
+ * #15 0x00007ffff6fd394b in g_main_dispatch (context=0x55555745f260) at ../glib/gmain.c:3325
+ * #16 g_main_context_dispatch (context=0x55555745f260) at ../glib/gmain.c:4043
+ * #17 0x00005555561ca939 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #18 0x00005555561ca9c7 in os_host_main_loop_wait (timeout=657614) at ../util/main-loop.c:310
+ * #19 0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #20 0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #21 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #22 0x00005555560d296f in main (argc=25, argv=0x7fffffffdb28) at ../system/main.c:80
+ *
+ * called by:
+ *   - util/qemu-coroutine.c|262| <<qemu_coroutine_create>> co = coroutine_pool_get();
+ */
 /* Get the next unused coroutine from the pool or return NULL */
 static Coroutine *coroutine_pool_get(void)
 {
     Coroutine *co;
 
+    /*
+     * 在以下调用coroutine_pool_get_local():
+     *   - util/qemu-coroutine.c|355| <<coroutine_pool_get>> co = coroutine_pool_get_local();
+     *   - util/qemu-coroutine.c|358| <<coroutine_pool_get>> co = coroutine_pool_get_local();
+     */
     co = coroutine_pool_get_local();
     if (!co) {
+        /*
+	 * 只在以下调用refill
+	 */
         coroutine_pool_refill_local();
         co = coroutine_pool_get_local();
     }
     return co;
 }
 
+/*
+ * 在以下调用coroutine_pool_put():
+ *   - util/qemu-coroutine.c|549| <<coroutine_delete>> coroutine_pool_put(co);
+ */
 static void coroutine_pool_put(Coroutine *co)
 {
+    /*
+     * 在以下定义get_ptr_local_pool():
+     *   - QEMU_DEFINE_STATIC_CO_TLS(CoroutinePool, local_pool);
+     * 在以下使用get_ptr_local_pool():
+     *   - util/qemu-coroutine.c|93| <<local_pool_cleanup>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|116| <<coroutine_pool_get_local>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|138| <<coroutine_pool_refill_local>> CoroutinePool *local_pool = get_ptr_local_pool();
+     *   - util/qemu-coroutine.c|195| <<coroutine_pool_put>> CoroutinePool *local_pool = get_ptr_local_pool();
+     */
     CoroutinePool *local_pool = get_ptr_local_pool();
     CoroutinePoolBatch *batch = QSLIST_FIRST(local_pool);
 
@@ -197,9 +604,18 @@ static void coroutine_pool_put(Coroutine *co)
         local_pool_cleanup_init_once();
     }
 
+    /*
+     * 在以下使用COROUTINE_POOL_BATCH_MAX_SIZE:
+     *   - util/qemu-coroutine.c|25| <<global>> COROUTINE_POOL_BATCH_MAX_SIZE = 128,
+     *   - util/qemu-coroutine.c|65| <<QSLIST_HEAD>> static unsigned int global_pool_max_size = COROUTINE_POOL_BATCH_MAX_SIZE;
+     *   - util/qemu-coroutine.c|204| <<coroutine_pool_put>> if (unlikely(batch->size >= COROUTINE_POOL_BATCH_MAX_SIZE)) {
+     */
     if (unlikely(batch->size >= COROUTINE_POOL_BATCH_MAX_SIZE)) {
         CoroutinePoolBatch *next = QSLIST_NEXT(batch, next);
 
+        /*
+	 * Local pool最多两个batch
+	 */
         /* Is the local pool full? */
         if (next) {
             QSLIST_REMOVE_HEAD(local_pool, next);
@@ -214,15 +630,92 @@ static void coroutine_pool_put(Coroutine *co)
     batch->size++;
 }
 
+/*
+ * 没有iothread应该就是mainloop
+ * (gdb) bt
+ * #0  coroutine_pool_get_local () at ../util/qemu-coroutine.c:116
+ * #1  0x00005555561cb785 in coroutine_pool_get () at ../util/qemu-coroutine.c:181
+ * #2  0x00005555561cb8c7 in qemu_coroutine_create (entry=0x5555560034f4 <blk_aio_write_entry>, opaque=0x5555582fd4d0)
+ *     at ../util/qemu-coroutine.c:222
+ * #3  0x00005555560033cc in blk_aio_prwv (blk=0x55555846f060, offset=540709888, bytes=1024, iobuf=0x5555577f63e8,
+ *     co_entry=0x5555560034f4 <blk_aio_write_entry>, flags=BDRV_REQ_REGISTERED_BUF,
+ *     cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350) at ../block/block-backend.c:1600
+ * #4  0x0000555556003927 in blk_aio_pwritev (blk=0x55555846f060, offset=540709888, qiov=0x5555577f63e8,
+ *     flags=BDRV_REQ_REGISTERED_BUF, cb=0x555555e2b9c9 <virtio_blk_rw_complete>, opaque=0x5555577f6350)
+ *     at ../block/block-backend.c:1722
+ * #5  0x0000555555e2c003 in submit_requests (s=0x5555584688c0, mrb=0x7fffffffd640, start=0, num_reqs=1, niov=-1) at ../hw/block/virtio-blk.c:259
+ * #6  0x0000555555e2c12f in virtio_blk_submit_multireq (s=0x5555584688c0, mrb=0x7fffffffd640) at ../hw/block/virtio-blk.c:294
+ * #7  0x0000555555e2e065 in virtio_blk_handle_vq (s=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1008
+ * #8  0x0000555555e2e0f8 in virtio_blk_handle_output (vdev=0x5555584688c0, vq=0x5555584b0960) at ../hw/block/virtio-blk.c:1028
+ * #9  0x0000555555e89d02 in virtio_queue_notify_vq (vq=0x5555584b0960) at ../hw/virtio/virtio.c:2484
+ * #10 0x0000555555e8d53c in virtio_queue_host_notifier_read (n=0x5555584b09d4) at ../hw/virtio/virtio.c:3869
+ * #11 0x00005555561a74d8 in aio_dispatch_handler (ctx=0x55555745ee50, node=0x7fffe43dfba0) at ../util/aio-posix.c:376
+ * #12 0x00005555561a76ed in aio_dispatch_handlers (ctx=0x55555745ee50) at ../util/aio-posix.c:427
+ * #13 0x00005555561a7749 in aio_dispatch (ctx=0x55555745ee50) at ../util/aio-posix.c:437
+ * #14 0x00005555561c9148 in aio_ctx_dispatch (source=0x55555745ee50, callback=0x0, user_data=0x0) at ../util/async.c:361
+ * #15 0x00007ffff6fd394b in g_main_dispatch (context=0x55555745f260) at ../glib/gmain.c:3325
+ * #16 g_main_context_dispatch (context=0x55555745f260) at ../glib/gmain.c:4043
+ * #17 0x00005555561ca939 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #18 0x00005555561ca9c7 in os_host_main_loop_wait (timeout=657614) at ../util/main-loop.c:310
+ * #19 0x00005555561caaf6 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #20 0x0000555555c2dc8a in qemu_main_loop () at ../system/runstate.c:835
+ * #21 0x00005555560d28b5 in qemu_default_main (opaque=0x0) at ../system/main.c:50
+ * #22 0x00005555560d296f in main (argc=25, argv=0x7fffffffdb28) at ../system/main.c:80
+ *
+ * 非test的对qemu_coroutine_create()的调用:
+ *   - block/aio_task.c|94| <<aio_task_pool_start_task>> qemu_coroutine_enter(qemu_coroutine_create(aio_task_co, task));
+ *   - block/blkverify.c|214| <<blkverify_co_prwv>> co_a = qemu_coroutine_create(blkverify_do_test_req, r);
+ *   - block/blkverify.c|215| <<blkverify_co_prwv>> co_b = qemu_coroutine_create(blkverify_do_raw_req, r);
+ *   - block/block-backend.c|1600| <<blk_aio_prwv>> co = qemu_coroutine_create(co_entry, acb);
+ *   - block/block-backend.c|1906| <<blk_aio_zone_report>> co = qemu_coroutine_create(blk_aio_zone_report_entry, acb);
+ *   - block/block-backend.c|1947| <<blk_aio_zone_mgmt>> co = qemu_coroutine_create(blk_aio_zone_mgmt_entry, acb);
+ *   - block/block-backend.c|1987| <<blk_aio_zone_append>> co = qemu_coroutine_create(blk_aio_zone_append_entry, acb);
+ *   - block/block-copy.c|989| <<block_copy_async>> .co = qemu_coroutine_create(block_copy_async_co_entry, call_state),
+ *   - block/export/vduse-blk.c|109| <<vduse_blk_vq_handler>> qemu_coroutine_create(vduse_blk_virtio_process_req, req);
+ *   - block/export/vhost-user-blk-server.c|99| <<vu_blk_process_vq>> qemu_coroutine_create(vu_blk_virtio_process_req, req);
+ *   - block/mirror.c|455| <<mirror_perform>> co = qemu_coroutine_create(mirror_co_read, op);
+ *   - block/mirror.c|458| <<mirror_perform>> co = qemu_coroutine_create(mirror_co_zero, op);
+ *   - block/mirror.c|461| <<mirror_perform>> co = qemu_coroutine_create(mirror_co_discard, op);
+ *   - block/qcow2.c|1969| <<qcow2_open>> qemu_coroutine_create(qcow2_open_entry, &qoc));
+ *   - block/qed.c|326| <<qed_need_check_timer_cb>> Coroutine *co = qemu_coroutine_create(qed_need_check_timer_entry, opaque);
+ *   - block/qed.c|383| <<bdrv_qed_drain_begin>> co = qemu_coroutine_create(qed_need_check_timer_entry, s);
+ *   - block/qed.c|594| <<bdrv_qed_open>> qemu_coroutine_enter(qemu_coroutine_create(bdrv_qed_open_entry, &qoc));
+ *   - block/quorum.c|331| <<quorum_rewrite_bad_versions>> co = qemu_coroutine_create(quorum_rewrite_entry, &data);
+ *   - block/quorum.c|629| <<read_quorum_children>> co = qemu_coroutine_create(read_quorum_children_entry, &data);
+ *   - block/quorum.c|743| <<quorum_co_pwritev>> co = qemu_coroutine_create(write_quorum_entry, &data);
+ *   - block/throttle-groups.c|445| <<throttle_group_restart_queue>> co = qemu_coroutine_create(throttle_group_restart_queue_entry, rd);
+ *   - hw/9pfs/9p.c|4220| <<pdu_submit>> co = qemu_coroutine_create(handler, pdu);
+ *   - hw/9pfs/9p.c|4377| <<v9fs_reset>> co = qemu_coroutine_create(virtfs_co_reset, &data);
+ *   - hw/remote/remote-obj.c|124| <<remote_object_machine_done>> co = qemu_coroutine_create(mpqemu_remote_msg_loop_co, comdev);
+ *   - job.c|1128| <<job_start>> job->co = qemu_coroutine_create(job_co_entry, job);
+ *   - migration/migration.c|942| <<migration_incoming_process>> Coroutine *co = qemu_coroutine_create(process_incoming_migration_co, NULL);
+ *   - monitor/hmp.c|1167| <<handle_hmp_command>> Coroutine *co = qemu_coroutine_create(handle_hmp_command_co, &data);
+ *   - monitor/monitor.c|724| <<monitor_init_globals>> qmp_dispatcher_co = qemu_coroutine_create(monitor_qmp_dispatcher_co, NULL);
+ *   - nbd/server.c|3209| <<nbd_client_receive_next_request>> client->recv_coroutine = qemu_coroutine_create(nbd_trip, req);
+ *   - nbd/server.c|3294| <<nbd_client_receive_next_request>> co = qemu_coroutine_create(nbd_co_client_start, client);
+ *   - net/colo-compare.c|822| <<compare_chr_send>> sendco->co = qemu_coroutine_create(_compare_chr_send, sendco);
+ *   - net/filter-mirror.c|127| <<filter_send>> Coroutine *co = qemu_coroutine_create(filter_send_co, &data);
+ *   - qemu-img.c|2138| <<convert_do_copy>> s->co[i] = qemu_coroutine_create(convert_co_do_copy, s);
+ *   - scsi/qemu-pr-helper.c|816| <<accept_client>> prh->co = qemu_coroutine_create(prh_co_entry, prh);
+ *   - tools/i386/qemu-vmsr-helper.c|295| <<accept_client>> vmsrh->co = qemu_coroutine_create(vh_co_entry, vmsrh);
+ *   - util/qemu-co-timeout.c|75| <<qemu_co_timeout>> co = qemu_coroutine_create(qemu_co_timeout_entry, s);
+ *   - util/vhost-user-server.c|446| <<vhost_user_server_attach_aio_context>> server->co_trip = qemu_coroutine_create(vu_client_trip, server);
+ */
 Coroutine *qemu_coroutine_create(CoroutineEntry *entry, void *opaque)
 {
     Coroutine *co = NULL;
 
     if (IS_ENABLED(CONFIG_COROUTINE_POOL)) {
+        /*
+	 * 只在此处调用
+	 */
         co = coroutine_pool_get();
     }
 
     if (!co) {
+        /*
+	 * 只在此处调用
+	 */
         co = qemu_coroutine_new();
     }
 
@@ -232,6 +725,10 @@ Coroutine *qemu_coroutine_create(CoroutineEntry *entry, void *opaque)
     return co;
 }
 
+/*
+ * 在以下调用coroutine_delete():
+ *   - util/qemu-coroutine.c|769| <<qemu_aio_coroutine_enter>> coroutine_delete(to);
+ */
 static void coroutine_delete(Coroutine *co)
 {
     co->caller = NULL;
@@ -243,6 +740,12 @@ static void coroutine_delete(Coroutine *co)
     }
 }
 
+/*
+ * 在以下调用qemu_aio_coroutine_enter():
+ *   - util/async.c|571| <<co_schedule_bh_cb>> qemu_aio_coroutine_enter(ctx, co);
+ *   - util/async.c|710| <<aio_co_enter>> qemu_aio_coroutine_enter(ctx, co);
+ *   - util/qemu-coroutine.c|625| <<qemu_coroutine_enter>> qemu_aio_coroutine_enter(qemu_get_current_aio_context(), co);
+ */
 void qemu_aio_coroutine_enter(AioContext *ctx, Coroutine *co)
 {
     QSIMPLEQ_HEAD(, Coroutine) pending = QSIMPLEQ_HEAD_INITIALIZER(pending);
@@ -339,6 +842,26 @@ void coroutine_fn qemu_coroutine_yield(void)
     qemu_coroutine_switch(self, to, COROUTINE_YIELD);
 }
 
+/*
+ * 注释:
+ * Return true if the coroutine is currently entered
+ *
+ * A coroutine is "entered" if it has not yielded from the current
+ * qemu_coroutine_enter() call used to run it.  This does not mean that the
+ * coroutine is currently executing code since it may have transferred control
+ * to another coroutine using qemu_coroutine_enter().
+ *
+ * When several coroutines enter each other there may be no way to know which
+ * ones have already been entered.  In such situations this function can be
+ * used to avoid recursively entering coroutines.
+ *
+ * 非test调用的地方:
+ *   - block/io_uring.c|219| <<luring_process_completions>> if (!qemu_coroutine_entered(luringcb->co)) {
+ *   - block/linux-aio.c|108| <<qemu_laio_process_completion>> if (!qemu_coroutine_entered(laiocb->co)) {
+ *   - hw/9pfs/xen-9p-backend.c|312| <<xen_9pfs_bh>> wait = ring->co != NULL && qemu_coroutine_entered(ring->co);
+ *   - nbd/server.c|776| <<nbd_server_tls_handshake>> if (!qemu_coroutine_entered(data->co)) {
+ *   - util/qemu-coroutine.c|630| <<qemu_coroutine_enter_if_inactive>> if (!qemu_coroutine_entered(co)) {
+ */
 bool qemu_coroutine_entered(Coroutine *co)
 {
     return co->caller;
@@ -349,18 +872,58 @@ AioContext *qemu_coroutine_get_aio_context(Coroutine *co)
     return co->ctx;
 }
 
+/*
+ * cakked by:
+ *   - hw/block/virtio-blk.c|2142| <<virtio_blk_device_realize>> qemu_coroutine_inc_pool_size(conf->num_queues * conf->queue_size / 2);
+ */
 void qemu_coroutine_inc_pool_size(unsigned int additional_pool_size)
 {
+    /*
+     * 在以下使用global_pool_lock:
+     *   - util/qemu-coroutine.c|138| <<QSLIST_HEAD>> static QemuMutex global_pool_lock;
+     *   - util/qemu-coroutine.c|276| <<coroutine_pool_refill_local>> WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+     *   - util/qemu-coroutine.c|294| <<coroutine_pool_put_global>> WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+     *   - util/qemu-coroutine.c|554| <<qemu_coroutine_inc_pool_size>> QEMU_LOCK_GUARD(&global_pool_lock);
+     *   - util/qemu-coroutine.c|560| <<qemu_coroutine_dec_pool_size>> QEMU_LOCK_GUARD(&global_pool_lock);
+     *   - util/qemu-coroutine.c|599| <<qemu_coroutine_init>> qemu_mutex_init(&global_pool_lock);
+     */
     QEMU_LOCK_GUARD(&global_pool_lock);
+    /*
+     * 在以下使用global_pool_max_size:
+     *   - util/qemu-coroutine.c|389| <<coroutine_pool_put_global>> unsigned int max = MIN(global_pool_max_size,
+     *   - util/qemu-coroutine.c|664| <<qemu_coroutine_inc_pool_size>> global_pool_max_size += additional_pool_size;
+     *   - util/qemu-coroutine.c|670| <<qemu_coroutine_dec_pool_size>> global_pool_max_size -= removing_pool_size;
+     */
     global_pool_max_size += additional_pool_size;
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|2190| <<virtio_blk_device_unrealize>> qemu_coroutine_dec_pool_size(conf->num_queues * conf->queue_size / 2);
+ */
 void qemu_coroutine_dec_pool_size(unsigned int removing_pool_size)
 {
     QEMU_LOCK_GUARD(&global_pool_lock);
+    /*
+     * 在以下使用global_pool_max_size:
+     *   - util/qemu-coroutine.c|389| <<coroutine_pool_put_global>> unsigned int max = MIN(global_pool_max_size,
+     *   - util/qemu-coroutine.c|664| <<qemu_coroutine_inc_pool_size>> global_pool_max_size += additional_pool_size;
+     *   - util/qemu-coroutine.c|670| <<qemu_coroutine_dec_pool_size>> global_pool_max_size -= removing_pool_size;
+     */
     global_pool_max_size -= removing_pool_size;
 }
 
+/*
+ * (gdb) bt
+ * #0  get_global_pool_hard_max_size () at ../util/qemu-coroutine.c:365
+ * #1  0x00005555561cbec7 in qemu_coroutine_init () at ../util/qemu-coroutine.c:400
+ * #2  0x000055555623192d in __libc_csu_init ()
+ * #3  0x00007ffff4bdb838 in __libc_start_main () from /lib64/libc.so.6
+ * #4  0x000055555588a80e in _start ()
+ *
+ * 在以下调用get_global_pool_hard_max_size():
+ *   - util/qemu-coroutine.c|715| <<qemu_coroutine_init>> global_pool_hard_max_size = get_global_pool_hard_max_size();
+ */
 static unsigned int get_global_pool_hard_max_size(void)
 {
 #ifdef __linux__
@@ -396,6 +959,21 @@ static unsigned int get_global_pool_hard_max_size(void)
 
 static void __attribute__((constructor)) qemu_coroutine_init(void)
 {
+    /*
+     * 在以下使用global_pool_lock:
+     *   - util/qemu-coroutine.c|138| <<QSLIST_HEAD>> static QemuMutex global_pool_lock;
+     *   - util/qemu-coroutine.c|276| <<coroutine_pool_refill_local>> WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+     *   - util/qemu-coroutine.c|294| <<coroutine_pool_put_global>> WITH_QEMU_LOCK_GUARD(&global_pool_lock) {
+     *   - util/qemu-coroutine.c|554| <<qemu_coroutine_inc_pool_size>> QEMU_LOCK_GUARD(&global_pool_lock);
+     *   - util/qemu-coroutine.c|560| <<qemu_coroutine_dec_pool_size>> QEMU_LOCK_GUARD(&global_pool_lock);
+     *   - util/qemu-coroutine.c|599| <<qemu_coroutine_init>> qemu_mutex_init(&global_pool_lock);
+     */
     qemu_mutex_init(&global_pool_lock);
+    /*
+     * 在以下使用global_pool_hard_max_size:
+     *   - util/qemu-coroutine.c|136| <<QSLIST_HEAD>> static unsigned int global_pool_hard_max_size;
+     *   - util/qemu-coroutine.c|296| <<coroutine_pool_put_global>> unsigned int max = MIN(global_pool_max_size, global_pool_hard_max_size);
+     *   - util/qemu-coroutine.c|600| <<qemu_coroutine_init>> global_pool_hard_max_size = get_global_pool_hard_max_size();
+     */
     global_pool_hard_max_size = get_global_pool_hard_max_size();
 }
diff --git a/util/qemu-timer.c b/util/qemu-timer.c
index 788466fe2..dd38b9460 100644
--- a/util/qemu-timer.c
+++ b/util/qemu-timer.c
@@ -322,6 +322,26 @@ int qemu_timeout_ns_to_ms(int64_t ns)
 /* qemu implementation of g_poll which uses a nanosecond timeout but is
  * otherwise identical to g_poll
  */
+/*
+ * 注释:
+ * qemu_poll_ns:
+ * @fds: Array of file descriptors
+ * @nfds: number of file descriptors
+ * @timeout: timeout in nanoseconds
+ *
+ * Perform a poll like g_poll but with a timeout in nanoseconds.
+ * See g_poll documentation for further details.
+ *
+ * Returns: number of fds ready
+ *
+ * 在以下使用qemu_poll_ns():
+ *   - hw/remote/vfio-user-obj.c|247| <<vfu_object_attach_ctx>> (void )qemu_poll_ns(pfds, 1, 500 * (int64_t)SCALE_MS);
+ *   - migration/rdma.c|1539| <<qemu_rdma_wait_comp_channel>> switch (qemu_poll_ns(pfds, 2, 100 * 1000 * 1000)) {
+ *   - util/fdmon-epoll.c|69| <<fdmon_epoll_wait>> ret = qemu_poll_ns(&pfd, 1, timeout);
+ *   - util/fdmon-poll.c|79| <<fdmon_poll_wait>> ret = qemu_poll_ns(pollfds, npfd, timeout);
+ *   - util/main-loop.c|338| <<os_host_main_loop_wait>> ret = qemu_poll_ns((GPollFD *)gpollfds->data, gpollfds->len, timeout);
+ *   - util/main-loop.c|560| <<os_host_main_loop_wait>> g_poll_ret = qemu_poll_ns(poll_fds, n_poll_fds + w->num, poll_timeout_ns);
+ */
 int qemu_poll_ns(GPollFD *fds, guint nfds, int64_t timeout)
 {
 #ifdef CONFIG_PPOLL
@@ -653,6 +673,10 @@ uint64_t timer_expire_time_ns(QEMUTimer *ts)
     return timer_pending(ts) ? ts->expire_time : -1;
 }
 
+/*
+ * called by:
+ *   - util/main-loop.c|665| <<main_loop_wait>> qemu_clock_run_all_timers();
+ */
 bool qemu_clock_run_all_timers(void)
 {
     bool progress = false;
diff --git a/util/rcu.c b/util/rcu.c
index fa32c942e..825057e60 100644
--- a/util/rcu.c
+++ b/util/rcu.c
@@ -43,11 +43,27 @@
 #define RCU_GP_LOCKED           (1UL << 0)
 #define RCU_GP_CTR              (1UL << 1)
 
+/*
+ *   - util/rcu.c|46| <<global>> unsigned long rcu_gp_ctr = RCU_GP_LOCKED
+ *   - include/qemu/rcu.h|87| <<rcu_read_lock>> ctr = qatomic_read(&rcu_gp_ctr);
+  4 util/rcu.c|69| <<rcu_gp_ongoing>> return v && (v != rcu_gp_ctr);
+  5 util/rcu.c|237| <<synchronize_rcu>> if (sizeof(rcu_gp_ctr) < 8) {
+  6 util/rcu.c|243| <<synchronize_rcu>> qatomic_set(&rcu_gp_ctr, rcu_gp_ctr ^ RCU_GP_CTR);
+  7 util/rcu.c|250| <<synchronize_rcu>> qatomic_set(&rcu_gp_ctr, rcu_gp_ctr ^ RCU_GP_CTR);
+  8 util/rcu.c|253| <<synchronize_rcu>> qatomic_set(&rcu_gp_ctr, rcu_gp_ctr + RCU_GP_CTR);
+ */
 unsigned long rcu_gp_ctr = RCU_GP_LOCKED;
 
 QemuEvent rcu_gp_event;
 static int in_drain_call_rcu;
 static QemuMutex rcu_registry_lock;
+/*
+ * 在以下使用rcu_sync_lock:
+ *   - util/rcu.c|173| <<synchronize_rcu>> QEMU_LOCK_GUARD(&rcu_sync_lock);
+ *   - util/rcu.c|436| <<rcu_init_complete>> qemu_mutex_init(&rcu_sync_lock);
+ *   - util/rcu.c|469| <<rcu_init_lock>> qemu_mutex_lock(&rcu_sync_lock);
+ *   - util/rcu.c|480| <<rcu_init_unlock>> qemu_mutex_unlock(&rcu_sync_lock);
+ */
 static QemuMutex rcu_sync_lock;
 
 /*
@@ -69,8 +85,24 @@ QEMU_DEFINE_CO_TLS(struct rcu_reader_data, rcu_reader)
 
 /* Protected by rcu_registry_lock.  */
 typedef QLIST_HEAD(, rcu_reader_data) ThreadList;
+/*
+ * 在以下使用registry:
+ *   - util/rcu.c|72| <<QLIST_HEAD>> static ThreadList registry = QLIST_HEAD_INITIALIZER(registry);
+ *   - util/rcu.c|86| <<wait_for_readers>> QLIST_FOREACH(index, &registry, node) {
+ *   - util/rcu.c|100| <<wait_for_readers>> QLIST_FOREACH_SAFE(index, &registry, node, tmp) {
+ *   - util/rcu.c|114| <<wait_for_readers>> if (QLIST_EMPTY(&registry)) {
+ *   - util/rcu.c|141| <<wait_for_readers>> QLIST_SWAP(&registry, &qsreaders, node);
+ *   - util/rcu.c|157| <<synchronize_rcu>> if (!QLIST_EMPTY(&registry)) {
+ *   - util/rcu.c|377| <<rcu_register_thread>> QLIST_INSERT_HEAD(&registry, get_ptr_rcu_reader(), node);
+ *   - util/rcu.c|460| <<rcu_init_child>> memset(&registry, 0, sizeof(registry));
+ */
 static ThreadList registry = QLIST_HEAD_INITIALIZER(registry);
 
+/*
+ * 在以下使用wait_for_readers():
+ *   - util/rcu.c|165| <<synchronize_rcu>> wait_for_readers();
+ *   - util/rcu.c|172| <<synchronize_rcu>> wait_for_readers();
+ */
 /* Wait for previous parity/grace period to be empty of readers.  */
 static void wait_for_readers(void)
 {
@@ -83,6 +115,17 @@ static void wait_for_readers(void)
          */
         qemu_event_reset(&rcu_gp_event);
 
+	/*
+	 * 在以下使用registry:
+	 *   - util/rcu.c|72| <<QLIST_HEAD>> static ThreadList registry = QLIST_HEAD_INITIALIZER(registry);
+	 *   - util/rcu.c|86| <<wait_for_readers>> QLIST_FOREACH(index, &registry, node) {
+	 *   - util/rcu.c|100| <<wait_for_readers>> QLIST_FOREACH_SAFE(index, &registry, node, tmp) {
+	 *   - util/rcu.c|114| <<wait_for_readers>> if (QLIST_EMPTY(&registry)) {
+	 *   - util/rcu.c|141| <<wait_for_readers>> QLIST_SWAP(&registry, &qsreaders, node);
+	 *   - util/rcu.c|157| <<synchronize_rcu>> if (!QLIST_EMPTY(&registry)) {
+	 *   - util/rcu.c|377| <<rcu_register_thread>> QLIST_INSERT_HEAD(&registry, get_ptr_rcu_reader(), node);
+	 *   - util/rcu.c|460| <<rcu_init_child>> memset(&registry, 0, sizeof(registry));
+	 */
         QLIST_FOREACH(index, &registry, node) {
             qatomic_set(&index->waiting, true);
         }
@@ -97,7 +140,23 @@ static void wait_for_readers(void)
          */
         smp_mb_global();
 
+	/*
+	 * 在以下使用registry:
+	 *   - util/rcu.c|72| <<QLIST_HEAD>> static ThreadList registry = QLIST_HEAD_INITIALIZER(registry);
+	 *   - util/rcu.c|86| <<wait_for_readers>> QLIST_FOREACH(index, &registry, node) {
+	 *   - util/rcu.c|100| <<wait_for_readers>> QLIST_FOREACH_SAFE(index, &registry, node, tmp) {
+	 *   - util/rcu.c|114| <<wait_for_readers>> if (QLIST_EMPTY(&registry)) {
+	 *   - util/rcu.c|141| <<wait_for_readers>> QLIST_SWAP(&registry, &qsreaders, node);
+	 *   - util/rcu.c|157| <<synchronize_rcu>> if (!QLIST_EMPTY(&registry)) {
+	 *   - util/rcu.c|377| <<rcu_register_thread>> QLIST_INSERT_HEAD(&registry, get_ptr_rcu_reader(), node);
+	 *   - util/rcu.c|460| <<rcu_init_child>> memset(&registry, 0, sizeof(registry));
+	 */
         QLIST_FOREACH_SAFE(index, &registry, node, tmp) {
+            /*
+	     * 注释:
+	     * Check whether a quiescent state was crossed between the beginning of
+	     * update_counter_and_wait and now.
+	     */
             if (!rcu_gp_ongoing(&index->ctr)) {
                 QLIST_REMOVE(index, node);
                 QLIST_INSERT_HEAD(&qsreaders, index, node);
@@ -141,8 +200,26 @@ static void wait_for_readers(void)
     QLIST_SWAP(&registry, &qsreaders, node);
 }
 
+/*
+ * 在以下使用synchronize_rcu():
+ *   - tests/unit/rcutorture.c|156| <<rcu_update_perf_test>> synchronize_rcu();
+ *   - tests/unit/rcutorture.c|337| <<rcu_update_stress_test>> synchronize_rcu();
+ *   - tests/unit/rcutorture.c|354| <<rcu_fake_update_stress_test>> synchronize_rcu();
+ *   - tests/unit/test-rcu-list.c|244| <<rcu_q_updater>> synchronize_rcu();
+ *   - tests/unit/test-rcu-list.c|246| <<rcu_q_updater>> synchronize_rcu();
+ *   - tests/unit/test-rcu-list.c|306| <<rcu_qtest>> synchronize_rcu();
+ *   - tests/unit/test-rcu-list.c|310| <<rcu_qtest>> synchronize_rcu();
+ *   - util/rcu.c|285| <<call_rcu_thread>> synchronize_rcu();
+ */
 void synchronize_rcu(void)
 {
+    /*
+     * 在以下使用rcu_sync_lock:
+     *   - util/rcu.c|173| <<synchronize_rcu>> QEMU_LOCK_GUARD(&rcu_sync_lock);
+     *   - util/rcu.c|436| <<rcu_init_complete>> qemu_mutex_init(&rcu_sync_lock);
+     *   - util/rcu.c|469| <<rcu_init_lock>> qemu_mutex_lock(&rcu_sync_lock);
+     *   - util/rcu.c|480| <<rcu_init_unlock>> qemu_mutex_unlock(&rcu_sync_lock);
+     */
     QEMU_LOCK_GUARD(&rcu_sync_lock);
 
     /* Write RCU-protected pointers before reading p_rcu_reader->ctr.
@@ -154,6 +231,17 @@ void synchronize_rcu(void)
     smp_mb_global();
 
     QEMU_LOCK_GUARD(&rcu_registry_lock);
+    /*
+     * 在以下使用registry:
+     *   - util/rcu.c|72| <<QLIST_HEAD>> static ThreadList registry = QLIST_HEAD_INITIALIZER(registry);
+     *   - util/rcu.c|86| <<wait_for_readers>> QLIST_FOREACH(index, &registry, node) {
+     *   - util/rcu.c|100| <<wait_for_readers>> QLIST_FOREACH_SAFE(index, &registry, node, tmp) {
+     *   - util/rcu.c|114| <<wait_for_readers>> if (QLIST_EMPTY(&registry)) {
+     *   - util/rcu.c|141| <<wait_for_readers>> QLIST_SWAP(&registry, &qsreaders, node);
+     *   - util/rcu.c|157| <<synchronize_rcu>> if (!QLIST_EMPTY(&registry)) {
+     *   - util/rcu.c|377| <<rcu_register_thread>> QLIST_INSERT_HEAD(&registry, get_ptr_rcu_reader(), node);
+     *   - util/rcu.c|460| <<rcu_init_child>> memset(&registry, 0, sizeof(registry));
+     */
     if (!QLIST_EMPTY(&registry)) {
         if (sizeof(rcu_gp_ctr) < 8) {
             /* For architectures with 32-bit longs, a two-subphases algorithm
@@ -162,6 +250,11 @@ void synchronize_rcu(void)
              * Switch parity: 0 -> 1, 1 -> 0.
              */
             qatomic_set(&rcu_gp_ctr, rcu_gp_ctr ^ RCU_GP_CTR);
+	    /*
+	     * 在以下使用wait_for_readers():
+             *   - util/rcu.c|165| <<synchronize_rcu>> wait_for_readers();
+             *   - util/rcu.c|172| <<synchronize_rcu>> wait_for_readers();
+	     */
             wait_for_readers();
             qatomic_set(&rcu_gp_ctr, rcu_gp_ctr ^ RCU_GP_CTR);
         } else {
@@ -169,6 +262,11 @@ void synchronize_rcu(void)
             qatomic_set(&rcu_gp_ctr, rcu_gp_ctr + RCU_GP_CTR);
         }
 
+	/*
+	 * 在以下使用wait_for_readers():
+         *   - util/rcu.c|165| <<synchronize_rcu>> wait_for_readers();
+         *   - util/rcu.c|172| <<synchronize_rcu>> wait_for_readers();
+	 */
         wait_for_readers();
     }
 }
@@ -252,6 +350,10 @@ retry:
     return node;
 }
 
+/*
+ * 在以下使用call_rcu_thread():
+ *   - util/rcu.c|444| <<rcu_init_complete>> qemu_thread_create(&thread, "call_rcu", call_rcu_thread, NULL, QEMU_THREAD_DETACHED);
+ */
 static void *call_rcu_thread(void *opaque)
 {
     struct rcu_head *node;
@@ -305,6 +407,14 @@ static void *call_rcu_thread(void *opaque)
     abort();
 }
 
+/*
+ * 在以下使用call_rcu1():
+ *   - include/qemu/rcu.h|150| <<call_rcu>> call_rcu1(({ \
+ *   - include/qemu/rcu.h|159| <<g_free_rcu>> call_rcu1(({ \
+ *   - tests/unit/test-rcu-list.c|223| <<rcu_q_updater>> call_rcu1(&prev_el->rcu, reclaim_list_el);
+ *   - tests/unit/test-rcu-list.c|300| <<rcu_qtest>> call_rcu1(&prev_el->rcu, reclaim_list_el);
+ *   - util/rcu.c|456| <<drain_call_rcu>> call_rcu1(&rcu_drain.rcu, drain_rcu_callback);
+ */
 void call_rcu1(struct rcu_head *node, void (*func)(struct rcu_head *node))
 {
     node->func = func;
@@ -374,6 +484,17 @@ void rcu_register_thread(void)
 {
     assert(get_ptr_rcu_reader()->ctr == 0);
     qemu_mutex_lock(&rcu_registry_lock);
+    /*
+     * 在以下使用registry:
+     *   - util/rcu.c|72| <<QLIST_HEAD>> static ThreadList registry = QLIST_HEAD_INITIALIZER(registry);
+     *   - util/rcu.c|86| <<wait_for_readers>> QLIST_FOREACH(index, &registry, node) {
+     *   - util/rcu.c|100| <<wait_for_readers>> QLIST_FOREACH_SAFE(index, &registry, node, tmp) {
+     *   - util/rcu.c|114| <<wait_for_readers>> if (QLIST_EMPTY(&registry)) {
+     *   - util/rcu.c|141| <<wait_for_readers>> QLIST_SWAP(&registry, &qsreaders, node);
+     *   - util/rcu.c|157| <<synchronize_rcu>> if (!QLIST_EMPTY(&registry)) {
+     *   - util/rcu.c|377| <<rcu_register_thread>> QLIST_INSERT_HEAD(&registry, get_ptr_rcu_reader(), node);
+     *   - util/rcu.c|460| <<rcu_init_child>> memset(&registry, 0, sizeof(registry));
+     */
     QLIST_INSERT_HEAD(&registry, get_ptr_rcu_reader(), node);
     qemu_mutex_unlock(&rcu_registry_lock);
 }
@@ -399,11 +520,23 @@ void rcu_remove_force_rcu_notifier(Notifier *n)
     qemu_mutex_unlock(&rcu_registry_lock);
 }
 
+/*
+ * 在以下使用rcu_init_complete():
+ *   - util/rcu.c|597| <<rcu_init_child>> rcu_init_complete();
+ *   - util/rcu.c|607| <<rcu_init>> rcu_init_complete();
+ */
 static void rcu_init_complete(void)
 {
     QemuThread thread;
 
     qemu_mutex_init(&rcu_registry_lock);
+    /*
+     * 在以下使用rcu_sync_lock:
+     *   - util/rcu.c|173| <<synchronize_rcu>> QEMU_LOCK_GUARD(&rcu_sync_lock);
+     *   - util/rcu.c|436| <<rcu_init_complete>> qemu_mutex_init(&rcu_sync_lock);
+     *   - util/rcu.c|469| <<rcu_init_lock>> qemu_mutex_lock(&rcu_sync_lock);
+     *   - util/rcu.c|480| <<rcu_init_unlock>> qemu_mutex_unlock(&rcu_sync_lock);
+     */
     qemu_mutex_init(&rcu_sync_lock);
     qemu_event_init(&rcu_gp_event, true);
 
@@ -431,12 +564,23 @@ void rcu_disable_atfork(void)
 }
 
 #ifdef CONFIG_POSIX
+/*
+ * 在以下使用rcu_init_lock():
+ *   - util/rcu.c|605| <<rcu_init>> pthread_atfork(rcu_init_lock, rcu_init_unlock, rcu_init_child);
+ */
 static void rcu_init_lock(void)
 {
     if (atfork_depth < 1) {
         return;
     }
 
+    /*
+     * 在以下使用rcu_sync_lock:
+     *   - util/rcu.c|173| <<synchronize_rcu>> QEMU_LOCK_GUARD(&rcu_sync_lock);
+     *   - util/rcu.c|436| <<rcu_init_complete>> qemu_mutex_init(&rcu_sync_lock);
+     *   - util/rcu.c|469| <<rcu_init_lock>> qemu_mutex_lock(&rcu_sync_lock);
+     *   - util/rcu.c|480| <<rcu_init_unlock>> qemu_mutex_unlock(&rcu_sync_lock);
+     */
     qemu_mutex_lock(&rcu_sync_lock);
     qemu_mutex_lock(&rcu_registry_lock);
 }
@@ -448,15 +592,37 @@ static void rcu_init_unlock(void)
     }
 
     qemu_mutex_unlock(&rcu_registry_lock);
+    /*
+     * 在以下使用rcu_sync_lock:
+     *   - util/rcu.c|173| <<synchronize_rcu>> QEMU_LOCK_GUARD(&rcu_sync_lock);
+     *   - util/rcu.c|436| <<rcu_init_complete>> qemu_mutex_init(&rcu_sync_lock);
+     *   - util/rcu.c|469| <<rcu_init_lock>> qemu_mutex_lock(&rcu_sync_lock);
+     *   - util/rcu.c|480| <<rcu_init_unlock>> qemu_mutex_unlock(&rcu_sync_lock);
+     */
     qemu_mutex_unlock(&rcu_sync_lock);
 }
 
+/*
+ * 在以下使用rcu_init_child():
+ *   - util/rcu.c|605| <<rcu_init>> pthread_atfork(rcu_init_lock, rcu_init_unlock, rcu_init_child);
+ */
 static void rcu_init_child(void)
 {
     if (atfork_depth < 1) {
         return;
     }
 
+    /*
+     * 在以下使用registry:
+     *   - util/rcu.c|72| <<QLIST_HEAD>> static ThreadList registry = QLIST_HEAD_INITIALIZER(registry);
+     *   - util/rcu.c|86| <<wait_for_readers>> QLIST_FOREACH(index, &registry, node) {
+     *   - util/rcu.c|100| <<wait_for_readers>> QLIST_FOREACH_SAFE(index, &registry, node, tmp) {
+     *   - util/rcu.c|114| <<wait_for_readers>> if (QLIST_EMPTY(&registry)) {
+     *   - util/rcu.c|141| <<wait_for_readers>> QLIST_SWAP(&registry, &qsreaders, node);
+     *   - util/rcu.c|157| <<synchronize_rcu>> if (!QLIST_EMPTY(&registry)) {
+     *   - util/rcu.c|377| <<rcu_register_thread>> QLIST_INSERT_HEAD(&registry, get_ptr_rcu_reader(), node);
+     *   - util/rcu.c|460| <<rcu_init_child>> memset(&registry, 0, sizeof(registry));
+     */
     memset(&registry, 0, sizeof(registry));
     rcu_init_complete();
 }
-- 
2.39.5 (Apple Git-154)

