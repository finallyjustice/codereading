From c76810d94a713338aff78bb113687e71c256d1b8 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 15 Apr 2019 16:05:47 +0800
Subject: [PATCH 1/1] block comment for block and drivers for mainline v5.1-rc5

This is for mainline v5.1-rc5

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 fs/io_uring.c                   |  19 +++++
 include/linux/percpu-refcount.h |  90 ++++++++++++++++++++++
 lib/percpu-refcount.c           | 163 ++++++++++++++++++++++++++++++++++++++++
 3 files changed, 272 insertions(+)

diff --git a/fs/io_uring.c b/fs/io_uring.c
index 89aa841..71d7aa3 100644
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@ -272,6 +272,16 @@ struct sock *io_uring_get_socket(struct file *file)
 }
 EXPORT_SYMBOL(io_uring_get_socket);
 
+/*
+ * callstack的一个例子
+ * [0] io_ring_ctx_ref_free
+ * [0] percpu_ref_switch_to_atomic_rcu
+ * [0] rcu_core
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] smp_apic_timer_interrupt
+ * [0] apic_timer_interrupt
+ */
 static void io_ring_ctx_ref_free(struct percpu_ref *ref)
 {
 	struct io_ring_ctx *ctx = container_of(ref, struct io_ring_ctx, refs);
@@ -2532,6 +2542,10 @@ static int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,
 	return ret;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|2594| <<io_ring_ctx_wait_and_kill>> io_ring_ctx_free(ctx);
+ */
 static void io_ring_ctx_free(struct io_ring_ctx *ctx)
 {
 	io_finish_async(ctx);
@@ -2582,6 +2596,11 @@ static int io_uring_fasync(int fd, struct file *file, int on)
 	return fasync_helper(fd, file, on, &ctx->cq_fasync);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2616| <<io_uring_release>> io_ring_ctx_wait_and_kill(ctx);
+ *   - fs/io_uring.c|2895| <<io_uring_create>> io_ring_ctx_wait_and_kill(ctx);
+ */
 static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)
 {
 	mutex_lock(&ctx->uring_lock);
diff --git a/include/linux/percpu-refcount.h b/include/linux/percpu-refcount.h
index b297cd1..e1e686c 100644
--- a/include/linux/percpu-refcount.h
+++ b/include/linux/percpu-refcount.h
@@ -76,12 +76,24 @@ enum {
 	 * with this flag, the ref will stay in atomic mode until
 	 * percpu_ref_switch_to_percpu() is invoked on it.
 	 */
+	/*
+	 * used by:
+	 *   - block/blk-core.c|533| <<blk_alloc_queue_node>> PERCPU_REF_INIT_ATOMIC, GFP_KERNEL))
+	 *   - drivers/infiniband/sw/rdmavt/mr.c|735| <<rvt_alloc_fmr>> PERCPU_REF_INIT_ATOMIC);
+	 *   - lib/percpu-refcount.c|105| <<percpu_ref_init>> ref->force_atomic = flags & PERCPU_REF_INIT_ATOMIC;
+	 *   - lib/percpu-refcount.c|107| <<percpu_ref_init>> if (flags & (PERCPU_REF_INIT_ATOMIC | PERCPU_REF_INIT_DEAD))
+	 */
 	PERCPU_REF_INIT_ATOMIC	= 1 << 0,
 
 	/*
 	 * Start dead w/ ref == 0 in atomic mode.  Must be revived with
 	 * percpu_ref_reinit() before used.  Implies INIT_ATOMIC.
 	 */
+	/*
+	 * called by:
+	 *   - lib/percpu-refcount.c|107| <<percpu_ref_init>> if (flags & (PERCPU_REF_INIT_ATOMIC | PERCPU_REF_INIT_DEAD))
+	 *   - lib/percpu-refcount.c|112| <<percpu_ref_init>> if (flags & PERCPU_REF_INIT_DEAD)
+	 */
 	PERCPU_REF_INIT_DEAD	= 1 << 1,
 };
 
@@ -123,8 +135,36 @@ void percpu_ref_reinit(struct percpu_ref *ref);
  *
  * There are no implied RCU grace periods between kill and release.
  */
+/*
+ * called by:
+ *   - block/blk-cgroup.c|415| <<blkg_destroy>> percpu_ref_kill(&blkg->refcnt);
+ *   - block/blk-mq.c|150| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+ *   - drivers/dax/device.c|46| <<dev_dax_percpu_kill>> percpu_ref_kill(ref);
+ *   - drivers/infiniband/sw/rdmavt/mr.c|275| <<rvt_free_lkey>> percpu_ref_kill(&mr->refcount);
+ *   - drivers/nvme/target/core.c|585| <<nvmet_ns_disable>> percpu_ref_kill(&ns->ref);
+ *   - drivers/pci/p2pdma.c|96| <<pci_p2pdma_percpu_kill>> percpu_ref_kill(ref);
+ *   - drivers/target/target_core_transport.c|2915| <<target_sess_cmd_list_set_waiting>> percpu_ref_kill(&se_sess->cmd_count);
+ *   - drivers/target/target_core_transport.c|2947| <<transport_clear_lun_ref>> percpu_ref_kill(&lun->lun_ref);
+ *   - fs/aio.c|631| <<free_ioctx_users>> percpu_ref_kill(&ctx->reqs);
+ *   - fs/aio.c|850| <<kill_ioctx>> percpu_ref_kill(&ctx->users);
+ *   - fs/io_uring.c|2602| <<io_ring_ctx_wait_and_kill>> percpu_ref_kill(&ctx->refs);
+ *   - fs/io_uring.c|2942| <<__io_uring_register>> percpu_ref_kill(&ctx->refs);
+ *   - include/linux/genhd.h|696| <<hd_struct_kill>> percpu_ref_kill(&part->ref);
+ *   - kernel/cgroup/cgroup.c|2173| <<cgroup_kill_sb>> percpu_ref_kill(&root->cgrp.self.refcnt);
+ *   - kernel/cgroup/cgroup.c|5302| <<__acquires>> percpu_ref_kill(&cgrp->self.refcnt);
+ *   - mm/backing-dev.c|526| <<cgwb_kill>> percpu_ref_kill(&wb->refcnt);
+ *   - mm/hmm.c|990| <<hmm_devmem_ref_kill>> percpu_ref_kill(ref);
+ *
+ * 调用的时候ref->percpu_count_ptr不能已经dead了
+ * 设置ref->percpu_count_ptr的dead, 调用__percpu_ref_switch_to_percpu()
+ * (confirm_kill是NULL)
+ * 最后put一下percpu_ref_put(ref)
+ */
 static inline void percpu_ref_kill(struct percpu_ref *ref)
 {
+	/*
+	 * 调用的时候ref->percpu_count_ptr不能已经dead了
+	 */
 	percpu_ref_kill_and_confirm(ref, NULL);
 }
 
@@ -134,6 +174,12 @@ static inline void percpu_ref_kill(struct percpu_ref *ref)
  * because doing so forces the compiler to generate two conditional
  * branches as it can't assume that @ref->percpu_count is not NULL.
  */
+/*
+ * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+ * 则返回false
+ * 否则更新参数为ref->percpu_count_ptr
+ * 并且返回true
+ */
 static inline bool __ref_is_percpu(struct percpu_ref *ref,
 					  unsigned long __percpu **percpu_countp)
 {
@@ -174,12 +220,24 @@ static inline bool __ref_is_percpu(struct percpu_ref *ref,
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * 对于percpu的(非atomic的)不会增加ref->count
+ *
+ * 对于atomic的或者dead的, atomic_long_add(nr, &ref->count);
+ * 对于percpu的, atomic_long_add(nr, &ref->count);
+ */
 static inline void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
 {
 	unsigned long __percpu *percpu_count;
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	if (__ref_is_percpu(ref, &percpu_count))
 		this_cpu_add(*percpu_count, nr);
 	else
@@ -196,6 +254,12 @@ static inline void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * 对于percpu的(非atomic的)不会增加ref->count
+ *
+ * 对于atomic的或者dead的, atomic_long_add(nr, &ref->count);
+ * 对于percpu的, atomic_long_add(nr, &ref->count);
+ */
 static inline void percpu_ref_get(struct percpu_ref *ref)
 {
 	percpu_ref_get_many(ref, 1);
@@ -210,6 +274,20 @@ static inline void percpu_ref_get(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|391| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+ *   - block/blk-mq.c|939| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+ *   - fs/io_uring.c|415| <<io_get_req>> if (!percpu_ref_tryget(&ctx->refs))
+ *   - fs/io_uring.c|2678| <<SYSCALL_DEFINE6>> if (!percpu_ref_tryget(&ctx->refs))
+ *   - include/linux/backing-dev-defs.h|242| <<wb_tryget>> return percpu_ref_tryget(&wb->refcnt);
+ *   - include/linux/blk-cgroup.h|502| <<blkg_tryget>> return blkg && percpu_ref_tryget(&blkg->refcnt);
+ *   - include/linux/cgroup.h|340| <<css_tryget>> return percpu_ref_tryget(&css->refcnt);
+ *
+ * 如果是percpu的直接this_cpu_inc(*percpu_count)并且返回true
+ * 否则用atomic_long_inc_not_zero(&ref->count)只有在ref->count不为0的时候才增加
+ * 如果之前为0则返回0
+ */
 static inline bool percpu_ref_tryget(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
@@ -217,6 +295,12 @@ static inline bool percpu_ref_tryget(struct percpu_ref *ref)
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 调用的时候ref->percpu_count_ptr不能已经dead了
+	 * 设置ref->percpu_count_ptr的dead, 调用__percpu_ref_switch_to_percpu()
+	 * (confirm_kill是NULL)
+	 * 最后put一下percpu_ref_put(ref)
+	 */
 	if (__ref_is_percpu(ref, &percpu_count)) {
 		this_cpu_inc(*percpu_count);
 		ret = true;
@@ -279,6 +363,12 @@ static inline void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	if (__ref_is_percpu(ref, &percpu_count))
 		this_cpu_sub(*percpu_count, nr);
 	else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
diff --git a/lib/percpu-refcount.c b/lib/percpu-refcount.c
index 9877682..f745cca 100644
--- a/lib/percpu-refcount.c
+++ b/lib/percpu-refcount.c
@@ -33,9 +33,32 @@
 
 #define PERCPU_COUNT_BIAS	(1LU << (BITS_PER_LONG - 1))
 
+/*
+ * used by:
+ *   - lib/percpu-refcount.c|271| <<__percpu_ref_switch_mode>> lockdep_assert_held(&percpu_ref_switch_lock);
+ *   - lib/percpu-refcount.c|279| <<__percpu_ref_switch_mode>> percpu_ref_switch_lock);
+ *   - lib/percpu-refcount.c|312| <<percpu_ref_switch_to_atomic>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|317| <<percpu_ref_switch_to_atomic>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|358| <<percpu_ref_switch_to_percpu>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|363| <<percpu_ref_switch_to_percpu>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|395| <<percpu_ref_kill_and_confirm>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|404| <<percpu_ref_kill_and_confirm>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|446| <<percpu_ref_resurrect>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|455| <<percpu_ref_resurrect>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ */
 static DEFINE_SPINLOCK(percpu_ref_switch_lock);
+/*
+ * 在以下使用:
+ *   - lib/percpu-refcount.c|129| <<percpu_ref_call_confirm_rcu>> wake_up_all(&percpu_ref_switch_waitq);
+ *   - lib/percpu-refcount.c|243| <<__percpu_ref_switch_mode>> wait_event_lock_irq(percpu_ref_switch_waitq, !ref->confirm_switch,
+ *   - lib/percpu-refcount.c|297| <<percpu_ref_switch_to_atomic_sync>> wait_event(percpu_ref_switch_waitq, !ref->confirm_switch);
+ */
 static DECLARE_WAIT_QUEUE_HEAD(percpu_ref_switch_waitq);
 
+/*
+ * 把ref->percpu_count_ptr后面的flag去掉, 返回percpu的地址
+ * 因为在percpu_ref_init()中, 无论用不用atomic, percpu的地址一定会分配
+ */
 static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)
 {
 	return (unsigned long __percpu *)
@@ -56,6 +79,12 @@ static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)
  * Note that @release must not sleep - it may potentially be called from RCU
  * callback context by percpu_ref_kill().
  */
+/*
+ * 调用的几个例子:
+ *   - fs/io_uring.c|291| <<io_ring_ctx_alloc>> if (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free, 0, GFP_KERNEL)) {
+ *
+ * 无论用不用atomic, percpu的地址一定会分配
+ */
 int percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,
 		    unsigned int flags, gfp_t gfp)
 {
@@ -63,6 +92,11 @@ int percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,
 			     __alignof__(unsigned long));
 	unsigned long start_count = 0;
 
+	/*
+	 * 类型: unsigned long percpu_count_ptr;
+	 *
+	 * 不一定用, 但是一定会分配percpu的内存
+	 */
 	ref->percpu_count_ptr = (unsigned long)
 		__alloc_percpu_gfp(sizeof(unsigned long), align, gfp);
 	if (!ref->percpu_count_ptr)
@@ -98,8 +132,20 @@ EXPORT_SYMBOL_GPL(percpu_ref_init);
  * where percpu_ref_init() succeeded but other parts of the initialization
  * of the embedding object failed.
  */
+/*
+ * 调用的几个例子:
+ *   - fs/io_uring.c|2568| <<io_ring_ctx_free>> percpu_ref_exit(&ctx->refs);
+ *   - block/blk-core.c|380| <<blk_cleanup_queue>> percpu_ref_exit(&q->q_usage_counter);
+ *   - block/blk-core.c|542| <<blk_alloc_queue_node>> percpu_ref_exit(&q->q_usage_counter);
+ *
+ * 核心思想是释放percpu的内存
+ */
 void percpu_ref_exit(struct percpu_ref *ref)
 {
+	/*
+	 * 把ref->percpu_count_ptr后面的flag去掉, 返回percpu的地址
+	 * 因为在percpu_ref_init()中, 无论用不用atomic, percpu的地址一定会分配
+	 */
 	unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
 
 	if (percpu_count) {
@@ -111,6 +157,10 @@ void percpu_ref_exit(struct percpu_ref *ref)
 }
 EXPORT_SYMBOL_GPL(percpu_ref_exit);
 
+/*
+ * called by only:
+ *   - lib/percpu-refcount.c|177| <<percpu_ref_switch_to_atomic_rcu>> percpu_ref_call_confirm_rcu(rcu);
+ */
 static void percpu_ref_call_confirm_rcu(struct rcu_head *rcu)
 {
 	struct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);
@@ -123,6 +173,10 @@ static void percpu_ref_call_confirm_rcu(struct rcu_head *rcu)
 	percpu_ref_put(ref);
 }
 
+/*
+ * called only by:
+ *   - lib/percpu-refcount.c|193| <<__percpu_ref_switch_to_atomic>> call_rcu(&ref->rcu, percpu_ref_switch_to_atomic_rcu);
+ */
 static void percpu_ref_switch_to_atomic_rcu(struct rcu_head *rcu)
 {
 	struct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);
@@ -158,10 +212,18 @@ static void percpu_ref_switch_to_atomic_rcu(struct rcu_head *rcu)
 	percpu_ref_call_confirm_rcu(rcu);
 }
 
+/*
+ * used only by:
+ *   - lib/percpu-refcount.c|239| <<__percpu_ref_switch_to_atomic>> ref->confirm_switch = confirm_switch ?: percpu_ref_noop_confirm_switch;
+ */
 static void percpu_ref_noop_confirm_switch(struct percpu_ref *ref)
 {
 }
 
+/*
+ * called by only:
+ *   - lib/percpu-refcount.c|235| <<__percpu_ref_switch_mode>> __percpu_ref_switch_to_atomic(ref, confirm_switch);
+ */
 static void __percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 					  percpu_ref_func_t *confirm_switch)
 {
@@ -184,8 +246,19 @@ static void __percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 	call_rcu(&ref->rcu, percpu_ref_switch_to_atomic_rcu);
 }
 
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|237| <<__percpu_ref_switch_mode>> __percpu_ref_switch_to_percpu(ref);
+ *
+ * 核心思想是把percpu的counter们都清0，
+ * 然后把ref->percpu_count_ptr去掉atomic重新写入ref->percpu_count_ptr
+ */
 static void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 {
+	/*
+	 * 把ref->percpu_count_ptr后面的flag去掉, 返回percpu的地址
+	 * 因为在percpu_ref_init()中, 无论用不用atomic, percpu的地址一定会分配
+	 */
 	unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
 	int cpu;
 
@@ -194,6 +267,9 @@ static void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 	if (!(ref->percpu_count_ptr & __PERCPU_REF_ATOMIC))
 		return;
 
+	/*
+	 * 这里为什么要加PERCPU_COUNT_BIAS??
+	 */
 	atomic_long_add(PERCPU_COUNT_BIAS, &ref->count);
 
 	/*
@@ -205,10 +281,23 @@ static void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 	for_each_possible_cpu(cpu)
 		*per_cpu_ptr(percpu_count, cpu) = 0;
 
+	/*
+	 * 把ref->percpu_count_ptr去掉atomic重新写入ref->percpu_count_ptr
+	 */
 	smp_store_release(&ref->percpu_count_ptr,
 			  ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC);
 }
 
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|280| <<percpu_ref_switch_to_atomic>> __percpu_ref_switch_mode(ref, confirm_switch);
+ *   - lib/percpu-refcount.c|326| <<percpu_ref_switch_to_percpu>> __percpu_ref_switch_mode(ref, NULL);
+ *   - lib/percpu-refcount.c|360| <<percpu_ref_kill_and_confirm>> __percpu_ref_switch_mode(ref, confirm_kill);
+ *   - lib/percpu-refcount.c|412| <<percpu_ref_resurrect>> __percpu_ref_switch_mode(ref, NULL);
+ *
+ * 只有在要转为atomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+ * 否则调用__percpu_ref_switch_to_percpu()
+ */
 static void __percpu_ref_switch_mode(struct percpu_ref *ref,
 				     percpu_ref_func_t *confirm_switch)
 {
@@ -219,9 +308,30 @@ static void __percpu_ref_switch_mode(struct percpu_ref *ref,
 	 * its completion.  If the caller ensures that ATOMIC switching
 	 * isn't in progress, this function can be called from any context.
 	 */
+	/*
+	 * sleep until a condition gets true. The condition is checked
+	 * under the lock. This is expected to be called with the lock
+	 * taken.
+	 *
+	 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
+	 * @condition evaluates to true. The @condition is checked each time
+	 * the waitqueue @wq_head is woken up.
+	 *     
+	 * wake_up() has to be called after changing any variable that could
+	 * change the result of the wait condition.
+	 *
+	 * This is supposed to be called while holding the lock. The lock is
+	 * dropped before going to sleep and is reacquired afterwards.
+	 */
 	wait_event_lock_irq(percpu_ref_switch_waitq, !ref->confirm_switch,
 			    percpu_ref_switch_lock);
 
+	/*
+	 * __percpu_ref_switch_to_atomic()和__percpu_ref_switch_to_percpu()
+	 * 都只在这里调用
+	 *
+	 * 只有在要转为stomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+	 */
 	if (ref->force_atomic || (ref->percpu_count_ptr & __PERCPU_REF_DEAD))
 		__percpu_ref_switch_to_atomic(ref, confirm_switch);
 	else
@@ -248,6 +358,10 @@ static void __percpu_ref_switch_mode(struct percpu_ref *ref,
  * mode.  If the caller ensures that @ref is not in the process of
  * switching to atomic mode, this function can be called from any context.
  */
+/*
+ * called by only:
+ *   - lib/percpu-refcount.c|381| <<percpu_ref_switch_to_atomic_sync>> percpu_ref_switch_to_atomic(ref, NULL);
+ */
 void percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 				 percpu_ref_func_t *confirm_switch)
 {
@@ -270,6 +384,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_atomic);
  * switch to complete.  Caller must ensure that no other thread
  * will switch back to percpu mode.
  */
+/*
+ * called by:
+ *   - block/blk-pm.c|86| <<blk_pre_runtime_suspend>> percpu_ref_switch_to_atomic_sync(&q->q_usage_counter);
+ *   - drivers/md/md.c|2347| <<set_in_sync>> percpu_ref_switch_to_atomic_sync(&mddev->writes_pending);
+ */
 void percpu_ref_switch_to_atomic_sync(struct percpu_ref *ref)
 {
 	percpu_ref_switch_to_atomic(ref, NULL);
@@ -295,6 +414,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_atomic_sync);
  * mode.  If the caller ensures that @ref is not in the process of
  * switching to atomic mode, this function can be called from any context.
  */
+/*
+ * called by:
+ *   - block/blk-sysfs.c|926| <<blk_register_queue>> percpu_ref_switch_to_percpu(&q->q_usage_counter);
+ *   - drivers/md/md.c|2361| <<set_in_sync>> percpu_ref_switch_to_percpu(&mddev->writes_pending);
+ */
 void percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 {
 	unsigned long flags;
@@ -325,6 +449,16 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_percpu);
  *
  * There are no implied RCU grace periods between kill and release.
  */
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|734| <<nvmet_sq_destroy>> percpu_ref_kill_and_confirm(&sq->ref, nvmet_confirm_sq);
+ *   - include/linux/percpu-refcount.h|128| <<percpu_ref_kill>> percpu_ref_kill_and_confirm(ref, NULL);
+ *   - kernel/cgroup/cgroup.c|5218| <<kill_css>> percpu_ref_kill_and_confirm(&css->refcnt, css_killed_ref_fn);
+ *
+ * 调用的时候ref->percpu_count_ptr不能已经dead了
+ * 设置ref->percpu_count_ptr的dead, 调用__percpu_ref_switch_to_percpu()
+ * 最后put一下percpu_ref_put(ref)
+ */
 void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
 				 percpu_ref_func_t *confirm_kill)
 {
@@ -332,10 +466,17 @@ void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
 
 	spin_lock_irqsave(&percpu_ref_switch_lock, flags);
 
+	/* 调用的时候ref->percpu_count_ptr不能已经dead了 */
 	WARN_ONCE(ref->percpu_count_ptr & __PERCPU_REF_DEAD,
 		  "%s called more than once on %pf!", __func__, ref->release);
 
 	ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	/*
+	 * 只有在要转为atomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+	 * 否则调用__percpu_ref_switch_to_percpu()
+	 *
+	 * 因为上面设置了dead, 所以先面会调用__percpu_ref_switch_to_percpu()
+	 */
 	__percpu_ref_switch_mode(ref, confirm_kill);
 	percpu_ref_put(ref);
 
@@ -354,6 +495,10 @@ EXPORT_SYMBOL_GPL(percpu_ref_kill_and_confirm);
  * Note that percpu_ref_tryget[_live]() are safe to perform on @ref while
  * this function is in progress.
  */
+/*
+ * called by only:
+ *   - fs/io_uring.c|2976| <<__io_uring_register>> percpu_ref_reinit(&ctx->refs);
+ */
 void percpu_ref_reinit(struct percpu_ref *ref)
 {
 	WARN_ON_ONCE(!percpu_ref_is_zero(ref));
@@ -376,6 +521,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_reinit);
  * Note that percpu_ref_tryget[_live]() are safe to perform on @ref while
  * this function is in progress.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|206| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+ *   - lib/percpu-refcount.c|473| <<percpu_ref_reinit>> percpu_ref_resurrect(ref);
+ */
 void percpu_ref_resurrect(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
@@ -383,11 +533,24 @@ void percpu_ref_resurrect(struct percpu_ref *ref)
 
 	spin_lock_irqsave(&percpu_ref_switch_lock, flags);
 
+	/*
+	 * 到了这一步了必须是dead了才能resurrect
+	 */
 	WARN_ON_ONCE(!(ref->percpu_count_ptr & __PERCPU_REF_DEAD));
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	WARN_ON_ONCE(__ref_is_percpu(ref, &percpu_count));
 
 	ref->percpu_count_ptr &= ~__PERCPU_REF_DEAD;
 	percpu_ref_get(ref);
+	/*
+	 * 只有在要转为atomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+	 * 否则调用__percpu_ref_switch_to_percpu()
+	 */
 	__percpu_ref_switch_mode(ref, NULL);
 
 	spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
-- 
2.7.4

