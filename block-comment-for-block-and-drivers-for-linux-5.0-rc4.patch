From b27bdb6b45090de7e37f4697c6216f1503951fbc Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang@oracle.com>
Date: Tue, 12 Feb 2019 23:59:49 +0800
Subject: [PATCH 1/1] block comment for block and drivers for linux-5.0-rc4

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/blk-merge.c      |   4 +
 block/blk-mq-cpumap.c  |  18 ++
 block/blk-mq-debugfs.c |  16 ++
 block/blk-mq-pci.c     |   6 +
 block/blk-mq-sched.c   | 106 ++++++++
 block/blk-mq-sched.h   |   4 +
 block/blk-mq-sysfs.c   |  45 ++++
 block/blk-mq-tag.c     |  24 ++
 block/blk-mq-tag.h     |  34 +++
 block/blk-mq-virtio.c  |   5 +
 block/blk-mq.c         | 696 +++++++++++++++++++++++++++++++++++++++++++++++++
 block/blk-mq.h         |  68 +++++
 block/blk-settings.c   |   9 +
 block/elevator.c       |  16 ++
 include/linux/blk-mq.h | 227 ++++++++++++++++
 include/linux/blkdev.h |  80 ++++++
 16 files changed, 1358 insertions(+)

diff --git a/block/blk-merge.c b/block/blk-merge.c
index 71e9ac0..ea4c2d1 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -242,6 +242,10 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	return do_split ? new : NULL;
 }
 
+/*
+ * 根据块设备请求队列的limits.max_sectors和limits.max_segmetns
+ * 来拆分bio,适应设备缓存.会在函数blk_set_default_limits中设置
+ */
 void blk_queue_split(struct request_queue *q, struct bio **bio)
 {
 	struct bio *split, *res;
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 03a5348..e55553f 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -31,6 +31,19 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|50| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|52| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3223| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[0]);
+ *   - block/blk-mq.c|3526| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[0]);
+ *   - drivers/nvme/host/pci.c|512| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|1830| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2083| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2084| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|6940| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1818| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[0]);
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
@@ -63,11 +76,16 @@ EXPORT_SYMBOL_GPL(blk_mq_map_queues);
  * We have no quick way of doing reverse lookups. This is only used at
  * queue init time, so runtime isn't important.
  */
+/*
+ * 找到index代表的hw queue对应的cpu (sw queue)
+ * 返回这个cpu的node
+ */
 int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int index)
 {
 	int i;
 
 	for_each_possible_cpu(i) {
+		/* 猜测是每一个sw queue对应的hw queue??? */
 		if (index == qmap->mq_map[i])
 			return local_memory_node(cpu_to_node(i));
 	}
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index f812083..1767dcc 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -836,6 +836,15 @@ static const struct blk_mq_debugfs_attr blk_mq_debugfs_ctx_attrs[] = {
 	{},
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|865| <<blk_mq_debugfs_register>> if (!debugfs_create_files(q->debugfs_dir, q,
+ *   - block/blk-mq-debugfs.c|920| <<blk_mq_debugfs_register_ctx>> if (!debugfs_create_files(ctx_dir, ctx, blk_mq_debugfs_ctx_attrs))
+ *   - block/blk-mq-debugfs.c|941| <<blk_mq_debugfs_register_hctx>> if (!debugfs_create_files(hctx->debugfs_dir, hctx,
+ *   - block/blk-mq-debugfs.c|1000| <<blk_mq_debugfs_register_sched>> if (!debugfs_create_files(q->sched_debugfs_dir, q,
+ *   - block/blk-mq-debugfs.c|1046| <<blk_mq_debugfs_register_rqos>> if (!debugfs_create_files(rqos->debugfs_dir, rqos,
+ *   - block/blk-mq-debugfs.c|1077| <<blk_mq_debugfs_register_sched_hctx>> if (!debugfs_create_files(hctx->sched_debugfs_dir, hctx,
+ */
 static bool debugfs_create_files(struct dentry *parent, void *data,
 				 const struct blk_mq_debugfs_attr *attr)
 {
@@ -849,6 +858,10 @@ static bool debugfs_create_files(struct dentry *parent, void *data,
 	return true;
 }
 
+/*
+ * called by only:
+ *   - block/blk-sysfs.c|944| <<blk_register_queue>> blk_mq_debugfs_register(q);
+ */
 int blk_mq_debugfs_register(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -857,6 +870,9 @@ int blk_mq_debugfs_register(struct request_queue *q)
 	if (!blk_debugfs_root)
 		return -ENOENT;
 
+	/*
+	 * 添加/sys/kernel/debug/block/vda
+	 */
 	q->debugfs_dir = debugfs_create_dir(kobject_name(q->kobj.parent),
 					    blk_debugfs_root);
 	if (!q->debugfs_dir)
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index 1dce185..09e1ef4 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -31,6 +31,12 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|510| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|6942| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5792| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[0],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index 140933e..4daad99 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -16,6 +16,9 @@
 #include "blk-mq-tag.h"
 #include "blk-wbt.h"
 
+/*
+ * 没人调用
+ */
 void blk_mq_sched_free_hctx_data(struct request_queue *q,
 				 void (*exit)(struct blk_mq_hw_ctx *))
 {
@@ -31,6 +34,10 @@ void blk_mq_sched_free_hctx_data(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_free_hctx_data);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|432| <<blk_mq_get_request>> blk_mq_sched_assign_ioc(rq);
+ */
 void blk_mq_sched_assign_ioc(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -61,6 +68,10 @@ void blk_mq_sched_assign_ioc(struct request *rq)
  * Mark a hardware queue as needing a restart. For shared queues, maintain
  * a count of how many hardware queues are marked for restart.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|216| <<blk_mq_sched_dispatch_requests>> blk_mq_sched_mark_restart_hctx(hctx);
+ */
 void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
@@ -70,12 +81,23 @@ void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_mark_restart_hctx);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|527| <<__blk_mq_free_request>> blk_mq_sched_restart(hctx);
+ *
+ * 如果hctx->state设置了BLK_MQ_S_SCHED_RESTART, 清空BLK_MQ_S_SCHED_RESTART
+ * 并且调用blk_mq_run_hw_queue(hctx, true)
+ */
 void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
 {
 	if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 		return;
 	clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
 
+	/*
+	 * 猜测是request(tag)不够的时候才mark成restart???
+	 */
+
 	blk_mq_run_hw_queue(hctx, true);
 }
 
@@ -84,6 +106,14 @@ void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *   - block/blk-mq-sched.c|224| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *
+ * 核心思想是用e->type->ops.dispatch_request()获取调度器中下一个request, 放入一个list
+ * 然后用blk_mq_dispatch_rq_list()下发list中的request
+ */
 static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -130,6 +160,11 @@ static struct blk_mq_ctx *blk_mq_next_ctx(struct blk_mq_hw_ctx *hctx,
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|212| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ *   - block/blk-mq-sched.c|218| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ */
 static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -166,6 +201,10 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|1425| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -301,6 +340,9 @@ EXPORT_SYMBOL_GPL(blk_mq_bio_list_merge);
  * merge with. Currently includes a hand-wavy stop count of 8, to not spend
  * too much time checking for merges.
  */
+/*
+ * 尝试是否可以和ctx->rq_lists[type]的merge
+ */
 static bool blk_mq_attempt_merge(struct request_queue *q,
 				 struct blk_mq_hw_ctx *hctx,
 				 struct blk_mq_ctx *ctx, struct bio *bio)
@@ -317,6 +359,10 @@ static bool blk_mq_attempt_merge(struct request_queue *q,
 	return false;
 }
 
+/*
+ * 先尝试是否可以用e->type->ops.bio_merge进行merge
+ * 尝试是否可以和ctx->rq_lists[type]的merge
+ */
 bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 {
 	struct elevator_queue *e = q->elevator;
@@ -335,6 +381,9 @@ bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 			!list_empty_careful(&ctx->rq_lists[type])) {
 		/* default per sw-queue merge */
 		spin_lock(&ctx->lock);
+		/*
+		 * 尝试是否可以和ctx->rq_lists[type]的merge
+		 */
 		ret = blk_mq_attempt_merge(q, hctx, ctx, bio);
 		spin_unlock(&ctx->lock);
 	}
@@ -349,12 +398,22 @@ bool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_try_insert_merge);
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|4702| <<bfq_insert_request>> blk_mq_sched_request_inserted(rq);
+ *   - block/kyber-iosched.c|616| <<kyber_insert_requests>> blk_mq_sched_request_inserted(rq);
+ *   - block/mq-deadline.c|507| <<dd_insert_request>> blk_mq_sched_request_inserted(rq);
+ */
 void blk_mq_sched_request_inserted(struct request *rq)
 {
 	trace_block_rq_insert(rq->q, rq);
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|431| <<blk_mq_sched_insert_request>> if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
+ */
 static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 				       bool has_sched,
 				       struct request *rq)
@@ -373,6 +432,22 @@ static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 	return false;
 }
 
+/*
+ * 一个非常重要的函数!!!!!
+ *
+ * called by:
+ *   - block/blk-exec.c|61| <<blk_execute_rq_nowait>> blk_mq_sched_insert_request(rq, at_head, true, false);
+ *   - block/blk-mq.c|775| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, true, false, false);
+ *   - block/blk-mq.c|781| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, false, false, false);
+ *   - block/blk-mq.c|1922| <<blk_mq_try_issue_directly>> blk_mq_sched_insert_request(rq, false,
+ *   - block/blk-mq.c|1955| <<blk_mq_try_issue_list_directly>> blk_mq_sched_insert_request(rq, false, true, false);
+ *   - block/blk-mq.c|2089| <<blk_mq_make_request>> blk_mq_sched_insert_request(rq, false, true, true);
+ *
+ * 如果不能用e->type->ops.insert_requests,
+ * 就把list拼接到ctx->rq_lists[type], 其实就是把list中的request们放入ctx->rq_lists[type]
+ * 一个hctx可能对应多个ctx
+ * 把ctx在hctx中对应的hctx->ctx_map设置上
+ */
 void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 				 bool run_queue, bool async)
 {
@@ -399,6 +474,11 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		e->type->ops.insert_requests(hctx, &list, at_head);
 	} else {
 		spin_lock(&ctx->lock);
+		/*
+		 * 把list拼接到ctx->rq_lists[type], 其实就是把list中的request们放入ctx->rq_lists[type]
+		 * 一个hctx可能对应多个ctx
+		 * 把ctx在hctx中对应的hctx->ctx_map设置上
+		 */
 		__blk_mq_insert_request(hctx, rq, at_head);
 		spin_unlock(&ctx->lock);
 	}
@@ -408,6 +488,11 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		blk_mq_run_hw_queue(hctx, async);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1780| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx,
+ *   - block/blk-mq.c|1801| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx, &rq_list,
+ */
 void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 				  struct blk_mq_ctx *ctx,
 				  struct list_head *list, bool run_queue_async)
@@ -443,6 +528,10 @@ static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq-sched.c|497| <<blk_mq_init_sched>> ret = blk_mq_sched_alloc_tags(q, hctx, i);
+ */
 static int blk_mq_sched_alloc_tags(struct request_queue *q,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
@@ -450,11 +539,18 @@ static int blk_mq_sched_alloc_tags(struct request_queue *q,
 	struct blk_mq_tag_set *set = q->tag_set;
 	int ret;
 
+	/*
+	 * 为hctx_idx代表的hw queue分配并初始化blk_mq_tags
+	 */
 	hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
 					       set->reserved_tags);
 	if (!hctx->sched_tags)
 		return -ENOMEM;
 
+	/*
+	 * 核心思想是为blk_mq_tags->page_list分配页面
+	 * 然后这些页面对应的位置被blk_mq_tags->static_rqs[i]所指向
+	 */
 	ret = blk_mq_alloc_rqs(set, hctx->sched_tags, hctx_idx, q->nr_requests);
 	if (ret)
 		blk_mq_sched_free_tags(set, hctx, hctx_idx);
@@ -472,6 +568,11 @@ static void blk_mq_sched_tags_teardown(struct request_queue *q)
 		blk_mq_sched_free_tags(set, hctx, i);
 }
 
+/*
+ * called by:
+ *   - block/elevator.c|577| <<elevator_switch_mq>> ret = blk_mq_init_sched(q, new_e);
+ *   - block/elevator.c|623| <<elevator_init_mq>> err = blk_mq_init_sched(q, e);
+ */
 int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -526,6 +627,11 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|564| <<blk_mq_init_sched>> blk_mq_exit_sched(q, eq);
+ *   - block/elevator.c|184| <<elevator_exit>> blk_mq_exit_sched(q, e);
+ */
 void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq-sched.h b/block/blk-mq-sched.h
index c7bdb52..401c694 100644
--- a/block/blk-mq-sched.h
+++ b/block/blk-mq-sched.h
@@ -29,6 +29,10 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx);
 int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e);
 void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e);
 
+/*
+ * 先尝试是否可以用e->type->ops.bio_merge进行merge
+ * 尝试是否可以和ctx->rq_lists[type]的merge
+ */
 static inline bool
 blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 {
diff --git a/block/blk-mq-sysfs.c b/block/blk-mq-sysfs.c
index 3f9c3f4..c4b9dbf 100644
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@ -224,6 +224,12 @@ static struct kobj_type blk_mq_hw_ktype = {
 	.release	= blk_mq_hw_sysfs_release,
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|271| <<blk_mq_unregister_dev>> blk_mq_unregister_hctx(hctx);
+ *   - block/blk-mq-sysfs.c|339| <<__blk_mq_register_dev>> blk_mq_unregister_hctx(q->queue_hw_ctx[i]);
+ *   - block/blk-mq-sysfs.c|368| <<blk_mq_sysfs_unregister>> blk_mq_unregister_hctx(hctx);
+ */
 static void blk_mq_unregister_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_ctx *ctx;
@@ -238,6 +244,11 @@ static void blk_mq_unregister_hctx(struct blk_mq_hw_ctx *hctx)
 	kobject_del(&hctx->kobj);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|327| <<__blk_mq_register_dev>> ret = blk_mq_register_hctx(hctx);
+ *   - block/blk-mq-sysfs.c|384| <<blk_mq_sysfs_register>> ret = blk_mq_register_hctx(hctx);
+ */
 static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -247,6 +258,7 @@ static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 	if (!hctx->nr_ctx)
 		return 0;
 
+	/* 第二个参数是parent */
 	ret = kobject_add(&hctx->kobj, q->mq_kobj, "%u", hctx->queue_num);
 	if (ret)
 		return ret;
@@ -260,6 +272,10 @@ static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|1003| <<blk_unregister_queue>> blk_mq_unregister_dev(disk_to_dev(disk), q);
+ */
 void blk_mq_unregister_dev(struct device *dev, struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -277,11 +293,20 @@ void blk_mq_unregister_dev(struct device *dev, struct request_queue *q)
 	q->mq_sysfs_init_done = false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2986| <<blk_mq_alloc_and_init_hctx>> blk_mq_hctx_kobj_init(hctx);
+ */
 void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx)
 {
 	kobject_init(&hctx->kobj, &blk_mq_hw_ktype);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2878| <<blk_mq_release>> blk_mq_sysfs_deinit(q);
+ *   - block/blk-mq.c|3200| <<blk_mq_init_allocated_queue>> blk_mq_sysfs_deinit(q);
+ */
 void blk_mq_sysfs_deinit(struct request_queue *q)
 {
 	struct blk_mq_ctx *ctx;
@@ -294,6 +319,10 @@ void blk_mq_sysfs_deinit(struct request_queue *q)
 	kobject_put(q->mq_kobj);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3114| <<blk_mq_init_allocated_queue>> blk_mq_sysfs_init(q);
+ */
 void blk_mq_sysfs_init(struct request_queue *q)
 {
 	struct blk_mq_ctx *ctx;
@@ -309,6 +338,11 @@ void blk_mq_sysfs_init(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|367| <<blk_mq_register_dev>> ret = __blk_mq_register_dev(dev, q); blk_mq_register_dev()没人调用!!!
+ *   - block/blk-sysfs.c|943| <<blk_register_queue>> __blk_mq_register_dev(dev, q);
+ */
 int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -344,6 +378,9 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	return ret;
 }
 
+/*
+ * 没人调用
+ */
 int blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	int ret;
@@ -355,6 +392,10 @@ int blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3607| <<__blk_mq_update_nr_hw_queues>> blk_mq_sysfs_unregister(q);
+ */
 void blk_mq_sysfs_unregister(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -371,6 +412,10 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3627| <<__blk_mq_update_nr_hw_queues>> blk_mq_sysfs_register(q);
+ */
 int blk_mq_sysfs_register(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index 2089c6c..785bc4e 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -50,6 +50,10 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called only by:
+ *   - block/blk-mq-tag.h|83| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -96,6 +100,10 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
+	/*
+	 * BLK_MQ_REQ_INTERNAL只在一个地方设置 (存在q->elevator的情况下):
+	 *   - block/blk-mq.c|378| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 */
 	if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
 	    !hctx_may_queue(data->hctx, bt))
 		return -1;
@@ -415,6 +423,11 @@ static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 				       node);
 }
 
+/*
+ * 分配和初始化:
+ * blk_mq_tags->bitmap_tags
+ * blk_mq_tags->breserved_tags
+ */
 static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 						   int node, int alloc_policy)
 {
@@ -435,6 +448,12 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|2097| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
+ *
+ * 分配并初始化blk_mq_tags
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
@@ -453,6 +472,11 @@ struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 	tags->nr_tags = total_tags;
 	tags->nr_reserved_tags = reserved_tags;
 
+	/*
+	 * 分配和初始化:
+	 * blk_mq_tags->bitmap_tags
+	 * blk_mq_tags->breserved_tags
+	 */
 	return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
 }
 
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 61deab0..088f43b 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -7,16 +7,40 @@
 /*
  * Tag address space map.
  */
+/*
+ * 每一个hw queue一个
+ */
 struct blk_mq_tags {
 	unsigned int nr_tags;
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-debugfs.c|480| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues)); 
+	 *   - block/blk-mq-tag.c|34| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|60| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|85| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
 	struct sbitmap_queue breserved_tags;
 
+	/*
+	 * 在blk_mq_alloc_rq_map()分配set->queue_depth个一维指针
+	 *
+	 * 在以下被实际使用(分配或者临时分配):
+	 *   - block/blk-mq-tag.h|86| <<blk_mq_tag_set_rq>> hctx->tags->rqs[tag] = rq;
+	 *   - block/blk-mq.c|313| <<blk_mq_rq_ctx_init>> data->hctx->tags->rqs[rq->tag] = rq
+	 *   - block/blk-mq.c|1062| <<blk_mq_get_driver_tag>> data.hctx->tags->rqs[rq->tag] = rq;
+	 */
 	struct request **rqs;
+	/*
+	 * 在blk_mq_alloc_rq_map()分配set->queue_depth个一维指针
+	 *
+	 * 指针真正的元素在blk_mq_alloc_rqs()分配:
+	 *   - block/blk-mq.c|2307| <<blk_mq_alloc_rqs>> tags->static_rqs[i] = rq;
+	 */
 	struct request **static_rqs;
 	struct list_head page_list;
 };
@@ -61,6 +85,11 @@ static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|951| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2343| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -75,6 +104,11 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * in flight at the same time. The caller has to make sure the tag
  * can't be freed.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|220| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+ *   - block/blk-flush.c|303| <<blk_kick_flush>> blk_mq_tag_set_rq(flush_rq->mq_hctx, first_rq->tag, flush_rq);
+ */
 static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 		unsigned int tag, struct request *rq)
 {
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 3708271..0abb2ed 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -29,6 +29,11 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|694| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[0], vblk->vdev, 0);
+ *   - drivers/scsi/virtio_scsi.c|674| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 8f5b533..2c9fd96 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -38,6 +38,28 @@
 #include "blk-mq-sched.h"
 #include "blk-rq-qos.h"
 
+/*
+ * 一共两处对queue_rq()的调用:
+ *   - blk_mq_try_issue_directly()-->__blk_mq_issue_directly()
+ *   - blk_mq_dispatch_rq_list()
+ *
+ *
+ *
+ * BLK_MQ_F_xxx只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用
+ * blk_mq_init_hctx()中把blk_mq_tag_set->flags拷贝到blk_mq_hw_hctx->flags:
+ *   - block/blk-mq.c|2402| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+ *
+ * BLK_MQ_S_xxx只用在blk_mq_hw_ctx->state
+ *
+ * BLK_MQ_REQ_xxx只被blk_mq_alloc_data->flags使用
+ *
+ * RQF_xxx用在request->rq_flags
+ *
+ * MQ_RQ_xxx用在request->state
+ *
+ * QUEUE_FLAG_xxx在request_queue->queue_flags中使用
+ */
+
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
@@ -71,11 +93,34 @@ static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 /*
  * Mark this ctx as having pending work in this hardware queue
  */
+/*
+ * 一个hctx可能对应多个ctx
+ * 把ctx在hctx中对应的hctx->ctx_map设置上
+ */
 static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 				     struct blk_mq_ctx *ctx)
 {
+	/*
+	 * index_hw在以下初始化:
+	 *   - block/blk-mq.c|2627| <<blk_mq_map_swqueue>> ctx->index_hw[hctx->type] = hctx->nr_ctx;
+	 */
 	const int bit = ctx->index_hw[hctx->type];
 
+	/*
+	 * ctx_map在以下被使用:               
+	 *   - block/blk-mq-debugfs.c|470| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sched.c|177| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq.c|89| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|101| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 *   - block/blk-mq.c|102| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|110| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|1011| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1057| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off,
+	 *   - block/blk-mq.c|2395| <<blk_mq_exit_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2457| <<blk_mq_init_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2490| <<blk_mq_init_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2710| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 */
 	if (!sbitmap_test_bit(&hctx->ctx_map, bit))
 		sbitmap_set_bit(&hctx->ctx_map, bit);
 }
@@ -131,6 +176,10 @@ static bool blk_mq_check_inflight_rw(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * called only by:
+ *   - block/genhd.c|94| <<part_in_flight_rw>> blk_mq_in_flight_rw(q, part, inflight);
+ */
 void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 			 unsigned int inflight[2])
 {
@@ -262,6 +311,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * called only by:
+ *   - block/blk-core.c|286| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -287,16 +340,35 @@ static inline bool blk_mq_need_time_stamp(struct request *rq)
 	return (rq->rq_flags & RQF_IO_STAT) || rq->q->elevator;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|456| <<blk_mq_get_request>> rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags);
+ *
+ * 在根据tag获取request的时候
+ * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 使用data->hctx->sched_tags
+ * 否则使用data->hctx->tags
+ */
 static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		unsigned int tag, unsigned int op)
 {
+	/*
+	 * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 使用data->hctx->sched_tags
+	 * 否则使用data->hctx->tags
+	 */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct request *rq = tags->static_rqs[tag];
 	req_flags_t rq_flags = 0;
 
+	/*
+	 * BLK_MQ_REQ_INTERNAL只在一个地方设置 (存在q->elevator的情况下):
+	 *   - block/blk-mq.c|378| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 */
 	if (data->flags & BLK_MQ_REQ_INTERNAL) {
 		rq->tag = -1;
 		rq->internal_tag = tag;
+		/*
+		 * 到了blk_mq_get_driver_tag()再设置data->hctx->tags->rqs[rq->tag]
+		 */
 	} else {
 		if (data->hctx->flags & BLK_MQ_F_TAG_SHARED) {
 			rq_flags = RQF_MQ_INFLIGHT;
@@ -369,6 +441,10 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 		data->flags |= BLK_MQ_REQ_NOWAIT;
 
 	if (e) {
+		/*
+		 * BLK_MQ_REQ_INTERNAL只在一个地方设置 (存在q->elevator的情况下):
+		 *   - block/blk-mq.c|378| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+		 */
 		data->flags |= BLK_MQ_REQ_INTERNAL;
 
 		/*
@@ -384,6 +460,12 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 		blk_mq_tag_busy(data->hctx);
 	}
 
+	/*
+	 * 获得的tag用来索引tags->static_rqs???
+	 * 这里的tags可能是:
+	 * 1. data->hctx->sched_tags
+	 * 2. data->hctx->tags
+	 */
 	tag = blk_mq_get_tag(data);
 	if (tag == BLK_MQ_TAG_FAIL) {
 		if (put_ctx_on_error) {
@@ -394,6 +476,11 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 		return NULL;
 	}
 
+	/*
+	 * 在根据tag获取request的时候
+	 * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 使用data->hctx->sched_tags
+	 * 否则使用data->hctx->tags
+	 */
 	rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags);
 	if (!op_is_flush(data->cmd_flags)) {
 		rq->elv.icq = NULL;
@@ -409,6 +496,16 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|579| <<blk_get_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/block/mtip32xx/mtip32xx.c|994| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+ *   - drivers/block/sx8.c|511| <<carm_array_info>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/block/sx8.c|564| <<carm_send_special>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/ide/ide-atapi.c|201| <<ide_prep_sense>> sense_rq = blk_mq_alloc_request(drive->queue, REQ_OP_DRV_IN,
+ *   - drivers/nvme/host/core.c|438| <<nvme_alloc_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/scsi/fnic/fnic_scsi.c|2272| <<fnic_scsi_host_start_tag>> dummy = blk_mq_alloc_request(q, REQ_OP_WRITE, BLK_MQ_REQ_NOWAIT);
+ */
 struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 		blk_mq_req_flags_t flags)
 {
@@ -435,6 +532,10 @@ struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 }
 EXPORT_SYMBOL(blk_mq_alloc_request);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|440| <<nvme_alloc_request>> req = blk_mq_alloc_request_hctx(q, op, flags,
+ */
 struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 	unsigned int op, blk_mq_req_flags_t flags, unsigned int hctx_idx)
 {
@@ -481,6 +582,11 @@ struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|587| <<blk_mq_free_request>> __blk_mq_free_request(rq);
+ *   - block/blk-mq.c|961| <<blk_mq_check_expired>> __blk_mq_free_request(rq);
+ */
 static void __blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -498,6 +604,28 @@ static void __blk_mq_free_request(struct request *rq)
 	blk_queue_exit(q);
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|1866| <<bfq_bio_merge>> blk_mq_free_request(free);
+ *   - block/blk-core.c|589| <<blk_put_request>> blk_mq_free_request(req);
+ *   - block/blk-mq.c|613| <<__blk_mq_end_request>> blk_mq_free_request(rq->next_rq);
+ *   - block/blk-mq.c|614| <<__blk_mq_end_request>> blk_mq_free_request(rq);
+ *   - block/mq-deadline.c|483| <<dd_bio_merge>> blk_mq_free_request(free);
+ *   - drivers/block/mtip32xx/mtip32xx.c|1011| <<mtip_exec_internal_command>> blk_mq_free_request(rq);
+ *   - drivers/block/mtip32xx/mtip32xx.c|1057| <<mtip_exec_internal_command>> blk_mq_free_request(rq);
+ *   - drivers/ide/ide-atapi.c|217| <<ide_prep_sense>> blk_mq_free_request(sense_rq);
+ *   - drivers/ide/ide-cd.c|272| <<ide_cd_free_sense>> blk_mq_free_request(drive->sense_rq);
+ *   - drivers/ide/ide-probe.c|983| <<drive_release_dev>> blk_mq_free_request(drive->sense_rq);
+ *   - drivers/nvme/host/core.c|787| <<__nvme_submit_sync_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|888| <<nvme_submit_user_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|898| <<nvme_keep_alive_end_io>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/lightnvm.c|658| <<nvme_nvm_end_io>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/lightnvm.c|730| <<nvme_nvm_submit_io_sync>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/lightnvm.c|877| <<nvme_nvm_submit_user_cmd>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/pci.c|1226| <<abort_endio>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/pci.c|2247| <<nvme_del_queue_end>> blk_mq_free_request(req);
+ *   - drivers/scsi/fnic/fnic_scsi.c|2291| <<fnic_scsi_host_end_tag>> blk_mq_free_request(dummy);
+ */
 void blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -529,6 +657,9 @@ void blk_mq_free_request(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_free_request);
 
+/*
+ * 被很多的调用
+ */
 inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	u64 now = 0;
@@ -557,6 +688,9 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(__blk_mq_end_request);
 
+/*
+ * 被很多的调用
+ */
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
@@ -565,6 +699,10 @@ void blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(blk_mq_end_request);
 
+/*
+ * used by:
+ *   - block/blk-mq.c|737| <<__blk_mq_complete_request>> rq->csd.func = __blk_mq_complete_request_remote;
+ */
 static void __blk_mq_complete_request_remote(void *data)
 {
 	struct request *rq = data;
@@ -573,6 +711,10 @@ static void __blk_mq_complete_request_remote(void *data)
 	q->mq_ops->complete(rq);
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|779| <<blk_mq_complete_request>> __blk_mq_complete_request(rq);
+ */
 static void __blk_mq_complete_request(struct request *rq)
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
@@ -648,6 +790,12 @@ static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
  *	Ends all I/O on a request. It does not handle partial completions.
  *	The actual completion happens out-of-order, through a IPI handler.
  **/
+/*
+ * 被很多调用, 比如:
+ *   - drivers/block/virtio_blk.c|243| <<virtblk_done>> blk_mq_complete_request(req);
+ *   - drivers/block/xen-blkfront.c|1648| <<blkif_interrupt>> blk_mq_complete_request(req);
+ *   - drivers/scsi/scsi_lib.c|1655| <<scsi_mq_done>> if (unlikely(!blk_mq_complete_request(cmd->request)))
+ */
 bool blk_mq_complete_request(struct request *rq)
 {
 	if (unlikely(blk_should_fake_timeout(rq->q)))
@@ -696,6 +844,12 @@ void blk_mq_start_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_start_request);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|862| <<blk_mq_requeue_request>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|1429| <<blk_mq_dispatch_rq_list>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|1964| <<__blk_mq_issue_directly>> __blk_mq_requeue_request(rq);
+ */
 static void __blk_mq_requeue_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -754,6 +908,11 @@ static void blk_mq_requeue_work(struct work_struct *work)
 	blk_mq_run_hw_queues(q, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|137| <<blk_flush_queue_rq>> blk_mq_add_to_requeue_list(rq, add_front, true);
+ *   - block/blk-mq.c|868| <<blk_mq_requeue_request>> blk_mq_add_to_requeue_list(rq, true, kick_requeue_list);
+ */
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 				bool kick_requeue_list)
 {
@@ -831,6 +990,10 @@ bool blk_mq_queue_inflight(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_queue_inflight);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1044| <<blk_mq_check_expired>> blk_mq_rq_timed_out(rq, reserved);
+ */
 static void blk_mq_rq_timed_out(struct request *req, bool reserved)
 {
 	req->rq_flags |= RQF_TIMED_OUT;
@@ -846,6 +1009,11 @@ static void blk_mq_rq_timed_out(struct request *req, bool reserved)
 	blk_add_timer(req);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1022| <<blk_mq_check_expired>> if (!blk_mq_req_expired(rq, next))
+ *   - block/blk-mq.c|1043| <<blk_mq_check_expired>> if (blk_mq_req_expired(rq, next))
+ */
 static bool blk_mq_req_expired(struct request *rq, unsigned long *next)
 {
 	unsigned long deadline;
@@ -987,6 +1155,10 @@ struct dispatch_rq_data {
 	struct request *rq;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1050| <<blk_mq_dequeue_from_ctx>> dispatch_rq_from_ctx, &data);
+ */
 static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 		void *data)
 {
@@ -1007,6 +1179,10 @@ static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 	return !dispatch_data->rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|148| <<blk_mq_do_dispatch_ctx>> rq = blk_mq_dequeue_from_ctx(hctx, ctx);
+ */
 struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_ctx *start)
 {
@@ -1030,6 +1206,14 @@ static inline unsigned int queued_to_index(unsigned int queued)
 	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1109| <<blk_mq_mark_tag_wait>> return blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1134| <<blk_mq_mark_tag_wait>> ret = blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1225| <<blk_mq_dispatch_rq_list>> if (!blk_mq_get_driver_tag(rq)) {
+ *   - block/blk-mq.c|1257| <<blk_mq_dispatch_rq_list>> bd.last = !blk_mq_get_driver_tag(nxt);
+ *   - block/blk-mq.c|1851| <<blk_mq_try_issue_directly>> if (!blk_mq_get_driver_tag(rq)) {
+ */
 bool blk_mq_get_driver_tag(struct request *rq)
 {
 	struct blk_mq_alloc_data data = {
@@ -1043,6 +1227,9 @@ bool blk_mq_get_driver_tag(struct request *rq)
 	if (rq->tag != -1)
 		goto done;
 
+	/*
+	 * sched_tags是struct blk_mq_tags *sched_tags;
+	 */
 	if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
 		data.flags |= BLK_MQ_REQ_RESERVED;
 
@@ -1060,6 +1247,12 @@ bool blk_mq_get_driver_tag(struct request *rq)
 	return rq->tag != -1;
 }
 
+/*
+ * used by:
+ *   - block/blk-mq.c|2707| <<blk_mq_init_hctx>> init_waitqueue_func_entry(&hctx->dispatch_wait, blk_mq_dispatch_wake);
+ *
+ * 初始化为hctx->dispatch_wait的func
+ */
 static int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode,
 				int flags, void *key)
 {
@@ -1152,13 +1345,28 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
  * - take 4 as factor for avoiding to get too small(0) result, and this
  *   factor doesn't matter because EWMA decreases exponentially
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1518| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|1521| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|2009| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|2014| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|2018| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ */
 static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 {
 	unsigned int ewma;
 
+	/*
+	 * 如果支持elevator就永远不修改hctx->dispatch_busy
+	 */
 	if (hctx->queue->elevator)
 		return;
 
+	/*
+	 * 只有这处是修改dispatch_busy的地方:
+	 *   - block/blk-mq.c|1221| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 */
 	ewma = hctx->dispatch_busy;
 
 	if (!ewma && !busy)
@@ -1177,6 +1385,13 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 /*
  * Returns true if we did some work AND can potentially do more.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|114| <<blk_mq_do_dispatch_sched>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|164| <<blk_mq_do_dispatch_ctx>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|208| <<blk_mq_sched_dispatch_requests>> if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+ *   - block/blk-mq-sched.c|221| <<blk_mq_sched_dispatch_requests>> blk_mq_dispatch_rq_list(q, &rq_list, false);
+ */
 bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			     bool got_budget)
 {
@@ -1200,6 +1415,11 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 
 		rq = list_first_entry(list, struct request, queuelist);
 
+		/*
+		 * struct request有以下两个元素:
+		 *     struct blk_mq_ctx *mq_ctx;
+		 *     struct blk_mq_hw_ctx *mq_hctx;
+		 */
 		hctx = rq->mq_hctx;
 		if (!got_budget && !blk_mq_get_dispatch_budget(hctx))
 			break;
@@ -1331,6 +1551,11 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	return (queued + errors) != 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1647| <<__blk_mq_delay_run_hw_queue>> __blk_mq_run_hw_queue(hctx);
+ *   - block/blk-mq.c|1812| <<blk_mq_run_work_fn>> __blk_mq_run_hw_queue(hctx);
+ */
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	int srcu_idx;
@@ -1428,6 +1653,11 @@ static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 	return next_cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1661| <<blk_mq_delay_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, true, msecs);
+ *   - block/blk-mq.c|1684| <<blk_mq_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, async, 0);
+ */
 static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 					unsigned long msecs)
 {
@@ -1449,6 +1679,12 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 				    msecs_to_jiffies(msecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1524| <<blk_mq_dispatch_rq_list>> blk_mq_delay_run_hw_queue(hctx, BLK_MQ_RESOURCE_DELAY);
+ *   - drivers/ide/ide-io.c|451| <<ide_requeue_and_plug>> blk_mq_delay_run_hw_queue(q->queue_hw_ctx[0], 3);
+ *   - drivers/scsi/scsi_lib.c|1684| <<scsi_mq_get_budget>> blk_mq_delay_run_hw_queue(hctx, SCSI_QUEUE_DELAY);
+ */
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 {
 	__blk_mq_delay_run_hw_queue(hctx, true, msecs);
@@ -1622,6 +1858,14 @@ static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 		list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|462| <<blk_mq_sched_insert_request>> __blk_mq_insert_request(hctx, rq, at_head);
+ *
+ * 把list拼接到ctx->rq_lists[type], 其实就是把list中的request们放入ctx->rq_lists[type]
+ * 一个hctx可能对应多个ctx
+ * 把ctx在hctx中对应的hctx->ctx_map设置上
+ */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			     bool at_head)
 {
@@ -1629,7 +1873,14 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 
 	lockdep_assert_held(&ctx->lock);
 
+	/*
+	 * 把list拼接到ctx->rq_lists[type], 其实就是把list中的request们放入ctx->rq_lists[type]
+	 */
 	__blk_mq_insert_req_list(hctx, rq, at_head);
+	/*
+	 * 一个hctx可能对应多个ctx
+	 * 把ctx在hctx中对应的hctx->ctx_map设置上
+	 */
 	blk_mq_hctx_mark_pending(hctx, ctx);
 }
 
@@ -1637,6 +1888,11 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|392| <<blk_insert_flush>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|1913| <<blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, run_queue);
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
@@ -1649,6 +1905,14 @@ void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 		blk_mq_run_hw_queue(hctx, false);
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq-sched.c|494| <<blk_mq_sched_insert_requests>> blk_mq_insert_requests(hctx, ctx, list);
+ *
+ * 把list拼接到ctx->rq_lists[type], 其实就是把list中的request们放入ctx->rq_lists[type]
+ * 一个hctx可能对应多个ctx
+ * 把ctx在hctx中对应的hctx->ctx_map设置上
+ */
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 			    struct list_head *list)
 
@@ -1667,6 +1931,10 @@ void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 
 	spin_lock(&ctx->lock);
 	list_splice_tail_init(list, &ctx->rq_lists[type]);
+	/*
+	 * 一个hctx可能对应多个ctx
+	 * 把ctx在hctx中对应的hctx->ctx_map设置上
+	 */
 	blk_mq_hctx_mark_pending(hctx, ctx);
 	spin_unlock(&ctx->lock);
 }
@@ -1749,6 +2017,10 @@ static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
 	blk_account_io_start(rq, true);
 }
 
+/*
+ * 只被blk_mq_try_issue_directly()调用
+ * 这个函数是两个直接调用queue_rq()的函数之一
+ */
 static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    blk_qc_t *cookie, bool last)
@@ -1788,6 +2060,13 @@ static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1875| <<blk_mq_try_issue_list_directly>> ret = blk_mq_try_issue_directly(hctx, rq, &unused,
+ *   - block/blk-mq.c|1999| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, same_queue_rq,
+ *   - block/blk-mq.c|2006| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, rq, &cookie, false, true);
+ *   - block/blk-core.c|1262| <<blk_insert_cloned_request>> return blk_mq_try_issue_directly(rq->mq_hctx, rq, &unused, true, true);
+ */
 blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 						struct request *rq,
 						blk_qc_t *cookie,
@@ -1860,6 +2139,10 @@ blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq-sched.c|427| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -1905,6 +2188,7 @@ static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
 static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 {
 	const int is_sync = op_is_sync(bio->bi_opf);
+	/* 如果有REQ_FUA或者REQ_PREFLUSH */
 	const int is_flush_fua = op_is_flush(bio->bi_opf);
 	struct blk_mq_alloc_data data = { .flags = 0};
 	struct request *rq;
@@ -1919,10 +2203,26 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 	if (!bio_integrity_prep(bio))
 		return BLK_QC_T_NONE;
 
+	/*
+	 * blk_queue_nomerges(): request_queue->queue_flags是否设置了QUEUE_FLAG_NOMERGES
+	 * 在以下设置QUEUE_FLAG_NOMERGES:
+	 *   - block/blk-core.c|335| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+	 *   - block/blk-sysfs.c|316| <<queue_nomerges_store>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, q);
+	 *   - block/blk-sysfs.c|319| <<queue_nomerges_store>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+	 *   - drivers/block/loop.c|220| <<__loop_update_dio>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+	 *   - drivers/block/loop.c|223| <<__loop_update_dio>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+	 *   - drivers/block/loop.c|1962| <<loop_add>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+	 *   - drivers/scsi/megaraid/megaraid_sas_base.c|1914| <<megasas_set_nvme_device_properties>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, sdev->request_queue);
+	 *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|2356| <<scsih_slave_configure>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES,
+	 */
 	if (!is_flush_fua && !blk_queue_nomerges(q) &&
 	    blk_attempt_plug_merge(q, bio, &same_queue_rq))
 		return BLK_QC_T_NONE;
 
+	/*
+	 * 先尝试是否可以用e->type->ops.bio_merge进行merge
+	 * 尝试是否可以和ctx->rq_lists[type]的merge
+	 */
 	if (blk_mq_sched_bio_merge(q, bio))
 		return BLK_QC_T_NONE;
 
@@ -1943,6 +2243,14 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 
 	cookie = request_to_qc_t(data.hctx, rq);
 
+	/*
+	 * 这里插播一个使用plug的例子
+	 *   blk_start_plug(&plug);
+	 *   for (i = 0; i < *batch_count; i++)
+	 *            write_dirty_buffer(journal->j_chkpt_bhs[i], REQ_SYNC);
+	 *   blk_finish_plug(&plug);
+	 */
+
 	plug = current->plug;
 	if (unlikely(is_flush_fua)) {
 		blk_mq_put_ctx(data.ctx);
@@ -2007,12 +2315,25 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 	} else {
 		blk_mq_put_ctx(data.ctx);
 		blk_mq_bio_to_request(rq, bio);
+		/*
+		 * 如果不能用e->type->ops.insert_requests,
+		 * 就把list拼接到ctx->rq_lists[type], 其实就是把list中的request们放入ctx->rq_lists[type]
+		 * 一个hctx可能对应多个ctx
+		 * 把ctx在hctx中对应的hctx->ctx_map设置上
+		 */
 		blk_mq_sched_insert_request(rq, false, true, true);
 	}
 
 	return cookie;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|505| <<blk_mq_sched_free_tags>> blk_mq_free_rqs(set, hctx->sched_tags, hctx_idx);
+ *   - block/blk-mq-tag.c|528| <<blk_mq_tag_update_depth>> blk_mq_free_rqs(set, *tagsptr, hctx->queue_num);
+ *   - block/blk-mq.c|2512| <<blk_mq_alloc_rqs>> blk_mq_free_rqs(set, tags, hctx_idx);
+ *   - block/blk-mq.c|2758| <<blk_mq_free_map_and_requests>> blk_mq_free_rqs(set, set->tags[hctx_idx], hctx_idx);
+ */
 void blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 		     unsigned int hctx_idx)
 {
@@ -2043,6 +2364,15 @@ void blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|506| <<blk_mq_sched_free_tags>> blk_mq_free_rq_map(hctx->sched_tags);
+ *   - block/blk-mq-tag.c|524| <<blk_mq_tag_update_depth>> blk_mq_free_rq_map(new);
+ *   - block/blk-mq-tag.c|529| <<blk_mq_tag_update_depth>> blk_mq_free_rq_map(*tagsptr);
+ *   - block/blk-mq.c|2749| <<__blk_mq_alloc_rq_map>> blk_mq_free_rq_map(set->tags[hctx_idx]);
+ *   - block/blk-mq.c|2759| <<blk_mq_free_map_and_requests>> blk_mq_free_rq_map(set->tags[hctx_idx]);
+ *   - block/blk-mq.c|3407| <<__blk_mq_alloc_rq_maps>> blk_mq_free_rq_map(set->tags[i]);
+ */
 void blk_mq_free_rq_map(struct blk_mq_tags *tags)
 {
 	kfree(tags->rqs);
@@ -2053,23 +2383,74 @@ void blk_mq_free_rq_map(struct blk_mq_tags *tags)
 	blk_mq_free_tags(tags);
 }
 
+/*
+ * 每个hctx_idx调用一次
+ * [0] blk_mq_alloc_rq_map
+ * [0] __blk_mq_alloc_rq_map
+ * [0] blk_mq_alloc_tag_set
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * 每个hctx_idx调用一次
+ * [0] blk_mq_alloc_rq_map
+ * [0] blk_mq_init_sched
+ * [0] elevator_switch_mq
+ * [0] elevator_switch
+ * [0] elv_iosched_store
+ * [0] queue_attr_store
+ * [0] kernfs_fop_write
+ * [0] __vfs_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - block/blk-mq-sched.c|453| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+ *   - block/blk-mq-tag.c|494| <<blk_mq_tag_update_depth>> new = blk_mq_alloc_rq_map(set, hctx->queue_num, tdepth,
+ *   - block/blk-mq.c|2403| <<__blk_mq_alloc_rq_map>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+ *
+ * 为hctx_idx代表的hw queue分配并初始化blk_mq_tags
+ */
 struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 					unsigned int hctx_idx,
 					unsigned int nr_tags,
 					unsigned int reserved_tags)
 {
+	/* 每个hw queue一个 */
 	struct blk_mq_tags *tags;
 	int node;
 
+	/*
+	 * 找到index代表的hw queue对应的cpu (sw queue)
+	 * 返回这个cpu的node
+	 */
 	node = blk_mq_hw_queue_to_node(&set->map[0], hctx_idx);
 	if (node == NUMA_NO_NODE)
 		node = set->numa_node;
 
+	/*
+	 * 分配并初始化blk_mq_tags
+	 */
 	tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
 				BLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));
 	if (!tags)
 		return NULL;
 
+	/*
+	 * struct request **rqs;
+	 */
 	tags->rqs = kcalloc_node(nr_tags, sizeof(struct request *),
 				 GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY,
 				 node);
@@ -2078,6 +2459,9 @@ struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 		return NULL;
 	}
 
+	/*
+	 * struct request **static_rqs;
+	 */
 	tags->static_rqs = kcalloc_node(nr_tags, sizeof(struct request *),
 					GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY,
 					node);
@@ -2095,6 +2479,11 @@ static size_t order_to_size(unsigned int order)
 	return (size_t)PAGE_SIZE << order;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2500| <<blk_mq_alloc_rqs>> if (blk_mq_init_request(set, rq, hctx_idx, node)) {
+ *   - block/blk-mq.c|2656| <<blk_mq_init_hctx>> if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx, node))
+ */
 static int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,
 			       unsigned int hctx_idx, int node)
 {
@@ -2110,6 +2499,15 @@ static int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|462| <<blk_mq_sched_alloc_tags>> ret = blk_mq_alloc_rqs(set, hctx->sched_tags, hctx_idx, q->nr_requests);
+ *   - block/blk-mq-tag.c|502| <<blk_mq_tag_update_depth>> ret = blk_mq_alloc_rqs(set, new, hctx->queue_num, tdepth);
+ *   - block/blk-mq.c|2446| <<__blk_mq_alloc_rq_map>> ret = blk_mq_alloc_rqs(set, set->tags[hctx_idx], hctx_idx,
+ *
+ * 核心思想是为blk_mq_tags->page_list分配页面
+ * 然后这些页面对应的位置被blk_mq_tags->static_rqs[i]所指向
+ */
 int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 		     unsigned int hctx_idx, unsigned int depth)
 {
@@ -2117,6 +2515,10 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 	size_t rq_size, left;
 	int node;
 
+	/*
+	 * 找到index代表的hw queue对应的cpu (sw queue)
+	 * 返回这个cpu的node
+	 */
 	node = blk_mq_hw_queue_to_node(&set->map[0], hctx_idx);
 	if (node == NUMA_NO_NODE)
 		node = set->numa_node;
@@ -2263,6 +2665,10 @@ static void blk_mq_exit_hw_queues(struct request_queue *q,
 	}
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|2864| <<blk_mq_alloc_and_init_hctx>> if (blk_mq_init_hctx(q, set, hctx, hctx_idx)) {
+ */
 static int blk_mq_init_hctx(struct request_queue *q,
 		struct blk_mq_tag_set *set,
 		struct blk_mq_hw_ctx *hctx, unsigned hctx_idx)
@@ -2281,12 +2687,24 @@ static int blk_mq_init_hctx(struct request_queue *q,
 
 	cpuhp_state_add_instance_nocalls(CPUHP_BLK_MQ_DEAD, &hctx->cpuhp_dead);
 
+	/*
+	 * set->tags在以下分配第二维
+	 *   - block/blk-mq.c|2501| <<__blk_mq_alloc_rq_map>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+	 */
 	hctx->tags = set->tags[hctx_idx];
 
 	/*
 	 * Allocate space for all possible cpus to avoid allocation at
 	 * runtime
 	 */
+	/*
+	 * blk_mq_hw_ctx:
+	 *     unsigned short          nr_ctx;
+	 *     struct blk_mq_ctx       **ctxs;
+	 *
+	 * 在以下初始化每一个元素
+	 *   - block/blk-mq.c|2594| <<blk_mq_map_swqueue>> hctx->ctxs[hctx->nr_ctx++] = ctx;
+	 */
 	hctx->ctxs = kmalloc_array_node(nr_cpu_ids, sizeof(void *),
 			GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY, node);
 	if (!hctx->ctxs)
@@ -2333,6 +2751,10 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	return -1;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|3134| <<blk_mq_init_allocated_queue>> blk_mq_init_cpu_queues(q, set->nr_hw_queues);
+ */
 static void blk_mq_init_cpu_queues(struct request_queue *q,
 				   unsigned int nr_hw_queues)
 {
@@ -2340,10 +2762,14 @@ static void blk_mq_init_cpu_queues(struct request_queue *q,
 	unsigned int i, j;
 
 	for_each_possible_cpu(i) {
+		/* 这里的i是index */
 		struct blk_mq_ctx *__ctx = per_cpu_ptr(q->queue_ctx, i);
 		struct blk_mq_hw_ctx *hctx;
 		int k;
 
+		/*
+		 * 这里的i是index, __ctx->cpu和__ctx的index相等
+		 */
 		__ctx->cpu = i;
 		spin_lock_init(&__ctx->lock);
 		for (k = HCTX_TYPE_DEFAULT; k < HCTX_MAX_TYPES; k++)
@@ -2363,15 +2789,34 @@ static void blk_mq_init_cpu_queues(struct request_queue *q,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2528| <<blk_mq_map_swqueue>> !__blk_mq_alloc_rq_map(set, hctx_idx)) {
+ *   - block/blk-mq.c|2998| <<__blk_mq_alloc_rq_maps>> if (!__blk_mq_alloc_rq_map(set, i))
+ *
+ * 为hctx_idx代表的hw queue分配并初始化blk_mq_tags为blk_mq_tag_set->tags[hctx_idx]
+ * 然后为blk_mq_tags->page_list分配页面
+ * 然后这些页面对应的位置被blk_mq_tags->static_rqs[i]所指
+ */
 static bool __blk_mq_alloc_rq_map(struct blk_mq_tag_set *set, int hctx_idx)
 {
 	int ret = 0;
 
+	/*
+	 * struct blk_mq_tags      **tags;
+	 * tags是一个数组指针, 这里分配初始化每一个指针
+	 *
+	 * 为hctx_idx代表的hw queue分配并初始化blk_mq_tags
+	 */
 	set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
 					set->queue_depth, set->reserved_tags);
 	if (!set->tags[hctx_idx])
 		return false;
 
+	/*
+	 * 核心思想是为blk_mq_tags->page_list分配页面
+	 * 然后这些页面对应的位置被blk_mq_tags->static_rqs[i]所指
+	 */
 	ret = blk_mq_alloc_rqs(set, set->tags[hctx_idx], hctx_idx,
 				set->queue_depth);
 	if (!ret)
@@ -2392,6 +2837,11 @@ static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3047| <<blk_mq_init_allocated_queue>> blk_mq_map_swqueue(q);
+ *   - block/blk-mq.c|3484| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_swqueue(q);
+ */
 static void blk_mq_map_swqueue(struct request_queue *q)
 {
 	unsigned int i, j, hctx_idx;
@@ -2404,6 +2854,7 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 */
 	mutex_lock(&q->sysfs_lock);
 
+	/* 对于每一个在用的(不是set中指定的)request_queue->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		cpumask_clear(hctx->cpumask);
 		hctx->nr_ctx = 0;
@@ -2415,9 +2866,21 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 *
 	 * If the cpu isn't present, the cpu is mapped to first hctx.
 	 */
+	/*
+	 * 这个i也是request_queue->queue_ctx的索引
+	 */
 	for_each_possible_cpu(i) {
+		/*
+		 * 猜测是每一个sw queue对应的hw queue???
+		 */
 		hctx_idx = set->map[0].mq_map[i];
 		/* unmapped hw queue can be remapped after CPU topo changed */
+		/*
+		 * set->tags可能之前用blk_mq_alloc_rq_map()分配第二维
+		 *   - block/blk-mq.c|2501| <<__blk_mq_alloc_rq_map>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+		 *
+		 * 如果之前没有分配过(比如增加了cpu), 这里用__blk_mq_alloc_rq_map()分配
+		 */
 		if (!set->tags[hctx_idx] &&
 		    !__blk_mq_alloc_rq_map(set, hctx_idx)) {
 			/*
@@ -2429,6 +2892,12 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			set->map[0].mq_map[i] = 0;
 		}
 
+		/*
+		 * 在blk_mq_init_cpu_queues()中初始化ctx->cpu:
+		 * 这里的i是index, __ctx->cpu和__ctx的index相等
+		 * __ctx->cpu = i;
+		 */
+
 		ctx = per_cpu_ptr(q->queue_ctx, i);
 		for (j = 0; j < set->nr_maps; j++) {
 			if (!set->map[j].nr_queues)
@@ -2447,6 +2916,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			cpumask_set_cpu(i, hctx->cpumask);
 			hctx->type = j;
 			ctx->index_hw[hctx->type] = hctx->nr_ctx;
+			/*
+			 * 这一行最重要!!!
+			 */
 			hctx->ctxs[hctx->nr_ctx++] = ctx;
 
 			/*
@@ -2459,6 +2931,7 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 
 	mutex_unlock(&q->sysfs_lock);
 
+	/* 对于每一个在用的(不是set中指定的)request_queue->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		/*
 		 * If no software queues are mapped to this hardware queue,
@@ -2476,6 +2949,12 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			continue;
 		}
 
+		/*
+		 * i是索引的(q)->nr_hw_queues
+		 *
+		 * 在以下分配第二维
+		 *   - block/blk-mq.c|2501| <<__blk_mq_alloc_rq_map>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+		 */
 		hctx->tags = set->tags[i];
 		WARN_ON(!hctx->tags);
 
@@ -2498,11 +2977,17 @@ static void blk_mq_map_swqueue(struct request_queue *q)
  * Caller needs to ensure that we're either frozen/quiesced, or that
  * the queue isn't live yet.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2927| <<blk_mq_update_tag_set_depth>> queue_set_hctx_shared(q, shared);
+ *   - block/blk-mq.c|2970| <<blk_mq_add_queue_tag_set>> queue_set_hctx_shared(q, true);
+ */
 static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 {
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+	/* 对于每一个在用的(不是set中指定的)request_queue->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (shared)
 			hctx->flags |= BLK_MQ_F_TAG_SHARED;
@@ -2541,6 +3026,13 @@ static void blk_mq_del_queue_tag_set(struct request_queue *q)
 	INIT_LIST_HEAD(&q->tag_set_list);
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|3062| <<blk_mq_init_allocated_queue>> blk_mq_add_queue_tag_set(set, q);
+ *
+ * 核心思想是把request_queue->tag_set_list链接到blk_mq_tag_set->tag_list
+ * 多个request_queue可能共享1个blk_mq_tag_set
+ */
 static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 				     struct request_queue *q)
 {
@@ -2563,11 +3055,31 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 }
 
 /* All allocations will be freed in release handler of q->mq_kobj */
+/*
+ * called by only:
+ *   - block/blk-mq.c|3022| <<blk_mq_init_allocated_queue>> if (blk_mq_alloc_ctxs(q))
+ *
+ * 先分配一个游荡的blk_mq_ctxs, 然后为其blk_mq_ctxs->queue_ctx分配percpu的blk_mq_ctx
+ * percpu的每一个per_cpu_ptr(ctxs->queue_ctx, cpu)->ctxs指向游荡的blk_mq_ctxs
+ * request_queue->queue_ctx指向percpu的blk_mq_ctxs->queue_ctx
+ *
+ * 说白了就是分配request_queue->queue_ctx
+ */
 static int blk_mq_alloc_ctxs(struct request_queue *q)
 {
 	struct blk_mq_ctxs *ctxs;
 	int cpu;
 
+	/*
+	 * sw queues
+	 *     struct blk_mq_ctx __percpu      *queue_ctx;
+	 *     unsigned int            nr_queues;
+	 *
+	 * hw dispatch queues
+	 *     struct blk_mq_hw_ctx    **queue_hw_ctx; ---> 二维指针!!!!
+	 *     unsigned int            nr_hw_queues;
+	 */
+
 	ctxs = kzalloc(sizeof(*ctxs), GFP_KERNEL);
 	if (!ctxs)
 		return -ENOMEM;
@@ -2687,6 +3199,19 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 {
 	struct blk_mq_hw_ctx *hctx;
 
+	/*
+	 * request_quue的sw queues
+	 *     struct blk_mq_ctx __percpu      *queue_ctx;
+	 *     unsigned int            nr_queues;
+	 *
+	 * request_queue的hw dispatch queues
+	 *     struct blk_mq_hw_ctx    **queue_hw_ctx; ---> 二维指针!!!!
+	 *     unsigned int            nr_hw_queues;
+	 */
+
+	/*
+	 * 分配struct blk_mq_hw_ctx
+	 */
 	hctx = kzalloc_node(blk_mq_hw_ctx_size(set),
 			GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY,
 			node);
@@ -2714,10 +3239,23 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 	return hctx;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2979| <<blk_mq_init_allocated_queue>> blk_mq_realloc_hw_ctxs(set, q);
+ *   - block/blk-mq.c|3450| <<__blk_mq_update_nr_hw_queues>> blk_mq_realloc_hw_ctxs(set, q);
+ *
+ * 核心思想是为每一个hw queue调用blk_mq_alloc_and_init_hctx()分配和初始化第二维的q->queue_hw_ctx
+ * 会让每一个hw queue: hctx->tags = set->tags[hctx_idx];
+ */
 static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 						struct request_queue *q)
 {
 	int i, j, end;
+	/*
+	 * 在blk_mq_init_allocated_queue()分配了第一维的数组
+	 *
+	 * 这里的类型是struct blk_mq_hw_ctx    **queue_hw_ctx;
+	 */
 	struct blk_mq_hw_ctx **hctxs = q->queue_hw_ctx;
 
 	/* protect against switching io scheduler  */
@@ -2726,6 +3264,10 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 		int node;
 		struct blk_mq_hw_ctx *hctx;
 
+		/*
+		 * 找到index代表的hw queue对应的cpu (sw queue)
+		 * 返回这个cpu的node
+		 */
 		node = blk_mq_hw_queue_to_node(&set->map[0], i);
 		/*
 		 * If the hw queue has been mapped to another numa node,
@@ -2741,6 +3283,10 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 				blk_mq_exit_hctx(q, set, hctxs[i], i);
 				kobject_put(&hctxs[i]->kobj);
 			}
+			/*
+			 * 因为上面有struct blk_mq_hw_ctx **hctxs = q->queue_hw_ctx;
+			 * 所以这里等价于q->queue_hw_ctx[i] = hctx
+			 */
 			hctxs[i] = hctx;
 		} else {
 			if (hctxs[i])
@@ -2786,6 +3332,7 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
  */
 static unsigned int nr_hw_queues(struct blk_mq_tag_set *set)
 {
+	/* 只有nvme会用到不只是1 */
 	if (set->nr_maps == 1)
 		return nr_cpu_ids;
 
@@ -2804,22 +3351,51 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	if (!q->poll_cb)
 		goto err_exit;
 
+	/*
+	 * 先分配一个游荡的blk_mq_ctxs, 然后为其blk_mq_ctxs->queue_ctx分配percpu的blk_mq_ctx
+	 * percpu的每一个per_cpu_ptr(ctxs->queue_ctx, cpu)->ctxs指向游荡的blk_mq_ctxs
+	 * request_queue->queue_ctx指向percpu的blk_mq_ctxs->queue_ctx
+	 *
+	 * 说白了就是分配request_queue->queue_ctx
+	 */
 	if (blk_mq_alloc_ctxs(q))
 		goto err_exit;
 
 	/* init q->mq_kobj and sw queues' kobjects */
 	blk_mq_sysfs_init(q);
 
+	/*
+	 * sw queues
+	 *     struct blk_mq_ctx __percpu      *queue_ctx;
+	 *     unsigned int            nr_queues;
+	 *
+	 * hw dispatch queues
+	 *     struct blk_mq_hw_ctx    **queue_hw_ctx; ---> 二维指针!!!!
+	 *     unsigned int            nr_hw_queues;
+	 */
+
+	/*
+	 * 上面说的queue_hw_ctx是二维指针, 这里只是分配了指针数组
+	 */
 	q->nr_queues = nr_hw_queues(set);
 	q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
 						GFP_KERNEL, set->numa_node);
 	if (!q->queue_hw_ctx)
 		goto err_sys_init;
 
+	/*
+	 * 核心思想是为每一个hw queue调用blk_mq_alloc_and_init_hctx()分配和初始化第二维的q->queue_hw_ctx
+	 * 会让每一个hw queue: hctx->tags = set->tags[hctx_idx];
+	 */
 	blk_mq_realloc_hw_ctxs(set, q);
 	if (!q->nr_hw_queues)
 		goto err_hctxs;
 
+	/*
+	 * set->timeout:
+	 *   nvme是nvme_timeout
+	 *   scsi是scsi_timeout
+	 */
 	INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
 	blk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ);
 
@@ -2852,6 +3428,10 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	q->poll_nsec = -1;
 
 	blk_mq_init_cpu_queues(q, set->nr_hw_queues);
+	/*
+	 * 核心思想是把request_queue->tag_set_list链接到blk_mq_tag_set->tag_list
+	 * 多个request_queue可能共享1个blk_mq_tag_set
+	 */
 	blk_mq_add_queue_tag_set(set, q);
 	blk_mq_map_swqueue(q);
 
@@ -2883,11 +3463,21 @@ void blk_mq_free_queue(struct request_queue *q)
 	blk_mq_exit_hw_queues(q, set, set->nr_hw_queues);
 }
 
+/*
+ * 对于每一个hw quueue(hctx_idx)分配并初始化blk_mq_tags为blk_mq_tag_set->tags[hctx_idx]
+ * 然后为blk_mq_tags->page_list分配页面
+ * 然后这些页面对应的位置被blk_mq_tags->static_rqs[i]所指
+ */
 static int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 {
 	int i;
 
 	for (i = 0; i < set->nr_hw_queues; i++)
+		/*
+		 * 为hctx_idx代表的hw queue分配并初始化blk_mq_tags为blk_mq_tag_set->tags[hctx_idx]
+		 * 然后为blk_mq_tags->page_list分配页面
+		 * 然后这些页面对应的位置被blk_mq_tags->static_rqs[i]所指
+		 */
 		if (!__blk_mq_alloc_rq_map(set, i))
 			goto out_unwind;
 
@@ -2905,6 +3495,11 @@ static int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
  * may reduce the depth asked for, if memory is tight. set->queue_depth
  * will be updated to reflect the allocated depth.
  */
+/*
+ * 对于每一个hw quueue(hctx_idx)分配并初始化blk_mq_tags为blk_mq_tag_set->tags[hctx_idx]
+ * 然后为blk_mq_tags->page_list分配页面
+ * 然后这些页面对应的位置被blk_mq_tags->static_rqs[i]所指
+ */
 static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 {
 	unsigned int depth;
@@ -2912,10 +3507,16 @@ static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 
 	depth = set->queue_depth;
 	do {
+		/*
+		 * 对于每一个hw quueue(hctx_idx)分配并初始化blk_mq_tags为blk_mq_tag_set->tags[hctx_idx]
+		 * 然后为blk_mq_tags->page_list分配页面
+		 * 然后这些页面对应的位置被blk_mq_tags->static_rqs[i]所指
+		 */
 		err = __blk_mq_alloc_rq_maps(set);
 		if (!err)
 			break;
 
+		/* 如果上面分配失败, queue_depth减半, 继续重试 */
 		set->queue_depth >>= 1;
 		if (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN) {
 			err = -ENOMEM;
@@ -2935,6 +3536,9 @@ static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * 这个函数主要更新blk_mq_queue_map[0]->mq_map
+ */
 static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 {
 	if (set->ops->map_queues && !is_kdump_kernel()) {
@@ -2954,6 +3558,7 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 		 * killing stale mapping since one CPU may not be mapped
 		 * to any hw queue.
 		 */
+		/* 清空每一个blk_mq_queue_map->mq_map[cpu] */
 		for (i = 0; i < set->nr_maps; i++)
 			blk_mq_clear_mq_map(&set->map[i]);
 
@@ -2970,6 +3575,18 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
  * requested depth down, if it's too large. In that case, the set
  * value will be stored in set->queue_depth.
  */
+/*
+ * virtio的例子:
+ * memset(&vblk->tag_set, 0, sizeof(vblk->tag_set));
+ * vblk->tag_set.ops = &virtio_mq_ops;
+ * vblk->tag_set.queue_depth = virtblk_queue_depth;
+ * vblk->tag_set.numa_node = NUMA_NO_NODE;
+ * vblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+ * vblk->tag_set.cmd_size = sizeof(struct virtblk_req) +
+ *                          sizeof(struct scatterlist) * sg_elems;
+ * vblk->tag_set.driver_data = vblk;
+ * vblk->tag_set.nr_hw_queues = vblk->num_vqs;
+ */
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
 	int i, ret;
@@ -2995,6 +3612,7 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 		set->queue_depth = BLK_MQ_MAX_DEPTH;
 	}
 
+	/* 如果没有设置nr_maps就设置成1 */
 	if (!set->nr_maps)
 		set->nr_maps = 1;
 	else if (set->nr_maps > HCTX_MAX_TYPES)
@@ -3014,9 +3632,17 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	 * There is no use for more h/w queues than cpus if we just have
 	 * a single map
 	 */
+	/*
+	 * 只有nvme会用到不只是1
+	 */
 	if (set->nr_maps == 1 && set->nr_hw_queues > nr_cpu_ids)
 		set->nr_hw_queues = nr_cpu_ids;
 
+	/*
+	 * tags的类型:
+	 * struct blk_mq_tags      **tags;
+	 * 这里分配的一维指针数组
+	 */
 	set->tags = kcalloc_node(nr_hw_queues(set), sizeof(struct blk_mq_tags *),
 				 GFP_KERNEL, set->numa_node);
 	if (!set->tags)
@@ -3024,18 +3650,33 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 
 	ret = -ENOMEM;
 	for (i = 0; i < set->nr_maps; i++) {
+		/*
+		 * struct blk_mq_queue_map的:
+		 *     unsigned int *mq_map;
+		 */
 		set->map[i].mq_map = kcalloc_node(nr_cpu_ids,
 						  sizeof(set->map[i].mq_map[0]),
 						  GFP_KERNEL, set->numa_node);
+		/* 猜测是每一个sw queue对应的hw queue??? */
 		if (!set->map[i].mq_map)
 			goto out_free_mq_map;
 		set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
 	}
 
+	/*
+	 * mq_map在上面的blk_mq_alloc_tag_set()分配
+	 *
+	 * 这个函数主要更新blk_mq_queue_map[0]->mq_map
+	 */
 	ret = blk_mq_update_queue_map(set);
 	if (ret)
 		goto out_free_mq_map;
 
+	/*
+	 * 对于每一个hw queue(hctx_idx)分配并初始化blk_mq_tags为blk_mq_tag_set->tags[hctx_idx]
+	 * 然后为blk_mq_tags->page_list分配页面
+	 * 然后这些页面对应的位置被blk_mq_tags->static_rqs[i]所指
+	 */
 	ret = blk_mq_alloc_rq_maps(set);
 	if (ret)
 		goto out_free_mq_map;
@@ -3073,6 +3714,10 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_free_tag_set);
 
+/*
+ * called only by:
+ *   - block/blk-sysfs.c|81| <<queue_requests_store>> err = blk_mq_update_nr_requests(q, nr);
+ */
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -3160,6 +3805,10 @@ static bool blk_mq_elv_switch_none(struct list_head *head,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3839| <<__blk_mq_update_nr_hw_queues>> blk_mq_elv_switch_back(&head, q);
+ */
 static void blk_mq_elv_switch_back(struct list_head *head,
 		struct request_queue *q)
 {
@@ -3183,6 +3832,10 @@ static void blk_mq_elv_switch_back(struct list_head *head,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3848| <<blk_mq_update_nr_hw_queues>> __blk_mq_update_nr_hw_queues(set, nr_hw_queues);
+ */
 static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 							int nr_hw_queues)
 {
@@ -3246,6 +3899,16 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1154| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2504| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2353| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|899| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1595| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|492| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
@@ -3255,6 +3918,10 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
 
 /* Enable polling stats and return whether they were already enabled. */
+/*
+ * called by:
+ *   - block/blk-mq.c|3898| <<blk_mq_poll_nsecs>> if (!blk_poll_stats_enable(q))
+ */
 static bool blk_poll_stats_enable(struct request_queue *q)
 {
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
@@ -3264,6 +3931,10 @@ static bool blk_poll_stats_enable(struct request_queue *q)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|671| <<__blk_mq_end_request>> blk_mq_poll_stats_start(rq->q);
+ */
 static void blk_mq_poll_stats_start(struct request_queue *q)
 {
 	/*
@@ -3277,6 +3948,10 @@ static void blk_mq_poll_stats_start(struct request_queue *q)
 	blk_stat_activate_msecs(q->poll_cb, 100);
 }
 
+/*
+ * used by:
+ *   - block/blk-mq.c|3303| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ */
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 {
 	struct request_queue *q = cb->data;
@@ -3288,6 +3963,10 @@ static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3941| <<blk_mq_poll_hybrid_sleep>> nsecs = blk_mq_poll_nsecs(q, hctx, rq);
+ */
 static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 				       struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
@@ -3321,6 +4000,10 @@ static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3997| <<blk_mq_poll_hybrid>> return blk_mq_poll_hybrid_sleep(q, hctx, rq);
+ */
 static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 				     struct blk_mq_hw_ctx *hctx,
 				     struct request *rq)
@@ -3376,6 +4059,10 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|4033| <<blk_poll>> if (blk_mq_poll_hybrid(q, hctx, cookie))
+ */
 static bool blk_mq_poll_hybrid(struct request_queue *q,
 			       struct blk_mq_hw_ctx *hctx, blk_qc_t cookie)
 {
@@ -3413,6 +4100,15 @@ static bool blk_mq_poll_hybrid(struct request_queue *q,
  *    looping until at least one completion is found, unless the task is
  *    otherwise marked running (or we need to reschedule).
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|747| <<nvme_execute_rq_polled>> blk_poll(q, request_to_qc_t(rq->mq_hctx, rq), true);
+ *   - fs/block_dev.c|258| <<__blkdev_direct_IO_simple>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/block_dev.c|446| <<__blkdev_direct_IO>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/direct-io.c|521| <<dio_await_one>> !blk_poll(dio->bio_disk->queue, dio->bio_cookie, true))
+ *   - fs/iomap.c|1930| <<iomap_dio_rw>> !blk_poll(dio->submit.last_queue,
+ *   - mm/page_io.c|414| <<swap_readpage>> if (!blk_poll(disk->queue, qc, true))
+ */
 int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq.h b/block/blk-mq.h
index d943d46..3ea8174 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -7,8 +7,18 @@
 
 struct blk_mq_tag_set;
 
+/*
+ * 在blk_mq_alloc_ctxs()为request_queue分配过blk_mq_ctxs
+ */
 struct blk_mq_ctxs {
 	struct kobject kobj;
+	/*
+	 * 先为struct blk_mq_ctxs分配:
+	 *   - block/blk-mq.c|2743| <<blk_mq_alloc_ctxs>> ctxs->queue_ctx = alloc_percpu(struct blk_mq_ctx);
+	 *
+	 * 再把percpu的总指针给request_queue:
+	 *   - block/blk-mq.c|2753| <<blk_mq_alloc_ctxs>> q->queue_ctx = ctxs->queue_ctx;
+	 */
 	struct blk_mq_ctx __percpu	*queue_ctx;
 };
 
@@ -18,10 +28,36 @@ struct blk_mq_ctxs {
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 在以下被使用:
+		 *   - block/blk-mq-debugfs.c|663| <<CTX_RQ_SEQ_OPS>> return seq_list_start(&ctx->rq_lists[type], *pos); \
+		 *   - block/blk-mq-debugfs.c|671| <<CTX_RQ_SEQ_OPS>> return seq_list_next(v, &ctx->rq_lists[type], pos); \
+		 *   - block/blk-mq-sched.c|312| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
+		 *   - block/blk-mq-sched.c|335| <<__blk_mq_sched_bio_merge>> !list_empty_careful(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|994| <<flush_busy_ctx>> list_splice_tail_init(&ctx->rq_lists[type], flush_data->list);
+		 *   - block/blk-mq.c|1029| <<dispatch_rq_from_ctx>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|1030| <<dispatch_rq_from_ctx>> dispatch_data->rq = list_entry_rq(ctx->rq_lists[type].next);
+		 *   - block/blk-mq.c|1032| <<dispatch_rq_from_ctx>> if (list_empty(&ctx->rq_lists[type]))
+		 *   - block/blk-mq.c|1673| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1675| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1722| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|2345| <<blk_mq_hctx_notify_dead>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|2346| <<blk_mq_hctx_notify_dead>> list_splice_init(&ctx->rq_lists[type], &tmp);
+		 *   - block/blk-mq.c|2512| <<blk_mq_init_cpu_queues>> INIT_LIST_HEAD(&__ctx->rq_lists[k]);
+		 */
 		struct list_head	rq_lists[HCTX_MAX_TYPES];
 	} ____cacheline_aligned_in_smp;
 
+	/*
+	 * 在blk_mq_init_cpu_queues()中初始化:
+	 * 这里的i是index, __ctx->cpu和__ctx的index相等
+	 * __ctx->cpu = i;
+	 */
 	unsigned int		cpu;
+	/*
+	 * 在以下初始化:
+	 *   - block/blk-mq.c|2627| <<blk_mq_map_swqueue>> ctx->index_hw[hctx->type] = hctx->nr_ctx;
+	 */
 	unsigned short		index_hw[HCTX_MAX_TYPES];
 
 	/* incremented at dispatch time */
@@ -32,6 +68,10 @@ struct blk_mq_ctx {
 	unsigned long		____cacheline_aligned_in_smp rq_completed[2];
 
 	struct request_queue	*queue;
+	/*
+	 * 在以下初始化:
+	 *   - block/blk-mq.c|2797| <<blk_mq_alloc_ctxs>> ctx->ctxs = ctxs;
+	 */
 	struct blk_mq_ctxs      *ctxs;
 	struct kobject		kobj;
 } ____cacheline_aligned_in_smp;
@@ -143,6 +183,13 @@ static inline enum mq_rq_state blk_mq_rq_state(struct request *rq)
 static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
 					   unsigned int cpu)
 {
+	/*
+	 * 先为struct blk_mq_ctxs分配:
+	 *   - block/blk-mq.c|2743| <<blk_mq_alloc_ctxs>> ctxs->queue_ctx = alloc_percpu(struct blk_mq_ctx);
+	 *
+	 * 再把percpu的总指针给request_queue:
+	 *   - block/blk-mq.c|2753| <<blk_mq_alloc_ctxs>> q->queue_ctx = ctxs->queue_ctx;
+	 */
 	return per_cpu_ptr(q->queue_ctx, cpu);
 }
 
@@ -174,11 +221,29 @@ struct blk_mq_alloc_data {
 	struct blk_mq_hw_ctx *hctx;
 };
 
+/*
+ * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 使用data->hctx->sched_tags
+ * 否则使用data->hctx->tags
+ */
 static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
 {
+	/*
+	 * BLK_MQ_REQ_INTERNAL只在一个地方设置 (存在q->elevator的情况下):
+	 *   - block/blk-mq.c|378| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 */
 	if (data->flags & BLK_MQ_REQ_INTERNAL)
 		return data->hctx->sched_tags;
 
+	/*
+	 * 在blk_mq_alloc_and_init_hctx()-->blk_mq_init_hctx()设置:
+	 *     hctx->tags = set->tags[hctx_idx];
+	 *
+	 * 在blk_mq_map_swqueue()也可能会设置:
+	 *     block/blk-mq.c|2640| <<blk_mq_map_swqueue>> hctx->tags = set->tags[i];
+	 *
+	 * set->tags在以下分配第二维
+	 *   - block/blk-mq.c|2501| <<__blk_mq_alloc_rq_map>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+	 */
 	return data->hctx->tags;
 }
 
@@ -242,6 +307,9 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
+/*
+ * 清空每一个blk_mq_queue_map->mq_map[cpu]
+ */
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 3e7038e..03aab35 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -18,6 +18,11 @@
 unsigned long blk_max_low_pfn;
 EXPORT_SYMBOL(blk_max_low_pfn);
 
+/*
+ * 在以下设置和使用:
+ *   - block/blk-settings.c|854| <<blk_settings_init>> blk_max_pfn = max_pfn - 1;
+ *   - block/bounce.c|371| <<blk_queue_bounce>> if (q->limits.bounce_pfn >= blk_max_pfn)
+ */
 unsigned long blk_max_pfn;
 
 void blk_queue_rq_timeout(struct request_queue *q, unsigned int timeout)
@@ -264,6 +269,10 @@ EXPORT_SYMBOL(blk_queue_max_write_zeroes_sectors);
  *    Enables a low level driver to set an upper limit on the number of
  *    hw data segments in a request.
  **/
+/*
+ * 在queue_max_segments()返回结果:
+ *   - include/linux/blkdev.h|1276| <<queue_max_segments>> return q->limits.max_segments;
+ */
 void blk_queue_max_segments(struct request_queue *q, unsigned short max_segments)
 {
 	if (!max_segments) {
diff --git a/block/elevator.c b/block/elevator.c
index f05e90d..ea963be 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -560,6 +560,18 @@ void elv_unregister(struct elevator_type *e)
 }
 EXPORT_SYMBOL_GPL(elv_unregister);
 
+/*
+ * [0] elevator_switch_mq
+ * [0] elevator_switch       
+ * [0] elv_iosched_store
+ * [0] queue_attr_store
+ * [0] kernfs_fop_write
+ * [0] __vfs_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 int elevator_switch_mq(struct request_queue *q,
 			      struct elevator_type *new_e)
 {
@@ -600,6 +612,10 @@ int elevator_switch_mq(struct request_queue *q,
  * queue devices.  If deadline isn't available OR we have multiple queues,
  * default to "none".
  */
+/*
+ * called only by:
+ *   - block/blk-mq.c|2901| <<blk_mq_init_allocated_queue>> ret = elevator_init_mq(q);
+ */
 int elevator_init_mq(struct request_queue *q)
 {
 	struct elevator_type *e;
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 0e030f5..a1f94fb 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -15,6 +15,14 @@ struct blk_flush_queue;
 struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 在以下插入新元素:
+		 *   - block/blk-mq-sched.c|228| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *   - block/blk-mq-sched.c|414| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1484| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1864| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2543| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 */
 		struct list_head	dispatch;
 		unsigned long		state;		/* BLK_MQ_S_* flags */
 	} ____cacheline_aligned_in_smp;
@@ -32,22 +40,73 @@ struct blk_mq_hw_ctx {
 
 	void			*driver_data;
 
+	/*
+	 * 在以下被使用:
+	 *   - block/blk-mq-debugfs.c|470| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sched.c|177| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq.c|89| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|101| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 *   - block/blk-mq.c|102| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|110| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|1011| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1057| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off,
+	 *   - block/blk-mq.c|2395| <<blk_mq_exit_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2457| <<blk_mq_init_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2490| <<blk_mq_init_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2710| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 */
 	struct sbitmap		ctx_map;
 
+	/*
+	 * 用到dispatch_from的地方:
+	 *   - block/blk-mq-sched.c|172| <<blk_mq_do_dispatch_ctx>> struct blk_mq_ctx *ctx = READ_ONCE(hctx->dispatch_from);
+	 *   - block/blk-mq-sched.c|201| <<blk_mq_do_dispatch_ctx>> WRITE_ONCE(hctx->dispatch_from, ctx);
+	 *   - block/blk-mq.c|2605| <<blk_mq_map_swqueue>> hctx->dispatch_from = NULL;
+	 */
 	struct blk_mq_ctx	*dispatch_from;
+	/*
+	 * 修改dispatch_busy的地方:
+	 *   - block/blk-mq.c|1221| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 */
 	unsigned int		dispatch_busy;
 
 	unsigned short		type;
 	unsigned short		nr_ctx;
+	/*
+	 * 在以下分配cpu个数目的struct blk_mq_ctxs
+	 *   - block/blk-mq.c|2406| <<blk_mq_init_hctx>> hctx->ctxs = kmalloc_array_node(nr_cpu_ids, sizeof(void *),
+	 *
+	 * 在以下初始化每一个元素
+	 *   - block/blk-mq.c|2594| <<blk_mq_map_swqueue>> hctx->ctxs[hctx->nr_ctx++] = ctx;
+	 */
 	struct blk_mq_ctx	**ctxs;
 
 	spinlock_t		dispatch_wait_lock;
 	wait_queue_entry_t	dispatch_wait;
 	atomic_t		wait_index;
 
+	/*
+	 * 在blk_mq_alloc_and_init_hctx()-->blk_mq_init_hctx()设置:
+	 *     hctx->tags = set->tags[hctx_idx];
+	 *
+	 * 在blk_mq_map_swqueue()也可能会设置:
+	 *     block/blk-mq.c|2640| <<blk_mq_map_swqueue>> hctx->tags = set->tags[i];
+	 *
+	 * set->tags在以下分配第二维
+	 *   - block/blk-mq.c|2501| <<__blk_mq_alloc_rq_map>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+	 */
 	struct blk_mq_tags	*tags;
+	/*
+	 * 在以下分配:
+	 *   - block/blk-mq-sched.c|457| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+	 */
 	struct blk_mq_tags	*sched_tags;
 
+	/*
+	 * 只在以下修改:
+	 *   - block/blk-mq-debugfs.c|619| <<hctx_queued_write>> hctx->queued = 0;
+	 *   - block/blk-mq.c|438| <<blk_mq_get_request>> data->hctx->queued++;
+	 */
 	unsigned long		queued;
 	unsigned long		run;
 #define BLK_MQ_MAX_DISPATCH_ORDER	7
@@ -76,8 +135,27 @@ struct blk_mq_hw_ctx {
 };
 
 struct blk_mq_queue_map {
+	/*
+	 * 猜测是每一个sw queue对应的hw queue???
+	 */
 	unsigned int *mq_map;
+	/*
+	 * 在以下进行修改:
+	 *   - block/blk-mq.c|3318| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 */
 	unsigned int nr_queues;
+	/*
+	 * 在以下修改:
+	 *   - drivers/nvme/host/pci.c|508| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+	 *   - drivers/nvme/host/rdma.c|1805| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|1811| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/rdma.c|1815| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|1825| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+	 *   - drivers/nvme/host/rdma.c|1828| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset +=
+	 *   - drivers/nvme/host/tcp.c|2069| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/tcp.c|2075| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/tcp.c|2081| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 */
 	unsigned int queue_offset;
 };
 
@@ -97,10 +175,34 @@ struct blk_mq_tag_set {
 	 * share maps between types.
 	 */
 	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
+	/*
+	 * 在以下设置:
+	 *   - block/blk-mq.c|2680| <<blk_mq_init_sq_queue>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3040| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3051| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - drivers/nvme/host/rdma.c|750| <<nvme_rdma_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+	 *   - drivers/nvme/host/tcp.c|1441| <<nvme_tcp_alloc_tagset>> set->nr_maps = 2 ;
+	 */
 	unsigned int		nr_maps;	/* nr entries in map[] */
 	const struct blk_mq_ops	*ops;
 	unsigned int		nr_hw_queues;	/* nr hw queues across maps */
 	unsigned int		queue_depth;	/* max hw supported */
+	/*
+	 * 在以下设置:
+	 *   - block/blk-mq-tag.c|454| <<blk_mq_init_tags>> tags->nr_reserved_tags = reserved_tags;
+	 *   - drivers/block/mtip32xx/mtip32xx.c|3724| <<mtip_block_initialize>> dd->tags.reserved_tags = 1;
+	 *   - drivers/ide/ide-probe.c|782| <<ide_init_queue>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|2427| <<nvme_fc_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|3061| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/host/rdma.c|729| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/rdma.c|742| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/tcp.c|1423| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/tcp.c|1434| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/target/loop.c|364| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/target/loop.c|536| <<nvme_loop_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 *
+	 * 在virtblk上是0
+	 */
 	unsigned int		reserved_tags;
 	unsigned int		cmd_size;	/* per-request extra data */
 	int			numa_node;
@@ -108,9 +210,17 @@ struct blk_mq_tag_set {
 	unsigned int		flags;		/* BLK_MQ_F_* */
 	void			*driver_data;
 
+	/*
+	 * 在以下分配第二维
+	 *   - block/blk-mq.c|2501| <<__blk_mq_alloc_rq_map>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+	 */
 	struct blk_mq_tags	**tags;
 
 	struct mutex		tag_list_lock;
+	/*
+	 * 在以下添加:
+	 *   - block/blk-mq.c|2705| <<blk_mq_add_queue_tag_set>> list_add_tail_rcu(&q->tag_set_list, &set->tag_list);
+	 */
 	struct list_head	tag_list;
 };
 
@@ -216,25 +326,134 @@ struct blk_mq_ops {
 };
 
 enum {
+	/*
+	 * 下面的BLK_MQ_F_xxx只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用
+	 * blk_mq_init_hctx()中把blk_mq_tag_set->flags拷贝到blk_mq_hw_hctx->flags:
+	 *   - block/blk-mq.c|2402| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 */
+	/*
+	 * 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x1):
+	 *   - block/blk-mq-sched.c|334| <<__blk_mq_sched_bio_merge>> if ((hctx->flags & BLK_MQ_F_SHOULD_MERGE) &&
+	 *   - drivers/block/null_blk_main.c|1553| <<null_init_tag_set>> set->flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/block/virtio_blk.c|787| <<virtblk_probe>> vblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/block/xen-blkfront.c|980| <<xlvbd_init_blk_queue>> info->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
+	 *   - drivers/nvme/host/pci.c|2340| <<nvme_dev_add>> dev->tagset.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/nvme/host/tcp.c|1436| <<nvme_tcp_alloc_tagset>> set->flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/scsi/scsi_lib.c|1902| <<scsi_mq_setup_tags>> shost->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
+	 */
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
+	/*
+	 * 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x2):
+	 *   - block/blk-mq-tag.c|78| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq-tag.h|79| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq-tag.h|92| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq.c|311| <<blk_mq_rq_ctx_init>> if (data->hctx->flags & BLK_MQ_F_TAG_SHARED) {
+	 *   - block/blk-mq.c|1116| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED)) {
+	 *   - block/blk-mq.c|1258| <<blk_mq_dispatch_rq_list>> if (hctx->flags & BLK_MQ_F_TAG_SHARED)
+	 *   - block/blk-mq.c|2681| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2683| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2709| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2733| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_SHARED)) {
+	 *   - block/blk-mq.c|2734| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2738| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_SHARED)
+	 */
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
+	/*
+	 * 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x4):
+	 *   - block/blk-mq.c|3097| <<blk_mq_init_allocated_queue>> if (!(set->flags & BLK_MQ_F_SG_MERGE))
+	 *   - drivers/block/loop.c|1940| <<loop_add>> lo->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
+	 *   - drivers/block/xen-blkfront.c|980| <<xlvbd_init_blk_queue>> info->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
+	 *   - drivers/scsi/scsi_lib.c|1902| <<scsi_mq_setup_tags>> shost->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
+	 */
 	BLK_MQ_F_SG_MERGE	= 1 << 2,
+	/*
+	 * 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x20):
+	 *   - block/blk-mq.c|245| <<blk_mq_quiesce_queue>> if (hctx->flags & BLK_MQ_F_BLOCKING)
+	 *   - block/blk-mq.c|640| <<hctx_unlock>> if (!(hctx->flags & BLK_MQ_F_BLOCKING))
+	 *   - block/blk-mq.c|649| <<hctx_lock>> if (!(hctx->flags & BLK_MQ_F_BLOCKING)) {
+	 *   - block/blk-mq.c|1406| <<__blk_mq_run_hw_queue>> might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
+	 *   - block/blk-mq.c|1474| <<__blk_mq_delay_run_hw_queue>> if (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {
+	 *   - block/blk-mq.c|2362| <<blk_mq_exit_hctx>> if (hctx->flags & BLK_MQ_F_BLOCKING)
+	 *   - block/blk-mq.c|2451| <<blk_mq_init_hctx>> if (hctx->flags & BLK_MQ_F_BLOCKING)
+	 *   - block/blk-mq.c|2878| <<blk_mq_hw_ctx_size>> if (tag_set->flags & BLK_MQ_F_BLOCKING)
+	 *   - drivers/block/null_blk_main.c|1323| <<null_queue_rq>> might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
+	 *   - drivers/block/null_blk_main.c|1559| <<null_init_tag_set>> set->flags |= BLK_MQ_F_BLOCKING;
+	 */
 	BLK_MQ_F_BLOCKING	= 1 << 5,
+	/*
+	 * 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x40):
+	 *   - block/blk-mq.c|3126| <<blk_mq_init_allocated_queue>> if (!(set->flags & BLK_MQ_F_NO_SCHED)) {
+	 *   - block/elevator.c|704| <<elv_support_iosched>> if (q->tag_set && (q->tag_set->flags & BLK_MQ_F_NO_SCHED))
+	 *   - drivers/block/null_blk_main.c|1555| <<null_init_tag_set>> set->flags |= BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/pci.c|1649| <<nvme_alloc_admin_tags>> dev->admin_tagset.flags = BLK_MQ_F_NO_SCHED;
+	 */
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
+	/*
+	 * 只在下面使用:
+	 *   - include/linux/blk-mq.h|393| <<BLK_MQ_FLAG_TO_ALLOC_POLICY>> ((flags >> BLK_MQ_F_ALLOC_POLICY_START_BIT) & \
+	 *   - include/linux/blk-mq.h|397| <<BLK_ALLOC_POLICY_TO_MQ_FLAG>> << BLK_MQ_F_ALLOC_POLICY_START_BIT)
+	 */
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
+	/*
+	 * 只在下面使用:
+	 *   - include/linux/blk-mq.h|394| <<BLK_MQ_FLAG_TO_ALLOC_POLICY>> ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1))
+	 *   - include/linux/blk-mq.h|396| <<BLK_ALLOC_POLICY_TO_MQ_FLAG>> ((policy & ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1)) \
+	 */
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
+	/*
+	 * BLK_MQ_S_xxx只用在blk_mq_hw_ctx->state
+	 */
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq.c|1569| <<blk_mq_stop_hw_queue>> set_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1594| <<blk_mq_start_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1615| <<blk_mq_start_stopped_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1639| <<blk_mq_run_work_fn>> if (test_bit(BLK_MQ_S_STOPPED, &hctx->state))
+	 *   - block/blk-mq.h|201| <<blk_mq_hctx_stopped>> return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 */
 	BLK_MQ_S_STOPPED	= 0,
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-tag.c|32| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|33| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|61| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|80| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 */
 	BLK_MQ_S_TAG_ACTIVE	= 1,
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1117| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq.c|1118| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *
+	 * 用来触发blk_mq_run_hw_queue()
+	 */
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
 
 	BLK_MQ_CPU_WORK_BATCH	= 8,
 };
+
+/*
+ * used by:
+ *   - block/blk-mq-debugfs.c|263| <<hctx_flags_show>> const int alloc_policy = BLK_MQ_FLAG_TO_ALLOC_POLICY(hctx->flags);
+ *   - block/blk-mq.c|2168| <<blk_mq_alloc_rq_map>> BLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));
+ */
 #define BLK_MQ_FLAG_TO_ALLOC_POLICY(flags) \
 	((flags >> BLK_MQ_F_ALLOC_POLICY_START_BIT) & \
 		((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1))
+/*
+ * used by:
+ *   - block/blk-mq-debugfs.c|273| <<hctx_flags_show>> hctx->flags ^ BLK_ALLOC_POLICY_TO_MQ_FLAG(alloc_policy),
+ *   - drivers/block/skd_main.c|2847| <<skd_cons_disk>> BLK_ALLOC_POLICY_TO_MQ_FLAG(BLK_TAG_ALLOC_FIFO);
+ *   - drivers/scsi/scsi_lib.c|1904| <<scsi_mq_setup_tags>> BLK_ALLOC_POLICY_TO_MQ_FLAG(shost->hostt->tag_alloc_policy);
+ */
 #define BLK_ALLOC_POLICY_TO_MQ_FLAG(policy) \
 	((policy & ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1)) \
 		<< BLK_MQ_F_ALLOC_POLICY_START_BIT)
@@ -259,12 +478,19 @@ bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 
 bool blk_mq_queue_inflight(struct request_queue *q);
 
+/*
+ * BLK_MQ_REQ_xxx只被blk_mq_alloc_data->flags使用
+ */
 enum {
 	/* return when out of requests */
 	BLK_MQ_REQ_NOWAIT	= (__force blk_mq_req_flags_t)(1 << 0),
 	/* allocate from reserved pool */
 	BLK_MQ_REQ_RESERVED	= (__force blk_mq_req_flags_t)(1 << 1),
 	/* allocate internal/sched tag */
+	/*
+	 * 只在一个地方设置 (存在q->elevator的情况下):
+	 *   - block/blk-mq.c|378| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 */
 	BLK_MQ_REQ_INTERNAL	= (__force blk_mq_req_flags_t)(1 << 2),
 	/* set RQF_PREEMPT */
 	BLK_MQ_REQ_PREEMPT	= (__force blk_mq_req_flags_t)(1 << 3),
@@ -349,6 +575,7 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	return rq + 1;
 }
 
+/* 对于每一个在用的(不是set中指定的)request_queue->queue_hw_ctx[i] */
 #define queue_for_each_hw_ctx(q, hctx, i)				\
 	for ((i) = 0; (i) < (q)->nr_hw_queues &&			\
 	     ({ hctx = (q)->queue_hw_ctx[i]; 1; }); (i)++)
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 338604d..d7d0137 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -62,6 +62,10 @@ typedef void (rq_end_io_fn)(struct request *, blk_status_t);
  * request flags */
 typedef __u32 __bitwise req_flags_t;
 
+/*
+ * RQF_xxx用在request->rq_flags
+ */
+
 /* elevator knows about this request */
 #define RQF_SORTED		((__force req_flags_t)(1 << 0))
 /* drive already may have started this one */
@@ -114,6 +118,9 @@ typedef __u32 __bitwise req_flags_t;
 /*
  * Request state for blk-mq.
  */
+/*
+ * MQ_RQ_xxx用在request->state
+ */
 enum mq_rq_state {
 	MQ_RQ_IDLE		= 0,
 	MQ_RQ_IN_FLIGHT		= 1,
@@ -404,13 +411,33 @@ struct request_queue {
 	const struct blk_mq_ops	*mq_ops;
 
 	/* sw queues */
+	/*
+	 * 先为struct blk_mq_ctxs分配:
+	 *   - block/blk-mq.c|2743| <<blk_mq_alloc_ctxs>> ctxs->queue_ctx = alloc_percpu(struct blk_mq_ctx);
+	 *
+	 * 再把percpu的总指针给request_queue:
+	 *   - block/blk-mq.c|2753| <<blk_mq_alloc_ctxs>> q->queue_ctx = ctxs->queue_ctx;
+	 */
 	struct blk_mq_ctx __percpu	*queue_ctx;
+	/*
+	 * 在以下修改 (更多情况是cpu的数目):
+	 *   - block/blk-mq.c|3041| <<blk_mq_init_allocated_queue>> q->nr_queues = nr_hw_queues(set);
+	 */
 	unsigned int		nr_queues;
 
 	unsigned int		queue_depth;
 
 	/* hw dispatch queues */
+	/*
+	 * 在blk_mq_init_allocated_queue()分配了第一维的数组
+	 *
+	 * 相当于在blk_mq_realloc_hw_ctxs()分配了第二维的数组
+	 */
 	struct blk_mq_hw_ctx	**queue_hw_ctx;
+	/*
+	 * 在以下修改 (更多情况是驱动设置的hw queue数目):
+	 *   - block/blk-mq.c|2973| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 */
 	unsigned int		nr_hw_queues;
 
 	struct backing_dev_info	*backing_dev_info;
@@ -481,7 +508,19 @@ struct request_queue {
 	struct blk_stat_callback	*poll_cb;
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
+	/*
+	 * 设置为blk_rq_timed_out_timer()
+	 */
 	struct timer_list	timeout;
+	/*
+	 * 在以下使用:
+	 *   - block/blk-core.c|234| <<blk_sync_queue>> cancel_work_sync(&q->timeout_work);
+	 *   - block/blk-core.c|462| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work);
+	 *   - block/blk-core.c|508| <<blk_alloc_queue_node>> INIT_WORK(&q->timeout_work, NULL);
+	 *   - block/blk-mq.c|916| <<blk_mq_timeout_work>> container_of(work, struct request_queue, timeout_work);
+	 *   - block/blk-mq.c|3056| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+	 *   - block/blk-timeout.c|88| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+	 */
 	struct work_struct	timeout_work;
 
 	struct list_head	icq_list;
@@ -552,6 +591,10 @@ struct request_queue {
 	struct percpu_ref	q_usage_counter;
 	struct list_head	all_q_node;
 
+	/*
+	 * 在以下设置:
+	 *   - block/blk-mq.c|3059| <<blk_mq_init_allocated_queue>> q->tag_set = set;
+	 */
 	struct blk_mq_tag_set	*tag_set;
 	struct list_head	tag_set_list;
 	struct bio_set		bio_split;
@@ -572,13 +615,36 @@ struct request_queue {
 	u64			write_hints[BLK_MAX_WRITE_HINTS];
 };
 
+/*
+ * QUEUE_FLAG_xxx在request_queue->queue_flags中使用
+ */
+
+/*
+ * 只在一个地方使用:
+ *   - include/linux/blkdev.h|659| <<blk_queue_stopped>> #define blk_queue_stopped(q) test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_STOPPED	1	/* queue is stopped */
 #define QUEUE_FLAG_DYING	2	/* queue being torn down */
 #define QUEUE_FLAG_BIDI		4	/* queue supports bidi requests */
+/*
+ * 在以下使用:
+ *   - block/blk-core.c|335| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+ *   - block/blk-sysfs.c|316| <<queue_nomerges_store>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, q);
+ *   - block/blk-sysfs.c|319| <<queue_nomerges_store>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+ *   - drivers/block/loop.c|220| <<__loop_update_dio>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/block/loop.c|223| <<__loop_update_dio>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/block/loop.c|1962| <<loop_add>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1914| <<megasas_set_nvme_device_properties>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, sdev->request_queue);
+ *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|2356| <<scsih_slave_configure>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES,
+ */
 #define QUEUE_FLAG_NOMERGES     5	/* disable merge attempts */
 #define QUEUE_FLAG_SAME_COMP	6	/* complete on same CPU-group */
 #define QUEUE_FLAG_FAIL_IO	7	/* fake timeout */
 #define QUEUE_FLAG_NONROT	9	/* non-rotational device (SSD) */
+/*
+ * 只在以下被使用:
+ *   - drivers/block/xen-blkfront.c|933| <<blkif_set_queue_limits>> blk_queue_flag_set(QUEUE_FLAG_VIRT, rq);
+ */
 #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
 #define QUEUE_FLAG_IO_STAT     10	/* do disk/partitions IO accounting */
 #define QUEUE_FLAG_DISCARD     11	/* supports DISCARD */
@@ -598,6 +664,12 @@ struct request_queue {
 #define QUEUE_FLAG_POLL_STATS  25	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED  26	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 27	/* queue supports SCSI commands */
+/*
+ * 在以下被使用:
+ *   - block/blk-mq.c|223| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+ *   - block/blk-mq.c|264| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+ *   - include/linux/blkdev.h|682| <<blk_queue_quiesced>> #define blk_queue_quiesced(q) test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_QUIESCED    28	/* queue has been quiesced */
 #define QUEUE_FLAG_PCI_P2PDMA  29	/* device supports PCI p2p requests */
 
@@ -616,6 +688,7 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 #define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
 #define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
 #define blk_queue_init_done(q)	test_bit(QUEUE_FLAG_INIT_DONE, &(q)->queue_flags)
+/* request_queue->queue_flags是否设置了QUEUE_FLAG_NOMERGES */
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_noxmerges(q)	\
 	test_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)
@@ -634,6 +707,13 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
 			     REQ_FAILFAST_DRIVER))
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|216| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|1562| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+ *   - block/blk-mq.c|1916| <<blk_mq_try_issue_directly>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q))) {
+ *   - drivers/mmc/core/queue.c|465| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+ */
 #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
 #define blk_queue_pm_only(q)	atomic_read(&(q)->pm_only)
 #define blk_queue_fua(q)	test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
-- 
2.7.4

