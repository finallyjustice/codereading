From c986a7a6365788c5f51c7d54995e8f445f16d35a Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Fri, 19 Apr 2019 05:52:00 +0800
Subject: [PATCH 1/1] block comment for block and drivers for mainline v5.1-rc5

This is for mainline v5.1-rc5

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 drivers/scsi/scsi_scan.c        |   6 +
 drivers/scsi/sd.c               |  34 ++
 drivers/scsi/virtio_scsi.c      |  52 ++++
 fs/io_uring.c                   | 664 ++++++++++++++++++++++++++++++++++++++++
 include/linux/percpu-refcount.h |  90 ++++++
 include/uapi/linux/io_uring.h   | 139 ++++++++-
 lib/percpu-refcount.c           | 163 ++++++++++
 7 files changed, 1147 insertions(+), 1 deletion(-)

diff --git a/drivers/scsi/scsi_scan.c b/drivers/scsi/scsi_scan.c
index 53380e0..4a3a7d6 100644
--- a/drivers/scsi/scsi_scan.c
+++ b/drivers/scsi/scsi_scan.c
@@ -1654,6 +1654,12 @@ static void scsi_scan_channel(struct Scsi_Host *shost, unsigned int channel,
 				id, lun, rescan);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_proc.c|255| <<scsi_add_single_device>> error = scsi_scan_host_selected(shost, channel, id, lun,
+ *   - drivers/scsi/scsi_scan.c|1816| <<do_scsi_scan_host>> scsi_scan_host_selected(shost, SCAN_WILD_CARD, SCAN_WILD_CARD,
+ *   - drivers/scsi/scsi_sysfs.c|149| <<scsi_scan>> res = scsi_scan_host_selected(shost, channel, id, lun,
+ */
 int scsi_scan_host_selected(struct Scsi_Host *shost, unsigned int channel,
 			    unsigned int id, u64 lun,
 			    enum scsi_scan_mode rescan)
diff --git a/drivers/scsi/sd.c b/drivers/scsi/sd.c
index 2b2bc4b..9e99f61 100644
--- a/drivers/scsi/sd.c
+++ b/drivers/scsi/sd.c
@@ -3364,6 +3364,40 @@ static void sd_probe_async(void *data, async_cookie_t cookie)
  *	Assume sd_probe is not re-entrant (for time being)
  *	Also think about sd_probe() and sd_remove() running coincidentally.
  **/
+/*
+ * virtio scsi的调用
+ * [0] sd_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init_sd
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * ata的调用
+ * [0] sd_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] bus_for_each_drv
+ * [0] __device_attach
+ * [0] bus_probe_device
+ * [0] device_add
+ * [0] scsi_sysfs_add_sdev
+ * [0] scsi_probe_and_add_lun
+ * [0] __scsi_add_device
+ * [0] ata_scsi_scan_host
+ * [0] async_run_entry_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static int sd_probe(struct device *dev)
 {
 	struct scsi_device *sdp = to_scsi_device(dev);
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index f8cb7c2..0a4bea6 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -109,6 +109,12 @@ static void virtscsi_compute_resid(struct scsi_cmnd *sc, u32 resid)
  *
  * Called with vq_lock held.
  */
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|203| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|213| <<virtscsi_poll_requests>> virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|538| <<virtscsi_queuecommand>> virtscsi_complete_cmd(vscsi, cmd);
+ */
 static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
@@ -172,6 +178,13 @@ static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 	sc->scsi_done(sc);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|209| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|218| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i],
+ *   - drivers/scsi/virtio_scsi.c|235| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+ *   - drivers/scsi/virtio_scsi.c|381| <<virtscsi_event_done>> virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
+ */
 static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 			     struct virtio_scsi_vq *virtscsi_vq,
 			     void (*fn)(struct virtio_scsi *vscsi, void *buf))
@@ -496,6 +509,37 @@ static struct virtio_scsi_vq *virtscsi_pick_vq_mq(struct virtio_scsi *vscsi,
 	return &vscsi->req_vqs[hwq];
 }
 
+/*
+ * [0] virtscsi_queuecommand
+ * [0] scsi_queue_rq
+ * [0] blk_mq_dispatch_rq_list
+ * [0] blk_mq_sched_dispatch_requests
+ * [0] __blk_mq_run_hw_queue
+ * [0] __blk_mq_delay_run_hw_queue
+ * [0] blk_mq_run_hw_queue
+ * [0] blk_mq_sched_insert_request
+ * [0] blk_execute_rq
+ * [0] __scsi_execute
+ * [0] scsi_probe_and_add_lun
+ * [0] __scsi_scan_target
+ * [0] scsi_scan_channel
+ * [0] scsi_scan_host_selected
+ * [0] scsi_scan_host
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 static int virtscsi_queuecommand(struct Scsi_Host *shost,
 				 struct scsi_cmnd *sc)
 {
@@ -720,6 +764,11 @@ static void virtscsi_remove_vqs(struct virtio_device *vdev)
 	vdev->config->del_vqs(vdev);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|818| <<virtscsi_probe>> err = virtscsi_init(vdev, vscsi);
+ *   - drivers/scsi/virtio_scsi.c|895| <<virtscsi_restore>> err = virtscsi_init(vdev, vscsi);
+ */
 static int virtscsi_init(struct virtio_device *vdev,
 			 struct virtio_scsi *vscsi)
 {
@@ -747,6 +796,9 @@ static int virtscsi_init(struct virtio_device *vdev,
 	names[0] = "control";
 	names[1] = "event";
 	for (i = VIRTIO_SCSI_VQ_BASE; i < num_vqs; i++) {
+		/*
+		 * 设置重要的中断函数
+		 */
 		callbacks[i] = virtscsi_req_done;
 		names[i] = "request";
 	}
diff --git a/fs/io_uring.c b/fs/io_uring.c
index 89aa841..1b18fa3 100644
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@ -62,9 +62,39 @@
 
 #include "internal.h"
 
+/*
+ * 对于sq
+ * user修改tail, kernel修改head
+ * The head and tail values are used to manage entries in the ring; if the
+ * two values are equal, the ring is empty. User-space code adds an entry
+ * by putting its index into array[r.tail] and incrementing the tail
+ * pointer; only the kernel side should change r.head. Once one or more
+ * entries have been placed in the ring, they can be submitted with a call
+ * to:
+ *
+ *    int io_uring_enter(unsigned int fd, u32 to_submit, u32 min_complete, u32 flags);
+ *
+ * 对于cq
+ * kernel修改tail, user修改head
+ * In this ring, the r.head index points to the first available completion
+ * event, while r.tail points to the last; user space should only change
+ * r.head.
+ */
+
 #define IORING_MAX_ENTRIES	4096
 #define IORING_MAX_FIXED_FILES	1024
 
+/*
+ * The head and tail values are used to manage entries in the ring; if the 
+ * two values are equal, the ring is empty. User-space code adds an entry
+ * by putting its index into array[r.tail] and incrementing the tail
+ * pointer; only the kernel side should change r.head. Once one or more
+ * entries have been placed in the ring, they can be submitted with a call
+ * to:
+ *
+ * int io_uring_enter(unsigned int fd, u32 to_submit, u32 min_complete, u32 flags);
+ */
+
 struct io_uring {
 	u32 head ____cacheline_aligned_in_smp;
 	u32 tail ____cacheline_aligned_in_smp;
@@ -104,6 +134,13 @@ struct async_list {
 	size_t			io_pages;
 };
 
+/*
+ * io_allocate_scq_urings()执行后:
+ * struct io_ring_ctx
+ *   struct io_sq_ring    *sq_ring; --> 指向struct io_sq_ring + 一组u32 array[]
+ *   struct io_uring_sqe  *sq_sqes; --> 指向一组struct io_uring_sqe[]
+ *   struct io_cq_ring    *cq_ring; --> 指向struct io_cq_ring + 一组struct io_uring_cqe[]
+ */
 struct io_ring_ctx {
 	struct {
 		struct percpu_ref	refs;
@@ -116,8 +153,21 @@ struct io_ring_ctx {
 
 		/* SQ ring */
 		struct io_sq_ring	*sq_ring;
+		/*
+		 * ctx->cached_sq_head修改的地方:
+		 *   - fs/io_uring.c|2085| <<io_drop_sqring>> ctx->cached_sq_head--;
+		 *   - fs/io_uring.c|2119| <<io_get_sqring>> ctx->cached_sq_head++;
+		 *   - fs/io_uring.c|2124| <<io_get_sqring>> ctx->cached_sq_head++;
+		 *
+		 * 对于sq
+		 * user修改tail, kernel修改head
+		 */
 		unsigned		cached_sq_head;
 		unsigned		sq_entries;
+		/*
+		 * 唯一修改的地方:
+		 *   - fs/io_uring.c|2792| <<io_allocate_scq_urings>> ctx->sq_mask = sq_ring->ring_mask;
+		 */
 		unsigned		sq_mask;
 		unsigned		sq_thread_idle;
 		struct io_uring_sqe	*sq_sqes;
@@ -127,6 +177,17 @@ struct io_ring_ctx {
 	struct workqueue_struct	*sqo_wq;
 	struct task_struct	*sqo_thread;	/* if using sq thread polling */
 	struct mm_struct	*sqo_mm;
+	/*
+	 * sqo_wait在以下使用:
+	 *   - fs/io_uring.c|587| <<io_cqring_ev_posted>> if (waitqueue_active(&ctx->sqo_wait))
+	 *   - fs/io_uring.c|588| <<io_cqring_ev_posted>> wake_up(&ctx->sqo_wait);
+	 *   - fs/io_uring.c|2077| <<io_sq_thread>> prepare_to_wait(&ctx->sqo_wait, &wait,
+	 *   - fs/io_uring.c|2086| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|2092| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|2098| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|2448| <<io_sq_offload_start>> init_waitqueue_head(&ctx->sqo_wait);
+	 *   - fs/io_uring.c|2891| <<SYSCALL_DEFINE6>> wake_up(&ctx->sqo_wait);
+	 */
 	wait_queue_head_t	sqo_wait;
 	unsigned		sqo_stop;
 
@@ -135,7 +196,17 @@ struct io_ring_ctx {
 		struct io_cq_ring	*cq_ring;
 		unsigned		cached_cq_tail;
 		unsigned		cq_entries;
+		/*
+		 * 唯一修改的地方:
+		 *   - fs/io_uring.c|2815| <<io_allocate_scq_urings>> ctx->cq_mask = cq_ring->ring_mask;
+		 */
 		unsigned		cq_mask;
+		/*
+		 * cq_wait在以下使用:
+		 *   - fs/io_uring.c|468| <<io_commit_cqring>> if (wq_has_sleeper(&ctx->cq_wait)) {
+		 *   - fs/io_uring.c|469| <<io_commit_cqring>> wake_up_interruptible(&ctx->cq_wait);
+		 *   - fs/io_uring.c|2713| <<io_uring_poll>> poll_wait(file, &ctx->cq_wait, wait);
+		 */
 		struct wait_queue_head	cq_wait;
 		struct fasync_struct	*cq_fasync;
 	} ____cacheline_aligned_in_smp;
@@ -154,10 +225,28 @@ struct io_ring_ctx {
 
 	struct user_struct	*user;
 
+	/*
+	 * used by:
+	 *   - fs/io_uring.c|326| <<io_ring_ctx_ref_free>> complete(&ctx->ctx_done);
+	 *   - fs/io_uring.c|345| <<io_ring_ctx_alloc>> init_completion(&ctx->ctx_done);
+	 *   - fs/io_uring.c|2649| <<io_ring_ctx_wait_and_kill>> wait_for_completion(&ctx->ctx_done);
+	 *   - fs/io_uring.c|2985| <<__io_uring_register>> wait_for_completion(&ctx->ctx_done);
+	 *   - fs/io_uring.c|3012| <<__io_uring_register>> reinit_completion(&ctx->ctx_done);
+	 */
 	struct completion	ctx_done;
 
 	struct {
 		struct mutex		uring_lock;
+		/*
+		 * wait在以下被使用:
+		 *   - fs/io_uring.c|450| <<io_ring_ctx_alloc>> init_waitqueue_head(&ctx->wait);
+		 *   - fs/io_uring.c|556| <<io_cqring_ev_posted>> if (waitqueue_active(&ctx->wait))
+		 *   - fs/io_uring.c|557| <<io_cqring_ev_posted>> wake_up(&ctx->wait);
+		 *   - fs/io_uring.c|579| <<io_ring_drop_ctx_refs>> if (waitqueue_active(&ctx->wait))
+		 *   - fs/io_uring.c|580| <<io_ring_drop_ctx_refs>> wake_up(&ctx->wait);
+		 *   - fs/io_uring.c|2183| <<io_cqring_wait>> prepare_to_wait(&ctx->wait, &wait, TASK_INTERRUPTIBLE);
+		 *   - fs/io_uring.c|2198| <<io_cqring_wait>> finish_wait(&ctx->wait, &wait);
+		 */
 		wait_queue_head_t	wait;
 	} ____cacheline_aligned_in_smp;
 
@@ -170,10 +259,39 @@ struct io_ring_ctx {
 		 * For SQPOLL, only the single threaded io_sq_thread() will
 		 * manipulate the list, hence no extra locking is needed there.
 		 */
+		/*
+		 * poll_list在以下被使用:
+		 *   - fs/io_uring.c|370| <<io_ring_ctx_alloc>> INIT_LIST_HEAD(&ctx->poll_list);
+		 *   - fs/io_uring.c|587| <<io_do_iopoll>> list_for_each_entry_safe(req, tmp, &ctx->poll_list, list) {
+		 *   - fs/io_uring.c|625| <<io_iopoll_getevents>> while (!list_empty(&ctx->poll_list)) {
+		 *   - fs/io_uring.c|648| <<io_iopoll_reap_events>> while (!list_empty(&ctx->poll_list)) {
+		 *   - fs/io_uring.c|727| <<io_iopoll_req_issued>> if (list_empty(&ctx->poll_list)) {
+		 *   - fs/io_uring.c|732| <<io_iopoll_req_issued>> list_req = list_first_entry(&ctx->poll_list, struct io_kiocb,
+		 *   - fs/io_uring.c|743| <<io_iopoll_req_issued>> list_add(&req->list, &ctx->poll_list);
+		 *   - fs/io_uring.c|745| <<io_iopoll_req_issued>> list_add_tail(&req->list, &ctx->poll_list);
+		 */
 		struct list_head	poll_list;
+		/*
+		 * cancel_list在以下被使用:
+		 *   - fs/io_uring.c|382| <<io_ring_ctx_alloc>> INIT_LIST_HEAD(&ctx->cancel_list);
+		 *   - fs/io_uring.c|1219| <<io_poll_remove_all>> while (!list_empty(&ctx->cancel_list)) {
+		 *   - fs/io_uring.c|1220| <<io_poll_remove_all>> req = list_first_entry(&ctx->cancel_list, struct io_kiocb,list);
+		 *   - fs/io_uring.c|1243| <<io_poll_remove>> list_for_each_entry_safe(poll_req, next, &ctx->cancel_list, list) {
+		 *   - fs/io_uring.c|1397| <<io_poll_add>> list_add_tail(&req->list, &ctx->cancel_list);
+		 */
 		struct list_head	cancel_list;
 	} ____cacheline_aligned_in_smp;
 
+	/*
+	 * pending_async在以下被使用:
+	 *   - fs/io_uring.c|383| <<io_ring_ctx_alloc>> for (i = 0; i < ARRAY_SIZE(ctx->pending_async); i++) {
+	 *   - fs/io_uring.c|384| <<io_ring_ctx_alloc>> spin_lock_init(&ctx->pending_async[i].lock);
+	 *   - fs/io_uring.c|385| <<io_ring_ctx_alloc>> INIT_LIST_HEAD(&ctx->pending_async[i].list);
+	 *   - fs/io_uring.c|386| <<io_ring_ctx_alloc>> atomic_set(&ctx->pending_async[i].cnt, 0);
+	 *   - fs/io_uring.c|991| <<io_async_list_note>> struct async_list *async_list = &req->ctx->pending_async[rw];
+	 *   - fs/io_uring.c|1491| <<io_async_list_from_sqe>> return &ctx->pending_async[READ];
+	 *   - fs/io_uring.c|1494| <<io_async_list_from_sqe>> return &ctx->pending_async[WRITE];
+	 */
 	struct async_list	pending_async[2];
 
 #if defined(CONFIG_UNIX)
@@ -182,7 +300,13 @@ struct io_ring_ctx {
 };
 
 struct sqe_submit {
+	/*
+	 * 例子是&ctx->sq_sqes[head]
+	 */
 	const struct io_uring_sqe	*sqe;
+	/*
+	 * 例子是READ_ONCE(ring->array[head & ctx->sq_mask])
+	 */
 	unsigned short			index;
 	bool				has_user;
 	bool				needs_lock;
@@ -211,6 +335,11 @@ struct io_poll_iocb {
 struct io_kiocb {
 	union {
 		struct file		*file;
+		/*
+		 * 有两处设置ki_complete:
+		 *   - fs/io_uring.c|872| <<io_prep_rw>> kiocb->ki_complete = io_complete_rw_iopoll;
+		 *   - fs/io_uring.c|876| <<io_prep_rw>> kiocb->ki_complete = io_complete_rw;
+		 */
 		struct kiocb		rw;
 		struct io_poll_iocb	poll;
 	};
@@ -252,13 +381,33 @@ struct io_submit_state {
 	unsigned int		fd;
 	unsigned int		has_refs;
 	unsigned int		used_refs;
+	/*
+	 * 修改ios_left的地方:
+	 *   - fs/io_uring.c|814| <<io_file_get>> state->ios_left--;
+	 *   - fs/io_uring.c|826| <<io_file_get>> state->ios_left--;
+	 *   - fs/io_uring.c|1792| <<io_submit_state_start>> state->ios_left = max_ios;
+	 */
 	unsigned int		ios_left;
 };
 
+/*
+ * used by:
+ *   - fs/io_uring.c|518| <<io_get_req>> req = kmem_cache_alloc(req_cachep, gfp);
+ *   - fs/io_uring.c|526| <<io_get_req>> ret = kmem_cache_alloc_bulk(req_cachep, gfp, sz, state->reqs);
+ *   - fs/io_uring.c|533| <<io_get_req>> state->reqs[0] = kmem_cache_alloc(req_cachep, gfp);
+ *   - fs/io_uring.c|560| <<io_free_req_many>> kmem_cache_free_bulk(req_cachep, *nr, reqs);
+ *   - fs/io_uring.c|571| <<io_free_req>> kmem_cache_free(req_cachep, req);
+ *   - fs/io_uring.c|1785| <<io_submit_state_end>> kmem_cache_free_bulk(req_cachep, state->free_reqs,
+ *   - fs/io_uring.c|3128| <<io_uring_init>> req_cachep = KMEM_CACHE(io_kiocb, SLAB_HWCACHE_ALIGN | SLAB_PANIC);
+ */
 static struct kmem_cache *req_cachep;
 
 static const struct file_operations io_uring_fops;
 
+/*
+ * called only by:
+ *   - net/unix/scm.c|38| <<unix_get_socket>> u_sock = io_uring_get_socket(filp);
+ */
 struct sock *io_uring_get_socket(struct file *file)
 {
 #if defined(CONFIG_UNIX)
@@ -272,13 +421,44 @@ struct sock *io_uring_get_socket(struct file *file)
 }
 EXPORT_SYMBOL(io_uring_get_socket);
 
+/*
+ * callstack的一个例子
+ * [0] io_ring_ctx_ref_free
+ * [0] percpu_ref_switch_to_atomic_rcu
+ * [0] rcu_core
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] smp_apic_timer_interrupt
+ * [0] apic_timer_interrupt
+ *
+ * used only by:
+ *   - fs/io_uring.c|410| <<io_ring_ctx_alloc>> if (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free, 0, GFP_KERNEL)) {
+ *
+ * 被用做io_ring_ctx->refs这个percpu_ref的release函数
+ */
 static void io_ring_ctx_ref_free(struct percpu_ref *ref)
 {
 	struct io_ring_ctx *ctx = container_of(ref, struct io_ring_ctx, refs);
 
+	/*
+	 * used by:
+	 *   - fs/io_uring.c|326| <<io_ring_ctx_ref_free>> complete(&ctx->ctx_done);
+	 *   - fs/io_uring.c|345| <<io_ring_ctx_alloc>> init_completion(&ctx->ctx_done);
+	 *   - fs/io_uring.c|2649| <<io_ring_ctx_wait_and_kill>> wait_for_completion(&ctx->ctx_done);
+	 *   - fs/io_uring.c|2985| <<__io_uring_register>> wait_for_completion(&ctx->ctx_done);
+	 *   - fs/io_uring.c|3012| <<__io_uring_register>> reinit_completion(&ctx->ctx_done);
+	 */
 	complete(&ctx->ctx_done);
 }
 
+/*
+ * 分配和简单初始化io_ring_ctx
+ *
+ * called by only:
+ *   - fs/io_uring.c|2972| <<io_uring_create>> ctx = io_ring_ctx_alloc(p);
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)-->io_uring_setup()-->io_uring_create()-->io_ring_ctx_alloc()
+ */
 static struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)
 {
 	struct io_ring_ctx *ctx;
@@ -309,12 +489,21 @@ static struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)
 	return ctx;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|527| <<io_cqring_add_event>> io_commit_cqring(ctx);
+ *   - fs/io_uring.c|647| <<io_iopoll_complete>> io_commit_cqring(ctx);
+ *   - fs/io_uring.c|1330| <<io_poll_complete>> io_commit_cqring(ctx);
+ */
 static void io_commit_cqring(struct io_ring_ctx *ctx)
 {
 	struct io_cq_ring *ring = ctx->cq_ring;
 
 	if (ctx->cached_cq_tail != READ_ONCE(ring->r.tail)) {
 		/* order cqe stores with ring update */
+		/*
+		 * barrier之后调用WRITE_ONCE(*p, v)
+		 */
 		smp_store_release(&ring->r.tail, ctx->cached_cq_tail);
 
 		/*
@@ -324,12 +513,25 @@ static void io_commit_cqring(struct io_ring_ctx *ctx)
 		smp_wmb();
 
 		if (wq_has_sleeper(&ctx->cq_wait)) {
+			/*
+			 * cq_wait在以下使用:
+			 *   - fs/io_uring.c|468| <<io_commit_cqring>> if (wq_has_sleeper(&ctx->cq_wait)) {
+			 *   - fs/io_uring.c|469| <<io_commit_cqring>> wake_up_interruptible(&ctx->cq_wait);
+			 *   - fs/io_uring.c|2713| <<io_uring_poll>> poll_wait(file, &ctx->cq_wait, wait);
+			 */
 			wake_up_interruptible(&ctx->cq_wait);
+			/*
+			 * 通知用户进程???
+			 */
 			kill_fasync(&ctx->cq_fasync, SIGIO, POLL_IN);
 		}
 	}
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|500| <<io_cqring_fill_event>> cqe = io_get_cqring(ctx);
+ */
 static struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)
 {
 	struct io_cq_ring *ring = ctx->cq_ring;
@@ -338,6 +540,9 @@ static struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)
 	tail = ctx->cached_cq_tail;
 	/* See comment at the top of the file */
 	smp_rmb();
+	/*
+	 * 为什么还有一个不可以??
+	 */
 	if (tail + 1 == READ_ONCE(ring->r.head))
 		return NULL;
 
@@ -345,6 +550,14 @@ static struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)
 	return &ring->cqes[tail & ctx->cq_mask];
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|556| <<io_cqring_add_event>> io_cqring_fill_event(ctx, user_data, res, ev_flags);
+ *   - fs/io_uring.c|658| <<io_iopoll_complete>> io_cqring_fill_event(ctx, req->user_data, req->error, 0);
+ *   - fs/io_uring.c|1359| <<io_poll_complete>> io_cqring_fill_event(ctx, req->user_data, mangle_poll(mask), 0);
+ *
+ * 这个函数是把返回的数据结果写入cq的某一个entry
+ */
 static void io_cqring_fill_event(struct io_ring_ctx *ctx, u64 ki_user_data,
 				 long res, unsigned ev_flags)
 {
@@ -355,6 +568,7 @@ static void io_cqring_fill_event(struct io_ring_ctx *ctx, u64 ki_user_data,
 	 * submission (by quite a lot). Increment the overflow count in
 	 * the ring.
 	 */
+	/* 只在这里被调用 */
 	cqe = io_get_cqring(ctx);
 	if (cqe) {
 		WRITE_ONCE(cqe->user_data, ki_user_data);
@@ -367,14 +581,53 @@ static void io_cqring_fill_event(struct io_ring_ctx *ctx, u64 ki_user_data,
 	}
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|572| <<io_cqring_add_event>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|1403| <<io_poll_complete_work>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|1428| <<io_poll_wake>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|1518| <<io_poll_add>> io_cqring_ev_posted(ctx);
+ *
+ * 唤醒ctx->wait和ctx->sqo_wait
+ */
 static void io_cqring_ev_posted(struct io_ring_ctx *ctx)
 {
+	/*
+	 * wait在以下被使用:
+	 *   - fs/io_uring.c|450| <<io_ring_ctx_alloc>> init_waitqueue_head(&ctx->wait);
+	 *   - fs/io_uring.c|556| <<io_cqring_ev_posted>> if (waitqueue_active(&ctx->wait))
+	 *   - fs/io_uring.c|557| <<io_cqring_ev_posted>> wake_up(&ctx->wait);
+	 *   - fs/io_uring.c|579| <<io_ring_drop_ctx_refs>> if (waitqueue_active(&ctx->wait))
+	 *   - fs/io_uring.c|580| <<io_ring_drop_ctx_refs>> wake_up(&ctx->wait);
+	 *   - fs/io_uring.c|2183| <<io_cqring_wait>> prepare_to_wait(&ctx->wait, &wait, TASK_INTERRUPTIBLE);
+	 *   - fs/io_uring.c|2198| <<io_cqring_wait>> finish_wait(&ctx->wait, &wait);
+	 */
 	if (waitqueue_active(&ctx->wait))
 		wake_up(&ctx->wait);
+	/*
+	 * sqo_wait在以下使用:
+	 *   - fs/io_uring.c|587| <<io_cqring_ev_posted>> if (waitqueue_active(&ctx->sqo_wait))
+	 *   - fs/io_uring.c|588| <<io_cqring_ev_posted>> wake_up(&ctx->sqo_wait);
+	 *   - fs/io_uring.c|2077| <<io_sq_thread>> prepare_to_wait(&ctx->sqo_wait, &wait,
+	 *   - fs/io_uring.c|2086| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|2092| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|2098| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|2448| <<io_sq_offload_start>> init_waitqueue_head(&ctx->sqo_wait);
+	 *   - fs/io_uring.c|2891| <<SYSCALL_DEFINE6>> wake_up(&ctx->sqo_wait);
+	 */
 	if (waitqueue_active(&ctx->sqo_wait))
 		wake_up(&ctx->sqo_wait);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|869| <<io_complete_rw>> io_cqring_add_event(req->ctx, req->user_data, res, 0);
+ *   - fs/io_uring.c|1306| <<io_nop>> io_cqring_add_event(ctx, user_data, err, 0);
+ *   - fs/io_uring.c|1355| <<io_fsync>> io_cqring_add_event(req->ctx, sqe->user_data, ret, 0);
+ *   - fs/io_uring.c|1413| <<io_poll_remove>> io_cqring_add_event(req->ctx, sqe->user_data, ret, 0);
+ *   - fs/io_uring.c|1713| <<io_sq_wq_submit_work>> io_cqring_add_event(ctx, sqe->user_data, ret, 0);
+ *   - fs/io_uring.c|2023| <<io_submit_sqes>> io_cqring_add_event(ctx, sqes[i].sqe->user_data, ret, 0);
+ */
 static void io_cqring_add_event(struct io_ring_ctx *ctx, u64 user_data,
 				long res, unsigned ev_flags)
 {
@@ -388,6 +641,13 @@ static void io_cqring_add_event(struct io_ring_ctx *ctx, u64 user_data,
 	io_cqring_ev_posted(ctx);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|691| <<io_get_req>> io_ring_drop_ctx_refs(ctx, 1);
+ *   - fs/io_uring.c|699| <<io_free_req_many>> io_ring_drop_ctx_refs(ctx, *nr);
+ *   - fs/io_uring.c|708| <<io_free_req>> io_ring_drop_ctx_refs(req->ctx, 1);
+ *   - fs/io_uring.c|2965| <<SYSCALL_DEFINE6(io_uring_enter)>> io_ring_drop_ctx_refs(ctx, 1);
+ */
 static void io_ring_drop_ctx_refs(struct io_ring_ctx *ctx, unsigned refs)
 {
 	percpu_ref_put_many(&ctx->refs, refs);
@@ -396,6 +656,11 @@ static void io_ring_drop_ctx_refs(struct io_ring_ctx *ctx, unsigned refs)
 		wake_up(&ctx->wait);
 }
 
+/*
+ * called by:
+ *   - called by only:
+ *   - fs/io_uring.c|1656| <<io_submit_sqe>> req = io_get_req(ctx, state);
+ */
 static struct io_kiocb *io_get_req(struct io_ring_ctx *ctx,
 				   struct io_submit_state *state)
 {
@@ -413,6 +678,12 @@ static struct io_kiocb *io_get_req(struct io_ring_ctx *ctx,
 		size_t sz;
 		int ret;
 
+		/*
+		 * 修改ios_left的地方:
+		 *   - fs/io_uring.c|814| <<io_file_get>> state->ios_left--;
+		 *   - fs/io_uring.c|826| <<io_file_get>> state->ios_left--;
+		 *   - fs/io_uring.c|1792| <<io_submit_state_start>> state->ios_left = max_ios;
+		 */
 		sz = min_t(size_t, state->ios_left, ARRAY_SIZE(state->reqs));
 		ret = kmem_cache_alloc_bulk(req_cachep, gfp, sz, state->reqs);
 
@@ -462,6 +733,20 @@ static void io_free_req(struct io_kiocb *req)
 	kmem_cache_free(req_cachep, req);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|879| <<io_complete_rw>> io_put_req(req);
+ *   - fs/io_uring.c|1316| <<io_nop>> io_put_req(req);
+ *   - fs/io_uring.c|1365| <<io_fsync>> io_put_req(req);
+ *   - fs/io_uring.c|1423| <<io_poll_remove>> io_put_req(req);
+ *   - fs/io_uring.c|1464| <<io_poll_complete_work>> io_put_req(req);
+ *   - fs/io_uring.c|1489| <<io_poll_wake>> io_put_req(req);
+ *   - fs/io_uring.c|1579| <<io_poll_add>> io_put_req(req);
+ *   - fs/io_uring.c|1719| <<io_sq_wq_submit_work>> io_put_req(req);
+ *   - fs/io_uring.c|1723| <<io_sq_wq_submit_work>> io_put_req(req);
+ *   - fs/io_uring.c|1903| <<io_submit_sqe>> io_put_req(req);
+ *   - fs/io_uring.c|1907| <<io_submit_sqe>> io_put_req(req);
+ */
 static void io_put_req(struct io_kiocb *req)
 {
 	if (refcount_dec_and_test(&req->refs))
@@ -506,6 +791,10 @@ static void io_iopoll_complete(struct io_ring_ctx *ctx, unsigned int *nr_events,
 	io_free_req_many(ctx, reqs, &to_free);
 }
 
+/*
+ * called only by:
+ *   - fs/io_uring.c|841| <<io_iopoll_getevents>> ret = io_do_iopoll(ctx, nr_events, min);
+ */
 static int io_do_iopoll(struct io_ring_ctx *ctx, unsigned int *nr_events,
 			long min)
 {
@@ -556,6 +845,11 @@ static int io_do_iopoll(struct io_ring_ctx *ctx, unsigned int *nr_events,
  * non-spinning poll check - we'll still enter the driver poll loop, but only
  * as a non-spinning completion check.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|864| <<io_iopoll_reap_events>> io_iopoll_getevents(ctx, &nr_events, 1);
+ *   - fs/io_uring.c|880| <<io_iopoll_check>> ret = io_iopoll_getevents(ctx, nr_events, tmin);
+ */
 static int io_iopoll_getevents(struct io_ring_ctx *ctx, unsigned int *nr_events,
 				long min)
 {
@@ -590,6 +884,11 @@ static void io_iopoll_reap_events(struct io_ring_ctx *ctx)
 	mutex_unlock(&ctx->uring_lock);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2102| <<io_sq_thread>> io_iopoll_check(ctx, &nr_events, 0);
+ *   - fs/io_uring.c|2986| <<SYSCALL_DEFINE6(io_uring_enter)>> ret = io_iopoll_check(ctx, &nr_events, min_complete);
+ */
 static int io_iopoll_check(struct io_ring_ctx *ctx, unsigned *nr_events,
 			   long min)
 {
@@ -610,6 +909,11 @@ static int io_iopoll_check(struct io_ring_ctx *ctx, unsigned *nr_events,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|922| <<io_complete_rw>> kiocb_end_write(kiocb);
+ *   - fs/io_uring.c|932| <<io_complete_rw_iopoll>> kiocb_end_write(kiocb);
+ */
 static void kiocb_end_write(struct kiocb *kiocb)
 {
 	if (kiocb->ki_flags & IOCB_WRITE) {
@@ -741,6 +1045,11 @@ static bool io_file_supports_async(struct file *file)
 	return false;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1242| <<io_read>> ret = io_prep_rw(req, s, force_nonblock, state);
+ *   - fs/io_uring.c|1289| <<io_write>> ret = io_prep_rw(req, s, force_nonblock, state);
+ */
 static int io_prep_rw(struct io_kiocb *req, const struct sqe_submit *s,
 		      bool force_nonblock, struct io_submit_state *state)
 {
@@ -939,6 +1248,13 @@ static void io_async_list_note(int rw, struct io_kiocb *req, size_t len)
 	async_list->io_end = io_end;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1658| <<__io_submit_sqe>> ret = io_read(req, s, force_nonblock, state);
+ *   - fs/io_uring.c|1666| <<__io_submit_sqe>> ret = io_read(req, s, force_nonblock, state);
+ *
+ * 处理IORING_OP_READV或者IORING_OP_READ_FIXED
+ */
 static int io_read(struct io_kiocb *req, const struct sqe_submit *s,
 		   bool force_nonblock, struct io_submit_state *state)
 {
@@ -986,6 +1302,13 @@ static int io_read(struct io_kiocb *req, const struct sqe_submit *s,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1670| <<__io_submit_sqe>> ret = io_write(req, s, force_nonblock, state);
+ *   - fs/io_uring.c|1676| <<__io_submit_sqe>> ret = io_write(req, s, force_nonblock, state);
+ *
+ * 处理IORING_OP_WRITEV或者IORING_OP_WRITE_FIXED
+ */
 static int io_write(struct io_kiocb *req, const struct sqe_submit *s,
 		    bool force_nonblock, struct io_submit_state *state)
 {
@@ -1060,6 +1383,10 @@ static int io_write(struct io_kiocb *req, const struct sqe_submit *s,
 /*
  * IORING_OP_NOP just posts a completion event, nothing else.
  */
+/*
+ * 处理IORING_OP_NOP:
+ *   - fs/io_uring.c|1660| <<__io_submit_sqe>> ret = io_nop(req, req->user_data);
+ */
 static int io_nop(struct io_kiocb *req, u64 user_data)
 {
 	struct io_ring_ctx *ctx = req->ctx;
@@ -1073,6 +1400,10 @@ static int io_nop(struct io_kiocb *req, u64 user_data)
 	return 0;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|1415| <<io_fsync>> ret = io_prep_fsync(req, sqe);
+ */
 static int io_prep_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 {
 	struct io_ring_ctx *ctx = req->ctx;
@@ -1337,6 +1668,11 @@ static int io_poll_add(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 	return ipt.error;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1769| <<io_sq_wq_submit_work>> ret = __io_submit_sqe(ctx, req, s, false, NULL);
+ *   - fs/io_uring.c|1935| <<io_submit_sqe>> ret = __io_submit_sqe(ctx, req, s, true, state);
+ */
 static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 			   const struct sqe_submit *s, bool force_nonblock,
 			   struct io_submit_state *state)
@@ -1345,6 +1681,9 @@ static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 
 	if (unlikely(s->index >= ctx->sq_entries))
 		return -EINVAL;
+	/*
+	 * req->user_data是u64 user_data;
+	 */
 	req->user_data = READ_ONCE(s->sqe->user_data);
 
 	opcode = READ_ONCE(s->sqe->opcode);
@@ -1415,6 +1754,11 @@ static struct async_list *io_async_list_from_sqe(struct io_ring_ctx *ctx,
 	}
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1777| <<io_sq_wq_submit_work>> if (io_sqe_needs_user(sqe) && !cur_mm) {
+ *   - fs/io_uring.c|2224| <<io_sq_thread>> if (all_fixed && io_sqe_needs_user(sqes[i].sqe))
+ */
 static inline bool io_sqe_needs_user(const struct io_uring_sqe *sqe)
 {
 	u8 opcode = READ_ONCE(sqe->opcode);
@@ -1423,6 +1767,10 @@ static inline bool io_sqe_needs_user(const struct io_uring_sqe *sqe)
 		 opcode == IORING_OP_WRITE_FIXED);
 }
 
+/*
+ * used by:
+ *   - fs/io_uring.c|1974| <<io_submit_sqe>> INIT_WORK(&req->work, io_sq_wq_submit_work);
+ */
 static void io_sq_wq_submit_work(struct work_struct *work)
 {
 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
@@ -1607,6 +1955,11 @@ static int io_req_set_file(struct io_ring_ctx *ctx, const struct sqe_submit *s,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2120| <<io_submit_sqes>> ret = io_submit_sqe(ctx, &sqes[i], statep);
+ *   - fs/io_uring.c|2285| <<io_ring_submit>> ret = io_submit_sqe(ctx, &s, statep);
+ */
 static int io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 			 struct io_submit_state *state)
 {
@@ -1625,6 +1978,9 @@ static int io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 	if (unlikely(ret))
 		goto out;
 
+	/*
+	 * 核心的函数!!!
+	 */
 	ret = __io_submit_sqe(ctx, req, s, true, state);
 	if (ret == -EAGAIN) {
 		struct io_uring_sqe *sqe_copy;
@@ -1668,6 +2024,13 @@ static int io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 /*
  * Batched submission is done, ensure local IO is flushed out.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|2139| <<io_submit_sqes>> io_submit_state_end(&state);
+ *   - fs/io_uring.c|2304| <<io_ring_submit>> io_submit_state_end(statep);
+ *
+ * 核心思想是blk_finish_plug(&state->plug);
+ */
 static void io_submit_state_end(struct io_submit_state *state)
 {
 	blk_finish_plug(&state->plug);
@@ -1680,6 +2043,13 @@ static void io_submit_state_end(struct io_submit_state *state)
 /*
  * Start submission side cache.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|2117| <<io_submit_sqes>> io_submit_state_start(&state, ctx, nr);
+ *   - fs/io_uring.c|2279| <<io_ring_submit>> io_submit_state_start(&state, ctx, to_submit);
+ *
+ * 核心思想是blk_start_plug(&state->plug);
+ */
 static void io_submit_state_start(struct io_submit_state *state,
 				  struct io_ring_ctx *ctx, unsigned max_ios)
 {
@@ -1689,6 +2059,13 @@ static void io_submit_state_start(struct io_submit_state *state,
 	state->ios_left = max_ios;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2262| <<io_sq_thread>> io_commit_sqring(ctx);
+ *   - fs/io_uring.c|2301| <<io_ring_submit>> io_commit_sqring(ctx);
+ *
+ * 将ctx->cached_sq_head同步到io_sq_ring->r.head
+ */
 static void io_commit_sqring(struct io_ring_ctx *ctx)
 {
 	struct io_sq_ring *ring = ctx->sq_ring;
@@ -1725,6 +2102,16 @@ static void io_drop_sqring(struct io_ring_ctx *ctx)
  * used, it's important that those reads are done through READ_ONCE() to
  * prevent a re-load down the line.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|2209| <<io_sq_thread>> if (!io_get_sqring(ctx, &sqes[0])) {
+ *   - fs/io_uring.c|2239| <<io_sq_thread>> if (!io_get_sqring(ctx, &sqes[0])) {
+ *   - fs/io_uring.c|2268| <<io_sq_thread>> } while (io_get_sqring(ctx, &sqes[i]));
+ *   - fs/io_uring.c|2307| <<io_ring_submit>> if (!io_get_sqring(ctx, &s))
+ *
+ * 核心思想, 取出head = ring->array[ctx->cached_sq_head & ctx->sq_mask]
+ * 把结果存入参数的sqe_submit, 增加ctx->cached_sq_head
+ */
 static bool io_get_sqring(struct io_ring_ctx *ctx, struct sqe_submit *s)
 {
 	struct io_sq_ring *ring = ctx->sq_ring;
@@ -1738,14 +2125,30 @@ static bool io_get_sqring(struct io_ring_ctx *ctx, struct sqe_submit *s)
 	 * 2) allows the kernel side to track the head on its own, even
 	 *    though the application is the one updating it.
 	 */
+	/*
+	 * ctx->cached_sq_head修改的地方:
+	 *   - fs/io_uring.c|2085| <<io_drop_sqring>> ctx->cached_sq_head--;
+	 *   - fs/io_uring.c|2119| <<io_get_sqring>> ctx->cached_sq_head++;
+	 *   - fs/io_uring.c|2124| <<io_get_sqring>> ctx->cached_sq_head++;
+	 *
+	 * 对于sq
+	 * user修改tail, kernel修改head
+	 */
 	head = ctx->cached_sq_head;
 	/* See comment at the top of this file */
 	smp_rmb();
+	/*
+	 * r的类型: struct io_uring r;
+	 */
 	if (head == READ_ONCE(ring->r.tail))
 		return false;
 
+	/*
+	 * array是最后的位置: u32 array[]
+	 */
 	head = READ_ONCE(ring->array[head & ctx->sq_mask]);
 	if (head < ctx->sq_entries) {
+		/* s是参数的struct sqe_submit *s */
 		s->index = head;
 		s->sqe = &ctx->sq_sqes[head];
 		ctx->cached_sq_head++;
@@ -1760,6 +2163,10 @@ static bool io_get_sqring(struct io_ring_ctx *ctx, struct sqe_submit *s)
 	return false;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2279| <<io_sq_thread>> inflight += io_submit_sqes(ctx, sqes, i, cur_mm != NULL,
+ */
 static int io_submit_sqes(struct io_ring_ctx *ctx, struct sqe_submit *sqes,
 			  unsigned int nr, bool has_user, bool mm_fault)
 {
@@ -1794,6 +2201,11 @@ static int io_submit_sqes(struct io_ring_ctx *ctx, struct sqe_submit *sqes,
 	return submitted;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2624| <<io_sq_offload_start>> ctx->sqo_thread = kthread_create_on_cpu(io_sq_thread,
+ *   - fs/io_uring.c|2628| <<io_sq_offload_start>> ctx->sqo_thread = kthread_create(io_sq_thread, ctx,
+ */
 static int io_sq_thread(void *data)
 {
 	struct sqe_submit sqes[IO_IOPOLL_BATCH];
@@ -1923,12 +2335,17 @@ static int io_sq_thread(void *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|3100| <<SYSCALL_DEFINE6(io_uring_enter)>> submitted = io_ring_submit(ctx, to_submit);
+ */
 static int io_ring_submit(struct io_ring_ctx *ctx, unsigned int to_submit)
 {
 	struct io_submit_state state, *statep = NULL;
 	int i, ret = 0, submit = 0;
 
 	if (to_submit > IO_PLUG_THRESHOLD) {
+		/* 核心思想是blk_start_plug(&state->plug); */
 		io_submit_state_start(&state, ctx, to_submit);
 		statep = &state;
 	}
@@ -1936,23 +2353,39 @@ static int io_ring_submit(struct io_ring_ctx *ctx, unsigned int to_submit)
 	for (i = 0; i < to_submit; i++) {
 		struct sqe_submit s;
 
+		/*
+		 * 核心思想, 取出head = ring->array[ctx->cached_sq_head & ctx->sq_mask]
+		 * 把结果存入参数的sqe_submit, 增加ctx->cached_sq_head
+		 */
 		if (!io_get_sqring(ctx, &s))
 			break;
 
+		/*
+		 * 此时
+		 * s.sqe = &ctx->sq_sqes[head]
+		 * s.index = READ_ONCE(ring->array[head & ctx->sq_mask])
+		 */
+
 		s.has_user = true;
 		s.needs_lock = false;
 		s.needs_fixed_file = false;
 
+		/*
+		 * 猜测这里是下发下去
+		 */
 		ret = io_submit_sqe(ctx, &s, statep);
 		if (ret) {
+			/* ctx->cached_sq_head-- */
 			io_drop_sqring(ctx);
 			break;
 		}
 
 		submit++;
 	}
+	/* 将ctx->cached_sq_head同步到io_sq_ring->r.head */
 	io_commit_sqring(ctx);
 
+	/* 核心思想是blk_finish_plug(&state->plug); */
 	if (statep)
 		io_submit_state_end(statep);
 
@@ -1968,6 +2401,10 @@ static unsigned io_cqring_events(struct io_cq_ring *ring)
  * Wait until events become available, if we don't already have some. The
  * application must reap them itself, as they reside on the shared cq ring.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|3146| <<SYSCALL_DEFINE6(io_uring_enter)>> ret = io_cqring_wait(ctx, min_complete, sig, sigsz);
+ */
 static int io_cqring_wait(struct io_ring_ctx *ctx, int min_events,
 			  const sigset_t __user *sig, size_t sigsz)
 {
@@ -2049,6 +2486,11 @@ static int io_sqe_files_unregister(struct io_ring_ctx *ctx)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2069| <<io_finish_async>> io_sq_thread_stop(ctx);
+ *   - fs/io_uring.c|2285| <<io_sq_offload_start>> io_sq_thread_stop(ctx);
+ */
 static void io_sq_thread_stop(struct io_ring_ctx *ctx)
 {
 	if (ctx->sqo_thread) {
@@ -2227,6 +2669,10 @@ static int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,
 	return ret;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|3301| <<io_uring_create>> ret = io_sq_offload_start(ctx, p);
+ */
 static int io_sq_offload_start(struct io_ring_ctx *ctx,
 			       struct io_uring_params *p)
 {
@@ -2327,12 +2773,22 @@ static void *io_mem_alloc(size_t size)
 	return (void *) __get_free_pages(gfp_flags, get_order(size));
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|3007| <<io_ring_ctx_free>> ring_pages(ctx->sq_entries, ctx->cq_entries));
+ *   - fs/io_uring.c|3291| <<io_uring_create>> ring_pages(p->sq_entries, p->cq_entries)); 
+ *   - fs/io_uring.c|3301| <<io_uring_create>> io_unaccount_mem(user, ring_pages(p->sq_entries,
+ */
 static unsigned long ring_pages(unsigned sq_entries, unsigned cq_entries)
 {
 	struct io_sq_ring *sq_ring;
 	struct io_cq_ring *cq_ring;
 	size_t bytes;
 
+	/*
+	 * array在io_sq_ring的类型: u32 array[] --> 在struct的最后
+	 * 分配sq_entries个u32(作为array)和sq_ring放在一起
+	 */
 	bytes = struct_size(sq_ring, array, sq_entries);
 	bytes += array_size(sizeof(struct io_uring_sqe), sq_entries);
 	bytes += struct_size(cq_ring, cqes, cq_entries);
@@ -2340,6 +2796,12 @@ static unsigned long ring_pages(unsigned sq_entries, unsigned cq_entries)
 	return (bytes + PAGE_SIZE - 1) / PAGE_SIZE;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2977| <<io_sqe_buffer_register>> io_sqe_buffer_unregister(ctx);
+ *   - fs/io_uring.c|2992| <<io_ring_ctx_free>> io_sqe_buffer_unregister(ctx);
+ *   - fs/io_uring.c|3402| <<__io_uring_register>> ret = io_sqe_buffer_unregister(ctx);
+ */
 static int io_sqe_buffer_unregister(struct io_ring_ctx *ctx)
 {
 	int i, j;
@@ -2365,6 +2827,10 @@ static int io_sqe_buffer_unregister(struct io_ring_ctx *ctx)
 	return 0;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|2880| <<io_sqe_buffer_register>> ret = io_copy_iov(ctx, &iov, arg, i);
+ */
 static int io_copy_iov(struct io_ring_ctx *ctx, struct iovec *dst,
 		       void __user *arg, unsigned index)
 {
@@ -2390,6 +2856,10 @@ static int io_copy_iov(struct io_ring_ctx *ctx, struct iovec *dst,
 	return 0;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|3412| <<__io_uring_register>> ret = io_sqe_buffer_register(ctx, arg, nr_args);
+ */
 static int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,
 				  unsigned nr_args)
 {
@@ -2532,6 +3002,10 @@ static int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,
 	return ret;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|2594| <<io_ring_ctx_wait_and_kill>> io_ring_ctx_free(ctx);
+ */
 static void io_ring_ctx_free(struct io_ring_ctx *ctx)
 {
 	io_finish_async(ctx);
@@ -2559,6 +3033,9 @@ static void io_ring_ctx_free(struct io_ring_ctx *ctx)
 	kfree(ctx);
 }
 
+/*
+ * struct file_operations io_uring_fops.poll = io_uring_poll()
+ */
 static __poll_t io_uring_poll(struct file *file, poll_table *wait)
 {
 	struct io_ring_ctx *ctx = file->private_data;
@@ -2575,6 +3052,9 @@ static __poll_t io_uring_poll(struct file *file, poll_table *wait)
 	return mask;
 }
 
+/*
+ * struct file_operations io_uring_fops.fasync = io_uring_fasync()
+ */
 static int io_uring_fasync(int fd, struct file *file, int on)
 {
 	struct io_ring_ctx *ctx = file->private_data;
@@ -2582,6 +3062,11 @@ static int io_uring_fasync(int fd, struct file *file, int on)
 	return fasync_helper(fd, file, on, &ctx->cq_fasync);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2616| <<io_uring_release>> io_ring_ctx_wait_and_kill(ctx);
+ *   - fs/io_uring.c|2895| <<io_uring_create>> io_ring_ctx_wait_and_kill(ctx);
+ */
 static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)
 {
 	mutex_lock(&ctx->uring_lock);
@@ -2594,6 +3079,9 @@ static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)
 	io_ring_ctx_free(ctx);
 }
 
+/*
+ * struct file_operations io_uring_fops.release = io_uring_release()
+ */
 static int io_uring_release(struct inode *inode, struct file *file)
 {
 	struct io_ring_ctx *ctx = file->private_data;
@@ -2603,6 +3091,9 @@ static int io_uring_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * struct file_operations io_uring_fops.mmap = io_uring_mmap()
+ */
 static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	loff_t offset = (loff_t) vma->vm_pgoff << PAGE_SHIFT;
@@ -2614,12 +3105,15 @@ static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
 
 	switch (offset) {
 	case IORING_OFF_SQ_RING:
+		/* 类型是struct io_sq_ring *sq_ring; */
 		ptr = ctx->sq_ring;
 		break;
 	case IORING_OFF_SQES:
+		/* 类型是struct io_uring_sqe *sq_sqes; */
 		ptr = ctx->sq_sqes;
 		break;
 	case IORING_OFF_CQ_RING:
+		/* 类型是struct io_cq_ring *cq_ring; */
 		ptr = ctx->cq_ring;
 		break;
 	default:
@@ -2631,9 +3125,15 @@ static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
 		return -EINVAL;
 
 	pfn = virt_to_phys(ptr) >> PAGE_SHIFT;
+	/* remap kernel memory to userspace */
 	return remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);
 }
 
+/*
+ * If flags contains IORING_ENTER_GETEVENTS and min_complete is nonzero,
+ * io_uring_enter() will block until at least that many operations have
+ * completed.
+ */
 SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
 		u32, min_complete, u32, flags, const sigset_t __user *, sig,
 		size_t, sigsz)
@@ -2655,6 +3155,13 @@ SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
 		goto out_fput;
 
 	ret = -ENXIO;
+	/*
+	 * io_allocate_scq_urings()执行后:
+	 * struct io_ring_ctx
+	 *   struct io_sq_ring    *sq_ring; --> 指向struct io_sq_ring + 一组u32 array[]
+	 *   struct io_uring_sqe  *sq_sqes; --> 指向一组struct io_uring_sqe[]
+	 *   struct io_cq_ring    *cq_ring; --> 指向struct io_cq_ring + 一组struct io_uring_cqe[]
+	 */
 	ctx = f.file->private_data;
 	if (!percpu_ref_tryget(&ctx->refs))
 		goto out_fput;
@@ -2665,6 +3172,17 @@ SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
 	 * we were asked to.
 	 */
 	if (ctx->flags & IORING_SETUP_SQPOLL) {
+		/*
+		 * sqo_wait在以下使用:
+		 *   - fs/io_uring.c|587| <<io_cqring_ev_posted>> if (waitqueue_active(&ctx->sqo_wait))
+		 *   - fs/io_uring.c|588| <<io_cqring_ev_posted>> wake_up(&ctx->sqo_wait);
+		 *   - fs/io_uring.c|2077| <<io_sq_thread>> prepare_to_wait(&ctx->sqo_wait, &wait,
+		 *   - fs/io_uring.c|2086| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+		 *   - fs/io_uring.c|2092| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+		 *   - fs/io_uring.c|2098| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+		 *   - fs/io_uring.c|2448| <<io_sq_offload_start>> init_waitqueue_head(&ctx->sqo_wait);
+		 *   - fs/io_uring.c|2891| <<SYSCALL_DEFINE6>> wake_up(&ctx->sqo_wait);
+		 */
 		if (flags & IORING_ENTER_SQ_WAKEUP)
 			wake_up(&ctx->sqo_wait);
 		submitted = to_submit;
@@ -2682,6 +3200,11 @@ SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
 		if (submitted < 0)
 			goto out_ctx;
 	}
+	/*
+	 * If flags contains IORING_ENTER_GETEVENTS and min_complete is
+	 * nonzero, io_uring_enter() will block until at least that many
+	 * operations have completed.
+	 */
 	if (flags & IORING_ENTER_GETEVENTS) {
 		unsigned nr_events = 0;
 
@@ -2701,6 +3224,9 @@ SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
 			ret = io_iopoll_check(ctx, &nr_events, min_complete);
 			mutex_unlock(&ctx->uring_lock);
 		} else {
+			/*
+			 * sig和sigsz都是用户空间通过系统调用传下来的
+			 */
 			ret = io_cqring_wait(ctx, min_complete, sig, sigsz);
 		}
 	}
@@ -2719,6 +3245,18 @@ static const struct file_operations io_uring_fops = {
 	.fasync		= io_uring_fasync,
 };
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|3381| <<io_uring_create>> ret = io_allocate_scq_urings(ctx, p);
+ *
+ * io_allocate_scq_urings()执行后:
+ * struct io_ring_ctx
+ *   struct io_sq_ring    *sq_ring; --> 指向struct io_sq_ring + 一组u32 array[]
+ *   struct io_uring_sqe  *sq_sqes; --> 指向一组struct io_uring_sqe[]
+ *   struct io_cq_ring    *cq_ring; --> 指向struct io_cq_ring + 一组struct io_uring_cqe[]
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)-->io_uring_setup()-->io_uring_create()-->io_allocate_scq_urings()
+ */
 static int io_allocate_scq_urings(struct io_ring_ctx *ctx,
 				  struct io_uring_params *p)
 {
@@ -2726,6 +3264,10 @@ static int io_allocate_scq_urings(struct io_ring_ctx *ctx,
 	struct io_cq_ring *cq_ring;
 	size_t size;
 
+	/*
+	 * array在io_sq_ring的类型: u32 array[] --> 在struct的最后
+	 * 分配sq_entries个u32(作为array)和sq_ring放在一起
+	 */
 	sq_ring = io_mem_alloc(struct_size(sq_ring, array, p->sq_entries));
 	if (!sq_ring)
 		return -ENOMEM;
@@ -2740,12 +3282,20 @@ static int io_allocate_scq_urings(struct io_ring_ctx *ctx,
 	if (size == SIZE_MAX)
 		return -EOVERFLOW;
 
+	/*
+	 * 类型是struct io_uring_sqe *sq_sqes
+	 * 指向的是p->sq_entries=sq_ring->ring_entries=ctx->sq_entries个'struct io_uring_sqe'
+	 */
 	ctx->sq_sqes = io_mem_alloc(size);
 	if (!ctx->sq_sqes) {
 		io_mem_free(ctx->sq_ring);
 		return -ENOMEM;
 	}
 
+	/*
+	 * cqes在struct io_cq_ring类型的cq_ring中的定义是: struct io_uring_cqe cqes[];
+	 * 这里在分配cq_ring的时候一起分配了struct io_cq_ring + 一组struct io_uring_cqe[]
+	 */
 	cq_ring = io_mem_alloc(struct_size(cq_ring, cqes, p->cq_entries));
 	if (!cq_ring) {
 		io_mem_free(ctx->sq_ring);
@@ -2767,6 +3317,12 @@ static int io_allocate_scq_urings(struct io_ring_ctx *ctx,
  * fd to gain access to the SQ/CQ ring details. If UNIX sockets are enabled,
  * we have to tie this fd to a socket for file garbage collection purposes.
  */
+/*
+ * called by only:
+ *   - fs/io_uring.c|3389| <<io_uring_create>> ret = io_uring_get_fd(ctx);
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)-->io_uring_setup()-->io_uring_create()-->io_uring_get_fd()
+ */
 static int io_uring_get_fd(struct io_ring_ctx *ctx)
 {
 	struct file *file;
@@ -2805,6 +3361,12 @@ static int io_uring_get_fd(struct io_ring_ctx *ctx)
 	return ret;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|3040| <<io_uring_setup>> ret = io_uring_create(entries, &p);
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)-->io_uring_setup()-->io_uring_create()
+ */
 static int io_uring_create(unsigned entries, struct io_uring_params *p)
 {
 	struct user_struct *user = NULL;
@@ -2821,6 +3383,7 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p)
 	 * since the sqes are only used at submission time. This allows for
 	 * some flexibility in overcommitting a bit.
 	 */
+	/* round the given value up to nearest power of two */
 	p->sq_entries = roundup_pow_of_two(entries);
 	p->cq_entries = 2 * p->sq_entries;
 
@@ -2836,6 +3399,9 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p)
 		}
 	}
 
+	/*
+	 * 分配和简单初始化io_ring_ctx
+	 */
 	ctx = io_ring_ctx_alloc(p);
 	if (!ctx) {
 		if (account_mem)
@@ -2848,18 +3414,36 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p)
 	ctx->account_mem = account_mem;
 	ctx->user = user;
 
+	/*
+	 * 分配sq和cq的ring buffer
+	 *
+	 * 该函数执行后:
+	 * struct io_ring_ctx
+	 *   struct io_sq_ring    *sq_ring; --> 指向struct io_sq_ring + 一组u32 array[]
+	 *   struct io_uring_sqe  *sq_sqes; --> 指向一组struct io_uring_sqe[]
+	 *   struct io_cq_ring    *cq_ring; --> 指向struct io_cq_ring + 一组struct io_uring_cqe[]
+	 */
 	ret = io_allocate_scq_urings(ctx, p);
 	if (ret)
 		goto err;
 
+	/*
+	 * 核心思想是根据相关的情况创建kthread和workqueue
+	 */
 	ret = io_sq_offload_start(ctx, p);
 	if (ret)
 		goto err;
 
+	/*
+	 * 获取一个&io_uring_fops的fd
+	 */
 	ret = io_uring_get_fd(ctx);
 	if (ret < 0)
 		goto err;
 
+	/*
+	 * 这里是一些offset, 是在io_sq_ring中的offset
+	 */
 	memset(&p->sq_off, 0, sizeof(p->sq_off));
 	p->sq_off.head = offsetof(struct io_sq_ring, r.head);
 	p->sq_off.tail = offsetof(struct io_sq_ring, r.tail);
@@ -2867,14 +3451,23 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p)
 	p->sq_off.ring_entries = offsetof(struct io_sq_ring, ring_entries);
 	p->sq_off.flags = offsetof(struct io_sq_ring, flags);
 	p->sq_off.dropped = offsetof(struct io_sq_ring, dropped);
+	/*
+	 * 在io_allocate_scq_urings()中分配的array在struct io_sq_ring的最后面
+	 */
 	p->sq_off.array = offsetof(struct io_sq_ring, array);
 
+	/*
+	 * 这里是一些offset, 是在io_sq_ring中的offset
+	 */
 	memset(&p->cq_off, 0, sizeof(p->cq_off));
 	p->cq_off.head = offsetof(struct io_cq_ring, r.head);
 	p->cq_off.tail = offsetof(struct io_cq_ring, r.tail);
 	p->cq_off.ring_mask = offsetof(struct io_cq_ring, ring_mask);
 	p->cq_off.ring_entries = offsetof(struct io_cq_ring, ring_entries);
 	p->cq_off.overflow = offsetof(struct io_cq_ring, overflow);
+	/*
+	 * 在io_allocate_scq_urings()中分配的cqes在struct io_cq_ring的最后面
+	 */
 	p->cq_off.cqes = offsetof(struct io_cq_ring, cqes);
 	return ret;
 err:
@@ -2887,6 +3480,41 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p)
  * ring size, we return the actual sq/cq ring sizes (among other things) in the
  * params structure passed in.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|3075| <<SYSCALL_DEFINE2(io_uring_setup)>> return io_uring_setup(entries, params);
+ *
+ * The return value from io_uring_setup() is a file descriptor that can
+ * then be passed to mmap() to map the buffer into the process's address
+ * space.
+ *
+ * More specifically, three calls are needed to map:
+ * 1. the two ring buffers and
+ * 2. an array of submission-queue entries;
+ *
+ * io_uring_create()执行后:
+ * struct io_ring_ctx
+ *   struct io_sq_ring    *sq_ring; --> 指向struct io_sq_ring + 一组u32 array[]
+ *   struct io_uring_sqe  *sq_sqes; --> 指向一组struct io_uring_sqe[]
+ *   struct io_cq_ring    *cq_ring; --> 指向struct io_cq_ring + 一组struct io_uring_cqe[]
+ *
+ * the information needed to do this mapping will be found in the sq_off
+ * and cq_off fields of the io_uring_params structure.
+ *
+ * 用户空间需要调用:
+ *
+ * subqueue = mmap(0, params.sq_off.array + params.sq_entries*sizeof(__u32),
+ *                 PROT_READ|PROT_WRITE|MAP_SHARED|MAP_POPULATE,
+ *                 ring_fd, IORING_OFF_SQ_RING);
+ *
+ * sqentries = mmap(0, params.sq_entries*sizeof(struct io_uring_sqe),
+ *                  PROT_READ|PROT_WRITE|MAP_SHARED|MAP_POPULATE,
+ *                  ring_fd, IORING_OFF_SQES);
+ *
+ * cqentries = mmap(0, params.cq_off.cqes + params.cq_entries*sizeof(struct io_uring_cqe),
+ *                  PROT_READ|PROT_WRITE|MAP_SHARED|MAP_POPULATE,
+ *                  ring_fd, IORING_OFF_CQ_RING);
+ */
 static long io_uring_setup(u32 entries, struct io_uring_params __user *params)
 {
 	struct io_uring_params p;
@@ -2904,6 +3532,9 @@ static long io_uring_setup(u32 entries, struct io_uring_params __user *params)
 			IORING_SETUP_SQ_AFF))
 		return -EINVAL;
 
+	/*
+	 * 只看这一个函数就可以了
+	 */
 	ret = io_uring_create(entries, &p);
 	if (ret < 0)
 		return ret;
@@ -2920,6 +3551,10 @@ SYSCALL_DEFINE2(io_uring_setup, u32, entries,
 	return io_uring_setup(entries, params);
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|3632| <<SYSCALL_DEFINE4(io_uring_register)>> ret = __io_uring_register(ctx, opcode, arg, nr_args);
+ */
 static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 			       void __user *arg, unsigned nr_args)
 {
@@ -2958,6 +3593,28 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 	return ret;
 }
 
+/*
+ * There is ability to map a program's I/O buffers into the kernel. This
+ * mapping normally happens with each I/O operation so that data can be
+ * copied into or out of the buffers; the buffers are unmapped when the
+ * operation completes. If the buffers will be used many times over the
+ * course of the program's execution, it is far more efficient to map them
+ * once and leave them in place. This mapping is done by filling in yet
+ * another structure describing the buffers to be mapped: 
+ *
+ * In this case, the opcode should be IORING_REGISTER_BUFFERS. The buffers
+ * will remain mapped for as long as the initial file descriptor remains
+ * open, unless the program explicitly unmaps them with
+ * IORING_UNREGISTER_BUFFERS. Mapping buffers in this way is essentially
+ * locking memory into RAM, so the usual resource limit that applies to
+ * mlock() applies here as well. When performing I/O to premapped buffers,
+ * the IORING_OP_READ_FIXED and IORING_OP_WRITE_FIXED operations should be
+ * used.
+ *
+ * There is also an IORING_REGISTER_FILES operation that can be used to
+ * optimize situations where many operations will be performed on the same
+ * file(s).
+ */
 SYSCALL_DEFINE4(io_uring_register, unsigned int, fd, unsigned int, opcode,
 		void __user *, arg, unsigned int, nr_args)
 {
@@ -2989,3 +3646,10 @@ static int __init io_uring_init(void)
 	return 0;
 };
 __initcall(io_uring_init);
+
+/*
+ * 一共3个系统调用:
+ *   - SYSCALL_DEFINE6(io_uring_enter)
+ *   - SYSCALL_DEFINE2(io_uring_setup)
+ *   - SYSCALL_DEFINE4(io_uring_register)
+ */
diff --git a/include/linux/percpu-refcount.h b/include/linux/percpu-refcount.h
index b297cd1..e1e686c 100644
--- a/include/linux/percpu-refcount.h
+++ b/include/linux/percpu-refcount.h
@@ -76,12 +76,24 @@ enum {
 	 * with this flag, the ref will stay in atomic mode until
 	 * percpu_ref_switch_to_percpu() is invoked on it.
 	 */
+	/*
+	 * used by:
+	 *   - block/blk-core.c|533| <<blk_alloc_queue_node>> PERCPU_REF_INIT_ATOMIC, GFP_KERNEL))
+	 *   - drivers/infiniband/sw/rdmavt/mr.c|735| <<rvt_alloc_fmr>> PERCPU_REF_INIT_ATOMIC);
+	 *   - lib/percpu-refcount.c|105| <<percpu_ref_init>> ref->force_atomic = flags & PERCPU_REF_INIT_ATOMIC;
+	 *   - lib/percpu-refcount.c|107| <<percpu_ref_init>> if (flags & (PERCPU_REF_INIT_ATOMIC | PERCPU_REF_INIT_DEAD))
+	 */
 	PERCPU_REF_INIT_ATOMIC	= 1 << 0,
 
 	/*
 	 * Start dead w/ ref == 0 in atomic mode.  Must be revived with
 	 * percpu_ref_reinit() before used.  Implies INIT_ATOMIC.
 	 */
+	/*
+	 * called by:
+	 *   - lib/percpu-refcount.c|107| <<percpu_ref_init>> if (flags & (PERCPU_REF_INIT_ATOMIC | PERCPU_REF_INIT_DEAD))
+	 *   - lib/percpu-refcount.c|112| <<percpu_ref_init>> if (flags & PERCPU_REF_INIT_DEAD)
+	 */
 	PERCPU_REF_INIT_DEAD	= 1 << 1,
 };
 
@@ -123,8 +135,36 @@ void percpu_ref_reinit(struct percpu_ref *ref);
  *
  * There are no implied RCU grace periods between kill and release.
  */
+/*
+ * called by:
+ *   - block/blk-cgroup.c|415| <<blkg_destroy>> percpu_ref_kill(&blkg->refcnt);
+ *   - block/blk-mq.c|150| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+ *   - drivers/dax/device.c|46| <<dev_dax_percpu_kill>> percpu_ref_kill(ref);
+ *   - drivers/infiniband/sw/rdmavt/mr.c|275| <<rvt_free_lkey>> percpu_ref_kill(&mr->refcount);
+ *   - drivers/nvme/target/core.c|585| <<nvmet_ns_disable>> percpu_ref_kill(&ns->ref);
+ *   - drivers/pci/p2pdma.c|96| <<pci_p2pdma_percpu_kill>> percpu_ref_kill(ref);
+ *   - drivers/target/target_core_transport.c|2915| <<target_sess_cmd_list_set_waiting>> percpu_ref_kill(&se_sess->cmd_count);
+ *   - drivers/target/target_core_transport.c|2947| <<transport_clear_lun_ref>> percpu_ref_kill(&lun->lun_ref);
+ *   - fs/aio.c|631| <<free_ioctx_users>> percpu_ref_kill(&ctx->reqs);
+ *   - fs/aio.c|850| <<kill_ioctx>> percpu_ref_kill(&ctx->users);
+ *   - fs/io_uring.c|2602| <<io_ring_ctx_wait_and_kill>> percpu_ref_kill(&ctx->refs);
+ *   - fs/io_uring.c|2942| <<__io_uring_register>> percpu_ref_kill(&ctx->refs);
+ *   - include/linux/genhd.h|696| <<hd_struct_kill>> percpu_ref_kill(&part->ref);
+ *   - kernel/cgroup/cgroup.c|2173| <<cgroup_kill_sb>> percpu_ref_kill(&root->cgrp.self.refcnt);
+ *   - kernel/cgroup/cgroup.c|5302| <<__acquires>> percpu_ref_kill(&cgrp->self.refcnt);
+ *   - mm/backing-dev.c|526| <<cgwb_kill>> percpu_ref_kill(&wb->refcnt);
+ *   - mm/hmm.c|990| <<hmm_devmem_ref_kill>> percpu_ref_kill(ref);
+ *
+ * 调用的时候ref->percpu_count_ptr不能已经dead了
+ * 设置ref->percpu_count_ptr的dead, 调用__percpu_ref_switch_to_percpu()
+ * (confirm_kill是NULL)
+ * 最后put一下percpu_ref_put(ref)
+ */
 static inline void percpu_ref_kill(struct percpu_ref *ref)
 {
+	/*
+	 * 调用的时候ref->percpu_count_ptr不能已经dead了
+	 */
 	percpu_ref_kill_and_confirm(ref, NULL);
 }
 
@@ -134,6 +174,12 @@ static inline void percpu_ref_kill(struct percpu_ref *ref)
  * because doing so forces the compiler to generate two conditional
  * branches as it can't assume that @ref->percpu_count is not NULL.
  */
+/*
+ * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+ * 则返回false
+ * 否则更新参数为ref->percpu_count_ptr
+ * 并且返回true
+ */
 static inline bool __ref_is_percpu(struct percpu_ref *ref,
 					  unsigned long __percpu **percpu_countp)
 {
@@ -174,12 +220,24 @@ static inline bool __ref_is_percpu(struct percpu_ref *ref,
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * 对于percpu的(非atomic的)不会增加ref->count
+ *
+ * 对于atomic的或者dead的, atomic_long_add(nr, &ref->count);
+ * 对于percpu的, atomic_long_add(nr, &ref->count);
+ */
 static inline void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
 {
 	unsigned long __percpu *percpu_count;
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	if (__ref_is_percpu(ref, &percpu_count))
 		this_cpu_add(*percpu_count, nr);
 	else
@@ -196,6 +254,12 @@ static inline void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * 对于percpu的(非atomic的)不会增加ref->count
+ *
+ * 对于atomic的或者dead的, atomic_long_add(nr, &ref->count);
+ * 对于percpu的, atomic_long_add(nr, &ref->count);
+ */
 static inline void percpu_ref_get(struct percpu_ref *ref)
 {
 	percpu_ref_get_many(ref, 1);
@@ -210,6 +274,20 @@ static inline void percpu_ref_get(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|391| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+ *   - block/blk-mq.c|939| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+ *   - fs/io_uring.c|415| <<io_get_req>> if (!percpu_ref_tryget(&ctx->refs))
+ *   - fs/io_uring.c|2678| <<SYSCALL_DEFINE6>> if (!percpu_ref_tryget(&ctx->refs))
+ *   - include/linux/backing-dev-defs.h|242| <<wb_tryget>> return percpu_ref_tryget(&wb->refcnt);
+ *   - include/linux/blk-cgroup.h|502| <<blkg_tryget>> return blkg && percpu_ref_tryget(&blkg->refcnt);
+ *   - include/linux/cgroup.h|340| <<css_tryget>> return percpu_ref_tryget(&css->refcnt);
+ *
+ * 如果是percpu的直接this_cpu_inc(*percpu_count)并且返回true
+ * 否则用atomic_long_inc_not_zero(&ref->count)只有在ref->count不为0的时候才增加
+ * 如果之前为0则返回0
+ */
 static inline bool percpu_ref_tryget(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
@@ -217,6 +295,12 @@ static inline bool percpu_ref_tryget(struct percpu_ref *ref)
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 调用的时候ref->percpu_count_ptr不能已经dead了
+	 * 设置ref->percpu_count_ptr的dead, 调用__percpu_ref_switch_to_percpu()
+	 * (confirm_kill是NULL)
+	 * 最后put一下percpu_ref_put(ref)
+	 */
 	if (__ref_is_percpu(ref, &percpu_count)) {
 		this_cpu_inc(*percpu_count);
 		ret = true;
@@ -279,6 +363,12 @@ static inline void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	if (__ref_is_percpu(ref, &percpu_count))
 		this_cpu_sub(*percpu_count, nr);
 	else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index e234086..e69d972 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -37,16 +37,67 @@ struct io_uring_sqe {
 /*
  * sqe->flags
  */
+/*
+ * used by:
+ *   - fs/io_uring.c|1941| <<io_req_set_file>> if (flags & IOSQE_FIXED_FILE) {
+ *   - fs/io_uring.c|1970| <<io_submit_sqe>> if (unlikely(s->sqe->flags & ~IOSQE_FIXED_FILE))
+ */
 #define IOSQE_FIXED_FILE	(1U << 0)	/* use fixed fileset */
 
 /*
  * io_uring_setup() flags
  */
+/*
+ * used by:
+ *   - fs/io_uring.c|591| <<io_iopoll_reap_events>> if (!(ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|793| <<io_prep_rw>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|1078| <<io_nop>> if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1096| <<io_prep_fsync>> if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1172| <<io_poll_remove>> if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1293| <<io_poll_add>> if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1398| <<__io_submit_sqe>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|1828| <<io_sq_thread>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|2718| <<SYSCALL_DEFINE6(io_uring_enter)>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|2922| <<io_uring_setup>> if (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |
+ */
 #define IORING_SETUP_IOPOLL	(1U << 0)	/* io_context is polled */
+/*
+ * used by:
+ *   - fs/io_uring.c|2257| <<io_sq_offload_start>> if (ctx->flags & IORING_SETUP_SQPOLL) {
+ *   - fs/io_uring.c|2686| <<SYSCALL_DEFINE6>> if (ctx->flags & IORING_SETUP_SQPOLL) {
+ *   - fs/io_uring.c|2922| <<io_uring_setup>> if (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |
+ *
+ * There is also a fully polled mode that (almost) eliminates the need to
+ * make any system calls at all. This mode is enabled by setting the
+ * IORING_SETUP_SQPOLL flag at ring setup time. A call to io_uring_enter()
+ * will kick off a kernel thread that will occasionally poll the submission
+ * queue and automatically submit any requests found there; receive-queue
+ * polling is also performed if it has been requested. As long as the
+ * application continues to submit I/O and consume the results, I/O will
+ * happen with no further system calls.
+ *
+ * Eventually, though (after one second currently), the kernel will get
+ * bored if no new requests are submitted and the polling will stop. When
+ * that happens, the flags field in the submission queue structure will
+ * have the IORING_SQ_NEED_WAKEUP bit set. The application should check
+ * for this bit and, if it is set, make a new call to io_uring_enter() to
+ * start the mechanism up again.
+ */
 #define IORING_SETUP_SQPOLL	(1U << 1)	/* SQ poll thread */
+/*
+ * used by:
+ *   - fs/io_uring.c|2262| <<io_sq_offload_start>> if (p->flags & IORING_SETUP_SQ_AFF) {
+ *   - fs/io_uring.c|2279| <<io_sq_offload_start>> } else if (p->flags & IORING_SETUP_SQ_AFF) {
+ *   - fs/io_uring.c|2923| <<io_uring_setup>> IORING_SETUP_SQ_AFF))
+ */
 #define IORING_SETUP_SQ_AFF	(1U << 2)	/* sq_thread_cpu is valid */
 
 #define IORING_OP_NOP		0
+/*
+ * used by:
+ *   - fs/io_uring.c|1694| <<__io_submit_sqe>> case IORING_OP_READV:
+ *   - fs/io_uring.c|1746| <<io_async_list_from_sqe>> case IORING_OP_READV:
+ */
 #define IORING_OP_READV		1
 #define IORING_OP_WRITEV	2
 #define IORING_OP_FSYNC		3
@@ -58,6 +109,11 @@ struct io_uring_sqe {
 /*
  * sqe->fsync_flags
  */
+/*
+ * used by:
+ *   - fs/io_uring.c|1115| <<io_fsync>> if (unlikely(fsync_flags & ~IORING_FSYNC_DATASYNC))
+ *   - fs/io_uring.c|1128| <<io_fsync>> fsync_flags & IORING_FSYNC_DATASYNC);
+ */
 #define IORING_FSYNC_DATASYNC	(1U << 0)
 
 /*
@@ -72,13 +128,39 @@ struct io_uring_cqe {
 /*
  * Magic offsets for the application to mmap the data it needs
  */
+/*
+ * two ring buffers and an array of submission-queue entries
+ */
+/*
+ * used by:
+ *   - fs/io_uring.c|3107| <<io_uring_mmap>> case IORING_OFF_SQ_RING:
+ */
 #define IORING_OFF_SQ_RING		0ULL
+/*
+ * used by:
+ *   - fs/io_uring.c|3115| <<io_uring_mmap>> case IORING_OFF_CQ_RING:
+ */
 #define IORING_OFF_CQ_RING		0x8000000ULL
+/*
+ * used by:
+ *   - fs/io_uring.c|3111| <<io_uring_mmap>> case IORING_OFF_SQES:
+ */
 #define IORING_OFF_SQES			0x10000000ULL
 
 /*
  * Filled with the offset for mmap(2)
  */
+/*
+ * 在以下修改:
+ *  fs/io_uring.c|3031| <<io_uring_create>> memset(&p->sq_off, 0, sizeof(p->sq_off));
+ *  fs/io_uring.c|3032| <<io_uring_create>> p->sq_off.head = offsetof(struct io_sq_ring, r.head);
+ *  fs/io_uring.c|3033| <<io_uring_create>> p->sq_off.tail = offsetof(struct io_sq_ring, r.tail);
+ *  fs/io_uring.c|3034| <<io_uring_create>> p->sq_off.ring_mask = offsetof(struct io_sq_ring, ring_mask);
+ *  fs/io_uring.c|3035| <<io_uring_create>> p->sq_off.ring_entries = offsetof(struct io_sq_ring, ring_entries);
+ *  fs/io_uring.c|3036| <<io_uring_create>> p->sq_off.flags = offsetof(struct io_sq_ring, flags);
+ *  fs/io_uring.c|3037| <<io_uring_create>> p->sq_off.dropped = offsetof(struct io_sq_ring, dropped);
+ *  fs/io_uring.c|3038| <<io_uring_create>> p->sq_off.array = offsetof(struct io_sq_ring, array);
+ */
 struct io_sqring_offsets {
 	__u32 head;
 	__u32 tail;
@@ -94,8 +176,24 @@ struct io_sqring_offsets {
 /*
  * sq_ring->flags
  */
+/*
+ * used by:
+ *   - fs/io_uring.c|2280| <<io_sq_thread>> ctx->sq_ring->flags |= IORING_SQ_NEED_WAKEUP;
+ *   - fs/io_uring.c|2293| <<io_sq_thread>> ctx->sq_ring->flags &= ~IORING_SQ_NEED_WAKEUP;
+ *   - fs/io_uring.c|2299| <<io_sq_thread>> ctx->sq_ring->flags &= ~IORING_SQ_NEED_WAKEUP;
+ */
 #define IORING_SQ_NEED_WAKEUP	(1U << 0) /* needs io_uring_enter wakeup */
 
+/*
+ * 在以下设置:
+ *  fs/io_uring.c|3040| <<io_uring_create>> memset(&p->cq_off, 0, sizeof(p->cq_off));
+ *  fs/io_uring.c|3041| <<io_uring_create>> p->cq_off.head = offsetof(struct io_cq_ring, r.head);
+ *  fs/io_uring.c|3042| <<io_uring_create>> p->cq_off.tail = offsetof(struct io_cq_ring, r.tail);
+ *  fs/io_uring.c|3043| <<io_uring_create>> p->cq_off.ring_mask = offsetof(struct io_cq_ring, ring_mask);
+ *  fs/io_uring.c|3044| <<io_uring_create>> p->cq_off.ring_entries = offsetof(struct io_cq_ring, ring_entries);
+ *  fs/io_uring.c|3045| <<io_uring_create>> p->cq_off.overflow = offsetof(struct io_cq_ring, overflow);
+ *  fs/io_uring.c|3046| <<io_uring_create>> p->cq_off.cqes = offsetof(struct io_cq_ring, cqes);
+ */
 struct io_cqring_offsets {
 	__u32 head;
 	__u32 tail;
@@ -109,20 +207,59 @@ struct io_cqring_offsets {
 /*
  * io_uring_enter(2) flags
  */
-#define IORING_ENTER_GETEVENTS	(1U << 0)
+/*
+ * used by (主要是第二个):
+ *   - fs/io_uring.c|3146| <<SYSCALL_DEFINE6(io_uring_enter)>> if (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP))
+ *   - fs/io_uring.c|3192| <<SYSCALL_DEFINE6(io_uring_enter)>> if (flags & IORING_ENTER_GETEVENTS) {
+ */
+#define IOING_ENTER_GETEVENTS	(1U << 0)
+/*
+ * used by (主要是第二个):
+ *   - fs/io_uring.c|3146| <<SYSCALL_DEFINE6(io_uring_enter)>> if (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP))
+ *   - fs/io_uring.c|3175| <<SYSCALL_DEFINE6(io_uring_enter)>> if (flags & IORING_ENTER_SQ_WAKEUP)
+ */
 #define IORING_ENTER_SQ_WAKEUP	(1U << 1)
 
 /*
  * Passed in for io_uring_setup(2). Copied back with updated info on success
  */
 struct io_uring_params {
+	/*
+	 * 在以下设置:
+	 *   - fs/io_uring.c|3387| <<io_uring_create>> p->sq_entries = roundup_pow_of_two(entries);
+	 */
 	__u32 sq_entries;
+	/*
+	 * 在以下设置:
+	 *   - fs/io_uring.c|3388| <<io_uring_create>> p->cq_entries = 2 * p->sq_entries;
+	 */
 	__u32 cq_entries;
 	__u32 flags;
 	__u32 sq_thread_cpu;
 	__u32 sq_thread_idle;
 	__u32 resv[5];
+	/*
+	 * 在以下设置:
+	 *   - fs/io_uring.c|3447| <<io_uring_create>> memset(&p->sq_off, 0, sizeof(p->sq_off));
+	 *   - fs/io_uring.c|3448| <<io_uring_create>> p->sq_off.head = offsetof(struct io_sq_ring, r.head);
+	 *   - fs/io_uring.c|3449| <<io_uring_create>> p->sq_off.tail = offsetof(struct io_sq_ring, r.tail);
+	 *   - fs/io_uring.c|3450| <<io_uring_create>> p->sq_off.ring_mask = offsetof(struct io_sq_ring, ring_mask);
+	 *   - fs/io_uring.c|3451| <<io_uring_create>> p->sq_off.ring_entries = offsetof(struct io_sq_ring, ring_entries);
+	 *   - fs/io_uring.c|3452| <<io_uring_create>> p->sq_off.flags = offsetof(struct io_sq_ring, flags);
+	 *   - fs/io_uring.c|3453| <<io_uring_create>> p->sq_off.dropped = offsetof(struct io_sq_ring, dropped);
+	 *   - fs/io_uring.c|3457| <<io_uring_create>> p->sq_off.array = offsetof(struct io_sq_ring, array);
+	 */
 	struct io_sqring_offsets sq_off;
+	/*
+	 * 在以下设置:
+	 *   - fs/io_uring.c|3462| <<io_uring_create>> memset(&p->cq_off, 0, sizeof(p->cq_off));
+	 *   - fs/io_uring.c|3463| <<io_uring_create>> p->cq_off.head = offsetof(struct io_cq_ring, r.head);
+	 *   - fs/io_uring.c|3464| <<io_uring_create>> p->cq_off.tail = offsetof(struct io_cq_ring, r.tail);
+	 *   - fs/io_uring.c|3465| <<io_uring_create>> p->cq_off.ring_mask = offsetof(struct io_cq_ring, ring_mask);
+	 *   - fs/io_uring.c|3466| <<io_uring_create>> p->cq_off.ring_entries = offsetof(struct io_cq_ring, ring_entries);
+	 *   - fs/io_uring.c|3467| <<io_uring_create>> p->cq_off.overflow = offsetof(struct io_cq_ring, overflow);
+	 *   - fs/io_uring.c|3471| <<io_uring_create>> p->cq_off.cqes = offsetof(struct io_cq_ring, cqes);
+	 */
 	struct io_cqring_offsets cq_off;
 };
 
diff --git a/lib/percpu-refcount.c b/lib/percpu-refcount.c
index 9877682..f745cca 100644
--- a/lib/percpu-refcount.c
+++ b/lib/percpu-refcount.c
@@ -33,9 +33,32 @@
 
 #define PERCPU_COUNT_BIAS	(1LU << (BITS_PER_LONG - 1))
 
+/*
+ * used by:
+ *   - lib/percpu-refcount.c|271| <<__percpu_ref_switch_mode>> lockdep_assert_held(&percpu_ref_switch_lock);
+ *   - lib/percpu-refcount.c|279| <<__percpu_ref_switch_mode>> percpu_ref_switch_lock);
+ *   - lib/percpu-refcount.c|312| <<percpu_ref_switch_to_atomic>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|317| <<percpu_ref_switch_to_atomic>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|358| <<percpu_ref_switch_to_percpu>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|363| <<percpu_ref_switch_to_percpu>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|395| <<percpu_ref_kill_and_confirm>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|404| <<percpu_ref_kill_and_confirm>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|446| <<percpu_ref_resurrect>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|455| <<percpu_ref_resurrect>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ */
 static DEFINE_SPINLOCK(percpu_ref_switch_lock);
+/*
+ * 在以下使用:
+ *   - lib/percpu-refcount.c|129| <<percpu_ref_call_confirm_rcu>> wake_up_all(&percpu_ref_switch_waitq);
+ *   - lib/percpu-refcount.c|243| <<__percpu_ref_switch_mode>> wait_event_lock_irq(percpu_ref_switch_waitq, !ref->confirm_switch,
+ *   - lib/percpu-refcount.c|297| <<percpu_ref_switch_to_atomic_sync>> wait_event(percpu_ref_switch_waitq, !ref->confirm_switch);
+ */
 static DECLARE_WAIT_QUEUE_HEAD(percpu_ref_switch_waitq);
 
+/*
+ * 把ref->percpu_count_ptr后面的flag去掉, 返回percpu的地址
+ * 因为在percpu_ref_init()中, 无论用不用atomic, percpu的地址一定会分配
+ */
 static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)
 {
 	return (unsigned long __percpu *)
@@ -56,6 +79,12 @@ static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)
  * Note that @release must not sleep - it may potentially be called from RCU
  * callback context by percpu_ref_kill().
  */
+/*
+ * 调用的几个例子:
+ *   - fs/io_uring.c|291| <<io_ring_ctx_alloc>> if (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free, 0, GFP_KERNEL)) {
+ *
+ * 无论用不用atomic, percpu的地址一定会分配
+ */
 int percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,
 		    unsigned int flags, gfp_t gfp)
 {
@@ -63,6 +92,11 @@ int percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,
 			     __alignof__(unsigned long));
 	unsigned long start_count = 0;
 
+	/*
+	 * 类型: unsigned long percpu_count_ptr;
+	 *
+	 * 不一定用, 但是一定会分配percpu的内存
+	 */
 	ref->percpu_count_ptr = (unsigned long)
 		__alloc_percpu_gfp(sizeof(unsigned long), align, gfp);
 	if (!ref->percpu_count_ptr)
@@ -98,8 +132,20 @@ EXPORT_SYMBOL_GPL(percpu_ref_init);
  * where percpu_ref_init() succeeded but other parts of the initialization
  * of the embedding object failed.
  */
+/*
+ * 调用的几个例子:
+ *   - fs/io_uring.c|2568| <<io_ring_ctx_free>> percpu_ref_exit(&ctx->refs);
+ *   - block/blk-core.c|380| <<blk_cleanup_queue>> percpu_ref_exit(&q->q_usage_counter);
+ *   - block/blk-core.c|542| <<blk_alloc_queue_node>> percpu_ref_exit(&q->q_usage_counter);
+ *
+ * 核心思想是释放percpu的内存
+ */
 void percpu_ref_exit(struct percpu_ref *ref)
 {
+	/*
+	 * 把ref->percpu_count_ptr后面的flag去掉, 返回percpu的地址
+	 * 因为在percpu_ref_init()中, 无论用不用atomic, percpu的地址一定会分配
+	 */
 	unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
 
 	if (percpu_count) {
@@ -111,6 +157,10 @@ void percpu_ref_exit(struct percpu_ref *ref)
 }
 EXPORT_SYMBOL_GPL(percpu_ref_exit);
 
+/*
+ * called by only:
+ *   - lib/percpu-refcount.c|177| <<percpu_ref_switch_to_atomic_rcu>> percpu_ref_call_confirm_rcu(rcu);
+ */
 static void percpu_ref_call_confirm_rcu(struct rcu_head *rcu)
 {
 	struct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);
@@ -123,6 +173,10 @@ static void percpu_ref_call_confirm_rcu(struct rcu_head *rcu)
 	percpu_ref_put(ref);
 }
 
+/*
+ * called only by:
+ *   - lib/percpu-refcount.c|193| <<__percpu_ref_switch_to_atomic>> call_rcu(&ref->rcu, percpu_ref_switch_to_atomic_rcu);
+ */
 static void percpu_ref_switch_to_atomic_rcu(struct rcu_head *rcu)
 {
 	struct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);
@@ -158,10 +212,18 @@ static void percpu_ref_switch_to_atomic_rcu(struct rcu_head *rcu)
 	percpu_ref_call_confirm_rcu(rcu);
 }
 
+/*
+ * used only by:
+ *   - lib/percpu-refcount.c|239| <<__percpu_ref_switch_to_atomic>> ref->confirm_switch = confirm_switch ?: percpu_ref_noop_confirm_switch;
+ */
 static void percpu_ref_noop_confirm_switch(struct percpu_ref *ref)
 {
 }
 
+/*
+ * called by only:
+ *   - lib/percpu-refcount.c|235| <<__percpu_ref_switch_mode>> __percpu_ref_switch_to_atomic(ref, confirm_switch);
+ */
 static void __percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 					  percpu_ref_func_t *confirm_switch)
 {
@@ -184,8 +246,19 @@ static void __percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 	call_rcu(&ref->rcu, percpu_ref_switch_to_atomic_rcu);
 }
 
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|237| <<__percpu_ref_switch_mode>> __percpu_ref_switch_to_percpu(ref);
+ *
+ * 核心思想是把percpu的counter们都清0，
+ * 然后把ref->percpu_count_ptr去掉atomic重新写入ref->percpu_count_ptr
+ */
 static void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 {
+	/*
+	 * 把ref->percpu_count_ptr后面的flag去掉, 返回percpu的地址
+	 * 因为在percpu_ref_init()中, 无论用不用atomic, percpu的地址一定会分配
+	 */
 	unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
 	int cpu;
 
@@ -194,6 +267,9 @@ static void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 	if (!(ref->percpu_count_ptr & __PERCPU_REF_ATOMIC))
 		return;
 
+	/*
+	 * 这里为什么要加PERCPU_COUNT_BIAS??
+	 */
 	atomic_long_add(PERCPU_COUNT_BIAS, &ref->count);
 
 	/*
@@ -205,10 +281,23 @@ static void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 	for_each_possible_cpu(cpu)
 		*per_cpu_ptr(percpu_count, cpu) = 0;
 
+	/*
+	 * 把ref->percpu_count_ptr去掉atomic重新写入ref->percpu_count_ptr
+	 */
 	smp_store_release(&ref->percpu_count_ptr,
 			  ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC);
 }
 
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|280| <<percpu_ref_switch_to_atomic>> __percpu_ref_switch_mode(ref, confirm_switch);
+ *   - lib/percpu-refcount.c|326| <<percpu_ref_switch_to_percpu>> __percpu_ref_switch_mode(ref, NULL);
+ *   - lib/percpu-refcount.c|360| <<percpu_ref_kill_and_confirm>> __percpu_ref_switch_mode(ref, confirm_kill);
+ *   - lib/percpu-refcount.c|412| <<percpu_ref_resurrect>> __percpu_ref_switch_mode(ref, NULL);
+ *
+ * 只有在要转为atomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+ * 否则调用__percpu_ref_switch_to_percpu()
+ */
 static void __percpu_ref_switch_mode(struct percpu_ref *ref,
 				     percpu_ref_func_t *confirm_switch)
 {
@@ -219,9 +308,30 @@ static void __percpu_ref_switch_mode(struct percpu_ref *ref,
 	 * its completion.  If the caller ensures that ATOMIC switching
 	 * isn't in progress, this function can be called from any context.
 	 */
+	/*
+	 * sleep until a condition gets true. The condition is checked
+	 * under the lock. This is expected to be called with the lock
+	 * taken.
+	 *
+	 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
+	 * @condition evaluates to true. The @condition is checked each time
+	 * the waitqueue @wq_head is woken up.
+	 *     
+	 * wake_up() has to be called after changing any variable that could
+	 * change the result of the wait condition.
+	 *
+	 * This is supposed to be called while holding the lock. The lock is
+	 * dropped before going to sleep and is reacquired afterwards.
+	 */
 	wait_event_lock_irq(percpu_ref_switch_waitq, !ref->confirm_switch,
 			    percpu_ref_switch_lock);
 
+	/*
+	 * __percpu_ref_switch_to_atomic()和__percpu_ref_switch_to_percpu()
+	 * 都只在这里调用
+	 *
+	 * 只有在要转为stomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+	 */
 	if (ref->force_atomic || (ref->percpu_count_ptr & __PERCPU_REF_DEAD))
 		__percpu_ref_switch_to_atomic(ref, confirm_switch);
 	else
@@ -248,6 +358,10 @@ static void __percpu_ref_switch_mode(struct percpu_ref *ref,
  * mode.  If the caller ensures that @ref is not in the process of
  * switching to atomic mode, this function can be called from any context.
  */
+/*
+ * called by only:
+ *   - lib/percpu-refcount.c|381| <<percpu_ref_switch_to_atomic_sync>> percpu_ref_switch_to_atomic(ref, NULL);
+ */
 void percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 				 percpu_ref_func_t *confirm_switch)
 {
@@ -270,6 +384,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_atomic);
  * switch to complete.  Caller must ensure that no other thread
  * will switch back to percpu mode.
  */
+/*
+ * called by:
+ *   - block/blk-pm.c|86| <<blk_pre_runtime_suspend>> percpu_ref_switch_to_atomic_sync(&q->q_usage_counter);
+ *   - drivers/md/md.c|2347| <<set_in_sync>> percpu_ref_switch_to_atomic_sync(&mddev->writes_pending);
+ */
 void percpu_ref_switch_to_atomic_sync(struct percpu_ref *ref)
 {
 	percpu_ref_switch_to_atomic(ref, NULL);
@@ -295,6 +414,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_atomic_sync);
  * mode.  If the caller ensures that @ref is not in the process of
  * switching to atomic mode, this function can be called from any context.
  */
+/*
+ * called by:
+ *   - block/blk-sysfs.c|926| <<blk_register_queue>> percpu_ref_switch_to_percpu(&q->q_usage_counter);
+ *   - drivers/md/md.c|2361| <<set_in_sync>> percpu_ref_switch_to_percpu(&mddev->writes_pending);
+ */
 void percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 {
 	unsigned long flags;
@@ -325,6 +449,16 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_percpu);
  *
  * There are no implied RCU grace periods between kill and release.
  */
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|734| <<nvmet_sq_destroy>> percpu_ref_kill_and_confirm(&sq->ref, nvmet_confirm_sq);
+ *   - include/linux/percpu-refcount.h|128| <<percpu_ref_kill>> percpu_ref_kill_and_confirm(ref, NULL);
+ *   - kernel/cgroup/cgroup.c|5218| <<kill_css>> percpu_ref_kill_and_confirm(&css->refcnt, css_killed_ref_fn);
+ *
+ * 调用的时候ref->percpu_count_ptr不能已经dead了
+ * 设置ref->percpu_count_ptr的dead, 调用__percpu_ref_switch_to_percpu()
+ * 最后put一下percpu_ref_put(ref)
+ */
 void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
 				 percpu_ref_func_t *confirm_kill)
 {
@@ -332,10 +466,17 @@ void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
 
 	spin_lock_irqsave(&percpu_ref_switch_lock, flags);
 
+	/* 调用的时候ref->percpu_count_ptr不能已经dead了 */
 	WARN_ONCE(ref->percpu_count_ptr & __PERCPU_REF_DEAD,
 		  "%s called more than once on %pf!", __func__, ref->release);
 
 	ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	/*
+	 * 只有在要转为atomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+	 * 否则调用__percpu_ref_switch_to_percpu()
+	 *
+	 * 因为上面设置了dead, 所以先面会调用__percpu_ref_switch_to_percpu()
+	 */
 	__percpu_ref_switch_mode(ref, confirm_kill);
 	percpu_ref_put(ref);
 
@@ -354,6 +495,10 @@ EXPORT_SYMBOL_GPL(percpu_ref_kill_and_confirm);
  * Note that percpu_ref_tryget[_live]() are safe to perform on @ref while
  * this function is in progress.
  */
+/*
+ * called by only:
+ *   - fs/io_uring.c|2976| <<__io_uring_register>> percpu_ref_reinit(&ctx->refs);
+ */
 void percpu_ref_reinit(struct percpu_ref *ref)
 {
 	WARN_ON_ONCE(!percpu_ref_is_zero(ref));
@@ -376,6 +521,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_reinit);
  * Note that percpu_ref_tryget[_live]() are safe to perform on @ref while
  * this function is in progress.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|206| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+ *   - lib/percpu-refcount.c|473| <<percpu_ref_reinit>> percpu_ref_resurrect(ref);
+ */
 void percpu_ref_resurrect(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
@@ -383,11 +533,24 @@ void percpu_ref_resurrect(struct percpu_ref *ref)
 
 	spin_lock_irqsave(&percpu_ref_switch_lock, flags);
 
+	/*
+	 * 到了这一步了必须是dead了才能resurrect
+	 */
 	WARN_ON_ONCE(!(ref->percpu_count_ptr & __PERCPU_REF_DEAD));
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	WARN_ON_ONCE(__ref_is_percpu(ref, &percpu_count));
 
 	ref->percpu_count_ptr &= ~__PERCPU_REF_DEAD;
 	percpu_ref_get(ref);
+	/*
+	 * 只有在要转为atomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+	 * 否则调用__percpu_ref_switch_to_percpu()
+	 */
 	__percpu_ref_switch_mode(ref, NULL);
 
 	spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
-- 
2.7.4

