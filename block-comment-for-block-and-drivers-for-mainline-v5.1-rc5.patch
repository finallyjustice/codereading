From 7d747ab10d529581b77e859f13086f3c4b4d0e1b Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang@oracle.com>
Date: Thu, 18 Apr 2019 06:01:46 +0800
Subject: [PATCH 1/1] block comment for block and drivers for mainline v5.1-rc5

This is for mainline v5.1-rc5

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
Signed-off-by: Dongli Zhang <dongli.zhang@oracle.com>
---
 fs/io_uring.c                   | 161 +++++++++++++++++++++++++++++++++++++++
 include/linux/percpu-refcount.h |  90 ++++++++++++++++++++++
 include/uapi/linux/io_uring.h   |  46 ++++++++++++
 lib/percpu-refcount.c           | 163 ++++++++++++++++++++++++++++++++++++++++
 4 files changed, 460 insertions(+)

diff --git a/fs/io_uring.c b/fs/io_uring.c
index 89aa841..38b453f 100644
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@ -62,9 +62,39 @@
 
 #include "internal.h"
 
+/*
+ * 对于sq
+ * user修改tail, kernel修改head
+ * The head and tail values are used to manage entries in the ring; if the
+ * two values are equal, the ring is empty. User-space code adds an entry
+ * by putting its index into array[r.tail] and incrementing the tail
+ * pointer; only the kernel side should change r.head. Once one or more
+ * entries have been placed in the ring, they can be submitted with a call
+ * to:
+ *
+ *    int io_uring_enter(unsigned int fd, u32 to_submit, u32 min_complete, u32 flags);
+ *
+ * 对于cq
+ * kernel修改tail, user修改head
+ * In this ring, the r.head index points to the first available completion
+ * event, while r.tail points to the last; user space should only change
+ * r.head.
+ */
+
 #define IORING_MAX_ENTRIES	4096
 #define IORING_MAX_FIXED_FILES	1024
 
+/*
+ * The head and tail values are used to manage entries in the ring; if the 
+ * two values are equal, the ring is empty. User-space code adds an entry
+ * by putting its index into array[r.tail] and incrementing the tail
+ * pointer; only the kernel side should change r.head. Once one or more
+ * entries have been placed in the ring, they can be submitted with a call
+ * to:
+ *
+ * int io_uring_enter(unsigned int fd, u32 to_submit, u32 min_complete, u32 flags);
+ */
+
 struct io_uring {
 	u32 head ____cacheline_aligned_in_smp;
 	u32 tail ____cacheline_aligned_in_smp;
@@ -104,6 +134,13 @@ struct async_list {
 	size_t			io_pages;
 };
 
+/*
+ * io_allocate_scq_urings()执行后:
+ * struct io_ring_ctx
+ *   struct io_sq_ring    *sq_ring; --> 指向struct io_sq_ring + 一组u32 array[]
+ *   struct io_uring_sqe  *sq_sqes; --> 指向一组struct io_uring_sqe[]
+ *   struct io_cq_ring    *cq_ring; --> 指向struct io_cq_ring + 一组struct io_uring_cqe[]
+ */
 struct io_ring_ctx {
 	struct {
 		struct percpu_ref	refs;
@@ -118,6 +155,10 @@ struct io_ring_ctx {
 		struct io_sq_ring	*sq_ring;
 		unsigned		cached_sq_head;
 		unsigned		sq_entries;
+		/*
+		 * 唯一修改的地方:
+		 *   - fs/io_uring.c|2792| <<io_allocate_scq_urings>> ctx->sq_mask = sq_ring->ring_mask;
+		 */
 		unsigned		sq_mask;
 		unsigned		sq_thread_idle;
 		struct io_uring_sqe	*sq_sqes;
@@ -135,6 +176,10 @@ struct io_ring_ctx {
 		struct io_cq_ring	*cq_ring;
 		unsigned		cached_cq_tail;
 		unsigned		cq_entries;
+		/*
+		 * 唯一修改的地方:
+		 *   - fs/io_uring.c|2815| <<io_allocate_scq_urings>> ctx->cq_mask = cq_ring->ring_mask;
+		 */
 		unsigned		cq_mask;
 		struct wait_queue_head	cq_wait;
 		struct fasync_struct	*cq_fasync;
@@ -154,6 +199,14 @@ struct io_ring_ctx {
 
 	struct user_struct	*user;
 
+	/*
+	 * used by:
+	 *   - fs/io_uring.c|326| <<io_ring_ctx_ref_free>> complete(&ctx->ctx_done);
+	 *   - fs/io_uring.c|345| <<io_ring_ctx_alloc>> init_completion(&ctx->ctx_done);
+	 *   - fs/io_uring.c|2649| <<io_ring_ctx_wait_and_kill>> wait_for_completion(&ctx->ctx_done);
+	 *   - fs/io_uring.c|2985| <<__io_uring_register>> wait_for_completion(&ctx->ctx_done);
+	 *   - fs/io_uring.c|3012| <<__io_uring_register>> reinit_completion(&ctx->ctx_done);
+	 */
 	struct completion	ctx_done;
 
 	struct {
@@ -170,10 +223,39 @@ struct io_ring_ctx {
 		 * For SQPOLL, only the single threaded io_sq_thread() will
 		 * manipulate the list, hence no extra locking is needed there.
 		 */
+		/*
+		 * poll_list在以下被使用:
+		 *   - fs/io_uring.c|370| <<io_ring_ctx_alloc>> INIT_LIST_HEAD(&ctx->poll_list);
+		 *   - fs/io_uring.c|587| <<io_do_iopoll>> list_for_each_entry_safe(req, tmp, &ctx->poll_list, list) {
+		 *   - fs/io_uring.c|625| <<io_iopoll_getevents>> while (!list_empty(&ctx->poll_list)) {
+		 *   - fs/io_uring.c|648| <<io_iopoll_reap_events>> while (!list_empty(&ctx->poll_list)) {
+		 *   - fs/io_uring.c|727| <<io_iopoll_req_issued>> if (list_empty(&ctx->poll_list)) {
+		 *   - fs/io_uring.c|732| <<io_iopoll_req_issued>> list_req = list_first_entry(&ctx->poll_list, struct io_kiocb,
+		 *   - fs/io_uring.c|743| <<io_iopoll_req_issued>> list_add(&req->list, &ctx->poll_list);
+		 *   - fs/io_uring.c|745| <<io_iopoll_req_issued>> list_add_tail(&req->list, &ctx->poll_list);
+		 */
 		struct list_head	poll_list;
+		/*
+		 * cancel_list在以下被使用:
+		 *   - fs/io_uring.c|382| <<io_ring_ctx_alloc>> INIT_LIST_HEAD(&ctx->cancel_list);
+		 *   - fs/io_uring.c|1219| <<io_poll_remove_all>> while (!list_empty(&ctx->cancel_list)) {
+		 *   - fs/io_uring.c|1220| <<io_poll_remove_all>> req = list_first_entry(&ctx->cancel_list, struct io_kiocb,list);
+		 *   - fs/io_uring.c|1243| <<io_poll_remove>> list_for_each_entry_safe(poll_req, next, &ctx->cancel_list, list) {
+		 *   - fs/io_uring.c|1397| <<io_poll_add>> list_add_tail(&req->list, &ctx->cancel_list);
+		 */
 		struct list_head	cancel_list;
 	} ____cacheline_aligned_in_smp;
 
+	/*
+	 * pending_async在以下被使用:
+	 *   - fs/io_uring.c|383| <<io_ring_ctx_alloc>> for (i = 0; i < ARRAY_SIZE(ctx->pending_async); i++) {
+	 *   - fs/io_uring.c|384| <<io_ring_ctx_alloc>> spin_lock_init(&ctx->pending_async[i].lock);
+	 *   - fs/io_uring.c|385| <<io_ring_ctx_alloc>> INIT_LIST_HEAD(&ctx->pending_async[i].list);
+	 *   - fs/io_uring.c|386| <<io_ring_ctx_alloc>> atomic_set(&ctx->pending_async[i].cnt, 0);
+	 *   - fs/io_uring.c|991| <<io_async_list_note>> struct async_list *async_list = &req->ctx->pending_async[rw];
+	 *   - fs/io_uring.c|1491| <<io_async_list_from_sqe>> return &ctx->pending_async[READ];
+	 *   - fs/io_uring.c|1494| <<io_async_list_from_sqe>> return &ctx->pending_async[WRITE];
+	 */
 	struct async_list	pending_async[2];
 
 #if defined(CONFIG_UNIX)
@@ -182,7 +264,13 @@ struct io_ring_ctx {
 };
 
 struct sqe_submit {
+	/*
+	 * 例子是&ctx->sq_sqes[head]
+	 */
 	const struct io_uring_sqe	*sqe;
+	/*
+	 * 例子是READ_ONCE(ring->array[head & ctx->sq_mask])
+	 */
 	unsigned short			index;
 	bool				has_user;
 	bool				needs_lock;
@@ -211,6 +299,11 @@ struct io_poll_iocb {
 struct io_kiocb {
 	union {
 		struct file		*file;
+		/*
+		 * 有两处设置ki_complete:
+		 *   - fs/io_uring.c|872| <<io_prep_rw>> kiocb->ki_complete = io_complete_rw_iopoll;
+		 *   - fs/io_uring.c|876| <<io_prep_rw>> kiocb->ki_complete = io_complete_rw;
+		 */
 		struct kiocb		rw;
 		struct io_poll_iocb	poll;
 	};
@@ -252,9 +345,25 @@ struct io_submit_state {
 	unsigned int		fd;
 	unsigned int		has_refs;
 	unsigned int		used_refs;
+	/*
+	 * 修改ios_left的地方:
+	 *   - fs/io_uring.c|814| <<io_file_get>> state->ios_left--;
+	 *   - fs/io_uring.c|826| <<io_file_get>> state->ios_left--;
+	 *   - fs/io_uring.c|1792| <<io_submit_state_start>> state->ios_left = max_ios;
+	 */
 	unsigned int		ios_left;
 };
 
+/*
+ * used by:
+ *   - fs/io_uring.c|518| <<io_get_req>> req = kmem_cache_alloc(req_cachep, gfp);
+ *   - fs/io_uring.c|526| <<io_get_req>> ret = kmem_cache_alloc_bulk(req_cachep, gfp, sz, state->reqs);
+ *   - fs/io_uring.c|533| <<io_get_req>> state->reqs[0] = kmem_cache_alloc(req_cachep, gfp);
+ *   - fs/io_uring.c|560| <<io_free_req_many>> kmem_cache_free_bulk(req_cachep, *nr, reqs);
+ *   - fs/io_uring.c|571| <<io_free_req>> kmem_cache_free(req_cachep, req);
+ *   - fs/io_uring.c|1785| <<io_submit_state_end>> kmem_cache_free_bulk(req_cachep, state->free_reqs,
+ *   - fs/io_uring.c|3128| <<io_uring_init>> req_cachep = KMEM_CACHE(io_kiocb, SLAB_HWCACHE_ALIGN | SLAB_PANIC);
+ */
 static struct kmem_cache *req_cachep;
 
 static const struct file_operations io_uring_fops;
@@ -272,6 +381,21 @@ struct sock *io_uring_get_socket(struct file *file)
 }
 EXPORT_SYMBOL(io_uring_get_socket);
 
+/*
+ * callstack的一个例子
+ * [0] io_ring_ctx_ref_free
+ * [0] percpu_ref_switch_to_atomic_rcu
+ * [0] rcu_core
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] smp_apic_timer_interrupt
+ * [0] apic_timer_interrupt
+ *
+ * used only by:
+ *   - fs/io_uring.c|410| <<io_ring_ctx_alloc>> if (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free, 0, GFP_KERNEL)) {
+ *
+ * 被用做io_ring_ctx->refs这个percpu_ref的release函数
+ */
 static void io_ring_ctx_ref_free(struct percpu_ref *ref)
 {
 	struct io_ring_ctx *ctx = container_of(ref, struct io_ring_ctx, refs);
@@ -2532,6 +2656,10 @@ static int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,
 	return ret;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|2594| <<io_ring_ctx_wait_and_kill>> io_ring_ctx_free(ctx);
+ */
 static void io_ring_ctx_free(struct io_ring_ctx *ctx)
 {
 	io_finish_async(ctx);
@@ -2582,6 +2710,11 @@ static int io_uring_fasync(int fd, struct file *file, int on)
 	return fasync_helper(fd, file, on, &ctx->cq_fasync);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2616| <<io_uring_release>> io_ring_ctx_wait_and_kill(ctx);
+ *   - fs/io_uring.c|2895| <<io_uring_create>> io_ring_ctx_wait_and_kill(ctx);
+ */
 static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)
 {
 	mutex_lock(&ctx->uring_lock);
@@ -2958,6 +3091,28 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 	return ret;
 }
 
+/*
+ * There is ability to map a program's I/O buffers into the kernel. This
+ * mapping normally happens with each I/O operation so that data can be
+ * copied into or out of the buffers; the buffers are unmapped when the
+ * operation completes. If the buffers will be used many times over the
+ * course of the program's execution, it is far more efficient to map them
+ * once and leave them in place. This mapping is done by filling in yet
+ * another structure describing the buffers to be mapped: 
+ *
+ * In this case, the opcode should be IORING_REGISTER_BUFFERS. The buffers
+ * will remain mapped for as long as the initial file descriptor remains
+ * open, unless the program explicitly unmaps them with
+ * IORING_UNREGISTER_BUFFERS. Mapping buffers in this way is essentially
+ * locking memory into RAM, so the usual resource limit that applies to
+ * mlock() applies here as well. When performing I/O to premapped buffers,
+ * the IORING_OP_READ_FIXED and IORING_OP_WRITE_FIXED operations should be
+ * used.
+ *
+ * There is also an IORING_REGISTER_FILES operation that can be used to
+ * optimize situations where many operations will be performed on the same
+ * file(s).
+ */
 SYSCALL_DEFINE4(io_uring_register, unsigned int, fd, unsigned int, opcode,
 		void __user *, arg, unsigned int, nr_args)
 {
@@ -2989,3 +3144,9 @@ static int __init io_uring_init(void)
 	return 0;
 };
 __initcall(io_uring_init);
+
+/*
+ * SYSCALL_DEFINE6(io_uring_enter)
+ * SYSCALL_DEFINE2(io_uring_setup)
+ * SYSCALL_DEFINE4(io_uring_register)
+ */
diff --git a/include/linux/percpu-refcount.h b/include/linux/percpu-refcount.h
index b297cd1..e1e686c 100644
--- a/include/linux/percpu-refcount.h
+++ b/include/linux/percpu-refcount.h
@@ -76,12 +76,24 @@ enum {
 	 * with this flag, the ref will stay in atomic mode until
 	 * percpu_ref_switch_to_percpu() is invoked on it.
 	 */
+	/*
+	 * used by:
+	 *   - block/blk-core.c|533| <<blk_alloc_queue_node>> PERCPU_REF_INIT_ATOMIC, GFP_KERNEL))
+	 *   - drivers/infiniband/sw/rdmavt/mr.c|735| <<rvt_alloc_fmr>> PERCPU_REF_INIT_ATOMIC);
+	 *   - lib/percpu-refcount.c|105| <<percpu_ref_init>> ref->force_atomic = flags & PERCPU_REF_INIT_ATOMIC;
+	 *   - lib/percpu-refcount.c|107| <<percpu_ref_init>> if (flags & (PERCPU_REF_INIT_ATOMIC | PERCPU_REF_INIT_DEAD))
+	 */
 	PERCPU_REF_INIT_ATOMIC	= 1 << 0,
 
 	/*
 	 * Start dead w/ ref == 0 in atomic mode.  Must be revived with
 	 * percpu_ref_reinit() before used.  Implies INIT_ATOMIC.
 	 */
+	/*
+	 * called by:
+	 *   - lib/percpu-refcount.c|107| <<percpu_ref_init>> if (flags & (PERCPU_REF_INIT_ATOMIC | PERCPU_REF_INIT_DEAD))
+	 *   - lib/percpu-refcount.c|112| <<percpu_ref_init>> if (flags & PERCPU_REF_INIT_DEAD)
+	 */
 	PERCPU_REF_INIT_DEAD	= 1 << 1,
 };
 
@@ -123,8 +135,36 @@ void percpu_ref_reinit(struct percpu_ref *ref);
  *
  * There are no implied RCU grace periods between kill and release.
  */
+/*
+ * called by:
+ *   - block/blk-cgroup.c|415| <<blkg_destroy>> percpu_ref_kill(&blkg->refcnt);
+ *   - block/blk-mq.c|150| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+ *   - drivers/dax/device.c|46| <<dev_dax_percpu_kill>> percpu_ref_kill(ref);
+ *   - drivers/infiniband/sw/rdmavt/mr.c|275| <<rvt_free_lkey>> percpu_ref_kill(&mr->refcount);
+ *   - drivers/nvme/target/core.c|585| <<nvmet_ns_disable>> percpu_ref_kill(&ns->ref);
+ *   - drivers/pci/p2pdma.c|96| <<pci_p2pdma_percpu_kill>> percpu_ref_kill(ref);
+ *   - drivers/target/target_core_transport.c|2915| <<target_sess_cmd_list_set_waiting>> percpu_ref_kill(&se_sess->cmd_count);
+ *   - drivers/target/target_core_transport.c|2947| <<transport_clear_lun_ref>> percpu_ref_kill(&lun->lun_ref);
+ *   - fs/aio.c|631| <<free_ioctx_users>> percpu_ref_kill(&ctx->reqs);
+ *   - fs/aio.c|850| <<kill_ioctx>> percpu_ref_kill(&ctx->users);
+ *   - fs/io_uring.c|2602| <<io_ring_ctx_wait_and_kill>> percpu_ref_kill(&ctx->refs);
+ *   - fs/io_uring.c|2942| <<__io_uring_register>> percpu_ref_kill(&ctx->refs);
+ *   - include/linux/genhd.h|696| <<hd_struct_kill>> percpu_ref_kill(&part->ref);
+ *   - kernel/cgroup/cgroup.c|2173| <<cgroup_kill_sb>> percpu_ref_kill(&root->cgrp.self.refcnt);
+ *   - kernel/cgroup/cgroup.c|5302| <<__acquires>> percpu_ref_kill(&cgrp->self.refcnt);
+ *   - mm/backing-dev.c|526| <<cgwb_kill>> percpu_ref_kill(&wb->refcnt);
+ *   - mm/hmm.c|990| <<hmm_devmem_ref_kill>> percpu_ref_kill(ref);
+ *
+ * 调用的时候ref->percpu_count_ptr不能已经dead了
+ * 设置ref->percpu_count_ptr的dead, 调用__percpu_ref_switch_to_percpu()
+ * (confirm_kill是NULL)
+ * 最后put一下percpu_ref_put(ref)
+ */
 static inline void percpu_ref_kill(struct percpu_ref *ref)
 {
+	/*
+	 * 调用的时候ref->percpu_count_ptr不能已经dead了
+	 */
 	percpu_ref_kill_and_confirm(ref, NULL);
 }
 
@@ -134,6 +174,12 @@ static inline void percpu_ref_kill(struct percpu_ref *ref)
  * because doing so forces the compiler to generate two conditional
  * branches as it can't assume that @ref->percpu_count is not NULL.
  */
+/*
+ * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+ * 则返回false
+ * 否则更新参数为ref->percpu_count_ptr
+ * 并且返回true
+ */
 static inline bool __ref_is_percpu(struct percpu_ref *ref,
 					  unsigned long __percpu **percpu_countp)
 {
@@ -174,12 +220,24 @@ static inline bool __ref_is_percpu(struct percpu_ref *ref,
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * 对于percpu的(非atomic的)不会增加ref->count
+ *
+ * 对于atomic的或者dead的, atomic_long_add(nr, &ref->count);
+ * 对于percpu的, atomic_long_add(nr, &ref->count);
+ */
 static inline void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
 {
 	unsigned long __percpu *percpu_count;
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	if (__ref_is_percpu(ref, &percpu_count))
 		this_cpu_add(*percpu_count, nr);
 	else
@@ -196,6 +254,12 @@ static inline void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * 对于percpu的(非atomic的)不会增加ref->count
+ *
+ * 对于atomic的或者dead的, atomic_long_add(nr, &ref->count);
+ * 对于percpu的, atomic_long_add(nr, &ref->count);
+ */
 static inline void percpu_ref_get(struct percpu_ref *ref)
 {
 	percpu_ref_get_many(ref, 1);
@@ -210,6 +274,20 @@ static inline void percpu_ref_get(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|391| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+ *   - block/blk-mq.c|939| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+ *   - fs/io_uring.c|415| <<io_get_req>> if (!percpu_ref_tryget(&ctx->refs))
+ *   - fs/io_uring.c|2678| <<SYSCALL_DEFINE6>> if (!percpu_ref_tryget(&ctx->refs))
+ *   - include/linux/backing-dev-defs.h|242| <<wb_tryget>> return percpu_ref_tryget(&wb->refcnt);
+ *   - include/linux/blk-cgroup.h|502| <<blkg_tryget>> return blkg && percpu_ref_tryget(&blkg->refcnt);
+ *   - include/linux/cgroup.h|340| <<css_tryget>> return percpu_ref_tryget(&css->refcnt);
+ *
+ * 如果是percpu的直接this_cpu_inc(*percpu_count)并且返回true
+ * 否则用atomic_long_inc_not_zero(&ref->count)只有在ref->count不为0的时候才增加
+ * 如果之前为0则返回0
+ */
 static inline bool percpu_ref_tryget(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
@@ -217,6 +295,12 @@ static inline bool percpu_ref_tryget(struct percpu_ref *ref)
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 调用的时候ref->percpu_count_ptr不能已经dead了
+	 * 设置ref->percpu_count_ptr的dead, 调用__percpu_ref_switch_to_percpu()
+	 * (confirm_kill是NULL)
+	 * 最后put一下percpu_ref_put(ref)
+	 */
 	if (__ref_is_percpu(ref, &percpu_count)) {
 		this_cpu_inc(*percpu_count);
 		ret = true;
@@ -279,6 +363,12 @@ static inline void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	if (__ref_is_percpu(ref, &percpu_count))
 		this_cpu_sub(*percpu_count, nr);
 	else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index e234086..4267776 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -42,8 +42,49 @@ struct io_uring_sqe {
 /*
  * io_uring_setup() flags
  */
+/*
+ * used by:
+ *   - fs/io_uring.c|591| <<io_iopoll_reap_events>> if (!(ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|793| <<io_prep_rw>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|1078| <<io_nop>> if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1096| <<io_prep_fsync>> if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1172| <<io_poll_remove>> if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1293| <<io_poll_add>> if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1398| <<__io_submit_sqe>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|1828| <<io_sq_thread>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|2718| <<SYSCALL_DEFINE6(io_uring_enter)>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|2922| <<io_uring_setup>> if (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |
+ */
 #define IORING_SETUP_IOPOLL	(1U << 0)	/* io_context is polled */
+/*
+ * used by:
+ *   - fs/io_uring.c|2257| <<io_sq_offload_start>> if (ctx->flags & IORING_SETUP_SQPOLL) {
+ *   - fs/io_uring.c|2686| <<SYSCALL_DEFINE6>> if (ctx->flags & IORING_SETUP_SQPOLL) {
+ *   - fs/io_uring.c|2922| <<io_uring_setup>> if (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |
+ *
+ * There is also a fully polled mode that (almost) eliminates the need to
+ * make any system calls at all. This mode is enabled by setting the
+ * IORING_SETUP_SQPOLL flag at ring setup time. A call to io_uring_enter()
+ * will kick off a kernel thread that will occasionally poll the submission
+ * queue and automatically submit any requests found there; receive-queue
+ * polling is also performed if it has been requested. As long as the
+ * application continues to submit I/O and consume the results, I/O will
+ * happen with no further system calls.
+ *
+ * Eventually, though (after one second currently), the kernel will get
+ * bored if no new requests are submitted and the polling will stop. When
+ * that happens, the flags field in the submission queue structure will
+ * have the IORING_SQ_NEED_WAKEUP bit set. The application should check
+ * for this bit and, if it is set, make a new call to io_uring_enter() to
+ * start the mechanism up again.
+ */
 #define IORING_SETUP_SQPOLL	(1U << 1)	/* SQ poll thread */
+/*
+ * used by:
+ *   - fs/io_uring.c|2262| <<io_sq_offload_start>> if (p->flags & IORING_SETUP_SQ_AFF) {
+ *   - fs/io_uring.c|2279| <<io_sq_offload_start>> } else if (p->flags & IORING_SETUP_SQ_AFF) {
+ *   - fs/io_uring.c|2923| <<io_uring_setup>> IORING_SETUP_SQ_AFF))
+ */
 #define IORING_SETUP_SQ_AFF	(1U << 2)	/* sq_thread_cpu is valid */
 
 #define IORING_OP_NOP		0
@@ -58,6 +99,11 @@ struct io_uring_sqe {
 /*
  * sqe->fsync_flags
  */
+/*
+ * used by:
+ *   - fs/io_uring.c|1115| <<io_fsync>> if (unlikely(fsync_flags & ~IORING_FSYNC_DATASYNC))
+ *   - fs/io_uring.c|1128| <<io_fsync>> fsync_flags & IORING_FSYNC_DATASYNC);
+ */
 #define IORING_FSYNC_DATASYNC	(1U << 0)
 
 /*
diff --git a/lib/percpu-refcount.c b/lib/percpu-refcount.c
index 9877682..f745cca 100644
--- a/lib/percpu-refcount.c
+++ b/lib/percpu-refcount.c
@@ -33,9 +33,32 @@
 
 #define PERCPU_COUNT_BIAS	(1LU << (BITS_PER_LONG - 1))
 
+/*
+ * used by:
+ *   - lib/percpu-refcount.c|271| <<__percpu_ref_switch_mode>> lockdep_assert_held(&percpu_ref_switch_lock);
+ *   - lib/percpu-refcount.c|279| <<__percpu_ref_switch_mode>> percpu_ref_switch_lock);
+ *   - lib/percpu-refcount.c|312| <<percpu_ref_switch_to_atomic>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|317| <<percpu_ref_switch_to_atomic>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|358| <<percpu_ref_switch_to_percpu>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|363| <<percpu_ref_switch_to_percpu>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|395| <<percpu_ref_kill_and_confirm>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|404| <<percpu_ref_kill_and_confirm>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|446| <<percpu_ref_resurrect>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|455| <<percpu_ref_resurrect>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ */
 static DEFINE_SPINLOCK(percpu_ref_switch_lock);
+/*
+ * 在以下使用:
+ *   - lib/percpu-refcount.c|129| <<percpu_ref_call_confirm_rcu>> wake_up_all(&percpu_ref_switch_waitq);
+ *   - lib/percpu-refcount.c|243| <<__percpu_ref_switch_mode>> wait_event_lock_irq(percpu_ref_switch_waitq, !ref->confirm_switch,
+ *   - lib/percpu-refcount.c|297| <<percpu_ref_switch_to_atomic_sync>> wait_event(percpu_ref_switch_waitq, !ref->confirm_switch);
+ */
 static DECLARE_WAIT_QUEUE_HEAD(percpu_ref_switch_waitq);
 
+/*
+ * 把ref->percpu_count_ptr后面的flag去掉, 返回percpu的地址
+ * 因为在percpu_ref_init()中, 无论用不用atomic, percpu的地址一定会分配
+ */
 static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)
 {
 	return (unsigned long __percpu *)
@@ -56,6 +79,12 @@ static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)
  * Note that @release must not sleep - it may potentially be called from RCU
  * callback context by percpu_ref_kill().
  */
+/*
+ * 调用的几个例子:
+ *   - fs/io_uring.c|291| <<io_ring_ctx_alloc>> if (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free, 0, GFP_KERNEL)) {
+ *
+ * 无论用不用atomic, percpu的地址一定会分配
+ */
 int percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,
 		    unsigned int flags, gfp_t gfp)
 {
@@ -63,6 +92,11 @@ int percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,
 			     __alignof__(unsigned long));
 	unsigned long start_count = 0;
 
+	/*
+	 * 类型: unsigned long percpu_count_ptr;
+	 *
+	 * 不一定用, 但是一定会分配percpu的内存
+	 */
 	ref->percpu_count_ptr = (unsigned long)
 		__alloc_percpu_gfp(sizeof(unsigned long), align, gfp);
 	if (!ref->percpu_count_ptr)
@@ -98,8 +132,20 @@ EXPORT_SYMBOL_GPL(percpu_ref_init);
  * where percpu_ref_init() succeeded but other parts of the initialization
  * of the embedding object failed.
  */
+/*
+ * 调用的几个例子:
+ *   - fs/io_uring.c|2568| <<io_ring_ctx_free>> percpu_ref_exit(&ctx->refs);
+ *   - block/blk-core.c|380| <<blk_cleanup_queue>> percpu_ref_exit(&q->q_usage_counter);
+ *   - block/blk-core.c|542| <<blk_alloc_queue_node>> percpu_ref_exit(&q->q_usage_counter);
+ *
+ * 核心思想是释放percpu的内存
+ */
 void percpu_ref_exit(struct percpu_ref *ref)
 {
+	/*
+	 * 把ref->percpu_count_ptr后面的flag去掉, 返回percpu的地址
+	 * 因为在percpu_ref_init()中, 无论用不用atomic, percpu的地址一定会分配
+	 */
 	unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
 
 	if (percpu_count) {
@@ -111,6 +157,10 @@ void percpu_ref_exit(struct percpu_ref *ref)
 }
 EXPORT_SYMBOL_GPL(percpu_ref_exit);
 
+/*
+ * called by only:
+ *   - lib/percpu-refcount.c|177| <<percpu_ref_switch_to_atomic_rcu>> percpu_ref_call_confirm_rcu(rcu);
+ */
 static void percpu_ref_call_confirm_rcu(struct rcu_head *rcu)
 {
 	struct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);
@@ -123,6 +173,10 @@ static void percpu_ref_call_confirm_rcu(struct rcu_head *rcu)
 	percpu_ref_put(ref);
 }
 
+/*
+ * called only by:
+ *   - lib/percpu-refcount.c|193| <<__percpu_ref_switch_to_atomic>> call_rcu(&ref->rcu, percpu_ref_switch_to_atomic_rcu);
+ */
 static void percpu_ref_switch_to_atomic_rcu(struct rcu_head *rcu)
 {
 	struct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);
@@ -158,10 +212,18 @@ static void percpu_ref_switch_to_atomic_rcu(struct rcu_head *rcu)
 	percpu_ref_call_confirm_rcu(rcu);
 }
 
+/*
+ * used only by:
+ *   - lib/percpu-refcount.c|239| <<__percpu_ref_switch_to_atomic>> ref->confirm_switch = confirm_switch ?: percpu_ref_noop_confirm_switch;
+ */
 static void percpu_ref_noop_confirm_switch(struct percpu_ref *ref)
 {
 }
 
+/*
+ * called by only:
+ *   - lib/percpu-refcount.c|235| <<__percpu_ref_switch_mode>> __percpu_ref_switch_to_atomic(ref, confirm_switch);
+ */
 static void __percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 					  percpu_ref_func_t *confirm_switch)
 {
@@ -184,8 +246,19 @@ static void __percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 	call_rcu(&ref->rcu, percpu_ref_switch_to_atomic_rcu);
 }
 
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|237| <<__percpu_ref_switch_mode>> __percpu_ref_switch_to_percpu(ref);
+ *
+ * 核心思想是把percpu的counter们都清0，
+ * 然后把ref->percpu_count_ptr去掉atomic重新写入ref->percpu_count_ptr
+ */
 static void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 {
+	/*
+	 * 把ref->percpu_count_ptr后面的flag去掉, 返回percpu的地址
+	 * 因为在percpu_ref_init()中, 无论用不用atomic, percpu的地址一定会分配
+	 */
 	unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
 	int cpu;
 
@@ -194,6 +267,9 @@ static void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 	if (!(ref->percpu_count_ptr & __PERCPU_REF_ATOMIC))
 		return;
 
+	/*
+	 * 这里为什么要加PERCPU_COUNT_BIAS??
+	 */
 	atomic_long_add(PERCPU_COUNT_BIAS, &ref->count);
 
 	/*
@@ -205,10 +281,23 @@ static void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 	for_each_possible_cpu(cpu)
 		*per_cpu_ptr(percpu_count, cpu) = 0;
 
+	/*
+	 * 把ref->percpu_count_ptr去掉atomic重新写入ref->percpu_count_ptr
+	 */
 	smp_store_release(&ref->percpu_count_ptr,
 			  ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC);
 }
 
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|280| <<percpu_ref_switch_to_atomic>> __percpu_ref_switch_mode(ref, confirm_switch);
+ *   - lib/percpu-refcount.c|326| <<percpu_ref_switch_to_percpu>> __percpu_ref_switch_mode(ref, NULL);
+ *   - lib/percpu-refcount.c|360| <<percpu_ref_kill_and_confirm>> __percpu_ref_switch_mode(ref, confirm_kill);
+ *   - lib/percpu-refcount.c|412| <<percpu_ref_resurrect>> __percpu_ref_switch_mode(ref, NULL);
+ *
+ * 只有在要转为atomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+ * 否则调用__percpu_ref_switch_to_percpu()
+ */
 static void __percpu_ref_switch_mode(struct percpu_ref *ref,
 				     percpu_ref_func_t *confirm_switch)
 {
@@ -219,9 +308,30 @@ static void __percpu_ref_switch_mode(struct percpu_ref *ref,
 	 * its completion.  If the caller ensures that ATOMIC switching
 	 * isn't in progress, this function can be called from any context.
 	 */
+	/*
+	 * sleep until a condition gets true. The condition is checked
+	 * under the lock. This is expected to be called with the lock
+	 * taken.
+	 *
+	 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
+	 * @condition evaluates to true. The @condition is checked each time
+	 * the waitqueue @wq_head is woken up.
+	 *     
+	 * wake_up() has to be called after changing any variable that could
+	 * change the result of the wait condition.
+	 *
+	 * This is supposed to be called while holding the lock. The lock is
+	 * dropped before going to sleep and is reacquired afterwards.
+	 */
 	wait_event_lock_irq(percpu_ref_switch_waitq, !ref->confirm_switch,
 			    percpu_ref_switch_lock);
 
+	/*
+	 * __percpu_ref_switch_to_atomic()和__percpu_ref_switch_to_percpu()
+	 * 都只在这里调用
+	 *
+	 * 只有在要转为stomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+	 */
 	if (ref->force_atomic || (ref->percpu_count_ptr & __PERCPU_REF_DEAD))
 		__percpu_ref_switch_to_atomic(ref, confirm_switch);
 	else
@@ -248,6 +358,10 @@ static void __percpu_ref_switch_mode(struct percpu_ref *ref,
  * mode.  If the caller ensures that @ref is not in the process of
  * switching to atomic mode, this function can be called from any context.
  */
+/*
+ * called by only:
+ *   - lib/percpu-refcount.c|381| <<percpu_ref_switch_to_atomic_sync>> percpu_ref_switch_to_atomic(ref, NULL);
+ */
 void percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 				 percpu_ref_func_t *confirm_switch)
 {
@@ -270,6 +384,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_atomic);
  * switch to complete.  Caller must ensure that no other thread
  * will switch back to percpu mode.
  */
+/*
+ * called by:
+ *   - block/blk-pm.c|86| <<blk_pre_runtime_suspend>> percpu_ref_switch_to_atomic_sync(&q->q_usage_counter);
+ *   - drivers/md/md.c|2347| <<set_in_sync>> percpu_ref_switch_to_atomic_sync(&mddev->writes_pending);
+ */
 void percpu_ref_switch_to_atomic_sync(struct percpu_ref *ref)
 {
 	percpu_ref_switch_to_atomic(ref, NULL);
@@ -295,6 +414,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_atomic_sync);
  * mode.  If the caller ensures that @ref is not in the process of
  * switching to atomic mode, this function can be called from any context.
  */
+/*
+ * called by:
+ *   - block/blk-sysfs.c|926| <<blk_register_queue>> percpu_ref_switch_to_percpu(&q->q_usage_counter);
+ *   - drivers/md/md.c|2361| <<set_in_sync>> percpu_ref_switch_to_percpu(&mddev->writes_pending);
+ */
 void percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 {
 	unsigned long flags;
@@ -325,6 +449,16 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_percpu);
  *
  * There are no implied RCU grace periods between kill and release.
  */
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|734| <<nvmet_sq_destroy>> percpu_ref_kill_and_confirm(&sq->ref, nvmet_confirm_sq);
+ *   - include/linux/percpu-refcount.h|128| <<percpu_ref_kill>> percpu_ref_kill_and_confirm(ref, NULL);
+ *   - kernel/cgroup/cgroup.c|5218| <<kill_css>> percpu_ref_kill_and_confirm(&css->refcnt, css_killed_ref_fn);
+ *
+ * 调用的时候ref->percpu_count_ptr不能已经dead了
+ * 设置ref->percpu_count_ptr的dead, 调用__percpu_ref_switch_to_percpu()
+ * 最后put一下percpu_ref_put(ref)
+ */
 void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
 				 percpu_ref_func_t *confirm_kill)
 {
@@ -332,10 +466,17 @@ void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
 
 	spin_lock_irqsave(&percpu_ref_switch_lock, flags);
 
+	/* 调用的时候ref->percpu_count_ptr不能已经dead了 */
 	WARN_ONCE(ref->percpu_count_ptr & __PERCPU_REF_DEAD,
 		  "%s called more than once on %pf!", __func__, ref->release);
 
 	ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	/*
+	 * 只有在要转为atomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+	 * 否则调用__percpu_ref_switch_to_percpu()
+	 *
+	 * 因为上面设置了dead, 所以先面会调用__percpu_ref_switch_to_percpu()
+	 */
 	__percpu_ref_switch_mode(ref, confirm_kill);
 	percpu_ref_put(ref);
 
@@ -354,6 +495,10 @@ EXPORT_SYMBOL_GPL(percpu_ref_kill_and_confirm);
  * Note that percpu_ref_tryget[_live]() are safe to perform on @ref while
  * this function is in progress.
  */
+/*
+ * called by only:
+ *   - fs/io_uring.c|2976| <<__io_uring_register>> percpu_ref_reinit(&ctx->refs);
+ */
 void percpu_ref_reinit(struct percpu_ref *ref)
 {
 	WARN_ON_ONCE(!percpu_ref_is_zero(ref));
@@ -376,6 +521,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_reinit);
  * Note that percpu_ref_tryget[_live]() are safe to perform on @ref while
  * this function is in progress.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|206| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+ *   - lib/percpu-refcount.c|473| <<percpu_ref_reinit>> percpu_ref_resurrect(ref);
+ */
 void percpu_ref_resurrect(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
@@ -383,11 +533,24 @@ void percpu_ref_resurrect(struct percpu_ref *ref)
 
 	spin_lock_irqsave(&percpu_ref_switch_lock, flags);
 
+	/*
+	 * 到了这一步了必须是dead了才能resurrect
+	 */
 	WARN_ON_ONCE(!(ref->percpu_count_ptr & __PERCPU_REF_DEAD));
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	WARN_ON_ONCE(__ref_is_percpu(ref, &percpu_count));
 
 	ref->percpu_count_ptr &= ~__PERCPU_REF_DEAD;
 	percpu_ref_get(ref);
+	/*
+	 * 只有在要转为atomic或者dead的时候调用__percpu_ref_switch_to_atomic()
+	 * 否则调用__percpu_ref_switch_to_percpu()
+	 */
 	__percpu_ref_switch_mode(ref, NULL);
 
 	spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
-- 
2.7.4

