From 113e741692d2c8ebef25c30a1427094da56b712f Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 24 Mar 2025 12:57:33 -0700
Subject: [PATCH 1/1] linux-v6.14

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/msr-index.h     |  49 ++++
 arch/x86/include/asm/nospec-branch.h |  38 ++++
 arch/x86/include/asm/pgtable.h       |  16 ++
 arch/x86/kernel/alternative.c        |  34 +++
 arch/x86/kernel/cpu/bugs.c           | 323 +++++++++++++++++++++++++++
 arch/x86/kernel/cpu/common.c         |  16 ++
 arch/x86/kvm/mmu/spte.c              |  16 ++
 arch/x86/kvm/svm/svm.c               |   9 +
 arch/x86/kvm/vmx/nested.c            |   9 +
 arch/x86/kvm/vmx/vmx.c               |  57 +++++
 arch/x86/kvm/x86.c                   |  49 ++++
 arch/x86/mm/init.c                   |  16 ++
 arch/x86/mm/mmap.c                   |  16 ++
 arch/x86/mm/tlb.c                    |  52 +++++
 arch/x86/net/bpf_jit_comp.c          |  14 ++
 include/net/tc_wrapper.h             |  14 ++
 kernel/kthread.c                     |  10 +
 kernel/sched/core.c                  |  20 ++
 net/netfilter/nf_tables_core.c       |  14 ++
 19 files changed, 772 insertions(+)

diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 72765b2fe0d8..18171dffbf3a 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -67,6 +67,39 @@
 #define MSR_TEST_CTRL_SPLIT_LOCK_DETECT_BIT	29
 #define MSR_TEST_CTRL_SPLIT_LOCK_DETECT		BIT(MSR_TEST_CTRL_SPLIT_LOCK_DETECT_BIT)
 
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|101| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL, 
+ *   - arch/x86/kvm/x86.c|324| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|572| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|182| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|201| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|247| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1360| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|2936| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3095| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4251| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2058| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2320| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2345| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7270| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7281| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|13647| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|13649| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|13652| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ */
 #define MSR_IA32_SPEC_CTRL		0x00000048 /* Speculation Control */
 #define SPEC_CTRL_IBRS			BIT(0)	   /* Indirect Branch Restricted Speculation */
 #define SPEC_CTRL_STIBP_SHIFT		1	   /* Single Thread Indirect Branch Predictor (STIBP) bit */
@@ -83,6 +116,22 @@
 							| SPEC_CTRL_RRSBA_DIS_S \
 							| SPEC_CTRL_BHI_DIS_S)
 
+/*
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|102| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|575| <<firmware_restrict_branch_speculation_start>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|613| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4477| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7887| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3779| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3806| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
 #define MSR_IA32_PRED_CMD		0x00000049 /* Prediction Command */
 #define PRED_CMD_IBPB			BIT(0)	   /* Indirect Branch Prediction Barrier */
 #define PRED_CMD_SBPB			BIT(7)	   /* Selective Branch Prediction Barrier */
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index aee26bb8230f..8fc61992fac6 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -322,6 +322,14 @@
 #endif
 .endm
 
+/*
+ * 在以下使用X86_FEATURE_CLEAR_BHB_LOOP:
+ *   - arch/x86/include/asm/cpufeatures.h|479| <<global>> #define X86_FEATURE_CLEAR_BHB_LOOP (21*32+ 1)
+ *   - arch/x86/include/asm/nospec-branch.h|327| <<CLEAR_BRANCH_HISTORY>>
+ *                       ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
+ *   - arch/x86/kernel/cpu/bugs.c|1928| <<bhi_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+ *   - arch/x86/kernel/cpu/bugs.c|3114| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_LOOP))
+ */
 #ifdef CONFIG_X86_64
 .macro CLEAR_BRANCH_HISTORY
 	ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
@@ -444,6 +452,20 @@ static inline void call_depth_return_thunk(void) {}
 # define THUNK_TARGET(addr) [thunk_target] "r" (addr)
 
 #else /* CONFIG_X86_32 */
+/*
+ * 在以下使用X86_FEATURE_RETPOLINE:
+ *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+ *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+ *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, 
+ *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+ *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+ *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+ *
+ * v6.13或以前的在nospec-branch.h有两处
+ */
 /*
  * For i386 we use the original ret-equivalent retpoline, because
  * otherwise we'll run out of registers. We don't care about CET
@@ -519,8 +541,24 @@ void alternative_msr_write(unsigned int msr, u64 val, unsigned int feature)
 
 extern u64 x86_pred_cmd;
 
+/*
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ */
 static inline void indirect_branch_prediction_barrier(void)
 {
+	/*
+	 * 在以下使用X86_FEATURE_USE_IBPB:
+	 *   - arch/x86/include/asm/nospec-branch.h|547| <<indirect_branch_prediction_barrier>>
+	 *       alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+	 *   - arch/x86/kernel/cpu/bugs.c|1469| <<spectre_v2_user_select_mitigation>>
+	 *       setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
+	 */
 	alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
 }
 
diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
index 593f10aabd45..7f2957c8bf99 100644
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@ -1730,6 +1730,22 @@ extern bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot);
 
 static inline bool arch_has_pfn_modify_check(void)
 {
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	return boot_cpu_has_bug(X86_BUG_L1TF);
 }
 
diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c
index c71b575bf229..61838dcbb49e 100644
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@ -665,6 +665,20 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
 	BUG_ON(reg == 4);
 
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
 	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
 		if (cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))
@@ -1835,6 +1849,16 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 		leave_mm();
 
 	temp_state.mm = this_cpu_read(cpu_tlbstate.loaded_mm);
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(NULL, mm, current);
 
 	/*
@@ -1861,6 +1885,16 @@ static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 {
 	lockdep_assert_irqs_disabled();
 
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(NULL, prev_state.mm, current);
 
 	/* Clear the cpumask, to indicate no TLB flushing is needed anywhere */
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index a5d0998d7604..8aba1c7c0ef9 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -34,6 +34,209 @@
 
 #include "cpu.h"
 
+/*
+ * Reference:
+ * https://blogs.oracle.com/linux/post/an-update-on-meltdown-and-enhanced-ibrs
+ * https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2018/03/07/spectre-mitigation
+ * https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2018/03/24/retpoline
+ *
+ *
+ * IBRS:
+ * commit 2dbb887e875b1de3ca8f40ddf26bcfe55798c609
+ * Author: Peter Zijlstra <peterz@infradead.org>
+ * Date:   Tue Jun 14 23:15:53 2022 +0200
+ *
+ * x86/entry: Add kernel IBRS implementation
+ *
+ *
+ * STIBP:
+ * commit 53c613fe6349994f023245519265999eed75957f
+ * Author: Jiri Kosina <jikos@kernel.org>
+ * Date:   Tue Sep 25 14:38:55 2018 +0200
+ *
+ * x86/speculation: Enable cross-hyperthread spectre v2 STIBP mitigation
+ * 好像一直开着, 不是好像, 应该是就是 :)
+ *
+ *
+ * IBPB:
+ * commit 20ffa1caecca4db8f79fe665acdeaa5af815a24d
+ * Author: David Woodhouse <dwmw@amazon.co.uk>
+ * Date:   Thu Jan 25 16:14:15 2018 +0000
+ *
+ * x86/speculation: Add basic IBPB (Indirect Branch Prediction Barrier) support
+ * 请看下面关于IBPB
+ *
+ *
+ * RETPOLINE.
+ * commit 76b043848fd22dbf7f8bf3a1452f8c70d557b860
+ * Author: David Woodhouse <dwmw@amazon.co.uk>
+ * Date:   Thu Jan 11 21:46:25 2018 +0000
+ *
+ * x86/retpoline: Add initial retpoline support
+ * 主要依靠编译器的选项.
+ * 还有依靠汇编的CALL_NOSPEC, JMP_NOSPEC, RETPOLINE_CALL, RETPOLINE_JMP.
+ *
+ *
+ * RSB Filling.
+ * commit c995efd5a740d9cbafbf58bde4973e8b50b4d761
+ * Author: David Woodhouse <dwmw@amazon.co.uk>
+ * Date:   Fri Jan 12 17:49:25 2018 +0000
+ *
+ * x86/retpoline: Fill RSB on context switch for affected CPUs
+ *
+ * 用到X86_FEATURE_RSB_CTXSW的地方.
+ * arch/x86/entry/entry_32.S:	FILL_RETURN_BUFFER %ebx, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
+ * arch/x86/entry/entry_64.S:	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
+ * arch/x86/kernel/cpu/bugs.c:	setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
+ * arch/x86/kernel/cpu/bugs.c:			  boot_cpu_has(X86_FEATURE_RSB_CTXSW) ? "; RSB filling" : "",
+ *
+ *
+ * BHI.
+ * 注意:
+ * #define X86_FEATURE_CLEAR_BHB_LOOP      (21*32+ 1) // Clear branch history at syscall entry using SW loop
+ * #define X86_FEATURE_BHI_CTRL            (21*32+ 2) // BHI_DIS_S HW control available
+ * #define X86_FEATURE_CLEAR_BHB_HW        (21*32+ 3) // BHI_DIS_S HW control enabled
+ * #define X86_FEATURE_CLEAR_BHB_LOOP_ON_VMEXIT (21*32+ 4) // Clear branch history at vmexit using SW loop
+ */
+
+/*
+ * hv# cpuid -l 0x7 -1 | grep IBP
+ *       IBRS/IBPB: indirect branch restrictions  = true
+ *       STIBP: 1 thr indirect branch predictor   = true
+ *
+ * hv# cpuid -l 0x80000008 -1
+ * CPU:
+ *    Physical Address and Linear Address Size (0x80000008/eax):
+ *       maximum physical address bits         = 0x2e (46)
+ *       maximum linear (virtual) address bits = 0x30 (48)
+ *       maximum guest physical address bits   = 0x0 (0)
+ *    Extended Feature Extensions ID (0x80000008/ebx):
+ *       CLZERO instruction                       = false
+ *       instructions retired count support       = false
+ *       always save/restore error pointers       = false
+ *       INVLPGB instruction                      = false
+ *       RDPRU instruction                        = false
+ *       memory bandwidth enforcement             = false
+ *       MCOMMIT instruction                      = false
+ *       WBNOINVD instruction                     = false
+ *       IBPB: indirect branch prediction barrier = false
+ *       interruptible WBINVD, WBNOINVD           = false
+ *       IBRS: indirect branch restr speculation  = false
+ *       STIBP: 1 thr indirect branch predictor   = false
+ *       CPU prefers: IBRS always on              = false
+ *       CPU prefers: STIBP always on             = false
+ *       IBRS preferred over software solution    = false
+ *       IBRS provides same mode protection       = false
+ *       EFER[LMSLE] not supported                = false
+ *       INVLPGB supports TLB flush guest nested  = false
+ *       ppin processor id number supported       = false
+ *       SSBD: speculative store bypass disable   = false
+ *       virtualized SSBD                         = false
+ *       SSBD fixed in hardware                   = false
+ *       CPPC: collaborative processor perf ctrl  = false
+ *       PSFD: predictive store forward disable   = false
+ *       not vulnerable to branch type confusion  = false
+ *       branch sampling feature support          = false
+ *    Size Identifiers (0x80000008/ecx):
+ *       ApicIdCoreIdSize                    = 0x0 (0)
+ *       performance time-stamp counter size = 40 bits (0)
+ *    Feature Extended Size (0x80000008/edx):
+ *       max page count for INVLPGB instruction = 0x0 (0)
+ *       RDPRU instruction max input support    = 0x0 (0)
+ */
+
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+ *
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *
+ * 在以下使用X86_FEATURE_USE_IBPB:
+ *   - arch/x86/include/asm/nospec-branch.h|547| <<indirect_branch_prediction_barrier>>
+ *       alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/kernel/cpu/bugs.c|1469| <<spectre_v2_user_select_mitigation>>
+ *       setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
+ *
+ * 部分例子.
+ *
+ * context_switch()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ *
+ * switch_mm()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ */
+
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|101| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/x86.c|324| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|572| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|182| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|201| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|247| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1360| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|2936| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3095| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4251| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2058| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2320| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2345| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7270| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7281| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|13647| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|13649| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|13652| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ *
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|102| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|575| <<firmware_restrict_branch_speculation_start>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|613| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4477| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7887| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3779| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3806| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
+
 static void __init spectre_v1_select_mitigation(void);
 static void __init spectre_v2_select_mitigation(void);
 static void __init retbleed_select_mitigation(void);
@@ -61,6 +264,23 @@ EXPORT_PER_CPU_SYMBOL_GPL(x86_spec_ctrl_current);
 u64 x86_pred_cmd __ro_after_init = PRED_CMD_IBPB;
 EXPORT_SYMBOL_GPL(x86_pred_cmd);
 
+/*
+ * 在以下使用x86_arch_cap_msr:
+ *   - arch/x86/kernel/cpu/bugs.c|324| <<cpu_select_mitigations>> x86_arch_cap_msr = x86_read_arch_cap_msr(); 
+ *   - arch/x86/kernel/cpu/bugs.c|523| <<taa_select_mitigation>> if ( (x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+ *   - arch/x86/kernel/cpu/bugs.c|524| <<taa_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_TSX_CTRL_MSR))
+ *   - arch/x86/kernel/cpu/bugs.c|615| <<mmio_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_FBSDP_NO))
+ *   - arch/x86/kernel/cpu/bugs.c|625| <<mmio_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_FB_CLEAR) ||
+ *   - arch/x86/kernel/cpu/bugs.c|628| <<mmio_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_MDS_NO)))
+ *   - arch/x86/kernel/cpu/bugs.c|686| <<rfds_select_mitigation>> if (x86_arch_cap_msr & ARCH_CAP_RFDS_CLEAR)
+ *   - arch/x86/kernel/cpu/bugs.c|846| <<srbds_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+ *                                                                     !boot_cpu_has(X86_FEATURE_RTM) &&
+ *   - arch/x86/kernel/cpu/bugs.c|986| <<gds_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_GDS_CTRL)) {
+ *   - arch/x86/kernel/cpu/bugs.c|1750| <<spec_ctrl_disable_kernel_rrsba>> if (!(x86_arch_cap_msr & ARCH_CAP_RRSBA)) {
+ *   - arch/x86/kernel/cpu/bugs.c|2154| <<update_mds_branch_idle>> (x86_arch_cap_msr & ARCH_CAP_FBSDP_NO)) {
+ *
+ * 很多cap, 有security的, 也有其他的.
+ */
 static u64 __ro_after_init x86_arch_cap_msr;
 
 static DEFINE_MUTEX(spec_ctrl_mutex);
@@ -146,6 +366,23 @@ void __init cpu_select_mitigations(void)
 		x86_spec_ctrl_base &= ~SPEC_CTRL_MITIGATIONS_MASK;
 	}
 
+	/*
+	 * 在以下使用x86_arch_cap_msr:
+	 *   - arch/x86/kernel/cpu/bugs.c|324| <<cpu_select_mitigations>> x86_arch_cap_msr = x86_read_arch_cap_msr();
+	 *   - arch/x86/kernel/cpu/bugs.c|523| <<taa_select_mitigation>> if ( (x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|524| <<taa_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_TSX_CTRL_MSR))
+	 *   - arch/x86/kernel/cpu/bugs.c|615| <<mmio_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_FBSDP_NO))
+	 *   - arch/x86/kernel/cpu/bugs.c|625| <<mmio_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_FB_CLEAR) ||
+	 *   - arch/x86/kernel/cpu/bugs.c|628| <<mmio_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_MDS_NO)))
+	 *   - arch/x86/kernel/cpu/bugs.c|686| <<rfds_select_mitigation>> if (x86_arch_cap_msr & ARCH_CAP_RFDS_CLEAR)
+	 *   - arch/x86/kernel/cpu/bugs.c|846| <<srbds_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+	 *                                                                     !boot_cpu_has(X86_FEATURE_RTM) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|986| <<gds_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_GDS_CTRL)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|1750| <<spec_ctrl_disable_kernel_rrsba>> if (!(x86_arch_cap_msr & ARCH_CAP_RRSBA)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2154| <<update_mds_branch_idle>> (x86_arch_cap_msr & ARCH_CAP_FBSDP_NO)) {
+	 *
+	 * 很多cap, 有security的, 也有其他的.
+	 */
 	x86_arch_cap_msr = x86_read_arch_cap_msr();
 
 	/* Select the proper CPU mitigations before patching alternatives: */
@@ -1364,6 +1601,13 @@ spectre_v2_user_select_mitigation(void)
 
 	/* Initialize Indirect Branch Prediction Barrier */
 	if (boot_cpu_has(X86_FEATURE_IBPB)) {
+		/*
+		 * 在以下使用X86_FEATURE_USE_IBPB:
+		 *   - arch/x86/include/asm/nospec-branch.h|547| <<indirect_branch_prediction_barrier>>
+		 *       alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+		 *   - arch/x86/kernel/cpu/bugs.c|1469| <<spectre_v2_user_select_mitigation>>
+		 *       setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
+		 */
 		setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
 
 		spectre_v2_user_ibpb = mode;
@@ -1673,6 +1917,20 @@ static void __init bhi_select_mitigation(void)
 	if (bhi_mitigation == BHI_MITIGATION_OFF)
 		return;
 
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	/* Retpoline mitigates against BHI unless the CPU has RRSBA behavior */
 	if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
 	    !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE)) {
@@ -1694,6 +1952,27 @@ static void __init bhi_select_mitigation(void)
 		return;
 	}
 
+	/*
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_LOOP:
+	 *   - arch/x86/include/asm/cpufeatures.h|479| <<global>> #define X86_FEATURE_CLEAR_BHB_LOOP (21*32+ 1) 
+	 *   - arch/x86/include/asm/nospec-branch.h|327| <<CLEAR_BRANCH_HISTORY>>
+	 *                       ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
+	 *   - arch/x86/kernel/cpu/bugs.c|1928| <<bhi_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+	 *   - arch/x86/kernel/cpu/bugs.c|3114| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_LOOP))
+	 *
+	 *
+	 * 使用CLEAR_BRANCH_HISTORY和CLEAR_BRANCH_HISTORY_VMEXIT的地方:
+	 * arch/x86/entry/common.c: * 4) int80_emulation() does a CLEAR_BRANCH_HISTORY. While FRED will
+	 * arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/entry/entry_64.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/include/asm/nospec-branch.h:.macro CLEAR_BRANCH_HISTORY
+	 * arch/x86/include/asm/nospec-branch.h:.macro CLEAR_BRANCH_HISTORY_VMEXIT
+	 * arch/x86/include/asm/nospec-branch.h:#define CLEAR_BRANCH_HISTORY
+	 * arch/x86/include/asm/nospec-branch.h:#define CLEAR_BRANCH_HISTORY_VMEXIT
+	 * arch/x86/kvm/vmx/vmenter.S:	CLEAR_BRANCH_HISTORY_VMEXIT
+	 */
 	pr_info("Spectre BHI mitigation: SW BHB clearing on syscall and VM exit\n");
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP_ON_VMEXIT);
@@ -1796,6 +2075,20 @@ static void __init spectre_v2_select_mitigation(void)
 
 	case SPECTRE_V2_RETPOLINE:
 	case SPECTRE_V2_EIBRS_RETPOLINE:
+		/*
+		 * 在以下使用X86_FEATURE_RETPOLINE:
+		 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+		 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+		 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+		 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+		 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+		 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+		 *
+		 * v6.13或以前的在nospec-branch.h有两处
+		 */
 		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
 		break;
 	}
@@ -1854,6 +2147,11 @@ static void __init spectre_v2_select_mitigation(void)
 	 *
 	 * FIXME: Is this pointless for retbleed-affected AMD?
 	 */
+	/*
+	 * 在以下使用X86_FEATURE_RSB_CTXSW:
+	 *   - arch/x86/kernel/cpu/bugs.c|2101| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW); 
+	 *   - arch/x86/kernel/cpu/bugs.c|3143| <<spectre_v2_show_state>> boot_cpu_has(X86_FEATURE_RSB_CTXSW) ? "; RSB filling" : "",
+	 */
 	setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
 	pr_info("Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\n");
 
@@ -2272,6 +2570,15 @@ static int ib_prctl_set(struct task_struct *task, unsigned long ctrl)
 		if (ctrl == PR_SPEC_FORCE_DISABLE)
 			task_set_spec_ib_force_disable(task);
 		task_update_spec_tif(task);
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (task == current)
 			indirect_branch_prediction_barrier();
 		break;
@@ -2443,6 +2750,22 @@ static void __init l1tf_select_mitigation(void)
 {
 	u64 half_pa;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_L1TF))
 		return;
 
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7cce91b19fb2..dfd105a5352d 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1458,6 +1458,22 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (cpu_matches(cpu_vuln_whitelist, NO_L1TF))
 		return;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	setup_force_cpu_bug(X86_BUG_L1TF);
 }
 
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 22551e2f1d00..0e4952cdc1b0 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -495,6 +495,22 @@ void kvm_mmu_reset_all_pte_masks(void)
 	 */
 	shadow_nonpresent_or_rsvd_mask = 0;
 	low_phys_bits = boot_cpu_data.x86_phys_bits;
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (boot_cpu_has_bug(X86_BUG_L1TF) &&
 	    !WARN_ON_ONCE(boot_cpu_data.x86_cache_bits >=
 			  52 - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)) {
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index e67de787fc71..27c353b76bd1 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -1559,6 +1559,15 @@ static void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	if (sd->current_vmcb != svm->vmcb) {
 		sd->current_vmcb = svm->vmcb;
 
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (!cpu_feature_enabled(X86_FEATURE_IBPB_ON_VMEXIT))
 			indirect_branch_prediction_barrier();
 	}
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index ed8a3cb53961..750f18194b86 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -5026,6 +5026,15 @@ void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	 * doesn't isolate different VMCSs, i.e. in this case, doesn't provide
 	 * separate modes for L2 vs L1.
 	 */
+	/*
+	 * 在以下使用indirect_branch_prediction_barrier():
+	 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+	 */
 	if (guest_cpu_cap_has(vcpu, X86_FEATURE_SPEC_CTRL))
 		indirect_branch_prediction_barrier();
 
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 3b92f893b239..bc9d7fdd4b95 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -252,6 +252,22 @@ static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
 	struct page *page;
 	unsigned int i;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
 		l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
 		return 0;
@@ -345,6 +361,22 @@ static int vmentry_l1d_flush_set(const char *s, const struct kernel_param *kp)
 	if (l1tf < 0)
 		return l1tf;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has(X86_BUG_L1TF))
 		return 0;
 
@@ -1477,6 +1509,15 @@ void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
 		 * performs IBPB on nested VM-Exit (a single nested transition
 		 * may switch the active VMCS multiple times).
 		 */
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (!buddy || WARN_ON_ONCE(buddy->vmcs != prev))
 			indirect_branch_prediction_barrier();
 	}
@@ -7645,6 +7686,22 @@ int vmx_vm_init(struct kvm *kvm)
 	if (!ple_gap)
 		kvm->arch.pause_in_guest = true;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
 		switch (l1tf_mitigation) {
 		case L1TF_MITIGATION_OFF:
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 4b64ab350bcd..ca6ad224deac 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -89,6 +89,55 @@
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|101| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/x86.c|324| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|572| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|182| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|201| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|247| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1360| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|2936| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3095| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4251| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2058| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2320| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2345| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7270| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7281| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|13647| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|13649| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|13652| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ *
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|102| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|575| <<firmware_restrict_branch_speculation_start>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|613| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4477| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7887| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3779| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3806| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
+
 #define MAX_IO_MSRS 256
 #define KVM_MAX_MCE_BANKS 32
 
diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c
index 62aa4d66a032..5bf8824dbc92 100644
--- a/arch/x86/mm/init.c
+++ b/arch/x86/mm/init.c
@@ -1039,6 +1039,22 @@ unsigned long arch_max_swapfile_size(void)
 
 	pages = generic_max_swapfile_size();
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
 		/* Limit the swap file size to MAX_PA/2 for L1TF workaround */
 		unsigned long long l1tf_limit = l1tf_pfn_limit();
diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c
index b8a6ffffb451..f96d032a8096 100644
--- a/arch/x86/mm/mmap.c
+++ b/arch/x86/mm/mmap.c
@@ -232,6 +232,22 @@ int valid_mmap_phys_addr_range(unsigned long pfn, size_t count)
  */
 bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot)
 {
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_L1TF))
 		return true;
 	if (!__pte_needs_invert(pgprot_val(prot)))
diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c
index 6cf881a942bb..a4185e7b4a5e 100644
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@ -325,6 +325,16 @@ void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	unsigned long flags;
 
 	local_irq_save(flags);
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(NULL, next, tsk);
 	local_irq_restore(flags);
 }
@@ -378,6 +388,10 @@ static unsigned long mm_mangle_tif_spec_bits(struct task_struct *next)
 	return (unsigned long)next->mm | spec_bits;
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/tlb.c|645| <<switch_mm_irqs_off>> cond_mitigation(tsk);
+ */
 static void cond_mitigation(struct task_struct *next)
 {
 	unsigned long prev_mm, next_mm;
@@ -436,6 +450,15 @@ static void cond_mitigation(struct task_struct *next)
 		 * Issue IBPB only if the mm's are different and one or
 		 * both have the IBPB bit set.
 		 */
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (next_mm != prev_mm &&
 		    (next_mm | prev_mm) & LAST_USER_MM_IBPB)
 			indirect_branch_prediction_barrier();
@@ -447,6 +470,15 @@ static void cond_mitigation(struct task_struct *next)
 		 * different context than the user space task which ran
 		 * last on this CPU.
 		 */
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if ((prev_mm & ~LAST_USER_MM_SPEC_MASK) !=
 					(unsigned long)next->mm)
 			indirect_branch_prediction_barrier();
@@ -496,6 +528,16 @@ static inline void cr4_update_pce_mm(struct mm_struct *mm) { }
  * 'cpu_tlbstate.loaded_mm' instead because it does not always keep
  * 'current->active_mm' up to date.
  */
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+ */
 void switch_mm_irqs_off(struct mm_struct *unused, struct mm_struct *next,
 			struct task_struct *tsk)
 {
@@ -782,6 +824,16 @@ static void flush_tlb_func(void *info)
 		 * This should be rare, with native_flush_tlb_multi() skipping
 		 * IPIs to lazy TLB mode CPUs.
 		 */
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+		 */
 		switch_mm_irqs_off(NULL, &init_mm, NULL);
 		return;
 	}
diff --git a/arch/x86/net/bpf_jit_comp.c b/arch/x86/net/bpf_jit_comp.c
index a43fc5af973d..e35491c116b2 100644
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@ -657,6 +657,20 @@ static void emit_indirect_jump(u8 **pprog, int reg, u8 *ip)
 		EMIT_LFENCE();
 		EMIT2(0xFF, 0xE0 + reg);
 	} else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+		/*
+		 * 在以下使用X86_FEATURE_RETPOLINE:
+		 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+		 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+		 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+		 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+		 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+		 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+		 *
+		 * v6.13或以前的在nospec-branch.h有两处
+		 */
 		OPTIMIZER_HIDE_VAR(reg);
 		if (cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))
 			emit_jump(&prog, &__x86_indirect_jump_thunk_array[reg], ip);
diff --git a/include/net/tc_wrapper.h b/include/net/tc_wrapper.h
index ffe58a02537c..985729c0e8f5 100644
--- a/include/net/tc_wrapper.h
+++ b/include/net/tc_wrapper.h
@@ -202,6 +202,20 @@ static inline int tc_classify(struct sk_buff *skb, const struct tcf_proto *tp,
 static inline void tc_wrapper_init(void)
 {
 #ifdef CONFIG_X86
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
 		static_branch_enable(&tc_skip_wrapper);
 #endif
diff --git a/kernel/kthread.c b/kernel/kthread.c
index 5dc5b0d7238e..8bf33b737f41 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -1618,6 +1618,16 @@ void kthread_use_mm(struct mm_struct *mm)
 	tsk->active_mm = mm;
 	tsk->mm = mm;
 	membarrier_update_current_mm(mm);
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(active_mm, mm, tsk);
 	local_irq_enable();
 	task_unlock(tsk);
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 042351c7afce..4bf21060b2b7 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -5359,6 +5359,16 @@ context_switch(struct rq *rq, struct task_struct *prev,
 		 * case 'prev->active_mm == next->mm' through
 		 * finish_task_switch()'s mmdrop().
 		 */
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+		 */
 		switch_mm_irqs_off(prev->active_mm, next->mm, next);
 		lru_gen_use_mm(next->mm);
 
@@ -7955,6 +7965,16 @@ static void sched_force_init_mm(void)
 		mmgrab_lazy_tlb(&init_mm);
 		local_irq_disable();
 		current->active_mm = &init_mm;
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+		 */
 		switch_mm_irqs_off(mm, &init_mm, current);
 		local_irq_enable();
 		finish_arch_post_lock_switch();
diff --git a/net/netfilter/nf_tables_core.c b/net/netfilter/nf_tables_core.c
index 75598520b0fa..079c7faecfde 100644
--- a/net/netfilter/nf_tables_core.c
+++ b/net/netfilter/nf_tables_core.c
@@ -32,6 +32,20 @@ static bool nf_skip_indirect_calls(void)
 
 static void __init nf_skip_indirect_calls_enable(void)
 {
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
 		static_branch_enable(&nf_tables_skip_direct_calls);
 }
-- 
2.39.5 (Apple Git-154)

