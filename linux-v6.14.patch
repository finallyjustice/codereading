From ba7f6eb8e12c3683aa7347bb6f165c928abbabd8 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 31 Mar 2025 08:59:12 -0700
Subject: [PATCH 1/1] linux-v6.14

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/events/zhaoxin/core.c       |   4 +
 arch/x86/include/asm/hardirq.h       |   6 +
 arch/x86/include/asm/kvm_host.h      |  15 +
 arch/x86/include/asm/msr-index.h     |  49 ++++
 arch/x86/include/asm/nospec-branch.h |  38 +++
 arch/x86/include/asm/pgtable.h       |  16 ++
 arch/x86/kernel/alternative.c        |  34 +++
 arch/x86/kernel/cpu/bugs.c           | 396 +++++++++++++++++++++++++++
 arch/x86/kernel/cpu/common.c         |  16 ++
 arch/x86/kernel/irq.c                |  19 ++
 arch/x86/kvm/lapic.c                 |  32 +++
 arch/x86/kvm/mmu/mmu.c               |  10 +
 arch/x86/kvm/mmu/spte.c              |  16 ++
 arch/x86/kvm/svm/svm.c               |   9 +
 arch/x86/kvm/vmx/nested.c            |  26 ++
 arch/x86/kvm/vmx/vmx.c               | 254 +++++++++++++++++
 arch/x86/kvm/x86.c                   | 154 +++++++++++
 arch/x86/mm/init.c                   |  16 ++
 arch/x86/mm/mmap.c                   |  16 ++
 arch/x86/mm/tlb.c                    |  52 ++++
 arch/x86/net/bpf_jit_comp.c          |  14 +
 drivers/vhost/net.c                  |  11 +
 drivers/vhost/scsi.c                 |  26 ++
 drivers/vhost/vhost.c                | 101 +++++++
 include/net/tc_wrapper.h             |  14 +
 kernel/cpu.c                         |  23 ++
 kernel/kthread.c                     |  10 +
 kernel/sched/core.c                  |  20 ++
 net/netfilter/nf_tables_core.c       |  14 +
 29 files changed, 1411 insertions(+)

diff --git a/arch/x86/events/zhaoxin/core.c b/arch/x86/events/zhaoxin/core.c
index 2fd9b0cf9..0011863ca 100644
--- a/arch/x86/events/zhaoxin/core.c
+++ b/arch/x86/events/zhaoxin/core.c
@@ -502,6 +502,10 @@ static __init void zhaoxin_arch_events_quirk(void)
 	}
 }
 
+/*
+ * 处理X86_VENDOR_ZHAOXIN:
+ *   - arch/x86/events/core.c|2076| <<init_hw_perf_events>> err = zhaoxin_pmu_init();
+ */
 __init int zhaoxin_pmu_init(void)
 {
 	union cpuid10_edx edx;
diff --git a/arch/x86/include/asm/hardirq.h b/arch/x86/include/asm/hardirq.h
index 6ffa8b75f..6c8139607 100644
--- a/arch/x86/include/asm/hardirq.h
+++ b/arch/x86/include/asm/hardirq.h
@@ -73,6 +73,12 @@ extern u64 arch_irq_stat(void);
  * This function is called from noinstr interrupt contexts
  * and must be inlined to not get instrumentation.
  */
+/*
+ * 在以下使用kvm_set_cpu_l1tf_flush_l1d():
+ *   - arch/x86/include/asm/idtentry.h|215| <<DEFINE_IDTENTRY_IRQ>> kvm_set_cpu_l1tf_flush_l1d(); \
+ *   - arch/x86/include/asm/idtentry.h|260| <<DEFINE_IDTENTRY_SYSVEC>> kvm_set_cpu_l1tf_flush_l1d(); \
+ *   - arch/x86/include/asm/idtentry.h|299| <<DEFINE_IDTENTRY_SYSVEC_SIMPLE>> kvm_set_cpu_l1tf_flush_l1d(); \
+ */
 static __always_inline void kvm_set_cpu_l1tf_flush_l1d(void)
 {
 	__this_cpu_write(irq_stat.kvm_cpu_l1tf_flush_l1d, 1);
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 32ae3aa50..de0ad8140 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1027,6 +1027,16 @@ struct kvm_vcpu_arch {
 	/* be preempted when it's in kernel-mode(cpl=0) */
 	bool preempted_in_kernel;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	/* Flush the L1 Data cache for L1TF mitigation on VMENTER */
 	bool l1tf_flush_l1d;
 
@@ -1603,6 +1613,11 @@ struct kvm_vcpu_stat {
 	u64 signal_exits;
 	u64 irq_window_exits;
 	u64 nmi_window_exits;
+	/*
+	 * 在以下使用kvm_vcpu_stat->l1d_flush:
+	 *   - arch/x86/kvm/x86.c|322| <<global>> STATS_DESC_COUNTER(VCPU, l1d_flush),
+	 *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> vcpu->stat.l1d_flush++;
+	 */
 	u64 l1d_flush;
 	u64 halt_exits;
 	u64 request_irq_exits;
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 72765b2fe..18171dffb 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -67,6 +67,39 @@
 #define MSR_TEST_CTRL_SPLIT_LOCK_DETECT_BIT	29
 #define MSR_TEST_CTRL_SPLIT_LOCK_DETECT		BIT(MSR_TEST_CTRL_SPLIT_LOCK_DETECT_BIT)
 
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|101| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL, 
+ *   - arch/x86/kvm/x86.c|324| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|572| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|182| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|201| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|247| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1360| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|2936| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3095| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4251| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2058| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2320| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2345| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7270| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7281| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|13647| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|13649| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|13652| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ */
 #define MSR_IA32_SPEC_CTRL		0x00000048 /* Speculation Control */
 #define SPEC_CTRL_IBRS			BIT(0)	   /* Indirect Branch Restricted Speculation */
 #define SPEC_CTRL_STIBP_SHIFT		1	   /* Single Thread Indirect Branch Predictor (STIBP) bit */
@@ -83,6 +116,22 @@
 							| SPEC_CTRL_RRSBA_DIS_S \
 							| SPEC_CTRL_BHI_DIS_S)
 
+/*
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|102| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|575| <<firmware_restrict_branch_speculation_start>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|613| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4477| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7887| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3779| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3806| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
 #define MSR_IA32_PRED_CMD		0x00000049 /* Prediction Command */
 #define PRED_CMD_IBPB			BIT(0)	   /* Indirect Branch Prediction Barrier */
 #define PRED_CMD_SBPB			BIT(7)	   /* Selective Branch Prediction Barrier */
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index aee26bb82..8fc61992f 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -322,6 +322,14 @@
 #endif
 .endm
 
+/*
+ * 在以下使用X86_FEATURE_CLEAR_BHB_LOOP:
+ *   - arch/x86/include/asm/cpufeatures.h|479| <<global>> #define X86_FEATURE_CLEAR_BHB_LOOP (21*32+ 1)
+ *   - arch/x86/include/asm/nospec-branch.h|327| <<CLEAR_BRANCH_HISTORY>>
+ *                       ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
+ *   - arch/x86/kernel/cpu/bugs.c|1928| <<bhi_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+ *   - arch/x86/kernel/cpu/bugs.c|3114| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_LOOP))
+ */
 #ifdef CONFIG_X86_64
 .macro CLEAR_BRANCH_HISTORY
 	ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
@@ -444,6 +452,20 @@ static inline void call_depth_return_thunk(void) {}
 # define THUNK_TARGET(addr) [thunk_target] "r" (addr)
 
 #else /* CONFIG_X86_32 */
+/*
+ * 在以下使用X86_FEATURE_RETPOLINE:
+ *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+ *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+ *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, 
+ *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+ *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+ *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+ *
+ * v6.13或以前的在nospec-branch.h有两处
+ */
 /*
  * For i386 we use the original ret-equivalent retpoline, because
  * otherwise we'll run out of registers. We don't care about CET
@@ -519,8 +541,24 @@ void alternative_msr_write(unsigned int msr, u64 val, unsigned int feature)
 
 extern u64 x86_pred_cmd;
 
+/*
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ */
 static inline void indirect_branch_prediction_barrier(void)
 {
+	/*
+	 * 在以下使用X86_FEATURE_USE_IBPB:
+	 *   - arch/x86/include/asm/nospec-branch.h|547| <<indirect_branch_prediction_barrier>>
+	 *       alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+	 *   - arch/x86/kernel/cpu/bugs.c|1469| <<spectre_v2_user_select_mitigation>>
+	 *       setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
+	 */
 	alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
 }
 
diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
index 593f10aab..7f2957c8b 100644
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@ -1730,6 +1730,22 @@ extern bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot);
 
 static inline bool arch_has_pfn_modify_check(void)
 {
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	return boot_cpu_has_bug(X86_BUG_L1TF);
 }
 
diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c
index c71b575bf..61838dcbb 100644
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@ -665,6 +665,20 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
 	BUG_ON(reg == 4);
 
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
 	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
 		if (cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))
@@ -1835,6 +1849,16 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 		leave_mm();
 
 	temp_state.mm = this_cpu_read(cpu_tlbstate.loaded_mm);
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(NULL, mm, current);
 
 	/*
@@ -1861,6 +1885,16 @@ static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 {
 	lockdep_assert_irqs_disabled();
 
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(NULL, prev_state.mm, current);
 
 	/* Clear the cpumask, to indicate no TLB flushing is needed anywhere */
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index a5d0998d7..54de1c4d0 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -34,6 +34,209 @@
 
 #include "cpu.h"
 
+/*
+ * Reference:
+ * https://blogs.oracle.com/linux/post/an-update-on-meltdown-and-enhanced-ibrs
+ * https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2018/03/07/spectre-mitigation
+ * https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2018/03/24/retpoline
+ *
+ *
+ * IBRS:
+ * commit 2dbb887e875b1de3ca8f40ddf26bcfe55798c609
+ * Author: Peter Zijlstra <peterz@infradead.org>
+ * Date:   Tue Jun 14 23:15:53 2022 +0200
+ *
+ * x86/entry: Add kernel IBRS implementation
+ *
+ *
+ * STIBP:
+ * commit 53c613fe6349994f023245519265999eed75957f
+ * Author: Jiri Kosina <jikos@kernel.org>
+ * Date:   Tue Sep 25 14:38:55 2018 +0200
+ *
+ * x86/speculation: Enable cross-hyperthread spectre v2 STIBP mitigation
+ * 好像一直开着, 不是好像, 应该是就是 :)
+ *
+ *
+ * IBPB:
+ * commit 20ffa1caecca4db8f79fe665acdeaa5af815a24d
+ * Author: David Woodhouse <dwmw@amazon.co.uk>
+ * Date:   Thu Jan 25 16:14:15 2018 +0000
+ *
+ * x86/speculation: Add basic IBPB (Indirect Branch Prediction Barrier) support
+ * 请看下面关于IBPB
+ *
+ *
+ * RETPOLINE.
+ * commit 76b043848fd22dbf7f8bf3a1452f8c70d557b860
+ * Author: David Woodhouse <dwmw@amazon.co.uk>
+ * Date:   Thu Jan 11 21:46:25 2018 +0000
+ *
+ * x86/retpoline: Add initial retpoline support
+ * 主要依靠编译器的选项.
+ * 还有依靠汇编的CALL_NOSPEC, JMP_NOSPEC, RETPOLINE_CALL, RETPOLINE_JMP.
+ *
+ *
+ * RSB Filling.
+ * commit c995efd5a740d9cbafbf58bde4973e8b50b4d761
+ * Author: David Woodhouse <dwmw@amazon.co.uk>
+ * Date:   Fri Jan 12 17:49:25 2018 +0000
+ *
+ * x86/retpoline: Fill RSB on context switch for affected CPUs
+ *
+ * 用到X86_FEATURE_RSB_CTXSW的地方.
+ * arch/x86/entry/entry_32.S:	FILL_RETURN_BUFFER %ebx, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
+ * arch/x86/entry/entry_64.S:	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
+ * arch/x86/kernel/cpu/bugs.c:	setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
+ * arch/x86/kernel/cpu/bugs.c:			  boot_cpu_has(X86_FEATURE_RSB_CTXSW) ? "; RSB filling" : "",
+ *
+ *
+ * BHI.
+ * 注意:
+ * #define X86_FEATURE_CLEAR_BHB_LOOP      (21*32+ 1) // Clear branch history at syscall entry using SW loop
+ * #define X86_FEATURE_BHI_CTRL            (21*32+ 2) // BHI_DIS_S HW control available
+ * #define X86_FEATURE_CLEAR_BHB_HW        (21*32+ 3) // BHI_DIS_S HW control enabled
+ * #define X86_FEATURE_CLEAR_BHB_LOOP_ON_VMEXIT (21*32+ 4) // Clear branch history at vmexit using SW loop
+ */
+
+/*
+ * hv# cpuid -l 0x7 -1 | grep IBP
+ *       IBRS/IBPB: indirect branch restrictions  = true
+ *       STIBP: 1 thr indirect branch predictor   = true
+ *
+ * hv# cpuid -l 0x80000008 -1
+ * CPU:
+ *    Physical Address and Linear Address Size (0x80000008/eax):
+ *       maximum physical address bits         = 0x2e (46)
+ *       maximum linear (virtual) address bits = 0x30 (48)
+ *       maximum guest physical address bits   = 0x0 (0)
+ *    Extended Feature Extensions ID (0x80000008/ebx):
+ *       CLZERO instruction                       = false
+ *       instructions retired count support       = false
+ *       always save/restore error pointers       = false
+ *       INVLPGB instruction                      = false
+ *       RDPRU instruction                        = false
+ *       memory bandwidth enforcement             = false
+ *       MCOMMIT instruction                      = false
+ *       WBNOINVD instruction                     = false
+ *       IBPB: indirect branch prediction barrier = false
+ *       interruptible WBINVD, WBNOINVD           = false
+ *       IBRS: indirect branch restr speculation  = false
+ *       STIBP: 1 thr indirect branch predictor   = false
+ *       CPU prefers: IBRS always on              = false
+ *       CPU prefers: STIBP always on             = false
+ *       IBRS preferred over software solution    = false
+ *       IBRS provides same mode protection       = false
+ *       EFER[LMSLE] not supported                = false
+ *       INVLPGB supports TLB flush guest nested  = false
+ *       ppin processor id number supported       = false
+ *       SSBD: speculative store bypass disable   = false
+ *       virtualized SSBD                         = false
+ *       SSBD fixed in hardware                   = false
+ *       CPPC: collaborative processor perf ctrl  = false
+ *       PSFD: predictive store forward disable   = false
+ *       not vulnerable to branch type confusion  = false
+ *       branch sampling feature support          = false
+ *    Size Identifiers (0x80000008/ecx):
+ *       ApicIdCoreIdSize                    = 0x0 (0)
+ *       performance time-stamp counter size = 40 bits (0)
+ *    Feature Extended Size (0x80000008/edx):
+ *       max page count for INVLPGB instruction = 0x0 (0)
+ *       RDPRU instruction max input support    = 0x0 (0)
+ */
+
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+ *
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *
+ * 在以下使用X86_FEATURE_USE_IBPB:
+ *   - arch/x86/include/asm/nospec-branch.h|547| <<indirect_branch_prediction_barrier>>
+ *       alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/kernel/cpu/bugs.c|1469| <<spectre_v2_user_select_mitigation>>
+ *       setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
+ *
+ * 部分例子.
+ *
+ * context_switch()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ *
+ * switch_mm()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ */
+
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|101| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/x86.c|324| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|572| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|182| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|201| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|247| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1360| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|2936| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3095| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4251| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2058| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2320| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2345| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7270| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7281| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|13647| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|13649| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|13652| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ *
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|102| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|575| <<firmware_restrict_branch_speculation_start>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|613| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4477| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7887| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3779| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3806| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
+
 static void __init spectre_v1_select_mitigation(void);
 static void __init spectre_v2_select_mitigation(void);
 static void __init retbleed_select_mitigation(void);
@@ -61,6 +264,23 @@ EXPORT_PER_CPU_SYMBOL_GPL(x86_spec_ctrl_current);
 u64 x86_pred_cmd __ro_after_init = PRED_CMD_IBPB;
 EXPORT_SYMBOL_GPL(x86_pred_cmd);
 
+/*
+ * 在以下使用x86_arch_cap_msr:
+ *   - arch/x86/kernel/cpu/bugs.c|324| <<cpu_select_mitigations>> x86_arch_cap_msr = x86_read_arch_cap_msr(); 
+ *   - arch/x86/kernel/cpu/bugs.c|523| <<taa_select_mitigation>> if ( (x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+ *   - arch/x86/kernel/cpu/bugs.c|524| <<taa_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_TSX_CTRL_MSR))
+ *   - arch/x86/kernel/cpu/bugs.c|615| <<mmio_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_FBSDP_NO))
+ *   - arch/x86/kernel/cpu/bugs.c|625| <<mmio_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_FB_CLEAR) ||
+ *   - arch/x86/kernel/cpu/bugs.c|628| <<mmio_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_MDS_NO)))
+ *   - arch/x86/kernel/cpu/bugs.c|686| <<rfds_select_mitigation>> if (x86_arch_cap_msr & ARCH_CAP_RFDS_CLEAR)
+ *   - arch/x86/kernel/cpu/bugs.c|846| <<srbds_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+ *                                                                     !boot_cpu_has(X86_FEATURE_RTM) &&
+ *   - arch/x86/kernel/cpu/bugs.c|986| <<gds_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_GDS_CTRL)) {
+ *   - arch/x86/kernel/cpu/bugs.c|1750| <<spec_ctrl_disable_kernel_rrsba>> if (!(x86_arch_cap_msr & ARCH_CAP_RRSBA)) {
+ *   - arch/x86/kernel/cpu/bugs.c|2154| <<update_mds_branch_idle>> (x86_arch_cap_msr & ARCH_CAP_FBSDP_NO)) {
+ *
+ * 很多cap, 有security的, 也有其他的.
+ */
 static u64 __ro_after_init x86_arch_cap_msr;
 
 static DEFINE_MUTEX(spec_ctrl_mutex);
@@ -146,6 +366,23 @@ void __init cpu_select_mitigations(void)
 		x86_spec_ctrl_base &= ~SPEC_CTRL_MITIGATIONS_MASK;
 	}
 
+	/*
+	 * 在以下使用x86_arch_cap_msr:
+	 *   - arch/x86/kernel/cpu/bugs.c|324| <<cpu_select_mitigations>> x86_arch_cap_msr = x86_read_arch_cap_msr();
+	 *   - arch/x86/kernel/cpu/bugs.c|523| <<taa_select_mitigation>> if ( (x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|524| <<taa_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_TSX_CTRL_MSR))
+	 *   - arch/x86/kernel/cpu/bugs.c|615| <<mmio_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_FBSDP_NO))
+	 *   - arch/x86/kernel/cpu/bugs.c|625| <<mmio_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_FB_CLEAR) ||
+	 *   - arch/x86/kernel/cpu/bugs.c|628| <<mmio_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_MDS_NO)))
+	 *   - arch/x86/kernel/cpu/bugs.c|686| <<rfds_select_mitigation>> if (x86_arch_cap_msr & ARCH_CAP_RFDS_CLEAR)
+	 *   - arch/x86/kernel/cpu/bugs.c|846| <<srbds_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+	 *                                                                     !boot_cpu_has(X86_FEATURE_RTM) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|986| <<gds_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_GDS_CTRL)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|1750| <<spec_ctrl_disable_kernel_rrsba>> if (!(x86_arch_cap_msr & ARCH_CAP_RRSBA)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2154| <<update_mds_branch_idle>> (x86_arch_cap_msr & ARCH_CAP_FBSDP_NO)) {
+	 *
+	 * 很多cap, 有security的, 也有其他的.
+	 */
 	x86_arch_cap_msr = x86_read_arch_cap_msr();
 
 	/* Select the proper CPU mitigations before patching alternatives: */
@@ -256,6 +493,16 @@ static void __init mds_select_mitigation(void)
 
 		setup_force_cpu_cap(X86_FEATURE_CLEAR_CPU_BUF);
 
+		/*
+		 * 在以下调用cpu_smt_disable():
+		 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+		 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+		 */
 		if (!boot_cpu_has(X86_BUG_MSBDS_ONLY) &&
 		    (mds_nosmt || cpu_mitigations_auto_nosmt()))
 			cpu_smt_disable(false);
@@ -358,6 +605,16 @@ static void __init taa_select_mitigation(void)
 	 */
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_CPU_BUF);
 
+	/*
+	 * 在以下调用cpu_smt_disable():
+	 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+	 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+	 */
 	if (taa_nosmt || cpu_mitigations_auto_nosmt())
 		cpu_smt_disable(false);
 }
@@ -455,6 +712,16 @@ static void __init mmio_select_mitigation(void)
 	else
 		mmio_mitigation = MMIO_MITIGATION_UCODE_NEEDED;
 
+	/*
+	 * 在以下调用cpu_smt_disable():
+	 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+	 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+	 */
 	if (mmio_nosmt || cpu_mitigations_auto_nosmt())
 		cpu_smt_disable(false);
 }
@@ -1146,6 +1413,16 @@ static void __init retbleed_select_mitigation(void)
 		break;
 	}
 
+	/*
+	 * 在以下调用cpu_smt_disable():
+	 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+	 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+	 */
 	if (mitigate_smt && !boot_cpu_has(X86_FEATURE_STIBP) &&
 	    (retbleed_nosmt || cpu_mitigations_auto_nosmt()))
 		cpu_smt_disable(false);
@@ -1364,6 +1641,13 @@ spectre_v2_user_select_mitigation(void)
 
 	/* Initialize Indirect Branch Prediction Barrier */
 	if (boot_cpu_has(X86_FEATURE_IBPB)) {
+		/*
+		 * 在以下使用X86_FEATURE_USE_IBPB:
+		 *   - arch/x86/include/asm/nospec-branch.h|547| <<indirect_branch_prediction_barrier>>
+		 *       alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+		 *   - arch/x86/kernel/cpu/bugs.c|1469| <<spectre_v2_user_select_mitigation>>
+		 *       setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
+		 */
 		setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
 
 		spectre_v2_user_ibpb = mode;
@@ -1673,6 +1957,20 @@ static void __init bhi_select_mitigation(void)
 	if (bhi_mitigation == BHI_MITIGATION_OFF)
 		return;
 
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	/* Retpoline mitigates against BHI unless the CPU has RRSBA behavior */
 	if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
 	    !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE)) {
@@ -1694,6 +1992,27 @@ static void __init bhi_select_mitigation(void)
 		return;
 	}
 
+	/*
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_LOOP:
+	 *   - arch/x86/include/asm/cpufeatures.h|479| <<global>> #define X86_FEATURE_CLEAR_BHB_LOOP (21*32+ 1) 
+	 *   - arch/x86/include/asm/nospec-branch.h|327| <<CLEAR_BRANCH_HISTORY>>
+	 *                       ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
+	 *   - arch/x86/kernel/cpu/bugs.c|1928| <<bhi_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+	 *   - arch/x86/kernel/cpu/bugs.c|3114| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_LOOP))
+	 *
+	 *
+	 * 使用CLEAR_BRANCH_HISTORY和CLEAR_BRANCH_HISTORY_VMEXIT的地方:
+	 * arch/x86/entry/common.c: * 4) int80_emulation() does a CLEAR_BRANCH_HISTORY. While FRED will
+	 * arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/entry/entry_64.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/include/asm/nospec-branch.h:.macro CLEAR_BRANCH_HISTORY
+	 * arch/x86/include/asm/nospec-branch.h:.macro CLEAR_BRANCH_HISTORY_VMEXIT
+	 * arch/x86/include/asm/nospec-branch.h:#define CLEAR_BRANCH_HISTORY
+	 * arch/x86/include/asm/nospec-branch.h:#define CLEAR_BRANCH_HISTORY_VMEXIT
+	 * arch/x86/kvm/vmx/vmenter.S:	CLEAR_BRANCH_HISTORY_VMEXIT
+	 */
 	pr_info("Spectre BHI mitigation: SW BHB clearing on syscall and VM exit\n");
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP_ON_VMEXIT);
@@ -1796,6 +2115,20 @@ static void __init spectre_v2_select_mitigation(void)
 
 	case SPECTRE_V2_RETPOLINE:
 	case SPECTRE_V2_EIBRS_RETPOLINE:
+		/*
+		 * 在以下使用X86_FEATURE_RETPOLINE:
+		 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+		 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+		 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+		 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+		 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+		 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+		 *
+		 * v6.13或以前的在nospec-branch.h有两处
+		 */
 		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
 		break;
 	}
@@ -1854,6 +2187,11 @@ static void __init spectre_v2_select_mitigation(void)
 	 *
 	 * FIXME: Is this pointless for retbleed-affected AMD?
 	 */
+	/*
+	 * 在以下使用X86_FEATURE_RSB_CTXSW:
+	 *   - arch/x86/kernel/cpu/bugs.c|2101| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW); 
+	 *   - arch/x86/kernel/cpu/bugs.c|3143| <<spectre_v2_show_state>> boot_cpu_has(X86_FEATURE_RSB_CTXSW) ? "; RSB filling" : "",
+	 */
 	setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
 	pr_info("Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\n");
 
@@ -2272,6 +2610,15 @@ static int ib_prctl_set(struct task_struct *task, unsigned long ctrl)
 		if (ctrl == PR_SPEC_FORCE_DISABLE)
 			task_set_spec_ib_force_disable(task);
 		task_update_spec_tif(task);
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (task == current)
 			indirect_branch_prediction_barrier();
 		break;
@@ -2397,6 +2744,25 @@ enum l1tf_mitigations l1tf_mitigation __ro_after_init =
 #if IS_ENABLED(CONFIG_KVM_INTEL)
 EXPORT_SYMBOL_GPL(l1tf_mitigation);
 #endif
+/*
+ * 在以下使用l1tf_vmx_mitigation:
+ *   - arch/x86/kernel/cpu/bugs.c|2707| <<global>> enum vmx_l1d_flush_state l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+ *   - arch/x86/kernel/cpu/bugs.c|3039| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO)
+ *   - arch/x86/kernel/cpu/bugs.c|3042| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_EPT_DISABLED ||
+ *   - arch/x86/kernel/cpu/bugs.c|3043| <<l1tf_show_state>> (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER &&
+ *   - arch/x86/kernel/cpu/bugs.c|3046| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation]);
+ *   - arch/x86/kernel/cpu/bugs.c|3050| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation],
+ *   - arch/x86/kvm/vmx/vmx.c|297| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+ *   - arch/x86/kvm/vmx/vmx.c|302| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_EPT_DISABLED;
+ *   - arch/x86/kvm/vmx/vmx.c|307| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+ *   - arch/x86/kvm/vmx/vmx.c|353| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = l1tf;
+ *   - arch/x86/kvm/vmx/vmx.c|414| <<vmentry_l1d_flush_set>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO) {
+ *   - arch/x86/kvm/vmx/vmx.c|427| <<vmentry_l1d_flush_get>> if (WARN_ON_ONCE(l1tf_vmx_mitigation >= ARRAY_SIZE(vmentry_l1d_param)))
+ *   - arch/x86/kvm/vmx/vmx.c|430| <<vmentry_l1d_flush_get>> return sysfs_emit(s, "%s\n", vmentry_l1d_param[l1tf_vmx_mitigation].option);
+ *   - arch/x86/kvm/vmx/vmx.c|7759| <<vmx_vm_init>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER)
+ *   - arch/x86/kvm/vmx/vmx.c|8690| <<vmx_cleanup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+ *   - arch/x86/kvm/x86.c|1664| <<kvm_get_arch_capabilities>> if (l1tf_vmx_mitigation != VMENTER_L1D_FLUSH_NEVER)
+ */
 enum vmx_l1d_flush_state l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
 EXPORT_SYMBOL_GPL(l1tf_vmx_mitigation);
 
@@ -2414,6 +2780,10 @@ EXPORT_SYMBOL_GPL(l1tf_vmx_mitigation);
  * instead of the reported physical bits and adjust them on the affected
  * machines to 44bit if the reported bits are less than 44.
  */
+/*
+ * 只在以下调用override_cache_bits():
+ *   - arch/x86/kernel/cpu/bugs.c|2796| <<l1tf_select_mitigation>> override_cache_bits(&boot_cpu_data);
+ */
 static void override_cache_bits(struct cpuinfo_x86 *c)
 {
 	if (c->x86 != 6)
@@ -2443,6 +2813,22 @@ static void __init l1tf_select_mitigation(void)
 {
 	u64 half_pa;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_L1TF))
 		return;
 
@@ -2460,6 +2846,16 @@ static void __init l1tf_select_mitigation(void)
 		break;
 	case L1TF_MITIGATION_FLUSH_NOSMT:
 	case L1TF_MITIGATION_FULL:
+		/*
+		 * 在以下调用cpu_smt_disable():
+		 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+		 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+		 */
 		cpu_smt_disable(false);
 		break;
 	case L1TF_MITIGATION_FULL_FORCE:
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7cce91b19..dfd105a53 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1458,6 +1458,22 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (cpu_matches(cpu_vuln_whitelist, NO_L1TF))
 		return;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	setup_force_cpu_bug(X86_BUG_L1TF);
 }
 
diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c
index feca4f20b..59c6b9838 100644
--- a/arch/x86/kernel/irq.c
+++ b/arch/x86/kernel/irq.c
@@ -469,6 +469,25 @@ DEFINE_IDTENTRY_SYSVEC(sysvec_posted_msi_notification)
 	 */
 	pi_clear_on(pid);
 
+	/*
+	 * 11 // Posted-Interrupt Descriptor
+	 * 12 struct pi_desc {
+	 * 13         union {
+	 * 14                 u32 pir[8];     // Posted interrupt requested
+	 * 15                 u64 pir64[4];
+	 * 16         };
+	 * 17         union {
+	 * 18                 struct {
+	 * 19                         u16     notifications; // Suppress and outstanding bits
+	 * 20                         u8      nv;
+	 * 21                         u8      rsvd_2;
+	 * 22                         u32     ndst;
+	 * 23                 };
+	 * 24                 u64 control;
+	 * 25         };
+	 * 26         u32 rsvd[6];
+	 * 27 } __aligned(64);
+	 */
 	/*
 	 * There could be a race of PI notification and the clearing of ON bit,
 	 * process PIR bits one last time such that handling the new interrupts
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index a009c94c2..de7a698ba 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -662,6 +662,13 @@ static u8 count_vectors(void *bitmap)
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|704| <<kvm_apic_update_irr>>
+ *          bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+ *   - arch/x86/kvm/vmx/nested.c|3941| <<vmx_complete_nested_posted_interrupt>>
+ *          __kvm_apic_update_irr(vmx->nested.pi_desc->pir, vapic_page, &max_irr);
+ */
 bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 {
 	u32 i, vec;
@@ -698,9 +705,20 @@ bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 }
 EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6983| <<vmx_sync_pir_to_irr>> kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
+ */
 bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/lapic.c|704| <<kvm_apic_update_irr>>
+	 *          bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+	 *   - arch/x86/kvm/vmx/nested.c|3941| <<vmx_complete_nested_posted_interrupt>>
+	 *          __kvm_apic_update_irr(vmx->nested.pi_desc->pir, vapic_page, &max_irr);
+	 */
 	bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
 
 	if (unlikely(!apic->apicv_active && irr_updated))
@@ -951,6 +969,20 @@ static bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)
 static int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)
 {
 	int highest_irr;
+	/*
+	 * 在以下使用sync_pir_to_irr:
+	 *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+	 *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+	 *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *
+	 * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+	 */
 	if (kvm_x86_ops.sync_pir_to_irr)
 		highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
 	else
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 816087039..4f37356c7 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -4607,6 +4607,16 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 	 */
 	BUILD_BUG_ON(lower_32_bits(PFERR_SYNTHETIC_MASK));
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	vcpu->arch.l1tf_flush_l1d = true;
 	if (!flags) {
 		trace_kvm_page_fault(vcpu, fault_address, error_code);
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 22551e2f1..0e4952cdc 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -495,6 +495,22 @@ void kvm_mmu_reset_all_pte_masks(void)
 	 */
 	shadow_nonpresent_or_rsvd_mask = 0;
 	low_phys_bits = boot_cpu_data.x86_phys_bits;
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (boot_cpu_has_bug(X86_BUG_L1TF) &&
 	    !WARN_ON_ONCE(boot_cpu_data.x86_cache_bits >=
 			  52 - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)) {
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index e67de787f..27c353b76 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -1559,6 +1559,15 @@ static void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	if (sd->current_vmcb != svm->vmcb) {
 		sd->current_vmcb = svm->vmcb;
 
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (!cpu_feature_enabled(X86_FEATURE_IBPB_ON_VMEXIT))
 			indirect_branch_prediction_barrier();
 	}
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index ed8a3cb53..66789a626 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -3739,6 +3739,16 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 	if (unlikely(status != NVMX_VMENTRY_SUCCESS))
 		goto vmentry_failed;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	/* Hide L1D cache contents from the nested guest.  */
 	vmx->vcpu.arch.l1tf_flush_l1d = true;
 
@@ -3938,6 +3948,13 @@ static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 		if (!vapic_page)
 			goto mmio_needed;
 
+		/*
+		 * called by:           
+		 *   - arch/x86/kvm/lapic.c|704| <<kvm_apic_update_irr>>
+		 *          bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+		 *   - arch/x86/kvm/vmx/nested.c|3941| <<vmx_complete_nested_posted_interrupt>>
+		 *          __kvm_apic_update_irr(vmx->nested.pi_desc->pir, vapic_page, &max_irr);
+		 */
 		__kvm_apic_update_irr(vmx->nested.pi_desc->pir,
 			vapic_page, &max_irr);
 		status = vmcs_read16(GUEST_INTR_STATUS);
@@ -5026,6 +5043,15 @@ void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	 * doesn't isolate different VMCSs, i.e. in this case, doesn't provide
 	 * separate modes for L2 vs L1.
 	 */
+	/*
+	 * 在以下使用indirect_branch_prediction_barrier():
+	 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+	 */
 	if (guest_cpu_cap_has(vcpu, X86_FEATURE_SPEC_CTRL))
 		indirect_branch_prediction_barrier();
 
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 3b92f893b..a303d3ad4 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -225,10 +225,44 @@ module_param(pt_mode, int, S_IRUGO);
 
 struct x86_pmu_lbr __ro_after_init vmx_lbr_caps;
 
+/*
+ * 在以下使用vmx_l1d_should_flush:
+ *   - arch/x86/kvm/vmx/vmx.c|228| <<global>> static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|356| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|358| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|7393| <<vmx_vcpu_enter_exit>> if (static_branch_unlikely(&vmx_l1d_should_flush))
+ */
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+/*
+ * 在以下使用vmx_l1d_flush_cond:
+ *   - arch/x86/kvm/vmx/vmx.c|229| <<global>> static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|361| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|363| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|6742| <<vmx_l1d_flush>> if (static_branch_likely(&vmx_l1d_flush_cond)) {
+ */
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
+/*
+ * 在以下使用vmx_l1d_flush_mutex:
+ *   - arch/x86/kvm/vmx/vmx.c|419| <<vmentry_l1d_flush_set>> mutex_lock(&vmx_l1d_flush_mutex);
+ *   - arch/x86/kvm/vmx/vmx.c|421| <<vmentry_l1d_flush_set>> mutex_unlock(&vmx_l1d_flush_mutex);
+ */
 static DEFINE_MUTEX(vmx_l1d_flush_mutex);
 
+/*
+ * enum vmx_l1d_flush_state {
+ *     VMENTER_L1D_FLUSH_AUTO,
+ *     VMENTER_L1D_FLUSH_NEVER,
+ *     VMENTER_L1D_FLUSH_COND,
+ *     VMENTER_L1D_FLUSH_ALWAYS,
+ *     VMENTER_L1D_FLUSH_EPT_DISABLED,
+ *     VMENTER_L1D_FLUSH_NOT_REQUIRED,
+ * };
+ *
+ * 在以下使用vmentry_l1d_flush_param:
+ *   - arch/x86/kvm/vmx/vmx.c|243| <<global>> static enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush_param = VMENTER_L1D_FLUSH_AUTO;
+ *   - arch/x86/kvm/vmx/vmx.c|415| <<vmentry_l1d_flush_set>> vmentry_l1d_flush_param = l1tf;
+ *   - arch/x86/kvm/vmx/vmx.c|8733| <<vmx_init>> r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+ */
 /* Storage for pre module init parameter parsing */
 static enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush_param = VMENTER_L1D_FLUSH_AUTO;
 
@@ -245,13 +279,71 @@ static const struct {
 };
 
 #define L1D_CACHE_ORDER 4
+/*
+ * 在以下使用vmx_l1d_flush_pages:
+ *   - arch/x86/kvm/vmx/vmx.c|306| <<vmx_setup_l1d_flush>> if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+ *   - arch/x86/kvm/vmx/vmx.c|315| <<vmx_setup_l1d_flush>> vmx_l1d_flush_pages = page_address(page);
+ *   - arch/x86/kvm/vmx/vmx.c|323| <<vmx_setup_l1d_flush>> memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1, PAGE_SIZE);
+ *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> asm volatile(...:: [flush_pages] "r" (vmx_l1d_flush_pages),
+ *   - arch/x86/kvm/vmx/vmx.c|8660| <<vmx_cleanup_l1d_flush>> if (vmx_l1d_flush_pages) {
+ *   - arch/x86/kvm/vmx/vmx.c|8661| <<vmx_cleanup_l1d_flush>> free_pages((unsigned long )vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+ *   - arch/x86/kvm/vmx/vmx.c|8662| <<vmx_cleanup_l1d_flush>> vmx_l1d_flush_pages = NULL;
+ */
 static void *vmx_l1d_flush_pages;
 
+/*
+ * enum vmx_l1d_flush_state {
+ *     VMENTER_L1D_FLUSH_AUTO,
+ *     VMENTER_L1D_FLUSH_NEVER,
+ *     VMENTER_L1D_FLUSH_COND,
+ *     VMENTER_L1D_FLUSH_ALWAYS,
+ *     VMENTER_L1D_FLUSH_EPT_DISABLED,
+ *     VMENTER_L1D_FLUSH_NOT_REQUIRED,
+ * };
+ *
+ * 在以下使用vmx_setup_l1d_flush():
+ *   - arch/x86/kvm/vmx/vmx.c|395| <<vmentry_l1d_flush_set>> ret = vmx_setup_l1d_flush(l1tf);
+ *   - arch/x86/kvm/vmx/vmx.c|8708| <<vmx_init>> r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+ */
 static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
 {
 	struct page *page;
 	unsigned int i;
 
+	/*
+	 * 在以下使用l1tf_vmx_mitigation:
+	 *   - arch/x86/kernel/cpu/bugs.c|2707| <<global>> enum vmx_l1d_flush_state l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+	 *   - arch/x86/kernel/cpu/bugs.c|3039| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO)
+	 *   - arch/x86/kernel/cpu/bugs.c|3042| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_EPT_DISABLED ||
+	 *   - arch/x86/kernel/cpu/bugs.c|3043| <<l1tf_show_state>> (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER &&
+	 *   - arch/x86/kernel/cpu/bugs.c|3046| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation]);
+	 *   - arch/x86/kernel/cpu/bugs.c|3050| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation],
+	 *   - arch/x86/kvm/vmx/vmx.c|297| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+	 *   - arch/x86/kvm/vmx/vmx.c|302| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_EPT_DISABLED;
+	 *   - arch/x86/kvm/vmx/vmx.c|307| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+	 *   - arch/x86/kvm/vmx/vmx.c|353| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = l1tf;
+	 *   - arch/x86/kvm/vmx/vmx.c|414| <<vmentry_l1d_flush_set>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO) {
+	 *   - arch/x86/kvm/vmx/vmx.c|427| <<vmentry_l1d_flush_get>> if (WARN_ON_ONCE(l1tf_vmx_mitigation >= ARRAY_SIZE(vmentry_l1d_param)))
+	 *   - arch/x86/kvm/vmx/vmx.c|430| <<vmentry_l1d_flush_get>> return sysfs_emit(s, "%s\n", vmentry_l1d_param[l1tf_vmx_mitigation].option);
+	 *   - arch/x86/kvm/vmx/vmx.c|7759| <<vmx_vm_init>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER)
+	 *   - arch/x86/kvm/vmx/vmx.c|8690| <<vmx_cleanup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+	 *   - arch/x86/kvm/x86.c|1664| <<kvm_get_arch_capabilities>> if (l1tf_vmx_mitigation != VMENTER_L1D_FLUSH_NEVER)
+	 *
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
 		l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
 		return 0;
@@ -296,6 +388,16 @@ static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
 		page = alloc_pages(GFP_KERNEL, L1D_CACHE_ORDER);
 		if (!page)
 			return -ENOMEM;
+		/*
+		 * 在以下使用vmx_l1d_flush_pages:
+		 *   - arch/x86/kvm/vmx/vmx.c|306| <<vmx_setup_l1d_flush>> if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+		 *   - arch/x86/kvm/vmx/vmx.c|315| <<vmx_setup_l1d_flush>> vmx_l1d_flush_pages = page_address(page);
+		 *   - arch/x86/kvm/vmx/vmx.c|323| <<vmx_setup_l1d_flush>> memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1, PAGE_SIZE);
+		 *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> asm volatile(...:: [flush_pages] "r" (vmx_l1d_flush_pages),
+		 *   - arch/x86/kvm/vmx/vmx.c|8660| <<vmx_cleanup_l1d_flush>> if (vmx_l1d_flush_pages) {
+		 *   - arch/x86/kvm/vmx/vmx.c|8661| <<vmx_cleanup_l1d_flush>> free_pages((unsigned long )vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+		 *   - arch/x86/kvm/vmx/vmx.c|8662| <<vmx_cleanup_l1d_flush>> vmx_l1d_flush_pages = NULL;
+		 */
 		vmx_l1d_flush_pages = page_address(page);
 
 		/*
@@ -316,6 +418,15 @@ static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
 	else
 		static_branch_disable(&vmx_l1d_should_flush);
 
+	/*
+	 * 在以下使用vmx_l1d_flush_cond:
+	 *   - arch/x86/kvm/vmx/vmx.c|229| <<global>> static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|361| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|363| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|6742| <<vmx_l1d_flush>> if (static_branch_likely(&vmx_l1d_flush_cond)) {
+	 *
+	 * vmx_l1d_flush_cond可以让flush不一定真的执行.
+	 */
 	if (l1tf == VMENTER_L1D_FLUSH_COND)
 		static_branch_enable(&vmx_l1d_flush_cond);
 	else
@@ -337,6 +448,13 @@ static int vmentry_l1d_flush_parse(const char *s)
 	return -EINVAL;
 }
 
+/*
+ * static const struct kernel_param_ops vmentry_l1d_flush_ops = {
+ *     .set = vmentry_l1d_flush_set,
+ *     .get = vmentry_l1d_flush_get,
+ * };
+ * module_param_cb(vmentry_l1d_flush, &vmentry_l1d_flush_ops, NULL, 0644);
+ */
 static int vmentry_l1d_flush_set(const char *s, const struct kernel_param *kp)
 {
 	int l1tf, ret;
@@ -345,6 +463,22 @@ static int vmentry_l1d_flush_set(const char *s, const struct kernel_param *kp)
 	if (l1tf < 0)
 		return l1tf;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has(X86_BUG_L1TF))
 		return 0;
 
@@ -1477,6 +1611,15 @@ void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
 		 * performs IBPB on nested VM-Exit (a single nested transition
 		 * may switch the active VMCS multiple times).
 		 */
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (!buddy || WARN_ON_ONCE(buddy->vmcs != prev))
 			indirect_branch_prediction_barrier();
 	}
@@ -6665,17 +6808,57 @@ int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
  * information but as all relevant affected CPUs have 32KiB L1D cache size
  * there is no point in doing so.
  */
+/*
+ * 例子:
+ * L1TF leverages speculative execution to access data that should be
+ * protected. Here's a basic breakdown:
+ *
+ * Execution speculation: Under certain conditions, the CPU may speculatively
+ * execute instructions that access memory locations (such as the L1 cache)
+ * which are not supposed to be visible to the current process.
+ *
+ * Data leakage: Even if the speculative execution is rolled back, the data
+ * accessed may remain in the L1 cache, making it possible for an attacker to
+ * use side-channel techniques to read the cached data.
+ *
+ * Attacking privileged memory: L1TF specifically impacts systems where an
+ * attacker running on a less privileged or user-level process can read data
+ * from higher-privileged memory regions (like kernel space memory).
+ *
+ * 在以下调用vmx_l1d_flush():
+ *   - arch/x86/kvm/vmx/vmx.c|7394| <<vmx_vcpu_enter_exit>> vmx_l1d_flush(vcpu);
+ */
 static noinstr void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 {
 	int size = PAGE_SIZE << L1D_CACHE_ORDER;
 
+	/*
+	 * 在以下使用vmx_l1d_flush_cond:
+	 *   - arch/x86/kvm/vmx/vmx.c|229| <<global>> static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|361| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|363| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|6742| <<vmx_l1d_flush>> if (static_branch_likely(&vmx_l1d_flush_cond)) {
+	 */
 	/*
 	 * This code is only executed when the flush mode is 'cond' or
 	 * 'always'
 	 */
 	if (static_branch_likely(&vmx_l1d_flush_cond)) {
+		/*
+		 * 最终的结果以flush_l1d为准
+		 */
 		bool flush_l1d;
 
+		/*
+		 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+		 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+		 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+		 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+		 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+		 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+		 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+		 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+		 */
 		/*
 		 * Clear the per-vcpu flush bit, it gets set again if the vCPU
 		 * is reloaded, i.e. if the vCPU is scheduled out or if KVM
@@ -6685,6 +6868,12 @@ static noinstr void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 		flush_l1d = vcpu->arch.l1tf_flush_l1d;
 		vcpu->arch.l1tf_flush_l1d = false;
 
+		/*
+		 * 在以下使用kvm_set_cpu_l1tf_flush_l1d():
+		 *   - arch/x86/include/asm/idtentry.h|215| <<DEFINE_IDTENTRY_IRQ>> kvm_set_cpu_l1tf_flush_l1d(); \
+		 *   - arch/x86/include/asm/idtentry.h|260| <<DEFINE_IDTENTRY_SYSVEC>> kvm_set_cpu_l1tf_flush_l1d(); \
+		 *   - arch/x86/include/asm/idtentry.h|299| <<DEFINE_IDTENTRY_SYSVEC_SIMPLE>> kvm_set_cpu_l1tf_flush_l1d(); \
+		 */
 		/*
 		 * Clear the per-cpu flush bit, it gets set again from
 		 * the interrupt handlers.
@@ -6696,13 +6885,31 @@ static noinstr void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 			return;
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_stat->l1d_flush:
+	 *   - arch/x86/kvm/x86.c|322| <<global>> STATS_DESC_COUNTER(VCPU, l1d_flush),
+	 *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> vcpu->stat.l1d_flush++;
+	 */
 	vcpu->stat.l1d_flush++;
 
+	/*
+	 * 似乎是如果硬件支持的话
+	 */
 	if (static_cpu_has(X86_FEATURE_FLUSH_L1D)) {
 		native_wrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);
 		return;
 	}
 
+	/*
+	 * 在以下使用vmx_l1d_flush_pages:
+	 *   - arch/x86/kvm/vmx/vmx.c|306| <<vmx_setup_l1d_flush>> if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+	 *   - arch/x86/kvm/vmx/vmx.c|315| <<vmx_setup_l1d_flush>> vmx_l1d_flush_pages = page_address(page);
+	 *   - arch/x86/kvm/vmx/vmx.c|323| <<vmx_setup_l1d_flush>> memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1, PAGE_SIZE);
+	 *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> asm volatile(...:: [flush_pages] "r" (vmx_l1d_flush_pages),
+	 *   - arch/x86/kvm/vmx/vmx.c|8660| <<vmx_cleanup_l1d_flush>> if (vmx_l1d_flush_pages) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8661| <<vmx_cleanup_l1d_flush>> free_pages((unsigned long )vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+	 *   - arch/x86/kvm/vmx/vmx.c|8662| <<vmx_cleanup_l1d_flush>> vmx_l1d_flush_pages = NULL;
+	 */
 	asm volatile(
 		/* First ensure the pages are in the TLB */
 		"xorl	%%eax, %%eax\n"
@@ -6922,6 +7129,20 @@ static void vmx_set_rvi(int vector)
 	}
 }
 
+/*
+ * 在以下使用sync_pir_to_irr:
+ *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+ *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+ *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+ *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+ *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+ *
+ * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+ */
 int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7304,6 +7525,13 @@ static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
 
 	guest_state_enter_irqoff();
 
+	/*
+	 * 在以下使用vmx_l1d_should_flush:
+	 *   - arch/x86/kvm/vmx/vmx.c|228| <<global>> static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+	 *   - arch/x86/kvm/vmx/vmx.c|356| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_should_flush);
+	 *   - arch/x86/kvm/vmx/vmx.c|358| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_should_flush);
+	 *   - arch/x86/kvm/vmx/vmx.c|7393| <<vmx_vcpu_enter_exit>> if (static_branch_unlikely(&vmx_l1d_should_flush))
+	 */
 	/*
 	 * L1D Flush includes CPU buffer clear to mitigate MDS, but VERW
 	 * mitigation for MDS is done late in VMentry and is still
@@ -7645,6 +7873,22 @@ int vmx_vm_init(struct kvm *kvm)
 	if (!ple_gap)
 		kvm->arch.pause_in_guest = true;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
 		switch (l1tf_mitigation) {
 		case L1TF_MITIGATION_OFF:
@@ -8586,6 +8830,16 @@ __init int vmx_hardware_setup(void)
 
 static void vmx_cleanup_l1d_flush(void)
 {
+	/*
+	 * 在以下使用vmx_l1d_flush_pages:
+	 *   - arch/x86/kvm/vmx/vmx.c|306| <<vmx_setup_l1d_flush>> if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+	 *   - arch/x86/kvm/vmx/vmx.c|315| <<vmx_setup_l1d_flush>> vmx_l1d_flush_pages = page_address(page);
+	 *   - arch/x86/kvm/vmx/vmx.c|323| <<vmx_setup_l1d_flush>> memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1, PAGE_SIZE);
+	 *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> asm volatile(...:: [flush_pages] "r" (vmx_l1d_flush_pages),
+	 *   - arch/x86/kvm/vmx/vmx.c|8660| <<vmx_cleanup_l1d_flush>> if (vmx_l1d_flush_pages) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8661| <<vmx_cleanup_l1d_flush>> free_pages((unsigned long )vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+	 *   - arch/x86/kvm/vmx/vmx.c|8662| <<vmx_cleanup_l1d_flush>> vmx_l1d_flush_pages = NULL;
+	 */
 	if (vmx_l1d_flush_pages) {
 		free_pages((unsigned long)vmx_l1d_flush_pages, L1D_CACHE_ORDER);
 		vmx_l1d_flush_pages = NULL;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 4b64ab350..f2e03f10e 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -89,6 +89,55 @@
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|101| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/x86.c|324| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|572| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|182| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|201| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|247| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1360| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|2936| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3095| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4251| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2058| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2320| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2345| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7270| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7281| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|13647| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|13649| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|13652| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ *
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|102| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|575| <<firmware_restrict_branch_speculation_start>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|613| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4477| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7887| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3779| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3806| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
+
 #define MAX_IO_MSRS 256
 #define KVM_MAX_MCE_BANKS 32
 
@@ -1603,6 +1652,25 @@ static u64 kvm_get_arch_capabilities(void)
 	 */
 	data |= ARCH_CAP_PSCHANGE_MC_NO;
 
+	/*
+	 * 在以下使用l1tf_vmx_mitigation:
+	 *   - arch/x86/kernel/cpu/bugs.c|2707| <<global>> enum vmx_l1d_flush_state l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+	 *   - arch/x86/kernel/cpu/bugs.c|3039| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO)
+	 *   - arch/x86/kernel/cpu/bugs.c|3042| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_EPT_DISABLED ||
+	 *   - arch/x86/kernel/cpu/bugs.c|3043| <<l1tf_show_state>> (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER &&
+	 *   - arch/x86/kernel/cpu/bugs.c|3046| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation]);
+	 *   - arch/x86/kernel/cpu/bugs.c|3050| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation],
+	 *   - arch/x86/kvm/vmx/vmx.c|297| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+	 *   - arch/x86/kvm/vmx/vmx.c|302| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_EPT_DISABLED;
+	 *   - arch/x86/kvm/vmx/vmx.c|307| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+	 *   - arch/x86/kvm/vmx/vmx.c|353| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = l1tf;
+	 *   - arch/x86/kvm/vmx/vmx.c|414| <<vmentry_l1d_flush_set>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO) {
+	 *   - arch/x86/kvm/vmx/vmx.c|427| <<vmentry_l1d_flush_get>> if (WARN_ON_ONCE(l1tf_vmx_mitigation >= ARRAY_SIZE(vmentry_l1d_param)))
+	 *   - arch/x86/kvm/vmx/vmx.c|430| <<vmentry_l1d_flush_get>> return sysfs_emit(s, "%s\n", vmentry_l1d_param[l1tf_vmx_mitigation].option);
+	 *   - arch/x86/kvm/vmx/vmx.c|7759| <<vmx_vm_init>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER)
+	 *   - arch/x86/kvm/vmx/vmx.c|8690| <<vmx_cleanup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+	 *   - arch/x86/kvm/x86.c|1664| <<kvm_get_arch_capabilities>> if (l1tf_vmx_mitigation != VMENTER_L1D_FLUSH_NEVER)
+	 */
 	/*
 	 * If we're doing cache flushes (either "always" or "cond")
 	 * we will do one whenever the guest does a vmlaunch/vmresume.
@@ -4948,6 +5016,16 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	vcpu->arch.l1tf_flush_l1d = true;
 
 	if (vcpu->scheduled_out && pmu->version && pmu->event_count) {
@@ -5085,6 +5163,20 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
+	/*
+	 * 在以下使用sync_pir_to_irr:
+	 *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+	 *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+	 *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *
+	 * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+	 */
 	kvm_x86_call(sync_pir_to_irr)(vcpu);
 
 	return kvm_apic_get_state(vcpu, s);
@@ -7716,6 +7808,16 @@ static int emulator_write_std(struct x86_emulate_ctxt *ctxt, gva_t addr, void *v
 int kvm_write_guest_virt_system(struct kvm_vcpu *vcpu, gva_t addr, void *val,
 				unsigned int bytes, struct x86_exception *exception)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	/* kvm_write_guest_virt_system can pull in tons of pages. */
 	vcpu->arch.l1tf_flush_l1d = true;
 
@@ -9111,6 +9213,16 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 		return handle_emulation_failure(vcpu, emulation_type);
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	vcpu->arch.l1tf_flush_l1d = true;
 
 	if (!(emulation_type & EMULTYPE_NO_DECODE)) {
@@ -10642,6 +10754,20 @@ static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)
 
 	bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
 
+	/*
+	 * 在以下使用sync_pir_to_irr:
+	 *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+	 *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+	 *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *
+	 * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+	 */
 	kvm_x86_call(sync_pir_to_irr)(vcpu);
 
 	if (irqchip_split(vcpu->kvm))
@@ -10925,6 +11051,20 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	 */
 	smp_mb__after_srcu_read_unlock();
 
+	/*
+	 * 在以下使用sync_pir_to_irr:
+	 *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+	 *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+	 *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *
+	 * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+	 */
 	/*
 	 * Process pending posted interrupts to handle the case where the
 	 * notification IRQ arrived in the host, or was never sent (because the
@@ -10987,6 +11127,20 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
 
+		/*
+		 * 在以下使用sync_pir_to_irr:
+		 *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+		 *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+		 *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+		 *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+		 *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+		 *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+		 *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+		 *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+		 *
+		 * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+		 */
 		if (kvm_lapic_enabled(vcpu))
 			kvm_x86_call(sync_pir_to_irr)(vcpu);
 
diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c
index 62aa4d66a..5bf8824db 100644
--- a/arch/x86/mm/init.c
+++ b/arch/x86/mm/init.c
@@ -1039,6 +1039,22 @@ unsigned long arch_max_swapfile_size(void)
 
 	pages = generic_max_swapfile_size();
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
 		/* Limit the swap file size to MAX_PA/2 for L1TF workaround */
 		unsigned long long l1tf_limit = l1tf_pfn_limit();
diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c
index b8a6ffffb..f96d032a8 100644
--- a/arch/x86/mm/mmap.c
+++ b/arch/x86/mm/mmap.c
@@ -232,6 +232,22 @@ int valid_mmap_phys_addr_range(unsigned long pfn, size_t count)
  */
 bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot)
 {
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_L1TF))
 		return true;
 	if (!__pte_needs_invert(pgprot_val(prot)))
diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c
index 6cf881a94..a4185e7b4 100644
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@ -325,6 +325,16 @@ void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	unsigned long flags;
 
 	local_irq_save(flags);
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(NULL, next, tsk);
 	local_irq_restore(flags);
 }
@@ -378,6 +388,10 @@ static unsigned long mm_mangle_tif_spec_bits(struct task_struct *next)
 	return (unsigned long)next->mm | spec_bits;
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/tlb.c|645| <<switch_mm_irqs_off>> cond_mitigation(tsk);
+ */
 static void cond_mitigation(struct task_struct *next)
 {
 	unsigned long prev_mm, next_mm;
@@ -436,6 +450,15 @@ static void cond_mitigation(struct task_struct *next)
 		 * Issue IBPB only if the mm's are different and one or
 		 * both have the IBPB bit set.
 		 */
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (next_mm != prev_mm &&
 		    (next_mm | prev_mm) & LAST_USER_MM_IBPB)
 			indirect_branch_prediction_barrier();
@@ -447,6 +470,15 @@ static void cond_mitigation(struct task_struct *next)
 		 * different context than the user space task which ran
 		 * last on this CPU.
 		 */
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if ((prev_mm & ~LAST_USER_MM_SPEC_MASK) !=
 					(unsigned long)next->mm)
 			indirect_branch_prediction_barrier();
@@ -496,6 +528,16 @@ static inline void cr4_update_pce_mm(struct mm_struct *mm) { }
  * 'cpu_tlbstate.loaded_mm' instead because it does not always keep
  * 'current->active_mm' up to date.
  */
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+ */
 void switch_mm_irqs_off(struct mm_struct *unused, struct mm_struct *next,
 			struct task_struct *tsk)
 {
@@ -782,6 +824,16 @@ static void flush_tlb_func(void *info)
 		 * This should be rare, with native_flush_tlb_multi() skipping
 		 * IPIs to lazy TLB mode CPUs.
 		 */
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+		 */
 		switch_mm_irqs_off(NULL, &init_mm, NULL);
 		return;
 	}
diff --git a/arch/x86/net/bpf_jit_comp.c b/arch/x86/net/bpf_jit_comp.c
index a43fc5af9..e35491c11 100644
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@ -657,6 +657,20 @@ static void emit_indirect_jump(u8 **pprog, int reg, u8 *ip)
 		EMIT_LFENCE();
 		EMIT2(0xFF, 0xE0 + reg);
 	} else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+		/*
+		 * 在以下使用X86_FEATURE_RETPOLINE:
+		 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+		 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+		 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+		 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+		 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+		 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+		 *
+		 * v6.13或以前的在nospec-branch.h有两处
+		 */
 		OPTIMIZER_HIDE_VAR(reg);
 		if (cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))
 			emit_jump(&prog, &__x86_indirect_jump_thunk_array[reg], ip);
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index b9b9e9d40..e82b63e95 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -1650,6 +1650,12 @@ static long vhost_net_set_owner(struct vhost_net *n)
 	int r;
 
 	mutex_lock(&n->dev.mutex);
+	/*
+	 * 在以下调用vhost_dev_has_owner():
+	 *   - drivers/vhost/net.c|1653| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+	 *   - drivers/vhost/vhost.c|868| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+	 *   - drivers/vhost/vhost.c|937| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+	 */
 	if (vhost_dev_has_owner(&n->dev)) {
 		r = -EBUSY;
 		goto out;
@@ -1657,6 +1663,11 @@ static long vhost_net_set_owner(struct vhost_net *n)
 	r = vhost_net_set_ubuf_info(n);
 	if (r)
 		goto out;
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1660| <<vhost_net_set_owner>> r = vhost_dev_set_owner(&n->dev);
+	 *   - drivers/vhost/vhost.c|2149| <<vhost_dev_ioctl(VHOST_SET_OWNER)>> r = vhost_dev_set_owner(d);
+	 */
 	r = vhost_dev_set_owner(&n->dev);
 	if (r)
 		vhost_net_clear_ubuf_info(n);
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 718fa4e0b..4ab0d915f 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -39,6 +39,32 @@
 
 #include "vhost.h"
 
+/*
+ * Legacy的方式.
+ *
+ * VHOST_SET_OWNER
+ * -> vhost_dev_set_owner()
+ *    -> vhost_worker_create()
+ *    -> for (i = 0; i < dev->nvqs; i++)
+ *         __vhost_vq_attach_worker(dev->vqs[i], worker)
+ *
+ * 新的multiqueue/worker的方式.
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_NEW_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_NEW_WORKER
+ *          -> vhost_new_worker()
+ *             -> vhost_worker_create()
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_ATTACH_VRING_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_ATTACH_VRING_WORKER
+ *          -> vhost_vq_attach_worker()
+ *             -> __vhost_vq_attach_worker()
+ */
+
 #define VHOST_SCSI_VERSION  "v0.1"
 #define VHOST_SCSI_NAMELEN 256
 #define VHOST_SCSI_MAX_CDB_SIZE 32
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 63612faea..526999f38 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -581,6 +581,12 @@ long vhost_dev_check_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_check_owner);
 
+/*
+ * 在以下调用vhost_dev_has_owner():
+ *   - drivers/vhost/net.c|1653| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+ *   - drivers/vhost/vhost.c|868| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+ *   - drivers/vhost/vhost.c|937| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+ */
 /* Caller should have device mutex */
 bool vhost_dev_has_owner(struct vhost_dev *dev)
 {
@@ -588,6 +594,10 @@ bool vhost_dev_has_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_has_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|942| <<vhost_dev_set_owner>> vhost_attach_mm(dev);
+ */
 static void vhost_attach_mm(struct vhost_dev *dev)
 {
 	/* No owner, become one */
@@ -605,6 +615,11 @@ static void vhost_attach_mm(struct vhost_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|975| <<vhost_dev_set_owner>> vhost_detach_mm(dev);
+ *   - drivers/vhost/vhost.c|1067| <<vhost_dev_cleanup>> vhost_detach_mm(dev);
+ */
 static void vhost_detach_mm(struct vhost_dev *dev)
 {
 	if (!dev->mm)
@@ -618,6 +633,11 @@ static void vhost_detach_mm(struct vhost_dev *dev)
 	dev->mm = NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|648| <<vhost_workers_free>> vhost_worker_destroy(dev, worker);
+ *   - drivers/vhost/vhost.c|829| <<vhost_free_worker>> vhost_worker_destroy(dev, worker);
+ */
 static void vhost_worker_destroy(struct vhost_dev *dev,
 				 struct vhost_worker *worker)
 {
@@ -630,6 +650,10 @@ static void vhost_worker_destroy(struct vhost_dev *dev,
 	kfree(worker);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1066| <<vhost_dev_cleanup>> vhost_workers_free(dev);
+ */
 static void vhost_workers_free(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
@@ -649,6 +673,11 @@ static void vhost_workers_free(struct vhost_dev *dev)
 	xa_destroy(&dev->worker_xa);
 }
 
+/*
+ * 在以下两个地方调用vhost_worker_create():
+ *   - drivers/vhost/vhost.c|787| <<vhost_new_worker>> worker = vhost_worker_create(dev);
+ *   - drivers/vhost/vhost.c|945| <<vhost_dev_set_owner>> worker = vhost_worker_create(dev);
+ */
 static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
@@ -690,6 +719,11 @@ static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 	return NULL;
 }
 
+/*
+ * 在以下使用__vhost_vq_attach_worker():
+ *   - drivers/vhost/vhost.c|782| <<vhost_vq_attach_worker>> __vhost_vq_attach_worker(vq, worker);
+ *   - drivers/vhost/vhost.c|967| <<vhost_dev_set_owner>> __vhost_vq_attach_worker(dev->vqs[i], worker);
+ */
 /* Caller must have device mutex */
 static void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 				     struct vhost_worker *worker)
@@ -759,6 +793,10 @@ static void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 	mutex_unlock(&old_worker->mutex);
 }
 
+/*
+ * 处理VHOST_ATTACH_VRING_WORKER:
+ *   - drivers/vhost/vhost.c|905| <<vhost_worker_ioctl(VHOST_ATTACH_VRING_WORKER)>> ret = vhost_vq_attach_worker(vq, &ring_worker);
+ */
  /* Caller must have device mutex */
 static int vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 				  struct vhost_vring_worker *info)
@@ -774,16 +812,30 @@ static int vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 	if (!worker || worker->id != info->worker_id)
 		return -ENODEV;
 
+	/*
+	 * 在以下使用__vhost_vq_attach_worker():
+	 *   - drivers/vhost/vhost.c|782| <<vhost_vq_attach_worker>> __vhost_vq_attach_worker(vq, worker);
+	 *   - drivers/vhost/vhost.c|967| <<vhost_dev_set_owner>> __vhost_vq_attach_worker(dev->vqs[i], worker);
+	 */
 	__vhost_vq_attach_worker(vq, worker);
 	return 0;
 }
 
+/*
+ * 在以下调用vhost_new_worker():
+ *   - drivers/vhost/vhost.c|878| <<vhost_worker_ioctl(VHOST_NEW_WORKER)>> ret = vhost_new_worker(dev, &state);
+ */
 /* Caller must have device mutex */
 static int vhost_new_worker(struct vhost_dev *dev,
 			    struct vhost_worker_state *info)
 {
 	struct vhost_worker *worker;
 
+	/*
+	 * 在以下两个地方调用vhost_worker_create():
+	 *   - drivers/vhost/vhost.c|787| <<vhost_new_worker>> worker = vhost_worker_create(dev);
+	 *   - drivers/vhost/vhost.c|945| <<vhost_dev_set_owner>> worker = vhost_worker_create(dev);
+	 */
 	worker = vhost_worker_create(dev);
 	if (!worker)
 		return -ENOMEM;
@@ -792,6 +844,10 @@ static int vhost_new_worker(struct vhost_dev *dev,
 	return 0;
 }
 
+/*
+ * 处理VHOST_FREE_WORKER:
+ *   - drivers/vhost/vhost.c|885| <<vhost_worker_ioctl(VHOST_FREE_WORKER)>> return vhost_free_worker(dev, &state);
+ */
 /* Caller must have device mutex */
 static int vhost_free_worker(struct vhost_dev *dev,
 			     struct vhost_worker_state *info)
@@ -820,6 +876,11 @@ static int vhost_free_worker(struct vhost_dev *dev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|894| <<vhost_worker_ioctl>> ret = vhost_get_vq_from_user(dev, argp, &vq, &idx);
+ *   - drivers/vhost/vhost.c|1984| <<vhost_vring_ioctl>> r = vhost_get_vq_from_user(d, argp, &vq, &idx);
+ */
 static int vhost_get_vq_from_user(struct vhost_dev *dev, void __user *argp,
 				  struct vhost_virtqueue **vq, u32 *id)
 {
@@ -841,6 +902,14 @@ static int vhost_get_vq_from_user(struct vhost_dev *dev, void __user *argp,
 	return 0;
 }
 
+/*
+ * 处理四种:
+ * - VHOST_NEW_WORKER
+ * - VHOST_FREE_WORKER
+ * - VHOST_ATTACH_VRING_WORKER
+ * - VHOST_GET_VRING_WORKER
+ * drivers/vhost/scsi.c|2093| <<vhost_scsi_ioctl>> r = vhost_worker_ioctl(&vs->dev, ioctl, argp);
+ */
 /* Caller must have device mutex */
 long vhost_worker_ioctl(struct vhost_dev *dev, unsigned int ioctl,
 			void __user *argp)
@@ -855,6 +924,12 @@ long vhost_worker_ioctl(struct vhost_dev *dev, unsigned int ioctl,
 	if (!dev->use_worker)
 		return -EINVAL;
 
+	/*
+	 * 在以下调用vhost_dev_has_owner():
+	 *   - drivers/vhost/net.c|1653| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+	 *   - drivers/vhost/vhost.c|868| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+	 *   - drivers/vhost/vhost.c|937| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+	 */
 	if (!vhost_dev_has_owner(dev))
 		return -EINVAL;
 
@@ -917,12 +992,23 @@ long vhost_worker_ioctl(struct vhost_dev *dev, unsigned int ioctl,
 }
 EXPORT_SYMBOL_GPL(vhost_worker_ioctl);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1660| <<vhost_net_set_owner>> r = vhost_dev_set_owner(&n->dev);
+ *   - drivers/vhost/vhost.c|2149| <<vhost_dev_ioctl(VHOST_SET_OWNER)>> r = vhost_dev_set_owner(d);
+ */
 /* Caller should have device mutex */
 long vhost_dev_set_owner(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
 	int err, i;
 
+	/*
+	 * 在以下调用vhost_dev_has_owner():
+	 *   - drivers/vhost/net.c|1653| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+	 *   - drivers/vhost/vhost.c|868| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+	 *   - drivers/vhost/vhost.c|937| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+	 */
 	/* Is there an owner already? */
 	if (vhost_dev_has_owner(dev)) {
 		err = -EBUSY;
@@ -942,12 +1028,22 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 		 * below since we don't have to worry about vsock queueing
 		 * while we free the worker.
 		 */
+		/*
+		 * 在以下两个地方调用vhost_worker_create():
+		 *   - drivers/vhost/vhost.c|787| <<vhost_new_worker>> worker = vhost_worker_create(dev);
+		 *   - drivers/vhost/vhost.c|945| <<vhost_dev_set_owner>> worker = vhost_worker_create(dev);
+		 */
 		worker = vhost_worker_create(dev);
 		if (!worker) {
 			err = -ENOMEM;
 			goto err_worker;
 		}
 
+		/*
+		 * 在以下使用__vhost_vq_attach_worker():
+		 *   - drivers/vhost/vhost.c|782| <<vhost_vq_attach_worker>> __vhost_vq_attach_worker(vq, worker);
+		 *   - drivers/vhost/vhost.c|967| <<vhost_dev_set_owner>> __vhost_vq_attach_worker(dev->vqs[i], worker);
+		 */
 		for (i = 0; i < dev->nvqs; i++)
 			__vhost_vq_attach_worker(dev->vqs[i], worker);
 	}
@@ -2131,6 +2227,11 @@ long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 
 	/* If you are not the owner, you can become one */
 	if (ioctl == VHOST_SET_OWNER) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1660| <<vhost_net_set_owner>> r = vhost_dev_set_owner(&n->dev);
+		 *   - drivers/vhost/vhost.c|2149| <<vhost_dev_ioctl(VHOST_SET_OWNER)>> r = vhost_dev_set_owner(d);
+		 */
 		r = vhost_dev_set_owner(d);
 		goto done;
 	}
diff --git a/include/net/tc_wrapper.h b/include/net/tc_wrapper.h
index ffe58a025..985729c0e 100644
--- a/include/net/tc_wrapper.h
+++ b/include/net/tc_wrapper.h
@@ -202,6 +202,20 @@ static inline int tc_classify(struct sk_buff *skb, const struct tcf_proto *tp,
 static inline void tc_wrapper_init(void)
 {
 #ifdef CONFIG_X86
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
 		static_branch_enable(&tc_skip_wrapper);
 #endif
diff --git a/kernel/cpu.c b/kernel/cpu.c
index 07455d253..efdc8cb33 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -606,6 +606,16 @@ enum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;
 static unsigned int cpu_smt_max_threads __ro_after_init;
 unsigned int cpu_smt_num_threads __read_mostly = UINT_MAX;
 
+/*
+ * 在以下调用cpu_smt_disable():
+ *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+ *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+ */
 void __init cpu_smt_disable(bool force)
 {
 	if (!cpu_smt_possible())
@@ -649,6 +659,16 @@ void __init cpu_smt_set_num_threads(unsigned int num_threads,
 
 static int __init smt_cmdline_disable(char *str)
 {
+	/*
+	 * 在以下调用cpu_smt_disable():
+	 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+	 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+	 */
 	cpu_smt_disable(str && !strcmp(str, "force"));
 	return 0;
 }
@@ -3209,6 +3229,9 @@ static int __init mitigations_parse_cmdline(char *arg)
 	return 0;
 }
 
+/*
+ * 特别多调用
+ */
 /* mitigations=off */
 bool cpu_mitigations_off(void)
 {
diff --git a/kernel/kthread.c b/kernel/kthread.c
index 5dc5b0d72..8bf33b737 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -1618,6 +1618,16 @@ void kthread_use_mm(struct mm_struct *mm)
 	tsk->active_mm = mm;
 	tsk->mm = mm;
 	membarrier_update_current_mm(mm);
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(active_mm, mm, tsk);
 	local_irq_enable();
 	task_unlock(tsk);
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 042351c7a..4bf21060b 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -5359,6 +5359,16 @@ context_switch(struct rq *rq, struct task_struct *prev,
 		 * case 'prev->active_mm == next->mm' through
 		 * finish_task_switch()'s mmdrop().
 		 */
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+		 */
 		switch_mm_irqs_off(prev->active_mm, next->mm, next);
 		lru_gen_use_mm(next->mm);
 
@@ -7955,6 +7965,16 @@ static void sched_force_init_mm(void)
 		mmgrab_lazy_tlb(&init_mm);
 		local_irq_disable();
 		current->active_mm = &init_mm;
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+		 */
 		switch_mm_irqs_off(mm, &init_mm, current);
 		local_irq_enable();
 		finish_arch_post_lock_switch();
diff --git a/net/netfilter/nf_tables_core.c b/net/netfilter/nf_tables_core.c
index 75598520b..079c7faec 100644
--- a/net/netfilter/nf_tables_core.c
+++ b/net/netfilter/nf_tables_core.c
@@ -32,6 +32,20 @@ static bool nf_skip_indirect_calls(void)
 
 static void __init nf_skip_indirect_calls_enable(void)
 {
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
 		static_branch_enable(&nf_tables_skip_direct_calls);
 }
-- 
2.39.5 (Apple Git-154)

