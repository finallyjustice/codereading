From 5912e3ae4b140b039fd8488b007f57ca40ecfd6a Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Wed, 23 Apr 2025 14:43:33 -0700
Subject: [PATCH 1/1] linux-v6.14

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/kvm/pmu-emul.c                     |   25 +
 arch/x86/entry/calling.h                      |   22 +
 arch/x86/events/core.c                        |   11 +
 arch/x86/events/zhaoxin/core.c                |    4 +
 arch/x86/include/asm/hardirq.h                |   14 +
 arch/x86/include/asm/kvm_host.h               |   76 ++
 arch/x86/include/asm/msr-index.h              |   54 +
 arch/x86/include/asm/nospec-branch.h          |   38 +
 arch/x86/include/asm/pgtable.h                |   16 +
 arch/x86/include/asm/spec-ctrl.h              |   13 +
 arch/x86/kernel/alternative.c                 |   34 +
 arch/x86/kernel/cpu/amd.c                     |    3 +
 arch/x86/kernel/cpu/bugs.c                    | 1099 +++++++++++++++++
 arch/x86/kernel/cpu/common.c                  |  169 +++
 arch/x86/kernel/cpu/cpu.h                     |    7 +
 arch/x86/kernel/irq.c                         |   19 +
 arch/x86/kernel/process.c                     |   12 +
 arch/x86/kernel/ptrace.c                      |    9 +
 arch/x86/kernel/smpboot.c                     |   10 +
 arch/x86/kvm/lapic.c                          |   32 +
 arch/x86/kvm/mmu/mmu.c                        |   10 +
 arch/x86/kvm/mmu/spte.c                       |   16 +
 arch/x86/kvm/pmu.c                            |  403 ++++++
 arch/x86/kvm/pmu.h                            |   64 +
 arch/x86/kvm/svm/pmu.c                        |   13 +
 arch/x86/kvm/svm/svm.c                        |    9 +
 arch/x86/kvm/vmx/nested.c                     |   26 +
 arch/x86/kvm/vmx/pmu_intel.c                  |   51 +
 arch/x86/kvm/vmx/vmx.c                        |  671 ++++++++++
 arch/x86/kvm/vmx/vmx.h                        |   45 +
 arch/x86/kvm/x86.c                            |  246 ++++
 arch/x86/mm/init.c                            |   16 +
 arch/x86/mm/mmap.c                            |   16 +
 arch/x86/mm/pti.c                             |   13 +
 arch/x86/mm/tlb.c                             |   52 +
 arch/x86/net/bpf_jit_comp.c                   |   14 +
 drivers/acpi/acpica/acmacros.h                |    7 +
 drivers/acpi/acpica/uterror.c                 |    4 +
 drivers/acpi/acpica/uteval.c                  |   14 +
 drivers/acpi/scan.c                           |    8 +
 drivers/idle/intel_idle.c                     |   10 +
 drivers/scsi/virtio_scsi.c                    |  102 ++
 drivers/vhost/net.c                           |   11 +
 drivers/vhost/scsi.c                          |   26 +
 drivers/vhost/vhost.c                         |  105 ++
 include/net/tc_wrapper.h                      |   14 +
 kernel/cpu.c                                  |   23 +
 kernel/events/core.c                          |  105 ++
 kernel/events/hw_breakpoint.c                 |   12 +
 kernel/kthread.c                              |   10 +
 kernel/sched/core.c                           |   20 +
 kernel/watchdog_perf.c                        |   25 +
 net/netfilter/nf_tables_core.c                |   14 +
 .../selftests/kvm/x86/pmu_counters_test.c     |  108 ++
 54 files changed, 3950 insertions(+)

diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
index 6c5950b9c..b6552016b 100644
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@ -738,6 +738,31 @@ static void kvm_pmu_create_perf_event(struct kvm_pmc *pmc)
 
 	attr.sample_period = compute_period(pmc, kvm_pmu_get_pmc_value(pmc));
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|741| <<kvm_pmu_create_perf_event>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|328| <<kvm_pmu_create_perf_event>>
+	 *      event = perf_event_create_kernel_counter(attr, -1, current, kvm_riscv_pmu_overflow, pmc);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|969| <<measure_residency_fn>>
+	 *      miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|974| <<measure_residency_fn>>
+	 *      hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kvm/pmu.c|227| <<pmc_reprogram_counter>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|243| <<intel_pmu_create_guest_lbr_event>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+	 *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>>
+	 *      return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+	 *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>>
+	 *      bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+	 *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>>
+	 *      return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+	 *   - kernel/watchdog_perf.c|135| <<hardlockup_detector_event_create>>
+	 *      evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 *   - kernel/watchdog_perf.c|140| <<hardlockup_detector_event_create>>
+	 *      evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 */
 	event = perf_event_create_kernel_counter(&attr, -1, current,
 						 kvm_pmu_perf_overflow, pmc);
 
diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h
index ea8177062..5fa15af77 100644
--- a/arch/x86/entry/calling.h
+++ b/arch/x86/entry/calling.h
@@ -289,6 +289,28 @@ For 32-bit we have the following conventions - kernel is built with
 
 #endif
 
+/*
+ * 在以下使用percpu x86_spec_ctrl_current:
+ *   - arch/x86/entry/calling.h|321| <<IBRS_ENTER>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+ *   - arch/x86/entry/calling.h|341| <<IBRS_EXIT>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+ *   - arch/x86/include/asm/nospec-branch.h|567| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+ *   - arch/x86/kernel/cpu/bugs.c|261| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+ *   - arch/x86/include/asm/spec-ctrl.h|86| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|293| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|303| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+ *   - arch/x86/kernel/cpu/bugs.c|306| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|318| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+ *   - arch/x86/kvm/vmx/vmx.c|7501| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+ *
+ * 在以下使用X86_FEATURE_KERNEL_IBRS:
+ *   - arch/x86/entry/calling.h|306| <<IBRS_ENTER>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+ *   - arch/x86/entry/calling.h|335| <<IBRS_EXIT>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+ *   - arch/x86/kernel/cpu/bugs.c|370| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+ *   - arch/x86/kernel/cpu/bugs.c|2252| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+ *   - arch/x86/kernel/smpboot.c|1388| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+ *   - arch/x86/kvm/vmx/vmx.c|7529| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+ *   - drivers/idle/intel_idle.c|2092| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+ */
 /*
  * IBRS kernel mitigation for Spectre_v2.
  *
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 2092d6153..8471dfe1c 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -702,8 +702,19 @@ void x86_pmu_disable_all(void)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7427| <<atomic_switch_perf_msrs>> msrs = perf_guest_get_msrs(&nr_msrs, (void *)pmu);
+ */
 struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr, void *data)
 {
+	/*
+	 * 或许?
+	 * static_call_update(x86_pmu_guest_get_msrs, x86_pmu.guest_get_msrs);
+	 *
+	 * arch/x86/events/intel/core.c|5363| <<global>> .guest_get_msrs = core_guest_get_msrs,
+	 * arch/x86/events/intel/core.c|5425| <<global>> .guest_get_msrs = intel_guest_get_msrs,
+	 */
 	return static_call(x86_pmu_guest_get_msrs)(nr, data);
 }
 EXPORT_SYMBOL_GPL(perf_guest_get_msrs);
diff --git a/arch/x86/events/zhaoxin/core.c b/arch/x86/events/zhaoxin/core.c
index 2fd9b0cf9..0011863ca 100644
--- a/arch/x86/events/zhaoxin/core.c
+++ b/arch/x86/events/zhaoxin/core.c
@@ -502,6 +502,10 @@ static __init void zhaoxin_arch_events_quirk(void)
 	}
 }
 
+/*
+ * 处理X86_VENDOR_ZHAOXIN:
+ *   - arch/x86/events/core.c|2076| <<init_hw_perf_events>> err = zhaoxin_pmu_init();
+ */
 __init int zhaoxin_pmu_init(void)
 {
 	union cpuid10_edx edx;
diff --git a/arch/x86/include/asm/hardirq.h b/arch/x86/include/asm/hardirq.h
index 6ffa8b75f..32b94f58f 100644
--- a/arch/x86/include/asm/hardirq.h
+++ b/arch/x86/include/asm/hardirq.h
@@ -73,16 +73,30 @@ extern u64 arch_irq_stat(void);
  * This function is called from noinstr interrupt contexts
  * and must be inlined to not get instrumentation.
  */
+/*
+ * 在以下使用kvm_set_cpu_l1tf_flush_l1d():
+ *   - arch/x86/include/asm/idtentry.h|215| <<DEFINE_IDTENTRY_IRQ>> kvm_set_cpu_l1tf_flush_l1d(); \
+ *   - arch/x86/include/asm/idtentry.h|260| <<DEFINE_IDTENTRY_SYSVEC>> kvm_set_cpu_l1tf_flush_l1d(); \
+ *   - arch/x86/include/asm/idtentry.h|299| <<DEFINE_IDTENTRY_SYSVEC_SIMPLE>> kvm_set_cpu_l1tf_flush_l1d(); \
+ */
 static __always_inline void kvm_set_cpu_l1tf_flush_l1d(void)
 {
 	__this_cpu_write(irq_stat.kvm_cpu_l1tf_flush_l1d, 1);
 }
 
+/*
+ * 在以下使用kvm_clear_cpu_l1tf_flush_l1d():
+ *   - arch/x86/kvm/vmx/vmx.c|7224| <<vmx_l1d_flush>> kvm_clear_cpu_l1tf_flush_l1d();
+ */
 static __always_inline void kvm_clear_cpu_l1tf_flush_l1d(void)
 {
 	__this_cpu_write(irq_stat.kvm_cpu_l1tf_flush_l1d, 0);
 }
 
+/*
+ * 在以下使用kvm_get_cpu_l1tf_flush_l1d():
+ *   - arch/x86/kvm/vmx/vmx.c|7223| <<vmx_l1d_flush>> flush_l1d |= kvm_get_cpu_l1tf_flush_l1d();
+ */
 static __always_inline bool kvm_get_cpu_l1tf_flush_l1d(void)
 {
 	return __this_cpu_read(irq_stat.kvm_cpu_l1tf_flush_l1d);
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 32ae3aa50..5b0588da6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -90,7 +90,26 @@
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|1195| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|5033| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *   - arch/x86/kvm/x86.c|10914| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *
+ * 处理函数是kvm_pmu_handle_event().
+ */
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|144| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8589| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10916| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *   - arch/x86/kvm/x86.c|11294| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+ *
+ * 处理函数是kvm_pmu_deliver_pmi().
+ */
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #ifdef CONFIG_KVM_SMM
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
@@ -531,6 +550,19 @@ struct kvm_pmc {
 	 * eventsel value for general purpose counters,
 	 * ctrl value for fixed counters.
 	 */
+	/*
+	 * 在以下使用kvm_pmc->current_config:
+	 *   - arch/x86/kvm/pmu.c|385| <<pmc_release_perf_event>> pmc->current_config = 0;
+	 *   - arch/x86/kvm/pmu.c|602| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+	 *   - arch/x86/kvm/pmu.c|607| <<reprogram_counter>> pmc->current_config = new_config;
+	 *   - arch/x86/kvm/svm/pmu.c|226| <<amd_pmu_init>> pmu->gp_counters[i].current_config = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_init>> pmu->gp_counters[i].current_config = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|609| <<intel_pmu_init>> pmu->fixed_counters[i].current_config = 0;
+	 *
+	 * only for creating or reusing perf_event,
+	 * eventsel value for general purpose counters,
+	 * ctrl value for fixed counters.
+	 */
 	u64 current_config;
 };
 
@@ -554,6 +586,20 @@ struct kvm_pmu {
 	u64 fixed_ctr_ctrl;
 	u64 fixed_ctr_ctrl_rsvd;
 	u64 global_ctrl;
+	/*
+	 * 在以下使用kvm_pmu->global_status:
+	 *   - arch/x86/kvm/pmu.c|116| <<__kvm_perf_overflow>> skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT,
+	 *              (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/pmu.c|119| <<__kvm_perf_overflow>> __set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/pmu.c|717| <<kvm_pmu_get_msr(MSR_CORE_PERF_GLOBAL_STATUS/)>> msr_info->data = pmu->global_status;
+	 *   - arch/x86/kvm/pmu.c|758| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_STATUS/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>>
+	 *              pmu->global_status = data;
+	 *   - arch/x86/kvm/pmu.c|783| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR)>>
+	 *              pmu->global_status &= ~data;
+	 *   - arch/x86/kvm/pmu.c|812| <<kvm_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|8591| <<vmx_handle_intel_pt_intr>> __set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
+	 *              (unsigned long *)&vcpu->arch.pmu.global_status);
+	 */
 	u64 global_status;
 	u64 counter_bitmask[2];
 	u64 global_ctrl_rsvd;
@@ -568,6 +614,21 @@ struct kvm_pmu {
 	 * set in a single access, e.g. to reprogram all counters when the PMU
 	 * filter changes.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|164| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|604| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|622| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|874| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1189| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|612| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1190| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(... sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1193| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	union {
 		DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
 		atomic64_t __reprogram_pmi;
@@ -1027,6 +1088,16 @@ struct kvm_vcpu_arch {
 	/* be preempted when it's in kernel-mode(cpl=0) */
 	bool preempted_in_kernel;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	/* Flush the L1 Data cache for L1TF mitigation on VMENTER */
 	bool l1tf_flush_l1d;
 
@@ -1603,6 +1674,11 @@ struct kvm_vcpu_stat {
 	u64 signal_exits;
 	u64 irq_window_exits;
 	u64 nmi_window_exits;
+	/*
+	 * 在以下使用kvm_vcpu_stat->l1d_flush:
+	 *   - arch/x86/kvm/x86.c|322| <<global>> STATS_DESC_COUNTER(VCPU, l1d_flush),
+	 *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> vcpu->stat.l1d_flush++;
+	 */
 	u64 l1d_flush;
 	u64 halt_exits;
 	u64 request_irq_exits;
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 72765b2fe..81e44c636 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -67,6 +67,44 @@
 #define MSR_TEST_CTRL_SPLIT_LOCK_DETECT_BIT	29
 #define MSR_TEST_CTRL_SPLIT_LOCK_DETECT		BIT(MSR_TEST_CTRL_SPLIT_LOCK_DETECT_BIT)
 
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|101| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL, 
+ *   - arch/x86/kvm/x86.c|324| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|572| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|182| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|201| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|247| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1360| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|2936| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3095| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4251| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2058| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2320| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2345| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7270| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7281| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|13647| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|13649| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|13652| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ *
+ * 主要在以下使用SPEC_CTRL_IBRS:
+ *   - arch/x86/entry/calling.h|364| <<IBRS_EXIT>> andl $(~SPEC_CTRL_IBRS), %edx
+ *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_start>> spec_ctrl_current() | SPEC_CTRL_IBRS, \
+ *   - arch/x86/kernel/cpu/bugs.c|2376| <<spectre_v2_select_mitigation>> x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
+ */
 #define MSR_IA32_SPEC_CTRL		0x00000048 /* Speculation Control */
 #define SPEC_CTRL_IBRS			BIT(0)	   /* Indirect Branch Restricted Speculation */
 #define SPEC_CTRL_STIBP_SHIFT		1	   /* Single Thread Indirect Branch Predictor (STIBP) bit */
@@ -83,6 +121,22 @@
 							| SPEC_CTRL_RRSBA_DIS_S \
 							| SPEC_CTRL_BHI_DIS_S)
 
+/*
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|102| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|575| <<firmware_restrict_branch_speculation_start>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|613| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4477| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7887| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3779| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3806| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
 #define MSR_IA32_PRED_CMD		0x00000049 /* Prediction Command */
 #define PRED_CMD_IBPB			BIT(0)	   /* Indirect Branch Prediction Barrier */
 #define PRED_CMD_SBPB			BIT(7)	   /* Selective Branch Prediction Barrier */
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index aee26bb82..8fc61992f 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -322,6 +322,14 @@
 #endif
 .endm
 
+/*
+ * 在以下使用X86_FEATURE_CLEAR_BHB_LOOP:
+ *   - arch/x86/include/asm/cpufeatures.h|479| <<global>> #define X86_FEATURE_CLEAR_BHB_LOOP (21*32+ 1)
+ *   - arch/x86/include/asm/nospec-branch.h|327| <<CLEAR_BRANCH_HISTORY>>
+ *                       ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
+ *   - arch/x86/kernel/cpu/bugs.c|1928| <<bhi_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+ *   - arch/x86/kernel/cpu/bugs.c|3114| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_LOOP))
+ */
 #ifdef CONFIG_X86_64
 .macro CLEAR_BRANCH_HISTORY
 	ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
@@ -444,6 +452,20 @@ static inline void call_depth_return_thunk(void) {}
 # define THUNK_TARGET(addr) [thunk_target] "r" (addr)
 
 #else /* CONFIG_X86_32 */
+/*
+ * 在以下使用X86_FEATURE_RETPOLINE:
+ *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+ *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+ *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, 
+ *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+ *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+ *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+ *
+ * v6.13或以前的在nospec-branch.h有两处
+ */
 /*
  * For i386 we use the original ret-equivalent retpoline, because
  * otherwise we'll run out of registers. We don't care about CET
@@ -519,8 +541,24 @@ void alternative_msr_write(unsigned int msr, u64 val, unsigned int feature)
 
 extern u64 x86_pred_cmd;
 
+/*
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ */
 static inline void indirect_branch_prediction_barrier(void)
 {
+	/*
+	 * 在以下使用X86_FEATURE_USE_IBPB:
+	 *   - arch/x86/include/asm/nospec-branch.h|547| <<indirect_branch_prediction_barrier>>
+	 *       alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+	 *   - arch/x86/kernel/cpu/bugs.c|1469| <<spectre_v2_user_select_mitigation>>
+	 *       setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
+	 */
 	alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
 }
 
diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
index 593f10aab..7f2957c8b 100644
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@ -1730,6 +1730,22 @@ extern bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot);
 
 static inline bool arch_has_pfn_modify_check(void)
 {
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	return boot_cpu_has_bug(X86_BUG_L1TF);
 }
 
diff --git a/arch/x86/include/asm/spec-ctrl.h b/arch/x86/include/asm/spec-ctrl.h
index 658b690b2..e69636c37 100644
--- a/arch/x86/include/asm/spec-ctrl.h
+++ b/arch/x86/include/asm/spec-ctrl.h
@@ -83,6 +83,19 @@ static inline u64 ssbd_tif_to_amd_ls_cfg(u64 tifn)
  */
 static __always_inline void __update_spec_ctrl(u64 val)
 {
+	/*
+	 * 在以下使用percpu x86_spec_ctrl_current:
+	 *   - arch/x86/entry/calling.h|321| <<IBRS_ENTER>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/entry/calling.h|341| <<IBRS_EXIT>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/include/asm/nospec-branch.h|567| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|261| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/include/asm/spec-ctrl.h|86| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|293| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|303| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+	 *   - arch/x86/kernel/cpu/bugs.c|306| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|318| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+	 *   - arch/x86/kvm/vmx/vmx.c|7501| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+	 */
 	__this_cpu_write(x86_spec_ctrl_current, val);
 	native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
 }
diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c
index c71b575bf..61838dcbb 100644
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@ -665,6 +665,20 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
 	BUG_ON(reg == 4);
 
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
 	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
 		if (cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))
@@ -1835,6 +1849,16 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 		leave_mm();
 
 	temp_state.mm = this_cpu_read(cpu_tlbstate.loaded_mm);
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(NULL, mm, current);
 
 	/*
@@ -1861,6 +1885,16 @@ static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 {
 	lockdep_assert_irqs_disabled();
 
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(NULL, prev_state.mm, current);
 
 	/* Clear the cpumask, to indicate no TLB flushing is needed anywhere */
diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 54194f599..679483bcd 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -1067,6 +1067,9 @@ static void init_amd(struct cpuinfo_x86 *c)
 	 * to protect against any future refactoring/code reorganization which might
 	 * miss setting this important bit.
 	 */
+	/*
+	 * Automatic IBRS似乎是AMD的feature
+	 */
 	if (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
 	    cpu_has(c, X86_FEATURE_AUTOIBRS))
 		WARN_ON_ONCE(msr_set_bit(MSR_EFER, _EFER_AUTOIBRS) < 0);
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index a5d0998d7..23bcce8a3 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -34,6 +34,244 @@
 
 #include "cpu.h"
 
+/*
+ * Reference:
+ * https://blogs.oracle.com/linux/post/an-update-on-meltdown-and-enhanced-ibrs
+ * https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2018/03/07/spectre-mitigation
+ * https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2018/03/24/retpoline
+ *
+ *
+ * IBRS:
+ * commit 2dbb887e875b1de3ca8f40ddf26bcfe55798c609
+ * Author: Peter Zijlstra <peterz@infradead.org>
+ * Date:   Tue Jun 14 23:15:53 2022 +0200
+ *
+ * x86/entry: Add kernel IBRS implementation
+ *
+ *
+ * EIBRS
+ *
+ * Enhanced IBRS supports an 'always on' model in which IBRS is enabled
+ * once (by setting IA32_SPEC_CTRL.IBRS) and never disabled. If
+ * IA32_SPEC_CTRL.IBRS = 1 on a processor with enhanced IBRS, the
+ * predicted targets of indirect branches executed cannot be controlled
+ * by software that was executed in a less privileged predictor mode or
+ * on another logical processor.
+ *
+ * As a result, software operating on a processor with enhanced IBRS
+ * need not use WRMSR to set IA32_SPEC_CTRL.IBRS after every transition
+ * to a more privileged predictor mode. Software can isolate predictor
+ * modes effectively simply by setting the bit once. On parts that
+ * enumerated enhanced IBRS, software need not disable IBRS or STIBP
+ * prior to entering a sleep state such as MWAIT or HLT.
+ *
+ * On processors with enhanced IBRS, an RSB overwrite sequence may not
+ * suffice to prevent the predicted target of a near return from using
+ * an RSB entry created in a less privileged predictor mode.  Software
+ * can prevent this by enabling SMEP (for transitions from user mode to
+ * supervisor mode) and by having IA32_SPEC_CTRL.IBRS set during VM
+ * exits. Processors with enhanced IBRS still support the usage model
+ * where IBRS is set only in the OS/VMM for OSes that enable SMEP. To
+ * do this, such processors will ensure that guest behavior cannot
+ * control the RSB after a VM exit once IBRS is set, even if IBRS was
+ * not set at the time of the VM exit.
+ *
+ * If the guest has cleared IBRS, the hypervisor should set IBRS after
+ * the VM exit, just as it would do on processors supporting IBRS but
+ * not enhanced IBRS. As with IBRS, enhanced IBRS does not prevent
+ * software from affecting the predicted target of an indirect branch
+ * executed at the same predictor mode. For such cases, software should
+ * use the IBPB command.
+ *
+ *
+ * STIBP:
+ * commit 53c613fe6349994f023245519265999eed75957f
+ * Author: Jiri Kosina <jikos@kernel.org>
+ * Date:   Tue Sep 25 14:38:55 2018 +0200
+ *
+ * x86/speculation: Enable cross-hyperthread spectre v2 STIBP mitigation
+ * 好像一直开着, 不是好像, 应该是就是 :)
+ *
+ *
+ * IBPB:
+ * commit 20ffa1caecca4db8f79fe665acdeaa5af815a24d
+ * Author: David Woodhouse <dwmw@amazon.co.uk>
+ * Date:   Thu Jan 25 16:14:15 2018 +0000
+ *
+ * x86/speculation: Add basic IBPB (Indirect Branch Prediction Barrier) support
+ * 请看下面关于IBPB
+ *
+ *
+ * RETPOLINE.
+ * commit 76b043848fd22dbf7f8bf3a1452f8c70d557b860
+ * Author: David Woodhouse <dwmw@amazon.co.uk>
+ * Date:   Thu Jan 11 21:46:25 2018 +0000
+ *
+ * x86/retpoline: Add initial retpoline support
+ * 主要依靠编译器的选项.
+ * 还有依靠汇编的CALL_NOSPEC, JMP_NOSPEC, RETPOLINE_CALL, RETPOLINE_JMP.
+ *
+ *
+ * RSB Filling.
+ * commit c995efd5a740d9cbafbf58bde4973e8b50b4d761
+ * Author: David Woodhouse <dwmw@amazon.co.uk>
+ * Date:   Fri Jan 12 17:49:25 2018 +0000
+ *
+ * x86/retpoline: Fill RSB on context switch for affected CPUs
+ *
+ * 用到X86_FEATURE_RSB_CTXSW的地方.
+ * arch/x86/entry/entry_32.S:	FILL_RETURN_BUFFER %ebx, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
+ * arch/x86/entry/entry_64.S:	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
+ * arch/x86/kernel/cpu/bugs.c:	setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
+ * arch/x86/kernel/cpu/bugs.c:			  boot_cpu_has(X86_FEATURE_RSB_CTXSW) ? "; RSB filling" : "",
+ *
+ *
+ * BHI.
+ * 注意:
+ * #define X86_FEATURE_CLEAR_BHB_LOOP      (21*32+ 1) // Clear branch history at syscall entry using SW loop
+ * #define X86_FEATURE_BHI_CTRL            (21*32+ 2) // BHI_DIS_S HW control available
+ * #define X86_FEATURE_CLEAR_BHB_HW        (21*32+ 3) // BHI_DIS_S HW control enabled
+ * #define X86_FEATURE_CLEAR_BHB_LOOP_ON_VMEXIT (21*32+ 4) // Clear branch history at vmexit using SW loop
+ */
+
+/*
+ * hv# cpuid -l 0x7 -1 | grep IBP
+ *       IBRS/IBPB: indirect branch restrictions  = true
+ *       STIBP: 1 thr indirect branch predictor   = true
+ *
+ * hv# cpuid -l 0x80000008 -1
+ * CPU:
+ *    Physical Address and Linear Address Size (0x80000008/eax):
+ *       maximum physical address bits         = 0x2e (46)
+ *       maximum linear (virtual) address bits = 0x30 (48)
+ *       maximum guest physical address bits   = 0x0 (0)
+ *    Extended Feature Extensions ID (0x80000008/ebx):
+ *       CLZERO instruction                       = false
+ *       instructions retired count support       = false
+ *       always save/restore error pointers       = false
+ *       INVLPGB instruction                      = false
+ *       RDPRU instruction                        = false
+ *       memory bandwidth enforcement             = false
+ *       MCOMMIT instruction                      = false
+ *       WBNOINVD instruction                     = false
+ *       IBPB: indirect branch prediction barrier = false
+ *       interruptible WBINVD, WBNOINVD           = false
+ *       IBRS: indirect branch restr speculation  = false
+ *       STIBP: 1 thr indirect branch predictor   = false
+ *       CPU prefers: IBRS always on              = false
+ *       CPU prefers: STIBP always on             = false
+ *       IBRS preferred over software solution    = false
+ *       IBRS provides same mode protection       = false
+ *       EFER[LMSLE] not supported                = false
+ *       INVLPGB supports TLB flush guest nested  = false
+ *       ppin processor id number supported       = false
+ *       SSBD: speculative store bypass disable   = false
+ *       virtualized SSBD                         = false
+ *       SSBD fixed in hardware                   = false
+ *       CPPC: collaborative processor perf ctrl  = false
+ *       PSFD: predictive store forward disable   = false
+ *       not vulnerable to branch type confusion  = false
+ *       branch sampling feature support          = false
+ *    Size Identifiers (0x80000008/ecx):
+ *       ApicIdCoreIdSize                    = 0x0 (0)
+ *       performance time-stamp counter size = 40 bits (0)
+ *    Feature Extended Size (0x80000008/edx):
+ *       max page count for INVLPGB instruction = 0x0 (0)
+ *       RDPRU instruction max input support    = 0x0 (0)
+ */
+
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+ *
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *
+ * 在以下使用X86_FEATURE_USE_IBPB:
+ *   - arch/x86/include/asm/nospec-branch.h|547| <<indirect_branch_prediction_barrier>>
+ *       alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/kernel/cpu/bugs.c|1469| <<spectre_v2_user_select_mitigation>>
+ *       setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
+ *
+ * 部分例子.
+ *
+ * context_switch()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ *
+ * switch_mm()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ */
+
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|101| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/x86.c|324| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|572| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|182| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|201| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|247| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1360| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|2936| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3095| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4251| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2058| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2320| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2345| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7270| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7281| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|13647| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|13649| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|13652| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ *
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|102| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|575| <<firmware_restrict_branch_speculation_start>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|613| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4477| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7887| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3779| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3806| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
+
 static void __init spectre_v1_select_mitigation(void);
 static void __init spectre_v2_select_mitigation(void);
 static void __init retbleed_select_mitigation(void);
@@ -50,26 +288,143 @@ static void __init l1d_flush_select_mitigation(void);
 static void __init srso_select_mitigation(void);
 static void __init gds_select_mitigation(void);
 
+/*
+ * 在以下使用x86_spec_ctrl_base:
+ *   - arch/x86/kernel/cpu/bugs.c|379| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|386| <<cpu_select_mitigations>> x86_spec_ctrl_base &= ~SPEC_CTRL_MITIGATIONS_MASK;
+ *   - arch/x86/kernel/cpu/bugs.c|1945| <<spec_ctrl_disable_kernel_rrsba>> x86_spec_ctrl_base |= SPEC_CTRL_RRSBA_DIS_S;
+ *   - arch/x86/kernel/cpu/bugs.c|1946| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2006| <<spec_ctrl_bhi_dis>> x86_spec_ctrl_base |= SPEC_CTRL_BHI_DIS_S;
+ *   - arch/x86/kernel/cpu/bugs.c|2007| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2180| <<spectre_v2_select_mitigation>> x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
+ *   - arch/x86/kernel/cpu/bugs.c|2181| <<spectre_v2_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2317| <<update_stibp_msr>> u64 val = spec_ctrl_current() | (x86_spec_ctrl_base & SPEC_CTRL_STIBP);
+ *   - arch/x86/kernel/cpu/bugs.c|2324| <<update_stibp_strict>> u64 mask = x86_spec_ctrl_base & ~SPEC_CTRL_STIBP;
+ *   - arch/x86/kernel/cpu/bugs.c|2329| <<update_stibp_strict>> if (mask == x86_spec_ctrl_base)
+ *   - arch/x86/kernel/cpu/bugs.c|2334| <<update_stibp_strict>> x86_spec_ctrl_base = mask;
+ *   - arch/x86/kernel/cpu/bugs.c|2550| <<__ssb_select_mitigation>> x86_spec_ctrl_base |= SPEC_CTRL_SSBD;
+ *   - arch/x86/kernel/cpu/bugs.c|2551| <<__ssb_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2814| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/process.c|618| <<__speculation_ctrl_update>> u64 msr = x86_spec_ctrl_base;
+ */
 /* The base value of the SPEC_CTRL MSR without task-specific bits set */
 u64 x86_spec_ctrl_base;
 EXPORT_SYMBOL_GPL(x86_spec_ctrl_base);
 
+/*
+ * 在以下使用percpu x86_spec_ctrl_current:
+ *   - arch/x86/entry/calling.h|321| <<IBRS_ENTER>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+ *   - arch/x86/entry/calling.h|341| <<IBRS_EXIT>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+ *   - arch/x86/include/asm/nospec-branch.h|567| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+ *   - arch/x86/kernel/cpu/bugs.c|261| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+ *   - arch/x86/include/asm/spec-ctrl.h|86| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|293| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|303| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+ *   - arch/x86/kernel/cpu/bugs.c|306| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|318| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+ *   - arch/x86/kvm/vmx/vmx.c|7501| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+ */
 /* The current value of the SPEC_CTRL MSR with task-specific bits set */
 DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
 EXPORT_PER_CPU_SYMBOL_GPL(x86_spec_ctrl_current);
 
+/*
+ * 在以下使用x86_pred_cmd:
+ *   - arch/x86/kernel/cpu/bugs.c|331| <<global>> u64 x86_pred_cmd __ro_after_init = PRED_CMD_IBPB;
+ *   - arch/x86/include/asm/nospec-branch.h|562| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/kernel/cpu/bugs.c|3538| <<srso_select_mitigation>> x86_pred_cmd = PRED_CMD_SBPB;
+ */
 u64 x86_pred_cmd __ro_after_init = PRED_CMD_IBPB;
 EXPORT_SYMBOL_GPL(x86_pred_cmd);
 
+/*
+ * 在以下使用x86_arch_cap_msr:
+ *   - arch/x86/kernel/cpu/bugs.c|324| <<cpu_select_mitigations>> x86_arch_cap_msr = x86_read_arch_cap_msr(); 
+ *   - arch/x86/kernel/cpu/bugs.c|523| <<taa_select_mitigation>> if ( (x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+ *   - arch/x86/kernel/cpu/bugs.c|524| <<taa_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_TSX_CTRL_MSR))
+ *   - arch/x86/kernel/cpu/bugs.c|615| <<mmio_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_FBSDP_NO))
+ *   - arch/x86/kernel/cpu/bugs.c|625| <<mmio_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_FB_CLEAR) ||
+ *   - arch/x86/kernel/cpu/bugs.c|628| <<mmio_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_MDS_NO)))
+ *   - arch/x86/kernel/cpu/bugs.c|686| <<rfds_select_mitigation>> if (x86_arch_cap_msr & ARCH_CAP_RFDS_CLEAR)
+ *   - arch/x86/kernel/cpu/bugs.c|846| <<srbds_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+ *                                                                     !boot_cpu_has(X86_FEATURE_RTM) &&
+ *   - arch/x86/kernel/cpu/bugs.c|986| <<gds_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_GDS_CTRL)) {
+ *   - arch/x86/kernel/cpu/bugs.c|1750| <<spec_ctrl_disable_kernel_rrsba>> if (!(x86_arch_cap_msr & ARCH_CAP_RRSBA)) {
+ *   - arch/x86/kernel/cpu/bugs.c|2154| <<update_mds_branch_idle>> (x86_arch_cap_msr & ARCH_CAP_FBSDP_NO)) {
+ *
+ * 很多cap, 有security的, 也有其他的.
+ */
 static u64 __ro_after_init x86_arch_cap_msr;
 
+/*
+ * 在以下使用spec_ctrl_mutex:
+ *   - arch/x86/kernel/cpu/bugs.c|353| <<global>> static DEFINE_MUTEX(spec_ctrl_mutex);
+ *   - arch/x86/kernel/cpu/bugs.c|2795| <<cpu_bugs_smt_update>> mutex_lock(&spec_ctrl_mutex);
+ *   - arch/x86/kernel/cpu/bugs.c|2846| <<cpu_bugs_smt_update>> mutex_unlock(&spec_ctrl_mutex);
+ */
 static DEFINE_MUTEX(spec_ctrl_mutex);
 
+/*
+ * 在以下使用x86_return_thunk:
+ *   - arch/x86/kernel/cpu/bugs.c|355| <<global>> void (*x86_return_thunk)(void ) __ro_after_init = __x86_return_thunk;
+ *   - arch/x86/kernel/alternative.c|816| <<patch_return>> __text_gen_insn(bytes, JMP32_INSN_OPCODE, addr, x86_return_thunk, i);
+ *   - arch/x86/kernel/cpu/bugs.c|1606| <<retbleed_select_mitigation>> x86_return_thunk = retbleed_return_thunk;
+ *   - arch/x86/kernel/cpu/bugs.c|1641| <<retbleed_select_mitigation>> x86_return_thunk = call_depth_return_thunk;
+ *   - arch/x86/kernel/cpu/bugs.c|3588| <<srso_select_mitigation>> x86_return_thunk = srso_alias_return_thunk;
+ *   - arch/x86/kernel/cpu/bugs.c|3591| <<srso_select_mitigation>> x86_return_thunk = srso_return_thunk;
+ *   - arch/x86/kernel/ftrace.c|361| <<create_trampoline>> __text_gen_insn(ip, JMP32_INSN_OPCODE, ip, x86_return_thunk, JMP32_INSN_SIZE);
+ *   - arch/x86/kernel/static_call.c|85| <<__static_call_transform>> code = text_gen_insn(JMP32_INSN_OPCODE, insn, x86_return_thunk);
+ *   - arch/x86/kernel/static_call.c|94| <<__static_call_transform>> func = x86_return_thunk;
+ *   - arch/x86/net/bpf_jit_comp.c|693| <<emit_return>> emit_jump(&prog, x86_return_thunk, ip);
+ */
 void (*x86_return_thunk)(void) __ro_after_init = __x86_return_thunk;
 
+/*
+ * BSP的callstack.
+ *
+ * [0] update_spec_ctrl
+ * [0] spectre_v2_select_mitigation
+ * [0] cpu_select_mitigations
+ * [0] arch_cpu_finalize_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ *
+ * AP的callstack.
+ *
+ * [0] update_spec_ctrl
+ * [0] x86_spec_ctrl_setup_ap
+ * [0] identify_secondary_cpu
+ * [0] smp_store_cpu_info
+ * [0] start_secondary
+ * [0] common_startup_64
+ *
+ * 在以下使用update_spec_ctrl:
+ *   - arch/x86/kernel/cpu/bugs.c|2004| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2065| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2242| <<spectre_v2_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2379| <<update_stibp_msr>> update_spec_ctrl(val);
+ *   - arch/x86/kernel/cpu/bugs.c|2612| <<__ssb_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2875| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+ */
 /* Update SPEC_CTRL MSR and its cached copy unconditionally */
 static void update_spec_ctrl(u64 val)
 {
+	/*
+	 * 在以下使用percpu x86_spec_ctrl_current:
+	 *   - arch/x86/entry/calling.h|321| <<IBRS_ENTER>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/entry/calling.h|341| <<IBRS_EXIT>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/include/asm/nospec-branch.h|567| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|261| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/include/asm/spec-ctrl.h|86| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|293| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|303| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+	 *   - arch/x86/kernel/cpu/bugs.c|306| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|318| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+	 *   - arch/x86/kvm/vmx/vmx.c|7501| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+	 */
 	this_cpu_write(x86_spec_ctrl_current, val);
 	wrmsrl(MSR_IA32_SPEC_CTRL, val);
 }
@@ -78,13 +433,42 @@ static void update_spec_ctrl(u64 val)
  * Keep track of the SPEC_CTRL MSR value for the current task, which may differ
  * from x86_spec_ctrl_base due to STIBP/SSB in __speculation_ctrl_update().
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/process.c|644| <<__speculation_ctrl_update>> update_spec_ctrl_cond(msr);
+ *
+ * 似乎主要是给STIBP和SSB用(根据注释)
+ */
 void update_spec_ctrl_cond(u64 val)
 {
+	/*
+	 * 在以下使用percpu x86_spec_ctrl_current:
+	 *   - arch/x86/entry/calling.h|321| <<IBRS_ENTER>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/entry/calling.h|341| <<IBRS_EXIT>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/include/asm/nospec-branch.h|567| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|261| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/include/asm/spec-ctrl.h|86| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|293| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|303| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+	 *   - arch/x86/kernel/cpu/bugs.c|306| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|318| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+	 *   - arch/x86/kvm/vmx/vmx.c|7501| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+	 */
 	if (this_cpu_read(x86_spec_ctrl_current) == val)
 		return;
 
 	this_cpu_write(x86_spec_ctrl_current, val);
 
+	/*
+	 * 在以下使用X86_FEATURE_KERNEL_IBRS:
+	 *   - arch/x86/entry/calling.h|306| <<IBRS_ENTER>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/entry/calling.h|335| <<IBRS_EXIT>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/kernel/cpu/bugs.c|370| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kernel/cpu/bugs.c|2252| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+	 *   - arch/x86/kernel/smpboot.c|1388| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kvm/vmx/vmx.c|7529| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+	 *   - drivers/idle/intel_idle.c|2092| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+	 */
 	/*
 	 * When KERNEL_IBRS this MSR is written on return-to-user, unless
 	 * forced the update can be delayed until that time.
@@ -93,8 +477,31 @@ void update_spec_ctrl_cond(u64 val)
 		wrmsrl(MSR_IA32_SPEC_CTRL, val);
 }
 
+/*
+ * 在以下调用spec_ctrl_current():
+ *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_start>>
+ *              alternative_msr_write(MSR_IA32_SPEC_CTRL, spec_ctrl_current() | SPEC_CTRL_IBRS, \
+ *   - arch/x86/include/asm/nospec-branch.h|590| <<firmware_restrict_branch_speculation_end>>
+ *              alternative_msr_write(MSR_IA32_SPEC_CTRL, spec_ctrl_current(), \
+ *   - drivers/idle/intel_idle.c|184| <<intel_idle_ibrs>> u64 spec_ctrl = spec_ctrl_current();
+ *   - arch/x86/kernel/cpu/bugs.c|2378| <<update_stibp_msr>> u64 val = spec_ctrl_current() | (x86_spec_ctrl_base & SPEC_CTRL_STIBP);
+ *   - drivers/idle/intel_idle.c|184| <<intel_idle_ibrs>> u64 spec_ctrl = spec_ctrl_current();
+ */
 noinstr u64 spec_ctrl_current(void)
 {
+	/*
+	 * 在以下使用percpu x86_spec_ctrl_current:
+	 *   - arch/x86/entry/calling.h|321| <<IBRS_ENTER>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/entry/calling.h|341| <<IBRS_EXIT>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/include/asm/nospec-branch.h|567| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|261| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/include/asm/spec-ctrl.h|86| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|293| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|303| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+	 *   - arch/x86/kernel/cpu/bugs.c|306| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|318| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+	 *   - arch/x86/kvm/vmx/vmx.c|7501| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+	 */
 	return this_cpu_read(x86_spec_ctrl_current);
 }
 EXPORT_SYMBOL_GPL(spec_ctrl_current);
@@ -128,6 +535,26 @@ DEFINE_STATIC_KEY_FALSE(switch_mm_cond_l1d_flush);
 DEFINE_STATIC_KEY_FALSE(mmio_stale_data_clear);
 EXPORT_SYMBOL_GPL(mmio_stale_data_clear);
 
+/*
+ * [0] spectre_v1_select_mitigation
+ * [0] cpu_select_mitigations
+ * [0] arch_cpu_finalize_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ *
+ * [    0.246019] Spectre V1 : Mitigation: usercopy/swapgs barriers and __user pointer sanitization
+ * [    0.254980] Spectre V2 : Mitigation: Retpolines
+ * [    0.255979] Spectre V2 : Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch
+ * [    0.256978] Spectre V2 : Spectre v2 / SpectreRSB : Filling RSB on VMEXIT
+ * [    0.257979] Spectre V2 : Enabling Restricted Speculation for firmware calls
+ * [    0.258980] Spectre V2 : mitigation: Enabling conditional Indirect Branch Prediction Barrier
+ * [    0.259979] Speculative Store Bypass: Mitigation: Speculative Store Bypass disabled via prctl
+ * [    0.260982] MDS: Mitigation: Clear CPU buffers
+ * [    0.261978] TAA: Mitigation: Clear CPU buffers
+ * [    0.262978] MMIO Stale Data: Vulnerable: Clear CPU buffers attempted, no microcode
+ */
 void __init cpu_select_mitigations(void)
 {
 	/*
@@ -146,6 +573,23 @@ void __init cpu_select_mitigations(void)
 		x86_spec_ctrl_base &= ~SPEC_CTRL_MITIGATIONS_MASK;
 	}
 
+	/*
+	 * 在以下使用x86_arch_cap_msr:
+	 *   - arch/x86/kernel/cpu/bugs.c|324| <<cpu_select_mitigations>> x86_arch_cap_msr = x86_read_arch_cap_msr();
+	 *   - arch/x86/kernel/cpu/bugs.c|523| <<taa_select_mitigation>> if ( (x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|524| <<taa_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_TSX_CTRL_MSR))
+	 *   - arch/x86/kernel/cpu/bugs.c|615| <<mmio_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_FBSDP_NO))
+	 *   - arch/x86/kernel/cpu/bugs.c|625| <<mmio_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_FB_CLEAR) ||
+	 *   - arch/x86/kernel/cpu/bugs.c|628| <<mmio_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_MDS_NO)))
+	 *   - arch/x86/kernel/cpu/bugs.c|686| <<rfds_select_mitigation>> if (x86_arch_cap_msr & ARCH_CAP_RFDS_CLEAR)
+	 *   - arch/x86/kernel/cpu/bugs.c|846| <<srbds_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+	 *                                                                     !boot_cpu_has(X86_FEATURE_RTM) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|986| <<gds_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_GDS_CTRL)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|1750| <<spec_ctrl_disable_kernel_rrsba>> if (!(x86_arch_cap_msr & ARCH_CAP_RRSBA)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2154| <<update_mds_branch_idle>> (x86_arch_cap_msr & ARCH_CAP_FBSDP_NO)) {
+	 *
+	 * 很多cap, 有security的, 也有其他的.
+	 */
 	x86_arch_cap_msr = x86_read_arch_cap_msr();
 
 	/* Select the proper CPU mitigations before patching alternatives: */
@@ -256,6 +700,16 @@ static void __init mds_select_mitigation(void)
 
 		setup_force_cpu_cap(X86_FEATURE_CLEAR_CPU_BUF);
 
+		/*
+		 * 在以下调用cpu_smt_disable():
+		 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+		 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+		 */
 		if (!boot_cpu_has(X86_BUG_MSBDS_ONLY) &&
 		    (mds_nosmt || cpu_mitigations_auto_nosmt()))
 			cpu_smt_disable(false);
@@ -358,6 +812,16 @@ static void __init taa_select_mitigation(void)
 	 */
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_CPU_BUF);
 
+	/*
+	 * 在以下调用cpu_smt_disable():
+	 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+	 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+	 */
 	if (taa_nosmt || cpu_mitigations_auto_nosmt())
 		cpu_smt_disable(false);
 }
@@ -455,6 +919,16 @@ static void __init mmio_select_mitigation(void)
 	else
 		mmio_mitigation = MMIO_MITIGATION_UCODE_NEEDED;
 
+	/*
+	 * 在以下调用cpu_smt_disable():
+	 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+	 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+	 */
 	if (mmio_nosmt || cpu_mitigations_auto_nosmt())
 		cpu_smt_disable(false);
 }
@@ -871,10 +1345,24 @@ enum spectre_v1_mitigation {
 	SPECTRE_V1_MITIGATION_AUTO,
 };
 
+/*
+ * 在以下使用spectre_v1_mitigation:
+ *   - arch/x86/kernel/cpu/bugs.c|1161| <<global>> static enum spectre_v1_mitigation spectre_v1_mitigation __ro_after_init =
+ *   - arch/x86/kernel/cpu/bugs.c|1227| <<spectre_v1_select_mitigation>> spectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;
+ *   - arch/x86/kernel/cpu/bugs.c|1231| <<spectre_v1_select_mitigation>> if (spectre_v1_mitigation == SPECTRE_V1_MITIGATION_AUTO) {
+ *   - arch/x86/kernel/cpu/bugs.c|1272| <<spectre_v1_select_mitigation>> pr_info("%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+ *   - arch/x86/kernel/cpu/bugs.c|1277| <<nospectre_v1_cmdline>> spectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;
+ *   - arch/x86/kernel/cpu/bugs.c|3447| <<cpu_show_common>> return sysfs_emit(buf, "%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+ */
 static enum spectre_v1_mitigation spectre_v1_mitigation __ro_after_init =
 	IS_ENABLED(CONFIG_MITIGATION_SPECTRE_V1) ?
 		SPECTRE_V1_MITIGATION_AUTO : SPECTRE_V1_MITIGATION_NONE;
 
+/*
+ * 在以下使用spectre_v1_strings[]:
+ *   - arch/x86/kernel/cpu/bugs.c|1222| <<spectre_v1_select_mitigation>> pr_info("%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+ *   - arch/x86/kernel/cpu/bugs.c|3380| <<cpu_show_common>> return sysfs_emit(buf, "%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+ */
 static const char * const spectre_v1_strings[] = {
 	[SPECTRE_V1_MITIGATION_NONE] = "Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers",
 	[SPECTRE_V1_MITIGATION_AUTO] = "Mitigation: usercopy/swapgs barriers and __user pointer sanitization",
@@ -884,6 +1372,10 @@ static const char * const spectre_v1_strings[] = {
  * Does SMAP provide full mitigation against speculative kernel access to
  * userspace?
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|1245| <<spectre_v1_select_mitigation>> !smap_works_speculatively()) {
+ */
 static bool smap_works_speculatively(void)
 {
 	if (!boot_cpu_has(X86_FEATURE_SMAP))
@@ -901,8 +1393,36 @@ static bool smap_works_speculatively(void)
 	return true;
 }
 
+/*
+ * [0] spectre_v1_select_mitigation
+ * [0] cpu_select_mitigations
+ * [0] arch_cpu_finalize_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ *
+ * [    0.246019] Spectre V1 : Mitigation: usercopy/swapgs barriers and __user pointer sanitization
+ * [    0.254980] Spectre V2 : Mitigation: Retpolines
+ * [    0.255979] Spectre V2 : Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch
+ * [    0.256978] Spectre V2 : Spectre v2 / SpectreRSB : Filling RSB on VMEXIT
+ * [    0.257979] Spectre V2 : Enabling Restricted Speculation for firmware calls
+ * [    0.258980] Spectre V2 : mitigation: Enabling conditional Indirect Branch Prediction Barrier
+ * [    0.259979] Speculative Store Bypass: Mitigation: Speculative Store Bypass disabled via prctl
+ * [    0.260982] MDS: Mitigation: Clear CPU buffers
+ * [    0.261978] TAA: Mitigation: Clear CPU buffers
+ * [    0.262978] MMIO Stale Data: Vulnerable: Clear CPU buffers attempted, no microcode
+ */
 static void __init spectre_v1_select_mitigation(void)
 {
+	/*
+	 * 在以下使用X86_BUG_SPECTRE_V1:
+	 *   - arch/x86/include/asm/cpufeatures.h|513| <<global>> #define X86_BUG_SPECTRE_V1 X86_BUG(15)
+	 *   - arch/x86/kernel/cpu/bugs.c|1173| <<spectre_v1_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1) || cpu_mitigations_off()) {
+	 *   - arch/x86/kernel/cpu/bugs.c|3345| <<cpu_show_common>> case X86_BUG_SPECTRE_V1:
+	 *   - arch/x86/kernel/cpu/bugs.c|3401| <<cpu_show_spectre_v1>> return cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V1);
+	 *   - arch/x86/kernel/cpu/common.c|1332| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1) || cpu_mitigations_off()) {
 		spectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;
 		return;
@@ -944,6 +1464,11 @@ static void __init spectre_v1_select_mitigation(void)
 		}
 	}
 
+	/*
+	 * 在以下使用spectre_v1_strings[]:
+	 *   - arch/x86/kernel/cpu/bugs.c|1222| <<spectre_v1_select_mitigation>> pr_info("%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+	 *   - arch/x86/kernel/cpu/bugs.c|3380| <<cpu_show_common>> return sysfs_emit(buf, "%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+	 */
 	pr_info("%s\n", spectre_v1_strings[spectre_v1_mitigation]);
 }
 
@@ -1032,13 +1557,36 @@ early_param("retbleed", retbleed_parse_cmdline);
 #define RETBLEED_UNTRAIN_MSG "WARNING: BTB untrained return thunk mitigation is only effective on AMD/Hygon!\n"
 #define RETBLEED_INTEL_MSG "WARNING: Spectre v2 mitigation leaves CPU vulnerable to RETBleed attacks, data leaks possible!\n"
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|577| <<cpu_select_mitigations>> retbleed_select_mitigation();
+ */
 static void __init retbleed_select_mitigation(void)
 {
 	bool mitigate_smt = false;
 
+	/*
+	 * 在以下使用X86_BUG_RETBLEED:
+	 *   - arch/x86/kernel/cpu/bugs.c|1519| <<retbleed_parse_cmdline>> setup_force_cpu_bug(X86_BUG_RETBLEED);
+	 *   - arch/x86/kernel/cpu/bugs.c|1538| <<retbleed_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_RETBLEED) || cpu_mitigations_off())
+	 *   - arch/x86/kernel/cpu/bugs.c|2468| <<spectre_v2_select_mitigation>> boot_cpu_has_bug(X86_BUG_RETBLEED) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|2696| <<spectre_v2_select_mitigation>> if (boot_cpu_has_bug(X86_BUG_RETBLEED) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|3980| <<cpu_show_common>> case X86_BUG_RETBLEED:
+	 *   - arch/x86/kernel/cpu/bugs.c|4079| <<cpu_show_retbleed>> return cpu_show_common(dev, attr, buf, X86_BUG_RETBLEED);
+	 *   - arch/x86/kernel/cpu/common.c|1511| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_RETBLEED);
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_RETBLEED) || cpu_mitigations_off())
 		return;
 
+	/*
+	 * enum retbleed_mitigation_cmd {
+	 *     RETBLEED_CMD_OFF,
+	 *     RETBLEED_CMD_AUTO,
+	 *     RETBLEED_CMD_UNRET,
+	 *     RETBLEED_CMD_IBPB,
+	 *     RETBLEED_CMD_STUFF,
+	 * };
+	 */
 	switch (retbleed_cmd) {
 	case RETBLEED_CMD_OFF:
 		return;
@@ -1099,8 +1647,32 @@ static void __init retbleed_select_mitigation(void)
 		break;
 	}
 
+	/*
+	 * enum retbleed_mitigation {
+	 *     RETBLEED_MITIGATION_NONE,
+	 *     RETBLEED_MITIGATION_UNRET,
+	 *     RETBLEED_MITIGATION_IBPB,
+	 *     RETBLEED_MITIGATION_IBRS,
+	 *     RETBLEED_MITIGATION_EIBRS,
+	 *     RETBLEED_MITIGATION_STUFF,
+	 * };
+	 */
 	switch (retbleed_mitigation) {
 	case RETBLEED_MITIGATION_UNRET:
+		/*
+		 * 在以下使用X86_FEATURE_RETHUNK:
+		 *   - arch/x86/kernel/alternative.c|814| <<patch_return>> if (cpu_feature_enabled(X86_FEATURE_RETHUNK)) {
+		 *   - arch/x86/kernel/alternative.c|832| <<apply_returns>> if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		 *   - arch/x86/kernel/cpu/bugs.c|1603| <<retbleed_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETHUNK);
+		 *   - arch/x86/kernel/cpu/bugs.c|1626| <<retbleed_select_mitigation>> setup_clear_cpu_cap(X86_FEATURE_RETHUNK);
+		 *   - arch/x86/kernel/cpu/bugs.c|1638| <<retbleed_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETHUNK);
+		 *   - arch/x86/kernel/cpu/bugs.c|3583| <<srso_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETHUNK);
+		 *   - arch/x86/kernel/cpu/bugs.c|3615| <<srso_select_mitigation>> setup_clear_cpu_cap(X86_FEATURE_RETHUNK);
+		 *   - arch/x86/kernel/ftrace.c|360| <<create_trampoline>> if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		 *   - arch/x86/kernel/static_call.c|84| <<__static_call_transform>> if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		 *   - arch/x86/kernel/static_call.c|93| <<__static_call_transform>> if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		 *   - arch/x86/net/bpf_jit_comp.c|692| <<emit_return>> if (cpu_feature_enabled(X86_FEATURE_RETHUNK)) {
+		 */
 		setup_force_cpu_cap(X86_FEATURE_RETHUNK);
 		setup_force_cpu_cap(X86_FEATURE_UNRET);
 
@@ -1146,6 +1718,16 @@ static void __init retbleed_select_mitigation(void)
 		break;
 	}
 
+	/*
+	 * 在以下调用cpu_smt_disable():
+	 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+	 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+	 */
 	if (mitigate_smt && !boot_cpu_has(X86_FEATURE_STIBP) &&
 	    (retbleed_nosmt || cpu_mitigations_auto_nosmt()))
 		cpu_smt_disable(false);
@@ -1176,8 +1758,24 @@ static void __init retbleed_select_mitigation(void)
 #undef pr_fmt
 #define pr_fmt(fmt)     "Spectre V2 : " fmt
 
+/*
+ * 主要在以下使用spectre_v2_user_stibp来判断怎样实现:
+ *   - arch/x86/kernel/cpu/bugs.c|2654| <<cpu_bugs_smt_update>> switch (spectre_v2_user_stibp) {
+ */
 static enum spectre_v2_user_mitigation spectre_v2_user_stibp __ro_after_init =
 	SPECTRE_V2_USER_NONE;
+/*
+ * 在以下使用spectre_v2_user_ibpb:
+ *   - arch/x86/kernel/cpu/bugs.c|1919| <<spectre_v2_user_select_mitigation>> spectre_v2_user_ibpb = mode;
+ *   - arch/x86/kernel/cpu/bugs.c|1927| <<spectre_v2_user_select_mitigation>> spectre_v2_user_ibpb = SPECTRE_V2_USER_STRICT;
+ *   - arch/x86/kernel/cpu/bugs.c|2945| <<is_spec_ib_user_controlled>> return spectre_v2_user_ibpb == SPECTRE_V2_USER_PRCTL ||
+ *   - arch/x86/kernel/cpu/bugs.c|2946| <<is_spec_ib_user_controlled>> spectre_v2_user_ibpb == SPECTRE_V2_USER_SECCOMP ||
+ *   - arch/x86/kernel/cpu/bugs.c|2955| <<ib_prctl_set>> if (spectre_v2_user_ibpb == SPECTRE_V2_USER_NONE &&
+ *   - arch/x86/kernel/cpu/bugs.c|2987| <<ib_prctl_set>> if (spectre_v2_user_ibpb == SPECTRE_V2_USER_NONE &&
+ *   - arch/x86/kernel/cpu/bugs.c|3036| <<arch_seccomp_spec_mitigate>> if (spectre_v2_user_ibpb == SPECTRE_V2_USER_SECCOMP ||
+ *   - arch/x86/kernel/cpu/bugs.c|3080| <<ib_prctl_get>> if (spectre_v2_user_ibpb == SPECTRE_V2_USER_NONE &&
+ *   - arch/x86/kernel/cpu/bugs.c|3089| <<ib_prctl_get>> } else if (spectre_v2_user_ibpb == SPECTRE_V2_USER_STRICT ||
+ */
 static enum spectre_v2_user_mitigation spectre_v2_user_ibpb __ro_after_init =
 	SPECTRE_V2_USER_NONE;
 
@@ -1260,6 +1858,10 @@ enum spectre_v2_user_cmd {
 	SPECTRE_V2_USER_CMD_SECCOMP_IBPB,
 };
 
+/*
+ * 只在以下使用spectre_v2_user_strings[]:
+ *   - arch/x86/kernel/cpu/bugs.c|1945| <<spectre_v2_user_select_mitigation>> pr_info("%s\n", spectre_v2_user_strings[mode]);
+ */
 static const char * const spectre_v2_user_strings[] = {
 	[SPECTRE_V2_USER_NONE]			= "User space: Vulnerable",
 	[SPECTRE_V2_USER_STRICT]		= "User space: Mitigation: STIBP protection",
@@ -1288,14 +1890,28 @@ static void __init spec_v2_user_print_cond(const char *reason, bool secure)
 		pr_info("spectre_v2_user=%s forced on command line.\n", reason);
 }
 
+/*
+ * 在以下使用spectre_v2_cmd:
+ *   - arch/x86/kernel/cpu/bugs.c|1828| <<spectre_v2_parse_user_cmdline>> switch (spectre_v2_cmd) {
+ *   - arch/x86/kernel/cpu/bugs.c|2651| <<spectre_v2_select_mitigation>> spectre_v2_cmd = cmd;
+ */
 static __ro_after_init enum spectre_v2_mitigation_cmd spectre_v2_cmd;
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|1938| <<spectre_v2_user_select_mitigation>> cmd = spectre_v2_parse_user_cmdline();
+ */
 static enum spectre_v2_user_cmd __init
 spectre_v2_parse_user_cmdline(void)
 {
 	char arg[20];
 	int ret, i;
 
+	/*
+	 * 在以下使用spectre_v2_cmd:
+	 *   - arch/x86/kernel/cpu/bugs.c|1828| <<spectre_v2_parse_user_cmdline>> switch (spectre_v2_cmd) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2651| <<spectre_v2_select_mitigation>> spectre_v2_cmd = cmd;
+	 */
 	switch (spectre_v2_cmd) {
 	case SPECTRE_V2_CMD_NONE:
 		return SPECTRE_V2_USER_CMD_NONE;
@@ -1322,16 +1938,83 @@ spectre_v2_parse_user_cmdline(void)
 	return SPECTRE_V2_USER_CMD_AUTO;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|2476| <<spectre_v2_select_mitigation>> if (spectre_v2_in_ibrs_mode(mode)) {
+ *   - arch/x86/kernel/cpu/bugs.c|2645| <<spectre_v2_select_mitigation>> } else if (boot_cpu_has(X86_FEATURE_IBRS) && !spectre_v2_in_ibrs_mode(mode)) {
+ */
 static inline bool spectre_v2_in_ibrs_mode(enum spectre_v2_mitigation mode)
 {
 	return spectre_v2_in_eibrs_mode(mode) || mode == SPECTRE_V2_IBRS;
 }
 
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+ *
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *
+ * 在以下使用X86_FEATURE_USE_IBPB:
+ *   - arch/x86/include/asm/nospec-branch.h|547| <<indirect_branch_prediction_barrier>>
+ *       alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/kernel/cpu/bugs.c|1469| <<spectre_v2_user_select_mitigation>>
+ *       setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
+ *
+ * 部分例子.
+ *
+ * context_switch()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ *
+ * switch_mm()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ */
 static void __init
 spectre_v2_user_select_mitigation(void)
 {
+	/*
+	 * enum spectre_v2_user_mitigation {
+	 *     SPECTRE_V2_USER_NONE,
+	 *     SPECTRE_V2_USER_STRICT,
+	 *     SPECTRE_V2_USER_STRICT_PREFERRED,
+	 *     SPECTRE_V2_USER_PRCTL,
+	 *     SPECTRE_V2_USER_SECCOMP,
+	 * };
+	 */
 	enum spectre_v2_user_mitigation mode = SPECTRE_V2_USER_NONE;
 	bool smt_possible = IS_ENABLED(CONFIG_SMP);
+	/*
+	 * enum spectre_v2_user_cmd {
+	 *     SPECTRE_V2_USER_CMD_NONE,
+	 *     SPECTRE_V2_USER_CMD_AUTO,
+	 *     SPECTRE_V2_USER_CMD_FORCE,
+	 *     SPECTRE_V2_USER_CMD_PRCTL,
+	 *     SPECTRE_V2_USER_CMD_PRCTL_IBPB,
+	 *     SPECTRE_V2_USER_CMD_SECCOMP,
+	 *     SPECTRE_V2_USER_CMD_SECCOMP_IBPB,
+	 * };
+	 */
 	enum spectre_v2_user_cmd cmd;
 
 	if (!boot_cpu_has(X86_FEATURE_IBPB) && !boot_cpu_has(X86_FEATURE_STIBP))
@@ -1341,6 +2024,9 @@ spectre_v2_user_select_mitigation(void)
 	    cpu_smt_control == CPU_SMT_NOT_SUPPORTED)
 		smt_possible = false;
 
+	/*
+	 * 处理"spectre_v2_user="
+	 */
 	cmd = spectre_v2_parse_user_cmdline();
 	switch (cmd) {
 	case SPECTRE_V2_USER_CMD_NONE:
@@ -1364,6 +2050,13 @@ spectre_v2_user_select_mitigation(void)
 
 	/* Initialize Indirect Branch Prediction Barrier */
 	if (boot_cpu_has(X86_FEATURE_IBPB)) {
+		/*
+		 * 在以下使用X86_FEATURE_USE_IBPB:
+		 *   - arch/x86/include/asm/nospec-branch.h|547| <<indirect_branch_prediction_barrier>>
+		 *       alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+		 *   - arch/x86/kernel/cpu/bugs.c|1469| <<spectre_v2_user_select_mitigation>>
+		 *       setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
+		 */
 		setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
 
 		spectre_v2_user_ibpb = mode;
@@ -1465,6 +2158,20 @@ static void __init spec_v2_print_cond(const char *reason, bool secure)
 
 static enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)
 {
+	/*
+	 * enum spectre_v2_mitigation_cmd {
+	 *     SPECTRE_V2_CMD_NONE,
+	 *     SPECTRE_V2_CMD_AUTO,
+	 *     SPECTRE_V2_CMD_FORCE,
+	 *     SPECTRE_V2_CMD_RETPOLINE,
+	 *     SPECTRE_V2_CMD_RETPOLINE_GENERIC,
+	 *     SPECTRE_V2_CMD_RETPOLINE_LFENCE,
+	 *     SPECTRE_V2_CMD_EIBRS,
+	 *     SPECTRE_V2_CMD_EIBRS_RETPOLINE,
+	 *     SPECTRE_V2_CMD_EIBRS_LFENCE,
+	 *     SPECTRE_V2_CMD_IBRS,
+	 * };
+	 */
 	enum spectre_v2_mitigation_cmd cmd;
 	char arg[20];
 	int ret, i;
@@ -1505,6 +2212,17 @@ static enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)
 	     cmd == SPECTRE_V2_CMD_EIBRS_LFENCE ||
 	     cmd == SPECTRE_V2_CMD_EIBRS_RETPOLINE) &&
 	    !boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		/*
+		 * 在以下使用X86_FEATURE_IBRS_ENHANCED:
+		 *   - arch/x86/kernel/cpu/bugs.c|1980| <<spectre_v2_parse_cmdline>> if (... !boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2229| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2309| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))
+		 *   - arch/x86/kernel/cpu/common.c|1405| <<cpu_set_bug_bits>> setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
+		 *   - arch/x86/kernel/cpu/common.c|1495| <<cpu_set_bug_bits>> if (... (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED) ||
+		 *
+		 * 在Intel上是(x86_arch_cap_msr & ARCH_CAP_IBRS_ALL)
+		 * 在AMD上是cpu_has(c, X86_FEATURE_AUTOIBRS)
+		 */
 		pr_err("%s selected but CPU doesn't have Enhanced or Automatic IBRS. Switching to AUTO select\n",
 		       mitigation_options[i].option);
 		return SPECTRE_V2_CMD_AUTO;
@@ -1668,11 +2386,29 @@ static int __init spectre_bhi_parse_cmdline(char *str)
 }
 early_param("spectre_bhi", spectre_bhi_parse_cmdline);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|2571| <<spectre_v2_select_mitigation>> bhi_select_mitigation();
+ */
 static void __init bhi_select_mitigation(void)
 {
 	if (bhi_mitigation == BHI_MITIGATION_OFF)
 		return;
 
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	/* Retpoline mitigates against BHI unless the CPU has RRSBA behavior */
 	if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
 	    !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE)) {
@@ -1694,6 +2430,27 @@ static void __init bhi_select_mitigation(void)
 		return;
 	}
 
+	/*
+	 * 在以下使用X86_FEATURE_CLEAR_BHB_LOOP:
+	 *   - arch/x86/include/asm/cpufeatures.h|479| <<global>> #define X86_FEATURE_CLEAR_BHB_LOOP (21*32+ 1) 
+	 *   - arch/x86/include/asm/nospec-branch.h|327| <<CLEAR_BRANCH_HISTORY>>
+	 *                       ALTERNATIVE "", "call clear_bhb_loop", X86_FEATURE_CLEAR_BHB_LOOP
+	 *   - arch/x86/kernel/cpu/bugs.c|1928| <<bhi_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
+	 *   - arch/x86/kernel/cpu/bugs.c|3114| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_CLEAR_BHB_LOOP))
+	 *
+	 *
+	 * 使用CLEAR_BRANCH_HISTORY和CLEAR_BRANCH_HISTORY_VMEXIT的地方:
+	 * arch/x86/entry/common.c: * 4) int80_emulation() does a CLEAR_BRANCH_HISTORY. While FRED will
+	 * arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/entry/entry_64_compat.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/entry/entry_64.S:	CLEAR_BRANCH_HISTORY
+	 * arch/x86/include/asm/nospec-branch.h:.macro CLEAR_BRANCH_HISTORY
+	 * arch/x86/include/asm/nospec-branch.h:.macro CLEAR_BRANCH_HISTORY_VMEXIT
+	 * arch/x86/include/asm/nospec-branch.h:#define CLEAR_BRANCH_HISTORY
+	 * arch/x86/include/asm/nospec-branch.h:#define CLEAR_BRANCH_HISTORY_VMEXIT
+	 * arch/x86/kvm/vmx/vmenter.S:	CLEAR_BRANCH_HISTORY_VMEXIT
+	 */
 	pr_info("Spectre BHI mitigation: SW BHB clearing on syscall and VM exit\n");
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP);
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP_ON_VMEXIT);
@@ -1701,6 +2458,20 @@ static void __init bhi_select_mitigation(void)
 
 static void __init spectre_v2_select_mitigation(void)
 {
+	/*
+	 * enum spectre_v2_mitigation_cmd {
+	 *     SPECTRE_V2_CMD_NONE,
+	 *     SPECTRE_V2_CMD_AUTO,
+	 *     SPECTRE_V2_CMD_FORCE,
+	 *     SPECTRE_V2_CMD_RETPOLINE,
+	 *     SPECTRE_V2_CMD_RETPOLINE_GENERIC,
+	 *     SPECTRE_V2_CMD_RETPOLINE_LFENCE,
+	 *     SPECTRE_V2_CMD_EIBRS,
+	 *     SPECTRE_V2_CMD_EIBRS_RETPOLINE,
+	 *     SPECTRE_V2_CMD_EIBRS_LFENCE,
+	 *     SPECTRE_V2_CMD_IBRS,
+	 * };
+	 */
 	enum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();
 	enum spectre_v2_mitigation mode = SPECTRE_V2_NONE;
 
@@ -1718,6 +2489,49 @@ static void __init spectre_v2_select_mitigation(void)
 
 	case SPECTRE_V2_CMD_FORCE:
 	case SPECTRE_V2_CMD_AUTO:
+		/*
+		 * 在以下使用X86_FEATURE_IBRS_ENHANCED:
+		 *   - arch/x86/kernel/cpu/bugs.c|1980| <<spectre_v2_parse_cmdline>> if (... !boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2229| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2309| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))
+		 *   - arch/x86/kernel/cpu/common.c|1405| <<cpu_set_bug_bits>> setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
+		 *   - arch/x86/kernel/cpu/common.c|1495| <<cpu_set_bug_bits>> if (... (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED) ||
+		 *
+		 * Enhanced IBRS supports an 'always on' model in which IBRS is enabled
+		 * once (by setting IA32_SPEC_CTRL.IBRS) and never disabled. If
+		 * IA32_SPEC_CTRL.IBRS = 1 on a processor with enhanced IBRS, the
+		 * predicted targets of indirect branches executed cannot be controlled
+		 * by software that was executed in a less privileged predictor mode or
+		 * on another logical processor.
+		 *
+		 * As a result, software operating on a processor with enhanced IBRS
+		 * need not use WRMSR to set IA32_SPEC_CTRL.IBRS after every transition
+		 * to a more privileged predictor mode. Software can isolate predictor
+		 * modes effectively simply by setting the bit once. On parts that
+		 * enumerated enhanced IBRS, software need not disable IBRS or STIBP
+		 * prior to entering a sleep state such as MWAIT or HLT.
+		 *
+		 * On processors with enhanced IBRS, an RSB overwrite sequence may not
+		 * suffice to prevent the predicted target of a near return from using
+		 * an RSB entry created in a less privileged predictor mode.  Software
+		 * can prevent this by enabling SMEP (for transitions from user mode to
+		 * supervisor mode) and by having IA32_SPEC_CTRL.IBRS set during VM
+		 * exits. Processors with enhanced IBRS still support the usage model
+		 * where IBRS is set only in the OS/VMM for OSes that enable SMEP. To
+		 * do this, such processors will ensure that guest behavior cannot
+		 * control the RSB after a VM exit once IBRS is set, even if IBRS was
+		 * not set at the time of the VM exit.
+		 *
+		 * If the guest has cleared IBRS, the hypervisor should set IBRS after
+		 * the VM exit, just as it would do on processors supporting IBRS but
+		 * not enhanced IBRS. As with IBRS, enhanced IBRS does not prevent
+		 * software from affecting the predicted target of an indirect branch
+		 * executed at the same predictor mode. For such cases, software should
+		 * use the IBPB command.
+		 *
+		 * 在Intel上是(x86_arch_cap_msr & ARCH_CAP_IBRS_ALL)
+		 * 在AMD上是cpu_has(c, X86_FEATURE_AUTOIBRS)
+		 */
 		if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
 			mode = SPECTRE_V2_EIBRS;
 			break;
@@ -1771,9 +2585,27 @@ static void __init spectre_v2_select_mitigation(void)
 
 	if (spectre_v2_in_ibrs_mode(mode)) {
 		if (boot_cpu_has(X86_FEATURE_AUTOIBRS)) {
+			/*
+			 * Automatic IBRS似乎是AMD的feature
+			 */
 			msr_set_bit(MSR_EFER, _EFER_AUTOIBRS);
 		} else {
+			/*
+			 * 主要在以下使用SPEC_CTRL_IBRS:
+			 *   - arch/x86/entry/calling.h|364| <<IBRS_EXIT>> andl $(~SPEC_CTRL_IBRS), %edx
+			 *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_start>> spec_ctrl_current() | SPEC_CTRL_IBRS, \
+			 *   - arch/x86/kernel/cpu/bugs.c|2376| <<spectre_v2_select_mitigation>> x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
+			 */
 			x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
+			/*
+			 * 在以下使用update_spec_ctrl:
+			 *   - arch/x86/kernel/cpu/bugs.c|2004| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2065| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2242| <<spectre_v2_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2379| <<update_stibp_msr>> update_spec_ctrl(val);
+			 *   - arch/x86/kernel/cpu/bugs.c|2612| <<__ssb_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2875| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+			 */
 			update_spec_ctrl(x86_spec_ctrl_base);
 		}
 	}
@@ -1784,7 +2616,32 @@ static void __init spectre_v2_select_mitigation(void)
 		break;
 
 	case SPECTRE_V2_IBRS:
+		/*
+		 * 在以下使用X86_FEATURE_KERNEL_IBRS:
+		 *   - arch/x86/entry/calling.h|306| <<IBRS_ENTER>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+		 *   - arch/x86/entry/calling.h|335| <<IBRS_EXIT>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+		 *   - arch/x86/kernel/cpu/bugs.c|370| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+		 *   - arch/x86/kernel/cpu/bugs.c|2252| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+		 *   - arch/x86/kernel/smpboot.c|1388| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+		 *   - arch/x86/kvm/vmx/vmx.c|7529| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+		 *   - drivers/idle/intel_idle.c|2092| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+		 *
+		 * 重点是这里
+		 * 设置了X86_FEATURE_KERNEL_IBRS
+		 * 才能在IBRS_ENTER和IBRS_EXIT重复使用
+		 */
 		setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+		/*
+		 * 在以下使用X86_FEATURE_IBRS_ENHANCED:
+		 *   - arch/x86/kernel/cpu/bugs.c|1980| <<spectre_v2_parse_cmdline>> if (... !boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2229| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2309| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))
+		 *   - arch/x86/kernel/cpu/common.c|1405| <<cpu_set_bug_bits>> setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
+		 *   - arch/x86/kernel/cpu/common.c|1495| <<cpu_set_bug_bits>> if (... (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED) ||
+		 *
+		 * 在Intel上是(x86_arch_cap_msr & ARCH_CAP_IBRS_ALL)
+		 * 在AMD上是cpu_has(c, X86_FEATURE_AUTOIBRS)
+		 */
 		if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))
 			pr_warn(SPECTRE_V2_IBRS_PERF_MSG);
 		break;
@@ -1796,6 +2653,20 @@ static void __init spectre_v2_select_mitigation(void)
 
 	case SPECTRE_V2_RETPOLINE:
 	case SPECTRE_V2_EIBRS_RETPOLINE:
+		/*
+		 * 在以下使用X86_FEATURE_RETPOLINE:
+		 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+		 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+		 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+		 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+		 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+		 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+		 *
+		 * v6.13或以前的在nospec-branch.h有两处
+		 */
 		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
 		break;
 	}
@@ -1814,6 +2685,17 @@ static void __init spectre_v2_select_mitigation(void)
 		bhi_select_mitigation();
 
 	spectre_v2_enabled = mode;
+	/*
+	 * 2038 static const char * const spectre_v2_strings[] = {
+	 * 2039         [SPECTRE_V2_NONE]                       = "Vulnerable",
+	 * 2040         [SPECTRE_V2_RETPOLINE]                  = "Mitigation: Retpolines",
+	 * 2041         [SPECTRE_V2_LFENCE]                     = "Mitigation: LFENCE",
+	 * 2042         [SPECTRE_V2_EIBRS]                      = "Mitigation: Enhanced / Automatic IBRS",
+	 * 2043         [SPECTRE_V2_EIBRS_LFENCE]               = "Mitigation: Enhanced / Automatic IBRS + LFENCE",
+	 * 2044         [SPECTRE_V2_EIBRS_RETPOLINE]            = "Mitigation: Enhanced / Automatic IBRS + Retpolines",
+	 * 2045         [SPECTRE_V2_IBRS]                       = "Mitigation: IBRS",
+	 * 2046 };
+	 */
 	pr_info("%s\n", spectre_v2_strings[mode]);
 
 	/*
@@ -1854,6 +2736,20 @@ static void __init spectre_v2_select_mitigation(void)
 	 *
 	 * FIXME: Is this pointless for retbleed-affected AMD?
 	 */
+	/*
+	 * RSB Filling.
+	 * commit c995efd5a740d9cbafbf58bde4973e8b50b4d761
+	 * Author: David Woodhouse <dwmw@amazon.co.uk>
+	 * Date:   Fri Jan 12 17:49:25 2018 +0000
+	 *
+	 * x86/retpoline: Fill RSB on context switch for affected CPUs
+	 *
+	 * 用到X86_FEATURE_RSB_CTXSW的地方.
+	 * arch/x86/entry/entry_32.S:   FILL_RETURN_BUFFER %ebx, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
+	 * arch/x86/entry/entry_64.S:   FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
+	 * arch/x86/kernel/cpu/bugs.c:  setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
+	 * arch/x86/kernel/cpu/bugs.c:                    boot_cpu_has(X86_FEATURE_RSB_CTXSW) ? "; RSB filling" : "",
+	 */
 	setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
 	pr_info("Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\n");
 
@@ -1885,16 +2781,29 @@ static void __init spectre_v2_select_mitigation(void)
 		pr_info("Enabling Restricted Speculation for firmware calls\n");
 	}
 
+	/*
+	 * 在以下使用spectre_v2_cmd:
+	 *   - arch/x86/kernel/cpu/bugs.c|1828| <<spectre_v2_parse_user_cmdline>> switch (spectre_v2_cmd) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2651| <<spectre_v2_select_mitigation>> spectre_v2_cmd = cmd;
+	 */
 	/* Set up IBPB and STIBP depending on the general spectre V2 command */
 	spectre_v2_cmd = cmd;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|2593| <<update_stibp_strict>> on_each_cpu(update_stibp_msr, NULL, 1);
+ */
 static void update_stibp_msr(void * __unused)
 {
 	u64 val = spec_ctrl_current() | (x86_spec_ctrl_base & SPEC_CTRL_STIBP);
 	update_spec_ctrl(val);
 }
 
+/*
+ * 在以下调用update_stibp_strict():
+ *   - arch/x86/kernel/cpu/bugs.c|2647| <<cpu_bugs_smt_update>> update_stibp_strict();
+ */
 /* Update x86_spec_ctrl_base in case SMT state changed. */
 static void update_stibp_strict(void)
 {
@@ -1950,6 +2859,10 @@ static void update_mds_branch_idle(void)
 #define TAA_MSG_SMT "TAA CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html for more details.\n"
 #define MMIO_MSG_SMT "MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.\n"
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/common.c|2506| <<arch_smt_update>> cpu_bugs_smt_update();
+ */
 void cpu_bugs_smt_update(void)
 {
 	mutex_lock(&spec_ctrl_mutex);
@@ -2217,6 +3130,12 @@ static int ssb_prctl_set(struct task_struct *task, unsigned long ctrl)
 	return 0;
 }
 
+/*
+ * 在以下使用is_spec_ib_user_controlled():
+ *   - arch/x86/kernel/cpu/bugs.c|2974| <<ib_prctl_set>> if (!is_spec_ib_user_controlled() ||
+ *   - arch/x86/kernel/cpu/bugs.c|2991| <<ib_prctl_set>> if (!is_spec_ib_user_controlled())
+ *   - arch/x86/kernel/cpu/bugs.c|3083| <<ib_prctl_get>> else if (is_spec_ib_user_controlled()) {
+ */
 static bool is_spec_ib_user_controlled(void)
 {
 	return spectre_v2_user_ibpb == SPECTRE_V2_USER_PRCTL ||
@@ -2225,6 +3144,11 @@ static bool is_spec_ib_user_controlled(void)
 		spectre_v2_user_stibp == SPECTRE_V2_USER_SECCOMP;
 }
 
+/*
+ * 在以下使用ib_prctl_set():
+ *   - arch/x86/kernel/cpu/bugs.c|3023| <<arch_prctl_spec_ctrl_set>> return ib_prctl_set(task, ctrl);
+ *   - arch/x86/kernel/cpu/bugs.c|3038| <<arch_seccomp_spec_mitigate>> ib_prctl_set(task, PR_SPEC_FORCE_DISABLE);
+ */
 static int ib_prctl_set(struct task_struct *task, unsigned long ctrl)
 {
 	switch (ctrl) {
@@ -2272,6 +3196,15 @@ static int ib_prctl_set(struct task_struct *task, unsigned long ctrl)
 		if (ctrl == PR_SPEC_FORCE_DISABLE)
 			task_set_spec_ib_force_disable(task);
 		task_update_spec_tif(task);
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (task == current)
 			indirect_branch_prediction_barrier();
 		break;
@@ -2281,6 +3214,11 @@ static int ib_prctl_set(struct task_struct *task, unsigned long ctrl)
 	return 0;
 }
 
+/*
+ * 在以下使用arch_prctl_spec_ctrl_set():
+ *   - kernel/sys.c|2687| <<SYSCALL_DEFINE5(prctl:PR_SET_SPECULATION_CTRL)>>
+ *          error = arch_prctl_spec_ctrl_set(me, arg2, arg3);
+ */
 int arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,
 			     unsigned long ctrl)
 {
@@ -2391,12 +3329,66 @@ EXPORT_SYMBOL_GPL(itlb_multihit_kvm_mitigation);
 #undef pr_fmt
 #define pr_fmt(fmt)	"L1TF: " fmt
 
+/*
+ * 在以下使用l1tf_mitigation:
+ *   - arch/x86/kernel/cpu/bugs.c|3354| <<l1tf_select_mitigation>> l1tf_mitigation = L1TF_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|3356| <<l1tf_select_mitigation>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOSMT;
+ *   - arch/x86/kernel/cpu/bugs.c|3360| <<l1tf_select_mitigation>> switch (l1tf_mitigation) {
+ *   - arch/x86/kernel/cpu/bugs.c|3390| <<l1tf_select_mitigation>> if (l1tf_mitigation != L1TF_MITIGATION_OFF &&
+ *   - arch/x86/kernel/cpu/bugs.c|3412| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|3414| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOWARN;
+ *   - arch/x86/kernel/cpu/bugs.c|3416| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH;
+ *   - arch/x86/kernel/cpu/bugs.c|3418| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOSMT;
+ *   - arch/x86/kernel/cpu/bugs.c|3420| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FULL;
+ *   - arch/x86/kernel/cpu/bugs.c|3422| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FULL_FORCE;
+ *   - arch/x86/kvm/vmx/vmx.c|364| <<vmx_setup_l1d_flush>> switch (l1tf_mitigation) {
+ *   - arch/x86/kvm/vmx/vmx.c|378| <<vmx_setup_l1d_flush>> } else if (l1tf_mitigation == L1TF_MITIGATION_FULL_FORCE) {
+ *   - arch/x86/kvm/vmx/vmx.c|8287| <<vmx_vm_init>> switch (l1tf_mitigation) {
+ *   - arch/x86/mm/init.c|1058| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+ *
+ * enum l1tf_mitigations {
+ *     L1TF_MITIGATION_OFF,
+ *     L1TF_MITIGATION_FLUSH_NOWARN,
+ *     L1TF_MITIGATION_FLUSH,
+ *     L1TF_MITIGATION_FLUSH_NOSMT,
+ *     L1TF_MITIGATION_FULL,
+ *     L1TF_MITIGATION_FULL_FORCE
+ * };
+ */
 /* Default mitigation for L1TF-affected CPUs */
 enum l1tf_mitigations l1tf_mitigation __ro_after_init =
 	IS_ENABLED(CONFIG_MITIGATION_L1TF) ? L1TF_MITIGATION_FLUSH : L1TF_MITIGATION_OFF;
 #if IS_ENABLED(CONFIG_KVM_INTEL)
 EXPORT_SYMBOL_GPL(l1tf_mitigation);
 #endif
+/*
+ * 在以下使用l1tf_vmx_mitigation:
+ *   - arch/x86/kernel/cpu/bugs.c|2707| <<global>> enum vmx_l1d_flush_state l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+ *   - arch/x86/kernel/cpu/bugs.c|3039| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO)
+ *   - arch/x86/kernel/cpu/bugs.c|3042| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_EPT_DISABLED ||
+ *   - arch/x86/kernel/cpu/bugs.c|3043| <<l1tf_show_state>> (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER &&
+ *   - arch/x86/kernel/cpu/bugs.c|3046| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation]);
+ *   - arch/x86/kernel/cpu/bugs.c|3050| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation],
+ *   - arch/x86/kvm/vmx/vmx.c|297| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+ *   - arch/x86/kvm/vmx/vmx.c|302| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_EPT_DISABLED;
+ *   - arch/x86/kvm/vmx/vmx.c|307| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+ *   - arch/x86/kvm/vmx/vmx.c|353| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = l1tf;
+ *   - arch/x86/kvm/vmx/vmx.c|414| <<vmentry_l1d_flush_set>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO) {
+ *   - arch/x86/kvm/vmx/vmx.c|427| <<vmentry_l1d_flush_get>> if (WARN_ON_ONCE(l1tf_vmx_mitigation >= ARRAY_SIZE(vmentry_l1d_param)))
+ *   - arch/x86/kvm/vmx/vmx.c|430| <<vmentry_l1d_flush_get>> return sysfs_emit(s, "%s\n", vmentry_l1d_param[l1tf_vmx_mitigation].option);
+ *   - arch/x86/kvm/vmx/vmx.c|7759| <<vmx_vm_init>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER)
+ *   - arch/x86/kvm/vmx/vmx.c|8690| <<vmx_cleanup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+ *   - arch/x86/kvm/x86.c|1664| <<kvm_get_arch_capabilities>> if (l1tf_vmx_mitigation != VMENTER_L1D_FLUSH_NEVER)
+ *
+ * enum vmx_l1d_flush_state {
+ *     VMENTER_L1D_FLUSH_AUTO,
+ *     VMENTER_L1D_FLUSH_NEVER,
+ *     VMENTER_L1D_FLUSH_COND,
+ *     VMENTER_L1D_FLUSH_ALWAYS,
+ *     VMENTER_L1D_FLUSH_EPT_DISABLED,
+ *     VMENTER_L1D_FLUSH_NOT_REQUIRED,
+ * };
+ */
 enum vmx_l1d_flush_state l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
 EXPORT_SYMBOL_GPL(l1tf_vmx_mitigation);
 
@@ -2414,6 +3406,10 @@ EXPORT_SYMBOL_GPL(l1tf_vmx_mitigation);
  * instead of the reported physical bits and adjust them on the affected
  * machines to 44bit if the reported bits are less than 44.
  */
+/*
+ * 只在以下调用override_cache_bits():
+ *   - arch/x86/kernel/cpu/bugs.c|2796| <<l1tf_select_mitigation>> override_cache_bits(&boot_cpu_data);
+ */
 static void override_cache_bits(struct cpuinfo_x86 *c)
 {
 	if (c->x86 != 6)
@@ -2439,10 +3435,30 @@ static void override_cache_bits(struct cpuinfo_x86 *c)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|585| <<cpu_select_mitigations>> l1tf_select_mitigation();
+ */
 static void __init l1tf_select_mitigation(void)
 {
 	u64 half_pa;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_L1TF))
 		return;
 
@@ -2460,6 +3476,16 @@ static void __init l1tf_select_mitigation(void)
 		break;
 	case L1TF_MITIGATION_FLUSH_NOSMT:
 	case L1TF_MITIGATION_FULL:
+		/*
+		 * 在以下调用cpu_smt_disable():
+		 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+		 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+		 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+		 */
 		cpu_smt_disable(false);
 		break;
 	case L1TF_MITIGATION_FULL_FORCE:
@@ -2483,6 +3509,11 @@ static void __init l1tf_select_mitigation(void)
 		return;
 	}
 
+	/*
+	 * 在以下使用X86_FEATURE_L1TF_PTEINV:
+	 *   - arch/x86/kernel/cpu/bugs.c|3400| <<l1tf_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_L1TF_PTEINV);
+	 *   - arch/x86/kernel/cpu/bugs.c|3916| <<cpu_show_common>> if (boot_cpu_has(X86_FEATURE_L1TF_PTEINV))
+	 */
 	setup_force_cpu_cap(X86_FEATURE_L1TF_PTEINV);
 }
 
@@ -2930,6 +3961,37 @@ static ssize_t gds_show_state(char *buf)
 	return sysfs_emit(buf, "%s\n", gds_strings[gds_mitigation]);
 }
 
+/*
+ * # cat /sys/devices/system/cpu/vulnerabilities/spectre_v1 
+ * Mitigation: usercopy/swapgs barriers and __user pointer sanitization
+ *
+ * [0] cpu_show_common
+ * [0] cpu_show_spectre_v1
+ * [0] dev_attr_show
+ * [0] sysfs_kf_seq_show
+ * [0] seq_read_iter
+ * [0] vfs_read
+ * [0] ksys_read
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下调用cpu_show_common():
+ *   - arch/x86/kernel/cpu/bugs.c|3396| <<cpu_show_meltdown>> return cpu_show_common(dev, attr, buf, X86_BUG_CPU_MELTDOWN);
+ *   - arch/x86/kernel/cpu/bugs.c|3401| <<cpu_show_spectre_v1>> return cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V1);
+ *   - arch/x86/kernel/cpu/bugs.c|3406| <<cpu_show_spectre_v2>> return cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V2);
+ *   - arch/x86/kernel/cpu/bugs.c|3411| <<cpu_show_spec_store_bypass>> return cpu_show_common(dev, attr, buf, X86_BUG_SPEC_STORE_BYPASS);
+ *   - arch/x86/kernel/cpu/bugs.c|3416| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+ *   - arch/x86/kernel/cpu/bugs.c|3421| <<cpu_show_mds>> return cpu_show_common(dev, attr, buf, X86_BUG_MDS);
+ *   - arch/x86/kernel/cpu/bugs.c|3426| <<cpu_show_tsx_async_abort>> return cpu_show_common(dev, attr, buf, X86_BUG_TAA);
+ *   - arch/x86/kernel/cpu/bugs.c|3431| <<cpu_show_itlb_multihit>> return cpu_show_common(dev, attr, buf, X86_BUG_ITLB_MULTIHIT);
+ *   - arch/x86/kernel/cpu/bugs.c|3436| <<cpu_show_srbds>> return cpu_show_common(dev, attr, buf, X86_BUG_SRBDS);
+ *   - arch/x86/kernel/cpu/bugs.c|3442| <<cpu_show_mmio_stale_data>> return cpu_show_common(dev, attr, buf, X86_BUG_MMIO_UNKNOWN);
+ *   - arch/x86/kernel/cpu/bugs.c|3444| <<cpu_show_mmio_stale_data>> return cpu_show_common(dev, attr, buf, X86_BUG_MMIO_STALE_DATA);
+ *   - arch/x86/kernel/cpu/bugs.c|3449| <<cpu_show_retbleed>> return cpu_show_common(dev, attr, buf, X86_BUG_RETBLEED);
+ *   - arch/x86/kernel/cpu/bugs.c|3454| <<cpu_show_spec_rstack_overflow>> return cpu_show_common(dev, attr, buf, X86_BUG_SRSO);
+ *   - arch/x86/kernel/cpu/bugs.c|3459| <<cpu_show_gds>> return cpu_show_common(dev, attr, buf, X86_BUG_GDS);
+ *   - arch/x86/kernel/cpu/bugs.c|3464| <<cpu_show_reg_file_data_sampling>> return cpu_show_common(dev, attr, buf, X86_BUG_RFDS);
+ */
 static ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
 			       char *buf, unsigned int bug)
 {
@@ -2947,6 +4009,18 @@ static ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr
 		break;
 
 	case X86_BUG_SPECTRE_V1:
+		/*
+		 * 在以下使用X86_BUG_SPECTRE_V1:
+		 *   - arch/x86/include/asm/cpufeatures.h|513| <<global>> #define X86_BUG_SPECTRE_V1 X86_BUG(15)
+		 *   - arch/x86/kernel/cpu/bugs.c|1173| <<spectre_v1_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1) || cpu_mitigations_off()) {
+		 *   - arch/x86/kernel/cpu/bugs.c|3345| <<cpu_show_common>> case X86_BUG_SPECTRE_V1:
+		 *   - arch/x86/kernel/cpu/bugs.c|3401| <<cpu_show_spectre_v1>> return cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V1);
+		 *   - arch/x86/kernel/cpu/common.c|1332| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
+		 *
+		 * 在以下使用spectre_v1_strings[]:
+		 *   - arch/x86/kernel/cpu/bugs.c|1222| <<spectre_v1_select_mitigation>> pr_info("%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+		 *   - arch/x86/kernel/cpu/bugs.c|3380| <<cpu_show_common>> return sysfs_emit(buf, "%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+		 */
 		return sysfs_emit(buf, "%s\n", spectre_v1_strings[spectre_v1_mitigation]);
 
 	case X86_BUG_SPECTRE_V2:
@@ -3000,8 +4074,33 @@ ssize_t cpu_show_meltdown(struct device *dev, struct device_attribute *attr, cha
 	return cpu_show_common(dev, attr, buf, X86_BUG_CPU_MELTDOWN);
 }
 
+/*
+ * # cat /sys/devices/system/cpu/vulnerabilities/spectre_v1 
+ * Mitigation: usercopy/swapgs barriers and __user pointer sanitization
+ *
+ * [0] cpu_show_common
+ * [0] cpu_show_spectre_v1
+ * [0] dev_attr_show
+ * [0] sysfs_kf_seq_show
+ * [0] seq_read_iter
+ * [0] vfs_read
+ * [0] ksys_read
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用cpu_show_spectre_v1():
+ *   - drivers/base/cpu.c|605| <<global>> static DEVICE_ATTR(spectre_v1, 0444, cpu_show_spectre_v1, NULL);
+ */
 ssize_t cpu_show_spectre_v1(struct device *dev, struct device_attribute *attr, char *buf)
 {
+	/*
+	 * 在以下使用X86_BUG_SPECTRE_V1:
+	 *   - arch/x86/include/asm/cpufeatures.h|513| <<global>> #define X86_BUG_SPECTRE_V1 X86_BUG(15)
+	 *   - arch/x86/kernel/cpu/bugs.c|1173| <<spectre_v1_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1) || cpu_mitigations_off()) {
+	 *   - arch/x86/kernel/cpu/bugs.c|3345| <<cpu_show_common>> case X86_BUG_SPECTRE_V1:
+	 *   - arch/x86/kernel/cpu/bugs.c|3401| <<cpu_show_spectre_v1>> return cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V1);
+	 *   - arch/x86/kernel/cpu/common.c|1332| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
+	 */
 	return cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V1);
 }
 
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7cce91b19..b01164f85 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1135,6 +1135,21 @@ static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 #define VULNWL_HYGON(family, whitelist)		\
 	VULNWL(HYGON, family, X86_MODEL_ANY, whitelist)
 
+/*
+ * 在以下使用cpu_vuln_whitelist[]:
+ *   - arch/x86/kernel/cpu/common.c|1325| <<cpu_set_bug_bits>> if (!cpu_matches(cpu_vuln_whitelist, NO_ITLB_MULTIHIT) &&
+ *   - arch/x86/kernel/cpu/common.c|1329| <<cpu_set_bug_bits>> if (cpu_matches(cpu_vuln_whitelist, NO_SPECULATION))
+ *   - arch/x86/kernel/cpu/common.c|1334| <<cpu_set_bug_bits>> if (!cpu_matches(cpu_vuln_whitelist, NO_SPECTRE_V2))
+ *   - arch/x86/kernel/cpu/common.c|1337| <<cpu_set_bug_bits>> if (!cpu_matches(cpu_vuln_whitelist, NO_SSB) &&
+ *   - arch/x86/kernel/cpu/common.c|1353| <<cpu_set_bug_bits>> if (!cpu_matches(cpu_vuln_whitelist, NO_EIBRS_PBRSB) &&
+ *   - arch/x86/kernel/cpu/common.c|1358| <<cpu_set_bug_bits>> if (!cpu_matches(cpu_vuln_whitelist, NO_MDS) &&
+ *   - arch/x86/kernel/cpu/common.c|1361| <<cpu_set_bug_bits>> if (cpu_matches(cpu_vuln_whitelist, MSBDS_ONLY))
+ *   - arch/x86/kernel/cpu/common.c|1365| <<cpu_set_bug_bits>> if (!cpu_matches(cpu_vuln_whitelist, NO_SWAPGS))
+ *   - arch/x86/kernel/cpu/common.c|1409| <<cpu_set_bug_bits>> else if (!cpu_matches(cpu_vuln_whitelist, NO_MMIO))
+ *   - arch/x86/kernel/cpu/common.c|1441| <<cpu_set_bug_bits>> !cpu_matches(cpu_vuln_whitelist, NO_BHI) &&
+ *   - arch/x86/kernel/cpu/common.c|1449| <<cpu_set_bug_bits>> if (cpu_matches(cpu_vuln_whitelist, NO_MELTDOWN))
+ *   - arch/x86/kernel/cpu/common.c|1458| <<cpu_set_bug_bits>> if (cpu_matches(cpu_vuln_whitelist, NO_L1TF))
+ */
 static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 	VULNWL(ANY,	4, X86_MODEL_ANY,	NO_SPECULATION),
 	VULNWL(CENTAUR,	5, X86_MODEL_ANY,	NO_SPECULATION),
@@ -1227,6 +1242,16 @@ static const __initconst struct x86_cpu_id cpu_vuln_whitelist[] = {
 /* CPU is affected by Register File Data Sampling */
 #define RFDS		BIT(7)
 
+/*
+ * 在以下使用cpu_vuln_blacklist[]:
+ *   - arch/x86/kernel/cpu/common.c|1317| <<vulnerable_to_rfds>> return cpu_matches(cpu_vuln_blacklist, RFDS);
+ *   - arch/x86/kernel/cpu/common.c|1393| <<cpu_set_bug_bits>> cpu_matches(cpu_vuln_blacklist, SRBDS | MMIO_SBDS))
+ *   - arch/x86/kernel/cpu/common.c|1407| <<cpu_set_bug_bits>> if (cpu_matches(cpu_vuln_blacklist, MMIO))
+ *   - arch/x86/kernel/cpu/common.c|1414| <<cpu_set_bug_bits>> if (cpu_matches(cpu_vuln_blacklist, RETBLEED) || (x86_arch_cap_msr & ARCH_CAP_RSBA))
+ *   - arch/x86/kernel/cpu/common.c|1418| <<cpu_set_bug_bits>> if (cpu_matches(cpu_vuln_blacklist, SMT_RSB))
+ *   - arch/x86/kernel/cpu/common.c|1422| <<cpu_set_bug_bits>> if (cpu_matches(cpu_vuln_blacklist, SRSO))
+ *   - arch/x86/kernel/cpu/common.c|1432| <<cpu_set_bug_bits>> if (cpu_matches(cpu_vuln_blacklist, GDS) && !(x86_arch_cap_msr & ARCH_CAP_GDS_NO) &&
+ */
 static const struct x86_cpu_id cpu_vuln_blacklist[] __initconst = {
 	VULNBL_INTEL_STEPS(INTEL_IVYBRIDGE,	     X86_STEP_MAX,	SRBDS),
 	VULNBL_INTEL_STEPS(INTEL_HASWELL,	     X86_STEP_MAX,	SRBDS),
@@ -1282,6 +1307,13 @@ static bool __init cpu_matches(const struct x86_cpu_id *table, unsigned long whi
 	return m && !!(m->driver_data & which);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/apic.c|1698| <<x2apic_hw_locked>> x86_arch_cap_msr = x86_read_arch_cap_msr();
+ *   - arch/x86/kernel/cpu/bugs.c|406| <<cpu_select_mitigations>> x86_arch_cap_msr = x86_read_arch_cap_msr();
+ *   - arch/x86/kernel/cpu/common.c|1360| <<cpu_set_bug_bits>> u64 x86_arch_cap_msr = x86_read_arch_cap_msr();
+ *   - arch/x86/kernel/cpu/tsx.c|189| <<tsx_init>> if (x86_read_arch_cap_msr() & ARCH_CAP_TSX_CTRL_MSR) {
+ */
 u64 x86_read_arch_cap_msr(void)
 {
 	u64 x86_arch_cap_msr = 0;
@@ -1317,6 +1349,19 @@ static bool __init vulnerable_to_rfds(u64 x86_arch_cap_msr)
 	return cpu_matches(cpu_vuln_blacklist, RFDS);
 }
 
+/*
+ * [0] cpu_set_bug_bits()
+ * [0] early_identify_cpu
+ * [0] early_cpu_init
+ * [0] setup_arch
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ *
+ * called by:
+ *   - arch/x86/kernel/cpu/common.c|1640| <<early_identify_cpu>> cpu_set_bug_bits(c);
+ */
 static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 {
 	u64 x86_arch_cap_msr = x86_read_arch_cap_msr();
@@ -1329,6 +1374,14 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (cpu_matches(cpu_vuln_whitelist, NO_SPECULATION))
 		return;
 
+	/*
+	 * 在以下使用X86_BUG_SPECTRE_V1:
+	 *   - arch/x86/include/asm/cpufeatures.h|513| <<global>> #define X86_BUG_SPECTRE_V1 X86_BUG(15)
+	 *   - arch/x86/kernel/cpu/bugs.c|1173| <<spectre_v1_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1) || cpu_mitigations_off()) {
+	 *   - arch/x86/kernel/cpu/bugs.c|3345| <<cpu_show_common>> case X86_BUG_SPECTRE_V1:
+	 *   - arch/x86/kernel/cpu/bugs.c|3401| <<cpu_show_spectre_v1>> return cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V1);
+	 *   - arch/x86/kernel/cpu/common.c|1332| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
+	 */
 	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
 
 	if (!cpu_matches(cpu_vuln_whitelist, NO_SPECTRE_V2))
@@ -1349,6 +1402,49 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if ((x86_arch_cap_msr & ARCH_CAP_IBRS_ALL) ||
 	    (cpu_has(c, X86_FEATURE_AUTOIBRS) &&
 	     !cpu_feature_enabled(X86_FEATURE_SEV_SNP))) {
+		/*
+		 * 在以下使用X86_FEATURE_IBRS_ENHANCED:
+		 *   - arch/x86/kernel/cpu/bugs.c|1980| <<spectre_v2_parse_cmdline>> if (... !boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2229| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2309| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))
+		 *   - arch/x86/kernel/cpu/common.c|1405| <<cpu_set_bug_bits>> setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
+		 *   - arch/x86/kernel/cpu/common.c|1495| <<cpu_set_bug_bits>> if (... (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED) ||
+		 *
+		 * Enhanced IBRS supports an 'always on' model in which IBRS is enabled
+		 * once (by setting IA32_SPEC_CTRL.IBRS) and never disabled. If
+		 * IA32_SPEC_CTRL.IBRS = 1 on a processor with enhanced IBRS, the
+		 * predicted targets of indirect branches executed cannot be controlled
+		 * by software that was executed in a less privileged predictor mode or
+		 * on another logical processor.
+		 *
+		 * As a result, software operating on a processor with enhanced IBRS
+		 * need not use WRMSR to set IA32_SPEC_CTRL.IBRS after every transition
+		 * to a more privileged predictor mode. Software can isolate predictor
+		 * modes effectively simply by setting the bit once. On parts that
+		 * enumerated enhanced IBRS, software need not disable IBRS or STIBP
+		 * prior to entering a sleep state such as MWAIT or HLT.
+		 *
+		 * On processors with enhanced IBRS, an RSB overwrite sequence may not
+		 * suffice to prevent the predicted target of a near return from using
+		 * an RSB entry created in a less privileged predictor mode.  Software
+		 * can prevent this by enabling SMEP (for transitions from user mode to
+		 * supervisor mode) and by having IA32_SPEC_CTRL.IBRS set during VM
+		 * exits. Processors with enhanced IBRS still support the usage model
+		 * where IBRS is set only in the OS/VMM for OSes that enable SMEP. To
+		 * do this, such processors will ensure that guest behavior cannot
+		 * control the RSB after a VM exit once IBRS is set, even if IBRS was
+		 * not set at the time of the VM exit.
+		 *
+		 * If the guest has cleared IBRS, the hypervisor should set IBRS after
+		 * the VM exit, just as it would do on processors supporting IBRS but
+		 * not enhanced IBRS. As with IBRS, enhanced IBRS does not prevent
+		 * software from affecting the predicted target of an indirect branch
+		 * executed at the same predictor mode. For such cases, software should
+		 * use the IBPB command.
+		 *
+		 * 在Intel上是(x86_arch_cap_msr & ARCH_CAP_IBRS_ALL)
+		 * 在AMD上是cpu_has(c, X86_FEATURE_AUTOIBRS)
+		 */
 		setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
 		if (!cpu_matches(cpu_vuln_whitelist, NO_EIBRS_PBRSB) &&
 		    !(x86_arch_cap_msr & ARCH_CAP_PBRSB_NO))
@@ -1436,6 +1532,49 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (vulnerable_to_rfds(x86_arch_cap_msr))
 		setup_force_cpu_bug(X86_BUG_RFDS);
 
+	/*
+	 * 在以下使用X86_FEATURE_IBRS_ENHANCED:
+	 *   - arch/x86/kernel/cpu/bugs.c|1980| <<spectre_v2_parse_cmdline>> if (... !boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2229| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2309| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))
+	 *   - arch/x86/kernel/cpu/common.c|1405| <<cpu_set_bug_bits>> setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
+	 *   - arch/x86/kernel/cpu/common.c|1495| <<cpu_set_bug_bits>> if (... (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED) ||
+	 *
+	 * Enhanced IBRS supports an 'always on' model in which IBRS is enabled
+	 * once (by setting IA32_SPEC_CTRL.IBRS) and never disabled. If
+	 * IA32_SPEC_CTRL.IBRS = 1 on a processor with enhanced IBRS, the
+	 * predicted targets of indirect branches executed cannot be controlled
+	 * by software that was executed in a less privileged predictor mode or
+	 * on another logical processor.
+	 *
+	 * As a result, software operating on a processor with enhanced IBRS
+	 * need not use WRMSR to set IA32_SPEC_CTRL.IBRS after every transition
+	 * to a more privileged predictor mode. Software can isolate predictor
+	 * modes effectively simply by setting the bit once. On parts that
+	 * enumerated enhanced IBRS, software need not disable IBRS or STIBP
+	 * prior to entering a sleep state such as MWAIT or HLT.
+	 *
+	 * On processors with enhanced IBRS, an RSB overwrite sequence may not
+	 * suffice to prevent the predicted target of a near return from using
+	 * an RSB entry created in a less privileged predictor mode.  Software
+	 * can prevent this by enabling SMEP (for transitions from user mode to
+	 * supervisor mode) and by having IA32_SPEC_CTRL.IBRS set during VM
+	 * exits. Processors with enhanced IBRS still support the usage model
+	 * where IBRS is set only in the OS/VMM for OSes that enable SMEP. To
+	 * do this, such processors will ensure that guest behavior cannot
+	 * control the RSB after a VM exit once IBRS is set, even if IBRS was
+	 * not set at the time of the VM exit.
+	 *
+	 * If the guest has cleared IBRS, the hypervisor should set IBRS after
+	 * the VM exit, just as it would do on processors supporting IBRS but
+	 * not enhanced IBRS. As with IBRS, enhanced IBRS does not prevent
+	 * software from affecting the predicted target of an indirect branch
+	 * executed at the same predictor mode. For such cases, software should
+	 * use the IBPB command.
+	 *
+	 * 在Intel上是(x86_arch_cap_msr & ARCH_CAP_IBRS_ALL)
+	 * 在AMD上是cpu_has(c, X86_FEATURE_AUTOIBRS)
+	 */
 	/* When virtualized, eIBRS could be hidden, assume vulnerable */
 	if (!(x86_arch_cap_msr & ARCH_CAP_BHI_NO) &&
 	    !cpu_matches(cpu_vuln_whitelist, NO_BHI) &&
@@ -1458,6 +1597,22 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (cpu_matches(cpu_vuln_whitelist, NO_L1TF))
 		return;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	setup_force_cpu_bug(X86_BUG_L1TF);
 }
 
@@ -1585,6 +1740,10 @@ static void __init cpu_parse_early_param(void)
  * WARNING: this function is only called on the boot CPU.  Don't add code
  * here that is supposed to run on all CPUs.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/common.c|1706| <<early_cpu_init>> early_identify_cpu(&boot_cpu_data);
+ */
 static void __init early_identify_cpu(struct cpuinfo_x86 *c)
 {
 	memset(&c->x86_capability, 0, sizeof(c->x86_capability));
@@ -1666,6 +1825,10 @@ void __init init_cpu_devs(void)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/setup.c|785| <<setup_arch>> early_cpu_init();
+ */
 void __init early_cpu_init(void)
 {
 #ifdef CONFIG_PROCESSOR_SELECT
@@ -2337,6 +2500,12 @@ void microcode_check(struct cpuinfo_x86 *prev_info)
 /*
  * Invoked from core CPU hotplug code after hotplug operations
  */
+/*
+ * 在以下使用arch_smt_update():
+ *   - arch/x86/kernel/cpu/common.c|2532| <<arch_cpu_finalize_init>> arch_smt_update();
+ *   - kernel/cpu.c|1481| <<_cpu_down>> arch_smt_update();
+ *   - kernel/cpu.c|1714| <<_cpu_up>> arch_smt_update();
+ */
 void arch_smt_update(void)
 {
 	/* Handle the speculative execution misfeatures */
diff --git a/arch/x86/kernel/cpu/cpu.h b/arch/x86/kernel/cpu/cpu.h
index 1beccefba..7ffd90dba 100644
--- a/arch/x86/kernel/cpu/cpu.h
+++ b/arch/x86/kernel/cpu/cpu.h
@@ -92,6 +92,13 @@ extern void update_gds_msr(void);
 
 extern enum spectre_v2_mitigation spectre_v2_enabled;
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/amd.c|1073| <<init_amd>> if (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+ *   - arch/x86/kernel/cpu/bugs.c|1779| <<spectre_v2_in_ibrs_mode>> return spectre_v2_in_eibrs_mode(mode) || mode == SPECTRE_V2_IBRS;
+ *   - arch/x86/kernel/cpu/bugs.c|1864| <<spectre_v2_user_select_mitigation>> (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+ *   - arch/x86/kernel/cpu/bugs.c|3409| <<stibp_state>> if (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+ */
 static inline bool spectre_v2_in_eibrs_mode(enum spectre_v2_mitigation mode)
 {
 	return mode == SPECTRE_V2_EIBRS ||
diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c
index feca4f20b..59c6b9838 100644
--- a/arch/x86/kernel/irq.c
+++ b/arch/x86/kernel/irq.c
@@ -469,6 +469,25 @@ DEFINE_IDTENTRY_SYSVEC(sysvec_posted_msi_notification)
 	 */
 	pi_clear_on(pid);
 
+	/*
+	 * 11 // Posted-Interrupt Descriptor
+	 * 12 struct pi_desc {
+	 * 13         union {
+	 * 14                 u32 pir[8];     // Posted interrupt requested
+	 * 15                 u64 pir64[4];
+	 * 16         };
+	 * 17         union {
+	 * 18                 struct {
+	 * 19                         u16     notifications; // Suppress and outstanding bits
+	 * 20                         u8      nv;
+	 * 21                         u8      rsvd_2;
+	 * 22                         u32     ndst;
+	 * 23                 };
+	 * 24                 u64 control;
+	 * 25         };
+	 * 26         u32 rsvd[6];
+	 * 27 } __aligned(64);
+	 */
 	/*
 	 * There could be a race of PI notification and the clearing of ON bit,
 	 * process PIR bits one last time such that handling the new interrupts
diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 6da6769d7..297b14435 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -611,6 +611,12 @@ static __always_inline void amd_set_ssb_virt_state(unsigned long tifn)
  * tifp: Previous task's thread flags
  * tifn: Next task's thread flags
  */
+/*
+ * 在以下调用__speculation_ctrl_update():
+ *   - arch/x86/kernel/process.c|670| <<speculation_ctrl_update>> __speculation_ctrl_update(~tif, tif);
+ *   - arch/x86/kernel/process.c|722| <<__switch_to_xtra>> __speculation_ctrl_update(tifp, tifn);
+ *   - arch/x86/kernel/process.c|728| <<__switch_to_xtra>> __speculation_ctrl_update(~tifn, tifn);
+ */
 static __always_inline void __speculation_ctrl_update(unsigned long tifp,
 						      unsigned long tifn)
 {
@@ -667,6 +673,12 @@ void speculation_ctrl_update(unsigned long tif)
 
 	/* Forced update. Make sure all relevant TIF flags are different */
 	local_irq_save(flags);
+	/*
+	 * 在以下调用__speculation_ctrl_update():
+	 *   - arch/x86/kernel/process.c|670| <<speculation_ctrl_update>> __speculation_ctrl_update(~tif, tif);
+	 *   - arch/x86/kernel/process.c|722| <<__switch_to_xtra>> __speculation_ctrl_update(tifp, tifn);
+	 *   - arch/x86/kernel/process.c|728| <<__switch_to_xtra>> __speculation_ctrl_update(~tifn, tifn);
+	 */
 	__speculation_ctrl_update(~tif, tif);
 	local_irq_restore(flags);
 }
diff --git a/arch/x86/kernel/ptrace.c b/arch/x86/kernel/ptrace.c
index 095f04bda..ed94dbd1f 100644
--- a/arch/x86/kernel/ptrace.c
+++ b/arch/x86/kernel/ptrace.c
@@ -523,6 +523,11 @@ static int ptrace_fill_bp_fields(struct perf_event_attr *attr,
 	return err;
 }
 
+/*
+ * 在以下使用ptrace_register_breakpoint():
+ *   - arch/x86/kernel/ptrace.c|581| <<ptrace_write_dr7>> bp = ptrace_register_breakpoint(tsk,
+ *   - arch/x86/kernel/ptrace.c|649| <<ptrace_set_breakpoint_addr>> bp = ptrace_register_breakpoint(tsk,
+ */
 static struct perf_event *
 ptrace_register_breakpoint(struct task_struct *tsk, int len, int type,
 				unsigned long addr, bool disabled)
@@ -627,6 +632,10 @@ static unsigned long ptrace_get_debugreg(struct task_struct *tsk, int n)
 	return val;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/ptrace.c|677| <<ptrace_set_debugreg>> rc = ptrace_set_breakpoint_addr(tsk, n, val);
+ */
 static int ptrace_set_breakpoint_addr(struct task_struct *tsk, int nr,
 				      unsigned long addr)
 {
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c10850ae6..a1e349627 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1385,6 +1385,16 @@ void __noreturn hlt_play_dead(void)
  */
 void native_play_dead(void)
 {
+	/*
+	 * 在以下使用X86_FEATURE_KERNEL_IBRS:
+	 *   - arch/x86/entry/calling.h|306| <<IBRS_ENTER>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/entry/calling.h|335| <<IBRS_EXIT>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/kernel/cpu/bugs.c|370| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kernel/cpu/bugs.c|2252| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+	 *   - arch/x86/kernel/smpboot.c|1388| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kvm/vmx/vmx.c|7529| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+	 *   - drivers/idle/intel_idle.c|2092| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
 		__update_spec_ctrl(0);
 
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index a009c94c2..de7a698ba 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -662,6 +662,13 @@ static u8 count_vectors(void *bitmap)
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|704| <<kvm_apic_update_irr>>
+ *          bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+ *   - arch/x86/kvm/vmx/nested.c|3941| <<vmx_complete_nested_posted_interrupt>>
+ *          __kvm_apic_update_irr(vmx->nested.pi_desc->pir, vapic_page, &max_irr);
+ */
 bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 {
 	u32 i, vec;
@@ -698,9 +705,20 @@ bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 }
 EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6983| <<vmx_sync_pir_to_irr>> kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
+ */
 bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/lapic.c|704| <<kvm_apic_update_irr>>
+	 *          bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+	 *   - arch/x86/kvm/vmx/nested.c|3941| <<vmx_complete_nested_posted_interrupt>>
+	 *          __kvm_apic_update_irr(vmx->nested.pi_desc->pir, vapic_page, &max_irr);
+	 */
 	bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
 
 	if (unlikely(!apic->apicv_active && irr_updated))
@@ -951,6 +969,20 @@ static bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)
 static int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)
 {
 	int highest_irr;
+	/*
+	 * 在以下使用sync_pir_to_irr:
+	 *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+	 *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+	 *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *
+	 * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+	 */
 	if (kvm_x86_ops.sync_pir_to_irr)
 		highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
 	else
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 816087039..4f37356c7 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -4607,6 +4607,16 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 	 */
 	BUILD_BUG_ON(lower_32_bits(PFERR_SYNTHETIC_MASK));
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	vcpu->arch.l1tf_flush_l1d = true;
 	if (!flags) {
 		trace_kvm_page_fault(vcpu, fault_address, error_code);
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 22551e2f1..0e4952cdc 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -495,6 +495,22 @@ void kvm_mmu_reset_all_pte_masks(void)
 	 */
 	shadow_nonpresent_or_rsvd_mask = 0;
 	low_phys_bits = boot_cpu_data.x86_phys_bits;
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (boot_cpu_has_bug(X86_BUG_L1TF) &&
 	    !WARN_ON_ONCE(boot_cpu_data.x86_cache_bits >=
 			  52 - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)) {
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 75e9cfc68..7e0818730 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -23,6 +23,94 @@
 #include "lapic.h"
 #include "pmu.h"
 
+/*
+ * intel_pmu_enable_event
+ * x86_pmu_start
+ * x86_pmu_enable
+ * __perf_event_task_sched_in
+ * finish_task_switch.isra.0
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ * 
+ * intel_pmu_enable_event
+ * x86_pmu_start
+ * x86_pmu_enable
+ * event_function
+ * remote_function
+ * generic_exec_single
+ * smp_call_function_single
+ * event_function_call
+ * perf_event_enable
+ * pmc_resume_counter
+ * reprogram_gp_counter
+ * intel_pmu_set_msr
+ * vmx_set_msr
+ * __kvm_set_msr
+ * kvm_emulate_wrmsr
+ * vmx_handle_exit
+ * vcpu_enter_guest
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * intel_pmu_disable_event
+ * x86_pmu_stop
+ * x86_pmu_del
+ * event_sched_out.part.0
+ * group_sched_out.part.0
+ * ctx_sched_out
+ * perf_event_context_sched_out
+ * __perf_event_task_sched_out
+ * prepare_task_switch
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * intel_pmu_disable_event
+ * x86_pmu_stop
+ * x86_pmu_del
+ * event_sched_out.part.0
+ * group_sched_out.part.0
+ * ctx_sched_out
+ * ctx_resched
+ * event_function
+ * remote_function
+ * generic_exec_single
+ * smp_call_function_single
+ * event_function_call
+ * perf_event_enable
+ * pmc_resume_counter
+ * reprogram_gp_counter
+ * intel_pmu_set_msr
+ * vmx_set_msr
+ * __kvm_set_msr
+ * kvm_emulate_wrmsr
+ * vmx_handle_exit
+ * vcpu_enter_guest
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
+
 /* This is enough to filter the vast majority of currently defined events. */
 #define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300
 
@@ -96,11 +184,32 @@ void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 #undef __KVM_X86_PMU_OP
 }
 
+/*
+ * 在以下调用__kvm_perf_overflow():
+ *   - arch/x86/kvm/pmu.c|146| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|535| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
 	bool skip_pmi = false;
 
+	/*
+	 * 在以下使用kvm_pmu->global_status:
+	 *   - arch/x86/kvm/pmu.c|116| <<__kvm_perf_overflow>> skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT,
+	 *              (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/pmu.c|119| <<__kvm_perf_overflow>> __set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/pmu.c|717| <<kvm_pmu_get_msr(MSR_CORE_PERF_GLOBAL_STATUS/)>> msr_info->data = pmu->global_status;
+	 *   - arch/x86/kvm/pmu.c|758| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_STATUS/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>>
+	 *              pmu->global_status = data;
+	 *   - arch/x86/kvm/pmu.c|783| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR)>>
+	 *              pmu->global_status &= ~data;
+	 *   - arch/x86/kvm/pmu.c|812| <<kvm_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|8591| <<vmx_handle_intel_pt_intr>> __set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
+	 *              (unsigned long *)&vcpu->arch.pmu.global_status);
+	 * 
+	 * 一般pebs用precise_ip
+	 */
 	if (pmc->perf_event && pmc->perf_event->attr.precise_ip) {
 		if (!in_pmi) {
 			/*
@@ -119,10 +228,25 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		__set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
 	}
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|144| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8589| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10916| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11294| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * 处理函数是kvm_pmu_deliver_pmi().
+	 */
 	if (pmc->intr && !skip_pmi)
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
 }
 
+/*
+ * 在以下使用kvm_perf_overflow():
+ *   - arch/x86/kvm/pmu.c|228| <<pmc_reprogram_counter>>
+ *      event = perf_event_create_kernel_counter(&attr, -1, current,
+ *                   kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -134,11 +258,41 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 	 * to be reprogrammed, e.g. if a PMI for the previous event races with
 	 * KVM's handling of a related guest WRMSR.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|164| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|604| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|622| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|874| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1189| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|612| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1190| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(... sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1193| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
 		return;
 
+	/*
+	 * 在以下调用__kvm_perf_overflow():
+	 *   - arch/x86/kvm/pmu.c|146| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+	 *   - arch/x86/kvm/pmu.c|535| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+	 */
 	__kvm_perf_overflow(pmc, true);
 
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|1195| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|5033| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *   - arch/x86/kvm/x86.c|10914| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *
+	 * 处理函数是kvm_pmu_handle_event().
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
@@ -164,6 +318,12 @@ static u64 pmc_get_pebs_precise_level(struct kvm_pmc *pmc)
 	return 1;
 }
 
+/*
+ * 在以下调用get_sample_period():
+ *   - arch/x86/kvm/pmu.c|194| <<pmc_reprogram_counter>> attr.sample_period = get_sample_period(pmc, pmc->counter);
+ *   - arch/x86/kvm/pmu.c|264| <<pmc_resume_counter>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+ *   - arch/x86/kvm/pmu.c|303| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter));
+ */
 static u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 {
 	u64 sample_period = (-counter_value) & pmc_bitmask(pmc);
@@ -173,6 +333,14 @@ static u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 	return sample_period;
 }
 
+/*
+ * 在以下使用pmc_reprogram_counter():
+ *   - arch/x86/kvm/pmu.c|609| <<reprogram_counter>> return pmc_reprogram_counter(pmc,
+ *          PERF_TYPE_RAW, (eventsel & pmu->raw_event_mask),
+ *          !(eventsel & ARCH_PERFMON_EVENTSEL_USR),
+ *          !(eventsel & ARCH_PERFMON_EVENTSEL_OS),
+ *          eventsel & ARCH_PERFMON_EVENTSEL_INT);
+ */
 static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 				 bool exclude_user, bool exclude_kernel,
 				 bool intr)
@@ -191,6 +359,12 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 	};
 	bool pebs = test_bit(pmc->idx, (unsigned long *)&pmu->pebs_enable);
 
+	/*
+	 * 在以下调用get_sample_period():
+	 *   - arch/x86/kvm/pmu.c|194| <<pmc_reprogram_counter>> attr.sample_period = get_sample_period(pmc, pmc->counter);
+	 *   - arch/x86/kvm/pmu.c|264| <<pmc_resume_counter>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+	 *   - arch/x86/kvm/pmu.c|303| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter));
+	 */
 	attr.sample_period = get_sample_period(pmc, pmc->counter);
 
 	if ((attr.config & HSW_IN_TX_CHECKPOINTED) &&
@@ -212,6 +386,31 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 		attr.precise_ip = pmc_get_pebs_precise_level(pmc);
 	}
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|741| <<kvm_pmu_create_perf_event>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|328| <<kvm_pmu_create_perf_event>>
+	 *      event = perf_event_create_kernel_counter(attr, -1, current, kvm_riscv_pmu_overflow, pmc);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|969| <<measure_residency_fn>>
+	 *      miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|974| <<measure_residency_fn>>
+	 *      hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kvm/pmu.c|227| <<pmc_reprogram_counter>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|243| <<intel_pmu_create_guest_lbr_event>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+	 *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>>
+	 *      return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+	 *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>>
+	 *      bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+	 *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>>
+	 *      return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+	 *   - kernel/watchdog_perf.c|135| <<hardlockup_detector_event_create>>
+	 *      evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 *   - kernel/watchdog_perf.c|140| <<hardlockup_detector_event_create>>
+	 *      evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 */
 	event = perf_event_create_kernel_counter(&attr, -1, current,
 						 kvm_perf_overflow, pmc);
 	if (IS_ERR(event)) {
@@ -258,6 +457,20 @@ static bool pmc_resume_counter(struct kvm_pmc *pmc)
 	if (!pmc->perf_event)
 		return false;
 
+	/*
+	 * 在以下调用perf_event_period():
+	 *   - arch/riscv/kvm/vcpu_pmu.c|542| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_period(pmc->perf_event,
+	 *                kvm_pmu_get_sample_period(pmc));
+	 *   - arch/x86/kvm/pmu.c|263| <<pmc_resume_counter>> if (is_sampling_event(pmc->perf_event) &&
+	 *                perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+	 *   - arch/x86/kvm/pmu.c|302| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event,
+	 *                get_sample_period(pmc, pmc->counter));
+	 *
+	 * 在以下调用get_sample_period():
+	 *   - arch/x86/kvm/pmu.c|194| <<pmc_reprogram_counter>> attr.sample_period = get_sample_period(pmc, pmc->counter);
+	 *   - arch/x86/kvm/pmu.c|264| <<pmc_resume_counter>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+	 *   - arch/x86/kvm/pmu.c|303| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter));
+	 */
 	/* recalibrate sample period and check if it's accepted by perf core */
 	if (is_sampling_event(pmc->perf_event) &&
 	    perf_event_period(pmc->perf_event,
@@ -275,11 +488,42 @@ static bool pmc_resume_counter(struct kvm_pmc *pmc)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|394| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+ *   - arch/x86/kvm/pmu.c|605| <<reprogram_counter>> pmc_release_perf_event(pmc);
+ */
 static void pmc_release_perf_event(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
+		/*
+		 * 在以下使用perf_event_release_kernel():
+		 *   - arch/arm64/kvm/pmu-emul.c|208| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/riscv/kvm/vcpu_pmu.c|82| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1058| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1060| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+		 *   - arch/x86/kvm/pmu.c|383| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|198| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+		 *   - kernel/events/core.c|5589| <<perf_release>> perf_event_release_kernel(file->private_data);
+		 *   - kernel/events/hw_breakpoint.c|829| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+		 *   - kernel/watchdog_perf.c|233| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+		 *   - kernel/watchdog_perf.c|300| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+		 */
 		perf_event_release_kernel(pmc->perf_event);
 		pmc->perf_event = NULL;
+		/*
+		 * 在以下使用kvm_pmc->current_config:
+		 *   - arch/x86/kvm/pmu.c|385| <<pmc_release_perf_event>> pmc->current_config = 0;
+		 *   - arch/x86/kvm/pmu.c|602| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+		 *   - arch/x86/kvm/pmu.c|607| <<reprogram_counter>> pmc->current_config = new_config;
+		 *   - arch/x86/kvm/svm/pmu.c|226| <<amd_pmu_init>> pmu->gp_counters[i].current_config = 0;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_init>> pmu->gp_counters[i].current_config = 0;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|609| <<intel_pmu_init>> pmu->fixed_counters[i].current_config = 0;
+		 *
+		 * only for creating or reusing perf_event,
+		 * eventsel value for general purpose counters,
+		 * ctrl value for fixed counters.
+		 */
 		pmc->current_config = 0;
 		pmc_to_pmu(pmc)->event_count--;
 	}
@@ -299,6 +543,20 @@ static void pmc_update_sample_period(struct kvm_pmc *pmc)
 	    !is_sampling_event(pmc->perf_event))
 		return;
 
+	/*
+	 * 在以下调用perf_event_period():
+	 *   - arch/riscv/kvm/vcpu_pmu.c|542| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_period(pmc->perf_event,
+	 *                kvm_pmu_get_sample_period(pmc));
+	 *   - arch/x86/kvm/pmu.c|263| <<pmc_resume_counter>> if (is_sampling_event(pmc->perf_event) &&
+	 *                perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+	 *   - arch/x86/kvm/pmu.c|302| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event,
+	 *                get_sample_period(pmc, pmc->counter));
+	 *
+	 * 在以下调用get_sample_period():
+	 *   - arch/x86/kvm/pmu.c|194| <<pmc_reprogram_counter>> attr.sample_period = get_sample_period(pmc, pmc->counter);
+	 *   - arch/x86/kvm/pmu.c|264| <<pmc_resume_counter>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+	 *   - arch/x86/kvm/pmu.c|303| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter));
+	 */
 	perf_event_period(pmc->perf_event,
 			  get_sample_period(pmc, pmc->counter));
 }
@@ -460,6 +718,11 @@ static int reprogram_counter(struct kvm_pmc *pmc)
 	if (!pmc_event_is_allowed(pmc))
 		return 0;
 
+	/*
+	 * 在以下调用__kvm_perf_overflow():
+	 *   - arch/x86/kvm/pmu.c|146| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+	 *   - arch/x86/kvm/pmu.c|535| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+	 */
 	if (emulate_overflow)
 		__kvm_perf_overflow(pmc, false);
 
@@ -478,6 +741,19 @@ static int reprogram_counter(struct kvm_pmc *pmc)
 		new_config = (u64)fixed_ctr_ctrl;
 	}
 
+	/*
+	 * 在以下使用kvm_pmc->current_config:
+	 *   - arch/x86/kvm/pmu.c|385| <<pmc_release_perf_event>> pmc->current_config = 0;
+	 *   - arch/x86/kvm/pmu.c|602| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+	 *   - arch/x86/kvm/pmu.c|607| <<reprogram_counter>> pmc->current_config = new_config;
+	 *   - arch/x86/kvm/svm/pmu.c|226| <<amd_pmu_init>> pmu->gp_counters[i].current_config = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_init>> pmu->gp_counters[i].current_config = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|609| <<intel_pmu_init>> pmu->fixed_counters[i].current_config = 0;
+	 *
+	 * only for creating or reusing perf_event,
+	 * eventsel value for general purpose counters,
+	 * ctrl value for fixed counters.
+	 */
 	if (pmc->current_config == new_config && pmc_resume_counter(pmc))
 		return 0;
 
@@ -485,6 +761,9 @@ static int reprogram_counter(struct kvm_pmc *pmc)
 
 	pmc->current_config = new_config;
 
+	/*
+	 * 只在此处调用
+	 */
 	return pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
 				     (eventsel & pmu->raw_event_mask),
 				     !(eventsel & ARCH_PERFMON_EVENTSEL_USR),
@@ -492,6 +771,19 @@ static int reprogram_counter(struct kvm_pmc *pmc)
 				     eventsel & ARCH_PERFMON_EVENTSEL_INT);
 }
 
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|1195| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|5033| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *   - arch/x86/kvm/x86.c|10914| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *
+ * * 处理函数是kvm_pmu_handle_event().
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|10935| <<vcpu_enter_guest(KVM_REQ_PMU)>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	DECLARE_BITMAP(bitmap, X86_PMC_IDX_MAX);
@@ -499,6 +791,21 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 	struct kvm_pmc *pmc;
 	int bit;
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|164| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|604| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|622| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|874| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1189| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|612| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1190| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(... sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1193| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
 
 	/*
@@ -643,6 +950,20 @@ int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	switch (msr) {
 	case MSR_CORE_PERF_GLOBAL_STATUS:
 	case MSR_AMD64_PERF_CNTR_GLOBAL_STATUS:
+		/*
+		 * 在以下使用kvm_pmu->global_status:
+		 *   - arch/x86/kvm/pmu.c|116| <<__kvm_perf_overflow>> skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT,
+		 *              (unsigned long *)&pmu->global_status);
+		 *   - arch/x86/kvm/pmu.c|119| <<__kvm_perf_overflow>> __set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
+		 *   - arch/x86/kvm/pmu.c|717| <<kvm_pmu_get_msr(MSR_CORE_PERF_GLOBAL_STATUS/)>> msr_info->data = pmu->global_status;
+		 *   - arch/x86/kvm/pmu.c|758| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_STATUS/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>>
+		 *              pmu->global_status = data;
+		 *   - arch/x86/kvm/pmu.c|783| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR)>>
+		 *              pmu->global_status &= ~data;
+		 *   - arch/x86/kvm/pmu.c|812| <<kvm_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+		 *   - arch/x86/kvm/vmx/vmx.c|8591| <<vmx_handle_intel_pt_intr>> __set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
+		 *              (unsigned long *)&vcpu->arch.pmu.global_status);
+		 */
 		msr_info->data = pmu->global_status;
 		break;
 	case MSR_AMD64_PERF_CNTR_GLOBAL_CTL:
@@ -684,6 +1005,20 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		if (data & pmu->global_status_rsvd)
 			return 1;
 
+		/*
+		 * 在以下使用kvm_pmu->global_status:
+		 *   - arch/x86/kvm/pmu.c|116| <<__kvm_perf_overflow>> skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT,
+		 *              (unsigned long *)&pmu->global_status);
+		 *   - arch/x86/kvm/pmu.c|119| <<__kvm_perf_overflow>> __set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
+		 *   - arch/x86/kvm/pmu.c|717| <<kvm_pmu_get_msr(MSR_CORE_PERF_GLOBAL_STATUS/)>> msr_info->data = pmu->global_status;
+		 *   - arch/x86/kvm/pmu.c|758| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_STATUS/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>>
+		 *              pmu->global_status = data;
+		 *   - arch/x86/kvm/pmu.c|783| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR)>>
+		 *              pmu->global_status &= ~data;
+		 *   - arch/x86/kvm/pmu.c|812| <<kvm_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+		 *   - arch/x86/kvm/vmx/vmx.c|8591| <<vmx_handle_intel_pt_intr>> __set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
+		 *              (unsigned long *)&vcpu->arch.pmu.global_status);
+		 */
 		pmu->global_status = data;
 		break;
 	case MSR_AMD64_PERF_CNTR_GLOBAL_CTL:
@@ -708,6 +1043,20 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 			return 1;
 		fallthrough;
 	case MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR:
+		/*
+		 * 在以下使用kvm_pmu->global_status:
+		 *   - arch/x86/kvm/pmu.c|116| <<__kvm_perf_overflow>> skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT,
+		 *              (unsigned long *)&pmu->global_status);
+		 *   - arch/x86/kvm/pmu.c|119| <<__kvm_perf_overflow>> __set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
+		 *   - arch/x86/kvm/pmu.c|717| <<kvm_pmu_get_msr(MSR_CORE_PERF_GLOBAL_STATUS/)>> msr_info->data = pmu->global_status;
+		 *   - arch/x86/kvm/pmu.c|758| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_STATUS/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>>
+		 *              pmu->global_status = data;
+		 *   - arch/x86/kvm/pmu.c|783| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR)>>
+		 *              pmu->global_status &= ~data;
+		 *   - arch/x86/kvm/pmu.c|812| <<kvm_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+		 *   - arch/x86/kvm/vmx/vmx.c|8591| <<vmx_handle_intel_pt_intr>> __set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
+		 *              (unsigned long *)&vcpu->arch.pmu.global_status);
+		 */
 		if (!msr_info->host_initiated)
 			pmu->global_status &= ~data;
 		break;
@@ -727,6 +1076,21 @@ static void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 
 	pmu->need_cleanup = false;
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|164| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|604| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|622| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|874| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1189| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|612| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1190| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(... sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1193| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
 
 	kvm_for_each_pmc(pmu, pmc, i, pmu->all_valid_pmc_idx) {
@@ -738,6 +1102,20 @@ static void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 			pmc->eventsel = 0;
 	}
 
+	/*
+	 * 在以下使用kvm_pmu->global_status:
+	 *   - arch/x86/kvm/pmu.c|116| <<__kvm_perf_overflow>> skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT,
+	 *              (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/pmu.c|119| <<__kvm_perf_overflow>> __set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/pmu.c|717| <<kvm_pmu_get_msr(MSR_CORE_PERF_GLOBAL_STATUS/)>> msr_info->data = pmu->global_status;
+	 *   - arch/x86/kvm/pmu.c|758| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_STATUS/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>>
+	 *              pmu->global_status = data;
+	 *   - arch/x86/kvm/pmu.c|783| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR)>>
+	 *              pmu->global_status &= ~data;
+	 *   - arch/x86/kvm/pmu.c|812| <<kvm_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|8591| <<vmx_handle_intel_pt_intr>> __set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
+	 *              (unsigned long *)&vcpu->arch.pmu.global_status);
+	 */
 	pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
 
 	kvm_pmu_call(reset)(vcpu);
@@ -1031,9 +1409,34 @@ int kvm_vm_ioctl_set_pmu_event_filter(struct kvm *kvm, void __user *argp)
 	BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
 		     sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|164| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|604| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|622| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|874| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1189| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|612| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1190| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(... sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1193| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
 
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|1195| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|5033| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *   - arch/x86/kvm/x86.c|10914| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *
+	 * 处理函数是kvm_pmu_handle_event().
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
 
 	r = 0;
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index ad89d0bd6..49a44606f 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -102,6 +102,15 @@ static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 	return pmu->counter_bitmask[pmc->type];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|535| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/pmu.c|575| <<pmc_write_counter>> pmc->counter += val - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/pmu.c|910| <<kvm_pmu_rdpmc>> *data = pmc_read_counter(pmc) & mask;
+ *   - arch/x86/kvm/svm/pmu.c|136| <<amd_pmu_get_msr>> msr_info->data = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|358| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|363| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ */
 static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 {
 	u64 counter, enabled, running;
@@ -229,10 +238,40 @@ static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 
 static inline void kvm_pmu_request_counter_reprogram(struct kvm_pmc *pmc)
 {
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|164| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|604| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|622| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|874| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1189| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|612| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1190| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(... sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1193| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|1195| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|5033| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *   - arch/x86/kvm/x86.c|10914| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *
+	 * 处理函数是kvm_pmu_handle_event().
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
+/*
+ * 在以下使用reprogram_counters():
+ *   - arch/x86/kvm/pmu.c|946| <<kvm_pmu_set_msr>> reprogram_counters(pmu, diff);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|402| <<intel_pmu_set_msr>> reprogram_counters(pmu, diff);
+ */
 static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 {
 	int bit;
@@ -240,8 +279,33 @@ static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 	if (!diff)
 		return;
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|164| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|604| <<kvm_pmu_handle_event>> bitmap_copy(bitmap, pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|622| <<kvm_pmu_handle_event>> set_bit(pmc->idx, pmu->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|874| <<kvm_pmu_reset>> bitmap_zero(pmu->reprogram_pmi, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|1189| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+	 *   - arch/x86/kvm/pmu.h|232| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.h|244| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 在以下使用kvm_pmu->__reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|612| <<kvm_pmu_handle_event>> atomic64_andnot(*(s64 *)bitmap, &pmu->__reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|1190| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(... sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+	 *   - arch/x86/kvm/pmu.c|1193| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+	 */
 	for_each_set_bit(bit, (unsigned long *)&diff, X86_PMC_IDX_MAX)
 		set_bit(bit, pmu->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|1195| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|5033| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *   - arch/x86/kvm/x86.c|10914| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *
+	 * 处理函数是kvm_pmu_handle_event().
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
 }
 
diff --git a/arch/x86/kvm/svm/pmu.c b/arch/x86/kvm/svm/pmu.c
index 288f7f2a4..f0bff2b4e 100644
--- a/arch/x86/kvm/svm/pmu.c
+++ b/arch/x86/kvm/svm/pmu.c
@@ -223,6 +223,19 @@ static void amd_pmu_init(struct kvm_vcpu *vcpu)
 		pmu->gp_counters[i].type = KVM_PMC_GP;
 		pmu->gp_counters[i].vcpu = vcpu;
 		pmu->gp_counters[i].idx = i;
+		/*
+		 * 在以下使用kvm_pmc->current_config:
+		 *   - arch/x86/kvm/pmu.c|385| <<pmc_release_perf_event>> pmc->current_config = 0;
+		 *   - arch/x86/kvm/pmu.c|602| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+		 *   - arch/x86/kvm/pmu.c|607| <<reprogram_counter>> pmc->current_config = new_config;
+		 *   - arch/x86/kvm/svm/pmu.c|226| <<amd_pmu_init>> pmu->gp_counters[i].current_config = 0;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_init>> pmu->gp_counters[i].current_config = 0;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|609| <<intel_pmu_init>> pmu->fixed_counters[i].current_config = 0;
+		 *
+		 * only for creating or reusing perf_event,
+		 * eventsel value for general purpose counters,
+		 * ctrl value for fixed counters.
+		 */
 		pmu->gp_counters[i].current_config = 0;
 	}
 }
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index e67de787f..27c353b76 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -1559,6 +1559,15 @@ static void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	if (sd->current_vmcb != svm->vmcb) {
 		sd->current_vmcb = svm->vmcb;
 
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (!cpu_feature_enabled(X86_FEATURE_IBPB_ON_VMEXIT))
 			indirect_branch_prediction_barrier();
 	}
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index ed8a3cb53..66789a626 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -3739,6 +3739,16 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 	if (unlikely(status != NVMX_VMENTRY_SUCCESS))
 		goto vmentry_failed;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	/* Hide L1D cache contents from the nested guest.  */
 	vmx->vcpu.arch.l1tf_flush_l1d = true;
 
@@ -3938,6 +3948,13 @@ static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 		if (!vapic_page)
 			goto mmio_needed;
 
+		/*
+		 * called by:           
+		 *   - arch/x86/kvm/lapic.c|704| <<kvm_apic_update_irr>>
+		 *          bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+		 *   - arch/x86/kvm/vmx/nested.c|3941| <<vmx_complete_nested_posted_interrupt>>
+		 *          __kvm_apic_update_irr(vmx->nested.pi_desc->pir, vapic_page, &max_irr);
+		 */
 		__kvm_apic_update_irr(vmx->nested.pi_desc->pir,
 			vapic_page, &max_irr);
 		status = vmcs_read16(GUEST_INTR_STATUS);
@@ -5026,6 +5043,15 @@ void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	 * doesn't isolate different VMCSs, i.e. in this case, doesn't provide
 	 * separate modes for L2 vs L1.
 	 */
+	/*
+	 * 在以下使用indirect_branch_prediction_barrier():
+	 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+	 */
 	if (guest_cpu_cap_has(vcpu, X86_FEATURE_SPEC_CTRL))
 		indirect_branch_prediction_barrier();
 
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index 77012b2ec..17c830c79 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -195,6 +195,19 @@ static inline void intel_pmu_release_guest_lbr_event(struct kvm_vcpu *vcpu)
 	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
 
 	if (lbr_desc->event) {
+		/*
+		 * 在以下使用perf_event_release_kernel():
+		 *   - arch/arm64/kvm/pmu-emul.c|208| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/riscv/kvm/vcpu_pmu.c|82| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1058| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1060| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+		 *   - arch/x86/kvm/pmu.c|383| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|198| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+		 *   - kernel/events/core.c|5589| <<perf_release>> perf_event_release_kernel(file->private_data);
+		 *   - kernel/events/hw_breakpoint.c|829| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+		 *   - kernel/watchdog_perf.c|233| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+		 *   - kernel/watchdog_perf.c|300| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+		 */
 		perf_event_release_kernel(lbr_desc->event);
 		lbr_desc->event = NULL;
 		vcpu_to_pmu(vcpu)->event_count--;
@@ -240,6 +253,31 @@ int intel_pmu_create_guest_lbr_event(struct kvm_vcpu *vcpu)
 		return 0;
 	}
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|741| <<kvm_pmu_create_perf_event>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|328| <<kvm_pmu_create_perf_event>>
+	 *      event = perf_event_create_kernel_counter(attr, -1, current, kvm_riscv_pmu_overflow, pmc);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|969| <<measure_residency_fn>>
+	 *      miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|974| <<measure_residency_fn>>
+	 *      hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kvm/pmu.c|227| <<pmc_reprogram_counter>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|243| <<intel_pmu_create_guest_lbr_event>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+	 *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>>
+	 *      return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+	 *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>>
+	 *      bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+	 *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>>
+	 *      return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+	 *   - kernel/watchdog_perf.c|135| <<hardlockup_detector_event_create>>
+	 *      evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 *   - kernel/watchdog_perf.c|140| <<hardlockup_detector_event_create>>
+	 *      evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 */
 	event = perf_event_create_kernel_counter(&attr, -1,
 						current, NULL, NULL);
 	if (IS_ERR(event)) {
@@ -574,6 +612,19 @@ static void intel_pmu_init(struct kvm_vcpu *vcpu)
 		pmu->gp_counters[i].type = KVM_PMC_GP;
 		pmu->gp_counters[i].vcpu = vcpu;
 		pmu->gp_counters[i].idx = i;
+		/*
+		 * 在以下使用kvm_pmc->current_config:
+		 *   - arch/x86/kvm/pmu.c|385| <<pmc_release_perf_event>> pmc->current_config = 0;
+		 *   - arch/x86/kvm/pmu.c|602| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+		 *   - arch/x86/kvm/pmu.c|607| <<reprogram_counter>> pmc->current_config = new_config;
+		 *   - arch/x86/kvm/svm/pmu.c|226| <<amd_pmu_init>> pmu->gp_counters[i].current_config = 0;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_init>> pmu->gp_counters[i].current_config = 0;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|609| <<intel_pmu_init>> pmu->fixed_counters[i].current_config = 0;
+		 *
+		 * only for creating or reusing perf_event,
+		 * eventsel value for general purpose counters,
+		 * ctrl value for fixed counters.
+		 */
 		pmu->gp_counters[i].current_config = 0;
 	}
 
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 3b92f893b..6517306e6 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -225,10 +225,44 @@ module_param(pt_mode, int, S_IRUGO);
 
 struct x86_pmu_lbr __ro_after_init vmx_lbr_caps;
 
+/*
+ * 在以下使用vmx_l1d_should_flush:
+ *   - arch/x86/kvm/vmx/vmx.c|228| <<global>> static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|356| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|358| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|7393| <<vmx_vcpu_enter_exit>> if (static_branch_unlikely(&vmx_l1d_should_flush))
+ */
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+/*
+ * 在以下使用vmx_l1d_flush_cond:
+ *   - arch/x86/kvm/vmx/vmx.c|229| <<global>> static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|361| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|363| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|6742| <<vmx_l1d_flush>> if (static_branch_likely(&vmx_l1d_flush_cond)) {
+ */
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
+/*
+ * 在以下使用vmx_l1d_flush_mutex:
+ *   - arch/x86/kvm/vmx/vmx.c|419| <<vmentry_l1d_flush_set>> mutex_lock(&vmx_l1d_flush_mutex);
+ *   - arch/x86/kvm/vmx/vmx.c|421| <<vmentry_l1d_flush_set>> mutex_unlock(&vmx_l1d_flush_mutex);
+ */
 static DEFINE_MUTEX(vmx_l1d_flush_mutex);
 
+/*
+ * enum vmx_l1d_flush_state {
+ *     VMENTER_L1D_FLUSH_AUTO,
+ *     VMENTER_L1D_FLUSH_NEVER,
+ *     VMENTER_L1D_FLUSH_COND,
+ *     VMENTER_L1D_FLUSH_ALWAYS,
+ *     VMENTER_L1D_FLUSH_EPT_DISABLED,
+ *     VMENTER_L1D_FLUSH_NOT_REQUIRED,
+ * };
+ *
+ * 在以下使用vmentry_l1d_flush_param:
+ *   - arch/x86/kvm/vmx/vmx.c|243| <<global>> static enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush_param = VMENTER_L1D_FLUSH_AUTO;
+ *   - arch/x86/kvm/vmx/vmx.c|415| <<vmentry_l1d_flush_set>> vmentry_l1d_flush_param = l1tf;
+ *   - arch/x86/kvm/vmx/vmx.c|8733| <<vmx_init>> r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+ */
 /* Storage for pre module init parameter parsing */
 static enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush_param = VMENTER_L1D_FLUSH_AUTO;
 
@@ -245,13 +279,71 @@ static const struct {
 };
 
 #define L1D_CACHE_ORDER 4
+/*
+ * 在以下使用vmx_l1d_flush_pages:
+ *   - arch/x86/kvm/vmx/vmx.c|306| <<vmx_setup_l1d_flush>> if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+ *   - arch/x86/kvm/vmx/vmx.c|315| <<vmx_setup_l1d_flush>> vmx_l1d_flush_pages = page_address(page);
+ *   - arch/x86/kvm/vmx/vmx.c|323| <<vmx_setup_l1d_flush>> memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1, PAGE_SIZE);
+ *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> asm volatile(...:: [flush_pages] "r" (vmx_l1d_flush_pages),
+ *   - arch/x86/kvm/vmx/vmx.c|8660| <<vmx_cleanup_l1d_flush>> if (vmx_l1d_flush_pages) {
+ *   - arch/x86/kvm/vmx/vmx.c|8661| <<vmx_cleanup_l1d_flush>> free_pages((unsigned long )vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+ *   - arch/x86/kvm/vmx/vmx.c|8662| <<vmx_cleanup_l1d_flush>> vmx_l1d_flush_pages = NULL;
+ */
 static void *vmx_l1d_flush_pages;
 
+/*
+ * enum vmx_l1d_flush_state {
+ *     VMENTER_L1D_FLUSH_AUTO,
+ *     VMENTER_L1D_FLUSH_NEVER,
+ *     VMENTER_L1D_FLUSH_COND,
+ *     VMENTER_L1D_FLUSH_ALWAYS,
+ *     VMENTER_L1D_FLUSH_EPT_DISABLED,
+ *     VMENTER_L1D_FLUSH_NOT_REQUIRED,
+ * };
+ *
+ * 在以下使用vmx_setup_l1d_flush():
+ *   - arch/x86/kvm/vmx/vmx.c|395| <<vmentry_l1d_flush_set>> ret = vmx_setup_l1d_flush(l1tf);
+ *   - arch/x86/kvm/vmx/vmx.c|8708| <<vmx_init>> r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+ */
 static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
 {
 	struct page *page;
 	unsigned int i;
 
+	/*
+	 * 在以下使用l1tf_vmx_mitigation:
+	 *   - arch/x86/kernel/cpu/bugs.c|2707| <<global>> enum vmx_l1d_flush_state l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+	 *   - arch/x86/kernel/cpu/bugs.c|3039| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO)
+	 *   - arch/x86/kernel/cpu/bugs.c|3042| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_EPT_DISABLED ||
+	 *   - arch/x86/kernel/cpu/bugs.c|3043| <<l1tf_show_state>> (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER &&
+	 *   - arch/x86/kernel/cpu/bugs.c|3046| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation]);
+	 *   - arch/x86/kernel/cpu/bugs.c|3050| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation],
+	 *   - arch/x86/kvm/vmx/vmx.c|297| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+	 *   - arch/x86/kvm/vmx/vmx.c|302| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_EPT_DISABLED;
+	 *   - arch/x86/kvm/vmx/vmx.c|307| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+	 *   - arch/x86/kvm/vmx/vmx.c|353| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = l1tf;
+	 *   - arch/x86/kvm/vmx/vmx.c|414| <<vmentry_l1d_flush_set>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO) {
+	 *   - arch/x86/kvm/vmx/vmx.c|427| <<vmentry_l1d_flush_get>> if (WARN_ON_ONCE(l1tf_vmx_mitigation >= ARRAY_SIZE(vmentry_l1d_param)))
+	 *   - arch/x86/kvm/vmx/vmx.c|430| <<vmentry_l1d_flush_get>> return sysfs_emit(s, "%s\n", vmentry_l1d_param[l1tf_vmx_mitigation].option);
+	 *   - arch/x86/kvm/vmx/vmx.c|7759| <<vmx_vm_init>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER)
+	 *   - arch/x86/kvm/vmx/vmx.c|8690| <<vmx_cleanup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+	 *   - arch/x86/kvm/x86.c|1664| <<kvm_get_arch_capabilities>> if (l1tf_vmx_mitigation != VMENTER_L1D_FLUSH_NEVER)
+	 *
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
 		l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
 		return 0;
@@ -296,6 +388,16 @@ static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
 		page = alloc_pages(GFP_KERNEL, L1D_CACHE_ORDER);
 		if (!page)
 			return -ENOMEM;
+		/*
+		 * 在以下使用vmx_l1d_flush_pages:
+		 *   - arch/x86/kvm/vmx/vmx.c|306| <<vmx_setup_l1d_flush>> if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+		 *   - arch/x86/kvm/vmx/vmx.c|315| <<vmx_setup_l1d_flush>> vmx_l1d_flush_pages = page_address(page);
+		 *   - arch/x86/kvm/vmx/vmx.c|323| <<vmx_setup_l1d_flush>> memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1, PAGE_SIZE);
+		 *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> asm volatile(...:: [flush_pages] "r" (vmx_l1d_flush_pages),
+		 *   - arch/x86/kvm/vmx/vmx.c|8660| <<vmx_cleanup_l1d_flush>> if (vmx_l1d_flush_pages) {
+		 *   - arch/x86/kvm/vmx/vmx.c|8661| <<vmx_cleanup_l1d_flush>> free_pages((unsigned long )vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+		 *   - arch/x86/kvm/vmx/vmx.c|8662| <<vmx_cleanup_l1d_flush>> vmx_l1d_flush_pages = NULL;
+		 */
 		vmx_l1d_flush_pages = page_address(page);
 
 		/*
@@ -316,6 +418,15 @@ static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
 	else
 		static_branch_disable(&vmx_l1d_should_flush);
 
+	/*
+	 * 在以下使用vmx_l1d_flush_cond:
+	 *   - arch/x86/kvm/vmx/vmx.c|229| <<global>> static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|361| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|363| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|6742| <<vmx_l1d_flush>> if (static_branch_likely(&vmx_l1d_flush_cond)) {
+	 *
+	 * vmx_l1d_flush_cond可以让flush不一定真的执行.
+	 */
 	if (l1tf == VMENTER_L1D_FLUSH_COND)
 		static_branch_enable(&vmx_l1d_flush_cond);
 	else
@@ -337,6 +448,13 @@ static int vmentry_l1d_flush_parse(const char *s)
 	return -EINVAL;
 }
 
+/*
+ * static const struct kernel_param_ops vmentry_l1d_flush_ops = {
+ *     .set = vmentry_l1d_flush_set,
+ *     .get = vmentry_l1d_flush_get,
+ * };
+ * module_param_cb(vmentry_l1d_flush, &vmentry_l1d_flush_ops, NULL, 0644);
+ */
 static int vmentry_l1d_flush_set(const char *s, const struct kernel_param *kp)
 {
 	int l1tf, ret;
@@ -345,6 +463,22 @@ static int vmentry_l1d_flush_set(const char *s, const struct kernel_param *kp)
 	if (l1tf < 0)
 		return l1tf;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has(X86_BUG_L1TF))
 		return 0;
 
@@ -669,6 +803,11 @@ static inline bool cpu_need_virtualize_apic_accesses(struct kvm_vcpu *vcpu)
 	return flexpriority_enabled && lapic_in_kernel(vcpu);
 }
 
+/*
+ * 在以下使用vmx_get_passthrough_msr_slot():
+ *   - arch/x86/kvm/vmx/vmx.c|4183| <<vmx_disable_intercept_for_msr>> idx = vmx_get_passthrough_msr_slot(msr);
+ *   - arch/x86/kvm/vmx/vmx.c|4225| <<vmx_enable_intercept_for_msr>> idx = vmx_get_passthrough_msr_slot(msr);
+ */
 static int vmx_get_passthrough_msr_slot(u32 msr)
 {
 	int i;
@@ -694,6 +833,28 @@ static int vmx_get_passthrough_msr_slot(u32 msr)
 		return -ENOENT;
 	}
 
+	/*
+	 * static u32 vmx_possible_passthrough_msrs[MAX_POSSIBLE_PASSTHROUGH_MSRS] = {
+	 *     MSR_IA32_SPEC_CTRL,
+	 *     MSR_IA32_PRED_CMD,
+	 *     MSR_IA32_FLUSH_CMD,
+	 *     MSR_IA32_TSC,
+	 * #ifdef CONFIG_X86_64
+	 *     MSR_FS_BASE,
+	 *     MSR_GS_BASE,
+	 *     MSR_KERNEL_GS_BASE,
+	 *     MSR_IA32_XFD,
+	 *     MSR_IA32_XFD_ERR,
+	 * #endif
+	 *     MSR_IA32_SYSENTER_CS,
+	 *     MSR_IA32_SYSENTER_ESP,
+	 *     MSR_IA32_SYSENTER_EIP,
+	 *     MSR_CORE_C1_RES,
+	 *     MSR_CORE_C3_RESIDENCY,
+	 *     MSR_CORE_C6_RESIDENCY,
+	 *     MSR_CORE_C7_RESIDENCY,
+	 * };
+	 */
 	for (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++) {
 		if (vmx_possible_passthrough_msrs[i] == msr)
 			return i;
@@ -703,6 +864,16 @@ static int vmx_get_passthrough_msr_slot(u32 msr)
 	return -ENOENT;
 }
 
+/*
+ * 在以下调用vmx_find_uret_msr():
+ *   - arch/x86/kvm/vmx/nested.c|4833| <<nested_vmx_get_vmcs01_guest_efer>> efer_msr = vmx_find_uret_msr(vmx, MSR_EFER);
+ *   - arch/x86/kvm/vmx/vmx.c|2046| <<vmx_setup_uret_msr>> uret_msr = vmx_find_uret_msr(vmx, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|2322| <<vmx_get_msr>> msr = vmx_find_uret_msr(vmx, msr_info->index);
+ *   - arch/x86/kvm/vmx/vmx.c|2650| <<vmx_set_msr>> msr = vmx_find_uret_msr(vmx, msr_index);
+ *   - arch/x86/kvm/vmx/vmx.c|3315| <<vmx_set_efer>> if (!vmx_find_uret_msr(vmx, MSR_EFER))
+ *   - arch/x86/kvm/vmx/vmx.c|7831| <<vmx_vcpu_create>> tsx_ctrl = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|8158| <<vmx_vcpu_after_set_cpuid>> msr = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);
+ */
 struct vmx_uret_msr *vmx_find_uret_msr(struct vcpu_vmx *vmx, u32 msr)
 {
 	int i;
@@ -713,9 +884,25 @@ struct vmx_uret_msr *vmx_find_uret_msr(struct vcpu_vmx *vmx, u32 msr)
 	return NULL;
 }
 
+/*
+ * 在以下调用vmx_set_guest_uret_msr():
+ *   - arch/x86/kvm/vmx/vmx.c|2753| <<vmx_set_msr>> ret = vmx_set_guest_uret_msr(vmx, msr, data);
+ *   - arch/x86/kvm/vmx/vmx.c|8291| <<vmx_vcpu_after_set_cpuid>> vmx_set_guest_uret_msr(vmx, msr, enabled ? 0 : TSX_CTRL_RTM_DISABLE);
+ */
 static int vmx_set_guest_uret_msr(struct vcpu_vmx *vmx,
 				  struct vmx_uret_msr *msr, u64 data)
 {
+	/*
+	 * 在以下使用vcpu_vmx->guest_uret_msrs[MAX_NR_USER_RETURN_MSRS]:
+	 *   - arch/x86/kvm/vmx/vmx.c|846| <<vmx_find_uret_msr>> return &vmx->guest_uret_msrs[i];
+	 *   - arch/x86/kvm/vmx/vmx.c|853| <<vmx_set_guest_uret_msr>> unsigned int slot = msr - vmx->guest_uret_msrs;
+	 *   - arch/x86/kvm/vmx/vmx.c|1299| <<update_transition_efer>> vmx->guest_uret_msrs[i].data = guest_efer;
+	 *   - arch/x86/kvm/vmx/vmx.c|1300| <<update_transition_efer>> vmx->guest_uret_msrs[i].mask = ~ignore_bits;
+	 *   - arch/x86/kvm/vmx/vmx.c|1455| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs[i].load_into_hardware)
+	 *   - arch/x86/kvm/vmx/vmx.c|1459| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].data,
+	 *   - arch/x86/kvm/vmx/vmx.c|1460| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].mask);
+	 *   - arch/x86/kvm/vmx/vmx.c|7824| <<vmx_vcpu_create>> vmx->guest_uret_msrs[i].mask = -1ull;
+	 */
 	unsigned int slot = msr - vmx->guest_uret_msrs;
 	int ret = 0;
 
@@ -864,6 +1051,15 @@ static u32 vmx_read_guest_seg_ar(struct vcpu_vmx *vmx, unsigned seg)
 	return *p;
 }
 
+/*
+ * 在以下使用vmx_update_exception_bitmap():
+ *   - arch/x86/kvm/vmx/main.c|45| <<global>> .update_exception_bitmap = vmx_update_exception_bitmap,
+ *   - arch/x86/kvm/vmx/nested.c|2670| <<prepare_vmcs02>> vmx_update_exception_bitmap(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|2504| <<vmx_set_msr>> vmx_update_exception_bitmap(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|3321| <<enter_pmode>> vmx_update_exception_bitmap(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|3401| <<enter_rmode>> vmx_update_exception_bitmap(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8323| <<vmx_vcpu_after_set_cpuid>> vmx_update_exception_bitmap(vcpu);
+ */
 void vmx_update_exception_bitmap(struct kvm_vcpu *vcpu)
 {
 	u32 eb;
@@ -932,6 +1128,10 @@ void vmx_update_exception_bitmap(struct kvm_vcpu *vcpu)
 /*
  * Check if MSR is intercepted for currently loaded MSR bitmap.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1137| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ */
 static bool msr_write_intercepted(struct vcpu_vmx *vmx, u32 msr)
 {
 	if (!(exec_controls_get(vmx) & CPU_BASED_USE_MSR_BITMAPS))
@@ -952,19 +1152,59 @@ unsigned int __vmx_vcpu_run_flags(struct vcpu_vmx *vmx)
 	 * to change it directly without causing a vmexit.  In that case read
 	 * it after vmexit and store it in vmx->spec_ctrl.
 	 */
+	/*
+	 * 在以下使用VMX_RUN_SAVE_SPEC_CTRL:
+	 *   - arch/x86/kvm/vmx/vmx.c|1138| <<__vmx_vcpu_run_flags>> flags |= VMX_RUN_SAVE_SPEC_CTRL;
+	 *   - arch/x86/kvm/vmx/vmx.c|7638| <<vmx_spec_ctrl_restore_host>> if (flags & VMX_RUN_SAVE_SPEC_CTRL)
+	 */
 	if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
 		flags |= VMX_RUN_SAVE_SPEC_CTRL;
 
 	return flags;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1121| <<clear_atomic_switch_msr>> clear_atomic_switch_msr_special(vmx, VM_ENTRY_LOAD_IA32_EFER, VM_EXIT_LOAD_IA32_EFER);
+ *   - arch/x86/kvm/vmx/vmx.c|1129| <<clear_atomic_switch_msr>> clear_atomic_switch_msr_special(vmx, VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL, VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);
+ */
 static __always_inline void clear_atomic_switch_msr_special(struct vcpu_vmx *vmx,
 		unsigned long entry, unsigned long exit)
 {
+	/*
+	 * 在以下使用BUILD_CONTROLS_SHADOW:
+	 *   - arch/x86/kvm/vmx/vmx.h|625| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(vm_entry, VM_ENTRY_CONTROLS, 32)
+	 *   - arch/x86/kvm/vmx/vmx.h|626| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(vm_exit, VM_EXIT_CONTROLS, 32)
+	 *   - arch/x86/kvm/vmx/vmx.h|627| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(pin, PIN_BASED_VM_EXEC_CONTROL, 32)
+	 *   - arch/x86/kvm/vmx/vmx.h|628| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(exec, CPU_BASED_VM_EXEC_CONTROL, 32)
+	 *   - arch/x86/kvm/vmx/vmx.h|629| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(secondary_exec, SECONDARY_VM_EXEC_CONTROL, 32)
+	 *   - arch/x86/kvm/vmx/vmx.h|630| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(tertiary_exec, TERTIARY_VM_EXEC_CONTROL, 64)
+	 */
 	vm_entry_controls_clearbit(vmx, entry);
 	vm_exit_controls_clearbit(vmx, exit);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|1009| <<nested_vmx_get_vmexit_msr_value>> int i = vmx_find_loadstore_msr_slot(&vmx->msr_autostore.guest,
+ *   - arch/x86/kvm/vmx/nested.c|1106| <<prepare_vmx_msr_autostore_list>> msr_autostore_slot = vmx_find_loadstore_msr_slot(autostore, msr_index);
+ *   - arch/x86/kvm/vmx/vmx.c|1189| <<clear_atomic_switch_msr>> i = vmx_find_loadstore_msr_slot(&m->guest, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|1197| <<clear_atomic_switch_msr>> i = vmx_find_loadstore_msr_slot(&m->host, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|1272| <<add_atomic_switch_msr>> i = vmx_find_loadstore_msr_slot(&m->guest, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|1274| <<add_atomic_switch_msr>> j = vmx_find_loadstore_msr_slot(&m->host, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|6624| <<dump_vmcs>> efer_slot = vmx_find_loadstore_msr_slot(&vmx->msr_autoload.guest, MSR_EFER);
+ *
+ * struct vmx_msr_entry {
+ *     u32 index;
+ *     u32 reserved;
+ *     u64 value;
+ * } __aligned(16);
+ *
+ * struct vmx_msrs {
+ *     unsigned int            nr;
+ *     struct vmx_msr_entry    val[MAX_NR_LOADSTORE_MSRS];
+ * };
+ */
 int vmx_find_loadstore_msr_slot(struct vmx_msrs *m, u32 msr)
 {
 	unsigned int i;
@@ -976,6 +1216,80 @@ int vmx_find_loadstore_msr_slot(struct vmx_msrs *m, u32 msr)
 	return -ENOENT;
 }
 
+/*
+ * 第三种 3. 还有一个是general的auto load/save.
+165 
+166 # vm exit:
+167 
+168 The following VM-exit control fields determine how MSRs are stored on VM exits:
+169 
+170 - VM-exit MSR-store count (32 bits): This field specifies the number of MSRs to
+171 be stored on VM exit. It is recommended that this count not exceed 512. 1
+172 Otherwise, unpredictable processor behavior (including a machine check) may
+173 result during VM exit.
+174 
+175 - VM-exit MSR-store address (64 bits): This field contains the physical address
+176 of the VM-exit MSR-store area.  The area is a table of entries, 16 bytes per
+177 entry, where the number of entries is given by the VM-exit MSR- store count.
+178 The format of each entry is given in Table 25-15. If the VM-exit MSR-store
+179 count is not zero, the address must be 16-byte aligned.
+180 
+181 The following VM-exit control fields determine how MSRs are loaded on VM exits:
+182 
+183 - VM-exit MSR-load count (32 bits). This field contains the number of MSRs to
+184 be loaded on VM exit. It is recommended that this count not exceed 512.
+185 Otherwise, unpredictable processor behavior (including a machine check) may
+186 result during VM exit.
+187 
+188 - VM-exit MSR-load address (64 bits). This field contains the physical address
+189 of the VM-exit MSR-load area.  The area is a table of entries, 16 bytes per
+190 entry, where the number of entries is given by the VM-exit MSR-load count (see
+191 Table 25-15). If the VM-exit MSR-load count is not zero, the address must be
+192 16-byte aligned.
+193 
+194 # vm entry:
+195 
+196 A VMM may specify a list of MSRs to be loaded on VM entries:
+197 
+198 - VM-entry MSR-load count (32 bits). This field contains the number of MSRs to
+199 be loaded on VM entry. It is recommended that this count not exceed 512.
+200 Otherwise, unpredictable processor behavior (including a machine check) may
+201 result during VM entry.
+202 
+203 - VM-entry MSR-load address (64 bits). This field contains the physical address
+204 of the VM-entry MSR-load area. The area is a table of entries, 16 bytes per
+205 entry, where the number of entries is given by the VM-entry MSR-load count. The
+206 format of entries is described in Table 25-15. If the VM-entry MSR-load count
+207 is not zero, the address must be 16-byte aligned.
+208 
+209 362         struct msr_autoload {
+210 363                 struct vmx_msrs guest;
+211 364                 struct vmx_msrs host;
+212 365         } msr_autoload;
+213 366
+214 367         struct msr_autostore {
+215 368                 struct vmx_msrs guest;
+216 369         } msr_autostore;
+217 
+218  33 struct vmx_msrs {
+219  34         unsigned int            nr;
+220  35         struct vmx_msr_entry    val[MAX_NR_LOADSTORE_MSRS];
+221  36 };
+ *
+ *
+ * ADDR在init_vmcs()的时候就写了.
+ * 5164         vmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);
+ * 5165         vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);
+ * 5166         vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host.val));
+ * 5167         vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);
+ * 5168         vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest.val));
+ *
+ *
+ * 在以下调用clear_atomic_switch_msr():
+ *   - arch/x86/kvm/vmx/vmx.c|1464| <<update_transition_efer>> clear_atomic_switch_msr(vmx, MSR_EFER);
+ *   - arch/x86/kvm/vmx/vmx.c|1472| <<update_transition_efer>> clear_atomic_switch_msr(vmx, MSR_EFER);
+ *   arch/x86/kvm/vmx/vmx.c|7738| <<atomic_switch_perf_msrs>> clear_atomic_switch_msr(vmx, msrs[i].msr);
+ */
 static void clear_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr)
 {
 	int i;
@@ -1016,6 +1330,21 @@ static void clear_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr)
 	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1369| <<add_atomic_switch_msr>> add_atomic_switch_msr_special(vmx,
+ *                                           VM_ENTRY_LOAD_IA32_EFER,
+ *                                           VM_EXIT_LOAD_IA32_EFER,
+ *                                           GUEST_IA32_EFER,
+ *                                           HOST_IA32_EFER,
+ *                                           guest_val, host_val);
+ *  - arch/x86/kvm/vmx/vmx.c|1380| <<add_atomic_switch_msr>> add_atomic_switch_msr_special(vmx,
+ *                                           VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,
+ *                                           VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL,
+ *                                           GUEST_IA32_PERF_GLOBAL_CTRL,
+ *                                           HOST_IA32_PERF_GLOBAL_CTRL,
+ *                                           guest_val, host_val);
+ */
 static __always_inline void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,
 		unsigned long entry, unsigned long exit,
 		unsigned long guest_val_vmcs, unsigned long host_val_vmcs,
@@ -1028,10 +1357,31 @@ static __always_inline void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,
 	vm_exit_controls_setbit(vmx, exit);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1335| <<update_transition_efer>> add_atomic_switch_msr(vmx, MSR_EFER, guest_efer, kvm_host.efer, false);
+ *   - arch/x86/kvm/vmx/vmx.c|7579| <<atomic_switch_perf_msrs>> add_atomic_switch_msr(vmx, msrs[i].msr, msrs[i].guest, msrs[i].host, false);
+ */
 static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
 				  u64 guest_val, u64 host_val, bool entry_only)
 {
 	int i, j = 0;
+	/*
+	 * struct vmx_msrs {
+	 *     unsigned int            nr;
+	 *     struct vmx_msr_entry    val[MAX_NR_LOADSTORE_MSRS];
+	 * };
+	 *
+	 *
+	 * 292         struct msr_autoload {
+	 * 293                 struct vmx_msrs guest;
+	 * 294                 struct vmx_msrs host;
+	 * 295         } msr_autoload;
+	 * 296
+	 * 297         struct msr_autostore {
+	 * 298                 struct vmx_msrs guest;
+	 * 299         } msr_autostore;
+	 */
 	struct msr_autoload *m = &vmx->msr_autoload;
 
 	switch (msr) {
@@ -1066,6 +1416,29 @@ static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
 		wrmsrl(MSR_IA32_PEBS_ENABLE, 0);
 	}
 
+	/*
+	 * alled by:
+	 *   - arch/x86/kvm/vmx/nested.c|1009| <<nested_vmx_get_vmexit_msr_value>> int i = vmx_find_loadstore_msr_slot(&vmx->msr_autostore.guest,
+	 *   - arch/x86/kvm/vmx/nested.c|1106| <<prepare_vmx_msr_autostore_list>> msr_autostore_slot = vmx_find_loadstore_msr_slot(autostore, msr_index);
+	 *   - arch/x86/kvm/vmx/vmx.c|1189| <<clear_atomic_switch_msr>> i = vmx_find_loadstore_msr_slot(&m->guest, msr);
+	 *   - arch/x86/kvm/vmx/vmx.c|1197| <<clear_atomic_switch_msr>> i = vmx_find_loadstore_msr_slot(&m->host, msr);
+	 *   - arch/x86/kvm/vmx/vmx.c|1272| <<add_atomic_switch_msr>> i = vmx_find_loadstore_msr_slot(&m->guest, msr);
+	 *   - arch/x86/kvm/vmx/vmx.c|1274| <<add_atomic_switch_msr>> j = vmx_find_loadstore_msr_slot(&m->host, msr);
+	 *   - arch/x86/kvm/vmx/vmx.c|6624| <<dump_vmcs>> efer_slot = vmx_find_loadstore_msr_slot(&vmx->msr_autoload.guest, MSR_EFER);
+	 *
+	 * struct vmx_msr_entry {
+	 *     u32 index;
+	 *     u32 reserved;
+	 *     u64 value;
+	 * } __aligned(16);
+	 *
+	 * struct vmx_msrs {
+	 *     unsigned int            nr;
+	 *     struct vmx_msr_entry    val[MAX_NR_LOADSTORE_MSRS];
+	 *
+	 *
+	 * struct msr_autoload *m = &vmx->msr_autoload;
+	 */
 	i = vmx_find_loadstore_msr_slot(&m->guest, msr);
 	if (!entry_only)
 		j = vmx_find_loadstore_msr_slot(&m->host, msr);
@@ -1094,6 +1467,10 @@ static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
 	m->host.val[j].value = host_val;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2075| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_EFER, update_transition_efer(vmx));
+ */
 static bool update_transition_efer(struct vcpu_vmx *vmx)
 {
 	u64 guest_efer = vmx->vcpu.arch.efer;
@@ -1141,6 +1518,17 @@ static bool update_transition_efer(struct vcpu_vmx *vmx)
 	guest_efer &= ~ignore_bits;
 	guest_efer |= kvm_host.efer & ignore_bits;
 
+	/*
+	 * 在以下使用vcpu_vmx->guest_uret_msrs[MAX_NR_USER_RETURN_MSRS]:
+	 *   - arch/x86/kvm/vmx/vmx.c|846| <<vmx_find_uret_msr>> return &vmx->guest_uret_msrs[i];
+	 *   - arch/x86/kvm/vmx/vmx.c|853| <<vmx_set_guest_uret_msr>> unsigned int slot = msr - vmx->guest_uret_msrs;
+	 *   - arch/x86/kvm/vmx/vmx.c|1299| <<update_transition_efer>> vmx->guest_uret_msrs[i].data = guest_efer;
+	 *   - arch/x86/kvm/vmx/vmx.c|1300| <<update_transition_efer>> vmx->guest_uret_msrs[i].mask = ~ignore_bits;
+	 *   - arch/x86/kvm/vmx/vmx.c|1455| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs[i].load_into_hardware)
+	 *   - arch/x86/kvm/vmx/vmx.c|1459| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].data,
+	 *   - arch/x86/kvm/vmx/vmx.c|1460| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].mask);
+	 *   - arch/x86/kvm/vmx/vmx.c|7824| <<vmx_vcpu_create>> vmx->guest_uret_msrs[i].mask = -1ull;
+	 */
 	vmx->guest_uret_msrs[i].data = guest_efer;
 	vmx->guest_uret_msrs[i].mask = ~ignore_bits;
 
@@ -1294,6 +1682,23 @@ void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 	 * when guest state is loaded. This happens when guest transitions
 	 * to/from long-mode by setting MSR_EFER.LMA.
 	 */
+	/*
+	 * 在以下使用vcpu_vmx->guest_uret_msrs[MAX_NR_USER_RETURN_MSRS]:
+	 *   - arch/x86/kvm/vmx/vmx.c|846| <<vmx_find_uret_msr>> return &vmx->guest_uret_msrs[i];
+	 *   - arch/x86/kvm/vmx/vmx.c|853| <<vmx_set_guest_uret_msr>> unsigned int slot = msr - vmx->guest_uret_msrs;
+	 *   - arch/x86/kvm/vmx/vmx.c|1299| <<update_transition_efer>> vmx->guest_uret_msrs[i].data = guest_efer;
+	 *   - arch/x86/kvm/vmx/vmx.c|1300| <<update_transition_efer>> vmx->guest_uret_msrs[i].mask = ~ignore_bits;
+	 *   - arch/x86/kvm/vmx/vmx.c|1455| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs[i].load_into_hardware)
+	 *   - arch/x86/kvm/vmx/vmx.c|1459| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].data,
+	 *   - arch/x86/kvm/vmx/vmx.c|1460| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].mask);
+	 *   - arch/x86/kvm/vmx/vmx.c|7824| <<vmx_vcpu_create>> vmx->guest_uret_msrs[i].mask = -1ull;
+	 *
+	 * 在以下使用vcpu_vmx->guest_uret_msrs_loaded:
+	 *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs_loaded) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1527| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs_loaded = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|1618| <<vmx_prepare_switch_to_host>> vmx->guest_uret_msrs_loaded = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|2194| <<vmx_setup_uret_msrs>> vmx->guest_uret_msrs_loaded = false;
+	 */
 	if (!vmx->guest_uret_msrs_loaded) {
 		vmx->guest_uret_msrs_loaded = true;
 		for (i = 0; i < kvm_nr_uret_msrs; ++i) {
@@ -1477,6 +1882,15 @@ void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
 		 * performs IBPB on nested VM-Exit (a single nested transition
 		 * may switch the active VMCS multiple times).
 		 */
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (!buddy || WARN_ON_ONCE(buddy->vmcs != prev))
 			indirect_branch_prediction_barrier();
 	}
@@ -1874,9 +2288,30 @@ void vmx_inject_exception(struct kvm_vcpu *vcpu)
 	vmx_clear_hlt(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2071| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_STAR, load_syscall_msrs);
+ *   - arch/x86/kvm/vmx/vmx.c|2072| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_LSTAR, load_syscall_msrs);
+ *   - arch/x86/kvm/vmx/vmx.c|2073| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_SYSCALL_MASK, load_syscall_msrs);
+ *   - arch/x86/kvm/vmx/vmx.c|2075| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_EFER, update_transition_efer(vmx));
+ *   - arch/x86/kvm/vmx/vmx.c|2077| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_TSC_AUX,
+ *   - arch/x86/kvm/vmx/vmx.c|2087| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_IA32_TSX_CTRL, boot_cpu_has(X86_FEATURE_RTM));
+ */
 static void vmx_setup_uret_msr(struct vcpu_vmx *vmx, unsigned int msr,
 			       bool load_into_hardware)
 {
+	/*
+	 * struct vmx_msrs {
+	 *     unsigned int            nr;
+	 *     struct vmx_msr_entry    val[MAX_NR_LOADSTORE_MSRS];
+	 * };
+	 *
+	 * struct vmx_uret_msr {
+	 *     bool load_into_hardware;
+	 *     u64 data;
+	 *     u64 mask;
+	 * };
+	 */
 	struct vmx_uret_msr *uret_msr;
 
 	uret_msr = vmx_find_uret_msr(vmx, msr);
@@ -1892,6 +2327,12 @@ static void vmx_setup_uret_msr(struct vcpu_vmx *vmx, unsigned int msr,
  * an MSR here does _NOT_ mean it's not emulated, only that it will not be
  * loaded into hardware when running the guest.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|3329| <<vmx_set_efer>> vmx_setup_uret_msrs(vmx);
+ *   - arch/x86/kvm/vmx/vmx.c|5012| <<init_vmcs>> vmx_setup_uret_msrs(vmx);
+ *   - arch/x86/kvm/vmx/vmx.c|8134| <<vmx_vcpu_after_set_cpuid>> vmx_setup_uret_msrs(vmx);
+ */
 static void vmx_setup_uret_msrs(struct vcpu_vmx *vmx)
 {
 #ifdef CONFIG_X86_64
@@ -1926,6 +2367,13 @@ static void vmx_setup_uret_msrs(struct vcpu_vmx *vmx)
 	 * The set of MSRs to load may have changed, reload MSRs before the
 	 * next VM-Enter.
 	 */
+	/*
+	 * 在以下使用vcpu_vmx->guest_uret_msrs_loaded:
+	 *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs_loaded) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1527| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs_loaded = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|1618| <<vmx_prepare_switch_to_host>> vmx->guest_uret_msrs_loaded = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|2194| <<vmx_setup_uret_msrs>> vmx->guest_uret_msrs_loaded = false;
+	 */
 	vmx->guest_uret_msrs_loaded = false;
 }
 
@@ -4001,6 +4449,28 @@ static void vmx_msr_bitmap_l01_changed(struct vcpu_vmx *vmx)
 	vmx->nested.force_msr_bitmap_recalc = true;
 }
 
+/*
+ * 在以下调用vmx_disable_intercept_for_msr():
+ *   - arch/x86/kvm/vmx/vmx.c|2501| <<vmx_set_msr(MSR_IA32_XFD)>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_XFD, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|2600| <<vmx_set_msr(MSR_IA32_SPEC_CTRL)>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|4409| <<vmx_update_msr_bitmap_x2apic>> vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|4410| <<vmx_update_msr_bitmap_x2apic>> vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|4412| <<vmx_update_msr_bitmap_x2apic>> vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_ICR), MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|4452| <<vmx_msr_filter_changed>> vmx_disable_intercept_for_msr(vcpu, msr, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|4455| <<vmx_msr_filter_changed>> vmx_disable_intercept_for_msr(vcpu, msr, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7987| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_TSC, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|7989| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_FS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7990| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_GS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7991| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7993| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7994| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7995| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7997| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C1_RES, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|7998| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C3_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|7999| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C6_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|8000| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C7_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.h|446| <<vmx_set_intercept_for_msr>> vmx_disable_intercept_for_msr(vcpu, msr, type);
+ */
 void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4016,6 +4486,11 @@ void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 	 * Mark the desired intercept state in shadow bitmap, this is needed
 	 * for resync when the MSR filters change.
 	 */
+	/*
+	 * 在以下使用vmx_get_passthrough_msr_slot():
+	 *   - arch/x86/kvm/vmx/vmx.c|4183| <<vmx_disable_intercept_for_msr>> idx = vmx_get_passthrough_msr_slot(msr);
+	 *   - arch/x86/kvm/vmx/vmx.c|4225| <<vmx_enable_intercept_for_msr>> idx = vmx_get_passthrough_msr_slot(msr);
+	 */
 	idx = vmx_get_passthrough_msr_slot(msr);
 	if (idx >= 0) {
 		if (type & MSR_TYPE_R)
@@ -4043,6 +4518,11 @@ void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 		vmx_clear_msr_bitmap_write(msr_bitmap, msr);
 }
 
+/*
+ * 在以下调用vmx_enable_intercept_for_msr():
+ *   - arch/x86/kvm/vmx/vmx.c|4292| <<vmx_update_msr_bitmap_x2apic>> vmx_enable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TMCCT), MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.h|433| <<vmx_set_intercept_for_msr>> vmx_enable_intercept_for_msr(vcpu, msr, type);
+ */
 void vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4058,6 +4538,11 @@ void vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 	 * Mark the desired intercept state in shadow bitmap, this is needed
 	 * for resync when the MSR filter changes.
 	 */
+	/*
+	 * 在以下使用vmx_get_passthrough_msr_slot():
+	 *   - arch/x86/kvm/vmx/vmx.c|4183| <<vmx_disable_intercept_for_msr>> idx = vmx_get_passthrough_msr_slot(msr);
+	 *   - arch/x86/kvm/vmx/vmx.c|4225| <<vmx_enable_intercept_for_msr>> idx = vmx_get_passthrough_msr_slot(msr);
+	 */
 	idx = vmx_get_passthrough_msr_slot(msr);
 	if (idx >= 0) {
 		if (type & MSR_TYPE_R)
@@ -6665,17 +7150,57 @@ int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
  * information but as all relevant affected CPUs have 32KiB L1D cache size
  * there is no point in doing so.
  */
+/*
+ * 例子:
+ * L1TF leverages speculative execution to access data that should be
+ * protected. Here's a basic breakdown:
+ *
+ * Execution speculation: Under certain conditions, the CPU may speculatively
+ * execute instructions that access memory locations (such as the L1 cache)
+ * which are not supposed to be visible to the current process.
+ *
+ * Data leakage: Even if the speculative execution is rolled back, the data
+ * accessed may remain in the L1 cache, making it possible for an attacker to
+ * use side-channel techniques to read the cached data.
+ *
+ * Attacking privileged memory: L1TF specifically impacts systems where an
+ * attacker running on a less privileged or user-level process can read data
+ * from higher-privileged memory regions (like kernel space memory).
+ *
+ * 在以下调用vmx_l1d_flush():
+ *   - arch/x86/kvm/vmx/vmx.c|7394| <<vmx_vcpu_enter_exit>> vmx_l1d_flush(vcpu);
+ */
 static noinstr void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 {
 	int size = PAGE_SIZE << L1D_CACHE_ORDER;
 
+	/*
+	 * 在以下使用vmx_l1d_flush_cond:
+	 *   - arch/x86/kvm/vmx/vmx.c|229| <<global>> static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|361| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|363| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_flush_cond);
+	 *   - arch/x86/kvm/vmx/vmx.c|6742| <<vmx_l1d_flush>> if (static_branch_likely(&vmx_l1d_flush_cond)) {
+	 */
 	/*
 	 * This code is only executed when the flush mode is 'cond' or
 	 * 'always'
 	 */
 	if (static_branch_likely(&vmx_l1d_flush_cond)) {
+		/*
+		 * 最终的结果以flush_l1d为准
+		 */
 		bool flush_l1d;
 
+		/*
+		 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+		 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+		 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+		 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+		 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+		 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+		 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+		 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+		 */
 		/*
 		 * Clear the per-vcpu flush bit, it gets set again if the vCPU
 		 * is reloaded, i.e. if the vCPU is scheduled out or if KVM
@@ -6685,6 +7210,12 @@ static noinstr void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 		flush_l1d = vcpu->arch.l1tf_flush_l1d;
 		vcpu->arch.l1tf_flush_l1d = false;
 
+		/*
+		 * 在以下使用kvm_set_cpu_l1tf_flush_l1d():
+		 *   - arch/x86/include/asm/idtentry.h|215| <<DEFINE_IDTENTRY_IRQ>> kvm_set_cpu_l1tf_flush_l1d(); \
+		 *   - arch/x86/include/asm/idtentry.h|260| <<DEFINE_IDTENTRY_SYSVEC>> kvm_set_cpu_l1tf_flush_l1d(); \
+		 *   - arch/x86/include/asm/idtentry.h|299| <<DEFINE_IDTENTRY_SYSVEC_SIMPLE>> kvm_set_cpu_l1tf_flush_l1d(); \
+		 */
 		/*
 		 * Clear the per-cpu flush bit, it gets set again from
 		 * the interrupt handlers.
@@ -6696,13 +7227,31 @@ static noinstr void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 			return;
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_stat->l1d_flush:
+	 *   - arch/x86/kvm/x86.c|322| <<global>> STATS_DESC_COUNTER(VCPU, l1d_flush),
+	 *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> vcpu->stat.l1d_flush++;
+	 */
 	vcpu->stat.l1d_flush++;
 
+	/*
+	 * 似乎是如果硬件支持的话
+	 */
 	if (static_cpu_has(X86_FEATURE_FLUSH_L1D)) {
 		native_wrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);
 		return;
 	}
 
+	/*
+	 * 在以下使用vmx_l1d_flush_pages:
+	 *   - arch/x86/kvm/vmx/vmx.c|306| <<vmx_setup_l1d_flush>> if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+	 *   - arch/x86/kvm/vmx/vmx.c|315| <<vmx_setup_l1d_flush>> vmx_l1d_flush_pages = page_address(page);
+	 *   - arch/x86/kvm/vmx/vmx.c|323| <<vmx_setup_l1d_flush>> memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1, PAGE_SIZE);
+	 *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> asm volatile(...:: [flush_pages] "r" (vmx_l1d_flush_pages),
+	 *   - arch/x86/kvm/vmx/vmx.c|8660| <<vmx_cleanup_l1d_flush>> if (vmx_l1d_flush_pages) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8661| <<vmx_cleanup_l1d_flush>> free_pages((unsigned long )vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+	 *   - arch/x86/kvm/vmx/vmx.c|8662| <<vmx_cleanup_l1d_flush>> vmx_l1d_flush_pages = NULL;
+	 */
 	asm volatile(
 		/* First ensure the pages are in the TLB */
 		"xorl	%%eax, %%eax\n"
@@ -6922,6 +7471,20 @@ static void vmx_set_rvi(int vector)
 	}
 }
 
+/*
+ * 在以下使用sync_pir_to_irr:
+ *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+ *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+ *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+ *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+ *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+ *
+ * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+ */
 int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7192,9 +7755,19 @@ void vmx_cancel_injection(struct kvm_vcpu *vcpu)
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7662| <<vmx_vcpu_run>> atomic_switch_perf_msrs(vmx);
+ */
 static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
 {
 	int i, nr_msrs;
+	/*
+	 * struct perf_guest_switch_msr {
+	 *     unsigned msr;
+	 *     u64 host, guest;
+	 * };
+	 */
 	struct perf_guest_switch_msr *msrs;
 	struct kvm_pmu *pmu = vcpu_to_pmu(&vmx->vcpu);
 
@@ -7202,6 +7775,9 @@ static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
 	if (pmu->pebs_enable & pmu->global_ctrl)
 		intel_pmu_cross_mapped_check(pmu);
 
+	/*
+	 * 只在此处调用perf_guest_get_msrs().
+	 */
 	/* Note, nr_msrs may be garbage if perf_guest_get_msrs() returns NULL. */
 	msrs = perf_guest_get_msrs(&nr_msrs, (void *)pmu);
 	if (!msrs)
@@ -7252,11 +7828,29 @@ void noinstr vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp)
 void noinstr vmx_spec_ctrl_restore_host(struct vcpu_vmx *vmx,
 					unsigned int flags)
 {
+	/*
+	 * 在以下使用percpu x86_spec_ctrl_current:
+	 *   - arch/x86/entry/calling.h|321| <<IBRS_ENTER>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/entry/calling.h|341| <<IBRS_EXIT>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/include/asm/nospec-branch.h|567| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|261| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/include/asm/spec-ctrl.h|86| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|293| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|303| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+	 *   - arch/x86/kernel/cpu/bugs.c|306| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|318| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+	 *   - arch/x86/kvm/vmx/vmx.c|7501| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+	 */
 	u64 hostval = this_cpu_read(x86_spec_ctrl_current);
 
 	if (!cpu_feature_enabled(X86_FEATURE_MSR_SPEC_CTRL))
 		return;
 
+	/*
+	 * 在以下使用VMX_RUN_SAVE_SPEC_CTRL:
+	 *   - arch/x86/kvm/vmx/vmx.c|1138| <<__vmx_vcpu_run_flags>> flags |= VMX_RUN_SAVE_SPEC_CTRL;
+	 *   - arch/x86/kvm/vmx/vmx.c|7638| <<vmx_spec_ctrl_restore_host>> if (flags & VMX_RUN_SAVE_SPEC_CTRL)
+	 */
 	if (flags & VMX_RUN_SAVE_SPEC_CTRL)
 		vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
 
@@ -7267,6 +7861,16 @@ void noinstr vmx_spec_ctrl_restore_host(struct vcpu_vmx *vmx,
 	 * transitioning from a less privileged predictor mode, regardless of
 	 * whether the guest/host values differ.
 	 */
+	/*
+	 * 在以下使用X86_FEATURE_KERNEL_IBRS:
+	 *   - arch/x86/entry/calling.h|306| <<IBRS_ENTER>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/entry/calling.h|335| <<IBRS_EXIT>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/kernel/cpu/bugs.c|370| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kernel/cpu/bugs.c|2252| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+	 *   - arch/x86/kernel/smpboot.c|1388| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kvm/vmx/vmx.c|7529| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+	 *   - drivers/idle/intel_idle.c|2092| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
 	    vmx->spec_ctrl != hostval)
 		native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
@@ -7304,6 +7908,13 @@ static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
 
 	guest_state_enter_irqoff();
 
+	/*
+	 * 在以下使用vmx_l1d_should_flush:
+	 *   - arch/x86/kvm/vmx/vmx.c|228| <<global>> static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+	 *   - arch/x86/kvm/vmx/vmx.c|356| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_should_flush);
+	 *   - arch/x86/kvm/vmx/vmx.c|358| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_should_flush);
+	 *   - arch/x86/kvm/vmx/vmx.c|7393| <<vmx_vcpu_enter_exit>> if (static_branch_unlikely(&vmx_l1d_should_flush))
+	 */
 	/*
 	 * L1D Flush includes CPU buffer clear to mitigate MDS, but VERW
 	 * mitigation for MDS is done late in VMentry and is still
@@ -7544,6 +8155,17 @@ int vmx_vcpu_create(struct kvm_vcpu *vcpu)
 			goto free_vpid;
 	}
 
+	/*
+	 * 在以下使用vcpu_vmx->guest_uret_msrs[MAX_NR_USER_RETURN_MSRS]:
+	 *   - arch/x86/kvm/vmx/vmx.c|846| <<vmx_find_uret_msr>> return &vmx->guest_uret_msrs[i];
+	 *   - arch/x86/kvm/vmx/vmx.c|853| <<vmx_set_guest_uret_msr>> unsigned int slot = msr - vmx->guest_uret_msrs;
+	 *   - arch/x86/kvm/vmx/vmx.c|1299| <<update_transition_efer>> vmx->guest_uret_msrs[i].data = guest_efer;
+	 *   - arch/x86/kvm/vmx/vmx.c|1300| <<update_transition_efer>> vmx->guest_uret_msrs[i].mask = ~ignore_bits;
+	 *   - arch/x86/kvm/vmx/vmx.c|1455| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs[i].load_into_hardware)
+	 *   - arch/x86/kvm/vmx/vmx.c|1459| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].data,
+	 *   - arch/x86/kvm/vmx/vmx.c|1460| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].mask);
+	 *   - arch/x86/kvm/vmx/vmx.c|7824| <<vmx_vcpu_create>> vmx->guest_uret_msrs[i].mask = -1ull;
+	 */
 	for (i = 0; i < kvm_nr_uret_msrs; ++i)
 		vmx->guest_uret_msrs[i].mask = -1ull;
 	if (boot_cpu_has(X86_FEATURE_RTM)) {
@@ -7645,6 +8267,22 @@ int vmx_vm_init(struct kvm *kvm)
 	if (!ple_gap)
 		kvm->arch.pause_in_guest = true;
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
 		switch (l1tf_mitigation) {
 		case L1TF_MITIGATION_OFF:
@@ -8342,7 +8980,30 @@ static unsigned int vmx_handle_intel_pt_intr(void)
 	if (!vcpu || !kvm_handling_nmi_from_guest(vcpu))
 		return 0;
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|144| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8589| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10916| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11294| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * 处理函数是kvm_pmu_deliver_pmi().
+	 */
 	kvm_make_request(KVM_REQ_PMI, vcpu);
+	/*
+	 * 在以下使用kvm_pmu->global_status:
+	 *   - arch/x86/kvm/pmu.c|116| <<__kvm_perf_overflow>> skip_pmi = __test_and_set_bit(GLOBAL_STATUS_BUFFER_OVF_BIT,
+	 *              (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/pmu.c|119| <<__kvm_perf_overflow>> __set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/pmu.c|717| <<kvm_pmu_get_msr(MSR_CORE_PERF_GLOBAL_STATUS/)>> msr_info->data = pmu->global_status;
+	 *   - arch/x86/kvm/pmu.c|758| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_STATUS/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS)>>
+	 *              pmu->global_status = data;
+	 *   - arch/x86/kvm/pmu.c|783| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL/MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR)>>
+	 *              pmu->global_status &= ~data;
+	 *   - arch/x86/kvm/pmu.c|812| <<kvm_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|8591| <<vmx_handle_intel_pt_intr>> __set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
+	 *              (unsigned long *)&vcpu->arch.pmu.global_status);
+	 */
 	__set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
 		  (unsigned long *)&vcpu->arch.pmu.global_status);
 	return 1;
@@ -8586,6 +9247,16 @@ __init int vmx_hardware_setup(void)
 
 static void vmx_cleanup_l1d_flush(void)
 {
+	/*
+	 * 在以下使用vmx_l1d_flush_pages:
+	 *   - arch/x86/kvm/vmx/vmx.c|306| <<vmx_setup_l1d_flush>> if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+	 *   - arch/x86/kvm/vmx/vmx.c|315| <<vmx_setup_l1d_flush>> vmx_l1d_flush_pages = page_address(page);
+	 *   - arch/x86/kvm/vmx/vmx.c|323| <<vmx_setup_l1d_flush>> memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1, PAGE_SIZE);
+	 *   - arch/x86/kvm/vmx/vmx.c|6765| <<vmx_l1d_flush>> asm volatile(...:: [flush_pages] "r" (vmx_l1d_flush_pages),
+	 *   - arch/x86/kvm/vmx/vmx.c|8660| <<vmx_cleanup_l1d_flush>> if (vmx_l1d_flush_pages) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8661| <<vmx_cleanup_l1d_flush>> free_pages((unsigned long )vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+	 *   - arch/x86/kvm/vmx/vmx.c|8662| <<vmx_cleanup_l1d_flush>> vmx_l1d_flush_pages = NULL;
+	 */
 	if (vmx_l1d_flush_pages) {
 		free_pages((unsigned long)vmx_l1d_flush_pages, L1D_CACHE_ORDER);
 		vmx_l1d_flush_pages = NULL;
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 951e44dc9..8cfeaf078 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -271,7 +271,25 @@ struct vcpu_vmx {
 	 * of 64-bit mode or if EFER.SCE=1, thus the SYSCALL MSRs don't need to
 	 * be loaded into hardware if those conditions aren't met.
 	 */
+	/*
+	 * 在以下使用vcpu_vmx->guest_uret_msrs[MAX_NR_USER_RETURN_MSRS]:
+	 *   - arch/x86/kvm/vmx/vmx.c|846| <<vmx_find_uret_msr>> return &vmx->guest_uret_msrs[i];
+	 *   - arch/x86/kvm/vmx/vmx.c|853| <<vmx_set_guest_uret_msr>> unsigned int slot = msr - vmx->guest_uret_msrs;
+	 *   - arch/x86/kvm/vmx/vmx.c|1299| <<update_transition_efer>> vmx->guest_uret_msrs[i].data = guest_efer;
+	 *   - arch/x86/kvm/vmx/vmx.c|1300| <<update_transition_efer>> vmx->guest_uret_msrs[i].mask = ~ignore_bits;
+	 *   - arch/x86/kvm/vmx/vmx.c|1455| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs[i].load_into_hardware)
+	 *   - arch/x86/kvm/vmx/vmx.c|1459| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].data,
+	 *   - arch/x86/kvm/vmx/vmx.c|1460| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].mask);
+	 *   - arch/x86/kvm/vmx/vmx.c|7824| <<vmx_vcpu_create>> vmx->guest_uret_msrs[i].mask = -1ull;
+	 */
 	struct vmx_uret_msr   guest_uret_msrs[MAX_NR_USER_RETURN_MSRS];
+	/*
+	 * 在以下使用vcpu_vmx->guest_uret_msrs_loaded:
+	 *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs_loaded) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1527| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs_loaded = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|1618| <<vmx_prepare_switch_to_host>> vmx->guest_uret_msrs_loaded = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|2194| <<vmx_setup_uret_msrs>> vmx->guest_uret_msrs_loaded = false;
+	 */
 	bool                  guest_uret_msrs_loaded;
 #ifdef CONFIG_X86_64
 	u64		      msr_host_kernel_gs_base;
@@ -426,6 +444,24 @@ u64 vmx_get_l2_tsc_multiplier(struct kvm_vcpu *vcpu);
 
 gva_t vmx_get_untagged_addr(struct kvm_vcpu *vcpu, gva_t gva, unsigned int flags);
 
+/*
+ * 在以下使用vmx_set_intercept_for_msr():
+ *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|685| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|690| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|691| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/vmx.c|4404| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW, !(mode & MSR_BITMAP_MODE_X2APIC));
+ *   - arch/x86/kvm/vmx/vmx.c|4422| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4423| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4424| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4425| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4427| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4428| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|8296| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R, !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+ *   - arch/x86/kvm/vmx/vmx.c|8300| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W, !guest_has_pred_cmd_msr(vcpu));
+ *   - arch/x86/kvm/vmx/vmx.c|8304| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W, !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+ */
 static inline void vmx_set_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr,
 					     int type, bool value)
 {
@@ -585,6 +621,15 @@ static inline u8 vmx_get_rvi(void)
 #define KVM_OPTIONAL_VMX_TERTIARY_VM_EXEC_CONTROL			\
 	(TERTIARY_EXEC_IPI_VIRT)
 
+/*
+ * 在以下使用BUILD_CONTROLS_SHADOW:
+ *   - arch/x86/kvm/vmx/vmx.h|625| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(vm_entry, VM_ENTRY_CONTROLS, 32)
+ *   - arch/x86/kvm/vmx/vmx.h|626| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(vm_exit, VM_EXIT_CONTROLS, 32)
+ *   - arch/x86/kvm/vmx/vmx.h|627| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(pin, PIN_BASED_VM_EXEC_CONTROL, 32)
+ *   - arch/x86/kvm/vmx/vmx.h|628| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(exec, CPU_BASED_VM_EXEC_CONTROL, 32)
+ *   - arch/x86/kvm/vmx/vmx.h|629| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(secondary_exec, SECONDARY_VM_EXEC_CONTROL, 32)
+ *   - arch/x86/kvm/vmx/vmx.h|630| <<BUILD_CONTROLS_SHADOW>> BUILD_CONTROLS_SHADOW(tertiary_exec, TERTIARY_VM_EXEC_CONTROL, 64)
+ */
 #define BUILD_CONTROLS_SHADOW(lname, uname, bits)						\
 static inline void lname##_controls_set(struct vcpu_vmx *vmx, u##bits val)			\
 {												\
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 4b64ab350..f923b2c81 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -89,6 +89,55 @@
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|101| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/x86.c|324| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|572| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|581| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|182| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|201| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|247| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1360| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|2936| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3095| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4251| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2058| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2320| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2345| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7270| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7281| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|13647| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|13649| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|13652| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ *
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|102| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|575| <<firmware_restrict_branch_speculation_start>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|613| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4477| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7887| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3779| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3806| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
+
 #define MAX_IO_MSRS 256
 #define KVM_MAX_MCE_BANKS 32
 
@@ -213,6 +262,15 @@ struct kvm_user_return_msrs {
 
 u32 __read_mostly kvm_nr_uret_msrs;
 EXPORT_SYMBOL_GPL(kvm_nr_uret_msrs);
+/*
+ * 在以下使用kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]:
+ *   - arch/x86/kvm/x86.c|265| <<global>> static u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];
+ *   - arch/x86/kvm/x86.c|630| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+ *   - arch/x86/kvm/x86.c|658| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+ *   - arch/x86/kvm/x86.c|668| <<kvm_find_user_return_msr>> if (kvm_uret_msrs_list[i] == msr)
+ *   - arch/x86/kvm/x86.c|682| <<kvm_user_return_msr_cpu_online>> rdmsrl_safe(kvm_uret_msrs_list[i], &value);
+ *   - arch/x86/kvm/x86.c|696| <<kvm_set_user_return_msr>> err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
+ */
 static u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];
 static struct kvm_user_return_msrs __percpu *user_return_msrs;
 
@@ -578,6 +636,15 @@ static void kvm_on_user_return(struct user_return_notifier *urn)
 	for (slot = 0; slot < kvm_nr_uret_msrs; ++slot) {
 		values = &msrs->values[slot];
 		if (values->host != values->curr) {
+			/*
+			 * 在以下使用kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]:
+			 *   - arch/x86/kvm/x86.c|265| <<global>> static u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];
+			 *   - arch/x86/kvm/x86.c|630| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+			 *   - arch/x86/kvm/x86.c|658| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+			 *   - arch/x86/kvm/x86.c|668| <<kvm_find_user_return_msr>> if (kvm_uret_msrs_list[i] == msr)
+			 *   - arch/x86/kvm/x86.c|682| <<kvm_user_return_msr_cpu_online>> rdmsrl_safe(kvm_uret_msrs_list[i], &value);
+			 *   - arch/x86/kvm/x86.c|696| <<kvm_set_user_return_msr>> err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
+			 */
 			wrmsrl(kvm_uret_msrs_list[slot], values->host);
 			values->curr = values->host;
 		}
@@ -606,6 +673,15 @@ int kvm_add_user_return_msr(u32 msr)
 	if (kvm_probe_user_return_msr(msr))
 		return -1;
 
+	/*
+	 * 在以下使用kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]:
+	 *   - arch/x86/kvm/x86.c|265| <<global>> static u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];
+	 *   - arch/x86/kvm/x86.c|630| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+	 *   - arch/x86/kvm/x86.c|658| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+	 *   - arch/x86/kvm/x86.c|668| <<kvm_find_user_return_msr>> if (kvm_uret_msrs_list[i] == msr)
+	 *   - arch/x86/kvm/x86.c|682| <<kvm_user_return_msr_cpu_online>> rdmsrl_safe(kvm_uret_msrs_list[i], &value);
+	 *   - arch/x86/kvm/x86.c|696| <<kvm_set_user_return_msr>> err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
+	 */
 	kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
 	return kvm_nr_uret_msrs++;
 }
@@ -616,6 +692,15 @@ int kvm_find_user_return_msr(u32 msr)
 	int i;
 
 	for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+		/*
+		 * 在以下使用kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]:
+		 *   - arch/x86/kvm/x86.c|265| <<global>> static u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];
+		 *   - arch/x86/kvm/x86.c|630| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+		 *   - arch/x86/kvm/x86.c|658| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+		 *   - arch/x86/kvm/x86.c|668| <<kvm_find_user_return_msr>> if (kvm_uret_msrs_list[i] == msr)
+		 *   - arch/x86/kvm/x86.c|682| <<kvm_user_return_msr_cpu_online>> rdmsrl_safe(kvm_uret_msrs_list[i], &value);
+		 *   - arch/x86/kvm/x86.c|696| <<kvm_set_user_return_msr>> err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
+		 */
 		if (kvm_uret_msrs_list[i] == msr)
 			return i;
 	}
@@ -630,6 +715,15 @@ static void kvm_user_return_msr_cpu_online(void)
 	int i;
 
 	for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+		/*
+		 * 在以下使用kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]:
+		 *   - arch/x86/kvm/x86.c|265| <<global>> static u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];
+		 *   - arch/x86/kvm/x86.c|630| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+		 *   - arch/x86/kvm/x86.c|658| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+		 *   - arch/x86/kvm/x86.c|668| <<kvm_find_user_return_msr>> if (kvm_uret_msrs_list[i] == msr)
+		 *   - arch/x86/kvm/x86.c|682| <<kvm_user_return_msr_cpu_online>> rdmsrl_safe(kvm_uret_msrs_list[i], &value);
+		 *   - arch/x86/kvm/x86.c|696| <<kvm_set_user_return_msr>> err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
+		 */
 		rdmsrl_safe(kvm_uret_msrs_list[i], &value);
 		msrs->values[i].host = value;
 		msrs->values[i].curr = value;
@@ -644,6 +738,15 @@ int kvm_set_user_return_msr(unsigned slot, u64 value, u64 mask)
 	value = (value & mask) | (msrs->values[slot].host & ~mask);
 	if (value == msrs->values[slot].curr)
 		return 0;
+	/*
+	 * 在以下使用kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS]:
+	 *   - arch/x86/kvm/x86.c|265| <<global>> static u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];
+	 *   - arch/x86/kvm/x86.c|630| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+	 *   - arch/x86/kvm/x86.c|658| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+	 *   - arch/x86/kvm/x86.c|668| <<kvm_find_user_return_msr>> if (kvm_uret_msrs_list[i] == msr)
+	 *   - arch/x86/kvm/x86.c|682| <<kvm_user_return_msr_cpu_online>> rdmsrl_safe(kvm_uret_msrs_list[i], &value);
+	 *   - arch/x86/kvm/x86.c|696| <<kvm_set_user_return_msr>> err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
+	 */
 	err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
 	if (err)
 		return 1;
@@ -1603,6 +1706,25 @@ static u64 kvm_get_arch_capabilities(void)
 	 */
 	data |= ARCH_CAP_PSCHANGE_MC_NO;
 
+	/*
+	 * 在以下使用l1tf_vmx_mitigation:
+	 *   - arch/x86/kernel/cpu/bugs.c|2707| <<global>> enum vmx_l1d_flush_state l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+	 *   - arch/x86/kernel/cpu/bugs.c|3039| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO)
+	 *   - arch/x86/kernel/cpu/bugs.c|3042| <<l1tf_show_state>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_EPT_DISABLED ||
+	 *   - arch/x86/kernel/cpu/bugs.c|3043| <<l1tf_show_state>> (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER &&
+	 *   - arch/x86/kernel/cpu/bugs.c|3046| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation]);
+	 *   - arch/x86/kernel/cpu/bugs.c|3050| <<l1tf_show_state>> l1tf_vmx_states[l1tf_vmx_mitigation],
+	 *   - arch/x86/kvm/vmx/vmx.c|297| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+	 *   - arch/x86/kvm/vmx/vmx.c|302| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_EPT_DISABLED;
+	 *   - arch/x86/kvm/vmx/vmx.c|307| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;
+	 *   - arch/x86/kvm/vmx/vmx.c|353| <<vmx_setup_l1d_flush>> l1tf_vmx_mitigation = l1tf;
+	 *   - arch/x86/kvm/vmx/vmx.c|414| <<vmentry_l1d_flush_set>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO) {
+	 *   - arch/x86/kvm/vmx/vmx.c|427| <<vmentry_l1d_flush_get>> if (WARN_ON_ONCE(l1tf_vmx_mitigation >= ARRAY_SIZE(vmentry_l1d_param)))
+	 *   - arch/x86/kvm/vmx/vmx.c|430| <<vmentry_l1d_flush_get>> return sysfs_emit(s, "%s\n", vmentry_l1d_param[l1tf_vmx_mitigation].option);
+	 *   - arch/x86/kvm/vmx/vmx.c|7759| <<vmx_vm_init>> if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER)
+	 *   - arch/x86/kvm/vmx/vmx.c|8690| <<vmx_cleanup_l1d_flush>> l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+	 *   - arch/x86/kvm/x86.c|1664| <<kvm_get_arch_capabilities>> if (l1tf_vmx_mitigation != VMENTER_L1D_FLUSH_NEVER)
+	 */
 	/*
 	 * If we're doing cache flushes (either "always" or "cond")
 	 * we will do one whenever the guest does a vmlaunch/vmresume.
@@ -4948,10 +5070,30 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	vcpu->arch.l1tf_flush_l1d = true;
 
 	if (vcpu->scheduled_out && pmu->version && pmu->event_count) {
 		pmu->need_cleanup = true;
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/kvm/pmu.c|1195| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|5033| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *   - arch/x86/kvm/x86.c|10914| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *
+		 * 处理函数是kvm_pmu_handle_event().
+		 */
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
 
@@ -5085,6 +5227,20 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
+	/*
+	 * 在以下使用sync_pir_to_irr:
+	 *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+	 *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+	 *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *
+	 * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+	 */
 	kvm_x86_call(sync_pir_to_irr)(vcpu);
 
 	return kvm_apic_get_state(vcpu, s);
@@ -7716,6 +7872,16 @@ static int emulator_write_std(struct x86_emulate_ctxt *ctxt, gva_t addr, void *v
 int kvm_write_guest_virt_system(struct kvm_vcpu *vcpu, gva_t addr, void *val,
 				unsigned int bytes, struct x86_exception *exception)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	/* kvm_write_guest_virt_system can pull in tons of pages. */
 	vcpu->arch.l1tf_flush_l1d = true;
 
@@ -9111,6 +9277,16 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 		return handle_emulation_failure(vcpu, emulation_type);
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1tf_flush_l1d:
+	 *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_handle_page_fault>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/nested.c|3743| <<nested_vmx_run>> vmx->vcpu.arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6751| <<vmx_l1d_flush>> flush_l1d = vcpu->arch.l1tf_flush_l1d;
+	 *   - arch/x86/kvm/vmx/vmx.c|6752| <<vmx_l1d_flush>> vcpu->arch.l1tf_flush_l1d = false;
+	 *   - arch/x86/kvm/x86.c|5000| <<kvm_arch_vcpu_load>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|7783| <<kvm_write_guest_virt_system>> vcpu->arch.l1tf_flush_l1d = true;
+	 *   - arch/x86/kvm/x86.c|9177| <<x86_emulate_instruction>> vcpu->arch.l1tf_flush_l1d = true;
+	 */
 	vcpu->arch.l1tf_flush_l1d = true;
 
 	if (!(emulation_type & EMULTYPE_NO_DECODE)) {
@@ -10642,6 +10818,20 @@ static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)
 
 	bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
 
+	/*
+	 * 在以下使用sync_pir_to_irr:
+	 *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+	 *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+	 *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *
+	 * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+	 */
 	kvm_x86_call(sync_pir_to_irr)(vcpu);
 
 	if (irqchip_split(vcpu->kvm))
@@ -10785,8 +10975,27 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		}
 		if (kvm_check_request(KVM_REQ_STEAL_UPDATE, vcpu))
 			record_steal_time(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/kvm/pmu.c|1195| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|233| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|245| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|5033| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *   - arch/x86/kvm/x86.c|10914| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *
+		 * 处理函数是kvm_pmu_handle_event().
+		 */
 		if (kvm_check_request(KVM_REQ_PMU, vcpu))
 			kvm_pmu_handle_event(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMI:
+		 *   - arch/x86/kvm/pmu.c|144| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|8589| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|10916| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+		 *   - arch/x86/kvm/x86.c|11294| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+		 *
+		 * 处理函数是kvm_pmu_deliver_pmi().
+		 */
 		if (kvm_check_request(KVM_REQ_PMI, vcpu))
 			kvm_pmu_deliver_pmi(vcpu);
 #ifdef CONFIG_KVM_SMM
@@ -10925,6 +11134,20 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	 */
 	smp_mb__after_srcu_read_unlock();
 
+	/*
+	 * 在以下使用sync_pir_to_irr:
+	 *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+	 *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+	 *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+	 *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+	 *
+	 * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+	 */
 	/*
 	 * Process pending posted interrupts to handle the case where the
 	 * notification IRQ arrived in the host, or was never sent (because the
@@ -10987,6 +11210,20 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
 
+		/*
+		 * 在以下使用sync_pir_to_irr:
+		 *   - arch/x86/kvm/vmx/main.c|105| <<global>> .sync_pir_to_irr = vmx_sync_pir_to_irr,
+		 *   - arch/x86/include/asm/kvm-x86-ops.h|92| <<KVM_X86_OP_OPTIONAL>> KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
+		 *   - arch/x86/kvm/lapic.c|954| <<apic_has_interrupt_for_ppr>> if (kvm_x86_ops.sync_pir_to_irr)
+		 *   - arch/x86/kvm/lapic.c|955| <<apic_has_interrupt_for_ppr>> highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|8546| <<vmx_hardware_setup>> vt_x86_ops.sync_pir_to_irr = NULL;
+		 *   - arch/x86/kvm/x86.c|5137| <<kvm_vcpu_ioctl_get_lapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+		 *   - arch/x86/kvm/x86.c|10694| <<vcpu_scan_ioapic>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+		 *   - arch/x86/kvm/x86.c|10985| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+		 *   - arch/x86/kvm/x86.c|11040| <<vcpu_enter_guest>> kvm_x86_call(sync_pir_to_irr)(vcpu);
+		 *
+		 * kvm_x86_ops vt_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+		 */
 		if (kvm_lapic_enabled(vcpu))
 			kvm_x86_call(sync_pir_to_irr)(vcpu);
 
@@ -11137,6 +11374,15 @@ static bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)
 		return true;
 #endif
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|144| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8589| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10916| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11294| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * 处理函数是kvm_pmu_deliver_pmi().
+	 */
 	if (kvm_test_request(KVM_REQ_PMI, vcpu))
 		return true;
 
diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c
index 62aa4d66a..5bf8824db 100644
--- a/arch/x86/mm/init.c
+++ b/arch/x86/mm/init.c
@@ -1039,6 +1039,22 @@ unsigned long arch_max_swapfile_size(void)
 
 	pages = generic_max_swapfile_size();
 
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
 		/* Limit the swap file size to MAX_PA/2 for L1TF workaround */
 		unsigned long long l1tf_limit = l1tf_pfn_limit();
diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c
index b8a6ffffb..f96d032a8 100644
--- a/arch/x86/mm/mmap.c
+++ b/arch/x86/mm/mmap.c
@@ -232,6 +232,22 @@ int valid_mmap_phys_addr_range(unsigned long pfn, size_t count)
  */
 bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot)
 {
+	/*
+	 * 在以下使用X86_BUG_L1TF:
+	 *   - arch/x86/include/asm/cpufeatures.h|516| <<global>> #define X86_BUG_L1TF X86_BUG(18)
+	 *   - arch/x86/include/asm/pgtable.h|1733| <<arch_has_pfn_modify_check>> return boot_cpu_has_bug(X86_BUG_L1TF)
+	 *   - arch/x86/kernel/cpu/bugs.c|2747| <<l1tf_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|2792| <<l1tf_cmdline>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 *   - arch/x86/kernel/cpu/bugs.c|3259| <<cpu_show_common>> case X86_BUG_L1TF:
+	 *   - arch/x86/kernel/cpu/bugs.c|3321| <<cpu_show_l1tf>> return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+	 *   - arch/x86/kernel/cpu/common.c|1461| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_L1TF);
+	 *   - arch/x86/kvm/mmu/spte.c|498| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|255| <<vmx_setup_l1d_flush>> if (!boot_cpu_has_bug(X86_BUG_L1TF)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|348| <<vmentry_l1d_flush_set>> if (!boot_cpu_has(X86_BUG_L1TF))
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_vm_init>> if (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {
+	 *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+	 *   - arch/x86/mm/mmap.c|235| <<pfn_modify_allowed>> if (!boot_cpu_has_bug(X86_BUG_L1TF))
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_L1TF))
 		return true;
 	if (!__pte_needs_invert(pgprot_val(prot)))
diff --git a/arch/x86/mm/pti.c b/arch/x86/mm/pti.c
index 5f0d57993..9414b1938 100644
--- a/arch/x86/mm/pti.c
+++ b/arch/x86/mm/pti.c
@@ -57,12 +57,21 @@
 #define	PTI_LEVEL_KERNEL_IMAGE	PTI_CLONE_PTE
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/mm/pti.c|83| <<pti_check_boottime_disable>> pti_print_if_insecure("disabled on XEN PV.");
+ *   - arch/x86/mm/pti.c|90| <<pti_check_boottime_disable>> pti_print_if_insecure("disabled on command line.");
+ */
 static void __init pti_print_if_insecure(const char *reason)
 {
 	if (boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))
 		pr_info("%s\n", reason);
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/pti.c|95| <<pti_check_boottime_disable>> pti_print_if_secure("force enabled on command line.");
+ */
 static void __init pti_print_if_secure(const char *reason)
 {
 	if (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))
@@ -663,6 +672,10 @@ void __init pti_init(void)
  * mapped RO and/or NX.  These changes need to be cloned again to the
  * userspace page-table.
  */
+/*
+ * called by:
+ *   - init/main.c|1473| <<kernel_init>> pti_finalize();
+ */
 void pti_finalize(void)
 {
 	if (!boot_cpu_has(X86_FEATURE_PTI))
diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c
index 6cf881a94..a4185e7b4 100644
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@ -325,6 +325,16 @@ void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	unsigned long flags;
 
 	local_irq_save(flags);
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(NULL, next, tsk);
 	local_irq_restore(flags);
 }
@@ -378,6 +388,10 @@ static unsigned long mm_mangle_tif_spec_bits(struct task_struct *next)
 	return (unsigned long)next->mm | spec_bits;
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/tlb.c|645| <<switch_mm_irqs_off>> cond_mitigation(tsk);
+ */
 static void cond_mitigation(struct task_struct *next)
 {
 	unsigned long prev_mm, next_mm;
@@ -436,6 +450,15 @@ static void cond_mitigation(struct task_struct *next)
 		 * Issue IBPB only if the mm's are different and one or
 		 * both have the IBPB bit set.
 		 */
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (next_mm != prev_mm &&
 		    (next_mm | prev_mm) & LAST_USER_MM_IBPB)
 			indirect_branch_prediction_barrier();
@@ -447,6 +470,15 @@ static void cond_mitigation(struct task_struct *next)
 		 * different context than the user space task which ran
 		 * last on this CPU.
 		 */
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2276| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1563| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5030| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if ((prev_mm & ~LAST_USER_MM_SPEC_MASK) !=
 					(unsigned long)next->mm)
 			indirect_branch_prediction_barrier();
@@ -496,6 +528,16 @@ static inline void cr4_update_pce_mm(struct mm_struct *mm) { }
  * 'cpu_tlbstate.loaded_mm' instead because it does not always keep
  * 'current->active_mm' up to date.
  */
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+ */
 void switch_mm_irqs_off(struct mm_struct *unused, struct mm_struct *next,
 			struct task_struct *tsk)
 {
@@ -782,6 +824,16 @@ static void flush_tlb_func(void *info)
 		 * This should be rare, with native_flush_tlb_multi() skipping
 		 * IPIs to lazy TLB mode CPUs.
 		 */
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+		 */
 		switch_mm_irqs_off(NULL, &init_mm, NULL);
 		return;
 	}
diff --git a/arch/x86/net/bpf_jit_comp.c b/arch/x86/net/bpf_jit_comp.c
index a43fc5af9..e35491c11 100644
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@ -657,6 +657,20 @@ static void emit_indirect_jump(u8 **pprog, int reg, u8 *ip)
 		EMIT_LFENCE();
 		EMIT2(0xFF, 0xE0 + reg);
 	} else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+		/*
+		 * 在以下使用X86_FEATURE_RETPOLINE:
+		 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+		 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+		 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+		 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+		 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+		 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+		 *
+		 * v6.13或以前的在nospec-branch.h有两处
+		 */
 		OPTIMIZER_HIDE_VAR(reg);
 		if (cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))
 			emit_jump(&prog, &__x86_indirect_jump_thunk_array[reg], ip);
diff --git a/drivers/acpi/acpica/acmacros.h b/drivers/acpi/acpica/acmacros.h
index de83dd222..c7d0fe7a8 100644
--- a/drivers/acpi/acpica/acmacros.h
+++ b/drivers/acpi/acpica/acmacros.h
@@ -422,6 +422,13 @@
  * These macros are used for both the debug and non-debug versions of the code.
  */
 #define ACPI_ERROR_NAMESPACE(s, p, e)       acpi_ut_prefixed_namespace_error (AE_INFO, s, p, e);
+/*
+ * 在以下使用ACPI_ERROR_METHOD():
+ *   - drivers/acpi/acpica/psparse.c|529| <<acpi_ps_parse_aml>> ACPI_ERROR_METHOD("Aborting method",
+ *   - drivers/acpi/acpica/uteval.c|68| <<acpi_ut_evaluate_object>> ACPI_ERROR_METHOD("Method execution failed",
+ *   - drivers/acpi/acpica/uteval.c|79| <<acpi_ut_evaluate_object>> ACPI_ERROR_METHOD("No object was returned from",
+ *   - drivers/acpi/acpica/uteval.c|130| <<acpi_ut_evaluate_object>> ACPI_ERROR_METHOD("Return object type is incorrect",
+ */
 #define ACPI_ERROR_METHOD(s, n, p, e)       acpi_ut_method_error (AE_INFO, s, n, p, e);
 #define ACPI_WARN_PREDEFINED(plist)         acpi_ut_predefined_warning plist
 #define ACPI_INFO_PREDEFINED(plist)         acpi_ut_predefined_info plist
diff --git a/drivers/acpi/acpica/uterror.c b/drivers/acpi/acpica/uterror.c
index 918aca7c4..8bc6ead03 100644
--- a/drivers/acpi/acpica/uterror.c
+++ b/drivers/acpi/acpica/uterror.c
@@ -295,6 +295,10 @@ acpi_ut_namespace_error(const char *module_name,
  *
  ******************************************************************************/
 
+/*
+ * 在以下使用acpi_ut_method_error():
+ *   - drivers/acpi/acpica/acmacros.h|425| <<ACPI_ERROR_METHOD>> #define ACPI_ERROR_METHOD(s, n, p, e) acpi_ut_method_error (AE_INFO, s, n, p, e);
+ */
 void
 acpi_ut_method_error(const char *module_name,
 		     u32 line_number,
diff --git a/drivers/acpi/acpica/uteval.c b/drivers/acpi/acpica/uteval.c
index 3e5173d03..4b36ae1a9 100644
--- a/drivers/acpi/acpica/uteval.c
+++ b/drivers/acpi/acpica/uteval.c
@@ -76,6 +76,13 @@ acpi_ut_evaluate_object(struct acpi_namespace_node *prefix_node,
 
 	if (!info->return_object) {
 		if (expected_return_btypes) {
+			/*
+			 * 在以下使用ACPI_ERROR_METHOD():
+			 *   - drivers/acpi/acpica/psparse.c|529| <<acpi_ps_parse_aml>> ACPI_ERROR_METHOD("Aborting method",
+			 *   - drivers/acpi/acpica/uteval.c|68| <<acpi_ut_evaluate_object>> ACPI_ERROR_METHOD("Method execution failed",
+			 *   - drivers/acpi/acpica/uteval.c|79| <<acpi_ut_evaluate_object>> ACPI_ERROR_METHOD("No object was returned from",
+			 *   - drivers/acpi/acpica/uteval.c|130| <<acpi_ut_evaluate_object>> ACPI_ERROR_METHOD("Return object type is incorrect",
+			 */
 			ACPI_ERROR_METHOD("No object was returned from",
 					  prefix_node, path, AE_NOT_EXIST);
 
@@ -127,6 +134,13 @@ acpi_ut_evaluate_object(struct acpi_namespace_node *prefix_node,
 	/* Is the return object one of the expected types? */
 
 	if (!(expected_return_btypes & return_btype)) {
+		/*
+		 * 在以下使用ACPI_ERROR_METHOD():
+		 *   - drivers/acpi/acpica/psparse.c|529| <<acpi_ps_parse_aml>> ACPI_ERROR_METHOD("Aborting method",
+		 *   - drivers/acpi/acpica/uteval.c|68| <<acpi_ut_evaluate_object>> ACPI_ERROR_METHOD("Method execution failed",
+		 *   - drivers/acpi/acpica/uteval.c|79| <<acpi_ut_evaluate_object>> ACPI_ERROR_METHOD("No object was returned from",
+		 *   - drivers/acpi/acpica/uteval.c|130| <<acpi_ut_evaluate_object>> ACPI_ERROR_METHOD("Return object type is incorrect",
+		 */
 		ACPI_ERROR_METHOD("Return object type is incorrect",
 				  prefix_node, path, AE_TYPE);
 
diff --git a/drivers/acpi/scan.c b/drivers/acpi/scan.c
index 9f4efa8f7..7cfea61ac 100644
--- a/drivers/acpi/scan.c
+++ b/drivers/acpi/scan.c
@@ -316,6 +316,10 @@ static void acpi_scan_check_subtree(struct acpi_device *adev)
 	acpi_scan_check_and_detach(adev, (void *)flags);
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|433| <<acpi_generic_hotplug_event>> return acpi_scan_hot_remove(adev);
+ */
 static int acpi_scan_hot_remove(struct acpi_device *device)
 {
 	acpi_handle handle = device->handle;
@@ -415,6 +419,10 @@ static int acpi_scan_bus_check(struct acpi_device *adev)
 	return acpi_scan_rescan_bus(adev);
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|457| <<acpi_device_hotplug>> error = acpi_generic_hotplug_event(adev, src);
+ */
 static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 {
 	switch (type) {
diff --git a/drivers/idle/intel_idle.c b/drivers/idle/intel_idle.c
index 0fdb1d131..cd94a7089 100644
--- a/drivers/idle/intel_idle.c
+++ b/drivers/idle/intel_idle.c
@@ -2089,6 +2089,16 @@ static void state_update_enter_method(struct cpuidle_state *state, int cstate)
 		return;
 	}
 
+	/*
+	 * 在以下使用X86_FEATURE_KERNEL_IBRS:
+	 *   - arch/x86/entry/calling.h|306| <<IBRS_ENTER>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/entry/calling.h|335| <<IBRS_EXIT>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/kernel/cpu/bugs.c|370| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kernel/cpu/bugs.c|2252| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+	 *   - arch/x86/kernel/smpboot.c|1388| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kvm/vmx/vmx.c|7529| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+	 *   - drivers/idle/intel_idle.c|2092| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
 			((state->flags & CPUIDLE_FLAG_IBRS) || ibrs_off)) {
 		/*
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 21ce3e940..33e704697 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -32,6 +32,74 @@
 
 #include "sd.h"
 
+/*
+ * [kworker/u8:4+kacpi_hotplug]
+ *
+ * [0] del_gendisk
+ * [0] sd_remove
+ * [0] __device_release_driver
+ * [0] device_release_driver
+ * [0] bus_remove_device
+ * [0] device_del
+ * [0] __scsi_remove_device
+ * [0] scsi_forget_host
+ * [0] scsi_remove_host
+ * [0] virtscsi_remove
+ * [0] virtio_dev_remove
+ * [0] __device_release_driver
+ * [0] device_release_driver
+ * [0] bus_remove_device
+ * [0] device_del
+ * [0] device_unregister
+ * [0] unregister_virtio_device
+ * [0] virtio_pci_remove
+ * [0] pci_device_remove
+ * [0] __device_release_driver
+ * [0] device_release_driver
+ * [0] pci_stop_bus_device
+ * [0] pci_stop_and_remove_bus_device
+ * [0] disable_slot
+ * [0] acpiphp_disable_and_eject_slot
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * /usr/lib/systemd/systemd-udevd
+ *
+ * [0] wait_on_page_bit_common
+ * [0] wait_on_page_writeback
+ * [0] __filemap_fdatawait_range
+ * [0] filemap_write_and_wait_range
+ * [0] bdev_disk_changed
+ * [0] blkdev_get_whole
+ * [0] blkdev_get_by_dev
+ * [0] blkdev_common_ioctl
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+*
+* dd
+*
+* [0] wait_on_page_bit_common
+* [0] wait_on_page_writeback
+* [0] __filemap_fdatawait_range
+* [0] filemap_write_and_wait_range
+* [0] blkdev_put
+* [0] blkdev_close
+* [0] __fput
+* [0] task_work_run
+* [0] exit_to_user_mode_prepare
+* [0] syscall_exit_to_user_mode
+* [0] do_syscall_64
+* [0] entry_SYSCALL_64_after_hwframe
+ */
+
 #define VIRTIO_SCSI_MEMPOOL_SZ 64
 #define VIRTIO_SCSI_EVENT_LEN 8
 #define VIRTIO_SCSI_VQ_BASE 2
@@ -989,6 +1057,40 @@ static int virtscsi_probe(struct virtio_device *vdev)
 	return err;
 }
 
+/*
+ * [0] del_gendisk
+ * [0] sd_remove
+ * [0] __device_release_driver
+ * [0] device_release_driver
+ * [0] bus_remove_device
+ * [0] device_del
+ * [0] __scsi_remove_device
+ * [0] scsi_forget_host
+ * [0] scsi_remove_host
+ * [0] virtscsi_remove
+ * [0] virtio_dev_remove
+ * [0] __device_release_driver
+ * [0] device_release_driver
+ * [0] bus_remove_device
+ * [0] device_del
+ * [0] device_unregister
+ * [0] unregister_virtio_device
+ * [0] virtio_pci_remove
+ * [0] pci_device_remove
+ * [0] __device_release_driver
+ * [0] device_release_driver
+ * [0] pci_stop_bus_device
+ * [0] pci_stop_and_remove_bus_device
+ * [0] disable_slot
+ * [0] acpiphp_disable_and_eject_slot
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void virtscsi_remove(struct virtio_device *vdev)
 {
 	struct Scsi_Host *shost = virtio_scsi_host(vdev);
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index b9b9e9d40..e82b63e95 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -1650,6 +1650,12 @@ static long vhost_net_set_owner(struct vhost_net *n)
 	int r;
 
 	mutex_lock(&n->dev.mutex);
+	/*
+	 * 在以下调用vhost_dev_has_owner():
+	 *   - drivers/vhost/net.c|1653| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+	 *   - drivers/vhost/vhost.c|868| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+	 *   - drivers/vhost/vhost.c|937| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+	 */
 	if (vhost_dev_has_owner(&n->dev)) {
 		r = -EBUSY;
 		goto out;
@@ -1657,6 +1663,11 @@ static long vhost_net_set_owner(struct vhost_net *n)
 	r = vhost_net_set_ubuf_info(n);
 	if (r)
 		goto out;
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1660| <<vhost_net_set_owner>> r = vhost_dev_set_owner(&n->dev);
+	 *   - drivers/vhost/vhost.c|2149| <<vhost_dev_ioctl(VHOST_SET_OWNER)>> r = vhost_dev_set_owner(d);
+	 */
 	r = vhost_dev_set_owner(&n->dev);
 	if (r)
 		vhost_net_clear_ubuf_info(n);
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 718fa4e0b..4ab0d915f 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -39,6 +39,32 @@
 
 #include "vhost.h"
 
+/*
+ * Legacy的方式.
+ *
+ * VHOST_SET_OWNER
+ * -> vhost_dev_set_owner()
+ *    -> vhost_worker_create()
+ *    -> for (i = 0; i < dev->nvqs; i++)
+ *         __vhost_vq_attach_worker(dev->vqs[i], worker)
+ *
+ * 新的multiqueue/worker的方式.
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_NEW_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_NEW_WORKER
+ *          -> vhost_new_worker()
+ *             -> vhost_worker_create()
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_ATTACH_VRING_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_ATTACH_VRING_WORKER
+ *          -> vhost_vq_attach_worker()
+ *             -> __vhost_vq_attach_worker()
+ */
+
 #define VHOST_SCSI_VERSION  "v0.1"
 #define VHOST_SCSI_NAMELEN 256
 #define VHOST_SCSI_MAX_CDB_SIZE 32
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 63612faea..0c441b509 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -581,6 +581,12 @@ long vhost_dev_check_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_check_owner);
 
+/*
+ * 在以下调用vhost_dev_has_owner():
+ *   - drivers/vhost/net.c|1653| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+ *   - drivers/vhost/vhost.c|868| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+ *   - drivers/vhost/vhost.c|937| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+ */
 /* Caller should have device mutex */
 bool vhost_dev_has_owner(struct vhost_dev *dev)
 {
@@ -588,6 +594,10 @@ bool vhost_dev_has_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_has_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|942| <<vhost_dev_set_owner>> vhost_attach_mm(dev);
+ */
 static void vhost_attach_mm(struct vhost_dev *dev)
 {
 	/* No owner, become one */
@@ -605,6 +615,11 @@ static void vhost_attach_mm(struct vhost_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|975| <<vhost_dev_set_owner>> vhost_detach_mm(dev);
+ *   - drivers/vhost/vhost.c|1067| <<vhost_dev_cleanup>> vhost_detach_mm(dev);
+ */
 static void vhost_detach_mm(struct vhost_dev *dev)
 {
 	if (!dev->mm)
@@ -618,6 +633,11 @@ static void vhost_detach_mm(struct vhost_dev *dev)
 	dev->mm = NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|648| <<vhost_workers_free>> vhost_worker_destroy(dev, worker);
+ *   - drivers/vhost/vhost.c|829| <<vhost_free_worker>> vhost_worker_destroy(dev, worker);
+ */
 static void vhost_worker_destroy(struct vhost_dev *dev,
 				 struct vhost_worker *worker)
 {
@@ -630,6 +650,10 @@ static void vhost_worker_destroy(struct vhost_dev *dev,
 	kfree(worker);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1066| <<vhost_dev_cleanup>> vhost_workers_free(dev);
+ */
 static void vhost_workers_free(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
@@ -649,6 +673,11 @@ static void vhost_workers_free(struct vhost_dev *dev)
 	xa_destroy(&dev->worker_xa);
 }
 
+/*
+ * 在以下两个地方调用vhost_worker_create():
+ *   - drivers/vhost/vhost.c|787| <<vhost_new_worker>> worker = vhost_worker_create(dev);
+ *   - drivers/vhost/vhost.c|945| <<vhost_dev_set_owner>> worker = vhost_worker_create(dev);
+ */
 static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
@@ -690,6 +719,11 @@ static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 	return NULL;
 }
 
+/*
+ * 在以下使用__vhost_vq_attach_worker():
+ *   - drivers/vhost/vhost.c|782| <<vhost_vq_attach_worker>> __vhost_vq_attach_worker(vq, worker);
+ *   - drivers/vhost/vhost.c|967| <<vhost_dev_set_owner>> __vhost_vq_attach_worker(dev->vqs[i], worker);
+ */
 /* Caller must have device mutex */
 static void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 				     struct vhost_worker *worker)
@@ -759,6 +793,10 @@ static void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 	mutex_unlock(&old_worker->mutex);
 }
 
+/*
+ * 处理VHOST_ATTACH_VRING_WORKER:
+ *   - drivers/vhost/vhost.c|905| <<vhost_worker_ioctl(VHOST_ATTACH_VRING_WORKER)>> ret = vhost_vq_attach_worker(vq, &ring_worker);
+ */
  /* Caller must have device mutex */
 static int vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 				  struct vhost_vring_worker *info)
@@ -774,16 +812,30 @@ static int vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 	if (!worker || worker->id != info->worker_id)
 		return -ENODEV;
 
+	/*
+	 * 在以下使用__vhost_vq_attach_worker():
+	 *   - drivers/vhost/vhost.c|782| <<vhost_vq_attach_worker>> __vhost_vq_attach_worker(vq, worker);
+	 *   - drivers/vhost/vhost.c|967| <<vhost_dev_set_owner>> __vhost_vq_attach_worker(dev->vqs[i], worker);
+	 */
 	__vhost_vq_attach_worker(vq, worker);
 	return 0;
 }
 
+/*
+ * 在以下调用vhost_new_worker():
+ *   - drivers/vhost/vhost.c|878| <<vhost_worker_ioctl(VHOST_NEW_WORKER)>> ret = vhost_new_worker(dev, &state);
+ */
 /* Caller must have device mutex */
 static int vhost_new_worker(struct vhost_dev *dev,
 			    struct vhost_worker_state *info)
 {
 	struct vhost_worker *worker;
 
+	/*
+	 * 在以下两个地方调用vhost_worker_create():
+	 *   - drivers/vhost/vhost.c|787| <<vhost_new_worker>> worker = vhost_worker_create(dev);
+	 *   - drivers/vhost/vhost.c|945| <<vhost_dev_set_owner>> worker = vhost_worker_create(dev);
+	 */
 	worker = vhost_worker_create(dev);
 	if (!worker)
 		return -ENOMEM;
@@ -792,6 +844,10 @@ static int vhost_new_worker(struct vhost_dev *dev,
 	return 0;
 }
 
+/*
+ * 处理VHOST_FREE_WORKER:
+ *   - drivers/vhost/vhost.c|885| <<vhost_worker_ioctl(VHOST_FREE_WORKER)>> return vhost_free_worker(dev, &state);
+ */
 /* Caller must have device mutex */
 static int vhost_free_worker(struct vhost_dev *dev,
 			     struct vhost_worker_state *info)
@@ -820,6 +876,11 @@ static int vhost_free_worker(struct vhost_dev *dev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|894| <<vhost_worker_ioctl>> ret = vhost_get_vq_from_user(dev, argp, &vq, &idx);
+ *   - drivers/vhost/vhost.c|1984| <<vhost_vring_ioctl>> r = vhost_get_vq_from_user(d, argp, &vq, &idx);
+ */
 static int vhost_get_vq_from_user(struct vhost_dev *dev, void __user *argp,
 				  struct vhost_virtqueue **vq, u32 *id)
 {
@@ -841,6 +902,14 @@ static int vhost_get_vq_from_user(struct vhost_dev *dev, void __user *argp,
 	return 0;
 }
 
+/*
+ * 处理四种:
+ * - VHOST_NEW_WORKER
+ * - VHOST_FREE_WORKER
+ * - VHOST_ATTACH_VRING_WORKER
+ * - VHOST_GET_VRING_WORKER
+ * drivers/vhost/scsi.c|2093| <<vhost_scsi_ioctl>> r = vhost_worker_ioctl(&vs->dev, ioctl, argp);
+ */
 /* Caller must have device mutex */
 long vhost_worker_ioctl(struct vhost_dev *dev, unsigned int ioctl,
 			void __user *argp)
@@ -855,6 +924,12 @@ long vhost_worker_ioctl(struct vhost_dev *dev, unsigned int ioctl,
 	if (!dev->use_worker)
 		return -EINVAL;
 
+	/*
+	 * 在以下调用vhost_dev_has_owner():
+	 *   - drivers/vhost/net.c|1653| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+	 *   - drivers/vhost/vhost.c|868| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+	 *   - drivers/vhost/vhost.c|937| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+	 */
 	if (!vhost_dev_has_owner(dev))
 		return -EINVAL;
 
@@ -917,12 +992,23 @@ long vhost_worker_ioctl(struct vhost_dev *dev, unsigned int ioctl,
 }
 EXPORT_SYMBOL_GPL(vhost_worker_ioctl);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1660| <<vhost_net_set_owner>> r = vhost_dev_set_owner(&n->dev);
+ *   - drivers/vhost/vhost.c|2149| <<vhost_dev_ioctl(VHOST_SET_OWNER)>> r = vhost_dev_set_owner(d);
+ */
 /* Caller should have device mutex */
 long vhost_dev_set_owner(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
 	int err, i;
 
+	/*
+	 * 在以下调用vhost_dev_has_owner():
+	 *   - drivers/vhost/net.c|1653| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+	 *   - drivers/vhost/vhost.c|868| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+	 *   - drivers/vhost/vhost.c|937| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+	 */
 	/* Is there an owner already? */
 	if (vhost_dev_has_owner(dev)) {
 		err = -EBUSY;
@@ -942,12 +1028,22 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 		 * below since we don't have to worry about vsock queueing
 		 * while we free the worker.
 		 */
+		/*
+		 * 在以下两个地方调用vhost_worker_create():
+		 *   - drivers/vhost/vhost.c|787| <<vhost_new_worker>> worker = vhost_worker_create(dev);
+		 *   - drivers/vhost/vhost.c|945| <<vhost_dev_set_owner>> worker = vhost_worker_create(dev);
+		 */
 		worker = vhost_worker_create(dev);
 		if (!worker) {
 			err = -ENOMEM;
 			goto err_worker;
 		}
 
+		/*
+		 * 在以下使用__vhost_vq_attach_worker():
+		 *   - drivers/vhost/vhost.c|782| <<vhost_vq_attach_worker>> __vhost_vq_attach_worker(vq, worker);
+		 *   - drivers/vhost/vhost.c|967| <<vhost_dev_set_owner>> __vhost_vq_attach_worker(dev->vqs[i], worker);
+		 */
 		for (i = 0; i < dev->nvqs; i++)
 			__vhost_vq_attach_worker(dev->vqs[i], worker);
 	}
@@ -2131,6 +2227,11 @@ long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 
 	/* If you are not the owner, you can become one */
 	if (ioctl == VHOST_SET_OWNER) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1660| <<vhost_net_set_owner>> r = vhost_dev_set_owner(&n->dev);
+		 *   - drivers/vhost/vhost.c|2149| <<vhost_dev_ioctl(VHOST_SET_OWNER)>> r = vhost_dev_set_owner(d);
+		 */
 		r = vhost_dev_set_owner(d);
 		goto done;
 	}
@@ -2304,6 +2405,10 @@ static int log_used(struct vhost_virtqueue *vq, u64 used_offset, u64 len)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1222| <<handle_rx>> vhost_log_write(vq, vq_log, log, vhost_len, vq->iov, in);
+ */
 int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 		    unsigned int log_num, u64 len, struct iovec *iov, int count)
 {
diff --git a/include/net/tc_wrapper.h b/include/net/tc_wrapper.h
index ffe58a025..985729c0e 100644
--- a/include/net/tc_wrapper.h
+++ b/include/net/tc_wrapper.h
@@ -202,6 +202,20 @@ static inline int tc_classify(struct sk_buff *skb, const struct tcf_proto *tp,
 static inline void tc_wrapper_init(void)
 {
 #ifdef CONFIG_X86
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
 		static_branch_enable(&tc_skip_wrapper);
 #endif
diff --git a/kernel/cpu.c b/kernel/cpu.c
index 07455d253..efdc8cb33 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -606,6 +606,16 @@ enum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;
 static unsigned int cpu_smt_max_threads __ro_after_init;
 unsigned int cpu_smt_num_threads __read_mostly = UINT_MAX;
 
+/*
+ * 在以下调用cpu_smt_disable():
+ *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+ *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+ */
 void __init cpu_smt_disable(bool force)
 {
 	if (!cpu_smt_possible())
@@ -649,6 +659,16 @@ void __init cpu_smt_set_num_threads(unsigned int num_threads,
 
 static int __init smt_cmdline_disable(char *str)
 {
+	/*
+	 * 在以下调用cpu_smt_disable():
+	 *   - arch/x86/kernel/cpu/bugs.c|498| <<mds_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|599| <<taa_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|696| <<mmio_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|1388| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2805| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+	 *   - arch/x86/kernel/cpu/bugs.c|2808| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+	 *   - kernel/cpu.c|652| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+	 */
 	cpu_smt_disable(str && !strcmp(str, "force"));
 	return 0;
 }
@@ -3209,6 +3229,9 @@ static int __init mitigations_parse_cmdline(char *arg)
 	return 0;
 }
 
+/*
+ * 特别多调用
+ */
 /* mitigations=off */
 bool cpu_mitigations_off(void)
 {
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 823aa0824..7c3226272 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -5459,6 +5459,19 @@ static void put_event(struct perf_event *event)
  * object, it will not preserve its functionality. Once the last 'user'
  * gives up the object, we'll destroy the thing.
  */
+/*
+ * 在以下使用perf_event_release_kernel():
+ *   - arch/arm64/kvm/pmu-emul.c|208| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/riscv/kvm/vcpu_pmu.c|82| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1058| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1060| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+ *   - arch/x86/kvm/pmu.c|383| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|198| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+ *   - kernel/events/core.c|5589| <<perf_release>> perf_event_release_kernel(file->private_data);
+ *   - kernel/events/hw_breakpoint.c|829| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+ *   - kernel/watchdog_perf.c|233| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+ *   - kernel/watchdog_perf.c|300| <<watchdog_hardlockup_probe>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+ */
 int perf_event_release_kernel(struct perf_event *event)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -5919,6 +5932,10 @@ static void perf_event_for_each(struct perf_event *event,
 		perf_event_for_each_child(sibling, func);
 }
 
+/*
+ * 在以下使用__perf_event_period():
+ *   - kernel/events/core.c|5982| <<_perf_event_period>> event_function_call(event, __perf_event_period, &value);
+ */
 static void __perf_event_period(struct perf_event *event,
 				struct perf_cpu_context *cpuctx,
 				struct perf_event_context *ctx,
@@ -5927,6 +5944,11 @@ static void __perf_event_period(struct perf_event *event,
 	u64 value = *((u64 *)info);
 	bool active;
 
+	/*
+	 * struct perf_event *event:
+	 * -> struct perf_event_attr attr;
+	 *    -> freq           :  1, // use freq, not period
+	 */
 	if (event->attr.freq) {
 		event->attr.sample_freq = value;
 	} else {
@@ -5961,6 +5983,11 @@ static int perf_event_check_period(struct perf_event *event, u64 value)
 	return event->pmu->check_period(event, value);
 }
 
+/*
+ * 在以下调用_perf_event_period():
+ *   - kernel/events/core.c|5993| <<perf_event_period>> ret = _perf_event_period(event, value);
+ *   - kernel/events/core.c|6039| <<_perf_ioctl(PERF_EVENT_IOC_PERIOD)>> return _perf_event_period(event, value);
+ */
 static int _perf_event_period(struct perf_event *event, u64 value)
 {
 	if (!is_sampling_event(event))
@@ -5984,6 +6011,15 @@ static int _perf_event_period(struct perf_event *event, u64 value)
 	return 0;
 }
 
+/*
+ * 在以下调用perf_event_period():
+ *   - arch/riscv/kvm/vcpu_pmu.c|542| <<kvm_riscv_vcpu_pmu_ctr_start>> perf_event_period(pmc->perf_event,
+ *                kvm_pmu_get_sample_period(pmc));
+ *   - arch/x86/kvm/pmu.c|263| <<pmc_resume_counter>> if (is_sampling_event(pmc->perf_event) &&
+ *                perf_event_period(pmc->perf_event, get_sample_period(pmc, pmc->counter)))
+ *   - arch/x86/kvm/pmu.c|302| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event,
+ *                get_sample_period(pmc, pmc->counter));
+ */
 int perf_event_period(struct perf_event *event, u64 value)
 {
 	struct perf_event_context *ctx;
@@ -12246,6 +12282,17 @@ static void account_event(struct perf_event *event)
 /*
  * Allocate and initialize an event structure
  */
+/*
+ * 在以下使用perf_event_alloc():
+ *   - kernel/events/core.c|12944| <<SYSCALL_DEFINE5(perf_event_open)>>
+ *            event = perf_event_alloc(&attr, cpu, task, group_leader, NULL, NULL, NULL, cgroup_fd);
+ *   - kernel/events/core.c|13279| <<perf_event_create_kernel_counter>>
+ *            event = perf_event_alloc(attr, cpu, task, NULL, NULL, overflow_handler, context, -1);
+ *   - kernel/events/core.c|13783| <<inherit_event>>
+ *            child_event = perf_event_alloc(&parent_event->attr,
+ *                    parent_event->cpu, child, group_leader, parent_event, NULL, NULL, -1);
+ *   - security/security.c|5915| <<security_perf_event_alloc>> rc = call_int_hook(perf_event_alloc, event);
+ */
 static struct perf_event *
 perf_event_alloc(struct perf_event_attr *attr, int cpu,
 		 struct task_struct *task,
@@ -12905,6 +12952,17 @@ SYSCALL_DEFINE5(perf_event_open,
 	if (flags & PERF_FLAG_PID_CGROUP)
 		cgroup_fd = pid;
 
+	/*
+	 * 在以下使用perf_event_alloc():
+	 *   - kernel/events/core.c|12944| <<SYSCALL_DEFINE5(perf_event_open)>>
+	 *            event = perf_event_alloc(&attr, cpu, task, group_leader, NULL, NULL, NULL, cgroup_fd);
+	 *   - kernel/events/core.c|13279| <<perf_event_create_kernel_counter>>
+	 *            event = perf_event_alloc(attr, cpu, task, NULL, NULL, overflow_handler, context, -1);
+	 *   - kernel/events/core.c|13783| <<inherit_event>>
+	 *            child_event = perf_event_alloc(&parent_event->attr,
+	 *                    parent_event->cpu, child, group_leader, parent_event, NULL, NULL, -1);
+	 *   - security/security.c|5915| <<security_perf_event_alloc>> rc = call_int_hook(perf_event_alloc, event);
+	 */
 	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
 				 NULL, NULL, cgroup_fd);
 	if (IS_ERR(event)) {
@@ -13196,6 +13254,31 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|741| <<kvm_pmu_create_perf_event>>
+ *      event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+ *   - arch/riscv/kvm/vcpu_pmu.c|328| <<kvm_pmu_create_perf_event>>
+ *      event = perf_event_create_kernel_counter(attr, -1, current, kvm_riscv_pmu_overflow, pmc);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|969| <<measure_residency_fn>>
+ *      miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|974| <<measure_residency_fn>>
+ *      hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+ *   - arch/x86/kvm/pmu.c|227| <<pmc_reprogram_counter>>
+ *      event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|243| <<intel_pmu_create_guest_lbr_event>>
+ *      event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+ *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>>
+ *      return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+ *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>>
+ *      bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+ *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>>
+ *      return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+ *   - kernel/watchdog_perf.c|135| <<hardlockup_detector_event_create>>
+ *      evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+ *   - kernel/watchdog_perf.c|140| <<hardlockup_detector_event_create>>
+ *      evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
@@ -13215,6 +13298,17 @@ perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 	if (attr->aux_output || attr->aux_action)
 		return ERR_PTR(-EINVAL);
 
+	/*
+	 * 在以下使用perf_event_alloc():
+	 *   - kernel/events/core.c|12944| <<SYSCALL_DEFINE5(perf_event_open)>>
+	 *            event = perf_event_alloc(&attr, cpu, task, group_leader, NULL, NULL, NULL, cgroup_fd);
+	 *   - kernel/events/core.c|13279| <<perf_event_create_kernel_counter>>
+	 *            event = perf_event_alloc(attr, cpu, task, NULL, NULL, overflow_handler, context, -1);
+	 *   - kernel/events/core.c|13783| <<inherit_event>>
+	 *            child_event = perf_event_alloc(&parent_event->attr,
+	 *                    parent_event->cpu, child, group_leader, parent_event, NULL, NULL, -1);
+	 *   - security/security.c|5915| <<security_perf_event_alloc>> rc = call_int_hook(perf_event_alloc, event);
+	 */
 	event = perf_event_alloc(attr, cpu, task, NULL, NULL,
 				 overflow_handler, context, -1);
 	if (IS_ERR(event)) {
@@ -13719,6 +13813,17 @@ inherit_event(struct perf_event *parent_event,
 	if (parent_event->parent)
 		parent_event = parent_event->parent;
 
+	/*
+	 * 在以下使用perf_event_alloc():
+	 *   - kernel/events/core.c|12944| <<SYSCALL_DEFINE5(perf_event_open)>>
+	 *            event = perf_event_alloc(&attr, cpu, task, group_leader, NULL, NULL, NULL, cgroup_fd);
+	 *   - kernel/events/core.c|13279| <<perf_event_create_kernel_counter>>
+	 *            event = perf_event_alloc(attr, cpu, task, NULL, NULL, overflow_handler, context, -1);
+	 *   - kernel/events/core.c|13783| <<inherit_event>>
+	 *            child_event = perf_event_alloc(&parent_event->attr,
+	 *                    parent_event->cpu, child, group_leader, parent_event, NULL, NULL, -1);
+	 *   - security/security.c|5915| <<security_perf_event_alloc>> rc = call_int_hook(perf_event_alloc, event);
+	 */
 	child_event = perf_event_alloc(&parent_event->attr,
 					   parent_event->cpu,
 					   child,
diff --git a/kernel/events/hw_breakpoint.c b/kernel/events/hw_breakpoint.c
index bc4a61029..a746454bc 100644
--- a/kernel/events/hw_breakpoint.c
+++ b/kernel/events/hw_breakpoint.c
@@ -737,6 +737,18 @@ int register_perf_hw_breakpoint(struct perf_event *bp)
  * @context: context data could be used in the triggered callback
  * @tsk: pointer to 'task_struct' of the process to which the address belongs
  */
+/*
+ * 在以下使用register_user_hw_breakpoint():
+ *   - arch/arm/kernel/ptrace.c|427| <<ptrace_hbp_create>> return register_user_hw_breakpoint(&attr, ptrace_hbptriggered, NULL, tsk);
+ *   - arch/arm64/kernel/ptrace.c|315| <<ptrace_hbp_create>> bp = register_user_hw_breakpoint(&attr, ptrace_hbptriggered, NULL, tsk);
+ *   - arch/loongarch/kernel/ptrace.c|482| <<ptrace_hbp_create>> bp = register_user_hw_breakpoint(&attr, ptrace_hbptriggered, NULL, tsk);
+ *   - arch/loongarch/kernel/ptrace.c|1050| <<set_single_step>> bp = register_user_hw_breakpoint(&attr, ptrace_triggered, NULL, tsk);
+ *   - arch/powerpc/kernel/ptrace/ptrace-noadv.c|154| <<ptrace_set_debugreg>> thread->ptrace_bps[0] = bp = register_user_hw_breakpoint(&attr, ptrace_triggered, NULL, task);
+ *   - arch/powerpc/kernel/ptrace/ptrace-noadv.c|243| <<ppc_set_hwdebug>> bp = register_user_hw_breakpoint(&attr, ptrace_triggered, NULL, child);
+ *   - arch/sh/kernel/ptrace_32.c|89| <<set_single_step>> bp = register_user_hw_breakpoint(&attr, ptrace_triggered, NULL, tsk);
+ *   - arch/x86/kernel/ptrace.c|540| <<ptrace_register_breakpoint>> return register_user_hw_breakpoint(&attr, ptrace_triggered, NULL, tsk);
+ *   - arch/xtensa/kernel/ptrace.c|398| <<ptrace_hbp_create>> return register_user_hw_breakpoint(&attr, ptrace_hbptriggered, NULL, tsk);
+ */
 struct perf_event *
 register_user_hw_breakpoint(struct perf_event_attr *attr,
 			    perf_overflow_handler_t triggered,
diff --git a/kernel/kthread.c b/kernel/kthread.c
index 5dc5b0d72..8bf33b737 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -1618,6 +1618,16 @@ void kthread_use_mm(struct mm_struct *mm)
 	tsk->active_mm = mm;
 	tsk->mm = mm;
 	membarrier_update_current_mm(mm);
+	/*
+	 * 在以下调用switch_mm_irqs_off():
+	 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+	 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+	 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+	 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+	 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+	 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+	 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+	 */
 	switch_mm_irqs_off(active_mm, mm, tsk);
 	local_irq_enable();
 	task_unlock(tsk);
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 042351c7a..4bf21060b 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -5359,6 +5359,16 @@ context_switch(struct rq *rq, struct task_struct *prev,
 		 * case 'prev->active_mm == next->mm' through
 		 * finish_task_switch()'s mmdrop().
 		 */
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+		 */
 		switch_mm_irqs_off(prev->active_mm, next->mm, next);
 		lru_gen_use_mm(next->mm);
 
@@ -7955,6 +7965,16 @@ static void sched_force_init_mm(void)
 		mmgrab_lazy_tlb(&init_mm);
 		local_irq_disable();
 		current->active_mm = &init_mm;
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1838| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1864| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|785| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1621| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5362| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *   - kernel/sched/core.c|7958| <<sched_force_init_mm>> switch_mm_irqs_off(mm, &init_mm, current);
+		 */
 		switch_mm_irqs_off(mm, &init_mm, current);
 		local_irq_enable();
 		finish_arch_post_lock_switch();
diff --git a/kernel/watchdog_perf.c b/kernel/watchdog_perf.c
index 59c1d86a7..21bba11b4 100644
--- a/kernel/watchdog_perf.c
+++ b/kernel/watchdog_perf.c
@@ -131,6 +131,31 @@ static int hardlockup_detector_event_create(void)
 	wd_attr = &wd_hw_attr;
 	wd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|741| <<kvm_pmu_create_perf_event>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+	 *   - arch/riscv/kvm/vcpu_pmu.c|328| <<kvm_pmu_create_perf_event>>
+	 *      event = perf_event_create_kernel_counter(attr, -1, current, kvm_riscv_pmu_overflow, pmc);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|969| <<measure_residency_fn>>
+	 *      miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|974| <<measure_residency_fn>>
+	 *      hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu, NULL, NULL, NULL);
+	 *   - arch/x86/kvm/pmu.c|227| <<pmc_reprogram_counter>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|243| <<intel_pmu_create_guest_lbr_event>>
+	 *      event = perf_event_create_kernel_counter(&attr, -1, current, NULL, NULL);
+	 *   - kernel/events/hw_breakpoint.c|746| <<register_user_hw_breakpoint>>
+	 *      return perf_event_create_kernel_counter(attr, -1, tsk, triggered, context);
+	 *   - kernel/events/hw_breakpoint.c|856| <<register_wide_hw_breakpoint>>
+	 *      bp = perf_event_create_kernel_counter(attr, cpu, NULL, triggered, context);
+	 *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>>
+	 *      return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+	 *   - kernel/watchdog_perf.c|135| <<hardlockup_detector_event_create>>
+	 *      evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 *   - kernel/watchdog_perf.c|140| <<hardlockup_detector_event_create>>
+	 *      evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+	 */
 	/* Try to register using hardware perf events */
 	evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
 					       watchdog_overflow_callback, NULL);
diff --git a/net/netfilter/nf_tables_core.c b/net/netfilter/nf_tables_core.c
index 75598520b..079c7faec 100644
--- a/net/netfilter/nf_tables_core.c
+++ b/net/netfilter/nf_tables_core.c
@@ -32,6 +32,20 @@ static bool nf_skip_indirect_calls(void)
 
 static void __init nf_skip_indirect_calls_enable(void)
 {
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|468| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE,
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1677| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1799| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+	 *   - arch/x86/kernel/cpu/bugs.c|2863| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *
+	 * v6.13或以前的在nospec-branch.h有两处
+	 */
 	if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
 		static_branch_enable(&nf_tables_skip_direct_calls);
 }
diff --git a/tools/testing/selftests/kvm/x86/pmu_counters_test.c b/tools/testing/selftests/kvm/x86/pmu_counters_test.c
index 698cb3698..f664e5fa3 100644
--- a/tools/testing/selftests/kvm/x86/pmu_counters_test.c
+++ b/tools/testing/selftests/kvm/x86/pmu_counters_test.c
@@ -7,6 +7,46 @@
 #include "pmu.h"
 #include "processor.h"
 
+/*
+ * 旧的注释.
+ * ------------------------------
+ *
+ * 核心函数是test_intel_counters(). 用下面的循环, 对不同的PMU version都进行了测试.
+ * perf_caps目前就两个: 是否支持PMU_CAP_FW_WRITES, 也就是用的legacy寄存器的值还是新的.
+ *
+ * 1075         for (v = 0; v <= max_pmu_version; v++) {
+ * 1076                 for (i = 0; i < ARRAY_SIZE(perf_caps); i++) {
+ * ... ...
+ * 甚至还用到了更多的循环, 测试某些arch even支持/不支持是什么样子.
+ *                          for (j = 0; j <= nr_arch_events + 1; j++) {
+ *
+ *
+ * ---> 对arch event测试的基本思想(不管支持不支持):
+ *
+ * 根据参数提供的PMU虚拟化的配置创建VM (支持的event数目)
+ * 然后在VM上测试: 对每一个arch event测试 (没有host的assert!!!):
+ *
+ * guest_test_arch_event(idx)只测试idx指定的事件
+ * 管函数中的intel_event_to_feature[]数组定义了很多要测试的事件
+136
+137 在gp counter的每一个counter和fixed(如果支持)测试参数idx指定的event (下面的要跑好多次):
+138 1. 先把counter清空0
+139 2. GUEST_MEASURE_EVENT(): 激活PMC, 运行一些代码(主要是loop 10次, 还有flush/move), disable PMC
+140 3. guest_assert_event_count(): 读取pmc的值, 确定和预期的counter是否相等
+141
+142
+143
+144 ---> 对gp counter测试的基本思想: 主要测试能不能都写, 是否有期望的值(应该不测试增长), 是否有期望的exception(UD)
+145
+146 测试MSR_CORE_PERF_GLOBAL_CTRL
+147 通过对GP的PMCs进行读写测试: for (i = 0; i < nr_possible_counters; i++)
+148 判断是否成功
+149 判断期望的fail是否完成
+150
+151
+152 ---> 对fixed的测试差不多, 除了测试能不能读写, 还增加了判断counter是否增长(不是0): GUEST_ASSERT_NE(val, 0);
+ */
+
 /* Number of iterations of the loop for the guest measurement payload. */
 #define NUM_LOOPS			10
 
@@ -98,6 +138,10 @@ static uint8_t guest_get_pmu_version(void)
  * Sanity check that in all cases, the event doesn't count when it's disabled,
  * and that KVM correctly emulates the write of an arbitrary value.
  */
+/*
+ * 在以下使用guest_assert_event_count():
+ *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|186| <<GUEST_TEST_EVENT>> guest_assert_event_count(_idx, _event, _pmc, _pmc_msr); \
+ */
 static void guest_assert_event_count(uint8_t idx,
 				     struct kvm_x86_pmu_feature event,
 				     uint32_t pmc, uint32_t pmc_msr)
@@ -172,6 +216,13 @@ do {										\
 	);									\
 } while (0)
 
+/*
+ * 在以下使用GUEST_TEST_EVENT():
+ *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|193| <<__guest_test_arch_event>>
+ *        GUEST_TEST_EVENT(idx, event, pmc, pmc_msr, ctrl_msr, ctrl_msr_value, "");
+ *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|196| <<__guest_test_arch_event>>
+ *        GUEST_TEST_EVENT(idx, event, pmc, pmc_msr, ctrl_msr, ctrl_msr_value, KVM_FEP);
+ */
 #define GUEST_TEST_EVENT(_idx, _event, _pmc, _pmc_msr, _ctrl_msr, _value, FEP)	\
 do {										\
 	wrmsr(pmc_msr, 0);							\
@@ -190,6 +241,13 @@ static void __guest_test_arch_event(uint8_t idx, struct kvm_x86_pmu_feature even
 				    uint32_t pmc, uint32_t pmc_msr,
 				    uint32_t ctrl_msr, uint64_t ctrl_msr_value)
 {
+	/*
+	 * 在以下使用GUEST_TEST_EVENT():
+	 *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|193| <<__guest_test_arch_event>>
+	 *        GUEST_TEST_EVENT(idx, event, pmc, pmc_msr, ctrl_msr, ctrl_msr_value, "");
+	 *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|196| <<__guest_test_arch_event>>
+	 *        GUEST_TEST_EVENT(idx, event, pmc, pmc_msr, ctrl_msr, ctrl_msr_value, KVM_FEP);
+	 */
 	GUEST_TEST_EVENT(idx, event, pmc, pmc_msr, ctrl_msr, ctrl_msr_value, "");
 
 	if (is_forced_emulation_enabled)
@@ -335,6 +393,13 @@ __GUEST_ASSERT(expect_gp ? vector == GP_VECTOR : !vector,			\
 		       "Expected " #insn "(0x%x) to yield 0x%lx, got 0x%lx",	\
 		       msr, expected_val, val);
 
+/*
+ * 在以下调用guest_test_rdpmc():
+ *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|408| <<guest_rd_wr_counters>>
+ *        guest_test_rdpmc(rdpmc_idx, expect_success, expected_val);
+ *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|417| <<guest_rd_wr_counters>>
+ *        guest_test_rdpmc(rdpmc_idx, false, -1ull);
+ */
 static void guest_test_rdpmc(uint32_t rdpmc_idx, bool expect_success,
 			     uint64_t expected_val)
 {
@@ -355,6 +420,14 @@ static void guest_test_rdpmc(uint32_t rdpmc_idx, bool expect_success,
 		GUEST_ASSERT_PMC_VALUE(RDPMC, rdpmc_idx, val, expected_val);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|456| <<guest_test_gp_counters>>
+ *        guest_rd_wr_counters(base_msr, MAX_NR_GP_COUNTERS, nr_gp_counters, 0);
+ *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|494| <<guest_test_fixed_counters>>
+ *        guest_rd_wr_counters(MSR_CORE_PERF_FIXED_CTR0, MAX_NR_FIXED_COUNTERS, nr_fixed_counters,
+ *                             supported_bitmask);
+ */
 static void guest_rd_wr_counters(uint32_t base_msr, uint8_t nr_possible_counters,
 				 uint8_t nr_counters, uint32_t or_mask)
 {
@@ -405,6 +478,13 @@ static void guest_rd_wr_counters(uint32_t base_msr, uint8_t nr_possible_counters
 		if (base_msr == MSR_CORE_PERF_FIXED_CTR0)
 			rdpmc_idx |= INTEL_RDPMC_FIXED;
 
+		/*
+		 * 在以下调用guest_test_rdpmc():
+		 *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|408| <<guest_rd_wr_counters>>
+		 *        guest_test_rdpmc(rdpmc_idx, expect_success, expected_val);
+		 *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|417| <<guest_rd_wr_counters>>
+		 *        guest_test_rdpmc(rdpmc_idx, false, -1ull);
+		 */
 		guest_test_rdpmc(rdpmc_idx, expect_success, expected_val);
 
 		/*
@@ -414,6 +494,13 @@ static void guest_rd_wr_counters(uint32_t base_msr, uint8_t nr_possible_counters
 		 */
 		GUEST_ASSERT(!expect_success || !pmu_has_fast_mode);
 		rdpmc_idx |= INTEL_RDPMC_FAST;
+		/*
+		 * 在以下调用guest_test_rdpmc():
+		 *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|408| <<guest_rd_wr_counters>>
+		 *        guest_test_rdpmc(rdpmc_idx, expect_success, expected_val);
+		 *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|417| <<guest_rd_wr_counters>>
+		 *        guest_test_rdpmc(rdpmc_idx, false, -1ull);
+		 */
 		guest_test_rdpmc(rdpmc_idx, false, -1ull);
 
 		vector = wrmsr_safe(msr, 0);
@@ -421,6 +508,11 @@ static void guest_rd_wr_counters(uint32_t base_msr, uint8_t nr_possible_counters
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|466| <<test_gp_counters>> vm = pmu_vm_create_with_one_vcpu(&vcpu,
+ *           guest_test_gp_counters, pmu_version, perf_capabilities);
+ */
 static void guest_test_gp_counters(void)
 {
 	uint8_t pmu_version = guest_get_pmu_version();
@@ -453,6 +545,14 @@ static void guest_test_gp_counters(void)
 	else
 		base_msr = MSR_IA32_PERFCTR0;
 
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|456| <<guest_test_gp_counters>>
+	 *        guest_rd_wr_counters(base_msr, MAX_NR_GP_COUNTERS, nr_gp_counters, 0);
+	 *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|494| <<guest_test_fixed_counters>>
+	 *        guest_rd_wr_counters(MSR_CORE_PERF_FIXED_CTR0, MAX_NR_FIXED_COUNTERS, nr_fixed_counters,
+	 *                             supported_bitmask);
+	 */
 	guest_rd_wr_counters(base_msr, MAX_NR_GP_COUNTERS, nr_gp_counters, 0);
 	GUEST_DONE();
 }
@@ -491,6 +591,14 @@ static void guest_test_fixed_counters(void)
 	if (guest_get_pmu_version() >= 5)
 		supported_bitmask = this_cpu_property(X86_PROPERTY_PMU_FIXED_COUNTERS_BITMASK);
 
+	/*
+	 * called by:
+	 *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|456| <<guest_test_gp_counters>>
+	 *        guest_rd_wr_counters(base_msr, MAX_NR_GP_COUNTERS, nr_gp_counters, 0);
+	 *   - tools/testing/selftests/kvm/x86/pmu_counters_test.c|494| <<guest_test_fixed_counters>>
+	 *        guest_rd_wr_counters(MSR_CORE_PERF_FIXED_CTR0, MAX_NR_FIXED_COUNTERS, nr_fixed_counters,
+	 *                             supported_bitmask);
+	 */
 	guest_rd_wr_counters(MSR_CORE_PERF_FIXED_CTR0, MAX_NR_FIXED_COUNTERS,
 			     nr_fixed_counters, supported_bitmask);
 
-- 
2.39.5 (Apple Git-154)

