From ec087c9c25600c9206862d061e47119d3849cbc2 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sat, 29 May 2021 14:01:08 -0700
Subject: [PATCH 1/1] linux uek5 v4.14.35-2047.502.4.1

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/uapi/asm/kvm_para.h |   5 +
 arch/x86/kernel/apic/apic.c          |  35 +++
 arch/x86/kernel/apic/probe_64.c      |   8 +
 arch/x86/kernel/kvm.c                |  10 +
 arch/x86/kvm/irq_comm.c              |  35 +++
 arch/x86/kvm/lapic.c                 |   4 +
 arch/x86/kvm/x86.c                   |   4 +
 drivers/net/virtio_net.c             | 243 +++++++++++++++++++++
 drivers/vhost/net.c                  |  82 +++++++
 drivers/vhost/vhost.c                | 179 ++++++++++++++++
 drivers/vhost/vhost.h                |  43 ++++
 drivers/virtio/virtio_ring.c         | 306 +++++++++++++++++++++++++++
 include/uapi/linux/virtio_ring.h     |  49 +++++
 13 files changed, 1003 insertions(+)

diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index 21d5f0240595..7faf57abba9a 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -60,6 +60,11 @@ struct kvm_steal_time {
 };
 
 #define KVM_VCPU_PREEMPTED          (1 << 0)
+/*
+ * 在以下使用KVM_VCPU_FLUSH_TLB:
+ *   - arch/x86/kernel/kvm.c|623| <<kvm_flush_tlb_others>> state | KVM_VCPU_FLUSH_TLB))
+ *   - arch/x86/kvm/x86.c|2457| <<record_steal_time>> if (xchg(&st->preempted, 0) & KVM_VCPU_FLUSH_TLB)
+ */
 #define KVM_VCPU_FLUSH_TLB          (1 << 1)
 
 #define KVM_CLOCK_PAIRING_WALLCLOCK 0
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index ee33f0951322..b2717d08c111 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -90,6 +90,15 @@ static unsigned int disabled_cpu_apicid __read_mostly = BAD_APICID;
  * This variable controls which CPUs receive external NMIs.  By default,
  * external NMIs are delivered only to the BSP.
  */
+/*
+ * 在以下使用apic_extnmi:
+ *   - arch/x86/kernel/apic/apic.c|1319| <<init_bsp_APIC>> if (apic_extnmi == APIC_EXTNMI_NONE)
+ *   - arch/x86/kernel/apic/apic.c|1560| <<setup_local_APIC>> if ((!cpu && apic_extnmi != APIC_EXTNMI_NONE) ||
+ *   - arch/x86/kernel/apic/apic.c|1561| <<setup_local_APIC>> apic_extnmi == APIC_EXTNMI_ALL)
+ *   - arch/x86/kernel/apic/apic.c|2795| <<apic_set_extnmi>> apic_extnmi = APIC_EXTNMI_ALL;
+ *   - arch/x86/kernel/apic/apic.c|2797| <<apic_set_extnmi>> apic_extnmi = APIC_EXTNMI_NONE;
+ *   - arch/x86/kernel/apic/apic.c|2799| <<apic_set_extnmi>> apic_extnmi = APIC_EXTNMI_BSP;
+ */
 static int apic_extnmi = APIC_EXTNMI_BSP;
 
 /*
@@ -1418,6 +1427,11 @@ static void apic_pending_intr_clear(void)
  * Used to setup local APIC while initializing BSP or bringing up APs.
  * Always called with preemption disabled.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/apic.c|1598| <<apic_ap_setup>> setup_local_APIC();
+ *   - arch/x86/kernel/apic/apic.c|2413| <<apic_bsp_setup>> setup_local_APIC();
+ */
 void setup_local_APIC(void)
 {
 	int cpu = smp_processor_id();
@@ -1434,6 +1448,10 @@ void setup_local_APIC(void)
 	 * SPIV. Soft disable it before doing further initialization.
 	 */
 	value = apic_read(APIC_SPIV);
+	/*
+	 * Allows software to temporarily enable or disable the local APIC.
+	 * 第8位控制
+	 */
 	value &= ~APIC_SPIV_APIC_ENABLED;
 	apic_write(APIC_SPIV, value);
 
@@ -1749,6 +1767,10 @@ static inline void try_to_enable_x2apic(int remap_mode) { }
 static inline void __x2apic_enable(void) { }
 #endif /* !CONFIG_X86_X2APIC */
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/probe_64.c|32| <<default_setup_apic_routing>> enable_IR_x2apic();
+ */
 void __init enable_IR_x2apic(void)
 {
 	unsigned long flags;
@@ -2430,6 +2452,11 @@ int __init apic_bsp_setup(bool upmode)
  * This initializes the IO-APIC and APIC hardware if this is
  * a UP kernel.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/apic.c|2471| <<up_late_init>> APIC_init_uniprocessor();
+ *   - arch/x86/kernel/smpboot.c|1338| <<native_smp_prepare_cpus>> if (APIC_init_uniprocessor())
+ */
 int __init APIC_init_uniprocessor(void)
 {
 	if (disable_apic) {
@@ -2791,6 +2818,14 @@ static int __init apic_set_extnmi(char *arg)
 	if (!arg)
 		return -EINVAL;
 
+	/*
+	 * bsp:  External NMI is delivered only to CPU 0
+	 * all:  External NMIs are broadcast to all CPUs as a
+	 *       backup of CPU 0
+	 * none: External NMI is masked for all CPUs. This is
+	 *       useful so that a dump capture kernel won't be
+	 *       shot down by NMI
+	 */
 	if (!strncmp("all", arg, 3))
 		apic_extnmi = APIC_EXTNMI_ALL;
 	else if (!strncmp("none", arg, 4))
diff --git a/arch/x86/kernel/apic/probe_64.c b/arch/x86/kernel/apic/probe_64.c
index c303054b90b5..6b6d0127ffa2 100644
--- a/arch/x86/kernel/apic/probe_64.c
+++ b/arch/x86/kernel/apic/probe_64.c
@@ -25,12 +25,20 @@
 /*
  * Check the APIC IDs in bios_cpu_apicid and choose the APIC mode.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1358| <<native_smp_prepare_cpus>> default_setup_apic_routing();
+ */
 void __init default_setup_apic_routing(void)
 {
 	struct apic **drv;
 
 	enable_IR_x2apic();
 
+	/*
+	 * [    0.045605] x2apic enabled
+	 * [    0.046007] Switched APIC routing to physical x2apic.
+	 */
 	for (drv = __apicdrivers; drv < __apicdrivers_end; drv++) {
 		if ((*drv)->probe && (*drv)->probe()) {
 			if (apic != *drv) {
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index d41230f9c67f..28c1e3a4137c 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -449,6 +449,12 @@ static void __init sev_map_percpu_data(void)
 #ifdef CONFIG_SMP
 #define KVM_IPI_CLUSTER_SIZE	(2 * BITS_PER_LONG)
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|505| <<kvm_send_ipi_mask>> __send_ipi_mask(mask, vector);
+ *   - arch/x86/kernel/kvm.c|517| <<kvm_send_ipi_mask_allbutself>> __send_ipi_mask(local_mask, vector);
+ *   - arch/x86/kernel/kvm.c|527| <<kvm_send_ipi_all>> __send_ipi_mask(cpu_online_mask, vector);
+ */
 static void __send_ipi_mask(const struct cpumask *mask, int vector)
 {
 	unsigned long flags;
@@ -592,6 +598,10 @@ static void __init kvm_apf_trap_init(void)
 
 static DEFINE_PER_CPU(cpumask_var_t, __pv_tlb_mask);
 
+/*
+ * 在以下使用kvm_flush_tlb_others():
+ *   - arch/x86/kernel/kvm.c|643| <<kvm_guest_init>> pv_mmu_ops.flush_tlb_others = kvm_flush_tlb_others;
+ */
 static void kvm_flush_tlb_others(const struct cpumask *cpumask,
 			const struct flush_tlb_info *info)
 {
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 4d000aea05e0..5f714e5e758f 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -112,6 +112,41 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 	return r;
 }
 
+/*
+ * # sudo /usr/share/bcc/tools/trace -t -C  'kvm_set_msi_irq'
+ * TIME     CPU PID     TID     COMM            FUNC
+ * 1.409018 4   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.430408 4   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.557921 1   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.594878 5   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.624964 4   23372   23372   vhost-23357     kvm_set_msi_irq
+ *
+ * 9.586209 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ * 9.586283 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ * 9.586399 5   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 9.586470 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ * 9.586482 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ *
+ * kvm_set_msi_irq
+ * irqfd_wakeup
+ * __wake_up_common
+ * __wake_up_locked_key
+ * eventfd_signal
+ * vhost_signal
+ * vhost_add_used_and_signal_n
+ * handle_rx
+ * handle_rx_net
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ *
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|155| <<kvm_set_msi>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|187| <<kvm_arch_set_irq_inatomic>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|428| <<kvm_scan_ioapic_routes>> kvm_set_msi_irq(vcpu->kvm, entry, &irq);
+ *   - arch/x86/kvm/svm.c|5309| <<get_pi_vcpu_info>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/vmx/vmx.c|7543| <<vmx_update_pi_irte>> kvm_set_msi_irq(kvm, e, &irq);
+ */
 void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 		     struct kvm_lapic_irq *irq)
 {
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index baf4a218d186..edb1ed48a3cf 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -1214,6 +1214,10 @@ void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_set_eoi_accelerated);
 
+/*
+ * 处理APIC_ICR:
+ *   - arch/x86/kvm/lapic.c|1904| <<kvm_lapic_reg_write>> apic_send_ipi(apic);
+ */
 static void apic_send_ipi(struct kvm_lapic *apic)
 {
 	u32 icr_low = kvm_lapic_get_reg(apic, APIC_ICR);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 242555889380..ae97161610c7 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -2434,6 +2434,10 @@ static void kvm_vcpu_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 	kvm_x86_ops->tlb_flush(vcpu, invalidate_gpa);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7762| <<vcpu_enter_guest>> record_steal_time(vcpu);
+ */
 static void record_steal_time(struct kvm_vcpu *vcpu)
 {
 	struct kvm_host_map map;
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 9a91ce8037c3..cf0e91161b68 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -39,15 +39,39 @@
 static int napi_weight = NAPI_POLL_WEIGHT;
 module_param(napi_weight, int, 0444);
 
+/*
+ * 在以下使用napi_tx:
+ *   - drivers/net/virtio_net.c|2623| <<virtnet_alloc_queues>> napi_tx ? napi_weight : 0);
+ */
 static bool csum = true, gso = true, napi_tx;
 module_param(csum, bool, 0444);
 module_param(gso, bool, 0444);
 module_param(napi_tx, bool, 0644);
 
 /* FIXME: MTU in config. */
+/*
+ * 在以下使用GOOD_PACKET_LEN:
+ *   - drivers/net/virtio_net.c|570| <<receive_small>> unsigned int buflen = SKB_DATA_ALIGN(GOOD_PACKET_LEN + headroom) +
+ *   - drivers/net/virtio_net.c|599| <<receive_small>> buflen = SKB_DATA_ALIGN(GOOD_PACKET_LEN + headroom) +
+ *   - drivers/net/virtio_net.c|980| <<add_recvbuf_small>> int len = vi->hdr_len + VIRTNET_RX_PAD + GOOD_PACKET_LEN + xdp_headroom;
+ *   - drivers/net/virtio_net.c|992| <<add_recvbuf_small>> vi->hdr_len + GOOD_PACKET_LEN);
+ *   - drivers/net/virtio_net.c|2516| <<mergeable_min_buf_len>> (unsigned int )GOOD_PACKET_LEN);
+ */
 #define GOOD_PACKET_LEN (ETH_HLEN + VLAN_HLEN + ETH_DATA_LEN)
+/*
+ * 在以下使用GOOD_COPY_LEN:
+ *   - drivers/net/virtio_net.c|361| <<page_to_skb>> skb = napi_alloc_skb(&rq->napi, GOOD_COPY_LEN);
+ */
 #define GOOD_COPY_LEN	128
 
+/*
+ * 在以下使用VIRTNET_RX_PAD:
+ *   - drivers/net/virtio_net.c|568| <<receive_small>> unsigned int header_offset = VIRTNET_RX_PAD + xdp_headroom;
+ *   - drivers/net/virtio_net.c|597| <<receive_small>> header_offset = VIRTNET_RX_PAD + xdp_headroom;
+ *   - drivers/net/virtio_net.c|612| <<receive_small>> xdp.data_hard_start = buf + VIRTNET_RX_PAD + vi->hdr_len;
+ *   - drivers/net/virtio_net.c|980| <<add_recvbuf_small>> int len = vi->hdr_len + VIRTNET_RX_PAD + GOOD_PACKET_LEN + xdp_headroom;
+ *   - drivers/net/virtio_net.c|991| <<add_recvbuf_small>> sg_init_one(rq->sg, buf + VIRTNET_RX_PAD + xdp_headroom,
+ */
 #define VIRTNET_RX_PAD (NET_IP_ALIGN + NET_SKB_PAD)
 
 /* Amount of XDP headroom to prepend to packets for use by xdp_adjust_head */
@@ -148,6 +172,13 @@ struct receive_queue {
 	/* Name of this receive queue: input.$index */
 	char name[40];
 
+	/*
+	 * 在以下使用receive_queue->xdp_rxq:
+	 *   - drivers/net/virtio_net.c|615| <<receive_small>> xdp.rxq = &rq->xdp_rxq;
+	 *   - drivers/net/virtio_net.c|757| <<receive_mergeable>> xdp.rxq = &rq->xdp_rxq;
+	 *   - drivers/net/virtio_net.c|1335| <<virtnet_open>> err = xdp_rxq_info_reg(&vi->rq[i].xdp_rxq, dev, i);
+	 *   - drivers/net/virtio_net.c|1680| <<virtnet_close>> xdp_rxq_info_unreg(&vi->rq[i].xdp_rxq);
+	 */
 	struct xdp_rxq_info xdp_rxq;
 };
 
@@ -164,6 +195,14 @@ struct control_buf {
 
 struct virtnet_info {
 	struct virtio_device *vdev;
+	/*
+	 * 在以下使用virtnet_info->cvq:
+	 *   - drivers/net/virtio_net.c|1606| <<virtnet_send_command>> virtqueue_add_sgs(vi->cvq, sgs, out_num, 1, vi, GFP_ATOMIC);
+	 *   - drivers/net/virtio_net.c|1608| <<virtnet_send_command>> if (unlikely(!virtqueue_kick(vi->cvq)))
+	 *   - drivers/net/virtio_net.c|1614| <<virtnet_send_command>> while (!virtqueue_get_buf(vi->cvq, &tmp) &&
+	 *   - drivers/net/virtio_net.c|1615| <<virtnet_send_command>> !virtqueue_is_broken(vi->cvq))
+	 *   - drivers/net/virtio_net.c|2667| <<virtnet_find_vqs>> vi->cvq = vqs[total_vqs - 1];
+	 */
 	struct virtqueue *cvq;
 	struct net_device *dev;
 	struct send_queue *sq;
@@ -195,12 +234,41 @@ struct virtnet_info {
 	u8 hdr_len;
 
 	/* Work struct for refilling if we run low on memory. */
+	/*
+	 * 在以下使用virtnet_info->refill:
+	 *   - drivers/net/virtio_net.c|1311| <<refill_work>> container_of(work, struct virtnet_info, refill.work);
+	 *   - drivers/net/virtio_net.c|1326| <<refill_work>> schedule_delayed_work(&vi->refill, HZ/2);
+	 *   - drivers/net/virtio_net.c|1358| <<virtnet_receive>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1467| <<virtnet_open>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1820| <<_virtnet_set_queues>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1842| <<virtnet_close>> cancel_delayed_work_sync(&vi->refill);
+	 *   - drivers/net/virtio_net.c|2336| <<virtnet_freeze_down>> cancel_delayed_work_sync(&vi->refill);
+	 *   - drivers/net/virtio_net.c|2362| <<virtnet_restore_up>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|2837| <<virtnet_alloc_queues>> INIT_DELAYED_WORK(&vi->refill, refill_work);
+	 *   - drivers/net/virtio_net.c|3180| <<virtnet_probe>> cancel_delayed_work_sync(&vi->refill);
+	 */
 	struct delayed_work refill;
 
 	/* Work struct for config space updates */
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|2268| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|2512| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|2546| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|2980| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 *   - drivers/net/virtio_net.c|3076| <<virtnet_probe>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|3128| <<virtnet_remove>> flush_work(&vi->config_work);
+	 */
 	struct work_struct config_work;
 
 	/* Does the affinity hint is set for virtqueues? */
+	/*
+	 * 在以下使用virtnet_info->affinity_hint_set:
+	 *   - drivers/net/virtio_net.c|1254| <<virtnet_napi_tx_enable>> if (!vi->affinity_hint_set) {
+	 *   - drivers/net/virtio_net.c|1871| <<virtnet_clean_affinity>> if (vi->affinity_hint_set) {
+	 *   - drivers/net/virtio_net.c|1877| <<virtnet_clean_affinity>> vi->affinity_hint_set = false;
+	 *   - drivers/net/virtio_net.c|1904| <<virtnet_set_affinity>> vi->affinity_hint_set = true;
+	 */
 	bool affinity_hint_set;
 
 	/* CPU hotplug instances for online & dead */
@@ -252,6 +320,13 @@ static int rxq2vq(int rxq)
 	return rxq * 2;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|365| <<page_to_skb>> hdr = skb_vnet_hdr(skb);
+ *   - drivers/net/virtio_net.c|659| <<receive_small>> memcpy(skb_vnet_hdr(skb), buf, vi->hdr_len);
+ *   - drivers/net/virtio_net.c|940| <<receive_buf>> hdr = skb_vnet_hdr(skb);
+ *   - drivers/net/virtio_net.c|1391| <<xmit_skb>> hdr = skb_vnet_hdr(skb);
+ */
 static inline struct virtio_net_hdr_mrg_rxbuf *skb_vnet_hdr(struct sk_buff *skb)
 {
 	return (struct virtio_net_hdr_mrg_rxbuf *)skb->cb;
@@ -261,6 +336,16 @@ static inline struct virtio_net_hdr_mrg_rxbuf *skb_vnet_hdr(struct sk_buff *skb)
  * private is used to chain pages for big packets, put the whole
  * most recent used list in the beginning for reuse
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|418| <<page_to_skb>> give_pages(rq, page);
+ *   - drivers/net/virtio_net.c|690| <<receive_big>> give_pages(rq, page);
+ *   - drivers/net/virtio_net.c|923| <<receive_buf>> give_pages(rq, buf);
+ *   - drivers/net/virtio_net.c|1013| <<add_recvbuf_big>> give_pages(rq, list);
+ *   - drivers/net/virtio_net.c|1025| <<add_recvbuf_big>> give_pages(rq, list);
+ *   - drivers/net/virtio_net.c|1043| <<add_recvbuf_big>> give_pages(rq, first);
+ *   - drivers/net/virtio_net.c|2484| <<free_unused_bufs>> give_pages(&vi->rq[i], buf);
+ */
 static void give_pages(struct receive_queue *rq, struct page *page)
 {
 	struct page *end;
@@ -271,6 +356,12 @@ static void give_pages(struct receive_queue *rq, struct page *page)
 	rq->pages = page;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1010| <<add_recvbuf_big>> first = get_a_page(rq, gfp);
+ *   - drivers/net/virtio_net.c|1023| <<add_recvbuf_big>> first = get_a_page(rq, gfp);
+ *   - drivers/net/virtio_net.c|2438| <<_free_receive_bufs>> __free_pages(get_a_page(&vi->rq[i], GFP_KERNEL), 0);
+ */
 static struct page *get_a_page(struct receive_queue *rq, gfp_t gfp_mask)
 {
 	struct page *p = rq->pages;
@@ -284,20 +375,43 @@ static struct page *get_a_page(struct receive_queue *rq, gfp_t gfp_mask)
 	return p;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|308| <<virtqueue_napi_complete>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|323| <<skb_xmit_done>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|1149| <<skb_recv_done>> virtqueue_napi_schedule(&rq->napi, rvq);
+ *   - drivers/net/virtio_net.c|1161| <<virtnet_napi_enable>> virtqueue_napi_schedule(napi, vq);
+ */
 static void virtqueue_napi_schedule(struct napi_struct *napi,
 				    struct virtqueue *vq)
 {
+	/*
+	 * Test if NAPI routine is already running, and if not mark
+	 * it as running.  This is used as a condition variable
+	 * insure only one NAPI poll instance runs.  We also make
+	 * sure there is no pending NAPI disable.
+	 */
 	if (napi_schedule_prep(napi)) {
 		virtqueue_disable_cb(vq);
 		__napi_schedule(napi);
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1316| <<virtnet_poll>> virtqueue_napi_complete(napi, rq->vq, received);
+ *   - drivers/net/virtio_net.c|1364| <<virtnet_poll_tx>> virtqueue_napi_complete(napi, sq->vq, 0);
+ */
 static void virtqueue_napi_complete(struct napi_struct *napi,
 				    struct virtqueue *vq, int processed)
 {
 	int opaque;
 
+	/*
+	 * 在以下调用virtqueue_enable_cb_prepare():
+	 *   - drivers/net/virtio_net.c|397| <<virtqueue_napi_complete>> opaque = virtqueue_enable_cb_prepare(vq);
+	 *   - drivers/virtio/virtio_ring.c|879| <<virtqueue_enable_cb>> unsigned last_used_idx = virtqueue_enable_cb_prepare(_vq);
+	 */
 	opaque = virtqueue_enable_cb_prepare(vq);
 	if (napi_complete_done(napi, processed)) {
 		if (unlikely(virtqueue_poll(vq, opaque)))
@@ -340,6 +454,12 @@ static unsigned int mergeable_ctx_to_truesize(void *mrg_ctx)
 }
 
 /* Called from bottom half context */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|680| <<receive_big>> struct sk_buff *skb = page_to_skb(vi, rq, page, 0, len,
+ *   - drivers/net/virtio_net.c|774| <<receive_mergeable>> head_skb = page_to_skb(vi, rq, xdp_page,
+ *   - drivers/net/virtio_net.c|826| <<receive_mergeable>> head_skb = page_to_skb(vi, rq, page, offset, len, truesize, !xdp_prog);
+ */
 static struct sk_buff *page_to_skb(struct virtnet_info *vi,
 				   struct receive_queue *rq,
 				   struct page *page, unsigned int offset,
@@ -687,6 +807,10 @@ static struct sk_buff *receive_big(struct net_device *dev,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1029| <<receive_buf>> skb = receive_mergeable(dev, vi, rq, buf, ctx, len, xdp_xmit);
+ */
 static struct sk_buff *receive_mergeable(struct net_device *dev,
 					 struct virtnet_info *vi,
 					 struct receive_queue *rq,
@@ -1041,6 +1165,11 @@ static int add_recvbuf_big(struct virtnet_info *vi, struct receive_queue *rq,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1080| <<add_recvbuf_mergeable>> len = get_mergeable_buf_len(rq, &rq->mrg_avg_pkt_len, room);
+ *   - drivers/net/virtio_net.c|2685| <<mergeable_rx_buffer_size_show>> get_mergeable_buf_len(&vi->rq[queue_index], avg,
+ */
 static unsigned int get_mergeable_buf_len(struct receive_queue *rq,
 					  struct ewma_pkt_len *avg_pkt_len,
 					  unsigned int room)
@@ -1107,6 +1236,13 @@ static int add_recvbuf_mergeable(struct virtnet_info *vi,
  * before we're receiving packets, or from refill_work which is
  * careful to disable receiving (using napi_disable).
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1189| <<refill_work>> still_empty = !try_fill_recv(vi, rq, GFP_KERNEL);
+ *   - drivers/net/virtio_net.c|1223| <<virtnet_receive>> if (!try_fill_recv(vi, rq, GFP_ATOMIC))
+ *   - drivers/net/virtio_net.c|1321| <<virtnet_open>> if (!try_fill_recv(vi, &vi->rq[i], GFP_KERNEL))
+ *   - drivers/net/virtio_net.c|2149| <<virtnet_restore_up>> if (!try_fill_recv(vi, &vi->rq[i], GFP_KERNEL))
+ */
 static bool try_fill_recv(struct virtnet_info *vi, struct receive_queue *rq,
 			  gfp_t gfp)
 {
@@ -1151,6 +1287,13 @@ static void virtnet_napi_enable(struct virtqueue *vq, struct napi_struct *napi)
 	local_bh_enable();
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1426| <<virtnet_open>> virtnet_napi_tx_enable(vi, vi->sq[i].vq, &vi->sq[i].napi);
+ *   - drivers/net/virtio_net.c|2255| <<virtnet_restore_up>> virtnet_napi_tx_enable(vi, vi->sq[i].vq,
+ *   - drivers/net/virtio_net.c|2377| <<virtnet_xdp_set>> virtnet_napi_tx_enable(vi, vi->sq[i].vq,
+ *   - drivers/net/virtio_net.c|2388| <<virtnet_xdp_set>> virtnet_napi_tx_enable(vi, vi->sq[i].vq,
+ */
 static void virtnet_napi_tx_enable(struct virtnet_info *vi,
 				   struct virtqueue *vq,
 				   struct napi_struct *napi)
@@ -1197,6 +1340,10 @@ static void refill_work(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1433| <<virtnet_poll>> received = virtnet_receive(rq, budget, &xdp_xmit);
+ */
 static int virtnet_receive(struct receive_queue *rq, int budget, bool *xdp_xmit)
 {
 	struct virtnet_info *vi = rq->vq->vdev->priv;
@@ -1232,6 +1379,13 @@ static int virtnet_receive(struct receive_queue *rq, int budget, bool *xdp_xmit)
 	return received;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1381| <<virtnet_poll_cleantx>> free_old_xmit_skbs(sq);
+ *   - drivers/net/virtio_net.c|1451| <<virtnet_poll_tx>> free_old_xmit_skbs(sq);
+ *   - drivers/net/virtio_net.c|1520| <<start_xmit>> free_old_xmit_skbs(sq);
+ *   - drivers/net/virtio_net.c|1563| <<start_xmit>> free_old_xmit_skbs(sq);
+ */
 static void free_old_xmit_skbs(struct send_queue *sq)
 {
 	struct sk_buff *skb;
@@ -1270,6 +1424,10 @@ static bool is_xdp_raw_buffer_queue(struct virtnet_info *vi, int q)
 		return false;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1396| <<virtnet_poll>> virtnet_poll_cleantx(rq);
+ */
 static void virtnet_poll_cleantx(struct receive_queue *rq)
 {
 	struct virtnet_info *vi = rq->vq->vdev->priv;
@@ -1332,6 +1490,10 @@ static int virtnet_open(struct net_device *dev)
 	return 0;
 }
 
+/*
+ * 在以下使用virtnet_poll_tx():
+ *   - drivers/net/virtio_net.c|2705| <<virtnet_alloc_queues>> netif_tx_napi_add(vi->dev, &vi->sq[i].napi, virtnet_poll_tx,
+ */
 static int virtnet_poll_tx(struct napi_struct *napi, int budget)
 {
 	struct send_queue *sq = container_of(napi, struct send_queue, napi);
@@ -1413,11 +1575,26 @@ static netdev_tx_t start_xmit(struct sk_buff *skb, struct net_device *dev)
 	int err;
 	struct netdev_queue *txq = netdev_get_tx_queue(dev, qnum);
 	bool kick = !skb->xmit_more;
+	/*
+	 * struct send_queue *sq:
+	 * -> struct napi_struct napi;
+	 *    -> int weight;
+	 */
 	bool use_napi = sq->napi.weight;
 
 	/* Free up any pending old buffers before queueing new ones. */
 	free_old_xmit_skbs(sq);
 
+	/*
+	 * This re-enables callbacks but hints to the other side to delay
+	 * interrupts until most of the available buffers have been processed;
+	 * it returns "false" if there are many pending buffers in the queue,
+	 * to detect a possible race between the driver checking for more work,
+	 * and enabling callbacks.
+	 *
+	 * Caller must ensure we don't call this with other virtqueue
+	 * operations at the same time (except where noted).
+	 */
 	if (use_napi && kick)
 		virtqueue_enable_cb_delayed(sq->vq);
 
@@ -1478,6 +1655,18 @@ static netdev_tx_t start_xmit(struct sk_buff *skb, struct net_device *dev)
  * supported by the hypervisor, as indicated by feature bits, should
  * never fail unless improperly formatted.
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1642| <<virtnet_set_mac_address>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MAC,
+ *   - drivers/net/virtio_net.c|1719| <<virtnet_ack_link_announce>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_ANNOUNCE,
+ *   - drivers/net/virtio_net.c|1736| <<_virtnet_set_queues>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MQ,
+ *   - drivers/net/virtio_net.c|1798| <<virtnet_set_rx_mode>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_RX,
+ *   - drivers/net/virtio_net.c|1805| <<virtnet_set_rx_mode>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_RX,
+ *   - drivers/net/virtio_net.c|1841| <<virtnet_set_rx_mode>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MAC,
+ *   - drivers/net/virtio_net.c|1857| <<virtnet_vlan_rx_add_vid>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_VLAN,
+ *   - drivers/net/virtio_net.c|1872| <<virtnet_vlan_rx_kill_vid>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_VLAN,
+ *   - drivers/net/virtio_net.c|2273| <<virtnet_set_guest_offloads>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_GUEST_OFFLOADS,
+ */
 static bool virtnet_send_command(struct virtnet_info *vi, u8 class, u8 cmd,
 				 struct scatterlist *out)
 {
@@ -1863,6 +2052,9 @@ static void virtnet_cpu_notif_remove(struct virtnet_info *vi)
 					    &vi->node_dead);
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_ringparam = virtnet_get_ringparam()
+ */
 static void virtnet_get_ringparam(struct net_device *dev,
 				struct ethtool_ringparam *ring)
 {
@@ -1875,6 +2067,9 @@ static void virtnet_get_ringparam(struct net_device *dev,
 }
 
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_drvinfo = virtnet_get_drvinfo()
+ */
 static void virtnet_get_drvinfo(struct net_device *dev,
 				struct ethtool_drvinfo *info)
 {
@@ -1888,6 +2083,9 @@ static void virtnet_get_drvinfo(struct net_device *dev,
 }
 
 /* TODO: Eliminate OOO packets during switching */
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.set_channels = virtnet_set_channels()
+ */
 static int virtnet_set_channels(struct net_device *dev,
 				struct ethtool_channels *channels)
 {
@@ -1924,6 +2122,9 @@ static int virtnet_set_channels(struct net_device *dev,
 	return err;
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_strings = virtnet_get_strings()
+ */
 static void virtnet_get_strings(struct net_device *dev, u32 stringset, u8 *data)
 {
 	struct virtnet_info *vi = netdev_priv(dev);
@@ -1951,6 +2152,9 @@ static void virtnet_get_strings(struct net_device *dev, u32 stringset, u8 *data)
 	}
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_sset_count = virtnet_get_sset_count()
+ */
 static int virtnet_get_sset_count(struct net_device *dev, int sset)
 {
 	struct virtnet_info *vi = netdev_priv(dev);
@@ -1964,6 +2168,9 @@ static int virtnet_get_sset_count(struct net_device *dev, int sset)
 	}
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_ethtool_stats = virtnet_get_ethtool_stats()
+ */
 static void virtnet_get_ethtool_stats(struct net_device *dev,
 				      struct ethtool_stats *stats, u64 *data)
 {
@@ -2001,6 +2208,9 @@ static void virtnet_get_ethtool_stats(struct net_device *dev,
 	}
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_channels = virtnet_get_channels()
+ */
 static void virtnet_get_channels(struct net_device *dev,
 				 struct ethtool_channels *channels)
 {
@@ -2040,6 +2250,9 @@ virtnet_validate_ethtool_cmd(const struct ethtool_link_ksettings *cmd)
 			     __ETHTOOL_LINK_MODE_MASK_NBITS);
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.set_link_ksettings = virtnet_set_link_ksettings()
+ */
 static int virtnet_set_link_ksettings(struct net_device *dev,
 				      const struct ethtool_link_ksettings *cmd)
 {
@@ -2058,6 +2271,9 @@ static int virtnet_set_link_ksettings(struct net_device *dev,
 	return 0;
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_link_ksettings = virtnet_get_link_ksettings()
+ */
 static int virtnet_get_link_ksettings(struct net_device *dev,
 				      struct ethtool_link_ksettings *cmd)
 {
@@ -2078,6 +2294,11 @@ static void virtnet_init_settings(struct net_device *dev)
 	vi->duplex = DUPLEX_UNKNOWN;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2533| <<virtnet_config_changed_work>> virtnet_update_settings(vi);
+ *   - drivers/net/virtio_net.c|3079| <<virtnet_probe>> virtnet_update_settings(vi);
+ */
 static void virtnet_update_settings(struct virtnet_info *vi)
 {
 	u32 speed;
@@ -2110,6 +2331,10 @@ static const struct ethtool_ops virtnet_ethtool_ops = {
 	.set_link_ksettings = virtnet_set_link_ksettings,
 };
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|3197| <<virtnet_freeze>> virtnet_freeze_down(vdev);
+ */
 static void virtnet_freeze_down(struct virtio_device *vdev)
 {
 	struct virtnet_info *vi = vdev->priv;
@@ -2357,6 +2582,18 @@ static const struct net_device_ops virtnet_netdev = {
 	.ndo_get_phys_port_name	= virtnet_get_phys_port_name,
 };
 
+/*
+ * 在以下使用virtnet_info->config_work:
+ *   - drivers/net/virtio_net.c|2268| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+ *   - drivers/net/virtio_net.c|2512| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+ *   - drivers/net/virtio_net.c|2546| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+ *   - drivers/net/virtio_net.c|2980| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+ *   - drivers/net/virtio_net.c|3076| <<virtnet_probe>> schedule_work(&vi->config_work);
+ *   - drivers/net/virtio_net.c|3128| <<virtnet_remove>> flush_work(&vi->config_work);
+ *
+ * 在以下使用virtnet_config_changed_work():
+ *   - drivers/net/virtio_net.c|2980| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+ */
 static void virtnet_config_changed_work(struct work_struct *work)
 {
 	struct virtnet_info *vi =
@@ -2390,6 +2627,9 @@ static void virtnet_config_changed_work(struct work_struct *work)
 	}
 }
 
+/*
+ * struct virtio_driver virtio_net_driver.config_changed = virtnet_config_changed()
+ */
 static void virtnet_config_changed(struct virtio_device *vdev)
 {
 	struct virtnet_info *vi = vdev->priv;
@@ -2726,6 +2966,9 @@ static bool virtnet_validate_features(struct virtio_device *vdev)
 #define MIN_MTU ETH_MIN_MTU
 #define MAX_MTU ETH_MAX_MTU
 
+/*
+ * struct virtio_driver virtio_net_driver.validate = virtnet_validate()
+ */
 static int virtnet_validate(struct virtio_device *vdev)
 {
 	if (!vdev->config->get) {
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 8fe07622ae59..d4820d73e3a6 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -388,6 +388,12 @@ static bool vhost_can_busy_poll(struct vhost_dev *dev,
 	       !vhost_has_work(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|773| <<handle_rx>> vhost_net_disable_vq(net, vq);
+ *   - drivers/vhost/net.c|970| <<vhost_net_stop_vq>> vhost_net_disable_vq(n, vq);
+ *   - drivers/vhost/net.c|1150| <<vhost_net_set_backend>> vhost_net_disable_vq(n, vq);
+ */
 static void vhost_net_disable_vq(struct vhost_net *n,
 				 struct vhost_virtqueue *vq)
 {
@@ -620,6 +626,9 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
 	struct vhost_virtqueue *vq = &nvq->vq;
 	unsigned long uninitialized_var(endtime);
+	/*
+	 * 返回的等价skb->len
+	 */
 	int len = peek_head_len(rvq, sk);
 
 	if (!len && vq->busyloop_timeout) {
@@ -662,6 +671,14 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
  * @quota       - headcount quota, 1 for big buffer
  *	returns number of buffer heads allocated, negative on error
  */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|804| <<handle_rx>> headcount = get_rx_bufs(vq, vq->heads, vhost_len,
+ *
+ * 827                 headcount = get_rx_bufs(vq, vq->heads, vhost_len,
+ * 828                                         &in, vq_log, &log,
+ * 829                                         likely(mergeable) ? UIO_MAXIOV : 1);
+ */
 static int get_rx_bufs(struct vhost_virtqueue *vq,
 		       struct vring_used_elem *heads,
 		       int datalen,
@@ -672,6 +689,10 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 {
 	unsigned int out, in;
 	int seg = 0;
+	/*
+	 * headcount只在while循环每一次的结束增加!!
+	 * 不到while循环第一次结束的discard都不会影响avail index !!!
+	 */
 	int headcount = 0;
 	unsigned d;
 	int r, nlogs = 0;
@@ -680,11 +701,26 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 	 */
 	u32 uninitialized_var(len);
 
+	/*
+	 * 每次while循环结束前datalen会减少
+	 */
 	while (datalen > 0 && headcount < quota) {
 		if (unlikely(seg >= UIO_MAXIOV)) {
 			r = -ENOBUFS;
+			/*
+			 * goto err会调用vhost_discard_vq_desc()
+			 *
+			 * 这里不会导致endless loop???
+			 */
 			goto err;
 		}
+		/*
+		 * seg一开始是0
+		 *
+		 * 如果是net的rx, 每次只取一个seg吧
+		 *
+		 * vhost_get_vq_desc()会增加vq->last_avail_idx++
+		 */
 		r = vhost_get_vq_desc(vq, vq->iov + seg,
 				      ARRAY_SIZE(vq->iov) - seg, &out,
 				      &in, log, log_num);
@@ -696,6 +732,9 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 			r = 0;
 			goto err;
 		}
+		/*
+		 * 因为这是收包, 不能有out!!!
+		 */
 		if (unlikely(out || in <= 0)) {
 			vq_err(vq, "unexpected descriptor format for RX: "
 				"out %d, in %d\n", out, in);
@@ -710,6 +749,10 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 		len = iov_length(vq->iov + seg, in);
 		heads[headcount].len = cpu_to_vhost32(vq, len);
 		datalen -= len;
+		/*
+		 * headcount只在这里增加!!
+		 * 不到这里的discard都不会影响avail index !!!
+		 */
 		++headcount;
 		seg += in;
 	}
@@ -725,12 +768,21 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 	}
 	return headcount;
 err:
+	/*
+	 * headcount只在while循环每一次的结束增加!!
+	 * 不到while循环第一次结束的discard都不会影响avail index !!!
+	 */
 	vhost_discard_vq_desc(vq, headcount);
 	return r;
 }
 
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|893| <<handle_rx_kick>> handle_rx(net);
+ *   - drivers/vhost/net.c|907| <<handle_rx_net>> handle_rx(net);
+ */
 static void handle_rx(struct vhost_net *net)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];
@@ -763,12 +815,20 @@ static void handle_rx(struct vhost_net *net)
 	if (!sock)
 		goto out;
 
+	/*
+	 * !vq->iotlb的话直接返回1
+	 */
 	if (!vq_iotlb_prefetch(vq))
 		goto out;
 
 	vhost_disable_notify(&net->dev, vq);
 	vhost_net_disable_vq(net, vq);
 
+	/*
+	 * rhck的例子
+	 *   vhost_hlen = 0,
+	 *   sock_hlen = 12,
+	 */
 	vhost_hlen = nvq->vhost_hlen;
 	sock_hlen = nvq->sock_hlen;
 
@@ -777,12 +837,23 @@ static void handle_rx(struct vhost_net *net)
 	mergeable = vhost_has_feature(vq, VIRTIO_NET_F_MRG_RXBUF);
 
 	do {
+		/*
+		 * 返回的等价skb->len
+		 */
 		sock_len = vhost_net_rx_peek_head_len(net, sock->sk);
 
 		if (!sock_len)
 			break;
 		sock_len += sock_hlen;
 		vhost_len = sock_len + vhost_hlen;
+		/*
+		 * struct vhost_virtqueue *vq:
+		 * -> struct vring_used_elem *heads;
+		 *
+		 * in的类型: unsigned uninitialized_var(in)
+		 *
+		 * 因为可能有indirect, headcount是response的数目, in是用的desc的数目
+		 */
 		headcount = get_rx_bufs(vq, vq->heads, vhost_len,
 					&in, vq_log, &log,
 					likely(mergeable) ? UIO_MAXIOV : 1);
@@ -791,6 +862,9 @@ static void handle_rx(struct vhost_net *net)
 			goto out;
 		/* OK, now we need to know about added descriptors. */
 		if (!headcount) {
+			/*
+			 * 这里概率不大 但是可以遇到
+			 */
 			if (unlikely(vhost_enable_notify(&net->dev, vq))) {
 				/* They have slipped one in as we were
 				 * doing that: check again. */
@@ -804,6 +878,10 @@ static void handle_rx(struct vhost_net *net)
 		if (nvq->rx_array)
 			msg.msg_control = vhost_net_buf_consume(&nvq->rxq);
 		/* On overrun, truncate and discard */
+		/*
+		 * 这里看着挺有问题的!!!
+		 * 但是似乎(headcount > UIO_MAXIOV)的时候get_rx_bufs()里已经discard了
+		 */
 		if (unlikely(headcount > UIO_MAXIOV)) {
 			iov_iter_init(&msg.msg_iter, READ, vq->iov, 1, 1);
 			err = sock->ops->recvmsg(sock, &msg,
@@ -828,6 +906,7 @@ static void handle_rx(struct vhost_net *net)
 		if (unlikely(err != sock_len)) {
 			pr_debug("Discarded rx packet: "
 				 " len %d, expected %zd\n", err, sock_len);
+			/* 会 vq->last_avail_idx -= n; */
 			vhost_discard_vq_desc(vq, headcount);
 			continue;
 		}
@@ -860,6 +939,9 @@ static void handle_rx(struct vhost_net *net)
 		if (unlikely(vq_log))
 			vhost_log_write(vq, vq_log, log, vhost_len,
 					vq->iov, in);
+		/*
+		 * total_len只在这里增加, 就用在判断vhost_exceeds_weight()
+		 */
 		total_len += vhost_len;
 	} while (likely(!vhost_exceeds_weight(vq, ++recv_pkts, total_len)));
 
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 78f26cd4eef9..b887f34f9977 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -47,7 +47,18 @@ enum {
 	VHOST_MEMORY_F_LOG = 0x1,
 };
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2413| <<vhost_notify>> if (vhost_get_avail(vq, event, vhost_used_event(vq))) {
+ */
 #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1888| <<vhost_update_avail_event>> vhost_avail_event(vq)))
+ *   - drivers/vhost/vhost.c|1895| <<vhost_update_avail_event>> used = vhost_avail_event(vq);
+ *   - drivers/vhost/vhost.c|1897| <<vhost_update_avail_event>> sizeof *vhost_avail_event(vq));
+ *   - drivers/vhost/vhost.c|2514| <<vhost_enable_notify>> vhost_avail_event(vq), r);
+ */
 #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
 
 INTERVAL_TREE_DEFINE(struct vhost_umem_node,
@@ -1857,6 +1868,12 @@ int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 }
 EXPORT_SYMBOL_GPL(vhost_log_write);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1912| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|2447| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|2484| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+ */
 static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 {
 	void __user *used;
@@ -1876,6 +1893,10 @@ static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2558| <<vhost_enable_notify>> r = vhost_update_avail_event(vq, vq->avail_idx);
+ */
 static int vhost_update_avail_event(struct vhost_virtqueue *vq, u16 avail_event)
 {
 	if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
@@ -2105,6 +2126,19 @@ static int get_indirect(struct vhost_virtqueue *vq,
  * This function returns the descriptor number found, or vq->num (which is
  * never a valid descriptor number) if none was found.  A negative code is
  * returned on error. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|429| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/net.c|439| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/net.c|694| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg,
+ *   - drivers/vhost/scsi.c|508| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/scsi.c|886| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/test.c|62| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/vsock.c|121| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/vsock.c|460| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *
+ * vhost_get_vq_desc()会增加vq->last_avail_idx++
+ */
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
@@ -2239,10 +2273,28 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 	} while ((i = next_desc(vq, &desc)) != -1);
 
 	/* On success, increment avail index. */
+	/*
+	 * 在以下设置vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1440| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|2248| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 */
 	vq->last_avail_idx++;
 
 	/* Assume notifications from guest are disabled at this point,
 	 * if they aren't we would need to update avail_event index. */
+	/*
+	 * 在以下设置vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|321| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|2502| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2539| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|1869| <<vhost_update_used_flags>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2500| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2537| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 */
 	BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
 	return head;
 }
@@ -2251,12 +2303,26 @@ EXPORT_SYMBOL_GPL(vhost_get_vq_desc);
 /* Reverse the effect of vhost_get_vq_desc. Useful for error handling. */
 void vhost_discard_vq_desc(struct vhost_virtqueue *vq, int n)
 {
+	/*
+	 * 在以下设置vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1440| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|2248| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 */
 	vq->last_avail_idx -= n;
 }
 EXPORT_SYMBOL_GPL(vhost_discard_vq_desc);
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|598| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+ *   - drivers/vhost/vhost.c|2424| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+ *   - drivers/vhost/vsock.c|190| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(pkt->hdr) + payload_len);
+ *   - drivers/vhost/vsock.c|493| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, len);
+ */
 int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 {
 	struct vring_used_elem heads = {
@@ -2268,6 +2334,13 @@ int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 }
 EXPORT_SYMBOL_GPL(vhost_add_used);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2328| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+ *   - drivers/vhost/vhost.c|2334| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+ *
+ * 核心思想是把count个heads给copy到shared used buffer
+ */
 static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 			    struct vring_used_elem *heads,
 			    unsigned count)
@@ -2277,6 +2350,15 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 	int start;
 
 	start = vq->last_used_idx & (vq->num - 1);
+	/*
+	 * struct vhost_virtqueue *vq:
+	 * -> struct vring_desc __user *desc;
+	 * -> struct vring_avail __user *avail;
+	 * -> struct vring_used __user *used;
+	 *    -> __virtio16 flags;
+	 *    -> __virtio16 idx;
+	 *    -> struct vring_used_elem ring[];
+	 */
 	used = vq->used->ring + start;
 	if (count == 1) {
 		if (vhost_put_user(vq, heads[0].id, &used->id)) {
@@ -2298,6 +2380,12 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 		log_used(vq, ((void __user *)used - (void __user *)vq->used),
 			 count * sizeof *used);
 	}
+	/*
+	 * 在以下设置vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|318| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1930| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2308| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 */
 	old = vq->last_used_idx;
 	new = (vq->last_used_idx += count);
 	/* If the driver never bothers to signal in a very long while,
@@ -2311,6 +2399,11 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2273| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+ *   - drivers/vhost/vhost.c|2434| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+ */
 int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 		     unsigned count)
 {
@@ -2347,6 +2440,10 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2391| <<vhost_signal>> if (vq->call_ctx && vhost_notify(dev, vq))
+ */
 static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__u16 old, new;
@@ -2357,6 +2454,9 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 	 * interrupts. */
 	smp_mb();
 
+	/*
+	 * 不太见VIRTIO_F_NOTIFY_ON_EMPTY使用
+	 */
 	if (vhost_has_feature(vq, VIRTIO_F_NOTIFY_ON_EMPTY) &&
 	    unlikely(vq->avail_idx == vq->last_avail_idx))
 		return true;
@@ -2369,22 +2469,53 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 		}
 		return !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));
 	}
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|319| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2313| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2382| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2384| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	old = vq->signalled_used;
 	v = vq->signalled_used_valid;
+	/*
+	 * 在以下设置vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|318| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1930| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2308| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 */
 	new = vq->signalled_used = vq->last_used_idx;
 	vq->signalled_used_valid = true;
 
 	if (unlikely(!v))
 		return true;
 
+	/*
+	 * 获取((__virtio16 __user *)&vq->avail->ring[vq->num])
+	 */
 	if (vhost_get_avail(vq, event, vhost_used_event(vq))) {
 		vq_err(vq, "Failed to get used event idx");
 		return true;
 	}
+	/*
+	 * event_idx : vq->avail->ring[vq->num]
+	 * new_idx   : 当前的vq->last_used_idx (也是vq->signalled_used)
+	 * old       : 上一次的vq->signalled_used
+	 *
+	 * (__u16)(new_idx - event_idx - 1) < (__u16)(new_idx - old);
+	 */
 	return vring_need_event(vhost16_to_cpu(vq, event), new, old);
 }
 
 /* This actually signals the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|611| <<vhost_scsi_complete_cmd_work>> vhost_signal(&vs->dev, &vs->vqs[vq].vq);
+ *   - drivers/vhost/vhost.c|2425| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.c|2435| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vsock.c|220| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|500| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+ */
 void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	/* Signal the Guest tell them we used something up. */
@@ -2394,6 +2525,15 @@ void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_signal);
 
 /* And here's the combo meal deal.  Supersize me! */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|577| <<handle_tx>> vhost_add_used_and_signal(&net->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|537| <<vhost_scsi_do_evt_work>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|875| <<vhost_scsi_send_bad_target>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|1217| <<vhost_scsi_send_tmf_resp>> vhost_add_used_and_signal(&vs->dev, vq, vq_desc, 0);
+ *   - drivers/vhost/scsi.c|1309| <<vhost_scsi_send_an_resp>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+ *   - drivers/vhost/test.c|88| <<handle_vq>> vhost_add_used_and_signal(&n->dev, vq, head, 0);
+ */
 void vhost_add_used_and_signal(struct vhost_dev *dev,
 			       struct vhost_virtqueue *vq,
 			       unsigned int head, int len)
@@ -2404,6 +2544,11 @@ void vhost_add_used_and_signal(struct vhost_dev *dev,
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal);
 
 /* multi-buffer version of vhost_add_used_and_signal */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|344| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+ *   - drivers/vhost/net.c|865| <<handle_rx>> vhost_add_used_and_signal_n(&net->dev, vq, vq->heads,
+ */
 void vhost_add_used_and_signal_n(struct vhost_dev *dev,
 				 struct vhost_virtqueue *vq,
 				 struct vring_used_elem *heads, unsigned count)
@@ -2432,11 +2577,34 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_vq_avail_empty);
 
 /* OK, now we need to know about added descriptors. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|504| <<handle_tx>> if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|642| <<vhost_net_rx_peek_head_len>> else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|797| <<handle_rx>> if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/scsi.c|516| <<vhost_scsi_do_evt_work>> if (vhost_enable_notify(&vs->dev, vq))
+ *   - drivers/vhost/scsi.c|899| <<vhost_scsi_get_desc>> if (unlikely(vhost_enable_notify(&vs->dev, vq))) {
+ *   - drivers/vhost/test.c|71| <<handle_vq>> if (unlikely(vhost_enable_notify(&n->dev, vq))) {
+ *   - drivers/vhost/vsock.c|112| <<vhost_transport_do_send_pkt>> vhost_enable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|138| <<vhost_transport_do_send_pkt>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ *   - drivers/vhost/vsock.c|466| <<vhost_vsock_handle_tx_kick>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ */
 bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__virtio16 avail_idx;
 	int r;
 
+	/*
+	 * 在以下设置vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|321| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|2502| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2539| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|1869| <<vhost_update_used_flags>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2500| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2537| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 */
 	if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
 		return false;
 	vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
@@ -2474,6 +2642,17 @@ void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	int r;
 
+	/*
+	 * 在以下设置vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|321| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|2502| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2539| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|1869| <<vhost_update_used_flags>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2500| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2537| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 */
 	if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
 		return;
 	vq->used_flags |= VRING_USED_F_NO_NOTIFY;
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index f7ec63af627d..6d5e134b0cb6 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -108,21 +108,60 @@ struct vhost_virtqueue {
 	vhost_work_fn_t handle_kick;
 
 	/* Last available index we saw. */
+	/*
+	 * 在以下设置vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1440| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|2248| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 */
 	u16 last_avail_idx;
 
 	/* Caches available index value from user. */
 	u16 avail_idx;
 
 	/* Last index we used. */
+	/*
+	 * 在以下设置vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|318| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1930| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2308| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 */
 	u16 last_used_idx;
 
 	/* Used flags */
+	/*
+	 * 在以下设置vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|321| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|2502| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2539| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|1869| <<vhost_update_used_flags>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2500| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2537| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 */
 	u16 used_flags;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下设置vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|319| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2313| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2382| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2384| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 signalled_used;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|320| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|1918| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2314| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2386| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2388| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	bool signalled_used_valid;
 
 	/* Log writes to used structure. */
@@ -132,6 +171,10 @@ struct vhost_virtqueue {
 	struct iovec iov[UIO_MAXIOV];
 	struct iovec iotlb_iov[64];
 	struct iovec *indirect;
+	/*
+	 * 在以下分配vhost_virtqueue->heads:
+	 *   - drivers/vhost/vhost.c|412| <<vhost_dev_alloc_iovecs>> vq->heads = kmalloc(sizeof *vq->heads * UIO_MAXIOV, GFP_KERNEL);
+	 */
 	struct vring_used_elem *heads;
 	/* Protected by virtqueue mutex. */
 	struct vhost_umem *umem;
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 51278f8bd3ab..6d44e6df1477 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -68,6 +68,10 @@ struct vring_virtqueue {
 	struct vring vring;
 
 	/* Can we use weak barriers? */
+	/*
+	 * 在以下设置vring_virtqueue->weak_barriers:
+	 *   - drivers/virtio/virtio_ring.c|1011| <<__vring_new_virtqueue>> vq->weak_barriers = weak_barriers;
+	 */
 	bool weak_barriers;
 
 	/* Other side has made a mess, don't try any more. */
@@ -82,15 +86,67 @@ struct vring_virtqueue {
 	/* Head of free buffer list. */
 	unsigned int free_head;
 	/* Number we've added since last sync. */
+	/*
+	 * 在以下设置vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|434| <<virtqueue_add>> vq->num_added++;
+	 *   - drivers/virtio/virtio_ring.c|590| <<virtqueue_kick_prepare>> vq->num_added = 0;
+	 *   - drivers/virtio/virtio_ring.c|1026| <<__vring_new_virtqueue>> vq->num_added = 0;
+	 * 在以下使用vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|441| <<virtqueue_add>> if (unlikely(vq->num_added == (1 << 16) - 1))
+	 *   - drivers/virtio/virtio_ring.c|588| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 */
 	unsigned int num_added;
 
 	/* Last used index we've seen. */
+	/*
+	 * 在以下设置vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|869| <<virtqueue_get_buf_ctx>> vq->last_used_idx++;
+	 *   - drivers/virtio/virtio_ring.c|1199| <<__vring_new_virtqueue>> vq->last_used_idx = 0;
+	 * 在以下使用vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|804| <<more_used>> return vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev, vq->vring.used->idx);
+	 *   - drivers/virtio/virtio_ring.c|853| <<virtqueue_get_buf_ctx>> last_used = (vq->last_used_idx & (vq->vring.num - 1));
+	 *   - drivers/virtio/virtio_ring.c|892| <<virtqueue_get_buf_ctx>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx));
+	 *   - drivers/virtio/virtio_ring.c|991| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+	 *   - drivers/virtio/virtio_ring.c|1089| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|1103| <<virtqueue_enable_cb_delayed>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));
+	 *   - drivers/virtio/virtio_ring.c|1105| <<virtqueue_enable_cb_delayed>> if (unlikely((u16)(virtio16_to_cpu(_vq->vdev, vq->vring.used->idx) - vq->last_used_idx) > bufs)) {
+	 */
 	u16 last_used_idx;
 
 	/* Last written value to avail->flags */
+	/*
+	 * 在以下设置vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|776| <<virtqueue_disable_cb>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|809| <<virtqueue_enable_cb_prepare>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|884| <<virtqueue_enable_cb_delayed>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|990| <<__vring_new_virtqueue>> vq->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1005| <<__vring_new_virtqueue>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 * 在以下使用vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|743| <<virtqueue_get_buf_ctx>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|775| <<virtqueue_disable_cb>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|778| <<virtqueue_disable_cb>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|808| <<virtqueue_enable_cb_prepare>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|811| <<virtqueue_enable_cb_prepare>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|883| <<virtqueue_enable_cb_delayed>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1007| <<__vring_new_virtqueue>> vq->vring.avail->flags = cpu_to_virtio16(vdev, vq->avail_flags_shadow);
+	 */
 	u16 avail_flags_shadow;
 
 	/* Last written value to avail->idx in guest byte order */
+	/*
+	 * 在以下修改vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|432| <<virtqueue_add>> vq->avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|961| <<virtqueue_detach_unused_buf>> vq->avail_idx_shadow--;
+	 *   - drivers/virtio/virtio_ring.c|1025| <<__vring_new_virtqueue>> vq->avail_idx_shadow = 0;
+	 * 在以下使用vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|426| <<virtqueue_add>> avail = vq->avail_idx_shadow & (vq->vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|433| <<virtqueue_add>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|588| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|589| <<virtqueue_kick_prepare>> new = vq->avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|923| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|962| <<virtqueue_detach_unused_buf>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 */
 	u16 avail_idx_shadow;
 
 	/* How to notify other side. FIXME: commonalize hcalls! */
@@ -226,6 +282,12 @@ static void vring_unmap_one(const struct vring_virtqueue *vq,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|358| <<virtqueue_add>> if (vring_mapping_error(vq, addr))
+ *   - drivers/virtio/virtio_ring.c|371| <<virtqueue_add>> if (vring_mapping_error(vq, addr))
+ *   - drivers/virtio/virtio_ring.c|389| <<virtqueue_add>> if (vring_mapping_error(vq, addr))
+ */
 static int vring_mapping_error(const struct vring_virtqueue *vq,
 			       dma_addr_t addr)
 {
@@ -548,6 +610,13 @@ EXPORT_SYMBOL_GPL(virtqueue_add_inbuf_ctx);
  * This is sometimes useful because the virtqueue_kick_prepare() needs
  * to be serialized, but the actual virtqueue_notify() call does not.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|301| <<virtio_queue_rq>> if (bd->last && virtqueue_kick_prepare(vblk->vqs[qid].vq))
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|973| <<rpmsg_probe>> notify = virtqueue_kick_prepare(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|476| <<virtscsi_kick_cmd>> needs_kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|649| <<virtqueue_kick>> if (virtqueue_kick_prepare(vq))
+ */
 bool virtqueue_kick_prepare(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -559,8 +628,30 @@ bool virtqueue_kick_prepare(struct virtqueue *_vq)
 	 * event. */
 	virtio_mb(vq->weak_barriers);
 
+	/*
+	 * 在以下修改vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|432| <<virtqueue_add>> vq->avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|961| <<virtqueue_detach_unused_buf>> vq->avail_idx_shadow--;
+	 *   - drivers/virtio/virtio_ring.c|1025| <<__vring_new_virtqueue>> vq->avail_idx_shadow = 0;
+	 * 在以下使用vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|426| <<virtqueue_add>> avail = vq->avail_idx_shadow & (vq->vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|433| <<virtqueue_add>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|588| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|589| <<virtqueue_kick_prepare>> new = vq->avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|923| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|962| <<virtqueue_detach_unused_buf>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 */
 	old = vq->avail_idx_shadow - vq->num_added;
 	new = vq->avail_idx_shadow;
+	/*
+	 * 在以下设置vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|434| <<virtqueue_add>> vq->num_added++;
+	 *   - drivers/virtio/virtio_ring.c|590| <<virtqueue_kick_prepare>> vq->num_added = 0;
+	 *   - drivers/virtio/virtio_ring.c|1026| <<__vring_new_virtqueue>> vq->num_added = 0;
+	 * 在以下使用vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|441| <<virtqueue_add>> if (unlikely(vq->num_added == (1 << 16) - 1))
+	 *   - drivers/virtio/virtio_ring.c|588| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 */
 	vq->num_added = 0;
 
 #ifdef DEBUG
@@ -572,6 +663,9 @@ bool virtqueue_kick_prepare(struct virtqueue *_vq)
 #endif
 
 	if (vq->event) {
+		/*
+		 * return (__u16)(new_idx - event_idx - 1) < (__u16)(new_idx - old);
+		 */
 		needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev, vring_avail_event(&vq->vring)),
 					      new, old);
 	} else {
@@ -590,6 +684,13 @@ EXPORT_SYMBOL_GPL(virtqueue_kick_prepare);
  *
  * Returns false if host notify failed or queue is broken, otherwise true.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|306| <<virtio_queue_rq>> virtqueue_notify(vblk->vqs[qid].vq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|984| <<rpmsg_probe>> virtqueue_notify(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|481| <<virtscsi_kick_cmd>> virtqueue_notify(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|650| <<virtqueue_kick>> return virtqueue_notify(vq);
+ */
 bool virtqueue_notify(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -618,6 +719,43 @@ EXPORT_SYMBOL_GPL(virtqueue_notify);
  *
  * Returns false if kick failed, otherwise true.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|289| <<virtio_queue_rq>> virtqueue_kick(vblk->vqs[qid].vq); 
+ *   - drivers/char/hw_random/virtio-rng.c|63| <<register_buffer>> virtqueue_kick(vi->vq);
+ *   - drivers/char/virtio_console.c|513| <<add_inbuf>> virtqueue_kick(vq); 
+ *   - drivers/char/virtio_console.c|582| <<__send_control_msg>> virtqueue_kick(vq);
+ *   - drivers/char/virtio_console.c|636| <<__send_to_port>> virtqueue_kick(out_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|179| <<virtio_crypto_alg_ablkcipher_init_session>> virtqueue_kick(vcrypto->ctrl_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|252| <<virtio_crypto_alg_ablkcipher_close_session>> virtqueue_kick(vcrypto->ctrl_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|466| <<__virtio_crypto_ablkcipher_do_req>> virtqueue_kick(data_vq->vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|568| <<virtio_crypto_ablkcipher_crypt_req>> virtqueue_kick(data_vq->vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|298| <<virtio_gpu_queue_ctrl_buffer_locked>> virtqueue_kick(vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|373| <<virtio_gpu_queue_cursor>> virtqueue_kick(vq);
+ *   - drivers/net/caif/caif_virtio.c|589| <<cfv_netdev_tx>> virtqueue_kick(cfv->vq_tx);
+ *   - drivers/net/virtio_net.c|530| <<virtnet_xdp_flush>> virtqueue_kick(sq->vq);
+ *   - drivers/net/virtio_net.c|1243| <<try_fill_recv>> virtqueue_kick(rq->vq);
+ *   - drivers/net/virtio_net.c|1617| <<start_xmit>> virtqueue_kick(sq->vq);
+ *   - drivers/net/virtio_net.c|1665| <<virtnet_send_command>> if (unlikely(!virtqueue_kick(vi->cvq)))
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|654| <<rpmsg_send_offchannel_raw>> virtqueue_kick(vrp->svq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|802| <<rpmsg_recv_done>> virtqueue_kick(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|284| <<virtscsi_kick_event>> virtqueue_kick(vscsi->event_vq.vq);
+ *   - drivers/virtio/virtio_balloon.c|123| <<tell_host>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_balloon.c|328| <<stats_handle_request>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_balloon.c|463| <<init_vqs>> virtqueue_kick(vb->stats_vq);
+ *   - drivers/virtio/virtio_input.c|48| <<virtinput_recv_events>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_input.c|77| <<virtinput_send_status>> virtqueue_kick(vi->sts);
+ *   - drivers/virtio/virtio_input.c|196| <<virtinput_fill_evt>> virtqueue_kick(vi->evt);
+ *   - drivers/virtio/virtio_ring.c|442| <<virtqueue_add>> virtqueue_kick(_vq);
+ *   - net/9p/trans_virtio.c|308| <<p9_virtio_request>> virtqueue_kick(chan->vq);
+ *   - net/9p/trans_virtio.c|501| <<p9_virtio_zc_request>> virtqueue_kick(chan->vq);
+ *   - net/vmw_vsock/virtio_transport.c|174| <<virtio_transport_send_pkt_work>> virtqueue_kick(vq);
+ *   - net/vmw_vsock/virtio_transport.c|300| <<virtio_vsock_rx_fill>> virtqueue_kick(vq);
+ *   - net/vmw_vsock/virtio_transport.c|371| <<virtio_vsock_event_fill>> virtqueue_kick(vsock->vqs[VSOCK_VQ_EVENT]);
+ *   - net/vmw_vsock/virtio_transport.c|431| <<virtio_transport_event_work>> virtqueue_kick(vsock->vqs[VSOCK_VQ_EVENT]);
+ *   - tools/virtio/virtio_test.c|176| <<run_test>> if (unlikely(!virtqueue_kick(vq->vq)))
+ *   - tools/virtio/vringh_test.c|402| <<bool>> virtqueue_kick(vq);
+ */
 bool virtqueue_kick(struct virtqueue *vq)
 {
 	if (virtqueue_kick_prepare(vq))
@@ -675,6 +813,11 @@ static void detach_buf(struct vring_virtqueue *vq, unsigned int head,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|858| <<virtqueue_get_buf_ctx>> if (!more_used(vq)) {
+ *   - drivers/virtio/virtio_ring.c|1223| <<vring_interrupt>> if (!more_used(vq)) {
+ */
 static inline bool more_used(const struct vring_virtqueue *vq)
 {
 	return vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev, vq->vring.used->idx);
@@ -696,6 +839,12 @@ static inline bool more_used(const struct vring_virtqueue *vq)
  * Returns NULL if there are no used buffers, or the "data" token
  * handed to virtqueue_add_*().
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|932| <<receive_mergeable>> buf = virtqueue_get_buf_ctx(rq->vq, &len, &ctx);
+ *   - drivers/net/virtio_net.c|1331| <<virtnet_receive>> (buf = virtqueue_get_buf_ctx(rq->vq, &len, &ctx))) {
+ *   - drivers/virtio/virtio_ring.c|883| <<virtqueue_get_buf>> return virtqueue_get_buf_ctx(_vq, len, NULL);
+ */
 void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 			    void **ctx)
 {
@@ -736,10 +885,39 @@ void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 	/* detach_buf clears data, so grab it now. */
 	ret = vq->desc_state[i].data;
 	detach_buf(vq, i, ctx);
+	/*
+	 * 在以下设置vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|869| <<virtqueue_get_buf_ctx>> vq->last_used_idx++;
+	 *   - drivers/virtio/virtio_ring.c|1199| <<__vring_new_virtqueue>> vq->last_used_idx = 0;
+	 * 在以下使用vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|804| <<more_used>> return vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev, vq->vring.used->idx);
+	 *   - drivers/virtio/virtio_ring.c|853| <<virtqueue_get_buf_ctx>> last_used = (vq->last_used_idx & (vq->vring.num - 1));
+	 *   - drivers/virtio/virtio_ring.c|892| <<virtqueue_get_buf_ctx>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx));
+	 *   - drivers/virtio/virtio_ring.c|991| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+	 *   - drivers/virtio/virtio_ring.c|1089| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|1103| <<virtqueue_enable_cb_delayed>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));
+	 *   - drivers/virtio/virtio_ring.c|1105| <<virtqueue_enable_cb_delayed>> if (unlikely((u16)(virtio16_to_cpu(_vq->vdev, vq->vring.used->idx) - vq->last_used_idx) > bufs)) {
+	 */
 	vq->last_used_idx++;
 	/* If we expect an interrupt for the next entry, tell host
 	 * by writing event index and flush out the write before
 	 * the read in the next get_buf call. */
+	/*
+	 * 在以下使用vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|743| <<virtqueue_get_buf_ctx>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|775| <<virtqueue_disable_cb>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|776| <<virtqueue_disable_cb>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|778| <<virtqueue_disable_cb>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|808| <<virtqueue_enable_cb_prepare>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|809| <<virtqueue_enable_cb_prepare>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|811| <<virtqueue_enable_cb_prepare>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|883| <<virtqueue_enable_cb_delayed>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|884| <<virtqueue_enable_cb_delayed>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|990| <<__vring_new_virtqueue>> vq->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1005| <<__vring_new_virtqueue>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1007| <<__vring_new_virtqueue>> vq->vring.avail->flags = cpu_to_virtio16(vdev, vq->avail_flags_shadow);
+	 */
 	if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
 		virtio_store_mb(vq->weak_barriers,
 				&vring_used_event(&vq->vring),
@@ -768,10 +946,54 @@ EXPORT_SYMBOL_GPL(virtqueue_get_buf);
  *
  * Unlike other operations, this need not be serialized.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|216| <<virtblk_done>> virtqueue_disable_cb(vq);
+ *   - drivers/char/virtio_console.c|2190| <<virtcons_freeze>> virtqueue_disable_cb(portdev->c_ivq);
+ *   - drivers/char/virtio_console.c|2198| <<virtcons_freeze>> virtqueue_disable_cb(portdev->c_ivq);
+ *   - drivers/char/virtio_console.c|2201| <<virtcons_freeze>> virtqueue_disable_cb(port->in_vq);
+ *   - drivers/char/virtio_console.c|2202| <<virtcons_freeze>> virtqueue_disable_cb(port->out_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|47| <<virtcrypto_dataq_callback>> virtqueue_disable_cb(vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|206| <<virtio_gpu_dequeue_ctrl_func>> virtqueue_disable_cb(vgdev->ctrlq.vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|249| <<virtio_gpu_dequeue_cursor_func>> virtqueue_disable_cb(vgdev->cursorq.vq);
+ *   - drivers/net/caif/caif_virtio.c|203| <<cfv_release_used_buf>> virtqueue_disable_cb(cfv->vq_tx);
+ *   - drivers/net/caif/caif_virtio.c|462| <<cfv_netdev_close>> virtqueue_disable_cb(cfv->vq_tx);
+ *   - drivers/net/caif/caif_virtio.c|713| <<cfv_probe>> virtqueue_disable_cb(cfv->vq_tx);
+ *   - drivers/net/virtio_net.c|382| <<virtqueue_napi_schedule>> virtqueue_disable_cb(vq);
+ *   - drivers/net/virtio_net.c|402| <<virtqueue_napi_complete>> virtqueue_disable_cb(vq);
+ *   - drivers/net/virtio_net.c|412| <<skb_xmit_done>> virtqueue_disable_cb(vq);
+ *   - drivers/net/virtio_net.c|1611| <<start_xmit>> virtqueue_disable_cb(sq->vq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|523| <<rpmsg_downref_sleepers>> virtqueue_disable_cb(vrp->svq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|953| <<rpmsg_probe>> virtqueue_disable_cb(vrp->svq);
+ *   - drivers/scsi/virtio_scsi.c|221| <<virtscsi_vq_done>> virtqueue_disable_cb(vq);
+ *   - net/vmw_vsock/virtio_transport.c|320| <<virtio_transport_tx_work>> virtqueue_disable_cb(vq);
+ *   - net/vmw_vsock/virtio_transport.c|422| <<virtio_transport_event_work>> virtqueue_disable_cb(vq);
+ *   - net/vmw_vsock/virtio_transport.c|550| <<virtio_transport_rx_work>> virtqueue_disable_cb(vq);
+ *   - tools/virtio/virtio_test.c|166| <<run_test>> virtqueue_disable_cb(vq->vq);
+ *   - tools/virtio/vringh_test.c|394| <<bool>> virtqueue_disable_cb(vq);
+ *   - tools/virtio/vringh_test.c|427| <<bool>> virtqueue_disable_cb(vq);
+ */
 void virtqueue_disable_cb(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
 
+	/*
+	 * 在以下设置vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|776| <<virtqueue_disable_cb>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|809| <<virtqueue_enable_cb_prepare>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|884| <<virtqueue_enable_cb_delayed>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|990| <<__vring_new_virtqueue>> vq->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1005| <<__vring_new_virtqueue>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 * 在以下使用vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|743| <<virtqueue_get_buf_ctx>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|775| <<virtqueue_disable_cb>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|778| <<virtqueue_disable_cb>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|808| <<virtqueue_enable_cb_prepare>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|811| <<virtqueue_enable_cb_prepare>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|883| <<virtqueue_enable_cb_delayed>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1007| <<__vring_new_virtqueue>> vq->vring.avail->flags = cpu_to_virtio16(vdev, vq->avail_flags_shadow);
+	 */
 	if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
 		vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
 		if (!vq->event)
@@ -793,6 +1015,11 @@ EXPORT_SYMBOL_GPL(virtqueue_disable_cb);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * calld by:
+ *   - drivers/net/virtio_net.c|397| <<virtqueue_napi_complete>> opaque = virtqueue_enable_cb_prepare(vq);
+ *   - drivers/virtio/virtio_ring.c|879| <<virtqueue_enable_cb>> unsigned last_used_idx = virtqueue_enable_cb_prepare(_vq);
+ */
 unsigned virtqueue_enable_cb_prepare(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -805,11 +1032,31 @@ unsigned virtqueue_enable_cb_prepare(struct virtqueue *_vq)
 	/* Depending on the VIRTIO_RING_F_EVENT_IDX feature, we need to
 	 * either clear the flags bit or point the event index at the next
 	 * entry. Always do both to keep code simple. */
+	/*
+	 * 在以下设置vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|776| <<virtqueue_disable_cb>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|809| <<virtqueue_enable_cb_prepare>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|884| <<virtqueue_enable_cb_delayed>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|990| <<__vring_new_virtqueue>> vq->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1005| <<__vring_new_virtqueue>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 * 在以下使用vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|743| <<virtqueue_get_buf_ctx>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|775| <<virtqueue_disable_cb>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|778| <<virtqueue_disable_cb>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|808| <<virtqueue_enable_cb_prepare>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|811| <<virtqueue_enable_cb_prepare>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|883| <<virtqueue_enable_cb_delayed>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1007| <<__vring_new_virtqueue>> vq->vring.avail->flags = cpu_to_virtio16(vdev, vq->avail_flags_shadow);
+	 */
 	if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
 		vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
 		if (!vq->event)
 			vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
 	}
+	/*
+	 * 这里更新这是让backend有call的机会
+	 */
 	vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
 	END_USE(vq);
 	return last_used_idx;
@@ -825,6 +1072,11 @@ EXPORT_SYMBOL_GPL(virtqueue_enable_cb_prepare);
  *
  * This does not need to be serialized.
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|399| <<virtqueue_napi_complete>> if (unlikely(virtqueue_poll(vq, opaque)))
+ *   - drivers/virtio/virtio_ring.c|1046| <<virtqueue_enable_cb>> return !virtqueue_poll(_vq, last_used_idx);
+ */
 bool virtqueue_poll(struct virtqueue *_vq, unsigned last_used_idx)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -848,6 +1100,20 @@ EXPORT_SYMBOL_GPL(virtqueue_poll);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|225| <<virtblk_done>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|56| <<virtcrypto_dataq_callback>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|209| <<virtio_gpu_dequeue_ctrl_func>> } while (!virtqueue_enable_cb(vgdev->ctrlq.vq));
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|251| <<virtio_gpu_dequeue_cursor_func>> } while (!virtqueue_enable_cb(vgdev->cursorq.vq));
+ *   - drivers/net/caif/caif_virtio.c|565| <<cfv_netdev_tx>> virtqueue_enable_cb(cfv->vq_tx);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|496| <<rpmsg_upref_sleepers>> virtqueue_enable_cb(vrp->svq);
+ *   - drivers/scsi/virtio_scsi.c|227| <<virtscsi_vq_done>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|325| <<virtio_transport_tx_work>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|429| <<virtio_transport_event_work>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|581| <<virtio_transport_rx_work>> } while (!virtqueue_enable_cb(vq));
+ *   - tools/virtio/virtio_test.c|199| <<run_test>> if (virtqueue_enable_cb(vq->vq))
+ */
 bool virtqueue_enable_cb(struct virtqueue *_vq)
 {
 	unsigned last_used_idx = virtqueue_enable_cb_prepare(_vq);
@@ -868,6 +1134,14 @@ EXPORT_SYMBOL_GPL(virtqueue_enable_cb);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1512| <<start_xmit>> virtqueue_enable_cb_delayed(sq->vq);
+ *   - drivers/net/virtio_net.c|1550| <<start_xmit>> unlikely(!virtqueue_enable_cb_delayed(sq->vq))) {
+ *   - tools/virtio/virtio_test.c|196| <<run_test>> if (virtqueue_enable_cb_delayed(vq->vq))
+ *   - tools/virtio/vringh_test.c|387| <<bool>> if (!virtqueue_enable_cb_delayed(vq))
+ *   - tools/virtio/vringh_test.c|421| <<bool>> if (!virtqueue_enable_cb_delayed(vq))
+ */
 bool virtqueue_enable_cb_delayed(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -888,6 +1162,16 @@ bool virtqueue_enable_cb_delayed(struct virtqueue *_vq)
 	/* TODO: tune this threshold */
 	bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vringh.c|478| <<__vringh_need_notify>> err = getu16(vrh, &used_event, &vring_used_event(&vrh->vring));
+	 *   - drivers/vhost/vringh.c|481| <<__vringh_need_notify>> &vring_used_event(&vrh->vring));
+	 *   - drivers/virtio/virtio_ring.c|869| <<virtqueue_get_buf_ctx>> &vring_used_event(&vq->vring),
+	 *   - drivers/virtio/virtio_ring.c|942| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+	 *   - drivers/virtio/virtio_ring.c|1029| <<virtqueue_enable_cb_delayed>> &vring_used_event(&vq->vring),
+	 *   - tools/virtio/ringtest/virtio_ring_0_9.c|219| <<enable_call>> vring_used_event(&ring) = guest.last_used_idx;
+	 *   - tools/virtio/ringtest/virtio_ring_0_9.c|326| <<call_used>> need = vring_need_event(vring_used_event(&ring),
+	 */
 	virtio_store_mb(vq->weak_barriers,
 			&vring_used_event(&vq->vring),
 			cpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));
@@ -1156,6 +1440,16 @@ void vring_del_virtqueue(struct virtqueue *_vq)
 EXPORT_SYMBOL_GPL(vring_del_virtqueue);
 
 /* Manipulates transport-specific feature bits. */
+/*
+ * called by:
+ *   - drivers/misc/mic/vop/vop_main.c|142| <<vop_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|213| <<rproc_virtio_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/s390/virtio/kvm_virtio.c|104| <<kvm_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|801| <<virtio_ccw_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_mmio.c|131| <<vm_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_legacy.c|38| <<vp_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_modern.c|162| <<vp_finalize_features>> vring_transport_features(vdev);
+ */
 void vring_transport_features(struct virtio_device *vdev)
 {
 	unsigned int i;
@@ -1206,6 +1500,12 @@ EXPORT_SYMBOL_GPL(virtqueue_is_broken);
  * This should prevent the device from being used, allowing drivers to
  * recover.  You may need to grab appropriate locks to flush.
  */
+/*
+ * called by:
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|233| <<virtcrypto_update_status>> virtio_break_device(vcrypto->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1161| <<virtio_ccw_remove>> virtio_break_device(&vcdev->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1178| <<virtio_ccw_offline>> virtio_break_device(&vcdev->vdev);
+ */
 void virtio_break_device(struct virtio_device *dev)
 {
 	struct virtqueue *_vq;
@@ -1249,6 +1549,12 @@ dma_addr_t virtqueue_get_used_addr(struct virtqueue *_vq)
 }
 EXPORT_SYMBOL_GPL(virtqueue_get_used_addr);
 
+/*
+ * called by:
+ *   - include/linux/virtio.h|99| <<virtqueue_get_desc>> return virtqueue_get_vring(vq)->desc;
+ *   - include/linux/virtio.h|103| <<virtqueue_get_avail>> return virtqueue_get_vring(vq)->avail;
+ *   - include/linux/virtio.h|107| <<virtqueue_get_used>> return virtqueue_get_vring(vq)->used;
+ */
 const struct vring *virtqueue_get_vring(struct virtqueue *vq)
 {
 	return &to_vvq(vq)->vring;
diff --git a/include/uapi/linux/virtio_ring.h b/include/uapi/linux/virtio_ring.h
index 6d5d5faa989b..8fc1e5bd17b7 100644
--- a/include/uapi/linux/virtio_ring.h
+++ b/include/uapi/linux/virtio_ring.h
@@ -47,10 +47,33 @@
 /* The Host uses this in used->flags to advise the Guest: don't kick me when
  * you add a buffer.  It's unreliable, so it's simply an optimization.  Guest
  * will still kick if it's out of buffers. */
+/*
+ * 在以下使用VRING_USED_F_NO_NOTIFY:
+ *   - drivers/vhost/vhost.c|2252| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+ *   - drivers/vhost/vhost.c|2500| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+ *   - drivers/vhost/vhost.c|2502| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+ *   - drivers/vhost/vhost.c|2537| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+ *   - drivers/vhost/vhost.c|2539| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+ *   - drivers/vhost/vringh.c|545| <<__vringh_notify_disable>> VRING_USED_F_NO_NOTIFY)) {
+ *   - drivers/virtio/virtio_ring.c|672| <<virtqueue_kick_prepare>> needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
+ */
 #define VRING_USED_F_NO_NOTIFY	1
 /* The Guest uses this in avail->flags to advise the Host: don't interrupt me
  * when you consume a buffer.  It's unreliable, so it's simply an
  * optimization.  */
+/*
+ * 在以下使用VRING_AVAIL_F_NO_INTERRUPT:
+ *   - drivers/vhost/vhost.c|2400| <<vhost_notify>> return !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));
+ *   - drivers/vhost/vringh.c|474| <<__vringh_need_notify>> return (!(flags & VRING_AVAIL_F_NO_INTERRUPT));
+ *   - drivers/virtio/virtio_ring.c|916| <<virtqueue_get_buf_ctx>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+ *   - drivers/virtio/virtio_ring.c|992| <<virtqueue_disable_cb>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+ *   - drivers/virtio/virtio_ring.c|993| <<virtqueue_disable_cb>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+ *   - drivers/virtio/virtio_ring.c|1030| <<virtqueue_enable_cb_prepare>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+ *   - drivers/virtio/virtio_ring.c|1031| <<virtqueue_enable_cb_prepare>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+ *   - drivers/virtio/virtio_ring.c|1135| <<virtqueue_enable_cb_delayed>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+ *   - drivers/virtio/virtio_ring.c|1136| <<virtqueue_enable_cb_delayed>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+ *   - drivers/virtio/virtio_ring.c|1267| <<bool>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+ */
 #define VRING_AVAIL_F_NO_INTERRUPT	1
 
 /* We support indirect buffer descriptors */
@@ -137,7 +160,25 @@ struct vring {
  */
 /* We publish the used event index at the end of the available ring, and vice
  * versa. They are at the end for backwards compatibility. */
+/*
+ * called by:
+ *   - drivers/vhost/vringh.c|478| <<__vringh_need_notify>> err = getu16(vrh, &used_event, &vring_used_event(&vrh->vring));
+ *   - drivers/vhost/vringh.c|481| <<__vringh_need_notify>> &vring_used_event(&vrh->vring));
+ *   - drivers/virtio/virtio_ring.c|869| <<virtqueue_get_buf_ctx>> &vring_used_event(&vq->vring),
+ *   - drivers/virtio/virtio_ring.c|942| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|1029| <<virtqueue_enable_cb_delayed>> &vring_used_event(&vq->vring),
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|219| <<enable_call>> vring_used_event(&ring) = guest.last_used_idx;
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|326| <<call_used>> need = vring_need_event(vring_used_event(&ring),
+ */
 #define vring_used_event(vr) ((vr)->avail->ring[(vr)->num])
+/*
+ * called by:
+ *   - drivers/vhost/vringh.c|514| <<__vringh_notify_enable>> if (putu16(vrh, &vring_avail_event(&vrh->vring),
+ *   - drivers/vhost/vringh.c|517| <<__vringh_notify_enable>> &vring_avail_event(&vrh->vring));
+ *   - drivers/virtio/virtio_ring.c|669| <<virtqueue_kick_prepare>> needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev, vring_avail_event(&vq->vring)),
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|233| <<kick_available>> need = vring_need_event(vring_avail_event(&ring),
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|252| <<enable_kick>> vring_avail_event(&ring) = host.used_idx;
+ */
 #define vring_avail_event(vr) (*(__virtio16 *)&(vr)->used->ring[(vr)->num])
 
 static inline void vring_init(struct vring *vr, unsigned int num, void *p,
@@ -161,6 +202,14 @@ static inline unsigned vring_size(unsigned int num, unsigned long align)
 /* Assuming a given event_idx value from the other side, if
  * we have just incremented index from old to new_idx,
  * should we trigger an event? */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2424| <<vhost_notify>> return vring_need_event(vhost16_to_cpu(vq, event), new, old);
+ *   - drivers/vhost/vringh.c|489| <<__vringh_need_notify>> notify = vring_need_event(used_event,
+ *   - drivers/virtio/virtio_ring.c|669| <<virtqueue_kick_prepare>> needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev, vring_avail_event(&vq->vring)), new, old);
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|233| <<kick_available>> need = vring_need_event(vring_avail_event(&ring),
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|326| <<call_used>> need = vring_need_event(vring_used_event(&ring),
+ */
 static inline int vring_need_event(__u16 event_idx, __u16 new_idx, __u16 old)
 {
 	/* Note: Xen has similar logic for notification hold-off
-- 
2.17.1

