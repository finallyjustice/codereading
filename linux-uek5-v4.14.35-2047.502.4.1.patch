From c9daf37f671f21ae6e7432f350f4f2c0eb178ee6 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 13 Jun 2021 23:54:31 -0700
Subject: [PATCH 1/1] linux uek5 v4.14.35-2047.502.4.1

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/apic.h           |  19 ++
 arch/x86/include/asm/kvm_host.h       |  19 ++
 arch/x86/include/asm/x2apic.h         |   7 +
 arch/x86/include/uapi/asm/kvm_para.h  |  11 +
 arch/x86/kernel/apic/apic.c           | 111 +++++++++
 arch/x86/kernel/apic/probe_64.c       |  22 ++
 arch/x86/kernel/apic/x2apic_cluster.c |  19 ++
 arch/x86/kernel/apic/x2apic_phys.c    |  40 ++++
 arch/x86/kernel/kvm.c                 |  36 +++
 arch/x86/kvm/irq.c                    |   9 +
 arch/x86/kvm/irq_comm.c               |  58 +++++
 arch/x86/kvm/lapic.c                  | 282 +++++++++++++++++++++++
 arch/x86/kvm/lapic.h                  |  75 ++++++
 arch/x86/kvm/vmx/vmcs.h               |   8 +
 arch/x86/kvm/vmx/vmx.c                | 131 +++++++++++
 arch/x86/kvm/vmx/vmx.h                |  19 ++
 arch/x86/kvm/x86.c                    |  49 ++++
 arch/x86/kvm/x86.h                    |   5 +
 drivers/net/virtio_net.c              | 243 ++++++++++++++++++++
 drivers/vhost/net.c                   |  86 +++++++
 drivers/vhost/vhost.c                 | 276 ++++++++++++++++++++++
 drivers/vhost/vhost.h                 |  43 ++++
 drivers/virtio/virtio_ring.c          | 316 ++++++++++++++++++++++++++
 include/linux/hrtimer.h               |  27 +++
 include/linux/kvm_host.h              |  13 +-
 include/uapi/linux/kvm_para.h         |   6 +
 include/uapi/linux/vhost.h            |  25 ++
 include/uapi/linux/virtio_ring.h      |  82 +++++++
 kernel/sched/core.c                   |  18 ++
 kernel/sched/sched.h                  |   5 +
 kernel/sched/stats.h                  |   8 +
 kernel/smp.c                          |   4 +
 kernel/time/clockevents.c             |  29 +++
 kernel/time/hrtimer.c                 |  50 ++++
 kernel/time/tick-oneshot.c            |  13 ++
 kernel/time/tick-sched.c              |   4 +
 virt/kvm/eventfd.c                    |  18 ++
 virt/kvm/irqchip.c                    |  34 +++
 virt/kvm/kvm_main.c                   |  14 ++
 39 files changed, 2233 insertions(+), 1 deletion(-)

diff --git a/arch/x86/include/asm/apic.h b/arch/x86/include/asm/apic.h
index 97d686b821b5..f4fa1da4b794 100644
--- a/arch/x86/include/asm/apic.h
+++ b/arch/x86/include/asm/apic.h
@@ -280,6 +280,25 @@ struct apic {
 	int (*apic_id_registered)(void);
 
 	u32 irq_delivery_mode;
+	/*
+	 * 在以下使用apic->irq_dest_mode:
+	 *   - arch/x86/kernel/apic/apic_flat_64.c|155| <<global>> .irq_dest_mode = 1,
+	 *   - arch/x86/kernel/apic/apic_flat_64.c|250| <<global>> .irq_dest_mode = 0,
+	 *   - arch/x86/kernel/apic/apic_noop.c|122| <<global>> .irq_dest_mode = 1,
+	 *   - arch/x86/kernel/apic/apic_numachip.c|250| <<global>> .irq_dest_mode = 0,
+	 *   - arch/x86/kernel/apic/apic_numachip.c|301| <<global>> .irq_dest_mode = 0,
+	 *   - arch/x86/kernel/apic/bigsmp_32.c|136| <<global>> .irq_dest_mode = 0,
+	 *   - arch/x86/kernel/apic/probe_32.c|85| <<global>> .irq_dest_mode = 1,
+	 *   - arch/x86/kernel/apic/x2apic_cluster.c|243| <<global>> .irq_dest_mode = 1,
+	 *   - arch/x86/kernel/apic/x2apic_phys.c|133| <<global>> .irq_dest_mode = 0,
+	 *   - arch/x86/kernel/apic/x2apic_uv_x.c|725| <<global>> .irq_dest_mode = 0,
+	 *   - arch/x86/kernel/apic/io_apic.c|2906| <<mp_setup_entry>> entry->dest_mode = apic->irq_dest_mode;
+	 *   - arch/x86/kernel/apic/msi.c|38| <<__irq_msi_compose_msg>> ((apic->irq_dest_mode == 0) ?
+	 *   - arch/x86/platform/uv/uv_irq.c|39| <<uv_program_mmr>> entry->dest_mode = apic->irq_dest_mode;
+	 *   - drivers/iommu/amd_iommu.c|4125| <<irq_remapping_prepare_irte>> apic->irq_dest_mode, irq_cfg->vector,
+	 *   - drivers/iommu/amd_iommu.c|4377| <<amd_ir_set_vcpu_affinity>> irte->lo.fields_remap.dm = apic->irq_dest_mode;
+	 *   - drivers/iommu/intel_irq_remapping.c|1067| <<prepare_irte>> irte->dst_mode = apic->irq_dest_mode;
+	 */
 	u32 irq_dest_mode;
 
 	const struct cpumask *(*target_cpus)(void);
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 826fbad70c56..ba131ca42819 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -727,6 +727,14 @@ struct kvm_vcpu_arch {
 	unsigned long last_retry_addr;
 
 	struct {
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.halted:
+		 *   - arch/x86/kvm/x86.c|7781| <<vcpu_enter_guest>> vcpu->arch.apf.halted = true;
+		 *   - arch/x86/kvm/x86.c|8056| <<vcpu_block>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|8073| <<kvm_vcpu_running>> !vcpu->arch.apf.halted);
+		 *   - arch/x86/kvm/x86.c|8925| <<kvm_vcpu_reset>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|9890| <<kvm_arch_async_page_present>> vcpu->arch.apf.halted = false;
+		 */
 		bool halted;
 		gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)];
 		struct gfn_to_hva_cache data;
@@ -875,8 +883,19 @@ struct kvm_arch {
 
 	gpa_t wall_clock;
 
+	/*
+	 * 在以下使用kvm_arch->mwait_in_guest:
+	 *   - arch/x86/kvm/x86.c|4662| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X86_DISABLE_EXITS)>> kvm->arch.mwait_in_guest = true;
+	 *   - arch/x86/kvm/x86.h|330| <<kvm_mwait_in_guest>> return kvm->arch.mwait_in_guest;
+	 */
 	bool mwait_in_guest;
 	bool hlt_in_guest;
+	/*
+	 * 在以下使用kvm_arch->pause_in_guest:
+	 *   - arch/x86/kvm/vmx/vmx.c|6874| <<vmx_vm_init>> kvm->arch.pause_in_guest = true;
+	 *   - arch/x86/kvm/x86.c|4666| <<kvm_vm_ioctl_enable_cap>> kvm->arch.pause_in_guest = true;
+	 *   - arch/x86/kvm/x86.h|340| <<kvm_pause_in_guest>> return kvm->arch.pause_in_guest;
+	 */
 	bool pause_in_guest;
 
 	unsigned long irq_sources_bitmap;
diff --git a/arch/x86/include/asm/x2apic.h b/arch/x86/include/asm/x2apic.h
index 78ccf28d17db..320d5e53d11a 100644
--- a/arch/x86/include/asm/x2apic.h
+++ b/arch/x86/include/asm/x2apic.h
@@ -20,6 +20,13 @@ static int x2apic_apic_id_registered(void)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|33| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|76| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(dest, vector, apic->dest_logical);
+ *   - arch/x86/kernel/apic/x2apic_phys.c|45| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_PHYSICAL);
+ *   - arch/x86/kernel/apic/x2apic_phys.c|63| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(per_cpu(x86_cpu_to_apicid, query_cpu),
+ */
 static void
 __x2apic_send_IPI_dest(unsigned int apicid, int vector, unsigned int dest)
 {
diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index 21d5f0240595..aef2f287d653 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -59,7 +59,18 @@ struct kvm_steal_time {
 	__u32 pad[11];
 };
 
+/*
+ * 在以下使用KVM_VCPU_PREEMPTED:
+ *   - arch/x86/kernel/kvm.c|621| <<kvm_flush_tlb_others>> if ((state & KVM_VCPU_PREEMPTED)) {
+ *   - arch/x86/kernel/kvm.c|819| <<__kvm_vcpu_is_preempted>> return !!(src->preempted & KVM_VCPU_PREEMPTED);
+ *   - arch/x86/kvm/x86.c|3345| <<kvm_steal_time_set_preempted>> st->preempted = vcpu->arch.st.preempted = KVM_VCPU_PREEMPTED;
+ */
 #define KVM_VCPU_PREEMPTED          (1 << 0)
+/*
+ * 在以下使用KVM_VCPU_FLUSH_TLB:
+ *   - arch/x86/kernel/kvm.c|623| <<kvm_flush_tlb_others>> state | KVM_VCPU_FLUSH_TLB))
+ *   - arch/x86/kvm/x86.c|2457| <<record_steal_time>> if (xchg(&st->preempted, 0) & KVM_VCPU_FLUSH_TLB)
+ */
 #define KVM_VCPU_FLUSH_TLB          (1 << 1)
 
 #define KVM_CLOCK_PAIRING_WALLCLOCK 0
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index ee33f0951322..218c4609de72 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -59,6 +59,18 @@
 #include <asm/intel-family.h>
 #include <asm/irq_regs.h>
 
+/*
+ * https://tcbbd.moe/ref-and-spec/intel-sdm/sdm-basic-ch10/
+ *
+ * 通过LINT0和LINT1这两个引脚接收的本地中断
+ * 通过IOAPIC接收的外部中断，以及通过MSI方式收到的外部中断
+ * 其他CPU(甚至自己)发来的IPI
+ * APIC Timer产生的中断
+ * Performance Monitoring Counter产生的中断
+ * 温度传感器产生的中断
+ * APIC内部错误引发的中断
+ */
+
 unsigned int num_processors;
 
 unsigned disabled_cpus;
@@ -90,6 +102,15 @@ static unsigned int disabled_cpu_apicid __read_mostly = BAD_APICID;
  * This variable controls which CPUs receive external NMIs.  By default,
  * external NMIs are delivered only to the BSP.
  */
+/*
+ * 在以下使用apic_extnmi:
+ *   - arch/x86/kernel/apic/apic.c|1319| <<init_bsp_APIC>> if (apic_extnmi == APIC_EXTNMI_NONE)
+ *   - arch/x86/kernel/apic/apic.c|1560| <<setup_local_APIC>> if ((!cpu && apic_extnmi != APIC_EXTNMI_NONE) ||
+ *   - arch/x86/kernel/apic/apic.c|1561| <<setup_local_APIC>> apic_extnmi == APIC_EXTNMI_ALL)
+ *   - arch/x86/kernel/apic/apic.c|2795| <<apic_set_extnmi>> apic_extnmi = APIC_EXTNMI_ALL;
+ *   - arch/x86/kernel/apic/apic.c|2797| <<apic_set_extnmi>> apic_extnmi = APIC_EXTNMI_NONE;
+ *   - arch/x86/kernel/apic/apic.c|2799| <<apic_set_extnmi>> apic_extnmi = APIC_EXTNMI_BSP;
+ */
 static int apic_extnmi = APIC_EXTNMI_BSP;
 
 /*
@@ -328,6 +349,11 @@ int lapic_get_maxlvt(void)
  * We do reads before writes even if unnecessary, to get around the
  * P5 APIC double write bug.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/apic.c|524| <<lapic_timer_set_periodic_oneshot>> __setup_APIC_LVTT(lapic_timer_frequency, oneshot, 1);
+ *   - arch/x86/kernel/apic/apic.c|874| <<calibrate_APIC_clock>> __setup_APIC_LVTT(0xffffffff, 0, 0);
+ */
 static void __setup_APIC_LVTT(unsigned int clocks, int oneshot, int irqen)
 {
 	unsigned int lvtt_value, tmp_value;
@@ -468,6 +494,24 @@ static int lapic_next_event(unsigned long delta,
 	return 0;
 }
 
+/*
+ * 一个例子
+ * lapic_next_deadline
+ * tick_program_event
+ * hrtimer_interrupt
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * call_cpuidle
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * secondary_startup_64
+ *
+ * 在以下使用lapic_next_deadline():
+ *   - arch/x86/kernel/apic/apic.c|685| <<setup_APIC_timer>> levt->set_next_event = lapic_next_deadline;
+ */
 static int lapic_next_deadline(unsigned long delta,
 			       struct clock_event_device *evt)
 {
@@ -493,6 +537,11 @@ static int lapic_timer_shutdown(struct clock_event_device *evt)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/apic.c|553| <<lapic_timer_set_periodic>> return lapic_timer_set_periodic_oneshot(evt, false);
+ *   - arch/x86/kernel/apic/apic.c|558| <<lapic_timer_set_oneshot>> return lapic_timer_set_periodic_oneshot(evt, true);
+ */
 static inline int
 lapic_timer_set_periodic_oneshot(struct clock_event_device *evt, bool oneshot)
 {
@@ -644,6 +693,13 @@ static __init bool apic_validate_deadline_timer(void)
  * Setup the local APIC timer for this CPU. Copy the initialized values
  * of the boot CPU and register the clock event in the framework.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/apic.c|1035| <<setup_boot_APIC_clock>> setup_APIC_timer();
+ *   - arch/x86/kernel/apic/apic.c|1043| <<setup_boot_APIC_clock>> setup_APIC_timer();
+ *   - arch/x86/kernel/apic/apic.c|1055| <<setup_boot_APIC_clock>> setup_APIC_timer();
+ *   - arch/x86/kernel/apic/apic.c|1061| <<setup_secondary_APIC_clock>> setup_APIC_timer();
+ */
 static void setup_APIC_timer(void)
 {
 	struct clock_event_device *levt = this_cpu_ptr(&lapic_events);
@@ -654,9 +710,31 @@ static void setup_APIC_timer(void)
 		lapic_clockevent.rating = 150;
 	}
 
+	/*
+	 * 580 static struct clock_event_device lapic_clockevent = {
+	 * 581         .name                           = "lapic",
+	 * 582         .features                       = CLOCK_EVT_FEAT_PERIODIC |
+	 * 583                                           CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_C3STOP
+	 * 584                                           | CLOCK_EVT_FEAT_DUMMY,
+	 * 585         .shift                          = 32,
+	 * 586         .set_state_shutdown             = lapic_timer_shutdown,
+	 * 587         .set_state_periodic             = lapic_timer_set_periodic,
+	 * 588         .set_state_oneshot              = lapic_timer_set_oneshot,
+	 * 589         .set_state_oneshot_stopped      = lapic_timer_shutdown,
+	 * 590         .set_next_event                 = lapic_next_event,
+	 * 591         .broadcast                      = lapic_timer_broadcast,
+	 * 592         .rating                         = 100,
+	 * 593         .irq                            = -1,
+	 * 594 };
+	 * 595 static DEFINE_PER_CPU(struct clock_event_device, lapic_events);
+	 */
 	memcpy(levt, &lapic_clockevent, sizeof(*levt));
 	levt->cpumask = cpumask_of(smp_processor_id());
 
+	/*
+	 * # cat /sys/devices/system/clockevents/clockevent0/current_device 
+	 * lapic-deadline
+	 */
 	if (this_cpu_has(X86_FEATURE_TSC_DEADLINE_TIMER)) {
 		levt->name = "lapic-deadline";
 		levt->features &= ~(CLOCK_EVT_FEAT_PERIODIC |
@@ -1072,6 +1150,9 @@ static void local_apic_timer_interrupt(void)
 	 */
 	inc_irq_stat(apic_timer_irqs);
 
+	/*
+	 * hrtimer_interrupt()
+	 */
 	evt->event_handler(evt);
 }
 
@@ -1418,6 +1499,11 @@ static void apic_pending_intr_clear(void)
  * Used to setup local APIC while initializing BSP or bringing up APs.
  * Always called with preemption disabled.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/apic.c|1598| <<apic_ap_setup>> setup_local_APIC();
+ *   - arch/x86/kernel/apic/apic.c|2413| <<apic_bsp_setup>> setup_local_APIC();
+ */
 void setup_local_APIC(void)
 {
 	int cpu = smp_processor_id();
@@ -1434,6 +1520,10 @@ void setup_local_APIC(void)
 	 * SPIV. Soft disable it before doing further initialization.
 	 */
 	value = apic_read(APIC_SPIV);
+	/*
+	 * Allows software to temporarily enable or disable the local APIC.
+	 * 第8位控制
+	 */
 	value &= ~APIC_SPIV_APIC_ENABLED;
 	apic_write(APIC_SPIV, value);
 
@@ -1698,6 +1788,10 @@ static __init void x2apic_enable(void)
 	__x2apic_enable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/apic.c|1802| <<enable_IR_x2apic>> try_to_enable_x2apic(ir_stat);
+ */
 static __init void try_to_enable_x2apic(int remap_mode)
 {
 	if (x2apic_state == X2APIC_DISABLED)
@@ -1749,6 +1843,10 @@ static inline void try_to_enable_x2apic(int remap_mode) { }
 static inline void __x2apic_enable(void) { }
 #endif /* !CONFIG_X86_X2APIC */
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/probe_64.c|32| <<default_setup_apic_routing>> enable_IR_x2apic();
+ */
 void __init enable_IR_x2apic(void)
 {
 	unsigned long flags;
@@ -2430,6 +2528,11 @@ int __init apic_bsp_setup(bool upmode)
  * This initializes the IO-APIC and APIC hardware if this is
  * a UP kernel.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/apic.c|2471| <<up_late_init>> APIC_init_uniprocessor();
+ *   - arch/x86/kernel/smpboot.c|1338| <<native_smp_prepare_cpus>> if (APIC_init_uniprocessor())
+ */
 int __init APIC_init_uniprocessor(void)
 {
 	if (disable_apic) {
@@ -2791,6 +2894,14 @@ static int __init apic_set_extnmi(char *arg)
 	if (!arg)
 		return -EINVAL;
 
+	/*
+	 * bsp:  External NMI is delivered only to CPU 0
+	 * all:  External NMIs are broadcast to all CPUs as a
+	 *       backup of CPU 0
+	 * none: External NMI is masked for all CPUs. This is
+	 *       useful so that a dump capture kernel won't be
+	 *       shot down by NMI
+	 */
 	if (!strncmp("all", arg, 3))
 		apic_extnmi = APIC_EXTNMI_ALL;
 	else if (!strncmp("none", arg, 4))
diff --git a/arch/x86/kernel/apic/probe_64.c b/arch/x86/kernel/apic/probe_64.c
index c303054b90b5..111ff55e70fc 100644
--- a/arch/x86/kernel/apic/probe_64.c
+++ b/arch/x86/kernel/apic/probe_64.c
@@ -25,12 +25,30 @@
 /*
  * Check the APIC IDs in bios_cpu_apicid and choose the APIC mode.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1358| <<native_smp_prepare_cpus>> default_setup_apic_routing();
+ */
 void __init default_setup_apic_routing(void)
 {
 	struct apic **drv;
 
 	enable_IR_x2apic();
 
+	/*
+	 * 在以下使用apic_driver():
+	 *   - arch/x86/kernel/apic/apic_numachip.c|291| <<global>> apic_driver(apic_numachip1);
+	 *   - arch/x86/kernel/apic/apic_numachip.c|342| <<global>> apic_driver(apic_numachip2);
+	 *   - arch/x86/kernel/apic/bigsmp_32.c|198| <<global>> apic_driver(apic_bigsmp);
+	 *   - arch/x86/kernel/apic/probe_32.c|127| <<global>> apic_driver(apic_default);
+	 *   - arch/x86/kernel/apic/x2apic_cluster.c|283| <<global>> apic_driver(apic_x2apic_cluster);
+	 *   - arch/x86/kernel/apic/x2apic_phys.c|173| <<global>> apic_driver(apic_x2apic_phys);
+	 *   - arch/x86/kernel/apic/x2apic_uv_x.c|1759| <<global>> apic_driver(apic_x2apic_uv_x);
+	 *   - arch/x86/xen/apic.c|228| <<global>> apic_driver(xen_pv_apic);
+	 *
+	 * [    0.045605] x2apic enabled
+	 * [    0.046007] Switched APIC routing to physical x2apic.
+	 */
 	for (drv = __apicdrivers; drv < __apicdrivers_end; drv++) {
 		if ((*drv)->probe && (*drv)->probe()) {
 			if (apic != *drv) {
@@ -53,6 +71,10 @@ void apic_send_IPI_self(int vector)
 	__default_send_IPI_shortcut(APIC_DEST_SELF, vector, APIC_DEST_PHYSICAL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/acpi/boot.c|159| <<acpi_parse_madt>> default_acpi_madt_oem_check(madt->header.oem_id,
+ */
 int __init default_acpi_madt_oem_check(char *oem_id, char *oem_table_id)
 {
 	struct apic **drv;
diff --git a/arch/x86/kernel/apic/x2apic_cluster.c b/arch/x86/kernel/apic/x2apic_cluster.c
index ec6a004b0f55..08d42d292084 100644
--- a/arch/x86/kernel/apic/x2apic_cluster.c
+++ b/arch/x86/kernel/apic/x2apic_cluster.c
@@ -12,7 +12,26 @@
 #include <asm/x2apic.h>
 
 static DEFINE_PER_CPU(u32, x86_cpu_to_logical_apicid);
+/*
+ * 在以下使用cpus_in_cluster:
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|64| <<__x2apic_send_IPI_mask>> cpus_in_cluster_ptr = per_cpu(cpus_in_cluster, cpu);
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|143| <<init_x2apic_ldr>> cpumask_set_cpu(this_cpu, per_cpu(cpus_in_cluster, this_cpu));
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|147| <<init_x2apic_ldr>> cpumask_set_cpu(this_cpu, per_cpu(cpus_in_cluster, cpu));
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|148| <<init_x2apic_ldr>> cpumask_set_cpu(cpu, per_cpu(cpus_in_cluster, this_cpu));
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|157| <<x2apic_prepare_cpu>> if (!zalloc_cpumask_var(&per_cpu(cpus_in_cluster, cpu), GFP_KERNEL))
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|161| <<x2apic_prepare_cpu>> free_cpumask_var(per_cpu(cpus_in_cluster, cpu));
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|175| <<x2apic_dead_cpu>> cpumask_clear_cpu(this_cpu, per_cpu(cpus_in_cluster, cpu));
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|176| <<x2apic_dead_cpu>> cpumask_clear_cpu(cpu, per_cpu(cpus_in_cluster, this_cpu));
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|178| <<x2apic_dead_cpu>> free_cpumask_var(per_cpu(cpus_in_cluster, this_cpu));
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|197| <<x2apic_cluster_probe>> cpumask_set_cpu(cpu, per_cpu(cpus_in_cluster, cpu));
+ */
 static DEFINE_PER_CPU(cpumask_var_t, cpus_in_cluster);
+/*
+ * 在以下使用ipi_mask:
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|55| <<__x2apic_send_IPI_mask>> ipi_mask_ptr = this_cpu_cpumask_var_ptr(ipi_mask);
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|160| <<x2apic_prepare_cpu>> if (!zalloc_cpumask_var(&per_cpu(ipi_mask, cpu), GFP_KERNEL)) {
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|179| <<x2apic_dead_cpu>> free_cpumask_var(per_cpu(ipi_mask, this_cpu));
+ */
 static DEFINE_PER_CPU(cpumask_var_t, ipi_mask);
 
 static int x2apic_acpi_madt_oem_check(char *oem_id, char *oem_table_id)
diff --git a/arch/x86/kernel/apic/x2apic_phys.c b/arch/x86/kernel/apic/x2apic_phys.c
index b94d35320f85..18a575f62240 100644
--- a/arch/x86/kernel/apic/x2apic_phys.c
+++ b/arch/x86/kernel/apic/x2apic_phys.c
@@ -9,6 +9,14 @@
 #include <asm/smp.h>
 #include <asm/x2apic.h>
 
+/*
+ * 在以下使用x2apic_phys:
+ *   - arch/x86/kernel/apic/apic.c|1739| <<try_to_enable_x2apic>> x2apic_phys = 1;
+ *   - arch/x86/kernel/apic/x2apic_phys.c|18| <<set_x2apic_phys_mode>> x2apic_phys = 1;
+ *   - arch/x86/kernel/apic/x2apic_phys.c|37| <<x2apic_acpi_madt_oem_check>> return x2apic_enabled() && (x2apic_phys || x2apic_fadt_phys());
+ *   - arch/x86/kernel/apic/x2apic_phys.c|96| <<x2apic_phys_probe>> if (x2apic_mode && (x2apic_phys || x2apic_fadt_phys()))
+ *   - arch/x86/kernel/cpu/mshyperv.c|340| <<ms_hyperv_init_platform>> x2apic_phys = 1;
+ */
 int x2apic_phys;
 
 static struct apic apic_x2apic_phys;
@@ -82,6 +90,13 @@ static void x2apic_send_IPI_allbutself(int vector)
 	__x2apic_send_IPI_mask(cpu_online_mask, vector, APIC_DEST_ALLBUT);
 }
 
+/*
+ * kvm会修改为以下:
+ * apic->send_IPI_mask = kvm_send_ipi_mask;
+ * apic->send_IPI_mask_allbutself = kvm_send_ipi_mask_allbutself;
+ * apic->send_IPI_allbutself = kvm_send_ipi_allbutself;
+ * apic->send_IPI_all = kvm_send_ipi_all;
+ */
 static void x2apic_send_IPI_all(int vector)
 {
 	__x2apic_send_IPI_mask(cpu_online_mask, vector, APIC_DEST_ALLINC);
@@ -99,6 +114,31 @@ static int x2apic_phys_probe(void)
 	return apic == &apic_x2apic_phys;
 }
 
+/*
+ * 在以下使用apic->irq_dest_mode:
+ *   - arch/x86/kernel/apic/apic_flat_64.c|155| <<global>> .irq_dest_mode = 1,
+ *   - arch/x86/kernel/apic/apic_flat_64.c|250| <<global>> .irq_dest_mode = 0,
+ *   - arch/x86/kernel/apic/apic_noop.c|122| <<global>> .irq_dest_mode = 1,
+ *   - arch/x86/kernel/apic/apic_numachip.c|250| <<global>> .irq_dest_mode = 0,
+ *   - arch/x86/kernel/apic/apic_numachip.c|301| <<global>> .irq_dest_mode = 0,
+ *   - arch/x86/kernel/apic/bigsmp_32.c|136| <<global>> .irq_dest_mode = 0,
+ *   - arch/x86/kernel/apic/probe_32.c|85| <<global>> .irq_dest_mode = 1,
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|243| <<global>> .irq_dest_mode = 1,
+ *   - arch/x86/kernel/apic/x2apic_phys.c|133| <<global>> .irq_dest_mode = 0,
+ *   - arch/x86/kernel/apic/x2apic_uv_x.c|725| <<global>> .irq_dest_mode = 0,
+ *   - arch/x86/kernel/apic/io_apic.c|2906| <<mp_setup_entry>> entry->dest_mode = apic->irq_dest_mode;
+ *   - arch/x86/kernel/apic/msi.c|38| <<__irq_msi_compose_msg>> ((apic->irq_dest_mode == 0) ?
+ *   - arch/x86/platform/uv/uv_irq.c|39| <<uv_program_mmr>> entry->dest_mode = apic->irq_dest_mode;
+ *   - drivers/iommu/amd_iommu.c|4125| <<irq_remapping_prepare_irte>> apic->irq_dest_mode, irq_cfg->vector,
+ *   - drivers/iommu/amd_iommu.c|4377| <<amd_ir_set_vcpu_affinity>> irte->lo.fields_remap.dm = apic->irq_dest_mode;
+ *   - drivers/iommu/intel_irq_remapping.c|1067| <<prepare_irte>> irte->dst_mode = apic->irq_dest_mode;
+ *
+ * kvm会修改为以下:
+ * apic->send_IPI_mask = kvm_send_ipi_mask;
+ * apic->send_IPI_mask_allbutself = kvm_send_ipi_mask_allbutself;
+ * apic->send_IPI_allbutself = kvm_send_ipi_allbutself;
+ * apic->send_IPI_all = kvm_send_ipi_all;
+ */
 static struct apic apic_x2apic_phys __ro_after_init = {
 
 	.name				= "physical x2apic",
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index d41230f9c67f..7d6c7f6fb3b2 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -449,6 +449,12 @@ static void __init sev_map_percpu_data(void)
 #ifdef CONFIG_SMP
 #define KVM_IPI_CLUSTER_SIZE	(2 * BITS_PER_LONG)
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|505| <<kvm_send_ipi_mask>> __send_ipi_mask(mask, vector);
+ *   - arch/x86/kernel/kvm.c|517| <<kvm_send_ipi_mask_allbutself>> __send_ipi_mask(local_mask, vector);
+ *   - arch/x86/kernel/kvm.c|527| <<kvm_send_ipi_all>> __send_ipi_mask(cpu_online_mask, vector);
+ */
 static void __send_ipi_mask(const struct cpumask *mask, int vector)
 {
 	unsigned long flags;
@@ -590,16 +596,33 @@ static void __init kvm_apf_trap_init(void)
 	update_intr_gate(X86_TRAP_PF, async_page_fault);
 }
 
+/*
+ * 在以下使用__pv_tlb_mask:
+ *   - arch/x86/kernel/kvm.c|611| <<kvm_flush_tlb_others>> struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_tlb_mask);
+ *   - arch/x86/kernel/kvm.c|764| <<kvm_setup_pv_tlb_flush>> zalloc_cpumask_var_node(per_cpu_ptr(&__pv_tlb_mask, cpu),
+ */
 static DEFINE_PER_CPU(cpumask_var_t, __pv_tlb_mask);
 
+/*
+ * 在以下使用kvm_flush_tlb_others():
+ *   - arch/x86/kernel/kvm.c|643| <<kvm_guest_init>> pv_mmu_ops.flush_tlb_others = kvm_flush_tlb_others;
+ */
 static void kvm_flush_tlb_others(const struct cpumask *cpumask,
 			const struct flush_tlb_info *info)
 {
 	u8 state;
 	int cpu;
 	struct kvm_steal_time *src;
+	/*
+	 * 在以下使用__pv_tlb_mask:
+	 *   - arch/x86/kernel/kvm.c|611| <<kvm_flush_tlb_others>> struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_tlb_mask);
+	 *   - arch/x86/kernel/kvm.c|764| <<kvm_setup_pv_tlb_flush>> zalloc_cpumask_var_node(per_cpu_ptr(&__pv_tlb_mask, cpu),
+	 */
 	struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_tlb_mask);
 
+	/*
+	 * 从cpumask copy到 flushmask
+	 */
 	cpumask_copy(flushmask, cpumask);
 	/*
 	 * We have to call flush only on online vCPUs. And
@@ -608,6 +631,19 @@ static void kvm_flush_tlb_others(const struct cpumask *cpumask,
 	for_each_cpu(cpu, flushmask) {
 		src = &per_cpu(steal_time, cpu);
 		state = READ_ONCE(src->preempted);
+		/*
+		 * 在以下使用KVM_VCPU_PREEMPTED:
+		 *   - arch/x86/kernel/kvm.c|621| <<kvm_flush_tlb_others>> if ((state & KVM_VCPU_PREEMPTED)) {
+		 *   - arch/x86/kernel/kvm.c|819| <<__kvm_vcpu_is_preempted>> return !!(src->preempted & KVM_VCPU_PREEMPTED);
+		 *   - arch/x86/kvm/x86.c|3345| <<kvm_steal_time_set_preempted>> st->preempted = vcpu->arch.st.preempted = KVM_VCPU_PREEMPTED;
+		 *
+		 * 在以下使用KVM_VCPU_FLUSH_TLB:
+		 *   - arch/x86/kernel/kvm.c|623| <<kvm_flush_tlb_others>> state | KVM_VCPU_FLUSH_TLB))
+		 *   - arch/x86/kvm/x86.c|2457| <<record_steal_time>> if (xchg(&st->preempted, 0) & KVM_VCPU_FLUSH_TLB)
+		 *
+		 * #define KVM_VCPU_PREEMPTED          (1 << 0)
+		 * #define KVM_VCPU_FLUSH_TLB          (1 << 1)
+		 */
 		if ((state & KVM_VCPU_PREEMPTED)) {
 			if (try_cmpxchg(&src->preempted, &state,
 					state | KVM_VCPU_FLUSH_TLB))
diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index faa264822cee..02507c8d684c 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -31,6 +31,11 @@
  * check if there are pending timer events
  * to be processed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8088| <<vcpu_run>> if (kvm_cpu_has_pending_timer(vcpu))
+ *   - virt/kvm/kvm_main.c|2444| <<kvm_vcpu_check_block>> if (kvm_cpu_has_pending_timer(vcpu))
+ */
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu))
@@ -160,6 +165,10 @@ int kvm_cpu_get_interrupt(struct kvm_vcpu *v)
 }
 EXPORT_SYMBOL_GPL(kvm_cpu_get_interrupt);
 
+/*
+ * called by(处理kvm_cpu_has_pending_timer()):
+ *   - arch/x86/kvm/x86.c|8074| <<vcpu_run>> kvm_inject_pending_timer_irqs(vcpu);
+ */
 void kvm_inject_pending_timer_irqs(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu))
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 4d000aea05e0..1bcdf2c0f3ba 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -55,6 +55,15 @@ static int kvm_set_ioapic_irq(struct kvm_kernel_irq_routing_entry *e,
 				line_status);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|331| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+ *   - arch/x86/kvm/ioapic.c|375| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe,
+ *   - arch/x86/kvm/ioapic.c|379| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+ *   - arch/x86/kvm/irq_comm.c|192| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1249| <<apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+ *   - arch/x86/kvm/x86.c|7133| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+ */
 int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, struct dest_map *dest_map)
 {
@@ -112,6 +121,41 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 	return r;
 }
 
+/*
+ * # sudo /usr/share/bcc/tools/trace -t -C  'kvm_set_msi_irq'
+ * TIME     CPU PID     TID     COMM            FUNC
+ * 1.409018 4   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.430408 4   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.557921 1   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.594878 5   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.624964 4   23372   23372   vhost-23357     kvm_set_msi_irq
+ *
+ * 9.586209 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ * 9.586283 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ * 9.586399 5   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 9.586470 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ * 9.586482 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ *
+ * kvm_set_msi_irq
+ * irqfd_wakeup
+ * __wake_up_common
+ * __wake_up_locked_key
+ * eventfd_signal
+ * vhost_signal
+ * vhost_add_used_and_signal_n
+ * handle_rx
+ * handle_rx_net
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ *
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|155| <<kvm_set_msi>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|187| <<kvm_arch_set_irq_inatomic>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|428| <<kvm_scan_ioapic_routes>> kvm_set_msi_irq(vcpu->kvm, entry, &irq);
+ *   - arch/x86/kvm/svm.c|5309| <<get_pi_vcpu_info>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/vmx/vmx.c|7543| <<vmx_update_pi_irte>> kvm_set_msi_irq(kvm, e, &irq);
+ */
 void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 		     struct kvm_lapic_irq *irq)
 {
@@ -141,6 +185,11 @@ static inline bool kvm_msi_route_invalid(struct kvm *kvm,
 	return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
 }
 
+/*
+ * called by
+ *   - arch/x86/kvm/irq_comm.c|359| <<kvm_set_routing_entry>> e->set = kvm_set_msi;
+ *   - virt/kvm/irqchip.c|78| <<kvm_send_userspace_msi>> return kvm_set_msi(&route, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false);
+ */
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 		struct kvm *kvm, int irq_source_id, int level, bool line_status)
 {
@@ -168,6 +217,11 @@ static int kvm_hv_set_sint(struct kvm_kernel_irq_routing_entry *e,
 	return kvm_hv_synic_set_irq(kvm, e->hv_sint.vcpu, e->hv_sint.sint);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|178| <<kvm_arch_set_irq_inatomic>> int __attribute__((weak)) kvm_arch_set_irq_inatomic(
+ *   - virt/kvm/eventfd.c|212| <<irqfd_wakeup>> if (kvm_arch_set_irq_inatomic(&irq, kvm,
+ */
 int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 			      struct kvm *kvm, int irq_source_id, int level,
 			      bool line_status)
@@ -279,6 +333,10 @@ bool kvm_arch_can_set_irq_routing(struct kvm *kvm)
 	return irqchip_in_kernel(kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|180| <<setup_routing_entry>> r = kvm_set_routing_entry(kvm, e, ue);
+ */
 int kvm_set_routing_entry(struct kvm *kvm,
 			  struct kvm_kernel_irq_routing_entry *e,
 			  const struct kvm_irq_routing_entry *ue)
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index baf4a218d186..9688aad1688a 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -44,6 +44,10 @@
 #include "cpuid.h"
 #include "hyperv.h"
 
+/*
+ * https://www.codeleading.com/article/91825014645/
+ */
+
 #ifndef CONFIG_X86_64
 #define mod_64(x, y) ((x) - (y) * div64_u64(x, y))
 #else
@@ -387,6 +391,11 @@ static u8 count_vectors(void *bitmap)
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|424| <<kvm_apic_update_irr>> return __kvm_apic_update_irr(pir, apic->regs, max_irr);
+ *   - arch/x86/kvm/vmx/nested.c|3368| <<vmx_complete_nested_posted_interrupt>> __kvm_apic_update_irr(vmx->nested.pi_desc->pir,
+ */
 bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 {
 	u32 i, vec;
@@ -417,6 +426,10 @@ bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 }
 EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6087| <<vmx_sync_pir_to_irr>> kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
+ */
 bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -425,6 +438,14 @@ bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|460| <<apic_clear_irr>> apic_find_highest_irr(apic));
+ *   - arch/x86/kvm/lapic.c|548| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|679| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+ *   - arch/x86/kvm/lapic.c|2482| <<kvm_apic_set_state>> apic_find_highest_irr(apic));
+ *   - arch/x86/kvm/lapic.c|2599| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+ */
 static inline int apic_search_irr(struct kvm_lapic *apic)
 {
 	return find_highest_vector(apic->regs + APIC_IRR);
@@ -553,6 +574,17 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|658| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|1450| <<kvm_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq_comm.c|97| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq_comm.c|119| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|614| <<kvm_pv_send_ipi>> count += kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|627| <<kvm_pv_send_ipi>> count += kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|986| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|999| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
@@ -1037,6 +1069,11 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
  * Add a pending IRQ into lapic.
  * Return 1 if successfully added and 0 if discarded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|561| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector,
+ *   - arch/x86/kvm/lapic.c|2267| <<kvm_apic_local_deliver>> return __apic_accept_irq(apic, mode, vector, 1, trig_mode,
+ */
 static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map)
@@ -1214,6 +1251,10 @@ void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_set_eoi_accelerated);
 
+/*
+ * 处理APIC_ICR:
+ *   - arch/x86/kvm/lapic.c|1904| <<kvm_lapic_reg_write>> apic_send_ipi(apic);
+ */
 static void apic_send_ipi(struct kvm_lapic *apic)
 {
 	u32 icr_low = kvm_lapic_get_reg(apic, APIC_ICR);
@@ -1455,6 +1496,14 @@ static void apic_update_lvtt(struct kvm_lapic *apic)
 	u32 timer_mode = kvm_lapic_get_reg(apic, APIC_LVTT) &
 			apic->lapic_timer.timer_mode_mask;
 
+	/*
+	 * 在以下使用kvm_timer->timer_mode:
+	 *   - arch/x86/kvm/lapic.c|316| <<apic_lvtt_oneshot>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_ONESHOT;
+	 *   - arch/x86/kvm/lapic.c|321| <<apic_lvtt_period>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_PERIODIC;
+	 *   - arch/x86/kvm/lapic.c|326| <<apic_lvtt_tscdeadline>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_TSCDEADLINE;
+	 *   - arch/x86/kvm/lapic.c|1499| <<apic_update_lvtt>> if (apic->lapic_timer.timer_mode != timer_mode) {
+	 *   - arch/x86/kvm/lapic.c|1507| <<apic_update_lvtt>> apic->lapic_timer.timer_mode = timer_mode;
+	 */
 	if (apic->lapic_timer.timer_mode != timer_mode) {
 		if (apic_lvtt_tscdeadline(apic) != (timer_mode ==
 				APIC_LVT_TIMER_TSCDEADLINE)) {
@@ -1468,6 +1517,39 @@ static void apic_update_lvtt(struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * 例子们
+ * apic_timer_expired
+ * handle_preemption_timer
+ * vmx_handle_exit
+ * __dta_vcpu_enter_guest_1388
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * apic_timer_expired
+ * __hrtimer_run_queues
+ * hrtimer_interrupt
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * call_cpuidle
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * secondary_startup_64
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1622| <<start_sw_tscdeadline>> apic_timer_expired(apic);
+ *   - arch/x86/kvm/lapic.c|1714| <<start_sw_period>> apic_timer_expired(apic);
+ *   - arch/x86/kvm/lapic.c|1773| <<start_hv_timer>> apic_timer_expired(apic);
+ *   - arch/x86/kvm/lapic.c|1816| <<kvm_lapic_expired_hv_timer>> apic_timer_expired(apic);
+ *   - arch/x86/kvm/lapic.c|2324| <<apic_timer_fn>> apic_timer_expired(apic);
+ */
 static void apic_timer_expired(struct kvm_lapic *apic)
 {
 	struct kvm_vcpu *vcpu = apic->vcpu;
@@ -1557,6 +1639,10 @@ void wait_lapic_expire(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1802| <<start_sw_timer>> start_sw_tscdeadline(apic);
+ */
 static void start_sw_tscdeadline(struct kvm_lapic *apic)
 {
 	u64 guest_tsc, tscdeadline = apic->lapic_timer.tscdeadline;
@@ -1578,6 +1664,18 @@ static void start_sw_tscdeadline(struct kvm_lapic *apic)
 		ns = (tscdeadline - guest_tsc) * 1000000ULL;
 		do_div(ns, this_tsc_khz);
 		expire = ktime_add_ns(now, ns);
+		/*
+		 * 在以下使用lapic_timer_advance_ns:
+		 *   - arch/x86/kvm/x86.c|142| <<global>> module_param(lapic_timer_advance_ns, uint, S_IRUGO | S_IWUSR);
+		 *   - arch/x86/kvm/lapic.c|1584| <<wait_lapic_expire>> nsec_to_cycles(vcpu, lapic_timer_advance_ns)));
+		 *   - arch/x86/kvm/lapic.c|1591| <<wait_lapic_expire>> lapic_timer_advance_ns -= min((unsigned int )ns,
+		 *   - arch/x86/kvm/lapic.c|1592| <<wait_lapic_expire>> lapic_timer_advance_ns / LAPIC_TIMER_ADVANCE_ADJUST_STEP);
+		 *   - arch/x86/kvm/lapic.c|1597| <<wait_lapic_expire>> lapic_timer_advance_ns += min((unsigned int )ns,
+		 *   - arch/x86/kvm/lapic.c|1598| <<wait_lapic_expire>> lapic_timer_advance_ns / LAPIC_TIMER_ADVANCE_ADJUST_STEP);
+		 *   - arch/x86/kvm/lapic.c|1630| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, lapic_timer_advance_ns);
+		 *   - arch/x86/kvm/vmx/vmx.c|7259| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu, lapic_timer_advance_ns);
+		 *   - arch/x86/kvm/x86.c|7926| <<vcpu_enter_guest>> if (lapic_timer_advance_ns)
+		 */
 		expire = ktime_sub_ns(expire, lapic_timer_advance_ns);
 		hrtimer_start(&apic->lapic_timer.timer,
 				expire, HRTIMER_MODE_ABS_PINNED);
@@ -1613,6 +1711,10 @@ static void update_target_expiration(struct kvm_lapic *apic, uint32_t old_diviso
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2024| <<start_apic_timer>> && !set_target_expiration(apic))
+ */
 static bool set_target_expiration(struct kvm_lapic *apic)
 {
 	ktime_t now;
@@ -1692,15 +1794,47 @@ bool kvm_lapic_hv_timer_in_use(struct kvm_vcpu *vcpu)
 	if (!lapic_in_kernel(vcpu))
 		return false;
 
+	/*
+	 * 在以下修改kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1765| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1787| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1756| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1763| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1850| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1880| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1911| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1921| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
 }
 EXPORT_SYMBOL_GPL(kvm_lapic_hv_timer_in_use);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1851| <<start_sw_timer>> cancel_hv_timer(apic);
+ *   - arch/x86/kvm/lapic.c|1883| <<kvm_lapic_expired_hv_timer>> cancel_hv_timer(apic);
+ */
 static void cancel_hv_timer(struct kvm_lapic *apic)
 {
 	WARN_ON(preemptible());
 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	/*
+	 * vmx_cancel_hv_timer()
+	 */
 	kvm_x86_ops->cancel_hv_timer(apic->vcpu);
+	/*
+	 * 在以下修改kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1765| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1787| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1756| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1763| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1850| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1880| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1911| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1921| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	apic->lapic_timer.hv_timer_in_use = false;
 }
 
@@ -1719,6 +1853,9 @@ static bool start_hv_timer(struct kvm_lapic *apic)
 	if (!ktimer->tscdeadline)
 		return false;
 
+	/*
+	 * vmx_set_hv_timer()
+	 */
 	r = kvm_x86_ops->set_hv_timer(apic->vcpu, ktimer->tscdeadline);
 	if (r < 0)
 		return false;
@@ -1741,11 +1878,63 @@ static bool start_hv_timer(struct kvm_lapic *apic)
 	return true;
 }
 
+/*
+ * start_sw_timer
+ * start_apic_timer
+ * kvm_set_lapic_tscdeadline_msr
+ * kvm_set_msr_common
+ * vmx_set_msr
+ * kvm_set_msr
+ * handle_wrmsr
+ * __dta_vmx_handle_exit_306
+ * __dta_vcpu_enter_guest_1388
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * start_sw_timer
+ * kvm_lapic_switch_to_hv_timer
+ * vmx_post_block
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * start_sw_timer
+ * vmx_pre_block
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1810| <<restart_apic_timer>> start_sw_timer(apic);
+ *   - arch/x86/kvm/lapic.c|1852| <<kvm_lapic_switch_to_sw_timer>> start_sw_timer(apic);
+ */
 static void start_sw_timer(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
 
 	WARN_ON(preemptible());
+	/*
+	 * 在以下修改kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1765| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1787| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1756| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1763| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1850| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1880| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1911| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1921| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	if (apic->lapic_timer.hv_timer_in_use)
 		cancel_hv_timer(apic);
 	if (!apic_lvtt_period(apic) && atomic_read(&ktimer->pending))
@@ -1758,6 +1947,14 @@ static void start_sw_timer(struct kvm_lapic *apic)
 	trace_kvm_hv_timer_state(apic->vcpu->vcpu_id, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1893| <<kvm_lapic_expired_hv_timer>> restart_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|1906| <<kvm_lapic_switch_to_hv_timer>> restart_apic_timer(vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|1931| <<kvm_lapic_restart_hv_timer>> restart_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|1942| <<start_apic_timer>> restart_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2080| <<kvm_lapic_reg_write>> restart_apic_timer(apic);
+ */
 static void restart_apic_timer(struct kvm_lapic *apic)
 {
 	preempt_disable();
@@ -1766,12 +1963,35 @@ static void restart_apic_timer(struct kvm_lapic *apic)
 	preempt_enable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5485| <<handle_preemption_timer>> kvm_lapic_expired_hv_timer(vcpu);
+ */
 void kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch
+	 *    -> struct kvm_lapic *apic;
+	 *       -> struct kvm_timer lapic_timer;
+	 *          -> struct hrtimer timer;
+	 */
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	preempt_disable();
 	/* If the preempt notifier has already run, it also called apic_timer_expired */
+	/*
+	 * 在以下修改kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1765| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1787| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1756| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1763| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1850| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1880| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1911| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1921| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	if (!apic->lapic_timer.hv_timer_in_use)
 		goto out;
 	WARN_ON(swait_active(&vcpu->wq));
@@ -1787,18 +2007,38 @@ void kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_lapic_expired_hv_timer);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7489| <<vmx_post_block>> kvm_lapic_switch_to_hv_timer(vcpu);
+ */
 void kvm_lapic_switch_to_hv_timer(struct kvm_vcpu *vcpu)
 {
 	restart_apic_timer(vcpu->arch.apic);
 }
 EXPORT_SYMBOL_GPL(kvm_lapic_switch_to_hv_timer);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7470| <<vmx_pre_block>> kvm_lapic_switch_to_sw_timer(vcpu);
+ */
 void kvm_lapic_switch_to_sw_timer(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	preempt_disable();
 	/* Possibly the TSC deadline timer is not enabled yet */
+	/*
+	 * 在以下修改kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1765| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1787| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1756| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1763| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1850| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1880| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1911| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1921| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	if (apic->lapic_timer.hv_timer_in_use)
 		start_sw_timer(apic);
 	preempt_enable();
@@ -1813,6 +2053,12 @@ void kvm_lapic_restart_hv_timer(struct kvm_vcpu *vcpu)
 	restart_apic_timer(apic);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2099| <<kvm_lapic_reg_write>> start_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2249| <<kvm_set_lapic_tscdeadline_msr>> start_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2650| <<kvm_apic_set_state>> start_apic_timer(apic);
+ */
 static void start_apic_timer(struct kvm_lapic *apic)
 {
 	atomic_set(&apic->lapic_timer.pending, 0);
@@ -2085,6 +2331,10 @@ u64 kvm_get_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu)
 	return apic->lapic_timer.tscdeadline;
 }
 
+/*
+ * 处理MSR_IA32_TSCDEADLINE:
+ *   - arch/x86/kvm/x86.c|2564| <<kvm_set_msr_common>> kvm_set_lapic_tscdeadline_msr(vcpu, data);
+ */
 void kvm_set_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2093,6 +2343,9 @@ void kvm_set_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu, u64 data)
 			apic_lvtt_period(apic))
 		return;
 
+	/*
+	 * cancel a timer and wait for the handler to finish.
+	 */
 	hrtimer_cancel(&apic->lapic_timer.timer);
 	apic->lapic_timer.tscdeadline = data;
 	start_apic_timer(apic);
@@ -2241,6 +2494,10 @@ static bool lapic_is_periodic(struct kvm_lapic *apic)
 	return apic_lvtt_period(apic);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|37| <<kvm_cpu_has_pending_timer>> return apic_has_pending_timer(vcpu);
+ */
 int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2251,6 +2508,12 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2392| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/lapic.c|2493| <<kvm_inject_apic_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/pmu.c|305| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -2260,6 +2523,11 @@ int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 		vector = reg & APIC_VECTOR_MASK;
 		mode = reg & APIC_MODE_MASK;
 		trig_mode = reg & APIC_LVT_LEVEL_TRIGGER;
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/lapic.c|561| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector,
+		 *   - arch/x86/kvm/lapic.c|2267| <<kvm_apic_local_deliver>> return __apic_accept_irq(apic, mode, vector, 1, trig_mode,
+		 */
 		return __apic_accept_irq(apic, mode, vector, 1, trig_mode,
 					NULL);
 	}
@@ -2279,6 +2547,10 @@ static const struct kvm_io_device_ops apic_mmio_ops = {
 	.write    = apic_mmio_write,
 };
 
+/*
+ * 在以下使用apic_timer_fn():
+ *   - arch/x86/kvm/lapic.c|2357| <<kvm_create_lapic>> apic->lapic_timer.timer.function = apic_timer_fn;
+ */
 static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 {
 	struct kvm_timer *ktimer = container_of(data, struct kvm_timer, timer);
@@ -2334,6 +2606,12 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|96| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+ *   - arch/x86/kvm/irq.c|122| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+ *   - arch/x86/kvm/lapic.c|2506| <<kvm_get_apic_interrupt>> int vector = kvm_apic_has_interrupt(vcpu);
+ */
 int kvm_apic_has_interrupt(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2359,6 +2637,10 @@ int kvm_apic_accept_pic_intr(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|166| <<kvm_inject_pending_timer_irqs>> kvm_inject_apic_timer_irqs(vcpu);
+ */
 void kvm_inject_apic_timer_irqs(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index ff6ef9c3d760..375097ed39d0 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -26,12 +26,82 @@ enum lapic_mode {
 struct kvm_timer {
 	struct hrtimer timer;
 	s64 period; 				/* unit: ns */
+	/*
+	 * 在以下设置kvm_timer->timer_mode:
+	 *   - arch/x86/kvm/lapic.c|1702| <<update_target_expiration>> apic->lapic_timer.target_expiration = ktime_add_ns(now, ns_remaining_new);
+	 *   - arch/x86/kvm/lapic.c|1734| <<set_target_expiration>> apic->lapic_timer.target_expiration = ktime_add_ns(now, apic->lapic_timer.period);
+	 *   - arch/x86/kvm/lapic.c|1752| <<advance_periodic_target_expiration>> apic->lapic_timer.target_expiration =
+	 *   - arch/x86/kvm/lapic.c|2611| <<kvm_inject_apic_timer_irqs>> apic->lapic_timer.target_expiration = 0;
+	 * 在以下使用kvm_timer->timer_mode:
+	 *   - arch/x86/kvm/lapic.c|1303| <<apic_get_tmcct>> remaining = ktime_sub(apic->lapic_timer.target_expiration, now);
+	 *   - arch/x86/kvm/lapic.c|1691| <<update_target_expiration>> remaining = ktime_sub(apic->lapic_timer.target_expiration, now);
+	 *   - arch/x86/kvm/lapic.c|1753| <<advance_periodic_target_expiration>> ktime_add_ns(apic->lapic_timer.target_expiration,
+	 *   - arch/x86/kvm/lapic.c|1755| <<advance_periodic_target_expiration>> delta = ktime_sub(apic->lapic_timer.target_expiration, now);
+	 *   - arch/x86/kvm/lapic.c|1766| <<start_sw_period>> apic->lapic_timer.target_expiration)) {
+	 *   - arch/x86/kvm/lapic.c|1776| <<start_sw_period>> apic->lapic_timer.target_expiration,
+	 */
 	ktime_t target_expiration;
+	/*
+	 * 在以下使用kvm_timer->timer_mode:
+	 *   - arch/x86/kvm/lapic.c|316| <<apic_lvtt_oneshot>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_ONESHOT;
+	 *   - arch/x86/kvm/lapic.c|321| <<apic_lvtt_period>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_PERIODIC;
+	 *   - arch/x86/kvm/lapic.c|326| <<apic_lvtt_tscdeadline>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_TSCDEADLINE;
+	 *   - arch/x86/kvm/lapic.c|1499| <<apic_update_lvtt>> if (apic->lapic_timer.timer_mode != timer_mode) {
+	 *   - arch/x86/kvm/lapic.c|1507| <<apic_update_lvtt>> apic->lapic_timer.timer_mode = timer_mode;
+	 */
 	u32 timer_mode;
+	/*
+	 * 在以下使用kvm_timer->timer_mode_mask:
+	 *   - arch/x86/kvm/cpuid.c|93| <<kvm_update_cpuid>> apic->lapic_timer.timer_mode_mask = 3 << 17;
+	 *   - arch/x86/kvm/cpuid.c|95| <<kvm_update_cpuid>> apic->lapic_timer.timer_mode_mask = 1 << 17;
+	 *   - arch/x86/kvm/lapic.c|1497| <<apic_update_lvtt>> apic->lapic_timer.timer_mode_mask;
+	 *   - arch/x86/kvm/lapic.c|2140| <<kvm_lapic_reg_write>> val &= (apic_lvt_mask[0] | apic->lapic_timer.timer_mode_mask);
+	 */
 	u32 timer_mode_mask;
+	/*
+	 * 在以下使用kvm_timer->tscdeadline:
+	 *   - arch/x86/kvm/lapic.c|1501| <<apic_update_lvtt>> apic->lapic_timer.tscdeadline = 0;
+	 *   - arch/x86/kvm/lapic.c|1561| <<apic_timer_expired>> ktimer->expired_tscdeadline = ktimer->tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|1636| <<start_sw_tscdeadline>> u64 guest_tsc, tscdeadline = apic->lapic_timer.tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|1644| <<start_sw_tscdeadline>> if (unlikely(!tscdeadline || !this_tsc_khz))
+	 *   - arch/x86/kvm/lapic.c|1651| <<start_sw_tscdeadline>> if (likely(tscdeadline > guest_tsc)) {
+	 *   - arch/x86/kvm/lapic.c|1652| <<start_sw_tscdeadline>> ns = (tscdeadline - guest_tsc) * 1000000ULL;
+	 *   - arch/x86/kvm/lapic.c|1695| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+	 *   - arch/x86/kvm/lapic.c|1712| <<set_target_expiration>> apic->lapic_timer.tscdeadline = 0;
+	 *   - arch/x86/kvm/lapic.c|1728| <<set_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+	 *   - arch/x86/kvm/lapic.c|1752| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+	 *   - arch/x86/kvm/lapic.c|1810| <<start_hv_timer>> if (!ktimer->tscdeadline)
+	 *   - arch/x86/kvm/lapic.c|1813| <<start_hv_timer>> r = kvm_x86_ops->set_hv_timer(apic->vcpu, ktimer->tscdeadline);
+	 *   - arch/x86/kvm/lapic.c|2242| <<kvm_get_lapic_tscdeadline_msr>> return apic->lapic_timer.tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|2258| <<kvm_set_lapic_tscdeadline_msr>> apic->lapic_timer.tscdeadline = data;
+	 *   - arch/x86/kvm/lapic.c|2559| <<kvm_inject_apic_timer_irqs>> apic->lapic_timer.tscdeadline = 0;
+	 *   - arch/x86/kvm/lapic.c|2561| <<kvm_inject_apic_timer_irqs>> apic->lapic_timer.tscdeadline = 0;
+	 */
 	u64 tscdeadline;
+	/*
+	 * 在以下使用kvm_timer->expired_tscdeadline:
+	 *   - arch/x86/kvm/lapic.c|1561| <<apic_timer_expired>> ktimer->expired_tscdeadline = ktimer->tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|1595| <<wait_lapic_expire>> if (apic->lapic_timer.expired_tscdeadline == 0)
+	 *   - arch/x86/kvm/lapic.c|1601| <<wait_lapic_expire>> tsc_deadline = apic->lapic_timer.expired_tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|1602| <<wait_lapic_expire>> apic->lapic_timer.expired_tscdeadline = 0;
+	 */
 	u64 expired_tscdeadline;
+	/*
+	 * 会使用apic_has_pending_timer()查询
+	 */
 	atomic_t pending;			/* accumulated triggered timers */
+	/*
+	 * 在以下修改kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1765| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1787| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1756| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1763| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1850| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1880| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1911| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1921| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	bool hv_timer_in_use;
 };
 
@@ -132,6 +202,11 @@ static inline void kvm_lapic_set_vector(int vec, void *bitmap)
 	set_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1076| <<__apic_accept_irq>> kvm_lapic_set_irr(vector, apic);
+ *   - arch/x86/kvm/svm.c|5214| <<svm_deliver_avic_intr>> kvm_lapic_set_irr(vec, vcpu->arch.apic);
+ */
 static inline void kvm_lapic_set_irr(int vec, struct kvm_lapic *apic)
 {
 	kvm_lapic_set_vector(vec, apic->regs + APIC_IRR);
diff --git a/arch/x86/kvm/vmx/vmcs.h b/arch/x86/kvm/vmx/vmcs.h
index 6def3ba88e3b..b1979b3798ca 100644
--- a/arch/x86/kvm/vmx/vmcs.h
+++ b/arch/x86/kvm/vmx/vmcs.h
@@ -52,6 +52,14 @@ struct loaded_vmcs {
 	int cpu;
 	bool launched;
 	bool nmi_known_unmasked;
+	/*
+	 * 在以下使用loaded_vmcs->hv_timer_armed:
+	 *   - arch/x86/kvm/vmx/nested.c|2018| <<prepare_vmcs02_early>> vmx->loaded_vmcs->hv_timer_armed = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6367| <<vmx_arm_hv_timer>> if (!vmx->loaded_vmcs->hv_timer_armed)
+	 *   - arch/x86/kvm/vmx/vmx.c|6370| <<vmx_arm_hv_timer>> vmx->loaded_vmcs->hv_timer_armed = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6420| <<vmx_update_hv_timer>> if (vmx->loaded_vmcs->hv_timer_armed)
+	 *   - arch/x86/kvm/vmx/vmx.c|6423| <<vmx_update_hv_timer>> vmx->loaded_vmcs->hv_timer_armed = false;
+	 */
 	bool hv_timer_armed;
 	/* Support for vnmi-less CPUs */
 	int soft_vnmi_blocked;
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index d60edd838993..c877f2087b50 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -6356,26 +6356,62 @@ static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
 					msrs[i].host, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6379| <<vmx_update_hv_timer>> vmx_arm_hv_timer(vmx, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|6392| <<vmx_update_hv_timer>> vmx_arm_hv_timer(vmx, delta_tsc);
+ */
 static void vmx_arm_hv_timer(struct vcpu_vmx *vmx, u32 val)
 {
 	vmcs_write32(VMX_PREEMPTION_TIMER_VALUE, val);
+	/*
+	 * 在以下使用loaded_vmcs->hv_timer_armed:
+	 *   - arch/x86/kvm/vmx/nested.c|2018| <<prepare_vmcs02_early>> vmx->loaded_vmcs->hv_timer_armed = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6367| <<vmx_arm_hv_timer>> if (!vmx->loaded_vmcs->hv_timer_armed)
+	 *   - arch/x86/kvm/vmx/vmx.c|6370| <<vmx_arm_hv_timer>> vmx->loaded_vmcs->hv_timer_armed = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6420| <<vmx_update_hv_timer>> if (vmx->loaded_vmcs->hv_timer_armed)
+	 *   - arch/x86/kvm/vmx/vmx.c|6423| <<vmx_update_hv_timer>> vmx->loaded_vmcs->hv_timer_armed = false;
+	 */
 	if (!vmx->loaded_vmcs->hv_timer_armed)
 		vmcs_set_bits(PIN_BASED_VM_EXEC_CONTROL,
 			      PIN_BASED_VMX_PREEMPTION_TIMER);
 	vmx->loaded_vmcs->hv_timer_armed = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6605| <<vmx_vcpu_run>> vmx_update_hv_timer(vcpu);
+ */
 static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	u64 tscl;
 	u32 delta_tsc;
 
+	/*
+	 * 在以下设置vcpu_vmx->req_immediate_exit:
+	 *   - arch/x86/kvm/vmx/vmx.c|1043| <<vmx_prepare_switch_to_guest>> vmx->req_immediate_exit = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7147| <<vmx_request_immediate_exit>> to_vmx(vcpu)->req_immediate_exit = true;
+	 * 在以下使用vcpu_vmx->req_immediate_exit:
+	 *   - arch/x86/kvm/vmx/vmx.c|5484| <<handle_preemption_timer>> if (!to_vmx(vcpu)->req_immediate_exit)
+	 *   - arch/x86/kvm/vmx/vmx.c|6378| <<vmx_update_hv_timer>> if (vmx->req_immediate_exit) {
+	 */
 	if (vmx->req_immediate_exit) {
 		vmx_arm_hv_timer(vmx, 0);
 		return;
 	}
 
+	/*
+	 * 在以下使用vcpu_vmx->hv_deadline_tsc:
+	 *   - arch/x86/kvm/vmx/vmx.c|4040| <<vmx_vcpu_setup>> vmx->hv_deadline_tsc = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|6383| <<vmx_update_hv_timer>> if (vmx->hv_deadline_tsc != -1) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6385| <<vmx_update_hv_timer>> if (vmx->hv_deadline_tsc > tscl)
+	 *   - arch/x86/kvm/vmx/vmx.c|6387| <<vmx_update_hv_timer>> delta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >>
+	 *   - arch/x86/kvm/vmx/vmx.c|7287| <<vmx_set_hv_timer>> vmx->hv_deadline_tsc = tscl + delta_tsc;
+	 *   - arch/x86/kvm/vmx/vmx.c|7293| <<vmx_cancel_hv_timer>> to_vmx(vcpu)->hv_deadline_tsc = -1;
+	 *
+	 * apic deadline value in host tsc
+	 */
 	if (vmx->hv_deadline_tsc != -1) {
 		tscl = rdtsc();
 		if (vmx->hv_deadline_tsc > tscl)
@@ -6389,6 +6425,14 @@ static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
 		return;
 	}
 
+	/*
+	 * 在以下使用loaded_vmcs->hv_timer_armed:
+	 *   - arch/x86/kvm/vmx/nested.c|2018| <<prepare_vmcs02_early>> vmx->loaded_vmcs->hv_timer_armed = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6367| <<vmx_arm_hv_timer>> if (!vmx->loaded_vmcs->hv_timer_armed)
+	 *   - arch/x86/kvm/vmx/vmx.c|6370| <<vmx_arm_hv_timer>> vmx->loaded_vmcs->hv_timer_armed = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6420| <<vmx_update_hv_timer>> if (vmx->loaded_vmcs->hv_timer_armed)
+	 *   - arch/x86/kvm/vmx/vmx.c|6423| <<vmx_update_hv_timer>> vmx->loaded_vmcs->hv_timer_armed = false;
+	 */
 	if (vmx->loaded_vmcs->hv_timer_armed)
 		vmcs_clear_bits(PIN_BASED_VM_EXEC_CONTROL,
 				PIN_BASED_VMX_PREEMPTION_TIMER);
@@ -7249,6 +7293,59 @@ static int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc)
 	struct vcpu_vmx *vmx;
 	u64 tscl, guest_tscl, delta_tsc, lapic_timer_advance_cycles;
 
+	/*
+	 * https://patchwork.kernel.org/project/kvm/patch/1523362546-20909-1-git-send-email-karahmed@amazon.de/
+	 *
+	 * The VMX-preemption timer is used by KVM as a way to set deadlines for the
+	 * guest (i.e. timer emulation). That was safe till very recently when
+	 * capability KVM_X86_DISABLE_EXITS_MWAIT to disable intercepting MWAIT was
+	 * introduced. According to Intel SDM 25.5.1:
+	 *
+	 * """
+	 * The VMX-preemption timer operates in the C-states C0, C1, and C2; it also
+	 * operates in the shutdown and wait-for-SIPI states. If the timer counts down
+	 * to zero in any state other than the wait-for SIPI state, the logical
+	 * processor transitions to the C0 C-state and causes a VM exit; the timer
+	 * does not cause a VM exit if it counts down to zero in the wait-for-SIPI
+	 * state. The timer is not decremented in C-states deeper than C2.
+	 * """
+	 *
+	 * Now once the guest issues the MWAIT with a c-state deeper than
+	 * C2 the preemption timer will never wake it up again since it stopped
+	 * ticking! Usually this is compensated by other activities in the system that
+	 * would wake the core from the deep C-state (and cause a VMExit). For
+	 * example, if the host itself is ticking or it received interrupts, etc!
+	 *
+	 * So disable the VMX-preemption timer if MWAIT is exposed to the guest!
+	 *
+	 * mwait穿透是否实现了
+	 *
+	 * 随便测试都是false
+	 *     mwait_in_guest = false,
+	 *     hlt_in_guest = false,
+	 *     pause_in_guest = false,
+	 *
+	 *
+	 * http://liujunming.top/2020/05/01/Introduction-to-halt-pause-monitor-mwait-instruction/
+	 *
+	 * monitor and mwait
+	 *
+	 * Executing the HLT instruction on a idle logical processor puts the
+	 * targeted processor in a non-execution state. This requires another
+	 * processor (when posting work for the halted logical processor) to
+	 * wake up the halted processor using an inter-processor interrupt. The
+	 * posting and servicing of such an interrupt introduces a delay in the
+	 * servicing of new work requests.
+	 *
+	 * MONITOR sets up an effective address range that is monitored for
+	 * write-to-memory activities; MWAIT places the processor in an
+	 * optimized state (this may vary between different implementations)
+	 * until a write to the monitored address range occurs.
+	 *
+	 * 引入了monitor 和 mwait指令后，避免了hlt导致的the posting and
+	 * servicing of such an interrupt introduces a delay in the servicing
+	 * of new work requests.
+	 */
 	if (kvm_mwait_in_guest(vcpu->kvm))
 		return -EOPNOTSUPP;
 
@@ -7280,12 +7377,34 @@ static int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc)
 	if (delta_tsc >> (cpu_preemption_timer_multi + 32))
 		return -ERANGE;
 
+	/*
+	 * 在以下使用vcpu_vmx->hv_deadline_tsc:
+	 *   - arch/x86/kvm/vmx/vmx.c|4040| <<vmx_vcpu_setup>> vmx->hv_deadline_tsc = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|6383| <<vmx_update_hv_timer>> if (vmx->hv_deadline_tsc != -1) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6385| <<vmx_update_hv_timer>> if (vmx->hv_deadline_tsc > tscl)
+	 *   - arch/x86/kvm/vmx/vmx.c|6387| <<vmx_update_hv_timer>> delta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >>
+	 *   - arch/x86/kvm/vmx/vmx.c|7287| <<vmx_set_hv_timer>> vmx->hv_deadline_tsc = tscl + delta_tsc;
+	 *   - arch/x86/kvm/vmx/vmx.c|7293| <<vmx_cancel_hv_timer>> to_vmx(vcpu)->hv_deadline_tsc = -1;
+	 *
+	 * apic deadline value in host tsc
+	 */
 	vmx->hv_deadline_tsc = tscl + delta_tsc;
 	return delta_tsc == 0;
 }
 
 static void vmx_cancel_hv_timer(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用vcpu_vmx->hv_deadline_tsc:
+	 *   - arch/x86/kvm/vmx/vmx.c|4040| <<vmx_vcpu_setup>> vmx->hv_deadline_tsc = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|6383| <<vmx_update_hv_timer>> if (vmx->hv_deadline_tsc != -1) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6385| <<vmx_update_hv_timer>> if (vmx->hv_deadline_tsc > tscl)
+	 *   - arch/x86/kvm/vmx/vmx.c|6387| <<vmx_update_hv_timer>> delta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >>
+	 *   - arch/x86/kvm/vmx/vmx.c|7287| <<vmx_set_hv_timer>> vmx->hv_deadline_tsc = tscl + delta_tsc;
+	 *   - arch/x86/kvm/vmx/vmx.c|7293| <<vmx_cancel_hv_timer>> to_vmx(vcpu)->hv_deadline_tsc = -1;
+	 *
+	 * apic deadline value in host tsc
+	 */
 	to_vmx(vcpu)->hv_deadline_tsc = -1;
 }
 #endif
@@ -7466,6 +7585,18 @@ static int vmx_pre_block(struct kvm_vcpu *vcpu)
 	if (pi_pre_block(vcpu))
 		return 1;
 
+	/*
+	 * 在以下修改kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1765| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1787| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1756| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1763| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1850| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1880| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1911| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1921| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	if (kvm_lapic_hv_timer_in_use(vcpu))
 		kvm_lapic_switch_to_sw_timer(vcpu);
 
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index c8817c05aad2..8c68c6e53f9e 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -243,6 +243,14 @@ struct vcpu_vmx {
 	int ple_window;
 	bool ple_window_dirty;
 
+	/*
+	 * 在以下设置vcpu_vmx->req_immediate_exit:
+	 *   - arch/x86/kvm/vmx/vmx.c|1043| <<vmx_prepare_switch_to_guest>> vmx->req_immediate_exit = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7147| <<vmx_request_immediate_exit>> to_vmx(vcpu)->req_immediate_exit = true;
+	 * 在以下使用vcpu_vmx->req_immediate_exit:
+	 *   - arch/x86/kvm/vmx/vmx.c|5484| <<handle_preemption_timer>> if (!to_vmx(vcpu)->req_immediate_exit)
+	 *   - arch/x86/kvm/vmx/vmx.c|6378| <<vmx_update_hv_timer>> if (vmx->req_immediate_exit) {
+	 */
 	bool req_immediate_exit;
 
 	/* Support for PML */
@@ -250,6 +258,17 @@ struct vcpu_vmx {
 	struct page *pml_pg;
 
 	/* apic deadline value in host tsc */
+	/*
+	 * 在以下使用vcpu_vmx->hv_deadline_tsc:
+	 *   - arch/x86/kvm/vmx/vmx.c|4040| <<vmx_vcpu_setup>> vmx->hv_deadline_tsc = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|6383| <<vmx_update_hv_timer>> if (vmx->hv_deadline_tsc != -1) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6385| <<vmx_update_hv_timer>> if (vmx->hv_deadline_tsc > tscl)
+	 *   - arch/x86/kvm/vmx/vmx.c|6387| <<vmx_update_hv_timer>> delta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >>
+	 *   - arch/x86/kvm/vmx/vmx.c|7287| <<vmx_set_hv_timer>> vmx->hv_deadline_tsc = tscl + delta_tsc;
+	 *   - arch/x86/kvm/vmx/vmx.c|7293| <<vmx_cancel_hv_timer>> to_vmx(vcpu)->hv_deadline_tsc = -1;
+	 *
+	 * apic deadline value in host tsc
+	 */
 	u64 hv_deadline_tsc;
 
 	u64 current_tsc_ratio;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 242555889380..f3b398610ea8 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -138,6 +138,18 @@ static u32 __read_mostly tsc_tolerance_ppm = 250;
 module_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);
 
 /* lapic timer advance (tscdeadline mode only) in nanoseconds */
+/*
+ * 在以下使用lapic_timer_advance_ns:
+ *   - arch/x86/kvm/x86.c|142| <<global>> module_param(lapic_timer_advance_ns, uint, S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/lapic.c|1584| <<wait_lapic_expire>> nsec_to_cycles(vcpu, lapic_timer_advance_ns)));
+ *   - arch/x86/kvm/lapic.c|1591| <<wait_lapic_expire>> lapic_timer_advance_ns -= min((unsigned int )ns,
+ *   - arch/x86/kvm/lapic.c|1592| <<wait_lapic_expire>> lapic_timer_advance_ns / LAPIC_TIMER_ADVANCE_ADJUST_STEP);
+ *   - arch/x86/kvm/lapic.c|1597| <<wait_lapic_expire>> lapic_timer_advance_ns += min((unsigned int )ns,
+ *   - arch/x86/kvm/lapic.c|1598| <<wait_lapic_expire>> lapic_timer_advance_ns / LAPIC_TIMER_ADVANCE_ADJUST_STEP);
+ *   - arch/x86/kvm/lapic.c|1630| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, lapic_timer_advance_ns);
+ *   - arch/x86/kvm/vmx/vmx.c|7259| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu, lapic_timer_advance_ns);
+ *   - arch/x86/kvm/x86.c|7926| <<vcpu_enter_guest>> if (lapic_timer_advance_ns)
+ */
 unsigned int __read_mostly lapic_timer_advance_ns = 1000;
 module_param(lapic_timer_advance_ns, uint, S_IRUGO | S_IWUSR);
 EXPORT_SYMBOL_GPL(lapic_timer_advance_ns);
@@ -2174,6 +2186,10 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v)
 				sizeof(vcpu->hv_clock.version));
 }
 
+/*
+ * called by (KVM_REQ_CLOCK_UPDATE):
+ *   - arch/x86/kvm/x86.c|7742| <<vcpu_enter_guest>> r = kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -2434,6 +2450,10 @@ static void kvm_vcpu_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 	kvm_x86_ops->tlb_flush(vcpu, invalidate_gpa);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7762| <<vcpu_enter_guest>> record_steal_time(vcpu);
+ */
 static void record_steal_time(struct kvm_vcpu *vcpu)
 {
 	struct kvm_host_map map;
@@ -3320,6 +3340,10 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	kvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3371| <<kvm_arch_vcpu_put>> kvm_steal_time_set_preempted(vcpu);
+ */
 static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)
 {
 	struct kvm_host_map map;
@@ -7752,6 +7776,13 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			r = 0;
 			goto out;
 		}
+		/*
+		 * 在以下使用KVM_REQ_APF_HALT:
+		 *   - arch/x86/include/asm/kvm_host.h|60| <<global>> #define KVM_REQ_APF_HALT KVM_ARCH_REQ(7)
+		 *   - arch/x86/kvm/mmu.c|4201| <<try_async_pf>> kvm_make_request(KVM_REQ_APF_HALT, vcpu);
+		 *   - arch/x86/kvm/x86.c|7779| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|9843| <<kvm_arch_async_page_not_present>> kvm_make_request(KVM_REQ_APF_HALT, vcpu);
+		 */
 		if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {
 			/* Page is swapped out. Do synthetic halt */
 			vcpu->arch.apf.halted = true;
@@ -7996,14 +8027,24 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8081| <<vcpu_run>> r = vcpu_block(kvm, vcpu);
+ */
 static inline int vcpu_block(struct kvm *kvm, struct kvm_vcpu *vcpu)
 {
+	/*
+	 * vmx_pre_block()
+	 */
 	if (!kvm_arch_vcpu_runnable(vcpu) &&
 	    (!kvm_x86_ops->pre_block || kvm_x86_ops->pre_block(vcpu) == 0)) {
 		srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);
 		kvm_vcpu_block(vcpu);
 		vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);
 
+		/*
+		 * vmx_post_block()
+		 */
 		if (kvm_x86_ops->post_block)
 			kvm_x86_ops->post_block(vcpu);
 
@@ -8035,6 +8076,14 @@ static inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
 	if (is_guest_mode(vcpu) && kvm_x86_ops->check_nested_events)
 		kvm_x86_ops->check_nested_events(vcpu);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apf.halted:
+	 *   - arch/x86/kvm/x86.c|7781| <<vcpu_enter_guest>> vcpu->arch.apf.halted = true;
+	 *   - arch/x86/kvm/x86.c|8056| <<vcpu_block>> vcpu->arch.apf.halted = false;
+	 *   - arch/x86/kvm/x86.c|8073| <<kvm_vcpu_running>> !vcpu->arch.apf.halted);
+	 *   - arch/x86/kvm/x86.c|8925| <<kvm_vcpu_reset>> vcpu->arch.apf.halted = false;
+	 *   - arch/x86/kvm/x86.c|9890| <<kvm_arch_async_page_present>> vcpu->arch.apf.halted = false;
+	 */
 	return (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&
 		!vcpu->arch.apf.halted);
 }
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index bf9f5a8c2d70..f9e8295892f8 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -327,6 +327,11 @@ static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 
 static inline bool kvm_mwait_in_guest(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->mwait_in_guest:
+	 *   - arch/x86/kvm/x86.c|4662| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X86_DISABLE_EXITS)>> kvm->arch.mwait_in_guest = true;
+	 *   - arch/x86/kvm/x86.h|330| <<kvm_mwait_in_guest>> return kvm->arch.mwait_in_guest;
+	 */
 	return kvm->arch.mwait_in_guest;
 }
 
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 9a91ce8037c3..cf0e91161b68 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -39,15 +39,39 @@
 static int napi_weight = NAPI_POLL_WEIGHT;
 module_param(napi_weight, int, 0444);
 
+/*
+ * 在以下使用napi_tx:
+ *   - drivers/net/virtio_net.c|2623| <<virtnet_alloc_queues>> napi_tx ? napi_weight : 0);
+ */
 static bool csum = true, gso = true, napi_tx;
 module_param(csum, bool, 0444);
 module_param(gso, bool, 0444);
 module_param(napi_tx, bool, 0644);
 
 /* FIXME: MTU in config. */
+/*
+ * 在以下使用GOOD_PACKET_LEN:
+ *   - drivers/net/virtio_net.c|570| <<receive_small>> unsigned int buflen = SKB_DATA_ALIGN(GOOD_PACKET_LEN + headroom) +
+ *   - drivers/net/virtio_net.c|599| <<receive_small>> buflen = SKB_DATA_ALIGN(GOOD_PACKET_LEN + headroom) +
+ *   - drivers/net/virtio_net.c|980| <<add_recvbuf_small>> int len = vi->hdr_len + VIRTNET_RX_PAD + GOOD_PACKET_LEN + xdp_headroom;
+ *   - drivers/net/virtio_net.c|992| <<add_recvbuf_small>> vi->hdr_len + GOOD_PACKET_LEN);
+ *   - drivers/net/virtio_net.c|2516| <<mergeable_min_buf_len>> (unsigned int )GOOD_PACKET_LEN);
+ */
 #define GOOD_PACKET_LEN (ETH_HLEN + VLAN_HLEN + ETH_DATA_LEN)
+/*
+ * 在以下使用GOOD_COPY_LEN:
+ *   - drivers/net/virtio_net.c|361| <<page_to_skb>> skb = napi_alloc_skb(&rq->napi, GOOD_COPY_LEN);
+ */
 #define GOOD_COPY_LEN	128
 
+/*
+ * 在以下使用VIRTNET_RX_PAD:
+ *   - drivers/net/virtio_net.c|568| <<receive_small>> unsigned int header_offset = VIRTNET_RX_PAD + xdp_headroom;
+ *   - drivers/net/virtio_net.c|597| <<receive_small>> header_offset = VIRTNET_RX_PAD + xdp_headroom;
+ *   - drivers/net/virtio_net.c|612| <<receive_small>> xdp.data_hard_start = buf + VIRTNET_RX_PAD + vi->hdr_len;
+ *   - drivers/net/virtio_net.c|980| <<add_recvbuf_small>> int len = vi->hdr_len + VIRTNET_RX_PAD + GOOD_PACKET_LEN + xdp_headroom;
+ *   - drivers/net/virtio_net.c|991| <<add_recvbuf_small>> sg_init_one(rq->sg, buf + VIRTNET_RX_PAD + xdp_headroom,
+ */
 #define VIRTNET_RX_PAD (NET_IP_ALIGN + NET_SKB_PAD)
 
 /* Amount of XDP headroom to prepend to packets for use by xdp_adjust_head */
@@ -148,6 +172,13 @@ struct receive_queue {
 	/* Name of this receive queue: input.$index */
 	char name[40];
 
+	/*
+	 * 在以下使用receive_queue->xdp_rxq:
+	 *   - drivers/net/virtio_net.c|615| <<receive_small>> xdp.rxq = &rq->xdp_rxq;
+	 *   - drivers/net/virtio_net.c|757| <<receive_mergeable>> xdp.rxq = &rq->xdp_rxq;
+	 *   - drivers/net/virtio_net.c|1335| <<virtnet_open>> err = xdp_rxq_info_reg(&vi->rq[i].xdp_rxq, dev, i);
+	 *   - drivers/net/virtio_net.c|1680| <<virtnet_close>> xdp_rxq_info_unreg(&vi->rq[i].xdp_rxq);
+	 */
 	struct xdp_rxq_info xdp_rxq;
 };
 
@@ -164,6 +195,14 @@ struct control_buf {
 
 struct virtnet_info {
 	struct virtio_device *vdev;
+	/*
+	 * 在以下使用virtnet_info->cvq:
+	 *   - drivers/net/virtio_net.c|1606| <<virtnet_send_command>> virtqueue_add_sgs(vi->cvq, sgs, out_num, 1, vi, GFP_ATOMIC);
+	 *   - drivers/net/virtio_net.c|1608| <<virtnet_send_command>> if (unlikely(!virtqueue_kick(vi->cvq)))
+	 *   - drivers/net/virtio_net.c|1614| <<virtnet_send_command>> while (!virtqueue_get_buf(vi->cvq, &tmp) &&
+	 *   - drivers/net/virtio_net.c|1615| <<virtnet_send_command>> !virtqueue_is_broken(vi->cvq))
+	 *   - drivers/net/virtio_net.c|2667| <<virtnet_find_vqs>> vi->cvq = vqs[total_vqs - 1];
+	 */
 	struct virtqueue *cvq;
 	struct net_device *dev;
 	struct send_queue *sq;
@@ -195,12 +234,41 @@ struct virtnet_info {
 	u8 hdr_len;
 
 	/* Work struct for refilling if we run low on memory. */
+	/*
+	 * 在以下使用virtnet_info->refill:
+	 *   - drivers/net/virtio_net.c|1311| <<refill_work>> container_of(work, struct virtnet_info, refill.work);
+	 *   - drivers/net/virtio_net.c|1326| <<refill_work>> schedule_delayed_work(&vi->refill, HZ/2);
+	 *   - drivers/net/virtio_net.c|1358| <<virtnet_receive>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1467| <<virtnet_open>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1820| <<_virtnet_set_queues>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1842| <<virtnet_close>> cancel_delayed_work_sync(&vi->refill);
+	 *   - drivers/net/virtio_net.c|2336| <<virtnet_freeze_down>> cancel_delayed_work_sync(&vi->refill);
+	 *   - drivers/net/virtio_net.c|2362| <<virtnet_restore_up>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|2837| <<virtnet_alloc_queues>> INIT_DELAYED_WORK(&vi->refill, refill_work);
+	 *   - drivers/net/virtio_net.c|3180| <<virtnet_probe>> cancel_delayed_work_sync(&vi->refill);
+	 */
 	struct delayed_work refill;
 
 	/* Work struct for config space updates */
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|2268| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|2512| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|2546| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|2980| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 *   - drivers/net/virtio_net.c|3076| <<virtnet_probe>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|3128| <<virtnet_remove>> flush_work(&vi->config_work);
+	 */
 	struct work_struct config_work;
 
 	/* Does the affinity hint is set for virtqueues? */
+	/*
+	 * 在以下使用virtnet_info->affinity_hint_set:
+	 *   - drivers/net/virtio_net.c|1254| <<virtnet_napi_tx_enable>> if (!vi->affinity_hint_set) {
+	 *   - drivers/net/virtio_net.c|1871| <<virtnet_clean_affinity>> if (vi->affinity_hint_set) {
+	 *   - drivers/net/virtio_net.c|1877| <<virtnet_clean_affinity>> vi->affinity_hint_set = false;
+	 *   - drivers/net/virtio_net.c|1904| <<virtnet_set_affinity>> vi->affinity_hint_set = true;
+	 */
 	bool affinity_hint_set;
 
 	/* CPU hotplug instances for online & dead */
@@ -252,6 +320,13 @@ static int rxq2vq(int rxq)
 	return rxq * 2;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|365| <<page_to_skb>> hdr = skb_vnet_hdr(skb);
+ *   - drivers/net/virtio_net.c|659| <<receive_small>> memcpy(skb_vnet_hdr(skb), buf, vi->hdr_len);
+ *   - drivers/net/virtio_net.c|940| <<receive_buf>> hdr = skb_vnet_hdr(skb);
+ *   - drivers/net/virtio_net.c|1391| <<xmit_skb>> hdr = skb_vnet_hdr(skb);
+ */
 static inline struct virtio_net_hdr_mrg_rxbuf *skb_vnet_hdr(struct sk_buff *skb)
 {
 	return (struct virtio_net_hdr_mrg_rxbuf *)skb->cb;
@@ -261,6 +336,16 @@ static inline struct virtio_net_hdr_mrg_rxbuf *skb_vnet_hdr(struct sk_buff *skb)
  * private is used to chain pages for big packets, put the whole
  * most recent used list in the beginning for reuse
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|418| <<page_to_skb>> give_pages(rq, page);
+ *   - drivers/net/virtio_net.c|690| <<receive_big>> give_pages(rq, page);
+ *   - drivers/net/virtio_net.c|923| <<receive_buf>> give_pages(rq, buf);
+ *   - drivers/net/virtio_net.c|1013| <<add_recvbuf_big>> give_pages(rq, list);
+ *   - drivers/net/virtio_net.c|1025| <<add_recvbuf_big>> give_pages(rq, list);
+ *   - drivers/net/virtio_net.c|1043| <<add_recvbuf_big>> give_pages(rq, first);
+ *   - drivers/net/virtio_net.c|2484| <<free_unused_bufs>> give_pages(&vi->rq[i], buf);
+ */
 static void give_pages(struct receive_queue *rq, struct page *page)
 {
 	struct page *end;
@@ -271,6 +356,12 @@ static void give_pages(struct receive_queue *rq, struct page *page)
 	rq->pages = page;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1010| <<add_recvbuf_big>> first = get_a_page(rq, gfp);
+ *   - drivers/net/virtio_net.c|1023| <<add_recvbuf_big>> first = get_a_page(rq, gfp);
+ *   - drivers/net/virtio_net.c|2438| <<_free_receive_bufs>> __free_pages(get_a_page(&vi->rq[i], GFP_KERNEL), 0);
+ */
 static struct page *get_a_page(struct receive_queue *rq, gfp_t gfp_mask)
 {
 	struct page *p = rq->pages;
@@ -284,20 +375,43 @@ static struct page *get_a_page(struct receive_queue *rq, gfp_t gfp_mask)
 	return p;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|308| <<virtqueue_napi_complete>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|323| <<skb_xmit_done>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|1149| <<skb_recv_done>> virtqueue_napi_schedule(&rq->napi, rvq);
+ *   - drivers/net/virtio_net.c|1161| <<virtnet_napi_enable>> virtqueue_napi_schedule(napi, vq);
+ */
 static void virtqueue_napi_schedule(struct napi_struct *napi,
 				    struct virtqueue *vq)
 {
+	/*
+	 * Test if NAPI routine is already running, and if not mark
+	 * it as running.  This is used as a condition variable
+	 * insure only one NAPI poll instance runs.  We also make
+	 * sure there is no pending NAPI disable.
+	 */
 	if (napi_schedule_prep(napi)) {
 		virtqueue_disable_cb(vq);
 		__napi_schedule(napi);
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1316| <<virtnet_poll>> virtqueue_napi_complete(napi, rq->vq, received);
+ *   - drivers/net/virtio_net.c|1364| <<virtnet_poll_tx>> virtqueue_napi_complete(napi, sq->vq, 0);
+ */
 static void virtqueue_napi_complete(struct napi_struct *napi,
 				    struct virtqueue *vq, int processed)
 {
 	int opaque;
 
+	/*
+	 * 在以下调用virtqueue_enable_cb_prepare():
+	 *   - drivers/net/virtio_net.c|397| <<virtqueue_napi_complete>> opaque = virtqueue_enable_cb_prepare(vq);
+	 *   - drivers/virtio/virtio_ring.c|879| <<virtqueue_enable_cb>> unsigned last_used_idx = virtqueue_enable_cb_prepare(_vq);
+	 */
 	opaque = virtqueue_enable_cb_prepare(vq);
 	if (napi_complete_done(napi, processed)) {
 		if (unlikely(virtqueue_poll(vq, opaque)))
@@ -340,6 +454,12 @@ static unsigned int mergeable_ctx_to_truesize(void *mrg_ctx)
 }
 
 /* Called from bottom half context */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|680| <<receive_big>> struct sk_buff *skb = page_to_skb(vi, rq, page, 0, len,
+ *   - drivers/net/virtio_net.c|774| <<receive_mergeable>> head_skb = page_to_skb(vi, rq, xdp_page,
+ *   - drivers/net/virtio_net.c|826| <<receive_mergeable>> head_skb = page_to_skb(vi, rq, page, offset, len, truesize, !xdp_prog);
+ */
 static struct sk_buff *page_to_skb(struct virtnet_info *vi,
 				   struct receive_queue *rq,
 				   struct page *page, unsigned int offset,
@@ -687,6 +807,10 @@ static struct sk_buff *receive_big(struct net_device *dev,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1029| <<receive_buf>> skb = receive_mergeable(dev, vi, rq, buf, ctx, len, xdp_xmit);
+ */
 static struct sk_buff *receive_mergeable(struct net_device *dev,
 					 struct virtnet_info *vi,
 					 struct receive_queue *rq,
@@ -1041,6 +1165,11 @@ static int add_recvbuf_big(struct virtnet_info *vi, struct receive_queue *rq,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1080| <<add_recvbuf_mergeable>> len = get_mergeable_buf_len(rq, &rq->mrg_avg_pkt_len, room);
+ *   - drivers/net/virtio_net.c|2685| <<mergeable_rx_buffer_size_show>> get_mergeable_buf_len(&vi->rq[queue_index], avg,
+ */
 static unsigned int get_mergeable_buf_len(struct receive_queue *rq,
 					  struct ewma_pkt_len *avg_pkt_len,
 					  unsigned int room)
@@ -1107,6 +1236,13 @@ static int add_recvbuf_mergeable(struct virtnet_info *vi,
  * before we're receiving packets, or from refill_work which is
  * careful to disable receiving (using napi_disable).
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1189| <<refill_work>> still_empty = !try_fill_recv(vi, rq, GFP_KERNEL);
+ *   - drivers/net/virtio_net.c|1223| <<virtnet_receive>> if (!try_fill_recv(vi, rq, GFP_ATOMIC))
+ *   - drivers/net/virtio_net.c|1321| <<virtnet_open>> if (!try_fill_recv(vi, &vi->rq[i], GFP_KERNEL))
+ *   - drivers/net/virtio_net.c|2149| <<virtnet_restore_up>> if (!try_fill_recv(vi, &vi->rq[i], GFP_KERNEL))
+ */
 static bool try_fill_recv(struct virtnet_info *vi, struct receive_queue *rq,
 			  gfp_t gfp)
 {
@@ -1151,6 +1287,13 @@ static void virtnet_napi_enable(struct virtqueue *vq, struct napi_struct *napi)
 	local_bh_enable();
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1426| <<virtnet_open>> virtnet_napi_tx_enable(vi, vi->sq[i].vq, &vi->sq[i].napi);
+ *   - drivers/net/virtio_net.c|2255| <<virtnet_restore_up>> virtnet_napi_tx_enable(vi, vi->sq[i].vq,
+ *   - drivers/net/virtio_net.c|2377| <<virtnet_xdp_set>> virtnet_napi_tx_enable(vi, vi->sq[i].vq,
+ *   - drivers/net/virtio_net.c|2388| <<virtnet_xdp_set>> virtnet_napi_tx_enable(vi, vi->sq[i].vq,
+ */
 static void virtnet_napi_tx_enable(struct virtnet_info *vi,
 				   struct virtqueue *vq,
 				   struct napi_struct *napi)
@@ -1197,6 +1340,10 @@ static void refill_work(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1433| <<virtnet_poll>> received = virtnet_receive(rq, budget, &xdp_xmit);
+ */
 static int virtnet_receive(struct receive_queue *rq, int budget, bool *xdp_xmit)
 {
 	struct virtnet_info *vi = rq->vq->vdev->priv;
@@ -1232,6 +1379,13 @@ static int virtnet_receive(struct receive_queue *rq, int budget, bool *xdp_xmit)
 	return received;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1381| <<virtnet_poll_cleantx>> free_old_xmit_skbs(sq);
+ *   - drivers/net/virtio_net.c|1451| <<virtnet_poll_tx>> free_old_xmit_skbs(sq);
+ *   - drivers/net/virtio_net.c|1520| <<start_xmit>> free_old_xmit_skbs(sq);
+ *   - drivers/net/virtio_net.c|1563| <<start_xmit>> free_old_xmit_skbs(sq);
+ */
 static void free_old_xmit_skbs(struct send_queue *sq)
 {
 	struct sk_buff *skb;
@@ -1270,6 +1424,10 @@ static bool is_xdp_raw_buffer_queue(struct virtnet_info *vi, int q)
 		return false;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1396| <<virtnet_poll>> virtnet_poll_cleantx(rq);
+ */
 static void virtnet_poll_cleantx(struct receive_queue *rq)
 {
 	struct virtnet_info *vi = rq->vq->vdev->priv;
@@ -1332,6 +1490,10 @@ static int virtnet_open(struct net_device *dev)
 	return 0;
 }
 
+/*
+ * 在以下使用virtnet_poll_tx():
+ *   - drivers/net/virtio_net.c|2705| <<virtnet_alloc_queues>> netif_tx_napi_add(vi->dev, &vi->sq[i].napi, virtnet_poll_tx,
+ */
 static int virtnet_poll_tx(struct napi_struct *napi, int budget)
 {
 	struct send_queue *sq = container_of(napi, struct send_queue, napi);
@@ -1413,11 +1575,26 @@ static netdev_tx_t start_xmit(struct sk_buff *skb, struct net_device *dev)
 	int err;
 	struct netdev_queue *txq = netdev_get_tx_queue(dev, qnum);
 	bool kick = !skb->xmit_more;
+	/*
+	 * struct send_queue *sq:
+	 * -> struct napi_struct napi;
+	 *    -> int weight;
+	 */
 	bool use_napi = sq->napi.weight;
 
 	/* Free up any pending old buffers before queueing new ones. */
 	free_old_xmit_skbs(sq);
 
+	/*
+	 * This re-enables callbacks but hints to the other side to delay
+	 * interrupts until most of the available buffers have been processed;
+	 * it returns "false" if there are many pending buffers in the queue,
+	 * to detect a possible race between the driver checking for more work,
+	 * and enabling callbacks.
+	 *
+	 * Caller must ensure we don't call this with other virtqueue
+	 * operations at the same time (except where noted).
+	 */
 	if (use_napi && kick)
 		virtqueue_enable_cb_delayed(sq->vq);
 
@@ -1478,6 +1655,18 @@ static netdev_tx_t start_xmit(struct sk_buff *skb, struct net_device *dev)
  * supported by the hypervisor, as indicated by feature bits, should
  * never fail unless improperly formatted.
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1642| <<virtnet_set_mac_address>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MAC,
+ *   - drivers/net/virtio_net.c|1719| <<virtnet_ack_link_announce>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_ANNOUNCE,
+ *   - drivers/net/virtio_net.c|1736| <<_virtnet_set_queues>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MQ,
+ *   - drivers/net/virtio_net.c|1798| <<virtnet_set_rx_mode>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_RX,
+ *   - drivers/net/virtio_net.c|1805| <<virtnet_set_rx_mode>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_RX,
+ *   - drivers/net/virtio_net.c|1841| <<virtnet_set_rx_mode>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MAC,
+ *   - drivers/net/virtio_net.c|1857| <<virtnet_vlan_rx_add_vid>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_VLAN,
+ *   - drivers/net/virtio_net.c|1872| <<virtnet_vlan_rx_kill_vid>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_VLAN,
+ *   - drivers/net/virtio_net.c|2273| <<virtnet_set_guest_offloads>> if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_GUEST_OFFLOADS,
+ */
 static bool virtnet_send_command(struct virtnet_info *vi, u8 class, u8 cmd,
 				 struct scatterlist *out)
 {
@@ -1863,6 +2052,9 @@ static void virtnet_cpu_notif_remove(struct virtnet_info *vi)
 					    &vi->node_dead);
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_ringparam = virtnet_get_ringparam()
+ */
 static void virtnet_get_ringparam(struct net_device *dev,
 				struct ethtool_ringparam *ring)
 {
@@ -1875,6 +2067,9 @@ static void virtnet_get_ringparam(struct net_device *dev,
 }
 
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_drvinfo = virtnet_get_drvinfo()
+ */
 static void virtnet_get_drvinfo(struct net_device *dev,
 				struct ethtool_drvinfo *info)
 {
@@ -1888,6 +2083,9 @@ static void virtnet_get_drvinfo(struct net_device *dev,
 }
 
 /* TODO: Eliminate OOO packets during switching */
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.set_channels = virtnet_set_channels()
+ */
 static int virtnet_set_channels(struct net_device *dev,
 				struct ethtool_channels *channels)
 {
@@ -1924,6 +2122,9 @@ static int virtnet_set_channels(struct net_device *dev,
 	return err;
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_strings = virtnet_get_strings()
+ */
 static void virtnet_get_strings(struct net_device *dev, u32 stringset, u8 *data)
 {
 	struct virtnet_info *vi = netdev_priv(dev);
@@ -1951,6 +2152,9 @@ static void virtnet_get_strings(struct net_device *dev, u32 stringset, u8 *data)
 	}
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_sset_count = virtnet_get_sset_count()
+ */
 static int virtnet_get_sset_count(struct net_device *dev, int sset)
 {
 	struct virtnet_info *vi = netdev_priv(dev);
@@ -1964,6 +2168,9 @@ static int virtnet_get_sset_count(struct net_device *dev, int sset)
 	}
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_ethtool_stats = virtnet_get_ethtool_stats()
+ */
 static void virtnet_get_ethtool_stats(struct net_device *dev,
 				      struct ethtool_stats *stats, u64 *data)
 {
@@ -2001,6 +2208,9 @@ static void virtnet_get_ethtool_stats(struct net_device *dev,
 	}
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_channels = virtnet_get_channels()
+ */
 static void virtnet_get_channels(struct net_device *dev,
 				 struct ethtool_channels *channels)
 {
@@ -2040,6 +2250,9 @@ virtnet_validate_ethtool_cmd(const struct ethtool_link_ksettings *cmd)
 			     __ETHTOOL_LINK_MODE_MASK_NBITS);
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.set_link_ksettings = virtnet_set_link_ksettings()
+ */
 static int virtnet_set_link_ksettings(struct net_device *dev,
 				      const struct ethtool_link_ksettings *cmd)
 {
@@ -2058,6 +2271,9 @@ static int virtnet_set_link_ksettings(struct net_device *dev,
 	return 0;
 }
 
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.get_link_ksettings = virtnet_get_link_ksettings()
+ */
 static int virtnet_get_link_ksettings(struct net_device *dev,
 				      struct ethtool_link_ksettings *cmd)
 {
@@ -2078,6 +2294,11 @@ static void virtnet_init_settings(struct net_device *dev)
 	vi->duplex = DUPLEX_UNKNOWN;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2533| <<virtnet_config_changed_work>> virtnet_update_settings(vi);
+ *   - drivers/net/virtio_net.c|3079| <<virtnet_probe>> virtnet_update_settings(vi);
+ */
 static void virtnet_update_settings(struct virtnet_info *vi)
 {
 	u32 speed;
@@ -2110,6 +2331,10 @@ static const struct ethtool_ops virtnet_ethtool_ops = {
 	.set_link_ksettings = virtnet_set_link_ksettings,
 };
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|3197| <<virtnet_freeze>> virtnet_freeze_down(vdev);
+ */
 static void virtnet_freeze_down(struct virtio_device *vdev)
 {
 	struct virtnet_info *vi = vdev->priv;
@@ -2357,6 +2582,18 @@ static const struct net_device_ops virtnet_netdev = {
 	.ndo_get_phys_port_name	= virtnet_get_phys_port_name,
 };
 
+/*
+ * 在以下使用virtnet_info->config_work:
+ *   - drivers/net/virtio_net.c|2268| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+ *   - drivers/net/virtio_net.c|2512| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+ *   - drivers/net/virtio_net.c|2546| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+ *   - drivers/net/virtio_net.c|2980| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+ *   - drivers/net/virtio_net.c|3076| <<virtnet_probe>> schedule_work(&vi->config_work);
+ *   - drivers/net/virtio_net.c|3128| <<virtnet_remove>> flush_work(&vi->config_work);
+ *
+ * 在以下使用virtnet_config_changed_work():
+ *   - drivers/net/virtio_net.c|2980| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+ */
 static void virtnet_config_changed_work(struct work_struct *work)
 {
 	struct virtnet_info *vi =
@@ -2390,6 +2627,9 @@ static void virtnet_config_changed_work(struct work_struct *work)
 	}
 }
 
+/*
+ * struct virtio_driver virtio_net_driver.config_changed = virtnet_config_changed()
+ */
 static void virtnet_config_changed(struct virtio_device *vdev)
 {
 	struct virtnet_info *vi = vdev->priv;
@@ -2726,6 +2966,9 @@ static bool virtnet_validate_features(struct virtio_device *vdev)
 #define MIN_MTU ETH_MIN_MTU
 #define MAX_MTU ETH_MAX_MTU
 
+/*
+ * struct virtio_driver virtio_net_driver.validate = virtnet_validate()
+ */
 static int virtnet_validate(struct virtio_device *vdev)
 {
 	if (!vdev->config->get) {
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 8fe07622ae59..7b077d3a2252 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -388,6 +388,12 @@ static bool vhost_can_busy_poll(struct vhost_dev *dev,
 	       !vhost_has_work(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|773| <<handle_rx>> vhost_net_disable_vq(net, vq);
+ *   - drivers/vhost/net.c|970| <<vhost_net_stop_vq>> vhost_net_disable_vq(n, vq);
+ *   - drivers/vhost/net.c|1150| <<vhost_net_set_backend>> vhost_net_disable_vq(n, vq);
+ */
 static void vhost_net_disable_vq(struct vhost_net *n,
 				 struct vhost_virtqueue *vq)
 {
@@ -620,6 +626,9 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
 	struct vhost_virtqueue *vq = &nvq->vq;
 	unsigned long uninitialized_var(endtime);
+	/*
+	 * 返回的等价skb->len
+	 */
 	int len = peek_head_len(rvq, sk);
 
 	if (!len && vq->busyloop_timeout) {
@@ -662,6 +671,14 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
  * @quota       - headcount quota, 1 for big buffer
  *	returns number of buffer heads allocated, negative on error
  */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|804| <<handle_rx>> headcount = get_rx_bufs(vq, vq->heads, vhost_len,
+ *
+ * 827                 headcount = get_rx_bufs(vq, vq->heads, vhost_len,
+ * 828                                         &in, vq_log, &log,
+ * 829                                         likely(mergeable) ? UIO_MAXIOV : 1);
+ */
 static int get_rx_bufs(struct vhost_virtqueue *vq,
 		       struct vring_used_elem *heads,
 		       int datalen,
@@ -672,6 +689,10 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 {
 	unsigned int out, in;
 	int seg = 0;
+	/*
+	 * headcount只在while循环每一次的结束增加!!
+	 * 不到while循环第一次结束的discard都不会影响avail index !!!
+	 */
 	int headcount = 0;
 	unsigned d;
 	int r, nlogs = 0;
@@ -680,11 +701,26 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 	 */
 	u32 uninitialized_var(len);
 
+	/*
+	 * 每次while循环结束前datalen会减少
+	 */
 	while (datalen > 0 && headcount < quota) {
 		if (unlikely(seg >= UIO_MAXIOV)) {
 			r = -ENOBUFS;
+			/*
+			 * goto err会调用vhost_discard_vq_desc()
+			 *
+			 * 这里不会导致endless loop???
+			 */
 			goto err;
 		}
+		/*
+		 * seg一开始是0
+		 *
+		 * 如果是net的rx, 每次只取一个seg吧
+		 *
+		 * vhost_get_vq_desc()会增加vq->last_avail_idx++
+		 */
 		r = vhost_get_vq_desc(vq, vq->iov + seg,
 				      ARRAY_SIZE(vq->iov) - seg, &out,
 				      &in, log, log_num);
@@ -696,6 +732,9 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 			r = 0;
 			goto err;
 		}
+		/*
+		 * 因为这是收包, 不能有out!!!
+		 */
 		if (unlikely(out || in <= 0)) {
 			vq_err(vq, "unexpected descriptor format for RX: "
 				"out %d, in %d\n", out, in);
@@ -710,6 +749,10 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 		len = iov_length(vq->iov + seg, in);
 		heads[headcount].len = cpu_to_vhost32(vq, len);
 		datalen -= len;
+		/*
+		 * headcount只在这里增加!!
+		 * 不到这里的discard都不会影响avail index !!!
+		 */
 		++headcount;
 		seg += in;
 	}
@@ -725,12 +768,21 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 	}
 	return headcount;
 err:
+	/*
+	 * headcount只在while循环每一次的结束增加!!
+	 * 不到while循环第一次结束的discard都不会影响avail index !!!
+	 */
 	vhost_discard_vq_desc(vq, headcount);
 	return r;
 }
 
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|893| <<handle_rx_kick>> handle_rx(net);
+ *   - drivers/vhost/net.c|907| <<handle_rx_net>> handle_rx(net);
+ */
 static void handle_rx(struct vhost_net *net)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];
@@ -763,12 +815,20 @@ static void handle_rx(struct vhost_net *net)
 	if (!sock)
 		goto out;
 
+	/*
+	 * !vq->iotlb的话直接返回1
+	 */
 	if (!vq_iotlb_prefetch(vq))
 		goto out;
 
 	vhost_disable_notify(&net->dev, vq);
 	vhost_net_disable_vq(net, vq);
 
+	/*
+	 * rhck的例子
+	 *   vhost_hlen = 0,
+	 *   sock_hlen = 12,
+	 */
 	vhost_hlen = nvq->vhost_hlen;
 	sock_hlen = nvq->sock_hlen;
 
@@ -777,12 +837,23 @@ static void handle_rx(struct vhost_net *net)
 	mergeable = vhost_has_feature(vq, VIRTIO_NET_F_MRG_RXBUF);
 
 	do {
+		/*
+		 * 返回的等价skb->len
+		 */
 		sock_len = vhost_net_rx_peek_head_len(net, sock->sk);
 
 		if (!sock_len)
 			break;
 		sock_len += sock_hlen;
 		vhost_len = sock_len + vhost_hlen;
+		/*
+		 * struct vhost_virtqueue *vq:
+		 * -> struct vring_used_elem *heads;
+		 *
+		 * in的类型: unsigned uninitialized_var(in)
+		 *
+		 * 因为可能有indirect, headcount是response的数目, in是用的desc的数目
+		 */
 		headcount = get_rx_bufs(vq, vq->heads, vhost_len,
 					&in, vq_log, &log,
 					likely(mergeable) ? UIO_MAXIOV : 1);
@@ -791,6 +862,9 @@ static void handle_rx(struct vhost_net *net)
 			goto out;
 		/* OK, now we need to know about added descriptors. */
 		if (!headcount) {
+			/*
+			 * 这里概率不大 但是可以遇到
+			 */
 			if (unlikely(vhost_enable_notify(&net->dev, vq))) {
 				/* They have slipped one in as we were
 				 * doing that: check again. */
@@ -804,6 +878,10 @@ static void handle_rx(struct vhost_net *net)
 		if (nvq->rx_array)
 			msg.msg_control = vhost_net_buf_consume(&nvq->rxq);
 		/* On overrun, truncate and discard */
+		/*
+		 * 这里看着挺有问题的!!!
+		 * 但是似乎(headcount > UIO_MAXIOV)的时候get_rx_bufs()里已经discard了
+		 */
 		if (unlikely(headcount > UIO_MAXIOV)) {
 			iov_iter_init(&msg.msg_iter, READ, vq->iov, 1, 1);
 			err = sock->ops->recvmsg(sock, &msg,
@@ -828,6 +906,7 @@ static void handle_rx(struct vhost_net *net)
 		if (unlikely(err != sock_len)) {
 			pr_debug("Discarded rx packet: "
 				 " len %d, expected %zd\n", err, sock_len);
+			/* 会 vq->last_avail_idx -= n; */
 			vhost_discard_vq_desc(vq, headcount);
 			continue;
 		}
@@ -860,6 +939,9 @@ static void handle_rx(struct vhost_net *net)
 		if (unlikely(vq_log))
 			vhost_log_write(vq, vq_log, log, vhost_len,
 					vq->iov, in);
+		/*
+		 * total_len只在这里增加, 就用在判断vhost_exceeds_weight()
+		 */
 		total_len += vhost_len;
 	} while (likely(!vhost_exceeds_weight(vq, ++recv_pkts, total_len)));
 
@@ -1192,6 +1274,10 @@ static long vhost_net_set_backend(struct vhost_net *n, unsigned index, int fd)
 	return r;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1398| <<vhost_net_ioctl(VHOST_RESET_OWNER)>> return vhost_net_reset_owner(n);
+ */
 static long vhost_net_reset_owner(struct vhost_net *n)
 {
 	struct socket *tx_sock = NULL;
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 78f26cd4eef9..ec385d6e4fe0 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -47,7 +47,18 @@ enum {
 	VHOST_MEMORY_F_LOG = 0x1,
 };
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2413| <<vhost_notify>> if (vhost_get_avail(vq, event, vhost_used_event(vq))) {
+ */
 #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1888| <<vhost_update_avail_event>> vhost_avail_event(vq)))
+ *   - drivers/vhost/vhost.c|1895| <<vhost_update_avail_event>> used = vhost_avail_event(vq);
+ *   - drivers/vhost/vhost.c|1897| <<vhost_update_avail_event>> sizeof *vhost_avail_event(vq));
+ *   - drivers/vhost/vhost.c|2514| <<vhost_enable_notify>> vhost_avail_event(vq), r);
+ */
 #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
 
 INTERVAL_TREE_DEFINE(struct vhost_umem_node,
@@ -306,6 +317,11 @@ bool vhost_vq_is_setup(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_is_setup);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|479| <<vhost_dev_init>> vhost_vq_reset(dev, vq);
+ *   - drivers/vhost/vhost.c|665| <<vhost_dev_cleanup>> vhost_vq_reset(dev, dev->vqs[i]);
+ */
 static void vhost_vq_reset(struct vhost_dev *dev,
 			   struct vhost_virtqueue *vq)
 {
@@ -433,6 +449,15 @@ bool vhost_exceeds_weight(struct vhost_virtqueue *vq,
 }
 EXPORT_SYMBOL_GPL(vhost_exceeds_weight);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1025| <<vhost_net_open>> vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,
+ *   - drivers/vhost/scsi.c|1880| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs, VHOST_SCSI_MAX_VQ, VHOST_SCSI_WEIGHT, 0);
+ *   - drivers/vhost/test.c|123| <<vhost_test_open>> vhost_dev_init(dev, vqs, VHOST_TEST_VQ_MAX,
+ *   - drivers/vhost/vsock.c|629| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs),
+ *
+ * 在QEMU中reboot的时候不会调用第二次
+ */
 void vhost_dev_init(struct vhost_dev *dev,
 		    struct vhost_virtqueue **vqs, int nvqs,
 		    int weight, int byte_weight)
@@ -1857,9 +1882,26 @@ int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 }
 EXPORT_SYMBOL_GPL(vhost_log_write);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1912| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|2447| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|2484| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+ */
 static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 {
 	void __user *used;
+	/*
+	 * 在以下设置vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|321| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|2502| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2539| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|1869| <<vhost_update_used_flags>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2500| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2537| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 */
 	if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
 			   &vq->used->flags) < 0)
 		return -EFAULT;
@@ -1876,6 +1918,10 @@ static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2558| <<vhost_enable_notify>> r = vhost_update_avail_event(vq, vq->avail_idx);
+ */
 static int vhost_update_avail_event(struct vhost_virtqueue *vq, u16 avail_event)
 {
 	if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
@@ -1895,6 +1941,13 @@ static int vhost_update_avail_event(struct vhost_virtqueue *vq, u16 avail_event)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1230| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/scsi.c|1696| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+ *   - drivers/vhost/test.c|204| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+ *   - drivers/vhost/vsock.c|540| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+ */
 int vhost_vq_init_access(struct vhost_virtqueue *vq)
 {
 	__virtio16 last_used_idx;
@@ -2105,6 +2158,19 @@ static int get_indirect(struct vhost_virtqueue *vq,
  * This function returns the descriptor number found, or vq->num (which is
  * never a valid descriptor number) if none was found.  A negative code is
  * returned on error. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|429| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/net.c|439| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/net.c|694| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg,
+ *   - drivers/vhost/scsi.c|508| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/scsi.c|886| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/test.c|62| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/vsock.c|121| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/vsock.c|460| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *
+ * vhost_get_vq_desc()会增加vq->last_avail_idx++
+ */
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
@@ -2239,10 +2305,28 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 	} while ((i = next_desc(vq, &desc)) != -1);
 
 	/* On success, increment avail index. */
+	/*
+	 * 在以下设置vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1440| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|2248| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 */
 	vq->last_avail_idx++;
 
 	/* Assume notifications from guest are disabled at this point,
 	 * if they aren't we would need to update avail_event index. */
+	/*
+	 * 在以下设置vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|321| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|2502| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2539| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|1869| <<vhost_update_used_flags>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2500| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2537| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 */
 	BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
 	return head;
 }
@@ -2251,12 +2335,26 @@ EXPORT_SYMBOL_GPL(vhost_get_vq_desc);
 /* Reverse the effect of vhost_get_vq_desc. Useful for error handling. */
 void vhost_discard_vq_desc(struct vhost_virtqueue *vq, int n)
 {
+	/*
+	 * 在以下设置vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1440| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|2248| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 */
 	vq->last_avail_idx -= n;
 }
 EXPORT_SYMBOL_GPL(vhost_discard_vq_desc);
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|598| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+ *   - drivers/vhost/vhost.c|2424| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+ *   - drivers/vhost/vsock.c|190| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(pkt->hdr) + payload_len);
+ *   - drivers/vhost/vsock.c|493| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, len);
+ */
 int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 {
 	struct vring_used_elem heads = {
@@ -2268,6 +2366,13 @@ int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 }
 EXPORT_SYMBOL_GPL(vhost_add_used);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2328| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+ *   - drivers/vhost/vhost.c|2334| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+ *
+ * 核心思想是把count个heads给copy到shared used buffer
+ */
 static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 			    struct vring_used_elem *heads,
 			    unsigned count)
@@ -2277,6 +2382,15 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 	int start;
 
 	start = vq->last_used_idx & (vq->num - 1);
+	/*
+	 * struct vhost_virtqueue *vq:
+	 * -> struct vring_desc __user *desc;
+	 * -> struct vring_avail __user *avail;
+	 * -> struct vring_used __user *used;
+	 *    -> __virtio16 flags;
+	 *    -> __virtio16 idx;
+	 *    -> struct vring_used_elem ring[];
+	 */
 	used = vq->used->ring + start;
 	if (count == 1) {
 		if (vhost_put_user(vq, heads[0].id, &used->id)) {
@@ -2298,6 +2412,12 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 		log_used(vq, ((void __user *)used - (void __user *)vq->used),
 			 count * sizeof *used);
 	}
+	/*
+	 * 在以下设置vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|318| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1930| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2308| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 */
 	old = vq->last_used_idx;
 	new = (vq->last_used_idx += count);
 	/* If the driver never bothers to signal in a very long while,
@@ -2311,6 +2431,11 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2273| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+ *   - drivers/vhost/vhost.c|2434| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+ */
 int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 		     unsigned count)
 {
@@ -2347,6 +2472,10 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2391| <<vhost_signal>> if (vq->call_ctx && vhost_notify(dev, vq))
+ */
 static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__u16 old, new;
@@ -2357,6 +2486,9 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 	 * interrupts. */
 	smp_mb();
 
+	/*
+	 * 不太见VIRTIO_F_NOTIFY_ON_EMPTY使用
+	 */
 	if (vhost_has_feature(vq, VIRTIO_F_NOTIFY_ON_EMPTY) &&
 	    unlikely(vq->avail_idx == vq->last_avail_idx))
 		return true;
@@ -2369,22 +2501,80 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 		}
 		return !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));
 	}
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|319| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2313| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2382| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2384| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	old = vq->signalled_used;
 	v = vq->signalled_used_valid;
+	/*
+	 * 在以下设置vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|318| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1930| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2308| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 */
 	new = vq->signalled_used = vq->last_used_idx;
 	vq->signalled_used_valid = true;
 
 	if (unlikely(!v))
 		return true;
 
+	/*
+	 * 获取((__virtio16 __user *)&vq->avail->ring[vq->num])
+	 */
 	if (vhost_get_avail(vq, event, vhost_used_event(vq))) {
 		vq_err(vq, "Failed to get used event idx");
 		return true;
 	}
+	/*
+	 * event_idx : vq->avail->ring[vq->num]
+	 * new_idx   : 当前的vq->last_used_idx (也是vq->signalled_used)
+	 * old       : 上一次的vq->signalled_used
+	 *
+	 * (__u16)(new_idx - event_idx - 1) < (__u16)(new_idx - old);
+	 */
 	return vring_need_event(vhost16_to_cpu(vq, event), new, old);
 }
 
 /* This actually signals the guest, using eventfd. */
+/*
+ * # sudo /usr/share/bcc/tools/trace -t -C  'kvm_set_msi_irq'
+ * TIME     CPU PID     TID     COMM            FUNC
+ * 1.409018 4   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.430408 4   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.557921 1   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.594878 5   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 1.624964 4   23372   23372   vhost-23357     kvm_set_msi_irq
+ *
+ * 9.586209 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ * 9.586283 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ * 9.586399 5   23372   23372   vhost-23357     kvm_set_msi_irq
+ * 9.586470 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ * 9.586482 17  23357   23357   qemu-system-x86 kvm_set_msi_irq
+ *
+ * kvm_set_msi_irq
+ * irqfd_wakeup
+ * __wake_up_common
+ * __wake_up_locked_key
+ * eventfd_signal
+ * vhost_signal
+ * vhost_add_used_and_signal_n
+ * handle_rx
+ * handle_rx_net
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|611| <<vhost_scsi_complete_cmd_work>> vhost_signal(&vs->dev, &vs->vqs[vq].vq);
+ *   - drivers/vhost/vhost.c|2425| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.c|2435| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vsock.c|220| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|500| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+ */
 void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	/* Signal the Guest tell them we used something up. */
@@ -2394,6 +2584,15 @@ void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_signal);
 
 /* And here's the combo meal deal.  Supersize me! */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|577| <<handle_tx>> vhost_add_used_and_signal(&net->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|537| <<vhost_scsi_do_evt_work>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|875| <<vhost_scsi_send_bad_target>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|1217| <<vhost_scsi_send_tmf_resp>> vhost_add_used_and_signal(&vs->dev, vq, vq_desc, 0);
+ *   - drivers/vhost/scsi.c|1309| <<vhost_scsi_send_an_resp>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+ *   - drivers/vhost/test.c|88| <<handle_vq>> vhost_add_used_and_signal(&n->dev, vq, head, 0);
+ */
 void vhost_add_used_and_signal(struct vhost_dev *dev,
 			       struct vhost_virtqueue *vq,
 			       unsigned int head, int len)
@@ -2404,6 +2603,11 @@ void vhost_add_used_and_signal(struct vhost_dev *dev,
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal);
 
 /* multi-buffer version of vhost_add_used_and_signal */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|344| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+ *   - drivers/vhost/net.c|865| <<handle_rx>> vhost_add_used_and_signal_n(&net->dev, vq, vq->heads,
+ */
 void vhost_add_used_and_signal_n(struct vhost_dev *dev,
 				 struct vhost_virtqueue *vq,
 				 struct vring_used_elem *heads, unsigned count)
@@ -2432,11 +2636,53 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_vq_avail_empty);
 
 /* OK, now we need to know about added descriptors. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|504| <<handle_tx>> if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|642| <<vhost_net_rx_peek_head_len>> else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|797| <<handle_rx>> if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/scsi.c|516| <<vhost_scsi_do_evt_work>> if (vhost_enable_notify(&vs->dev, vq))
+ *   - drivers/vhost/scsi.c|899| <<vhost_scsi_get_desc>> if (unlikely(vhost_enable_notify(&vs->dev, vq))) {
+ *   - drivers/vhost/test.c|71| <<handle_vq>> if (unlikely(vhost_enable_notify(&n->dev, vq))) {
+ *   - drivers/vhost/vsock.c|112| <<vhost_transport_do_send_pkt>> vhost_enable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|138| <<vhost_transport_do_send_pkt>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ *   - drivers/vhost/vsock.c|466| <<vhost_vsock_handle_tx_kick>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ */
 bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__virtio16 avail_idx;
 	int r;
 
+	/*
+	 * vm reboot之后是1 ...
+	 * crash> vhost_virtqueue ffff97ffa41400b0 | grep used
+	 *   used = 0x7fb3e89ed240, 
+	 *   last_used_idx = 32642, 
+	 *   used_flags = 1, 
+	 *   signalled_used = 32642, 
+	 *   signalled_used_valid = true, 
+	 *   log_used = false, 
+	 *
+	 * ifconfig down之后变成了0
+	 *
+	 * crash> vhost_virtqueue ffff97ffa41400b0 | grep used
+	 *   used = 0x7fb3e89ed240, 
+	 *   last_used_idx = 32659, 
+	 *   used_flags = 0, 
+	 *   signalled_used = 32659, 
+	 *   signalled_used_valid = true, 
+	 *   log_used = false,
+	 *
+	 * 在以下设置vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|321| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|2502| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2539| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|1869| <<vhost_update_used_flags>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2500| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2537| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 */
 	if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
 		return false;
 	vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
@@ -2470,10 +2716,40 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_enable_notify);
 
 /* We don't need to be notified again. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|486| <<handle_tx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|511| <<handle_tx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|637| <<vhost_net_rx_peek_head_len>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|652| <<vhost_net_rx_peek_head_len>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|824| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|871| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/scsi.c|507| <<vhost_scsi_do_evt_work>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|900| <<vhost_scsi_get_desc>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1012| <<vhost_scsi_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1337| <<vhost_scsi_ctl_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/test.c|59| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/test.c|72| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/vsock.c|99| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|139| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|448| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|467| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ */
 void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	int r;
 
+	/*
+	 * 在以下设置vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|321| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|2502| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2539| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|1869| <<vhost_update_used_flags>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2500| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2537| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 */
 	if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
 		return;
 	vq->used_flags |= VRING_USED_F_NO_NOTIFY;
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index f7ec63af627d..6d5e134b0cb6 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -108,21 +108,60 @@ struct vhost_virtqueue {
 	vhost_work_fn_t handle_kick;
 
 	/* Last available index we saw. */
+	/*
+	 * 在以下设置vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1440| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|2248| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 */
 	u16 last_avail_idx;
 
 	/* Caches available index value from user. */
 	u16 avail_idx;
 
 	/* Last index we used. */
+	/*
+	 * 在以下设置vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|318| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1930| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2308| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 */
 	u16 last_used_idx;
 
 	/* Used flags */
+	/*
+	 * 在以下设置vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|321| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|2502| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2539| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|1869| <<vhost_update_used_flags>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2500| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2537| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 */
 	u16 used_flags;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下设置vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|319| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2313| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2382| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2384| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 signalled_used;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|320| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|1918| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2314| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2386| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2388| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	bool signalled_used_valid;
 
 	/* Log writes to used structure. */
@@ -132,6 +171,10 @@ struct vhost_virtqueue {
 	struct iovec iov[UIO_MAXIOV];
 	struct iovec iotlb_iov[64];
 	struct iovec *indirect;
+	/*
+	 * 在以下分配vhost_virtqueue->heads:
+	 *   - drivers/vhost/vhost.c|412| <<vhost_dev_alloc_iovecs>> vq->heads = kmalloc(sizeof *vq->heads * UIO_MAXIOV, GFP_KERNEL);
+	 */
 	struct vring_used_elem *heads;
 	/* Protected by virtqueue mutex. */
 	struct vhost_umem *umem;
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 51278f8bd3ab..d00d9cae8bb1 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -68,6 +68,10 @@ struct vring_virtqueue {
 	struct vring vring;
 
 	/* Can we use weak barriers? */
+	/*
+	 * 在以下设置vring_virtqueue->weak_barriers:
+	 *   - drivers/virtio/virtio_ring.c|1011| <<__vring_new_virtqueue>> vq->weak_barriers = weak_barriers;
+	 */
 	bool weak_barriers;
 
 	/* Other side has made a mess, don't try any more. */
@@ -82,15 +86,67 @@ struct vring_virtqueue {
 	/* Head of free buffer list. */
 	unsigned int free_head;
 	/* Number we've added since last sync. */
+	/*
+	 * 在以下设置vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|434| <<virtqueue_add>> vq->num_added++;
+	 *   - drivers/virtio/virtio_ring.c|590| <<virtqueue_kick_prepare>> vq->num_added = 0;
+	 *   - drivers/virtio/virtio_ring.c|1026| <<__vring_new_virtqueue>> vq->num_added = 0;
+	 * 在以下使用vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|441| <<virtqueue_add>> if (unlikely(vq->num_added == (1 << 16) - 1))
+	 *   - drivers/virtio/virtio_ring.c|588| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 */
 	unsigned int num_added;
 
 	/* Last used index we've seen. */
+	/*
+	 * 在以下设置vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|869| <<virtqueue_get_buf_ctx>> vq->last_used_idx++;
+	 *   - drivers/virtio/virtio_ring.c|1199| <<__vring_new_virtqueue>> vq->last_used_idx = 0;
+	 * 在以下使用vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|804| <<more_used>> return vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev, vq->vring.used->idx);
+	 *   - drivers/virtio/virtio_ring.c|853| <<virtqueue_get_buf_ctx>> last_used = (vq->last_used_idx & (vq->vring.num - 1));
+	 *   - drivers/virtio/virtio_ring.c|892| <<virtqueue_get_buf_ctx>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx));
+	 *   - drivers/virtio/virtio_ring.c|991| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+	 *   - drivers/virtio/virtio_ring.c|1089| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|1103| <<virtqueue_enable_cb_delayed>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));
+	 *   - drivers/virtio/virtio_ring.c|1105| <<virtqueue_enable_cb_delayed>> if (unlikely((u16)(virtio16_to_cpu(_vq->vdev, vq->vring.used->idx) - vq->last_used_idx) > bufs)) {
+	 */
 	u16 last_used_idx;
 
 	/* Last written value to avail->flags */
+	/*
+	 * 在以下设置vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|776| <<virtqueue_disable_cb>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|809| <<virtqueue_enable_cb_prepare>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|884| <<virtqueue_enable_cb_delayed>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|990| <<__vring_new_virtqueue>> vq->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1005| <<__vring_new_virtqueue>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 * 在以下使用vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|743| <<virtqueue_get_buf_ctx>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|775| <<virtqueue_disable_cb>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|778| <<virtqueue_disable_cb>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|808| <<virtqueue_enable_cb_prepare>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|811| <<virtqueue_enable_cb_prepare>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|883| <<virtqueue_enable_cb_delayed>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1007| <<__vring_new_virtqueue>> vq->vring.avail->flags = cpu_to_virtio16(vdev, vq->avail_flags_shadow);
+	 */
 	u16 avail_flags_shadow;
 
 	/* Last written value to avail->idx in guest byte order */
+	/*
+	 * 在以下修改vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|432| <<virtqueue_add>> vq->avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|961| <<virtqueue_detach_unused_buf>> vq->avail_idx_shadow--;
+	 *   - drivers/virtio/virtio_ring.c|1025| <<__vring_new_virtqueue>> vq->avail_idx_shadow = 0;
+	 * 在以下使用vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|426| <<virtqueue_add>> avail = vq->avail_idx_shadow & (vq->vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|433| <<virtqueue_add>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|588| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|589| <<virtqueue_kick_prepare>> new = vq->avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|923| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|962| <<virtqueue_detach_unused_buf>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 */
 	u16 avail_idx_shadow;
 
 	/* How to notify other side. FIXME: commonalize hcalls! */
@@ -226,6 +282,12 @@ static void vring_unmap_one(const struct vring_virtqueue *vq,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|358| <<virtqueue_add>> if (vring_mapping_error(vq, addr))
+ *   - drivers/virtio/virtio_ring.c|371| <<virtqueue_add>> if (vring_mapping_error(vq, addr))
+ *   - drivers/virtio/virtio_ring.c|389| <<virtqueue_add>> if (vring_mapping_error(vq, addr))
+ */
 static int vring_mapping_error(const struct vring_virtqueue *vq,
 			       dma_addr_t addr)
 {
@@ -548,6 +610,13 @@ EXPORT_SYMBOL_GPL(virtqueue_add_inbuf_ctx);
  * This is sometimes useful because the virtqueue_kick_prepare() needs
  * to be serialized, but the actual virtqueue_notify() call does not.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|301| <<virtio_queue_rq>> if (bd->last && virtqueue_kick_prepare(vblk->vqs[qid].vq))
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|973| <<rpmsg_probe>> notify = virtqueue_kick_prepare(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|476| <<virtscsi_kick_cmd>> needs_kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|649| <<virtqueue_kick>> if (virtqueue_kick_prepare(vq))
+ */
 bool virtqueue_kick_prepare(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -559,8 +628,30 @@ bool virtqueue_kick_prepare(struct virtqueue *_vq)
 	 * event. */
 	virtio_mb(vq->weak_barriers);
 
+	/*
+	 * 在以下修改vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|432| <<virtqueue_add>> vq->avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|961| <<virtqueue_detach_unused_buf>> vq->avail_idx_shadow--;
+	 *   - drivers/virtio/virtio_ring.c|1025| <<__vring_new_virtqueue>> vq->avail_idx_shadow = 0;
+	 * 在以下使用vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|426| <<virtqueue_add>> avail = vq->avail_idx_shadow & (vq->vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|433| <<virtqueue_add>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|588| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|589| <<virtqueue_kick_prepare>> new = vq->avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|923| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|962| <<virtqueue_detach_unused_buf>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 */
 	old = vq->avail_idx_shadow - vq->num_added;
 	new = vq->avail_idx_shadow;
+	/*
+	 * 在以下设置vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|434| <<virtqueue_add>> vq->num_added++;
+	 *   - drivers/virtio/virtio_ring.c|590| <<virtqueue_kick_prepare>> vq->num_added = 0;
+	 *   - drivers/virtio/virtio_ring.c|1026| <<__vring_new_virtqueue>> vq->num_added = 0;
+	 * 在以下使用vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|441| <<virtqueue_add>> if (unlikely(vq->num_added == (1 << 16) - 1))
+	 *   - drivers/virtio/virtio_ring.c|588| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 */
 	vq->num_added = 0;
 
 #ifdef DEBUG
@@ -572,6 +663,9 @@ bool virtqueue_kick_prepare(struct virtqueue *_vq)
 #endif
 
 	if (vq->event) {
+		/*
+		 * return (__u16)(new_idx - event_idx - 1) < (__u16)(new_idx - old);
+		 */
 		needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev, vring_avail_event(&vq->vring)),
 					      new, old);
 	} else {
@@ -590,6 +684,13 @@ EXPORT_SYMBOL_GPL(virtqueue_kick_prepare);
  *
  * Returns false if host notify failed or queue is broken, otherwise true.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|306| <<virtio_queue_rq>> virtqueue_notify(vblk->vqs[qid].vq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|984| <<rpmsg_probe>> virtqueue_notify(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|481| <<virtscsi_kick_cmd>> virtqueue_notify(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|650| <<virtqueue_kick>> return virtqueue_notify(vq);
+ */
 bool virtqueue_notify(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -618,6 +719,43 @@ EXPORT_SYMBOL_GPL(virtqueue_notify);
  *
  * Returns false if kick failed, otherwise true.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|289| <<virtio_queue_rq>> virtqueue_kick(vblk->vqs[qid].vq); 
+ *   - drivers/char/hw_random/virtio-rng.c|63| <<register_buffer>> virtqueue_kick(vi->vq);
+ *   - drivers/char/virtio_console.c|513| <<add_inbuf>> virtqueue_kick(vq); 
+ *   - drivers/char/virtio_console.c|582| <<__send_control_msg>> virtqueue_kick(vq);
+ *   - drivers/char/virtio_console.c|636| <<__send_to_port>> virtqueue_kick(out_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|179| <<virtio_crypto_alg_ablkcipher_init_session>> virtqueue_kick(vcrypto->ctrl_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|252| <<virtio_crypto_alg_ablkcipher_close_session>> virtqueue_kick(vcrypto->ctrl_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|466| <<__virtio_crypto_ablkcipher_do_req>> virtqueue_kick(data_vq->vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|568| <<virtio_crypto_ablkcipher_crypt_req>> virtqueue_kick(data_vq->vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|298| <<virtio_gpu_queue_ctrl_buffer_locked>> virtqueue_kick(vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|373| <<virtio_gpu_queue_cursor>> virtqueue_kick(vq);
+ *   - drivers/net/caif/caif_virtio.c|589| <<cfv_netdev_tx>> virtqueue_kick(cfv->vq_tx);
+ *   - drivers/net/virtio_net.c|530| <<virtnet_xdp_flush>> virtqueue_kick(sq->vq);
+ *   - drivers/net/virtio_net.c|1243| <<try_fill_recv>> virtqueue_kick(rq->vq);
+ *   - drivers/net/virtio_net.c|1617| <<start_xmit>> virtqueue_kick(sq->vq);
+ *   - drivers/net/virtio_net.c|1665| <<virtnet_send_command>> if (unlikely(!virtqueue_kick(vi->cvq)))
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|654| <<rpmsg_send_offchannel_raw>> virtqueue_kick(vrp->svq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|802| <<rpmsg_recv_done>> virtqueue_kick(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|284| <<virtscsi_kick_event>> virtqueue_kick(vscsi->event_vq.vq);
+ *   - drivers/virtio/virtio_balloon.c|123| <<tell_host>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_balloon.c|328| <<stats_handle_request>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_balloon.c|463| <<init_vqs>> virtqueue_kick(vb->stats_vq);
+ *   - drivers/virtio/virtio_input.c|48| <<virtinput_recv_events>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_input.c|77| <<virtinput_send_status>> virtqueue_kick(vi->sts);
+ *   - drivers/virtio/virtio_input.c|196| <<virtinput_fill_evt>> virtqueue_kick(vi->evt);
+ *   - drivers/virtio/virtio_ring.c|442| <<virtqueue_add>> virtqueue_kick(_vq);
+ *   - net/9p/trans_virtio.c|308| <<p9_virtio_request>> virtqueue_kick(chan->vq);
+ *   - net/9p/trans_virtio.c|501| <<p9_virtio_zc_request>> virtqueue_kick(chan->vq);
+ *   - net/vmw_vsock/virtio_transport.c|174| <<virtio_transport_send_pkt_work>> virtqueue_kick(vq);
+ *   - net/vmw_vsock/virtio_transport.c|300| <<virtio_vsock_rx_fill>> virtqueue_kick(vq);
+ *   - net/vmw_vsock/virtio_transport.c|371| <<virtio_vsock_event_fill>> virtqueue_kick(vsock->vqs[VSOCK_VQ_EVENT]);
+ *   - net/vmw_vsock/virtio_transport.c|431| <<virtio_transport_event_work>> virtqueue_kick(vsock->vqs[VSOCK_VQ_EVENT]);
+ *   - tools/virtio/virtio_test.c|176| <<run_test>> if (unlikely(!virtqueue_kick(vq->vq)))
+ *   - tools/virtio/vringh_test.c|402| <<bool>> virtqueue_kick(vq);
+ */
 bool virtqueue_kick(struct virtqueue *vq)
 {
 	if (virtqueue_kick_prepare(vq))
@@ -675,6 +813,11 @@ static void detach_buf(struct vring_virtqueue *vq, unsigned int head,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|858| <<virtqueue_get_buf_ctx>> if (!more_used(vq)) {
+ *   - drivers/virtio/virtio_ring.c|1223| <<vring_interrupt>> if (!more_used(vq)) {
+ */
 static inline bool more_used(const struct vring_virtqueue *vq)
 {
 	return vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev, vq->vring.used->idx);
@@ -696,6 +839,12 @@ static inline bool more_used(const struct vring_virtqueue *vq)
  * Returns NULL if there are no used buffers, or the "data" token
  * handed to virtqueue_add_*().
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|932| <<receive_mergeable>> buf = virtqueue_get_buf_ctx(rq->vq, &len, &ctx);
+ *   - drivers/net/virtio_net.c|1331| <<virtnet_receive>> (buf = virtqueue_get_buf_ctx(rq->vq, &len, &ctx))) {
+ *   - drivers/virtio/virtio_ring.c|883| <<virtqueue_get_buf>> return virtqueue_get_buf_ctx(_vq, len, NULL);
+ */
 void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 			    void **ctx)
 {
@@ -736,10 +885,39 @@ void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 	/* detach_buf clears data, so grab it now. */
 	ret = vq->desc_state[i].data;
 	detach_buf(vq, i, ctx);
+	/*
+	 * 在以下设置vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|869| <<virtqueue_get_buf_ctx>> vq->last_used_idx++;
+	 *   - drivers/virtio/virtio_ring.c|1199| <<__vring_new_virtqueue>> vq->last_used_idx = 0;
+	 * 在以下使用vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|804| <<more_used>> return vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev, vq->vring.used->idx);
+	 *   - drivers/virtio/virtio_ring.c|853| <<virtqueue_get_buf_ctx>> last_used = (vq->last_used_idx & (vq->vring.num - 1));
+	 *   - drivers/virtio/virtio_ring.c|892| <<virtqueue_get_buf_ctx>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx));
+	 *   - drivers/virtio/virtio_ring.c|991| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+	 *   - drivers/virtio/virtio_ring.c|1089| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|1103| <<virtqueue_enable_cb_delayed>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));
+	 *   - drivers/virtio/virtio_ring.c|1105| <<virtqueue_enable_cb_delayed>> if (unlikely((u16)(virtio16_to_cpu(_vq->vdev, vq->vring.used->idx) - vq->last_used_idx) > bufs)) {
+	 */
 	vq->last_used_idx++;
 	/* If we expect an interrupt for the next entry, tell host
 	 * by writing event index and flush out the write before
 	 * the read in the next get_buf call. */
+	/*
+	 * 在以下使用vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|743| <<virtqueue_get_buf_ctx>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|775| <<virtqueue_disable_cb>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|776| <<virtqueue_disable_cb>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|778| <<virtqueue_disable_cb>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|808| <<virtqueue_enable_cb_prepare>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|809| <<virtqueue_enable_cb_prepare>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|811| <<virtqueue_enable_cb_prepare>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|883| <<virtqueue_enable_cb_delayed>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|884| <<virtqueue_enable_cb_delayed>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|990| <<__vring_new_virtqueue>> vq->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1005| <<__vring_new_virtqueue>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1007| <<__vring_new_virtqueue>> vq->vring.avail->flags = cpu_to_virtio16(vdev, vq->avail_flags_shadow);
+	 */
 	if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
 		virtio_store_mb(vq->weak_barriers,
 				&vring_used_event(&vq->vring),
@@ -768,10 +946,54 @@ EXPORT_SYMBOL_GPL(virtqueue_get_buf);
  *
  * Unlike other operations, this need not be serialized.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|216| <<virtblk_done>> virtqueue_disable_cb(vq);
+ *   - drivers/char/virtio_console.c|2190| <<virtcons_freeze>> virtqueue_disable_cb(portdev->c_ivq);
+ *   - drivers/char/virtio_console.c|2198| <<virtcons_freeze>> virtqueue_disable_cb(portdev->c_ivq);
+ *   - drivers/char/virtio_console.c|2201| <<virtcons_freeze>> virtqueue_disable_cb(port->in_vq);
+ *   - drivers/char/virtio_console.c|2202| <<virtcons_freeze>> virtqueue_disable_cb(port->out_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|47| <<virtcrypto_dataq_callback>> virtqueue_disable_cb(vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|206| <<virtio_gpu_dequeue_ctrl_func>> virtqueue_disable_cb(vgdev->ctrlq.vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|249| <<virtio_gpu_dequeue_cursor_func>> virtqueue_disable_cb(vgdev->cursorq.vq);
+ *   - drivers/net/caif/caif_virtio.c|203| <<cfv_release_used_buf>> virtqueue_disable_cb(cfv->vq_tx);
+ *   - drivers/net/caif/caif_virtio.c|462| <<cfv_netdev_close>> virtqueue_disable_cb(cfv->vq_tx);
+ *   - drivers/net/caif/caif_virtio.c|713| <<cfv_probe>> virtqueue_disable_cb(cfv->vq_tx);
+ *   - drivers/net/virtio_net.c|382| <<virtqueue_napi_schedule>> virtqueue_disable_cb(vq);
+ *   - drivers/net/virtio_net.c|402| <<virtqueue_napi_complete>> virtqueue_disable_cb(vq);
+ *   - drivers/net/virtio_net.c|412| <<skb_xmit_done>> virtqueue_disable_cb(vq);
+ *   - drivers/net/virtio_net.c|1611| <<start_xmit>> virtqueue_disable_cb(sq->vq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|523| <<rpmsg_downref_sleepers>> virtqueue_disable_cb(vrp->svq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|953| <<rpmsg_probe>> virtqueue_disable_cb(vrp->svq);
+ *   - drivers/scsi/virtio_scsi.c|221| <<virtscsi_vq_done>> virtqueue_disable_cb(vq);
+ *   - net/vmw_vsock/virtio_transport.c|320| <<virtio_transport_tx_work>> virtqueue_disable_cb(vq);
+ *   - net/vmw_vsock/virtio_transport.c|422| <<virtio_transport_event_work>> virtqueue_disable_cb(vq);
+ *   - net/vmw_vsock/virtio_transport.c|550| <<virtio_transport_rx_work>> virtqueue_disable_cb(vq);
+ *   - tools/virtio/virtio_test.c|166| <<run_test>> virtqueue_disable_cb(vq->vq);
+ *   - tools/virtio/vringh_test.c|394| <<bool>> virtqueue_disable_cb(vq);
+ *   - tools/virtio/vringh_test.c|427| <<bool>> virtqueue_disable_cb(vq);
+ */
 void virtqueue_disable_cb(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
 
+	/*
+	 * 在以下设置vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|776| <<virtqueue_disable_cb>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|809| <<virtqueue_enable_cb_prepare>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|884| <<virtqueue_enable_cb_delayed>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|990| <<__vring_new_virtqueue>> vq->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1005| <<__vring_new_virtqueue>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 * 在以下使用vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|743| <<virtqueue_get_buf_ctx>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|775| <<virtqueue_disable_cb>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|778| <<virtqueue_disable_cb>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|808| <<virtqueue_enable_cb_prepare>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|811| <<virtqueue_enable_cb_prepare>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|883| <<virtqueue_enable_cb_delayed>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1007| <<__vring_new_virtqueue>> vq->vring.avail->flags = cpu_to_virtio16(vdev, vq->avail_flags_shadow);
+	 */
 	if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
 		vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
 		if (!vq->event)
@@ -793,6 +1015,11 @@ EXPORT_SYMBOL_GPL(virtqueue_disable_cb);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * calld by:
+ *   - drivers/net/virtio_net.c|397| <<virtqueue_napi_complete>> opaque = virtqueue_enable_cb_prepare(vq);
+ *   - drivers/virtio/virtio_ring.c|879| <<virtqueue_enable_cb>> unsigned last_used_idx = virtqueue_enable_cb_prepare(_vq);
+ */
 unsigned virtqueue_enable_cb_prepare(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -805,11 +1032,31 @@ unsigned virtqueue_enable_cb_prepare(struct virtqueue *_vq)
 	/* Depending on the VIRTIO_RING_F_EVENT_IDX feature, we need to
 	 * either clear the flags bit or point the event index at the next
 	 * entry. Always do both to keep code simple. */
+	/*
+	 * 在以下设置vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|776| <<virtqueue_disable_cb>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|809| <<virtqueue_enable_cb_prepare>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|884| <<virtqueue_enable_cb_delayed>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|990| <<__vring_new_virtqueue>> vq->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1005| <<__vring_new_virtqueue>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 * 在以下使用vring_virtqueue->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|743| <<virtqueue_get_buf_ctx>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|775| <<virtqueue_disable_cb>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|778| <<virtqueue_disable_cb>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|808| <<virtqueue_enable_cb_prepare>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|811| <<virtqueue_enable_cb_prepare>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|883| <<virtqueue_enable_cb_delayed>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1007| <<__vring_new_virtqueue>> vq->vring.avail->flags = cpu_to_virtio16(vdev, vq->avail_flags_shadow);
+	 */
 	if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
 		vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
 		if (!vq->event)
 			vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
 	}
+	/*
+	 * 这里更新这是让backend有call的机会
+	 */
 	vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
 	END_USE(vq);
 	return last_used_idx;
@@ -825,6 +1072,11 @@ EXPORT_SYMBOL_GPL(virtqueue_enable_cb_prepare);
  *
  * This does not need to be serialized.
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|399| <<virtqueue_napi_complete>> if (unlikely(virtqueue_poll(vq, opaque)))
+ *   - drivers/virtio/virtio_ring.c|1046| <<virtqueue_enable_cb>> return !virtqueue_poll(_vq, last_used_idx);
+ */
 bool virtqueue_poll(struct virtqueue *_vq, unsigned last_used_idx)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -848,6 +1100,20 @@ EXPORT_SYMBOL_GPL(virtqueue_poll);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|225| <<virtblk_done>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|56| <<virtcrypto_dataq_callback>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|209| <<virtio_gpu_dequeue_ctrl_func>> } while (!virtqueue_enable_cb(vgdev->ctrlq.vq));
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|251| <<virtio_gpu_dequeue_cursor_func>> } while (!virtqueue_enable_cb(vgdev->cursorq.vq));
+ *   - drivers/net/caif/caif_virtio.c|565| <<cfv_netdev_tx>> virtqueue_enable_cb(cfv->vq_tx);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|496| <<rpmsg_upref_sleepers>> virtqueue_enable_cb(vrp->svq);
+ *   - drivers/scsi/virtio_scsi.c|227| <<virtscsi_vq_done>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|325| <<virtio_transport_tx_work>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|429| <<virtio_transport_event_work>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|581| <<virtio_transport_rx_work>> } while (!virtqueue_enable_cb(vq));
+ *   - tools/virtio/virtio_test.c|199| <<run_test>> if (virtqueue_enable_cb(vq->vq))
+ */
 bool virtqueue_enable_cb(struct virtqueue *_vq)
 {
 	unsigned last_used_idx = virtqueue_enable_cb_prepare(_vq);
@@ -868,6 +1134,14 @@ EXPORT_SYMBOL_GPL(virtqueue_enable_cb);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1512| <<start_xmit>> virtqueue_enable_cb_delayed(sq->vq);
+ *   - drivers/net/virtio_net.c|1550| <<start_xmit>> unlikely(!virtqueue_enable_cb_delayed(sq->vq))) {
+ *   - tools/virtio/virtio_test.c|196| <<run_test>> if (virtqueue_enable_cb_delayed(vq->vq))
+ *   - tools/virtio/vringh_test.c|387| <<bool>> if (!virtqueue_enable_cb_delayed(vq))
+ *   - tools/virtio/vringh_test.c|421| <<bool>> if (!virtqueue_enable_cb_delayed(vq))
+ */
 bool virtqueue_enable_cb_delayed(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -886,8 +1160,28 @@ bool virtqueue_enable_cb_delayed(struct virtqueue *_vq)
 			vq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);
 	}
 	/* TODO: tune this threshold */
+	/*
+	 * 在以下修改vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|432| <<virtqueue_add>> vq->avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|961| <<virtqueue_detach_unused_buf>> vq->avail_idx_shadow--;
+	 *   - drivers/virtio/virtio_ring.c|1025| <<__vring_new_virtqueue>> vq->avail_idx_shadow = 0;
+	 *
+	 * 在以下设置vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|869| <<virtqueue_get_buf_ctx>> vq->last_used_idx++;
+	 *   - drivers/virtio/virtio_ring.c|1199| <<__vring_new_virtqueue>> vq->last_used_idx = 0;
+	 */
 	bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vringh.c|478| <<__vringh_need_notify>> err = getu16(vrh, &used_event, &vring_used_event(&vrh->vring));
+	 *   - drivers/vhost/vringh.c|481| <<__vringh_need_notify>> &vring_used_event(&vrh->vring));
+	 *   - drivers/virtio/virtio_ring.c|869| <<virtqueue_get_buf_ctx>> &vring_used_event(&vq->vring),
+	 *   - drivers/virtio/virtio_ring.c|942| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+	 *   - drivers/virtio/virtio_ring.c|1029| <<virtqueue_enable_cb_delayed>> &vring_used_event(&vq->vring),
+	 *   - tools/virtio/ringtest/virtio_ring_0_9.c|219| <<enable_call>> vring_used_event(&ring) = guest.last_used_idx;
+	 *   - tools/virtio/ringtest/virtio_ring_0_9.c|326| <<call_used>> need = vring_need_event(vring_used_event(&ring),
+	 */
 	virtio_store_mb(vq->weak_barriers,
 			&vring_used_event(&vq->vring),
 			cpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));
@@ -1156,6 +1450,16 @@ void vring_del_virtqueue(struct virtqueue *_vq)
 EXPORT_SYMBOL_GPL(vring_del_virtqueue);
 
 /* Manipulates transport-specific feature bits. */
+/*
+ * called by:
+ *   - drivers/misc/mic/vop/vop_main.c|142| <<vop_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|213| <<rproc_virtio_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/s390/virtio/kvm_virtio.c|104| <<kvm_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|801| <<virtio_ccw_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_mmio.c|131| <<vm_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_legacy.c|38| <<vp_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_modern.c|162| <<vp_finalize_features>> vring_transport_features(vdev);
+ */
 void vring_transport_features(struct virtio_device *vdev)
 {
 	unsigned int i;
@@ -1206,6 +1510,12 @@ EXPORT_SYMBOL_GPL(virtqueue_is_broken);
  * This should prevent the device from being used, allowing drivers to
  * recover.  You may need to grab appropriate locks to flush.
  */
+/*
+ * called by:
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|233| <<virtcrypto_update_status>> virtio_break_device(vcrypto->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1161| <<virtio_ccw_remove>> virtio_break_device(&vcdev->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1178| <<virtio_ccw_offline>> virtio_break_device(&vcdev->vdev);
+ */
 void virtio_break_device(struct virtio_device *dev)
 {
 	struct virtqueue *_vq;
@@ -1249,6 +1559,12 @@ dma_addr_t virtqueue_get_used_addr(struct virtqueue *_vq)
 }
 EXPORT_SYMBOL_GPL(virtqueue_get_used_addr);
 
+/*
+ * called by:
+ *   - include/linux/virtio.h|99| <<virtqueue_get_desc>> return virtqueue_get_vring(vq)->desc;
+ *   - include/linux/virtio.h|103| <<virtqueue_get_avail>> return virtqueue_get_vring(vq)->avail;
+ *   - include/linux/virtio.h|107| <<virtqueue_get_used>> return virtqueue_get_vring(vq)->used;
+ */
 const struct vring *virtqueue_get_vring(struct virtqueue *vq)
 {
 	return &to_vvq(vq)->vring;
diff --git a/include/linux/hrtimer.h b/include/linux/hrtimer.h
index 5511dc963dd5..427d48327227 100644
--- a/include/linux/hrtimer.h
+++ b/include/linux/hrtimer.h
@@ -184,6 +184,33 @@ struct hrtimer_cpu_base {
 	unsigned int			in_hrtirq	: 1,
 					hres_active	: 1,
 					hang_detected	: 1;
+	/*
+	 * 在以下使用hrtimer_cpu_base->expires_next:
+	 *   - kernel/time/hrtimer.c|175| <<hrtimer_check_target>> return expires <= new_base->cpu_base->expires_next;
+	 *   - kernel/time/hrtimer.c|470| <<__hrtimer_get_next_event>> ktime_t expires, expires_next = KTIME_MAX;
+	 *   - kernel/time/hrtimer.c|483| <<__hrtimer_get_next_event>> if (expires < expires_next) {
+	 *   - kernel/time/hrtimer.c|484| <<__hrtimer_get_next_event>> expires_next = expires;
+	 *   - kernel/time/hrtimer.c|493| <<__hrtimer_get_next_event>> if (expires_next < 0)
+	 *   - kernel/time/hrtimer.c|494| <<__hrtimer_get_next_event>> expires_next = 0; 
+	 *   - kernel/time/hrtimer.c|495| <<__hrtimer_get_next_event>> return expires_next;
+	 *   - kernel/time/hrtimer.c|558| <<hrtimer_force_reprogram>> ktime_t expires_next; 
+	 *   - kernel/time/hrtimer.c|563| <<hrtimer_force_reprogram>> expires_next = __hrtimer_get_next_event(cpu_base);
+	 *   - kernel/time/hrtimer.c|565| <<hrtimer_force_reprogram>> if (skip_equal && expires_next == cpu_base->expires_next)
+	 *   - kernel/time/hrtimer.c|568| <<hrtimer_force_reprogram>> cpu_base->expires_next = expires_next;
+	 *   - kernel/time/hrtimer.c|587| <<hrtimer_force_reprogram>> tick_program_event(cpu_base->expires_next, 1);
+	 *   - kernel/time/hrtimer.c|629| <<hrtimer_reprogram>> if (expires >= cpu_base->expires_next)
+	 *   - kernel/time/hrtimer.c|648| <<hrtimer_reprogram>> cpu_base->expires_next = expires;
+	 *   - kernel/time/hrtimer.c|657| <<hrtimer_init_hres>> base->expires_next = KTIME_MAX;
+	 *   - kernel/time/hrtimer.c|1306| <<hrtimer_interrupt>> ktime_t expires_next, now, entry_time, delta;
+	 *   - kernel/time/hrtimer.c|1324| <<hrtimer_interrupt>> cpu_base->expires_next = KTIME_MAX;
+	 *   - kernel/time/hrtimer.c|1347| <<hrtimer_interrupt>> expires_next = __hrtimer_get_next_event(cpu_base);
+	 *   - kernel/time/hrtimer.c|1352| <<hrtimer_interrupt>> cpu_base->expires_next = expires_next;
+	 *   - kernel/time/hrtimer.c|1357| <<hrtimer_interrupt>> if (!tick_program_event(expires_next, 0)) {
+	 *   - kernel/time/hrtimer.c|1397| <<hrtimer_interrupt>> expires_next = ktime_add_ns(now, 100 * NSEC_PER_MSEC);
+	 *   - kernel/time/hrtimer.c|1399| <<hrtimer_interrupt>> expires_next = ktime_add(now, delta);
+	 *   - kernel/time/hrtimer.c|1400| <<hrtimer_interrupt>> tick_program_event(expires_next, 1);
+	 *   - kernel/time/timer_list.c|155| <<print_cpu>> P_ns(expires_next);
+	 */
 	ktime_t				expires_next;
 	struct hrtimer			*next_timer;
 	unsigned int			nr_events;
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b2020579bfc3..fd20619c13e7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -373,7 +373,7 @@ struct kvm_kernel_irq_routing_entry {
 			u32 data;
 			u32 flags;
 			u32 devid;
-		} msi;
+		} msi; // --->>>> 注意 msi !!!
 		struct kvm_s390_adapter_int adapter;
 		struct kvm_hv_sint hv_sint;
 	};
@@ -382,6 +382,13 @@ struct kvm_kernel_irq_routing_entry {
 
 #ifdef CONFIG_HAVE_KVM_IRQ_ROUTING
 struct kvm_irq_routing_table {
+	/*
+	 * 对于x86 ...
+	 * #define KVM_IRQCHIP_PIC_MASTER   0
+	 * #define KVM_IRQCHIP_PIC_SLAVE    1
+	 * #define KVM_IRQCHIP_IOAPIC       2
+	 * #define KVM_NR_IRQCHIPS          3
+	 */
 	int chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS];
 	u32 nr_rt_entries;
 	/*
@@ -1191,6 +1198,10 @@ static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 
 static inline bool kvm_request_pending(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> u64 requests;
+	 */
 	return READ_ONCE(vcpu->requests);
 }
 
diff --git a/include/uapi/linux/kvm_para.h b/include/uapi/linux/kvm_para.h
index 6c0ce49931e5..7cb30a859edb 100644
--- a/include/uapi/linux/kvm_para.h
+++ b/include/uapi/linux/kvm_para.h
@@ -27,6 +27,12 @@
 #define KVM_HC_MIPS_EXIT_VM		7
 #define KVM_HC_MIPS_CONSOLE_OUTPUT	8
 #define KVM_HC_CLOCK_PAIRING		9
+/*
+ * 在以下使用KVM_HC_SEND_IPI:
+ *   - arch/x86/kernel/kvm.c|493| <<__send_ipi_mask>> kvm_hypercall4(KVM_HC_SEND_IPI, (unsigned long )ipi_bitmap,
+ *   - arch/x86/kernel/kvm.c|502| <<__send_ipi_mask>> kvm_hypercall4(KVM_HC_SEND_IPI, (unsigned long )ipi_bitmap,
+ *   - arch/x86/kvm/x86.c|7185| <<kvm_emulate_hypercall>> case KVM_HC_SEND_IPI:
+ */
 #define KVM_HC_SEND_IPI		10
 
 /*
diff --git a/include/uapi/linux/vhost.h b/include/uapi/linux/vhost.h
index c51f8e5cc608..f15280a907c4 100644
--- a/include/uapi/linux/vhost.h
+++ b/include/uapi/linux/vhost.h
@@ -124,6 +124,31 @@ struct vhost_memory {
  * be modified while ring is running (bound to a device). */
 #define VHOST_SET_VRING_NUM _IOW(VHOST_VIRTIO, 0x10, struct vhost_vring_state)
 /* Set addresses for the ring. */
+/*
+ * 这是gdb的qemu-6.0.0的调用
+ * (gdb) bt
+ * #0  0x000055be200149a4 in vhost_virtqueue_start (dev=0x55be21df0890, vdev=0x55be22b90300, vq=0x55be21df0b50, idx=3) at ../hw/virtio/vhost.c:1057
+ * #1  0x000055be20016891 in vhost_dev_start (hdev=0x55be21df0890, vdev=0x55be22b90300) at ../hw/virtio/vhost.c:1734
+ * #2  0x000055be1fccc8b5 in vhost_net_start_one (net=0x55be21df0890, dev=0x55be22b90300) at ../hw/net/vhost_net.c:246
+ * #3  0x000055be1fcccd73 in vhost_net_start (dev=0x55be22b90300, ncs=0x55be22baac80, total_queues=2) at ../hw/net/vhost_net.c:351
+ * #4  0x000055be200095e1 in virtio_net_vhost_status (n=0x55be22b90300, status=15 '\017') at ../hw/net/virtio-net.c:288
+ * #5  0x000055be2000988c in virtio_net_set_status (vdev=0x55be22b90300, status=15 '\017') at ../hw/net/virtio-net.c:369
+ * #6  0x000055be1ffd07f0 in virtio_set_status (vdev=0x55be22b90300, val=15 '\017') at ../hw/virtio/virtio.c:1958
+ * #7  0x000055be1fc7d0af in virtio_pci_common_write (opaque=0x55be22b88160, addr=20, val=15, size=1) at ../hw/virtio/virtio-pci.c:1260
+ * #8  0x000055be2003abd3 in memory_region_write_accessor (mr=0x55be22b88b50, addr=20, value=0x7f68777fd6c8, size=1, shift=0, mask=255, attrs=...) at ../softmmu/memory.c:491
+ * #9  0x000055be2003adf7 in access_with_adjusted_size (addr=20, value=0x7f68777fd6c8, size=1, access_size_min=1, access_size_max=4, access_fn= 0x55be2003aaee <memory_region_write_accessor>,
+ *                              mr=0x55be22b88b50, attrs=...) at ../softmmu/memory.c:552
+ * #10 0x000055be2003ddf6 in memory_region_dispatch_write (mr=0x55be22b88b50, addr=20, data=15, op=MO_8, attrs=...) at ../softmmu/memory.c:1502
+ * #11 0x000055be2006a2cc in flatview_write_continue (fv=0x7f686c008230, addr=4261412884, attrs=..., ptr=0x7f69877a2028, len=1, addr1=20, l=1, mr=0x55be22b88b50) at ../softmmu/physmem.c:2746
+ * #12 0x000055be2006a411 in flatview_write (fv=0x7f686c008230, addr=4261412884, attrs=..., buf=0x7f69877a2028, len=1) at ../softmmu/physmem.c:2786
+ * #13 0x000055be2006a77d in address_space_write (as=0x55be20c94bc0 <address_space_memory>, addr=4261412884, attrs=..., buf=0x7f69877a2028, len=1) at ../softmmu/physmem.c:2878
+ * #14 0x000055be2006a7ea in address_space_rw (as=0x55be20c94bc0 <address_space_memory>, addr=4261412884, attrs=..., buf=0x7f69877a2028, len=1, is_write=true) at ../softmmu/physmem.c:2888
+ * #15 0x000055be1fff59c6 in kvm_cpu_exec (cpu=0x55be21e89650) at ../accel/kvm/kvm-all.c:2517
+ * #16 0x000055be20037aa9 in kvm_vcpu_thread_fn (arg=0x55be21e89650) at ../accel/kvm/kvm-accel-ops.c:49
+ * #17 0x000055be202437ea in qemu_thread_start (args=0x55be21e96780) at ../util/qemu-thread-posix.c:521
+ * #18 0x00007f6984be0ea5 in start_thread () at /lib64/libpthread.so.0
+ * #19 0x00007f69849098cd in clone () at /lib64/libc.so.6
+ */
 #define VHOST_SET_VRING_ADDR _IOW(VHOST_VIRTIO, 0x11, struct vhost_vring_addr)
 /* Base value where queue looks for available descriptors */
 #define VHOST_SET_VRING_BASE _IOW(VHOST_VIRTIO, 0x12, struct vhost_vring_state)
diff --git a/include/uapi/linux/virtio_ring.h b/include/uapi/linux/virtio_ring.h
index 6d5d5faa989b..04fc1616e260 100644
--- a/include/uapi/linux/virtio_ring.h
+++ b/include/uapi/linux/virtio_ring.h
@@ -47,10 +47,33 @@
 /* The Host uses this in used->flags to advise the Guest: don't kick me when
  * you add a buffer.  It's unreliable, so it's simply an optimization.  Guest
  * will still kick if it's out of buffers. */
+/*
+ * 在以下使用VRING_USED_F_NO_NOTIFY:
+ *   - drivers/vhost/vhost.c|2252| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+ *   - drivers/vhost/vhost.c|2500| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+ *   - drivers/vhost/vhost.c|2502| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+ *   - drivers/vhost/vhost.c|2537| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+ *   - drivers/vhost/vhost.c|2539| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+ *   - drivers/vhost/vringh.c|545| <<__vringh_notify_disable>> VRING_USED_F_NO_NOTIFY)) {
+ *   - drivers/virtio/virtio_ring.c|672| <<virtqueue_kick_prepare>> needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
+ */
 #define VRING_USED_F_NO_NOTIFY	1
 /* The Guest uses this in avail->flags to advise the Host: don't interrupt me
  * when you consume a buffer.  It's unreliable, so it's simply an
  * optimization.  */
+/*
+ * 在以下使用VRING_AVAIL_F_NO_INTERRUPT:
+ *   - drivers/vhost/vhost.c|2400| <<vhost_notify>> return !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));
+ *   - drivers/vhost/vringh.c|474| <<__vringh_need_notify>> return (!(flags & VRING_AVAIL_F_NO_INTERRUPT));
+ *   - drivers/virtio/virtio_ring.c|916| <<virtqueue_get_buf_ctx>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+ *   - drivers/virtio/virtio_ring.c|992| <<virtqueue_disable_cb>> if (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+ *   - drivers/virtio/virtio_ring.c|993| <<virtqueue_disable_cb>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+ *   - drivers/virtio/virtio_ring.c|1030| <<virtqueue_enable_cb_prepare>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+ *   - drivers/virtio/virtio_ring.c|1031| <<virtqueue_enable_cb_prepare>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+ *   - drivers/virtio/virtio_ring.c|1135| <<virtqueue_enable_cb_delayed>> if (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+ *   - drivers/virtio/virtio_ring.c|1136| <<virtqueue_enable_cb_delayed>> vq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+ *   - drivers/virtio/virtio_ring.c|1267| <<bool>> vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+ */
 #define VRING_AVAIL_F_NO_INTERRUPT	1
 
 /* We support indirect buffer descriptors */
@@ -60,6 +83,22 @@
  * at the end of the avail ring. Host should ignore the avail->flags field. */
 /* The Host publishes the avail index for which it expects a kick
  * at the end of the used ring. Guest should ignore the used->flags field. */
+/*
+ * 在以下使用VIRTIO_RING_F_EVENT_IDX:
+ *   - drivers/vhost/vhost.c|1199| <<vq_access_ok>> size_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
+ *   - drivers/vhost/vhost.c|1257| <<vq_iotlb_prefetch>> size_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
+ *   - drivers/vhost/vhost.c|1289| <<vq_log_access_ok>> size_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
+ *   - drivers/vhost/vhost.c|2464| <<vhost_notify>> if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+ *   - drivers/vhost/vhost.c|2638| <<vhost_enable_notify>> if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+ *   - drivers/vhost/vhost.c|2686| <<vhost_disable_notify>> if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+ *   - drivers/vhost/vringh.c|619| <<vringh_init_user>> vrh->event_indices = (features & (1 << VIRTIO_RING_F_EVENT_IDX));
+ *   - drivers/vhost/vringh.c|880| <<vringh_init_kern>> vrh->event_indices = (features & (1 << VIRTIO_RING_F_EVENT_IDX));
+ *   - drivers/virtio/virtio_ring.c|1295| <<bool>> vq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);
+ *   - drivers/virtio/virtio_ring.c|1471| <<vring_transport_features>> case VIRTIO_RING_F_EVENT_IDX:
+ *   - tools/virtio/virtio_test.c|265| <<main>> (1ULL << VIRTIO_RING_F_EVENT_IDX) | (1ULL << VIRTIO_F_VERSION_1);
+ *   - tools/virtio/virtio_test.c|278| <<main>> features &= ~(1ULL << VIRTIO_RING_F_EVENT_IDX);
+ *   - tools/virtio/vringh_test.c|461| <<main>> __virtio_set_bit(&vdev, VIRTIO_RING_F_EVENT_IDX);
+ */
 #define VIRTIO_RING_F_EVENT_IDX		29
 
 /* Virtio ring descriptors: 16 bytes.  These can chain together via "next". */
@@ -137,9 +176,33 @@ struct vring {
  */
 /* We publish the used event index at the end of the available ring, and vice
  * versa. They are at the end for backwards compatibility. */
+/*
+ * called by:
+ *   - drivers/vhost/vringh.c|478| <<__vringh_need_notify>> err = getu16(vrh, &used_event, &vring_used_event(&vrh->vring));
+ *   - drivers/vhost/vringh.c|481| <<__vringh_need_notify>> &vring_used_event(&vrh->vring));
+ *   - drivers/virtio/virtio_ring.c|869| <<virtqueue_get_buf_ctx>> &vring_used_event(&vq->vring),
+ *   - drivers/virtio/virtio_ring.c|942| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|1029| <<virtqueue_enable_cb_delayed>> &vring_used_event(&vq->vring),
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|219| <<enable_call>> vring_used_event(&ring) = guest.last_used_idx;
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|326| <<call_used>> need = vring_need_event(vring_used_event(&ring),
+ */
 #define vring_used_event(vr) ((vr)->avail->ring[(vr)->num])
+/*
+ * called by:
+ *   - drivers/vhost/vringh.c|514| <<__vringh_notify_enable>> if (putu16(vrh, &vring_avail_event(&vrh->vring),
+ *   - drivers/vhost/vringh.c|517| <<__vringh_notify_enable>> &vring_avail_event(&vrh->vring));
+ *   - drivers/virtio/virtio_ring.c|669| <<virtqueue_kick_prepare>> needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev, vring_avail_event(&vq->vring)),
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|233| <<kick_available>> need = vring_need_event(vring_avail_event(&ring),
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|252| <<enable_kick>> vring_avail_event(&ring) = host.used_idx;
+ */
 #define vring_avail_event(vr) (*(__virtio16 *)&(vr)->used->ring[(vr)->num])
 
+/*
+ * called by:
+ *   - drivers/misc/mic/vop/vop_vringh.c|336| <<vop_virtio_add_device>> vring_init(&vr->vr, num, vr->va, MIC_VIRTIO_RING_ALIGN);
+ *   - drivers/virtio/virtio_ring.c|1403| <<bool>> vring_init(&vring, num, queue, vring_align);
+ *   - drivers/virtio/virtio_ring.c|1433| <<bool>> vring_init(&vring, num, pages, vring_align);
+ */
 static inline void vring_init(struct vring *vr, unsigned int num, void *p,
 			      unsigned long align)
 {
@@ -152,6 +215,17 @@ static inline void vring_init(struct vring *vr, unsigned int num, void *p,
 
 static inline unsigned vring_size(unsigned int num, unsigned long align)
 {
+	/*
+	 * sizeof(struct vring_desc) * num
+	 *
+	 * sizeof(__virtio16) * num
+	 *
+	 * sizeof(struct vring_used_elem) * num
+	 *
+	 *
+	 * sizeof(__virtio16) * 3: avail flags + idx + event
+	 * sizeof(__virtio16) * 3: used flags + idx + event
+	 */
 	return ((sizeof(struct vring_desc) * num + sizeof(__virtio16) * (3 + num)
 		 + align - 1) & ~(align - 1))
 		+ sizeof(__virtio16) * 3 + sizeof(struct vring_used_elem) * num;
@@ -161,6 +235,14 @@ static inline unsigned vring_size(unsigned int num, unsigned long align)
 /* Assuming a given event_idx value from the other side, if
  * we have just incremented index from old to new_idx,
  * should we trigger an event? */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2424| <<vhost_notify>> return vring_need_event(vhost16_to_cpu(vq, event), new, old);
+ *   - drivers/vhost/vringh.c|489| <<__vringh_need_notify>> notify = vring_need_event(used_event,
+ *   - drivers/virtio/virtio_ring.c|669| <<virtqueue_kick_prepare>> needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev, vring_avail_event(&vq->vring)), new, old);
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|233| <<kick_available>> need = vring_need_event(vring_avail_event(&ring),
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|326| <<call_used>> need = vring_need_event(vring_used_event(&ring),
+ */
 static inline int vring_need_event(__u16 event_idx, __u16 new_idx, __u16 old)
 {
 	/* Note: Xen has similar logic for notification hold-off
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index d2a1267ee462..a8c7a3479442 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3051,6 +3051,24 @@ unsigned long long task_sched_runtime(struct task_struct *p)
  * This function gets called by the timer code, with HZ frequency.
  * We call it with interrupts disabled.
  */
+/*
+ * 5.4上的例子
+ * scheduler_tick
+ * tick_sched_handle
+ * tick_sched_timer
+ * __hrtimer_run_queues
+ * hrtimer_interrupt
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ * native_safe_halt
+ * default_idle
+ * arch_cpu_idle
+ * default_idle_call
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * secondary_startup_64
+ */
 void scheduler_tick(void)
 {
 	int cpu = smp_processor_id();
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e9f04ce90dc4..79d1089fc3c6 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1731,6 +1731,11 @@ static inline void sub_nr_running(struct rq *rq, unsigned count)
 	sched_update_tick_dependency(rq);
 }
 
+/*
+ * called by:
+ *   - kernel/sched/core.c|3097| <<scheduler_tick>> rq_last_tick_reset(rq);
+ *   - kernel/sched/idle_task.c|51| <<put_prev_task_idle>> rq_last_tick_reset(rq);
+ */
 static inline void rq_last_tick_reset(struct rq *rq)
 {
 #ifdef CONFIG_NO_HZ_FULL
diff --git a/kernel/sched/stats.h b/kernel/sched/stats.h
index 5d30f4b7d344..ecb2d771021f 100644
--- a/kernel/sched/stats.h
+++ b/kernel/sched/stats.h
@@ -17,6 +17,10 @@ rq_sched_info_arrive(struct rq *rq, unsigned long long delta)
 /*
  * Expects runqueue lock to be held for atomicity of update
  */
+/*
+ * called by:
+ *   - kernel/sched/stats.h|213| <<sched_info_depart>> rq_sched_info_depart(rq, delta);
+ */
 static inline void
 rq_sched_info_depart(struct rq *rq, unsigned long long delta)
 {
@@ -205,6 +209,10 @@ static inline void sched_info_queued(struct rq *rq, struct task_struct *t)
  * sched_info_queued() to mark that it has now again started waiting on
  * the runqueue.
  */
+/*
+ * called by:
+ *   - kernel/sched/stats.h|234| <<__sched_info_switch>> sched_info_depart(rq, prev);
+ */
 static inline void sched_info_depart(struct rq *rq, struct task_struct *t)
 {
 	unsigned long long delta = rq_clock(rq) -
diff --git a/kernel/smp.c b/kernel/smp.c
index 44ce7a6dd1bc..7648db56ff18 100644
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@ -437,6 +437,10 @@ void smp_call_function_many(const struct cpumask *mask,
 
 	cfd = this_cpu_ptr(&cfd_data);
 
+	/*
+	 * 应该是把mask和cpu_online_mask给and
+	 * 结果到cfd->cpumask
+	 */
 	cpumask_and(cfd->cpumask, mask, cpu_online_mask);
 	__cpumask_clear_cpu(this_cpu, cfd->cpumask);
 
diff --git a/kernel/time/clockevents.c b/kernel/time/clockevents.c
index 4237e0744e26..e83bfafba8ef 100644
--- a/kernel/time/clockevents.c
+++ b/kernel/time/clockevents.c
@@ -303,6 +303,32 @@ static int clockevents_program_min_delta(struct clock_event_device *dev)
  *
  * Returns 0 on success, -ETIME when the event is in the past.
  */
+/*
+ * 一个例子
+ * clockevents_program_event
+ * hrtimer_interrupt
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ * native_safe_halt
+ * default_idle
+ * arch_cpu_idle
+ * default_idle_call
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * secondary_startup_64
+ *
+ * called by:
+ *   - kernel/time/clockevents.c|520| <<__clockevents_update_freq>> return clockevents_program_event(dev, dev->next_event, false);
+ *   - kernel/time/tick-broadcast.c|334| <<tick_handle_periodic_broadcast>> clockevents_program_event(dev, next, true);
+ *   - kernel/time/tick-broadcast.c|568| <<tick_broadcast_set_event>> clockevents_program_event(bc, expires, 1);
+ *   - kernel/time/tick-broadcast.c|952| <<hotplug_cpu__broadcast_tick_pull>> clockevents_program_event(bc, bc->next_event, 1);
+ *   - kernel/time/tick-common.c|125| <<tick_handle_periodic>> if (!clockevents_program_event(dev, next, false))
+ *   - kernel/time/tick-common.c|167| <<tick_setup_periodic>> if (!clockevents_program_event(dev, next, false))
+ *   - kernel/time/tick-oneshot.c|47| <<tick_program_event>> return clockevents_program_event(dev, expires, force);
+ *   - kernel/time/tick-oneshot.c|58| <<tick_resume_oneshot>> clockevents_program_event(dev, ktime_get(), true);
+ *   - kernel/time/tick-oneshot.c|70| <<tick_setup_oneshot>> clockevents_program_event(newdev, next_event, true);
+ */
 int clockevents_program_event(struct clock_event_device *dev, ktime_t expires,
 			      bool force)
 {
@@ -336,6 +362,9 @@ int clockevents_program_event(struct clock_event_device *dev, ktime_t expires,
 	delta = max(delta, (int64_t) dev->min_delta_ns);
 
 	clc = ((unsigned long long) delta * dev->mult) >> dev->shift;
+	/*
+	 * lapic_next_deadline()
+	 */
 	rc = dev->set_next_event((unsigned long) clc, dev);
 
 	return (rc && force) ? clockevents_program_min_delta(dev) : rc;
diff --git a/kernel/time/hrtimer.c b/kernel/time/hrtimer.c
index e2c41ab09461..de3bf3ec3ef4 100644
--- a/kernel/time/hrtimer.c
+++ b/kernel/time/hrtimer.c
@@ -1295,6 +1295,11 @@ static void __hrtimer_run_queues(struct hrtimer_cpu_base *cpu_base, ktime_t now)
  * High resolution timer interrupt
  * Called with interrupts disabled
  */
+/*
+ * 在以下使用hrtimer_interrupt():
+ *   - kernel/time/hrtimer.c|1392| <<__hrtimer_peek_ahead_timers>> hrtimer_interrupt(td->evtdev);
+ *   - kernel/time/tick-oneshot.c|130| <<tick_init_highres>> return tick_switch_to_oneshot(hrtimer_interrupt);
+ */
 void hrtimer_interrupt(struct clock_event_device *dev)
 {
 	struct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);
@@ -1318,6 +1323,24 @@ void hrtimer_interrupt(struct clock_event_device *dev)
 	 */
 	cpu_base->expires_next = KTIME_MAX;
 
+	/*
+	 * 5.4上的例子
+	 * scheduler_tick
+	 * tick_sched_handle
+	 * tick_sched_timer
+	 * __hrtimer_run_queues
+	 * hrtimer_interrupt
+	 * smp_apic_timer_interrupt
+	 * apic_timer_interrupt
+	 * native_safe_halt
+	 * default_idle
+	 * arch_cpu_idle
+	 * default_idle_call
+	 * do_idle
+	 * cpu_startup_entry
+	 * start_secondary
+	 * secondary_startup_64
+	 */
 	__hrtimer_run_queues(cpu_base, now);
 
 	/* Reevaluate the clock bases for the next expiry */
@@ -1326,6 +1349,33 @@ void hrtimer_interrupt(struct clock_event_device *dev)
 	 * Store the new expiry value so the migration code can verify
 	 * against it.
 	 */
+	/*
+	 * 在以下使用hrtimer_cpu_base->expires_next:
+	 *   - kernel/time/hrtimer.c|175| <<hrtimer_check_target>> return expires <= new_base->cpu_base->expires_next;
+	 *   - kernel/time/hrtimer.c|470| <<__hrtimer_get_next_event>> ktime_t expires, expires_next = KTIME_MAX;
+	 *   - kernel/time/hrtimer.c|483| <<__hrtimer_get_next_event>> if (expires < expires_next) {
+	 *   - kernel/time/hrtimer.c|484| <<__hrtimer_get_next_event>> expires_next = expires;
+	 *   - kernel/time/hrtimer.c|493| <<__hrtimer_get_next_event>> if (expires_next < 0)
+	 *   - kernel/time/hrtimer.c|494| <<__hrtimer_get_next_event>> expires_next = 0;
+	 *   - kernel/time/hrtimer.c|495| <<__hrtimer_get_next_event>> return expires_next;
+	 *   - kernel/time/hrtimer.c|558| <<hrtimer_force_reprogram>> ktime_t expires_next;
+	 *   - kernel/time/hrtimer.c|563| <<hrtimer_force_reprogram>> expires_next = __hrtimer_get_next_event(cpu_base);
+	 *   - kernel/time/hrtimer.c|565| <<hrtimer_force_reprogram>> if (skip_equal && expires_next == cpu_base->expires_next)
+	 *   - kernel/time/hrtimer.c|568| <<hrtimer_force_reprogram>> cpu_base->expires_next = expires_next;
+	 *   - kernel/time/hrtimer.c|587| <<hrtimer_force_reprogram>> tick_program_event(cpu_base->expires_next, 1);
+	 *   - kernel/time/hrtimer.c|629| <<hrtimer_reprogram>> if (expires >= cpu_base->expires_next)
+	 *   - kernel/time/hrtimer.c|648| <<hrtimer_reprogram>> cpu_base->expires_next = expires;
+	 *   - kernel/time/hrtimer.c|657| <<hrtimer_init_hres>> base->expires_next = KTIME_MAX;
+	 *   - kernel/time/hrtimer.c|1306| <<hrtimer_interrupt>> ktime_t expires_next, now, entry_time, delta;
+	 *   - kernel/time/hrtimer.c|1324| <<hrtimer_interrupt>> cpu_base->expires_next = KTIME_MAX;
+	 *   - kernel/time/hrtimer.c|1347| <<hrtimer_interrupt>> expires_next = __hrtimer_get_next_event(cpu_base);
+	 *   - kernel/time/hrtimer.c|1352| <<hrtimer_interrupt>> cpu_base->expires_next = expires_next;
+	 *   - kernel/time/hrtimer.c|1357| <<hrtimer_interrupt>> if (!tick_program_event(expires_next, 0)) {
+	 *   - kernel/time/hrtimer.c|1397| <<hrtimer_interrupt>> expires_next = ktime_add_ns(now, 100 * NSEC_PER_MSEC);
+	 *   - kernel/time/hrtimer.c|1399| <<hrtimer_interrupt>> expires_next = ktime_add(now, delta);
+	 *   - kernel/time/hrtimer.c|1400| <<hrtimer_interrupt>> tick_program_event(expires_next, 1);
+	 *   - kernel/time/timer_list.c|155| <<print_cpu>> P_ns(expires_next);
+	 */
 	cpu_base->expires_next = expires_next;
 	cpu_base->in_hrtirq = 0;
 	raw_spin_unlock(&cpu_base->lock);
diff --git a/kernel/time/tick-oneshot.c b/kernel/time/tick-oneshot.c
index 6b009c207671..e705db3c58de 100644
--- a/kernel/time/tick-oneshot.c
+++ b/kernel/time/tick-oneshot.c
@@ -24,6 +24,19 @@
 /**
  * tick_program_event
  */
+/*
+ * called by:
+ *   - kernel/time/hrtimer.c|587| <<hrtimer_force_reprogram>> tick_program_event(cpu_base->expires_next, 1);
+ *   - kernel/time/hrtimer.c|649| <<hrtimer_reprogram>> tick_program_event(expires, 1);
+ *   - kernel/time/hrtimer.c|1339| <<hrtimer_interrupt>> if (!tick_program_event(expires_next, 0)) {
+ *   - kernel/time/hrtimer.c|1382| <<hrtimer_interrupt>> tick_program_event(expires_next, 1);
+ *   - kernel/time/tick-broadcast.c|845| <<__tick_broadcast_oneshot_control>> tick_program_event(dev->next_event, 1);
+ *   - kernel/time/tick-internal.h|105| <<__tick_broadcast_oneshot_control>> extern int tick_program_event(ktime_t expires, int force);
+ *   - kernel/time/tick-sched.c|672| <<tick_nohz_restart>> tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
+ *   - kernel/time/tick-sched.c|831| <<tick_nohz_stop_sched_tick>> tick_program_event(tick, 1);
+ *   - kernel/time/tick-sched.c|1129| <<tick_nohz_handler>> tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
+ *   - kernel/time/tick-sched.c|1166| <<tick_nohz_switch_to_nohz>> tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
+ */
 int tick_program_event(ktime_t expires, int force)
 {
 	struct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);
diff --git a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c
index 3c7b400512eb..83f9f5815266 100644
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@ -1206,6 +1206,10 @@ void tick_irq_enter(void)
  * We rearm the timer until we get disabled by the idle code.
  * Called with interrupts disabled.
  */
+/*
+ * 在以下使用tick_sched_timer():
+ *   - kernel/time/tick-sched.c|1258| <<tick_setup_sched_timer>> ts->sched_timer.function = tick_sched_timer;
+ */
 static enum hrtimer_restart tick_sched_timer(struct hrtimer *timer)
 {
 	struct tick_sched *ts =
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index e4d90224507a..4c0b8d37febd 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -201,6 +201,10 @@ irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 	if (flags & POLLIN) {
 		idx = srcu_read_lock(&kvm->irq_srcu);
 		do {
+			/*
+			 * struct kvm_kernel_irqfd *irqfd:
+			 * -> struct kvm_kernel_irq_routing_entry irq_entry;
+			 */
 			seq = read_seqcount_begin(&irqfd->irq_entry_sc);
 			irq = irqfd->irq_entry;
 		} while (read_seqcount_retry(&irqfd->irq_entry_sc, seq));
@@ -249,6 +253,12 @@ irqfd_ptable_queue_proc(struct file *file, wait_queue_head_t *wqh,
 static void irqfd_update(struct kvm *kvm, struct kvm_kernel_irqfd *irqfd)
 {
 	struct kvm_kernel_irq_routing_entry *e;
+	/*
+	 * #define KVM_IRQCHIP_PIC_MASTER   0
+	 * #define KVM_IRQCHIP_PIC_SLAVE    1
+	 * #define KVM_IRQCHIP_IOAPIC       2
+	 * #define KVM_NR_IRQCHIPS          3
+	 */
 	struct kvm_kernel_irq_routing_entry entries[KVM_NR_IRQCHIPS];
 	int n_entries;
 
@@ -284,6 +294,10 @@ int  __attribute__((weak)) kvm_arch_update_irqfd_routing(
 }
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|581| <<kvm_irqfd>> return kvm_irqfd_assign(kvm, args);
+ */
 static int
 kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -605,6 +619,10 @@ kvm_irqfd_release(struct kvm *kvm)
  * Take note of a change in irq routing.
  * Caller must invoke synchronize_srcu(&kvm->irq_srcu) afterwards.
  */
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|257| <<kvm_set_irq_routing>> kvm_irq_routing_update(kvm);
+ */
 void kvm_irq_routing_update(struct kvm *kvm)
 {
 	struct kvm_kernel_irqfd *irqfd;
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 0bfcbb9a9e62..9a6a442e920b 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -84,9 +84,27 @@ int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi)
  *  = 0   Interrupt was coalesced (previous irq is still pending)
  *  > 0   Number of CPUs interrupt was delivered to
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|250| <<pit_do_work>> kvm_set_irq(kvm, pit->irq_source_id, 0, 1, false);
+ *   - arch/x86/kvm/i8254.c|251| <<pit_do_work>> kvm_set_irq(kvm, pit->irq_source_id, 0, 0, false);
+ *   - arch/x86/kvm/x86.c|4582| <<kvm_vm_ioctl_irq_line>> irq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,
+ *   - virt/kvm/eventfd.c|55| <<irqfd_inject>> kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 1,
+ *   - virt/kvm/eventfd.c|57| <<irqfd_inject>> kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 0,
+ *   - virt/kvm/eventfd.c|60| <<irqfd_inject>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+ *   - virt/kvm/eventfd.c|81| <<irqfd_resampler_ack>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+ *   - virt/kvm/eventfd.c|106| <<irqfd_resampler_shutdown>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+ */
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 		bool line_status)
 {
+	/*
+	 * 对于x86来说 ...
+	 * #define KVM_IRQCHIP_PIC_MASTER   0
+	 * #define KVM_IRQCHIP_PIC_SLAVE    1
+	 * #define KVM_IRQCHIP_IOAPIC       2
+	 * #define KVM_NR_IRQCHIPS          3
+	 */
 	struct kvm_kernel_irq_routing_entry irq_set[KVM_NR_IRQCHIPS];
 	int ret = -1, i, idx;
 
@@ -102,6 +120,9 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 
 	while (i--) {
 		int r;
+		/*
+		 * kvm_set_msi()
+		 */
 		r = irq_set[i].set(&irq_set[i], kvm, irq_source_id, level,
 				   line_status);
 		if (r < 0)
@@ -141,6 +162,10 @@ void kvm_free_irq_routing(struct kvm *kvm)
 	free_irq_routing_table(rt);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|257| <<kvm_set_irq_routing>> r = setup_routing_entry(kvm, new, e, ue);
+ */
 static int setup_routing_entry(struct kvm *kvm,
 			       struct kvm_irq_routing_table *rt,
 			       struct kvm_kernel_irq_routing_entry *e,
@@ -189,6 +214,15 @@ bool __weak kvm_arch_can_set_irq_routing(struct kvm *kvm)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/mpic.c|1650| <<mpic_set_default_irq_routing>> kvm_set_irq_routing(opp->kvm, routing, 0, 0);
+ *   - arch/s390/kvm/kvm-s390.c|1866| <<kvm_arch_vm_ioctl>> r = kvm_set_irq_routing(kvm, &routing, 0, 0);
+ *   - arch/x86/kvm/irq_comm.c|434| <<kvm_setup_default_irq_routing>> return kvm_set_irq_routing(kvm, default_routing,
+ *   - arch/x86/kvm/irq_comm.c|442| <<kvm_setup_empty_irq_routing>> return kvm_set_irq_routing(kvm, empty_routing, 0, 0);
+ *   - virt/kvm/arm/vgic/vgic-irqfd.c|125| <<kvm_vgic_setup_default_irq_routing>> ret = kvm_set_irq_routing(kvm, entries, nr, 0);
+ *   - virt/kvm/kvm_main.c|3481| <<kvm_vm_ioctl(KVM_SET_GSI_ROUTING)>> r = kvm_set_irq_routing(kvm, entries, routing.nr,
+ */
 int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *ue,
 			unsigned nr,
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 2effd647a2cd..5bdccd57e247 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -2455,6 +2455,20 @@ static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
 /*
  * The vCPU has executed a HLT instruction with in-kernel mode enabled.
  */
+/*
+ * called by:
+ *   - arch/arm/kvm/handle_exit.c|83| <<kvm_handle_wfx>> kvm_vcpu_block(vcpu);
+ *   - arch/arm64/kvm/handle_exit.c|110| <<kvm_handle_wfx>> kvm_vcpu_block(vcpu);
+ *   - arch/mips/kvm/emulate.c|978| <<kvm_mips_emul_wait>> kvm_vcpu_block(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr.c|355| <<kvmppc_set_msr_pr>> kvm_vcpu_block(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr_papr.c|382| <<kvmppc_h_pr>> kvm_vcpu_block(vcpu);
+ *   - arch/powerpc/kvm/booke.c|704| <<kvmppc_core_prepare_to_enter>> kvm_vcpu_block(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|245| <<kvmppc_kvm_pv>> kvm_vcpu_block(vcpu);
+ *   - arch/s390/kvm/interrupt.c|1085| <<kvm_s390_handle_wait>> kvm_vcpu_block(vcpu);
+ *   - arch/x86/kvm/x86.c|8028| <<vcpu_block>> kvm_vcpu_block(vcpu);
+ *   - arch/x86/kvm/x86.c|8236| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_block(vcpu);
+ *   - virt/kvm/arm/psci.c|92| <<kvm_psci_vcpu_suspend>> kvm_vcpu_block(vcpu);
+ */
 void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	ktime_t start, cur;
-- 
2.17.1

