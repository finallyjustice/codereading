From 13d814d495a17701b03953db932bdc4bd6ec5f43 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Wed, 4 Mar 2020 01:23:38 -0800
Subject: [PATCH 1/1] linux block for linux-5.5

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/badblocks.c                |   73 ++
 block/blk-core.c                 |   16 +
 block/blk-flush.c                |  433 ++++++++++++
 block/blk-lib.c                  |    5 +
 block/blk-mq-cpumap.c            |   28 +
 block/blk-mq-pci.c               |   32 +
 block/blk-mq-rdma.c              |    5 +
 block/blk-mq-sched.c             |   29 +
 block/blk-mq-sysfs.c             |   19 +
 block/blk-mq-tag.c               |   33 +
 block/blk-mq-tag.h               |   53 ++
 block/blk-mq-virtio.c            |   31 +
 block/blk-mq.c                   |  591 ++++++++++++++++
 block/blk-mq.h                   |   87 +++
 block/blk-settings.c             |   41 ++
 block/blk-softirq.c              |   39 ++
 block/blk-stat.c                 |  275 ++++++++
 block/blk-stat.h                 |   27 +
 block/blk-sysfs.c                |   19 +
 block/blk-zoned.c                |  166 +++++
 block/blk.h                      |   67 ++
 block/genhd.c                    |  113 ++++
 block/partition-generic.c        |   98 +++
 block/partitions/check.c         |   99 +++
 drivers/block/loop.c             |  100 +++
 drivers/block/loop.h             |   15 +
 drivers/block/null_blk.h         |  101 +++
 drivers/block/null_blk_main.c    | 1073 ++++++++++++++++++++++++++++++
 drivers/block/null_blk_zoned.c   |  101 +++
 drivers/block/virtio_blk.c       |   25 +
 drivers/nvme/host/core.c         |  100 +++
 drivers/nvme/host/fault_inject.c |   35 +
 drivers/nvme/host/multipath.c    |   17 +
 drivers/nvme/host/nvme.h         |   22 +
 drivers/nvme/host/pci.c          |  113 ++++
 drivers/nvme/target/loop.c       |    3 +
 drivers/pci/msi.c                |   14 +
 drivers/virtio/virtio_ring.c     |   24 +
 fs/block_dev.c                   |    9 +
 fs/direct-io.c                   |   15 +
 fs/ext4/file.c                   |    5 +
 fs/iomap/direct-io.c             |   28 +
 include/linux/badblocks.h        |   61 ++
 include/linux/bio.h              |    6 +
 include/linux/blk-mq.h           |  166 +++++
 include/linux/blk_types.h        |   34 +
 include/linux/blkdev.h           |  160 +++++
 include/linux/fs.h               |   32 +
 include/linux/genhd.h            |   42 ++
 include/linux/nvme.h             |   13 +
 include/linux/virtio_config.h    |    7 +
 include/scsi/scsi_device.h       |   18 +
 include/scsi/scsi_host.h         |    6 +
 include/uapi/linux/fs.h          |   11 +
 kernel/irq/msi.c                 |   22 +
 kernel/trace/blktrace.c          |   13 +
 lib/fault-inject.c               |   12 +
 57 files changed, 4782 insertions(+)

diff --git a/block/badblocks.c b/block/badblocks.c
index 2e5f5697db35..0696c5925b32 100644
--- a/block/badblocks.c
+++ b/block/badblocks.c
@@ -16,6 +16,10 @@
 #include <linux/types.h>
 #include <linux/slab.h>
 
+/*
+ * 一个u64表示一个最多512-sector的范围
+ */
+
 /**
  * badblocks_check() - check a given range for bad sectors
  * @bb:		the badblocks structure that holds all badblock information
@@ -50,6 +54,20 @@
  * -1: there are bad blocks which have not yet been acknowledged in metadata.
  * plus the start/length of the first bad section we overlap.
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1999| <<null_handle_badblocks>> if (badblocks_check(bb, sector, nr_sectors, &first_bad, &bad_sectors))
+ *   - drivers/md/md.h|214| <<is_badblock>> int rv = badblocks_check(&rdev->badblocks, rdev->data_offset + s,
+ *   - drivers/nvdimm/nd.h|424| <<is_bad_pmem>> return !!badblocks_check(bb, sector, len / 512, &first_bad,
+ *   - drivers/nvdimm/pfn_devs.c|390| <<nd_pfn_clear_memmap_errors>> bb_present = badblocks_check(&nd_region->bb, meta_start,
+ *
+ * badblocks_check() - check a given range for bad sectors
+ * @bb:         the badblocks structure that holds all badblock information
+ * @s:          sector (start) at which to check for badblocks
+ * @sectors:    number of sectors to check for badblocks
+ * @first_bad:  pointer to store location of the first badblock
+ * @bad_sectors: pointer to store number of badblocks after @first_bad
+ */
 int badblocks_check(struct badblocks *bb, sector_t s, int sectors,
 			sector_t *first_bad, int *bad_sectors)
 {
@@ -160,6 +178,14 @@ static void badblocks_update_acked(struct badblocks *bb)
  *  0: success
  *  1: failed to set badblocks (out of space)
  */
+/*
+ * called by:
+ *   - block/badblocks.c|565| <<badblocks_store>> if (badblocks_set(bb, sector, length, !unack))
+ *   - drivers/block/null_blk_main.c|685| <<nullb_device_badblocks_store>> ret = badblocks_set(&t_dev->badblocks, start,
+ *   - drivers/md/md.c|1665| <<super_1_load>> if (badblocks_set(&rdev->badblocks, sector, count, 1))
+ *   - drivers/md/md.c|9243| <<rdev_set_badblocks>> rv = badblocks_set(&rdev->badblocks, s, sectors, 0);
+ *   - drivers/nvdimm/badrange.c|170| <<set_badblock>> if (badblocks_set(bb, s, num, 1)
+ */
 int badblocks_set(struct badblocks *bb, sector_t s, int sectors,
 			int acknowledged)
 {
@@ -291,10 +317,16 @@ int badblocks_set(struct badblocks *bb, sector_t s, int sectors,
 		} else {
 			int this_sectors = sectors;
 
+			/*
+			 * 从p+hi拷贝到p+hi+1, 相当于集体右移1格
+			 */
 			memmove(p + hi + 1, p + hi,
 				(bb->count - hi) * 8);
 			bb->count++;
 
+			/*
+			 * 一个u64最多只能表示BB_MAX_LEN=512个sector
+			 */
 			if (this_sectors > BB_MAX_LEN)
 				this_sectors = BB_MAX_LEN;
 			p[hi] = BB_MAKE(s, this_sectors, acknowledged);
@@ -464,6 +496,14 @@ EXPORT_SYMBOL_GPL(ack_all_badblocks);
  * Return:
  *  Length of returned data
  */
+/*
+ * called by:
+ *   - block/genhd.c|928| <<disk_badblocks_show>> return badblocks_show(disk->bb, page, 0);
+ *   - drivers/block/null_blk_main.c|636| <<nullb_device_badblocks_show>> return badblocks_show(&t_dev->badblocks, page, 0);
+ *   - drivers/md/md.c|3372| <<bb_show>> return badblocks_show(&rdev->badblocks, page, 0);
+ *   - drivers/md/md.c|3387| <<ubb_show>> return badblocks_show(&rdev->badblocks, page, 1);
+ *   - drivers/nvdimm/region_devs.c|540| <<region_badblocks_show>> rc = badblocks_show(&nd_region->bb, buf, 0);
+ */
 ssize_t badblocks_show(struct badblocks *bb, char *page, int unack)
 {
 	size_t len;
@@ -481,6 +521,9 @@ ssize_t badblocks_show(struct badblocks *bb, char *page, int unack)
 	i = 0;
 
 	while (len < PAGE_SIZE && i < bb->count) {
+		/*
+		 * 这里p[i]是一个u64
+		 */
 		sector_t s = BB_OFFSET(p[i]);
 		unsigned int length = BB_LEN(p[i]);
 		int ack = BB_ACK(p[i]);
@@ -490,6 +533,14 @@ ssize_t badblocks_show(struct badblocks *bb, char *page, int unack)
 		if (unack && ack)
 			continue;
 
+		/*
+		 * The return value is the number of characters which would be
+		 * generated for the given input, excluding the trailing null,
+		 * as per ISO C99.  If the return is greater than or equal to
+		 * @size, the resulting string is truncated.
+		 *
+		 * 输入的最后有一个'\n'
+		 */
 		len += snprintf(page+len, PAGE_SIZE-len, "%llu %u\n",
 				(unsigned long long)s << bb->shift,
 				length << bb->shift);
@@ -514,6 +565,12 @@ EXPORT_SYMBOL_GPL(badblocks_show);
  * Return:
  *  Length of the buffer processed or -ve error.
  */
+/*
+ * called by:
+ *   - block/genhd.c|940| <<disk_badblocks_store>> return badblocks_store(disk->bb, page, len, 0);
+ *   - drivers/md/md.c|3376| <<bb_store>> int rv = badblocks_store(&rdev->badblocks, page, len, 0);
+ *   - drivers/md/md.c|3391| <<ubb_store>> return badblocks_store(&rdev->badblocks, page, len, 1);
+ */
 ssize_t badblocks_store(struct badblocks *bb, const char *page, size_t len,
 			int unack)
 {
@@ -541,6 +598,11 @@ ssize_t badblocks_store(struct badblocks *bb, const char *page, size_t len,
 }
 EXPORT_SYMBOL_GPL(badblocks_store);
 
+/*
+ * called by:
+ *   - block/badblocks.c|577| <<badblocks_init>> return __badblocks_init(NULL, bb, enable);
+ *   - block/badblocks.c|585| <<devm_init_badblocks>> return __badblocks_init(dev, bb, 1);
+ */
 static int __badblocks_init(struct device *dev, struct badblocks *bb,
 		int enable)
 {
@@ -550,6 +612,12 @@ static int __badblocks_init(struct device *dev, struct badblocks *bb,
 		bb->shift = 0;
 	else
 		bb->shift = -1;
+	/*
+	 * devm_kzalloc():
+	 * Managed kmalloc.  Memory allocated with this function is
+	 * automatically freed on driver detach.  Like all other devres
+	 * resources, guaranteed alignment is unsigned long long.
+	 */
 	if (dev)
 		bb->page = devm_kzalloc(dev, PAGE_SIZE, GFP_KERNEL);
 	else
@@ -572,6 +640,11 @@ static int __badblocks_init(struct device *dev, struct badblocks *bb,
  *  0: success
  *  -ve errno: on error
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|865| <<null_alloc_dev>> if (badblocks_init(&dev->badblocks, 0)) {
+ *   - drivers/md/md.c|3557| <<md_rdev_init>> return badblocks_init(&rdev->badblocks, 0);
+ */
 int badblocks_init(struct badblocks *bb, int enable)
 {
 	return __badblocks_init(NULL, bb, enable);
diff --git a/block/blk-core.c b/block/blk-core.c
index 089e890ab208..5c1d5ad64462 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -476,6 +476,18 @@ static void blk_timeout_work(struct work_struct *work)
  * @gfp_mask: memory allocation flags
  * @node_id: NUMA node to allocate memory from
  */
+/*
+ * called by:
+ *   - block/blk-core.c|394| <<blk_alloc_queue>> return blk_alloc_queue_node(gfp_mask, NUMA_NO_NODE);
+ *   - block/blk-mq.c|2663| <<blk_mq_init_queue>> uninit_q = blk_alloc_queue_node(GFP_KERNEL, set->numa_node);
+ *   - drivers/block/drbd/drbd_main.c|2804| <<drbd_create_device>> q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
+ *   - drivers/block/null_blk_main.c|1724| <<null_add_dev>> nullb->q = blk_alloc_queue_node(GFP_KERNEL, dev->home_node);
+ *   - drivers/block/umem.c|888| <<mm_pci_probe>> card->queue = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
+ *   - drivers/lightnvm/core.c|383| <<nvm_create_tgt>> tqueue = blk_alloc_queue_node(GFP_KERNEL, dev->q->node);
+ *   - drivers/md/dm.c|1948| <<alloc_dev>> md->queue = blk_alloc_queue_node(GFP_KERNEL, numa_node_id);
+ *   - drivers/nvdimm/pmem.c|404| <<pmem_attach_disk>> q = blk_alloc_queue_node(GFP_KERNEL, dev_to_node(dev));
+ *   - drivers/nvme/host/multipath.c|380| <<nvme_mpath_alloc_disk>> q = blk_alloc_queue_node(GFP_KERNEL, ctrl->numa_node);
+ */
 struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 {
 	struct request_queue *q;
@@ -1390,6 +1402,10 @@ void blk_account_io_start(struct request *rq, bool new_io)
  * Steal bios from a request and add them to a bio list.
  * The request must not have been partially completed before.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|78| <<nvme_failover_req>> blk_steal_bios(&ns->head->requeue_list, req);
+ */
 void blk_steal_bios(struct bio_list *list, struct request *rq)
 {
 	if (rq->bio) {
diff --git a/block/blk-flush.c b/block/blk-flush.c
index 3f977c517960..9aa810fcab67 100644
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@ -76,13 +76,81 @@
 #include "blk-mq-tag.h"
 #include "blk-mq-sched.h"
 
+/*
+ * http://www.unjeep.com/article/40696.html
+ *
+ * 硬盘在控制器上的一块内存芯片,其类型一般以SDRAM为主,具有极快的存取速度,
+ * 它是硬盘内部存储和外界接口之间的缓冲器.由于硬盘的内部数据传输速度和外界
+ * 介面传输速度不同,缓存在其中起到一个缓冲的作用.缓存的大小与速度是直接关
+ * 系到硬盘的传输速度的重要因素,能够大幅度地提高硬盘整体性能.
+ *
+ * 如果硬盘的cache启用了,那么很有可能写入的数据是写到了硬盘的cache中,而没
+ * 有真正写到磁盘介质上.
+ *
+ * 在linux下,查看磁盘cache是否开启可通过hdparm命令:
+ *
+ * #hdparm -W /dev/sdx    //是否开启cache，1为enable
+ * #hdparm -W 0 /dev/sdx  //关闭cache
+ * #hdparm -W 1 /dev/sdx  //enable cache
+ *
+ * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+ * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+ *
+ * blk_insert_flush()是非常重要的入口!
+ *
+ *
+ * 部分调用blk_queue_write_cache()的例子:
+ *   - drivers/block/loop.c|1007| <<loop_set_fd>> blk_queue_write_cache(lo->lo_queue, true, false);
+ *   - drivers/block/nbd.c|1136| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, true);
+ *   - drivers/block/nbd.c|1138| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, false);
+ *   - drivers/block/nbd.c|1141| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, false, false);
+ *   - drivers/block/null_blk_main.c|1742| <<null_add_dev>> blk_queue_write_cache(nullb->q, true, true);
+ *   - rivers/block/virtio_blk.c|609| <<virtblk_update_cache_mode>> blk_queue_write_cache(vblk->disk->queue, writeback, false);
+ *   - drivers/block/xen-blkfront.c|1014| <<xlvbd_flush>> blk_queue_write_cache(info->rq, info->feature_flush ? true : false,
+ *   - drivers/ide/ide-disk.c|554| <<update_flush>> blk_queue_write_cache(drive->queue, wc, false);
+ *   - drivers/md/dm-table.c|1910| <<dm_table_set_restrictions>> blk_queue_write_cache(q, wc, fua);
+ *   - drivers/md/md.c|5506| <<md_alloc>> blk_queue_write_cache(mddev->queue, true, true);
+ *   - drivers/nvme/host/core.c|2204| <<nvme_set_queue_limits>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/nvme/host/multipath.c|393| <<nvme_mpath_alloc_disk>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/scsi/sd.c|152| <<sd_set_flush_flag>> blk_queue_write_cache(sdkp->disk->queue, wc, fua);
+ */
+
+/*
+ * 冲刷的过程中request_queue为什么要使用双缓冲队列来存放fs_request?
+ *
+ * 双缓冲队列可以做到只执行一次冲刷请求就可以完成多个fs_request的冲刷要求.队列自
+ * 带的冲刷request在执行的过程中,blk_insert_flush()可以被调用多次,来自上层的
+ * fs_request被添加到pending1队列,等待冲刷request的下一次执行,当冲刷requst可以再
+ * 次被执行时,pending1队列不再接收新的fs_request(fs_request被加入到pending2队列),
+ * 冲刷request执行完毕后,pending1队列所有的fs_request的PREFLUSH/POSTFLUSH执行完毕.
+ * The actual execution of flush is double buffered.  Whenever a request
+ * needs to execute PRE or POSTFLUSH, it queues at
+ * fq->flush_queue[fq->flush_pending_idx].  Once certain criteria are met, a
+ * REQ_OP_FLUSH is issued and the pending_idx is toggled.  When the flush
+ * completes, all the requests which were pending are proceeded to the next
+ * step.  This allows arbitrary merging of different types of PREFLUSH/FUA
+ * requests.
+ */
+
 /* PREFLUSH/FUA sequences */
 enum {
+	/*
+	 * 在以下使用REQ_FSEQ_PREFLUSH:
+	 *   - block/blk-flush.c|117| <<global>> REQ_FSEQ_ACTIONS = REQ_FSEQ_PREFLUSH | REQ_FSEQ_DATA |
+	 *   - block/blk-flush.c|143| <<blk_flush_policy>> policy |= REQ_FSEQ_PREFLUSH;
+	 *   - block/blk-flush.c|244| <<blk_flush_complete_seq>> case REQ_FSEQ_PREFLUSH:
+	 *   - block/blk-flush.c|323| <<flush_end_io>> BUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);
+	 *   - block/blk-flush.c|486| <<blk_insert_flush>> !(policy & (REQ_FSEQ_PREFLUSH | REQ_FSEQ_POSTFLUSH))) {
+	 */
 	REQ_FSEQ_PREFLUSH	= (1 << 0), /* pre-flushing in progress */
 	REQ_FSEQ_DATA		= (1 << 1), /* data write in progress */
 	REQ_FSEQ_POSTFLUSH	= (1 << 2), /* post-flushing in progress */
 	REQ_FSEQ_DONE		= (1 << 3),
 
+	/*
+	 * 在以下使用REQ_FSEQ_ACTIONS:
+	 *   - block/blk-flush.c|511| <<blk_insert_flush>> blk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);
+	 */
 	REQ_FSEQ_ACTIONS	= REQ_FSEQ_PREFLUSH | REQ_FSEQ_DATA |
 				  REQ_FSEQ_POSTFLUSH,
 
@@ -96,16 +164,52 @@ enum {
 static void blk_kick_flush(struct request_queue *q,
 			   struct blk_flush_queue *fq, unsigned int flags);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|377| <<blk_insert_flush>> unsigned int policy = blk_flush_policy(fflags, rq);
+ *
+ * 根据情况把REQ_FSEQ_DATA,REQ_FSEQ_PREFLUSH和REQ_FSEQ_POSTFLUSH返回到返回值中
+ * 这些可以表明flush需要做的步骤(包含3步)
+ *  - REQ_FSEQ_PREFLUSH(在数据请求以前冲刷磁盘缓存)
+ *  - REQ_FSEQ_DATA(写入数据请求)
+ *  - REQ_FSEQ_POSTFLUSH(在数据请求之后冲刷磁盘缓存)
+ * 如果磁盘不支持FUA,那么可以使用REQ_FSEQ_POSTFLUSH代替
+ * 假定我们分析的场景中,磁盘不支持FUA,则最终我们的冲刷策略为3步都做(policy=111).
+ */
 static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
 {
+	/*
+	 * 因为blk_flush_policy()只被blk_insert_flush()调用
+	 * 所以参数的fflags来自q->queue_flags
+	 */
 	unsigned int policy = 0;
 
 	if (blk_rq_sectors(rq))
 		policy |= REQ_FSEQ_DATA;
 
+	/*
+	 * 在以下设置QUEUE_FLAG_WC的几个例子:
+	 *   - block/blk-settings.c|824| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+	 *   - block/blk-settings.c|826| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_WC, q)
+	 *   - block/blk-sysfs.c|530| <<queue_wc_store>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+	 *   - drivers/md/dm-table.c|1905| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_WC))) {
+	 */
 	if (fflags & (1UL << QUEUE_FLAG_WC)) {
 		if (rq->cmd_flags & REQ_PREFLUSH)
 			policy |= REQ_FSEQ_PREFLUSH;
+		/*
+		 * 使用QUEUE_FLAG_FUA的地方:
+		 *   - block/blk-flush.c|158| <<blk_flush_policy>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&
+		 *   - block/blk-flush.c|471| <<blk_insert_flush>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)))
+		 *   - block/blk-settings.c|828| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_FUA, q);
+		 *   - block/blk-settings.c|830| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_FUA, q);
+		 *   - block/blk-sysfs.c|539| <<queue_fua_show>> return sprintf(page, "%u\n", test_bit(QUEUE_FLAG_FUA, &q->queue_flags));
+		 *   - drivers/md/dm-table.c|1907| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_FUA)))
+		 *   - drivers/target/target_core_iblock.c|703| <<iblock_execute_rw>> if (test_bit(QUEUE_FLAG_FUA, &q->queue_flags)) {
+		 *   - include/linux/blkdev.h|733| <<blk_queue_fua>> #define blk_queue_fua(q) test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
+		 *
+		 * 如果磁盘不支持FUA,那么可以使用REQ_FSEQ_POSTFLUSH代替
+		 */
 		if (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&
 		    (rq->cmd_flags & REQ_FUA))
 			policy |= REQ_FSEQ_POSTFLUSH;
@@ -113,11 +217,27 @@ static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
 	return policy;
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|184| <<blk_flush_complete_seq>> seq = blk_flush_cur_seq(rq);
+ *   - block/blk-flush.c|262| <<flush_end_io>> unsigned int seq = blk_flush_cur_seq(rq);
+ *
+ * 获得rq->flush.seq中第一个为0的bit (因为rq->flush.seq表示的是可以跳过的)
+ * struct {
+ *     unsigned int            seq;
+ *     struct list_head        list;
+ *     rq_end_io_fn            *saved_end_io;
+ * } flush;
+ */
 static unsigned int blk_flush_cur_seq(struct request *rq)
 {
 	return 1 << ffz(rq->flush.seq);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|211| <<blk_flush_complete_seq>> blk_flush_restore_request(rq);
+ */
 static void blk_flush_restore_request(struct request *rq)
 {
 	/*
@@ -132,11 +252,20 @@ static void blk_flush_restore_request(struct request *rq)
 	rq->end_io = rq->flush.saved_end_io;
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|199| <<blk_flush_complete_seq>> blk_flush_queue_rq(rq, true);
+ *   - block/blk-flush.c|341| <<blk_kick_flush>> blk_flush_queue_rq(flush_rq, false);
+ */
 static void blk_flush_queue_rq(struct request *rq, bool add_front)
 {
 	blk_mq_add_to_requeue_list(rq, add_front, true);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|231| <<flush_end_io>> blk_account_io_flush(flush_rq);
+ */
 static void blk_account_io_flush(struct request *rq)
 {
 	struct hd_struct *part = &rq->rq_disk->part0;
@@ -164,11 +293,65 @@ static void blk_account_io_flush(struct request *rq)
  * RETURNS:
  * %true if requests were added to the dispatch queue, %false otherwise.
  */
+/*
+ * [0] blk_flush_complete_seq
+ * [0] blk_insert_flush
+ * [0] blk_mq_make_request
+ * [0] generic_make_request
+ * [0] submit_bio
+ * [0] submit_bh_wbc.isra.55
+ * [0] jbd2_write_superblock
+ * [0] jbd2_mark_journal_empty
+ * [0] jbd2_journal_flush
+ * [0] ext4_mark_recovery_complete.isra.224
+ * [0] ext4_fill_super
+ * [0] mount_bdev
+ * [0] legacy_get_tree
+ * [0] vfs_get_tree
+ * [0] do_mount
+ * [0] do_mount_root
+ * [0] mount_block_root
+ * [0] mount_root
+ * [0] prepare_namespace
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] blk_flush_complete_seq
+ * [0] mq_flush_data_end_io
+ * [0] blk_mq_complete_request
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * called by:
+ *   - block/blk-flush.c|265| <<flush_end_io>> blk_flush_complete_seq(rq, fq, seq, error);
+ *   - block/blk-flush.c|366| <<mq_flush_data_end_io>> blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error);
+ *   - block/blk-flush.c|444| <<blk_insert_flush>> blk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);
+ */
 static void blk_flush_complete_seq(struct request *rq,
 				   struct blk_flush_queue *fq,
 				   unsigned int seq, blk_status_t error)
 {
 	struct request_queue *q = rq->q;
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下使用flush_pending_idx:
+	 *   - block/blk-flush.c|276| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|392| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 * 默认初始化是1
+	 *
+	 * 在这个函数里REQ_FSEQ_DATA和REQ_FSEQ_DONE都不用pending (&fq->flush_queue[fq->flush_pending_idx])
+	 * 只有REQ_FSEQ_PREFLUSH和REQ_FSEQ_POSTFLUSH才用
+	 */
 	struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
 	unsigned int cmd_flags;
 
@@ -176,22 +359,52 @@ static void blk_flush_complete_seq(struct request *rq,
 	rq->flush.seq |= seq;
 	cmd_flags = rq->cmd_flags;
 
+	/*
+	 * 第一次从blk_insert_flush()进来, error是0
+	 *
+	 * blk_flush_cur_seq():
+	 * 获得rq->flush.seq中第一个为0的bit (因为rq->flush.seq表示的是可以跳过的)
+	 * struct {
+	 *     unsigned int            seq;
+	 *     struct list_head        list;
+	 *     rq_end_io_fn            *saved_end_io;
+	 * } flush;
+	 *
+	 * 假设三步都要执行,第一次到这里就是REQ_FSEQ_PREFLUSH
+	 */
 	if (likely(!error))
 		seq = blk_flush_cur_seq(rq);
 	else
 		seq = REQ_FSEQ_DONE;
 
+	/*
+	 * 假设三步都要执行,
+	 * 第一次从blk_insert_flush()到这里就是REQ_FSEQ_PREFLUSH
+	 */
 	switch (seq) {
 	case REQ_FSEQ_PREFLUSH:
 	case REQ_FSEQ_POSTFLUSH:
 		/* queue for flush */
+		/*
+		 * pending来自上面的&fq->flush_queue[fq->flush_pending_idx];
+		 */
 		if (list_empty(pending))
 			fq->flush_pending_since = jiffies;
+		/*
+		 * 插入的时候插入的是tail!!!!!
+		 */
 		list_move_tail(&rq->flush.list, pending);
 		break;
 
 	case REQ_FSEQ_DATA:
 		list_move_tail(&rq->flush.list, &fq->flush_data_in_flight);
+		/*
+		 * called by:
+		 *   - block/blk-flush.c|199| <<blk_flush_complete_seq>> blk_flush_queue_rq(rq, true);
+		 *   - block/blk-flush.c|341| <<blk_kick_flush>> blk_flush_queue_rq(flush_rq, false);
+		 *
+		 * 下面下发end的时候应该调用mq_flush_data_end_io()
+		 */
 		blk_flush_queue_rq(rq, true);
 		break;
 
@@ -215,6 +428,10 @@ static void blk_flush_complete_seq(struct request *rq,
 	blk_kick_flush(q, fq, cmd_flags);
 }
 
+/*
+ * 在以下使用flush_end_io():
+ *   - block/blk-flush.c|339| <<blk_kick_flush>> flush_rq->end_io = flush_end_io;
+ */
 static void flush_end_io(struct request *flush_rq, blk_status_t error)
 {
 	struct request_queue *q = flush_rq->q;
@@ -251,10 +468,40 @@ static void flush_end_io(struct request *flush_rq, blk_status_t error)
 	BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
 
 	/* account completion of the flush request */
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下修改flush_running_idx: 
+	 *   - block/blk-flush.c|362| <<flush_end_io>> fq->flush_running_idx ^= 1;
+	 */
 	fq->flush_running_idx ^= 1;
 
 	/* and push the waiting requests to the next stage */
 	list_for_each_entry_safe(rq, n, running, flush.list) {
+		/*
+		 * 这里的代码写的太隐晦了
+		 *
+		 * 假设是PREFLUSH执行完了到的这里, seq没清空过
+		 * 所以这里获得rq->flush.seq中第一个为0的bit 
+		 * 因为rq->flush.seq表示的是可以跳过的)肯定还是PREFLUSH
+		 *
+		 * 所以seq作为参数传入下面的blk_flush_complete_seq()的时候
+		 * 是告诉后面PREFLUSH要被跳过了!!!
+		 */
+		/*
+		 * 但是这里又不明白为什么是list_for_each_entry_safe()遍历所有的request.
+		 * 根据代码阅读发现这些request实际会被blk_flush_complete_seq()忽略preflush.
+		 * 相当于下发了多个preflush只执行了一次.
+		 * 根据下面的资料可以解释 (http://www.unjeep.com/article/40696.html):
+		 *
+		 * 冲刷的过程中request_queue为什么要使用双缓冲队列来存放fs_request.?
+                 * A:双缓冲队列可以做到只执行一次冲刷请求就可以完成多个fs_request的冲刷要求.
+		 * 队列自带的冲刷request在执行的过程中,blk_insert_flush()可以被调用多次,来自
+		 * 上层的fs_request被添加到pending1队列,等待冲刷request的下一次执行,当冲刷
+		 * requst可以再次被执行时,pending1队列不再接收新的fs_request(fs_request被加入
+		 * 到pending2队列),冲刷request执行完毕后,pending1队列所有的fs_request的
+		 * PREFLUSH/POSTFLUSH执行完毕.
+		 */
 		unsigned int seq = blk_flush_cur_seq(rq);
 
 		BUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);
@@ -278,15 +525,60 @@ static void flush_end_io(struct request *flush_rq, blk_status_t error)
  * spin_lock_irq(fq->mq_flush_lock)
  *
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|415| <<blk_flush_complete_seq>> blk_kick_flush(q, fq, cmd_flags);
+ */
 static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 			   unsigned int flags)
 {
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下使用flush_pending_idx:
+	 *   - block/blk-flush.c|276| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|392| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 *
+	 * 
+	 * 使用flush_queue[2]的地方:
+	 *   - block/blk-flush.c|352| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|454| <<flush_end_io>> running = &fq->flush_queue[fq->flush_running_idx];
+	 *   - block/blk-flush.c|501| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|849| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[0]);
+	 *   - block/blk-flush.c|850| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[1]);
+	 */
 	struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	/*
+	 * 插入的是tail, 用的是head
+	 * 注意, 这里只是用, 没有取出来!!!
+	 */
 	struct request *first_rq =
 		list_first_entry(pending, struct request, flush.list);
+	/*
+	 * 使用flush_rq的地方:
+	 *   - block/blk-flush.c|504| <<blk_kick_flush>> struct request *flush_rq = fq->flush_rq;
+	 *   - block/blk-flush.c|845| <<blk_alloc_flush_queue>> fq->flush_rq = kzalloc_node(rq_sz, flags, node);
+	 *   - block/blk-flush.c|846| <<blk_alloc_flush_queue>> if (!fq->flush_rq)
+	 *   - block/blk-flush.c|875| <<blk_free_flush_queue>> kfree(fq->flush_rq);
+	 *   - block/blk-mq.c|2303| <<blk_mq_exit_hctx>> set->ops->exit_request(set, hctx->fq->flush_rq, hctx_idx);
+	 *   - block/blk-mq.c|2361| <<blk_mq_init_hctx>> if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx,
+	 *   - block/blk.h|81| <<is_flush_rq>> return hctx->fq->flush_rq == req;
+	 */
 	struct request *flush_rq = fq->flush_rq;
 
 	/* C1 described at the top of this file */
+	/*
+	 * C1. At any given time, only one flush shall be in progress.  This makes
+	 *     double buffering sufficient.
+	 *
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 *
+	 * 在以下修改flush_running_idx:
+	 *   - block/blk-flush.c|362| <<flush_end_io>> fq->flush_running_idx ^= 1;
+	 */
 	if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
 		return;
 
@@ -296,6 +588,16 @@ static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 	 * assigned to empty flushes, and we deadlock if we are expecting
 	 * other requests to make progress. Don't defer for that case.
 	 */
+	/*
+	 * C2. Flush is deferred if any request is executing DATA of its sequence.
+	 *     This avoids issuing separate POSTFLUSHes for requests which shared
+	 *     PREFLUSH.
+	 *
+	 * C3. The second condition is ignored if there is a request which has
+	 *     waited longer than FLUSH_PENDING_TIMEOUT.  This is to avoid
+	 *     starvation in the unlikely case where there are continuous stream of
+	 *     FUA (without PREFLUSH) requests.
+	 */
 	if (!list_empty(&fq->flush_data_in_flight) && q->elevator &&
 	    time_before(jiffies,
 			fq->flush_pending_since + FLUSH_PENDING_TIMEOUT))
@@ -337,6 +639,31 @@ static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 	blk_flush_queue_rq(flush_rq, false);
 }
 
+/*
+ * [0] mq_flush_data_end_io
+ * [0] blk_mq_complete_request
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * [0] mq_flush_data_end_io
+ * [0] blk_mq_complete_request
+ * [0] nvme_irq
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * 在以下使用mq_flush_data_end_io():
+ *   - block/blk-flush.c|437| <<blk_insert_flush>> rq->end_io = mq_flush_data_end_io;
+ */
 static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
 {
 	struct request_queue *q = rq->q;
@@ -370,13 +697,54 @@ static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
  * @rq is being submitted.  Analyze what needs to be done and put it on the
  * right queue.
  */
+/*
+ * [0] blk_insert_flush
+ * [0] blk_mq_make_request
+ * [0] generic_make_request
+ * [0] submit_bio
+ * [0] submit_bh_wbc.isra
+ * [0] __sync_dirty_buffer
+ * [0] ext4_commit_super
+ * [0] ext4_mark_recovery_complete.isra
+ * [0] ext4_fill_super
+ * [0] mount_bdev
+ * [0] legacy_get_tree
+ * [0] vfs_get_tree
+ * [0] do_mount
+ * [0] do_mount_root
+ * [0] mount_block_root
+ * [0] mount_root
+ * [0] prepare_namespace
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - block/blk-mq-sched.c|388| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+ *   - block/blk-mq.c|2011| <<blk_mq_make_request>> blk_insert_flush(rq);
+ */
 void blk_insert_flush(struct request *rq)
 {
 	struct request_queue *q = rq->q;
 	unsigned long fflags = q->queue_flags;	/* may change, cache */
+	/*
+	 * 根据情况把REQ_FSEQ_DATA,REQ_FSEQ_PREFLUSH和REQ_FSEQ_POSTFLUSH返回到返回值中
+	 * 这些可以表明flush需要做的步骤(包含3步) ---> 也就是说,policy的3个bit表明要做哪些步骤!!!!
+	 *  - REQ_FSEQ_PREFLUSH(在数据请求以前冲刷磁盘缓存)
+	 *  - REQ_FSEQ_DATA(写入数据请求)
+	 *  - REQ_FSEQ_POSTFLUSH(在数据请求之后冲刷磁盘缓存)
+	 * 如果磁盘不支持FUA,那么可以使用REQ_FSEQ_POSTFLUSH代替
+	 * 假定我们分析的场景中,磁盘不支持FUA,则最终我们的冲刷策略为3步都做(policy=111).
+	 */
 	unsigned int policy = blk_flush_policy(fflags, rq);
+	/* 根据rq获取对应的hctx的blk_mq_hw_ctx->fq */
 	struct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);
 
+	/*
+	 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+	 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+	 */
+
 	/*
 	 * @policy now records what operations need to be done.  Adjust
 	 * REQ_PREFLUSH and FUA for the driver.
@@ -398,6 +766,10 @@ void blk_insert_flush(struct request *rq)
 	 * advertise a write-back cache.  In this case, simply
 	 * complete the request.
 	 */
+	/*
+	 * 正如上面注释,policy的3个bit表明要做哪些步骤!!!!
+	 * 没设置任何bit说明什么也不做
+	 */
 	if (!policy) {
 		blk_mq_end_request(rq, 0);
 		return;
@@ -422,12 +794,40 @@ void blk_insert_flush(struct request *rq)
 	 */
 	memset(&rq->flush, 0, sizeof(rq->flush));
 	INIT_LIST_HEAD(&rq->flush.list);
+	/*
+	 * 在以下使用RQF_FLUSH_SEQ:
+	 *   - block/blk-core.c|244| <<req_bio_endio>> if (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))
+	 *   - block/blk-core.c|1347| <<blk_account_io_done>> !(req->rq_flags & RQF_FLUSH_SEQ)) {
+	 *   - block/blk-flush.c|205| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|426| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|528| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+	 *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> if (rq->rq_flags & RQF_FLUSH_SEQ) {
+	 *   - block/blk-mq-sched.c|387| <<blk_mq_sched_insert_request>> if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
+	 *   - include/linux/blkdev.h|120| <<RQF_NOMERGE_FLAGS>> (RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
+	 */
 	rq->rq_flags |= RQF_FLUSH_SEQ;
+	/*
+	 * struct {
+	 *     unsigned int            seq;
+	 *     struct list_head        list;
+	 *     rq_end_io_fn            *saved_end_io;
+	 * } flush;
+	 *
+	 * 使用saved_end_io的地方:
+	 *   - block/blk-flush.c|221| <<blk_flush_restore_request>> rq->end_io = rq->flush.saved_end_io;
+	 *   - block/blk-flush.c|615| <<blk_insert_flush>> rq->flush.saved_end_io = rq->end_io;
+	 */
 	rq->flush.saved_end_io = rq->end_io; /* Usually NULL */
 
 	rq->end_io = mq_flush_data_end_io;
 
 	spin_lock_irq(&fq->mq_flush_lock);
+	/*
+	 * 参数REQ_FSEQ_ACTIONS & ~policy表示可以跳过的步骤!!!
+	 * blk_insert_flush()初始化request的完成函数为flush_data_end_io()后,
+	 * 调用REQ_FSEQ_ACTIONS & ~policy把可以跳过的步骤对应的位置填1,
+	 * 调用blk_flush_complete_seq()开始冲刷过程.
+	 */
 	blk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);
 	spin_unlock_irq(&fq->mq_flush_lock);
 }
@@ -443,6 +843,31 @@ void blk_insert_flush(struct request *rq)
  *    room for storing the error offset in case of a flush error, if they
  *    wish to.
  */
+/*
+ * called by:
+ *   - drivers/md/dm-integrity.c|2551| <<bitmap_flush_work>> blkdev_issue_flush(ic->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|663| <<dmz_write_sb>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|705| <<dmz_write_dirty_mblocks>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|774| <<dmz_flush_metadata>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/raid5-ppl.c|1040| <<ppl_recover>> ret = blkdev_issue_flush(rdev->bdev, GFP_KERNEL, NULL);
+ *   - drivers/nvme/target/io-cmd-bdev.c|229| <<nvmet_bdev_flush>> if (blkdev_issue_flush(req->ns->bdev, GFP_KERNEL, NULL))
+ *   - fs/block_dev.c|675| <<blkdev_fsync>> error = blkdev_issue_flush(bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/fsync.c|179| <<ext4_sync_file>> err = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/ialloc.c|1422| <<ext4_init_inode_table>> blkdev_issue_flush(sb->s_bdev, GFP_NOFS, NULL);
+ *   - fs/ext4/super.c|5170| <<ext4_sync_fs>> err = blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/fat/file.c|198| <<fat_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/inode.c|343| <<hfsplus_file_fsync>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/super.c|242| <<hfsplus_sync_fs>> blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/jbd2/checkpoint.c|417| <<jbd2_cleanup_journal_tail>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|778| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|885| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/recovery.c|289| <<jbd2_journal_recover>> err2 = blkdev_issue_flush(journal->j_fs_dev, GFP_KERNEL, NULL);
+ *   - fs/libfs.c|1044| <<generic_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/nilfs2/the_nilfs.h|378| <<nilfs_flush_device>> err = blkdev_issue_flush(nilfs->ns_bdev, GFP_KERNEL, NULL);
+ *   - fs/ocfs2/file.c|197| <<ocfs2_sync_file>> ret = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/reiserfs/file.c|162| <<reiserfs_sync_file>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/xfs/xfs_super.c|339| <<xfs_blkdev_issue_flush>> blkdev_issue_flush(buftarg->bt_bdev, GFP_NOFS, NULL);
+ */
 int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 		sector_t *error_sector)
 {
@@ -485,6 +910,10 @@ int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 }
 EXPORT_SYMBOL(blkdev_issue_flush);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2415| <<blk_mq_alloc_hctx>> hctx->fq = blk_alloc_flush_queue(q, hctx->numa_node, set->cmd_size,
+ */
 struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 		int node, int cmd_size, gfp_t flags)
 {
@@ -517,6 +946,10 @@ struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|43| <<blk_mq_hw_sysfs_release>> blk_free_flush_queue(hctx->fq);
+ */
 void blk_free_flush_queue(struct blk_flush_queue *fq)
 {
 	/* bio based request queue hasn't flush queue */
diff --git a/block/blk-lib.c b/block/blk-lib.c
index 5f2c429d4378..f526ffa3bfb9 100644
--- a/block/blk-lib.c
+++ b/block/blk-lib.c
@@ -209,6 +209,11 @@ int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 }
 EXPORT_SYMBOL(blkdev_issue_write_same);
 
+/*
+ * called by:
+ *   - block/blk-lib.c|335| <<__blkdev_issue_zeroout>> ret = __blkdev_issue_write_zeroes(bdev, sector, nr_sects, gfp_mask,
+ *   - block/blk-lib.c|375| <<blkdev_issue_zeroout>> ret = __blkdev_issue_write_zeroes(bdev, sector, nr_sects,
+ */
 static int __blkdev_issue_write_zeroes(struct block_device *bdev,
 		sector_t sector, sector_t nr_sects, gfp_t gfp_mask,
 		struct bio **biop, unsigned flags)
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 0157f2b3485a..5675db0eb4cf 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -15,9 +15,17 @@
 #include "blk.h"
 #include "blk-mq.h"
 
+/*
+ * 参数的'struct blk_mq_queue_map'是在set中的
+ */
 static int queue_index(struct blk_mq_queue_map *qmap,
 		       unsigned int nr_queues, const int q)
 {
+	/*
+	 * First hardware queue to map onto. Used by the PCIe NVMe
+	 * driver to map each hardware queue type (enum hctx_type) onto a distinct
+	 * set of hardware queues.
+	 */
 	return qmap->queue_offset + (q % nr_queues);
 }
 
@@ -32,6 +40,20 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|42| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|44| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3016| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3309| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|454| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|1852| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2195| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2196| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/nvme/host/tcp.c|2205| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7146| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1778| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
@@ -83,6 +105,12 @@ EXPORT_SYMBOL_GPL(blk_mq_map_queues);
  * We have no quick way of doing reverse lookups. This is only used at
  * queue init time, so runtime isn't important.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2092| <<blk_mq_alloc_rq_map>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2148| <<blk_mq_alloc_rqs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2805| <<blk_mq_realloc_hw_ctxs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], i);
+ */
 int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int index)
 {
 	int i;
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index b595a94c4d16..c3f4495a2201 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -11,6 +11,32 @@
 
 #include "blk-mq.h"
 
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */
+
 /**
  * blk_mq_pci_map_queues - provide a default queue mapping for PCI device
  * @qmap:	CPU to hardware queue map.
@@ -23,6 +49,12 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|452| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7148| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5830| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[HCTX_TYPE_DEFAULT],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
diff --git a/block/blk-mq-rdma.c b/block/blk-mq-rdma.c
index 14f968e58b8f..9292c590e68e 100644
--- a/block/blk-mq-rdma.c
+++ b/block/blk-mq-rdma.c
@@ -21,6 +21,11 @@
  * @set->nr_hw_queues, or @dev does not provide an affinity mask for a
  * vector, we fallback to the naive mapping.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1840| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/nvme/host/rdma.c|1842| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_READ],
+ */
 int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		struct ib_device *dev, int first_vec)
 {
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index ca22afd47b3d..d3e5cafe940b 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -85,6 +85,11 @@ void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|215| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *   - block/blk-mq-sched.c|220| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ */
 static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -167,6 +172,18 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1425| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ *
+ * blk_mq_delay_run_hw_queue() or blk_mq_run_hw_queue()
+ *  -> __blk_mq_delay_run_hw_queue()
+ *      -> __blk_mq_run_hw_queue()
+ *
+ * __blk_mq_delay_run_hw_queue()
+ *  -> 调度hctx->run_work = blk_mq_run_work_fn()
+ *      -> __blk_mq_run_hw_queue()
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -357,6 +374,10 @@ void blk_mq_sched_request_inserted(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
 
+/*
+ * called by only:
+ *   - block/blk-mq-sched.c|411| <<blk_mq_sched_insert_request>> if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
+ */
 static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 				       bool has_sched,
 				       struct request *rq)
@@ -375,6 +396,14 @@ static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-exec.c|64| <<blk_execute_rq_nowait>> blk_mq_sched_insert_request(rq, at_head, true, false);
+ *   - block/blk-mq.c|812| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, true, false, false);
+ *   - block/blk-mq.c|818| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, false, false, false);
+ *   - block/blk-mq.c|2177| <<blk_mq_make_request>> blk_mq_sched_insert_request(rq, false, true, true);
+ *   - block/blk-mq.c|2205| <<blk_mq_make_request>> blk_mq_sched_insert_request(rq, false, true, true);
+ */
 void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 				 bool run_queue, bool async)
 {
diff --git a/block/blk-mq-sysfs.c b/block/blk-mq-sysfs.c
index 062229395a50..7d3eea033d4f 100644
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@ -253,6 +253,9 @@ static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 	if (ret)
 		return ret;
 
+	/*
+	 * 对于hctx->nr_ctx范围内的每一个hctx->ctxs[i]
+	 */
 	hctx_for_each_ctx(hctx, ctx, i) {
 		ret = kobject_add(&ctx->kobj, &hctx->kobj, "cpu%u", ctx->cpu);
 		if (ret)
@@ -269,6 +272,7 @@ void blk_mq_unregister_dev(struct device *dev, struct request_queue *q)
 
 	lockdep_assert_held(&q->sysfs_dir_lock);
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_unregister_hctx(hctx);
 
@@ -296,6 +300,10 @@ void blk_mq_sysfs_deinit(struct request_queue *q)
 	kobject_put(q->mq_kobj);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2871| <<blk_mq_init_allocated_queue>> blk_mq_sysfs_init(q);
+ */
 void blk_mq_sysfs_init(struct request_queue *q)
 {
 	struct blk_mq_ctx *ctx;
@@ -311,6 +319,10 @@ void blk_mq_sysfs_init(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|983| <<blk_register_queue>> __blk_mq_register_dev(dev, q);
+ */
 int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -325,6 +337,7 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 
 	kobject_uevent(q->mq_kobj, KOBJ_ADD);
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		ret = blk_mq_register_hctx(hctx);
 		if (ret)
@@ -355,6 +368,7 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	if (!q->mq_sysfs_init_done)
 		goto unlock;
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_unregister_hctx(hctx);
 
@@ -362,6 +376,10 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	mutex_unlock(&q->sysfs_dir_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3317| <<__blk_mq_update_nr_hw_queues>> blk_mq_sysfs_register(q);
+ */
 int blk_mq_sysfs_register(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -371,6 +389,7 @@ int blk_mq_sysfs_register(struct request_queue *q)
 	if (!q->mq_sysfs_init_done)
 		goto unlock;
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		ret = blk_mq_register_hctx(hctx);
 		if (ret)
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index fbacde454718..a21165e5e42a 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -21,8 +21,28 @@
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|60| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 使用BLK_MQ_S_TAG_ACTIVE的地方:
+	 *   - block/blk-mq-tag.c|26| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|27| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|51| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|70| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *
+	 * 使用active_queues的地方:
+	 *   - block/blk-mq-tag.c|28| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|54| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|79| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *
+	 * For shared tag users, we track the number of currently active users
+	 * and attempt to provide a fair share of the tag depth for each of them.
+	 */
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
 	    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		atomic_inc(&hctx->tags->active_queues);
@@ -33,6 +53,11 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|56| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|273| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -60,6 +85,10 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|94| <<__blk_mq_get_tag>> !hctx_may_queue(data->hctx, bt))
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
@@ -452,6 +481,10 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2096| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 15bc74acb57e..ad5f9f38367c 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -9,14 +9,37 @@
  */
 struct blk_mq_tags {
 	unsigned int nr_tags;
+	/*
+	 * 设置nr_reserved_tags的地方:
+	 *   - block/blk-mq-tag.c|471| <<blk_mq_init_tags>> tags->nr_reserved_tags = reserved_tags;
+	 */
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 使用active_queues的地方:
+	 *   - block/blk-mq-tag.c|28| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|54| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|79| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
 	struct sbitmap_queue breserved_tags;
 
 	struct request **rqs;
+	/*
+	 * static_rqs一共是二维:
+	 *   - block/blk-mq.c|289| <<blk_mq_rq_ctx_init>> struct request *rq = tags->static_rqs[tag];
+	 *   - block/blk-mq.c|2053| <<blk_mq_free_rqs>> struct request *rq = tags->static_rqs[i];
+	 *   - block/blk-mq.c|2058| <<blk_mq_free_rqs>> tags->static_rqs[i] = NULL;
+	 *   - block/blk-mq.c|2078| <<blk_mq_free_rq_map>> kfree(tags->static_rqs);
+	 *   - block/blk-mq.c|2079| <<blk_mq_free_rq_map>> tags->static_rqs = NULL;
+	 *   - block/blk-mq.c|2109| <<blk_mq_alloc_rq_map>> tags->static_rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *   - block/blk-mq.c|2112| <<blk_mq_alloc_rq_map>> if (!tags->static_rqs) {
+	 *   - block/blk-mq.c|2201| <<blk_mq_alloc_rqs>> tags->static_rqs[i] = rq;
+	 *   - block/blk-mq.c|2203| <<blk_mq_alloc_rqs>> tags->static_rqs[i] = NULL;
+	 */
 	struct request **static_rqs;
 	struct list_head page_list;
 };
@@ -35,11 +58,21 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|130| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|177| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1133| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
 	if (!hctx)
 		return &bt->ws[0];
+	/*
+	 * 这里似乎是唯一使用的地方:
+	 * Index of next available dispatch_wait queue to insert requests.
+	 */
 	return sbq_wait_ptr(bt, &hctx->wait_index);
 }
 
@@ -52,6 +85,11 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * calld by:
+ *   - block/blk-mq.c|387| <<blk_mq_get_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|1067| <<blk_mq_get_driver_tag>> shared = blk_mq_tag_busy(data.hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -60,6 +98,11 @@ static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|963| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2264| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -74,12 +117,22 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * in flight at the same time. The caller has to make sure the tag
  * can't be freed.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|243| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+ *   - block/blk-flush.c|326| <<blk_kick_flush>> blk_mq_tag_set_rq(flush_rq->mq_hctx, first_rq->tag, flush_rq);
+ */
 static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 		unsigned int tag, struct request *rq)
 {
 	hctx->tags->rqs[tag] = rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|189| <<blk_mq_put_tag>> if (!blk_mq_tag_is_reserved(tags, tag)) {
+ *   - block/blk-mq.c|1064| <<blk_mq_get_driver_tag>> if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
+ */
 static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
 					  unsigned int tag)
 {
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 488341628256..16ece920ec49 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -9,6 +9,32 @@
 #include <linux/module.h>
 #include "blk-mq.h"
 
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */
+
 /**
  * blk_mq_virtio_map_queues - provide a default queue mapping for virtio device
  * @qmap:	CPU to hardware queue map.
@@ -21,6 +47,11 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|697| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 323c9cb28066..9e65b9ee84d5 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -43,13 +43,22 @@
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2833| <<blk_mq_init_allocated_queue>> blk_mq_poll_stats_bkt,
+ *   - block/blk-mq.c|3365| <<blk_mq_poll_nsecs>> bucket = blk_mq_poll_stats_bkt(rq);
+ */
 static int blk_mq_poll_stats_bkt(const struct request *rq)
 {
 	int ddir, sectors, bucket;
 
+	/* read or write */
 	ddir = rq_data_dir(rq);
 	sectors = blk_rq_stats_sectors(rq);
 
+	/*
+	 * ilog2(sectors)相当于获得sectors的order吧
+	 */
 	bucket = ddir + 2 * ilog2(sectors);
 
 	if (bucket < 0)
@@ -64,6 +73,10 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
  * Check if any of the ctx, dispatch list or elevator
  * have pending work in this hardware queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1469| <<blk_mq_run_hw_queue>> blk_mq_hctx_has_pending(hctx);
+ */
 static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
 	return !list_empty_careful(&hctx->dispatch) ||
@@ -280,6 +293,11 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		rq->tag = -1;
 		rq->internal_tag = tag;
 	} else {
+		/*
+		 * struct blk_mq_alloc_data:
+		 *   -> struct blk_mq_ctx *ctx;
+		 *   -> struct blk_mq_hw_ctx *hctx;
+		 */
 		if (data->hctx->flags & BLK_MQ_F_TAG_SHARED) {
 			rq_flags = RQF_MQ_INFLIGHT;
 			atomic_inc(&data->hctx->nr_active);
@@ -397,6 +415,16 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|600| <<blk_get_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/block/mtip32xx/mtip32xx.c|985| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+ *   - drivers/block/sx8.c|511| <<carm_array_info>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/block/sx8.c|564| <<carm_send_special>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/ide/ide-atapi.c|202| <<ide_prep_sense>> sense_rq = blk_mq_alloc_request(drive->queue, REQ_OP_DRV_IN,
+ *   - drivers/nvme/host/core.c|489| <<nvme_alloc_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/scsi/fnic/fnic_scsi.c|2300| <<fnic_scsi_host_start_tag>> dummy = blk_mq_alloc_request(q, REQ_OP_WRITE, BLK_MQ_REQ_NOWAIT);
+ */
 struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 		blk_mq_req_flags_t flags)
 {
@@ -541,6 +569,29 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(__blk_mq_end_request);
 
+/*
+ * 部分调用blk_mq_end_request()的例子:
+ *   - block/blk-flush.c|421| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+ *   - block/blk-flush.c|774| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+ *   - block/blk-mq.c|1366| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+ *   - block/blk-mq.c|1979| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+ *   - block/blk-mq.c|2015| <<blk_mq_try_issue_list_directly>> blk_mq_end_request(rq, ret);
+ *   - block/bsg-lib.c|158| <<bsg_teardown_job>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/block/loop.c|487| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/nbd.c|340| <<nbd_complete_rq>> blk_mq_end_request(req, cmd->status);
+ *   - drivers/block/null_blk_main.c|677| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+ *   - drivers/block/virtio_blk.c|226| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+ *   - drivers/block/xen-blkfront.c|918| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+ *   - drivers/block/xen-blkfront.c|2113| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+ *   - drivers/ide/ide-cd.c|765| <<cdrom_newpc_intr>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/ide/ide-pm.c|50| <<ide_pm_execute_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/ide/ide-pm.c|220| <<ide_complete_pm_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/md/dm-rq.c|174| <<dm_end_request>> blk_mq_end_request(rq, error);
+ *   - drivers/md/dm-rq.c|271| <<dm_softirq_done>> blk_mq_end_request(rq, tio->error);
+ *   - drivers/nvme/host/core.c|307| <<nvme_complete_rq>> blk_mq_end_request(req, status);
+ *   - drivers/nvme/host/multipath.c|76| <<nvme_failover_req>> blk_mq_end_request(req, 0);
+ *   - drivers/scsi/scsi_transport_fc.c|3581| <<fc_bsg_job_timeout>> blk_mq_end_request(req, BLK_STS_IOERR);
+ */
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
@@ -557,6 +608,10 @@ static void __blk_mq_complete_request_remote(void *data)
 	q->mq_ops->complete(rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|652| <<blk_mq_complete_request>> __blk_mq_complete_request(rq);
+ */
 static void __blk_mq_complete_request(struct request *rq)
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
@@ -632,6 +687,25 @@ static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
  *	Ends all I/O on a request. It does not handle partial completions.
  *	The actual completion happens out-of-order, through a IPI handler.
  **/
+/*
+ * 部分调用的例子:
+ *   - block/bsg-lib.c|186| <<bsg_job_done>> blk_mq_complete_request(blk_mq_rq_from_pdu(job));
+ *   - drivers/block/loop.c|499| <<lo_rw_aio_do_completion>> blk_mq_complete_request(rq);
+ *   - drivers/block/loop.c|1957| <<loop_handle_cmd>> blk_mq_complete_request(rq);
+ *   - drivers/block/nbd.c|452| <<nbd_xmit_timeout>> blk_mq_complete_request(req);
+ *   - drivers/block/nbd.c|788| <<recv_work>> blk_mq_complete_request(blk_mq_rq_from_pdu(cmd));
+ *   - drivers/block/nbd.c|804| <<nbd_clear_req>> blk_mq_complete_request(req);
+ *   - drivers/block/null_blk_main.c|1242| <<nullb_complete_cmd>> blk_mq_complete_request(cmd->rq);
+ *   - drivers/block/null_blk_main.c|1369| <<null_timeout_rq>> blk_mq_complete_request(rq);
+ *   - drivers/block/virtio_blk.c|244| <<virtblk_done>> blk_mq_complete_request(req);
+ *   - drivers/block/xen-blkfront.c|1648| <<blkif_interrupt>> blk_mq_complete_request(req);
+ *   - drivers/md/dm-rq.c|291| <<dm_complete_request>> blk_mq_complete_request(rq);
+ *   - drivers/nvme/host/core.c|321| <<nvme_cancel_request>> blk_mq_complete_request(req);
+ *   - drivers/nvme/host/nvme.h|460| <<nvme_end_request>> blk_mq_complete_request(req);
+ *   - drivers/s390/block/dasd.c|2786| <<__dasd_cleanup_cqr>> blk_mq_complete_request(req);
+ *   - drivers/s390/block/scm_blk.c|259| <<scm_request_finish>> blk_mq_complete_request(scmrq->request[i]);
+ *   - drivers/scsi/scsi_lib.c|1618| <<scsi_mq_done>> if (unlikely(!blk_mq_complete_request(cmd->request)))
+ */
 bool blk_mq_complete_request(struct request *rq)
 {
 	if (unlikely(blk_should_fake_timeout(rq->q)))
@@ -647,7 +721,23 @@ void blk_mq_start_request(struct request *rq)
 
 	trace_block_rq_issue(q, rq);
 
+	/*
+	 * 在以下使用QUEUE_FLAG_STATS:
+	 *   - block/blk-mq.c|650| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+	 *   - block/blk-stat.c|152| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|162| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|188| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 */
 	if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+		/*
+		 * 在以下使用io_start_time_ns:
+		 *   - block/bfq-iosched.c|5912| <<bfq_finish_requeue_request>> rq->io_start_time_ns,
+		 *   - block/blk-mq.c|671| <<blk_mq_start_request>> rq->io_start_time_ns = ktime_get_ns();
+		 *   - block/blk-mq.c|327| <<blk_mq_rq_ctx_init>> rq->io_start_time_ns = 0;
+		 *   - block/blk-stat.c|59| <<blk_stat_add>> value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
+		 *   - block/blk-wbt.c|619| <<wbt_issue>> rwb->sync_issue = rq->io_start_time_ns;
+		 *   - block/kyber-iosched.c|651| <<kyber_completed_request>> now - rq->io_start_time_ns);
+		 */
 		rq->io_start_time_ns = ktime_get_ns();
 		rq->stats_sectors = blk_rq_sectors(rq);
 		rq->rq_flags |= RQF_STATS;
@@ -780,6 +870,25 @@ void blk_mq_delay_kick_requeue_list(struct request_queue *q,
 }
 EXPORT_SYMBOL(blk_mq_delay_kick_requeue_list);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3917| <<blk_mq_poll_hybrid>> rq = blk_mq_tag_to_rq(hctx->tags, blk_qc_t_to_tag(cookie));
+ *   - block/blk-mq.c|3919| <<blk_mq_poll_hybrid>> rq = blk_mq_tag_to_rq(hctx->sched_tags, blk_qc_t_to_tag(cookie));
+ *   - drivers/block/mtip32xx/mtip32xx.c|167| <<mtip_cmd_from_tag>> return blk_mq_rq_to_pdu(blk_mq_tag_to_rq(hctx->tags, tag));
+ *   - drivers/block/nbd.c|696| <<nbd_read_stat>> req = blk_mq_tag_to_rq(nbd->tag_set.tags[hwq],
+ *   - drivers/block/skd_main.c|1520| <<skd_isr_completion_posted>> WARN_ON_ONCE(blk_mq_tag_to_rq(skdev->tag_set.tags[hwq],
+ *   - drivers/block/skd_main.c|1526| <<skd_isr_completion_posted>> rq = blk_mq_tag_to_rq(skdev->tag_set.tags[hwq], tag);
+ *   - drivers/block/sx8.c|926| <<carm_handle_resp>> rq = blk_mq_tag_to_rq(host->tag_set.tags[0], msg_idx);
+ *   - drivers/nvme/host/pci.c|975| <<nvme_handle_cqe>> req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
+ *   - drivers/nvme/host/rdma.c|1443| <<nvme_rdma_process_nvme_rsp>> rq = blk_mq_tag_to_rq(nvme_rdma_tagset(queue), cqe->command_id);
+ *   - drivers/nvme/host/tcp.c|433| <<nvme_tcp_process_nvme_cqe>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), cqe->command_id);
+ *   - drivers/nvme/host/tcp.c|453| <<nvme_tcp_handle_c2h_data>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|557| <<nvme_tcp_handle_r2t>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|642| <<nvme_tcp_recv_data>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|741| <<nvme_tcp_recv_ddgst>> struct request *rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue),
+ *   - drivers/nvme/target/loop.c|111| <<nvme_loop_queue_response>> rq = blk_mq_tag_to_rq(nvme_loop_tagset(queue), cqe->command_id);
+ *   - include/scsi/scsi_tcq.h|33| <<scsi_host_find_tag>> req = blk_mq_tag_to_rq(shost->tag_set.tags[hwq],
+ */
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)
 {
 	if (tag < tags->nr_tags) {
@@ -1148,6 +1257,39 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
  * - take 4 as factor for avoiding to get too small(0) result, and this
  *   factor doesn't matter because EWMA decreases exponentially
  */
+/*
+ * commit 6e768717304bdbe8d2897ca8298f6b58863fdc41
+ * Author: Ming Lei <ming.lei@redhat.com>
+ * Date:   Tue Jul 3 09:03:16 2018 -0600
+ *
+ * blk-mq: dequeue request one by one from sw queue if hctx is busy
+ *
+ * It won't be efficient to dequeue request one by one from sw queue,
+ * but we have to do that when queue is busy for better merge performance.
+ *
+ * This patch takes the Exponential Weighted Moving Average(EWMA) to figure
+ * out if queue is busy, then only dequeue request one by one from sw queue
+ * when queue is busy.
+ *
+ * Fixes: b347689ffbca ("blk-mq-sched: improve dispatching from sw queue")
+ * Cc: Kashyap Desai <kashyap.desai@broadcom.com>
+ * Cc: Laurence Oberman <loberman@redhat.com>
+ * Cc: Omar Sandoval <osandov@fb.com>
+ * Cc: Christoph Hellwig <hch@lst.de>
+ * Cc: Bart Van Assche <bart.vanassche@wdc.com>
+ * Cc: Hannes Reinecke <hare@suse.de>
+ * Reported-by: Kashyap Desai <kashyap.desai@broadcom.com>
+ * Tested-by: Kashyap Desai <kashyap.desai@broadcom.com>
+ * Signed-off-by: Ming Lei <ming.lei@redhat.com>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ *
+ * called by:
+ *   - block/blk-mq.c|1372| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|1375| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|1838| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|1843| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|1847| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ */
 static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 {
 	unsigned int ewma;
@@ -1155,14 +1297,34 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 	if (hctx->queue->elevator)
 		return;
 
+	/*
+	 * 在以下使用dispatch_busy:
+	 *   - block/blk-mq-debugfs.c|621| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+	 *   - block/blk-mq-sched.c|217| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+	 *   - block/blk-mq-sched.c|436| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+	 *   - block/blk-mq.c|1215| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+	 *   - block/blk-mq.c|1225| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 *   - block/blk-mq.c|2071| <<blk_mq_make_request>> !data.hctx->dispatch_busy) {
+	 */
 	ewma = hctx->dispatch_busy;
 
 	if (!ewma && !busy)
 		return;
 
+	/*
+	 * BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT - 1 = 8 - 1 = 7
+	 * 这里相当于ewma = hctx->dispatch_busy * 7
+	 */
 	ewma *= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT - 1;
+	/*
+	 * 当busy的时候ewma = ewma + 1 << 4
+	 * 相当于ewma加上16
+	 */
 	if (busy)
 		ewma += 1 << BLK_MQ_DISPATCH_BUSY_EWMA_FACTOR;
+	/*
+	 * 相当于ewma = ewma / 8
+	 */
 	ewma /= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT;
 
 	hctx->dispatch_busy = ewma;
@@ -1173,6 +1335,16 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 /*
  * Returns true if we did some work AND can potentially do more.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|120| <<blk_mq_do_dispatch_sched>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|170| <<blk_mq_do_dispatch_ctx>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|218| <<blk_mq_sched_dispatch_requests>> if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+ *   - block/blk-mq-sched.c|231| <<blk_mq_sched_dispatch_requests>> blk_mq_dispatch_rq_list(q, &rq_list, false);
+ *
+ * 核心思想是处理list上每一个request,处理不了就放入hctx->dispatch
+ * 然后根据情况决定是否run
+ */
 bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			     bool got_budget)
 {
@@ -1220,6 +1392,7 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			}
 		}
 
+		/* 到了此时才在list上删除 */
 		list_del_init(&rq->queuelist);
 
 		bd.rq = rq;
@@ -1278,6 +1451,21 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			q->mq_ops->commit_rqs(hctx);
 
 		spin_lock(&hctx->lock);
+		/*
+		 * 在以下添加rq到hctx->dispatch:
+		 *   - block/blk-mq-sched.c|376| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1414| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1794| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2373| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 * 在以下使用hctx->dispatch:
+		 *   - block/blk-mq.c|2491| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+		 *   - block/blk-mq-sched.c|199| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *   - block/blk-mq-sched.c|196| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+		 *   - block/blk-mq-sched.c|198| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+		 *   - block/blk-mq.c|82| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+		 *   - block/blk-mq-debugfs.c|363| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+		 *   - block/blk-mq-debugfs.c|370| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+		 */
 		list_splice_init(list, &hctx->dispatch);
 		spin_unlock(&hctx->lock);
 
@@ -1327,6 +1515,11 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	return (queued + errors) != 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1493| <<__blk_mq_delay_run_hw_queue>> __blk_mq_run_hw_queue(hctx);
+ *   - block/blk-mq.c|1654| <<blk_mq_run_work_fn>> __blk_mq_run_hw_queue(hctx);
+ */
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	int srcu_idx;
@@ -1424,6 +1617,11 @@ static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 	return next_cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1507| <<blk_mq_delay_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, true, msecs);
+ *   - block/blk-mq.c|1530| <<blk_mq_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, async, 0);
+ */
 static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 					unsigned long msecs)
 {
@@ -1441,16 +1639,46 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 		put_cpu();
 	}
 
+	/*
+	 * 在以下使用run_work:
+	 *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 */
 	kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
 				    msecs_to_jiffies(msecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1370| <<blk_mq_dispatch_rq_list>> blk_mq_delay_run_hw_queue(hctx, BLK_MQ_RESOURCE_DELAY);
+ *   - drivers/ide/ide-io.c|453| <<ide_requeue_and_plug>> blk_mq_delay_run_hw_queue(q->queue_hw_ctx[0], 3);
+ *   - drivers/scsi/scsi_lib.c|1639| <<scsi_mq_get_budget>> blk_mq_delay_run_hw_queue(hctx, SCSI_QUEUE_DELAY);
+ */
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 {
 	__blk_mq_delay_run_hw_queue(hctx, true, msecs);
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|80| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|419| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|453| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|168| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1148| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1451| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1650| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1715| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1735| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1821| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2135| <<blk_mq_make_request>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2399| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|698| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ */
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1582,6 +1810,14 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 }
 EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
 
+/*
+ * 在以下使用run_work:
+ *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+ *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+ *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+ *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+ *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+ */
 static void blk_mq_run_work_fn(struct work_struct *work)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1629,6 +1865,14 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|518| <<blk_insert_flush>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|787| <<blk_mq_requeue_work>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|1884| <<__blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, run_queue);
+ *   - block/blk-mq.c|1900| <<blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, true);
+ *   - block/blk-mq.c|1934| <<blk_mq_try_issue_list_directly>> blk_mq_request_bypass_insert(rq,
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
@@ -1787,6 +2031,11 @@ static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2037| <<blk_mq_try_issue_directly>> ret = __blk_mq_try_issue_directly(hctx, rq, cookie, false, true);
+ *   - block/blk-mq.c|2054| <<blk_mq_request_issue_directly>> ret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true, last);
+ */
 static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 						struct request *rq,
 						blk_qc_t *cookie,
@@ -2275,6 +2524,10 @@ static int blk_mq_hw_ctx_size(struct blk_mq_tag_set *tag_set)
 	return hw_ctx_size;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2765| <<blk_mq_alloc_and_init_hctx>> if (blk_mq_init_hctx(q, set, hctx, hctx_idx))
+ */
 static int blk_mq_init_hctx(struct request_queue *q,
 		struct blk_mq_tag_set *set,
 		struct blk_mq_hw_ctx *hctx, unsigned hctx_idx)
@@ -2302,6 +2555,10 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	return -1;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2761| <<blk_mq_alloc_and_init_hctx>> hctx = blk_mq_alloc_hctx(q, set, node);
+ */
 static struct blk_mq_hw_ctx *
 blk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,
 		int node)
@@ -2321,6 +2578,14 @@ blk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,
 		node = set->numa_node;
 	hctx->numa_node = node;
 
+	/*
+	 * 在以下使用run_work:
+	 *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 */
 	INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
 	spin_lock_init(&hctx->lock);
 	INIT_LIST_HEAD(&hctx->dispatch);
@@ -2429,6 +2694,35 @@ static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ *
+ * called by:
+ *   - block/blk-mq.c|2910| <<blk_mq_init_allocated_queue>> blk_mq_map_swqueue(q);
+ *   - block/blk-mq.c|3312| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_swqueue(q);
+ */
 static void blk_mq_map_swqueue(struct request_queue *q)
 {
 	unsigned int i, j, hctx_idx;
@@ -2436,6 +2730,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	struct blk_mq_ctx *ctx;
 	struct blk_mq_tag_set *set = q->tag_set;
 
+	/*
+	 * 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i]
+	 */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		cpumask_clear(hctx->cpumask);
 		hctx->nr_ctx = 0;
@@ -2448,6 +2745,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 * If the cpu isn't present, the cpu is mapped to first hctx.
 	 */
 	for_each_possible_cpu(i) {
+		/*
+		 * hctx_idx用来索引set->tags[hctx_idx]
+		 */
 		hctx_idx = set->map[HCTX_TYPE_DEFAULT].mq_map[i];
 		/* unmapped hw queue can be remapped after CPU topo changed */
 		if (!set->tags[hctx_idx] &&
@@ -2470,6 +2770,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			}
 
 			hctx = blk_mq_map_queue_type(q, j, i);
+			/*
+			 *  struct blk_mq_hw_ctx *hctxs[HCTX_MAX_TYPES];
+			 */
 			ctx->hctxs[j] = hctx;
 			/*
 			 * If the CPU is already set in the mask, then we've
@@ -2535,6 +2838,11 @@ static void blk_mq_map_swqueue(struct request_queue *q)
  * Caller needs to ensure that we're either frozen/quiesced, or that
  * the queue isn't live yet.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2589| <<blk_mq_update_tag_set_depth>> queue_set_hctx_shared(q, shared);
+ *   - block/blk-mq.c|2625| <<blk_mq_add_queue_tag_set>> queue_set_hctx_shared(q, true);
+ */
 static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2548,6 +2856,11 @@ static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2604| <<blk_mq_del_queue_tag_set>> blk_mq_update_tag_set_depth(set, false);
+ *   - block/blk-mq.c|2622| <<blk_mq_add_queue_tag_set>> blk_mq_update_tag_set_depth(set, true);
+ */
 static void blk_mq_update_tag_set_depth(struct blk_mq_tag_set *set,
 					bool shared)
 {
@@ -2600,8 +2913,16 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 }
 
 /* All allocations will be freed in release handler of q->mq_kobj */
+/*
+ * called by:
+ *   - block/blk-mq.c|2867| <<blk_mq_init_allocated_queue>> if (blk_mq_alloc_ctxs(q))
+ */
 static int blk_mq_alloc_ctxs(struct request_queue *q)
 {
+	/*
+	 * struct request_queue:
+	 *  -> struct blk_mq_ctx __percpu *queue_ctx;
+	 */
 	struct blk_mq_ctxs *ctxs;
 	int cpu;
 
@@ -2656,6 +2977,33 @@ void blk_mq_release(struct request_queue *q)
 	blk_mq_sysfs_deinit(q);
 }
 
+/*
+ * 部分调用blk_mq_init_queue()的例子:
+ *   - block/blk-mq.c|2732| <<blk_mq_init_sq_queue>> q = blk_mq_init_queue(set);
+ *   - block/bsg-lib.c|387| <<bsg_setup_queue>> q = blk_mq_init_queue(set);
+ *   - drivers/block/loop.c|2022| <<loop_add>> lo->lo_queue = blk_mq_init_queue(&lo->tag_set);
+ *   - drivers/block/nbd.c|1692| <<nbd_dev_add>> q = blk_mq_init_queue(&nbd->tag_set);
+ *   - drivers/block/null_blk_main.c|1717| <<null_add_dev>> nullb->q = blk_mq_init_queue(nullb->tag_set);
+ *   - drivers/block/rbd.c|5042| <<rbd_init_disk>> q = blk_mq_init_queue(&rbd_dev->tag_set);
+ *   - drivers/block/virtio_blk.c|802| <<virtblk_probe>> q = blk_mq_init_queue(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|986| <<xlvbd_init_blk_queue>> rq = blk_mq_init_queue(&info->tag_set);
+ *   - drivers/ide/ide-probe.c|790| <<ide_init_queue>> q = blk_mq_init_queue(set);
+ *   - drivers/nvme/host/core.c|3495| <<nvme_alloc_ns>> ns->queue = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/fc.c|2481| <<nvme_fc_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3148| <<nvme_fc_init_ctrl>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/fc.c|3154| <<nvme_fc_init_ctrl>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|1628| <<nvme_alloc_admin_tags>> dev->ctrl.admin_q = blk_mq_init_queue(&dev->admin_tagset);
+ *   - drivers/nvme/host/rdma.c|809| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/rdma.c|815| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/rdma.c|886| <<nvme_rdma_configure_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/tcp.c|1676| <<nvme_tcp_configure_io_queues>> ctrl->connect_q = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/tcp.c|1729| <<nvme_tcp_configure_admin_queue>> ctrl->fabrics_q = blk_mq_init_queue(ctrl->admin_tagset);
+ *   - drivers/nvme/host/tcp.c|1735| <<nvme_tcp_configure_admin_queue>> ctrl->admin_q = blk_mq_init_queue(ctrl->admin_tagset);
+ *   - drivers/nvme/target/loop.c|362| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|368| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|529| <<nvme_loop_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1871| <<scsi_mq_alloc_queue>> sdev->request_queue = blk_mq_init_queue(&sdev->host->tag_set);
+ */
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)
 {
 	struct request_queue *uninit_q, *q;
@@ -2710,6 +3058,10 @@ struct request_queue *blk_mq_init_sq_queue(struct blk_mq_tag_set *set,
 }
 EXPORT_SYMBOL(blk_mq_init_sq_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2814| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+ */
 static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 		struct blk_mq_tag_set *set, struct request_queue *q,
 		int hctx_idx, int node)
@@ -2744,6 +3096,11 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2876| <<blk_mq_init_allocated_queue>> blk_mq_realloc_hw_ctxs(set, q);
+ *   - block/blk-mq.c|3304| <<__blk_mq_update_nr_hw_queues>> blk_mq_realloc_hw_ctxs(set, q);
+ */
 static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 						struct request_queue *q)
 {
@@ -2762,6 +3119,12 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 			memcpy(new_hctxs, hctxs, q->nr_hw_queues *
 			       sizeof(*hctxs));
 		q->queue_hw_ctx = new_hctxs;
+		/*
+		 * 设置nr_hw_queues的地方:
+		 *   - block/blk-mq.c|3103| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+		 *   - block/blk-mq.c|3147| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+		 *   - block/blk-mq.c|3233| <<blk_mq_init_allocated_queue>> q->nr_hw_queues = 0;
+		 */
 		q->nr_hw_queues = set->nr_hw_queues;
 		kfree(hctxs);
 		hctxs = new_hctxs;
@@ -2822,6 +3185,11 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2700| <<blk_mq_init_queue>> q = blk_mq_init_allocated_queue(set, uninit_q, false);
+ *   - drivers/md/dm-rq.c|566| <<dm_mq_init_request_queue>> q = blk_mq_init_allocated_queue(md->tag_set, md->queue, true);
+ */
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q,
 						  bool elevator_init)
@@ -2959,6 +3327,11 @@ static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3109| <<blk_mq_alloc_tag_set>> ret = blk_mq_update_queue_map(set);
+ *   - block/blk-mq.c|3301| <<__blk_mq_update_nr_hw_queues>> blk_mq_update_queue_map(set);
+ */
 static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 {
 	if (set->ops->map_queues && !is_kdump_kernel()) {
@@ -2978,6 +3351,10 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 		 * killing stale mapping since one CPU may not be mapped
 		 * to any hw queue.
 		 */
+		/*
+		 * 对于每一种类型的map,
+		 * 清空所有cpu的blk_mq_queue_map->mq_map[cpu] = 0
+		 */
 		for (i = 0; i < set->nr_maps; i++)
 			blk_mq_clear_mq_map(&set->map[i]);
 
@@ -3017,6 +3394,27 @@ static int blk_mq_realloc_tag_set_tags(struct blk_mq_tag_set *set,
  * requested depth down, if it's too large. In that case, the set
  * value will be stored in set->queue_depth.
  */
+/*
+ * 调用blk_mq_alloc_tag_set()的例子:
+ *   - block/blk-mq.c|2947| <<blk_mq_init_sq_queue>> ret = blk_mq_alloc_tag_set(set);
+ *   - block/bsg-lib.c|384| <<bsg_setup_queue>> if (blk_mq_alloc_tag_set(set))
+ *   - drivers/block/loop.c|2033| <<loop_add>> err = blk_mq_alloc_tag_set(&lo->tag_set);
+ *   - drivers/block/nbd.c|1688| <<nbd_dev_add>> err = blk_mq_alloc_tag_set(&nbd->tag_set);
+ *   - drivers/block/null_blk_main.c|1716| <<null_init_tag_set>> return blk_mq_alloc_tag_set(set);
+ *   - drivers/block/virtio_blk.c|819| <<virtblk_probe>> err = blk_mq_alloc_tag_set(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|984| <<xlvbd_init_blk_queue>> if (blk_mq_alloc_tag_set(&info->tag_set))
+ *   - drivers/ide/ide-probe.c|787| <<ide_init_queue>> if (blk_mq_alloc_tag_set(set))
+ *   - drivers/md/dm-rq.c|562| <<dm_mq_init_request_queue>> err = blk_mq_alloc_tag_set(md->tag_set);
+ *   - drivers/nvme/host/fc.c|2475| <<nvme_fc_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3143| <<nvme_fc_init_ctrl>> ret = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|1645| <<nvme_alloc_admin_tags>> if (blk_mq_alloc_tag_set(&dev->admin_tagset))
+ *   - drivers/nvme/host/pci.c|2328| <<nvme_dev_add>> ret = blk_mq_alloc_tag_set(&dev->tagset);
+ *   - drivers/nvme/host/rdma.c|755| <<nvme_rdma_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+ *   - drivers/nvme/host/tcp.c|1493| <<nvme_tcp_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+ *   - drivers/nvme/target/loop.c|357| <<nvme_loop_configure_admin_queue>> error = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|525| <<nvme_loop_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1906| <<scsi_mq_setup_tags>> return blk_mq_alloc_tag_set(&shost->tag_set);
+ */
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
 	int i, ret;
@@ -3069,6 +3467,13 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 
 	ret = -ENOMEM;
 	for (i = 0; i < set->nr_maps; i++) {
+		/*
+		 * struct blk_mq_queue_map {
+		 *      unsigned int *mq_map;
+		 *      unsigned int nr_queues;
+		 *      unsigned int queue_offset;
+		 * };
+		 */
 		set->map[i].mq_map = kcalloc_node(nr_cpu_ids,
 						  sizeof(set->map[i].mq_map[0]),
 						  GFP_KERNEL, set->numa_node);
@@ -3233,6 +3638,15 @@ static void blk_mq_elv_switch_back(struct list_head *head,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * 有三处queue数量的地方:
+ *   - blk_mq_tag_set->nr_hw_queues
+ *   - blk_mq_queue_map->nr_queues
+ *   - request_queue->nr_hw_queues
+ *
+ * called by:
+ *   - block/blk-mq.c|3673| <<blk_mq_update_nr_hw_queues>> __blk_mq_update_nr_hw_queues(set, nr_hw_queues);
+ */
 static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 							int nr_hw_queues)
 {
@@ -3297,6 +3711,22 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1249| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/null_blk_main.c|323| <<nullb_apply_submit_queues>> blk_mq_update_nr_hw_queues(set, submit_queues);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2554| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2336| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|892| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1682| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|471| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *
+ * 有三处queue数量的地方:
+ *   - blk_mq_tag_set->nr_hw_queues
+ *   - blk_mq_queue_map->nr_queues
+ *   - request_queue->nr_hw_queues
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
@@ -3306,8 +3736,19 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
 
 /* Enable polling stats and return whether they were already enabled. */
+/*
+ * called by:
+ *   - block/blk-mq.c|3373| <<blk_mq_poll_nsecs>> if (!blk_poll_stats_enable(q))
+ */
 static bool blk_poll_stats_enable(struct request_queue *q)
 {
+	/*
+	 * 在以下使用QUEUE_FLAG_POLL_STATS:
+	 *   - block/blk-mq.c|3331| <<blk_poll_stats_enable>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+	 *   - block/blk-mq.c|3332| <<blk_poll_stats_enable>> blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
+	 *   - block/blk-mq.c|3344| <<blk_mq_poll_stats_start>> if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+	 *   - block/blk-sysfs.c|880| <<__blk_release_queue>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
+	 */
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
 	    blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
 		return true;
@@ -3315,6 +3756,10 @@ static bool blk_poll_stats_enable(struct request_queue *q)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|539| <<__blk_mq_end_request>> blk_mq_poll_stats_start(rq->q);
+ */
 static void blk_mq_poll_stats_start(struct request_queue *q)
 {
 	/*
@@ -3328,6 +3773,12 @@ static void blk_mq_poll_stats_start(struct request_queue *q)
 	blk_stat_activate_msecs(q->poll_cb, 100);
 }
 
+/*
+ * 在以下使用blk_mq_poll_stats_fn():
+ *   - block/blk-mq.c|2852| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *
+ * 核心思想是把BLK_MQ_POLL_STATS_BKTS个bucket设置q->poll_stat[bucket] = cb->stat[bucket]
+ */
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 {
 	struct request_queue *q = cb->data;
@@ -3339,6 +3790,12 @@ static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3416| <<blk_mq_poll_hybrid_sleep>> nsecs = blk_mq_poll_nsecs(q, hctx, rq);
+ *
+ * 这个函数应该就是用来估算一个要sleep的时间
+ */
 static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 				       struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
@@ -3372,6 +3829,10 @@ static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3471| <<blk_mq_poll_hybrid>> return blk_mq_poll_hybrid_sleep(q, hctx, rq);
+ */
 static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 				     struct blk_mq_hw_ctx *hctx,
 				     struct request *rq)
@@ -3381,6 +3842,13 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	unsigned int nsecs;
 	ktime_t kt;
 
+	/*
+	 * 在以下使用RQF_MQ_POLL_SLEPT:
+	 *   - block/blk-mq.c|3444| <<blk_mq_poll_hybrid_sleep>> if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
+	 *   - block/blk-mq.c|3461| <<blk_mq_poll_hybrid_sleep>> rq->rq_flags |= RQF_MQ_POLL_SLEPT;
+	 *
+	 * already slept for hybrid poll
+	 */
 	if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
 		return false;
 
@@ -3404,6 +3872,9 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	 * This will be replaced with the stats tracking code, using
 	 * 'avg_completion_time / 2' as the pre-sleep target.
 	 */
+	/*
+	 * kt是非常核心的时间!!!
+	 */
 	kt = nsecs;
 
 	mode = HRTIMER_MODE_REL;
@@ -3426,6 +3897,33 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	return true;
 }
 
+/*
+ * 在以下patch加入的函数
+ * commit 06426adf072bca62ac31ea396ff2159a34f276c2
+ * Author: Jens Axboe <axboe@fb.com>
+ * Date:   Mon Nov 14 13:01:59 2016 -0700
+ *
+ * blk-mq: implement hybrid poll mode for sync O_DIRECT
+ *
+ * This patch enables a hybrid polling mode. Instead of polling after IO
+ * submission, we can induce an artificial delay, and then poll after that.
+ * For example, if the IO is presumed to complete in 8 usecs from now, we
+ * can sleep for 4 usecs, wake up, and then do our polling. This still puts
+ * a sleep/wakeup cycle in the IO path, but instead of the wakeup happening
+ * after the IO has completed, it'll happen before. With this hybrid
+ * scheme, we can achieve big latency reductions while still using the same
+ * (or less) amount of CPU.
+ *
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ * Tested-By: Stephen Bates <sbates@raithlin.com>
+ * Reviewed-By: Stephen Bates <sbates@raithlin.com>
+ *
+ * called by:
+ *   - block/blk-mq.c|3547| <<blk_poll>> if (blk_mq_poll_hybrid(q, hctx, cookie))
+ *
+ * If the device access time exceeds the IRQ model overhead,
+ * sleeping before the I/O completion will not hurt latency
+ */
 static bool blk_mq_poll_hybrid(struct request_queue *q,
 			       struct blk_mq_hw_ctx *hctx, blk_qc_t cookie)
 {
@@ -3463,6 +3961,87 @@ static bool blk_mq_poll_hybrid(struct request_queue *q,
  *    looping until at least one completion is found, unless the task is
  *    otherwise marked running (or we need to reschedule).
  */
+/*
+ * poll是可以有专门的hctx的. 用cookie来标记用的哪个hctx甚至哪个tag.
+ *
+ * blk_qc_t_to_queue_num()把cookie转换成q->queue_hw_ctx[]的index.
+ *
+ * blk_qc_t_to_tag()把cookie转换成tag.
+ *
+ * ext4_direct_IO_write()
+ *  -> __blockdev_direct_IO()
+ *      -> do_blockdev_direct_IO
+ *          -> dio_bio_submit
+ *              -> submit_bio()
+ *          -> dio_await_completion()
+ *              -> dio_await_one()
+ *                  -> blk_poll()
+ *
+ * cookie一直通过submit_bio()返回, 然后通过blk_poll()去poll()查看IO是否完成.
+ *
+ * slides: I/O Latency Optimization with Polling
+ *
+ *
+ * 根据一位网友的测试:
+ * 从上述测试结果来看,IO-Polling对于sync模式的direct-io的延迟有较好的提升,
+ * sync模式下,无论4K随机读或者随机写IO压力下,延迟平均大约减少5μs,而这5μs
+ * 几乎就是中断模式下,处理中断时,上下文切换的时间差.
+ * 相比随机读,对随机写的延迟降低约20%,这对延迟敏感的IO请求来说是极大的性能提升.
+ *
+ *
+ * 根据blk_mq_map_queue(), 只有REQ_HIPRI的才会被map到poll的queue.
+ *
+ * 102 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
+ * 103                                                      unsigned int flags,
+ * 104                                                      struct blk_mq_ctx *ctx)
+ * 105 {
+ * 106         enum hctx_type type = HCTX_TYPE_DEFAULT;
+ * 107
+ * 108         //
+ * 109         // The caller ensure that if REQ_HIPRI, poll must be enabled.
+ * 110         //
+ * 111         if (flags & REQ_HIPRI)
+ * 112                 type = HCTX_TYPE_POLL;
+ * 113         else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
+ * 114                 type = HCTX_TYPE_READ;115
+ * 116         return ctx->hctxs[type];
+ * 117 }
+ *
+ * 激活poll的方法:
+ *
+ * 激活io_poll:
+ * # echo 1 > /sys/block/nvme0n1/queue/io_poll
+ *
+ * 在fio中使用pvsync2加上hipri:
+ * # fio -name iops -rw=read -bs=4k -runtime=60 -iodepth 32 -filename /dev/nvme0n1 -ioengine pvsync2 -direct=1 -hipri=1
+ *
+ * [0] blk_poll
+ * [0] __blkdev_direct_IO_simple
+ * [0] blkdev_direct_IO
+ * [0] generic_file_read_iter
+ * [0] do_iter_readv_writev
+ * [0] do_iter_read
+ * [0] vfs_readv
+ * [0] do_preadv
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|812| <<nvme_execute_rq_polled>> blk_poll(q, request_to_qc_t(rq->mq_hctx, rq), true);
+ *   - fs/block_dev.c|257| <<__blkdev_direct_IO_simple>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/block_dev.c|295| <<blkdev_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), wait);
+ *   - fs/block_dev.c|451| <<__blkdev_direct_IO>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/direct-io.c|502| <<dio_await_one>> !blk_poll(dio->bio_disk->queue, dio->bio_cookie, true))
+ *   - fs/iomap/direct-io.c|57| <<iomap_dio_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), spin);
+ *   - fs/iomap/direct-io.c|562| <<iomap_dio_rw>> !blk_poll(dio->submit.last_queue,
+ *   - mm/page_io.c|424| <<swap_readpage>> if (!blk_poll(disk->queue, qc, true))
+ *
+ * Polling is tried for any block I/O belonging to a high-priority I/O context (IOCB_HIPRI)
+ *
+ * For applications, set only for preadv2/pwritev2 with RWF_HIPRI flag
+ *
+ * Not related to ioprio_set!
+ */
 int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -3475,6 +4054,10 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 	if (current->plug)
 		blk_flush_plug_list(current->plug, false);
 
+	/*
+	 * blk_qc_t_to_queue_num():
+	 * 把cookie的第31位(最高位)清空,然后向右移动16位,获得queue num
+	 */
 	hctx = q->queue_hw_ctx[blk_qc_t_to_queue_num(cookie)];
 
 	/*
@@ -3495,6 +4078,9 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 
 		hctx->poll_invoked++;
 
+		/*
+		 * nvme pci的例子是nvme_poll()
+		 */
 		ret = q->mq_ops->poll(hctx);
 		if (ret > 0) {
 			hctx->poll_success++;
@@ -3517,6 +4103,11 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 }
 EXPORT_SYMBOL_GPL(blk_poll);
 
+/*
+ * called by:
+ *   - drivers/scsi/bnx2i/bnx2i_hwi.c|1918| <<bnx2i_queue_scsi_cmd_resp>> p = &per_cpu(bnx2i_percpu, blk_mq_rq_cpu(sc->request));
+ *   - drivers/scsi/csiostor/csio_scsi.c|1789| <<csio_queuecommand>> sqset = &hw->sqset[ln->portid][blk_mq_rq_cpu(cmnd->request)];
+ */
 unsigned int blk_mq_rq_cpu(struct request *rq)
 {
 	return rq->mq_ctx->cpu;
diff --git a/block/blk-mq.h b/block/blk-mq.h
index eaaca8fc1c28..917b0aae5766 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -9,19 +9,82 @@ struct blk_mq_tag_set;
 
 struct blk_mq_ctxs {
 	struct kobject kobj;
+	/*
+	 * 主要设置和使用queue_ctx的地方:
+	 *   - block/blk-mq-sysfs.c|22| <<blk_mq_sysfs_release>> free_percpu(ctxs->queue_ctx);
+	 *   - block/blk-mq.c|2493| <<blk_mq_map_swqueue>> ctx = per_cpu_ptr(q->queue_ctx, i);
+	 *   - block/blk-mq.c|2641| <<blk_mq_alloc_ctxs>> ctxs->queue_ctx = alloc_percpu(struct blk_mq_ctx);
+	 *   - block/blk-mq.c|2642| <<blk_mq_alloc_ctxs>> if (!ctxs->queue_ctx)
+	 *   - block/blk-mq.c|2646| <<blk_mq_alloc_ctxs>> struct blk_mq_ctx *ctx = per_cpu_ptr(ctxs->queue_ctx, cpu);
+	 *   - block/blk-mq.c|2651| <<blk_mq_alloc_ctxs>> q->queue_ctx = ctxs->queue_ctx;
+	 */
 	struct blk_mq_ctx __percpu	*queue_ctx;
 };
 
 /**
  * struct blk_mq_ctx - State for a software queue facing the submitting CPUs
  */
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 使用rq_lists[]的地方:
+		 *   - block/blk-mq-debugfs.c|632| <<CTX_RQ_SEQ_OPS>> return seq_list_start(&ctx->rq_lists[type], *pos); \
+		 *   - block/blk-mq-debugfs.c|640| <<CTX_RQ_SEQ_OPS>> return seq_list_next(v, &ctx->rq_lists[type], pos); \
+		 *   - block/blk-mq-sched.c|316| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio, nr_segs)) {
+		 *   - block/blk-mq-sched.c|338| <<__blk_mq_sched_bio_merge>> !list_empty_careful(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|982| <<flush_busy_ctx>> list_splice_tail_init(&ctx->rq_lists[type], flush_data->list);
+		 *   - block/blk-mq.c|1017| <<dispatch_rq_from_ctx>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|1018| <<dispatch_rq_from_ctx>> dispatch_data->rq = list_entry_rq(ctx->rq_lists[type].next);
+		 *   - block/blk-mq.c|1020| <<dispatch_rq_from_ctx>> if (list_empty(&ctx->rq_lists[type]))
+		 *   - block/blk-mq.c|1641| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1643| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1690| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|2235| <<blk_mq_hctx_notify_dead>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|2236| <<blk_mq_hctx_notify_dead>> list_splice_init(&ctx->rq_lists[type], &tmp);
+		 *   - block/blk-mq.c|2416| <<blk_mq_init_cpu_queues>> INIT_LIST_HEAD(&__ctx->rq_lists[k]);
+		 */
 		struct list_head	rq_lists[HCTX_MAX_TYPES];
 	} ____cacheline_aligned_in_smp;
 
 	unsigned int		cpu;
+	/*
+	 * 设置index_hw[]的地方:
+	 *   - block/blk-mq.c|2513| <<blk_mq_map_swqueue>> ctx->index_hw[hctx->type] = hctx->nr_ctx;
+	 * 其余使用index_hw[]的地方:
+	 *   - block/blk-mq-sched.c|121| <<blk_mq_next_ctx>> unsigned short idx = ctx->index_hw[hctx->type];
+	 *   - block/blk-mq.c|93| <<blk_mq_hctx_mark_pending>> const int bit = ctx->index_hw[hctx->type];
+	 *   - block/blk-mq.c|102| <<blk_mq_hctx_clear_pending>> const int bit = ctx->index_hw[hctx->type];
+	 *   - block/blk-mq.c|1031| <<blk_mq_dequeue_from_ctx>> unsigned off = start ? start->index_hw[hctx->type] : 0;
+	 *   - block/kyber-iosched.c|570| <<kyber_bio_merge>> struct kyber_ctx_queue *kcq = &khd->kcqs[ctx->index_hw[hctx->type]];
+	 *   - block/kyber-iosched.c|595| <<kyber_insert_requests>> struct kyber_ctx_queue *kcq = &khd->kcqs[rq->mq_ctx->index_hw[hctx->type]];
+	 *   - block/kyber-iosched.c|604| <<kyber_insert_requests>> rq->mq_ctx->index_hw[hctx->type]);
+	 */
 	unsigned short		index_hw[HCTX_MAX_TYPES];
 	struct blk_mq_hw_ctx 	*hctxs[HCTX_MAX_TYPES];
 
@@ -30,6 +93,12 @@ struct blk_mq_ctx {
 	unsigned long		rq_merged;
 
 	/* incremented at completion time */
+	/*
+	 * 使用rq_completed[2]的地方:
+	 *   - block/blk-mq-debugfs.c|700| <<ctx_completed_show>> seq_printf(m, "%lu %lu\n", ctx->rq_completed[1], ctx->rq_completed[0]);
+	 *   - block/blk-mq-debugfs.c|709| <<ctx_completed_write>> ctx->rq_completed[0] = ctx->rq_completed[1] = 0;
+	 *   - block/blk-mq.c|516| <<blk_mq_free_request>> ctx->rq_completed[rq_is_sync(rq)]++;
+	 */
 	unsigned long		____cacheline_aligned_in_smp rq_completed[2];
 
 	struct request_queue	*queue;
@@ -86,10 +155,21 @@ extern int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int);
  * @type: the hctx type index
  * @cpu: CPU
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2425| <<blk_mq_init_cpu_queues>> hctx = blk_mq_map_queue_type(q, j, i);
+ *   - block/blk-mq.c|2496| <<blk_mq_map_swqueue>> ctx->hctxs[j] = blk_mq_map_queue_type(q,
+ *   - block/blk-mq.c|2501| <<blk_mq_map_swqueue>> hctx = blk_mq_map_queue_type(q, j, i);
+ *   - block/blk-mq.c|2524| <<blk_mq_map_swqueue>> ctx->hctxs[j] = blk_mq_map_queue_type(q,
+ */
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *q,
 							  enum hctx_type type,
 							  unsigned int cpu)
 {
+	/*
+	 * 设置queue_hw_ctx的地方:
+	 *   - block/blk-mq.c|2793| <<blk_mq_realloc_hw_ctxs>> q->queue_hw_ctx = new_hctxs;
+	 */
 	return q->queue_hw_ctx[q->tag_set->map[type].mq_map[cpu]];
 }
 
@@ -216,6 +296,13 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|45| <<blk_mq_pci_map_queues>> blk_mq_clear_mq_map(qmap);
+ *   - block/blk-mq.c|3011| <<blk_mq_update_queue_map>> blk_mq_clear_mq_map(&set->map[i]);
+ *
+ * 清空所有cpu的blk_mq_queue_map->mq_map[cpu] = 0
+ */
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index c8eda2e7b91e..f19b843ced8e 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -180,6 +180,18 @@ EXPORT_SYMBOL(blk_queue_bounce_limit);
  *    per-device basis in /sys/block/<device>/queue/max_sectors_kb.
  *    The soft limit can not exceed max_hw_sectors.
  **/
+/*
+ * 部分调用blk_queue_max_hw_sectors()的例子:
+ *   - drivers/block/loop.c|2112| <<loop_add>> blk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);
+ *   - drivers/block/nbd.c|1709| <<nbd_dev_add>> blk_queue_max_hw_sectors(disk->queue, 65536);
+ *   - drivers/block/virtio_blk.c|852| <<virtblk_probe>> blk_queue_max_hw_sectors(q, -1U);
+ *   - drivers/block/xen-blkfront.c|947| <<blkif_set_queue_limits>> blk_queue_max_hw_sectors(rq, (segments * XEN_PAGE_SIZE) / 512);
+ *   - drivers/nvme/host/core.c|2226| <<nvme_set_queue_limits>> blk_queue_max_hw_sectors(q, ctrl->max_hw_sectors);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1942| <<megasas_set_nvme_device_properties>> blk_queue_max_hw_sectors(sdev->request_queue, (max_io_size / 512));
+ *   - drivers/scsi/scsi_lib.c|1803| <<__scsi_init_queue>> blk_queue_max_hw_sectors(q, shost->max_sectors);
+ *   - drivers/scsi/scsi_scan.c|902| <<scsi_add_lun>> blk_queue_max_hw_sectors(sdev->request_queue, 512);
+ *   - drivers/scsi/scsi_scan.c|908| <<scsi_add_lun>> blk_queue_max_hw_sectors(sdev->request_queue, 1024);
+ */
 void blk_queue_max_hw_sectors(struct request_queue *q, unsigned int max_hw_sectors)
 {
 	struct queue_limits *limits = &q->limits;
@@ -250,6 +262,19 @@ EXPORT_SYMBOL(blk_queue_max_write_same_sectors);
  * @q:  the request queue for the device
  * @max_write_zeroes_sectors: maximum number of sectors to write per command
  **/
+/*
+ * 部分调用blk_queue_max_write_zeroes_sectors()的例子:
+ *   - drivers/block/loop.c|903| <<loop_config_discard>> blk_queue_max_write_zeroes_sectors(q, 0);
+ *   - drivers/block/loop.c|912| <<loop_config_discard>> blk_queue_max_write_zeroes_sectors(q, UINT_MAX >> 9);
+ *   - drivers/block/virtio_blk.c|923| <<virtblk_probe>> blk_queue_max_write_zeroes_sectors(q, v ? v : UINT_MAX);
+ *   - drivers/md/raid0.c|402| <<raid0_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, mddev->chunk_sectors);
+ *   - drivers/md/raid1.c|3114| <<raid1_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, 0);
+ *   - drivers/md/raid10.c|3767| <<raid10_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, 0);
+ *   - drivers/md/raid5.c|7454| <<raid5_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, 0);
+ *   - drivers/nvme/host/core.c|1723| <<nvme_config_discard>> blk_queue_max_write_zeroes_sectors(queue, UINT_MAX);
+ *   - drivers/nvme/host/core.c|1748| <<nvme_config_write_zeroes>> blk_queue_max_write_zeroes_sectors(disk->queue,
+ *   - drivers/scsi/sd.c|1002| <<sd_config_write_same>> blk_queue_max_write_zeroes_sectors(q, sdkp->max_ws_blocks *
+ */
 void blk_queue_max_write_zeroes_sectors(struct request_queue *q,
 		unsigned int max_write_zeroes_sectors)
 {
@@ -818,6 +843,22 @@ EXPORT_SYMBOL(blk_set_queue_depth);
  *
  * Tell the block layer about the write cache of @q.
  */
+/*
+ * 部分调用blk_queue_write_cache()的例子:
+ *   - drivers/block/loop.c|1007| <<loop_set_fd>> blk_queue_write_cache(lo->lo_queue, true, false);
+ *   - drivers/block/nbd.c|1136| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, true);
+ *   - drivers/block/nbd.c|1138| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, false);
+ *   - drivers/block/nbd.c|1141| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, false, false);
+ *   - drivers/block/null_blk_main.c|1742| <<null_add_dev>> blk_queue_write_cache(nullb->q, true, true);
+ *   - rivers/block/virtio_blk.c|609| <<virtblk_update_cache_mode>> blk_queue_write_cache(vblk->disk->queue, writeback, false);
+ *   - drivers/block/xen-blkfront.c|1014| <<xlvbd_flush>> blk_queue_write_cache(info->rq, info->feature_flush ? true : false,
+ *   - drivers/ide/ide-disk.c|554| <<update_flush>> blk_queue_write_cache(drive->queue, wc, false);
+ *   - drivers/md/dm-table.c|1910| <<dm_table_set_restrictions>> blk_queue_write_cache(q, wc, fua);
+ *   - drivers/md/md.c|5506| <<md_alloc>> blk_queue_write_cache(mddev->queue, true, true);
+ *   - drivers/nvme/host/core.c|2204| <<nvme_set_queue_limits>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/nvme/host/multipath.c|393| <<nvme_mpath_alloc_disk>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/scsi/sd.c|152| <<sd_set_flush_flag>> blk_queue_write_cache(sdkp->disk->queue, wc, fua);
+ */
 void blk_queue_write_cache(struct request_queue *q, bool wc, bool fua)
 {
 	if (wc)
diff --git a/block/blk-softirq.c b/block/blk-softirq.c
index 6e7ec87d49fa..71a852ff1c86 100644
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@ -14,12 +14,30 @@
 
 #include "blk.h"
 
+/*
+ * 在以下使用blk_cpu_done:
+ *   - block/blk-softirq.c|28| <<blk_done_softirq>> cpu_list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|47| <<trigger_softirq>> list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|86| <<blk_softirq_cpu_dead>> list_splice_init(&per_cpu(blk_cpu_done, cpu),
+ *   - block/blk-softirq.c|87| <<blk_softirq_cpu_dead>> this_cpu_ptr(&blk_cpu_done));
+ *   - block/blk-softirq.c|126| <<__blk_complete_request>> list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|148| <<blk_softirq_init>> INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
+ *
+ * 上面放的是request
+ */
 static DEFINE_PER_CPU(struct list_head, blk_cpu_done);
 
 /*
  * Softirq action handler - move entries to local list and loop over them
  * while passing them to the queue registered handler.
  */
+/*
+ * 在以下使用blk_done_softirq():
+ *   - 针对percpu的blk_cpu_done上的每一个request
+ * 调用rq->q->mq_ops->complete(rq)
+ * Softirq action handler - move entries to local list and loop over them
+ * while passing them to the queue registered handler.
+ */
 static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 {
 	struct list_head *cpu_list, local_list;
@@ -39,6 +57,13 @@ static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 }
 
 #ifdef CONFIG_SMP
+/*
+ * 在以下使用trigger_softirq():
+ *   - block/blk-softirq.c|62| <<raise_blk_irq>> data->func = trigger_softirq;
+ *
+ * 把data表示的request放入percpu的blk_cpu_done
+ * 触发raise_softirq_irqoff(BLOCK_SOFTIRQ) = blk_done_softirq()
+ */
 static void trigger_softirq(void *data)
 {
 	struct request *rq = data;
@@ -54,6 +79,10 @@ static void trigger_softirq(void *data)
 /*
  * Setup and invoke a run of 'trigger_softirq' on the given cpu.
  */
+/*
+ * called by:
+ *   - block/blk-softirq.c|137| <<__blk_complete_request>> } else if (raise_blk_irq(ccpu, req))
+ */
 static int raise_blk_irq(int cpu, struct request *rq)
 {
 	if (cpu_online(cpu)) {
@@ -76,6 +105,12 @@ static int raise_blk_irq(int cpu, struct request *rq)
 }
 #endif
 
+/*
+ * 在blk_softirq_init()中被使用:
+ * cpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,
+ *                           "block/softirq:dead", NULL,
+ *                           blk_softirq_cpu_dead);
+ */
 static int blk_softirq_cpu_dead(unsigned int cpu)
 {
 	/*
@@ -91,6 +126,10 @@ static int blk_softirq_cpu_dead(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|591| <<__blk_mq_complete_request>> __blk_complete_request(rq);
+ */
 void __blk_complete_request(struct request *req)
 {
 	struct request_queue *q = req->q;
diff --git a/block/blk-stat.c b/block/blk-stat.c
index 7da302ff88d0..6882129dba25 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -12,12 +12,115 @@
 #include "blk-mq.h"
 #include "blk.h"
 
+/*
+ * 核心的patch
+ *
+ * commit 34dbad5d26e2f4b88e60f0e9ad03f99480802812
+ * Author: Omar Sandoval <osandov@fb.com>
+ * Date:   Tue Mar 21 08:56:08 2017 -0700
+ *
+ * blk-stat: convert to callback-based statistics reporting
+ *
+ * Currently, statistics are gathered in ~0.13s windows, and users grab the
+ * statistics whenever they need them. This is not ideal for both in-tree
+ * users:
+ *
+ * 1. Writeback throttling wants its own dynamically sized window of
+ *    statistics. Since the blk-stats statistics are reset after every
+ *    window and the wbt windows don't line up with the blk-stats windows,
+ *    wbt doesn't see every I/O.
+ * 2. Polling currently grabs the statistics on every I/O. Again, depending
+ *    on how the window lines up, we may miss some I/Os. It's also
+ *    unnecessary overhead to get the statistics on every I/O; the hybrid
+ *    polling heuristic would be just as happy with the statistics from the
+ *    previous full window.
+ *
+ * This reworks the blk-stats infrastructure to be callback-based: users
+ * register a callback that they want called at a given time with all of
+ * the statistics from the window during which the callback was active.
+ * Users can dynamically bucketize the statistics. wbt and polling both
+ * currently use read vs. write, but polling can be extended to further
+ * subdivide based on request size.
+ *
+ * The callbacks are kept on an RCU list, and each callback has percpu
+ * stats buffers. There will only be a few users, so the overhead on the
+ * I/O completion side is low. The stats flushing is also simplified
+ * considerably: since the timer function is responsible for clearing the
+ * statistics, we don't have to worry about stale statistics.
+ *
+ * wbt is a trivial conversion. After the conversion, the windowing problem
+ * mentioned above is fixed.
+ *
+ * For polling, we register an extra callback that caches the previous
+ * window's statistics in the struct request_queue for the hybrid polling
+ * heuristic to use.
+ *
+ * Since we no longer have a single stats buffer for the request queue,
+ * this also removes the sysfs and debugfs stats entries. To replace those,
+ * we add a debugfs entry for the poll statistics.
+ *
+ * Signed-off-by: Omar Sandoval <osandov@fb.com>
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ *
+ *
+ * 在blk_alloc_queue_stats()分配一个struct blk_queue_stats结构,
+ * 初始化清空callback链表,设置stats->enable_accounting = false.
+ *
+ * blk_alloc_queue_node()
+ *  -> blk_alloc_queue_stats()
+ *
+ * 在poll的时候把struct blk_stat_callback链接入q->stats->callbacks链表
+ * 设置blk_queue_flag_set(QUEUE_FLAG_STATS, q)
+ *
+ * blk_poll()
+ *  -> blk_mq_poll_hybrid()
+ *      -> blk_mq_poll_hybrid_sleep()
+ *          -> blk_mq_poll_nsecs()
+ *              -> blk_poll_stats_enable()
+ *                  -> blk_stat_add_callback()
+ *
+ * 在end request的时候, 会调用blk_stat_add().
+ * 核心思想是找到request的request_queue->stats,遍历其callback链表,
+ * 对于链表上的每一个struct blk_stat_callback, 调用cb->bucket_fn()选取bucket,
+ * 然后把参数的now汇入对应bucket的struct blk_rq_stat
+ *
+ * 在end request的时候, 还会触发timer,
+ *
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ *
+ * timer最终触发blk_stat_timer_fn()->blk_mq_poll_stats_fn().
+ * blk_mq_poll_stats_fn()的核心思想是把每个bucket的
+ * q->poll_stat[bucket] = cb->stat[bucket];
+ *
+ * 在poll的时候会根据这些stat决定要先在hybrid的时候sleep多久!
+ *
+ *
+ * slides: I/O Latency Optimization with Polling
+ */
+
 struct blk_queue_stats {
 	struct list_head callbacks;
 	spinlock_t lock;
+	/*
+	 * 在以下使用enable_accounting:
+	 *   - block/blk-stat.c|161| <<blk_stat_remove_callback>> if (list_empty(&q->stats->callbacks) && !q->stats->enable_accounting)
+	 *   - block/blk-stat.c|187| <<blk_stat_enable_accounting>> q->stats->enable_accounting = true;
+	 *   - block/blk-stat.c|203| <<blk_alloc_queue_stats>> stats->enable_accounting = false;
+	 */
 	bool enable_accounting;
 };
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|199| <<latency_stat_init>> blk_rq_stat_init(&stat->rqs);
+ *   - block/blk-stat.c|93| <<blk_stat_timer_fn>> blk_rq_stat_init(&cb->stat[bucket]);
+ *   - block/blk-stat.c|101| <<blk_stat_timer_fn>> blk_rq_stat_init(&cpu_stat[bucket]);
+ *   - block/blk-stat.c|153| <<blk_stat_add_callback>> blk_rq_stat_init(&cpu_stat[bucket]);
+ */
 void blk_rq_stat_init(struct blk_rq_stat *stat)
 {
 	stat->min = -1ULL;
@@ -26,6 +129,11 @@ void blk_rq_stat_init(struct blk_rq_stat *stat)
 }
 
 /* src is a per-cpu stat, mean isn't initialized */
+/*
+ * called by:
+ *   - block/blk-iolatency.c|210| <<latency_stat_sum>> blk_rq_stat_sum(&sum->rqs, &stat->rqs);
+ *   - block/blk-stat.c|100| <<blk_stat_timer_fn>> blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
+ */
 void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 {
 	if (!src->nr_samples)
@@ -40,6 +148,18 @@ void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 	dst->nr_samples += src->nr_samples;
 }
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|222| <<latency_stat_record_time>> blk_rq_stat_add(&stat->rqs, req_time);
+ *   - block/blk-stat.c|80| <<blk_stat_add>> blk_rq_stat_add(stat, value);
+ *
+ * 调用的一个例子:
+ * __blk_mq_end_request()
+ *  -> blk_stat_add()
+ *      -> blk_rq_stat_add()
+ *
+ * 把参数的value汇入struct blk_rq_stat
+ */
 void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 {
 	stat->min = min(stat->min, value);
@@ -48,6 +168,14 @@ void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|540| <<__blk_mq_end_request>> blk_stat_add(rq, now);
+ *
+ * 核心思想是找到request的request_queue->stats,遍历其callback链表,
+ * 对于链表上的每一个struct blk_stat_callback, 调用cb->bucket_fn()选取bucket,
+ * 然后把参数的now汇入对应bucket的struct blk_rq_stat
+ */
 void blk_stat_add(struct request *rq, u64 now)
 {
 	struct request_queue *q = rq->q;
@@ -56,6 +184,15 @@ void blk_stat_add(struct request *rq, u64 now)
 	int bucket, cpu;
 	u64 value;
 
+	/*
+	 * 在以下使用io_start_time_ns:
+	 *   - block/bfq-iosched.c|5912| <<bfq_finish_requeue_request>> rq->io_start_time_ns,
+	 *   - block/blk-mq.c|671| <<blk_mq_start_request>> rq->io_start_time_ns = ktime_get_ns();
+	 *   - block/blk-mq.c|327| <<blk_mq_rq_ctx_init>> rq->io_start_time_ns = 0;
+	 *   - block/blk-stat.c|59| <<blk_stat_add>> value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
+	 *   - block/blk-wbt.c|619| <<wbt_issue>> rwb->sync_issue = rq->io_start_time_ns;
+	 *   - block/kyber-iosched.c|651| <<kyber_completed_request>> now - rq->io_start_time_ns);
+	 */
 	value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
 
 	blk_throtl_stat_add(rq, value);
@@ -70,13 +207,34 @@ void blk_stat_add(struct request *rq, u64 now)
 		if (bucket < 0)
 			continue;
 
+		/*
+		 * struct blk_stat_callback:
+		 *  -> struct blk_rq_stat __percpu *cpu_stat;
+		 *  -> struct blk_rq_stat stat;
+		 */
 		stat = &per_cpu_ptr(cb->cpu_stat, cpu)[bucket];
+		/*
+		 * 把参数的value汇入struct blk_rq_stat
+		 */
 		blk_rq_stat_add(stat, value);
 	}
 	put_cpu();
 	rcu_read_unlock();
 }
 
+/*
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ * 说明end request的时候mod timer
+ * The timer callback will be called when the window expires
+ * to gather block statistics during a time window in milliseconds.
+ *
+ * 在以下使用blk_stat_timer_fn():
+ *   - block/blk-stat.c|137| <<blk_stat_alloc_callback>> timer_setup(&cb->timer, blk_stat_timer_fn, 0);
+ */
 static void blk_stat_timer_fn(struct timer_list *t)
 {
 	struct blk_stat_callback *cb = from_timer(cb, t, timer);
@@ -89,16 +247,44 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	for_each_online_cpu(cpu) {
 		struct blk_rq_stat *cpu_stat;
 
+		/*
+		 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+		 */
 		cpu_stat = per_cpu_ptr(cb->cpu_stat, cpu);
 		for (bucket = 0; bucket < cb->buckets; bucket++) {
+			/*
+			 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+			 */
 			blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
 			blk_rq_stat_init(&cpu_stat[bucket]);
 		}
 	}
 
+	/*
+	 * __blk_mq_end_request()
+	 *  -> blk_mq_poll_stats_start()
+	 *      -> blk_stat_activate_msecs()
+	 *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+	 *
+	 * 说明end request的时候mod timer
+	 * The timer callback will be called when the window expires
+	 * to gather block statistics during a time window in milliseconds.
+	 *
+	 * -----------------------------------------
+	 *
+	 * 从blk_mq_init_allocated_queue()进来blk_stat_alloc_callback()
+	 * 的话timer_fn是blk_mq_poll_stats_fn()
+	 */
 	cb->timer_fn(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2861| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *   - block/blk-wbt.c|829| <<wbt_init>> rwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);
+ *
+ * 分配一个struct blk_stat_callback
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
@@ -110,12 +296,18 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 	if (!cb)
 		return NULL;
 
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话buckets是BLK_MQ_POLL_STATS_BKTS
+	 */
 	cb->stat = kmalloc_array(buckets, sizeof(struct blk_rq_stat),
 				 GFP_KERNEL);
 	if (!cb->stat) {
 		kfree(cb);
 		return NULL;
 	}
+	/*
+	 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+	 */
 	cb->cpu_stat = __alloc_percpu(buckets * sizeof(struct blk_rq_stat),
 				      __alignof__(struct blk_rq_stat));
 	if (!cb->cpu_stat) {
@@ -124,15 +316,59 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 		return NULL;
 	}
 
+	/*
+	 * 调用timer_fn的地方:
+	 *   - block/blk-stat.c|257| <<blk_stat_timer_fn>> cb->timer_fn(cb);
+	 *
+	 * 从blk_mq_init_allocated_queue()进来的话timer_fn是blk_mq_poll_stats_fn()
+	 */
 	cb->timer_fn = timer_fn;
+	/*
+	 * 调用bucket_fn的地方:
+	 *   - block/blk-stat.c|206| <<blk_stat_add>> bucket = cb->bucket_fn(rq);
+	 *
+	 * 从blk_mq_init_allocated_queue()进来的话bucket_fn是blk_mq_poll_stats_bkt()
+	 */
 	cb->bucket_fn = bucket_fn;
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话data是'struct request_queue'
+	 */
 	cb->data = data;
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话buckets是BLK_MQ_POLL_STATS_BKTS
+	 */
 	cb->buckets = buckets;
+	/*
+	 * __blk_mq_end_request()
+	 *  -> blk_mq_poll_stats_start()
+	 *      -> blk_stat_activate_msecs()
+	 *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+	 *
+	 * 说明end request的时候mod timer
+	 * The timer callback will be called when the window expires
+	 * to gather block statistics during a time window in milliseconds.
+	 */
 	timer_setup(&cb->timer, blk_stat_timer_fn, 0);
 
 	return cb;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3354| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|852| <<wbt_init>> blk_stat_add_callback(q, rwb->cb);
+ *
+ * poll调用的例子
+ * blk_poll()
+ *  -> blk_mq_poll_hybrid()
+ *      -> blk_mq_poll_hybrid_sleep()
+ *          -> blk_mq_poll_nsecs()
+ *              -> blk_poll_stats_enable()
+ *                  -> blk_stat_add_callback()
+ *
+ * 核心思想是把struct blk_stat_callback链接入q->stats->callbacks链表
+ * 设置blk_queue_flag_set(QUEUE_FLAG_STATS, q)
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -149,10 +385,22 @@ void blk_stat_add_callback(struct request_queue *q,
 
 	spin_lock(&q->stats->lock);
 	list_add_tail_rcu(&cb->list, &q->stats->callbacks);
+	/*
+	 * 在以下使用QUEUE_FLAG_STATS:
+	 *   - block/blk-mq.c|670| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+	 *   - block/blk-stat.c|320| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|335| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|376| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 */
 	blk_queue_flag_set(QUEUE_FLAG_STATS, q);
 	spin_unlock(&q->stats->lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|881| <<__blk_release_queue>> blk_stat_remove_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|696| <<wbt_exit>> blk_stat_remove_callback(q, rwb->cb);
+ */
 void blk_stat_remove_callback(struct request_queue *q,
 			      struct blk_stat_callback *cb)
 {
@@ -165,6 +413,10 @@ void blk_stat_remove_callback(struct request_queue *q,
 	del_timer_sync(&cb->timer);
 }
 
+/*
+ * 在以下使用blk_stat_free_callback_rcu():
+ *   - block/blk-stat.c|187| <<blk_stat_free_callback>> call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
+ */
 static void blk_stat_free_callback_rcu(struct rcu_head *head)
 {
 	struct blk_stat_callback *cb;
@@ -175,12 +427,23 @@ static void blk_stat_free_callback_rcu(struct rcu_head *head)
 	kfree(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2922| <<blk_mq_init_allocated_queue>> blk_stat_free_callback(q->poll_cb);
+ *   - block/blk-sysfs.c|882| <<__blk_release_queue>> blk_stat_free_callback(q->poll_cb);
+ *   - block/blk-wbt.c|697| <<wbt_exit>> blk_stat_free_callback(rwb->cb);
+ */
 void blk_stat_free_callback(struct blk_stat_callback *cb)
 {
 	if (cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
 
+/*
+ * called by:
+ *   - block/blk-throttle.c|2503| <<blk_throtl_register_queue>> blk_stat_enable_accounting(q);
+ *   - block/kyber-iosched.c|431| <<kyber_init_sched>> blk_stat_enable_accounting(q);
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	spin_lock(&q->stats->lock);
@@ -190,6 +453,13 @@ void blk_stat_enable_accounting(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_stat_enable_accounting);
 
+/*
+ * called by:
+ *   - block/blk-core.c|515| <<blk_alloc_queue_node>> q->stats = blk_alloc_queue_stats();
+ *
+ * 分配一个struct blk_queue_stats结构, 初始化清空callback链表
+ * 设置stats->enable_accounting = false;
+ */
 struct blk_queue_stats *blk_alloc_queue_stats(void)
 {
 	struct blk_queue_stats *stats;
@@ -205,6 +475,11 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 	return stats;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|562| <<blk_alloc_queue_node>> blk_free_queue_stats(q->stats);
+ *   - block/blk-sysfs.c|884| <<__blk_release_queue>> blk_free_queue_stats(q->stats);
+ */
 void blk_free_queue_stats(struct blk_queue_stats *stats)
 {
 	if (!stats)
diff --git a/block/blk-stat.h b/block/blk-stat.h
index 17b47a86eefb..5cd0d92b34a7 100644
--- a/block/blk-stat.h
+++ b/block/blk-stat.h
@@ -126,6 +126,12 @@ void blk_stat_free_callback(struct blk_stat_callback *cb);
  * gathering statistics.
  * @cb: The callback.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3369| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+ *   - block/blk-stat.c|203| <<blk_stat_add>> if (!blk_stat_is_active(cb))
+ *   - block/blk-wbt.c|593| <<wbt_wait>> if (!blk_stat_is_active(rwb->cb))
+ */
 static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
 {
 	return timer_pending(&cb->timer);
@@ -139,12 +145,20 @@ static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-wbt.c|349| <<rwb_arm_timer>> blk_stat_activate_nsecs(rwb->cb, rwb->cur_win_nsec);
+ */
 static inline void blk_stat_activate_nsecs(struct blk_stat_callback *cb,
 					   u64 nsecs)
 {
 	mod_timer(&cb->timer, jiffies + nsecs_to_jiffies(nsecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-wbt.c|712| <<wbt_disable_default>> blk_stat_deactivate(rwb->cb);
+ */
 static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
 {
 	del_timer_sync(&cb->timer);
@@ -158,6 +172,19 @@ static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3372| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+ *
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ * 说明end request的时候mod timer
+ * The timer callback will be called when the window expires
+ * to gather block statistics during a time window in milliseconds.
+ */
 static inline void blk_stat_activate_msecs(struct blk_stat_callback *cb,
 					   unsigned int msecs)
 {
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index fca9b158f4a0..231bb884efdb 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -390,8 +390,23 @@ static ssize_t queue_poll_delay_store(struct request_queue *q, const char *page,
 	return count;
 }
 
+/*
+ * nvme.poll_queues=2
+ *
+ * # cat /sys/block/nvme0n1/queue/io_poll
+ */
 static ssize_t queue_poll_show(struct request_queue *q, char *page)
 {
+	/*
+	 * 使用QUEUE_FLAG_POLL的地方:
+	 *   - block/blk-mq.c|3021| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-core.c|936| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - block/blk-mq.c|3774| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - drivers/nvme/host/core.c|805| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+	 *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+	 */
 	return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
 }
 
@@ -873,6 +888,10 @@ static void blk_exit_queue(struct request_queue *q)
  *     of the request queue reaches zero, blk_release_queue is called to release
  *     all allocated resources of the request queue.
  */
+/*
+ * 在以下使用__blk_release_queue():
+ *   - block/blk-sysfs.c|912| <<blk_release_queue>> INIT_WORK(&q->release_work, __blk_release_queue);
+ */
 static void __blk_release_queue(struct work_struct *work)
 {
 	struct request_queue *q = container_of(work, typeof(*q), release_work);
diff --git a/block/blk-zoned.c b/block/blk-zoned.c
index d00fcfd71dfe..fe7c472240e5 100644
--- a/block/blk-zoned.c
+++ b/block/blk-zoned.c
@@ -20,6 +20,74 @@
 
 #include "blk.h"
 
+/*
+ * 使用SMR(Shingled Magnetic Recording)技术后,可以增加存储密度,
+ * 但是带来的开销就是写操作必须是连续的(sequential write).
+ *
+ * 这样就把存储分成好多个zone,每个zone都是写连续的.
+ *
+ * NVMe的FTL层会建立mapping,帮助管理真实的存储.但是跳过FTL会有更多好处.
+ * 可以把NVMe切成多个zoned namespace,这样每个app可以直接使用自己的zoned namespace:
+ *
+ * - Reduced TCO due to minimal DRAM requirement per SSD
+ * - Additional savings due to decreased need for over provisioning of NAND media
+ * - Better drive endurance by reducing write amplification
+ * - Dramatically reduced latency
+ * - Significantly improved throughput
+ *
+ * null_blk和fio都支持zoned block.
+ *
+ * 相关的网站:
+ * - https://zonedstorage.io/introduction/zoned-storage/
+ * - https://zonedstorage.io/benchmarking/fio/
+ * - https://zonedstorage.io/getting-started/prerequisite/
+ * - https://blog.westerndigital.com/what-is-zoned-storage-initiative/
+ * - https://blog.westerndigital.com/storage-architectures-zettabyte-age/
+ */
+
+/*
+ * We've been doing a lot of work with the open source and Linux communities to
+ * contribute to the core technologies of SMR (Shingled Magnetic Recording). By
+ * overlaying tracks on a disk, we can achieve roughly a 20% increase in
+ * capacity. This requires data to be written sequentially so that it will not
+ * alter an underlying write track.
+ *
+ * Rearchitecting can require some effort initially, but the density and cost
+ * benefits are substantial and demonstrate all the advantages of purpose-built
+ * hardware and software-aware constructs. Today, our customers are already
+ * deploying SMR technology, and we expect that 50% of the HDD exabytes we ship
+ * will be on SMR by 2023.
+ *
+ *
+ * Zoned block devices are quite different than the block devices most people
+ * are used to. The concept came from shingled magnetic recording (SMR)
+ * devices, which allow much higher density storage, but that extra capacity
+ * comes with a price: less flexibility. Zoned devices have regions (zones)
+ * that can only be written sequentially; there is no random access for writes
+ * to those zones. Linux already supports these devices, and filesystems are
+ * adding support as well, but some applications may want a simpler, more
+ * straightforward interface; that's what a new filesystem, zonefs, is
+ * targeting.
+ *
+ *
+ * https://zonedstorage.io
+ *
+ * The zones of zoned storage devices must be written sequentially. Each zone
+ * of the device address space has a write pointer that keeps track of the
+ * position of the next write. Data in a zone cannot be directly overwritten.
+ * The zone must first be erased using a special command (zone reset). The
+ * figure below illustrates this principle.
+ *
+ * Linux ZBD interface implementation provides functions to discover the zone
+ * configuration of a zoned device and functions to manage zones (e.g. Zone
+ * reset). Furthermore, the Linux kernel ZBD support also modifies the kernel
+ * block I/O stack to ensure that the device access constraints (zone spanning
+ * commands, sequential write ordering, etc) are met.
+ */
+
+/*
+ * 没人使用
+ */
 static inline sector_t blk_zone_start(struct request_queue *q,
 				      sector_t sector)
 {
@@ -31,6 +99,11 @@ static inline sector_t blk_zone_start(struct request_queue *q,
 /*
  * Return true if a request is a write requests that needs zone write locking.
  */
+/*
+ * called by:
+ *   - include/linux/blkdev.h|1849| <<blk_req_zone_write_lock>> if (blk_req_needs_zone_write_lock(rq))
+ *   - include/linux/blkdev.h|1867| <<blk_req_can_dispatch_to_zone>> if (!blk_req_needs_zone_write_lock(rq))
+ */
 bool blk_req_needs_zone_write_lock(struct request *rq)
 {
 	if (!rq->q->seq_zones_wlock)
@@ -50,6 +123,10 @@ bool blk_req_needs_zone_write_lock(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_req_needs_zone_write_lock);
 
+/*
+ * called by:
+ *   - include/linux/blkdev.h|1850| <<blk_req_zone_write_lock>> __blk_req_zone_write_lock(rq);
+ */
 void __blk_req_zone_write_lock(struct request *rq)
 {
 	if (WARN_ON_ONCE(test_and_set_bit(blk_rq_zone_no(rq),
@@ -61,6 +138,10 @@ void __blk_req_zone_write_lock(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(__blk_req_zone_write_lock);
 
+/*
+ * called by:
+ *   - include/linux/blkdev.h|1856| <<blk_req_zone_write_unlock>> __blk_req_zone_write_unlock(rq);
+ */
 void __blk_req_zone_write_unlock(struct request *rq)
 {
 	rq->rq_flags &= ~RQF_ZONE_WRITE_LOCKED;
@@ -77,6 +158,13 @@ EXPORT_SYMBOL_GPL(__blk_req_zone_write_unlock);
  * Return the total number of zones of a zoned block device.  For a block
  * device without zone capabilities, the number of zones is always 0.
  */
+/*
+ * called by:
+ *   - block/ioctl.c|515| <<blkdev_ioctl>> return put_uint(arg, blkdev_nr_zones(bdev->bd_disk));
+ *   - drivers/block/null_blk_main.c|1947| <<null_gendisk_register>> nullb->q->nr_zones = blkdev_nr_zones(disk);
+ *   - drivers/md/dm-table.c|1962| <<dm_table_set_restrictions>> q->nr_zones = blkdev_nr_zones(t->md->disk);
+ *   - drivers/md/dm-zoned-target.c|730| <<dmz_get_zoned_device>> dev->nr_zones = blkdev_nr_zones(dev->bdev->bd_disk);
+ */
 unsigned int blkdev_nr_zones(struct gendisk *disk)
 {
 	sector_t zone_sectors = blk_queue_zone_sectors(disk->queue);
@@ -106,6 +194,15 @@ EXPORT_SYMBOL_GPL(blkdev_nr_zones);
  *    Note: The caller must use memalloc_noXX_save/restore() calls to control
  *    memory allocations done within this function.
  */
+/*
+ * called by:
+ *   - block/blk-zoned.c|263| <<blkdev_report_zones_ioctl>> ret = blkdev_report_zones(bdev, rep.sector, rep.nr_zones,
+ *   - drivers/md/dm-flakey.c|469| <<flakey_report_zones>> return blkdev_report_zones(fc->dev->bdev, sector, nr_zones,
+ *   - drivers/md/dm-linear.c|146| <<linear_report_zones>> return blkdev_report_zones(lc->dev->bdev, sector, nr_zones,
+ *   - drivers/md/dm-zoned-metadata.c|1179| <<dmz_init_zones>> ret = blkdev_report_zones(dev->bdev, 0, BLK_ALL_ZONES, dmz_init_zone,
+ *   - drivers/md/dm-zoned-metadata.c|1223| <<dmz_update_zone>> ret = blkdev_report_zones(zmd->dev->bdev, dmz_start_sect(zmd, zone), 1,
+ *   - fs/f2fs/super.c|2938| <<init_blkz_info>> ret = blkdev_report_zones(bdev, 0, BLK_ALL_ZONES, f2fs_report_zone_cb,
+ */
 int blkdev_report_zones(struct block_device *bdev, sector_t sector,
 			unsigned int nr_zones, report_zones_cb cb, void *data)
 {
@@ -123,6 +220,10 @@ int blkdev_report_zones(struct block_device *bdev, sector_t sector,
 }
 EXPORT_SYMBOL_GPL(blkdev_report_zones);
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|248| <<blkdev_zone_mgmt>> blkdev_allow_reset_all_zones(bdev, sector, nr_sectors)) {
+ */
 static inline bool blkdev_allow_reset_all_zones(struct block_device *bdev,
 						sector_t sector,
 						sector_t nr_sectors)
@@ -153,6 +254,12 @@ static inline bool blkdev_allow_reset_all_zones(struct block_device *bdev,
  *    The operation to execute on each zone can be a zone reset, open, close
  *    or finish request.
  */
+/*
+ * called by:
+ *   - block/blk-zoned.c|322| <<blkdev_zone_mgmt_ioctl>> return blkdev_zone_mgmt(bdev, op, zrange.sector, zrange.nr_sectors,
+ *   - drivers/md/dm-zoned-metadata.c|1289| <<dmz_reset_zone>> ret = blkdev_zone_mgmt(dev->bdev, REQ_OP_ZONE_RESET,
+ *   - fs/f2fs/segment.c|1784| <<__f2fs_issue_discard_zone>> return blkdev_zone_mgmt(bdev, REQ_OP_ZONE_RESET,
+ */
 int blkdev_zone_mgmt(struct block_device *bdev, enum req_opf op,
 		     sector_t sector, sector_t nr_sectors,
 		     gfp_t gfp_mask)
@@ -173,6 +280,31 @@ int blkdev_zone_mgmt(struct block_device *bdev, enum req_opf op,
 	if (!op_is_zone_mgmt(op))
 		return -EOPNOTSUPP;
 
+	/*
+	 * #define _GNU_SOURCE 1
+	 * #include <sys/ioctl.h>
+	 * #include <sys/types.h>
+	 * #include <sys/stat.h>
+	 * #include <fcntl.h>
+	 *
+	 * typedef unsigned long long __u64;
+	 *
+	 * struct blk_zone_range {
+	 *	__u64 sector;
+	 *	__u64 nr_sectors;
+	 * };
+	 *
+	 * #define BLKRESETZONE    _IOW(0x12, 131, struct blk_zone_range)
+	 *
+	 * int main(void)
+	 * {
+	 *	int fd = open("/dev/nullb0", O_RDWR|O_DIRECT);
+	 *	struct blk_zone_range zr = {4096, 0xfffffffffffff000ULL};
+	 *	ioctl(fd, BLKRESETZONE, &zr);
+	 *	return 0;
+	 *
+	 * 这里似乎有个bug??
+	 */
 	if (!nr_sectors || end_sector > capacity)
 		/* Out of range */
 		return -EINVAL;
@@ -217,6 +349,10 @@ struct zone_report_args {
 	struct blk_zone __user *zones;
 };
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|316| <<blkdev_report_zones_ioctl>> blkdev_copy_zone_to_user, &args)
+ */
 static int blkdev_copy_zone_to_user(struct blk_zone *zone, unsigned int idx,
 				    void *data)
 {
@@ -231,6 +367,10 @@ static int blkdev_copy_zone_to_user(struct blk_zone *zone, unsigned int idx,
  * BLKREPORTZONE ioctl processing.
  * Called from blkdev_ioctl.
  */
+/*
+ * 处理blkdev_ioctl(BLKREPORTZONE):
+ *   - block/ioctl.c|506| <<blkdev_ioctl>> return blkdev_report_zones_ioctl(bdev, mode, cmd, arg);
+ */
 int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,
 			      unsigned int cmd, unsigned long arg)
 {
@@ -275,6 +415,10 @@ int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,
  * BLKRESETZONE, BLKOPENZONE, BLKCLOSEZONE and BLKFINISHZONE ioctl processing.
  * Called from blkdev_ioctl.
  */
+/*
+ * 处理blkdev_ioctl(BLKFINISHZONE):
+ *   - block/ioctl.c|511| <<blkdev_ioctl>> return blkdev_zone_mgmt_ioctl(bdev, mode, cmd, arg);
+ */
 int blkdev_zone_mgmt_ioctl(struct block_device *bdev, fmode_t mode,
 			   unsigned int cmd, unsigned long arg)
 {
@@ -323,6 +467,11 @@ int blkdev_zone_mgmt_ioctl(struct block_device *bdev, fmode_t mode,
 				GFP_KERNEL);
 }
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|452| <<blk_revalidate_zone_cb>> blk_alloc_zone_bitmap(q->node, args->nr_zones);
+ *   - block/blk-zoned.c|462| <<blk_revalidate_zone_cb>> blk_alloc_zone_bitmap(q->node, args->nr_zones);
+ */
 static inline unsigned long *blk_alloc_zone_bitmap(int node,
 						   unsigned int nr_zones)
 {
@@ -330,6 +479,11 @@ static inline unsigned long *blk_alloc_zone_bitmap(int node,
 			    GFP_NOIO, node);
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|910| <<__blk_release_queue>> blk_queue_free_zone_bitmaps(q);
+ *   - block/blk-zoned.c|529| <<blk_revalidate_disk_zones>> blk_queue_free_zone_bitmaps(q);
+ */
 void blk_queue_free_zone_bitmaps(struct request_queue *q)
 {
 	kfree(q->conv_zones_bitmap);
@@ -350,6 +504,13 @@ struct blk_revalidate_zone_args {
 /*
  * Helper function to check the validity of zones of a zoned block device.
  */
+/*
+ * 在以下使用blk_revalidate_zone_cb():
+ *   - block/blk-zoned.c|512| <<blk_revalidate_disk_zones>> blk_revalidate_zone_cb, &args);
+ *
+ * 543         ret = disk->fops->report_zones(disk, 0, UINT_MAX,
+ * 544                                        blk_revalidate_zone_cb, &args);
+ */
 static int blk_revalidate_zone_cb(struct blk_zone *zone, unsigned int idx,
 				  void *data)
 {
@@ -432,6 +593,11 @@ static int blk_revalidate_zone_cb(struct blk_zone *zone, unsigned int idx,
  * drivers only q->nr_zones needs to be updated so that the sysfs exposed value
  * is correct.
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1941| <<null_gendisk_register>> int ret = blk_revalidate_disk_zones(disk);
+ *   - drivers/scsi/sd_zbc.c|440| <<sd_zbc_read_zones>> ret = blk_revalidate_disk_zones(disk);
+ */
 int blk_revalidate_disk_zones(struct gendisk *disk)
 {
 	struct request_queue *q = disk->queue;
diff --git a/block/blk.h b/block/blk.h
index 0b8884353f6b..bffe40d4a663 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -15,22 +15,83 @@
 extern struct dentry *blk_debugfs_root;
 #endif
 
+/*
+ * blk_flush_queue来自blk_mq_hw_ctx->fq
+ */
 struct blk_flush_queue {
 	unsigned int		flush_queue_delayed:1;
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下使用flush_pending_idx:
+	 *   - block/blk-flush.c|276| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|392| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 */
 	unsigned int		flush_pending_idx:1;
+	/*
+	 * 在以下修改flush_running_idx:
+	 *   - block/blk-flush.c|362| <<flush_end_io>> fq->flush_running_idx ^= 1;
+	 * 在以下使用flush_running_idx:
+	 *   - block/blk-flush.c|358| <<flush_end_io>> running = &fq->flush_queue[fq->flush_running_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 */
 	unsigned int		flush_running_idx:1;
 	blk_status_t 		rq_status;
 	unsigned long		flush_pending_since;
+	/*
+	 * 使用flush_queue[2]的地方:
+	 *   - block/blk-flush.c|352| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|454| <<flush_end_io>> running = &fq->flush_queue[fq->flush_running_idx];
+	 *   - block/blk-flush.c|501| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|849| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[0]);
+	 *   - block/blk-flush.c|850| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[1]);
+	 */
 	struct list_head	flush_queue[2];
+	/*
+	 * 在以下使用flush_data_in_flight:
+	 *   - block/blk-flush.c|400| <<blk_flush_complete_seq>> list_move_tail(&rq->flush.list, &fq->flush_data_in_flight);
+	 *   - block/blk-flush.c|599| <<blk_kick_flush>> if (!list_empty(&fq->flush_data_in_flight) && q->elevator &&
+	 *   - block/blk-flush.c|934| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_data_in_flight);
+	 */
 	struct list_head	flush_data_in_flight;
+	/*
+	 * 使用flush_rq的地方:
+	 *   - block/blk-flush.c|504| <<blk_kick_flush>> struct request *flush_rq = fq->flush_rq;
+	 *   - block/blk-flush.c|845| <<blk_alloc_flush_queue>> fq->flush_rq = kzalloc_node(rq_sz, flags, node);
+	 *   - block/blk-flush.c|846| <<blk_alloc_flush_queue>> if (!fq->flush_rq)
+	 *   - block/blk-flush.c|875| <<blk_free_flush_queue>> kfree(fq->flush_rq);
+	 *   - block/blk-mq.c|2303| <<blk_mq_exit_hctx>> set->ops->exit_request(set, hctx->fq->flush_rq, hctx_idx);
+	 *   - block/blk-mq.c|2361| <<blk_mq_init_hctx>> if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx,
+	 *   - block/blk.h|81| <<is_flush_rq>> return hctx->fq->flush_rq == req;
+	 */
 	struct request		*flush_rq;
 
 	/*
 	 * flush_rq shares tag with this rq, both can't be active
 	 * at the same time
 	 */
+	/*
+	 * orig_rq在以下使用:
+	 *   - block/blk-flush.c|458| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+	 *   - block/blk-flush.c|624| <<blk_kick_flush>> fq->orig_rq = first_rq;
+	 */
 	struct request		*orig_rq;
 	struct lock_class_key	key;
+	/*
+	 * 使用mq_flush_lock的地方:
+	 *   - block/blk-flush.c|434| <<flush_end_io>> spin_lock_irqsave(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|438| <<flush_end_io>> spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|469| <<flush_end_io>> spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|631| <<mq_flush_data_end_io>> spin_lock_irqsave(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|633| <<mq_flush_data_end_io>> spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|771| <<blk_insert_flush>> spin_lock_irq(&fq->mq_flush_lock);
+	 *   - block/blk-flush.c|779| <<blk_insert_flush>> spin_unlock_irq(&fq->mq_flush_lock);
+	 *   - block/blk-flush.c|874| <<blk_alloc_flush_queue>> spin_lock_init(&fq->mq_flush_lock);
+	 *   - block/blk-flush.c|886| <<blk_alloc_flush_queue>> lockdep_set_class(&fq->mq_flush_lock, &fq->key);
+	 */
 	spinlock_t		mq_flush_lock;
 };
 
@@ -38,6 +99,12 @@ extern struct kmem_cache *blk_requestq_cachep;
 extern struct kobj_type blk_queue_ktype;
 extern struct ida blk_queue_ida;
 
+/*
+ * called by:
+ *   - block/blk-flush.c|301| <<flush_end_io>> struct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);
+ *   - block/blk-flush.c|427| <<mq_flush_data_end_io>> struct blk_flush_queue *fq = blk_get_flush_queue(q, ctx);
+ *   - block/blk-flush.c|464| <<blk_insert_flush>> struct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);
+ */
 static inline struct blk_flush_queue *
 blk_get_flush_queue(struct request_queue *q, struct blk_mq_ctx *ctx)
 {
diff --git a/block/genhd.c b/block/genhd.c
index ff6268970ddc..ef46052eeceb 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -86,6 +86,10 @@ unsigned int part_in_flight(struct request_queue *q, struct hd_struct *part)
 	return inflight;
 }
 
+/*
+ * called by:
+ *   - block/partition-generic.c|159| <<part_inflight_show>> part_in_flight_rw(q, p, inflight);
+ */
 void part_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 		       unsigned int inflight[2])
 {
@@ -108,8 +112,24 @@ void part_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 		inflight[1] = 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|858| <<blk_partition_remap>> p = __disk_get_part(bio->bi_disk, bio->bi_partno);
+ *   - block/genhd.c|148| <<disk_get_part>> part = __disk_get_part(disk, partno); 
+ *   - fs/buffer.c|3040| <<guard_bio_eod>> part = __disk_get_part(bio->bi_disk, bio->bi_partno);
+ *
+ * struct gendisk:
+ *  -> struct disk_part_tbl __rcu *part_tbl;
+ *  -> struct hd_struct part0;
+ * 返回gendisk->part_tlb->part[partno]
+ */
 struct hd_struct *__disk_get_part(struct gendisk *disk, int partno)
 {
+	/*
+	 * struct gendisk:
+	 *  -> struct disk_part_tbl __rcu *part_tbl;
+	 *  -> struct hd_struct part0;
+	 */
 	struct disk_part_tbl *ptbl = rcu_dereference(disk->part_tbl);
 
 	if (unlikely(partno < 0 || partno >= ptbl->len))
@@ -131,11 +151,32 @@ struct hd_struct *__disk_get_part(struct gendisk *disk, int partno)
  * RETURNS:
  * Pointer to the found partition on success, NULL if not found.
  */
+/*
+ * called by:
+ *   - block/genhd.c|965| <<bdget_disk>> part = disk_get_part(disk, partno);
+ *   - block/genhd.c|1495| <<blk_lookup_devt>> part = disk_get_part(disk, partno);
+ *   - block/ioctl.c|74| <<blkpg_ioctl>> part = disk_get_part(disk, partno);
+ *   - block/ioctl.c|112| <<blkpg_ioctl>> part = disk_get_part(disk, partno);
+ *   - fs/block_dev.c|1598| <<__blkdev_get>> bdev->bd_part = disk_get_part(disk, partno);
+ *   - fs/block_dev.c|1649| <<__blkdev_get>> bdev->bd_part = disk_get_part(disk, partno);
+ *   - init/do_mounts.c|154| <<devt_from_partuuid>> part = disk_get_part(disk, dev_to_part(dev)->partno + offset);
+ *
+ * struct gendisk:
+ *  -> struct disk_part_tbl __rcu *part_tbl;
+ *  -> struct hd_struct part0;
+ * 返回gendisk->part_tlb->part[partno]
+ */
 struct hd_struct *disk_get_part(struct gendisk *disk, int partno)
 {
 	struct hd_struct *part;
 
 	rcu_read_lock();
+	/*
+	 * struct gendisk:
+	 *  -> struct disk_part_tbl __rcu *part_tbl;
+	 *  -> struct hd_struct part0;
+	 * 返回gendisk->part_tlb->part[partno]
+	 */
 	part = __disk_get_part(disk, partno);
 	if (part)
 		get_device(part_to_dev(part));
@@ -189,6 +230,20 @@ EXPORT_SYMBOL_GPL(disk_part_iter_init);
  * CONTEXT:
  * Don't care.
  */
+/*
+ * called by:
+ *   - block/genhd.c|733| <<register_disk>> while ((part = disk_part_iter_next(&piter)))
+ *   - block/genhd.c|855| <<del_gendisk>> while ((part = disk_part_iter_next(&piter))) {
+ *   - block/genhd.c|1058| <<printk_all_partitions>> while ((part = disk_part_iter_next(&piter))) {
+ *   - block/genhd.c|1153| <<show_partition>> while ((part = disk_part_iter_next(&piter)))
+ *   - block/genhd.c|1477| <<diskstats_show>> while ((hd = disk_part_iter_next(&piter))) {
+ *   - block/genhd.c|1691| <<set_disk_ro>> while ((part = disk_part_iter_next(&piter)))
+ *   - block/ioctl.c|58| <<blkpg_ioctl>> while ((part = disk_part_iter_next(&piter))) {
+ *   - block/ioctl.c|132| <<blkpg_ioctl>> while ((lpart = disk_part_iter_next(&piter))) {
+ *   - block/partition-generic.c|502| <<blk_drop_partitions>> while ((part = disk_part_iter_next(&piter)))
+ *   - drivers/s390/block/dasd.c|445| <<dasd_state_ready_to_online>> while ((part = disk_part_iter_next(&piter)))
+ *   - drivers/s390/block/dasd.c|472| <<dasd_state_online_to_ready>> while ((part = disk_part_iter_next(&piter)))
+ */
 struct hd_struct *disk_part_iter_next(struct disk_part_iter *piter)
 {
 	struct disk_part_tbl *ptbl;
@@ -305,6 +360,14 @@ EXPORT_SYMBOL_GPL(disk_map_sector_rcu);
  * Can be deleted altogether. Later.
  *
  */
+/*
+ * 在以下使用major_names[]:
+ *   - block/genhd.c|327| <<blkdev_show>> for (dp = major_names[major_to_index(offset)]; dp; dp = dp->next)
+ *   - block/genhd.c|363| <<register_blkdev>> for (index = ARRAY_SIZE(major_names)-1; index > 0; index--) {
+ *   - block/genhd.c|364| <<register_blkdev>> if (major_names[index] == NULL)
+ *   - block/genhd.c|397| <<register_blkdev>> for (n = &major_names[index]; *n; n = &(*n)->next) {
+ *   - block/genhd.c|425| <<unregister_blkdev>> for (n = &major_names[index]; *n; n = &(*n)->next)
+ */
 #define BLKDEV_MAJOR_HASH_SIZE 255
 static struct blk_major_name {
 	struct blk_major_name *next;
@@ -351,6 +414,23 @@ void blkdev_show(struct seq_file *seqf, off_t offset)
  * See Documentation/admin-guide/devices.txt for the list of allocated
  * major numbers.
  */
+/*
+ * 部分调用register_blkdev()的例子:
+ *   - block/genhd.c|1105| <<genhd_device_init>> register_blkdev(BLOCK_EXT_MAJOR, "blkext");
+ *   - drivers/block/loop.c|2274| <<loop_init>> if (register_blkdev(LOOP_MAJOR, "loop")) {
+ *   - drivers/block/nbd.c|2372| <<nbd_init>> if (register_blkdev(NBD_MAJOR, "nbd"))
+ *   - drivers/block/null_blk_main.c|1845| <<null_init>> null_major = register_blkdev(0, "nullb");
+ *   - drivers/block/virtio_blk.c|1033| <<init>> major = register_blkdev(0, "virtblk");
+ *   - drivers/block/xen-blkfront.c|2718| <<xlblk_init>> if (register_blkdev(XENVBD_MAJOR, DEV_NAME)) {
+ *   - drivers/ide/ide-probe.c|1002| <<hwif_init>> if (register_blkdev(hwif->major, hwif->name))
+ *   - drivers/md/dm.c|235| <<local_init>> r = register_blkdev(_major, _name);
+ *   - drivers/md/md.c|9328| <<md_init>> if ((ret = register_blkdev(MD_MAJOR, "md")) < 0)
+ *   - drivers/md/md.c|9331| <<md_init>> if ((ret = register_blkdev(0, "mdp")) < 0)
+ *   - drivers/scsi/sd.c|3644| <<init_sd>> if (register_blkdev(sd_major(i), "sd") != 0)
+ *   - drivers/scsi/sr.c|1036| <<init_sr>> rc = register_blkdev(SCSI_CDROM_MAJOR, "sr");
+ *
+ * 核心思想是在major_names[]找到一个可用的major
+ */
 int register_blkdev(unsigned int major, const char *name)
 {
 	struct blk_major_name **n, *p;
@@ -923,11 +1003,31 @@ EXPORT_SYMBOL(get_gendisk);
  * RETURNS:
  * Resulting block_device on success, NULL on failure.
  */
+/*
+ * called by:
+ *   - block/genhd.c|684| <<register_disk>> bdev = bdget_disk(disk, 0);
+ *   - block/genhd.c|1655| <<invalidate_partition>> struct block_device *bdev = bdget_disk(disk, partno);
+ *   - drivers/block/nbd.c|302| <<nbd_size_update>> struct block_device *bdev = bdget_disk(nbd->disk, 0);
+ *   - drivers/block/nbd.c|1483| <<nbd_release>> struct block_device *bdev = bdget_disk(disk, 0);
+ *   - drivers/block/xen-blkfront.c|2145| <<blkfront_closing>> bdev = bdget_disk(info->gd, 0);
+ *   - drivers/block/xen-blkfront.c|2506| <<blkfront_remove>> bdev = bdget_disk(disk, 0);
+ *   - drivers/block/xen-blkfront.c|2588| <<blkif_release>> bdev = bdget_disk(disk, 0);
+ *   - drivers/md/dm.c|1984| <<alloc_dev>> md->bdev = bdget_disk(md->disk, 0);
+ *   - fs/block_dev.c|1167| <<bd_start_claiming>> whole = bdget_disk(disk, 0);
+ *   - fs/block_dev.c|1459| <<revalidate_disk>> struct block_device *bdev = bdget_disk(disk, 0);
+ *   - fs/block_dev.c|1640| <<__blkdev_get>> whole = bdget_disk(disk, 0);
+ */
 struct block_device *bdget_disk(struct gendisk *disk, int partno)
 {
 	struct hd_struct *part;
 	struct block_device *bdev = NULL;
 
+	/*
+	 * struct gendisk:
+	 *  -> struct disk_part_tbl __rcu *part_tbl;
+	 *  -> struct hd_struct part0;
+	 * 返回gendisk->part_tlb->part[partno]
+	 */
 	part = disk_get_part(disk, partno);
 	if (part)
 		bdev = bdget(part_devt(part));
@@ -1283,6 +1383,15 @@ static void disk_replace_part_tbl(struct gendisk *disk,
  * RETURNS:
  * 0 on success, -errno on failure.
  */
+/*
+ * called by:
+ *   - block/genhd.c|1517| <<__alloc_disk_node>> if (disk_expand_part_tbl(disk, 0)) {
+ *   - block/partition-generic.c|342| <<add_partition>> err = disk_expand_part_tbl(disk, partno);
+ *   - block/partition-generic.c|593| <<blk_add_partitions>> disk_expand_part_tbl(disk, highest);
+ *
+ * 核心思想就是gendisk->parttlb->part[partno]数组太短
+ * 需要扩展以下
+ */
 int disk_expand_part_tbl(struct gendisk *disk, int partno)
 {
 	struct disk_part_tbl *old_ptbl =
@@ -1465,6 +1574,10 @@ dev_t blk_lookup_devt(const char *name, int partno)
 }
 EXPORT_SYMBOL(blk_lookup_devt);
 
+/*
+ * called by:
+ *   - include/linux/genhd.h|682| <<alloc_disk_node>> __disk = __alloc_disk_node(minors, node_id); \
+ */
 struct gendisk *__alloc_disk_node(int minors, int node_id)
 {
 	struct gendisk *disk;
diff --git a/block/partition-generic.c b/block/partition-generic.c
index 564fae77711d..c4af34d61e89 100644
--- a/block/partition-generic.c
+++ b/block/partition-generic.c
@@ -223,6 +223,15 @@ static const struct attribute_group *part_attr_groups[] = {
 	NULL
 };
 
+/*
+ * [0] part_release
+ * [0] device_release
+ * [0] kobject_cleanup
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void part_release(struct device *dev)
 {
 	struct hd_struct *p = dev_to_part(dev);
@@ -259,6 +268,10 @@ static void delete_partition_work_fn(struct work_struct *work)
 	put_device(part_to_dev(part));
 }
 
+/*
+ * called by:
+ *   - include/linux/genhd.h|718| <<hd_ref_init>> if (percpu_ref_init(&part->ref, __delete_partition, 0,
+ */
 void __delete_partition(struct percpu_ref *ref)
 {
 	struct hd_struct *part = container_of(ref, struct hd_struct, ref);
@@ -270,6 +283,14 @@ void __delete_partition(struct percpu_ref *ref)
  * Must be called either with bd_mutex held, before a disk can be opened or
  * after all disk users are gone.
  */
+/*
+ * called by:
+ *   - block/genhd.c|858| <<del_gendisk>> delete_partition(disk, part->partno);
+ *   - block/ioctl.c|94| <<blkpg_ioctl>> delete_partition(disk, partno);
+ *   - block/partition-generic.c|503| <<blk_drop_partitions>> delete_partition(disk, part->partno);
+ *
+ * 针对数据结构来说,主要是清空gendisk->part_tlb->part[partno]为NULL
+ */
 void delete_partition(struct gendisk *disk, int partno)
 {
 	struct disk_part_tbl *ptbl =
@@ -309,12 +330,22 @@ static DEVICE_ATTR(whole_disk, 0444, whole_disk_show, NULL);
  * Must be called either with bd_mutex held, before a disk can be opened or
  * after all disk users are gone.
  */
+/*
+ * called by:
+ *   - block/ioctl.c|69| <<blkpg_ioctl>> part = add_partition(disk, partno, start, length,
+ *   - block/partition-generic.c|520| <<blk_add_partition>> part = add_partition(disk, p, from, size, state->parts[p].flags,
+ *
+ * 核心思想就是分配hd_struct, 设置其start_sect, nr_sects和partno等,
+ * 分配devt和调用device_add()
+ * 然如放入gendisk->parttlb->part[partno]
+ */
 struct hd_struct *add_partition(struct gendisk *disk, int partno,
 				sector_t start, sector_t len, int flags,
 				struct partition_meta_info *info)
 {
 	struct hd_struct *p;
 	dev_t devt = MKDEV(0, 0);
+	/* (&(disk)->part0.__dev) */
 	struct device *ddev = disk_to_dev(disk);
 	struct device *pdev;
 	struct disk_part_tbl *ptbl;
@@ -339,6 +370,10 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 		break;
 	}
 
+	/*
+	 * 核心思想就是gendisk->parttlb->part[partno]数组太短
+	 * 需要扩展以
+	 */
 	err = disk_expand_part_tbl(disk, partno);
 	if (err)
 		return ERR_PTR(err);
@@ -347,6 +382,7 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 	if (ptbl->part[partno])
 		return ERR_PTR(-EBUSY);
 
+	/* p是struct hd_struct */
 	p = kzalloc(sizeof(*p), GFP_KERNEL);
 	if (!p)
 		return ERR_PTR(-EBUSY);
@@ -357,6 +393,11 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 	}
 
 	seqcount_init(&p->nr_sects_seq);
+	/*
+	 * struct hd_struct:
+	 *  -> struct device __dev;
+	 * 返回hd_struct->__dev
+	 */
 	pdev = part_to_dev(p);
 
 	p->start_sect = start;
@@ -420,6 +461,7 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 	}
 
 	/* everything is up and running, commence */
+	/* 这一行是非常核心的 !!! */
 	rcu_assign_pointer(ptbl->part[partno], p);
 
 	/* suppress uevent if the disk suppresses it */
@@ -460,6 +502,10 @@ static bool disk_unlock_native_capacity(struct gendisk *disk)
 	}
 }
 
+/*
+ * called by:
+ *   - fs/block_dev.c|1528| <<bdev_disk_changed>> ret = blk_drop_partitions(disk, bdev);
+ */
 int blk_drop_partitions(struct gendisk *disk, struct block_device *bdev)
 {
 	struct disk_part_iter piter;
@@ -482,6 +528,14 @@ int blk_drop_partitions(struct gendisk *disk, struct block_device *bdev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/partition-generic.c|600| <<blk_add_partitions>> if (!blk_add_partition(disk, bdev, state, p))
+ *
+ * 核心思想就是分配hd_struct, 设置其start_sect, nr_sects和partno等,
+ * 分配devt和调用device_add()
+ * 然如放入gendisk->parttlb->part[partno]
+ */
 static bool blk_add_partition(struct gendisk *disk, struct block_device *bdev,
 		struct parsed_partitions *state, int p)
 {
@@ -517,6 +571,11 @@ static bool blk_add_partition(struct gendisk *disk, struct block_device *bdev,
 		size = get_capacity(disk) - from;
 	}
 
+	/*
+	 * 核心思想就是分配hd_struct, 设置其start_sect, nr_sects和partno等,
+	 * 分配devt和调用device_add()
+	 * 然如放入gendisk->parttlb->part[partno]
+	 */
 	part = add_partition(disk, p, from, size, state->parts[p].flags,
 			     &state->parts[p].info);
 	if (IS_ERR(part) && PTR_ERR(part) != -ENXIO) {
@@ -532,6 +591,40 @@ static bool blk_add_partition(struct gendisk *disk, struct block_device *bdev,
 	return true;
 }
 
+/*
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - fs/block_dev.c|1531| <<bdev_disk_changed>> ret = blk_add_partitions(disk, bdev);
+ */
 int blk_add_partitions(struct gendisk *disk, struct block_device *bdev)
 {
 	struct parsed_partitions *state;
@@ -602,6 +695,11 @@ int blk_add_partitions(struct gendisk *disk, struct block_device *bdev)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/partitions/check.h|38| <<read_part_sector>> return read_dev_sector(state->bdev, n, p);
+ *   - drivers/scsi/scsicam.c|42| <<scsi_bios_ptable>> void *data = read_dev_sector(bdev, 0, &sect);
+ */
 unsigned char *read_dev_sector(struct block_device *bdev, sector_t n, Sector *p)
 {
 	struct address_space *mapping = bdev->bd_inode->i_mapping;
diff --git a/block/partitions/check.c b/block/partitions/check.c
index ffe408fead0c..148b2cd90ce0 100644
--- a/block/partitions/check.c
+++ b/block/partitions/check.c
@@ -112,6 +112,10 @@ static int (*check_part[])(struct parsed_partitions *) = {
 	NULL
 };
 
+/*
+ * 针对gendisk分配一个struct parsed_partitions
+ * 主要要分配parsed_partitions->parts[]数组,用来临时保存part的元数据
+ */
 static struct parsed_partitions *allocate_partitions(struct gendisk *hd)
 {
 	struct parsed_partitions *state;
@@ -122,6 +126,16 @@ static struct parsed_partitions *allocate_partitions(struct gendisk *hd)
 		return NULL;
 
 	nr = disk_max_parts(hd);
+	/*
+	 * struct parsed_partitions:
+	 *  -> struct {
+	 *      -> sector_t from;
+	 *      -> sector_t size;
+	 *      -> int flags;
+	 *      -> bool has_info;
+	 *      -> struct partition_meta_info info;
+	 *     } *parts;
+	 */
 	state->parts = vzalloc(array_size(nr, sizeof(state->parts[0])));
 	if (!state->parts) {
 		kfree(state);
@@ -139,12 +153,77 @@ void free_partitions(struct parsed_partitions *state)
 	kfree(state);
 }
 
+/*
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] loop_reread_partitions
+ * [0] loop_set_status
+ * [0] loop_set_status64
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - block/partition-generic.c|584| <<blk_add_partitions>> state = check_partition(disk, bdev);
+ */
 struct parsed_partitions *
 check_partition(struct gendisk *hd, struct block_device *bdev)
 {
 	struct parsed_partitions *state;
 	int i, res, err;
 
+	/*
+	 * 针对gendisk分配一个struct parsed_partitions
+	 * 主要要分配parsed_partitions->parts[]数组,用来临时保存part的元数据
+	 */
 	state = allocate_partitions(hd);
 	if (!state)
 		return NULL;
@@ -164,6 +243,26 @@ check_partition(struct gendisk *hd, struct block_device *bdev)
 	i = res = err = 0;
 	while (!res && check_part[i]) {
 		memset(state->parts, 0, state->limit * sizeof(state->parts[0]));
+		/*
+		 * struct parsed_partitions {
+		 *     struct block_device *bdev;
+		 *     char name[BDEVNAME_SIZE];
+		 *     struct {
+		 *         sector_t from;
+		 *         sector_t size;
+		 *         int flags;
+		 *         bool has_info;
+		 *         struct partition_meta_info info;
+		 *     } *parts;
+		 *     int next;
+		 *     int limit;
+		 *     bool access_beyond_eod;
+		 *     char *pp_buf;
+		 * };
+		 *
+		 * 用virtio-blk在ubuntu的启动盘时, 是adfspart_check_POWERTEC()
+		 * 其他的也是这个, 只是其他的可能没有part, 所以res不是>0
+		 */
 		res = check_part[i++](state);
 		if (res < 0) {
 			/* We have hit an I/O error which we don't report now.
diff --git a/drivers/block/loop.c b/drivers/block/loop.c
index 739b372a5112..423bba5bcbc0 100644
--- a/drivers/block/loop.c
+++ b/drivers/block/loop.c
@@ -167,6 +167,12 @@ static loff_t get_loop_size(struct loop_device *lo, struct file *file)
 	return get_size(lo->lo_offset, lo->lo_sizelimit, file);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|634| <<loop_update_dio>> __loop_update_dio(lo, io_is_direct(lo->lo_backing_file) |
+ *   - drivers/block/loop.c|1356| <<loop_set_status>> __loop_update_dio(lo, lo->use_dio);
+ *   - drivers/block/loop.c|1539| <<loop_set_dio>> __loop_update_dio(lo, !!arg);
+ */
 static void __loop_update_dio(struct loop_device *lo, bool dio)
 {
 	struct file *file = lo->lo_backing_file;
@@ -453,6 +459,9 @@ static int lo_req_flush(struct loop_device *lo, struct request *rq)
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops loop_mq_ops.complete = lo_complete_rq()
+ */
 static void lo_complete_rq(struct request *rq)
 {
 	struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
@@ -599,6 +608,19 @@ static int do_req_filebacked(struct loop_device *lo, struct request *rq)
 	case REQ_OP_FLUSH:
 		return lo_req_flush(lo, rq);
 	case REQ_OP_WRITE_ZEROES:
+		/*
+		 * FALLOC_FL_ZERO_RANGE的注释:
+		 * FALLOC_FL_ZERO_RANGE is used to convert a range of file to zeros preferably
+		 * without issuing data IO. Blocks should be preallocated for the regions that
+		 * span holes in the file, and the entire range is preferable converted to
+		 * unwritten extents - even though file system may choose to zero out the
+		 * extent or do whatever which will result in reading zeros from the range
+		 * while the range remains allocated for the file.
+		 *
+		 * This can be also used to preallocate blocks past EOF in the same way as
+		 * with fallocate. Flag FALLOC_FL_KEEP_SIZE should cause the inode
+		 * size to remain the same
+		 */
 		/*
 		 * If the caller doesn't want deallocation, call zeroout to
 		 * write zeroes the range.  Otherwise, punch them out.
@@ -635,6 +657,21 @@ static inline void loop_update_dio(struct loop_device *lo)
 			lo->use_dio);
 }
 
+/*
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] loop_reread_partitions
+ * [0] loop_set_status
+ * [0] loop_set_status64
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void loop_reread_partitions(struct loop_device *lo,
 				   struct block_device *bdev)
 {
@@ -1205,6 +1242,10 @@ static int __loop_clr_fd(struct loop_device *lo, bool release)
 	return err;
 }
 
+/*
+ * called by (处理LOOP_CLR_FD):
+ *   - drivers/block/loop.c|1621| <<lo_ioctl>> return loop_clr_fd(lo);
+ */
 static int loop_clr_fd(struct loop_device *lo)
 {
 	int err;
@@ -1237,6 +1278,12 @@ static int loop_clr_fd(struct loop_device *lo)
 	return __loop_clr_fd(lo, false);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|1491| <<loop_set_status_old>> return loop_set_status(lo, &info64);
+ *   - drivers/block/loop.c|1501| <<loop_set_status64>> return loop_set_status(lo, &info64);
+ *   - drivers/block/loop.c|1771| <<loop_set_status_compat>> return loop_set_status(lo, &info64);
+ */
 static int
 loop_set_status(struct loop_device *lo, const struct loop_info64 *info)
 {
@@ -1515,6 +1562,10 @@ static int loop_set_capacity(struct loop_device *lo)
 	return figure_loop_size(lo, lo->lo_offset, lo->lo_sizelimit);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|1597| <<lo_simple_ioctl>> err = loop_set_dio(lo, arg);
+ */
 static int loop_set_dio(struct loop_device *lo, unsigned long arg)
 {
 	int error = -ENXIO;
@@ -1591,6 +1642,9 @@ static int lo_simple_ioctl(struct loop_device *lo, unsigned int cmd,
 	return err;
 }
 
+/*
+ * struct block_device_operations lo_fops.ioctl = lo_ioctl()
+ */
 static int lo_ioctl(struct block_device *bdev, fmode_t mode,
 	unsigned int cmd, unsigned long arg)
 {
@@ -1754,6 +1808,9 @@ loop_get_status_compat(struct loop_device *lo,
 	return err;
 }
 
+/*
+ * struct block_device_operations lo_fops.compat_ioctl = lo_compat_ioctl()
+ */
 static int lo_compat_ioctl(struct block_device *bdev, fmode_t mode,
 			   unsigned int cmd, unsigned long arg)
 {
@@ -1789,6 +1846,9 @@ static int lo_compat_ioctl(struct block_device *bdev, fmode_t mode,
 }
 #endif
 
+/*
+ * struct block_device_operations lo_fops.open = lo_open()
+ */
 static int lo_open(struct block_device *bdev, fmode_t mode)
 {
 	struct loop_device *lo;
@@ -1809,6 +1869,9 @@ static int lo_open(struct block_device *bdev, fmode_t mode)
 	return err;
 }
 
+/*
+ * struct block_device_operations lo_fops.release = lo_release()
+ */
 static void lo_release(struct gendisk *disk, fmode_t mode)
 {
 	struct loop_device *lo;
@@ -1842,6 +1905,10 @@ static void lo_release(struct gendisk *disk, fmode_t mode)
 	mutex_unlock(&loop_ctl_mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2085| <<loop_add>> disk->fops = &lo_fops;
+ */
 static const struct block_device_operations lo_fops = {
 	.owner =	THIS_MODULE,
 	.open =		lo_open,
@@ -1863,6 +1930,10 @@ MODULE_PARM_DESC(max_part, "Maximum number of partitions per loop device");
 MODULE_LICENSE("GPL");
 MODULE_ALIAS_BLOCKDEV_MAJOR(LOOP_MAJOR);
 
+/*
+ * called by:
+ *   - drivers/block/cryptoloop.c|188| <<init_cryptoloop>> int rc = loop_register_transfer(&cryptoloop_funcs);
+ */
 int loop_register_transfer(struct loop_func_table *funcs)
 {
 	unsigned int n = funcs->number;
@@ -1901,6 +1972,9 @@ int loop_unregister_transfer(int number)
 EXPORT_SYMBOL(loop_register_transfer);
 EXPORT_SYMBOL(loop_unregister_transfer);
 
+/*
+ * struct blk_mq_ops loop_mq_ops.queue_rq = loop_queue_rq()
+ */
 static blk_status_t loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 		const struct blk_mq_queue_data *bd)
 {
@@ -1937,6 +2011,10 @@ static blk_status_t loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2011| <<loop_queue_work>> loop_handle_cmd(cmd);
+ */
 static void loop_handle_cmd(struct loop_cmd *cmd)
 {
 	struct request *rq = blk_mq_rq_from_pdu(cmd);
@@ -1958,6 +2036,10 @@ static void loop_handle_cmd(struct loop_cmd *cmd)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2019| <<loop_init_request>> kthread_init_work(&cmd->work, loop_queue_work);
+ */
 static void loop_queue_work(struct kthread_work *work)
 {
 	struct loop_cmd *cmd =
@@ -1966,6 +2048,9 @@ static void loop_queue_work(struct kthread_work *work)
 	loop_handle_cmd(cmd);
 }
 
+/*
+ * struct blk_mq_ops loop_mq_ops.init_request = loop_init_request()
+ */
 static int loop_init_request(struct blk_mq_tag_set *set, struct request *rq,
 		unsigned int hctx_idx, unsigned int numa_node)
 {
@@ -1975,12 +2060,23 @@ static int loop_init_request(struct blk_mq_tag_set *set, struct request *rq,
 	return 0;
 }
 
+/*
+ * used by:
+ *   - drivers/block/loop.c|2025| <<loop_add>> lo->tag_set.ops = &loop_mq_ops;
+ */
 static const struct blk_mq_ops loop_mq_ops = {
 	.queue_rq       = loop_queue_rq,
 	.init_request	= loop_init_request,
 	.complete	= lo_complete_rq,
 };
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2161| <<loop_probe>> err = loop_add(&lo, MINOR(dev) >> part_shift);
+ *   - drivers/block/loop.c|2190| <<loop_control_ioctl>> ret = loop_add(&lo, parm);
+ *   - drivers/block/loop.c|2212| <<loop_control_ioctl>> ret = loop_add(&lo, -1);
+ *   - drivers/block/loop.c|2300| <<loop_init>> loop_add(&lo, i);
+ */
 static int loop_add(struct loop_device **l, int i)
 {
 	struct loop_device *lo;
@@ -2154,6 +2250,10 @@ static struct kobject *loop_probe(dev_t dev, int *part, void *data)
 	return kobj;
 }
 
+/*
+ * struct file_operations loop_ctl_fops.unlocked_ioctl = loop_control_ioctl()
+ * struct file_operations loop_ctl_fops.compat_ioctl = loop_control_ioctl()
+ */
 static long loop_control_ioctl(struct file *file, unsigned int cmd,
 			       unsigned long parm)
 {
diff --git a/drivers/block/loop.h b/drivers/block/loop.h
index af75a5ee4094..d68889eac7ea 100644
--- a/drivers/block/loop.h
+++ b/drivers/block/loop.h
@@ -20,6 +20,21 @@
 /* Possible states of device */
 enum {
 	Lo_unbound,
+	/*
+	 * 使用Lo_bound的地方:
+	 *   - drivers/block/loop.c|692| <<loop_validate_file>> if (l->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|721| <<loop_change_fd>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1051| <<loop_set_fd>> lo->lo_state = Lo_bound;
+	 *   - drivers/block/loop.c|1240| <<loop_clr_fd>> if (lo->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|1283| <<loop_set_status>> if (lo->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|1396| <<loop_get_status>> if (lo->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|1537| <<loop_set_capacity>> if (unlikely(lo->lo_state != Lo_bound))
+	 *   - drivers/block/loop.c|1550| <<loop_set_dio>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1565| <<loop_set_block_size>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1863| <<lo_release>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1873| <<lo_release>> } else if (lo->lo_state == Lo_bound) {
+	 *   - drivers/block/loop.c|1958| <<loop_queue_rq>> if (lo->lo_state != Lo_bound)
+	 */
 	Lo_bound,
 	Lo_rundown,
 };
diff --git a/drivers/block/null_blk.h b/drivers/block/null_blk.h
index bc837862b767..3b859f1975ef 100644
--- a/drivers/block/null_blk.h
+++ b/drivers/block/null_blk.h
@@ -14,24 +14,61 @@
 #include <linux/fault-inject.h>
 
 struct nullb_cmd {
+	/*
+	 * list没人用, 可以删了??
+	 *   - drivers/block/null_blk_main.c|2440| <<setup_commands>> INIT_LIST_HEAD(&cmd->list);
+	 */
 	struct list_head list;
+	/*
+	 * ll_list没人用, 可以删了??
+	 *   - drivers/block/null_blk_main.c|2441| <<setup_commands>> cmd->ll_list.next = NULL;
+	 */
 	struct llist_node ll_list;
+	/*
+	 * csd没人用, 可以删除了
+	 */
 	struct __call_single_data csd;
 	struct request *rq;
 	struct bio *bio;
 	unsigned int tag;
 	blk_status_t error;
 	struct nullb_queue *nq;
+	/*
+	 * 表示complete req的方式, 比方不用softirq, 而用一个timer触发
+	 */
 	struct hrtimer timer;
 };
 
 struct nullb_queue {
+	/*
+	 * 在以下使用tag_map:
+	 *   - drivers/block/null_blk_main.c|922| <<put_tag>> clear_bit_unlock(tag, nq->tag_map);
+	 *   - drivers/block/null_blk_main.c|946| <<get_tag>> tag = find_first_zero_bit(nq->tag_map, nq->queue_depth);
+	 *   - drivers/block/null_blk_main.c|949| <<get_tag>> } while (test_and_set_bit_lock(tag, nq->tag_map));
+	 *   - drivers/block/null_blk_main.c|2292| <<cleanup_queue>> kfree(nq->tag_map);
+	 *   - drivers/block/null_blk_main.c|2420| <<setup_commands>> nq->tag_map = kcalloc(tag_size, sizeof(unsigned long ), GFP_KERNEL);
+	 *   - drivers/block/null_blk_main.c|2421| <<setup_commands>> if (!nq->tag_map) {
+	 */
 	unsigned long *tag_map;
 	wait_queue_head_t wait;
 	unsigned int queue_depth;
 	struct nullb_device *dev;
+	/*
+	 * 在以下使用requeue_selection:
+	 *   - drivers/block/null_blk_main.c|2258| <<null_queue_rq>> nq->requeue_selection++;
+	 *   - drivers/block/null_blk_main.c|2259| <<null_queue_rq>> if (nq->requeue_selection & 1)
+	 */
 	unsigned int requeue_selection;
 
+	/*
+	 * 在以下使用cmds:
+	 *   - drivers/block/null_blk_main.c|1003| <<__alloc_cmd>> cmd = &nq->cmds[tag];
+	 *   - drivers/block/null_blk_main.c|2305| <<cleanup_queue>> kfree(nq->cmds);
+	 *   - drivers/block/null_blk_main.c|2427| <<setup_commands>> nq->cmds = kcalloc(nq->queue_depth, sizeof(*cmd), GFP_KERNEL);
+	 *   - drivers/block/null_blk_main.c|2428| <<setup_commands>> if (!nq->cmds)
+	 *   - drivers/block/null_blk_main.c|2434| <<setup_commands>> kfree(nq->cmds);
+	 *   - drivers/block/null_blk_main.c|2439| <<setup_commands>> cmd = &nq->cmds[i];
+	 */
 	struct nullb_cmd *cmds;
 };
 
@@ -41,14 +78,40 @@ struct nullb_device {
 	struct radix_tree_root data; /* data stored in the disk */
 	struct radix_tree_root cache; /* disk cache data */
 	unsigned long flags; /* device flags */
+	/*
+	 * 使用curr_cache的地方:
+	 *   - drivers/block/null_blk_main.c|1282| <<null_free_sector>> nullb->dev->curr_cache -= PAGE_SIZE;
+	 *   - drivers/block/null_blk_main.c|1316| <<null_radix_tree_insert>> nullb->dev->curr_cache += PAGE_SIZE;
+	 *   - drivers/block/null_blk_main.c|1361| <<null_free_device_storage>> dev->curr_cache = 0;
+	 *   - drivers/block/null_blk_main.c|1530| <<null_flush_cache_page>> nullb->dev->curr_cache -= PAGE_SIZE;
+	 *   - drivers/block/null_blk_main.c|1550| <<null_make_cache_space>> nullb->dev->curr_cache + n || nullb->dev->curr_cache == 0)
+	 *   - drivers/block/null_blk_main.c|1779| <<null_handle_flush>> if (err || nullb->dev->curr_cache == 0)
+	 */
 	unsigned int curr_cache;
 	struct badblocks badblocks;
 
 	unsigned int nr_zones;
 	struct blk_zone *zones;
+	/*
+	 * 在以下使用zone_size_sects:
+	 *   - drivers/block/null_blk_main.c|1946| <<null_gendisk_register>> nullb->dev->zone_size_sects);
+	 *   - drivers/block/null_blk_zoned.c|21| <<null_zone_no>> return sect >> ilog2(dev->zone_size_sects);
+	 *   - drivers/block/null_blk_zoned.c|39| <<null_zone_init>> dev->zone_size_sects = dev->zone_size << ZONE_SIZE_SHIFT;
+	 *   - drivers/block/null_blk_zoned.c|41| <<null_zone_init>> (SECTOR_SHIFT + ilog2(dev->zone_size_sects));
+	 *   - drivers/block/null_blk_zoned.c|57| <<null_zone_init>> zone->len = dev->zone_size_sects;
+	 *   - drivers/block/null_blk_zoned.c|62| <<null_zone_init>> sector += dev->zone_size_sects;
+	 *   - drivers/block/null_blk_zoned.c|69| <<null_zone_init>> zone->len = dev->zone_size_sects;
+	 *   - drivers/block/null_blk_zoned.c|73| <<null_zone_init>> sector += dev->zone_size_sects;
+	 */
 	sector_t zone_size_sects;
 
 	unsigned long size; /* device size in MB */
+	/*
+	 * 在以下使用completion_nsec:
+	 *   - drivers/block/null_blk_main.c|452| <<global>> NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
+	 *   - drivers/block/null_blk_main.c|715| <<null_alloc_dev>> dev->completion_nsec = g_completion_nsec;
+	 *   - drivers/block/null_blk_main.c|930| <<null_cmd_end_timer>> ktime_t kt = cmd->nq->dev->completion_nsec;
+	 */
 	unsigned long completion_nsec; /* time in ns to complete a request */
 	unsigned long cache_size; /* disk cache size in MB */
 	unsigned long zone_size; /* zone size in MB if device is zoned */
@@ -62,8 +125,19 @@ struct nullb_device {
 	unsigned int index; /* index of the disk, only valid with a disk */
 	unsigned int mbps; /* Bandwidth throttle cap (in MB/s) */
 	bool blocking; /* blocking blk-mq device */
+	/*
+	 * 在以下使用null_device->use_per_mpde_hctx:
+	 *   - drivers/block/null_blk_main.c|671| <<null_alloc_dev>> dev->use_per_node_hctx = g_use_per_node_hctx;
+	 *   - drivers/block/null_blk_main.c|2083| <<null_validate_conf>> if (dev->queue_mode == NULL_Q_MQ && dev->use_per_node_hctx) {
+	 */
 	bool use_per_node_hctx; /* use per-node allocation for hardware context */
 	bool power; /* power on/off the device */
+	/*
+	 * 在以下使用memory_backed:
+	 *   - drivers/block/null_blk_main.c|338| <<global>> NULLB_DEVICE_ATTR(memory_backed, bool, NULL);
+	 *   - drivers/block/null_blk_main.c|1554| <<null_handle_cmd>> if (dev->memory_backed)
+	 *   - drivers/block/null_blk_main.c|2023| <<null_validate_conf>> if (dev->memory_backed)
+	 */
 	bool memory_backed; /* if data is stored in memory */
 	bool discard; /* if support discard */
 	bool zoned; /* if device is zoned */
@@ -71,6 +145,7 @@ struct nullb_device {
 
 struct nullb {
 	struct nullb_device *dev;
+	/* 把这个设置加入到nullb_list */
 	struct list_head list;
 	unsigned int index;
 	struct request_queue *q;
@@ -78,8 +153,34 @@ struct nullb {
 	struct blk_mq_tag_set *tag_set;
 	struct blk_mq_tag_set __tag_set;
 	unsigned int queue_depth;
+	/*
+	 * 使用cur_bytes的地方:
+	 *   - drivers/block/null_blk_main.c|1964| <<null_handle_throttled>> if (atomic_long_sub_return(blk_rq_bytes(rq), &nullb->cur_bytes) < 0) {
+	 *   - drivers/block/null_blk_main.c|1967| <<null_handle_throttled>> if (atomic_long_read(&nullb->cur_bytes) > 0)
+	 *   - drivers/block/null_blk_main.c|2119| <<nullb_bwtimer_fn>> if (atomic_long_read(&nullb->cur_bytes) == mb_per_tick(mbps))
+	 *   - drivers/block/null_blk_main.c|2122| <<nullb_bwtimer_fn>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(mbps));
+	 *   - drivers/block/null_blk_main.c|2140| <<nullb_setup_bwtimer>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(nullb->dev->mbps));
+	 *   - drivers/block/null_blk_main.c|2330| <<null_del_dev>> atomic_long_set(&nullb->cur_bytes, LONG_MAX);
+	 */
 	atomic_long_t cur_bytes;
+	/*
+	 * bw_timer在以下使用:
+	 *   - drivers/block/null_blk_main.c|1961| <<null_handle_throttled>> if (!hrtimer_active(&nullb->bw_timer))
+	 *   - drivers/block/null_blk_main.c|1962| <<null_handle_throttled>> hrtimer_restart(&nullb->bw_timer);
+	 *   - drivers/block/null_blk_main.c|2115| <<nullb_bwtimer_fn>> struct nullb *nullb = container_of(timer, struct nullb, bw_timer);
+	 *   - drivers/block/null_blk_main.c|2125| <<nullb_bwtimer_fn>> hrtimer_forward_now(&nullb->bw_timer, timer_interval);
+	 *   - drivers/block/null_blk_main.c|2138| <<nullb_setup_bwtimer>> hrtimer_init(&nullb->bw_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	 *   - drivers/block/null_blk_main.c|2139| <<nullb_setup_bwtimer>> nullb->bw_timer.function = nullb_bwtimer_fn;
+	 *   - drivers/block/null_blk_main.c|2141| <<nullb_setup_bwtimer>> hrtimer_start(&nullb->bw_timer, timer_interval, HRTIMER_MODE_REL);
+	 *   - drivers/block/null_blk_main.c|2329| <<null_del_dev>> hrtimer_cancel(&nullb->bw_timer);
+	 */
 	struct hrtimer bw_timer;
+	/*
+	 * 在以下使用cache_flush_pos:
+	 *   - drivers/block/null_blk_main.c|1551| <<null_make_cache_space>> (void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
+	 *   - drivers/block/null_blk_main.c|1557| <<null_make_cache_space>> nullb->cache_flush_pos = c_pages[i]->page->index;
+	 *   - drivers/block/null_blk_main.c|1581| <<null_make_cache_space>> nullb->cache_flush_pos = 0;
+	 */
 	unsigned long cache_flush_pos;
 	spinlock_t lock;
 
diff --git a/drivers/block/null_blk_main.c b/drivers/block/null_blk_main.c
index ae8d4bc532b0..c4cc593d8253 100644
--- a/drivers/block/null_blk_main.c
+++ b/drivers/block/null_blk_main.c
@@ -11,13 +11,34 @@
 #include <linux/init.h>
 #include "null_blk.h"
 
+/*
+ * null_transfer()是比较核心的函数
+ */
+
+/* 这个shift表示一个page里sector的数量 */
 #define PAGE_SECTORS_SHIFT	(PAGE_SHIFT - SECTOR_SHIFT)
+/* 这个表示一个page里sector的数量 */
 #define PAGE_SECTORS		(1 << PAGE_SECTORS_SHIFT)
 #define SECTOR_MASK		(PAGE_SECTORS - 1)
 
+/*
+ * 在以下使用FREE_BATCH:
+ *   - drivers/block/null_blk_main.c|1014| <<null_free_device_storage>> struct nullb_page *ret, *t_pages[FREE_BATCH];
+ *   - drivers/block/null_blk_main.c|1023| <<null_free_device_storage>> (void **)t_pages, pos, FREE_BATCH);
+ *   - drivers/block/null_blk_main.c|1033| <<null_free_device_storage>> } while (nr_pages == FREE_BATCH);
+ *   - drivers/block/null_blk_main.c|1184| <<null_make_cache_space>> struct nullb_page *c_pages[FREE_BATCH];
+ *   - drivers/block/null_blk_main.c|1193| <<null_make_cache_space>> (void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
+ */
 #define FREE_BATCH		16
 
 #define TICKS_PER_SEC		50ULL
+/*
+ * 在以下使用TIMER_INTERVAL:
+ *   - drivers/block/null_blk_main.c|1664| <<nullb_bwtimer_fn>> ktime_t timer_interval = ktime_set(0, TIMER_INTERVAL);
+ *   - drivers/block/null_blk_main.c|1684| <<nullb_setup_bwtimer>> ktime_t timer_interval = ktime_set(0, TIMER_INTERVAL);
+ *
+ * 每个tick的nsec
+ */
 #define TIMER_INTERVAL		(NSEC_PER_SEC / TICKS_PER_SEC)
 
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -25,8 +46,17 @@ static DECLARE_FAULT_ATTR(null_timeout_attr);
 static DECLARE_FAULT_ATTR(null_requeue_attr);
 #endif
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1605| <<nullb_bwtimer_fn>> if (atomic_long_read(&nullb->cur_bytes) == mb_per_tick(mbps))
+ *   - drivers/block/null_blk_main.c|1608| <<nullb_bwtimer_fn>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(mbps));
+ *   - drivers/block/null_blk_main.c|1626| <<nullb_setup_bwtimer>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(nullb->dev->mbps));
+ */
 static inline u64 mb_per_tick(int mbps)
 {
+	/*
+	 * 1 << 20 相当于1MB
+	 */
 	return (1 << 20) / TICKS_PER_SEC * ((u64) mbps);
 }
 
@@ -39,12 +69,43 @@ static inline u64 mb_per_tick(int mbps)
  * CACHE:	Device is using a write-back cache.
  */
 enum nullb_device_flags {
+	/*
+	 * 在以下使用NULLB_DEV_FL_CONFIGURED:
+	 *   - drivers/block/null_blk_main.c|304| <<NULLB_DEVICE_ATTR>> else if (test_bit(NULLB_DEV_FL_CONFIGURED, &dev->flags)) \
+	 *   - drivers/block/null_blk_main.c|376| <<nullb_device_power_store>> set_bit(NULLB_DEV_FL_CONFIGURED, &dev->flags);
+	 *   - drivers/block/null_blk_main.c|385| <<nullb_device_power_store>> clear_bit(NULLB_DEV_FL_CONFIGURED, &dev->flags);
+	 */
 	NULLB_DEV_FL_CONFIGURED	= 0,
+	/*
+	 * 在以下使用NULLB_DEV_FL_UP:
+	 *   - drivers/block/null_blk_main.c|369| <<nullb_device_power_store>> if (test_and_set_bit(NULLB_DEV_FL_UP, &dev->flags))
+	 *   - drivers/block/null_blk_main.c|372| <<nullb_device_power_store>> clear_bit(NULLB_DEV_FL_UP, &dev->flags);
+	 *   - drivers/block/null_blk_main.c|379| <<nullb_device_power_store>> if (test_and_clear_bit(NULLB_DEV_FL_UP, &dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|513| <<nullb_group_drop_item>> if (test_and_clear_bit(NULLB_DEV_FL_UP, &dev->flags)) {
+	 */
 	NULLB_DEV_FL_UP		= 1,
+	/*
+	 * 在以下使用NULLB_DEV_FL_THROTTLED:
+	 *   - drivers/block/null_blk_main.c|1543| <<null_handle_cmd>> if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|1772| <<null_del_dev>> if (test_bit(NULLB_DEV_FL_THROTTLED, &nullb->dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|2156| <<null_add_dev>> set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
+	 */
 	NULLB_DEV_FL_THROTTLED	= 2,
+	/*
+	 * 在以下使用NULLB_DEV_FL_CACHE:
+	 *   - drivers/block/null_blk_main.c|574| <<null_cache_active>> return test_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 *   - drivers/block/null_blk_main.c|2161| <<null_add_dev>> set_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 */
 	NULLB_DEV_FL_CACHE	= 3,
 };
 
+/*
+ * 一个page中sector的数量加上2
+ * The highest 2 bits of bitmap are for special purpose. LOCK means the cache
+ * page is being flushing to storage. FREE means the cache page is freed and
+ * should be skipped from flushing to storage. Please see
+ * null_make_cache_space
+ */
 #define MAP_SZ		((PAGE_SIZE >> SECTOR_SHIFT) + 2)
 /*
  * nullb_page is a page in memory for nullb devices.
@@ -62,18 +123,72 @@ struct nullb_page {
 	struct page *page;
 	DECLARE_BITMAP(bitmap, MAP_SZ);
 };
+/*
+ * 在以下使用NULLB_PAGE_LOCK:
+ *   - drivers/block/null_blk_main.c|1151| <<null_free_page>> if (test_bit(NULLB_PAGE_LOCK, t_page->bitmap))
+ *   - drivers/block/null_blk_main.c|1356| <<null_flush_cache_page>> __clear_bit(NULLB_PAGE_LOCK, c_page->bitmap);
+ *   - drivers/block/null_blk_main.c|1421| <<null_make_cache_space>> if (test_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap))
+ *   - drivers/block/null_blk_main.c|1424| <<null_make_cache_space>> __set_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap);
+ */
 #define NULLB_PAGE_LOCK (MAP_SZ - 1)
+/*
+ * 在以下使用NULLB_PAGE_FREE:
+ *   - drivers/block/null_blk_main.c|1150| <<null_free_page>> __set_bit(NULLB_PAGE_FREE, t_page->bitmap);
+ *   - drivers/block/null_blk_main.c|1357| <<null_flush_cache_page>> if (test_bit(NULLB_PAGE_FREE, c_page->bitmap)) {
+ */
 #define NULLB_PAGE_FREE (MAP_SZ - 2)
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1786| <<null_add_dev>> list_add_tail(&nullb->list, &nullb_list);
+ *   - drivers/block/null_blk_main.c|1889| <<null_init>> while (!list_empty(&nullb_list)) {
+ *   - drivers/block/null_blk_main.c|1890| <<null_init>> nullb = list_entry(nullb_list.next, struct nullb, list);
+ *   - drivers/block/null_blk_main.c|1913| <<null_exit>> while (!list_empty(&nullb_list)) {
+ *   - drivers/block/null_blk_main.c|1916| <<null_exit>> nullb = list_entry(nullb_list.next, struct nullb, list);
+ *
+ * 添加struct nullb
+ */
 static LIST_HEAD(nullb_list);
 static struct mutex lock;
+/*
+ * 使用null_major的地方:
+ *   - drivers/block/null_blk_main.c|1918| <<null_gendisk_register>> disk->major = null_major;
+ *   - drivers/block/null_blk_main.c|2252| <<null_init>> null_major = register_blkdev(0, "nullb");
+ *   - drivers/block/null_blk_main.c|2253| <<null_init>> if (null_major < 0) {
+ *   - drivers/block/null_blk_main.c|2254| <<null_init>> ret = null_major;
+ *   - drivers/block/null_blk_main.c|2281| <<null_init>> unregister_blkdev(null_major, "nullb");
+ *   - drivers/block/null_blk_main.c|2296| <<null_exit>> unregister_blkdev(null_major, "nullb");
+ */
 static int null_major;
+/*
+ * 在以下使用nullb_indexes:
+ *   - drivers/block/null_blk_main.c|1790| <<null_del_dev>> ida_simple_remove(&nullb_indexes, nullb->index);
+ *   - drivers/block/null_blk_main.c|2205| <<null_add_dev>> nullb->index = ida_simple_get(&nullb_indexes, 0, 0, GFP_KERNEL);
+ */
 static DEFINE_IDA(nullb_indexes);
 static struct blk_mq_tag_set tag_set;
 
 enum {
+	/*
+	 * 使用NULL_IRQ_NONE的地方:
+	 *   - drivers/block/null_blk_main.c|297| <<null_set_irqmode>> return null_param_store_val(str, &g_irqmode, NULL_IRQ_NONE,
+	 *   - drivers/block/null_blk_main.c|1932| <<nullb_complete_cmd>> case NULL_IRQ_NONE:
+	 */
 	NULL_IRQ_NONE		= 0,
+	/*
+	 * 使用NULL_IRQ_SOFTIRQ的地方:
+	 *   - drivers/block/null_blk_main.c|293| <<global>> static int g_irqmode = NULL_IRQ_SOFTIRQ;
+	 *   - drivers/block/null_blk_main.c|1914| <<nullb_complete_cmd>> case NULL_IRQ_SOFTIRQ:
+	 */
 	NULL_IRQ_SOFTIRQ	= 1,
+	/*
+	 * 使用NULL_IRQ_TIMER的地方:
+	 *   - drivers/block/null_blk_main.c|298| <<null_set_irqmode>> NULL_IRQ_TIMER);
+	 *   - drivers/block/null_blk_main.c|982| <<__alloc_cmd>> if (nq->dev->irqmode == NULL_IRQ_TIMER) {
+	 *   - drivers/block/null_blk_main.c|1940| <<nullb_complete_cmd>> case NULL_IRQ_TIMER:
+	 *   - drivers/block/null_blk_main.c|2117| <<null_queue_rq>> if (nq->dev->irqmode == NULL_IRQ_TIMER) {
+	 *   - drivers/block/null_blk_main.c|2470| <<null_validate_conf>> dev->irqmode = min_t(unsigned int , dev->irqmode, NULL_IRQ_TIMER);
+	 */
 	NULL_IRQ_TIMER		= 2,
 };
 
@@ -83,10 +198,27 @@ enum {
 	NULL_Q_MQ		= 2,
 };
 
+/*
+ * 在以下使用g_no_sched:
+ *   - drivers/block/null_blk_main.c|128| <<global>> module_param_named(no_sched, g_no_sched, int , 0444);
+ *   - drivers/block/null_blk_main.c|2001| <<null_init_tag_set>> if (g_no_sched)
+ */
 static int g_no_sched;
 module_param_named(no_sched, g_no_sched, int, 0444);
 MODULE_PARM_DESC(no_sched, "No io scheduler");
 
+/*
+ * 在一下使用g_submit_queues:
+ *   - drivers/block/null_blk_main.c|132| <<global>> module_param_named(submit_queues, g_submit_queues, int , 0444);
+ *   - drivers/block/null_blk_main.c|622| <<null_alloc_dev>> dev->submit_queues = g_submit_queues;
+ *   - drivers/block/null_blk_main.c|1995| <<null_init_tag_set>> g_submit_queues;
+ *   - drivers/block/null_blk_main.c|2270| <<null_init>> if (g_submit_queues != nr_online_nodes) {
+ *   - drivers/block/null_blk_main.c|2273| <<null_init>> g_submit_queues = nr_online_nodes;
+ *   - drivers/block/null_blk_main.c|2275| <<null_init>> } else if (g_submit_queues > nr_cpu_ids)
+ *   - drivers/block/null_blk_main.c|2276| <<null_init>> g_submit_queues = nr_cpu_ids;
+ *   - drivers/block/null_blk_main.c|2277| <<null_init>> else if (g_submit_queues <= 0)
+ *   - drivers/block/null_blk_main.c|2278| <<null_init>> g_submit_queues = 1;
+ */
 static int g_submit_queues = 1;
 module_param_named(submit_queues, g_submit_queues, int, 0444);
 MODULE_PARM_DESC(submit_queues, "Number of submission queues");
@@ -96,15 +228,43 @@ module_param_named(home_node, g_home_node, int, 0444);
 MODULE_PARM_DESC(home_node, "Home node for the device");
 
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
+/*
+ * 在以下使用g_timeout_str:
+ *   - drivers/block/null_blk_main.c|173| <<global>> module_param_string(timeout, g_timeout_str, sizeof(g_timeout_str), 0444);
+ *   - drivers/block/null_blk_main.c|1732| <<should_timeout_request>> if (g_timeout_str[0])
+ *   - drivers/block/null_blk_main.c|2161| <<null_setup_fault>> if (!__null_setup_fault(&null_timeout_attr, g_timeout_str))
+ *
+ * 设置了"null_blk.timeout=20,100,0,5", 在null_setup_fault()初始化
+ * [  202.328984] null_blk: rq 000000009163ec16 timed out
+ * [  202.328993] null_blk: rq 00000000f9644e82 timed out
+ * [  202.328995] null_blk: rq 00000000d8e5c61a timed out
+ * [  202.328997] null_blk: rq 00000000fafa43fd timed out
+ * [  202.328999] null_blk: rq 00000000f0eb258c timed out
+ * 查看Documentation/fault-injection/fault-injection.rst
+ */
 static char g_timeout_str[80];
 module_param_string(timeout, g_timeout_str, sizeof(g_timeout_str), 0444);
 
+/*
+ * 在以下使用g_requeue_str:
+ *   - drivers/block/null_blk_main.c|176| <<global>> module_param_string(requeue, g_requeue_str, sizeof(g_requeue_str), 0444);
+ *   - drivers/block/null_blk_main.c|1745| <<should_requeue_request>> if (g_requeue_str[0])
+ *   - drivers/block/null_blk_main.c|2163| <<null_setup_fault>> if (!__null_setup_fault(&null_requeue_attr, g_requeue_str))
+ *
+ * 设置了"null_blk.requeue=20,100,0,5", 在null_setup_fault()初始化
+ * 查看Documentation/fault-injection/fault-injection.rst
+ */
 static char g_requeue_str[80];
 module_param_string(requeue, g_requeue_str, sizeof(g_requeue_str), 0444);
 #endif
 
 static int g_queue_mode = NULL_Q_MQ;
 
+/*
+ * 在以下使用null_param_store_val:
+ *   - drivers/block/null_blk_main.c|235| <<null_set_queue_mode>> return null_param_store_val(str, &g_queue_mode, NULL_Q_BIO, NULL_Q_MQ);
+ *   - drivers/block/null_blk_main.c|280| <<null_set_irqmode>> return null_param_store_val(str, &g_irqmode, NULL_IRQ_NONE,
+ */
 static int null_param_store_val(const char *str, int *val, int min, int max)
 {
 	int ret, new_val;
@@ -125,6 +285,10 @@ static int null_set_queue_mode(const char *str, const struct kernel_param *kp)
 	return null_param_store_val(str, &g_queue_mode, NULL_Q_BIO, NULL_Q_MQ);
 }
 
+/*
+ * 仅在以下使用null_queue_mode_param_ops:
+ *   - drivers/block/null_blk_main.c|206| <<global>> device_param_cb(queue_mode, &null_queue_mode_param_ops, &g_queue_mode, 0444);
+ */
 static const struct kernel_param_ops null_queue_mode_param_ops = {
 	.set	= null_set_queue_mode,
 	.get	= param_get_int,
@@ -145,6 +309,12 @@ static unsigned int nr_devices = 1;
 module_param(nr_devices, uint, 0444);
 MODULE_PARM_DESC(nr_devices, "Number of devices to register");
 
+/*
+ * 在以下使用g_blocking:
+ *   - drivers/block/null_blk_main.c|222| <<global>> module_param_named(blocking, g_blocking, bool, 0444);
+ *   - drivers/block/null_blk_main.c|660| <<null_alloc_dev>> dev->blocking = g_blocking;
+ *   - drivers/block/null_blk_main.c|2037| <<null_init_tag_set>> if ((nullb && nullb->dev->blocking) || g_blocking)
+ */
 static bool g_blocking;
 module_param_named(blocking, g_blocking, bool, 0444);
 MODULE_PARM_DESC(blocking, "Register as a blocking blk-mq driver device");
@@ -161,6 +331,10 @@ static int null_set_irqmode(const char *str, const struct kernel_param *kp)
 					NULL_IRQ_TIMER);
 }
 
+/*
+ * 在以下使用null_irqmode_param_ops:
+ *   - drivers/block/null_blk_main.c|242| <<global>> device_param_cb(irqmode, &null_irqmode_param_ops, &g_irqmode, 0444);
+ */
 static const struct kernel_param_ops null_irqmode_param_ops = {
 	.set	= null_set_irqmode,
 	.get	= param_get_int,
@@ -169,6 +343,11 @@ static const struct kernel_param_ops null_irqmode_param_ops = {
 device_param_cb(irqmode, &null_irqmode_param_ops, &g_irqmode, 0444);
 MODULE_PARM_DESC(irqmode, "IRQ completion handler. 0-none, 1-softirq, 2-timer");
 
+/*
+ * 在以下使用g_completion_nsec:
+ *   - drivers/block/null_blk_main.c|302| <<global>> module_param_named(completion_nsec, g_completion_nsec, ulong, 0444);
+ *   - drivers/block/null_blk_main.c|715| <<null_alloc_dev>> dev->completion_nsec = g_completion_nsec;
+ */
 static unsigned long g_completion_nsec = 10000;
 module_param_named(completion_nsec, g_completion_nsec, ulong, 0444);
 MODULE_PARM_DESC(completion_nsec, "Time in ns to complete a request in hardware. Default: 10,000ns");
@@ -177,6 +356,12 @@ static int g_hw_queue_depth = 64;
 module_param_named(hw_queue_depth, g_hw_queue_depth, int, 0444);
 MODULE_PARM_DESC(hw_queue_depth, "Queue depth for each hardware queue. Default: 64");
 
+/*
+ * 在以下使用g_use_per_node_hctx:
+ *   - drivers/block/null_blk_main.c|264| <<global>> module_param_named(use_per_node_hctx, g_use_per_node_hctx, bool, 0444);
+ *   - drivers/block/null_blk_main.c|671| <<null_alloc_dev>> dev->use_per_node_hctx = g_use_per_node_hctx;
+ *   - drivers/block/null_blk_main.c|2311| <<null_init>> if (g_queue_mode == NULL_Q_MQ && g_use_per_node_hctx) {
+ */
 static bool g_use_per_node_hctx;
 module_param_named(use_per_node_hctx, g_use_per_node_hctx, bool, 0444);
 MODULE_PARM_DESC(use_per_node_hctx, "Use per-node allocation for hardware context queues. Default: false");
@@ -189,6 +374,11 @@ static unsigned long g_zone_size = 256;
 module_param_named(zone_size, g_zone_size, ulong, S_IRUGO);
 MODULE_PARM_DESC(zone_size, "Zone size in MB when block device is zoned. Must be power-of-two: Default: 256");
 
+/*
+ * 在以下使用g_zone_nr_conv:
+ *   - drivers/block/null_blk_main.c|328| <<global>> module_param_named(zone_nr_conv, g_zone_nr_conv, uint, 0444);
+ *   - drivers/block/null_blk_main.c|726| <<null_alloc_dev>> dev->zone_nr_conv = g_zone_nr_conv;
+ */
 static unsigned int g_zone_nr_conv;
 module_param_named(zone_nr_conv, g_zone_nr_conv, uint, 0444);
 MODULE_PARM_DESC(zone_nr_conv, "Number of conventional zones when block device is zoned. Default: 0");
@@ -204,22 +394,26 @@ static inline struct nullb_device *to_nullb_device(struct config_item *item)
 	return item ? container_of(item, struct nullb_device, item) : NULL;
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的show, 类型uint */
 static inline ssize_t nullb_device_uint_attr_show(unsigned int val, char *page)
 {
 	return snprintf(page, PAGE_SIZE, "%u\n", val);
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的show, 类型ulong */
 static inline ssize_t nullb_device_ulong_attr_show(unsigned long val,
 	char *page)
 {
 	return snprintf(page, PAGE_SIZE, "%lu\n", val);
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的show, 类型bool */
 static inline ssize_t nullb_device_bool_attr_show(bool val, char *page)
 {
 	return snprintf(page, PAGE_SIZE, "%u\n", val);
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的store, 类型uint */
 static ssize_t nullb_device_uint_attr_store(unsigned int *val,
 	const char *page, size_t count)
 {
@@ -234,6 +428,7 @@ static ssize_t nullb_device_uint_attr_store(unsigned int *val,
 	return count;
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的store, 类型ulong */
 static ssize_t nullb_device_ulong_attr_store(unsigned long *val,
 	const char *page, size_t count)
 {
@@ -248,6 +443,7 @@ static ssize_t nullb_device_ulong_attr_store(unsigned long *val,
 	return count;
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的store, 类型bool */
 static ssize_t nullb_device_bool_attr_store(bool *val, const char *page,
 	size_t count)
 {
@@ -263,6 +459,27 @@ static ssize_t nullb_device_bool_attr_store(bool *val, const char *page,
 }
 
 /* The following macro should only be used with TYPE = {uint, ulong, bool}. */
+/*
+ * 在以下使用NULLB_DEVICE_ATTR():
+ *   - drivers/block/null_blk_main.c|451| <<global>> NULLB_DEVICE_ATTR(size, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|452| <<global>> NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|453| <<global>> NULLB_DEVICE_ATTR(submit_queues, uint, nullb_apply_submit_queues);
+ *   - drivers/block/null_blk_main.c|454| <<global>> NULLB_DEVICE_ATTR(home_node, uint, NULL);
+ *   - drivers/block/null_blk_main.c|455| <<global>> NULLB_DEVICE_ATTR(queue_mode, uint, NULL);
+ *   - drivers/block/null_blk_main.c|456| <<global>> NULLB_DEVICE_ATTR(blocksize, uint, NULL);
+ *   - drivers/block/null_blk_main.c|457| <<global>> NULLB_DEVICE_ATTR(irqmode, uint, NULL);
+ *   - drivers/block/null_blk_main.c|458| <<global>> NULLB_DEVICE_ATTR(hw_queue_depth, uint, NULL);
+ *   - drivers/block/null_blk_main.c|459| <<global>> NULLB_DEVICE_ATTR(index, uint, NULL);
+ *   - drivers/block/null_blk_main.c|460| <<global>> NULLB_DEVICE_ATTR(blocking, bool, NULL);
+ *   - drivers/block/null_blk_main.c|461| <<global>> NULLB_DEVICE_ATTR(use_per_node_hctx, bool, NULL);
+ *   - drivers/block/null_blk_main.c|462| <<global>> NULLB_DEVICE_ATTR(memory_backed, bool, NULL);
+ *   - drivers/block/null_blk_main.c|463| <<global>> NULLB_DEVICE_ATTR(discard, bool, NULL);
+ *   - drivers/block/null_blk_main.c|464| <<global>> NULLB_DEVICE_ATTR(mbps, uint, NULL);
+ *   - drivers/block/null_blk_main.c|465| <<global>> NULLB_DEVICE_ATTR(cache_size, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|466| <<global>> NULLB_DEVICE_ATTR(zoned, bool, NULL);
+ *   - drivers/block/null_blk_main.c|467| <<global>> NULLB_DEVICE_ATTR(zone_size, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|468| <<global>> NULLB_DEVICE_ATTR(zone_nr_conv, uint, NULL);
+ */
 #define NULLB_DEVICE_ATTR(NAME, TYPE, APPLY)					\
 static ssize_t									\
 nullb_device_##NAME##_show(struct config_item *item, char *page)		\
@@ -293,6 +510,12 @@ nullb_device_##NAME##_store(struct config_item *item, const char *page,		\
 }										\
 CONFIGFS_ATTR(nullb_device_, NAME);
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|329| <<global>> NULLB_DEVICE_ATTR(submit_queues, uint, nullb_apply_submit_queues);
+ *
+ * nullb_device_submit_queues_store()
+ */
 static int nullb_apply_submit_queues(struct nullb_device *dev,
 				     unsigned int submit_queues)
 {
@@ -307,6 +530,53 @@ static int nullb_apply_submit_queues(struct nullb_device *dev,
 	return set->nr_hw_queues == submit_queues ? 0 : -ENOMEM;
 }
 
+/*
+ * commit 3bf2bd20734e3e6ffda53719a9c10fb3ee9c5ffa
+ * Author: Shaohua Li <shli@fb.com>
+ * Date:   Mon Aug 14 15:04:53 2017 -0700
+ * 
+ * nullb: add configfs interface
+ *
+ * Add configfs interface for nullb. configfs interface is more flexible
+ * and easy to configure in a per-disk basis.
+ *
+ * Configuration is something like this:
+ * mount -t configfs none /mnt
+ *
+ * Checking which features the driver supports:
+ * cat /mnt/nullb/features
+ *
+ * The 'features' attribute is for future extension. We probably will add
+ * new features into the driver, userspace can check this attribute to find
+ * the supported features.
+ *
+ * Create/remove a device:
+ * mkdir/rmdir /mnt/nullb/a
+ *
+ * Then configure the device by setting attributes under /mnt/nullb/a, most
+ * of nullb supported module parameters are converted to attributes:
+ * size; // device size in MB
+ * completion_nsec; // time in ns to complete a request
+ * submit_queues; // number of submission queues
+ * home_node; // home node for the device
+ * queue_mode; // block interface
+ * blocksize; // block size
+ * irqmode; // IRQ completion handler
+ * hw_queue_depth; // queue depth
+ * use_lightnvm; // register as a LightNVM device
+ * blocking; // blocking blk-mq device
+ * use_per_node_hctx; // use per-node allocation for hardware context
+ *
+ * Note, creating a device doesn't create a disk immediately. Creating a
+ * disk is done in two phases: create a device and then power on the
+ * device. Next patch will introduce device power on.
+ *
+ * Based on original patch from Kyungchan Koh
+ *
+ * Signed-off-by: Kyungchan Koh <kkc6196@fb.com>
+ * Signed-off-by: Shaohua Li <shli@fb.com>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ */
 NULLB_DEVICE_ATTR(size, ulong, NULL);
 NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
 NULLB_DEVICE_ATTR(submit_queues, uint, nullb_apply_submit_queues);
@@ -326,11 +596,13 @@ NULLB_DEVICE_ATTR(zoned, bool, NULL);
 NULLB_DEVICE_ATTR(zone_size, ulong, NULL);
 NULLB_DEVICE_ATTR(zone_nr_conv, uint, NULL);
 
+/* 用在下面的CONFIGFS_ATTR(nullb_device_, power); */
 static ssize_t nullb_device_power_show(struct config_item *item, char *page)
 {
 	return nullb_device_bool_attr_show(to_nullb_device(item)->power, page);
 }
 
+/* 用在下面的CONFIGFS_ATTR(nullb_device_, power); */
 static ssize_t nullb_device_power_store(struct config_item *item,
 				     const char *page, size_t count)
 {
@@ -367,6 +639,7 @@ static ssize_t nullb_device_power_store(struct config_item *item,
 
 CONFIGFS_ATTR(nullb_device_, power);
 
+/* 用在下面的CONFIGFS_ATTR(nullb_device_, badblocks); */
 static ssize_t nullb_device_badblocks_show(struct config_item *item, char *page)
 {
 	struct nullb_device *t_dev = to_nullb_device(item);
@@ -374,6 +647,11 @@ static ssize_t nullb_device_badblocks_show(struct config_item *item, char *page)
 	return badblocks_show(&t_dev->badblocks, page, 0);
 }
 
+/*
+ * 用在下面的CONFIGFS_ATTR(nullb_device_, badblocks);
+ *
+ * 使用方式 "echo "+10240-20470" > badblocks", 代表起始和终止的vector
+ */
 static ssize_t nullb_device_badblocks_store(struct config_item *item,
 				     const char *page, size_t count)
 {
@@ -386,6 +664,7 @@ static ssize_t nullb_device_badblocks_store(struct config_item *item,
 	if (!orig)
 		return -ENOMEM;
 
+	/* Removes leading and trailing whitespace from @s. */
 	buf = strstrip(orig);
 
 	ret = -EINVAL;
@@ -406,6 +685,13 @@ static ssize_t nullb_device_badblocks_store(struct config_item *item,
 		goto out;
 	/* enable badblocks */
 	cmpxchg(&t_dev->badblocks.shift, -1, 0);
+	/*
+	 * badblocks_set() - Add a range of bad blocks to the table.
+	 * @bb:         the badblocks structure that holds all badblock information
+	 * @s:          first sector to mark as bad
+	 * @sectors:    number of sectors to mark as bad
+	 * @acknowledged: weather to mark the bad sectors as acknowledged
+	 */
 	if (buf[0] == '+')
 		ret = badblocks_set(&t_dev->badblocks, start,
 			end - start + 1, 1);
@@ -420,6 +706,11 @@ static ssize_t nullb_device_badblocks_store(struct config_item *item,
 }
 CONFIGFS_ATTR(nullb_device_, badblocks);
 
+/*
+ * struct config_item_type nullb_device_type.ct_attrs = nullb_device_attrs[]
+ *
+ * configfs中所有的参数!!!!
+ */
 static struct configfs_attribute *nullb_device_attrs[] = {
 	&nullb_device_attr_size,
 	&nullb_device_attr_completion_nsec,
@@ -444,6 +735,7 @@ static struct configfs_attribute *nullb_device_attrs[] = {
 	NULL,
 };
 
+/* struct configfs_item_operations nullb_device_ops.release = nullb_device_release() */
 static void nullb_device_release(struct config_item *item)
 {
 	struct nullb_device *dev = to_nullb_device(item);
@@ -452,16 +744,24 @@ static void nullb_device_release(struct config_item *item)
 	null_free_dev(dev);
 }
 
+/* struct config_item_type nullb_device_type.ct_item_ops = &nullb_device_ops */
 static struct configfs_item_operations nullb_device_ops = {
 	.release	= nullb_device_release,
 };
 
+/*
+ * 在以下使用nullb_device_type:
+ *   - drivers/block/null_blk_main.c|711| <<nullb_group_make_item>> config_item_init_type_name(&dev->item, name, &nullb_device_type);
+ */
 static const struct config_item_type nullb_device_type = {
 	.ct_item_ops	= &nullb_device_ops,
 	.ct_attrs	= nullb_device_attrs,
 	.ct_owner	= THIS_MODULE,
 };
 
+/*
+ * struct configfs_group_operations nullb_group_ops.make_item = nullb_group_make_item()
+ */
 static struct
 config_item *nullb_group_make_item(struct config_group *group, const char *name)
 {
@@ -476,6 +776,9 @@ config_item *nullb_group_make_item(struct config_group *group, const char *name)
 	return &dev->item;
 }
 
+/*
+ * struct configfs_group_operations nullb_group_ops.drop_item = nullb_group_drop_item()
+ */
 static void
 nullb_group_drop_item(struct config_group *group, struct config_item *item)
 {
@@ -514,6 +817,14 @@ static const struct config_item_type nullb_group_type = {
 	.ct_owner	= THIS_MODULE,
 };
 
+/*
+ * 使用nullb_subsys的地方:
+ *   - drivers/block/null_blk_main.c|2176| <<null_init>> config_group_init(&nullb_subsys.su_group);
+ *   - drivers/block/null_blk_main.c|2177| <<null_init>> mutex_init(&nullb_subsys.su_mutex);
+ *   - drivers/block/null_blk_main.c|2179| <<null_init>> ret = configfs_register_subsystem(&nullb_subsys);
+ *   - drivers/block/null_blk_main.c|2219| <<null_init>> configfs_unregister_subsystem(&nullb_subsys);
+ *   - drivers/block/null_blk_main.c|2230| <<null_exit>> configfs_unregister_subsystem(&nullb_subsys);
+ */
 static struct configfs_subsystem nullb_subsys = {
 	.su_group = {
 		.cg_item = {
@@ -523,11 +834,36 @@ static struct configfs_subsystem nullb_subsys = {
 	},
 };
 
+/*
+ * 上面都是configfs的, 下面是核心代码了!!!
+ */
+
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1098| <<copy_to_nullb>> if (null_cache_active(nullb) && !is_fua)
+ *   - drivers/block/null_blk_main.c|1103| <<copy_to_nullb>> !null_cache_active(nullb) || is_fua);
+ *   - drivers/block/null_blk_main.c|1141| <<copy_from_nullb>> !null_cache_active(nullb));
+ *   - drivers/block/null_blk_main.c|1187| <<null_handle_discard>> if (null_cache_active(nullb))
+ *   - drivers/block/null_blk_main.c|1203| <<null_handle_flush>> if (!null_cache_active(nullb))
+ *   - drivers/block/null_blk_main.c|1705| <<null_del_dev>> if (null_cache_active(nullb))
+ */
 static inline int null_cache_active(struct nullb *nullb)
 {
+	/*
+	 * 在以下使用NULLB_DEV_FL_CACHE:
+	 *   - drivers/block/null_blk_main.c|574| <<null_cache_active>> return test_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 *   - drivers/block/null_blk_main.c|2161| <<null_add_dev>> set_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 *
+	 * Device is using a write-back cache.
+	 */
 	return test_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|470| <<nullb_group_make_item>> dev = null_alloc_dev();
+ *   - drivers/block/null_blk_main.c|1852| <<null_init>> dev = null_alloc_dev();
+ */
 static struct nullb_device *null_alloc_dev(void)
 {
 	struct nullb_device *dev;
@@ -543,6 +879,12 @@ static struct nullb_device *null_alloc_dev(void)
 	}
 
 	dev->size = g_gb * 1024;
+	/*
+	 * 在以下使用completion_nsec:
+	 *   - drivers/block/null_blk_main.c|452| <<global>> NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
+	 *   - drivers/block/null_blk_main.c|715| <<null_alloc_dev>> dev->completion_nsec = g_completion_nsec;
+	 *   - drivers/block/null_blk_main.c|930| <<null_cmd_end_timer>> ktime_t kt = cmd->nq->dev->completion_nsec;
+	 */
 	dev->completion_nsec = g_completion_nsec;
 	dev->submit_queues = g_submit_queues;
 	dev->home_node = g_home_node;
@@ -551,6 +893,11 @@ static struct nullb_device *null_alloc_dev(void)
 	dev->irqmode = g_irqmode;
 	dev->hw_queue_depth = g_hw_queue_depth;
 	dev->blocking = g_blocking;
+	/*
+	 * 在以下使用null_device->use_per_mpde_hctx:
+	 *   - drivers/block/null_blk_main.c|671| <<null_alloc_dev>> dev->use_per_node_hctx = g_use_per_node_hctx;
+	 *   - drivers/block/null_blk_main.c|2083| <<null_validate_conf>> if (dev->queue_mode == NULL_Q_MQ && dev->use_per_node_hctx) {
+	 */
 	dev->use_per_node_hctx = g_use_per_node_hctx;
 	dev->zoned = g_zoned;
 	dev->zone_size = g_zone_size;
@@ -558,6 +905,13 @@ static struct nullb_device *null_alloc_dev(void)
 	return dev;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|460| <<nullb_device_release>> null_free_dev(dev);
+ *   - drivers/block/null_blk_main.c|2202| <<null_init>> null_free_dev(dev);
+ *   - drivers/block/null_blk_main.c|2215| <<null_init>> null_free_dev(dev);
+ *   - drivers/block/null_blk_main.c|2241| <<null_exit>> null_free_dev(dev);
+ */
 static void null_free_dev(struct nullb_device *dev)
 {
 	if (!dev)
@@ -568,19 +922,50 @@ static void null_free_dev(struct nullb_device *dev)
 	kfree(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|622| <<free_cmd>> put_tag(cmd->nq, cmd->tag);
+ *
+ * end_cmd()
+ *  -> free_cmd()
+ *      -> put_tag()
+ *
+ * 清空nullb_queue->tag_map中tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */
 static void put_tag(struct nullb_queue *nq, unsigned int tag)
 {
+	/*
+	 * Clear a bit in memory, for unlock
+	 */
+	/*
+	 * struct nullb_queue:
+	 *  -> unsigned long *tag_map;
+	 */
 	clear_bit_unlock(tag, nq->tag_map);
 
+	/*
+	 * 在alloc_cmd()中可能等待在nq->wait
+	 */
 	if (waitqueue_active(&nq->wait))
 		wake_up(&nq->wait);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|622| <<__alloc_cmd>> tag = get_tag(nq);
+ *
+ * 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+ */
 static unsigned int get_tag(struct nullb_queue *nq)
 {
 	unsigned int tag;
 
 	do {
+		/*
+		 * Find the first cleared bit in a memory region.
+		 * 这里的memory region就是nq->tag_map
+		 */
 		tag = find_first_zero_bit(nq->tag_map, nq->queue_depth);
 		if (tag >= nq->queue_depth)
 			return -1U;
@@ -589,6 +974,17 @@ static unsigned int get_tag(struct nullb_queue *nq)
 	return tag;
 }
 
+/*
+ * called only by:
+ *   - drivers/block/null_blk_main.c|685| <<end_cmd>> free_cmd(cmd);
+ *
+ * end_cmd()
+ *  -> free_cmd()
+ *      -> put_tag()
+ *
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */ 
 static void free_cmd(struct nullb_cmd *cmd)
 {
 	put_tag(cmd->nq, cmd->tag);
@@ -596,13 +992,37 @@ static void free_cmd(struct nullb_cmd *cmd)
 
 static enum hrtimer_restart null_cmd_timer_expired(struct hrtimer *timer);
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|630| <<alloc_cmd>> cmd = __alloc_cmd(nq);
+ *   - drivers/block/null_blk_main.c|636| <<alloc_cmd>> cmd = __alloc_cmd(nq);
+ *
+ * null_queue_bio()
+ *  -> alloc_cmd()
+ *      -> __alloc_cmd()
+ *
+ * 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+ * 然后返回nullb_queue->cmd[tag]
+ */
 static struct nullb_cmd *__alloc_cmd(struct nullb_queue *nq)
 {
 	struct nullb_cmd *cmd;
 	unsigned int tag;
 
+	/* 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag */
 	tag = get_tag(nq);
 	if (tag != -1U) {
+		/*
+		 * struct nullb_queue {
+		 *	unsigned long *tag_map;
+		 *	wait_queue_head_t wait;
+		 *	unsigned int queue_depth;
+		 *	struct nullb_device *dev;
+		 *	unsigned int requeue_selection;
+		 *
+		 *	struct nullb_cmd *cmds;
+		 * };
+		 */
 		cmd = &nq->cmds[tag];
 		cmd->tag = tag;
 		cmd->nq = nq;
@@ -617,11 +1037,27 @@ static struct nullb_cmd *__alloc_cmd(struct nullb_queue *nq)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1346| <<null_queue_bio>> cmd = alloc_cmd(nq, 1);
+ *
+ * null_queue_bio()
+ *  -> alloc_cmd()
+ *      -> __alloc_cmd()
+ *
+ * 核心思想是在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+ * 然后返回nullb_queue->cmd[tag]
+ * 分配不到的话如果can_wait设置了会睡眠 (put_tag()会唤醒)
+ */
 static struct nullb_cmd *alloc_cmd(struct nullb_queue *nq, int can_wait)
 {
 	struct nullb_cmd *cmd;
 	DEFINE_WAIT(wait);
 
+	/*
+	 * 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+	 * 然后返回nullb_queue->cmd[tag]
+	 */
 	cmd = __alloc_cmd(nq);
 	if (cmd || !can_wait)
 		return cmd;
@@ -639,12 +1075,46 @@ static struct nullb_cmd *alloc_cmd(struct nullb_queue *nq, int can_wait)
 	return cmd;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|666| <<null_cmd_timer_expired>> end_cmd(container_of(timer, struct nullb_cmd, timer));
+ *   - drivers/block/null_blk_main.c|680| <<null_complete_rq>> end_cmd(blk_mq_rq_to_pdu(rq));
+ *   - drivers/block/null_blk_main.c|1253| <<nullb_complete_cmd>> end_cmd(cmd);
+ *   - drivers/block/null_blk_main.c|1258| <<nullb_complete_cmd>> end_cmd(cmd);
+ *
+ * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */
 static void end_cmd(struct nullb_cmd *cmd)
 {
 	int queue_mode = cmd->nq->dev->queue_mode;
 
 	switch (queue_mode)  {
 	case NULL_Q_MQ:
+		/*
+		 * 部分调用blk_mq_end_request()的例子:
+		 *   - block/blk-flush.c|421| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+		 *   - block/blk-flush.c|774| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+		 *   - block/blk-mq.c|1366| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+		 *   - block/blk-mq.c|1979| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+		 *   - block/blk-mq.c|2015| <<blk_mq_try_issue_list_directly>> blk_mq_end_request(rq, ret);
+		 *   - block/bsg-lib.c|158| <<bsg_teardown_job>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/block/loop.c|487| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+		 *   - drivers/block/nbd.c|340| <<nbd_complete_rq>> blk_mq_end_request(req, cmd->status);
+		 *   - drivers/block/null_blk_main.c|677| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+		 *   - drivers/block/virtio_blk.c|226| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+		 *   - drivers/block/xen-blkfront.c|918| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+		 *   - drivers/block/xen-blkfront.c|2113| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+		 *   - drivers/ide/ide-cd.c|765| <<cdrom_newpc_intr>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/ide/ide-pm.c|50| <<ide_pm_execute_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/ide/ide-pm.c|220| <<ide_complete_pm_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/md/dm-rq.c|174| <<dm_end_request>> blk_mq_end_request(rq, error);
+		 *   - drivers/md/dm-rq.c|271| <<dm_softirq_done>> blk_mq_end_request(rq, tio->error);
+		 *   - drivers/nvme/host/core.c|307| <<nvme_complete_rq>> blk_mq_end_request(req, status);
+		 *   - drivers/nvme/host/multipath.c|76| <<nvme_failover_req>> blk_mq_end_request(req, 0);
+		 *   - drivers/scsi/scsi_transport_fc.c|3581| <<fc_bsg_job_timeout>> blk_mq_end_request(req, BLK_STS_IOERR);
+		 */
 		blk_mq_end_request(cmd->rq, cmd->error);
 		return;
 	case NULL_Q_BIO:
@@ -653,16 +1123,47 @@ static void end_cmd(struct nullb_cmd *cmd)
 		break;
 	}
 
+	/*
+	 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+	 * 根据情况唤醒nullb_queue->wait
+	 */
 	free_cmd(cmd);
 }
 
+/*
+ * 在null_cmd_end_timer()触发()
+ *
+ * 使用null_cmd_timer_expired()的地方:
+ *   - drivers/block/null_blk_main.c|660| <<__alloc_cmd>> cmd->timer.function = null_cmd_timer_expired;
+ *   - drivers/block/null_blk_main.c|1611| <<null_queue_rq>> cmd->timer.function = null_cmd_timer_expired;
+ *
+ * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ * 返回HRTIMER_NORESTART
+ */
 static enum hrtimer_restart null_cmd_timer_expired(struct hrtimer *timer)
 {
+	/*
+	 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+	 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+	 * 根据情况唤醒nullb_queue->wait
+	 */
 	end_cmd(container_of(timer, struct nullb_cmd, timer));
 
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1447| <<nullb_complete_cmd>> null_cmd_end_timer(cmd);
+ *
+ * 调用null_cmd_timer_expired()来...
+ * ... 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ * 返回HRTIMER_NORESTART
+ */
 static void null_cmd_end_timer(struct nullb_cmd *cmd)
 {
 	ktime_t kt = cmd->nq->dev->completion_nsec;
@@ -670,13 +1171,37 @@ static void null_cmd_end_timer(struct nullb_cmd *cmd)
 	hrtimer_start(&cmd->timer, kt, HRTIMER_MODE_REL);
 }
 
+/*
+ * struct blk_mq_ops null_mq_ops.complete = null_complete_rq()
+ *
+ * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */
 static void null_complete_rq(struct request *rq)
 {
 	end_cmd(blk_mq_rq_to_pdu(rq));
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|862| <<null_insert_page>> t_page = null_alloc_page(GFP_NOIO);
+ *
+ * 分配一个nullb_page(包括nullb_page->page), 初始化nullb_pabe->bitmap
+ */
 static struct nullb_page *null_alloc_page(gfp_t gfp_flags)
 {
+	/*
+	 * struct nullb_page {
+	 *	struct page *page;
+	 *	DECLARE_BITMAP(bitmap, MAP_SZ);
+	 * };
+	 *
+	 * The highest 2 bits of bitmap are for special purpose. LOCK means the cache
+	 * page is being flushing to storage. FREE means the cache page is freed and
+	 * should be skipped from flushing to storage. Please see
+	 * null_make_cache_space
+	 */
 	struct nullb_page *t_page;
 
 	t_page = kmalloc(sizeof(struct nullb_page), gfp_flags);
@@ -695,6 +1220,18 @@ static struct nullb_page *null_alloc_page(gfp_t gfp_flags)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|826| <<null_free_sector>> null_free_page(ret);
+ *   - drivers/block/null_blk_main.c|845| <<null_radix_tree_insert>> null_free_page(t_page);
+ *   - drivers/block/null_blk_main.c|878| <<null_free_device_storage>> null_free_page(ret);
+ *   - drivers/block/null_blk_main.c|966| <<null_insert_page>> null_free_page(t_page);
+ *   - drivers/block/null_blk_main.c|990| <<null_flush_cache_page>> null_free_page(c_page);
+ *   - drivers/block/null_blk_main.c|994| <<null_flush_cache_page>> null_free_page(t_page);
+ *   - drivers/block/null_blk_main.c|1019| <<null_flush_cache_page>> null_free_page(ret);
+ *
+ * "根据情况"释放nullb_page
+ */
 static void null_free_page(struct nullb_page *t_page)
 {
 	__set_bit(NULLB_PAGE_FREE, t_page->bitmap);
@@ -704,6 +1241,13 @@ static void null_free_page(struct nullb_page *t_page)
 	kfree(t_page);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|823| <<null_free_sector>> if (null_page_empty(t_page)) {
+ *   - drivers/block/null_blk_main.c|991| <<null_flush_cache_page>> if (t_page && null_page_empty(t_page)) {
+ *
+ * 根据nullb_page->bitmap判断是否还有可用的sector
+ */
 static bool null_page_empty(struct nullb_page *page)
 {
 	int size = MAP_SZ - 2;
@@ -711,6 +1255,20 @@ static bool null_page_empty(struct nullb_page *page)
 	return find_first_bit(page->bitmap, size) == size;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1116| <<copy_to_nullb>> null_free_sector(nullb, sector, true);
+ *   - drivers/block/null_blk_main.c|1186| <<null_handle_discard>> null_free_sector(nullb, sector, false);
+ *   - drivers/block/null_blk_main.c|1188| <<null_handle_discard>> null_free_sector(nullb, sector, true);
+ *
+ * struct nullb:
+ *  -> struct nullb_device *dev;
+ *      -> struct radix_tree_root data; // data stored in the disk
+ *      -> struct radix_tree_root cache; // disk cache data
+ * null_free_sector()根据参数is_cache决定用data还是cache的radix tree
+ * 找到sector对应的nullb_page, 清空对应nullb_page->bitmap
+ * 如果都清空了, 在radix tree上拿走并释放
+ */
 static void null_free_sector(struct nullb *nullb, sector_t sector,
 	bool is_cache)
 {
@@ -719,14 +1277,26 @@ static void null_free_sector(struct nullb *nullb, sector_t sector,
 	struct nullb_page *t_page, *ret;
 	struct radix_tree_root *root;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
 	idx = sector >> PAGE_SECTORS_SHIFT;
+	/*
+	 * SECTOR_MASK表示page中的mask们
+	 */
 	sector_bit = (sector & SECTOR_MASK);
 
 	t_page = radix_tree_lookup(root, idx);
 	if (t_page) {
 		__clear_bit(sector_bit, t_page->bitmap);
 
+		/*
+		 * 根据nullb_page->bitmap判断是否还有可用的sector
+		 */
 		if (null_page_empty(t_page)) {
 			ret = radix_tree_delete_item(root, idx, t_page);
 			WARN_ON(ret != t_page);
@@ -737,11 +1307,28 @@ static void null_free_sector(struct nullb *nullb, sector_t sector,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|936| <<null_insert_page>> t_page = null_radix_tree_insert(nullb, idx, t_page, !ignore_cache);
+ *
+ * struct nullb:
+ *  -> struct nullb_device *dev;
+ *      -> struct radix_tree_root data; // data stored in the disk
+ *      -> struct radix_tree_root cache; // disk cache data
+ * null_radix_tree_insert()根据参数is_cache决定用data还是cache的radix tree
+ * 把nullb_page插入radix tree
+ */
 static struct nullb_page *null_radix_tree_insert(struct nullb *nullb, u64 idx,
 	struct nullb_page *t_page, bool is_cache)
 {
 	struct radix_tree_root *root;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
 
 	if (radix_tree_insert(root, idx, t_page)) {
@@ -754,6 +1341,13 @@ static struct nullb_page *null_radix_tree_insert(struct nullb *nullb, u64 idx,
 	return t_page;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|459| <<nullb_device_release>> null_free_device_storage(dev, false);
+ *   - drivers/block/null_blk_main.c|1621| <<null_del_dev>> null_free_device_storage(nullb->dev, true);
+ *
+ * 释放nullb_device上某个radix tree的所有page
+ */
 static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 {
 	unsigned long pos = 0;
@@ -761,6 +1355,12 @@ static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 	struct nullb_page *ret, *t_pages[FREE_BATCH];
 	struct radix_tree_root *root;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &dev->cache : &dev->data;
 
 	do {
@@ -773,6 +1373,7 @@ static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 			pos = t_pages[i]->page->index;
 			ret = radix_tree_delete_item(root, pos, t_pages[i]);
 			WARN_ON(ret != t_pages[i]);
+			/* "根据情况"释放nullb_page */
 			null_free_page(ret);
 		}
 
@@ -783,6 +1384,17 @@ static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 		dev->curr_cache = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|906| <<null_lookup_page>> page = __null_lookup_page(nullb, sector, for_write, true);
+ *   - drivers/block/null_blk_main.c|909| <<null_lookup_page>> return __null_lookup_page(nullb, sector, for_write, false);
+ *
+ * struct nullb:
+ *  -> struct nullb_device *dev;
+ *      -> struct radix_tree_root data; // data stored in the disk
+ *      -> struct radix_tree_root cache; // disk cache data
+ * 根据参数is_cache决定用data还是cache的radix tree中搜索nullb_page
+ */
 static struct nullb_page *__null_lookup_page(struct nullb *nullb,
 	sector_t sector, bool for_write, bool is_cache)
 {
@@ -794,6 +1406,12 @@ static struct nullb_page *__null_lookup_page(struct nullb *nullb,
 	idx = sector >> PAGE_SECTORS_SHIFT;
 	sector_bit = (sector & SECTOR_MASK);
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
 	t_page = radix_tree_lookup(root, idx);
 	WARN_ON(t_page && t_page->page->index != idx);
@@ -804,11 +1422,27 @@ static struct nullb_page *__null_lookup_page(struct nullb *nullb,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|920| <<null_insert_page>> t_page = null_lookup_page(nullb, sector, true, ignore_cache);
+ *   - drivers/block/null_blk_main.c|944| <<null_insert_page>> return null_lookup_page(nullb, sector, true, ignore_cache);
+ *   - drivers/block/null_blk_main.c|1106| <<copy_from_nullb>> t_page = null_lookup_page(nullb, sector, false,
+ *
+ * 根据参数ignore_cache决定用data还是cache的radix tree中搜索nullb_page
+ * 如果没有ignore_cahce, 就先在cache中搜索, 再在data搜索
+ */
 static struct nullb_page *null_lookup_page(struct nullb *nullb,
 	sector_t sector, bool for_write, bool ignore_cache)
 {
 	struct nullb_page *page = NULL;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 * 根据参数is_cache决定用data还是cache的radix tree中搜索nullb_page
+	 */
 	if (!ignore_cache)
 		page = __null_lookup_page(nullb, sector, for_write, true);
 	if (page)
@@ -816,6 +1450,11 @@ static struct nullb_page *null_lookup_page(struct nullb *nullb,
 	return __null_lookup_page(nullb, sector, for_write, false);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|957| <<null_flush_cache_page>> t_page = null_insert_page(nullb, idx << PAGE_SECTORS_SHIFT, true);
+ *   - drivers/block/null_blk_main.c|1068| <<copy_to_nullb>> t_page = null_insert_page(nullb, sector,
+ */
 static struct nullb_page *null_insert_page(struct nullb *nullb,
 					   sector_t sector, bool ignore_cache)
 	__releases(&nullb->lock)
@@ -830,6 +1469,9 @@ static struct nullb_page *null_insert_page(struct nullb *nullb,
 
 	spin_unlock_irq(&nullb->lock);
 
+	/*
+	 * 分配一个nullb_page(包括nullb_page->page), 初始化nullb_pabe->bitmap
+	 */
 	t_page = null_alloc_page(GFP_NOIO);
 	if (!t_page)
 		goto out_lock;
@@ -840,6 +1482,14 @@ static struct nullb_page *null_insert_page(struct nullb *nullb,
 	spin_lock_irq(&nullb->lock);
 	idx = sector >> PAGE_SECTORS_SHIFT;
 	t_page->page->index = idx;
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 * null_radix_tree_insert()根据参数is_cache决定用data还是cache的radix tree
+	 * 把nullb_page插入radix tree
+	 */
 	t_page = null_radix_tree_insert(nullb, idx, t_page, !ignore_cache);
 	radix_tree_preload_end();
 
@@ -851,6 +1501,10 @@ static struct nullb_page *null_insert_page(struct nullb *nullb,
 	return null_lookup_page(nullb, sector, true, ignore_cache);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1029| <<null_make_cache_space>> err = null_flush_cache_page(nullb, c_pages[i]);
+ */
 static int null_flush_cache_page(struct nullb *nullb, struct nullb_page *c_page)
 {
 	int i;
@@ -894,12 +1548,20 @@ static int null_flush_cache_page(struct nullb *nullb, struct nullb_page *c_page)
 	kunmap_atomic(src);
 
 	ret = radix_tree_delete_item(&nullb->dev->cache, idx, c_page);
+	/* "根据情况"释放nullb_page */
 	null_free_page(ret);
 	nullb->dev->curr_cache -= PAGE_SIZE;
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1065| <<copy_to_nullb>> null_make_cache_space(nullb, PAGE_SIZE);
+ *   - drivers/block/null_blk_main.c|1174| <<null_handle_flush>> err = null_make_cache_space(nullb,
+ *
+ * 从存储flush掉n大小的cache
+ */
 static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 {
 	int i, err, nr_pages;
@@ -911,6 +1573,17 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 	     nullb->dev->curr_cache + n || nullb->dev->curr_cache == 0)
 		return 0;
 
+	/*
+	 * radix_tree_gang_lookup - perform multiple lookup on a radix tree
+	 * @root:          radix tree root
+	 * @results:       where the results of the lookup are placed
+	 * @first_index:   start the lookup from this key
+	 * @max_items:     place up to this many items at *results
+	 *
+	 * Performs an index-ascending scan of the tree for present items.  Places
+	 * them at *@results and returns the number of items which were placed at
+	 * @results.
+	 */
 	nr_pages = radix_tree_gang_lookup(&nullb->dev->cache,
 			(void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
 	/*
@@ -923,6 +1596,13 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 		 * We found the page which is being flushed to disk by other
 		 * threads
 		 */
+		/*
+		 * 在以下使用NULLB_PAGE_LOCK:
+		 *   - drivers/block/null_blk_main.c|1151| <<null_free_page>> if (test_bit(NULLB_PAGE_LOCK, t_page->bitmap))
+		 *   - drivers/block/null_blk_main.c|1356| <<null_flush_cache_page>> __clear_bit(NULLB_PAGE_LOCK, c_page->bitmap);
+		 *   - drivers/block/null_blk_main.c|1421| <<null_make_cache_space>> if (test_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap))
+		 *   - drivers/block/null_blk_main.c|1424| <<null_make_cache_space>> __set_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap);
+		 */
 		if (test_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap))
 			c_pages[i] = NULL;
 		else
@@ -941,6 +1621,12 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 	flushed += one_round << PAGE_SHIFT;
 
 	if (n > flushed) {
+		/*
+		 * 在以下使用cache_flush_pos:
+		 *   - drivers/block/null_blk_main.c|1551| <<null_make_cache_space>> (void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
+		 *   - drivers/block/null_blk_main.c|1557| <<null_make_cache_space>> nullb->cache_flush_pos = c_pages[i]->page->index;
+		 *   - drivers/block/null_blk_main.c|1581| <<null_make_cache_space>> nullb->cache_flush_pos = 0;
+		 */
 		if (nr_pages == 0)
 			nullb->cache_flush_pos = 0;
 		if (one_round == 0) {
@@ -953,6 +1639,10 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1189| <<null_transfer>> err = copy_to_nullb(nullb, page, off, sector, len, is_fua);
+ */
 static int copy_to_nullb(struct nullb *nullb, struct page *source,
 	unsigned int off, sector_t sector, size_t n, bool is_fua)
 {
@@ -961,9 +1651,20 @@ static int copy_to_nullb(struct nullb *nullb, struct page *source,
 	struct nullb_page *t_page;
 	void *dst, *src;
 
+	/*
+	 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+	 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+	 *
+	 * 由null_cache_active()和is_fua决定是否用cache
+	 */
+
 	while (count < n) {
 		temp = min_t(size_t, nullb->dev->blocksize, n - count);
 
+		/*
+		 * null_cache_active()
+		 * 判断是否支持write-back cache (NULLB_DEV_FL_CACHE)
+		 */
 		if (null_cache_active(nullb) && !is_fua)
 			null_make_cache_space(nullb, PAGE_SIZE);
 
@@ -981,6 +1682,15 @@ static int copy_to_nullb(struct nullb *nullb, struct page *source,
 
 		__set_bit(sector & SECTOR_MASK, t_page->bitmap);
 
+		/*
+		 * struct nullb:
+		 *  -> struct nullb_device *dev;
+		 *      -> struct radix_tree_root data; // data stored in the disk
+		 *      -> struct radix_tree_root cache; // disk cache data
+		 * null_free_sector()根据参数is_cache决定用data还是cache的radix tree
+		 * 找到sector对应的nullb_page, 清空对应nullb_page->bitmap
+		 * 如果都清空了, 在radix tree上拿走并释放
+		 */
 		if (is_fua)
 			null_free_sector(nullb, sector, true);
 
@@ -990,6 +1700,13 @@ static int copy_to_nullb(struct nullb *nullb, struct page *source,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1178| <<null_transfer>> err = copy_from_nullb(nullb, page, off,
+ *
+ * 这里应该是读的工作,如果支持write-back cache (NULLB_DEV_FL_CACHE),就用cache radix tree
+ * 否则用data的radix tree, 把数据读到内存
+ */
 static int copy_from_nullb(struct nullb *nullb, struct page *dest,
 	unsigned int off, sector_t sector, size_t n)
 {
@@ -1002,6 +1719,14 @@ static int copy_from_nullb(struct nullb *nullb, struct page *dest,
 		temp = min_t(size_t, nullb->dev->blocksize, n - count);
 
 		offset = (sector & SECTOR_MASK) << SECTOR_SHIFT;
+		/*
+		 * null_lookup_page():
+		 * 根据参数ignore_cache决定用data还是cache的radix tree中搜索nullb_page
+		 * 如果没有ignore_cahce, 就先在cache中搜索, 再在data搜索
+		 *
+		 * null_cache_active():
+		 * Device is using a write-back cache.
+		 */
 		t_page = null_lookup_page(nullb, sector, false,
 			!null_cache_active(nullb));
 
@@ -1022,6 +1747,12 @@ static int copy_from_nullb(struct nullb *nullb, struct page *dest,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1185| <<null_transfer>> nullb_fill_pattern(nullb, page, len, off);
+ *
+ * 把page的从off开始的len的长度的内存设置成0xFF
+ */
 static void nullb_fill_pattern(struct nullb *nullb, struct page *page,
 			       unsigned int len, unsigned int off)
 {
@@ -1032,6 +1763,11 @@ static void nullb_fill_pattern(struct nullb *nullb, struct page *page,
 	kunmap_atomic(dst);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1208| <<null_handle_rq>> null_handle_discard(nullb, sector, blk_rq_bytes(rq));
+ *   - drivers/block/null_blk_main.c|1242| <<null_handle_bio>> null_handle_discard(nullb, sector,
+ */
 static void null_handle_discard(struct nullb *nullb, sector_t sector, size_t n)
 {
 	size_t temp;
@@ -1048,6 +1784,10 @@ static void null_handle_discard(struct nullb *nullb, sector_t sector, size_t n)
 	spin_unlock_irq(&nullb->lock);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1381| <<null_handle_cmd>> cmd->error = errno_to_blk_status(null_handle_flush(nullb));
+ */
 static int null_handle_flush(struct nullb *nullb)
 {
 	int err;
@@ -1068,6 +1808,11 @@ static int null_handle_flush(struct nullb *nullb)
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1236| <<null_handle_rq>> err = null_transfer(nullb, bvec.bv_page, len, bvec.bv_offset,
+ *   - drivers/block/null_blk_main.c|1271| <<null_handle_bio>> err = null_transfer(nullb, bvec.bv_page, len, bvec.bv_offset,
+ */
 static int null_transfer(struct nullb *nullb, struct page *page,
 	unsigned int len, unsigned int off, bool is_write, sector_t sector,
 	bool is_fua)
@@ -1082,23 +1827,39 @@ static int null_transfer(struct nullb *nullb, struct page *page,
 				sector, len);
 
 		if (valid_len) {
+			/*
+			 * 这里应该是读的工作,如果支持write-back cache (NULLB_DEV_FL_CACHE),就用cache radix tree
+			 * 否则用data的radix tree, 把数据读到内存
+			 */
 			err = copy_from_nullb(nullb, page, off,
 				sector, valid_len);
 			off += valid_len;
 			len -= valid_len;
 		}
 
+		/*
+		 * nullb_fill_pattern()把page的从off开始的len的长度的内存设置成0xFF
+		 * 猜测对于非zoned的blk的len=0
+		 */
 		if (len)
 			nullb_fill_pattern(nullb, page, len, off);
 		flush_dcache_page(page);
 	} else {
 		flush_dcache_page(page);
+		/*
+		 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+		 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+		 */
 		err = copy_to_nullb(nullb, page, off, sector, len, is_fua);
 	}
 
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1348| <<null_handle_memory_backed>> err = null_handle_rq(cmd);
+ */
 static int null_handle_rq(struct nullb_cmd *cmd)
 {
 	struct request *rq = cmd->rq;
@@ -1117,8 +1878,16 @@ static int null_handle_rq(struct nullb_cmd *cmd)
 	}
 
 	spin_lock_irq(&nullb->lock);
+	/*
+	 * struct bio_vec bvec;
+	 * struct req_iterator iter;
+	 */
 	rq_for_each_segment(bvec, rq, iter) {
 		len = bvec.bv_len;
+		/*
+		 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+		 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+		 */
 		err = null_transfer(nullb, bvec.bv_page, len, bvec.bv_offset,
 				     op_is_write(req_op(rq)), sector,
 				     req_op(rq) & REQ_FUA);
@@ -1133,6 +1902,10 @@ static int null_handle_rq(struct nullb_cmd *cmd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1346| <<null_handle_memory_backed>> err = null_handle_bio(cmd);
+ */
 static int null_handle_bio(struct nullb_cmd *cmd)
 {
 	struct bio *bio = cmd->bio;
@@ -1167,6 +1940,12 @@ static int null_handle_bio(struct nullb_cmd *cmd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1290| <<null_handle_throttled>> null_stop_queue(nullb);
+ *
+ * 对于multiqueue调用blk_mq_stop_hw_queues(q)
+ */
 static void null_stop_queue(struct nullb *nullb)
 {
 	struct request_queue *q = nullb->q;
@@ -1175,6 +1954,14 @@ static void null_stop_queue(struct nullb *nullb)
 		blk_mq_stop_hw_queues(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1318| <<null_handle_throttled>> null_restart_queue_async(nullb);
+ *   - drivers/block/null_blk_main.c|1437| <<nullb_bwtimer_fn>> null_restart_queue_async(nullb);
+ *   - drivers/block/null_blk_main.c|1611| <<null_del_dev>> null_restart_queue_async(nullb);
+ *
+ * 对于multiqueue调用blk_mq_start_stopped_hw_queues(q, true)
+ */
 static void null_restart_queue_async(struct nullb *nullb)
 {
 	struct request_queue *q = nullb->q;
@@ -1183,6 +1970,10 @@ static void null_restart_queue_async(struct nullb *nullb)
 		blk_mq_start_stopped_hw_queues(q, true);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1400| <<null_handle_cmd>> sts = null_handle_throttled(cmd);
+ */
 static inline blk_status_t null_handle_throttled(struct nullb_cmd *cmd)
 {
 	struct nullb_device *dev = cmd->nq->dev;
@@ -1204,6 +1995,10 @@ static inline blk_status_t null_handle_throttled(struct nullb_cmd *cmd)
 	return sts;
 }
 
+/*
+ * called by;
+ *   - drivers/block/null_blk_main.c|1411| <<null_handle_cmd>> cmd->error = null_handle_badblocks(cmd, sector, nr_sectors);
+ */
 static inline blk_status_t null_handle_badblocks(struct nullb_cmd *cmd,
 						 sector_t sector,
 						 sector_t nr_sectors)
@@ -1218,6 +2013,10 @@ static inline blk_status_t null_handle_badblocks(struct nullb_cmd *cmd,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1417| <<null_handle_cmd>> cmd->error = null_handle_memory_backed(cmd, op);
+ */
 static inline blk_status_t null_handle_memory_backed(struct nullb_cmd *cmd,
 						     enum req_opf op)
 {
@@ -1232,6 +2031,14 @@ static inline blk_status_t null_handle_memory_backed(struct nullb_cmd *cmd,
 	return errno_to_blk_status(err);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1321| <<null_handle_cmd>> nullb_complete_cmd(cmd);
+ *
+ * null_queue_bio() or null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> nullb_complete_cmd()
+ */
 static inline void nullb_complete_cmd(struct nullb_cmd *cmd)
 {
 	/* Complete IO by inline, softirq or timer */
@@ -1245,11 +2052,21 @@ static inline void nullb_complete_cmd(struct nullb_cmd *cmd)
 			/*
 			 * XXX: no proper submitting cpu information available.
 			 */
+			/*
+			 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+			 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+			 * 根据情况唤醒nullb_queue->wait
+			 */
 			end_cmd(cmd);
 			break;
 		}
 		break;
 	case NULL_IRQ_NONE:
+		/*
+		 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+		 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+		 * 根据情况唤醒nullb_queue->wait
+		 */
 		end_cmd(cmd);
 		break;
 	case NULL_IRQ_TIMER:
@@ -1258,6 +2075,11 @@ static inline void nullb_complete_cmd(struct nullb_cmd *cmd)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1373| <<null_queue_bio>> null_handle_cmd(cmd, sector, nr_sectors, bio_op(bio));
+ *   - drivers/block/null_blk_main.c|1441| <<null_queue_rq>> return null_handle_cmd(cmd, sector, nr_sectors, req_op(bd->rq));
+ */
 static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 				    sector_t nr_sectors, enum req_opf op)
 {
@@ -1265,6 +2087,12 @@ static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 	struct nullb *nullb = dev->nullb;
 	blk_status_t sts;
 
+	/*
+	 * 在以下使用NULLB_DEV_FL_THROTTLED:
+	 *   - drivers/block/null_blk_main.c|1543| <<null_handle_cmd>> if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|1772| <<null_del_dev>> if (test_bit(NULLB_DEV_FL_THROTTLED, &nullb->dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|2156| <<null_add_dev>> set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
+	 */
 	if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
 		sts = null_handle_throttled(cmd);
 		if (sts != BLK_STS_OK)
@@ -1282,6 +2110,9 @@ static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 			goto out;
 	}
 
+	/*
+	 * 对于nullb0默认是0
+	 */
 	if (dev->memory_backed)
 		cmd->error = null_handle_memory_backed(cmd, op);
 
@@ -1289,10 +2120,22 @@ static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 		cmd->error = null_handle_zoned(cmd, op, sector, nr_sectors);
 
 out:
+	/*
+	 * called by:
+	 *   - drivers/block/null_blk_main.c|1321| <<null_handle_cmd>> nullb_complete_cmd(cmd);
+	 *
+	 * null_queue_bio() or null_queue_rq():
+	 *  -> null_handle_cmd()
+	 *      -> nullb_complete_cmd()
+	 */
 	nullb_complete_cmd(cmd);
 	return BLK_STS_OK;
 }
 
+/*
+ * 在以下使用nullb_bwtimer_fn():
+ *   - drivers/block/null_blk_main.c|1449| <<nullb_setup_bwtimer>> nullb->bw_timer.function = nullb_bwtimer_fn;
+ */
 static enum hrtimer_restart nullb_bwtimer_fn(struct hrtimer *timer)
 {
 	struct nullb *nullb = container_of(timer, struct nullb, bw_timer);
@@ -1310,16 +2153,36 @@ static enum hrtimer_restart nullb_bwtimer_fn(struct hrtimer *timer)
 	return HRTIMER_RESTART;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1983| <<null_add_dev>> nullb_setup_bwtimer(nullb);
+ *
+ * 在dev->mbps和NULLB_DEV_FL_THROTTLED的情况使用
+ */
 static void nullb_setup_bwtimer(struct nullb *nullb)
 {
 	ktime_t timer_interval = ktime_set(0, TIMER_INTERVAL);
 
+	/*
+	 * HRTIMER_MODE_ABS             - Time value is absolute
+	 * HRTIMER_MODE_REL             - Time value is relative to now
+	 * HRTIMER_MODE_PINNED          - Timer is bound to CPU (is only considered
+	 *                                when starting the timer)
+	 * HRTIMER_MODE_SOFT            - Timer callback function will be executed in
+	 *                                soft irq context
+	 * HRTIMER_MODE_HARD            - Timer callback function will be executed in
+	 *                                hard irq context even on PREEMPT_RT.
+	 */
 	hrtimer_init(&nullb->bw_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	nullb->bw_timer.function = nullb_bwtimer_fn;
 	atomic_long_set(&nullb->cur_bytes, mb_per_tick(nullb->dev->mbps));
 	hrtimer_start(&nullb->bw_timer, timer_interval, HRTIMER_MODE_REL);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1473| <<null_queue_bio>> struct nullb_queue *nq = nullb_to_queue(nullb);
+ */
 static struct nullb_queue *nullb_to_queue(struct nullb *nullb)
 {
 	int index = 0;
@@ -1330,6 +2193,10 @@ static struct nullb_queue *nullb_to_queue(struct nullb *nullb)
 	return &nullb->queues[index];
 }
 
+/*
+ * 在以下使用null_queue_bio():
+ *   - drivers/block/null_blk_main.c|1832| <<null_add_dev>> blk_queue_make_request(nullb->q, null_queue_bio);
+ */
 static blk_qc_t null_queue_bio(struct request_queue *q, struct bio *bio)
 {
 	sector_t sector = bio->bi_iter.bi_sector;
@@ -1345,6 +2212,10 @@ static blk_qc_t null_queue_bio(struct request_queue *q, struct bio *bio)
 	return BLK_QC_T_NONE;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1546| <<null_queue_rq>> if (should_timeout_request(bd->rq))
+ */
 static bool should_timeout_request(struct request *rq)
 {
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -1354,6 +2225,10 @@ static bool should_timeout_request(struct request *rq)
 	return false;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1533| <<null_queue_rq>> if (should_requeue_request(bd->rq)) {
+ */
 static bool should_requeue_request(struct request *rq)
 {
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -1363,6 +2238,9 @@ static bool should_requeue_request(struct request *rq)
 	return false;
 }
 
+/*
+ * struct blk_mq_ops null_mq_ops.timeout= null_timeout_rq()
+ */
 static enum blk_eh_timer_return null_timeout_rq(struct request *rq, bool res)
 {
 	pr_info("rq %p timed out\n", rq);
@@ -1370,6 +2248,9 @@ static enum blk_eh_timer_return null_timeout_rq(struct request *rq, bool res)
 	return BLK_EH_DONE;
 }
 
+/*
+ * struct blk_mq_ops null_mq_ops.queue_rq = null_queue_rq()
+ */
 static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 			 const struct blk_mq_queue_data *bd)
 {
@@ -1381,7 +2262,22 @@ static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 
 	if (nq->dev->irqmode == NULL_IRQ_TIMER) {
+		/*
+		 * hrtimer_init - initialize a timer to the given clock
+		 * @timer:      the timer to be initialized
+		 * @clock_id:   the clock to be used
+		 * @mode:       The modes which are relevant for intitialization:
+		 *              HRTIMER_MODE_ABS, HRTIMER_MODE_REL, HRTIMER_MODE_ABS_SOFT,
+		 *              HRTIMER_MODE_REL_SOFT
+		 */
 		hrtimer_init(&cmd->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+		/*
+		 * null_cmd_timer_expired()的实现:
+		 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+		 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+		 * 根据情况唤醒nullb_queue->wait
+		 * 返回HRTIMER_NORESTART
+		 */
 		cmd->timer.function = null_cmd_timer_expired;
 	}
 	cmd->rq = bd->rq;
@@ -1389,6 +2285,9 @@ static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 
 	blk_mq_start_request(bd->rq);
 
+	/*
+	 * 类似fault injection, 需要BLK_DEV_NULL_BLK_FAULT_INJECTION
+	 */
 	if (should_requeue_request(bd->rq)) {
 		/*
 		 * Alternate between hitting the core BUSY path, and the
@@ -1405,21 +2304,38 @@ static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 	if (should_timeout_request(bd->rq))
 		return BLK_STS_OK;
 
+	/*
+	 * sector_t nr_sectors = blk_rq_sectors(bd->rq);
+	 * sector_t sector = blk_rq_pos(bd->rq);
+	 */
 	return null_handle_cmd(cmd, sector, nr_sectors, req_op(bd->rq));
 }
 
+/*
+ * 在以下使用null_mq_ops:
+ *   - drivers/block/null_blk_main.c|1701| <<null_init_tag_set>> set->ops = &null_mq_ops;
+ */
 static const struct blk_mq_ops null_mq_ops = {
 	.queue_rq       = null_queue_rq,
 	.complete	= null_complete_rq,
 	.timeout	= null_timeout_rq,
 };
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1552| <<cleanup_queues>> cleanup_queue(&nullb->queues[i]);
+ */
 static void cleanup_queue(struct nullb_queue *nq)
 {
 	kfree(nq->tag_map);
 	kfree(nq->cmds);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1578| <<null_del_dev>> cleanup_queues(nullb);
+ *   - drivers/block/null_blk_main.c|1995| <<null_add_dev>> cleanup_queues(nullb);
+ */
 static void cleanup_queues(struct nullb *nullb)
 {
 	int i;
@@ -1430,6 +2346,13 @@ static void cleanup_queues(struct nullb *nullb)
 	kfree(nullb->queues);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|367| <<nullb_device_power_store>> null_del_dev(dev->nullb);
+ *   - drivers/block/null_blk_main.c|495| <<nullb_group_drop_item>> null_del_dev(dev->nullb);
+ *   - drivers/block/null_blk_main.c|2088| <<null_init>> null_del_dev(nullb);
+ *   - drivers/block/null_blk_main.c|2114| <<null_exit>> null_del_dev(nullb);
+ */
 static void null_del_dev(struct nullb *nullb)
 {
 	struct nullb_device *dev = nullb->dev;
@@ -1458,6 +2381,10 @@ static void null_del_dev(struct nullb *nullb)
 	dev->nullb = NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1926| <<null_add_dev>> null_config_discard(nullb);
+ */
 static void null_config_discard(struct nullb *nullb)
 {
 	if (nullb->dev->discard == false)
@@ -1468,11 +2395,20 @@ static void null_config_discard(struct nullb *nullb)
 	blk_queue_flag_set(QUEUE_FLAG_DISCARD, nullb->q);
 }
 
+/*
+ * 在以下使用null_ops:
+ *   - drivers/block/null_blk_main.c|1573| <<null_gendisk_register>> disk->fops = &null_ops;
+ */
 static const struct block_device_operations null_ops = {
 	.owner		= THIS_MODULE,
 	.report_zones	= null_report_zones,
 };
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1626| <<null_init_queues>> null_init_queue(nullb, nq);
+ *   - drivers/block/null_blk_main.c|1678| <<init_driver_queues>> null_init_queue(nullb, nq);
+ */
 static void null_init_queue(struct nullb *nullb, struct nullb_queue *nq)
 {
 	BUG_ON(!nullb);
@@ -1483,6 +2419,10 @@ static void null_init_queue(struct nullb *nullb, struct nullb_queue *nq)
 	nq->dev = nullb->dev;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1880| <<null_add_dev>> null_init_queues(nullb);
+ */
 static void null_init_queues(struct nullb *nullb)
 {
 	struct request_queue *q = nullb->q;
@@ -1490,6 +2430,7 @@ static void null_init_queues(struct nullb *nullb)
 	struct nullb_queue *nq;
 	int i;
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (!hctx->nr_ctx || !hctx->tags)
 			continue;
@@ -1500,6 +2441,10 @@ static void null_init_queues(struct nullb *nullb)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1680| <<init_driver_queues>> ret = setup_commands(nq);
+ */
 static int setup_commands(struct nullb_queue *nq)
 {
 	struct nullb_cmd *cmd;
@@ -1518,7 +2463,15 @@ static int setup_commands(struct nullb_queue *nq)
 
 	for (i = 0; i < nq->queue_depth; i++) {
 		cmd = &nq->cmds[i];
+		/*
+		 * list没人用, 可以删了??
+		 *   - drivers/block/null_blk_main.c|2440| <<setup_commands>> INIT_LIST_HEAD(&cmd->list);
+		 */
 		INIT_LIST_HEAD(&cmd->list);
+		/*
+		 * ll_list没人用, 可以删了??
+		 *   - drivers/block/null_blk_main.c|2441| <<setup_commands>> cmd->ll_list.next = NULL;
+		 */
 		cmd->ll_list.next = NULL;
 		cmd->tag = -1U;
 	}
@@ -1526,8 +2479,15 @@ static int setup_commands(struct nullb_queue *nq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1851| <<null_add_dev>> rv = setup_queues(nullb);
+ */
 static int setup_queues(struct nullb *nullb)
 {
+	/*
+	 * struct nullb_queue *queues;
+	 */
 	nullb->queues = kcalloc(nullb->dev->submit_queues,
 				sizeof(struct nullb_queue),
 				GFP_KERNEL);
@@ -1539,6 +2499,10 @@ static int setup_queues(struct nullb *nullb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1888| <<null_add_dev>> rv = init_driver_queues(nullb);
+ */
 static int init_driver_queues(struct nullb *nullb)
 {
 	struct nullb_queue *nq;
@@ -1557,6 +2521,10 @@ static int init_driver_queues(struct nullb *nullb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1930| <<null_add_dev>> rv = null_gendisk_register(nullb);
+ */
 static int null_gendisk_register(struct nullb *nullb)
 {
 	sector_t size = ((sector_t)nullb->dev->size * SZ_1M) >> SECTOR_SHIFT;
@@ -1593,6 +2561,11 @@ static int null_gendisk_register(struct nullb *nullb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1810| <<null_add_dev>> rv = null_init_tag_set(nullb, nullb->tag_set);
+ *   - drivers/block/null_blk_main.c|1943| <<null_init>> ret = null_init_tag_set(NULL, &tag_set);
+ */
 static int null_init_tag_set(struct nullb *nullb, struct blk_mq_tag_set *set)
 {
 	set->ops = &null_mq_ops;
@@ -1610,9 +2583,34 @@ static int null_init_tag_set(struct nullb *nullb, struct blk_mq_tag_set *set)
 	if ((nullb && nullb->dev->blocking) || g_blocking)
 		set->flags |= BLK_MQ_F_BLOCKING;
 
+	/*
+	 * 调用blk_mq_alloc_tag_set()的例子:
+	 *   - block/blk-mq.c|2947| <<blk_mq_init_sq_queue>> ret = blk_mq_alloc_tag_set(set);
+	 *   - block/bsg-lib.c|384| <<bsg_setup_queue>> if (blk_mq_alloc_tag_set(set))
+	 *   - drivers/block/loop.c|2033| <<loop_add>> err = blk_mq_alloc_tag_set(&lo->tag_set);
+	 *   - drivers/block/nbd.c|1688| <<nbd_dev_add>> err = blk_mq_alloc_tag_set(&nbd->tag_set);
+	 *   - drivers/block/null_blk_main.c|1716| <<null_init_tag_set>> return blk_mq_alloc_tag_set(set);
+	 *   - drivers/block/virtio_blk.c|819| <<virtblk_probe>> err = blk_mq_alloc_tag_set(&vblk->tag_set);
+	 *   - drivers/block/xen-blkfront.c|984| <<xlvbd_init_blk_queue>> if (blk_mq_alloc_tag_set(&info->tag_set))
+	 *   - drivers/ide/ide-probe.c|787| <<ide_init_queue>> if (blk_mq_alloc_tag_set(set))
+	 *   - drivers/md/dm-rq.c|562| <<dm_mq_init_request_queue>> err = blk_mq_alloc_tag_set(md->tag_set);
+	 *   - drivers/nvme/host/fc.c|2475| <<nvme_fc_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+	 *   - drivers/nvme/host/fc.c|3143| <<nvme_fc_init_ctrl>> ret = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+	 *   - drivers/nvme/host/pci.c|1645| <<nvme_alloc_admin_tags>> if (blk_mq_alloc_tag_set(&dev->admin_tagset))
+	 *   - drivers/nvme/host/pci.c|2328| <<nvme_dev_add>> ret = blk_mq_alloc_tag_set(&dev->tagset);
+	 *   - drivers/nvme/host/rdma.c|755| <<nvme_rdma_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+	 *   - drivers/nvme/host/tcp.c|1493| <<nvme_tcp_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+	 *   - drivers/nvme/target/loop.c|357| <<nvme_loop_configure_admin_queue>> error = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+	 *   - drivers/nvme/target/loop.c|525| <<nvme_loop_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+	 *   - drivers/scsi/scsi_lib.c|1906| <<scsi_mq_setup_tags>> return blk_mq_alloc_tag_set(&shost->tag_set);
+	 */
 	return blk_mq_alloc_tag_set(set);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1837| <<null_add_dev>> rv = null_validate_conf(dev);
+ */
 static int null_validate_conf(struct nullb_device *dev)
 {
 	dev->blocksize = round_down(dev->blocksize, 512);
@@ -1651,6 +2649,11 @@ static int null_validate_conf(struct nullb_device *dev)
 }
 
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1824| <<null_setup_fault>> if (!__null_setup_fault(&null_timeout_attr, g_timeout_str))
+ *   - drivers/block/null_blk_main.c|1826| <<null_setup_fault>> if (!__null_setup_fault(&null_requeue_attr, g_requeue_str))
+ */
 static bool __null_setup_fault(struct fault_attr *attr, char *str)
 {
 	if (!str[0])
@@ -1664,6 +2667,12 @@ static bool __null_setup_fault(struct fault_attr *attr, char *str)
 }
 #endif
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1871| <<null_add_dev>> if (!null_setup_fault())
+ *
+ * Documentation/fault-injection/fault-injection.rst
+ */
 static bool null_setup_fault(void)
 {
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -1675,6 +2684,11 @@ static bool null_setup_fault(void)
 	return true;
 }
 
+/*
+ * calld by:
+ *   - drivers/block/null_blk_main.c|356| <<nullb_device_power_store>> if (null_add_dev(dev)) {
+ *   - drivers/block/null_blk_main.c|2027| <<null_init>> ret = null_add_dev(dev);
+ */
 static int null_add_dev(struct nullb_device *dev)
 {
 	struct nullb *nullb;
@@ -1699,11 +2713,20 @@ static int null_add_dev(struct nullb_device *dev)
 		goto out_free_nullb;
 
 	if (dev->queue_mode == NULL_Q_MQ) {
+		/*
+		 * 如果是shared就使用全局的tag_set,
+		 * 否则就使用nullb自己的: nullb->__tag_set
+		 */
 		if (shared_tags) {
 			nullb->tag_set = &tag_set;
 			rv = 0;
 		} else {
 			nullb->tag_set = &nullb->__tag_set;
+			/*
+			 * called by:
+			 *   - drivers/block/null_blk_main.c|1810| <<null_add_dev>> rv = null_init_tag_set(nullb, nullb->tag_set);
+			 *   - drivers/block/null_blk_main.c|1943| <<null_init>> ret = null_init_tag_set(NULL, &tag_set);
+			 */
 			rv = null_init_tag_set(nullb, nullb->tag_set);
 		}
 
@@ -1732,13 +2755,21 @@ static int null_add_dev(struct nullb_device *dev)
 			goto out_cleanup_blk_queue;
 	}
 
+	/* Bandwidth throttle cap (in MB/s) */
 	if (dev->mbps) {
+		/*
+		 * 在以下使用NULLB_DEV_FL_THROTTLED:
+		 *   - drivers/block/null_blk_main.c|1543| <<null_handle_cmd>> if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+		 *   - drivers/block/null_blk_main.c|1772| <<null_del_dev>> if (test_bit(NULLB_DEV_FL_THROTTLED, &nullb->dev->flags)) {
+		 *   - drivers/block/null_blk_main.c|2156| <<null_add_dev>> set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
+		 */
 		set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
 		nullb_setup_bwtimer(nullb);
 	}
 
 	if (dev->cache_size > 0) {
 		set_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+		/* 把QUEUE_FLAG_WC和QUEUE_FLAG_FUA都设置上 */
 		blk_queue_write_cache(nullb->q, true, true);
 	}
 
@@ -1774,6 +2805,16 @@ static int null_add_dev(struct nullb_device *dev)
 		goto out_cleanup_zone;
 
 	mutex_lock(&lock);
+	/*
+	 * called by:
+	 *   - drivers/block/null_blk_main.c|1786| <<null_add_dev>> list_add_tail(&nullb->list, &nullb_list);
+	 *   - drivers/block/null_blk_main.c|1889| <<null_init>> while (!list_empty(&nullb_list)) {
+	 *   - drivers/block/null_blk_main.c|1890| <<null_init>> nullb = list_entry(nullb_list.next, struct nullb, list);
+	 *   - drivers/block/null_blk_main.c|1913| <<null_exit>> while (!list_empty(&nullb_list)) {
+	 *   - drivers/block/null_blk_main.c|1916| <<null_exit>> nullb = list_entry(nullb_list.next, struct nullb, list);
+	 *
+	 * 添加struct nullb
+	 */
 	list_add_tail(&nullb->list, &nullb_list);
 	mutex_unlock(&lock);
 
@@ -1801,6 +2842,9 @@ static int __init null_init(void)
 	struct nullb *nullb;
 	struct nullb_device *dev;
 
+	/*
+	 * "Block size (in bytes)"
+	 */
 	if (g_bs > PAGE_SIZE) {
 		pr_warn("invalid block size\n");
 		pr_warn("defaults block size to %lu\n", PAGE_SIZE);
@@ -1816,7 +2860,22 @@ static int __init null_init(void)
 		pr_err("legacy IO path no longer available\n");
 		return -EINVAL;
 	}
+	/*
+	 * g_submit_queues: "Number of submission queues"
+	 */
 	if (g_queue_mode == NULL_Q_MQ && g_use_per_node_hctx) {
+		/*
+		 * 在一下使用g_submit_queues:
+		 *   - drivers/block/null_blk_main.c|132| <<global>> module_param_named(submit_queues, g_submit_queues, int , 0444);
+		 *   - drivers/block/null_blk_main.c|622| <<null_alloc_dev>> dev->submit_queues = g_submit_queues;
+		 *   - drivers/block/null_blk_main.c|1995| <<null_init_tag_set>> g_submit_queues;
+		 *   - drivers/block/null_blk_main.c|2270| <<null_init>> if (g_submit_queues != nr_online_nodes) {
+		 *   - drivers/block/null_blk_main.c|2273| <<null_init>> g_submit_queues = nr_online_nodes;
+		 *   - drivers/block/null_blk_main.c|2275| <<null_init>> } else if (g_submit_queues > nr_cpu_ids)
+		 *   - drivers/block/null_blk_main.c|2276| <<null_init>> g_submit_queues = nr_cpu_ids;
+		 *   - drivers/block/null_blk_main.c|2277| <<null_init>> else if (g_submit_queues <= 0)
+		 *   - drivers/block/null_blk_main.c|2278| <<null_init>> g_submit_queues = 1;
+		 */
 		if (g_submit_queues != nr_online_nodes) {
 			pr_warn("submit_queues param is set to %u.\n",
 							nr_online_nodes);
@@ -1827,7 +2886,15 @@ static int __init null_init(void)
 	else if (g_submit_queues <= 0)
 		g_submit_queues = 1;
 
+	/*
+	 * "Share tag set between devices for blk-mq"
+	 */
 	if (g_queue_mode == NULL_Q_MQ && shared_tags) {
+		/*
+		 * called by:
+		 *   - drivers/block/null_blk_main.c|1810| <<null_add_dev>> rv = null_init_tag_set(nullb, nullb->tag_set);
+		 *   - drivers/block/null_blk_main.c|1943| <<null_init>> ret = null_init_tag_set(NULL, &tag_set);
+		 */
 		ret = null_init_tag_set(NULL, &tag_set);
 		if (ret)
 			return ret;
@@ -1842,6 +2909,9 @@ static int __init null_init(void)
 
 	mutex_init(&lock);
 
+	/*
+	 * 核心思想是在major_names[]找到一个可用的major
+	 */
 	null_major = register_blkdev(0, "nullb");
 	if (null_major < 0) {
 		ret = null_major;
@@ -1849,6 +2919,9 @@ static int __init null_init(void)
 	}
 
 	for (i = 0; i < nr_devices; i++) {
+		/*
+		 * 主要分配struct nullb_device和参数设置
+		 */
 		dev = null_alloc_dev();
 		if (!dev) {
 			ret = -ENOMEM;
diff --git a/drivers/block/null_blk_zoned.c b/drivers/block/null_blk_zoned.c
index ed34785dd64b..ea699c69f9a6 100644
--- a/drivers/block/null_blk_zoned.c
+++ b/drivers/block/null_blk_zoned.c
@@ -3,13 +3,32 @@
 #include "null_blk.h"
 
 /* zone_size in MBs to sectors. */
+/*
+ * 在以下使用ZONE_SIZE_SHIFT:
+ *   - drivers/block/null_blk_zoned.c|24| <<null_zone_init>> dev->zone_size_sects = dev->zone_size << ZONE_SIZE_SHIFT;
+ *
+ * 1往左移11位是2048
+ */
 #define ZONE_SIZE_SHIFT		11
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_zoned.c|78| <<null_report_zones>> first_zone = null_zone_no(dev, sector);
+ *   - drivers/block/null_blk_zoned.c|104| <<null_zone_valid_read_len>> struct blk_zone *zone = &dev->zones[null_zone_no(dev, sector)];
+ *   - drivers/block/null_blk_zoned.c|122| <<null_zone_write>> unsigned int zno = null_zone_no(dev, sector);
+ *   - drivers/block/null_blk_zoned.c|158| <<null_zone_mgmt>> struct blk_zone *zone = &dev->zones[null_zone_no(dev, sector)];
+ *
+ * 把sector转化为zone number
+ */
 static inline unsigned int null_zone_no(struct nullb_device *dev, sector_t sect)
 {
 	return sect >> ilog2(dev->zone_size_sects);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1746| <<null_add_dev>> rv = null_zone_init(dev);
+ */
 int null_zone_init(struct nullb_device *dev)
 {
 	sector_t dev_size = (sector_t)dev->size * 1024 * 1024;
@@ -61,11 +80,25 @@ int null_zone_init(struct nullb_device *dev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|566| <<null_free_dev>> null_zone_exit(dev);
+ *   - drivers/block/null_blk_main.c|1783| <<null_add_dev>> null_zone_exit(dev);
+ */
 void null_zone_exit(struct nullb_device *dev)
 {
+	/* struct blk_zone *zones; */
 	kvfree(dev->zones);
 }
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|181| <<blkdev_report_zones>> return disk->fops->report_zones(disk, sector, nr_zones, cb, data);
+ *   - block/blk-zoned.c|557| <<blk_revalidate_disk_zones>> ret = disk->fops->report_zones(disk, 0, UINT_MAX,
+ *   - drivers/md/dm.c|503| <<dm_blk_report_zones>> ret = tgt->type->report_zones(tgt, &args, nr_zones);
+ *
+ * struct block_device_operations null_ops.report_zones = null_report_zones()
+ */
 int null_report_zones(struct gendisk *disk, sector_t sector,
 		unsigned int nr_zones, report_zones_cb cb, void *data)
 {
@@ -97,6 +130,14 @@ int null_report_zones(struct gendisk *disk, sector_t sector,
 	return nr_zones;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1081| <<null_transfer>> valid_len = null_zone_valid_read_len(nullb,
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_transfer():
+ *      -> null_zone_valid_read_len()
+ */
 size_t null_zone_valid_read_len(struct nullb *nullb,
 				sector_t sector, unsigned int len)
 {
@@ -115,13 +156,35 @@ size_t null_zone_valid_read_len(struct nullb *nullb,
 	return (zone->wp - sector) << SECTOR_SHIFT;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_zoned.c|214| <<null_handle_zoned>> return null_zone_write(cmd, sector, nr_sectors);
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> null_handle_zoned()
+ *          -> null_zone_write()
+ */
 static blk_status_t null_zone_write(struct nullb_cmd *cmd, sector_t sector,
 		     unsigned int nr_sectors)
 {
 	struct nullb_device *dev = cmd->nq->dev;
+	/* 把sector转化为zone number */
 	unsigned int zno = null_zone_no(dev, sector);
 	struct blk_zone *zone = &dev->zones[zno];
 
+	/*
+	 * @BLK_ZONE_COND_NOT_WP: The zone has no write pointer, it is conventional.
+	 * @BLK_ZONE_COND_EMPTY: The zone is empty.
+	 * @BLK_ZONE_COND_IMP_OPEN: The zone is open, but not explicitly opened.
+	 * @BLK_ZONE_COND_EXP_OPEN: The zones was explicitly opened by an
+	 *                          OPEN ZONE command.
+	 * @BLK_ZONE_COND_CLOSED: The zone was [explicitly] closed after writing.
+	 * @BLK_ZONE_COND_FULL: The zone is marked as full, possibly by a zone
+	 *                      FINISH ZONE command.
+	 * @BLK_ZONE_COND_READONLY: The zone is read-only.
+	 * @BLK_ZONE_COND_OFFLINE: The zone is offline (sectors cannot be read/written).
+	 */
 	switch (zone->cond) {
 	case BLK_ZONE_COND_FULL:
 		/* Cannot write to a full zone */
@@ -138,6 +201,18 @@ static blk_status_t null_zone_write(struct nullb_cmd *cmd, sector_t sector,
 		if (zone->cond != BLK_ZONE_COND_EXP_OPEN)
 			zone->cond = BLK_ZONE_COND_IMP_OPEN;
 
+		/*
+		 * 关于zoned block device:
+		 * Zoned block devices are quite different than the block devices most people
+		 * are used to. The concept came from shingled magnetic recording (SMR)
+		 * devices, which allow much higher density storage, but that extra capacity
+		 * comes with a price: less flexibility. Zoned devices have regions (zones)
+		 * that can only be written sequentially; there is no random access for writes
+		 * to those zones. Linux already supports these devices, and filesystems are
+		 * adding support as well, but some applications may want a simpler, more
+		 * straightforward interface; that's what a new filesystem, zonefs, is
+		 * targeting.
+		 */
 		zone->wp += nr_sectors;
 		if (zone->wp == zone->start + zone->len)
 			zone->cond = BLK_ZONE_COND_FULL;
@@ -151,6 +226,15 @@ static blk_status_t null_zone_write(struct nullb_cmd *cmd, sector_t sector,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_zoned.c|220| <<null_handle_zoned>> return null_zone_mgmt(cmd, op, sector);
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> null_handle_zoned()
+ *          -> null_zone_mgmt()
+ */
 static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 				   sector_t sector)
 {
@@ -160,6 +244,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 
 	switch (op) {
 	case REQ_OP_ZONE_RESET_ALL:
+		/* reset all the zone present on the device */
 		for (i = 0; i < dev->nr_zones; i++) {
 			if (zone[i].type == BLK_ZONE_TYPE_CONVENTIONAL)
 				continue;
@@ -168,6 +253,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 		}
 		break;
 	case REQ_OP_ZONE_RESET:
+		/* reset a zone write pointer */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 
@@ -175,6 +261,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 		zone->wp = zone->start;
 		break;
 	case REQ_OP_ZONE_OPEN:
+		/* Open a zone */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 		if (zone->cond == BLK_ZONE_COND_FULL)
@@ -183,6 +270,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 		zone->cond = BLK_ZONE_COND_EXP_OPEN;
 		break;
 	case REQ_OP_ZONE_CLOSE:
+		/* Close a zone */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 		if (zone->cond == BLK_ZONE_COND_FULL)
@@ -194,6 +282,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 			zone->cond = BLK_ZONE_COND_CLOSED;
 		break;
 	case REQ_OP_ZONE_FINISH:
+		/* Transition a zone to full */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 
@@ -206,6 +295,18 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1289| <<null_handle_cmd>> cmd->error = null_handle_zoned(cmd, op, sector, nr_sectors);
+ *
+ * null_handle_zoned()有2种可能调用别人:
+ *  -> null_zone_write()
+ *  -> null_zone_mgmt()
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> null_handle_zoned()
+ */
 blk_status_t null_handle_zoned(struct nullb_cmd *cmd, enum req_opf op,
 			       sector_t sector, sector_t nr_sectors)
 {
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index 7ffd719d89de..9efe65ba3914 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -721,6 +721,27 @@ static const struct blk_mq_ops virtio_mq_ops = {
 static unsigned int virtblk_queue_depth;
 module_param_named(queue_depth, virtblk_queue_depth, uint, 0444);
 
+/*
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 static int virtblk_probe(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk;
@@ -905,6 +926,9 @@ static int virtblk_probe(struct virtio_device *vdev)
 	virtblk_update_capacity(vblk, false);
 	virtio_device_ready(vdev);
 
+	/*
+	 * 和gendisk/partition非常重要的接口!!
+	 */
 	device_add_disk(&vdev->dev, vblk->disk, virtblk_attr_groups);
 	return 0;
 
@@ -1030,6 +1054,7 @@ static int __init init(void)
 	if (!virtblk_wq)
 		return -ENOMEM;
 
+	/* 核心思想是在major_names[]找到一个可用的major */
 	major = register_blkdev(0, "virtblk");
 	if (major < 0) {
 		error = major;
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index 5dc32b72e7fa..69b9ea283b2e 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -479,6 +479,16 @@ static inline void nvme_clear_nvme_request(struct request *req)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|829| <<__nvme_submit_sync_cmd>> req = nvme_alloc_request(q, cmd, flags, qid);
+ *   - drivers/nvme/host/core.c|912| <<nvme_submit_user_cmd>> req = nvme_alloc_request(q, cmd, 0, NVME_QID_ANY);
+ *   - drivers/nvme/host/core.c|986| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+ *   - drivers/nvme/host/lightnvm.c|656| <<nvme_nvm_alloc_request>> rq = nvme_alloc_request(q, (struct nvme_command *)cmd, 0, NVME_QID_ANY);
+ *   - drivers/nvme/host/lightnvm.c|770| <<nvme_nvm_submit_user_cmd>> rq = nvme_alloc_request(q, (struct nvme_command *)vcmd, 0,
+ *   - drivers/nvme/host/pci.c|1346| <<nvme_timeout>> abort_req = nvme_alloc_request(dev->ctrl.admin_q, &cmd,
+ *   - drivers/nvme/host/pci.c|2255| <<nvme_delete_queue>> req = nvme_alloc_request(q, &cmd, BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
+ */
 struct request *nvme_alloc_request(struct request_queue *q,
 		struct nvme_command *cmd, blk_mq_req_flags_t flags, int qid)
 {
@@ -752,6 +762,15 @@ void nvme_cleanup_cmd(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/trace.h|47| <<global>> TRACE_EVENT(nvme_setup_cmd,
+ *   - drivers/nvme/host/fc.c|2342| <<nvme_fc_queue_rq>> ret = nvme_setup_cmd(ns, rq, sqe);
+ *   - drivers/nvme/host/pci.c|896| <<nvme_queue_rq>> ret = nvme_setup_cmd(ns, req, &cmnd);
+ *   - drivers/nvme/host/rdma.c|1759| <<nvme_rdma_queue_rq>> ret = nvme_setup_cmd(ns, rq, c);
+ *   - drivers/nvme/host/tcp.c|2106| <<nvme_tcp_setup_cmd_pdu>> ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+ *   - drivers/nvme/target/loop.c|144| <<nvme_loop_queue_rq>> ret = nvme_setup_cmd(ns, req, &iod->cmd);
+ */
 blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmd)
 {
@@ -763,6 +782,9 @@ blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 	switch (req_op(req)) {
 	case REQ_OP_DRV_IN:
 	case REQ_OP_DRV_OUT:
+		/*
+		 * 用blk_mq_rq_to_pdu(req)返回struct nvme_request
+		 */
 		memcpy(cmd, nvme_req(req)->cmd, sizeof(*cmd));
 		break;
 	case REQ_OP_FLUSH:
@@ -1018,6 +1040,37 @@ static void nvme_keep_alive_work(struct work_struct *work)
 	}
 }
 
+/*
+ * commit 038bd4cb6766c69b5b9c77507f389cc718a36842
+ * Author: Sagi Grimberg <sagi@grimberg.me>
+ * Date:   Mon Jun 13 16:45:28 2016 +0200
+ *
+ * nvme: add keep-alive support
+ *
+ * Periodic keep-alive is a mandatory feature in NVMe over Fabrics, and
+ * optional in NVMe 1.2.1 for PCIe.  This patch adds periodic keep-alive
+ * sent from the host to verify that the controller is still responsive
+ * and vice-versa.  The keep-alive timeout is user-defined (with
+ * keep_alive_tmo connection parameter) and defaults to 5 seconds.
+ *
+ * In order to avoid a race condition where the host sends a keep-alive
+ * competing with the target side keep-alive timeout expiration, the host
+ * adds a grace period of 10 seconds when publishing the keep-alive timeout
+ * to the target.
+ *
+ * In case a keep-alive failed (or timed out), a transport specific error
+ * recovery kicks in.
+ *
+ * For now only NVMe over Fabrics is wired up to support keep alive, but
+ * we can add PCIe support easily once controllers actually supporting it
+ * become available.
+ *
+ * Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
+ * Reviewed-by: Steve Wise <swise@chelsio.com>
+ * Signed-off-by: Christoph Hellwig <hch@lst.de>
+ * Reviewed-by: Keith Busch <keith.busch@intel.com>
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ */
 static void nvme_start_keep_alive(struct nvme_ctrl *ctrl)
 {
 	if (unlikely(ctrl->kato == 0))
@@ -1312,6 +1365,34 @@ static u32 nvme_known_admin_effects(u8 opcode)
 	return 0;
 }
 
+/*
+ * commit 84fef62d135b6e47b52f4e9280b5dbc5bb0050ba
+ * Author: Keith Busch <keith.busch@intel.com>
+ * Date:   Tue Nov 7 10:28:32 2017 -0700
+ *
+ * nvme: check admin passthru command effects
+ *
+ * The NVMe standard provides a command effects log page so the host may
+ * be aware of special requirements it may need to do for a particular
+ * command. For example, the command may need to run with IO quiesced to
+ * prevent timeouts or undefined behavior, or it may change the logical block
+ * formats that determine how the host needs to construct future commands.
+ *
+ * This patch saves the nvme command effects log page if the controller
+ * supports it, and performs appropriate actions before and after an admin
+ * passthrough command is completed. If the controller does not support the
+ * command effects log page, the driver will define the effects for known
+ * opcodes. The nvme format and santize are the only commands in this patch
+ * with known effects.
+ *
+ * Signed-off-by: Keith Busch <keith.busch@intel.com>
+ * Signed-off-by: Christoph Hellwig <hch@lst.de>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|1473| <<nvme_user_cmd>> effects = nvme_passthru_start(ctrl, ns, cmd.opcode);
+ *   - drivers/nvme/host/core.c|1520| <<nvme_user_cmd64>> effects = nvme_passthru_start(ctrl, ns, cmd.opcode);
+ */
 static u32 nvme_passthru_start(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 								u8 opcode)
 {
@@ -1357,6 +1438,11 @@ static void nvme_update_formats(struct nvme_ctrl *ctrl)
 	up_read(&ctrl->namespaces_rwsem);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1451| <<nvme_user_cmd>> nvme_passthru_end(ctrl, effects);
+ *   - drivers/nvme/host/core.c|1498| <<nvme_user_cmd64>> nvme_passthru_end(ctrl, effects);
+ */
 static void nvme_passthru_end(struct nvme_ctrl *ctrl, u32 effects)
 {
 	/*
@@ -2061,6 +2147,11 @@ const struct block_device_operations nvme_ns_head_ops = {
 };
 #endif /* CONFIG_NVME_MULTIPATH */
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2197| <<nvme_disable_ctrl>> return nvme_wait_ready(ctrl, ctrl->cap, false);
+ *   - drivers/nvme/host/core.c|2236| <<nvme_enable_ctrl>> return nvme_wait_ready(ctrl, ctrl->cap, true);
+ */
 static int nvme_wait_ready(struct nvme_ctrl *ctrl, u64 cap, bool enabled)
 {
 	unsigned long timeout =
@@ -2731,6 +2822,15 @@ static int nvme_get_effects_log(struct nvme_ctrl *ctrl)
  * register in our nvme_ctrl structure.  This should be called as soon as
  * the admin queue is fully up and running.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1408| <<nvme_passthru_end>> nvme_init_identify(ctrl);
+ *   - drivers/nvme/host/fc.c|2681| <<nvme_fc_create_association>> ret = nvme_init_identify(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|2628| <<nvme_reset_work>> result = nvme_init_identify(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|835| <<nvme_rdma_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1752| <<nvme_tcp_configure_admin_queue>> error = nvme_init_identify(ctrl);
+ *   - drivers/nvme/target/loop.c|389| <<nvme_loop_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+ */
 int nvme_init_identify(struct nvme_ctrl *ctrl)
 {
 	struct nvme_id_ctrl *id;
diff --git a/drivers/nvme/host/fault_inject.c b/drivers/nvme/host/fault_inject.c
index 1352159733b0..541235af7032 100644
--- a/drivers/nvme/host/fault_inject.c
+++ b/drivers/nvme/host/fault_inject.c
@@ -12,9 +12,35 @@ static DECLARE_FAULT_ATTR(fail_default_attr);
 /* optional fault injection attributes boot time option:
  * nvme_core.fail_request=<interval>,<probability>,<space>,<times>
  */
+/*
+ * "nvme_core.fail_request=20,100,0,5":
+ * [    3.132027] FAULT_INJECTION: forcing a failure.
+ *                name fault_inject, interval 20, probability 100, space 0, times 3
+ * [    3.132029] CPU: 0 PID: 247 Comm: systemd-udevd Not tainted 5.5.0+ #3
+ * [    3.132030] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-59-gc9ba5276e321-prebuilt.qemu.org 04/01/2014
+ * [    3.132030] Call Trace:
+ * [    3.132032]  <IRQ>
+ * [    3.132036]  dump_stack+0x50/0x6b
+ * [    3.132038]  should_fail+0x13c/0x160
+ * [    3.132040]  nvme_should_fail+0x30/0xa0
+ * [    3.132042]  nvme_irq+0x136/0x210
+ * [    3.132044]  __handle_irq_event_percpu+0x3b/0x180
+ * [    3.132045]  handle_irq_event_percpu+0x2b/0x70
+ * [    3.132046]  handle_irq_event+0x22/0x40
+ * [    3.132047]  handle_edge_irq+0x75/0x190
+ * [    3.132049]  do_IRQ+0x41/0xd0
+ * [    3.132064]  common_interrupt+0xf/0xf
+ * [    3.132064]  </IRQ>
+ * [    3.132065] RIP: 0033:0x557263ceebe0
+ */
 static char *fail_request;
 module_param(fail_request, charp, 0000);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3665| <<nvme_alloc_ns>> nvme_fault_inject_init(&ns->fault_inject, ns->disk->disk_name);
+ *   - drivers/nvme/host/core.c|4194| <<nvme_init_ctrl>> nvme_fault_inject_init(&ctrl->fault_inject, dev_name(ctrl->device));
+ */
 void nvme_fault_inject_init(struct nvme_fault_inject *fault_inj,
 			    const char *dev_name)
 {
@@ -48,12 +74,21 @@ void nvme_fault_inject_init(struct nvme_fault_inject *fault_inj,
 	debugfs_create_bool("dont_retry", 0600, dir, &fault_inj->dont_retry);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3692| <<nvme_ns_remove>> nvme_fault_inject_fini(&ns->fault_inject);
+ *   - drivers/nvme/host/core.c|4095| <<nvme_uninit_ctrl>> nvme_fault_inject_fini(&ctrl->fault_inject);
+ */
 void nvme_fault_inject_fini(struct nvme_fault_inject *fault_inject)
 {
 	/* remove debugfs directories */
 	debugfs_remove_recursive(fault_inject->parent);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/nvme.h|481| <<nvme_end_request>> nvme_should_fail(req);
+ */
 void nvme_should_fail(struct request *req)
 {
 	struct gendisk *disk = req->rq_disk;
diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 797c18337d96..0ea2967762f9 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -64,6 +64,10 @@ void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|296| <<nvme_complete_rq>> nvme_failover_req(req);
+ */
 void nvme_failover_req(struct request *req)
 {
 	struct nvme_ns *ns = req->q->queuedata;
@@ -657,6 +661,10 @@ static int nvme_set_ns_ana_state(struct nvme_ctrl *ctrl,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3659| <<nvme_alloc_ns>> nvme_mpath_add_disk(ns, id);
+ */
 void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id)
 {
 	if (nvme_ctrl_use_ana(ns->ctrl)) {
@@ -686,6 +694,10 @@ void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 	put_disk(head->disk);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2911| <<nvme_init_identify>> ret = nvme_mpath_init(ctrl, id);
+ */
 int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 {
 	int error;
@@ -694,6 +706,7 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	if (!multipath || !ctrl->subsys || !(ctrl->subsys->cmic & (1 << 3)))
 		return 0;
 
+	/* ANA = Asymmetric Namespace Access */
 	ctrl->anacap = id->anacap;
 	ctrl->anatt = id->anatt;
 	ctrl->nanagrpid = le32_to_cpu(id->nanagrpid);
@@ -715,6 +728,10 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	}
 
 	INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+	/*
+	 * 根据最新的commit, 这里似乎有leak, 多次分配
+	 * 最好调用个kfree()
+	 */
 	ctrl->ana_log_buf = kmalloc(ctrl->ana_log_size, GFP_KERNEL);
 	if (!ctrl->ana_log_buf) {
 		error = -ENOMEM;
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 1024fec7914c..bc5f5814df06 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -208,6 +208,28 @@ struct nvme_ctrl {
 	struct device ctrl_device;
 	struct device *device;	/* char device */
 	struct cdev cdev;
+	/*
+	 * 在以下使用reset_work:
+	 *   - drivers/nvme/host/core.c|133| <<nvme_try_sched_reset>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|143| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|155| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|169| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|2944| <<nvme_fc_reset_ctrl_work>> container_of(work, struct nvme_fc_ctrl, ctrl.reset_work);
+	 *   - drivers/nvme/host/fc.c|3107| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/fc.c|3204| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2572| <<nvme_reset_work>> container_of(work, struct nvme_dev, ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2798| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2830| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/pci.c|2902| <<nvme_reset_done>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2929| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|3094| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/rdma.c|1906| <<nvme_rdma_reset_ctrl_work>> container_of(work, struct nvme_rdma_ctrl, ctrl.reset_work);
+	 *   - drivers/nvme/host/rdma.c|2018| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|1945| <<nvme_reset_ctrl_work>> container_of(work, struct nvme_ctrl, reset_work);
+	 *   - drivers/nvme/host/tcp.c|2298| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|446| <<nvme_loop_reset_ctrl_work>> container_of(work, struct nvme_loop_ctrl, ctrl.reset_work);
+	 *   - drivers/nvme/target/loop.c|580| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	struct work_struct reset_work;
 	struct work_struct delete_work;
 	wait_queue_head_t state_wq;
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index 365a2ddbeaa7..8335d841a10f 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -165,6 +165,14 @@ struct nvme_queue {
 	spinlock_t sq_lock;
 	void *sq_cmds;
 	 /* only used for poll queues: */
+	/*
+	 * 在以下使用cq_poll_lock:
+	 *   - drivers/nvme/host/pci.c|1063| <<nvme_poll_irqdisable>> spin_lock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1065| <<nvme_poll_irqdisable>> spin_unlock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1106| <<nvme_poll>> spin_lock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1108| <<nvme_poll>> spin_unlock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1500| <<nvme_alloc_queue>> spin_lock_init(&nvmeq->cq_poll_lock);
+	 */
 	spinlock_t cq_poll_lock ____cacheline_aligned_in_smp;
 	volatile struct nvme_completion *cqes;
 	struct blk_mq_tags **tags;
@@ -948,6 +956,10 @@ static inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)
 		writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|983| <<nvme_complete_cqes>> nvme_handle_cqe(nvmeq, start);
+ */
 static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 {
 	volatile struct nvme_completion *cqe = &nvmeq->cqes[idx];
@@ -977,6 +989,12 @@ static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 	nvme_end_request(req, cqe->status, cqe->result);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1032| <<nvme_irq>> nvme_complete_cqes(nvmeq, start, end);
+ *   - drivers/nvme/host/pci.c|1072| <<nvme_poll_irqdisable>> nvme_complete_cqes(nvmeq, start, end);
+ *   - drivers/nvme/host/pci.c|1110| <<nvme_poll>> nvme_complete_cqes(nvmeq, start, end);
+ */
 static void nvme_complete_cqes(struct nvme_queue *nvmeq, u16 start, u16 end)
 {
 	while (start != end) {
@@ -986,6 +1004,10 @@ static void nvme_complete_cqes(struct nvme_queue *nvmeq, u16 start, u16 end)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1033| <<nvme_process_cq>> nvme_update_cq_head(nvmeq);
+ */
 static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
 {
 	if (nvmeq->cq_head == nvmeq->q_depth - 1) {
@@ -996,6 +1018,13 @@ static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1046| <<nvme_irq>> nvme_process_cq(nvmeq, &start, &end, -1);
+ *   - drivers/nvme/host/pci.c|1082| <<nvme_poll_irqdisable>> found = nvme_process_cq(nvmeq, &start, &end, tag);
+ *   - drivers/nvme/host/pci.c|1086| <<nvme_poll_irqdisable>> found = nvme_process_cq(nvmeq, &start, &end, tag);
+ *   - drivers/nvme/host/pci.c|1125| <<nvme_poll>> found = nvme_process_cq(nvmeq, &start, &end, -1);
+ */
 static inline int nvme_process_cq(struct nvme_queue *nvmeq, u16 *start,
 				  u16 *end, unsigned int tag)
 {
@@ -1073,6 +1102,27 @@ static int nvme_poll_irqdisable(struct nvme_queue *nvmeq, unsigned int tag)
 	return found;
 }
 
+/*
+ * 激活poll的方法:
+ *
+ * 激活io_poll:
+ * # echo 1 > /sys/block/nvme0n1/queue/io_poll
+ *
+ * 在fio中使用pvsync2加上hipri:
+ * # fio -name iops -rw=read -bs=4k -runtime=60 -iodepth 32 -filename /dev/nvme0n1 -ioengine pvsync2 -direct=1 -hipri=1
+ *
+ * [0] nvme_poll
+ * [0] blk_poll
+ * [0] __blkdev_direct_IO_simple
+ * [0] blkdev_direct_IO
+ * [0] generic_file_read_iter
+ * [0] do_iter_readv_writev
+ * [0] do_iter_read
+ * [0] vfs_readv
+ * [0] do_preadv
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int nvme_poll(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
@@ -1232,6 +1282,10 @@ static void nvme_warn_reset(struct nvme_dev *dev, u32 csts)
 			 csts, result);
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.timeout = nvme_timeout()
+ * struct blk_mq_ops nvme_mq_ops.timeout = nvme_timeout()
+ */
 static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -1492,6 +1546,33 @@ static int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)
 	return -ENOMEM;
 }
 
+/*
+ * qemu下nvme的一个例子:
+ * [0] msi_domain_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [    0.682898] orabug: msi_domain_set_affinity() mask=f
+ * [    0.695652] orabug: msi_domain_set_affinity() mask=f
+ * [    0.706281] orabug: msi_domain_set_affinity() mask=1
+ * [    0.715123] orabug: msi_domain_set_affinity() mask=2
+ * [    0.724427] orabug: msi_domain_set_affinity() mask=4
+ * [    0.733339] orabug: msi_domain_set_affinity() mask=8
+ *
+ * called by:
+ *   - drivers/nvme/host/pci.c|1555| <<nvme_create_queue>> result = queue_request_irq(nvmeq);
+ *   - drivers/nvme/host/pci.c|1703| <<nvme_pci_configure_admin_queue>> result = queue_request_irq(nvmeq);
+ *   - drivers/nvme/host/pci.c|2160| <<nvme_setup_io_queues>> result = queue_request_irq(adminq);
+ */
 static int queue_request_irq(struct nvme_queue *nvmeq)
 {
 	struct pci_dev *pdev = to_pci_dev(nvmeq->dev->dev);
@@ -2045,8 +2126,26 @@ static void nvme_calc_irq_sets(struct irq_affinity *affd, unsigned int nrirqs)
 	affd->nr_sets = nr_read_queues ? 2 : 1;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2146| <<nvme_setup_io_queues>> result = nvme_setup_irqs(dev, nr_io_queues);
+ */
 static int nvme_setup_irqs(struct nvme_dev *dev, unsigned int nr_io_queues)
 {
+	/*
+	 * struct irq_affinity - Description for automatic irq affinity assignements
+	 * @pre_vectors:        Don't apply affinity to @pre_vectors at beginning of
+	 *                      the MSI(-X) vector space
+	 * @post_vectors:       Don't apply affinity to @post_vectors at end of
+	 *                      the MSI(-X) vector space
+	 * @nr_sets:            The number of interrupt sets for which affinity
+	 *                      spreading is required
+	 * @set_size:           Array holding the size of each interrupt set
+	 * @calc_sets:          Callback for calculating the number and size
+	 *                      of interrupt sets
+	 * @priv:               Private data for usage by @calc_sets, usually a
+	 *                      pointer to driver/device specific data.
+	 */
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 	struct irq_affinity affd = {
 		.pre_vectors	= 1,
@@ -2521,6 +2620,20 @@ static void nvme_remove_dead_ctrl(struct nvme_dev *dev)
 		nvme_put_ctrl(&dev->ctrl);
 }
 
+/*
+ * pci在以下调用:
+ *   - drivers/nvme/host/core.c|133| <<nvme_try_sched_reset>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+ *   - drivers/nvme/host/core.c|143| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+ *   - drivers/nvme/host/core.c|155| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/core.c|169| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/pci.c|2798| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|2902| <<nvme_reset_done>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|2929| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|3094| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+ *
+ * 在以下使用nvme_reset_work():
+ *   - drivers/nvme/host/pci.c|2830| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+ */
 static void nvme_reset_work(struct work_struct *work)
 {
 	struct nvme_dev *dev =
diff --git a/drivers/nvme/target/loop.c b/drivers/nvme/target/loop.c
index 4df4ebde208a..78c72579e2a1 100644
--- a/drivers/nvme/target/loop.c
+++ b/drivers/nvme/target/loop.c
@@ -564,6 +564,9 @@ static struct nvmet_port *nvme_loop_find_port(struct nvme_ctrl *ctrl)
 	return found;
 }
 
+/*
+ * struct nvmf_transport_ops nvme_loop_transport.create_ctrl = nvme_loop_create_ctrl()
+ */
 static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 		struct nvmf_ctrl_options *opts)
 {
diff --git a/drivers/pci/msi.c b/drivers/pci/msi.c
index c7709e49f0e4..b2825afe777d 100644
--- a/drivers/pci/msi.c
+++ b/drivers/pci/msi.c
@@ -1190,6 +1190,20 @@ int pci_alloc_irq_vectors_affinity(struct pci_dev *dev, unsigned int min_vecs,
 				   unsigned int max_vecs, unsigned int flags,
 				   struct irq_affinity *affd)
 {
+	/*
+	 * struct irq_affinity - Description for automatic irq affinity assignements
+	 * @pre_vectors:        Don't apply affinity to @pre_vectors at beginning of
+	 *                      the MSI(-X) vector space
+	 * @post_vectors:       Don't apply affinity to @post_vectors at end of
+	 *                      the MSI(-X) vector space
+	 * @nr_sets:            The number of interrupt sets for which affinity
+	 *                      spreading is required
+	 * @set_size:           Array holding the size of each interrupt set
+	 * @calc_sets:          Callback for calculating the number and size
+	 *                      of interrupt sets
+	 * @priv:               Private data for usage by @calc_sets, usually a
+	 *                      pointer to driver/device specific data.
+	 */
 	struct irq_affinity msi_default_affd = {0};
 	int msix_vecs = -ENOSPC;
 	int msi_vecs = -ENOSPC;
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 867c7ebd3f10..3d74c50dff79 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -91,6 +91,22 @@ struct vring_virtqueue {
 	bool packed_ring;
 
 	/* Is DMA API used? */
+	/*
+	 * 在以下设置use_dma_api:
+	 *   - drivers/virtio/virtio_ring.c|1610| <<vring_create_virtqueue_packed>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 *   - drivers/virtio/virtio_ring.c|2084| <<__vring_new_virtqueue>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 * 在以下使用use_dma_api:
+	 *   - drivers/virtio/virtio_ring.c|329| <<vring_map_one_sg>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|346| <<vring_map_single>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|356| <<vring_mapping_error>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|372| <<vring_unmap_one_split>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|919| <<vring_unmap_state_packed>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|942| <<vring_unmap_desc_packed>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|1034| <<virtqueue_add_indirect_packed>> if (vq->use_dma_api) {
+	 *   - drivers/virtio/virtio_ring.c|1165| <<virtqueue_add_packed>> if (unlikely(vq->use_dma_api)) {
+	 *   - drivers/virtio/virtio_ring.c|1297| <<detach_buf_packed>> if (unlikely(vq->use_dma_api)) {
+	 *   - drivers/virtio/virtio_ring.c|1314| <<detach_buf_packed>> if (vq->use_dma_api) {
+	 */
 	bool use_dma_api;
 
 	/* Can we use weak barriers? */
@@ -238,6 +254,14 @@ static inline bool virtqueue_use_indirect(struct virtqueue *_vq,
  * unconditionally on data path.
  */
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|281| <<virtio_max_dma_size>> if (vring_use_dma_api(vdev))
+ *   - drivers/virtio/virtio_ring.c|291| <<vring_alloc_queue>> if (vring_use_dma_api(vdev)) {
+ *   - drivers/virtio/virtio_ring.c|324| <<vring_free_queue>> if (vring_use_dma_api(vdev))
+ *   - drivers/virtio/virtio_ring.c|1626| <<bool>> vq->use_dma_api = vring_use_dma_api(vdev);
+ *   - drivers/virtio/virtio_ring.c|2100| <<bool>> vq->use_dma_api = vring_use_dma_api(vdev);
+ */
 static bool vring_use_dma_api(struct virtio_device *vdev)
 {
 	if (!virtio_has_iommu_quirk(vdev))
diff --git a/fs/block_dev.c b/fs/block_dev.c
index 69bf2fb6f7cd..10e87c18fdbe 100644
--- a/fs/block_dev.c
+++ b/fs/block_dev.c
@@ -1508,6 +1508,15 @@ EXPORT_SYMBOL(bd_set_size);
 
 static void __blkdev_put(struct block_device *bdev, fmode_t mode, int for_part);
 
+/*
+ * called by:
+ *   - block/ioctl.c|168| <<blkdev_reread_part>> ret = bdev_disk_changed(bdev, false);
+ *   - drivers/block/loop.c|644| <<loop_reread_partitions>> rc = bdev_disk_changed(bdev, false);
+ *   - drivers/block/loop.c|1171| <<__loop_clr_fd>> err = bdev_disk_changed(bdev, false);
+ *   - drivers/s390/block/dasd_genhd.c|120| <<dasd_scan_partitions>> rc = bdev_disk_changed(bdev, false);
+ *   - fs/block_dev.c|1634| <<__blkdev_get>> bdev_disk_changed(bdev, ret == -ENOMEDIUM);
+ *   - fs/block_dev.c|1669| <<__blkdev_get>> bdev_disk_changed(bdev, ret == -ENOMEDIUM);
+ */
 int bdev_disk_changed(struct block_device *bdev, bool invalidate)
 {
 	struct gendisk *disk = bdev->bd_disk;
diff --git a/fs/direct-io.c b/fs/direct-io.c
index 00b4d15bb811..356584570bc3 100644
--- a/fs/direct-io.c
+++ b/fs/direct-io.c
@@ -940,6 +940,10 @@ static inline void dio_zero_block(struct dio *dio, struct dio_submit *sdio,
  * it should set b_size to PAGE_SIZE or more inside get_block().  This gives
  * fine alignment but still allows this function to work in PAGE_SIZE units.
  */
+/*
+ * called by:
+ *   - fs/direct-io.c|1311| <<do_blockdev_direct_IO>> retval = do_direct_IO(dio, &sdio, &map_bh);
+ */
 static int do_direct_IO(struct dio *dio, struct dio_submit *sdio,
 			struct buffer_head *map_bh)
 {
@@ -1141,6 +1145,10 @@ static inline int drop_refcount(struct dio *dio)
  * individual fields and will generate much worse code. This is important
  * for the whole file.
  */
+/*
+ * called by:
+ *   - fs/direct-io.c|1393| <<__blockdev_direct_IO>> return do_blockdev_direct_IO(iocb, inode, bdev, iter, get_block,
+ */
 static inline ssize_t
 do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 		      struct block_device *bdev, struct iov_iter *iter,
@@ -1372,6 +1380,13 @@ do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 	return retval;
 }
 
+/*
+ * called by:
+ *   - fs/btrfs/inode.c|8741| <<btrfs_direct_IO>> ret = __blockdev_direct_IO(iocb, inode,
+ *   - fs/f2fs/data.c|2972| <<f2fs_direct_IO>> err = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - fs/ocfs2/aops.c|2470| <<ocfs2_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - include/linux/fs.h|3171| <<blockdev_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev, iter,
+ */
 ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     struct block_device *bdev, struct iov_iter *iter,
 			     get_block_t get_block,
diff --git a/fs/ext4/file.c b/fs/ext4/file.c
index 6a7293a5cda2..a3a0432c6db4 100644
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@ -36,6 +36,11 @@
 #include "acl.h"
 #include "truncate.h"
 
+/*
+ * called by:
+ *   - fs/ext4/file.c|64| <<ext4_dio_read_iter>> if (!ext4_dio_supported(inode)) {
+ *   - fs/ext4/file.c|382| <<ext4_dio_write_iter>> if (!ext4_dio_supported(inode)) {
+ */
 static bool ext4_dio_supported(struct inode *inode)
 {
 	if (IS_ENABLED(CONFIG_FS_ENCRYPTION) && IS_ENCRYPTED(inode))
diff --git a/fs/iomap/direct-io.c b/fs/iomap/direct-io.c
index 23837926c0c5..d5663a649119 100644
--- a/fs/iomap/direct-io.c
+++ b/fs/iomap/direct-io.c
@@ -58,6 +58,11 @@ int iomap_dio_iopoll(struct kiocb *kiocb, bool spin)
 }
 EXPORT_SYMBOL_GPL(iomap_dio_iopoll);
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|194| <<iomap_dio_zero>> iomap_dio_submit_bio(dio, iomap, bio);
+ *   - fs/iomap/direct-io.c|306| <<iomap_dio_bio_actor>> iomap_dio_submit_bio(dio, iomap, bio);
+ */
 static void iomap_dio_submit_bio(struct iomap_dio *dio, struct iomap *iomap,
 		struct bio *bio)
 {
@@ -174,6 +179,11 @@ static void iomap_dio_bio_end_io(struct bio *bio)
 	}
 }
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|256| <<iomap_dio_bio_actor>> iomap_dio_zero(dio, iomap, pos - pad, pad);
+ *   - fs/iomap/direct-io.c|321| <<iomap_dio_bio_actor>> iomap_dio_zero(dio, iomap, pos, fs_block_size - pad);
+ */
 static void
 iomap_dio_zero(struct iomap_dio *dio, struct iomap *iomap, loff_t pos,
 		unsigned len)
@@ -194,6 +204,11 @@ iomap_dio_zero(struct iomap_dio *dio, struct iomap *iomap, loff_t pos,
 	iomap_dio_submit_bio(dio, iomap, bio);
 }
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|380| <<iomap_dio_actor>> return iomap_dio_bio_actor(inode, pos, length, dio, iomap);
+ *   - fs/iomap/direct-io.c|382| <<iomap_dio_actor>> return iomap_dio_bio_actor(inode, pos, length, dio, iomap);
+ */
 static loff_t
 iomap_dio_bio_actor(struct inode *inode, loff_t pos, loff_t length,
 		struct iomap_dio *dio, struct iomap *iomap)
@@ -363,6 +378,10 @@ iomap_dio_inline_actor(struct inode *inode, loff_t pos, loff_t length,
 	return copied;
 }
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|523| <<iomap_dio_rw>> iomap_dio_actor);
+ */
 static loff_t
 iomap_dio_actor(struct inode *inode, loff_t pos, loff_t length,
 		void *data, struct iomap *iomap, struct iomap *srcmap)
@@ -397,6 +416,15 @@ iomap_dio_actor(struct inode *inode, loff_t pos, loff_t length,
  * may be pure data writes. In that case, we still need to do a full data sync
  * completion.
  */
+/*
+ * called by:
+ *   - fs/ext4/file.c|77| <<ext4_dio_read_iter>> ret = iomap_dio_rw(iocb, to, &ext4_iomap_ops, NULL,
+ *   - fs/ext4/file.c|438| <<ext4_dio_write_iter>> ret = iomap_dio_rw(iocb, from, &ext4_iomap_ops, &ext4_dio_write_ops,
+ *   - fs/gfs2/file.c|774| <<gfs2_file_direct_read>> ret = iomap_dio_rw(iocb, to, &gfs2_iomap_ops, NULL,
+ *   - fs/gfs2/file.c|810| <<gfs2_file_direct_write>> ret = iomap_dio_rw(iocb, from, &gfs2_iomap_ops, NULL,
+ *   - fs/xfs/xfs_file.c|191| <<xfs_file_dio_aio_read>> ret = iomap_dio_rw(iocb, to, &xfs_read_iomap_ops, NULL,
+ *   - fs/xfs/xfs_file.c|554| <<xfs_file_dio_aio_write>> ret = iomap_dio_rw(iocb, from, &xfs_direct_write_iomap_ops,
+ */
 ssize_t
 iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
diff --git a/include/linux/badblocks.h b/include/linux/badblocks.h
index 2426276b9bd3..1d7eeb551403 100644
--- a/include/linux/badblocks.h
+++ b/include/linux/badblocks.h
@@ -8,13 +8,40 @@
 #include <linux/stddef.h>
 #include <linux/types.h>
 
+/*
+ * 在以下使用BB_LEN_MASK:
+ *   - include/linux/badblocks.h|16| <<BB_LEN>> #define BB_LEN(x) (((x) & BB_LEN_MASK) + 1)
+ *
+ * 最后9个bit都是1 (表示1个range的长度, 最多512个sector)
+ */
 #define BB_LEN_MASK	(0x00000000000001FFULL)
+/*
+ * 在以下使用BB_OFFSET_MASK:
+ *   - include/linux/badblocks.h|15| <<BB_OFFSET>> #define BB_OFFSET(x) (((x) & BB_OFFSET_MASK) >> 9)
+ */
 #define BB_OFFSET_MASK	(0x7FFFFFFFFFFFFE00ULL)
+/*
+ * 在以下使用BB_ACK_MASK:
+ *   - include/linux/badblocks.h|17| <<BB_ACK>> #define BB_ACK(x) (!!((x) & BB_ACK_MASK))
+ */
 #define BB_ACK_MASK	(0x8000000000000000ULL)
 #define BB_MAX_LEN	512
 #define BB_OFFSET(x)	(((x) & BB_OFFSET_MASK) >> 9)
 #define BB_LEN(x)	(((x) & BB_LEN_MASK) + 1)
 #define BB_ACK(x)	(!!((x) & BB_ACK_MASK))
+/*
+ * 在以下调用BB_MAKE():
+ *   - block/badblocks.c|244| <<badblocks_set>> p[lo] = BB_MAKE(a, e-a, ack);
+ *   - block/badblocks.c|251| <<badblocks_set>> p[lo] = BB_MAKE(a, BB_MAX_LEN, ack);
+ *   - block/badblocks.c|276| <<badblocks_set>> p[hi] = BB_MAKE(a, e-a, ack);
+ *   - block/badblocks.c|279| <<badblocks_set>> p[hi] = BB_MAKE(a, BB_MAX_LEN, ack);
+ *   - block/badblocks.c|299| <<badblocks_set>> p[lo] = BB_MAKE(BB_OFFSET(p[lo]), newlen, ack);
+ *   - block/badblocks.c|322| <<badblocks_set>> p[hi] = BB_MAKE(s, this_sectors, acknowledged);
+ *   - block/badblocks.c|408| <<badblocks_clear>> p[lo] = BB_MAKE(a, s-a, ack);
+ *   - block/badblocks.c|411| <<badblocks_clear>> p[lo] = BB_MAKE(target, end - target, ack);
+ *   - block/badblocks.c|425| <<badblocks_clear>> p[lo] = BB_MAKE(start, s - start, ack);
+ *   - block/badblocks.c|471| <<ack_all_badblocks>> p[i] = BB_MAKE(start, len, 1);
+ */
 #define BB_MAKE(a, l, ack) (((a)<<9) | ((l)-1) | ((u64)(!!(ack)) << 63))
 
 /* Bad block numbers are stored sorted in a single page.
@@ -22,18 +49,52 @@
  * 54 bits are sector number, 9 bits are extent size,
  * 1 bit is an 'acknowledged' flag.
  */
+/*
+ * 在以下使用MAX_BADBLOCKS:
+ *   - block/badblocks.c|309| <<badblocks_set>> if (bb->count >= MAX_BADBLOCKS) {
+ *   - block/badblocks.c|402| <<badblocks_clear>> if (bb->count >= MAX_BADBLOCKS) {
+ */
 #define MAX_BADBLOCKS	(PAGE_SIZE/8)
 
 struct badblocks {
 	struct device *dev;	/* set by devm_init_badblocks */
+	/*
+	 * 在以下修改count:
+	 *   - block/badblocks.c|294| <<badblocks_set>> bb->count--;
+	 *   - block/badblocks.c|310| <<badblocks_set>> bb->count++;
+	 *   - block/badblocks.c|399| <<badblocks_clear>> bb->count++;
+	 *   - block/badblocks.c|428| <<badblocks_clear>> bb->count -= (hi - lo - 1);
+	 *   - block/badblocks.c|581| <<__badblocks_init>> bb->count = 0;
+	 */
 	int count;		/* count of bad blocks */
+	/*
+	 * 在以下使用unacked_exist:
+	 *   - block/badblocks.c|148| <<badblocks_update_acked>> if (!bb->unacked_exist)
+	 *   - block/badblocks.c|159| <<badblocks_update_acked>> bb->unacked_exist = 0;
+	 *   - block/badblocks.c|330| <<badblocks_set>> bb->unacked_exist = 1;
+	 *   - block/badblocks.c|462| <<ack_all_badblocks>> if (bb->changed == 0 && bb->unacked_exist) {
+	 *   - block/badblocks.c|474| <<ack_all_badblocks>> bb->unacked_exist = 0;
+	 *   - block/badblocks.c|537| <<badblocks_show>> bb->unacked_exist = 0;
+	 *   - drivers/md/md.c|2810| <<state_show>> rdev->badblocks.unacked_exist))
+	 *   - drivers/md/md.c|2819| <<state_show>> (rdev->badblocks.unacked_exist
+	 *   - drivers/md/md.c|2903| <<state_store>> rdev->badblocks.unacked_exist) {
+	 */
 	int unacked_exist;	/* there probably are unacknowledged
 				 * bad blocks.  This is only cleared
 				 * when a read discovers none
 				 */
+	/*
+	 * 设置shift的地方:
+	 *   - block/badblocks.c|600| <<__badblocks_init>> bb->shift = 0;
+	 *   - block/badblocks.c|602| <<__badblocks_init>> bb->shift = -1;
+	 *   - block/badblocks.c|614| <<__badblocks_init>> bb->shift = -1;
+	 */
 	int shift;		/* shift from sectors to block size
 				 * a -ve shift means badblocks are
 				 * disabled.*/
+	/*
+	 * 在__badblocks_init()中分配PAGE_SIZE
+	 */
 	u64 *page;		/* badblock list */
 	int changed;
 	seqlock_t lock;
diff --git a/include/linux/bio.h b/include/linux/bio.h
index 853d92ceee64..c1c44f9e943f 100644
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@ -827,6 +827,12 @@ static inline int bio_integrity_add_page(struct bio *bio, struct page *page,
  * must be found by the caller. This is different than IRQ driven IO, where
  * it's safe to wait for IO to complete.
  */
+/*
+ * called by:
+ *   - fs/block_dev.c|249| <<__blkdev_direct_IO_simple>> bio_set_polled(&bio, iocb);
+ *   - fs/block_dev.c|410| <<__blkdev_direct_IO>> bio_set_polled(bio, iocb);
+ *   - fs/iomap/direct-io.c|67| <<iomap_dio_submit_bio>> bio_set_polled(bio, dio->iocb);
+ */
 static inline void bio_set_polled(struct bio *bio, struct kiocb *kiocb)
 {
 	bio->bi_opf |= REQ_HIPRI;
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 11cfd6470b1a..ef5e7d2cb6da 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -23,6 +23,22 @@ struct blk_mq_hw_ctx {
 		 * resources) could not be sent to the hardware. As soon as the
 		 * driver can send new requests, requests at this list will
 		 * be sent first for a fairer dispatch.
+		 */
+		/*
+		 * 在以下添加rq到hctx->dispatch:
+		 *   - block/blk-mq-sched.c|376| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1414| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1794| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2373| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 * 在以下使用hctx->dispatch:
+		 *   - block/blk-mq.c|2491| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+		 *   - block/blk-mq-sched.c|199| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *   - block/blk-mq-sched.c|196| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+		 *   - block/blk-mq-sched.c|198| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+		 *   - block/blk-mq.c|82| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+		 *   - block/blk-mq-debugfs.c|363| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+		 *   - block/blk-mq-debugfs.c|370| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+
 		 */
 		struct list_head	dispatch;
 		 /**
@@ -35,6 +51,14 @@ struct blk_mq_hw_ctx {
 	/**
 	 * @run_work: Used for scheduling a hardware queue run at a later time.
 	 */
+	/*
+	 * 在以下使用run_work:
+	 *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 */
 	struct delayed_work	run_work;
 	/** @cpumask: Map of available CPUs where this hctx can run. */
 	cpumask_var_t		cpumask;
@@ -86,6 +110,15 @@ struct blk_mq_hw_ctx {
 	 * decide if the hw_queue is busy using Exponential Weighted Moving
 	 * Average algorithm.
 	 */
+	/*
+	 * 在以下使用dispatch_busy:
+	 *   - block/blk-mq-debugfs.c|621| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+	 *   - block/blk-mq-sched.c|217| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+	 *   - block/blk-mq-sched.c|436| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+	 *   - block/blk-mq.c|1215| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+	 *   - block/blk-mq.c|1225| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 *   - block/blk-mq.c|2071| <<blk_mq_make_request>> !data.hctx->dispatch_busy) {
+	 */
 	unsigned int		dispatch_busy;
 
 	/** @type: HCTX_TYPE_* flags. Type of hardware queue. */
@@ -107,6 +140,10 @@ struct blk_mq_hw_ctx {
 	 * @wait_index: Index of next available dispatch_wait queue to insert
 	 * requests.
 	 */
+	/*
+	 * 使用的地方:
+	 *   - block/blk-mq-tag.h|43| <<bt_wait_ptr>> return sbq_wait_ptr(bt, &hctx->wait_index);
+	 */
 	atomic_t		wait_index;
 
 	/**
@@ -138,6 +175,16 @@ struct blk_mq_hw_ctx {
 	 * @nr_active: Number of active requests. Only used when a tag set is
 	 * shared across request queues.
 	 */
+	/*
+	 * 修改nr_active的地方:
+	 *   - block/blk-mq.c|298| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|1072| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|518| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.h|207| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|2348| <<blk_mq_alloc_hctx>> atomic_set(&hctx->nr_active, 0);
+	 * 读取nr_active的地方:
+	 *   - block/blk-mq-tag.c|87| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 */
 	atomic_t		nr_active;
 
 	/** @cpuhp_dead: List to store request if some CPU die. */
@@ -163,6 +210,16 @@ struct blk_mq_hw_ctx {
 #endif
 
 	/** @hctx_list:	List of all hardware queues. */
+	/*
+	 * 实际是if this hctx is not in use, this is an entry in q->unused_hctx_list.
+	 *   - block/blk-mq.c|2476| <<blk_mq_exit_hctx>> list_add(&hctx->hctx_list, &q->unused_hctx_list);
+	 *   - block/blk-mq.c|2576| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->hctx_list);
+	 *   - block/blk-mq.c|2944| <<blk_mq_release>> WARN_ON_ONCE(hctx && list_empty(&hctx->hctx_list));
+	 *   - block/blk-mq.c|2947| <<blk_mq_release>> list_for_each_entry_safe(hctx, next, &q->unused_hctx_list, hctx_list) {
+	 *   - block/blk-mq.c|2948| <<blk_mq_release>> list_del_init(&hctx->hctx_list);
+	 *   - block/blk-mq.c|3054| <<blk_mq_alloc_and_init_hctx>> list_for_each_entry(tmp, &q->unused_hctx_list, hctx_list) {
+	 *   - block/blk-mq.c|3061| <<blk_mq_alloc_and_init_hctx>> list_del_init(&hctx->hctx_list);
+	 */
 	struct list_head	hctx_list;
 
 	/**
@@ -185,7 +242,31 @@ struct blk_mq_hw_ctx {
  */
 struct blk_mq_queue_map {
 	unsigned int *mq_map;
+	/*
+	 * 修改map[]->nr_queues的地方:
+	 *   - block/blk-mq.c|3457| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 *   - drivers/nvme/host/pci.c|440| <<nvme_pci_map_queues>> map->nr_queues = dev->io_queues[i];
+	 *   - drivers/nvme/host/rdma.c|1824| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1827| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1833| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1836| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1847| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2179| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2182| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2188| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2191| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2200| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 */
 	unsigned int nr_queues;
+	/*
+	 * 对于nvme来说
+	 * The poll queue(s) doesn't have an IRQ (and hence IRQ
+	 * affinity), so use the regular blk-mq cpu mapping
+	 *
+	 * First hardware queue to map onto. Used by the PCIe NVMe
+	 * driver to map each hardware queue type (enum hctx_type) onto a distinct
+	 * set of hardware queues.
+	 */
 	unsigned int queue_offset;
 };
 
@@ -232,10 +313,73 @@ enum hctx_type {
  * @tag_list:	   List of the request queues that use this tag set. See also
  *		   request_queue.tag_set_list.
  */
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */ 
 struct blk_mq_tag_set {
 	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
+	/*
+	 * 修改set->nr_maps的地方:
+	 *   - block/blk-mq.c|3013| <<blk_mq_init_sq_queue>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3409| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3420| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - drivers/block/paride/pd.c|908| <<pd_probe_drive>> disk->tag_set.nr_maps = 1;
+	 *   - drivers/block/sx8.c|1463| <<carm_init_one>> host->tag_set.nr_maps = 1;
+	 *   - drivers/nvme/host/pci.c|2309| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+	 *   - drivers/nvme/host/pci.c|2311| <<nvme_dev_add>> dev->tagset.nr_maps++;
+	 *   - drivers/nvme/host/rdma.c|752| <<nvme_rdma_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+	 *   - drivers/nvme/host/tcp.c|1490| <<nvme_tcp_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+	 */
 	unsigned int		nr_maps;
 	const struct blk_mq_ops	*ops;
+	/*
+	 * 部分修改set->nr_hw_queues的地方:
+	 *   - block/blk-mq.c|3022| <<blk_mq_init_sq_queue>> set->nr_hw_queues = 1;
+	 *   - block/blk-mq.c|3361| <<blk_mq_realloc_tag_set_tags>> set->nr_hw_queues = new_nr_hw_queues;
+	 *   - block/blk-mq.c|3429| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = 1;
+	 *   - block/blk-mq.c|3438| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = nr_cpu_ids;
+	 *   - block/blk-mq.c|3655| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = nr_hw_queues;
+	 *   - block/blk-mq.c|3663| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = prev_nr_hw_queues;
+	 *   - drivers/block/loop.c|2107| <<loop_add>> lo->tag_set.nr_hw_queues = 1;
+	 *   - drivers/block/nbd.c|1679| <<nbd_dev_add>> nbd->tag_set.nr_hw_queues = 1;
+	 *   - drivers/block/null_blk_main.c|1968| <<null_init_tag_set>> set->nr_hw_queues = nullb ? nullb->dev->submit_queues :
+	 *   - drivers/block/virtio_blk.c|817| <<virtblk_probe>> vblk->tag_set.nr_hw_queues = vblk->num_vqs;
+	 *   - drivers/block/xen-blkfront.c|968| <<xlvbd_init_blk_queue>> info->tag_set.nr_hw_queues = info->nr_rings;
+	 *   - drivers/md/dm-rq.c|551| <<dm_mq_init_request_queue>> md->tag_set->nr_hw_queues = dm_get_blk_mq_nr_hw_queues();
+	 *   - drivers/nvme/host/fc.c|2472| <<nvme_fc_create_io_queues>> ctrl->tag_set.nr_hw_queues = ctrl->ctrl.queue_count - 1;
+	 *   - drivers/nvme/host/fc.c|3139| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.nr_hw_queues = 1;
+	 *   - drivers/nvme/host/pci.c|1636| <<nvme_alloc_admin_tags>> dev->admin_tagset.nr_hw_queues = 1;
+	 *   - drivers/nvme/host/pci.c|2308| <<nvme_dev_add>> dev->tagset.nr_hw_queues = dev->online_queues - 1;
+	 *   - drivers/nvme/host/rdma.c|736| <<nvme_rdma_alloc_tagset>> set->nr_hw_queues = 1;
+	 *   - drivers/nvme/host/rdma.c|750| <<nvme_rdma_alloc_tagset>> set->nr_hw_queues = nctrl->queue_count - 1;
+	 *   - drivers/nvme/host/tcp.c|1476| <<nvme_tcp_alloc_tagset>> set->nr_hw_queues = 1;
+	 *   - drivers/nvme/host/tcp.c|1488| <<nvme_tcp_alloc_tagset>> set->nr_hw_queues = nctrl->queue_count - 1;
+	 *   - drivers/nvme/target/loop.c|347| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.nr_hw_queues = 1;
+	 *   - drivers/nvme/target/loop.c|521| <<nvme_loop_create_io_queues>> ctrl->tag_set.nr_hw_queues = ctrl->ctrl.queue_count - 1;
+	 *   - drivers/scsi/scsi_lib.c|1897| <<scsi_mq_setup_tags>> shost->tag_set.nr_hw_queues = shost->nr_hw_queues ? : 1;
+	 */
 	unsigned int		nr_hw_queues;
 	unsigned int		queue_depth;
 	unsigned int		reserved_tags;
@@ -248,6 +392,10 @@ struct blk_mq_tag_set {
 	struct blk_mq_tags	**tags;
 
 	struct mutex		tag_list_lock;
+	/*
+	 * 添加的地方:
+	 *   - block/blk-mq.c|2626| <<blk_mq_add_queue_tag_set>> list_add_tail_rcu(&q->tag_set_list, &set->tag_list);
+	 */
 	struct list_head	tag_list;
 };
 
@@ -394,6 +542,13 @@ enum {
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
 	BLK_MQ_S_STOPPED	= 0,
+	/*
+	 * 使用BLK_MQ_S_TAG_ACTIVE的地方:
+	 *   - block/blk-mq-tag.c|26| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|27| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|51| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|70| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 */
 	BLK_MQ_S_TAG_ACTIVE	= 1,
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
@@ -476,6 +631,11 @@ static inline int blk_mq_request_started(struct request *rq)
 	return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|384| <<blk_mq_tagset_count_completed_rqs>> if (blk_mq_request_completed(rq))
+ *   - drivers/nvme/host/core.c|317| <<nvme_cancel_request>> if (blk_mq_request_completed(req))
+ */
 static inline int blk_mq_request_completed(struct request *rq)
 {
 	return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
@@ -548,10 +708,16 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	return rq + 1;
 }
 
+/*
+ * 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i]
+ */
 #define queue_for_each_hw_ctx(q, hctx, i)				\
 	for ((i) = 0; (i) < (q)->nr_hw_queues &&			\
 	     ({ hctx = (q)->queue_hw_ctx[i]; 1; }); (i)++)
 
+/*
+ * 对于hctx->nr_ctx范围内的每一个hctx->ctxs[i]
+ */
 #define hctx_for_each_ctx(hctx, ctx, i)					\
 	for ((i) = 0; (i) < (hctx)->nr_ctx &&				\
 	     ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index 70254ae11769..e95cb9461557 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -301,7 +301,19 @@ enum req_opf {
 	REQ_OP_SCSI_IN		= 32,
 	REQ_OP_SCSI_OUT		= 33,
 	/* Driver private requests */
+	/*
+	 * 部分使用REQ_OP_DRV_IN的例子:
+	 *   - drivers/block/virtio_blk.c|306| <<virtio_queue_rq>> case REQ_OP_DRV_IN:
+	 *   - drivers/block/virtio_blk.c|369| <<virtblk_get_id>> req = blk_get_request(q, REQ_OP_DRV_IN, 0);
+	 *   - drivers/nvme/host/core.c|485| <<nvme_alloc_request>> unsigned op = nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;
+	 *   - drivers/nvme/host/core.c|764| <<nvme_setup_cmd>> case REQ_OP_DRV_IN:
+	 */
 	REQ_OP_DRV_IN		= 34,
+	/*
+	 * 部分使用REQ_OP_DRV_OUT的例子:
+	 *   - drivers/nvme/host/core.c|485| <<nvme_alloc_request>> unsigned op = nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;
+	 *   - drivers/nvme/host/core.c|765| <<nvme_setup_cmd>> case REQ_OP_DRV_OUT:
+	 */
 	REQ_OP_DRV_OUT		= 35,
 
 	REQ_OP_LAST,
@@ -362,6 +374,16 @@ enum req_flag_bits {
 #define REQ_CGROUP_PUNT		(1ULL << __REQ_CGROUP_PUNT)
 
 #define REQ_NOUNMAP		(1ULL << __REQ_NOUNMAP)
+/*
+ * 使用REQ_HIPRI的地方:
+ *   - block/blk-core.c|937| <<generic_make_request_checks>> bio->bi_opf &= ~REQ_HIPRI;
+ *   - block/blk-mq.c|608| <<__blk_mq_complete_request>> if ((rq->cmd_flags & REQ_HIPRI) ||
+ *   - block/blk-mq.h|191| <<blk_mq_map_queue>> if (flags & REQ_HIPRI)
+ *   - drivers/nvme/host/core.c|807| <<nvme_execute_rq_polled>> rq->cmd_flags |= REQ_HIPRI;
+ *   - fs/direct-io.c|1243| <<do_blockdev_direct_IO>> dio->op_flags |= REQ_HIPRI;
+ *   - include/linux/bio.h|832| <<bio_set_polled>> bio->bi_opf |= REQ_HIPRI;
+ *   - mm/page_io.c|412| <<swap_readpage>> bio->bi_opf |= REQ_HIPRI;
+ */
 #define REQ_HIPRI		(1ULL << __REQ_HIPRI)
 
 #define REQ_DRV			(1ULL << __REQ_DRV)
@@ -461,16 +483,28 @@ static inline bool blk_qc_t_valid(blk_qc_t cookie)
 	return cookie != BLK_QC_T_NONE && cookie != BLK_QC_T_EAGAIN;
 }
 
+/*
+ * 把cookie的第31位(最高位)清空,然后向右移动16位,获得queue num
+ */
 static inline unsigned int blk_qc_t_to_queue_num(blk_qc_t cookie)
 {
+	/*
+	 * ~BLK_QC_T_INTERNAL的第31位是0, 剩下都是1
+	 */
 	return (cookie & ~BLK_QC_T_INTERNAL) >> BLK_QC_T_SHIFT;
 }
 
+/*
+ * 获取cookie的低16位, 表示tag id
+ */
 static inline unsigned int blk_qc_t_to_tag(blk_qc_t cookie)
 {
 	return cookie & ((1u << BLK_QC_T_SHIFT) - 1);
 }
 
+/*
+ * 如果cookie最高位31位设置了,就是INTERNAL
+ */
 static inline bool blk_qc_t_is_internal(blk_qc_t cookie)
 {
 	return (cookie & BLK_QC_T_INTERNAL) != 0;
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4c636c42ad68..81d5a0d3e61d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -72,6 +72,17 @@ typedef __u32 __bitwise req_flags_t;
 /* may not be passed by ioscheduler */
 #define RQF_SOFTBARRIER		((__force req_flags_t)(1 << 3))
 /* request for flush sequence */
+/*
+ * 在以下使用RQF_FLUSH_SEQ:
+ *   - block/blk-core.c|244| <<req_bio_endio>> if (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))
+ *   - block/blk-core.c|1347| <<blk_account_io_done>> !(req->rq_flags & RQF_FLUSH_SEQ)) {
+ *   - block/blk-flush.c|205| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|426| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|528| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+ *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> if (rq->rq_flags & RQF_FLUSH_SEQ) {
+ *   - block/blk-mq-sched.c|387| <<blk_mq_sched_insert_request>> if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
+ *   - include/linux/blkdev.h|120| <<RQF_NOMERGE_FLAGS>> (RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
+ */
 #define RQF_FLUSH_SEQ		((__force req_flags_t)(1 << 4))
 /* merge of different types, fail separately */
 #define RQF_MIXED_MERGE		((__force req_flags_t)(1 << 5))
@@ -106,6 +117,11 @@ typedef __u32 __bitwise req_flags_t;
 /* The per-zone write lock is held for this request */
 #define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
 /* already slept for hybrid poll */
+/*
+ * 在以下使用RQF_MQ_POLL_SLEPT:
+ *   - block/blk-mq.c|3444| <<blk_mq_poll_hybrid_sleep>> if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
+ *   - block/blk-mq.c|3461| <<blk_mq_poll_hybrid_sleep>> rq->rq_flags |= RQF_MQ_POLL_SLEPT;
+ */
 #define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
 /* ->timeout has been called, don't expire again */
 #define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))
@@ -120,6 +136,13 @@ typedef __u32 __bitwise req_flags_t;
 enum mq_rq_state {
 	MQ_RQ_IDLE		= 0,
 	MQ_RQ_IN_FLIGHT		= 1,
+	/*
+	 * 使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+	 *   - block/blk-mq.c|580| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|3474| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 *   - include/linux/blk-mq.h|481| <<blk_mq_request_completed>> return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
+	 */
 	MQ_RQ_COMPLETE		= 2,
 };
 
@@ -188,6 +211,11 @@ struct request {
 		struct {
 			unsigned int		seq;
 			struct list_head	list;
+			/*
+			 * 使用saved_end_io的地方:
+			 *   - block/blk-flush.c|221| <<blk_flush_restore_request>> rq->end_io = rq->flush.saved_end_io;
+			 *   - block/blk-flush.c|615| <<blk_insert_flush>> rq->flush.saved_end_io = rq->end_io;
+			 */
 			rq_end_io_fn		*saved_end_io;
 		} flush;
 	};
@@ -211,6 +239,12 @@ struct request {
 	 * with blk_rq_sectors(rq), except that it never be zeroed
 	 * by completion.
 	 */
+	/*
+	 * 在以下使用stats_sectors:
+	 *   - block/blk-mq.c|328| <<blk_mq_rq_ctx_init>> rq->stats_sectors = 0;
+	 *   - block/blk-mq.c|681| <<blk_mq_start_request>> rq->stats_sectors = blk_rq_sectors(rq);
+	 *   - include/linux/blkdev.h|970| <<blk_rq_stats_sectors>> return rq->stats_sectors;
+	 */
 	unsigned short stats_sectors;
 
 	/*
@@ -409,7 +443,17 @@ struct request_queue {
 	unsigned int		queue_depth;
 
 	/* hw dispatch queues */
+	/*
+	 * 设置queue_hw_ctx的地方:
+	 *   - block/blk-mq.c|2793| <<blk_mq_realloc_hw_ctxs>> q->queue_hw_ctx = new_hctxs;
+	 */
 	struct blk_mq_hw_ctx	**queue_hw_ctx;
+	/*
+	 * 设置nr_hw_queues的地方:
+	 *   - block/blk-mq.c|3103| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3147| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3233| <<blk_mq_init_allocated_queue>> q->nr_hw_queues = 0;
+	 */
 	unsigned int		nr_hw_queues;
 
 	struct backing_dev_info	*backing_dev_info;
@@ -475,9 +519,41 @@ struct request_queue {
 	unsigned int		dma_alignment;
 
 	unsigned int		rq_timeout;
+	/*
+	 * 设置poll_nsec的地方:
+	 *   - block/blk-mq.c|2906| <<blk_mq_init_allocated_queue>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|384| <<queue_poll_delay_store>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|386| <<queue_poll_delay_store>> q->poll_nsec = val * 1000;
+	 * 使用poll_nsec的地方:
+	 *   - block/blk-mq.c|3460| <<blk_mq_poll_hybrid_sleep>> if (q->poll_nsec > 0)
+	 *   - block/blk-mq.c|3461| <<blk_mq_poll_hybrid_sleep>> nsecs = q->poll_nsec;
+	 *   - block/blk-mq.c|3531| <<blk_mq_poll_hybrid>> if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
+	 *   - block/blk-sysfs.c|363| <<queue_poll_delay_show>> if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
+	 *   - block/blk-sysfs.c|366| <<queue_poll_delay_show>> val = q->poll_nsec / 1000;
+	 */
 	int			poll_nsec;
 
+	/*
+	 * 使用poll_cb的地方:
+	 *   - block/blk-mq.c|2861| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+	 *   - block/blk-mq.c|2864| <<blk_mq_init_allocated_queue>> if (!q->poll_cb)
+	 *   - block/blk-mq.c|2922| <<blk_mq_init_allocated_queue>> blk_stat_free_callback(q->poll_cb);
+	 *   - block/blk-mq.c|2923| <<blk_mq_init_allocated_queue>> q->poll_cb = NULL;
+	 *   - block/blk-mq.c|3354| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+	 *   - block/blk-mq.c|3369| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+	 *   - block/blk-mq.c|3372| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+	 *   - block/blk-sysfs.c|881| <<__blk_release_queue>> blk_stat_remove_callback(q, q->poll_cb);
+	 *   - block/blk-sysfs.c|882| <<__blk_release_queue>> blk_stat_free_callback(q->poll_cb);
+	 */
 	struct blk_stat_callback	*poll_cb;
+	/*
+	 * 使用poll_stat[]的地方:
+	 *   - block/blk-mq-debugfs.c|34| <<queue_poll_stat_show>> print_stat(m, &q->poll_stat[2 * bucket]);
+	 *   - block/blk-mq-debugfs.c|38| <<queue_poll_stat_show>> print_stat(m, &q->poll_stat[2 * bucket + 1]);
+	 *   - block/blk-mq.c|3388| <<blk_mq_poll_stats_fn>> q->poll_stat[bucket] = cb->stat[bucket];
+	 *   - block/blk-mq.c|3425| <<blk_mq_poll_nsecs>> if (q->poll_stat[bucket].nr_samples)
+	 *   - block/blk-mq.c|3426| <<blk_mq_poll_nsecs>> ret = (q->poll_stat[bucket].mean + 1) / 2;
+	 */
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
 	struct timer_list	timeout;
@@ -600,11 +676,53 @@ struct request_queue {
 #define QUEUE_FLAG_SAME_FORCE	12	/* force complete on same CPU */
 #define QUEUE_FLAG_DEAD		13	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE	14	/* queue is initialized */
+/*
+ * 使用QUEUE_FLAG_POLL的地方:
+ *   - block/blk-mq.c|3021| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+ *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+ *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+ *   - block/blk-core.c|936| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ *   - block/blk-mq.c|3774| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ *   - drivers/nvme/host/core.c|805| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+ *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+ */
 #define QUEUE_FLAG_POLL		16	/* IO polling enabled if set */
+/*
+ * 在以下设置QUEUE_FLAG_WC的几个例子:
+ *   - block/blk-settings.c|824| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+ *   - block/blk-settings.c|826| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_WC, q)
+ *   - block/blk-sysfs.c|530| <<queue_wc_store>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+ *   - drivers/md/dm-table.c|1905| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_WC))) {
+ */
 #define QUEUE_FLAG_WC		17	/* Write back caching */
+/*
+ * 使用QUEUE_FLAG_FUA的地方:
+ *   - block/blk-flush.c|158| <<blk_flush_policy>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&
+ *   - block/blk-flush.c|471| <<blk_insert_flush>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)))
+ *   - block/blk-settings.c|828| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_FUA, q);
+ *   - block/blk-settings.c|830| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_FUA, q);
+ *   - block/blk-sysfs.c|539| <<queue_fua_show>> return sprintf(page, "%u\n", test_bit(QUEUE_FLAG_FUA, &q->queue_flags));
+ *   - drivers/md/dm-table.c|1907| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_FUA)))
+ *   - drivers/target/target_core_iblock.c|703| <<iblock_execute_rw>> if (test_bit(QUEUE_FLAG_FUA, &q->queue_flags)) {
+ *   - include/linux/blkdev.h|733| <<blk_queue_fua>> #define blk_queue_fua(q) test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 在以下使用QUEUE_FLAG_STATS:
+ *   - block/blk-mq.c|670| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ *   - block/blk-stat.c|320| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|335| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|376| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
+/*
+ * 在以下使用QUEUE_FLAG_POLL_STATS:
+ *   - block/blk-mq.c|3331| <<blk_poll_stats_enable>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+ *   - block/blk-mq.c|3332| <<blk_poll_stats_enable>> blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
+ *   - block/blk-mq.c|3344| <<blk_mq_poll_stats_start>> if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+ *   - block/blk-sysfs.c|880| <<__blk_release_queue>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
+ */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
@@ -948,6 +1066,16 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 
 static inline unsigned int blk_rq_stats_sectors(const struct request *rq)
 {
+	/*
+	 * rq sectors used for blk stats. It has the same value
+	 * with blk_rq_sectors(rq), except that it never be zeroed
+	 * by completion.
+	 *
+	 * 在以下使用stats_sectors:
+	 *   - block/blk-mq.c|328| <<blk_mq_rq_ctx_init>> rq->stats_sectors = 0;
+	 *   - block/blk-mq.c|681| <<blk_mq_start_request>> rq->stats_sectors = blk_rq_sectors(rq);
+	 *   - include/linux/blkdev.h|970| <<blk_rq_stats_sectors>> return rq->stats_sectors;
+	 */
 	return rq->stats_sectors;
 }
 
@@ -987,6 +1115,14 @@ static inline struct bio_vec req_bvec(struct request *rq)
 	return mp_bvec_iter_bvec(rq->bio->bi_io_vec, rq->bio->bi_iter);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1236| <<blk_cloned_rq_check_limits>> if (blk_rq_sectors(rq) > blk_queue_get_max_sectors(q, req_op(rq))) {
+ *   - block/blk-core.c|1239| <<blk_cloned_rq_check_limits>> blk_queue_get_max_sectors(q, req_op(rq)));
+ *   - drivers/scsi/cxlflash/vlun.c|433| <<write_same16>> const u32 ws_limit = blk_queue_get_max_sectors(sdev->request_queue,
+ *   - include/linux/blkdev.h|1157| <<blk_rq_get_max_sectors>> return blk_queue_get_max_sectors(q, req_op(rq));
+ *   - include/linux/blkdev.h|1160| <<blk_rq_get_max_sectors>> blk_queue_get_max_sectors(q, req_op(rq)));
+ */
 static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 						     int op)
 {
@@ -1000,6 +1136,7 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 	if (unlikely(op == REQ_OP_WRITE_ZEROES))
 		return q->limits.max_write_zeroes_sectors;
 
+	/* max sectors for a request for this queue */
 	return q->limits.max_sectors;
 }
 
@@ -1017,6 +1154,14 @@ static inline unsigned int blk_max_size_offset(struct request_queue *q,
 			(offset & (q->limits.chunk_sectors - 1))));
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|668| <<bio_attempt_discard_merge>> blk_rq_get_max_sectors(req, blk_rq_pos(req)))
+ *   - block/blk-merge.c|600| <<ll_back_merge_fn>> blk_rq_get_max_sectors(req, blk_rq_pos(req))) {
+ *   - block/blk-merge.c|616| <<ll_front_merge_fn>> blk_rq_get_max_sectors(req, bio->bi_iter.bi_sector)) {
+ *   - block/blk-merge.c|632| <<req_attempt_discard_merge>> blk_rq_get_max_sectors(req, blk_rq_pos(req)))
+ *   - block/blk-merge.c|654| <<ll_merge_requests_fn>> blk_rq_get_max_sectors(req, blk_rq_pos(req)))
+ */
 static inline unsigned int blk_rq_get_max_sectors(struct request *rq,
 						  sector_t offset)
 {
@@ -1417,10 +1562,25 @@ static inline unsigned int bdev_write_same(struct block_device *bdev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-lib.c|227| <<__blkdev_issue_write_zeroes>> max_write_zeroes_sectors = bdev_write_zeroes_sectors(bdev);
+ *   - block/blk-lib.c|365| <<blkdev_issue_zeroout>> bool try_write_zeroes = !!bdev_write_zeroes_sectors(bdev);
+ *   - block/blk-lib.c|394| <<blkdev_issue_zeroout>> if (!bdev_write_zeroes_sectors(bdev)) {
+ *   - drivers/md/dm-kcopyd.c|834| <<dm_kcopyd_copy>> if (!bdev_write_zeroes_sectors(job->dests[i].bdev)) {
+ *   - drivers/target/target_core_iblock.c|120| <<iblock_configure_device>> max_write_zeroes_sectors = bdev_write_zeroes_sectors(bd);
+ *   - drivers/target/target_core_iblock.c|471| <<iblock_execute_write_same>> if (bdev_write_zeroes_sectors(bdev)) {
+ *
+ * 返回The maximum number of write zeroes sectors (in 512-byte sectors) in
+ * one segment.
+ */
 static inline unsigned int bdev_write_zeroes_sectors(struct block_device *bdev)
 {
 	struct request_queue *q = bdev_get_queue(bdev);
 
+	/*
+	 * 设置的一处: blk_queue_max_write_zeroes_sectors()
+	 */
 	if (q)
 		return q->limits.max_write_zeroes_sectors;
 
diff --git a/include/linux/fs.h b/include/linux/fs.h
index 98e0349adb52..887db5860145 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -309,6 +309,25 @@ enum rw_hint {
 #define IOCB_EVENTFD		(1 << 0)
 #define IOCB_APPEND		(1 << 1)
 #define IOCB_DIRECT		(1 << 2)
+/*
+ * 设置和取消IOCB_HIPRI的地方:
+ *   - include/linux/fs.h|3427| <<kiocb_set_rw_flags>> ki->ki_flags |= IOCB_HIPRI;
+ *   - fs/io_uring.c|1536| <<io_prep_rw>> kiocb->ki_flags |= IOCB_HIPRI;
+ *   - fs/aio.c|1476| <<aio_prep_rw>> req->ki_flags &= ~IOCB_HIPRI;
+ * 其他使用IOCB_HIPRI的地方:
+ *   - fs/block_dev.c|248| <<__blkdev_direct_IO_simple>> if (iocb->ki_flags & IOCB_HIPRI)
+ *   - fs/block_dev.c|256| <<__blkdev_direct_IO_simple>> if (!(iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/block_dev.c|346| <<__blkdev_direct_IO>> bool is_poll = (iocb->ki_flags & IOCB_HIPRI) != 0;
+ *   - fs/block_dev.c|409| <<__blkdev_direct_IO>> if (iocb->ki_flags & IOCB_HIPRI) {
+ *   - fs/block_dev.c|450| <<__blkdev_direct_IO>> if (!(iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/direct-io.c|501| <<dio_await_one>> if (!(dio->iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/direct-io.c|1250| <<do_blockdev_direct_IO>> if (iocb->ki_flags & IOCB_HIPRI)
+ *   - fs/io_uring.c|1540| <<io_prep_rw>> if (kiocb->ki_flags & IOCB_HIPRI)
+ *   - fs/io_uring.c|1710| <<loop_rw_iter>> if (kiocb->ki_flags & IOCB_HIPRI)
+ *   - fs/iomap/direct-io.c|71| <<iomap_dio_submit_bio>> if (dio->iocb->ki_flags & IOCB_HIPRI)
+ *   - fs/iomap/direct-io.c|584| <<iomap_dio_rw>> if (!(iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/overlayfs/file.c|218| <<ovl_iocb_to_rwf>> if (ifl & IOCB_HIPRI)
+ */
 #define IOCB_HIPRI		(1 << 3)
 #define IOCB_DSYNC		(1 << 4)
 #define IOCB_SYNC		(1 << 5)
@@ -3163,6 +3182,19 @@ ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     dio_iodone_t end_io, dio_submit_t submit_io,
 			     int flags);
 
+/*
+ * called by:
+ *   - drivers/staging/exfat/exfat_super.c|3089| <<exfat_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, exfat_get_block);
+ *   - fs/affs/file.c|409| <<affs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, affs_get_block);
+ *   - fs/ext2/inode.c|948| <<ext2_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, ext2_get_block);
+ *   - fs/fat/inode.c|288| <<fat_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, fat_get_block);
+ *   - fs/hfs/inode.c|137| <<hfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfs_get_block);
+ *   - fs/hfsplus/inode.c|134| <<hfsplus_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfsplus_get_block);
+ *   - fs/jfs/inode.c|342| <<jfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, jfs_get_block);
+ *   - fs/nilfs2/inode.c|303| <<nilfs_direct_IO>> return blockdev_direct_IO(iocb, inode, iter, nilfs_get_block);
+ *   - fs/reiserfs/inode.c|3270| <<reiserfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter,
+ *   - fs/udf/inode.c|224| <<udf_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, udf_get_block);
+ */
 static inline ssize_t blockdev_direct_IO(struct kiocb *iocb,
 					 struct inode *inode,
 					 struct iov_iter *iter,
diff --git a/include/linux/genhd.h b/include/linux/genhd.h
index ea4c133b4139..56baa3980e06 100644
--- a/include/linux/genhd.h
+++ b/include/linux/genhd.h
@@ -24,6 +24,11 @@
 #define dev_to_disk(device)	container_of((device), struct gendisk, part0.__dev)
 #define dev_to_part(device)	container_of((device), struct hd_struct, __dev)
 #define disk_to_dev(disk)	(&(disk)->part0.__dev)
+/*
+ * struct hd_struct:
+ *  -> struct device __dev;
+ * 返回hd_struct->__dev
+ */
 #define part_to_dev(part)	(&((part)->__dev))
 
 extern struct device_type part_type;
@@ -142,6 +147,15 @@ struct hd_struct {
 #define GENHD_FL_EXT_DEVT			64 /* allow extended devt */
 #define GENHD_FL_NATIVE_CAPACITY		128
 #define GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE	256
+/*
+ * 在以下使用GENHD_FL_NO_PART_SCAN:
+ *   - block/genhd.c|799| <<__device_add_disk>> disk->flags |= GENHD_FL_NO_PART_SCAN;
+ *   - drivers/block/loop.c|1193| <<__loop_clr_fd>> lo->lo_disk->flags |= GENHD_FL_NO_PART_SCAN;
+ *   - drivers/block/loop.c|1349| <<loop_set_status>> lo->lo_disk->flags &= ~GENHD_FL_NO_PART_SCAN;
+ *   - drivers/block/loop.c|2063| <<loop_add>> disk->flags |= GENHD_FL_NO_PART_SCAN;
+ *   - drivers/mmc/core/block.c|2322| <<mmc_blk_alloc_req>> md->disk->flags |= GENHD_FL_NO_PART_SCAN
+ *   - include/linux/genhd.h|250| <<disk_part_scan_enabled>> !(disk->flags & GENHD_FL_NO_PART_SCAN);
+ */
 #define GENHD_FL_NO_PART_SCAN			512
 #define GENHD_FL_HIDDEN				1024
 
@@ -245,6 +259,20 @@ static inline bool disk_part_scan_enabled(struct gendisk *disk)
 		!(disk->flags & GENHD_FL_NO_PART_SCAN);
 }
 
+/*
+ * 这里似乎有bug, 根据最新的一个fix:
+ * Commit b72053072c0b ("block: allow partitions on host aware zone
+ * devices") introduced the helper function disk_has_partitions() to check
+ * if a given disk has valid partitions. However, since this function result
+ * directly depends on the disk partition table length rather than the
+ * actual existence of valid partitions in the table, it returns true even
+ * after all partitions are removed from the disk. For host aware zoned
+ * block devices, this results in zone management support to be kept
+ * disabled even after removing all partitions.
+ *
+ * called by:
+ *   - drivers/scsi/sd.c|2961| <<sd_read_block_characteristics>> if (sdkp->zoned == 1 && !disk_has_partitions(sdkp->disk)) {
+ */
 static inline bool disk_has_partitions(struct gendisk *disk)
 {
 	bool ret = false;
@@ -671,6 +699,16 @@ extern ssize_t part_fail_store(struct device *dev,
 			       const char *buf, size_t count);
 #endif /* CONFIG_FAIL_MAKE_REQUEST */
 
+/*
+ * called by:
+ *   - drivers/block/mtip32xx/mtip32xx.c|3591| <<mtip_block_initialize>> dd->disk = alloc_disk_node(MTIP_MAX_MINORS, dd->numa_node);
+ *   - drivers/block/null_blk_main.c|1565| <<null_gendisk_register>> disk = nullb->disk = alloc_disk_node(1, nullb->dev->home_node);
+ *   - drivers/ide/ide-gd.c|389| <<ide_gd_probe>> g = alloc_disk_node(IDE_DISK_MINORS, hwif_to_node(drive->hwif));
+ *   - drivers/md/dm.c|1954| <<alloc_dev>> md->disk = alloc_disk_node(1, md->numa_node_id);
+ *   - drivers/nvdimm/pmem.c|451| <<pmem_attach_disk>> disk = alloc_disk_node(0, nid);
+ *   - drivers/nvme/host/core.c|3533| <<nvme_alloc_ns>> disk = alloc_disk_node(0, node);
+ *   - include/linux/genhd.h|690| <<alloc_disk>> #define alloc_disk(minors) alloc_disk_node(minors, NUMA_NO_NODE)
+ */
 #define alloc_disk_node(minors, node_id)				\
 ({									\
 	static struct lock_class_key __key;				\
@@ -712,6 +750,10 @@ static inline void hd_struct_put(struct hd_struct *part)
 	percpu_ref_put(&part->ref);
 }
 
+/*
+ * called by:
+ *   - block/partition-generic.c|298| <<delete_partition>> hd_struct_kill(part);
+ */
 static inline void hd_struct_kill(struct hd_struct *part)
 {
 	percpu_ref_kill(&part->ref);
diff --git a/include/linux/nvme.h b/include/linux/nvme.h
index 3d5189f46cb1..b61d3c71d0bf 100644
--- a/include/linux/nvme.h
+++ b/include/linux/nvme.h
@@ -102,6 +102,19 @@ enum {
 	NVME_REG_INTMS	= 0x000c,	/* Interrupt Mask Set */
 	NVME_REG_INTMC	= 0x0010,	/* Interrupt Mask Clear */
 	NVME_REG_CC	= 0x0014,	/* Controller Configuration */
+	/*
+	 * 在以下使用NVME_REG_CSTS:
+	 *   - drivers/nvme/host/core.c|2102| <<nvme_wait_ready>> while ((ret = ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts)) == 0) {
+	 *   - drivers/nvme/host/core.c|2198| <<nvme_shutdown_ctrl>> while ((ret = ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts)) == 0) {
+	 *   - drivers/nvme/host/core.c|3884| <<nvme_ctrl_pp_status>> if (ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts))
+	 *   - drivers/nvme/host/pci.c|1263| <<nvme_timeout>> u32 csts = readl(dev->bar + NVME_REG_CSTS);
+	 *   - drivers/nvme/host/pci.c|1705| <<nvme_pci_configure_admin_queue>> (readl(dev->bar + NVME_REG_CSTS) & NVME_CSTS_NSSRO))
+	 *   - drivers/nvme/host/pci.c|1706| <<nvme_pci_configure_admin_queue>> writel(NVME_CSTS_NSSRO, dev->bar + NVME_REG_CSTS);
+	 *   - drivers/nvme/host/pci.c|2358| <<nvme_pci_enable>> if (readl(dev->bar + NVME_REG_CSTS) == -1) {
+	 *   - drivers/nvme/host/pci.c|2456| <<nvme_dev_disable>> u32 csts = readl(dev->bar + NVME_REG_CSTS);
+	 *   - drivers/nvme/target/fabrics-cmd.c|64| <<nvmet_execute_prop_get>> case NVME_REG_CSTS:
+	 *   - drivers/pci/quirks.c|3855| <<nvme_disable_and_flr>> u32 status = readl(bar + NVME_REG_CSTS);
+	 */
 	NVME_REG_CSTS	= 0x001c,	/* Controller Status */
 	NVME_REG_NSSR	= 0x0020,	/* NVM Subsystem Reset */
 	NVME_REG_AQA	= 0x0024,	/* Admin Queue Attributes */
diff --git a/include/linux/virtio_config.h b/include/linux/virtio_config.h
index bb4cc4910750..297ba40099f6 100644
--- a/include/linux/virtio_config.h
+++ b/include/linux/virtio_config.h
@@ -165,6 +165,13 @@ static inline bool virtio_has_feature(const struct virtio_device *vdev,
  * virtio_has_iommu_quirk - determine whether this device has the iommu quirk
  * @vdev: the device
  */
+/*
+ * called by:
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|975| <<virtio_gpu_cmd_transfer_to_host_3d>> bool use_dma_api = !virtio_has_iommu_quirk(vgdev->vdev);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|1050| <<virtio_gpu_object_attach>> bool use_dma_api = !virtio_has_iommu_quirk(vgdev->vdev);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|1104| <<virtio_gpu_object_detach>> bool use_dma_api = !virtio_has_iommu_quirk(vgdev->vdev);
+ *   - drivers/virtio/virtio_ring.c|259| <<vring_use_dma_api>> if (!virtio_has_iommu_quirk(vdev))
+ */
 static inline bool virtio_has_iommu_quirk(const struct virtio_device *vdev)
 {
 	/*
diff --git a/include/scsi/scsi_device.h b/include/scsi/scsi_device.h
index 3ed836db5306..acb7bbd956f8 100644
--- a/include/scsi/scsi_device.h
+++ b/include/scsi/scsi_device.h
@@ -106,6 +106,24 @@ struct scsi_device {
 	struct list_head    siblings;   /* list of all devices on this host */
 	struct list_head    same_target_siblings; /* just the devices sharing same target id */
 
+	/*
+	 * 使用device_busy的地方:
+	 *   - drivers/scsi/scsi_sysfs.c|664| <<global>> static DEVICE_ATTR(device_busy, S_IRUGO, sdev_show_device_busy, NULL);
+	 *   - drivers/message/fusion/mptsas.c|3759| <<mptsas_send_link_status_event>> atomic_read(&sdev->device_busy)));
+	 *   - drivers/scsi/megaraid/megaraid_sas_fusion.c|2829| <<megasas_build_ldio_fusion>> atomic_read(&scp->device->device_busy) >
+	 *   - drivers/scsi/megaraid/megaraid_sas_fusion.c|3162| <<megasas_build_syspd_fusion>> atomic_read(&scmd->device->device_busy) > MR_DEVICE_HIGH_IOPS_DEPTH)
+	 *   - drivers/scsi/mpt3sas/mpt3sas_base.c|3488| <<_base_get_high_iops_msix_index>> if (atomic_read(&scmd->device->device_busy) >
+	 *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|2996| <<scsih_dev_reset>> if (r == SUCCESS && atomic_read(&scmd->device->device_busy))
+	 *   - drivers/scsi/scsi_lib.c|357| <<scsi_device_unbusy>> atomic_dec(&sdev->device_busy);
+	 *   - drivers/scsi/scsi_lib.c|413| <<scsi_device_is_busy>> if (atomic_read(&sdev->device_busy) >= sdev->queue_depth)
+	 *   - drivers/scsi/scsi_lib.c|1287| <<scsi_dev_queue_ready>> busy = atomic_inc_return(&sdev->device_busy) - 1;
+	 *   - drivers/scsi/scsi_lib.c|1306| <<scsi_dev_queue_ready>> atomic_dec(&sdev->device_busy);
+	 *   - drivers/scsi/scsi_lib.c|1627| <<scsi_mq_put_budget>> atomic_dec(&sdev->device_busy);
+	 *   - drivers/scsi/scsi_lib.c|1638| <<scsi_mq_get_budget>> if (atomic_read(&sdev->device_busy) == 0 && !scsi_device_blocked(sdev))
+	 *   - drivers/scsi/scsi_lib.c|1709| <<scsi_queue_rq>> if (atomic_read(&sdev->device_busy) ||
+	 *   - drivers/scsi/scsi_sysfs.c|662| <<sdev_show_device_busy>> return snprintf(buf, 20, "%d\n", atomic_read(&sdev->device_busy));
+	 *   - drivers/scsi/sg.c|2510| <<sg_proc_seq_show_dev>> (int ) atomic_read(&scsidp->device_busy),
+	 */
 	atomic_t device_busy;		/* commands actually active on LLDD */
 	atomic_t device_blocked;	/* Device returned QUEUE_FULL. */
 
diff --git a/include/scsi/scsi_host.h b/include/scsi/scsi_host.h
index f577647bf5f2..d99940273777 100644
--- a/include/scsi/scsi_host.h
+++ b/include/scsi/scsi_host.h
@@ -598,6 +598,12 @@ struct Scsi_Host {
 	 * can_queue. In other words, the total queue depth per host
 	 * is nr_hw_queues * can_queue.
 	 */
+	/*
+	 * 修改request_queue->nr_hw_queues的地方:
+	 *   - block/blk-mq.c|3103| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3147| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3233| <<blk_mq_init_allocated_queue>> q->nr_hw_queues = 0;
+	 */
 	unsigned nr_hw_queues;
 	unsigned active_mode:2;
 	unsigned unchecked_isa_dma:1;
diff --git a/include/uapi/linux/fs.h b/include/uapi/linux/fs.h
index 379a612f8f1d..f1fd79764d02 100644
--- a/include/uapi/linux/fs.h
+++ b/include/uapi/linux/fs.h
@@ -285,6 +285,13 @@ struct fsxattr {
 typedef int __bitwise __kernel_rwf_t;
 
 /* high priority request, poll if possible */
+/*
+ * 在以下使用RWF_HIPRI:
+ *   - fs/overlayfs/file.c|219| <<ovl_iocb_to_rwf>> flags |= RWF_HIPRI;
+ *   - fs/read_write.c|706| <<do_loop_readv_writev>> if (flags & ~RWF_HIPRI)
+ *   - include/linux/fs.h|3426| <<kiocb_set_rw_flags>> if (flags & RWF_HIPRI)
+ *   - include/uapi/linux/fs.h|303| <<RWF_SUPPORTED>> #define RWF_SUPPORTED (RWF_HIPRI | RWF_DSYNC | RWF_SYNC | RWF_NOWAIT |\
+ */
 #define RWF_HIPRI	((__force __kernel_rwf_t)0x00000001)
 
 /* per-IO O_DSYNC */
@@ -300,6 +307,10 @@ typedef int __bitwise __kernel_rwf_t;
 #define RWF_APPEND	((__force __kernel_rwf_t)0x00000010)
 
 /* mask of flags supported by the kernel */
+/*
+ * 在以下使用RWF_SUPPORTED:
+ *   - include/linux/fs.h|3418| <<kiocb_set_rw_flags>> if (unlikely(flags & ~RWF_SUPPORTED))
+ */
 #define RWF_SUPPORTED	(RWF_HIPRI | RWF_DSYNC | RWF_SYNC | RWF_NOWAIT |\
 			 RWF_APPEND)
 
diff --git a/kernel/irq/msi.c b/kernel/irq/msi.c
index ad26fbcfbfc8..80a8e4975c0d 100644
--- a/kernel/irq/msi.c
+++ b/kernel/irq/msi.c
@@ -98,6 +98,28 @@ static void msi_check_level(struct irq_domain *domain, struct msi_msg *msg)
  * Intended to be used by MSI interrupt controllers which are
  * implemented with hierarchical domains.
  */
+/*
+ * qemu下nvme的一个例子:
+ * [0] msi_domain_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [    0.682898] orabug: msi_domain_set_affinity() mask=f
+ * [    0.695652] orabug: msi_domain_set_affinity() mask=f
+ * [    0.706281] orabug: msi_domain_set_affinity() mask=1
+ * [    0.715123] orabug: msi_domain_set_affinity() mask=2
+ * [    0.724427] orabug: msi_domain_set_affinity() mask=4
+ * [    0.733339] orabug: msi_domain_set_affinity() mask=8
+ */
 int msi_domain_set_affinity(struct irq_data *irq_data,
 			    const struct cpumask *mask, bool force)
 {
diff --git a/kernel/trace/blktrace.c b/kernel/trace/blktrace.c
index 475e29498bca..2dafe471a164 100644
--- a/kernel/trace/blktrace.c
+++ b/kernel/trace/blktrace.c
@@ -209,6 +209,19 @@ static const u32 ddir_act[2] = { BLK_TC_ACT(BLK_TC_READ),
  * The worker for the various blk_add_trace*() types. Fills out a
  * blk_io_trace structure and places it in a per-cpu subbuffer.
  */
+/*
+ * called by:
+ *   - kernel/trace/blktrace.c|809| <<blk_add_trace_rq>> __blk_add_trace(bt, blk_rq_trace_sector(rq), nr_bytes, req_op(rq),
+ *   - kernel/trace/blktrace.c|861| <<blk_add_trace_bio>> __blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,
+ *   - kernel/trace/blktrace.c|911| <<blk_add_trace_getrq>> __blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_GETRQ, 0, 0,
+ *   - kernel/trace/blktrace.c|927| <<blk_add_trace_sleeprq>> __blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_SLEEPRQ,
+ *   - kernel/trace/blktrace.c|937| <<blk_add_trace_plug>> __blk_add_trace(bt, 0, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL, 0);
+ *   - kernel/trace/blktrace.c|954| <<blk_add_trace_unplug>> __blk_add_trace(bt, 0, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu, 0);
+ *   - kernel/trace/blktrace.c|967| <<blk_add_trace_split>> __blk_add_trace(bt, bio->bi_iter.bi_sector,
+ *   - kernel/trace/blktrace.c|1001| <<blk_add_trace_bio_remap>> __blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,
+ *   - kernel/trace/blktrace.c|1034| <<blk_add_trace_rq_remap>> __blk_add_trace(bt, blk_rq_pos(rq), blk_rq_bytes(rq),
+ *   - kernel/trace/blktrace.c|1059| <<blk_add_driver_data>> __blk_add_trace(bt, blk_rq_trace_sector(rq), blk_rq_bytes(rq), 0, 0,
+ */
 static void __blk_add_trace(struct blk_trace *bt, sector_t sector, int bytes,
 		     int op, int op_flags, u32 what, int error, int pdu_len,
 		     void *pdu_data, u64 cgid)
diff --git a/lib/fault-inject.c b/lib/fault-inject.c
index 8186ca84910b..0f460e99d3eb 100644
--- a/lib/fault-inject.c
+++ b/lib/fault-inject.c
@@ -15,6 +15,18 @@
  * setup_fault_attr() is a helper function for various __setup handlers, so it
  * returns 0 on error, because that is what __setup handlers do.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/iommu.c|82| <<setup_fail_iommu>> return setup_fault_attr(&fail_iommu, str);
+ *   - block/blk-core.c|773| <<setup_fail_make_request>> return setup_fault_attr(&fail_make_request, str);
+ *   - block/blk-timeout.c|19| <<setup_fail_io_timeout>> return setup_fault_attr(&fail_io_timeout, str);
+ *   - drivers/block/null_blk_main.c|2651| <<__null_setup_fault>> if (!setup_fault_attr(attr, str))
+ *   - drivers/mmc/core/debugfs.c|240| <<mmc_add_host_debugfs>> setup_fault_attr(&fail_default_attr, fail_request);
+ *   - drivers/nvme/host/fault_inject.c|26| <<nvme_fault_inject_init>> setup_fault_attr(&fail_default_attr, fail_request);
+ *   - kernel/futex.c|288| <<setup_fail_futex>> return setup_fault_attr(&fail_futex.attr, str);
+ *   - mm/failslab.c|38| <<setup_failslab>> return setup_fault_attr(&failslab.attr, str);
+ *   - mm/page_alloc.c|3327| <<setup_fail_page_alloc>> return setup_fault_attr(&fail_page_alloc.attr, str);
+ */
 int setup_fault_attr(struct fault_attr *attr, char *str)
 {
 	unsigned long probability;
-- 
2.17.1

