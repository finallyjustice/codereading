From 43f01147b79eb52cba6e0b4f409e2385a62abafd Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 2 Jun 2020 00:51:21 -0700
Subject: [PATCH 1/1] linux block for linux-5.5

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/kernel/apic/msi.c        |   37 +
 arch/x86/kernel/apic/vector.c     |   35 +
 block/badblocks.c                 |   73 ++
 block/bio.c                       |    7 +
 block/blk-cgroup.c                |   20 +
 block/blk-core.c                  |  189 +++
 block/blk-exec.c                  |   15 +
 block/blk-flush.c                 |  433 +++++++
 block/blk-lib.c                   |    5 +
 block/blk-merge.c                 |   29 +
 block/blk-mq-cpumap.c             |   28 +
 block/blk-mq-debugfs.c            |    9 +
 block/blk-mq-pci.c                |   32 +
 block/blk-mq-rdma.c               |    5 +
 block/blk-mq-sched.c              |   43 +
 block/blk-mq-sysfs.c              |   19 +
 block/blk-mq-tag.c                |  252 ++++
 block/blk-mq-tag.h                |   53 +
 block/blk-mq-virtio.c             |   31 +
 block/blk-mq.c                    | 1149 +++++++++++++++++
 block/blk-mq.h                    |   87 ++
 block/blk-settings.c              |   94 ++
 block/blk-softirq.c               |   39 +
 block/blk-stat.c                  |  275 ++++
 block/blk-stat.h                  |   27 +
 block/blk-sysfs.c                 |   34 +
 block/blk-timeout.c               |   17 +
 block/blk-zoned.c                 |  166 +++
 block/blk.h                       |   76 ++
 block/genhd.c                     |  113 ++
 block/partition-generic.c         |   98 ++
 block/partitions/check.c          |   99 ++
 drivers/ata/libata-scsi.c         |    3 +
 drivers/block/loop.c              |  100 ++
 drivers/block/loop.h              |   15 +
 drivers/block/nbd.c               |  615 +++++++++
 drivers/block/null_blk.h          |  101 ++
 drivers/block/null_blk_main.c     | 1073 ++++++++++++++++
 drivers/block/null_blk_zoned.c    |  101 ++
 drivers/block/virtio_blk.c        |   31 +
 drivers/nvme/host/core.c          | 1985 +++++++++++++++++++++++++++++
 drivers/nvme/host/fabrics.c       |  224 ++++
 drivers/nvme/host/fabrics.h       |   10 +
 drivers/nvme/host/fault_inject.c  |   60 +
 drivers/nvme/host/fc.c            |  149 +++
 drivers/nvme/host/hwmon.c         |   52 +
 drivers/nvme/host/multipath.c     |  388 ++++++
 drivers/nvme/host/nvme.h          |  507 ++++++++
 drivers/nvme/host/pci.c           | 1940 +++++++++++++++++++++++++++-
 drivers/nvme/host/tcp.c           |   20 +
 drivers/nvme/target/configfs.c    |   79 ++
 drivers/nvme/target/core.c        |  229 ++++
 drivers/nvme/target/discovery.c   |   44 +
 drivers/nvme/target/fabrics-cmd.c |    9 +
 drivers/nvme/target/fc.c          |   40 +
 drivers/nvme/target/fcloop.c      |  286 +++++
 drivers/nvme/target/io-cmd-file.c |   56 +
 drivers/nvme/target/loop.c        |  532 ++++++++
 drivers/nvme/target/nvmet.h       |   57 +
 drivers/pci/msi.c                 |  105 ++
 drivers/virtio/virtio_ring.c      |   24 +
 fs/block_dev.c                    |    9 +
 fs/direct-io.c                    |   15 +
 fs/ext4/file.c                    |    5 +
 fs/iomap/direct-io.c              |   28 +
 include/linux/badblocks.h         |   61 +
 include/linux/bio.h               |    6 +
 include/linux/blk-mq.h            |  256 ++++
 include/linux/blk_types.h         |   34 +
 include/linux/blkdev.h            |  290 +++++
 include/linux/fs.h                |   32 +
 include/linux/genhd.h             |   63 +
 include/linux/irq.h               |   68 +
 include/linux/irqdesc.h           |    7 +
 include/linux/nvme-fc.h           |   24 +
 include/linux/nvme.h              |  172 +++
 include/linux/pci.h               |    6 +
 include/linux/percpu-refcount.h   |  196 +++
 include/linux/sbitmap.h           |    3 +
 include/linux/virtio_config.h     |    7 +
 include/scsi/scsi_device.h        |   18 +
 include/scsi/scsi_host.h          |    6 +
 include/uapi/linux/fs.h           |   11 +
 kernel/irq/affinity.c             |   14 +
 kernel/irq/chip.c                 |   12 +
 kernel/irq/irqdesc.c              |   11 +
 kernel/irq/irqdomain.c            |    8 +
 kernel/irq/manage.c               |   16 +
 kernel/irq/matrix.c               |    4 +
 kernel/irq/msi.c                  |   61 +
 kernel/irq/proc.c                 |   26 +
 kernel/trace/blktrace.c           |   13 +
 lib/fault-inject.c                |   12 +
 lib/percpu-refcount.c             |   11 +
 lib/sbitmap.c                     |   23 +
 95 files changed, 13951 insertions(+), 1 deletion(-)

diff --git a/arch/x86/kernel/apic/msi.c b/arch/x86/kernel/apic/msi.c
index 7f7533462474..e26ebeaea339 100644
--- a/arch/x86/kernel/apic/msi.c
+++ b/arch/x86/kernel/apic/msi.c
@@ -23,6 +23,37 @@
 
 static struct irq_domain *msi_default_domain;
 
+/*
+ * [0] irq_msi_compose_msg
+ * [0] irq_chip_compose_msi_msg+0x
+ * [0] msi_domain_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+
+ * [0] irq_msi_compose_msg
+ * [0] irq_chip_compose_msi_msg
+ * [0] msi_domain_activate
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void irq_msi_compose_msg(struct irq_data *data, struct msi_msg *msg)
 {
 	struct irq_cfg *cfg = irqd_cfg(data);
@@ -32,6 +63,12 @@ static void irq_msi_compose_msg(struct irq_data *data, struct msi_msg *msg)
 	if (x2apic_enabled())
 		msg->address_hi |= MSI_ADDR_EXT_DEST_ID(cfg->dest_apicid);
 
+	/*
+	 * cfg->dest_apicid和cfg->vector更新的地方:
+	 *   - arch/x86/kernel/apic/vector.c|147| <<apic_update_irq_cfg>> apicd->hw_irq_cfg.dest_apicid = apic->calc_dest_apicid(cpu);
+	 *   - arch/x86/kernel/apic/vector.c|150| <<apic_update_irq_cfg>> apicd->hw_irq_cfg.dest_apicid);
+	 */
+
 	msg->address_lo =
 		MSI_ADDR_BASE_LO |
 		((apic->irq_dest_mode == 0) ?
diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c
index 2c5676b0a6e7..35a783930ef6 100644
--- a/arch/x86/kernel/apic/vector.c
+++ b/arch/x86/kernel/apic/vector.c
@@ -114,9 +114,31 @@ static void free_apic_chip_data(struct apic_chip_data *apicd)
 	kfree(apicd);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|180| <<vector_assign_managed_shutdown>> apic_update_irq_cfg(irqd, MANAGED_IRQ_SHUTDOWN_VECTOR, cpu);
+ *   - arch/x86/kernel/apic/vector.c|252| <<assign_vector_locked>> apic_update_irq_cfg(irqd, vector, cpu);
+ *   - arch/x86/kernel/apic/vector.c|325| <<assign_managed_vector>> apic_update_irq_cfg(irqd, vector, cpu);
+ *   - arch/x86/kernel/apic/vector.c|516| <<vector_configure_legacy>> apic_update_irq_cfg(irqd, apicd->vector, apicd->cpu);
+ */
 static void apic_update_irq_cfg(struct irq_data *irqd, unsigned int vector,
 				unsigned int cpu)
 {
+	/*
+	 * struct apic_chip_data {
+	 *	struct irq_cfg          hw_irq_cfg;
+	 *	unsigned int            vector;
+	 *	unsigned int            prev_vector;
+	 *	unsigned int            cpu;
+	 *	unsigned int            prev_cpu;
+	 *	unsigned int            irq;
+	 *	struct hlist_node       clist;
+	 *	unsigned int            move_in_progress        : 1,
+	 *				is_managed              : 1,
+	 *				can_reserve             : 1,
+	 *				has_reserved            : 1;
+	 * };
+	 */
 	struct apic_chip_data *apicd = apic_chip_data(irqd);
 
 	lockdep_assert_held(&vector_lock);
@@ -304,6 +326,11 @@ assign_irq_vector_policy(struct irq_data *irqd, struct irq_alloc_info *info)
 	return reserve_irq_vector(irqd);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|427| <<activate_managed>> ret = assign_managed_vector(irqd, vector_searchmask);
+ *   - arch/x86/kernel/apic/vector.c|790| <<apic_set_affinity>> err = assign_managed_vector(irqd, vector_searchmask);
+ */
 static int
 assign_managed_vector(struct irq_data *irqd, const struct cpumask *dest)
 {
@@ -785,6 +812,14 @@ static int apic_set_affinity(struct irq_data *irqd,
 		return IRQ_SET_MASK_OK;
 
 	raw_spin_lock(&vector_lock);
+	/*
+	 * cpumask_and - *dstp = *src1p & *src2p
+	 * @dstp: the cpumask result
+	 * @src1p: the first input
+	 * @src2p: the second input
+	 *
+	 * If *@dstp is empty, returns 0, else returns 1
+	 */
 	cpumask_and(vector_searchmask, dest, cpu_online_mask);
 	if (irqd_affinity_is_managed(irqd))
 		err = assign_managed_vector(irqd, vector_searchmask);
diff --git a/block/badblocks.c b/block/badblocks.c
index 2e5f5697db35..0696c5925b32 100644
--- a/block/badblocks.c
+++ b/block/badblocks.c
@@ -16,6 +16,10 @@
 #include <linux/types.h>
 #include <linux/slab.h>
 
+/*
+ * 一个u64表示一个最多512-sector的范围
+ */
+
 /**
  * badblocks_check() - check a given range for bad sectors
  * @bb:		the badblocks structure that holds all badblock information
@@ -50,6 +54,20 @@
  * -1: there are bad blocks which have not yet been acknowledged in metadata.
  * plus the start/length of the first bad section we overlap.
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1999| <<null_handle_badblocks>> if (badblocks_check(bb, sector, nr_sectors, &first_bad, &bad_sectors))
+ *   - drivers/md/md.h|214| <<is_badblock>> int rv = badblocks_check(&rdev->badblocks, rdev->data_offset + s,
+ *   - drivers/nvdimm/nd.h|424| <<is_bad_pmem>> return !!badblocks_check(bb, sector, len / 512, &first_bad,
+ *   - drivers/nvdimm/pfn_devs.c|390| <<nd_pfn_clear_memmap_errors>> bb_present = badblocks_check(&nd_region->bb, meta_start,
+ *
+ * badblocks_check() - check a given range for bad sectors
+ * @bb:         the badblocks structure that holds all badblock information
+ * @s:          sector (start) at which to check for badblocks
+ * @sectors:    number of sectors to check for badblocks
+ * @first_bad:  pointer to store location of the first badblock
+ * @bad_sectors: pointer to store number of badblocks after @first_bad
+ */
 int badblocks_check(struct badblocks *bb, sector_t s, int sectors,
 			sector_t *first_bad, int *bad_sectors)
 {
@@ -160,6 +178,14 @@ static void badblocks_update_acked(struct badblocks *bb)
  *  0: success
  *  1: failed to set badblocks (out of space)
  */
+/*
+ * called by:
+ *   - block/badblocks.c|565| <<badblocks_store>> if (badblocks_set(bb, sector, length, !unack))
+ *   - drivers/block/null_blk_main.c|685| <<nullb_device_badblocks_store>> ret = badblocks_set(&t_dev->badblocks, start,
+ *   - drivers/md/md.c|1665| <<super_1_load>> if (badblocks_set(&rdev->badblocks, sector, count, 1))
+ *   - drivers/md/md.c|9243| <<rdev_set_badblocks>> rv = badblocks_set(&rdev->badblocks, s, sectors, 0);
+ *   - drivers/nvdimm/badrange.c|170| <<set_badblock>> if (badblocks_set(bb, s, num, 1)
+ */
 int badblocks_set(struct badblocks *bb, sector_t s, int sectors,
 			int acknowledged)
 {
@@ -291,10 +317,16 @@ int badblocks_set(struct badblocks *bb, sector_t s, int sectors,
 		} else {
 			int this_sectors = sectors;
 
+			/*
+			 * 从p+hi拷贝到p+hi+1, 相当于集体右移1格
+			 */
 			memmove(p + hi + 1, p + hi,
 				(bb->count - hi) * 8);
 			bb->count++;
 
+			/*
+			 * 一个u64最多只能表示BB_MAX_LEN=512个sector
+			 */
 			if (this_sectors > BB_MAX_LEN)
 				this_sectors = BB_MAX_LEN;
 			p[hi] = BB_MAKE(s, this_sectors, acknowledged);
@@ -464,6 +496,14 @@ EXPORT_SYMBOL_GPL(ack_all_badblocks);
  * Return:
  *  Length of returned data
  */
+/*
+ * called by:
+ *   - block/genhd.c|928| <<disk_badblocks_show>> return badblocks_show(disk->bb, page, 0);
+ *   - drivers/block/null_blk_main.c|636| <<nullb_device_badblocks_show>> return badblocks_show(&t_dev->badblocks, page, 0);
+ *   - drivers/md/md.c|3372| <<bb_show>> return badblocks_show(&rdev->badblocks, page, 0);
+ *   - drivers/md/md.c|3387| <<ubb_show>> return badblocks_show(&rdev->badblocks, page, 1);
+ *   - drivers/nvdimm/region_devs.c|540| <<region_badblocks_show>> rc = badblocks_show(&nd_region->bb, buf, 0);
+ */
 ssize_t badblocks_show(struct badblocks *bb, char *page, int unack)
 {
 	size_t len;
@@ -481,6 +521,9 @@ ssize_t badblocks_show(struct badblocks *bb, char *page, int unack)
 	i = 0;
 
 	while (len < PAGE_SIZE && i < bb->count) {
+		/*
+		 * 这里p[i]是一个u64
+		 */
 		sector_t s = BB_OFFSET(p[i]);
 		unsigned int length = BB_LEN(p[i]);
 		int ack = BB_ACK(p[i]);
@@ -490,6 +533,14 @@ ssize_t badblocks_show(struct badblocks *bb, char *page, int unack)
 		if (unack && ack)
 			continue;
 
+		/*
+		 * The return value is the number of characters which would be
+		 * generated for the given input, excluding the trailing null,
+		 * as per ISO C99.  If the return is greater than or equal to
+		 * @size, the resulting string is truncated.
+		 *
+		 * 输入的最后有一个'\n'
+		 */
 		len += snprintf(page+len, PAGE_SIZE-len, "%llu %u\n",
 				(unsigned long long)s << bb->shift,
 				length << bb->shift);
@@ -514,6 +565,12 @@ EXPORT_SYMBOL_GPL(badblocks_show);
  * Return:
  *  Length of the buffer processed or -ve error.
  */
+/*
+ * called by:
+ *   - block/genhd.c|940| <<disk_badblocks_store>> return badblocks_store(disk->bb, page, len, 0);
+ *   - drivers/md/md.c|3376| <<bb_store>> int rv = badblocks_store(&rdev->badblocks, page, len, 0);
+ *   - drivers/md/md.c|3391| <<ubb_store>> return badblocks_store(&rdev->badblocks, page, len, 1);
+ */
 ssize_t badblocks_store(struct badblocks *bb, const char *page, size_t len,
 			int unack)
 {
@@ -541,6 +598,11 @@ ssize_t badblocks_store(struct badblocks *bb, const char *page, size_t len,
 }
 EXPORT_SYMBOL_GPL(badblocks_store);
 
+/*
+ * called by:
+ *   - block/badblocks.c|577| <<badblocks_init>> return __badblocks_init(NULL, bb, enable);
+ *   - block/badblocks.c|585| <<devm_init_badblocks>> return __badblocks_init(dev, bb, 1);
+ */
 static int __badblocks_init(struct device *dev, struct badblocks *bb,
 		int enable)
 {
@@ -550,6 +612,12 @@ static int __badblocks_init(struct device *dev, struct badblocks *bb,
 		bb->shift = 0;
 	else
 		bb->shift = -1;
+	/*
+	 * devm_kzalloc():
+	 * Managed kmalloc.  Memory allocated with this function is
+	 * automatically freed on driver detach.  Like all other devres
+	 * resources, guaranteed alignment is unsigned long long.
+	 */
 	if (dev)
 		bb->page = devm_kzalloc(dev, PAGE_SIZE, GFP_KERNEL);
 	else
@@ -572,6 +640,11 @@ static int __badblocks_init(struct device *dev, struct badblocks *bb,
  *  0: success
  *  -ve errno: on error
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|865| <<null_alloc_dev>> if (badblocks_init(&dev->badblocks, 0)) {
+ *   - drivers/md/md.c|3557| <<md_rdev_init>> return badblocks_init(&rdev->badblocks, 0);
+ */
 int badblocks_init(struct badblocks *bb, int enable)
 {
 	return __badblocks_init(NULL, bb, enable);
diff --git a/block/bio.c b/block/bio.c
index 94d697217887..71fdfba6fc20 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -1752,6 +1752,13 @@ void bio_check_pages_dirty(struct bio *bio)
 	schedule_work(&bio_dirty_work);
 }
 
+/*
+ * called by:
+ *   - block/bio.c|1778| <<generic_start_io_acct>> update_io_ticks(part, jiffies);
+ *   - block/bio.c|1796| <<generic_end_io_acct>> update_io_ticks(part, now);
+ *   - block/blk-core.c|1354| <<blk_account_io_done>> update_io_ticks(part, jiffies);
+ *   - block/blk-core.c|1396| <<blk_account_io_start>> update_io_ticks(part, jiffies);
+ */
 void update_io_ticks(struct hd_struct *part, unsigned long now)
 {
 	unsigned long stamp;
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index a229b94d5390..1a21b15ada50 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -34,6 +34,26 @@
 
 #define MAX_KEY_LEN 100
 
+/*
+ * 来自mailing list
+ * Now the main block cgroup isolation policy:
+ * blk-iocost and bfq are weght based, blk-iolatency is latency based.
+ * The blk-iotrack can track the real percentage for IOs,kB,on disk time(d2c),
+ * and total time, it’s a good indicator to the real weight. For blk-iolatency, the
+ * blk-iotrack has 8 lantency thresholds to show latency distribution, so if we
+ * change these thresholds around to blk-iolateny.target.latency, we can tune
+ * the target latency to a more proper value.
+ *
+ * blk-iotrack extends the basic io.stat. It just export the important
+ * basic io statistics
+ * for cgroup,like what /prof/diskstats for block device. And it’s easy
+ * programming,
+ * iotrack just working like iostat, but focus on cgroup.
+ *
+ * blk-iotrack is friendly with these block cgroup isolation policies, a
+ * indicator for cgroup weight and lantency.
+ */
+
 /*
  * blkcg_pol_mutex protects blkcg_policy[] and policy [de]activation.
  * blkcg_pol_register_mutex nests outside of it and synchronizes entire
diff --git a/block/blk-core.c b/block/blk-core.c
index 089e890ab208..30d6a7235cd3 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -280,6 +280,12 @@ EXPORT_SYMBOL(blk_dump_rq_flags);
 void blk_sync_queue(struct request_queue *q)
 {
 	del_timer_sync(&q->timeout);
+	/*
+	 * Cancel @work and wait for its execution to finish.  This function
+	 * can be used even if the work re-queues itself or migrates to
+	 * another workqueue.  On return from this function, @work is
+	 * guaranteed to be not pending or executing on any CPU.
+	 */
 	cancel_work_sync(&q->timeout_work);
 }
 EXPORT_SYMBOL(blk_sync_queue);
@@ -311,6 +317,15 @@ void blk_put_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL(blk_put_queue);
 
+/*
+ * called by:
+ *   - block/blk-core.c|408| <<blk_cleanup_queue>> blk_set_queue_dying(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|4216| <<mtip_pci_remove>> blk_set_queue_dying(dd->queue);
+ *   - drivers/block/rbd.c|7273| <<do_rbd_remove>> blk_set_queue_dying(rbd_dev->disk->queue);
+ *   - drivers/md/dm.c|2356| <<__dm_destroy>> blk_set_queue_dying(md->queue);
+ *   - drivers/nvme/host/core.c|132| <<nvme_set_queue_dying>> blk_set_queue_dying(ns->queue);
+ *   - drivers/nvme/host/multipath.c|717| <<nvme_mpath_remove_disk>> blk_set_queue_dying(head->disk->queue);
+ */
 void blk_set_queue_dying(struct request_queue *q)
 {
 	blk_queue_flag_set(QUEUE_FLAG_DYING, q);
@@ -337,11 +352,68 @@ EXPORT_SYMBOL_GPL(blk_set_queue_dying);
  * Mark @q DYING, drain all pending requests, mark @q DEAD, destroy and
  * put it.  All future requests will be failed immediately with -ENODEV.
  */
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|3160| <<blk_mq_init_queue>> blk_cleanup_queue(uninit_q);
+ *   - block/bsg-lib.c|328| <<bsg_remove_queue>> blk_cleanup_queue(q);
+ *   - block/bsg-lib.c|405| <<bsg_setup_queue>> blk_cleanup_queue(q);
+ *   - drivers/block/loop.c|2175| <<loop_add>> blk_cleanup_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|2189| <<loop_remove>> blk_cleanup_queue(lo->lo_queue);
+ *   - drivers/block/nbd.c|228| <<nbd_dev_remove>> blk_cleanup_queue(q);
+ *   - drivers/block/null_blk_main.c|2372| <<null_del_dev>> blk_cleanup_queue(nullb->q);
+ *   - drivers/block/null_blk_main.c|2826| <<null_add_dev>> blk_cleanup_queue(nullb->q);
+ *   - drivers/block/virtio_blk.c|965| <<virtblk_remove>> blk_cleanup_queue(vblk->disk->queue);
+ *   - drivers/block/xen-blkfront.c|1210| <<xlvbd_release_gendisk>> blk_cleanup_queue(info->rq);
+ *   - drivers/ide/ide-probe.c|986| <<drive_release_dev>> blk_cleanup_queue(drive->queue);
+ *   - drivers/md/bcache/super.c|797| <<bcache_device_free>> blk_cleanup_queue(disk->queue);
+ *   - drivers/md/dm.c|1887| <<cleanup_mapped_device>> blk_cleanup_queue(md->queue);
+ *   - drivers/md/md.c|5378| <<md_free>> blk_cleanup_queue(mddev->queue);
+ *   - drivers/md/md.c|5491| <<md_alloc>> blk_cleanup_queue(mddev->queue);
+ *   - drivers/nvme/host/core.c|4255| <<nvme_alloc_ns>> blk_cleanup_queue(ns->queue);
+ *   - drivers/nvme/host/core.c|4286| <<nvme_ns_remove>> blk_cleanup_queue(ns->queue);
+ *   - drivers/nvme/host/fc.c|2023| <<nvme_fc_ctrl_free>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/nvme/host/fc.c|2033| <<nvme_fc_ctrl_free>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/fc.c|2034| <<nvme_fc_ctrl_free>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/host/fc.c|2502| <<nvme_fc_create_io_queues>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/nvme/host/fc.c|3228| <<nvme_fc_init_ctrl>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/fc.c|3230| <<nvme_fc_init_ctrl>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/host/multipath.c|434| <<nvme_mpath_alloc_disk>> blk_cleanup_queue(q);
+ *   - drivers/nvme/host/multipath.c|721| <<nvme_mpath_remove_disk>> blk_cleanup_queue(head->disk->queue);
+ *   - drivers/nvme/host/pci.c|2535| <<nvme_dev_remove_admin>> blk_cleanup_queue(dev->ctrl.admin_q);
+ *   - drivers/nvme/host/rdma.c|766| <<nvme_rdma_destroy_admin_queue>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/rdma.c|767| <<nvme_rdma_destroy_admin_queue>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/host/rdma.c|845| <<nvme_rdma_configure_admin_queue>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/rdma.c|848| <<nvme_rdma_configure_admin_queue>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/host/rdma.c|865| <<nvme_rdma_destroy_io_queues>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/nvme/host/rdma.c|904| <<nvme_rdma_configure_io_queues>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/nvme/host/tcp.c|1655| <<nvme_tcp_destroy_io_queues>> blk_cleanup_queue(ctrl->connect_q);
+ *   - drivers/nvme/host/tcp.c|1694| <<nvme_tcp_configure_io_queues>> blk_cleanup_queue(ctrl->connect_q);
+ *   - drivers/nvme/host/tcp.c|1707| <<nvme_tcp_destroy_admin_queue>> blk_cleanup_queue(ctrl->admin_q);
+ *   - drivers/nvme/host/tcp.c|1708| <<nvme_tcp_destroy_admin_queue>> blk_cleanup_queue(ctrl->fabrics_q);
+ *   - drivers/nvme/host/tcp.c|1762| <<nvme_tcp_configure_admin_queue>> blk_cleanup_queue(ctrl->admin_q);
+ *   - drivers/nvme/host/tcp.c|1765| <<nvme_tcp_configure_admin_queue>> blk_cleanup_queue(ctrl->fabrics_q);
+ *   - drivers/nvme/target/loop.c|256| <<nvme_loop_destroy_admin_queue>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/target/loop.c|257| <<nvme_loop_destroy_admin_queue>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/target/loop.c|273| <<nvme_loop_free_ctrl>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/nvme/target/loop.c|396| <<nvme_loop_configure_admin_queue>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/target/loop.c|398| <<nvme_loop_configure_admin_queue>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/target/loop.c|542| <<nvme_loop_create_io_queues>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/scsi/scsi_sysfs.c|1441| <<__scsi_remove_device>> blk_cleanup_queue(sdev->request_queue);
+ *
+ * blk_cleanup_queue - shutdown a request queue
+ * @q: request queue to shutdown
+ *
+ * Mark @q DYING, drain all pending requests, mark @q DEAD, destroy and
+ * put it.  All future requests will be failed immediately with -ENODEV.
+ */
 void blk_cleanup_queue(struct request_queue *q)
 {
 	WARN_ON_ONCE(blk_queue_registered(q));
 
 	/* mark @q DYING, no new request or merges will be allowed afterwards */
+	/*
+	 * 下面还会设置成QUEUE_FLAG_DEAD
+	 */
 	blk_set_queue_dying(q);
 
 	blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
@@ -357,6 +429,12 @@ void blk_cleanup_queue(struct request_queue *q)
 
 	rq_qos_exit(q);
 
+	/*
+	 * 在以下使用QUEUE_FLAG_DEAD:
+	 *   - block/blk-core.c|366| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_DEAD, q);
+	 *   - drivers/block/mtip32xx/mtip32xx.c|152| <<mtip_check_surprise_removal>> blk_queue_flag_set(QUEUE_FLAG_DEAD, dd->queue);
+	 *   - include/linux/blkdev.h|749| <<blk_queue_dead>> #define blk_queue_dead(q) test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
+	 */
 	blk_queue_flag_set(QUEUE_FLAG_DEAD, q);
 
 	/* for synchronous bio-based driver finish in-flight integrity i/o */
@@ -385,6 +463,14 @@ void blk_cleanup_queue(struct request_queue *q)
 	percpu_ref_exit(&q->q_usage_counter);
 
 	/* @q is and will stay empty, shutdown and put */
+	/*
+	 * 初始化的时候是: kobject_init(&q->kobj, &blk_queue_ktype);
+	 *
+	 *  940 struct kobj_type blk_queue_ktype = {
+	 *  941         .sysfs_ops      = &queue_sysfs_ops,
+	 *  942         .release        = blk_release_queue,
+	 *  943 };
+	 */
 	blk_put_queue(q);
 }
 EXPORT_SYMBOL(blk_cleanup_queue);
@@ -400,6 +486,27 @@ EXPORT_SYMBOL(blk_alloc_queue);
  * @q: request queue pointer
  * @flags: BLK_MQ_REQ_NOWAIT and/or BLK_MQ_REQ_PREEMPT
  */
+/*
+ * 一个hang的例子:
+ * # cat /proc/113/stack
+ * [<0>] blk_queue_enter+0x434/0x660
+ * [<0>] direct_make_request+0xa5/0x280
+ * [<0>] nvme_ns_head_make_request+0x43e/0xd60
+ * [<0>] generic_make_request+0x1eb/0x8e0
+ * [<0>] nvme_requeue_work+0x104/0x160
+ * [<0>] process_one_work+0x89c/0x1530
+ * [<0>] worker_thread+0x87/0x1010
+ * [<0>] kthread+0x2ea/0x3a0
+ * [<0>] ret_from_fork+0x35/0x40
+ *
+ * called by:
+ *   - block/blk-core.c|1142| <<generic_make_request>> if (likely(blk_queue_enter(q, flags) == 0)) {
+ *   - block/blk-core.c|1201| <<direct_make_request>> if (unlikely(blk_queue_enter(q, nowait ? BLK_MQ_REQ_NOWAIT : 0))) {
+ *   - block/blk-mq.c|454| <<blk_mq_alloc_request>> ret = blk_queue_enter(q, flags);
+ *   - block/blk-mq.c|491| <<blk_mq_alloc_request_hctx>> ret = blk_queue_enter(q, flags);
+ *   - fs/block_dev.c|708| <<bdev_read_page>> result = blk_queue_enter(bdev->bd_queue, 0);
+ *   - fs/block_dev.c|745| <<bdev_write_page>> result = blk_queue_enter(bdev->bd_queue, 0);
+ */
 int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 {
 	const bool pm = flags & BLK_MQ_REQ_PREEMPT;
@@ -408,6 +515,9 @@ int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 		bool success = false;
 
 		rcu_read_lock();
+		/*
+		 * blk_queue_exit()会put
+		 */
 		if (percpu_ref_tryget_live(&q->q_usage_counter)) {
 			/*
 			 * The code that increments the pm_only counter is
@@ -437,6 +547,14 @@ int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 		 */
 		smp_rmb();
 
+		/*
+		 * 在以下使用mq_freeze_depth:
+		 *   - block/blk-core.c|536| <<blk_queue_enter>> (!q->mq_freeze_depth &&
+		 *   - block/blk-mq.c|155| <<blk_freeze_queue_start>> if (++q->mq_freeze_depth == 1) {
+		 *   - block/blk-mq.c|221| <<blk_mq_unfreeze_queue>> q->mq_freeze_depth--;
+		 *   - block/blk-mq.c|222| <<blk_mq_unfreeze_queue>> WARN_ON_ONCE(q->mq_freeze_depth < 0);
+		 *   - block/blk-mq.c|223| <<blk_mq_unfreeze_queue>> if (!q->mq_freeze_depth) {
+		 */
 		wait_event(q->mq_freeze_wq,
 			   (!q->mq_freeze_depth &&
 			    (pm || (blk_pm_request_resume(q),
@@ -447,6 +565,20 @@ int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1150| <<generic_make_request>> blk_queue_exit(q);
+ *   - block/blk-core.c|1211| <<direct_make_request>> blk_queue_exit(q);
+ *   - block/blk-mq-tag.c|510| <<blk_mq_queue_tag_busy_iter>> blk_queue_exit(q);
+ *   - block/blk-mq.c|418| <<blk_mq_get_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|459| <<blk_mq_alloc_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|501| <<blk_mq_alloc_request_hctx>> blk_queue_exit(q);
+ *   - block/blk-mq.c|508| <<blk_mq_alloc_request_hctx>> blk_queue_exit(q);
+ *   - block/blk-mq.c|536| <<__blk_mq_free_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|1246| <<blk_mq_timeout_work>> blk_queue_exit(q);
+ *   - fs/block_dev.c|713| <<bdev_read_page>> blk_queue_exit(bdev->bd_queue);
+ *   - fs/block_dev.c|758| <<bdev_write_page>> blk_queue_exit(bdev->bd_queue);
+ */
 void blk_queue_exit(struct request_queue *q)
 {
 	percpu_ref_put(&q->q_usage_counter);
@@ -460,6 +592,10 @@ static void blk_queue_usage_counter_release(struct percpu_ref *ref)
 	wake_up_all(&q->mq_freeze_wq);
 }
 
+/*
+ * 在以下使用blk_rq_timed_out_timer():
+ *   - block/blk-core.c|532| <<blk_alloc_queue_node>> timer_setup(&q->timeout, blk_rq_timed_out_timer, 0);
+ */
 static void blk_rq_timed_out_timer(struct timer_list *t)
 {
 	struct request_queue *q = from_timer(q, t, timeout);
@@ -476,6 +612,18 @@ static void blk_timeout_work(struct work_struct *work)
  * @gfp_mask: memory allocation flags
  * @node_id: NUMA node to allocate memory from
  */
+/*
+ * called by:
+ *   - block/blk-core.c|394| <<blk_alloc_queue>> return blk_alloc_queue_node(gfp_mask, NUMA_NO_NODE);
+ *   - block/blk-mq.c|2663| <<blk_mq_init_queue>> uninit_q = blk_alloc_queue_node(GFP_KERNEL, set->numa_node);
+ *   - drivers/block/drbd/drbd_main.c|2804| <<drbd_create_device>> q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
+ *   - drivers/block/null_blk_main.c|1724| <<null_add_dev>> nullb->q = blk_alloc_queue_node(GFP_KERNEL, dev->home_node);
+ *   - drivers/block/umem.c|888| <<mm_pci_probe>> card->queue = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
+ *   - drivers/lightnvm/core.c|383| <<nvm_create_tgt>> tqueue = blk_alloc_queue_node(GFP_KERNEL, dev->q->node);
+ *   - drivers/md/dm.c|1948| <<alloc_dev>> md->queue = blk_alloc_queue_node(GFP_KERNEL, numa_node_id);
+ *   - drivers/nvdimm/pmem.c|404| <<pmem_attach_disk>> q = blk_alloc_queue_node(GFP_KERNEL, dev_to_node(dev));
+ *   - drivers/nvme/host/multipath.c|380| <<nvme_mpath_alloc_disk>> q = blk_alloc_queue_node(GFP_KERNEL, ctrl->numa_node);
+ */
 struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 {
 	struct request_queue *q;
@@ -1390,6 +1538,10 @@ void blk_account_io_start(struct request *rq, bool new_io)
  * Steal bios from a request and add them to a bio list.
  * The request must not have been partially completed before.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|78| <<nvme_failover_req>> blk_steal_bios(&ns->head->requeue_list, req);
+ */
 void blk_steal_bios(struct bio_list *list, struct request *rq)
 {
 	if (rq->bio) {
@@ -1433,6 +1585,35 @@ EXPORT_SYMBOL_GPL(blk_steal_bios);
  *     %false - this request doesn't have any more data
  *     %true  - this request has more data
  **/
+/*
+ * called by:
+ *   - arch/um/drivers/ubd_kern.c|530| <<ubd_handler>> if (!blk_update_request(io_req->req, io_req->error, io_req->length))
+ *   - block/blk-mq.c|597| <<blk_mq_end_request>> if (blk_update_request(rq, error, blk_rq_bytes(rq)))
+ *   - drivers/block/amiflop.c|1518| <<amiflop_queue_rq>> } while (blk_update_request(rq, err, blk_rq_cur_bytes(rq)));
+ *   - drivers/block/aoe/aoecmd.c|1054| <<aoe_end_request>> } while (blk_update_request(rq, bok ? BLK_STS_OK : BLK_STS_IOERR, bio->bi_iter.bi_size));
+ *   - drivers/block/ataflop.c|459| <<fd_end_request_cur>> if (!blk_update_request(fd_request, err,
+ *   - drivers/block/floppy.c|2223| <<floppy_end_request>> if (blk_update_request(req, error, nr_sectors << 9))
+ *   - drivers/block/loop.c|482| <<lo_complete_rq>> blk_update_request(rq, BLK_STS_OK, cmd->ret);
+ *   - drivers/block/paride/pcd.c|836| <<next_request>> if (!blk_update_request(pcd_req, err, blk_rq_cur_bytes(pcd_req))) {
+ *   - drivers/block/paride/pd.c|458| <<run_fsm>> if (!blk_update_request(pd_req, err,
+ *   - drivers/block/paride/pd.c|528| <<pd_next_buf>> if (!blk_update_request(pd_req, 0, blk_rq_cur_bytes(pd_req))) {
+ *   - drivers/block/paride/pf.c|831| <<pf_end_request>> if (!blk_update_request(pf_req, err, blk_rq_cur_bytes(pf_req))) {
+ *   - drivers/block/swim.c|546| <<swim_queue_rq>> } while (blk_update_request(req, err, blk_rq_cur_bytes(req)));
+ *   - drivers/block/swim3.c|266| <<swim3_end_request>> if (blk_update_request(req, err, nr_bytes))
+ *   - drivers/block/swim3.c|741| <<swim3_interrupt>> blk_update_request(req, 0, n << 9);
+ *   - drivers/block/xsysace.c|737| <<ace_fsm_dostate>> if (blk_update_request(ace->req, BLK_STS_OK,
+ *   - drivers/ide/ide-io.c|70| <<ide_end_rq>> if (!blk_update_request(rq, error, nr_bytes)) {
+ *   - drivers/md/dm-rq.c|123| <<end_clone_bio>> blk_update_request(tio->orig, BLK_STS_OK, tio->completed);
+ *   - drivers/memstick/core/ms_block.c|1912| <<msb_io_work>> if (len && !blk_update_request(req, BLK_STS_OK, len)) {
+ *   - drivers/memstick/core/mspro_block.c|710| <<mspro_block_issue_req>> chunk = blk_update_request(msb->block_req,
+ *   - drivers/memstick/core/mspro_block.c|774| <<mspro_block_complete_req>> chunk = blk_update_request(msb->block_req,
+ *   - drivers/mmc/core/block.c|1439| <<mmc_blk_cqe_complete_rq>> if (blk_update_request(req, BLK_STS_OK, mrq->data->bytes_xfered))
+ *   - drivers/mmc/core/block.c|1688| <<mmc_blk_read_single>> } while (blk_update_request(req, error, 512));
+ *   - drivers/mmc/core/block.c|1694| <<mmc_blk_read_single>> blk_update_request(req, BLK_STS_IOERR, 512);
+ *   - drivers/mmc/core/block.c|1893| <<mmc_blk_mq_complete_rq>> if (blk_update_request(req, BLK_STS_OK, nr_bytes))
+ *   - drivers/mtd/mtd_blkdevs.c|175| <<mtd_blktrans_work>> if (!blk_update_request(req, res, blk_rq_cur_bytes(req))) {
+ *   - drivers/scsi/scsi_lib.c|576| <<scsi_end_request>> if (blk_update_request(req, error, bytes))
+ */
 bool blk_update_request(struct request *req, blk_status_t error,
 		unsigned int nr_bytes)
 {
@@ -1764,6 +1945,14 @@ struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug, void *data,
 }
 EXPORT_SYMBOL(blk_check_plugged);
 
+/*
+ * called by:
+ *   - block/blk-core.c|1805| <<blk_finish_plug>> blk_flush_plug_list(plug, false);
+ *   - block/blk-mq.c|2263| <<blk_mq_make_request>> blk_flush_plug_list(plug, false);
+ *   - block/blk-mq.c|4094| <<blk_poll>> blk_flush_plug_list(current->plug, false);
+ *   - include/linux/blkdev.h|1332| <<blk_flush_plug>> blk_flush_plug_list(plug, false);
+ *   - include/linux/blkdev.h|1340| <<blk_schedule_flush_plug>> blk_flush_plug_list(plug, true);
+ */
 void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 {
 	flush_plug_callbacks(plug, from_schedule);
diff --git a/block/blk-exec.c b/block/blk-exec.c
index e20a852ae432..2fdeb7867228 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -45,6 +45,21 @@ static void blk_end_sync_rq(struct request *rq, blk_status_t error)
  * Note:
  *    This function will invoke @done directly if the queue is dead.
  */
+/*
+ * called by:
+ *   - block/blk-exec.c|86| <<blk_execute_rq>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, blk_end_sync_rq);
+ *   - drivers/block/sx8.c|542| <<carm_array_info>> blk_execute_rq_nowait(host->oob_q, NULL, rq, true, NULL);
+ *   - drivers/block/sx8.c|581| <<carm_send_special>> blk_execute_rq_nowait(host->oob_q, NULL, rq, true, NULL);
+ *   - drivers/nvme/host/core.c|1031| <<nvme_execute_rq_polled>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, nvme_end_sync_rq);
+ *   - drivers/nvme/host/core.c|1256| <<nvme_keep_alive>> blk_execute_rq_nowait(rq->q, NULL, rq, 0, nvme_keep_alive_end_io);
+ *   - drivers/nvme/host/lightnvm.c|698| <<nvme_nvm_submit_io>> blk_execute_rq_nowait(q, NULL, rq, 0, nvme_nvm_end_io);
+ *   - drivers/nvme/host/pci.c|1893| <<nvme_timeout>> blk_execute_rq_nowait(abort_req->q, NULL, abort_req, 0, abort_endio);
+ *   - drivers/nvme/host/pci.c|3189| <<nvme_delete_queue>> blk_execute_rq_nowait(q, NULL, req, false,
+ *   - drivers/scsi/scsi_error.c|1994| <<scsi_eh_lock_door>> blk_execute_rq_nowait(req->q, NULL, req, 1, eh_lock_door_done);
+ *   - drivers/scsi/sg.c|836| <<sg_common_write>> blk_execute_rq_nowait(sdp->device->request_queue, sdp->disk,
+ *   - drivers/scsi/st.c|587| <<st_scsi_execute>> blk_execute_rq_nowait(req->q, NULL, req, 1, st_scsi_execute_end);
+ *   - drivers/target/target_core_pscsi.c|1003| <<pscsi_execute_cmd>> blk_execute_rq_nowait(pdv->pdv_sd->request_queue, NULL, req,
+ */
 void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 			   struct request *rq, int at_head,
 			   rq_end_io_fn *done)
diff --git a/block/blk-flush.c b/block/blk-flush.c
index 3f977c517960..9aa810fcab67 100644
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@ -76,13 +76,81 @@
 #include "blk-mq-tag.h"
 #include "blk-mq-sched.h"
 
+/*
+ * http://www.unjeep.com/article/40696.html
+ *
+ * 硬盘在控制器上的一块内存芯片,其类型一般以SDRAM为主,具有极快的存取速度,
+ * 它是硬盘内部存储和外界接口之间的缓冲器.由于硬盘的内部数据传输速度和外界
+ * 介面传输速度不同,缓存在其中起到一个缓冲的作用.缓存的大小与速度是直接关
+ * 系到硬盘的传输速度的重要因素,能够大幅度地提高硬盘整体性能.
+ *
+ * 如果硬盘的cache启用了,那么很有可能写入的数据是写到了硬盘的cache中,而没
+ * 有真正写到磁盘介质上.
+ *
+ * 在linux下,查看磁盘cache是否开启可通过hdparm命令:
+ *
+ * #hdparm -W /dev/sdx    //是否开启cache，1为enable
+ * #hdparm -W 0 /dev/sdx  //关闭cache
+ * #hdparm -W 1 /dev/sdx  //enable cache
+ *
+ * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+ * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+ *
+ * blk_insert_flush()是非常重要的入口!
+ *
+ *
+ * 部分调用blk_queue_write_cache()的例子:
+ *   - drivers/block/loop.c|1007| <<loop_set_fd>> blk_queue_write_cache(lo->lo_queue, true, false);
+ *   - drivers/block/nbd.c|1136| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, true);
+ *   - drivers/block/nbd.c|1138| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, false);
+ *   - drivers/block/nbd.c|1141| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, false, false);
+ *   - drivers/block/null_blk_main.c|1742| <<null_add_dev>> blk_queue_write_cache(nullb->q, true, true);
+ *   - rivers/block/virtio_blk.c|609| <<virtblk_update_cache_mode>> blk_queue_write_cache(vblk->disk->queue, writeback, false);
+ *   - drivers/block/xen-blkfront.c|1014| <<xlvbd_flush>> blk_queue_write_cache(info->rq, info->feature_flush ? true : false,
+ *   - drivers/ide/ide-disk.c|554| <<update_flush>> blk_queue_write_cache(drive->queue, wc, false);
+ *   - drivers/md/dm-table.c|1910| <<dm_table_set_restrictions>> blk_queue_write_cache(q, wc, fua);
+ *   - drivers/md/md.c|5506| <<md_alloc>> blk_queue_write_cache(mddev->queue, true, true);
+ *   - drivers/nvme/host/core.c|2204| <<nvme_set_queue_limits>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/nvme/host/multipath.c|393| <<nvme_mpath_alloc_disk>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/scsi/sd.c|152| <<sd_set_flush_flag>> blk_queue_write_cache(sdkp->disk->queue, wc, fua);
+ */
+
+/*
+ * 冲刷的过程中request_queue为什么要使用双缓冲队列来存放fs_request?
+ *
+ * 双缓冲队列可以做到只执行一次冲刷请求就可以完成多个fs_request的冲刷要求.队列自
+ * 带的冲刷request在执行的过程中,blk_insert_flush()可以被调用多次,来自上层的
+ * fs_request被添加到pending1队列,等待冲刷request的下一次执行,当冲刷requst可以再
+ * 次被执行时,pending1队列不再接收新的fs_request(fs_request被加入到pending2队列),
+ * 冲刷request执行完毕后,pending1队列所有的fs_request的PREFLUSH/POSTFLUSH执行完毕.
+ * The actual execution of flush is double buffered.  Whenever a request
+ * needs to execute PRE or POSTFLUSH, it queues at
+ * fq->flush_queue[fq->flush_pending_idx].  Once certain criteria are met, a
+ * REQ_OP_FLUSH is issued and the pending_idx is toggled.  When the flush
+ * completes, all the requests which were pending are proceeded to the next
+ * step.  This allows arbitrary merging of different types of PREFLUSH/FUA
+ * requests.
+ */
+
 /* PREFLUSH/FUA sequences */
 enum {
+	/*
+	 * 在以下使用REQ_FSEQ_PREFLUSH:
+	 *   - block/blk-flush.c|117| <<global>> REQ_FSEQ_ACTIONS = REQ_FSEQ_PREFLUSH | REQ_FSEQ_DATA |
+	 *   - block/blk-flush.c|143| <<blk_flush_policy>> policy |= REQ_FSEQ_PREFLUSH;
+	 *   - block/blk-flush.c|244| <<blk_flush_complete_seq>> case REQ_FSEQ_PREFLUSH:
+	 *   - block/blk-flush.c|323| <<flush_end_io>> BUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);
+	 *   - block/blk-flush.c|486| <<blk_insert_flush>> !(policy & (REQ_FSEQ_PREFLUSH | REQ_FSEQ_POSTFLUSH))) {
+	 */
 	REQ_FSEQ_PREFLUSH	= (1 << 0), /* pre-flushing in progress */
 	REQ_FSEQ_DATA		= (1 << 1), /* data write in progress */
 	REQ_FSEQ_POSTFLUSH	= (1 << 2), /* post-flushing in progress */
 	REQ_FSEQ_DONE		= (1 << 3),
 
+	/*
+	 * 在以下使用REQ_FSEQ_ACTIONS:
+	 *   - block/blk-flush.c|511| <<blk_insert_flush>> blk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);
+	 */
 	REQ_FSEQ_ACTIONS	= REQ_FSEQ_PREFLUSH | REQ_FSEQ_DATA |
 				  REQ_FSEQ_POSTFLUSH,
 
@@ -96,16 +164,52 @@ enum {
 static void blk_kick_flush(struct request_queue *q,
 			   struct blk_flush_queue *fq, unsigned int flags);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|377| <<blk_insert_flush>> unsigned int policy = blk_flush_policy(fflags, rq);
+ *
+ * 根据情况把REQ_FSEQ_DATA,REQ_FSEQ_PREFLUSH和REQ_FSEQ_POSTFLUSH返回到返回值中
+ * 这些可以表明flush需要做的步骤(包含3步)
+ *  - REQ_FSEQ_PREFLUSH(在数据请求以前冲刷磁盘缓存)
+ *  - REQ_FSEQ_DATA(写入数据请求)
+ *  - REQ_FSEQ_POSTFLUSH(在数据请求之后冲刷磁盘缓存)
+ * 如果磁盘不支持FUA,那么可以使用REQ_FSEQ_POSTFLUSH代替
+ * 假定我们分析的场景中,磁盘不支持FUA,则最终我们的冲刷策略为3步都做(policy=111).
+ */
 static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
 {
+	/*
+	 * 因为blk_flush_policy()只被blk_insert_flush()调用
+	 * 所以参数的fflags来自q->queue_flags
+	 */
 	unsigned int policy = 0;
 
 	if (blk_rq_sectors(rq))
 		policy |= REQ_FSEQ_DATA;
 
+	/*
+	 * 在以下设置QUEUE_FLAG_WC的几个例子:
+	 *   - block/blk-settings.c|824| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+	 *   - block/blk-settings.c|826| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_WC, q)
+	 *   - block/blk-sysfs.c|530| <<queue_wc_store>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+	 *   - drivers/md/dm-table.c|1905| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_WC))) {
+	 */
 	if (fflags & (1UL << QUEUE_FLAG_WC)) {
 		if (rq->cmd_flags & REQ_PREFLUSH)
 			policy |= REQ_FSEQ_PREFLUSH;
+		/*
+		 * 使用QUEUE_FLAG_FUA的地方:
+		 *   - block/blk-flush.c|158| <<blk_flush_policy>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&
+		 *   - block/blk-flush.c|471| <<blk_insert_flush>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)))
+		 *   - block/blk-settings.c|828| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_FUA, q);
+		 *   - block/blk-settings.c|830| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_FUA, q);
+		 *   - block/blk-sysfs.c|539| <<queue_fua_show>> return sprintf(page, "%u\n", test_bit(QUEUE_FLAG_FUA, &q->queue_flags));
+		 *   - drivers/md/dm-table.c|1907| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_FUA)))
+		 *   - drivers/target/target_core_iblock.c|703| <<iblock_execute_rw>> if (test_bit(QUEUE_FLAG_FUA, &q->queue_flags)) {
+		 *   - include/linux/blkdev.h|733| <<blk_queue_fua>> #define blk_queue_fua(q) test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
+		 *
+		 * 如果磁盘不支持FUA,那么可以使用REQ_FSEQ_POSTFLUSH代替
+		 */
 		if (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&
 		    (rq->cmd_flags & REQ_FUA))
 			policy |= REQ_FSEQ_POSTFLUSH;
@@ -113,11 +217,27 @@ static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
 	return policy;
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|184| <<blk_flush_complete_seq>> seq = blk_flush_cur_seq(rq);
+ *   - block/blk-flush.c|262| <<flush_end_io>> unsigned int seq = blk_flush_cur_seq(rq);
+ *
+ * 获得rq->flush.seq中第一个为0的bit (因为rq->flush.seq表示的是可以跳过的)
+ * struct {
+ *     unsigned int            seq;
+ *     struct list_head        list;
+ *     rq_end_io_fn            *saved_end_io;
+ * } flush;
+ */
 static unsigned int blk_flush_cur_seq(struct request *rq)
 {
 	return 1 << ffz(rq->flush.seq);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|211| <<blk_flush_complete_seq>> blk_flush_restore_request(rq);
+ */
 static void blk_flush_restore_request(struct request *rq)
 {
 	/*
@@ -132,11 +252,20 @@ static void blk_flush_restore_request(struct request *rq)
 	rq->end_io = rq->flush.saved_end_io;
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|199| <<blk_flush_complete_seq>> blk_flush_queue_rq(rq, true);
+ *   - block/blk-flush.c|341| <<blk_kick_flush>> blk_flush_queue_rq(flush_rq, false);
+ */
 static void blk_flush_queue_rq(struct request *rq, bool add_front)
 {
 	blk_mq_add_to_requeue_list(rq, add_front, true);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|231| <<flush_end_io>> blk_account_io_flush(flush_rq);
+ */
 static void blk_account_io_flush(struct request *rq)
 {
 	struct hd_struct *part = &rq->rq_disk->part0;
@@ -164,11 +293,65 @@ static void blk_account_io_flush(struct request *rq)
  * RETURNS:
  * %true if requests were added to the dispatch queue, %false otherwise.
  */
+/*
+ * [0] blk_flush_complete_seq
+ * [0] blk_insert_flush
+ * [0] blk_mq_make_request
+ * [0] generic_make_request
+ * [0] submit_bio
+ * [0] submit_bh_wbc.isra.55
+ * [0] jbd2_write_superblock
+ * [0] jbd2_mark_journal_empty
+ * [0] jbd2_journal_flush
+ * [0] ext4_mark_recovery_complete.isra.224
+ * [0] ext4_fill_super
+ * [0] mount_bdev
+ * [0] legacy_get_tree
+ * [0] vfs_get_tree
+ * [0] do_mount
+ * [0] do_mount_root
+ * [0] mount_block_root
+ * [0] mount_root
+ * [0] prepare_namespace
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] blk_flush_complete_seq
+ * [0] mq_flush_data_end_io
+ * [0] blk_mq_complete_request
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * called by:
+ *   - block/blk-flush.c|265| <<flush_end_io>> blk_flush_complete_seq(rq, fq, seq, error);
+ *   - block/blk-flush.c|366| <<mq_flush_data_end_io>> blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error);
+ *   - block/blk-flush.c|444| <<blk_insert_flush>> blk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);
+ */
 static void blk_flush_complete_seq(struct request *rq,
 				   struct blk_flush_queue *fq,
 				   unsigned int seq, blk_status_t error)
 {
 	struct request_queue *q = rq->q;
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下使用flush_pending_idx:
+	 *   - block/blk-flush.c|276| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|392| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 * 默认初始化是1
+	 *
+	 * 在这个函数里REQ_FSEQ_DATA和REQ_FSEQ_DONE都不用pending (&fq->flush_queue[fq->flush_pending_idx])
+	 * 只有REQ_FSEQ_PREFLUSH和REQ_FSEQ_POSTFLUSH才用
+	 */
 	struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
 	unsigned int cmd_flags;
 
@@ -176,22 +359,52 @@ static void blk_flush_complete_seq(struct request *rq,
 	rq->flush.seq |= seq;
 	cmd_flags = rq->cmd_flags;
 
+	/*
+	 * 第一次从blk_insert_flush()进来, error是0
+	 *
+	 * blk_flush_cur_seq():
+	 * 获得rq->flush.seq中第一个为0的bit (因为rq->flush.seq表示的是可以跳过的)
+	 * struct {
+	 *     unsigned int            seq;
+	 *     struct list_head        list;
+	 *     rq_end_io_fn            *saved_end_io;
+	 * } flush;
+	 *
+	 * 假设三步都要执行,第一次到这里就是REQ_FSEQ_PREFLUSH
+	 */
 	if (likely(!error))
 		seq = blk_flush_cur_seq(rq);
 	else
 		seq = REQ_FSEQ_DONE;
 
+	/*
+	 * 假设三步都要执行,
+	 * 第一次从blk_insert_flush()到这里就是REQ_FSEQ_PREFLUSH
+	 */
 	switch (seq) {
 	case REQ_FSEQ_PREFLUSH:
 	case REQ_FSEQ_POSTFLUSH:
 		/* queue for flush */
+		/*
+		 * pending来自上面的&fq->flush_queue[fq->flush_pending_idx];
+		 */
 		if (list_empty(pending))
 			fq->flush_pending_since = jiffies;
+		/*
+		 * 插入的时候插入的是tail!!!!!
+		 */
 		list_move_tail(&rq->flush.list, pending);
 		break;
 
 	case REQ_FSEQ_DATA:
 		list_move_tail(&rq->flush.list, &fq->flush_data_in_flight);
+		/*
+		 * called by:
+		 *   - block/blk-flush.c|199| <<blk_flush_complete_seq>> blk_flush_queue_rq(rq, true);
+		 *   - block/blk-flush.c|341| <<blk_kick_flush>> blk_flush_queue_rq(flush_rq, false);
+		 *
+		 * 下面下发end的时候应该调用mq_flush_data_end_io()
+		 */
 		blk_flush_queue_rq(rq, true);
 		break;
 
@@ -215,6 +428,10 @@ static void blk_flush_complete_seq(struct request *rq,
 	blk_kick_flush(q, fq, cmd_flags);
 }
 
+/*
+ * 在以下使用flush_end_io():
+ *   - block/blk-flush.c|339| <<blk_kick_flush>> flush_rq->end_io = flush_end_io;
+ */
 static void flush_end_io(struct request *flush_rq, blk_status_t error)
 {
 	struct request_queue *q = flush_rq->q;
@@ -251,10 +468,40 @@ static void flush_end_io(struct request *flush_rq, blk_status_t error)
 	BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
 
 	/* account completion of the flush request */
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下修改flush_running_idx: 
+	 *   - block/blk-flush.c|362| <<flush_end_io>> fq->flush_running_idx ^= 1;
+	 */
 	fq->flush_running_idx ^= 1;
 
 	/* and push the waiting requests to the next stage */
 	list_for_each_entry_safe(rq, n, running, flush.list) {
+		/*
+		 * 这里的代码写的太隐晦了
+		 *
+		 * 假设是PREFLUSH执行完了到的这里, seq没清空过
+		 * 所以这里获得rq->flush.seq中第一个为0的bit 
+		 * 因为rq->flush.seq表示的是可以跳过的)肯定还是PREFLUSH
+		 *
+		 * 所以seq作为参数传入下面的blk_flush_complete_seq()的时候
+		 * 是告诉后面PREFLUSH要被跳过了!!!
+		 */
+		/*
+		 * 但是这里又不明白为什么是list_for_each_entry_safe()遍历所有的request.
+		 * 根据代码阅读发现这些request实际会被blk_flush_complete_seq()忽略preflush.
+		 * 相当于下发了多个preflush只执行了一次.
+		 * 根据下面的资料可以解释 (http://www.unjeep.com/article/40696.html):
+		 *
+		 * 冲刷的过程中request_queue为什么要使用双缓冲队列来存放fs_request.?
+                 * A:双缓冲队列可以做到只执行一次冲刷请求就可以完成多个fs_request的冲刷要求.
+		 * 队列自带的冲刷request在执行的过程中,blk_insert_flush()可以被调用多次,来自
+		 * 上层的fs_request被添加到pending1队列,等待冲刷request的下一次执行,当冲刷
+		 * requst可以再次被执行时,pending1队列不再接收新的fs_request(fs_request被加入
+		 * 到pending2队列),冲刷request执行完毕后,pending1队列所有的fs_request的
+		 * PREFLUSH/POSTFLUSH执行完毕.
+		 */
 		unsigned int seq = blk_flush_cur_seq(rq);
 
 		BUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);
@@ -278,15 +525,60 @@ static void flush_end_io(struct request *flush_rq, blk_status_t error)
  * spin_lock_irq(fq->mq_flush_lock)
  *
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|415| <<blk_flush_complete_seq>> blk_kick_flush(q, fq, cmd_flags);
+ */
 static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 			   unsigned int flags)
 {
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下使用flush_pending_idx:
+	 *   - block/blk-flush.c|276| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|392| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 *
+	 * 
+	 * 使用flush_queue[2]的地方:
+	 *   - block/blk-flush.c|352| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|454| <<flush_end_io>> running = &fq->flush_queue[fq->flush_running_idx];
+	 *   - block/blk-flush.c|501| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|849| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[0]);
+	 *   - block/blk-flush.c|850| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[1]);
+	 */
 	struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	/*
+	 * 插入的是tail, 用的是head
+	 * 注意, 这里只是用, 没有取出来!!!
+	 */
 	struct request *first_rq =
 		list_first_entry(pending, struct request, flush.list);
+	/*
+	 * 使用flush_rq的地方:
+	 *   - block/blk-flush.c|504| <<blk_kick_flush>> struct request *flush_rq = fq->flush_rq;
+	 *   - block/blk-flush.c|845| <<blk_alloc_flush_queue>> fq->flush_rq = kzalloc_node(rq_sz, flags, node);
+	 *   - block/blk-flush.c|846| <<blk_alloc_flush_queue>> if (!fq->flush_rq)
+	 *   - block/blk-flush.c|875| <<blk_free_flush_queue>> kfree(fq->flush_rq);
+	 *   - block/blk-mq.c|2303| <<blk_mq_exit_hctx>> set->ops->exit_request(set, hctx->fq->flush_rq, hctx_idx);
+	 *   - block/blk-mq.c|2361| <<blk_mq_init_hctx>> if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx,
+	 *   - block/blk.h|81| <<is_flush_rq>> return hctx->fq->flush_rq == req;
+	 */
 	struct request *flush_rq = fq->flush_rq;
 
 	/* C1 described at the top of this file */
+	/*
+	 * C1. At any given time, only one flush shall be in progress.  This makes
+	 *     double buffering sufficient.
+	 *
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 *
+	 * 在以下修改flush_running_idx:
+	 *   - block/blk-flush.c|362| <<flush_end_io>> fq->flush_running_idx ^= 1;
+	 */
 	if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
 		return;
 
@@ -296,6 +588,16 @@ static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 	 * assigned to empty flushes, and we deadlock if we are expecting
 	 * other requests to make progress. Don't defer for that case.
 	 */
+	/*
+	 * C2. Flush is deferred if any request is executing DATA of its sequence.
+	 *     This avoids issuing separate POSTFLUSHes for requests which shared
+	 *     PREFLUSH.
+	 *
+	 * C3. The second condition is ignored if there is a request which has
+	 *     waited longer than FLUSH_PENDING_TIMEOUT.  This is to avoid
+	 *     starvation in the unlikely case where there are continuous stream of
+	 *     FUA (without PREFLUSH) requests.
+	 */
 	if (!list_empty(&fq->flush_data_in_flight) && q->elevator &&
 	    time_before(jiffies,
 			fq->flush_pending_since + FLUSH_PENDING_TIMEOUT))
@@ -337,6 +639,31 @@ static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 	blk_flush_queue_rq(flush_rq, false);
 }
 
+/*
+ * [0] mq_flush_data_end_io
+ * [0] blk_mq_complete_request
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * [0] mq_flush_data_end_io
+ * [0] blk_mq_complete_request
+ * [0] nvme_irq
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * 在以下使用mq_flush_data_end_io():
+ *   - block/blk-flush.c|437| <<blk_insert_flush>> rq->end_io = mq_flush_data_end_io;
+ */
 static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
 {
 	struct request_queue *q = rq->q;
@@ -370,13 +697,54 @@ static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
  * @rq is being submitted.  Analyze what needs to be done and put it on the
  * right queue.
  */
+/*
+ * [0] blk_insert_flush
+ * [0] blk_mq_make_request
+ * [0] generic_make_request
+ * [0] submit_bio
+ * [0] submit_bh_wbc.isra
+ * [0] __sync_dirty_buffer
+ * [0] ext4_commit_super
+ * [0] ext4_mark_recovery_complete.isra
+ * [0] ext4_fill_super
+ * [0] mount_bdev
+ * [0] legacy_get_tree
+ * [0] vfs_get_tree
+ * [0] do_mount
+ * [0] do_mount_root
+ * [0] mount_block_root
+ * [0] mount_root
+ * [0] prepare_namespace
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - block/blk-mq-sched.c|388| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+ *   - block/blk-mq.c|2011| <<blk_mq_make_request>> blk_insert_flush(rq);
+ */
 void blk_insert_flush(struct request *rq)
 {
 	struct request_queue *q = rq->q;
 	unsigned long fflags = q->queue_flags;	/* may change, cache */
+	/*
+	 * 根据情况把REQ_FSEQ_DATA,REQ_FSEQ_PREFLUSH和REQ_FSEQ_POSTFLUSH返回到返回值中
+	 * 这些可以表明flush需要做的步骤(包含3步) ---> 也就是说,policy的3个bit表明要做哪些步骤!!!!
+	 *  - REQ_FSEQ_PREFLUSH(在数据请求以前冲刷磁盘缓存)
+	 *  - REQ_FSEQ_DATA(写入数据请求)
+	 *  - REQ_FSEQ_POSTFLUSH(在数据请求之后冲刷磁盘缓存)
+	 * 如果磁盘不支持FUA,那么可以使用REQ_FSEQ_POSTFLUSH代替
+	 * 假定我们分析的场景中,磁盘不支持FUA,则最终我们的冲刷策略为3步都做(policy=111).
+	 */
 	unsigned int policy = blk_flush_policy(fflags, rq);
+	/* 根据rq获取对应的hctx的blk_mq_hw_ctx->fq */
 	struct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);
 
+	/*
+	 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+	 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+	 */
+
 	/*
 	 * @policy now records what operations need to be done.  Adjust
 	 * REQ_PREFLUSH and FUA for the driver.
@@ -398,6 +766,10 @@ void blk_insert_flush(struct request *rq)
 	 * advertise a write-back cache.  In this case, simply
 	 * complete the request.
 	 */
+	/*
+	 * 正如上面注释,policy的3个bit表明要做哪些步骤!!!!
+	 * 没设置任何bit说明什么也不做
+	 */
 	if (!policy) {
 		blk_mq_end_request(rq, 0);
 		return;
@@ -422,12 +794,40 @@ void blk_insert_flush(struct request *rq)
 	 */
 	memset(&rq->flush, 0, sizeof(rq->flush));
 	INIT_LIST_HEAD(&rq->flush.list);
+	/*
+	 * 在以下使用RQF_FLUSH_SEQ:
+	 *   - block/blk-core.c|244| <<req_bio_endio>> if (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))
+	 *   - block/blk-core.c|1347| <<blk_account_io_done>> !(req->rq_flags & RQF_FLUSH_SEQ)) {
+	 *   - block/blk-flush.c|205| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|426| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|528| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+	 *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> if (rq->rq_flags & RQF_FLUSH_SEQ) {
+	 *   - block/blk-mq-sched.c|387| <<blk_mq_sched_insert_request>> if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
+	 *   - include/linux/blkdev.h|120| <<RQF_NOMERGE_FLAGS>> (RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
+	 */
 	rq->rq_flags |= RQF_FLUSH_SEQ;
+	/*
+	 * struct {
+	 *     unsigned int            seq;
+	 *     struct list_head        list;
+	 *     rq_end_io_fn            *saved_end_io;
+	 * } flush;
+	 *
+	 * 使用saved_end_io的地方:
+	 *   - block/blk-flush.c|221| <<blk_flush_restore_request>> rq->end_io = rq->flush.saved_end_io;
+	 *   - block/blk-flush.c|615| <<blk_insert_flush>> rq->flush.saved_end_io = rq->end_io;
+	 */
 	rq->flush.saved_end_io = rq->end_io; /* Usually NULL */
 
 	rq->end_io = mq_flush_data_end_io;
 
 	spin_lock_irq(&fq->mq_flush_lock);
+	/*
+	 * 参数REQ_FSEQ_ACTIONS & ~policy表示可以跳过的步骤!!!
+	 * blk_insert_flush()初始化request的完成函数为flush_data_end_io()后,
+	 * 调用REQ_FSEQ_ACTIONS & ~policy把可以跳过的步骤对应的位置填1,
+	 * 调用blk_flush_complete_seq()开始冲刷过程.
+	 */
 	blk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);
 	spin_unlock_irq(&fq->mq_flush_lock);
 }
@@ -443,6 +843,31 @@ void blk_insert_flush(struct request *rq)
  *    room for storing the error offset in case of a flush error, if they
  *    wish to.
  */
+/*
+ * called by:
+ *   - drivers/md/dm-integrity.c|2551| <<bitmap_flush_work>> blkdev_issue_flush(ic->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|663| <<dmz_write_sb>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|705| <<dmz_write_dirty_mblocks>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|774| <<dmz_flush_metadata>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/raid5-ppl.c|1040| <<ppl_recover>> ret = blkdev_issue_flush(rdev->bdev, GFP_KERNEL, NULL);
+ *   - drivers/nvme/target/io-cmd-bdev.c|229| <<nvmet_bdev_flush>> if (blkdev_issue_flush(req->ns->bdev, GFP_KERNEL, NULL))
+ *   - fs/block_dev.c|675| <<blkdev_fsync>> error = blkdev_issue_flush(bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/fsync.c|179| <<ext4_sync_file>> err = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/ialloc.c|1422| <<ext4_init_inode_table>> blkdev_issue_flush(sb->s_bdev, GFP_NOFS, NULL);
+ *   - fs/ext4/super.c|5170| <<ext4_sync_fs>> err = blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/fat/file.c|198| <<fat_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/inode.c|343| <<hfsplus_file_fsync>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/super.c|242| <<hfsplus_sync_fs>> blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/jbd2/checkpoint.c|417| <<jbd2_cleanup_journal_tail>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|778| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|885| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/recovery.c|289| <<jbd2_journal_recover>> err2 = blkdev_issue_flush(journal->j_fs_dev, GFP_KERNEL, NULL);
+ *   - fs/libfs.c|1044| <<generic_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/nilfs2/the_nilfs.h|378| <<nilfs_flush_device>> err = blkdev_issue_flush(nilfs->ns_bdev, GFP_KERNEL, NULL);
+ *   - fs/ocfs2/file.c|197| <<ocfs2_sync_file>> ret = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/reiserfs/file.c|162| <<reiserfs_sync_file>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/xfs/xfs_super.c|339| <<xfs_blkdev_issue_flush>> blkdev_issue_flush(buftarg->bt_bdev, GFP_NOFS, NULL);
+ */
 int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 		sector_t *error_sector)
 {
@@ -485,6 +910,10 @@ int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 }
 EXPORT_SYMBOL(blkdev_issue_flush);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2415| <<blk_mq_alloc_hctx>> hctx->fq = blk_alloc_flush_queue(q, hctx->numa_node, set->cmd_size,
+ */
 struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 		int node, int cmd_size, gfp_t flags)
 {
@@ -517,6 +946,10 @@ struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|43| <<blk_mq_hw_sysfs_release>> blk_free_flush_queue(hctx->fq);
+ */
 void blk_free_flush_queue(struct blk_flush_queue *fq)
 {
 	/* bio based request queue hasn't flush queue */
diff --git a/block/blk-lib.c b/block/blk-lib.c
index 5f2c429d4378..f526ffa3bfb9 100644
--- a/block/blk-lib.c
+++ b/block/blk-lib.c
@@ -209,6 +209,11 @@ int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 }
 EXPORT_SYMBOL(blkdev_issue_write_same);
 
+/*
+ * called by:
+ *   - block/blk-lib.c|335| <<__blkdev_issue_zeroout>> ret = __blkdev_issue_write_zeroes(bdev, sector, nr_sects, gfp_mask,
+ *   - block/blk-lib.c|375| <<blkdev_issue_zeroout>> ret = __blkdev_issue_write_zeroes(bdev, sector, nr_sects,
+ */
 static int __blkdev_issue_write_zeroes(struct block_device *bdev,
 		sector_t sector, sector_t nr_sects, gfp_t gfp_mask,
 		struct bio **biop, unsigned flags)
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 1534ed736363..310e15dbf490 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -140,15 +140,44 @@ static struct bio *blk_bio_write_same_split(struct request_queue *q,
  * requests that are submitted to a block device if the start of a bio is not
  * aligned to a physical block boundary.
  */
+/*
+ * called by:
+ *   - block/blk-merge.c|252| <<blk_bio_segment_split>> const unsigned max_sectors = get_max_io_size(q, bio);
+ */
 static inline unsigned get_max_io_size(struct request_queue *q,
 				       struct bio *bio)
 {
+	/*
+	 * Return maximum size of a request at given offset. Only valid for
+	 * file system requests.
+	 */
 	unsigned sectors = blk_max_size_offset(q, bio->bi_iter.bi_sector);
 	unsigned max_sectors = sectors;
+	/*
+	 * 假设512向右移9位是1, 1个bs
+	 * 假设4096向右移9位是8, 8个bs (二进制1000)
+	 *
+	 * 在virtio测试的结果:
+	 * [  101.603442] orabug: sectors=2560, max_sectors=2560, pbs=1, lbs=1, start_offset=0, q->limits.max_sectors=2560
+	 * [  101.605255] orabug: sectors=2560, max_sectors=2560, pbs=1, lbs=1, start_offset=0, q->limits.max_sectors=2560
+	 * [  101.606388] orabug: sectors=2560, max_sectors=2560, pbs=1, lbs=1, start_offset=0, q->limits.max_sectors=2560
+	 */
 	unsigned pbs = queue_physical_block_size(q) >> SECTOR_SHIFT;
 	unsigned lbs = queue_logical_block_size(q) >> SECTOR_SHIFT;
 	unsigned start_offset = bio->bi_iter.bi_sector & (pbs - 1);
 
+	/*
+	 * Return the maximum number of sectors from the start of a bio that may be
+	 * submitted as a single request to a block device. If enough sectors remain,
+	 * align the end to the physical block size. Otherwise align the end to the
+	 * logical block size. This approach minimizes the number of non-aligned
+	 * requests that are submitted to a block device if the start of a bio is not
+	 * aligned to a physical block boundary.
+	 */
+
+	/*
+	 * max_sectors只在下面两行修改, 修改的时候只用pbs修改
+	 */
 	max_sectors += start_offset;
 	max_sectors &= ~(pbs - 1);
 	if (max_sectors > start_offset)
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 0157f2b3485a..5675db0eb4cf 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -15,9 +15,17 @@
 #include "blk.h"
 #include "blk-mq.h"
 
+/*
+ * 参数的'struct blk_mq_queue_map'是在set中的
+ */
 static int queue_index(struct blk_mq_queue_map *qmap,
 		       unsigned int nr_queues, const int q)
 {
+	/*
+	 * First hardware queue to map onto. Used by the PCIe NVMe
+	 * driver to map each hardware queue type (enum hctx_type) onto a distinct
+	 * set of hardware queues.
+	 */
 	return qmap->queue_offset + (q % nr_queues);
 }
 
@@ -32,6 +40,20 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|42| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|44| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3016| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3309| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|454| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|1852| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2195| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2196| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/nvme/host/tcp.c|2205| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7146| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1778| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
@@ -83,6 +105,12 @@ EXPORT_SYMBOL_GPL(blk_mq_map_queues);
  * We have no quick way of doing reverse lookups. This is only used at
  * queue init time, so runtime isn't important.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2092| <<blk_mq_alloc_rq_map>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2148| <<blk_mq_alloc_rqs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2805| <<blk_mq_realloc_hw_ctxs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], i);
+ */
 int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int index)
 {
 	int i;
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index b3f2ba483992..096c3fed55a9 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -128,6 +128,11 @@ static const char *const blk_queue_flag_name[] = {
 };
 #undef QUEUE_FLAG_NAME
 
+/*
+ * virtblk的例子
+ * # cat /sys/kernel/debug/block/vdb/state
+ * SAME_COMP|IO_STAT|DISCARD|INIT_DONE|WC|STATS|REGISTERED
+ */
 static int queue_state_show(void *data, struct seq_file *m)
 {
 	struct request_queue *q = data;
@@ -606,6 +611,10 @@ static ssize_t hctx_run_write(void *data, const char __user *buf, size_t count,
 	return count;
 }
 
+/*
+ * 在以下使用hctx_active_show():
+ *   - block/blk-mq-debugfs.c|797| <<global>> {"active", 0400, hctx_active_show},
+ */
 static int hctx_active_show(void *data, struct seq_file *m)
 {
 	struct blk_mq_hw_ctx *hctx = data;
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index b595a94c4d16..c3f4495a2201 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -11,6 +11,32 @@
 
 #include "blk-mq.h"
 
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */
+
 /**
  * blk_mq_pci_map_queues - provide a default queue mapping for PCI device
  * @qmap:	CPU to hardware queue map.
@@ -23,6 +49,12 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|452| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7148| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5830| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[HCTX_TYPE_DEFAULT],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
diff --git a/block/blk-mq-rdma.c b/block/blk-mq-rdma.c
index 14f968e58b8f..9292c590e68e 100644
--- a/block/blk-mq-rdma.c
+++ b/block/blk-mq-rdma.c
@@ -21,6 +21,11 @@
  * @set->nr_hw_queues, or @dev does not provide an affinity mask for a
  * vector, we fallback to the naive mapping.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1840| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/nvme/host/rdma.c|1842| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_READ],
+ */
 int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		struct ib_device *dev, int first_vec)
 {
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index ca22afd47b3d..c3e897fc45a4 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -85,6 +85,11 @@ void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|215| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *   - block/blk-mq-sched.c|220| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ */
 static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -167,6 +172,18 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1425| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ *
+ * blk_mq_delay_run_hw_queue() or blk_mq_run_hw_queue()
+ *  -> __blk_mq_delay_run_hw_queue()
+ *      -> __blk_mq_run_hw_queue()
+ *
+ * __blk_mq_delay_run_hw_queue()
+ *  -> 调度hctx->run_work = blk_mq_run_work_fn()
+ *      -> __blk_mq_run_hw_queue()
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -357,6 +374,10 @@ void blk_mq_sched_request_inserted(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
 
+/*
+ * called by only:
+ *   - block/blk-mq-sched.c|411| <<blk_mq_sched_insert_request>> if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
+ */
 static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 				       bool has_sched,
 				       struct request *rq)
@@ -375,6 +396,14 @@ static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-exec.c|64| <<blk_execute_rq_nowait>> blk_mq_sched_insert_request(rq, at_head, true, false);
+ *   - block/blk-mq.c|812| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, true, false, false);
+ *   - block/blk-mq.c|818| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, false, false, false);
+ *   - block/blk-mq.c|2177| <<blk_mq_make_request>> blk_mq_sched_insert_request(rq, false, true, true);
+ *   - block/blk-mq.c|2205| <<blk_mq_make_request>> blk_mq_sched_insert_request(rq, false, true, true);
+ */
 void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 				 bool run_queue, bool async)
 {
@@ -410,6 +439,11 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		blk_mq_run_hw_queue(hctx, async);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1995| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx,
+ *   - block/blk-mq.c|2016| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx, &rq_list,
+ */
 void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 				  struct blk_mq_ctx *ctx,
 				  struct list_head *list, bool run_queue_async)
@@ -433,6 +467,15 @@ void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 		 * busy in case of 'none' scheduler, and this way may save
 		 * us one extra enqueue & dequeue to sw queue.
 		 */
+		/*
+		 * 在以下使用dispatch_busy:
+		 *   - block/blk-mq-debugfs.c|621| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+		 *   - block/blk-mq-sched.c|217| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+		 *   - block/blk-mq-sched.c|436| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+		 *   - block/blk-mq.c|1215| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+		 *   - block/blk-mq.c|1225| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+		 *   - block/blk-mq.c|2071| <<blk_mq_make_request>> !data.hctx->dispatch_busy) {
+		 */
 		if (!hctx->dispatch_busy && !e && !run_queue_async) {
 			blk_mq_try_issue_list_directly(hctx, list);
 			if (list_empty(list))
diff --git a/block/blk-mq-sysfs.c b/block/blk-mq-sysfs.c
index 062229395a50..7d3eea033d4f 100644
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@ -253,6 +253,9 @@ static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 	if (ret)
 		return ret;
 
+	/*
+	 * 对于hctx->nr_ctx范围内的每一个hctx->ctxs[i]
+	 */
 	hctx_for_each_ctx(hctx, ctx, i) {
 		ret = kobject_add(&ctx->kobj, &hctx->kobj, "cpu%u", ctx->cpu);
 		if (ret)
@@ -269,6 +272,7 @@ void blk_mq_unregister_dev(struct device *dev, struct request_queue *q)
 
 	lockdep_assert_held(&q->sysfs_dir_lock);
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_unregister_hctx(hctx);
 
@@ -296,6 +300,10 @@ void blk_mq_sysfs_deinit(struct request_queue *q)
 	kobject_put(q->mq_kobj);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2871| <<blk_mq_init_allocated_queue>> blk_mq_sysfs_init(q);
+ */
 void blk_mq_sysfs_init(struct request_queue *q)
 {
 	struct blk_mq_ctx *ctx;
@@ -311,6 +319,10 @@ void blk_mq_sysfs_init(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|983| <<blk_register_queue>> __blk_mq_register_dev(dev, q);
+ */
 int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -325,6 +337,7 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 
 	kobject_uevent(q->mq_kobj, KOBJ_ADD);
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		ret = blk_mq_register_hctx(hctx);
 		if (ret)
@@ -355,6 +368,7 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	if (!q->mq_sysfs_init_done)
 		goto unlock;
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_unregister_hctx(hctx);
 
@@ -362,6 +376,10 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	mutex_unlock(&q->sysfs_dir_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3317| <<__blk_mq_update_nr_hw_queues>> blk_mq_sysfs_register(q);
+ */
 int blk_mq_sysfs_register(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -371,6 +389,7 @@ int blk_mq_sysfs_register(struct request_queue *q)
 	if (!q->mq_sysfs_init_done)
 		goto unlock;
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		ret = blk_mq_register_hctx(hctx);
 		if (ret)
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index fbacde454718..44d4b13b8db7 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -21,8 +21,28 @@
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|60| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 使用BLK_MQ_S_TAG_ACTIVE的地方:
+	 *   - block/blk-mq-tag.c|26| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|27| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|51| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|70| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *
+	 * 使用active_queues的地方:
+	 *   - block/blk-mq-tag.c|28| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|54| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|79| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *
+	 * For shared tag users, we track the number of currently active users
+	 * and attempt to provide a fair share of the tag depth for each of them.
+	 */
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
 	    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		atomic_inc(&hctx->tags->active_queues);
@@ -33,6 +53,11 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|56| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|273| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -44,6 +69,10 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|111| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -60,6 +89,10 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|94| <<__blk_mq_get_tag>> !hctx_may_queue(data->hctx, bt))
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
@@ -87,6 +120,12 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	return atomic_read(&hctx->nr_active) < depth;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|152| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|174| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|180| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ */
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
@@ -99,6 +138,11 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 		return __sbitmap_queue_get(bt);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|414| <<blk_mq_get_request>> tag = blk_mq_get_tag(data);
+ *   - block/blk-mq.c|1304| <<blk_mq_get_driver_tag>> rq->tag = blk_mq_get_tag(&data);
+ */
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
@@ -120,6 +164,9 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		tag_offset = tags->nr_reserved_tags;
 	}
 
+	/*
+	 * 这里会set sbitmap的bit
+	 */
 	tag = __blk_mq_get_tag(data, bt);
 	if (tag != -1)
 		goto found_tag;
@@ -183,6 +230,12 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	return tag + tag_offset;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|527| <<__blk_mq_free_request>> blk_mq_put_tag(hctx, hctx->tags, ctx, rq->tag);
+ *   - block/blk-mq.c|529| <<__blk_mq_free_request>> blk_mq_put_tag(hctx, hctx->sched_tags, ctx, sched_tag);
+ *   - block/blk-mq.h|282| <<__blk_mq_put_driver_tag>> blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
+ */
 void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 		    struct blk_mq_ctx *ctx, unsigned int tag)
 {
@@ -204,6 +257,10 @@ struct bt_iter_data {
 	bool reserved;
 };
 
+/*
+ * 在以下使用bt_iter():
+ *   - block/blk-mq-tag.c|301| <<bt_for_each>> sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
+ */
 static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_iter_data *iter_data = data;
@@ -220,6 +277,10 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 	 * We can hit rq == NULL here, because the tagging functions
 	 * test and set the bit before assigning ->rqs[].
 	 */
+	/*
+	 * struct blk_mq_hw_ctx *hctx:
+	 *  -> struct request_queue *queue;
+	 */
 	if (rq && rq->q == hctx->queue)
 		return iter_data->fn(hctx, rq, iter_data->data, reserved);
 	return true;
@@ -239,6 +300,11 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|589| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|590| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
+ */
 static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 			busy_iter_fn *fn, void *data, bool reserved)
 {
@@ -259,6 +325,13 @@ struct bt_tags_iter_data {
 	bool reserved;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|356| <<bt_tags_for_each>> sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
+ *
+ * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+ * 才调用iter_data->fn()!!!
+ */
 static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_tags_iter_data *iter_data = data;
@@ -274,6 +347,11 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 	 * test and set the bit before assining ->rqs[].
 	 */
 	rq = tags->rqs[bitnr];
+	/*
+	 * !!!!!!!!!!!!!!!!!!!!!!
+	 * blk_mq_request_started()返回(blk_mq_rq_state(rq) != MQ_RQ_IDLE)
+	 * !!!!!!!!!!!!!!!!!!!!!!
+	 */
 	if (rq && blk_mq_request_started(rq))
 		return iter_data->fn(rq, iter_data->data, reserved);
 
@@ -293,6 +371,18 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|373| <<blk_mq_all_tag_busy_iter>> bt_tags_for_each(tags, &tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|374| <<blk_mq_all_tag_busy_iter>> bt_tags_for_each(tags, &tags->bitmap_tags, fn, priv, false);
+ *
+ * !!!!!!!!!
+ * 关于bt_tags_iter():
+ * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+ * 才调用iter_data->fn()!!!
+ * 是针对started request的
+ * !!!!!!!!!
+ */
 static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 			     busy_tag_iter_fn *fn, void *data, bool reserved)
 {
@@ -303,6 +393,13 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 		.reserved = reserved,
 	};
 
+	/*
+	 * !!!!!!!!!
+	 * 关于bt_tags_iter():
+	 * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+	 * 才调用iter_data->fn()!!!
+	 * !!!!!!!!!
+	 */
 	if (tags->rqs)
 		sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
 }
@@ -317,9 +414,28 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|475| <<blk_mq_tagset_busy_iter>> blk_mq_all_tag_busy_iter(tagset->tags[i], fn, priv);
+ *
+ * !!!!!!!!!
+ * 关于bt_tags_iter():
+ * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+ * 才调用iter_data->fn()!!!
+ * 是针对started request的
+ * !!!!!!!!!
+ */
 static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
 		busy_tag_iter_fn *fn, void *priv)
 {
+	/*
+	 * !!!!!!!!!
+	 * 关于bt_tags_iter():
+	 * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+	 * 才调用iter_data->fn()!!!
+	 * 是针对started request的
+	 * !!!!!!!!!
+	 */
 	if (tags->nr_reserved_tags)
 		bt_tags_for_each(tags, &tags->breserved_tags, fn, priv, true);
 	bt_tags_for_each(tags, &tags->bitmap_tags, fn, priv, false);
@@ -335,23 +451,79 @@ static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|419| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq,
+ *   - block/blk-mq-tag.c|504| <<blk_mq_tagset_wait_completed_request>> blk_mq_tagset_busy_iter(tagset,
+ *   - drivers/block/mtip32xx/mtip32xx.c|2684| <<mtip_service_thread>> blk_mq_tagset_busy_iter(&dd->tags, mtip_queue_cmd, dd);
+ *   - drivers/block/mtip32xx/mtip32xx.c|2689| <<mtip_service_thread>> blk_mq_tagset_busy_iter(&dd->tags,
+ *   - drivers/block/mtip32xx/mtip32xx.c|3803| <<mtip_block_remove>> blk_mq_tagset_busy_iter(&dd->tags, mtip_no_dev_cleanup, dd);
+ *   - drivers/block/nbd.c|811| <<nbd_clear_que>> blk_mq_tagset_busy_iter(&nbd->tag_set, nbd_clear_req, NULL);
+ *   - drivers/block/skd_main.c|396| <<skd_in_flight>> blk_mq_tagset_busy_iter(&skdev->tag_set, skd_inc_in_flight, &count);
+ *   - drivers/block/skd_main.c|1917| <<skd_recover_requests>> blk_mq_tagset_busy_iter(&skdev->tag_set, skd_recover_request, skdev);
+ *   - drivers/nvme/host/fc.c|2864| <<nvme_fc_delete_association>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/host/fc.c|2887| <<nvme_fc_delete_association>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/nvme/host/pci.c|3916| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3917| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|919| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->ctrl.admin_tagset,
+ *   - drivers/nvme/host/rdma.c|935| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->ctrl.tagset,
+ *   - drivers/nvme/host/tcp.c|1780| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->admin_tagset,
+ *   - drivers/nvme/host/tcp.c|1803| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|630| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/target/loop.c|640| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/scsi/hosts.c|578| <<scsi_host_busy>> blk_mq_tagset_busy_iter(&shost->tag_set,
+ *
+ * !!!!!!!!!
+ * 关于bt_tags_iter():
+ * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+ * 才调用iter_data->fn()!!!
+ * 是针对started request的
+ * !!!!!!!!!
+ */
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv)
 {
 	int i;
 
 	for (i = 0; i < tagset->nr_hw_queues; i++) {
+		/*
+		 * !!!!!!!!!
+		 * 关于bt_tags_iter():
+		 * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+		 * 才调用iter_data->fn()!!!
+		 * 是针对started request的
+		 * !!!!!!!!!
+		 */
 		if (tagset->tags && tagset->tags[i])
 			blk_mq_all_tag_busy_iter(tagset->tags[i], fn, priv);
 	}
 }
 EXPORT_SYMBOL(blk_mq_tagset_busy_iter);
 
+/*
+ * 在以下使用:
+ *   - block/blk-mq-tag.c|415| <<blk_mq_tagset_wait_completed_request>> blk_mq_tagset_count_completed_rqs, &count);
+ */
 static bool blk_mq_tagset_count_completed_rqs(struct request *rq,
 		void *data, bool reserved)
 {
 	unsigned *count = data;
 
+	/*
+	 * 使用MQ_RQ_IN_FLIGHT的地方:
+	 *   - block/blk-mq-debugfs.c|317| <<global>> [MQ_RQ_IN_FLIGHT] = "in_flight",
+	 *   - block/blk-mq.c|791| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *   - block/blk-mq.c|1017| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|1059| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 *
+	 * 使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+	 *   - block/blk-mq.c|580| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|3474| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 *   - include/linux/blk-mq.h|481| <<blk_mq_request_completed>> return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
+	 *
+	 * 如果blk_mq_rq_state(rq) == MQ_RQ_COMPLETE
+	 *//
 	if (blk_mq_request_completed(rq))
 		(*count)++;
 	return true;
@@ -364,11 +536,42 @@ static bool blk_mq_tagset_count_completed_rqs(struct request *rq,
  *
  * Note: This function has to be run after all IO queues are shutdown
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2792| <<nvme_fc_delete_association>> blk_mq_tagset_wait_completed_request(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|2815| <<nvme_fc_delete_association>> blk_mq_tagset_wait_completed_request(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|3819| <<nvme_dev_disable>> blk_mq_tagset_wait_completed_request(&dev->tagset);
+ *   - drivers/nvme/host/pci.c|3820| <<nvme_dev_disable>> blk_mq_tagset_wait_completed_request(&dev->admin_tagset);
+ *   - drivers/nvme/host/rdma.c|921| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_wait_completed_request(ctrl->ctrl.admin_tagset);
+ *   - drivers/nvme/host/rdma.c|937| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_wait_completed_request(ctrl->ctrl.tagset);
+ *   - drivers/nvme/host/tcp.c|1782| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_wait_completed_request(ctrl->admin_tagset);
+ *   - drivers/nvme/host/tcp.c|1799| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_wait_completed_request(ctrl->tagset);
+ *   - drivers/nvme/target/loop.c|412| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_wait_completed_request(&ctrl->tag_set);
+ *   - drivers/nvme/target/loop.c|422| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_wait_completed_request(&ctrl->admin_tag_set);
+ *
+ * !!!!!!!!!
+ * 关于bt_tags_iter():
+ * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+ * 才调用iter_data->fn()!!!
+ * 是针对started request的
+ * !!!!!!!!!
+ */
 void blk_mq_tagset_wait_completed_request(struct blk_mq_tag_set *tagset)
 {
 	while (true) {
 		unsigned count = 0;
 
+		/*
+		 * blk_mq_tagset_count_completed_rqs():
+		 * 如果blk_mq_rq_state(rq) == MQ_RQ_COMPLETE就增加count
+		 *
+		 * !!!!!!!!!
+		 * 关于bt_tags_iter():
+		 * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+		 * 才调用iter_data->fn()!!!
+		 * 是针对started request的
+		 * !!!!!!!!!
+		 */
 		blk_mq_tagset_busy_iter(tagset,
 				blk_mq_tagset_count_completed_rqs, &count);
 		if (!count)
@@ -392,6 +595,15 @@ EXPORT_SYMBOL(blk_mq_tagset_wait_completed_request);
  * called for all requests on all queues that share that tag set and not only
  * for requests associated with @q.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|128| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|138| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|1313| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|1442| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ *
+ * 这个函数会先percpu_ref_tryget(&q->q_usage_counter)
+ */
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv)
 {
@@ -425,6 +637,11 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 	blk_queue_exit(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|651| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->bitmap_tags, depth, round_robin, node))
+ *   - block/blk-mq-tag.c|653| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->breserved_tags, tags->nr_reserved_tags, round_robin,
+ */
 static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 		    bool round_robin, int node)
 {
@@ -432,6 +649,10 @@ static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 				       node);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|694| <<blk_mq_init_tags>> return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
+ */
 static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 						   int node, int alloc_policy)
 {
@@ -452,6 +673,10 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2096| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
@@ -473,6 +698,12 @@ struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 	return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2751| <<blk_mq_free_rq_map>> blk_mq_free_tags(tags);
+ *   - block/blk-mq.c|2775| <<blk_mq_alloc_rq_map>> blk_mq_free_tags(tags);
+ *   - block/blk-mq.c|2784| <<blk_mq_alloc_rq_map>> blk_mq_free_tags(tags);
+ */
 void blk_mq_free_tags(struct blk_mq_tags *tags)
 {
 	sbitmap_queue_free(&tags->bitmap_tags);
@@ -480,6 +711,11 @@ void blk_mq_free_tags(struct blk_mq_tags *tags)
 	kfree(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|4033| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,
+ *   - block/blk-mq.c|4036| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,
+ */
 int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 			    struct blk_mq_tags **tagsptr, unsigned int tdepth,
 			    bool can_grow)
@@ -545,6 +781,22 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
  * Note: When called for a request that is queued on a non-multiqueue request
  * queue, the hardware context index is set to zero.
  */
+/*
+ * called by:
+ *   - drivers/block/nbd.c|178| <<nbd_cmd_handle>> u32 tag = blk_mq_unique_tag(req);
+ *   - drivers/block/skd_main.c|486| <<skd_mq_queue_rq>> const u32 tag = blk_mq_unique_tag(req);
+ *   - drivers/block/skd_main.c|607| <<skd_timed_out>> blk_mq_unique_tag(req));
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|2368| <<srp_queuecommand>> tag = blk_mq_unique_tag(scmnd->request);
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|2998| <<srp_abort>> tag = blk_mq_unique_tag(scmnd->request);
+ *   - drivers/nvme/host/nvme.h|194| <<nvme_req_qid>> return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+ *   - drivers/scsi/cxlflash/main.c|439| <<cmd_to_target_hwq>> tag = blk_mq_unique_tag(scp->request);
+ *   - drivers/scsi/lpfc/lpfc_scsi.c|645| <<lpfc_get_scsi_buf_s4>> tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/qla2xxx/qla_os.c|822| <<qla2xxx_queuecommand>> tag = blk_mq_unique_tag(cmd->request);
+ *   - drivers/scsi/scsi_debug.c|3698| <<get_queue>> u32 tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|5625| <<scsi_debug_queuecommand>> blk_mq_unique_tag(scp->request), b);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5299| <<pqi_get_hw_queue>> hw_queue = blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(scmd->request));
+ *   - drivers/scsi/virtio_scsi.c|535| <<virtscsi_pick_vq_mq>> u32 tag = blk_mq_unique_tag(sc->request);
+ */
 u32 blk_mq_unique_tag(struct request *rq)
 {
 	return (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 15bc74acb57e..ad5f9f38367c 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -9,14 +9,37 @@
  */
 struct blk_mq_tags {
 	unsigned int nr_tags;
+	/*
+	 * 设置nr_reserved_tags的地方:
+	 *   - block/blk-mq-tag.c|471| <<blk_mq_init_tags>> tags->nr_reserved_tags = reserved_tags;
+	 */
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 使用active_queues的地方:
+	 *   - block/blk-mq-tag.c|28| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|54| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|79| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
 	struct sbitmap_queue breserved_tags;
 
 	struct request **rqs;
+	/*
+	 * static_rqs一共是二维:
+	 *   - block/blk-mq.c|289| <<blk_mq_rq_ctx_init>> struct request *rq = tags->static_rqs[tag];
+	 *   - block/blk-mq.c|2053| <<blk_mq_free_rqs>> struct request *rq = tags->static_rqs[i];
+	 *   - block/blk-mq.c|2058| <<blk_mq_free_rqs>> tags->static_rqs[i] = NULL;
+	 *   - block/blk-mq.c|2078| <<blk_mq_free_rq_map>> kfree(tags->static_rqs);
+	 *   - block/blk-mq.c|2079| <<blk_mq_free_rq_map>> tags->static_rqs = NULL;
+	 *   - block/blk-mq.c|2109| <<blk_mq_alloc_rq_map>> tags->static_rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *   - block/blk-mq.c|2112| <<blk_mq_alloc_rq_map>> if (!tags->static_rqs) {
+	 *   - block/blk-mq.c|2201| <<blk_mq_alloc_rqs>> tags->static_rqs[i] = rq;
+	 *   - block/blk-mq.c|2203| <<blk_mq_alloc_rqs>> tags->static_rqs[i] = NULL;
+	 */
 	struct request **static_rqs;
 	struct list_head page_list;
 };
@@ -35,11 +58,21 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|130| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|177| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1133| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
 	if (!hctx)
 		return &bt->ws[0];
+	/*
+	 * 这里似乎是唯一使用的地方:
+	 * Index of next available dispatch_wait queue to insert requests.
+	 */
 	return sbq_wait_ptr(bt, &hctx->wait_index);
 }
 
@@ -52,6 +85,11 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * calld by:
+ *   - block/blk-mq.c|387| <<blk_mq_get_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|1067| <<blk_mq_get_driver_tag>> shared = blk_mq_tag_busy(data.hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -60,6 +98,11 @@ static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|963| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2264| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -74,12 +117,22 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * in flight at the same time. The caller has to make sure the tag
  * can't be freed.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|243| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+ *   - block/blk-flush.c|326| <<blk_kick_flush>> blk_mq_tag_set_rq(flush_rq->mq_hctx, first_rq->tag, flush_rq);
+ */
 static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 		unsigned int tag, struct request *rq)
 {
 	hctx->tags->rqs[tag] = rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|189| <<blk_mq_put_tag>> if (!blk_mq_tag_is_reserved(tags, tag)) {
+ *   - block/blk-mq.c|1064| <<blk_mq_get_driver_tag>> if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
+ */
 static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
 					  unsigned int tag)
 {
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 488341628256..16ece920ec49 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -9,6 +9,32 @@
 #include <linux/module.h>
 #include "blk-mq.h"
 
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */
+
 /**
  * blk_mq_virtio_map_queues - provide a default queue mapping for virtio device
  * @qmap:	CPU to hardware queue map.
@@ -21,6 +47,11 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|697| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 323c9cb28066..44ffb7f53b47 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -43,13 +43,22 @@
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2833| <<blk_mq_init_allocated_queue>> blk_mq_poll_stats_bkt,
+ *   - block/blk-mq.c|3365| <<blk_mq_poll_nsecs>> bucket = blk_mq_poll_stats_bkt(rq);
+ */
 static int blk_mq_poll_stats_bkt(const struct request *rq)
 {
 	int ddir, sectors, bucket;
 
+	/* read or write */
 	ddir = rq_data_dir(rq);
 	sectors = blk_rq_stats_sectors(rq);
 
+	/*
+	 * ilog2(sectors)相当于获得sectors的order吧
+	 */
 	bucket = ddir + 2 * ilog2(sectors);
 
 	if (bucket < 0)
@@ -64,6 +73,10 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
  * Check if any of the ctx, dispatch list or elevator
  * have pending work in this hardware queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1469| <<blk_mq_run_hw_queue>> blk_mq_hctx_has_pending(hctx);
+ */
 static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
 	return !list_empty_careful(&hctx->dispatch) ||
@@ -108,6 +121,10 @@ static bool blk_mq_check_inflight(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/genhd.c|75| <<part_in_flight>> return blk_mq_in_flight(q, part);
+ */
 unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)
 {
 	struct mq_inflight mi = { .part = part };
@@ -117,6 +134,10 @@ unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)
 	return mi.inflight[0] + mi.inflight[1];
 }
 
+/*
+ * called by:
+ *   - block/genhd.c|99| <<part_in_flight_rw>> blk_mq_in_flight_rw(q, part, inflight);
+ */
 void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 			 unsigned int inflight[2])
 {
@@ -127,10 +148,69 @@ void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 	inflight[1] = mi.inflight[1];
 }
 
+/*
+ * [0] blk_freeze_queue_start
+ * [0] blk_mq_freeze_queue
+ * [0] nvme_update_disk_info
+ * [0] __nvme_revalidate_disk
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] blk_freeze_queue_start
+ * [0] blk_mq_freeze_queue
+ * [0] nvme_update_disk_info
+ * [0] __nvme_revalidate_disk
+ * [0] nvme_revalidate_disk
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - block/blk-core.c|338| <<blk_set_queue_dying>> blk_freeze_queue_start(q);
+ *   - block/blk-mq.c|243| <<blk_freeze_queue>> blk_freeze_queue_start(q);
+ *   - block/blk-pm.c|79| <<blk_pre_runtime_suspend>> blk_freeze_queue_start(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|3801| <<mtip_block_remove>> blk_freeze_queue_start(dd->queue);
+ *   - drivers/nvdimm/pmem.c|327| <<pmem_pagemap_kill>> blk_freeze_queue_start(q);
+ *   - drivers/nvme/host/core.c|5973| <<nvme_start_freeze>> blk_freeze_queue_start(ns->queue);
+ *   - drivers/nvme/host/multipath.c|72| <<nvme_mpath_start_freeze>> blk_freeze_queue_start(h->disk->queue);
+ *
+ * 核心是ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+ * 这样blk_queue_enter()这样的函数就没法前进了
+ */
 void blk_freeze_queue_start(struct request_queue *q)
 {
+	/*
+	 * 在以下使用mq_freeze_lock:
+	 *   - block/blk-core.c|610| <<blk_alloc_queue_node>> mutex_init(&q->mq_freeze_lock);
+	 *   - block/blk-mq.c|145| <<blk_freeze_queue_start>> mutex_lock(&q->mq_freeze_lock);
+	 *   - block/blk-mq.c|148| <<blk_freeze_queue_start>> mutex_unlock(&q->mq_freeze_lock);
+	 *   - block/blk-mq.c|152| <<blk_freeze_queue_start>> mutex_unlock(&q->mq_freeze_lock);
+	 *   - block/blk-mq.c|207| <<blk_mq_unfreeze_queue>> mutex_lock(&q->mq_freeze_lock);
+	 *   - block/blk-mq.c|214| <<blk_mq_unfreeze_queue>> mutex_unlock(&q->mq_freeze_lock);
+	 */
 	mutex_lock(&q->mq_freeze_lock);
+	/*
+	 * 在以下使用mq_freeze_depth:
+	 *   - block/blk-core.c|536| <<blk_queue_enter>> (!q->mq_freeze_depth &&
+	 *   - block/blk-mq.c|155| <<blk_freeze_queue_start>> if (++q->mq_freeze_depth == 1) {
+	 *   - block/blk-mq.c|221| <<blk_mq_unfreeze_queue>> q->mq_freeze_depth--;
+	 *   - block/blk-mq.c|222| <<blk_mq_unfreeze_queue>> WARN_ON_ONCE(q->mq_freeze_depth < 0);
+	 *   - block/blk-mq.c|223| <<blk_mq_unfreeze_queue>> if (!q->mq_freeze_depth) {
+	 */
 	if (++q->mq_freeze_depth == 1) {
+		/*
+		 * 核心是ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+		 */
 		percpu_ref_kill(&q->q_usage_counter);
 		mutex_unlock(&q->mq_freeze_lock);
 		if (queue_is_mq(q))
@@ -141,15 +221,53 @@ void blk_freeze_queue_start(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_freeze_queue_start);
 
+/*
+ * 一个hang的例子:
+ * # cat /proc/100/stack
+ * [<0>] blk_mq_freeze_queue_wait+0xda/0x1e0
+ * [<0>] nvme_update_disk_info+0xb8/0xb40
+ * [<0>] __nvme_revalidate_disk+0x305/0x750
+ * [<0>] nvme_revalidate_disk+0x1f2/0x3c0
+ * [<0>] revalidate_disk+0x64/0x130
+ * [<0>] nvme_validate_ns+0xdd/0x1c20
+ * [<0>] nvme_scan_work+0x434/0x553
+ * [<0>] process_one_work+0x89c/0x1530
+ * [<0>] worker_thread+0x87/0x1010
+ * [<0>] kthread+0x2ea/0x3a0
+ * [<0>] ret_from_fork+0x35/0x40
+ *
+ * called by:
+ *   - block/blk-mq.c|186| <<blk_freeze_queue>> blk_mq_freeze_queue_wait(q);
+ *   - drivers/nvme/host/core.c|4920| <<nvme_wait_freeze>> blk_mq_freeze_queue_wait(ns->queue);
+ *   - drivers/nvme/host/multipath.c|32| <<nvme_mpath_wait_freeze>> blk_mq_freeze_queue_wait(h->disk->queue);
+ */
 void blk_mq_freeze_queue_wait(struct request_queue *q)
 {
+	/*
+	 * 在以下使用mq_freeze_wq:
+	 *   - block/blk-core.c|310| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|344| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|535| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-core.c|569| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|655| <<blk_alloc_queue_node>> init_waitqueue_head(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|174| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|185| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|225| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
 }
 EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4991| <<nvme_wait_freeze_timeout>> timeout = blk_mq_freeze_queue_wait_timeout(ns->queue, timeout);
+ */
 int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
 				     unsigned long timeout)
 {
+	/*
+	 * put的一个例子就是blk_queue_exit()
+	 */
 	return wait_event_timeout(q->mq_freeze_wq,
 					percpu_ref_is_zero(&q->q_usage_counter),
 					timeout);
@@ -160,6 +278,11 @@ EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait_timeout);
  * Guarantee no request is in use, so we can change any data structure of
  * the queue afterward.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|428| <<blk_cleanup_queue>> blk_freeze_queue(q);
+ *   - block/blk-mq.c|285| <<blk_mq_freeze_queue>> blk_freeze_queue(q);
+ */
 void blk_freeze_queue(struct request_queue *q)
 {
 	/*
@@ -173,6 +296,28 @@ void blk_freeze_queue(struct request_queue *q)
 	blk_mq_freeze_queue_wait(q);
 }
 
+/*
+ * 部分在以下调用blk_mq_freeze_queue():
+ *   - block/blk-cgroup.c|1269| <<blkcg_activate_policy>> blk_mq_freeze_queue(q);
+ *   - block/blk-cgroup.c|1366| <<blkcg_deactivate_policy>> blk_mq_freeze_queue(q);
+ *   - block/blk-iolatency.c|850| <<iolatency_set_limit>> blk_mq_freeze_queue(blkg->q);
+ *   - block/blk-mq.c|3168| <<blk_mq_update_tag_set_depth>> blk_mq_freeze_queue(q);
+ *   - block/blk-mq.c|3875| <<blk_mq_update_nr_requests>> blk_mq_freeze_queue(q);
+ *   - block/blk-mq.c|4002| <<__blk_mq_update_nr_hw_queues>> blk_mq_freeze_queue(q);
+ *   - block/blk-sysfs.c|496| <<queue_wb_lat_store>> blk_mq_freeze_queue(q);
+ *   - block/blk-zoned.c|629| <<blk_revalidate_disk_zones>> blk_mq_freeze_queue(q);
+ *   - block/elevator.c|688| <<elevator_init_mq>> blk_mq_freeze_queue(q);
+ *   - block/elevator.c|716| <<elevator_switch>> blk_mq_freeze_queue(q);
+ *   - drivers/block/loop.c|223| <<__loop_update_dio>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|763| <<loop_change_fd>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1155| <<__loop_clr_fd>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1321| <<loop_set_status>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1598| <<loop_set_block_size>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1900| <<lo_release>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/nvme/host/core.c|2603| <<nvme_update_disk_info>> blk_mq_freeze_queue(disk->queue);
+ *   - drivers/scsi/scsi_lib.c|2562| <<scsi_device_quiesce>> blk_mq_freeze_queue(q);
+ *   - drivers/scsi/sd.c|3490| <<scsi_disk_release>> blk_mq_freeze_queue(q);
+ */
 void blk_mq_freeze_queue(struct request_queue *q)
 {
 	/*
@@ -183,9 +328,42 @@ void blk_mq_freeze_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_freeze_queue);
 
+/*
+ * 在以下部分调用blk_mq_unfreeze_queue():
+ *   - block/blk-cgroup.c|1327| <<blkcg_activate_policy>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-cgroup.c|1384| <<blkcg_deactivate_policy>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-iolatency.c|859| <<iolatency_set_limit>> blk_mq_unfreeze_queue(blkg->q);
+ *   - block/blk-mq.c|3170| <<blk_mq_update_tag_set_depth>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-mq.c|3903| <<blk_mq_update_nr_requests>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-mq.c|4048| <<__blk_mq_update_nr_hw_queues>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-pm.c|90| <<blk_pre_runtime_suspend>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-sysfs.c|502| <<queue_wb_lat_store>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-zoned.c|640| <<blk_revalidate_disk_zones>> blk_mq_unfreeze_queue(q);
+ *   - block/elevator.c|694| <<elevator_init_mq>> blk_mq_unfreeze_queue(q);
+ *   - block/elevator.c|722| <<elevator_switch>> blk_mq_unfreeze_queue(q);
+ *   - drivers/block/loop.c|232| <<__loop_update_dio>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|770| <<loop_change_fd>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1190| <<__loop_clr_fd>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1391| <<loop_set_status>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1615| <<loop_set_block_size>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1901| <<lo_release>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/nvme/host/core.c|2655| <<nvme_update_disk_info>> blk_mq_unfreeze_queue(disk->queue);
+ *   - drivers/nvme/host/core.c|5913| <<nvme_unfreeze>> blk_mq_unfreeze_queue(ns->queue);
+ *   - drivers/nvme/host/multipath.c|44| <<nvme_mpath_unfreeze>> blk_mq_unfreeze_queue(h->disk->queue);
+ *   - drivers/scsi/scsi_lib.c|2570| <<scsi_device_quiesce>> blk_mq_unfreeze_queue(q);
+ *   - drivers/scsi/sd.c|3491| <<scsi_disk_release>> blk_mq_unfreeze_queue(q);
+ */
 void blk_mq_unfreeze_queue(struct request_queue *q)
 {
 	mutex_lock(&q->mq_freeze_lock);
+	/*
+	 * 在以下使用mq_freeze_depth:
+	 *   - block/blk-core.c|536| <<blk_queue_enter>> (!q->mq_freeze_depth &&
+	 *   - block/blk-mq.c|155| <<blk_freeze_queue_start>> if (++q->mq_freeze_depth == 1) {
+	 *   - block/blk-mq.c|221| <<blk_mq_unfreeze_queue>> q->mq_freeze_depth--;
+	 *   - block/blk-mq.c|222| <<blk_mq_unfreeze_queue>> WARN_ON_ONCE(q->mq_freeze_depth < 0);
+	 *   - block/blk-mq.c|223| <<blk_mq_unfreeze_queue>> if (!q->mq_freeze_depth) {
+	 */
 	q->mq_freeze_depth--;
 	WARN_ON_ONCE(q->mq_freeze_depth < 0);
 	if (!q->mq_freeze_depth) {
@@ -202,6 +380,12 @@ EXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);
  */
 void blk_mq_quiesce_queue_nowait(struct request_queue *q)
 {
+	/*
+	 * 在以下使用QUEUE_FLAG_QUIESCED:
+	 *   - block/blk-mq.c|218| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+	 *   - block/blk-mq.c|259| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+	 *   - include/linux/blkdev.h|771| <<blk_queue_quiesced>> #define blk_queue_quiesced(q) test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
+	 */
 	blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
 }
 EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue_nowait);
@@ -215,14 +399,43 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue_nowait);
  * sure no dispatch can happen until the queue is unquiesced via
  * blk_mq_unquiesce_queue().
  */
+/*
+ * 在preempt下的spinlock, 或者非preempt下的不调用都会hang在rcu
+ * [<0>] __wait_rcu_gp
+ * [<0>] synchronize_rcu
+ * [<0>] blk_mq_quiesce_queue
+ * [<0>] nvme_stop_queues
+ * [<0>] nvme_dev_disable
+ * [<0>] nvme_reset_work
+ * [<0>] process_one_work
+ * [<0>] worker_thread
+ * [<0>] kthread
+ * [<0>] ret_from_fork
+ */
 void blk_mq_quiesce_queue(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
 	unsigned int i;
 	bool rcu = false;
 
+	/*
+	 * 在以下调用blk_queue_quiesced()检查request_queue是否quiesced:
+	 *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+	 *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+	 *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+	 *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+	 */
 	blk_mq_quiesce_queue_nowait(q);
 
+	/*
+	 * 在以下调用hctx_lock():
+	 *   - block/blk-mq.c|1772| <<__blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+	 *   - block/blk-mq.c|1907| <<blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+	 *   - block/blk-mq.c|2308| <<blk_mq_try_issue_directly>> hctx_lock(hctx, &srcu_idx);
+	 *   - block/blk-mq.c|2331| <<blk_mq_request_issue_directly>> hctx_lock(hctx, &srcu_idx);
+	 *
+	 * 下面的sync rcu的一个例子是blk_mq_run_hw_queue()的hctx_lock(hctx, &srcu_idx);
+	 */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (hctx->flags & BLK_MQ_F_BLOCKING)
 			synchronize_srcu(hctx->srcu);
@@ -243,6 +456,18 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue);
  */
 void blk_mq_unquiesce_queue(struct request_queue *q)
 {
+	/*
+	 * 在以下调用blk_queue_quiesced()检查request_queue是否quiesced:
+	 *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+	 *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+	 *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+	 *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+	 *
+	 * 在以下使用QUEUE_FLAG_QUIESCED:
+	 *   - block/blk-mq.c|218| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+	 *   - block/blk-mq.c|259| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+	 *   - include/linux/blkdev.h|771| <<blk_queue_quiesced>> #define blk_queue_quiesced(q) test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
+	 */
 	blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
 
 	/* dispatch requests which are inserted during quiescing */
@@ -269,6 +494,10 @@ static inline bool blk_mq_need_time_stamp(struct request *rq)
 	return (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|463| <<blk_mq_get_request>> rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags, alloc_time_ns);
+ */
 static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		unsigned int tag, unsigned int op, u64 alloc_time_ns)
 {
@@ -280,6 +509,11 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		rq->tag = -1;
 		rq->internal_tag = tag;
 	} else {
+		/*
+		 * struct blk_mq_alloc_data:
+		 *   -> struct blk_mq_ctx *ctx;
+		 *   -> struct blk_mq_hw_ctx *hctx;
+		 */
 		if (data->hctx->flags & BLK_MQ_F_TAG_SHARED) {
 			rq_flags = RQF_MQ_INFLIGHT;
 			atomic_inc(&data->hctx->nr_active);
@@ -331,6 +565,12 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|439| <<blk_mq_alloc_request>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|488| <<blk_mq_alloc_request_hctx>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|2289| <<blk_mq_make_request>> rq = blk_mq_get_request(q, bio, &data);
+ */
 static struct request *blk_mq_get_request(struct request_queue *q,
 					  struct bio *bio,
 					  struct blk_mq_alloc_data *data)
@@ -341,6 +581,14 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	bool clear_ctx_on_error = false;
 	u64 alloc_time_ns = 0;
 
+	/*
+	 * blk_queue_enter_live():
+	 *   -> percpu_ref_get(&q->q_usage_counter);
+	 * Given that running in generic_make_request() context
+	 * guarantees that a live reference against q_usage_counter has
+	 * been established, further references under that same context
+	 * need not check that the queue has been frozen (marked dead).
+	 */
 	blk_queue_enter_live(q);
 
 	/* alloc_time includes depth and tag waits */
@@ -397,6 +645,16 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|600| <<blk_get_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/block/mtip32xx/mtip32xx.c|985| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+ *   - drivers/block/sx8.c|511| <<carm_array_info>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/block/sx8.c|564| <<carm_send_special>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/ide/ide-atapi.c|202| <<ide_prep_sense>> sense_rq = blk_mq_alloc_request(drive->queue, REQ_OP_DRV_IN,
+ *   - drivers/nvme/host/core.c|489| <<nvme_alloc_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/scsi/fnic/fnic_scsi.c|2300| <<fnic_scsi_host_start_tag>> dummy = blk_mq_alloc_request(q, REQ_OP_WRITE, BLK_MQ_REQ_NOWAIT);
+ */
 struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 		blk_mq_req_flags_t flags)
 {
@@ -467,6 +725,11 @@ struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|570| <<blk_mq_free_request>> __blk_mq_free_request(rq);
+ *   - block/blk-mq.c|1152| <<blk_mq_check_expired>> __blk_mq_free_request(rq);
+ */
 static void __blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -484,6 +747,26 @@ static void __blk_mq_free_request(struct request *rq)
 	blk_queue_exit(q);
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|2236| <<bfq_bio_merge>> blk_mq_free_request(free);
+ *   - block/blk-core.c|677| <<blk_put_request>> blk_mq_free_request(req);
+ *   - block/blk-mq.c|595| <<__blk_mq_end_request>> blk_mq_free_request(rq);
+ *   - block/mq-deadline.c|475| <<dd_bio_merge>> blk_mq_free_request(free);
+ *   - drivers/block/mtip32xx/mtip32xx.c|1002| <<mtip_exec_internal_command>> blk_mq_free_request(rq);
+ *   - drivers/block/mtip32xx/mtip32xx.c|1048| <<mtip_exec_internal_command>> blk_mq_free_request(rq);
+ *   - drivers/ide/ide-atapi.c|218| <<ide_prep_sense>> blk_mq_free_request(sense_rq);
+ *   - drivers/ide/ide-cd.c|272| <<ide_cd_free_sense>> blk_mq_free_request(drive->sense_rq);
+ *   - drivers/ide/ide-probe.c|984| <<drive_release_dev>> blk_mq_free_request(drive->sense_rq);
+ *   - drivers/nvme/host/core.c|1103| <<__nvme_submit_sync_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|1233| <<nvme_submit_user_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|1243| <<nvme_keep_alive_end_io>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/lightnvm.c|644| <<nvme_nvm_end_io>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/lightnvm.c|848| <<nvme_nvm_submit_user_cmd>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/pci.c|1843| <<abort_endio>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/pci.c|3445| <<nvme_del_queue_end>> blk_mq_free_request(req);
+ *   - drivers/scsi/fnic/fnic_scsi.c|2319| <<fnic_scsi_host_end_tag>> blk_mq_free_request(dummy);
+ */
 void blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -501,6 +784,17 @@ void blk_mq_free_request(struct request *rq)
 	}
 
 	ctx->rq_completed[rq_is_sync(rq)]++;
+	/*
+	 * 修改nr_active的地方:
+	 *   - block/blk-mq.c|298| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|1072| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|518| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.h|207| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|2348| <<blk_mq_alloc_hctx>> atomic_set(&hctx->nr_active, 0);
+	 * 读取nr_active的地方:
+	 *   - block/blk-mq-tag.c|87| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 *   - block/blk-mq-tag.c|116| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 */
 	if (rq->rq_flags & RQF_MQ_INFLIGHT)
 		atomic_dec(&hctx->nr_active);
 
@@ -509,12 +803,26 @@ void blk_mq_free_request(struct request *rq)
 
 	rq_qos_done(q, rq);
 
+	/*
+	 * 使用MQ_RQ_IDLE的地方:
+	 *   - block/blk-mq-debugfs.c|316| <<global>> [MQ_RQ_IDLE] = "idle",
+	 *   - block/blk-mq.c|559| <<blk_mq_free_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|788| <<blk_mq_start_request>> WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|825| <<__blk_mq_requeue_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|2493| <<blk_mq_init_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - include/linux/blk-mq.h|639| <<blk_mq_request_started>> return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
+	 */
 	WRITE_ONCE(rq->state, MQ_RQ_IDLE);
 	if (refcount_dec_and_test(&rq->ref))
 		__blk_mq_free_request(rq);
 }
 EXPORT_SYMBOL_GPL(blk_mq_free_request);
 
+/*
+ * 部分在以下调用__blk_mq_end_request():
+ *   - block/blk-mq.c|627| <<blk_mq_end_request>> __blk_mq_end_request(rq, error);
+ *   - drivers/scsi/scsi_lib.c|610| <<scsi_end_request>> __blk_mq_end_request(req, error);
+ */
 inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	u64 now = 0;
@@ -541,6 +849,29 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(__blk_mq_end_request);
 
+/*
+ * 部分调用blk_mq_end_request()的例子:
+ *   - block/blk-flush.c|421| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+ *   - block/blk-flush.c|774| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+ *   - block/blk-mq.c|1366| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+ *   - block/blk-mq.c|1979| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+ *   - block/blk-mq.c|2015| <<blk_mq_try_issue_list_directly>> blk_mq_end_request(rq, ret);
+ *   - block/bsg-lib.c|158| <<bsg_teardown_job>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/block/loop.c|487| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/nbd.c|340| <<nbd_complete_rq>> blk_mq_end_request(req, cmd->status);
+ *   - drivers/block/null_blk_main.c|677| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+ *   - drivers/block/virtio_blk.c|226| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+ *   - drivers/block/xen-blkfront.c|918| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+ *   - drivers/block/xen-blkfront.c|2113| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+ *   - drivers/ide/ide-cd.c|765| <<cdrom_newpc_intr>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/ide/ide-pm.c|50| <<ide_pm_execute_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/ide/ide-pm.c|220| <<ide_complete_pm_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/md/dm-rq.c|174| <<dm_end_request>> blk_mq_end_request(rq, error);
+ *   - drivers/md/dm-rq.c|271| <<dm_softirq_done>> blk_mq_end_request(rq, tio->error);
+ *   - drivers/nvme/host/core.c|307| <<nvme_complete_rq>> blk_mq_end_request(req, status);
+ *   - drivers/nvme/host/multipath.c|76| <<nvme_failover_req>> blk_mq_end_request(req, 0);
+ *   - drivers/scsi/scsi_transport_fc.c|3581| <<fc_bsg_job_timeout>> blk_mq_end_request(req, BLK_STS_IOERR);
+ */
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
@@ -557,6 +888,10 @@ static void __blk_mq_complete_request_remote(void *data)
 	q->mq_ops->complete(rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|652| <<blk_mq_complete_request>> __blk_mq_complete_request(rq);
+ */
 static void __blk_mq_complete_request(struct request *rq)
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
@@ -564,6 +899,27 @@ static void __blk_mq_complete_request(struct request *rq)
 	bool shared = false;
 	int cpu;
 
+	/*
+	 * 使用MQ_RQ_IDLE的地方:
+	 *   - block/blk-mq-debugfs.c|316| <<global>> [MQ_RQ_IDLE] = "idle",
+	 *   - block/blk-mq.c|559| <<blk_mq_free_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|788| <<blk_mq_start_request>> WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|825| <<__blk_mq_requeue_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|2493| <<blk_mq_init_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - include/linux/blk-mq.h|639| <<blk_mq_request_started>> return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
+	 *
+	 * 使用MQ_RQ_IN_FLIGHT的地方:
+	 *   - block/blk-mq-debugfs.c|317| <<global>> [MQ_RQ_IN_FLIGHT] = "in_flight",
+	 *   - block/blk-mq.c|791| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *   - block/blk-mq.c|1017| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|1059| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 *
+	 * 使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+	 *   - block/blk-mq.c|580| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|3474| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 *   - include/linux/blk-mq.h|481| <<blk_mq_request_completed>> return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
+	 */
 	WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
 	/*
 	 * Most of single queue controllers, there is only one irq vector
@@ -599,11 +955,21 @@ static void __blk_mq_complete_request(struct request *rq)
 		rq->csd.flags = 0;
 		smp_call_function_single_async(ctx->cpu, &rq->csd);
 	} else {
+		/*
+		 * 比如nvme_loop_complete_rq()
+		 */
 		q->mq_ops->complete(rq);
 	}
 	put_cpu();
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1774| <<__blk_mq_run_hw_queue>> hctx_unlock(hctx, srcu_idx);
+ *   - block/blk-mq.c|1910| <<blk_mq_run_hw_queue>> hctx_unlock(hctx, srcu_idx);
+ *   - block/blk-mq.c|2316| <<blk_mq_try_issue_directly>> hctx_unlock(hctx, srcu_idx);
+ *   - block/blk-mq.c|2333| <<blk_mq_request_issue_directly>> hctx_unlock(hctx, srcu_idx);
+ */
 static void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)
 	__releases(hctx->srcu)
 {
@@ -613,6 +979,13 @@ static void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)
 		srcu_read_unlock(hctx->srcu, srcu_idx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1772| <<__blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+ *   - block/blk-mq.c|1907| <<blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+ *   - block/blk-mq.c|2308| <<blk_mq_try_issue_directly>> hctx_lock(hctx, &srcu_idx);
+ *   - block/blk-mq.c|2331| <<blk_mq_request_issue_directly>> hctx_lock(hctx, &srcu_idx);
+ */
 static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
 	__acquires(hctx->srcu)
 {
@@ -632,6 +1005,25 @@ static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
  *	Ends all I/O on a request. It does not handle partial completions.
  *	The actual completion happens out-of-order, through a IPI handler.
  **/
+/*
+ * 部分调用的例子:
+ *   - block/bsg-lib.c|186| <<bsg_job_done>> blk_mq_complete_request(blk_mq_rq_from_pdu(job));
+ *   - drivers/block/loop.c|499| <<lo_rw_aio_do_completion>> blk_mq_complete_request(rq);
+ *   - drivers/block/loop.c|1957| <<loop_handle_cmd>> blk_mq_complete_request(rq);
+ *   - drivers/block/nbd.c|452| <<nbd_xmit_timeout>> blk_mq_complete_request(req);
+ *   - drivers/block/nbd.c|788| <<recv_work>> blk_mq_complete_request(blk_mq_rq_from_pdu(cmd));
+ *   - drivers/block/nbd.c|804| <<nbd_clear_req>> blk_mq_complete_request(req);
+ *   - drivers/block/null_blk_main.c|1242| <<nullb_complete_cmd>> blk_mq_complete_request(cmd->rq);
+ *   - drivers/block/null_blk_main.c|1369| <<null_timeout_rq>> blk_mq_complete_request(rq);
+ *   - drivers/block/virtio_blk.c|244| <<virtblk_done>> blk_mq_complete_request(req);
+ *   - drivers/block/xen-blkfront.c|1648| <<blkif_interrupt>> blk_mq_complete_request(req);
+ *   - drivers/md/dm-rq.c|291| <<dm_complete_request>> blk_mq_complete_request(rq);
+ *   - drivers/nvme/host/core.c|321| <<nvme_cancel_request>> blk_mq_complete_request(req);
+ *   - drivers/nvme/host/nvme.h|460| <<nvme_end_request>> blk_mq_complete_request(req);
+ *   - drivers/s390/block/dasd.c|2786| <<__dasd_cleanup_cqr>> blk_mq_complete_request(req);
+ *   - drivers/s390/block/scm_blk.c|259| <<scm_request_finish>> blk_mq_complete_request(scmrq->request[i]);
+ *   - drivers/scsi/scsi_lib.c|1618| <<scsi_mq_done>> if (unlikely(!blk_mq_complete_request(cmd->request)))
+ */
 bool blk_mq_complete_request(struct request *rq)
 {
 	if (unlikely(blk_should_fake_timeout(rq->q)))
@@ -641,13 +1033,51 @@ bool blk_mq_complete_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_complete_request);
 
+/*
+ * 部分调用blk_mq_start_request()的例子:
+ *   - block/bsg-lib.c|272| <<bsg_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/block/loop.c|1985| <<loop_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/block/nbd.c|882| <<nbd_handle_cmd>> blk_mq_start_request(req);
+ *   - drivers/block/nbd.c|891| <<nbd_handle_cmd>> blk_mq_start_request(req);
+ *   - drivers/block/nbd.c|915| <<nbd_handle_cmd>> blk_mq_start_request(req);
+ *   - drivers/block/nbd.c|926| <<nbd_handle_cmd>> blk_mq_start_request(req);
+ *   - drivers/block/null_blk_main.c|2286| <<null_queue_rq>> blk_mq_start_request(bd->rq);
+ *   - drivers/block/virtio_blk.c|319| <<virtio_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/block/xen-blkfront.c|891| <<blkif_queue_rq>> blk_mq_start_request(qd->rq);
+ *   - drivers/ide/ide-io.c|578| <<ide_queue_rq>> blk_mq_start_request(bd->rq);
+ *   - drivers/md/dm-rq.c|450| <<dm_start_request>> blk_mq_start_request(orig);
+ *   - drivers/nvme/host/fabrics.c|556| <<nvmf_fail_nonready_command>> blk_mq_start_request(rq);
+ *   - drivers/nvme/host/fc.c|2283| <<nvme_fc_start_fcp_op>> blk_mq_start_request(op->rq);
+ *   - drivers/nvme/host/pci.c|1050| <<nvme_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/nvme/host/rdma.c|1763| <<nvme_rdma_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/nvme/host/tcp.c|2165| <<nvme_tcp_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/nvme/target/loop.c|148| <<nvme_loop_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1601| <<scsi_mq_prep_fn>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1677| <<scsi_queue_rq>> blk_mq_start_request(req);
+ */
 void blk_mq_start_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
 
 	trace_block_rq_issue(q, rq);
 
+	/*
+	 * 在以下使用QUEUE_FLAG_STATS:
+	 *   - block/blk-mq.c|650| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+	 *   - block/blk-stat.c|152| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|162| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|188| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 */
 	if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+		/*
+		 * 在以下使用io_start_time_ns:
+		 *   - block/bfq-iosched.c|5912| <<bfq_finish_requeue_request>> rq->io_start_time_ns,
+		 *   - block/blk-mq.c|671| <<blk_mq_start_request>> rq->io_start_time_ns = ktime_get_ns();
+		 *   - block/blk-mq.c|327| <<blk_mq_rq_ctx_init>> rq->io_start_time_ns = 0;
+		 *   - block/blk-stat.c|59| <<blk_stat_add>> value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
+		 *   - block/blk-wbt.c|619| <<wbt_issue>> rwb->sync_issue = rq->io_start_time_ns;
+		 *   - block/kyber-iosched.c|651| <<kyber_completed_request>> now - rq->io_start_time_ns);
+		 */
 		rq->io_start_time_ns = ktime_get_ns();
 		rq->stats_sectors = blk_rq_sectors(rq);
 		rq->rq_flags |= RQF_STATS;
@@ -657,6 +1087,13 @@ void blk_mq_start_request(struct request *rq)
 	WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
 
 	blk_add_timer(rq);
+	/*
+	 * 使用MQ_RQ_IN_FLIGHT的地方:
+	 *   - block/blk-mq-debugfs.c|317| <<global>> [MQ_RQ_IN_FLIGHT] = "in_flight",
+	 *   - block/blk-mq.c|791| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *   - block/blk-mq.c|1017| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|1059| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 */
 	WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
 
 	if (q->dma_drain_size && blk_rq_bytes(rq)) {
@@ -675,6 +1112,12 @@ void blk_mq_start_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_start_request);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|810| <<blk_mq_requeue_request>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|1483| <<blk_mq_dispatch_rq_list>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|2087| <<__blk_mq_issue_directly>> __blk_mq_requeue_request(rq);
+ */
 static void __blk_mq_requeue_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -692,6 +1135,29 @@ static void __blk_mq_requeue_request(struct request *rq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|484| <<lo_complete_rq>> blk_mq_requeue_request(rq, true);
+ *   - drivers/block/nbd.c|170| <<nbd_requeue_cmd>> blk_mq_requeue_request(req, true);
+ *   - drivers/block/null_blk_main.c|2300| <<null_queue_rq>> blk_mq_requeue_request(bd->rq, true);
+ *   - drivers/block/skd_main.c|1425| <<skd_resolve_req_exception>> blk_mq_requeue_request(req, true);
+ *   - drivers/block/skd_main.c|1435| <<skd_resolve_req_exception>> blk_mq_requeue_request(req, true);
+ *   - drivers/block/sunvdc.c|1120| <<vdc_requeue_inflight>> blk_mq_requeue_request(req, false);
+ *   - drivers/block/xen-blkfront.c|2053| <<blkif_recover>> blk_mq_requeue_request(req, false);
+ *   - drivers/ide/ide-cd.c|261| <<ide_cd_breathe>> blk_mq_requeue_request(rq, false);
+ *   - drivers/ide/ide-io.c|450| <<ide_requeue_and_plug>> blk_mq_requeue_request(rq, false);
+ *   - drivers/md/dm-rq.c|191| <<dm_mq_delay_requeue_request>> blk_mq_requeue_request(rq, false);
+ *   - drivers/memstick/core/ms_block.c|2056| <<msb_stop>> blk_mq_requeue_request(msb->req, false);
+ *   - drivers/mmc/core/block.c|1435| <<mmc_blk_cqe_complete_rq>> blk_mq_requeue_request(req, true);
+ *   - drivers/mmc/core/block.c|1440| <<mmc_blk_cqe_complete_rq>> blk_mq_requeue_request(req, true);
+ *   - drivers/mmc/core/block.c|1894| <<mmc_blk_mq_complete_rq>> blk_mq_requeue_request(req, true);
+ *   - drivers/mmc/core/block.c|1900| <<mmc_blk_mq_complete_rq>> blk_mq_requeue_request(req, true);
+ *   - drivers/nvme/host/core.c|338| <<nvme_retry_req>> blk_mq_requeue_request(req, false);
+ *   - drivers/s390/block/dasd.c|2964| <<_dasd_requeue_request>> blk_mq_requeue_request(req, false);
+ *   - drivers/s390/block/scm_blk.c|243| <<scm_request_requeue>> blk_mq_requeue_request(scmrq->request[i], false);
+ *   - drivers/scsi/scsi_lib.c|163| <<scsi_mq_requeue_cmd>> blk_mq_requeue_request(cmd->request, true);
+ *   - drivers/scsi/scsi_lib.c|202| <<__scsi_queue_insert>> blk_mq_requeue_request(cmd->request, true);
+ */
 void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list)
 {
 	__blk_mq_requeue_request(rq);
@@ -704,6 +1170,17 @@ void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list)
 }
 EXPORT_SYMBOL(blk_mq_requeue_request);
 
+/*
+ * 在以下使用q->requeue_work:
+ *   - block/blk-mq.c|800| <<blk_mq_requeue_work>> container_of(work, struct request_queue, requeue_work.work);
+ *   - block/blk-mq.c|861| <<blk_mq_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
+ *   - block/blk-mq.c|868| <<blk_mq_delay_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work,
+ *   - block/blk-mq.c|3231| <<blk_mq_init_allocated_queue>> INIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work);
+ *   - block/blk-sysfs.c|906| <<__blk_release_queue>> cancel_delayed_work_sync(&q->requeue_work);
+ *
+ * 在以下使用blk_mq_requeue_work():
+ *   - block/blk-mq.c|3231| <<blk_mq_init_allocated_queue>> INIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work);
+ */
 static void blk_mq_requeue_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -720,6 +1197,7 @@ static void blk_mq_requeue_work(struct work_struct *work)
 			continue;
 
 		rq->rq_flags &= ~RQF_SOFTBARRIER;
+		/* 从rq_list取下来 */
 		list_del_init(&rq->queuelist);
 		/*
 		 * If RQF_DONTPREP, rq has contained some driver specific
@@ -741,6 +1219,20 @@ static void blk_mq_requeue_work(struct work_struct *work)
 	blk_mq_run_hw_queues(q, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|262| <<blk_flush_queue_rq>> blk_mq_add_to_requeue_list(rq, add_front, true);
+ *   - block/blk-mq.c|793| <<blk_mq_requeue_request>> blk_mq_add_to_requeue_list(rq, true, kick_requeue_list);
+ *
+ * 两个例子:
+ * blk_kick_flush()
+ *  -> blk_flush_queue_rq()
+ *      -> blk_mq_add_to_requeue_list()
+ *
+ * blk_flush_complete_seq()
+ *  -> blk_flush_queue_rq()
+ *      -> blk_mq_add_to_requeue_list()
+ */
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 				bool kick_requeue_list)
 {
@@ -766,8 +1258,19 @@ void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 		blk_mq_kick_requeue_list(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|168| <<queue_state_write>> blk_mq_kick_requeue_list(q);
+ *   - block/blk-mq.c|895| <<blk_mq_add_to_requeue_list>> blk_mq_kick_requeue_list(q);
+ *   - drivers/block/xen-blkfront.c|2056| <<blkif_recover>> blk_mq_kick_requeue_list(info->rq);
+ *   - drivers/md/dm-rq.c|68| <<dm_start_queue>> blk_mq_kick_requeue_list(q);
+ *   - drivers/s390/block/scm_blk.c|247| <<scm_request_requeue>> blk_mq_kick_requeue_list(bdev->rq);
+ */
 void blk_mq_kick_requeue_list(struct request_queue *q)
 {
+	/*
+	 * blk_mq_requeue_work()
+	 */
 	kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
 }
 EXPORT_SYMBOL(blk_mq_kick_requeue_list);
@@ -780,6 +1283,27 @@ void blk_mq_delay_kick_requeue_list(struct request_queue *q,
 }
 EXPORT_SYMBOL(blk_mq_delay_kick_requeue_list);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3917| <<blk_mq_poll_hybrid>> rq = blk_mq_tag_to_rq(hctx->tags, blk_qc_t_to_tag(cookie));
+ *   - block/blk-mq.c|3919| <<blk_mq_poll_hybrid>> rq = blk_mq_tag_to_rq(hctx->sched_tags, blk_qc_t_to_tag(cookie));
+ *   - drivers/block/mtip32xx/mtip32xx.c|167| <<mtip_cmd_from_tag>> return blk_mq_rq_to_pdu(blk_mq_tag_to_rq(hctx->tags, tag));
+ *   - drivers/block/nbd.c|696| <<nbd_read_stat>> req = blk_mq_tag_to_rq(nbd->tag_set.tags[hwq],
+ *   - drivers/block/skd_main.c|1520| <<skd_isr_completion_posted>> WARN_ON_ONCE(blk_mq_tag_to_rq(skdev->tag_set.tags[hwq],
+ *   - drivers/block/skd_main.c|1526| <<skd_isr_completion_posted>> rq = blk_mq_tag_to_rq(skdev->tag_set.tags[hwq], tag);
+ *   - drivers/block/sx8.c|926| <<carm_handle_resp>> rq = blk_mq_tag_to_rq(host->tag_set.tags[0], msg_idx);
+ *   - drivers/nvme/host/pci.c|975| <<nvme_handle_cqe>> req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
+ *   - drivers/nvme/host/rdma.c|1443| <<nvme_rdma_process_nvme_rsp>> rq = blk_mq_tag_to_rq(nvme_rdma_tagset(queue), cqe->command_id);
+ *   - drivers/nvme/host/tcp.c|433| <<nvme_tcp_process_nvme_cqe>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), cqe->command_id);
+ *   - drivers/nvme/host/tcp.c|453| <<nvme_tcp_handle_c2h_data>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|557| <<nvme_tcp_handle_r2t>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|642| <<nvme_tcp_recv_data>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|741| <<nvme_tcp_recv_ddgst>> struct request *rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue),
+ *   - drivers/nvme/target/loop.c|111| <<nvme_loop_queue_response>> rq = blk_mq_tag_to_rq(nvme_loop_tagset(queue), cqe->command_id);
+ *   - include/scsi/scsi_tcq.h|33| <<scsi_host_find_tag>> req = blk_mq_tag_to_rq(shost->tag_set.tags[hwq],
+ *
+ * 核心思想就是返回tags->rqs[tag] (struct request类型)
+ */
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)
 {
 	if (tag < tags->nr_tags) {
@@ -791,6 +1315,10 @@ struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)
 }
 EXPORT_SYMBOL(blk_mq_tag_to_rq);
 
+/*
+ * 在以下使用blk_mq_rq_inflight():
+ *   - block/blk-mq.c|1338| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ */
 static bool blk_mq_rq_inflight(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			       void *priv, bool reserved)
 {
@@ -798,6 +1326,13 @@ static bool blk_mq_rq_inflight(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	 * If we find a request that is inflight and the queue matches,
 	 * we know the queue is busy. Return false to stop the iteration.
 	 */
+	/*
+	 * 使用MQ_RQ_IN_FLIGHT的地方:
+	 *   - block/blk-mq-debugfs.c|317| <<global>> [MQ_RQ_IN_FLIGHT] = "in_flight",
+	 *   - block/blk-mq.c|791| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *   - block/blk-mq.c|1017| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|1059| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 */
 	if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
 		bool *busy = priv;
 
@@ -808,6 +1343,10 @@ static bool blk_mq_rq_inflight(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/md/dm.c|672| <<md_in_flight>> return blk_mq_queue_inflight(md->queue);
+ */
 bool blk_mq_queue_inflight(struct request_queue *q)
 {
 	bool busy = false;
@@ -817,6 +1356,10 @@ bool blk_mq_queue_inflight(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_queue_inflight);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1095| <<blk_mq_check_expired>> blk_mq_rq_timed_out(rq, reserved);
+ */
 static void blk_mq_rq_timed_out(struct request *req, bool reserved)
 {
 	req->rq_flags |= RQF_TIMED_OUT;
@@ -832,10 +1375,22 @@ static void blk_mq_rq_timed_out(struct request *req, bool reserved)
 	blk_add_timer(req);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1363| <<blk_mq_check_expired>> if (!blk_mq_req_expired(rq, next))
+ *   - block/blk-mq.c|1384| <<blk_mq_check_expired>> if (blk_mq_req_expired(rq, next))
+ */
 static bool blk_mq_req_expired(struct request *rq, unsigned long *next)
 {
 	unsigned long deadline;
 
+	/*
+	 * 使用MQ_RQ_IN_FLIGHT的地方:
+	 *   - block/blk-mq-debugfs.c|317| <<global>> [MQ_RQ_IN_FLIGHT] = "in_flight",
+	 *   - block/blk-mq.c|791| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *   - block/blk-mq.c|1017| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|1059| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 */
 	if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
 		return false;
 	if (rq->rq_flags & RQF_TIMED_OUT)
@@ -852,6 +1407,10 @@ static bool blk_mq_req_expired(struct request *rq, unsigned long *next)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1129| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, void *priv, bool reserved)
 {
@@ -893,6 +1452,10 @@ static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * 在以下使用blk_mq_timeout_work():
+ *   - block/blk-mq.c|3339| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+ */
 static void blk_mq_timeout_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -1019,6 +1582,14 @@ static inline unsigned int queued_to_index(unsigned int queued)
 	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1619| <<blk_mq_mark_tag_wait>> return blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1645| <<blk_mq_mark_tag_wait>> ret = blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1788| <<blk_mq_dispatch_rq_list>> if (!blk_mq_get_driver_tag(rq)) {
+ *   - block/blk-mq.c|1821| <<blk_mq_dispatch_rq_list>> bd.last = !blk_mq_get_driver_tag(nxt);
+ *   - block/blk-mq.c|2494| <<__blk_mq_try_issue_directly>> if (!blk_mq_get_driver_tag(rq)) {
+ */
 bool blk_mq_get_driver_tag(struct request *rq)
 {
 	struct blk_mq_alloc_data data = {
@@ -1148,6 +1719,39 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
  * - take 4 as factor for avoiding to get too small(0) result, and this
  *   factor doesn't matter because EWMA decreases exponentially
  */
+/*
+ * commit 6e768717304bdbe8d2897ca8298f6b58863fdc41
+ * Author: Ming Lei <ming.lei@redhat.com>
+ * Date:   Tue Jul 3 09:03:16 2018 -0600
+ *
+ * blk-mq: dequeue request one by one from sw queue if hctx is busy
+ *
+ * It won't be efficient to dequeue request one by one from sw queue,
+ * but we have to do that when queue is busy for better merge performance.
+ *
+ * This patch takes the Exponential Weighted Moving Average(EWMA) to figure
+ * out if queue is busy, then only dequeue request one by one from sw queue
+ * when queue is busy.
+ *
+ * Fixes: b347689ffbca ("blk-mq-sched: improve dispatching from sw queue")
+ * Cc: Kashyap Desai <kashyap.desai@broadcom.com>
+ * Cc: Laurence Oberman <loberman@redhat.com>
+ * Cc: Omar Sandoval <osandov@fb.com>
+ * Cc: Christoph Hellwig <hch@lst.de>
+ * Cc: Bart Van Assche <bart.vanassche@wdc.com>
+ * Cc: Hannes Reinecke <hare@suse.de>
+ * Reported-by: Kashyap Desai <kashyap.desai@broadcom.com>
+ * Tested-by: Kashyap Desai <kashyap.desai@broadcom.com>
+ * Signed-off-by: Ming Lei <ming.lei@redhat.com>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ *
+ * called by:
+ *   - block/blk-mq.c|1372| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|1375| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|1838| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|1843| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|1847| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ */
 static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 {
 	unsigned int ewma;
@@ -1155,14 +1759,34 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 	if (hctx->queue->elevator)
 		return;
 
+	/*
+	 * 在以下使用dispatch_busy:
+	 *   - block/blk-mq-debugfs.c|621| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+	 *   - block/blk-mq-sched.c|217| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+	 *   - block/blk-mq-sched.c|436| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+	 *   - block/blk-mq.c|1215| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+	 *   - block/blk-mq.c|1225| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 *   - block/blk-mq.c|2071| <<blk_mq_make_request>> !data.hctx->dispatch_busy) {
+	 */
 	ewma = hctx->dispatch_busy;
 
 	if (!ewma && !busy)
 		return;
 
+	/*
+	 * BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT - 1 = 8 - 1 = 7
+	 * 这里相当于ewma = hctx->dispatch_busy * 7
+	 */
 	ewma *= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT - 1;
+	/*
+	 * 当busy的时候ewma = ewma + 1 << 4
+	 * 相当于ewma加上16
+	 */
 	if (busy)
 		ewma += 1 << BLK_MQ_DISPATCH_BUSY_EWMA_FACTOR;
+	/*
+	 * 相当于ewma = ewma / 8
+	 */
 	ewma /= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT;
 
 	hctx->dispatch_busy = ewma;
@@ -1173,6 +1797,16 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 /*
  * Returns true if we did some work AND can potentially do more.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|120| <<blk_mq_do_dispatch_sched>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|170| <<blk_mq_do_dispatch_ctx>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|218| <<blk_mq_sched_dispatch_requests>> if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+ *   - block/blk-mq-sched.c|231| <<blk_mq_sched_dispatch_requests>> blk_mq_dispatch_rq_list(q, &rq_list, false);
+ *
+ * 核心思想是处理list上每一个request,处理不了就放入hctx->dispatch
+ * 然后根据情况决定是否run
+ */
 bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			     bool got_budget)
 {
@@ -1220,6 +1854,7 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			}
 		}
 
+		/* 到了此时才在list上删除 */
 		list_del_init(&rq->queuelist);
 
 		bd.rq = rq;
@@ -1278,6 +1913,21 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			q->mq_ops->commit_rqs(hctx);
 
 		spin_lock(&hctx->lock);
+		/*
+		 * 在以下添加rq到hctx->dispatch:
+		 *   - block/blk-mq-sched.c|376| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1414| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1794| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2373| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 * 在以下使用hctx->dispatch:
+		 *   - block/blk-mq.c|2491| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+		 *   - block/blk-mq-sched.c|199| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *   - block/blk-mq-sched.c|196| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+		 *   - block/blk-mq-sched.c|198| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+		 *   - block/blk-mq.c|82| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+		 *   - block/blk-mq-debugfs.c|363| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+		 *   - block/blk-mq-debugfs.c|370| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+		 */
 		list_splice_init(list, &hctx->dispatch);
 		spin_unlock(&hctx->lock);
 
@@ -1327,6 +1977,11 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	return (queued + errors) != 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1493| <<__blk_mq_delay_run_hw_queue>> __blk_mq_run_hw_queue(hctx);
+ *   - block/blk-mq.c|1654| <<blk_mq_run_work_fn>> __blk_mq_run_hw_queue(hctx);
+ */
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	int srcu_idx;
@@ -1364,6 +2019,13 @@ static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 
 	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 
+	/*
+	 * called by:
+	 *   - block/blk-mq.c|1772| <<__blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+	 *   - block/blk-mq.c|1907| <<blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+	 *   - block/blk-mq.c|2308| <<blk_mq_try_issue_directly>> hctx_lock(hctx, &srcu_idx);
+	 *   - block/blk-mq.c|2331| <<blk_mq_request_issue_directly>> hctx_lock(hctx, &srcu_idx);
+	 */
 	hctx_lock(hctx, &srcu_idx);
 	blk_mq_sched_dispatch_requests(hctx);
 	hctx_unlock(hctx, srcu_idx);
@@ -1424,6 +2086,11 @@ static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 	return next_cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1507| <<blk_mq_delay_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, true, msecs);
+ *   - block/blk-mq.c|1530| <<blk_mq_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, async, 0);
+ */
 static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 					unsigned long msecs)
 {
@@ -1441,16 +2108,46 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 		put_cpu();
 	}
 
+	/*
+	 * 在以下使用run_work:
+	 *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 */
 	kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
 				    msecs_to_jiffies(msecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1370| <<blk_mq_dispatch_rq_list>> blk_mq_delay_run_hw_queue(hctx, BLK_MQ_RESOURCE_DELAY);
+ *   - drivers/ide/ide-io.c|453| <<ide_requeue_and_plug>> blk_mq_delay_run_hw_queue(q->queue_hw_ctx[0], 3);
+ *   - drivers/scsi/scsi_lib.c|1639| <<scsi_mq_get_budget>> blk_mq_delay_run_hw_queue(hctx, SCSI_QUEUE_DELAY);
+ */
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 {
 	__blk_mq_delay_run_hw_queue(hctx, true, msecs);
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|80| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|419| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|453| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|168| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1148| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1451| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1650| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1715| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1735| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1821| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2135| <<blk_mq_make_request>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2399| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|698| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ */
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1582,6 +2279,14 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 }
 EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
 
+/*
+ * 在以下使用run_work:
+ *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+ *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+ *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+ *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+ *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+ */
 static void blk_mq_run_work_fn(struct work_struct *work)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1629,6 +2334,14 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|518| <<blk_insert_flush>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|787| <<blk_mq_requeue_work>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|1884| <<__blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, run_queue);
+ *   - block/blk-mq.c|1900| <<blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, true);
+ *   - block/blk-mq.c|1934| <<blk_mq_try_issue_list_directly>> blk_mq_request_bypass_insert(rq,
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
@@ -1680,6 +2393,10 @@ static int plug_rq_cmp(void *priv, struct list_head *a, struct list_head *b)
 	return blk_rq_pos(rqa) > blk_rq_pos(rqb);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1788| <<blk_flush_plug_list>> blk_mq_flush_plug_list(plug, from_schedule);
+ */
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 {
 	struct blk_mq_hw_ctx *this_hctx;
@@ -1748,6 +2465,10 @@ static void blk_mq_bio_to_request(struct request *rq, struct bio *bio,
 	blk_account_io_start(rq, true);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2135| <<__blk_mq_try_issue_directly>> return __blk_mq_issue_directly(hctx, rq, cookie, last);
+ */
 static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    blk_qc_t *cookie, bool last)
@@ -1787,6 +2508,11 @@ static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2037| <<blk_mq_try_issue_directly>> ret = __blk_mq_try_issue_directly(hctx, rq, cookie, false, true);
+ *   - block/blk-mq.c|2054| <<blk_mq_request_issue_directly>> ret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true, last);
+ */
 static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 						struct request *rq,
 						blk_qc_t *cookie,
@@ -1828,6 +2554,11 @@ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2505| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, same_queue_rq,
+ *   - block/blk-mq.c|2510| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, rq, &cookie);
+ */
 static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, blk_qc_t *cookie)
 {
@@ -1847,6 +2578,11 @@ static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	hctx_unlock(hctx, srcu_idx);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1281| <<blk_insert_cloned_request>> return blk_mq_request_issue_directly(rq, true);
+ *   - block/blk-mq.c|2161| <<blk_mq_try_issue_list_directly>> ret = blk_mq_request_issue_directly(rq, list_empty(list));
+ */
 blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 {
 	blk_status_t ret;
@@ -1861,6 +2597,10 @@ blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|466| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -2191,6 +2931,27 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
  * software queue to the hw queue dispatch list, and ensure that it
  * gets run.
  */
+/*
+ * [0] blk_mq_hctx_notify_dead
+ * [0] cpuhp_invoke_callback
+ * [0] _cpu_down
+ * [0] do_cpu_down
+ * [0] device_offline
+ * [0] online_store
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * blk_mq_hctx_notify_dead()对每一个block都会调用一次
+ * 测试的时候调用了好多次, 比如每个nbd都会调用一次
+ *
+ * 执行这里的时候, cpu是要被offline的cpu id
+ * smp_processor_id()是当前执行这个函数的cpu id
+ * 这两个不是一个id
+ * hotplug的时候是在一个cpu为另外一个cpu执行blk_mq_hctx_notify_dead()
+ */
 static int blk_mq_hctx_notify_dead(unsigned int cpu, struct hlist_node *node)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2198,6 +2959,11 @@ static int blk_mq_hctx_notify_dead(unsigned int cpu, struct hlist_node *node)
 	LIST_HEAD(tmp);
 	enum hctx_type type;
 
+	/*
+	 * struct blk_mq_hw_ctx:
+	 *	// @cpuhp_dead: List to store request if some CPU die.
+	 *	struct hlist_node       cpuhp_dead;
+	 */
 	hctx = hlist_entry_safe(node, struct blk_mq_hw_ctx, cpuhp_dead);
 	ctx = __blk_mq_get_ctx(hctx->queue, cpu);
 	type = hctx->type;
@@ -2275,6 +3041,10 @@ static int blk_mq_hw_ctx_size(struct blk_mq_tag_set *tag_set)
 	return hw_ctx_size;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2765| <<blk_mq_alloc_and_init_hctx>> if (blk_mq_init_hctx(q, set, hctx, hctx_idx))
+ */
 static int blk_mq_init_hctx(struct request_queue *q,
 		struct blk_mq_tag_set *set,
 		struct blk_mq_hw_ctx *hctx, unsigned hctx_idx)
@@ -2302,6 +3072,10 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	return -1;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2761| <<blk_mq_alloc_and_init_hctx>> hctx = blk_mq_alloc_hctx(q, set, node);
+ */
 static struct blk_mq_hw_ctx *
 blk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,
 		int node)
@@ -2321,6 +3095,14 @@ blk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,
 		node = set->numa_node;
 	hctx->numa_node = node;
 
+	/*
+	 * 在以下使用run_work:
+	 *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 */
 	INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
 	spin_lock_init(&hctx->lock);
 	INIT_LIST_HEAD(&hctx->dispatch);
@@ -2429,6 +3211,35 @@ static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ *
+ * called by:
+ *   - block/blk-mq.c|2910| <<blk_mq_init_allocated_queue>> blk_mq_map_swqueue(q);
+ *   - block/blk-mq.c|3312| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_swqueue(q);
+ */
 static void blk_mq_map_swqueue(struct request_queue *q)
 {
 	unsigned int i, j, hctx_idx;
@@ -2436,6 +3247,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	struct blk_mq_ctx *ctx;
 	struct blk_mq_tag_set *set = q->tag_set;
 
+	/*
+	 * 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i]
+	 */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		cpumask_clear(hctx->cpumask);
 		hctx->nr_ctx = 0;
@@ -2448,6 +3262,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 * If the cpu isn't present, the cpu is mapped to first hctx.
 	 */
 	for_each_possible_cpu(i) {
+		/*
+		 * hctx_idx用来索引set->tags[hctx_idx]
+		 */
 		hctx_idx = set->map[HCTX_TYPE_DEFAULT].mq_map[i];
 		/* unmapped hw queue can be remapped after CPU topo changed */
 		if (!set->tags[hctx_idx] &&
@@ -2470,6 +3287,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			}
 
 			hctx = blk_mq_map_queue_type(q, j, i);
+			/*
+			 *  struct blk_mq_hw_ctx *hctxs[HCTX_MAX_TYPES];
+			 */
 			ctx->hctxs[j] = hctx;
 			/*
 			 * If the CPU is already set in the mask, then we've
@@ -2535,6 +3355,11 @@ static void blk_mq_map_swqueue(struct request_queue *q)
  * Caller needs to ensure that we're either frozen/quiesced, or that
  * the queue isn't live yet.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2589| <<blk_mq_update_tag_set_depth>> queue_set_hctx_shared(q, shared);
+ *   - block/blk-mq.c|2625| <<blk_mq_add_queue_tag_set>> queue_set_hctx_shared(q, true);
+ */
 static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2548,6 +3373,11 @@ static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2604| <<blk_mq_del_queue_tag_set>> blk_mq_update_tag_set_depth(set, false);
+ *   - block/blk-mq.c|2622| <<blk_mq_add_queue_tag_set>> blk_mq_update_tag_set_depth(set, true);
+ */
 static void blk_mq_update_tag_set_depth(struct blk_mq_tag_set *set,
 					bool shared)
 {
@@ -2600,8 +3430,16 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 }
 
 /* All allocations will be freed in release handler of q->mq_kobj */
+/*
+ * called by:
+ *   - block/blk-mq.c|2867| <<blk_mq_init_allocated_queue>> if (blk_mq_alloc_ctxs(q))
+ */
 static int blk_mq_alloc_ctxs(struct request_queue *q)
 {
+	/*
+	 * struct request_queue:
+	 *  -> struct blk_mq_ctx __percpu *queue_ctx;
+	 */
 	struct blk_mq_ctxs *ctxs;
 	int cpu;
 
@@ -2656,6 +3494,36 @@ void blk_mq_release(struct request_queue *q)
 	blk_mq_sysfs_deinit(q);
 }
 
+/*
+ * 部分调用blk_mq_init_queue()的例子:
+ *   - block/blk-mq.c|2732| <<blk_mq_init_sq_queue>> q = blk_mq_init_queue(set);
+ *   - block/bsg-lib.c|387| <<bsg_setup_queue>> q = blk_mq_init_queue(set);
+ *   - drivers/block/loop.c|2022| <<loop_add>> lo->lo_queue = blk_mq_init_queue(&lo->tag_set);
+ *   - drivers/block/nbd.c|1692| <<nbd_dev_add>> q = blk_mq_init_queue(&nbd->tag_set);
+ *   - drivers/block/null_blk_main.c|1717| <<null_add_dev>> nullb->q = blk_mq_init_queue(nullb->tag_set);
+ *   - drivers/block/rbd.c|5042| <<rbd_init_disk>> q = blk_mq_init_queue(&rbd_dev->tag_set);
+ *   - drivers/block/virtio_blk.c|802| <<virtblk_probe>> q = blk_mq_init_queue(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|986| <<xlvbd_init_blk_queue>> rq = blk_mq_init_queue(&info->tag_set);
+ *   - drivers/ide/ide-probe.c|790| <<ide_init_queue>> q = blk_mq_init_queue(set);
+ *   - drivers/nvme/host/core.c|3495| <<nvme_alloc_ns>> ns->queue = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/fc.c|2481| <<nvme_fc_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3148| <<nvme_fc_init_ctrl>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/fc.c|3154| <<nvme_fc_init_ctrl>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|1628| <<nvme_alloc_admin_tags>> dev->ctrl.admin_q = blk_mq_init_queue(&dev->admin_tagset);
+ *   - drivers/nvme/host/rdma.c|809| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/rdma.c|815| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/rdma.c|886| <<nvme_rdma_configure_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/tcp.c|1676| <<nvme_tcp_configure_io_queues>> ctrl->connect_q = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/tcp.c|1729| <<nvme_tcp_configure_admin_queue>> ctrl->fabrics_q = blk_mq_init_queue(ctrl->admin_tagset);
+ *   - drivers/nvme/host/tcp.c|1735| <<nvme_tcp_configure_admin_queue>> ctrl->admin_q = blk_mq_init_queue(ctrl->admin_tagset);
+ *   - drivers/nvme/target/loop.c|362| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|368| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|529| <<nvme_loop_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1871| <<scsi_mq_alloc_queue>> sdev->request_queue = blk_mq_init_queue(&sdev->host->tag_set);
+ *
+ * blk_mq_init_queue()更加像是从tagset中新分配一个request_queue
+ * 一个tagset可以有多个request_queue
+ */
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)
 {
 	struct request_queue *uninit_q, *q;
@@ -2710,6 +3578,10 @@ struct request_queue *blk_mq_init_sq_queue(struct blk_mq_tag_set *set,
 }
 EXPORT_SYMBOL(blk_mq_init_sq_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2814| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+ */
 static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 		struct blk_mq_tag_set *set, struct request_queue *q,
 		int hctx_idx, int node)
@@ -2744,6 +3616,11 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2876| <<blk_mq_init_allocated_queue>> blk_mq_realloc_hw_ctxs(set, q);
+ *   - block/blk-mq.c|3304| <<__blk_mq_update_nr_hw_queues>> blk_mq_realloc_hw_ctxs(set, q);
+ */
 static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 						struct request_queue *q)
 {
@@ -2762,6 +3639,12 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 			memcpy(new_hctxs, hctxs, q->nr_hw_queues *
 			       sizeof(*hctxs));
 		q->queue_hw_ctx = new_hctxs;
+		/*
+		 * 设置nr_hw_queues的地方:
+		 *   - block/blk-mq.c|3103| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+		 *   - block/blk-mq.c|3147| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+		 *   - block/blk-mq.c|3233| <<blk_mq_init_allocated_queue>> q->nr_hw_queues = 0;
+		 */
 		q->nr_hw_queues = set->nr_hw_queues;
 		kfree(hctxs);
 		hctxs = new_hctxs;
@@ -2822,6 +3705,11 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2700| <<blk_mq_init_queue>> q = blk_mq_init_allocated_queue(set, uninit_q, false);
+ *   - drivers/md/dm-rq.c|566| <<dm_mq_init_request_queue>> q = blk_mq_init_allocated_queue(md->tag_set, md->queue, true);
+ */
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q,
 						  bool elevator_init)
@@ -2959,6 +3847,11 @@ static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3109| <<blk_mq_alloc_tag_set>> ret = blk_mq_update_queue_map(set);
+ *   - block/blk-mq.c|3301| <<__blk_mq_update_nr_hw_queues>> blk_mq_update_queue_map(set);
+ */
 static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 {
 	if (set->ops->map_queues && !is_kdump_kernel()) {
@@ -2978,6 +3871,10 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 		 * killing stale mapping since one CPU may not be mapped
 		 * to any hw queue.
 		 */
+		/*
+		 * 对于每一种类型的map,
+		 * 清空所有cpu的blk_mq_queue_map->mq_map[cpu] = 0
+		 */
 		for (i = 0; i < set->nr_maps; i++)
 			blk_mq_clear_mq_map(&set->map[i]);
 
@@ -3017,6 +3914,27 @@ static int blk_mq_realloc_tag_set_tags(struct blk_mq_tag_set *set,
  * requested depth down, if it's too large. In that case, the set
  * value will be stored in set->queue_depth.
  */
+/*
+ * 调用blk_mq_alloc_tag_set()的例子:
+ *   - block/blk-mq.c|2947| <<blk_mq_init_sq_queue>> ret = blk_mq_alloc_tag_set(set);
+ *   - block/bsg-lib.c|384| <<bsg_setup_queue>> if (blk_mq_alloc_tag_set(set))
+ *   - drivers/block/loop.c|2033| <<loop_add>> err = blk_mq_alloc_tag_set(&lo->tag_set);
+ *   - drivers/block/nbd.c|1688| <<nbd_dev_add>> err = blk_mq_alloc_tag_set(&nbd->tag_set);
+ *   - drivers/block/null_blk_main.c|1716| <<null_init_tag_set>> return blk_mq_alloc_tag_set(set);
+ *   - drivers/block/virtio_blk.c|819| <<virtblk_probe>> err = blk_mq_alloc_tag_set(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|984| <<xlvbd_init_blk_queue>> if (blk_mq_alloc_tag_set(&info->tag_set))
+ *   - drivers/ide/ide-probe.c|787| <<ide_init_queue>> if (blk_mq_alloc_tag_set(set))
+ *   - drivers/md/dm-rq.c|562| <<dm_mq_init_request_queue>> err = blk_mq_alloc_tag_set(md->tag_set);
+ *   - drivers/nvme/host/fc.c|2475| <<nvme_fc_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3143| <<nvme_fc_init_ctrl>> ret = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|1645| <<nvme_alloc_admin_tags>> if (blk_mq_alloc_tag_set(&dev->admin_tagset))
+ *   - drivers/nvme/host/pci.c|2328| <<nvme_dev_add>> ret = blk_mq_alloc_tag_set(&dev->tagset);
+ *   - drivers/nvme/host/rdma.c|755| <<nvme_rdma_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+ *   - drivers/nvme/host/tcp.c|1493| <<nvme_tcp_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+ *   - drivers/nvme/target/loop.c|357| <<nvme_loop_configure_admin_queue>> error = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|525| <<nvme_loop_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1906| <<scsi_mq_setup_tags>> return blk_mq_alloc_tag_set(&shost->tag_set);
+ */
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
 	int i, ret;
@@ -3069,6 +3987,13 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 
 	ret = -ENOMEM;
 	for (i = 0; i < set->nr_maps; i++) {
+		/*
+		 * struct blk_mq_queue_map {
+		 *      unsigned int *mq_map;
+		 *      unsigned int nr_queues;
+		 *      unsigned int queue_offset;
+		 * };
+		 */
 		set->map[i].mq_map = kcalloc_node(nr_cpu_ids,
 						  sizeof(set->map[i].mq_map[0]),
 						  GFP_KERNEL, set->numa_node);
@@ -3101,6 +4026,44 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_alloc_tag_set);
 
+/*
+ * 部分调用blk_mq_free_tag_set()的例子:
+ *   - block/blk-mq.c|3321| <<blk_mq_init_sq_queue>> blk_mq_free_tag_set(set)
+ *   - drivers/block/loop.c|2177| <<loop_add>> blk_mq_free_tag_set(&lo->tag_set);
+ *   - drivers/block/loop.c|2190| <<loop_remove>> blk_mq_free_tag_set(&lo->tag_set);
+ *   - drivers/block/nbd.c|229| <<nbd_dev_remove>> blk_mq_free_tag_set(&nbd->tag_set);
+ *   - drivers/block/nbd.c|1726| <<nbd_dev_add>> blk_mq_free_tag_set(&nbd->tag_set);
+ *   - drivers/block/null_blk_main.c|2375| <<null_del_dev>> blk_mq_free_tag_set(nullb->tag_set);
+ *   - drivers/block/null_blk_main.c|2829| <<null_add_dev>> blk_mq_free_tag_set(nullb->tag_set);
+ *   - drivers/block/null_blk_main.c|2952| <<null_init>> blk_mq_free_tag_set(&tag_set);
+ *   - drivers/block/null_blk_main.c|2976| <<null_exit>> blk_mq_free_tag_set(&tag_set);
+ *   - drivers/block/virtio_blk.c|942| <<virtblk_probe>> blk_mq_free_tag_set(&vblk->tag_set);
+ *   - drivers/block/virtio_blk.c|967| <<virtblk_remove>> blk_mq_free_tag_set(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|988| <<xlvbd_init_blk_queue>> blk_mq_free_tag_set(&info->tag_set);
+ *   - drivers/block/xen-blkfront.c|1211| <<xlvbd_release_gendisk>> blk_mq_free_tag_set(&info->tag_set);
+ *   - drivers/md/dm-rq.c|575| <<dm_mq_init_request_queue>> blk_mq_free_tag_set(md->tag_set);
+ *   - drivers/md/dm-rq.c|585| <<dm_mq_cleanup_mapped_device>> blk_mq_free_tag_set(md->tag_set);
+ *   - drivers/nvme/host/fc.c|2024| <<nvme_fc_ctrl_free>> blk_mq_free_tag_set(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|2035| <<nvme_fc_ctrl_free>> blk_mq_free_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/fc.c|2504| <<nvme_fc_create_io_queues>> blk_mq_free_tag_set(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3232| <<nvme_fc_init_ctrl>> blk_mq_free_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|2545| <<nvme_dev_remove_admin>> blk_mq_free_tag_set(&dev->admin_tagset);
+ *   - drivers/nvme/host/pci.c|2585| <<nvme_alloc_admin_tags>> blk_mq_free_tag_set(&dev->admin_tagset);
+ *   - drivers/nvme/host/pci.c|3948| <<nvme_free_tagset>> blk_mq_free_tag_set(&dev->tagset);
+ *   - drivers/nvme/host/rdma.c|768| <<nvme_rdma_destroy_admin_queue>> blk_mq_free_tag_set(ctrl->ctrl.admin_tagset);
+ *   - drivers/nvme/host/rdma.c|851| <<nvme_rdma_configure_admin_queue>> blk_mq_free_tag_set(ctrl->ctrl.admin_tagset);
+ *   - drivers/nvme/host/rdma.c|866| <<nvme_rdma_destroy_io_queues>> blk_mq_free_tag_set(ctrl->ctrl.tagset);
+ *   - drivers/nvme/host/rdma.c|907| <<nvme_rdma_configure_io_queues>> blk_mq_free_tag_set(ctrl->ctrl.tagset);
+ *   - drivers/nvme/host/tcp.c|1656| <<nvme_tcp_destroy_io_queues>> blk_mq_free_tag_set(ctrl->tagset);
+ *   - drivers/nvme/host/tcp.c|1697| <<nvme_tcp_configure_io_queues>> blk_mq_free_tag_set(ctrl->tagset);
+ *   - drivers/nvme/host/tcp.c|1709| <<nvme_tcp_destroy_admin_queue>> blk_mq_free_tag_set(ctrl->admin_tagset);
+ *   - drivers/nvme/host/tcp.c|1768| <<nvme_tcp_configure_admin_queue>> blk_mq_free_tag_set(ctrl->admin_tagset);
+ *   - drivers/nvme/target/loop.c|258| <<nvme_loop_destroy_admin_queue>> blk_mq_free_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|274| <<nvme_loop_free_ctrl>> blk_mq_free_tag_set(&ctrl->tag_set);
+ *   - drivers/nvme/target/loop.c|400| <<nvme_loop_configure_admin_queue>> blk_mq_free_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|544| <<nvme_loop_create_io_queues>> blk_mq_free_tag_set(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1911| <<scsi_mq_destroy_tags>> blk_mq_free_tag_set(&shost->tag_set);
+ */
 void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 {
 	int i, j;
@@ -3233,6 +4196,15 @@ static void blk_mq_elv_switch_back(struct list_head *head,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * 有三处queue数量的地方:
+ *   - blk_mq_tag_set->nr_hw_queues
+ *   - blk_mq_queue_map->nr_queues
+ *   - request_queue->nr_hw_queues
+ *
+ * called by:
+ *   - block/blk-mq.c|3673| <<blk_mq_update_nr_hw_queues>> __blk_mq_update_nr_hw_queues(set, nr_hw_queues);
+ */
 static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 							int nr_hw_queues)
 {
@@ -3297,6 +4269,22 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1249| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/null_blk_main.c|323| <<nullb_apply_submit_queues>> blk_mq_update_nr_hw_queues(set, submit_queues);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2554| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2336| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|892| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1682| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|471| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *
+ * 有三处queue数量的地方:
+ *   - blk_mq_tag_set->nr_hw_queues
+ *   - blk_mq_queue_map->nr_queues
+ *   - request_queue->nr_hw_queues
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
@@ -3306,8 +4294,19 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
 
 /* Enable polling stats and return whether they were already enabled. */
+/*
+ * called by:
+ *   - block/blk-mq.c|3373| <<blk_mq_poll_nsecs>> if (!blk_poll_stats_enable(q))
+ */
 static bool blk_poll_stats_enable(struct request_queue *q)
 {
+	/*
+	 * 在以下使用QUEUE_FLAG_POLL_STATS:
+	 *   - block/blk-mq.c|3331| <<blk_poll_stats_enable>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+	 *   - block/blk-mq.c|3332| <<blk_poll_stats_enable>> blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
+	 *   - block/blk-mq.c|3344| <<blk_mq_poll_stats_start>> if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+	 *   - block/blk-sysfs.c|880| <<__blk_release_queue>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
+	 */
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
 	    blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
 		return true;
@@ -3315,6 +4314,10 @@ static bool blk_poll_stats_enable(struct request_queue *q)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|539| <<__blk_mq_end_request>> blk_mq_poll_stats_start(rq->q);
+ */
 static void blk_mq_poll_stats_start(struct request_queue *q)
 {
 	/*
@@ -3328,6 +4331,12 @@ static void blk_mq_poll_stats_start(struct request_queue *q)
 	blk_stat_activate_msecs(q->poll_cb, 100);
 }
 
+/*
+ * 在以下使用blk_mq_poll_stats_fn():
+ *   - block/blk-mq.c|2852| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *
+ * 核心思想是把BLK_MQ_POLL_STATS_BKTS个bucket设置q->poll_stat[bucket] = cb->stat[bucket]
+ */
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 {
 	struct request_queue *q = cb->data;
@@ -3339,6 +4348,12 @@ static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3416| <<blk_mq_poll_hybrid_sleep>> nsecs = blk_mq_poll_nsecs(q, hctx, rq);
+ *
+ * 这个函数应该就是用来估算一个要sleep的时间
+ */
 static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 				       struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
@@ -3372,6 +4387,10 @@ static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3471| <<blk_mq_poll_hybrid>> return blk_mq_poll_hybrid_sleep(q, hctx, rq);
+ */
 static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 				     struct blk_mq_hw_ctx *hctx,
 				     struct request *rq)
@@ -3381,6 +4400,13 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	unsigned int nsecs;
 	ktime_t kt;
 
+	/*
+	 * 在以下使用RQF_MQ_POLL_SLEPT:
+	 *   - block/blk-mq.c|3444| <<blk_mq_poll_hybrid_sleep>> if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
+	 *   - block/blk-mq.c|3461| <<blk_mq_poll_hybrid_sleep>> rq->rq_flags |= RQF_MQ_POLL_SLEPT;
+	 *
+	 * already slept for hybrid poll
+	 */
 	if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
 		return false;
 
@@ -3404,6 +4430,9 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	 * This will be replaced with the stats tracking code, using
 	 * 'avg_completion_time / 2' as the pre-sleep target.
 	 */
+	/*
+	 * kt是非常核心的时间!!!
+	 */
 	kt = nsecs;
 
 	mode = HRTIMER_MODE_REL;
@@ -3426,6 +4455,33 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	return true;
 }
 
+/*
+ * 在以下patch加入的函数
+ * commit 06426adf072bca62ac31ea396ff2159a34f276c2
+ * Author: Jens Axboe <axboe@fb.com>
+ * Date:   Mon Nov 14 13:01:59 2016 -0700
+ *
+ * blk-mq: implement hybrid poll mode for sync O_DIRECT
+ *
+ * This patch enables a hybrid polling mode. Instead of polling after IO
+ * submission, we can induce an artificial delay, and then poll after that.
+ * For example, if the IO is presumed to complete in 8 usecs from now, we
+ * can sleep for 4 usecs, wake up, and then do our polling. This still puts
+ * a sleep/wakeup cycle in the IO path, but instead of the wakeup happening
+ * after the IO has completed, it'll happen before. With this hybrid
+ * scheme, we can achieve big latency reductions while still using the same
+ * (or less) amount of CPU.
+ *
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ * Tested-By: Stephen Bates <sbates@raithlin.com>
+ * Reviewed-By: Stephen Bates <sbates@raithlin.com>
+ *
+ * called by:
+ *   - block/blk-mq.c|3547| <<blk_poll>> if (blk_mq_poll_hybrid(q, hctx, cookie))
+ *
+ * If the device access time exceeds the IRQ model overhead,
+ * sleeping before the I/O completion will not hurt latency
+ */
 static bool blk_mq_poll_hybrid(struct request_queue *q,
 			       struct blk_mq_hw_ctx *hctx, blk_qc_t cookie)
 {
@@ -3463,6 +4519,87 @@ static bool blk_mq_poll_hybrid(struct request_queue *q,
  *    looping until at least one completion is found, unless the task is
  *    otherwise marked running (or we need to reschedule).
  */
+/*
+ * poll是可以有专门的hctx的. 用cookie来标记用的哪个hctx甚至哪个tag.
+ *
+ * blk_qc_t_to_queue_num()把cookie转换成q->queue_hw_ctx[]的index.
+ *
+ * blk_qc_t_to_tag()把cookie转换成tag.
+ *
+ * ext4_direct_IO_write()
+ *  -> __blockdev_direct_IO()
+ *      -> do_blockdev_direct_IO
+ *          -> dio_bio_submit
+ *              -> submit_bio()
+ *          -> dio_await_completion()
+ *              -> dio_await_one()
+ *                  -> blk_poll()
+ *
+ * cookie一直通过submit_bio()返回, 然后通过blk_poll()去poll()查看IO是否完成.
+ *
+ * slides: I/O Latency Optimization with Polling
+ *
+ *
+ * 根据一位网友的测试:
+ * 从上述测试结果来看,IO-Polling对于sync模式的direct-io的延迟有较好的提升,
+ * sync模式下,无论4K随机读或者随机写IO压力下,延迟平均大约减少5μs,而这5μs
+ * 几乎就是中断模式下,处理中断时,上下文切换的时间差.
+ * 相比随机读,对随机写的延迟降低约20%,这对延迟敏感的IO请求来说是极大的性能提升.
+ *
+ *
+ * 根据blk_mq_map_queue(), 只有REQ_HIPRI的才会被map到poll的queue.
+ *
+ * 102 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
+ * 103                                                      unsigned int flags,
+ * 104                                                      struct blk_mq_ctx *ctx)
+ * 105 {
+ * 106         enum hctx_type type = HCTX_TYPE_DEFAULT;
+ * 107
+ * 108         //
+ * 109         // The caller ensure that if REQ_HIPRI, poll must be enabled.
+ * 110         //
+ * 111         if (flags & REQ_HIPRI)
+ * 112                 type = HCTX_TYPE_POLL;
+ * 113         else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
+ * 114                 type = HCTX_TYPE_READ;115
+ * 116         return ctx->hctxs[type];
+ * 117 }
+ *
+ * 激活poll的方法:
+ *
+ * 激活io_poll:
+ * # echo 1 > /sys/block/nvme0n1/queue/io_poll
+ *
+ * 在fio中使用pvsync2加上hipri:
+ * # fio -name iops -rw=read -bs=4k -runtime=60 -iodepth 32 -filename /dev/nvme0n1 -ioengine pvsync2 -direct=1 -hipri=1
+ *
+ * [0] blk_poll
+ * [0] __blkdev_direct_IO_simple
+ * [0] blkdev_direct_IO
+ * [0] generic_file_read_iter
+ * [0] do_iter_readv_writev
+ * [0] do_iter_read
+ * [0] vfs_readv
+ * [0] do_preadv
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|812| <<nvme_execute_rq_polled>> blk_poll(q, request_to_qc_t(rq->mq_hctx, rq), true);
+ *   - fs/block_dev.c|257| <<__blkdev_direct_IO_simple>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/block_dev.c|295| <<blkdev_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), wait);
+ *   - fs/block_dev.c|451| <<__blkdev_direct_IO>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/direct-io.c|502| <<dio_await_one>> !blk_poll(dio->bio_disk->queue, dio->bio_cookie, true))
+ *   - fs/iomap/direct-io.c|57| <<iomap_dio_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), spin);
+ *   - fs/iomap/direct-io.c|562| <<iomap_dio_rw>> !blk_poll(dio->submit.last_queue,
+ *   - mm/page_io.c|424| <<swap_readpage>> if (!blk_poll(disk->queue, qc, true))
+ *
+ * Polling is tried for any block I/O belonging to a high-priority I/O context (IOCB_HIPRI)
+ *
+ * For applications, set only for preadv2/pwritev2 with RWF_HIPRI flag
+ *
+ * Not related to ioprio_set!
+ */
 int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -3475,6 +4612,10 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 	if (current->plug)
 		blk_flush_plug_list(current->plug, false);
 
+	/*
+	 * blk_qc_t_to_queue_num():
+	 * 把cookie的第31位(最高位)清空,然后向右移动16位,获得queue num
+	 */
 	hctx = q->queue_hw_ctx[blk_qc_t_to_queue_num(cookie)];
 
 	/*
@@ -3495,6 +4636,9 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 
 		hctx->poll_invoked++;
 
+		/*
+		 * nvme pci的例子是nvme_poll()
+		 */
 		ret = q->mq_ops->poll(hctx);
 		if (ret > 0) {
 			hctx->poll_success++;
@@ -3517,6 +4661,11 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 }
 EXPORT_SYMBOL_GPL(blk_poll);
 
+/*
+ * called by:
+ *   - drivers/scsi/bnx2i/bnx2i_hwi.c|1918| <<bnx2i_queue_scsi_cmd_resp>> p = &per_cpu(bnx2i_percpu, blk_mq_rq_cpu(sc->request));
+ *   - drivers/scsi/csiostor/csio_scsi.c|1789| <<csio_queuecommand>> sqset = &hw->sqset[ln->portid][blk_mq_rq_cpu(cmnd->request)];
+ */
 unsigned int blk_mq_rq_cpu(struct request *rq)
 {
 	return rq->mq_ctx->cpu;
diff --git a/block/blk-mq.h b/block/blk-mq.h
index eaaca8fc1c28..917b0aae5766 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -9,19 +9,82 @@ struct blk_mq_tag_set;
 
 struct blk_mq_ctxs {
 	struct kobject kobj;
+	/*
+	 * 主要设置和使用queue_ctx的地方:
+	 *   - block/blk-mq-sysfs.c|22| <<blk_mq_sysfs_release>> free_percpu(ctxs->queue_ctx);
+	 *   - block/blk-mq.c|2493| <<blk_mq_map_swqueue>> ctx = per_cpu_ptr(q->queue_ctx, i);
+	 *   - block/blk-mq.c|2641| <<blk_mq_alloc_ctxs>> ctxs->queue_ctx = alloc_percpu(struct blk_mq_ctx);
+	 *   - block/blk-mq.c|2642| <<blk_mq_alloc_ctxs>> if (!ctxs->queue_ctx)
+	 *   - block/blk-mq.c|2646| <<blk_mq_alloc_ctxs>> struct blk_mq_ctx *ctx = per_cpu_ptr(ctxs->queue_ctx, cpu);
+	 *   - block/blk-mq.c|2651| <<blk_mq_alloc_ctxs>> q->queue_ctx = ctxs->queue_ctx;
+	 */
 	struct blk_mq_ctx __percpu	*queue_ctx;
 };
 
 /**
  * struct blk_mq_ctx - State for a software queue facing the submitting CPUs
  */
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 使用rq_lists[]的地方:
+		 *   - block/blk-mq-debugfs.c|632| <<CTX_RQ_SEQ_OPS>> return seq_list_start(&ctx->rq_lists[type], *pos); \
+		 *   - block/blk-mq-debugfs.c|640| <<CTX_RQ_SEQ_OPS>> return seq_list_next(v, &ctx->rq_lists[type], pos); \
+		 *   - block/blk-mq-sched.c|316| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio, nr_segs)) {
+		 *   - block/blk-mq-sched.c|338| <<__blk_mq_sched_bio_merge>> !list_empty_careful(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|982| <<flush_busy_ctx>> list_splice_tail_init(&ctx->rq_lists[type], flush_data->list);
+		 *   - block/blk-mq.c|1017| <<dispatch_rq_from_ctx>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|1018| <<dispatch_rq_from_ctx>> dispatch_data->rq = list_entry_rq(ctx->rq_lists[type].next);
+		 *   - block/blk-mq.c|1020| <<dispatch_rq_from_ctx>> if (list_empty(&ctx->rq_lists[type]))
+		 *   - block/blk-mq.c|1641| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1643| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1690| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|2235| <<blk_mq_hctx_notify_dead>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|2236| <<blk_mq_hctx_notify_dead>> list_splice_init(&ctx->rq_lists[type], &tmp);
+		 *   - block/blk-mq.c|2416| <<blk_mq_init_cpu_queues>> INIT_LIST_HEAD(&__ctx->rq_lists[k]);
+		 */
 		struct list_head	rq_lists[HCTX_MAX_TYPES];
 	} ____cacheline_aligned_in_smp;
 
 	unsigned int		cpu;
+	/*
+	 * 设置index_hw[]的地方:
+	 *   - block/blk-mq.c|2513| <<blk_mq_map_swqueue>> ctx->index_hw[hctx->type] = hctx->nr_ctx;
+	 * 其余使用index_hw[]的地方:
+	 *   - block/blk-mq-sched.c|121| <<blk_mq_next_ctx>> unsigned short idx = ctx->index_hw[hctx->type];
+	 *   - block/blk-mq.c|93| <<blk_mq_hctx_mark_pending>> const int bit = ctx->index_hw[hctx->type];
+	 *   - block/blk-mq.c|102| <<blk_mq_hctx_clear_pending>> const int bit = ctx->index_hw[hctx->type];
+	 *   - block/blk-mq.c|1031| <<blk_mq_dequeue_from_ctx>> unsigned off = start ? start->index_hw[hctx->type] : 0;
+	 *   - block/kyber-iosched.c|570| <<kyber_bio_merge>> struct kyber_ctx_queue *kcq = &khd->kcqs[ctx->index_hw[hctx->type]];
+	 *   - block/kyber-iosched.c|595| <<kyber_insert_requests>> struct kyber_ctx_queue *kcq = &khd->kcqs[rq->mq_ctx->index_hw[hctx->type]];
+	 *   - block/kyber-iosched.c|604| <<kyber_insert_requests>> rq->mq_ctx->index_hw[hctx->type]);
+	 */
 	unsigned short		index_hw[HCTX_MAX_TYPES];
 	struct blk_mq_hw_ctx 	*hctxs[HCTX_MAX_TYPES];
 
@@ -30,6 +93,12 @@ struct blk_mq_ctx {
 	unsigned long		rq_merged;
 
 	/* incremented at completion time */
+	/*
+	 * 使用rq_completed[2]的地方:
+	 *   - block/blk-mq-debugfs.c|700| <<ctx_completed_show>> seq_printf(m, "%lu %lu\n", ctx->rq_completed[1], ctx->rq_completed[0]);
+	 *   - block/blk-mq-debugfs.c|709| <<ctx_completed_write>> ctx->rq_completed[0] = ctx->rq_completed[1] = 0;
+	 *   - block/blk-mq.c|516| <<blk_mq_free_request>> ctx->rq_completed[rq_is_sync(rq)]++;
+	 */
 	unsigned long		____cacheline_aligned_in_smp rq_completed[2];
 
 	struct request_queue	*queue;
@@ -86,10 +155,21 @@ extern int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int);
  * @type: the hctx type index
  * @cpu: CPU
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2425| <<blk_mq_init_cpu_queues>> hctx = blk_mq_map_queue_type(q, j, i);
+ *   - block/blk-mq.c|2496| <<blk_mq_map_swqueue>> ctx->hctxs[j] = blk_mq_map_queue_type(q,
+ *   - block/blk-mq.c|2501| <<blk_mq_map_swqueue>> hctx = blk_mq_map_queue_type(q, j, i);
+ *   - block/blk-mq.c|2524| <<blk_mq_map_swqueue>> ctx->hctxs[j] = blk_mq_map_queue_type(q,
+ */
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *q,
 							  enum hctx_type type,
 							  unsigned int cpu)
 {
+	/*
+	 * 设置queue_hw_ctx的地方:
+	 *   - block/blk-mq.c|2793| <<blk_mq_realloc_hw_ctxs>> q->queue_hw_ctx = new_hctxs;
+	 */
 	return q->queue_hw_ctx[q->tag_set->map[type].mq_map[cpu]];
 }
 
@@ -216,6 +296,13 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|45| <<blk_mq_pci_map_queues>> blk_mq_clear_mq_map(qmap);
+ *   - block/blk-mq.c|3011| <<blk_mq_update_queue_map>> blk_mq_clear_mq_map(&set->map[i]);
+ *
+ * 清空所有cpu的blk_mq_queue_map->mq_map[cpu] = 0
+ */
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index c8eda2e7b91e..14a8834157b6 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -180,6 +180,18 @@ EXPORT_SYMBOL(blk_queue_bounce_limit);
  *    per-device basis in /sys/block/<device>/queue/max_sectors_kb.
  *    The soft limit can not exceed max_hw_sectors.
  **/
+/*
+ * 部分调用blk_queue_max_hw_sectors()的例子:
+ *   - drivers/block/loop.c|2112| <<loop_add>> blk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);
+ *   - drivers/block/nbd.c|1709| <<nbd_dev_add>> blk_queue_max_hw_sectors(disk->queue, 65536);
+ *   - drivers/block/virtio_blk.c|852| <<virtblk_probe>> blk_queue_max_hw_sectors(q, -1U);
+ *   - drivers/block/xen-blkfront.c|947| <<blkif_set_queue_limits>> blk_queue_max_hw_sectors(rq, (segments * XEN_PAGE_SIZE) / 512);
+ *   - drivers/nvme/host/core.c|2226| <<nvme_set_queue_limits>> blk_queue_max_hw_sectors(q, ctrl->max_hw_sectors);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1942| <<megasas_set_nvme_device_properties>> blk_queue_max_hw_sectors(sdev->request_queue, (max_io_size / 512));
+ *   - drivers/scsi/scsi_lib.c|1803| <<__scsi_init_queue>> blk_queue_max_hw_sectors(q, shost->max_sectors);
+ *   - drivers/scsi/scsi_scan.c|902| <<scsi_add_lun>> blk_queue_max_hw_sectors(sdev->request_queue, 512);
+ *   - drivers/scsi/scsi_scan.c|908| <<scsi_add_lun>> blk_queue_max_hw_sectors(sdev->request_queue, 1024);
+ */
 void blk_queue_max_hw_sectors(struct request_queue *q, unsigned int max_hw_sectors)
 {
 	struct queue_limits *limits = &q->limits;
@@ -250,6 +262,19 @@ EXPORT_SYMBOL(blk_queue_max_write_same_sectors);
  * @q:  the request queue for the device
  * @max_write_zeroes_sectors: maximum number of sectors to write per command
  **/
+/*
+ * 部分调用blk_queue_max_write_zeroes_sectors()的例子:
+ *   - drivers/block/loop.c|903| <<loop_config_discard>> blk_queue_max_write_zeroes_sectors(q, 0);
+ *   - drivers/block/loop.c|912| <<loop_config_discard>> blk_queue_max_write_zeroes_sectors(q, UINT_MAX >> 9);
+ *   - drivers/block/virtio_blk.c|923| <<virtblk_probe>> blk_queue_max_write_zeroes_sectors(q, v ? v : UINT_MAX);
+ *   - drivers/md/raid0.c|402| <<raid0_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, mddev->chunk_sectors);
+ *   - drivers/md/raid1.c|3114| <<raid1_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, 0);
+ *   - drivers/md/raid10.c|3767| <<raid10_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, 0);
+ *   - drivers/md/raid5.c|7454| <<raid5_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, 0);
+ *   - drivers/nvme/host/core.c|1723| <<nvme_config_discard>> blk_queue_max_write_zeroes_sectors(queue, UINT_MAX);
+ *   - drivers/nvme/host/core.c|1748| <<nvme_config_write_zeroes>> blk_queue_max_write_zeroes_sectors(disk->queue,
+ *   - drivers/scsi/sd.c|1002| <<sd_config_write_same>> blk_queue_max_write_zeroes_sectors(q, sdkp->max_ws_blocks *
+ */
 void blk_queue_max_write_zeroes_sectors(struct request_queue *q,
 		unsigned int max_write_zeroes_sectors)
 {
@@ -328,6 +353,38 @@ EXPORT_SYMBOL(blk_queue_max_segment_size);
  *   storage device can address.  The default of 512 covers most
  *   hardware.
  **/
+/*
+ * commit c72758f33784e5e2a1a4bb9421ef3e6de8f9fcf3
+ * Author: Martin K. Petersen <martin.petersen@oracle.com>
+ * Date:   Fri May 22 17:17:53 2009 -0400
+ *
+ * block: Export I/O topology for block devices and partitions
+ *
+ * To support devices with physical block sizes bigger than 512 bytes we
+ * need to ensure proper alignment.  This patch adds support for exposing
+ * I/O topology characteristics as devices are stacked.
+ *
+ *   logical_block_size is the smallest unit the device can address.
+ *
+ *   physical_block_size indicates the smallest I/O the device can write
+ *   without incurring a read-modify-write penalty.
+ *
+ *   The io_min parameter is the smallest preferred I/O size reported by
+ *   the device.  In many cases this is the same as the physical block
+ *   size.  However, the io_min parameter can be scaled up when stacking
+ *   (RAID5 chunk size > physical block size).
+ *
+ *   The io_opt characteristic indicates the optimal I/O size reported by
+ *   the device.  This is usually the stripe width for arrays.
+ *
+ *   The alignment_offset parameter indicates the number of bytes the start
+ *   of the device/partition is offset from the device's natural alignment.
+ *   Partition tools and MD/DM utilities can use this to pad their offsets
+ *   so filesystems start on proper boundaries.
+ *
+ *   Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
+ *   Signed-off-by: Jens Axboe <jens.axboe@oracle.com>
+ */
 void blk_queue_logical_block_size(struct request_queue *q, unsigned int size)
 {
 	q->limits.logical_block_size = size;
@@ -818,6 +875,43 @@ EXPORT_SYMBOL(blk_set_queue_depth);
  *
  * Tell the block layer about the write cache of @q.
  */
+/*
+ * 部分调用blk_queue_write_cache()的例子:
+ *   - drivers/block/loop.c|1007| <<loop_set_fd>> blk_queue_write_cache(lo->lo_queue, true, false);
+ *   - drivers/block/nbd.c|1136| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, true);
+ *   - drivers/block/nbd.c|1138| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, false);
+ *   - drivers/block/nbd.c|1141| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, false, false);
+ *   - drivers/block/null_blk_main.c|1742| <<null_add_dev>> blk_queue_write_cache(nullb->q, true, true);
+ *   - rivers/block/virtio_blk.c|609| <<virtblk_update_cache_mode>> blk_queue_write_cache(vblk->disk->queue, writeback, false);
+ *   - drivers/block/xen-blkfront.c|1014| <<xlvbd_flush>> blk_queue_write_cache(info->rq, info->feature_flush ? true : false,
+ *   - drivers/ide/ide-disk.c|554| <<update_flush>> blk_queue_write_cache(drive->queue, wc, false);
+ *   - drivers/md/dm-table.c|1910| <<dm_table_set_restrictions>> blk_queue_write_cache(q, wc, fua);
+ *   - drivers/md/md.c|5506| <<md_alloc>> blk_queue_write_cache(mddev->queue, true, true);
+ *   - drivers/nvme/host/core.c|2204| <<nvme_set_queue_limits>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/nvme/host/multipath.c|393| <<nvme_mpath_alloc_disk>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/scsi/sd.c|152| <<sd_set_flush_flag>> blk_queue_write_cache(sdkp->disk->queue, wc, fua);
+ *
+ * http://www.unjeep.com/article/40696.html
+ *
+ * 硬盘在控制器上的一块内存芯片,其类型一般以SDRAM为主,具有极快的存取速度,
+ * 它是硬盘内部存储和外界接口之间的缓冲器.由于硬盘的内部数据传输速度和外界
+ * 介面传输速度不同,缓存在其中起到一个缓冲的作用.缓存的大小与速度是直接关
+ * 系到硬盘的传输速度的重要因素,能够大幅度地提高硬盘整体性能.
+ *
+ * 如果硬盘的cache启用了,那么很有可能写入的数据是写到了硬盘的cache中,而没
+ * 有真正写到磁盘介质上.
+ *
+ * 在linux下,查看磁盘cache是否开启可通过hdparm命令:
+ *
+ * #hdparm -W /dev/sdx    //是否开启cache，1为enable
+ * #hdparm -W 0 /dev/sdx  //关闭cache
+ * #hdparm -W 1 /dev/sdx  //enable cache
+ *
+ * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+ * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+ *
+ * blk_insert_flush()是非常重要的入口!
+ */
 void blk_queue_write_cache(struct request_queue *q, bool wc, bool fua)
 {
 	if (wc)
diff --git a/block/blk-softirq.c b/block/blk-softirq.c
index 6e7ec87d49fa..71a852ff1c86 100644
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@ -14,12 +14,30 @@
 
 #include "blk.h"
 
+/*
+ * 在以下使用blk_cpu_done:
+ *   - block/blk-softirq.c|28| <<blk_done_softirq>> cpu_list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|47| <<trigger_softirq>> list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|86| <<blk_softirq_cpu_dead>> list_splice_init(&per_cpu(blk_cpu_done, cpu),
+ *   - block/blk-softirq.c|87| <<blk_softirq_cpu_dead>> this_cpu_ptr(&blk_cpu_done));
+ *   - block/blk-softirq.c|126| <<__blk_complete_request>> list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|148| <<blk_softirq_init>> INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
+ *
+ * 上面放的是request
+ */
 static DEFINE_PER_CPU(struct list_head, blk_cpu_done);
 
 /*
  * Softirq action handler - move entries to local list and loop over them
  * while passing them to the queue registered handler.
  */
+/*
+ * 在以下使用blk_done_softirq():
+ *   - 针对percpu的blk_cpu_done上的每一个request
+ * 调用rq->q->mq_ops->complete(rq)
+ * Softirq action handler - move entries to local list and loop over them
+ * while passing them to the queue registered handler.
+ */
 static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 {
 	struct list_head *cpu_list, local_list;
@@ -39,6 +57,13 @@ static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 }
 
 #ifdef CONFIG_SMP
+/*
+ * 在以下使用trigger_softirq():
+ *   - block/blk-softirq.c|62| <<raise_blk_irq>> data->func = trigger_softirq;
+ *
+ * 把data表示的request放入percpu的blk_cpu_done
+ * 触发raise_softirq_irqoff(BLOCK_SOFTIRQ) = blk_done_softirq()
+ */
 static void trigger_softirq(void *data)
 {
 	struct request *rq = data;
@@ -54,6 +79,10 @@ static void trigger_softirq(void *data)
 /*
  * Setup and invoke a run of 'trigger_softirq' on the given cpu.
  */
+/*
+ * called by:
+ *   - block/blk-softirq.c|137| <<__blk_complete_request>> } else if (raise_blk_irq(ccpu, req))
+ */
 static int raise_blk_irq(int cpu, struct request *rq)
 {
 	if (cpu_online(cpu)) {
@@ -76,6 +105,12 @@ static int raise_blk_irq(int cpu, struct request *rq)
 }
 #endif
 
+/*
+ * 在blk_softirq_init()中被使用:
+ * cpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,
+ *                           "block/softirq:dead", NULL,
+ *                           blk_softirq_cpu_dead);
+ */
 static int blk_softirq_cpu_dead(unsigned int cpu)
 {
 	/*
@@ -91,6 +126,10 @@ static int blk_softirq_cpu_dead(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|591| <<__blk_mq_complete_request>> __blk_complete_request(rq);
+ */
 void __blk_complete_request(struct request *req)
 {
 	struct request_queue *q = req->q;
diff --git a/block/blk-stat.c b/block/blk-stat.c
index 7da302ff88d0..6882129dba25 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -12,12 +12,115 @@
 #include "blk-mq.h"
 #include "blk.h"
 
+/*
+ * 核心的patch
+ *
+ * commit 34dbad5d26e2f4b88e60f0e9ad03f99480802812
+ * Author: Omar Sandoval <osandov@fb.com>
+ * Date:   Tue Mar 21 08:56:08 2017 -0700
+ *
+ * blk-stat: convert to callback-based statistics reporting
+ *
+ * Currently, statistics are gathered in ~0.13s windows, and users grab the
+ * statistics whenever they need them. This is not ideal for both in-tree
+ * users:
+ *
+ * 1. Writeback throttling wants its own dynamically sized window of
+ *    statistics. Since the blk-stats statistics are reset after every
+ *    window and the wbt windows don't line up with the blk-stats windows,
+ *    wbt doesn't see every I/O.
+ * 2. Polling currently grabs the statistics on every I/O. Again, depending
+ *    on how the window lines up, we may miss some I/Os. It's also
+ *    unnecessary overhead to get the statistics on every I/O; the hybrid
+ *    polling heuristic would be just as happy with the statistics from the
+ *    previous full window.
+ *
+ * This reworks the blk-stats infrastructure to be callback-based: users
+ * register a callback that they want called at a given time with all of
+ * the statistics from the window during which the callback was active.
+ * Users can dynamically bucketize the statistics. wbt and polling both
+ * currently use read vs. write, but polling can be extended to further
+ * subdivide based on request size.
+ *
+ * The callbacks are kept on an RCU list, and each callback has percpu
+ * stats buffers. There will only be a few users, so the overhead on the
+ * I/O completion side is low. The stats flushing is also simplified
+ * considerably: since the timer function is responsible for clearing the
+ * statistics, we don't have to worry about stale statistics.
+ *
+ * wbt is a trivial conversion. After the conversion, the windowing problem
+ * mentioned above is fixed.
+ *
+ * For polling, we register an extra callback that caches the previous
+ * window's statistics in the struct request_queue for the hybrid polling
+ * heuristic to use.
+ *
+ * Since we no longer have a single stats buffer for the request queue,
+ * this also removes the sysfs and debugfs stats entries. To replace those,
+ * we add a debugfs entry for the poll statistics.
+ *
+ * Signed-off-by: Omar Sandoval <osandov@fb.com>
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ *
+ *
+ * 在blk_alloc_queue_stats()分配一个struct blk_queue_stats结构,
+ * 初始化清空callback链表,设置stats->enable_accounting = false.
+ *
+ * blk_alloc_queue_node()
+ *  -> blk_alloc_queue_stats()
+ *
+ * 在poll的时候把struct blk_stat_callback链接入q->stats->callbacks链表
+ * 设置blk_queue_flag_set(QUEUE_FLAG_STATS, q)
+ *
+ * blk_poll()
+ *  -> blk_mq_poll_hybrid()
+ *      -> blk_mq_poll_hybrid_sleep()
+ *          -> blk_mq_poll_nsecs()
+ *              -> blk_poll_stats_enable()
+ *                  -> blk_stat_add_callback()
+ *
+ * 在end request的时候, 会调用blk_stat_add().
+ * 核心思想是找到request的request_queue->stats,遍历其callback链表,
+ * 对于链表上的每一个struct blk_stat_callback, 调用cb->bucket_fn()选取bucket,
+ * 然后把参数的now汇入对应bucket的struct blk_rq_stat
+ *
+ * 在end request的时候, 还会触发timer,
+ *
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ *
+ * timer最终触发blk_stat_timer_fn()->blk_mq_poll_stats_fn().
+ * blk_mq_poll_stats_fn()的核心思想是把每个bucket的
+ * q->poll_stat[bucket] = cb->stat[bucket];
+ *
+ * 在poll的时候会根据这些stat决定要先在hybrid的时候sleep多久!
+ *
+ *
+ * slides: I/O Latency Optimization with Polling
+ */
+
 struct blk_queue_stats {
 	struct list_head callbacks;
 	spinlock_t lock;
+	/*
+	 * 在以下使用enable_accounting:
+	 *   - block/blk-stat.c|161| <<blk_stat_remove_callback>> if (list_empty(&q->stats->callbacks) && !q->stats->enable_accounting)
+	 *   - block/blk-stat.c|187| <<blk_stat_enable_accounting>> q->stats->enable_accounting = true;
+	 *   - block/blk-stat.c|203| <<blk_alloc_queue_stats>> stats->enable_accounting = false;
+	 */
 	bool enable_accounting;
 };
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|199| <<latency_stat_init>> blk_rq_stat_init(&stat->rqs);
+ *   - block/blk-stat.c|93| <<blk_stat_timer_fn>> blk_rq_stat_init(&cb->stat[bucket]);
+ *   - block/blk-stat.c|101| <<blk_stat_timer_fn>> blk_rq_stat_init(&cpu_stat[bucket]);
+ *   - block/blk-stat.c|153| <<blk_stat_add_callback>> blk_rq_stat_init(&cpu_stat[bucket]);
+ */
 void blk_rq_stat_init(struct blk_rq_stat *stat)
 {
 	stat->min = -1ULL;
@@ -26,6 +129,11 @@ void blk_rq_stat_init(struct blk_rq_stat *stat)
 }
 
 /* src is a per-cpu stat, mean isn't initialized */
+/*
+ * called by:
+ *   - block/blk-iolatency.c|210| <<latency_stat_sum>> blk_rq_stat_sum(&sum->rqs, &stat->rqs);
+ *   - block/blk-stat.c|100| <<blk_stat_timer_fn>> blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
+ */
 void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 {
 	if (!src->nr_samples)
@@ -40,6 +148,18 @@ void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 	dst->nr_samples += src->nr_samples;
 }
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|222| <<latency_stat_record_time>> blk_rq_stat_add(&stat->rqs, req_time);
+ *   - block/blk-stat.c|80| <<blk_stat_add>> blk_rq_stat_add(stat, value);
+ *
+ * 调用的一个例子:
+ * __blk_mq_end_request()
+ *  -> blk_stat_add()
+ *      -> blk_rq_stat_add()
+ *
+ * 把参数的value汇入struct blk_rq_stat
+ */
 void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 {
 	stat->min = min(stat->min, value);
@@ -48,6 +168,14 @@ void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|540| <<__blk_mq_end_request>> blk_stat_add(rq, now);
+ *
+ * 核心思想是找到request的request_queue->stats,遍历其callback链表,
+ * 对于链表上的每一个struct blk_stat_callback, 调用cb->bucket_fn()选取bucket,
+ * 然后把参数的now汇入对应bucket的struct blk_rq_stat
+ */
 void blk_stat_add(struct request *rq, u64 now)
 {
 	struct request_queue *q = rq->q;
@@ -56,6 +184,15 @@ void blk_stat_add(struct request *rq, u64 now)
 	int bucket, cpu;
 	u64 value;
 
+	/*
+	 * 在以下使用io_start_time_ns:
+	 *   - block/bfq-iosched.c|5912| <<bfq_finish_requeue_request>> rq->io_start_time_ns,
+	 *   - block/blk-mq.c|671| <<blk_mq_start_request>> rq->io_start_time_ns = ktime_get_ns();
+	 *   - block/blk-mq.c|327| <<blk_mq_rq_ctx_init>> rq->io_start_time_ns = 0;
+	 *   - block/blk-stat.c|59| <<blk_stat_add>> value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
+	 *   - block/blk-wbt.c|619| <<wbt_issue>> rwb->sync_issue = rq->io_start_time_ns;
+	 *   - block/kyber-iosched.c|651| <<kyber_completed_request>> now - rq->io_start_time_ns);
+	 */
 	value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
 
 	blk_throtl_stat_add(rq, value);
@@ -70,13 +207,34 @@ void blk_stat_add(struct request *rq, u64 now)
 		if (bucket < 0)
 			continue;
 
+		/*
+		 * struct blk_stat_callback:
+		 *  -> struct blk_rq_stat __percpu *cpu_stat;
+		 *  -> struct blk_rq_stat stat;
+		 */
 		stat = &per_cpu_ptr(cb->cpu_stat, cpu)[bucket];
+		/*
+		 * 把参数的value汇入struct blk_rq_stat
+		 */
 		blk_rq_stat_add(stat, value);
 	}
 	put_cpu();
 	rcu_read_unlock();
 }
 
+/*
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ * 说明end request的时候mod timer
+ * The timer callback will be called when the window expires
+ * to gather block statistics during a time window in milliseconds.
+ *
+ * 在以下使用blk_stat_timer_fn():
+ *   - block/blk-stat.c|137| <<blk_stat_alloc_callback>> timer_setup(&cb->timer, blk_stat_timer_fn, 0);
+ */
 static void blk_stat_timer_fn(struct timer_list *t)
 {
 	struct blk_stat_callback *cb = from_timer(cb, t, timer);
@@ -89,16 +247,44 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	for_each_online_cpu(cpu) {
 		struct blk_rq_stat *cpu_stat;
 
+		/*
+		 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+		 */
 		cpu_stat = per_cpu_ptr(cb->cpu_stat, cpu);
 		for (bucket = 0; bucket < cb->buckets; bucket++) {
+			/*
+			 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+			 */
 			blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
 			blk_rq_stat_init(&cpu_stat[bucket]);
 		}
 	}
 
+	/*
+	 * __blk_mq_end_request()
+	 *  -> blk_mq_poll_stats_start()
+	 *      -> blk_stat_activate_msecs()
+	 *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+	 *
+	 * 说明end request的时候mod timer
+	 * The timer callback will be called when the window expires
+	 * to gather block statistics during a time window in milliseconds.
+	 *
+	 * -----------------------------------------
+	 *
+	 * 从blk_mq_init_allocated_queue()进来blk_stat_alloc_callback()
+	 * 的话timer_fn是blk_mq_poll_stats_fn()
+	 */
 	cb->timer_fn(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2861| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *   - block/blk-wbt.c|829| <<wbt_init>> rwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);
+ *
+ * 分配一个struct blk_stat_callback
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
@@ -110,12 +296,18 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 	if (!cb)
 		return NULL;
 
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话buckets是BLK_MQ_POLL_STATS_BKTS
+	 */
 	cb->stat = kmalloc_array(buckets, sizeof(struct blk_rq_stat),
 				 GFP_KERNEL);
 	if (!cb->stat) {
 		kfree(cb);
 		return NULL;
 	}
+	/*
+	 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+	 */
 	cb->cpu_stat = __alloc_percpu(buckets * sizeof(struct blk_rq_stat),
 				      __alignof__(struct blk_rq_stat));
 	if (!cb->cpu_stat) {
@@ -124,15 +316,59 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 		return NULL;
 	}
 
+	/*
+	 * 调用timer_fn的地方:
+	 *   - block/blk-stat.c|257| <<blk_stat_timer_fn>> cb->timer_fn(cb);
+	 *
+	 * 从blk_mq_init_allocated_queue()进来的话timer_fn是blk_mq_poll_stats_fn()
+	 */
 	cb->timer_fn = timer_fn;
+	/*
+	 * 调用bucket_fn的地方:
+	 *   - block/blk-stat.c|206| <<blk_stat_add>> bucket = cb->bucket_fn(rq);
+	 *
+	 * 从blk_mq_init_allocated_queue()进来的话bucket_fn是blk_mq_poll_stats_bkt()
+	 */
 	cb->bucket_fn = bucket_fn;
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话data是'struct request_queue'
+	 */
 	cb->data = data;
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话buckets是BLK_MQ_POLL_STATS_BKTS
+	 */
 	cb->buckets = buckets;
+	/*
+	 * __blk_mq_end_request()
+	 *  -> blk_mq_poll_stats_start()
+	 *      -> blk_stat_activate_msecs()
+	 *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+	 *
+	 * 说明end request的时候mod timer
+	 * The timer callback will be called when the window expires
+	 * to gather block statistics during a time window in milliseconds.
+	 */
 	timer_setup(&cb->timer, blk_stat_timer_fn, 0);
 
 	return cb;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3354| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|852| <<wbt_init>> blk_stat_add_callback(q, rwb->cb);
+ *
+ * poll调用的例子
+ * blk_poll()
+ *  -> blk_mq_poll_hybrid()
+ *      -> blk_mq_poll_hybrid_sleep()
+ *          -> blk_mq_poll_nsecs()
+ *              -> blk_poll_stats_enable()
+ *                  -> blk_stat_add_callback()
+ *
+ * 核心思想是把struct blk_stat_callback链接入q->stats->callbacks链表
+ * 设置blk_queue_flag_set(QUEUE_FLAG_STATS, q)
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -149,10 +385,22 @@ void blk_stat_add_callback(struct request_queue *q,
 
 	spin_lock(&q->stats->lock);
 	list_add_tail_rcu(&cb->list, &q->stats->callbacks);
+	/*
+	 * 在以下使用QUEUE_FLAG_STATS:
+	 *   - block/blk-mq.c|670| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+	 *   - block/blk-stat.c|320| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|335| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|376| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 */
 	blk_queue_flag_set(QUEUE_FLAG_STATS, q);
 	spin_unlock(&q->stats->lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|881| <<__blk_release_queue>> blk_stat_remove_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|696| <<wbt_exit>> blk_stat_remove_callback(q, rwb->cb);
+ */
 void blk_stat_remove_callback(struct request_queue *q,
 			      struct blk_stat_callback *cb)
 {
@@ -165,6 +413,10 @@ void blk_stat_remove_callback(struct request_queue *q,
 	del_timer_sync(&cb->timer);
 }
 
+/*
+ * 在以下使用blk_stat_free_callback_rcu():
+ *   - block/blk-stat.c|187| <<blk_stat_free_callback>> call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
+ */
 static void blk_stat_free_callback_rcu(struct rcu_head *head)
 {
 	struct blk_stat_callback *cb;
@@ -175,12 +427,23 @@ static void blk_stat_free_callback_rcu(struct rcu_head *head)
 	kfree(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2922| <<blk_mq_init_allocated_queue>> blk_stat_free_callback(q->poll_cb);
+ *   - block/blk-sysfs.c|882| <<__blk_release_queue>> blk_stat_free_callback(q->poll_cb);
+ *   - block/blk-wbt.c|697| <<wbt_exit>> blk_stat_free_callback(rwb->cb);
+ */
 void blk_stat_free_callback(struct blk_stat_callback *cb)
 {
 	if (cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
 
+/*
+ * called by:
+ *   - block/blk-throttle.c|2503| <<blk_throtl_register_queue>> blk_stat_enable_accounting(q);
+ *   - block/kyber-iosched.c|431| <<kyber_init_sched>> blk_stat_enable_accounting(q);
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	spin_lock(&q->stats->lock);
@@ -190,6 +453,13 @@ void blk_stat_enable_accounting(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_stat_enable_accounting);
 
+/*
+ * called by:
+ *   - block/blk-core.c|515| <<blk_alloc_queue_node>> q->stats = blk_alloc_queue_stats();
+ *
+ * 分配一个struct blk_queue_stats结构, 初始化清空callback链表
+ * 设置stats->enable_accounting = false;
+ */
 struct blk_queue_stats *blk_alloc_queue_stats(void)
 {
 	struct blk_queue_stats *stats;
@@ -205,6 +475,11 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 	return stats;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|562| <<blk_alloc_queue_node>> blk_free_queue_stats(q->stats);
+ *   - block/blk-sysfs.c|884| <<__blk_release_queue>> blk_free_queue_stats(q->stats);
+ */
 void blk_free_queue_stats(struct blk_queue_stats *stats)
 {
 	if (!stats)
diff --git a/block/blk-stat.h b/block/blk-stat.h
index 17b47a86eefb..5cd0d92b34a7 100644
--- a/block/blk-stat.h
+++ b/block/blk-stat.h
@@ -126,6 +126,12 @@ void blk_stat_free_callback(struct blk_stat_callback *cb);
  * gathering statistics.
  * @cb: The callback.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3369| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+ *   - block/blk-stat.c|203| <<blk_stat_add>> if (!blk_stat_is_active(cb))
+ *   - block/blk-wbt.c|593| <<wbt_wait>> if (!blk_stat_is_active(rwb->cb))
+ */
 static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
 {
 	return timer_pending(&cb->timer);
@@ -139,12 +145,20 @@ static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-wbt.c|349| <<rwb_arm_timer>> blk_stat_activate_nsecs(rwb->cb, rwb->cur_win_nsec);
+ */
 static inline void blk_stat_activate_nsecs(struct blk_stat_callback *cb,
 					   u64 nsecs)
 {
 	mod_timer(&cb->timer, jiffies + nsecs_to_jiffies(nsecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-wbt.c|712| <<wbt_disable_default>> blk_stat_deactivate(rwb->cb);
+ */
 static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
 {
 	del_timer_sync(&cb->timer);
@@ -158,6 +172,19 @@ static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3372| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+ *
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ * 说明end request的时候mod timer
+ * The timer callback will be called when the window expires
+ * to gather block statistics during a time window in milliseconds.
+ */
 static inline void blk_stat_activate_msecs(struct blk_stat_callback *cb,
 					   unsigned int msecs)
 {
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index fca9b158f4a0..5a5efecadc25 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -390,8 +390,23 @@ static ssize_t queue_poll_delay_store(struct request_queue *q, const char *page,
 	return count;
 }
 
+/*
+ * nvme.poll_queues=2
+ *
+ * # cat /sys/block/nvme0n1/queue/io_poll
+ */
 static ssize_t queue_poll_show(struct request_queue *q, char *page)
 {
+	/*
+	 * 使用QUEUE_FLAG_POLL的地方:
+	 *   - block/blk-mq.c|3021| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-core.c|936| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - block/blk-mq.c|3774| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - drivers/nvme/host/core.c|805| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+	 *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+	 */
 	return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
 }
 
@@ -401,6 +416,11 @@ static ssize_t queue_poll_store(struct request_queue *q, const char *page,
 	unsigned long poll_on;
 	ssize_t ret;
 
+	/*
+	 * 必须设置nvme.poll_queues=2, 否则会:
+	 * # echo 1 > /sys/block/nvme0n1/queue/io_poll
+	 * bash: echo: write error: Invalid argument
+	 */
 	if (!q->tag_set || q->tag_set->nr_maps <= HCTX_TYPE_POLL ||
 	    !q->tag_set->map[HCTX_TYPE_POLL].nr_queues)
 		return -EINVAL;
@@ -409,6 +429,16 @@ static ssize_t queue_poll_store(struct request_queue *q, const char *page,
 	if (ret < 0)
 		return ret;
 
+	/*
+	 * 使用QUEUE_FLAG_POLL的地方:
+	 *   - block/blk-mq.c|3021| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-core.c|936| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - block/blk-mq.c|3774| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - drivers/nvme/host/core.c|805| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+	 *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+	 */
 	if (poll_on)
 		blk_queue_flag_set(QUEUE_FLAG_POLL, q);
 	else
@@ -873,6 +903,10 @@ static void blk_exit_queue(struct request_queue *q)
  *     of the request queue reaches zero, blk_release_queue is called to release
  *     all allocated resources of the request queue.
  */
+/*
+ * 在以下使用__blk_release_queue():
+ *   - block/blk-sysfs.c|912| <<blk_release_queue>> INIT_WORK(&q->release_work, __blk_release_queue);
+ */
 static void __blk_release_queue(struct work_struct *work)
 {
 	struct request_queue *q = container_of(work, typeof(*q), release_work);
diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 8aa68fae96ad..8bd7e5e77024 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -14,14 +14,31 @@
 
 static DECLARE_FAULT_ATTR(fail_io_timeout);
 
+/*
+ * echo 100 > /sys/kernel/debug/fail_io_timeout/probability
+ * echo 1000 > /sys/kernel/debug/fail_io_timeout/times
+ * echo 1 > /sys/block/nvme0n1/io-timeout-fail
+ * dd if=/dev/nvme0n1 of=/dev/null bs=512 count=1
+ */
 static int __init setup_fail_io_timeout(char *str)
 {
 	return setup_fault_attr(&fail_io_timeout, str);
 }
 __setup("fail_io_timeout=", setup_fail_io_timeout);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|776| <<blk_mq_complete_request>> if (unlikely(blk_should_fake_timeout(rq->q)))
+ */
 int blk_should_fake_timeout(struct request_queue *q)
 {
+	/*
+	 * 在以下使用QUEUE_FLAG_FAIL_IO:
+	 *   - block/blk-timeout.c|25| <<blk_should_fake_timeout>> if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
+	 *   - block/blk-timeout.c|45| <<part_timeout_show>> int set = test_bit(QUEUE_FLAG_FAIL_IO, &disk->queue->queue_flags);
+	 *   - block/blk-timeout.c|62| <<part_timeout_store>> blk_queue_flag_set(QUEUE_FLAG_FAIL_IO, q);
+	 *   - block/blk-timeout.c|64| <<part_timeout_store>> blk_queue_flag_clear(QUEUE_FLAG_FAIL_IO, q);
+	 */
 	if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
 		return 0;
 
diff --git a/block/blk-zoned.c b/block/blk-zoned.c
index d00fcfd71dfe..fe7c472240e5 100644
--- a/block/blk-zoned.c
+++ b/block/blk-zoned.c
@@ -20,6 +20,74 @@
 
 #include "blk.h"
 
+/*
+ * 使用SMR(Shingled Magnetic Recording)技术后,可以增加存储密度,
+ * 但是带来的开销就是写操作必须是连续的(sequential write).
+ *
+ * 这样就把存储分成好多个zone,每个zone都是写连续的.
+ *
+ * NVMe的FTL层会建立mapping,帮助管理真实的存储.但是跳过FTL会有更多好处.
+ * 可以把NVMe切成多个zoned namespace,这样每个app可以直接使用自己的zoned namespace:
+ *
+ * - Reduced TCO due to minimal DRAM requirement per SSD
+ * - Additional savings due to decreased need for over provisioning of NAND media
+ * - Better drive endurance by reducing write amplification
+ * - Dramatically reduced latency
+ * - Significantly improved throughput
+ *
+ * null_blk和fio都支持zoned block.
+ *
+ * 相关的网站:
+ * - https://zonedstorage.io/introduction/zoned-storage/
+ * - https://zonedstorage.io/benchmarking/fio/
+ * - https://zonedstorage.io/getting-started/prerequisite/
+ * - https://blog.westerndigital.com/what-is-zoned-storage-initiative/
+ * - https://blog.westerndigital.com/storage-architectures-zettabyte-age/
+ */
+
+/*
+ * We've been doing a lot of work with the open source and Linux communities to
+ * contribute to the core technologies of SMR (Shingled Magnetic Recording). By
+ * overlaying tracks on a disk, we can achieve roughly a 20% increase in
+ * capacity. This requires data to be written sequentially so that it will not
+ * alter an underlying write track.
+ *
+ * Rearchitecting can require some effort initially, but the density and cost
+ * benefits are substantial and demonstrate all the advantages of purpose-built
+ * hardware and software-aware constructs. Today, our customers are already
+ * deploying SMR technology, and we expect that 50% of the HDD exabytes we ship
+ * will be on SMR by 2023.
+ *
+ *
+ * Zoned block devices are quite different than the block devices most people
+ * are used to. The concept came from shingled magnetic recording (SMR)
+ * devices, which allow much higher density storage, but that extra capacity
+ * comes with a price: less flexibility. Zoned devices have regions (zones)
+ * that can only be written sequentially; there is no random access for writes
+ * to those zones. Linux already supports these devices, and filesystems are
+ * adding support as well, but some applications may want a simpler, more
+ * straightforward interface; that's what a new filesystem, zonefs, is
+ * targeting.
+ *
+ *
+ * https://zonedstorage.io
+ *
+ * The zones of zoned storage devices must be written sequentially. Each zone
+ * of the device address space has a write pointer that keeps track of the
+ * position of the next write. Data in a zone cannot be directly overwritten.
+ * The zone must first be erased using a special command (zone reset). The
+ * figure below illustrates this principle.
+ *
+ * Linux ZBD interface implementation provides functions to discover the zone
+ * configuration of a zoned device and functions to manage zones (e.g. Zone
+ * reset). Furthermore, the Linux kernel ZBD support also modifies the kernel
+ * block I/O stack to ensure that the device access constraints (zone spanning
+ * commands, sequential write ordering, etc) are met.
+ */
+
+/*
+ * 没人使用
+ */
 static inline sector_t blk_zone_start(struct request_queue *q,
 				      sector_t sector)
 {
@@ -31,6 +99,11 @@ static inline sector_t blk_zone_start(struct request_queue *q,
 /*
  * Return true if a request is a write requests that needs zone write locking.
  */
+/*
+ * called by:
+ *   - include/linux/blkdev.h|1849| <<blk_req_zone_write_lock>> if (blk_req_needs_zone_write_lock(rq))
+ *   - include/linux/blkdev.h|1867| <<blk_req_can_dispatch_to_zone>> if (!blk_req_needs_zone_write_lock(rq))
+ */
 bool blk_req_needs_zone_write_lock(struct request *rq)
 {
 	if (!rq->q->seq_zones_wlock)
@@ -50,6 +123,10 @@ bool blk_req_needs_zone_write_lock(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_req_needs_zone_write_lock);
 
+/*
+ * called by:
+ *   - include/linux/blkdev.h|1850| <<blk_req_zone_write_lock>> __blk_req_zone_write_lock(rq);
+ */
 void __blk_req_zone_write_lock(struct request *rq)
 {
 	if (WARN_ON_ONCE(test_and_set_bit(blk_rq_zone_no(rq),
@@ -61,6 +138,10 @@ void __blk_req_zone_write_lock(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(__blk_req_zone_write_lock);
 
+/*
+ * called by:
+ *   - include/linux/blkdev.h|1856| <<blk_req_zone_write_unlock>> __blk_req_zone_write_unlock(rq);
+ */
 void __blk_req_zone_write_unlock(struct request *rq)
 {
 	rq->rq_flags &= ~RQF_ZONE_WRITE_LOCKED;
@@ -77,6 +158,13 @@ EXPORT_SYMBOL_GPL(__blk_req_zone_write_unlock);
  * Return the total number of zones of a zoned block device.  For a block
  * device without zone capabilities, the number of zones is always 0.
  */
+/*
+ * called by:
+ *   - block/ioctl.c|515| <<blkdev_ioctl>> return put_uint(arg, blkdev_nr_zones(bdev->bd_disk));
+ *   - drivers/block/null_blk_main.c|1947| <<null_gendisk_register>> nullb->q->nr_zones = blkdev_nr_zones(disk);
+ *   - drivers/md/dm-table.c|1962| <<dm_table_set_restrictions>> q->nr_zones = blkdev_nr_zones(t->md->disk);
+ *   - drivers/md/dm-zoned-target.c|730| <<dmz_get_zoned_device>> dev->nr_zones = blkdev_nr_zones(dev->bdev->bd_disk);
+ */
 unsigned int blkdev_nr_zones(struct gendisk *disk)
 {
 	sector_t zone_sectors = blk_queue_zone_sectors(disk->queue);
@@ -106,6 +194,15 @@ EXPORT_SYMBOL_GPL(blkdev_nr_zones);
  *    Note: The caller must use memalloc_noXX_save/restore() calls to control
  *    memory allocations done within this function.
  */
+/*
+ * called by:
+ *   - block/blk-zoned.c|263| <<blkdev_report_zones_ioctl>> ret = blkdev_report_zones(bdev, rep.sector, rep.nr_zones,
+ *   - drivers/md/dm-flakey.c|469| <<flakey_report_zones>> return blkdev_report_zones(fc->dev->bdev, sector, nr_zones,
+ *   - drivers/md/dm-linear.c|146| <<linear_report_zones>> return blkdev_report_zones(lc->dev->bdev, sector, nr_zones,
+ *   - drivers/md/dm-zoned-metadata.c|1179| <<dmz_init_zones>> ret = blkdev_report_zones(dev->bdev, 0, BLK_ALL_ZONES, dmz_init_zone,
+ *   - drivers/md/dm-zoned-metadata.c|1223| <<dmz_update_zone>> ret = blkdev_report_zones(zmd->dev->bdev, dmz_start_sect(zmd, zone), 1,
+ *   - fs/f2fs/super.c|2938| <<init_blkz_info>> ret = blkdev_report_zones(bdev, 0, BLK_ALL_ZONES, f2fs_report_zone_cb,
+ */
 int blkdev_report_zones(struct block_device *bdev, sector_t sector,
 			unsigned int nr_zones, report_zones_cb cb, void *data)
 {
@@ -123,6 +220,10 @@ int blkdev_report_zones(struct block_device *bdev, sector_t sector,
 }
 EXPORT_SYMBOL_GPL(blkdev_report_zones);
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|248| <<blkdev_zone_mgmt>> blkdev_allow_reset_all_zones(bdev, sector, nr_sectors)) {
+ */
 static inline bool blkdev_allow_reset_all_zones(struct block_device *bdev,
 						sector_t sector,
 						sector_t nr_sectors)
@@ -153,6 +254,12 @@ static inline bool blkdev_allow_reset_all_zones(struct block_device *bdev,
  *    The operation to execute on each zone can be a zone reset, open, close
  *    or finish request.
  */
+/*
+ * called by:
+ *   - block/blk-zoned.c|322| <<blkdev_zone_mgmt_ioctl>> return blkdev_zone_mgmt(bdev, op, zrange.sector, zrange.nr_sectors,
+ *   - drivers/md/dm-zoned-metadata.c|1289| <<dmz_reset_zone>> ret = blkdev_zone_mgmt(dev->bdev, REQ_OP_ZONE_RESET,
+ *   - fs/f2fs/segment.c|1784| <<__f2fs_issue_discard_zone>> return blkdev_zone_mgmt(bdev, REQ_OP_ZONE_RESET,
+ */
 int blkdev_zone_mgmt(struct block_device *bdev, enum req_opf op,
 		     sector_t sector, sector_t nr_sectors,
 		     gfp_t gfp_mask)
@@ -173,6 +280,31 @@ int blkdev_zone_mgmt(struct block_device *bdev, enum req_opf op,
 	if (!op_is_zone_mgmt(op))
 		return -EOPNOTSUPP;
 
+	/*
+	 * #define _GNU_SOURCE 1
+	 * #include <sys/ioctl.h>
+	 * #include <sys/types.h>
+	 * #include <sys/stat.h>
+	 * #include <fcntl.h>
+	 *
+	 * typedef unsigned long long __u64;
+	 *
+	 * struct blk_zone_range {
+	 *	__u64 sector;
+	 *	__u64 nr_sectors;
+	 * };
+	 *
+	 * #define BLKRESETZONE    _IOW(0x12, 131, struct blk_zone_range)
+	 *
+	 * int main(void)
+	 * {
+	 *	int fd = open("/dev/nullb0", O_RDWR|O_DIRECT);
+	 *	struct blk_zone_range zr = {4096, 0xfffffffffffff000ULL};
+	 *	ioctl(fd, BLKRESETZONE, &zr);
+	 *	return 0;
+	 *
+	 * 这里似乎有个bug??
+	 */
 	if (!nr_sectors || end_sector > capacity)
 		/* Out of range */
 		return -EINVAL;
@@ -217,6 +349,10 @@ struct zone_report_args {
 	struct blk_zone __user *zones;
 };
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|316| <<blkdev_report_zones_ioctl>> blkdev_copy_zone_to_user, &args)
+ */
 static int blkdev_copy_zone_to_user(struct blk_zone *zone, unsigned int idx,
 				    void *data)
 {
@@ -231,6 +367,10 @@ static int blkdev_copy_zone_to_user(struct blk_zone *zone, unsigned int idx,
  * BLKREPORTZONE ioctl processing.
  * Called from blkdev_ioctl.
  */
+/*
+ * 处理blkdev_ioctl(BLKREPORTZONE):
+ *   - block/ioctl.c|506| <<blkdev_ioctl>> return blkdev_report_zones_ioctl(bdev, mode, cmd, arg);
+ */
 int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,
 			      unsigned int cmd, unsigned long arg)
 {
@@ -275,6 +415,10 @@ int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,
  * BLKRESETZONE, BLKOPENZONE, BLKCLOSEZONE and BLKFINISHZONE ioctl processing.
  * Called from blkdev_ioctl.
  */
+/*
+ * 处理blkdev_ioctl(BLKFINISHZONE):
+ *   - block/ioctl.c|511| <<blkdev_ioctl>> return blkdev_zone_mgmt_ioctl(bdev, mode, cmd, arg);
+ */
 int blkdev_zone_mgmt_ioctl(struct block_device *bdev, fmode_t mode,
 			   unsigned int cmd, unsigned long arg)
 {
@@ -323,6 +467,11 @@ int blkdev_zone_mgmt_ioctl(struct block_device *bdev, fmode_t mode,
 				GFP_KERNEL);
 }
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|452| <<blk_revalidate_zone_cb>> blk_alloc_zone_bitmap(q->node, args->nr_zones);
+ *   - block/blk-zoned.c|462| <<blk_revalidate_zone_cb>> blk_alloc_zone_bitmap(q->node, args->nr_zones);
+ */
 static inline unsigned long *blk_alloc_zone_bitmap(int node,
 						   unsigned int nr_zones)
 {
@@ -330,6 +479,11 @@ static inline unsigned long *blk_alloc_zone_bitmap(int node,
 			    GFP_NOIO, node);
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|910| <<__blk_release_queue>> blk_queue_free_zone_bitmaps(q);
+ *   - block/blk-zoned.c|529| <<blk_revalidate_disk_zones>> blk_queue_free_zone_bitmaps(q);
+ */
 void blk_queue_free_zone_bitmaps(struct request_queue *q)
 {
 	kfree(q->conv_zones_bitmap);
@@ -350,6 +504,13 @@ struct blk_revalidate_zone_args {
 /*
  * Helper function to check the validity of zones of a zoned block device.
  */
+/*
+ * 在以下使用blk_revalidate_zone_cb():
+ *   - block/blk-zoned.c|512| <<blk_revalidate_disk_zones>> blk_revalidate_zone_cb, &args);
+ *
+ * 543         ret = disk->fops->report_zones(disk, 0, UINT_MAX,
+ * 544                                        blk_revalidate_zone_cb, &args);
+ */
 static int blk_revalidate_zone_cb(struct blk_zone *zone, unsigned int idx,
 				  void *data)
 {
@@ -432,6 +593,11 @@ static int blk_revalidate_zone_cb(struct blk_zone *zone, unsigned int idx,
  * drivers only q->nr_zones needs to be updated so that the sysfs exposed value
  * is correct.
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1941| <<null_gendisk_register>> int ret = blk_revalidate_disk_zones(disk);
+ *   - drivers/scsi/sd_zbc.c|440| <<sd_zbc_read_zones>> ret = blk_revalidate_disk_zones(disk);
+ */
 int blk_revalidate_disk_zones(struct gendisk *disk)
 {
 	struct request_queue *q = disk->queue;
diff --git a/block/blk.h b/block/blk.h
index 0b8884353f6b..b5e8ae15d920 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -15,22 +15,83 @@
 extern struct dentry *blk_debugfs_root;
 #endif
 
+/*
+ * blk_flush_queue来自blk_mq_hw_ctx->fq
+ */
 struct blk_flush_queue {
 	unsigned int		flush_queue_delayed:1;
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下使用flush_pending_idx:
+	 *   - block/blk-flush.c|276| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|392| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 */
 	unsigned int		flush_pending_idx:1;
+	/*
+	 * 在以下修改flush_running_idx:
+	 *   - block/blk-flush.c|362| <<flush_end_io>> fq->flush_running_idx ^= 1;
+	 * 在以下使用flush_running_idx:
+	 *   - block/blk-flush.c|358| <<flush_end_io>> running = &fq->flush_queue[fq->flush_running_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 */
 	unsigned int		flush_running_idx:1;
 	blk_status_t 		rq_status;
 	unsigned long		flush_pending_since;
+	/*
+	 * 使用flush_queue[2]的地方:
+	 *   - block/blk-flush.c|352| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|454| <<flush_end_io>> running = &fq->flush_queue[fq->flush_running_idx];
+	 *   - block/blk-flush.c|501| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|849| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[0]);
+	 *   - block/blk-flush.c|850| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[1]);
+	 */
 	struct list_head	flush_queue[2];
+	/*
+	 * 在以下使用flush_data_in_flight:
+	 *   - block/blk-flush.c|400| <<blk_flush_complete_seq>> list_move_tail(&rq->flush.list, &fq->flush_data_in_flight);
+	 *   - block/blk-flush.c|599| <<blk_kick_flush>> if (!list_empty(&fq->flush_data_in_flight) && q->elevator &&
+	 *   - block/blk-flush.c|934| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_data_in_flight);
+	 */
 	struct list_head	flush_data_in_flight;
+	/*
+	 * 使用flush_rq的地方:
+	 *   - block/blk-flush.c|504| <<blk_kick_flush>> struct request *flush_rq = fq->flush_rq;
+	 *   - block/blk-flush.c|845| <<blk_alloc_flush_queue>> fq->flush_rq = kzalloc_node(rq_sz, flags, node);
+	 *   - block/blk-flush.c|846| <<blk_alloc_flush_queue>> if (!fq->flush_rq)
+	 *   - block/blk-flush.c|875| <<blk_free_flush_queue>> kfree(fq->flush_rq);
+	 *   - block/blk-mq.c|2303| <<blk_mq_exit_hctx>> set->ops->exit_request(set, hctx->fq->flush_rq, hctx_idx);
+	 *   - block/blk-mq.c|2361| <<blk_mq_init_hctx>> if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx,
+	 *   - block/blk.h|81| <<is_flush_rq>> return hctx->fq->flush_rq == req;
+	 */
 	struct request		*flush_rq;
 
 	/*
 	 * flush_rq shares tag with this rq, both can't be active
 	 * at the same time
 	 */
+	/*
+	 * orig_rq在以下使用:
+	 *   - block/blk-flush.c|458| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+	 *   - block/blk-flush.c|624| <<blk_kick_flush>> fq->orig_rq = first_rq;
+	 */
 	struct request		*orig_rq;
 	struct lock_class_key	key;
+	/*
+	 * 使用mq_flush_lock的地方:
+	 *   - block/blk-flush.c|434| <<flush_end_io>> spin_lock_irqsave(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|438| <<flush_end_io>> spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|469| <<flush_end_io>> spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|631| <<mq_flush_data_end_io>> spin_lock_irqsave(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|633| <<mq_flush_data_end_io>> spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|771| <<blk_insert_flush>> spin_lock_irq(&fq->mq_flush_lock);
+	 *   - block/blk-flush.c|779| <<blk_insert_flush>> spin_unlock_irq(&fq->mq_flush_lock);
+	 *   - block/blk-flush.c|874| <<blk_alloc_flush_queue>> spin_lock_init(&fq->mq_flush_lock);
+	 *   - block/blk-flush.c|886| <<blk_alloc_flush_queue>> lockdep_set_class(&fq->mq_flush_lock, &fq->key);
+	 */
 	spinlock_t		mq_flush_lock;
 };
 
@@ -38,6 +99,12 @@ extern struct kmem_cache *blk_requestq_cachep;
 extern struct kobj_type blk_queue_ktype;
 extern struct ida blk_queue_ida;
 
+/*
+ * called by:
+ *   - block/blk-flush.c|301| <<flush_end_io>> struct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);
+ *   - block/blk-flush.c|427| <<mq_flush_data_end_io>> struct blk_flush_queue *fq = blk_get_flush_queue(q, ctx);
+ *   - block/blk-flush.c|464| <<blk_insert_flush>> struct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);
+ */
 static inline struct blk_flush_queue *
 blk_get_flush_queue(struct request_queue *q, struct blk_mq_ctx *ctx)
 {
@@ -61,6 +128,10 @@ void blk_free_flush_queue(struct blk_flush_queue *q);
 
 void blk_freeze_queue(struct request_queue *q);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|484| <<blk_mq_get_request>> blk_queue_enter_live(q);
+ */
 static inline void blk_queue_enter_live(struct request_queue *q)
 {
 	/*
@@ -107,6 +178,11 @@ static inline bool bvec_gap_to_prev(struct request_queue *q,
 	return __bvec_gap_to_prev(q, bprv, offset);
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|31| <<blk_rq_append_bio>> blk_rq_bio_prep(rq, *bio, nr_segs);
+ *   - block/blk-mq.c|2088| <<blk_mq_bio_to_request>> blk_rq_bio_prep(rq, bio, nr_segs);
+ */
 static inline void blk_rq_bio_prep(struct request *rq, struct bio *bio,
 		unsigned int nr_segs)
 {
diff --git a/block/genhd.c b/block/genhd.c
index ff6268970ddc..ef46052eeceb 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -86,6 +86,10 @@ unsigned int part_in_flight(struct request_queue *q, struct hd_struct *part)
 	return inflight;
 }
 
+/*
+ * called by:
+ *   - block/partition-generic.c|159| <<part_inflight_show>> part_in_flight_rw(q, p, inflight);
+ */
 void part_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 		       unsigned int inflight[2])
 {
@@ -108,8 +112,24 @@ void part_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 		inflight[1] = 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|858| <<blk_partition_remap>> p = __disk_get_part(bio->bi_disk, bio->bi_partno);
+ *   - block/genhd.c|148| <<disk_get_part>> part = __disk_get_part(disk, partno); 
+ *   - fs/buffer.c|3040| <<guard_bio_eod>> part = __disk_get_part(bio->bi_disk, bio->bi_partno);
+ *
+ * struct gendisk:
+ *  -> struct disk_part_tbl __rcu *part_tbl;
+ *  -> struct hd_struct part0;
+ * 返回gendisk->part_tlb->part[partno]
+ */
 struct hd_struct *__disk_get_part(struct gendisk *disk, int partno)
 {
+	/*
+	 * struct gendisk:
+	 *  -> struct disk_part_tbl __rcu *part_tbl;
+	 *  -> struct hd_struct part0;
+	 */
 	struct disk_part_tbl *ptbl = rcu_dereference(disk->part_tbl);
 
 	if (unlikely(partno < 0 || partno >= ptbl->len))
@@ -131,11 +151,32 @@ struct hd_struct *__disk_get_part(struct gendisk *disk, int partno)
  * RETURNS:
  * Pointer to the found partition on success, NULL if not found.
  */
+/*
+ * called by:
+ *   - block/genhd.c|965| <<bdget_disk>> part = disk_get_part(disk, partno);
+ *   - block/genhd.c|1495| <<blk_lookup_devt>> part = disk_get_part(disk, partno);
+ *   - block/ioctl.c|74| <<blkpg_ioctl>> part = disk_get_part(disk, partno);
+ *   - block/ioctl.c|112| <<blkpg_ioctl>> part = disk_get_part(disk, partno);
+ *   - fs/block_dev.c|1598| <<__blkdev_get>> bdev->bd_part = disk_get_part(disk, partno);
+ *   - fs/block_dev.c|1649| <<__blkdev_get>> bdev->bd_part = disk_get_part(disk, partno);
+ *   - init/do_mounts.c|154| <<devt_from_partuuid>> part = disk_get_part(disk, dev_to_part(dev)->partno + offset);
+ *
+ * struct gendisk:
+ *  -> struct disk_part_tbl __rcu *part_tbl;
+ *  -> struct hd_struct part0;
+ * 返回gendisk->part_tlb->part[partno]
+ */
 struct hd_struct *disk_get_part(struct gendisk *disk, int partno)
 {
 	struct hd_struct *part;
 
 	rcu_read_lock();
+	/*
+	 * struct gendisk:
+	 *  -> struct disk_part_tbl __rcu *part_tbl;
+	 *  -> struct hd_struct part0;
+	 * 返回gendisk->part_tlb->part[partno]
+	 */
 	part = __disk_get_part(disk, partno);
 	if (part)
 		get_device(part_to_dev(part));
@@ -189,6 +230,20 @@ EXPORT_SYMBOL_GPL(disk_part_iter_init);
  * CONTEXT:
  * Don't care.
  */
+/*
+ * called by:
+ *   - block/genhd.c|733| <<register_disk>> while ((part = disk_part_iter_next(&piter)))
+ *   - block/genhd.c|855| <<del_gendisk>> while ((part = disk_part_iter_next(&piter))) {
+ *   - block/genhd.c|1058| <<printk_all_partitions>> while ((part = disk_part_iter_next(&piter))) {
+ *   - block/genhd.c|1153| <<show_partition>> while ((part = disk_part_iter_next(&piter)))
+ *   - block/genhd.c|1477| <<diskstats_show>> while ((hd = disk_part_iter_next(&piter))) {
+ *   - block/genhd.c|1691| <<set_disk_ro>> while ((part = disk_part_iter_next(&piter)))
+ *   - block/ioctl.c|58| <<blkpg_ioctl>> while ((part = disk_part_iter_next(&piter))) {
+ *   - block/ioctl.c|132| <<blkpg_ioctl>> while ((lpart = disk_part_iter_next(&piter))) {
+ *   - block/partition-generic.c|502| <<blk_drop_partitions>> while ((part = disk_part_iter_next(&piter)))
+ *   - drivers/s390/block/dasd.c|445| <<dasd_state_ready_to_online>> while ((part = disk_part_iter_next(&piter)))
+ *   - drivers/s390/block/dasd.c|472| <<dasd_state_online_to_ready>> while ((part = disk_part_iter_next(&piter)))
+ */
 struct hd_struct *disk_part_iter_next(struct disk_part_iter *piter)
 {
 	struct disk_part_tbl *ptbl;
@@ -305,6 +360,14 @@ EXPORT_SYMBOL_GPL(disk_map_sector_rcu);
  * Can be deleted altogether. Later.
  *
  */
+/*
+ * 在以下使用major_names[]:
+ *   - block/genhd.c|327| <<blkdev_show>> for (dp = major_names[major_to_index(offset)]; dp; dp = dp->next)
+ *   - block/genhd.c|363| <<register_blkdev>> for (index = ARRAY_SIZE(major_names)-1; index > 0; index--) {
+ *   - block/genhd.c|364| <<register_blkdev>> if (major_names[index] == NULL)
+ *   - block/genhd.c|397| <<register_blkdev>> for (n = &major_names[index]; *n; n = &(*n)->next) {
+ *   - block/genhd.c|425| <<unregister_blkdev>> for (n = &major_names[index]; *n; n = &(*n)->next)
+ */
 #define BLKDEV_MAJOR_HASH_SIZE 255
 static struct blk_major_name {
 	struct blk_major_name *next;
@@ -351,6 +414,23 @@ void blkdev_show(struct seq_file *seqf, off_t offset)
  * See Documentation/admin-guide/devices.txt for the list of allocated
  * major numbers.
  */
+/*
+ * 部分调用register_blkdev()的例子:
+ *   - block/genhd.c|1105| <<genhd_device_init>> register_blkdev(BLOCK_EXT_MAJOR, "blkext");
+ *   - drivers/block/loop.c|2274| <<loop_init>> if (register_blkdev(LOOP_MAJOR, "loop")) {
+ *   - drivers/block/nbd.c|2372| <<nbd_init>> if (register_blkdev(NBD_MAJOR, "nbd"))
+ *   - drivers/block/null_blk_main.c|1845| <<null_init>> null_major = register_blkdev(0, "nullb");
+ *   - drivers/block/virtio_blk.c|1033| <<init>> major = register_blkdev(0, "virtblk");
+ *   - drivers/block/xen-blkfront.c|2718| <<xlblk_init>> if (register_blkdev(XENVBD_MAJOR, DEV_NAME)) {
+ *   - drivers/ide/ide-probe.c|1002| <<hwif_init>> if (register_blkdev(hwif->major, hwif->name))
+ *   - drivers/md/dm.c|235| <<local_init>> r = register_blkdev(_major, _name);
+ *   - drivers/md/md.c|9328| <<md_init>> if ((ret = register_blkdev(MD_MAJOR, "md")) < 0)
+ *   - drivers/md/md.c|9331| <<md_init>> if ((ret = register_blkdev(0, "mdp")) < 0)
+ *   - drivers/scsi/sd.c|3644| <<init_sd>> if (register_blkdev(sd_major(i), "sd") != 0)
+ *   - drivers/scsi/sr.c|1036| <<init_sr>> rc = register_blkdev(SCSI_CDROM_MAJOR, "sr");
+ *
+ * 核心思想是在major_names[]找到一个可用的major
+ */
 int register_blkdev(unsigned int major, const char *name)
 {
 	struct blk_major_name **n, *p;
@@ -923,11 +1003,31 @@ EXPORT_SYMBOL(get_gendisk);
  * RETURNS:
  * Resulting block_device on success, NULL on failure.
  */
+/*
+ * called by:
+ *   - block/genhd.c|684| <<register_disk>> bdev = bdget_disk(disk, 0);
+ *   - block/genhd.c|1655| <<invalidate_partition>> struct block_device *bdev = bdget_disk(disk, partno);
+ *   - drivers/block/nbd.c|302| <<nbd_size_update>> struct block_device *bdev = bdget_disk(nbd->disk, 0);
+ *   - drivers/block/nbd.c|1483| <<nbd_release>> struct block_device *bdev = bdget_disk(disk, 0);
+ *   - drivers/block/xen-blkfront.c|2145| <<blkfront_closing>> bdev = bdget_disk(info->gd, 0);
+ *   - drivers/block/xen-blkfront.c|2506| <<blkfront_remove>> bdev = bdget_disk(disk, 0);
+ *   - drivers/block/xen-blkfront.c|2588| <<blkif_release>> bdev = bdget_disk(disk, 0);
+ *   - drivers/md/dm.c|1984| <<alloc_dev>> md->bdev = bdget_disk(md->disk, 0);
+ *   - fs/block_dev.c|1167| <<bd_start_claiming>> whole = bdget_disk(disk, 0);
+ *   - fs/block_dev.c|1459| <<revalidate_disk>> struct block_device *bdev = bdget_disk(disk, 0);
+ *   - fs/block_dev.c|1640| <<__blkdev_get>> whole = bdget_disk(disk, 0);
+ */
 struct block_device *bdget_disk(struct gendisk *disk, int partno)
 {
 	struct hd_struct *part;
 	struct block_device *bdev = NULL;
 
+	/*
+	 * struct gendisk:
+	 *  -> struct disk_part_tbl __rcu *part_tbl;
+	 *  -> struct hd_struct part0;
+	 * 返回gendisk->part_tlb->part[partno]
+	 */
 	part = disk_get_part(disk, partno);
 	if (part)
 		bdev = bdget(part_devt(part));
@@ -1283,6 +1383,15 @@ static void disk_replace_part_tbl(struct gendisk *disk,
  * RETURNS:
  * 0 on success, -errno on failure.
  */
+/*
+ * called by:
+ *   - block/genhd.c|1517| <<__alloc_disk_node>> if (disk_expand_part_tbl(disk, 0)) {
+ *   - block/partition-generic.c|342| <<add_partition>> err = disk_expand_part_tbl(disk, partno);
+ *   - block/partition-generic.c|593| <<blk_add_partitions>> disk_expand_part_tbl(disk, highest);
+ *
+ * 核心思想就是gendisk->parttlb->part[partno]数组太短
+ * 需要扩展以下
+ */
 int disk_expand_part_tbl(struct gendisk *disk, int partno)
 {
 	struct disk_part_tbl *old_ptbl =
@@ -1465,6 +1574,10 @@ dev_t blk_lookup_devt(const char *name, int partno)
 }
 EXPORT_SYMBOL(blk_lookup_devt);
 
+/*
+ * called by:
+ *   - include/linux/genhd.h|682| <<alloc_disk_node>> __disk = __alloc_disk_node(minors, node_id); \
+ */
 struct gendisk *__alloc_disk_node(int minors, int node_id)
 {
 	struct gendisk *disk;
diff --git a/block/partition-generic.c b/block/partition-generic.c
index 564fae77711d..c4af34d61e89 100644
--- a/block/partition-generic.c
+++ b/block/partition-generic.c
@@ -223,6 +223,15 @@ static const struct attribute_group *part_attr_groups[] = {
 	NULL
 };
 
+/*
+ * [0] part_release
+ * [0] device_release
+ * [0] kobject_cleanup
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void part_release(struct device *dev)
 {
 	struct hd_struct *p = dev_to_part(dev);
@@ -259,6 +268,10 @@ static void delete_partition_work_fn(struct work_struct *work)
 	put_device(part_to_dev(part));
 }
 
+/*
+ * called by:
+ *   - include/linux/genhd.h|718| <<hd_ref_init>> if (percpu_ref_init(&part->ref, __delete_partition, 0,
+ */
 void __delete_partition(struct percpu_ref *ref)
 {
 	struct hd_struct *part = container_of(ref, struct hd_struct, ref);
@@ -270,6 +283,14 @@ void __delete_partition(struct percpu_ref *ref)
  * Must be called either with bd_mutex held, before a disk can be opened or
  * after all disk users are gone.
  */
+/*
+ * called by:
+ *   - block/genhd.c|858| <<del_gendisk>> delete_partition(disk, part->partno);
+ *   - block/ioctl.c|94| <<blkpg_ioctl>> delete_partition(disk, partno);
+ *   - block/partition-generic.c|503| <<blk_drop_partitions>> delete_partition(disk, part->partno);
+ *
+ * 针对数据结构来说,主要是清空gendisk->part_tlb->part[partno]为NULL
+ */
 void delete_partition(struct gendisk *disk, int partno)
 {
 	struct disk_part_tbl *ptbl =
@@ -309,12 +330,22 @@ static DEVICE_ATTR(whole_disk, 0444, whole_disk_show, NULL);
  * Must be called either with bd_mutex held, before a disk can be opened or
  * after all disk users are gone.
  */
+/*
+ * called by:
+ *   - block/ioctl.c|69| <<blkpg_ioctl>> part = add_partition(disk, partno, start, length,
+ *   - block/partition-generic.c|520| <<blk_add_partition>> part = add_partition(disk, p, from, size, state->parts[p].flags,
+ *
+ * 核心思想就是分配hd_struct, 设置其start_sect, nr_sects和partno等,
+ * 分配devt和调用device_add()
+ * 然如放入gendisk->parttlb->part[partno]
+ */
 struct hd_struct *add_partition(struct gendisk *disk, int partno,
 				sector_t start, sector_t len, int flags,
 				struct partition_meta_info *info)
 {
 	struct hd_struct *p;
 	dev_t devt = MKDEV(0, 0);
+	/* (&(disk)->part0.__dev) */
 	struct device *ddev = disk_to_dev(disk);
 	struct device *pdev;
 	struct disk_part_tbl *ptbl;
@@ -339,6 +370,10 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 		break;
 	}
 
+	/*
+	 * 核心思想就是gendisk->parttlb->part[partno]数组太短
+	 * 需要扩展以
+	 */
 	err = disk_expand_part_tbl(disk, partno);
 	if (err)
 		return ERR_PTR(err);
@@ -347,6 +382,7 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 	if (ptbl->part[partno])
 		return ERR_PTR(-EBUSY);
 
+	/* p是struct hd_struct */
 	p = kzalloc(sizeof(*p), GFP_KERNEL);
 	if (!p)
 		return ERR_PTR(-EBUSY);
@@ -357,6 +393,11 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 	}
 
 	seqcount_init(&p->nr_sects_seq);
+	/*
+	 * struct hd_struct:
+	 *  -> struct device __dev;
+	 * 返回hd_struct->__dev
+	 */
 	pdev = part_to_dev(p);
 
 	p->start_sect = start;
@@ -420,6 +461,7 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 	}
 
 	/* everything is up and running, commence */
+	/* 这一行是非常核心的 !!! */
 	rcu_assign_pointer(ptbl->part[partno], p);
 
 	/* suppress uevent if the disk suppresses it */
@@ -460,6 +502,10 @@ static bool disk_unlock_native_capacity(struct gendisk *disk)
 	}
 }
 
+/*
+ * called by:
+ *   - fs/block_dev.c|1528| <<bdev_disk_changed>> ret = blk_drop_partitions(disk, bdev);
+ */
 int blk_drop_partitions(struct gendisk *disk, struct block_device *bdev)
 {
 	struct disk_part_iter piter;
@@ -482,6 +528,14 @@ int blk_drop_partitions(struct gendisk *disk, struct block_device *bdev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/partition-generic.c|600| <<blk_add_partitions>> if (!blk_add_partition(disk, bdev, state, p))
+ *
+ * 核心思想就是分配hd_struct, 设置其start_sect, nr_sects和partno等,
+ * 分配devt和调用device_add()
+ * 然如放入gendisk->parttlb->part[partno]
+ */
 static bool blk_add_partition(struct gendisk *disk, struct block_device *bdev,
 		struct parsed_partitions *state, int p)
 {
@@ -517,6 +571,11 @@ static bool blk_add_partition(struct gendisk *disk, struct block_device *bdev,
 		size = get_capacity(disk) - from;
 	}
 
+	/*
+	 * 核心思想就是分配hd_struct, 设置其start_sect, nr_sects和partno等,
+	 * 分配devt和调用device_add()
+	 * 然如放入gendisk->parttlb->part[partno]
+	 */
 	part = add_partition(disk, p, from, size, state->parts[p].flags,
 			     &state->parts[p].info);
 	if (IS_ERR(part) && PTR_ERR(part) != -ENXIO) {
@@ -532,6 +591,40 @@ static bool blk_add_partition(struct gendisk *disk, struct block_device *bdev,
 	return true;
 }
 
+/*
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - fs/block_dev.c|1531| <<bdev_disk_changed>> ret = blk_add_partitions(disk, bdev);
+ */
 int blk_add_partitions(struct gendisk *disk, struct block_device *bdev)
 {
 	struct parsed_partitions *state;
@@ -602,6 +695,11 @@ int blk_add_partitions(struct gendisk *disk, struct block_device *bdev)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/partitions/check.h|38| <<read_part_sector>> return read_dev_sector(state->bdev, n, p);
+ *   - drivers/scsi/scsicam.c|42| <<scsi_bios_ptable>> void *data = read_dev_sector(bdev, 0, &sect);
+ */
 unsigned char *read_dev_sector(struct block_device *bdev, sector_t n, Sector *p)
 {
 	struct address_space *mapping = bdev->bd_inode->i_mapping;
diff --git a/block/partitions/check.c b/block/partitions/check.c
index ffe408fead0c..148b2cd90ce0 100644
--- a/block/partitions/check.c
+++ b/block/partitions/check.c
@@ -112,6 +112,10 @@ static int (*check_part[])(struct parsed_partitions *) = {
 	NULL
 };
 
+/*
+ * 针对gendisk分配一个struct parsed_partitions
+ * 主要要分配parsed_partitions->parts[]数组,用来临时保存part的元数据
+ */
 static struct parsed_partitions *allocate_partitions(struct gendisk *hd)
 {
 	struct parsed_partitions *state;
@@ -122,6 +126,16 @@ static struct parsed_partitions *allocate_partitions(struct gendisk *hd)
 		return NULL;
 
 	nr = disk_max_parts(hd);
+	/*
+	 * struct parsed_partitions:
+	 *  -> struct {
+	 *      -> sector_t from;
+	 *      -> sector_t size;
+	 *      -> int flags;
+	 *      -> bool has_info;
+	 *      -> struct partition_meta_info info;
+	 *     } *parts;
+	 */
 	state->parts = vzalloc(array_size(nr, sizeof(state->parts[0])));
 	if (!state->parts) {
 		kfree(state);
@@ -139,12 +153,77 @@ void free_partitions(struct parsed_partitions *state)
 	kfree(state);
 }
 
+/*
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] loop_reread_partitions
+ * [0] loop_set_status
+ * [0] loop_set_status64
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - block/partition-generic.c|584| <<blk_add_partitions>> state = check_partition(disk, bdev);
+ */
 struct parsed_partitions *
 check_partition(struct gendisk *hd, struct block_device *bdev)
 {
 	struct parsed_partitions *state;
 	int i, res, err;
 
+	/*
+	 * 针对gendisk分配一个struct parsed_partitions
+	 * 主要要分配parsed_partitions->parts[]数组,用来临时保存part的元数据
+	 */
 	state = allocate_partitions(hd);
 	if (!state)
 		return NULL;
@@ -164,6 +243,26 @@ check_partition(struct gendisk *hd, struct block_device *bdev)
 	i = res = err = 0;
 	while (!res && check_part[i]) {
 		memset(state->parts, 0, state->limit * sizeof(state->parts[0]));
+		/*
+		 * struct parsed_partitions {
+		 *     struct block_device *bdev;
+		 *     char name[BDEVNAME_SIZE];
+		 *     struct {
+		 *         sector_t from;
+		 *         sector_t size;
+		 *         int flags;
+		 *         bool has_info;
+		 *         struct partition_meta_info info;
+		 *     } *parts;
+		 *     int next;
+		 *     int limit;
+		 *     bool access_beyond_eod;
+		 *     char *pp_buf;
+		 * };
+		 *
+		 * 用virtio-blk在ubuntu的启动盘时, 是adfspart_check_POWERTEC()
+		 * 其他的也是这个, 只是其他的可能没有part, 所以res不是>0
+		 */
 		res = check_part[i++](state);
 		if (res < 0) {
 			/* We have hit an I/O error which we don't report now.
diff --git a/drivers/ata/libata-scsi.c b/drivers/ata/libata-scsi.c
index 58e09ffe8b9c..522921903e21 100644
--- a/drivers/ata/libata-scsi.c
+++ b/drivers/ata/libata-scsi.c
@@ -2017,6 +2017,9 @@ static int ata_scsi_translate(struct ata_device *dev, struct scsi_cmnd *cmd,
 	if (xlat_func(qc))
 		goto early_finish;
 
+	/*
+	 * 一个例子ata_std_qc_defer()
+	 */
 	if (ap->ops->qc_defer) {
 		if ((rc = ap->ops->qc_defer(qc)))
 			goto defer;
diff --git a/drivers/block/loop.c b/drivers/block/loop.c
index 739b372a5112..423bba5bcbc0 100644
--- a/drivers/block/loop.c
+++ b/drivers/block/loop.c
@@ -167,6 +167,12 @@ static loff_t get_loop_size(struct loop_device *lo, struct file *file)
 	return get_size(lo->lo_offset, lo->lo_sizelimit, file);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|634| <<loop_update_dio>> __loop_update_dio(lo, io_is_direct(lo->lo_backing_file) |
+ *   - drivers/block/loop.c|1356| <<loop_set_status>> __loop_update_dio(lo, lo->use_dio);
+ *   - drivers/block/loop.c|1539| <<loop_set_dio>> __loop_update_dio(lo, !!arg);
+ */
 static void __loop_update_dio(struct loop_device *lo, bool dio)
 {
 	struct file *file = lo->lo_backing_file;
@@ -453,6 +459,9 @@ static int lo_req_flush(struct loop_device *lo, struct request *rq)
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops loop_mq_ops.complete = lo_complete_rq()
+ */
 static void lo_complete_rq(struct request *rq)
 {
 	struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
@@ -599,6 +608,19 @@ static int do_req_filebacked(struct loop_device *lo, struct request *rq)
 	case REQ_OP_FLUSH:
 		return lo_req_flush(lo, rq);
 	case REQ_OP_WRITE_ZEROES:
+		/*
+		 * FALLOC_FL_ZERO_RANGE的注释:
+		 * FALLOC_FL_ZERO_RANGE is used to convert a range of file to zeros preferably
+		 * without issuing data IO. Blocks should be preallocated for the regions that
+		 * span holes in the file, and the entire range is preferable converted to
+		 * unwritten extents - even though file system may choose to zero out the
+		 * extent or do whatever which will result in reading zeros from the range
+		 * while the range remains allocated for the file.
+		 *
+		 * This can be also used to preallocate blocks past EOF in the same way as
+		 * with fallocate. Flag FALLOC_FL_KEEP_SIZE should cause the inode
+		 * size to remain the same
+		 */
 		/*
 		 * If the caller doesn't want deallocation, call zeroout to
 		 * write zeroes the range.  Otherwise, punch them out.
@@ -635,6 +657,21 @@ static inline void loop_update_dio(struct loop_device *lo)
 			lo->use_dio);
 }
 
+/*
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] loop_reread_partitions
+ * [0] loop_set_status
+ * [0] loop_set_status64
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void loop_reread_partitions(struct loop_device *lo,
 				   struct block_device *bdev)
 {
@@ -1205,6 +1242,10 @@ static int __loop_clr_fd(struct loop_device *lo, bool release)
 	return err;
 }
 
+/*
+ * called by (处理LOOP_CLR_FD):
+ *   - drivers/block/loop.c|1621| <<lo_ioctl>> return loop_clr_fd(lo);
+ */
 static int loop_clr_fd(struct loop_device *lo)
 {
 	int err;
@@ -1237,6 +1278,12 @@ static int loop_clr_fd(struct loop_device *lo)
 	return __loop_clr_fd(lo, false);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|1491| <<loop_set_status_old>> return loop_set_status(lo, &info64);
+ *   - drivers/block/loop.c|1501| <<loop_set_status64>> return loop_set_status(lo, &info64);
+ *   - drivers/block/loop.c|1771| <<loop_set_status_compat>> return loop_set_status(lo, &info64);
+ */
 static int
 loop_set_status(struct loop_device *lo, const struct loop_info64 *info)
 {
@@ -1515,6 +1562,10 @@ static int loop_set_capacity(struct loop_device *lo)
 	return figure_loop_size(lo, lo->lo_offset, lo->lo_sizelimit);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|1597| <<lo_simple_ioctl>> err = loop_set_dio(lo, arg);
+ */
 static int loop_set_dio(struct loop_device *lo, unsigned long arg)
 {
 	int error = -ENXIO;
@@ -1591,6 +1642,9 @@ static int lo_simple_ioctl(struct loop_device *lo, unsigned int cmd,
 	return err;
 }
 
+/*
+ * struct block_device_operations lo_fops.ioctl = lo_ioctl()
+ */
 static int lo_ioctl(struct block_device *bdev, fmode_t mode,
 	unsigned int cmd, unsigned long arg)
 {
@@ -1754,6 +1808,9 @@ loop_get_status_compat(struct loop_device *lo,
 	return err;
 }
 
+/*
+ * struct block_device_operations lo_fops.compat_ioctl = lo_compat_ioctl()
+ */
 static int lo_compat_ioctl(struct block_device *bdev, fmode_t mode,
 			   unsigned int cmd, unsigned long arg)
 {
@@ -1789,6 +1846,9 @@ static int lo_compat_ioctl(struct block_device *bdev, fmode_t mode,
 }
 #endif
 
+/*
+ * struct block_device_operations lo_fops.open = lo_open()
+ */
 static int lo_open(struct block_device *bdev, fmode_t mode)
 {
 	struct loop_device *lo;
@@ -1809,6 +1869,9 @@ static int lo_open(struct block_device *bdev, fmode_t mode)
 	return err;
 }
 
+/*
+ * struct block_device_operations lo_fops.release = lo_release()
+ */
 static void lo_release(struct gendisk *disk, fmode_t mode)
 {
 	struct loop_device *lo;
@@ -1842,6 +1905,10 @@ static void lo_release(struct gendisk *disk, fmode_t mode)
 	mutex_unlock(&loop_ctl_mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2085| <<loop_add>> disk->fops = &lo_fops;
+ */
 static const struct block_device_operations lo_fops = {
 	.owner =	THIS_MODULE,
 	.open =		lo_open,
@@ -1863,6 +1930,10 @@ MODULE_PARM_DESC(max_part, "Maximum number of partitions per loop device");
 MODULE_LICENSE("GPL");
 MODULE_ALIAS_BLOCKDEV_MAJOR(LOOP_MAJOR);
 
+/*
+ * called by:
+ *   - drivers/block/cryptoloop.c|188| <<init_cryptoloop>> int rc = loop_register_transfer(&cryptoloop_funcs);
+ */
 int loop_register_transfer(struct loop_func_table *funcs)
 {
 	unsigned int n = funcs->number;
@@ -1901,6 +1972,9 @@ int loop_unregister_transfer(int number)
 EXPORT_SYMBOL(loop_register_transfer);
 EXPORT_SYMBOL(loop_unregister_transfer);
 
+/*
+ * struct blk_mq_ops loop_mq_ops.queue_rq = loop_queue_rq()
+ */
 static blk_status_t loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 		const struct blk_mq_queue_data *bd)
 {
@@ -1937,6 +2011,10 @@ static blk_status_t loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2011| <<loop_queue_work>> loop_handle_cmd(cmd);
+ */
 static void loop_handle_cmd(struct loop_cmd *cmd)
 {
 	struct request *rq = blk_mq_rq_from_pdu(cmd);
@@ -1958,6 +2036,10 @@ static void loop_handle_cmd(struct loop_cmd *cmd)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2019| <<loop_init_request>> kthread_init_work(&cmd->work, loop_queue_work);
+ */
 static void loop_queue_work(struct kthread_work *work)
 {
 	struct loop_cmd *cmd =
@@ -1966,6 +2048,9 @@ static void loop_queue_work(struct kthread_work *work)
 	loop_handle_cmd(cmd);
 }
 
+/*
+ * struct blk_mq_ops loop_mq_ops.init_request = loop_init_request()
+ */
 static int loop_init_request(struct blk_mq_tag_set *set, struct request *rq,
 		unsigned int hctx_idx, unsigned int numa_node)
 {
@@ -1975,12 +2060,23 @@ static int loop_init_request(struct blk_mq_tag_set *set, struct request *rq,
 	return 0;
 }
 
+/*
+ * used by:
+ *   - drivers/block/loop.c|2025| <<loop_add>> lo->tag_set.ops = &loop_mq_ops;
+ */
 static const struct blk_mq_ops loop_mq_ops = {
 	.queue_rq       = loop_queue_rq,
 	.init_request	= loop_init_request,
 	.complete	= lo_complete_rq,
 };
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2161| <<loop_probe>> err = loop_add(&lo, MINOR(dev) >> part_shift);
+ *   - drivers/block/loop.c|2190| <<loop_control_ioctl>> ret = loop_add(&lo, parm);
+ *   - drivers/block/loop.c|2212| <<loop_control_ioctl>> ret = loop_add(&lo, -1);
+ *   - drivers/block/loop.c|2300| <<loop_init>> loop_add(&lo, i);
+ */
 static int loop_add(struct loop_device **l, int i)
 {
 	struct loop_device *lo;
@@ -2154,6 +2250,10 @@ static struct kobject *loop_probe(dev_t dev, int *part, void *data)
 	return kobj;
 }
 
+/*
+ * struct file_operations loop_ctl_fops.unlocked_ioctl = loop_control_ioctl()
+ * struct file_operations loop_ctl_fops.compat_ioctl = loop_control_ioctl()
+ */
 static long loop_control_ioctl(struct file *file, unsigned int cmd,
 			       unsigned long parm)
 {
diff --git a/drivers/block/loop.h b/drivers/block/loop.h
index af75a5ee4094..d68889eac7ea 100644
--- a/drivers/block/loop.h
+++ b/drivers/block/loop.h
@@ -20,6 +20,21 @@
 /* Possible states of device */
 enum {
 	Lo_unbound,
+	/*
+	 * 使用Lo_bound的地方:
+	 *   - drivers/block/loop.c|692| <<loop_validate_file>> if (l->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|721| <<loop_change_fd>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1051| <<loop_set_fd>> lo->lo_state = Lo_bound;
+	 *   - drivers/block/loop.c|1240| <<loop_clr_fd>> if (lo->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|1283| <<loop_set_status>> if (lo->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|1396| <<loop_get_status>> if (lo->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|1537| <<loop_set_capacity>> if (unlikely(lo->lo_state != Lo_bound))
+	 *   - drivers/block/loop.c|1550| <<loop_set_dio>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1565| <<loop_set_block_size>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1863| <<lo_release>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1873| <<lo_release>> } else if (lo->lo_state == Lo_bound) {
+	 *   - drivers/block/loop.c|1958| <<loop_queue_rq>> if (lo->lo_state != Lo_bound)
+	 */
 	Lo_bound,
 	Lo_rundown,
 };
diff --git a/drivers/block/nbd.c b/drivers/block/nbd.c
index b4607dd96185..d514bdcfc4bb 100644
--- a/drivers/block/nbd.c
+++ b/drivers/block/nbd.c
@@ -47,8 +47,32 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/nbd.h>
 
+/*
+ * 在以下使用nbd_index_idr:
+ *   - drivers/block/nbd.c|401| <<nbd_put>> idr_remove(&nbd_index_idr, nbd->index);
+ *   - drivers/block/nbd.c|1870| <<nbd_dev_add>> err = idr_alloc(&nbd_index_idr, nbd, index, index + 1,
+ *   - drivers/block/nbd.c|1875| <<nbd_dev_add>> err = idr_alloc(&nbd_index_idr, nbd, 0, 0, GFP_KERNEL);
+ *   - drivers/block/nbd.c|1949| <<nbd_dev_add>> idr_remove(&nbd_index_idr, index);
+ *   - drivers/block/nbd.c|2048| <<nbd_genl_connect>> ret = idr_for_each(&nbd_index_idr, &find_free_cb, &nbd);
+ *   - drivers/block/nbd.c|2057| <<nbd_genl_connect>> nbd = idr_find(&nbd_index_idr, new_index);
+ *   - drivers/block/nbd.c|2060| <<nbd_genl_connect>> nbd = idr_find(&nbd_index_idr, index);
+ *   - drivers/block/nbd.c|2068| <<nbd_genl_connect>> nbd = idr_find(&nbd_index_idr, index);
+ *   - drivers/block/nbd.c|2228| <<nbd_genl_disconnect>> nbd = idr_find(&nbd_index_idr, index);
+ *   - drivers/block/nbd.c|2269| <<nbd_genl_reconfigure>> nbd = idr_find(&nbd_index_idr, index);
+ *   - drivers/block/nbd.c|2504| <<nbd_genl_status>> ret = idr_for_each(&nbd_index_idr, &status_cb, reply);
+ *   - drivers/block/nbd.c|2511| <<nbd_genl_status>> nbd = idr_find(&nbd_index_idr, index);
+ *   - drivers/block/nbd.c|2676| <<nbd_cleanup>> idr_for_each(&nbd_index_idr, &nbd_exit_cb, &del_list);
+ *   - drivers/block/nbd.c|2699| <<nbd_cleanup>> idr_destroy(&nbd_index_idr);
+ *
+ * 这个还有些tag的bitmap的意思
+ */
 static DEFINE_IDR(nbd_index_idr);
 static DEFINE_MUTEX(nbd_index_mutex);
+/*
+ * 在以下使用nbd_total_devices:
+ *   - drivers/block/nbd.c|1823| <<nbd_dev_add>> nbd_total_devices++;
+ *   - drivers/block/nbd.c|2356| <<nbd_genl_status>> msg_size *= (index == -1) ? nbd_total_devices : 1;
+ */
 static int nbd_total_devices = 0;
 
 struct nbd_sock {
@@ -72,16 +96,83 @@ struct link_dead_args {
 	int index;
 };
 
+/*
+ * 在以下使用NBD_RT_TIMEDOUT:
+ *   - drivers/block/nbd.c|566| <<nbd_xmit_timeout>> set_bit(NBD_RT_TIMEDOUT, &config->runtime_flags);
+ *   - drivers/block/nbd.c|1433| <<nbd_start_device_ioctl>> if (test_bit(NBD_RT_TIMEDOUT, &config->runtime_flags))
+ */
 #define NBD_RT_TIMEDOUT			0
+/*
+ * 在以下使用NBD_RT_DISCONNECT_REQUESTED:
+ *   - drivers/block/nbd.c|516| <<nbd_disconnected>> test_bit(NBD_RT_DISCONNECT_REQUESTED, &config->runtime_flags);
+ *   - drivers/block/nbd.c|534| <<nbd_mark_nsock_dead>> if (test_and_clear_bit(NBD_RT_DISCONNECT_REQUESTED,
+ *   - drivers/block/nbd.c|1498| <<nbd_disconnect>> set_bit(NBD_RT_DISCONNECT_REQUESTED, &config->runtime_flags);
+ *   - drivers/block/nbd.c|1646| <<nbd_start_device_ioctl>> if (test_bit(NBD_RT_DISCONNECT_REQUESTED, &config->runtime_flags))
+ */
 #define NBD_RT_DISCONNECT_REQUESTED	1
+/*
+ * 在以下使用NBD_RT_DISCONNECTED:
+ *   - drivers/block/nbd.c|515| <<nbd_disconnected>> return test_bit(NBD_RT_DISCONNECTED, &config->runtime_flags) ||
+ *   - drivers/block/nbd.c|536| <<nbd_mark_nsock_dead>> set_bit(NBD_RT_DISCONNECTED,
+ *   - drivers/block/nbd.c|610| <<sock_shutdown>> if (test_and_set_bit(NBD_RT_DISCONNECTED, &config->runtime_flags))
+ *   - drivers/block/nbd.c|1128| <<find_fallback>> if (test_bit(NBD_RT_DISCONNECTED, &config->runtime_flags)
+ *   - drivers/block/nbd.c|1169| <<wait_for_reconnect>> if (test_bit(NBD_RT_DISCONNECTED, &config->runtime_flags))
+ *   - drivers/block/nbd.c|1427| <<nbd_reconnect_socket>> clear_bit(NBD_RT_DISCONNECTED, &config->runtime_flags);
+ */
 #define NBD_RT_DISCONNECTED		2
+/*
+ * 在以下使用NBD_RT_HAS_PID_FILE:
+ *   - drivers/block/nbd.c|1523| <<nbd_config_put>> if (test_and_clear_bit(NBD_RT_HAS_PID_FILE,
+ *   - drivers/block/nbd.c|1589| <<nbd_start_device>> set_bit(NBD_RT_HAS_PID_FILE, &config->runtime_flags);
+ */
 #define NBD_RT_HAS_PID_FILE		3
+/*
+ * 在以下使用NBD_RT_HAS_CONFIG_REF:
+ *   - drivers/block/nbd.c|1659| <<nbd_clear_sock_ioctl>> if (test_and_clear_bit(NBD_RT_HAS_CONFIG_REF,
+ *   - drivers/block/nbd.c|2385| <<nbd_genl_connect>> set_bit(NBD_RT_HAS_CONFIG_REF, &config->runtime_flags);
+ *   - drivers/block/nbd.c|2412| <<nbd_disconnect_and_put>> if (test_and_clear_bit(NBD_RT_HAS_CONFIG_REF,
+ */
 #define NBD_RT_HAS_CONFIG_REF		4
+/*
+ * 在以下使用NBD_RT_BOUND:
+ *   - drivers/block/nbd.c|1369| <<nbd_add_socket>> !test_bit(NBD_RT_BOUND, &config->runtime_flags))
+ *   - drivers/block/nbd.c|1374| <<nbd_add_socket>> test_bit(NBD_RT_BOUND, &config->runtime_flags))) {
+ *   - drivers/block/nbd.c|1785| <<nbd_ioctl>> if (!test_bit(NBD_RT_BOUND, &config->runtime_flags) ||
+ *   - drivers/block/nbd.c|2345| <<nbd_genl_connect>> set_bit(NBD_RT_BOUND, &config->runtime_flags);
+ *   - drivers/block/nbd.c|2529| <<nbd_genl_reconfigure>> if (!test_bit(NBD_RT_BOUND, &config->runtime_flags) ||
+ */
 #define NBD_RT_BOUND			5
+/*
+ * 在以下使用NBD_RT_DESTROY_ON_DISCONNECT:
+ *   - drivers/block/nbd.c|2050| <<nbd_genl_connect>> set_bit(NBD_RT_DESTROY_ON_DISCONNECT,
+ *   - drivers/block/nbd.c|2226| <<nbd_genl_reconfigure>> if (!test_and_set_bit(NBD_RT_DESTROY_ON_DISCONNECT,
+ *   - drivers/block/nbd.c|2231| <<nbd_genl_reconfigure>> if (test_and_clear_bit(NBD_RT_DESTROY_ON_DISCONNECT,
+ */
 #define NBD_RT_DESTROY_ON_DISCONNECT	6
+/*
+ * 在以下使用NBD_RT_DISCONNECT_ON_CLOSE:
+ *   - drivers/block/nbd.c|1591| <<nbd_release>> if (test_bit(NBD_RT_DISCONNECT_ON_CLOSE, &nbd->config->runtime_flags) &&
+ *   - drivers/block/nbd.c|2039| <<nbd_genl_connect>> set_bit(NBD_RT_DISCONNECT_ON_CLOSE,
+ *   - drivers/block/nbd.c|2219| <<nbd_genl_reconfigure>> set_bit(NBD_RT_DISCONNECT_ON_CLOSE,
+ *   - drivers/block/nbd.c|2222| <<nbd_genl_reconfigure>> clear_bit(NBD_RT_DISCONNECT_ON_CLOSE,
+ */
 #define NBD_RT_DISCONNECT_ON_CLOSE	7
 
+/*
+ * 在以下使用NBD_DESTROY_ON_DISCONNECT:
+ *   - drivers/block/nbd.c|381| <<nbd_dev_remove>> if (test_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags) && nbd->destroy_complete)
+ *   - drivers/block/nbd.c|2103| <<nbd_genl_connect>> if (test_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags) &&
+ *   - drivers/block/nbd.c|2167| <<nbd_genl_connect>> set_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags);
+ *   - drivers/block/nbd.c|2170| <<nbd_genl_connect>> clear_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags);
+ *   - drivers/block/nbd.c|2344| <<nbd_genl_reconfigure>> set_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags);
+ *   - drivers/block/nbd.c|2349| <<nbd_genl_reconfigure>> clear_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags);
+ */
 #define NBD_DESTROY_ON_DISCONNECT	0
+/*
+ * 在以下使用NBD_DISCONNECT_REQUESTED:
+ *   - drivers/block/nbd.c|1390| <<nbd_disconnect>> set_bit(NBD_DISCONNECT_REQUESTED, &nbd->flags);
+ *   - drivers/block/nbd.c|2104| <<nbd_genl_connect>> test_bit(NBD_DISCONNECT_REQUESTED, &nbd->flags)) {
+ */
 #define NBD_DISCONNECT_REQUESTED	1
 
 struct nbd_config {
@@ -91,6 +182,15 @@ struct nbd_config {
 
 	struct nbd_sock **socks;
 	int num_connections;
+	/*
+	 * 在以下使用live_connections:
+	 *   - drivers/block/nbd.c|568| <<nbd_mark_nsock_dead>> if (atomic_dec_return(&nbd->config->live_connections) == 0) {
+	 *   - drivers/block/nbd.c|712| <<nbd_xmit_timeout>> atomic_read(&config->live_connections),
+	 *   - drivers/block/nbd.c|1207| <<wait_for_reconnect>> atomic_read(&config->live_connections) > 0,
+	 *   - drivers/block/nbd.c|1412| <<nbd_add_socket>> atomic_inc(&config->live_connections);
+	 *   - drivers/block/nbd.c|1469| <<nbd_reconnect_socket>> atomic_inc(&config->live_connections);
+	 *   - drivers/block/nbd.c|1813| <<nbd_alloc_config>> atomic_set(&config->live_connections, 0);
+	 */
 	atomic_t live_connections;
 	wait_queue_head_t conn_wait;
 
@@ -99,6 +199,11 @@ struct nbd_config {
 	loff_t blksize;
 	loff_t bytesize;
 #if IS_ENABLED(CONFIG_DEBUG_FS)
+	/*
+	 * 在以下使用nbd_config->dbg_dir:
+	 *   - drivers/block/nbd.c|2003| <<nbd_dev_dbg_init>> config->dbg_dir = dir;
+	 *   - drivers/block/nbd.c|2016| <<nbd_dev_dbg_close>> debugfs_remove_recursive(nbd->config->dbg_dir);
+	 */
 	struct dentry *dbg_dir;
 #endif
 };
@@ -107,7 +212,38 @@ struct nbd_device {
 	struct blk_mq_tag_set tag_set;
 
 	int index;
+	/*
+	 * 在以下使用nbd_device->config_refs:
+	 *   - drivers/block/nbd.c|391| <<nbd_xmit_timeout>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|879| <<nbd_handle_cmd>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1090| <<nbd_reconnect_socket>> refcount_inc(&nbd->config_refs);
+	 *   - drivers/block/nbd.c|1188| <<nbd_config_put>> if (refcount_dec_and_mutex_lock(&nbd->config_refs,
+	 *   - drivers/block/nbd.c|1275| <<nbd_start_device>> refcount_inc(&nbd->config_refs);
+	 *   - drivers/block/nbd.c|1454| <<nbd_open>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1458| <<nbd_open>> if (refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1468| <<nbd_open>> refcount_set(&nbd->config_refs, 1);
+	 *   - drivers/block/nbd.c|1713| <<nbd_dev_add>> refcount_set(&nbd->config_refs, 0);
+	 *   - drivers/block/nbd.c|1742| <<find_free_cb>> if (!refcount_read(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1875| <<nbd_genl_connect>> if (refcount_read(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1895| <<nbd_genl_connect>> refcount_set(&nbd->config_refs, 1);
+	 *   - drivers/block/nbd.c|1964| <<nbd_genl_connect>> refcount_inc(&nbd->config_refs);
+	 *   - drivers/block/nbd.c|2018| <<nbd_genl_disconnect>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|2060| <<nbd_genl_reconfigure>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|2208| <<populate_nbd_status>> if (refcount_read(&nbd->config_refs))
+	 */
 	refcount_t config_refs;
+	/*
+	 * 在以下使用nbd_device->refs:
+	 *   - drivers/block/nbd.c|294| <<nbd_put>> if (refcount_dec_and_mutex_lock(&nbd->refs,
+	 *   - drivers/block/nbd.c|1520| <<nbd_open>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|1539| <<nbd_open>> refcount_inc(&nbd->refs);
+	 *   - drivers/block/nbd.c|1784| <<nbd_dev_add>> refcount_set(&nbd->refs, 1);
+	 *   - drivers/block/nbd.c|1934| <<nbd_genl_connect>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|2081| <<nbd_genl_disconnect>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|2122| <<nbd_genl_reconfigure>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|2169| <<nbd_genl_reconfigure>> refcount_inc(&nbd->refs);
+	 *   - drivers/block/nbd.c|2510| <<nbd_cleanup>> if (refcount_read(&nbd->refs) != 1)
+	 */
 	refcount_t refs;
 	struct nbd_config *config;
 	struct mutex config_lock;
@@ -118,10 +254,27 @@ struct nbd_device {
 	struct task_struct *task_recv;
 	struct task_struct *task_setup;
 
+	/*
+	 * 在以下使用nbd_device->destroy_complete:
+	 *   - drivers/block/nbd.c|314| <<nbd_dev_remove>> if (test_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags) && nbd->destroy_complete)
+	 *   - drivers/block/nbd.c|315| <<nbd_dev_remove>> complete(nbd->destroy_complete);
+	 *   - drivers/block/nbd.c|1817| <<nbd_dev_add>> nbd->destroy_complete = NULL;
+	 *   - drivers/block/nbd.c|1935| <<nbd_genl_connect>> DECLARE_COMPLETION_ONSTACK(destroy_complete);
+	 *   - drivers/block/nbd.c|1990| <<nbd_genl_connect>> nbd->destroy_complete = &destroy_complete;
+	 *   - drivers/block/nbd.c|1994| <<nbd_genl_connect>> wait_for_completion(&destroy_complete);
+	 */
 	struct completion *destroy_complete;
 	unsigned long flags;
 };
 
+/*
+ * 在以下使用NBD_CMD_REQUEUED:
+ *   - drivers/block/nbd.c|239| <<nbd_requeue_cmd>> if (!test_and_set_bit(NBD_CMD_REQUEUED, &cmd->flags))
+ *   - drivers/block/nbd.c|714| <<nbd_send_cmd>> set_bit(NBD_CMD_REQUEUED, &cmd->flags);
+ *   - drivers/block/nbd.c|755| <<nbd_send_cmd>> set_bit(NBD_CMD_REQUEUED, &cmd->flags);
+ *   - drivers/block/nbd.c|839| <<nbd_read_stat>> if (test_bit(NBD_CMD_REQUEUED, &cmd->flags)) {
+ *   - drivers/block/nbd.c|1091| <<nbd_queue_rq>> clear_bit(NBD_CMD_REQUEUED, &cmd->flags);
+ */
 #define NBD_CMD_REQUEUED	1
 
 struct nbd_cmd {
@@ -132,21 +285,79 @@ struct nbd_cmd {
 	int retries;
 	blk_status_t status;
 	unsigned long flags;
+	/*
+	 * 在以下使用nbd_cmd->cmd_cookie:
+	 *   - drivers/block/nbd.c|330| <<nbd_cmd_handle>> u64 cookie = cmd->cmd_cookie;
+	 *   - drivers/block/nbd.c|778| <<nbd_send_cmd>> cmd->cmd_cookie++;
+	 *   - drivers/block/nbd.c|927| <<nbd_read_stat>> if (cmd->cmd_cookie != nbd_handle_to_cookie(handle)) {
+	 *   - drivers/block/nbd.c|929| <<nbd_read_stat>> req, cmd->cmd_cookie, nbd_handle_to_cookie(handle));
+	 */
 	u32 cmd_cookie;
 };
 
 #if IS_ENABLED(CONFIG_DEBUG_FS)
+/*
+ * 在以下使用nbd_dbg_dir:
+ *   - drivers/block/nbd.c|1983| <<nbd_dev_dbg_init>> if (!nbd_dbg_dir)
+ *   - drivers/block/nbd.c|1986| <<nbd_dev_dbg_init>> dir = debugfs_create_dir(nbd_name(nbd), nbd_dbg_dir);
+ *   - drivers/block/nbd.c|2020| <<nbd_dbg_init>> nbd_dbg_dir = dbg_dir;
+ *   - drivers/block/nbd.c|2027| <<nbd_dbg_close>> debugfs_remove_recursive(nbd_dbg_dir);
+ */
 static struct dentry *nbd_dbg_dir;
 #endif
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1851| <<nbd_dev_dbg_init>> dir = debugfs_create_dir(nbd_name(nbd), nbd_dbg_dir);
+ *   - drivers/block/nbd.c|1854| <<nbd_dev_dbg_init>> nbd_name(nbd));
+ */
 #define nbd_name(nbd) ((nbd)->disk->disk_name)
 
 #define NBD_MAGIC 0x68797548
 
+/*
+ * 在以下使用NBD_DEF_BLKSIZE:
+ *   - drivers/block/nbd.c|1623| <<__nbd_ioctl>> arg = NBD_DEF_BLKSIZE;
+ *   - drivers/block/nbd.c|1705| <<nbd_alloc_config>> config->blksize = NBD_DEF_BLKSIZE;
+ *   - drivers/block/nbd.c|2116| <<nbd_genl_size_set>> bsize = NBD_DEF_BLKSIZE;
+ */
 #define NBD_DEF_BLKSIZE 1024
 
+/*
+ * 在以下使用nbds_max:
+ *   - drivers/block/nbd.c|2470| <<global>> module_param(nbds_max, int , 0444);
+ *   - drivers/block/nbd.c|2471| <<global>> MODULE_PARM_DESC(nbds_max, "number of network block devices to initialize (default: 16)");
+ *   - drivers/block/nbd.c|2412| <<nbd_init>> if (nbds_max > 1UL << (MINORBITS - part_shift))
+ *   - drivers/block/nbd.c|2425| <<nbd_init>> for (i = 0; i < nbds_max; i++)
+ *
+ * number of network block devices to initialize (default: 16)
+ */
 static unsigned int nbds_max = 16;
+/*
+ * 在以下使用max_paert:
+ *   - drivers/block/nbd.c|2472| <<global>> module_param(max_part, int , 0444);
+ *   - drivers/block/nbd.c|2473| <<global>> MODULE_PARM_DESC(max_part, "number of partitions per device (default: 16)");
+ *   - drivers/block/nbd.c|1337| <<nbd_start_device_ioctl>> if (max_part)
+ *   - drivers/block/nbd.c|2389| <<nbd_init>> if (max_part < 0) {
+ *   - drivers/block/nbd.c|2395| <<nbd_init>> if (max_part > 0) {
+ *   - drivers/block/nbd.c|2396| <<nbd_init>> part_shift = fls(max_part);
+ *   - drivers/block/nbd.c|2406| <<nbd_init>> max_part = (1UL << part_shift) - 1;
+ *
+ * number of partitions per device (default: 16)
+ */
 static int max_part = 16;
+/*
+ * 在以下使用part_shift:
+ *   - drivers/block/nbd.c|1719| <<nbd_dev_add>> disk = alloc_disk(1 << part_shift);
+ *   - drivers/block/nbd.c|1777| <<nbd_dev_add>> disk->first_minor = index << part_shift;
+ *   - drivers/block/nbd.c|2411| <<nbd_init>> part_shift = 0;
+ *   - drivers/block/nbd.c|2413| <<nbd_init>> part_shift = fls(max_part);
+ *   - drivers/block/nbd.c|2423| <<nbd_init>> max_part = (1UL << part_shift) - 1;
+ *   - drivers/block/nbd.c|2426| <<nbd_init>> if ((1UL << part_shift) > DISK_MAX_PARTS)
+ *   - drivers/block/nbd.c|2429| <<nbd_init>> if (nbds_max > 1UL << (MINORBITS - part_shift))
+ *
+ * 在alloc_disk()的时候会分配给disk->minors
+ */
 static int part_shift;
 
 static int nbd_dev_dbg_init(struct nbd_device *nbd);
@@ -157,40 +368,92 @@ static int nbd_genl_status(struct sk_buff *skb, struct genl_info *info);
 static void nbd_dead_link_work(struct work_struct *work);
 static void nbd_disconnect_and_put(struct nbd_device *nbd);
 
+/*
+ * struct nbd_device:
+ *   -> struct gendisk *disk;
+ */
 static inline struct device *nbd_to_dev(struct nbd_device *nbd)
 {
+	/*
+	 * struct nbd_device:
+	 *   -> struct gendisk *disk;
+	 */
 	return disk_to_dev(nbd->disk);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|676| <<nbd_send_cmd>> handle = nbd_cmd_handle(cmd);
+ *   - drivers/block/nbd.c|692| <<nbd_send_cmd>> handle = nbd_cmd_handle(cmd);
+ */
 static void nbd_requeue_cmd(struct nbd_cmd *cmd)
 {
 	struct request *req = blk_mq_rq_from_pdu(cmd);
 
+	/*
+	 * 在以下使用NBD_CMD_REQUEUED:
+	 *   - drivers/block/nbd.c|239| <<nbd_requeue_cmd>> if (!test_and_set_bit(NBD_CMD_REQUEUED, &cmd->flags))
+	 *   - drivers/block/nbd.c|714| <<nbd_send_cmd>> set_bit(NBD_CMD_REQUEUED, &cmd->flags);
+	 *   - drivers/block/nbd.c|755| <<nbd_send_cmd>> set_bit(NBD_CMD_REQUEUED, &cmd->flags);
+	 *   - drivers/block/nbd.c|839| <<nbd_read_stat>> if (test_bit(NBD_CMD_REQUEUED, &cmd->flags)) {
+	 *   - drivers/block/nbd.c|1091| <<nbd_queue_rq>> clear_bit(NBD_CMD_REQUEUED, &cmd->flags);
+	 */
 	if (!test_and_set_bit(NBD_CMD_REQUEUED, &cmd->flags))
 		blk_mq_requeue_request(req, true);
 }
 
 #define NBD_COOKIE_BITS 32
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|681| <<nbd_send_cmd>> handle = nbd_cmd_handle(cmd);
+ *   - drivers/block/nbd.c|697| <<nbd_send_cmd>> handle = nbd_cmd_handle(cmd);
+ *
+ * handle的高32位是cookie, 低32位是unique tag
+ */
 static u64 nbd_cmd_handle(struct nbd_cmd *cmd)
 {
 	struct request *req = blk_mq_rq_from_pdu(cmd);
+	/*
+	 * The tag field in struct request is unique per hardware queue but not over
+	 * all hardware queues. Hence this function that returns a tag with the
+	 * hardware context index in the upper bits and the per hardware queue tag in
+	 * the lower bits.
+	 */
 	u32 tag = blk_mq_unique_tag(req);
+	/*
+	 * 在以下使用nbd_cmd->cmd_cookie:
+	 *   - drivers/block/nbd.c|330| <<nbd_cmd_handle>> u64 cookie = cmd->cmd_cookie;
+	 *   - drivers/block/nbd.c|778| <<nbd_send_cmd>> cmd->cmd_cookie++;
+	 *   - drivers/block/nbd.c|927| <<nbd_read_stat>> if (cmd->cmd_cookie != nbd_handle_to_cookie(handle)) {
+	 *   - drivers/block/nbd.c|929| <<nbd_read_stat>> req, cmd->cmd_cookie, nbd_handle_to_cookie(handle));
+	 */
 	u64 cookie = cmd->cmd_cookie;
 
 	return (cookie << NBD_COOKIE_BITS) | tag;
 }
 
+/*
+ * handle的高32位是cookie, 低32位是unique tag
+ */
 static u32 nbd_handle_to_tag(u64 handle)
 {
 	return (u32)handle;
 }
 
+/*
+ * handle的高32位是cookie, 低32位是unique tag
+ */
 static u32 nbd_handle_to_cookie(u64 handle)
 {
 	return (u32)(handle >> NBD_COOKIE_BITS);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|640| <<nbd_xmit_timeout>> req, nbdcmd_to_ascii(req_to_nbd_cmd_type(req)),
+ *   - drivers/block/nbd.c|794| <<nbd_send_cmd>> req, nbdcmd_to_ascii(type),
+ */
 static const char *nbdcmd_to_ascii(int cmd)
 {
 	switch (cmd) {
@@ -212,11 +475,22 @@ static ssize_t pid_show(struct device *dev,
 	return sprintf(buf, "%d\n", task_pid_nr(nbd->task_recv));
 }
 
+/*
+ * 在以下使用pid_attr:
+ *   - drivers/block/nbd.c|1509| <<nbd_config_put>> device_remove_file(disk_to_dev(nbd->disk), &pid_attr);
+ *   - drivers/block/nbd.c|1568| <<nbd_start_device>> error = device_create_file(disk_to_dev(nbd->disk), &pid_attr);
+ *
+ * 这个怎么用的呢??
+ */
 static const struct device_attribute pid_attr = {
 	.attr = { .name = "pid", .mode = 0444},
 	.show = pid_show,
 };
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|297| <<nbd_put>> nbd_dev_remove(nbd);
+ */
 static void nbd_dev_remove(struct nbd_device *nbd)
 {
 	struct gendisk *disk = nbd->disk;
@@ -237,14 +511,53 @@ static void nbd_dev_remove(struct nbd_device *nbd)
 	 * totally removed to avoid duplicate creation of the same
 	 * one.
 	 */
+	/*
+	 * 在以下使用nbd_device->destroy_complete:
+	 *   - drivers/block/nbd.c|314| <<nbd_dev_remove>> if (test_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags) && nbd->destroy_complete)
+	 *   - drivers/block/nbd.c|315| <<nbd_dev_remove>> complete(nbd->destroy_complete);
+	 *   - drivers/block/nbd.c|1817| <<nbd_dev_add>> nbd->destroy_complete = NULL;
+	 *   - drivers/block/nbd.c|1935| <<nbd_genl_connect>> DECLARE_COMPLETION_ONSTACK(destroy_complete);
+	 *   - drivers/block/nbd.c|1990| <<nbd_genl_connect>> nbd->destroy_complete = &destroy_complete;
+	 *   - drivers/block/nbd.c|1994| <<nbd_genl_connect>> wait_for_completion(&destroy_complete);
+	 */
 	if (test_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags) && nbd->destroy_complete)
 		complete(nbd->destroy_complete);
 
 	kfree(nbd);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1290| <<nbd_config_put>> nbd_put(nbd);
+ *   - drivers/block/nbd.c|1560| <<nbd_release>> nbd_put(nbd);
+ *   - drivers/block/nbd.c|1947| <<nbd_genl_connect>> nbd_put(nbd);
+ *   - drivers/block/nbd.c|1955| <<nbd_genl_connect>> nbd_put(nbd);
+ *   - drivers/block/nbd.c|1961| <<nbd_genl_connect>> nbd_put(nbd);
+ *   - drivers/block/nbd.c|2039| <<nbd_genl_connect>> nbd_put(nbd);
+ *   - drivers/block/nbd.c|2089| <<nbd_genl_disconnect>> nbd_put(nbd);
+ *   - drivers/block/nbd.c|2094| <<nbd_genl_disconnect>> nbd_put(nbd);
+ *   - drivers/block/nbd.c|2133| <<nbd_genl_reconfigure>> nbd_put(nbd);
+ *   - drivers/block/nbd.c|2219| <<nbd_genl_reconfigure>> nbd_put(nbd);
+ *   - drivers/block/nbd.c|2221| <<nbd_genl_reconfigure>> nbd_put(nbd);
+ *   - drivers/block/nbd.c|2512| <<nbd_cleanup>> nbd_put(nbd);
+ */
 static void nbd_put(struct nbd_device *nbd)
 {
+	/*
+	 * 在以下使用nbd_device->refs:
+	 *   - drivers/block/nbd.c|294| <<nbd_put>> if (refcount_dec_and_mutex_lock(&nbd->refs,
+	 *   - drivers/block/nbd.c|1520| <<nbd_open>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|1539| <<nbd_open>> refcount_inc(&nbd->refs);
+	 *   - drivers/block/nbd.c|1784| <<nbd_dev_add>> refcount_set(&nbd->refs, 1);
+	 *   - drivers/block/nbd.c|1934| <<nbd_genl_connect>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|2081| <<nbd_genl_disconnect>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|2122| <<nbd_genl_reconfigure>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|2169| <<nbd_genl_reconfigure>> refcount_inc(&nbd->refs);
+	 *   - drivers/block/nbd.c|2510| <<nbd_cleanup>> if (refcount_read(&nbd->refs) != 1)
+	 *
+	 * return holding mutex if able to decrement refcount to 0
+	 * Return: true and hold mutex if able to decrement refcount to 0, false otherwise
+	 */
 	if (refcount_dec_and_mutex_lock(&nbd->refs,
 					&nbd_index_mutex)) {
 		idr_remove(&nbd_index_idr, nbd->index);
@@ -253,12 +566,26 @@ static void nbd_put(struct nbd_device *nbd)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|557| <<nbd_mark_nsock_dead>> if (!nsock->dead && notify && !nbd_disconnected(nbd->config)) {
+ *   - drivers/block/nbd.c|1007| <<nbd_read_stat>> if (!nbd_disconnected(config))
+ *   - drivers/block/nbd.c|1077| <<nbd_read_stat>> if (nbd_disconnected(config) ||
+ *   - drivers/block/nbd.c|1858| <<nbd_open>> } else if (nbd_disconnected(nbd->config)) {
+ */
 static int nbd_disconnected(struct nbd_config *config)
 {
 	return test_bit(NBD_RT_DISCONNECTED, &config->runtime_flags) ||
 		test_bit(NBD_RT_DISCONNECT_REQUESTED, &config->runtime_flags);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|651| <<sock_shutdown>> nbd_mark_nsock_dead(nbd, nsock, 0);
+ *   - drivers/block/nbd.c|730| <<nbd_xmit_timeout>> nbd_mark_nsock_dead(nbd, nsock, 1);
+ *   - drivers/block/nbd.c|1115| <<recv_work>> nbd_mark_nsock_dead(nbd, nsock, 1);
+ *   - drivers/block/nbd.c|1284| <<nbd_handle_cmd>> nbd_mark_nsock_dead(nbd, nsock, 1);
+ */
 static void nbd_mark_nsock_dead(struct nbd_device *nbd, struct nbd_sock *nsock,
 				int notify)
 {
@@ -296,6 +623,11 @@ static void nbd_size_clear(struct nbd_device *nbd)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|622| <<nbd_size_set>> nbd_size_update(nbd);
+ *   - drivers/block/nbd.c|1646| <<nbd_start_device>> nbd_size_update(nbd);
+ */
 static void nbd_size_update(struct nbd_device *nbd)
 {
 	struct nbd_config *config = nbd->config;
@@ -388,6 +720,25 @@ static enum blk_eh_timer_return nbd_xmit_timeout(struct request *req,
 	if (!mutex_trylock(&cmd->lock))
 		return BLK_EH_RESET_TIMER;
 
+	/*
+	 * 在以下使用nbd_device->config_refs:
+	 *   - drivers/block/nbd.c|391| <<nbd_xmit_timeout>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|879| <<nbd_handle_cmd>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1090| <<nbd_reconnect_socket>> refcount_inc(&nbd->config_refs);
+	 *   - drivers/block/nbd.c|1188| <<nbd_config_put>> if (refcount_dec_and_mutex_lock(&nbd->config_refs,
+	 *   - drivers/block/nbd.c|1275| <<nbd_start_device>> refcount_inc(&nbd->config_refs);
+	 *   - drivers/block/nbd.c|1454| <<nbd_open>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1458| <<nbd_open>> if (refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1468| <<nbd_open>> refcount_set(&nbd->config_refs, 1);
+	 *   - drivers/block/nbd.c|1713| <<nbd_dev_add>> refcount_set(&nbd->config_refs, 0);
+	 *   - drivers/block/nbd.c|1742| <<find_free_cb>> if (!refcount_read(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1875| <<nbd_genl_connect>> if (refcount_read(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1895| <<nbd_genl_connect>> refcount_set(&nbd->config_refs, 1);
+	 *   - drivers/block/nbd.c|1964| <<nbd_genl_connect>> refcount_inc(&nbd->config_refs);
+	 *   - drivers/block/nbd.c|2018| <<nbd_genl_disconnect>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|2060| <<nbd_genl_reconfigure>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|2208| <<populate_nbd_status>> if (refcount_read(&nbd->config_refs))
+	 */
 	if (!refcount_inc_not_zero(&nbd->config_refs)) {
 		cmd->status = BLK_STS_TIMEOUT;
 		mutex_unlock(&cmd->lock);
@@ -456,6 +807,14 @@ static enum blk_eh_timer_return nbd_xmit_timeout(struct request *req,
 /*
  *  Send or receive packet.
  */
+/*
+ * called by:
+ *   - drivers/block/nbd.c|700| <<nbd_send_cmd>> result = sock_xmit(nbd, index, 1, &from,
+ *   - drivers/block/nbd.c|746| <<nbd_send_cmd>> result = sock_xmit(nbd, index, 1, &from, flags, &sent);
+ *   - drivers/block/nbd.c|798| <<nbd_read_stat>> result = sock_xmit(nbd, index, 0, &to, MSG_WAITALL, NULL);
+ *   - drivers/block/nbd.c|859| <<nbd_read_stat>> result = sock_xmit(nbd, index, 0, &to, MSG_WAITALL, NULL);
+ *   - drivers/block/nbd.c|1285| <<send_disconnects>> ret = sock_xmit(nbd, i, 1, &from, 0, NULL);
+ */
 static int sock_xmit(struct nbd_device *nbd, int index, int send,
 		     struct iov_iter *iter, int msg_flags, int *sent)
 {
@@ -512,6 +871,10 @@ static inline int was_interrupted(int result)
 }
 
 /* always call with the tx_lock held */
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1061| <<nbd_handle_cmd>> ret = nbd_send_cmd(nbd, cmd, index);
+ */
 static int nbd_send_cmd(struct nbd_device *nbd, struct nbd_cmd *cmd, int index)
 {
 	struct request *req = blk_mq_rq_from_pdu(cmd);
@@ -659,6 +1022,10 @@ static int nbd_send_cmd(struct nbd_device *nbd, struct nbd_cmd *cmd, int index)
 }
 
 /* NULL returned = something went wrong, inform userspace */
+/*
+ * called by:
+ *   - drivers/block/nbd.c|999| <<recv_work>> cmd = nbd_read_stat(nbd, args->index);
+ */
 static struct nbd_cmd *nbd_read_stat(struct nbd_device *nbd, int index)
 {
 	struct nbd_config *config = nbd->config;
@@ -765,6 +1132,11 @@ static struct nbd_cmd *nbd_read_stat(struct nbd_device *nbd, int index)
 	return ret ? ERR_PTR(ret) : cmd;
 }
 
+/*
+ * 在以下使用recv_work():
+ *   - drivers/block/nbd.c|1133| <<nbd_reconnect_socket>> INIT_WORK(&args->work, recv_work);
+ *   - drivers/block/nbd.c|1314| <<nbd_start_device>> INIT_WORK(&args->work, recv_work);
+ */
 static void recv_work(struct work_struct *work)
 {
 	struct recv_thread_args *args = container_of(work,
@@ -793,6 +1165,10 @@ static void recv_work(struct work_struct *work)
 	kfree(args);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|936| <<nbd_clear_que>> blk_mq_tagset_busy_iter(&nbd->tag_set, nbd_clear_req, NULL);
+ */
 static bool nbd_clear_req(struct request *req, void *data, bool reserved)
 {
 	struct nbd_cmd *cmd = blk_mq_rq_to_pdu(req);
@@ -805,6 +1181,10 @@ static bool nbd_clear_req(struct request *req, void *data, bool reserved)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1307| <<nbd_clear_sock>> nbd_clear_que(nbd);
+ */
 static void nbd_clear_que(struct nbd_device *nbd)
 {
 	blk_mq_quiesce_queue(nbd->disk->queue);
@@ -868,6 +1248,10 @@ static int wait_for_reconnect(struct nbd_device *nbd)
 				  config->dead_conn_timeout) > 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1168| <<nbd_queue_rq>> ret = nbd_handle_cmd(cmd, hctx->queue_num);
+ */
 static int nbd_handle_cmd(struct nbd_cmd *cmd, int index)
 {
 	struct request *req = blk_mq_rq_from_pdu(cmd);
@@ -963,6 +1347,14 @@ static blk_status_t nbd_queue_rq(struct blk_mq_hw_ctx *hctx,
 	 * done sending everything over the wire.
 	 */
 	mutex_lock(&cmd->lock);
+	/*
+	 * 在以下使用NBD_CMD_REQUEUED:
+	 *   - drivers/block/nbd.c|239| <<nbd_requeue_cmd>> if (!test_and_set_bit(NBD_CMD_REQUEUED, &cmd->flags))
+	 *   - drivers/block/nbd.c|714| <<nbd_send_cmd>> set_bit(NBD_CMD_REQUEUED, &cmd->flags);
+	 *   - drivers/block/nbd.c|755| <<nbd_send_cmd>> set_bit(NBD_CMD_REQUEUED, &cmd->flags);
+	 *   - drivers/block/nbd.c|839| <<nbd_read_stat>> if (test_bit(NBD_CMD_REQUEUED, &cmd->flags)) {
+	 *   - drivers/block/nbd.c|1091| <<nbd_queue_rq>> clear_bit(NBD_CMD_REQUEUED, &cmd->flags);
+	 */
 	clear_bit(NBD_CMD_REQUEUED, &cmd->flags);
 
 	/* We can be called directly from the user space process, which means we
@@ -970,6 +1362,9 @@ static blk_status_t nbd_queue_rq(struct blk_mq_hw_ctx *hctx,
 	 * this case we need to return that we are busy, otherwise error out as
 	 * appropriate.
 	 */
+	/*
+	 * 只在这里调用
+	 */
 	ret = nbd_handle_cmd(cmd, hctx->queue_num);
 	if (ret < 0)
 		ret = BLK_STS_IOERR;
@@ -1000,6 +1395,11 @@ static struct socket *nbd_get_socket(struct nbd_device *nbd, unsigned long fd,
 	return sock;
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1600| <<__nbd_ioctl>> return nbd_add_socket(nbd, arg, false);
+ *   - drivers/block/nbd.c|2251| <<nbd_genl_connect>> ret = nbd_add_socket(nbd, fd, true);
+ */
 static int nbd_add_socket(struct nbd_device *nbd, unsigned long arg,
 			  bool netlink)
 {
@@ -1176,6 +1576,11 @@ static int nbd_disconnect(struct nbd_device *nbd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1322| <<nbd_config_put>> nbd_clear_sock(nbd);
+ *   - drivers/block/nbd.c|2111| <<nbd_disconnect_and_put>> nbd_clear_sock(nbd);
+ */
 static void nbd_clear_sock(struct nbd_device *nbd)
 {
 	sock_shutdown(nbd);
@@ -1291,6 +1696,16 @@ static int nbd_start_device_ioctl(struct nbd_device *nbd, struct block_device *b
 	if (ret)
 		return ret;
 
+	/*
+	 * 在以下使用max_paert:
+	 *   - drivers/block/nbd.c|2472| <<global>> module_param(max_part, int , 0444);
+	 *   - drivers/block/nbd.c|2473| <<global>> MODULE_PARM_DESC(max_part, "number of partitions per device (default: 16)");
+	 *   - drivers/block/nbd.c|1337| <<nbd_start_device_ioctl>> if (max_part)
+	 *   - drivers/block/nbd.c|2389| <<nbd_init>> if (max_part < 0) {
+	 *   - drivers/block/nbd.c|2395| <<nbd_init>> if (max_part > 0) {
+	 *   - drivers/block/nbd.c|2396| <<nbd_init>> part_shift = fls(max_part);
+	 *   - drivers/block/nbd.c|2406| <<nbd_init>> max_part = (1UL << part_shift) - 1;
+	 */
 	if (max_part)
 		bdev->bd_invalidated = 1;
 	mutex_unlock(&nbd->config_lock);
@@ -1390,6 +1805,10 @@ static int __nbd_ioctl(struct block_device *bdev, struct nbd_device *nbd,
 	return -ENOTTY;
 }
 
+/*
+ * struct block_device_operations nbd_fops.ioctl = nbd_ioctl()
+ * struct block_device_operations nbd_fops.compat_ioctl = nbd_ioctl()
+ */
 static int nbd_ioctl(struct block_device *bdev, fmode_t mode,
 		     unsigned int cmd, unsigned long arg)
 {
@@ -1436,6 +1855,9 @@ static struct nbd_config *nbd_alloc_config(void)
 	return config;
 }
 
+/*
+ * struct block_device_operations nbd_fops.open = nbd_open()
+ */
 static int nbd_open(struct block_device *bdev, fmode_t mode)
 {
 	struct nbd_device *nbd;
@@ -1459,6 +1881,10 @@ static int nbd_open(struct block_device *bdev, fmode_t mode)
 			mutex_unlock(&nbd->config_lock);
 			goto out;
 		}
+		/*
+		 * struct nbd_device *nbd:
+		 *   -> struct nbd_config *config;
+		 */
 		config = nbd->config = nbd_alloc_config();
 		if (!config) {
 			ret = -ENOMEM;
@@ -1477,6 +1903,9 @@ static int nbd_open(struct block_device *bdev, fmode_t mode)
 	return ret;
 }
 
+/*
+ * struct block_device_operations nbd_fops.release = nbd_release()
+ */
 static void nbd_release(struct gendisk *disk, fmode_t mode)
 {
 	struct nbd_device *nbd = disk->private_data;
@@ -1558,6 +1987,10 @@ static const struct file_operations nbd_dbg_flags_ops = {
 	.release = single_release,
 };
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1654| <<nbd_start_device>> nbd_dev_dbg_init(nbd);
+ */
 static int nbd_dev_dbg_init(struct nbd_device *nbd)
 {
 	struct dentry *dir;
@@ -1572,6 +2005,11 @@ static int nbd_dev_dbg_init(struct nbd_device *nbd)
 			nbd_name(nbd));
 		return -EIO;
 	}
+	/*
+	 * 在以下使用nbd_config->dbg_dir:
+	 *   - drivers/block/nbd.c|2003| <<nbd_dev_dbg_init>> config->dbg_dir = dir;
+	 *   - drivers/block/nbd.c|2016| <<nbd_dev_dbg_close>> debugfs_remove_recursive(nbd->config->dbg_dir);
+	 */
 	config->dbg_dir = dir;
 
 	debugfs_create_file("tasks", 0444, dir, nbd, &nbd_dbg_tasks_ops);
@@ -1588,6 +2026,10 @@ static void nbd_dev_dbg_close(struct nbd_device *nbd)
 	debugfs_remove_recursive(nbd->config->dbg_dir);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|2646| <<nbd_init>> nbd_dbg_init();
+ */
 static int nbd_dbg_init(void)
 {
 	struct dentry *dbg_dir;
@@ -1596,6 +2038,13 @@ static int nbd_dbg_init(void)
 	if (!dbg_dir)
 		return -EIO;
 
+	/*
+	 * 在以下使用nbd_dbg_dir:
+	 *   - drivers/block/nbd.c|1983| <<nbd_dev_dbg_init>> if (!nbd_dbg_dir)
+	 *   - drivers/block/nbd.c|1986| <<nbd_dev_dbg_init>> dir = debugfs_create_dir(nbd_name(nbd), nbd_dbg_dir);
+	 *   - drivers/block/nbd.c|2020| <<nbd_dbg_init>> nbd_dbg_dir = dbg_dir;
+	 *   - drivers/block/nbd.c|2027| <<nbd_dbg_close>> debugfs_remove_recursive(nbd_dbg_dir);
+	 */
 	nbd_dbg_dir = dbg_dir;
 
 	return 0;
@@ -1645,6 +2094,12 @@ static const struct blk_mq_ops nbd_mq_ops = {
 	.timeout	= nbd_xmit_timeout,
 };
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1928| <<nbd_genl_connect>> new_index = nbd_dev_add(-1);
+ *   - drivers/block/nbd.c|1939| <<nbd_genl_connect>> ret = nbd_dev_add(index);
+ *   - drivers/block/nbd.c|2513| <<nbd_init>> nbd_dev_add(i);
+ */
 static int nbd_dev_add(int index)
 {
 	struct nbd_device *nbd;
@@ -1685,10 +2140,20 @@ static int nbd_dev_add(int index)
 	nbd->tag_set.driver_data = nbd;
 	nbd->destroy_complete = NULL;
 
+	/*
+	 * Alloc a tag set to be associated with one or more request queues.
+	 * May fail with EINVAL for various error conditions. May adjust the
+	 * requested depth down, if it's too large. In that case, the set
+	 * value will be stored in set->queue_depth.
+	 */
 	err = blk_mq_alloc_tag_set(&nbd->tag_set);
 	if (err)
 		goto out_free_idr;
 
+	/*
+	 * blk_mq_init_queue()更加像是从tagset中新分配一个request_queue
+	 * 一个tagset可以有多个request_queue
+	 */
 	q = blk_mq_init_queue(&nbd->tag_set);
 	if (IS_ERR(q)) {
 		err = PTR_ERR(q);
@@ -1719,6 +2184,11 @@ static int nbd_dev_add(int index)
 	disk->private_data = nbd;
 	sprintf(disk->disk_name, "nbd%d", index);
 	add_disk(disk);
+	/*
+	 * 在以下使用nbd_total_devices:
+	 *   - drivers/block/nbd.c|1823| <<nbd_dev_add>> nbd_total_devices++;
+	 *   - drivers/block/nbd.c|2356| <<nbd_genl_status>> msg_size *= (index == -1) ? nbd_total_devices : 1;
+	 */
 	nbd_total_devices++;
 	return index;
 
@@ -1739,6 +2209,25 @@ static int find_free_cb(int id, void *ptr, void *data)
 	struct nbd_device *nbd = ptr;
 	struct nbd_device **found = data;
 
+	/*
+	 * 在以下使用nbd_device->config_refs:
+	 *   - drivers/block/nbd.c|391| <<nbd_xmit_timeout>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|879| <<nbd_handle_cmd>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1090| <<nbd_reconnect_socket>> refcount_inc(&nbd->config_refs);
+	 *   - drivers/block/nbd.c|1188| <<nbd_config_put>> if (refcount_dec_and_mutex_lock(&nbd->config_refs,
+	 *   - drivers/block/nbd.c|1275| <<nbd_start_device>> refcount_inc(&nbd->config_refs);
+	 *   - drivers/block/nbd.c|1454| <<nbd_open>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1458| <<nbd_open>> if (refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1468| <<nbd_open>> refcount_set(&nbd->config_refs, 1);
+	 *   - drivers/block/nbd.c|1713| <<nbd_dev_add>> refcount_set(&nbd->config_refs, 0);
+	 *   - drivers/block/nbd.c|1742| <<find_free_cb>> if (!refcount_read(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1875| <<nbd_genl_connect>> if (refcount_read(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|1895| <<nbd_genl_connect>> refcount_set(&nbd->config_refs, 1);
+	 *   - drivers/block/nbd.c|1964| <<nbd_genl_connect>> refcount_inc(&nbd->config_refs);
+	 *   - drivers/block/nbd.c|2018| <<nbd_genl_disconnect>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|2060| <<nbd_genl_reconfigure>> if (!refcount_inc_not_zero(&nbd->config_refs)) {
+	 *   - drivers/block/nbd.c|2208| <<populate_nbd_status>> if (refcount_read(&nbd->config_refs))
+	 */
 	if (!refcount_read(&nbd->config_refs)) {
 		*found = nbd;
 		return 1;
@@ -1747,6 +2236,9 @@ static int find_free_cb(int id, void *ptr, void *data)
 }
 
 /* Netlink interface. */
+/*
+ * struct genl_family nbd_genl_family.policy = nbd_attr_policy[]
+ */
 static const struct nla_policy nbd_attr_policy[NBD_ATTR_MAX + 1] = {
 	[NBD_ATTR_INDEX]		=	{ .type = NLA_U32 },
 	[NBD_ATTR_SIZE_BYTES]		=	{ .type = NLA_U64 },
@@ -1796,6 +2288,9 @@ static int nbd_genl_size_set(struct genl_info *info, struct nbd_device *nbd)
 	return 0;
 }
 
+/*
+ * 处理struct genl_ops nbd_connect_genl_ops[NBD_CMD_CONNECT]
+ */
 static int nbd_genl_connect(struct sk_buff *skb, struct genl_info *info)
 {
 	DECLARE_COMPLETION_ONSTACK(destroy_complete);
@@ -1824,6 +2319,12 @@ static int nbd_genl_connect(struct sk_buff *skb, struct genl_info *info)
 		ret = idr_for_each(&nbd_index_idr, &find_free_cb, &nbd);
 		if (ret == 0) {
 			int new_index;
+			/*
+			 * 在以下调用nbd_dev_add():
+			 *   - drivers/block/nbd.c|1928| <<nbd_genl_connect>> new_index = nbd_dev_add(-1);
+			 *   - drivers/block/nbd.c|1939| <<nbd_genl_connect>> ret = nbd_dev_add(index);
+			 *   - drivers/block/nbd.c|2513| <<nbd_init>> nbd_dev_add(i);
+			 */
 			new_index = nbd_dev_add(-1);
 			if (new_index < 0) {
 				mutex_unlock(&nbd_index_mutex);
@@ -1861,6 +2362,18 @@ static int nbd_genl_connect(struct sk_buff *skb, struct genl_info *info)
 		goto again;
 	}
 
+	/*
+	 * 在以下使用nbd_device->refs:
+	 *   - drivers/block/nbd.c|294| <<nbd_put>> if (refcount_dec_and_mutex_lock(&nbd->refs,
+	 *   - drivers/block/nbd.c|1520| <<nbd_open>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|1539| <<nbd_open>> refcount_inc(&nbd->refs);
+	 *   - drivers/block/nbd.c|1784| <<nbd_dev_add>> refcount_set(&nbd->refs, 1);
+	 *   - drivers/block/nbd.c|1934| <<nbd_genl_connect>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|2081| <<nbd_genl_disconnect>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|2122| <<nbd_genl_reconfigure>> if (!refcount_inc_not_zero(&nbd->refs)) {
+	 *   - drivers/block/nbd.c|2169| <<nbd_genl_reconfigure>> refcount_inc(&nbd->refs);
+	 *   - drivers/block/nbd.c|2510| <<nbd_cleanup>> if (refcount_read(&nbd->refs) != 1)
+	 */
 	if (!refcount_inc_not_zero(&nbd->refs)) {
 		mutex_unlock(&nbd_index_mutex);
 		if (index == -1)
@@ -1970,6 +2483,11 @@ static int nbd_genl_connect(struct sk_buff *skb, struct genl_info *info)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1735| <<nbd_release>> nbd_disconnect_and_put(nbd);
+ *   - drivers/block/nbd.c|2322| <<nbd_genl_disconnect>> nbd_disconnect_and_put(nbd);
+ */
 static void nbd_disconnect_and_put(struct nbd_device *nbd)
 {
 	mutex_lock(&nbd->config_lock);
@@ -1987,6 +2505,9 @@ static void nbd_disconnect_and_put(struct nbd_device *nbd)
 		nbd_config_put(nbd);
 }
 
+/*
+ * 处理struct genl_ops nbd_connect_genl_ops[NBD_CMD_DISCONNECT]
+ */
 static int nbd_genl_disconnect(struct sk_buff *skb, struct genl_info *info)
 {
 	struct nbd_device *nbd;
@@ -2025,6 +2546,9 @@ static int nbd_genl_disconnect(struct sk_buff *skb, struct genl_info *info)
 	return 0;
 }
 
+/*
+ * 处理struct genl_ops nbd_connect_genl_ops[NBD_CMD_RECONFIGURE]
+ */
 static int nbd_genl_reconfigure(struct sk_buff *skb, struct genl_info *info)
 {
 	struct nbd_device *nbd = NULL;
@@ -2152,6 +2676,9 @@ static int nbd_genl_reconfigure(struct sk_buff *skb, struct genl_info *info)
 	return ret;
 }
 
+/*
+ * struct genl_family nbd_genl_family.ops = nbd_connect_genl_ops[]
+ */
 static const struct genl_ops nbd_connect_genl_ops[] = {
 	{
 		.cmd	= NBD_CMD_CONNECT,
@@ -2175,10 +2702,22 @@ static const struct genl_ops nbd_connect_genl_ops[] = {
 	},
 };
 
+/*
+ * struct genl_family nbd_genl_family.mcgrps = nbd_mcast_grps[]
+ */
 static const struct genl_multicast_group nbd_mcast_grps[] = {
 	{ .name = NBD_GENL_MCAST_GROUP_NAME, },
 };
 
+/*
+ * 在以下使用nbd_genl_family:
+ *   - drivers/block/nbd.c|2311| <<nbd_genl_status>> reply_head = genlmsg_put_reply(reply, info, &nbd_genl_family, 0,
+ *   - drivers/block/nbd.c|2353| <<nbd_connect_reply>> msg_head = genlmsg_put_reply(skb, info, &nbd_genl_family, 0,
+ *   - drivers/block/nbd.c|2377| <<nbd_mcast_index>> msg_head = genlmsg_put(skb, 0, 0, &nbd_genl_family, 0,
+ *   - drivers/block/nbd.c|2389| <<nbd_mcast_index>> genlmsg_multicast(&nbd_genl_family, skb, 0, 0, GFP_KERNEL);
+ *   - drivers/block/nbd.c|2435| <<nbd_init>> if (genl_register_family(&nbd_genl_family)) {
+ *   - drivers/block/nbd.c|2477| <<nbd_cleanup>> genl_unregister_family(&nbd_genl_family);
+ */
 static struct genl_family nbd_genl_family __ro_after_init = {
 	.hdrsize	= 0,
 	.name		= NBD_GENL_FAMILY_NAME,
@@ -2192,6 +2731,11 @@ static struct genl_family nbd_genl_family __ro_after_init = {
 	.n_mcgrps	= ARRAY_SIZE(nbd_mcast_grps),
 };
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|2348| <<status_cb>> return populate_nbd_status(nbd, (struct sk_buff *)data);
+ *   - drivers/block/nbd.c|2390| <<nbd_genl_status>> ret = populate_nbd_status(nbd, reply);
+ */
 static int populate_nbd_status(struct nbd_device *nbd, struct sk_buff *reply)
 {
 	struct nlattr *dev_opt;
@@ -2227,6 +2771,9 @@ static int status_cb(int id, void *ptr, void *data)
 	return populate_nbd_status(nbd, (struct sk_buff *)data);
 }
 
+/*
+ * 处理struct genl_ops nbd_connect_genl_ops[NBD_CMD_STATUS]
+ */
 static int nbd_genl_status(struct sk_buff *skb, struct genl_info *info)
 {
 	struct nlattr *dev_list;
@@ -2281,6 +2828,10 @@ static int nbd_genl_status(struct sk_buff *skb, struct genl_info *info)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|2214| <<nbd_genl_connect>> nbd_connect_reply(info, nbd->index);
+ */
 static void nbd_connect_reply(struct genl_info *info, int index)
 {
 	struct sk_buff *skb;
@@ -2305,6 +2856,10 @@ static void nbd_connect_reply(struct genl_info *info, int index)
 	genlmsg_reply(skb, info);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|2609| <<nbd_dead_link_work>> nbd_mcast_index(args->index);
+ */
 static void nbd_mcast_index(int index)
 {
 	struct sk_buff *skb;
@@ -2329,6 +2884,10 @@ static void nbd_mcast_index(int index)
 	genlmsg_multicast(&nbd_genl_family, skb, 0, 0, GFP_KERNEL);
 }
 
+/*
+ * 在以下使用nbd_dead_link_work():
+ *   - drivers/block/nbd.c|420| <<nbd_mark_nsock_dead>> INIT_WORK(&args->work, nbd_dead_link_work);
+ */
 static void nbd_dead_link_work(struct work_struct *work)
 {
 	struct link_dead_args *args = container_of(work, struct link_dead_args,
@@ -2343,6 +2902,16 @@ static int __init nbd_init(void)
 
 	BUILD_BUG_ON(sizeof(struct nbd_request) != 28);
 
+	/*
+	 * 在以下使用max_part:
+	 *   - drivers/block/nbd.c|2472| <<global>> module_param(max_part, int , 0444);
+	 *   - drivers/block/nbd.c|2473| <<global>> MODULE_PARM_DESC(max_part, "number of partitions per device (default: 16)");
+	 *   - drivers/block/nbd.c|1337| <<nbd_start_device_ioctl>> if (max_part)
+	 *   - drivers/block/nbd.c|2389| <<nbd_init>> if (max_part < 0) {
+	 *   - drivers/block/nbd.c|2395| <<nbd_init>> if (max_part > 0) {
+	 *   - drivers/block/nbd.c|2396| <<nbd_init>> part_shift = fls(max_part);
+	 *   - drivers/block/nbd.c|2406| <<nbd_init>> max_part = (1UL << part_shift) - 1;
+	 */
 	if (max_part < 0) {
 		printk(KERN_ERR "nbd: max_part must be >= 0\n");
 		return -EINVAL;
@@ -2350,6 +2919,16 @@ static int __init nbd_init(void)
 
 	part_shift = 0;
 	if (max_part > 0) {
+		/*
+		 * 在以下使用part_shift:
+		 *   - drivers/block/nbd.c|1719| <<nbd_dev_add>> disk = alloc_disk(1 << part_shift);
+		 *   - drivers/block/nbd.c|1777| <<nbd_dev_add>> disk->first_minor = index << part_shift;
+		 *   - drivers/block/nbd.c|2411| <<nbd_init>> part_shift = 0;
+		 *   - drivers/block/nbd.c|2413| <<nbd_init>> part_shift = fls(max_part);
+		 *   - drivers/block/nbd.c|2423| <<nbd_init>> max_part = (1UL << part_shift) - 1;
+		 *   - drivers/block/nbd.c|2426| <<nbd_init>> if ((1UL << part_shift) > DISK_MAX_PARTS)
+		 *   - drivers/block/nbd.c|2429| <<nbd_init>> if (nbds_max > 1UL << (MINORBITS - part_shift))
+		 */
 		part_shift = fls(max_part);
 
 		/*
@@ -2372,6 +2951,9 @@ static int __init nbd_init(void)
 	if (register_blkdev(NBD_MAJOR, "nbd"))
 		return -EIO;
 
+	/*
+	 * register a generic netlink family
+	 */
 	if (genl_register_family(&nbd_genl_family)) {
 		unregister_blkdev(NBD_MAJOR, "nbd");
 		return -EINVAL;
@@ -2385,6 +2967,10 @@ static int __init nbd_init(void)
 	return 0;
 }
 
+/*
+ * 在以下使用nbd_exit_cb():
+ *   - drivers/block/nbd.c|2582| <<nbd_cleanup>> idr_for_each(&nbd_index_idr, &nbd_exit_cb, &del_list);
+ */
 static int nbd_exit_cb(int id, void *ptr, void *data)
 {
 	struct list_head *list = (struct list_head *)data;
@@ -2402,12 +2988,41 @@ static void __exit nbd_cleanup(void)
 	nbd_dbg_close();
 
 	mutex_lock(&nbd_index_mutex);
+	/*
+	 * 在以下使用nbd_index_idr:
+	 *   - drivers/block/nbd.c|401| <<nbd_put>> idr_remove(&nbd_index_idr, nbd->index);
+	 *   - drivers/block/nbd.c|1870| <<nbd_dev_add>> err = idr_alloc(&nbd_index_idr, nbd, index, index + 1,
+	 *   - drivers/block/nbd.c|1875| <<nbd_dev_add>> err = idr_alloc(&nbd_index_idr, nbd, 0, 0, GFP_KERNEL);
+	 *   - drivers/block/nbd.c|1949| <<nbd_dev_add>> idr_remove(&nbd_index_idr, index);
+	 *   - drivers/block/nbd.c|2048| <<nbd_genl_connect>> ret = idr_for_each(&nbd_index_idr, &find_free_cb, &nbd);
+	 *   - drivers/block/nbd.c|2057| <<nbd_genl_connect>> nbd = idr_find(&nbd_index_idr, new_index);
+	 *   - drivers/block/nbd.c|2060| <<nbd_genl_connect>> nbd = idr_find(&nbd_index_idr, index);
+	 *   - drivers/block/nbd.c|2068| <<nbd_genl_connect>> nbd = idr_find(&nbd_index_idr, index);
+	 *   - drivers/block/nbd.c|2228| <<nbd_genl_disconnect>> nbd = idr_find(&nbd_index_idr, index);
+	 *   - drivers/block/nbd.c|2269| <<nbd_genl_reconfigure>> nbd = idr_find(&nbd_index_idr, index);
+	 *   - drivers/block/nbd.c|2504| <<nbd_genl_status>> ret = idr_for_each(&nbd_index_idr, &status_cb, reply);
+	 *   - drivers/block/nbd.c|2511| <<nbd_genl_status>> nbd = idr_find(&nbd_index_idr, index);
+	 *   - drivers/block/nbd.c|2676| <<nbd_cleanup>> idr_for_each(&nbd_index_idr, &nbd_exit_cb, &del_list);
+	 *   - drivers/block/nbd.c|2699| <<nbd_cleanup>> idr_destroy(&nbd_index_idr);
+	 */
 	idr_for_each(&nbd_index_idr, &nbd_exit_cb, &del_list);
 	mutex_unlock(&nbd_index_mutex);
 
 	while (!list_empty(&del_list)) {
 		nbd = list_first_entry(&del_list, struct nbd_device, list);
 		list_del_init(&nbd->list);
+		/*
+		 * 在以下使用nbd_device->nbd:
+		 *   - drivers/block/nbd.c|294| <<nbd_put>> if (refcount_dec_and_mutex_lock(&nbd->refs,
+		 *   - drivers/block/nbd.c|1520| <<nbd_open>> if (!refcount_inc_not_zero(&nbd->refs)) {
+		 *   - drivers/block/nbd.c|1539| <<nbd_open>> refcount_inc(&nbd->refs);
+		 *   - drivers/block/nbd.c|1784| <<nbd_dev_add>> refcount_set(&nbd->refs, 1);
+		 *   - drivers/block/nbd.c|1934| <<nbd_genl_connect>> if (!refcount_inc_not_zero(&nbd->refs)) {
+		 *   - drivers/block/nbd.c|2081| <<nbd_genl_disconnect>> if (!refcount_inc_not_zero(&nbd->refs)) {
+		 *   - drivers/block/nbd.c|2122| <<nbd_genl_reconfigure>> if (!refcount_inc_not_zero(&nbd->refs)) {
+		 *   - drivers/block/nbd.c|2169| <<nbd_genl_reconfigure>> refcount_inc(&nbd->refs);
+		 *   - drivers/block/nbd.c|2510| <<nbd_cleanup>> if (refcount_read(&nbd->refs) != 1)
+		 */
 		if (refcount_read(&nbd->refs) != 1)
 			printk(KERN_ERR "nbd: possibly leaking a device\n");
 		nbd_put(nbd);
diff --git a/drivers/block/null_blk.h b/drivers/block/null_blk.h
index bc837862b767..3b859f1975ef 100644
--- a/drivers/block/null_blk.h
+++ b/drivers/block/null_blk.h
@@ -14,24 +14,61 @@
 #include <linux/fault-inject.h>
 
 struct nullb_cmd {
+	/*
+	 * list没人用, 可以删了??
+	 *   - drivers/block/null_blk_main.c|2440| <<setup_commands>> INIT_LIST_HEAD(&cmd->list);
+	 */
 	struct list_head list;
+	/*
+	 * ll_list没人用, 可以删了??
+	 *   - drivers/block/null_blk_main.c|2441| <<setup_commands>> cmd->ll_list.next = NULL;
+	 */
 	struct llist_node ll_list;
+	/*
+	 * csd没人用, 可以删除了
+	 */
 	struct __call_single_data csd;
 	struct request *rq;
 	struct bio *bio;
 	unsigned int tag;
 	blk_status_t error;
 	struct nullb_queue *nq;
+	/*
+	 * 表示complete req的方式, 比方不用softirq, 而用一个timer触发
+	 */
 	struct hrtimer timer;
 };
 
 struct nullb_queue {
+	/*
+	 * 在以下使用tag_map:
+	 *   - drivers/block/null_blk_main.c|922| <<put_tag>> clear_bit_unlock(tag, nq->tag_map);
+	 *   - drivers/block/null_blk_main.c|946| <<get_tag>> tag = find_first_zero_bit(nq->tag_map, nq->queue_depth);
+	 *   - drivers/block/null_blk_main.c|949| <<get_tag>> } while (test_and_set_bit_lock(tag, nq->tag_map));
+	 *   - drivers/block/null_blk_main.c|2292| <<cleanup_queue>> kfree(nq->tag_map);
+	 *   - drivers/block/null_blk_main.c|2420| <<setup_commands>> nq->tag_map = kcalloc(tag_size, sizeof(unsigned long ), GFP_KERNEL);
+	 *   - drivers/block/null_blk_main.c|2421| <<setup_commands>> if (!nq->tag_map) {
+	 */
 	unsigned long *tag_map;
 	wait_queue_head_t wait;
 	unsigned int queue_depth;
 	struct nullb_device *dev;
+	/*
+	 * 在以下使用requeue_selection:
+	 *   - drivers/block/null_blk_main.c|2258| <<null_queue_rq>> nq->requeue_selection++;
+	 *   - drivers/block/null_blk_main.c|2259| <<null_queue_rq>> if (nq->requeue_selection & 1)
+	 */
 	unsigned int requeue_selection;
 
+	/*
+	 * 在以下使用cmds:
+	 *   - drivers/block/null_blk_main.c|1003| <<__alloc_cmd>> cmd = &nq->cmds[tag];
+	 *   - drivers/block/null_blk_main.c|2305| <<cleanup_queue>> kfree(nq->cmds);
+	 *   - drivers/block/null_blk_main.c|2427| <<setup_commands>> nq->cmds = kcalloc(nq->queue_depth, sizeof(*cmd), GFP_KERNEL);
+	 *   - drivers/block/null_blk_main.c|2428| <<setup_commands>> if (!nq->cmds)
+	 *   - drivers/block/null_blk_main.c|2434| <<setup_commands>> kfree(nq->cmds);
+	 *   - drivers/block/null_blk_main.c|2439| <<setup_commands>> cmd = &nq->cmds[i];
+	 */
 	struct nullb_cmd *cmds;
 };
 
@@ -41,14 +78,40 @@ struct nullb_device {
 	struct radix_tree_root data; /* data stored in the disk */
 	struct radix_tree_root cache; /* disk cache data */
 	unsigned long flags; /* device flags */
+	/*
+	 * 使用curr_cache的地方:
+	 *   - drivers/block/null_blk_main.c|1282| <<null_free_sector>> nullb->dev->curr_cache -= PAGE_SIZE;
+	 *   - drivers/block/null_blk_main.c|1316| <<null_radix_tree_insert>> nullb->dev->curr_cache += PAGE_SIZE;
+	 *   - drivers/block/null_blk_main.c|1361| <<null_free_device_storage>> dev->curr_cache = 0;
+	 *   - drivers/block/null_blk_main.c|1530| <<null_flush_cache_page>> nullb->dev->curr_cache -= PAGE_SIZE;
+	 *   - drivers/block/null_blk_main.c|1550| <<null_make_cache_space>> nullb->dev->curr_cache + n || nullb->dev->curr_cache == 0)
+	 *   - drivers/block/null_blk_main.c|1779| <<null_handle_flush>> if (err || nullb->dev->curr_cache == 0)
+	 */
 	unsigned int curr_cache;
 	struct badblocks badblocks;
 
 	unsigned int nr_zones;
 	struct blk_zone *zones;
+	/*
+	 * 在以下使用zone_size_sects:
+	 *   - drivers/block/null_blk_main.c|1946| <<null_gendisk_register>> nullb->dev->zone_size_sects);
+	 *   - drivers/block/null_blk_zoned.c|21| <<null_zone_no>> return sect >> ilog2(dev->zone_size_sects);
+	 *   - drivers/block/null_blk_zoned.c|39| <<null_zone_init>> dev->zone_size_sects = dev->zone_size << ZONE_SIZE_SHIFT;
+	 *   - drivers/block/null_blk_zoned.c|41| <<null_zone_init>> (SECTOR_SHIFT + ilog2(dev->zone_size_sects));
+	 *   - drivers/block/null_blk_zoned.c|57| <<null_zone_init>> zone->len = dev->zone_size_sects;
+	 *   - drivers/block/null_blk_zoned.c|62| <<null_zone_init>> sector += dev->zone_size_sects;
+	 *   - drivers/block/null_blk_zoned.c|69| <<null_zone_init>> zone->len = dev->zone_size_sects;
+	 *   - drivers/block/null_blk_zoned.c|73| <<null_zone_init>> sector += dev->zone_size_sects;
+	 */
 	sector_t zone_size_sects;
 
 	unsigned long size; /* device size in MB */
+	/*
+	 * 在以下使用completion_nsec:
+	 *   - drivers/block/null_blk_main.c|452| <<global>> NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
+	 *   - drivers/block/null_blk_main.c|715| <<null_alloc_dev>> dev->completion_nsec = g_completion_nsec;
+	 *   - drivers/block/null_blk_main.c|930| <<null_cmd_end_timer>> ktime_t kt = cmd->nq->dev->completion_nsec;
+	 */
 	unsigned long completion_nsec; /* time in ns to complete a request */
 	unsigned long cache_size; /* disk cache size in MB */
 	unsigned long zone_size; /* zone size in MB if device is zoned */
@@ -62,8 +125,19 @@ struct nullb_device {
 	unsigned int index; /* index of the disk, only valid with a disk */
 	unsigned int mbps; /* Bandwidth throttle cap (in MB/s) */
 	bool blocking; /* blocking blk-mq device */
+	/*
+	 * 在以下使用null_device->use_per_mpde_hctx:
+	 *   - drivers/block/null_blk_main.c|671| <<null_alloc_dev>> dev->use_per_node_hctx = g_use_per_node_hctx;
+	 *   - drivers/block/null_blk_main.c|2083| <<null_validate_conf>> if (dev->queue_mode == NULL_Q_MQ && dev->use_per_node_hctx) {
+	 */
 	bool use_per_node_hctx; /* use per-node allocation for hardware context */
 	bool power; /* power on/off the device */
+	/*
+	 * 在以下使用memory_backed:
+	 *   - drivers/block/null_blk_main.c|338| <<global>> NULLB_DEVICE_ATTR(memory_backed, bool, NULL);
+	 *   - drivers/block/null_blk_main.c|1554| <<null_handle_cmd>> if (dev->memory_backed)
+	 *   - drivers/block/null_blk_main.c|2023| <<null_validate_conf>> if (dev->memory_backed)
+	 */
 	bool memory_backed; /* if data is stored in memory */
 	bool discard; /* if support discard */
 	bool zoned; /* if device is zoned */
@@ -71,6 +145,7 @@ struct nullb_device {
 
 struct nullb {
 	struct nullb_device *dev;
+	/* 把这个设置加入到nullb_list */
 	struct list_head list;
 	unsigned int index;
 	struct request_queue *q;
@@ -78,8 +153,34 @@ struct nullb {
 	struct blk_mq_tag_set *tag_set;
 	struct blk_mq_tag_set __tag_set;
 	unsigned int queue_depth;
+	/*
+	 * 使用cur_bytes的地方:
+	 *   - drivers/block/null_blk_main.c|1964| <<null_handle_throttled>> if (atomic_long_sub_return(blk_rq_bytes(rq), &nullb->cur_bytes) < 0) {
+	 *   - drivers/block/null_blk_main.c|1967| <<null_handle_throttled>> if (atomic_long_read(&nullb->cur_bytes) > 0)
+	 *   - drivers/block/null_blk_main.c|2119| <<nullb_bwtimer_fn>> if (atomic_long_read(&nullb->cur_bytes) == mb_per_tick(mbps))
+	 *   - drivers/block/null_blk_main.c|2122| <<nullb_bwtimer_fn>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(mbps));
+	 *   - drivers/block/null_blk_main.c|2140| <<nullb_setup_bwtimer>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(nullb->dev->mbps));
+	 *   - drivers/block/null_blk_main.c|2330| <<null_del_dev>> atomic_long_set(&nullb->cur_bytes, LONG_MAX);
+	 */
 	atomic_long_t cur_bytes;
+	/*
+	 * bw_timer在以下使用:
+	 *   - drivers/block/null_blk_main.c|1961| <<null_handle_throttled>> if (!hrtimer_active(&nullb->bw_timer))
+	 *   - drivers/block/null_blk_main.c|1962| <<null_handle_throttled>> hrtimer_restart(&nullb->bw_timer);
+	 *   - drivers/block/null_blk_main.c|2115| <<nullb_bwtimer_fn>> struct nullb *nullb = container_of(timer, struct nullb, bw_timer);
+	 *   - drivers/block/null_blk_main.c|2125| <<nullb_bwtimer_fn>> hrtimer_forward_now(&nullb->bw_timer, timer_interval);
+	 *   - drivers/block/null_blk_main.c|2138| <<nullb_setup_bwtimer>> hrtimer_init(&nullb->bw_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	 *   - drivers/block/null_blk_main.c|2139| <<nullb_setup_bwtimer>> nullb->bw_timer.function = nullb_bwtimer_fn;
+	 *   - drivers/block/null_blk_main.c|2141| <<nullb_setup_bwtimer>> hrtimer_start(&nullb->bw_timer, timer_interval, HRTIMER_MODE_REL);
+	 *   - drivers/block/null_blk_main.c|2329| <<null_del_dev>> hrtimer_cancel(&nullb->bw_timer);
+	 */
 	struct hrtimer bw_timer;
+	/*
+	 * 在以下使用cache_flush_pos:
+	 *   - drivers/block/null_blk_main.c|1551| <<null_make_cache_space>> (void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
+	 *   - drivers/block/null_blk_main.c|1557| <<null_make_cache_space>> nullb->cache_flush_pos = c_pages[i]->page->index;
+	 *   - drivers/block/null_blk_main.c|1581| <<null_make_cache_space>> nullb->cache_flush_pos = 0;
+	 */
 	unsigned long cache_flush_pos;
 	spinlock_t lock;
 
diff --git a/drivers/block/null_blk_main.c b/drivers/block/null_blk_main.c
index ae8d4bc532b0..c4cc593d8253 100644
--- a/drivers/block/null_blk_main.c
+++ b/drivers/block/null_blk_main.c
@@ -11,13 +11,34 @@
 #include <linux/init.h>
 #include "null_blk.h"
 
+/*
+ * null_transfer()是比较核心的函数
+ */
+
+/* 这个shift表示一个page里sector的数量 */
 #define PAGE_SECTORS_SHIFT	(PAGE_SHIFT - SECTOR_SHIFT)
+/* 这个表示一个page里sector的数量 */
 #define PAGE_SECTORS		(1 << PAGE_SECTORS_SHIFT)
 #define SECTOR_MASK		(PAGE_SECTORS - 1)
 
+/*
+ * 在以下使用FREE_BATCH:
+ *   - drivers/block/null_blk_main.c|1014| <<null_free_device_storage>> struct nullb_page *ret, *t_pages[FREE_BATCH];
+ *   - drivers/block/null_blk_main.c|1023| <<null_free_device_storage>> (void **)t_pages, pos, FREE_BATCH);
+ *   - drivers/block/null_blk_main.c|1033| <<null_free_device_storage>> } while (nr_pages == FREE_BATCH);
+ *   - drivers/block/null_blk_main.c|1184| <<null_make_cache_space>> struct nullb_page *c_pages[FREE_BATCH];
+ *   - drivers/block/null_blk_main.c|1193| <<null_make_cache_space>> (void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
+ */
 #define FREE_BATCH		16
 
 #define TICKS_PER_SEC		50ULL
+/*
+ * 在以下使用TIMER_INTERVAL:
+ *   - drivers/block/null_blk_main.c|1664| <<nullb_bwtimer_fn>> ktime_t timer_interval = ktime_set(0, TIMER_INTERVAL);
+ *   - drivers/block/null_blk_main.c|1684| <<nullb_setup_bwtimer>> ktime_t timer_interval = ktime_set(0, TIMER_INTERVAL);
+ *
+ * 每个tick的nsec
+ */
 #define TIMER_INTERVAL		(NSEC_PER_SEC / TICKS_PER_SEC)
 
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -25,8 +46,17 @@ static DECLARE_FAULT_ATTR(null_timeout_attr);
 static DECLARE_FAULT_ATTR(null_requeue_attr);
 #endif
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1605| <<nullb_bwtimer_fn>> if (atomic_long_read(&nullb->cur_bytes) == mb_per_tick(mbps))
+ *   - drivers/block/null_blk_main.c|1608| <<nullb_bwtimer_fn>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(mbps));
+ *   - drivers/block/null_blk_main.c|1626| <<nullb_setup_bwtimer>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(nullb->dev->mbps));
+ */
 static inline u64 mb_per_tick(int mbps)
 {
+	/*
+	 * 1 << 20 相当于1MB
+	 */
 	return (1 << 20) / TICKS_PER_SEC * ((u64) mbps);
 }
 
@@ -39,12 +69,43 @@ static inline u64 mb_per_tick(int mbps)
  * CACHE:	Device is using a write-back cache.
  */
 enum nullb_device_flags {
+	/*
+	 * 在以下使用NULLB_DEV_FL_CONFIGURED:
+	 *   - drivers/block/null_blk_main.c|304| <<NULLB_DEVICE_ATTR>> else if (test_bit(NULLB_DEV_FL_CONFIGURED, &dev->flags)) \
+	 *   - drivers/block/null_blk_main.c|376| <<nullb_device_power_store>> set_bit(NULLB_DEV_FL_CONFIGURED, &dev->flags);
+	 *   - drivers/block/null_blk_main.c|385| <<nullb_device_power_store>> clear_bit(NULLB_DEV_FL_CONFIGURED, &dev->flags);
+	 */
 	NULLB_DEV_FL_CONFIGURED	= 0,
+	/*
+	 * 在以下使用NULLB_DEV_FL_UP:
+	 *   - drivers/block/null_blk_main.c|369| <<nullb_device_power_store>> if (test_and_set_bit(NULLB_DEV_FL_UP, &dev->flags))
+	 *   - drivers/block/null_blk_main.c|372| <<nullb_device_power_store>> clear_bit(NULLB_DEV_FL_UP, &dev->flags);
+	 *   - drivers/block/null_blk_main.c|379| <<nullb_device_power_store>> if (test_and_clear_bit(NULLB_DEV_FL_UP, &dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|513| <<nullb_group_drop_item>> if (test_and_clear_bit(NULLB_DEV_FL_UP, &dev->flags)) {
+	 */
 	NULLB_DEV_FL_UP		= 1,
+	/*
+	 * 在以下使用NULLB_DEV_FL_THROTTLED:
+	 *   - drivers/block/null_blk_main.c|1543| <<null_handle_cmd>> if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|1772| <<null_del_dev>> if (test_bit(NULLB_DEV_FL_THROTTLED, &nullb->dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|2156| <<null_add_dev>> set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
+	 */
 	NULLB_DEV_FL_THROTTLED	= 2,
+	/*
+	 * 在以下使用NULLB_DEV_FL_CACHE:
+	 *   - drivers/block/null_blk_main.c|574| <<null_cache_active>> return test_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 *   - drivers/block/null_blk_main.c|2161| <<null_add_dev>> set_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 */
 	NULLB_DEV_FL_CACHE	= 3,
 };
 
+/*
+ * 一个page中sector的数量加上2
+ * The highest 2 bits of bitmap are for special purpose. LOCK means the cache
+ * page is being flushing to storage. FREE means the cache page is freed and
+ * should be skipped from flushing to storage. Please see
+ * null_make_cache_space
+ */
 #define MAP_SZ		((PAGE_SIZE >> SECTOR_SHIFT) + 2)
 /*
  * nullb_page is a page in memory for nullb devices.
@@ -62,18 +123,72 @@ struct nullb_page {
 	struct page *page;
 	DECLARE_BITMAP(bitmap, MAP_SZ);
 };
+/*
+ * 在以下使用NULLB_PAGE_LOCK:
+ *   - drivers/block/null_blk_main.c|1151| <<null_free_page>> if (test_bit(NULLB_PAGE_LOCK, t_page->bitmap))
+ *   - drivers/block/null_blk_main.c|1356| <<null_flush_cache_page>> __clear_bit(NULLB_PAGE_LOCK, c_page->bitmap);
+ *   - drivers/block/null_blk_main.c|1421| <<null_make_cache_space>> if (test_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap))
+ *   - drivers/block/null_blk_main.c|1424| <<null_make_cache_space>> __set_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap);
+ */
 #define NULLB_PAGE_LOCK (MAP_SZ - 1)
+/*
+ * 在以下使用NULLB_PAGE_FREE:
+ *   - drivers/block/null_blk_main.c|1150| <<null_free_page>> __set_bit(NULLB_PAGE_FREE, t_page->bitmap);
+ *   - drivers/block/null_blk_main.c|1357| <<null_flush_cache_page>> if (test_bit(NULLB_PAGE_FREE, c_page->bitmap)) {
+ */
 #define NULLB_PAGE_FREE (MAP_SZ - 2)
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1786| <<null_add_dev>> list_add_tail(&nullb->list, &nullb_list);
+ *   - drivers/block/null_blk_main.c|1889| <<null_init>> while (!list_empty(&nullb_list)) {
+ *   - drivers/block/null_blk_main.c|1890| <<null_init>> nullb = list_entry(nullb_list.next, struct nullb, list);
+ *   - drivers/block/null_blk_main.c|1913| <<null_exit>> while (!list_empty(&nullb_list)) {
+ *   - drivers/block/null_blk_main.c|1916| <<null_exit>> nullb = list_entry(nullb_list.next, struct nullb, list);
+ *
+ * 添加struct nullb
+ */
 static LIST_HEAD(nullb_list);
 static struct mutex lock;
+/*
+ * 使用null_major的地方:
+ *   - drivers/block/null_blk_main.c|1918| <<null_gendisk_register>> disk->major = null_major;
+ *   - drivers/block/null_blk_main.c|2252| <<null_init>> null_major = register_blkdev(0, "nullb");
+ *   - drivers/block/null_blk_main.c|2253| <<null_init>> if (null_major < 0) {
+ *   - drivers/block/null_blk_main.c|2254| <<null_init>> ret = null_major;
+ *   - drivers/block/null_blk_main.c|2281| <<null_init>> unregister_blkdev(null_major, "nullb");
+ *   - drivers/block/null_blk_main.c|2296| <<null_exit>> unregister_blkdev(null_major, "nullb");
+ */
 static int null_major;
+/*
+ * 在以下使用nullb_indexes:
+ *   - drivers/block/null_blk_main.c|1790| <<null_del_dev>> ida_simple_remove(&nullb_indexes, nullb->index);
+ *   - drivers/block/null_blk_main.c|2205| <<null_add_dev>> nullb->index = ida_simple_get(&nullb_indexes, 0, 0, GFP_KERNEL);
+ */
 static DEFINE_IDA(nullb_indexes);
 static struct blk_mq_tag_set tag_set;
 
 enum {
+	/*
+	 * 使用NULL_IRQ_NONE的地方:
+	 *   - drivers/block/null_blk_main.c|297| <<null_set_irqmode>> return null_param_store_val(str, &g_irqmode, NULL_IRQ_NONE,
+	 *   - drivers/block/null_blk_main.c|1932| <<nullb_complete_cmd>> case NULL_IRQ_NONE:
+	 */
 	NULL_IRQ_NONE		= 0,
+	/*
+	 * 使用NULL_IRQ_SOFTIRQ的地方:
+	 *   - drivers/block/null_blk_main.c|293| <<global>> static int g_irqmode = NULL_IRQ_SOFTIRQ;
+	 *   - drivers/block/null_blk_main.c|1914| <<nullb_complete_cmd>> case NULL_IRQ_SOFTIRQ:
+	 */
 	NULL_IRQ_SOFTIRQ	= 1,
+	/*
+	 * 使用NULL_IRQ_TIMER的地方:
+	 *   - drivers/block/null_blk_main.c|298| <<null_set_irqmode>> NULL_IRQ_TIMER);
+	 *   - drivers/block/null_blk_main.c|982| <<__alloc_cmd>> if (nq->dev->irqmode == NULL_IRQ_TIMER) {
+	 *   - drivers/block/null_blk_main.c|1940| <<nullb_complete_cmd>> case NULL_IRQ_TIMER:
+	 *   - drivers/block/null_blk_main.c|2117| <<null_queue_rq>> if (nq->dev->irqmode == NULL_IRQ_TIMER) {
+	 *   - drivers/block/null_blk_main.c|2470| <<null_validate_conf>> dev->irqmode = min_t(unsigned int , dev->irqmode, NULL_IRQ_TIMER);
+	 */
 	NULL_IRQ_TIMER		= 2,
 };
 
@@ -83,10 +198,27 @@ enum {
 	NULL_Q_MQ		= 2,
 };
 
+/*
+ * 在以下使用g_no_sched:
+ *   - drivers/block/null_blk_main.c|128| <<global>> module_param_named(no_sched, g_no_sched, int , 0444);
+ *   - drivers/block/null_blk_main.c|2001| <<null_init_tag_set>> if (g_no_sched)
+ */
 static int g_no_sched;
 module_param_named(no_sched, g_no_sched, int, 0444);
 MODULE_PARM_DESC(no_sched, "No io scheduler");
 
+/*
+ * 在一下使用g_submit_queues:
+ *   - drivers/block/null_blk_main.c|132| <<global>> module_param_named(submit_queues, g_submit_queues, int , 0444);
+ *   - drivers/block/null_blk_main.c|622| <<null_alloc_dev>> dev->submit_queues = g_submit_queues;
+ *   - drivers/block/null_blk_main.c|1995| <<null_init_tag_set>> g_submit_queues;
+ *   - drivers/block/null_blk_main.c|2270| <<null_init>> if (g_submit_queues != nr_online_nodes) {
+ *   - drivers/block/null_blk_main.c|2273| <<null_init>> g_submit_queues = nr_online_nodes;
+ *   - drivers/block/null_blk_main.c|2275| <<null_init>> } else if (g_submit_queues > nr_cpu_ids)
+ *   - drivers/block/null_blk_main.c|2276| <<null_init>> g_submit_queues = nr_cpu_ids;
+ *   - drivers/block/null_blk_main.c|2277| <<null_init>> else if (g_submit_queues <= 0)
+ *   - drivers/block/null_blk_main.c|2278| <<null_init>> g_submit_queues = 1;
+ */
 static int g_submit_queues = 1;
 module_param_named(submit_queues, g_submit_queues, int, 0444);
 MODULE_PARM_DESC(submit_queues, "Number of submission queues");
@@ -96,15 +228,43 @@ module_param_named(home_node, g_home_node, int, 0444);
 MODULE_PARM_DESC(home_node, "Home node for the device");
 
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
+/*
+ * 在以下使用g_timeout_str:
+ *   - drivers/block/null_blk_main.c|173| <<global>> module_param_string(timeout, g_timeout_str, sizeof(g_timeout_str), 0444);
+ *   - drivers/block/null_blk_main.c|1732| <<should_timeout_request>> if (g_timeout_str[0])
+ *   - drivers/block/null_blk_main.c|2161| <<null_setup_fault>> if (!__null_setup_fault(&null_timeout_attr, g_timeout_str))
+ *
+ * 设置了"null_blk.timeout=20,100,0,5", 在null_setup_fault()初始化
+ * [  202.328984] null_blk: rq 000000009163ec16 timed out
+ * [  202.328993] null_blk: rq 00000000f9644e82 timed out
+ * [  202.328995] null_blk: rq 00000000d8e5c61a timed out
+ * [  202.328997] null_blk: rq 00000000fafa43fd timed out
+ * [  202.328999] null_blk: rq 00000000f0eb258c timed out
+ * 查看Documentation/fault-injection/fault-injection.rst
+ */
 static char g_timeout_str[80];
 module_param_string(timeout, g_timeout_str, sizeof(g_timeout_str), 0444);
 
+/*
+ * 在以下使用g_requeue_str:
+ *   - drivers/block/null_blk_main.c|176| <<global>> module_param_string(requeue, g_requeue_str, sizeof(g_requeue_str), 0444);
+ *   - drivers/block/null_blk_main.c|1745| <<should_requeue_request>> if (g_requeue_str[0])
+ *   - drivers/block/null_blk_main.c|2163| <<null_setup_fault>> if (!__null_setup_fault(&null_requeue_attr, g_requeue_str))
+ *
+ * 设置了"null_blk.requeue=20,100,0,5", 在null_setup_fault()初始化
+ * 查看Documentation/fault-injection/fault-injection.rst
+ */
 static char g_requeue_str[80];
 module_param_string(requeue, g_requeue_str, sizeof(g_requeue_str), 0444);
 #endif
 
 static int g_queue_mode = NULL_Q_MQ;
 
+/*
+ * 在以下使用null_param_store_val:
+ *   - drivers/block/null_blk_main.c|235| <<null_set_queue_mode>> return null_param_store_val(str, &g_queue_mode, NULL_Q_BIO, NULL_Q_MQ);
+ *   - drivers/block/null_blk_main.c|280| <<null_set_irqmode>> return null_param_store_val(str, &g_irqmode, NULL_IRQ_NONE,
+ */
 static int null_param_store_val(const char *str, int *val, int min, int max)
 {
 	int ret, new_val;
@@ -125,6 +285,10 @@ static int null_set_queue_mode(const char *str, const struct kernel_param *kp)
 	return null_param_store_val(str, &g_queue_mode, NULL_Q_BIO, NULL_Q_MQ);
 }
 
+/*
+ * 仅在以下使用null_queue_mode_param_ops:
+ *   - drivers/block/null_blk_main.c|206| <<global>> device_param_cb(queue_mode, &null_queue_mode_param_ops, &g_queue_mode, 0444);
+ */
 static const struct kernel_param_ops null_queue_mode_param_ops = {
 	.set	= null_set_queue_mode,
 	.get	= param_get_int,
@@ -145,6 +309,12 @@ static unsigned int nr_devices = 1;
 module_param(nr_devices, uint, 0444);
 MODULE_PARM_DESC(nr_devices, "Number of devices to register");
 
+/*
+ * 在以下使用g_blocking:
+ *   - drivers/block/null_blk_main.c|222| <<global>> module_param_named(blocking, g_blocking, bool, 0444);
+ *   - drivers/block/null_blk_main.c|660| <<null_alloc_dev>> dev->blocking = g_blocking;
+ *   - drivers/block/null_blk_main.c|2037| <<null_init_tag_set>> if ((nullb && nullb->dev->blocking) || g_blocking)
+ */
 static bool g_blocking;
 module_param_named(blocking, g_blocking, bool, 0444);
 MODULE_PARM_DESC(blocking, "Register as a blocking blk-mq driver device");
@@ -161,6 +331,10 @@ static int null_set_irqmode(const char *str, const struct kernel_param *kp)
 					NULL_IRQ_TIMER);
 }
 
+/*
+ * 在以下使用null_irqmode_param_ops:
+ *   - drivers/block/null_blk_main.c|242| <<global>> device_param_cb(irqmode, &null_irqmode_param_ops, &g_irqmode, 0444);
+ */
 static const struct kernel_param_ops null_irqmode_param_ops = {
 	.set	= null_set_irqmode,
 	.get	= param_get_int,
@@ -169,6 +343,11 @@ static const struct kernel_param_ops null_irqmode_param_ops = {
 device_param_cb(irqmode, &null_irqmode_param_ops, &g_irqmode, 0444);
 MODULE_PARM_DESC(irqmode, "IRQ completion handler. 0-none, 1-softirq, 2-timer");
 
+/*
+ * 在以下使用g_completion_nsec:
+ *   - drivers/block/null_blk_main.c|302| <<global>> module_param_named(completion_nsec, g_completion_nsec, ulong, 0444);
+ *   - drivers/block/null_blk_main.c|715| <<null_alloc_dev>> dev->completion_nsec = g_completion_nsec;
+ */
 static unsigned long g_completion_nsec = 10000;
 module_param_named(completion_nsec, g_completion_nsec, ulong, 0444);
 MODULE_PARM_DESC(completion_nsec, "Time in ns to complete a request in hardware. Default: 10,000ns");
@@ -177,6 +356,12 @@ static int g_hw_queue_depth = 64;
 module_param_named(hw_queue_depth, g_hw_queue_depth, int, 0444);
 MODULE_PARM_DESC(hw_queue_depth, "Queue depth for each hardware queue. Default: 64");
 
+/*
+ * 在以下使用g_use_per_node_hctx:
+ *   - drivers/block/null_blk_main.c|264| <<global>> module_param_named(use_per_node_hctx, g_use_per_node_hctx, bool, 0444);
+ *   - drivers/block/null_blk_main.c|671| <<null_alloc_dev>> dev->use_per_node_hctx = g_use_per_node_hctx;
+ *   - drivers/block/null_blk_main.c|2311| <<null_init>> if (g_queue_mode == NULL_Q_MQ && g_use_per_node_hctx) {
+ */
 static bool g_use_per_node_hctx;
 module_param_named(use_per_node_hctx, g_use_per_node_hctx, bool, 0444);
 MODULE_PARM_DESC(use_per_node_hctx, "Use per-node allocation for hardware context queues. Default: false");
@@ -189,6 +374,11 @@ static unsigned long g_zone_size = 256;
 module_param_named(zone_size, g_zone_size, ulong, S_IRUGO);
 MODULE_PARM_DESC(zone_size, "Zone size in MB when block device is zoned. Must be power-of-two: Default: 256");
 
+/*
+ * 在以下使用g_zone_nr_conv:
+ *   - drivers/block/null_blk_main.c|328| <<global>> module_param_named(zone_nr_conv, g_zone_nr_conv, uint, 0444);
+ *   - drivers/block/null_blk_main.c|726| <<null_alloc_dev>> dev->zone_nr_conv = g_zone_nr_conv;
+ */
 static unsigned int g_zone_nr_conv;
 module_param_named(zone_nr_conv, g_zone_nr_conv, uint, 0444);
 MODULE_PARM_DESC(zone_nr_conv, "Number of conventional zones when block device is zoned. Default: 0");
@@ -204,22 +394,26 @@ static inline struct nullb_device *to_nullb_device(struct config_item *item)
 	return item ? container_of(item, struct nullb_device, item) : NULL;
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的show, 类型uint */
 static inline ssize_t nullb_device_uint_attr_show(unsigned int val, char *page)
 {
 	return snprintf(page, PAGE_SIZE, "%u\n", val);
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的show, 类型ulong */
 static inline ssize_t nullb_device_ulong_attr_show(unsigned long val,
 	char *page)
 {
 	return snprintf(page, PAGE_SIZE, "%lu\n", val);
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的show, 类型bool */
 static inline ssize_t nullb_device_bool_attr_show(bool val, char *page)
 {
 	return snprintf(page, PAGE_SIZE, "%u\n", val);
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的store, 类型uint */
 static ssize_t nullb_device_uint_attr_store(unsigned int *val,
 	const char *page, size_t count)
 {
@@ -234,6 +428,7 @@ static ssize_t nullb_device_uint_attr_store(unsigned int *val,
 	return count;
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的store, 类型ulong */
 static ssize_t nullb_device_ulong_attr_store(unsigned long *val,
 	const char *page, size_t count)
 {
@@ -248,6 +443,7 @@ static ssize_t nullb_device_ulong_attr_store(unsigned long *val,
 	return count;
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的store, 类型bool */
 static ssize_t nullb_device_bool_attr_store(bool *val, const char *page,
 	size_t count)
 {
@@ -263,6 +459,27 @@ static ssize_t nullb_device_bool_attr_store(bool *val, const char *page,
 }
 
 /* The following macro should only be used with TYPE = {uint, ulong, bool}. */
+/*
+ * 在以下使用NULLB_DEVICE_ATTR():
+ *   - drivers/block/null_blk_main.c|451| <<global>> NULLB_DEVICE_ATTR(size, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|452| <<global>> NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|453| <<global>> NULLB_DEVICE_ATTR(submit_queues, uint, nullb_apply_submit_queues);
+ *   - drivers/block/null_blk_main.c|454| <<global>> NULLB_DEVICE_ATTR(home_node, uint, NULL);
+ *   - drivers/block/null_blk_main.c|455| <<global>> NULLB_DEVICE_ATTR(queue_mode, uint, NULL);
+ *   - drivers/block/null_blk_main.c|456| <<global>> NULLB_DEVICE_ATTR(blocksize, uint, NULL);
+ *   - drivers/block/null_blk_main.c|457| <<global>> NULLB_DEVICE_ATTR(irqmode, uint, NULL);
+ *   - drivers/block/null_blk_main.c|458| <<global>> NULLB_DEVICE_ATTR(hw_queue_depth, uint, NULL);
+ *   - drivers/block/null_blk_main.c|459| <<global>> NULLB_DEVICE_ATTR(index, uint, NULL);
+ *   - drivers/block/null_blk_main.c|460| <<global>> NULLB_DEVICE_ATTR(blocking, bool, NULL);
+ *   - drivers/block/null_blk_main.c|461| <<global>> NULLB_DEVICE_ATTR(use_per_node_hctx, bool, NULL);
+ *   - drivers/block/null_blk_main.c|462| <<global>> NULLB_DEVICE_ATTR(memory_backed, bool, NULL);
+ *   - drivers/block/null_blk_main.c|463| <<global>> NULLB_DEVICE_ATTR(discard, bool, NULL);
+ *   - drivers/block/null_blk_main.c|464| <<global>> NULLB_DEVICE_ATTR(mbps, uint, NULL);
+ *   - drivers/block/null_blk_main.c|465| <<global>> NULLB_DEVICE_ATTR(cache_size, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|466| <<global>> NULLB_DEVICE_ATTR(zoned, bool, NULL);
+ *   - drivers/block/null_blk_main.c|467| <<global>> NULLB_DEVICE_ATTR(zone_size, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|468| <<global>> NULLB_DEVICE_ATTR(zone_nr_conv, uint, NULL);
+ */
 #define NULLB_DEVICE_ATTR(NAME, TYPE, APPLY)					\
 static ssize_t									\
 nullb_device_##NAME##_show(struct config_item *item, char *page)		\
@@ -293,6 +510,12 @@ nullb_device_##NAME##_store(struct config_item *item, const char *page,		\
 }										\
 CONFIGFS_ATTR(nullb_device_, NAME);
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|329| <<global>> NULLB_DEVICE_ATTR(submit_queues, uint, nullb_apply_submit_queues);
+ *
+ * nullb_device_submit_queues_store()
+ */
 static int nullb_apply_submit_queues(struct nullb_device *dev,
 				     unsigned int submit_queues)
 {
@@ -307,6 +530,53 @@ static int nullb_apply_submit_queues(struct nullb_device *dev,
 	return set->nr_hw_queues == submit_queues ? 0 : -ENOMEM;
 }
 
+/*
+ * commit 3bf2bd20734e3e6ffda53719a9c10fb3ee9c5ffa
+ * Author: Shaohua Li <shli@fb.com>
+ * Date:   Mon Aug 14 15:04:53 2017 -0700
+ * 
+ * nullb: add configfs interface
+ *
+ * Add configfs interface for nullb. configfs interface is more flexible
+ * and easy to configure in a per-disk basis.
+ *
+ * Configuration is something like this:
+ * mount -t configfs none /mnt
+ *
+ * Checking which features the driver supports:
+ * cat /mnt/nullb/features
+ *
+ * The 'features' attribute is for future extension. We probably will add
+ * new features into the driver, userspace can check this attribute to find
+ * the supported features.
+ *
+ * Create/remove a device:
+ * mkdir/rmdir /mnt/nullb/a
+ *
+ * Then configure the device by setting attributes under /mnt/nullb/a, most
+ * of nullb supported module parameters are converted to attributes:
+ * size; // device size in MB
+ * completion_nsec; // time in ns to complete a request
+ * submit_queues; // number of submission queues
+ * home_node; // home node for the device
+ * queue_mode; // block interface
+ * blocksize; // block size
+ * irqmode; // IRQ completion handler
+ * hw_queue_depth; // queue depth
+ * use_lightnvm; // register as a LightNVM device
+ * blocking; // blocking blk-mq device
+ * use_per_node_hctx; // use per-node allocation for hardware context
+ *
+ * Note, creating a device doesn't create a disk immediately. Creating a
+ * disk is done in two phases: create a device and then power on the
+ * device. Next patch will introduce device power on.
+ *
+ * Based on original patch from Kyungchan Koh
+ *
+ * Signed-off-by: Kyungchan Koh <kkc6196@fb.com>
+ * Signed-off-by: Shaohua Li <shli@fb.com>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ */
 NULLB_DEVICE_ATTR(size, ulong, NULL);
 NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
 NULLB_DEVICE_ATTR(submit_queues, uint, nullb_apply_submit_queues);
@@ -326,11 +596,13 @@ NULLB_DEVICE_ATTR(zoned, bool, NULL);
 NULLB_DEVICE_ATTR(zone_size, ulong, NULL);
 NULLB_DEVICE_ATTR(zone_nr_conv, uint, NULL);
 
+/* 用在下面的CONFIGFS_ATTR(nullb_device_, power); */
 static ssize_t nullb_device_power_show(struct config_item *item, char *page)
 {
 	return nullb_device_bool_attr_show(to_nullb_device(item)->power, page);
 }
 
+/* 用在下面的CONFIGFS_ATTR(nullb_device_, power); */
 static ssize_t nullb_device_power_store(struct config_item *item,
 				     const char *page, size_t count)
 {
@@ -367,6 +639,7 @@ static ssize_t nullb_device_power_store(struct config_item *item,
 
 CONFIGFS_ATTR(nullb_device_, power);
 
+/* 用在下面的CONFIGFS_ATTR(nullb_device_, badblocks); */
 static ssize_t nullb_device_badblocks_show(struct config_item *item, char *page)
 {
 	struct nullb_device *t_dev = to_nullb_device(item);
@@ -374,6 +647,11 @@ static ssize_t nullb_device_badblocks_show(struct config_item *item, char *page)
 	return badblocks_show(&t_dev->badblocks, page, 0);
 }
 
+/*
+ * 用在下面的CONFIGFS_ATTR(nullb_device_, badblocks);
+ *
+ * 使用方式 "echo "+10240-20470" > badblocks", 代表起始和终止的vector
+ */
 static ssize_t nullb_device_badblocks_store(struct config_item *item,
 				     const char *page, size_t count)
 {
@@ -386,6 +664,7 @@ static ssize_t nullb_device_badblocks_store(struct config_item *item,
 	if (!orig)
 		return -ENOMEM;
 
+	/* Removes leading and trailing whitespace from @s. */
 	buf = strstrip(orig);
 
 	ret = -EINVAL;
@@ -406,6 +685,13 @@ static ssize_t nullb_device_badblocks_store(struct config_item *item,
 		goto out;
 	/* enable badblocks */
 	cmpxchg(&t_dev->badblocks.shift, -1, 0);
+	/*
+	 * badblocks_set() - Add a range of bad blocks to the table.
+	 * @bb:         the badblocks structure that holds all badblock information
+	 * @s:          first sector to mark as bad
+	 * @sectors:    number of sectors to mark as bad
+	 * @acknowledged: weather to mark the bad sectors as acknowledged
+	 */
 	if (buf[0] == '+')
 		ret = badblocks_set(&t_dev->badblocks, start,
 			end - start + 1, 1);
@@ -420,6 +706,11 @@ static ssize_t nullb_device_badblocks_store(struct config_item *item,
 }
 CONFIGFS_ATTR(nullb_device_, badblocks);
 
+/*
+ * struct config_item_type nullb_device_type.ct_attrs = nullb_device_attrs[]
+ *
+ * configfs中所有的参数!!!!
+ */
 static struct configfs_attribute *nullb_device_attrs[] = {
 	&nullb_device_attr_size,
 	&nullb_device_attr_completion_nsec,
@@ -444,6 +735,7 @@ static struct configfs_attribute *nullb_device_attrs[] = {
 	NULL,
 };
 
+/* struct configfs_item_operations nullb_device_ops.release = nullb_device_release() */
 static void nullb_device_release(struct config_item *item)
 {
 	struct nullb_device *dev = to_nullb_device(item);
@@ -452,16 +744,24 @@ static void nullb_device_release(struct config_item *item)
 	null_free_dev(dev);
 }
 
+/* struct config_item_type nullb_device_type.ct_item_ops = &nullb_device_ops */
 static struct configfs_item_operations nullb_device_ops = {
 	.release	= nullb_device_release,
 };
 
+/*
+ * 在以下使用nullb_device_type:
+ *   - drivers/block/null_blk_main.c|711| <<nullb_group_make_item>> config_item_init_type_name(&dev->item, name, &nullb_device_type);
+ */
 static const struct config_item_type nullb_device_type = {
 	.ct_item_ops	= &nullb_device_ops,
 	.ct_attrs	= nullb_device_attrs,
 	.ct_owner	= THIS_MODULE,
 };
 
+/*
+ * struct configfs_group_operations nullb_group_ops.make_item = nullb_group_make_item()
+ */
 static struct
 config_item *nullb_group_make_item(struct config_group *group, const char *name)
 {
@@ -476,6 +776,9 @@ config_item *nullb_group_make_item(struct config_group *group, const char *name)
 	return &dev->item;
 }
 
+/*
+ * struct configfs_group_operations nullb_group_ops.drop_item = nullb_group_drop_item()
+ */
 static void
 nullb_group_drop_item(struct config_group *group, struct config_item *item)
 {
@@ -514,6 +817,14 @@ static const struct config_item_type nullb_group_type = {
 	.ct_owner	= THIS_MODULE,
 };
 
+/*
+ * 使用nullb_subsys的地方:
+ *   - drivers/block/null_blk_main.c|2176| <<null_init>> config_group_init(&nullb_subsys.su_group);
+ *   - drivers/block/null_blk_main.c|2177| <<null_init>> mutex_init(&nullb_subsys.su_mutex);
+ *   - drivers/block/null_blk_main.c|2179| <<null_init>> ret = configfs_register_subsystem(&nullb_subsys);
+ *   - drivers/block/null_blk_main.c|2219| <<null_init>> configfs_unregister_subsystem(&nullb_subsys);
+ *   - drivers/block/null_blk_main.c|2230| <<null_exit>> configfs_unregister_subsystem(&nullb_subsys);
+ */
 static struct configfs_subsystem nullb_subsys = {
 	.su_group = {
 		.cg_item = {
@@ -523,11 +834,36 @@ static struct configfs_subsystem nullb_subsys = {
 	},
 };
 
+/*
+ * 上面都是configfs的, 下面是核心代码了!!!
+ */
+
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1098| <<copy_to_nullb>> if (null_cache_active(nullb) && !is_fua)
+ *   - drivers/block/null_blk_main.c|1103| <<copy_to_nullb>> !null_cache_active(nullb) || is_fua);
+ *   - drivers/block/null_blk_main.c|1141| <<copy_from_nullb>> !null_cache_active(nullb));
+ *   - drivers/block/null_blk_main.c|1187| <<null_handle_discard>> if (null_cache_active(nullb))
+ *   - drivers/block/null_blk_main.c|1203| <<null_handle_flush>> if (!null_cache_active(nullb))
+ *   - drivers/block/null_blk_main.c|1705| <<null_del_dev>> if (null_cache_active(nullb))
+ */
 static inline int null_cache_active(struct nullb *nullb)
 {
+	/*
+	 * 在以下使用NULLB_DEV_FL_CACHE:
+	 *   - drivers/block/null_blk_main.c|574| <<null_cache_active>> return test_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 *   - drivers/block/null_blk_main.c|2161| <<null_add_dev>> set_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 *
+	 * Device is using a write-back cache.
+	 */
 	return test_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|470| <<nullb_group_make_item>> dev = null_alloc_dev();
+ *   - drivers/block/null_blk_main.c|1852| <<null_init>> dev = null_alloc_dev();
+ */
 static struct nullb_device *null_alloc_dev(void)
 {
 	struct nullb_device *dev;
@@ -543,6 +879,12 @@ static struct nullb_device *null_alloc_dev(void)
 	}
 
 	dev->size = g_gb * 1024;
+	/*
+	 * 在以下使用completion_nsec:
+	 *   - drivers/block/null_blk_main.c|452| <<global>> NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
+	 *   - drivers/block/null_blk_main.c|715| <<null_alloc_dev>> dev->completion_nsec = g_completion_nsec;
+	 *   - drivers/block/null_blk_main.c|930| <<null_cmd_end_timer>> ktime_t kt = cmd->nq->dev->completion_nsec;
+	 */
 	dev->completion_nsec = g_completion_nsec;
 	dev->submit_queues = g_submit_queues;
 	dev->home_node = g_home_node;
@@ -551,6 +893,11 @@ static struct nullb_device *null_alloc_dev(void)
 	dev->irqmode = g_irqmode;
 	dev->hw_queue_depth = g_hw_queue_depth;
 	dev->blocking = g_blocking;
+	/*
+	 * 在以下使用null_device->use_per_mpde_hctx:
+	 *   - drivers/block/null_blk_main.c|671| <<null_alloc_dev>> dev->use_per_node_hctx = g_use_per_node_hctx;
+	 *   - drivers/block/null_blk_main.c|2083| <<null_validate_conf>> if (dev->queue_mode == NULL_Q_MQ && dev->use_per_node_hctx) {
+	 */
 	dev->use_per_node_hctx = g_use_per_node_hctx;
 	dev->zoned = g_zoned;
 	dev->zone_size = g_zone_size;
@@ -558,6 +905,13 @@ static struct nullb_device *null_alloc_dev(void)
 	return dev;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|460| <<nullb_device_release>> null_free_dev(dev);
+ *   - drivers/block/null_blk_main.c|2202| <<null_init>> null_free_dev(dev);
+ *   - drivers/block/null_blk_main.c|2215| <<null_init>> null_free_dev(dev);
+ *   - drivers/block/null_blk_main.c|2241| <<null_exit>> null_free_dev(dev);
+ */
 static void null_free_dev(struct nullb_device *dev)
 {
 	if (!dev)
@@ -568,19 +922,50 @@ static void null_free_dev(struct nullb_device *dev)
 	kfree(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|622| <<free_cmd>> put_tag(cmd->nq, cmd->tag);
+ *
+ * end_cmd()
+ *  -> free_cmd()
+ *      -> put_tag()
+ *
+ * 清空nullb_queue->tag_map中tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */
 static void put_tag(struct nullb_queue *nq, unsigned int tag)
 {
+	/*
+	 * Clear a bit in memory, for unlock
+	 */
+	/*
+	 * struct nullb_queue:
+	 *  -> unsigned long *tag_map;
+	 */
 	clear_bit_unlock(tag, nq->tag_map);
 
+	/*
+	 * 在alloc_cmd()中可能等待在nq->wait
+	 */
 	if (waitqueue_active(&nq->wait))
 		wake_up(&nq->wait);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|622| <<__alloc_cmd>> tag = get_tag(nq);
+ *
+ * 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+ */
 static unsigned int get_tag(struct nullb_queue *nq)
 {
 	unsigned int tag;
 
 	do {
+		/*
+		 * Find the first cleared bit in a memory region.
+		 * 这里的memory region就是nq->tag_map
+		 */
 		tag = find_first_zero_bit(nq->tag_map, nq->queue_depth);
 		if (tag >= nq->queue_depth)
 			return -1U;
@@ -589,6 +974,17 @@ static unsigned int get_tag(struct nullb_queue *nq)
 	return tag;
 }
 
+/*
+ * called only by:
+ *   - drivers/block/null_blk_main.c|685| <<end_cmd>> free_cmd(cmd);
+ *
+ * end_cmd()
+ *  -> free_cmd()
+ *      -> put_tag()
+ *
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */ 
 static void free_cmd(struct nullb_cmd *cmd)
 {
 	put_tag(cmd->nq, cmd->tag);
@@ -596,13 +992,37 @@ static void free_cmd(struct nullb_cmd *cmd)
 
 static enum hrtimer_restart null_cmd_timer_expired(struct hrtimer *timer);
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|630| <<alloc_cmd>> cmd = __alloc_cmd(nq);
+ *   - drivers/block/null_blk_main.c|636| <<alloc_cmd>> cmd = __alloc_cmd(nq);
+ *
+ * null_queue_bio()
+ *  -> alloc_cmd()
+ *      -> __alloc_cmd()
+ *
+ * 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+ * 然后返回nullb_queue->cmd[tag]
+ */
 static struct nullb_cmd *__alloc_cmd(struct nullb_queue *nq)
 {
 	struct nullb_cmd *cmd;
 	unsigned int tag;
 
+	/* 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag */
 	tag = get_tag(nq);
 	if (tag != -1U) {
+		/*
+		 * struct nullb_queue {
+		 *	unsigned long *tag_map;
+		 *	wait_queue_head_t wait;
+		 *	unsigned int queue_depth;
+		 *	struct nullb_device *dev;
+		 *	unsigned int requeue_selection;
+		 *
+		 *	struct nullb_cmd *cmds;
+		 * };
+		 */
 		cmd = &nq->cmds[tag];
 		cmd->tag = tag;
 		cmd->nq = nq;
@@ -617,11 +1037,27 @@ static struct nullb_cmd *__alloc_cmd(struct nullb_queue *nq)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1346| <<null_queue_bio>> cmd = alloc_cmd(nq, 1);
+ *
+ * null_queue_bio()
+ *  -> alloc_cmd()
+ *      -> __alloc_cmd()
+ *
+ * 核心思想是在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+ * 然后返回nullb_queue->cmd[tag]
+ * 分配不到的话如果can_wait设置了会睡眠 (put_tag()会唤醒)
+ */
 static struct nullb_cmd *alloc_cmd(struct nullb_queue *nq, int can_wait)
 {
 	struct nullb_cmd *cmd;
 	DEFINE_WAIT(wait);
 
+	/*
+	 * 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+	 * 然后返回nullb_queue->cmd[tag]
+	 */
 	cmd = __alloc_cmd(nq);
 	if (cmd || !can_wait)
 		return cmd;
@@ -639,12 +1075,46 @@ static struct nullb_cmd *alloc_cmd(struct nullb_queue *nq, int can_wait)
 	return cmd;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|666| <<null_cmd_timer_expired>> end_cmd(container_of(timer, struct nullb_cmd, timer));
+ *   - drivers/block/null_blk_main.c|680| <<null_complete_rq>> end_cmd(blk_mq_rq_to_pdu(rq));
+ *   - drivers/block/null_blk_main.c|1253| <<nullb_complete_cmd>> end_cmd(cmd);
+ *   - drivers/block/null_blk_main.c|1258| <<nullb_complete_cmd>> end_cmd(cmd);
+ *
+ * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */
 static void end_cmd(struct nullb_cmd *cmd)
 {
 	int queue_mode = cmd->nq->dev->queue_mode;
 
 	switch (queue_mode)  {
 	case NULL_Q_MQ:
+		/*
+		 * 部分调用blk_mq_end_request()的例子:
+		 *   - block/blk-flush.c|421| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+		 *   - block/blk-flush.c|774| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+		 *   - block/blk-mq.c|1366| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+		 *   - block/blk-mq.c|1979| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+		 *   - block/blk-mq.c|2015| <<blk_mq_try_issue_list_directly>> blk_mq_end_request(rq, ret);
+		 *   - block/bsg-lib.c|158| <<bsg_teardown_job>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/block/loop.c|487| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+		 *   - drivers/block/nbd.c|340| <<nbd_complete_rq>> blk_mq_end_request(req, cmd->status);
+		 *   - drivers/block/null_blk_main.c|677| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+		 *   - drivers/block/virtio_blk.c|226| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+		 *   - drivers/block/xen-blkfront.c|918| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+		 *   - drivers/block/xen-blkfront.c|2113| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+		 *   - drivers/ide/ide-cd.c|765| <<cdrom_newpc_intr>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/ide/ide-pm.c|50| <<ide_pm_execute_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/ide/ide-pm.c|220| <<ide_complete_pm_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/md/dm-rq.c|174| <<dm_end_request>> blk_mq_end_request(rq, error);
+		 *   - drivers/md/dm-rq.c|271| <<dm_softirq_done>> blk_mq_end_request(rq, tio->error);
+		 *   - drivers/nvme/host/core.c|307| <<nvme_complete_rq>> blk_mq_end_request(req, status);
+		 *   - drivers/nvme/host/multipath.c|76| <<nvme_failover_req>> blk_mq_end_request(req, 0);
+		 *   - drivers/scsi/scsi_transport_fc.c|3581| <<fc_bsg_job_timeout>> blk_mq_end_request(req, BLK_STS_IOERR);
+		 */
 		blk_mq_end_request(cmd->rq, cmd->error);
 		return;
 	case NULL_Q_BIO:
@@ -653,16 +1123,47 @@ static void end_cmd(struct nullb_cmd *cmd)
 		break;
 	}
 
+	/*
+	 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+	 * 根据情况唤醒nullb_queue->wait
+	 */
 	free_cmd(cmd);
 }
 
+/*
+ * 在null_cmd_end_timer()触发()
+ *
+ * 使用null_cmd_timer_expired()的地方:
+ *   - drivers/block/null_blk_main.c|660| <<__alloc_cmd>> cmd->timer.function = null_cmd_timer_expired;
+ *   - drivers/block/null_blk_main.c|1611| <<null_queue_rq>> cmd->timer.function = null_cmd_timer_expired;
+ *
+ * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ * 返回HRTIMER_NORESTART
+ */
 static enum hrtimer_restart null_cmd_timer_expired(struct hrtimer *timer)
 {
+	/*
+	 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+	 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+	 * 根据情况唤醒nullb_queue->wait
+	 */
 	end_cmd(container_of(timer, struct nullb_cmd, timer));
 
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1447| <<nullb_complete_cmd>> null_cmd_end_timer(cmd);
+ *
+ * 调用null_cmd_timer_expired()来...
+ * ... 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ * 返回HRTIMER_NORESTART
+ */
 static void null_cmd_end_timer(struct nullb_cmd *cmd)
 {
 	ktime_t kt = cmd->nq->dev->completion_nsec;
@@ -670,13 +1171,37 @@ static void null_cmd_end_timer(struct nullb_cmd *cmd)
 	hrtimer_start(&cmd->timer, kt, HRTIMER_MODE_REL);
 }
 
+/*
+ * struct blk_mq_ops null_mq_ops.complete = null_complete_rq()
+ *
+ * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */
 static void null_complete_rq(struct request *rq)
 {
 	end_cmd(blk_mq_rq_to_pdu(rq));
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|862| <<null_insert_page>> t_page = null_alloc_page(GFP_NOIO);
+ *
+ * 分配一个nullb_page(包括nullb_page->page), 初始化nullb_pabe->bitmap
+ */
 static struct nullb_page *null_alloc_page(gfp_t gfp_flags)
 {
+	/*
+	 * struct nullb_page {
+	 *	struct page *page;
+	 *	DECLARE_BITMAP(bitmap, MAP_SZ);
+	 * };
+	 *
+	 * The highest 2 bits of bitmap are for special purpose. LOCK means the cache
+	 * page is being flushing to storage. FREE means the cache page is freed and
+	 * should be skipped from flushing to storage. Please see
+	 * null_make_cache_space
+	 */
 	struct nullb_page *t_page;
 
 	t_page = kmalloc(sizeof(struct nullb_page), gfp_flags);
@@ -695,6 +1220,18 @@ static struct nullb_page *null_alloc_page(gfp_t gfp_flags)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|826| <<null_free_sector>> null_free_page(ret);
+ *   - drivers/block/null_blk_main.c|845| <<null_radix_tree_insert>> null_free_page(t_page);
+ *   - drivers/block/null_blk_main.c|878| <<null_free_device_storage>> null_free_page(ret);
+ *   - drivers/block/null_blk_main.c|966| <<null_insert_page>> null_free_page(t_page);
+ *   - drivers/block/null_blk_main.c|990| <<null_flush_cache_page>> null_free_page(c_page);
+ *   - drivers/block/null_blk_main.c|994| <<null_flush_cache_page>> null_free_page(t_page);
+ *   - drivers/block/null_blk_main.c|1019| <<null_flush_cache_page>> null_free_page(ret);
+ *
+ * "根据情况"释放nullb_page
+ */
 static void null_free_page(struct nullb_page *t_page)
 {
 	__set_bit(NULLB_PAGE_FREE, t_page->bitmap);
@@ -704,6 +1241,13 @@ static void null_free_page(struct nullb_page *t_page)
 	kfree(t_page);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|823| <<null_free_sector>> if (null_page_empty(t_page)) {
+ *   - drivers/block/null_blk_main.c|991| <<null_flush_cache_page>> if (t_page && null_page_empty(t_page)) {
+ *
+ * 根据nullb_page->bitmap判断是否还有可用的sector
+ */
 static bool null_page_empty(struct nullb_page *page)
 {
 	int size = MAP_SZ - 2;
@@ -711,6 +1255,20 @@ static bool null_page_empty(struct nullb_page *page)
 	return find_first_bit(page->bitmap, size) == size;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1116| <<copy_to_nullb>> null_free_sector(nullb, sector, true);
+ *   - drivers/block/null_blk_main.c|1186| <<null_handle_discard>> null_free_sector(nullb, sector, false);
+ *   - drivers/block/null_blk_main.c|1188| <<null_handle_discard>> null_free_sector(nullb, sector, true);
+ *
+ * struct nullb:
+ *  -> struct nullb_device *dev;
+ *      -> struct radix_tree_root data; // data stored in the disk
+ *      -> struct radix_tree_root cache; // disk cache data
+ * null_free_sector()根据参数is_cache决定用data还是cache的radix tree
+ * 找到sector对应的nullb_page, 清空对应nullb_page->bitmap
+ * 如果都清空了, 在radix tree上拿走并释放
+ */
 static void null_free_sector(struct nullb *nullb, sector_t sector,
 	bool is_cache)
 {
@@ -719,14 +1277,26 @@ static void null_free_sector(struct nullb *nullb, sector_t sector,
 	struct nullb_page *t_page, *ret;
 	struct radix_tree_root *root;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
 	idx = sector >> PAGE_SECTORS_SHIFT;
+	/*
+	 * SECTOR_MASK表示page中的mask们
+	 */
 	sector_bit = (sector & SECTOR_MASK);
 
 	t_page = radix_tree_lookup(root, idx);
 	if (t_page) {
 		__clear_bit(sector_bit, t_page->bitmap);
 
+		/*
+		 * 根据nullb_page->bitmap判断是否还有可用的sector
+		 */
 		if (null_page_empty(t_page)) {
 			ret = radix_tree_delete_item(root, idx, t_page);
 			WARN_ON(ret != t_page);
@@ -737,11 +1307,28 @@ static void null_free_sector(struct nullb *nullb, sector_t sector,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|936| <<null_insert_page>> t_page = null_radix_tree_insert(nullb, idx, t_page, !ignore_cache);
+ *
+ * struct nullb:
+ *  -> struct nullb_device *dev;
+ *      -> struct radix_tree_root data; // data stored in the disk
+ *      -> struct radix_tree_root cache; // disk cache data
+ * null_radix_tree_insert()根据参数is_cache决定用data还是cache的radix tree
+ * 把nullb_page插入radix tree
+ */
 static struct nullb_page *null_radix_tree_insert(struct nullb *nullb, u64 idx,
 	struct nullb_page *t_page, bool is_cache)
 {
 	struct radix_tree_root *root;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
 
 	if (radix_tree_insert(root, idx, t_page)) {
@@ -754,6 +1341,13 @@ static struct nullb_page *null_radix_tree_insert(struct nullb *nullb, u64 idx,
 	return t_page;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|459| <<nullb_device_release>> null_free_device_storage(dev, false);
+ *   - drivers/block/null_blk_main.c|1621| <<null_del_dev>> null_free_device_storage(nullb->dev, true);
+ *
+ * 释放nullb_device上某个radix tree的所有page
+ */
 static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 {
 	unsigned long pos = 0;
@@ -761,6 +1355,12 @@ static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 	struct nullb_page *ret, *t_pages[FREE_BATCH];
 	struct radix_tree_root *root;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &dev->cache : &dev->data;
 
 	do {
@@ -773,6 +1373,7 @@ static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 			pos = t_pages[i]->page->index;
 			ret = radix_tree_delete_item(root, pos, t_pages[i]);
 			WARN_ON(ret != t_pages[i]);
+			/* "根据情况"释放nullb_page */
 			null_free_page(ret);
 		}
 
@@ -783,6 +1384,17 @@ static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 		dev->curr_cache = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|906| <<null_lookup_page>> page = __null_lookup_page(nullb, sector, for_write, true);
+ *   - drivers/block/null_blk_main.c|909| <<null_lookup_page>> return __null_lookup_page(nullb, sector, for_write, false);
+ *
+ * struct nullb:
+ *  -> struct nullb_device *dev;
+ *      -> struct radix_tree_root data; // data stored in the disk
+ *      -> struct radix_tree_root cache; // disk cache data
+ * 根据参数is_cache决定用data还是cache的radix tree中搜索nullb_page
+ */
 static struct nullb_page *__null_lookup_page(struct nullb *nullb,
 	sector_t sector, bool for_write, bool is_cache)
 {
@@ -794,6 +1406,12 @@ static struct nullb_page *__null_lookup_page(struct nullb *nullb,
 	idx = sector >> PAGE_SECTORS_SHIFT;
 	sector_bit = (sector & SECTOR_MASK);
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
 	t_page = radix_tree_lookup(root, idx);
 	WARN_ON(t_page && t_page->page->index != idx);
@@ -804,11 +1422,27 @@ static struct nullb_page *__null_lookup_page(struct nullb *nullb,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|920| <<null_insert_page>> t_page = null_lookup_page(nullb, sector, true, ignore_cache);
+ *   - drivers/block/null_blk_main.c|944| <<null_insert_page>> return null_lookup_page(nullb, sector, true, ignore_cache);
+ *   - drivers/block/null_blk_main.c|1106| <<copy_from_nullb>> t_page = null_lookup_page(nullb, sector, false,
+ *
+ * 根据参数ignore_cache决定用data还是cache的radix tree中搜索nullb_page
+ * 如果没有ignore_cahce, 就先在cache中搜索, 再在data搜索
+ */
 static struct nullb_page *null_lookup_page(struct nullb *nullb,
 	sector_t sector, bool for_write, bool ignore_cache)
 {
 	struct nullb_page *page = NULL;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 * 根据参数is_cache决定用data还是cache的radix tree中搜索nullb_page
+	 */
 	if (!ignore_cache)
 		page = __null_lookup_page(nullb, sector, for_write, true);
 	if (page)
@@ -816,6 +1450,11 @@ static struct nullb_page *null_lookup_page(struct nullb *nullb,
 	return __null_lookup_page(nullb, sector, for_write, false);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|957| <<null_flush_cache_page>> t_page = null_insert_page(nullb, idx << PAGE_SECTORS_SHIFT, true);
+ *   - drivers/block/null_blk_main.c|1068| <<copy_to_nullb>> t_page = null_insert_page(nullb, sector,
+ */
 static struct nullb_page *null_insert_page(struct nullb *nullb,
 					   sector_t sector, bool ignore_cache)
 	__releases(&nullb->lock)
@@ -830,6 +1469,9 @@ static struct nullb_page *null_insert_page(struct nullb *nullb,
 
 	spin_unlock_irq(&nullb->lock);
 
+	/*
+	 * 分配一个nullb_page(包括nullb_page->page), 初始化nullb_pabe->bitmap
+	 */
 	t_page = null_alloc_page(GFP_NOIO);
 	if (!t_page)
 		goto out_lock;
@@ -840,6 +1482,14 @@ static struct nullb_page *null_insert_page(struct nullb *nullb,
 	spin_lock_irq(&nullb->lock);
 	idx = sector >> PAGE_SECTORS_SHIFT;
 	t_page->page->index = idx;
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 * null_radix_tree_insert()根据参数is_cache决定用data还是cache的radix tree
+	 * 把nullb_page插入radix tree
+	 */
 	t_page = null_radix_tree_insert(nullb, idx, t_page, !ignore_cache);
 	radix_tree_preload_end();
 
@@ -851,6 +1501,10 @@ static struct nullb_page *null_insert_page(struct nullb *nullb,
 	return null_lookup_page(nullb, sector, true, ignore_cache);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1029| <<null_make_cache_space>> err = null_flush_cache_page(nullb, c_pages[i]);
+ */
 static int null_flush_cache_page(struct nullb *nullb, struct nullb_page *c_page)
 {
 	int i;
@@ -894,12 +1548,20 @@ static int null_flush_cache_page(struct nullb *nullb, struct nullb_page *c_page)
 	kunmap_atomic(src);
 
 	ret = radix_tree_delete_item(&nullb->dev->cache, idx, c_page);
+	/* "根据情况"释放nullb_page */
 	null_free_page(ret);
 	nullb->dev->curr_cache -= PAGE_SIZE;
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1065| <<copy_to_nullb>> null_make_cache_space(nullb, PAGE_SIZE);
+ *   - drivers/block/null_blk_main.c|1174| <<null_handle_flush>> err = null_make_cache_space(nullb,
+ *
+ * 从存储flush掉n大小的cache
+ */
 static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 {
 	int i, err, nr_pages;
@@ -911,6 +1573,17 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 	     nullb->dev->curr_cache + n || nullb->dev->curr_cache == 0)
 		return 0;
 
+	/*
+	 * radix_tree_gang_lookup - perform multiple lookup on a radix tree
+	 * @root:          radix tree root
+	 * @results:       where the results of the lookup are placed
+	 * @first_index:   start the lookup from this key
+	 * @max_items:     place up to this many items at *results
+	 *
+	 * Performs an index-ascending scan of the tree for present items.  Places
+	 * them at *@results and returns the number of items which were placed at
+	 * @results.
+	 */
 	nr_pages = radix_tree_gang_lookup(&nullb->dev->cache,
 			(void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
 	/*
@@ -923,6 +1596,13 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 		 * We found the page which is being flushed to disk by other
 		 * threads
 		 */
+		/*
+		 * 在以下使用NULLB_PAGE_LOCK:
+		 *   - drivers/block/null_blk_main.c|1151| <<null_free_page>> if (test_bit(NULLB_PAGE_LOCK, t_page->bitmap))
+		 *   - drivers/block/null_blk_main.c|1356| <<null_flush_cache_page>> __clear_bit(NULLB_PAGE_LOCK, c_page->bitmap);
+		 *   - drivers/block/null_blk_main.c|1421| <<null_make_cache_space>> if (test_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap))
+		 *   - drivers/block/null_blk_main.c|1424| <<null_make_cache_space>> __set_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap);
+		 */
 		if (test_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap))
 			c_pages[i] = NULL;
 		else
@@ -941,6 +1621,12 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 	flushed += one_round << PAGE_SHIFT;
 
 	if (n > flushed) {
+		/*
+		 * 在以下使用cache_flush_pos:
+		 *   - drivers/block/null_blk_main.c|1551| <<null_make_cache_space>> (void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
+		 *   - drivers/block/null_blk_main.c|1557| <<null_make_cache_space>> nullb->cache_flush_pos = c_pages[i]->page->index;
+		 *   - drivers/block/null_blk_main.c|1581| <<null_make_cache_space>> nullb->cache_flush_pos = 0;
+		 */
 		if (nr_pages == 0)
 			nullb->cache_flush_pos = 0;
 		if (one_round == 0) {
@@ -953,6 +1639,10 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1189| <<null_transfer>> err = copy_to_nullb(nullb, page, off, sector, len, is_fua);
+ */
 static int copy_to_nullb(struct nullb *nullb, struct page *source,
 	unsigned int off, sector_t sector, size_t n, bool is_fua)
 {
@@ -961,9 +1651,20 @@ static int copy_to_nullb(struct nullb *nullb, struct page *source,
 	struct nullb_page *t_page;
 	void *dst, *src;
 
+	/*
+	 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+	 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+	 *
+	 * 由null_cache_active()和is_fua决定是否用cache
+	 */
+
 	while (count < n) {
 		temp = min_t(size_t, nullb->dev->blocksize, n - count);
 
+		/*
+		 * null_cache_active()
+		 * 判断是否支持write-back cache (NULLB_DEV_FL_CACHE)
+		 */
 		if (null_cache_active(nullb) && !is_fua)
 			null_make_cache_space(nullb, PAGE_SIZE);
 
@@ -981,6 +1682,15 @@ static int copy_to_nullb(struct nullb *nullb, struct page *source,
 
 		__set_bit(sector & SECTOR_MASK, t_page->bitmap);
 
+		/*
+		 * struct nullb:
+		 *  -> struct nullb_device *dev;
+		 *      -> struct radix_tree_root data; // data stored in the disk
+		 *      -> struct radix_tree_root cache; // disk cache data
+		 * null_free_sector()根据参数is_cache决定用data还是cache的radix tree
+		 * 找到sector对应的nullb_page, 清空对应nullb_page->bitmap
+		 * 如果都清空了, 在radix tree上拿走并释放
+		 */
 		if (is_fua)
 			null_free_sector(nullb, sector, true);
 
@@ -990,6 +1700,13 @@ static int copy_to_nullb(struct nullb *nullb, struct page *source,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1178| <<null_transfer>> err = copy_from_nullb(nullb, page, off,
+ *
+ * 这里应该是读的工作,如果支持write-back cache (NULLB_DEV_FL_CACHE),就用cache radix tree
+ * 否则用data的radix tree, 把数据读到内存
+ */
 static int copy_from_nullb(struct nullb *nullb, struct page *dest,
 	unsigned int off, sector_t sector, size_t n)
 {
@@ -1002,6 +1719,14 @@ static int copy_from_nullb(struct nullb *nullb, struct page *dest,
 		temp = min_t(size_t, nullb->dev->blocksize, n - count);
 
 		offset = (sector & SECTOR_MASK) << SECTOR_SHIFT;
+		/*
+		 * null_lookup_page():
+		 * 根据参数ignore_cache决定用data还是cache的radix tree中搜索nullb_page
+		 * 如果没有ignore_cahce, 就先在cache中搜索, 再在data搜索
+		 *
+		 * null_cache_active():
+		 * Device is using a write-back cache.
+		 */
 		t_page = null_lookup_page(nullb, sector, false,
 			!null_cache_active(nullb));
 
@@ -1022,6 +1747,12 @@ static int copy_from_nullb(struct nullb *nullb, struct page *dest,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1185| <<null_transfer>> nullb_fill_pattern(nullb, page, len, off);
+ *
+ * 把page的从off开始的len的长度的内存设置成0xFF
+ */
 static void nullb_fill_pattern(struct nullb *nullb, struct page *page,
 			       unsigned int len, unsigned int off)
 {
@@ -1032,6 +1763,11 @@ static void nullb_fill_pattern(struct nullb *nullb, struct page *page,
 	kunmap_atomic(dst);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1208| <<null_handle_rq>> null_handle_discard(nullb, sector, blk_rq_bytes(rq));
+ *   - drivers/block/null_blk_main.c|1242| <<null_handle_bio>> null_handle_discard(nullb, sector,
+ */
 static void null_handle_discard(struct nullb *nullb, sector_t sector, size_t n)
 {
 	size_t temp;
@@ -1048,6 +1784,10 @@ static void null_handle_discard(struct nullb *nullb, sector_t sector, size_t n)
 	spin_unlock_irq(&nullb->lock);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1381| <<null_handle_cmd>> cmd->error = errno_to_blk_status(null_handle_flush(nullb));
+ */
 static int null_handle_flush(struct nullb *nullb)
 {
 	int err;
@@ -1068,6 +1808,11 @@ static int null_handle_flush(struct nullb *nullb)
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1236| <<null_handle_rq>> err = null_transfer(nullb, bvec.bv_page, len, bvec.bv_offset,
+ *   - drivers/block/null_blk_main.c|1271| <<null_handle_bio>> err = null_transfer(nullb, bvec.bv_page, len, bvec.bv_offset,
+ */
 static int null_transfer(struct nullb *nullb, struct page *page,
 	unsigned int len, unsigned int off, bool is_write, sector_t sector,
 	bool is_fua)
@@ -1082,23 +1827,39 @@ static int null_transfer(struct nullb *nullb, struct page *page,
 				sector, len);
 
 		if (valid_len) {
+			/*
+			 * 这里应该是读的工作,如果支持write-back cache (NULLB_DEV_FL_CACHE),就用cache radix tree
+			 * 否则用data的radix tree, 把数据读到内存
+			 */
 			err = copy_from_nullb(nullb, page, off,
 				sector, valid_len);
 			off += valid_len;
 			len -= valid_len;
 		}
 
+		/*
+		 * nullb_fill_pattern()把page的从off开始的len的长度的内存设置成0xFF
+		 * 猜测对于非zoned的blk的len=0
+		 */
 		if (len)
 			nullb_fill_pattern(nullb, page, len, off);
 		flush_dcache_page(page);
 	} else {
 		flush_dcache_page(page);
+		/*
+		 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+		 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+		 */
 		err = copy_to_nullb(nullb, page, off, sector, len, is_fua);
 	}
 
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1348| <<null_handle_memory_backed>> err = null_handle_rq(cmd);
+ */
 static int null_handle_rq(struct nullb_cmd *cmd)
 {
 	struct request *rq = cmd->rq;
@@ -1117,8 +1878,16 @@ static int null_handle_rq(struct nullb_cmd *cmd)
 	}
 
 	spin_lock_irq(&nullb->lock);
+	/*
+	 * struct bio_vec bvec;
+	 * struct req_iterator iter;
+	 */
 	rq_for_each_segment(bvec, rq, iter) {
 		len = bvec.bv_len;
+		/*
+		 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+		 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+		 */
 		err = null_transfer(nullb, bvec.bv_page, len, bvec.bv_offset,
 				     op_is_write(req_op(rq)), sector,
 				     req_op(rq) & REQ_FUA);
@@ -1133,6 +1902,10 @@ static int null_handle_rq(struct nullb_cmd *cmd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1346| <<null_handle_memory_backed>> err = null_handle_bio(cmd);
+ */
 static int null_handle_bio(struct nullb_cmd *cmd)
 {
 	struct bio *bio = cmd->bio;
@@ -1167,6 +1940,12 @@ static int null_handle_bio(struct nullb_cmd *cmd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1290| <<null_handle_throttled>> null_stop_queue(nullb);
+ *
+ * 对于multiqueue调用blk_mq_stop_hw_queues(q)
+ */
 static void null_stop_queue(struct nullb *nullb)
 {
 	struct request_queue *q = nullb->q;
@@ -1175,6 +1954,14 @@ static void null_stop_queue(struct nullb *nullb)
 		blk_mq_stop_hw_queues(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1318| <<null_handle_throttled>> null_restart_queue_async(nullb);
+ *   - drivers/block/null_blk_main.c|1437| <<nullb_bwtimer_fn>> null_restart_queue_async(nullb);
+ *   - drivers/block/null_blk_main.c|1611| <<null_del_dev>> null_restart_queue_async(nullb);
+ *
+ * 对于multiqueue调用blk_mq_start_stopped_hw_queues(q, true)
+ */
 static void null_restart_queue_async(struct nullb *nullb)
 {
 	struct request_queue *q = nullb->q;
@@ -1183,6 +1970,10 @@ static void null_restart_queue_async(struct nullb *nullb)
 		blk_mq_start_stopped_hw_queues(q, true);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1400| <<null_handle_cmd>> sts = null_handle_throttled(cmd);
+ */
 static inline blk_status_t null_handle_throttled(struct nullb_cmd *cmd)
 {
 	struct nullb_device *dev = cmd->nq->dev;
@@ -1204,6 +1995,10 @@ static inline blk_status_t null_handle_throttled(struct nullb_cmd *cmd)
 	return sts;
 }
 
+/*
+ * called by;
+ *   - drivers/block/null_blk_main.c|1411| <<null_handle_cmd>> cmd->error = null_handle_badblocks(cmd, sector, nr_sectors);
+ */
 static inline blk_status_t null_handle_badblocks(struct nullb_cmd *cmd,
 						 sector_t sector,
 						 sector_t nr_sectors)
@@ -1218,6 +2013,10 @@ static inline blk_status_t null_handle_badblocks(struct nullb_cmd *cmd,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1417| <<null_handle_cmd>> cmd->error = null_handle_memory_backed(cmd, op);
+ */
 static inline blk_status_t null_handle_memory_backed(struct nullb_cmd *cmd,
 						     enum req_opf op)
 {
@@ -1232,6 +2031,14 @@ static inline blk_status_t null_handle_memory_backed(struct nullb_cmd *cmd,
 	return errno_to_blk_status(err);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1321| <<null_handle_cmd>> nullb_complete_cmd(cmd);
+ *
+ * null_queue_bio() or null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> nullb_complete_cmd()
+ */
 static inline void nullb_complete_cmd(struct nullb_cmd *cmd)
 {
 	/* Complete IO by inline, softirq or timer */
@@ -1245,11 +2052,21 @@ static inline void nullb_complete_cmd(struct nullb_cmd *cmd)
 			/*
 			 * XXX: no proper submitting cpu information available.
 			 */
+			/*
+			 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+			 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+			 * 根据情况唤醒nullb_queue->wait
+			 */
 			end_cmd(cmd);
 			break;
 		}
 		break;
 	case NULL_IRQ_NONE:
+		/*
+		 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+		 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+		 * 根据情况唤醒nullb_queue->wait
+		 */
 		end_cmd(cmd);
 		break;
 	case NULL_IRQ_TIMER:
@@ -1258,6 +2075,11 @@ static inline void nullb_complete_cmd(struct nullb_cmd *cmd)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1373| <<null_queue_bio>> null_handle_cmd(cmd, sector, nr_sectors, bio_op(bio));
+ *   - drivers/block/null_blk_main.c|1441| <<null_queue_rq>> return null_handle_cmd(cmd, sector, nr_sectors, req_op(bd->rq));
+ */
 static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 				    sector_t nr_sectors, enum req_opf op)
 {
@@ -1265,6 +2087,12 @@ static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 	struct nullb *nullb = dev->nullb;
 	blk_status_t sts;
 
+	/*
+	 * 在以下使用NULLB_DEV_FL_THROTTLED:
+	 *   - drivers/block/null_blk_main.c|1543| <<null_handle_cmd>> if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|1772| <<null_del_dev>> if (test_bit(NULLB_DEV_FL_THROTTLED, &nullb->dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|2156| <<null_add_dev>> set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
+	 */
 	if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
 		sts = null_handle_throttled(cmd);
 		if (sts != BLK_STS_OK)
@@ -1282,6 +2110,9 @@ static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 			goto out;
 	}
 
+	/*
+	 * 对于nullb0默认是0
+	 */
 	if (dev->memory_backed)
 		cmd->error = null_handle_memory_backed(cmd, op);
 
@@ -1289,10 +2120,22 @@ static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 		cmd->error = null_handle_zoned(cmd, op, sector, nr_sectors);
 
 out:
+	/*
+	 * called by:
+	 *   - drivers/block/null_blk_main.c|1321| <<null_handle_cmd>> nullb_complete_cmd(cmd);
+	 *
+	 * null_queue_bio() or null_queue_rq():
+	 *  -> null_handle_cmd()
+	 *      -> nullb_complete_cmd()
+	 */
 	nullb_complete_cmd(cmd);
 	return BLK_STS_OK;
 }
 
+/*
+ * 在以下使用nullb_bwtimer_fn():
+ *   - drivers/block/null_blk_main.c|1449| <<nullb_setup_bwtimer>> nullb->bw_timer.function = nullb_bwtimer_fn;
+ */
 static enum hrtimer_restart nullb_bwtimer_fn(struct hrtimer *timer)
 {
 	struct nullb *nullb = container_of(timer, struct nullb, bw_timer);
@@ -1310,16 +2153,36 @@ static enum hrtimer_restart nullb_bwtimer_fn(struct hrtimer *timer)
 	return HRTIMER_RESTART;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1983| <<null_add_dev>> nullb_setup_bwtimer(nullb);
+ *
+ * 在dev->mbps和NULLB_DEV_FL_THROTTLED的情况使用
+ */
 static void nullb_setup_bwtimer(struct nullb *nullb)
 {
 	ktime_t timer_interval = ktime_set(0, TIMER_INTERVAL);
 
+	/*
+	 * HRTIMER_MODE_ABS             - Time value is absolute
+	 * HRTIMER_MODE_REL             - Time value is relative to now
+	 * HRTIMER_MODE_PINNED          - Timer is bound to CPU (is only considered
+	 *                                when starting the timer)
+	 * HRTIMER_MODE_SOFT            - Timer callback function will be executed in
+	 *                                soft irq context
+	 * HRTIMER_MODE_HARD            - Timer callback function will be executed in
+	 *                                hard irq context even on PREEMPT_RT.
+	 */
 	hrtimer_init(&nullb->bw_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	nullb->bw_timer.function = nullb_bwtimer_fn;
 	atomic_long_set(&nullb->cur_bytes, mb_per_tick(nullb->dev->mbps));
 	hrtimer_start(&nullb->bw_timer, timer_interval, HRTIMER_MODE_REL);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1473| <<null_queue_bio>> struct nullb_queue *nq = nullb_to_queue(nullb);
+ */
 static struct nullb_queue *nullb_to_queue(struct nullb *nullb)
 {
 	int index = 0;
@@ -1330,6 +2193,10 @@ static struct nullb_queue *nullb_to_queue(struct nullb *nullb)
 	return &nullb->queues[index];
 }
 
+/*
+ * 在以下使用null_queue_bio():
+ *   - drivers/block/null_blk_main.c|1832| <<null_add_dev>> blk_queue_make_request(nullb->q, null_queue_bio);
+ */
 static blk_qc_t null_queue_bio(struct request_queue *q, struct bio *bio)
 {
 	sector_t sector = bio->bi_iter.bi_sector;
@@ -1345,6 +2212,10 @@ static blk_qc_t null_queue_bio(struct request_queue *q, struct bio *bio)
 	return BLK_QC_T_NONE;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1546| <<null_queue_rq>> if (should_timeout_request(bd->rq))
+ */
 static bool should_timeout_request(struct request *rq)
 {
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -1354,6 +2225,10 @@ static bool should_timeout_request(struct request *rq)
 	return false;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1533| <<null_queue_rq>> if (should_requeue_request(bd->rq)) {
+ */
 static bool should_requeue_request(struct request *rq)
 {
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -1363,6 +2238,9 @@ static bool should_requeue_request(struct request *rq)
 	return false;
 }
 
+/*
+ * struct blk_mq_ops null_mq_ops.timeout= null_timeout_rq()
+ */
 static enum blk_eh_timer_return null_timeout_rq(struct request *rq, bool res)
 {
 	pr_info("rq %p timed out\n", rq);
@@ -1370,6 +2248,9 @@ static enum blk_eh_timer_return null_timeout_rq(struct request *rq, bool res)
 	return BLK_EH_DONE;
 }
 
+/*
+ * struct blk_mq_ops null_mq_ops.queue_rq = null_queue_rq()
+ */
 static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 			 const struct blk_mq_queue_data *bd)
 {
@@ -1381,7 +2262,22 @@ static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 
 	if (nq->dev->irqmode == NULL_IRQ_TIMER) {
+		/*
+		 * hrtimer_init - initialize a timer to the given clock
+		 * @timer:      the timer to be initialized
+		 * @clock_id:   the clock to be used
+		 * @mode:       The modes which are relevant for intitialization:
+		 *              HRTIMER_MODE_ABS, HRTIMER_MODE_REL, HRTIMER_MODE_ABS_SOFT,
+		 *              HRTIMER_MODE_REL_SOFT
+		 */
 		hrtimer_init(&cmd->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+		/*
+		 * null_cmd_timer_expired()的实现:
+		 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+		 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+		 * 根据情况唤醒nullb_queue->wait
+		 * 返回HRTIMER_NORESTART
+		 */
 		cmd->timer.function = null_cmd_timer_expired;
 	}
 	cmd->rq = bd->rq;
@@ -1389,6 +2285,9 @@ static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 
 	blk_mq_start_request(bd->rq);
 
+	/*
+	 * 类似fault injection, 需要BLK_DEV_NULL_BLK_FAULT_INJECTION
+	 */
 	if (should_requeue_request(bd->rq)) {
 		/*
 		 * Alternate between hitting the core BUSY path, and the
@@ -1405,21 +2304,38 @@ static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 	if (should_timeout_request(bd->rq))
 		return BLK_STS_OK;
 
+	/*
+	 * sector_t nr_sectors = blk_rq_sectors(bd->rq);
+	 * sector_t sector = blk_rq_pos(bd->rq);
+	 */
 	return null_handle_cmd(cmd, sector, nr_sectors, req_op(bd->rq));
 }
 
+/*
+ * 在以下使用null_mq_ops:
+ *   - drivers/block/null_blk_main.c|1701| <<null_init_tag_set>> set->ops = &null_mq_ops;
+ */
 static const struct blk_mq_ops null_mq_ops = {
 	.queue_rq       = null_queue_rq,
 	.complete	= null_complete_rq,
 	.timeout	= null_timeout_rq,
 };
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1552| <<cleanup_queues>> cleanup_queue(&nullb->queues[i]);
+ */
 static void cleanup_queue(struct nullb_queue *nq)
 {
 	kfree(nq->tag_map);
 	kfree(nq->cmds);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1578| <<null_del_dev>> cleanup_queues(nullb);
+ *   - drivers/block/null_blk_main.c|1995| <<null_add_dev>> cleanup_queues(nullb);
+ */
 static void cleanup_queues(struct nullb *nullb)
 {
 	int i;
@@ -1430,6 +2346,13 @@ static void cleanup_queues(struct nullb *nullb)
 	kfree(nullb->queues);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|367| <<nullb_device_power_store>> null_del_dev(dev->nullb);
+ *   - drivers/block/null_blk_main.c|495| <<nullb_group_drop_item>> null_del_dev(dev->nullb);
+ *   - drivers/block/null_blk_main.c|2088| <<null_init>> null_del_dev(nullb);
+ *   - drivers/block/null_blk_main.c|2114| <<null_exit>> null_del_dev(nullb);
+ */
 static void null_del_dev(struct nullb *nullb)
 {
 	struct nullb_device *dev = nullb->dev;
@@ -1458,6 +2381,10 @@ static void null_del_dev(struct nullb *nullb)
 	dev->nullb = NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1926| <<null_add_dev>> null_config_discard(nullb);
+ */
 static void null_config_discard(struct nullb *nullb)
 {
 	if (nullb->dev->discard == false)
@@ -1468,11 +2395,20 @@ static void null_config_discard(struct nullb *nullb)
 	blk_queue_flag_set(QUEUE_FLAG_DISCARD, nullb->q);
 }
 
+/*
+ * 在以下使用null_ops:
+ *   - drivers/block/null_blk_main.c|1573| <<null_gendisk_register>> disk->fops = &null_ops;
+ */
 static const struct block_device_operations null_ops = {
 	.owner		= THIS_MODULE,
 	.report_zones	= null_report_zones,
 };
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1626| <<null_init_queues>> null_init_queue(nullb, nq);
+ *   - drivers/block/null_blk_main.c|1678| <<init_driver_queues>> null_init_queue(nullb, nq);
+ */
 static void null_init_queue(struct nullb *nullb, struct nullb_queue *nq)
 {
 	BUG_ON(!nullb);
@@ -1483,6 +2419,10 @@ static void null_init_queue(struct nullb *nullb, struct nullb_queue *nq)
 	nq->dev = nullb->dev;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1880| <<null_add_dev>> null_init_queues(nullb);
+ */
 static void null_init_queues(struct nullb *nullb)
 {
 	struct request_queue *q = nullb->q;
@@ -1490,6 +2430,7 @@ static void null_init_queues(struct nullb *nullb)
 	struct nullb_queue *nq;
 	int i;
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (!hctx->nr_ctx || !hctx->tags)
 			continue;
@@ -1500,6 +2441,10 @@ static void null_init_queues(struct nullb *nullb)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1680| <<init_driver_queues>> ret = setup_commands(nq);
+ */
 static int setup_commands(struct nullb_queue *nq)
 {
 	struct nullb_cmd *cmd;
@@ -1518,7 +2463,15 @@ static int setup_commands(struct nullb_queue *nq)
 
 	for (i = 0; i < nq->queue_depth; i++) {
 		cmd = &nq->cmds[i];
+		/*
+		 * list没人用, 可以删了??
+		 *   - drivers/block/null_blk_main.c|2440| <<setup_commands>> INIT_LIST_HEAD(&cmd->list);
+		 */
 		INIT_LIST_HEAD(&cmd->list);
+		/*
+		 * ll_list没人用, 可以删了??
+		 *   - drivers/block/null_blk_main.c|2441| <<setup_commands>> cmd->ll_list.next = NULL;
+		 */
 		cmd->ll_list.next = NULL;
 		cmd->tag = -1U;
 	}
@@ -1526,8 +2479,15 @@ static int setup_commands(struct nullb_queue *nq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1851| <<null_add_dev>> rv = setup_queues(nullb);
+ */
 static int setup_queues(struct nullb *nullb)
 {
+	/*
+	 * struct nullb_queue *queues;
+	 */
 	nullb->queues = kcalloc(nullb->dev->submit_queues,
 				sizeof(struct nullb_queue),
 				GFP_KERNEL);
@@ -1539,6 +2499,10 @@ static int setup_queues(struct nullb *nullb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1888| <<null_add_dev>> rv = init_driver_queues(nullb);
+ */
 static int init_driver_queues(struct nullb *nullb)
 {
 	struct nullb_queue *nq;
@@ -1557,6 +2521,10 @@ static int init_driver_queues(struct nullb *nullb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1930| <<null_add_dev>> rv = null_gendisk_register(nullb);
+ */
 static int null_gendisk_register(struct nullb *nullb)
 {
 	sector_t size = ((sector_t)nullb->dev->size * SZ_1M) >> SECTOR_SHIFT;
@@ -1593,6 +2561,11 @@ static int null_gendisk_register(struct nullb *nullb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1810| <<null_add_dev>> rv = null_init_tag_set(nullb, nullb->tag_set);
+ *   - drivers/block/null_blk_main.c|1943| <<null_init>> ret = null_init_tag_set(NULL, &tag_set);
+ */
 static int null_init_tag_set(struct nullb *nullb, struct blk_mq_tag_set *set)
 {
 	set->ops = &null_mq_ops;
@@ -1610,9 +2583,34 @@ static int null_init_tag_set(struct nullb *nullb, struct blk_mq_tag_set *set)
 	if ((nullb && nullb->dev->blocking) || g_blocking)
 		set->flags |= BLK_MQ_F_BLOCKING;
 
+	/*
+	 * 调用blk_mq_alloc_tag_set()的例子:
+	 *   - block/blk-mq.c|2947| <<blk_mq_init_sq_queue>> ret = blk_mq_alloc_tag_set(set);
+	 *   - block/bsg-lib.c|384| <<bsg_setup_queue>> if (blk_mq_alloc_tag_set(set))
+	 *   - drivers/block/loop.c|2033| <<loop_add>> err = blk_mq_alloc_tag_set(&lo->tag_set);
+	 *   - drivers/block/nbd.c|1688| <<nbd_dev_add>> err = blk_mq_alloc_tag_set(&nbd->tag_set);
+	 *   - drivers/block/null_blk_main.c|1716| <<null_init_tag_set>> return blk_mq_alloc_tag_set(set);
+	 *   - drivers/block/virtio_blk.c|819| <<virtblk_probe>> err = blk_mq_alloc_tag_set(&vblk->tag_set);
+	 *   - drivers/block/xen-blkfront.c|984| <<xlvbd_init_blk_queue>> if (blk_mq_alloc_tag_set(&info->tag_set))
+	 *   - drivers/ide/ide-probe.c|787| <<ide_init_queue>> if (blk_mq_alloc_tag_set(set))
+	 *   - drivers/md/dm-rq.c|562| <<dm_mq_init_request_queue>> err = blk_mq_alloc_tag_set(md->tag_set);
+	 *   - drivers/nvme/host/fc.c|2475| <<nvme_fc_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+	 *   - drivers/nvme/host/fc.c|3143| <<nvme_fc_init_ctrl>> ret = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+	 *   - drivers/nvme/host/pci.c|1645| <<nvme_alloc_admin_tags>> if (blk_mq_alloc_tag_set(&dev->admin_tagset))
+	 *   - drivers/nvme/host/pci.c|2328| <<nvme_dev_add>> ret = blk_mq_alloc_tag_set(&dev->tagset);
+	 *   - drivers/nvme/host/rdma.c|755| <<nvme_rdma_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+	 *   - drivers/nvme/host/tcp.c|1493| <<nvme_tcp_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+	 *   - drivers/nvme/target/loop.c|357| <<nvme_loop_configure_admin_queue>> error = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+	 *   - drivers/nvme/target/loop.c|525| <<nvme_loop_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+	 *   - drivers/scsi/scsi_lib.c|1906| <<scsi_mq_setup_tags>> return blk_mq_alloc_tag_set(&shost->tag_set);
+	 */
 	return blk_mq_alloc_tag_set(set);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1837| <<null_add_dev>> rv = null_validate_conf(dev);
+ */
 static int null_validate_conf(struct nullb_device *dev)
 {
 	dev->blocksize = round_down(dev->blocksize, 512);
@@ -1651,6 +2649,11 @@ static int null_validate_conf(struct nullb_device *dev)
 }
 
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1824| <<null_setup_fault>> if (!__null_setup_fault(&null_timeout_attr, g_timeout_str))
+ *   - drivers/block/null_blk_main.c|1826| <<null_setup_fault>> if (!__null_setup_fault(&null_requeue_attr, g_requeue_str))
+ */
 static bool __null_setup_fault(struct fault_attr *attr, char *str)
 {
 	if (!str[0])
@@ -1664,6 +2667,12 @@ static bool __null_setup_fault(struct fault_attr *attr, char *str)
 }
 #endif
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1871| <<null_add_dev>> if (!null_setup_fault())
+ *
+ * Documentation/fault-injection/fault-injection.rst
+ */
 static bool null_setup_fault(void)
 {
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -1675,6 +2684,11 @@ static bool null_setup_fault(void)
 	return true;
 }
 
+/*
+ * calld by:
+ *   - drivers/block/null_blk_main.c|356| <<nullb_device_power_store>> if (null_add_dev(dev)) {
+ *   - drivers/block/null_blk_main.c|2027| <<null_init>> ret = null_add_dev(dev);
+ */
 static int null_add_dev(struct nullb_device *dev)
 {
 	struct nullb *nullb;
@@ -1699,11 +2713,20 @@ static int null_add_dev(struct nullb_device *dev)
 		goto out_free_nullb;
 
 	if (dev->queue_mode == NULL_Q_MQ) {
+		/*
+		 * 如果是shared就使用全局的tag_set,
+		 * 否则就使用nullb自己的: nullb->__tag_set
+		 */
 		if (shared_tags) {
 			nullb->tag_set = &tag_set;
 			rv = 0;
 		} else {
 			nullb->tag_set = &nullb->__tag_set;
+			/*
+			 * called by:
+			 *   - drivers/block/null_blk_main.c|1810| <<null_add_dev>> rv = null_init_tag_set(nullb, nullb->tag_set);
+			 *   - drivers/block/null_blk_main.c|1943| <<null_init>> ret = null_init_tag_set(NULL, &tag_set);
+			 */
 			rv = null_init_tag_set(nullb, nullb->tag_set);
 		}
 
@@ -1732,13 +2755,21 @@ static int null_add_dev(struct nullb_device *dev)
 			goto out_cleanup_blk_queue;
 	}
 
+	/* Bandwidth throttle cap (in MB/s) */
 	if (dev->mbps) {
+		/*
+		 * 在以下使用NULLB_DEV_FL_THROTTLED:
+		 *   - drivers/block/null_blk_main.c|1543| <<null_handle_cmd>> if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+		 *   - drivers/block/null_blk_main.c|1772| <<null_del_dev>> if (test_bit(NULLB_DEV_FL_THROTTLED, &nullb->dev->flags)) {
+		 *   - drivers/block/null_blk_main.c|2156| <<null_add_dev>> set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
+		 */
 		set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
 		nullb_setup_bwtimer(nullb);
 	}
 
 	if (dev->cache_size > 0) {
 		set_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+		/* 把QUEUE_FLAG_WC和QUEUE_FLAG_FUA都设置上 */
 		blk_queue_write_cache(nullb->q, true, true);
 	}
 
@@ -1774,6 +2805,16 @@ static int null_add_dev(struct nullb_device *dev)
 		goto out_cleanup_zone;
 
 	mutex_lock(&lock);
+	/*
+	 * called by:
+	 *   - drivers/block/null_blk_main.c|1786| <<null_add_dev>> list_add_tail(&nullb->list, &nullb_list);
+	 *   - drivers/block/null_blk_main.c|1889| <<null_init>> while (!list_empty(&nullb_list)) {
+	 *   - drivers/block/null_blk_main.c|1890| <<null_init>> nullb = list_entry(nullb_list.next, struct nullb, list);
+	 *   - drivers/block/null_blk_main.c|1913| <<null_exit>> while (!list_empty(&nullb_list)) {
+	 *   - drivers/block/null_blk_main.c|1916| <<null_exit>> nullb = list_entry(nullb_list.next, struct nullb, list);
+	 *
+	 * 添加struct nullb
+	 */
 	list_add_tail(&nullb->list, &nullb_list);
 	mutex_unlock(&lock);
 
@@ -1801,6 +2842,9 @@ static int __init null_init(void)
 	struct nullb *nullb;
 	struct nullb_device *dev;
 
+	/*
+	 * "Block size (in bytes)"
+	 */
 	if (g_bs > PAGE_SIZE) {
 		pr_warn("invalid block size\n");
 		pr_warn("defaults block size to %lu\n", PAGE_SIZE);
@@ -1816,7 +2860,22 @@ static int __init null_init(void)
 		pr_err("legacy IO path no longer available\n");
 		return -EINVAL;
 	}
+	/*
+	 * g_submit_queues: "Number of submission queues"
+	 */
 	if (g_queue_mode == NULL_Q_MQ && g_use_per_node_hctx) {
+		/*
+		 * 在一下使用g_submit_queues:
+		 *   - drivers/block/null_blk_main.c|132| <<global>> module_param_named(submit_queues, g_submit_queues, int , 0444);
+		 *   - drivers/block/null_blk_main.c|622| <<null_alloc_dev>> dev->submit_queues = g_submit_queues;
+		 *   - drivers/block/null_blk_main.c|1995| <<null_init_tag_set>> g_submit_queues;
+		 *   - drivers/block/null_blk_main.c|2270| <<null_init>> if (g_submit_queues != nr_online_nodes) {
+		 *   - drivers/block/null_blk_main.c|2273| <<null_init>> g_submit_queues = nr_online_nodes;
+		 *   - drivers/block/null_blk_main.c|2275| <<null_init>> } else if (g_submit_queues > nr_cpu_ids)
+		 *   - drivers/block/null_blk_main.c|2276| <<null_init>> g_submit_queues = nr_cpu_ids;
+		 *   - drivers/block/null_blk_main.c|2277| <<null_init>> else if (g_submit_queues <= 0)
+		 *   - drivers/block/null_blk_main.c|2278| <<null_init>> g_submit_queues = 1;
+		 */
 		if (g_submit_queues != nr_online_nodes) {
 			pr_warn("submit_queues param is set to %u.\n",
 							nr_online_nodes);
@@ -1827,7 +2886,15 @@ static int __init null_init(void)
 	else if (g_submit_queues <= 0)
 		g_submit_queues = 1;
 
+	/*
+	 * "Share tag set between devices for blk-mq"
+	 */
 	if (g_queue_mode == NULL_Q_MQ && shared_tags) {
+		/*
+		 * called by:
+		 *   - drivers/block/null_blk_main.c|1810| <<null_add_dev>> rv = null_init_tag_set(nullb, nullb->tag_set);
+		 *   - drivers/block/null_blk_main.c|1943| <<null_init>> ret = null_init_tag_set(NULL, &tag_set);
+		 */
 		ret = null_init_tag_set(NULL, &tag_set);
 		if (ret)
 			return ret;
@@ -1842,6 +2909,9 @@ static int __init null_init(void)
 
 	mutex_init(&lock);
 
+	/*
+	 * 核心思想是在major_names[]找到一个可用的major
+	 */
 	null_major = register_blkdev(0, "nullb");
 	if (null_major < 0) {
 		ret = null_major;
@@ -1849,6 +2919,9 @@ static int __init null_init(void)
 	}
 
 	for (i = 0; i < nr_devices; i++) {
+		/*
+		 * 主要分配struct nullb_device和参数设置
+		 */
 		dev = null_alloc_dev();
 		if (!dev) {
 			ret = -ENOMEM;
diff --git a/drivers/block/null_blk_zoned.c b/drivers/block/null_blk_zoned.c
index ed34785dd64b..ea699c69f9a6 100644
--- a/drivers/block/null_blk_zoned.c
+++ b/drivers/block/null_blk_zoned.c
@@ -3,13 +3,32 @@
 #include "null_blk.h"
 
 /* zone_size in MBs to sectors. */
+/*
+ * 在以下使用ZONE_SIZE_SHIFT:
+ *   - drivers/block/null_blk_zoned.c|24| <<null_zone_init>> dev->zone_size_sects = dev->zone_size << ZONE_SIZE_SHIFT;
+ *
+ * 1往左移11位是2048
+ */
 #define ZONE_SIZE_SHIFT		11
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_zoned.c|78| <<null_report_zones>> first_zone = null_zone_no(dev, sector);
+ *   - drivers/block/null_blk_zoned.c|104| <<null_zone_valid_read_len>> struct blk_zone *zone = &dev->zones[null_zone_no(dev, sector)];
+ *   - drivers/block/null_blk_zoned.c|122| <<null_zone_write>> unsigned int zno = null_zone_no(dev, sector);
+ *   - drivers/block/null_blk_zoned.c|158| <<null_zone_mgmt>> struct blk_zone *zone = &dev->zones[null_zone_no(dev, sector)];
+ *
+ * 把sector转化为zone number
+ */
 static inline unsigned int null_zone_no(struct nullb_device *dev, sector_t sect)
 {
 	return sect >> ilog2(dev->zone_size_sects);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1746| <<null_add_dev>> rv = null_zone_init(dev);
+ */
 int null_zone_init(struct nullb_device *dev)
 {
 	sector_t dev_size = (sector_t)dev->size * 1024 * 1024;
@@ -61,11 +80,25 @@ int null_zone_init(struct nullb_device *dev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|566| <<null_free_dev>> null_zone_exit(dev);
+ *   - drivers/block/null_blk_main.c|1783| <<null_add_dev>> null_zone_exit(dev);
+ */
 void null_zone_exit(struct nullb_device *dev)
 {
+	/* struct blk_zone *zones; */
 	kvfree(dev->zones);
 }
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|181| <<blkdev_report_zones>> return disk->fops->report_zones(disk, sector, nr_zones, cb, data);
+ *   - block/blk-zoned.c|557| <<blk_revalidate_disk_zones>> ret = disk->fops->report_zones(disk, 0, UINT_MAX,
+ *   - drivers/md/dm.c|503| <<dm_blk_report_zones>> ret = tgt->type->report_zones(tgt, &args, nr_zones);
+ *
+ * struct block_device_operations null_ops.report_zones = null_report_zones()
+ */
 int null_report_zones(struct gendisk *disk, sector_t sector,
 		unsigned int nr_zones, report_zones_cb cb, void *data)
 {
@@ -97,6 +130,14 @@ int null_report_zones(struct gendisk *disk, sector_t sector,
 	return nr_zones;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1081| <<null_transfer>> valid_len = null_zone_valid_read_len(nullb,
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_transfer():
+ *      -> null_zone_valid_read_len()
+ */
 size_t null_zone_valid_read_len(struct nullb *nullb,
 				sector_t sector, unsigned int len)
 {
@@ -115,13 +156,35 @@ size_t null_zone_valid_read_len(struct nullb *nullb,
 	return (zone->wp - sector) << SECTOR_SHIFT;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_zoned.c|214| <<null_handle_zoned>> return null_zone_write(cmd, sector, nr_sectors);
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> null_handle_zoned()
+ *          -> null_zone_write()
+ */
 static blk_status_t null_zone_write(struct nullb_cmd *cmd, sector_t sector,
 		     unsigned int nr_sectors)
 {
 	struct nullb_device *dev = cmd->nq->dev;
+	/* 把sector转化为zone number */
 	unsigned int zno = null_zone_no(dev, sector);
 	struct blk_zone *zone = &dev->zones[zno];
 
+	/*
+	 * @BLK_ZONE_COND_NOT_WP: The zone has no write pointer, it is conventional.
+	 * @BLK_ZONE_COND_EMPTY: The zone is empty.
+	 * @BLK_ZONE_COND_IMP_OPEN: The zone is open, but not explicitly opened.
+	 * @BLK_ZONE_COND_EXP_OPEN: The zones was explicitly opened by an
+	 *                          OPEN ZONE command.
+	 * @BLK_ZONE_COND_CLOSED: The zone was [explicitly] closed after writing.
+	 * @BLK_ZONE_COND_FULL: The zone is marked as full, possibly by a zone
+	 *                      FINISH ZONE command.
+	 * @BLK_ZONE_COND_READONLY: The zone is read-only.
+	 * @BLK_ZONE_COND_OFFLINE: The zone is offline (sectors cannot be read/written).
+	 */
 	switch (zone->cond) {
 	case BLK_ZONE_COND_FULL:
 		/* Cannot write to a full zone */
@@ -138,6 +201,18 @@ static blk_status_t null_zone_write(struct nullb_cmd *cmd, sector_t sector,
 		if (zone->cond != BLK_ZONE_COND_EXP_OPEN)
 			zone->cond = BLK_ZONE_COND_IMP_OPEN;
 
+		/*
+		 * 关于zoned block device:
+		 * Zoned block devices are quite different than the block devices most people
+		 * are used to. The concept came from shingled magnetic recording (SMR)
+		 * devices, which allow much higher density storage, but that extra capacity
+		 * comes with a price: less flexibility. Zoned devices have regions (zones)
+		 * that can only be written sequentially; there is no random access for writes
+		 * to those zones. Linux already supports these devices, and filesystems are
+		 * adding support as well, but some applications may want a simpler, more
+		 * straightforward interface; that's what a new filesystem, zonefs, is
+		 * targeting.
+		 */
 		zone->wp += nr_sectors;
 		if (zone->wp == zone->start + zone->len)
 			zone->cond = BLK_ZONE_COND_FULL;
@@ -151,6 +226,15 @@ static blk_status_t null_zone_write(struct nullb_cmd *cmd, sector_t sector,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_zoned.c|220| <<null_handle_zoned>> return null_zone_mgmt(cmd, op, sector);
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> null_handle_zoned()
+ *          -> null_zone_mgmt()
+ */
 static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 				   sector_t sector)
 {
@@ -160,6 +244,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 
 	switch (op) {
 	case REQ_OP_ZONE_RESET_ALL:
+		/* reset all the zone present on the device */
 		for (i = 0; i < dev->nr_zones; i++) {
 			if (zone[i].type == BLK_ZONE_TYPE_CONVENTIONAL)
 				continue;
@@ -168,6 +253,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 		}
 		break;
 	case REQ_OP_ZONE_RESET:
+		/* reset a zone write pointer */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 
@@ -175,6 +261,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 		zone->wp = zone->start;
 		break;
 	case REQ_OP_ZONE_OPEN:
+		/* Open a zone */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 		if (zone->cond == BLK_ZONE_COND_FULL)
@@ -183,6 +270,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 		zone->cond = BLK_ZONE_COND_EXP_OPEN;
 		break;
 	case REQ_OP_ZONE_CLOSE:
+		/* Close a zone */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 		if (zone->cond == BLK_ZONE_COND_FULL)
@@ -194,6 +282,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 			zone->cond = BLK_ZONE_COND_CLOSED;
 		break;
 	case REQ_OP_ZONE_FINISH:
+		/* Transition a zone to full */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 
@@ -206,6 +295,18 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1289| <<null_handle_cmd>> cmd->error = null_handle_zoned(cmd, op, sector, nr_sectors);
+ *
+ * null_handle_zoned()有2种可能调用别人:
+ *  -> null_zone_write()
+ *  -> null_zone_mgmt()
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> null_handle_zoned()
+ */
 blk_status_t null_handle_zoned(struct nullb_cmd *cmd, enum req_opf op,
 			       sector_t sector, sector_t nr_sectors)
 {
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index 7ffd719d89de..b06ec75cb24c 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -721,6 +721,27 @@ static const struct blk_mq_ops virtio_mq_ops = {
 static unsigned int virtblk_queue_depth;
 module_param_named(queue_depth, virtblk_queue_depth, uint, 0444);
 
+/*
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 static int virtblk_probe(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk;
@@ -789,6 +810,9 @@ static int virtblk_probe(struct virtio_device *vdev)
 	vblk->tag_set.queue_depth = virtblk_queue_depth;
 	vblk->tag_set.numa_node = NUMA_NO_NODE;
 	vblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	/*
+	 * sg_elems从上面VIRTIO_BLK_F_SEG_MAX获得
+	 */
 	vblk->tag_set.cmd_size =
 		sizeof(struct virtblk_req) +
 		sizeof(struct scatterlist) * sg_elems;
@@ -842,6 +866,9 @@ static int virtblk_probe(struct virtio_device *vdev)
 	blk_queue_max_segment_size(q, max_size);
 
 	/* Host can optionally specify the block size of the device */
+	/*
+	 * 如果后端是个file, 通过ioctl的BLKSSZGET和BLKPBSZGET获得
+	 */
 	err = virtio_cread_feature(vdev, VIRTIO_BLK_F_BLK_SIZE,
 				   struct virtio_blk_config, blk_size,
 				   &blk_size);
@@ -905,6 +932,9 @@ static int virtblk_probe(struct virtio_device *vdev)
 	virtblk_update_capacity(vblk, false);
 	virtio_device_ready(vdev);
 
+	/*
+	 * 和gendisk/partition非常重要的接口!!
+	 */
 	device_add_disk(&vdev->dev, vblk->disk, virtblk_attr_groups);
 	return 0;
 
@@ -1030,6 +1060,7 @@ static int __init init(void)
 	if (!virtblk_wq)
 		return -ENOMEM;
 
+	/* 核心思想是在major_names[]找到一个可用的major */
 	major = register_blkdev(0, "virtblk");
 	if (major < 0) {
 		error = major;
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index 5dc32b72e7fa..f45330319ed8 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -30,33 +30,72 @@
 
 #define NVME_MINORS		(1U << MINORBITS)
 
+/*
+ * 在以下真正使用admin_timeout:
+ *   - drivers/nvme/host/core.c|4078| <<nvme_fw_act_work>> msecs_to_jiffies(admin_timeout * 1000);
+ *   - drivers/nvme/host/nvme.h|26| <<ADMIN_TIMEOUT>> #define ADMIN_TIMEOUT (admin_timeout * HZ)
+ *
+ * 默认60秒
+ */
 unsigned int admin_timeout = 60;
 module_param(admin_timeout, uint, 0644);
 MODULE_PARM_DESC(admin_timeout, "timeout in seconds for admin commands");
 EXPORT_SYMBOL_GPL(admin_timeout);
 
+/*
+ * 在以下真正使用nvme_io_timeout:
+ *   - drivers/nvme/host/nvme.h|23| <<NVME_IO_TIMEOUT>> #define NVME_IO_TIMEOUT (nvme_io_timeout * HZ)
+ *
+ * 默认30秒
+ */
 unsigned int nvme_io_timeout = 30;
 module_param_named(io_timeout, nvme_io_timeout, uint, 0644);
 MODULE_PARM_DESC(io_timeout, "timeout in seconds for I/O");
 EXPORT_SYMBOL_GPL(nvme_io_timeout);
 
+/*
+ * 在以下使用shutdown_timeout:
+ *   - drivers/nvme/host/core.c|3632| <<nvme_init_identify>> shutdown_timeout, 60);
+ *   - drivers/nvme/host/core.c|3634| <<nvme_init_identify>> if (ctrl->shutdown_timeout != shutdown_timeout)
+ *   - drivers/nvme/host/core.c|3639| <<nvme_init_identify>> ctrl->shutdown_timeout = shutdown_timeout;
+ *
+ * 通过nvme_ctrl->shutdown_timeout在nvme_shutdown_ctrl()的时候用
+ */
 static unsigned char shutdown_timeout = 5;
 module_param(shutdown_timeout, byte, 0644);
 MODULE_PARM_DESC(shutdown_timeout, "timeout in seconds for controller shutdown");
 
+/*
+ * 在以下使用nvme_max_retries:
+ *   - drivers/nvme/host/core.c|407| <<nvme_req_needs_retry>> if (nvme_req(req)->retries >= nvme_max_retries)
+ */
 static u8 nvme_max_retries = 5;
 module_param_named(max_retries, nvme_max_retries, byte, 0644);
 MODULE_PARM_DESC(max_retries, "max number of retries a command may have");
 
+/*
+ * 在以下使用default_ps_max_latency_us:
+ *   - drivers/nvme/host/core.c|5292| <<nvme_init_ctrl>> min(default_ps_max_latency_us, (unsigned long )S32_MAX));
+ */
 static unsigned long default_ps_max_latency_us = 100000;
 module_param(default_ps_max_latency_us, ulong, 0644);
 MODULE_PARM_DESC(default_ps_max_latency_us,
 		 "max power saving latency for new devices; use PM QOS to change per device");
 
+/* Autonomous Power State Transition */
+/*
+ * 在以下使用force_apst:
+ *   - drivers/nvme/host/core.c|3591| <<nvme_init_identify>> if (force_apst && (ctrl->quirks & NVME_QUIRK_NO_DEEPEST_PS)) {
+ *   - drivers/nvme/host/core.c|3645| <<nvme_init_identify>> if (force_apst && id->apsta) {
+ */
 static bool force_apst;
 module_param(force_apst, bool, 0644);
 MODULE_PARM_DESC(force_apst, "allow APST for newly enumerated devices even if quirked off");
 
+/*
+ * 在以下使用streams:
+ *   - drivers/nvme/host/core.c|791| <<nvme_configure_directives>> if (!streams)
+ */
 static bool streams;
 module_param(streams, bool, 0644);
 MODULE_PARM_DESC(streams, "turn on support for Streams write directives");
@@ -81,12 +120,56 @@ EXPORT_SYMBOL_GPL(nvme_reset_wq);
 struct workqueue_struct *nvme_delete_wq;
 EXPORT_SYMBOL_GPL(nvme_delete_wq);
 
+/*
+ * 在以下使用nvme_subsystems:
+ *   - drivers/nvme/host/core.c|2655| <<__nvme_find_get_subsystem>> list_for_each_entry(subsys, &nvme_subsystems, entry) {
+ *   - drivers/nvme/host/core.c|2797| <<nvme_init_subsystem>> list_add_tail(&subsys->entry, &nvme_subsystems);
+ *
+ * 管理所有的nvme_subsystem->entry
+ */
 static LIST_HEAD(nvme_subsystems);
+/*
+ * 在以下使用nvme_subsystems_lock:
+ *   - drivers/nvme/host/core.c|3044| <<nvme_destroy_subsystem>> mutex_lock(&nvme_subsystems_lock);
+ *   - drivers/nvme/host/core.c|3046| <<nvme_destroy_subsystem>> mutex_unlock(&nvme_subsystems_lock);
+ *   - drivers/nvme/host/core.c|3062| <<__nvme_find_get_subsystem>> lockdep_assert_held(&nvme_subsystems_lock);
+ *   - drivers/nvme/host/core.c|3145| <<nvme_validate_cntlid>> lockdep_assert_held(&nvme_subsystems_lock);
+ *   - drivers/nvme/host/core.c|3206| <<nvme_init_subsystem>> mutex_lock(&nvme_subsystems_lock);
+ *   - drivers/nvme/host/core.c|3240| <<nvme_init_subsystem>> mutex_unlock(&nvme_subsystems_lock);
+ *   - drivers/nvme/host/core.c|3246| <<nvme_init_subsystem>> mutex_unlock(&nvme_subsystems_lock);
+ *   - drivers/nvme/host/core.c|4864| <<nvme_free_ctrl>> mutex_lock(&nvme_subsystems_lock);
+ *   - drivers/nvme/host/core.c|4867| <<nvme_free_ctrl>> mutex_unlock(&nvme_subsystems_lock);
+ */
 static DEFINE_MUTEX(nvme_subsystems_lock);
 
 static DEFINE_IDA(nvme_instance_ida);
+/*
+ * 在以下使用nvme_chr_devt:
+ *   - drivers/nvme/host/core.c|4270| <<nvme_init_ctrl>> ctrl->device->devt = MKDEV(MAJOR(nvme_chr_devt), ctrl->instance);
+ *   - drivers/nvme/host/core.c|4480| <<nvme_core_init>> result = alloc_chrdev_region(&nvme_chr_devt, 0, NVME_MINORS, "nvme");
+ *   - drivers/nvme/host/core.c|4501| <<nvme_core_init>> unregister_chrdev_region(nvme_chr_devt, NVME_MINORS);
+ *   - drivers/nvme/host/core.c|4516| <<nvme_core_exit>> unregister_chrdev_region(nvme_chr_devt, NVME_MINORS);
+ */
 static dev_t nvme_chr_devt;
+/*
+ * 在以下使用nvme_class:
+ *   - drivers/nvme/host/core.c|5430| <<nvme_init_ctrl>> ctrl->device->class = nvme_class;
+ *   - drivers/nvme/host/core.c|5694| <<nvme_core_init>> nvme_class = class_create(THIS_MODULE, "nvme");
+ *   - drivers/nvme/host/core.c|5695| <<nvme_core_init>> if (IS_ERR(nvme_class)) {
+ *   - drivers/nvme/host/core.c|5696| <<nvme_core_init>> result = PTR_ERR(nvme_class);
+ *   - drivers/nvme/host/core.c|5699| <<nvme_core_init>> nvme_class->dev_uevent = nvme_class_uevent;
+ *   - drivers/nvme/host/core.c|5709| <<nvme_core_init>> class_destroy(nvme_class);
+ *   - drivers/nvme/host/core.c|5725| <<nvme_core_exit>> class_destroy(nvme_class);
+ */
 static struct class *nvme_class;
+/*
+ * 在以下使用nvme_subsys_class:
+ *   - drivers/nvme/host/core.c|3332| <<nvme_init_subsystem>> subsys->dev.class = nvme_subsys_class;
+ *   - drivers/nvme/host/core.c|5418| <<nvme_core_init>> nvme_subsys_class = class_create(THIS_MODULE, "nvme-subsystem");
+ *   - drivers/nvme/host/core.c|5419| <<nvme_core_init>> if (IS_ERR(nvme_subsys_class)) {
+ *   - drivers/nvme/host/core.c|5420| <<nvme_core_init>> result = PTR_ERR(nvme_subsys_class);
+ *   - drivers/nvme/host/core.c|5441| <<nvme_core_exit>> class_destroy(nvme_subsys_class);
+ */
 static struct class *nvme_subsys_class;
 
 static int nvme_revalidate_disk(struct gendisk *disk);
@@ -94,12 +177,23 @@ static void nvme_put_subsystem(struct nvme_subsystem *subsys);
 static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 					   unsigned nsid);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1461| <<nvme_update_formats>> nvme_set_queue_dying(ns);
+ *   - drivers/nvme/host/core.c|4326| <<nvme_kill_queues>> nvme_set_queue_dying(ns);
+ */
 static void nvme_set_queue_dying(struct nvme_ns *ns)
 {
 	/*
 	 * Revalidating a dead namespace sets capacity to 0. This will end
 	 * buffered writers dirtying pages that can't be synced.
 	 */
+	/*
+	 * 在以下使用NVME_NS_DEAD:
+	 *   - drivers/nvme/host/core.c|181| <<nvme_set_queue_dying>> if (!ns->disk || test_and_set_bit(NVME_NS_DEAD, &ns->flags))
+	 *   - drivers/nvme/host/core.c|2504| <<nvme_revalidate_disk>> if (test_bit(NVME_NS_DEAD, &ns->flags)) {
+	 *   - drivers/nvme/host/core.c|4679| <<nvme_remove_invalid_namespaces>> if (ns->head->ns_id > nsid || test_bit(NVME_NS_DEAD, &ns->flags))
+	 */
 	if (!ns->disk || test_and_set_bit(NVME_NS_DEAD, &ns->flags))
 		return;
 	blk_set_queue_dying(ns->queue);
@@ -108,14 +202,57 @@ static void nvme_set_queue_dying(struct nvme_ns *ns)
 	/*
 	 * Revalidate after unblocking dispatchers that may be holding bd_butex
 	 */
+	/*
+	 * This routine is a wrapper for lower-level driver's revalidate_disk
+	 * call-backs.  It is used to do common pre and post operations needed
+	 * for all revalidate_disk operations.
+	 */
+	/* nvme_revalidate_disk() */
 	revalidate_disk(ns->disk);
 }
 
+/*
+ * [0] nvme_queue_scan
+ * [0] nvme_start_ctrl
+ * [0] nvme_loop_create_ctrl
+ * [0] nvmf_dev_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [  143.581757] Workqueue: events nvmet_async_event_work
+ * [  143.581758] Call Trace:
+ * [  143.581764]  dump_stack+0x64/0x83
+ * [  143.581766]  nvme_queue_scan+0x17/0x50
+ * [  143.581769]  nvme_complete_async_event+0x17b/0x1c0
+ * [  143.581771]  nvmet_req_complete+0xc/0x40
+ * [  143.581773]  nvmet_async_event_work+0x89/0xb0
+ * [  143.581775]  process_one_work+0x15b/0x360
+ * [  143.581782]  worker_thread+0x44/0x3d0
+ * [  143.581784]  kthread+0xf3/0x130
+ * [  143.581786]  ? max_active_store+0x80/0x80
+ * [  143.581787]  ? kthread_bind+0x10/0x10
+ * [  143.581789]  ret_from_fork+0x35/0x40
+ * [  143.581796] nvme nvme0: rescanning namespaces.
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|1476| <<nvme_passthru_end>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|3114| <<nvme_dev_ioctl>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|3148| <<nvme_sysfs_rescan>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|4043| <<nvme_handle_aen_notice>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|4122| <<nvme_start_ctrl>> nvme_queue_scan(ctrl);
+ *
+ * # echo 1 > /sys/block/nvme0n1/device/rescan_controller
+ */
 static void nvme_queue_scan(struct nvme_ctrl *ctrl)
 {
 	/*
 	 * Only new queue scan work when admin and IO queues are both alive
 	 */
+	/*
+	 * nvme_scan_work()
+	 */
 	if (ctrl->state == NVME_CTRL_LIVE && ctrl->tagset)
 		queue_work(nvme_wq, &ctrl->scan_work);
 }
@@ -126,32 +263,103 @@ static void nvme_queue_scan(struct nvme_ctrl *ctrl)
  * code paths that can't be interrupted by other reset attempts. A hot removal
  * may prevent this from succeeding.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4085| <<nvme_fw_act_work>> nvme_try_sched_reset(ctrl);
+ *   - drivers/nvme/host/pci.c|3529| <<nvme_reset_done>> if (!nvme_try_sched_reset(&dev->ctrl))
+ *   - drivers/nvme/host/pci.c|3595| <<nvme_resume>> return nvme_try_sched_reset(&ndev->ctrl);
+ *   - drivers/nvme/host/pci.c|3675| <<nvme_simple_resume>> return nvme_try_sched_reset(&ndev->ctrl);
+ */
 int nvme_try_sched_reset(struct nvme_ctrl *ctrl)
 {
 	if (ctrl->state != NVME_CTRL_RESETTING)
 		return -EBUSY;
+	/*
+	 * 在以下定于reset_work:
+	 *   - drivers/nvme/host/fc.c|3107| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2830| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2018| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2298| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|580| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
 		return -EBUSY;
 	return 0;
 }
 EXPORT_SYMBOL_GPL(nvme_try_sched_reset);
 
+/*
+ * [0] nvme_reset_ctrl
+ * [0] nvme_probe
+ * [0] local_pci_probe
+ * [0] pci_device_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * loop启动没调用reset work!
+ * [0] nvme_reset_ctrl
+ * [0] nvme_reset_ctrl_sync
+ * [0] nvme_sysfs_reset
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|195| <<nvme_reset_ctrl_sync>> ret = nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|1080| <<nvme_keep_alive_work>> nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|776| <<nvme_fc_ctrl_connectivity_loss>> if (nvme_reset_ctrl(&ctrl->ctrl)) {
+ *   - drivers/nvme/host/fc.c|2106| <<nvme_fc_error_recovery>> nvme_reset_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/multipath.c|114| <<nvme_failover_req>> nvme_reset_ctrl(ns->ctrl);
+ *   - drivers/nvme/host/multipath.c|591| <<nvme_anatt_timeout>> nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/pci.c|1485| <<nvme_timeout>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|1532| <<nvme_timeout>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3492| <<nvme_probe>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3720| <<nvme_slot_reset>> nvme_reset_ctrl(&dev->ctrl);
+ */
 int nvme_reset_ctrl(struct nvme_ctrl *ctrl)
 {
 	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))
 		return -EBUSY;
+	/*
+	 * 在以下定于reset_work:
+	 *   - drivers/nvme/host/fc.c|3107| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2830| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2018| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2298| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|580| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
 		return -EBUSY;
 	return 0;
 }
 EXPORT_SYMBOL_GPL(nvme_reset_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3157| <<nvme_dev_ioctl>> return nvme_reset_ctrl_sync(ctrl);
+ *   - drivers/nvme/host/core.c|3182| <<nvme_sysfs_reset>> ret = nvme_reset_ctrl_sync(ctrl);
+ */
 int nvme_reset_ctrl_sync(struct nvme_ctrl *ctrl)
 {
 	int ret;
 
 	ret = nvme_reset_ctrl(ctrl);
 	if (!ret) {
+		/*
+		 * wait for a work to finish executing the last queueing instance
+		 * @work: the work to flush
+		 */
 		flush_work(&ctrl->reset_work);
 		if (ctrl->state != NVME_CTRL_LIVE)
 			ret = -ENETRESET;
@@ -161,6 +369,11 @@ int nvme_reset_ctrl_sync(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_reset_ctrl_sync);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|247| <<nvme_delete_ctrl_work>> nvme_do_delete_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|272| <<nvme_delete_ctrl_sync>> nvme_do_delete_ctrl(ctrl);
+ */
 static void nvme_do_delete_ctrl(struct nvme_ctrl *ctrl)
 {
 	dev_info(ctrl->device,
@@ -169,6 +382,9 @@ static void nvme_do_delete_ctrl(struct nvme_ctrl *ctrl)
 	flush_work(&ctrl->reset_work);
 	nvme_stop_ctrl(ctrl);
 	nvme_remove_namespaces(ctrl);
+	/*
+	 * 没找到pci的实现
+	 */
 	ctrl->ops->delete_ctrl(ctrl);
 	nvme_uninit_ctrl(ctrl);
 	nvme_put_ctrl(ctrl);
@@ -182,16 +398,42 @@ static void nvme_delete_ctrl_work(struct work_struct *work)
 	nvme_do_delete_ctrl(ctrl);
 }
 
+/*
+ * 调用者没有pci:
+ *   - drivers/nvme/host/fc.c|780| <<nvme_fc_ctrl_connectivity_loss>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/fc.c|847| <<nvme_fc_unregister_remoteport>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/fc.c|2909| <<nvme_fc_reconnect_or_delete>> WARN_ON(nvme_delete_ctrl(&ctrl->ctrl));
+ *   - drivers/nvme/host/fc.c|3521| <<nvme_fc_delete_controllers>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|977| <<nvme_rdma_reconnect_or_remove>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|2101| <<nvme_rdma_remove_one>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|2141| <<nvme_rdma_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1822| <<nvme_tcp_reconnect_or_remove>> nvme_delete_ctrl(ctrl);
+ *   - drivers/nvme/host/tcp.c|2409| <<nvme_tcp_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|438| <<nvme_loop_delete_ctrl>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|713| <<nvme_loop_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+ *
+ * 调用者没有pci
+ */
 int nvme_delete_ctrl(struct nvme_ctrl *ctrl)
 {
 	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_DELETING))
 		return -EBUSY;
+	/*
+	 * 在以下使用delete_worl:
+	 *   - drivers/nvme/host/core.c|362| <<nvme_delete_ctrl_work>> container_of(work, struct nvme_ctrl, delete_work);
+	 *   - drivers/nvme/host/core.c|387| <<nvme_delete_ctrl>> if (!queue_work(nvme_delete_wq, &ctrl->delete_work))
+	 *   - drivers/nvme/host/core.c|5801| <<nvme_init_ctrl>> INIT_WORK(&ctrl->delete_work, nvme_delete_ctrl_work);
+	 */
 	if (!queue_work(nvme_delete_wq, &ctrl->delete_work))
 		return -EBUSY;
 	return 0;
 }
 EXPORT_SYMBOL_GPL(nvme_delete_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3449| <<nvme_sysfs_delete>> nvme_delete_ctrl_sync(ctrl);
+ */
 static int nvme_delete_ctrl_sync(struct nvme_ctrl *ctrl)
 {
 	int ret = 0;
@@ -209,11 +451,24 @@ static int nvme_delete_ctrl_sync(struct nvme_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|793| <<nvme_setup_rw>> if (WARN_ON_ONCE(!nvme_ns_has_pi(ns)))
+ *   - drivers/nvme/host/core.c|2011| <<nvme_update_disk_info>> if ((ns->ms && !nvme_ns_has_pi(ns) && !blk_get_integrity(disk)) ||
+ */
 static inline bool nvme_ns_has_pi(struct nvme_ns *ns)
 {
 	return ns->pi_type && ns->ms == sizeof(struct t10_pi_tuple);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|391| <<nvme_complete_rq>> blk_status_t status = nvme_error_status(nvme_req(req)->status);
+ *   - drivers/nvme/host/core.c|2148| <<nvme_revalidate_disk>> ret = blk_status_to_errno(nvme_error_status(ret));
+ *   - drivers/nvme/host/core.c|3681| <<nvme_alloc_ns_head>> ret = blk_status_to_errno(nvme_error_status(ret));
+ *   - drivers/nvme/host/core.c|3731| <<nvme_init_ns_head>> ret = blk_status_to_errno(nvme_error_status(ret));
+ *   - drivers/nvme/host/core.c|3920| <<nvme_alloc_ns>> ret = blk_status_to_errno(nvme_error_status(ret));
+ */
 static blk_status_t nvme_error_status(u16 status)
 {
 	switch (status & 0x7ff) {
@@ -252,10 +507,21 @@ static blk_status_t nvme_error_status(u16 status)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|400| <<nvme_complete_rq>> if (unlikely(status != BLK_STS_OK && nvme_req_needs_retry(req))) {
+ */
 static inline bool nvme_req_needs_retry(struct request *req)
 {
 	if (blk_noretry_request(req))
 		return false;
+	/*
+	 * Do Not Retry (DNR): If set to '1', indicates that if the same command is re-submitted to any
+	 * controller in the NVM subsystem, then that re-submitted command is expected to fail. If cleared to
+	 * '0', indicates that the same command may succeed if retried. If a command is aborted due to time
+	 * limited error recovery (refer to section 5.21.1.5), this bit should be cleared to '0'. If the SCT and
+	 * SC fields are cleared to 0h, then this bit should be cleared to '0'.
+	 */
 	if (nvme_req(req)->status & NVME_SC_DNR)
 		return false;
 	if (nvme_req(req)->retries >= nvme_max_retries)
@@ -263,6 +529,10 @@ static inline bool nvme_req_needs_retry(struct request *req)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|408| <<nvme_complete_rq>> nvme_retry_req(req);
+ */
 static void nvme_retry_req(struct request *req)
 {
 	struct nvme_ns *ns = req->q->queuedata;
@@ -270,6 +540,18 @@ static void nvme_retry_req(struct request *req)
 	u16 crd;
 
 	/* The mask and shift result must be <= 3 */
+	/*
+	 * Command Retry Delay (CRD): If the DNR bit is cleared to ‘0’ and the host has set the Advanced
+	 * Command Retry Enable (ACRE) field to 1h in the Host Behavior Support feature (refer to section
+	 * 5.21.1.22), then:
+	 * a) a 00b CRD value indicates a command retry delay time of zero (i.e., the host may retry the
+	 * command immediately); and
+	 * b) a non-zero CRD value selects a field in the Identify Controller data structure (refer to Figure
+	 * 247) that indicates the command retry delay time:
+	 *   - a 01b CRD value selects the Command Retry Delay Time 1 (CRDT1) field;
+	 *   - a 10b CRD value selects the Command Retry Delay Time 2 (CRDT2) field; and
+	 *   - a 11b CRD value selects the Command Retry Delay Time 3 (CRDT3) field.
+	 */
 	crd = (nvme_req(req)->status & NVME_SC_CRD) >> 11;
 	if (ns && crd)
 		delay = ns->ctrl->crdt[crd - 1] * 100;
@@ -279,6 +561,17 @@ static void nvme_retry_req(struct request *req)
 	blk_mq_delay_kick_requeue_list(req->q, delay);
 }
 
+/*
+ * called by:
+ *   - struct blk_mq_ops nvme_tcp_mq_ops.complete = nvme_complete_rq()
+ *   - struct blk_mq_ops nvme_tcp_admin_mq_ops.complete = nvme_complete_rq()
+ *   - drivers/nvme/host/fabrics.c|557| <<nvmf_fail_nonready_command>> nvme_complete_rq(rq);
+ *   - drivers/nvme/host/fc.c|2402| <<nvme_fc_complete_rq>> nvme_complete_rq(rq);
+ *   - drivers/nvme/host/pci.c|1202| <<nvme_pci_complete_rq>> nvme_complete_rq(req);
+ *   - drivers/nvme/host/rdma.c|1814| <<nvme_rdma_complete_rq>> nvme_complete_rq(rq);
+ *   - drivers/nvme/host/trace.h|85| <<__field>> TRACE_EVENT(nvme_complete_rq,
+ *   - drivers/nvme/target/loop.c|80| <<nvme_loop_complete_rq>> nvme_complete_rq(req);
+ */
 void nvme_complete_rq(struct request *req)
 {
 	blk_status_t status = nvme_error_status(nvme_req(req)->status);
@@ -290,14 +583,27 @@ void nvme_complete_rq(struct request *req)
 	if (nvme_req(req)->ctrl->kas)
 		nvme_req(req)->ctrl->comp_seen = true;
 
+	/*
+	 * 只在这里调用nvme_req_needs_retry()
+	 */
 	if (unlikely(status != BLK_STS_OK && nvme_req_needs_retry(req))) {
 		if ((req->cmd_flags & REQ_NVME_MPATH) &&
 		    blk_path_error(status)) {
+			/* 只在此处调用nvme_failover_req() */
 			nvme_failover_req(req);
 			return;
 		}
 
+		/*
+		 * 在以下使用QUEUE_FLAG_DYING:
+		 *   - block/blk-core.c|322| <<blk_set_queue_dying>> blk_queue_flag_set(QUEUE_FLAG_DYING, q);
+		 *   - block/blk-core.c|412| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_DYING, q);
+		 *   - include/linux/blkdev.h|813| <<blk_queue_dying>> #define blk_queue_dying(q) test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
+		 */
 		if (!blk_queue_dying(req->q)) {
+			/*
+			 * 只在这里调用nvme_retry_req()
+			 */
 			nvme_retry_req(req);
 			return;
 		}
@@ -308,6 +614,17 @@ void nvme_complete_rq(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_complete_rq);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3082| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3083| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|920| <<nvme_rdma_teardown_admin_queue>> nvme_cancel_request, &ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|936| <<nvme_rdma_teardown_io_queues>> nvme_cancel_request, &ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1781| <<nvme_tcp_teardown_admin_queue>> nvme_cancel_request, ctrl);
+ *   - drivers/nvme/host/tcp.c|1798| <<nvme_tcp_teardown_io_queues>> nvme_cancel_request, ctrl);
+ *   - drivers/nvme/target/loop.c|411| <<nvme_loop_shutdown_ctrl>> nvme_cancel_request, &ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|421| <<nvme_loop_shutdown_ctrl>> nvme_cancel_request, &ctrl->ctrl);
+ */
 bool nvme_cancel_request(struct request *req, void *data, bool reserved)
 {
 	dev_dbg_ratelimited(((struct nvme_ctrl *) data)->device,
@@ -317,12 +634,52 @@ bool nvme_cancel_request(struct request *req, void *data, bool reserved)
 	if (blk_mq_request_completed(req))
 		return true;
 
+	/*
+	 * 在以下使用NVME_SC_HOST_ABORTED_CMD:
+	 *   - drivers/nvme/host/core.c|490| <<nvme_cancel_request>> nvme_req(req)->status = NVME_SC_HOST_ABORTED_CMD;
+	 *   - drivers/nvme/host/multipath.c|102| <<nvme_failover_req>> case NVME_SC_HOST_ABORTED_CMD:
+	 *
+	 * nvme_req(req)返回nvme_request
+	 */
 	nvme_req(req)->status = NVME_SC_HOST_ABORTED_CMD;
 	blk_mq_complete_request(req);
 	return true;
 }
 EXPORT_SYMBOL_GPL(nvme_cancel_request);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|264| <<nvme_reset_ctrl>> if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))
+ *   - drivers/nvme/host/core.c|343| <<nvme_delete_ctrl>> if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_DELETING))
+ *   - drivers/nvme/host/core.c|364| <<nvme_delete_ctrl_sync>> if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_DELETING))
+ *   - drivers/nvme/host/core.c|683| <<nvme_wait_reset>> nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING) ||
+ *   - drivers/nvme/host/core.c|5064| <<nvme_fw_act_work>> if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_LIVE))
+ *   - drivers/nvme/host/core.c|5093| <<nvme_handle_aen_notice>> if (nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))
+ *   - drivers/nvme/host/fc.c|2805| <<nvme_fc_create_association>> changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
+ *   - drivers/nvme/host/fc.c|3008| <<__nvme_fc_terminate_io>> !nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING))
+ *   - drivers/nvme/host/fc.c|3255| <<nvme_fc_init_ctrl>> if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RESETTING) ||
+ *   - drivers/nvme/host/fc.c|3256| <<nvme_fc_init_ctrl>> !nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {
+ *   - drivers/nvme/host/fc.c|3281| <<nvme_fc_init_ctrl>> nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_DELETING);
+ *   - drivers/nvme/host/pci.c|1973| <<nvme_timeout>> nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING);
+ *   - drivers/nvme/host/pci.c|3980| <<nvme_remove_dead_ctrl>> nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING);
+ *   - drivers/nvme/host/pci.c|4127| <<nvme_reset_work>> if (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_CONNECTING)) {
+ *   - drivers/nvme/host/pci.c|4209| <<nvme_reset_work>> if (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_LIVE)) {
+ *   - drivers/nvme/host/pci.c|4514| <<nvme_remove>> nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING);
+ *   - drivers/nvme/host/pci.c|4519| <<nvme_remove>> nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DEAD);
+ *   - drivers/nvme/host/rdma.c|1023| <<nvme_rdma_setup_ctrl>> changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
+ *   - drivers/nvme/host/rdma.c|1077| <<nvme_rdma_error_recovery_work>> if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {
+ *   - drivers/nvme/host/rdma.c|1088| <<nvme_rdma_error_recovery>> if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RESETTING))
+ *   - drivers/nvme/host/rdma.c|1911| <<nvme_rdma_reset_ctrl_work>> if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {
+ *   - drivers/nvme/host/rdma.c|2036| <<nvme_rdma_create_ctrl>> changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING);
+ *   - drivers/nvme/host/tcp.c|422| <<nvme_tcp_error_recovery>> if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))
+ *   - drivers/nvme/host/tcp.c|1858| <<nvme_tcp_setup_ctrl>> if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_LIVE)) {
+ *   - drivers/nvme/host/tcp.c|1914| <<nvme_tcp_error_recovery_work>> if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_CONNECTING)) {
+ *   - drivers/nvme/host/tcp.c|1950| <<nvme_reset_ctrl_work>> if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_CONNECTING)) {
+ *   - drivers/nvme/host/tcp.c|2344| <<nvme_tcp_create_ctrl>> if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {
+ *   - drivers/nvme/target/loop.c|486| <<nvme_loop_reset_ctrl_work>> if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {
+ *   - drivers/nvme/target/loop.c|507| <<nvme_loop_reset_ctrl_work>> changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
+ *   - drivers/nvme/target/loop.c|680| <<nvme_loop_create_ctrl>> changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
+ */
 bool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,
 		enum nvme_ctrl_state new_state)
 {
@@ -391,6 +748,12 @@ bool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,
 
 	if (changed) {
 		ctrl->state = new_state;
+		/*
+		 * 在以下使用state_wq:
+		 *   - drivers/nvme/host/core.c|535| <<nvme_change_ctrl_state>> wake_up_all(&ctrl->state_wq);
+		 *   - drivers/nvme/host/core.c|571| <<nvme_wait_reset>> wait_event(ctrl->state_wq,
+		 *   - drivers/nvme/host/core.c|4515| <<nvme_init_ctrl>> init_waitqueue_head(&ctrl->state_wq);
+		 */
 		wake_up_all(&ctrl->state_wq);
 	}
 
@@ -404,6 +767,10 @@ EXPORT_SYMBOL_GPL(nvme_change_ctrl_state);
 /*
  * Returns true for sink states that can't ever transition back to live.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|684| <<nvme_wait_reset>> nvme_state_terminal(ctrl));
+ */
 static bool nvme_state_terminal(struct nvme_ctrl *ctrl)
 {
 	switch (ctrl->state) {
@@ -425,8 +792,18 @@ static bool nvme_state_terminal(struct nvme_ctrl *ctrl)
  * Waits for the controller state to be resetting, or returns false if it is
  * not possible to ever transition to that state.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3102| <<nvme_disable_prepare_reset>> if (!nvme_wait_reset(&dev->ctrl))
+ */
 bool nvme_wait_reset(struct nvme_ctrl *ctrl)
 {
+	/*
+	 * 在以下使用state_wq:
+	 *   - drivers/nvme/host/core.c|535| <<nvme_change_ctrl_state>> wake_up_all(&ctrl->state_wq);
+	 *   - drivers/nvme/host/core.c|571| <<nvme_wait_reset>> wait_event(ctrl->state_wq,
+	 *   - drivers/nvme/host/core.c|4515| <<nvme_init_ctrl>> init_waitqueue_head(&ctrl->state_wq);
+	 */
 	wait_event(ctrl->state_wq,
 		   nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING) ||
 		   nvme_state_terminal(ctrl));
@@ -434,13 +811,22 @@ bool nvme_wait_reset(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_wait_reset);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|593| <<nvme_put_ns_head>> kref_put(&head->ref, nvme_free_ns_head);
+ */
 static void nvme_free_ns_head(struct kref *ref)
 {
+	/*
+	 * struct nvme_ns_head:
+	 *   - struct kref ref;
+	 */
 	struct nvme_ns_head *head =
 		container_of(ref, struct nvme_ns_head, ref);
 
 	nvme_mpath_remove_disk(head);
 	ida_simple_remove(&head->subsys->ns_ida, head->instance);
+	/* 链接在nvme_subsystem->nheads */
 	list_del_init(&head->entry);
 	cleanup_srcu_struct(&head->srcu);
 	nvme_put_subsystem(head->subsys);
@@ -452,6 +838,10 @@ static void nvme_put_ns_head(struct nvme_ns_head *head)
 	kref_put(&head->ref, nvme_free_ns_head);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|783| <<nvme_put_ns>> kref_put(&ns->kref, nvme_free_ns);
+ */
 static void nvme_free_ns(struct kref *kref)
 {
 	struct nvme_ns *ns = container_of(kref, struct nvme_ns, kref);
@@ -465,11 +855,25 @@ static void nvme_free_ns(struct kref *kref)
 	kfree(ns);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2041| <<nvme_open>> nvme_put_ns(ns);
+ *   - drivers/nvme/host/core.c|2051| <<nvme_release>> nvme_put_ns(ns);
+ *   - drivers/nvme/host/core.c|3540| <<nvme_dev_user_cmd>> nvme_put_ns(ns);
+ *   - drivers/nvme/host/core.c|4322| <<nvme_ns_remove>> nvme_put_ns(ns);
+ *   - drivers/nvme/host/core.c|4338| <<nvme_validate_ns>> nvme_put_ns(ns);
+ *   - drivers/nvme/host/core.c|4400| <<nvme_scan_ns_list>> nvme_put_ns(ns);
+ */
 static void nvme_put_ns(struct nvme_ns *ns)
 {
 	kref_put(&ns->kref, nvme_free_ns);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|821| <<nvme_alloc_request>> nvme_clear_nvme_request(req);
+ *   - drivers/nvme/host/core.c|1118| <<nvme_setup_cmd>> nvme_clear_nvme_request(req);
+ */
 static inline void nvme_clear_nvme_request(struct request *req)
 {
 	if (!(req->rq_flags & RQF_DONTPREP)) {
@@ -479,6 +883,16 @@ static inline void nvme_clear_nvme_request(struct request *req)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|829| <<__nvme_submit_sync_cmd>> req = nvme_alloc_request(q, cmd, flags, qid);
+ *   - drivers/nvme/host/core.c|912| <<nvme_submit_user_cmd>> req = nvme_alloc_request(q, cmd, 0, NVME_QID_ANY);
+ *   - drivers/nvme/host/core.c|986| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+ *   - drivers/nvme/host/lightnvm.c|656| <<nvme_nvm_alloc_request>> rq = nvme_alloc_request(q, (struct nvme_command *)cmd, 0, NVME_QID_ANY);
+ *   - drivers/nvme/host/lightnvm.c|770| <<nvme_nvm_submit_user_cmd>> rq = nvme_alloc_request(q, (struct nvme_command *)vcmd, 0,
+ *   - drivers/nvme/host/pci.c|1346| <<nvme_timeout>> abort_req = nvme_alloc_request(dev->ctrl.admin_q, &cmd,
+ *   - drivers/nvme/host/pci.c|2255| <<nvme_delete_queue>> req = nvme_alloc_request(q, &cmd, BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
+ */
 struct request *nvme_alloc_request(struct request_queue *q,
 		struct nvme_command *cmd, blk_mq_req_flags_t flags, int qid)
 {
@@ -523,11 +937,20 @@ static int nvme_disable_streams(struct nvme_ctrl *ctrl)
 	return nvme_toggle_streams(ctrl, false);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|709| <<nvme_configure_directives>> ret = nvme_enable_streams(ctrl);
+ */
 static int nvme_enable_streams(struct nvme_ctrl *ctrl)
 {
 	return nvme_toggle_streams(ctrl, true);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|893| <<nvme_configure_directives>> ret = nvme_get_stream_params(ctrl, &s, NVME_NSID_ALL);
+ *   - drivers/nvme/host/core.c|4495| <<nvme_setup_streams_ns>> ret = nvme_get_stream_params(ctrl, &s, ns->head->ns_id);
+ */
 static int nvme_get_stream_params(struct nvme_ctrl *ctrl,
 				  struct streams_directive_params *s, u32 nsid)
 {
@@ -545,6 +968,10 @@ static int nvme_get_stream_params(struct nvme_ctrl *ctrl,
 	return nvme_submit_sync_cmd(ctrl->admin_q, &c, s, sizeof(*s));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3237| <<nvme_init_identify>> ret = nvme_configure_directives(ctrl);
+ */
 static int nvme_configure_directives(struct nvme_ctrl *ctrl)
 {
 	struct streams_directive_params s;
@@ -580,6 +1007,10 @@ static int nvme_configure_directives(struct nvme_ctrl *ctrl)
  * Check if 'req' has a write hint associated with it. If it does, assign
  * a valid namespace stream to the write.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|859| <<nvme_setup_rw>> nvme_assign_write_stream(ctrl, req, &control, &dsmgmt);
+ */
 static void nvme_assign_write_stream(struct nvme_ctrl *ctrl,
 				     struct request *req, u16 *control,
 				     u32 *dsmgmt)
@@ -601,6 +1032,32 @@ static void nvme_assign_write_stream(struct nvme_ctrl *ctrl,
 		req->q->write_hints[streamid] += blk_rq_bytes(req) >> 9;
 }
 
+/*
+ * struct nvme_command {
+ *	union {
+ *		struct nvme_common_command common;
+ *		struct nvme_rw_command rw;
+ *		struct nvme_identify identify;
+ *		struct nvme_features features;
+ *		struct nvme_create_cq create_cq;
+ *		struct nvme_create_sq create_sq;
+ *		struct nvme_delete_queue delete_queue;
+ *		struct nvme_download_firmware dlfw;
+ *		struct nvme_format_cmd format;
+ *		struct nvme_dsm_cmd dsm;
+ *		struct nvme_write_zeroes_cmd write_zeroes;
+ *		struct nvme_abort_cmd abort;
+ *		struct nvme_get_log_page_command get_log_page;
+ *		struct nvmf_common_command fabrics;
+ *		struct nvmf_connect_command connect;
+ *		struct nvmf_property_set_command prop_set;
+ *		struct nvmf_property_get_command prop_get;
+ *		struct nvme_dbbuf dbbuf;
+ *		struct nvme_directive_cmd directive;
+ *	};
+ * };
+ */
+
 static inline void nvme_setup_flush(struct nvme_ns *ns,
 		struct nvme_command *cmnd)
 {
@@ -608,6 +1065,11 @@ static inline void nvme_setup_flush(struct nvme_ns *ns,
 	cmnd->common.nsid = cpu_to_le32(ns->head->ns_id);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1050| <<nvme_setup_write_zeroes>> return nvme_setup_discard(ns, req, cmnd);
+ *   - drivers/nvme/host/core.c|1176| <<nvme_setup_cmd>> ret = nvme_setup_discard(ns, req, cmd); --> REQ_OP_DISCARD
+ */
 static blk_status_t nvme_setup_discard(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmnd)
 {
@@ -684,6 +1146,10 @@ static inline blk_status_t nvme_setup_write_zeroes(struct nvme_ns *ns,
 	return BLK_STS_OK;
 }
 
+/*
+ * 处理REQ_OP_READ和REQ_OP_WRITE:
+ *   - drivers/nvme/host/core.c|978| <<nvme_setup_cmd>> ret = nvme_setup_rw(ns, req, cmd);
+ */
 static inline blk_status_t nvme_setup_rw(struct nvme_ns *ns,
 		struct request *req, struct nvme_command *cmnd)
 {
@@ -738,6 +1204,16 @@ static inline blk_status_t nvme_setup_rw(struct nvme_ns *ns,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|395| <<nvme_complete_rq>> nvme_cleanup_cmd(req);
+ *   - drivers/nvme/host/fc.c|2269| <<nvme_fc_start_fcp_op>> nvme_cleanup_cmd(op->rq);
+ *   - drivers/nvme/host/fc.c|2309| <<nvme_fc_start_fcp_op>> nvme_cleanup_cmd(op->rq);
+ *   - drivers/nvme/host/pci.c|1056| <<nvme_queue_rq>> nvme_cleanup_cmd(req);
+ *   - drivers/nvme/host/rdma.c|1791| <<nvme_rdma_queue_rq>> nvme_cleanup_cmd(rq);
+ *   - drivers/nvme/host/tcp.c|2139| <<nvme_tcp_setup_cmd_pdu>> nvme_cleanup_cmd(rq);
+ *   - drivers/nvme/target/loop.c|160| <<nvme_loop_queue_rq>> nvme_cleanup_cmd(req);
+ */
 void nvme_cleanup_cmd(struct request *req)
 {
 	if (req->rq_flags & RQF_SPECIAL_PAYLOAD) {
@@ -752,6 +1228,15 @@ void nvme_cleanup_cmd(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/trace.h|47| <<global>> TRACE_EVENT(nvme_setup_cmd,
+ *   - drivers/nvme/host/fc.c|2342| <<nvme_fc_queue_rq>> ret = nvme_setup_cmd(ns, rq, sqe);
+ *   - drivers/nvme/host/pci.c|896| <<nvme_queue_rq>> ret = nvme_setup_cmd(ns, req, &cmnd);
+ *   - drivers/nvme/host/rdma.c|1759| <<nvme_rdma_queue_rq>> ret = nvme_setup_cmd(ns, rq, c);
+ *   - drivers/nvme/host/tcp.c|2106| <<nvme_tcp_setup_cmd_pdu>> ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+ *   - drivers/nvme/target/loop.c|144| <<nvme_loop_queue_rq>> ret = nvme_setup_cmd(ns, req, &iod->cmd);
+ */
 blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmd)
 {
@@ -763,6 +1248,9 @@ blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 	switch (req_op(req)) {
 	case REQ_OP_DRV_IN:
 	case REQ_OP_DRV_OUT:
+		/*
+		 * 用blk_mq_rq_to_pdu(req)返回struct nvme_request
+		 */
 		memcpy(cmd, nvme_req(req)->cmd, sizeof(*cmd));
 		break;
 	case REQ_OP_FLUSH:
@@ -789,6 +1277,10 @@ blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 }
 EXPORT_SYMBOL_GPL(nvme_setup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1008| <<nvme_execute_rq_polled>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, nvme_end_sync_rq);
+ */
 static void nvme_end_sync_rq(struct request *rq, blk_status_t error)
 {
 	struct completion *waiting = rq->end_io_data;
@@ -797,6 +1289,10 @@ static void nvme_end_sync_rq(struct request *rq, blk_status_t error)
 	complete(waiting);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1052| <<__nvme_submit_sync_cmd>> nvme_execute_rq_polled(req->q, NULL, req, at_head);
+ */
 static void nvme_execute_rq_polled(struct request_queue *q,
 		struct gendisk *bd_disk, struct request *rq, int at_head)
 {
@@ -818,6 +1314,17 @@ static void nvme_execute_rq_polled(struct request_queue *q,
  * Returns 0 on success.  If the result is negative, it's a Linux error code;
  * if the result is positive, it's an NVM Express status code
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|947| <<nvme_submit_sync_cmd>> return __nvme_submit_sync_cmd(q, cmd, NULL, buffer, bufflen, 0,
+ *   - drivers/nvme/host/core.c|1300| <<nvme_features>> ret = __nvme_submit_sync_cmd(dev->admin_q, &c, &res,
+ *   - drivers/nvme/host/core.c|2188| <<nvme_sec_submit>> return __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, NULL, buffer, len,
+ *   - drivers/nvme/host/fabrics.c|153| <<nvmf_reg_read32>> ret = __nvme_submit_sync_cmd(ctrl->fabrics_q, &cmd, &res, NULL, 0, 0,
+ *   - drivers/nvme/host/fabrics.c|200| <<nvmf_reg_read64>> ret = __nvme_submit_sync_cmd(ctrl->fabrics_q, &cmd, &res, NULL, 0, 0,
+ *   - drivers/nvme/host/fabrics.c|246| <<nvmf_reg_write32>> ret = __nvme_submit_sync_cmd(ctrl->fabrics_q, &cmd, NULL, NULL, 0, 0,
+ *   - drivers/nvme/host/fabrics.c|399| <<nvmf_connect_admin_queue>> ret = __nvme_submit_sync_cmd(ctrl->fabrics_q, &cmd, &res,
+ *   - drivers/nvme/host/fabrics.c|462| <<nvmf_connect_io_queue>> ret = __nvme_submit_sync_cmd(ctrl->connect_q, &cmd, &res,
+ */
 int __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
 		union nvme_result *result, void *buffer, unsigned bufflen,
 		unsigned timeout, int qid, int at_head,
@@ -854,6 +1361,25 @@ int __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
 }
 EXPORT_SYMBOL_GPL(__nvme_submit_sync_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|593| <<nvme_toggle_streams>> return nvme_submit_sync_cmd(ctrl->admin_q, &c, NULL, 0);
+ *   - drivers/nvme/host/core.c|620| <<nvme_get_stream_params>> return nvme_submit_sync_cmd(ctrl->admin_q, &c, s, sizeof(*s));
+ *   - drivers/nvme/host/core.c|1169| <<nvme_identify_ctrl>> error = nvme_submit_sync_cmd(dev->admin_q, &c, *id,
+ *   - drivers/nvme/host/core.c|1193| <<nvme_identify_ns_descs>> status = nvme_submit_sync_cmd(ctrl->admin_q, &c, data,
+ *   - drivers/nvme/host/core.c|1255| <<nvme_identify_ns_list>> return nvme_submit_sync_cmd(dev->admin_q, &c, ns_list,
+ *   - drivers/nvme/host/core.c|1279| <<nvme_identify_ns>> error = nvme_submit_sync_cmd(ctrl->admin_q, &c, *id, sizeof(**id));
+ *   - drivers/nvme/host/core.c|2113| <<nvme_pr_command>> ret = nvme_submit_sync_cmd(ns->queue, &c, data, 16);
+ *   - drivers/nvme/host/core.c|2894| <<nvme_get_log>> return nvme_submit_sync_cmd(ctrl->admin_q, &c, log, size);
+ *   - drivers/nvme/host/lightnvm.c|445| <<nvme_nvm_identity>> ret = nvme_submit_sync_cmd(ns->ctrl->admin_q, (struct nvme_command *)&c,
+ *   - drivers/nvme/host/lightnvm.c|496| <<nvme_nvm_get_bb_tbl>> ret = nvme_submit_sync_cmd(ctrl->admin_q, (struct nvme_command *)&c,
+ *   - drivers/nvme/host/lightnvm.c|544| <<nvme_nvm_set_bb_tbl>> ret = nvme_submit_sync_cmd(ns->ctrl->admin_q, (struct nvme_command *)&c,
+ *   - drivers/nvme/host/pci.c|429| <<nvme_dbbuf_set>> if (nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0)) {
+ *   - drivers/nvme/host/pci.c|1323| <<adapter_delete_queue>> return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+ *   - drivers/nvme/host/pci.c|1354| <<adapter_alloc_cq>> return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+ *   - drivers/nvme/host/pci.c|1391| <<adapter_alloc_sq>> return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+ *   - drivers/nvme/host/pci.c|2319| <<nvme_set_host_mem>> ret = nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+ */
 int nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
 		void *buffer, unsigned bufflen)
 {
@@ -862,6 +1388,10 @@ int nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
 }
 EXPORT_SYMBOL_GPL(nvme_submit_sync_cmd);
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/core.c|1168| <<nvme_submit_user_cmd>> meta = nvme_add_user_metadata(bio, meta_buffer, meta_len,
+ */
 static void *nvme_add_user_metadata(struct bio *bio, void __user *ubuf,
 		unsigned len, u32 seed, bool write)
 {
@@ -896,6 +1426,12 @@ static void *nvme_add_user_metadata(struct bio *bio, void __user *ubuf,
 	return ERR_PTR(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1589| <<nvme_submit_io>> return nvme_submit_user_cmd(ns->queue, &c,
+ *   - drivers/nvme/host/core.c|1742| <<nvme_user_cmd>> status = nvme_submit_user_cmd(ns ? ns->queue : ctrl->admin_q, &c,
+ *   - drivers/nvme/host/core.c|1789| <<nvme_user_cmd64>> status = nvme_submit_user_cmd(ns ? ns->queue : ctrl->admin_q, &c,
+ */
 static int nvme_submit_user_cmd(struct request_queue *q,
 		struct nvme_command *cmd, void __user *ubuffer,
 		unsigned bufflen, void __user *meta_buffer, unsigned meta_len,
@@ -954,6 +1490,10 @@ static int nvme_submit_user_cmd(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * 在以下使用nvme_keep_alive_end_io():
+ *   - drivers/nvme/host/core.c|1300| <<nvme_keep_alive>> blk_execute_rq_nowait(rq->q, NULL, rq, 0, nvme_keep_alive_end_io);
+ */
 static void nvme_keep_alive_end_io(struct request *rq, blk_status_t status)
 {
 	struct nvme_ctrl *ctrl = rq->end_io_data;
@@ -975,14 +1515,46 @@ static void nvme_keep_alive_end_io(struct request *rq, blk_status_t status)
 	    ctrl->state == NVME_CTRL_CONNECTING)
 		startka = true;
 	spin_unlock_irqrestore(&ctrl->lock, flags);
+	/*
+	 * 在以下使用nvme_ctrl->ka_work:
+	 *   - drivers/nvme/host/core.c|1285| <<nvme_keep_alive_end_io>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1308| <<nvme_keep_alive_work>> struct nvme_ctrl, ka_work);
+	 *   - drivers/nvme/host/core.c|1315| <<nvme_keep_alive_work>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1384| <<nvme_start_keep_alive>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1399| <<nvme_stop_keep_alive>> cancel_delayed_work_sync(&ctrl->ka_work);
+	 *   - drivers/nvme/host/core.c|5026| <<nvme_init_ctrl>> INIT_DELAYED_WORK(&ctrl->ka_work, nvme_keep_alive_work);
+	 */
 	if (startka)
 		schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1319| <<nvme_keep_alive_work>> if (nvme_keep_alive(ctrl)) {
+ */
 static int nvme_keep_alive(struct nvme_ctrl *ctrl)
 {
 	struct request *rq;
 
+	/*
+	 * 在以下使用BLK_MQ_REQ_RESERVED:
+	 *   - block/blk-mq-tag.c|151| <<blk_mq_get_tag>> if (data->flags & BLK_MQ_REQ_RESERVED) {
+	 *   - block/blk-mq-tag.c|207| <<blk_mq_get_tag>> if (data->flags & BLK_MQ_REQ_RESERVED)
+	 *   - block/blk-mq.c|430| <<blk_mq_get_request>> !(data->flags & BLK_MQ_REQ_RESERVED))
+	 *   - block/blk-mq.c|1381| <<blk_mq_get_driver_tag>> data.flags |= BLK_MQ_REQ_RESERVED;
+	 *   - drivers/block/mtip32xx/mtip32xx.c|985| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+	 *   - drivers/ide/ide-atapi.c|203| <<ide_prep_sense>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/nvme/host/core.c|1306| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+	 *   - drivers/nvme/host/fabrics.c|401| <<nvmf_connect_admin_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, false);
+	 *   - drivers/nvme/host/fabrics.c|464| <<nvmf_connect_io_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, poll);
+	 *
+	 * 比方在blk_mq_get_tag()中就会用tags->breserved_tags
+	 *
+	 * 在以下使用ka_cmd:
+	 *   - drivers/nvme/host/core.c|1300| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+	 *   - drivers/nvme/host/core.c|5038| <<nvme_init_ctrl>> memset(&ctrl->ka_cmd, 0, sizeof(ctrl->ka_cmd));
+	 *   - drivers/nvme/host/core.c|5039| <<nvme_init_ctrl>> ctrl->ka_cmd.common.opcode = nvme_admin_keep_alive;
+	 */
 	rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
 			NVME_QID_ANY);
 	if (IS_ERR(rq))
@@ -996,15 +1568,43 @@ static int nvme_keep_alive(struct nvme_ctrl *ctrl)
 	return 0;
 }
 
+/*
+ * 在以下使用nvme_ctrl->ka_work:
+ *   - drivers/nvme/host/core.c|1285| <<nvme_keep_alive_end_io>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+ *   - drivers/nvme/host/core.c|1308| <<nvme_keep_alive_work>> struct nvme_ctrl, ka_work);
+ *   - drivers/nvme/host/core.c|1315| <<nvme_keep_alive_work>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+ *   - drivers/nvme/host/core.c|1384| <<nvme_start_keep_alive>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+ *   - drivers/nvme/host/core.c|1399| <<nvme_stop_keep_alive>> cancel_delayed_work_sync(&ctrl->ka_work);
+ *   - drivers/nvme/host/core.c|5026| <<nvme_init_ctrl>> INIT_DELAYED_WORK(&ctrl->ka_work, nvme_keep_alive_work);
+ *
+ * 在以下使用nvme_keep_alive_work():
+ *   - drivers/nvme/host/core.c|5043| <<nvme_init_ctrl>> INIT_DELAYED_WORK(&ctrl->ka_work, nvme_keep_alive_work);
+ */
 static void nvme_keep_alive_work(struct work_struct *work)
 {
 	struct nvme_ctrl *ctrl = container_of(to_delayed_work(work),
 			struct nvme_ctrl, ka_work);
 	bool comp_seen = ctrl->comp_seen;
 
+	/*
+	 * 在以下使用NVME_CTRL_ATTR_TBKAS:
+	 *   - drivers/nvme/host/core.c|1350| <<nvme_keep_alive_work>> if ((ctrl->ctratt & NVME_CTRL_ATTR_TBKAS) && comp_seen) {
+	 *   - drivers/nvme/target/admin-cmd.c|367| <<nvmet_execute_identify_ctrl>> NVME_CTRL_ATTR_TBKAS);
+	 *
+	 * comp_seen说明有io的completion完成过,肯定是alive的
+	 * 不用再submit alive的cmd了
+	 */
 	if ((ctrl->ctratt & NVME_CTRL_ATTR_TBKAS) && comp_seen) {
 		dev_dbg(ctrl->device,
 			"reschedule traffic based keep-alive timer\n");
+		/*
+		 * 在以下使用comp_seen:
+		 *   - drivers/nvme/host/core.c|456| <<nvme_complete_rq>> nvme_req(req)->ctrl->comp_seen = true;
+		 *   - drivers/nvme/host/core.c|1282| <<nvme_keep_alive_end_io>> ctrl->comp_seen = false;
+		 *   - drivers/nvme/host/core.c|1348| <<nvme_keep_alive_work>> bool comp_seen = ctrl->comp_seen;
+		 *   - drivers/nvme/host/core.c|1350| <<nvme_keep_alive_work>> if ((ctrl->ctratt & NVME_CTRL_ATTR_TBKAS) && comp_seen) {
+		 *   - drivers/nvme/host/core.c|1353| <<nvme_keep_alive_work>> ctrl->comp_seen = false;
+		 */
 		ctrl->comp_seen = false;
 		schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
 		return;
@@ -1018,23 +1618,121 @@ static void nvme_keep_alive_work(struct work_struct *work)
 	}
 }
 
+/*
+ * commit 038bd4cb6766c69b5b9c77507f389cc718a36842
+ * Author: Sagi Grimberg <sagi@grimberg.me>
+ * Date:   Mon Jun 13 16:45:28 2016 +0200
+ *
+ * nvme: add keep-alive support
+ *
+ * Periodic keep-alive is a mandatory feature in NVMe over Fabrics, and
+ * optional in NVMe 1.2.1 for PCIe.  This patch adds periodic keep-alive
+ * sent from the host to verify that the controller is still responsive
+ * and vice-versa.  The keep-alive timeout is user-defined (with
+ * keep_alive_tmo connection parameter) and defaults to 5 seconds.
+ *
+ * In order to avoid a race condition where the host sends a keep-alive
+ * competing with the target side keep-alive timeout expiration, the host
+ * adds a grace period of 10 seconds when publishing the keep-alive timeout
+ * to the target.
+ *
+ * In case a keep-alive failed (or timed out), a transport specific error
+ * recovery kicks in.
+ *
+ * For now only NVMe over Fabrics is wired up to support keep alive, but
+ * we can add PCIe support easily once controllers actually supporting it
+ * become available.
+ *
+ * Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
+ * Reviewed-by: Steve Wise <swise@chelsio.com>
+ * Signed-off-by: Christoph Hellwig <hch@lst.de>
+ * Reviewed-by: Keith Busch <keith.busch@intel.com>
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4478| <<nvme_start_ctrl>> nvme_start_keep_alive(ctrl);
+ *
+ * 目前没见到pci用keep-alive
+ */
 static void nvme_start_keep_alive(struct nvme_ctrl *ctrl)
 {
+	/*
+	 * 设置nvme_ctrl->kato的地方 (没有pci):
+	 *   - drivers/nvme/host/fc.c|3119| <<nvme_fc_init_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/host/rdma.c|2023| <<nvme_rdma_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/host/tcp.c|2293| <<nvme_tcp_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/target/loop.c|593| <<nvme_loop_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 * 部分使用nvme_ctrl->kato的地方:
+	 *   - drivers/nvme/host/core.c|1285| <<nvme_keep_alive_end_io>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1297| <<nvme_keep_alive>> rq->timeout = ctrl->kato * HZ;
+	 *   - drivers/nvme/host/core.c|1315| <<nvme_keep_alive_work>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1366| <<nvme_start_keep_alive>> if (unlikely(ctrl->kato == 0))
+	 *   - drivers/nvme/host/core.c|1369| <<nvme_start_keep_alive>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1381| <<nvme_stop_keep_alive>> if (unlikely(ctrl->kato == 0))
+	 *   - drivers/nvme/host/core.c|4904| <<nvme_start_ctrl>> if (ctrl->kato)
+	 */
 	if (unlikely(ctrl->kato == 0))
 		return;
 
+	/*
+	 * 调用nvme_keep_alive_work()
+	 *
+	 * schedule_delayed_work - put work task in global workqueue after delay
+	 * @dwork: job to be done
+	 * @delay: number of jiffies to wait or 0 for immediate execution
+	 *
+	 * After waiting for a given time this puts a job in the kernel-global
+	 * workqueue.
+	 */
 	schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4756| <<nvme_stop_ctrl>> nvme_stop_keep_alive(ctrl);
+ *   - drivers/nvme/host/fc.c|2927| <<__nvme_fc_terminate_io>> nvme_stop_keep_alive(&ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|1071| <<nvme_rdma_error_recovery_work>> nvme_stop_keep_alive(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1907| <<nvme_tcp_error_recovery_work>> nvme_stop_keep_alive(ctrl);
+ */
 void nvme_stop_keep_alive(struct nvme_ctrl *ctrl)
 {
+	/*
+	 * 设置nvme_ctrl->kato的地方 (没有pci):
+	 *   - drivers/nvme/host/fc.c|3119| <<nvme_fc_init_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/host/rdma.c|2023| <<nvme_rdma_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/host/tcp.c|2293| <<nvme_tcp_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/target/loop.c|593| <<nvme_loop_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 * 部分使用nvme_ctrl->kato的地方:
+	 *   - drivers/nvme/host/core.c|1285| <<nvme_keep_alive_end_io>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1297| <<nvme_keep_alive>> rq->timeout = ctrl->kato * HZ;
+	 *   - drivers/nvme/host/core.c|1315| <<nvme_keep_alive_work>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1366| <<nvme_start_keep_alive>> if (unlikely(ctrl->kato == 0))
+	 *   - drivers/nvme/host/core.c|1369| <<nvme_start_keep_alive>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1381| <<nvme_stop_keep_alive>> if (unlikely(ctrl->kato == 0))
+	 *   - drivers/nvme/host/core.c|4904| <<nvme_start_ctrl>> if (ctrl->kato)
+	 */
 	if (unlikely(ctrl->kato == 0))
 		return;
 
+	/*
+	 * 在以下使用nvme_ctrl->ka_work:
+	 *   - drivers/nvme/host/core.c|1285| <<nvme_keep_alive_end_io>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1308| <<nvme_keep_alive_work>> struct nvme_ctrl, ka_work);
+	 *   - drivers/nvme/host/core.c|1315| <<nvme_keep_alive_work>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1384| <<nvme_start_keep_alive>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1399| <<nvme_stop_keep_alive>> cancel_delayed_work_sync(&ctrl->ka_work);
+	 *   - drivers/nvme/host/core.c|5026| <<nvme_init_ctrl>> INIT_DELAYED_WORK(&ctrl->ka_work, nvme_keep_alive_work);
+	 */
 	cancel_delayed_work_sync(&ctrl->ka_work);
 }
 EXPORT_SYMBOL_GPL(nvme_stop_keep_alive);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3128| <<nvme_init_identify>> ret = nvme_identify_ctrl(ctrl, &id);
+ *   - drivers/nvme/host/core.c|4199| <<nvme_scan_work>> if (nvme_identify_ctrl(ctrl, &id))
+ */
 static int nvme_identify_ctrl(struct nvme_ctrl *dev, struct nvme_id_ctrl **id)
 {
 	struct nvme_command c = { };
@@ -1055,6 +1753,10 @@ static int nvme_identify_ctrl(struct nvme_ctrl *dev, struct nvme_id_ctrl **id)
 	return error;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2081| <<nvme_report_ns_ids>> ret = nvme_identify_ns_descs(ctrl, nsid, ids);
+ */
 static int nvme_identify_ns_descs(struct nvme_ctrl *ctrl, unsigned nsid,
 		struct nvme_ns_ids *ids)
 {
@@ -1127,6 +1829,17 @@ static int nvme_identify_ns_descs(struct nvme_ctrl *ctrl, unsigned nsid,
 	return status;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4838| <<nvme_scan_ns_list>> ret = nvme_identify_ns_list(ctrl, prev, ns_list);
+ *
+ * A list of 1,024 namespace IDs is returned to the host containing active NSIDs in increasing order that are
+ * greater than the value specified in the Namespace Identifier (NSID) field of the command. The controller
+ * should abort the command with status code Invalid Namespace or Format if the NSID field is set to
+ * FFFFFFFEh or FFFFFFFFh. The NSID field may be cleared to 0h to retrieve a Namespace List including
+ * the namespace starting with NSID of 1h. The data structure returned is a Namespace List (refer to section
+ * 4.10).
+ */
 static int nvme_identify_ns_list(struct nvme_ctrl *dev, unsigned nsid, __le32 *ns_list)
 {
 	struct nvme_command c = { };
@@ -1138,12 +1851,22 @@ static int nvme_identify_ns_list(struct nvme_ctrl *dev, unsigned nsid, __le32 *n
 				    NVME_IDENTIFY_DATA_SIZE);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2068| <<nvme_revalidate_disk>> ret = nvme_identify_ns(ctrl, ns->head->ns_id, &id);
+ *   - drivers/nvme/host/core.c|3782| <<nvme_alloc_ns>> ret = nvme_identify_ns(ctrl, nsid, &id);
+ */
 static int nvme_identify_ns(struct nvme_ctrl *ctrl,
 		unsigned nsid, struct nvme_id_ns **id)
 {
 	struct nvme_command c = { };
 	int error;
 
+	/*
+	 * The Identify command returns a data buffer that describes information about the NVM subsystem, the
+	 * controller or the namespace(s). The data structure is 4,096 bytes in size.
+	 */
+
 	/* gcc-4.4.4 (at least) has issues with initializers and anon unions */
 	c.identify.opcode = nvme_admin_identify;
 	c.identify.nsid = cpu_to_le32(nsid);
@@ -1181,6 +1904,16 @@ static int nvme_features(struct nvme_ctrl *dev, u8 op, unsigned int fid,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1520| <<nvme_set_queue_count>> status = nvme_set_features(ctrl, NVME_FEAT_NUM_QUEUES, q_count, NULL, 0,
+ *   - drivers/nvme/host/core.c|1554| <<nvme_enable_aen>> status = nvme_set_features(ctrl, NVME_FEAT_ASYNC_EVENT, supported_aens,
+ *   - drivers/nvme/host/core.c|2589| <<nvme_configure_timestamp>> ret = nvme_set_features(ctrl, NVME_FEAT_TIMESTAMP, 0, &ts, sizeof(ts),
+ *   - drivers/nvme/host/core.c|2611| <<nvme_configure_acre>> ret = nvme_set_features(ctrl, NVME_FEAT_HOST_BEHAVIOR, 0,
+ *   - drivers/nvme/host/core.c|2731| <<nvme_configure_apst>> ret = nvme_set_features(ctrl, NVME_FEAT_AUTO_PST, apste,
+ *   - drivers/nvme/host/hwmon.c|55| <<nvme_set_temp_thresh>> ret = nvme_set_features(ctrl, NVME_FEAT_TEMP_THRESH, threshold, NULL, 0,
+ *   - drivers/nvme/host/pci.c|3619| <<nvme_set_power_state>> return nvme_set_features(ctrl, NVME_FEAT_POWER_MGMT, ps, NULL, 0, NULL);
+ */
 int nvme_set_features(struct nvme_ctrl *dev, unsigned int fid,
 		      unsigned int dword11, void *buffer, size_t buflen,
 		      u32 *result)
@@ -1199,6 +1932,17 @@ int nvme_get_features(struct nvme_ctrl *dev, unsigned int fid,
 }
 EXPORT_SYMBOL_GPL(nvme_get_features);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2449| <<nvme_fc_create_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/fc.c|2523| <<nvme_fc_recreate_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2538| <<nvme_setup_io_queues>> result = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/rdma.c|664| <<nvme_rdma_alloc_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/tcp.c|1635| <<nvme_tcp_alloc_io_queues>> ret = nvme_set_queue_count(ctrl, &nr_io_queues);
+ *   - drivers/nvme/target/loop.c|299| <<nvme_loop_init_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ *   
+ * 通过admin queue设置
+ */
 int nvme_set_queue_count(struct nvme_ctrl *ctrl, int *count)
 {
 	u32 q_count = (*count - 1) | ((*count - 1) << 16);
@@ -1231,6 +1975,12 @@ EXPORT_SYMBOL_GPL(nvme_set_queue_count);
 	(NVME_AEN_CFG_NS_ATTR | NVME_AEN_CFG_FW_ACT | \
 	 NVME_AEN_CFG_ANA_CHANGE | NVME_AEN_CFG_DISC_CHANGE)
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4499| <<nvme_start_ctrl>> nvme_enable_aen(ctrl);
+ *
+ * Asynchronous Event Notification
+ */
 static void nvme_enable_aen(struct nvme_ctrl *ctrl)
 {
 	u32 result, supported_aens = ctrl->oaes & NVME_AEN_SUPPORTED;
@@ -1245,9 +1995,21 @@ static void nvme_enable_aen(struct nvme_ctrl *ctrl)
 		dev_warn(ctrl->device, "Failed to configure AEN (cfg %x)\n",
 			 supported_aens);
 
+	/*
+	 * 在以下使用async_event_work:
+	 *   - drivers/nvme/host/core.c|1580| <<nvme_enable_aen>> queue_work(nvme_wq, &ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4345| <<nvme_async_event_work>> container_of(work, struct nvme_ctrl, async_event_work);
+	 *   - drivers/nvme/host/core.c|4483| <<nvme_complete_async_event>> queue_work(nvme_wq, &ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4500| <<nvme_stop_ctrl>> flush_work(&ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4589| <<nvme_init_ctrl>> INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
+	 */
 	queue_work(nvme_wq, &ctrl->async_event_work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1936| <<nvme_ioctl>> ret = nvme_submit_io(ns, argp);
+ */
 static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
 {
 	struct nvme_user_io io;
@@ -1298,6 +2060,10 @@ static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
 			metadata, meta_len, lower_32_bits(io.slba), NULL, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2122| <<nvme_passthru_start>> effects |= nvme_known_admin_effects(opcode);
+ */
 static u32 nvme_known_admin_effects(u8 opcode)
 {
 	switch (opcode) {
@@ -1312,6 +2078,34 @@ static u32 nvme_known_admin_effects(u8 opcode)
 	return 0;
 }
 
+/*
+ * commit 84fef62d135b6e47b52f4e9280b5dbc5bb0050ba
+ * Author: Keith Busch <keith.busch@intel.com>
+ * Date:   Tue Nov 7 10:28:32 2017 -0700
+ *
+ * nvme: check admin passthru command effects
+ *
+ * The NVMe standard provides a command effects log page so the host may
+ * be aware of special requirements it may need to do for a particular
+ * command. For example, the command may need to run with IO quiesced to
+ * prevent timeouts or undefined behavior, or it may change the logical block
+ * formats that determine how the host needs to construct future commands.
+ *
+ * This patch saves the nvme command effects log page if the controller
+ * supports it, and performs appropriate actions before and after an admin
+ * passthrough command is completed. If the controller does not support the
+ * command effects log page, the driver will define the effects for known
+ * opcodes. The nvme format and santize are the only commands in this patch
+ * with known effects.
+ *
+ * Signed-off-by: Keith Busch <keith.busch@intel.com>
+ * Signed-off-by: Christoph Hellwig <hch@lst.de>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|1473| <<nvme_user_cmd>> effects = nvme_passthru_start(ctrl, ns, cmd.opcode);
+ *   - drivers/nvme/host/core.c|1520| <<nvme_user_cmd64>> effects = nvme_passthru_start(ctrl, ns, cmd.opcode);
+ */
 static u32 nvme_passthru_start(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 								u8 opcode)
 {
@@ -1346,6 +2140,10 @@ static u32 nvme_passthru_start(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 	return effects;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2129| <<nvme_passthru_end>> nvme_update_formats(ctrl);
+ */
 static void nvme_update_formats(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -1357,6 +2155,11 @@ static void nvme_update_formats(struct nvme_ctrl *ctrl)
 	up_read(&ctrl->namespaces_rwsem);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1451| <<nvme_user_cmd>> nvme_passthru_end(ctrl, effects);
+ *   - drivers/nvme/host/core.c|1498| <<nvme_user_cmd64>> nvme_passthru_end(ctrl, effects);
+ */
 static void nvme_passthru_end(struct nvme_ctrl *ctrl, u32 effects)
 {
 	/*
@@ -1379,6 +2182,13 @@ static void nvme_passthru_end(struct nvme_ctrl *ctrl, u32 effects)
 		nvme_queue_scan(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1894| <<nvme_handle_ctrl_ioctl>> ret = nvme_user_cmd(ctrl, NULL, argp);
+ *   - drivers/nvme/host/core.c|1933| <<nvme_ioctl>> ret = nvme_user_cmd(ns->ctrl, ns, argp);
+ *   - drivers/nvme/host/core.c|3375| <<nvme_dev_user_cmd>> ret = nvme_user_cmd(ctrl, ns, argp);
+ *   - drivers/nvme/host/core.c|3392| <<nvme_dev_ioctl>> return nvme_user_cmd(ctrl, NULL, argp);
+ */
 static int nvme_user_cmd(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 			struct nvme_passthru_cmd __user *ucmd)
 {
@@ -1412,6 +2222,11 @@ static int nvme_user_cmd(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 	if (cmd.timeout_ms)
 		timeout = msecs_to_jiffies(cmd.timeout_ms);
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/core.c|1473| <<nvme_user_cmd>> effects = nvme_passthru_start(ctrl, ns, cmd.opcode);
+	 *   - drivers/nvme/host/core.c|1520| <<nvme_user_cmd64>> effects = nvme_passthru_start(ctrl, ns, cmd.opcode);
+	 */
 	effects = nvme_passthru_start(ctrl, ns, cmd.opcode);
 	status = nvme_submit_user_cmd(ns ? ns->queue : ctrl->admin_q, &c,
 			(void __user *)(uintptr_t)cmd.addr, cmd.data_len,
@@ -1427,6 +2242,12 @@ static int nvme_user_cmd(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 	return status;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1897| <<nvme_handle_ctrl_ioctl>> ret = nvme_user_cmd64(ctrl, NULL, argp);
+ *   - drivers/nvme/host/core.c|1939| <<nvme_ioctl>> ret = nvme_user_cmd64(ns->ctrl, ns, argp);
+ *   - drivers/nvme/host/core.c|3394| <<nvme_dev_ioctl>> return nvme_user_cmd64(ctrl, NULL, argp);
+ */
 static int nvme_user_cmd64(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 			struct nvme_passthru_cmd64 __user *ucmd)
 {
@@ -1478,6 +2299,11 @@ static int nvme_user_cmd64(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
  * Issue ioctl requests on the first available path.  Note that unlike normal
  * block layer requests we will not retry failed request on another controller.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2129| <<nvme_ioctl>> ns = nvme_get_ns_from_disk(bdev->bd_disk, &head, &srcu_idx);
+ *   - drivers/nvme/host/core.c|2536| <<nvme_pr_command>> ns = nvme_get_ns_from_disk(bdev->bd_disk, &head, &srcu_idx);
+ */
 static struct nvme_ns *nvme_get_ns_from_disk(struct gendisk *disk,
 		struct nvme_ns_head **head, int *srcu_idx)
 {
@@ -1498,12 +2324,22 @@ static struct nvme_ns *nvme_get_ns_from_disk(struct gendisk *disk,
 	return disk->private_data;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2294| <<nvme_handle_ctrl_ioctl>> nvme_put_ns_from_disk(head, srcu_idx);
+ *   - drivers/nvme/host/core.c|2358| <<nvme_ioctl>> nvme_put_ns_from_disk(head, srcu_idx);
+ *   - drivers/nvme/host/core.c|2763| <<nvme_pr_command>> nvme_put_ns_from_disk(head, srcu_idx);
+ */
 static void nvme_put_ns_from_disk(struct nvme_ns_head *head, int idx)
 {
 	if (head)
 		srcu_read_unlock(&head->srcu, idx);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2334| <<nvme_ioctl>> if (is_ctrl_ioctl(cmd))
+ */
 static bool is_ctrl_ioctl(unsigned int cmd)
 {
 	if (cmd == NVME_IOCTL_ADMIN_CMD || cmd == NVME_IOCTL_ADMIN64_CMD)
@@ -1513,6 +2349,10 @@ static bool is_ctrl_ioctl(unsigned int cmd)
 	return false;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1925| <<nvme_ioctl>> return nvme_handle_ctrl_ioctl(ns, cmd, argp, head, srcu_idx);
+ */
 static int nvme_handle_ctrl_ioctl(struct nvme_ns *ns, unsigned int cmd,
 				  void __user *argp,
 				  struct nvme_ns_head *head,
@@ -1539,6 +2379,12 @@ static int nvme_handle_ctrl_ioctl(struct nvme_ns *ns, unsigned int cmd,
 	return ret;
 }
 
+/*
+ * struct block_device_operations nvme_fops.ioctl = nvme_ioctl()
+ * struct block_device_operations nvme_fops.compat_ioctl = nvme_ioctl()
+ * struct block_device_operations nvme_ns_head_ops.ioctl = nvme_ioctl()
+ * struct block_device_operations nvme_ns_head_ops.compat_ioctl = nvme_ioctl()
+ */
 static int nvme_ioctl(struct block_device *bdev, fmode_t mode,
 		unsigned int cmd, unsigned long arg)
 {
@@ -1584,6 +2430,9 @@ static int nvme_ioctl(struct block_device *bdev, fmode_t mode,
 	return ret;
 }
 
+/*
+ * struct block_device_operations nvme_fops.open = nvme_open()
+ */
 static int nvme_open(struct block_device *bdev, fmode_t mode)
 {
 	struct nvme_ns *ns = bdev->bd_disk->private_data;
@@ -1606,6 +2455,9 @@ static int nvme_open(struct block_device *bdev, fmode_t mode)
 	return -ENXIO;
 }
 
+/*
+ * struct block_device_operations nvme_fops.release = nvme_release()
+ */
 static void nvme_release(struct gendisk *disk, fmode_t mode)
 {
 	struct nvme_ns *ns = disk->private_data;
@@ -1614,6 +2466,10 @@ static void nvme_release(struct gendisk *disk, fmode_t mode)
 	nvme_put_ns(ns);
 }
 
+/*
+ * struct block_device_operations nvme_fops.getgeo = nvme_getgeo()
+ * struct block_device_operations nvme_ns_head_ops.getgeo = nvme_getgeo()
+ */
 static int nvme_getgeo(struct block_device *bdev, struct hd_geometry *geo)
 {
 	/* some standard values */
@@ -1624,6 +2480,10 @@ static int nvme_getgeo(struct block_device *bdev, struct hd_geometry *geo)
 }
 
 #ifdef CONFIG_BLK_DEV_INTEGRITY
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2173| <<nvme_update_disk_info>> nvme_init_integrity(disk, ns->ms, ns->pi_type);
+ */
 static void nvme_init_integrity(struct gendisk *disk, u16 ms, u8 pi_type)
 {
 	struct blk_integrity integrity;
@@ -1655,12 +2515,20 @@ static void nvme_init_integrity(struct gendisk *disk, u16 ms, u8 pi_type)
 }
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2654| <<__nvme_revalidate_disk>> nvme_set_chunk_size(ns);
+ */
 static void nvme_set_chunk_size(struct nvme_ns *ns)
 {
 	u32 chunk_size = nvme_lba_to_sect(ns, ns->noiob);
 	blk_queue_chunk_sectors(ns->queue, rounddown_pow_of_two(chunk_size));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2617| <<nvme_update_disk_info>> nvme_config_discard(disk, ns);
+ */
 static void nvme_config_discard(struct gendisk *disk, struct nvme_ns *ns)
 {
 	struct nvme_ctrl *ctrl = ns->ctrl;
@@ -1692,6 +2560,10 @@ static void nvme_config_discard(struct gendisk *disk, struct nvme_ns *ns)
 		blk_queue_max_write_zeroes_sectors(queue, UINT_MAX);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2618| <<nvme_update_disk_info>> nvme_config_write_zeroes(disk, ns);
+ */
 static void nvme_config_write_zeroes(struct gendisk *disk, struct nvme_ns *ns)
 {
 	u64 max_blocks;
@@ -1718,6 +2590,14 @@ static void nvme_config_write_zeroes(struct gendisk *disk, struct nvme_ns *ns)
 					   nvme_lba_to_sect(ns, max_blocks));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2300| <<nvme_revalidate_disk>> ret = nvme_report_ns_ids(ctrl, ns->head->ns_id, id, &ids);
+ *   - drivers/nvme/host/core.c|3905| <<nvme_alloc_ns_head>> ret = nvme_report_ns_ids(ctrl, nsid, id, &head->ids);
+ *   - drivers/nvme/host/core.c|4002| <<nvme_init_ns_head>> ret = nvme_report_ns_ids(ctrl, nsid, id, &ids);
+ *
+ * 核心思想是填充最后一个参数
+ */
 static int nvme_report_ns_ids(struct nvme_ctrl *ctrl, unsigned int nsid,
 		struct nvme_id_ns *id, struct nvme_ns_ids *ids)
 {
@@ -1757,6 +2637,11 @@ static bool nvme_ns_ids_equal(struct nvme_ns_ids *a, struct nvme_ns_ids *b)
 		memcmp(&a->eui64, &b->eui64, sizeof(a->eui64)) == 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2248| <<__nvme_revalidate_disk>> nvme_update_disk_info(disk, ns, id);
+ *   - drivers/nvme/host/core.c|2251| <<__nvme_revalidate_disk>> nvme_update_disk_info(ns->head->disk, ns, id);
+ */
 static void nvme_update_disk_info(struct gendisk *disk,
 		struct nvme_ns *ns, struct nvme_id_ns *id)
 {
@@ -1823,6 +2708,11 @@ static void nvme_update_disk_info(struct gendisk *disk,
 	blk_mq_unfreeze_queue(disk->queue);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2356| <<nvme_revalidate_disk>> __nvme_revalidate_disk(disk, id);
+ *   - drivers/nvme/host/core.c|4255| <<nvme_alloc_ns>> __nvme_revalidate_disk(disk, id);
+ */
 static void __nvme_revalidate_disk(struct gendisk *disk, struct nvme_id_ns *id)
 {
 	struct nvme_ns *ns = disk->private_data;
@@ -1855,6 +2745,9 @@ static void __nvme_revalidate_disk(struct gendisk *disk, struct nvme_id_ns *id)
 #endif
 }
 
+/*
+ * struct block_device_operations nvme_fops.revalidate = nvme_revalidate_disk()
+ */
 static int nvme_revalidate_disk(struct gendisk *disk)
 {
 	struct nvme_ns *ns = disk->private_data;
@@ -1863,6 +2756,12 @@ static int nvme_revalidate_disk(struct gendisk *disk)
 	struct nvme_ns_ids ids;
 	int ret = 0;
 
+	/*
+	 * 在以下使用NVME_NS_DEAD:
+	 *   - drivers/nvme/host/core.c|181| <<nvme_set_queue_dying>> if (!ns->disk || test_and_set_bit(NVME_NS_DEAD, &ns->flags))
+	 *   - drivers/nvme/host/core.c|2504| <<nvme_revalidate_disk>> if (test_bit(NVME_NS_DEAD, &ns->flags)) {
+	 *   - drivers/nvme/host/core.c|4679| <<nvme_remove_invalid_namespaces>> if (ns->head->ns_id > nsid || test_bit(NVME_NS_DEAD, &ns->flags))
+	 */
 	if (test_bit(NVME_NS_DEAD, &ns->flags)) {
 		set_capacity(disk, 0);
 		return -ENODEV;
@@ -1878,6 +2777,9 @@ static int nvme_revalidate_disk(struct gendisk *disk)
 	}
 
 	__nvme_revalidate_disk(disk, id);
+	/*
+	 * 核心思想是填充最后一个参数
+	 */
 	ret = nvme_report_ns_ids(ctrl, ns->head->ns_id, id, &ids);
 	if (ret)
 		goto free_id;
@@ -1902,6 +2804,12 @@ static int nvme_revalidate_disk(struct gendisk *disk)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2795| <<nvme_pr_reserve>> cdw10 = nvme_pr_type(type) << 8;
+ *   - drivers/nvme/host/core.c|2806| <<nvme_pr_preempt>> u32 cdw10 = nvme_pr_type(type) << 8 | (abort ? 2 : 1);
+ *   - drivers/nvme/host/core.c|2824| <<nvme_pr_release>> u32 cdw10 = nvme_pr_type(type) << 8 | (key ? 1 << 3 : 0);
+ */
 static char nvme_pr_type(enum pr_type type)
 {
 	switch (type) {
@@ -1922,6 +2830,14 @@ static char nvme_pr_type(enum pr_type type)
 	}
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2781| <<nvme_pr_register>> return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_register);
+ *   - drivers/nvme/host/core.c|2797| <<nvme_pr_reserve>> return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_acquire);
+ *   - drivers/nvme/host/core.c|2807| <<nvme_pr_preempt>> return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_acquire);
+ *   - drivers/nvme/host/core.c|2816| <<nvme_pr_clear>> return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_register);
+ *   - drivers/nvme/host/core.c|2825| <<nvme_pr_release>> return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_release);
+ */
 static int nvme_pr_command(struct block_device *bdev, u32 cdw10,
 				u64 key, u64 sa_key, u8 op)
 {
@@ -1948,6 +2864,9 @@ static int nvme_pr_command(struct block_device *bdev, u32 cdw10,
 	return ret;
 }
 
+/*
+ * struct pr_ops nvme_pr_ops.pr_register = nvme_pr_register()
+ */
 static int nvme_pr_register(struct block_device *bdev, u64 old,
 		u64 new, unsigned flags)
 {
@@ -1962,6 +2881,9 @@ static int nvme_pr_register(struct block_device *bdev, u64 old,
 	return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_register);
 }
 
+/*
+ * struct pr_ops nvme_pr_ops.pr_reserve = nvme_pr_reserve()
+ */
 static int nvme_pr_reserve(struct block_device *bdev, u64 key,
 		enum pr_type type, unsigned flags)
 {
@@ -1975,6 +2897,9 @@ static int nvme_pr_reserve(struct block_device *bdev, u64 key,
 	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_acquire);
 }
 
+/*
+ * struct pr_ops nvme_pr_ops.pr_preempt = nvme_pr_preempt()
+ */
 static int nvme_pr_preempt(struct block_device *bdev, u64 old, u64 new,
 		enum pr_type type, bool abort)
 {
@@ -1982,18 +2907,46 @@ static int nvme_pr_preempt(struct block_device *bdev, u64 old, u64 new,
 	return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_acquire);
 }
 
+/*
+ * struct pr_ops nvme_pr_ops.nvme_pr_clear()
+ */
 static int nvme_pr_clear(struct block_device *bdev, u64 key)
 {
 	u32 cdw10 = 1 | (key ? 1 << 3 : 0);
 	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_register);
 }
 
+/*
+ * struct pr_ops nvme_pr_ops.pr_release = nvme_pr_release()
+ */
 static int nvme_pr_release(struct block_device *bdev, u64 key, enum pr_type type)
 {
 	u32 cdw10 = nvme_pr_type(type) << 8 | (key ? 1 << 3 : 0);
 	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_release);
 }
 
+/*
+ * commit bbd3e064362e5057cc4799ba2e4d68c7593e490b
+ * Author: Christoph Hellwig <hch@lst.de>
+ * Date:   Thu Oct 15 14:10:48 2015 +0200
+ *
+ * block: add an API for Persistent Reservations
+ *
+ * This commits adds a driver API and ioctls for controlling Persistent
+ * Reservations s/genericly/generically/ at the block layer.  Persistent
+ * Reservations are supported by SCSI and NVMe and allow controlling who gets
+ * access to a device in a shared storage setup.
+ *
+ * Note that we add a pr_ops structure to struct block_device_operations
+ * instead of adding the members directly to avoid bloating all instances
+ * of devices that will never support Persistent Reservations.
+ *
+ * Signed-off-by: Christoph Hellwig <hch@lst.de>
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ *
+ * struct block_device_operations nvme_fops.pr_ops = nvme_pr_ops
+ * struct block_device_operations nvme_ns_head_ops.pr_ops = nvme_pr_ops
+ */
 static const struct pr_ops nvme_pr_ops = {
 	.pr_register	= nvme_pr_register,
 	.pr_reserve	= nvme_pr_reserve,
@@ -2003,6 +2956,10 @@ static const struct pr_ops nvme_pr_ops = {
 };
 
 #ifdef CONFIG_BLK_SED_OPAL
+/*
+ * 在以下使用nvme_sec_submit():
+ *   - drivers/nvme/host/pci.c|4178| <<nvme_reset_work>> init_opal_dev(&dev->ctrl, &nvme_sec_submit);
+ */
 int nvme_sec_submit(void *data, u16 spsp, u8 secp, void *buffer, size_t len,
 		bool send)
 {
@@ -2024,6 +2981,12 @@ int nvme_sec_submit(void *data, u16 spsp, u8 secp, void *buffer, size_t len,
 EXPORT_SYMBOL_GPL(nvme_sec_submit);
 #endif /* CONFIG_BLK_SED_OPAL */
 
+/*
+ * 在以下使用nvme_fops:
+ *   - drivers/nvme/host/core.c|4057| <<dev_to_ns_head>> if (disk->fops == &nvme_fops)
+ *   - drivers/nvme/host/core.c|4166| <<nvme_ns_id_attrs_are_visible>> if (dev_to_disk(dev)->fops != &nvme_fops)
+ *   - drivers/nvme/host/core.c|4700| <<nvme_alloc_ns>> disk->fops = &nvme_fops;
+ */
 static const struct block_device_operations nvme_fops = {
 	.owner		= THIS_MODULE,
 	.ioctl		= nvme_ioctl,
@@ -2036,6 +2999,7 @@ static const struct block_device_operations nvme_fops = {
 };
 
 #ifdef CONFIG_NVME_MULTIPATH
+/* struct block_device_operations nvme_ns_head_ops.open = nvme_ns_head_open() */
 static int nvme_ns_head_open(struct block_device *bdev, fmode_t mode)
 {
 	struct nvme_ns_head *head = bdev->bd_disk->private_data;
@@ -2045,11 +3009,19 @@ static int nvme_ns_head_open(struct block_device *bdev, fmode_t mode)
 	return 0;
 }
 
+/*
+ * struct block_device_operations nvme_ns_head_ops.release = nvme_ns_head_release()
+ */
 static void nvme_ns_head_release(struct gendisk *disk, fmode_t mode)
 {
 	nvme_put_ns_head(disk->private_data);
 }
 
+/*
+ * 在以下使用nvme_ns_head_ops:
+ *   - drivers/nvme/host/core.c|1877| <<nvme_get_ns_from_disk>> if (disk->fops == &nvme_ns_head_ops) {
+ *   - drivers/nvme/host/multipath.c|425| <<nvme_mpath_alloc_disk>> head->disk->fops = &nvme_ns_head_ops;
+ */
 const struct block_device_operations nvme_ns_head_ops = {
 	.owner		= THIS_MODULE,
 	.open		= nvme_ns_head_open,
@@ -2061,6 +3033,11 @@ const struct block_device_operations nvme_ns_head_ops = {
 };
 #endif /* CONFIG_NVME_MULTIPATH */
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2197| <<nvme_disable_ctrl>> return nvme_wait_ready(ctrl, ctrl->cap, false);
+ *   - drivers/nvme/host/core.c|2236| <<nvme_enable_ctrl>> return nvme_wait_ready(ctrl, ctrl->cap, true);
+ */
 static int nvme_wait_ready(struct nvme_ctrl *ctrl, u64 cap, bool enabled)
 {
 	unsigned long timeout =
@@ -2094,6 +3071,13 @@ static int nvme_wait_ready(struct nvme_ctrl *ctrl, u64 cap, bool enabled)
  * bits', but doing so may cause the device to complete commands to the
  * admin queue ... and we don't know what memory that might be pointing at!
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1572| <<nvme_disable_admin_queue>> nvme_disable_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|1875| <<nvme_pci_configure_admin_queue>> result = nvme_disable_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1894| <<nvme_rdma_shutdown_ctrl>> nvme_disable_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1933| <<nvme_tcp_teardown_ctrl>> nvme_disable_ctrl(ctrl);
+ */
 int nvme_disable_ctrl(struct nvme_ctrl *ctrl)
 {
 	int ret;
@@ -2112,6 +3096,14 @@ int nvme_disable_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_disable_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2672| <<nvme_fc_create_association>> ret = nvme_enable_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|1891| <<nvme_pci_configure_admin_queue>> result = nvme_enable_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|826| <<nvme_rdma_configure_admin_queue>> error = nvme_enable_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1746| <<nvme_tcp_configure_admin_queue>> error = nvme_enable_ctrl(ctrl);
+ *   - drivers/nvme/target/loop.c|380| <<nvme_loop_configure_admin_queue>> error = nvme_enable_ctrl(&ctrl->ctrl);
+ */
 int nvme_enable_ctrl(struct nvme_ctrl *ctrl)
 {
 	/*
@@ -2138,6 +3130,15 @@ int nvme_enable_ctrl(struct nvme_ctrl *ctrl)
 
 	ctrl->page_size = 1 << page_shift;
 
+	/*
+	 * Submission and Completion Queue Entry Sizes for the NVM command set.
+	 * (In bytes and specified as a power of two (2^n)).
+	 *
+	 * #define NVME_ADM_SQES       6
+	 * #define NVME_NVM_IOSQES         6
+	 * #define NVME_NVM_IOCQES         4
+	 */
+
 	ctrl->ctrl_config = NVME_CC_CSS_NVM;
 	ctrl->ctrl_config |= (page_shift - 12) << NVME_CC_MPS_SHIFT;
 	ctrl->ctrl_config |= NVME_CC_AMS_RR | NVME_CC_SHN_NONE;
@@ -2151,6 +3152,13 @@ int nvme_enable_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_enable_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1631| <<nvme_disable_admin_queue>> nvme_shutdown_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1892| <<nvme_rdma_shutdown_ctrl>> nvme_shutdown_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1931| <<nvme_tcp_teardown_ctrl>> nvme_shutdown_ctrl(ctrl);
+ *   - drivers/nvme/target/loop.c|418| <<nvme_loop_shutdown_ctrl>> nvme_shutdown_ctrl(&ctrl->ctrl);
+ */
 int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl)
 {
 	unsigned long timeout = jiffies + (ctrl->shutdown_timeout * HZ);
@@ -2182,6 +3190,11 @@ int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_shutdown_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3048| <<nvme_init_identify>> nvme_set_queue_limits(ctrl, ctrl->admin_q);
+ *   - drivers/nvme/host/core.c|3780| <<nvme_alloc_ns>> nvme_set_queue_limits(ctrl, ns->queue);
+ */
 static void nvme_set_queue_limits(struct nvme_ctrl *ctrl,
 		struct request_queue *q)
 {
@@ -2204,6 +3217,10 @@ static void nvme_set_queue_limits(struct nvme_ctrl *ctrl,
 	blk_queue_write_cache(q, vwc, vwc);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3345| <<nvme_init_identify>> ret = nvme_configure_timestamp(ctrl);
+ */
 static int nvme_configure_timestamp(struct nvme_ctrl *ctrl)
 {
 	__le64 ts;
@@ -2221,6 +3238,24 @@ static int nvme_configure_timestamp(struct nvme_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3960| <<nvme_init_identify>> ret = nvme_configure_acre(ctrl);
+ *
+ * Advanced Command Retry Enable (ACRE): If set to 1h, then the Command Interrupted status
+ * code is enabled (refer to Figure 126) and command retry delays are enabled. The controller may
+ * use the Command Interrupted status code and may indicate a command retry delay by setting the
+ * Command Retry Delay (CRD) field to a non-zero value in the Status field of a Completion Queue
+ * Entry, refer to Figure 124. A host that sets this field to 1h indicates host support for the command
+ * retry behaviors that are specified for both the Command Interrupted status code and non-zero
+ * values in the CRD field.
+ *
+ * If cleared to 0h, then both the Command Interrupted status code and command retry delays are
+ * disabled. The controller shall not use the Command Interrupted status code, and shall clear the
+ * CRD field to 0h in all CQEs.
+ *
+ * All values other than 0h and 1h are reserved.
+ */
 static int nvme_configure_acre(struct nvme_ctrl *ctrl)
 {
 	struct nvme_feat_host_behavior *host;
@@ -2241,6 +3276,11 @@ static int nvme_configure_acre(struct nvme_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3296| <<nvme_set_latency_tolerance>> nvme_configure_apst(ctrl);
+ *   - drivers/nvme/host/core.c|3948| <<nvme_init_identify>> ret = nvme_configure_apst(ctrl);
+ */
 static int nvme_configure_apst(struct nvme_ctrl *ctrl)
 {
 	/*
@@ -2364,6 +3404,13 @@ static int nvme_configure_apst(struct nvme_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4708| <<nvme_init_ctrl>> ctrl->device->power.set_latency_tolerance = nvme_set_latency_tolerance;
+ *
+ * struct nvme_ctrl:
+ *  - struct device *device;
+ */
 static void nvme_set_latency_tolerance(struct device *dev, s32 val)
 {
 	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
@@ -2397,6 +3444,12 @@ struct nvme_core_quirk_entry {
 	unsigned long quirks;
 };
 
+/*
+ * 在以下使用core_quirks:
+ *   - drivers/nvme/host/core.c|3834| <<nvme_init_identify>> for (i = 0; i < ARRAY_SIZE(core_quirks); i++) {
+ *   - drivers/nvme/host/core.c|3835| <<nvme_init_identify>> if (quirk_matches(id, &core_quirks[i]))
+ *   - drivers/nvme/host/core.c|3836| <<nvme_init_identify>> ctrl->quirks |= core_quirks[i].quirks;
+ */
 static const struct nvme_core_quirk_entry core_quirks[] = {
 	{
 		/*
@@ -2420,6 +3473,11 @@ static const struct nvme_core_quirk_entry core_quirks[] = {
 };
 
 /* match is null-terminated but idstr is space-padded. */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2915| <<quirk_matches>> string_matches(id->mn, q->mn, sizeof(id->mn)) &&
+ *   - drivers/nvme/host/core.c|2916| <<quirk_matches>> string_matches(id->fr, q->fr, sizeof(id->fr));
+ */
 static bool string_matches(const char *idstr, const char *match, size_t len)
 {
 	size_t matchlen;
@@ -2440,6 +3498,10 @@ static bool string_matches(const char *idstr, const char *match, size_t len)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3273| <<nvme_init_identify>> if (quirk_matches(id, &core_quirks[i]))
+ */
 static bool quirk_matches(const struct nvme_id_ctrl *id,
 			  const struct nvme_core_quirk_entry *q)
 {
@@ -2448,6 +3510,10 @@ static bool quirk_matches(const struct nvme_id_ctrl *id,
 		string_matches(id->fr, q->fr, sizeof(id->fr));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3103| <<nvme_init_subsystem>> nvme_init_subnqn(subsys, ctrl, id);
+ */
 static void nvme_init_subnqn(struct nvme_subsystem *subsys, struct nvme_ctrl *ctrl,
 		struct nvme_id_ctrl *id)
 {
@@ -2466,6 +3532,15 @@ static void nvme_init_subnqn(struct nvme_subsystem *subsys, struct nvme_ctrl *ct
 	}
 
 	/* Generate a "fake" NQN per Figure 254 in NVMe 1.3 + ECN 001 */
+	/*
+	 * 例子: nqn.2014.08.org.nvmexpress:80861af4deadbeaf1           QEMU NVMe Ctrl
+	 *
+	 * # cat /sys/block/nvme0n1/device/subsysnqn 
+	 * nqn.2014.08.org.nvmexpress:80861af4deadbeaf1           QEMU NVMe Ctrl
+	 *
+	 * # cat /sys/block/nvme0n1/device/subsysnqn 
+	 * nqn.2017-03.jp.co.toshiba:KXG60ZNV256G NVMe TOSHIBA 256GB:999A61GOK3JL
+	 */
 	off = snprintf(subsys->subnqn, NVMF_NQN_SIZE,
 			"nqn.2014.08.org.nvmexpress:%04x%04x",
 			le16_to_cpu(id->vid), le16_to_cpu(id->ssvid));
@@ -2476,6 +3551,10 @@ static void nvme_init_subnqn(struct nvme_subsystem *subsys, struct nvme_ctrl *ct
 	memset(subsys->subnqn + off, 0, sizeof(subsys->subnqn) - off);
 }
 
+/*
+ * 在以下使用nvme_release_subsystem():
+ *   - drivers/nvme/host/core.c|3333| <<nvme_init_subsystem>> subsys->dev.release = nvme_release_subsystem;
+ */
 static void nvme_release_subsystem(struct device *dev)
 {
 	struct nvme_subsystem *subsys =
@@ -2486,12 +3565,17 @@ static void nvme_release_subsystem(struct device *dev)
 	kfree(subsys);
 }
 
+/*
+ * 在以下使用nvme_destroy_subsystem():
+ *   - drivers/nvme/host/core.c|3197| <<nvme_put_subsystem>> kref_put(&subsys->ref, nvme_destroy_subsystem);
+ */
 static void nvme_destroy_subsystem(struct kref *ref)
 {
 	struct nvme_subsystem *subsys =
 			container_of(ref, struct nvme_subsystem, ref);
 
 	mutex_lock(&nvme_subsystems_lock);
+	/* 从nvme_subsystems删除 */
 	list_del(&subsys->entry);
 	mutex_unlock(&nvme_subsystems_lock);
 
@@ -2500,11 +3584,23 @@ static void nvme_destroy_subsystem(struct kref *ref)
 	put_device(&subsys->dev);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|655| <<nvme_free_ns_head>> nvme_put_subsystem(head->subsys);
+ *   - drivers/nvme/host/core.c|3420| <<nvme_init_subsystem>> nvme_put_subsystem(subsys);
+ *   - drivers/nvme/host/core.c|5136| <<nvme_free_ctrl>> nvme_put_subsystem(subsys);
+ */
 static void nvme_put_subsystem(struct nvme_subsystem *subsys)
 {
 	kref_put(&subsys->ref, nvme_destroy_subsystem);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3339| <<nvme_init_subsystem>> found = __nvme_find_get_subsystem(subsys->subnqn);
+ *
+ * 遍历nvme_subsystems上的nvme_subsystem是否有nvme_subsystem->subsysnqn和参数的相同
+ */
 static struct nvme_subsystem *__nvme_find_get_subsystem(const char *subsysnqn)
 {
 	struct nvme_subsystem *subsys;
@@ -2522,6 +3618,11 @@ static struct nvme_subsystem *__nvme_find_get_subsystem(const char *subsysnqn)
 	if (!strcmp(subsysnqn, NVME_DISC_SUBSYS_NAME))
 		return NULL;
 
+	/*
+	 * 在以下使用nvme_subsystems:
+	 *   - drivers/nvme/host/core.c|2655| <<__nvme_find_get_subsystem>> list_for_each_entry(subsys, &nvme_subsystems, entry) {
+	 *   - drivers/nvme/host/core.c|2797| <<nvme_init_subsystem>> list_add_tail(&subsys->entry, &nvme_subsystems);
+	 */
 	list_for_each_entry(subsys, &nvme_subsystems, entry) {
 		if (strcmp(subsys->subnqn, subsysnqn))
 			continue;
@@ -2583,6 +3684,13 @@ static const struct attribute_group *nvme_subsys_attrs_groups[] = {
 	NULL,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3126| <<nvme_init_subsystem>> if (!nvme_validate_cntlid(subsys, ctrl, id)) {
+ *
+ * Controller ID (CNTLID): Contains the NVM subsystem unique controller identifier
+ * associated with the controller.
+ */
 static bool nvme_validate_cntlid(struct nvme_subsystem *subsys,
 		struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 {
@@ -2614,6 +3722,37 @@ static bool nvme_validate_cntlid(struct nvme_subsystem *subsys,
 	return true;
 }
 
+/*
+ * # nvme list-subsys
+ * nvme-subsys0 - NQN=testnqn2
+ * \
+ *  +- nvme0 loop 
+ * nvme-subsys1 - NQN=testnqn
+ * \
+ *  +- nvme1 loop
+ *
+ *
+ * # ls /sys/block/nvme1n2/device/subsystem/
+ * nvme-subsys0  nvme-subsys1
+ *
+ *
+ * # ls /sys/block/nvme1n2/device/subsystem/nvme-subsys1/
+ * firmware_rev  iopolicy  model  nvme1  nvme1n1  nvme1n2  power  serial  subsysnqn  subsystem  uevent
+ * # ls /sys/block/nvme1n2/device/subsystem/nvme-subsys0/
+ * firmware_rev  iopolicy  model  nvme0  nvme0n1  power  serial  subsysnqn  subsystem  uevent
+ *
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|3015| <<nvme_init_identify>> ret = nvme_init_subsystem(ctrl, id);
+ *
+ * An NVMe subsystem includes one or more controllers. 
+ *
+ * 每个nvme pci device (func) 是一个controller.
+ * 可能很多个controller输入同一个subsystem. 但是pci的入口都是从probe controller开始的.
+ * subsystem的数据结构的分配是初始化每一个controller的时候on demand的.
+ * 如果两个controller属于同一个subsystem, 那初始化第二个controller的时候就不会再分配subsystem的结构了.
+ * 每一个nvme0或者nvme1都是一个controller.
+ */
 static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 {
 	struct nvme_subsystem *subsys, *found;
@@ -2626,7 +3765,9 @@ static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	subsys->instance = -1;
 	mutex_init(&subsys->lock);
 	kref_init(&subsys->ref);
+	/* struct list_head ctrls; */
 	INIT_LIST_HEAD(&subsys->ctrls);
+	/* 管理所有的nvme_ns_head->entry */
 	INIT_LIST_HEAD(&subsys->nsheads);
 	nvme_init_subnqn(subsys, ctrl, id);
 	memcpy(subsys->serial, id->sn, sizeof(subsys->serial));
@@ -2639,6 +3780,14 @@ static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	subsys->iopolicy = NVME_IOPOLICY_NUMA;
 #endif
 
+	/*
+	 * 在以下使用nvme_subsys_class:
+	 *   - drivers/nvme/host/core.c|3332| <<nvme_init_subsystem>> subsys->dev.class = nvme_subsys_class;
+	 *   - drivers/nvme/host/core.c|5418| <<nvme_core_init>> nvme_subsys_class = class_create(THIS_MODULE, "nvme-subsystem");
+	 *   - drivers/nvme/host/core.c|5419| <<nvme_core_init>> if (IS_ERR(nvme_subsys_class)) {
+	 *   - drivers/nvme/host/core.c|5420| <<nvme_core_init>> result = PTR_ERR(nvme_subsys_class);
+	 *   - drivers/nvme/host/core.c|5441| <<nvme_core_exit>> class_destroy(nvme_subsys_class);
+	 */
 	subsys->dev.class = nvme_subsys_class;
 	subsys->dev.release = nvme_release_subsystem;
 	subsys->dev.groups = nvme_subsys_attrs_groups;
@@ -2646,6 +3795,9 @@ static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	device_initialize(&subsys->dev);
 
 	mutex_lock(&nvme_subsystems_lock);
+	/*
+	 * 遍历nvme_subsystems上的nvme_subsystem是否有nvme_subsystem->subsysnqn和参数的相同
+	 */
 	found = __nvme_find_get_subsystem(subsys->subnqn);
 	if (found) {
 		put_device(&subsys->dev);
@@ -2664,9 +3816,24 @@ static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 			goto out_unlock;
 		}
 		ida_init(&subsys->ns_ida);
+		/*
+		 * 在以下使用nvme_subsystems:
+		 *   - drivers/nvme/host/core.c|2655| <<__nvme_find_get_subsystem>> list_for_each_entry(subsys, &nvme_subsystems, entry) {
+		 *   - drivers/nvme/host/core.c|2797| <<nvme_init_subsystem>> list_add_tail(&subsys->entry, &nvme_subsystems);
+		 *
+		 * 管理所有的nvme_subsystem->entry
+		 */
 		list_add_tail(&subsys->entry, &nvme_subsystems);
 	}
 
+	/*
+	 * sysfs_create_link - create symlink between two objects.
+	 *      @kobj:  object whose directory we're creating the link in.
+	 *      @target:        object we're pointing to.
+	 *      @name:          name of the symlink.
+	 *
+	 * 上面有dev_set_name(&subsys->dev, "nvme-subsys%d", ctrl->instance);
+	 */
 	ret = sysfs_create_link(&subsys->dev.kobj, &ctrl->device->kobj,
 				dev_name(ctrl->device));
 	if (ret) {
@@ -2689,6 +3856,15 @@ static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2907| <<nvme_get_effects_log>> ret = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_CMD_EFFECTS, 0,
+ *   - drivers/nvme/host/core.c|3936| <<nvme_clear_changed_ns_log>> error = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_CHANGED_NS, 0, log,
+ *   - drivers/nvme/host/core.c|4122| <<nvme_get_fw_slot_info>> if (nvme_get_log(ctrl, NVME_NSID_ALL, 0, NVME_LOG_FW_SLOT, log,
+ *   - drivers/nvme/host/hwmon.c|67| <<nvme_hwmon_get_smart_log>> ret = nvme_get_log(data->ctrl, NVME_NSID_ALL, NVME_LOG_SMART, 0,
+ *   - drivers/nvme/host/lightnvm.c|595| <<nvme_nvm_get_chk_meta>> ret = nvme_get_log(ctrl, ns->head->ns_id,
+ *   - drivers/nvme/host/multipath.c|551| <<nvme_read_ana_log>> error = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_ANA, 0,
+ */
 int nvme_get_log(struct nvme_ctrl *ctrl, u32 nsid, u8 log_page, u8 lsp,
 		void *log, size_t size, u64 offset)
 {
@@ -2707,6 +3883,15 @@ int nvme_get_log(struct nvme_ctrl *ctrl, u32 nsid, u8 log_page, u8 lsp,
 	return nvme_submit_sync_cmd(ctrl->admin_q, &c, log, size);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3270| <<nvme_init_identify>> ret = nvme_get_effects_log(ctrl);
+ *
+ * This log page is used to describe the commands that the controller supports and the effects of those
+ * commands on the state of the NVM subsystem. The log page is 4,096 bytes in size. There is one
+ * Commands Supported and Effects data structure per Admin command and one Commands Supported and
+ * Effects data structure per I/O command (based on the I/O Command Set selected in CC.CSS).
+ */
 static int nvme_get_effects_log(struct nvme_ctrl *ctrl)
 {
 	int ret;
@@ -2717,6 +3902,20 @@ static int nvme_get_effects_log(struct nvme_ctrl *ctrl)
 	if (!ctrl->effects)
 		return 0;
 
+	/*
+	 * 在以下使用effects:
+	 *   - drivers/nvme/host/core.c|2063| <<nvme_passthru_start>> if (ctrl->effects)
+	 *   - drivers/nvme/host/core.c|2064| <<nvme_passthru_start>> effects = le32_to_cpu(ctrl->effects->iocs[opcode]);
+	 *   - drivers/nvme/host/core.c|2072| <<nvme_passthru_start>> if (ctrl->effects)
+	 *   - drivers/nvme/host/core.c|2073| <<nvme_passthru_start>> effects = le32_to_cpu(ctrl->effects->acs[opcode]);
+	 *   - drivers/nvme/host/core.c|3750| <<nvme_get_effects_log>> if (!ctrl->effects)
+	 *   - drivers/nvme/host/core.c|3751| <<nvme_get_effects_log>> ctrl->effects = kzalloc(sizeof(*ctrl->effects), GFP_KERNEL);
+	 *   - drivers/nvme/host/core.c|3753| <<nvme_get_effects_log>> if (!ctrl->effects)
+	 *   - drivers/nvme/host/core.c|3757| <<nvme_get_effects_log>> ctrl->effects, sizeof(*ctrl->effects), 0);
+	 *   - drivers/nvme/host/core.c|3759| <<nvme_get_effects_log>> kfree(ctrl->effects);
+	 *   - drivers/nvme/host/core.c|3760| <<nvme_get_effects_log>> ctrl->effects = NULL;
+	 *   - drivers/nvme/host/core.c|5616| <<nvme_free_ctrl>> kfree(ctrl->effects);
+	 */
 	ret = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_CMD_EFFECTS, 0,
 			ctrl->effects, sizeof(*ctrl->effects), 0);
 	if (ret) {
@@ -2731,6 +3930,19 @@ static int nvme_get_effects_log(struct nvme_ctrl *ctrl)
  * register in our nvme_ctrl structure.  This should be called as soon as
  * the admin queue is fully up and running.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1408| <<nvme_passthru_end>> nvme_init_identify(ctrl);
+ *   - drivers/nvme/host/fc.c|2681| <<nvme_fc_create_association>> ret = nvme_init_identify(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|2628| <<nvme_reset_work>> result = nvme_init_identify(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|835| <<nvme_rdma_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1752| <<nvme_tcp_configure_admin_queue>> error = nvme_init_identify(ctrl);
+ *   - drivers/nvme/target/loop.c|389| <<nvme_loop_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+ *
+ * Initialize the cached copies of the Identify data and various controller
+ * register in our nvme_ctrl structure.  This should be called as soon as
+ * the admin queue is fully up and running.
+ */
 int nvme_init_identify(struct nvme_ctrl *ctrl)
 {
 	struct nvme_id_ctrl *id;
@@ -2767,6 +3979,13 @@ int nvme_init_identify(struct nvme_ctrl *ctrl)
 	if (!ctrl->identified) {
 		int i;
 
+		/*
+		 * 每个nvme pci device (func) 是一个controller.
+		 * 可能很多个controller输入同一个subsystem. 但是pci的入口都是从probe controller开始的.
+		 * subsystem的数据结构的分配是初始化每一个controller的时候on demand的.
+		 * 如果两个controller属于同一个subsystem, 那初始化第二个controller的时候就不会再分配subsystem的结构了.
+		 * 每一个nvme0或者nvme1都是一个controller.
+		 */
 		ret = nvme_init_subsystem(ctrl, id);
 		if (ret)
 			goto out_free;
@@ -2813,6 +4032,11 @@ int nvme_init_identify(struct nvme_ctrl *ctrl)
 	nvme_set_queue_limits(ctrl, ctrl->admin_q);
 	ctrl->sgls = le32_to_cpu(id->sgls);
 	ctrl->kas = le16_to_cpu(id->kas);
+	/*
+	 * 在以下使用nvme_ctrl->max_namespaces:
+	 *   - drivers/nvme/host/core.c|3614| <<nvme_init_identify>> ctrl->max_namespaces = le32_to_cpu(id->mnan);
+	 *   - drivers/nvme/host/multipath.c|770| <<nvme_mpath_init>> ctrl->ana_log_size += ctrl->max_namespaces * sizeof(__le32);
+	 */
 	ctrl->max_namespaces = le32_to_cpu(id->mnan);
 	ctrl->ctratt = le32_to_cpu(id->ctratt);
 
@@ -2904,6 +4128,10 @@ int nvme_init_identify(struct nvme_ctrl *ctrl)
 	if (ret < 0)
 		return ret;
 
+	/*
+	 * 只在以下修改:
+	 *   - drivers/nvme/host/core.c|3451| <<nvme_init_identify>> ctrl->identified = true;
+	 */
 	if (!ctrl->identified)
 		nvme_hwmon_init(ctrl);
 
@@ -2917,6 +4145,9 @@ int nvme_init_identify(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_init_identify);
 
+/*
+ * struct file_operations nvme_dev_fops.open = nvme_dev_open()
+ */
 static int nvme_dev_open(struct inode *inode, struct file *file)
 {
 	struct nvme_ctrl *ctrl =
@@ -2933,6 +4164,10 @@ static int nvme_dev_open(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * 处理nvme ioctl的NVME_IOCTL_IO_CMD:
+ *   - drivers/nvme/host/core.c|3492| <<nvme_dev_ioctl>> return nvme_dev_user_cmd(ctrl, argp);
+ */
 static int nvme_dev_user_cmd(struct nvme_ctrl *ctrl, void __user *argp)
 {
 	struct nvme_ns *ns;
@@ -2966,6 +4201,9 @@ static int nvme_dev_user_cmd(struct nvme_ctrl *ctrl, void __user *argp)
 	return ret;
 }
 
+/*
+ * struct file_operations nvme_dev_fops.unlocked_ioctl = nvme_dev_ioctl()
+ */
 static long nvme_dev_ioctl(struct file *file, unsigned int cmd,
 		unsigned long arg)
 {
@@ -2983,6 +4221,9 @@ static long nvme_dev_ioctl(struct file *file, unsigned int cmd,
 		dev_warn(ctrl->device, "resetting controller\n");
 		return nvme_reset_ctrl_sync(ctrl);
 	case NVME_IOCTL_SUBSYS_RESET:
+		/*
+		 * 只在这里被调用
+		 */
 		return nvme_reset_subsystem(ctrl);
 	case NVME_IOCTL_RESCAN:
 		nvme_queue_scan(ctrl);
@@ -2992,6 +4233,10 @@ static long nvme_dev_ioctl(struct file *file, unsigned int cmd,
 	}
 }
 
+/*
+ * 在以下使用nvme_dev_fops:
+ *   - drivers/nvme/host/core.c|5657| <<nvme_init_ctrl>> cdev_init(&ctrl->cdev, &nvme_dev_fops);
+ */
 static const struct file_operations nvme_dev_fops = {
 	.owner		= THIS_MODULE,
 	.open		= nvme_dev_open,
@@ -2999,6 +4244,13 @@ static const struct file_operations nvme_dev_fops = {
 	.compat_ioctl	= compat_ptr_ioctl,
 };
 
+/*
+ * nvme_sysfs_reset()
+ *  -> nvme_reset_ctrl_sync()
+ *      -> nvme_reset_ctrl()
+ *          -> nvme_loop_reset_ctrl_work() or nvme_reset_work()
+ *      -> flush_work()
+ */
 static ssize_t nvme_sysfs_reset(struct device *dev,
 				struct device_attribute *attr, const char *buf,
 				size_t count)
@@ -3013,6 +4265,10 @@ static ssize_t nvme_sysfs_reset(struct device *dev,
 }
 static DEVICE_ATTR(reset_controller, S_IWUSR, NULL, nvme_sysfs_reset);
 
+/*
+ * 在以下使用nvme_sysfs_rescan():
+ *   - drivers/nvme/host/core.c|4042| <<global>> static DEVICE_ATTR(rescan_controller, S_IWUSR, NULL, nvme_sysfs_rescan);
+ */
 static ssize_t nvme_sysfs_rescan(struct device *dev,
 				struct device_attribute *attr, const char *buf,
 				size_t count)
@@ -3024,6 +4280,15 @@ static ssize_t nvme_sysfs_rescan(struct device *dev,
 }
 static DEVICE_ATTR(rescan_controller, S_IWUSR, NULL, nvme_sysfs_rescan);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4055| <<wwid_show>> struct nvme_ns_head *head = dev_to_ns_head(dev);
+ *   - drivers/nvme/host/core.c|4086| <<nguid_show>> return sprintf(buf, "%pU\n", dev_to_ns_head(dev)->ids.nguid);
+ *   - drivers/nvme/host/core.c|4093| <<uuid_show>> struct nvme_ns_ids *ids = &dev_to_ns_head(dev)->ids;
+ *   - drivers/nvme/host/core.c|4110| <<eui_show>> return sprintf(buf, "%8ph\n", dev_to_ns_head(dev)->ids.eui64);
+ *   - drivers/nvme/host/core.c|4117| <<nsid_show>> return sprintf(buf, "%d\n", dev_to_ns_head(dev)->ns_id);
+ *   - drivers/nvme/host/core.c|4138| <<nvme_ns_id_attrs_are_visible>> struct nvme_ns_ids *ids = &dev_to_ns_head(dev)->ids;
+ */
 static inline struct nvme_ns_head *dev_to_ns_head(struct device *dev)
 {
 	struct gendisk *disk = dev_to_disk(dev);
@@ -3270,6 +4535,9 @@ static struct attribute *nvme_dev_attrs[] = {
 	NULL
 };
 
+/*
+ * struct attribute_group nvme_dev_attrs_group.is_visible = nvme_dev_attrs_are_visible()
+ */
 static umode_t nvme_dev_attrs_are_visible(struct kobject *kobj,
 		struct attribute *a, int n)
 {
@@ -3289,11 +4557,21 @@ static struct attribute_group nvme_dev_attrs_group = {
 	.is_visible	= nvme_dev_attrs_are_visible,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4712| <<nvme_init_ctrl>> ctrl->device->groups = nvme_dev_attr_groups;
+ */
 static const struct attribute_group *nvme_dev_attr_groups[] = {
 	&nvme_dev_attrs_group,
 	NULL,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4066| <<nvme_init_ns_head>> head = __nvme_find_ns_head(ctrl->subsys, nsid);
+ *
+ * 遍历nvme_subsystem->nheads上的所有nvme_ns_head寻找nsid(nvme_ns_head->entry)
+ */
 static struct nvme_ns_head *__nvme_find_ns_head(struct nvme_subsystem *subsys,
 		unsigned nsid)
 {
@@ -3301,6 +4579,9 @@ static struct nvme_ns_head *__nvme_find_ns_head(struct nvme_subsystem *subsys,
 
 	lockdep_assert_held(&subsys->lock);
 
+	/*
+	 * nvme_subsystem->nsheads管理所有的nvme_ns_head->entry
+	 */
 	list_for_each_entry(h, &subsys->nsheads, entry) {
 		if (h->ns_id == nsid && kref_get_unless_zero(&h->ref))
 			return h;
@@ -3309,6 +4590,10 @@ static struct nvme_ns_head *__nvme_find_ns_head(struct nvme_subsystem *subsys,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3977| <<nvme_alloc_ns_head>> ret = __nvme_check_ids(ctrl->subsys, head);
+ */
 static int __nvme_check_ids(struct nvme_subsystem *subsys,
 		struct nvme_ns_head *new)
 {
@@ -3326,6 +4611,10 @@ static int __nvme_check_ids(struct nvme_subsystem *subsys,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3648| <<nvme_init_ns_head>> head = nvme_alloc_ns_head(ctrl, nsid, id);
+ */
 static struct nvme_ns_head *nvme_alloc_ns_head(struct nvme_ctrl *ctrl,
 		unsigned nsid, struct nvme_id_ns *id)
 {
@@ -3344,6 +4633,7 @@ static struct nvme_ns_head *nvme_alloc_ns_head(struct nvme_ctrl *ctrl,
 	if (ret < 0)
 		goto out_free_head;
 	head->instance = ret;
+	/* 管理所有的nvme_ns->siblings */
 	INIT_LIST_HEAD(&head->list);
 	ret = init_srcu_struct(&head->srcu);
 	if (ret)
@@ -3352,6 +4642,7 @@ static struct nvme_ns_head *nvme_alloc_ns_head(struct nvme_ctrl *ctrl,
 	head->ns_id = nsid;
 	kref_init(&head->ref);
 
+	/* 核心思想是填充最后一个参数 */
 	ret = nvme_report_ns_ids(ctrl, nsid, id, &head->ids);
 	if (ret)
 		goto out_cleanup_srcu;
@@ -3367,6 +4658,9 @@ static struct nvme_ns_head *nvme_alloc_ns_head(struct nvme_ctrl *ctrl,
 	if (ret)
 		goto out_cleanup_srcu;
 
+	/*
+	 * 管理所有的nvme_ns_head->entry
+	 */
 	list_add_tail(&head->entry, &ctrl->subsys->nsheads);
 
 	kref_get(&ctrl->subsys->ref);
@@ -3384,15 +4678,73 @@ static struct nvme_ns_head *nvme_alloc_ns_head(struct nvme_ctrl *ctrl,
 	return ERR_PTR(ret);
 }
 
+/*
+ * commit ed754e5deeb17f4e675c84e4b6c640cc7344e498
+ * Author: Christoph Hellwig <hch@lst.de>
+ * Date:   Thu Nov 9 13:50:43 2017 +0100
+ *
+ * nvme: track shared namespaces
+ *
+ * Introduce a new struct nvme_ns_head that holds information about an actual
+ * namespace, unlike struct nvme_ns, which only holds the per-controller
+ * namespace information.  For private namespaces there is a 1:1 relation of
+ * the two, but for shared namespaces this lets us discover all the paths to
+ * it.  For now only the identifiers are moved to the new structure, but most
+ * of the information in struct nvme_ns should eventually move over.
+ *
+ * To allow lockless path lookup the list of nvme_ns structures per
+ * nvme_ns_head is protected by SRCU, which requires freeing the nvme_ns
+ * structure through call_srcu.
+ *
+ * Signed-off-by: Christoph Hellwig <hch@lst.de>
+ * Reviewed-by: Keith Busch <keith.busch@intel.com>
+ * Reviewed-by: Javier González <javier@cnexlabs.com>
+ * Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
+ * Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
+ * Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
+ * Reviewed-by: Hannes Reinecke <hare@suse.com>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|3791| <<nvme_alloc_ns>> ret = nvme_init_ns_head(ns, nsid, id);
+ *
+ * struct nvme_ns:
+ *   -> struct nvme_ns_head *head;
+ */
 static int nvme_init_ns_head(struct nvme_ns *ns, unsigned nsid,
 		struct nvme_id_ns *id)
 {
 	struct nvme_ctrl *ctrl = ns->ctrl;
+	/*
+	 * Bit 0: If set to '1', then the namespace may be attached to two or more controllers in the
+	 * NVM subsystem concurrently (i.e., may be a shared namespace). If cleared to '0', then
+	 * the namespace is a private namespace and is able to be attached to only one controller
+	 * at a time.
+	 */
 	bool is_shared = id->nmic & (1 << 0);
 	struct nvme_ns_head *head = NULL;
 	int ret = 0;
 
+	/*
+	 * Namespace Multi-path I/O and Namespace Sharing Capabilities (NMIC): This field
+	 * specifies multi-path I/O and namespace sharing capabilities of the namespace.
+	 *
+	 * Bits 7:1 are reserved.
+	 *
+	 * Bit 0: If set to '1', then the namespace may be attached to two or more controllers in the
+	 * NVM subsystem concurrently (i.e., may be a shared namespace). If cleared to '0', then
+	 * the namespace is a private namespace and is able to be attached to only one controller
+	 * at a time.
+	 */
+
+	/*
+	 * An NVMe subsystem includes one or more controllers.
+	 */
 	mutex_lock(&ctrl->subsys->lock);
+	/*
+	 * __nvme_find_ns_head():
+	 * 遍历nvme_subsystem->nheads上的所有nvme_ns_head寻找nsid(nvme_ns_head->entry)
+	 */
 	if (is_shared)
 		head = __nvme_find_ns_head(ctrl->subsys, nsid);
 	if (!head) {
@@ -3404,6 +4756,7 @@ static int nvme_init_ns_head(struct nvme_ns *ns, unsigned nsid,
 	} else {
 		struct nvme_ns_ids ids;
 
+		/* 核心思想是填充最后一个参数 */
 		ret = nvme_report_ns_ids(ctrl, nsid, id, &ids);
 		if (ret)
 			goto out_unlock;
@@ -3417,6 +4770,9 @@ static int nvme_init_ns_head(struct nvme_ns *ns, unsigned nsid,
 		}
 	}
 
+	/*
+	 * 把属于一个共享的namespace的nsid加入nvme_ns_head->list
+	 */
 	list_add_tail(&ns->siblings, &head->list);
 	ns->head = head;
 
@@ -3427,6 +4783,10 @@ static int nvme_init_ns_head(struct nvme_ns *ns, unsigned nsid,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4349| <<nvme_scan_work>> list_sort(NULL, &ctrl->namespaces, ns_cmp);
+ */
 static int ns_cmp(void *priv, struct list_head *a, struct list_head *b)
 {
 	struct nvme_ns *nsa = container_of(a, struct nvme_ns, list);
@@ -3435,6 +4795,11 @@ static int ns_cmp(void *priv, struct list_head *a, struct list_head *b)
 	return nsa->head->ns_id - nsb->head->ns_id;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3884| <<nvme_validate_ns>> ns = nvme_find_get_ns(ctrl, nsid);
+ *   - drivers/nvme/host/core.c|3936| <<nvme_scan_ns_list>> ns = nvme_find_get_ns(ctrl, prev);
+ */
 static struct nvme_ns *nvme_find_get_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 {
 	struct nvme_ns *ns, *ret = NULL;
@@ -3442,6 +4807,10 @@ static struct nvme_ns *nvme_find_get_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	down_read(&ctrl->namespaces_rwsem);
 	list_for_each_entry(ns, &ctrl->namespaces, list) {
 		if (ns->head->ns_id == nsid) {
+			/*
+			 * Increment refcount for object unless it is zero.
+			 * Return non-zero if the increment succeeded. Otherwise return 0.
+			 */
 			if (!kref_get_unless_zero(&ns->kref))
 				continue;
 			ret = ns;
@@ -3454,6 +4823,10 @@ static struct nvme_ns *nvme_find_get_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4150| <<nvme_alloc_ns>> nvme_setup_streams_ns(ctrl, ns);
+ */
 static int nvme_setup_streams_ns(struct nvme_ctrl *ctrl, struct nvme_ns *ns)
 {
 	struct streams_directive_params s;
@@ -3480,6 +4853,20 @@ static int nvme_setup_streams_ns(struct nvme_ctrl *ctrl, struct nvme_ns *ns)
 	return 0;
 }
 
+/*
+ * [0] nvme_alloc_ns
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|3752| <<nvme_validate_ns>> nvme_alloc_ns(ctrl, nsid);
+ *
+ * 每个ns一个request_queue, 一个gendisk
+ */
 static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 {
 	struct nvme_ns *ns;
@@ -3492,6 +4879,18 @@ static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	if (!ns)
 		return -ENOMEM;
 
+	/*
+	 * struct nvme_ctrl:
+	 *   - struct blk_mq_tag_set *tagset;
+	 *   - struct blk_mq_tag_set *admin_tagset;
+	 *
+	 * struct nvme_dev:
+	 *   - struct blk_mq_tag_set tagset;
+	 *   - struct blk_mq_tag_set admin_tagset;
+	 *
+	 * blk_mq_init_queue()更加像是从tagset中新分配一个request_queue
+	 * 一个tagset可以有多个request_queue
+	 */
 	ns->queue = blk_mq_init_queue(ctrl->tagset);
 	if (IS_ERR(ns->queue)) {
 		ret = PTR_ERR(ns->queue);
@@ -3509,6 +4908,7 @@ static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	ns->queue->queuedata = ns;
 	ns->ctrl = ctrl;
 
+	/* 初始化为1 */
 	kref_init(&ns->kref);
 	ns->lba_shift = 9; /* set to a default value for 512 until disk is validated */
 
@@ -3562,6 +4962,15 @@ static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	device_add_disk(ctrl->device, ns->disk, nvme_ns_id_attr_groups);
 
 	nvme_mpath_add_disk(ns, id);
+	/*
+	 * controller和namespace各自有fault_injection
+	 *
+	 * # ls /sys/kernel/debug/nvme0/fault_inject/
+	 * dont_retry  interval  probability  space  status  task-filter  times  verbose  verbose_ratelimit_burst  verbose_ratelimit_interval_ms
+	 *
+	 * # ls /sys/kernel/debug/nvme0n1/fault_inject/
+	 * dont_retry  interval  probability  space  status  task-filter  times  verbose  verbose_ratelimit_burst  verbose_ratelimit_interval_ms
+	 */
 	nvme_fault_inject_init(&ns->fault_inject, ns->disk->disk_name);
 	kfree(id);
 
@@ -3584,22 +4993,51 @@ static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4248| <<nvme_validate_ns>> nvme_ns_remove(ns);
+ *   - drivers/nvme/host/core.c|4268| <<nvme_remove_invalid_namespaces>> nvme_ns_remove(ns);
+ *   - drivers/nvme/host/core.c|4303| <<nvme_scan_ns_list>> nvme_ns_remove(ns);
+ *   - drivers/nvme/host/core.c|4453| <<nvme_remove_namespaces>> nvme_ns_remove(ns);
+ *
+ * struct nvme_ns:
+ *  - struct request_queue *queue
+ */
 static void nvme_ns_remove(struct nvme_ns *ns)
 {
+	/*
+	 * 在以下使用NVME_NS_REMOVING:
+	 *   - drivers/nvme/host/core.c|4284| <<nvme_ns_remove>> if (test_and_set_bit(NVME_NS_REMOVING, &ns->flags))
+	 *   - drivers/nvme/host/multipath.c|182| <<nvme_path_is_disabled>> test_bit(NVME_NS_REMOVING, &ns->flags);
+	 */
 	if (test_and_set_bit(NVME_NS_REMOVING, &ns->flags))
 		return;
 
 	nvme_fault_inject_fini(&ns->fault_inject);
 
 	mutex_lock(&ns->ctrl->subsys->lock);
+	/* 链接在nvme_ns_head->list */
 	list_del_rcu(&ns->siblings);
 	mutex_unlock(&ns->ctrl->subsys->lock);
 	synchronize_rcu(); /* guarantee not available in head->list */
 	nvme_mpath_clear_current_path(ns);
 	synchronize_srcu(&ns->head->srcu); /* wait for concurrent submissions */
 
+	/*
+	 * 使用GENHD_FL_UP的部分例子:
+	 *   - block/genhd.c|795| <<__device_add_disk>> disk->flags |= GENHD_FL_UP;
+	 *   - block/genhd.c|879| <<del_gendisk>> disk->flags &= ~GENHD_FL_UP;
+	 *   - block/genhd.c|982| <<get_gendisk>> !(disk->flags & GENHD_FL_UP))) {
+	 */
 	if (ns->disk && ns->disk->flags & GENHD_FL_UP) {
 		del_gendisk(ns->disk);
+		/*
+		 * blk_cleanup_queue - shutdown a request queue
+		 * @q: request queue to shutdown
+		 *
+		 * Mark @q DYING, drain all pending requests, mark @q DEAD, destroy and
+		 * put it.  All future requests will be failed immediately with -ENODEV.
+		 */
 		blk_cleanup_queue(ns->queue);
 		if (blk_get_integrity(ns->disk))
 			blk_integrity_unregister(ns->disk);
@@ -3613,12 +5051,30 @@ static void nvme_ns_remove(struct nvme_ns *ns)
 	nvme_put_ns(ns);
 }
 
+/*
+ * [0] Workqueue: nvme-wq nvme_scan_work
+ * [0] Call Trace:
+ * [0]  nvme_scan_ns_sequential
+ * [0]  nvme_scan_work
+ * [0]  process_one_work
+ * [0]  worker_thread
+ * [0]  kthread
+ * [0]  ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|3795| <<nvme_scan_ns_list>> nvme_validate_ns(ctrl, nsid);
+ *   - drivers/nvme/host/core.c|3819| <<nvme_scan_ns_sequential>> nvme_validate_ns(ctrl, i);
+ */
 static void nvme_validate_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 {
 	struct nvme_ns *ns;
 
 	ns = nvme_find_get_ns(ctrl, nsid);
 	if (ns) {
+		/*
+		 * nvme是nvme_revalidate_disk(), 应该是期望返回0吧
+		 * 所以如果只是rescan的话因为ns(request_queue)还在, 不用alloc了
+		 */
 		if (ns->disk && revalidate_disk(ns->disk))
 			nvme_ns_remove(ns);
 		nvme_put_ns(ns);
@@ -3626,6 +5082,13 @@ static void nvme_validate_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 		nvme_alloc_ns(ctrl, nsid);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1752| <<nvme_passthru_end>> nvme_remove_invalid_namespaces(ctrl, NVME_NSID_ALL);
+ *   - drivers/nvme/host/core.c|4254| <<nvme_remove_invalid_namespaces>> static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
+ *   - drivers/nvme/host/core.c|4311| <<nvme_scan_ns_list>> nvme_remove_invalid_namespaces(ctrl, prev);
+ *   - drivers/nvme/host/core.c|4328| <<nvme_scan_ns_sequential>> nvme_remove_invalid_namespaces(ctrl, nn);
+ */
 static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 					unsigned nsid)
 {
@@ -3644,6 +5107,16 @@ static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4036| <<nvme_scan_work>> if (!nvme_scan_ns_list(ctrl, nn))
+ *
+ * The NVMe 1.1 specification provides an identify mode to return a
+ * list of active namespaces. This is more efficient to discover which
+ * namespace identifiers are active on a controller, providing potentially
+ * significant improvement in scan time for controllers with sparesly
+ * populated namespaces.
+ */
 static int nvme_scan_ns_list(struct nvme_ctrl *ctrl, unsigned nn)
 {
 	struct nvme_ns *ns;
@@ -3657,6 +5130,16 @@ static int nvme_scan_ns_list(struct nvme_ctrl *ctrl, unsigned nn)
 		return -ENOMEM;
 
 	for (i = 0; i < num_lists; i++) {
+		/*
+		 * 只在这里调用
+		 *
+		 * A list of 1,024 namespace IDs is returned to the host containing active NSIDs in increasing order that are
+		 * greater than the value specified in the Namespace Identifier (NSID) field of the command. The controller
+		 * should abort the command with status code Invalid Namespace or Format if the NSID field is set to
+		 * FFFFFFFEh or FFFFFFFFh. The NSID field may be cleared to 0h to retrieve a Namespace List including
+		 * the namespace starting with NSID of 1h. The data structure returned is a Namespace List (refer to section
+		 * 1769  * 4.10)
+		 */
 		ret = nvme_identify_ns_list(ctrl, prev, ns_list);
 		if (ret)
 			goto free;
@@ -3685,16 +5168,34 @@ static int nvme_scan_ns_list(struct nvme_ctrl *ctrl, unsigned nn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4039| <<nvme_scan_work>> nvme_scan_ns_sequential(ctrl, nn);
+ */
 static void nvme_scan_ns_sequential(struct nvme_ctrl *ctrl, unsigned nn)
 {
 	unsigned i;
 
+	/* 在qemu上nn=1 */
 	for (i = 1; i <= nn; i++)
 		nvme_validate_ns(ctrl, i);
 
 	nvme_remove_invalid_namespaces(ctrl, nn);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4397| <<nvme_scan_work>> nvme_clear_changed_ns_log(ctrl);
+ *
+ * This log page is used to describe namespaces attached to this controller that have:
+ * a) changed information in their Identify Namespace data structure since the last time the log page
+ * b) was read;
+ * c) been added; and
+ * been deleted.
+ * The log page contains a Namespace List with up to 1,024 entries. If more than 1,024 namespaces have
+ * changed attributes since the last time the log page was read, the first entry in the log page shall be set to
+ * FFFFFFFFh and the remainder of the list shall be zero filled.
+ */
 static void nvme_clear_changed_ns_log(struct nvme_ctrl *ctrl)
 {
 	size_t log_size = NVME_MAX_CHANGED_NAMESPACES * sizeof(__le32);
@@ -3720,6 +5221,34 @@ static void nvme_clear_changed_ns_log(struct nvme_ctrl *ctrl)
 	kfree(log);
 }
 
+/*
+ * 在qemu测试的时候, 先...
+ * [0] nvme_async_probe
+ * [0] async_run_entry_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 再...
+ * [0] nvme_queue_scan
+ * [0] nvme_start_ctrl
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|120| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+ *   - drivers/nvme/host/core.c|3902| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+ *   - drivers/nvme/host/pci.c|3420| <<nvme_async_probe>> flush_work(&dev->ctrl.scan_work);
+ *
+ * 在以下使用nvme_scan_work():
+ *   - drivers/nvme/host/core.c|4180| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+ *
+ * # echo 1 > /sys/block/nvme0n1/device/rescan_controller
+ */
 static void nvme_scan_work(struct work_struct *work)
 {
 	struct nvme_ctrl *ctrl =
@@ -3731,18 +5260,48 @@ static void nvme_scan_work(struct work_struct *work)
 	if (ctrl->state != NVME_CTRL_LIVE || !ctrl->tagset)
 		return;
 
+	/*
+	 * 在以下使用NVME_AER_NOTICE_NS_CHANGED:
+	 *   - drivers/nvme/host/core.c|4949| <<nvme_scan_work>> if (test_and_clear_bit(NVME_AER_NOTICE_NS_CHANGED, &ctrl->events)) {
+	 *   - - drivers/nvme/host/core.c|5220| <<nvme_handle_aen_notice>> case NVME_AER_NOTICE_NS_CHANGED:
+	 *   - drivers/nvme/host/core.c|5221| <<nvme_handle_aen_notice>> set_bit(NVME_AER_NOTICE_NS_CHANGED, &ctrl->events);
+	 *   - drivers/nvme/host/trace.h|131| <<__field>> aer_name(NVME_AER_NOTICE_NS_CHANGED),
+	 *   - drivers/nvme/target/core.c|231| <<nvmet_ns_changed>> NVME_AER_NOTICE_NS_CHANGED,
+	 */
 	if (test_and_clear_bit(NVME_AER_NOTICE_NS_CHANGED, &ctrl->events)) {
 		dev_info(ctrl->device, "rescanning namespaces.\n");
 		nvme_clear_changed_ns_log(ctrl);
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/core.c|3128| <<nvme_init_identify>> ret = nvme_identify_ctrl(ctrl, &id);
+	 *   - drivers/nvme/host/core.c|4199| <<nvme_scan_work>> if (nvme_identify_ctrl(ctrl, &id))
+	 */
 	if (nvme_identify_ctrl(ctrl, &id))
 		return;
 
 	mutex_lock(&ctrl->scan_lock);
 	nn = le32_to_cpu(id->nn);
+	/*
+	 * 在qemu上ctrl->vs=0x10200
+	 * NVME_VS(1, 0, 0)=0x10000
+	 * NVME_VS(1, 1, 0)=0x10100
+	 *
+	 * # nvme  show-regs -H
+	 *
+	 * 从以下可以获得vs:
+	 *   - drivers/nvme/host/core.c|3333| <<nvme_init_identify>> ret = ctrl->ops->reg_read32(ctrl, NVME_REG_VS, &ctrl->vs);
+	 */
 	if (ctrl->vs >= NVME_VS(1, 1, 0) &&
 	    !(ctrl->quirks & NVME_QUIRK_IDENTIFY_CNS)) {
+		/*
+		 * The NVMe 1.1 specification provides an identify mode to return a
+		 * list of active namespaces. This is more efficient to discover which
+		 * namespace identifiers are active on a controller, providing potentially
+		 * significant improvement in scan time for controllers with sparesly
+		 * populated namespaces.
+		 */
 		if (!nvme_scan_ns_list(ctrl, nn))
 			goto out_free_id;
 	}
@@ -3760,6 +5319,18 @@ static void nvme_scan_work(struct work_struct *work)
  * controller failure. It is up to the caller to ensure the namespace list is
  * not modified by scan work while this function is executing.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|250| <<nvme_do_delete_ctrl>> nvme_remove_namespaces(ctrl);
+ *   - drivers/nvme/host/pci.c|4156| <<nvme_reset_work>> nvme_remove_namespaces(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|4490| <<nvme_remove>> nvme_remove_namespaces(&dev->ctrl);
+ *
+ * This function iterates the namespace list unlocked to allow recovery from
+ * controller failure. It is up to the caller to ensure the namespace list is
+ * not modified by scan work while this function is executing.
+ *
+ * 这个函数执行完之后ctrl->namespaces就没元素了
+ */
 void nvme_remove_namespaces(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns, *next;
@@ -3773,6 +5344,7 @@ void nvme_remove_namespaces(struct nvme_ctrl *ctrl)
 	nvme_mpath_clear_ctrl_paths(ctrl);
 
 	/* prevent racing with ns scanning */
+	/* nvme_scan_work() */
 	flush_work(&ctrl->scan_work);
 
 	/*
@@ -3785,6 +5357,7 @@ void nvme_remove_namespaces(struct nvme_ctrl *ctrl)
 		nvme_kill_queues(ctrl);
 
 	down_write(&ctrl->namespaces_rwsem);
+	/* 从这行之后namespaces的list就空了 */
 	list_splice_init(&ctrl->namespaces, &ns_list);
 	up_write(&ctrl->namespaces_rwsem);
 
@@ -3793,6 +5366,36 @@ void nvme_remove_namespaces(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_remove_namespaces);
 
+/*
+ * 不只一种调用方法.
+ *
+ * nvme_class_uevent
+ * [0] dev_uevent
+ * [0] kobject_uevent_env
+ * [0] device_add
+ * [0] cdev_device_add
+ * [0] nvme_init_ctrl
+ * [0] nvme_loop_create_ctrl
+ * [0] nvmf_dev_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] nvme_class_uevent
+ * [0] dev_uevent
+ * [0] uevent_show
+ * [0] dev_attr_show
+ * [0] sysfs_kf_seq_show
+ * [0] seq_read
+ * [0] vfs_read
+ * [0] ksys_read
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用nvme_class_uevent():
+ *   - drivers/nvme/host/core.c|4541| <<nvme_core_init>> nvme_class->dev_uevent = nvme_class_uevent;
+ */
 static int nvme_class_uevent(struct device *dev, struct kobj_uevent_env *env)
 {
 	struct nvme_ctrl *ctrl =
@@ -3820,6 +5423,10 @@ static int nvme_class_uevent(struct device *dev, struct kobj_uevent_env *env)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|5094| <<nvme_async_event_work>> nvme_aen_uevent(ctrl);
+ */
 static void nvme_aen_uevent(struct nvme_ctrl *ctrl)
 {
 	char *envp[2] = { NULL, NULL };
@@ -3836,15 +5443,45 @@ static void nvme_aen_uevent(struct nvme_ctrl *ctrl)
 	kfree(envp[0]);
 }
 
+/*
+ * AEN requests are special as they don't time out and can
+ * survive any kind of queue freeze and often don't respond to
+ * aborts.  We don't even bother to allocate a struct request
+ * for them but rather special case them here.
+ *
+ * 使用admin queue来接受async event notification
+ * 比如IO的sq的head写错了, 就会触发admin queue接收一个cqe用来aen
+ *
+ * 关于admin ring buffer和async event, nvme的admin queue的tagset
+ * 有30个元素, 但是ring buffer有32个,除了一个用来表示full, 另外
+ * 一个用在async event
+ *
+ * 在以下使用async_event_work:
+ *   - drivers/nvme/host/core.c|1580| <<nvme_enable_aen>> queue_work(nvme_wq, &ctrl->async_event_work);
+ *   - drivers/nvme/host/core.c|4345| <<nvme_async_event_work>> container_of(work, struct nvme_ctrl, async_event_work);
+ *   - drivers/nvme/host/core.c|4483| <<nvme_complete_async_event>> queue_work(nvme_wq, &ctrl->async_event_work);
+ *   - drivers/nvme/host/core.c|4500| <<nvme_stop_ctrl>> flush_work(&ctrl->async_event_work);
+ *   - drivers/nvme/host/core.c|4589| <<nvme_init_ctrl>> INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
+ *
+ * 在以下使用nvme_async_event_work():
+ *   - drivers/nvme/host/core.c|4837| <<nvme_init_ctrl>> INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
+ */
 static void nvme_async_event_work(struct work_struct *work)
 {
 	struct nvme_ctrl *ctrl =
 		container_of(work, struct nvme_ctrl, async_event_work);
 
 	nvme_aen_uevent(ctrl);
+	/*
+	 * 参考nvme_is_aen_req()
+	 */
 	ctrl->ops->submit_async_event(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|5279| <<nvme_fw_act_work>> while (nvme_ctrl_pp_status(ctrl)) {
+ */
 static bool nvme_ctrl_pp_status(struct nvme_ctrl *ctrl)
 {
 
@@ -3859,6 +5496,10 @@ static bool nvme_ctrl_pp_status(struct nvme_ctrl *ctrl)
 	return ((ctrl->ctrl_config & NVME_CC_ENABLE) && (csts & NVME_CSTS_PP));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4600| <<nvme_fw_act_work>> nvme_get_fw_slot_info(ctrl);
+ */
 static void nvme_get_fw_slot_info(struct nvme_ctrl *ctrl)
 {
 	struct nvme_fw_slot_info_log *log;
@@ -3873,6 +5514,38 @@ static void nvme_get_fw_slot_info(struct nvme_ctrl *ctrl)
 	kfree(log);
 }
 
+/*
+ * commit b6dccf7fae4331b0ea41cf087e3f02d5db9161dc
+ * Author: Arnav Dawn <a.dawn@samsung.com>
+ * Date:   Wed Jul 12 16:10:40 2017 +0530
+ *
+ * nvme: add support for FW activation without reset
+ *
+ * This patch adds support for handling Fw activation without reset
+ * On completion of FW-activation-starting AER, all queues are
+ * paused till CSTS.PP is cleared or timed out (exceeds max time for
+ * fw activtion MTFA). If device fails to clear CSTS.PP within MTFA,
+ * driver issues reset controller.
+ * 
+ * Signed-off-by: Arnav Dawn <a.dawn@samsung.com>
+ * Reviewed-by: Keith Busch <keith.busch@intel.com>
+ * Reviewed-by: Christoph Hellwig <hch@lst.de>
+ * Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
+ *
+ * 在以下使用fw_act_work:
+ *   - drivers/nvme/host/core.c|5268| <<nvme_fw_act_work>> struct nvme_ctrl, fw_act_work);
+ *   - drivers/nvme/host/core.c|5334| <<nvme_handle_aen_notice>> queue_work(nvme_wq, &ctrl->fw_act_work);
+ *   - drivers/nvme/host/core.c|5418| <<nvme_stop_ctrl>> cancel_work_sync(&ctrl->fw_act_work);
+ *   - drivers/nvme/host/core.c|5563| <<nvme_init_ctrl>> INIT_WORK(&ctrl->fw_act_work, nvme_fw_act_work);
+ *
+ * 在以下使用nvme_fw_act_work():
+ *   - drivers/nvme/host/core.c|4299| <<nvme_init_ctrl>> INIT_WORK(&ctrl->fw_act_work, nvme_fw_act_work);
+ *
+ * Firmware Activation Starting: The controller is starting a firmware activation process during
+ * which command processing is paused. Host software may use CSTS.PP to determine when
+ * command processing has resumed. To clear this event, host software reads the Firmware Slot
+ * Information log page.
+ */
 static void nvme_fw_act_work(struct work_struct *work)
 {
 	struct nvme_ctrl *ctrl = container_of(work,
@@ -3905,6 +5578,25 @@ static void nvme_fw_act_work(struct work_struct *work)
 	nvme_get_fw_slot_info(ctrl);
 }
 
+/*
+ * [  143.581757] Workqueue: events nvmet_async_event_work
+ * [  143.581758] Call Trace:
+ * [  143.581764]  dump_stack+0x64/0x83
+ * [  143.581766]  nvme_queue_scan+0x17/0x50
+ * [  143.581769]  nvme_complete_async_event+0x17b/0x1c0
+ * [  143.581771]  nvmet_req_complete+0xc/0x40
+ * [  143.581773]  nvmet_async_event_work+0x89/0xb0
+ * [  143.581775]  process_one_work+0x15b/0x360
+ * [  143.581782]  worker_thread+0x44/0x3d0
+ * [  143.581784]  kthread+0xf3/0x130
+ * [  143.581786]  ? max_active_store+0x80/0x80
+ * [  143.581787]  ? kthread_bind+0x10/0x10
+ * [  143.581789]  ret_from_fork+0x35/0x40
+ * [  143.581796] nvme nvme0: rescanning namespaces.
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|4663| <<nvme_complete_async_event>> nvme_handle_aen_notice(ctrl, result);
+ */
 static void nvme_handle_aen_notice(struct nvme_ctrl *ctrl, u32 result)
 {
 	u32 aer_notice_type = (result & 0xff00) >> 8;
@@ -3917,6 +5609,12 @@ static void nvme_handle_aen_notice(struct nvme_ctrl *ctrl, u32 result)
 		nvme_queue_scan(ctrl);
 		break;
 	case NVME_AER_NOTICE_FW_ACT_STARTING:
+		/*
+		 * Firmware Activation Starting: The controller is starting a firmware activation process during
+		 * which command processing is paused. Host software may use CSTS.PP to determine when
+		 * command processing has resumed. To clear this event, host software reads the Firmware Slot
+		 * Information log page.
+		 */
 		/*
 		 * We are (ab)using the RESETTING state to prevent subsequent
 		 * recovery actions from interfering with the controller's
@@ -3927,8 +5625,22 @@ static void nvme_handle_aen_notice(struct nvme_ctrl *ctrl, u32 result)
 		break;
 #ifdef CONFIG_NVME_MULTIPATH
 	case NVME_AER_NOTICE_ANA:
+		/*
+		 * 在以下使用NVME_AER_NOTICE_ANA:
+		 *   - drivers/nvme/host/core.c|5032| <<nvme_handle_aen_notice>> case NVME_AER_NOTICE_ANA:
+		 *   - drivers/nvme/host/trace.h|132| <<__field>> aer_name(NVME_AER_NOTICE_ANA),
+		 *   - drivers/nvme/target/core.c|248| <<nvmet_send_ana_event>> NVME_AER_NOTICE_ANA, NVME_LOG_ANA);
+		 */
 		if (!ctrl->ana_log_buf)
 			break;
+		/*
+		 * 在以下使用nvme_ctrl->ana_work:
+		 *   - drivers/nvme/host/core.c|5025| <<nvme_handle_aen_notice>> queue_work(nvme_wq, &ctrl->ana_work);
+		 *   - drivers/nvme/host/multipath.c|102| <<nvme_failover_req>> queue_work(nvme_wq, &ns->ctrl->ana_work);
+		 *   - drivers/nvme/host/multipath.c|627| <<nvme_ana_work>> struct nvme_ctrl *ctrl = container_of(work, struct nvme_ctrl, ana_work);
+		 *   - drivers/nvme/host/multipath.c|645| <<nvme_mpath_stop>> cancel_work_sync(&ctrl->ana_work);
+		 *   - drivers/nvme/host/multipath.c|781| <<nvme_mpath_init>> INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+		 */
 		queue_work(nvme_wq, &ctrl->ana_work);
 		break;
 #endif
@@ -3940,6 +5652,18 @@ static void nvme_handle_aen_notice(struct nvme_ctrl *ctrl, u32 result)
 	}
 }
 
+/*
+ * Async Event Notification
+ *
+ * called by:
+ *   - drivers/nvme/host/fc.c|1693| <<nvme_fc_fcpio_done>> nvme_complete_async_event(&queue->ctrl->ctrl, status, &result);
+ *   - drivers/nvme/host/pci.c|1112| <<nvme_handle_cqe>> nvme_complete_async_event(&nvmeq->dev->ctrl,
+ *   - drivers/nvme/host/rdma.c|1504| <<nvme_rdma_recv_done>> nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+ *   - drivers/nvme/host/tcp.c|496| <<nvme_tcp_handle_comp>> nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+ *   - drivers/nvme/target/loop.c|106| <<nvme_loop_queue_response>> nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+ *
+ * The driver can handle tracking only one AEN request
+ */
 void nvme_complete_async_event(struct nvme_ctrl *ctrl, __le16 status,
 		volatile union nvme_result *res)
 {
@@ -3967,6 +5691,15 @@ void nvme_complete_async_event(struct nvme_ctrl *ctrl, __le16 status,
 }
 EXPORT_SYMBOL_GPL(nvme_complete_async_event);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|235| <<nvme_do_delete_ctrl>> nvme_stop_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|2949| <<nvme_fc_reset_ctrl_work>> nvme_stop_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|3569| <<nvme_remove>> nvme_stop_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1908| <<nvme_rdma_reset_ctrl_work>> nvme_stop_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1947| <<nvme_reset_ctrl_work>> nvme_stop_ctrl(ctrl);
+ *   - drivers/nvme/target/loop.c|450| <<nvme_loop_reset_ctrl_work>> nvme_stop_ctrl(&ctrl->ctrl);
+ */
 void nvme_stop_ctrl(struct nvme_ctrl *ctrl)
 {
 	nvme_mpath_stop(ctrl);
@@ -3976,8 +5709,34 @@ void nvme_stop_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_stop_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2736| <<nvme_fc_create_association>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|3128| <<nvme_reset_work>> nvme_start_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1031| <<nvme_rdma_setup_ctrl>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1865| <<nvme_tcp_setup_ctrl>> nvme_start_ctrl(ctrl);
+ *   - drivers/nvme/target/loop.c|477| <<nvme_loop_reset_ctrl_work>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|633| <<nvme_loop_create_ctrl>> nvme_start_ctrl(&ctrl->ctrl);
+ *
+ * 这个函数会触发ns的scan: queue_work(nvme_wq, &ctrl->scan_work);
+ */
 void nvme_start_ctrl(struct nvme_ctrl *ctrl)
 {
+	/*
+	 * 设置nvme_ctrl->kato的地方 (没有pci):
+	 *   - drivers/nvme/host/fc.c|3119| <<nvme_fc_init_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/host/rdma.c|2023| <<nvme_rdma_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/host/tcp.c|2293| <<nvme_tcp_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/target/loop.c|593| <<nvme_loop_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 * 部分使用nvme_ctrl->kato的地方:
+	 *   - drivers/nvme/host/core.c|1285| <<nvme_keep_alive_end_io>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1297| <<nvme_keep_alive>> rq->timeout = ctrl->kato * HZ;
+	 *   - drivers/nvme/host/core.c|1315| <<nvme_keep_alive_work>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1366| <<nvme_start_keep_alive>> if (unlikely(ctrl->kato == 0))
+	 *   - drivers/nvme/host/core.c|1369| <<nvme_start_keep_alive>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1381| <<nvme_stop_keep_alive>> if (unlikely(ctrl->kato == 0))
+	 *   - drivers/nvme/host/core.c|4904| <<nvme_start_ctrl>> if (ctrl->kato)
+	 */
 	if (ctrl->kato)
 		nvme_start_keep_alive(ctrl);
 
@@ -3985,11 +5744,30 @@ void nvme_start_ctrl(struct nvme_ctrl *ctrl)
 
 	if (ctrl->queue_count > 1) {
 		nvme_queue_scan(ctrl);
+		/*
+		 * 在以下调用blk_queue_quiesced()检查request_queue是否quiesced:
+		 *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+		 *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+		 *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+		 *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+		 *
+		 * 核心思想是为所有的ns调用blk_mq_unquiesce_queue()
+		 */
 		nvme_start_queues(ctrl);
 	}
 }
 EXPORT_SYMBOL_GPL(nvme_start_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|255| <<nvme_do_delete_ctrl>> nvme_uninit_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|3211| <<nvme_fc_init_ctrl>> nvme_uninit_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|4440| <<nvme_remove>> nvme_uninit_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|2055| <<nvme_rdma_create_ctrl>> nvme_uninit_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|2366| <<nvme_tcp_create_ctrl>> nvme_uninit_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|487| <<nvme_loop_reset_ctrl_work>> nvme_uninit_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|642| <<nvme_loop_create_ctrl>> nvme_uninit_ctrl(&ctrl->ctrl);
+ */
 void nvme_uninit_ctrl(struct nvme_ctrl *ctrl)
 {
 	nvme_fault_inject_fini(&ctrl->fault_inject);
@@ -3998,6 +5776,20 @@ void nvme_uninit_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_uninit_ctrl);
 
+/*
+ * [0] nvme_free_ctrl
+ * [0] device_release
+ * [0] kobject_cleanup
+ * [0] nvme_sysfs_delete
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用nvme_free_ctrl():
+ *   - drivers/nvme/host/core.c|4805| <<nvme_init_ctrl>> ctrl->device->release = nvme_free_ctrl;
+ */
 static void nvme_free_ctrl(struct device *dev)
 {
 	struct nvme_ctrl *ctrl =
@@ -4013,6 +5805,7 @@ static void nvme_free_ctrl(struct device *dev)
 
 	if (subsys) {
 		mutex_lock(&nvme_subsystems_lock);
+		/* 链接入subsys->ctrls */
 		list_del(&ctrl->subsys_entry);
 		sysfs_remove_link(&subsys->dev.kobj, dev_name(ctrl->device));
 		mutex_unlock(&nvme_subsystems_lock);
@@ -4029,6 +5822,28 @@ static void nvme_free_ctrl(struct device *dev)
  * earliest initialization so that we have the initialized structured around
  * during probing.
  */
+/*
+ * [0] nvme_init_ctrl
+ * [0] nvme_loop_create_ctrl
+ * [0] nvmf_dev_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/fc.c|3167| <<nvme_fc_init_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_fc_ctrl_ops, 0);
+ *   - drivers/nvme/host/pci.c|3490| <<nvme_probe>> result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
+ *   - drivers/nvme/host/rdma.c|2031| <<nvme_rdma_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
+ *   - drivers/nvme/host/tcp.c|2340| <<nvme_tcp_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_tcp_ctrl_ops, 0);
+ *   - drivers/nvme/target/loop.c|585| <<nvme_loop_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_loop_ctrl_ops,
+ *
+ * 每个nvme pci device (func) 是一个controller.
+ * 可能很多个controller输入同一个subsystem. 但是pci的入口都是从probe controller开始的.
+ * subsystem的数据结构的分配是初始化每一个controller的时候on demand的.
+ * 如果两个controller属于同一个subsystem, 那初始化第二个controller的时候就不会再分配subsystem的结构了.
+ * 每一个nvme0或者nvme1都是一个controller.
+ */
 int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
 		const struct nvme_ctrl_ops *ops, unsigned long quirks)
 {
@@ -4046,10 +5861,20 @@ int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
 	INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
 	INIT_WORK(&ctrl->fw_act_work, nvme_fw_act_work);
 	INIT_WORK(&ctrl->delete_work, nvme_delete_ctrl_work);
+	/*
+	 * 在以下使用state_wq:
+	 *   - drivers/nvme/host/core.c|535| <<nvme_change_ctrl_state>> wake_up_all(&ctrl->state_wq);
+	 *   - drivers/nvme/host/core.c|571| <<nvme_wait_reset>> wait_event(ctrl->state_wq,
+	 *   - drivers/nvme/host/core.c|4515| <<nvme_init_ctrl>> init_waitqueue_head(&ctrl->state_wq);
+	 */
 	init_waitqueue_head(&ctrl->state_wq);
 
 	INIT_DELAYED_WORK(&ctrl->ka_work, nvme_keep_alive_work);
 	memset(&ctrl->ka_cmd, 0, sizeof(ctrl->ka_cmd));
+	/*
+	 * struct nvme_ctrl:
+	 *  - struct nvme_command ka_cmd;
+	 */
 	ctrl->ka_cmd.common.opcode = nvme_admin_keep_alive;
 
 	BUILD_BUG_ON(NVME_DSM_MAX_RANGES * sizeof(struct nvme_dsm_range) >
@@ -4112,6 +5937,12 @@ EXPORT_SYMBOL_GPL(nvme_init_ctrl);
  * Call this function when the driver determines it is unable to get the
  * controller in a state capable of servicing IO.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4028| <<nvme_remove_namespaces>> nvme_kill_queues(ctrl);
+ *   - drivers/nvme/host/pci.c|3133| <<nvme_remove_dead_ctrl>> nvme_kill_queues(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3287| <<nvme_reset_work>> nvme_kill_queues(&dev->ctrl);
+ */
 void nvme_kill_queues(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -4129,6 +5960,12 @@ void nvme_kill_queues(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_kill_queues);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1532| <<nvme_passthru_end>> nvme_unfreeze(ctrl);
+ *   - drivers/nvme/host/pci.c|3294| <<nvme_reset_work>> nvme_unfreeze(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3665| <<nvme_suspend>> nvme_unfreeze(ctrl);
+ */
 void nvme_unfreeze(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -4140,6 +5977,12 @@ void nvme_unfreeze(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_unfreeze);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3429| <<nvme_dev_disable>> nvme_wait_freeze_timeout(&dev->ctrl, NVME_IO_TIMEOUT);
+ *
+ * 对每一个namespace调用blk_mq_freeze_queue_wait_timeout()
+ */
 void nvme_wait_freeze_timeout(struct nvme_ctrl *ctrl, long timeout)
 {
 	struct nvme_ns *ns;
@@ -4154,6 +5997,12 @@ void nvme_wait_freeze_timeout(struct nvme_ctrl *ctrl, long timeout)
 }
 EXPORT_SYMBOL_GPL(nvme_wait_freeze_timeout);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1501| <<nvme_passthru_start>> nvme_wait_freeze(ctrl);
+ *   - drivers/nvme/host/pci.c|3292| <<nvme_reset_work>> nvme_wait_freeze(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3632| <<nvme_suspend>> nvme_wait_freeze(ctrl);
+ */
 void nvme_wait_freeze(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -4165,33 +6014,103 @@ void nvme_wait_freeze(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_wait_freeze);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1500| <<nvme_passthru_start>> nvme_start_freeze(ctrl);
+ *   - drivers/nvme/host/pci.c|3030| <<nvme_dev_disable>> nvme_start_freeze(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3631| <<nvme_suspend>> nvme_start_freeze(ctrl);
+ *
+ * 为每一个namespace的request_queue调用blk_freeze_queue_start()
+ * 核心是ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+ * 这样blk_queue_enter()这样的函数就没法前进了
+ */
 void nvme_start_freeze(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
 
 	down_read(&ctrl->namespaces_rwsem);
+	/*
+	 * blk_freeze_queue_start():
+	 * 核心是ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	 * 这样blk_queue_enter()这样的函数就没法前进了
+	 */
 	list_for_each_entry(ns, &ctrl->namespaces, list)
 		blk_freeze_queue_start(ns->queue);
 	up_read(&ctrl->namespaces_rwsem);
 }
 EXPORT_SYMBOL_GPL(nvme_start_freeze);
 
+/*
+ * 在preempt下的spinlock, 或者非preempt下的不调用都会hang在rcu
+ * [<0>] __wait_rcu_gp
+ * [<0>] synchronize_rcu
+ * [<0>] blk_mq_quiesce_queue
+ * [<0>] nvme_stop_queues
+ * [<0>] nvme_dev_disable
+ * [<0>] nvme_reset_work
+ * [<0>] process_one_work
+ * [<0>] worker_thread
+ * [<0>] kthread
+ * [<0>] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|4647| <<nvme_fw_act_work>> nvme_stop_queues(ctrl);
+ *   - drivers/nvme/host/fc.c|2789| <<nvme_fc_delete_association>> nvme_stop_queues(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|3807| <<nvme_dev_disable>> nvme_stop_queues(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|932| <<nvme_rdma_teardown_io_queues>> nvme_stop_queues(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1794| <<nvme_tcp_teardown_io_queues>> nvme_stop_queues(ctrl);
+ *   - drivers/nvme/target/loop.c|409| <<nvme_loop_shutdown_ctrl>> nvme_stop_queues(&ctrl->ctrl);
+ */
 void nvme_stop_queues(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
 
 	down_read(&ctrl->namespaces_rwsem);
+	/*
+	 * 在以下调用blk_queue_quiesced()检查request_queue是否quiesced:
+	 *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+	 *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+	 *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+	 *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+	 */
 	list_for_each_entry(ns, &ctrl->namespaces, list)
 		blk_mq_quiesce_queue(ns->queue);
 	up_read(&ctrl->namespaces_rwsem);
 }
 EXPORT_SYMBOL_GPL(nvme_stop_queues);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4018| <<nvme_fw_act_work>> nvme_start_queues(ctrl);
+ *   - drivers/nvme/host/core.c|4103| <<nvme_start_ctrl>> nvme_start_queues(ctrl);
+ *   - drivers/nvme/host/fc.c|2851| <<nvme_fc_delete_association>> nvme_start_queues(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|2918| <<nvme_dev_disable>> nvme_start_queues(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3111| <<nvme_reset_work>> nvme_start_queues(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|940| <<nvme_rdma_teardown_io_queues>> nvme_start_queues(&ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|1073| <<nvme_rdma_error_recovery_work>> nvme_start_queues(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1802| <<nvme_tcp_teardown_io_queues>> nvme_start_queues(ctrl);
+ *   - drivers/nvme/host/tcp.c|1910| <<nvme_tcp_error_recovery_work>> nvme_start_queues(ctrl);
+ *
+ * 在以下调用blk_queue_quiesced()检查request_queue是否quiesced:
+ *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+ *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+ *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+ *
+ * 核心思想是为所有的ns调用blk_mq_unquiesce_queue()
+ */
 void nvme_start_queues(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
 
 	down_read(&ctrl->namespaces_rwsem);
+	/*
+	 * 在以下调用blk_queue_quiesced()检查request_queue是否quiesced:
+	 *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+	 *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+	 *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+	 *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+	 */
 	list_for_each_entry(ns, &ctrl->namespaces, list)
 		blk_mq_unquiesce_queue(ns->queue);
 	up_read(&ctrl->namespaces_rwsem);
@@ -4199,15 +6118,45 @@ void nvme_start_queues(struct nvme_ctrl *ctrl)
 EXPORT_SYMBOL_GPL(nvme_start_queues);
 
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2717| <<nvme_reset_work>> nvme_sync_queues(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3027| <<nvme_reset_prepare>> nvme_sync_queues(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3133| <<nvme_suspend>> nvme_sync_queues(ctrl);
+ *
+ * 核心思想是对每一个namespace的request_queue和admin的requests调用:
+ * blk_sync_queue():
+ *  -> del_timer_sync(&q->timeout);
+ *  -> cancel_work_sync(&q->timeout_work);
+ *
+ * A block device may call blk_sync_queue to ensure that any
+ *     such activity is cancelled, thus allowing it to release resources
+ *     that the callbacks might use.
+ *
+ * 也就是说,这样timeout就不会再发生了
+ */
 void nvme_sync_queues(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
 
 	down_read(&ctrl->namespaces_rwsem);
+	/*
+	 * The block layer may perform asynchronous callback activity
+	 * on a queue, such as calling the unplug function after a timeout.
+	 * A block device may call blk_sync_queue to ensure that any
+	 * such activity is cancelled, thus allowing it to release resources
+	 * that the callbacks might use. The caller must already have made sure
+	 * that its ->make_request_fn will not re-add plugging prior to calling
+	 * this function
+	 */
 	list_for_each_entry(ns, &ctrl->namespaces, list)
 		blk_sync_queue(ns->queue);
 	up_read(&ctrl->namespaces_rwsem);
 
+	/*
+	 * struct nvme_ctrl:
+	 *   - struct request_queue *admin_q;
+	 */
 	if (ctrl->admin_q)
 		blk_sync_queue(ctrl->admin_q);
 }
@@ -4263,6 +6212,16 @@ static int __init nvme_core_init(void)
 	if (result < 0)
 		goto destroy_delete_wq;
 
+	/*
+	 * 在以下使用nvme_class:
+	 *   - drivers/nvme/host/core.c|5430| <<nvme_init_ctrl>> ctrl->device->class = nvme_class;
+	 *   - drivers/nvme/host/core.c|5694| <<nvme_core_init>> nvme_class = class_create(THIS_MODULE, "nvme");
+	 *   - drivers/nvme/host/core.c|5695| <<nvme_core_init>> if (IS_ERR(nvme_class)) {
+	 *   - drivers/nvme/host/core.c|5696| <<nvme_core_init>> result = PTR_ERR(nvme_class);
+	 *   - drivers/nvme/host/core.c|5699| <<nvme_core_init>> nvme_class->dev_uevent = nvme_class_uevent;
+	 *   - drivers/nvme/host/core.c|5709| <<nvme_core_init>> class_destroy(nvme_class);
+	 *   - drivers/nvme/host/core.c|5725| <<nvme_core_exit>> class_destroy(nvme_class);
+	 */
 	nvme_class = class_create(THIS_MODULE, "nvme");
 	if (IS_ERR(nvme_class)) {
 		result = PTR_ERR(nvme_class);
@@ -4270,6 +6229,14 @@ static int __init nvme_core_init(void)
 	}
 	nvme_class->dev_uevent = nvme_class_uevent;
 
+	/*
+	 * 在以下使用nvme_subsys_class:
+	 *   - drivers/nvme/host/core.c|3332| <<nvme_init_subsystem>> subsys->dev.class = nvme_subsys_class;
+	 *   - drivers/nvme/host/core.c|5418| <<nvme_core_init>> nvme_subsys_class = class_create(THIS_MODULE, "nvme-subsystem");
+	 *   - drivers/nvme/host/core.c|5419| <<nvme_core_init>> if (IS_ERR(nvme_subsys_class)) {
+	 *   - drivers/nvme/host/core.c|5420| <<nvme_core_init>> result = PTR_ERR(nvme_subsys_class);
+	 *   - drivers/nvme/host/core.c|5441| <<nvme_core_exit>> class_destroy(nvme_subsys_class);
+	 */
 	nvme_subsys_class = class_create(THIS_MODULE, "nvme-subsystem");
 	if (IS_ERR(nvme_subsys_class)) {
 		result = PTR_ERR(nvme_subsys_class);
@@ -4293,7 +6260,25 @@ static int __init nvme_core_init(void)
 
 static void __exit nvme_core_exit(void)
 {
+	/*
+	 * 在以下使用nvme_subsys_class:
+	 *   - drivers/nvme/host/core.c|3332| <<nvme_init_subsystem>> subsys->dev.class = nvme_subsys_class;
+	 *   - drivers/nvme/host/core.c|5418| <<nvme_core_init>> nvme_subsys_class = class_create(THIS_MODULE, "nvme-subsystem");
+	 *   - drivers/nvme/host/core.c|5419| <<nvme_core_init>> if (IS_ERR(nvme_subsys_class)) {
+	 *   - drivers/nvme/host/core.c|5420| <<nvme_core_init>> result = PTR_ERR(nvme_subsys_class);
+	 *   - drivers/nvme/host/core.c|5441| <<nvme_core_exit>> class_destroy(nvme_subsys_class);
+	 */
 	class_destroy(nvme_subsys_class);
+	/*
+	 * 在以下使用nvme_class:
+	 *   - drivers/nvme/host/core.c|5430| <<nvme_init_ctrl>> ctrl->device->class = nvme_class;
+	 *   - drivers/nvme/host/core.c|5694| <<nvme_core_init>> nvme_class = class_create(THIS_MODULE, "nvme");
+	 *   - drivers/nvme/host/core.c|5695| <<nvme_core_init>> if (IS_ERR(nvme_class)) {
+	 *   - drivers/nvme/host/core.c|5696| <<nvme_core_init>> result = PTR_ERR(nvme_class);
+	 *   - drivers/nvme/host/core.c|5699| <<nvme_core_init>> nvme_class->dev_uevent = nvme_class_uevent;
+	 *   - drivers/nvme/host/core.c|5709| <<nvme_core_init>> class_destroy(nvme_class);
+	 *   - drivers/nvme/host/core.c|5725| <<nvme_core_exit>> class_destroy(nvme_class);
+	 */
 	class_destroy(nvme_class);
 	unregister_chrdev_region(nvme_chr_devt, NVME_MINORS);
 	destroy_workqueue(nvme_delete_wq);
diff --git a/drivers/nvme/host/fabrics.c b/drivers/nvme/host/fabrics.c
index 74b8818ac9a1..12a2ec9fbbfe 100644
--- a/drivers/nvme/host/fabrics.c
+++ b/drivers/nvme/host/fabrics.c
@@ -13,18 +13,52 @@
 #include "nvme.h"
 #include "fabrics.h"
 
+/*
+ * 在以下使用nvmf_transports:
+ *   - drivers/nvme/host/fabrics.c|506| <<nvmf_register_transport>> list_add_tail(&ops->entry, &nvmf_transports);
+ *   - drivers/nvme/host/fabrics.c|537| <<nvmf_lookup_transport>> list_for_each_entry(ops, &nvmf_transports, entry) {
+ *
+ * 链接着struct nvmf_transport_ops
+ */
 static LIST_HEAD(nvmf_transports);
 static DECLARE_RWSEM(nvmf_transports_rwsem);
 
+/*
+ * 在以下使用nvmf_hosts:
+ *   - drivers/nvme/host/fabrics.c|28| <<__nvmf_host_find>> list_for_each_entry(host, &nvmf_hosts, list) {
+ *   - drivers/nvme/host/fabrics.c|54| <<nvmf_host_add>> list_add_tail(&host->list, &nvmf_hosts);
+ *   - drivers/nvme/host/fabrics.c|74| <<nvmf_host_default>> list_add_tail(&host->list, &nvmf_hosts);
+ *
+ * 链接着struct nvmf_host
+ */
 static LIST_HEAD(nvmf_hosts);
 static DEFINE_MUTEX(nvmf_hosts_mutex);
 
+/*
+ * 在以下使用nvmf_default_host:
+ *   - drivers/nvme/host/fabrics.c|898| <<nvmf_parse_options>> kref_get(&nvmf_default_host->ref);
+ *   - drivers/nvme/host/fabrics.c|899| <<nvmf_parse_options>> opts->host = nvmf_default_host;
+ *   - drivers/nvme/host/fabrics.c|1174| <<nvmf_init>> nvmf_default_host = nvmf_host_default();
+ *   - drivers/nvme/host/fabrics.c|1175| <<nvmf_init>> if (!nvmf_default_host)
+ *   - drivers/nvme/host/fabrics.c|1206| <<nvmf_init>> nvmf_host_put(nvmf_default_host);
+ *   - drivers/nvme/host/fabrics.c|1215| <<nvmf_exit>> nvmf_host_put(nvmf_default_host);
+ */
 static struct nvmf_host *nvmf_default_host;
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|41| <<nvmf_host_add>> host = __nvmf_host_find(hostnqn);
+ */
 static struct nvmf_host *__nvmf_host_find(const char *hostnqn)
 {
 	struct nvmf_host *host;
 
+	/*
+	 * 在以下使用nvmf_hosts:
+	 *   - drivers/nvme/host/fabrics.c|28| <<__nvmf_host_find>> list_for_each_entry(host, &nvmf_hosts, list) {
+	 *   - drivers/nvme/host/fabrics.c|54| <<nvmf_host_add>> list_add_tail(&host->list, &nvmf_hosts);
+	 *   - drivers/nvme/host/fabrics.c|74| <<nvmf_host_default>> list_add_tail(&host->list, &nvmf_hosts);
+	 */
 	list_for_each_entry(host, &nvmf_hosts, list) {
 		if (!strcmp(host->nqn, hostnqn))
 			return host;
@@ -33,6 +67,10 @@ static struct nvmf_host *__nvmf_host_find(const char *hostnqn)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|782| <<nvmf_parse_options>> opts->host = nvmf_host_add(p);
+ */
 static struct nvmf_host *nvmf_host_add(const char *hostnqn)
 {
 	struct nvmf_host *host;
@@ -48,15 +86,33 @@ static struct nvmf_host *nvmf_host_add(const char *hostnqn)
 	if (!host)
 		goto out_unlock;
 
+	/*
+	 * struct nvmf_host {
+	 *	struct kref             ref;
+	 *	struct list_head        list;
+	 *	char                    nqn[NVMF_NQN_SIZE];
+	 *	uuid_t                  id;
+	 * };
+	 */
 	kref_init(&host->ref);
 	strlcpy(host->nqn, hostnqn, NVMF_NQN_SIZE);
 
+	/*
+	 * 在以下使用nvmf_hosts:
+	 *   - drivers/nvme/host/fabrics.c|28| <<__nvmf_host_find>> list_for_each_entry(host, &nvmf_hosts, list) {
+	 *   - drivers/nvme/host/fabrics.c|54| <<nvmf_host_add>> list_add_tail(&host->list, &nvmf_hosts);
+	 *   - drivers/nvme/host/fabrics.c|74| <<nvmf_host_default>> list_add_tail(&host->list, &nvmf_hosts);
+	 */
 	list_add_tail(&host->list, &nvmf_hosts);
 out_unlock:
 	mutex_unlock(&nvmf_hosts_mutex);
 	return host;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1218| <<nvmf_init>> nvmf_default_host = nvmf_host_default();
+ */
 static struct nvmf_host *nvmf_host_default(void)
 {
 	struct nvmf_host *host;
@@ -71,12 +127,22 @@ static struct nvmf_host *nvmf_host_default(void)
 		"nqn.2014-08.org.nvmexpress:uuid:%pUb", &host->id);
 
 	mutex_lock(&nvmf_hosts_mutex);
+	/*
+	 * 在以下使用nvmf_hosts:
+	 *   - drivers/nvme/host/fabrics.c|28| <<__nvmf_host_find>> list_for_each_entry(host, &nvmf_hosts, list) {
+	 *   - drivers/nvme/host/fabrics.c|54| <<nvmf_host_add>> list_add_tail(&host->list, &nvmf_hosts);
+	 *   - drivers/nvme/host/fabrics.c|74| <<nvmf_host_default>> list_add_tail(&host->list, &nvmf_hosts);
+	 */
 	list_add_tail(&host->list, &nvmf_hosts);
 	mutex_unlock(&nvmf_hosts_mutex);
 
 	return host;
 }
 
+/*
+ * 在以下使用nvmf_host_destroy():
+ *   - drivers/nvme/host/fabrics.c|152| <<nvmf_host_put>> kref_put(&host->ref, nvmf_host_destroy);
+ */
 static void nvmf_host_destroy(struct kref *ref)
 {
 	struct nvmf_host *host = container_of(ref, struct nvmf_host, ref);
@@ -88,6 +154,13 @@ static void nvmf_host_destroy(struct kref *ref)
 	kfree(host);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|882| <<nvmf_parse_options>> nvmf_host_put(opts->host);
+ *   - drivers/nvme/host/fabrics.c|1104| <<nvmf_free_options>> nvmf_host_put(opts->host);
+ *   - drivers/nvme/host/fabrics.c|1344| <<nvmf_init>> nvmf_host_put(nvmf_default_host);
+ *   - drivers/nvme/host/fabrics.c|1353| <<nvmf_exit>> nvmf_host_put(nvmf_default_host);
+ */
 static void nvmf_host_put(struct nvmf_host *host)
 {
 	if (host)
@@ -100,6 +173,9 @@ static void nvmf_host_put(struct nvmf_host *host)
  * @buf:	OUTPUT parameter that will contain the address/port
  * @size:	buffer size
  */
+/*
+ * struct nvme_ctrl_ops nvme_loop_ctrl_ops.get_address = nvmf_get_address()
+ */
 int nvmf_get_address(struct nvme_ctrl *ctrl, char *buf, int size)
 {
 	int len = 0;
@@ -268,6 +344,11 @@ EXPORT_SYMBOL_GPL(nvmf_reg_write32);
  *
  * @data: This is the "Data" portion of a submission capsule.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|472| <<nvmf_connect_admin_queue>> nvmf_log_connect_error(ctrl, ret, le32_to_cpu(res.u32),
+ *   - drivers/nvme/host/fabrics.c|542| <<nvmf_connect_io_queue>> nvmf_log_connect_error(ctrl, ret, le32_to_cpu(res.u32),
+ */
 static void nvmf_log_connect_error(struct nvme_ctrl *ctrl,
 		int errval, int offset, struct nvme_command *cmd,
 		struct nvmf_connect_data *data)
@@ -364,6 +445,13 @@ static void nvmf_log_connect_error(struct nvme_ctrl *ctrl,
  *	< 0: Linux errno error code
  *
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2733| <<nvme_fc_create_association>> ret = nvmf_connect_admin_queue(&ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|618| <<nvme_rdma_start_queue>> ret = nvmf_connect_admin_queue(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1447| <<nvme_tcp_start_queue>> ret = nvmf_connect_admin_queue(nctrl);
+ *   - drivers/nvme/target/loop.c|581| <<nvme_loop_configure_admin_queue>> error = nvmf_connect_admin_queue(&ctrl->ctrl);
+ */
 int nvmf_connect_admin_queue(struct nvme_ctrl *ctrl)
 {
 	struct nvme_command cmd;
@@ -434,6 +522,13 @@ EXPORT_SYMBOL_GPL(nvmf_connect_admin_queue);
  *	> 0: NVMe error status code
  *	< 0: Linux errno error code
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2069| <<nvme_fc_connect_io_queues>> ret = nvmf_connect_io_queue(&ctrl->ctrl, i, false);
+ *   - drivers/nvme/host/rdma.c|616| <<nvme_rdma_start_queue>> ret = nvmf_connect_io_queue(&ctrl->ctrl, idx, poll);
+ *   - drivers/nvme/host/tcp.c|1445| <<nvme_tcp_start_queue>> ret = nvmf_connect_io_queue(nctrl, idx, false);
+ *   - drivers/nvme/target/loop.c|507| <<nvme_loop_connect_io_queues>> ret = nvmf_connect_io_queue(&ctrl->ctrl, i, false);
+ */
 int nvmf_connect_io_queue(struct nvme_ctrl *ctrl, u16 qid, bool poll)
 {
 	struct nvme_command cmd;
@@ -490,12 +585,24 @@ EXPORT_SYMBOL_GPL(nvmf_should_reconnect);
  * being implemented to the common NVMe fabrics library. Part of
  * the overall init sequence of starting up a fabrics driver.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3644| <<nvme_fc_init_module>> ret = nvmf_register_transport(&nvme_fc_transport);
+ *   - drivers/nvme/host/rdma.c|2121| <<nvme_rdma_init_module>> ret = nvmf_register_transport(&nvme_rdma_transport);
+ *   - drivers/nvme/host/tcp.c|2403| <<nvme_tcp_init_module>> nvmf_register_transport(&nvme_tcp_transport);
+ *   - drivers/nvme/target/loop.c|1110| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+ */
 int nvmf_register_transport(struct nvmf_transport_ops *ops)
 {
 	if (!ops->create_ctrl)
 		return -EINVAL;
 
 	down_write(&nvmf_transports_rwsem);
+	/*
+	 * 在以下使用nvmf_transports:
+	 *   - drivers/nvme/host/fabrics.c|506| <<nvmf_register_transport>> list_add_tail(&ops->entry, &nvmf_transports);
+	 *   - drivers/nvme/host/fabrics.c|537| <<nvmf_lookup_transport>> list_for_each_entry(ops, &nvmf_transports, entry) {
+	 */
 	list_add_tail(&ops->entry, &nvmf_transports);
 	up_write(&nvmf_transports_rwsem);
 
@@ -520,6 +627,10 @@ void nvmf_unregister_transport(struct nvmf_transport_ops *ops)
 }
 EXPORT_SYMBOL_GPL(nvmf_unregister_transport);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1027| <<nvmf_create_ctrl>> ops = nvmf_lookup_transport(opts);
+ */
 static struct nvmf_transport_ops *nvmf_lookup_transport(
 		struct nvmf_ctrl_options *opts)
 {
@@ -544,6 +655,13 @@ static struct nvmf_transport_ops *nvmf_lookup_transport(
  * Note: commands used to initialize the controller will be marked for failfast.
  * Note: nvme cli/ioctl commands are marked for failfast.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2414| <<nvme_fc_queue_rq>> return nvmf_fail_nonready_command(&queue->ctrl->ctrl, rq);
+ *   - drivers/nvme/host/rdma.c|1745| <<nvme_rdma_queue_rq>> return nvmf_fail_nonready_command(&queue->ctrl->ctrl, rq);
+ *   - drivers/nvme/host/tcp.c|2165| <<nvme_tcp_queue_rq>> return nvmf_fail_nonready_command(&queue->ctrl->ctrl, rq);
+ *   - drivers/nvme/target/loop.c|229| <<nvme_loop_queue_rq>> return nvmf_fail_nonready_command(&queue->ctrl->ctrl, req);
+ */
 blk_status_t nvmf_fail_nonready_command(struct nvme_ctrl *ctrl,
 		struct request *rq)
 {
@@ -559,6 +677,10 @@ blk_status_t nvmf_fail_nonready_command(struct nvme_ctrl *ctrl,
 }
 EXPORT_SYMBOL_GPL(nvmf_fail_nonready_command);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.h|187| <<nvmf_check_ready>> return __nvmf_check_ready(ctrl, rq, queue_live);
+ */
 bool __nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
 		bool queue_live)
 {
@@ -592,6 +714,18 @@ bool __nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
 }
 EXPORT_SYMBOL_GPL(__nvmf_check_ready);
 
+/*
+ * 在以下使用opt_tokens:
+ *   - drivers/nvme/host/fabrics.c|729| <<nvmf_parse_options>> token = match_token(p, opt_tokens, args);
+ *   - drivers/nvme/host/fabrics.c|989| <<nvmf_check_required_opts>> for (i = 0; i < ARRAY_SIZE(opt_tokens); i++) {
+ *   - drivers/nvme/host/fabrics.c|990| <<nvmf_check_required_opts>> if ((opt_tokens[i].token & required_opts) &&
+ *   - drivers/nvme/host/fabrics.c|991| <<nvmf_check_required_opts>> !(opt_tokens[i].token & opts->mask)) {
+ *   - drivers/nvme/host/fabrics.c|993| <<nvmf_check_required_opts>> opt_tokens[i].pattern);
+ *   - drivers/nvme/host/fabrics.c|1039| <<nvmf_check_allowed_opts>> for (i = 0; i < ARRAY_SIZE(opt_tokens); i++) {
+ *   - drivers/nvme/host/fabrics.c|1040| <<nvmf_check_allowed_opts>> if ((opt_tokens[i].token & opts->mask) &&
+ *   - drivers/nvme/host/fabrics.c|1041| <<nvmf_check_allowed_opts>> (opt_tokens[i].token & ~allowed_opts)) {
+ *   - drivers/nvme/host/fabrics.c|1043| <<nvmf_check_allowed_opts>> opt_tokens[i].pattern);
+ */
 static const match_table_t opt_tokens = {
 	{ NVMF_OPT_TRANSPORT,		"transport=%s"		},
 	{ NVMF_OPT_TRADDR,		"traddr=%s"		},
@@ -615,6 +749,10 @@ static const match_table_t opt_tokens = {
 	{ NVMF_OPT_ERR,			NULL			}
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1095| <<nvmf_create_ctrl>> ret = nvmf_parse_options(opts, buf);
+ */
 static int nvmf_parse_options(struct nvmf_ctrl_options *opts,
 		const char *buf)
 {
@@ -899,9 +1037,17 @@ static int nvmf_parse_options(struct nvmf_ctrl_options *opts,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1107| <<nvmf_create_ctrl>> ret = nvmf_check_required_opts(opts, NVMF_REQUIRED_OPTS);
+ *   - drivers/nvme/host/fabrics.c|1127| <<nvmf_create_ctrl>> ret = nvmf_check_required_opts(opts, ops->required_opts);
+ */
 static int nvmf_check_required_opts(struct nvmf_ctrl_options *opts,
 		unsigned int required_opts)
 {
+	/*
+	 * 期待if语句里的内容被skip
+	 */
 	if ((opts->mask & required_opts) != required_opts) {
 		int i;
 
@@ -919,6 +1065,11 @@ static int nvmf_check_required_opts(struct nvmf_ctrl_options *opts,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1960| <<nvme_rdma_existing_controller>> found = nvmf_ip_options_match(&ctrl->ctrl, opts);
+ *   - drivers/nvme/host/tcp.c|2275| <<nvme_tcp_existing_controller>> found = nvmf_ip_options_match(&ctrl->ctrl, opts);
+ */
 bool nvmf_ip_options_match(struct nvme_ctrl *ctrl,
 		struct nvmf_ctrl_options *opts)
 {
@@ -949,6 +1100,10 @@ bool nvmf_ip_options_match(struct nvme_ctrl *ctrl,
 }
 EXPORT_SYMBOL_GPL(nvmf_ip_options_match);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1130| <<nvmf_create_ctrl>> ret = nvmf_check_allowed_opts(opts, NVMF_ALLOWED_OPTS |
+ */
 static int nvmf_check_allowed_opts(struct nvmf_ctrl_options *opts,
 		unsigned int allowed_opts)
 {
@@ -969,6 +1124,14 @@ static int nvmf_check_allowed_opts(struct nvmf_ctrl_options *opts,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1153| <<nvmf_create_ctrl>> nvmf_free_options(opts);
+ *   - drivers/nvme/host/fc.c|2118| <<nvme_fc_ctrl_free>> nvmf_free_options(ctrl->ctrl.opts);
+ *   - drivers/nvme/host/rdma.c|956| <<nvme_rdma_free_ctrl>> nvmf_free_options(nctrl->opts);
+ *   - drivers/nvme/host/tcp.c|1983| <<nvme_tcp_free_ctrl>> nvmf_free_options(nctrl->opts);
+ *   - drivers/nvme/target/loop.c|435| <<nvme_loop_free_ctrl>> nvmf_free_options(nctrl->opts);
+ */
 void nvmf_free_options(struct nvmf_ctrl_options *opts)
 {
 	nvmf_host_put(opts->host);
@@ -981,12 +1144,32 @@ void nvmf_free_options(struct nvmf_ctrl_options *opts)
 }
 EXPORT_SYMBOL_GPL(nvmf_free_options);
 
+/*
+ * 在以下使用NVMF_REQUIRED_OPTS:
+ *   - drivers/nvme/host/fabrics.c|1186| <<nvmf_create_ctrl>> ret = nvmf_check_required_opts(opts, NVMF_REQUIRED_OPTS);
+ *   - drivers/nvme/host/fabrics.c|1189| <<nvmf_create_ctrl>> opts->mask &= ~NVMF_REQUIRED_OPTS;
+ */
 #define NVMF_REQUIRED_OPTS	(NVMF_OPT_TRANSPORT | NVMF_OPT_NQN)
+/*
+ * 在以下使用NVMF_ALLOWED_OPTS:
+ *   - drivers/nvme/host/fabrics.c|1228| <<nvmf_create_ctrl>> ret = nvmf_check_allowed_opts(opts, NVMF_ALLOWED_OPTS |
+ */
 #define NVMF_ALLOWED_OPTS	(NVMF_OPT_QUEUE_SIZE | NVMF_OPT_NR_IO_QUEUES | \
 				 NVMF_OPT_KATO | NVMF_OPT_HOSTNQN | \
 				 NVMF_OPT_HOST_ID | NVMF_OPT_DUP_CONNECT |\
 				 NVMF_OPT_DISABLE_SQFLOW)
 
+/*
+ * [0] nvmf_create_ctrl
+ * [0] nvmf_dev_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1270| <<nvmf_dev_write>> ctrl = nvmf_create_ctrl(nvmf_device, buf);
+ */
 static struct nvme_ctrl *
 nvmf_create_ctrl(struct device *dev, const char *buf)
 {
@@ -999,6 +1182,10 @@ nvmf_create_ctrl(struct device *dev, const char *buf)
 	if (!opts)
 		return ERR_PTR(-ENOMEM);
 
+	/*
+	 * 参数buf的一个例子:
+	 * transport=loop,hostnqn=hostnqn,nqn=testnqn,traddr=10.0.2.15
+	 */
 	ret = nvmf_parse_options(opts, buf);
 	if (ret)
 		goto out_free_opts;
@@ -1011,12 +1198,20 @@ nvmf_create_ctrl(struct device *dev, const char *buf)
 	 * the lookup below.  Then clear the generic flags so that transport
 	 * drivers don't have to care about them.
 	 */
+	/*
+	 * 在以下使用NVMF_REQUIRED_OPTS:
+	 *   - drivers/nvme/host/fabrics.c|1186| <<nvmf_create_ctrl>> ret = nvmf_check_required_opts(opts, NVMF_REQUIRED_OPTS);
+	 *   - drivers/nvme/host/fabrics.c|1189| <<nvmf_create_ctrl>> opts->mask &= ~NVMF_REQUIRED_OPTS;
+	 */
 	ret = nvmf_check_required_opts(opts, NVMF_REQUIRED_OPTS);
 	if (ret)
 		goto out_free_opts;
 	opts->mask &= ~NVMF_REQUIRED_OPTS;
 
 	down_read(&nvmf_transports_rwsem);
+	/*
+	 * 只在这里调用
+	 */
 	ops = nvmf_lookup_transport(opts);
 	if (!ops) {
 		pr_info("no handler found for transport %s.\n",
@@ -1034,11 +1229,17 @@ nvmf_create_ctrl(struct device *dev, const char *buf)
 	ret = nvmf_check_required_opts(opts, ops->required_opts);
 	if (ret)
 		goto out_module_put;
+	/*
+	 * 只在此处使用NVMF_ALLOWED_OPTS
+	 */
 	ret = nvmf_check_allowed_opts(opts, NVMF_ALLOWED_OPTS |
 				ops->allowed_opts | ops->required_opts);
 	if (ret)
 		goto out_module_put;
 
+	/*
+	 * nvme_loop_create_ctrl()
+	 */
 	ctrl = ops->create_ctrl(dev, opts);
 	if (IS_ERR(ctrl)) {
 		ret = PTR_ERR(ctrl);
@@ -1054,6 +1255,14 @@ nvmf_create_ctrl(struct device *dev, const char *buf)
 out_unlock:
 	up_read(&nvmf_transports_rwsem);
 out_free_opts:
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/fabrics.c|1153| <<nvmf_create_ctrl>> nvmf_free_options(opts);
+	 *   - drivers/nvme/host/fc.c|2118| <<nvme_fc_ctrl_free>> nvmf_free_options(ctrl->ctrl.opts);
+	 *   - drivers/nvme/host/rdma.c|956| <<nvme_rdma_free_ctrl>> nvmf_free_options(nctrl->opts);
+	 *   - drivers/nvme/host/tcp.c|1983| <<nvme_tcp_free_ctrl>> nvmf_free_options(nctrl->opts);
+	 *   - drivers/nvme/target/loop.c|435| <<nvme_loop_free_ctrl>> nvmf_free_options(nctrl->opts);
+	 */
 	nvmf_free_options(opts);
 	return ERR_PTR(ret);
 }
@@ -1062,6 +1271,15 @@ static struct class *nvmf_class;
 static struct device *nvmf_device;
 static DEFINE_MUTEX(nvmf_dev_mutex);
 
+/*
+ * [0] nvme_init_ctrl
+ * [0] nvme_loop_create_ctrl
+ * [0] nvmf_dev_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static ssize_t nvmf_dev_write(struct file *file, const char __user *ubuf,
 		size_t count, loff_t *pos)
 {
@@ -1073,6 +1291,9 @@ static ssize_t nvmf_dev_write(struct file *file, const char __user *ubuf,
 	if (count > PAGE_SIZE)
 		return -ENOMEM;
 
+	/*
+	 * duplicate memory region from user space and NUL-terminate
+	 */
 	buf = memdup_user_nul(ubuf, count);
 	if (IS_ERR(buf))
 		return PTR_ERR(buf);
@@ -1083,6 +1304,9 @@ static ssize_t nvmf_dev_write(struct file *file, const char __user *ubuf,
 		goto out_unlock;
 	}
 
+	/*
+	 * 只在这里调用
+	 */
 	ctrl = nvmf_create_ctrl(nvmf_device, buf);
 	if (IS_ERR(ctrl)) {
 		ret = PTR_ERR(ctrl);
diff --git a/drivers/nvme/host/fabrics.h b/drivers/nvme/host/fabrics.h
index a0ec40ab62ee..688544355968 100644
--- a/drivers/nvme/host/fabrics.h
+++ b/drivers/nvme/host/fabrics.h
@@ -179,6 +179,16 @@ bool __nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
 bool nvmf_ip_options_match(struct nvme_ctrl *ctrl,
 		struct nvmf_ctrl_options *opts);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2413| <<nvme_fc_queue_rq>> !nvmf_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+ *   - drivers/nvme/host/rdma.c|1744| <<nvme_rdma_queue_rq>> if (!nvmf_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+ *   - drivers/nvme/host/tcp.c|2164| <<nvme_tcp_queue_rq>> if (!nvmf_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+ *   - drivers/nvme/target/loop.c|228| <<nvme_loop_queue_rq>> if (!nvmf_check_ready(&queue->ctrl->ctrl, req, queue_ready))
+ *
+ * 只要状态是NVME_CTRL_LIVE就一定返回true
+ * 否则返回__nvmf_check_ready(ctrl, rq, queue_live)
+ */
 static inline bool nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
 		bool queue_live)
 {
diff --git a/drivers/nvme/host/fault_inject.c b/drivers/nvme/host/fault_inject.c
index 1352159733b0..7626c13c89dc 100644
--- a/drivers/nvme/host/fault_inject.c
+++ b/drivers/nvme/host/fault_inject.c
@@ -8,13 +8,52 @@
 #include <linux/moduleparam.h>
 #include "nvme.h"
 
+/*
+ * 在以下使用fail_default_attr:
+ *   - drivers/nvme/host/fault_inject.c|60| <<nvme_fault_inject_init>> setup_fault_attr(&fail_default_attr, fail_request);
+ *   - drivers/nvme/host/fault_inject.c|69| <<nvme_fault_inject_init>> *attr = fail_default_attr;
+ */
 static DECLARE_FAULT_ATTR(fail_default_attr);
 /* optional fault injection attributes boot time option:
  * nvme_core.fail_request=<interval>,<probability>,<space>,<times>
  */
+/*
+ * "nvme_core.fail_request=20,100,0,5":
+ * [    3.132027] FAULT_INJECTION: forcing a failure.
+ *                name fault_inject, interval 20, probability 100, space 0, times 3
+ * [    3.132029] CPU: 0 PID: 247 Comm: systemd-udevd Not tainted 5.5.0+ #3
+ * [    3.132030] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-59-gc9ba5276e321-prebuilt.qemu.org 04/01/2014
+ * [    3.132030] Call Trace:
+ * [    3.132032]  <IRQ>
+ * [    3.132036]  dump_stack+0x50/0x6b
+ * [    3.132038]  should_fail+0x13c/0x160
+ * [    3.132040]  nvme_should_fail+0x30/0xa0
+ * [    3.132042]  nvme_irq+0x136/0x210
+ * [    3.132044]  __handle_irq_event_percpu+0x3b/0x180
+ * [    3.132045]  handle_irq_event_percpu+0x2b/0x70
+ * [    3.132046]  handle_irq_event+0x22/0x40
+ * [    3.132047]  handle_edge_irq+0x75/0x190
+ * [    3.132049]  do_IRQ+0x41/0xd0
+ * [    3.132064]  common_interrupt+0xf/0xf
+ * [    3.132064]  </IRQ>
+ * [    3.132065] RIP: 0033:0x557263ceebe0
+ */
 static char *fail_request;
 module_param(fail_request, charp, 0000);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3665| <<nvme_alloc_ns>> nvme_fault_inject_init(&ns->fault_inject, ns->disk->disk_name);
+ *   - drivers/nvme/host/core.c|4194| <<nvme_init_ctrl>> nvme_fault_inject_init(&ctrl->fault_inject, dev_name(ctrl->device));
+ *
+ * controller和namespace各自有fault_injection
+ *
+ * # ls /sys/kernel/debug/nvme0/fault_inject/
+ * dont_retry  interval  probability  space  status  task-filter  times  verbose  verbose_ratelimit_burst  verbose_ratelimit_interval_ms
+ *
+ * # ls /sys/kernel/debug/nvme0n1/fault_inject/
+ * dont_retry  interval  probability  space  status  task-filter  times  verbose  verbose_ratelimit_burst  verbose_ratelimit_interval_ms
+ */
 void nvme_fault_inject_init(struct nvme_fault_inject *fault_inj,
 			    const char *dev_name)
 {
@@ -48,18 +87,39 @@ void nvme_fault_inject_init(struct nvme_fault_inject *fault_inj,
 	debugfs_create_bool("dont_retry", 0600, dir, &fault_inj->dont_retry);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3692| <<nvme_ns_remove>> nvme_fault_inject_fini(&ns->fault_inject);
+ *   - drivers/nvme/host/core.c|4095| <<nvme_uninit_ctrl>> nvme_fault_inject_fini(&ctrl->fault_inject);
+ */
 void nvme_fault_inject_fini(struct nvme_fault_inject *fault_inject)
 {
 	/* remove debugfs directories */
 	debugfs_remove_recursive(fault_inject->parent);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/nvme.h|481| <<nvme_end_request>> nvme_should_fail(req);
+ */
 void nvme_should_fail(struct request *req)
 {
+	/*
+	 * 主要设置rq_disk的地方:
+	 *   - block/blk-exec.c|55| <<blk_execute_rq_nowait>> rq->rq_disk = bd_disk;
+	 *   - block/blk-flush.c|636| <<blk_kick_flush>> flush_rq->rq_disk = first_rq->rq_disk;
+	 *   - block/blk-mq.c|323| <<blk_mq_rq_ctx_init>> rq->rq_disk = NULL;
+	 *   - block/blk.h|191| <<blk_rq_bio_prep>> rq->rq_disk = bio->bi_disk;
+	 */
 	struct gendisk *disk = req->rq_disk;
 	struct nvme_fault_inject *fault_inject = NULL;
 	u16 status;
 
+	/*
+	 * 正如nvme_fault_inject_init()注释的
+	 * controller和namespace各自有fault_injection
+	 * 似乎如果没有disk说明是admin queue
+	 */
 	if (disk) {
 		struct nvme_ns *ns = disk->private_data;
 
diff --git a/drivers/nvme/host/fc.c b/drivers/nvme/host/fc.c
index 5a70ac395d53..47b68e762ff7 100644
--- a/drivers/nvme/host/fc.c
+++ b/drivers/nvme/host/fc.c
@@ -104,10 +104,27 @@ struct nvme_fc_lport {
 
 	struct ida			endp_cnt;
 	struct list_head		port_list;	/* nvme_fc_port_list */
+	/*
+	 * 在以下使用nvme_fc_lport->endp_list:
+	 *   - drivers/nvme/host/fc.c|240| <<nvme_fc_free_lport>> WARN_ON(!list_empty(&lport->endp_list));
+	 *   - drivers/nvme/host/fc.c|408| <<nvme_fc_register_localport>> INIT_LIST_HEAD(&newrec->endp_list);
+	 *   - drivers/nvme/host/fc.c|599| <<nvme_fc_attach_to_suspended_rport>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+	 *   - drivers/nvme/host/fc.c|758| <<nvme_fc_register_remoteport>> list_add_tail(&newrec->endp_list, &lport->endp_list);
+	 *   - drivers/nvme/host/fc.c|3371| <<nvme_fc_create_ctrl>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+	 *   - drivers/nvme/host/fc.c|3419| <<nvme_fc_nvme_discovery_store>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+	 *   - drivers/nvme/host/fc.c|3565| <<nvme_fc_cleanup_for_unload>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+	 */
 	struct list_head		endp_list;
 	struct device			*dev;	/* physical device for dma */
 	struct nvme_fc_port_template	*ops;
 	struct kref			ref;
+	/*
+	 * 在以下使用nvme_fc_lport->act_rport_cnt:
+	 *   - drivers/nvme/host/fc.c|420| <<nvme_fc_register_localport>> atomic_set(&newrec->act_rport_cnt, 0);
+	 *   - drivers/nvme/host/fc.c|492| <<nvme_fc_unregister_localport>> if (atomic_read(&lport->act_rport_cnt) == 0)
+	 *   - drivers/nvme/host/fc.c|2612| <<nvme_fc_rport_active_on_lport>> atomic_inc(&lport->act_rport_cnt);
+	 *   - drivers/nvme/host/fc.c|2621| <<nvme_fc_rport_inactive_on_lport>> cnt = atomic_dec_return(&lport->act_rport_cnt);
+	 */
 	atomic_t                        act_rport_cnt;
 } __aligned(sizeof(u64));	/* alignment for other things alloc'd with */
 
@@ -198,6 +215,16 @@ fcp_req_to_fcp_op(struct nvmefc_fcp_req *fcpreq)
 
 static DEFINE_SPINLOCK(nvme_fc_lock);
 
+/*
+ * 在以下使用nvme_fc_lport_list:
+ *   - drivers/nvme/host/fc.c|235| <<nvme_fc_free_lport>> if (nvme_fc_waiting_to_unload && list_empty(&nvme_fc_lport_list))
+ *   - drivers/nvme/host/fc.c|270| <<nvme_fc_attach_to_unreg_lport>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+ *   - drivers/nvme/host/fc.c|413| <<nvme_fc_register_localport>> list_add_tail(&newrec->port_list, &nvme_fc_lport_list);
+ *   - drivers/nvme/host/fc.c|3346| <<nvme_fc_create_ctrl>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+ *   - drivers/nvme/host/fc.c|3398| <<nvme_fc_nvme_discovery_store>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+ *   - drivers/nvme/host/fc.c|3544| <<nvme_fc_cleanup_for_unload>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+ *   - drivers/nvme/host/fc.c|3558| <<nvme_fc_exit_module>> if (!list_empty(&nvme_fc_lport_list)) {
+ */
 static LIST_HEAD(nvme_fc_lport_list);
 static DEFINE_IDA(nvme_fc_local_port_cnt);
 static DEFINE_IDA(nvme_fc_ctrl_cnt);
@@ -328,6 +355,12 @@ nvme_fc_attach_to_unreg_lport(struct nvme_fc_port_info *pinfo,
  * a completion status. Must be 0 upon success; a negative errno
  * (ex: -ENXIO) upon failure.
  */
+/*
+ * called by:
+ *   - drivers/nvme/target/fcloop.c|936| <<fcloop_create_local_port>> ret = nvme_fc_register_localport(&pinfo, &fctemplate, NULL, &localport);
+ *   - drivers/scsi/lpfc/lpfc_nvme.c|2165| <<lpfc_nvme_create_localport>> ret = nvme_fc_register_localport(&nfcp_info, &lpfc_nvme_template,
+ *   - drivers/scsi/qla2xxx/qla_nvme.c|706| <<qla_nvme_register_hba>> ret = nvme_fc_register_localport(&pinfo, tmpl,
+ */
 int
 nvme_fc_register_localport(struct nvme_fc_port_info *pinfo,
 			struct nvme_fc_port_template *template,
@@ -404,12 +437,25 @@ nvme_fc_register_localport(struct nvme_fc_port_info *pinfo,
 	newrec->localport.port_num = idx;
 
 	spin_lock_irqsave(&nvme_fc_lock, flags);
+	/*
+	 * 在以下使用nvme_fc_lport_list:
+	 *   - drivers/nvme/host/fc.c|235| <<nvme_fc_free_lport>> if (nvme_fc_waiting_to_unload && list_empty(&nvme_fc_lport_list))
+	 *   - drivers/nvme/host/fc.c|270| <<nvme_fc_attach_to_unreg_lport>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+	 *   - drivers/nvme/host/fc.c|413| <<nvme_fc_register_localport>> list_add_tail(&newrec->port_list, &nvme_fc_lport_list);
+	 *   - drivers/nvme/host/fc.c|3346| <<nvme_fc_create_ctrl>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+	 *   - drivers/nvme/host/fc.c|3398| <<nvme_fc_nvme_discovery_store>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+	 *   - drivers/nvme/host/fc.c|3544| <<nvme_fc_cleanup_for_unload>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+	 *   - drivers/nvme/host/fc.c|3558| <<nvme_fc_exit_module>> if (!list_empty(&nvme_fc_lport_list)) {
+	 */
 	list_add_tail(&newrec->port_list, &nvme_fc_lport_list);
 	spin_unlock_irqrestore(&nvme_fc_lock, flags);
 
 	if (dev)
 		dma_set_seg_boundary(dev, template->dma_boundary);
 
+	/*
+	 * 函数的最后一个参数: struct nvme_fc_local_port **portptr
+	 */
 	*portptr = &newrec->localport;
 	return 0;
 
@@ -472,6 +518,13 @@ EXPORT_SYMBOL_GPL(nvme_fc_unregister_localport);
  */
 #define FCNVME_TRADDR_LENGTH		64
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|726| <<nvme_fc_register_remoteport>> nvme_fc_signal_discovery_scan(lport, newrec);
+ *   - drivers/nvme/host/fc.c|769| <<nvme_fc_register_remoteport>> nvme_fc_signal_discovery_scan(lport, newrec);
+ *   - drivers/nvme/host/fc.c|932| <<nvme_fc_rescan_remoteport>> nvme_fc_signal_discovery_scan(rport->lport, rport);
+ *   - drivers/nvme/host/fc.c|3497| <<nvme_fc_nvme_discovery_store>> nvme_fc_signal_discovery_scan(lport, rport);
+ */
 static void
 nvme_fc_signal_discovery_scan(struct nvme_fc_lport *lport,
 		struct nvme_fc_rport *rport)
@@ -480,6 +533,11 @@ nvme_fc_signal_discovery_scan(struct nvme_fc_lport *lport,
 	char tgtaddr[FCNVME_TRADDR_LENGTH];	/* NVMEFC_TRADDR=...*/
 	char *envp[4] = { "FC_EVENT=nvmediscovery", hostaddr, tgtaddr, NULL };
 
+	/*
+	 * struct nvme_fc_rport:
+	 *   - struct nvme_fc_remote_port remoteport;
+	 *       - u32 port_role
+	 */
 	if (!(rport->remoteport.port_role & FC_PORT_ROLE_NVME_DISCOVERY))
 		return;
 
@@ -643,6 +701,12 @@ __nvme_fc_set_dev_loss_tmo(struct nvme_fc_rport *rport,
  * a completion status. Must be 0 upon success; a negative errno
  * (ex: -ENXIO) upon failure.
  */
+/*
+ * called by:
+ *   - drivers/nvme/target/fcloop.c|1131| <<fcloop_create_remote_port>> ret = nvme_fc_register_remoteport(nport->lport->localport,
+ *   - drivers/scsi/lpfc/lpfc_nvme.c|2400| <<lpfc_nvme_register_port>> ret = nvme_fc_register_remoteport(localport, &rpinfo, &remote_port);
+ *   - drivers/scsi/qla2xxx/qla_nvme.c|63| <<qla_nvme_register_remote>> ret = nvme_fc_register_remoteport(vha->nvme_local_port, &req,
+ */
 int
 nvme_fc_register_remoteport(struct nvme_fc_local_port *localport,
 				struct nvme_fc_port_info *pinfo,
@@ -714,6 +778,16 @@ nvme_fc_register_remoteport(struct nvme_fc_local_port *localport,
 	__nvme_fc_set_dev_loss_tmo(newrec, pinfo);
 
 	spin_lock_irqsave(&nvme_fc_lock, flags);
+	/*
+	 * 在以下使用nvme_fc_lport->endp_list:
+	 *   - drivers/nvme/host/fc.c|240| <<nvme_fc_free_lport>> WARN_ON(!list_empty(&lport->endp_list));
+	 *   - drivers/nvme/host/fc.c|408| <<nvme_fc_register_localport>> INIT_LIST_HEAD(&newrec->endp_list);
+	 *   - drivers/nvme/host/fc.c|599| <<nvme_fc_attach_to_suspended_rport>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+	 *   - drivers/nvme/host/fc.c|758| <<nvme_fc_register_remoteport>> list_add_tail(&newrec->endp_list, &lport->endp_list);
+	 *   - drivers/nvme/host/fc.c|3371| <<nvme_fc_create_ctrl>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+	 *   - drivers/nvme/host/fc.c|3419| <<nvme_fc_nvme_discovery_store>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+	 *   - drivers/nvme/host/fc.c|3565| <<nvme_fc_cleanup_for_unload>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+	 */
 	list_add_tail(&newrec->endp_list, &lport->endp_list);
 	spin_unlock_irqrestore(&nvme_fc_lock, flags);
 
@@ -3048,6 +3122,10 @@ nvme_fc_existing_controller(struct nvme_fc_rport *rport,
 	return found;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3453| <<nvme_fc_create_ctrl>> ctrl = nvme_fc_init_ctrl(dev, opts, lport, rport);
+ */
 static struct nvme_ctrl *
 nvme_fc_init_ctrl(struct device *dev, struct nvmf_ctrl_options *opts,
 	struct nvme_fc_lport *lport, struct nvme_fc_rport *rport)
@@ -3250,6 +3328,11 @@ struct nvmet_fc_traddr {
 	u64	pn;
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3392| <<nvme_fc_parse_traddr>> if (__nvme_fc_parse_u64(&wwn, &traddr->nn))
+ *   - drivers/nvme/host/fc.c|3396| <<nvme_fc_parse_traddr>> if (__nvme_fc_parse_u64(&wwn, &traddr->pn))
+ */
 static int
 __nvme_fc_parse_u64(substring_t *sstr, u64 *val)
 {
@@ -3267,18 +3350,52 @@ __nvme_fc_parse_u64(substring_t *sstr, u64 *val)
  * As kernel parsers need the 0x to determine number base, universally
  * build string to parse with 0x prefix before parsing name strings.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3393| <<nvme_fc_create_ctrl>> ret = nvme_fc_parse_traddr(&raddr, opts->traddr, NVMF_TRADDR_SIZE);
+ *   - drivers/nvme/host/fc.c|3397| <<nvme_fc_create_ctrl>> ret = nvme_fc_parse_traddr(&laddr, opts->host_traddr, NVMF_TRADDR_SIZE);
+ */
 static int
 nvme_fc_parse_traddr(struct nvmet_fc_traddr *traddr, char *buf, size_t blen)
 {
+	/*
+	 * #define NVME_FC_TRADDR_HEXNAMELEN 16
+	 *
+	 * 2 + 16 + 1 = 19
+	 */
 	char name[2 + NVME_FC_TRADDR_HEXNAMELEN + 1];
+	/*
+	 * typedef struct {
+	 *	char *from;
+	 *	char *to;
+	 * } substring_t;
+	 */
 	substring_t wwn = { name, &name[sizeof(name)-1] };
 	int nnoffset, pnoffset;
 
+	/*
+	 * strnlen():
+	 * Find the length of a length-limited string
+	 */
+
 	/* validate if string is one of the 2 allowed formats */
+	/*
+	 * strnlen - Find the length of a length-limited string
+	 * @s: The string to be sized
+	 * @count: The maximum number of bytes to search
+	 */
 	if (strnlen(buf, blen) == NVME_FC_TRADDR_MAXLENGTH &&
 			!strncmp(buf, "nn-0x", NVME_FC_TRADDR_OXNNLEN) &&
 			!strncmp(&buf[NVME_FC_TRADDR_MAX_PN_OFFSET],
 				"pn-0x", NVME_FC_TRADDR_OXNNLEN)) {
+		/*
+		 * traddr": "nn-0x3333333333333333:pn-0x1111111111111111"
+		 *
+		 * 这个if要满足以下几个条件:
+		 * 1. 长度等于 2 * (5 + 16) + 1
+		 * 2. 前5个字符是"nn-0x"
+		 * 3. 从(5 + 16 + 1)的5个字符是"pn-0x"
+		 */
 		nnoffset = NVME_FC_TRADDR_OXNNLEN;
 		pnoffset = NVME_FC_TRADDR_MAX_PN_OFFSET +
 						NVME_FC_TRADDR_OXNNLEN;
@@ -3286,6 +3403,14 @@ nvme_fc_parse_traddr(struct nvmet_fc_traddr *traddr, char *buf, size_t blen)
 			!strncmp(buf, "nn-", NVME_FC_TRADDR_NNLEN) &&
 			!strncmp(&buf[NVME_FC_TRADDR_MIN_PN_OFFSET],
 				"pn-", NVME_FC_TRADDR_NNLEN))) {
+		/*
+		 * traddr": "nn-0x3333333333333333:pn-0x1111111111111111"
+		 *
+		 * 这个if要满足以下几个条件:
+		 * 1. 长度等于 2 * (3 + 16) + 1
+		 * 2. 前3个字符是"nn-"
+		 * 3. 从(3 + 16 + 1)的3个字符是"nn-"
+		 */
 		nnoffset = NVME_FC_TRADDR_NNLEN;
 		pnoffset = NVME_FC_TRADDR_MIN_PN_OFFSET + NVME_FC_TRADDR_NNLEN;
 	} else
@@ -3331,11 +3456,35 @@ nvme_fc_create_ctrl(struct device *dev, struct nvmf_ctrl_options *opts)
 
 	/* find the host and remote ports to connect together */
 	spin_lock_irqsave(&nvme_fc_lock, flags);
+	/*
+	 * 在以下使用nvme_fc_lport_list:
+	 *   - drivers/nvme/host/fc.c|235| <<nvme_fc_free_lport>> if (nvme_fc_waiting_to_unload && list_empty(&nvme_fc_lport_list))
+	 *   - drivers/nvme/host/fc.c|270| <<nvme_fc_attach_to_unreg_lport>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+	 *   - drivers/nvme/host/fc.c|413| <<nvme_fc_register_localport>> list_add_tail(&newrec->port_list, &nvme_fc_lport_list);
+	 *   - drivers/nvme/host/fc.c|3346| <<nvme_fc_create_ctrl>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+	 *   - drivers/nvme/host/fc.c|3398| <<nvme_fc_nvme_discovery_store>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+	 *   - drivers/nvme/host/fc.c|3544| <<nvme_fc_cleanup_for_unload>> list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+	 *   - drivers/nvme/host/fc.c|3558| <<nvme_fc_exit_module>> if (!list_empty(&nvme_fc_lport_list)) {
+	 *
+	 * 寻找struct nvme_fc_lport
+	 */
 	list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
 		if (lport->localport.node_name != laddr.nn ||
 		    lport->localport.port_name != laddr.pn)
 			continue;
 
+		/*
+		 * 在以下使用nvme_fc_lport->endp_list:
+		 *   - drivers/nvme/host/fc.c|240| <<nvme_fc_free_lport>> WARN_ON(!list_empty(&lport->endp_list));
+		 *   - drivers/nvme/host/fc.c|408| <<nvme_fc_register_localport>> INIT_LIST_HEAD(&newrec->endp_list);
+		 *   - drivers/nvme/host/fc.c|599| <<nvme_fc_attach_to_suspended_rport>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+		 *   - drivers/nvme/host/fc.c|758| <<nvme_fc_register_remoteport>> list_add_tail(&newrec->endp_list, &lport->endp_list);
+		 *   - drivers/nvme/host/fc.c|3371| <<nvme_fc_create_ctrl>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+		 *   - drivers/nvme/host/fc.c|3419| <<nvme_fc_nvme_discovery_store>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+		 *   - drivers/nvme/host/fc.c|3565| <<nvme_fc_cleanup_for_unload>> list_for_each_entry(rport, &lport->endp_list, endp_list) {
+		 *
+		 * 寻找struct nvme_fc_rport
+		 */
 		list_for_each_entry(rport, &lport->endp_list, endp_list) {
 			if (rport->remoteport.node_name != raddr.nn ||
 			    rport->remoteport.port_name != raddr.pn)
diff --git a/drivers/nvme/host/hwmon.c b/drivers/nvme/host/hwmon.c
index a5af21f5d370..8e8321f85372 100644
--- a/drivers/nvme/host/hwmon.c
+++ b/drivers/nvme/host/hwmon.c
@@ -9,6 +9,39 @@
 
 #include "nvme.h"
 
+/*
+ * commit 400b6a7b13a3fd71cff087139ce45dd1e5fff444
+ * Author: Guenter Roeck <linux@roeck-us.net>
+ * Date:   Wed Nov 6 06:35:18 2019 -0800
+ *
+ * nvme: Add hardware monitoring support
+ *
+ * nvme devices report temperature information in the controller information
+ * (for limits) and in the smart log. Currently, the only means to retrieve
+ * this information is the nvme command line interface, which requires
+ * super-user privileges.
+ *
+ * At the same time, it would be desirable to be able to use NVMe temperature
+ * information for thermal control.
+ *
+ * This patch adds support to read NVMe temperatures from the kernel using the
+ * hwmon API and adds temperature zones for NVMe drives. The thermal subsystem
+ * can use this information to set thermal policies, and userspace can access
+ * it using libsensors and/or the "sensors" command.
+ *
+ * Example output from the "sensors" command:
+ *
+ * nvme0-pci-0100
+ * Adapter: PCI adapter
+ * Composite:    +39.0°C  (high = +85.0°C, crit = +85.0°C)
+ * Sensor 1:     +39.0°C
+ * Sensor 2:     +41.0°C
+ *
+ * Reviewed-by: Christoph Hellwig <hch@lst.de>
+ * Signed-off-by: Guenter Roeck <linux@roeck-us.net>
+ * Signed-off-by: Keith Busch <kbusch@kernel.org>
+ */
+
 /* These macros should be moved to linux/temperature.h */
 #define MILLICELSIUS_TO_KELVIN(t) DIV_ROUND_CLOSEST((t) + 273150, 1000)
 #define KELVIN_TO_MILLICELSIUS(t) ((t) * 1000L - 273150)
@@ -19,6 +52,11 @@ struct nvme_hwmon_data {
 	struct mutex read_lock;
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/hwmon.c|87| <<nvme_hwmon_read>> return nvme_get_temp_thresh(data->ctrl, channel, false, val);
+ *   - drivers/nvme/host/hwmon.c|89| <<nvme_hwmon_read>> return nvme_get_temp_thresh(data->ctrl, channel, true, val);
+ */
 static int nvme_get_temp_thresh(struct nvme_ctrl *ctrl, int sensor, bool under,
 				long *temp)
 {
@@ -60,10 +98,20 @@ static int nvme_set_temp_thresh(struct nvme_ctrl *ctrl, int sensor, bool under,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/hwmon.c|98| <<nvme_hwmon_read>> err = nvme_hwmon_get_smart_log(data);
+ *   - drivers/nvme/host/hwmon.c|249| <<nvme_hwmon_init>> err = nvme_hwmon_get_smart_log(data);
+ */
 static int nvme_hwmon_get_smart_log(struct nvme_hwmon_data *data)
 {
 	int ret;
 
+	/*
+	 * 在以下使用NVME_LOG_SMART:
+	 *   - drivers/nvme/host/hwmon.c|67| <<nvme_hwmon_get_smart_log>> ret = nvme_get_log(data->ctrl, NVME_NSID_ALL, NVME_LOG_SMART, 0,
+	 *   - drivers/nvme/target/admin-cmd.c|303| <<nvmet_execute_get_log_page>> case NVME_LOG_SMART:
+	 */
 	ret = nvme_get_log(data->ctrl, NVME_NSID_ALL, NVME_LOG_SMART, 0,
 			   &data->log, sizeof(data->log), 0);
 
@@ -228,6 +276,10 @@ static const struct hwmon_chip_info nvme_hwmon_chip_info = {
 	.info	= nvme_hwmon_info,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3432| <<nvme_init_identify>> nvme_hwmon_init(ctrl);
+ */
 void nvme_hwmon_init(struct nvme_ctrl *ctrl)
 {
 	struct device *dev = ctrl->dev;
diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 797c18337d96..342c82ce6ee7 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -7,11 +7,33 @@
 #include <trace/events/block.h>
 #include "nvme.h"
 
+/*
+ * Shared namespaces may be accessed through controllers via multiple PCIe ports (refer to section 1.4.1) 
+ * or fabric ports (refer to the NVMe over Fabrics specification). The controllers that provide access to a shared
+ * namespace may provide identical access characteristics through all controllers (i.e., symmetric access), or
+ * may provide different access characteristics through some controllers (i.e., asymmetric access).
+ *
+ * Private namespaces are accessed by only one controller at a time. The access characteristics of the
+ * namespace through that controller may be impacted as a result of changes to the internal configuration of
+ * the NVM subsystem. If the access characteristics of the namespace through that controller are impacted
+ * by the internal configuration of the NVM subsystem, then asymmetric access occurs.
+ */
+
+/*
+ * 在以下使用multipath:
+ *   - drivers/nvme/host/multipath.c|59| <<nvme_set_disk_name>> if (!multipath) {
+ *   - drivers/nvme/host/multipath.c|420| <<nvme_mpath_alloc_disk>> if (!(ctrl->subsys->cmic & (1 << 1)) || !multipath)
+ *   - drivers/nvme/host/multipath.c|819| <<nvme_mpath_init>> if (!multipath || !ctrl->subsys || !(ctrl->subsys->cmic & (1 << 3)))
+ */
 static bool multipath = true;
 module_param(multipath, bool, 0444);
 MODULE_PARM_DESC(multipath,
 	"turn on native support for multiple controllers per subsystem");
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1933| <<nvme_passthru_end>> nvme_mpath_unfreeze(ctrl->subsys);
+ */
 void nvme_mpath_unfreeze(struct nvme_subsystem *subsys)
 {
 	struct nvme_ns_head *h;
@@ -22,6 +44,10 @@ void nvme_mpath_unfreeze(struct nvme_subsystem *subsys)
 			blk_mq_unfreeze_queue(h->disk->queue);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1899| <<nvme_passthru_start>> nvme_mpath_wait_freeze(ctrl->subsys);
+ */
 void nvme_mpath_wait_freeze(struct nvme_subsystem *subsys)
 {
 	struct nvme_ns_head *h;
@@ -32,6 +58,10 @@ void nvme_mpath_wait_freeze(struct nvme_subsystem *subsys)
 			blk_mq_freeze_queue_wait(h->disk->queue);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1898| <<nvme_passthru_start>> nvme_mpath_start_freeze(ctrl->subsys);
+ */
 void nvme_mpath_start_freeze(struct nvme_subsystem *subsys)
 {
 	struct nvme_ns_head *h;
@@ -49,6 +79,10 @@ void nvme_mpath_start_freeze(struct nvme_subsystem *subsys)
  * and those that have a single controller and use the controller node
  * directly.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4470| <<nvme_alloc_ns>> nvme_set_disk_name(disk_name, ns, ctrl, &flags);
+ */
 void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
 			struct nvme_ctrl *ctrl, int *flags)
 {
@@ -57,6 +91,10 @@ void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
 	} else if (ns->head->disk) {
 		sprintf(disk_name, "nvme%dc%dn%d", ctrl->subsys->instance,
 				ctrl->instance, ns->head->instance);
+		/*
+		 * !!!! 很重要
+		 * 就不会在/dev/下面显示了!!!
+		 */
 		*flags = GENHD_FL_HIDDEN;
 	} else {
 		sprintf(disk_name, "nvme%dn%d", ctrl->subsys->instance,
@@ -64,6 +102,10 @@ void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|296| <<nvme_complete_rq>> nvme_failover_req(req);
+ */
 void nvme_failover_req(struct request *req)
 {
 	struct nvme_ns *ns = req->q->queuedata;
@@ -71,6 +113,16 @@ void nvme_failover_req(struct request *req)
 	unsigned long flags;
 
 	spin_lock_irqsave(&ns->head->requeue_lock, flags);
+	/*
+	 * 在以下使用requeue_list:
+	 *   - drivers/nvme/host/multipath.c|116| <<nvme_failover_req>> blk_steal_bios(&ns->head->requeue_list, req);
+	 *   - drivers/nvme/host/multipath.c|433| <<nvme_ns_head_make_request>> bio_list_add(&head->requeue_list, bio);
+	 *   - drivers/nvme/host/multipath.c|453| <<nvme_requeue_work>> next = bio_list_get(&head->requeue_list);
+	 *   - drivers/nvme/host/multipath.c|479| <<nvme_mpath_alloc_disk>> bio_list_init(&head->requeue_list);
+	 *
+	 * Steal bios from a request and add them to a bio list.
+	 * The request must not have been partially completed before.
+	 */
 	blk_steal_bios(&ns->head->requeue_list, req);
 	spin_unlock_irqrestore(&ns->head->requeue_lock, flags);
 	blk_mq_end_request(req, 0);
@@ -90,7 +142,21 @@ void nvme_failover_req(struct request *req)
 		 */
 		nvme_mpath_clear_current_path(ns);
 		if (ns->ctrl->ana_log_buf) {
+			/*
+			 * 在以下使用NVME_NS_ANA_PENDING:
+			 *   - drivers/nvme/host/multipath.c|123| <<nvme_failover_req>> set_bit(NVME_NS_ANA_PENDING, &ns->flags);
+			 *   - drivers/nvme/host/multipath.c|222| <<nvme_path_is_disabled>> test_bit(NVME_NS_ANA_PENDING, &ns->flags) ||
+			 *   - drivers/nvme/host/multipath.c|576| <<nvme_update_ns_ana_state>> clear_bit(NVME_NS_ANA_PENDING, &ns->flags);
+			 */
 			set_bit(NVME_NS_ANA_PENDING, &ns->flags);
+			/*
+			 * 在以下使用nvme_ctrl->ana_work:
+			 *   - drivers/nvme/host/core.c|5025| <<nvme_handle_aen_notice>> queue_work(nvme_wq, &ctrl->ana_work);
+			 *   - drivers/nvme/host/multipath.c|102| <<nvme_failover_req>> queue_work(nvme_wq, &ns->ctrl->ana_work);
+			 *   - drivers/nvme/host/multipath.c|627| <<nvme_ana_work>> struct nvme_ctrl *ctrl = container_of(work, struct nvme_ctrl, ana_work);
+			 *   - drivers/nvme/host/multipath.c|645| <<nvme_mpath_stop>> cancel_work_sync(&ctrl->ana_work);
+			 *   - drivers/nvme/host/multipath.c|781| <<nvme_mpath_init>> INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+			 */
 			queue_work(nvme_wq, &ns->ctrl->ana_work);
 		}
 		break;
@@ -114,12 +180,27 @@ void nvme_failover_req(struct request *req)
 	kblockd_schedule_work(&ns->head->requeue_work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|441| <<nvme_change_ctrl_state>> nvme_kick_requeue_lists(ctrl);
+ */
 void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
 
 	down_read(&ctrl->namespaces_rwsem);
 	list_for_each_entry(ns, &ctrl->namespaces, list) {
+		/*
+		 * 在以下使用requeue_work:
+		 *   - drivers/nvme/host/multipath.c|164| <<nvme_failover_req>> kblockd_schedule_work(&ns->head->requeue_work);
+		 *   - drivers/nvme/host/multipath.c|178| <<nvme_kick_requeue_lists>> kblockd_schedule_work(&ns->head->requeue_work);
+		 *   - drivers/nvme/host/multipath.c|245| <<nvme_mpath_clear_ctrl_paths>> kblockd_schedule_work(&ns->head->requeue_work);
+		 *   - drivers/nvme/host/multipath.c|481| <<nvme_mpath_alloc_disk>> INIT_WORK(&head->requeue_work, nvme_requeue_work);
+		 *   - drivers/nvme/host/multipath.c|556| <<nvme_mpath_set_live>> kblockd_schedule_work(&ns->head->requeue_work);
+		 *   - drivers/nvme/host/multipath.c|906| <<nvme_mpath_remove_disk>> kblockd_schedule_work(&head->requeue_work);
+		 *   - drivers/nvme/host/multipath.c|907| <<nvme_mpath_remove_disk>> flush_work(&head->requeue_work);
+		 *   - drivers/nvme/host/nvme.h|996| <<nvme_mpath_check_last_path>> kblockd_schedule_work(&head->requeue_work);
+		 */
 		if (ns->head->disk)
 			kblockd_schedule_work(&ns->head->requeue_work);
 	}
@@ -135,8 +216,22 @@ static const char *nvme_ana_state_names[] = {
 	[NVME_ANA_CHANGE]		= "change",
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4566| <<nvme_ns_remove>> nvme_mpath_clear_current_path(ns);
+ *   - drivers/nvme/host/multipath.c|103| <<nvme_failover_req>> nvme_mpath_clear_current_path(ns);
+ *   - drivers/nvme/host/multipath.c|123| <<nvme_failover_req>> nvme_mpath_clear_current_path(ns);
+ *   - drivers/nvme/host/multipath.c|188| <<nvme_mpath_clear_ctrl_paths>> if (nvme_mpath_clear_current_path(ns))
+ */
 bool nvme_mpath_clear_current_path(struct nvme_ns *ns)
 {
+	/*
+	 * Anchor structure for namespaces.  There is one for each namespace in a
+	 * NVMe subsystem that any of our controllers can see, and the namespace
+	 * structure for each controller is chained of it.  For private namespaces
+	 * there is a 1:1 relation to our namespace structures, that is ->list
+	 * only ever has a single entry for private namespaces.
+	 */
 	struct nvme_ns_head *head = ns->head;
 	bool changed = false;
 	int node;
@@ -145,6 +240,14 @@ bool nvme_mpath_clear_current_path(struct nvme_ns *ns)
 		goto out;
 
 	for_each_node(node) {
+		/*
+		 * 在以下使用current_path:
+		 *   - drivers/nvme/host/multipath.c|156| <<nvme_mpath_clear_current_path>> if (ns == rcu_access_pointer(head->current_path[node])) {
+		 *   - drivers/nvme/host/multipath.c|157| <<nvme_mpath_clear_current_path>> rcu_assign_pointer(head->current_path[node], NULL);
+		 *   - drivers/nvme/host/multipath.c|220| <<__nvme_find_path>> rcu_assign_pointer(head->current_path[node], found);
+		 *   - drivers/nvme/host/multipath.c|263| <<nvme_round_robin_path>> rcu_assign_pointer(head->current_path[node], found);
+		 *   - drivers/nvme/host/multipath.c|278| <<nvme_find_path>> ns = srcu_dereference(head->current_path[node], &head->srcu);
+		 */
 		if (ns == rcu_access_pointer(head->current_path[node])) {
 			rcu_assign_pointer(head->current_path[node], NULL);
 			changed = true;
@@ -154,6 +257,10 @@ bool nvme_mpath_clear_current_path(struct nvme_ns *ns)
 	return changed;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4844| <<nvme_remove_namespaces>> nvme_mpath_clear_ctrl_paths(ctrl);
+ */
 void nvme_mpath_clear_ctrl_paths(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -161,12 +268,29 @@ void nvme_mpath_clear_ctrl_paths(struct nvme_ctrl *ctrl)
 	mutex_lock(&ctrl->scan_lock);
 	down_read(&ctrl->namespaces_rwsem);
 	list_for_each_entry(ns, &ctrl->namespaces, list)
+		/*
+		 * 在以下使用requeue_work:
+		 *   - drivers/nvme/host/multipath.c|164| <<nvme_failover_req>> kblockd_schedule_work(&ns->head->requeue_work);
+		 *   - drivers/nvme/host/multipath.c|178| <<nvme_kick_requeue_lists>> kblockd_schedule_work(&ns->head->requeue_work);
+		 *   - drivers/nvme/host/multipath.c|245| <<nvme_mpath_clear_ctrl_paths>> kblockd_schedule_work(&ns->head->requeue_work);
+		 *   - drivers/nvme/host/multipath.c|481| <<nvme_mpath_alloc_disk>> INIT_WORK(&head->requeue_work, nvme_requeue_work);
+		 *   - drivers/nvme/host/multipath.c|556| <<nvme_mpath_set_live>> kblockd_schedule_work(&ns->head->requeue_work);
+		 *   - drivers/nvme/host/multipath.c|906| <<nvme_mpath_remove_disk>> kblockd_schedule_work(&head->requeue_work);
+		 *   - drivers/nvme/host/multipath.c|907| <<nvme_mpath_remove_disk>> flush_work(&head->requeue_work);
+		 *   - drivers/nvme/host/nvme.h|996| <<nvme_mpath_check_last_path>> kblockd_schedule_work(&head->requeue_work);
+		 */
 		if (nvme_mpath_clear_current_path(ns))
 			kblockd_schedule_work(&ns->head->requeue_work);
 	up_read(&ctrl->namespaces_rwsem);
 	mutex_unlock(&ctrl->scan_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|237| <<__nvme_find_path>> if (nvme_path_is_disabled(ns))
+ *   - drivers/nvme/host/multipath.c|286| <<nvme_round_robin_path>> if (nvme_path_is_disabled(old))
+ *   - drivers/nvme/host/multipath.c|294| <<nvme_round_robin_path>> if (nvme_path_is_disabled(ns))
+ */
 static bool nvme_path_is_disabled(struct nvme_ns *ns)
 {
 	return ns->ctrl->state != NVME_CTRL_LIVE ||
@@ -174,11 +298,19 @@ static bool nvme_path_is_disabled(struct nvme_ns *ns)
 		test_bit(NVME_NS_REMOVING, &ns->flags);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|282| <<nvme_find_path>> ns = __nvme_find_path(head, node);
+ *   - drivers/nvme/host/multipath.c|442| <<nvme_mpath_set_live>> __nvme_find_path(head, node);
+ */
 static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head, int node)
 {
 	int found_distance = INT_MAX, fallback_distance = INT_MAX, distance;
 	struct nvme_ns *found = NULL, *fallback = NULL, *ns;
 
+	/*
+	 * head->list管理所有的nvme_ns->siblings
+	 */
 	list_for_each_entry_rcu(ns, &head->list, siblings) {
 		if (nvme_path_is_disabled(ns))
 			continue;
@@ -208,14 +340,30 @@ static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head, int node)
 
 	if (!found)
 		found = fallback;
+	/*
+	 * 在以下使用current_path:
+	 *   - drivers/nvme/host/multipath.c|156| <<nvme_mpath_clear_current_path>> if (ns == rcu_access_pointer(head->current_path[node])) {
+	 *   - drivers/nvme/host/multipath.c|157| <<nvme_mpath_clear_current_path>> rcu_assign_pointer(head->current_path[node], NULL);
+	 *   - drivers/nvme/host/multipath.c|220| <<__nvme_find_path>> rcu_assign_pointer(head->current_path[node], found);
+	 *   - drivers/nvme/host/multipath.c|263| <<nvme_round_robin_path>> rcu_assign_pointer(head->current_path[node], found);
+	 *   - drivers/nvme/host/multipath.c|278| <<nvme_find_path>> ns = srcu_dereference(head->current_path[node], &head->srcu);
+	 */
 	if (found)
 		rcu_assign_pointer(head->current_path[node], found);
 	return found;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|339| <<nvme_round_robin_path>> for (ns = nvme_next_ns(head, old);
+ *   - drivers/nvme/host/multipath.c|341| <<nvme_round_robin_path>> ns = nvme_next_ns(head, ns)) {
+ */
 static struct nvme_ns *nvme_next_ns(struct nvme_ns_head *head,
 		struct nvme_ns *ns)
 {
+	/*
+	 * 管理所有的nvme_ns->siblings
+	 */
 	ns = list_next_or_null_rcu(&head->list, &ns->siblings, struct nvme_ns,
 			siblings);
 	if (ns)
@@ -223,11 +371,18 @@ static struct nvme_ns *nvme_next_ns(struct nvme_ns_head *head,
 	return list_first_or_null_rcu(&head->list, struct nvme_ns, siblings);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|379| <<nvme_find_path>> ns = nvme_round_robin_path(head, node, ns);
+ */
 static struct nvme_ns *nvme_round_robin_path(struct nvme_ns_head *head,
 		int node, struct nvme_ns *old)
 {
 	struct nvme_ns *ns, *found, *fallback = NULL;
 
+	/*
+	 * 管理所有的nvme_ns->siblings
+	 */
 	if (list_is_singular(&head->list)) {
 		if (nvme_path_is_disabled(old))
 			return NULL;
@@ -256,17 +411,35 @@ static struct nvme_ns *nvme_round_robin_path(struct nvme_ns_head *head,
 	return found;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|380| <<nvme_find_path>> if (unlikely(!ns || !nvme_path_is_optimized(ns)))
+ *   - drivers/nvme/host/multipath.c|570| <<nvme_mpath_set_live>> if (nvme_path_is_optimized(ns)) {
+ */
 static inline bool nvme_path_is_optimized(struct nvme_ns *ns)
 {
 	return ns->ctrl->state == NVME_CTRL_LIVE &&
 		ns->ana_state == NVME_ANA_OPTIMIZED;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1692| <<nvme_get_ns_from_disk>> ns = nvme_find_path(*head);
+ *   - drivers/nvme/host/multipath.c|322| <<nvme_ns_head_make_request>> ns = nvme_find_path(head);
+ */
 inline struct nvme_ns *nvme_find_path(struct nvme_ns_head *head)
 {
 	int node = numa_node_id();
 	struct nvme_ns *ns;
 
+	/*
+	 * 在以下使用current_path:
+	 *   - drivers/nvme/host/multipath.c|156| <<nvme_mpath_clear_current_path>> if (ns == rcu_access_pointer(head->current_path[node])) {
+	 *   - drivers/nvme/host/multipath.c|157| <<nvme_mpath_clear_current_path>> rcu_assign_pointer(head->current_path[node], NULL);
+	 *   - drivers/nvme/host/multipath.c|220| <<__nvme_find_path>> rcu_assign_pointer(head->current_path[node], found);
+	 *   - drivers/nvme/host/multipath.c|263| <<nvme_round_robin_path>> rcu_assign_pointer(head->current_path[node], found);
+	 *   - drivers/nvme/host/multipath.c|278| <<nvme_find_path>> ns = srcu_dereference(head->current_path[node], &head->srcu);
+	 */
 	ns = srcu_dereference(head->current_path[node], &head->srcu);
 	if (READ_ONCE(head->subsys->iopolicy) == NVME_IOPOLICY_RR && ns)
 		ns = nvme_round_robin_path(head, node, ns);
@@ -275,10 +448,17 @@ inline struct nvme_ns *nvme_find_path(struct nvme_ns_head *head)
 	return ns;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|433| <<nvme_ns_head_make_request>> } else if (nvme_available_path(head)) {
+ */
 static bool nvme_available_path(struct nvme_ns_head *head)
 {
 	struct nvme_ns *ns;
 
+	/*
+	 * head->list管理所有的nvme_ns->siblings
+	 */
 	list_for_each_entry_rcu(ns, &head->list, siblings) {
 		switch (ns->ctrl->state) {
 		case NVME_CTRL_LIVE:
@@ -293,6 +473,10 @@ static bool nvme_available_path(struct nvme_ns_head *head)
 	return false;
 }
 
+/*
+ * 在以下使用nvme_ns_head_make_request():
+ *   - drivers/nvme/host/multipath.c|500| <<nvme_mpath_alloc_disk>> blk_queue_make_request(q, nvme_ns_head_make_request);
+ */
 static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
 		struct bio *bio)
 {
@@ -336,6 +520,19 @@ static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * 在以下使用requeue_work:
+ *   - drivers/nvme/host/multipath.c|164| <<nvme_failover_req>> kblockd_schedule_work(&ns->head->requeue_work);
+ *   - drivers/nvme/host/multipath.c|178| <<nvme_kick_requeue_lists>> kblockd_schedule_work(&ns->head->requeue_work);
+ *   - drivers/nvme/host/multipath.c|245| <<nvme_mpath_clear_ctrl_paths>> kblockd_schedule_work(&ns->head->requeue_work);
+ *   - drivers/nvme/host/multipath.c|556| <<nvme_mpath_set_live>> kblockd_schedule_work(&ns->head->requeue_work);
+ *   - drivers/nvme/host/multipath.c|906| <<nvme_mpath_remove_disk>> kblockd_schedule_work(&head->requeue_work);
+ *   - drivers/nvme/host/multipath.c|907| <<nvme_mpath_remove_disk>> flush_work(&head->requeue_work);
+ *   - drivers/nvme/host/nvme.h|996| <<nvme_mpath_check_last_path>> kblockd_schedule_work(&head->requeue_work);
+ *
+ * 在以下使用nvme_requeue_work():
+ *   - drivers/nvme/host/multipath.c|481| <<nvme_mpath_alloc_disk>> INIT_WORK(&head->requeue_work, nvme_requeue_work);
+ */
 static void nvme_requeue_work(struct work_struct *work)
 {
 	struct nvme_ns_head *head =
@@ -343,6 +540,13 @@ static void nvme_requeue_work(struct work_struct *work)
 	struct bio *bio, *next;
 
 	spin_lock_irq(&head->requeue_lock);
+	/*
+	 * 在以下使用requeue_list:
+	 *   - drivers/nvme/host/multipath.c|116| <<nvme_failover_req>> blk_steal_bios(&ns->head->requeue_list, req);
+	 *   - drivers/nvme/host/multipath.c|433| <<nvme_ns_head_make_request>> bio_list_add(&head->requeue_list, bio);
+	 *   - drivers/nvme/host/multipath.c|453| <<nvme_requeue_work>> next = bio_list_get(&head->requeue_list);
+	 *   - drivers/nvme/host/multipath.c|479| <<nvme_mpath_alloc_disk>> bio_list_init(&head->requeue_list);
+	 */
 	next = bio_list_get(&head->requeue_list);
 	spin_unlock_irq(&head->requeue_lock);
 
@@ -359,6 +563,10 @@ static void nvme_requeue_work(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3601| <<nvme_alloc_ns_head>> ret = nvme_mpath_alloc_disk(ctrl, head);
+ */
 int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 {
 	struct request_queue *q;
@@ -369,6 +577,11 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	spin_lock_init(&head->requeue_lock);
 	INIT_WORK(&head->requeue_work, nvme_requeue_work);
 
+	/*
+	 * 在qemu的nvme上下面的if会退出
+	 * cmic=0x0000000000000000, multipath=1
+	 */
+
 	/*
 	 * Add a multipath node if the subsystems supports multiple controllers.
 	 * We also do this for private namespaces as the namespace sharing data could
@@ -409,6 +622,11 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|528| <<nvme_update_ns_ana_state>> nvme_mpath_set_live(ns);
+ *   - drivers/nvme/host/multipath.c|706| <<nvme_mpath_add_disk>> nvme_mpath_set_live(ns);
+ */
 static void nvme_mpath_set_live(struct nvme_ns *ns)
 {
 	struct nvme_ns_head *head = ns->head;
@@ -435,10 +653,16 @@ static void nvme_mpath_set_live(struct nvme_ns *ns)
 	kblockd_schedule_work(&ns->head->requeue_work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|549| <<nvme_read_ana_log>> error = nvme_parse_ana_log(ctrl, &nr_change_groups,
+ *   - drivers/nvme/host/multipath.c|673| <<nvme_mpath_add_disk>> nvme_parse_ana_log(ns->ctrl, ns, nvme_set_ns_ana_state);
+ */
 static int nvme_parse_ana_log(struct nvme_ctrl *ctrl, void *data,
 		int (*cb)(struct nvme_ctrl *ctrl, struct nvme_ana_group_desc *,
 			void *))
 {
+	/* 用NVME_LOG_ANA读取的 */
 	void *base = ctrl->ana_log_buf;
 	size_t offset = sizeof(struct nvme_ana_rsp_hdr);
 	int error, i;
@@ -479,17 +703,43 @@ static int nvme_parse_ana_log(struct nvme_ctrl *ctrl, void *data,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|644| <<nvme_update_ns_ana_state>> if (nvme_state_is_live(ns->ana_state))
+ */
 static inline bool nvme_state_is_live(enum nvme_ana_state state)
 {
 	return state == NVME_ANA_OPTIMIZED || state == NVME_ANA_NONOPTIMIZED;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|556| <<nvme_update_ana_state>> nvme_update_ns_ana_state(desc, ns);
+ *   - drivers/nvme/host/multipath.c|685| <<nvme_set_ns_ana_state>> nvme_update_ns_ana_state(desc, ns);
+ */
 static void nvme_update_ns_ana_state(struct nvme_ana_group_desc *desc,
 		struct nvme_ns *ns)
 {
 	mutex_lock(&ns->head->lock);
+	/*
+	 * 在以下使用nvme_ns->ana_grpid:
+	 *   - drivers/nvme/host/multipath.c|770| <<global>> DEVICE_ATTR_RO(ana_grpid);
+	 *   - drivers/nvme/target/configfs.c|493| <<global>> CONFIGFS_ATTR(nvmet_ns_, ana_grpid);
+	 *   - drivers/nvme/host/multipath.c|574| <<nvme_update_ns_ana_state>> ns->ana_grpid = le32_to_cpu(desc->grpid);
+	 *   - drivers/nvme/host/multipath.c|768| <<ana_grpid_show>> return sprintf(buf, "%d\n", nvme_get_ns_from_dev(dev)->ana_grpid);
+	 *   - drivers/nvme/host/multipath.c|790| <<nvme_set_ns_ana_state>> if (ns->ana_grpid == le32_to_cpu(desc->grpid)) {
+	 *   - drivers/nvme/host/multipath.c|806| <<nvme_mpath_add_disk>> ns->ana_grpid = le32_to_cpu(id->anagrpid);
+	 *
+	 * 这里的ns是struct nvme_ns;
+	 */
 	ns->ana_grpid = le32_to_cpu(desc->grpid);
 	ns->ana_state = desc->state;
+	/*
+	 * 在以下使用NVME_NS_ANA_PENDING:
+	 *   - drivers/nvme/host/multipath.c|123| <<nvme_failover_req>> set_bit(NVME_NS_ANA_PENDING, &ns->flags);
+	 *   - drivers/nvme/host/multipath.c|222| <<nvme_path_is_disabled>> test_bit(NVME_NS_ANA_PENDING, &ns->flags) ||
+	 *   - drivers/nvme/host/multipath.c|576| <<nvme_update_ns_ana_state>> clear_bit(NVME_NS_ANA_PENDING, &ns->flags);
+	 */
 	clear_bit(NVME_NS_ANA_PENDING, &ns->flags);
 
 	if (nvme_state_is_live(ns->ana_state))
@@ -497,6 +747,10 @@ static void nvme_update_ns_ana_state(struct nvme_ana_group_desc *desc,
 	mutex_unlock(&ns->head->lock);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|578| <<nvme_read_ana_log>> nvme_update_ana_state);
+ */
 static int nvme_update_ana_state(struct nvme_ctrl *ctrl,
 		struct nvme_ana_group_desc *desc, void *data)
 {
@@ -508,6 +762,14 @@ static int nvme_update_ana_state(struct nvme_ctrl *ctrl,
 			le32_to_cpu(desc->grpid),
 			nvme_ana_state_names[desc->state]);
 
+	/*
+	 * 在以下使用NVME_ANA_CHANGE:
+	 *   - drivers/nvme/host/multipath.c|189| <<global>> [NVME_ANA_CHANGE] = "change",
+	 *   - drivers/nvme/target/configfs.c|1024| <<global>> { NVME_ANA_CHANGE, "change" },
+	 *   - drivers/nvme/host/multipath.c|592| <<nvme_parse_ana_log>> if (WARN_ON_ONCE(desc->state > NVME_ANA_CHANGE))
+	 *   - drivers/nvme/host/multipath.c|664| <<nvme_update_ana_state>> if (desc->state == NVME_ANA_CHANGE)
+	 *   - drivers/nvme/target/core.c|809| <<nvmet_check_ana_state>> if (unlikely(state == NVME_ANA_CHANGE))
+	 */
 	if (desc->state == NVME_ANA_CHANGE)
 		(*nr_change_groups)++;
 
@@ -529,6 +791,11 @@ static int nvme_update_ana_state(struct nvme_ctrl *ctrl,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|606| <<nvme_ana_work>> nvme_read_ana_log(ctrl);
+ *   - drivers/nvme/host/multipath.c|769| <<nvme_mpath_init>> error = nvme_read_ana_log(ctrl);
+ */
 static int nvme_read_ana_log(struct nvme_ctrl *ctrl)
 {
 	u32 nr_change_groups = 0;
@@ -558,6 +825,14 @@ static int nvme_read_ana_log(struct nvme_ctrl *ctrl)
 	 * We also double the ANATT value to provide some slack for transports
 	 * or AEN processing overhead.
 	 */
+	/*
+	 * 在以下使用anatt_timer:
+	 *   - drivers/nvme/host/multipath.c|629| <<nvme_read_ana_log>> mod_timer(&ctrl->anatt_timer, ctrl->anatt * HZ * 2 + jiffies);
+	 *   - drivers/nvme/host/multipath.c|631| <<nvme_read_ana_log>> del_timer_sync(&ctrl->anatt_timer);
+	 *   - drivers/nvme/host/multipath.c|654| <<nvme_anatt_timeout>> struct nvme_ctrl *ctrl = from_timer(ctrl, t, anatt_timer);
+	 *   - drivers/nvme/host/multipath.c|668| <<nvme_mpath_stop>> del_timer_sync(&ctrl->anatt_timer);
+	 *   - drivers/nvme/host/multipath.c|821| <<nvme_mpath_init>> timer_setup(&ctrl->anatt_timer, nvme_anatt_timeout, 0);
+	 */
 	if (nr_change_groups)
 		mod_timer(&ctrl->anatt_timer, ctrl->anatt * HZ * 2 + jiffies);
 	else
@@ -567,6 +842,14 @@ static int nvme_read_ana_log(struct nvme_ctrl *ctrl)
 	return error;
 }
 
+/*
+ * 在以下使用nvme_ctrl->ana_work:
+ *   - drivers/nvme/host/core.c|5025| <<nvme_handle_aen_notice>> queue_work(nvme_wq, &ctrl->ana_work);
+ *   - drivers/nvme/host/multipath.c|102| <<nvme_failover_req>> queue_work(nvme_wq, &ns->ctrl->ana_work);
+ *   - drivers/nvme/host/multipath.c|627| <<nvme_ana_work>> struct nvme_ctrl *ctrl = container_of(work, struct nvme_ctrl, ana_work);
+ *   - drivers/nvme/host/multipath.c|645| <<nvme_mpath_stop>> cancel_work_sync(&ctrl->ana_work);
+ *   - drivers/nvme/host/multipath.c|781| <<nvme_mpath_init>> INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+ */
 static void nvme_ana_work(struct work_struct *work)
 {
 	struct nvme_ctrl *ctrl = container_of(work, struct nvme_ctrl, ana_work);
@@ -574,6 +857,14 @@ static void nvme_ana_work(struct work_struct *work)
 	nvme_read_ana_log(ctrl);
 }
 
+/*
+ * 在以下使用anatt_timer:
+ *   - drivers/nvme/host/multipath.c|629| <<nvme_read_ana_log>> mod_timer(&ctrl->anatt_timer, ctrl->anatt * HZ * 2 + jiffies);
+ *   - drivers/nvme/host/multipath.c|631| <<nvme_read_ana_log>> del_timer_sync(&ctrl->anatt_timer);
+ *   - drivers/nvme/host/multipath.c|654| <<nvme_anatt_timeout>> struct nvme_ctrl *ctrl = from_timer(ctrl, t, anatt_timer);
+ *   - drivers/nvme/host/multipath.c|668| <<nvme_mpath_stop>> del_timer_sync(&ctrl->anatt_timer);
+ *   - drivers/nvme/host/multipath.c|821| <<nvme_mpath_init>> timer_setup(&ctrl->anatt_timer, nvme_anatt_timeout, 0);
+ */
 static void nvme_anatt_timeout(struct timer_list *t)
 {
 	struct nvme_ctrl *ctrl = from_timer(ctrl, t, anatt_timer);
@@ -582,11 +873,23 @@ static void nvme_anatt_timeout(struct timer_list *t)
 	nvme_reset_ctrl(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|5086| <<nvme_stop_ctrl>> nvme_mpath_stop(ctrl);
+ */
 void nvme_mpath_stop(struct nvme_ctrl *ctrl)
 {
 	if (!nvme_ctrl_use_ana(ctrl))
 		return;
 	del_timer_sync(&ctrl->anatt_timer);
+	/*
+	 * 在以下使用nvme_ctrl->ana_work:
+	 *   - drivers/nvme/host/core.c|5025| <<nvme_handle_aen_notice>> queue_work(nvme_wq, &ctrl->ana_work);
+	 *   - drivers/nvme/host/multipath.c|102| <<nvme_failover_req>> queue_work(nvme_wq, &ns->ctrl->ana_work);
+	 *   - drivers/nvme/host/multipath.c|627| <<nvme_ana_work>> struct nvme_ctrl *ctrl = container_of(work, struct nvme_ctrl, ana_work);
+	 *   - drivers/nvme/host/multipath.c|645| <<nvme_mpath_stop>> cancel_work_sync(&ctrl->ana_work);
+	 *   - drivers/nvme/host/multipath.c|781| <<nvme_mpath_init>> INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+	 */
 	cancel_work_sync(&ctrl->ana_work);
 }
 
@@ -594,6 +897,14 @@ void nvme_mpath_stop(struct nvme_ctrl *ctrl)
 	struct device_attribute subsys_attr_##_name =	\
 		__ATTR(_name, _mode, _show, _store)
 
+/*
+ * 在以下使用nvme_iopolicy_names:
+ *   - drivers/nvme/host/multipath.c|664| <<nvme_subsys_iopolicy_show>> nvme_iopolicy_names[READ_ONCE(subsys->iopolicy)]);
+ *   - drivers/nvme/host/multipath.c|674| <<nvme_subsys_iopolicy_store>> for (i = 0; i < ARRAY_SIZE(nvme_iopolicy_names); i++) {
+ *   - drivers/nvme/host/multipath.c|675| <<nvme_subsys_iopolicy_store>> if (sysfs_streq(buf, nvme_iopolicy_names[i])) {
+ *
+ * # echo round-robin > /sys/block/nvme0n1/device/iopolicy
+ */
 static const char *nvme_iopolicy_names[] = {
 	[NVME_IOPOLICY_NUMA]	= "numa",
 	[NVME_IOPOLICY_RR]	= "round-robin",
@@ -628,6 +939,10 @@ static ssize_t nvme_subsys_iopolicy_store(struct device *dev,
 SUBSYS_ATTR_RW(iopolicy, S_IRUGO | S_IWUSR,
 		      nvme_subsys_iopolicy_show, nvme_subsys_iopolicy_store);
 
+/*
+ * # cat /sys/block/nvme0n1/device/nvme2/nvme0c2n1/ana_grpid
+ * 1
+ */
 static ssize_t ana_grpid_show(struct device *dev, struct device_attribute *attr,
 		char *buf)
 {
@@ -644,6 +959,10 @@ static ssize_t ana_state_show(struct device *dev, struct device_attribute *attr,
 }
 DEVICE_ATTR_RO(ana_state);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|724| <<nvme_mpath_add_disk>> nvme_parse_ana_log(ns->ctrl, ns, nvme_set_ns_ana_state);
+ */
 static int nvme_set_ns_ana_state(struct nvme_ctrl *ctrl,
 		struct nvme_ana_group_desc *desc, void *data)
 {
@@ -657,8 +976,15 @@ static int nvme_set_ns_ana_state(struct nvme_ctrl *ctrl,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3659| <<nvme_alloc_ns>> nvme_mpath_add_disk(ns, id);
+ */
 void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id)
 {
+	/*
+	 * nvme_ctrl_use_ana(): 判断ctrl->ana_log_buf是否为NULL
+	 */
 	if (nvme_ctrl_use_ana(ns->ctrl)) {
 		mutex_lock(&ns->ctrl->ana_lock);
 		ns->ana_grpid = le32_to_cpu(id->anagrpid);
@@ -672,6 +998,10 @@ void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|651| <<nvme_free_ns_head>> nvme_mpath_remove_disk(head);
+ */
 void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 {
 	if (!head->disk)
@@ -682,27 +1012,67 @@ void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 	/* make sure all pending bios are cleaned up */
 	kblockd_schedule_work(&head->requeue_work);
 	flush_work(&head->requeue_work);
+	/*
+	 * blk_cleanup_queue - shutdown a request queue
+	 * @q: request queue to shutdown
+	 *
+	 * Mark @q DYING, drain all pending requests, mark @q DEAD, destroy and
+	 * put it.  All future requests will be failed immediately with -ENODEV.
+	 */
 	blk_cleanup_queue(head->disk->queue);
 	put_disk(head->disk);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2911| <<nvme_init_identify>> ret = nvme_mpath_init(ctrl, id);
+ */
 int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 {
 	int error;
 
 	/* check if multipath is enabled and we have the capability */
+	/*
+	 * cmic:
+	 * Bit 0: If set to '1', then the namespace may be attached to two or more controllers in the
+	 * NVM subsystem concurrently (i.e., may be a shared namespace). If cleared to '0', then
+	 * the namespace is a private namespace and is able to be attached to only one controller
+	 * at a time.
+	 */
 	if (!multipath || !ctrl->subsys || !(ctrl->subsys->cmic & (1 << 3)))
 		return 0;
 
+	/* ANA = Asymmetric Namespace Access */
 	ctrl->anacap = id->anacap;
 	ctrl->anatt = id->anatt;
+	/*
+	 * 在以下使用nvme_ctrl->nanagrpid:
+	 *   - drivers/nvme/host/multipath.c|763| <<nvme_mpath_init>> ctrl->nanagrpid = le32_to_cpu(id->nanagrpid);
+	 *   - drivers/nvme/host/multipath.c|769| <<nvme_mpath_init>> ctrl->nanagrpid * sizeof(struct nvme_ana_group_desc);
+	 *
+	 * target在以下响应:
+	 *   - drivers/nvme/target/admin-cmd.c|429| <<nvmet_execute_identify_ctrl>> id->nanagrpid = cpu_to_le32(NVMET_MAX_ANAGRPS);
+	 */
 	ctrl->nanagrpid = le32_to_cpu(id->nanagrpid);
 	ctrl->anagrpmax = le32_to_cpu(id->anagrpmax);
 
 	mutex_init(&ctrl->ana_lock);
+	/*
+	 * 在以下使用anatt_timer:
+	 *   - drivers/nvme/host/multipath.c|629| <<nvme_read_ana_log>> mod_timer(&ctrl->anatt_timer, ctrl->anatt * HZ * 2 + jiffies);
+	 *   - drivers/nvme/host/multipath.c|631| <<nvme_read_ana_log>> del_timer_sync(&ctrl->anatt_timer);
+	 *   - drivers/nvme/host/multipath.c|654| <<nvme_anatt_timeout>> struct nvme_ctrl *ctrl = from_timer(ctrl, t, anatt_timer);
+	 *   - drivers/nvme/host/multipath.c|668| <<nvme_mpath_stop>> del_timer_sync(&ctrl->anatt_timer);
+	 *   - drivers/nvme/host/multipath.c|821| <<nvme_mpath_init>> timer_setup(&ctrl->anatt_timer, nvme_anatt_timeout, 0);
+	 */
 	timer_setup(&ctrl->anatt_timer, nvme_anatt_timeout, 0);
 	ctrl->ana_log_size = sizeof(struct nvme_ana_rsp_hdr) +
 		ctrl->nanagrpid * sizeof(struct nvme_ana_group_desc);
+	/*
+	 * 在以下使用nvme_ctrl->max_namespaces:
+	 *   - drivers/nvme/host/core.c|3614| <<nvme_init_identify>> ctrl->max_namespaces = le32_to_cpu(id->mnan);
+	 *   - drivers/nvme/host/multipath.c|770| <<nvme_mpath_init>> ctrl->ana_log_size += ctrl->max_namespaces * sizeof(__le32);
+	 */
 	ctrl->ana_log_size += ctrl->max_namespaces * sizeof(__le32);
 
 	if (ctrl->ana_log_size > ctrl->max_hw_sectors << SECTOR_SHIFT) {
@@ -714,7 +1084,21 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 		return 0;
 	}
 
+	/*
+	 * 在以下使用nvme_ctrl->ana_work:
+	 *   - drivers/nvme/host/core.c|5025| <<nvme_handle_aen_notice>> queue_work(nvme_wq, &ctrl->ana_work);
+	 *   - drivers/nvme/host/multipath.c|102| <<nvme_failover_req>> queue_work(nvme_wq, &ns->ctrl->ana_work);
+	 *   - drivers/nvme/host/multipath.c|627| <<nvme_ana_work>> struct nvme_ctrl *ctrl = container_of(work, struct nvme_ctrl, ana_work);
+	 *   - drivers/nvme/host/multipath.c|645| <<nvme_mpath_stop>> cancel_work_sync(&ctrl->ana_work);
+	 *   - drivers/nvme/host/multipath.c|781| <<nvme_mpath_init>> INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+	 *
+	 * 在nvme_handle_aen_notice()和nvme_failover_req()来trigger
+	 */
 	INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+	/*
+	 * 根据最新的commit, 这里似乎有leak, 多次分配
+	 * 最好调用个kfree()
+	 */
 	ctrl->ana_log_buf = kmalloc(ctrl->ana_log_size, GFP_KERNEL);
 	if (!ctrl->ana_log_buf) {
 		error = -ENOMEM;
@@ -732,6 +1116,10 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	return error;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|5165| <<nvme_free_ctrl>> nvme_mpath_uninit(ctrl);
+ */
 void nvme_mpath_uninit(struct nvme_ctrl *ctrl)
 {
 	kfree(ctrl->ana_log_buf);
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 1024fec7914c..0e704027105c 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -20,9 +20,33 @@
 #include <trace/events/block.h>
 
 extern unsigned int nvme_io_timeout;
+/*
+ * 在以下使用NVME_IO_TIMEOUT:
+ *   - drivers/nvme/host/fc.c|2473| <<nvme_fc_create_io_queues>> ctrl->tag_set.timeout = NVME_IO_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|2800| <<nvme_dev_add>> dev->tagset.timeout = NVME_IO_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|3041| <<nvme_dev_disable>> nvme_wait_freeze_timeout(&dev->ctrl, NVME_IO_TIMEOUT);
+ *   - drivers/nvme/host/rdma.c|751| <<nvme_rdma_alloc_tagset>> set->timeout = NVME_IO_TIMEOUT;
+ *   - drivers/nvme/host/tcp.c|1489| <<nvme_tcp_alloc_tagset>> set->timeout = NVME_IO_TIMEOUT;
+ *   - drivers/nvme/target/loop.c|522| <<nvme_loop_create_io_queues>> ctrl->tag_set.timeout = NVME_IO_TIMEOUT;
+ */
 #define NVME_IO_TIMEOUT	(nvme_io_timeout * HZ)
 
 extern unsigned int admin_timeout;
+/*
+ * 在以下使用ADMIN_TIMEOUT:
+ *   - drivers/nvme/host/core.c|863| <<__nvme_submit_sync_cmd>> req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/core.c|946| <<nvme_submit_user_cmd>> req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/core.c|2132| <<nvme_sec_submit>> ADMIN_TIMEOUT, NVME_QID_ANY, 1, 0, false);
+ *   - drivers/nvme/host/fc.c|3140| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/lightnvm.c|777| <<nvme_nvm_submit_user_cmd>> rq->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|1560| <<nvme_timeout>> abort_req->timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|1945| <<nvme_alloc_admin_tags>> dev->admin_tagset.timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|2747| <<nvme_delete_queue>> req->timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|2763| <<__nvme_disable_io_queues>> timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/rdma.c|737| <<nvme_rdma_alloc_tagset>> set->timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/tcp.c|1477| <<nvme_tcp_alloc_tagset>> set->timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/target/loop.c|348| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.timeout = ADMIN_TIMEOUT;
+ */
 #define ADMIN_TIMEOUT	(admin_timeout * HZ)
 
 #define NVME_DEFAULT_KATO	5
@@ -144,6 +168,13 @@ struct nvme_request {
 /*
  * Mark a bio as coming in through the mpath node.
  */
+/*
+ * 在以下使用REQ_NVME_MPATH:
+ *   - drivers/nvme/host/core.c|435| <<nvme_complete_rq>> if ((req->cmd_flags & REQ_NVME_MPATH) &&
+ *   - drivers/nvme/host/fabrics.c|552| <<nvmf_fail_nonready_command>> !blk_noretry_request(rq) && !(rq->cmd_flags & REQ_NVME_MPATH))
+ *   - drivers/nvme/host/multipath.c|335| <<nvme_ns_head_make_request>> bio->bi_opf |= REQ_NVME_MPATH;
+ *   - drivers/nvme/host/nvme.h|715| <<nvme_trace_bio_complete>> if (req->cmd_flags & REQ_NVME_MPATH)
+ */
 #define REQ_NVME_MPATH		REQ_DRV
 
 enum {
@@ -189,6 +220,14 @@ struct nvme_fault_inject {
 };
 
 struct nvme_ctrl {
+	/*
+	 * 在以下使用comp_seen:
+	 *   - drivers/nvme/host/core.c|456| <<nvme_complete_rq>> nvme_req(req)->ctrl->comp_seen = true;
+	 *   - drivers/nvme/host/core.c|1282| <<nvme_keep_alive_end_io>> ctrl->comp_seen = false;
+	 *   - drivers/nvme/host/core.c|1348| <<nvme_keep_alive_work>> bool comp_seen = ctrl->comp_seen;
+	 *   - drivers/nvme/host/core.c|1350| <<nvme_keep_alive_work>> if ((ctrl->ctratt & NVME_CTRL_ATTR_TBKAS) && comp_seen) {
+	 *   - drivers/nvme/host/core.c|1353| <<nvme_keep_alive_work>> ctrl->comp_seen = false;
+	 */
 	bool comp_seen;
 	enum nvme_ctrl_state state;
 	bool identified;
@@ -202,24 +241,146 @@ struct nvme_ctrl {
 	int instance;
 	int numa_node;
 	struct blk_mq_tag_set *tagset;
+	/*
+	 * 在以下设置admin_tagset:
+	 *   - drivers/nvme/host/fc.c|3146| <<nvme_fc_init_ctrl>> ctrl->ctrl.admin_tagset = &ctrl->admin_tag_set;
+	 *   - drivers/nvme/host/pci.c|1953| <<nvme_alloc_admin_tags>> dev->ctrl.admin_tagset = &dev->admin_tagset;
+	 *   - drivers/nvme/host/rdma.c|803| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+	 *   - drivers/nvme/host/tcp.c|1723| <<nvme_tcp_configure_admin_queue>> ctrl->admin_tagset = nvme_tcp_alloc_tagset(ctrl, true);
+	 *   - drivers/nvme/target/loop.c|360| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.admin_tagset = &ctrl->admin_tag_set;
+	 */
 	struct blk_mq_tag_set *admin_tagset;
+	/*
+	 * 链接着nvme_ns->list
+	 */
 	struct list_head namespaces;
+	/*
+	 * 在以下使用namespaces_rwsem:
+	 *   - drivers/nvme/host/core.c|1768| <<nvme_update_formats>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|1772| <<nvme_update_formats>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|3520| <<nvme_dev_user_cmd>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|3537| <<nvme_dev_user_cmd>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|3544| <<nvme_dev_user_cmd>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4095| <<nvme_find_get_ns>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4110| <<nvme_find_get_ns>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4244| <<nvme_alloc_ns>> down_write(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4246| <<nvme_alloc_ns>> up_write(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4303| <<nvme_ns_remove>> down_write(&ns->ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4305| <<nvme_ns_remove>> up_write(&ns->ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4342| <<nvme_remove_invalid_namespaces>> down_write(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4347| <<nvme_remove_invalid_namespaces>> up_write(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4500| <<nvme_scan_work>> down_write(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4502| <<nvme_scan_work>> up_write(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4534| <<nvme_remove_namespaces>> down_write(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4536| <<nvme_remove_namespaces>> up_write(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4867| <<nvme_init_ctrl>> init_rwsem(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4951| <<nvme_kill_queues>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4960| <<nvme_kill_queues>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4974| <<nvme_unfreeze>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4977| <<nvme_unfreeze>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4989| <<nvme_wait_freeze_timeout>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|4995| <<nvme_wait_freeze_timeout>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|5009| <<nvme_wait_freeze>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|5012| <<nvme_wait_freeze>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|5026| <<nvme_start_freeze>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|5029| <<nvme_start_freeze>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|5046| <<nvme_stop_queues>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|5049| <<nvme_stop_queues>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|5069| <<nvme_start_queues>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|5072| <<nvme_start_queues>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|5092| <<nvme_sync_queues>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/core.c|5104| <<nvme_sync_queues>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/multipath.c|129| <<nvme_kick_requeue_lists>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/multipath.c|134| <<nvme_kick_requeue_lists>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/multipath.c|170| <<nvme_mpath_clear_ctrl_paths>> down_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/multipath.c|174| <<nvme_mpath_clear_ctrl_paths>> up_read(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/multipath.c|549| <<nvme_update_ana_state>> down_write(&ctrl->namespaces_rwsem);
+	 *   - drivers/nvme/host/multipath.c|560| <<nvme_update_ana_state>> up_write(&ctrl->namespaces_rwsem);
+	 */
 	struct rw_semaphore namespaces_rwsem;
+	/*
+	 * 在以下使用ctrl_device:
+	 *   - drivers/nvme/host/core.c|5074| <<nvme_class_uevent>> container_of(dev, struct nvme_ctrl, ctrl_device);
+	 *   - drivers/nvme/host/core.c|5381| <<nvme_free_ctrl>> container_of(dev, struct nvme_ctrl, ctrl_device);
+	 *   - drivers/nvme/host/core.c|5467| <<nvme_init_ctrl>> device_initialize(&ctrl->ctrl_device);
+	 *   - drivers/nvme/host/core.c|5468| <<nvme_init_ctrl>> ctrl->device = &ctrl->ctrl_device;
+	 */
 	struct device ctrl_device;
 	struct device *device;	/* char device */
 	struct cdev cdev;
+	/*
+	 * 在以下使用reset_work:
+	 *   - drivers/nvme/host/core.c|133| <<nvme_try_sched_reset>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|143| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|155| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|169| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|2944| <<nvme_fc_reset_ctrl_work>> container_of(work, struct nvme_fc_ctrl, ctrl.reset_work);
+	 *   - drivers/nvme/host/fc.c|3107| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/fc.c|3204| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2572| <<nvme_reset_work>> container_of(work, struct nvme_dev, ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2798| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2830| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/pci.c|2902| <<nvme_reset_done>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2929| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|3094| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/rdma.c|1906| <<nvme_rdma_reset_ctrl_work>> container_of(work, struct nvme_rdma_ctrl, ctrl.reset_work);
+	 *   - drivers/nvme/host/rdma.c|2018| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|1945| <<nvme_reset_ctrl_work>> container_of(work, struct nvme_ctrl, reset_work);
+	 *   - drivers/nvme/host/tcp.c|2298| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|446| <<nvme_loop_reset_ctrl_work>> container_of(work, struct nvme_loop_ctrl, ctrl.reset_work);
+	 *   - drivers/nvme/target/loop.c|580| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	struct work_struct reset_work;
+	/*
+	 * 在以下使用delete_worl:
+	 *   - drivers/nvme/host/core.c|362| <<nvme_delete_ctrl_work>> container_of(work, struct nvme_ctrl, delete_work);
+	 *   - drivers/nvme/host/core.c|387| <<nvme_delete_ctrl>> if (!queue_work(nvme_delete_wq, &ctrl->delete_work))
+	 *   - drivers/nvme/host/core.c|5801| <<nvme_init_ctrl>> INIT_WORK(&ctrl->delete_work, nvme_delete_ctrl_work);
+	 */
 	struct work_struct delete_work;
+	/*
+	 * 在以下使用state_wq:
+	 *   - drivers/nvme/host/core.c|535| <<nvme_change_ctrl_state>> wake_up_all(&ctrl->state_wq);
+	 *   - drivers/nvme/host/core.c|571| <<nvme_wait_reset>> wait_event(ctrl->state_wq,
+	 *   - drivers/nvme/host/core.c|4515| <<nvme_init_ctrl>> init_waitqueue_head(&ctrl->state_wq);
+	 */
 	wait_queue_head_t state_wq;
 
 	struct nvme_subsystem *subsys;
+	/*
+	 * 链接入subsys->ctrls
+	 */
 	struct list_head subsys_entry;
 
 	struct opal_dev *opal_dev;
 
 	char name[12];
+	/*
+	 * Controller ID (CNTLID): Contains the NVM subsystem unique controller identifier
+	 * associated with the controller.
+	 */
 	u16 cntlid;
 
+	/*
+	 * 使用ctrl_config的地方:
+	 *   - drivers/nvme/host/core.c|2192| <<nvme_disable_ctrl>> ctrl->ctrl_config &= ~NVME_CC_SHN_MASK;
+	 *   - drivers/nvme/host/core.c|2193| <<nvme_disable_ctrl>> ctrl->ctrl_config &= ~NVME_CC_ENABLE;
+	 *   - drivers/nvme/host/core.c|2195| <<nvme_disable_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|2232| <<nvme_enable_ctrl>> ctrl->ctrl_config = NVME_CC_CSS_NVM;
+	 *   - drivers/nvme/host/core.c|2233| <<nvme_enable_ctrl>> ctrl->ctrl_config |= (page_shift - 12) << NVME_CC_MPS_SHIFT;
+	 *   - drivers/nvme/host/core.c|2234| <<nvme_enable_ctrl>> ctrl->ctrl_config |= NVME_CC_AMS_RR | NVME_CC_SHN_NONE;
+	 *   - drivers/nvme/host/core.c|2235| <<nvme_enable_ctrl>> ctrl->ctrl_config |= NVME_CC_IOSQES | NVME_CC_IOCQES;
+	 *   - drivers/nvme/host/core.c|2236| <<nvme_enable_ctrl>> ctrl->ctrl_config |= NVME_CC_ENABLE;
+	 *   - drivers/nvme/host/core.c|2238| <<nvme_enable_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|2251| <<nvme_shutdown_ctrl>> ctrl->ctrl_config &= ~NVME_CC_SHN_MASK;
+	 *   - drivers/nvme/host/core.c|2252| <<nvme_shutdown_ctrl>> ctrl->ctrl_config |= NVME_CC_SHN_NORMAL;
+	 *   - drivers/nvme/host/core.c|2254| <<nvme_shutdown_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|3959| <<nvme_ctrl_pp_status>> return ((ctrl->ctrl_config & NVME_CC_ENABLE) && (csts & NVME_CSTS_PP));
+	 *   - drivers/nvme/host/pci.c|2703| <<nvme_reset_work>> bool was_suspend = !!(dev->ctrl.ctrl_config & NVME_CC_SHN_NORMAL);
+	 *   - drivers/nvme/host/pci.c|2715| <<nvme_reset_work>> if (dev->ctrl.ctrl_config & NVME_CC_ENABLE)
+	 *
+	 * 最终会被写入bar0或bar1的NVME_REG_CC = 0x0014, Controller Configuration
+	 */
 	u32 ctrl_config;
 	u16 mtfa;
 	u32 queue_count;
@@ -234,7 +395,20 @@ struct nvme_ctrl {
 	u16 nssa;
 	u16 nr_streams;
 	u16 sqsize;
+	/*
+	 * 在以下使用nvme_ctrl->max_namespaces:
+	 *   - drivers/nvme/host/core.c|3614| <<nvme_init_identify>> ctrl->max_namespaces = le32_to_cpu(id->mnan);
+	 *   - drivers/nvme/host/multipath.c|770| <<nvme_mpath_init>> ctrl->ana_log_size += ctrl->max_namespaces * sizeof(__le32);
+	 */
 	u32 max_namespaces;
+	/*
+	 * 在以下使用abort_limit:
+	 *   - drivers/nvme/host/core.c|3345| <<nvme_init_identify>> atomic_set(&ctrl->abort_limit, id->acl + 1);
+	 *   - drivers/nvme/host/pci.c|1715| <<abort_endio>> atomic_inc(&nvmeq->dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1862| <<nvme_timeout>> if (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {
+	 *   - drivers/nvme/host/pci.c|1863| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1887| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 */
 	atomic_t abort_limit;
 	u8 vwc;
 	u32 vs;
@@ -247,16 +421,83 @@ struct nvme_ctrl {
 	u32 oaes;
 	u32 aen_result;
 	u32 ctratt;
+	/*
+	 * 使用nvme_ctrl->shutdown_timeout的地方:
+	 *   - drivers/nvme/host/core.c|2820| <<nvme_shutdown_ctrl>> unsigned long timeout = jiffies + (ctrl->shutdown_timeout * HZ);
+	 *   - drivers/nvme/host/core.c|3637| <<nvme_init_identify>> ctrl->shutdown_timeout = clamp_t(unsigned int , transition_time,
+	 *   - drivers/nvme/host/core.c|3640| <<nvme_init_identify>> if (ctrl->shutdown_timeout != shutdown_timeout)
+	 *   - drivers/nvme/host/core.c|3643| <<nvme_init_identify>> ctrl->shutdown_timeout);
+	 *   - drivers/nvme/host/core.c|3645| <<nvme_init_identify>> ctrl->shutdown_timeout = shutdown_timeout;
+	 */
 	unsigned int shutdown_timeout;
+	/*
+	 * 设置nvme_ctrl->kato的地方 (没有pci):
+	 *   - drivers/nvme/host/fc.c|3119| <<nvme_fc_init_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/host/rdma.c|2023| <<nvme_rdma_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/host/tcp.c|2293| <<nvme_tcp_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 *   - drivers/nvme/target/loop.c|593| <<nvme_loop_create_ctrl>> ctrl->ctrl.kato = opts->kato;
+	 * 部分使用nvme_ctrl->kato的地方:
+	 *   - drivers/nvme/host/core.c|1285| <<nvme_keep_alive_end_io>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1297| <<nvme_keep_alive>> rq->timeout = ctrl->kato * HZ;
+	 *   - drivers/nvme/host/core.c|1315| <<nvme_keep_alive_work>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1366| <<nvme_start_keep_alive>> if (unlikely(ctrl->kato == 0))
+	 *   - drivers/nvme/host/core.c|1369| <<nvme_start_keep_alive>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1381| <<nvme_stop_keep_alive>> if (unlikely(ctrl->kato == 0))
+	 *   - drivers/nvme/host/core.c|4904| <<nvme_start_ctrl>> if (ctrl->kato)
+	 */
 	unsigned int kato;
 	bool subsystem;
 	unsigned long quirks;
 	struct nvme_id_power_state psd[32];
+	/*
+	 * 在以下使用effects:
+	 *   - drivers/nvme/host/core.c|2063| <<nvme_passthru_start>> if (ctrl->effects)
+	 *   - drivers/nvme/host/core.c|2064| <<nvme_passthru_start>> effects = le32_to_cpu(ctrl->effects->iocs[opcode]);
+	 *   - drivers/nvme/host/core.c|2072| <<nvme_passthru_start>> if (ctrl->effects)
+	 *   - drivers/nvme/host/core.c|2073| <<nvme_passthru_start>> effects = le32_to_cpu(ctrl->effects->acs[opcode]);
+	 *   - drivers/nvme/host/core.c|3750| <<nvme_get_effects_log>> if (!ctrl->effects)
+	 *   - drivers/nvme/host/core.c|3751| <<nvme_get_effects_log>> ctrl->effects = kzalloc(sizeof(*ctrl->effects), GFP_KERNEL);
+	 *   - drivers/nvme/host/core.c|3753| <<nvme_get_effects_log>> if (!ctrl->effects)
+	 *   - drivers/nvme/host/core.c|3757| <<nvme_get_effects_log>> ctrl->effects, sizeof(*ctrl->effects), 0);
+	 *   - drivers/nvme/host/core.c|3759| <<nvme_get_effects_log>> kfree(ctrl->effects);
+	 *   - drivers/nvme/host/core.c|3760| <<nvme_get_effects_log>> ctrl->effects = NULL;
+	 *   - drivers/nvme/host/core.c|5616| <<nvme_free_ctrl>> kfree(ctrl->effects);
+	 */
 	struct nvme_effects_log *effects;
 	struct work_struct scan_work;
+	/*
+	 * 在以下使用async_event_work:
+	 *   - drivers/nvme/host/core.c|1580| <<nvme_enable_aen>> queue_work(nvme_wq, &ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4345| <<nvme_async_event_work>> container_of(work, struct nvme_ctrl, async_event_work);
+	 *   - drivers/nvme/host/core.c|4483| <<nvme_complete_async_event>> queue_work(nvme_wq, &ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4500| <<nvme_stop_ctrl>> flush_work(&ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4589| <<nvme_init_ctrl>> INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
+	 */
 	struct work_struct async_event_work;
+	/*
+	 * 在以下使用nvme_ctrl->ka_work:
+	 *   - drivers/nvme/host/core.c|1285| <<nvme_keep_alive_end_io>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1308| <<nvme_keep_alive_work>> struct nvme_ctrl, ka_work);
+	 *   - drivers/nvme/host/core.c|1315| <<nvme_keep_alive_work>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1384| <<nvme_start_keep_alive>> schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+	 *   - drivers/nvme/host/core.c|1399| <<nvme_stop_keep_alive>> cancel_delayed_work_sync(&ctrl->ka_work);
+	 *   - drivers/nvme/host/core.c|5026| <<nvme_init_ctrl>> INIT_DELAYED_WORK(&ctrl->ka_work, nvme_keep_alive_work);
+	 */
 	struct delayed_work ka_work;
+	/*
+	 * 在以下使用ka_cmd:
+	 *   - drivers/nvme/host/core.c|1300| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+	 *   - drivers/nvme/host/core.c|5038| <<nvme_init_ctrl>> memset(&ctrl->ka_cmd, 0, sizeof(ctrl->ka_cmd));
+	 *   - drivers/nvme/host/core.c|5039| <<nvme_init_ctrl>> ctrl->ka_cmd.common.opcode = nvme_admin_keep_alive;
+	 */
 	struct nvme_command ka_cmd;
+	/*
+	 * 在以下使用fw_act_work:
+	 *   - drivers/nvme/host/core.c|5268| <<nvme_fw_act_work>> struct nvme_ctrl, fw_act_work);
+	 *   - drivers/nvme/host/core.c|5334| <<nvme_handle_aen_notice>> queue_work(nvme_wq, &ctrl->fw_act_work);
+	 *   - drivers/nvme/host/core.c|5418| <<nvme_stop_ctrl>> cancel_work_sync(&ctrl->fw_act_work);
+	 *   - drivers/nvme/host/core.c|5563| <<nvme_init_ctrl>> INIT_WORK(&ctrl->fw_act_work, nvme_fw_act_work);
+	 */
 	struct work_struct fw_act_work;
 	unsigned long events;
 
@@ -265,11 +506,61 @@ struct nvme_ctrl {
 	u8 anacap;
 	u8 anatt;
 	u32 anagrpmax;
+	/*
+	 * 在以下使用nvme_ctrl->nanagrpid:
+	 *   - drivers/nvme/host/multipath.c|763| <<nvme_mpath_init>> ctrl->nanagrpid = le32_to_cpu(id->nanagrpid);
+	 *   - drivers/nvme/host/multipath.c|769| <<nvme_mpath_init>> ctrl->nanagrpid * sizeof(struct nvme_ana_group_desc);
+	 *
+	 * target在以下响应:
+	 *   - drivers/nvme/target/admin-cmd.c|429| <<nvmet_execute_identify_ctrl>> id->nanagrpid = cpu_to_le32(NVMET_MAX_ANAGRPS);
+	 */
 	u32 nanagrpid;
 	struct mutex ana_lock;
+	/*
+	 * 在以下使用nvme_ctrl->ana_log_buf:
+	 *   - drivers/nvme/host/core.c|5023| <<nvme_handle_aen_notice>> if (!ctrl->ana_log_buf)
+	 *   - drivers/nvme/host/multipath.c|100| <<nvme_failover_req>> if (ns->ctrl->ana_log_buf) {
+	 *   - drivers/nvme/host/multipath.c|483| <<nvme_parse_ana_log>> void *base = ctrl->ana_log_buf;
+	 *   - drivers/nvme/host/multipath.c|489| <<nvme_parse_ana_log>> for (i = 0; i < le16_to_cpu(ctrl->ana_log_buf->ngrps); i++) {
+	 *   - drivers/nvme/host/multipath.c|594| <<nvme_read_ana_log>> ctrl->ana_log_buf, ctrl->ana_log_size, 0);
+	 *   - drivers/nvme/host/multipath.c|786| <<nvme_mpath_init>> ctrl->ana_log_buf = kmalloc(ctrl->ana_log_size, GFP_KERNEL);
+	 *   - drivers/nvme/host/multipath.c|787| <<nvme_mpath_init>> if (!ctrl->ana_log_buf) {
+	 *   - drivers/nvme/host/multipath.c|797| <<nvme_mpath_init>> kfree(ctrl->ana_log_buf);
+	 *   - drivers/nvme/host/multipath.c|798| <<nvme_mpath_init>> ctrl->ana_log_buf = NULL;
+	 *   - drivers/nvme/host/multipath.c|805| <<nvme_mpath_uninit>> kfree(ctrl->ana_log_buf);
+	 *   - drivers/nvme/host/multipath.c|806| <<nvme_mpath_uninit>> ctrl->ana_log_buf = NULL;
+	 *   - drivers/nvme/host/nvme.h|872| <<nvme_ctrl_use_ana>> return ctrl->ana_log_buf != NULL;
+	 */
 	struct nvme_ana_rsp_hdr *ana_log_buf;
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/multipath.c|580| <<nvme_parse_ana_log>> if (WARN_ON_ONCE(offset > ctrl->ana_log_size - sizeof(*desc)))
+	 *   - drivers/nvme/host/multipath.c|596| <<nvme_parse_ana_log>> if (WARN_ON_ONCE(offset > ctrl->ana_log_size - nsid_buf_size))
+	 *   - drivers/nvme/host/multipath.c|697| <<nvme_read_ana_log>> ctrl->ana_log_buf, ctrl->ana_log_size, 0);
+	 *   - drivers/nvme/host/multipath.c|940| <<nvme_mpath_init>> ctrl->ana_log_size = sizeof(struct nvme_ana_rsp_hdr) +
+	 *   - drivers/nvme/host/multipath.c|947| <<nvme_mpath_init>> ctrl->ana_log_size += ctrl->max_namespaces * sizeof(__le32);
+	 *   - drivers/nvme/host/multipath.c|949| <<nvme_mpath_init>> if (ctrl->ana_log_size > ctrl->max_hw_sectors << SECTOR_SHIFT) {
+	 *   - drivers/nvme/host/multipath.c|952| <<nvme_mpath_init>> ctrl->ana_log_size,
+	 *   - drivers/nvme/host/multipath.c|971| <<nvme_mpath_init>> ctrl->ana_log_buf = kmalloc(ctrl->ana_log_size, GFP_KERNEL);
+	 */
 	size_t ana_log_size;
+	/*
+	 * 在以下使用anatt_timer:
+	 *   - drivers/nvme/host/multipath.c|629| <<nvme_read_ana_log>> mod_timer(&ctrl->anatt_timer, ctrl->anatt * HZ * 2 + jiffies);
+	 *   - drivers/nvme/host/multipath.c|631| <<nvme_read_ana_log>> del_timer_sync(&ctrl->anatt_timer);
+	 *   - drivers/nvme/host/multipath.c|654| <<nvme_anatt_timeout>> struct nvme_ctrl *ctrl = from_timer(ctrl, t, anatt_timer);
+	 *   - drivers/nvme/host/multipath.c|668| <<nvme_mpath_stop>> del_timer_sync(&ctrl->anatt_timer);
+	 *   - drivers/nvme/host/multipath.c|821| <<nvme_mpath_init>> timer_setup(&ctrl->anatt_timer, nvme_anatt_timeout, 0);
+	 */
 	struct timer_list anatt_timer;
+	/*
+	 * 在以下使用nvme_ctrl->ana_work:
+	 *   - drivers/nvme/host/core.c|5025| <<nvme_handle_aen_notice>> queue_work(nvme_wq, &ctrl->ana_work);
+	 *   - drivers/nvme/host/multipath.c|102| <<nvme_failover_req>> queue_work(nvme_wq, &ns->ctrl->ana_work);
+	 *   - drivers/nvme/host/multipath.c|627| <<nvme_ana_work>> struct nvme_ctrl *ctrl = container_of(work, struct nvme_ctrl, ana_work);
+	 *   - drivers/nvme/host/multipath.c|645| <<nvme_mpath_stop>> cancel_work_sync(&ctrl->ana_work);
+	 *   - drivers/nvme/host/multipath.c|781| <<nvme_mpath_init>> INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+	 */
 	struct work_struct ana_work;
 #endif
 
@@ -310,19 +601,61 @@ struct nvme_subsystem {
 	 * a separate refcount.
 	 */
 	struct kref		ref;
+	/*
+	 * 链接到nvme_subsystems
+	 */
 	struct list_head	entry;
 	struct mutex		lock;
 	struct list_head	ctrls;
+	/*
+	 * 使用nsheads的地方:
+	 *   - drivers/nvme/host/core.c|3615| <<nvme_alloc_ns_head>> list_add_tail(&head->entry, &ctrl->subsys->nsheads);
+	 *   - drivers/nvme/host/core.c|2857| <<nvme_init_subsystem>> INIT_LIST_HEAD(&subsys->nsheads);
+	 *   - drivers/nvme/host/core.c|3549| <<__nvme_find_ns_head>> list_for_each_entry(h, &subsys->nsheads, entry) {
+	 *   - drivers/nvme/host/core.c|3564| <<__nvme_check_ids>> list_for_each_entry(h, &subsys->nsheads, entry) {
+	 *   - drivers/nvme/host/multipath.c|20| <<nvme_mpath_unfreeze>> list_for_each_entry(h, &subsys->nsheads, entry)
+	 *   - drivers/nvme/host/multipath.c|30| <<nvme_mpath_wait_freeze>> list_for_each_entry(h, &subsys->nsheads, entry)
+	 *   - drivers/nvme/host/multipath.c|40| <<nvme_mpath_start_freeze>> list_for_each_entry(h, &subsys->nsheads, entry)
+	 *
+	 * 管理所有的nvme_ns_head->entry
+	 */
 	struct list_head	nsheads;
+	/*
+	 * 在以下使用nvme_subsystem->subnqn:
+	 *   - drivers/nvme/host/core.c|3018| <<nvme_init_subnqn>> strlcpy(subsys->subnqn, id->subnqn, NVMF_NQN_SIZE);
+	 *   - drivers/nvme/host/core.c|3036| <<nvme_init_subnqn>> off = snprintf(subsys->subnqn, NVMF_NQN_SIZE,
+	 *   - drivers/nvme/host/core.c|3039| <<nvme_init_subnqn>> memcpy(subsys->subnqn + off, id->sn, sizeof(id->sn));
+	 *   - drivers/nvme/host/core.c|3041| <<nvme_init_subnqn>> memcpy(subsys->subnqn + off, id->mn, sizeof(id->mn));
+	 *   - drivers/nvme/host/core.c|3043| <<nvme_init_subnqn>> memset(subsys->subnqn + off, 0, sizeof(subsys->subnqn) - off);
+	 *   - drivers/nvme/host/core.c|3093| <<__nvme_find_get_subsystem>> if (strcmp(subsys->subnqn, subsysnqn))
+	 *   - drivers/nvme/host/core.c|3114| <<nvme_subsys_show_nqn>> return snprintf(buf, PAGE_SIZE, "%s\n", subsys->subnqn);
+	 *   - drivers/nvme/host/core.c|3224| <<nvme_init_subsystem>> found = __nvme_find_get_subsystem(subsys->subnqn);
+	 *   - drivers/nvme/host/core.c|3846| <<nvme_sysfs_show_subsysnqn>> return snprintf(buf, PAGE_SIZE, "%s\n", ctrl->subsys->subnqn);
+	 */
 	char			subnqn[NVMF_NQN_SIZE];
 	char			serial[20];
 	char			model[40];
 	char			firmware_rev[8];
+	/*
+	 * Bit 0: If set to '1', then the namespace may be attached to two or more controllers in the
+	 * NVM subsystem concurrently (i.e., may be a shared namespace). If cleared to '0', then
+	 * the namespace is a private namespace and is able to be attached to only one controller
+	 * at a time.
+	 */
 	u8			cmic;
 	u16			vendor_id;
 	u16			awupf;	/* 0's based awupf value. */
 	struct ida		ns_ida;
 #ifdef CONFIG_NVME_MULTIPATH
+	/*
+	 * 在以下使用iopolicy:
+	 *   - drivers/nvme/host/multipath.c|660| <<global>> SUBSYS_ATTR_RW(iopolicy, S_IRUGO | S_IWUSR,
+	 *   - drivers/nvme/host/core.c|3329| <<nvme_init_subsystem>> subsys->iopolicy = NVME_IOPOLICY_NUMA;
+	 *   - drivers/nvme/host/multipath.c|199| <<__nvme_find_path>> if (READ_ONCE(head->subsys->iopolicy) == NVME_IOPOLICY_NUMA)
+	 *   - drivers/nvme/host/multipath.c|289| <<nvme_find_path>> if (READ_ONCE(head->subsys->iopolicy) == NVME_IOPOLICY_RR && ns)
+	 *   - drivers/nvme/host/multipath.c|641| <<nvme_subsys_iopolicy_show>> nvme_iopolicy_names[READ_ONCE(subsys->iopolicy)]);
+	 *   - drivers/nvme/host/multipath.c|653| <<nvme_subsys_iopolicy_store>> WRITE_ONCE(subsys->iopolicy, i);
+	 */
 	enum nvme_iopolicy	iopolicy;
 #endif
 };
@@ -344,25 +677,68 @@ struct nvme_ns_ids {
  * only ever has a single entry for private namespaces.
  */
 struct nvme_ns_head {
+	/* 管理所有的nvme_ns->siblings */
 	struct list_head	list;
 	struct srcu_struct      srcu;
 	struct nvme_subsystem	*subsys;
 	unsigned		ns_id;
 	struct nvme_ns_ids	ids;
+	/*
+	 * 链接在nvme_subsystem->nheads
+	 */
 	struct list_head	entry;
 	struct kref		ref;
 	int			instance;
 #ifdef CONFIG_NVME_MULTIPATH
 	struct gendisk		*disk;
+	/*
+	 * 在以下使用requeue_list:
+	 *   - drivers/nvme/host/multipath.c|116| <<nvme_failover_req>> blk_steal_bios(&ns->head->requeue_list, req);
+	 *   - drivers/nvme/host/multipath.c|433| <<nvme_ns_head_make_request>> bio_list_add(&head->requeue_list, bio);
+	 *   - drivers/nvme/host/multipath.c|453| <<nvme_requeue_work>> next = bio_list_get(&head->requeue_list);
+	 *   - drivers/nvme/host/multipath.c|479| <<nvme_mpath_alloc_disk>> bio_list_init(&head->requeue_list);
+	 */
 	struct bio_list		requeue_list;
+	/*
+	 * 在以下:
+	 *   - drivers/nvme/host/multipath.c|77| <<nvme_failover_req>> spin_lock_irqsave(&ns->head->requeue_lock, flags);
+	 *   - drivers/nvme/host/multipath.c|79| <<nvme_failover_req>> spin_unlock_irqrestore(&ns->head->requeue_lock, flags);
+	 *   - drivers/nvme/host/multipath.c|343| <<nvme_ns_head_make_request>> spin_lock_irq(&head->requeue_lock);
+	 *   - drivers/nvme/host/multipath.c|345| <<nvme_ns_head_make_request>> spin_unlock_irq(&head->requeue_lock);
+	 *   - drivers/nvme/host/multipath.c|363| <<nvme_requeue_work>> spin_lock_irq(&head->requeue_lock);
+	 *   - drivers/nvme/host/multipath.c|365| <<nvme_requeue_work>> spin_unlock_irq(&head->requeue_lock);
+	 *   - drivers/nvme/host/multipath.c|391| <<nvme_mpath_alloc_disk>> spin_lock_init(&head->requeue_lock);
+	 */
 	spinlock_t		requeue_lock;
+	/*
+	 * 在以下使用requeue_work:
+	 *   - drivers/nvme/host/multipath.c|164| <<nvme_failover_req>> kblockd_schedule_work(&ns->head->requeue_work);
+	 *   - drivers/nvme/host/multipath.c|178| <<nvme_kick_requeue_lists>> kblockd_schedule_work(&ns->head->requeue_work);
+	 *   - drivers/nvme/host/multipath.c|245| <<nvme_mpath_clear_ctrl_paths>> kblockd_schedule_work(&ns->head->requeue_work);
+	 *   - drivers/nvme/host/multipath.c|481| <<nvme_mpath_alloc_disk>> INIT_WORK(&head->requeue_work, nvme_requeue_work);
+	 *   - drivers/nvme/host/multipath.c|556| <<nvme_mpath_set_live>> kblockd_schedule_work(&ns->head->requeue_work);
+	 *   - drivers/nvme/host/multipath.c|906| <<nvme_mpath_remove_disk>> kblockd_schedule_work(&head->requeue_work);
+	 *   - drivers/nvme/host/multipath.c|907| <<nvme_mpath_remove_disk>> flush_work(&head->requeue_work);
+	 *   - drivers/nvme/host/nvme.h|996| <<nvme_mpath_check_last_path>> kblockd_schedule_work(&head->requeue_work);
+	 */
 	struct work_struct	requeue_work;
 	struct mutex		lock;
+	/*
+	 * 在以下使用current_path:
+	 *   - drivers/nvme/host/multipath.c|156| <<nvme_mpath_clear_current_path>> if (ns == rcu_access_pointer(head->current_path[node])) {
+	 *   - drivers/nvme/host/multipath.c|157| <<nvme_mpath_clear_current_path>> rcu_assign_pointer(head->current_path[node], NULL);
+	 *   - drivers/nvme/host/multipath.c|220| <<__nvme_find_path>> rcu_assign_pointer(head->current_path[node], found);
+	 *   - drivers/nvme/host/multipath.c|263| <<nvme_round_robin_path>> rcu_assign_pointer(head->current_path[node], found);
+	 *   - drivers/nvme/host/multipath.c|278| <<nvme_find_path>> ns = srcu_dereference(head->current_path[node], &head->srcu);
+	 */
 	struct nvme_ns __rcu	*current_path[];
 #endif
 };
 
 struct nvme_ns {
+	/*
+	 * 链接在ctrl->namespaces
+	 */
 	struct list_head list;
 
 	struct nvme_ctrl *ctrl;
@@ -370,13 +746,31 @@ struct nvme_ns {
 	struct gendisk *disk;
 #ifdef CONFIG_NVME_MULTIPATH
 	enum nvme_ana_state ana_state;
+	/*
+	 * 在以下使用nvme_ns->ana_grpid:
+	 *   - drivers/nvme/host/multipath.c|770| <<global>> DEVICE_ATTR_RO(ana_grpid);
+	 *   - drivers/nvme/target/configfs.c|493| <<global>> CONFIGFS_ATTR(nvmet_ns_, ana_grpid);
+	 *   - drivers/nvme/host/multipath.c|574| <<nvme_update_ns_ana_state>> ns->ana_grpid = le32_to_cpu(desc->grpid);
+	 *   - drivers/nvme/host/multipath.c|768| <<ana_grpid_show>> return sprintf(buf, "%d\n", nvme_get_ns_from_dev(dev)->ana_grpid);
+	 *   - drivers/nvme/host/multipath.c|790| <<nvme_set_ns_ana_state>> if (ns->ana_grpid == le32_to_cpu(desc->grpid)) {
+	 *   - drivers/nvme/host/multipath.c|806| <<nvme_mpath_add_disk>> ns->ana_grpid = le32_to_cpu(id->anagrpid);
+	 */
 	u32 ana_grpid;
 #endif
+	/*
+	 * 链接在nvme_ns_head->list
+	 */
 	struct list_head siblings;
 	struct nvm_dev *ndev;
 	struct kref kref;
 	struct nvme_ns_head *head;
 
+	/*
+	 * 在以下设置lba_shift:
+	 *   - drivers/nvme/host/core.c|2307| <<__nvme_revalidate_disk>> ns->lba_shift = id->lbaf[id->flbas & NVME_NS_FLBAS_LBA_MASK].ds;
+	 *   - drivers/nvme/host/core.c|2309| <<__nvme_revalidate_disk>> ns->lba_shift = 9;
+	 *   - drivers/nvme/host/core.c|4222| <<nvme_alloc_ns>> ns->lba_shift = 9;
+	 */
 	int lba_shift;
 	u16 ms;
 	u16 sgs;
@@ -384,11 +778,39 @@ struct nvme_ns {
 	bool ext;
 	u8 pi_type;
 	unsigned long flags;
+/*
+ * 在以下使用NVME_NS_REMOVING:
+ *   - drivers/nvme/host/core.c|4284| <<nvme_ns_remove>> if (test_and_set_bit(NVME_NS_REMOVING, &ns->flags))
+ *   - drivers/nvme/host/multipath.c|182| <<nvme_path_is_disabled>> test_bit(NVME_NS_REMOVING, &ns->flags);
+ */
 #define NVME_NS_REMOVING	0
+/*
+ * 在以下使用NVME_NS_DEAD:
+ *   - drivers/nvme/host/core.c|181| <<nvme_set_queue_dying>> if (!ns->disk || test_and_set_bit(NVME_NS_DEAD, &ns->flags))
+ *   - drivers/nvme/host/core.c|2504| <<nvme_revalidate_disk>> if (test_bit(NVME_NS_DEAD, &ns->flags)) {
+ *   - drivers/nvme/host/core.c|4679| <<nvme_remove_invalid_namespaces>> if (ns->head->ns_id > nsid || test_bit(NVME_NS_DEAD, &ns->flags))
+ */
 #define NVME_NS_DEAD     	1
+	/*
+	 * 在以下使用NVME_NS_ANA_PENDING:
+	 *   - drivers/nvme/host/multipath.c|123| <<nvme_failover_req>> set_bit(NVME_NS_ANA_PENDING, &ns->flags);
+	 *   - drivers/nvme/host/multipath.c|222| <<nvme_path_is_disabled>> test_bit(NVME_NS_ANA_PENDING, &ns->flags) ||
+	 *   - drivers/nvme/host/multipath.c|576| <<nvme_update_ns_ana_state>> clear_bit(NVME_NS_ANA_PENDING, &ns->flags);
+	 */
 #define NVME_NS_ANA_PENDING	2
+	/*
+	 * 在以下使用noiob:
+	 *   - drivers/nvme/host/core.c|2122| <<nvme_set_chunk_size>> u32 chunk_size = nvme_lba_to_sect(ns, ns->noiob);
+	 *   - drivers/nvme/host/core.c|2319| <<__nvme_revalidate_disk>> if (ns->noiob)
+	 */
 	u16 noiob;
 
+	/*
+	 * 使用nvme_ns->fault_inject的地方:
+	 *   - drivers/nvme/host/core.c|4274| <<nvme_alloc_ns>> nvme_fault_inject_init(&ns->fault_inject, ns->disk->disk_name);
+	 *   - drivers/nvme/host/core.c|4316| <<nvme_ns_remove>> nvme_fault_inject_fini(&ns->fault_inject);
+	 *   - drivers/nvme/host/fault_inject.c|122| <<nvme_should_fail>> fault_inject = &ns->fault_inject;
+	 */
 	struct nvme_fault_inject fault_inject;
 
 };
@@ -425,10 +847,18 @@ static inline void nvme_fault_inject_fini(struct nvme_fault_inject *fault_inj)
 static inline void nvme_should_fail(struct request *req) {}
 #endif
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4057| <<nvme_dev_ioctl>> return nvme_reset_subsystem(ctrl);
+ */
 static inline int nvme_reset_subsystem(struct nvme_ctrl *ctrl)
 {
 	if (!ctrl->subsystem)
 		return -ENOTTY;
+	/*
+	 * 只在以下使用NVME_REG_NSSR:
+	 *   - drivers/nvme/host/nvme.h|830| <<nvme_reset_subsystem>> return ctrl->ops->reg_write32(ctrl, NVME_REG_NSSR, 0x4E564D65);
+	 */
 	return ctrl->ops->reg_write32(ctrl, NVME_REG_NSSR, 0x4E564D65);
 }
 
@@ -448,6 +878,17 @@ static inline sector_t nvme_lba_to_sect(struct nvme_ns *ns, u64 lba)
 	return lba << (ns->lba_shift - SECTOR_SHIFT);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|1702| <<nvme_fc_fcpio_done>> nvme_end_request(rq, status, result);
+ *   - drivers/nvme/host/pci.c|1167| <<nvme_handle_cqe>> nvme_end_request(req, cqe->status, cqe->result);
+ *   - drivers/nvme/host/rdma.c|1126| <<nvme_rdma_inv_rkey_done>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/rdma.c|1335| <<nvme_rdma_send_done>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/rdma.c|1478| <<nvme_rdma_process_nvme_rsp>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/tcp.c|442| <<nvme_tcp_process_nvme_cqe>> nvme_end_request(rq, cqe->status, cqe->result);
+ *   - drivers/nvme/host/tcp.c|632| <<nvme_tcp_end_request>> nvme_end_request(rq, cpu_to_le16(status << 1), res);
+ *   - drivers/nvme/target/loop.c|119| <<nvme_loop_queue_response>> nvme_end_request(rq, cqe->status, cqe->result);
+ */
 static inline void nvme_end_request(struct request *req, __le16 status,
 		union nvme_result result)
 {
@@ -460,16 +901,59 @@ static inline void nvme_end_request(struct request *req, __le16 status,
 	blk_mq_complete_request(req);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|203| <<nvme_delete_ctrl_sync>> nvme_get_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|1621| <<nvme_handle_ctrl_ioctl>> nvme_get_ctrl(ns->ctrl);
+ *   - drivers/nvme/host/core.c|3686| <<nvme_alloc_ns>> nvme_get_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|3184| <<nvme_fc_init_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|3131| <<nvme_remove_dead_ctrl>> nvme_get_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3489| <<nvme_probe>> nvme_get_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|2046| <<nvme_rdma_create_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|2357| <<nvme_tcp_create_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|624| <<nvme_loop_create_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ */
 static inline void nvme_get_ctrl(struct nvme_ctrl *ctrl)
 {
 	get_device(ctrl->device);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|268| <<nvme_do_delete_ctrl>> nvme_put_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|322| <<nvme_delete_ctrl_sync>> nvme_put_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|665| <<nvme_free_ns>> nvme_put_ctrl(ns->ctrl);
+ *   - drivers/nvme/host/core.c|1996| <<nvme_handle_ctrl_ioctl>> nvme_put_ctrl(ctrl);
+ *   - drivers/nvme/host/fabrics.c|1136| <<nvmf_dev_release>> nvme_put_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|3187| <<nvme_fc_init_ctrl>> nvme_put_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/fc.c|3214| <<nvme_fc_init_ctrl>> nvme_put_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|3985| <<nvme_remove_dead_ctrl>> nvme_put_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|4205| <<nvme_remove_dead_ctrl_work>> nvme_put_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|4323| <<nvme_async_probe>> nvme_put_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|4518| <<nvme_remove>> nvme_put_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|2056| <<nvme_rdma_create_ctrl>> nvme_put_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|2367| <<nvme_tcp_create_ctrl>> nvme_put_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|488| <<nvme_loop_reset_ctrl_work>> nvme_put_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|644| <<nvme_loop_create_ctrl>> nvme_put_ctrl(&ctrl->ctrl);
+ */
 static inline void nvme_put_ctrl(struct nvme_ctrl *ctrl)
 {
+	/*
+	 * 在以下使用nvme_free_ctrl():
+	 *   - drivers/nvme/host/core.c|4805| <<nvme_init_ctrl>> ctrl->device->release = nvme_free_ctrl;
+	 *
+	 * struct nvme_ctrl:
+	 *   - struct device *dev;
+	 *   - struct device *device;
+	 */
 	put_device(ctrl->device);
 }
 
+/*
+ * 关于admin ring buffer和async event, nvme的admin queue的tagset
+ * 有30个元素, 但是ring buffer有32个,除了一个用来表示full, 另外
+ * 一个用在async event
+ */
 static inline bool nvme_is_aen_req(u16 qid, __u16 command_id)
 {
 	return !qid && command_id >= NVME_AQ_BLK_MQ_DEPTH;
@@ -540,8 +1024,31 @@ extern const struct attribute_group *nvme_ns_id_attr_groups[];
 extern const struct block_device_operations nvme_ns_head_ops;
 
 #ifdef CONFIG_NVME_MULTIPATH
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3958| <<nvme_ns_id_attrs_are_visible>> if (!nvme_ctrl_use_ana(nvme_get_ns_from_dev(dev)->ctrl))
+ *   - drivers/nvme/host/multipath.c|755| <<nvme_mpath_stop>> if (!nvme_ctrl_use_ana(ctrl))
+ *   - drivers/nvme/host/multipath.c|852| <<nvme_mpath_add_disk>> if (nvme_ctrl_use_ana(ns->ctrl)) {
+ *
+ * 判断ctrl->ana_log_buf是否为NULL
+ */
 static inline bool nvme_ctrl_use_ana(struct nvme_ctrl *ctrl)
 {
+	/*
+	 * 在以下使用nvme_ctrl->ana_log_buf:
+	 *   - drivers/nvme/host/core.c|5023| <<nvme_handle_aen_notice>> if (!ctrl->ana_log_buf)
+	 *   - drivers/nvme/host/multipath.c|100| <<nvme_failover_req>> if (ns->ctrl->ana_log_buf) {
+	 *   - drivers/nvme/host/multipath.c|483| <<nvme_parse_ana_log>> void *base = ctrl->ana_log_buf;
+	 *   - drivers/nvme/host/multipath.c|489| <<nvme_parse_ana_log>> for (i = 0; i < le16_to_cpu(ctrl->ana_log_buf->ngrps); i++) {
+	 *   - drivers/nvme/host/multipath.c|594| <<nvme_read_ana_log>> ctrl->ana_log_buf, ctrl->ana_log_size, 0);
+	 *   - drivers/nvme/host/multipath.c|786| <<nvme_mpath_init>> ctrl->ana_log_buf = kmalloc(ctrl->ana_log_size, GFP_KERNEL);
+	 *   - drivers/nvme/host/multipath.c|787| <<nvme_mpath_init>> if (!ctrl->ana_log_buf) {
+	 *   - drivers/nvme/host/multipath.c|797| <<nvme_mpath_init>> kfree(ctrl->ana_log_buf);
+	 *   - drivers/nvme/host/multipath.c|798| <<nvme_mpath_init>> ctrl->ana_log_buf = NULL;
+	 *   - drivers/nvme/host/multipath.c|805| <<nvme_mpath_uninit>> kfree(ctrl->ana_log_buf);
+	 *   - drivers/nvme/host/multipath.c|806| <<nvme_mpath_uninit>> ctrl->ana_log_buf = NULL;
+	 *   - drivers/nvme/host/nvme.h|872| <<nvme_ctrl_use_ana>> return ctrl->ana_log_buf != NULL;
+	 */
 	return ctrl->ana_log_buf != NULL;
 }
 
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index 365a2ddbeaa7..f32004c4d7ae 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -28,6 +28,15 @@
 #include "trace.h"
 #include "nvme.h"
 
+/*
+ * /sys/block/nvme0n1/device/reset_controller
+ *
+ * /sys/block/nvme0n1/device/rescan_controller
+ */
+
+/*
+ * sqes用来表示sq的每个entry的大小
+ */
 #define SQ_SIZE(q)	((q)->q_depth << (q)->sqes)
 #define CQ_SIZE(q)	((q)->q_depth * sizeof(struct nvme_completion))
 
@@ -37,21 +46,68 @@
  * These can be higher, but we need to ensure that any command doesn't
  * require an sg allocation that needs more than a page of data.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2708| <<nvme_reset_work>> NVME_MAX_KB_SZ << 1, dma_max_mapping_size(dev->dev) >> 9);
+ *   - drivers/nvme/host/pci.c|2948| <<nvme_probe>> alloc_size = nvme_pci_iod_alloc_size(dev, NVME_MAX_KB_SZ,
+ */
 #define NVME_MAX_KB_SZ	4096
 #define NVME_MAX_SEGS	127
 
+/*
+ * 在以下使用use_threaded_interrupts:
+ *   - drivers/nvme/host/pci.c|44| <<global>> module_param(use_threaded_interrupts, int , 0);
+ *   - drivers/nvme/host/pci.c|1614| <<queue_request_irq>> if (use_threaded_interrupts) {
+ */
 static int use_threaded_interrupts;
 module_param(use_threaded_interrupts, int, 0);
 
+/*
+ * 在以下使用use_cmb_sqes:
+ *   - drivers/nvme/host/pci.c|47| <<global>> module_param(use_cmb_sqes, bool, 0444);
+ *   - drivers/nvme/host/pci.c|48| <<global>> MODULE_PARM_DESC(use_cmb_sqes, "use controller's memory buffer for I/O SQes");
+ *   - drivers/nvme/host/pci.c|1925| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+ */
 static bool use_cmb_sqes = true;
 module_param(use_cmb_sqes, bool, 0444);
 MODULE_PARM_DESC(use_cmb_sqes, "use controller's memory buffer for I/O SQes");
 
+/*
+ * Most modern SSDs include onboard DRAM, typically in a ratio of 1GB RAM per
+ * 1TB of NAND flash memory. This RAM is usually dedicated to tracking where
+ * each logical block address is physically stored on the NAND
+ * flash—information that changes with every write operation due to the wear
+ * leveling that flash memory requires. This information must also be consulted
+ * in order to complete any read operation. The standard DRAM to NAND ratio
+ * provides enough RAM for the SSD controller to use a simple and fast lookup
+ * table instead of more complicated data structures. This greatly reduces the
+ * work the SSD controller needs to do to handle IO operations, and is key to
+ * offering consistent performance.
+ *
+ * SSDs that omit this DRAM can be cheaper and smaller, but because they can
+ * only store their mapping tables in the flash memory instead of much faster
+ * DRAM, there's a substantial performance penalty. In the worst case, read
+ * latency is doubled as potentially every read request from the host first
+ * requires a NAND flash read to look up the logical to physical address
+ * mapping, then a second read to actually fetch the requested data.
+ *
+ * 在以下使用max_host_mem_size_mb:
+ *   - drivers/nvme/host/pci.c|51| <<global>> module_param(max_host_mem_size_mb, uint, 0444);
+ *   - drivers/nvme/host/pci.c|52| <<global>> MODULE_PARM_DESC(max_host_mem_size_mb,
+ *   - drivers/nvme/host/pci.c|2081| <<nvme_setup_host_mem>> u64 max = (u64)max_host_mem_size_mb * SZ_1M;
+ *   - drivers/nvme/host/pci.c|2091| <<nvme_setup_host_mem>> min >> ilog2(SZ_1M), max_host_mem_size_mb);
+ */
 static unsigned int max_host_mem_size_mb = 128;
 module_param(max_host_mem_size_mb, uint, 0444);
 MODULE_PARM_DESC(max_host_mem_size_mb,
 	"Maximum Host Memory Buffer (HMB) size per controller (in MiB)");
 
+/*
+ * 在以下使用sgl_threshold:
+ *   - drivers/nvme/host/pci.c|56| <<global>> module_param(sgl_threshold, uint, 0644);
+ *   - drivers/nvme/host/pci.c|57| <<global>> MODULE_PARM_DESC(sgl_threshold,
+ *   - drivers/nvme/host/pci.c|542| <<nvme_pci_use_sgls>> if (!sgl_threshold || avg_seg_size < sgl_threshold)
+ */
 static unsigned int sgl_threshold = SZ_32K;
 module_param(sgl_threshold, uint, 0644);
 MODULE_PARM_DESC(sgl_threshold,
@@ -68,12 +124,26 @@ static int io_queue_depth = 1024;
 module_param_cb(io_queue_depth, &io_queue_depth_ops, &io_queue_depth, 0644);
 MODULE_PARM_DESC(io_queue_depth, "set io queue depth, should >= 2");
 
+/*
+ * 在以下使用write_queues:
+ *   - drivers/nvme/host/pci.c|346| <<max_io_queues>> return num_possible_cpus() + write_queues + poll_queues;
+ *   - drivers/nvme/host/pci.c|2524| <<nvme_calc_irq_sets>> } else if (nrirqs == 1 || !write_queues) {
+ *   - drivers/nvme/host/pci.c|2526| <<nvme_calc_irq_sets>> } else if (write_queues >= nrirqs) {
+ *   - drivers/nvme/host/pci.c|2529| <<nvme_calc_irq_sets>> nr_read_queues = nrirqs - write_queues;
+ *   - drivers/nvme/host/pci.c|3860| <<nvme_init>> write_queues = min(write_queues, num_possible_cpus());
+ */
 static unsigned int write_queues;
 module_param(write_queues, uint, 0644);
 MODULE_PARM_DESC(write_queues,
 	"Number of queues to use for writes. If not set, reads and writes "
 	"will share a queue set.");
 
+/*
+ * 在以下使用poll_queues:
+ *   - drivers/nvme/host/pci.c|346| <<max_io_queues>> return num_possible_cpus() + write_queues + poll_queues;
+ *   - drivers/nvme/host/pci.c|2574| <<nvme_setup_irqs>> this_p_queues = poll_queues;
+ *   - drivers/nvme/host/pci.c|3861| <<nvme_init>> poll_queues = min(poll_queues, num_possible_cpus());
+ */
 static unsigned int poll_queues;
 module_param(poll_queues, uint, 0644);
 MODULE_PARM_DESC(poll_queues, "Number of queues to use for polled IO.");
@@ -95,36 +165,170 @@ struct nvme_dev {
 	struct device *dev;
 	struct dma_pool *prp_page_pool;
 	struct dma_pool *prp_small_pool;
+	/*
+	 * 修改nvme_dev->online_queues的地方:
+	 *   - drivers/nvme/host/pci.c|2167| <<nvme_suspend_queue>> nvmeq->dev->online_queues--;
+	 *   - drivers/nvme/host/pci.c|2428| <<nvme_init_queue>> dev->online_queues++;
+	 *   - drivers/nvme/host/pci.c|2498| <<nvme_create_queue>> dev->online_queues--;
+	 *   - drivers/nvme/host/pci.c|2744| <<nvme_pci_configure_admin_queue>> dev->online_queues--;
+	 */
 	unsigned online_queues;
 	unsigned max_qid;
+	/*
+	 * 在以下使用io_queues:
+	 *   - drivers/nvme/host/pci.c|752| <<nvme_pci_map_queues>> map->nr_queues = dev->io_queues[i];
+	 *   - drivers/nvme/host/pci.c|2518| <<nvme_create_io_queues>> if (max != 1 && dev->io_queues[HCTX_TYPE_POLL]) {
+	 *   - drivers/nvme/host/pci.c|2519| <<nvme_create_io_queues>> rw_queues = dev->io_queues[HCTX_TYPE_DEFAULT] +
+	 *   - drivers/nvme/host/pci.c|2520| <<nvme_create_io_queues>> dev->io_queues[HCTX_TYPE_READ];
+	 *   - drivers/nvme/host/pci.c|2948| <<nvme_calc_irq_sets>> dev->io_queues[HCTX_TYPE_DEFAULT] = nrirqs - nr_read_queues;
+	 *   - drivers/nvme/host/pci.c|2950| <<nvme_calc_irq_sets>> dev->io_queues[HCTX_TYPE_READ] = nr_read_queues;
+	 *   - drivers/nvme/host/pci.c|2997| <<nvme_setup_irqs>> dev->io_queues[HCTX_TYPE_POLL] = this_p_queues;
+	 *   - drivers/nvme/host/pci.c|3000| <<nvme_setup_irqs>> dev->io_queues[HCTX_TYPE_DEFAULT] = 1;
+	 *   - drivers/nvme/host/pci.c|3001| <<nvme_setup_irqs>> dev->io_queues[HCTX_TYPE_READ] = 0;
+	 *   - drivers/nvme/host/pci.c|3113| <<nvme_setup_io_queues>> dev->max_qid = result + dev->io_queues[HCTX_TYPE_POLL];
+	 *   - drivers/nvme/host/pci.c|3146| <<nvme_setup_io_queues>> dev->io_queues[HCTX_TYPE_DEFAULT],
+	 *   - drivers/nvme/host/pci.c|3147| <<nvme_setup_io_queues>> dev->io_queues[HCTX_TYPE_READ],
+	 *   - drivers/nvme/host/pci.c|3148| <<nvme_setup_io_queues>> dev->io_queues[HCTX_TYPE_POLL]);
+	 *   - drivers/nvme/host/pci.c|3278| <<nvme_dev_add>> if (dev->io_queues[HCTX_TYPE_POLL])
+	 */
 	unsigned io_queues[HCTX_MAX_TYPES];
 	unsigned int num_vecs;
 	int q_depth;
+	/*
+	 * 在以下使用io_sqes:
+	 *   - drivers/nvme/host/pci.c|1592| <<nvme_alloc_queue>> nvmeq->sqes = qid ? dev->io_sqes : NVME_ADM_SQES;
+	 *   - drivers/nvme/host/pci.c|2559| <<nvme_pci_enable>> dev->io_sqes = 7;
+	 *   - drivers/nvme/host/pci.c|2561| <<nvme_pci_enable>> dev->io_sqes = NVME_NVM_IOSQES;
+	 */
 	int io_sqes;
 	u32 db_stride;
 	void __iomem *bar;
 	unsigned long bar_mapped_size;
+	/*
+	 * 在以下使用remove_work:
+	 *   - drivers/nvme/host/pci.c|3181| <<nvme_remove_dead_ctrl>> if (!queue_work(nvme_wq, &dev->remove_work))
+	 *   - drivers/nvme/host/pci.c|3369| <<nvme_remove_dead_ctrl_work>> struct nvme_dev *dev = container_of(work, struct nvme_dev, remove_work);
+	 *   - drivers/nvme/host/pci.c|3511| <<nvme_probe>> INIT_WORK(&dev->remove_work, nvme_remove_dead_ctrl_work);
+	 */
 	struct work_struct remove_work;
+	/*
+	 * 在以下使用shutdown_lock:
+	 *   - drivers/nvme/host/pci.c|2570| <<nvme_dev_disable>> mutex_lock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2615| <<nvme_dev_disable>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2719| <<nvme_reset_work>> mutex_lock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2745| <<nvme_reset_work>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2821| <<nvme_reset_work>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2965| <<nvme_probe>> mutex_init(&dev->shutdown_lock);
+	 */
 	struct mutex shutdown_lock;
 	bool subsystem;
+	/*
+	 * 在以下使用cmb_size:
+	 *   - drivers/nvme/host/pci.c|1548| <<nvme_cmb_qdepth>> if (q_size_aligned * nr_io_queues > dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1549| <<nvme_cmb_qdepth>> u64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);
+	 *   - drivers/nvme/host/pci.c|1946| <<nvme_map_cmb>> if (dev->cmb_size)
+	 *   - drivers/nvme/host/pci.c|1976| <<nvme_map_cmb>> dev->cmb_size = size;
+	 *   - drivers/nvme/host/pci.c|1991| <<nvme_release_cmb>> if (dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1994| <<nvme_release_cmb>> dev->cmb_size = 0;
+	 */
 	u64 cmb_size;
+	/*
+	 * 在以下使用cmb_use_sqes:
+	 *   - drivers/nvme/host/pci.c|1570| <<nvme_alloc_sq_cmds>> if (qid && dev->cmb_use_sqes && (dev->cmbsz & NVME_CMBSZ_SQS)) {
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+	 *   - drivers/nvme/host/pci.c|2306| <<nvme_setup_io_queues>> if (dev->cmb_use_sqes) {
+	 *   - drivers/nvme/host/pci.c|2312| <<nvme_setup_io_queues>> dev->cmb_use_sqes = false;
+	 */
 	bool cmb_use_sqes;
+	/*
+	 * 在以下使用cmbsz:
+	 *   - drivers/nvme/host/pci.c|1570| <<nvme_alloc_sq_cmds>> if (qid && dev->cmb_use_sqes && (dev->cmbsz & NVME_CMBSZ_SQS)) {
+	 *   - drivers/nvme/host/pci.c|1915| <<nvme_cmb_show>> ndev->cmbloc, ndev->cmbsz);
+	 *   - drivers/nvme/host/pci.c|1921| <<nvme_cmb_size_unit>> u8 szu = (dev->cmbsz >> NVME_CMBSZ_SZU_SHIFT) & NVME_CMBSZ_SZU_MASK;
+	 *   - drivers/nvme/host/pci.c|1928| <<nvme_cmb_size>> return (dev->cmbsz >> NVME_CMBSZ_SZ_SHIFT) & NVME_CMBSZ_SZ_MASK;
+	 *   - drivers/nvme/host/pci.c|1949| <<nvme_map_cmb>> dev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ);
+	 *   - drivers/nvme/host/pci.c|1950| <<nvme_map_cmb>> if (!dev->cmbsz)
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+	 *   - drivers/nvme/host/pci.c|1979| <<nvme_map_cmb>> if ((dev->cmbsz & (NVME_CMBSZ_WDS | NVME_CMBSZ_RDS)) ==
+	 */
 	u32 cmbsz;
+	/*
+	 * 在以下使用cmbloc:
+	 *   - drivers/nvme/host/pci.c|1915| <<nvme_cmb_show>> ndev->cmbloc, ndev->cmbsz);
+	 *   - drivers/nvme/host/pci.c|1952| <<nvme_map_cmb>> dev->cmbloc = readl(dev->bar + NVME_REG_CMBLOC);
+	 *   - drivers/nvme/host/pci.c|1955| <<nvme_map_cmb>> offset = nvme_cmb_size_unit(dev) * NVME_CMB_OFST(dev->cmbloc);
+	 *   - drivers/nvme/host/pci.c|1956| <<nvme_map_cmb>> bar = NVME_CMB_BIR(dev->cmbloc);
+	 *   - include/linux/nvme.h|160| <<NVME_CMB_BIR>> #define NVME_CMB_BIR(cmbloc) ((cmbloc) & 0x7)
+	 *   - include/linux/nvme.h|161| <<NVME_CMB_OFST>> #define NVME_CMB_OFST(cmbloc) (((cmbloc) >> 12) & 0xfffff)
+	 */
 	u32 cmbloc;
 	struct nvme_ctrl ctrl;
 	u32 last_ps;
 
+	/*
+	 * 在以下使用iod_mempool:
+	 *   - drivers/nvme/host/pci.c|976| <<nvme_unmap_data>> mempool_free(iod->sg, dev->iod_mempool);
+	 *   - drivers/nvme/host/pci.c|1226| <<nvme_map_data>> iod->sg = mempool_alloc(dev->iod_mempool, GFP_ATOMIC);
+	 *   - drivers/nvme/host/pci.c|3629| <<nvme_pci_free_ctrl>> mempool_destroy(dev->iod_mempool);
+	 *   - drivers/nvme/host/pci.c|4036| <<nvme_probe>> dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
+	 *   - drivers/nvme/host/pci.c|4040| <<nvme_probe>> if (!dev->iod_mempool) {
+	 *   - drivers/nvme/host/pci.c|4063| <<nvme_probe>> mempool_destroy(dev->iod_mempool);
+	 */
 	mempool_t *iod_mempool;
 
 	/* shadow doorbell buffer support: */
+	/*
+	 * 5.7 Doorbell Buffer Config command
+	 * The Doorbell Buffer Config command is used to provide two separate memory buffers that mirror the
+	 * controller's doorbell registers defined in section 3. This command is intended for emulated controllers and
+	 * is not typically supported by a physical NVMe controller. The two buffers are known as "Shadow Doorbell"
+	 * and "EventIdx", respectively. Refer to section 7.13 for an example of how these buffers may be used.
+	 *
+	 *
+	 * nvme: improve performance for virtual NVMe devices
+	 *
+	 * This change provides a mechanism to reduce the number of MMIO doorbell
+	 * writes for the NVMe driver. When running in a virtualized environment
+	 * like QEMU, the cost of an MMIO is quite hefy here. The main idea for
+	 * the patch is provide the device two memory location locations:
+	 * 1) to store the doorbell values so they can be lookup without the doorbell
+	 * MMIO write
+	 * 2) to store an event index.
+	 * I believe the doorbell value is obvious, the event index not so much.
+	 * Similar to the virtio specification, the virtual device can tell the
+	 * driver (guest OS) not to write MMIO unless you are writing past this
+	 * value.
+	 *
+	 * FYI: doorbell values are written by the nvme driver (guest OS) and the
+	 * event index is written by the virtual device (host OS).
+	 *
+	 * The patch implements a new admin command that will communicate where
+	 * these two memory locations reside. If the command fails, the nvme
+	 * driver will work as before without any optimizations.
+	 */
 	u32 *dbbuf_dbs;
 	dma_addr_t dbbuf_dbs_dma_addr;
 	u32 *dbbuf_eis;
 	dma_addr_t dbbuf_eis_dma_addr;
 
 	/* host memory buffer support: */
+	/*
+	 * The Host Memory Buffer feature provides a mechanism for the host to allocate a portion of host memory
+	 * for the exclusive use of the controller. After a successful completion of a Set Features command enabling
+	 * the host memory buffer, the host shall not write to:
+	 * a)The Host Memory Descriptor List (refer to Figure 296); and
+	 * b)the associated host memory region (i.e., the memory regions described by the Host Memory
+	 * Descriptor List),
+	 * until the host memory buffer has been disabled.
+	 */
 	u64 host_mem_size;
 	u32 nr_host_mem_descs;
+	/*
+	 * 在以下使用host_mem_descs_dma:
+	 *   - drivers/nvme/host/pci.c|2599| <<nvme_set_host_mem>> u64 dma_addr = dev->host_mem_descs_dma;
+	 *   - drivers/nvme/host/pci.c|2640| <<nvme_free_host_mem>> dev->host_mem_descs, dev->host_mem_descs_dma);
+	 *   - drivers/nvme/host/pci.c|2691| <<__nvme_alloc_host_mem>> dev->host_mem_descs_dma = descs_dma;
+	 */
 	dma_addr_t host_mem_descs_dma;
 	struct nvme_host_mem_buf_desc *host_mem_descs;
 	void **host_mem_desc_bufs;
@@ -141,11 +345,17 @@ static int io_queue_depth_set(const char *val, const struct kernel_param *kp)
 	return param_set_int(val, kp);
 }
 
+/*
+ * 返回submission queue tail doorbell
+ */
 static inline unsigned int sq_idx(unsigned int qid, u32 stride)
 {
 	return qid * 2 * stride;
 }
 
+/*
+ * 返回completion queue head doorbell
+ */
 static inline unsigned int cq_idx(unsigned int qid, u32 stride)
 {
 	return (qid * 2 + 1) * stride;
@@ -162,32 +372,130 @@ static inline struct nvme_dev *to_nvme_dev(struct nvme_ctrl *ctrl)
  */
 struct nvme_queue {
 	struct nvme_dev *dev;
+	/*
+	 * 在以下使用sq_lock:
+	 *   - drivers/nvme/host/pci.c|872| <<nvme_submit_cmd>> spin_lock(&nvmeq->sq_lock);
+	 *   - drivers/nvme/host/pci.c|884| <<nvme_submit_cmd>> spin_unlock(&nvmeq->sq_lock);
+	 *   - drivers/nvme/host/pci.c|914| <<nvme_commit_rqs>> spin_lock(&nvmeq->sq_lock);
+	 *   - drivers/nvme/host/pci.c|924| <<nvme_commit_rqs>> spin_unlock(&nvmeq->sq_lock); 
+	 *   - drivers/nvme/host/pci.c|2296| <<nvme_alloc_queue>> spin_lock_init(&nvmeq->sq_lock);
+	 */
 	spinlock_t sq_lock;
 	void *sq_cmds;
 	 /* only used for poll queues: */
+	/*
+	 * 在以下使用cq_poll_lock:
+	 *   - drivers/nvme/host/pci.c|1063| <<nvme_poll_irqdisable>> spin_lock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1065| <<nvme_poll_irqdisable>> spin_unlock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1106| <<nvme_poll>> spin_lock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1108| <<nvme_poll>> spin_unlock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1500| <<nvme_alloc_queue>> spin_lock_init(&nvmeq->cq_poll_lock);
+	 */
 	spinlock_t cq_poll_lock ____cacheline_aligned_in_smp;
 	volatile struct nvme_completion *cqes;
 	struct blk_mq_tags **tags;
+	/*
+	 * 在以下使用sq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1324| <<adapter_alloc_sq>> c.create_sq.prp1 = cpu_to_le64(nvmeq->sq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1523| <<nvme_free_queue>> nvmeq->sq_cmds, nvmeq->sq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1609| <<nvme_alloc_sq_cmds>> nvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev,
+	 *   - drivers/nvme/host/pci.c|1611| <<nvme_alloc_sq_cmds>> if (nvmeq->sq_dma_addr) {
+	 *   - drivers/nvme/host/pci.c|1621| <<nvme_alloc_sq_cmds>> &nvmeq->sq_dma_addr, GFP_KERNEL);
+	 *   - drivers/nvme/host/pci.c|1888| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
+	 */
 	dma_addr_t sq_dma_addr;
+	/*
+	 * 在以下使用cq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1294| <<adapter_alloc_cq>> c.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1514| <<nvme_free_queue>> (void *)nvmeq->cqes, nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1637| <<nvme_alloc_queue>> &nvmeq->cq_dma_addr, GFP_KERNEL);
+	 *   - drivers/nvme/host/pci.c|1657| <<nvme_alloc_queue>> nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1889| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
+	 */
 	dma_addr_t cq_dma_addr;
+	/*
+	 * 在以下使用q_db:
+	 *   - drivers/nvme/host/pci.c|729| <<nvme_write_sq_db>> writel(nvmeq->sq_tail, nvmeq->q_db);
+	 *   - drivers/nvme/host/pci.c|1218| <<nvme_ring_cq_doorbell>> writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
+	 *   - drivers/nvme/host/pci.c|1902| <<nvme_alloc_queue>> nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
+	 *   - drivers/nvme/host/pci.c|1969| <<nvme_init_queue>> nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
+	 *   - drivers/nvme/host/pci.c|2838| <<nvme_setup_io_queues>> adminq->q_db = dev->dbs;
+	 */
 	u32 __iomem *q_db;
 	u16 q_depth;
+	/*
+	 * 设置cq_vector的地方:
+	 *   - drivers/nvme/host/pci.c|1796| <<nvme_create_queue>> nvmeq->cq_vector = vector;
+	 *   - drivers/nvme/host/pci.c|1969| <<nvme_pci_configure_admin_queue>> nvmeq->cq_vector = 0;
+	 */
 	u16 cq_vector;
+	/*
+	 * 主要在以下使用pci的sq_tail:
+	 *   - drivers/nvme/host/pci.c|850| <<nvme_write_sq_db>> u16 next_tail = nvmeq->sq_tail + 1;
+	 *   - drivers/nvme/host/pci.c|858| <<nvme_write_sq_db>> if (nvme_dbbuf_update_and_check_event(nvmeq->sq_tail,
+	 *   - drivers/nvme/host/pci.c|860| <<nvme_write_sq_db>> writel(nvmeq->sq_tail, nvmeq->q_db);
+	 *   - drivers/nvme/host/pci.c|868| <<nvme_write_sq_db>> nvmeq->last_sq_tail = nvmeq->sq_tail;
+	 *   - drivers/nvme/host/pci.c|892| <<nvme_submit_cmd>> memcpy(nvmeq->sq_cmds + (nvmeq->sq_tail << nvmeq->sqes),
+	 *   - drivers/nvme/host/pci.c|894| <<nvme_submit_cmd>> if (++nvmeq->sq_tail == nvmeq->q_depth)
+	 *   - drivers/nvme/host/pci.c|895| <<nvme_submit_cmd>> nvmeq->sq_tail = 0;
+	 *   - drivers/nvme/host/pci.c|935| <<nvme_commit_rqs>> if (nvmeq->sq_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|1497| <<nvme_handle_cqe>> trace_nvme_sq(req, cqe->sq_head, nvmeq->sq_tail);
+	 *   - drivers/nvme/host/pci.c|2408| <<nvme_init_queue>> nvmeq->sq_tail = 0;
+	 */
 	u16 sq_tail;
+	/*
+	 * 在以下使用last_sq_tail:
+	 *   - drivers/nvme/host/pci.c|734| <<nvme_write_sq_db>> if (next_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|741| <<nvme_write_sq_db>> nvmeq->last_sq_tail = nvmeq->sq_tail;
+	 *   - drivers/nvme/host/pci.c|779| <<nvme_commit_rqs>> if (nvmeq->sq_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|2021| <<nvme_init_queue>> nvmeq->last_sq_tail = 0;
+	 */
 	u16 last_sq_tail;
 	u16 cq_head;
 	u16 qid;
 	u8 cq_phase;
 	u8 sqes;
 	unsigned long flags;
+/*
+ * 在以下使用NVMEQ_ENABLED:
+ *   - drivers/nvme/host/pci.c|1001| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+ *   - drivers/nvme/host/pci.c|1560| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+ *   - drivers/nvme/host/pci.c|1805| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+ *   - drivers/nvme/host/pci.c|1977| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+ *   - drivers/nvme/host/pci.c|2468| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+ *   - drivers/nvme/host/pci.c|2516| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+ */
 #define NVMEQ_ENABLED		0
+/*
+ * 在以下使用NVMEQ_SQ_CMB:
+ *   - drivers/nvme/host/pci.c|2076| <<nvme_free_queue>> if (test_and_clear_bit(NVMEQ_SQ_CMB, &nvmeq->flags)) {
+ *   - drivers/nvme/host/pci.c|2229| <<nvme_alloc_sq_cmds>> set_bit(NVMEQ_SQ_CMB, &nvmeq->flags);
+ */
 #define NVMEQ_SQ_CMB		1
 #define NVMEQ_DELETE_ERROR	2
+/*
+ * 在以下使用NVMEQ_POLLED:
+ *   - drivers/nvme/host/pci.c|1228| <<nvme_poll_irqdisable>> if (test_bit(NVMEQ_POLLED, &nvmeq->flags)) {
+ *   - drivers/nvme/host/pci.c|1325| <<adapter_alloc_cq>> if (!test_bit(NVMEQ_POLLED, &nvmeq->flags))
+ *   - drivers/nvme/host/pci.c|1592| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_POLLED, &nvmeq->flags))
+ *   - drivers/nvme/host/pci.c|1816| <<nvme_create_queue>> set_bit(NVMEQ_POLLED, &nvmeq->flags);
+ */
 #define NVMEQ_POLLED		3
 	u32 *dbbuf_sq_db;
 	u32 *dbbuf_cq_db;
 	u32 *dbbuf_sq_ei;
+	/*
+	 * 在以下使用dbbuf_cq_ei:
+	 *   - drivers/nvme/host/pci.c|434| <<nvme_dbbuf_init>> nvmeq->dbbuf_cq_ei = &dev->dbbuf_eis[cq_idx(qid, dev->db_stride)];
+	 *   - drivers/nvme/host/pci.c|1133| <<nvme_ring_cq_doorbell>> nvmeq->dbbuf_cq_ei))
+	 */
 	u32 *dbbuf_cq_ei;
+	/*
+	 * 在以下使用delete_done:
+	 *   - drivers/nvme/host/pci.c|3095| <<nvme_del_queue_end>> complete(&nvmeq->delete_done);
+	 *   - drivers/nvme/host/pci.c|3129| <<nvme_delete_queue>> init_completion(&nvmeq->delete_done);
+	 *   - drivers/nvme/host/pci.c|3157| <<__nvme_disable_io_queues>> timeout = wait_for_completion_io_timeout(&nvmeq->delete_done,
+	 */
 	struct completion delete_done;
 };
 
@@ -210,6 +518,11 @@ struct nvme_iod {
 	struct scatterlist *sg;
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|340| <<max_queue_count>> return 1 + max_io_queues();
+ *   - drivers/nvme/host/pci.c|2529| <<nvme_setup_io_queues>> nr_io_queues = max_io_queues();
+ */
 static unsigned int max_io_queues(void)
 {
 	return num_possible_cpus() + write_queues + poll_queues;
@@ -226,6 +539,10 @@ static inline unsigned int nvme_dbbuf_size(u32 stride)
 	return (max_queue_count() * 8 * stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3389| <<nvme_reset_work>> result = nvme_dbbuf_dma_alloc(dev);
+ */
 static int nvme_dbbuf_dma_alloc(struct nvme_dev *dev)
 {
 	unsigned int mem_size = nvme_dbbuf_size(dev->db_stride);
@@ -267,9 +584,21 @@ static void nvme_dbbuf_dma_free(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1778| <<nvme_init_queue>> nvme_dbbuf_init(dev, nvmeq, qid);
+ */
 static void nvme_dbbuf_init(struct nvme_dev *dev,
 			    struct nvme_queue *nvmeq, int qid)
 {
+	/*
+	 * 在qemu上测试:
+	 * [    0.832968] orabug: nvme_dbbuf_init() qid=0, 0x0000000000000000
+	 * [    0.835749] orabug: nvme_dbbuf_init() qid=1, 0x0000000000000000
+	 * [    0.836705] orabug: nvme_dbbuf_init() qid=2, 0x0000000000000000
+	 * [    0.837599] orabug: nvme_dbbuf_init() qid=3, 0x0000000000000000
+	 * [    0.838496] orabug: nvme_dbbuf_init() qid=4, 0x0000000000000000
+	 */
 	if (!dev->dbbuf_dbs || !qid)
 		return;
 
@@ -298,15 +627,39 @@ static void nvme_dbbuf_set(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|485| <<nvme_dbbuf_update_and_check_event>> if (!nvme_dbbuf_need_event(*dbbuf_ei, value, old_value))
+ */
 static inline int nvme_dbbuf_need_event(u16 event_idx, u16 new_idx, u16 old)
 {
 	return (u16)(new_idx - event_idx - 1) < (u16)(new_idx - old);
 }
 
 /* Update dbbuf and return true if an MMIO is required */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|651| <<nvme_write_sq_db>> if (nvme_dbbuf_update_and_check_event(nvmeq->sq_tail,
+ *   - drivers/nvme/host/pci.c|1132| <<nvme_ring_cq_doorbell>> if (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,
+ */
 static bool nvme_dbbuf_update_and_check_event(u16 value, u32 *dbbuf_db,
 					      volatile u32 *dbbuf_ei)
 {
+	/*
+	 * Controllers that support the Doorbell Buffer Config command are typically emulated controllers where this
+	 * feature is used to enhance the performance of host software running in Virtual Machines. If supported by
+	 * he controller, host software may enable Shadow Doorbell buffers by submitting the Doorbell Buffer Config
+	 * command (refer to section 5.7).
+	 * After the completion of the Doorbell Buffer Config command, host software shall submit commands by
+	 * updating the appropriate entry in the Shadow Doorbell buffer instead of updating the controller's
+	 * corresponding doorbell register. If updating an entry in the Shadow Doorbell buffer changes the value from
+	 * being less than or equal to the value of the corresponding EventIdx buffer entry to being greater than that
+	 * value, then the host shall also update the controller's corresponding doorbell register to match the value of
+	 * that entry in the Shadow Doorbell buffer. Queue wrap conditions shall be taken into account in all
+	 * comparisons in this paragraph.
+	 *
+	 * 在qemu测试(猜测bm也是如此), dbbuf_db总是NULL
+	 */
 	if (dbbuf_db) {
 		u16 old_value;
 
@@ -339,6 +692,10 @@ static bool nvme_dbbuf_update_and_check_event(u16 value, u32 *dbbuf_db,
  * as it only leads to a small amount of wasted memory for the lifetime of
  * the I/O.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|658| <<nvme_pci_iod_alloc_size>> alloc_size = sizeof(__le64 *) * nvme_npages(size, dev);
+ */
 static int nvme_npages(unsigned size, struct nvme_dev *dev)
 {
 	unsigned nprps = DIV_ROUND_UP(size + dev->ctrl.page_size,
@@ -350,11 +707,19 @@ static int nvme_npages(unsigned size, struct nvme_dev *dev)
  * Calculates the number of pages needed for the SGL segments. For example a 4k
  * page can accommodate 256 SGL descriptors.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|677| <<nvme_pci_iod_alloc_size>> alloc_size = sizeof(__le64 *) * nvme_pci_npages_sgl(nseg);
+ */
 static int nvme_pci_npages_sgl(unsigned int num_seg)
 {
 	return DIV_ROUND_UP(num_seg * sizeof(struct nvme_sgl_desc), PAGE_SIZE);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3823| <<nvme_probe>> alloc_size = nvme_pci_iod_alloc_size(dev, NVME_MAX_KB_SZ,
+ */
 static unsigned int nvme_pci_iod_alloc_size(struct nvme_dev *dev,
 		unsigned int size, unsigned int nseg, bool use_sgl)
 {
@@ -368,6 +733,9 @@ static unsigned int nvme_pci_iod_alloc_size(struct nvme_dev *dev,
 	return alloc_size + sizeof(struct scatterlist) * nseg;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.init_hctx = nvme_admin_init_hctx()
+ */
 static int nvme_admin_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 				unsigned int hctx_idx)
 {
@@ -379,10 +747,18 @@ static int nvme_admin_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	WARN_ON(nvmeq->tags);
 
 	hctx->driver_data = nvmeq;
+	/*
+	 * struct nvme_dev:
+	 *  - struct blk_mq_tag_set admin_tagset;
+	 *     - struct blk_mq_tags **tags;
+	 */
 	nvmeq->tags = &dev->admin_tagset.tags[0];
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.exit_hctx = nvme_admin_exit_hctx()
+ */
 static void nvme_admin_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
@@ -390,6 +766,9 @@ static void nvme_admin_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_i
 	nvmeq->tags = NULL;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_ops.init_hctx = nvme_init_hctx()
+ */
 static int nvme_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 			  unsigned int hctx_idx)
 {
@@ -404,6 +783,10 @@ static int nvme_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.init_request = nvme_init_request()
+ * struct blk_mq_ops nvme_mq_ops.init_request = nvme_init_request()
+ */
 static int nvme_init_request(struct blk_mq_tag_set *set, struct request *req,
 		unsigned int hctx_idx, unsigned int numa_node)
 {
@@ -419,6 +802,10 @@ static int nvme_init_request(struct blk_mq_tag_set *set, struct request *req,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|574| <<nvme_pci_map_queues>> offset = queue_irq_offset(dev);
+ */
 static int queue_irq_offset(struct nvme_dev *dev)
 {
 	/* if we have more than 1 vec, admin queue offsets us by 1 */
@@ -428,12 +815,18 @@ static int queue_irq_offset(struct nvme_dev *dev)
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_ops.map_queues = nvme_pci_map_queues()
+ */
 static int nvme_pci_map_queues(struct blk_mq_tag_set *set)
 {
 	struct nvme_dev *dev = set->driver_data;
 	int i, qoff, offset;
 
 	offset = queue_irq_offset(dev);
+	/*
+	 * i是read, write和poll的种类
+	 */
 	for (i = 0, qoff = 0; i < set->nr_maps; i++) {
 		struct blk_mq_queue_map *map = &set->map[i];
 
@@ -462,9 +855,18 @@ static int nvme_pci_map_queues(struct blk_mq_tag_set *set)
 /*
  * Write sq tail if we are asked to, or if the next command would wrap.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|767| <<nvme_submit_cmd>> nvme_write_sq_db(nvmeq, write_sq);
+ *   - drivers/nvme/host/pci.c|780| <<nvme_commit_rqs>> nvme_write_sq_db(nvmeq, true);
+ */
 static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
 {
 	if (!write_sq) {
+		/*
+		 * 如果不commit的话, sq_tail在增长, 其实next_tail一直比last_sq_tail大
+		 * 所以就会return
+		 */
 		u16 next_tail = nvmeq->sq_tail + 1;
 
 		if (next_tail == nvmeq->q_depth)
@@ -476,6 +878,13 @@ static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
 	if (nvme_dbbuf_update_and_check_event(nvmeq->sq_tail,
 			nvmeq->dbbuf_sq_db, nvmeq->dbbuf_sq_ei))
 		writel(nvmeq->sq_tail, nvmeq->q_db);
+	/*
+	 * 在以下使用last_sq_tail:
+	 *   - drivers/nvme/host/pci.c|734| <<nvme_write_sq_db>> if (next_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|741| <<nvme_write_sq_db>> nvmeq->last_sq_tail = nvmeq->sq_tail;
+	 *   - drivers/nvme/host/pci.c|779| <<nvme_commit_rqs>> if (nvmeq->sq_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|2021| <<nvme_init_queue>> nvmeq->last_sq_tail = 0;
+	 */
 	nvmeq->last_sq_tail = nvmeq->sq_tail;
 }
 
@@ -485,10 +894,21 @@ static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
  * @cmd: The command to send
  * @write_sq: whether to write to the SQ doorbell
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1185| <<nvme_queue_rq>> nvme_submit_cmd(nvmeq, &cmnd, bd->last);
+ *   - drivers/nvme/host/pci.c|1496| <<nvme_pci_submit_async_event>> nvme_submit_cmd(nvmeq, &c, true);
+ */
 static void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
 			    bool write_sq)
 {
 	spin_lock(&nvmeq->sq_lock);
+	/*
+	 * struct nvme_queue:
+	 *  - void *sq_cmds;
+	 *
+	 * nvmeq->sqes是每个sq entry的大小 (其实是最大)
+	 */
 	memcpy(nvmeq->sq_cmds + (nvmeq->sq_tail << nvmeq->sqes),
 	       cmd, sizeof(*cmd));
 	if (++nvmeq->sq_tail == nvmeq->q_depth)
@@ -497,22 +917,64 @@ static void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
 	spin_unlock(&nvmeq->sq_lock);
 }
 
+/*
+ * commit 04f3eafda6e05adc56afed4d3ae6e24aaa429058
+ * Author: Jens Axboe <axboe@kernel.dk>
+ * Date:   Thu Nov 29 10:02:29 2018 -0700
+ *
+ * nvme: implement mq_ops->commit_rqs() hook
+ *
+ * Split the command submission and the SQ doorbell ring, and add the
+ * doorbell ring as our ->commit_rqs() hook. This allows a list of
+ * requests to be issued, with nvme only writing the SQ update when
+ * it's necessary. This is more efficient if we have lists of requests
+ * to issue, particularly on virtualized hardware, where writing the
+ * SQ doorbell is more expensive than on real hardware. For those cases,
+ * performance increases of 2-3x have been observed.
+ *
+ * The use case for this is plugged IO, where blk-mq flushes a batch of
+ * requests at the time.
+ *
+ * Reviewed-by: Christoph Hellwig <hch@lst.de>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ *
+ * struct blk_mq_ops nvme_mq_ops.commit_rqs = nvme_commit_rqs()
+ */
 static void nvme_commit_rqs(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
 
 	spin_lock(&nvmeq->sq_lock);
+	/*
+	 * 在以下使用last_sq_tail:
+	 *   - drivers/nvme/host/pci.c|734| <<nvme_write_sq_db>> if (next_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|741| <<nvme_write_sq_db>> nvmeq->last_sq_tail = nvmeq->sq_tail;
+	 *   - drivers/nvme/host/pci.c|779| <<nvme_commit_rqs>> if (nvmeq->sq_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|2021| <<nvme_init_queue>> nvmeq->last_sq_tail = 0;
+	 */
 	if (nvmeq->sq_tail != nvmeq->last_sq_tail)
 		nvme_write_sq_db(nvmeq, true);
 	spin_unlock(&nvmeq->sq_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|955| <<nvme_unmap_data>> dma_pool_free(dev->prp_small_pool, nvme_pci_iod_list(req)[0],
+ *   - drivers/nvme/host/pci.c|959| <<nvme_unmap_data>> void *addr = nvme_pci_iod_list(req)[i];
+ *   - drivers/nvme/host/pci.c|1005| <<nvme_pci_setup_prps>> void **list = nvme_pci_iod_list(req);
+ *   - drivers/nvme/host/pci.c|1139| <<nvme_pci_setup_sgls>> nvme_pci_iod_list(req)[0] = sg_list;
+ *   - drivers/nvme/host/pci.c|1154| <<nvme_pci_setup_sgls>> nvme_pci_iod_list(req)[iod->npages++] = sg_list;
+ */
 static void **nvme_pci_iod_list(struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
 	return (void **)(iod->sg + blk_rq_nr_phys_segments(req));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1243| <<nvme_map_data>> iod->use_sgl = nvme_pci_use_sgls(dev, req);
+ */
 static inline bool nvme_pci_use_sgls(struct nvme_dev *dev, struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -533,6 +995,12 @@ static inline bool nvme_pci_use_sgls(struct nvme_dev *dev, struct request *req)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1116| <<nvme_map_data>> nvme_unmap_data(dev, req);
+ *   - drivers/nvme/host/pci.c|1182| <<nvme_queue_rq>> nvme_unmap_data(dev, req);
+ *   - drivers/nvme/host/pci.c|1201| <<nvme_pci_complete_rq>> nvme_unmap_data(dev, req);
+ */
 static void nvme_unmap_data(struct nvme_dev *dev, struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -767,6 +1235,10 @@ static blk_status_t nvme_pci_setup_sgls(struct nvme_dev *dev,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1248| <<nvme_map_data>> return nvme_setup_prp_simple(dev, req,
+ */
 static blk_status_t nvme_setup_prp_simple(struct nvme_dev *dev,
 		struct request *req, struct nvme_rw_command *cmnd,
 		struct bio_vec *bv)
@@ -786,6 +1258,10 @@ static blk_status_t nvme_setup_prp_simple(struct nvme_dev *dev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1253| <<nvme_map_data>> return nvme_setup_sgl_simple(dev, req,
+ */
 static blk_status_t nvme_setup_sgl_simple(struct nvme_dev *dev,
 		struct request *req, struct nvme_rw_command *cmnd,
 		struct bio_vec *bv)
@@ -804,6 +1280,10 @@ static blk_status_t nvme_setup_sgl_simple(struct nvme_dev *dev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1339| <<nvme_queue_rq>> ret = nvme_map_data(dev, req, &cmnd);
+ */
 static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 		struct nvme_command *cmnd)
 {
@@ -855,6 +1335,10 @@ static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1345| <<nvme_queue_rq>> ret = nvme_map_metadata(dev, req, &cmnd);
+ */
 static blk_status_t nvme_map_metadata(struct nvme_dev *dev, struct request *req,
 		struct nvme_command *cmnd)
 {
@@ -871,6 +1355,10 @@ static blk_status_t nvme_map_metadata(struct nvme_dev *dev, struct request *req,
 /*
  * NOTE: ns is NULL when called on the admin queue.
  */
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.queue_rq = nvme_queue_rq()
+ * struct blk_mq_ops nvme_mq_ops.queue_rq = nvme_queue_rq()
+ */
 static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 			 const struct blk_mq_queue_data *bd)
 {
@@ -882,6 +1370,11 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	struct nvme_command cmnd;
 	blk_status_t ret;
 
+	/*
+	 * 修改aborted的地方:
+	 *   - drivers/nvme/host/pci.c|1264| <<nvme_queue_rq>> iod->aborted = 0;
+	 *   - drivers/nvme/host/pci.c|1841| <<nvme_timeout>> iod->aborted = 1;
+	 */
 	iod->aborted = 0;
 	iod->npages = -1;
 	iod->nents = 0;
@@ -919,6 +1412,10 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.complete = nvme_pci_complete_rq()
+ * struct blk_mq_ops nvme_mq_ops.complete = nvme_pci_complete_rq()
+ */
 static void nvme_pci_complete_rq(struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -933,14 +1430,42 @@ static void nvme_pci_complete_rq(struct request *req)
 }
 
 /* We read the CQE phase first to check if the rest of the entry is valid */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1310| <<nvme_process_cq>> while (nvme_cqe_pending(nvmeq)) {
+ *   - drivers/nvme/host/pci.c|1353| <<nvme_irq_check>> if (nvme_cqe_pending(nvmeq))
+ *   - drivers/nvme/host/pci.c|1430| <<nvme_poll>> if (!nvme_cqe_pending(nvmeq))
+ */
 static inline bool nvme_cqe_pending(struct nvme_queue *nvmeq)
 {
+	/*
+	 * struct nvme_queue:
+	 *   - volatile struct nvme_completion *cqes;
+	 *
+	 * Phase Tag (P): Identifies whether a Completion Queue entry is new. The Phase Tag values for
+	 * all Completion Queue entries shall be initialized to '0' by host software prior to setting CC.EN to
+	 * '1'. When the controller places an entry in the Completion Queue, the controller shall invert the
+	 * Phase Tag to enable host software to discriminate a new entry. Specifically, for the first set of
+	 * completion queue entries after CC.EN is set to '1' all Phase Tags are set to '1' when they are
+	 * posted. For the second set of completion queue entries, when the controller has wrapped around
+	 * to the top of the Completion Queue, all Phase Tags are cleared to '0' when they are posted. The
+	 * value of the Phase Tag is inverted each pass through the Completion Queue.
+	 * This is a reserved bit in NVMe over Fabrics implementations.
+	 */
 	return (le16_to_cpu(nvmeq->cqes[nvmeq->cq_head].status) & 1) ==
 			nvmeq->cq_phase;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1223| <<nvme_process_cq>> nvme_ring_cq_doorbell(nvmeq);
+ */
 static inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)
 {
+	/*
+	 * sq: driver更新tail, controller更新head
+	 * cq: driver更新head, controller更新tail
+	 */
 	u16 head = nvmeq->cq_head;
 
 	if (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,
@@ -948,8 +1473,16 @@ static inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)
 		writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|983| <<nvme_complete_cqes>> nvme_handle_cqe(nvmeq, start);
+ */
 static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 {
+	/*
+	 * struct nvme_queue:
+	 *   - volatile struct nvme_completion *cqes;
+	 */
 	volatile struct nvme_completion *cqe = &nvmeq->cqes[idx];
 	struct request *req;
 
@@ -966,6 +1499,14 @@ static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 	 * aborts.  We don't even bother to allocate a struct request
 	 * for them but rather special case them here.
 	 */
+	/*
+	 * 使用admin queue来接受async event notification
+	 * 比如IO的sq的head写错了, 就会触发admin queue接收一个cqe用来aen
+	 *
+	 * 关于admin ring buffer和async event, nvme的admin queue的tagset
+	 * 有30个元素, 但是ring buffer有32个,除了一个用来表示full, 另外
+	 * 一个用在async event
+	 */
 	if (unlikely(nvme_is_aen_req(nvmeq->qid, cqe->command_id))) {
 		nvme_complete_async_event(&nvmeq->dev->ctrl,
 				cqe->status, &cqe->result);
@@ -974,9 +1515,22 @@ static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 
 	req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
 	trace_nvme_sq(req, cqe->sq_head, nvmeq->sq_tail);
+	/*
+	 * 一开始volatile struct nvme_completion *cqe = &nvmeq->cqes[idx];
+	 *
+	 * status是__le16  status; --> 16位
+	 */
 	nvme_end_request(req, cqe->status, cqe->result);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1032| <<nvme_irq>> nvme_complete_cqes(nvmeq, start, end);
+ *   - drivers/nvme/host/pci.c|1072| <<nvme_poll_irqdisable>> nvme_complete_cqes(nvmeq, start, end);
+ *   - drivers/nvme/host/pci.c|1110| <<nvme_poll>> nvme_complete_cqes(nvmeq, start, end);
+ *
+ * 函数实现中的条件是(start != end), 说明end不会被处理
+ */
 static void nvme_complete_cqes(struct nvme_queue *nvmeq, u16 start, u16 end)
 {
 	while (start != end) {
@@ -986,6 +1540,12 @@ static void nvme_complete_cqes(struct nvme_queue *nvmeq, u16 start, u16 end)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1033| <<nvme_process_cq>> nvme_update_cq_head(nvmeq);
+ *
+ * 核心思想是增加nvmeq->cq_head
+ */
 static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
 {
 	if (nvmeq->cq_head == nvmeq->q_depth - 1) {
@@ -996,15 +1556,29 @@ static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1046| <<nvme_irq>> nvme_process_cq(nvmeq, &start, &end, -1);
+ *   - drivers/nvme/host/pci.c|1082| <<nvme_poll_irqdisable>> found = nvme_process_cq(nvmeq, &start, &end, tag);
+ *   - drivers/nvme/host/pci.c|1086| <<nvme_poll_irqdisable>> found = nvme_process_cq(nvmeq, &start, &end, tag);
+ *   - drivers/nvme/host/pci.c|1125| <<nvme_poll>> found = nvme_process_cq(nvmeq, &start, &end, -1);
+ */
 static inline int nvme_process_cq(struct nvme_queue *nvmeq, u16 *start,
 				  u16 *end, unsigned int tag)
 {
 	int found = 0;
 
 	*start = nvmeq->cq_head;
+	/*
+	 * 只要有pending的就会处理下去, 哪怕没找到tag
+	 */
 	while (nvme_cqe_pending(nvmeq)) {
+		/*
+		 * 什么时候tag是-1呢?
+		 */
 		if (tag == -1U || nvmeq->cqes[nvmeq->cq_head].command_id == tag)
 			found++;
+		/* 核心思想是增加nvmeq->cq_head */
 		nvme_update_cq_head(nvmeq);
 	}
 	*end = nvmeq->cq_head;
@@ -1014,6 +1588,25 @@ static inline int nvme_process_cq(struct nvme_queue *nvmeq, u16 *start,
 	return found;
 }
 
+/*
+ * [0] nvme_irq
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * [0] nvme_irq
+ * [0] irq_thread_fn
+ * [0] irq_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 在以下使用nvme_irq():
+ *   - drivers/nvme/host/pci.c|1583| <<queue_request_irq>> nvme_irq, nvmeq, "nvme%dq%d", nr, nvmeq->qid);
+ *   - drivers/nvme/host/pci.c|1585| <<queue_request_irq>> return pci_request_irq(pdev, nvmeq->cq_vector, nvme_irq,
+ */
 static irqreturn_t nvme_irq(int irq, void *data)
 {
 	struct nvme_queue *nvmeq = data;
@@ -1029,6 +1622,14 @@ static irqreturn_t nvme_irq(int irq, void *data)
 	wmb();
 
 	if (start != end) {
+		/*
+		 * called by:
+		 *   - drivers/nvme/host/pci.c|1032| <<nvme_irq>> nvme_complete_cqes(nvmeq, start, end);
+		 *   - drivers/nvme/host/pci.c|1072| <<nvme_poll_irqdisable>> nvme_complete_cqes(nvmeq, start, end);
+		 *   - drivers/nvme/host/pci.c|1110| <<nvme_poll>> nvme_complete_cqes(nvmeq, start, end);
+		 *
+		 * 函数实现中的条件是(start != end), 说明end不会被处理
+		 */
 		nvme_complete_cqes(nvmeq, start, end);
 		return IRQ_HANDLED;
 	}
@@ -1036,6 +1637,18 @@ static irqreturn_t nvme_irq(int irq, void *data)
 	return ret;
 }
 
+/*
+ * [0] nvme_irq_check
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * 在以下使用nvme_irq_check():
+ *   - drivers/nvme/host/pci.c|2452| <<queue_request_irq>> return pci_request_irq(pdev, nvmeq->cq_vector, nvme_irq_check,
+ */
 static irqreturn_t nvme_irq_check(int irq, void *data)
 {
 	struct nvme_queue *nvmeq = data;
@@ -1048,6 +1661,12 @@ static irqreturn_t nvme_irq_check(int irq, void *data)
  * Poll for completions any queue, including those not dedicated to polling.
  * Can be called from any context.
  */
+/*
+ * calld by:
+ *   - drivers/nvme/host/pci.c|1318| <<nvme_timeout>> if (nvme_poll_irqdisable(nvmeq, req->tag)) {
+ *   - drivers/nvme/host/pci.c|1461| <<nvme_disable_admin_queue>> nvme_poll_irqdisable(nvmeq, -1);
+ *   - drivers/nvme/host/pci.c|2347| <<__nvme_disable_io_queues>> nvme_poll_irqdisable(nvmeq, -1);
+ */
 static int nvme_poll_irqdisable(struct nvme_queue *nvmeq, unsigned int tag)
 {
 	struct pci_dev *pdev = to_pci_dev(nvmeq->dev->dev);
@@ -1059,7 +1678,22 @@ static int nvme_poll_irqdisable(struct nvme_queue *nvmeq, unsigned int tag)
 	 * using the CQ lock.  For normal interrupt driven threads we have
 	 * to disable the interrupt to avoid racing with it.
 	 */
+	/*
+	 * 在以下使用NVMEQ_POLLED:
+	 *   - drivers/nvme/host/pci.c|1228| <<nvme_poll_irqdisable>> if (test_bit(NVMEQ_POLLED, &nvmeq->flags)) {
+	 *   - drivers/nvme/host/pci.c|1325| <<adapter_alloc_cq>> if (!test_bit(NVMEQ_POLLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1592| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_POLLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1816| <<nvme_create_queue>> set_bit(NVMEQ_POLLED, &nvmeq->flags);
+	 */
 	if (test_bit(NVMEQ_POLLED, &nvmeq->flags)) {
+		/*
+		 * 在以下使用cq_poll_lock:
+		 *   - drivers/nvme/host/pci.c|1063| <<nvme_poll_irqdisable>> spin_lock(&nvmeq->cq_poll_lock);
+		 *   - drivers/nvme/host/pci.c|1065| <<nvme_poll_irqdisable>> spin_unlock(&nvmeq->cq_poll_lock);
+		 *   - drivers/nvme/host/pci.c|1106| <<nvme_poll>> spin_lock(&nvmeq->cq_poll_lock);
+		 *   - drivers/nvme/host/pci.c|1108| <<nvme_poll>> spin_unlock(&nvmeq->cq_poll_lock);
+		 *   - drivers/nvme/host/pci.c|1500| <<nvme_alloc_queue>> spin_lock_init(&nvmeq->cq_poll_lock);
+		 */
 		spin_lock(&nvmeq->cq_poll_lock);
 		found = nvme_process_cq(nvmeq, &start, &end, tag);
 		spin_unlock(&nvmeq->cq_poll_lock);
@@ -1073,6 +1707,29 @@ static int nvme_poll_irqdisable(struct nvme_queue *nvmeq, unsigned int tag)
 	return found;
 }
 
+/*
+ * 激活poll的方法:
+ *
+ * 激活io_poll:
+ * # echo 1 > /sys/block/nvme0n1/queue/io_poll
+ *
+ * 在fio中使用pvsync2加上hipri:
+ * # fio -name iops -rw=read -bs=4k -runtime=60 -iodepth 32 -filename /dev/nvme0n1 -ioengine pvsync2 -direct=1 -hipri=1
+ *
+ * [0] nvme_poll
+ * [0] blk_poll
+ * [0] __blkdev_direct_IO_simple
+ * [0] blkdev_direct_IO
+ * [0] generic_file_read_iter
+ * [0] do_iter_readv_writev
+ * [0] do_iter_read
+ * [0] vfs_readv
+ * [0] do_preadv
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * struct blk_mq_ops nvme_mq_ops.poll = nvme_poll()
+ */
 static int nvme_poll(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
@@ -1082,6 +1739,14 @@ static int nvme_poll(struct blk_mq_hw_ctx *hctx)
 	if (!nvme_cqe_pending(nvmeq))
 		return 0;
 
+	/*
+	 * 在以下使用cq_poll_lock:
+	 *   - drivers/nvme/host/pci.c|1063| <<nvme_poll_irqdisable>> spin_lock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1065| <<nvme_poll_irqdisable>> spin_unlock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1106| <<nvme_poll>> spin_lock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1108| <<nvme_poll>> spin_unlock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1500| <<nvme_alloc_queue>> spin_lock_init(&nvmeq->cq_poll_lock);
+	 */
 	spin_lock(&nvmeq->cq_poll_lock);
 	found = nvme_process_cq(nvmeq, &start, &end, -1);
 	spin_unlock(&nvmeq->cq_poll_lock);
@@ -1090,6 +1755,12 @@ static int nvme_poll(struct blk_mq_hw_ctx *hctx)
 	return found;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4584| <<nvme_async_event_work>> ctrl->ops->submit_async_event(ctrl);
+ *
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.submit_async_event()
+ */
 static void nvme_pci_submit_async_event(struct nvme_ctrl *ctrl)
 {
 	struct nvme_dev *dev = to_nvme_dev(ctrl);
@@ -1098,10 +1769,22 @@ static void nvme_pci_submit_async_event(struct nvme_ctrl *ctrl)
 
 	memset(&c, 0, sizeof(c));
 	c.common.opcode = nvme_admin_async_event;
+	/*
+	 * 这里command_id是31
+	 *
+	 * 关于admin ring buffer和async event, nvme的admin queue的
+	 * tagset有30个元素, 但是ring buffer有32个,除了一个用来表示
+	 * full, 另外一个用在async event
+	 */
 	c.common.command_id = NVME_AQ_BLK_MQ_DEPTH;
 	nvme_submit_cmd(nvmeq, &c, true);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1755| <<adapter_delete_cq>> return adapter_delete_queue(dev, nvme_admin_delete_cq, cqid);
+ *   - drivers/nvme/host/pci.c|1764| <<adapter_delete_sq>> return adapter_delete_queue(dev, nvme_admin_delete_sq, sqid);
+ */
 static int adapter_delete_queue(struct nvme_dev *dev, u8 opcode, u16 id)
 {
 	struct nvme_command c;
@@ -1113,6 +1796,13 @@ static int adapter_delete_queue(struct nvme_dev *dev, u8 opcode, u16 id)
 	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1825| <<nvme_create_queue>> result = adapter_alloc_cq(dev, qid, nvmeq, vector);
+ *
+ * admin的sq和cq的dma地址是写入bar的
+ * 而io queue的dma通过admin的cmd下发设置
+ */
 static int adapter_alloc_cq(struct nvme_dev *dev, u16 qid,
 		struct nvme_queue *nvmeq, s16 vector)
 {
@@ -1137,6 +1827,13 @@ static int adapter_alloc_cq(struct nvme_dev *dev, u16 qid,
 	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1829| <<nvme_create_queue>> result = adapter_alloc_sq(dev, qid, nvmeq);
+ *
+ * admin的sq和cq的dma地址是写入bar的
+ * 而io queue的dma通过admin的cmd下发设置
+ */
 static int adapter_alloc_sq(struct nvme_dev *dev, u16 qid,
 						struct nvme_queue *nvmeq)
 {
@@ -1167,16 +1864,30 @@ static int adapter_alloc_sq(struct nvme_dev *dev, u16 qid,
 	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1916| <<nvme_create_queue>> adapter_delete_cq(dev, qid);
+ */
 static int adapter_delete_cq(struct nvme_dev *dev, u16 cqid)
 {
 	return adapter_delete_queue(dev, nvme_admin_delete_cq, cqid);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1914| <<nvme_create_queue>> adapter_delete_sq(dev, qid);
+ */
 static int adapter_delete_sq(struct nvme_dev *dev, u16 sqid)
 {
 	return adapter_delete_queue(dev, nvme_admin_delete_sq, sqid);
 }
 
+/*
+ * 在以下使用abort_endio():
+ *   - drivers/nvme/host/pci.c|2006| <<nvme_timeout>> blk_execute_rq_nowait(abort_req->q, NULL, abort_req, 0, abort_endio);
+ *
+ * admin的abort request的callback
+ */
 static void abort_endio(struct request *req, blk_status_t error)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -1184,10 +1895,27 @@ static void abort_endio(struct request *req, blk_status_t error)
 
 	dev_warn(nvmeq->dev->ctrl.device,
 		 "Abort status: 0x%x", nvme_req(req)->status);
+	/*
+	 * 在以下使用abort_limit:
+	 *   - drivers/nvme/host/core.c|3345| <<nvme_init_identify>> atomic_set(&ctrl->abort_limit, id->acl + 1);
+	 *   - drivers/nvme/host/pci.c|1715| <<abort_endio>> atomic_inc(&nvmeq->dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1862| <<nvme_timeout>> if (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {
+	 *   - drivers/nvme/host/pci.c|1863| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1887| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 */
 	atomic_inc(&nvmeq->dev->ctrl.abort_limit);
 	blk_mq_free_request(req);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1900| <<nvme_timeout>> if (nvme_should_reset(dev, csts)) {
+ *
+ * csts相当于:
+ * u32 csts = readl(dev->bar + NVME_REG_CSTS);
+ *
+ * 一个注释: 如果当前状态是NVME_CTRL_RESETTING或者NVME_CTRL_CONNECTING, 返回false
+ */
 static bool nvme_should_reset(struct nvme_dev *dev, u32 csts)
 {
 
@@ -1214,6 +1942,10 @@ static bool nvme_should_reset(struct nvme_dev *dev, u32 csts)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1901| <<nvme_timeout>> nvme_warn_reset(dev, csts);
+ */
 static void nvme_warn_reset(struct nvme_dev *dev, u32 csts)
 {
 	/* Read a config register to help see what died. */
@@ -1232,6 +1964,30 @@ static void nvme_warn_reset(struct nvme_dev *dev, u32 csts)
 			 csts, result);
 }
 
+/*
+ * Upon completion of the Abort command, the controller posts a completion queue entry to the Admin
+ * Completion Queue indicating the status for the Abort command and indicating whether the command to
+ * abort was aborted. Dword 0 of the completion queue entry indicates whether the command to abort was
+ * aborted.
+ * If the command to abort was successfully aborted, then a completion queue entry for the aborted command
+ * shall be posted to the appropriate Admin or I/O Completion Queue with a status of Command Abort
+ * Requested before the completion queue entry for the Abort command is posted to the Admin Completion
+ * Queue, and bit 0 of Dword 0 shall be cleared to '0' in the completion queue entry for the Abort command.
+ * If the command to abort was not aborted for any reason, then bit 0 of Dword 0 shall be set to ‘1’ in the
+ * completion queue entry for the Abort command.
+ *
+ * called by:
+ *   - block/blk-mq.c|1039| <<blk_mq_rq_timed_out>> ret = req->q->mq_ops->timeout(req, reserved);
+ *
+ * struct blk_mq_ops nvme_mq_admin_ops.timeout = nvme_timeout()
+ * struct blk_mq_ops nvme_mq_ops.timeout = nvme_timeout()
+ *
+ * blk_rq_timed_out_timer()
+ * -> 调用q->timeout_work = blk_mq_timeout_work()
+ *     -> 为每一个inflight request调用blk_mq_check_expired()
+ *        -> expire了就调用blk_mq_rq_timed_out()
+ *           -> req->q->mq_ops->timeout = nvme_timeout()
+ */
 static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -1239,6 +1995,7 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 	struct nvme_dev *dev = nvmeq->dev;
 	struct request *abort_req;
 	struct nvme_command cmd;
+	/* Controller Status */
 	u32 csts = readl(dev->bar + NVME_REG_CSTS);
 
 	/* If PCI error recovery process is happening, we cannot reset or
@@ -1251,6 +2008,9 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 	/*
 	 * Reset immediately if the controller is failed
 	 */
+	/*
+	 * 一个注释: 如果当前状态是NVME_CTRL_RESETTING或者NVME_CTRL_CONNECTING, 返回fals
+	 */
 	if (nvme_should_reset(dev, csts)) {
 		nvme_warn_reset(dev, csts);
 		nvme_dev_disable(dev, false);
@@ -1261,6 +2021,9 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 	/*
 	 * Did we miss an interrupt?
 	 */
+	/*
+	 * nvme_poll_irqdisable()返回的是cq上处理了的数目
+	 */
 	if (nvme_poll_irqdisable(nvmeq, req->tag)) {
 		dev_warn(dev->ctrl.device,
 			 "I/O %d QID %d timeout, completion polled\n",
@@ -1296,6 +2059,13 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
  	 * command was already aborted once before and still hasn't been
  	 * returned to the driver, or if this is the admin queue.
 	 */
+	/*
+	 * 修改aborted的地方:
+	 *   - drivers/nvme/host/pci.c|1264| <<nvme_queue_rq>> iod->aborted = 0;
+	 *   - drivers/nvme/host/pci.c|1841| <<nvme_timeout>> iod->aborted = 1;
+	 *
+	 * 在下面的某个地方修改iod->aborted
+	 */
 	if (!nvmeq->qid || iod->aborted) {
 		dev_warn(dev->ctrl.device,
 			 "I/O %d QID %d timeout, reset controller\n",
@@ -1307,10 +2077,32 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 		return BLK_EH_DONE;
 	}
 
+	/*
+	 * 在以下使用abort_limit:
+	 *   - drivers/nvme/host/core.c|3345| <<nvme_init_identify>> atomic_set(&ctrl->abort_limit, id->acl + 1);
+	 *   - drivers/nvme/host/pci.c|1715| <<abort_endio>> atomic_inc(&nvmeq->dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1862| <<nvme_timeout>> if (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {
+	 *   - drivers/nvme/host/pci.c|1863| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1887| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 *
+	 * To abort a large number of commands (e.g., a larger number of commands than the limit listed in the ACL
+	 * field), the host should follow the procedures described in section 7.3.3 to delete the I/O Submission Queue
+	 * and recreate the I/O Submission Queue.
+	 *
+	 * nvme controller只允许abort有限的(id->acl+1)的req.
+	 * 这里判断如果太多了, 就不能abort了. 重新reset timer,等待下一次.
+	 */
 	if (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {
 		atomic_inc(&dev->ctrl.abort_limit);
 		return BLK_EH_RESET_TIMER;
 	}
+	/*
+	 * 修改aborted的地方:
+	 *   - drivers/nvme/host/pci.c|1264| <<nvme_queue_rq>> iod->aborted = 0;
+	 *   - drivers/nvme/host/pci.c|1841| <<nvme_timeout>> iod->aborted = 1;
+	 *
+	 * 下面的部分是下发一个nvme_admin_abort_cmd来abort某个request->tag
+	 */
 	iod->aborted = 1;
 
 	memset(&cmd, 0, sizeof(cmd));
@@ -1331,6 +2123,9 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 
 	abort_req->timeout = ADMIN_TIMEOUT;
 	abort_req->end_io_data = NULL;
+	/*
+	 * abort的callback是abort_endio()
+	 */
 	blk_execute_rq_nowait(abort_req->q, NULL, abort_req, 0, abort_endio);
 
 	/*
@@ -1341,8 +2136,22 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 	return BLK_EH_RESET_TIMER;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2038| <<nvme_free_queues>> nvme_free_queue(&dev->queues[i]);
+ *
+ * 核心思想是释放sq和cq的dma ring buffer
+ */
 static void nvme_free_queue(struct nvme_queue *nvmeq)
 {
+	/*
+	 * 在以下使用cq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1294| <<adapter_alloc_cq>> c.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1514| <<nvme_free_queue>> (void *)nvmeq->cqes, nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1637| <<nvme_alloc_queue>> &nvmeq->cq_dma_addr, GFP_KERNEL);
+	 *   - drivers/nvme/host/pci.c|1657| <<nvme_alloc_queue>> nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1889| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
+	 */
 	dma_free_coherent(nvmeq->dev->dev, CQ_SIZE(nvmeq),
 				(void *)nvmeq->cqes, nvmeq->cq_dma_addr);
 	if (!nvmeq->sq_cmds)
@@ -1352,17 +2161,35 @@ static void nvme_free_queue(struct nvme_queue *nvmeq)
 		pci_free_p2pmem(to_pci_dev(nvmeq->dev->dev),
 				nvmeq->sq_cmds, SQ_SIZE(nvmeq));
 	} else {
+		/*
+		 *   - drivers/nvme/host/pci.c|1324| <<adapter_alloc_sq>> c.create_sq.prp1 = cpu_to_le64(nvmeq->sq_dma_addr);
+		 *   - drivers/nvme/host/pci.c|1523| <<nvme_free_queue>> nvmeq->sq_cmds, nvmeq->sq_dma_addr);
+		 *   - drivers/nvme/host/pci.c|1609| <<nvme_alloc_sq_cmds>> nvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev,
+		 *   - drivers/nvme/host/pci.c|1611| <<nvme_alloc_sq_cmds>> if (nvmeq->sq_dma_addr) {
+		 *   - drivers/nvme/host/pci.c|1621| <<nvme_alloc_sq_cmds>> &nvmeq->sq_dma_addr, GFP_KERNEL);
+		 *   - drivers/nvme/host/pci.c|1888| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
+		 */
 		dma_free_coherent(nvmeq->dev->dev, SQ_SIZE(nvmeq),
 				nvmeq->sq_cmds, nvmeq->sq_dma_addr);
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3403| <<nvme_dev_add>> nvme_free_queues(dev, dev->online_queues);
+ *   - drivers/nvme/host/pci.c|4226| <<nvme_remove>> nvme_free_queues(dev, 0);
+ *
+ * 核心思想是释放sq和cq的dma ring buffer
+ * queue是从dev->ctrl.queue_count - 1到参数的lowest
+ * 当被nvme_free_queues()调用的时候就是全部queue (dev->ctrl.queue_count-1到0)
+ */
 static void nvme_free_queues(struct nvme_dev *dev, int lowest)
 {
 	int i;
 
 	for (i = dev->ctrl.queue_count - 1; i >= lowest; i--) {
 		dev->ctrl.queue_count--;
+		/* 核心思想是释放sq和cq的dma ring buffer */
 		nvme_free_queue(&dev->queues[i]);
 	}
 }
@@ -1371,22 +2198,63 @@ static void nvme_free_queues(struct nvme_dev *dev, int lowest)
  * nvme_suspend_queue - put queue into suspended state
  * @nvmeq: queue to suspend
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2098| <<nvme_suspend_io_queues>> nvme_suspend_queue(&dev->queues[i]);
+ *   - drivers/nvme/host/pci.c|3652| <<nvme_dev_disable>> nvme_suspend_queue(&dev->queues[0]);
+ */
 static int nvme_suspend_queue(struct nvme_queue *nvmeq)
 {
+	/*
+	 * 在以下使用NVMEQ_ENABLED:
+	 *   - drivers/nvme/host/pci.c|1001| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+	 *   - drivers/nvme/host/pci.c|1560| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1805| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|2468| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *   - drivers/nvme/host/pci.c|2516| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *
+	 * 这样nvme_queue_rq()就会退出了
+	 */
 	if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
 		return 1;
 
 	/* ensure that nvme_queue_rq() sees NVMEQ_ENABLED cleared */
 	mb();
 
+	/*
+	 * 修改nvme_dev->online_queues的地方:
+	 *   - drivers/nvme/host/pci.c|2167| <<nvme_suspend_queue>> nvmeq->dev->online_queues--;
+	 *   - drivers/nvme/host/pci.c|2428| <<nvme_init_queue>> dev->online_queues++;
+	 *   - drivers/nvme/host/pci.c|2498| <<nvme_create_queue>> dev->online_queues--;
+	 *   - drivers/nvme/host/pci.c|2744| <<nvme_pci_configure_admin_queue>> dev->online_queues--;
+	 */
 	nvmeq->dev->online_queues--;
+	/*
+	 * blk_mq_quiesce_queue():
+	 *     wait until all ongoing dispatches have finished
+	 *
+	 * 这里只针对admin queue
+	 */
 	if (!nvmeq->qid && nvmeq->dev->ctrl.admin_q)
 		blk_mq_quiesce_queue(nvmeq->dev->ctrl.admin_q);
+	/*
+	 * 在以下使用NVMEQ_POLLED:
+	 *   - drivers/nvme/host/pci.c|1228| <<nvme_poll_irqdisable>> if (test_bit(NVMEQ_POLLED, &nvmeq->flags)) {
+	 *   - drivers/nvme/host/pci.c|1325| <<adapter_alloc_cq>> if (!test_bit(NVMEQ_POLLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1592| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_POLLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1816| <<nvme_create_queue>> set_bit(NVMEQ_POLLED, &nvmeq->flags);
+	 */
 	if (!test_and_clear_bit(NVMEQ_POLLED, &nvmeq->flags))
 		pci_free_irq(to_pci_dev(nvmeq->dev->dev), nvmeq->cq_vector, nvmeq);
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2767| <<nvme_setup_io_queues>> nvme_suspend_io_queues(dev);
+ *   - drivers/nvme/host/pci.c|3124| <<nvme_dev_disable>> nvme_suspend_io_queues(dev);
+ */
 static void nvme_suspend_io_queues(struct nvme_dev *dev)
 {
 	int i;
@@ -1395,6 +2263,10 @@ static void nvme_suspend_io_queues(struct nvme_dev *dev)
 		nvme_suspend_queue(&dev->queues[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3122| <<nvme_dev_disable>> nvme_disable_admin_queue(dev, shutdown);
+ */
 static void nvme_disable_admin_queue(struct nvme_dev *dev, bool shutdown)
 {
 	struct nvme_queue *nvmeq = &dev->queues[0];
@@ -1407,6 +2279,12 @@ static void nvme_disable_admin_queue(struct nvme_dev *dev, bool shutdown)
 	nvme_poll_irqdisable(nvmeq, -1);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3190| <<nvme_setup_io_queues>> result = nvme_cmb_qdepth(dev, nr_io_queues,
+ *
+ * cmb的内存可能不够, 这里根据cmb的大小和queue的数目重新计算q_depth并返回
+ */
 static int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,
 				int entry_size)
 {
@@ -1414,6 +2292,15 @@ static int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,
 	unsigned q_size_aligned = roundup(q_depth * entry_size,
 					  dev->ctrl.page_size);
 
+	/*
+	 * 在以下使用cmb_size:
+	 *   - drivers/nvme/host/pci.c|1548| <<nvme_cmb_qdepth>> if (q_size_aligned * nr_io_queues > dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1549| <<nvme_cmb_qdepth>> u64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);
+	 *   - drivers/nvme/host/pci.c|1946| <<nvme_map_cmb>> if (dev->cmb_size)
+	 *   - drivers/nvme/host/pci.c|1976| <<nvme_map_cmb>> dev->cmb_size = size;
+	 *   - drivers/nvme/host/pci.c|1991| <<nvme_release_cmb>> if (dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1994| <<nvme_release_cmb>> dev->cmb_size = 0;
+	 */
 	if (q_size_aligned * nr_io_queues > dev->cmb_size) {
 		u64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);
 		mem_per_q = round_down(mem_per_q, dev->ctrl.page_size);
@@ -1431,17 +2318,31 @@ static int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,
 	return q_depth;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1641| <<nvme_alloc_queue>> if (nvme_alloc_sq_cmds(dev, nvmeq, qid))
+ *
+ * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+ */
 static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
 				int qid)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 
+	/*
+	 * qid不能是0说明admin queue不用cmb
+	 */
 	if (qid && dev->cmb_use_sqes && (dev->cmbsz & NVME_CMBSZ_SQS)) {
 		nvmeq->sq_cmds = pci_alloc_p2pmem(pdev, SQ_SIZE(nvmeq));
 		if (nvmeq->sq_cmds) {
 			nvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev,
 							nvmeq->sq_cmds);
 			if (nvmeq->sq_dma_addr) {
+				/*
+				 * 在以下使用NVMEQ_SQ_CMB:
+				 *   - drivers/nvme/host/pci.c|2076| <<nvme_free_queue>> if (test_and_clear_bit(NVMEQ_SQ_CMB, &nvmeq->flags)) {
+				 *   - drivers/nvme/host/pci.c|2229| <<nvme_alloc_sq_cmds>> set_bit(NVMEQ_SQ_CMB, &nvmeq->flags);
+				 */
 				set_bit(NVMEQ_SQ_CMB, &nvmeq->flags);
 				return 0;
 			}
@@ -1457,20 +2358,60 @@ static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1879| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/pci.c|1913| <<nvme_create_io_queues>> if (nvme_alloc_queue(dev, i, dev->q_depth)) {
+ *
+ * 分配nvme_queue, 会分配ring buffer
+ * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+ * 分配cq的ring buffer的时候, 只用dma
+ */
 static int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)
 {
+	/*
+	 * struct nvme_dev:
+	 *   - struct nvme_queue *queues;
+	 * 是指针
+	 */
 	struct nvme_queue *nvmeq = &dev->queues[qid];
 
 	if (dev->ctrl.queue_count > qid)
 		return 0;
 
+	/*
+	 * 在以下使用io_sqes:
+	 *   - drivers/nvme/host/pci.c|1592| <<nvme_alloc_queue>> nvmeq->sqes = qid ? dev->io_sqes : NVME_ADM_SQES;
+	 *   - drivers/nvme/host/pci.c|2559| <<nvme_pci_enable>> dev->io_sqes = 7;
+	 *   - drivers/nvme/host/pci.c|2561| <<nvme_pci_enable>> dev->io_sqes = NVME_NVM_IOSQES;
+	 */
 	nvmeq->sqes = qid ? dev->io_sqes : NVME_ADM_SQES;
+	/*
+	 * SQ_SIZE(q)和CQ_SIZE(q)都是用q_depth
+	 *
+	 * nvme的admin queue的tagset有30个元素, 但是ring buffer有32个
+	 * 除了一个用来表示full, 另外一个用在async event
+	 */
 	nvmeq->q_depth = depth;
+	/*
+	 * 在以下使用cq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1294| <<adapter_alloc_cq>> c.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1514| <<nvme_free_queue>> (void *)nvmeq->cqes, nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1637| <<nvme_alloc_queue>> &nvmeq->cq_dma_addr, GFP_KERNEL);
+	 *   - drivers/nvme/host/pci.c|1657| <<nvme_alloc_queue>> nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1889| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
+	 *
+	 * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+	 * 但是下面分配cq的ring buffer的时候, 只分配dma
+	 */
 	nvmeq->cqes = dma_alloc_coherent(dev->dev, CQ_SIZE(nvmeq),
 					 &nvmeq->cq_dma_addr, GFP_KERNEL);
 	if (!nvmeq->cqes)
 		goto free_nvmeq;
 
+	/*
+	 * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+	 */
 	if (nvme_alloc_sq_cmds(dev, nvmeq, qid))
 		goto free_cqdma;
 
@@ -1492,6 +2433,33 @@ static int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)
 	return -ENOMEM;
 }
 
+/*
+ * qemu下nvme的一个例子:
+ * [0] msi_domain_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [    0.682898] orabug: msi_domain_set_affinity() mask=f
+ * [    0.695652] orabug: msi_domain_set_affinity() mask=f
+ * [    0.706281] orabug: msi_domain_set_affinity() mask=1
+ * [    0.715123] orabug: msi_domain_set_affinity() mask=2
+ * [    0.724427] orabug: msi_domain_set_affinity() mask=4
+ * [    0.733339] orabug: msi_domain_set_affinity() mask=8
+ *
+ * called by:
+ *   - drivers/nvme/host/pci.c|1555| <<nvme_create_queue>> result = queue_request_irq(nvmeq);
+ *   - drivers/nvme/host/pci.c|1703| <<nvme_pci_configure_admin_queue>> result = queue_request_irq(nvmeq);
+ *   - drivers/nvme/host/pci.c|2160| <<nvme_setup_io_queues>> result = queue_request_irq(adminq);
+ */
 static int queue_request_irq(struct nvme_queue *nvmeq)
 {
 	struct pci_dev *pdev = to_pci_dev(nvmeq->dev->dev);
@@ -1506,6 +2474,11 @@ static int queue_request_irq(struct nvme_queue *nvmeq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1811| <<nvme_create_queue>> nvme_init_queue(nvmeq, qid);
+ *   - drivers/nvme/host/pci.c|1989| <<nvme_pci_configure_admin_queue>> nvme_init_queue(nvmeq, 0);
+ */
 static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
 {
 	struct nvme_dev *dev = nvmeq->dev;
@@ -1521,6 +2494,10 @@ static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
 	wmb(); /* ensure the first interrupt sees the initialization */
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2038| <<nvme_create_io_queues>> ret = nvme_create_queue(&dev->queues[i], i, polled);
+ */
 static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 {
 	struct nvme_dev *dev = nvmeq->dev;
@@ -1529,6 +2506,14 @@ static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 
 	clear_bit(NVMEQ_DELETE_ERROR, &nvmeq->flags);
 
+	/*
+	 * 在以下使用NVMEQ_POLLED:
+	 *   - drivers/nvme/host/pci.c|1228| <<nvme_poll_irqdisable>> if (test_bit(NVMEQ_POLLED, &nvmeq->flags)) {
+	 *   - drivers/nvme/host/pci.c|1325| <<adapter_alloc_cq>> if (!test_bit(NVMEQ_POLLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1592| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_POLLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1816| <<nvme_create_queue>> set_bit(NVMEQ_POLLED, &nvmeq->flags);
+	 */
+
 	/*
 	 * A queue's vector matches the queue identifier unless the controller
 	 * has only one vector available.
@@ -1538,16 +2523,29 @@ static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 	else
 		set_bit(NVMEQ_POLLED, &nvmeq->flags);
 
+	/*
+	 * admin的sq和cq的dma地址是写入bar的
+	 * 而io queue的dma通过admin的cmd下发设置
+	 */
 	result = adapter_alloc_cq(dev, qid, nvmeq, vector);
 	if (result)
 		return result;
 
+	/*
+	 * admin的sq和cq的dma地址是写入bar的
+	 * 而io queue的dma通过admin的cmd下发设置
+	 */
 	result = adapter_alloc_sq(dev, qid, nvmeq);
 	if (result < 0)
 		return result;
 	if (result)
 		goto release_cq;
 
+	/*
+	 * 设置cq_vector的地方:
+	 *   - drivers/nvme/host/pci.c|1796| <<nvme_create_queue>> nvmeq->cq_vector = vector;
+	 *   - drivers/nvme/host/pci.c|1969| <<nvme_pci_configure_admin_queue>> nvmeq->cq_vector = 0;
+	 */
 	nvmeq->cq_vector = vector;
 	nvme_init_queue(nvmeq, qid);
 
@@ -1557,6 +2555,15 @@ static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 			goto release_sq;
 	}
 
+	/*
+	 * 在以下使用NVMEQ_ENABLED:
+	 *   - drivers/nvme/host/pci.c|1001| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+	 *   - drivers/nvme/host/pci.c|1560| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1805| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|2468| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *   - drivers/nvme/host/pci.c|2516| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+	 */
 	set_bit(NVMEQ_ENABLED, &nvmeq->flags);
 	return result;
 
@@ -1568,6 +2575,10 @@ static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 	return result;
 }
 
+/*
+ * 在以下使用nvme_mq_admin_ops:
+ *   - drivers/nvme/host/pci.c|1968| <<nvme_alloc_admin_tags>> dev->admin_tagset.ops = &nvme_mq_admin_ops;
+ */
 static const struct blk_mq_ops nvme_mq_admin_ops = {
 	.queue_rq	= nvme_queue_rq,
 	.complete	= nvme_pci_complete_rq,
@@ -1577,6 +2588,10 @@ static const struct blk_mq_ops nvme_mq_admin_ops = {
 	.timeout	= nvme_timeout,
 };
 
+/*
+ * 在以下使用nvme_mq_ops:
+ *   - drivers/nvme/host/pci.c|2851| <<nvme_dev_add>> dev->tagset.ops = &nvme_mq_ops;
+ */
 static const struct blk_mq_ops nvme_mq_ops = {
 	.queue_rq	= nvme_queue_rq,
 	.complete	= nvme_pci_complete_rq,
@@ -1588,6 +2603,12 @@ static const struct blk_mq_ops nvme_mq_ops = {
 	.poll		= nvme_poll,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2577| <<nvme_alloc_admin_tags>> nvme_dev_remove_admin(dev);
+ *   - drivers/nvme/host/pci.c|4365| <<nvme_remove>> nvme_dev_remove_admin(dev);
+ *   - drivers/nvme/host/pci.c|4374| <<nvme_remove>> nvme_dev_remove_admin(dev);
+ */
 static void nvme_dev_remove_admin(struct nvme_dev *dev)
 {
 	if (dev->ctrl.admin_q && !blk_queue_dying(dev->ctrl.admin_q)) {
@@ -1597,17 +2618,40 @@ static void nvme_dev_remove_admin(struct nvme_dev *dev)
 		 * queue to flush these to completion.
 		 */
 		blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+		/*
+		 * blk_cleanup_queue - shutdown a request queue
+		 *
+		 * Mark @q DYING, drain all pending requests, mark @q DEAD, destroy and
+		 * put it.  All future requests will be failed immediately with -ENODEV.
+		 */
 		blk_cleanup_queue(dev->ctrl.admin_q);
+		/*
+		 * request_queue都没了, 这里可以释放tagset了
+		 */
 		blk_mq_free_tag_set(&dev->admin_tagset);
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3073| <<nvme_reset_work>> result = nvme_alloc_admin_tags(dev);
+ *
+ * 初始化nvme_dev->admin_tagset.
+ * struct nvme_dev:
+ *   - struct nvme_queue *queues;
+ *   - struct blk_mq_tag_set tagset;
+ *   - struct blk_mq_tag_set admin_tagset;
+ *
+ * 这是初始化admin专用的tagset, 就一个queue (nr_hw_queues = 1)
+ * 会调用blk_mq_alloc_tag_set()和blk_mq_init_queue()
+ */
 static int nvme_alloc_admin_tags(struct nvme_dev *dev)
 {
 	if (!dev->ctrl.admin_q) {
 		dev->admin_tagset.ops = &nvme_mq_admin_ops;
 		dev->admin_tagset.nr_hw_queues = 1;
 
+		/* 30 */
 		dev->admin_tagset.queue_depth = NVME_AQ_MQ_TAG_DEPTH;
 		dev->admin_tagset.timeout = ADMIN_TIMEOUT;
 		dev->admin_tagset.numa_node = dev_to_node(dev->dev);
@@ -1619,11 +2663,17 @@ static int nvme_alloc_admin_tags(struct nvme_dev *dev)
 			return -ENOMEM;
 		dev->ctrl.admin_tagset = &dev->admin_tagset;
 
+		/*
+		 * admin_q是struct request_queue
+		 */
 		dev->ctrl.admin_q = blk_mq_init_queue(&dev->admin_tagset);
 		if (IS_ERR(dev->ctrl.admin_q)) {
 			blk_mq_free_tag_set(&dev->admin_tagset);
 			return -ENOMEM;
 		}
+		/*
+		 * admin_q是struct request_queue
+		 */
 		if (!blk_get_queue(dev->ctrl.admin_q)) {
 			nvme_dev_remove_admin(dev);
 			dev->ctrl.admin_q = NULL;
@@ -1635,11 +2685,25 @@ static int nvme_alloc_admin_tags(struct nvme_dev *dev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1864| <<nvme_pci_configure_admin_queue>> result = nvme_remap_bar(dev, db_bar_size(dev, 0));
+ *   - drivers/nvme/host/pci.c|2406| <<nvme_setup_io_queues>> size = db_bar_size(dev, nr_io_queues);
+ */
 static unsigned long db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)
 {
+	/*
+	 * 从0x1000开始才是door bell的register
+	 */
 	return NVME_REG_DBS + ((nr_io_queues + 1) * 8 * dev->db_stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1864| <<nvme_pci_configure_admin_queue>> result = nvme_remap_bar(dev, db_bar_size(dev, 0));
+ *   - drivers/nvme/host/pci.c|2407| <<nvme_setup_io_queues>> result = nvme_remap_bar(dev, size);
+ *   - drivers/nvme/host/pci.c|3087| <<nvme_dev_map>> if (nvme_remap_bar(dev, NVME_REG_DBS + 4096))
+ */
 static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -1650,6 +2714,7 @@ static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
 		return -ENOMEM;
 	if (dev->bar)
 		iounmap(dev->bar);
+	/* void __iomem *bar; */
 	dev->bar = ioremap(pci_resource_start(pdev, 0), size);
 	if (!dev->bar) {
 		dev->bar_mapped_size = 0;
@@ -1661,6 +2726,14 @@ static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3035| <<nvme_reset_work>> result = nvme_pci_configure_admin_queue(dev);
+ *
+ * 初始化(很复杂)nvme_dev->queues[0]
+ * struct nvme_dev:
+ *   - struct nvme_queue *queues;
+ */
 static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 {
 	int result;
@@ -1671,6 +2744,7 @@ static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 	if (result < 0)
 		return result;
 
+	/* bool subsystem; */
 	dev->subsystem = readl(dev->bar + NVME_REG_VS) >= NVME_VS(1, 1, 0) ?
 				NVME_CAP_NSSRC(dev->ctrl.cap) : 0;
 
@@ -1678,10 +2752,22 @@ static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 	    (readl(dev->bar + NVME_REG_CSTS) & NVME_CSTS_NSSRO))
 		writel(NVME_CSTS_NSSRO, dev->bar + NVME_REG_CSTS);
 
+	/*
+	 * 对于pci来说, nvme_enable_ctrl()和nvme_disable_ctrl()都是在nvme_pci_configure_admin_queue()调用
+	 */
 	result = nvme_disable_ctrl(&dev->ctrl);
 	if (result < 0)
 		return result;
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|1879| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+	 *   - drivers/nvme/host/pci.c|1913| <<nvme_create_io_queues>> if (nvme_alloc_queue(dev, i, dev->q_depth)) {
+	 *
+	 * 分配nvme_queue, 会分配ring buffer
+	 * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+	 * 分配cq的ring buffer的时候, 只用dma
+	 */
 	result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
 	if (result)
 		return result;
@@ -1690,32 +2776,80 @@ static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 	aqa = nvmeq->q_depth - 1;
 	aqa |= aqa << 16;
 
+	/* Admin Queue Attributes */
 	writel(aqa, dev->bar + NVME_REG_AQA);
+	/* Admin SQ Base Address */
 	lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
+	/* Admin CQ Base Address */
 	lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
 
+	/*
+	 * 在以下调用nvme_disable_ctrl(), 不是nvme_enable_ctrl()!!
+	 *   - drivers/nvme/host/pci.c|1572| <<nvme_disable_admin_queue>> nvme_disable_ctrl(&dev->ctrl);
+	 *   - drivers/nvme/host/pci.c|1875| <<nvme_pci_configure_admin_queue>> result = nvme_disable_ctrl(&dev->ctrl);
+	 *   - drivers/nvme/host/rdma.c|1894| <<nvme_rdma_shutdown_ctrl>> nvme_disable_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/tcp.c|1933| <<nvme_tcp_teardown_ctrl>> nvme_disable_ctrl(ctrl);
+	 */
 	result = nvme_enable_ctrl(&dev->ctrl);
 	if (result)
 		return result;
 
+	/*
+	 * 设置cq_vector的地方:
+	 *   - drivers/nvme/host/pci.c|1796| <<nvme_create_queue>> nvmeq->cq_vector = vector;
+	 *   - drivers/nvme/host/pci.c|1969| <<nvme_pci_configure_admin_queue>> nvmeq->cq_vector = 0;
+	 */
 	nvmeq->cq_vector = 0;
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|1811| <<nvme_create_queue>> nvme_init_queue(nvmeq, qid);
+	 *   - drivers/nvme/host/pci.c|1989| <<nvme_pci_configure_admin_queue>> nvme_init_queue(nvmeq, 0);
+	 */
 	nvme_init_queue(nvmeq, 0);
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|1555| <<nvme_create_queue>> result = queue_request_irq(nvmeq);
+	 *   - drivers/nvme/host/pci.c|1703| <<nvme_pci_configure_admin_queue>> result = queue_request_irq(nvmeq);
+	 *   - drivers/nvme/host/pci.c|2160| <<nvme_setup_io_queues>> result = queue_request_irq(adminq);
+	 */
 	result = queue_request_irq(nvmeq);
 	if (result) {
 		dev->online_queues--;
 		return result;
 	}
 
+	/*
+	 * 在以下使用NVMEQ_ENABLED:
+	 *   - drivers/nvme/host/pci.c|1001| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+	 *   - drivers/nvme/host/pci.c|1560| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1805| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|2468| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *   - drivers/nvme/host/pci.c|2516| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+	 */
 	set_bit(NVMEQ_ENABLED, &nvmeq->flags);
 	return result;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2595| <<nvme_setup_io_queues>> result = nvme_create_io_queues(dev);
+ */
 static int nvme_create_io_queues(struct nvme_dev *dev)
 {
 	unsigned i, max, rw_queues;
 	int ret = 0;
 
 	for (i = dev->ctrl.queue_count; i <= dev->max_qid; i++) {
+		/*
+		 * called by:
+		 *   - drivers/nvme/host/pci.c|1879| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+		 *   - drivers/nvme/host/pci.c|1913| <<nvme_create_io_queues>> if (nvme_alloc_queue(dev, i, dev->q_depth)) {
+		 *
+		 * 分配nvme_queue, 会分配ring buffer
+		 * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+		 * 分配cq的ring buffer的时候, 只用dma
+		 */
 		if (nvme_alloc_queue(dev, i, dev->q_depth)) {
 			ret = -ENOMEM;
 			break;
@@ -1733,6 +2867,12 @@ static int nvme_create_io_queues(struct nvme_dev *dev)
 	for (i = dev->online_queues; i <= max; i++) {
 		bool polled = i > rw_queues;
 
+		/*
+		 * struct nvme_dev:
+		 *   - struct nvme_queue *queues;
+		 *   - struct blk_mq_tag_set tagset;
+		 *   - struct blk_mq_tag_set admin_tagset;
+		 */
 		ret = nvme_create_queue(&dev->queues[i], i, polled);
 		if (ret)
 			break;
@@ -1758,6 +2898,11 @@ static ssize_t nvme_cmb_show(struct device *dev,
 }
 static DEVICE_ATTR(cmb, S_IRUGO, nvme_cmb_show, NULL);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2851| <<nvme_map_cmb>> size = nvme_cmb_size_unit(dev) * nvme_cmb_size(dev);
+ *   - drivers/nvme/host/pci.c|2852| <<nvme_map_cmb>> offset = nvme_cmb_size_unit(dev) * NVME_CMB_OFST(dev->cmbloc);
+ */
 static u64 nvme_cmb_size_unit(struct nvme_dev *dev)
 {
 	u8 szu = (dev->cmbsz >> NVME_CMBSZ_SZU_SHIFT) & NVME_CMBSZ_SZU_MASK;
@@ -1765,11 +2910,25 @@ static u64 nvme_cmb_size_unit(struct nvme_dev *dev)
 	return 1ULL << (12 + 4 * szu);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2851| <<nvme_map_cmb>> size = nvme_cmb_size_unit(dev) * nvme_cmb_size(dev);
+ */
 static u32 nvme_cmb_size(struct nvme_dev *dev)
 {
 	return (dev->cmbsz >> NVME_CMBSZ_SZ_SHIFT) & NVME_CMBSZ_SZ_MASK;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2592| <<nvme_pci_enable>> nvme_map_cmb(dev);
+ *
+ * nvme_reset_work()
+ *  -> nvme_pci_enable()
+ *      -> nvme_map_cmd()
+ *
+ * 在qemu的参数设置cmb_size_mb=64会得到dev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ) = 262681
+ */
 static void nvme_map_cmb(struct nvme_dev *dev)
 {
 	u64 size, offset;
@@ -1777,9 +2936,26 @@ static void nvme_map_cmb(struct nvme_dev *dev)
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 	int bar;
 
+	/*
+	 * 在以下使用cmb_size:
+	 *   - drivers/nvme/host/pci.c|1548| <<nvme_cmb_qdepth>> if (q_size_aligned * nr_io_queues > dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1549| <<nvme_cmb_qdepth>> u64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);
+	 *   - drivers/nvme/host/pci.c|1946| <<nvme_map_cmb>> if (dev->cmb_size)
+	 *   - drivers/nvme/host/pci.c|1976| <<nvme_map_cmb>> dev->cmb_size = size;
+	 *   - drivers/nvme/host/pci.c|1991| <<nvme_release_cmb>> if (dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1994| <<nvme_release_cmb>> dev->cmb_size = 0;
+	 */
 	if (dev->cmb_size)
 		return;
 
+	/*
+	 * Controller Memory Buffer Size
+	 * This optional register defines the size of the Controller Memory Buffer (refer to section 4.7). If the controller
+	 * does not support the Controller Memory Buffer feature or if the controller supports the Controller Memory
+	 * Buffer (CAP.CMBS) and CMBMSC.CRE is cleared to ‘0’, then this register shall be cleared to 0h.
+	 *
+	 * 在qemu的参数设置cmb_size_mb=64会得到dev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ) = 262681
+	 */
 	dev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ);
 	if (!dev->cmbsz)
 		return;
@@ -1801,15 +2977,50 @@ static void nvme_map_cmb(struct nvme_dev *dev)
 	if (size > bar_size - offset)
 		size = bar_size - offset;
 
+	/*
+	 * pci_p2pdma_add_resource - add memory for use as p2p memory
+	 * @pdev: the device to add the memory to
+	 * @bar: PCI BAR to add
+	 * @size: size of the memory to add, may be zero to use the whole BAR
+	 * @offset: offset into the PCI BAR
+	 *
+	 * The memory will be given ZONE_DEVICE struct pages so that it may
+	 * be used with any DMA request.
+	 */
 	if (pci_p2pdma_add_resource(pdev, bar, size, offset)) {
 		dev_warn(dev->ctrl.device,
 			 "failed to register the CMB\n");
 		return;
 	}
 
+	/*
+	 * 在以下使用cmb_size:
+	 *   - drivers/nvme/host/pci.c|1548| <<nvme_cmb_qdepth>> if (q_size_aligned * nr_io_queues > dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1549| <<nvme_cmb_qdepth>> u64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);
+	 *   - drivers/nvme/host/pci.c|1946| <<nvme_map_cmb>> if (dev->cmb_size)
+	 *   - drivers/nvme/host/pci.c|1976| <<nvme_map_cmb>> dev->cmb_size = size;
+	 *   - drivers/nvme/host/pci.c|1991| <<nvme_release_cmb>> if (dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1994| <<nvme_release_cmb>> dev->cmb_size = 0;
+	 */
 	dev->cmb_size = size;
+	/*
+	 * 在以下使用cmb_use_sqes:
+	 *   - drivers/nvme/host/pci.c|1570| <<nvme_alloc_sq_cmds>> if (qid && dev->cmb_use_sqes && (dev->cmbsz & NVME_CMBSZ_SQS)) {
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+	 *   - drivers/nvme/host/pci.c|2306| <<nvme_setup_io_queues>> if (dev->cmb_use_sqes) {
+	 *   - drivers/nvme/host/pci.c|2312| <<nvme_setup_io_queues>> dev->cmb_use_sqes = false;
+	 *
+	 * cmb_use_sqes:
+	 * "use controller's memory buffer for I/O SQes"
+	 */
 	dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
 
+	/*
+	 * Published memory can be used by other PCI device drivers for
+	 * peer-2-peer DMA operations. Non-published memory is reserved for
+	 * exclusive use of the device driver that registers the peer-to-peer
+	 * memory.
+	 */
 	if ((dev->cmbsz & (NVME_CMBSZ_WDS | NVME_CMBSZ_RDS)) ==
 			(NVME_CMBSZ_WDS | NVME_CMBSZ_RDS))
 		pci_p2pmem_publish(pdev, true);
@@ -1820,6 +3031,10 @@ static void nvme_map_cmb(struct nvme_dev *dev)
 			 "failed to add sysfs attribute for CMB\n");
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|4390| <<nvme_remove>> nvme_release_cmb(dev);
+ */
 static inline void nvme_release_cmb(struct nvme_dev *dev)
 {
 	if (dev->cmb_size) {
@@ -1829,9 +3044,22 @@ static inline void nvme_release_cmb(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2774| <<nvme_setup_host_mem>> ret = nvme_set_host_mem(dev, enable_bits);
+ *
+ * The Host Memory Buffer feature provides a mechanism for the host to allocate a portion of host memory
+ * for the exclusive use of the controller. After a successful completion of a Set Features command enabling
+ * the host memory buffer, the host shall not write to:
+ * a)The Host Memory Descriptor List (refer to Figure 296); and
+ * b)the associated host memory region (i.e., the memory regions described by the Host Memory
+ * Descriptor List),
+ * until the host memory buffer has been disabled.
+ */
 static int nvme_set_host_mem(struct nvme_dev *dev, u32 bits)
 {
 	u64 dma_addr = dev->host_mem_descs_dma;
+	/* 包含struct nvme_features features;作为union */
 	struct nvme_command c;
 	int ret;
 
@@ -1854,6 +3082,14 @@ static int nvme_set_host_mem(struct nvme_dev *dev, u32 bits)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3095| <<nvme_alloc_host_mem>> nvme_free_host_mem(dev);
+ *   - drivers/nvme/host/pci.c|3137| <<nvme_setup_host_mem>> nvme_free_host_mem(dev);
+ *   - drivers/nvme/host/pci.c|3148| <<nvme_setup_host_mem>> nvme_free_host_mem(dev);
+ *   - drivers/nvme/host/pci.c|3165| <<nvme_setup_host_mem>> nvme_free_host_mem(dev);
+ *   - drivers/nvme/host/pci.c|4391| <<nvme_remove>> nvme_free_host_mem(dev);
+ */
 static void nvme_free_host_mem(struct nvme_dev *dev)
 {
 	int i;
@@ -1876,6 +3112,10 @@ static void nvme_free_host_mem(struct nvme_dev *dev)
 	dev->nr_host_mem_descs = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2721| <<nvme_alloc_host_mem>> if (!__nvme_alloc_host_mem(dev, preferred, chunk_size)) {
+ */
 static int __nvme_alloc_host_mem(struct nvme_dev *dev, u64 preferred,
 		u32 chunk_size)
 {
@@ -1944,6 +3184,10 @@ static int __nvme_alloc_host_mem(struct nvme_dev *dev, u64 preferred,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2763| <<nvme_setup_host_mem>> if (nvme_alloc_host_mem(dev, min, preferred)) {
+ */
 static int nvme_alloc_host_mem(struct nvme_dev *dev, u64 min, u64 preferred)
 {
 	u32 chunk_size;
@@ -1962,6 +3206,28 @@ static int nvme_alloc_host_mem(struct nvme_dev *dev, u64 min, u64 preferred)
 	return -ENOMEM;
 }
 
+/*
+ * Most modern SSDs include onboard DRAM, typically in a ratio of 1GB RAM per
+ * 1TB of NAND flash memory. This RAM is usually dedicated to tracking where
+ * each logical block address is physically stored on the NAND
+ * flash—information that changes with every write operation due to the wear
+ * leveling that flash memory requires. This information must also be consulted
+ * in order to complete any read operation. The standard DRAM to NAND ratio
+ * provides enough RAM for the SSD controller to use a simple and fast lookup
+ * table instead of more complicated data structures. This greatly reduces the
+ * work the SSD controller needs to do to handle IO operations, and is key to
+ * offering consistent performance.
+ *
+ * SSDs that omit this DRAM can be cheaper and smaller, but because they can
+ * only store their mapping tables in the flash memory instead of much faster
+ * DRAM, there's a substantial performance penalty. In the worst case, read
+ * latency is doubled as potentially every read request from the host first
+ * requires a NAND flash read to look up the logical to physical address
+ * mapping, then a second read to actually fetch the requested data.
+ *
+ * called by:
+ *   - drivers/nvme/host/pci.c|2781| <<nvme_reset_work>> result = nvme_setup_host_mem(dev);
+ */
 static int nvme_setup_host_mem(struct nvme_dev *dev)
 {
 	u64 max = (u64)max_host_mem_size_mb * SZ_1M;
@@ -2011,6 +3277,10 @@ static int nvme_setup_host_mem(struct nvme_dev *dev)
  * nirqs is the number of interrupts available for write and read
  * queues. The core already reserved an interrupt for the admin queue.
  */
+/*
+ * 在以下使用nvme_calc_irq_sets():
+ *   - drivers/nvme/host/pci.c|2592| <<nvme_setup_irqs>> .calc_sets = nvme_calc_irq_sets,
+ */
 static void nvme_calc_irq_sets(struct irq_affinity *affd, unsigned int nrirqs)
 {
 	struct nvme_dev *dev = affd->priv;
@@ -2045,8 +3315,29 @@ static void nvme_calc_irq_sets(struct irq_affinity *affd, unsigned int nrirqs)
 	affd->nr_sets = nr_read_queues ? 2 : 1;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2146| <<nvme_setup_io_queues>> result = nvme_setup_irqs(dev, nr_io_queues);
+ *
+ * 会调用pci_alloc_irq_vectors_affinity()
+ * 刚进入的时候nr_io_queues是4, 到最后irq_queues是5
+ */
 static int nvme_setup_irqs(struct nvme_dev *dev, unsigned int nr_io_queues)
 {
+	/*
+	 * struct irq_affinity - Description for automatic irq affinity assignements
+	 * @pre_vectors:        Don't apply affinity to @pre_vectors at beginning of
+	 *                      the MSI(-X) vector space
+	 * @post_vectors:       Don't apply affinity to @post_vectors at end of
+	 *                      the MSI(-X) vector space
+	 * @nr_sets:            The number of interrupt sets for which affinity
+	 *                      spreading is required
+	 * @set_size:           Array holding the size of each interrupt set
+	 * @calc_sets:          Callback for calculating the number and size
+	 *                      of interrupt sets
+	 * @priv:               Private data for usage by @calc_sets, usually a
+	 *                      pointer to driver/device specific data.
+	 */
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 	struct irq_affinity affd = {
 		.pre_vectors	= 1,
@@ -2079,16 +3370,33 @@ static int nvme_setup_irqs(struct nvme_dev *dev, unsigned int nr_io_queues)
 	if (dev->ctrl.quirks & NVME_QUIRK_SINGLE_VECTOR)
 		irq_queues = 1;
 
+	/* 刚进入的时候nr_io_queues是4, 到最后irq_queues是5 */
 	return pci_alloc_irq_vectors_affinity(pdev, 1, irq_queues,
 			      PCI_IRQ_ALL_TYPES | PCI_IRQ_AFFINITY, &affd);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2747| <<nvme_setup_io_queues>> nvme_disable_io_queues(dev);
+ *   - drivers/nvme/host/pci.c|3102| <<nvme_dev_disable>> nvme_disable_io_queues(dev);
+ *
+ * 会调用nvme_poll_irqdisable()
+ */
 static void nvme_disable_io_queues(struct nvme_dev *dev)
 {
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|2585| <<nvme_disable_io_queues>> if (__nvme_disable_io_queues(dev, nvme_admin_delete_sq))
+	 *   - drivers/nvme/host/pci.c|2586| <<nvme_disable_io_queues>> __nvme_disable_io_queues(dev, nvme_admin_delete_cq);
+	 */
 	if (__nvme_disable_io_queues(dev, nvme_admin_delete_sq))
 		__nvme_disable_io_queues(dev, nvme_admin_delete_cq);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3131| <<nvme_reset_work>> result = nvme_setup_io_queues(dev);
+ */
 static int nvme_setup_io_queues(struct nvme_dev *dev)
 {
 	struct nvme_queue *adminq = &dev->queues[0];
@@ -2105,18 +3413,37 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 	if (dev->ctrl.quirks & NVME_QUIRK_SHARED_TAGS)
 		nr_io_queues = 1;
 
+	/*
+	 * 通过admin queue设置
+	 */
 	result = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);
 	if (result < 0)
 		return result;
 
 	if (nr_io_queues == 0)
 		return 0;
-	
+
+	/*
+	 * 在以下使用NVMEQ_ENABLED:
+	 *   - drivers/nvme/host/pci.c|1001| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+	 *   - drivers/nvme/host/pci.c|1560| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1805| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|2468| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *   - drivers/nvme/host/pci.c|2516| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+	 */
 	clear_bit(NVMEQ_ENABLED, &adminq->flags);
 
 	if (dev->cmb_use_sqes) {
 		result = nvme_cmb_qdepth(dev, nr_io_queues,
 				sizeof(struct nvme_command));
+		/*
+		 * 在以下使用cmb_use_sqes:
+		 *   - drivers/nvme/host/pci.c|1570| <<nvme_alloc_sq_cmds>> if (qid && dev->cmb_use_sqes && (dev->cmbsz & NVME_CMBSZ_SQS)) {
+		 *   - drivers/nvme/host/pci.c|1977| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+		 *   - drivers/nvme/host/pci.c|2306| <<nvme_setup_io_queues>> if (dev->cmb_use_sqes) {
+		 *   - drivers/nvme/host/pci.c|2312| <<nvme_setup_io_queues>> dev->cmb_use_sqes = false;
+		 */
 		if (result > 0)
 			dev->q_depth = result;
 		else
@@ -2141,14 +3468,25 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 	 * If we enable msix early due to not intx, disable it again before
 	 * setting up the full range we need.
 	 */
+	/*
+	 * 把这个pci设备上的
+	 */
 	pci_free_irq_vectors(pdev);
 
+	/*
+	 * 会调用pci_alloc_irq_vectors_affinity()
+	 * 假设io queue的数量是4, 调用pci_alloc_irq_vectors_affinity()的时候申请5个
+	 * 加上admin的
+	 *
+	 * 返回分配的vector的数量
+	 */
 	result = nvme_setup_irqs(dev, nr_io_queues);
 	if (result <= 0)
 		return -EIO;
 
 	dev->num_vecs = result;
 	result = max(result - 1, 1);
+	/* poll queue是不需要申请vector的 */
 	dev->max_qid = result + dev->io_queues[HCTX_TYPE_POLL];
 
 	/*
@@ -2157,11 +3495,20 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 	 * path to scale better, even if the receive path is limited by the
 	 * number of interrupts.
 	 */
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|1555| <<nvme_create_queue>> result = queue_request_irq(nvmeq);
+	 *   - drivers/nvme/host/pci.c|1703| <<nvme_pci_configure_admin_queue>> result = queue_request_irq(nvmeq);
+	 *   - drivers/nvme/host/pci.c|2160| <<nvme_setup_io_queues>> result = queue_request_irq(adminq);
+	 */
 	result = queue_request_irq(adminq);
 	if (result)
 		return result;
 	set_bit(NVMEQ_ENABLED, &adminq->flags);
 
+	/*
+	 * 只在这里调用
+	 */
 	result = nvme_create_io_queues(dev);
 	if (result || dev->online_queues < 2)
 		return result;
@@ -2179,6 +3526,11 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3105| <<nvme_del_cq_end>> nvme_del_queue_end(req, error);
+ *   - drivers/nvme/host/pci.c|3132| <<nvme_delete_queue>> nvme_del_cq_end : nvme_del_queue_end);
+ */
 static void nvme_del_queue_end(struct request *req, blk_status_t error)
 {
 	struct nvme_queue *nvmeq = req->end_io_data;
@@ -2197,6 +3549,15 @@ static void nvme_del_cq_end(struct request *req, blk_status_t error)
 	nvme_del_queue_end(req, error);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2765| <<__nvme_disable_io_queues>> if (nvme_delete_queue(&dev->queues[nr_queues], opcode))
+ *
+ * nvme_disable_io_queues()
+ *  -> __nvme_disable_io_queues()
+ *      -> nvme_delete_queue()
+ * 所以opcode只有nvme_admin_delete_cq
+ */
 static int nvme_delete_queue(struct nvme_queue *nvmeq, u8 opcode)
 {
 	struct request_queue *q = nvmeq->dev->ctrl.admin_q;
@@ -2207,6 +3568,11 @@ static int nvme_delete_queue(struct nvme_queue *nvmeq, u8 opcode)
 	cmd.delete_queue.opcode = opcode;
 	cmd.delete_queue.qid = cpu_to_le16(nvmeq->qid);
 
+	/*
+	 * blk_mq_init_queue()用来在一个已初始化的tagset上分配一个request_queue.
+	 * 因为admin_queue只有一个request_queue, 所以在初始化的时候调用blk_mq_init_queue().
+	 * 而io_queue可能被多个namespace共享, 所以在分配namespace的时候调用blk_mq_init_queue().
+	 */
 	req = nvme_alloc_request(q, &cmd, BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
 	if (IS_ERR(req))
 		return PTR_ERR(req);
@@ -2214,6 +3580,12 @@ static int nvme_delete_queue(struct nvme_queue *nvmeq, u8 opcode)
 	req->timeout = ADMIN_TIMEOUT;
 	req->end_io_data = nvmeq;
 
+	/*
+	 * 在以下使用delete_done:
+	 *   - drivers/nvme/host/pci.c|3095| <<nvme_del_queue_end>> complete(&nvmeq->delete_done);
+	 *   - drivers/nvme/host/pci.c|3129| <<nvme_delete_queue>> init_completion(&nvmeq->delete_done);
+	 *   - drivers/nvme/host/pci.c|3157| <<__nvme_disable_io_queues>> timeout = wait_for_completion_io_timeout(&nvmeq->delete_done,
+	 */
 	init_completion(&nvmeq->delete_done);
 	blk_execute_rq_nowait(q, NULL, req, false,
 			opcode == nvme_admin_delete_cq ?
@@ -2221,6 +3593,11 @@ static int nvme_delete_queue(struct nvme_queue *nvmeq, u8 opcode)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2585| <<nvme_disable_io_queues>> if (__nvme_disable_io_queues(dev, nvme_admin_delete_sq))
+ *   - drivers/nvme/host/pci.c|2586| <<nvme_disable_io_queues>> __nvme_disable_io_queues(dev, nvme_admin_delete_cq);
+ */
 static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode)
 {
 	int nr_queues = dev->online_queues - 1, sent = 0;
@@ -2237,6 +3614,20 @@ static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode)
 	while (sent) {
 		struct nvme_queue *nvmeq = &dev->queues[nr_queues + sent];
 
+		/*
+		 * 在以下使用delete_done:
+		 *   - drivers/nvme/host/pci.c|3095| <<nvme_del_queue_end>> complete(&nvmeq->delete_done);
+		 *   - drivers/nvme/host/pci.c|3129| <<nvme_delete_queue>> init_completion(&nvmeq->delete_done);
+		 *   - drivers/nvme/host/pci.c|3157| <<__nvme_disable_io_queues>> timeout = wait_for_completion_io_timeout(&nvmeq->delete_done,
+		 *
+		 * This waits for either a completion of a specific task to be signaled or for a
+		 * specified timeout to expire. The timeout is in jiffies. It is not
+		 * interruptible. The caller is accounted as waiting for IO (which traditionally
+		 * means blkio only).
+		 *
+		 * Return: 0 if timed out, and positive (at least 1, or number of jiffies left
+		 * till timeout) if completed.
+		 */
 		timeout = wait_for_completion_io_timeout(&nvmeq->delete_done,
 				timeout);
 		if (timeout == 0)
@@ -2254,6 +3645,10 @@ static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3302| <<nvme_reset_work>> nvme_dev_add(dev);
+ */
 static void nvme_dev_add(struct nvme_dev *dev)
 {
 	int ret;
@@ -2297,19 +3692,35 @@ static void nvme_dev_add(struct nvme_dev *dev)
 	nvme_dbbuf_set(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2720| <<nvme_reset_work>> result = nvme_pci_enable(dev);
+ */
 static int nvme_pci_enable(struct nvme_dev *dev)
 {
 	int result = -ENOMEM;
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 
+	/*
+	 * Initialize device before it's used by a driver. Ask low-level code
+	 * to enable Memory resources. Wake up the device if it was suspended.
+	 * Beware, this function can fail.
+	 */
 	if (pci_enable_device_mem(pdev))
 		return result;
 
+	/*
+	 * Enables bus-mastering on the device and calls pcibios_set_master()
+	 * to do the needed arch specific settings.
+	 */
 	pci_set_master(pdev);
 
 	if (dma_set_mask_and_coherent(dev->dev, DMA_BIT_MASK(64)))
 		goto disable;
 
+	/*
+	 * Controller Status
+	 */
 	if (readl(dev->bar + NVME_REG_CSTS) == -1) {
 		result = -ENODEV;
 		goto disable;
@@ -2326,10 +3737,37 @@ static int nvme_pci_enable(struct nvme_dev *dev)
 
 	dev->ctrl.cap = lo_hi_readq(dev->bar + NVME_REG_CAP);
 
+	/*
+	 * bit 0 - 15, Read only
+	 * Maximum Queue Entries Supported (MQES): This field indicates the
+	 * maximum individual queue size that the controller supports. For NVMe over PCIe
+	 * implementations, this value applies to the I/O Submission Queues and I/O
+	 * Completion Queues that the host creates. For NVMe over Fabrics
+	 * implementations, this value applies to only the I/O Submission Queues that the
+	 * host creates. This is a 0’s based value. The minimum value is 1h, indicating two
+	 * entries.
+	 */
 	dev->q_depth = min_t(int, NVME_CAP_MQES(dev->ctrl.cap) + 1,
 				io_queue_depth);
+	/* 从0开始的queue depth */
 	dev->ctrl.sqsize = dev->q_depth - 1; /* 0's based queue depth */
+	/*
+	 * Doorbell Stride (DSTRD): Each Submission Queue and Completion Queue
+	 * Doorbell register is 32-bits in size. This register indicates the stride between
+	 * doorbell registers. The stride is specified as (2 ^ (2 + DSTRD)) in bytes. A value
+	 * of 0h indicates a stride of 4 bytes, where the doorbell registers are packed without
+	 * reserved space between each register. Refer to section 8.6.
+	 */
 	dev->db_stride = 1 << NVME_CAP_STRIDE(dev->ctrl.cap);
+	/*
+	 * 从bar的0x1000 = 4096开始:
+	 * Submission Queue 0 Tail Doorbell (Admin)
+	 * Completion Queue 0 Head Doorbell (Admin)
+	 * Submission Queue 1 Tail Doorbell
+	 * Completion Queue 1 Head Doorbell
+	 * ... ...
+	 * 根据dev->db_stride中间有间隔的
+	 */
 	dev->dbs = dev->bar + 4096;
 
 	/*
@@ -2337,6 +3775,12 @@ static int nvme_pci_enable(struct nvme_dev *dev)
 	 * Interestingly they also seem to ignore the CC:IOSQES register
 	 * so we don't bother updating it here.
 	 */
+	/*
+	 * 在以下使用io_sqes:
+	 *   - drivers/nvme/host/pci.c|1592| <<nvme_alloc_queue>> nvmeq->sqes = qid ? dev->io_sqes : NVME_ADM_SQES;
+	 *   - drivers/nvme/host/pci.c|2559| <<nvme_pci_enable>> dev->io_sqes = 7;
+	 *   - drivers/nvme/host/pci.c|2561| <<nvme_pci_enable>> dev->io_sqes = NVME_NVM_IOSQES;
+	 */
 	if (dev->ctrl.quirks & NVME_QUIRK_128_BYTES_SQES)
 		dev->io_sqes = 7;
 	else
@@ -2363,6 +3807,13 @@ static int nvme_pci_enable(struct nvme_dev *dev)
 	 * Controllers with the shared tags quirk need the IO queue to be
 	 * big enough so that we get 32 tags for the admin queue
 	 */
+	/*
+	 * 这个情况设置:
+	 * 3390         { PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2005),
+	 * 3391                 .driver_data = NVME_QUIRK_SINGLE_VECTOR |
+	 * 3392                                 NVME_QUIRK_128_BYTES_SQES |
+	 * 3393                                 NVME_QUIRK_SHARED_TAGS },
+	 */
 	if ((dev->ctrl.quirks & NVME_QUIRK_SHARED_TAGS) &&
 	    (dev->q_depth < (NVME_AQ_DEPTH + 2))) {
 		dev->q_depth = NVME_AQ_DEPTH + 2;
@@ -2371,17 +3822,37 @@ static int nvme_pci_enable(struct nvme_dev *dev)
 	}
 
 
+	/*
+	 * nvme_reset_work()
+	 *  -> nvme_pci_enable()
+	 *      -> nvme_map_cmd()
+	 */
 	nvme_map_cmb(dev);
 
 	pci_enable_pcie_error_reporting(pdev);
+	/* save the PCI configuration space of a device before suspending */
 	pci_save_state(pdev);
 	return 0;
 
  disable:
+	/*
+	 * Signal to the system that the PCI device is not in use by the system
+	 * anymore.  This only involves disabling PCI bus-mastering, if active.
+	 *
+	 * Note we don't actually disable the device until all callers of
+	 * pci_enable_device() have called pci_disable_device().
+	 */
 	pci_disable_device(pdev);
 	return result;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|4321| <<nvme_probe>> nvme_dev_unmap(dev);
+ *   - drivers/nvme/host/pci.c|4408| <<nvme_remove>> nvme_dev_unmap(dev);
+ *
+ * unmap pci bar的内存吧
+ */
 static void nvme_dev_unmap(struct nvme_dev *dev)
 {
 	if (dev->bar)
@@ -2389,6 +3860,10 @@ static void nvme_dev_unmap(struct nvme_dev *dev)
 	pci_release_mem_regions(to_pci_dev(dev->dev));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3806| <<nvme_dev_disable>> nvme_pci_disable(dev);
+ */
 static void nvme_pci_disable(struct nvme_dev *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -2397,15 +3872,46 @@ static void nvme_pci_disable(struct nvme_dev *dev)
 
 	if (pci_is_enabled(pdev)) {
 		pci_disable_pcie_error_reporting(pdev);
+		/*
+		 * pci_disable_device - Disable PCI device after use
+		 * @dev: PCI device to be disabled
+		 *
+		 * Signal to the system that the PCI device is not in use by the system
+		 * anymore.  This only involves disabling PCI bus-mastering, if active.
+		 *
+		 * Note we don't actually disable the device until all callers of
+		 * pci_enable_device() have called pci_disable_device().
+		 */
 		pci_disable_device(pdev);
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1372| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|1401| <<nvme_timeout>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|1419| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2622| <<nvme_disable_prepare_reset>> nvme_dev_disable(dev, shutdown);
+ *   - drivers/nvme/host/pci.c|2679| <<nvme_remove_dead_ctrl>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2716| <<nvme_reset_work>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|3064| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|3071| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|3209| <<nvme_error_detected>> nvme_dev_disable(dev, false);
+ */
 static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 {
 	bool dead = true, freeze = false;
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 
+	/*
+	 * 在以下使用shutdown_lock:
+	 *   - drivers/nvme/host/pci.c|2570| <<nvme_dev_disable>> mutex_lock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2615| <<nvme_dev_disable>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2719| <<nvme_reset_work>> mutex_lock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2745| <<nvme_reset_work>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2821| <<nvme_reset_work>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2965| <<nvme_probe>> mutex_init(&dev->shutdown_lock);
+	 */
 	mutex_lock(&dev->shutdown_lock);
 	if (pci_is_enabled(pdev)) {
 		u32 csts = readl(dev->bar + NVME_REG_CSTS);
@@ -2413,6 +3919,12 @@ static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 		if (dev->ctrl.state == NVME_CTRL_LIVE ||
 		    dev->ctrl.state == NVME_CTRL_RESETTING) {
 			freeze = true;
+			/*
+			 * 为每一个namespace的request_queue调用blk_freeze_queue_start()
+			 * 核心是ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+			 *
+			 * 这样blk_queue_enter()这样的函数就没法前进了
+			 */
 			nvme_start_freeze(&dev->ctrl);
 		}
 		dead = !!((csts & NVME_CSTS_CFS) || !(csts & NVME_CSTS_RDY) ||
@@ -2423,21 +3935,84 @@ static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 	 * Give the controller a chance to complete all entered requests if
 	 * doing a safe shutdown.
 	 */
+	/*
+	 * 在qemu上"echo 1 > /sys/block/nvme0n1/device/reset_controller"的结果:
+	 * dead=0, shutdown=0, freeze=1
+	 *
+	 * 为每一个namespace的request_queue调用blk_mq_freeze_queue_wait_timeout()
+	 */
 	if (!dead && shutdown && freeze)
 		nvme_wait_freeze_timeout(&dev->ctrl, NVME_IO_TIMEOUT);
 
+	/*
+	 * blk_mq_quiesce_queue() - wait until all ongoing dispatches have finished
+	 * @q: request queue.
+	 *
+	 * Note: this function does not prevent that the struct request end_io()
+	 * callback function is invoked. Once this function is returned, we make
+	 * sure no dispatch can happen until the queue is unquiesced via
+	 * blk_mq_unquiesce_queue().
+	 *
+	 * 在preempt下的spinlock, 或者非preempt下的不调用都会hang在rcu
+	 * [0] __wait_rcu_gp
+	 * [0] synchronize_rcu
+	 * [0] blk_mq_quiesce_queue
+	 * [0] nvme_stop_queues
+	 * [0] nvme_dev_disable
+	 * [0] nvme_reset_work
+	 * [0] process_one_work
+	 * [0] worker_thread
+	 * [0] kthread
+	 * [0] ret_from_fork
+	 *
+	 * 为每一个namespace的request_queue调用blk_mq_quiesce_queue()
+	 *
+	 * 这里只是保证不dispatch, 不保证complete和硬件的处理
+	 *
+	 * 其实能保证某一些complete, 因为里面会等待rcu:
+	 * 1. 在non-preempt下发生了调度 (退出内核态或者sched)
+	 * 2. 在preempt下有spinlock包会了complete
+	 */
 	nvme_stop_queues(&dev->ctrl);
 
 	if (!dead && dev->ctrl.queue_count > 0) {
+		/*
+		 * nvme_disable_io_queues()会调用nvme_poll_irqdisable()
+		 */
 		nvme_disable_io_queues(dev);
 		nvme_disable_admin_queue(dev, shutdown);
 	}
+	/*
+	 * nvme_suspend_io_queues()
+	 *  -> nvme_suspend_queue()
+	 */ 
 	nvme_suspend_io_queues(dev);
 	nvme_suspend_queue(&dev->queues[0]);
 	nvme_pci_disable(dev);
 
+	/*
+	 * !!!!!!!!!
+	 * 关于bt_tags_iter():
+	 * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+	 * 才调用iter_data->fn()!!!
+	 * 是针对started request的
+	 * !!!!!!!!!
+	 */
 	blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
 	blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+	/*
+	 * blk_mq_tagset_wait_completed_request - wait until all completed req's
+	 * complete funtion is run
+	 *
+	 * Note: This function has to be run after all IO queues are shutdown
+	 *
+	 * !!!!!!!!!
+	 * 关于bt_tags_iter():
+	 * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+	 * 才调用iter_data->fn()!!!
+	 * 是针对started request的
+	 * !!!!!!!!!
+	 */
 	blk_mq_tagset_wait_completed_request(&dev->tagset);
 	blk_mq_tagset_wait_completed_request(&dev->admin_tagset);
 
@@ -2447,13 +4022,51 @@ static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 	 * deadlocking blk-mq hot-cpu notifier.
 	 */
 	if (shutdown) {
+		/*
+		 * 在以下调用blk_queue_quiesced()检查request_queue是否quiesced:
+		 *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+		 *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+		 *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+		 *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+		 *
+		 * 核心思想是为所有的ns调用blk_mq_unquiesce_queue()
+		 */
 		nvme_start_queues(&dev->ctrl);
+		/*
+		 * commit c8e9e9b7646ebe1c5066ddc420d7630876277eb4
+		 * Author: Keith Busch <keith.busch@intel.com>
+		 * Date:   Tue Apr 30 09:33:41 2019 -0600
+		 *
+		 * nvme-pci: unquiesce admin queue on shutdown
+		 *
+		 * Just like IO queues, the admin queue also will not be restarted after a
+		 * controller shutdown. Unquiesce this queue so that we do not block
+		 * request dispatch on a permanently disabled controller.
+		 *
+		 * Reported-by: Yufen Yu <yuyufen@huawei.com>
+		 * Signed-off-by: Keith Busch <keith.busch@intel.com>
+		 * Signed-off-by: Christoph Hellwig <hch@lst.de>
+		 *
+		 * 在以下调用blk_queue_quiesced():
+		 *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+		 *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+		 *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+		 *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+		 */
 		if (dev->ctrl.admin_q && !blk_queue_dying(dev->ctrl.admin_q))
 			blk_mq_unquiesce_queue(dev->ctrl.admin_q);
 	}
 	mutex_unlock(&dev->shutdown_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|4342| <<nvme_reset_prepare>> nvme_disable_prepare_reset(dev, false);
+ *   - drivers/nvme/host/pci.c|4363| <<nvme_shutdown>> nvme_disable_prepare_reset(dev, true);
+ *   - drivers/nvme/host/pci.c|4465| <<nvme_suspend>> return nvme_disable_prepare_reset(ndev, true);
+ *   - drivers/nvme/host/pci.c|4497| <<nvme_suspend>> ret = nvme_disable_prepare_reset(ndev, true);
+ *   - drivers/nvme/host/pci.c|4512| <<nvme_simple_suspend>> return nvme_disable_prepare_reset(ndev, true);
+ */
 static int nvme_disable_prepare_reset(struct nvme_dev *dev, bool shutdown)
 {
 	if (!nvme_wait_reset(&dev->ctrl))
@@ -2462,6 +4075,10 @@ static int nvme_disable_prepare_reset(struct nvme_dev *dev, bool shutdown)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|4559| <<nvme_probe>> result = nvme_setup_prp_pools(dev);
+ */
 static int nvme_setup_prp_pools(struct nvme_dev *dev)
 {
 	dev->prp_page_pool = dma_pool_create("prp list page", dev->dev,
@@ -2485,6 +4102,11 @@ static void nvme_release_prp_pools(struct nvme_dev *dev)
 	dma_pool_destroy(dev->prp_small_pool);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3174| <<nvme_pci_free_ctrl>> nvme_free_tagset(dev);
+ *   - drivers/nvme/host/pci.c|3352| <<nvme_reset_work>> nvme_free_tagset(dev);
+ */
 static void nvme_free_tagset(struct nvme_dev *dev)
 {
 	if (dev->tagset.tags)
@@ -2492,6 +4114,9 @@ static void nvme_free_tagset(struct nvme_dev *dev)
 	dev->ctrl.tagset = NULL;
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.free_ctrl = nvme_pci_free_ctrl()
+ */
 static void nvme_pci_free_ctrl(struct nvme_ctrl *ctrl)
 {
 	struct nvme_dev *dev = to_nvme_dev(ctrl);
@@ -2507,6 +4132,10 @@ static void nvme_pci_free_ctrl(struct nvme_ctrl *ctrl)
 	kfree(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3364| <<nvme_reset_work>> nvme_remove_dead_ctrl(dev);
+ */
 static void nvme_remove_dead_ctrl(struct nvme_dev *dev)
 {
 	/*
@@ -2521,10 +4150,87 @@ static void nvme_remove_dead_ctrl(struct nvme_dev *dev)
 		nvme_put_ctrl(&dev->ctrl);
 }
 
+/*
+ * --------------------------------------------
+ * 关于namespace ...
+ *
+ * scan和分配namespace的一个例子:
+ * [0] nvme_alloc_ns
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ *
+ * 在qemu测试的时候, 先...
+ * [0] nvme_async_probe
+ * [0] async_run_entry_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 再...
+ * [0] nvme_queue_scan
+ * [0] nvme_start_ctrl
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 调用nvme_scan_work()的地方:
+ *   - drivers/nvme/host/core.c|120| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+ *   - drivers/nvme/host/core.c|3902| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+ *   - drivers/nvme/host/pci.c|3420| <<nvme_async_probe>> flush_work(&dev->ctrl.scan_work);
+ * --------------------------------------------
+ *
+ *
+ * reset的时候freeze, 如果已经enable了:
+ *
+ * [0] blk_freeze_queue_start
+ * [0] nvme_start_freeze
+ * [0] nvme_dev_disable
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] blk_mq_unfreeze_queue
+ * [0] nvme_unfreeze
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ *
+ * pci在以下调用:
+ *   - drivers/nvme/host/core.c|133| <<nvme_try_sched_reset>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+ *   - drivers/nvme/host/core.c|143| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+ *   - drivers/nvme/host/core.c|155| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/core.c|169| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/pci.c|2798| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|2902| <<nvme_reset_done>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|2929| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|3094| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+ *
+ * 在以下使用nvme_reset_work():
+ *   - drivers/nvme/host/pci.c|2830| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+ *
+ * 函数的第一级实现主要用到nvme_dev->shutdown_lock
+ */
 static void nvme_reset_work(struct work_struct *work)
 {
 	struct nvme_dev *dev =
 		container_of(work, struct nvme_dev, ctrl.reset_work);
+	/*
+	 * ctrl.ctrl_config
+	 * 最终会被写入bar0或bar1的NVME_REG_CC = 0x0014, Controller Configuration
+	 */
 	bool was_suspend = !!(dev->ctrl.ctrl_config & NVME_CC_SHN_NORMAL);
 	int result;
 
@@ -2537,19 +4243,64 @@ static void nvme_reset_work(struct work_struct *work)
 	 * If we're called to reset a live controller first shut it down before
 	 * moving on.
 	 */
+	/*
+	 * nvme_dev_disable()
+	 *  -> nvme_start_freeze()
+	 *      -> blk_freeze_queue_start()
+	 */
 	if (dev->ctrl.ctrl_config & NVME_CC_ENABLE)
 		nvme_dev_disable(dev, false);
+	/*
+	 * 核心思想是对每一个namespace的request_queue和admin的requests调用:
+	 * blk_sync_queue():
+	 *  -> del_timer_sync(&q->timeout);
+	 *  -> cancel_work_sync(&q->timeout_work);
+	 *
+	 * A block device may call blk_sync_queue to ensure that any
+	 *     such activity is cancelled, thus allowing it to release resources
+	 *     that the callbacks might use.
+	 */
 	nvme_sync_queues(&dev->ctrl);
 
+	/*
+	 * 在以下使用shutdown_lock:
+	 *   - drivers/nvme/host/pci.c|2570| <<nvme_dev_disable>> mutex_lock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2615| <<nvme_dev_disable>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2719| <<nvme_reset_work>> mutex_lock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2745| <<nvme_reset_work>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2821| <<nvme_reset_work>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2965| <<nvme_probe>> mutex_init(&dev->shutdown_lock);
+	 */
 	mutex_lock(&dev->shutdown_lock);
 	result = nvme_pci_enable(dev);
 	if (result)
 		goto out_unlock;
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|3035| <<nvme_reset_work>> result = nvme_pci_configure_admin_queue(dev);
+	 *
+	 * 初始化(很复杂)nvme_dev->queues[0]
+	 * struct nvme_dev:
+	 *   - struct nvme_queue *queues;
+	 */
 	result = nvme_pci_configure_admin_queue(dev);
 	if (result)
 		goto out_unlock;
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|3073| <<nvme_reset_work>> result = nvme_alloc_admin_tags(dev);
+	 *
+	 * 初始化nvme_dev->admin_tagset.
+	 * struct nvme_dev:
+	 *   - struct nvme_queue *queues;
+	 *   - struct blk_mq_tag_set tagset;
+	 *   - struct blk_mq_tag_set admin_tagset;
+	 *
+	 * 这是初始化admin专用的tagset, 就一个queue (nr_hw_queues = 1)
+	 * 会调用blk_mq_alloc_tag_set()和blk_mq_init_queue()
+	 */
 	result = nvme_alloc_admin_tags(dev);
 	if (result)
 		goto out_unlock;
@@ -2580,6 +4331,15 @@ static void nvme_reset_work(struct work_struct *work)
 		goto out;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/core.c|1408| <<nvme_passthru_end>> nvme_init_identify(ctrl);
+	 *   - drivers/nvme/host/fc.c|2681| <<nvme_fc_create_association>> ret = nvme_init_identify(&ctrl->ctrl);
+	 *   - drivers/nvme/host/pci.c|2628| <<nvme_reset_work>> result = nvme_init_identify(&dev->ctrl);
+	 *   - drivers/nvme/host/rdma.c|835| <<nvme_rdma_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+	 *   - drivers/nvme/host/tcp.c|1752| <<nvme_tcp_configure_admin_queue>> error = nvme_init_identify(ctrl);
+	 *   - drivers/nvme/target/loop.c|389| <<nvme_loop_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+	 */
 	result = nvme_init_identify(&dev->ctrl);
 	if (result)
 		goto out;
@@ -2602,12 +4362,23 @@ static void nvme_reset_work(struct work_struct *work)
 				 "unable to allocate dma for dbbuf\n");
 	}
 
+	/*
+	 * Host Memory Buffer Preferred Size (HMPRE): This field indicates the preferred size
+	 * that the host is requested to allocate for the Host Memory Buffer feature in 4 KiB units.
+	 * This value shall be greater than or equal to the Host Memory Buffer Minimum Size. If
+	 * this field is non-zero, then the Host Memory Buffer feature is supported. If this field is
+	 * cleared to 0h, then the Host Memory Buffer feature is not supported.
+	 */
 	if (dev->ctrl.hmpre) {
 		result = nvme_setup_host_mem(dev);
 		if (result < 0)
 			goto out;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|3131| <<nvme_reset_work>> result = nvme_setup_io_queues(dev);
+	 */
 	result = nvme_setup_io_queues(dev);
 	if (result)
 		goto out;
@@ -2616,15 +4387,43 @@ static void nvme_reset_work(struct work_struct *work)
 	 * Keep the controller around but remove all namespaces if we don't have
 	 * any working I/O queue.
 	 */
+	/*
+	 * 修改nvme_dev->online_queues的地方:
+	 *   - drivers/nvme/host/pci.c|2167| <<nvme_suspend_queue>> nvmeq->dev->online_queues--;
+	 *   - drivers/nvme/host/pci.c|2428| <<nvme_init_queue>> dev->online_queues++;
+	 *   - drivers/nvme/host/pci.c|2498| <<nvme_create_queue>> dev->online_queues--;
+	 *   - drivers/nvme/host/pci.c|2744| <<nvme_pci_configure_admin_queue>> dev->online_queues--;
+	 */
 	if (dev->online_queues < 2) {
 		dev_warn(dev->ctrl.device, "IO queues not created\n");
+		/*
+		 * called by:
+		 *   - drivers/nvme/host/core.c|4028| <<nvme_remove_namespaces>> nvme_kill_queues(ctrl);
+		 *   - drivers/nvme/host/pci.c|3133| <<nvme_remove_dead_ctrl>> nvme_kill_queues(&dev->ctrl);
+		 *   - drivers/nvme/host/pci.c|3287| <<nvme_reset_work>> nvme_kill_queues(&dev->ctrl);
+		 */
 		nvme_kill_queues(&dev->ctrl);
 		nvme_remove_namespaces(&dev->ctrl);
 		nvme_free_tagset(dev);
 	} else {
+		/*
+		 * 在以下调用blk_queue_quiesced()检查request_queue是否quiesced:
+		 *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+		 *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+		 *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+		 *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+		 *
+		 * 核心思想是为所有的ns调用blk_mq_unquiesce_queue()
+		 */
 		nvme_start_queues(&dev->ctrl);
 		nvme_wait_freeze(&dev->ctrl);
+		/*
+		 * tagset在这里面初始化
+		 */
 		nvme_dev_add(dev);
+		/*
+		 * 第一次进来没有namespace, 所以不调用blk_mq_unfreeze_queue()
+		 */
 		nvme_unfreeze(&dev->ctrl);
 	}
 
@@ -2639,6 +4438,9 @@ static void nvme_reset_work(struct work_struct *work)
 		goto out;
 	}
 
+	/*
+	 * 这个函数会触发ns的scan: queue_work(nvme_wq, &ctrl->scan_work);
+	 */
 	nvme_start_ctrl(&dev->ctrl);
 	return;
 
@@ -2651,6 +4453,13 @@ static void nvme_reset_work(struct work_struct *work)
 	nvme_remove_dead_ctrl(dev);
 }
 
+/*
+ * 在以下使用nvme_remove_dead_ctrl_work():
+ *   - drivers/nvme/host/pci.c|3511| <<nvme_probe>> INIT_WORK(&dev->remove_work, nvme_remove_dead_ctrl_work);
+ *
+ * 在以下调用remove_work:
+ *   - drivers/nvme/host/pci.c|3181| <<nvme_remove_dead_ctrl>> if (!queue_work(nvme_wq, &dev->remove_work))
+ */
 static void nvme_remove_dead_ctrl_work(struct work_struct *work)
 {
 	struct nvme_dev *dev = container_of(work, struct nvme_dev, remove_work);
@@ -2661,24 +4470,36 @@ static void nvme_remove_dead_ctrl_work(struct work_struct *work)
 	nvme_put_ctrl(&dev->ctrl);
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.reg_read32 = nvme_pci_reg_read32()
+ */
 static int nvme_pci_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)
 {
 	*val = readl(to_nvme_dev(ctrl)->bar + off);
 	return 0;
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.reg_write32 = nvme_pci_reg_write32()
+ */
 static int nvme_pci_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val)
 {
 	writel(val, to_nvme_dev(ctrl)->bar + off);
 	return 0;
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.reg_read64 = nvme_pci_reg_read64()
+ */
 static int nvme_pci_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val)
 {
 	*val = lo_hi_readq(to_nvme_dev(ctrl)->bar + off);
 	return 0;
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.get_address()
+ */
 static int nvme_pci_get_address(struct nvme_ctrl *ctrl, char *buf, int size)
 {
 	struct pci_dev *pdev = to_pci_dev(to_nvme_dev(ctrl)->dev);
@@ -2686,6 +4507,10 @@ static int nvme_pci_get_address(struct nvme_ctrl *ctrl, char *buf, int size)
 	return snprintf(buf, size, "%s", dev_name(&pdev->dev));
 }
 
+/*
+ * 在以下使用nvme_pci_ctrl_ops:
+ *   - drivers/nvme/host/pci.c|3576| <<nvme_probe>> result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
+ */
 static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
 	.name			= "pcie",
 	.module			= THIS_MODULE,
@@ -2699,6 +4524,10 @@ static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
 	.get_address		= nvme_pci_get_address,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3502| <<nvme_probe>> result = nvme_dev_map(dev);
+ */
 static int nvme_dev_map(struct nvme_dev *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -2715,6 +4544,10 @@ static int nvme_dev_map(struct nvme_dev *dev)
 	return -ENODEV;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|4563| <<nvme_probe>> quirks |= check_vendor_combination_bug(pdev);
+ */
 static unsigned long check_vendor_combination_bug(struct pci_dev *pdev)
 {
 	if (pdev->vendor == 0x144d && pdev->device == 0xa802) {
@@ -2746,6 +4579,10 @@ static unsigned long check_vendor_combination_bug(struct pci_dev *pdev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3490| <<nvme_probe>> async_schedule(nvme_async_probe, dev);
+ */
 static void nvme_async_probe(void *data, async_cookie_t cookie)
 {
 	struct nvme_dev *dev = data;
@@ -2755,6 +4592,9 @@ static void nvme_async_probe(void *data, async_cookie_t cookie)
 	nvme_put_ctrl(&dev->ctrl);
 }
 
+/*
+ * struct pci_driver nvme_driver.probe = nvme_probe()
+ */
 static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 {
 	int node, result = -ENOMEM;
@@ -2766,10 +4606,15 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	if (node == NUMA_NO_NODE)
 		set_dev_node(&pdev->dev, first_memory_node);
 
+	/* 分配struct nvme_dev */
 	dev = kzalloc_node(sizeof(*dev), GFP_KERNEL, node);
 	if (!dev)
 		return -ENOMEM;
 
+	/*
+	 * stricy nvme_dev:
+	 *  -> struct nvme_queue *queues;
+	 */
 	dev->queues = kcalloc_node(max_queue_count(), sizeof(struct nvme_queue),
 					GFP_KERNEL, node);
 	if (!dev->queues)
@@ -2809,6 +4654,23 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 		goto release_pools;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/fc.c|3167| <<nvme_fc_init_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_fc_ctrl_ops, 0);
+	 *   - drivers/nvme/host/pci.c|3490| <<nvme_probe>> result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
+	 *   - drivers/nvme/host/rdma.c|2031| <<nvme_rdma_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
+	 *   - drivers/nvme/host/tcp.c|2340| <<nvme_tcp_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_tcp_ctrl_ops, 0);
+	 *   - drivers/nvme/target/loop.c|585| <<nvme_loop_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_loop_ctrl_ops,
+	 *
+	 * 每个nvme pci device (func) 是一个controller.
+	 * 可能很多个controller输入同一个subsystem. 但是pci的入口都是从probe controller开始的.
+	 * subsystem的数据结构的分配是初始化每一个controller的时候on demand的.
+	 * 如果两个controller属于同一个subsystem, 那初始化第二个controller的时候就不会再分配subsystem的结构了.
+	 * 每一个nvme0或者nvme1都是一个controller.
+	 *
+	 * struct nvme_dev:
+	 *   - struct nvme_ctrl ctrl;
+	 */
 	result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
 			quirks);
 	if (result)
@@ -2836,6 +4698,9 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	return result;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.reset_prepare = nvme_reset_prepare()
+ */
 static void nvme_reset_prepare(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2849,6 +4714,9 @@ static void nvme_reset_prepare(struct pci_dev *pdev)
 	nvme_sync_queues(&dev->ctrl);
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.reset_done = nvme_reset_done()
+ */
 static void nvme_reset_done(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2857,6 +4725,9 @@ static void nvme_reset_done(struct pci_dev *pdev)
 		flush_work(&dev->ctrl.reset_work);
 }
 
+/*
+ * struct pci_driver nvme_driver.shutdown = nvme_shutdown()
+ */
 static void nvme_shutdown(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2868,11 +4739,26 @@ static void nvme_shutdown(struct pci_dev *pdev)
  * state. This function must not have any dependencies on the device state in
  * order to proceed.
  */
+/*
+ * struct pci_driver nvme_driver.remove = nvme_remove()
+ *
+ * # echo 0000:00:04.0 >  /sys/bus/pci/devices/0000\:00\:04.0/driver/unbind
+ * [0] nvme_remove
+ * [0] pci_device_remove
+ * [0] device_release_driver_internal
+ * [0] unbind_store
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void nvme_remove(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
 
 	nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING);
+	/* 此时pci_get_drvdata()再也不能获得struct nvme_dev了 */
 	pci_set_drvdata(pdev, NULL);
 
 	if (!pci_device_is_present(pdev)) {
@@ -2881,16 +4767,39 @@ static void nvme_remove(struct pci_dev *pdev)
 		nvme_dev_remove_admin(dev);
 	}
 
+	/*
+	 * 等待nvme_reset_work()完成
+	 * 函数的第一级实现主要用到nvme_dev->shutdown_lock
+	 */
 	flush_work(&dev->ctrl.reset_work);
 	nvme_stop_ctrl(&dev->ctrl);
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/core.c|250| <<nvme_do_delete_ctrl>> nvme_remove_namespaces(ctrl);
+	 *   - drivers/nvme/host/pci.c|4156| <<nvme_reset_work>> nvme_remove_namespaces(&dev->ctrl);
+	 *   - drivers/nvme/host/pci.c|4490| <<nvme_remove>> nvme_remove_namespaces(&dev->ctrl);
+	 *
+	 * 这个函数执行完之后ctrl->namespaces就没元素了
+	 */
 	nvme_remove_namespaces(&dev->ctrl);
 	nvme_dev_disable(dev, true);
 	nvme_release_cmb(dev);
+	/* 是否dma memory */
 	nvme_free_host_mem(dev);
 	nvme_dev_remove_admin(dev);
+	/*
+	 * 核心思想是释放admin的sq和cq的dma ring buffer
+	 */
 	nvme_free_queues(dev, 0);
+	/*
+	 * 其中有一条是会删掉ctrl->device (char dev)
+	 */
 	nvme_uninit_ctrl(&dev->ctrl);
+	/*
+	 * 释放nvme_dev->prp_page_pool和nvme_dev->prp_small_pool
+	 */
 	nvme_release_prp_pools(dev);
+	/* unmap pci bar的内存吧 */
 	nvme_dev_unmap(dev);
 	nvme_put_ctrl(&dev->ctrl);
 }
@@ -2906,6 +4815,9 @@ static int nvme_set_power_state(struct nvme_ctrl *ctrl, u32 ps)
 	return nvme_set_features(ctrl, NVME_FEAT_POWER_MGMT, ps, NULL, 0, NULL);
 }
 
+/*
+ * struct dev_pm_ops nvme_dev_pm_ops.resume = nvme_resume()
+ */
 static int nvme_resume(struct device *dev)
 {
 	struct nvme_dev *ndev = pci_get_drvdata(to_pci_dev(dev));
@@ -2917,6 +4829,9 @@ static int nvme_resume(struct device *dev)
 	return 0;
 }
 
+/*
+ * struct dev_pm_ops nvme_dev_pm_ops.suspend = nvme_suspend()
+ */
 static int nvme_suspend(struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
@@ -2982,12 +4897,20 @@ static int nvme_suspend(struct device *dev)
 	return ret;
 }
 
+/*
+ * struct dev_pm_ops nvme_dev_pm_ops.freeze = nvme_simple_suspend()
+ * struct dev_pm_ops nvme_dev_pm_ops.poweroff = nvme_simple_suspend()
+ */
 static int nvme_simple_suspend(struct device *dev)
 {
 	struct nvme_dev *ndev = pci_get_drvdata(to_pci_dev(dev));
 	return nvme_disable_prepare_reset(ndev, true);
 }
 
+/*
+ * struct dev_pm_ops nvme_dev_pm_ops.thaw = nvme_simple_resume()
+ * struct dev_pm_ops nvme_dev_pm_ops.restore = nvme_simple_resume()
+ */
 static int nvme_simple_resume(struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
@@ -3006,6 +4929,9 @@ static const struct dev_pm_ops nvme_dev_pm_ops = {
 };
 #endif /* CONFIG_PM_SLEEP */
 
+/*
+ * struct pci_error_handlers nvme_err_handler.error_detected = nvme_error_detected()
+ */
 static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
 						pci_channel_state_t state)
 {
@@ -3032,6 +4958,9 @@ static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
 	return PCI_ERS_RESULT_NEED_RESET;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.slot_reset = nvme_slot_reset()
+ */
 static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -3042,6 +4971,9 @@ static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
 	return PCI_ERS_RESULT_RECOVERED;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.resume = nvme_error_resume()
+ */
 static void nvme_error_resume(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -3049,6 +4981,9 @@ static void nvme_error_resume(struct pci_dev *pdev)
 	flush_work(&dev->ctrl.reset_work);
 }
 
+/*
+ * struct pci_driver nvme_driver.err_handler = nvme_err_handler
+ */
 static const struct pci_error_handlers nvme_err_handler = {
 	.error_detected	= nvme_error_detected,
 	.slot_reset	= nvme_slot_reset,
@@ -3057,6 +4992,9 @@ static const struct pci_error_handlers nvme_err_handler = {
 	.reset_done	= nvme_reset_done,
 };
 
+/*
+ * struct pci_driver nvme_driver.id_table = nvme_id_table[]
+ */
 static const struct pci_device_id nvme_id_table[] = {
 	{ PCI_VDEVICE(INTEL, 0x0953),
 		.driver_data = NVME_QUIRK_STRIPE_SIZE |
diff --git a/drivers/nvme/host/tcp.c b/drivers/nvme/host/tcp.c
index 6d43b23a0fc8..6f2c6bffb48d 100644
--- a/drivers/nvme/host/tcp.c
+++ b/drivers/nvme/host/tcp.c
@@ -1786,6 +1786,12 @@ static void nvme_tcp_teardown_admin_queue(struct nvme_ctrl *ctrl,
 	nvme_tcp_destroy_admin_queue(ctrl, remove);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/tcp.c|1908| <<nvme_tcp_error_recovery_work>> nvme_tcp_teardown_io_queues(ctrl, false);
+ *   - drivers/nvme/host/tcp.c|1928| <<nvme_tcp_teardown_ctrl>> nvme_tcp_teardown_io_queues(ctrl, shutdown);
+ *   - drivers/nvme/host/tcp.c|2068| <<nvme_tcp_timeout>> nvme_tcp_teardown_io_queues(&ctrl->ctrl, false);
+ */
 static void nvme_tcp_teardown_io_queues(struct nvme_ctrl *ctrl,
 		bool remove)
 {
@@ -2405,6 +2411,20 @@ static void __exit nvme_tcp_cleanup_module(void)
 	nvmf_unregister_transport(&nvme_tcp_transport);
 
 	mutex_lock(&nvme_tcp_ctrl_mutex);
+	/*
+	 * 调用者没有pci:
+	 *   - drivers/nvme/host/fc.c|780| <<nvme_fc_ctrl_connectivity_loss>> nvme_delete_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/fc.c|847| <<nvme_fc_unregister_remoteport>> nvme_delete_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/fc.c|2909| <<nvme_fc_reconnect_or_delete>> WARN_ON(nvme_delete_ctrl(&ctrl->ctrl));
+	 *   - drivers/nvme/host/fc.c|3521| <<nvme_fc_delete_controllers>> nvme_delete_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/rdma.c|977| <<nvme_rdma_reconnect_or_remove>> nvme_delete_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/rdma.c|2101| <<nvme_rdma_remove_one>> nvme_delete_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/rdma.c|2141| <<nvme_rdma_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/tcp.c|1822| <<nvme_tcp_reconnect_or_remove>> nvme_delete_ctrl(ctrl);
+	 *   - drivers/nvme/host/tcp.c|2409| <<nvme_tcp_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/target/loop.c|438| <<nvme_loop_delete_ctrl>> nvme_delete_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/target/loop.c|713| <<nvme_loop_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+	 */
 	list_for_each_entry(ctrl, &nvme_tcp_ctrl_list, list)
 		nvme_delete_ctrl(&ctrl->ctrl);
 	mutex_unlock(&nvme_tcp_ctrl_mutex);
diff --git a/drivers/nvme/target/configfs.c b/drivers/nvme/target/configfs.c
index 98613a45bd3b..f984d543faaa 100644
--- a/drivers/nvme/target/configfs.c
+++ b/drivers/nvme/target/configfs.c
@@ -14,12 +14,50 @@
 
 #include "nvmet.h"
 
+/*
+ * nvmet_ns_make()
+ * nvmet_hosts_make_group()
+ * nvmet_subsys_make()
+ * nvmet_referral_make()
+ * nvmet_ana_groups_make_group()
+ * nvmet_ports_make()
+ */
+
+/*
+ * 在以下使用nvmet_host_type:
+ *   - rivers/nvme/target/configfs.c|1283| <<global>> static const struct config_item_type nvmet_host_type = {
+ *   - drivers/nvme/target/configfs.c|723| <<nvmet_allowed_hosts_allow_link>> if (target->ci_type != &nvmet_host_type) {
+ *   - drivers/nvme/target/configfs.c|1297| <<nvmet_hosts_make_group>> config_group_init_type_name(&host->group, name, &nvmet_host_type);
+ */
 static const struct config_item_type nvmet_host_type;
+/*
+ * 在以下使用nvmet_subsys_type:
+ *   - drivers/nvme/target/configfs.c|644| <<nvmet_port_subsys_allow_link>> if (target->ci_type != &nvmet_subsys_type) {
+ *   - drivers/nvme/target/configfs.c|925| <<nvmet_subsys_make>> config_group_init_type_name(&subsys->group, name, &nvmet_subsys_type);
+ */
 static const struct config_item_type nvmet_subsys_type;
 
+/*
+ * 在以下使用:
+ *   - drivers/nvme/target/configfs.c|21| <<global>> struct list_head *nvmet_ports = &nvmet_ports_list;
+ *   - drivers/nvme/target/configfs.c|1205| <<nvmet_ports_make>> list_add(&port->global_entry, &nvmet_ports_list);
+ */
 static LIST_HEAD(nvmet_ports_list);
+/*
+ * 在以下使用nvmet_ports:
+ *   - drivers/nvme/target/discovery.c|74| <<nvmet_subsys_disc_changed>> list_for_each_entry(port, nvmet_ports, global_entry)
+ */
 struct list_head *nvmet_ports = &nvmet_ports_list;
 
+/*
+ * 在以下使用nvmet_transport_names[]:
+ *   - drivers/nvme/target/configfs.c|257| <<nvmet_addr_trtype_show>> for (i = 0; i < ARRAY_SIZE(nvmet_transport_names); i++) {
+ *   - drivers/nvme/target/configfs.c|258| <<nvmet_addr_trtype_show>> if (port->disc_addr.trtype != nvmet_transport_names[i].type)
+ *   - drivers/nvme/target/configfs.c|260| <<nvmet_addr_trtype_show>> return sprintf(page, "%s\n", nvmet_transport_names[i].name);
+ *   - drivers/nvme/target/configfs.c|285| <<nvmet_addr_trtype_store>> for (i = 0; i < ARRAY_SIZE(nvmet_transport_names); i++) {
+ *   - drivers/nvme/target/configfs.c|286| <<nvmet_addr_trtype_store>> if (sysfs_streq(page, nvmet_transport_names[i].name))
+ *   - drivers/nvme/target/configfs.c|294| <<nvmet_addr_trtype_store>> port->disc_addr.trtype = nvmet_transport_names[i].type;
+ */
 static const struct nvmet_transport_name {
 	u8		type;
 	const char	*name;
@@ -694,6 +732,16 @@ static const struct config_item_type nvmet_port_subsys_type = {
 	.ct_owner		= THIS_MODULE,
 };
 
+/*
+ * [0] nvmet_allowed_hosts_allow_link
+ * [0] configfs_symlink
+ * [0] vfs_symlink
+ * [0] do_symlinkat
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * struct configfs_item_operations nvmet_allowed_hosts_item_ops.allow_link = nvmet_allowed_hosts_allow_link()
+ */
 static int nvmet_allowed_hosts_allow_link(struct config_item *parent,
 		struct config_item *target)
 {
@@ -702,6 +750,10 @@ static int nvmet_allowed_hosts_allow_link(struct config_item *parent,
 	struct nvmet_host_link *link, *p;
 	int ret;
 
+	/*
+	 * struct config_item *target:
+	 *  -> const struct config_item_type *ci_type;
+	 */
 	if (target->ci_type != &nvmet_host_type) {
 		pr_err("can only link hosts into the allowed_hosts directory!\n");
 		return -EINVAL;
@@ -890,11 +942,23 @@ static const struct config_item_type nvmet_subsys_type = {
 	.ct_owner		= THIS_MODULE,
 };
 
+/*
+ * struct configfs_group_operations nvmet_subsystems_group_ops.mek_group = nvmet_subsys_make()
+ */
 static struct config_group *nvmet_subsys_make(struct config_group *group,
 		const char *name)
 {
 	struct nvmet_subsys *subsys;
 
+	/*
+	 * 在以下使用NVME_DISC_SUBSYS_NAME:
+	 *   - drivers/nvme/host/core.c|3618| <<__nvme_find_get_subsystem>> if (!strcmp(subsysnqn, NVME_DISC_SUBSYS_NAME))
+	 *   - drivers/nvme/host/fabrics.c|684| <<nvmf_parse_options>> NVME_DISC_SUBSYS_NAME));
+	 *   - drivers/nvme/target/configfs.c|916| <<nvmet_subsys_make>> if (sysfs_streq(name, NVME_DISC_SUBSYS_NAME)) {
+	 *   - drivers/nvme/target/core.c|1398| <<nvmet_find_get_subsys>> if (!strcmp(NVME_DISC_SUBSYS_NAME, subsysnqn)) {
+	 *   - drivers/nvme/target/discovery.c|220| <<nvmet_execute_disc_get_log_page>> NVME_DISC_SUBSYS_NAME,
+	 *   - drivers/nvme/target/discovery.c|389| <<nvmet_init_discovery>> nvmet_subsys_alloc(NVME_DISC_SUBSYS_NAME, NVME_NQN_DISC);
+	 */
 	if (sysfs_streq(name, NVME_DISC_SUBSYS_NAME)) {
 		pr_err("can't create discovery subsystem through configfs\n");
 		return ERR_PTR(-EINVAL);
@@ -1174,6 +1238,9 @@ static const struct config_item_type nvmet_port_type = {
 	.ct_owner		= THIS_MODULE,
 };
 
+/*
+ * struct configfs_group_operations nvmet_ports_group_ops.make_group = nvmet_ports_make()
+ */
 static struct config_group *nvmet_ports_make(struct config_group *group,
 		const char *name)
 {
@@ -1202,6 +1269,14 @@ static struct config_group *nvmet_ports_make(struct config_group *group,
 			port->ana_state[i] = NVME_ANA_INACCESSIBLE;
 	}
 
+	/*
+	 * 在以下使用nvmet_ports:
+	 *   - drivers/nvme/target/discovery.c|74| <<nvmet_subsys_disc_changed>> list_for_each_entry(port, nvmet_ports, global_entry)
+	 *
+	 * 在以下使用nvmet_ports_list:
+	 *   - drivers/nvme/target/configfs.c|21| <<global>> struct list_head *nvmet_ports = &nvmet_ports_list;
+	 *   - drivers/nvme/target/configfs.c|1205| <<nvmet_ports_make>> list_add(&port->global_entry, &nvmet_ports_list);
+	 */
 	list_add(&port->global_entry, &nvmet_ports_list);
 
 	INIT_LIST_HEAD(&port->entry);
@@ -1302,6 +1377,10 @@ static struct configfs_subsystem nvmet_configfs_subsystem = {
 	},
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|1504| <<nvmet_init>> error = nvmet_init_configfs();
+ */
 int __init nvmet_init_configfs(void)
 {
 	int ret;
diff --git a/drivers/nvme/target/core.c b/drivers/nvme/target/core.c
index 28438b833c1b..e29943243037 100644
--- a/drivers/nvme/target/core.c
+++ b/drivers/nvme/target/core.c
@@ -15,7 +15,25 @@
 
 #include "nvmet.h"
 
+/*
+ * 在以下使用buffered_io_wq:
+ *   - drivers/nvme/target/core.c|1629| <<nvmet_init>> buffered_io_wq = alloc_workqueue("nvmet-buffered-io-wq",
+ *   - drivers/nvme/target/core.c|1631| <<nvmet_init>> if (!buffered_io_wq) {
+ *   - drivers/nvme/target/core.c|1648| <<nvmet_init>> destroy_workqueue(buffered_io_wq);
+ *   - drivers/nvme/target/core.c|1658| <<nvmet_exit>> destroy_workqueue(buffered_io_wq);
+ *   - drivers/nvme/target/io-cmd-file.c|20| <<nvmet_file_ns_disable>> flush_workqueue(buffered_io_wq);
+ *   - drivers/nvme/target/io-cmd-file.c|232| <<nvmet_file_submit_buffered_io>> queue_work(buffered_io_wq, &req->f.work);
+ */
 struct workqueue_struct *buffered_io_wq;
+/*
+ * 在以下使用nvmet_transports[]:
+ *   - drivers/nvme/target/core.c|268| <<nvmet_register_transport>> if (nvmet_transports[ops->type])
+ *   - drivers/nvme/target/core.c|271| <<nvmet_register_transport>> nvmet_transports[ops->type] = ops;
+ *   - drivers/nvme/target/core.c|281| <<nvmet_unregister_transport>> nvmet_transports[ops->type] = NULL;
+ *   - drivers/nvme/target/core.c|305| <<nvmet_enable_port>> ops = nvmet_transports[port->disc_addr.trtype];
+ *   - drivers/nvme/target/core.c|310| <<nvmet_enable_port>> ops = nvmet_transports[port->disc_addr.trtype];
+ *   - drivers/nvme/target/core.c|345| <<nvmet_disable_port>> ops = nvmet_transports[port->disc_addr.trtype];
+ */
 static const struct nvmet_fabrics_ops *nvmet_transports[NVMF_TRTYPE_MAX];
 static DEFINE_IDA(cntlid_ida);
 
@@ -37,6 +55,18 @@ static DEFINE_IDA(cntlid_ida);
  */
 DECLARE_RWSEM(nvmet_config_sem);
 
+/*
+ * 在以下使用nvmet_ana_group_enabled[]:
+ *   - drivers/nvme/target/admin-cmd.c|268| <<nvmet_execute_get_log_page_ana>> if (!nvmet_ana_group_enabled[grpid])
+ *   - drivers/nvme/target/admin-cmd.c|278| <<nvmet_execute_get_log_page_ana>> if (nvmet_ana_group_enabled[grpid])
+ *   - drivers/nvme/target/configfs.c|521| <<nvmet_ns_ana_grpid_store>> nvmet_ana_group_enabled[newgrpid]++;
+ *   - drivers/nvme/target/configfs.c|523| <<nvmet_ns_ana_grpid_store>> nvmet_ana_group_enabled[oldgrpid]--;
+ *   - drivers/nvme/target/configfs.c|1147| <<nvmet_ana_group_release>> nvmet_ana_group_enabled[grp->grpid]--;
+ *   - drivers/nvme/target/configfs.c|1188| <<nvmet_ana_groups_make_group>> nvmet_ana_group_enabled[grpid]++;
+ *   - drivers/nvme/target/core.c|665| <<nvmet_ns_free>> nvmet_ana_group_enabled[ns->anagrpid]--;
+ *   - drivers/nvme/target/core.c|691| <<nvmet_ns_alloc>> nvmet_ana_group_enabled[ns->anagrpid]++;
+ *   - drivers/nvme/target/core.c|1539| <<nvmet_init>> nvmet_ana_group_enabled[NVMET_DEFAULT_ANA_GRPID] = 1;
+ */
 u32 nvmet_ana_group_enabled[NVMET_MAX_ANAGRPS + 1];
 u64 nvmet_ana_chgcnt;
 DECLARE_RWSEM(nvmet_ana_sem);
@@ -85,6 +115,23 @@ inline u16 errno_to_nvme_status(struct nvmet_req *req, int errno)
 static struct nvmet_subsys *nvmet_find_get_subsys(struct nvmet_port *port,
 		const char *subsysnqn);
 
+/*
+ * called by:
+ *   - drivers/nvme/target/admin-cmd.c|59| <<nvmet_execute_get_log_page_error>> if (nvmet_copy_to_sgl(req, offset, &ctrl->slots[slot],
+ *   - drivers/nvme/target/admin-cmd.c|166| <<nvmet_execute_get_log_page_smart>> status = nvmet_copy_to_sgl(req, 0, log, sizeof(*log));
+ *   - drivers/nvme/target/admin-cmd.c|196| <<nvmet_execute_get_log_cmd_effects_ns>> status = nvmet_copy_to_sgl(req, 0, log, sizeof(*log));
+ *   - drivers/nvme/target/admin-cmd.c|217| <<nvmet_execute_get_log_changed_ns>> status = nvmet_copy_to_sgl(req, 0, ctrl->changed_ns_list, len);
+ *   - drivers/nvme/target/admin-cmd.c|271| <<nvmet_execute_get_log_page_ana>> status = nvmet_copy_to_sgl(req, offset, desc, len);
+ *   - drivers/nvme/target/admin-cmd.c|290| <<nvmet_execute_get_log_page_ana>> status = nvmet_copy_to_sgl(req, 0, &hdr, sizeof(hdr));
+ *   - drivers/nvme/target/admin-cmd.c|441| <<nvmet_execute_identify_ctrl>> status = nvmet_copy_to_sgl(req, 0, id, sizeof(*id));
+ *   - drivers/nvme/target/admin-cmd.c|510| <<nvmet_execute_identify_ns>> status = nvmet_copy_to_sgl(req, 0, id, sizeof(*id));
+ *   - drivers/nvme/target/admin-cmd.c|542| <<nvmet_execute_identify_nslist>> status = nvmet_copy_to_sgl(req, 0, list, buf_size);
+ *   - drivers/nvme/target/admin-cmd.c|558| <<nvmet_copy_ns_identifier>> status = nvmet_copy_to_sgl(req, *off, &desc, sizeof(desc));
+ *   - drivers/nvme/target/admin-cmd.c|563| <<nvmet_copy_ns_identifier>> status = nvmet_copy_to_sgl(req, *off, id, len);
+ *   - drivers/nvme/target/admin-cmd.c|838| <<nvmet_execute_get_features>> status = nvmet_copy_to_sgl(req, 0, &req->sq->ctrl->hostid,
+ *   - drivers/nvme/target/discovery.c|240| <<nvmet_execute_disc_get_log_page>> status = nvmet_copy_to_sgl(req, 0, buffer + offset, data_len);
+ *   - drivers/nvme/target/discovery.c|295| <<nvmet_execute_disc_identify>> status = nvmet_copy_to_sgl(req, 0, id, sizeof(*id));
+ */
 u16 nvmet_copy_to_sgl(struct nvmet_req *req, off_t off, const void *buf,
 		size_t len)
 {
@@ -95,6 +142,13 @@ u16 nvmet_copy_to_sgl(struct nvmet_req *req, off_t off, const void *buf,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fabrics-cmd.c|167| <<nvmet_execute_admin_connect>> status = nvmet_copy_from_sgl(req, 0, d, sizeof(*d));
+ *   - drivers/nvme/target/fabrics-cmd.c|234| <<nvmet_execute_io_connect>> status = nvmet_copy_from_sgl(req, 0, d, sizeof(*d));
+ *   - drivers/nvme/target/io-cmd-bdev.c|259| <<nvmet_bdev_execute_discard>> status = nvmet_copy_from_sgl(req, i * sizeof(range), &range,
+ *   - drivers/nvme/target/io-cmd-file.c|299| <<nvmet_file_execute_discard>> status = nvmet_copy_from_sgl(req, i * sizeof(range), &range,
+ */
 u16 nvmet_copy_from_sgl(struct nvmet_req *req, off_t off, void *buf, size_t len)
 {
 	if (sg_pcopy_to_buffer(req->sg, req->sg_cnt, buf, len, off) != len) {
@@ -173,6 +227,12 @@ static void nvmet_async_event_work(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|275| <<nvmet_ns_changed>> nvmet_add_async_event(ctrl, NVME_AER_TYPE_NOTICE,
+ *   - drivers/nvme/target/core.c|292| <<nvmet_send_ana_event>> nvmet_add_async_event(ctrl, NVME_AER_TYPE_NOTICE,
+ *   - drivers/nvme/target/discovery.c|38| <<__nvmet_disc_changed>> nvmet_add_async_event(ctrl, NVME_AER_TYPE_NOTICE,
+ */
 void nvmet_add_async_event(struct nvmet_ctrl *ctrl, u8 event_type,
 		u8 event_info, u8 log_page)
 {
@@ -295,6 +355,10 @@ void nvmet_port_del_ctrls(struct nvmet_port *port, struct nvmet_subsys *subsys)
 	mutex_unlock(&subsys->lock);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/configfs.c|682| <<nvmet_port_subsys_allow_link>> ret = nvmet_enable_port(port);
+ */
 int nvmet_enable_port(struct nvmet_port *port)
 {
 	const struct nvmet_fabrics_ops *ops;
@@ -510,6 +574,10 @@ static void nvmet_p2pmem_ns_add_p2p(struct nvmet_ctrl *ctrl,
 		ns->nsid);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/configfs.c|549| <<nvmet_ns_enable_store>> ret = nvmet_ns_enable(ns);
+ */
 int nvmet_ns_enable(struct nvmet_ns *ns)
 {
 	struct nvmet_subsys *subsys = ns->subsys;
@@ -640,6 +708,9 @@ struct nvmet_ns *nvmet_ns_alloc(struct nvmet_subsys *subsys, u32 nsid)
 	if (!ns)
 		return NULL;
 
+	/*
+	 * 参考nvmet_ns_enable()
+	 */
 	INIT_LIST_HEAD(&ns->dev_link);
 	init_completion(&ns->disable_done);
 
@@ -657,6 +728,10 @@ struct nvmet_ns *nvmet_ns_alloc(struct nvmet_subsys *subsys, u32 nsid)
 	return ns;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|782| <<__nvmet_req_complete>> nvmet_update_sq_head(req);
+ */
 static void nvmet_update_sq_head(struct nvmet_req *req)
 {
 	if (req->sq->size) {
@@ -700,10 +775,30 @@ static void nvmet_set_error(struct nvmet_req *req, u16 status)
 	req->cqe->status |= cpu_to_le16(1 << 14);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|796| <<nvmet_req_complete>> __nvmet_req_complete(req, status);
+ *   - drivers/nvme/target/core.c|1018| <<nvmet_req_init>> __nvmet_req_complete(req, status);
+ */
 static void __nvmet_req_complete(struct nvmet_req *req, u16 status)
 {
+	/*
+	 * 在以下使用sqhd_disabled:
+	 *   - drivers/nvme/target/core.c|781| <<__nvmet_req_complete>> if (!req->sq->sqhd_disabled)
+	 *   - drivers/nvme/target/fabrics-cmd.c|130| <<nvmet_install_queue>> req->sq->sqhd_disabled = true;
+	 *   - drivers/nvme/target/tcp.c|374| <<nvmet_setup_c2h_data_pdu>> pdu->hdr.flags = NVME_TCP_F_DATA_LAST | (queue->nvme_sq.sqhd_disabled ?
+	 *   - drivers/nvme/target/tcp.c|546| <<nvmet_try_send_data>> if (queue->nvme_sq.sqhd_disabled) {
+	 *   - drivers/nvme/target/tcp.c|554| <<nvmet_try_send_data>> if (queue->nvme_sq.sqhd_disabled) {
+	 *   - drivers/nvme/target/tcp.c|635| <<nvmet_try_send_ddgst>> if (queue->nvme_sq.sqhd_disabled) {
+	 */
 	if (!req->sq->sqhd_disabled)
 		nvmet_update_sq_head(req);
+	/*
+	 * struct nvmet_req:
+	 *   -> struct nvme_completion  *cqe;
+	 *   -> struct nvmet_sq         *sq;
+	 *   -> struct nvmet_cq         *cq;
+	 */
 	req->cqe->sq_id = cpu_to_le16(req->sq->qid);
 	req->cqe->command_id = req->cmd->common.command_id;
 
@@ -714,12 +809,22 @@ static void __nvmet_req_complete(struct nvmet_req *req, u16 status)
 
 	if (req->ns)
 		nvmet_put_namespace(req->ns);
+	/*
+	 * 比如nvme_loop_queue_response()
+	 *
+	 * 只在这里调用.queue_response(), 相当于触发interrupt handler了
+	 */
 	req->ops->queue_response(req);
 }
 
 void nvmet_req_complete(struct nvmet_req *req, u16 status)
 {
 	__nvmet_req_complete(req, status);
+	/*
+	 * struct nvmet_req *req:
+	 *  -> struct nvmet_sq *sq;
+	 *      -> struct percpu_ref ref;
+	 */
 	percpu_ref_put(&req->sq->ref);
 }
 EXPORT_SYMBOL_GPL(nvmet_req_complete);
@@ -750,12 +855,27 @@ static void nvmet_confirm_sq(struct percpu_ref *ref)
 	complete(&sq->confirm_done);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|758| <<nvmet_fc_delete_target_queue>> nvmet_sq_destroy(&queue->nvme_sq);
+ *   - drivers/nvme/target/loop.c|396| <<nvme_loop_destroy_admin_queue>> nvmet_sq_destroy(&ctrl->queues[0].nvme_sq);
+ *   - drivers/nvme/target/loop.c|454| <<nvme_loop_destroy_io_queues>> nvmet_sq_destroy(&ctrl->queues[i].nvme_sq);
+ *   - drivers/nvme/target/loop.c|617| <<nvme_loop_configure_admin_queue>> nvmet_sq_destroy(&ctrl->queues[0].nvme_sq);
+ *   - drivers/nvme/target/rdma.c|1068| <<nvmet_rdma_free_queue>> nvmet_sq_destroy(&queue->nvme_sq);
+ *   - drivers/nvme/target/rdma.c|1220| <<nvmet_rdma_alloc_queue>> nvmet_sq_destroy(&queue->nvme_sq);
+ *   - drivers/nvme/target/tcp.c|1340| <<nvmet_tcp_release_queue_work>> nvmet_sq_destroy(&queue->nvme_sq);
+ *   - drivers/nvme/target/tcp.c|1514| <<nvmet_tcp_alloc_queue>> nvmet_sq_destroy(&queue->nvme_sq);
+ */
 void nvmet_sq_destroy(struct nvmet_sq *sq)
 {
 	/*
 	 * If this is the admin queue, complete all AERs so that our
 	 * queue doesn't have outstanding requests on it.
 	 */
+	/*
+	 * struct nvmet_sq:
+	 *   - struct nvmet_ctrl *ctrl;
+	 */
 	if (sq->ctrl && sq->ctrl->sqs && sq->ctrl->sqs[0] == sq)
 		nvmet_async_events_free(sq->ctrl);
 	percpu_ref_kill_and_confirm(&sq->ref, nvmet_confirm_sq);
@@ -777,6 +897,14 @@ static void nvmet_sq_free(struct percpu_ref *ref)
 	complete(&sq->free_done);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|631| <<nvmet_fc_alloc_target_queue>> ret = nvmet_sq_init(&queue->nvme_sq);
+ *   - drivers/nvme/target/loop.c|518| <<nvme_loop_init_io_queues>> ret = nvmet_sq_init(&ctrl->queues[i].nvme_sq);
+ *   - drivers/nvme/target/loop.c|588| <<nvme_loop_configure_admin_queue>> error = nvmet_sq_init(&ctrl->queues[0].nvme_sq);
+ *   - drivers/nvme/target/rdma.c|1150| <<nvmet_rdma_alloc_queue>> ret = nvmet_sq_init(&queue->nvme_sq);
+ *   - drivers/nvme/target/tcp.c|1490| <<nvmet_tcp_alloc_queue>> ret = nvmet_sq_init(&queue->nvme_sq);
+ */
 int nvmet_sq_init(struct nvmet_sq *sq)
 {
 	int ret;
@@ -793,6 +921,10 @@ int nvmet_sq_init(struct nvmet_sq *sq)
 }
 EXPORT_SYMBOL_GPL(nvmet_sq_init);
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|839| <<nvmet_parse_io_cmd>> ret = nvmet_check_ana_state(req->port, req->ns);
+ */
 static inline u16 nvmet_check_ana_state(struct nvmet_port *port,
 		struct nvmet_ns *ns)
 {
@@ -822,6 +954,10 @@ static inline u16 nvmet_io_cmd_check_access(struct nvmet_req *req)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|898| <<nvmet_req_init>> status = nvmet_parse_io_cmd(req);
+ */
 static u16 nvmet_parse_io_cmd(struct nvmet_req *req)
 {
 	struct nvme_command *cmd = req->cmd;
@@ -847,12 +983,26 @@ static u16 nvmet_parse_io_cmd(struct nvmet_req *req)
 		return ret;
 	}
 
+	/*
+	 * struct nvmet_req:
+	 *  - struct nvmet_ns *ns;
+	 *     - struct file *file;
+	 */
 	if (req->ns->file)
 		return nvmet_file_parse_io_cmd(req);
 	else
 		return nvmet_bdev_parse_io_cmd(req);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/trace.h|65| <<global>> TRACE_EVENT(nvmet_req_init,
+ *   - drivers/nvme/target/fc.c|2225| <<nvmet_fc_handle_fcp_rqst>> ret = nvmet_req_init(&fod->req,
+ *   - drivers/nvme/target/loop.c|285| <<nvme_loop_queue_rq>> if (!nvmet_req_init(&iod->req, &queue->nvme_cq,
+ *   - drivers/nvme/target/loop.c|327| <<nvme_loop_submit_async_event>> if (!nvmet_req_init(&iod->req, &queue->nvme_cq, &queue->nvme_sq,
+ *   - drivers/nvme/target/rdma.c|767| <<nvmet_rdma_handle_command>> if (!nvmet_req_init(&cmd->req, &queue->nvme_cq,
+ *   - drivers/nvme/target/tcp.c|900| <<nvmet_tcp_done_recv_pdu>> if (unlikely(!nvmet_req_init(req, &queue->nvme_cq,
+ */
 bool nvmet_req_init(struct nvmet_req *req, struct nvmet_cq *cq,
 		struct nvmet_sq *sq, const struct nvmet_fabrics_ops *ops)
 {
@@ -913,6 +1063,11 @@ bool nvmet_req_init(struct nvmet_req *req, struct nvmet_cq *cq,
 	return true;
 
 fail:
+	/*
+	 * called by:
+	 *   - drivers/nvme/target/core.c|796| <<nvmet_req_complete>> __nvmet_req_complete(req, status);
+	 *   - drivers/nvme/target/core.c|1018| <<nvmet_req_init>> __nvmet_req_complete(req, status);
+	 */
 	__nvmet_req_complete(req, status);
 	return false;
 }
@@ -1018,6 +1173,17 @@ static inline u8 nvmet_cc_iocqes(u32 cc)
 	return (cc >> NVME_CC_IOCQES_SHIFT) & 0xf;
 }
 
+/*
+ * [0] nvmet_update_cc
+ * [0] nvmet_execute_prop_set
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/target/core.c|1146| <<nvmet_update_cc>> nvmet_start_ctrl(ctrl);
+ */
 static void nvmet_start_ctrl(struct nvmet_ctrl *ctrl)
 {
 	lockdep_assert_held(&ctrl->lock);
@@ -1082,6 +1248,10 @@ static void nvmet_init_cap(struct nvmet_ctrl *ctrl)
 	ctrl->cap |= NVMET_QUEUE_SIZE - 1;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fabrics-cmd.c|248| <<nvmet_execute_io_connect>> status = nvmet_ctrl_find_get(d->subsysnqn, d->hostnqn,
+ */
 u16 nvmet_ctrl_find_get(const char *subsysnqn, const char *hostnqn, u16 cntlid,
 		struct nvmet_req *req, struct nvmet_ctrl **ret)
 {
@@ -1139,6 +1309,13 @@ u16 nvmet_check_ctrl_status(struct nvmet_req *req, struct nvme_command *cmd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|1296| <<nvmet_alloc_ctrl>> if (!nvmet_host_allowed(subsys, hostnqn)) {
+ *   - drivers/nvme/target/discovery.c|38| <<nvmet_port_disc_changed>> if (subsys && !nvmet_host_allowed(subsys, ctrl->hostnqn))
+ *   - drivers/nvme/target/discovery.c|157| <<discovery_log_entries>> if (!nvmet_host_allowed(p->subsys, ctrl->hostnqn))
+ *   - drivers/nvme/target/discovery.c|214| <<nvmet_execute_disc_get_log_page>> if (!nvmet_host_allowed(p->subsys, ctrl->hostnqn))
+ */
 bool nvmet_host_allowed(struct nvmet_subsys *subsys, const char *hostnqn)
 {
 	struct nvmet_host_link *p;
@@ -1199,6 +1376,17 @@ static void nvmet_fatal_error_handler(struct work_struct *work)
 	ctrl->ops->delete_ctrl(ctrl);
 }
 
+/*
+ * [0] nvmet_alloc_ctrl
+ * [0] nvmet_execute_admin_connect
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/target/fabrics-cmd.c|186| <<nvmet_execute_admin_connect>> status = nvmet_alloc_ctrl(d->subsysnqn, d->hostnqn, req,
+ */
 u16 nvmet_alloc_ctrl(const char *subsysnqn, const char *hostnqn,
 		struct nvmet_req *req, u32 kato, struct nvmet_ctrl **ctrlp)
 {
@@ -1294,6 +1482,9 @@ u16 nvmet_alloc_ctrl(const char *subsysnqn, const char *hostnqn,
 	nvmet_start_keep_alive_timer(ctrl);
 
 	mutex_lock(&subsys->lock);
+	/*
+	 * 这里是很重要的把controller给了subsystem
+	 */
 	list_add_tail(&ctrl->subsys_entry, &subsys->ctrls);
 	nvmet_setup_p2p_ns_map(ctrl, req);
 	mutex_unlock(&subsys->lock);
@@ -1356,6 +1547,11 @@ void nvmet_ctrl_fatal_error(struct nvmet_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvmet_ctrl_fatal_error);
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|1163| <<nvmet_ctrl_find_get>> subsys = nvmet_find_get_subsys(req->port, subsysnqn);
+ *   - drivers/nvme/target/core.c|1286| <<nvmet_alloc_ctrl>> subsys = nvmet_find_get_subsys(req->port, subsysnqn);
+ */
 static struct nvmet_subsys *nvmet_find_get_subsys(struct nvmet_port *port,
 		const char *subsysnqn)
 {
@@ -1384,6 +1580,11 @@ static struct nvmet_subsys *nvmet_find_get_subsys(struct nvmet_port *port,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/configfs.c|967| <<nvmet_subsys_make>> subsys = nvmet_subsys_alloc(name, NVME_NQN_NVME);
+ *   - drivers/nvme/target/discovery.c|395| <<nvmet_init_discovery>> nvmet_subsys_alloc(NVME_DISC_SUBSYS_NAME, NVME_NQN_DISC);
+ */
 struct nvmet_subsys *nvmet_subsys_alloc(const char *subsysnqn,
 		enum nvme_subsys_type type)
 {
@@ -1427,6 +1628,10 @@ struct nvmet_subsys *nvmet_subsys_alloc(const char *subsysnqn,
 	return subsys;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|1532| <<nvmet_subsys_put>> kref_put(&subsys->ref, nvmet_subsys_free);
+ */
 static void nvmet_subsys_free(struct kref *ref)
 {
 	struct nvmet_subsys *subsys =
@@ -1438,6 +1643,10 @@ static void nvmet_subsys_free(struct kref *ref)
 	kfree(subsys);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/configfs.c|897| <<nvmet_subsys_release>> nvmet_subsys_del_ctrls(subsys);
+ */
 void nvmet_subsys_del_ctrls(struct nvmet_subsys *subsys)
 {
 	struct nvmet_ctrl *ctrl;
@@ -1448,6 +1657,14 @@ void nvmet_subsys_del_ctrls(struct nvmet_subsys *subsys)
 	mutex_unlock(&subsys->lock);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/configfs.c|932| <<nvmet_subsys_release>> nvmet_subsys_put(subsys);
+ *   - drivers/nvme/target/core.c|1193| <<nvmet_ctrl_find_get>> nvmet_subsys_put(subsys);
+ *   - drivers/nvme/target/core.c|1388| <<nvmet_alloc_ctrl>> nvmet_subsys_put(subsys);
+ *   - drivers/nvme/target/core.c|1415| <<nvmet_ctrl_free>> nvmet_subsys_put(subsys);
+ *   - drivers/nvme/target/discovery.c|401| <<nvmet_exit_discovery>> nvmet_subsys_put(nvmet_disc_subsys);
+ */
 void nvmet_subsys_put(struct nvmet_subsys *subsys)
 {
 	kref_put(&subsys->ref, nvmet_subsys_free);
@@ -1457,6 +1674,18 @@ static int __init nvmet_init(void)
 {
 	int error;
 
+	/*
+	 * 在以下使用nvmet_ana_group_enabled[]:
+	 *   - drivers/nvme/target/admin-cmd.c|268| <<nvmet_execute_get_log_page_ana>> if (!nvmet_ana_group_enabled[grpid])
+	 *   - drivers/nvme/target/admin-cmd.c|278| <<nvmet_execute_get_log_page_ana>> if (nvmet_ana_group_enabled[grpid])
+	 *   - drivers/nvme/target/configfs.c|521| <<nvmet_ns_ana_grpid_store>> nvmet_ana_group_enabled[newgrpid]++;
+	 *   - drivers/nvme/target/configfs.c|523| <<nvmet_ns_ana_grpid_store>> nvmet_ana_group_enabled[oldgrpid]--;
+	 *   - drivers/nvme/target/configfs.c|1147| <<nvmet_ana_group_release>> nvmet_ana_group_enabled[grp->grpid]--;
+	 *   - drivers/nvme/target/configfs.c|1188| <<nvmet_ana_groups_make_group>> nvmet_ana_group_enabled[grpid]++;
+	 *   - drivers/nvme/target/core.c|665| <<nvmet_ns_free>> nvmet_ana_group_enabled[ns->anagrpid]--;
+	 *   - drivers/nvme/target/core.c|691| <<nvmet_ns_alloc>> nvmet_ana_group_enabled[ns->anagrpid]++;
+	 *   - drivers/nvme/target/core.c|1539| <<nvmet_init>> nvmet_ana_group_enabled[NVMET_DEFAULT_ANA_GRPID] = 1;
+	 */
 	nvmet_ana_group_enabled[NVMET_DEFAULT_ANA_GRPID] = 1;
 
 	buffered_io_wq = alloc_workqueue("nvmet-buffered-io-wq",
diff --git a/drivers/nvme/target/discovery.c b/drivers/nvme/target/discovery.c
index 0c2274b21e15..6a7e278af792 100644
--- a/drivers/nvme/target/discovery.c
+++ b/drivers/nvme/target/discovery.c
@@ -8,6 +8,20 @@
 #include <generated/utsrelease.h>
 #include "nvmet.h"
 
+/*
+ * 在以下使用nvmet_disc_subsys:
+ *   - drivers/nvme/target/core.c|1443| <<nvmet_find_get_subsys>> if (!kref_get_unless_zero(&nvmet_disc_subsys->ref))
+ *   - drivers/nvme/target/core.c|1445| <<nvmet_find_get_subsys>> return nvmet_disc_subsys;
+ *   - drivers/nvme/target/discovery.c|36| <<nvmet_port_disc_changed>> mutex_lock(&nvmet_disc_subsys->lock);
+ *   - drivers/nvme/target/discovery.c|37| <<nvmet_port_disc_changed>> list_for_each_entry(ctrl, &nvmet_disc_subsys->ctrls, subsys_entry) {
+ *   - drivers/nvme/target/discovery.c|43| <<nvmet_port_disc_changed>> mutex_unlock(&nvmet_disc_subsys->lock);
+ *   - drivers/nvme/target/discovery.c|56| <<__nvmet_subsys_disc_changed>> mutex_lock(&nvmet_disc_subsys->lock);
+ *   - drivers/nvme/target/discovery.c|57| <<__nvmet_subsys_disc_changed>> list_for_each_entry(ctrl, &nvmet_disc_subsys->ctrls, subsys_entry) {
+ *   - drivers/nvme/target/discovery.c|63| <<__nvmet_subsys_disc_changed>> mutex_unlock(&nvmet_disc_subsys->lock);
+ *   - drivers/nvme/target/discovery.c|394| <<nvmet_init_discovery>> nvmet_disc_subsys =
+ *   - drivers/nvme/target/discovery.c|396| <<nvmet_init_discovery>> return PTR_ERR_OR_ZERO(nvmet_disc_subsys);
+ *   - drivers/nvme/target/discovery.c|401| <<nvmet_exit_discovery>> nvmet_subsys_put(nvmet_disc_subsys);
+ */
 struct nvmet_subsys *nvmet_disc_subsys;
 
 static u64 nvmet_genctr;
@@ -63,6 +77,12 @@ static void __nvmet_subsys_disc_changed(struct nvmet_port *port,
 	mutex_unlock(&nvmet_disc_subsys->lock);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/configfs.c|770| <<nvmet_allowed_hosts_allow_link>> nvmet_subsys_disc_changed(subsys, host);
+ *   - drivers/nvme/target/configfs.c|797| <<nvmet_allowed_hosts_drop_link>> nvmet_subsys_disc_changed(subsys, host);
+ *   - drivers/nvme/target/configfs.c|839| <<nvmet_subsys_attr_allow_any_host_store>> nvmet_subsys_disc_changed(subsys, NULL);
+ */
 void nvmet_subsys_disc_changed(struct nvmet_subsys *subsys,
 			       struct nvmet_host *host)
 {
@@ -344,6 +364,10 @@ static void nvmet_execute_disc_get_features(struct nvmet_req *req)
 	nvmet_req_complete(req, stat);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/admin-cmd.c|895| <<nvmet_parse_admin_cmd>> return nvmet_parse_discovery_cmd(req);
+ */
 u16 nvmet_parse_discovery_cmd(struct nvmet_req *req)
 {
 	struct nvme_command *cmd = req->cmd;
@@ -383,8 +407,28 @@ u16 nvmet_parse_discovery_cmd(struct nvmet_req *req)
 
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|1548| <<nvmet_init>> error = nvmet_init_discovery();
+ */
 int __init nvmet_init_discovery(void)
 {
+	/*
+	 * 在以下使用nvmet_disc_subsys:
+	 *   - drivers/nvme/target/core.c|1443| <<nvmet_find_get_subsys>> if (!kref_get_unless_zero(&nvmet_disc_subsys->ref))
+	 *   - drivers/nvme/target/core.c|1445| <<nvmet_find_get_subsys>> return nvmet_disc_subsys;
+	 *   - drivers/nvme/target/discovery.c|36| <<nvmet_port_disc_changed>> mutex_lock(&nvmet_disc_subsys->lock);
+	 *   - drivers/nvme/target/discovery.c|37| <<nvmet_port_disc_changed>> list_for_each_entry(ctrl, &nvmet_disc_subsys->ctrls, subsys_entry) {
+	 *   - drivers/nvme/target/discovery.c|43| <<nvmet_port_disc_changed>> mutex_unlock(&nvmet_disc_subsys->lock);
+	 *   - drivers/nvme/target/discovery.c|56| <<__nvmet_subsys_disc_changed>> mutex_lock(&nvmet_disc_subsys->lock);
+	 *   - drivers/nvme/target/discovery.c|57| <<__nvmet_subsys_disc_changed>> list_for_each_entry(ctrl, &nvmet_disc_subsys->ctrls, subsys_entry) {
+	 *   - drivers/nvme/target/discovery.c|63| <<__nvmet_subsys_disc_changed>> mutex_unlock(&nvmet_disc_subsys->lock);
+	 *   - drivers/nvme/target/discovery.c|394| <<nvmet_init_discovery>> nvmet_disc_subsys =
+	 *   - drivers/nvme/target/discovery.c|396| <<nvmet_init_discovery>> return PTR_ERR_OR_ZERO(nvmet_disc_subsys);
+	 *   - drivers/nvme/target/discovery.c|401| <<nvmet_exit_discovery>> nvmet_subsys_put(nvmet_disc_subsys);
+	 *
+	 * struct nvmet_subsys *nvmet_disc_subsys;
+	 */
 	nvmet_disc_subsys =
 		nvmet_subsys_alloc(NVME_DISC_SUBSYS_NAME, NVME_NQN_DISC);
 	return PTR_ERR_OR_ZERO(nvmet_disc_subsys);
diff --git a/drivers/nvme/target/fabrics-cmd.c b/drivers/nvme/target/fabrics-cmd.c
index f7297473d9eb..dd773434ba08 100644
--- a/drivers/nvme/target/fabrics-cmd.c
+++ b/drivers/nvme/target/fabrics-cmd.c
@@ -103,6 +103,11 @@ u16 nvmet_parse_fabrics_cmd(struct nvmet_req *req)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fabrics-cmd.c|201| <<nvmet_execute_admin_connect>> status = nvmet_install_queue(ctrl, req);
+ *   - drivers/nvme/target/fabrics-cmd.c|261| <<nvmet_execute_io_connect>> status = nvmet_install_queue(ctrl, req);
+ */
 static u16 nvmet_install_queue(struct nvmet_ctrl *ctrl, struct nvmet_req *req)
 {
 	struct nvmf_connect_command *c = &req->cmd->connect;
@@ -144,6 +149,10 @@ static u16 nvmet_install_queue(struct nvmet_ctrl *ctrl, struct nvmet_req *req)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fabrics-cmd.c|295| <<nvmet_parse_connect_cmd>> req->execute = nvmet_execute_admin_connect;
+ */
 static void nvmet_execute_admin_connect(struct nvmet_req *req)
 {
 	struct nvmf_connect_command *c = &req->cmd->connect;
diff --git a/drivers/nvme/target/fc.c b/drivers/nvme/target/fc.c
index a0db6371b43e..dc66c120d3ae 100644
--- a/drivers/nvme/target/fc.c
+++ b/drivers/nvme/target/fc.c
@@ -211,8 +211,20 @@ nvmet_req_to_fod(struct nvmet_req *nvme_req)
 
 static DEFINE_SPINLOCK(nvmet_fc_tgtlock);
 
+/*
+ * 在以下使用nvmet_fc_target_list:
+ *   - drivers/nvme/target/fc.c|1079| <<nvmet_fc_register_targetport>> list_add_tail(&newrec->tgt_list, &nvmet_fc_target_list);
+ *   - drivers/nvme/target/fc.c|1168| <<nvmet_fc_delete_ctrl>> list_for_each_entry_safe(tgtport, next, &nvmet_fc_target_list,
+ *   - drivers/nvme/target/fc.c|2530| <<nvmet_fc_add_port>> list_for_each_entry(tgtport, &nvmet_fc_target_list, tgt_list) {
+ *   - drivers/nvme/target/fc.c|2589| <<nvmet_fc_exit_module>> if (!list_empty(&nvmet_fc_target_list))
+ */
 static LIST_HEAD(nvmet_fc_target_list);
 static DEFINE_IDA(nvmet_fc_tgtport_cnt);
+/*
+ * 在以下使用nvmet_fc_portentry_list:
+ *   - drivers/nvme/target/fc.c|941| <<nvmet_fc_portentry_bind>> list_add_tail(&pe->pe_list, &nvmet_fc_portentry_list);
+ *   - drivers/nvme/target/fc.c|990| <<nvmet_fc_portentry_rebind_tgt>> list_for_each_entry(pe, &nvmet_fc_portentry_list, pe_list) {
+ */
 static LIST_HEAD(nvmet_fc_portentry_list);
 
 
@@ -1009,6 +1021,11 @@ nvmet_fc_portentry_rebind_tgt(struct nvmet_fc_tgtport *tgtport)
  * a completion status. Must be 0 upon success; a negative errno
  * (ex: -ENXIO) upon failure.
  */
+/*
+ * called by:
+ *   - drivers/nvme/target/fcloop.c|1228| <<fcloop_create_target_port>> ret = nvmet_fc_register_targetport(&tinfo, &tgttemplate, NULL,
+ *   - drivers/scsi/lpfc/lpfc_nvmet.c|1469| <<lpfc_nvmet_create_targetport>> error = nvmet_fc_register_targetport(&pinfo, &lpfc_tgttemplate,
+ */
 int
 nvmet_fc_register_targetport(struct nvmet_fc_port_info *pinfo,
 			struct nvmet_fc_target_template *template,
@@ -1071,6 +1088,15 @@ nvmet_fc_register_targetport(struct nvmet_fc_port_info *pinfo,
 	nvmet_fc_portentry_rebind_tgt(newrec);
 
 	spin_lock_irqsave(&nvmet_fc_tgtlock, flags);
+	/*
+	 * 在以下使用nvmet_fc_target_list:
+	 *   - drivers/nvme/target/fc.c|1079| <<nvmet_fc_register_targetport>> list_add_tail(&newrec->tgt_list, &nvmet_fc_target_list);
+	 *   - drivers/nvme/target/fc.c|1168| <<nvmet_fc_delete_ctrl>> list_for_each_entry_safe(tgtport, next, &nvmet_fc_target_list,
+	 *   - drivers/nvme/target/fc.c|2530| <<nvmet_fc_add_port>> list_for_each_entry(tgtport, &nvmet_fc_target_list, tgt_list) {
+	 *   - drivers/nvme/target/fc.c|2589| <<nvmet_fc_exit_module>> if (!list_empty(&nvmet_fc_target_list))
+	 *
+	 * 这里的nvmet_fc_add_port()是重点!
+	 */
 	list_add_tail(&newrec->tgt_list, &nvmet_fc_target_list);
 	spin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);
 
@@ -2452,6 +2478,10 @@ __nvme_fc_parse_u64(substring_t *sstr, u64 *val)
  * As kernel parsers need the 0x to determine number base, universally
  * build string to parse with 0x prefix before parsing name strings.
  */
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|2543| <<nvmet_fc_add_port>> ret = nvme_fc_parse_traddr(&traddr, port->disc_addr.traddr,
+ */
 static int
 nvme_fc_parse_traddr(struct nvmet_fc_traddr *traddr, char *buf, size_t blen)
 {
@@ -2495,6 +2525,9 @@ nvme_fc_parse_traddr(struct nvmet_fc_traddr *traddr, char *buf, size_t blen)
 	return -EINVAL;
 }
 
+/*
+ * struct nvmet_fabrics_ops nvmet_fc_tgt_fcp_ops.add_port = nvmet_fc_add_port()
+ */
 static int
 nvmet_fc_add_port(struct nvmet_port *port)
 {
@@ -2522,6 +2555,13 @@ nvmet_fc_add_port(struct nvmet_port *port)
 
 	ret = -ENXIO;
 	spin_lock_irqsave(&nvmet_fc_tgtlock, flags);
+	/*
+	 * 在以下使用nvmet_fc_target_list:
+	 *   - drivers/nvme/target/fc.c|1079| <<nvmet_fc_register_targetport>> list_add_tail(&newrec->tgt_list, &nvmet_fc_target_list);
+	 *   - drivers/nvme/target/fc.c|1168| <<nvmet_fc_delete_ctrl>> list_for_each_entry_safe(tgtport, next, &nvmet_fc_target_list,
+	 *   - drivers/nvme/target/fc.c|2530| <<nvmet_fc_add_port>> list_for_each_entry(tgtport, &nvmet_fc_target_list, tgt_list) {
+	 *   - drivers/nvme/target/fc.c|2589| <<nvmet_fc_exit_module>> if (!list_empty(&nvmet_fc_target_list))
+	 */
 	list_for_each_entry(tgtport, &nvmet_fc_target_list, tgt_list) {
 		if ((tgtport->fc_target_port.node_name == traddr.nn) &&
 		    (tgtport->fc_target_port.port_name == traddr.pn)) {
diff --git a/drivers/nvme/target/fcloop.c b/drivers/nvme/target/fcloop.c
index 1c50af6219f3..8adfa7ec615c 100644
--- a/drivers/nvme/target/fcloop.c
+++ b/drivers/nvme/target/fcloop.c
@@ -12,6 +12,15 @@
 #include <linux/nvme-fc-driver.h>
 #include <linux/nvme-fc.h>
 
+/*
+ * 核心的三个函数:
+ *   - fcloop_create_local_port()
+ *   - fcloop_create_remote_port()
+ *   - fcloop_create_target_port()
+ *   - fcloop_delete_local_port()
+ *   - fcloop_delete_remote_port()
+ *   - fcloop_delete_target_port()
+ */
 
 enum {
 	NVMF_OPT_ERR		= 0,
@@ -33,6 +42,11 @@ struct fcloop_ctrl_options {
 	u64			lpwwpn;
 };
 
+/*
+ * 在以下使用opt_tokens:
+ *   - drivers/nvme/target/fcloop.c|63| <<fcloop_parse_options>> token = match_token(p, opt_tokens, args);
+ *   - drivers/nvme/target/fcloop.c|141| <<fcloop_parse_nm_options>> token = match_token(p, opt_tokens, args);
+ */
 static const match_table_t opt_tokens = {
 	{ NVMF_OPT_WWNN,	"wwnn=%s"	},
 	{ NVMF_OPT_WWPN,	"wwpn=%s"	},
@@ -43,10 +57,18 @@ static const match_table_t opt_tokens = {
 	{ NVMF_OPT_ERR,		NULL		}
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fcloop.c|910| <<fcloop_create_local_port>> ret = fcloop_parse_options(opts, buf);
+ *   - drivers/nvme/target/fcloop.c|1021| <<fcloop_alloc_nport>> ret = fcloop_parse_options(opts, buf);
+ */
 static int
 fcloop_parse_options(struct fcloop_ctrl_options *opts,
 		const char *buf)
 {
+	/*
+	 * 为什么这里是MAX_OPT_ARGS呢
+	 */
 	substring_t args[MAX_OPT_ARGS];
 	char *options, *o, *p;
 	int token, ret = 0;
@@ -56,6 +78,9 @@ fcloop_parse_options(struct fcloop_ctrl_options *opts,
 	if (!options)
 		return -ENOMEM;
 
+	/*
+	 * echo "wwnn=0x3333333333333333,wwpn=0x1111111111111111,roles=64,lpwwnn=0x2222222222222222,lpwwpn=0x1111111111111111" > /sys/class/fcloop/ctl/add_remote_port
+	 */
 	while ((p = strsep(&o, ",\n")) != NULL) {
 		if (!*p)
 			continue;
@@ -64,10 +89,20 @@ fcloop_parse_options(struct fcloop_ctrl_options *opts,
 		opts->mask |= token;
 		switch (token) {
 		case NVMF_OPT_WWNN:
+			/*
+			 * 一个例子:
+			 * from = "0x2222222222222222"
+			 * from = 0xffff93aaa9d08885
+			 * to   = 0xffff93aaa9d08897
+			 */
 			if (match_u64(args, &token64)) {
 				ret = -EINVAL;
 				goto out_free_options;
 			}
+			/*
+			 * struct fcloop_ctrl_options:
+			 *  -> u64 wwnn;
+			 */
 			opts->wwnn = token64;
 			break;
 		case NVMF_OPT_WWPN:
@@ -118,6 +153,12 @@ fcloop_parse_options(struct fcloop_ctrl_options *opts,
 }
 
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fcloop.c|993| <<fcloop_delete_local_port>> ret = fcloop_parse_nm_options(dev, &nodename, &portname, buf);
+ *   - drivers/nvme/target/fcloop.c|1185| <<fcloop_delete_remote_port>> ret = fcloop_parse_nm_options(dev, &nodename, &portname, buf);
+ *   - drivers/nvme/target/fcloop.c|1280| <<fcloop_delete_target_port>> ret = fcloop_parse_nm_options(dev, &nodename, &portname, buf);
+ */
 static int
 fcloop_parse_nm_options(struct device *dev, u64 *nname, u64 *pname,
 		const char *buf)
@@ -138,6 +179,17 @@ fcloop_parse_nm_options(struct device *dev, u64 *nname, u64 *pname,
 		if (!*p)
 			continue;
 
+		/*
+		 * 在以下使用opt_tokens:
+		 *   - drivers/nvme/target/fcloop.c|63| <<fcloop_parse_options>> token = match_token(p, opt_tokens, args);
+		 *   - drivers/nvme/target/fcloop.c|141| <<fcloop_parse_nm_options>> token = match_token(p, opt_tokens, args);
+		 *
+		 * Find a token (and optional args) in a string:
+		 * Description: Detects which if any of a set of token strings has been passed
+		 * to it. Tokens can include up to MAX_OPT_ARGS instances of basic c-style
+		 * format identifiers which will be taken into account when matching the
+		 * tokens, and whose locations will be returned in the @args array.
+		 */
 		token = match_token(p, opt_tokens, args);
 		switch (token) {
 		case NVMF_OPT_WWNN:
@@ -184,7 +236,22 @@ fcloop_parse_nm_options(struct device *dev, u64 *nname, u64 *pname,
 
 
 static DEFINE_SPINLOCK(fcloop_lock);
+/*
+ * 在以下使用fcloop_lports:
+ *   - drivers/nvme/target/fcloop.c|998| <<fcloop_create_local_port>> list_add_tail(&lport->lport_list, &fcloop_lports);
+ *   - drivers/nvme/target/fcloop.c|1051| <<fcloop_delete_local_port>> list_for_each_entry(tlport, &fcloop_lports, lport_list) {
+ *   - drivers/nvme/target/fcloop.c|1113| <<fcloop_alloc_nport>> list_for_each_entry(tmplport, &fcloop_lports, lport_list) {
+ *   - drivers/nvme/target/fcloop.c|1482| <<fcloop_exit>> lport = list_first_entry_or_null(&fcloop_lports,
+ */
 static LIST_HEAD(fcloop_lports);
+/*
+ * 在以下使用fcloop_nports:
+ *   - drivers/nvme/target/fcloop.c|1129| <<fcloop_alloc_nport>> list_for_each_entry(nport, &fcloop_nports, nport_list) {
+ *   - drivers/nvme/target/fcloop.c|1152| <<fcloop_alloc_nport>> list_add_tail(&newnport->nport_list, &fcloop_nports);
+ *   - drivers/nvme/target/fcloop.c|1274| <<fcloop_delete_remote_port>> list_for_each_entry(tmpport, &fcloop_nports, nport_list) {
+ *   - drivers/nvme/target/fcloop.c|1369| <<fcloop_delete_target_port>> list_for_each_entry(tmpport, &fcloop_nports, nport_list) {
+ *   - drivers/nvme/target/fcloop.c|1460| <<fcloop_exit>> nport = list_first_entry_or_null(&fcloop_nports,
+ */
 static LIST_HEAD(fcloop_nports);
 
 struct fcloop_lport {
@@ -215,6 +282,16 @@ struct fcloop_nport {
 	struct fcloop_rport *rport;
 	struct fcloop_tport *tport;
 	struct fcloop_lport *lport;
+	/*
+	 * 在以下使用floop_nport->nport_list:
+	 *   - drivers/nvme/target/fcloop.c|845| <<fcloop_nport_free>> list_del(&nport->nport_list);
+	 *   - drivers/nvme/target/fcloop.c|1148| <<fcloop_alloc_nport>> INIT_LIST_HEAD(&newnport->nport_list);
+	 *   - drivers/nvme/target/fcloop.c|1192| <<fcloop_alloc_nport>> list_for_each_entry(nport, &fcloop_nports, nport_list) {
+	 *   - drivers/nvme/target/fcloop.c|1223| <<fcloop_alloc_nport>> list_add_tail(&newnport->nport_list, &fcloop_nports);
+	 *   - drivers/nvme/target/fcloop.c|1345| <<fcloop_delete_remote_port>> list_for_each_entry(tmpport, &fcloop_nports, nport_list) {
+	 *   - drivers/nvme/target/fcloop.c|1466| <<fcloop_delete_target_port>> list_for_each_entry(tmpport, &fcloop_nports, nport_list) {
+	 *   - drivers/nvme/target/fcloop.c|1558| <<fcloop_exit>> typeof(*nport), nport_list);
+	 */
 	struct list_head nport_list;
 	struct kref ref;
 	u64 node_name;
@@ -522,6 +599,10 @@ fcloop_tgt_fcprqst_done_work(struct work_struct *work)
 }
 
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2298| <<nvme_fc_start_fcp_op>> ret = ctrl->lport->ops->fcp_io(&ctrl->lport->localport,
+ */
 static int
 fcloop_fcp_req(struct nvme_fc_local_port *localport,
 			struct nvme_fc_remote_port *remoteport,
@@ -849,6 +930,10 @@ fcloop_targetport_delete(struct nvmet_fc_target_port *targetport)
 #define	FCLOOP_SGL_SEGS			256
 #define FCLOOP_DMABOUND_4G		0xFFFFFFFF
 
+/*
+ * 在以下使用fctemplate:
+ *   - drivers/nvme/target/fcloop.c|936| <<fcloop_create_local_port>> ret = nvme_fc_register_localport(&pinfo, &fctemplate, NULL, &localport);
+ */
 static struct nvme_fc_port_template fctemplate = {
 	.module			= THIS_MODULE,
 	.localport_delete	= fcloop_localport_delete,
@@ -870,6 +955,10 @@ static struct nvme_fc_port_template fctemplate = {
 	.fcprqst_priv_sz	= sizeof(struct fcloop_ini_fcpreq),
 };
 
+/*
+ * 在以下使用tgttemplate:
+ *   - drivers/nvme/target/fcloop.c|1228| <<fcloop_create_target_port>> ret = nvmet_fc_register_targetport(&tinfo, &tgttemplate, NULL,
+ */
 static struct nvmet_fc_target_template tgttemplate = {
 	.targetport_delete	= fcloop_targetport_delete,
 	.xmt_ls_rsp		= fcloop_xmt_ls_rsp,
@@ -899,6 +988,7 @@ fcloop_create_local_port(struct device *dev, struct device_attribute *attr,
 	unsigned long flags;
 	int ret = -ENOMEM;
 
+	/* 分配struct fcloop_lport */
 	lport = kzalloc(sizeof(*lport), GFP_KERNEL);
 	if (!lport)
 		return -ENOMEM;
@@ -907,6 +997,11 @@ fcloop_create_local_port(struct device *dev, struct device_attribute *attr,
 	if (!opts)
 		goto out_free_lport;
 
+	/*
+	 * 在以下调用fcloop_parse_options():
+	 *   - drivers/nvme/target/fcloop.c|910| <<fcloop_create_local_port>> ret = fcloop_parse_options(opts, buf);
+	 *   - drivers/nvme/target/fcloop.c|1021| <<fcloop_alloc_nport>> ret = fcloop_parse_options(opts, buf);
+	 */
 	ret = fcloop_parse_options(opts, buf);
 	if (ret)
 		goto out_free_opts;
@@ -923,6 +1018,28 @@ fcloop_create_local_port(struct device *dev, struct device_attribute *attr,
 	pinfo.port_role = opts->roles;
 	pinfo.port_id = opts->fcaddr;
 
+	/*
+	 * 在以下调用nvme_fc_register_localport():
+	 *   - drivers/nvme/target/fcloop.c|936| <<fcloop_create_local_port>> ret = nvme_fc_register_localport(&pinfo, &fctemplate, NULL, &localport);
+	 *   - drivers/scsi/lpfc/lpfc_nvme.c|2165| <<lpfc_nvme_create_localport>> ret = nvme_fc_register_localport(&nfcp_info, &lpfc_nvme_template,
+	 *   - drivers/scsi/qla2xxx/qla_nvme.c|706| <<qla_nvme_register_hba>> ret = nvme_fc_register_localport(&pinfo, tmpl,
+	 *
+	 * nvme_fc_register_localport - transport entry point called by an
+	 *                              LLDD to register the existence of a NVME
+	 *                              host FC port.
+	 * @pinfo:     pointer to information about the port to be registered
+	 * @template:  LLDD entrypoints and operational parameters for the port
+	 * @dev:       physical hardware device node port corresponds to. Will be
+	 *             used for DMA mappings
+	 * @portptr:   pointer to a local port pointer. Upon success, the routine
+	 *             will allocate a nvme_fc_local_port structure and place its
+	 *             address in the local port pointer. Upon failure, local port
+	 *             pointer will be set to 0.
+	 *
+	 * Returns:
+	 * a completion status. Must be 0 upon success; a negative errno
+	 * (ex: -ENXIO) upon failure.
+	 */
 	ret = nvme_fc_register_localport(&pinfo, &fctemplate, NULL, &localport);
 	if (!ret) {
 		/* success */
@@ -933,6 +1050,13 @@ fcloop_create_local_port(struct device *dev, struct device_attribute *attr,
 		INIT_LIST_HEAD(&lport->lport_list);
 
 		spin_lock_irqsave(&fcloop_lock, flags);
+		/*
+		 * 在以下使用fcloop_lports:
+		 *   - drivers/nvme/target/fcloop.c|998| <<fcloop_create_local_port>> list_add_tail(&lport->lport_list, &fcloop_lports);
+		 *   - drivers/nvme/target/fcloop.c|1051| <<fcloop_delete_local_port>> list_for_each_entry(tlport, &fcloop_lports, lport_list) {
+		 *   - drivers/nvme/target/fcloop.c|1113| <<fcloop_alloc_nport>> list_for_each_entry(tmplport, &fcloop_lports, lport_list) {
+		 *   - drivers/nvme/target/fcloop.c|1482| <<fcloop_exit>> lport = list_first_entry_or_null(&fcloop_lports,
+		 */
 		list_add_tail(&lport->lport_list, &fcloop_lports);
 		spin_unlock_irqrestore(&fcloop_lock, flags);
 	}
@@ -1004,6 +1128,11 @@ fcloop_delete_local_port(struct device *dev, struct device_attribute *attr,
 	return ret ? ret : count;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fcloop.c|1168| <<fcloop_create_remote_port>> nport = fcloop_alloc_nport(buf, count, true);
+ *   - drivers/nvme/target/fcloop.c|1267| <<fcloop_create_target_port>> nport = fcloop_alloc_nport(buf, count, false);
+ */
 static struct fcloop_nport *
 fcloop_alloc_nport(const char *buf, size_t count, bool remoteport)
 {
@@ -1011,6 +1140,12 @@ fcloop_alloc_nport(const char *buf, size_t count, bool remoteport)
 	struct fcloop_lport *tmplport, *lport = NULL;
 	struct fcloop_ctrl_options *opts;
 	unsigned long flags;
+	/*
+	 * #define RPORT_OPTS      (NVMF_OPT_WWNN | NVMF_OPT_WWPN |  \
+	 *			    NVMF_OPT_LPWWNN | NVMF_OPT_LPWWPN)
+	 *
+	 * #define TGTPORT_OPTS    (NVMF_OPT_WWNN | NVMF_OPT_WWPN)
+	 */
 	u32 opts_mask = (remoteport) ? RPORT_OPTS : TGTPORT_OPTS;
 	int ret;
 
@@ -1018,6 +1153,21 @@ fcloop_alloc_nport(const char *buf, size_t count, bool remoteport)
 	if (!opts)
 		return NULL;
 
+	/*
+	 * struct fcloop_ctrl_options {
+	 *	int                     mask;
+	 *	u64                     wwnn;
+	 *	u64                     wwpn;
+	 *	u32                     roles;
+	 *	u32                     fcaddr;
+	 *	u64                     lpwwnn;
+	 *	u64                     lpwwpn;
+	 * };
+	 *
+	 * 在以下调用fcloop_parse_options():
+	 *   - drivers/nvme/target/fcloop.c|910| <<fcloop_create_local_port>> ret = fcloop_parse_options(opts, buf);
+	 *   - drivers/nvme/target/fcloop.c|1021| <<fcloop_alloc_nport>> ret = fcloop_parse_options(opts, buf);
+	 */
 	ret = fcloop_parse_options(opts, buf);
 	if (ret)
 		goto out_free_opts;
@@ -1028,10 +1178,37 @@ fcloop_alloc_nport(const char *buf, size_t count, bool remoteport)
 		goto out_free_opts;
 	}
 
+	/*
+	 * struct fcloop_nport {
+	 *	struct fcloop_rport *rport;
+	 *	struct fcloop_tport *tport;
+	 *	struct fcloop_lport *lport;
+	 *	struct list_head nport_list;
+	 *	struct kref ref;
+	 *	u64 node_name;
+	 *	u64 port_name;
+	 *	u32 port_role;
+	 *	u32 port_id;
+	 * };
+	 *
+	 * struct fcloop_nport
+	 */
 	newnport = kzalloc(sizeof(*newnport), GFP_KERNEL);
 	if (!newnport)
 		goto out_free_opts;
 
+	/*
+	 * 在以下使用floop_nport->nport_list:
+	 *   - drivers/nvme/target/fcloop.c|845| <<fcloop_nport_free>> list_del(&nport->nport_list);
+	 *   - drivers/nvme/target/fcloop.c|1148| <<fcloop_alloc_nport>> INIT_LIST_HEAD(&newnport->nport_list);
+	 *   - drivers/nvme/target/fcloop.c|1192| <<fcloop_alloc_nport>> list_for_each_entry(nport, &fcloop_nports, nport_list) {
+	 *   - drivers/nvme/target/fcloop.c|1223| <<fcloop_alloc_nport>> list_add_tail(&newnport->nport_list, &fcloop_nports);
+	 *   - drivers/nvme/target/fcloop.c|1345| <<fcloop_delete_remote_port>> list_for_each_entry(tmpport, &fcloop_nports, nport_list) {
+	 *   - drivers/nvme/target/fcloop.c|1466| <<fcloop_delete_target_port>> list_for_each_entry(tmpport, &fcloop_nports, nport_list) {
+	 *   - drivers/nvme/target/fcloop.c|1558| <<fcloop_exit>> typeof(*nport), nport_list);
+	 *
+	 * struct fcloop_nport *newnport, *nport = NULL;
+	 */
 	INIT_LIST_HEAD(&newnport->nport_list);
 	newnport->node_name = opts->wwnn;
 	newnport->port_name = opts->wwpn;
@@ -1043,25 +1220,58 @@ fcloop_alloc_nport(const char *buf, size_t count, bool remoteport)
 
 	spin_lock_irqsave(&fcloop_lock, flags);
 
+	/*
+	 * 在以下使用fcloop_lports: 
+	 *   - drivers/nvme/target/fcloop.c|998| <<fcloop_create_local_port>> list_add_tail(&lport->lport_list, &fcloop_lports);
+	 *   - drivers/nvme/target/fcloop.c|1051| <<fcloop_delete_local_port>> list_for_each_entry(tlport, &fcloop_lports, lport_list) {
+	 *   - drivers/nvme/target/fcloop.c|1113| <<fcloop_alloc_nport>> list_for_each_entry(tmplport, &fcloop_lports, lport_list) {
+	 *   - drivers/nvme/target/fcloop.c|1482| <<fcloop_exit>> lport = list_first_entry_or_null(&fcloop_lports,
+	 *
+	 * struct fcloop_lport *tmplport
+	 */
 	list_for_each_entry(tmplport, &fcloop_lports, lport_list) {
 		if (tmplport->localport->node_name == opts->wwnn &&
 		    tmplport->localport->port_name == opts->wwpn)
 			goto out_invalid_opts;
 
+		/*
+		 * struct fcloop_lport *tmplport, *lport
+		 */
 		if (tmplport->localport->node_name == opts->lpwwnn &&
 		    tmplport->localport->port_name == opts->lpwwpn)
 			lport = tmplport;
 	}
 
+	/*
+	 * 函数的第三个参数(bool):
+	 *   - drivers/nvme/target/fcloop.c|1168| <<fcloop_create_remote_port>> nport = fcloop_alloc_nport(buf, count, true);
+	 *   - drivers/nvme/target/fcloop.c|1267| <<fcloop_create_target_port>> nport = fcloop_alloc_nport(buf, count, false);
+	 */
 	if (remoteport) {
 		if (!lport)
 			goto out_invalid_opts;
 		newnport->lport = lport;
 	}
 
+	/*
+	 * 在以下使用fcloop_nports:
+	 *   - drivers/nvme/target/fcloop.c|1129| <<fcloop_alloc_nport>> list_for_each_entry(nport, &fcloop_nports, nport_list)
+	 *   - drivers/nvme/target/fcloop.c|1152| <<fcloop_alloc_nport>> list_add_tail(&newnport->nport_list, &fcloop_nports);
+	 *   - drivers/nvme/target/fcloop.c|1274| <<fcloop_delete_remote_port>> list_for_each_entry(tmpport, &fcloop_nports, nport_list)
+	 *   - drivers/nvme/target/fcloop.c|1369| <<fcloop_delete_target_port>> list_for_each_entry(tmpport, &fcloop_nports, nport_list)
+	 *   - drivers/nvme/target/fcloop.c|1460| <<fcloop_exit>> nport = list_first_entry_or_null(&fcloop_nports,
+	 */
 	list_for_each_entry(nport, &fcloop_nports, nport_list) {
+		/*
+		 * 似乎remote和target的nport的wwnn和wwpn要一样???
+		 */
 		if (nport->node_name == opts->wwnn &&
 		    nport->port_name == opts->wwpn) {
+			/*
+			 * remote是函数的第三个参数(bool):
+			 *   - drivers/nvme/target/fcloop.c|1168| <<fcloop_create_remote_port>> nport = fcloop_alloc_nport(buf, count, true);
+			 *   - drivers/nvme/target/fcloop.c|1267| <<fcloop_create_target_port>> nport = fcloop_alloc_nport(buf, count, false);
+			 */
 			if ((remoteport && nport->rport) ||
 			    (!remoteport && nport->tport)) {
 				nport = NULL;
@@ -1072,21 +1282,40 @@ fcloop_alloc_nport(const char *buf, size_t count, bool remoteport)
 
 			spin_unlock_irqrestore(&fcloop_lock, flags);
 
+			/*
+			 * 函数的第三个参数(bool):
+			 *   - drivers/nvme/target/fcloop.c|1168| <<fcloop_create_remote_port>> nport = fcloop_alloc_nport(buf, count, true);
+			 *   - drivers/nvme/target/fcloop.c|1267| <<fcloop_create_target_port>> nport = fcloop_alloc_nport(buf, count, false);
+			 */
 			if (remoteport)
 				nport->lport = lport;
 			if (opts->mask & NVMF_OPT_ROLES)
 				nport->port_role = opts->roles;
 			if (opts->mask & NVMF_OPT_FCADDR)
 				nport->port_id = opts->fcaddr;
+			/*
+			 * goto out_free_newnport使用已有的nport, 而不是刚刚分配的newnport
+			 */
 			goto out_free_newnport;
 		}
 	}
 
+	/*
+	 * 在以下使用fcloop_nports:
+	 *   - drivers/nvme/target/fcloop.c|1129| <<fcloop_alloc_nport>> list_for_each_entry(nport, &fcloop_nports, nport_list) 
+	 *   - drivers/nvme/target/fcloop.c|1152| <<fcloop_alloc_nport>> list_add_tail(&newnport->nport_list, &fcloop_nports);
+	 *   - drivers/nvme/target/fcloop.c|1274| <<fcloop_delete_remote_port>> list_for_each_entry(tmpport, &fcloop_nports, nport_list) 
+	 *   - drivers/nvme/target/fcloop.c|1369| <<fcloop_delete_target_port>> list_for_each_entry(tmpport, &fcloop_nports, nport_list) 
+	 *   - drivers/nvme/target/fcloop.c|1460| <<fcloop_exit>> nport = list_first_entry_or_null(&fcloop_nports,
+	 *
+	 * struct fcloop_nport *newnport, *nport = NULL;
+	 */
 	list_add_tail(&newnport->nport_list, &fcloop_nports);
 
 	spin_unlock_irqrestore(&fcloop_lock, flags);
 
 	kfree(opts);
+	/* struct fcloop_nport *newnport, *nport = NULL; */
 	return newnport;
 
 out_invalid_opts:
@@ -1108,6 +1337,11 @@ fcloop_create_remote_port(struct device *dev, struct device_attribute *attr,
 	struct nvme_fc_port_info pinfo;
 	int ret;
 
+	/*
+	 * 在以下调用fcloop_alloc_nport():
+	 *   - drivers/nvme/target/fcloop.c|1168| <<fcloop_create_remote_port>> nport = fcloop_alloc_nport(buf, count, true);
+	 *   - drivers/nvme/target/fcloop.c|1267| <<fcloop_create_target_port>> nport = fcloop_alloc_nport(buf, count, false);
+	 */
 	nport = fcloop_alloc_nport(buf, count, true);
 	if (!nport)
 		return -EIO;
@@ -1118,6 +1352,27 @@ fcloop_create_remote_port(struct device *dev, struct device_attribute *attr,
 	pinfo.port_role = nport->port_role;
 	pinfo.port_id = nport->port_id;
 
+	/*
+	 * 在以下调用:
+	 *   - drivers/nvme/target/fcloop.c|1131| <<fcloop_create_remote_port>> ret = nvme_fc_register_remoteport(nport->lport->localport,
+	 *   - drivers/scsi/lpfc/lpfc_nvme.c|2400| <<lpfc_nvme_register_port>> ret = nvme_fc_register_remoteport(localport, &rpinfo, &remote_port);
+	 *   - drivers/scsi/qla2xxx/qla_nvme.c|63| <<qla_nvme_register_remote>> ret = nvme_fc_register_remoteport(vha->nvme_local_port, &req,
+	 *
+	 * nvme_fc_register_remoteport - transport entry point called by an
+	 *                              LLDD to register the existence of a NVME
+	 *                              subsystem FC port on its fabric.
+	 * @localport: pointer to the (registered) local port that the remote
+	 *             subsystem port is connected to.
+	 * @pinfo:     pointer to information about the port to be registered
+	 * @portptr:   pointer to a remote port pointer. Upon success, the routine
+	 *             will allocate a nvme_fc_remote_port structure and place its
+	 *             address in the remote port pointer. Upon failure, remote port
+	 *             pointer will be set to 0.
+	 *
+	 * Returns:
+	 * a completion status. Must be 0 upon success; a negative errno
+	 * (ex: -ENXIO) upon failure.
+	 */
 	ret = nvme_fc_register_remoteport(nport->lport->localport,
 						&pinfo, &remoteport);
 	if (ret || !remoteport) {
@@ -1207,6 +1462,11 @@ fcloop_create_target_port(struct device *dev, struct device_attribute *attr,
 	struct nvmet_fc_port_info tinfo;
 	int ret;
 
+	/*
+	 * 在以下调用fcloop_alloc_nport():
+	 *   - drivers/nvme/target/fcloop.c|1168| <<fcloop_create_remote_port>> nport = fcloop_alloc_nport(buf, count, true);
+	 *   - drivers/nvme/target/fcloop.c|1267| <<fcloop_create_target_port>> nport = fcloop_alloc_nport(buf, count, false);
+	 */
 	nport = fcloop_alloc_nport(buf, count, false);
 	if (!nport)
 		return -EIO;
@@ -1215,6 +1475,27 @@ fcloop_create_target_port(struct device *dev, struct device_attribute *attr,
 	tinfo.port_name = nport->port_name;
 	tinfo.port_id = nport->port_id;
 
+	/*
+	 * 在以下调用nvmet_fc_register_targetport():
+	 *   - drivers/nvme/target/fcloop.c|1228| <<fcloop_create_target_port>> ret = nvmet_fc_register_targetport(&tinfo, &tgttemplate, NULL,
+	 *   - drivers/scsi/lpfc/lpfc_nvmet.c|1469| <<lpfc_nvmet_create_targetport>> error = nvmet_fc_register_targetport(&pinfo, &lpfc_tgttemplate,
+	 *
+	 * nvme_fc_register_targetport - transport entry point called by an
+	 *                              LLDD to register the existence of a local
+	 *                              NVME subystem FC port.
+	 * @pinfo:     pointer to information about the port to be registered
+	 * @template:  LLDD entrypoints and operational parameters for the port
+	 * @dev:       physical hardware device node port corresponds to. Will be
+	 *             used for DMA mappings
+	 * @portptr:   pointer to a local port pointer. Upon success, the routine
+	 *             will allocate a nvme_fc_local_port structure and place its
+	 *             address in the local port pointer. Upon failure, local port
+	 *             pointer will be set to NULL.
+	 *
+	 * Returns:
+	 * a completion status. Must be 0 upon success; a negative errno
+	 * (ex: -ENXIO) upon failure.
+	 */
 	ret = nvmet_fc_register_targetport(&tinfo, &tgttemplate, NULL,
 						&targetport);
 	if (ret) {
@@ -1274,6 +1555,11 @@ fcloop_delete_target_port(struct device *dev, struct device_attribute *attr,
 	spin_lock_irqsave(&fcloop_lock, flags);
 
 	list_for_each_entry(tmpport, &fcloop_nports, nport_list) {
+		/*
+		 * struct fcloop_nport *tmpport:
+		 *   - u64 node_name;
+		 *   - u64 port_name;
+		 */
 		if (tmpport->node_name == nodename &&
 		    tmpport->port_name == portname && tmpport->tport) {
 			nport = tmpport;
diff --git a/drivers/nvme/target/io-cmd-file.c b/drivers/nvme/target/io-cmd-file.c
index caebfce06605..9dcf63360416 100644
--- a/drivers/nvme/target/io-cmd-file.c
+++ b/drivers/nvme/target/io-cmd-file.c
@@ -27,6 +27,10 @@ void nvmet_file_ns_disable(struct nvmet_ns *ns)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|530| <<nvmet_ns_enable>> ret = nvmet_file_ns_enable(ns);
+ */
 int nvmet_file_ns_enable(struct nvmet_ns *ns)
 {
 	int flags = O_RDWR | O_LARGEFILE;
@@ -98,6 +102,9 @@ static ssize_t nvmet_file_submit_bvec(struct nvmet_req *req, loff_t pos,
 	if (req->cmd->rw.opcode == nvme_cmd_write) {
 		if (req->cmd->rw.control & cpu_to_le16(NVME_RW_FUA))
 			ki_flags |= IOCB_DSYNC;
+		/*
+		 * 比如ext4_file_write_iter()
+		 */
 		call_iter = req->ns->file->f_op->write_iter;
 		rw = WRITE;
 	} else {
@@ -114,12 +121,25 @@ static ssize_t nvmet_file_submit_bvec(struct nvmet_req *req, loff_t pos,
 	return call_iter(iocb, &iter);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/io-cmd-file.c|195| <<nvmet_file_execute_io>> req->f.iocb.ki_complete = nvmet_file_io_done;
+ *   - drivers/nvme/target/io-cmd-file.c|218| <<nvmet_file_execute_io>> nvmet_file_io_done(&req->f.iocb, ret, 0);
+ */
 static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
 {
 	struct nvmet_req *req = container_of(iocb, struct nvmet_req, f.iocb);
 	u16 status = NVME_SC_SUCCESS;
 
 	if (req->f.bvec != req->inline_bvec) {
+		/*
+		 * 在以下使用mpool_alloc:
+		 *   - drivers/nvme/target/io-cmd-file.c|132| <<nvmet_file_io_done>> if (likely(req->f.mpool_alloc == false))
+		 *   - drivers/nvme/target/io-cmd-file.c|154| <<nvmet_file_execute_io>> if (req->f.mpool_alloc && nr_bvec > NVMET_MAX_MPOOL_BVEC)
+		 *   - drivers/nvme/target/io-cmd-file.c|265| <<nvmet_file_execute_rw>> req->f.mpool_alloc = true;
+		 *   - drivers/nvme/target/io-cmd-file.c|267| <<nvmet_file_execute_rw>> req->f.mpool_alloc = false;
+		 *   - drivers/nvme/target/io-cmd-file.c|270| <<nvmet_file_execute_rw>> if (likely(!req->f.mpool_alloc) &&
+		 */
 		if (likely(req->f.mpool_alloc == false))
 			kfree(req->f.bvec);
 		else
@@ -131,6 +151,12 @@ static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
 	nvmet_req_complete(req, status);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/io-cmd-file.c|231| <<nvmet_file_buffered_io_work>> nvmet_file_execute_io(req, 0);
+ *   - drivers/nvme/target/io-cmd-file.c|271| <<nvmet_file_execute_rw>> nvmet_file_execute_io(req, IOCB_NOWAIT))
+ *   - drivers/nvme/target/io-cmd-file.c|275| <<nvmet_file_execute_rw>> nvmet_file_execute_io(req, 0);
+ */
 static bool nvmet_file_execute_io(struct nvmet_req *req, int ki_flags)
 {
 	ssize_t nr_bvec = req->sg_cnt;
@@ -228,6 +254,10 @@ static void nvmet_file_submit_buffered_io(struct nvmet_req *req)
 	queue_work(buffered_io_wq, &req->f.work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/io-cmd-file.c|391| <<nvmet_file_parse_io_cmd>> req->execute = nvmet_file_execute_rw;
+ */
 static void nvmet_file_execute_rw(struct nvmet_req *req)
 {
 	ssize_t nr_bvec = req->sg_cnt;
@@ -240,6 +270,20 @@ static void nvmet_file_execute_rw(struct nvmet_req *req)
 		return;
 	}
 
+	/*
+	 * struct nvmet_req:
+	 *	union {
+	 *		struct {
+	 *			struct bio      inline_bio;
+	 *		} b;
+	 *		struct {
+	 *			bool                    mpool_alloc;
+	 *			struct kiocb            iocb;
+	 *			struct bio_vec          *bvec;
+	 *			struct work_struct      work;
+	 *		} f;
+	 *	};
+	 */
 	if (nr_bvec > NVMET_MAX_INLINE_BIOVEC)
 		req->f.bvec = kmalloc_array(nr_bvec, sizeof(struct bio_vec),
 				GFP_KERNEL);
@@ -249,6 +293,14 @@ static void nvmet_file_execute_rw(struct nvmet_req *req)
 	if (unlikely(!req->f.bvec)) {
 		/* fallback under memory pressure */
 		req->f.bvec = mempool_alloc(req->ns->bvec_pool, GFP_KERNEL);
+		/*
+		 * 在以下使用mpool_alloc:
+		 *   - drivers/nvme/target/io-cmd-file.c|132| <<nvmet_file_io_done>> if (likely(req->f.mpool_alloc == false))
+		 *   - drivers/nvme/target/io-cmd-file.c|154| <<nvmet_file_execute_io>> if (req->f.mpool_alloc && nr_bvec > NVMET_MAX_MPOOL_BVEC)
+		 *   - drivers/nvme/target/io-cmd-file.c|265| <<nvmet_file_execute_rw>> req->f.mpool_alloc = true;
+		 *   - drivers/nvme/target/io-cmd-file.c|267| <<nvmet_file_execute_rw>> req->f.mpool_alloc = false;
+		 *   - drivers/nvme/target/io-cmd-file.c|270| <<nvmet_file_execute_rw>> if (likely(!req->f.mpool_alloc) &&
+		 */
 		req->f.mpool_alloc = true;
 	} else
 		req->f.mpool_alloc = false;
@@ -372,6 +424,10 @@ static void nvmet_file_execute_write_zeroes(struct nvmet_req *req)
 	schedule_work(&req->f.work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|976| <<nvmet_parse_io_cmd>> return nvmet_file_parse_io_cmd(req);
+ */
 u16 nvmet_file_parse_io_cmd(struct nvmet_req *req)
 {
 	struct nvme_command *cmd = req->cmd;
diff --git a/drivers/nvme/target/loop.c b/drivers/nvme/target/loop.c
index 4df4ebde208a..42eafc3e766b 100644
--- a/drivers/nvme/target/loop.c
+++ b/drivers/nvme/target/loop.c
@@ -13,6 +13,10 @@
 #include "../host/nvme.h"
 #include "../host/fabrics.h"
 
+/*
+ * 在以下使用NVME_LOOP_MAX_SEGMENTS:
+ *   - drivers/nvme/target/loop.c|559| <<nvme_loop_configure_admin_queue>> (NVME_LOOP_MAX_SEGMENTS - 1) << (PAGE_SHIFT - 9);
+ */
 #define NVME_LOOP_MAX_SEGMENTS		256
 
 struct nvme_loop_iod {
@@ -40,12 +44,26 @@ struct nvme_loop_ctrl {
 	struct nvmet_port	*port;
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|254| <<nvme_loop_submit_async_event>> struct nvme_loop_ctrl *ctrl = to_loop_ctrl(arg);
+ *   - drivers/nvme/target/loop.c|373| <<nvme_loop_free_ctrl>> struct nvme_loop_ctrl *ctrl = to_loop_ctrl(nctrl);
+ *   - drivers/nvme/target/loop.c|590| <<nvme_loop_delete_ctrl_host>> nvme_loop_shutdown_ctrl(to_loop_ctrl(ctrl));
+ */
 static inline struct nvme_loop_ctrl *to_loop_ctrl(struct nvme_ctrl *ctrl)
 {
 	return container_of(ctrl, struct nvme_loop_ctrl, ctrl);
 }
 
 enum nvme_loop_queue_flags {
+	/*
+	 * 在以下使用NVME_LOOP_Q_LIVE:
+	 *   - drivers/nvme/target/loop.c|152| <<nvme_loop_queue_rq>> bool queue_ready = test_bit(NVME_LOOP_Q_LIVE, &queue->flags);
+	 *   - drivers/nvme/target/loop.c|290| <<nvme_loop_destroy_admin_queue>> clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
+	 *   - drivers/nvme/target/loop.c|323| <<nvme_loop_destroy_io_queues>> clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
+	 *   - drivers/nvme/target/loop.c|365| <<nvme_loop_connect_io_queues>> set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
+	 *   - drivers/nvme/target/loop.c|414| <<nvme_loop_configure_admin_queue>> set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
+	 */
 	NVME_LOOP_Q_LIVE	= 0,
 };
 
@@ -56,10 +74,32 @@ struct nvme_loop_queue {
 	unsigned long		flags;
 };
 
+/*
+ * 在以下使用nvme_loop_ports:
+ *   - drivers/nvme/target/loop.c|574| <<nvme_loop_find_port>> list_for_each_entry(p, &nvme_loop_ports, entry) {
+ *   - drivers/nvme/target/loop.c|684| <<nvme_loop_add_port>> list_add_tail(&port->entry, &nvme_loop_ports);
+ */
 static LIST_HEAD(nvme_loop_ports);
 static DEFINE_MUTEX(nvme_loop_ports_mutex);
 
+/*
+ * 在以下使用nvme_loop_ctrl_list:
+ *   - drivers/nvme/target/loop.c|501| <<nvme_loop_delete_ctrl>> list_for_each_entry(ctrl, &nvme_loop_ctrl_list, list) {
+ *   - drivers/nvme/target/loop.c|741| <<nvme_loop_create_ctrl>> list_add_tail(&ctrl->list, &nvme_loop_ctrl_list);
+ *   - drivers/nvme/target/loop.c|828| <<nvme_loop_cleanup_module>> list_for_each_entry_safe(ctrl, next, &nvme_loop_ctrl_list, list)
+ */
 static LIST_HEAD(nvme_loop_ctrl_list);
+/*
+ * 在以下使用nvme_loop_ctrl_mutex:
+ *   - drivers/nvme/target/loop.c|353| <<nvme_loop_free_ctrl>> mutex_lock(&nvme_loop_ctrl_mutex);
+ *   - drivers/nvme/target/loop.c|355| <<nvme_loop_free_ctrl>> mutex_unlock(&nvme_loop_ctrl_mutex);
+ *   - drivers/nvme/target/loop.c|568| <<nvme_loop_delete_ctrl>> mutex_lock(&nvme_loop_ctrl_mutex);
+ *   - drivers/nvme/target/loop.c|593| <<nvme_loop_delete_ctrl>> mutex_unlock(&nvme_loop_ctrl_mutex);
+ *   - drivers/nvme/target/loop.c|839| <<nvme_loop_create_ctrl>> mutex_lock(&nvme_loop_ctrl_mutex);
+ *   - drivers/nvme/target/loop.c|847| <<nvme_loop_create_ctrl>> mutex_unlock(&nvme_loop_ctrl_mutex);
+ *   - drivers/nvme/target/loop.c|951| <<nvme_loop_cleanup_module>> mutex_lock(&nvme_loop_ctrl_mutex);
+ *   - drivers/nvme/target/loop.c|960| <<nvme_loop_cleanup_module>> mutex_unlock(&nvme_loop_ctrl_mutex);
+ */
 static DEFINE_MUTEX(nvme_loop_ctrl_mutex);
 
 static void nvme_loop_queue_response(struct nvmet_req *nvme_req);
@@ -67,11 +107,26 @@ static void nvme_loop_delete_ctrl(struct nvmet_ctrl *ctrl);
 
 static const struct nvmet_fabrics_ops nvme_loop_ops;
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|119| <<nvme_loop_tagset>> u32 queue_idx = nvme_loop_queue_idx(queue);
+ *   - drivers/nvme/target/loop.c|141| <<nvme_loop_queue_response>> if (unlikely(nvme_is_aen_req(nvme_loop_queue_idx(queue),
+ *   - drivers/nvme/target/loop.c|155| <<nvme_loop_queue_response>> cqe->command_id, nvme_loop_queue_idx(queue));
+ */
 static inline int nvme_loop_queue_idx(struct nvme_loop_queue *queue)
 {
+	/*
+	 * struct nvme_loop_queue:
+	 *  -> struct nvme_loop_ctrl *ctrl;
+	 *      -> struct nvme_loop_queue *queues;
+	 */
 	return queue - queue->ctrl->queues;
 }
 
+/*
+ * struct blk_mq_ops nvme_loop_mq_ops.complete = nvme_loop_complete_rq()
+ * struct blk_mq_ops nvme_loop_admin_mq_ops.complete = nvme_loop_complete_rq()
+ */
 static void nvme_loop_complete_rq(struct request *req)
 {
 	struct nvme_loop_iod *iod = blk_mq_rq_to_pdu(req);
@@ -80,6 +135,10 @@ static void nvme_loop_complete_rq(struct request *req)
 	nvme_complete_rq(req);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|151| <<nvme_loop_queue_response>> rq = blk_mq_tag_to_rq(nvme_loop_tagset(queue), cqe->command_id);
+ */
 static struct blk_mq_tags *nvme_loop_tagset(struct nvme_loop_queue *queue)
 {
 	u32 queue_idx = nvme_loop_queue_idx(queue);
@@ -89,6 +148,14 @@ static struct blk_mq_tags *nvme_loop_tagset(struct nvme_loop_queue *queue)
 	return queue->ctrl->tag_set.tags[queue_idx - 1];
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|791| <<__nvmet_req_complete>> req->ops->queue_response(req);
+ *
+ * struct nvmet_fabrics_ops nvme_loop_ops.queue_response = nvme_loop_queue_response()
+ *
+ * 这个相当于interrupt handler了
+ */
 static void nvme_loop_queue_response(struct nvmet_req *req)
 {
 	struct nvme_loop_queue *queue =
@@ -103,11 +170,26 @@ static void nvme_loop_queue_response(struct nvmet_req *req)
 	 */
 	if (unlikely(nvme_is_aen_req(nvme_loop_queue_idx(queue),
 				     cqe->command_id))) {
+		/*
+		 * Async Event Notification
+		 *
+		 * called by:
+		 *   - drivers/nvme/host/fc.c|1693| <<nvme_fc_fcpio_done>> nvme_complete_async_event(&queue->ctrl->ctrl, status, &result);
+		 *   - drivers/nvme/host/pci.c|1112| <<nvme_handle_cqe>> nvme_complete_async_event(&nvmeq->dev->ctrl,
+		 *   - drivers/nvme/host/rdma.c|1504| <<nvme_rdma_recv_done>> nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+		 *   - drivers/nvme/host/tcp.c|496| <<nvme_tcp_handle_comp>> nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+		 *   - drivers/nvme/target/loop.c|106| <<nvme_loop_queue_response>> nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+		 *
+		 * The driver can handle tracking only one AEN request
+		 */
 		nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
 				&cqe->result);
 	} else {
 		struct request *rq;
 
+		/*
+		 * 核心思想就是返回tags->rqs[tag] (struct request类型)
+		 */
 		rq = blk_mq_tag_to_rq(nvme_loop_tagset(queue), cqe->command_id);
 		if (!rq) {
 			dev_err(queue->ctrl->ctrl.device,
@@ -116,37 +198,98 @@ static void nvme_loop_queue_response(struct nvmet_req *req)
 			return;
 		}
 
+		/*
+		 * called by:
+		 *   - drivers/nvme/host/fc.c|1702| <<nvme_fc_fcpio_done>> nvme_end_request(rq, status, result);
+		 *   - drivers/nvme/host/pci.c|1167| <<nvme_handle_cqe>> nvme_end_request(req, cqe->status, cqe->result);
+		 *   - drivers/nvme/host/rdma.c|1126| <<nvme_rdma_inv_rkey_done>> nvme_end_request(rq, req->status, req->result);
+		 *   - drivers/nvme/host/rdma.c|1335| <<nvme_rdma_send_done>> nvme_end_request(rq, req->status, req->result);
+		 *   - drivers/nvme/host/rdma.c|1478| <<nvme_rdma_process_nvme_rsp>> nvme_end_request(rq, req->status, req->result);
+		 *   - drivers/nvme/host/tcp.c|442| <<nvme_tcp_process_nvme_cqe>> nvme_end_request(rq, cqe->status, cqe->result);
+		 *   - drivers/nvme/host/tcp.c|632| <<nvme_tcp_end_request>> nvme_end_request(rq, cpu_to_le16(status << 1), res);
+		 *   - drivers/nvme/target/loop.c|119| <<nvme_loop_queue_response>> nvme_end_request(rq, cqe->status, cqe->result);
+		 */
 		nvme_end_request(rq, cqe->status, cqe->result);
 	}
 }
 
+/*
+ * 在以下使用nvme_loop_execute_work():
+ *   - drivers/nvme/target/loop.c|303| <<nvme_loop_init_iod>> INIT_WORK(&iod->work, nvme_loop_execute_work);
+ *
+ * 这里应该是在target了吧, 不是host
+ */
 static void nvme_loop_execute_work(struct work_struct *work)
 {
 	struct nvme_loop_iod *iod =
 		container_of(work, struct nvme_loop_iod, work);
 
+	/*
+	 * struct nvme_loop_iod:
+	 *  -> struct nvmet_req req;
+	 *
+	 * execute的一个例子是nvmet_file_execute_rw()
+	 *
+	 * 这里面要nvmet complete吧
+	 */
 	iod->req.execute(&iod->req);
 }
 
+/*
+ * struct blk_mq_ops nvme_loop_mq_ops.queue_rq = nvme_loop_queue_rq()
+ * struct blk_mq_ops nvme_loop_admin_mq_ops.queue_rq = nvme_loop_queue_rq()
+ */
 static blk_status_t nvme_loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 		const struct blk_mq_queue_data *bd)
 {
 	struct nvme_ns *ns = hctx->queue->queuedata;
 	struct nvme_loop_queue *queue = hctx->driver_data;
 	struct request *req = bd->rq;
+	/*
+	 * nvme_loop_iod在req后面, 所以跟req一起管理
+	 */
 	struct nvme_loop_iod *iod = blk_mq_rq_to_pdu(req);
+	/*
+	 * 在以下使用NVME_LOOP_Q_LIVE:
+	 *   - drivers/nvme/target/loop.c|152| <<nvme_loop_queue_rq>> bool queue_ready = test_bit(NVME_LOOP_Q_LIVE, &queue->flags);
+	 *   - drivers/nvme/target/loop.c|290| <<nvme_loop_destroy_admin_queue>> clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
+	 *   - drivers/nvme/target/loop.c|323| <<nvme_loop_destroy_io_queues>> clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
+	 *   - drivers/nvme/target/loop.c|365| <<nvme_loop_connect_io_queues>> set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
+	 *   - drivers/nvme/target/loop.c|414| <<nvme_loop_configure_admin_queue>> set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
+	 */
 	bool queue_ready = test_bit(NVME_LOOP_Q_LIVE, &queue->flags);
 	blk_status_t ret;
 
+	/*
+	 * 只要状态是NVME_CTRL_LIVE就一定返回true
+	 * 否则返回__nvmf_check_ready(ctrl, rq, queue_live)
+	 */
 	if (!nvmf_check_ready(&queue->ctrl->ctrl, req, queue_ready))
 		return nvmf_fail_nonready_command(&queue->ctrl->ctrl, req);
 
+	/*
+	 * struct nvme_loop_iod:
+	 *  -> struct nvme_command cmd;
+	 *  -> struct nvmet_req req;
+	 */
 	ret = nvme_setup_cmd(ns, req, &iod->cmd);
 	if (ret)
 		return ret;
 
+	/* 一个功能是设置MQ_RQ_IN_FLIGHT */
 	blk_mq_start_request(req);
 	iod->cmd.common.flags |= NVME_CMD_SGL_METABUF;
+	/*
+	 * struct nvme_loop_iod:
+	 *  - struct nvmet_req req;
+	 *     - struct nvmet_port *port;
+	 *
+	 * struct nvme_loop_queue:
+	 *  - struct nvme_loop_ctrl *ctrl;
+	 *     - struct nvmet_port *port;
+	 *
+	 * iod来自blk_mq_rq_to_pdu(req)
+	 */
 	iod->req.port = queue->ctrl->port;
 	if (!nvmet_req_init(&iod->req, &queue->nvme_cq,
 			&queue->nvme_sq, &nvme_loop_ops))
@@ -166,10 +309,19 @@ static blk_status_t nvme_loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 		iod->req.transfer_len = blk_rq_payload_bytes(req);
 	}
 
+	/*
+	 * 在nvme_loop_init_iod()初始化为:
+	 *  - INIT_WORK(&iod->work, nvme_loop_execute_work);
+	 *
+	 * iod来自blk_mq_rq_to_pdu(req)
+	 */
 	schedule_work(&iod->work);
 	return BLK_STS_OK;
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_loop_ctrl_ops.submit_async_event = nvme_loop_submit_async_event()
+ */
 static void nvme_loop_submit_async_event(struct nvme_ctrl *arg)
 {
 	struct nvme_loop_ctrl *ctrl = to_loop_ctrl(arg);
@@ -187,9 +339,18 @@ static void nvme_loop_submit_async_event(struct nvme_ctrl *arg)
 		return;
 	}
 
+	/*
+	 * 在nvme_loop_init_iod()初始化为:
+	 *  - INIT_WORK(&iod->work, nvme_loop_execute_work);
+	 */
 	schedule_work(&iod->work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|318| <<nvme_loop_init_request>> return nvme_loop_init_iod(ctrl, blk_mq_rq_to_pdu(req),
+ *   - drivers/nvme/target/loop.c|908| <<nvme_loop_create_ctrl>> nvme_loop_init_iod(ctrl, &ctrl->async_event_iod, 0);
+ */
 static int nvme_loop_init_iod(struct nvme_loop_ctrl *ctrl,
 		struct nvme_loop_iod *iod, unsigned int queue_idx)
 {
@@ -200,6 +361,10 @@ static int nvme_loop_init_iod(struct nvme_loop_ctrl *ctrl,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_loop_mq_ops.init_request = nvme_loop_init_request()
+ * struct blk_mq_ops nvme_loop_admin_mq_ops.init_request = nvme_loop_init_request()
+ */
 static int nvme_loop_init_request(struct blk_mq_tag_set *set,
 		struct request *req, unsigned int hctx_idx,
 		unsigned int numa_node)
@@ -211,6 +376,9 @@ static int nvme_loop_init_request(struct blk_mq_tag_set *set,
 			(set == &ctrl->tag_set) ? hctx_idx + 1 : 0);
 }
 
+/*
+ * struct blk_mq_ops nvme_loop_mq_ops.init_hctx = nvme_loop_init_hctx()
+ */
 static int nvme_loop_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -223,6 +391,9 @@ static int nvme_loop_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_loop_admin_mq_ops.init_hctx = nvme_loop_init_admin_hctx()
+ */
 static int nvme_loop_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -235,6 +406,10 @@ static int nvme_loop_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * 在以下使用nvme_loop_mq_ops:
+ *   - drivers/nvme/target/loop.c|799| <<nvme_loop_create_io_queues>> ctrl->tag_set.ops = &nvme_loop_mq_ops;
+ */
 static const struct blk_mq_ops nvme_loop_mq_ops = {
 	.queue_rq	= nvme_loop_queue_rq,
 	.complete	= nvme_loop_complete_rq,
@@ -242,6 +417,10 @@ static const struct blk_mq_ops nvme_loop_mq_ops = {
 	.init_hctx	= nvme_loop_init_hctx,
 };
 
+/*
+ * 在以下使用nvme_loop_admin_mq_ops:
+ *   - drivers/nvme/target/loop.c|514| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.ops = &nvme_loop_admin_mq_ops;
+ */
 static const struct blk_mq_ops nvme_loop_admin_mq_ops = {
 	.queue_rq	= nvme_loop_queue_rq,
 	.complete	= nvme_loop_complete_rq,
@@ -249,6 +428,12 @@ static const struct blk_mq_ops nvme_loop_admin_mq_ops = {
 	.init_hctx	= nvme_loop_init_admin_hctx,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|561| <<nvme_loop_shutdown_ctrl>> nvme_loop_destroy_admin_queue(ctrl);
+ *   - drivers/nvme/target/loop.c|721| <<nvme_loop_reset_ctrl_work>> nvme_loop_destroy_admin_queue(ctrl);
+ *   - drivers/nvme/target/loop.c|906| <<nvme_loop_create_ctrl>> nvme_loop_destroy_admin_queue(ctrl);
+ */
 static void nvme_loop_destroy_admin_queue(struct nvme_loop_ctrl *ctrl)
 {
 	clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
@@ -258,6 +443,23 @@ static void nvme_loop_destroy_admin_queue(struct nvme_loop_ctrl *ctrl)
 	blk_mq_free_tag_set(&ctrl->admin_tag_set);
 }
 
+/*
+ * [0] nvme_loop_free_ctrl
+ * [0] nvme_free_ctrl
+ * [0] device_release
+ * [0] kobject_put
+ * [0] nvme_sysfs_delete
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|5758| <<nvme_free_ctrl>> ctrl->ops->free_ctrl(ctrl);
+ *
+ * struct nvme_ctrl_ops nvme_loop_ctrl_ops.free_ctrl = nvme_loop_free_ctrl()
+ */
 static void nvme_loop_free_ctrl(struct nvme_ctrl *nctrl)
 {
 	struct nvme_loop_ctrl *ctrl = to_loop_ctrl(nctrl);
@@ -284,11 +486,29 @@ static void nvme_loop_destroy_io_queues(struct nvme_loop_ctrl *ctrl)
 	int i;
 
 	for (i = 1; i < ctrl->ctrl.queue_count; i++) {
+		/*
+		 * 在以下使用NVME_LOOP_Q_LIVE:
+		 *   - drivers/nvme/target/loop.c|152| <<nvme_loop_queue_rq>> bool queue_ready = test_bit(NVME_LOOP_Q_LIVE, &queue->flags);
+		 *   - drivers/nvme/target/loop.c|290| <<nvme_loop_destroy_admin_queue>> clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
+		 *   - drivers/nvme/target/loop.c|323| <<nvme_loop_destroy_io_queues>> clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
+		 *   - drivers/nvme/target/loop.c|365| <<nvme_loop_connect_io_queues>> set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
+		 *   - drivers/nvme/target/loop.c|414| <<nvme_loop_configure_admin_queue>> set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
+		 */
 		clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
 		nvmet_sq_destroy(&ctrl->queues[i].nvme_sq);
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|597| <<nvme_loop_reset_ctrl_work>> ret = nvme_loop_init_io_queues(ctrl);
+ *   - drivers/nvme/target/loop.c|642| <<nvme_loop_create_io_queues>> ret = nvme_loop_init_io_queues(ctrl);
+ *
+ * 调用的一个例子:
+ * nvme_loop_create_ctrl()
+ *  -> nvme_loop_create_io_queues()
+ *      -> nvme_loop_init_io_queues()
+ */
 static int nvme_loop_init_io_queues(struct nvme_loop_ctrl *ctrl)
 {
 	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
@@ -303,6 +523,12 @@ static int nvme_loop_init_io_queues(struct nvme_loop_ctrl *ctrl)
 	dev_info(ctrl->ctrl.device, "creating %d I/O queues.\n", nr_io_queues);
 
 	for (i = 1; i <= nr_io_queues; i++) {
+		/*
+		 * struct nvme_loop_ctrl:
+		 *  - struct nvme_loop_queue *queues;
+		 *     - struct nvmet_sq nvme_sq;
+		 *  - struct nvme_ctrl ctrl;
+		 */
 		ctrl->queues[i].ctrl = ctrl;
 		ret = nvmet_sq_init(&ctrl->queues[i].nvme_sq);
 		if (ret)
@@ -318,20 +544,45 @@ static int nvme_loop_init_io_queues(struct nvme_loop_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|731| <<nvme_loop_reset_ctrl_work>> ret = nvme_loop_connect_io_queues(ctrl);
+ *   - drivers/nvme/target/loop.c|803| <<nvme_loop_create_io_queues>> ret = nvme_loop_connect_io_queues(ctrl);
+ */
 static int nvme_loop_connect_io_queues(struct nvme_loop_ctrl *ctrl)
 {
 	int i, ret;
 
 	for (i = 1; i < ctrl->ctrl.queue_count; i++) {
+		/*
+		 * 在以下调用nvmf_connect_io_queue():
+		 *   - drivers/nvme/host/fc.c|2069| <<nvme_fc_connect_io_queues>> ret = nvmf_connect_io_queue(&ctrl->ctrl, i, false);
+		 *   - drivers/nvme/host/rdma.c|616| <<nvme_rdma_start_queue>> ret = nvmf_connect_io_queue(&ctrl->ctrl, idx, poll);
+		 *   - drivers/nvme/host/tcp.c|1445| <<nvme_tcp_start_queue>> ret = nvmf_connect_io_queue(nctrl, idx, false);
+		 *   - drivers/nvme/target/loop.c|507| <<nvme_loop_connect_io_queues>> ret = nvmf_connect_io_queue(&ctrl->ctrl, i, false);
+		 */
 		ret = nvmf_connect_io_queue(&ctrl->ctrl, i, false);
 		if (ret)
 			return ret;
+		/*
+		 * 在以下使用NVME_LOOP_Q_LIVE:
+		 *   - drivers/nvme/target/loop.c|152| <<nvme_loop_queue_rq>> bool queue_ready = test_bit(NVME_LOOP_Q_LIVE, &queue->flags);
+		 *   - drivers/nvme/target/loop.c|290| <<nvme_loop_destroy_admin_queue>> clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
+		 *   - drivers/nvme/target/loop.c|323| <<nvme_loop_destroy_io_queues>> clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
+		 *   - drivers/nvme/target/loop.c|365| <<nvme_loop_connect_io_queues>> set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
+		 *   - drivers/nvme/target/loop.c|414| <<nvme_loop_configure_admin_queue>> set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
+		 */
 		set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
 	}
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|582| <<nvme_loop_reset_ctrl_work>> ret = nvme_loop_configure_admin_queue(ctrl);
+ *   - drivers/nvme/target/loop.c|745| <<nvme_loop_create_ctrl>> ret = nvme_loop_configure_admin_queue(ctrl);
+ */
 static int nvme_loop_configure_admin_queue(struct nvme_loop_ctrl *ctrl)
 {
 	int error;
@@ -359,6 +610,12 @@ static int nvme_loop_configure_admin_queue(struct nvme_loop_ctrl *ctrl)
 		goto out_free_sq;
 	ctrl->ctrl.admin_tagset = &ctrl->admin_tag_set;
 
+	/*
+	 * 在loop的三处被调用:
+	 *   - drivers/nvme/target/loop.c|362| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+	 *   - drivers/nvme/target/loop.c|368| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+	 *   - drivers/nvme/target/loop.c|529| <<nvme_loop_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+	 */
 	ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
 	if (IS_ERR(ctrl->ctrl.fabrics_q)) {
 		error = PTR_ERR(ctrl->ctrl.fabrics_q);
@@ -375,6 +632,14 @@ static int nvme_loop_configure_admin_queue(struct nvme_loop_ctrl *ctrl)
 	if (error)
 		goto out_cleanup_queue;
 
+	/*
+	 * 在以下使用NVME_LOOP_Q_LIVE:
+	 *   - drivers/nvme/target/loop.c|152| <<nvme_loop_queue_rq>> bool queue_ready = test_bit(NVME_LOOP_Q_LIVE, &queue->flags);
+	 *   - drivers/nvme/target/loop.c|290| <<nvme_loop_destroy_admin_queue>> clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
+	 *   - drivers/nvme/target/loop.c|323| <<nvme_loop_destroy_io_queues>> clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
+	 *   - drivers/nvme/target/loop.c|365| <<nvme_loop_connect_io_queues>> set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);
+	 *   - drivers/nvme/target/loop.c|414| <<nvme_loop_configure_admin_queue>> set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
+	 */
 	set_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);
 
 	error = nvme_enable_ctrl(&ctrl->ctrl);
@@ -403,12 +668,33 @@ static int nvme_loop_configure_admin_queue(struct nvme_loop_ctrl *ctrl)
 	return error;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|461| <<nvme_loop_delete_ctrl_host>> nvme_loop_shutdown_ctrl(to_loop_ctrl(ctrl));
+ *   - drivers/nvme/target/loop.c|484| <<nvme_loop_reset_ctrl_work>> nvme_loop_shutdown_ctrl(ctrl);
+ */
 static void nvme_loop_shutdown_ctrl(struct nvme_loop_ctrl *ctrl)
 {
 	if (ctrl->ctrl.queue_count > 1) {
 		nvme_stop_queues(&ctrl->ctrl);
+		/*
+		 * !!!!!!!!!
+		 * 关于bt_tags_iter():
+		 * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+		 * 才调用iter_data->fn()!!!
+		 * 是针对started request的
+		 * !!!!!!!!!
+		 */
 		blk_mq_tagset_busy_iter(&ctrl->tag_set,
 					nvme_cancel_request, &ctrl->ctrl);
+		/*
+		 * !!!!!!!!!
+		 * 关于bt_tags_iter():
+		 * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+		 * 才调用iter_data->fn()!!!
+		 * 是针对started request的
+		 * !!!!!!!!!
+		 */
 		blk_mq_tagset_wait_completed_request(&ctrl->tag_set);
 		nvme_loop_destroy_io_queues(ctrl);
 	}
@@ -417,29 +703,154 @@ static void nvme_loop_shutdown_ctrl(struct nvme_loop_ctrl *ctrl)
 	if (ctrl->ctrl.state == NVME_CTRL_LIVE)
 		nvme_shutdown_ctrl(&ctrl->ctrl);
 
+	/*
+	 * !!!!!!!!!
+	 * 关于bt_tags_iter():
+	 * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+	 * 才调用iter_data->fn()!!!
+	 * 是针对started request的
+	 * !!!!!!!!!
+	 */
 	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
 				nvme_cancel_request, &ctrl->ctrl);
+	/*
+	 * !!!!!!!!!
+	 * 关于bt_tags_iter():
+	 * 只有不是MQ_RQ_IDLE (也就是必须是MQ_RQ_INFLIGHT或者MQ_RQ_COMPLETE)的情况下
+	 * 才调用iter_data->fn()!!!
+	 * 是针对started request的
+	 * !!!!!!!!!
+	 */
 	blk_mq_tagset_wait_completed_request(&ctrl->admin_tag_set);
 	nvme_loop_destroy_admin_queue(ctrl);
 }
 
+/*
+ * [0] nvme_loop_delete_ctrl_host
+ * [0] nvme_do_delete_ctrl
+ * [0] nvme_sysfs_delete.cold.97
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] nvme_loop_delete_ctrl_host
+ * [0] nvme_do_delete_ctrl
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * struct nvme_ctrl_ops nvme_loop_ctrl_ops.delete_ctrl = nvme_loop_delete_ctrl_host()
+ */
 static void nvme_loop_delete_ctrl_host(struct nvme_ctrl *ctrl)
 {
+	/*
+	 * called by:
+	 *   - drivers/nvme/target/loop.c|461| <<nvme_loop_delete_ctrl_host>> nvme_loop_shutdown_ctrl(to_loop_ctrl(ctrl));
+	 *   - drivers/nvme/target/loop.c|484| <<nvme_loop_reset_ctrl_work>> nvme_loop_shutdown_ctrl(ctrl);
+	 */
 	nvme_loop_shutdown_ctrl(to_loop_ctrl(ctrl));
 }
 
+/*
+ * [0] nvme_loop_delete_ctrl
+ * [0] nvmet_port_del_ctrls
+ * [0] nvmet_port_subsys_drop_link
+ * [0] configfs_unlink
+ * [0] vfs_unlink
+ * [0] do_unlinkat
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * struct nvmet_fabrics_ops nvme_loop_ops.delete_ctrl = nvme_loop_delete_ctrl()
+ */
 static void nvme_loop_delete_ctrl(struct nvmet_ctrl *nctrl)
 {
 	struct nvme_loop_ctrl *ctrl;
 
+	/*
+	 * 在以下使用nvme_loop_ctrl_mutex:
+	 *   - drivers/nvme/target/loop.c|353| <<nvme_loop_free_ctrl>> mutex_lock(&nvme_loop_ctrl_mutex);
+	 *   - drivers/nvme/target/loop.c|355| <<nvme_loop_free_ctrl>> mutex_unlock(&nvme_loop_ctrl_mutex);
+	 *   - drivers/nvme/target/loop.c|568| <<nvme_loop_delete_ctrl>> mutex_lock(&nvme_loop_ctrl_mutex);
+	 *   - drivers/nvme/target/loop.c|593| <<nvme_loop_delete_ctrl>> mutex_unlock(&nvme_loop_ctrl_mutex);
+	 *   - drivers/nvme/target/loop.c|839| <<nvme_loop_create_ctrl>> mutex_lock(&nvme_loop_ctrl_mutex);
+	 *   - drivers/nvme/target/loop.c|847| <<nvme_loop_create_ctrl>> mutex_unlock(&nvme_loop_ctrl_mutex);
+	 *   - drivers/nvme/target/loop.c|951| <<nvme_loop_cleanup_module>> mutex_lock(&nvme_loop_ctrl_mutex);
+	 *   - drivers/nvme/target/loop.c|960| <<nvme_loop_cleanup_module>> mutex_unlock(&nvme_loop_ctrl_mutex);
+	 */
 	mutex_lock(&nvme_loop_ctrl_mutex);
+	/*
+	 * 在以下使用nvme_loop_ctrl_list:
+	 *   - drivers/nvme/target/loop.c|501| <<nvme_loop_delete_ctrl>> list_for_each_entry(ctrl, &nvme_loop_ctrl_list, list) {
+	 *   - drivers/nvme/target/loop.c|741| <<nvme_loop_create_ctrl>> list_add_tail(&ctrl->list, &nvme_loop_ctrl_list);
+	 *   - drivers/nvme/target/loop.c|828| <<nvme_loop_cleanup_module>> list_for_each_entry_safe(ctrl, next, &nvme_loop_ctrl_list, list)
+	 */
 	list_for_each_entry(ctrl, &nvme_loop_ctrl_list, list) {
+		/*
+		 * 调用者没有pci:
+		 *   - drivers/nvme/host/fc.c|780| <<nvme_fc_ctrl_connectivity_loss>> nvme_delete_ctrl(&ctrl->ctrl);
+		 *   - drivers/nvme/host/fc.c|847| <<nvme_fc_unregister_remoteport>> nvme_delete_ctrl(&ctrl->ctrl);
+		 *   - drivers/nvme/host/fc.c|2909| <<nvme_fc_reconnect_or_delete>> WARN_ON(nvme_delete_ctrl(&ctrl->ctrl));
+		 *   - drivers/nvme/host/fc.c|3521| <<nvme_fc_delete_controllers>> nvme_delete_ctrl(&ctrl->ctrl);
+		 *   - drivers/nvme/host/rdma.c|977| <<nvme_rdma_reconnect_or_remove>> nvme_delete_ctrl(&ctrl->ctrl);
+		 *   - drivers/nvme/host/rdma.c|2101| <<nvme_rdma_remove_one>> nvme_delete_ctrl(&ctrl->ctrl);
+		 *   - drivers/nvme/host/rdma.c|2141| <<nvme_rdma_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+		 *   - drivers/nvme/host/tcp.c|1822| <<nvme_tcp_reconnect_or_remove>> nvme_delete_ctrl(ctrl);
+		 *   - drivers/nvme/host/tcp.c|2409| <<nvme_tcp_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+		 *   - drivers/nvme/target/loop.c|438| <<nvme_loop_delete_ctrl>> nvme_delete_ctrl(&ctrl->ctrl);
+		 *   - drivers/nvme/target/loop.c|713| <<nvme_loop_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+		 */
 		if (ctrl->ctrl.cntlid == nctrl->cntlid)
 			nvme_delete_ctrl(&ctrl->ctrl);
 	}
 	mutex_unlock(&nvme_loop_ctrl_mutex);
 }
 
+/*
+ * loop启动没调用reset work!
+ * [0] nvme_reset_ctrl
+ * [0] nvme_reset_ctrl_sync
+ * [0] nvme_sysfs_reset
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用reset_work:
+ *   - drivers/nvme/host/core.c|133| <<nvme_try_sched_reset>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+ *   - drivers/nvme/host/core.c|143| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+ *   - drivers/nvme/host/core.c|155| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/core.c|169| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/fc.c|2944| <<nvme_fc_reset_ctrl_work>> container_of(work, struct nvme_fc_ctrl, ctrl.reset_work);
+ *   - drivers/nvme/host/fc.c|3107| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+ *   - drivers/nvme/host/fc.c|3204| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|2572| <<nvme_reset_work>> container_of(work, struct nvme_dev, ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|2798| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|2830| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+ *   - drivers/nvme/host/pci.c|2902| <<nvme_reset_done>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|2929| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|3094| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/rdma.c|1906| <<nvme_rdma_reset_ctrl_work>> container_of(work, struct nvme_rdma_ctrl, ctrl.reset_work);
+ *   - drivers/nvme/host/rdma.c|2018| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+ *   - drivers/nvme/host/tcp.c|1945| <<nvme_reset_ctrl_work>> container_of(work, struct nvme_ctrl, reset_work);
+ *   - drivers/nvme/host/tcp.c|2298| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+ *   - drivers/nvme/target/loop.c|446| <<nvme_loop_reset_ctrl_work>> container_of(work, struct nvme_loop_ctrl, ctrl.reset_work);
+ *   - drivers/nvme/target/loop.c|580| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+ *
+ * 在以下使用nvme_loop_reset_ctrl_work():
+ *   - drivers/nvme/target/loop.c|637| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+ *
+ * 调用的一个例子:
+ * nvme_sysfs_reset()
+ *  -> nvme_reset_ctrl_sync()
+ *      -> nvme_reset_ctrl()
+ *          -> nvme_loop_reset_ctrl_work() or nvme_reset_work()
+ *      -> flush_work()
+ */
 static void nvme_loop_reset_ctrl_work(struct work_struct *work)
 {
 	struct nvme_loop_ctrl *ctrl =
@@ -447,6 +858,15 @@ static void nvme_loop_reset_ctrl_work(struct work_struct *work)
 	bool changed;
 	int ret;
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/core.c|235| <<nvme_do_delete_ctrl>> nvme_stop_ctrl(ctrl);
+	 *   - drivers/nvme/host/fc.c|2949| <<nvme_fc_reset_ctrl_work>> nvme_stop_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/pci.c|3569| <<nvme_remove>> nvme_stop_ctrl(&dev->ctrl);
+	 *   - drivers/nvme/host/rdma.c|1908| <<nvme_rdma_reset_ctrl_work>> nvme_stop_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/tcp.c|1947| <<nvme_reset_ctrl_work>> nvme_stop_ctrl(ctrl);
+	 *   - drivers/nvme/target/loop.c|450| <<nvme_loop_reset_ctrl_work>> nvme_stop_ctrl(&ctrl->ctrl);
+	 */
 	nvme_stop_ctrl(&ctrl->ctrl);
 	nvme_loop_shutdown_ctrl(ctrl);
 
@@ -488,6 +908,10 @@ static void nvme_loop_reset_ctrl_work(struct work_struct *work)
 	nvme_put_ctrl(&ctrl->ctrl);
 }
 
+/*
+ * 在以下使用nvme_loop_ctrl_ops:
+ *   - drivers/nvme/target/loop.c|874| <<nvme_loop_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_loop_ctrl_ops,
+ */
 static const struct nvme_ctrl_ops nvme_loop_ctrl_ops = {
 	.name			= "loop",
 	.module			= THIS_MODULE,
@@ -501,6 +925,10 @@ static const struct nvme_ctrl_ops nvme_loop_ctrl_ops = {
 	.get_address		= nvmf_get_address,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|769| <<nvme_loop_create_ctrl>> ret = nvme_loop_create_io_queues(ctrl);
+ */
 static int nvme_loop_create_io_queues(struct nvme_loop_ctrl *ctrl)
 {
 	int ret;
@@ -526,6 +954,14 @@ static int nvme_loop_create_io_queues(struct nvme_loop_ctrl *ctrl)
 	if (ret)
 		goto out_destroy_queues;
 
+	/*
+	 * loop在以下使用connect_q:
+	 *   - drivers/nvme/host/fabrics.c|462| <<nvmf_connect_io_queue>> ret = __nvme_submit_sync_cmd(ctrl->connect_q, &cmd, &res,
+	 *   - drivers/nvme/target/loop.c|827| <<nvme_loop_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+	 *   - drivers/nvme/target/loop.c|828| <<nvme_loop_create_io_queues>> if (IS_ERR(ctrl->ctrl.connect_q)) {
+	 *   - drivers/nvme/target/loop.c|829| <<nvme_loop_create_io_queues>> ret = PTR_ERR(ctrl->ctrl.connect_q);
+	 *   - drivers/nvme/target/loop.c|840| <<nvme_loop_create_io_queues>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+	 */
 	ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
 	if (IS_ERR(ctrl->ctrl.connect_q)) {
 		ret = PTR_ERR(ctrl->ctrl.connect_q);
@@ -547,11 +983,20 @@ static int nvme_loop_create_io_queues(struct nvme_loop_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|625| <<nvme_loop_create_ctrl>> ctrl->port = nvme_loop_find_port(&ctrl->ctrl);
+ */
 static struct nvmet_port *nvme_loop_find_port(struct nvme_ctrl *ctrl)
 {
 	struct nvmet_port *p, *found = NULL;
 
 	mutex_lock(&nvme_loop_ports_mutex);
+	/*
+	 * 在以下使用nvme_loop_ports:
+	 *   - drivers/nvme/target/loop.c|574| <<nvme_loop_find_port>> list_for_each_entry(p, &nvme_loop_ports, entry) {
+	 *   - drivers/nvme/target/loop.c|684| <<nvme_loop_add_port>> list_add_tail(&port->entry, &nvme_loop_ports);
+	 */
 	list_for_each_entry(p, &nvme_loop_ports, entry) {
 		/* if no transport address is specified use the first port */
 		if ((ctrl->opts->mask & NVMF_OPT_TRADDR) &&
@@ -564,9 +1009,25 @@ static struct nvmet_port *nvme_loop_find_port(struct nvme_ctrl *ctrl)
 	return found;
 }
 
+/*
+ * [0] nvme_init_ctrl
+ * [0] nvme_loop_create_ctrl
+ * [0] nvmf_dev_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * struct nvmf_transport_ops nvme_loop_transport.create_ctrl = nvme_loop_create_ctrl()
+ */
 static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 		struct nvmf_ctrl_options *opts)
 {
+	/*
+	 * struct nvme_loop_ctrl:
+	 *  -> struct nvme_ctrl ctrl;
+	 *      -> struct nvmf_ctrl_options *opts;
+	 */
 	struct nvme_loop_ctrl *ctrl;
 	bool changed;
 	int ret;
@@ -579,6 +1040,13 @@ static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 
 	INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
 
+	/*
+	 * 每个nvme pci device (func) 是一个controller.
+	 * 可能很多个controller输入同一个subsystem. 但是pci的入口都是从probe controller开始的.
+	 * subsystem的数据结构的分配是初始化每一个controller的时候on demand的.
+	 * 如果两个controller属于同一个subsystem, 那初始化第二个controller的时候就不会再分配subsystem的结构了.
+	 * 每一个nvme0或者nvme1都是一个controller.
+	 */
 	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_loop_ctrl_ops,
 				0 /* no quirks, we're perfect! */);
 	if (ret)
@@ -588,6 +1056,10 @@ static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 
 	ctrl->ctrl.sqsize = opts->queue_size - 1;
 	ctrl->ctrl.kato = opts->kato;
+	/*
+	 * struct nvme_loop_ctrl:
+	 *  -> struct nvmet_port *port;
+	 */
 	ctrl->port = nvme_loop_find_port(&ctrl->ctrl);
 
 	ctrl->queues = kcalloc(opts->nr_io_queues + 1, sizeof(*ctrl->queues),
@@ -624,9 +1096,18 @@ static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 	WARN_ON_ONCE(!changed);
 
 	mutex_lock(&nvme_loop_ctrl_mutex);
+	/*
+	 * 在以下使用nvme_loop_ctrl_list:
+	 *   - drivers/nvme/target/loop.c|501| <<nvme_loop_delete_ctrl>> list_for_each_entry(ctrl, &nvme_loop_ctrl_list, list) {
+	 *   - drivers/nvme/target/loop.c|741| <<nvme_loop_create_ctrl>> list_add_tail(&ctrl->list, &nvme_loop_ctrl_list);
+	 *   - drivers/nvme/target/loop.c|828| <<nvme_loop_cleanup_module>> list_for_each_entry_safe(ctrl, next, &nvme_loop_ctrl_list, list)
+	 */
 	list_add_tail(&ctrl->list, &nvme_loop_ctrl_list);
 	mutex_unlock(&nvme_loop_ctrl_mutex);
 
+	/*
+	 * 这个函数会触发ns的scan: queue_work(nvme_wq, &ctrl->scan_work);
+	 */
 	nvme_start_ctrl(&ctrl->ctrl);
 
 	return &ctrl->ctrl;
@@ -644,14 +1125,28 @@ static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 	return ERR_PTR(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|372| <<nvmet_enable_port>> ret = ops->add_port(port);
+ *
+ * struct nvmet_fabrics_ops nvme_loop_ops.add_port = nvme_loop_add_port()
+ */
 static int nvme_loop_add_port(struct nvmet_port *port)
 {
 	mutex_lock(&nvme_loop_ports_mutex);
+	/*
+	 * 在以下使用nvme_loop_ports:
+	 *   - drivers/nvme/target/loop.c|574| <<nvme_loop_find_port>> list_for_each_entry(p, &nvme_loop_ports, entry) {
+	 *   - drivers/nvme/target/loop.c|684| <<nvme_loop_add_port>> list_add_tail(&port->entry, &nvme_loop_ports);
+	 */
 	list_add_tail(&port->entry, &nvme_loop_ports);
 	mutex_unlock(&nvme_loop_ports_mutex);
 	return 0;
 }
 
+/*
+ * struct nvmet_fabrics_ops nvme_loop_ops.remove_port = nvme_loop_remove_port()
+ */
 static void nvme_loop_remove_port(struct nvmet_port *port)
 {
 	mutex_lock(&nvme_loop_ports_mutex);
@@ -664,9 +1159,23 @@ static void nvme_loop_remove_port(struct nvmet_port *port)
 	 * and free the port. This is to prevent active
 	 * ctrls from using a port after it's freed.
 	 */
+	/*
+	 * 在以下使用delete_work:
+	 *   - drivers/nvme/host/core.c|362| <<nvme_delete_ctrl_work>> container_of(work, struct nvme_ctrl, delete_work);
+	 *   - drivers/nvme/host/core.c|387| <<nvme_delete_ctrl>> if (!queue_work(nvme_delete_wq, &ctrl->delete_work))
+	 *   - drivers/nvme/host/core.c|5801| <<nvme_init_ctrl>> INIT_WORK(&ctrl->delete_work, nvme_delete_ctrl_work);
+	 */
 	flush_workqueue(nvme_delete_wq);
 }
 
+/*
+ * 在以下使用nvme_loop_ops:
+ *   - drivers/nvme/target/loop.c|196| <<nvme_loop_queue_rq>> &queue->nvme_sq, &nvme_loop_ops))
+ *   - drivers/nvme/target/loop.c|233| <<nvme_loop_submit_async_event>> &nvme_loop_ops)) {
+ *   - drivers/nvme/target/loop.c|809| <<nvme_loop_init_module>> ret = nvmet_register_transport(&nvme_loop_ops);
+ *   - drivers/nvme/target/loop.c|815| <<nvme_loop_init_module>> nvmet_unregister_transport(&nvme_loop_ops);
+ *   - drivers/nvme/target/loop.c|825| <<nvme_loop_cleanup_module>> nvmet_unregister_transport(&nvme_loop_ops);
+ */
 static const struct nvmet_fabrics_ops nvme_loop_ops = {
 	.owner		= THIS_MODULE,
 	.type		= NVMF_TRTYPE_LOOP,
@@ -676,6 +1185,11 @@ static const struct nvmet_fabrics_ops nvme_loop_ops = {
 	.delete_ctrl	= nvme_loop_delete_ctrl,
 };
 
+/*
+ * 在以下使用nvme_loop_transport:
+ *   - drivers/nvme/target/loop.c|813| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+ *   - drivers/nvme/target/loop.c|824| <<nvme_loop_cleanup_module>> nvmf_unregister_transport(&nvme_loop_transport);
+ */
 static struct nvmf_transport_ops nvme_loop_transport = {
 	.name		= "loop",
 	.module		= THIS_MODULE,
@@ -706,10 +1220,28 @@ static void __exit nvme_loop_cleanup_module(void)
 	nvmet_unregister_transport(&nvme_loop_ops);
 
 	mutex_lock(&nvme_loop_ctrl_mutex);
+	/*
+	 * 在以下使用nvme_loop_ctrl_list:
+	 *   - drivers/nvme/target/loop.c|501| <<nvme_loop_delete_ctrl>> list_for_each_entry(ctrl, &nvme_loop_ctrl_list, list) {
+	 *   - drivers/nvme/target/loop.c|741| <<nvme_loop_create_ctrl>> list_add_tail(&ctrl->list, &nvme_loop_ctrl_list);
+	 *   - drivers/nvme/target/loop.c|828| <<nvme_loop_cleanup_module>> list_for_each_entry_safe(ctrl, next, &nvme_loop_ctrl_list, list)
+	 *
+	 * 在以下调用nvme_delete_ctrl():
+	 *   - drivers/nvme/target/loop.c|438| <<nvme_loop_delete_ctrl>> nvme_delete_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/target/loop.c|713| <<nvme_loop_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+	 */
 	list_for_each_entry_safe(ctrl, next, &nvme_loop_ctrl_list, list)
 		nvme_delete_ctrl(&ctrl->ctrl);
 	mutex_unlock(&nvme_loop_ctrl_mutex);
 
+	/*
+	 * 在以下使用delete_worl:
+	 *   - drivers/nvme/host/core.c|362| <<nvme_delete_ctrl_work>> container_of(work, struct nvme_ctrl, delete_work);
+	 *   - drivers/nvme/host/core.c|387| <<nvme_delete_ctrl>> if (!queue_work(nvme_delete_wq, &ctrl->delete_work))
+	 *   - drivers/nvme/host/core.c|5801| <<nvme_init_ctrl>> INIT_WORK(&ctrl->delete_work, nvme_delete_ctrl_work);
+	 *
+	 * 调度delete_work
+	 */
 	flush_workqueue(nvme_delete_wq);
 }
 
diff --git a/drivers/nvme/target/nvmet.h b/drivers/nvme/target/nvmet.h
index 46df45e837c9..bf1e134f21b1 100644
--- a/drivers/nvme/target/nvmet.h
+++ b/drivers/nvme/target/nvmet.h
@@ -62,9 +62,36 @@ struct nvmet_ns {
 	uuid_t			uuid;
 	u32			anagrpid;
 
+	/*
+	 * 在以下使用nvmet_ns->buffered_io:
+	 *   - drivers/nvme/target/configfs.c|546| <<global>> CONFIGFS_ATTR(nvmet_ns_, buffered_io);
+	 *   - drivers/nvme/target/configfs.c|522| <<nvmet_ns_buffered_io_show>> return sprintf(page, "%d\n", to_nvmet_ns(item)->buffered_io);
+	 *   - drivers/nvme/target/configfs.c|541| <<nvmet_ns_buffered_io_store>> ns->buffered_io = val;
+	 *   - drivers/nvme/target/core.c|655| <<nvmet_ns_alloc>> ns->buffered_io = false;
+	 *   - drivers/nvme/target/io-cmd-file.c|19| <<nvmet_file_ns_disable>> if (ns->buffered_io)
+	 *   - drivers/nvme/target/io-cmd-file.c|36| <<nvmet_file_ns_enable>> if (!ns->buffered_io)
+	 *   - drivers/nvme/target/io-cmd-file.c|256| <<nvmet_file_execute_rw>> if (req->ns->buffered_io) {
+	 */
 	bool			buffered_io;
 	bool			enabled;
 	struct nvmet_subsys	*subsys;
+	/*
+	 * 在以下使用device_path:
+	 *   - drivers/nvme/target/configfs.c|342| <<global>> CONFIGFS_ATTR(nvmet_ns_, device_path);
+	 *   - drivers/nvme/target/configfs.c|307| <<nvmet_ns_device_path_show>> return sprintf(page, "%s\n", to_nvmet_ns(item)->device_path);
+	 *   - drivers/nvme/target/configfs.c|328| <<nvmet_ns_device_path_store>> kfree(ns->device_path);
+	 *   - drivers/nvme/target/configfs.c|330| <<nvmet_ns_device_path_store>> ns->device_path = kstrndup(page, len, GFP_KERNEL);
+	 *   - drivers/nvme/target/configfs.c|331| <<nvmet_ns_device_path_store>> if (!ns->device_path)
+	 *   - drivers/nvme/target/core.c|445| <<nvmet_p2pmem_ns_enable>> ns->device_path);
+	 *   - drivers/nvme/target/core.c|464| <<nvmet_p2pmem_ns_enable>> ns->device_path);
+	 *   - drivers/nvme/target/core.c|500| <<nvmet_p2pmem_ns_add_p2p>> dev_name(ctrl->p2p_client), ns->device_path);
+	 *   - drivers/nvme/target/core.c|631| <<nvmet_ns_free>> kfree(ns->device_path);
+	 *   - drivers/nvme/target/io-cmd-bdev.c|54| <<nvmet_bdev_ns_enable>> ns->bdev = blkdev_get_by_path(ns->device_path,
+	 *   - drivers/nvme/target/io-cmd-bdev.c|60| <<nvmet_bdev_ns_enable>> ns->device_path, PTR_ERR(ns->bdev));
+	 *   - drivers/nvme/target/io-cmd-file.c|39| <<nvmet_file_ns_enable>> ns->file = filp_open(ns->device_path, flags, 0);
+	 *   - drivers/nvme/target/io-cmd-file.c|42| <<nvmet_file_ns_enable>> ns->device_path, PTR_ERR(ns->file));
+	 *   - drivers/nvme/target/trace.h|57| <<__assign_disk_name>> strncpy(name, ns->device_path, DISK_NAME_LEN);
+	 */
 	const char		*device_path;
 
 	struct config_group	device_group;
@@ -98,7 +125,23 @@ struct nvmet_sq {
 	struct percpu_ref	ref;
 	u16			qid;
 	u16			size;
+	/*
+	 * 在以下使用nvme_sq->sqhd:
+	 *   - drivers/nvme/target/core.c|741| <<nvmet_update_sq_head>> old_sqhd = req->sq->sqhd;
+	 *   - drivers/nvme/target/core.c|743| <<nvmet_update_sq_head>> } while (cmpxchg(&req->sq->sqhd, old_sqhd, new_sqhd) !=
+	 *   - drivers/nvme/target/core.c|746| <<nvmet_update_sq_head>> req->cqe->sq_head = cpu_to_le16(req->sq->sqhd & 0x0000FFFF);
+	 *   - drivers/nvme/target/core.c|836| <<nvmet_sq_setup>> sq->sqhd = 0;
+	 */
 	u32			sqhd;
+	/*
+	 * 在以下使用sqhd_disabled:
+	 *   - drivers/nvme/target/core.c|781| <<__nvmet_req_complete>> if (!req->sq->sqhd_disabled)
+	 *   - drivers/nvme/target/fabrics-cmd.c|130| <<nvmet_install_queue>> req->sq->sqhd_disabled = true;
+	 *   - drivers/nvme/target/tcp.c|374| <<nvmet_setup_c2h_data_pdu>> pdu->hdr.flags = NVME_TCP_F_DATA_LAST | (queue->nvme_sq.sqhd_disabled ?
+	 *   - drivers/nvme/target/tcp.c|546| <<nvmet_try_send_data>> if (queue->nvme_sq.sqhd_disabled) {
+	 *   - drivers/nvme/target/tcp.c|554| <<nvmet_try_send_data>> if (queue->nvme_sq.sqhd_disabled) {
+	 *   - drivers/nvme/target/tcp.c|635| <<nvmet_try_send_ddgst>> if (queue->nvme_sq.sqhd_disabled) {
+	 */
 	bool			sqhd_disabled;
 	struct completion	free_done;
 	struct completion	confirm_done;
@@ -281,6 +324,12 @@ struct nvmet_fabrics_ops {
 	void (*discovery_chg)(struct nvmet_port *port);
 };
 
+/*
+ * 在以下使用NVMET_MAX_INLINE_BIOVEC:
+ *   - drivers/nvme/target/nvmet.h|321| <<global>> struct bio_vec inline_bvec[NVMET_MAX_INLINE_BIOVEC];
+ *   - drivers/nvme/target/io-cmd-file.c|256| <<nvmet_file_execute_rw>> if (nr_bvec > NVMET_MAX_INLINE_BIOVEC)
+ *   - drivers/nvme/target/nvmet.h|312| <<NVMET_MAX_INLINE_DATA_LEN>> #define NVMET_MAX_INLINE_DATA_LEN NVMET_MAX_INLINE_BIOVEC * PAGE_SIZE
+ */
 #define NVMET_MAX_INLINE_BIOVEC	8
 #define NVMET_MAX_INLINE_DATA_LEN NVMET_MAX_INLINE_BIOVEC * PAGE_SIZE
 
@@ -297,6 +346,14 @@ struct nvmet_req {
 			struct bio      inline_bio;
 		} b;
 		struct {
+			/*
+			 * 在以下使用mpool_alloc:
+			 *   - drivers/nvme/target/io-cmd-file.c|132| <<nvmet_file_io_done>> if (likely(req->f.mpool_alloc == false))
+			 *   - drivers/nvme/target/io-cmd-file.c|154| <<nvmet_file_execute_io>> if (req->f.mpool_alloc && nr_bvec > NVMET_MAX_MPOOL_BVEC)
+			 *   - drivers/nvme/target/io-cmd-file.c|265| <<nvmet_file_execute_rw>> req->f.mpool_alloc = true;
+			 *   - drivers/nvme/target/io-cmd-file.c|267| <<nvmet_file_execute_rw>> req->f.mpool_alloc = false;
+			 *   - drivers/nvme/target/io-cmd-file.c|270| <<nvmet_file_execute_rw>> if (likely(!req->f.mpool_alloc) &&
+			 */
 			bool			mpool_alloc;
 			struct kiocb            iocb;
 			struct bio_vec          *bvec;
diff --git a/drivers/pci/msi.c b/drivers/pci/msi.c
index c7709e49f0e4..750f7bccd386 100644
--- a/drivers/pci/msi.c
+++ b/drivers/pci/msi.c
@@ -691,6 +691,39 @@ static void __iomem *msix_map_region(struct pci_dev *dev, unsigned nr_entries)
 	return ioremap_nocache(phys_addr, nr_entries * PCI_MSIX_ENTRY_SIZE);
 }
 
+/*
+ * [0] __pci_enable_msix_range
+ * [0] pci_alloc_irq_vectors_affinity
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] __pci_enable_msix_range
+ * [0] pci_alloc_irq_vectors_affinity
+ * [0] vp_find_vqs_msix
+ * [0] vp_find_vqs
+ * [0] vp_modern_find_vqs
+ * [0] init_vq
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/pci/msi.c|794| <<msix_capability_init>> ret = msix_setup_entries(dev, base, entries, nvec, affd);
+ */
 static int msix_setup_entries(struct pci_dev *dev, void __iomem *base,
 			      struct msix_entry *entries, int nvec,
 			      struct irq_affinity *affd)
@@ -698,11 +731,41 @@ static int msix_setup_entries(struct pci_dev *dev, void __iomem *base,
 	struct irq_affinity_desc *curmsk, *masks = NULL;
 	struct msi_desc *entry;
 	int ret, i;
+	/* return the number of device's MSI-X table entries */
 	int vec_count = pci_msix_vec_count(dev);
 
+	/*
+	 * irq_create_affinity_masks - Create affinity masks for multiqueue spreading
+	 * @nvecs:      The total number of vectors
+	 * @affd:       Description of the affinity requirements
+	 *
+	 * Returns the irq_affinity_desc pointer or NULL if allocation failed.
+	 */
 	if (affd)
 		masks = irq_create_affinity_masks(nvec, affd);
 
+	/*
+	 * 在nvme和virtio-blk测试的时候...
+	 *
+	 * nvme: 8个cpu, 9个queue, 9个vector
+	 * 0 (admin) : masks[0].mask=0-7, masks[0].is_managed=0
+	 * 1 (io)    : masks[1].mask=0, masks[0].is_managed=1
+	 * 2 (io)    : masks[2].mask=1, masks[1].is_managed=1
+	 * 3 (io)    : masks[3].mask=2, masks[2].is_managed=1
+	 * 4 (io)    : masks[4].mask=3, masks[3].is_managed=1
+	 * 5 (io)    : masks[5].mask=4, masks[4].is_managed=1
+	 * 6 (io)    : masks[6].mask=5, masks[5].is_managed=1
+	 * 7 (io)    : masks[7].mask=6, masks[6].is_managed=1
+	 * 8 (io)    : masks[8].mask=7, masks[7].is_managed=1
+	 *
+	 * virtblk: 8个cpu, 5个queue, 5个vector
+	 * 0 (admin) : masks[0].mask=0-7, masks[0].is_managed=0
+	 * 1 (io)    : masks[1].mask=0-1, masks[1].is_managed=1
+	 * 2 (io)    : masks[2].mask=2-3, masks[2].is_managed=1
+	 * 3 (io)    : masks[3].mask=4-5, masks[3].is_managed=1
+	 * 4 (io)    : masks[4].mask=6-7, masks[4].is_managed=1
+	 */
+
 	for (i = 0, curmsk = masks; i < nvec; i++) {
 		entry = alloc_msi_entry(&dev->dev, 1, curmsk);
 		if (!entry) {
@@ -771,6 +834,10 @@ static void msix_program_entries(struct pci_dev *dev,
  * single MSI-X IRQ. A return of zero indicates the successful setup of
  * requested MSI-X entries with allocated IRQs or non-zero for otherwise.
  **/
+/*
+ * called by:
+ *   - drivers/pci/msi.c|1002| <<__pci_enable_msix>> return msix_capability_init(dev, entries, nvec, affd);
+ */
 static int msix_capability_init(struct pci_dev *dev, struct msix_entry *entries,
 				int nvec, struct irq_affinity *affd)
 {
@@ -1186,10 +1253,36 @@ EXPORT_SYMBOL(pci_enable_msix_range);
  * To get the Linux IRQ number used for a vector that can be passed to
  * request_irq() use the pci_irq_vector() helper.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2519| <<nvme_setup_irqs>> return pci_alloc_irq_vectors_affinity(pdev, 1, irq_queues,
+ *   - drivers/scsi/be2iscsi/be_main.c|3572| <<be2iscsi_enable_msix>> if (pci_alloc_irq_vectors_affinity(phba->pcidev, 2, nvec,
+ *   - drivers/scsi/csiostor/csio_isr.c|520| <<csio_enable_msix>> cnt = pci_alloc_irq_vectors_affinity(hw->pdev, min, cnt,
+ *   - drivers/scsi/hisi_sas/hisi_sas_v3_hw.c|2399| <<interrupt_init_v3_hw>> vectors = pci_alloc_irq_vectors_affinity(hisi_hba->pci_dev,
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|5795| <<__megasas_alloc_irq_vectors>> i = pci_alloc_irq_vectors_affinity(instance->pdev,
+ *   - drivers/scsi/mpt3sas/mpt3sas_base.c|3049| <<_base_alloc_irq_vectors>> i = pci_alloc_irq_vectors_affinity(ioc->pdev,
+ *   - drivers/scsi/qla2xxx/qla_isr.c|3478| <<qla24xx_enable_msix>> ret = pci_alloc_irq_vectors_affinity(ha->pdev, min_vecs,
+ *   - drivers/virtio/virtio_pci_common.c|133| <<vp_request_msix_vectors>> err = pci_alloc_irq_vectors_affinity(vp_dev->pci_dev, nvectors,
+ *   - include/linux/pci.h|1777| <<pci_alloc_irq_vectors>> return pci_alloc_irq_vectors_affinity(dev, min_vecs, max_vecs, flags,
+ */
 int pci_alloc_irq_vectors_affinity(struct pci_dev *dev, unsigned int min_vecs,
 				   unsigned int max_vecs, unsigned int flags,
 				   struct irq_affinity *affd)
 {
+	/*
+	 * struct irq_affinity - Description for automatic irq affinity assignements
+	 * @pre_vectors:        Don't apply affinity to @pre_vectors at beginning of
+	 *                      the MSI(-X) vector space
+	 * @post_vectors:       Don't apply affinity to @post_vectors at end of
+	 *                      the MSI(-X) vector space
+	 * @nr_sets:            The number of interrupt sets for which affinity
+	 *                      spreading is required
+	 * @set_size:           Array holding the size of each interrupt set
+	 * @calc_sets:          Callback for calculating the number and size
+	 *                      of interrupt sets
+	 * @priv:               Private data for usage by @calc_sets, usually a
+	 *                      pointer to driver/device specific data.
+	 */
 	struct irq_affinity msi_default_affd = {0};
 	int msix_vecs = -ENOSPC;
 	int msi_vecs = -ENOSPC;
@@ -1289,12 +1382,24 @@ EXPORT_SYMBOL(pci_irq_vector);
  * @dev:	PCI device to operate on
  * @nr:		device-relative interrupt vector index (0-based).
  */
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|65| <<blk_mq_pci_map_queues>> mask = pci_irq_get_affinity(pdev, queue + offset);
+ *   - drivers/scsi/hisi_sas/hisi_sas_v3_hw.c|2364| <<setup_reply_map_v3_hw>> mask = pci_irq_get_affinity(hisi_hba->pci_dev, queue +
+ *   - drivers/scsi/hpsa.c|7444| <<hpsa_setup_reply_map>> mask = pci_irq_get_affinity(h->pdev, queue);
+ *   - drivers/scsi/lpfc/lpfc_init.c|11124| <<lpfc_cpuhp_get_eq>> maskp = pci_irq_get_affinity(phba->pcidev, idx);
+ *   - drivers/scsi/lpfc/lpfc_init.c|11465| <<lpfc_sli4_enable_msix>> maskp = pci_irq_get_affinity(phba->pcidev, index);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|5706| <<megasas_setup_reply_map>> mask = pci_irq_get_affinity(instance->pdev, queue);
+ *   - drivers/scsi/mpt3sas/mpt3sas_base.c|2927| <<_base_assign_reply_queues>> mask = pci_irq_get_affinity(ioc->pdev,
+ *   - drivers/virtio/virtio_pci_common.c|455| <<vp_get_vq_affinity>> return pci_irq_get_affinity(vp_dev->pci_dev,
+ */
 const struct cpumask *pci_irq_get_affinity(struct pci_dev *dev, int nr)
 {
 	if (dev->msix_enabled) {
 		struct msi_desc *entry;
 		int i = 0;
 
+		/* entry的类型是struct msi_desc */
 		for_each_pci_msi_entry(entry, dev) {
 			if (i == nr)
 				return &entry->affinity->mask;
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 867c7ebd3f10..3d74c50dff79 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -91,6 +91,22 @@ struct vring_virtqueue {
 	bool packed_ring;
 
 	/* Is DMA API used? */
+	/*
+	 * 在以下设置use_dma_api:
+	 *   - drivers/virtio/virtio_ring.c|1610| <<vring_create_virtqueue_packed>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 *   - drivers/virtio/virtio_ring.c|2084| <<__vring_new_virtqueue>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 * 在以下使用use_dma_api:
+	 *   - drivers/virtio/virtio_ring.c|329| <<vring_map_one_sg>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|346| <<vring_map_single>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|356| <<vring_mapping_error>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|372| <<vring_unmap_one_split>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|919| <<vring_unmap_state_packed>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|942| <<vring_unmap_desc_packed>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|1034| <<virtqueue_add_indirect_packed>> if (vq->use_dma_api) {
+	 *   - drivers/virtio/virtio_ring.c|1165| <<virtqueue_add_packed>> if (unlikely(vq->use_dma_api)) {
+	 *   - drivers/virtio/virtio_ring.c|1297| <<detach_buf_packed>> if (unlikely(vq->use_dma_api)) {
+	 *   - drivers/virtio/virtio_ring.c|1314| <<detach_buf_packed>> if (vq->use_dma_api) {
+	 */
 	bool use_dma_api;
 
 	/* Can we use weak barriers? */
@@ -238,6 +254,14 @@ static inline bool virtqueue_use_indirect(struct virtqueue *_vq,
  * unconditionally on data path.
  */
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|281| <<virtio_max_dma_size>> if (vring_use_dma_api(vdev))
+ *   - drivers/virtio/virtio_ring.c|291| <<vring_alloc_queue>> if (vring_use_dma_api(vdev)) {
+ *   - drivers/virtio/virtio_ring.c|324| <<vring_free_queue>> if (vring_use_dma_api(vdev))
+ *   - drivers/virtio/virtio_ring.c|1626| <<bool>> vq->use_dma_api = vring_use_dma_api(vdev);
+ *   - drivers/virtio/virtio_ring.c|2100| <<bool>> vq->use_dma_api = vring_use_dma_api(vdev);
+ */
 static bool vring_use_dma_api(struct virtio_device *vdev)
 {
 	if (!virtio_has_iommu_quirk(vdev))
diff --git a/fs/block_dev.c b/fs/block_dev.c
index 69bf2fb6f7cd..10e87c18fdbe 100644
--- a/fs/block_dev.c
+++ b/fs/block_dev.c
@@ -1508,6 +1508,15 @@ EXPORT_SYMBOL(bd_set_size);
 
 static void __blkdev_put(struct block_device *bdev, fmode_t mode, int for_part);
 
+/*
+ * called by:
+ *   - block/ioctl.c|168| <<blkdev_reread_part>> ret = bdev_disk_changed(bdev, false);
+ *   - drivers/block/loop.c|644| <<loop_reread_partitions>> rc = bdev_disk_changed(bdev, false);
+ *   - drivers/block/loop.c|1171| <<__loop_clr_fd>> err = bdev_disk_changed(bdev, false);
+ *   - drivers/s390/block/dasd_genhd.c|120| <<dasd_scan_partitions>> rc = bdev_disk_changed(bdev, false);
+ *   - fs/block_dev.c|1634| <<__blkdev_get>> bdev_disk_changed(bdev, ret == -ENOMEDIUM);
+ *   - fs/block_dev.c|1669| <<__blkdev_get>> bdev_disk_changed(bdev, ret == -ENOMEDIUM);
+ */
 int bdev_disk_changed(struct block_device *bdev, bool invalidate)
 {
 	struct gendisk *disk = bdev->bd_disk;
diff --git a/fs/direct-io.c b/fs/direct-io.c
index 00b4d15bb811..356584570bc3 100644
--- a/fs/direct-io.c
+++ b/fs/direct-io.c
@@ -940,6 +940,10 @@ static inline void dio_zero_block(struct dio *dio, struct dio_submit *sdio,
  * it should set b_size to PAGE_SIZE or more inside get_block().  This gives
  * fine alignment but still allows this function to work in PAGE_SIZE units.
  */
+/*
+ * called by:
+ *   - fs/direct-io.c|1311| <<do_blockdev_direct_IO>> retval = do_direct_IO(dio, &sdio, &map_bh);
+ */
 static int do_direct_IO(struct dio *dio, struct dio_submit *sdio,
 			struct buffer_head *map_bh)
 {
@@ -1141,6 +1145,10 @@ static inline int drop_refcount(struct dio *dio)
  * individual fields and will generate much worse code. This is important
  * for the whole file.
  */
+/*
+ * called by:
+ *   - fs/direct-io.c|1393| <<__blockdev_direct_IO>> return do_blockdev_direct_IO(iocb, inode, bdev, iter, get_block,
+ */
 static inline ssize_t
 do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 		      struct block_device *bdev, struct iov_iter *iter,
@@ -1372,6 +1380,13 @@ do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 	return retval;
 }
 
+/*
+ * called by:
+ *   - fs/btrfs/inode.c|8741| <<btrfs_direct_IO>> ret = __blockdev_direct_IO(iocb, inode,
+ *   - fs/f2fs/data.c|2972| <<f2fs_direct_IO>> err = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - fs/ocfs2/aops.c|2470| <<ocfs2_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - include/linux/fs.h|3171| <<blockdev_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev, iter,
+ */
 ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     struct block_device *bdev, struct iov_iter *iter,
 			     get_block_t get_block,
diff --git a/fs/ext4/file.c b/fs/ext4/file.c
index 6a7293a5cda2..a3a0432c6db4 100644
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@ -36,6 +36,11 @@
 #include "acl.h"
 #include "truncate.h"
 
+/*
+ * called by:
+ *   - fs/ext4/file.c|64| <<ext4_dio_read_iter>> if (!ext4_dio_supported(inode)) {
+ *   - fs/ext4/file.c|382| <<ext4_dio_write_iter>> if (!ext4_dio_supported(inode)) {
+ */
 static bool ext4_dio_supported(struct inode *inode)
 {
 	if (IS_ENABLED(CONFIG_FS_ENCRYPTION) && IS_ENCRYPTED(inode))
diff --git a/fs/iomap/direct-io.c b/fs/iomap/direct-io.c
index 23837926c0c5..d5663a649119 100644
--- a/fs/iomap/direct-io.c
+++ b/fs/iomap/direct-io.c
@@ -58,6 +58,11 @@ int iomap_dio_iopoll(struct kiocb *kiocb, bool spin)
 }
 EXPORT_SYMBOL_GPL(iomap_dio_iopoll);
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|194| <<iomap_dio_zero>> iomap_dio_submit_bio(dio, iomap, bio);
+ *   - fs/iomap/direct-io.c|306| <<iomap_dio_bio_actor>> iomap_dio_submit_bio(dio, iomap, bio);
+ */
 static void iomap_dio_submit_bio(struct iomap_dio *dio, struct iomap *iomap,
 		struct bio *bio)
 {
@@ -174,6 +179,11 @@ static void iomap_dio_bio_end_io(struct bio *bio)
 	}
 }
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|256| <<iomap_dio_bio_actor>> iomap_dio_zero(dio, iomap, pos - pad, pad);
+ *   - fs/iomap/direct-io.c|321| <<iomap_dio_bio_actor>> iomap_dio_zero(dio, iomap, pos, fs_block_size - pad);
+ */
 static void
 iomap_dio_zero(struct iomap_dio *dio, struct iomap *iomap, loff_t pos,
 		unsigned len)
@@ -194,6 +204,11 @@ iomap_dio_zero(struct iomap_dio *dio, struct iomap *iomap, loff_t pos,
 	iomap_dio_submit_bio(dio, iomap, bio);
 }
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|380| <<iomap_dio_actor>> return iomap_dio_bio_actor(inode, pos, length, dio, iomap);
+ *   - fs/iomap/direct-io.c|382| <<iomap_dio_actor>> return iomap_dio_bio_actor(inode, pos, length, dio, iomap);
+ */
 static loff_t
 iomap_dio_bio_actor(struct inode *inode, loff_t pos, loff_t length,
 		struct iomap_dio *dio, struct iomap *iomap)
@@ -363,6 +378,10 @@ iomap_dio_inline_actor(struct inode *inode, loff_t pos, loff_t length,
 	return copied;
 }
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|523| <<iomap_dio_rw>> iomap_dio_actor);
+ */
 static loff_t
 iomap_dio_actor(struct inode *inode, loff_t pos, loff_t length,
 		void *data, struct iomap *iomap, struct iomap *srcmap)
@@ -397,6 +416,15 @@ iomap_dio_actor(struct inode *inode, loff_t pos, loff_t length,
  * may be pure data writes. In that case, we still need to do a full data sync
  * completion.
  */
+/*
+ * called by:
+ *   - fs/ext4/file.c|77| <<ext4_dio_read_iter>> ret = iomap_dio_rw(iocb, to, &ext4_iomap_ops, NULL,
+ *   - fs/ext4/file.c|438| <<ext4_dio_write_iter>> ret = iomap_dio_rw(iocb, from, &ext4_iomap_ops, &ext4_dio_write_ops,
+ *   - fs/gfs2/file.c|774| <<gfs2_file_direct_read>> ret = iomap_dio_rw(iocb, to, &gfs2_iomap_ops, NULL,
+ *   - fs/gfs2/file.c|810| <<gfs2_file_direct_write>> ret = iomap_dio_rw(iocb, from, &gfs2_iomap_ops, NULL,
+ *   - fs/xfs/xfs_file.c|191| <<xfs_file_dio_aio_read>> ret = iomap_dio_rw(iocb, to, &xfs_read_iomap_ops, NULL,
+ *   - fs/xfs/xfs_file.c|554| <<xfs_file_dio_aio_write>> ret = iomap_dio_rw(iocb, from, &xfs_direct_write_iomap_ops,
+ */
 ssize_t
 iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
diff --git a/include/linux/badblocks.h b/include/linux/badblocks.h
index 2426276b9bd3..1d7eeb551403 100644
--- a/include/linux/badblocks.h
+++ b/include/linux/badblocks.h
@@ -8,13 +8,40 @@
 #include <linux/stddef.h>
 #include <linux/types.h>
 
+/*
+ * 在以下使用BB_LEN_MASK:
+ *   - include/linux/badblocks.h|16| <<BB_LEN>> #define BB_LEN(x) (((x) & BB_LEN_MASK) + 1)
+ *
+ * 最后9个bit都是1 (表示1个range的长度, 最多512个sector)
+ */
 #define BB_LEN_MASK	(0x00000000000001FFULL)
+/*
+ * 在以下使用BB_OFFSET_MASK:
+ *   - include/linux/badblocks.h|15| <<BB_OFFSET>> #define BB_OFFSET(x) (((x) & BB_OFFSET_MASK) >> 9)
+ */
 #define BB_OFFSET_MASK	(0x7FFFFFFFFFFFFE00ULL)
+/*
+ * 在以下使用BB_ACK_MASK:
+ *   - include/linux/badblocks.h|17| <<BB_ACK>> #define BB_ACK(x) (!!((x) & BB_ACK_MASK))
+ */
 #define BB_ACK_MASK	(0x8000000000000000ULL)
 #define BB_MAX_LEN	512
 #define BB_OFFSET(x)	(((x) & BB_OFFSET_MASK) >> 9)
 #define BB_LEN(x)	(((x) & BB_LEN_MASK) + 1)
 #define BB_ACK(x)	(!!((x) & BB_ACK_MASK))
+/*
+ * 在以下调用BB_MAKE():
+ *   - block/badblocks.c|244| <<badblocks_set>> p[lo] = BB_MAKE(a, e-a, ack);
+ *   - block/badblocks.c|251| <<badblocks_set>> p[lo] = BB_MAKE(a, BB_MAX_LEN, ack);
+ *   - block/badblocks.c|276| <<badblocks_set>> p[hi] = BB_MAKE(a, e-a, ack);
+ *   - block/badblocks.c|279| <<badblocks_set>> p[hi] = BB_MAKE(a, BB_MAX_LEN, ack);
+ *   - block/badblocks.c|299| <<badblocks_set>> p[lo] = BB_MAKE(BB_OFFSET(p[lo]), newlen, ack);
+ *   - block/badblocks.c|322| <<badblocks_set>> p[hi] = BB_MAKE(s, this_sectors, acknowledged);
+ *   - block/badblocks.c|408| <<badblocks_clear>> p[lo] = BB_MAKE(a, s-a, ack);
+ *   - block/badblocks.c|411| <<badblocks_clear>> p[lo] = BB_MAKE(target, end - target, ack);
+ *   - block/badblocks.c|425| <<badblocks_clear>> p[lo] = BB_MAKE(start, s - start, ack);
+ *   - block/badblocks.c|471| <<ack_all_badblocks>> p[i] = BB_MAKE(start, len, 1);
+ */
 #define BB_MAKE(a, l, ack) (((a)<<9) | ((l)-1) | ((u64)(!!(ack)) << 63))
 
 /* Bad block numbers are stored sorted in a single page.
@@ -22,18 +49,52 @@
  * 54 bits are sector number, 9 bits are extent size,
  * 1 bit is an 'acknowledged' flag.
  */
+/*
+ * 在以下使用MAX_BADBLOCKS:
+ *   - block/badblocks.c|309| <<badblocks_set>> if (bb->count >= MAX_BADBLOCKS) {
+ *   - block/badblocks.c|402| <<badblocks_clear>> if (bb->count >= MAX_BADBLOCKS) {
+ */
 #define MAX_BADBLOCKS	(PAGE_SIZE/8)
 
 struct badblocks {
 	struct device *dev;	/* set by devm_init_badblocks */
+	/*
+	 * 在以下修改count:
+	 *   - block/badblocks.c|294| <<badblocks_set>> bb->count--;
+	 *   - block/badblocks.c|310| <<badblocks_set>> bb->count++;
+	 *   - block/badblocks.c|399| <<badblocks_clear>> bb->count++;
+	 *   - block/badblocks.c|428| <<badblocks_clear>> bb->count -= (hi - lo - 1);
+	 *   - block/badblocks.c|581| <<__badblocks_init>> bb->count = 0;
+	 */
 	int count;		/* count of bad blocks */
+	/*
+	 * 在以下使用unacked_exist:
+	 *   - block/badblocks.c|148| <<badblocks_update_acked>> if (!bb->unacked_exist)
+	 *   - block/badblocks.c|159| <<badblocks_update_acked>> bb->unacked_exist = 0;
+	 *   - block/badblocks.c|330| <<badblocks_set>> bb->unacked_exist = 1;
+	 *   - block/badblocks.c|462| <<ack_all_badblocks>> if (bb->changed == 0 && bb->unacked_exist) {
+	 *   - block/badblocks.c|474| <<ack_all_badblocks>> bb->unacked_exist = 0;
+	 *   - block/badblocks.c|537| <<badblocks_show>> bb->unacked_exist = 0;
+	 *   - drivers/md/md.c|2810| <<state_show>> rdev->badblocks.unacked_exist))
+	 *   - drivers/md/md.c|2819| <<state_show>> (rdev->badblocks.unacked_exist
+	 *   - drivers/md/md.c|2903| <<state_store>> rdev->badblocks.unacked_exist) {
+	 */
 	int unacked_exist;	/* there probably are unacknowledged
 				 * bad blocks.  This is only cleared
 				 * when a read discovers none
 				 */
+	/*
+	 * 设置shift的地方:
+	 *   - block/badblocks.c|600| <<__badblocks_init>> bb->shift = 0;
+	 *   - block/badblocks.c|602| <<__badblocks_init>> bb->shift = -1;
+	 *   - block/badblocks.c|614| <<__badblocks_init>> bb->shift = -1;
+	 */
 	int shift;		/* shift from sectors to block size
 				 * a -ve shift means badblocks are
 				 * disabled.*/
+	/*
+	 * 在__badblocks_init()中分配PAGE_SIZE
+	 */
 	u64 *page;		/* badblock list */
 	int changed;
 	seqlock_t lock;
diff --git a/include/linux/bio.h b/include/linux/bio.h
index 853d92ceee64..c1c44f9e943f 100644
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@ -827,6 +827,12 @@ static inline int bio_integrity_add_page(struct bio *bio, struct page *page,
  * must be found by the caller. This is different than IRQ driven IO, where
  * it's safe to wait for IO to complete.
  */
+/*
+ * called by:
+ *   - fs/block_dev.c|249| <<__blkdev_direct_IO_simple>> bio_set_polled(&bio, iocb);
+ *   - fs/block_dev.c|410| <<__blkdev_direct_IO>> bio_set_polled(bio, iocb);
+ *   - fs/iomap/direct-io.c|67| <<iomap_dio_submit_bio>> bio_set_polled(bio, dio->iocb);
+ */
 static inline void bio_set_polled(struct bio *bio, struct kiocb *kiocb)
 {
 	bio->bi_opf |= REQ_HIPRI;
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 11cfd6470b1a..7497996ce0ab 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -23,6 +23,22 @@ struct blk_mq_hw_ctx {
 		 * resources) could not be sent to the hardware. As soon as the
 		 * driver can send new requests, requests at this list will
 		 * be sent first for a fairer dispatch.
+		 */
+		/*
+		 * 在以下添加rq到hctx->dispatch:
+		 *   - block/blk-mq-sched.c|376| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1414| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1794| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2373| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 * 在以下使用hctx->dispatch:
+		 *   - block/blk-mq.c|2491| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+		 *   - block/blk-mq-sched.c|199| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *   - block/blk-mq-sched.c|196| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+		 *   - block/blk-mq-sched.c|198| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+		 *   - block/blk-mq.c|82| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+		 *   - block/blk-mq-debugfs.c|363| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+		 *   - block/blk-mq-debugfs.c|370| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+
 		 */
 		struct list_head	dispatch;
 		 /**
@@ -35,6 +51,14 @@ struct blk_mq_hw_ctx {
 	/**
 	 * @run_work: Used for scheduling a hardware queue run at a later time.
 	 */
+	/*
+	 * 在以下使用run_work:
+	 *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 */
 	struct delayed_work	run_work;
 	/** @cpumask: Map of available CPUs where this hctx can run. */
 	cpumask_var_t		cpumask;
@@ -86,6 +110,15 @@ struct blk_mq_hw_ctx {
 	 * decide if the hw_queue is busy using Exponential Weighted Moving
 	 * Average algorithm.
 	 */
+	/*
+	 * 在以下使用dispatch_busy:
+	 *   - block/blk-mq-debugfs.c|621| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+	 *   - block/blk-mq-sched.c|217| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+	 *   - block/blk-mq-sched.c|436| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+	 *   - block/blk-mq.c|1215| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+	 *   - block/blk-mq.c|1225| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 *   - block/blk-mq.c|2071| <<blk_mq_make_request>> !data.hctx->dispatch_busy) {
+	 */
 	unsigned int		dispatch_busy;
 
 	/** @type: HCTX_TYPE_* flags. Type of hardware queue. */
@@ -107,6 +140,10 @@ struct blk_mq_hw_ctx {
 	 * @wait_index: Index of next available dispatch_wait queue to insert
 	 * requests.
 	 */
+	/*
+	 * 使用的地方:
+	 *   - block/blk-mq-tag.h|43| <<bt_wait_ptr>> return sbq_wait_ptr(bt, &hctx->wait_index);
+	 */
 	atomic_t		wait_index;
 
 	/**
@@ -138,6 +175,17 @@ struct blk_mq_hw_ctx {
 	 * @nr_active: Number of active requests. Only used when a tag set is
 	 * shared across request queues.
 	 */
+	/*
+	 * 修改nr_active的地方:
+	 *   - block/blk-mq.c|298| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|1072| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|518| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.h|207| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|2348| <<blk_mq_alloc_hctx>> atomic_set(&hctx->nr_active, 0);
+	 * 读取nr_active的地方:
+	 *   - block/blk-mq-tag.c|87| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 *   - block/blk-mq-tag.c|116| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 */
 	atomic_t		nr_active;
 
 	/** @cpuhp_dead: List to store request if some CPU die. */
@@ -163,6 +211,16 @@ struct blk_mq_hw_ctx {
 #endif
 
 	/** @hctx_list:	List of all hardware queues. */
+	/*
+	 * 实际是if this hctx is not in use, this is an entry in q->unused_hctx_list.
+	 *   - block/blk-mq.c|2476| <<blk_mq_exit_hctx>> list_add(&hctx->hctx_list, &q->unused_hctx_list);
+	 *   - block/blk-mq.c|2576| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->hctx_list);
+	 *   - block/blk-mq.c|2944| <<blk_mq_release>> WARN_ON_ONCE(hctx && list_empty(&hctx->hctx_list));
+	 *   - block/blk-mq.c|2947| <<blk_mq_release>> list_for_each_entry_safe(hctx, next, &q->unused_hctx_list, hctx_list) {
+	 *   - block/blk-mq.c|2948| <<blk_mq_release>> list_del_init(&hctx->hctx_list);
+	 *   - block/blk-mq.c|3054| <<blk_mq_alloc_and_init_hctx>> list_for_each_entry(tmp, &q->unused_hctx_list, hctx_list) {
+	 *   - block/blk-mq.c|3061| <<blk_mq_alloc_and_init_hctx>> list_del_init(&hctx->hctx_list);
+	 */
 	struct list_head	hctx_list;
 
 	/**
@@ -185,7 +243,31 @@ struct blk_mq_hw_ctx {
  */
 struct blk_mq_queue_map {
 	unsigned int *mq_map;
+	/*
+	 * 修改map[]->nr_queues的地方:
+	 *   - block/blk-mq.c|3457| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 *   - drivers/nvme/host/pci.c|440| <<nvme_pci_map_queues>> map->nr_queues = dev->io_queues[i];
+	 *   - drivers/nvme/host/rdma.c|1824| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1827| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1833| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1836| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1847| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2179| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2182| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2188| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2191| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2200| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 */
 	unsigned int nr_queues;
+	/*
+	 * 对于nvme来说
+	 * The poll queue(s) doesn't have an IRQ (and hence IRQ
+	 * affinity), so use the regular blk-mq cpu mapping
+	 *
+	 * First hardware queue to map onto. Used by the PCIe NVMe
+	 * driver to map each hardware queue type (enum hctx_type) onto a distinct
+	 * set of hardware queues.
+	 */
 	unsigned int queue_offset;
 };
 
@@ -232,12 +314,93 @@ enum hctx_type {
  * @tag_list:	   List of the request queues that use this tag set. See also
  *		   request_queue.tag_set_list.
  */
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */ 
 struct blk_mq_tag_set {
 	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
+	/*
+	 * 修改set->nr_maps的地方:
+	 *   - block/blk-mq.c|3013| <<blk_mq_init_sq_queue>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3409| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3420| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - drivers/block/paride/pd.c|908| <<pd_probe_drive>> disk->tag_set.nr_maps = 1;
+	 *   - drivers/block/sx8.c|1463| <<carm_init_one>> host->tag_set.nr_maps = 1;
+	 *   - drivers/nvme/host/pci.c|2309| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+	 *   - drivers/nvme/host/pci.c|2311| <<nvme_dev_add>> dev->tagset.nr_maps++;
+	 *   - drivers/nvme/host/rdma.c|752| <<nvme_rdma_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+	 *   - drivers/nvme/host/tcp.c|1490| <<nvme_tcp_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+	 */
 	unsigned int		nr_maps;
 	const struct blk_mq_ops	*ops;
+	/*
+	 * 部分修改set->nr_hw_queues的地方:
+	 *   - block/blk-mq.c|3022| <<blk_mq_init_sq_queue>> set->nr_hw_queues = 1;
+	 *   - block/blk-mq.c|3361| <<blk_mq_realloc_tag_set_tags>> set->nr_hw_queues = new_nr_hw_queues;
+	 *   - block/blk-mq.c|3429| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = 1;
+	 *   - block/blk-mq.c|3438| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = nr_cpu_ids;
+	 *   - block/blk-mq.c|3655| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = nr_hw_queues;
+	 *   - block/blk-mq.c|3663| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = prev_nr_hw_queues;
+	 *   - drivers/block/loop.c|2107| <<loop_add>> lo->tag_set.nr_hw_queues = 1;
+	 *   - drivers/block/nbd.c|1679| <<nbd_dev_add>> nbd->tag_set.nr_hw_queues = 1;
+	 *   - drivers/block/null_blk_main.c|1968| <<null_init_tag_set>> set->nr_hw_queues = nullb ? nullb->dev->submit_queues :
+	 *   - drivers/block/virtio_blk.c|817| <<virtblk_probe>> vblk->tag_set.nr_hw_queues = vblk->num_vqs;
+	 *   - drivers/block/xen-blkfront.c|968| <<xlvbd_init_blk_queue>> info->tag_set.nr_hw_queues = info->nr_rings;
+	 *   - drivers/md/dm-rq.c|551| <<dm_mq_init_request_queue>> md->tag_set->nr_hw_queues = dm_get_blk_mq_nr_hw_queues();
+	 *   - drivers/nvme/host/fc.c|2472| <<nvme_fc_create_io_queues>> ctrl->tag_set.nr_hw_queues = ctrl->ctrl.queue_count - 1;
+	 *   - drivers/nvme/host/fc.c|3139| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.nr_hw_queues = 1;
+	 *   - drivers/nvme/host/pci.c|1636| <<nvme_alloc_admin_tags>> dev->admin_tagset.nr_hw_queues = 1;
+	 *   - drivers/nvme/host/pci.c|2308| <<nvme_dev_add>> dev->tagset.nr_hw_queues = dev->online_queues - 1;
+	 *   - drivers/nvme/host/rdma.c|736| <<nvme_rdma_alloc_tagset>> set->nr_hw_queues = 1;
+	 *   - drivers/nvme/host/rdma.c|750| <<nvme_rdma_alloc_tagset>> set->nr_hw_queues = nctrl->queue_count - 1;
+	 *   - drivers/nvme/host/tcp.c|1476| <<nvme_tcp_alloc_tagset>> set->nr_hw_queues = 1;
+	 *   - drivers/nvme/host/tcp.c|1488| <<nvme_tcp_alloc_tagset>> set->nr_hw_queues = nctrl->queue_count - 1;
+	 *   - drivers/nvme/target/loop.c|347| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.nr_hw_queues = 1;
+	 *   - drivers/nvme/target/loop.c|521| <<nvme_loop_create_io_queues>> ctrl->tag_set.nr_hw_queues = ctrl->ctrl.queue_count - 1;
+	 *   - drivers/scsi/scsi_lib.c|1897| <<scsi_mq_setup_tags>> shost->tag_set.nr_hw_queues = shost->nr_hw_queues ? : 1;
+	 */
 	unsigned int		nr_hw_queues;
 	unsigned int		queue_depth;
+	/*
+	 * 在以下使用nr_reserved_tags:
+	 *   - block/blk-mq-sched.c|511| <<blk_mq_sched_alloc_tags>> set->reserved_tags);
+	 *   - block/blk-mq.c|2938| <<__blk_mq_alloc_rq_map>> set->queue_depth, set->reserved_tags);
+	 *   - block/blk-mq.c|3580| <<blk_mq_alloc_rq_maps>> if (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN) {
+	 *   - block/blk-mq.c|3696| <<blk_mq_alloc_tag_set>> if (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN)
+	 * 在以下设置nr_reserved_tags:
+	 *   - drivers/ide/ide-probe.c|783| <<ide_init_queue>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|2465| <<nvme_fc_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|3133| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/host/pci.c|3578| <<nvme_dev_add>> dev->tagset.reserved_tags = NVME_AQ_DEPTH;
+	 *   - drivers/nvme/host/rdma.c|731| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/rdma.c|744| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/tcp.c|1472| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/tcp.c|1483| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/target/loop.c|342| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/target/loop.c|515| <<nvme_loop_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 */
 	unsigned int		reserved_tags;
 	unsigned int		cmd_size;
 	int			numa_node;
@@ -248,6 +411,10 @@ struct blk_mq_tag_set {
 	struct blk_mq_tags	**tags;
 
 	struct mutex		tag_list_lock;
+	/*
+	 * 添加的地方:
+	 *   - block/blk-mq.c|2626| <<blk_mq_add_queue_tag_set>> list_add_tail_rcu(&q->tag_set_list, &set->tag_list);
+	 */
 	struct list_head	tag_list;
 };
 
@@ -394,7 +561,22 @@ enum {
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
 	BLK_MQ_S_STOPPED	= 0,
+	/*
+	 * 使用BLK_MQ_S_TAG_ACTIVE的地方:
+	 *   - block/blk-mq-tag.c|26| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|27| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|51| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|70| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 */
 	BLK_MQ_S_TAG_ACTIVE	= 1,
+	/*
+	 * 使用BLK_MQ_S_SCHED_RESTART的地方:
+	 *   - block/blk-mq-sched.c|67| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|70| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|76| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|85| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
@@ -431,8 +613,29 @@ enum {
 	/* return when out of requests */
 	BLK_MQ_REQ_NOWAIT	= (__force blk_mq_req_flags_t)(1 << 0),
 	/* allocate from reserved pool */
+	/*
+	 * 在以下使用BLK_MQ_REQ_RESERVED:
+	 *   - block/blk-mq-tag.c|151| <<blk_mq_get_tag>> if (data->flags & BLK_MQ_REQ_RESERVED) {
+	 *   - block/blk-mq-tag.c|207| <<blk_mq_get_tag>> if (data->flags & BLK_MQ_REQ_RESERVED)
+	 *   - block/blk-mq.c|430| <<blk_mq_get_request>> !(data->flags & BLK_MQ_REQ_RESERVED))
+	 *   - block/blk-mq.c|1381| <<blk_mq_get_driver_tag>> data.flags |= BLK_MQ_REQ_RESERVED;
+	 *   - drivers/block/mtip32xx/mtip32xx.c|985| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+	 *   - drivers/ide/ide-atapi.c|203| <<ide_prep_sense>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/nvme/host/core.c|1306| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+	 *   - drivers/nvme/host/fabrics.c|401| <<nvmf_connect_admin_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, false);
+	 *   - drivers/nvme/host/fabrics.c|464| <<nvmf_connect_io_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, poll);
+	 *
+	 * 比方在blk_mq_get_tag()中就会用tags->breserved_tags
+	 */
 	BLK_MQ_REQ_RESERVED	= (__force blk_mq_req_flags_t)(1 << 1),
 	/* allocate internal/sched tag */
+	/*
+	 * 在以下使用BLK_MQ_REQ_INTERNAL:
+	 *   - block/blk-mq-tag.c|132| <<__blk_mq_get_tag>> if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
+	 *   - block/blk-mq.c|508| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_INTERNAL) {
+	 *   - block/blk-mq.c|610| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 *   - block/blk-mq.h|242| <<blk_mq_tags_from_data>> if (data->flags & BLK_MQ_REQ_INTERNAL)
+	 */
 	BLK_MQ_REQ_INTERNAL	= (__force blk_mq_req_flags_t)(1 << 2),
 	/* set RQF_PREEMPT */
 	BLK_MQ_REQ_PREEMPT	= (__force blk_mq_req_flags_t)(1 << 3),
@@ -471,13 +674,60 @@ static inline enum mq_rq_state blk_mq_rq_state(struct request *rq)
 	return READ_ONCE(rq->state);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|326| <<bt_tags_iter>> if (rq && blk_mq_request_started(rq))
+ *   - block/blk-mq.c|1094| <<__blk_mq_requeue_request>> if (blk_mq_request_started(rq)) {
+ *   - drivers/block/nbd.c|698| <<nbd_read_stat>> if (!req || !blk_mq_request_started(req)) {
+ *
+ * 返回(blk_mq_rq_state(rq) != MQ_RQ_IDLE)
+ */
 static inline int blk_mq_request_started(struct request *rq)
 {
+	/*
+	 * 使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+	 *   - block/blk-mq.c|580| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|3474| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 *   - include/linux/blk-mq.h|481| <<blk_mq_request_completed>> return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
+	 *
+	 * 使用MQ_RQ_IN_FLIGHT的地方:
+	 *   - block/blk-mq-debugfs.c|317| <<global>> [MQ_RQ_IN_FLIGHT] = "in_flight",
+	 *   - block/blk-mq.c|791| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *   - block/blk-mq.c|1017| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|1059| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 *
+	 * 使用MQ_RQ_IDLE的地方:
+	 *   - block/blk-mq-debugfs.c|316| <<global>> [MQ_RQ_IDLE] = "idle",
+	 *   - block/blk-mq.c|559| <<blk_mq_free_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|788| <<blk_mq_start_request>> WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|825| <<__blk_mq_requeue_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|2493| <<blk_mq_init_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - include/linux/blk-mq.h|639| <<blk_mq_request_started>> return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
+	 */
 	return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|384| <<blk_mq_tagset_count_completed_rqs>> if (blk_mq_request_completed(rq))
+ *   - drivers/nvme/host/core.c|317| <<nvme_cancel_request>> if (blk_mq_request_completed(req))
+ */
 static inline int blk_mq_request_completed(struct request *rq)
 {
+	/*
+	 * 使用MQ_RQ_IN_FLIGHT的地方:
+	 *   - block/blk-mq-debugfs.c|317| <<global>> [MQ_RQ_IN_FLIGHT] = "in_flight",
+	 *   - block/blk-mq.c|791| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *   - block/blk-mq.c|1017| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|1059| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 *
+	 * 使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+	 *   - block/blk-mq.c|580| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|3474| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 *   - include/linux/blk-mq.h|481| <<blk_mq_request_completed>> return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
+	 */
 	return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
 }
 
@@ -548,10 +798,16 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	return rq + 1;
 }
 
+/*
+ * 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i]
+ */
 #define queue_for_each_hw_ctx(q, hctx, i)				\
 	for ((i) = 0; (i) < (q)->nr_hw_queues &&			\
 	     ({ hctx = (q)->queue_hw_ctx[i]; 1; }); (i)++)
 
+/*
+ * 对于hctx->nr_ctx范围内的每一个hctx->ctxs[i]
+ */
 #define hctx_for_each_ctx(hctx, ctx, i)					\
 	for ((i) = 0; (i) < (hctx)->nr_ctx &&				\
 	     ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index 70254ae11769..e95cb9461557 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -301,7 +301,19 @@ enum req_opf {
 	REQ_OP_SCSI_IN		= 32,
 	REQ_OP_SCSI_OUT		= 33,
 	/* Driver private requests */
+	/*
+	 * 部分使用REQ_OP_DRV_IN的例子:
+	 *   - drivers/block/virtio_blk.c|306| <<virtio_queue_rq>> case REQ_OP_DRV_IN:
+	 *   - drivers/block/virtio_blk.c|369| <<virtblk_get_id>> req = blk_get_request(q, REQ_OP_DRV_IN, 0);
+	 *   - drivers/nvme/host/core.c|485| <<nvme_alloc_request>> unsigned op = nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;
+	 *   - drivers/nvme/host/core.c|764| <<nvme_setup_cmd>> case REQ_OP_DRV_IN:
+	 */
 	REQ_OP_DRV_IN		= 34,
+	/*
+	 * 部分使用REQ_OP_DRV_OUT的例子:
+	 *   - drivers/nvme/host/core.c|485| <<nvme_alloc_request>> unsigned op = nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;
+	 *   - drivers/nvme/host/core.c|765| <<nvme_setup_cmd>> case REQ_OP_DRV_OUT:
+	 */
 	REQ_OP_DRV_OUT		= 35,
 
 	REQ_OP_LAST,
@@ -362,6 +374,16 @@ enum req_flag_bits {
 #define REQ_CGROUP_PUNT		(1ULL << __REQ_CGROUP_PUNT)
 
 #define REQ_NOUNMAP		(1ULL << __REQ_NOUNMAP)
+/*
+ * 使用REQ_HIPRI的地方:
+ *   - block/blk-core.c|937| <<generic_make_request_checks>> bio->bi_opf &= ~REQ_HIPRI;
+ *   - block/blk-mq.c|608| <<__blk_mq_complete_request>> if ((rq->cmd_flags & REQ_HIPRI) ||
+ *   - block/blk-mq.h|191| <<blk_mq_map_queue>> if (flags & REQ_HIPRI)
+ *   - drivers/nvme/host/core.c|807| <<nvme_execute_rq_polled>> rq->cmd_flags |= REQ_HIPRI;
+ *   - fs/direct-io.c|1243| <<do_blockdev_direct_IO>> dio->op_flags |= REQ_HIPRI;
+ *   - include/linux/bio.h|832| <<bio_set_polled>> bio->bi_opf |= REQ_HIPRI;
+ *   - mm/page_io.c|412| <<swap_readpage>> bio->bi_opf |= REQ_HIPRI;
+ */
 #define REQ_HIPRI		(1ULL << __REQ_HIPRI)
 
 #define REQ_DRV			(1ULL << __REQ_DRV)
@@ -461,16 +483,28 @@ static inline bool blk_qc_t_valid(blk_qc_t cookie)
 	return cookie != BLK_QC_T_NONE && cookie != BLK_QC_T_EAGAIN;
 }
 
+/*
+ * 把cookie的第31位(最高位)清空,然后向右移动16位,获得queue num
+ */
 static inline unsigned int blk_qc_t_to_queue_num(blk_qc_t cookie)
 {
+	/*
+	 * ~BLK_QC_T_INTERNAL的第31位是0, 剩下都是1
+	 */
 	return (cookie & ~BLK_QC_T_INTERNAL) >> BLK_QC_T_SHIFT;
 }
 
+/*
+ * 获取cookie的低16位, 表示tag id
+ */
 static inline unsigned int blk_qc_t_to_tag(blk_qc_t cookie)
 {
 	return cookie & ((1u << BLK_QC_T_SHIFT) - 1);
 }
 
+/*
+ * 如果cookie最高位31位设置了,就是INTERNAL
+ */
 static inline bool blk_qc_t_is_internal(blk_qc_t cookie)
 {
 	return (cookie & BLK_QC_T_INTERNAL) != 0;
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4c636c42ad68..9f7cbd6f0964 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -72,6 +72,17 @@ typedef __u32 __bitwise req_flags_t;
 /* may not be passed by ioscheduler */
 #define RQF_SOFTBARRIER		((__force req_flags_t)(1 << 3))
 /* request for flush sequence */
+/*
+ * 在以下使用RQF_FLUSH_SEQ:
+ *   - block/blk-core.c|244| <<req_bio_endio>> if (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))
+ *   - block/blk-core.c|1347| <<blk_account_io_done>> !(req->rq_flags & RQF_FLUSH_SEQ)) {
+ *   - block/blk-flush.c|205| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|426| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|528| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+ *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> if (rq->rq_flags & RQF_FLUSH_SEQ) {
+ *   - block/blk-mq-sched.c|387| <<blk_mq_sched_insert_request>> if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
+ *   - include/linux/blkdev.h|120| <<RQF_NOMERGE_FLAGS>> (RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
+ */
 #define RQF_FLUSH_SEQ		((__force req_flags_t)(1 << 4))
 /* merge of different types, fail separately */
 #define RQF_MIXED_MERGE		((__force req_flags_t)(1 << 5))
@@ -106,6 +117,11 @@ typedef __u32 __bitwise req_flags_t;
 /* The per-zone write lock is held for this request */
 #define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
 /* already slept for hybrid poll */
+/*
+ * 在以下使用RQF_MQ_POLL_SLEPT:
+ *   - block/blk-mq.c|3444| <<blk_mq_poll_hybrid_sleep>> if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
+ *   - block/blk-mq.c|3461| <<blk_mq_poll_hybrid_sleep>> rq->rq_flags |= RQF_MQ_POLL_SLEPT;
+ */
 #define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
 /* ->timeout has been called, don't expire again */
 #define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))
@@ -118,8 +134,31 @@ typedef __u32 __bitwise req_flags_t;
  * Request state for blk-mq.
  */
 enum mq_rq_state {
+	/*
+	 * 使用MQ_RQ_IDLE的地方:
+	 *   - block/blk-mq-debugfs.c|316| <<global>> [MQ_RQ_IDLE] = "idle",
+	 *   - block/blk-mq.c|559| <<blk_mq_free_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|788| <<blk_mq_start_request>> WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|825| <<__blk_mq_requeue_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|2493| <<blk_mq_init_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - include/linux/blk-mq.h|639| <<blk_mq_request_started>> return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
+	 */
 	MQ_RQ_IDLE		= 0,
+	/*
+	 * 使用MQ_RQ_IN_FLIGHT的地方:
+	 *   - block/blk-mq-debugfs.c|317| <<global>> [MQ_RQ_IN_FLIGHT] = "in_flight",
+	 *   - block/blk-mq.c|791| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *   - block/blk-mq.c|1017| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|1059| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 */
 	MQ_RQ_IN_FLIGHT		= 1,
+	/*
+	 * 使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+	 *   - block/blk-mq.c|580| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|3474| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 *   - include/linux/blk-mq.h|481| <<blk_mq_request_completed>> return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
+	 */
 	MQ_RQ_COMPLETE		= 2,
 };
 
@@ -188,6 +227,11 @@ struct request {
 		struct {
 			unsigned int		seq;
 			struct list_head	list;
+			/*
+			 * 使用saved_end_io的地方:
+			 *   - block/blk-flush.c|221| <<blk_flush_restore_request>> rq->end_io = rq->flush.saved_end_io;
+			 *   - block/blk-flush.c|615| <<blk_insert_flush>> rq->flush.saved_end_io = rq->end_io;
+			 */
 			rq_end_io_fn		*saved_end_io;
 		} flush;
 	};
@@ -211,6 +255,12 @@ struct request {
 	 * with blk_rq_sectors(rq), except that it never be zeroed
 	 * by completion.
 	 */
+	/*
+	 * 在以下使用stats_sectors:
+	 *   - block/blk-mq.c|328| <<blk_mq_rq_ctx_init>> rq->stats_sectors = 0;
+	 *   - block/blk-mq.c|681| <<blk_mq_start_request>> rq->stats_sectors = blk_rq_sectors(rq);
+	 *   - include/linux/blkdev.h|970| <<blk_rq_stats_sectors>> return rq->stats_sectors;
+	 */
 	unsigned short stats_sectors;
 
 	/*
@@ -409,7 +459,17 @@ struct request_queue {
 	unsigned int		queue_depth;
 
 	/* hw dispatch queues */
+	/*
+	 * 设置queue_hw_ctx的地方:
+	 *   - block/blk-mq.c|2793| <<blk_mq_realloc_hw_ctxs>> q->queue_hw_ctx = new_hctxs;
+	 */
 	struct blk_mq_hw_ctx	**queue_hw_ctx;
+	/*
+	 * 设置nr_hw_queues的地方:
+	 *   - block/blk-mq.c|3103| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3147| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3233| <<blk_mq_init_allocated_queue>> q->nr_hw_queues = 0;
+	 */
 	unsigned int		nr_hw_queues;
 
 	struct backing_dev_info	*backing_dev_info;
@@ -475,9 +535,41 @@ struct request_queue {
 	unsigned int		dma_alignment;
 
 	unsigned int		rq_timeout;
+	/*
+	 * 设置poll_nsec的地方:
+	 *   - block/blk-mq.c|2906| <<blk_mq_init_allocated_queue>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|384| <<queue_poll_delay_store>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|386| <<queue_poll_delay_store>> q->poll_nsec = val * 1000;
+	 * 使用poll_nsec的地方:
+	 *   - block/blk-mq.c|3460| <<blk_mq_poll_hybrid_sleep>> if (q->poll_nsec > 0)
+	 *   - block/blk-mq.c|3461| <<blk_mq_poll_hybrid_sleep>> nsecs = q->poll_nsec;
+	 *   - block/blk-mq.c|3531| <<blk_mq_poll_hybrid>> if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
+	 *   - block/blk-sysfs.c|363| <<queue_poll_delay_show>> if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
+	 *   - block/blk-sysfs.c|366| <<queue_poll_delay_show>> val = q->poll_nsec / 1000;
+	 */
 	int			poll_nsec;
 
+	/*
+	 * 使用poll_cb的地方:
+	 *   - block/blk-mq.c|2861| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+	 *   - block/blk-mq.c|2864| <<blk_mq_init_allocated_queue>> if (!q->poll_cb)
+	 *   - block/blk-mq.c|2922| <<blk_mq_init_allocated_queue>> blk_stat_free_callback(q->poll_cb);
+	 *   - block/blk-mq.c|2923| <<blk_mq_init_allocated_queue>> q->poll_cb = NULL;
+	 *   - block/blk-mq.c|3354| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+	 *   - block/blk-mq.c|3369| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+	 *   - block/blk-mq.c|3372| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+	 *   - block/blk-sysfs.c|881| <<__blk_release_queue>> blk_stat_remove_callback(q, q->poll_cb);
+	 *   - block/blk-sysfs.c|882| <<__blk_release_queue>> blk_stat_free_callback(q->poll_cb);
+	 */
 	struct blk_stat_callback	*poll_cb;
+	/*
+	 * 使用poll_stat[]的地方:
+	 *   - block/blk-mq-debugfs.c|34| <<queue_poll_stat_show>> print_stat(m, &q->poll_stat[2 * bucket]);
+	 *   - block/blk-mq-debugfs.c|38| <<queue_poll_stat_show>> print_stat(m, &q->poll_stat[2 * bucket + 1]);
+	 *   - block/blk-mq.c|3388| <<blk_mq_poll_stats_fn>> q->poll_stat[bucket] = cb->stat[bucket];
+	 *   - block/blk-mq.c|3425| <<blk_mq_poll_nsecs>> if (q->poll_stat[bucket].nr_samples)
+	 *   - block/blk-mq.c|3426| <<blk_mq_poll_nsecs>> ret = (q->poll_stat[bucket].mean + 1) / 2;
+	 */
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
 	struct timer_list	timeout;
@@ -546,6 +638,14 @@ struct request_queue {
 	struct list_head	unused_hctx_list;
 	spinlock_t		unused_hctx_lock;
 
+	/*
+	 * 在以下使用mq_freeze_depth:
+	 *   - block/blk-core.c|536| <<blk_queue_enter>> (!q->mq_freeze_depth &&
+	 *   - block/blk-mq.c|155| <<blk_freeze_queue_start>> if (++q->mq_freeze_depth == 1) {
+	 *   - block/blk-mq.c|221| <<blk_mq_unfreeze_queue>> q->mq_freeze_depth--;
+	 *   - block/blk-mq.c|222| <<blk_mq_unfreeze_queue>> WARN_ON_ONCE(q->mq_freeze_depth < 0);
+	 *   - block/blk-mq.c|223| <<blk_mq_unfreeze_queue>> if (!q->mq_freeze_depth) {
+	 */
 	int			mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
@@ -557,12 +657,59 @@ struct request_queue {
 	struct throtl_data *td;
 #endif
 	struct rcu_head		rcu_head;
+	/*
+	 * 在以下使用mq_freeze_wq:
+	 *   - block/blk-core.c|310| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|344| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|535| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-core.c|569| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|655| <<blk_alloc_queue_node>> init_waitqueue_head(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|174| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|185| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|225| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	wait_queue_head_t	mq_freeze_wq;
 	/*
 	 * Protect concurrent access to q_usage_counter by
 	 * percpu_ref_kill() and percpu_ref_reinit().
 	 */
+	/*
+	 * 在以下使用mq_freeze_lock:
+	 *   - block/blk-core.c|610| <<blk_alloc_queue_node>> mutex_init(&q->mq_freeze_lock);
+	 *   - block/blk-mq.c|145| <<blk_freeze_queue_start>> mutex_lock(&q->mq_freeze_lock);
+	 *   - block/blk-mq.c|148| <<blk_freeze_queue_start>> mutex_unlock(&q->mq_freeze_lock);
+	 *   - block/blk-mq.c|152| <<blk_freeze_queue_start>> mutex_unlock(&q->mq_freeze_lock);
+	 *   - block/blk-mq.c|207| <<blk_mq_unfreeze_queue>> mutex_lock(&q->mq_freeze_lock);
+	 *   - block/blk-mq.c|214| <<blk_mq_unfreeze_queue>> mutex_unlock(&q->mq_freeze_lock);
+	 */
 	struct mutex		mq_freeze_lock;
+	/*
+	 * 在以下使用request_queue->q_usage_counter:
+	 *   - block/blk-core.c|448| <<blk_cleanup_queue>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-core.c|474| <<blk_queue_enter>> if (percpu_ref_tryget_live(&q->q_usage_counter)) {
+	 *   - block/blk-core.c|483| <<blk_queue_enter>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|515| <<blk_queue_exit>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|521| <<blk_queue_usage_counter_release>> container_of(ref, struct request_queue, q_usage_counter);
+	 *   - block/blk-core.c|616| <<blk_alloc_queue_node>> if (percpu_ref_init(&q->q_usage_counter,
+	 *   - block/blk-core.c|627| <<blk_alloc_queue_node>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-mq-sched.c|459| <<blk_mq_sched_insert_requests>> percpu_ref_get(&q->q_usage_counter);
+	 *   - block/blk-mq-sched.c|489| <<blk_mq_sched_insert_requests>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-mq-tag.c|493| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-mq.c|147| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+	 *   - block/blk-mq.c|165| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|173| <<blk_mq_freeze_queue_wait_timeout>> percpu_ref_is_zero(&q->q_usage_counter),
+	 *   - block/blk-mq.c|211| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+	 *   - block/blk-mq.c|1226| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-pm.c|86| <<blk_pre_runtime_suspend>> percpu_ref_switch_to_atomic_sync(&q->q_usage_counter);
+	 *   - block/blk-pm.c|87| <<blk_pre_runtime_suspend>> if (percpu_ref_is_zero(&q->q_usage_counter))
+	 *   - block/blk-sysfs.c|974| <<blk_register_queue>> percpu_ref_switch_to_percpu(&q->q_usage_counter);
+	 *   - block/blk.h|139| <<blk_queue_enter_live>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/nvdimm/pmem.c|312| <<pmem_pagemap_cleanup>> container_of(pgmap->ref, struct request_queue, q_usage_counter);
+	 *   - drivers/nvdimm/pmem.c|325| <<pmem_pagemap_kill>> container_of(pgmap->ref, struct request_queue, q_usage_counter);
+	 *   - drivers/nvdimm/pmem.c|409| <<pmem_attach_disk>> pmem->pgmap.ref = &q->q_usage_counter;
+	 *   - drivers/scsi/scsi_lib.c|608| <<scsi_end_request>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|618| <<scsi_end_request>> percpu_ref_put(&q->q_usage_counter);
+	 */
 	struct percpu_ref	q_usage_counter;
 
 	struct blk_mq_tag_set	*tag_set;
@@ -586,9 +733,32 @@ struct request_queue {
 };
 
 #define QUEUE_FLAG_STOPPED	0	/* queue is stopped */
+/*
+ * 在以下使用QUEUE_FLAG_DYING:
+ *   - block/blk-core.c|322| <<blk_set_queue_dying>> blk_queue_flag_set(QUEUE_FLAG_DYING, q);
+ *   - block/blk-core.c|412| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_DYING, q);
+ *   - include/linux/blkdev.h|813| <<blk_queue_dying>> #define blk_queue_dying(q) test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_DYING	1	/* queue being torn down */
 #define QUEUE_FLAG_NOMERGES     3	/* disable merge attempts */
+/*
+ * 在以下使用QUEUE_FLAG_SAME_COMP:
+ *   - block/blk-mq.c|870| <<__blk_mq_complete_request>> !test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags)) {
+ *   - block/blk-softirq.c|148| <<__blk_complete_request>> if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags) && ccpu != -1) {
+ *   - block/blk-sysfs.c|328| <<queue_rq_affinity_show>> bool set = test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags);
+ *   - block/blk-sysfs.c|346| <<queue_rq_affinity_store>> blk_queue_flag_set(QUEUE_FLAG_SAME_COMP, q);
+ *   - block/blk-sysfs.c|349| <<queue_rq_affinity_store>> blk_queue_flag_set(QUEUE_FLAG_SAME_COMP, q);
+ *   - block/blk-sysfs.c|352| <<queue_rq_affinity_store>> blk_queue_flag_clear(QUEUE_FLAG_SAME_COMP, q);
+ *   - include/linux/blkdev.h|831| <<QUEUE_FLAG_MQ_DEFAULT>> (1 << QUEUE_FLAG_SAME_COMP))
+ */
 #define QUEUE_FLAG_SAME_COMP	4	/* complete on same CPU-group */
+/*
+ * 在以下使用QUEUE_FLAG_FAIL_IO:
+ *   - block/blk-timeout.c|25| <<blk_should_fake_timeout>> if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
+ *   - block/blk-timeout.c|45| <<part_timeout_show>> int set = test_bit(QUEUE_FLAG_FAIL_IO, &disk->queue->queue_flags);
+ *   - block/blk-timeout.c|62| <<part_timeout_store>> blk_queue_flag_set(QUEUE_FLAG_FAIL_IO, q);
+ *   - block/blk-timeout.c|64| <<part_timeout_store>> blk_queue_flag_clear(QUEUE_FLAG_FAIL_IO, q);
+ */
 #define QUEUE_FLAG_FAIL_IO	5	/* fake timeout */
 #define QUEUE_FLAG_NONROT	6	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT		QUEUE_FLAG_NONROT /* paravirt device */
@@ -598,21 +768,79 @@ struct request_queue {
 #define QUEUE_FLAG_ADD_RANDOM	10	/* Contributes to random pool */
 #define QUEUE_FLAG_SECERASE	11	/* supports secure erase */
 #define QUEUE_FLAG_SAME_FORCE	12	/* force complete on same CPU */
+/*
+ * 在以下使用QUEUE_FLAG_DEAD:
+ *   - block/blk-core.c|366| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_DEAD, q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|152| <<mtip_check_surprise_removal>> blk_queue_flag_set(QUEUE_FLAG_DEAD, dd->queue);
+ *   - include/linux/blkdev.h|749| <<blk_queue_dead>> #define blk_queue_dead(q) test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_DEAD		13	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE	14	/* queue is initialized */
+/*
+ * 使用QUEUE_FLAG_POLL的地方:
+ *   - block/blk-mq.c|3021| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+ *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+ *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+ *   - block/blk-core.c|936| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ *   - block/blk-mq.c|3774| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ *   - drivers/nvme/host/core.c|805| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+ *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+ */
 #define QUEUE_FLAG_POLL		16	/* IO polling enabled if set */
+/*
+ * 在以下设置QUEUE_FLAG_WC的几个例子:
+ *   - block/blk-settings.c|824| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+ *   - block/blk-settings.c|826| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_WC, q)
+ *   - block/blk-sysfs.c|530| <<queue_wc_store>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+ *   - drivers/md/dm-table.c|1905| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_WC))) {
+ */
 #define QUEUE_FLAG_WC		17	/* Write back caching */
+/*
+ * 使用QUEUE_FLAG_FUA的地方:
+ *   - block/blk-flush.c|158| <<blk_flush_policy>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&
+ *   - block/blk-flush.c|471| <<blk_insert_flush>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)))
+ *   - block/blk-settings.c|828| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_FUA, q);
+ *   - block/blk-settings.c|830| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_FUA, q);
+ *   - block/blk-sysfs.c|539| <<queue_fua_show>> return sprintf(page, "%u\n", test_bit(QUEUE_FLAG_FUA, &q->queue_flags));
+ *   - drivers/md/dm-table.c|1907| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_FUA)))
+ *   - drivers/target/target_core_iblock.c|703| <<iblock_execute_rw>> if (test_bit(QUEUE_FLAG_FUA, &q->queue_flags)) {
+ *   - include/linux/blkdev.h|733| <<blk_queue_fua>> #define blk_queue_fua(q) test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 在以下使用QUEUE_FLAG_STATS:
+ *   - block/blk-mq.c|670| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ *   - block/blk-stat.c|320| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|335| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|376| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
+/*
+ * 在以下使用QUEUE_FLAG_POLL_STATS:
+ *   - block/blk-mq.c|3331| <<blk_poll_stats_enable>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+ *   - block/blk-mq.c|3332| <<blk_poll_stats_enable>> blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
+ *   - block/blk-mq.c|3344| <<blk_mq_poll_stats_start>> if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+ *   - block/blk-sysfs.c|880| <<__blk_release_queue>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
+ */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
+/*
+ * 在以下使用QUEUE_FLAG_QUIESCED:
+ *   - block/blk-mq.c|218| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+ *   - block/blk-mq.c|259| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+ *   - include/linux/blkdev.h|771| <<blk_queue_quiesced>> #define blk_queue_quiesced(q) test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 #define QUEUE_FLAG_ZONE_RESETALL 26	/* supports Zone Reset All */
 #define QUEUE_FLAG_RQ_ALLOC_TIME 27	/* record rq->alloc_time_ns */
 
+/*
+ * 在以下使用QUEUE_FLAG_MQ_DEFAULT:
+ *   - block/blk-mq.c|3624| <<blk_mq_init_allocated_queue>> q->queue_flags |= QUEUE_FLAG_MQ_DEFAULT;
+ */
 #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_SAME_COMP))
 
@@ -650,6 +878,13 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
 			     REQ_FAILFAST_DRIVER))
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+ *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+ *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+ */
 #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
 #define blk_queue_pm_only(q)	atomic_read(&(q)->pm_only)
 #define blk_queue_fua(q)	test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
@@ -948,6 +1183,16 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 
 static inline unsigned int blk_rq_stats_sectors(const struct request *rq)
 {
+	/*
+	 * rq sectors used for blk stats. It has the same value
+	 * with blk_rq_sectors(rq), except that it never be zeroed
+	 * by completion.
+	 *
+	 * 在以下使用stats_sectors:
+	 *   - block/blk-mq.c|328| <<blk_mq_rq_ctx_init>> rq->stats_sectors = 0;
+	 *   - block/blk-mq.c|681| <<blk_mq_start_request>> rq->stats_sectors = blk_rq_sectors(rq);
+	 *   - include/linux/blkdev.h|970| <<blk_rq_stats_sectors>> return rq->stats_sectors;
+	 */
 	return rq->stats_sectors;
 }
 
@@ -987,6 +1232,14 @@ static inline struct bio_vec req_bvec(struct request *rq)
 	return mp_bvec_iter_bvec(rq->bio->bi_io_vec, rq->bio->bi_iter);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1236| <<blk_cloned_rq_check_limits>> if (blk_rq_sectors(rq) > blk_queue_get_max_sectors(q, req_op(rq))) {
+ *   - block/blk-core.c|1239| <<blk_cloned_rq_check_limits>> blk_queue_get_max_sectors(q, req_op(rq)));
+ *   - drivers/scsi/cxlflash/vlun.c|433| <<write_same16>> const u32 ws_limit = blk_queue_get_max_sectors(sdev->request_queue,
+ *   - include/linux/blkdev.h|1157| <<blk_rq_get_max_sectors>> return blk_queue_get_max_sectors(q, req_op(rq));
+ *   - include/linux/blkdev.h|1160| <<blk_rq_get_max_sectors>> blk_queue_get_max_sectors(q, req_op(rq)));
+ */
 static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 						     int op)
 {
@@ -1000,6 +1253,7 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 	if (unlikely(op == REQ_OP_WRITE_ZEROES))
 		return q->limits.max_write_zeroes_sectors;
 
+	/* max sectors for a request for this queue */
 	return q->limits.max_sectors;
 }
 
@@ -1010,6 +1264,11 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 static inline unsigned int blk_max_size_offset(struct request_queue *q,
 					       sector_t offset)
 {
+	/*
+	 * 似乎virtio和xen没设置chunk_sectors.
+	 * xen设置了q->limits.max_sectors
+	 * virtio-blk就是-1U
+	 */
 	if (!q->limits.chunk_sectors)
 		return q->limits.max_sectors;
 
@@ -1017,6 +1276,14 @@ static inline unsigned int blk_max_size_offset(struct request_queue *q,
 			(offset & (q->limits.chunk_sectors - 1))));
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|668| <<bio_attempt_discard_merge>> blk_rq_get_max_sectors(req, blk_rq_pos(req)))
+ *   - block/blk-merge.c|600| <<ll_back_merge_fn>> blk_rq_get_max_sectors(req, blk_rq_pos(req))) {
+ *   - block/blk-merge.c|616| <<ll_front_merge_fn>> blk_rq_get_max_sectors(req, bio->bi_iter.bi_sector)) {
+ *   - block/blk-merge.c|632| <<req_attempt_discard_merge>> blk_rq_get_max_sectors(req, blk_rq_pos(req)))
+ *   - block/blk-merge.c|654| <<ll_merge_requests_fn>> blk_rq_get_max_sectors(req, blk_rq_pos(req)))
+ */
 static inline unsigned int blk_rq_get_max_sectors(struct request *rq,
 						  sector_t offset)
 {
@@ -1311,6 +1578,14 @@ static inline unsigned int queue_physical_block_size(const struct request_queue
 	return q->limits.physical_block_size;
 }
 
+/*
+ * called by:
+ *   - block/compat_ioctl.c|339| <<compat_blkdev_ioctl>> return compat_put_uint(arg, bdev_physical_block_size(bdev));
+ *   - block/ioctl.c|530| <<blkdev_ioctl>> return put_uint(arg, bdev_physical_block_size(bdev));
+ *   - drivers/block/xen-blkback/xenbus.c|925| <<connect>> bdev_physical_block_size(be->blkif->vbd.bdev));
+ *   - drivers/md/dm-log-writes.c|898| <<log_writes_io_hints>> limits->physical_block_size = bdev_physical_block_size(lc->dev->bdev);
+ *   - drivers/target/target_core_iblock.c|819| <<iblock_get_lbppbe>> int logs_per_phys = bdev_physical_block_size(bd) / bdev_logical_block_size(bd);
+ */
 static inline unsigned int bdev_physical_block_size(struct block_device *bdev)
 {
 	return queue_physical_block_size(bdev_get_queue(bdev));
@@ -1417,10 +1692,25 @@ static inline unsigned int bdev_write_same(struct block_device *bdev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-lib.c|227| <<__blkdev_issue_write_zeroes>> max_write_zeroes_sectors = bdev_write_zeroes_sectors(bdev);
+ *   - block/blk-lib.c|365| <<blkdev_issue_zeroout>> bool try_write_zeroes = !!bdev_write_zeroes_sectors(bdev);
+ *   - block/blk-lib.c|394| <<blkdev_issue_zeroout>> if (!bdev_write_zeroes_sectors(bdev)) {
+ *   - drivers/md/dm-kcopyd.c|834| <<dm_kcopyd_copy>> if (!bdev_write_zeroes_sectors(job->dests[i].bdev)) {
+ *   - drivers/target/target_core_iblock.c|120| <<iblock_configure_device>> max_write_zeroes_sectors = bdev_write_zeroes_sectors(bd);
+ *   - drivers/target/target_core_iblock.c|471| <<iblock_execute_write_same>> if (bdev_write_zeroes_sectors(bdev)) {
+ *
+ * 返回The maximum number of write zeroes sectors (in 512-byte sectors) in
+ * one segment.
+ */
 static inline unsigned int bdev_write_zeroes_sectors(struct block_device *bdev)
 {
 	struct request_queue *q = bdev_get_queue(bdev);
 
+	/*
+	 * 设置的一处: blk_queue_max_write_zeroes_sectors()
+	 */
 	if (q)
 		return q->limits.max_write_zeroes_sectors;
 
diff --git a/include/linux/fs.h b/include/linux/fs.h
index 98e0349adb52..887db5860145 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -309,6 +309,25 @@ enum rw_hint {
 #define IOCB_EVENTFD		(1 << 0)
 #define IOCB_APPEND		(1 << 1)
 #define IOCB_DIRECT		(1 << 2)
+/*
+ * 设置和取消IOCB_HIPRI的地方:
+ *   - include/linux/fs.h|3427| <<kiocb_set_rw_flags>> ki->ki_flags |= IOCB_HIPRI;
+ *   - fs/io_uring.c|1536| <<io_prep_rw>> kiocb->ki_flags |= IOCB_HIPRI;
+ *   - fs/aio.c|1476| <<aio_prep_rw>> req->ki_flags &= ~IOCB_HIPRI;
+ * 其他使用IOCB_HIPRI的地方:
+ *   - fs/block_dev.c|248| <<__blkdev_direct_IO_simple>> if (iocb->ki_flags & IOCB_HIPRI)
+ *   - fs/block_dev.c|256| <<__blkdev_direct_IO_simple>> if (!(iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/block_dev.c|346| <<__blkdev_direct_IO>> bool is_poll = (iocb->ki_flags & IOCB_HIPRI) != 0;
+ *   - fs/block_dev.c|409| <<__blkdev_direct_IO>> if (iocb->ki_flags & IOCB_HIPRI) {
+ *   - fs/block_dev.c|450| <<__blkdev_direct_IO>> if (!(iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/direct-io.c|501| <<dio_await_one>> if (!(dio->iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/direct-io.c|1250| <<do_blockdev_direct_IO>> if (iocb->ki_flags & IOCB_HIPRI)
+ *   - fs/io_uring.c|1540| <<io_prep_rw>> if (kiocb->ki_flags & IOCB_HIPRI)
+ *   - fs/io_uring.c|1710| <<loop_rw_iter>> if (kiocb->ki_flags & IOCB_HIPRI)
+ *   - fs/iomap/direct-io.c|71| <<iomap_dio_submit_bio>> if (dio->iocb->ki_flags & IOCB_HIPRI)
+ *   - fs/iomap/direct-io.c|584| <<iomap_dio_rw>> if (!(iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/overlayfs/file.c|218| <<ovl_iocb_to_rwf>> if (ifl & IOCB_HIPRI)
+ */
 #define IOCB_HIPRI		(1 << 3)
 #define IOCB_DSYNC		(1 << 4)
 #define IOCB_SYNC		(1 << 5)
@@ -3163,6 +3182,19 @@ ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     dio_iodone_t end_io, dio_submit_t submit_io,
 			     int flags);
 
+/*
+ * called by:
+ *   - drivers/staging/exfat/exfat_super.c|3089| <<exfat_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, exfat_get_block);
+ *   - fs/affs/file.c|409| <<affs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, affs_get_block);
+ *   - fs/ext2/inode.c|948| <<ext2_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, ext2_get_block);
+ *   - fs/fat/inode.c|288| <<fat_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, fat_get_block);
+ *   - fs/hfs/inode.c|137| <<hfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfs_get_block);
+ *   - fs/hfsplus/inode.c|134| <<hfsplus_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfsplus_get_block);
+ *   - fs/jfs/inode.c|342| <<jfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, jfs_get_block);
+ *   - fs/nilfs2/inode.c|303| <<nilfs_direct_IO>> return blockdev_direct_IO(iocb, inode, iter, nilfs_get_block);
+ *   - fs/reiserfs/inode.c|3270| <<reiserfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter,
+ *   - fs/udf/inode.c|224| <<udf_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, udf_get_block);
+ */
 static inline ssize_t blockdev_direct_IO(struct kiocb *iocb,
 					 struct inode *inode,
 					 struct iov_iter *iter,
diff --git a/include/linux/genhd.h b/include/linux/genhd.h
index ea4c133b4139..685bac52c282 100644
--- a/include/linux/genhd.h
+++ b/include/linux/genhd.h
@@ -24,6 +24,11 @@
 #define dev_to_disk(device)	container_of((device), struct gendisk, part0.__dev)
 #define dev_to_part(device)	container_of((device), struct hd_struct, __dev)
 #define disk_to_dev(disk)	(&(disk)->part0.__dev)
+/*
+ * struct hd_struct:
+ *  -> struct device __dev;
+ * 返回hd_struct->__dev
+ */
 #define part_to_dev(part)	(&((part)->__dev))
 
 extern struct device_type part_type;
@@ -137,11 +142,35 @@ struct hd_struct {
 /* 2 is unused */
 #define GENHD_FL_MEDIA_CHANGE_NOTIFY		4
 #define GENHD_FL_CD				8
+/*
+ * 在以下使用GENHD_FL_UP:
+ *   - block/genhd.c|795| <<__device_add_disk>> disk->flags |= GENHD_FL_UP;
+ *   - block/genhd.c|879| <<del_gendisk>> disk->flags &= ~GENHD_FL_UP;
+ *   - block/genhd.c|982| <<get_gendisk>> !(disk->flags & GENHD_FL_UP))) {
+ *   - drivers/block/skd_main.c|3038| <<skd_free_disk>> if (disk && (disk->flags & GENHD_FL_UP))
+ *   - drivers/block/sx8.c|1383| <<carm_free_disk>> if (disk->flags & GENHD_FL_UP)
+ *   - drivers/md/bcache/super.c|793| <<bcache_device_free>> if (disk->flags & GENHD_FL_UP)
+ *   - drivers/md/md.h|749| <<is_mddev_broken>> if (!(flags & GENHD_FL_UP)) {
+ *   - drivers/mmc/core/block.c|2623| <<mmc_blk_remove_req>> if (md->disk->flags & GENHD_FL_UP) {
+ *   - drivers/nvme/host/core.c|4296| <<nvme_ns_remove>> if (ns->disk && ns->disk->flags & GENHD_FL_UP) {
+ *   - drivers/nvme/host/multipath.c|448| <<nvme_mpath_set_live>> if (!(head->disk->flags & GENHD_FL_UP))
+ *   - drivers/nvme/host/multipath.c|715| <<nvme_mpath_remove_disk>> if (head->disk->flags & GENHD_FL_UP)
+ *   - fs/block_dev.c|1659| <<__blkdev_get>> if (!(disk->flags & GENHD_FL_UP) ||
+ */
 #define GENHD_FL_UP				16
 #define GENHD_FL_SUPPRESS_PARTITION_INFO	32
 #define GENHD_FL_EXT_DEVT			64 /* allow extended devt */
 #define GENHD_FL_NATIVE_CAPACITY		128
 #define GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE	256
+/*
+ * 在以下使用GENHD_FL_NO_PART_SCAN:
+ *   - block/genhd.c|799| <<__device_add_disk>> disk->flags |= GENHD_FL_NO_PART_SCAN;
+ *   - drivers/block/loop.c|1193| <<__loop_clr_fd>> lo->lo_disk->flags |= GENHD_FL_NO_PART_SCAN;
+ *   - drivers/block/loop.c|1349| <<loop_set_status>> lo->lo_disk->flags &= ~GENHD_FL_NO_PART_SCAN;
+ *   - drivers/block/loop.c|2063| <<loop_add>> disk->flags |= GENHD_FL_NO_PART_SCAN;
+ *   - drivers/mmc/core/block.c|2322| <<mmc_blk_alloc_req>> md->disk->flags |= GENHD_FL_NO_PART_SCAN
+ *   - include/linux/genhd.h|250| <<disk_part_scan_enabled>> !(disk->flags & GENHD_FL_NO_PART_SCAN);
+ */
 #define GENHD_FL_NO_PART_SCAN			512
 #define GENHD_FL_HIDDEN				1024
 
@@ -245,6 +274,20 @@ static inline bool disk_part_scan_enabled(struct gendisk *disk)
 		!(disk->flags & GENHD_FL_NO_PART_SCAN);
 }
 
+/*
+ * 这里似乎有bug, 根据最新的一个fix:
+ * Commit b72053072c0b ("block: allow partitions on host aware zone
+ * devices") introduced the helper function disk_has_partitions() to check
+ * if a given disk has valid partitions. However, since this function result
+ * directly depends on the disk partition table length rather than the
+ * actual existence of valid partitions in the table, it returns true even
+ * after all partitions are removed from the disk. For host aware zoned
+ * block devices, this results in zone management support to be kept
+ * disabled even after removing all partitions.
+ *
+ * called by:
+ *   - drivers/scsi/sd.c|2961| <<sd_read_block_characteristics>> if (sdkp->zoned == 1 && !disk_has_partitions(sdkp->disk)) {
+ */
 static inline bool disk_has_partitions(struct gendisk *disk)
 {
 	bool ret = false;
@@ -384,6 +427,12 @@ static inline void free_part_stats(struct hd_struct *part)
 	 part_stat_read(part, field[STAT_WRITE]) +			\
 	 part_stat_read(part, field[STAT_DISCARD]))
 
+/*
+ * called by:
+ *   - block/bio.c|1762| <<update_io_ticks>> __part_stat_add(part, io_ticks, 1);
+ *   - include/linux/genhd.h|419| <<part_stat_add>> __part_stat_add((part), field, addnd); \
+ *   - include/linux/genhd.h|421| <<part_stat_add>> __part_stat_add(&part_to_disk((part))->part0, \
+ */
 #define __part_stat_add(part, field, addnd)				\
 	(part_stat_get(part, field) += (addnd))
 
@@ -671,6 +720,16 @@ extern ssize_t part_fail_store(struct device *dev,
 			       const char *buf, size_t count);
 #endif /* CONFIG_FAIL_MAKE_REQUEST */
 
+/*
+ * called by:
+ *   - drivers/block/mtip32xx/mtip32xx.c|3591| <<mtip_block_initialize>> dd->disk = alloc_disk_node(MTIP_MAX_MINORS, dd->numa_node);
+ *   - drivers/block/null_blk_main.c|1565| <<null_gendisk_register>> disk = nullb->disk = alloc_disk_node(1, nullb->dev->home_node);
+ *   - drivers/ide/ide-gd.c|389| <<ide_gd_probe>> g = alloc_disk_node(IDE_DISK_MINORS, hwif_to_node(drive->hwif));
+ *   - drivers/md/dm.c|1954| <<alloc_dev>> md->disk = alloc_disk_node(1, md->numa_node_id);
+ *   - drivers/nvdimm/pmem.c|451| <<pmem_attach_disk>> disk = alloc_disk_node(0, nid);
+ *   - drivers/nvme/host/core.c|3533| <<nvme_alloc_ns>> disk = alloc_disk_node(0, node);
+ *   - include/linux/genhd.h|690| <<alloc_disk>> #define alloc_disk(minors) alloc_disk_node(minors, NUMA_NO_NODE)
+ */
 #define alloc_disk_node(minors, node_id)				\
 ({									\
 	static struct lock_class_key __key;				\
@@ -712,6 +771,10 @@ static inline void hd_struct_put(struct hd_struct *part)
 	percpu_ref_put(&part->ref);
 }
 
+/*
+ * called by:
+ *   - block/partition-generic.c|298| <<delete_partition>> hd_struct_kill(part);
+ */
 static inline void hd_struct_kill(struct hd_struct *part)
 {
 	percpu_ref_kill(&part->ref);
diff --git a/include/linux/irq.h b/include/linux/irq.h
index 7853eb9301f2..345b5be6a904 100644
--- a/include/linux/irq.h
+++ b/include/linux/irq.h
@@ -213,6 +213,13 @@ struct irq_data {
 enum {
 	IRQD_TRIGGER_MASK		= 0xf,
 	IRQD_SETAFFINITY_PENDING	= (1 <<  8),
+	/*
+	 * 在以下使用IRQD_ACTIVATED:
+	 *   - kernel/irq/debugfs.c|101| <<global>> BIT_MASK_DESCR(IRQD_ACTIVATED),
+	 *   - include/linux/irq.h|374| <<irqd_is_activated>> return __irqd_to_state(d) & IRQD_ACTIVATED;
+	 *   - include/linux/irq.h|379| <<irqd_set_activated>> __irqd_to_state(d) |= IRQD_ACTIVATED;
+	 *   - include/linux/irq.h|384| <<irqd_clr_activated>> __irqd_to_state(d) &= ~IRQD_ACTIVATED;
+	 */
 	IRQD_ACTIVATED			= (1 <<  9),
 	IRQD_NO_BALANCING		= (1 << 10),
 	IRQD_PER_CPU			= (1 << 11),
@@ -225,6 +232,12 @@ enum {
 	IRQD_IRQ_INPROGRESS		= (1 << 18),
 	IRQD_WAKEUP_ARMED		= (1 << 19),
 	IRQD_FORWARDED_TO_VCPU		= (1 << 20),
+	/*
+	 * 在以下使用IRQD_AFFINITY_MANAGED:
+	 *   - kernel/irq/debugfs.c|114| <<global>> BIT_MASK_DESCR(IRQD_AFFINITY_MANAGED),
+	 *   - include/linux/irq.h|350| <<irqd_affinity_is_managed>> return __irqd_to_state(d) & IRQD_AFFINITY_MANAGED;
+	 *   - kernel/irq/irqdesc.c|487| <<alloc_descs>> flags = IRQD_AFFINITY_MANAGED |
+	 */
 	IRQD_AFFINITY_MANAGED		= (1 << 21),
 	IRQD_IRQ_STARTED		= (1 << 22),
 	IRQD_MANAGED_SHUTDOWN		= (1 << 23),
@@ -345,6 +358,19 @@ static inline void irqd_clr_forwarded_to_vcpu(struct irq_data *d)
 	__irqd_to_state(d) &= ~IRQD_FORWARDED_TO_VCPU;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|136| <<apic_update_vector>> bool managed = irqd_affinity_is_managed(irqd);
+ *   - arch/x86/kernel/apic/vector.c|296| <<assign_irq_vector_policy>> if (irqd_affinity_is_managed(irqd))
+ *   - arch/x86/kernel/apic/vector.c|332| <<clear_irq_vector>> bool managed = irqd_affinity_is_managed(irqd);
+ *   - arch/x86/kernel/apic/vector.c|789| <<apic_set_affinity>> if (irqd_affinity_is_managed(irqd))
+ *   - kernel/irq/chip.c|198| <<__irq_startup_managed>> if (!irqd_affinity_is_managed(d))
+ *   - kernel/irq/chip.c|290| <<irq_activate>> if (!irqd_affinity_is_managed(d))
+ *   - kernel/irq/cpuhotplug.c|117| <<migrate_one_irq>> if (irqd_affinity_is_managed(d)) {
+ *   - kernel/irq/cpuhotplug.c|179| <<irq_restore_affinity_of_irq>> if (!irqd_affinity_is_managed(data) || !desc->action ||
+ *   - kernel/irq/manage.c|176| <<irq_can_set_affinity_usr>> !irqd_affinity_is_managed(&desc->irq_data);
+ *   - kernel/irq/manage.c|415| <<irq_setup_affinity>> if (irqd_affinity_is_managed(&desc->irq_data) ||
+ */
 static inline bool irqd_affinity_is_managed(struct irq_data *d)
 {
 	return __irqd_to_state(d) & IRQD_AFFINITY_MANAGED;
@@ -352,9 +378,21 @@ static inline bool irqd_affinity_is_managed(struct irq_data *d)
 
 static inline bool irqd_is_activated(struct irq_data *d)
 {
+	/*
+	 * 在以下使用IRQD_ACTIVATED:
+	 *   - kernel/irq/debugfs.c|101| <<global>> BIT_MASK_DESCR(IRQD_ACTIVATED),
+	 *   - include/linux/irq.h|374| <<irqd_is_activated>> return __irqd_to_state(d) & IRQD_ACTIVATED;
+	 *   - include/linux/irq.h|379| <<irqd_set_activated>> __irqd_to_state(d) |= IRQD_ACTIVATED;
+	 *   - include/linux/irq.h|384| <<irqd_clr_activated>> __irqd_to_state(d) &= ~IRQD_ACTIVATED;
+	 */
 	return __irqd_to_state(d) & IRQD_ACTIVATED;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/internals.h|461| <<irq_domain_activate_irq>> irqd_set_activated(data);
+ *   - kernel/irq/irqdomain.c|1655| <<irq_domain_activate_irq>> irqd_set_activated(irq_data);
+ */
 static inline void irqd_set_activated(struct irq_data *d)
 {
 	__irqd_to_state(d) |= IRQD_ACTIVATED;
@@ -825,6 +863,13 @@ static inline struct cpumask *irq_get_affinity_mask(int irq)
 
 static inline struct cpumask *irq_data_get_affinity_mask(struct irq_data *d)
 {
+	/*
+	 * struct irq_common_data  irq_common_data;
+	 *   - cpumask_var_t           affinity;
+	 *   - cpumask_var_t           effective_affinity;
+	 * struct irq_data         irq_data;
+	 *   - struct irq_common_data  *common;
+	 */
 	return d->common->affinity;
 }
 
@@ -834,6 +879,29 @@ struct cpumask *irq_data_get_effective_affinity_mask(struct irq_data *d)
 {
 	return d->common->effective_affinity;
 }
+/*
+ * [0] irq_data_update_effective_affinity()
+ * [0] apic_update_irq_cfg
+ * [0] assign_managed_vector.constprop.27
+ * [0] x86_vector_activate
+ * [0] __irq_domain_activate_irq
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * x86调用irq_data_update_effective_affinity()的例子:
+ *   - arch/x86/kernel/apic/vector.c|126| <<apic_update_irq_cfg>> irq_data_update_effective_affinity(irqd, cpumask_of(cpu));
+ *   - drivers/xen/events/events_base.c|1329| <<set_affinity_irq>> irq_data_update_effective_affinity(data, cpumask_of(tcpu));
+ */
 static inline void irq_data_update_effective_affinity(struct irq_data *d,
 						      const struct cpumask *m)
 {
diff --git a/include/linux/irqdesc.h b/include/linux/irqdesc.h
index d6e2ab538ef2..6364918a9402 100644
--- a/include/linux/irqdesc.h
+++ b/include/linux/irqdesc.h
@@ -129,6 +129,13 @@ static inline unsigned int irq_desc_get_irq(struct irq_desc *desc)
 
 static inline struct irq_data *irq_desc_get_irq_data(struct irq_desc *desc)
 {
+	/*
+	 * struct irq_common_data  irq_common_data;
+	 *   - cpumask_var_t           affinity;
+	 *   - cpumask_var_t           effective_affinity;
+	 * struct irq_data         irq_data;
+	 *   - struct irq_common_data  *common;
+	 */
 	return &desc->irq_data;
 }
 
diff --git a/include/linux/nvme-fc.h b/include/linux/nvme-fc.h
index e8c30b39bb27..c1810b0de86d 100644
--- a/include/linux/nvme-fc.h
+++ b/include/linux/nvme-fc.h
@@ -424,12 +424,36 @@ struct fcnvme_ls_disconnect_conn_acc {
 #define NVME_FC_TRADDR_NNLEN		3	/* "?n-" */
 #define NVME_FC_TRADDR_OXNNLEN		5	/* "?n-0x" */
 #define NVME_FC_TRADDR_HEXNAMELEN	16
+/*
+ * 在以下使用NVME_FC_TRADDR_MINLENGTH:
+ *   - drivers/nvme/host/fc.c|3378| <<nvme_fc_parse_traddr>> } else if ((strnlen(buf, blen) == NVME_FC_TRADDR_MINLENGTH &&
+ *   - drivers/nvme/target/fc.c|2500| <<nvme_fc_parse_traddr>> } else if ((strnlen(buf, blen) == NVME_FC_TRADDR_MINLENGTH &&
+ */
 #define NVME_FC_TRADDR_MINLENGTH	\
 		(2 * (NVME_FC_TRADDR_NNLEN + NVME_FC_TRADDR_HEXNAMELEN) + 1)
+/*
+ * 在以下使用NVME_FC_TRADDR_MAXLENGTH:
+ *   - drivers/nvme/host/fc.c|3371| <<nvme_fc_parse_traddr>> if (strnlen(buf, blen) == NVME_FC_TRADDR_MAXLENGTH &&
+ *   - drivers/nvme/target/fc.c|2493| <<nvme_fc_parse_traddr>> if (strnlen(buf, blen) == NVME_FC_TRADDR_MAXLENGTH &&
+ */
 #define NVME_FC_TRADDR_MAXLENGTH	\
 		(2 * (NVME_FC_TRADDR_OXNNLEN + NVME_FC_TRADDR_HEXNAMELEN) + 1)
+/*
+ * 在以下使用NVME_FC_TRADDR_MIN_PN_OFFSET:
+ *   - drivers/nvme/host/fc.c|3380| <<nvme_fc_parse_traddr>> !strncmp(&buf[NVME_FC_TRADDR_MIN_PN_OFFSET],
+ *   - drivers/nvme/host/fc.c|3383| <<nvme_fc_parse_traddr>> pnoffset = NVME_FC_TRADDR_MIN_PN_OFFSET + NVME_FC_TRADDR_NNLEN;
+ *   - drivers/nvme/target/fc.c|2502| <<nvme_fc_parse_traddr>> !strncmp(&buf[NVME_FC_TRADDR_MIN_PN_OFFSET],
+ *   - drivers/nvme/target/fc.c|2505| <<nvme_fc_parse_traddr>> pnoffset = NVME_FC_TRADDR_MIN_PN_OFFSET + NVME_FC_TRADDR_NNLEN;
+ */
 #define NVME_FC_TRADDR_MIN_PN_OFFSET	\
 		(NVME_FC_TRADDR_NNLEN + NVME_FC_TRADDR_HEXNAMELEN + 1)
+/*
+ * 在以下使用NVME_FC_TRADDR_MAX_PN_OFFSET:
+ *   - drivers/nvme/host/fc.c|3373| <<nvme_fc_parse_traddr>> !strncmp(&buf[NVME_FC_TRADDR_MAX_PN_OFFSET],
+ *   - drivers/nvme/host/fc.c|3376| <<nvme_fc_parse_traddr>> pnoffset = NVME_FC_TRADDR_MAX_PN_OFFSET +
+ *   - drivers/nvme/target/fc.c|2495| <<nvme_fc_parse_traddr>> !strncmp(&buf[NVME_FC_TRADDR_MAX_PN_OFFSET],
+ *   - drivers/nvme/target/fc.c|2498| <<nvme_fc_parse_traddr>> pnoffset = NVME_FC_TRADDR_MAX_PN_OFFSET +
+ */
 #define NVME_FC_TRADDR_MAX_PN_OFFSET	\
 		(NVME_FC_TRADDR_OXNNLEN + NVME_FC_TRADDR_HEXNAMELEN + 1)
 
diff --git a/include/linux/nvme.h b/include/linux/nvme.h
index 3d5189f46cb1..3f34a8a1f457 100644
--- a/include/linux/nvme.h
+++ b/include/linux/nvme.h
@@ -20,6 +20,15 @@
 #define NVMF_TRADDR_SIZE	256
 #define NVMF_TSAS_SIZE		256
 
+/*
+ * 在以下使用NVME_DISC_SUBSYS_NAME:
+ *   - drivers/nvme/host/core.c|3618| <<__nvme_find_get_subsystem>> if (!strcmp(subsysnqn, NVME_DISC_SUBSYS_NAME))
+ *   - drivers/nvme/host/fabrics.c|684| <<nvmf_parse_options>> NVME_DISC_SUBSYS_NAME));
+ *   - drivers/nvme/target/configfs.c|916| <<nvmet_subsys_make>> if (sysfs_streq(name, NVME_DISC_SUBSYS_NAME)) {
+ *   - drivers/nvme/target/core.c|1398| <<nvmet_find_get_subsys>> if (!strcmp(NVME_DISC_SUBSYS_NAME, subsysnqn)) {
+ *   - drivers/nvme/target/discovery.c|220| <<nvmet_execute_disc_get_log_page>> NVME_DISC_SUBSYS_NAME,
+ *   - drivers/nvme/target/discovery.c|389| <<nvmet_init_discovery>> nvmet_subsys_alloc(NVME_DISC_SUBSYS_NAME, NVME_NQN_DISC);
+ */
 #define NVME_DISC_SUBSYS_NAME	"nqn.2014-08.org.nvmexpress.discovery"
 
 #define NVME_RDMA_IP_PORT	4420
@@ -86,14 +95,61 @@ enum {
 	NVMF_RDMA_CMS_RDMA_CM	= 1, /* Sockets based endpoint addressing */
 };
 
+/*
+ * 在以下使用NVME_AQ_DEPTH:
+ *   - drivers/nvme/host/fabrics.c|378| <<nvmf_connect_admin_queue>> cmd.connect.sqsize = cpu_to_le16(NVME_AQ_DEPTH - 1);
+ *   - drivers/nvme/host/fc.c|2650| <<nvme_fc_create_association>> NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/fc.c|2655| <<nvme_fc_create_association>> NVME_AQ_DEPTH, (NVME_AQ_DEPTH / 4));
+ *   - drivers/nvme/host/pci.c|2507| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/pci.c|3372| <<nvme_dev_add>> dev->tagset.reserved_tags = NVME_AQ_DEPTH;
+ *   - drivers/nvme/host/pci.c|3514| <<nvme_pci_enable>> (dev->q_depth < (NVME_AQ_DEPTH + 2))) {
+ *   - drivers/nvme/host/pci.c|3515| <<nvme_pci_enable>> dev->q_depth = NVME_AQ_DEPTH + 2;
+ *   - drivers/nvme/host/rdma.c|783| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/rdma.c|1605| <<nvme_rdma_route_resolved>> priv.hrqsize = cpu_to_le16(NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/rdma.c|1606| <<nvme_rdma_route_resolved>> priv.hsqsize = cpu_to_le16(NVME_AQ_DEPTH - 1);
+ *   - drivers/nvme/host/tcp.c|1548| <<nvme_tcp_alloc_admin_queue>> ret = nvme_tcp_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/target/discovery.c|116| <<nvmet_format_discovery_entry>> e->asqsz = cpu_to_le16(NVME_AQ_DEPTH);
+ *   - drivers/nvme/target/rdma.c|1114| <<nvmet_rdma_parse_cm_connect_req>> if (!queue->host_qid && queue->recv_queue_size > NVME_AQ_DEPTH)
+ *   - include/linux/nvme.h|92| <<NVME_AQ_BLK_MQ_DEPTH>> #define NVME_AQ_BLK_MQ_DEPTH (NVME_AQ_DEPTH - NVME_NR_AEN_COMMANDS)
+ */
 #define NVME_AQ_DEPTH		32
+/*
+ * 在以下使用NVME_NR_AEN_COMMANDS:
+ *   - drivers/nvme/host/fc.c|1529| <<nvme_fc_abort_aen_ops>> for (i = 0; i < NVME_NR_AEN_COMMANDS; i++, aen_op++)
+ *   - drivers/nvme/host/fc.c|1791| <<nvme_fc_init_aen_ops>> for (i = 0; i < NVME_NR_AEN_COMMANDS; i++, aen_op++) {
+ *   - drivers/nvme/host/fc.c|1825| <<nvme_fc_term_aen_ops>> for (i = 0; i < NVME_NR_AEN_COMMANDS; i++, aen_op++) {
+ *   - include/linux/nvme.h|92| <<NVME_AQ_BLK_MQ_DEPTH>> #define NVME_AQ_BLK_MQ_DEPTH (NVME_AQ_DEPTH - NVME_NR_AEN_COMMANDS)
+ */
 #define NVME_NR_AEN_COMMANDS	1
+/*
+ * 在以下使用NVME_AQ_BLK_MQ_DEPTH:
+ *   - drivers/nvme/host/fc.c|1801| <<nvme_fc_init_aen_ops>> (NVME_AQ_BLK_MQ_DEPTH + i));
+ *   - drivers/nvme/host/fc.c|1813| <<nvme_fc_init_aen_ops>> sqe->common.command_id = NVME_AQ_BLK_MQ_DEPTH + i;
+ *   - drivers/nvme/host/nvme.h|619| <<nvme_is_aen_req>> return !qid && command_id >= NVME_AQ_BLK_MQ_DEPTH;
+ *   - drivers/nvme/host/pci.c|1685| <<nvme_pci_submit_async_event>> c.common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+ *   - drivers/nvme/host/rdma.c|1424| <<nvme_rdma_submit_async_event>> cmd->common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+ *   - drivers/nvme/host/tcp.c|2030| <<nvme_tcp_submit_async_event>> cmd->common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+ *   - drivers/nvme/target/loop.c|181| <<nvme_loop_submit_async_event>> iod->cmd.common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+ *   - include/linux/nvme.h|108| <<NVME_AQ_MQ_TAG_DEPTH>> #define NVME_AQ_MQ_TAG_DEPTH (NVME_AQ_BLK_MQ_DEPTH - 1)
+ *
+ * 32 - 1 = 31
+ */
 #define NVME_AQ_BLK_MQ_DEPTH	(NVME_AQ_DEPTH - NVME_NR_AEN_COMMANDS)
 
 /*
  * Subtract one to leave an empty queue entry for 'Full Queue' condition. See
  * NVM-Express 1.2 specification, section 4.1.2.
  */
+/*
+ * 在以下使用NVME_AQ_MQ_TAG_DEPTH:
+ *   - drivers/nvme/host/fc.c|3132| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+ *   - drivers/nvme/host/pci.c|2394| <<nvme_alloc_admin_tags>> dev->admin_tagset.queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+ *   - drivers/nvme/host/rdma.c|730| <<nvme_rdma_alloc_tagset>> set->queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+ *   - drivers/nvme/host/tcp.c|1471| <<nvme_tcp_alloc_tagset>> set->queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+ *   - drivers/nvme/target/loop.c|341| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+ *
+ * 31 - 1 = 30
+ */
 #define NVME_AQ_MQ_TAG_DEPTH	(NVME_AQ_BLK_MQ_DEPTH - 1)
 
 enum {
@@ -101,8 +157,36 @@ enum {
 	NVME_REG_VS	= 0x0008,	/* Version */
 	NVME_REG_INTMS	= 0x000c,	/* Interrupt Mask Set */
 	NVME_REG_INTMC	= 0x0010,	/* Interrupt Mask Clear */
+	/*
+	 * 在以下使用NVME_REG_CC:
+	 *   - drivers/nvme/host/core.c|2195| <<nvme_disable_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|2238| <<nvme_enable_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|2254| <<nvme_shutdown_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/target/fabrics-cmd.c|26| <<nvmet_execute_prop_set>> case NVME_REG_CC:
+	 *   - drivers/nvme/target/fabrics-cmd.c|61| <<nvmet_execute_prop_get>> case NVME_REG_CC:
+	 *   - drivers/pci/quirks.c|3822| <<nvme_disable_and_flr>> bar = pci_iomap(dev, 0, NVME_REG_CC + sizeof(cfg));
+	 *   - drivers/pci/quirks.c|3829| <<nvme_disable_and_flr>> cfg = readl(bar + NVME_REG_CC);
+	 *   - drivers/pci/quirks.c|3843| <<nvme_disable_and_flr>> writel(cfg, bar + NVME_REG_CC);
+	 */
 	NVME_REG_CC	= 0x0014,	/* Controller Configuration */
+	/*
+	 * 在以下使用NVME_REG_CSTS:
+	 *   - drivers/nvme/host/core.c|2102| <<nvme_wait_ready>> while ((ret = ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts)) == 0) {
+	 *   - drivers/nvme/host/core.c|2198| <<nvme_shutdown_ctrl>> while ((ret = ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts)) == 0) {
+	 *   - drivers/nvme/host/core.c|3884| <<nvme_ctrl_pp_status>> if (ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts))
+	 *   - drivers/nvme/host/pci.c|1263| <<nvme_timeout>> u32 csts = readl(dev->bar + NVME_REG_CSTS);
+	 *   - drivers/nvme/host/pci.c|1705| <<nvme_pci_configure_admin_queue>> (readl(dev->bar + NVME_REG_CSTS) & NVME_CSTS_NSSRO))
+	 *   - drivers/nvme/host/pci.c|1706| <<nvme_pci_configure_admin_queue>> writel(NVME_CSTS_NSSRO, dev->bar + NVME_REG_CSTS);
+	 *   - drivers/nvme/host/pci.c|2358| <<nvme_pci_enable>> if (readl(dev->bar + NVME_REG_CSTS) == -1) {
+	 *   - drivers/nvme/host/pci.c|2456| <<nvme_dev_disable>> u32 csts = readl(dev->bar + NVME_REG_CSTS);
+	 *   - drivers/nvme/target/fabrics-cmd.c|64| <<nvmet_execute_prop_get>> case NVME_REG_CSTS:
+	 *   - drivers/pci/quirks.c|3855| <<nvme_disable_and_flr>> u32 status = readl(bar + NVME_REG_CSTS);
+	 */
 	NVME_REG_CSTS	= 0x001c,	/* Controller Status */
+	/*
+	 * 只在以下使用NVME_REG_NSSR:
+	 *   - drivers/nvme/host/nvme.h|830| <<nvme_reset_subsystem>> return ctrl->ops->reg_write32(ctrl, NVME_REG_NSSR, 0x4E564D65);
+	 */
 	NVME_REG_NSSR	= 0x0020,	/* NVM Subsystem Reset */
 	NVME_REG_AQA	= 0x0024,	/* Admin Queue Attributes */
 	NVME_REG_ASQ	= 0x0028,	/* Admin SQ Base Address */
@@ -212,6 +296,11 @@ enum {
 
 enum nvme_ctrl_attr {
 	NVME_CTRL_ATTR_HID_128_BIT	= (1 << 0),
+	/*
+	 * 在以下使用NVME_CTRL_ATTR_TBKAS:
+	 *   - drivers/nvme/host/core.c|1350| <<nvme_keep_alive_work>> if ((ctrl->ctratt & NVME_CTRL_ATTR_TBKAS) && comp_seen) {
+	 *   - drivers/nvme/target/admin-cmd.c|367| <<nvmet_execute_identify_ctrl>> NVME_CTRL_ATTR_TBKAS);
+	 */
 	NVME_CTRL_ATTR_TBKAS		= (1 << 6),
 };
 
@@ -465,10 +554,25 @@ struct nvme_fw_slot_info_log {
 
 enum {
 	NVME_CMD_EFFECTS_CSUPP		= 1 << 0,
+	/*
+	 * 在以下使用NVME_CMD_EFFECTS_LBCC:
+	 *   - drivers/nvme/host/core.c|2067| <<nvme_known_admin_effects>> return NVME_CMD_EFFECTS_CSUPP | NVME_CMD_EFFECTS_LBCC |
+	 *   - drivers/nvme/host/core.c|2113| <<nvme_passthru_start>> if (effects & ~(NVME_CMD_EFFECTS_CSUPP | NVME_CMD_EFFECTS_LBCC))
+	 *   - drivers/nvme/host/core.c|2128| <<nvme_passthru_start>> if (effects & (NVME_CMD_EFFECTS_LBCC | NVME_CMD_EFFECTS_CSE_MASK)) {
+	 *   - drivers/nvme/host/core.c|2166| <<nvme_passthru_end>> if (effects & NVME_CMD_EFFECTS_LBCC)
+	 *   - drivers/nvme/host/core.c|2168| <<nvme_passthru_end>> if (effects & (NVME_CMD_EFFECTS_LBCC | NVME_CMD_EFFECTS_CSE_MASK)) {
+	 */
 	NVME_CMD_EFFECTS_LBCC		= 1 << 1,
 	NVME_CMD_EFFECTS_NCC		= 1 << 2,
 	NVME_CMD_EFFECTS_NIC		= 1 << 3,
 	NVME_CMD_EFFECTS_CCC		= 1 << 4,
+	/*
+	 * 在以下使用NVME_CMD_EFFECTS_CSE_MASK:
+	 *   - drivers/nvme/host/core.c|2068| <<nvme_known_admin_effects>> NVME_CMD_EFFECTS_CSE_MASK;
+	 *   - drivers/nvme/host/core.c|2070| <<nvme_known_admin_effects>> return NVME_CMD_EFFECTS_CSE_MASK;
+	 *   - drivers/nvme/host/core.c|2128| <<nvme_passthru_start>> if (effects & (NVME_CMD_EFFECTS_LBCC | NVME_CMD_EFFECTS_CSE_MASK)) {
+	 *   - drivers/nvme/host/core.c|2168| <<nvme_passthru_end>> if (effects & (NVME_CMD_EFFECTS_LBCC | NVME_CMD_EFFECTS_CSE_MASK)) {
+	 */
 	NVME_CMD_EFFECTS_CSE_MASK	= 3 << 16,
 	NVME_CMD_EFFECTS_UUID_SEL	= 1 << 19,
 };
@@ -484,6 +588,14 @@ enum nvme_ana_state {
 	NVME_ANA_NONOPTIMIZED		= 0x02,
 	NVME_ANA_INACCESSIBLE		= 0x03,
 	NVME_ANA_PERSISTENT_LOSS	= 0x04,
+	/*
+	 * 在以下使用NVME_ANA_CHANGE:
+	 *   - drivers/nvme/host/multipath.c|189| <<global>> [NVME_ANA_CHANGE] = "change",
+	 *   - drivers/nvme/target/configfs.c|1024| <<global>> { NVME_ANA_CHANGE, "change" },
+	 *   - drivers/nvme/host/multipath.c|592| <<nvme_parse_ana_log>> if (WARN_ON_ONCE(desc->state > NVME_ANA_CHANGE))
+	 *   - drivers/nvme/host/multipath.c|664| <<nvme_update_ana_state>> if (desc->state == NVME_ANA_CHANGE)
+	 *   - drivers/nvme/target/core.c|809| <<nvmet_check_ana_state>> if (unlikely(state == NVME_ANA_CHANGE))
+	 */
 	NVME_ANA_CHANGE			= 0x0f,
 };
 
@@ -522,9 +634,34 @@ enum {
 };
 
 enum {
+	/*
+	 * 在以下使用NVME_AER_NOTICE_NS_CHANGED:
+	 *   - drivers/nvme/host/core.c|4949| <<nvme_scan_work>> if (test_and_clear_bit(NVME_AER_NOTICE_NS_CHANGED, &ctrl->events)) {
+	 *   - drivers/nvme/host/core.c|5220| <<nvme_handle_aen_notice>> case NVME_AER_NOTICE_NS_CHANGED:
+	 *   - drivers/nvme/host/core.c|5221| <<nvme_handle_aen_notice>> set_bit(NVME_AER_NOTICE_NS_CHANGED, &ctrl->events);
+	 *   - drivers/nvme/host/trace.h|131| <<__field>> aer_name(NVME_AER_NOTICE_NS_CHANGED),
+	 *   - drivers/nvme/target/core.c|231| <<nvmet_ns_changed>> NVME_AER_NOTICE_NS_CHANGED,
+	 */
 	NVME_AER_NOTICE_NS_CHANGED	= 0x00,
+	/*
+	 * 在以下使用NVME_AER_NOTICE_FW_ACT_STARTING:
+	 *   - drivers/nvme/host/core.c|5327| <<nvme_handle_aen_notice>> case NVME_AER_NOTICE_FW_ACT_STARTING:
+	 *   - drivers/nvme/host/trace.h|133| <<__field>> aer_name(NVME_AER_NOTICE_FW_ACT_STARTING),
+	 */
 	NVME_AER_NOTICE_FW_ACT_STARTING = 0x01,
+	/*
+	 * 在以下使用NVME_AER_NOTICE_ANA:
+	 *   - drivers/nvme/host/core.c|5032| <<nvme_handle_aen_notice>> case NVME_AER_NOTICE_ANA:
+	 *   - drivers/nvme/host/trace.h|132| <<__field>> aer_name(NVME_AER_NOTICE_ANA),
+	 *   - drivers/nvme/target/core.c|248| <<nvmet_send_ana_event>> NVME_AER_NOTICE_ANA, NVME_LOG_ANA);
+	 */
 	NVME_AER_NOTICE_ANA		= 0x03,
+	/*
+	 * 在以下使用NVME_AER_NOTICE_DISC_CHANGED:
+	 *   - drivers/nvme/host/core.c|5306| <<nvme_handle_aen_notice>> case NVME_AER_NOTICE_DISC_CHANGED:
+	 *   - drivers/nvme/host/trace.h|134| <<__field>> aer_name(NVME_AER_NOTICE_DISC_CHANGED),
+	 *   - drivers/nvme/target/discovery.c|25| <<__nvmet_disc_changed>> NVME_AER_NOTICE_DISC_CHANGED, NVME_LOG_DISC);
+	 */
 	NVME_AER_NOTICE_DISC_CHANGED	= 0xf0,
 };
 
@@ -833,6 +970,14 @@ enum {
 enum nvme_admin_opcode {
 	nvme_admin_delete_sq		= 0x00,
 	nvme_admin_create_sq		= 0x01,
+	/*
+	 * 在以下使用nvme_admin_get_log_page:
+	 *   - drivers/nvme/host/core.c|3481| <<nvme_get_log>> c.get_log_page.opcode = nvme_admin_get_log_page;
+	 *   - drivers/nvme/target/admin-cmd.c|182| <<nvmet_execute_get_log_cmd_effects_ns>> log->acs[nvme_admin_get_log_page] = cpu_to_le32(1 << 0);
+	 *   - drivers/nvme/target/admin-cmd.c|902| <<nvmet_parse_admin_cmd>> case nvme_admin_get_log_page:
+	 *   - drivers/nvme/target/discovery.c|372| <<nvmet_parse_discovery_cmd>> case nvme_admin_get_log_page:
+	 *   - include/linux/nvme.h|980| <<show_admin_opcode_name>> nvme_admin_opcode_name(nvme_admin_get_log_page), \
+	 */
 	nvme_admin_get_log_page		= 0x02,
 	nvme_admin_delete_cq		= 0x04,
 	nvme_admin_create_cq		= 0x05,
@@ -840,12 +985,34 @@ enum nvme_admin_opcode {
 	nvme_admin_abort_cmd		= 0x08,
 	nvme_admin_set_features		= 0x09,
 	nvme_admin_get_features		= 0x0a,
+	/*
+	 * 在以下使用nvme_admin_async_event:
+	 *   - drivers/nvme/host/fc.c|1811| <<nvme_fc_init_aen_ops>> sqe->common.opcode = nvme_admin_async_event;
+	 *   - drivers/nvme/host/pci.c|1665| <<nvme_pci_submit_async_event>> c.common.opcode = nvme_admin_async_event;
+	 *   - drivers/nvme/host/rdma.c|1423| <<nvme_rdma_submit_async_event>> cmd->common.opcode = nvme_admin_async_event;
+	 *   - drivers/nvme/host/tcp.c|2029| <<nvme_tcp_submit_async_event>> cmd->common.opcode = nvme_admin_async_event;
+	 *   - drivers/nvme/target/admin-cmd.c|187| <<nvmet_execute_get_log_cmd_effects_ns>> log->acs[nvme_admin_async_event] = cpu_to_le32(1 << 0);
+	 *   - drivers/nvme/target/admin-cmd.c|917| <<nvmet_parse_admin_cmd>> case nvme_admin_async_event:
+	 *   - drivers/nvme/target/discovery.c|366| <<nvmet_parse_discovery_cmd>> case nvme_admin_async_event:
+	 *   - drivers/nvme/target/loop.c|180| <<nvme_loop_submit_async_event>> iod->cmd.common.opcode = nvme_admin_async_event;
+	 *   - drivers/scsi/qla2xxx/qla_nvme.c|403| <<qla2x00_start_nvme_mq>> if (cmd->sqe.common.opcode == nvme_admin_async_event) {
+	 *   - include/linux/nvme.h|899| <<show_admin_opcode_name>> nvme_admin_opcode_name(nvme_admin_async_event), \
+	 */
 	nvme_admin_async_event		= 0x0c,
 	nvme_admin_ns_mgmt		= 0x0d,
 	nvme_admin_activate_fw		= 0x10,
 	nvme_admin_download_fw		= 0x11,
 	nvme_admin_dev_self_test	= 0x14,
 	nvme_admin_ns_attach		= 0x15,
+	/*
+	 * 在以下使用nvme_admin_keep_alive:
+	 *   - drivers/nvme/host/core.c|5101| <<nvme_init_ctrl>> ctrl->ka_cmd.common.opcode = nvme_admin_keep_alive;
+	 *   - drivers/nvme/target/admin-cmd.c|188| <<nvmet_execute_get_log_cmd_effects_ns>> log->acs[nvme_admin_keep_alive] = cpu_to_le32(1 << 0);
+	 *   - drivers/nvme/target/admin-cmd.c|920| <<nvmet_parse_admin_cmd>> case nvme_admin_keep_alive:
+	 *   - drivers/nvme/target/discovery.c|369| <<nvmet_parse_discovery_cmd>> case nvme_admin_keep_alive:
+	 *   - drivers/scsi/lpfc/lpfc_nvme.c|1641| <<lpfc_nvme_fcp_io_submit>> if (sqe->opcode == nvme_admin_keep_alive)
+	 *   - include/linux/nvme.h|964| <<show_admin_opcode_name>> nvme_admin_opcode_name(nvme_admin_keep_alive), \
+	 */
 	nvme_admin_keep_alive		= 0x18,
 	nvme_admin_directive_send	= 0x19,
 	nvme_admin_directive_recv	= 0x1a,
@@ -1421,6 +1588,11 @@ enum {
 	NVME_SC_ANA_INACCESSIBLE	= 0x302,
 	NVME_SC_ANA_TRANSITION		= 0x303,
 	NVME_SC_HOST_PATH_ERROR		= 0x370,
+	/*
+	 * 在以下使用NVME_SC_HOST_ABORTED_CMD:
+	 *   - drivers/nvme/host/core.c|490| <<nvme_cancel_request>> nvme_req(req)->status = NVME_SC_HOST_ABORTED_CMD;
+	 *   - drivers/nvme/host/multipath.c|102| <<nvme_failover_req>> case NVME_SC_HOST_ABORTED_CMD:
+	 */
 	NVME_SC_HOST_ABORTED_CMD	= 0x371,
 
 	NVME_SC_CRD			= 0x1800,
diff --git a/include/linux/pci.h b/include/linux/pci.h
index c393dff2d66f..3c41cd0912d2 100644
--- a/include/linux/pci.h
+++ b/include/linux/pci.h
@@ -922,6 +922,12 @@ enum {
 #define PCI_IRQ_LEGACY		(1 << 0) /* Allow legacy interrupts */
 #define PCI_IRQ_MSI		(1 << 1) /* Allow MSI interrupts */
 #define PCI_IRQ_MSIX		(1 << 2) /* Allow MSI-X interrupts */
+/*
+ * 部分使用PCI_IRQ_AFFINITY的例子:
+ *   - drivers/nvme/host/pci.c|2212| <<nvme_setup_irqs>> PCI_IRQ_ALL_TYPES | PCI_IRQ_AFFINITY, &affd);
+ *   - drivers/pci/msi.c|1211| <<pci_alloc_irq_vectors_affinity>> if (flags & PCI_IRQ_AFFINITY) {
+ *   - drivers/virtio/virtio_pci_common.c|129| <<vp_request_msix_vectors>> flags |= PCI_IRQ_AFFINITY;
+ */
 #define PCI_IRQ_AFFINITY	(1 << 3) /* Auto-assign affinity */
 
 /* These external functions are only available when PCI support is enabled */
diff --git a/include/linux/percpu-refcount.h b/include/linux/percpu-refcount.h
index 390031e816dc..deaf9fe662b1 100644
--- a/include/linux/percpu-refcount.h
+++ b/include/linux/percpu-refcount.h
@@ -61,8 +61,35 @@ typedef void (percpu_ref_func_t)(struct percpu_ref *);
 
 /* flags set in the lower bits of percpu_ref->percpu_count_ptr */
 enum {
+	/*
+	 * 在以下使用__PERCPU_REF_ATOMIC:
+	 *   - include/linux/percpu-refcount.h|66| <<global>> __PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,
+	 *   - lib/percpu-refcount.c|76| <<percpu_ref_init>> ref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;
+	 *   - lib/percpu-refcount.c|175| <<__percpu_ref_switch_to_atomic>> if (ref->percpu_count_ptr & __PERCPU_REF_ATOMIC) {
+	 *   - lib/percpu-refcount.c|182| <<__percpu_ref_switch_to_atomic>> ref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;
+	 *   - lib/percpu-refcount.c|201| <<__percpu_ref_switch_to_percpu>> if (!(ref->percpu_count_ptr & __PERCPU_REF_ATOMIC))
+	 *   - lib/percpu-refcount.c|219| <<__percpu_ref_switch_to_percpu>> ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC);
+	 */
 	__PERCPU_REF_ATOMIC	= 1LU << 0,	/* operating in atomic mode */
+	/*
+	 * 在以下使用__PERCPU_REF_DEAD:
+	 *   - include/linux/percpu-refcount.h|66| <<global>> __PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,
+	 *   - include/linux/percpu-refcount.h|265| <<percpu_ref_tryget_live>> } else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
+	 *   - include/linux/percpu-refcount.h|323| <<percpu_ref_is_dying>> return ref->percpu_count_ptr & __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|83| <<percpu_ref_init>> ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|235| <<__percpu_ref_switch_mode>> if (ref->force_atomic || (ref->percpu_count_ptr & __PERCPU_REF_DEAD))
+	 *   - lib/percpu-refcount.c|345| <<percpu_ref_kill_and_confirm>> WARN_ONCE(ref->percpu_count_ptr & __PERCPU_REF_DEAD,
+	 *   - lib/percpu-refcount.c|348| <<percpu_ref_kill_and_confirm>> ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|396| <<percpu_ref_resurrect>> WARN_ON_ONCE(!(ref->percpu_count_ptr & __PERCPU_REF_DEAD));
+	 *   - lib/percpu-refcount.c|399| <<percpu_ref_resurrect>> ref->percpu_count_ptr &= ~__PERCPU_REF_DEAD;
+	 */
 	__PERCPU_REF_DEAD	= 1LU << 1,	/* (being) killed */
+	/*
+	 * 在以下使用__PERCPU_REF_ATOMIC_DEAD:
+	 *   - include/linux/percpu-refcount.h|169| <<__ref_is_percpu>> if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
+	 *   - lib/percpu-refcount.c|43| <<percpu_count_ptr>> (ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC_DEAD);
+	 *   - lib/percpu-refcount.c|113| <<percpu_ref_exit>> ref->percpu_count_ptr = __PERCPU_REF_ATOMIC_DEAD;
+	 */
 	__PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,
 
 	__PERCPU_REF_FLAG_BITS	= 2,
@@ -131,6 +158,26 @@ void percpu_ref_reinit(struct percpu_ref *ref);
  *
  * There are no implied RCU grace periods between kill and release.
  */
+/*
+ * called by:
+ *   - block/blk-cgroup.c|453| <<blkg_destroy>> percpu_ref_kill(&blkg->refcnt);
+ *   - block/blk-mq.c|164| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+ *   - drivers/infiniband/sw/rdmavt/mr.c|277| <<rvt_free_lkey>> percpu_ref_kill(&mr->refcount);
+ *   - drivers/nvme/target/core.c|609| <<nvmet_ns_disable>> percpu_ref_kill(&ns->ref);
+ *   - drivers/target/target_core_transport.c|2930| <<target_sess_cmd_list_set_waiting>> percpu_ref_kill(&se_sess->cmd_count);
+ *   - drivers/target/target_core_transport.c|2962| <<transport_clear_lun_ref>> percpu_ref_kill(&lun->lun_ref);
+ *   - fs/aio.c|629| <<free_ioctx_users>> percpu_ref_kill(&ctx->reqs);
+ *   - fs/aio.c|848| <<kill_ioctx>> percpu_ref_kill(&ctx->users);
+ *   - fs/io_uring.c|4981| <<io_ring_ctx_wait_and_kill>> percpu_ref_kill(&ctx->refs);
+ *   - fs/io_uring.c|5440| <<__io_uring_register>> percpu_ref_kill(&ctx->refs);
+ *   - include/linux/genhd.h|780| <<hd_struct_kill>> percpu_ref_kill(&part->ref);
+ *   - kernel/bpf/cgroup.c|28| <<cgroup_bpf_offline>> percpu_ref_kill(&cgrp->bpf.refcnt);
+ *   - kernel/cgroup/cgroup.c|2151| <<cgroup_kill_sb>> percpu_ref_kill(&root->cgrp.self.refcnt);
+ *   - kernel/cgroup/cgroup.c|5514| <<__acquires>> percpu_ref_kill(&cgrp->self.refcnt);
+ *   - mm/backing-dev.c|516| <<cgwb_kill>> percpu_ref_kill(&wb->refcnt);
+ *   - mm/memremap.c|84| <<dev_pagemap_kill>> percpu_ref_kill(pgmap->ref);
+ *   - mm/slab_common.c|766| <<kmemcg_cache_deactivate_after_rcu>> percpu_ref_kill(&s->memcg_params.refcnt);
+ */
 static inline void percpu_ref_kill(struct percpu_ref *ref)
 {
 	percpu_ref_kill_and_confirm(ref, NULL);
@@ -142,6 +189,15 @@ static inline void percpu_ref_kill(struct percpu_ref *ref)
  * because doing so forces the compiler to generate two conditional
  * branches as it can't assume that @ref->percpu_count is not NULL.
  */
+/*
+ * called by:
+ *   - include/linux/percpu-refcount.h|191| <<percpu_ref_get_many>> if (__ref_is_percpu(ref, &percpu_count))
+ *   - include/linux/percpu-refcount.h|228| <<percpu_ref_tryget>> if (__ref_is_percpu(ref, &percpu_count)) {
+ *   - include/linux/percpu-refcount.h|262| <<percpu_ref_tryget_live>> if (__ref_is_percpu(ref, &percpu_count)) {
+ *   - include/linux/percpu-refcount.h|290| <<percpu_ref_put_many>> if (__ref_is_percpu(ref, &percpu_count))
+ *   - include/linux/percpu-refcount.h|338| <<percpu_ref_is_zero>> if (__ref_is_percpu(ref, &percpu_count))
+ *   - lib/percpu-refcount.c|397| <<percpu_ref_resurrect>> WARN_ON_ONCE(__ref_is_percpu(ref, &percpu_count));
+ */
 static inline bool __ref_is_percpu(struct percpu_ref *ref,
 					  unsigned long __percpu **percpu_countp)
 {
@@ -182,6 +238,14 @@ static inline bool __ref_is_percpu(struct percpu_ref *ref,
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - include/linux/cgroup.h|334| <<css_get_many>> percpu_ref_get_many(&css->refcnt, n);
+ *   - include/linux/percpu-refcount.h|245| <<percpu_ref_get>> percpu_ref_get_many(ref, 1);
+ *   - mm/memremap.c|306| <<memremap_pages>> percpu_ref_get_many(pgmap->ref, pfn_end(pgmap) - pfn_first(pgmap));
+ *   - mm/slab.h|364| <<memcg_charge_slab>> percpu_ref_get_many(&s->memcg_params.refcnt, 1 << order);
+ *   - mm/slab.h|376| <<memcg_charge_slab>> percpu_ref_get_many(&s->memcg_params.refcnt, 1 << order);
+ */
 static inline void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
 {
 	unsigned long __percpu *percpu_count;
@@ -204,6 +268,28 @@ static inline void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|459| <<blk_mq_sched_insert_requests>> percpu_ref_get(&q->q_usage_counter);
+ *   - block/blk.h|139| <<blk_queue_enter_live>> percpu_ref_get(&q->q_usage_counter);
+ *   - drivers/md/md.c|8298| <<md_write_start>> percpu_ref_get(&mddev->writes_pending);
+ *   - drivers/md/md.c|8343| <<md_write_inc>> percpu_ref_get(&mddev->writes_pending);
+ *   - drivers/nvme/target/core.c|406| <<nvmet_find_namespace>> percpu_ref_get(&ns->ref);
+ *   - drivers/scsi/scsi_lib.c|608| <<scsi_end_request>> percpu_ref_get(&q->q_usage_counter);
+ *   - drivers/target/target_core_device.c|114| <<transport_lookup_cmd_lun>> percpu_ref_get(&se_lun->lun_ref);
+ *   - drivers/target/target_core_transport.c|2755| <<target_get_sess_cmd>> percpu_ref_get(&se_sess->cmd_count);
+ *   - fs/aio.c|780| <<ioctx_alloc>> percpu_ref_get(&ctx->users);
+ *   - fs/aio.c|781| <<ioctx_alloc>> percpu_ref_get(&ctx->reqs);
+ *   - fs/aio.c|1039| <<aio_get_req>> percpu_ref_get(&ctx->reqs);
+ *   - include/linux/backing-dev-defs.h|275| <<wb_get>> percpu_ref_get(&wb->refcnt);
+ *   - include/linux/blk-cgroup.h|477| <<blkg_get>> percpu_ref_get(&blkg->refcnt);
+ *   - include/linux/cgroup.h|321| <<css_get>> percpu_ref_get(&css->refcnt);
+ *   - include/linux/cgroup.h|939| <<cgroup_bpf_get>> percpu_ref_get(&cgrp->bpf.refcnt);
+ *   - include/linux/genhd.h|761| <<hd_struct_get>> percpu_ref_get(&part->ref);
+ *   - include/rdma/rdmavt_mr.h|132| <<rvt_get_mr>> percpu_ref_get(&mr->refcount);
+ *   - lib/percpu-refcount.c|196| <<__percpu_ref_switch_to_atomic>> percpu_ref_get(ref);
+ *   - lib/percpu-refcount.c|411| <<percpu_ref_resurrect>> percpu_ref_get(ref);
+ */
 static inline void percpu_ref_get(struct percpu_ref *ref)
 {
 	percpu_ref_get_many(ref, 1);
@@ -218,6 +304,17 @@ static inline void percpu_ref_get(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|493| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+ *   - block/blk-mq.c|1348| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+ *   - fs/io_uring.c|866| <<io_get_req>> if (!percpu_ref_tryget(&ctx->refs))
+ *   - fs/io_uring.c|5142| <<SYSCALL_DEFINE6>> if (!percpu_ref_tryget(&ctx->refs))
+ *   - include/linux/backing-dev-defs.h|264| <<wb_tryget>> return percpu_ref_tryget(&wb->refcnt);
+ *   - include/linux/blk-cgroup.h|489| <<blkg_tryget>> return blkg && percpu_ref_tryget(&blkg->refcnt);
+ *   - include/linux/cgroup.h|351| <<css_tryget>> return percpu_ref_tryget(&css->refcnt);
+ *   - mm/memcontrol.c|2804| <<memcg_kmem_get_cache>> else if (percpu_ref_tryget(&memcg_cachep->memcg_params.refcnt))
+ */
 static inline bool percpu_ref_tryget(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
@@ -252,6 +349,22 @@ static inline bool percpu_ref_tryget(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|506| <<blk_queue_enter>> if (percpu_ref_tryget_live(&q->q_usage_counter)) {
+ *   - drivers/nvme/target/core.c|921| <<nvmet_req_init>> if (unlikely(!percpu_ref_tryget_live(&sq->ref))) {
+ *   - drivers/pci/p2pdma.c|684| <<pci_alloc_p2pmem>> if (unlikely(!percpu_ref_tryget_live(ref))) {
+ *   - drivers/target/target_core_alua.c|959| <<core_alua_queue_state_change_ua>> if (!percpu_ref_tryget_live(&lun->lun_ref))
+ *   - drivers/target/target_core_device.c|70| <<transport_lookup_cmd_lun>> if (!percpu_ref_tryget_live(&se_lun->lun_ref)) {
+ *   - drivers/target/target_core_device.c|161| <<transport_lookup_tmr_lun>> if (!percpu_ref_tryget_live(&se_lun->lun_ref)) {
+ *   - drivers/target/target_core_pr.c|727| <<__core_scsi3_alloc_registration>> if (!percpu_ref_tryget_live(&lun_tmp->lun_ref))
+ *   - fs/aio.c|1067| <<lookup_ioctx>> if (percpu_ref_tryget_live(&ctx->users))
+ *   - include/linux/cgroup.h|368| <<css_tryget_online>> return percpu_ref_tryget_live(&css->refcnt);
+ *   - include/linux/genhd.h|766| <<hd_struct_try_get>> return percpu_ref_tryget_live(&part->ref);
+ *   - kernel/cgroup/cgroup-v1.c|1137| <<cgroup1_root_to_use>> if (!percpu_ref_tryget_live(&ss->root->cgrp.self.refcnt))
+ *   - kernel/cgroup/cgroup-v1.c|1214| <<cgroup1_get_tree>> if (!ret && !percpu_ref_tryget_live(&ctx->root->cgrp.self.refcnt))
+ *   - mm/memremap.c|404| <<get_dev_pagemap>> if (pgmap && !percpu_ref_tryget_live(pgmap->ref))
+ */
 static inline bool percpu_ref_tryget_live(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
@@ -263,6 +376,18 @@ static inline bool percpu_ref_tryget_live(struct percpu_ref *ref)
 		this_cpu_inc(*percpu_count);
 		ret = true;
 	} else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
+		/*
+		 * 在以下使用__PERCPU_REF_DEAD:
+		 *   - include/linux/percpu-refcount.h|66| <<global>> __PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,
+		 *   - include/linux/percpu-refcount.h|265| <<percpu_ref_tryget_live>> } else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
+		 *   - include/linux/percpu-refcount.h|323| <<percpu_ref_is_dying>> return ref->percpu_count_ptr & __PERCPU_REF_DEAD;
+		 *   - lib/percpu-refcount.c|83| <<percpu_ref_init>> ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+		 *   - lib/percpu-refcount.c|235| <<__percpu_ref_switch_mode>> if (ref->force_atomic || (ref->percpu_count_ptr & __PERCPU_REF_DEAD))
+		 *   - lib/percpu-refcount.c|345| <<percpu_ref_kill_and_confirm>> WARN_ONCE(ref->percpu_count_ptr & __PERCPU_REF_DEAD,
+		 *   - lib/percpu-refcount.c|348| <<percpu_ref_kill_and_confirm>> ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+		 *   - lib/percpu-refcount.c|396| <<percpu_ref_resurrect>> WARN_ON_ONCE(!(ref->percpu_count_ptr & __PERCPU_REF_DEAD));
+		 *   - lib/percpu-refcount.c|399| <<percpu_ref_resurrect>> ref->percpu_count_ptr &= ~__PERCPU_REF_DEAD;
+		 */
 		ret = atomic_long_inc_not_zero(&ref->count);
 	}
 
@@ -281,6 +406,13 @@ static inline bool percpu_ref_tryget_live(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|922| <<io_free_req_many>> percpu_ref_put_many(&ctx->refs, *nr);
+ *   - include/linux/cgroup.h|414| <<css_put_many>> percpu_ref_put_many(&css->refcnt, n);
+ *   - include/linux/percpu-refcount.h|345| <<percpu_ref_put>> percpu_ref_put_many(ref, 1);
+ *   - mm/slab.h|405| <<memcg_uncharge_slab>> percpu_ref_put_many(&s->memcg_params.refcnt, 1 << order);
+ */
 static inline void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
 {
 	unsigned long __percpu *percpu_count;
@@ -304,6 +436,48 @@ static inline void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|515| <<blk_queue_enter>> percpu_ref_put(&q->q_usage_counter);
+ *   - block/blk-core.c|561| <<blk_queue_exit>> percpu_ref_put(&q->q_usage_counter);
+ *   - block/blk-mq-sched.c|489| <<blk_mq_sched_insert_requests>> percpu_ref_put(&q->q_usage_counter);
+ *   - drivers/md/md.c|5419| <<mddev_init_writes_pending>> percpu_ref_put(&mddev->writes_pending);
+ *   - drivers/md/md.c|8323| <<md_write_start>> percpu_ref_put(&mddev->writes_pending);
+ *   - drivers/md/md.c|8349| <<md_write_end>> percpu_ref_put(&mddev->writes_pending);
+ *   - drivers/nvme/target/core.c|421| <<nvmet_put_namespace>> percpu_ref_put(&ns->ref);
+ *   - drivers/nvme/target/core.c|726| <<nvmet_req_complete>> percpu_ref_put(&req->sq->ref);
+ *   - drivers/nvme/target/core.c|939| <<nvmet_req_uninit>> percpu_ref_put(&req->sq->ref);
+ *   - drivers/pci/p2pdma.c|707| <<pci_free_p2pmem>> percpu_ref_put(ref);
+ *   - drivers/scsi/scsi_lib.c|618| <<scsi_end_request>> percpu_ref_put(&q->q_usage_counter);
+ *   - drivers/target/target_core_alua.c|998| <<core_alua_queue_state_change_ua>> percpu_ref_put(&lun->lun_ref);
+ *   - drivers/target/target_core_pr.c|775| <<__core_scsi3_alloc_registration>> percpu_ref_put(&lun_tmp->lun_ref);
+ *   - drivers/target/target_core_pr.c|794| <<__core_scsi3_alloc_registration>> percpu_ref_put(&lun_tmp->lun_ref);
+ *   - drivers/target/target_core_pr.c|806| <<__core_scsi3_alloc_registration>> percpu_ref_put(&lun_tmp->lun_ref);
+ *   - drivers/target/target_core_transport.c|2785| <<target_release_cmd_kref>> percpu_ref_put(&se_cmd->se_lun->lun_ref);
+ *   - drivers/target/target_core_transport.c|2800| <<target_release_cmd_kref>> percpu_ref_put(&se_sess->cmd_count);
+ *   - fs/aio.c|630| <<free_ioctx_users>> percpu_ref_put(&ctx->reqs);
+ *   - fs/aio.c|1081| <<iocb_destroy>> percpu_ref_put(&iocb->ki_ctx->reqs);
+ *   - fs/aio.c|1335| <<SYSCALL_DEFINE2>> percpu_ref_put(&ioctx->users);
+ *   - fs/aio.c|1367| <<COMPAT_SYSCALL_DEFINE2>> percpu_ref_put(&ioctx->users);
+ *   - fs/aio.c|1396| <<SYSCALL_DEFINE1>> percpu_ref_put(&ioctx->users);
+ *   - fs/aio.c|1928| <<SYSCALL_DEFINE3>> percpu_ref_put(&ctx->users);
+ *   - fs/aio.c|1970| <<COMPAT_SYSCALL_DEFINE3>> percpu_ref_put(&ctx->users);
+ *   - fs/aio.c|2023| <<SYSCALL_DEFINE3>> percpu_ref_put(&ctx->users);
+ *   - fs/aio.c|2041| <<do_io_getevents>> percpu_ref_put(&ioctx->users);
+ *   - fs/io_uring.c|914| <<io_get_req>> percpu_ref_put(&ctx->refs);
+ *   - fs/io_uring.c|944| <<__io_free_req>> percpu_ref_put(&ctx->refs);
+ *   - fs/io_uring.c|5184| <<SYSCALL_DEFINE6>> percpu_ref_put(&ctx->refs);
+ *   - include/linux/backing-dev-defs.h|293| <<wb_put>> percpu_ref_put(&wb->refcnt);
+ *   - include/linux/blk-cgroup.h|524| <<blkg_put>> percpu_ref_put(&blkg->refcnt);
+ *   - include/linux/cgroup.h|401| <<css_put>> percpu_ref_put(&css->refcnt);
+ *   - include/linux/cgroup.h|944| <<cgroup_bpf_put>> percpu_ref_put(&cgrp->bpf.refcnt);
+ *   - include/linux/genhd.h|771| <<hd_struct_put>> percpu_ref_put(&part->ref);
+ *   - include/linux/memremap.h|173| <<put_dev_pagemap>> percpu_ref_put(pgmap->ref);
+ *   - include/rdma/rdmavt_mr.h|127| <<rvt_put_mr>> percpu_ref_put(&mr->refcount);
+ *   - lib/percpu-refcount.c|130| <<percpu_ref_call_confirm_rcu>> percpu_ref_put(ref);
+ *   - lib/percpu-refcount.c|350| <<percpu_ref_kill_and_confirm>> percpu_ref_put(ref);
+ *   - mm/memcontrol.c|2818| <<memcg_kmem_put_cache>> percpu_ref_put(&cachep->memcg_params.refcnt);
+ */
 static inline void percpu_ref_put(struct percpu_ref *ref)
 {
 	percpu_ref_put_many(ref, 1);
@@ -318,6 +492,17 @@ static inline void percpu_ref_put(struct percpu_ref *ref)
  * This function is safe to call as long as @ref is between init and exit
  * and the caller is responsible for synchronizing against state changes.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|5437| <<__io_uring_register>> if (percpu_ref_is_dying(&ctx->refs))
+ *   - include/linux/backing-dev-defs.h|304| <<wb_dying>> return percpu_ref_is_dying(&wb->refcnt);
+ *   - include/linux/cgroup.h|389| <<css_is_dying>> return !(css->flags & CSS_NO_REF) && percpu_ref_is_dying(&css->refcnt);
+ *   - kernel/cgroup/cgroup-v1.c|1222| <<cgroup1_get_tree>> if (!ret && percpu_ref_is_dying(&ctx->root->cgrp.self.refcnt)) {
+ *   - kernel/cgroup/cgroup.c|2150| <<cgroup_kill_sb>> !percpu_ref_is_dying(&root->cgrp.self.refcnt))
+ *   - kernel/cgroup/cgroup.c|2944| <<__acquires>> if (!css || !percpu_ref_is_dying(&css->refcnt))
+ *   - kernel/cgroup/cgroup.c|3058| <<cgroup_apply_control_enable>> WARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));
+ *   - kernel/cgroup/cgroup.c|3104| <<cgroup_apply_control_disable>> WARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));
+ */
 static inline bool percpu_ref_is_dying(struct percpu_ref *ref)
 {
 	return ref->percpu_count_ptr & __PERCPU_REF_DEAD;
@@ -331,6 +516,17 @@ static inline bool percpu_ref_is_dying(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|193| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+ *   - block/blk-mq.c|208| <<blk_mq_freeze_queue_wait_timeout>> percpu_ref_is_zero(&q->q_usage_counter),
+ *   - block/blk-pm.c|87| <<blk_pre_runtime_suspend>> if (percpu_ref_is_zero(&q->q_usage_counter))
+ *   - drivers/md/md.c|2467| <<set_in_sync>> percpu_ref_is_zero(&mddev->writes_pending)) {
+ *   - drivers/target/target_core_transport.c|2947| <<target_wait_for_sess_cmds>> percpu_ref_is_zero(&se_sess->cmd_count),
+ *   - kernel/bpf/cgroup.c|246| <<update_effective_progs>> if (percpu_ref_is_zero(&desc->bpf.refcnt))
+ *   - kernel/bpf/cgroup.c|258| <<update_effective_progs>> if (percpu_ref_is_zero(&desc->bpf.refcnt)) {
+ *   - lib/percpu-refcount.c|369| <<percpu_ref_reinit>> WARN_ON_ONCE(!percpu_ref_is_zero(ref));
+ */
 static inline bool percpu_ref_is_zero(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
diff --git a/include/linux/sbitmap.h b/include/linux/sbitmap.h
index e40d019c3d9d..26ec9e227ad6 100644
--- a/include/linux/sbitmap.h
+++ b/include/linux/sbitmap.h
@@ -262,6 +262,9 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 		 */
 		depth += nr;
 		while (1) {
+			/*
+			 * Find the next set bit in a memory region.
+			 */
 			nr = find_next_bit(&word, depth, nr);
 			if (nr >= depth)
 				break;
diff --git a/include/linux/virtio_config.h b/include/linux/virtio_config.h
index bb4cc4910750..297ba40099f6 100644
--- a/include/linux/virtio_config.h
+++ b/include/linux/virtio_config.h
@@ -165,6 +165,13 @@ static inline bool virtio_has_feature(const struct virtio_device *vdev,
  * virtio_has_iommu_quirk - determine whether this device has the iommu quirk
  * @vdev: the device
  */
+/*
+ * called by:
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|975| <<virtio_gpu_cmd_transfer_to_host_3d>> bool use_dma_api = !virtio_has_iommu_quirk(vgdev->vdev);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|1050| <<virtio_gpu_object_attach>> bool use_dma_api = !virtio_has_iommu_quirk(vgdev->vdev);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|1104| <<virtio_gpu_object_detach>> bool use_dma_api = !virtio_has_iommu_quirk(vgdev->vdev);
+ *   - drivers/virtio/virtio_ring.c|259| <<vring_use_dma_api>> if (!virtio_has_iommu_quirk(vdev))
+ */
 static inline bool virtio_has_iommu_quirk(const struct virtio_device *vdev)
 {
 	/*
diff --git a/include/scsi/scsi_device.h b/include/scsi/scsi_device.h
index 3ed836db5306..acb7bbd956f8 100644
--- a/include/scsi/scsi_device.h
+++ b/include/scsi/scsi_device.h
@@ -106,6 +106,24 @@ struct scsi_device {
 	struct list_head    siblings;   /* list of all devices on this host */
 	struct list_head    same_target_siblings; /* just the devices sharing same target id */
 
+	/*
+	 * 使用device_busy的地方:
+	 *   - drivers/scsi/scsi_sysfs.c|664| <<global>> static DEVICE_ATTR(device_busy, S_IRUGO, sdev_show_device_busy, NULL);
+	 *   - drivers/message/fusion/mptsas.c|3759| <<mptsas_send_link_status_event>> atomic_read(&sdev->device_busy)));
+	 *   - drivers/scsi/megaraid/megaraid_sas_fusion.c|2829| <<megasas_build_ldio_fusion>> atomic_read(&scp->device->device_busy) >
+	 *   - drivers/scsi/megaraid/megaraid_sas_fusion.c|3162| <<megasas_build_syspd_fusion>> atomic_read(&scmd->device->device_busy) > MR_DEVICE_HIGH_IOPS_DEPTH)
+	 *   - drivers/scsi/mpt3sas/mpt3sas_base.c|3488| <<_base_get_high_iops_msix_index>> if (atomic_read(&scmd->device->device_busy) >
+	 *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|2996| <<scsih_dev_reset>> if (r == SUCCESS && atomic_read(&scmd->device->device_busy))
+	 *   - drivers/scsi/scsi_lib.c|357| <<scsi_device_unbusy>> atomic_dec(&sdev->device_busy);
+	 *   - drivers/scsi/scsi_lib.c|413| <<scsi_device_is_busy>> if (atomic_read(&sdev->device_busy) >= sdev->queue_depth)
+	 *   - drivers/scsi/scsi_lib.c|1287| <<scsi_dev_queue_ready>> busy = atomic_inc_return(&sdev->device_busy) - 1;
+	 *   - drivers/scsi/scsi_lib.c|1306| <<scsi_dev_queue_ready>> atomic_dec(&sdev->device_busy);
+	 *   - drivers/scsi/scsi_lib.c|1627| <<scsi_mq_put_budget>> atomic_dec(&sdev->device_busy);
+	 *   - drivers/scsi/scsi_lib.c|1638| <<scsi_mq_get_budget>> if (atomic_read(&sdev->device_busy) == 0 && !scsi_device_blocked(sdev))
+	 *   - drivers/scsi/scsi_lib.c|1709| <<scsi_queue_rq>> if (atomic_read(&sdev->device_busy) ||
+	 *   - drivers/scsi/scsi_sysfs.c|662| <<sdev_show_device_busy>> return snprintf(buf, 20, "%d\n", atomic_read(&sdev->device_busy));
+	 *   - drivers/scsi/sg.c|2510| <<sg_proc_seq_show_dev>> (int ) atomic_read(&scsidp->device_busy),
+	 */
 	atomic_t device_busy;		/* commands actually active on LLDD */
 	atomic_t device_blocked;	/* Device returned QUEUE_FULL. */
 
diff --git a/include/scsi/scsi_host.h b/include/scsi/scsi_host.h
index f577647bf5f2..d99940273777 100644
--- a/include/scsi/scsi_host.h
+++ b/include/scsi/scsi_host.h
@@ -598,6 +598,12 @@ struct Scsi_Host {
 	 * can_queue. In other words, the total queue depth per host
 	 * is nr_hw_queues * can_queue.
 	 */
+	/*
+	 * 修改request_queue->nr_hw_queues的地方:
+	 *   - block/blk-mq.c|3103| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3147| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3233| <<blk_mq_init_allocated_queue>> q->nr_hw_queues = 0;
+	 */
 	unsigned nr_hw_queues;
 	unsigned active_mode:2;
 	unsigned unchecked_isa_dma:1;
diff --git a/include/uapi/linux/fs.h b/include/uapi/linux/fs.h
index 379a612f8f1d..f1fd79764d02 100644
--- a/include/uapi/linux/fs.h
+++ b/include/uapi/linux/fs.h
@@ -285,6 +285,13 @@ struct fsxattr {
 typedef int __bitwise __kernel_rwf_t;
 
 /* high priority request, poll if possible */
+/*
+ * 在以下使用RWF_HIPRI:
+ *   - fs/overlayfs/file.c|219| <<ovl_iocb_to_rwf>> flags |= RWF_HIPRI;
+ *   - fs/read_write.c|706| <<do_loop_readv_writev>> if (flags & ~RWF_HIPRI)
+ *   - include/linux/fs.h|3426| <<kiocb_set_rw_flags>> if (flags & RWF_HIPRI)
+ *   - include/uapi/linux/fs.h|303| <<RWF_SUPPORTED>> #define RWF_SUPPORTED (RWF_HIPRI | RWF_DSYNC | RWF_SYNC | RWF_NOWAIT |\
+ */
 #define RWF_HIPRI	((__force __kernel_rwf_t)0x00000001)
 
 /* per-IO O_DSYNC */
@@ -300,6 +307,10 @@ typedef int __bitwise __kernel_rwf_t;
 #define RWF_APPEND	((__force __kernel_rwf_t)0x00000010)
 
 /* mask of flags supported by the kernel */
+/*
+ * 在以下使用RWF_SUPPORTED:
+ *   - include/linux/fs.h|3418| <<kiocb_set_rw_flags>> if (unlikely(flags & ~RWF_SUPPORTED))
+ */
 #define RWF_SUPPORTED	(RWF_HIPRI | RWF_DSYNC | RWF_SYNC | RWF_NOWAIT |\
 			 RWF_APPEND)
 
diff --git a/kernel/irq/affinity.c b/kernel/irq/affinity.c
index 4d89ad4fae3b..c1c6a1cf0691 100644
--- a/kernel/irq/affinity.c
+++ b/kernel/irq/affinity.c
@@ -412,6 +412,12 @@ static void default_calc_sets(struct irq_affinity *affd, unsigned int affvecs)
  *
  * Returns the irq_affinity_desc pointer or NULL if allocation failed.
  */
+/*
+ * called by:
+ *   - drivers/pci/msi.c|565| <<msi_setup_entry>> masks = irq_create_affinity_masks(nvec, affd);
+ *   - drivers/pci/msi.c|708| <<msix_setup_entries>> masks = irq_create_affinity_masks(nvec, affd);
+ *   - drivers/pci/msi.c|1250| <<pci_alloc_irq_vectors_affinity>> irq_create_affinity_masks(1, affd);
+ */
 struct irq_affinity_desc *
 irq_create_affinity_masks(unsigned int nvecs, struct irq_affinity *affd)
 {
@@ -437,6 +443,9 @@ irq_create_affinity_masks(unsigned int nvecs, struct irq_affinity *affd)
 		affd->calc_sets = default_calc_sets;
 
 	/* Recalculate the sets */
+	/*
+	 * nvme: nvme_calc_irq_sets()
+	 */
 	affd->calc_sets(affd, affvecs);
 
 	if (WARN_ON_ONCE(affd->nr_sets > IRQ_AFFINITY_MAX_SETS))
@@ -462,6 +471,11 @@ irq_create_affinity_masks(unsigned int nvecs, struct irq_affinity *affd)
 		unsigned int this_vecs = affd->set_size[i];
 		int ret;
 
+		/*
+		 * build affinity in two stages:
+		 *      1) spread present CPU on these vectors
+		 *      2) spread other possible CPUs on these vectors
+		 */
 		ret = irq_build_affinity_masks(curvec, this_vecs,
 					       curvec, masks);
 		if (ret) {
diff --git a/kernel/irq/chip.c b/kernel/irq/chip.c
index b3fa2d87d2f3..e46056543a5f 100644
--- a/kernel/irq/chip.c
+++ b/kernel/irq/chip.c
@@ -190,6 +190,10 @@ enum {
 };
 
 #ifdef CONFIG_SMP
+/*
+ * called by:
+ *   - kernel/irq/chip.c|266| <<irq_startup>> switch (__irq_startup_managed(desc, aff, force)) {
+ */
 static int
 __irq_startup_managed(struct irq_desc *desc, struct cpumask *aff, bool force)
 {
@@ -252,6 +256,14 @@ static int __irq_startup(struct irq_desc *desc)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/chip.c|245| <<__irq_startup>> ret = d->chip->irq_startup(d);
+ *   - kernel/irq/chip.c|299| <<irq_activate_and_startup>> return irq_startup(desc, resend, IRQ_START_FORCE);
+ *   - kernel/irq/cpuhotplug.c|184| <<irq_restore_affinity_of_irq>> irq_startup(desc, IRQ_RESEND, IRQ_START_COND);
+ *   - kernel/irq/manage.c|636| <<__enable_irq>> irq_startup(desc, IRQ_RESEND, IRQ_START_FORCE);
+ *   - kernel/irq/manage.c|1571| <<__setup_irq>> irq_startup(desc, IRQ_RESEND, IRQ_START_COND);
+ */
 int irq_startup(struct irq_desc *desc, bool resend, bool force)
 {
 	struct irq_data *d = irq_desc_get_irq_data(desc);
diff --git a/kernel/irq/irqdesc.c b/kernel/irq/irqdesc.c
index 5b8fdd659e54..4f981e920991 100644
--- a/kernel/irq/irqdesc.c
+++ b/kernel/irq/irqdesc.c
@@ -762,6 +762,17 @@ EXPORT_SYMBOL_GPL(irq_free_descs);
  *
  * Returns the first irq number or error code
  */
+/*
+ * called by:
+ *   - arch/s390/pci/pci_irq.c|279| <<arch_setup_msi_irqs>> irq = __irq_alloc_descs(-1, 0, 1, 0, THIS_MODULE, msi->affinity);
+ *   - arch/sparc/kernel/irq_64.c|245| <<irq_alloc>> irq = __irq_alloc_descs(-1, 1, 1, numa_node_id(), NULL, NULL);
+ *   - include/linux/irq.h|908| <<irq_alloc_descs>> __irq_alloc_descs(irq, from, cnt, node, THIS_MODULE, NULL)
+ *   - kernel/irq/devres.c|189| <<__devm_irq_alloc_descs>> base = __irq_alloc_descs(irq, from, cnt, node, owner, affinity);
+ *   - kernel/irq/irqdesc.c|817| <<irq_alloc_hwirqs>> int i, irq = __irq_alloc_descs(-1, 0, cnt, node, NULL, NULL);
+ *   - kernel/irq/irqdomain.c|1016| <<irq_domain_alloc_descs>> virq = __irq_alloc_descs(virq, virq, cnt, node, THIS_MODULE,
+ *   - kernel/irq/irqdomain.c|1022| <<irq_domain_alloc_descs>> virq = __irq_alloc_descs(-1, hint, cnt, node, THIS_MODULE,
+ *   - kernel/irq/irqdomain.c|1025| <<irq_domain_alloc_descs>> virq = __irq_alloc_descs(-1, 1, cnt, node, THIS_MODULE,
+ */
 int __ref
 __irq_alloc_descs(int irq, unsigned int from, unsigned int cnt, int node,
 		  struct module *owner, const struct irq_affinity_desc *affinity)
diff --git a/kernel/irq/irqdomain.c b/kernel/irq/irqdomain.c
index dd822fd8a7d5..dae331432037 100644
--- a/kernel/irq/irqdomain.c
+++ b/kernel/irq/irqdomain.c
@@ -1645,6 +1645,14 @@ static int __irq_domain_activate_irq(struct irq_data *irqd, bool reserve)
  * This is the second step to call domain_ops->activate to program interrupt
  * controllers, so the interrupt could actually get delivered.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/io_apic.c|2200| <<check_timer>> irq_domain_activate_irq(irq_data, false);
+ *   - arch/x86/kernel/apic/io_apic.c|2222| <<check_timer>> irq_domain_activate_irq(irq_data, false);
+ *   - kernel/irq/chip.c|224| <<__irq_startup_managed>> if (WARN_ON(irq_domain_activate_irq(d, false)))
+ *   - kernel/irq/chip.c|291| <<irq_activate>> return irq_domain_activate_irq(d, false);
+ *   - kernel/irq/msi.c|519| <<msi_domain_alloc_irqs>> ret = irq_domain_activate_irq(irq_data, can_reserve);
+ */
 int irq_domain_activate_irq(struct irq_data *irq_data, bool reserve)
 {
 	int ret = 0;
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index 1753486b440c..d050d3be6278 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -207,6 +207,14 @@ static void irq_validate_effective_affinity(struct irq_data *data)
 #endif
 }
 
+/*
+ * called by:
+ *   - kernel/irq/chip.c|272| <<irq_startup>> irq_do_set_affinity(d, aff, false);
+ *   - kernel/irq/cpuhotplug.c|131| <<migrate_one_irq>> err = irq_do_set_affinity(d, affinity, false);
+ *   - kernel/irq/manage.c|256| <<irq_try_set_affinity>> int ret = irq_do_set_affinity(data, dest, force);
+ *   - kernel/irq/manage.c|435| <<irq_setup_affinity>> ret = irq_do_set_affinity(&desc->irq_data, &mask, false);
+ *   - kernel/irq/migration.c|80| <<irq_move_masked_irq>> ret = irq_do_set_affinity(data, desc->pending_mask, false);
+ */
 int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,
 			bool force)
 {
@@ -217,6 +225,9 @@ int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,
 	if (!chip || !chip->irq_set_affinity)
 		return -EINVAL;
 
+	/*
+	 * msi_domain_set_affinity
+	 */
 	ret = chip->irq_set_affinity(data, mask, force);
 	switch (ret) {
 	case IRQ_SET_MASK_OK:
@@ -396,6 +407,11 @@ EXPORT_SYMBOL_GPL(irq_set_affinity_notifier);
 /*
  * Generic version of the affinity autoselector.
  */
+/*
+ * called by:
+ *   - kernel/irq/chip.c|269| <<irq_startup>> irq_setup_affinity(desc);
+ *   - kernel/irq/manage.c|457| <<irq_select_affinity_usr>> ret = irq_setup_affinity(desc);
+ */
 int irq_setup_affinity(struct irq_desc *desc)
 {
 	struct cpumask *set = irq_default_affinity;
diff --git a/kernel/irq/matrix.c b/kernel/irq/matrix.c
index 30cc217b8631..70b1ae641845 100644
--- a/kernel/irq/matrix.c
+++ b/kernel/irq/matrix.c
@@ -282,6 +282,10 @@ void irq_matrix_remove_managed(struct irq_matrix *m, const struct cpumask *msk)
  * @m:		Matrix pointer
  * @cpu:	On which CPU the interrupt should be allocated
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|319| <<assign_managed_vector>> vector = irq_matrix_alloc_managed(vector_matrix, vector_searchmask,
+ */
 int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,
 			     unsigned int *mapped_cpu)
 {
diff --git a/kernel/irq/msi.c b/kernel/irq/msi.c
index ad26fbcfbfc8..698cc5145d24 100644
--- a/kernel/irq/msi.c
+++ b/kernel/irq/msi.c
@@ -26,6 +26,14 @@
  * If @affinity is not NULL then an affinity array[@nvec] is allocated
  * and the affinity masks and flags from @affinity are copied.
  */
+/*
+ * called by:
+ *   - drivers/base/platform-msi.c|137| <<platform_msi_alloc_descs_with_irq>> desc = alloc_msi_entry(dev, 1, NULL);
+ *   - drivers/bus/fsl-mc/fsl-mc-msi.c|217| <<fsl_mc_msi_alloc_descs>> msi_desc = alloc_msi_entry(dev, 1, NULL);
+ *   - drivers/pci/msi.c|568| <<msi_setup_entry>> entry = alloc_msi_entry(&dev->dev, nvec, masks);
+ *   - drivers/pci/msi.c|707| <<msix_setup_entries>> entry = alloc_msi_entry(&dev->dev, 1, curmsk);
+ *   - drivers/soc/ti/ti_sci_inta_msi.c|81| <<ti_sci_inta_msi_alloc_descs>> msi_desc = alloc_msi_entry(dev, 1, NULL);
+ */
 struct msi_desc *alloc_msi_entry(struct device *dev, int nvec,
 				 const struct irq_affinity_desc *affinity)
 {
@@ -39,6 +47,7 @@ struct msi_desc *alloc_msi_entry(struct device *dev, int nvec,
 	desc->dev = dev;
 	desc->nvec_used = nvec;
 	if (affinity) {
+		/* duplicate region of memory */
 		desc->affinity = kmemdup(affinity,
 			nvec * sizeof(*desc->affinity), GFP_KERNEL);
 		if (!desc->affinity) {
@@ -98,6 +107,28 @@ static void msi_check_level(struct irq_domain *domain, struct msi_msg *msg)
  * Intended to be used by MSI interrupt controllers which are
  * implemented with hierarchical domains.
  */
+/*
+ * qemu下nvme的一个例子:
+ * [0] msi_domain_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [    0.682898] orabug: msi_domain_set_affinity() mask=f
+ * [    0.695652] orabug: msi_domain_set_affinity() mask=f
+ * [    0.706281] orabug: msi_domain_set_affinity() mask=1
+ * [    0.715123] orabug: msi_domain_set_affinity() mask=2
+ * [    0.724427] orabug: msi_domain_set_affinity() mask=4
+ * [    0.733339] orabug: msi_domain_set_affinity() mask=8
+ */
 int msi_domain_set_affinity(struct irq_data *irq_data,
 			    const struct cpumask *mask, bool force)
 {
@@ -105,6 +136,18 @@ int msi_domain_set_affinity(struct irq_data *irq_data,
 	struct msi_msg msg[2] = { [1] = { }, };
 	int ret;
 
+	/*
+	 * 对于virtblk和nvme, 到这里的时候:
+	 * parent->chip->name = APIC
+	 * parent->domain->name = VECTOR
+	 *
+	 * static struct irq_chip lapic_controller = {
+	 *	.name                   = "APIC",
+	 *	.irq_ack                = apic_ack_edge,
+	 *	.irq_set_affinity       = apic_set_affinity,
+	 *	.irq_retrigger          = apic_retrigger_irq,
+	 * };
+	 */
 	ret = parent->chip->irq_set_affinity(parent, mask, force);
 	if (ret >= 0 && ret != IRQ_SET_MASK_OK_DONE) {
 		BUG_ON(irq_chip_compose_msi_msg(irq_data, msg));
@@ -396,6 +439,24 @@ static bool msi_check_reservation_mode(struct irq_domain *domain,
  *
  * Returns 0 on success or an error code.
  */
+/*
+ * [0] msi_domain_alloc_irqs
+ * [0] native_setup_msi_irqs
+ * [0] __pci_enable_msix_range
+ * [0] pci_alloc_irq_vectors_affinity
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - arch/x86/kernel/apic/msi.c|116| <<native_setup_msi_irqs>> return msi_domain_alloc_irqs(domain, &dev->dev, nvec);
+ *   - drivers/base/platform-msi.c|265| <<platform_msi_domain_alloc_irqs>> err = msi_domain_alloc_irqs(dev->msi_domain, dev, nvec);
+ *   - drivers/bus/fsl-mc/fsl-mc-msi.c|259| <<fsl_mc_msi_domain_alloc_irqs>> error = msi_domain_alloc_irqs(msi_domain, dev, irq_count);
+ *   - drivers/pci/msi.c|41| <<pci_msi_setup_msi_irqs>> return msi_domain_alloc_irqs(domain, &dev->dev, nvec);
+ *   - drivers/soc/ti/ti_sci_inta_msi.c|115| <<ti_sci_inta_msi_domain_alloc_irqs>> ret = msi_domain_alloc_irqs(msi_domain, dev, nvec);
+ */
 int msi_domain_alloc_irqs(struct irq_domain *domain, struct device *dev,
 			  int nvec)
 {
diff --git a/kernel/irq/proc.c b/kernel/irq/proc.c
index cfc4f088a0e7..6b7c65a98c35 100644
--- a/kernel/irq/proc.c
+++ b/kernel/irq/proc.c
@@ -43,8 +43,34 @@ enum {
 	EFFECTIVE_LIST,
 };
 
+/*
+ * 更新effective_affinity的地方:
+ * [0] irq_data_update_effective_affinity()
+ * [0] apic_update_irq_cfg
+ * [0] assign_managed_vector.constprop.27
+ * [0] x86_vector_activate
+ * [0] __irq_domain_activate_irq
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static int show_irq_affinity(int type, struct seq_file *m)
 {
+	/*
+	 * struct irq_common_data  irq_common_data;
+	 *   -> cpumask_var_t           affinity;
+	 *   -> cpumask_var_t           effective_affinity;
+	 * struct irq_data         irq_data;
+	 */
 	struct irq_desc *desc = irq_to_desc((long)m->private);
 	const struct cpumask *mask;
 
diff --git a/kernel/trace/blktrace.c b/kernel/trace/blktrace.c
index 475e29498bca..2dafe471a164 100644
--- a/kernel/trace/blktrace.c
+++ b/kernel/trace/blktrace.c
@@ -209,6 +209,19 @@ static const u32 ddir_act[2] = { BLK_TC_ACT(BLK_TC_READ),
  * The worker for the various blk_add_trace*() types. Fills out a
  * blk_io_trace structure and places it in a per-cpu subbuffer.
  */
+/*
+ * called by:
+ *   - kernel/trace/blktrace.c|809| <<blk_add_trace_rq>> __blk_add_trace(bt, blk_rq_trace_sector(rq), nr_bytes, req_op(rq),
+ *   - kernel/trace/blktrace.c|861| <<blk_add_trace_bio>> __blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,
+ *   - kernel/trace/blktrace.c|911| <<blk_add_trace_getrq>> __blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_GETRQ, 0, 0,
+ *   - kernel/trace/blktrace.c|927| <<blk_add_trace_sleeprq>> __blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_SLEEPRQ,
+ *   - kernel/trace/blktrace.c|937| <<blk_add_trace_plug>> __blk_add_trace(bt, 0, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL, 0);
+ *   - kernel/trace/blktrace.c|954| <<blk_add_trace_unplug>> __blk_add_trace(bt, 0, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu, 0);
+ *   - kernel/trace/blktrace.c|967| <<blk_add_trace_split>> __blk_add_trace(bt, bio->bi_iter.bi_sector,
+ *   - kernel/trace/blktrace.c|1001| <<blk_add_trace_bio_remap>> __blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,
+ *   - kernel/trace/blktrace.c|1034| <<blk_add_trace_rq_remap>> __blk_add_trace(bt, blk_rq_pos(rq), blk_rq_bytes(rq),
+ *   - kernel/trace/blktrace.c|1059| <<blk_add_driver_data>> __blk_add_trace(bt, blk_rq_trace_sector(rq), blk_rq_bytes(rq), 0, 0,
+ */
 static void __blk_add_trace(struct blk_trace *bt, sector_t sector, int bytes,
 		     int op, int op_flags, u32 what, int error, int pdu_len,
 		     void *pdu_data, u64 cgid)
diff --git a/lib/fault-inject.c b/lib/fault-inject.c
index 8186ca84910b..0f460e99d3eb 100644
--- a/lib/fault-inject.c
+++ b/lib/fault-inject.c
@@ -15,6 +15,18 @@
  * setup_fault_attr() is a helper function for various __setup handlers, so it
  * returns 0 on error, because that is what __setup handlers do.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/iommu.c|82| <<setup_fail_iommu>> return setup_fault_attr(&fail_iommu, str);
+ *   - block/blk-core.c|773| <<setup_fail_make_request>> return setup_fault_attr(&fail_make_request, str);
+ *   - block/blk-timeout.c|19| <<setup_fail_io_timeout>> return setup_fault_attr(&fail_io_timeout, str);
+ *   - drivers/block/null_blk_main.c|2651| <<__null_setup_fault>> if (!setup_fault_attr(attr, str))
+ *   - drivers/mmc/core/debugfs.c|240| <<mmc_add_host_debugfs>> setup_fault_attr(&fail_default_attr, fail_request);
+ *   - drivers/nvme/host/fault_inject.c|26| <<nvme_fault_inject_init>> setup_fault_attr(&fail_default_attr, fail_request);
+ *   - kernel/futex.c|288| <<setup_fail_futex>> return setup_fault_attr(&fail_futex.attr, str);
+ *   - mm/failslab.c|38| <<setup_failslab>> return setup_fault_attr(&failslab.attr, str);
+ *   - mm/page_alloc.c|3327| <<setup_fail_page_alloc>> return setup_fault_attr(&fail_page_alloc.attr, str);
+ */
 int setup_fault_attr(struct fault_attr *attr, char *str)
 {
 	unsigned long probability;
diff --git a/lib/percpu-refcount.c b/lib/percpu-refcount.c
index 4f6c6ebbbbde..e34b157f1fc3 100644
--- a/lib/percpu-refcount.c
+++ b/lib/percpu-refcount.c
@@ -37,6 +37,12 @@
 static DEFINE_SPINLOCK(percpu_ref_switch_lock);
 static DECLARE_WAIT_QUEUE_HEAD(percpu_ref_switch_waitq);
 
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|107| <<percpu_ref_exit>> unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
+ *   - lib/percpu-refcount.c|136| <<percpu_ref_switch_to_atomic_rcu>> unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
+ *   - lib/percpu-refcount.c|196| <<__percpu_ref_switch_to_percpu>> unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
+ */
 static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)
 {
 	return (unsigned long __percpu *)
@@ -386,6 +392,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_reinit);
  * Note that percpu_ref_tryget[_live]() are safe to perform on @ref while
  * this function is in progress.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|301| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+ *   - lib/percpu-refcount.c|371| <<percpu_ref_reinit>> percpu_ref_resurrect(ref);
+ */
 void percpu_ref_resurrect(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index af88d1346dd7..6c519dbfa9fa 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -125,6 +125,15 @@ static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
 			return -1;
 		}
 
+		/*
+		 * test_and_set_bit_lock - Set a bit and return its old value, for lock
+		 * @nr: Bit to set
+		 * @addr: Address to count from
+		 *
+		 * This operation is atomic and provides acquire barrier semantics if
+		 * the returned value is 0.
+		 * It can be used to implement bit locks.
+		 */
 		if (!test_and_set_bit_lock(nr, word))
 			break;
 
@@ -418,6 +427,13 @@ void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_resize);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|128| <<__blk_mq_get_tag>> return __sbitmap_queue_get(bt);
+ *   - block/kyber-iosched.c|712| <<kyber_get_domain_token>> nr = __sbitmap_queue_get(domain_tokens);
+ *   - block/kyber-iosched.c|729| <<kyber_get_domain_token>> nr = __sbitmap_queue_get(domain_tokens);
+ *   - include/linux/sbitmap.h|432| <<sbitmap_queue_get>> nr = __sbitmap_queue_get(sbq);
+ */
 int __sbitmap_queue_get(struct sbitmap_queue *sbq)
 {
 	unsigned int hint, depth;
@@ -556,6 +572,13 @@ void sbitmap_queue_wake_up(struct sbitmap_queue *sbq)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_wake_up);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|242| <<blk_mq_put_tag>> sbitmap_queue_clear(&tags->bitmap_tags, real_tag, ctx->cpu);
+ *   - block/blk-mq-tag.c|245| <<blk_mq_put_tag>> sbitmap_queue_clear(&tags->breserved_tags, tag, ctx->cpu);
+ *   - block/kyber-iosched.c|547| <<rq_clear_domain_token>> sbitmap_queue_clear(&kqd->domain_tokens[sched_domain], nr,
+ *   - include/target/target_core_base.h|944| <<target_free_tag>> sbitmap_queue_clear(&sess->sess_tag_pool, cmd->map_tag, cmd->map_cpu);
+ */
 void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 			 unsigned int cpu)
 {
-- 
2.17.1

