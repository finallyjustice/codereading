From faae117281a4706d35fcec228075a33bc81a7a73 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 19 Apr 2020 16:17:35 -0700
Subject: [PATCH 1/1] linux block for linux-5.5

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/kernel/apic/msi.c       |   37 +
 arch/x86/kernel/apic/vector.c    |   35 +
 block/badblocks.c                |   73 ++
 block/bio.c                      |    7 +
 block/blk-cgroup.c               |   20 +
 block/blk-core.c                 |   63 ++
 block/blk-exec.c                 |   15 +
 block/blk-flush.c                |  433 +++++++++
 block/blk-lib.c                  |    5 +
 block/blk-merge.c                |   29 +
 block/blk-mq-cpumap.c            |   28 +
 block/blk-mq-debugfs.c           |    5 +
 block/blk-mq-pci.c               |   32 +
 block/blk-mq-rdma.c              |    5 +
 block/blk-mq-sched.c             |   43 +
 block/blk-mq-sysfs.c             |   19 +
 block/blk-mq-tag.c               |   33 +
 block/blk-mq-tag.h               |   53 ++
 block/blk-mq-virtio.c            |   31 +
 block/blk-mq.c                   |  730 +++++++++++++++
 block/blk-mq.h                   |   87 ++
 block/blk-settings.c             |   94 ++
 block/blk-softirq.c              |   39 +
 block/blk-stat.c                 |  275 ++++++
 block/blk-stat.h                 |   27 +
 block/blk-sysfs.c                |   19 +
 block/blk-zoned.c                |  166 ++++
 block/blk.h                      |   72 ++
 block/genhd.c                    |  113 +++
 block/partition-generic.c        |   98 ++
 block/partitions/check.c         |   99 ++
 drivers/ata/libata-scsi.c        |    3 +
 drivers/block/loop.c             |  100 ++
 drivers/block/loop.h             |   15 +
 drivers/block/null_blk.h         |  101 +++
 drivers/block/null_blk_main.c    | 1073 ++++++++++++++++++++++
 drivers/block/null_blk_zoned.c   |  101 +++
 drivers/block/virtio_blk.c       |   31 +
 drivers/nvme/host/core.c         |  864 ++++++++++++++++++
 drivers/nvme/host/fault_inject.c |   55 ++
 drivers/nvme/host/hwmon.c        |    4 +
 drivers/nvme/host/multipath.c    |   45 +
 drivers/nvme/host/nvme.h         |  149 +++
 drivers/nvme/host/pci.c          | 1463 +++++++++++++++++++++++++++++-
 drivers/nvme/target/loop.c       |    3 +
 drivers/pci/msi.c                |  105 +++
 drivers/virtio/virtio_ring.c     |   24 +
 fs/block_dev.c                   |    9 +
 fs/direct-io.c                   |   15 +
 fs/ext4/file.c                   |    5 +
 fs/iomap/direct-io.c             |   28 +
 include/linux/badblocks.h        |   61 ++
 include/linux/bio.h              |    6 +
 include/linux/blk-mq.h           |  174 ++++
 include/linux/blk_types.h        |   34 +
 include/linux/blkdev.h           |  186 ++++
 include/linux/fs.h               |   32 +
 include/linux/genhd.h            |   48 +
 include/linux/irq.h              |   68 ++
 include/linux/irqdesc.h          |    7 +
 include/linux/nvme.h             |   84 ++
 include/linux/pci.h              |    6 +
 include/linux/virtio_config.h    |    7 +
 include/scsi/scsi_device.h       |   18 +
 include/scsi/scsi_host.h         |    6 +
 include/uapi/linux/fs.h          |   11 +
 kernel/irq/affinity.c            |   14 +
 kernel/irq/chip.c                |   12 +
 kernel/irq/irqdesc.c             |   11 +
 kernel/irq/irqdomain.c           |    8 +
 kernel/irq/manage.c              |   16 +
 kernel/irq/matrix.c              |    4 +
 kernel/irq/msi.c                 |   61 ++
 kernel/irq/proc.c                |   26 +
 kernel/trace/blktrace.c          |   13 +
 lib/fault-inject.c               |   12 +
 76 files changed, 7902 insertions(+), 1 deletion(-)

diff --git a/arch/x86/kernel/apic/msi.c b/arch/x86/kernel/apic/msi.c
index 7f7533462474..e26ebeaea339 100644
--- a/arch/x86/kernel/apic/msi.c
+++ b/arch/x86/kernel/apic/msi.c
@@ -23,6 +23,37 @@
 
 static struct irq_domain *msi_default_domain;
 
+/*
+ * [0] irq_msi_compose_msg
+ * [0] irq_chip_compose_msi_msg+0x
+ * [0] msi_domain_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+
+ * [0] irq_msi_compose_msg
+ * [0] irq_chip_compose_msi_msg
+ * [0] msi_domain_activate
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void irq_msi_compose_msg(struct irq_data *data, struct msi_msg *msg)
 {
 	struct irq_cfg *cfg = irqd_cfg(data);
@@ -32,6 +63,12 @@ static void irq_msi_compose_msg(struct irq_data *data, struct msi_msg *msg)
 	if (x2apic_enabled())
 		msg->address_hi |= MSI_ADDR_EXT_DEST_ID(cfg->dest_apicid);
 
+	/*
+	 * cfg->dest_apicid和cfg->vector更新的地方:
+	 *   - arch/x86/kernel/apic/vector.c|147| <<apic_update_irq_cfg>> apicd->hw_irq_cfg.dest_apicid = apic->calc_dest_apicid(cpu);
+	 *   - arch/x86/kernel/apic/vector.c|150| <<apic_update_irq_cfg>> apicd->hw_irq_cfg.dest_apicid);
+	 */
+
 	msg->address_lo =
 		MSI_ADDR_BASE_LO |
 		((apic->irq_dest_mode == 0) ?
diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c
index 2c5676b0a6e7..35a783930ef6 100644
--- a/arch/x86/kernel/apic/vector.c
+++ b/arch/x86/kernel/apic/vector.c
@@ -114,9 +114,31 @@ static void free_apic_chip_data(struct apic_chip_data *apicd)
 	kfree(apicd);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|180| <<vector_assign_managed_shutdown>> apic_update_irq_cfg(irqd, MANAGED_IRQ_SHUTDOWN_VECTOR, cpu);
+ *   - arch/x86/kernel/apic/vector.c|252| <<assign_vector_locked>> apic_update_irq_cfg(irqd, vector, cpu);
+ *   - arch/x86/kernel/apic/vector.c|325| <<assign_managed_vector>> apic_update_irq_cfg(irqd, vector, cpu);
+ *   - arch/x86/kernel/apic/vector.c|516| <<vector_configure_legacy>> apic_update_irq_cfg(irqd, apicd->vector, apicd->cpu);
+ */
 static void apic_update_irq_cfg(struct irq_data *irqd, unsigned int vector,
 				unsigned int cpu)
 {
+	/*
+	 * struct apic_chip_data {
+	 *	struct irq_cfg          hw_irq_cfg;
+	 *	unsigned int            vector;
+	 *	unsigned int            prev_vector;
+	 *	unsigned int            cpu;
+	 *	unsigned int            prev_cpu;
+	 *	unsigned int            irq;
+	 *	struct hlist_node       clist;
+	 *	unsigned int            move_in_progress        : 1,
+	 *				is_managed              : 1,
+	 *				can_reserve             : 1,
+	 *				has_reserved            : 1;
+	 * };
+	 */
 	struct apic_chip_data *apicd = apic_chip_data(irqd);
 
 	lockdep_assert_held(&vector_lock);
@@ -304,6 +326,11 @@ assign_irq_vector_policy(struct irq_data *irqd, struct irq_alloc_info *info)
 	return reserve_irq_vector(irqd);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|427| <<activate_managed>> ret = assign_managed_vector(irqd, vector_searchmask);
+ *   - arch/x86/kernel/apic/vector.c|790| <<apic_set_affinity>> err = assign_managed_vector(irqd, vector_searchmask);
+ */
 static int
 assign_managed_vector(struct irq_data *irqd, const struct cpumask *dest)
 {
@@ -785,6 +812,14 @@ static int apic_set_affinity(struct irq_data *irqd,
 		return IRQ_SET_MASK_OK;
 
 	raw_spin_lock(&vector_lock);
+	/*
+	 * cpumask_and - *dstp = *src1p & *src2p
+	 * @dstp: the cpumask result
+	 * @src1p: the first input
+	 * @src2p: the second input
+	 *
+	 * If *@dstp is empty, returns 0, else returns 1
+	 */
 	cpumask_and(vector_searchmask, dest, cpu_online_mask);
 	if (irqd_affinity_is_managed(irqd))
 		err = assign_managed_vector(irqd, vector_searchmask);
diff --git a/block/badblocks.c b/block/badblocks.c
index 2e5f5697db35..0696c5925b32 100644
--- a/block/badblocks.c
+++ b/block/badblocks.c
@@ -16,6 +16,10 @@
 #include <linux/types.h>
 #include <linux/slab.h>
 
+/*
+ * 一个u64表示一个最多512-sector的范围
+ */
+
 /**
  * badblocks_check() - check a given range for bad sectors
  * @bb:		the badblocks structure that holds all badblock information
@@ -50,6 +54,20 @@
  * -1: there are bad blocks which have not yet been acknowledged in metadata.
  * plus the start/length of the first bad section we overlap.
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1999| <<null_handle_badblocks>> if (badblocks_check(bb, sector, nr_sectors, &first_bad, &bad_sectors))
+ *   - drivers/md/md.h|214| <<is_badblock>> int rv = badblocks_check(&rdev->badblocks, rdev->data_offset + s,
+ *   - drivers/nvdimm/nd.h|424| <<is_bad_pmem>> return !!badblocks_check(bb, sector, len / 512, &first_bad,
+ *   - drivers/nvdimm/pfn_devs.c|390| <<nd_pfn_clear_memmap_errors>> bb_present = badblocks_check(&nd_region->bb, meta_start,
+ *
+ * badblocks_check() - check a given range for bad sectors
+ * @bb:         the badblocks structure that holds all badblock information
+ * @s:          sector (start) at which to check for badblocks
+ * @sectors:    number of sectors to check for badblocks
+ * @first_bad:  pointer to store location of the first badblock
+ * @bad_sectors: pointer to store number of badblocks after @first_bad
+ */
 int badblocks_check(struct badblocks *bb, sector_t s, int sectors,
 			sector_t *first_bad, int *bad_sectors)
 {
@@ -160,6 +178,14 @@ static void badblocks_update_acked(struct badblocks *bb)
  *  0: success
  *  1: failed to set badblocks (out of space)
  */
+/*
+ * called by:
+ *   - block/badblocks.c|565| <<badblocks_store>> if (badblocks_set(bb, sector, length, !unack))
+ *   - drivers/block/null_blk_main.c|685| <<nullb_device_badblocks_store>> ret = badblocks_set(&t_dev->badblocks, start,
+ *   - drivers/md/md.c|1665| <<super_1_load>> if (badblocks_set(&rdev->badblocks, sector, count, 1))
+ *   - drivers/md/md.c|9243| <<rdev_set_badblocks>> rv = badblocks_set(&rdev->badblocks, s, sectors, 0);
+ *   - drivers/nvdimm/badrange.c|170| <<set_badblock>> if (badblocks_set(bb, s, num, 1)
+ */
 int badblocks_set(struct badblocks *bb, sector_t s, int sectors,
 			int acknowledged)
 {
@@ -291,10 +317,16 @@ int badblocks_set(struct badblocks *bb, sector_t s, int sectors,
 		} else {
 			int this_sectors = sectors;
 
+			/*
+			 * 从p+hi拷贝到p+hi+1, 相当于集体右移1格
+			 */
 			memmove(p + hi + 1, p + hi,
 				(bb->count - hi) * 8);
 			bb->count++;
 
+			/*
+			 * 一个u64最多只能表示BB_MAX_LEN=512个sector
+			 */
 			if (this_sectors > BB_MAX_LEN)
 				this_sectors = BB_MAX_LEN;
 			p[hi] = BB_MAKE(s, this_sectors, acknowledged);
@@ -464,6 +496,14 @@ EXPORT_SYMBOL_GPL(ack_all_badblocks);
  * Return:
  *  Length of returned data
  */
+/*
+ * called by:
+ *   - block/genhd.c|928| <<disk_badblocks_show>> return badblocks_show(disk->bb, page, 0);
+ *   - drivers/block/null_blk_main.c|636| <<nullb_device_badblocks_show>> return badblocks_show(&t_dev->badblocks, page, 0);
+ *   - drivers/md/md.c|3372| <<bb_show>> return badblocks_show(&rdev->badblocks, page, 0);
+ *   - drivers/md/md.c|3387| <<ubb_show>> return badblocks_show(&rdev->badblocks, page, 1);
+ *   - drivers/nvdimm/region_devs.c|540| <<region_badblocks_show>> rc = badblocks_show(&nd_region->bb, buf, 0);
+ */
 ssize_t badblocks_show(struct badblocks *bb, char *page, int unack)
 {
 	size_t len;
@@ -481,6 +521,9 @@ ssize_t badblocks_show(struct badblocks *bb, char *page, int unack)
 	i = 0;
 
 	while (len < PAGE_SIZE && i < bb->count) {
+		/*
+		 * 这里p[i]是一个u64
+		 */
 		sector_t s = BB_OFFSET(p[i]);
 		unsigned int length = BB_LEN(p[i]);
 		int ack = BB_ACK(p[i]);
@@ -490,6 +533,14 @@ ssize_t badblocks_show(struct badblocks *bb, char *page, int unack)
 		if (unack && ack)
 			continue;
 
+		/*
+		 * The return value is the number of characters which would be
+		 * generated for the given input, excluding the trailing null,
+		 * as per ISO C99.  If the return is greater than or equal to
+		 * @size, the resulting string is truncated.
+		 *
+		 * 输入的最后有一个'\n'
+		 */
 		len += snprintf(page+len, PAGE_SIZE-len, "%llu %u\n",
 				(unsigned long long)s << bb->shift,
 				length << bb->shift);
@@ -514,6 +565,12 @@ EXPORT_SYMBOL_GPL(badblocks_show);
  * Return:
  *  Length of the buffer processed or -ve error.
  */
+/*
+ * called by:
+ *   - block/genhd.c|940| <<disk_badblocks_store>> return badblocks_store(disk->bb, page, len, 0);
+ *   - drivers/md/md.c|3376| <<bb_store>> int rv = badblocks_store(&rdev->badblocks, page, len, 0);
+ *   - drivers/md/md.c|3391| <<ubb_store>> return badblocks_store(&rdev->badblocks, page, len, 1);
+ */
 ssize_t badblocks_store(struct badblocks *bb, const char *page, size_t len,
 			int unack)
 {
@@ -541,6 +598,11 @@ ssize_t badblocks_store(struct badblocks *bb, const char *page, size_t len,
 }
 EXPORT_SYMBOL_GPL(badblocks_store);
 
+/*
+ * called by:
+ *   - block/badblocks.c|577| <<badblocks_init>> return __badblocks_init(NULL, bb, enable);
+ *   - block/badblocks.c|585| <<devm_init_badblocks>> return __badblocks_init(dev, bb, 1);
+ */
 static int __badblocks_init(struct device *dev, struct badblocks *bb,
 		int enable)
 {
@@ -550,6 +612,12 @@ static int __badblocks_init(struct device *dev, struct badblocks *bb,
 		bb->shift = 0;
 	else
 		bb->shift = -1;
+	/*
+	 * devm_kzalloc():
+	 * Managed kmalloc.  Memory allocated with this function is
+	 * automatically freed on driver detach.  Like all other devres
+	 * resources, guaranteed alignment is unsigned long long.
+	 */
 	if (dev)
 		bb->page = devm_kzalloc(dev, PAGE_SIZE, GFP_KERNEL);
 	else
@@ -572,6 +640,11 @@ static int __badblocks_init(struct device *dev, struct badblocks *bb,
  *  0: success
  *  -ve errno: on error
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|865| <<null_alloc_dev>> if (badblocks_init(&dev->badblocks, 0)) {
+ *   - drivers/md/md.c|3557| <<md_rdev_init>> return badblocks_init(&rdev->badblocks, 0);
+ */
 int badblocks_init(struct badblocks *bb, int enable)
 {
 	return __badblocks_init(NULL, bb, enable);
diff --git a/block/bio.c b/block/bio.c
index 94d697217887..71fdfba6fc20 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -1752,6 +1752,13 @@ void bio_check_pages_dirty(struct bio *bio)
 	schedule_work(&bio_dirty_work);
 }
 
+/*
+ * called by:
+ *   - block/bio.c|1778| <<generic_start_io_acct>> update_io_ticks(part, jiffies);
+ *   - block/bio.c|1796| <<generic_end_io_acct>> update_io_ticks(part, now);
+ *   - block/blk-core.c|1354| <<blk_account_io_done>> update_io_ticks(part, jiffies);
+ *   - block/blk-core.c|1396| <<blk_account_io_start>> update_io_ticks(part, jiffies);
+ */
 void update_io_ticks(struct hd_struct *part, unsigned long now)
 {
 	unsigned long stamp;
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index a229b94d5390..1a21b15ada50 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -34,6 +34,26 @@
 
 #define MAX_KEY_LEN 100
 
+/*
+ * 来自mailing list
+ * Now the main block cgroup isolation policy:
+ * blk-iocost and bfq are weght based, blk-iolatency is latency based.
+ * The blk-iotrack can track the real percentage for IOs,kB,on disk time(d2c),
+ * and total time, it’s a good indicator to the real weight. For blk-iolatency, the
+ * blk-iotrack has 8 lantency thresholds to show latency distribution, so if we
+ * change these thresholds around to blk-iolateny.target.latency, we can tune
+ * the target latency to a more proper value.
+ *
+ * blk-iotrack extends the basic io.stat. It just export the important
+ * basic io statistics
+ * for cgroup,like what /prof/diskstats for block device. And it’s easy
+ * programming,
+ * iotrack just working like iostat, but focus on cgroup.
+ *
+ * blk-iotrack is friendly with these block cgroup isolation policies, a
+ * indicator for cgroup weight and lantency.
+ */
+
 /*
  * blkcg_pol_mutex protects blkcg_policy[] and policy [de]activation.
  * blkcg_pol_register_mutex nests outside of it and synchronizes entire
diff --git a/block/blk-core.c b/block/blk-core.c
index 089e890ab208..ae20c2b8935a 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -280,6 +280,12 @@ EXPORT_SYMBOL(blk_dump_rq_flags);
 void blk_sync_queue(struct request_queue *q)
 {
 	del_timer_sync(&q->timeout);
+	/*
+	 * Cancel @work and wait for its execution to finish.  This function
+	 * can be used even if the work re-queues itself or migrates to
+	 * another workqueue.  On return from this function, @work is
+	 * guaranteed to be not pending or executing on any CPU.
+	 */
 	cancel_work_sync(&q->timeout_work);
 }
 EXPORT_SYMBOL(blk_sync_queue);
@@ -460,6 +466,10 @@ static void blk_queue_usage_counter_release(struct percpu_ref *ref)
 	wake_up_all(&q->mq_freeze_wq);
 }
 
+/*
+ * 在以下使用blk_rq_timed_out_timer():
+ *   - block/blk-core.c|532| <<blk_alloc_queue_node>> timer_setup(&q->timeout, blk_rq_timed_out_timer, 0);
+ */
 static void blk_rq_timed_out_timer(struct timer_list *t)
 {
 	struct request_queue *q = from_timer(q, t, timeout);
@@ -476,6 +486,18 @@ static void blk_timeout_work(struct work_struct *work)
  * @gfp_mask: memory allocation flags
  * @node_id: NUMA node to allocate memory from
  */
+/*
+ * called by:
+ *   - block/blk-core.c|394| <<blk_alloc_queue>> return blk_alloc_queue_node(gfp_mask, NUMA_NO_NODE);
+ *   - block/blk-mq.c|2663| <<blk_mq_init_queue>> uninit_q = blk_alloc_queue_node(GFP_KERNEL, set->numa_node);
+ *   - drivers/block/drbd/drbd_main.c|2804| <<drbd_create_device>> q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
+ *   - drivers/block/null_blk_main.c|1724| <<null_add_dev>> nullb->q = blk_alloc_queue_node(GFP_KERNEL, dev->home_node);
+ *   - drivers/block/umem.c|888| <<mm_pci_probe>> card->queue = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
+ *   - drivers/lightnvm/core.c|383| <<nvm_create_tgt>> tqueue = blk_alloc_queue_node(GFP_KERNEL, dev->q->node);
+ *   - drivers/md/dm.c|1948| <<alloc_dev>> md->queue = blk_alloc_queue_node(GFP_KERNEL, numa_node_id);
+ *   - drivers/nvdimm/pmem.c|404| <<pmem_attach_disk>> q = blk_alloc_queue_node(GFP_KERNEL, dev_to_node(dev));
+ *   - drivers/nvme/host/multipath.c|380| <<nvme_mpath_alloc_disk>> q = blk_alloc_queue_node(GFP_KERNEL, ctrl->numa_node);
+ */
 struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 {
 	struct request_queue *q;
@@ -1390,6 +1412,10 @@ void blk_account_io_start(struct request *rq, bool new_io)
  * Steal bios from a request and add them to a bio list.
  * The request must not have been partially completed before.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|78| <<nvme_failover_req>> blk_steal_bios(&ns->head->requeue_list, req);
+ */
 void blk_steal_bios(struct bio_list *list, struct request *rq)
 {
 	if (rq->bio) {
@@ -1433,6 +1459,35 @@ EXPORT_SYMBOL_GPL(blk_steal_bios);
  *     %false - this request doesn't have any more data
  *     %true  - this request has more data
  **/
+/*
+ * called by:
+ *   - arch/um/drivers/ubd_kern.c|530| <<ubd_handler>> if (!blk_update_request(io_req->req, io_req->error, io_req->length))
+ *   - block/blk-mq.c|597| <<blk_mq_end_request>> if (blk_update_request(rq, error, blk_rq_bytes(rq)))
+ *   - drivers/block/amiflop.c|1518| <<amiflop_queue_rq>> } while (blk_update_request(rq, err, blk_rq_cur_bytes(rq)));
+ *   - drivers/block/aoe/aoecmd.c|1054| <<aoe_end_request>> } while (blk_update_request(rq, bok ? BLK_STS_OK : BLK_STS_IOERR, bio->bi_iter.bi_size));
+ *   - drivers/block/ataflop.c|459| <<fd_end_request_cur>> if (!blk_update_request(fd_request, err,
+ *   - drivers/block/floppy.c|2223| <<floppy_end_request>> if (blk_update_request(req, error, nr_sectors << 9))
+ *   - drivers/block/loop.c|482| <<lo_complete_rq>> blk_update_request(rq, BLK_STS_OK, cmd->ret);
+ *   - drivers/block/paride/pcd.c|836| <<next_request>> if (!blk_update_request(pcd_req, err, blk_rq_cur_bytes(pcd_req))) {
+ *   - drivers/block/paride/pd.c|458| <<run_fsm>> if (!blk_update_request(pd_req, err,
+ *   - drivers/block/paride/pd.c|528| <<pd_next_buf>> if (!blk_update_request(pd_req, 0, blk_rq_cur_bytes(pd_req))) {
+ *   - drivers/block/paride/pf.c|831| <<pf_end_request>> if (!blk_update_request(pf_req, err, blk_rq_cur_bytes(pf_req))) {
+ *   - drivers/block/swim.c|546| <<swim_queue_rq>> } while (blk_update_request(req, err, blk_rq_cur_bytes(req)));
+ *   - drivers/block/swim3.c|266| <<swim3_end_request>> if (blk_update_request(req, err, nr_bytes))
+ *   - drivers/block/swim3.c|741| <<swim3_interrupt>> blk_update_request(req, 0, n << 9);
+ *   - drivers/block/xsysace.c|737| <<ace_fsm_dostate>> if (blk_update_request(ace->req, BLK_STS_OK,
+ *   - drivers/ide/ide-io.c|70| <<ide_end_rq>> if (!blk_update_request(rq, error, nr_bytes)) {
+ *   - drivers/md/dm-rq.c|123| <<end_clone_bio>> blk_update_request(tio->orig, BLK_STS_OK, tio->completed);
+ *   - drivers/memstick/core/ms_block.c|1912| <<msb_io_work>> if (len && !blk_update_request(req, BLK_STS_OK, len)) {
+ *   - drivers/memstick/core/mspro_block.c|710| <<mspro_block_issue_req>> chunk = blk_update_request(msb->block_req,
+ *   - drivers/memstick/core/mspro_block.c|774| <<mspro_block_complete_req>> chunk = blk_update_request(msb->block_req,
+ *   - drivers/mmc/core/block.c|1439| <<mmc_blk_cqe_complete_rq>> if (blk_update_request(req, BLK_STS_OK, mrq->data->bytes_xfered))
+ *   - drivers/mmc/core/block.c|1688| <<mmc_blk_read_single>> } while (blk_update_request(req, error, 512));
+ *   - drivers/mmc/core/block.c|1694| <<mmc_blk_read_single>> blk_update_request(req, BLK_STS_IOERR, 512);
+ *   - drivers/mmc/core/block.c|1893| <<mmc_blk_mq_complete_rq>> if (blk_update_request(req, BLK_STS_OK, nr_bytes))
+ *   - drivers/mtd/mtd_blkdevs.c|175| <<mtd_blktrans_work>> if (!blk_update_request(req, res, blk_rq_cur_bytes(req))) {
+ *   - drivers/scsi/scsi_lib.c|576| <<scsi_end_request>> if (blk_update_request(req, error, bytes))
+ */
 bool blk_update_request(struct request *req, blk_status_t error,
 		unsigned int nr_bytes)
 {
@@ -1764,6 +1819,14 @@ struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug, void *data,
 }
 EXPORT_SYMBOL(blk_check_plugged);
 
+/*
+ * called by:
+ *   - block/blk-core.c|1805| <<blk_finish_plug>> blk_flush_plug_list(plug, false);
+ *   - block/blk-mq.c|2263| <<blk_mq_make_request>> blk_flush_plug_list(plug, false);
+ *   - block/blk-mq.c|4094| <<blk_poll>> blk_flush_plug_list(current->plug, false);
+ *   - include/linux/blkdev.h|1332| <<blk_flush_plug>> blk_flush_plug_list(plug, false);
+ *   - include/linux/blkdev.h|1340| <<blk_schedule_flush_plug>> blk_flush_plug_list(plug, true);
+ */
 void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 {
 	flush_plug_callbacks(plug, from_schedule);
diff --git a/block/blk-exec.c b/block/blk-exec.c
index e20a852ae432..2fdeb7867228 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -45,6 +45,21 @@ static void blk_end_sync_rq(struct request *rq, blk_status_t error)
  * Note:
  *    This function will invoke @done directly if the queue is dead.
  */
+/*
+ * called by:
+ *   - block/blk-exec.c|86| <<blk_execute_rq>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, blk_end_sync_rq);
+ *   - drivers/block/sx8.c|542| <<carm_array_info>> blk_execute_rq_nowait(host->oob_q, NULL, rq, true, NULL);
+ *   - drivers/block/sx8.c|581| <<carm_send_special>> blk_execute_rq_nowait(host->oob_q, NULL, rq, true, NULL);
+ *   - drivers/nvme/host/core.c|1031| <<nvme_execute_rq_polled>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, nvme_end_sync_rq);
+ *   - drivers/nvme/host/core.c|1256| <<nvme_keep_alive>> blk_execute_rq_nowait(rq->q, NULL, rq, 0, nvme_keep_alive_end_io);
+ *   - drivers/nvme/host/lightnvm.c|698| <<nvme_nvm_submit_io>> blk_execute_rq_nowait(q, NULL, rq, 0, nvme_nvm_end_io);
+ *   - drivers/nvme/host/pci.c|1893| <<nvme_timeout>> blk_execute_rq_nowait(abort_req->q, NULL, abort_req, 0, abort_endio);
+ *   - drivers/nvme/host/pci.c|3189| <<nvme_delete_queue>> blk_execute_rq_nowait(q, NULL, req, false,
+ *   - drivers/scsi/scsi_error.c|1994| <<scsi_eh_lock_door>> blk_execute_rq_nowait(req->q, NULL, req, 1, eh_lock_door_done);
+ *   - drivers/scsi/sg.c|836| <<sg_common_write>> blk_execute_rq_nowait(sdp->device->request_queue, sdp->disk,
+ *   - drivers/scsi/st.c|587| <<st_scsi_execute>> blk_execute_rq_nowait(req->q, NULL, req, 1, st_scsi_execute_end);
+ *   - drivers/target/target_core_pscsi.c|1003| <<pscsi_execute_cmd>> blk_execute_rq_nowait(pdv->pdv_sd->request_queue, NULL, req,
+ */
 void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 			   struct request *rq, int at_head,
 			   rq_end_io_fn *done)
diff --git a/block/blk-flush.c b/block/blk-flush.c
index 3f977c517960..9aa810fcab67 100644
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@ -76,13 +76,81 @@
 #include "blk-mq-tag.h"
 #include "blk-mq-sched.h"
 
+/*
+ * http://www.unjeep.com/article/40696.html
+ *
+ * 硬盘在控制器上的一块内存芯片,其类型一般以SDRAM为主,具有极快的存取速度,
+ * 它是硬盘内部存储和外界接口之间的缓冲器.由于硬盘的内部数据传输速度和外界
+ * 介面传输速度不同,缓存在其中起到一个缓冲的作用.缓存的大小与速度是直接关
+ * 系到硬盘的传输速度的重要因素,能够大幅度地提高硬盘整体性能.
+ *
+ * 如果硬盘的cache启用了,那么很有可能写入的数据是写到了硬盘的cache中,而没
+ * 有真正写到磁盘介质上.
+ *
+ * 在linux下,查看磁盘cache是否开启可通过hdparm命令:
+ *
+ * #hdparm -W /dev/sdx    //是否开启cache，1为enable
+ * #hdparm -W 0 /dev/sdx  //关闭cache
+ * #hdparm -W 1 /dev/sdx  //enable cache
+ *
+ * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+ * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+ *
+ * blk_insert_flush()是非常重要的入口!
+ *
+ *
+ * 部分调用blk_queue_write_cache()的例子:
+ *   - drivers/block/loop.c|1007| <<loop_set_fd>> blk_queue_write_cache(lo->lo_queue, true, false);
+ *   - drivers/block/nbd.c|1136| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, true);
+ *   - drivers/block/nbd.c|1138| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, false);
+ *   - drivers/block/nbd.c|1141| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, false, false);
+ *   - drivers/block/null_blk_main.c|1742| <<null_add_dev>> blk_queue_write_cache(nullb->q, true, true);
+ *   - rivers/block/virtio_blk.c|609| <<virtblk_update_cache_mode>> blk_queue_write_cache(vblk->disk->queue, writeback, false);
+ *   - drivers/block/xen-blkfront.c|1014| <<xlvbd_flush>> blk_queue_write_cache(info->rq, info->feature_flush ? true : false,
+ *   - drivers/ide/ide-disk.c|554| <<update_flush>> blk_queue_write_cache(drive->queue, wc, false);
+ *   - drivers/md/dm-table.c|1910| <<dm_table_set_restrictions>> blk_queue_write_cache(q, wc, fua);
+ *   - drivers/md/md.c|5506| <<md_alloc>> blk_queue_write_cache(mddev->queue, true, true);
+ *   - drivers/nvme/host/core.c|2204| <<nvme_set_queue_limits>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/nvme/host/multipath.c|393| <<nvme_mpath_alloc_disk>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/scsi/sd.c|152| <<sd_set_flush_flag>> blk_queue_write_cache(sdkp->disk->queue, wc, fua);
+ */
+
+/*
+ * 冲刷的过程中request_queue为什么要使用双缓冲队列来存放fs_request?
+ *
+ * 双缓冲队列可以做到只执行一次冲刷请求就可以完成多个fs_request的冲刷要求.队列自
+ * 带的冲刷request在执行的过程中,blk_insert_flush()可以被调用多次,来自上层的
+ * fs_request被添加到pending1队列,等待冲刷request的下一次执行,当冲刷requst可以再
+ * 次被执行时,pending1队列不再接收新的fs_request(fs_request被加入到pending2队列),
+ * 冲刷request执行完毕后,pending1队列所有的fs_request的PREFLUSH/POSTFLUSH执行完毕.
+ * The actual execution of flush is double buffered.  Whenever a request
+ * needs to execute PRE or POSTFLUSH, it queues at
+ * fq->flush_queue[fq->flush_pending_idx].  Once certain criteria are met, a
+ * REQ_OP_FLUSH is issued and the pending_idx is toggled.  When the flush
+ * completes, all the requests which were pending are proceeded to the next
+ * step.  This allows arbitrary merging of different types of PREFLUSH/FUA
+ * requests.
+ */
+
 /* PREFLUSH/FUA sequences */
 enum {
+	/*
+	 * 在以下使用REQ_FSEQ_PREFLUSH:
+	 *   - block/blk-flush.c|117| <<global>> REQ_FSEQ_ACTIONS = REQ_FSEQ_PREFLUSH | REQ_FSEQ_DATA |
+	 *   - block/blk-flush.c|143| <<blk_flush_policy>> policy |= REQ_FSEQ_PREFLUSH;
+	 *   - block/blk-flush.c|244| <<blk_flush_complete_seq>> case REQ_FSEQ_PREFLUSH:
+	 *   - block/blk-flush.c|323| <<flush_end_io>> BUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);
+	 *   - block/blk-flush.c|486| <<blk_insert_flush>> !(policy & (REQ_FSEQ_PREFLUSH | REQ_FSEQ_POSTFLUSH))) {
+	 */
 	REQ_FSEQ_PREFLUSH	= (1 << 0), /* pre-flushing in progress */
 	REQ_FSEQ_DATA		= (1 << 1), /* data write in progress */
 	REQ_FSEQ_POSTFLUSH	= (1 << 2), /* post-flushing in progress */
 	REQ_FSEQ_DONE		= (1 << 3),
 
+	/*
+	 * 在以下使用REQ_FSEQ_ACTIONS:
+	 *   - block/blk-flush.c|511| <<blk_insert_flush>> blk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);
+	 */
 	REQ_FSEQ_ACTIONS	= REQ_FSEQ_PREFLUSH | REQ_FSEQ_DATA |
 				  REQ_FSEQ_POSTFLUSH,
 
@@ -96,16 +164,52 @@ enum {
 static void blk_kick_flush(struct request_queue *q,
 			   struct blk_flush_queue *fq, unsigned int flags);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|377| <<blk_insert_flush>> unsigned int policy = blk_flush_policy(fflags, rq);
+ *
+ * 根据情况把REQ_FSEQ_DATA,REQ_FSEQ_PREFLUSH和REQ_FSEQ_POSTFLUSH返回到返回值中
+ * 这些可以表明flush需要做的步骤(包含3步)
+ *  - REQ_FSEQ_PREFLUSH(在数据请求以前冲刷磁盘缓存)
+ *  - REQ_FSEQ_DATA(写入数据请求)
+ *  - REQ_FSEQ_POSTFLUSH(在数据请求之后冲刷磁盘缓存)
+ * 如果磁盘不支持FUA,那么可以使用REQ_FSEQ_POSTFLUSH代替
+ * 假定我们分析的场景中,磁盘不支持FUA,则最终我们的冲刷策略为3步都做(policy=111).
+ */
 static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
 {
+	/*
+	 * 因为blk_flush_policy()只被blk_insert_flush()调用
+	 * 所以参数的fflags来自q->queue_flags
+	 */
 	unsigned int policy = 0;
 
 	if (blk_rq_sectors(rq))
 		policy |= REQ_FSEQ_DATA;
 
+	/*
+	 * 在以下设置QUEUE_FLAG_WC的几个例子:
+	 *   - block/blk-settings.c|824| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+	 *   - block/blk-settings.c|826| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_WC, q)
+	 *   - block/blk-sysfs.c|530| <<queue_wc_store>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+	 *   - drivers/md/dm-table.c|1905| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_WC))) {
+	 */
 	if (fflags & (1UL << QUEUE_FLAG_WC)) {
 		if (rq->cmd_flags & REQ_PREFLUSH)
 			policy |= REQ_FSEQ_PREFLUSH;
+		/*
+		 * 使用QUEUE_FLAG_FUA的地方:
+		 *   - block/blk-flush.c|158| <<blk_flush_policy>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&
+		 *   - block/blk-flush.c|471| <<blk_insert_flush>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)))
+		 *   - block/blk-settings.c|828| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_FUA, q);
+		 *   - block/blk-settings.c|830| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_FUA, q);
+		 *   - block/blk-sysfs.c|539| <<queue_fua_show>> return sprintf(page, "%u\n", test_bit(QUEUE_FLAG_FUA, &q->queue_flags));
+		 *   - drivers/md/dm-table.c|1907| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_FUA)))
+		 *   - drivers/target/target_core_iblock.c|703| <<iblock_execute_rw>> if (test_bit(QUEUE_FLAG_FUA, &q->queue_flags)) {
+		 *   - include/linux/blkdev.h|733| <<blk_queue_fua>> #define blk_queue_fua(q) test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
+		 *
+		 * 如果磁盘不支持FUA,那么可以使用REQ_FSEQ_POSTFLUSH代替
+		 */
 		if (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&
 		    (rq->cmd_flags & REQ_FUA))
 			policy |= REQ_FSEQ_POSTFLUSH;
@@ -113,11 +217,27 @@ static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
 	return policy;
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|184| <<blk_flush_complete_seq>> seq = blk_flush_cur_seq(rq);
+ *   - block/blk-flush.c|262| <<flush_end_io>> unsigned int seq = blk_flush_cur_seq(rq);
+ *
+ * 获得rq->flush.seq中第一个为0的bit (因为rq->flush.seq表示的是可以跳过的)
+ * struct {
+ *     unsigned int            seq;
+ *     struct list_head        list;
+ *     rq_end_io_fn            *saved_end_io;
+ * } flush;
+ */
 static unsigned int blk_flush_cur_seq(struct request *rq)
 {
 	return 1 << ffz(rq->flush.seq);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|211| <<blk_flush_complete_seq>> blk_flush_restore_request(rq);
+ */
 static void blk_flush_restore_request(struct request *rq)
 {
 	/*
@@ -132,11 +252,20 @@ static void blk_flush_restore_request(struct request *rq)
 	rq->end_io = rq->flush.saved_end_io;
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|199| <<blk_flush_complete_seq>> blk_flush_queue_rq(rq, true);
+ *   - block/blk-flush.c|341| <<blk_kick_flush>> blk_flush_queue_rq(flush_rq, false);
+ */
 static void blk_flush_queue_rq(struct request *rq, bool add_front)
 {
 	blk_mq_add_to_requeue_list(rq, add_front, true);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|231| <<flush_end_io>> blk_account_io_flush(flush_rq);
+ */
 static void blk_account_io_flush(struct request *rq)
 {
 	struct hd_struct *part = &rq->rq_disk->part0;
@@ -164,11 +293,65 @@ static void blk_account_io_flush(struct request *rq)
  * RETURNS:
  * %true if requests were added to the dispatch queue, %false otherwise.
  */
+/*
+ * [0] blk_flush_complete_seq
+ * [0] blk_insert_flush
+ * [0] blk_mq_make_request
+ * [0] generic_make_request
+ * [0] submit_bio
+ * [0] submit_bh_wbc.isra.55
+ * [0] jbd2_write_superblock
+ * [0] jbd2_mark_journal_empty
+ * [0] jbd2_journal_flush
+ * [0] ext4_mark_recovery_complete.isra.224
+ * [0] ext4_fill_super
+ * [0] mount_bdev
+ * [0] legacy_get_tree
+ * [0] vfs_get_tree
+ * [0] do_mount
+ * [0] do_mount_root
+ * [0] mount_block_root
+ * [0] mount_root
+ * [0] prepare_namespace
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] blk_flush_complete_seq
+ * [0] mq_flush_data_end_io
+ * [0] blk_mq_complete_request
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * called by:
+ *   - block/blk-flush.c|265| <<flush_end_io>> blk_flush_complete_seq(rq, fq, seq, error);
+ *   - block/blk-flush.c|366| <<mq_flush_data_end_io>> blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error);
+ *   - block/blk-flush.c|444| <<blk_insert_flush>> blk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);
+ */
 static void blk_flush_complete_seq(struct request *rq,
 				   struct blk_flush_queue *fq,
 				   unsigned int seq, blk_status_t error)
 {
 	struct request_queue *q = rq->q;
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下使用flush_pending_idx:
+	 *   - block/blk-flush.c|276| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|392| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 * 默认初始化是1
+	 *
+	 * 在这个函数里REQ_FSEQ_DATA和REQ_FSEQ_DONE都不用pending (&fq->flush_queue[fq->flush_pending_idx])
+	 * 只有REQ_FSEQ_PREFLUSH和REQ_FSEQ_POSTFLUSH才用
+	 */
 	struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
 	unsigned int cmd_flags;
 
@@ -176,22 +359,52 @@ static void blk_flush_complete_seq(struct request *rq,
 	rq->flush.seq |= seq;
 	cmd_flags = rq->cmd_flags;
 
+	/*
+	 * 第一次从blk_insert_flush()进来, error是0
+	 *
+	 * blk_flush_cur_seq():
+	 * 获得rq->flush.seq中第一个为0的bit (因为rq->flush.seq表示的是可以跳过的)
+	 * struct {
+	 *     unsigned int            seq;
+	 *     struct list_head        list;
+	 *     rq_end_io_fn            *saved_end_io;
+	 * } flush;
+	 *
+	 * 假设三步都要执行,第一次到这里就是REQ_FSEQ_PREFLUSH
+	 */
 	if (likely(!error))
 		seq = blk_flush_cur_seq(rq);
 	else
 		seq = REQ_FSEQ_DONE;
 
+	/*
+	 * 假设三步都要执行,
+	 * 第一次从blk_insert_flush()到这里就是REQ_FSEQ_PREFLUSH
+	 */
 	switch (seq) {
 	case REQ_FSEQ_PREFLUSH:
 	case REQ_FSEQ_POSTFLUSH:
 		/* queue for flush */
+		/*
+		 * pending来自上面的&fq->flush_queue[fq->flush_pending_idx];
+		 */
 		if (list_empty(pending))
 			fq->flush_pending_since = jiffies;
+		/*
+		 * 插入的时候插入的是tail!!!!!
+		 */
 		list_move_tail(&rq->flush.list, pending);
 		break;
 
 	case REQ_FSEQ_DATA:
 		list_move_tail(&rq->flush.list, &fq->flush_data_in_flight);
+		/*
+		 * called by:
+		 *   - block/blk-flush.c|199| <<blk_flush_complete_seq>> blk_flush_queue_rq(rq, true);
+		 *   - block/blk-flush.c|341| <<blk_kick_flush>> blk_flush_queue_rq(flush_rq, false);
+		 *
+		 * 下面下发end的时候应该调用mq_flush_data_end_io()
+		 */
 		blk_flush_queue_rq(rq, true);
 		break;
 
@@ -215,6 +428,10 @@ static void blk_flush_complete_seq(struct request *rq,
 	blk_kick_flush(q, fq, cmd_flags);
 }
 
+/*
+ * 在以下使用flush_end_io():
+ *   - block/blk-flush.c|339| <<blk_kick_flush>> flush_rq->end_io = flush_end_io;
+ */
 static void flush_end_io(struct request *flush_rq, blk_status_t error)
 {
 	struct request_queue *q = flush_rq->q;
@@ -251,10 +468,40 @@ static void flush_end_io(struct request *flush_rq, blk_status_t error)
 	BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
 
 	/* account completion of the flush request */
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下修改flush_running_idx: 
+	 *   - block/blk-flush.c|362| <<flush_end_io>> fq->flush_running_idx ^= 1;
+	 */
 	fq->flush_running_idx ^= 1;
 
 	/* and push the waiting requests to the next stage */
 	list_for_each_entry_safe(rq, n, running, flush.list) {
+		/*
+		 * 这里的代码写的太隐晦了
+		 *
+		 * 假设是PREFLUSH执行完了到的这里, seq没清空过
+		 * 所以这里获得rq->flush.seq中第一个为0的bit 
+		 * 因为rq->flush.seq表示的是可以跳过的)肯定还是PREFLUSH
+		 *
+		 * 所以seq作为参数传入下面的blk_flush_complete_seq()的时候
+		 * 是告诉后面PREFLUSH要被跳过了!!!
+		 */
+		/*
+		 * 但是这里又不明白为什么是list_for_each_entry_safe()遍历所有的request.
+		 * 根据代码阅读发现这些request实际会被blk_flush_complete_seq()忽略preflush.
+		 * 相当于下发了多个preflush只执行了一次.
+		 * 根据下面的资料可以解释 (http://www.unjeep.com/article/40696.html):
+		 *
+		 * 冲刷的过程中request_queue为什么要使用双缓冲队列来存放fs_request.?
+                 * A:双缓冲队列可以做到只执行一次冲刷请求就可以完成多个fs_request的冲刷要求.
+		 * 队列自带的冲刷request在执行的过程中,blk_insert_flush()可以被调用多次,来自
+		 * 上层的fs_request被添加到pending1队列,等待冲刷request的下一次执行,当冲刷
+		 * requst可以再次被执行时,pending1队列不再接收新的fs_request(fs_request被加入
+		 * 到pending2队列),冲刷request执行完毕后,pending1队列所有的fs_request的
+		 * PREFLUSH/POSTFLUSH执行完毕.
+		 */
 		unsigned int seq = blk_flush_cur_seq(rq);
 
 		BUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);
@@ -278,15 +525,60 @@ static void flush_end_io(struct request *flush_rq, blk_status_t error)
  * spin_lock_irq(fq->mq_flush_lock)
  *
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|415| <<blk_flush_complete_seq>> blk_kick_flush(q, fq, cmd_flags);
+ */
 static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 			   unsigned int flags)
 {
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下使用flush_pending_idx:
+	 *   - block/blk-flush.c|276| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|392| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 *
+	 * 
+	 * 使用flush_queue[2]的地方:
+	 *   - block/blk-flush.c|352| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|454| <<flush_end_io>> running = &fq->flush_queue[fq->flush_running_idx];
+	 *   - block/blk-flush.c|501| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|849| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[0]);
+	 *   - block/blk-flush.c|850| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[1]);
+	 */
 	struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	/*
+	 * 插入的是tail, 用的是head
+	 * 注意, 这里只是用, 没有取出来!!!
+	 */
 	struct request *first_rq =
 		list_first_entry(pending, struct request, flush.list);
+	/*
+	 * 使用flush_rq的地方:
+	 *   - block/blk-flush.c|504| <<blk_kick_flush>> struct request *flush_rq = fq->flush_rq;
+	 *   - block/blk-flush.c|845| <<blk_alloc_flush_queue>> fq->flush_rq = kzalloc_node(rq_sz, flags, node);
+	 *   - block/blk-flush.c|846| <<blk_alloc_flush_queue>> if (!fq->flush_rq)
+	 *   - block/blk-flush.c|875| <<blk_free_flush_queue>> kfree(fq->flush_rq);
+	 *   - block/blk-mq.c|2303| <<blk_mq_exit_hctx>> set->ops->exit_request(set, hctx->fq->flush_rq, hctx_idx);
+	 *   - block/blk-mq.c|2361| <<blk_mq_init_hctx>> if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx,
+	 *   - block/blk.h|81| <<is_flush_rq>> return hctx->fq->flush_rq == req;
+	 */
 	struct request *flush_rq = fq->flush_rq;
 
 	/* C1 described at the top of this file */
+	/*
+	 * C1. At any given time, only one flush shall be in progress.  This makes
+	 *     double buffering sufficient.
+	 *
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 *
+	 * 在以下修改flush_running_idx:
+	 *   - block/blk-flush.c|362| <<flush_end_io>> fq->flush_running_idx ^= 1;
+	 */
 	if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
 		return;
 
@@ -296,6 +588,16 @@ static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 	 * assigned to empty flushes, and we deadlock if we are expecting
 	 * other requests to make progress. Don't defer for that case.
 	 */
+	/*
+	 * C2. Flush is deferred if any request is executing DATA of its sequence.
+	 *     This avoids issuing separate POSTFLUSHes for requests which shared
+	 *     PREFLUSH.
+	 *
+	 * C3. The second condition is ignored if there is a request which has
+	 *     waited longer than FLUSH_PENDING_TIMEOUT.  This is to avoid
+	 *     starvation in the unlikely case where there are continuous stream of
+	 *     FUA (without PREFLUSH) requests.
+	 */
 	if (!list_empty(&fq->flush_data_in_flight) && q->elevator &&
 	    time_before(jiffies,
 			fq->flush_pending_since + FLUSH_PENDING_TIMEOUT))
@@ -337,6 +639,31 @@ static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 	blk_flush_queue_rq(flush_rq, false);
 }
 
+/*
+ * [0] mq_flush_data_end_io
+ * [0] blk_mq_complete_request
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * [0] mq_flush_data_end_io
+ * [0] blk_mq_complete_request
+ * [0] nvme_irq
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * 在以下使用mq_flush_data_end_io():
+ *   - block/blk-flush.c|437| <<blk_insert_flush>> rq->end_io = mq_flush_data_end_io;
+ */
 static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
 {
 	struct request_queue *q = rq->q;
@@ -370,13 +697,54 @@ static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
  * @rq is being submitted.  Analyze what needs to be done and put it on the
  * right queue.
  */
+/*
+ * [0] blk_insert_flush
+ * [0] blk_mq_make_request
+ * [0] generic_make_request
+ * [0] submit_bio
+ * [0] submit_bh_wbc.isra
+ * [0] __sync_dirty_buffer
+ * [0] ext4_commit_super
+ * [0] ext4_mark_recovery_complete.isra
+ * [0] ext4_fill_super
+ * [0] mount_bdev
+ * [0] legacy_get_tree
+ * [0] vfs_get_tree
+ * [0] do_mount
+ * [0] do_mount_root
+ * [0] mount_block_root
+ * [0] mount_root
+ * [0] prepare_namespace
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - block/blk-mq-sched.c|388| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+ *   - block/blk-mq.c|2011| <<blk_mq_make_request>> blk_insert_flush(rq);
+ */
 void blk_insert_flush(struct request *rq)
 {
 	struct request_queue *q = rq->q;
 	unsigned long fflags = q->queue_flags;	/* may change, cache */
+	/*
+	 * 根据情况把REQ_FSEQ_DATA,REQ_FSEQ_PREFLUSH和REQ_FSEQ_POSTFLUSH返回到返回值中
+	 * 这些可以表明flush需要做的步骤(包含3步) ---> 也就是说,policy的3个bit表明要做哪些步骤!!!!
+	 *  - REQ_FSEQ_PREFLUSH(在数据请求以前冲刷磁盘缓存)
+	 *  - REQ_FSEQ_DATA(写入数据请求)
+	 *  - REQ_FSEQ_POSTFLUSH(在数据请求之后冲刷磁盘缓存)
+	 * 如果磁盘不支持FUA,那么可以使用REQ_FSEQ_POSTFLUSH代替
+	 * 假定我们分析的场景中,磁盘不支持FUA,则最终我们的冲刷策略为3步都做(policy=111).
+	 */
 	unsigned int policy = blk_flush_policy(fflags, rq);
+	/* 根据rq获取对应的hctx的blk_mq_hw_ctx->fq */
 	struct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);
 
+	/*
+	 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+	 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+	 */
+
 	/*
 	 * @policy now records what operations need to be done.  Adjust
 	 * REQ_PREFLUSH and FUA for the driver.
@@ -398,6 +766,10 @@ void blk_insert_flush(struct request *rq)
 	 * advertise a write-back cache.  In this case, simply
 	 * complete the request.
 	 */
+	/*
+	 * 正如上面注释,policy的3个bit表明要做哪些步骤!!!!
+	 * 没设置任何bit说明什么也不做
+	 */
 	if (!policy) {
 		blk_mq_end_request(rq, 0);
 		return;
@@ -422,12 +794,40 @@ void blk_insert_flush(struct request *rq)
 	 */
 	memset(&rq->flush, 0, sizeof(rq->flush));
 	INIT_LIST_HEAD(&rq->flush.list);
+	/*
+	 * 在以下使用RQF_FLUSH_SEQ:
+	 *   - block/blk-core.c|244| <<req_bio_endio>> if (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))
+	 *   - block/blk-core.c|1347| <<blk_account_io_done>> !(req->rq_flags & RQF_FLUSH_SEQ)) {
+	 *   - block/blk-flush.c|205| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|426| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|528| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+	 *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> if (rq->rq_flags & RQF_FLUSH_SEQ) {
+	 *   - block/blk-mq-sched.c|387| <<blk_mq_sched_insert_request>> if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
+	 *   - include/linux/blkdev.h|120| <<RQF_NOMERGE_FLAGS>> (RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
+	 */
 	rq->rq_flags |= RQF_FLUSH_SEQ;
+	/*
+	 * struct {
+	 *     unsigned int            seq;
+	 *     struct list_head        list;
+	 *     rq_end_io_fn            *saved_end_io;
+	 * } flush;
+	 *
+	 * 使用saved_end_io的地方:
+	 *   - block/blk-flush.c|221| <<blk_flush_restore_request>> rq->end_io = rq->flush.saved_end_io;
+	 *   - block/blk-flush.c|615| <<blk_insert_flush>> rq->flush.saved_end_io = rq->end_io;
+	 */
 	rq->flush.saved_end_io = rq->end_io; /* Usually NULL */
 
 	rq->end_io = mq_flush_data_end_io;
 
 	spin_lock_irq(&fq->mq_flush_lock);
+	/*
+	 * 参数REQ_FSEQ_ACTIONS & ~policy表示可以跳过的步骤!!!
+	 * blk_insert_flush()初始化request的完成函数为flush_data_end_io()后,
+	 * 调用REQ_FSEQ_ACTIONS & ~policy把可以跳过的步骤对应的位置填1,
+	 * 调用blk_flush_complete_seq()开始冲刷过程.
+	 */
 	blk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);
 	spin_unlock_irq(&fq->mq_flush_lock);
 }
@@ -443,6 +843,31 @@ void blk_insert_flush(struct request *rq)
  *    room for storing the error offset in case of a flush error, if they
  *    wish to.
  */
+/*
+ * called by:
+ *   - drivers/md/dm-integrity.c|2551| <<bitmap_flush_work>> blkdev_issue_flush(ic->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|663| <<dmz_write_sb>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|705| <<dmz_write_dirty_mblocks>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|774| <<dmz_flush_metadata>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/raid5-ppl.c|1040| <<ppl_recover>> ret = blkdev_issue_flush(rdev->bdev, GFP_KERNEL, NULL);
+ *   - drivers/nvme/target/io-cmd-bdev.c|229| <<nvmet_bdev_flush>> if (blkdev_issue_flush(req->ns->bdev, GFP_KERNEL, NULL))
+ *   - fs/block_dev.c|675| <<blkdev_fsync>> error = blkdev_issue_flush(bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/fsync.c|179| <<ext4_sync_file>> err = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/ialloc.c|1422| <<ext4_init_inode_table>> blkdev_issue_flush(sb->s_bdev, GFP_NOFS, NULL);
+ *   - fs/ext4/super.c|5170| <<ext4_sync_fs>> err = blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/fat/file.c|198| <<fat_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/inode.c|343| <<hfsplus_file_fsync>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/super.c|242| <<hfsplus_sync_fs>> blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/jbd2/checkpoint.c|417| <<jbd2_cleanup_journal_tail>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|778| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|885| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/recovery.c|289| <<jbd2_journal_recover>> err2 = blkdev_issue_flush(journal->j_fs_dev, GFP_KERNEL, NULL);
+ *   - fs/libfs.c|1044| <<generic_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/nilfs2/the_nilfs.h|378| <<nilfs_flush_device>> err = blkdev_issue_flush(nilfs->ns_bdev, GFP_KERNEL, NULL);
+ *   - fs/ocfs2/file.c|197| <<ocfs2_sync_file>> ret = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/reiserfs/file.c|162| <<reiserfs_sync_file>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/xfs/xfs_super.c|339| <<xfs_blkdev_issue_flush>> blkdev_issue_flush(buftarg->bt_bdev, GFP_NOFS, NULL);
+ */
 int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 		sector_t *error_sector)
 {
@@ -485,6 +910,10 @@ int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 }
 EXPORT_SYMBOL(blkdev_issue_flush);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2415| <<blk_mq_alloc_hctx>> hctx->fq = blk_alloc_flush_queue(q, hctx->numa_node, set->cmd_size,
+ */
 struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 		int node, int cmd_size, gfp_t flags)
 {
@@ -517,6 +946,10 @@ struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|43| <<blk_mq_hw_sysfs_release>> blk_free_flush_queue(hctx->fq);
+ */
 void blk_free_flush_queue(struct blk_flush_queue *fq)
 {
 	/* bio based request queue hasn't flush queue */
diff --git a/block/blk-lib.c b/block/blk-lib.c
index 5f2c429d4378..f526ffa3bfb9 100644
--- a/block/blk-lib.c
+++ b/block/blk-lib.c
@@ -209,6 +209,11 @@ int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 }
 EXPORT_SYMBOL(blkdev_issue_write_same);
 
+/*
+ * called by:
+ *   - block/blk-lib.c|335| <<__blkdev_issue_zeroout>> ret = __blkdev_issue_write_zeroes(bdev, sector, nr_sects, gfp_mask,
+ *   - block/blk-lib.c|375| <<blkdev_issue_zeroout>> ret = __blkdev_issue_write_zeroes(bdev, sector, nr_sects,
+ */
 static int __blkdev_issue_write_zeroes(struct block_device *bdev,
 		sector_t sector, sector_t nr_sects, gfp_t gfp_mask,
 		struct bio **biop, unsigned flags)
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 1534ed736363..310e15dbf490 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -140,15 +140,44 @@ static struct bio *blk_bio_write_same_split(struct request_queue *q,
  * requests that are submitted to a block device if the start of a bio is not
  * aligned to a physical block boundary.
  */
+/*
+ * called by:
+ *   - block/blk-merge.c|252| <<blk_bio_segment_split>> const unsigned max_sectors = get_max_io_size(q, bio);
+ */
 static inline unsigned get_max_io_size(struct request_queue *q,
 				       struct bio *bio)
 {
+	/*
+	 * Return maximum size of a request at given offset. Only valid for
+	 * file system requests.
+	 */
 	unsigned sectors = blk_max_size_offset(q, bio->bi_iter.bi_sector);
 	unsigned max_sectors = sectors;
+	/*
+	 * 假设512向右移9位是1, 1个bs
+	 * 假设4096向右移9位是8, 8个bs (二进制1000)
+	 *
+	 * 在virtio测试的结果:
+	 * [  101.603442] orabug: sectors=2560, max_sectors=2560, pbs=1, lbs=1, start_offset=0, q->limits.max_sectors=2560
+	 * [  101.605255] orabug: sectors=2560, max_sectors=2560, pbs=1, lbs=1, start_offset=0, q->limits.max_sectors=2560
+	 * [  101.606388] orabug: sectors=2560, max_sectors=2560, pbs=1, lbs=1, start_offset=0, q->limits.max_sectors=2560
+	 */
 	unsigned pbs = queue_physical_block_size(q) >> SECTOR_SHIFT;
 	unsigned lbs = queue_logical_block_size(q) >> SECTOR_SHIFT;
 	unsigned start_offset = bio->bi_iter.bi_sector & (pbs - 1);
 
+	/*
+	 * Return the maximum number of sectors from the start of a bio that may be
+	 * submitted as a single request to a block device. If enough sectors remain,
+	 * align the end to the physical block size. Otherwise align the end to the
+	 * logical block size. This approach minimizes the number of non-aligned
+	 * requests that are submitted to a block device if the start of a bio is not
+	 * aligned to a physical block boundary.
+	 */
+
+	/*
+	 * max_sectors只在下面两行修改, 修改的时候只用pbs修改
+	 */
 	max_sectors += start_offset;
 	max_sectors &= ~(pbs - 1);
 	if (max_sectors > start_offset)
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 0157f2b3485a..5675db0eb4cf 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -15,9 +15,17 @@
 #include "blk.h"
 #include "blk-mq.h"
 
+/*
+ * 参数的'struct blk_mq_queue_map'是在set中的
+ */
 static int queue_index(struct blk_mq_queue_map *qmap,
 		       unsigned int nr_queues, const int q)
 {
+	/*
+	 * First hardware queue to map onto. Used by the PCIe NVMe
+	 * driver to map each hardware queue type (enum hctx_type) onto a distinct
+	 * set of hardware queues.
+	 */
 	return qmap->queue_offset + (q % nr_queues);
 }
 
@@ -32,6 +40,20 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|42| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|44| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3016| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3309| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|454| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|1852| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2195| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2196| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/nvme/host/tcp.c|2205| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7146| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1778| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
@@ -83,6 +105,12 @@ EXPORT_SYMBOL_GPL(blk_mq_map_queues);
  * We have no quick way of doing reverse lookups. This is only used at
  * queue init time, so runtime isn't important.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2092| <<blk_mq_alloc_rq_map>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2148| <<blk_mq_alloc_rqs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2805| <<blk_mq_realloc_hw_ctxs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], i);
+ */
 int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int index)
 {
 	int i;
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index b3f2ba483992..b169aef2e57b 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -128,6 +128,11 @@ static const char *const blk_queue_flag_name[] = {
 };
 #undef QUEUE_FLAG_NAME
 
+/*
+ * virtblk的例子
+ * # cat /sys/kernel/debug/block/vdb/state
+ * SAME_COMP|IO_STAT|DISCARD|INIT_DONE|WC|STATS|REGISTERED
+ */
 static int queue_state_show(void *data, struct seq_file *m)
 {
 	struct request_queue *q = data;
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index b595a94c4d16..c3f4495a2201 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -11,6 +11,32 @@
 
 #include "blk-mq.h"
 
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */
+
 /**
  * blk_mq_pci_map_queues - provide a default queue mapping for PCI device
  * @qmap:	CPU to hardware queue map.
@@ -23,6 +49,12 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|452| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7148| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5830| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[HCTX_TYPE_DEFAULT],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
diff --git a/block/blk-mq-rdma.c b/block/blk-mq-rdma.c
index 14f968e58b8f..9292c590e68e 100644
--- a/block/blk-mq-rdma.c
+++ b/block/blk-mq-rdma.c
@@ -21,6 +21,11 @@
  * @set->nr_hw_queues, or @dev does not provide an affinity mask for a
  * vector, we fallback to the naive mapping.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1840| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/nvme/host/rdma.c|1842| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_READ],
+ */
 int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		struct ib_device *dev, int first_vec)
 {
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index ca22afd47b3d..c3e897fc45a4 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -85,6 +85,11 @@ void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|215| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *   - block/blk-mq-sched.c|220| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ */
 static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -167,6 +172,18 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1425| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ *
+ * blk_mq_delay_run_hw_queue() or blk_mq_run_hw_queue()
+ *  -> __blk_mq_delay_run_hw_queue()
+ *      -> __blk_mq_run_hw_queue()
+ *
+ * __blk_mq_delay_run_hw_queue()
+ *  -> 调度hctx->run_work = blk_mq_run_work_fn()
+ *      -> __blk_mq_run_hw_queue()
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -357,6 +374,10 @@ void blk_mq_sched_request_inserted(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
 
+/*
+ * called by only:
+ *   - block/blk-mq-sched.c|411| <<blk_mq_sched_insert_request>> if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
+ */
 static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 				       bool has_sched,
 				       struct request *rq)
@@ -375,6 +396,14 @@ static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-exec.c|64| <<blk_execute_rq_nowait>> blk_mq_sched_insert_request(rq, at_head, true, false);
+ *   - block/blk-mq.c|812| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, true, false, false);
+ *   - block/blk-mq.c|818| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, false, false, false);
+ *   - block/blk-mq.c|2177| <<blk_mq_make_request>> blk_mq_sched_insert_request(rq, false, true, true);
+ *   - block/blk-mq.c|2205| <<blk_mq_make_request>> blk_mq_sched_insert_request(rq, false, true, true);
+ */
 void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 				 bool run_queue, bool async)
 {
@@ -410,6 +439,11 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		blk_mq_run_hw_queue(hctx, async);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1995| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx,
+ *   - block/blk-mq.c|2016| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx, &rq_list,
+ */
 void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 				  struct blk_mq_ctx *ctx,
 				  struct list_head *list, bool run_queue_async)
@@ -433,6 +467,15 @@ void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 		 * busy in case of 'none' scheduler, and this way may save
 		 * us one extra enqueue & dequeue to sw queue.
 		 */
+		/*
+		 * 在以下使用dispatch_busy:
+		 *   - block/blk-mq-debugfs.c|621| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+		 *   - block/blk-mq-sched.c|217| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+		 *   - block/blk-mq-sched.c|436| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+		 *   - block/blk-mq.c|1215| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+		 *   - block/blk-mq.c|1225| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+		 *   - block/blk-mq.c|2071| <<blk_mq_make_request>> !data.hctx->dispatch_busy) {
+		 */
 		if (!hctx->dispatch_busy && !e && !run_queue_async) {
 			blk_mq_try_issue_list_directly(hctx, list);
 			if (list_empty(list))
diff --git a/block/blk-mq-sysfs.c b/block/blk-mq-sysfs.c
index 062229395a50..7d3eea033d4f 100644
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@ -253,6 +253,9 @@ static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 	if (ret)
 		return ret;
 
+	/*
+	 * 对于hctx->nr_ctx范围内的每一个hctx->ctxs[i]
+	 */
 	hctx_for_each_ctx(hctx, ctx, i) {
 		ret = kobject_add(&ctx->kobj, &hctx->kobj, "cpu%u", ctx->cpu);
 		if (ret)
@@ -269,6 +272,7 @@ void blk_mq_unregister_dev(struct device *dev, struct request_queue *q)
 
 	lockdep_assert_held(&q->sysfs_dir_lock);
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_unregister_hctx(hctx);
 
@@ -296,6 +300,10 @@ void blk_mq_sysfs_deinit(struct request_queue *q)
 	kobject_put(q->mq_kobj);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2871| <<blk_mq_init_allocated_queue>> blk_mq_sysfs_init(q);
+ */
 void blk_mq_sysfs_init(struct request_queue *q)
 {
 	struct blk_mq_ctx *ctx;
@@ -311,6 +319,10 @@ void blk_mq_sysfs_init(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|983| <<blk_register_queue>> __blk_mq_register_dev(dev, q);
+ */
 int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -325,6 +337,7 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 
 	kobject_uevent(q->mq_kobj, KOBJ_ADD);
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		ret = blk_mq_register_hctx(hctx);
 		if (ret)
@@ -355,6 +368,7 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	if (!q->mq_sysfs_init_done)
 		goto unlock;
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_unregister_hctx(hctx);
 
@@ -362,6 +376,10 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	mutex_unlock(&q->sysfs_dir_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3317| <<__blk_mq_update_nr_hw_queues>> blk_mq_sysfs_register(q);
+ */
 int blk_mq_sysfs_register(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -371,6 +389,7 @@ int blk_mq_sysfs_register(struct request_queue *q)
 	if (!q->mq_sysfs_init_done)
 		goto unlock;
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		ret = blk_mq_register_hctx(hctx);
 		if (ret)
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index fbacde454718..a21165e5e42a 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -21,8 +21,28 @@
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|60| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 使用BLK_MQ_S_TAG_ACTIVE的地方:
+	 *   - block/blk-mq-tag.c|26| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|27| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|51| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|70| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *
+	 * 使用active_queues的地方:
+	 *   - block/blk-mq-tag.c|28| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|54| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|79| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *
+	 * For shared tag users, we track the number of currently active users
+	 * and attempt to provide a fair share of the tag depth for each of them.
+	 */
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
 	    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		atomic_inc(&hctx->tags->active_queues);
@@ -33,6 +53,11 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|56| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|273| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -60,6 +85,10 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|94| <<__blk_mq_get_tag>> !hctx_may_queue(data->hctx, bt))
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
@@ -452,6 +481,10 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2096| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 15bc74acb57e..ad5f9f38367c 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -9,14 +9,37 @@
  */
 struct blk_mq_tags {
 	unsigned int nr_tags;
+	/*
+	 * 设置nr_reserved_tags的地方:
+	 *   - block/blk-mq-tag.c|471| <<blk_mq_init_tags>> tags->nr_reserved_tags = reserved_tags;
+	 */
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 使用active_queues的地方:
+	 *   - block/blk-mq-tag.c|28| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|54| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|79| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
 	struct sbitmap_queue breserved_tags;
 
 	struct request **rqs;
+	/*
+	 * static_rqs一共是二维:
+	 *   - block/blk-mq.c|289| <<blk_mq_rq_ctx_init>> struct request *rq = tags->static_rqs[tag];
+	 *   - block/blk-mq.c|2053| <<blk_mq_free_rqs>> struct request *rq = tags->static_rqs[i];
+	 *   - block/blk-mq.c|2058| <<blk_mq_free_rqs>> tags->static_rqs[i] = NULL;
+	 *   - block/blk-mq.c|2078| <<blk_mq_free_rq_map>> kfree(tags->static_rqs);
+	 *   - block/blk-mq.c|2079| <<blk_mq_free_rq_map>> tags->static_rqs = NULL;
+	 *   - block/blk-mq.c|2109| <<blk_mq_alloc_rq_map>> tags->static_rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *   - block/blk-mq.c|2112| <<blk_mq_alloc_rq_map>> if (!tags->static_rqs) {
+	 *   - block/blk-mq.c|2201| <<blk_mq_alloc_rqs>> tags->static_rqs[i] = rq;
+	 *   - block/blk-mq.c|2203| <<blk_mq_alloc_rqs>> tags->static_rqs[i] = NULL;
+	 */
 	struct request **static_rqs;
 	struct list_head page_list;
 };
@@ -35,11 +58,21 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|130| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|177| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1133| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
 	if (!hctx)
 		return &bt->ws[0];
+	/*
+	 * 这里似乎是唯一使用的地方:
+	 * Index of next available dispatch_wait queue to insert requests.
+	 */
 	return sbq_wait_ptr(bt, &hctx->wait_index);
 }
 
@@ -52,6 +85,11 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * calld by:
+ *   - block/blk-mq.c|387| <<blk_mq_get_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|1067| <<blk_mq_get_driver_tag>> shared = blk_mq_tag_busy(data.hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -60,6 +98,11 @@ static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|963| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2264| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -74,12 +117,22 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * in flight at the same time. The caller has to make sure the tag
  * can't be freed.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|243| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+ *   - block/blk-flush.c|326| <<blk_kick_flush>> blk_mq_tag_set_rq(flush_rq->mq_hctx, first_rq->tag, flush_rq);
+ */
 static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 		unsigned int tag, struct request *rq)
 {
 	hctx->tags->rqs[tag] = rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|189| <<blk_mq_put_tag>> if (!blk_mq_tag_is_reserved(tags, tag)) {
+ *   - block/blk-mq.c|1064| <<blk_mq_get_driver_tag>> if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
+ */
 static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
 					  unsigned int tag)
 {
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 488341628256..16ece920ec49 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -9,6 +9,32 @@
 #include <linux/module.h>
 #include "blk-mq.h"
 
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */
+
 /**
  * blk_mq_virtio_map_queues - provide a default queue mapping for virtio device
  * @qmap:	CPU to hardware queue map.
@@ -21,6 +47,11 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|697| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 323c9cb28066..4be72f32842b 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -43,13 +43,22 @@
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2833| <<blk_mq_init_allocated_queue>> blk_mq_poll_stats_bkt,
+ *   - block/blk-mq.c|3365| <<blk_mq_poll_nsecs>> bucket = blk_mq_poll_stats_bkt(rq);
+ */
 static int blk_mq_poll_stats_bkt(const struct request *rq)
 {
 	int ddir, sectors, bucket;
 
+	/* read or write */
 	ddir = rq_data_dir(rq);
 	sectors = blk_rq_stats_sectors(rq);
 
+	/*
+	 * ilog2(sectors)相当于获得sectors的order吧
+	 */
 	bucket = ddir + 2 * ilog2(sectors);
 
 	if (bucket < 0)
@@ -64,6 +73,10 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
  * Check if any of the ctx, dispatch list or elevator
  * have pending work in this hardware queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1469| <<blk_mq_run_hw_queue>> blk_mq_hctx_has_pending(hctx);
+ */
 static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
 	return !list_empty_careful(&hctx->dispatch) ||
@@ -141,6 +154,12 @@ void blk_freeze_queue_start(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_freeze_queue_start);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|186| <<blk_freeze_queue>> blk_mq_freeze_queue_wait(q);
+ *   - drivers/nvme/host/core.c|4920| <<nvme_wait_freeze>> blk_mq_freeze_queue_wait(ns->queue);
+ *   - drivers/nvme/host/multipath.c|32| <<nvme_mpath_wait_freeze>> blk_mq_freeze_queue_wait(h->disk->queue);
+ */
 void blk_mq_freeze_queue_wait(struct request_queue *q)
 {
 	wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
@@ -221,6 +240,13 @@ void blk_mq_quiesce_queue(struct request_queue *q)
 	unsigned int i;
 	bool rcu = false;
 
+	/*
+	 * 在以下调用blk_queue_quiesced()检查request_queue是否quiesced:
+	 *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+	 *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+	 *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+	 *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+	 */
 	blk_mq_quiesce_queue_nowait(q);
 
 	queue_for_each_hw_ctx(q, hctx, i) {
@@ -280,6 +306,11 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		rq->tag = -1;
 		rq->internal_tag = tag;
 	} else {
+		/*
+		 * struct blk_mq_alloc_data:
+		 *   -> struct blk_mq_ctx *ctx;
+		 *   -> struct blk_mq_hw_ctx *hctx;
+		 */
 		if (data->hctx->flags & BLK_MQ_F_TAG_SHARED) {
 			rq_flags = RQF_MQ_INFLIGHT;
 			atomic_inc(&data->hctx->nr_active);
@@ -331,6 +362,12 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|439| <<blk_mq_alloc_request>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|488| <<blk_mq_alloc_request_hctx>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|2289| <<blk_mq_make_request>> rq = blk_mq_get_request(q, bio, &data);
+ */
 static struct request *blk_mq_get_request(struct request_queue *q,
 					  struct bio *bio,
 					  struct blk_mq_alloc_data *data)
@@ -397,6 +434,16 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|600| <<blk_get_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/block/mtip32xx/mtip32xx.c|985| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+ *   - drivers/block/sx8.c|511| <<carm_array_info>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/block/sx8.c|564| <<carm_send_special>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/ide/ide-atapi.c|202| <<ide_prep_sense>> sense_rq = blk_mq_alloc_request(drive->queue, REQ_OP_DRV_IN,
+ *   - drivers/nvme/host/core.c|489| <<nvme_alloc_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/scsi/fnic/fnic_scsi.c|2300| <<fnic_scsi_host_start_tag>> dummy = blk_mq_alloc_request(q, REQ_OP_WRITE, BLK_MQ_REQ_NOWAIT);
+ */
 struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 		blk_mq_req_flags_t flags)
 {
@@ -541,6 +588,29 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(__blk_mq_end_request);
 
+/*
+ * 部分调用blk_mq_end_request()的例子:
+ *   - block/blk-flush.c|421| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+ *   - block/blk-flush.c|774| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+ *   - block/blk-mq.c|1366| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+ *   - block/blk-mq.c|1979| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+ *   - block/blk-mq.c|2015| <<blk_mq_try_issue_list_directly>> blk_mq_end_request(rq, ret);
+ *   - block/bsg-lib.c|158| <<bsg_teardown_job>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/block/loop.c|487| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/nbd.c|340| <<nbd_complete_rq>> blk_mq_end_request(req, cmd->status);
+ *   - drivers/block/null_blk_main.c|677| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+ *   - drivers/block/virtio_blk.c|226| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+ *   - drivers/block/xen-blkfront.c|918| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+ *   - drivers/block/xen-blkfront.c|2113| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+ *   - drivers/ide/ide-cd.c|765| <<cdrom_newpc_intr>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/ide/ide-pm.c|50| <<ide_pm_execute_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/ide/ide-pm.c|220| <<ide_complete_pm_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/md/dm-rq.c|174| <<dm_end_request>> blk_mq_end_request(rq, error);
+ *   - drivers/md/dm-rq.c|271| <<dm_softirq_done>> blk_mq_end_request(rq, tio->error);
+ *   - drivers/nvme/host/core.c|307| <<nvme_complete_rq>> blk_mq_end_request(req, status);
+ *   - drivers/nvme/host/multipath.c|76| <<nvme_failover_req>> blk_mq_end_request(req, 0);
+ *   - drivers/scsi/scsi_transport_fc.c|3581| <<fc_bsg_job_timeout>> blk_mq_end_request(req, BLK_STS_IOERR);
+ */
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
@@ -557,6 +627,10 @@ static void __blk_mq_complete_request_remote(void *data)
 	q->mq_ops->complete(rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|652| <<blk_mq_complete_request>> __blk_mq_complete_request(rq);
+ */
 static void __blk_mq_complete_request(struct request *rq)
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
@@ -632,6 +706,25 @@ static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
  *	Ends all I/O on a request. It does not handle partial completions.
  *	The actual completion happens out-of-order, through a IPI handler.
  **/
+/*
+ * 部分调用的例子:
+ *   - block/bsg-lib.c|186| <<bsg_job_done>> blk_mq_complete_request(blk_mq_rq_from_pdu(job));
+ *   - drivers/block/loop.c|499| <<lo_rw_aio_do_completion>> blk_mq_complete_request(rq);
+ *   - drivers/block/loop.c|1957| <<loop_handle_cmd>> blk_mq_complete_request(rq);
+ *   - drivers/block/nbd.c|452| <<nbd_xmit_timeout>> blk_mq_complete_request(req);
+ *   - drivers/block/nbd.c|788| <<recv_work>> blk_mq_complete_request(blk_mq_rq_from_pdu(cmd));
+ *   - drivers/block/nbd.c|804| <<nbd_clear_req>> blk_mq_complete_request(req);
+ *   - drivers/block/null_blk_main.c|1242| <<nullb_complete_cmd>> blk_mq_complete_request(cmd->rq);
+ *   - drivers/block/null_blk_main.c|1369| <<null_timeout_rq>> blk_mq_complete_request(rq);
+ *   - drivers/block/virtio_blk.c|244| <<virtblk_done>> blk_mq_complete_request(req);
+ *   - drivers/block/xen-blkfront.c|1648| <<blkif_interrupt>> blk_mq_complete_request(req);
+ *   - drivers/md/dm-rq.c|291| <<dm_complete_request>> blk_mq_complete_request(rq);
+ *   - drivers/nvme/host/core.c|321| <<nvme_cancel_request>> blk_mq_complete_request(req);
+ *   - drivers/nvme/host/nvme.h|460| <<nvme_end_request>> blk_mq_complete_request(req);
+ *   - drivers/s390/block/dasd.c|2786| <<__dasd_cleanup_cqr>> blk_mq_complete_request(req);
+ *   - drivers/s390/block/scm_blk.c|259| <<scm_request_finish>> blk_mq_complete_request(scmrq->request[i]);
+ *   - drivers/scsi/scsi_lib.c|1618| <<scsi_mq_done>> if (unlikely(!blk_mq_complete_request(cmd->request)))
+ */
 bool blk_mq_complete_request(struct request *rq)
 {
 	if (unlikely(blk_should_fake_timeout(rq->q)))
@@ -641,13 +734,51 @@ bool blk_mq_complete_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_complete_request);
 
+/*
+ * 部分调用blk_mq_start_request()的例子:
+ *   - block/bsg-lib.c|272| <<bsg_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/block/loop.c|1985| <<loop_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/block/nbd.c|882| <<nbd_handle_cmd>> blk_mq_start_request(req);
+ *   - drivers/block/nbd.c|891| <<nbd_handle_cmd>> blk_mq_start_request(req);
+ *   - drivers/block/nbd.c|915| <<nbd_handle_cmd>> blk_mq_start_request(req);
+ *   - drivers/block/nbd.c|926| <<nbd_handle_cmd>> blk_mq_start_request(req);
+ *   - drivers/block/null_blk_main.c|2286| <<null_queue_rq>> blk_mq_start_request(bd->rq);
+ *   - drivers/block/virtio_blk.c|319| <<virtio_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/block/xen-blkfront.c|891| <<blkif_queue_rq>> blk_mq_start_request(qd->rq);
+ *   - drivers/ide/ide-io.c|578| <<ide_queue_rq>> blk_mq_start_request(bd->rq);
+ *   - drivers/md/dm-rq.c|450| <<dm_start_request>> blk_mq_start_request(orig);
+ *   - drivers/nvme/host/fabrics.c|556| <<nvmf_fail_nonready_command>> blk_mq_start_request(rq);
+ *   - drivers/nvme/host/fc.c|2283| <<nvme_fc_start_fcp_op>> blk_mq_start_request(op->rq);
+ *   - drivers/nvme/host/pci.c|1050| <<nvme_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/nvme/host/rdma.c|1763| <<nvme_rdma_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/nvme/host/tcp.c|2165| <<nvme_tcp_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/nvme/target/loop.c|148| <<nvme_loop_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1601| <<scsi_mq_prep_fn>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1677| <<scsi_queue_rq>> blk_mq_start_request(req);
+ */
 void blk_mq_start_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
 
 	trace_block_rq_issue(q, rq);
 
+	/*
+	 * 在以下使用QUEUE_FLAG_STATS:
+	 *   - block/blk-mq.c|650| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+	 *   - block/blk-stat.c|152| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|162| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|188| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 */
 	if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+		/*
+		 * 在以下使用io_start_time_ns:
+		 *   - block/bfq-iosched.c|5912| <<bfq_finish_requeue_request>> rq->io_start_time_ns,
+		 *   - block/blk-mq.c|671| <<blk_mq_start_request>> rq->io_start_time_ns = ktime_get_ns();
+		 *   - block/blk-mq.c|327| <<blk_mq_rq_ctx_init>> rq->io_start_time_ns = 0;
+		 *   - block/blk-stat.c|59| <<blk_stat_add>> value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
+		 *   - block/blk-wbt.c|619| <<wbt_issue>> rwb->sync_issue = rq->io_start_time_ns;
+		 *   - block/kyber-iosched.c|651| <<kyber_completed_request>> now - rq->io_start_time_ns);
+		 */
 		rq->io_start_time_ns = ktime_get_ns();
 		rq->stats_sectors = blk_rq_sectors(rq);
 		rq->rq_flags |= RQF_STATS;
@@ -675,6 +806,12 @@ void blk_mq_start_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_start_request);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|810| <<blk_mq_requeue_request>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|1483| <<blk_mq_dispatch_rq_list>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|2087| <<__blk_mq_issue_directly>> __blk_mq_requeue_request(rq);
+ */
 static void __blk_mq_requeue_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -692,6 +829,29 @@ static void __blk_mq_requeue_request(struct request *rq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|484| <<lo_complete_rq>> blk_mq_requeue_request(rq, true);
+ *   - drivers/block/nbd.c|170| <<nbd_requeue_cmd>> blk_mq_requeue_request(req, true);
+ *   - drivers/block/null_blk_main.c|2300| <<null_queue_rq>> blk_mq_requeue_request(bd->rq, true);
+ *   - drivers/block/skd_main.c|1425| <<skd_resolve_req_exception>> blk_mq_requeue_request(req, true);
+ *   - drivers/block/skd_main.c|1435| <<skd_resolve_req_exception>> blk_mq_requeue_request(req, true);
+ *   - drivers/block/sunvdc.c|1120| <<vdc_requeue_inflight>> blk_mq_requeue_request(req, false);
+ *   - drivers/block/xen-blkfront.c|2053| <<blkif_recover>> blk_mq_requeue_request(req, false);
+ *   - drivers/ide/ide-cd.c|261| <<ide_cd_breathe>> blk_mq_requeue_request(rq, false);
+ *   - drivers/ide/ide-io.c|450| <<ide_requeue_and_plug>> blk_mq_requeue_request(rq, false);
+ *   - drivers/md/dm-rq.c|191| <<dm_mq_delay_requeue_request>> blk_mq_requeue_request(rq, false);
+ *   - drivers/memstick/core/ms_block.c|2056| <<msb_stop>> blk_mq_requeue_request(msb->req, false);
+ *   - drivers/mmc/core/block.c|1435| <<mmc_blk_cqe_complete_rq>> blk_mq_requeue_request(req, true);
+ *   - drivers/mmc/core/block.c|1440| <<mmc_blk_cqe_complete_rq>> blk_mq_requeue_request(req, true);
+ *   - drivers/mmc/core/block.c|1894| <<mmc_blk_mq_complete_rq>> blk_mq_requeue_request(req, true);
+ *   - drivers/mmc/core/block.c|1900| <<mmc_blk_mq_complete_rq>> blk_mq_requeue_request(req, true);
+ *   - drivers/nvme/host/core.c|338| <<nvme_retry_req>> blk_mq_requeue_request(req, false);
+ *   - drivers/s390/block/dasd.c|2964| <<_dasd_requeue_request>> blk_mq_requeue_request(req, false);
+ *   - drivers/s390/block/scm_blk.c|243| <<scm_request_requeue>> blk_mq_requeue_request(scmrq->request[i], false);
+ *   - drivers/scsi/scsi_lib.c|163| <<scsi_mq_requeue_cmd>> blk_mq_requeue_request(cmd->request, true);
+ *   - drivers/scsi/scsi_lib.c|202| <<__scsi_queue_insert>> blk_mq_requeue_request(cmd->request, true);
+ */
 void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list)
 {
 	__blk_mq_requeue_request(rq);
@@ -704,6 +864,17 @@ void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list)
 }
 EXPORT_SYMBOL(blk_mq_requeue_request);
 
+/*
+ * 在以下使用q->requeue_work:
+ *   - block/blk-mq.c|800| <<blk_mq_requeue_work>> container_of(work, struct request_queue, requeue_work.work);
+ *   - block/blk-mq.c|861| <<blk_mq_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
+ *   - block/blk-mq.c|868| <<blk_mq_delay_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work,
+ *   - block/blk-mq.c|3231| <<blk_mq_init_allocated_queue>> INIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work);
+ *   - block/blk-sysfs.c|906| <<__blk_release_queue>> cancel_delayed_work_sync(&q->requeue_work);
+ *
+ * 在以下使用blk_mq_requeue_work():
+ *   - block/blk-mq.c|3231| <<blk_mq_init_allocated_queue>> INIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work);
+ */
 static void blk_mq_requeue_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -720,6 +891,7 @@ static void blk_mq_requeue_work(struct work_struct *work)
 			continue;
 
 		rq->rq_flags &= ~RQF_SOFTBARRIER;
+		/* 从rq_list取下来 */
 		list_del_init(&rq->queuelist);
 		/*
 		 * If RQF_DONTPREP, rq has contained some driver specific
@@ -741,6 +913,20 @@ static void blk_mq_requeue_work(struct work_struct *work)
 	blk_mq_run_hw_queues(q, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|262| <<blk_flush_queue_rq>> blk_mq_add_to_requeue_list(rq, add_front, true);
+ *   - block/blk-mq.c|793| <<blk_mq_requeue_request>> blk_mq_add_to_requeue_list(rq, true, kick_requeue_list);
+ *
+ * 两个例子:
+ * blk_kick_flush()
+ *  -> blk_flush_queue_rq()
+ *      -> blk_mq_add_to_requeue_list()
+ *
+ * blk_flush_complete_seq()
+ *  -> blk_flush_queue_rq()
+ *      -> blk_mq_add_to_requeue_list()
+ */
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 				bool kick_requeue_list)
 {
@@ -766,8 +952,19 @@ void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 		blk_mq_kick_requeue_list(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|168| <<queue_state_write>> blk_mq_kick_requeue_list(q);
+ *   - block/blk-mq.c|895| <<blk_mq_add_to_requeue_list>> blk_mq_kick_requeue_list(q);
+ *   - drivers/block/xen-blkfront.c|2056| <<blkif_recover>> blk_mq_kick_requeue_list(info->rq);
+ *   - drivers/md/dm-rq.c|68| <<dm_start_queue>> blk_mq_kick_requeue_list(q);
+ *   - drivers/s390/block/scm_blk.c|247| <<scm_request_requeue>> blk_mq_kick_requeue_list(bdev->rq);
+ */
 void blk_mq_kick_requeue_list(struct request_queue *q)
 {
+	/*
+	 * blk_mq_requeue_work()
+	 */
 	kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
 }
 EXPORT_SYMBOL(blk_mq_kick_requeue_list);
@@ -780,6 +977,25 @@ void blk_mq_delay_kick_requeue_list(struct request_queue *q,
 }
 EXPORT_SYMBOL(blk_mq_delay_kick_requeue_list);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3917| <<blk_mq_poll_hybrid>> rq = blk_mq_tag_to_rq(hctx->tags, blk_qc_t_to_tag(cookie));
+ *   - block/blk-mq.c|3919| <<blk_mq_poll_hybrid>> rq = blk_mq_tag_to_rq(hctx->sched_tags, blk_qc_t_to_tag(cookie));
+ *   - drivers/block/mtip32xx/mtip32xx.c|167| <<mtip_cmd_from_tag>> return blk_mq_rq_to_pdu(blk_mq_tag_to_rq(hctx->tags, tag));
+ *   - drivers/block/nbd.c|696| <<nbd_read_stat>> req = blk_mq_tag_to_rq(nbd->tag_set.tags[hwq],
+ *   - drivers/block/skd_main.c|1520| <<skd_isr_completion_posted>> WARN_ON_ONCE(blk_mq_tag_to_rq(skdev->tag_set.tags[hwq],
+ *   - drivers/block/skd_main.c|1526| <<skd_isr_completion_posted>> rq = blk_mq_tag_to_rq(skdev->tag_set.tags[hwq], tag);
+ *   - drivers/block/sx8.c|926| <<carm_handle_resp>> rq = blk_mq_tag_to_rq(host->tag_set.tags[0], msg_idx);
+ *   - drivers/nvme/host/pci.c|975| <<nvme_handle_cqe>> req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
+ *   - drivers/nvme/host/rdma.c|1443| <<nvme_rdma_process_nvme_rsp>> rq = blk_mq_tag_to_rq(nvme_rdma_tagset(queue), cqe->command_id);
+ *   - drivers/nvme/host/tcp.c|433| <<nvme_tcp_process_nvme_cqe>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), cqe->command_id);
+ *   - drivers/nvme/host/tcp.c|453| <<nvme_tcp_handle_c2h_data>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|557| <<nvme_tcp_handle_r2t>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|642| <<nvme_tcp_recv_data>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|741| <<nvme_tcp_recv_ddgst>> struct request *rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue),
+ *   - drivers/nvme/target/loop.c|111| <<nvme_loop_queue_response>> rq = blk_mq_tag_to_rq(nvme_loop_tagset(queue), cqe->command_id);
+ *   - include/scsi/scsi_tcq.h|33| <<scsi_host_find_tag>> req = blk_mq_tag_to_rq(shost->tag_set.tags[hwq],
+ */
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)
 {
 	if (tag < tags->nr_tags) {
@@ -817,6 +1033,10 @@ bool blk_mq_queue_inflight(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_queue_inflight);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1095| <<blk_mq_check_expired>> blk_mq_rq_timed_out(rq, reserved);
+ */
 static void blk_mq_rq_timed_out(struct request *req, bool reserved)
 {
 	req->rq_flags |= RQF_TIMED_OUT;
@@ -852,6 +1072,10 @@ static bool blk_mq_req_expired(struct request *rq, unsigned long *next)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1129| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, void *priv, bool reserved)
 {
@@ -893,6 +1117,10 @@ static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * 在以下使用blk_mq_timeout_work():
+ *   - block/blk-mq.c|3339| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+ */
 static void blk_mq_timeout_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -1148,6 +1376,39 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
  * - take 4 as factor for avoiding to get too small(0) result, and this
  *   factor doesn't matter because EWMA decreases exponentially
  */
+/*
+ * commit 6e768717304bdbe8d2897ca8298f6b58863fdc41
+ * Author: Ming Lei <ming.lei@redhat.com>
+ * Date:   Tue Jul 3 09:03:16 2018 -0600
+ *
+ * blk-mq: dequeue request one by one from sw queue if hctx is busy
+ *
+ * It won't be efficient to dequeue request one by one from sw queue,
+ * but we have to do that when queue is busy for better merge performance.
+ *
+ * This patch takes the Exponential Weighted Moving Average(EWMA) to figure
+ * out if queue is busy, then only dequeue request one by one from sw queue
+ * when queue is busy.
+ *
+ * Fixes: b347689ffbca ("blk-mq-sched: improve dispatching from sw queue")
+ * Cc: Kashyap Desai <kashyap.desai@broadcom.com>
+ * Cc: Laurence Oberman <loberman@redhat.com>
+ * Cc: Omar Sandoval <osandov@fb.com>
+ * Cc: Christoph Hellwig <hch@lst.de>
+ * Cc: Bart Van Assche <bart.vanassche@wdc.com>
+ * Cc: Hannes Reinecke <hare@suse.de>
+ * Reported-by: Kashyap Desai <kashyap.desai@broadcom.com>
+ * Tested-by: Kashyap Desai <kashyap.desai@broadcom.com>
+ * Signed-off-by: Ming Lei <ming.lei@redhat.com>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ *
+ * called by:
+ *   - block/blk-mq.c|1372| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|1375| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|1838| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|1843| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|1847| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ */
 static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 {
 	unsigned int ewma;
@@ -1155,14 +1416,34 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 	if (hctx->queue->elevator)
 		return;
 
+	/*
+	 * 在以下使用dispatch_busy:
+	 *   - block/blk-mq-debugfs.c|621| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+	 *   - block/blk-mq-sched.c|217| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+	 *   - block/blk-mq-sched.c|436| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+	 *   - block/blk-mq.c|1215| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+	 *   - block/blk-mq.c|1225| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 *   - block/blk-mq.c|2071| <<blk_mq_make_request>> !data.hctx->dispatch_busy) {
+	 */
 	ewma = hctx->dispatch_busy;
 
 	if (!ewma && !busy)
 		return;
 
+	/*
+	 * BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT - 1 = 8 - 1 = 7
+	 * 这里相当于ewma = hctx->dispatch_busy * 7
+	 */
 	ewma *= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT - 1;
+	/*
+	 * 当busy的时候ewma = ewma + 1 << 4
+	 * 相当于ewma加上16
+	 */
 	if (busy)
 		ewma += 1 << BLK_MQ_DISPATCH_BUSY_EWMA_FACTOR;
+	/*
+	 * 相当于ewma = ewma / 8
+	 */
 	ewma /= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT;
 
 	hctx->dispatch_busy = ewma;
@@ -1173,6 +1454,16 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 /*
  * Returns true if we did some work AND can potentially do more.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|120| <<blk_mq_do_dispatch_sched>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|170| <<blk_mq_do_dispatch_ctx>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|218| <<blk_mq_sched_dispatch_requests>> if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+ *   - block/blk-mq-sched.c|231| <<blk_mq_sched_dispatch_requests>> blk_mq_dispatch_rq_list(q, &rq_list, false);
+ *
+ * 核心思想是处理list上每一个request,处理不了就放入hctx->dispatch
+ * 然后根据情况决定是否run
+ */
 bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			     bool got_budget)
 {
@@ -1220,6 +1511,7 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			}
 		}
 
+		/* 到了此时才在list上删除 */
 		list_del_init(&rq->queuelist);
 
 		bd.rq = rq;
@@ -1278,6 +1570,21 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			q->mq_ops->commit_rqs(hctx);
 
 		spin_lock(&hctx->lock);
+		/*
+		 * 在以下添加rq到hctx->dispatch:
+		 *   - block/blk-mq-sched.c|376| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1414| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1794| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2373| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 * 在以下使用hctx->dispatch:
+		 *   - block/blk-mq.c|2491| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+		 *   - block/blk-mq-sched.c|199| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *   - block/blk-mq-sched.c|196| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+		 *   - block/blk-mq-sched.c|198| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+		 *   - block/blk-mq.c|82| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+		 *   - block/blk-mq-debugfs.c|363| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+		 *   - block/blk-mq-debugfs.c|370| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+		 */
 		list_splice_init(list, &hctx->dispatch);
 		spin_unlock(&hctx->lock);
 
@@ -1327,6 +1634,11 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	return (queued + errors) != 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1493| <<__blk_mq_delay_run_hw_queue>> __blk_mq_run_hw_queue(hctx);
+ *   - block/blk-mq.c|1654| <<blk_mq_run_work_fn>> __blk_mq_run_hw_queue(hctx);
+ */
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	int srcu_idx;
@@ -1424,6 +1736,11 @@ static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 	return next_cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1507| <<blk_mq_delay_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, true, msecs);
+ *   - block/blk-mq.c|1530| <<blk_mq_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, async, 0);
+ */
 static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 					unsigned long msecs)
 {
@@ -1441,16 +1758,46 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 		put_cpu();
 	}
 
+	/*
+	 * 在以下使用run_work:
+	 *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 */
 	kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
 				    msecs_to_jiffies(msecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1370| <<blk_mq_dispatch_rq_list>> blk_mq_delay_run_hw_queue(hctx, BLK_MQ_RESOURCE_DELAY);
+ *   - drivers/ide/ide-io.c|453| <<ide_requeue_and_plug>> blk_mq_delay_run_hw_queue(q->queue_hw_ctx[0], 3);
+ *   - drivers/scsi/scsi_lib.c|1639| <<scsi_mq_get_budget>> blk_mq_delay_run_hw_queue(hctx, SCSI_QUEUE_DELAY);
+ */
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 {
 	__blk_mq_delay_run_hw_queue(hctx, true, msecs);
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|80| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|419| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|453| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|168| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1148| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1451| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1650| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1715| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1735| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1821| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2135| <<blk_mq_make_request>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2399| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|698| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ */
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1582,6 +1929,14 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 }
 EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
 
+/*
+ * 在以下使用run_work:
+ *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+ *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+ *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+ *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+ *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+ */
 static void blk_mq_run_work_fn(struct work_struct *work)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1629,6 +1984,14 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|518| <<blk_insert_flush>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|787| <<blk_mq_requeue_work>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|1884| <<__blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, run_queue);
+ *   - block/blk-mq.c|1900| <<blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, true);
+ *   - block/blk-mq.c|1934| <<blk_mq_try_issue_list_directly>> blk_mq_request_bypass_insert(rq,
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
@@ -1680,6 +2043,10 @@ static int plug_rq_cmp(void *priv, struct list_head *a, struct list_head *b)
 	return blk_rq_pos(rqa) > blk_rq_pos(rqb);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1788| <<blk_flush_plug_list>> blk_mq_flush_plug_list(plug, from_schedule);
+ */
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 {
 	struct blk_mq_hw_ctx *this_hctx;
@@ -1748,6 +2115,10 @@ static void blk_mq_bio_to_request(struct request *rq, struct bio *bio,
 	blk_account_io_start(rq, true);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2135| <<__blk_mq_try_issue_directly>> return __blk_mq_issue_directly(hctx, rq, cookie, last);
+ */
 static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    blk_qc_t *cookie, bool last)
@@ -1787,6 +2158,11 @@ static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2037| <<blk_mq_try_issue_directly>> ret = __blk_mq_try_issue_directly(hctx, rq, cookie, false, true);
+ *   - block/blk-mq.c|2054| <<blk_mq_request_issue_directly>> ret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true, last);
+ */
 static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 						struct request *rq,
 						blk_qc_t *cookie,
@@ -1847,6 +2223,11 @@ static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	hctx_unlock(hctx, srcu_idx);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1281| <<blk_insert_cloned_request>> return blk_mq_request_issue_directly(rq, true);
+ *   - block/blk-mq.c|2161| <<blk_mq_try_issue_list_directly>> ret = blk_mq_request_issue_directly(rq, list_empty(list));
+ */
 blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 {
 	blk_status_t ret;
@@ -1861,6 +2242,10 @@ blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|466| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -2275,6 +2660,10 @@ static int blk_mq_hw_ctx_size(struct blk_mq_tag_set *tag_set)
 	return hw_ctx_size;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2765| <<blk_mq_alloc_and_init_hctx>> if (blk_mq_init_hctx(q, set, hctx, hctx_idx))
+ */
 static int blk_mq_init_hctx(struct request_queue *q,
 		struct blk_mq_tag_set *set,
 		struct blk_mq_hw_ctx *hctx, unsigned hctx_idx)
@@ -2302,6 +2691,10 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	return -1;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2761| <<blk_mq_alloc_and_init_hctx>> hctx = blk_mq_alloc_hctx(q, set, node);
+ */
 static struct blk_mq_hw_ctx *
 blk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,
 		int node)
@@ -2321,6 +2714,14 @@ blk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,
 		node = set->numa_node;
 	hctx->numa_node = node;
 
+	/*
+	 * 在以下使用run_work:
+	 *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 */
 	INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
 	spin_lock_init(&hctx->lock);
 	INIT_LIST_HEAD(&hctx->dispatch);
@@ -2429,6 +2830,35 @@ static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ *
+ * called by:
+ *   - block/blk-mq.c|2910| <<blk_mq_init_allocated_queue>> blk_mq_map_swqueue(q);
+ *   - block/blk-mq.c|3312| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_swqueue(q);
+ */
 static void blk_mq_map_swqueue(struct request_queue *q)
 {
 	unsigned int i, j, hctx_idx;
@@ -2436,6 +2866,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	struct blk_mq_ctx *ctx;
 	struct blk_mq_tag_set *set = q->tag_set;
 
+	/*
+	 * 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i]
+	 */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		cpumask_clear(hctx->cpumask);
 		hctx->nr_ctx = 0;
@@ -2448,6 +2881,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 * If the cpu isn't present, the cpu is mapped to first hctx.
 	 */
 	for_each_possible_cpu(i) {
+		/*
+		 * hctx_idx用来索引set->tags[hctx_idx]
+		 */
 		hctx_idx = set->map[HCTX_TYPE_DEFAULT].mq_map[i];
 		/* unmapped hw queue can be remapped after CPU topo changed */
 		if (!set->tags[hctx_idx] &&
@@ -2470,6 +2906,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			}
 
 			hctx = blk_mq_map_queue_type(q, j, i);
+			/*
+			 *  struct blk_mq_hw_ctx *hctxs[HCTX_MAX_TYPES];
+			 */
 			ctx->hctxs[j] = hctx;
 			/*
 			 * If the CPU is already set in the mask, then we've
@@ -2535,6 +2974,11 @@ static void blk_mq_map_swqueue(struct request_queue *q)
  * Caller needs to ensure that we're either frozen/quiesced, or that
  * the queue isn't live yet.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2589| <<blk_mq_update_tag_set_depth>> queue_set_hctx_shared(q, shared);
+ *   - block/blk-mq.c|2625| <<blk_mq_add_queue_tag_set>> queue_set_hctx_shared(q, true);
+ */
 static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2548,6 +2992,11 @@ static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2604| <<blk_mq_del_queue_tag_set>> blk_mq_update_tag_set_depth(set, false);
+ *   - block/blk-mq.c|2622| <<blk_mq_add_queue_tag_set>> blk_mq_update_tag_set_depth(set, true);
+ */
 static void blk_mq_update_tag_set_depth(struct blk_mq_tag_set *set,
 					bool shared)
 {
@@ -2600,8 +3049,16 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 }
 
 /* All allocations will be freed in release handler of q->mq_kobj */
+/*
+ * called by:
+ *   - block/blk-mq.c|2867| <<blk_mq_init_allocated_queue>> if (blk_mq_alloc_ctxs(q))
+ */
 static int blk_mq_alloc_ctxs(struct request_queue *q)
 {
+	/*
+	 * struct request_queue:
+	 *  -> struct blk_mq_ctx __percpu *queue_ctx;
+	 */
 	struct blk_mq_ctxs *ctxs;
 	int cpu;
 
@@ -2656,6 +3113,36 @@ void blk_mq_release(struct request_queue *q)
 	blk_mq_sysfs_deinit(q);
 }
 
+/*
+ * 部分调用blk_mq_init_queue()的例子:
+ *   - block/blk-mq.c|2732| <<blk_mq_init_sq_queue>> q = blk_mq_init_queue(set);
+ *   - block/bsg-lib.c|387| <<bsg_setup_queue>> q = blk_mq_init_queue(set);
+ *   - drivers/block/loop.c|2022| <<loop_add>> lo->lo_queue = blk_mq_init_queue(&lo->tag_set);
+ *   - drivers/block/nbd.c|1692| <<nbd_dev_add>> q = blk_mq_init_queue(&nbd->tag_set);
+ *   - drivers/block/null_blk_main.c|1717| <<null_add_dev>> nullb->q = blk_mq_init_queue(nullb->tag_set);
+ *   - drivers/block/rbd.c|5042| <<rbd_init_disk>> q = blk_mq_init_queue(&rbd_dev->tag_set);
+ *   - drivers/block/virtio_blk.c|802| <<virtblk_probe>> q = blk_mq_init_queue(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|986| <<xlvbd_init_blk_queue>> rq = blk_mq_init_queue(&info->tag_set);
+ *   - drivers/ide/ide-probe.c|790| <<ide_init_queue>> q = blk_mq_init_queue(set);
+ *   - drivers/nvme/host/core.c|3495| <<nvme_alloc_ns>> ns->queue = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/fc.c|2481| <<nvme_fc_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3148| <<nvme_fc_init_ctrl>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/fc.c|3154| <<nvme_fc_init_ctrl>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|1628| <<nvme_alloc_admin_tags>> dev->ctrl.admin_q = blk_mq_init_queue(&dev->admin_tagset);
+ *   - drivers/nvme/host/rdma.c|809| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/rdma.c|815| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/rdma.c|886| <<nvme_rdma_configure_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/tcp.c|1676| <<nvme_tcp_configure_io_queues>> ctrl->connect_q = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/tcp.c|1729| <<nvme_tcp_configure_admin_queue>> ctrl->fabrics_q = blk_mq_init_queue(ctrl->admin_tagset);
+ *   - drivers/nvme/host/tcp.c|1735| <<nvme_tcp_configure_admin_queue>> ctrl->admin_q = blk_mq_init_queue(ctrl->admin_tagset);
+ *   - drivers/nvme/target/loop.c|362| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|368| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|529| <<nvme_loop_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1871| <<scsi_mq_alloc_queue>> sdev->request_queue = blk_mq_init_queue(&sdev->host->tag_set);
+ *
+ * blk_mq_init_queue()更加像是从tagset中新分配一个request_queue
+ * 一个tagset可以有多个request_queue
+ */
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)
 {
 	struct request_queue *uninit_q, *q;
@@ -2710,6 +3197,10 @@ struct request_queue *blk_mq_init_sq_queue(struct blk_mq_tag_set *set,
 }
 EXPORT_SYMBOL(blk_mq_init_sq_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2814| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+ */
 static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 		struct blk_mq_tag_set *set, struct request_queue *q,
 		int hctx_idx, int node)
@@ -2744,6 +3235,11 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2876| <<blk_mq_init_allocated_queue>> blk_mq_realloc_hw_ctxs(set, q);
+ *   - block/blk-mq.c|3304| <<__blk_mq_update_nr_hw_queues>> blk_mq_realloc_hw_ctxs(set, q);
+ */
 static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 						struct request_queue *q)
 {
@@ -2762,6 +3258,12 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 			memcpy(new_hctxs, hctxs, q->nr_hw_queues *
 			       sizeof(*hctxs));
 		q->queue_hw_ctx = new_hctxs;
+		/*
+		 * 设置nr_hw_queues的地方:
+		 *   - block/blk-mq.c|3103| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+		 *   - block/blk-mq.c|3147| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+		 *   - block/blk-mq.c|3233| <<blk_mq_init_allocated_queue>> q->nr_hw_queues = 0;
+		 */
 		q->nr_hw_queues = set->nr_hw_queues;
 		kfree(hctxs);
 		hctxs = new_hctxs;
@@ -2822,6 +3324,11 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2700| <<blk_mq_init_queue>> q = blk_mq_init_allocated_queue(set, uninit_q, false);
+ *   - drivers/md/dm-rq.c|566| <<dm_mq_init_request_queue>> q = blk_mq_init_allocated_queue(md->tag_set, md->queue, true);
+ */
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q,
 						  bool elevator_init)
@@ -2959,6 +3466,11 @@ static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3109| <<blk_mq_alloc_tag_set>> ret = blk_mq_update_queue_map(set);
+ *   - block/blk-mq.c|3301| <<__blk_mq_update_nr_hw_queues>> blk_mq_update_queue_map(set);
+ */
 static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 {
 	if (set->ops->map_queues && !is_kdump_kernel()) {
@@ -2978,6 +3490,10 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 		 * killing stale mapping since one CPU may not be mapped
 		 * to any hw queue.
 		 */
+		/*
+		 * 对于每一种类型的map,
+		 * 清空所有cpu的blk_mq_queue_map->mq_map[cpu] = 0
+		 */
 		for (i = 0; i < set->nr_maps; i++)
 			blk_mq_clear_mq_map(&set->map[i]);
 
@@ -3017,6 +3533,27 @@ static int blk_mq_realloc_tag_set_tags(struct blk_mq_tag_set *set,
  * requested depth down, if it's too large. In that case, the set
  * value will be stored in set->queue_depth.
  */
+/*
+ * 调用blk_mq_alloc_tag_set()的例子:
+ *   - block/blk-mq.c|2947| <<blk_mq_init_sq_queue>> ret = blk_mq_alloc_tag_set(set);
+ *   - block/bsg-lib.c|384| <<bsg_setup_queue>> if (blk_mq_alloc_tag_set(set))
+ *   - drivers/block/loop.c|2033| <<loop_add>> err = blk_mq_alloc_tag_set(&lo->tag_set);
+ *   - drivers/block/nbd.c|1688| <<nbd_dev_add>> err = blk_mq_alloc_tag_set(&nbd->tag_set);
+ *   - drivers/block/null_blk_main.c|1716| <<null_init_tag_set>> return blk_mq_alloc_tag_set(set);
+ *   - drivers/block/virtio_blk.c|819| <<virtblk_probe>> err = blk_mq_alloc_tag_set(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|984| <<xlvbd_init_blk_queue>> if (blk_mq_alloc_tag_set(&info->tag_set))
+ *   - drivers/ide/ide-probe.c|787| <<ide_init_queue>> if (blk_mq_alloc_tag_set(set))
+ *   - drivers/md/dm-rq.c|562| <<dm_mq_init_request_queue>> err = blk_mq_alloc_tag_set(md->tag_set);
+ *   - drivers/nvme/host/fc.c|2475| <<nvme_fc_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3143| <<nvme_fc_init_ctrl>> ret = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|1645| <<nvme_alloc_admin_tags>> if (blk_mq_alloc_tag_set(&dev->admin_tagset))
+ *   - drivers/nvme/host/pci.c|2328| <<nvme_dev_add>> ret = blk_mq_alloc_tag_set(&dev->tagset);
+ *   - drivers/nvme/host/rdma.c|755| <<nvme_rdma_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+ *   - drivers/nvme/host/tcp.c|1493| <<nvme_tcp_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+ *   - drivers/nvme/target/loop.c|357| <<nvme_loop_configure_admin_queue>> error = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|525| <<nvme_loop_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1906| <<scsi_mq_setup_tags>> return blk_mq_alloc_tag_set(&shost->tag_set);
+ */
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
 	int i, ret;
@@ -3069,6 +3606,13 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 
 	ret = -ENOMEM;
 	for (i = 0; i < set->nr_maps; i++) {
+		/*
+		 * struct blk_mq_queue_map {
+		 *      unsigned int *mq_map;
+		 *      unsigned int nr_queues;
+		 *      unsigned int queue_offset;
+		 * };
+		 */
 		set->map[i].mq_map = kcalloc_node(nr_cpu_ids,
 						  sizeof(set->map[i].mq_map[0]),
 						  GFP_KERNEL, set->numa_node);
@@ -3233,6 +3777,15 @@ static void blk_mq_elv_switch_back(struct list_head *head,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * 有三处queue数量的地方:
+ *   - blk_mq_tag_set->nr_hw_queues
+ *   - blk_mq_queue_map->nr_queues
+ *   - request_queue->nr_hw_queues
+ *
+ * called by:
+ *   - block/blk-mq.c|3673| <<blk_mq_update_nr_hw_queues>> __blk_mq_update_nr_hw_queues(set, nr_hw_queues);
+ */
 static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 							int nr_hw_queues)
 {
@@ -3297,6 +3850,22 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1249| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/null_blk_main.c|323| <<nullb_apply_submit_queues>> blk_mq_update_nr_hw_queues(set, submit_queues);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2554| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2336| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|892| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1682| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|471| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *
+ * 有三处queue数量的地方:
+ *   - blk_mq_tag_set->nr_hw_queues
+ *   - blk_mq_queue_map->nr_queues
+ *   - request_queue->nr_hw_queues
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
@@ -3306,8 +3875,19 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
 
 /* Enable polling stats and return whether they were already enabled. */
+/*
+ * called by:
+ *   - block/blk-mq.c|3373| <<blk_mq_poll_nsecs>> if (!blk_poll_stats_enable(q))
+ */
 static bool blk_poll_stats_enable(struct request_queue *q)
 {
+	/*
+	 * 在以下使用QUEUE_FLAG_POLL_STATS:
+	 *   - block/blk-mq.c|3331| <<blk_poll_stats_enable>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+	 *   - block/blk-mq.c|3332| <<blk_poll_stats_enable>> blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
+	 *   - block/blk-mq.c|3344| <<blk_mq_poll_stats_start>> if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+	 *   - block/blk-sysfs.c|880| <<__blk_release_queue>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
+	 */
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
 	    blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
 		return true;
@@ -3315,6 +3895,10 @@ static bool blk_poll_stats_enable(struct request_queue *q)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|539| <<__blk_mq_end_request>> blk_mq_poll_stats_start(rq->q);
+ */
 static void blk_mq_poll_stats_start(struct request_queue *q)
 {
 	/*
@@ -3328,6 +3912,12 @@ static void blk_mq_poll_stats_start(struct request_queue *q)
 	blk_stat_activate_msecs(q->poll_cb, 100);
 }
 
+/*
+ * 在以下使用blk_mq_poll_stats_fn():
+ *   - block/blk-mq.c|2852| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *
+ * 核心思想是把BLK_MQ_POLL_STATS_BKTS个bucket设置q->poll_stat[bucket] = cb->stat[bucket]
+ */
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 {
 	struct request_queue *q = cb->data;
@@ -3339,6 +3929,12 @@ static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3416| <<blk_mq_poll_hybrid_sleep>> nsecs = blk_mq_poll_nsecs(q, hctx, rq);
+ *
+ * 这个函数应该就是用来估算一个要sleep的时间
+ */
 static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 				       struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
@@ -3372,6 +3968,10 @@ static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3471| <<blk_mq_poll_hybrid>> return blk_mq_poll_hybrid_sleep(q, hctx, rq);
+ */
 static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 				     struct blk_mq_hw_ctx *hctx,
 				     struct request *rq)
@@ -3381,6 +3981,13 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	unsigned int nsecs;
 	ktime_t kt;
 
+	/*
+	 * 在以下使用RQF_MQ_POLL_SLEPT:
+	 *   - block/blk-mq.c|3444| <<blk_mq_poll_hybrid_sleep>> if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
+	 *   - block/blk-mq.c|3461| <<blk_mq_poll_hybrid_sleep>> rq->rq_flags |= RQF_MQ_POLL_SLEPT;
+	 *
+	 * already slept for hybrid poll
+	 */
 	if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
 		return false;
 
@@ -3404,6 +4011,9 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	 * This will be replaced with the stats tracking code, using
 	 * 'avg_completion_time / 2' as the pre-sleep target.
 	 */
+	/*
+	 * kt是非常核心的时间!!!
+	 */
 	kt = nsecs;
 
 	mode = HRTIMER_MODE_REL;
@@ -3426,6 +4036,33 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	return true;
 }
 
+/*
+ * 在以下patch加入的函数
+ * commit 06426adf072bca62ac31ea396ff2159a34f276c2
+ * Author: Jens Axboe <axboe@fb.com>
+ * Date:   Mon Nov 14 13:01:59 2016 -0700
+ *
+ * blk-mq: implement hybrid poll mode for sync O_DIRECT
+ *
+ * This patch enables a hybrid polling mode. Instead of polling after IO
+ * submission, we can induce an artificial delay, and then poll after that.
+ * For example, if the IO is presumed to complete in 8 usecs from now, we
+ * can sleep for 4 usecs, wake up, and then do our polling. This still puts
+ * a sleep/wakeup cycle in the IO path, but instead of the wakeup happening
+ * after the IO has completed, it'll happen before. With this hybrid
+ * scheme, we can achieve big latency reductions while still using the same
+ * (or less) amount of CPU.
+ *
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ * Tested-By: Stephen Bates <sbates@raithlin.com>
+ * Reviewed-By: Stephen Bates <sbates@raithlin.com>
+ *
+ * called by:
+ *   - block/blk-mq.c|3547| <<blk_poll>> if (blk_mq_poll_hybrid(q, hctx, cookie))
+ *
+ * If the device access time exceeds the IRQ model overhead,
+ * sleeping before the I/O completion will not hurt latency
+ */
 static bool blk_mq_poll_hybrid(struct request_queue *q,
 			       struct blk_mq_hw_ctx *hctx, blk_qc_t cookie)
 {
@@ -3463,6 +4100,87 @@ static bool blk_mq_poll_hybrid(struct request_queue *q,
  *    looping until at least one completion is found, unless the task is
  *    otherwise marked running (or we need to reschedule).
  */
+/*
+ * poll是可以有专门的hctx的. 用cookie来标记用的哪个hctx甚至哪个tag.
+ *
+ * blk_qc_t_to_queue_num()把cookie转换成q->queue_hw_ctx[]的index.
+ *
+ * blk_qc_t_to_tag()把cookie转换成tag.
+ *
+ * ext4_direct_IO_write()
+ *  -> __blockdev_direct_IO()
+ *      -> do_blockdev_direct_IO
+ *          -> dio_bio_submit
+ *              -> submit_bio()
+ *          -> dio_await_completion()
+ *              -> dio_await_one()
+ *                  -> blk_poll()
+ *
+ * cookie一直通过submit_bio()返回, 然后通过blk_poll()去poll()查看IO是否完成.
+ *
+ * slides: I/O Latency Optimization with Polling
+ *
+ *
+ * 根据一位网友的测试:
+ * 从上述测试结果来看,IO-Polling对于sync模式的direct-io的延迟有较好的提升,
+ * sync模式下,无论4K随机读或者随机写IO压力下,延迟平均大约减少5μs,而这5μs
+ * 几乎就是中断模式下,处理中断时,上下文切换的时间差.
+ * 相比随机读,对随机写的延迟降低约20%,这对延迟敏感的IO请求来说是极大的性能提升.
+ *
+ *
+ * 根据blk_mq_map_queue(), 只有REQ_HIPRI的才会被map到poll的queue.
+ *
+ * 102 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
+ * 103                                                      unsigned int flags,
+ * 104                                                      struct blk_mq_ctx *ctx)
+ * 105 {
+ * 106         enum hctx_type type = HCTX_TYPE_DEFAULT;
+ * 107
+ * 108         //
+ * 109         // The caller ensure that if REQ_HIPRI, poll must be enabled.
+ * 110         //
+ * 111         if (flags & REQ_HIPRI)
+ * 112                 type = HCTX_TYPE_POLL;
+ * 113         else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
+ * 114                 type = HCTX_TYPE_READ;115
+ * 116         return ctx->hctxs[type];
+ * 117 }
+ *
+ * 激活poll的方法:
+ *
+ * 激活io_poll:
+ * # echo 1 > /sys/block/nvme0n1/queue/io_poll
+ *
+ * 在fio中使用pvsync2加上hipri:
+ * # fio -name iops -rw=read -bs=4k -runtime=60 -iodepth 32 -filename /dev/nvme0n1 -ioengine pvsync2 -direct=1 -hipri=1
+ *
+ * [0] blk_poll
+ * [0] __blkdev_direct_IO_simple
+ * [0] blkdev_direct_IO
+ * [0] generic_file_read_iter
+ * [0] do_iter_readv_writev
+ * [0] do_iter_read
+ * [0] vfs_readv
+ * [0] do_preadv
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|812| <<nvme_execute_rq_polled>> blk_poll(q, request_to_qc_t(rq->mq_hctx, rq), true);
+ *   - fs/block_dev.c|257| <<__blkdev_direct_IO_simple>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/block_dev.c|295| <<blkdev_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), wait);
+ *   - fs/block_dev.c|451| <<__blkdev_direct_IO>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/direct-io.c|502| <<dio_await_one>> !blk_poll(dio->bio_disk->queue, dio->bio_cookie, true))
+ *   - fs/iomap/direct-io.c|57| <<iomap_dio_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), spin);
+ *   - fs/iomap/direct-io.c|562| <<iomap_dio_rw>> !blk_poll(dio->submit.last_queue,
+ *   - mm/page_io.c|424| <<swap_readpage>> if (!blk_poll(disk->queue, qc, true))
+ *
+ * Polling is tried for any block I/O belonging to a high-priority I/O context (IOCB_HIPRI)
+ *
+ * For applications, set only for preadv2/pwritev2 with RWF_HIPRI flag
+ *
+ * Not related to ioprio_set!
+ */
 int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -3475,6 +4193,10 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 	if (current->plug)
 		blk_flush_plug_list(current->plug, false);
 
+	/*
+	 * blk_qc_t_to_queue_num():
+	 * 把cookie的第31位(最高位)清空,然后向右移动16位,获得queue num
+	 */
 	hctx = q->queue_hw_ctx[blk_qc_t_to_queue_num(cookie)];
 
 	/*
@@ -3495,6 +4217,9 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 
 		hctx->poll_invoked++;
 
+		/*
+		 * nvme pci的例子是nvme_poll()
+		 */
 		ret = q->mq_ops->poll(hctx);
 		if (ret > 0) {
 			hctx->poll_success++;
@@ -3517,6 +4242,11 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 }
 EXPORT_SYMBOL_GPL(blk_poll);
 
+/*
+ * called by:
+ *   - drivers/scsi/bnx2i/bnx2i_hwi.c|1918| <<bnx2i_queue_scsi_cmd_resp>> p = &per_cpu(bnx2i_percpu, blk_mq_rq_cpu(sc->request));
+ *   - drivers/scsi/csiostor/csio_scsi.c|1789| <<csio_queuecommand>> sqset = &hw->sqset[ln->portid][blk_mq_rq_cpu(cmnd->request)];
+ */
 unsigned int blk_mq_rq_cpu(struct request *rq)
 {
 	return rq->mq_ctx->cpu;
diff --git a/block/blk-mq.h b/block/blk-mq.h
index eaaca8fc1c28..917b0aae5766 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -9,19 +9,82 @@ struct blk_mq_tag_set;
 
 struct blk_mq_ctxs {
 	struct kobject kobj;
+	/*
+	 * 主要设置和使用queue_ctx的地方:
+	 *   - block/blk-mq-sysfs.c|22| <<blk_mq_sysfs_release>> free_percpu(ctxs->queue_ctx);
+	 *   - block/blk-mq.c|2493| <<blk_mq_map_swqueue>> ctx = per_cpu_ptr(q->queue_ctx, i);
+	 *   - block/blk-mq.c|2641| <<blk_mq_alloc_ctxs>> ctxs->queue_ctx = alloc_percpu(struct blk_mq_ctx);
+	 *   - block/blk-mq.c|2642| <<blk_mq_alloc_ctxs>> if (!ctxs->queue_ctx)
+	 *   - block/blk-mq.c|2646| <<blk_mq_alloc_ctxs>> struct blk_mq_ctx *ctx = per_cpu_ptr(ctxs->queue_ctx, cpu);
+	 *   - block/blk-mq.c|2651| <<blk_mq_alloc_ctxs>> q->queue_ctx = ctxs->queue_ctx;
+	 */
 	struct blk_mq_ctx __percpu	*queue_ctx;
 };
 
 /**
  * struct blk_mq_ctx - State for a software queue facing the submitting CPUs
  */
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 使用rq_lists[]的地方:
+		 *   - block/blk-mq-debugfs.c|632| <<CTX_RQ_SEQ_OPS>> return seq_list_start(&ctx->rq_lists[type], *pos); \
+		 *   - block/blk-mq-debugfs.c|640| <<CTX_RQ_SEQ_OPS>> return seq_list_next(v, &ctx->rq_lists[type], pos); \
+		 *   - block/blk-mq-sched.c|316| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio, nr_segs)) {
+		 *   - block/blk-mq-sched.c|338| <<__blk_mq_sched_bio_merge>> !list_empty_careful(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|982| <<flush_busy_ctx>> list_splice_tail_init(&ctx->rq_lists[type], flush_data->list);
+		 *   - block/blk-mq.c|1017| <<dispatch_rq_from_ctx>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|1018| <<dispatch_rq_from_ctx>> dispatch_data->rq = list_entry_rq(ctx->rq_lists[type].next);
+		 *   - block/blk-mq.c|1020| <<dispatch_rq_from_ctx>> if (list_empty(&ctx->rq_lists[type]))
+		 *   - block/blk-mq.c|1641| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1643| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1690| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|2235| <<blk_mq_hctx_notify_dead>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|2236| <<blk_mq_hctx_notify_dead>> list_splice_init(&ctx->rq_lists[type], &tmp);
+		 *   - block/blk-mq.c|2416| <<blk_mq_init_cpu_queues>> INIT_LIST_HEAD(&__ctx->rq_lists[k]);
+		 */
 		struct list_head	rq_lists[HCTX_MAX_TYPES];
 	} ____cacheline_aligned_in_smp;
 
 	unsigned int		cpu;
+	/*
+	 * 设置index_hw[]的地方:
+	 *   - block/blk-mq.c|2513| <<blk_mq_map_swqueue>> ctx->index_hw[hctx->type] = hctx->nr_ctx;
+	 * 其余使用index_hw[]的地方:
+	 *   - block/blk-mq-sched.c|121| <<blk_mq_next_ctx>> unsigned short idx = ctx->index_hw[hctx->type];
+	 *   - block/blk-mq.c|93| <<blk_mq_hctx_mark_pending>> const int bit = ctx->index_hw[hctx->type];
+	 *   - block/blk-mq.c|102| <<blk_mq_hctx_clear_pending>> const int bit = ctx->index_hw[hctx->type];
+	 *   - block/blk-mq.c|1031| <<blk_mq_dequeue_from_ctx>> unsigned off = start ? start->index_hw[hctx->type] : 0;
+	 *   - block/kyber-iosched.c|570| <<kyber_bio_merge>> struct kyber_ctx_queue *kcq = &khd->kcqs[ctx->index_hw[hctx->type]];
+	 *   - block/kyber-iosched.c|595| <<kyber_insert_requests>> struct kyber_ctx_queue *kcq = &khd->kcqs[rq->mq_ctx->index_hw[hctx->type]];
+	 *   - block/kyber-iosched.c|604| <<kyber_insert_requests>> rq->mq_ctx->index_hw[hctx->type]);
+	 */
 	unsigned short		index_hw[HCTX_MAX_TYPES];
 	struct blk_mq_hw_ctx 	*hctxs[HCTX_MAX_TYPES];
 
@@ -30,6 +93,12 @@ struct blk_mq_ctx {
 	unsigned long		rq_merged;
 
 	/* incremented at completion time */
+	/*
+	 * 使用rq_completed[2]的地方:
+	 *   - block/blk-mq-debugfs.c|700| <<ctx_completed_show>> seq_printf(m, "%lu %lu\n", ctx->rq_completed[1], ctx->rq_completed[0]);
+	 *   - block/blk-mq-debugfs.c|709| <<ctx_completed_write>> ctx->rq_completed[0] = ctx->rq_completed[1] = 0;
+	 *   - block/blk-mq.c|516| <<blk_mq_free_request>> ctx->rq_completed[rq_is_sync(rq)]++;
+	 */
 	unsigned long		____cacheline_aligned_in_smp rq_completed[2];
 
 	struct request_queue	*queue;
@@ -86,10 +155,21 @@ extern int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int);
  * @type: the hctx type index
  * @cpu: CPU
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2425| <<blk_mq_init_cpu_queues>> hctx = blk_mq_map_queue_type(q, j, i);
+ *   - block/blk-mq.c|2496| <<blk_mq_map_swqueue>> ctx->hctxs[j] = blk_mq_map_queue_type(q,
+ *   - block/blk-mq.c|2501| <<blk_mq_map_swqueue>> hctx = blk_mq_map_queue_type(q, j, i);
+ *   - block/blk-mq.c|2524| <<blk_mq_map_swqueue>> ctx->hctxs[j] = blk_mq_map_queue_type(q,
+ */
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *q,
 							  enum hctx_type type,
 							  unsigned int cpu)
 {
+	/*
+	 * 设置queue_hw_ctx的地方:
+	 *   - block/blk-mq.c|2793| <<blk_mq_realloc_hw_ctxs>> q->queue_hw_ctx = new_hctxs;
+	 */
 	return q->queue_hw_ctx[q->tag_set->map[type].mq_map[cpu]];
 }
 
@@ -216,6 +296,13 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|45| <<blk_mq_pci_map_queues>> blk_mq_clear_mq_map(qmap);
+ *   - block/blk-mq.c|3011| <<blk_mq_update_queue_map>> blk_mq_clear_mq_map(&set->map[i]);
+ *
+ * 清空所有cpu的blk_mq_queue_map->mq_map[cpu] = 0
+ */
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index c8eda2e7b91e..14a8834157b6 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -180,6 +180,18 @@ EXPORT_SYMBOL(blk_queue_bounce_limit);
  *    per-device basis in /sys/block/<device>/queue/max_sectors_kb.
  *    The soft limit can not exceed max_hw_sectors.
  **/
+/*
+ * 部分调用blk_queue_max_hw_sectors()的例子:
+ *   - drivers/block/loop.c|2112| <<loop_add>> blk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);
+ *   - drivers/block/nbd.c|1709| <<nbd_dev_add>> blk_queue_max_hw_sectors(disk->queue, 65536);
+ *   - drivers/block/virtio_blk.c|852| <<virtblk_probe>> blk_queue_max_hw_sectors(q, -1U);
+ *   - drivers/block/xen-blkfront.c|947| <<blkif_set_queue_limits>> blk_queue_max_hw_sectors(rq, (segments * XEN_PAGE_SIZE) / 512);
+ *   - drivers/nvme/host/core.c|2226| <<nvme_set_queue_limits>> blk_queue_max_hw_sectors(q, ctrl->max_hw_sectors);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1942| <<megasas_set_nvme_device_properties>> blk_queue_max_hw_sectors(sdev->request_queue, (max_io_size / 512));
+ *   - drivers/scsi/scsi_lib.c|1803| <<__scsi_init_queue>> blk_queue_max_hw_sectors(q, shost->max_sectors);
+ *   - drivers/scsi/scsi_scan.c|902| <<scsi_add_lun>> blk_queue_max_hw_sectors(sdev->request_queue, 512);
+ *   - drivers/scsi/scsi_scan.c|908| <<scsi_add_lun>> blk_queue_max_hw_sectors(sdev->request_queue, 1024);
+ */
 void blk_queue_max_hw_sectors(struct request_queue *q, unsigned int max_hw_sectors)
 {
 	struct queue_limits *limits = &q->limits;
@@ -250,6 +262,19 @@ EXPORT_SYMBOL(blk_queue_max_write_same_sectors);
  * @q:  the request queue for the device
  * @max_write_zeroes_sectors: maximum number of sectors to write per command
  **/
+/*
+ * 部分调用blk_queue_max_write_zeroes_sectors()的例子:
+ *   - drivers/block/loop.c|903| <<loop_config_discard>> blk_queue_max_write_zeroes_sectors(q, 0);
+ *   - drivers/block/loop.c|912| <<loop_config_discard>> blk_queue_max_write_zeroes_sectors(q, UINT_MAX >> 9);
+ *   - drivers/block/virtio_blk.c|923| <<virtblk_probe>> blk_queue_max_write_zeroes_sectors(q, v ? v : UINT_MAX);
+ *   - drivers/md/raid0.c|402| <<raid0_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, mddev->chunk_sectors);
+ *   - drivers/md/raid1.c|3114| <<raid1_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, 0);
+ *   - drivers/md/raid10.c|3767| <<raid10_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, 0);
+ *   - drivers/md/raid5.c|7454| <<raid5_run>> blk_queue_max_write_zeroes_sectors(mddev->queue, 0);
+ *   - drivers/nvme/host/core.c|1723| <<nvme_config_discard>> blk_queue_max_write_zeroes_sectors(queue, UINT_MAX);
+ *   - drivers/nvme/host/core.c|1748| <<nvme_config_write_zeroes>> blk_queue_max_write_zeroes_sectors(disk->queue,
+ *   - drivers/scsi/sd.c|1002| <<sd_config_write_same>> blk_queue_max_write_zeroes_sectors(q, sdkp->max_ws_blocks *
+ */
 void blk_queue_max_write_zeroes_sectors(struct request_queue *q,
 		unsigned int max_write_zeroes_sectors)
 {
@@ -328,6 +353,38 @@ EXPORT_SYMBOL(blk_queue_max_segment_size);
  *   storage device can address.  The default of 512 covers most
  *   hardware.
  **/
+/*
+ * commit c72758f33784e5e2a1a4bb9421ef3e6de8f9fcf3
+ * Author: Martin K. Petersen <martin.petersen@oracle.com>
+ * Date:   Fri May 22 17:17:53 2009 -0400
+ *
+ * block: Export I/O topology for block devices and partitions
+ *
+ * To support devices with physical block sizes bigger than 512 bytes we
+ * need to ensure proper alignment.  This patch adds support for exposing
+ * I/O topology characteristics as devices are stacked.
+ *
+ *   logical_block_size is the smallest unit the device can address.
+ *
+ *   physical_block_size indicates the smallest I/O the device can write
+ *   without incurring a read-modify-write penalty.
+ *
+ *   The io_min parameter is the smallest preferred I/O size reported by
+ *   the device.  In many cases this is the same as the physical block
+ *   size.  However, the io_min parameter can be scaled up when stacking
+ *   (RAID5 chunk size > physical block size).
+ *
+ *   The io_opt characteristic indicates the optimal I/O size reported by
+ *   the device.  This is usually the stripe width for arrays.
+ *
+ *   The alignment_offset parameter indicates the number of bytes the start
+ *   of the device/partition is offset from the device's natural alignment.
+ *   Partition tools and MD/DM utilities can use this to pad their offsets
+ *   so filesystems start on proper boundaries.
+ *
+ *   Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
+ *   Signed-off-by: Jens Axboe <jens.axboe@oracle.com>
+ */
 void blk_queue_logical_block_size(struct request_queue *q, unsigned int size)
 {
 	q->limits.logical_block_size = size;
@@ -818,6 +875,43 @@ EXPORT_SYMBOL(blk_set_queue_depth);
  *
  * Tell the block layer about the write cache of @q.
  */
+/*
+ * 部分调用blk_queue_write_cache()的例子:
+ *   - drivers/block/loop.c|1007| <<loop_set_fd>> blk_queue_write_cache(lo->lo_queue, true, false);
+ *   - drivers/block/nbd.c|1136| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, true);
+ *   - drivers/block/nbd.c|1138| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, true, false);
+ *   - drivers/block/nbd.c|1141| <<nbd_parse_flags>> blk_queue_write_cache(nbd->disk->queue, false, false);
+ *   - drivers/block/null_blk_main.c|1742| <<null_add_dev>> blk_queue_write_cache(nullb->q, true, true);
+ *   - rivers/block/virtio_blk.c|609| <<virtblk_update_cache_mode>> blk_queue_write_cache(vblk->disk->queue, writeback, false);
+ *   - drivers/block/xen-blkfront.c|1014| <<xlvbd_flush>> blk_queue_write_cache(info->rq, info->feature_flush ? true : false,
+ *   - drivers/ide/ide-disk.c|554| <<update_flush>> blk_queue_write_cache(drive->queue, wc, false);
+ *   - drivers/md/dm-table.c|1910| <<dm_table_set_restrictions>> blk_queue_write_cache(q, wc, fua);
+ *   - drivers/md/md.c|5506| <<md_alloc>> blk_queue_write_cache(mddev->queue, true, true);
+ *   - drivers/nvme/host/core.c|2204| <<nvme_set_queue_limits>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/nvme/host/multipath.c|393| <<nvme_mpath_alloc_disk>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/scsi/sd.c|152| <<sd_set_flush_flag>> blk_queue_write_cache(sdkp->disk->queue, wc, fua);
+ *
+ * http://www.unjeep.com/article/40696.html
+ *
+ * 硬盘在控制器上的一块内存芯片,其类型一般以SDRAM为主,具有极快的存取速度,
+ * 它是硬盘内部存储和外界接口之间的缓冲器.由于硬盘的内部数据传输速度和外界
+ * 介面传输速度不同,缓存在其中起到一个缓冲的作用.缓存的大小与速度是直接关
+ * 系到硬盘的传输速度的重要因素,能够大幅度地提高硬盘整体性能.
+ *
+ * 如果硬盘的cache启用了,那么很有可能写入的数据是写到了硬盘的cache中,而没
+ * 有真正写到磁盘介质上.
+ *
+ * 在linux下,查看磁盘cache是否开启可通过hdparm命令:
+ *
+ * #hdparm -W /dev/sdx    //是否开启cache，1为enable
+ * #hdparm -W 0 /dev/sdx  //关闭cache
+ * #hdparm -W 1 /dev/sdx  //enable cache
+ *
+ * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+ * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+ *
+ * blk_insert_flush()是非常重要的入口!
+ */
 void blk_queue_write_cache(struct request_queue *q, bool wc, bool fua)
 {
 	if (wc)
diff --git a/block/blk-softirq.c b/block/blk-softirq.c
index 6e7ec87d49fa..71a852ff1c86 100644
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@ -14,12 +14,30 @@
 
 #include "blk.h"
 
+/*
+ * 在以下使用blk_cpu_done:
+ *   - block/blk-softirq.c|28| <<blk_done_softirq>> cpu_list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|47| <<trigger_softirq>> list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|86| <<blk_softirq_cpu_dead>> list_splice_init(&per_cpu(blk_cpu_done, cpu),
+ *   - block/blk-softirq.c|87| <<blk_softirq_cpu_dead>> this_cpu_ptr(&blk_cpu_done));
+ *   - block/blk-softirq.c|126| <<__blk_complete_request>> list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|148| <<blk_softirq_init>> INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
+ *
+ * 上面放的是request
+ */
 static DEFINE_PER_CPU(struct list_head, blk_cpu_done);
 
 /*
  * Softirq action handler - move entries to local list and loop over them
  * while passing them to the queue registered handler.
  */
+/*
+ * 在以下使用blk_done_softirq():
+ *   - 针对percpu的blk_cpu_done上的每一个request
+ * 调用rq->q->mq_ops->complete(rq)
+ * Softirq action handler - move entries to local list and loop over them
+ * while passing them to the queue registered handler.
+ */
 static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 {
 	struct list_head *cpu_list, local_list;
@@ -39,6 +57,13 @@ static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 }
 
 #ifdef CONFIG_SMP
+/*
+ * 在以下使用trigger_softirq():
+ *   - block/blk-softirq.c|62| <<raise_blk_irq>> data->func = trigger_softirq;
+ *
+ * 把data表示的request放入percpu的blk_cpu_done
+ * 触发raise_softirq_irqoff(BLOCK_SOFTIRQ) = blk_done_softirq()
+ */
 static void trigger_softirq(void *data)
 {
 	struct request *rq = data;
@@ -54,6 +79,10 @@ static void trigger_softirq(void *data)
 /*
  * Setup and invoke a run of 'trigger_softirq' on the given cpu.
  */
+/*
+ * called by:
+ *   - block/blk-softirq.c|137| <<__blk_complete_request>> } else if (raise_blk_irq(ccpu, req))
+ */
 static int raise_blk_irq(int cpu, struct request *rq)
 {
 	if (cpu_online(cpu)) {
@@ -76,6 +105,12 @@ static int raise_blk_irq(int cpu, struct request *rq)
 }
 #endif
 
+/*
+ * 在blk_softirq_init()中被使用:
+ * cpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,
+ *                           "block/softirq:dead", NULL,
+ *                           blk_softirq_cpu_dead);
+ */
 static int blk_softirq_cpu_dead(unsigned int cpu)
 {
 	/*
@@ -91,6 +126,10 @@ static int blk_softirq_cpu_dead(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|591| <<__blk_mq_complete_request>> __blk_complete_request(rq);
+ */
 void __blk_complete_request(struct request *req)
 {
 	struct request_queue *q = req->q;
diff --git a/block/blk-stat.c b/block/blk-stat.c
index 7da302ff88d0..6882129dba25 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -12,12 +12,115 @@
 #include "blk-mq.h"
 #include "blk.h"
 
+/*
+ * 核心的patch
+ *
+ * commit 34dbad5d26e2f4b88e60f0e9ad03f99480802812
+ * Author: Omar Sandoval <osandov@fb.com>
+ * Date:   Tue Mar 21 08:56:08 2017 -0700
+ *
+ * blk-stat: convert to callback-based statistics reporting
+ *
+ * Currently, statistics are gathered in ~0.13s windows, and users grab the
+ * statistics whenever they need them. This is not ideal for both in-tree
+ * users:
+ *
+ * 1. Writeback throttling wants its own dynamically sized window of
+ *    statistics. Since the blk-stats statistics are reset after every
+ *    window and the wbt windows don't line up with the blk-stats windows,
+ *    wbt doesn't see every I/O.
+ * 2. Polling currently grabs the statistics on every I/O. Again, depending
+ *    on how the window lines up, we may miss some I/Os. It's also
+ *    unnecessary overhead to get the statistics on every I/O; the hybrid
+ *    polling heuristic would be just as happy with the statistics from the
+ *    previous full window.
+ *
+ * This reworks the blk-stats infrastructure to be callback-based: users
+ * register a callback that they want called at a given time with all of
+ * the statistics from the window during which the callback was active.
+ * Users can dynamically bucketize the statistics. wbt and polling both
+ * currently use read vs. write, but polling can be extended to further
+ * subdivide based on request size.
+ *
+ * The callbacks are kept on an RCU list, and each callback has percpu
+ * stats buffers. There will only be a few users, so the overhead on the
+ * I/O completion side is low. The stats flushing is also simplified
+ * considerably: since the timer function is responsible for clearing the
+ * statistics, we don't have to worry about stale statistics.
+ *
+ * wbt is a trivial conversion. After the conversion, the windowing problem
+ * mentioned above is fixed.
+ *
+ * For polling, we register an extra callback that caches the previous
+ * window's statistics in the struct request_queue for the hybrid polling
+ * heuristic to use.
+ *
+ * Since we no longer have a single stats buffer for the request queue,
+ * this also removes the sysfs and debugfs stats entries. To replace those,
+ * we add a debugfs entry for the poll statistics.
+ *
+ * Signed-off-by: Omar Sandoval <osandov@fb.com>
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ *
+ *
+ * 在blk_alloc_queue_stats()分配一个struct blk_queue_stats结构,
+ * 初始化清空callback链表,设置stats->enable_accounting = false.
+ *
+ * blk_alloc_queue_node()
+ *  -> blk_alloc_queue_stats()
+ *
+ * 在poll的时候把struct blk_stat_callback链接入q->stats->callbacks链表
+ * 设置blk_queue_flag_set(QUEUE_FLAG_STATS, q)
+ *
+ * blk_poll()
+ *  -> blk_mq_poll_hybrid()
+ *      -> blk_mq_poll_hybrid_sleep()
+ *          -> blk_mq_poll_nsecs()
+ *              -> blk_poll_stats_enable()
+ *                  -> blk_stat_add_callback()
+ *
+ * 在end request的时候, 会调用blk_stat_add().
+ * 核心思想是找到request的request_queue->stats,遍历其callback链表,
+ * 对于链表上的每一个struct blk_stat_callback, 调用cb->bucket_fn()选取bucket,
+ * 然后把参数的now汇入对应bucket的struct blk_rq_stat
+ *
+ * 在end request的时候, 还会触发timer,
+ *
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ *
+ * timer最终触发blk_stat_timer_fn()->blk_mq_poll_stats_fn().
+ * blk_mq_poll_stats_fn()的核心思想是把每个bucket的
+ * q->poll_stat[bucket] = cb->stat[bucket];
+ *
+ * 在poll的时候会根据这些stat决定要先在hybrid的时候sleep多久!
+ *
+ *
+ * slides: I/O Latency Optimization with Polling
+ */
+
 struct blk_queue_stats {
 	struct list_head callbacks;
 	spinlock_t lock;
+	/*
+	 * 在以下使用enable_accounting:
+	 *   - block/blk-stat.c|161| <<blk_stat_remove_callback>> if (list_empty(&q->stats->callbacks) && !q->stats->enable_accounting)
+	 *   - block/blk-stat.c|187| <<blk_stat_enable_accounting>> q->stats->enable_accounting = true;
+	 *   - block/blk-stat.c|203| <<blk_alloc_queue_stats>> stats->enable_accounting = false;
+	 */
 	bool enable_accounting;
 };
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|199| <<latency_stat_init>> blk_rq_stat_init(&stat->rqs);
+ *   - block/blk-stat.c|93| <<blk_stat_timer_fn>> blk_rq_stat_init(&cb->stat[bucket]);
+ *   - block/blk-stat.c|101| <<blk_stat_timer_fn>> blk_rq_stat_init(&cpu_stat[bucket]);
+ *   - block/blk-stat.c|153| <<blk_stat_add_callback>> blk_rq_stat_init(&cpu_stat[bucket]);
+ */
 void blk_rq_stat_init(struct blk_rq_stat *stat)
 {
 	stat->min = -1ULL;
@@ -26,6 +129,11 @@ void blk_rq_stat_init(struct blk_rq_stat *stat)
 }
 
 /* src is a per-cpu stat, mean isn't initialized */
+/*
+ * called by:
+ *   - block/blk-iolatency.c|210| <<latency_stat_sum>> blk_rq_stat_sum(&sum->rqs, &stat->rqs);
+ *   - block/blk-stat.c|100| <<blk_stat_timer_fn>> blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
+ */
 void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 {
 	if (!src->nr_samples)
@@ -40,6 +148,18 @@ void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 	dst->nr_samples += src->nr_samples;
 }
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|222| <<latency_stat_record_time>> blk_rq_stat_add(&stat->rqs, req_time);
+ *   - block/blk-stat.c|80| <<blk_stat_add>> blk_rq_stat_add(stat, value);
+ *
+ * 调用的一个例子:
+ * __blk_mq_end_request()
+ *  -> blk_stat_add()
+ *      -> blk_rq_stat_add()
+ *
+ * 把参数的value汇入struct blk_rq_stat
+ */
 void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 {
 	stat->min = min(stat->min, value);
@@ -48,6 +168,14 @@ void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|540| <<__blk_mq_end_request>> blk_stat_add(rq, now);
+ *
+ * 核心思想是找到request的request_queue->stats,遍历其callback链表,
+ * 对于链表上的每一个struct blk_stat_callback, 调用cb->bucket_fn()选取bucket,
+ * 然后把参数的now汇入对应bucket的struct blk_rq_stat
+ */
 void blk_stat_add(struct request *rq, u64 now)
 {
 	struct request_queue *q = rq->q;
@@ -56,6 +184,15 @@ void blk_stat_add(struct request *rq, u64 now)
 	int bucket, cpu;
 	u64 value;
 
+	/*
+	 * 在以下使用io_start_time_ns:
+	 *   - block/bfq-iosched.c|5912| <<bfq_finish_requeue_request>> rq->io_start_time_ns,
+	 *   - block/blk-mq.c|671| <<blk_mq_start_request>> rq->io_start_time_ns = ktime_get_ns();
+	 *   - block/blk-mq.c|327| <<blk_mq_rq_ctx_init>> rq->io_start_time_ns = 0;
+	 *   - block/blk-stat.c|59| <<blk_stat_add>> value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
+	 *   - block/blk-wbt.c|619| <<wbt_issue>> rwb->sync_issue = rq->io_start_time_ns;
+	 *   - block/kyber-iosched.c|651| <<kyber_completed_request>> now - rq->io_start_time_ns);
+	 */
 	value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
 
 	blk_throtl_stat_add(rq, value);
@@ -70,13 +207,34 @@ void blk_stat_add(struct request *rq, u64 now)
 		if (bucket < 0)
 			continue;
 
+		/*
+		 * struct blk_stat_callback:
+		 *  -> struct blk_rq_stat __percpu *cpu_stat;
+		 *  -> struct blk_rq_stat stat;
+		 */
 		stat = &per_cpu_ptr(cb->cpu_stat, cpu)[bucket];
+		/*
+		 * 把参数的value汇入struct blk_rq_stat
+		 */
 		blk_rq_stat_add(stat, value);
 	}
 	put_cpu();
 	rcu_read_unlock();
 }
 
+/*
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ * 说明end request的时候mod timer
+ * The timer callback will be called when the window expires
+ * to gather block statistics during a time window in milliseconds.
+ *
+ * 在以下使用blk_stat_timer_fn():
+ *   - block/blk-stat.c|137| <<blk_stat_alloc_callback>> timer_setup(&cb->timer, blk_stat_timer_fn, 0);
+ */
 static void blk_stat_timer_fn(struct timer_list *t)
 {
 	struct blk_stat_callback *cb = from_timer(cb, t, timer);
@@ -89,16 +247,44 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	for_each_online_cpu(cpu) {
 		struct blk_rq_stat *cpu_stat;
 
+		/*
+		 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+		 */
 		cpu_stat = per_cpu_ptr(cb->cpu_stat, cpu);
 		for (bucket = 0; bucket < cb->buckets; bucket++) {
+			/*
+			 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+			 */
 			blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
 			blk_rq_stat_init(&cpu_stat[bucket]);
 		}
 	}
 
+	/*
+	 * __blk_mq_end_request()
+	 *  -> blk_mq_poll_stats_start()
+	 *      -> blk_stat_activate_msecs()
+	 *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+	 *
+	 * 说明end request的时候mod timer
+	 * The timer callback will be called when the window expires
+	 * to gather block statistics during a time window in milliseconds.
+	 *
+	 * -----------------------------------------
+	 *
+	 * 从blk_mq_init_allocated_queue()进来blk_stat_alloc_callback()
+	 * 的话timer_fn是blk_mq_poll_stats_fn()
+	 */
 	cb->timer_fn(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2861| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *   - block/blk-wbt.c|829| <<wbt_init>> rwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);
+ *
+ * 分配一个struct blk_stat_callback
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
@@ -110,12 +296,18 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 	if (!cb)
 		return NULL;
 
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话buckets是BLK_MQ_POLL_STATS_BKTS
+	 */
 	cb->stat = kmalloc_array(buckets, sizeof(struct blk_rq_stat),
 				 GFP_KERNEL);
 	if (!cb->stat) {
 		kfree(cb);
 		return NULL;
 	}
+	/*
+	 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+	 */
 	cb->cpu_stat = __alloc_percpu(buckets * sizeof(struct blk_rq_stat),
 				      __alignof__(struct blk_rq_stat));
 	if (!cb->cpu_stat) {
@@ -124,15 +316,59 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 		return NULL;
 	}
 
+	/*
+	 * 调用timer_fn的地方:
+	 *   - block/blk-stat.c|257| <<blk_stat_timer_fn>> cb->timer_fn(cb);
+	 *
+	 * 从blk_mq_init_allocated_queue()进来的话timer_fn是blk_mq_poll_stats_fn()
+	 */
 	cb->timer_fn = timer_fn;
+	/*
+	 * 调用bucket_fn的地方:
+	 *   - block/blk-stat.c|206| <<blk_stat_add>> bucket = cb->bucket_fn(rq);
+	 *
+	 * 从blk_mq_init_allocated_queue()进来的话bucket_fn是blk_mq_poll_stats_bkt()
+	 */
 	cb->bucket_fn = bucket_fn;
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话data是'struct request_queue'
+	 */
 	cb->data = data;
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话buckets是BLK_MQ_POLL_STATS_BKTS
+	 */
 	cb->buckets = buckets;
+	/*
+	 * __blk_mq_end_request()
+	 *  -> blk_mq_poll_stats_start()
+	 *      -> blk_stat_activate_msecs()
+	 *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+	 *
+	 * 说明end request的时候mod timer
+	 * The timer callback will be called when the window expires
+	 * to gather block statistics during a time window in milliseconds.
+	 */
 	timer_setup(&cb->timer, blk_stat_timer_fn, 0);
 
 	return cb;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3354| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|852| <<wbt_init>> blk_stat_add_callback(q, rwb->cb);
+ *
+ * poll调用的例子
+ * blk_poll()
+ *  -> blk_mq_poll_hybrid()
+ *      -> blk_mq_poll_hybrid_sleep()
+ *          -> blk_mq_poll_nsecs()
+ *              -> blk_poll_stats_enable()
+ *                  -> blk_stat_add_callback()
+ *
+ * 核心思想是把struct blk_stat_callback链接入q->stats->callbacks链表
+ * 设置blk_queue_flag_set(QUEUE_FLAG_STATS, q)
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -149,10 +385,22 @@ void blk_stat_add_callback(struct request_queue *q,
 
 	spin_lock(&q->stats->lock);
 	list_add_tail_rcu(&cb->list, &q->stats->callbacks);
+	/*
+	 * 在以下使用QUEUE_FLAG_STATS:
+	 *   - block/blk-mq.c|670| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+	 *   - block/blk-stat.c|320| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|335| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|376| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 */
 	blk_queue_flag_set(QUEUE_FLAG_STATS, q);
 	spin_unlock(&q->stats->lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|881| <<__blk_release_queue>> blk_stat_remove_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|696| <<wbt_exit>> blk_stat_remove_callback(q, rwb->cb);
+ */
 void blk_stat_remove_callback(struct request_queue *q,
 			      struct blk_stat_callback *cb)
 {
@@ -165,6 +413,10 @@ void blk_stat_remove_callback(struct request_queue *q,
 	del_timer_sync(&cb->timer);
 }
 
+/*
+ * 在以下使用blk_stat_free_callback_rcu():
+ *   - block/blk-stat.c|187| <<blk_stat_free_callback>> call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
+ */
 static void blk_stat_free_callback_rcu(struct rcu_head *head)
 {
 	struct blk_stat_callback *cb;
@@ -175,12 +427,23 @@ static void blk_stat_free_callback_rcu(struct rcu_head *head)
 	kfree(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2922| <<blk_mq_init_allocated_queue>> blk_stat_free_callback(q->poll_cb);
+ *   - block/blk-sysfs.c|882| <<__blk_release_queue>> blk_stat_free_callback(q->poll_cb);
+ *   - block/blk-wbt.c|697| <<wbt_exit>> blk_stat_free_callback(rwb->cb);
+ */
 void blk_stat_free_callback(struct blk_stat_callback *cb)
 {
 	if (cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
 
+/*
+ * called by:
+ *   - block/blk-throttle.c|2503| <<blk_throtl_register_queue>> blk_stat_enable_accounting(q);
+ *   - block/kyber-iosched.c|431| <<kyber_init_sched>> blk_stat_enable_accounting(q);
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	spin_lock(&q->stats->lock);
@@ -190,6 +453,13 @@ void blk_stat_enable_accounting(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_stat_enable_accounting);
 
+/*
+ * called by:
+ *   - block/blk-core.c|515| <<blk_alloc_queue_node>> q->stats = blk_alloc_queue_stats();
+ *
+ * 分配一个struct blk_queue_stats结构, 初始化清空callback链表
+ * 设置stats->enable_accounting = false;
+ */
 struct blk_queue_stats *blk_alloc_queue_stats(void)
 {
 	struct blk_queue_stats *stats;
@@ -205,6 +475,11 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 	return stats;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|562| <<blk_alloc_queue_node>> blk_free_queue_stats(q->stats);
+ *   - block/blk-sysfs.c|884| <<__blk_release_queue>> blk_free_queue_stats(q->stats);
+ */
 void blk_free_queue_stats(struct blk_queue_stats *stats)
 {
 	if (!stats)
diff --git a/block/blk-stat.h b/block/blk-stat.h
index 17b47a86eefb..5cd0d92b34a7 100644
--- a/block/blk-stat.h
+++ b/block/blk-stat.h
@@ -126,6 +126,12 @@ void blk_stat_free_callback(struct blk_stat_callback *cb);
  * gathering statistics.
  * @cb: The callback.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3369| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+ *   - block/blk-stat.c|203| <<blk_stat_add>> if (!blk_stat_is_active(cb))
+ *   - block/blk-wbt.c|593| <<wbt_wait>> if (!blk_stat_is_active(rwb->cb))
+ */
 static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
 {
 	return timer_pending(&cb->timer);
@@ -139,12 +145,20 @@ static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-wbt.c|349| <<rwb_arm_timer>> blk_stat_activate_nsecs(rwb->cb, rwb->cur_win_nsec);
+ */
 static inline void blk_stat_activate_nsecs(struct blk_stat_callback *cb,
 					   u64 nsecs)
 {
 	mod_timer(&cb->timer, jiffies + nsecs_to_jiffies(nsecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-wbt.c|712| <<wbt_disable_default>> blk_stat_deactivate(rwb->cb);
+ */
 static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
 {
 	del_timer_sync(&cb->timer);
@@ -158,6 +172,19 @@ static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3372| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+ *
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ * 说明end request的时候mod timer
+ * The timer callback will be called when the window expires
+ * to gather block statistics during a time window in milliseconds.
+ */
 static inline void blk_stat_activate_msecs(struct blk_stat_callback *cb,
 					   unsigned int msecs)
 {
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index fca9b158f4a0..231bb884efdb 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -390,8 +390,23 @@ static ssize_t queue_poll_delay_store(struct request_queue *q, const char *page,
 	return count;
 }
 
+/*
+ * nvme.poll_queues=2
+ *
+ * # cat /sys/block/nvme0n1/queue/io_poll
+ */
 static ssize_t queue_poll_show(struct request_queue *q, char *page)
 {
+	/*
+	 * 使用QUEUE_FLAG_POLL的地方:
+	 *   - block/blk-mq.c|3021| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-core.c|936| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - block/blk-mq.c|3774| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - drivers/nvme/host/core.c|805| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+	 *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+	 */
 	return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
 }
 
@@ -873,6 +888,10 @@ static void blk_exit_queue(struct request_queue *q)
  *     of the request queue reaches zero, blk_release_queue is called to release
  *     all allocated resources of the request queue.
  */
+/*
+ * 在以下使用__blk_release_queue():
+ *   - block/blk-sysfs.c|912| <<blk_release_queue>> INIT_WORK(&q->release_work, __blk_release_queue);
+ */
 static void __blk_release_queue(struct work_struct *work)
 {
 	struct request_queue *q = container_of(work, typeof(*q), release_work);
diff --git a/block/blk-zoned.c b/block/blk-zoned.c
index d00fcfd71dfe..fe7c472240e5 100644
--- a/block/blk-zoned.c
+++ b/block/blk-zoned.c
@@ -20,6 +20,74 @@
 
 #include "blk.h"
 
+/*
+ * 使用SMR(Shingled Magnetic Recording)技术后,可以增加存储密度,
+ * 但是带来的开销就是写操作必须是连续的(sequential write).
+ *
+ * 这样就把存储分成好多个zone,每个zone都是写连续的.
+ *
+ * NVMe的FTL层会建立mapping,帮助管理真实的存储.但是跳过FTL会有更多好处.
+ * 可以把NVMe切成多个zoned namespace,这样每个app可以直接使用自己的zoned namespace:
+ *
+ * - Reduced TCO due to minimal DRAM requirement per SSD
+ * - Additional savings due to decreased need for over provisioning of NAND media
+ * - Better drive endurance by reducing write amplification
+ * - Dramatically reduced latency
+ * - Significantly improved throughput
+ *
+ * null_blk和fio都支持zoned block.
+ *
+ * 相关的网站:
+ * - https://zonedstorage.io/introduction/zoned-storage/
+ * - https://zonedstorage.io/benchmarking/fio/
+ * - https://zonedstorage.io/getting-started/prerequisite/
+ * - https://blog.westerndigital.com/what-is-zoned-storage-initiative/
+ * - https://blog.westerndigital.com/storage-architectures-zettabyte-age/
+ */
+
+/*
+ * We've been doing a lot of work with the open source and Linux communities to
+ * contribute to the core technologies of SMR (Shingled Magnetic Recording). By
+ * overlaying tracks on a disk, we can achieve roughly a 20% increase in
+ * capacity. This requires data to be written sequentially so that it will not
+ * alter an underlying write track.
+ *
+ * Rearchitecting can require some effort initially, but the density and cost
+ * benefits are substantial and demonstrate all the advantages of purpose-built
+ * hardware and software-aware constructs. Today, our customers are already
+ * deploying SMR technology, and we expect that 50% of the HDD exabytes we ship
+ * will be on SMR by 2023.
+ *
+ *
+ * Zoned block devices are quite different than the block devices most people
+ * are used to. The concept came from shingled magnetic recording (SMR)
+ * devices, which allow much higher density storage, but that extra capacity
+ * comes with a price: less flexibility. Zoned devices have regions (zones)
+ * that can only be written sequentially; there is no random access for writes
+ * to those zones. Linux already supports these devices, and filesystems are
+ * adding support as well, but some applications may want a simpler, more
+ * straightforward interface; that's what a new filesystem, zonefs, is
+ * targeting.
+ *
+ *
+ * https://zonedstorage.io
+ *
+ * The zones of zoned storage devices must be written sequentially. Each zone
+ * of the device address space has a write pointer that keeps track of the
+ * position of the next write. Data in a zone cannot be directly overwritten.
+ * The zone must first be erased using a special command (zone reset). The
+ * figure below illustrates this principle.
+ *
+ * Linux ZBD interface implementation provides functions to discover the zone
+ * configuration of a zoned device and functions to manage zones (e.g. Zone
+ * reset). Furthermore, the Linux kernel ZBD support also modifies the kernel
+ * block I/O stack to ensure that the device access constraints (zone spanning
+ * commands, sequential write ordering, etc) are met.
+ */
+
+/*
+ * 没人使用
+ */
 static inline sector_t blk_zone_start(struct request_queue *q,
 				      sector_t sector)
 {
@@ -31,6 +99,11 @@ static inline sector_t blk_zone_start(struct request_queue *q,
 /*
  * Return true if a request is a write requests that needs zone write locking.
  */
+/*
+ * called by:
+ *   - include/linux/blkdev.h|1849| <<blk_req_zone_write_lock>> if (blk_req_needs_zone_write_lock(rq))
+ *   - include/linux/blkdev.h|1867| <<blk_req_can_dispatch_to_zone>> if (!blk_req_needs_zone_write_lock(rq))
+ */
 bool blk_req_needs_zone_write_lock(struct request *rq)
 {
 	if (!rq->q->seq_zones_wlock)
@@ -50,6 +123,10 @@ bool blk_req_needs_zone_write_lock(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_req_needs_zone_write_lock);
 
+/*
+ * called by:
+ *   - include/linux/blkdev.h|1850| <<blk_req_zone_write_lock>> __blk_req_zone_write_lock(rq);
+ */
 void __blk_req_zone_write_lock(struct request *rq)
 {
 	if (WARN_ON_ONCE(test_and_set_bit(blk_rq_zone_no(rq),
@@ -61,6 +138,10 @@ void __blk_req_zone_write_lock(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(__blk_req_zone_write_lock);
 
+/*
+ * called by:
+ *   - include/linux/blkdev.h|1856| <<blk_req_zone_write_unlock>> __blk_req_zone_write_unlock(rq);
+ */
 void __blk_req_zone_write_unlock(struct request *rq)
 {
 	rq->rq_flags &= ~RQF_ZONE_WRITE_LOCKED;
@@ -77,6 +158,13 @@ EXPORT_SYMBOL_GPL(__blk_req_zone_write_unlock);
  * Return the total number of zones of a zoned block device.  For a block
  * device without zone capabilities, the number of zones is always 0.
  */
+/*
+ * called by:
+ *   - block/ioctl.c|515| <<blkdev_ioctl>> return put_uint(arg, blkdev_nr_zones(bdev->bd_disk));
+ *   - drivers/block/null_blk_main.c|1947| <<null_gendisk_register>> nullb->q->nr_zones = blkdev_nr_zones(disk);
+ *   - drivers/md/dm-table.c|1962| <<dm_table_set_restrictions>> q->nr_zones = blkdev_nr_zones(t->md->disk);
+ *   - drivers/md/dm-zoned-target.c|730| <<dmz_get_zoned_device>> dev->nr_zones = blkdev_nr_zones(dev->bdev->bd_disk);
+ */
 unsigned int blkdev_nr_zones(struct gendisk *disk)
 {
 	sector_t zone_sectors = blk_queue_zone_sectors(disk->queue);
@@ -106,6 +194,15 @@ EXPORT_SYMBOL_GPL(blkdev_nr_zones);
  *    Note: The caller must use memalloc_noXX_save/restore() calls to control
  *    memory allocations done within this function.
  */
+/*
+ * called by:
+ *   - block/blk-zoned.c|263| <<blkdev_report_zones_ioctl>> ret = blkdev_report_zones(bdev, rep.sector, rep.nr_zones,
+ *   - drivers/md/dm-flakey.c|469| <<flakey_report_zones>> return blkdev_report_zones(fc->dev->bdev, sector, nr_zones,
+ *   - drivers/md/dm-linear.c|146| <<linear_report_zones>> return blkdev_report_zones(lc->dev->bdev, sector, nr_zones,
+ *   - drivers/md/dm-zoned-metadata.c|1179| <<dmz_init_zones>> ret = blkdev_report_zones(dev->bdev, 0, BLK_ALL_ZONES, dmz_init_zone,
+ *   - drivers/md/dm-zoned-metadata.c|1223| <<dmz_update_zone>> ret = blkdev_report_zones(zmd->dev->bdev, dmz_start_sect(zmd, zone), 1,
+ *   - fs/f2fs/super.c|2938| <<init_blkz_info>> ret = blkdev_report_zones(bdev, 0, BLK_ALL_ZONES, f2fs_report_zone_cb,
+ */
 int blkdev_report_zones(struct block_device *bdev, sector_t sector,
 			unsigned int nr_zones, report_zones_cb cb, void *data)
 {
@@ -123,6 +220,10 @@ int blkdev_report_zones(struct block_device *bdev, sector_t sector,
 }
 EXPORT_SYMBOL_GPL(blkdev_report_zones);
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|248| <<blkdev_zone_mgmt>> blkdev_allow_reset_all_zones(bdev, sector, nr_sectors)) {
+ */
 static inline bool blkdev_allow_reset_all_zones(struct block_device *bdev,
 						sector_t sector,
 						sector_t nr_sectors)
@@ -153,6 +254,12 @@ static inline bool blkdev_allow_reset_all_zones(struct block_device *bdev,
  *    The operation to execute on each zone can be a zone reset, open, close
  *    or finish request.
  */
+/*
+ * called by:
+ *   - block/blk-zoned.c|322| <<blkdev_zone_mgmt_ioctl>> return blkdev_zone_mgmt(bdev, op, zrange.sector, zrange.nr_sectors,
+ *   - drivers/md/dm-zoned-metadata.c|1289| <<dmz_reset_zone>> ret = blkdev_zone_mgmt(dev->bdev, REQ_OP_ZONE_RESET,
+ *   - fs/f2fs/segment.c|1784| <<__f2fs_issue_discard_zone>> return blkdev_zone_mgmt(bdev, REQ_OP_ZONE_RESET,
+ */
 int blkdev_zone_mgmt(struct block_device *bdev, enum req_opf op,
 		     sector_t sector, sector_t nr_sectors,
 		     gfp_t gfp_mask)
@@ -173,6 +280,31 @@ int blkdev_zone_mgmt(struct block_device *bdev, enum req_opf op,
 	if (!op_is_zone_mgmt(op))
 		return -EOPNOTSUPP;
 
+	/*
+	 * #define _GNU_SOURCE 1
+	 * #include <sys/ioctl.h>
+	 * #include <sys/types.h>
+	 * #include <sys/stat.h>
+	 * #include <fcntl.h>
+	 *
+	 * typedef unsigned long long __u64;
+	 *
+	 * struct blk_zone_range {
+	 *	__u64 sector;
+	 *	__u64 nr_sectors;
+	 * };
+	 *
+	 * #define BLKRESETZONE    _IOW(0x12, 131, struct blk_zone_range)
+	 *
+	 * int main(void)
+	 * {
+	 *	int fd = open("/dev/nullb0", O_RDWR|O_DIRECT);
+	 *	struct blk_zone_range zr = {4096, 0xfffffffffffff000ULL};
+	 *	ioctl(fd, BLKRESETZONE, &zr);
+	 *	return 0;
+	 *
+	 * 这里似乎有个bug??
+	 */
 	if (!nr_sectors || end_sector > capacity)
 		/* Out of range */
 		return -EINVAL;
@@ -217,6 +349,10 @@ struct zone_report_args {
 	struct blk_zone __user *zones;
 };
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|316| <<blkdev_report_zones_ioctl>> blkdev_copy_zone_to_user, &args)
+ */
 static int blkdev_copy_zone_to_user(struct blk_zone *zone, unsigned int idx,
 				    void *data)
 {
@@ -231,6 +367,10 @@ static int blkdev_copy_zone_to_user(struct blk_zone *zone, unsigned int idx,
  * BLKREPORTZONE ioctl processing.
  * Called from blkdev_ioctl.
  */
+/*
+ * 处理blkdev_ioctl(BLKREPORTZONE):
+ *   - block/ioctl.c|506| <<blkdev_ioctl>> return blkdev_report_zones_ioctl(bdev, mode, cmd, arg);
+ */
 int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,
 			      unsigned int cmd, unsigned long arg)
 {
@@ -275,6 +415,10 @@ int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,
  * BLKRESETZONE, BLKOPENZONE, BLKCLOSEZONE and BLKFINISHZONE ioctl processing.
  * Called from blkdev_ioctl.
  */
+/*
+ * 处理blkdev_ioctl(BLKFINISHZONE):
+ *   - block/ioctl.c|511| <<blkdev_ioctl>> return blkdev_zone_mgmt_ioctl(bdev, mode, cmd, arg);
+ */
 int blkdev_zone_mgmt_ioctl(struct block_device *bdev, fmode_t mode,
 			   unsigned int cmd, unsigned long arg)
 {
@@ -323,6 +467,11 @@ int blkdev_zone_mgmt_ioctl(struct block_device *bdev, fmode_t mode,
 				GFP_KERNEL);
 }
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|452| <<blk_revalidate_zone_cb>> blk_alloc_zone_bitmap(q->node, args->nr_zones);
+ *   - block/blk-zoned.c|462| <<blk_revalidate_zone_cb>> blk_alloc_zone_bitmap(q->node, args->nr_zones);
+ */
 static inline unsigned long *blk_alloc_zone_bitmap(int node,
 						   unsigned int nr_zones)
 {
@@ -330,6 +479,11 @@ static inline unsigned long *blk_alloc_zone_bitmap(int node,
 			    GFP_NOIO, node);
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|910| <<__blk_release_queue>> blk_queue_free_zone_bitmaps(q);
+ *   - block/blk-zoned.c|529| <<blk_revalidate_disk_zones>> blk_queue_free_zone_bitmaps(q);
+ */
 void blk_queue_free_zone_bitmaps(struct request_queue *q)
 {
 	kfree(q->conv_zones_bitmap);
@@ -350,6 +504,13 @@ struct blk_revalidate_zone_args {
 /*
  * Helper function to check the validity of zones of a zoned block device.
  */
+/*
+ * 在以下使用blk_revalidate_zone_cb():
+ *   - block/blk-zoned.c|512| <<blk_revalidate_disk_zones>> blk_revalidate_zone_cb, &args);
+ *
+ * 543         ret = disk->fops->report_zones(disk, 0, UINT_MAX,
+ * 544                                        blk_revalidate_zone_cb, &args);
+ */
 static int blk_revalidate_zone_cb(struct blk_zone *zone, unsigned int idx,
 				  void *data)
 {
@@ -432,6 +593,11 @@ static int blk_revalidate_zone_cb(struct blk_zone *zone, unsigned int idx,
  * drivers only q->nr_zones needs to be updated so that the sysfs exposed value
  * is correct.
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1941| <<null_gendisk_register>> int ret = blk_revalidate_disk_zones(disk);
+ *   - drivers/scsi/sd_zbc.c|440| <<sd_zbc_read_zones>> ret = blk_revalidate_disk_zones(disk);
+ */
 int blk_revalidate_disk_zones(struct gendisk *disk)
 {
 	struct request_queue *q = disk->queue;
diff --git a/block/blk.h b/block/blk.h
index 0b8884353f6b..c20b7d4f5f97 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -15,22 +15,83 @@
 extern struct dentry *blk_debugfs_root;
 #endif
 
+/*
+ * blk_flush_queue来自blk_mq_hw_ctx->fq
+ */
 struct blk_flush_queue {
 	unsigned int		flush_queue_delayed:1;
+	/*
+	 * 在以下修改flushing_pending_idx:
+	 *   - block/blk-flush.c|416| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 * 在以下使用flush_pending_idx:
+	 *   - block/blk-flush.c|276| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|392| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 */
 	unsigned int		flush_pending_idx:1;
+	/*
+	 * 在以下修改flush_running_idx:
+	 *   - block/blk-flush.c|362| <<flush_end_io>> fq->flush_running_idx ^= 1;
+	 * 在以下使用flush_running_idx:
+	 *   - block/blk-flush.c|358| <<flush_end_io>> running = &fq->flush_queue[fq->flush_running_idx];
+	 *   - block/blk-flush.c|359| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|398| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 */
 	unsigned int		flush_running_idx:1;
 	blk_status_t 		rq_status;
 	unsigned long		flush_pending_since;
+	/*
+	 * 使用flush_queue[2]的地方:
+	 *   - block/blk-flush.c|352| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|454| <<flush_end_io>> running = &fq->flush_queue[fq->flush_running_idx];
+	 *   - block/blk-flush.c|501| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|849| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[0]);
+	 *   - block/blk-flush.c|850| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_queue[1]);
+	 */
 	struct list_head	flush_queue[2];
+	/*
+	 * 在以下使用flush_data_in_flight:
+	 *   - block/blk-flush.c|400| <<blk_flush_complete_seq>> list_move_tail(&rq->flush.list, &fq->flush_data_in_flight);
+	 *   - block/blk-flush.c|599| <<blk_kick_flush>> if (!list_empty(&fq->flush_data_in_flight) && q->elevator &&
+	 *   - block/blk-flush.c|934| <<blk_alloc_flush_queue>> INIT_LIST_HEAD(&fq->flush_data_in_flight);
+	 */
 	struct list_head	flush_data_in_flight;
+	/*
+	 * 使用flush_rq的地方:
+	 *   - block/blk-flush.c|504| <<blk_kick_flush>> struct request *flush_rq = fq->flush_rq;
+	 *   - block/blk-flush.c|845| <<blk_alloc_flush_queue>> fq->flush_rq = kzalloc_node(rq_sz, flags, node);
+	 *   - block/blk-flush.c|846| <<blk_alloc_flush_queue>> if (!fq->flush_rq)
+	 *   - block/blk-flush.c|875| <<blk_free_flush_queue>> kfree(fq->flush_rq);
+	 *   - block/blk-mq.c|2303| <<blk_mq_exit_hctx>> set->ops->exit_request(set, hctx->fq->flush_rq, hctx_idx);
+	 *   - block/blk-mq.c|2361| <<blk_mq_init_hctx>> if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx,
+	 *   - block/blk.h|81| <<is_flush_rq>> return hctx->fq->flush_rq == req;
+	 */
 	struct request		*flush_rq;
 
 	/*
 	 * flush_rq shares tag with this rq, both can't be active
 	 * at the same time
 	 */
+	/*
+	 * orig_rq在以下使用:
+	 *   - block/blk-flush.c|458| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+	 *   - block/blk-flush.c|624| <<blk_kick_flush>> fq->orig_rq = first_rq;
+	 */
 	struct request		*orig_rq;
 	struct lock_class_key	key;
+	/*
+	 * 使用mq_flush_lock的地方:
+	 *   - block/blk-flush.c|434| <<flush_end_io>> spin_lock_irqsave(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|438| <<flush_end_io>> spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|469| <<flush_end_io>> spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|631| <<mq_flush_data_end_io>> spin_lock_irqsave(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|633| <<mq_flush_data_end_io>> spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|771| <<blk_insert_flush>> spin_lock_irq(&fq->mq_flush_lock);
+	 *   - block/blk-flush.c|779| <<blk_insert_flush>> spin_unlock_irq(&fq->mq_flush_lock);
+	 *   - block/blk-flush.c|874| <<blk_alloc_flush_queue>> spin_lock_init(&fq->mq_flush_lock);
+	 *   - block/blk-flush.c|886| <<blk_alloc_flush_queue>> lockdep_set_class(&fq->mq_flush_lock, &fq->key);
+	 */
 	spinlock_t		mq_flush_lock;
 };
 
@@ -38,6 +99,12 @@ extern struct kmem_cache *blk_requestq_cachep;
 extern struct kobj_type blk_queue_ktype;
 extern struct ida blk_queue_ida;
 
+/*
+ * called by:
+ *   - block/blk-flush.c|301| <<flush_end_io>> struct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);
+ *   - block/blk-flush.c|427| <<mq_flush_data_end_io>> struct blk_flush_queue *fq = blk_get_flush_queue(q, ctx);
+ *   - block/blk-flush.c|464| <<blk_insert_flush>> struct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);
+ */
 static inline struct blk_flush_queue *
 blk_get_flush_queue(struct request_queue *q, struct blk_mq_ctx *ctx)
 {
@@ -107,6 +174,11 @@ static inline bool bvec_gap_to_prev(struct request_queue *q,
 	return __bvec_gap_to_prev(q, bprv, offset);
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|31| <<blk_rq_append_bio>> blk_rq_bio_prep(rq, *bio, nr_segs);
+ *   - block/blk-mq.c|2088| <<blk_mq_bio_to_request>> blk_rq_bio_prep(rq, bio, nr_segs);
+ */
 static inline void blk_rq_bio_prep(struct request *rq, struct bio *bio,
 		unsigned int nr_segs)
 {
diff --git a/block/genhd.c b/block/genhd.c
index ff6268970ddc..ef46052eeceb 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -86,6 +86,10 @@ unsigned int part_in_flight(struct request_queue *q, struct hd_struct *part)
 	return inflight;
 }
 
+/*
+ * called by:
+ *   - block/partition-generic.c|159| <<part_inflight_show>> part_in_flight_rw(q, p, inflight);
+ */
 void part_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 		       unsigned int inflight[2])
 {
@@ -108,8 +112,24 @@ void part_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 		inflight[1] = 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|858| <<blk_partition_remap>> p = __disk_get_part(bio->bi_disk, bio->bi_partno);
+ *   - block/genhd.c|148| <<disk_get_part>> part = __disk_get_part(disk, partno); 
+ *   - fs/buffer.c|3040| <<guard_bio_eod>> part = __disk_get_part(bio->bi_disk, bio->bi_partno);
+ *
+ * struct gendisk:
+ *  -> struct disk_part_tbl __rcu *part_tbl;
+ *  -> struct hd_struct part0;
+ * 返回gendisk->part_tlb->part[partno]
+ */
 struct hd_struct *__disk_get_part(struct gendisk *disk, int partno)
 {
+	/*
+	 * struct gendisk:
+	 *  -> struct disk_part_tbl __rcu *part_tbl;
+	 *  -> struct hd_struct part0;
+	 */
 	struct disk_part_tbl *ptbl = rcu_dereference(disk->part_tbl);
 
 	if (unlikely(partno < 0 || partno >= ptbl->len))
@@ -131,11 +151,32 @@ struct hd_struct *__disk_get_part(struct gendisk *disk, int partno)
  * RETURNS:
  * Pointer to the found partition on success, NULL if not found.
  */
+/*
+ * called by:
+ *   - block/genhd.c|965| <<bdget_disk>> part = disk_get_part(disk, partno);
+ *   - block/genhd.c|1495| <<blk_lookup_devt>> part = disk_get_part(disk, partno);
+ *   - block/ioctl.c|74| <<blkpg_ioctl>> part = disk_get_part(disk, partno);
+ *   - block/ioctl.c|112| <<blkpg_ioctl>> part = disk_get_part(disk, partno);
+ *   - fs/block_dev.c|1598| <<__blkdev_get>> bdev->bd_part = disk_get_part(disk, partno);
+ *   - fs/block_dev.c|1649| <<__blkdev_get>> bdev->bd_part = disk_get_part(disk, partno);
+ *   - init/do_mounts.c|154| <<devt_from_partuuid>> part = disk_get_part(disk, dev_to_part(dev)->partno + offset);
+ *
+ * struct gendisk:
+ *  -> struct disk_part_tbl __rcu *part_tbl;
+ *  -> struct hd_struct part0;
+ * 返回gendisk->part_tlb->part[partno]
+ */
 struct hd_struct *disk_get_part(struct gendisk *disk, int partno)
 {
 	struct hd_struct *part;
 
 	rcu_read_lock();
+	/*
+	 * struct gendisk:
+	 *  -> struct disk_part_tbl __rcu *part_tbl;
+	 *  -> struct hd_struct part0;
+	 * 返回gendisk->part_tlb->part[partno]
+	 */
 	part = __disk_get_part(disk, partno);
 	if (part)
 		get_device(part_to_dev(part));
@@ -189,6 +230,20 @@ EXPORT_SYMBOL_GPL(disk_part_iter_init);
  * CONTEXT:
  * Don't care.
  */
+/*
+ * called by:
+ *   - block/genhd.c|733| <<register_disk>> while ((part = disk_part_iter_next(&piter)))
+ *   - block/genhd.c|855| <<del_gendisk>> while ((part = disk_part_iter_next(&piter))) {
+ *   - block/genhd.c|1058| <<printk_all_partitions>> while ((part = disk_part_iter_next(&piter))) {
+ *   - block/genhd.c|1153| <<show_partition>> while ((part = disk_part_iter_next(&piter)))
+ *   - block/genhd.c|1477| <<diskstats_show>> while ((hd = disk_part_iter_next(&piter))) {
+ *   - block/genhd.c|1691| <<set_disk_ro>> while ((part = disk_part_iter_next(&piter)))
+ *   - block/ioctl.c|58| <<blkpg_ioctl>> while ((part = disk_part_iter_next(&piter))) {
+ *   - block/ioctl.c|132| <<blkpg_ioctl>> while ((lpart = disk_part_iter_next(&piter))) {
+ *   - block/partition-generic.c|502| <<blk_drop_partitions>> while ((part = disk_part_iter_next(&piter)))
+ *   - drivers/s390/block/dasd.c|445| <<dasd_state_ready_to_online>> while ((part = disk_part_iter_next(&piter)))
+ *   - drivers/s390/block/dasd.c|472| <<dasd_state_online_to_ready>> while ((part = disk_part_iter_next(&piter)))
+ */
 struct hd_struct *disk_part_iter_next(struct disk_part_iter *piter)
 {
 	struct disk_part_tbl *ptbl;
@@ -305,6 +360,14 @@ EXPORT_SYMBOL_GPL(disk_map_sector_rcu);
  * Can be deleted altogether. Later.
  *
  */
+/*
+ * 在以下使用major_names[]:
+ *   - block/genhd.c|327| <<blkdev_show>> for (dp = major_names[major_to_index(offset)]; dp; dp = dp->next)
+ *   - block/genhd.c|363| <<register_blkdev>> for (index = ARRAY_SIZE(major_names)-1; index > 0; index--) {
+ *   - block/genhd.c|364| <<register_blkdev>> if (major_names[index] == NULL)
+ *   - block/genhd.c|397| <<register_blkdev>> for (n = &major_names[index]; *n; n = &(*n)->next) {
+ *   - block/genhd.c|425| <<unregister_blkdev>> for (n = &major_names[index]; *n; n = &(*n)->next)
+ */
 #define BLKDEV_MAJOR_HASH_SIZE 255
 static struct blk_major_name {
 	struct blk_major_name *next;
@@ -351,6 +414,23 @@ void blkdev_show(struct seq_file *seqf, off_t offset)
  * See Documentation/admin-guide/devices.txt for the list of allocated
  * major numbers.
  */
+/*
+ * 部分调用register_blkdev()的例子:
+ *   - block/genhd.c|1105| <<genhd_device_init>> register_blkdev(BLOCK_EXT_MAJOR, "blkext");
+ *   - drivers/block/loop.c|2274| <<loop_init>> if (register_blkdev(LOOP_MAJOR, "loop")) {
+ *   - drivers/block/nbd.c|2372| <<nbd_init>> if (register_blkdev(NBD_MAJOR, "nbd"))
+ *   - drivers/block/null_blk_main.c|1845| <<null_init>> null_major = register_blkdev(0, "nullb");
+ *   - drivers/block/virtio_blk.c|1033| <<init>> major = register_blkdev(0, "virtblk");
+ *   - drivers/block/xen-blkfront.c|2718| <<xlblk_init>> if (register_blkdev(XENVBD_MAJOR, DEV_NAME)) {
+ *   - drivers/ide/ide-probe.c|1002| <<hwif_init>> if (register_blkdev(hwif->major, hwif->name))
+ *   - drivers/md/dm.c|235| <<local_init>> r = register_blkdev(_major, _name);
+ *   - drivers/md/md.c|9328| <<md_init>> if ((ret = register_blkdev(MD_MAJOR, "md")) < 0)
+ *   - drivers/md/md.c|9331| <<md_init>> if ((ret = register_blkdev(0, "mdp")) < 0)
+ *   - drivers/scsi/sd.c|3644| <<init_sd>> if (register_blkdev(sd_major(i), "sd") != 0)
+ *   - drivers/scsi/sr.c|1036| <<init_sr>> rc = register_blkdev(SCSI_CDROM_MAJOR, "sr");
+ *
+ * 核心思想是在major_names[]找到一个可用的major
+ */
 int register_blkdev(unsigned int major, const char *name)
 {
 	struct blk_major_name **n, *p;
@@ -923,11 +1003,31 @@ EXPORT_SYMBOL(get_gendisk);
  * RETURNS:
  * Resulting block_device on success, NULL on failure.
  */
+/*
+ * called by:
+ *   - block/genhd.c|684| <<register_disk>> bdev = bdget_disk(disk, 0);
+ *   - block/genhd.c|1655| <<invalidate_partition>> struct block_device *bdev = bdget_disk(disk, partno);
+ *   - drivers/block/nbd.c|302| <<nbd_size_update>> struct block_device *bdev = bdget_disk(nbd->disk, 0);
+ *   - drivers/block/nbd.c|1483| <<nbd_release>> struct block_device *bdev = bdget_disk(disk, 0);
+ *   - drivers/block/xen-blkfront.c|2145| <<blkfront_closing>> bdev = bdget_disk(info->gd, 0);
+ *   - drivers/block/xen-blkfront.c|2506| <<blkfront_remove>> bdev = bdget_disk(disk, 0);
+ *   - drivers/block/xen-blkfront.c|2588| <<blkif_release>> bdev = bdget_disk(disk, 0);
+ *   - drivers/md/dm.c|1984| <<alloc_dev>> md->bdev = bdget_disk(md->disk, 0);
+ *   - fs/block_dev.c|1167| <<bd_start_claiming>> whole = bdget_disk(disk, 0);
+ *   - fs/block_dev.c|1459| <<revalidate_disk>> struct block_device *bdev = bdget_disk(disk, 0);
+ *   - fs/block_dev.c|1640| <<__blkdev_get>> whole = bdget_disk(disk, 0);
+ */
 struct block_device *bdget_disk(struct gendisk *disk, int partno)
 {
 	struct hd_struct *part;
 	struct block_device *bdev = NULL;
 
+	/*
+	 * struct gendisk:
+	 *  -> struct disk_part_tbl __rcu *part_tbl;
+	 *  -> struct hd_struct part0;
+	 * 返回gendisk->part_tlb->part[partno]
+	 */
 	part = disk_get_part(disk, partno);
 	if (part)
 		bdev = bdget(part_devt(part));
@@ -1283,6 +1383,15 @@ static void disk_replace_part_tbl(struct gendisk *disk,
  * RETURNS:
  * 0 on success, -errno on failure.
  */
+/*
+ * called by:
+ *   - block/genhd.c|1517| <<__alloc_disk_node>> if (disk_expand_part_tbl(disk, 0)) {
+ *   - block/partition-generic.c|342| <<add_partition>> err = disk_expand_part_tbl(disk, partno);
+ *   - block/partition-generic.c|593| <<blk_add_partitions>> disk_expand_part_tbl(disk, highest);
+ *
+ * 核心思想就是gendisk->parttlb->part[partno]数组太短
+ * 需要扩展以下
+ */
 int disk_expand_part_tbl(struct gendisk *disk, int partno)
 {
 	struct disk_part_tbl *old_ptbl =
@@ -1465,6 +1574,10 @@ dev_t blk_lookup_devt(const char *name, int partno)
 }
 EXPORT_SYMBOL(blk_lookup_devt);
 
+/*
+ * called by:
+ *   - include/linux/genhd.h|682| <<alloc_disk_node>> __disk = __alloc_disk_node(minors, node_id); \
+ */
 struct gendisk *__alloc_disk_node(int minors, int node_id)
 {
 	struct gendisk *disk;
diff --git a/block/partition-generic.c b/block/partition-generic.c
index 564fae77711d..c4af34d61e89 100644
--- a/block/partition-generic.c
+++ b/block/partition-generic.c
@@ -223,6 +223,15 @@ static const struct attribute_group *part_attr_groups[] = {
 	NULL
 };
 
+/*
+ * [0] part_release
+ * [0] device_release
+ * [0] kobject_cleanup
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void part_release(struct device *dev)
 {
 	struct hd_struct *p = dev_to_part(dev);
@@ -259,6 +268,10 @@ static void delete_partition_work_fn(struct work_struct *work)
 	put_device(part_to_dev(part));
 }
 
+/*
+ * called by:
+ *   - include/linux/genhd.h|718| <<hd_ref_init>> if (percpu_ref_init(&part->ref, __delete_partition, 0,
+ */
 void __delete_partition(struct percpu_ref *ref)
 {
 	struct hd_struct *part = container_of(ref, struct hd_struct, ref);
@@ -270,6 +283,14 @@ void __delete_partition(struct percpu_ref *ref)
  * Must be called either with bd_mutex held, before a disk can be opened or
  * after all disk users are gone.
  */
+/*
+ * called by:
+ *   - block/genhd.c|858| <<del_gendisk>> delete_partition(disk, part->partno);
+ *   - block/ioctl.c|94| <<blkpg_ioctl>> delete_partition(disk, partno);
+ *   - block/partition-generic.c|503| <<blk_drop_partitions>> delete_partition(disk, part->partno);
+ *
+ * 针对数据结构来说,主要是清空gendisk->part_tlb->part[partno]为NULL
+ */
 void delete_partition(struct gendisk *disk, int partno)
 {
 	struct disk_part_tbl *ptbl =
@@ -309,12 +330,22 @@ static DEVICE_ATTR(whole_disk, 0444, whole_disk_show, NULL);
  * Must be called either with bd_mutex held, before a disk can be opened or
  * after all disk users are gone.
  */
+/*
+ * called by:
+ *   - block/ioctl.c|69| <<blkpg_ioctl>> part = add_partition(disk, partno, start, length,
+ *   - block/partition-generic.c|520| <<blk_add_partition>> part = add_partition(disk, p, from, size, state->parts[p].flags,
+ *
+ * 核心思想就是分配hd_struct, 设置其start_sect, nr_sects和partno等,
+ * 分配devt和调用device_add()
+ * 然如放入gendisk->parttlb->part[partno]
+ */
 struct hd_struct *add_partition(struct gendisk *disk, int partno,
 				sector_t start, sector_t len, int flags,
 				struct partition_meta_info *info)
 {
 	struct hd_struct *p;
 	dev_t devt = MKDEV(0, 0);
+	/* (&(disk)->part0.__dev) */
 	struct device *ddev = disk_to_dev(disk);
 	struct device *pdev;
 	struct disk_part_tbl *ptbl;
@@ -339,6 +370,10 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 		break;
 	}
 
+	/*
+	 * 核心思想就是gendisk->parttlb->part[partno]数组太短
+	 * 需要扩展以
+	 */
 	err = disk_expand_part_tbl(disk, partno);
 	if (err)
 		return ERR_PTR(err);
@@ -347,6 +382,7 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 	if (ptbl->part[partno])
 		return ERR_PTR(-EBUSY);
 
+	/* p是struct hd_struct */
 	p = kzalloc(sizeof(*p), GFP_KERNEL);
 	if (!p)
 		return ERR_PTR(-EBUSY);
@@ -357,6 +393,11 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 	}
 
 	seqcount_init(&p->nr_sects_seq);
+	/*
+	 * struct hd_struct:
+	 *  -> struct device __dev;
+	 * 返回hd_struct->__dev
+	 */
 	pdev = part_to_dev(p);
 
 	p->start_sect = start;
@@ -420,6 +461,7 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 	}
 
 	/* everything is up and running, commence */
+	/* 这一行是非常核心的 !!! */
 	rcu_assign_pointer(ptbl->part[partno], p);
 
 	/* suppress uevent if the disk suppresses it */
@@ -460,6 +502,10 @@ static bool disk_unlock_native_capacity(struct gendisk *disk)
 	}
 }
 
+/*
+ * called by:
+ *   - fs/block_dev.c|1528| <<bdev_disk_changed>> ret = blk_drop_partitions(disk, bdev);
+ */
 int blk_drop_partitions(struct gendisk *disk, struct block_device *bdev)
 {
 	struct disk_part_iter piter;
@@ -482,6 +528,14 @@ int blk_drop_partitions(struct gendisk *disk, struct block_device *bdev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/partition-generic.c|600| <<blk_add_partitions>> if (!blk_add_partition(disk, bdev, state, p))
+ *
+ * 核心思想就是分配hd_struct, 设置其start_sect, nr_sects和partno等,
+ * 分配devt和调用device_add()
+ * 然如放入gendisk->parttlb->part[partno]
+ */
 static bool blk_add_partition(struct gendisk *disk, struct block_device *bdev,
 		struct parsed_partitions *state, int p)
 {
@@ -517,6 +571,11 @@ static bool blk_add_partition(struct gendisk *disk, struct block_device *bdev,
 		size = get_capacity(disk) - from;
 	}
 
+	/*
+	 * 核心思想就是分配hd_struct, 设置其start_sect, nr_sects和partno等,
+	 * 分配devt和调用device_add()
+	 * 然如放入gendisk->parttlb->part[partno]
+	 */
 	part = add_partition(disk, p, from, size, state->parts[p].flags,
 			     &state->parts[p].info);
 	if (IS_ERR(part) && PTR_ERR(part) != -ENXIO) {
@@ -532,6 +591,40 @@ static bool blk_add_partition(struct gendisk *disk, struct block_device *bdev,
 	return true;
 }
 
+/*
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - fs/block_dev.c|1531| <<bdev_disk_changed>> ret = blk_add_partitions(disk, bdev);
+ */
 int blk_add_partitions(struct gendisk *disk, struct block_device *bdev)
 {
 	struct parsed_partitions *state;
@@ -602,6 +695,11 @@ int blk_add_partitions(struct gendisk *disk, struct block_device *bdev)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/partitions/check.h|38| <<read_part_sector>> return read_dev_sector(state->bdev, n, p);
+ *   - drivers/scsi/scsicam.c|42| <<scsi_bios_ptable>> void *data = read_dev_sector(bdev, 0, &sect);
+ */
 unsigned char *read_dev_sector(struct block_device *bdev, sector_t n, Sector *p)
 {
 	struct address_space *mapping = bdev->bd_inode->i_mapping;
diff --git a/block/partitions/check.c b/block/partitions/check.c
index ffe408fead0c..148b2cd90ce0 100644
--- a/block/partitions/check.c
+++ b/block/partitions/check.c
@@ -112,6 +112,10 @@ static int (*check_part[])(struct parsed_partitions *) = {
 	NULL
 };
 
+/*
+ * 针对gendisk分配一个struct parsed_partitions
+ * 主要要分配parsed_partitions->parts[]数组,用来临时保存part的元数据
+ */
 static struct parsed_partitions *allocate_partitions(struct gendisk *hd)
 {
 	struct parsed_partitions *state;
@@ -122,6 +126,16 @@ static struct parsed_partitions *allocate_partitions(struct gendisk *hd)
 		return NULL;
 
 	nr = disk_max_parts(hd);
+	/*
+	 * struct parsed_partitions:
+	 *  -> struct {
+	 *      -> sector_t from;
+	 *      -> sector_t size;
+	 *      -> int flags;
+	 *      -> bool has_info;
+	 *      -> struct partition_meta_info info;
+	 *     } *parts;
+	 */
 	state->parts = vzalloc(array_size(nr, sizeof(state->parts[0])));
 	if (!state->parts) {
 		kfree(state);
@@ -139,12 +153,77 @@ void free_partitions(struct parsed_partitions *state)
 	kfree(state);
 }
 
+/*
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] loop_reread_partitions
+ * [0] loop_set_status
+ * [0] loop_set_status64
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - block/partition-generic.c|584| <<blk_add_partitions>> state = check_partition(disk, bdev);
+ */
 struct parsed_partitions *
 check_partition(struct gendisk *hd, struct block_device *bdev)
 {
 	struct parsed_partitions *state;
 	int i, res, err;
 
+	/*
+	 * 针对gendisk分配一个struct parsed_partitions
+	 * 主要要分配parsed_partitions->parts[]数组,用来临时保存part的元数据
+	 */
 	state = allocate_partitions(hd);
 	if (!state)
 		return NULL;
@@ -164,6 +243,26 @@ check_partition(struct gendisk *hd, struct block_device *bdev)
 	i = res = err = 0;
 	while (!res && check_part[i]) {
 		memset(state->parts, 0, state->limit * sizeof(state->parts[0]));
+		/*
+		 * struct parsed_partitions {
+		 *     struct block_device *bdev;
+		 *     char name[BDEVNAME_SIZE];
+		 *     struct {
+		 *         sector_t from;
+		 *         sector_t size;
+		 *         int flags;
+		 *         bool has_info;
+		 *         struct partition_meta_info info;
+		 *     } *parts;
+		 *     int next;
+		 *     int limit;
+		 *     bool access_beyond_eod;
+		 *     char *pp_buf;
+		 * };
+		 *
+		 * 用virtio-blk在ubuntu的启动盘时, 是adfspart_check_POWERTEC()
+		 * 其他的也是这个, 只是其他的可能没有part, 所以res不是>0
+		 */
 		res = check_part[i++](state);
 		if (res < 0) {
 			/* We have hit an I/O error which we don't report now.
diff --git a/drivers/ata/libata-scsi.c b/drivers/ata/libata-scsi.c
index 58e09ffe8b9c..522921903e21 100644
--- a/drivers/ata/libata-scsi.c
+++ b/drivers/ata/libata-scsi.c
@@ -2017,6 +2017,9 @@ static int ata_scsi_translate(struct ata_device *dev, struct scsi_cmnd *cmd,
 	if (xlat_func(qc))
 		goto early_finish;
 
+	/*
+	 * 一个例子ata_std_qc_defer()
+	 */
 	if (ap->ops->qc_defer) {
 		if ((rc = ap->ops->qc_defer(qc)))
 			goto defer;
diff --git a/drivers/block/loop.c b/drivers/block/loop.c
index 739b372a5112..423bba5bcbc0 100644
--- a/drivers/block/loop.c
+++ b/drivers/block/loop.c
@@ -167,6 +167,12 @@ static loff_t get_loop_size(struct loop_device *lo, struct file *file)
 	return get_size(lo->lo_offset, lo->lo_sizelimit, file);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|634| <<loop_update_dio>> __loop_update_dio(lo, io_is_direct(lo->lo_backing_file) |
+ *   - drivers/block/loop.c|1356| <<loop_set_status>> __loop_update_dio(lo, lo->use_dio);
+ *   - drivers/block/loop.c|1539| <<loop_set_dio>> __loop_update_dio(lo, !!arg);
+ */
 static void __loop_update_dio(struct loop_device *lo, bool dio)
 {
 	struct file *file = lo->lo_backing_file;
@@ -453,6 +459,9 @@ static int lo_req_flush(struct loop_device *lo, struct request *rq)
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops loop_mq_ops.complete = lo_complete_rq()
+ */
 static void lo_complete_rq(struct request *rq)
 {
 	struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
@@ -599,6 +608,19 @@ static int do_req_filebacked(struct loop_device *lo, struct request *rq)
 	case REQ_OP_FLUSH:
 		return lo_req_flush(lo, rq);
 	case REQ_OP_WRITE_ZEROES:
+		/*
+		 * FALLOC_FL_ZERO_RANGE的注释:
+		 * FALLOC_FL_ZERO_RANGE is used to convert a range of file to zeros preferably
+		 * without issuing data IO. Blocks should be preallocated for the regions that
+		 * span holes in the file, and the entire range is preferable converted to
+		 * unwritten extents - even though file system may choose to zero out the
+		 * extent or do whatever which will result in reading zeros from the range
+		 * while the range remains allocated for the file.
+		 *
+		 * This can be also used to preallocate blocks past EOF in the same way as
+		 * with fallocate. Flag FALLOC_FL_KEEP_SIZE should cause the inode
+		 * size to remain the same
+		 */
 		/*
 		 * If the caller doesn't want deallocation, call zeroout to
 		 * write zeroes the range.  Otherwise, punch them out.
@@ -635,6 +657,21 @@ static inline void loop_update_dio(struct loop_device *lo)
 			lo->use_dio);
 }
 
+/*
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] loop_reread_partitions
+ * [0] loop_set_status
+ * [0] loop_set_status64
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void loop_reread_partitions(struct loop_device *lo,
 				   struct block_device *bdev)
 {
@@ -1205,6 +1242,10 @@ static int __loop_clr_fd(struct loop_device *lo, bool release)
 	return err;
 }
 
+/*
+ * called by (处理LOOP_CLR_FD):
+ *   - drivers/block/loop.c|1621| <<lo_ioctl>> return loop_clr_fd(lo);
+ */
 static int loop_clr_fd(struct loop_device *lo)
 {
 	int err;
@@ -1237,6 +1278,12 @@ static int loop_clr_fd(struct loop_device *lo)
 	return __loop_clr_fd(lo, false);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|1491| <<loop_set_status_old>> return loop_set_status(lo, &info64);
+ *   - drivers/block/loop.c|1501| <<loop_set_status64>> return loop_set_status(lo, &info64);
+ *   - drivers/block/loop.c|1771| <<loop_set_status_compat>> return loop_set_status(lo, &info64);
+ */
 static int
 loop_set_status(struct loop_device *lo, const struct loop_info64 *info)
 {
@@ -1515,6 +1562,10 @@ static int loop_set_capacity(struct loop_device *lo)
 	return figure_loop_size(lo, lo->lo_offset, lo->lo_sizelimit);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|1597| <<lo_simple_ioctl>> err = loop_set_dio(lo, arg);
+ */
 static int loop_set_dio(struct loop_device *lo, unsigned long arg)
 {
 	int error = -ENXIO;
@@ -1591,6 +1642,9 @@ static int lo_simple_ioctl(struct loop_device *lo, unsigned int cmd,
 	return err;
 }
 
+/*
+ * struct block_device_operations lo_fops.ioctl = lo_ioctl()
+ */
 static int lo_ioctl(struct block_device *bdev, fmode_t mode,
 	unsigned int cmd, unsigned long arg)
 {
@@ -1754,6 +1808,9 @@ loop_get_status_compat(struct loop_device *lo,
 	return err;
 }
 
+/*
+ * struct block_device_operations lo_fops.compat_ioctl = lo_compat_ioctl()
+ */
 static int lo_compat_ioctl(struct block_device *bdev, fmode_t mode,
 			   unsigned int cmd, unsigned long arg)
 {
@@ -1789,6 +1846,9 @@ static int lo_compat_ioctl(struct block_device *bdev, fmode_t mode,
 }
 #endif
 
+/*
+ * struct block_device_operations lo_fops.open = lo_open()
+ */
 static int lo_open(struct block_device *bdev, fmode_t mode)
 {
 	struct loop_device *lo;
@@ -1809,6 +1869,9 @@ static int lo_open(struct block_device *bdev, fmode_t mode)
 	return err;
 }
 
+/*
+ * struct block_device_operations lo_fops.release = lo_release()
+ */
 static void lo_release(struct gendisk *disk, fmode_t mode)
 {
 	struct loop_device *lo;
@@ -1842,6 +1905,10 @@ static void lo_release(struct gendisk *disk, fmode_t mode)
 	mutex_unlock(&loop_ctl_mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2085| <<loop_add>> disk->fops = &lo_fops;
+ */
 static const struct block_device_operations lo_fops = {
 	.owner =	THIS_MODULE,
 	.open =		lo_open,
@@ -1863,6 +1930,10 @@ MODULE_PARM_DESC(max_part, "Maximum number of partitions per loop device");
 MODULE_LICENSE("GPL");
 MODULE_ALIAS_BLOCKDEV_MAJOR(LOOP_MAJOR);
 
+/*
+ * called by:
+ *   - drivers/block/cryptoloop.c|188| <<init_cryptoloop>> int rc = loop_register_transfer(&cryptoloop_funcs);
+ */
 int loop_register_transfer(struct loop_func_table *funcs)
 {
 	unsigned int n = funcs->number;
@@ -1901,6 +1972,9 @@ int loop_unregister_transfer(int number)
 EXPORT_SYMBOL(loop_register_transfer);
 EXPORT_SYMBOL(loop_unregister_transfer);
 
+/*
+ * struct blk_mq_ops loop_mq_ops.queue_rq = loop_queue_rq()
+ */
 static blk_status_t loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 		const struct blk_mq_queue_data *bd)
 {
@@ -1937,6 +2011,10 @@ static blk_status_t loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2011| <<loop_queue_work>> loop_handle_cmd(cmd);
+ */
 static void loop_handle_cmd(struct loop_cmd *cmd)
 {
 	struct request *rq = blk_mq_rq_from_pdu(cmd);
@@ -1958,6 +2036,10 @@ static void loop_handle_cmd(struct loop_cmd *cmd)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2019| <<loop_init_request>> kthread_init_work(&cmd->work, loop_queue_work);
+ */
 static void loop_queue_work(struct kthread_work *work)
 {
 	struct loop_cmd *cmd =
@@ -1966,6 +2048,9 @@ static void loop_queue_work(struct kthread_work *work)
 	loop_handle_cmd(cmd);
 }
 
+/*
+ * struct blk_mq_ops loop_mq_ops.init_request = loop_init_request()
+ */
 static int loop_init_request(struct blk_mq_tag_set *set, struct request *rq,
 		unsigned int hctx_idx, unsigned int numa_node)
 {
@@ -1975,12 +2060,23 @@ static int loop_init_request(struct blk_mq_tag_set *set, struct request *rq,
 	return 0;
 }
 
+/*
+ * used by:
+ *   - drivers/block/loop.c|2025| <<loop_add>> lo->tag_set.ops = &loop_mq_ops;
+ */
 static const struct blk_mq_ops loop_mq_ops = {
 	.queue_rq       = loop_queue_rq,
 	.init_request	= loop_init_request,
 	.complete	= lo_complete_rq,
 };
 
+/*
+ * called by:
+ *   - drivers/block/loop.c|2161| <<loop_probe>> err = loop_add(&lo, MINOR(dev) >> part_shift);
+ *   - drivers/block/loop.c|2190| <<loop_control_ioctl>> ret = loop_add(&lo, parm);
+ *   - drivers/block/loop.c|2212| <<loop_control_ioctl>> ret = loop_add(&lo, -1);
+ *   - drivers/block/loop.c|2300| <<loop_init>> loop_add(&lo, i);
+ */
 static int loop_add(struct loop_device **l, int i)
 {
 	struct loop_device *lo;
@@ -2154,6 +2250,10 @@ static struct kobject *loop_probe(dev_t dev, int *part, void *data)
 	return kobj;
 }
 
+/*
+ * struct file_operations loop_ctl_fops.unlocked_ioctl = loop_control_ioctl()
+ * struct file_operations loop_ctl_fops.compat_ioctl = loop_control_ioctl()
+ */
 static long loop_control_ioctl(struct file *file, unsigned int cmd,
 			       unsigned long parm)
 {
diff --git a/drivers/block/loop.h b/drivers/block/loop.h
index af75a5ee4094..d68889eac7ea 100644
--- a/drivers/block/loop.h
+++ b/drivers/block/loop.h
@@ -20,6 +20,21 @@
 /* Possible states of device */
 enum {
 	Lo_unbound,
+	/*
+	 * 使用Lo_bound的地方:
+	 *   - drivers/block/loop.c|692| <<loop_validate_file>> if (l->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|721| <<loop_change_fd>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1051| <<loop_set_fd>> lo->lo_state = Lo_bound;
+	 *   - drivers/block/loop.c|1240| <<loop_clr_fd>> if (lo->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|1283| <<loop_set_status>> if (lo->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|1396| <<loop_get_status>> if (lo->lo_state != Lo_bound) {
+	 *   - drivers/block/loop.c|1537| <<loop_set_capacity>> if (unlikely(lo->lo_state != Lo_bound))
+	 *   - drivers/block/loop.c|1550| <<loop_set_dio>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1565| <<loop_set_block_size>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1863| <<lo_release>> if (lo->lo_state != Lo_bound)
+	 *   - drivers/block/loop.c|1873| <<lo_release>> } else if (lo->lo_state == Lo_bound) {
+	 *   - drivers/block/loop.c|1958| <<loop_queue_rq>> if (lo->lo_state != Lo_bound)
+	 */
 	Lo_bound,
 	Lo_rundown,
 };
diff --git a/drivers/block/null_blk.h b/drivers/block/null_blk.h
index bc837862b767..3b859f1975ef 100644
--- a/drivers/block/null_blk.h
+++ b/drivers/block/null_blk.h
@@ -14,24 +14,61 @@
 #include <linux/fault-inject.h>
 
 struct nullb_cmd {
+	/*
+	 * list没人用, 可以删了??
+	 *   - drivers/block/null_blk_main.c|2440| <<setup_commands>> INIT_LIST_HEAD(&cmd->list);
+	 */
 	struct list_head list;
+	/*
+	 * ll_list没人用, 可以删了??
+	 *   - drivers/block/null_blk_main.c|2441| <<setup_commands>> cmd->ll_list.next = NULL;
+	 */
 	struct llist_node ll_list;
+	/*
+	 * csd没人用, 可以删除了
+	 */
 	struct __call_single_data csd;
 	struct request *rq;
 	struct bio *bio;
 	unsigned int tag;
 	blk_status_t error;
 	struct nullb_queue *nq;
+	/*
+	 * 表示complete req的方式, 比方不用softirq, 而用一个timer触发
+	 */
 	struct hrtimer timer;
 };
 
 struct nullb_queue {
+	/*
+	 * 在以下使用tag_map:
+	 *   - drivers/block/null_blk_main.c|922| <<put_tag>> clear_bit_unlock(tag, nq->tag_map);
+	 *   - drivers/block/null_blk_main.c|946| <<get_tag>> tag = find_first_zero_bit(nq->tag_map, nq->queue_depth);
+	 *   - drivers/block/null_blk_main.c|949| <<get_tag>> } while (test_and_set_bit_lock(tag, nq->tag_map));
+	 *   - drivers/block/null_blk_main.c|2292| <<cleanup_queue>> kfree(nq->tag_map);
+	 *   - drivers/block/null_blk_main.c|2420| <<setup_commands>> nq->tag_map = kcalloc(tag_size, sizeof(unsigned long ), GFP_KERNEL);
+	 *   - drivers/block/null_blk_main.c|2421| <<setup_commands>> if (!nq->tag_map) {
+	 */
 	unsigned long *tag_map;
 	wait_queue_head_t wait;
 	unsigned int queue_depth;
 	struct nullb_device *dev;
+	/*
+	 * 在以下使用requeue_selection:
+	 *   - drivers/block/null_blk_main.c|2258| <<null_queue_rq>> nq->requeue_selection++;
+	 *   - drivers/block/null_blk_main.c|2259| <<null_queue_rq>> if (nq->requeue_selection & 1)
+	 */
 	unsigned int requeue_selection;
 
+	/*
+	 * 在以下使用cmds:
+	 *   - drivers/block/null_blk_main.c|1003| <<__alloc_cmd>> cmd = &nq->cmds[tag];
+	 *   - drivers/block/null_blk_main.c|2305| <<cleanup_queue>> kfree(nq->cmds);
+	 *   - drivers/block/null_blk_main.c|2427| <<setup_commands>> nq->cmds = kcalloc(nq->queue_depth, sizeof(*cmd), GFP_KERNEL);
+	 *   - drivers/block/null_blk_main.c|2428| <<setup_commands>> if (!nq->cmds)
+	 *   - drivers/block/null_blk_main.c|2434| <<setup_commands>> kfree(nq->cmds);
+	 *   - drivers/block/null_blk_main.c|2439| <<setup_commands>> cmd = &nq->cmds[i];
+	 */
 	struct nullb_cmd *cmds;
 };
 
@@ -41,14 +78,40 @@ struct nullb_device {
 	struct radix_tree_root data; /* data stored in the disk */
 	struct radix_tree_root cache; /* disk cache data */
 	unsigned long flags; /* device flags */
+	/*
+	 * 使用curr_cache的地方:
+	 *   - drivers/block/null_blk_main.c|1282| <<null_free_sector>> nullb->dev->curr_cache -= PAGE_SIZE;
+	 *   - drivers/block/null_blk_main.c|1316| <<null_radix_tree_insert>> nullb->dev->curr_cache += PAGE_SIZE;
+	 *   - drivers/block/null_blk_main.c|1361| <<null_free_device_storage>> dev->curr_cache = 0;
+	 *   - drivers/block/null_blk_main.c|1530| <<null_flush_cache_page>> nullb->dev->curr_cache -= PAGE_SIZE;
+	 *   - drivers/block/null_blk_main.c|1550| <<null_make_cache_space>> nullb->dev->curr_cache + n || nullb->dev->curr_cache == 0)
+	 *   - drivers/block/null_blk_main.c|1779| <<null_handle_flush>> if (err || nullb->dev->curr_cache == 0)
+	 */
 	unsigned int curr_cache;
 	struct badblocks badblocks;
 
 	unsigned int nr_zones;
 	struct blk_zone *zones;
+	/*
+	 * 在以下使用zone_size_sects:
+	 *   - drivers/block/null_blk_main.c|1946| <<null_gendisk_register>> nullb->dev->zone_size_sects);
+	 *   - drivers/block/null_blk_zoned.c|21| <<null_zone_no>> return sect >> ilog2(dev->zone_size_sects);
+	 *   - drivers/block/null_blk_zoned.c|39| <<null_zone_init>> dev->zone_size_sects = dev->zone_size << ZONE_SIZE_SHIFT;
+	 *   - drivers/block/null_blk_zoned.c|41| <<null_zone_init>> (SECTOR_SHIFT + ilog2(dev->zone_size_sects));
+	 *   - drivers/block/null_blk_zoned.c|57| <<null_zone_init>> zone->len = dev->zone_size_sects;
+	 *   - drivers/block/null_blk_zoned.c|62| <<null_zone_init>> sector += dev->zone_size_sects;
+	 *   - drivers/block/null_blk_zoned.c|69| <<null_zone_init>> zone->len = dev->zone_size_sects;
+	 *   - drivers/block/null_blk_zoned.c|73| <<null_zone_init>> sector += dev->zone_size_sects;
+	 */
 	sector_t zone_size_sects;
 
 	unsigned long size; /* device size in MB */
+	/*
+	 * 在以下使用completion_nsec:
+	 *   - drivers/block/null_blk_main.c|452| <<global>> NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
+	 *   - drivers/block/null_blk_main.c|715| <<null_alloc_dev>> dev->completion_nsec = g_completion_nsec;
+	 *   - drivers/block/null_blk_main.c|930| <<null_cmd_end_timer>> ktime_t kt = cmd->nq->dev->completion_nsec;
+	 */
 	unsigned long completion_nsec; /* time in ns to complete a request */
 	unsigned long cache_size; /* disk cache size in MB */
 	unsigned long zone_size; /* zone size in MB if device is zoned */
@@ -62,8 +125,19 @@ struct nullb_device {
 	unsigned int index; /* index of the disk, only valid with a disk */
 	unsigned int mbps; /* Bandwidth throttle cap (in MB/s) */
 	bool blocking; /* blocking blk-mq device */
+	/*
+	 * 在以下使用null_device->use_per_mpde_hctx:
+	 *   - drivers/block/null_blk_main.c|671| <<null_alloc_dev>> dev->use_per_node_hctx = g_use_per_node_hctx;
+	 *   - drivers/block/null_blk_main.c|2083| <<null_validate_conf>> if (dev->queue_mode == NULL_Q_MQ && dev->use_per_node_hctx) {
+	 */
 	bool use_per_node_hctx; /* use per-node allocation for hardware context */
 	bool power; /* power on/off the device */
+	/*
+	 * 在以下使用memory_backed:
+	 *   - drivers/block/null_blk_main.c|338| <<global>> NULLB_DEVICE_ATTR(memory_backed, bool, NULL);
+	 *   - drivers/block/null_blk_main.c|1554| <<null_handle_cmd>> if (dev->memory_backed)
+	 *   - drivers/block/null_blk_main.c|2023| <<null_validate_conf>> if (dev->memory_backed)
+	 */
 	bool memory_backed; /* if data is stored in memory */
 	bool discard; /* if support discard */
 	bool zoned; /* if device is zoned */
@@ -71,6 +145,7 @@ struct nullb_device {
 
 struct nullb {
 	struct nullb_device *dev;
+	/* 把这个设置加入到nullb_list */
 	struct list_head list;
 	unsigned int index;
 	struct request_queue *q;
@@ -78,8 +153,34 @@ struct nullb {
 	struct blk_mq_tag_set *tag_set;
 	struct blk_mq_tag_set __tag_set;
 	unsigned int queue_depth;
+	/*
+	 * 使用cur_bytes的地方:
+	 *   - drivers/block/null_blk_main.c|1964| <<null_handle_throttled>> if (atomic_long_sub_return(blk_rq_bytes(rq), &nullb->cur_bytes) < 0) {
+	 *   - drivers/block/null_blk_main.c|1967| <<null_handle_throttled>> if (atomic_long_read(&nullb->cur_bytes) > 0)
+	 *   - drivers/block/null_blk_main.c|2119| <<nullb_bwtimer_fn>> if (atomic_long_read(&nullb->cur_bytes) == mb_per_tick(mbps))
+	 *   - drivers/block/null_blk_main.c|2122| <<nullb_bwtimer_fn>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(mbps));
+	 *   - drivers/block/null_blk_main.c|2140| <<nullb_setup_bwtimer>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(nullb->dev->mbps));
+	 *   - drivers/block/null_blk_main.c|2330| <<null_del_dev>> atomic_long_set(&nullb->cur_bytes, LONG_MAX);
+	 */
 	atomic_long_t cur_bytes;
+	/*
+	 * bw_timer在以下使用:
+	 *   - drivers/block/null_blk_main.c|1961| <<null_handle_throttled>> if (!hrtimer_active(&nullb->bw_timer))
+	 *   - drivers/block/null_blk_main.c|1962| <<null_handle_throttled>> hrtimer_restart(&nullb->bw_timer);
+	 *   - drivers/block/null_blk_main.c|2115| <<nullb_bwtimer_fn>> struct nullb *nullb = container_of(timer, struct nullb, bw_timer);
+	 *   - drivers/block/null_blk_main.c|2125| <<nullb_bwtimer_fn>> hrtimer_forward_now(&nullb->bw_timer, timer_interval);
+	 *   - drivers/block/null_blk_main.c|2138| <<nullb_setup_bwtimer>> hrtimer_init(&nullb->bw_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	 *   - drivers/block/null_blk_main.c|2139| <<nullb_setup_bwtimer>> nullb->bw_timer.function = nullb_bwtimer_fn;
+	 *   - drivers/block/null_blk_main.c|2141| <<nullb_setup_bwtimer>> hrtimer_start(&nullb->bw_timer, timer_interval, HRTIMER_MODE_REL);
+	 *   - drivers/block/null_blk_main.c|2329| <<null_del_dev>> hrtimer_cancel(&nullb->bw_timer);
+	 */
 	struct hrtimer bw_timer;
+	/*
+	 * 在以下使用cache_flush_pos:
+	 *   - drivers/block/null_blk_main.c|1551| <<null_make_cache_space>> (void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
+	 *   - drivers/block/null_blk_main.c|1557| <<null_make_cache_space>> nullb->cache_flush_pos = c_pages[i]->page->index;
+	 *   - drivers/block/null_blk_main.c|1581| <<null_make_cache_space>> nullb->cache_flush_pos = 0;
+	 */
 	unsigned long cache_flush_pos;
 	spinlock_t lock;
 
diff --git a/drivers/block/null_blk_main.c b/drivers/block/null_blk_main.c
index ae8d4bc532b0..c4cc593d8253 100644
--- a/drivers/block/null_blk_main.c
+++ b/drivers/block/null_blk_main.c
@@ -11,13 +11,34 @@
 #include <linux/init.h>
 #include "null_blk.h"
 
+/*
+ * null_transfer()是比较核心的函数
+ */
+
+/* 这个shift表示一个page里sector的数量 */
 #define PAGE_SECTORS_SHIFT	(PAGE_SHIFT - SECTOR_SHIFT)
+/* 这个表示一个page里sector的数量 */
 #define PAGE_SECTORS		(1 << PAGE_SECTORS_SHIFT)
 #define SECTOR_MASK		(PAGE_SECTORS - 1)
 
+/*
+ * 在以下使用FREE_BATCH:
+ *   - drivers/block/null_blk_main.c|1014| <<null_free_device_storage>> struct nullb_page *ret, *t_pages[FREE_BATCH];
+ *   - drivers/block/null_blk_main.c|1023| <<null_free_device_storage>> (void **)t_pages, pos, FREE_BATCH);
+ *   - drivers/block/null_blk_main.c|1033| <<null_free_device_storage>> } while (nr_pages == FREE_BATCH);
+ *   - drivers/block/null_blk_main.c|1184| <<null_make_cache_space>> struct nullb_page *c_pages[FREE_BATCH];
+ *   - drivers/block/null_blk_main.c|1193| <<null_make_cache_space>> (void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
+ */
 #define FREE_BATCH		16
 
 #define TICKS_PER_SEC		50ULL
+/*
+ * 在以下使用TIMER_INTERVAL:
+ *   - drivers/block/null_blk_main.c|1664| <<nullb_bwtimer_fn>> ktime_t timer_interval = ktime_set(0, TIMER_INTERVAL);
+ *   - drivers/block/null_blk_main.c|1684| <<nullb_setup_bwtimer>> ktime_t timer_interval = ktime_set(0, TIMER_INTERVAL);
+ *
+ * 每个tick的nsec
+ */
 #define TIMER_INTERVAL		(NSEC_PER_SEC / TICKS_PER_SEC)
 
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -25,8 +46,17 @@ static DECLARE_FAULT_ATTR(null_timeout_attr);
 static DECLARE_FAULT_ATTR(null_requeue_attr);
 #endif
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1605| <<nullb_bwtimer_fn>> if (atomic_long_read(&nullb->cur_bytes) == mb_per_tick(mbps))
+ *   - drivers/block/null_blk_main.c|1608| <<nullb_bwtimer_fn>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(mbps));
+ *   - drivers/block/null_blk_main.c|1626| <<nullb_setup_bwtimer>> atomic_long_set(&nullb->cur_bytes, mb_per_tick(nullb->dev->mbps));
+ */
 static inline u64 mb_per_tick(int mbps)
 {
+	/*
+	 * 1 << 20 相当于1MB
+	 */
 	return (1 << 20) / TICKS_PER_SEC * ((u64) mbps);
 }
 
@@ -39,12 +69,43 @@ static inline u64 mb_per_tick(int mbps)
  * CACHE:	Device is using a write-back cache.
  */
 enum nullb_device_flags {
+	/*
+	 * 在以下使用NULLB_DEV_FL_CONFIGURED:
+	 *   - drivers/block/null_blk_main.c|304| <<NULLB_DEVICE_ATTR>> else if (test_bit(NULLB_DEV_FL_CONFIGURED, &dev->flags)) \
+	 *   - drivers/block/null_blk_main.c|376| <<nullb_device_power_store>> set_bit(NULLB_DEV_FL_CONFIGURED, &dev->flags);
+	 *   - drivers/block/null_blk_main.c|385| <<nullb_device_power_store>> clear_bit(NULLB_DEV_FL_CONFIGURED, &dev->flags);
+	 */
 	NULLB_DEV_FL_CONFIGURED	= 0,
+	/*
+	 * 在以下使用NULLB_DEV_FL_UP:
+	 *   - drivers/block/null_blk_main.c|369| <<nullb_device_power_store>> if (test_and_set_bit(NULLB_DEV_FL_UP, &dev->flags))
+	 *   - drivers/block/null_blk_main.c|372| <<nullb_device_power_store>> clear_bit(NULLB_DEV_FL_UP, &dev->flags);
+	 *   - drivers/block/null_blk_main.c|379| <<nullb_device_power_store>> if (test_and_clear_bit(NULLB_DEV_FL_UP, &dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|513| <<nullb_group_drop_item>> if (test_and_clear_bit(NULLB_DEV_FL_UP, &dev->flags)) {
+	 */
 	NULLB_DEV_FL_UP		= 1,
+	/*
+	 * 在以下使用NULLB_DEV_FL_THROTTLED:
+	 *   - drivers/block/null_blk_main.c|1543| <<null_handle_cmd>> if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|1772| <<null_del_dev>> if (test_bit(NULLB_DEV_FL_THROTTLED, &nullb->dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|2156| <<null_add_dev>> set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
+	 */
 	NULLB_DEV_FL_THROTTLED	= 2,
+	/*
+	 * 在以下使用NULLB_DEV_FL_CACHE:
+	 *   - drivers/block/null_blk_main.c|574| <<null_cache_active>> return test_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 *   - drivers/block/null_blk_main.c|2161| <<null_add_dev>> set_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 */
 	NULLB_DEV_FL_CACHE	= 3,
 };
 
+/*
+ * 一个page中sector的数量加上2
+ * The highest 2 bits of bitmap are for special purpose. LOCK means the cache
+ * page is being flushing to storage. FREE means the cache page is freed and
+ * should be skipped from flushing to storage. Please see
+ * null_make_cache_space
+ */
 #define MAP_SZ		((PAGE_SIZE >> SECTOR_SHIFT) + 2)
 /*
  * nullb_page is a page in memory for nullb devices.
@@ -62,18 +123,72 @@ struct nullb_page {
 	struct page *page;
 	DECLARE_BITMAP(bitmap, MAP_SZ);
 };
+/*
+ * 在以下使用NULLB_PAGE_LOCK:
+ *   - drivers/block/null_blk_main.c|1151| <<null_free_page>> if (test_bit(NULLB_PAGE_LOCK, t_page->bitmap))
+ *   - drivers/block/null_blk_main.c|1356| <<null_flush_cache_page>> __clear_bit(NULLB_PAGE_LOCK, c_page->bitmap);
+ *   - drivers/block/null_blk_main.c|1421| <<null_make_cache_space>> if (test_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap))
+ *   - drivers/block/null_blk_main.c|1424| <<null_make_cache_space>> __set_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap);
+ */
 #define NULLB_PAGE_LOCK (MAP_SZ - 1)
+/*
+ * 在以下使用NULLB_PAGE_FREE:
+ *   - drivers/block/null_blk_main.c|1150| <<null_free_page>> __set_bit(NULLB_PAGE_FREE, t_page->bitmap);
+ *   - drivers/block/null_blk_main.c|1357| <<null_flush_cache_page>> if (test_bit(NULLB_PAGE_FREE, c_page->bitmap)) {
+ */
 #define NULLB_PAGE_FREE (MAP_SZ - 2)
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1786| <<null_add_dev>> list_add_tail(&nullb->list, &nullb_list);
+ *   - drivers/block/null_blk_main.c|1889| <<null_init>> while (!list_empty(&nullb_list)) {
+ *   - drivers/block/null_blk_main.c|1890| <<null_init>> nullb = list_entry(nullb_list.next, struct nullb, list);
+ *   - drivers/block/null_blk_main.c|1913| <<null_exit>> while (!list_empty(&nullb_list)) {
+ *   - drivers/block/null_blk_main.c|1916| <<null_exit>> nullb = list_entry(nullb_list.next, struct nullb, list);
+ *
+ * 添加struct nullb
+ */
 static LIST_HEAD(nullb_list);
 static struct mutex lock;
+/*
+ * 使用null_major的地方:
+ *   - drivers/block/null_blk_main.c|1918| <<null_gendisk_register>> disk->major = null_major;
+ *   - drivers/block/null_blk_main.c|2252| <<null_init>> null_major = register_blkdev(0, "nullb");
+ *   - drivers/block/null_blk_main.c|2253| <<null_init>> if (null_major < 0) {
+ *   - drivers/block/null_blk_main.c|2254| <<null_init>> ret = null_major;
+ *   - drivers/block/null_blk_main.c|2281| <<null_init>> unregister_blkdev(null_major, "nullb");
+ *   - drivers/block/null_blk_main.c|2296| <<null_exit>> unregister_blkdev(null_major, "nullb");
+ */
 static int null_major;
+/*
+ * 在以下使用nullb_indexes:
+ *   - drivers/block/null_blk_main.c|1790| <<null_del_dev>> ida_simple_remove(&nullb_indexes, nullb->index);
+ *   - drivers/block/null_blk_main.c|2205| <<null_add_dev>> nullb->index = ida_simple_get(&nullb_indexes, 0, 0, GFP_KERNEL);
+ */
 static DEFINE_IDA(nullb_indexes);
 static struct blk_mq_tag_set tag_set;
 
 enum {
+	/*
+	 * 使用NULL_IRQ_NONE的地方:
+	 *   - drivers/block/null_blk_main.c|297| <<null_set_irqmode>> return null_param_store_val(str, &g_irqmode, NULL_IRQ_NONE,
+	 *   - drivers/block/null_blk_main.c|1932| <<nullb_complete_cmd>> case NULL_IRQ_NONE:
+	 */
 	NULL_IRQ_NONE		= 0,
+	/*
+	 * 使用NULL_IRQ_SOFTIRQ的地方:
+	 *   - drivers/block/null_blk_main.c|293| <<global>> static int g_irqmode = NULL_IRQ_SOFTIRQ;
+	 *   - drivers/block/null_blk_main.c|1914| <<nullb_complete_cmd>> case NULL_IRQ_SOFTIRQ:
+	 */
 	NULL_IRQ_SOFTIRQ	= 1,
+	/*
+	 * 使用NULL_IRQ_TIMER的地方:
+	 *   - drivers/block/null_blk_main.c|298| <<null_set_irqmode>> NULL_IRQ_TIMER);
+	 *   - drivers/block/null_blk_main.c|982| <<__alloc_cmd>> if (nq->dev->irqmode == NULL_IRQ_TIMER) {
+	 *   - drivers/block/null_blk_main.c|1940| <<nullb_complete_cmd>> case NULL_IRQ_TIMER:
+	 *   - drivers/block/null_blk_main.c|2117| <<null_queue_rq>> if (nq->dev->irqmode == NULL_IRQ_TIMER) {
+	 *   - drivers/block/null_blk_main.c|2470| <<null_validate_conf>> dev->irqmode = min_t(unsigned int , dev->irqmode, NULL_IRQ_TIMER);
+	 */
 	NULL_IRQ_TIMER		= 2,
 };
 
@@ -83,10 +198,27 @@ enum {
 	NULL_Q_MQ		= 2,
 };
 
+/*
+ * 在以下使用g_no_sched:
+ *   - drivers/block/null_blk_main.c|128| <<global>> module_param_named(no_sched, g_no_sched, int , 0444);
+ *   - drivers/block/null_blk_main.c|2001| <<null_init_tag_set>> if (g_no_sched)
+ */
 static int g_no_sched;
 module_param_named(no_sched, g_no_sched, int, 0444);
 MODULE_PARM_DESC(no_sched, "No io scheduler");
 
+/*
+ * 在一下使用g_submit_queues:
+ *   - drivers/block/null_blk_main.c|132| <<global>> module_param_named(submit_queues, g_submit_queues, int , 0444);
+ *   - drivers/block/null_blk_main.c|622| <<null_alloc_dev>> dev->submit_queues = g_submit_queues;
+ *   - drivers/block/null_blk_main.c|1995| <<null_init_tag_set>> g_submit_queues;
+ *   - drivers/block/null_blk_main.c|2270| <<null_init>> if (g_submit_queues != nr_online_nodes) {
+ *   - drivers/block/null_blk_main.c|2273| <<null_init>> g_submit_queues = nr_online_nodes;
+ *   - drivers/block/null_blk_main.c|2275| <<null_init>> } else if (g_submit_queues > nr_cpu_ids)
+ *   - drivers/block/null_blk_main.c|2276| <<null_init>> g_submit_queues = nr_cpu_ids;
+ *   - drivers/block/null_blk_main.c|2277| <<null_init>> else if (g_submit_queues <= 0)
+ *   - drivers/block/null_blk_main.c|2278| <<null_init>> g_submit_queues = 1;
+ */
 static int g_submit_queues = 1;
 module_param_named(submit_queues, g_submit_queues, int, 0444);
 MODULE_PARM_DESC(submit_queues, "Number of submission queues");
@@ -96,15 +228,43 @@ module_param_named(home_node, g_home_node, int, 0444);
 MODULE_PARM_DESC(home_node, "Home node for the device");
 
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
+/*
+ * 在以下使用g_timeout_str:
+ *   - drivers/block/null_blk_main.c|173| <<global>> module_param_string(timeout, g_timeout_str, sizeof(g_timeout_str), 0444);
+ *   - drivers/block/null_blk_main.c|1732| <<should_timeout_request>> if (g_timeout_str[0])
+ *   - drivers/block/null_blk_main.c|2161| <<null_setup_fault>> if (!__null_setup_fault(&null_timeout_attr, g_timeout_str))
+ *
+ * 设置了"null_blk.timeout=20,100,0,5", 在null_setup_fault()初始化
+ * [  202.328984] null_blk: rq 000000009163ec16 timed out
+ * [  202.328993] null_blk: rq 00000000f9644e82 timed out
+ * [  202.328995] null_blk: rq 00000000d8e5c61a timed out
+ * [  202.328997] null_blk: rq 00000000fafa43fd timed out
+ * [  202.328999] null_blk: rq 00000000f0eb258c timed out
+ * 查看Documentation/fault-injection/fault-injection.rst
+ */
 static char g_timeout_str[80];
 module_param_string(timeout, g_timeout_str, sizeof(g_timeout_str), 0444);
 
+/*
+ * 在以下使用g_requeue_str:
+ *   - drivers/block/null_blk_main.c|176| <<global>> module_param_string(requeue, g_requeue_str, sizeof(g_requeue_str), 0444);
+ *   - drivers/block/null_blk_main.c|1745| <<should_requeue_request>> if (g_requeue_str[0])
+ *   - drivers/block/null_blk_main.c|2163| <<null_setup_fault>> if (!__null_setup_fault(&null_requeue_attr, g_requeue_str))
+ *
+ * 设置了"null_blk.requeue=20,100,0,5", 在null_setup_fault()初始化
+ * 查看Documentation/fault-injection/fault-injection.rst
+ */
 static char g_requeue_str[80];
 module_param_string(requeue, g_requeue_str, sizeof(g_requeue_str), 0444);
 #endif
 
 static int g_queue_mode = NULL_Q_MQ;
 
+/*
+ * 在以下使用null_param_store_val:
+ *   - drivers/block/null_blk_main.c|235| <<null_set_queue_mode>> return null_param_store_val(str, &g_queue_mode, NULL_Q_BIO, NULL_Q_MQ);
+ *   - drivers/block/null_blk_main.c|280| <<null_set_irqmode>> return null_param_store_val(str, &g_irqmode, NULL_IRQ_NONE,
+ */
 static int null_param_store_val(const char *str, int *val, int min, int max)
 {
 	int ret, new_val;
@@ -125,6 +285,10 @@ static int null_set_queue_mode(const char *str, const struct kernel_param *kp)
 	return null_param_store_val(str, &g_queue_mode, NULL_Q_BIO, NULL_Q_MQ);
 }
 
+/*
+ * 仅在以下使用null_queue_mode_param_ops:
+ *   - drivers/block/null_blk_main.c|206| <<global>> device_param_cb(queue_mode, &null_queue_mode_param_ops, &g_queue_mode, 0444);
+ */
 static const struct kernel_param_ops null_queue_mode_param_ops = {
 	.set	= null_set_queue_mode,
 	.get	= param_get_int,
@@ -145,6 +309,12 @@ static unsigned int nr_devices = 1;
 module_param(nr_devices, uint, 0444);
 MODULE_PARM_DESC(nr_devices, "Number of devices to register");
 
+/*
+ * 在以下使用g_blocking:
+ *   - drivers/block/null_blk_main.c|222| <<global>> module_param_named(blocking, g_blocking, bool, 0444);
+ *   - drivers/block/null_blk_main.c|660| <<null_alloc_dev>> dev->blocking = g_blocking;
+ *   - drivers/block/null_blk_main.c|2037| <<null_init_tag_set>> if ((nullb && nullb->dev->blocking) || g_blocking)
+ */
 static bool g_blocking;
 module_param_named(blocking, g_blocking, bool, 0444);
 MODULE_PARM_DESC(blocking, "Register as a blocking blk-mq driver device");
@@ -161,6 +331,10 @@ static int null_set_irqmode(const char *str, const struct kernel_param *kp)
 					NULL_IRQ_TIMER);
 }
 
+/*
+ * 在以下使用null_irqmode_param_ops:
+ *   - drivers/block/null_blk_main.c|242| <<global>> device_param_cb(irqmode, &null_irqmode_param_ops, &g_irqmode, 0444);
+ */
 static const struct kernel_param_ops null_irqmode_param_ops = {
 	.set	= null_set_irqmode,
 	.get	= param_get_int,
@@ -169,6 +343,11 @@ static const struct kernel_param_ops null_irqmode_param_ops = {
 device_param_cb(irqmode, &null_irqmode_param_ops, &g_irqmode, 0444);
 MODULE_PARM_DESC(irqmode, "IRQ completion handler. 0-none, 1-softirq, 2-timer");
 
+/*
+ * 在以下使用g_completion_nsec:
+ *   - drivers/block/null_blk_main.c|302| <<global>> module_param_named(completion_nsec, g_completion_nsec, ulong, 0444);
+ *   - drivers/block/null_blk_main.c|715| <<null_alloc_dev>> dev->completion_nsec = g_completion_nsec;
+ */
 static unsigned long g_completion_nsec = 10000;
 module_param_named(completion_nsec, g_completion_nsec, ulong, 0444);
 MODULE_PARM_DESC(completion_nsec, "Time in ns to complete a request in hardware. Default: 10,000ns");
@@ -177,6 +356,12 @@ static int g_hw_queue_depth = 64;
 module_param_named(hw_queue_depth, g_hw_queue_depth, int, 0444);
 MODULE_PARM_DESC(hw_queue_depth, "Queue depth for each hardware queue. Default: 64");
 
+/*
+ * 在以下使用g_use_per_node_hctx:
+ *   - drivers/block/null_blk_main.c|264| <<global>> module_param_named(use_per_node_hctx, g_use_per_node_hctx, bool, 0444);
+ *   - drivers/block/null_blk_main.c|671| <<null_alloc_dev>> dev->use_per_node_hctx = g_use_per_node_hctx;
+ *   - drivers/block/null_blk_main.c|2311| <<null_init>> if (g_queue_mode == NULL_Q_MQ && g_use_per_node_hctx) {
+ */
 static bool g_use_per_node_hctx;
 module_param_named(use_per_node_hctx, g_use_per_node_hctx, bool, 0444);
 MODULE_PARM_DESC(use_per_node_hctx, "Use per-node allocation for hardware context queues. Default: false");
@@ -189,6 +374,11 @@ static unsigned long g_zone_size = 256;
 module_param_named(zone_size, g_zone_size, ulong, S_IRUGO);
 MODULE_PARM_DESC(zone_size, "Zone size in MB when block device is zoned. Must be power-of-two: Default: 256");
 
+/*
+ * 在以下使用g_zone_nr_conv:
+ *   - drivers/block/null_blk_main.c|328| <<global>> module_param_named(zone_nr_conv, g_zone_nr_conv, uint, 0444);
+ *   - drivers/block/null_blk_main.c|726| <<null_alloc_dev>> dev->zone_nr_conv = g_zone_nr_conv;
+ */
 static unsigned int g_zone_nr_conv;
 module_param_named(zone_nr_conv, g_zone_nr_conv, uint, 0444);
 MODULE_PARM_DESC(zone_nr_conv, "Number of conventional zones when block device is zoned. Default: 0");
@@ -204,22 +394,26 @@ static inline struct nullb_device *to_nullb_device(struct config_item *item)
 	return item ? container_of(item, struct nullb_device, item) : NULL;
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的show, 类型uint */
 static inline ssize_t nullb_device_uint_attr_show(unsigned int val, char *page)
 {
 	return snprintf(page, PAGE_SIZE, "%u\n", val);
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的show, 类型ulong */
 static inline ssize_t nullb_device_ulong_attr_show(unsigned long val,
 	char *page)
 {
 	return snprintf(page, PAGE_SIZE, "%lu\n", val);
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的show, 类型bool */
 static inline ssize_t nullb_device_bool_attr_show(bool val, char *page)
 {
 	return snprintf(page, PAGE_SIZE, "%u\n", val);
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的store, 类型uint */
 static ssize_t nullb_device_uint_attr_store(unsigned int *val,
 	const char *page, size_t count)
 {
@@ -234,6 +428,7 @@ static ssize_t nullb_device_uint_attr_store(unsigned int *val,
 	return count;
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的store, 类型ulong */
 static ssize_t nullb_device_ulong_attr_store(unsigned long *val,
 	const char *page, size_t count)
 {
@@ -248,6 +443,7 @@ static ssize_t nullb_device_ulong_attr_store(unsigned long *val,
 	return count;
 }
 
+/* 用在下面的NULLB_DEVICE_ATTR()的store, 类型bool */
 static ssize_t nullb_device_bool_attr_store(bool *val, const char *page,
 	size_t count)
 {
@@ -263,6 +459,27 @@ static ssize_t nullb_device_bool_attr_store(bool *val, const char *page,
 }
 
 /* The following macro should only be used with TYPE = {uint, ulong, bool}. */
+/*
+ * 在以下使用NULLB_DEVICE_ATTR():
+ *   - drivers/block/null_blk_main.c|451| <<global>> NULLB_DEVICE_ATTR(size, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|452| <<global>> NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|453| <<global>> NULLB_DEVICE_ATTR(submit_queues, uint, nullb_apply_submit_queues);
+ *   - drivers/block/null_blk_main.c|454| <<global>> NULLB_DEVICE_ATTR(home_node, uint, NULL);
+ *   - drivers/block/null_blk_main.c|455| <<global>> NULLB_DEVICE_ATTR(queue_mode, uint, NULL);
+ *   - drivers/block/null_blk_main.c|456| <<global>> NULLB_DEVICE_ATTR(blocksize, uint, NULL);
+ *   - drivers/block/null_blk_main.c|457| <<global>> NULLB_DEVICE_ATTR(irqmode, uint, NULL);
+ *   - drivers/block/null_blk_main.c|458| <<global>> NULLB_DEVICE_ATTR(hw_queue_depth, uint, NULL);
+ *   - drivers/block/null_blk_main.c|459| <<global>> NULLB_DEVICE_ATTR(index, uint, NULL);
+ *   - drivers/block/null_blk_main.c|460| <<global>> NULLB_DEVICE_ATTR(blocking, bool, NULL);
+ *   - drivers/block/null_blk_main.c|461| <<global>> NULLB_DEVICE_ATTR(use_per_node_hctx, bool, NULL);
+ *   - drivers/block/null_blk_main.c|462| <<global>> NULLB_DEVICE_ATTR(memory_backed, bool, NULL);
+ *   - drivers/block/null_blk_main.c|463| <<global>> NULLB_DEVICE_ATTR(discard, bool, NULL);
+ *   - drivers/block/null_blk_main.c|464| <<global>> NULLB_DEVICE_ATTR(mbps, uint, NULL);
+ *   - drivers/block/null_blk_main.c|465| <<global>> NULLB_DEVICE_ATTR(cache_size, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|466| <<global>> NULLB_DEVICE_ATTR(zoned, bool, NULL);
+ *   - drivers/block/null_blk_main.c|467| <<global>> NULLB_DEVICE_ATTR(zone_size, ulong, NULL);
+ *   - drivers/block/null_blk_main.c|468| <<global>> NULLB_DEVICE_ATTR(zone_nr_conv, uint, NULL);
+ */
 #define NULLB_DEVICE_ATTR(NAME, TYPE, APPLY)					\
 static ssize_t									\
 nullb_device_##NAME##_show(struct config_item *item, char *page)		\
@@ -293,6 +510,12 @@ nullb_device_##NAME##_store(struct config_item *item, const char *page,		\
 }										\
 CONFIGFS_ATTR(nullb_device_, NAME);
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|329| <<global>> NULLB_DEVICE_ATTR(submit_queues, uint, nullb_apply_submit_queues);
+ *
+ * nullb_device_submit_queues_store()
+ */
 static int nullb_apply_submit_queues(struct nullb_device *dev,
 				     unsigned int submit_queues)
 {
@@ -307,6 +530,53 @@ static int nullb_apply_submit_queues(struct nullb_device *dev,
 	return set->nr_hw_queues == submit_queues ? 0 : -ENOMEM;
 }
 
+/*
+ * commit 3bf2bd20734e3e6ffda53719a9c10fb3ee9c5ffa
+ * Author: Shaohua Li <shli@fb.com>
+ * Date:   Mon Aug 14 15:04:53 2017 -0700
+ * 
+ * nullb: add configfs interface
+ *
+ * Add configfs interface for nullb. configfs interface is more flexible
+ * and easy to configure in a per-disk basis.
+ *
+ * Configuration is something like this:
+ * mount -t configfs none /mnt
+ *
+ * Checking which features the driver supports:
+ * cat /mnt/nullb/features
+ *
+ * The 'features' attribute is for future extension. We probably will add
+ * new features into the driver, userspace can check this attribute to find
+ * the supported features.
+ *
+ * Create/remove a device:
+ * mkdir/rmdir /mnt/nullb/a
+ *
+ * Then configure the device by setting attributes under /mnt/nullb/a, most
+ * of nullb supported module parameters are converted to attributes:
+ * size; // device size in MB
+ * completion_nsec; // time in ns to complete a request
+ * submit_queues; // number of submission queues
+ * home_node; // home node for the device
+ * queue_mode; // block interface
+ * blocksize; // block size
+ * irqmode; // IRQ completion handler
+ * hw_queue_depth; // queue depth
+ * use_lightnvm; // register as a LightNVM device
+ * blocking; // blocking blk-mq device
+ * use_per_node_hctx; // use per-node allocation for hardware context
+ *
+ * Note, creating a device doesn't create a disk immediately. Creating a
+ * disk is done in two phases: create a device and then power on the
+ * device. Next patch will introduce device power on.
+ *
+ * Based on original patch from Kyungchan Koh
+ *
+ * Signed-off-by: Kyungchan Koh <kkc6196@fb.com>
+ * Signed-off-by: Shaohua Li <shli@fb.com>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ */
 NULLB_DEVICE_ATTR(size, ulong, NULL);
 NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
 NULLB_DEVICE_ATTR(submit_queues, uint, nullb_apply_submit_queues);
@@ -326,11 +596,13 @@ NULLB_DEVICE_ATTR(zoned, bool, NULL);
 NULLB_DEVICE_ATTR(zone_size, ulong, NULL);
 NULLB_DEVICE_ATTR(zone_nr_conv, uint, NULL);
 
+/* 用在下面的CONFIGFS_ATTR(nullb_device_, power); */
 static ssize_t nullb_device_power_show(struct config_item *item, char *page)
 {
 	return nullb_device_bool_attr_show(to_nullb_device(item)->power, page);
 }
 
+/* 用在下面的CONFIGFS_ATTR(nullb_device_, power); */
 static ssize_t nullb_device_power_store(struct config_item *item,
 				     const char *page, size_t count)
 {
@@ -367,6 +639,7 @@ static ssize_t nullb_device_power_store(struct config_item *item,
 
 CONFIGFS_ATTR(nullb_device_, power);
 
+/* 用在下面的CONFIGFS_ATTR(nullb_device_, badblocks); */
 static ssize_t nullb_device_badblocks_show(struct config_item *item, char *page)
 {
 	struct nullb_device *t_dev = to_nullb_device(item);
@@ -374,6 +647,11 @@ static ssize_t nullb_device_badblocks_show(struct config_item *item, char *page)
 	return badblocks_show(&t_dev->badblocks, page, 0);
 }
 
+/*
+ * 用在下面的CONFIGFS_ATTR(nullb_device_, badblocks);
+ *
+ * 使用方式 "echo "+10240-20470" > badblocks", 代表起始和终止的vector
+ */
 static ssize_t nullb_device_badblocks_store(struct config_item *item,
 				     const char *page, size_t count)
 {
@@ -386,6 +664,7 @@ static ssize_t nullb_device_badblocks_store(struct config_item *item,
 	if (!orig)
 		return -ENOMEM;
 
+	/* Removes leading and trailing whitespace from @s. */
 	buf = strstrip(orig);
 
 	ret = -EINVAL;
@@ -406,6 +685,13 @@ static ssize_t nullb_device_badblocks_store(struct config_item *item,
 		goto out;
 	/* enable badblocks */
 	cmpxchg(&t_dev->badblocks.shift, -1, 0);
+	/*
+	 * badblocks_set() - Add a range of bad blocks to the table.
+	 * @bb:         the badblocks structure that holds all badblock information
+	 * @s:          first sector to mark as bad
+	 * @sectors:    number of sectors to mark as bad
+	 * @acknowledged: weather to mark the bad sectors as acknowledged
+	 */
 	if (buf[0] == '+')
 		ret = badblocks_set(&t_dev->badblocks, start,
 			end - start + 1, 1);
@@ -420,6 +706,11 @@ static ssize_t nullb_device_badblocks_store(struct config_item *item,
 }
 CONFIGFS_ATTR(nullb_device_, badblocks);
 
+/*
+ * struct config_item_type nullb_device_type.ct_attrs = nullb_device_attrs[]
+ *
+ * configfs中所有的参数!!!!
+ */
 static struct configfs_attribute *nullb_device_attrs[] = {
 	&nullb_device_attr_size,
 	&nullb_device_attr_completion_nsec,
@@ -444,6 +735,7 @@ static struct configfs_attribute *nullb_device_attrs[] = {
 	NULL,
 };
 
+/* struct configfs_item_operations nullb_device_ops.release = nullb_device_release() */
 static void nullb_device_release(struct config_item *item)
 {
 	struct nullb_device *dev = to_nullb_device(item);
@@ -452,16 +744,24 @@ static void nullb_device_release(struct config_item *item)
 	null_free_dev(dev);
 }
 
+/* struct config_item_type nullb_device_type.ct_item_ops = &nullb_device_ops */
 static struct configfs_item_operations nullb_device_ops = {
 	.release	= nullb_device_release,
 };
 
+/*
+ * 在以下使用nullb_device_type:
+ *   - drivers/block/null_blk_main.c|711| <<nullb_group_make_item>> config_item_init_type_name(&dev->item, name, &nullb_device_type);
+ */
 static const struct config_item_type nullb_device_type = {
 	.ct_item_ops	= &nullb_device_ops,
 	.ct_attrs	= nullb_device_attrs,
 	.ct_owner	= THIS_MODULE,
 };
 
+/*
+ * struct configfs_group_operations nullb_group_ops.make_item = nullb_group_make_item()
+ */
 static struct
 config_item *nullb_group_make_item(struct config_group *group, const char *name)
 {
@@ -476,6 +776,9 @@ config_item *nullb_group_make_item(struct config_group *group, const char *name)
 	return &dev->item;
 }
 
+/*
+ * struct configfs_group_operations nullb_group_ops.drop_item = nullb_group_drop_item()
+ */
 static void
 nullb_group_drop_item(struct config_group *group, struct config_item *item)
 {
@@ -514,6 +817,14 @@ static const struct config_item_type nullb_group_type = {
 	.ct_owner	= THIS_MODULE,
 };
 
+/*
+ * 使用nullb_subsys的地方:
+ *   - drivers/block/null_blk_main.c|2176| <<null_init>> config_group_init(&nullb_subsys.su_group);
+ *   - drivers/block/null_blk_main.c|2177| <<null_init>> mutex_init(&nullb_subsys.su_mutex);
+ *   - drivers/block/null_blk_main.c|2179| <<null_init>> ret = configfs_register_subsystem(&nullb_subsys);
+ *   - drivers/block/null_blk_main.c|2219| <<null_init>> configfs_unregister_subsystem(&nullb_subsys);
+ *   - drivers/block/null_blk_main.c|2230| <<null_exit>> configfs_unregister_subsystem(&nullb_subsys);
+ */
 static struct configfs_subsystem nullb_subsys = {
 	.su_group = {
 		.cg_item = {
@@ -523,11 +834,36 @@ static struct configfs_subsystem nullb_subsys = {
 	},
 };
 
+/*
+ * 上面都是configfs的, 下面是核心代码了!!!
+ */
+
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1098| <<copy_to_nullb>> if (null_cache_active(nullb) && !is_fua)
+ *   - drivers/block/null_blk_main.c|1103| <<copy_to_nullb>> !null_cache_active(nullb) || is_fua);
+ *   - drivers/block/null_blk_main.c|1141| <<copy_from_nullb>> !null_cache_active(nullb));
+ *   - drivers/block/null_blk_main.c|1187| <<null_handle_discard>> if (null_cache_active(nullb))
+ *   - drivers/block/null_blk_main.c|1203| <<null_handle_flush>> if (!null_cache_active(nullb))
+ *   - drivers/block/null_blk_main.c|1705| <<null_del_dev>> if (null_cache_active(nullb))
+ */
 static inline int null_cache_active(struct nullb *nullb)
 {
+	/*
+	 * 在以下使用NULLB_DEV_FL_CACHE:
+	 *   - drivers/block/null_blk_main.c|574| <<null_cache_active>> return test_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 *   - drivers/block/null_blk_main.c|2161| <<null_add_dev>> set_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+	 *
+	 * Device is using a write-back cache.
+	 */
 	return test_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|470| <<nullb_group_make_item>> dev = null_alloc_dev();
+ *   - drivers/block/null_blk_main.c|1852| <<null_init>> dev = null_alloc_dev();
+ */
 static struct nullb_device *null_alloc_dev(void)
 {
 	struct nullb_device *dev;
@@ -543,6 +879,12 @@ static struct nullb_device *null_alloc_dev(void)
 	}
 
 	dev->size = g_gb * 1024;
+	/*
+	 * 在以下使用completion_nsec:
+	 *   - drivers/block/null_blk_main.c|452| <<global>> NULLB_DEVICE_ATTR(completion_nsec, ulong, NULL);
+	 *   - drivers/block/null_blk_main.c|715| <<null_alloc_dev>> dev->completion_nsec = g_completion_nsec;
+	 *   - drivers/block/null_blk_main.c|930| <<null_cmd_end_timer>> ktime_t kt = cmd->nq->dev->completion_nsec;
+	 */
 	dev->completion_nsec = g_completion_nsec;
 	dev->submit_queues = g_submit_queues;
 	dev->home_node = g_home_node;
@@ -551,6 +893,11 @@ static struct nullb_device *null_alloc_dev(void)
 	dev->irqmode = g_irqmode;
 	dev->hw_queue_depth = g_hw_queue_depth;
 	dev->blocking = g_blocking;
+	/*
+	 * 在以下使用null_device->use_per_mpde_hctx:
+	 *   - drivers/block/null_blk_main.c|671| <<null_alloc_dev>> dev->use_per_node_hctx = g_use_per_node_hctx;
+	 *   - drivers/block/null_blk_main.c|2083| <<null_validate_conf>> if (dev->queue_mode == NULL_Q_MQ && dev->use_per_node_hctx) {
+	 */
 	dev->use_per_node_hctx = g_use_per_node_hctx;
 	dev->zoned = g_zoned;
 	dev->zone_size = g_zone_size;
@@ -558,6 +905,13 @@ static struct nullb_device *null_alloc_dev(void)
 	return dev;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|460| <<nullb_device_release>> null_free_dev(dev);
+ *   - drivers/block/null_blk_main.c|2202| <<null_init>> null_free_dev(dev);
+ *   - drivers/block/null_blk_main.c|2215| <<null_init>> null_free_dev(dev);
+ *   - drivers/block/null_blk_main.c|2241| <<null_exit>> null_free_dev(dev);
+ */
 static void null_free_dev(struct nullb_device *dev)
 {
 	if (!dev)
@@ -568,19 +922,50 @@ static void null_free_dev(struct nullb_device *dev)
 	kfree(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|622| <<free_cmd>> put_tag(cmd->nq, cmd->tag);
+ *
+ * end_cmd()
+ *  -> free_cmd()
+ *      -> put_tag()
+ *
+ * 清空nullb_queue->tag_map中tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */
 static void put_tag(struct nullb_queue *nq, unsigned int tag)
 {
+	/*
+	 * Clear a bit in memory, for unlock
+	 */
+	/*
+	 * struct nullb_queue:
+	 *  -> unsigned long *tag_map;
+	 */
 	clear_bit_unlock(tag, nq->tag_map);
 
+	/*
+	 * 在alloc_cmd()中可能等待在nq->wait
+	 */
 	if (waitqueue_active(&nq->wait))
 		wake_up(&nq->wait);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|622| <<__alloc_cmd>> tag = get_tag(nq);
+ *
+ * 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+ */
 static unsigned int get_tag(struct nullb_queue *nq)
 {
 	unsigned int tag;
 
 	do {
+		/*
+		 * Find the first cleared bit in a memory region.
+		 * 这里的memory region就是nq->tag_map
+		 */
 		tag = find_first_zero_bit(nq->tag_map, nq->queue_depth);
 		if (tag >= nq->queue_depth)
 			return -1U;
@@ -589,6 +974,17 @@ static unsigned int get_tag(struct nullb_queue *nq)
 	return tag;
 }
 
+/*
+ * called only by:
+ *   - drivers/block/null_blk_main.c|685| <<end_cmd>> free_cmd(cmd);
+ *
+ * end_cmd()
+ *  -> free_cmd()
+ *      -> put_tag()
+ *
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */ 
 static void free_cmd(struct nullb_cmd *cmd)
 {
 	put_tag(cmd->nq, cmd->tag);
@@ -596,13 +992,37 @@ static void free_cmd(struct nullb_cmd *cmd)
 
 static enum hrtimer_restart null_cmd_timer_expired(struct hrtimer *timer);
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|630| <<alloc_cmd>> cmd = __alloc_cmd(nq);
+ *   - drivers/block/null_blk_main.c|636| <<alloc_cmd>> cmd = __alloc_cmd(nq);
+ *
+ * null_queue_bio()
+ *  -> alloc_cmd()
+ *      -> __alloc_cmd()
+ *
+ * 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+ * 然后返回nullb_queue->cmd[tag]
+ */
 static struct nullb_cmd *__alloc_cmd(struct nullb_queue *nq)
 {
 	struct nullb_cmd *cmd;
 	unsigned int tag;
 
+	/* 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag */
 	tag = get_tag(nq);
 	if (tag != -1U) {
+		/*
+		 * struct nullb_queue {
+		 *	unsigned long *tag_map;
+		 *	wait_queue_head_t wait;
+		 *	unsigned int queue_depth;
+		 *	struct nullb_device *dev;
+		 *	unsigned int requeue_selection;
+		 *
+		 *	struct nullb_cmd *cmds;
+		 * };
+		 */
 		cmd = &nq->cmds[tag];
 		cmd->tag = tag;
 		cmd->nq = nq;
@@ -617,11 +1037,27 @@ static struct nullb_cmd *__alloc_cmd(struct nullb_queue *nq)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1346| <<null_queue_bio>> cmd = alloc_cmd(nq, 1);
+ *
+ * null_queue_bio()
+ *  -> alloc_cmd()
+ *      -> __alloc_cmd()
+ *
+ * 核心思想是在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+ * 然后返回nullb_queue->cmd[tag]
+ * 分配不到的话如果can_wait设置了会睡眠 (put_tag()会唤醒)
+ */
 static struct nullb_cmd *alloc_cmd(struct nullb_queue *nq, int can_wait)
 {
 	struct nullb_cmd *cmd;
 	DEFINE_WAIT(wait);
 
+	/*
+	 * 在nullb_queue->tag_map中取出一个bit(设置)并返回作为tag
+	 * 然后返回nullb_queue->cmd[tag]
+	 */
 	cmd = __alloc_cmd(nq);
 	if (cmd || !can_wait)
 		return cmd;
@@ -639,12 +1075,46 @@ static struct nullb_cmd *alloc_cmd(struct nullb_queue *nq, int can_wait)
 	return cmd;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|666| <<null_cmd_timer_expired>> end_cmd(container_of(timer, struct nullb_cmd, timer));
+ *   - drivers/block/null_blk_main.c|680| <<null_complete_rq>> end_cmd(blk_mq_rq_to_pdu(rq));
+ *   - drivers/block/null_blk_main.c|1253| <<nullb_complete_cmd>> end_cmd(cmd);
+ *   - drivers/block/null_blk_main.c|1258| <<nullb_complete_cmd>> end_cmd(cmd);
+ *
+ * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */
 static void end_cmd(struct nullb_cmd *cmd)
 {
 	int queue_mode = cmd->nq->dev->queue_mode;
 
 	switch (queue_mode)  {
 	case NULL_Q_MQ:
+		/*
+		 * 部分调用blk_mq_end_request()的例子:
+		 *   - block/blk-flush.c|421| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+		 *   - block/blk-flush.c|774| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+		 *   - block/blk-mq.c|1366| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+		 *   - block/blk-mq.c|1979| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+		 *   - block/blk-mq.c|2015| <<blk_mq_try_issue_list_directly>> blk_mq_end_request(rq, ret);
+		 *   - block/bsg-lib.c|158| <<bsg_teardown_job>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/block/loop.c|487| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+		 *   - drivers/block/nbd.c|340| <<nbd_complete_rq>> blk_mq_end_request(req, cmd->status);
+		 *   - drivers/block/null_blk_main.c|677| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+		 *   - drivers/block/virtio_blk.c|226| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+		 *   - drivers/block/xen-blkfront.c|918| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+		 *   - drivers/block/xen-blkfront.c|2113| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+		 *   - drivers/ide/ide-cd.c|765| <<cdrom_newpc_intr>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/ide/ide-pm.c|50| <<ide_pm_execute_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/ide/ide-pm.c|220| <<ide_complete_pm_rq>> blk_mq_end_request(rq, BLK_STS_OK);
+		 *   - drivers/md/dm-rq.c|174| <<dm_end_request>> blk_mq_end_request(rq, error);
+		 *   - drivers/md/dm-rq.c|271| <<dm_softirq_done>> blk_mq_end_request(rq, tio->error);
+		 *   - drivers/nvme/host/core.c|307| <<nvme_complete_rq>> blk_mq_end_request(req, status);
+		 *   - drivers/nvme/host/multipath.c|76| <<nvme_failover_req>> blk_mq_end_request(req, 0);
+		 *   - drivers/scsi/scsi_transport_fc.c|3581| <<fc_bsg_job_timeout>> blk_mq_end_request(req, BLK_STS_IOERR);
+		 */
 		blk_mq_end_request(cmd->rq, cmd->error);
 		return;
 	case NULL_Q_BIO:
@@ -653,16 +1123,47 @@ static void end_cmd(struct nullb_cmd *cmd)
 		break;
 	}
 
+	/*
+	 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+	 * 根据情况唤醒nullb_queue->wait
+	 */
 	free_cmd(cmd);
 }
 
+/*
+ * 在null_cmd_end_timer()触发()
+ *
+ * 使用null_cmd_timer_expired()的地方:
+ *   - drivers/block/null_blk_main.c|660| <<__alloc_cmd>> cmd->timer.function = null_cmd_timer_expired;
+ *   - drivers/block/null_blk_main.c|1611| <<null_queue_rq>> cmd->timer.function = null_cmd_timer_expired;
+ *
+ * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ * 返回HRTIMER_NORESTART
+ */
 static enum hrtimer_restart null_cmd_timer_expired(struct hrtimer *timer)
 {
+	/*
+	 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+	 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+	 * 根据情况唤醒nullb_queue->wait
+	 */
 	end_cmd(container_of(timer, struct nullb_cmd, timer));
 
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1447| <<nullb_complete_cmd>> null_cmd_end_timer(cmd);
+ *
+ * 调用null_cmd_timer_expired()来...
+ * ... 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ * 返回HRTIMER_NORESTART
+ */
 static void null_cmd_end_timer(struct nullb_cmd *cmd)
 {
 	ktime_t kt = cmd->nq->dev->completion_nsec;
@@ -670,13 +1171,37 @@ static void null_cmd_end_timer(struct nullb_cmd *cmd)
 	hrtimer_start(&cmd->timer, kt, HRTIMER_MODE_REL);
 }
 
+/*
+ * struct blk_mq_ops null_mq_ops.complete = null_complete_rq()
+ *
+ * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+ * 清空nullb_queue->tag_map中cmd->tag对应的bit
+ * 根据情况唤醒nullb_queue->wait
+ */
 static void null_complete_rq(struct request *rq)
 {
 	end_cmd(blk_mq_rq_to_pdu(rq));
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|862| <<null_insert_page>> t_page = null_alloc_page(GFP_NOIO);
+ *
+ * 分配一个nullb_page(包括nullb_page->page), 初始化nullb_pabe->bitmap
+ */
 static struct nullb_page *null_alloc_page(gfp_t gfp_flags)
 {
+	/*
+	 * struct nullb_page {
+	 *	struct page *page;
+	 *	DECLARE_BITMAP(bitmap, MAP_SZ);
+	 * };
+	 *
+	 * The highest 2 bits of bitmap are for special purpose. LOCK means the cache
+	 * page is being flushing to storage. FREE means the cache page is freed and
+	 * should be skipped from flushing to storage. Please see
+	 * null_make_cache_space
+	 */
 	struct nullb_page *t_page;
 
 	t_page = kmalloc(sizeof(struct nullb_page), gfp_flags);
@@ -695,6 +1220,18 @@ static struct nullb_page *null_alloc_page(gfp_t gfp_flags)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|826| <<null_free_sector>> null_free_page(ret);
+ *   - drivers/block/null_blk_main.c|845| <<null_radix_tree_insert>> null_free_page(t_page);
+ *   - drivers/block/null_blk_main.c|878| <<null_free_device_storage>> null_free_page(ret);
+ *   - drivers/block/null_blk_main.c|966| <<null_insert_page>> null_free_page(t_page);
+ *   - drivers/block/null_blk_main.c|990| <<null_flush_cache_page>> null_free_page(c_page);
+ *   - drivers/block/null_blk_main.c|994| <<null_flush_cache_page>> null_free_page(t_page);
+ *   - drivers/block/null_blk_main.c|1019| <<null_flush_cache_page>> null_free_page(ret);
+ *
+ * "根据情况"释放nullb_page
+ */
 static void null_free_page(struct nullb_page *t_page)
 {
 	__set_bit(NULLB_PAGE_FREE, t_page->bitmap);
@@ -704,6 +1241,13 @@ static void null_free_page(struct nullb_page *t_page)
 	kfree(t_page);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|823| <<null_free_sector>> if (null_page_empty(t_page)) {
+ *   - drivers/block/null_blk_main.c|991| <<null_flush_cache_page>> if (t_page && null_page_empty(t_page)) {
+ *
+ * 根据nullb_page->bitmap判断是否还有可用的sector
+ */
 static bool null_page_empty(struct nullb_page *page)
 {
 	int size = MAP_SZ - 2;
@@ -711,6 +1255,20 @@ static bool null_page_empty(struct nullb_page *page)
 	return find_first_bit(page->bitmap, size) == size;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1116| <<copy_to_nullb>> null_free_sector(nullb, sector, true);
+ *   - drivers/block/null_blk_main.c|1186| <<null_handle_discard>> null_free_sector(nullb, sector, false);
+ *   - drivers/block/null_blk_main.c|1188| <<null_handle_discard>> null_free_sector(nullb, sector, true);
+ *
+ * struct nullb:
+ *  -> struct nullb_device *dev;
+ *      -> struct radix_tree_root data; // data stored in the disk
+ *      -> struct radix_tree_root cache; // disk cache data
+ * null_free_sector()根据参数is_cache决定用data还是cache的radix tree
+ * 找到sector对应的nullb_page, 清空对应nullb_page->bitmap
+ * 如果都清空了, 在radix tree上拿走并释放
+ */
 static void null_free_sector(struct nullb *nullb, sector_t sector,
 	bool is_cache)
 {
@@ -719,14 +1277,26 @@ static void null_free_sector(struct nullb *nullb, sector_t sector,
 	struct nullb_page *t_page, *ret;
 	struct radix_tree_root *root;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
 	idx = sector >> PAGE_SECTORS_SHIFT;
+	/*
+	 * SECTOR_MASK表示page中的mask们
+	 */
 	sector_bit = (sector & SECTOR_MASK);
 
 	t_page = radix_tree_lookup(root, idx);
 	if (t_page) {
 		__clear_bit(sector_bit, t_page->bitmap);
 
+		/*
+		 * 根据nullb_page->bitmap判断是否还有可用的sector
+		 */
 		if (null_page_empty(t_page)) {
 			ret = radix_tree_delete_item(root, idx, t_page);
 			WARN_ON(ret != t_page);
@@ -737,11 +1307,28 @@ static void null_free_sector(struct nullb *nullb, sector_t sector,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|936| <<null_insert_page>> t_page = null_radix_tree_insert(nullb, idx, t_page, !ignore_cache);
+ *
+ * struct nullb:
+ *  -> struct nullb_device *dev;
+ *      -> struct radix_tree_root data; // data stored in the disk
+ *      -> struct radix_tree_root cache; // disk cache data
+ * null_radix_tree_insert()根据参数is_cache决定用data还是cache的radix tree
+ * 把nullb_page插入radix tree
+ */
 static struct nullb_page *null_radix_tree_insert(struct nullb *nullb, u64 idx,
 	struct nullb_page *t_page, bool is_cache)
 {
 	struct radix_tree_root *root;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
 
 	if (radix_tree_insert(root, idx, t_page)) {
@@ -754,6 +1341,13 @@ static struct nullb_page *null_radix_tree_insert(struct nullb *nullb, u64 idx,
 	return t_page;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|459| <<nullb_device_release>> null_free_device_storage(dev, false);
+ *   - drivers/block/null_blk_main.c|1621| <<null_del_dev>> null_free_device_storage(nullb->dev, true);
+ *
+ * 释放nullb_device上某个radix tree的所有page
+ */
 static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 {
 	unsigned long pos = 0;
@@ -761,6 +1355,12 @@ static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 	struct nullb_page *ret, *t_pages[FREE_BATCH];
 	struct radix_tree_root *root;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &dev->cache : &dev->data;
 
 	do {
@@ -773,6 +1373,7 @@ static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 			pos = t_pages[i]->page->index;
 			ret = radix_tree_delete_item(root, pos, t_pages[i]);
 			WARN_ON(ret != t_pages[i]);
+			/* "根据情况"释放nullb_page */
 			null_free_page(ret);
 		}
 
@@ -783,6 +1384,17 @@ static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
 		dev->curr_cache = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|906| <<null_lookup_page>> page = __null_lookup_page(nullb, sector, for_write, true);
+ *   - drivers/block/null_blk_main.c|909| <<null_lookup_page>> return __null_lookup_page(nullb, sector, for_write, false);
+ *
+ * struct nullb:
+ *  -> struct nullb_device *dev;
+ *      -> struct radix_tree_root data; // data stored in the disk
+ *      -> struct radix_tree_root cache; // disk cache data
+ * 根据参数is_cache决定用data还是cache的radix tree中搜索nullb_page
+ */
 static struct nullb_page *__null_lookup_page(struct nullb *nullb,
 	sector_t sector, bool for_write, bool is_cache)
 {
@@ -794,6 +1406,12 @@ static struct nullb_page *__null_lookup_page(struct nullb *nullb,
 	idx = sector >> PAGE_SECTORS_SHIFT;
 	sector_bit = (sector & SECTOR_MASK);
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 */
 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
 	t_page = radix_tree_lookup(root, idx);
 	WARN_ON(t_page && t_page->page->index != idx);
@@ -804,11 +1422,27 @@ static struct nullb_page *__null_lookup_page(struct nullb *nullb,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|920| <<null_insert_page>> t_page = null_lookup_page(nullb, sector, true, ignore_cache);
+ *   - drivers/block/null_blk_main.c|944| <<null_insert_page>> return null_lookup_page(nullb, sector, true, ignore_cache);
+ *   - drivers/block/null_blk_main.c|1106| <<copy_from_nullb>> t_page = null_lookup_page(nullb, sector, false,
+ *
+ * 根据参数ignore_cache决定用data还是cache的radix tree中搜索nullb_page
+ * 如果没有ignore_cahce, 就先在cache中搜索, 再在data搜索
+ */
 static struct nullb_page *null_lookup_page(struct nullb *nullb,
 	sector_t sector, bool for_write, bool ignore_cache)
 {
 	struct nullb_page *page = NULL;
 
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 * 根据参数is_cache决定用data还是cache的radix tree中搜索nullb_page
+	 */
 	if (!ignore_cache)
 		page = __null_lookup_page(nullb, sector, for_write, true);
 	if (page)
@@ -816,6 +1450,11 @@ static struct nullb_page *null_lookup_page(struct nullb *nullb,
 	return __null_lookup_page(nullb, sector, for_write, false);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|957| <<null_flush_cache_page>> t_page = null_insert_page(nullb, idx << PAGE_SECTORS_SHIFT, true);
+ *   - drivers/block/null_blk_main.c|1068| <<copy_to_nullb>> t_page = null_insert_page(nullb, sector,
+ */
 static struct nullb_page *null_insert_page(struct nullb *nullb,
 					   sector_t sector, bool ignore_cache)
 	__releases(&nullb->lock)
@@ -830,6 +1469,9 @@ static struct nullb_page *null_insert_page(struct nullb *nullb,
 
 	spin_unlock_irq(&nullb->lock);
 
+	/*
+	 * 分配一个nullb_page(包括nullb_page->page), 初始化nullb_pabe->bitmap
+	 */
 	t_page = null_alloc_page(GFP_NOIO);
 	if (!t_page)
 		goto out_lock;
@@ -840,6 +1482,14 @@ static struct nullb_page *null_insert_page(struct nullb *nullb,
 	spin_lock_irq(&nullb->lock);
 	idx = sector >> PAGE_SECTORS_SHIFT;
 	t_page->page->index = idx;
+	/*
+	 * struct nullb:
+	 *  -> struct nullb_device *dev;
+	 *      -> struct radix_tree_root data; // data stored in the disk
+	 *      -> struct radix_tree_root cache; // disk cache data
+	 * null_radix_tree_insert()根据参数is_cache决定用data还是cache的radix tree
+	 * 把nullb_page插入radix tree
+	 */
 	t_page = null_radix_tree_insert(nullb, idx, t_page, !ignore_cache);
 	radix_tree_preload_end();
 
@@ -851,6 +1501,10 @@ static struct nullb_page *null_insert_page(struct nullb *nullb,
 	return null_lookup_page(nullb, sector, true, ignore_cache);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1029| <<null_make_cache_space>> err = null_flush_cache_page(nullb, c_pages[i]);
+ */
 static int null_flush_cache_page(struct nullb *nullb, struct nullb_page *c_page)
 {
 	int i;
@@ -894,12 +1548,20 @@ static int null_flush_cache_page(struct nullb *nullb, struct nullb_page *c_page)
 	kunmap_atomic(src);
 
 	ret = radix_tree_delete_item(&nullb->dev->cache, idx, c_page);
+	/* "根据情况"释放nullb_page */
 	null_free_page(ret);
 	nullb->dev->curr_cache -= PAGE_SIZE;
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1065| <<copy_to_nullb>> null_make_cache_space(nullb, PAGE_SIZE);
+ *   - drivers/block/null_blk_main.c|1174| <<null_handle_flush>> err = null_make_cache_space(nullb,
+ *
+ * 从存储flush掉n大小的cache
+ */
 static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 {
 	int i, err, nr_pages;
@@ -911,6 +1573,17 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 	     nullb->dev->curr_cache + n || nullb->dev->curr_cache == 0)
 		return 0;
 
+	/*
+	 * radix_tree_gang_lookup - perform multiple lookup on a radix tree
+	 * @root:          radix tree root
+	 * @results:       where the results of the lookup are placed
+	 * @first_index:   start the lookup from this key
+	 * @max_items:     place up to this many items at *results
+	 *
+	 * Performs an index-ascending scan of the tree for present items.  Places
+	 * them at *@results and returns the number of items which were placed at
+	 * @results.
+	 */
 	nr_pages = radix_tree_gang_lookup(&nullb->dev->cache,
 			(void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
 	/*
@@ -923,6 +1596,13 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 		 * We found the page which is being flushed to disk by other
 		 * threads
 		 */
+		/*
+		 * 在以下使用NULLB_PAGE_LOCK:
+		 *   - drivers/block/null_blk_main.c|1151| <<null_free_page>> if (test_bit(NULLB_PAGE_LOCK, t_page->bitmap))
+		 *   - drivers/block/null_blk_main.c|1356| <<null_flush_cache_page>> __clear_bit(NULLB_PAGE_LOCK, c_page->bitmap);
+		 *   - drivers/block/null_blk_main.c|1421| <<null_make_cache_space>> if (test_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap))
+		 *   - drivers/block/null_blk_main.c|1424| <<null_make_cache_space>> __set_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap);
+		 */
 		if (test_bit(NULLB_PAGE_LOCK, c_pages[i]->bitmap))
 			c_pages[i] = NULL;
 		else
@@ -941,6 +1621,12 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 	flushed += one_round << PAGE_SHIFT;
 
 	if (n > flushed) {
+		/*
+		 * 在以下使用cache_flush_pos:
+		 *   - drivers/block/null_blk_main.c|1551| <<null_make_cache_space>> (void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
+		 *   - drivers/block/null_blk_main.c|1557| <<null_make_cache_space>> nullb->cache_flush_pos = c_pages[i]->page->index;
+		 *   - drivers/block/null_blk_main.c|1581| <<null_make_cache_space>> nullb->cache_flush_pos = 0;
+		 */
 		if (nr_pages == 0)
 			nullb->cache_flush_pos = 0;
 		if (one_round == 0) {
@@ -953,6 +1639,10 @@ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1189| <<null_transfer>> err = copy_to_nullb(nullb, page, off, sector, len, is_fua);
+ */
 static int copy_to_nullb(struct nullb *nullb, struct page *source,
 	unsigned int off, sector_t sector, size_t n, bool is_fua)
 {
@@ -961,9 +1651,20 @@ static int copy_to_nullb(struct nullb *nullb, struct page *source,
 	struct nullb_page *t_page;
 	void *dst, *src;
 
+	/*
+	 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+	 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+	 *
+	 * 由null_cache_active()和is_fua决定是否用cache
+	 */
+
 	while (count < n) {
 		temp = min_t(size_t, nullb->dev->blocksize, n - count);
 
+		/*
+		 * null_cache_active()
+		 * 判断是否支持write-back cache (NULLB_DEV_FL_CACHE)
+		 */
 		if (null_cache_active(nullb) && !is_fua)
 			null_make_cache_space(nullb, PAGE_SIZE);
 
@@ -981,6 +1682,15 @@ static int copy_to_nullb(struct nullb *nullb, struct page *source,
 
 		__set_bit(sector & SECTOR_MASK, t_page->bitmap);
 
+		/*
+		 * struct nullb:
+		 *  -> struct nullb_device *dev;
+		 *      -> struct radix_tree_root data; // data stored in the disk
+		 *      -> struct radix_tree_root cache; // disk cache data
+		 * null_free_sector()根据参数is_cache决定用data还是cache的radix tree
+		 * 找到sector对应的nullb_page, 清空对应nullb_page->bitmap
+		 * 如果都清空了, 在radix tree上拿走并释放
+		 */
 		if (is_fua)
 			null_free_sector(nullb, sector, true);
 
@@ -990,6 +1700,13 @@ static int copy_to_nullb(struct nullb *nullb, struct page *source,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1178| <<null_transfer>> err = copy_from_nullb(nullb, page, off,
+ *
+ * 这里应该是读的工作,如果支持write-back cache (NULLB_DEV_FL_CACHE),就用cache radix tree
+ * 否则用data的radix tree, 把数据读到内存
+ */
 static int copy_from_nullb(struct nullb *nullb, struct page *dest,
 	unsigned int off, sector_t sector, size_t n)
 {
@@ -1002,6 +1719,14 @@ static int copy_from_nullb(struct nullb *nullb, struct page *dest,
 		temp = min_t(size_t, nullb->dev->blocksize, n - count);
 
 		offset = (sector & SECTOR_MASK) << SECTOR_SHIFT;
+		/*
+		 * null_lookup_page():
+		 * 根据参数ignore_cache决定用data还是cache的radix tree中搜索nullb_page
+		 * 如果没有ignore_cahce, 就先在cache中搜索, 再在data搜索
+		 *
+		 * null_cache_active():
+		 * Device is using a write-back cache.
+		 */
 		t_page = null_lookup_page(nullb, sector, false,
 			!null_cache_active(nullb));
 
@@ -1022,6 +1747,12 @@ static int copy_from_nullb(struct nullb *nullb, struct page *dest,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1185| <<null_transfer>> nullb_fill_pattern(nullb, page, len, off);
+ *
+ * 把page的从off开始的len的长度的内存设置成0xFF
+ */
 static void nullb_fill_pattern(struct nullb *nullb, struct page *page,
 			       unsigned int len, unsigned int off)
 {
@@ -1032,6 +1763,11 @@ static void nullb_fill_pattern(struct nullb *nullb, struct page *page,
 	kunmap_atomic(dst);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1208| <<null_handle_rq>> null_handle_discard(nullb, sector, blk_rq_bytes(rq));
+ *   - drivers/block/null_blk_main.c|1242| <<null_handle_bio>> null_handle_discard(nullb, sector,
+ */
 static void null_handle_discard(struct nullb *nullb, sector_t sector, size_t n)
 {
 	size_t temp;
@@ -1048,6 +1784,10 @@ static void null_handle_discard(struct nullb *nullb, sector_t sector, size_t n)
 	spin_unlock_irq(&nullb->lock);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1381| <<null_handle_cmd>> cmd->error = errno_to_blk_status(null_handle_flush(nullb));
+ */
 static int null_handle_flush(struct nullb *nullb)
 {
 	int err;
@@ -1068,6 +1808,11 @@ static int null_handle_flush(struct nullb *nullb)
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1236| <<null_handle_rq>> err = null_transfer(nullb, bvec.bv_page, len, bvec.bv_offset,
+ *   - drivers/block/null_blk_main.c|1271| <<null_handle_bio>> err = null_transfer(nullb, bvec.bv_page, len, bvec.bv_offset,
+ */
 static int null_transfer(struct nullb *nullb, struct page *page,
 	unsigned int len, unsigned int off, bool is_write, sector_t sector,
 	bool is_fua)
@@ -1082,23 +1827,39 @@ static int null_transfer(struct nullb *nullb, struct page *page,
 				sector, len);
 
 		if (valid_len) {
+			/*
+			 * 这里应该是读的工作,如果支持write-back cache (NULLB_DEV_FL_CACHE),就用cache radix tree
+			 * 否则用data的radix tree, 把数据读到内存
+			 */
 			err = copy_from_nullb(nullb, page, off,
 				sector, valid_len);
 			off += valid_len;
 			len -= valid_len;
 		}
 
+		/*
+		 * nullb_fill_pattern()把page的从off开始的len的长度的内存设置成0xFF
+		 * 猜测对于非zoned的blk的len=0
+		 */
 		if (len)
 			nullb_fill_pattern(nullb, page, len, off);
 		flush_dcache_page(page);
 	} else {
 		flush_dcache_page(page);
+		/*
+		 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+		 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+		 */
 		err = copy_to_nullb(nullb, page, off, sector, len, is_fua);
 	}
 
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1348| <<null_handle_memory_backed>> err = null_handle_rq(cmd);
+ */
 static int null_handle_rq(struct nullb_cmd *cmd)
 {
 	struct request *rq = cmd->rq;
@@ -1117,8 +1878,16 @@ static int null_handle_rq(struct nullb_cmd *cmd)
 	}
 
 	spin_lock_irq(&nullb->lock);
+	/*
+	 * struct bio_vec bvec;
+	 * struct req_iterator iter;
+	 */
 	rq_for_each_segment(bvec, rq, iter) {
 		len = bvec.bv_len;
+		/*
+		 * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+		 * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+		 */
 		err = null_transfer(nullb, bvec.bv_page, len, bvec.bv_offset,
 				     op_is_write(req_op(rq)), sector,
 				     req_op(rq) & REQ_FUA);
@@ -1133,6 +1902,10 @@ static int null_handle_rq(struct nullb_cmd *cmd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1346| <<null_handle_memory_backed>> err = null_handle_bio(cmd);
+ */
 static int null_handle_bio(struct nullb_cmd *cmd)
 {
 	struct bio *bio = cmd->bio;
@@ -1167,6 +1940,12 @@ static int null_handle_bio(struct nullb_cmd *cmd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1290| <<null_handle_throttled>> null_stop_queue(nullb);
+ *
+ * 对于multiqueue调用blk_mq_stop_hw_queues(q)
+ */
 static void null_stop_queue(struct nullb *nullb)
 {
 	struct request_queue *q = nullb->q;
@@ -1175,6 +1954,14 @@ static void null_stop_queue(struct nullb *nullb)
 		blk_mq_stop_hw_queues(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1318| <<null_handle_throttled>> null_restart_queue_async(nullb);
+ *   - drivers/block/null_blk_main.c|1437| <<nullb_bwtimer_fn>> null_restart_queue_async(nullb);
+ *   - drivers/block/null_blk_main.c|1611| <<null_del_dev>> null_restart_queue_async(nullb);
+ *
+ * 对于multiqueue调用blk_mq_start_stopped_hw_queues(q, true)
+ */
 static void null_restart_queue_async(struct nullb *nullb)
 {
 	struct request_queue *q = nullb->q;
@@ -1183,6 +1970,10 @@ static void null_restart_queue_async(struct nullb *nullb)
 		blk_mq_start_stopped_hw_queues(q, true);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1400| <<null_handle_cmd>> sts = null_handle_throttled(cmd);
+ */
 static inline blk_status_t null_handle_throttled(struct nullb_cmd *cmd)
 {
 	struct nullb_device *dev = cmd->nq->dev;
@@ -1204,6 +1995,10 @@ static inline blk_status_t null_handle_throttled(struct nullb_cmd *cmd)
 	return sts;
 }
 
+/*
+ * called by;
+ *   - drivers/block/null_blk_main.c|1411| <<null_handle_cmd>> cmd->error = null_handle_badblocks(cmd, sector, nr_sectors);
+ */
 static inline blk_status_t null_handle_badblocks(struct nullb_cmd *cmd,
 						 sector_t sector,
 						 sector_t nr_sectors)
@@ -1218,6 +2013,10 @@ static inline blk_status_t null_handle_badblocks(struct nullb_cmd *cmd,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1417| <<null_handle_cmd>> cmd->error = null_handle_memory_backed(cmd, op);
+ */
 static inline blk_status_t null_handle_memory_backed(struct nullb_cmd *cmd,
 						     enum req_opf op)
 {
@@ -1232,6 +2031,14 @@ static inline blk_status_t null_handle_memory_backed(struct nullb_cmd *cmd,
 	return errno_to_blk_status(err);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1321| <<null_handle_cmd>> nullb_complete_cmd(cmd);
+ *
+ * null_queue_bio() or null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> nullb_complete_cmd()
+ */
 static inline void nullb_complete_cmd(struct nullb_cmd *cmd)
 {
 	/* Complete IO by inline, softirq or timer */
@@ -1245,11 +2052,21 @@ static inline void nullb_complete_cmd(struct nullb_cmd *cmd)
 			/*
 			 * XXX: no proper submitting cpu information available.
 			 */
+			/*
+			 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+			 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+			 * 根据情况唤醒nullb_queue->wait
+			 */
 			end_cmd(cmd);
 			break;
 		}
 		break;
 	case NULL_IRQ_NONE:
+		/*
+		 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+		 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+		 * 根据情况唤醒nullb_queue->wait
+		 */
 		end_cmd(cmd);
 		break;
 	case NULL_IRQ_TIMER:
@@ -1258,6 +2075,11 @@ static inline void nullb_complete_cmd(struct nullb_cmd *cmd)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1373| <<null_queue_bio>> null_handle_cmd(cmd, sector, nr_sectors, bio_op(bio));
+ *   - drivers/block/null_blk_main.c|1441| <<null_queue_rq>> return null_handle_cmd(cmd, sector, nr_sectors, req_op(bd->rq));
+ */
 static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 				    sector_t nr_sectors, enum req_opf op)
 {
@@ -1265,6 +2087,12 @@ static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 	struct nullb *nullb = dev->nullb;
 	blk_status_t sts;
 
+	/*
+	 * 在以下使用NULLB_DEV_FL_THROTTLED:
+	 *   - drivers/block/null_blk_main.c|1543| <<null_handle_cmd>> if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|1772| <<null_del_dev>> if (test_bit(NULLB_DEV_FL_THROTTLED, &nullb->dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|2156| <<null_add_dev>> set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
+	 */
 	if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
 		sts = null_handle_throttled(cmd);
 		if (sts != BLK_STS_OK)
@@ -1282,6 +2110,9 @@ static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 			goto out;
 	}
 
+	/*
+	 * 对于nullb0默认是0
+	 */
 	if (dev->memory_backed)
 		cmd->error = null_handle_memory_backed(cmd, op);
 
@@ -1289,10 +2120,22 @@ static blk_status_t null_handle_cmd(struct nullb_cmd *cmd, sector_t sector,
 		cmd->error = null_handle_zoned(cmd, op, sector, nr_sectors);
 
 out:
+	/*
+	 * called by:
+	 *   - drivers/block/null_blk_main.c|1321| <<null_handle_cmd>> nullb_complete_cmd(cmd);
+	 *
+	 * null_queue_bio() or null_queue_rq():
+	 *  -> null_handle_cmd()
+	 *      -> nullb_complete_cmd()
+	 */
 	nullb_complete_cmd(cmd);
 	return BLK_STS_OK;
 }
 
+/*
+ * 在以下使用nullb_bwtimer_fn():
+ *   - drivers/block/null_blk_main.c|1449| <<nullb_setup_bwtimer>> nullb->bw_timer.function = nullb_bwtimer_fn;
+ */
 static enum hrtimer_restart nullb_bwtimer_fn(struct hrtimer *timer)
 {
 	struct nullb *nullb = container_of(timer, struct nullb, bw_timer);
@@ -1310,16 +2153,36 @@ static enum hrtimer_restart nullb_bwtimer_fn(struct hrtimer *timer)
 	return HRTIMER_RESTART;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1983| <<null_add_dev>> nullb_setup_bwtimer(nullb);
+ *
+ * 在dev->mbps和NULLB_DEV_FL_THROTTLED的情况使用
+ */
 static void nullb_setup_bwtimer(struct nullb *nullb)
 {
 	ktime_t timer_interval = ktime_set(0, TIMER_INTERVAL);
 
+	/*
+	 * HRTIMER_MODE_ABS             - Time value is absolute
+	 * HRTIMER_MODE_REL             - Time value is relative to now
+	 * HRTIMER_MODE_PINNED          - Timer is bound to CPU (is only considered
+	 *                                when starting the timer)
+	 * HRTIMER_MODE_SOFT            - Timer callback function will be executed in
+	 *                                soft irq context
+	 * HRTIMER_MODE_HARD            - Timer callback function will be executed in
+	 *                                hard irq context even on PREEMPT_RT.
+	 */
 	hrtimer_init(&nullb->bw_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	nullb->bw_timer.function = nullb_bwtimer_fn;
 	atomic_long_set(&nullb->cur_bytes, mb_per_tick(nullb->dev->mbps));
 	hrtimer_start(&nullb->bw_timer, timer_interval, HRTIMER_MODE_REL);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1473| <<null_queue_bio>> struct nullb_queue *nq = nullb_to_queue(nullb);
+ */
 static struct nullb_queue *nullb_to_queue(struct nullb *nullb)
 {
 	int index = 0;
@@ -1330,6 +2193,10 @@ static struct nullb_queue *nullb_to_queue(struct nullb *nullb)
 	return &nullb->queues[index];
 }
 
+/*
+ * 在以下使用null_queue_bio():
+ *   - drivers/block/null_blk_main.c|1832| <<null_add_dev>> blk_queue_make_request(nullb->q, null_queue_bio);
+ */
 static blk_qc_t null_queue_bio(struct request_queue *q, struct bio *bio)
 {
 	sector_t sector = bio->bi_iter.bi_sector;
@@ -1345,6 +2212,10 @@ static blk_qc_t null_queue_bio(struct request_queue *q, struct bio *bio)
 	return BLK_QC_T_NONE;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1546| <<null_queue_rq>> if (should_timeout_request(bd->rq))
+ */
 static bool should_timeout_request(struct request *rq)
 {
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -1354,6 +2225,10 @@ static bool should_timeout_request(struct request *rq)
 	return false;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1533| <<null_queue_rq>> if (should_requeue_request(bd->rq)) {
+ */
 static bool should_requeue_request(struct request *rq)
 {
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -1363,6 +2238,9 @@ static bool should_requeue_request(struct request *rq)
 	return false;
 }
 
+/*
+ * struct blk_mq_ops null_mq_ops.timeout= null_timeout_rq()
+ */
 static enum blk_eh_timer_return null_timeout_rq(struct request *rq, bool res)
 {
 	pr_info("rq %p timed out\n", rq);
@@ -1370,6 +2248,9 @@ static enum blk_eh_timer_return null_timeout_rq(struct request *rq, bool res)
 	return BLK_EH_DONE;
 }
 
+/*
+ * struct blk_mq_ops null_mq_ops.queue_rq = null_queue_rq()
+ */
 static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 			 const struct blk_mq_queue_data *bd)
 {
@@ -1381,7 +2262,22 @@ static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 
 	if (nq->dev->irqmode == NULL_IRQ_TIMER) {
+		/*
+		 * hrtimer_init - initialize a timer to the given clock
+		 * @timer:      the timer to be initialized
+		 * @clock_id:   the clock to be used
+		 * @mode:       The modes which are relevant for intitialization:
+		 *              HRTIMER_MODE_ABS, HRTIMER_MODE_REL, HRTIMER_MODE_ABS_SOFT,
+		 *              HRTIMER_MODE_REL_SOFT
+		 */
 		hrtimer_init(&cmd->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+		/*
+		 * null_cmd_timer_expired()的实现:
+		 * 完成了request, 调用blk_mq_end_request()或者bio_endio()
+		 * 清空nullb_queue->tag_map中cmd->tag对应的bit
+		 * 根据情况唤醒nullb_queue->wait
+		 * 返回HRTIMER_NORESTART
+		 */
 		cmd->timer.function = null_cmd_timer_expired;
 	}
 	cmd->rq = bd->rq;
@@ -1389,6 +2285,9 @@ static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 
 	blk_mq_start_request(bd->rq);
 
+	/*
+	 * 类似fault injection, 需要BLK_DEV_NULL_BLK_FAULT_INJECTION
+	 */
 	if (should_requeue_request(bd->rq)) {
 		/*
 		 * Alternate between hitting the core BUSY path, and the
@@ -1405,21 +2304,38 @@ static blk_status_t null_queue_rq(struct blk_mq_hw_ctx *hctx,
 	if (should_timeout_request(bd->rq))
 		return BLK_STS_OK;
 
+	/*
+	 * sector_t nr_sectors = blk_rq_sectors(bd->rq);
+	 * sector_t sector = blk_rq_pos(bd->rq);
+	 */
 	return null_handle_cmd(cmd, sector, nr_sectors, req_op(bd->rq));
 }
 
+/*
+ * 在以下使用null_mq_ops:
+ *   - drivers/block/null_blk_main.c|1701| <<null_init_tag_set>> set->ops = &null_mq_ops;
+ */
 static const struct blk_mq_ops null_mq_ops = {
 	.queue_rq       = null_queue_rq,
 	.complete	= null_complete_rq,
 	.timeout	= null_timeout_rq,
 };
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1552| <<cleanup_queues>> cleanup_queue(&nullb->queues[i]);
+ */
 static void cleanup_queue(struct nullb_queue *nq)
 {
 	kfree(nq->tag_map);
 	kfree(nq->cmds);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1578| <<null_del_dev>> cleanup_queues(nullb);
+ *   - drivers/block/null_blk_main.c|1995| <<null_add_dev>> cleanup_queues(nullb);
+ */
 static void cleanup_queues(struct nullb *nullb)
 {
 	int i;
@@ -1430,6 +2346,13 @@ static void cleanup_queues(struct nullb *nullb)
 	kfree(nullb->queues);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|367| <<nullb_device_power_store>> null_del_dev(dev->nullb);
+ *   - drivers/block/null_blk_main.c|495| <<nullb_group_drop_item>> null_del_dev(dev->nullb);
+ *   - drivers/block/null_blk_main.c|2088| <<null_init>> null_del_dev(nullb);
+ *   - drivers/block/null_blk_main.c|2114| <<null_exit>> null_del_dev(nullb);
+ */
 static void null_del_dev(struct nullb *nullb)
 {
 	struct nullb_device *dev = nullb->dev;
@@ -1458,6 +2381,10 @@ static void null_del_dev(struct nullb *nullb)
 	dev->nullb = NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1926| <<null_add_dev>> null_config_discard(nullb);
+ */
 static void null_config_discard(struct nullb *nullb)
 {
 	if (nullb->dev->discard == false)
@@ -1468,11 +2395,20 @@ static void null_config_discard(struct nullb *nullb)
 	blk_queue_flag_set(QUEUE_FLAG_DISCARD, nullb->q);
 }
 
+/*
+ * 在以下使用null_ops:
+ *   - drivers/block/null_blk_main.c|1573| <<null_gendisk_register>> disk->fops = &null_ops;
+ */
 static const struct block_device_operations null_ops = {
 	.owner		= THIS_MODULE,
 	.report_zones	= null_report_zones,
 };
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1626| <<null_init_queues>> null_init_queue(nullb, nq);
+ *   - drivers/block/null_blk_main.c|1678| <<init_driver_queues>> null_init_queue(nullb, nq);
+ */
 static void null_init_queue(struct nullb *nullb, struct nullb_queue *nq)
 {
 	BUG_ON(!nullb);
@@ -1483,6 +2419,10 @@ static void null_init_queue(struct nullb *nullb, struct nullb_queue *nq)
 	nq->dev = nullb->dev;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1880| <<null_add_dev>> null_init_queues(nullb);
+ */
 static void null_init_queues(struct nullb *nullb)
 {
 	struct request_queue *q = nullb->q;
@@ -1490,6 +2430,7 @@ static void null_init_queues(struct nullb *nullb)
 	struct nullb_queue *nq;
 	int i;
 
+	/* 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (!hctx->nr_ctx || !hctx->tags)
 			continue;
@@ -1500,6 +2441,10 @@ static void null_init_queues(struct nullb *nullb)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1680| <<init_driver_queues>> ret = setup_commands(nq);
+ */
 static int setup_commands(struct nullb_queue *nq)
 {
 	struct nullb_cmd *cmd;
@@ -1518,7 +2463,15 @@ static int setup_commands(struct nullb_queue *nq)
 
 	for (i = 0; i < nq->queue_depth; i++) {
 		cmd = &nq->cmds[i];
+		/*
+		 * list没人用, 可以删了??
+		 *   - drivers/block/null_blk_main.c|2440| <<setup_commands>> INIT_LIST_HEAD(&cmd->list);
+		 */
 		INIT_LIST_HEAD(&cmd->list);
+		/*
+		 * ll_list没人用, 可以删了??
+		 *   - drivers/block/null_blk_main.c|2441| <<setup_commands>> cmd->ll_list.next = NULL;
+		 */
 		cmd->ll_list.next = NULL;
 		cmd->tag = -1U;
 	}
@@ -1526,8 +2479,15 @@ static int setup_commands(struct nullb_queue *nq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1851| <<null_add_dev>> rv = setup_queues(nullb);
+ */
 static int setup_queues(struct nullb *nullb)
 {
+	/*
+	 * struct nullb_queue *queues;
+	 */
 	nullb->queues = kcalloc(nullb->dev->submit_queues,
 				sizeof(struct nullb_queue),
 				GFP_KERNEL);
@@ -1539,6 +2499,10 @@ static int setup_queues(struct nullb *nullb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1888| <<null_add_dev>> rv = init_driver_queues(nullb);
+ */
 static int init_driver_queues(struct nullb *nullb)
 {
 	struct nullb_queue *nq;
@@ -1557,6 +2521,10 @@ static int init_driver_queues(struct nullb *nullb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1930| <<null_add_dev>> rv = null_gendisk_register(nullb);
+ */
 static int null_gendisk_register(struct nullb *nullb)
 {
 	sector_t size = ((sector_t)nullb->dev->size * SZ_1M) >> SECTOR_SHIFT;
@@ -1593,6 +2561,11 @@ static int null_gendisk_register(struct nullb *nullb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1810| <<null_add_dev>> rv = null_init_tag_set(nullb, nullb->tag_set);
+ *   - drivers/block/null_blk_main.c|1943| <<null_init>> ret = null_init_tag_set(NULL, &tag_set);
+ */
 static int null_init_tag_set(struct nullb *nullb, struct blk_mq_tag_set *set)
 {
 	set->ops = &null_mq_ops;
@@ -1610,9 +2583,34 @@ static int null_init_tag_set(struct nullb *nullb, struct blk_mq_tag_set *set)
 	if ((nullb && nullb->dev->blocking) || g_blocking)
 		set->flags |= BLK_MQ_F_BLOCKING;
 
+	/*
+	 * 调用blk_mq_alloc_tag_set()的例子:
+	 *   - block/blk-mq.c|2947| <<blk_mq_init_sq_queue>> ret = blk_mq_alloc_tag_set(set);
+	 *   - block/bsg-lib.c|384| <<bsg_setup_queue>> if (blk_mq_alloc_tag_set(set))
+	 *   - drivers/block/loop.c|2033| <<loop_add>> err = blk_mq_alloc_tag_set(&lo->tag_set);
+	 *   - drivers/block/nbd.c|1688| <<nbd_dev_add>> err = blk_mq_alloc_tag_set(&nbd->tag_set);
+	 *   - drivers/block/null_blk_main.c|1716| <<null_init_tag_set>> return blk_mq_alloc_tag_set(set);
+	 *   - drivers/block/virtio_blk.c|819| <<virtblk_probe>> err = blk_mq_alloc_tag_set(&vblk->tag_set);
+	 *   - drivers/block/xen-blkfront.c|984| <<xlvbd_init_blk_queue>> if (blk_mq_alloc_tag_set(&info->tag_set))
+	 *   - drivers/ide/ide-probe.c|787| <<ide_init_queue>> if (blk_mq_alloc_tag_set(set))
+	 *   - drivers/md/dm-rq.c|562| <<dm_mq_init_request_queue>> err = blk_mq_alloc_tag_set(md->tag_set);
+	 *   - drivers/nvme/host/fc.c|2475| <<nvme_fc_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+	 *   - drivers/nvme/host/fc.c|3143| <<nvme_fc_init_ctrl>> ret = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+	 *   - drivers/nvme/host/pci.c|1645| <<nvme_alloc_admin_tags>> if (blk_mq_alloc_tag_set(&dev->admin_tagset))
+	 *   - drivers/nvme/host/pci.c|2328| <<nvme_dev_add>> ret = blk_mq_alloc_tag_set(&dev->tagset);
+	 *   - drivers/nvme/host/rdma.c|755| <<nvme_rdma_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+	 *   - drivers/nvme/host/tcp.c|1493| <<nvme_tcp_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+	 *   - drivers/nvme/target/loop.c|357| <<nvme_loop_configure_admin_queue>> error = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+	 *   - drivers/nvme/target/loop.c|525| <<nvme_loop_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+	 *   - drivers/scsi/scsi_lib.c|1906| <<scsi_mq_setup_tags>> return blk_mq_alloc_tag_set(&shost->tag_set);
+	 */
 	return blk_mq_alloc_tag_set(set);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1837| <<null_add_dev>> rv = null_validate_conf(dev);
+ */
 static int null_validate_conf(struct nullb_device *dev)
 {
 	dev->blocksize = round_down(dev->blocksize, 512);
@@ -1651,6 +2649,11 @@ static int null_validate_conf(struct nullb_device *dev)
 }
 
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1824| <<null_setup_fault>> if (!__null_setup_fault(&null_timeout_attr, g_timeout_str))
+ *   - drivers/block/null_blk_main.c|1826| <<null_setup_fault>> if (!__null_setup_fault(&null_requeue_attr, g_requeue_str))
+ */
 static bool __null_setup_fault(struct fault_attr *attr, char *str)
 {
 	if (!str[0])
@@ -1664,6 +2667,12 @@ static bool __null_setup_fault(struct fault_attr *attr, char *str)
 }
 #endif
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1871| <<null_add_dev>> if (!null_setup_fault())
+ *
+ * Documentation/fault-injection/fault-injection.rst
+ */
 static bool null_setup_fault(void)
 {
 #ifdef CONFIG_BLK_DEV_NULL_BLK_FAULT_INJECTION
@@ -1675,6 +2684,11 @@ static bool null_setup_fault(void)
 	return true;
 }
 
+/*
+ * calld by:
+ *   - drivers/block/null_blk_main.c|356| <<nullb_device_power_store>> if (null_add_dev(dev)) {
+ *   - drivers/block/null_blk_main.c|2027| <<null_init>> ret = null_add_dev(dev);
+ */
 static int null_add_dev(struct nullb_device *dev)
 {
 	struct nullb *nullb;
@@ -1699,11 +2713,20 @@ static int null_add_dev(struct nullb_device *dev)
 		goto out_free_nullb;
 
 	if (dev->queue_mode == NULL_Q_MQ) {
+		/*
+		 * 如果是shared就使用全局的tag_set,
+		 * 否则就使用nullb自己的: nullb->__tag_set
+		 */
 		if (shared_tags) {
 			nullb->tag_set = &tag_set;
 			rv = 0;
 		} else {
 			nullb->tag_set = &nullb->__tag_set;
+			/*
+			 * called by:
+			 *   - drivers/block/null_blk_main.c|1810| <<null_add_dev>> rv = null_init_tag_set(nullb, nullb->tag_set);
+			 *   - drivers/block/null_blk_main.c|1943| <<null_init>> ret = null_init_tag_set(NULL, &tag_set);
+			 */
 			rv = null_init_tag_set(nullb, nullb->tag_set);
 		}
 
@@ -1732,13 +2755,21 @@ static int null_add_dev(struct nullb_device *dev)
 			goto out_cleanup_blk_queue;
 	}
 
+	/* Bandwidth throttle cap (in MB/s) */
 	if (dev->mbps) {
+		/*
+		 * 在以下使用NULLB_DEV_FL_THROTTLED:
+		 *   - drivers/block/null_blk_main.c|1543| <<null_handle_cmd>> if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+		 *   - drivers/block/null_blk_main.c|1772| <<null_del_dev>> if (test_bit(NULLB_DEV_FL_THROTTLED, &nullb->dev->flags)) {
+		 *   - drivers/block/null_blk_main.c|2156| <<null_add_dev>> set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
+		 */
 		set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
 		nullb_setup_bwtimer(nullb);
 	}
 
 	if (dev->cache_size > 0) {
 		set_bit(NULLB_DEV_FL_CACHE, &nullb->dev->flags);
+		/* 把QUEUE_FLAG_WC和QUEUE_FLAG_FUA都设置上 */
 		blk_queue_write_cache(nullb->q, true, true);
 	}
 
@@ -1774,6 +2805,16 @@ static int null_add_dev(struct nullb_device *dev)
 		goto out_cleanup_zone;
 
 	mutex_lock(&lock);
+	/*
+	 * called by:
+	 *   - drivers/block/null_blk_main.c|1786| <<null_add_dev>> list_add_tail(&nullb->list, &nullb_list);
+	 *   - drivers/block/null_blk_main.c|1889| <<null_init>> while (!list_empty(&nullb_list)) {
+	 *   - drivers/block/null_blk_main.c|1890| <<null_init>> nullb = list_entry(nullb_list.next, struct nullb, list);
+	 *   - drivers/block/null_blk_main.c|1913| <<null_exit>> while (!list_empty(&nullb_list)) {
+	 *   - drivers/block/null_blk_main.c|1916| <<null_exit>> nullb = list_entry(nullb_list.next, struct nullb, list);
+	 *
+	 * 添加struct nullb
+	 */
 	list_add_tail(&nullb->list, &nullb_list);
 	mutex_unlock(&lock);
 
@@ -1801,6 +2842,9 @@ static int __init null_init(void)
 	struct nullb *nullb;
 	struct nullb_device *dev;
 
+	/*
+	 * "Block size (in bytes)"
+	 */
 	if (g_bs > PAGE_SIZE) {
 		pr_warn("invalid block size\n");
 		pr_warn("defaults block size to %lu\n", PAGE_SIZE);
@@ -1816,7 +2860,22 @@ static int __init null_init(void)
 		pr_err("legacy IO path no longer available\n");
 		return -EINVAL;
 	}
+	/*
+	 * g_submit_queues: "Number of submission queues"
+	 */
 	if (g_queue_mode == NULL_Q_MQ && g_use_per_node_hctx) {
+		/*
+		 * 在一下使用g_submit_queues:
+		 *   - drivers/block/null_blk_main.c|132| <<global>> module_param_named(submit_queues, g_submit_queues, int , 0444);
+		 *   - drivers/block/null_blk_main.c|622| <<null_alloc_dev>> dev->submit_queues = g_submit_queues;
+		 *   - drivers/block/null_blk_main.c|1995| <<null_init_tag_set>> g_submit_queues;
+		 *   - drivers/block/null_blk_main.c|2270| <<null_init>> if (g_submit_queues != nr_online_nodes) {
+		 *   - drivers/block/null_blk_main.c|2273| <<null_init>> g_submit_queues = nr_online_nodes;
+		 *   - drivers/block/null_blk_main.c|2275| <<null_init>> } else if (g_submit_queues > nr_cpu_ids)
+		 *   - drivers/block/null_blk_main.c|2276| <<null_init>> g_submit_queues = nr_cpu_ids;
+		 *   - drivers/block/null_blk_main.c|2277| <<null_init>> else if (g_submit_queues <= 0)
+		 *   - drivers/block/null_blk_main.c|2278| <<null_init>> g_submit_queues = 1;
+		 */
 		if (g_submit_queues != nr_online_nodes) {
 			pr_warn("submit_queues param is set to %u.\n",
 							nr_online_nodes);
@@ -1827,7 +2886,15 @@ static int __init null_init(void)
 	else if (g_submit_queues <= 0)
 		g_submit_queues = 1;
 
+	/*
+	 * "Share tag set between devices for blk-mq"
+	 */
 	if (g_queue_mode == NULL_Q_MQ && shared_tags) {
+		/*
+		 * called by:
+		 *   - drivers/block/null_blk_main.c|1810| <<null_add_dev>> rv = null_init_tag_set(nullb, nullb->tag_set);
+		 *   - drivers/block/null_blk_main.c|1943| <<null_init>> ret = null_init_tag_set(NULL, &tag_set);
+		 */
 		ret = null_init_tag_set(NULL, &tag_set);
 		if (ret)
 			return ret;
@@ -1842,6 +2909,9 @@ static int __init null_init(void)
 
 	mutex_init(&lock);
 
+	/*
+	 * 核心思想是在major_names[]找到一个可用的major
+	 */
 	null_major = register_blkdev(0, "nullb");
 	if (null_major < 0) {
 		ret = null_major;
@@ -1849,6 +2919,9 @@ static int __init null_init(void)
 	}
 
 	for (i = 0; i < nr_devices; i++) {
+		/*
+		 * 主要分配struct nullb_device和参数设置
+		 */
 		dev = null_alloc_dev();
 		if (!dev) {
 			ret = -ENOMEM;
diff --git a/drivers/block/null_blk_zoned.c b/drivers/block/null_blk_zoned.c
index ed34785dd64b..ea699c69f9a6 100644
--- a/drivers/block/null_blk_zoned.c
+++ b/drivers/block/null_blk_zoned.c
@@ -3,13 +3,32 @@
 #include "null_blk.h"
 
 /* zone_size in MBs to sectors. */
+/*
+ * 在以下使用ZONE_SIZE_SHIFT:
+ *   - drivers/block/null_blk_zoned.c|24| <<null_zone_init>> dev->zone_size_sects = dev->zone_size << ZONE_SIZE_SHIFT;
+ *
+ * 1往左移11位是2048
+ */
 #define ZONE_SIZE_SHIFT		11
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_zoned.c|78| <<null_report_zones>> first_zone = null_zone_no(dev, sector);
+ *   - drivers/block/null_blk_zoned.c|104| <<null_zone_valid_read_len>> struct blk_zone *zone = &dev->zones[null_zone_no(dev, sector)];
+ *   - drivers/block/null_blk_zoned.c|122| <<null_zone_write>> unsigned int zno = null_zone_no(dev, sector);
+ *   - drivers/block/null_blk_zoned.c|158| <<null_zone_mgmt>> struct blk_zone *zone = &dev->zones[null_zone_no(dev, sector)];
+ *
+ * 把sector转化为zone number
+ */
 static inline unsigned int null_zone_no(struct nullb_device *dev, sector_t sect)
 {
 	return sect >> ilog2(dev->zone_size_sects);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1746| <<null_add_dev>> rv = null_zone_init(dev);
+ */
 int null_zone_init(struct nullb_device *dev)
 {
 	sector_t dev_size = (sector_t)dev->size * 1024 * 1024;
@@ -61,11 +80,25 @@ int null_zone_init(struct nullb_device *dev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|566| <<null_free_dev>> null_zone_exit(dev);
+ *   - drivers/block/null_blk_main.c|1783| <<null_add_dev>> null_zone_exit(dev);
+ */
 void null_zone_exit(struct nullb_device *dev)
 {
+	/* struct blk_zone *zones; */
 	kvfree(dev->zones);
 }
 
+/*
+ * called by:
+ *   - block/blk-zoned.c|181| <<blkdev_report_zones>> return disk->fops->report_zones(disk, sector, nr_zones, cb, data);
+ *   - block/blk-zoned.c|557| <<blk_revalidate_disk_zones>> ret = disk->fops->report_zones(disk, 0, UINT_MAX,
+ *   - drivers/md/dm.c|503| <<dm_blk_report_zones>> ret = tgt->type->report_zones(tgt, &args, nr_zones);
+ *
+ * struct block_device_operations null_ops.report_zones = null_report_zones()
+ */
 int null_report_zones(struct gendisk *disk, sector_t sector,
 		unsigned int nr_zones, report_zones_cb cb, void *data)
 {
@@ -97,6 +130,14 @@ int null_report_zones(struct gendisk *disk, sector_t sector,
 	return nr_zones;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1081| <<null_transfer>> valid_len = null_zone_valid_read_len(nullb,
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_transfer():
+ *      -> null_zone_valid_read_len()
+ */
 size_t null_zone_valid_read_len(struct nullb *nullb,
 				sector_t sector, unsigned int len)
 {
@@ -115,13 +156,35 @@ size_t null_zone_valid_read_len(struct nullb *nullb,
 	return (zone->wp - sector) << SECTOR_SHIFT;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_zoned.c|214| <<null_handle_zoned>> return null_zone_write(cmd, sector, nr_sectors);
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> null_handle_zoned()
+ *          -> null_zone_write()
+ */
 static blk_status_t null_zone_write(struct nullb_cmd *cmd, sector_t sector,
 		     unsigned int nr_sectors)
 {
 	struct nullb_device *dev = cmd->nq->dev;
+	/* 把sector转化为zone number */
 	unsigned int zno = null_zone_no(dev, sector);
 	struct blk_zone *zone = &dev->zones[zno];
 
+	/*
+	 * @BLK_ZONE_COND_NOT_WP: The zone has no write pointer, it is conventional.
+	 * @BLK_ZONE_COND_EMPTY: The zone is empty.
+	 * @BLK_ZONE_COND_IMP_OPEN: The zone is open, but not explicitly opened.
+	 * @BLK_ZONE_COND_EXP_OPEN: The zones was explicitly opened by an
+	 *                          OPEN ZONE command.
+	 * @BLK_ZONE_COND_CLOSED: The zone was [explicitly] closed after writing.
+	 * @BLK_ZONE_COND_FULL: The zone is marked as full, possibly by a zone
+	 *                      FINISH ZONE command.
+	 * @BLK_ZONE_COND_READONLY: The zone is read-only.
+	 * @BLK_ZONE_COND_OFFLINE: The zone is offline (sectors cannot be read/written).
+	 */
 	switch (zone->cond) {
 	case BLK_ZONE_COND_FULL:
 		/* Cannot write to a full zone */
@@ -138,6 +201,18 @@ static blk_status_t null_zone_write(struct nullb_cmd *cmd, sector_t sector,
 		if (zone->cond != BLK_ZONE_COND_EXP_OPEN)
 			zone->cond = BLK_ZONE_COND_IMP_OPEN;
 
+		/*
+		 * 关于zoned block device:
+		 * Zoned block devices are quite different than the block devices most people
+		 * are used to. The concept came from shingled magnetic recording (SMR)
+		 * devices, which allow much higher density storage, but that extra capacity
+		 * comes with a price: less flexibility. Zoned devices have regions (zones)
+		 * that can only be written sequentially; there is no random access for writes
+		 * to those zones. Linux already supports these devices, and filesystems are
+		 * adding support as well, but some applications may want a simpler, more
+		 * straightforward interface; that's what a new filesystem, zonefs, is
+		 * targeting.
+		 */
 		zone->wp += nr_sectors;
 		if (zone->wp == zone->start + zone->len)
 			zone->cond = BLK_ZONE_COND_FULL;
@@ -151,6 +226,15 @@ static blk_status_t null_zone_write(struct nullb_cmd *cmd, sector_t sector,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_zoned.c|220| <<null_handle_zoned>> return null_zone_mgmt(cmd, op, sector);
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> null_handle_zoned()
+ *          -> null_zone_mgmt()
+ */
 static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 				   sector_t sector)
 {
@@ -160,6 +244,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 
 	switch (op) {
 	case REQ_OP_ZONE_RESET_ALL:
+		/* reset all the zone present on the device */
 		for (i = 0; i < dev->nr_zones; i++) {
 			if (zone[i].type == BLK_ZONE_TYPE_CONVENTIONAL)
 				continue;
@@ -168,6 +253,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 		}
 		break;
 	case REQ_OP_ZONE_RESET:
+		/* reset a zone write pointer */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 
@@ -175,6 +261,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 		zone->wp = zone->start;
 		break;
 	case REQ_OP_ZONE_OPEN:
+		/* Open a zone */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 		if (zone->cond == BLK_ZONE_COND_FULL)
@@ -183,6 +270,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 		zone->cond = BLK_ZONE_COND_EXP_OPEN;
 		break;
 	case REQ_OP_ZONE_CLOSE:
+		/* Close a zone */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 		if (zone->cond == BLK_ZONE_COND_FULL)
@@ -194,6 +282,7 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 			zone->cond = BLK_ZONE_COND_CLOSED;
 		break;
 	case REQ_OP_ZONE_FINISH:
+		/* Transition a zone to full */
 		if (zone->type == BLK_ZONE_TYPE_CONVENTIONAL)
 			return BLK_STS_IOERR;
 
@@ -206,6 +295,18 @@ static blk_status_t null_zone_mgmt(struct nullb_cmd *cmd, enum req_opf op,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1289| <<null_handle_cmd>> cmd->error = null_handle_zoned(cmd, op, sector, nr_sectors);
+ *
+ * null_handle_zoned()有2种可能调用别人:
+ *  -> null_zone_write()
+ *  -> null_zone_mgmt()
+ *
+ * null_queue_bio()或者null_queue_rq():
+ *  -> null_handle_cmd()
+ *      -> null_handle_zoned()
+ */
 blk_status_t null_handle_zoned(struct nullb_cmd *cmd, enum req_opf op,
 			       sector_t sector, sector_t nr_sectors)
 {
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index 7ffd719d89de..b06ec75cb24c 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -721,6 +721,27 @@ static const struct blk_mq_ops virtio_mq_ops = {
 static unsigned int virtblk_queue_depth;
 module_param_named(queue_depth, virtblk_queue_depth, uint, 0444);
 
+/*
+ * [0] check_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 static int virtblk_probe(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk;
@@ -789,6 +810,9 @@ static int virtblk_probe(struct virtio_device *vdev)
 	vblk->tag_set.queue_depth = virtblk_queue_depth;
 	vblk->tag_set.numa_node = NUMA_NO_NODE;
 	vblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	/*
+	 * sg_elems从上面VIRTIO_BLK_F_SEG_MAX获得
+	 */
 	vblk->tag_set.cmd_size =
 		sizeof(struct virtblk_req) +
 		sizeof(struct scatterlist) * sg_elems;
@@ -842,6 +866,9 @@ static int virtblk_probe(struct virtio_device *vdev)
 	blk_queue_max_segment_size(q, max_size);
 
 	/* Host can optionally specify the block size of the device */
+	/*
+	 * 如果后端是个file, 通过ioctl的BLKSSZGET和BLKPBSZGET获得
+	 */
 	err = virtio_cread_feature(vdev, VIRTIO_BLK_F_BLK_SIZE,
 				   struct virtio_blk_config, blk_size,
 				   &blk_size);
@@ -905,6 +932,9 @@ static int virtblk_probe(struct virtio_device *vdev)
 	virtblk_update_capacity(vblk, false);
 	virtio_device_ready(vdev);
 
+	/*
+	 * 和gendisk/partition非常重要的接口!!
+	 */
 	device_add_disk(&vdev->dev, vblk->disk, virtblk_attr_groups);
 	return 0;
 
@@ -1030,6 +1060,7 @@ static int __init init(void)
 	if (!virtblk_wq)
 		return -ENOMEM;
 
+	/* 核心思想是在major_names[]找到一个可用的major */
 	major = register_blkdev(0, "virtblk");
 	if (major < 0) {
 		error = major;
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index 5dc32b72e7fa..5df83b4958a7 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -30,11 +30,20 @@
 
 #define NVME_MINORS		(1U << MINORBITS)
 
+/*
+ * 在以下真正使用admin_timeout:
+ *   - drivers/nvme/host/core.c|4078| <<nvme_fw_act_work>> msecs_to_jiffies(admin_timeout * 1000);
+ *   - drivers/nvme/host/nvme.h|26| <<ADMIN_TIMEOUT>> #define ADMIN_TIMEOUT (admin_timeout * HZ)
+ */
 unsigned int admin_timeout = 60;
 module_param(admin_timeout, uint, 0644);
 MODULE_PARM_DESC(admin_timeout, "timeout in seconds for admin commands");
 EXPORT_SYMBOL_GPL(admin_timeout);
 
+/*
+ * 在以下真正使用nvme_io_timeout:
+ *   - drivers/nvme/host/nvme.h|23| <<NVME_IO_TIMEOUT>> #define NVME_IO_TIMEOUT (nvme_io_timeout * HZ)
+ */
 unsigned int nvme_io_timeout = 30;
 module_param_named(io_timeout, nvme_io_timeout, uint, 0644);
 MODULE_PARM_DESC(io_timeout, "timeout in seconds for I/O");
@@ -53,6 +62,7 @@ module_param(default_ps_max_latency_us, ulong, 0644);
 MODULE_PARM_DESC(default_ps_max_latency_us,
 		 "max power saving latency for new devices; use PM QOS to change per device");
 
+/* Autonomous Power State Transition */
 static bool force_apst;
 module_param(force_apst, bool, 0644);
 MODULE_PARM_DESC(force_apst, "allow APST for newly enumerated devices even if quirked off");
@@ -81,10 +91,22 @@ EXPORT_SYMBOL_GPL(nvme_reset_wq);
 struct workqueue_struct *nvme_delete_wq;
 EXPORT_SYMBOL_GPL(nvme_delete_wq);
 
+/*
+ * 在以下使用nvme_subsystems:
+ *   - drivers/nvme/host/core.c|2655| <<__nvme_find_get_subsystem>> list_for_each_entry(subsys, &nvme_subsystems, entry) {
+ *   - drivers/nvme/host/core.c|2797| <<nvme_init_subsystem>> list_add_tail(&subsys->entry, &nvme_subsystems);
+ */
 static LIST_HEAD(nvme_subsystems);
 static DEFINE_MUTEX(nvme_subsystems_lock);
 
 static DEFINE_IDA(nvme_instance_ida);
+/*
+ * 在以下使用nvme_chr_devt:
+ *   - drivers/nvme/host/core.c|4270| <<nvme_init_ctrl>> ctrl->device->devt = MKDEV(MAJOR(nvme_chr_devt), ctrl->instance);
+ *   - drivers/nvme/host/core.c|4480| <<nvme_core_init>> result = alloc_chrdev_region(&nvme_chr_devt, 0, NVME_MINORS, "nvme");
+ *   - drivers/nvme/host/core.c|4501| <<nvme_core_init>> unregister_chrdev_region(nvme_chr_devt, NVME_MINORS);
+ *   - drivers/nvme/host/core.c|4516| <<nvme_core_exit>> unregister_chrdev_region(nvme_chr_devt, NVME_MINORS);
+ */
 static dev_t nvme_chr_devt;
 static struct class *nvme_class;
 static struct class *nvme_subsys_class;
@@ -94,6 +116,11 @@ static void nvme_put_subsystem(struct nvme_subsystem *subsys);
 static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 					   unsigned nsid);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1461| <<nvme_update_formats>> nvme_set_queue_dying(ns);
+ *   - drivers/nvme/host/core.c|4326| <<nvme_kill_queues>> nvme_set_queue_dying(ns);
+ */
 static void nvme_set_queue_dying(struct nvme_ns *ns)
 {
 	/*
@@ -108,14 +135,32 @@ static void nvme_set_queue_dying(struct nvme_ns *ns)
 	/*
 	 * Revalidate after unblocking dispatchers that may be holding bd_butex
 	 */
+	/*
+	 * This routine is a wrapper for lower-level driver's revalidate_disk
+	 * call-backs.  It is used to do common pre and post operations needed
+	 * for all revalidate_disk operations.
+	 */
 	revalidate_disk(ns->disk);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1476| <<nvme_passthru_end>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|3114| <<nvme_dev_ioctl>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|3148| <<nvme_sysfs_rescan>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|4043| <<nvme_handle_aen_notice>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|4122| <<nvme_start_ctrl>> nvme_queue_scan(ctrl);
+ *
+ * # echo 1 > /sys/block/nvme0n1/device/rescan_controller
+ */
 static void nvme_queue_scan(struct nvme_ctrl *ctrl)
 {
 	/*
 	 * Only new queue scan work when admin and IO queues are both alive
 	 */
+	/*
+	 * nvme_scan_work()
+	 */
 	if (ctrl->state == NVME_CTRL_LIVE && ctrl->tagset)
 		queue_work(nvme_wq, &ctrl->scan_work);
 }
@@ -126,6 +171,13 @@ static void nvme_queue_scan(struct nvme_ctrl *ctrl)
  * code paths that can't be interrupted by other reset attempts. A hot removal
  * may prevent this from succeeding.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4085| <<nvme_fw_act_work>> nvme_try_sched_reset(ctrl);
+ *   - drivers/nvme/host/pci.c|3529| <<nvme_reset_done>> if (!nvme_try_sched_reset(&dev->ctrl))
+ *   - drivers/nvme/host/pci.c|3595| <<nvme_resume>> return nvme_try_sched_reset(&ndev->ctrl);
+ *   - drivers/nvme/host/pci.c|3675| <<nvme_simple_resume>> return nvme_try_sched_reset(&ndev->ctrl);
+ */
 int nvme_try_sched_reset(struct nvme_ctrl *ctrl)
 {
 	if (ctrl->state != NVME_CTRL_RESETTING)
@@ -136,6 +188,19 @@ int nvme_try_sched_reset(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_try_sched_reset);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|195| <<nvme_reset_ctrl_sync>> ret = nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|1080| <<nvme_keep_alive_work>> nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|776| <<nvme_fc_ctrl_connectivity_loss>> if (nvme_reset_ctrl(&ctrl->ctrl)) {
+ *   - drivers/nvme/host/fc.c|2106| <<nvme_fc_error_recovery>> nvme_reset_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/multipath.c|114| <<nvme_failover_req>> nvme_reset_ctrl(ns->ctrl);
+ *   - drivers/nvme/host/multipath.c|591| <<nvme_anatt_timeout>> nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/pci.c|1485| <<nvme_timeout>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|1532| <<nvme_timeout>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3492| <<nvme_probe>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3720| <<nvme_slot_reset>> nvme_reset_ctrl(&dev->ctrl);
+ */
 int nvme_reset_ctrl(struct nvme_ctrl *ctrl)
 {
 	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))
@@ -146,12 +211,21 @@ int nvme_reset_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_reset_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3157| <<nvme_dev_ioctl>> return nvme_reset_ctrl_sync(ctrl);
+ *   - drivers/nvme/host/core.c|3182| <<nvme_sysfs_reset>> ret = nvme_reset_ctrl_sync(ctrl);
+ */
 int nvme_reset_ctrl_sync(struct nvme_ctrl *ctrl)
 {
 	int ret;
 
 	ret = nvme_reset_ctrl(ctrl);
 	if (!ret) {
+		/*
+		 * wait for a work to finish executing the last queueing instance
+		 * @work: the work to flush
+		 */
 		flush_work(&ctrl->reset_work);
 		if (ctrl->state != NVME_CTRL_LIVE)
 			ret = -ENETRESET;
@@ -161,6 +235,11 @@ int nvme_reset_ctrl_sync(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_reset_ctrl_sync);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|247| <<nvme_delete_ctrl_work>> nvme_do_delete_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|272| <<nvme_delete_ctrl_sync>> nvme_do_delete_ctrl(ctrl);
+ */
 static void nvme_do_delete_ctrl(struct nvme_ctrl *ctrl)
 {
 	dev_info(ctrl->device,
@@ -169,6 +248,9 @@ static void nvme_do_delete_ctrl(struct nvme_ctrl *ctrl)
 	flush_work(&ctrl->reset_work);
 	nvme_stop_ctrl(ctrl);
 	nvme_remove_namespaces(ctrl);
+	/*
+	 * 没找到pci的实现
+	 */
 	ctrl->ops->delete_ctrl(ctrl);
 	nvme_uninit_ctrl(ctrl);
 	nvme_put_ctrl(ctrl);
@@ -182,6 +264,22 @@ static void nvme_delete_ctrl_work(struct work_struct *work)
 	nvme_do_delete_ctrl(ctrl);
 }
 
+/*
+ * 调用者没有pci:
+ *   - drivers/nvme/host/fc.c|780| <<nvme_fc_ctrl_connectivity_loss>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/fc.c|847| <<nvme_fc_unregister_remoteport>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/fc.c|2909| <<nvme_fc_reconnect_or_delete>> WARN_ON(nvme_delete_ctrl(&ctrl->ctrl));
+ *   - drivers/nvme/host/fc.c|3521| <<nvme_fc_delete_controllers>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|977| <<nvme_rdma_reconnect_or_remove>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|2101| <<nvme_rdma_remove_one>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|2141| <<nvme_rdma_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1822| <<nvme_tcp_reconnect_or_remove>> nvme_delete_ctrl(ctrl);
+ *   - drivers/nvme/host/tcp.c|2409| <<nvme_tcp_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|438| <<nvme_loop_delete_ctrl>> nvme_delete_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|713| <<nvme_loop_cleanup_module>> nvme_delete_ctrl(&ctrl->ctrl);
+ *
+ * 调用者没有pci
+ */
 int nvme_delete_ctrl(struct nvme_ctrl *ctrl)
 {
 	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_DELETING))
@@ -192,6 +290,10 @@ int nvme_delete_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_delete_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3449| <<nvme_sysfs_delete>> nvme_delete_ctrl_sync(ctrl);
+ */
 static int nvme_delete_ctrl_sync(struct nvme_ctrl *ctrl)
 {
 	int ret = 0;
@@ -209,11 +311,24 @@ static int nvme_delete_ctrl_sync(struct nvme_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|793| <<nvme_setup_rw>> if (WARN_ON_ONCE(!nvme_ns_has_pi(ns)))
+ *   - drivers/nvme/host/core.c|2011| <<nvme_update_disk_info>> if ((ns->ms && !nvme_ns_has_pi(ns) && !blk_get_integrity(disk)) ||
+ */
 static inline bool nvme_ns_has_pi(struct nvme_ns *ns)
 {
 	return ns->pi_type && ns->ms == sizeof(struct t10_pi_tuple);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|391| <<nvme_complete_rq>> blk_status_t status = nvme_error_status(nvme_req(req)->status);
+ *   - drivers/nvme/host/core.c|2148| <<nvme_revalidate_disk>> ret = blk_status_to_errno(nvme_error_status(ret));
+ *   - drivers/nvme/host/core.c|3681| <<nvme_alloc_ns_head>> ret = blk_status_to_errno(nvme_error_status(ret));
+ *   - drivers/nvme/host/core.c|3731| <<nvme_init_ns_head>> ret = blk_status_to_errno(nvme_error_status(ret));
+ *   - drivers/nvme/host/core.c|3920| <<nvme_alloc_ns>> ret = blk_status_to_errno(nvme_error_status(ret));
+ */
 static blk_status_t nvme_error_status(u16 status)
 {
 	switch (status & 0x7ff) {
@@ -252,10 +367,21 @@ static blk_status_t nvme_error_status(u16 status)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|400| <<nvme_complete_rq>> if (unlikely(status != BLK_STS_OK && nvme_req_needs_retry(req))) {
+ */
 static inline bool nvme_req_needs_retry(struct request *req)
 {
 	if (blk_noretry_request(req))
 		return false;
+	/*
+	 * Do Not Retry (DNR): If set to '1', indicates that if the same command is re-submitted to any
+	 * controller in the NVM subsystem, then that re-submitted command is expected to fail. If cleared to
+	 * '0', indicates that the same command may succeed if retried. If a command is aborted due to time
+	 * limited error recovery (refer to section 5.21.1.5), this bit should be cleared to '0'. If the SCT and
+	 * SC fields are cleared to 0h, then this bit should be cleared to '0'.
+	 */
 	if (nvme_req(req)->status & NVME_SC_DNR)
 		return false;
 	if (nvme_req(req)->retries >= nvme_max_retries)
@@ -263,6 +389,10 @@ static inline bool nvme_req_needs_retry(struct request *req)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|408| <<nvme_complete_rq>> nvme_retry_req(req);
+ */
 static void nvme_retry_req(struct request *req)
 {
 	struct nvme_ns *ns = req->q->queuedata;
@@ -270,6 +400,18 @@ static void nvme_retry_req(struct request *req)
 	u16 crd;
 
 	/* The mask and shift result must be <= 3 */
+	/*
+	 * Command Retry Delay (CRD): If the DNR bit is cleared to ‘0’ and the host has set the Advanced
+	 * Command Retry Enable (ACRE) field to 1h in the Host Behavior Support feature (refer to section
+	 * 5.21.1.22), then:
+	 * a) a 00b CRD value indicates a command retry delay time of zero (i.e., the host may retry the
+	 * command immediately); and
+	 * b) a non-zero CRD value selects a field in the Identify Controller data structure (refer to Figure
+	 * 247) that indicates the command retry delay time:
+	 *   - a 01b CRD value selects the Command Retry Delay Time 1 (CRDT1) field;
+	 *   - a 10b CRD value selects the Command Retry Delay Time 2 (CRDT2) field; and
+	 *   - a 11b CRD value selects the Command Retry Delay Time 3 (CRDT3) field.
+	 */
 	crd = (nvme_req(req)->status & NVME_SC_CRD) >> 11;
 	if (ns && crd)
 		delay = ns->ctrl->crdt[crd - 1] * 100;
@@ -279,6 +421,17 @@ static void nvme_retry_req(struct request *req)
 	blk_mq_delay_kick_requeue_list(req->q, delay);
 }
 
+/*
+ * called by:
+ *   - struct blk_mq_ops nvme_tcp_mq_ops.complete = nvme_complete_rq()
+ *   - struct blk_mq_ops nvme_tcp_admin_mq_ops.complete = nvme_complete_rq()
+ *   - drivers/nvme/host/fabrics.c|557| <<nvmf_fail_nonready_command>> nvme_complete_rq(rq);
+ *   - drivers/nvme/host/fc.c|2402| <<nvme_fc_complete_rq>> nvme_complete_rq(rq);
+ *   - drivers/nvme/host/pci.c|1202| <<nvme_pci_complete_rq>> nvme_complete_rq(req);
+ *   - drivers/nvme/host/rdma.c|1814| <<nvme_rdma_complete_rq>> nvme_complete_rq(rq);
+ *   - drivers/nvme/host/trace.h|85| <<__field>> TRACE_EVENT(nvme_complete_rq,
+ *   - drivers/nvme/target/loop.c|80| <<nvme_loop_complete_rq>> nvme_complete_rq(req);
+ */
 void nvme_complete_rq(struct request *req)
 {
 	blk_status_t status = nvme_error_status(nvme_req(req)->status);
@@ -290,6 +443,9 @@ void nvme_complete_rq(struct request *req)
 	if (nvme_req(req)->ctrl->kas)
 		nvme_req(req)->ctrl->comp_seen = true;
 
+	/*
+	 * 只在这里调用nvme_req_needs_retry()
+	 */
 	if (unlikely(status != BLK_STS_OK && nvme_req_needs_retry(req))) {
 		if ((req->cmd_flags & REQ_NVME_MPATH) &&
 		    blk_path_error(status)) {
@@ -298,6 +454,9 @@ void nvme_complete_rq(struct request *req)
 		}
 
 		if (!blk_queue_dying(req->q)) {
+			/*
+			 * 只在这里调用nvme_retry_req()
+			 */
 			nvme_retry_req(req);
 			return;
 		}
@@ -308,6 +467,17 @@ void nvme_complete_rq(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_complete_rq);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3082| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3083| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|920| <<nvme_rdma_teardown_admin_queue>> nvme_cancel_request, &ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|936| <<nvme_rdma_teardown_io_queues>> nvme_cancel_request, &ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1781| <<nvme_tcp_teardown_admin_queue>> nvme_cancel_request, ctrl);
+ *   - drivers/nvme/host/tcp.c|1798| <<nvme_tcp_teardown_io_queues>> nvme_cancel_request, ctrl);
+ *   - drivers/nvme/target/loop.c|411| <<nvme_loop_shutdown_ctrl>> nvme_cancel_request, &ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|421| <<nvme_loop_shutdown_ctrl>> nvme_cancel_request, &ctrl->ctrl);
+ */
 bool nvme_cancel_request(struct request *req, void *data, bool reserved)
 {
 	dev_dbg_ratelimited(((struct nvme_ctrl *) data)->device,
@@ -391,6 +561,12 @@ bool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,
 
 	if (changed) {
 		ctrl->state = new_state;
+		/*
+		 * 在以下使用state_wq:
+		 *   - drivers/nvme/host/core.c|535| <<nvme_change_ctrl_state>> wake_up_all(&ctrl->state_wq);
+		 *   - drivers/nvme/host/core.c|571| <<nvme_wait_reset>> wait_event(ctrl->state_wq,
+		 *   - drivers/nvme/host/core.c|4515| <<nvme_init_ctrl>> init_waitqueue_head(&ctrl->state_wq);
+		 */
 		wake_up_all(&ctrl->state_wq);
 	}
 
@@ -425,6 +601,10 @@ static bool nvme_state_terminal(struct nvme_ctrl *ctrl)
  * Waits for the controller state to be resetting, or returns false if it is
  * not possible to ever transition to that state.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3102| <<nvme_disable_prepare_reset>> if (!nvme_wait_reset(&dev->ctrl))
+ */
 bool nvme_wait_reset(struct nvme_ctrl *ctrl)
 {
 	wait_event(ctrl->state_wq,
@@ -434,6 +614,10 @@ bool nvme_wait_reset(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_wait_reset);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|593| <<nvme_put_ns_head>> kref_put(&head->ref, nvme_free_ns_head);
+ */
 static void nvme_free_ns_head(struct kref *ref)
 {
 	struct nvme_ns_head *head =
@@ -479,6 +663,16 @@ static inline void nvme_clear_nvme_request(struct request *req)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|829| <<__nvme_submit_sync_cmd>> req = nvme_alloc_request(q, cmd, flags, qid);
+ *   - drivers/nvme/host/core.c|912| <<nvme_submit_user_cmd>> req = nvme_alloc_request(q, cmd, 0, NVME_QID_ANY);
+ *   - drivers/nvme/host/core.c|986| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+ *   - drivers/nvme/host/lightnvm.c|656| <<nvme_nvm_alloc_request>> rq = nvme_alloc_request(q, (struct nvme_command *)cmd, 0, NVME_QID_ANY);
+ *   - drivers/nvme/host/lightnvm.c|770| <<nvme_nvm_submit_user_cmd>> rq = nvme_alloc_request(q, (struct nvme_command *)vcmd, 0,
+ *   - drivers/nvme/host/pci.c|1346| <<nvme_timeout>> abort_req = nvme_alloc_request(dev->ctrl.admin_q, &cmd,
+ *   - drivers/nvme/host/pci.c|2255| <<nvme_delete_queue>> req = nvme_alloc_request(q, &cmd, BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
+ */
 struct request *nvme_alloc_request(struct request_queue *q,
 		struct nvme_command *cmd, blk_mq_req_flags_t flags, int qid)
 {
@@ -523,6 +717,10 @@ static int nvme_disable_streams(struct nvme_ctrl *ctrl)
 	return nvme_toggle_streams(ctrl, false);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|709| <<nvme_configure_directives>> ret = nvme_enable_streams(ctrl);
+ */
 static int nvme_enable_streams(struct nvme_ctrl *ctrl)
 {
 	return nvme_toggle_streams(ctrl, true);
@@ -545,6 +743,10 @@ static int nvme_get_stream_params(struct nvme_ctrl *ctrl,
 	return nvme_submit_sync_cmd(ctrl->admin_q, &c, s, sizeof(*s));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3237| <<nvme_init_identify>> ret = nvme_configure_directives(ctrl);
+ */
 static int nvme_configure_directives(struct nvme_ctrl *ctrl)
 {
 	struct streams_directive_params s;
@@ -580,6 +782,10 @@ static int nvme_configure_directives(struct nvme_ctrl *ctrl)
  * Check if 'req' has a write hint associated with it. If it does, assign
  * a valid namespace stream to the write.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|859| <<nvme_setup_rw>> nvme_assign_write_stream(ctrl, req, &control, &dsmgmt);
+ */
 static void nvme_assign_write_stream(struct nvme_ctrl *ctrl,
 				     struct request *req, u16 *control,
 				     u32 *dsmgmt)
@@ -684,6 +890,10 @@ static inline blk_status_t nvme_setup_write_zeroes(struct nvme_ns *ns,
 	return BLK_STS_OK;
 }
 
+/*
+ * 处理REQ_OP_READ和REQ_OP_WRITE:
+ *   - drivers/nvme/host/core.c|978| <<nvme_setup_cmd>> ret = nvme_setup_rw(ns, req, cmd);
+ */
 static inline blk_status_t nvme_setup_rw(struct nvme_ns *ns,
 		struct request *req, struct nvme_command *cmnd)
 {
@@ -738,6 +948,16 @@ static inline blk_status_t nvme_setup_rw(struct nvme_ns *ns,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|395| <<nvme_complete_rq>> nvme_cleanup_cmd(req);
+ *   - drivers/nvme/host/fc.c|2269| <<nvme_fc_start_fcp_op>> nvme_cleanup_cmd(op->rq);
+ *   - drivers/nvme/host/fc.c|2309| <<nvme_fc_start_fcp_op>> nvme_cleanup_cmd(op->rq);
+ *   - drivers/nvme/host/pci.c|1056| <<nvme_queue_rq>> nvme_cleanup_cmd(req);
+ *   - drivers/nvme/host/rdma.c|1791| <<nvme_rdma_queue_rq>> nvme_cleanup_cmd(rq);
+ *   - drivers/nvme/host/tcp.c|2139| <<nvme_tcp_setup_cmd_pdu>> nvme_cleanup_cmd(rq);
+ *   - drivers/nvme/target/loop.c|160| <<nvme_loop_queue_rq>> nvme_cleanup_cmd(req);
+ */
 void nvme_cleanup_cmd(struct request *req)
 {
 	if (req->rq_flags & RQF_SPECIAL_PAYLOAD) {
@@ -752,6 +972,15 @@ void nvme_cleanup_cmd(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/trace.h|47| <<global>> TRACE_EVENT(nvme_setup_cmd,
+ *   - drivers/nvme/host/fc.c|2342| <<nvme_fc_queue_rq>> ret = nvme_setup_cmd(ns, rq, sqe);
+ *   - drivers/nvme/host/pci.c|896| <<nvme_queue_rq>> ret = nvme_setup_cmd(ns, req, &cmnd);
+ *   - drivers/nvme/host/rdma.c|1759| <<nvme_rdma_queue_rq>> ret = nvme_setup_cmd(ns, rq, c);
+ *   - drivers/nvme/host/tcp.c|2106| <<nvme_tcp_setup_cmd_pdu>> ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+ *   - drivers/nvme/target/loop.c|144| <<nvme_loop_queue_rq>> ret = nvme_setup_cmd(ns, req, &iod->cmd);
+ */
 blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmd)
 {
@@ -763,6 +992,9 @@ blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 	switch (req_op(req)) {
 	case REQ_OP_DRV_IN:
 	case REQ_OP_DRV_OUT:
+		/*
+		 * 用blk_mq_rq_to_pdu(req)返回struct nvme_request
+		 */
 		memcpy(cmd, nvme_req(req)->cmd, sizeof(*cmd));
 		break;
 	case REQ_OP_FLUSH:
@@ -789,6 +1021,10 @@ blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 }
 EXPORT_SYMBOL_GPL(nvme_setup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1008| <<nvme_execute_rq_polled>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, nvme_end_sync_rq);
+ */
 static void nvme_end_sync_rq(struct request *rq, blk_status_t error)
 {
 	struct completion *waiting = rq->end_io_data;
@@ -797,6 +1033,10 @@ static void nvme_end_sync_rq(struct request *rq, blk_status_t error)
 	complete(waiting);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1052| <<__nvme_submit_sync_cmd>> nvme_execute_rq_polled(req->q, NULL, req, at_head);
+ */
 static void nvme_execute_rq_polled(struct request_queue *q,
 		struct gendisk *bd_disk, struct request *rq, int at_head)
 {
@@ -818,6 +1058,17 @@ static void nvme_execute_rq_polled(struct request_queue *q,
  * Returns 0 on success.  If the result is negative, it's a Linux error code;
  * if the result is positive, it's an NVM Express status code
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|947| <<nvme_submit_sync_cmd>> return __nvme_submit_sync_cmd(q, cmd, NULL, buffer, bufflen, 0,
+ *   - drivers/nvme/host/core.c|1300| <<nvme_features>> ret = __nvme_submit_sync_cmd(dev->admin_q, &c, &res,
+ *   - drivers/nvme/host/core.c|2188| <<nvme_sec_submit>> return __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, NULL, buffer, len,
+ *   - drivers/nvme/host/fabrics.c|153| <<nvmf_reg_read32>> ret = __nvme_submit_sync_cmd(ctrl->fabrics_q, &cmd, &res, NULL, 0, 0,
+ *   - drivers/nvme/host/fabrics.c|200| <<nvmf_reg_read64>> ret = __nvme_submit_sync_cmd(ctrl->fabrics_q, &cmd, &res, NULL, 0, 0,
+ *   - drivers/nvme/host/fabrics.c|246| <<nvmf_reg_write32>> ret = __nvme_submit_sync_cmd(ctrl->fabrics_q, &cmd, NULL, NULL, 0, 0,
+ *   - drivers/nvme/host/fabrics.c|399| <<nvmf_connect_admin_queue>> ret = __nvme_submit_sync_cmd(ctrl->fabrics_q, &cmd, &res,
+ *   - drivers/nvme/host/fabrics.c|462| <<nvmf_connect_io_queue>> ret = __nvme_submit_sync_cmd(ctrl->connect_q, &cmd, &res,
+ */
 int __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
 		union nvme_result *result, void *buffer, unsigned bufflen,
 		unsigned timeout, int qid, int at_head,
@@ -854,6 +1105,25 @@ int __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
 }
 EXPORT_SYMBOL_GPL(__nvme_submit_sync_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|593| <<nvme_toggle_streams>> return nvme_submit_sync_cmd(ctrl->admin_q, &c, NULL, 0);
+ *   - drivers/nvme/host/core.c|620| <<nvme_get_stream_params>> return nvme_submit_sync_cmd(ctrl->admin_q, &c, s, sizeof(*s));
+ *   - drivers/nvme/host/core.c|1169| <<nvme_identify_ctrl>> error = nvme_submit_sync_cmd(dev->admin_q, &c, *id,
+ *   - drivers/nvme/host/core.c|1193| <<nvme_identify_ns_descs>> status = nvme_submit_sync_cmd(ctrl->admin_q, &c, data,
+ *   - drivers/nvme/host/core.c|1255| <<nvme_identify_ns_list>> return nvme_submit_sync_cmd(dev->admin_q, &c, ns_list,
+ *   - drivers/nvme/host/core.c|1279| <<nvme_identify_ns>> error = nvme_submit_sync_cmd(ctrl->admin_q, &c, *id, sizeof(**id));
+ *   - drivers/nvme/host/core.c|2113| <<nvme_pr_command>> ret = nvme_submit_sync_cmd(ns->queue, &c, data, 16);
+ *   - drivers/nvme/host/core.c|2894| <<nvme_get_log>> return nvme_submit_sync_cmd(ctrl->admin_q, &c, log, size);
+ *   - drivers/nvme/host/lightnvm.c|445| <<nvme_nvm_identity>> ret = nvme_submit_sync_cmd(ns->ctrl->admin_q, (struct nvme_command *)&c,
+ *   - drivers/nvme/host/lightnvm.c|496| <<nvme_nvm_get_bb_tbl>> ret = nvme_submit_sync_cmd(ctrl->admin_q, (struct nvme_command *)&c,
+ *   - drivers/nvme/host/lightnvm.c|544| <<nvme_nvm_set_bb_tbl>> ret = nvme_submit_sync_cmd(ns->ctrl->admin_q, (struct nvme_command *)&c,
+ *   - drivers/nvme/host/pci.c|429| <<nvme_dbbuf_set>> if (nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0)) {
+ *   - drivers/nvme/host/pci.c|1323| <<adapter_delete_queue>> return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+ *   - drivers/nvme/host/pci.c|1354| <<adapter_alloc_cq>> return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+ *   - drivers/nvme/host/pci.c|1391| <<adapter_alloc_sq>> return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+ *   - drivers/nvme/host/pci.c|2319| <<nvme_set_host_mem>> ret = nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+ */
 int nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
 		void *buffer, unsigned bufflen)
 {
@@ -862,6 +1132,10 @@ int nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
 }
 EXPORT_SYMBOL_GPL(nvme_submit_sync_cmd);
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/core.c|1168| <<nvme_submit_user_cmd>> meta = nvme_add_user_metadata(bio, meta_buffer, meta_len,
+ */
 static void *nvme_add_user_metadata(struct bio *bio, void __user *ubuf,
 		unsigned len, u32 seed, bool write)
 {
@@ -896,6 +1170,12 @@ static void *nvme_add_user_metadata(struct bio *bio, void __user *ubuf,
 	return ERR_PTR(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1589| <<nvme_submit_io>> return nvme_submit_user_cmd(ns->queue, &c,
+ *   - drivers/nvme/host/core.c|1742| <<nvme_user_cmd>> status = nvme_submit_user_cmd(ns ? ns->queue : ctrl->admin_q, &c,
+ *   - drivers/nvme/host/core.c|1789| <<nvme_user_cmd64>> status = nvme_submit_user_cmd(ns ? ns->queue : ctrl->admin_q, &c,
+ */
 static int nvme_submit_user_cmd(struct request_queue *q,
 		struct nvme_command *cmd, void __user *ubuffer,
 		unsigned bufflen, void __user *meta_buffer, unsigned meta_len,
@@ -1018,6 +1298,43 @@ static void nvme_keep_alive_work(struct work_struct *work)
 	}
 }
 
+/*
+ * commit 038bd4cb6766c69b5b9c77507f389cc718a36842
+ * Author: Sagi Grimberg <sagi@grimberg.me>
+ * Date:   Mon Jun 13 16:45:28 2016 +0200
+ *
+ * nvme: add keep-alive support
+ *
+ * Periodic keep-alive is a mandatory feature in NVMe over Fabrics, and
+ * optional in NVMe 1.2.1 for PCIe.  This patch adds periodic keep-alive
+ * sent from the host to verify that the controller is still responsive
+ * and vice-versa.  The keep-alive timeout is user-defined (with
+ * keep_alive_tmo connection parameter) and defaults to 5 seconds.
+ *
+ * In order to avoid a race condition where the host sends a keep-alive
+ * competing with the target side keep-alive timeout expiration, the host
+ * adds a grace period of 10 seconds when publishing the keep-alive timeout
+ * to the target.
+ *
+ * In case a keep-alive failed (or timed out), a transport specific error
+ * recovery kicks in.
+ *
+ * For now only NVMe over Fabrics is wired up to support keep alive, but
+ * we can add PCIe support easily once controllers actually supporting it
+ * become available.
+ *
+ * Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
+ * Reviewed-by: Steve Wise <swise@chelsio.com>
+ * Signed-off-by: Christoph Hellwig <hch@lst.de>
+ * Reviewed-by: Keith Busch <keith.busch@intel.com>
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4478| <<nvme_start_ctrl>> nvme_start_keep_alive(ctrl);
+ *
+ * 目前没见到pci用keep-alive
+ */
 static void nvme_start_keep_alive(struct nvme_ctrl *ctrl)
 {
 	if (unlikely(ctrl->kato == 0))
@@ -1035,6 +1352,11 @@ void nvme_stop_keep_alive(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_stop_keep_alive);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3128| <<nvme_init_identify>> ret = nvme_identify_ctrl(ctrl, &id);
+ *   - drivers/nvme/host/core.c|4199| <<nvme_scan_work>> if (nvme_identify_ctrl(ctrl, &id))
+ */
 static int nvme_identify_ctrl(struct nvme_ctrl *dev, struct nvme_id_ctrl **id)
 {
 	struct nvme_command c = { };
@@ -1055,6 +1377,10 @@ static int nvme_identify_ctrl(struct nvme_ctrl *dev, struct nvme_id_ctrl **id)
 	return error;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2081| <<nvme_report_ns_ids>> ret = nvme_identify_ns_descs(ctrl, nsid, ids);
+ */
 static int nvme_identify_ns_descs(struct nvme_ctrl *ctrl, unsigned nsid,
 		struct nvme_ns_ids *ids)
 {
@@ -1138,12 +1464,22 @@ static int nvme_identify_ns_list(struct nvme_ctrl *dev, unsigned nsid, __le32 *n
 				    NVME_IDENTIFY_DATA_SIZE);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2068| <<nvme_revalidate_disk>> ret = nvme_identify_ns(ctrl, ns->head->ns_id, &id);
+ *   - drivers/nvme/host/core.c|3782| <<nvme_alloc_ns>> ret = nvme_identify_ns(ctrl, nsid, &id);
+ */
 static int nvme_identify_ns(struct nvme_ctrl *ctrl,
 		unsigned nsid, struct nvme_id_ns **id)
 {
 	struct nvme_command c = { };
 	int error;
 
+	/*
+	 * The Identify command returns a data buffer that describes information about the NVM subsystem, the
+	 * controller or the namespace(s). The data structure is 4,096 bytes in size.
+	 */
+
 	/* gcc-4.4.4 (at least) has issues with initializers and anon unions */
 	c.identify.opcode = nvme_admin_identify;
 	c.identify.nsid = cpu_to_le32(nsid);
@@ -1181,6 +1517,16 @@ static int nvme_features(struct nvme_ctrl *dev, u8 op, unsigned int fid,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1520| <<nvme_set_queue_count>> status = nvme_set_features(ctrl, NVME_FEAT_NUM_QUEUES, q_count, NULL, 0,
+ *   - drivers/nvme/host/core.c|1554| <<nvme_enable_aen>> status = nvme_set_features(ctrl, NVME_FEAT_ASYNC_EVENT, supported_aens,
+ *   - drivers/nvme/host/core.c|2589| <<nvme_configure_timestamp>> ret = nvme_set_features(ctrl, NVME_FEAT_TIMESTAMP, 0, &ts, sizeof(ts),
+ *   - drivers/nvme/host/core.c|2611| <<nvme_configure_acre>> ret = nvme_set_features(ctrl, NVME_FEAT_HOST_BEHAVIOR, 0,
+ *   - drivers/nvme/host/core.c|2731| <<nvme_configure_apst>> ret = nvme_set_features(ctrl, NVME_FEAT_AUTO_PST, apste,
+ *   - drivers/nvme/host/hwmon.c|55| <<nvme_set_temp_thresh>> ret = nvme_set_features(ctrl, NVME_FEAT_TEMP_THRESH, threshold, NULL, 0,
+ *   - drivers/nvme/host/pci.c|3619| <<nvme_set_power_state>> return nvme_set_features(ctrl, NVME_FEAT_POWER_MGMT, ps, NULL, 0, NULL);
+ */
 int nvme_set_features(struct nvme_ctrl *dev, unsigned int fid,
 		      unsigned int dword11, void *buffer, size_t buflen,
 		      u32 *result)
@@ -1199,6 +1545,17 @@ int nvme_get_features(struct nvme_ctrl *dev, unsigned int fid,
 }
 EXPORT_SYMBOL_GPL(nvme_get_features);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2449| <<nvme_fc_create_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/fc.c|2523| <<nvme_fc_recreate_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2538| <<nvme_setup_io_queues>> result = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/rdma.c|664| <<nvme_rdma_alloc_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/tcp.c|1635| <<nvme_tcp_alloc_io_queues>> ret = nvme_set_queue_count(ctrl, &nr_io_queues);
+ *   - drivers/nvme/target/loop.c|299| <<nvme_loop_init_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ *   
+ * 通过admin queue设置
+ */
 int nvme_set_queue_count(struct nvme_ctrl *ctrl, int *count)
 {
 	u32 q_count = (*count - 1) | ((*count - 1) << 16);
@@ -1231,6 +1588,12 @@ EXPORT_SYMBOL_GPL(nvme_set_queue_count);
 	(NVME_AEN_CFG_NS_ATTR | NVME_AEN_CFG_FW_ACT | \
 	 NVME_AEN_CFG_ANA_CHANGE | NVME_AEN_CFG_DISC_CHANGE)
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4499| <<nvme_start_ctrl>> nvme_enable_aen(ctrl);
+ *
+ * Asynchronous Event Notification
+ */
 static void nvme_enable_aen(struct nvme_ctrl *ctrl)
 {
 	u32 result, supported_aens = ctrl->oaes & NVME_AEN_SUPPORTED;
@@ -1245,9 +1608,21 @@ static void nvme_enable_aen(struct nvme_ctrl *ctrl)
 		dev_warn(ctrl->device, "Failed to configure AEN (cfg %x)\n",
 			 supported_aens);
 
+	/*
+	 * 在以下使用async_event_work:
+	 *   - drivers/nvme/host/core.c|1580| <<nvme_enable_aen>> queue_work(nvme_wq, &ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4345| <<nvme_async_event_work>> container_of(work, struct nvme_ctrl, async_event_work);
+	 *   - drivers/nvme/host/core.c|4483| <<nvme_complete_async_event>> queue_work(nvme_wq, &ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4500| <<nvme_stop_ctrl>> flush_work(&ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4589| <<nvme_init_ctrl>> INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
+	 */
 	queue_work(nvme_wq, &ctrl->async_event_work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1936| <<nvme_ioctl>> ret = nvme_submit_io(ns, argp);
+ */
 static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
 {
 	struct nvme_user_io io;
@@ -1312,6 +1687,34 @@ static u32 nvme_known_admin_effects(u8 opcode)
 	return 0;
 }
 
+/*
+ * commit 84fef62d135b6e47b52f4e9280b5dbc5bb0050ba
+ * Author: Keith Busch <keith.busch@intel.com>
+ * Date:   Tue Nov 7 10:28:32 2017 -0700
+ *
+ * nvme: check admin passthru command effects
+ *
+ * The NVMe standard provides a command effects log page so the host may
+ * be aware of special requirements it may need to do for a particular
+ * command. For example, the command may need to run with IO quiesced to
+ * prevent timeouts or undefined behavior, or it may change the logical block
+ * formats that determine how the host needs to construct future commands.
+ *
+ * This patch saves the nvme command effects log page if the controller
+ * supports it, and performs appropriate actions before and after an admin
+ * passthrough command is completed. If the controller does not support the
+ * command effects log page, the driver will define the effects for known
+ * opcodes. The nvme format and santize are the only commands in this patch
+ * with known effects.
+ *
+ * Signed-off-by: Keith Busch <keith.busch@intel.com>
+ * Signed-off-by: Christoph Hellwig <hch@lst.de>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|1473| <<nvme_user_cmd>> effects = nvme_passthru_start(ctrl, ns, cmd.opcode);
+ *   - drivers/nvme/host/core.c|1520| <<nvme_user_cmd64>> effects = nvme_passthru_start(ctrl, ns, cmd.opcode);
+ */
 static u32 nvme_passthru_start(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 								u8 opcode)
 {
@@ -1357,6 +1760,11 @@ static void nvme_update_formats(struct nvme_ctrl *ctrl)
 	up_read(&ctrl->namespaces_rwsem);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1451| <<nvme_user_cmd>> nvme_passthru_end(ctrl, effects);
+ *   - drivers/nvme/host/core.c|1498| <<nvme_user_cmd64>> nvme_passthru_end(ctrl, effects);
+ */
 static void nvme_passthru_end(struct nvme_ctrl *ctrl, u32 effects)
 {
 	/*
@@ -1379,6 +1787,13 @@ static void nvme_passthru_end(struct nvme_ctrl *ctrl, u32 effects)
 		nvme_queue_scan(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1894| <<nvme_handle_ctrl_ioctl>> ret = nvme_user_cmd(ctrl, NULL, argp);
+ *   - drivers/nvme/host/core.c|1933| <<nvme_ioctl>> ret = nvme_user_cmd(ns->ctrl, ns, argp);
+ *   - drivers/nvme/host/core.c|3375| <<nvme_dev_user_cmd>> ret = nvme_user_cmd(ctrl, ns, argp);
+ *   - drivers/nvme/host/core.c|3392| <<nvme_dev_ioctl>> return nvme_user_cmd(ctrl, NULL, argp);
+ */
 static int nvme_user_cmd(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 			struct nvme_passthru_cmd __user *ucmd)
 {
@@ -1427,6 +1842,12 @@ static int nvme_user_cmd(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 	return status;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1897| <<nvme_handle_ctrl_ioctl>> ret = nvme_user_cmd64(ctrl, NULL, argp);
+ *   - drivers/nvme/host/core.c|1939| <<nvme_ioctl>> ret = nvme_user_cmd64(ns->ctrl, ns, argp);
+ *   - drivers/nvme/host/core.c|3394| <<nvme_dev_ioctl>> return nvme_user_cmd64(ctrl, NULL, argp);
+ */
 static int nvme_user_cmd64(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 			struct nvme_passthru_cmd64 __user *ucmd)
 {
@@ -1513,6 +1934,10 @@ static bool is_ctrl_ioctl(unsigned int cmd)
 	return false;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1925| <<nvme_ioctl>> return nvme_handle_ctrl_ioctl(ns, cmd, argp, head, srcu_idx);
+ */
 static int nvme_handle_ctrl_ioctl(struct nvme_ns *ns, unsigned int cmd,
 				  void __user *argp,
 				  struct nvme_ns_head *head,
@@ -1624,6 +2049,10 @@ static int nvme_getgeo(struct block_device *bdev, struct hd_geometry *geo)
 }
 
 #ifdef CONFIG_BLK_DEV_INTEGRITY
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2173| <<nvme_update_disk_info>> nvme_init_integrity(disk, ns->ms, ns->pi_type);
+ */
 static void nvme_init_integrity(struct gendisk *disk, u16 ms, u8 pi_type)
 {
 	struct blk_integrity integrity;
@@ -1718,6 +2147,12 @@ static void nvme_config_write_zeroes(struct gendisk *disk, struct nvme_ns *ns)
 					   nvme_lba_to_sect(ns, max_blocks));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2300| <<nvme_revalidate_disk>> ret = nvme_report_ns_ids(ctrl, ns->head->ns_id, id, &ids);
+ *   - drivers/nvme/host/core.c|3905| <<nvme_alloc_ns_head>> ret = nvme_report_ns_ids(ctrl, nsid, id, &head->ids);
+ *   - drivers/nvme/host/core.c|4002| <<nvme_init_ns_head>> ret = nvme_report_ns_ids(ctrl, nsid, id, &ids);
+ */
 static int nvme_report_ns_ids(struct nvme_ctrl *ctrl, unsigned int nsid,
 		struct nvme_id_ns *id, struct nvme_ns_ids *ids)
 {
@@ -1757,6 +2192,11 @@ static bool nvme_ns_ids_equal(struct nvme_ns_ids *a, struct nvme_ns_ids *b)
 		memcmp(&a->eui64, &b->eui64, sizeof(a->eui64)) == 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2248| <<__nvme_revalidate_disk>> nvme_update_disk_info(disk, ns, id);
+ *   - drivers/nvme/host/core.c|2251| <<__nvme_revalidate_disk>> nvme_update_disk_info(ns->head->disk, ns, id);
+ */
 static void nvme_update_disk_info(struct gendisk *disk,
 		struct nvme_ns *ns, struct nvme_id_ns *id)
 {
@@ -1855,6 +2295,9 @@ static void __nvme_revalidate_disk(struct gendisk *disk, struct nvme_id_ns *id)
 #endif
 }
 
+/*
+ * struct block_device_operations nvme_fops.revalidate = nvme_revalidate_disk()
+ */
 static int nvme_revalidate_disk(struct gendisk *disk)
 {
 	struct nvme_ns *ns = disk->private_data;
@@ -1994,6 +2437,25 @@ static int nvme_pr_release(struct block_device *bdev, u64 key, enum pr_type type
 	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_release);
 }
 
+/*
+ * commit bbd3e064362e5057cc4799ba2e4d68c7593e490b
+ * Author: Christoph Hellwig <hch@lst.de>
+ * Date:   Thu Oct 15 14:10:48 2015 +0200
+ *
+ * block: add an API for Persistent Reservations
+ *
+ * This commits adds a driver API and ioctls for controlling Persistent
+ * Reservations s/genericly/generically/ at the block layer.  Persistent
+ * Reservations are supported by SCSI and NVMe and allow controlling who gets
+ * access to a device in a shared storage setup.
+ *
+ * Note that we add a pr_ops structure to struct block_device_operations
+ * instead of adding the members directly to avoid bloating all instances
+ * of devices that will never support Persistent Reservations.
+ *
+ * Signed-off-by: Christoph Hellwig <hch@lst.de>
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ */
 static const struct pr_ops nvme_pr_ops = {
 	.pr_register	= nvme_pr_register,
 	.pr_reserve	= nvme_pr_reserve,
@@ -2050,6 +2512,11 @@ static void nvme_ns_head_release(struct gendisk *disk, fmode_t mode)
 	nvme_put_ns_head(disk->private_data);
 }
 
+/*
+ * 在以下使用nvme_ns_head_ops:
+ *   - drivers/nvme/host/core.c|1877| <<nvme_get_ns_from_disk>> if (disk->fops == &nvme_ns_head_ops) {
+ *   - drivers/nvme/host/multipath.c|425| <<nvme_mpath_alloc_disk>> head->disk->fops = &nvme_ns_head_ops;
+ */
 const struct block_device_operations nvme_ns_head_ops = {
 	.owner		= THIS_MODULE,
 	.open		= nvme_ns_head_open,
@@ -2061,6 +2528,11 @@ const struct block_device_operations nvme_ns_head_ops = {
 };
 #endif /* CONFIG_NVME_MULTIPATH */
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2197| <<nvme_disable_ctrl>> return nvme_wait_ready(ctrl, ctrl->cap, false);
+ *   - drivers/nvme/host/core.c|2236| <<nvme_enable_ctrl>> return nvme_wait_ready(ctrl, ctrl->cap, true);
+ */
 static int nvme_wait_ready(struct nvme_ctrl *ctrl, u64 cap, bool enabled)
 {
 	unsigned long timeout =
@@ -2094,6 +2566,13 @@ static int nvme_wait_ready(struct nvme_ctrl *ctrl, u64 cap, bool enabled)
  * bits', but doing so may cause the device to complete commands to the
  * admin queue ... and we don't know what memory that might be pointing at!
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1572| <<nvme_disable_admin_queue>> nvme_disable_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|1875| <<nvme_pci_configure_admin_queue>> result = nvme_disable_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1894| <<nvme_rdma_shutdown_ctrl>> nvme_disable_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1933| <<nvme_tcp_teardown_ctrl>> nvme_disable_ctrl(ctrl);
+ */
 int nvme_disable_ctrl(struct nvme_ctrl *ctrl)
 {
 	int ret;
@@ -2112,6 +2591,14 @@ int nvme_disable_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_disable_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2672| <<nvme_fc_create_association>> ret = nvme_enable_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|1891| <<nvme_pci_configure_admin_queue>> result = nvme_enable_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|826| <<nvme_rdma_configure_admin_queue>> error = nvme_enable_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1746| <<nvme_tcp_configure_admin_queue>> error = nvme_enable_ctrl(ctrl);
+ *   - drivers/nvme/target/loop.c|380| <<nvme_loop_configure_admin_queue>> error = nvme_enable_ctrl(&ctrl->ctrl);
+ */
 int nvme_enable_ctrl(struct nvme_ctrl *ctrl)
 {
 	/*
@@ -2151,6 +2638,13 @@ int nvme_enable_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_enable_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1631| <<nvme_disable_admin_queue>> nvme_shutdown_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1892| <<nvme_rdma_shutdown_ctrl>> nvme_shutdown_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1931| <<nvme_tcp_teardown_ctrl>> nvme_shutdown_ctrl(ctrl);
+ *   - drivers/nvme/target/loop.c|418| <<nvme_loop_shutdown_ctrl>> nvme_shutdown_ctrl(&ctrl->ctrl);
+ */
 int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl)
 {
 	unsigned long timeout = jiffies + (ctrl->shutdown_timeout * HZ);
@@ -2182,6 +2676,11 @@ int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_shutdown_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3048| <<nvme_init_identify>> nvme_set_queue_limits(ctrl, ctrl->admin_q);
+ *   - drivers/nvme/host/core.c|3780| <<nvme_alloc_ns>> nvme_set_queue_limits(ctrl, ns->queue);
+ */
 static void nvme_set_queue_limits(struct nvme_ctrl *ctrl,
 		struct request_queue *q)
 {
@@ -2204,6 +2703,10 @@ static void nvme_set_queue_limits(struct nvme_ctrl *ctrl,
 	blk_queue_write_cache(q, vwc, vwc);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3345| <<nvme_init_identify>> ret = nvme_configure_timestamp(ctrl);
+ */
 static int nvme_configure_timestamp(struct nvme_ctrl *ctrl)
 {
 	__le64 ts;
@@ -2364,6 +2867,10 @@ static int nvme_configure_apst(struct nvme_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4708| <<nvme_init_ctrl>> ctrl->device->power.set_latency_tolerance = nvme_set_latency_tolerance;
+ */
 static void nvme_set_latency_tolerance(struct device *dev, s32 val)
 {
 	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
@@ -2420,6 +2927,11 @@ static const struct nvme_core_quirk_entry core_quirks[] = {
 };
 
 /* match is null-terminated but idstr is space-padded. */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2915| <<quirk_matches>> string_matches(id->mn, q->mn, sizeof(id->mn)) &&
+ *   - drivers/nvme/host/core.c|2916| <<quirk_matches>> string_matches(id->fr, q->fr, sizeof(id->fr));
+ */
 static bool string_matches(const char *idstr, const char *match, size_t len)
 {
 	size_t matchlen;
@@ -2440,6 +2952,10 @@ static bool string_matches(const char *idstr, const char *match, size_t len)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3273| <<nvme_init_identify>> if (quirk_matches(id, &core_quirks[i]))
+ */
 static bool quirk_matches(const struct nvme_id_ctrl *id,
 			  const struct nvme_core_quirk_entry *q)
 {
@@ -2448,6 +2964,10 @@ static bool quirk_matches(const struct nvme_id_ctrl *id,
 		string_matches(id->fr, q->fr, sizeof(id->fr));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3103| <<nvme_init_subsystem>> nvme_init_subnqn(subsys, ctrl, id);
+ */
 static void nvme_init_subnqn(struct nvme_subsystem *subsys, struct nvme_ctrl *ctrl,
 		struct nvme_id_ctrl *id)
 {
@@ -2466,6 +2986,15 @@ static void nvme_init_subnqn(struct nvme_subsystem *subsys, struct nvme_ctrl *ct
 	}
 
 	/* Generate a "fake" NQN per Figure 254 in NVMe 1.3 + ECN 001 */
+	/*
+	 * 例子: nqn.2014.08.org.nvmexpress:80861af4deadbeaf1           QEMU NVMe Ctrl
+	 *
+	 * # cat /sys/block/nvme0n1/device/subsysnqn 
+	 * nqn.2014.08.org.nvmexpress:80861af4deadbeaf1           QEMU NVMe Ctrl
+	 *
+	 * # cat /sys/block/nvme0n1/device/subsysnqn 
+	 * nqn.2017-03.jp.co.toshiba:KXG60ZNV256G NVMe TOSHIBA 256GB:999A61GOK3JL
+	 */
 	off = snprintf(subsys->subnqn, NVMF_NQN_SIZE,
 			"nqn.2014.08.org.nvmexpress:%04x%04x",
 			le16_to_cpu(id->vid), le16_to_cpu(id->ssvid));
@@ -2583,6 +3112,10 @@ static const struct attribute_group *nvme_subsys_attrs_groups[] = {
 	NULL,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3126| <<nvme_init_subsystem>> if (!nvme_validate_cntlid(subsys, ctrl, id)) {
+ */
 static bool nvme_validate_cntlid(struct nvme_subsystem *subsys,
 		struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 {
@@ -2614,6 +3147,10 @@ static bool nvme_validate_cntlid(struct nvme_subsystem *subsys,
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3015| <<nvme_init_identify>> ret = nvme_init_subsystem(ctrl, id);
+ */
 static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 {
 	struct nvme_subsystem *subsys, *found;
@@ -2689,6 +3226,15 @@ static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2907| <<nvme_get_effects_log>> ret = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_CMD_EFFECTS, 0,
+ *   - drivers/nvme/host/core.c|3936| <<nvme_clear_changed_ns_log>> error = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_CHANGED_NS, 0, log,
+ *   - drivers/nvme/host/core.c|4122| <<nvme_get_fw_slot_info>> if (nvme_get_log(ctrl, NVME_NSID_ALL, 0, NVME_LOG_FW_SLOT, log,
+ *   - drivers/nvme/host/hwmon.c|67| <<nvme_hwmon_get_smart_log>> ret = nvme_get_log(data->ctrl, NVME_NSID_ALL, NVME_LOG_SMART, 0,
+ *   - drivers/nvme/host/lightnvm.c|595| <<nvme_nvm_get_chk_meta>> ret = nvme_get_log(ctrl, ns->head->ns_id,
+ *   - drivers/nvme/host/multipath.c|551| <<nvme_read_ana_log>> error = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_ANA, 0,
+ */
 int nvme_get_log(struct nvme_ctrl *ctrl, u32 nsid, u8 log_page, u8 lsp,
 		void *log, size_t size, u64 offset)
 {
@@ -2707,6 +3253,10 @@ int nvme_get_log(struct nvme_ctrl *ctrl, u32 nsid, u8 log_page, u8 lsp,
 	return nvme_submit_sync_cmd(ctrl->admin_q, &c, log, size);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3270| <<nvme_init_identify>> ret = nvme_get_effects_log(ctrl);
+ */
 static int nvme_get_effects_log(struct nvme_ctrl *ctrl)
 {
 	int ret;
@@ -2731,6 +3281,15 @@ static int nvme_get_effects_log(struct nvme_ctrl *ctrl)
  * register in our nvme_ctrl structure.  This should be called as soon as
  * the admin queue is fully up and running.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1408| <<nvme_passthru_end>> nvme_init_identify(ctrl);
+ *   - drivers/nvme/host/fc.c|2681| <<nvme_fc_create_association>> ret = nvme_init_identify(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|2628| <<nvme_reset_work>> result = nvme_init_identify(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|835| <<nvme_rdma_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1752| <<nvme_tcp_configure_admin_queue>> error = nvme_init_identify(ctrl);
+ *   - drivers/nvme/target/loop.c|389| <<nvme_loop_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+ */
 int nvme_init_identify(struct nvme_ctrl *ctrl)
 {
 	struct nvme_id_ctrl *id;
@@ -2904,6 +3463,10 @@ int nvme_init_identify(struct nvme_ctrl *ctrl)
 	if (ret < 0)
 		return ret;
 
+	/*
+	 * 只在以下修改:
+	 *   - drivers/nvme/host/core.c|3451| <<nvme_init_identify>> ctrl->identified = true;
+	 */
 	if (!ctrl->identified)
 		nvme_hwmon_init(ctrl);
 
@@ -2933,6 +3496,10 @@ static int nvme_dev_open(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3492| <<nvme_dev_ioctl>> return nvme_dev_user_cmd(ctrl, argp);
+ */
 static int nvme_dev_user_cmd(struct nvme_ctrl *ctrl, void __user *argp)
 {
 	struct nvme_ns *ns;
@@ -3270,6 +3837,9 @@ static struct attribute *nvme_dev_attrs[] = {
 	NULL
 };
 
+/*
+ * struct attribute_group nvme_dev_attrs_group.is_visible = nvme_dev_attrs_are_visible()
+ */
 static umode_t nvme_dev_attrs_are_visible(struct kobject *kobj,
 		struct attribute *a, int n)
 {
@@ -3289,6 +3859,10 @@ static struct attribute_group nvme_dev_attrs_group = {
 	.is_visible	= nvme_dev_attrs_are_visible,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4712| <<nvme_init_ctrl>> ctrl->device->groups = nvme_dev_attr_groups;
+ */
 static const struct attribute_group *nvme_dev_attr_groups[] = {
 	&nvme_dev_attrs_group,
 	NULL,
@@ -3326,6 +3900,10 @@ static int __nvme_check_ids(struct nvme_subsystem *subsys,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3648| <<nvme_init_ns_head>> head = nvme_alloc_ns_head(ctrl, nsid, id);
+ */
 static struct nvme_ns_head *nvme_alloc_ns_head(struct nvme_ctrl *ctrl,
 		unsigned nsid, struct nvme_id_ns *id)
 {
@@ -3384,14 +3962,65 @@ static struct nvme_ns_head *nvme_alloc_ns_head(struct nvme_ctrl *ctrl,
 	return ERR_PTR(ret);
 }
 
+/*
+ * commit ed754e5deeb17f4e675c84e4b6c640cc7344e498
+ * Author: Christoph Hellwig <hch@lst.de>
+ * Date:   Thu Nov 9 13:50:43 2017 +0100
+ *
+ * nvme: track shared namespaces
+ *
+ * Introduce a new struct nvme_ns_head that holds information about an actual
+ * namespace, unlike struct nvme_ns, which only holds the per-controller
+ * namespace information.  For private namespaces there is a 1:1 relation of
+ * the two, but for shared namespaces this lets us discover all the paths to
+ * it.  For now only the identifiers are moved to the new structure, but most
+ * of the information in struct nvme_ns should eventually move over.
+ *
+ * To allow lockless path lookup the list of nvme_ns structures per
+ * nvme_ns_head is protected by SRCU, which requires freeing the nvme_ns
+ * structure through call_srcu.
+ *
+ * Signed-off-by: Christoph Hellwig <hch@lst.de>
+ * Reviewed-by: Keith Busch <keith.busch@intel.com>
+ * Reviewed-by: Javier González <javier@cnexlabs.com>
+ * Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
+ * Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
+ * Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
+ * Reviewed-by: Hannes Reinecke <hare@suse.com>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|3791| <<nvme_alloc_ns>> ret = nvme_init_ns_head(ns, nsid, id);
+ *
+ * struct nvme_ns:
+ *   -> struct nvme_ns_head *head;
+ */
 static int nvme_init_ns_head(struct nvme_ns *ns, unsigned nsid,
 		struct nvme_id_ns *id)
 {
 	struct nvme_ctrl *ctrl = ns->ctrl;
+	/*
+	 * Bit 0: If set to '1', then the namespace may be attached to two or more controllers in the
+	 * NVM subsystem concurrently (i.e., may be a shared namespace). If cleared to '0', then
+	 * the namespace is a private namespace and is able to be attached to only one controller
+	 * at a time.
+	 */
 	bool is_shared = id->nmic & (1 << 0);
 	struct nvme_ns_head *head = NULL;
 	int ret = 0;
 
+	/*
+	 * Namespace Multi-path I/O and Namespace Sharing Capabilities (NMIC): This field
+	 * specifies multi-path I/O and namespace sharing capabilities of the namespace.
+	 *
+	 * Bits 7:1 are reserved.
+	 *
+	 * Bit 0: If set to '1', then the namespace may be attached to two or more controllers in the
+	 * NVM subsystem concurrently (i.e., may be a shared namespace). If cleared to '0', then
+	 * the namespace is a private namespace and is able to be attached to only one controller
+	 * at a time.
+	 */
+
 	mutex_lock(&ctrl->subsys->lock);
 	if (is_shared)
 		head = __nvme_find_ns_head(ctrl->subsys, nsid);
@@ -3417,6 +4046,9 @@ static int nvme_init_ns_head(struct nvme_ns *ns, unsigned nsid,
 		}
 	}
 
+	/*
+	 * 把属于一个共享的namespace的nsid加入nvme_ns_head->list
+	 */
 	list_add_tail(&ns->siblings, &head->list);
 	ns->head = head;
 
@@ -3427,6 +4059,10 @@ static int nvme_init_ns_head(struct nvme_ns *ns, unsigned nsid,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4349| <<nvme_scan_work>> list_sort(NULL, &ctrl->namespaces, ns_cmp);
+ */
 static int ns_cmp(void *priv, struct list_head *a, struct list_head *b)
 {
 	struct nvme_ns *nsa = container_of(a, struct nvme_ns, list);
@@ -3435,6 +4071,11 @@ static int ns_cmp(void *priv, struct list_head *a, struct list_head *b)
 	return nsa->head->ns_id - nsb->head->ns_id;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3884| <<nvme_validate_ns>> ns = nvme_find_get_ns(ctrl, nsid);
+ *   - drivers/nvme/host/core.c|3936| <<nvme_scan_ns_list>> ns = nvme_find_get_ns(ctrl, prev);
+ */
 static struct nvme_ns *nvme_find_get_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 {
 	struct nvme_ns *ns, *ret = NULL;
@@ -3442,6 +4083,10 @@ static struct nvme_ns *nvme_find_get_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	down_read(&ctrl->namespaces_rwsem);
 	list_for_each_entry(ns, &ctrl->namespaces, list) {
 		if (ns->head->ns_id == nsid) {
+			/*
+			 * Increment refcount for object unless it is zero.
+			 * Return non-zero if the increment succeeded. Otherwise return 0.
+			 */
 			if (!kref_get_unless_zero(&ns->kref))
 				continue;
 			ret = ns;
@@ -3454,6 +4099,10 @@ static struct nvme_ns *nvme_find_get_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4150| <<nvme_alloc_ns>> nvme_setup_streams_ns(ctrl, ns);
+ */
 static int nvme_setup_streams_ns(struct nvme_ctrl *ctrl, struct nvme_ns *ns)
 {
 	struct streams_directive_params s;
@@ -3480,6 +4129,20 @@ static int nvme_setup_streams_ns(struct nvme_ctrl *ctrl, struct nvme_ns *ns)
 	return 0;
 }
 
+/*
+ * [0] nvme_alloc_ns
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|3752| <<nvme_validate_ns>> nvme_alloc_ns(ctrl, nsid);
+ *
+ * 每个ns一个request_queue, 一个gendisk
+ */
 static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 {
 	struct nvme_ns *ns;
@@ -3492,6 +4155,18 @@ static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	if (!ns)
 		return -ENOMEM;
 
+	/*
+	 * struct nvme_ctrl:
+	 *   - struct blk_mq_tag_set *tagset;
+	 *   - struct blk_mq_tag_set *admin_tagset;
+	 *
+	 * struct nvme_dev:
+	 *   - struct blk_mq_tag_set tagset;
+	 *   - struct blk_mq_tag_set admin_tagset;
+	 *
+	 * blk_mq_init_queue()更加像是从tagset中新分配一个request_queue
+	 * 一个tagset可以有多个request_queue
+	 */
 	ns->queue = blk_mq_init_queue(ctrl->tagset);
 	if (IS_ERR(ns->queue)) {
 		ret = PTR_ERR(ns->queue);
@@ -3509,6 +4184,7 @@ static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	ns->queue->queuedata = ns;
 	ns->ctrl = ctrl;
 
+	/* 初始化为1 */
 	kref_init(&ns->kref);
 	ns->lba_shift = 9; /* set to a default value for 512 until disk is validated */
 
@@ -3584,6 +4260,13 @@ static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4248| <<nvme_validate_ns>> nvme_ns_remove(ns);
+ *   - drivers/nvme/host/core.c|4268| <<nvme_remove_invalid_namespaces>> nvme_ns_remove(ns);
+ *   - drivers/nvme/host/core.c|4303| <<nvme_scan_ns_list>> nvme_ns_remove(ns);
+ *   - drivers/nvme/host/core.c|4453| <<nvme_remove_namespaces>> nvme_ns_remove(ns);
+ */
 static void nvme_ns_remove(struct nvme_ns *ns)
 {
 	if (test_and_set_bit(NVME_NS_REMOVING, &ns->flags))
@@ -3613,6 +4296,11 @@ static void nvme_ns_remove(struct nvme_ns *ns)
 	nvme_put_ns(ns);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3795| <<nvme_scan_ns_list>> nvme_validate_ns(ctrl, nsid);
+ *   - drivers/nvme/host/core.c|3819| <<nvme_scan_ns_sequential>> nvme_validate_ns(ctrl, i);
+ */
 static void nvme_validate_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 {
 	struct nvme_ns *ns;
@@ -3626,6 +4314,13 @@ static void nvme_validate_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 		nvme_alloc_ns(ctrl, nsid);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1752| <<nvme_passthru_end>> nvme_remove_invalid_namespaces(ctrl, NVME_NSID_ALL);
+ *   - drivers/nvme/host/core.c|4254| <<nvme_remove_invalid_namespaces>> static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
+ *   - drivers/nvme/host/core.c|4311| <<nvme_scan_ns_list>> nvme_remove_invalid_namespaces(ctrl, prev);
+ *   - drivers/nvme/host/core.c|4328| <<nvme_scan_ns_sequential>> nvme_remove_invalid_namespaces(ctrl, nn);
+ */
 static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 					unsigned nsid)
 {
@@ -3644,6 +4339,10 @@ static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4036| <<nvme_scan_work>> if (!nvme_scan_ns_list(ctrl, nn))
+ */
 static int nvme_scan_ns_list(struct nvme_ctrl *ctrl, unsigned nn)
 {
 	struct nvme_ns *ns;
@@ -3685,6 +4384,10 @@ static int nvme_scan_ns_list(struct nvme_ctrl *ctrl, unsigned nn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4039| <<nvme_scan_work>> nvme_scan_ns_sequential(ctrl, nn);
+ */
 static void nvme_scan_ns_sequential(struct nvme_ctrl *ctrl, unsigned nn)
 {
 	unsigned i;
@@ -3695,6 +4398,10 @@ static void nvme_scan_ns_sequential(struct nvme_ctrl *ctrl, unsigned nn)
 	nvme_remove_invalid_namespaces(ctrl, nn);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4397| <<nvme_scan_work>> nvme_clear_changed_ns_log(ctrl);
+ */
 static void nvme_clear_changed_ns_log(struct nvme_ctrl *ctrl)
 {
 	size_t log_size = NVME_MAX_CHANGED_NAMESPACES * sizeof(__le32);
@@ -3720,6 +4427,34 @@ static void nvme_clear_changed_ns_log(struct nvme_ctrl *ctrl)
 	kfree(log);
 }
 
+/*
+ * 在qemu测试的时候, 先...
+ * [0] nvme_async_probe
+ * [0] async_run_entry_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 再...
+ * [0] nvme_queue_scan
+ * [0] nvme_start_ctrl
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|120| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+ *   - drivers/nvme/host/core.c|3902| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+ *   - drivers/nvme/host/pci.c|3420| <<nvme_async_probe>> flush_work(&dev->ctrl.scan_work);
+ *
+ * 在以下使用nvme_scan_work():
+ *   - drivers/nvme/host/core.c|4180| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+ *
+ * # echo 1 > /sys/block/nvme0n1/device/rescan_controller
+ */
 static void nvme_scan_work(struct work_struct *work)
 {
 	struct nvme_ctrl *ctrl =
@@ -3793,6 +4528,10 @@ void nvme_remove_namespaces(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_remove_namespaces);
 
+/*
+ * 在以下使用nvme_class_uevent():
+ *   - drivers/nvme/host/core.c|4541| <<nvme_core_init>> nvme_class->dev_uevent = nvme_class_uevent;
+ */
 static int nvme_class_uevent(struct device *dev, struct kobj_uevent_env *env)
 {
 	struct nvme_ctrl *ctrl =
@@ -3836,6 +4575,17 @@ static void nvme_aen_uevent(struct nvme_ctrl *ctrl)
 	kfree(envp[0]);
 }
 
+/*
+ * 在以下使用async_event_work:
+ *   - drivers/nvme/host/core.c|1580| <<nvme_enable_aen>> queue_work(nvme_wq, &ctrl->async_event_work);
+ *   - drivers/nvme/host/core.c|4345| <<nvme_async_event_work>> container_of(work, struct nvme_ctrl, async_event_work);
+ *   - drivers/nvme/host/core.c|4483| <<nvme_complete_async_event>> queue_work(nvme_wq, &ctrl->async_event_work);
+ *   - drivers/nvme/host/core.c|4500| <<nvme_stop_ctrl>> flush_work(&ctrl->async_event_work);
+ *   - drivers/nvme/host/core.c|4589| <<nvme_init_ctrl>> INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
+ *
+ * 在以下使用nvme_async_event_work():
+ *   - drivers/nvme/host/core.c|4837| <<nvme_init_ctrl>> INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
+ */
 static void nvme_async_event_work(struct work_struct *work)
 {
 	struct nvme_ctrl *ctrl =
@@ -3859,6 +4609,10 @@ static bool nvme_ctrl_pp_status(struct nvme_ctrl *ctrl)
 	return ((ctrl->ctrl_config & NVME_CC_ENABLE) && (csts & NVME_CSTS_PP));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4600| <<nvme_fw_act_work>> nvme_get_fw_slot_info(ctrl);
+ */
 static void nvme_get_fw_slot_info(struct nvme_ctrl *ctrl)
 {
 	struct nvme_fw_slot_info_log *log;
@@ -3873,6 +4627,10 @@ static void nvme_get_fw_slot_info(struct nvme_ctrl *ctrl)
 	kfree(log);
 }
 
+/*
+ * 在以下使用nvme_fw_act_work():
+ *   - drivers/nvme/host/core.c|4299| <<nvme_init_ctrl>> INIT_WORK(&ctrl->fw_act_work, nvme_fw_act_work);
+ */
 static void nvme_fw_act_work(struct work_struct *work)
 {
 	struct nvme_ctrl *ctrl = container_of(work,
@@ -3905,6 +4663,10 @@ static void nvme_fw_act_work(struct work_struct *work)
 	nvme_get_fw_slot_info(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4663| <<nvme_complete_async_event>> nvme_handle_aen_notice(ctrl, result);
+ */
 static void nvme_handle_aen_notice(struct nvme_ctrl *ctrl, u32 result)
 {
 	u32 aer_notice_type = (result & 0xff00) >> 8;
@@ -3940,6 +4702,18 @@ static void nvme_handle_aen_notice(struct nvme_ctrl *ctrl, u32 result)
 	}
 }
 
+/*
+ * Async Event Notification
+ *
+ * called by:
+ *   - drivers/nvme/host/fc.c|1693| <<nvme_fc_fcpio_done>> nvme_complete_async_event(&queue->ctrl->ctrl, status, &result);
+ *   - drivers/nvme/host/pci.c|1112| <<nvme_handle_cqe>> nvme_complete_async_event(&nvmeq->dev->ctrl,
+ *   - drivers/nvme/host/rdma.c|1504| <<nvme_rdma_recv_done>> nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+ *   - drivers/nvme/host/tcp.c|496| <<nvme_tcp_handle_comp>> nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+ *   - drivers/nvme/target/loop.c|106| <<nvme_loop_queue_response>> nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+ *
+ * The driver can handle tracking only one AEN request
+ */
 void nvme_complete_async_event(struct nvme_ctrl *ctrl, __le16 status,
 		volatile union nvme_result *res)
 {
@@ -3967,6 +4741,15 @@ void nvme_complete_async_event(struct nvme_ctrl *ctrl, __le16 status,
 }
 EXPORT_SYMBOL_GPL(nvme_complete_async_event);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|235| <<nvme_do_delete_ctrl>> nvme_stop_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|2949| <<nvme_fc_reset_ctrl_work>> nvme_stop_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|3569| <<nvme_remove>> nvme_stop_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1908| <<nvme_rdma_reset_ctrl_work>> nvme_stop_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1947| <<nvme_reset_ctrl_work>> nvme_stop_ctrl(ctrl);
+ *   - drivers/nvme/target/loop.c|450| <<nvme_loop_reset_ctrl_work>> nvme_stop_ctrl(&ctrl->ctrl);
+ */
 void nvme_stop_ctrl(struct nvme_ctrl *ctrl)
 {
 	nvme_mpath_stop(ctrl);
@@ -3976,6 +4759,15 @@ void nvme_stop_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_stop_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2736| <<nvme_fc_create_association>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|3128| <<nvme_reset_work>> nvme_start_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1031| <<nvme_rdma_setup_ctrl>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1865| <<nvme_tcp_setup_ctrl>> nvme_start_ctrl(ctrl);
+ *   - drivers/nvme/target/loop.c|477| <<nvme_loop_reset_ctrl_work>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|633| <<nvme_loop_create_ctrl>> nvme_start_ctrl(&ctrl->ctrl);
+ */
 void nvme_start_ctrl(struct nvme_ctrl *ctrl)
 {
 	if (ctrl->kato)
@@ -3998,6 +4790,10 @@ void nvme_uninit_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_uninit_ctrl);
 
+/*
+ * 在以下使用nvme_free_ctrl():
+ *   - drivers/nvme/host/core.c|4805| <<nvme_init_ctrl>> ctrl->device->release = nvme_free_ctrl;
+ */
 static void nvme_free_ctrl(struct device *dev)
 {
 	struct nvme_ctrl *ctrl =
@@ -4029,6 +4825,14 @@ static void nvme_free_ctrl(struct device *dev)
  * earliest initialization so that we have the initialized structured around
  * during probing.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3167| <<nvme_fc_init_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_fc_ctrl_ops, 0);
+ *   - drivers/nvme/host/pci.c|3490| <<nvme_probe>> result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
+ *   - drivers/nvme/host/rdma.c|2031| <<nvme_rdma_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
+ *   - drivers/nvme/host/tcp.c|2340| <<nvme_tcp_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_tcp_ctrl_ops, 0);
+ *   - drivers/nvme/target/loop.c|585| <<nvme_loop_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_loop_ctrl_ops,
+ */
 int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
 		const struct nvme_ctrl_ops *ops, unsigned long quirks)
 {
@@ -4112,6 +4916,12 @@ EXPORT_SYMBOL_GPL(nvme_init_ctrl);
  * Call this function when the driver determines it is unable to get the
  * controller in a state capable of servicing IO.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4028| <<nvme_remove_namespaces>> nvme_kill_queues(ctrl);
+ *   - drivers/nvme/host/pci.c|3133| <<nvme_remove_dead_ctrl>> nvme_kill_queues(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3287| <<nvme_reset_work>> nvme_kill_queues(&dev->ctrl);
+ */
 void nvme_kill_queues(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -4129,6 +4939,12 @@ void nvme_kill_queues(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_kill_queues);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1532| <<nvme_passthru_end>> nvme_unfreeze(ctrl);
+ *   - drivers/nvme/host/pci.c|3294| <<nvme_reset_work>> nvme_unfreeze(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3665| <<nvme_suspend>> nvme_unfreeze(ctrl);
+ */
 void nvme_unfreeze(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -4140,6 +4956,10 @@ void nvme_unfreeze(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_unfreeze);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3429| <<nvme_dev_disable>> nvme_wait_freeze_timeout(&dev->ctrl, NVME_IO_TIMEOUT);
+ */
 void nvme_wait_freeze_timeout(struct nvme_ctrl *ctrl, long timeout)
 {
 	struct nvme_ns *ns;
@@ -4154,6 +4974,12 @@ void nvme_wait_freeze_timeout(struct nvme_ctrl *ctrl, long timeout)
 }
 EXPORT_SYMBOL_GPL(nvme_wait_freeze_timeout);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1501| <<nvme_passthru_start>> nvme_wait_freeze(ctrl);
+ *   - drivers/nvme/host/pci.c|3292| <<nvme_reset_work>> nvme_wait_freeze(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3632| <<nvme_suspend>> nvme_wait_freeze(ctrl);
+ */
 void nvme_wait_freeze(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -4165,6 +4991,12 @@ void nvme_wait_freeze(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_wait_freeze);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1500| <<nvme_passthru_start>> nvme_start_freeze(ctrl);
+ *   - drivers/nvme/host/pci.c|3030| <<nvme_dev_disable>> nvme_start_freeze(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3631| <<nvme_suspend>> nvme_start_freeze(ctrl);
+ */
 void nvme_start_freeze(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -4187,6 +5019,18 @@ void nvme_stop_queues(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_stop_queues);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4018| <<nvme_fw_act_work>> nvme_start_queues(ctrl);
+ *   - drivers/nvme/host/core.c|4103| <<nvme_start_ctrl>> nvme_start_queues(ctrl);
+ *   - drivers/nvme/host/fc.c|2851| <<nvme_fc_delete_association>> nvme_start_queues(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|2918| <<nvme_dev_disable>> nvme_start_queues(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3111| <<nvme_reset_work>> nvme_start_queues(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|940| <<nvme_rdma_teardown_io_queues>> nvme_start_queues(&ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|1073| <<nvme_rdma_error_recovery_work>> nvme_start_queues(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1802| <<nvme_tcp_teardown_io_queues>> nvme_start_queues(ctrl);
+ *   - drivers/nvme/host/tcp.c|1910| <<nvme_tcp_error_recovery_work>> nvme_start_queues(ctrl);
+ */
 void nvme_start_queues(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -4199,11 +5043,31 @@ void nvme_start_queues(struct nvme_ctrl *ctrl)
 EXPORT_SYMBOL_GPL(nvme_start_queues);
 
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2717| <<nvme_reset_work>> nvme_sync_queues(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3027| <<nvme_reset_prepare>> nvme_sync_queues(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3133| <<nvme_suspend>> nvme_sync_queues(ctrl);
+ *
+ * 核心思想是对每一个namespace的request_queue和admin的requests调用:
+ * blk_sync_queue():
+ *  -> del_timer_sync(&q->timeout);
+ *  -> cancel_work_sync(&q->timeout_work);
+ */
 void nvme_sync_queues(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
 
 	down_read(&ctrl->namespaces_rwsem);
+	/*
+	 * The block layer may perform asynchronous callback activity
+	 * on a queue, such as calling the unplug function after a timeout.
+	 * A block device may call blk_sync_queue to ensure that any
+	 * such activity is cancelled, thus allowing it to release resources
+	 * that the callbacks might use. The caller must already have made sure
+	 * that its ->make_request_fn will not re-add plugging prior to calling
+	 * this function
+	 */
 	list_for_each_entry(ns, &ctrl->namespaces, list)
 		blk_sync_queue(ns->queue);
 	up_read(&ctrl->namespaces_rwsem);
diff --git a/drivers/nvme/host/fault_inject.c b/drivers/nvme/host/fault_inject.c
index 1352159733b0..aa7c0eaa3b6d 100644
--- a/drivers/nvme/host/fault_inject.c
+++ b/drivers/nvme/host/fault_inject.c
@@ -12,9 +12,43 @@ static DECLARE_FAULT_ATTR(fail_default_attr);
 /* optional fault injection attributes boot time option:
  * nvme_core.fail_request=<interval>,<probability>,<space>,<times>
  */
+/*
+ * "nvme_core.fail_request=20,100,0,5":
+ * [    3.132027] FAULT_INJECTION: forcing a failure.
+ *                name fault_inject, interval 20, probability 100, space 0, times 3
+ * [    3.132029] CPU: 0 PID: 247 Comm: systemd-udevd Not tainted 5.5.0+ #3
+ * [    3.132030] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-59-gc9ba5276e321-prebuilt.qemu.org 04/01/2014
+ * [    3.132030] Call Trace:
+ * [    3.132032]  <IRQ>
+ * [    3.132036]  dump_stack+0x50/0x6b
+ * [    3.132038]  should_fail+0x13c/0x160
+ * [    3.132040]  nvme_should_fail+0x30/0xa0
+ * [    3.132042]  nvme_irq+0x136/0x210
+ * [    3.132044]  __handle_irq_event_percpu+0x3b/0x180
+ * [    3.132045]  handle_irq_event_percpu+0x2b/0x70
+ * [    3.132046]  handle_irq_event+0x22/0x40
+ * [    3.132047]  handle_edge_irq+0x75/0x190
+ * [    3.132049]  do_IRQ+0x41/0xd0
+ * [    3.132064]  common_interrupt+0xf/0xf
+ * [    3.132064]  </IRQ>
+ * [    3.132065] RIP: 0033:0x557263ceebe0
+ */
 static char *fail_request;
 module_param(fail_request, charp, 0000);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3665| <<nvme_alloc_ns>> nvme_fault_inject_init(&ns->fault_inject, ns->disk->disk_name);
+ *   - drivers/nvme/host/core.c|4194| <<nvme_init_ctrl>> nvme_fault_inject_init(&ctrl->fault_inject, dev_name(ctrl->device));
+ *
+ * controller和namespace各自有fault_injection
+ *
+ * # ls /sys/kernel/debug/nvme0/fault_inject/
+ * dont_retry  interval  probability  space  status  task-filter  times  verbose  verbose_ratelimit_burst  verbose_ratelimit_interval_ms
+ *
+ * # ls /sys/kernel/debug/nvme0n1/fault_inject/
+ * dont_retry  interval  probability  space  status  task-filter  times  verbose  verbose_ratelimit_burst  verbose_ratelimit_interval_ms
+ */
 void nvme_fault_inject_init(struct nvme_fault_inject *fault_inj,
 			    const char *dev_name)
 {
@@ -48,18 +82,39 @@ void nvme_fault_inject_init(struct nvme_fault_inject *fault_inj,
 	debugfs_create_bool("dont_retry", 0600, dir, &fault_inj->dont_retry);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3692| <<nvme_ns_remove>> nvme_fault_inject_fini(&ns->fault_inject);
+ *   - drivers/nvme/host/core.c|4095| <<nvme_uninit_ctrl>> nvme_fault_inject_fini(&ctrl->fault_inject);
+ */
 void nvme_fault_inject_fini(struct nvme_fault_inject *fault_inject)
 {
 	/* remove debugfs directories */
 	debugfs_remove_recursive(fault_inject->parent);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/nvme.h|481| <<nvme_end_request>> nvme_should_fail(req);
+ */
 void nvme_should_fail(struct request *req)
 {
+	/*
+	 * 主要设置rq_disk的地方:
+	 *   - block/blk-exec.c|55| <<blk_execute_rq_nowait>> rq->rq_disk = bd_disk;
+	 *   - block/blk-flush.c|636| <<blk_kick_flush>> flush_rq->rq_disk = first_rq->rq_disk;
+	 *   - block/blk-mq.c|323| <<blk_mq_rq_ctx_init>> rq->rq_disk = NULL;
+	 *   - block/blk.h|191| <<blk_rq_bio_prep>> rq->rq_disk = bio->bi_disk;
+	 */
 	struct gendisk *disk = req->rq_disk;
 	struct nvme_fault_inject *fault_inject = NULL;
 	u16 status;
 
+	/*
+	 * 正如nvme_fault_inject_init()注释的
+	 * controller和namespace各自有fault_injection
+	 * 似乎如果没有disk说明是admin queue
+	 */
 	if (disk) {
 		struct nvme_ns *ns = disk->private_data;
 
diff --git a/drivers/nvme/host/hwmon.c b/drivers/nvme/host/hwmon.c
index a5af21f5d370..9c9fa3b785ca 100644
--- a/drivers/nvme/host/hwmon.c
+++ b/drivers/nvme/host/hwmon.c
@@ -228,6 +228,10 @@ static const struct hwmon_chip_info nvme_hwmon_chip_info = {
 	.info	= nvme_hwmon_info,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3432| <<nvme_init_identify>> nvme_hwmon_init(ctrl);
+ */
 void nvme_hwmon_init(struct nvme_ctrl *ctrl)
 {
 	struct device *dev = ctrl->dev;
diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 797c18337d96..95b9a92a7a0c 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -64,6 +64,10 @@ void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|296| <<nvme_complete_rq>> nvme_failover_req(req);
+ */
 void nvme_failover_req(struct request *req)
 {
 	struct nvme_ns *ns = req->q->queuedata;
@@ -114,6 +118,10 @@ void nvme_failover_req(struct request *req)
 	kblockd_schedule_work(&ns->head->requeue_work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|441| <<nvme_change_ctrl_state>> nvme_kick_requeue_lists(ctrl);
+ */
 void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -174,6 +182,11 @@ static bool nvme_path_is_disabled(struct nvme_ns *ns)
 		test_bit(NVME_NS_REMOVING, &ns->flags);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|282| <<nvme_find_path>> ns = __nvme_find_path(head, node);
+ *   - drivers/nvme/host/multipath.c|442| <<nvme_mpath_set_live>> __nvme_find_path(head, node);
+ */
 static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head, int node)
 {
 	int found_distance = INT_MAX, fallback_distance = INT_MAX, distance;
@@ -262,6 +275,11 @@ static inline bool nvme_path_is_optimized(struct nvme_ns *ns)
 		ns->ana_state == NVME_ANA_OPTIMIZED;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1692| <<nvme_get_ns_from_disk>> ns = nvme_find_path(*head);
+ *   - drivers/nvme/host/multipath.c|322| <<nvme_ns_head_make_request>> ns = nvme_find_path(head);
+ */
 inline struct nvme_ns *nvme_find_path(struct nvme_ns_head *head)
 {
 	int node = numa_node_id();
@@ -359,6 +377,10 @@ static void nvme_requeue_work(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3601| <<nvme_alloc_ns_head>> ret = nvme_mpath_alloc_disk(ctrl, head);
+ */
 int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 {
 	struct request_queue *q;
@@ -369,6 +391,11 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	spin_lock_init(&head->requeue_lock);
 	INIT_WORK(&head->requeue_work, nvme_requeue_work);
 
+	/*
+	 * 在qemu的nvme上下面的if会退出
+	 * cmic=0x0000000000000000, multipath=1
+	 */
+
 	/*
 	 * Add a multipath node if the subsystems supports multiple controllers.
 	 * We also do this for private namespaces as the namespace sharing data could
@@ -435,6 +462,11 @@ static void nvme_mpath_set_live(struct nvme_ns *ns)
 	kblockd_schedule_work(&ns->head->requeue_work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|549| <<nvme_read_ana_log>> error = nvme_parse_ana_log(ctrl, &nr_change_groups,
+ *   - drivers/nvme/host/multipath.c|673| <<nvme_mpath_add_disk>> nvme_parse_ana_log(ns->ctrl, ns, nvme_set_ns_ana_state);
+ */
 static int nvme_parse_ana_log(struct nvme_ctrl *ctrl, void *data,
 		int (*cb)(struct nvme_ctrl *ctrl, struct nvme_ana_group_desc *,
 			void *))
@@ -657,6 +689,10 @@ static int nvme_set_ns_ana_state(struct nvme_ctrl *ctrl,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3659| <<nvme_alloc_ns>> nvme_mpath_add_disk(ns, id);
+ */
 void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id)
 {
 	if (nvme_ctrl_use_ana(ns->ctrl)) {
@@ -686,6 +722,10 @@ void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 	put_disk(head->disk);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2911| <<nvme_init_identify>> ret = nvme_mpath_init(ctrl, id);
+ */
 int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 {
 	int error;
@@ -694,6 +734,7 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	if (!multipath || !ctrl->subsys || !(ctrl->subsys->cmic & (1 << 3)))
 		return 0;
 
+	/* ANA = Asymmetric Namespace Access */
 	ctrl->anacap = id->anacap;
 	ctrl->anatt = id->anatt;
 	ctrl->nanagrpid = le32_to_cpu(id->nanagrpid);
@@ -715,6 +756,10 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	}
 
 	INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+	/*
+	 * 根据最新的commit, 这里似乎有leak, 多次分配
+	 * 最好调用个kfree()
+	 */
 	ctrl->ana_log_buf = kmalloc(ctrl->ana_log_size, GFP_KERNEL);
 	if (!ctrl->ana_log_buf) {
 		error = -ENOMEM;
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 1024fec7914c..116ad5a6134c 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -20,9 +20,33 @@
 #include <trace/events/block.h>
 
 extern unsigned int nvme_io_timeout;
+/*
+ * 在以下使用NVME_IO_TIMEOUT:
+ *   - drivers/nvme/host/fc.c|2473| <<nvme_fc_create_io_queues>> ctrl->tag_set.timeout = NVME_IO_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|2800| <<nvme_dev_add>> dev->tagset.timeout = NVME_IO_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|3041| <<nvme_dev_disable>> nvme_wait_freeze_timeout(&dev->ctrl, NVME_IO_TIMEOUT);
+ *   - drivers/nvme/host/rdma.c|751| <<nvme_rdma_alloc_tagset>> set->timeout = NVME_IO_TIMEOUT;
+ *   - drivers/nvme/host/tcp.c|1489| <<nvme_tcp_alloc_tagset>> set->timeout = NVME_IO_TIMEOUT;
+ *   - drivers/nvme/target/loop.c|522| <<nvme_loop_create_io_queues>> ctrl->tag_set.timeout = NVME_IO_TIMEOUT;
+ */
 #define NVME_IO_TIMEOUT	(nvme_io_timeout * HZ)
 
 extern unsigned int admin_timeout;
+/*
+ * 在以下使用ADMIN_TIMEOUT:
+ *   - drivers/nvme/host/core.c|863| <<__nvme_submit_sync_cmd>> req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/core.c|946| <<nvme_submit_user_cmd>> req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/core.c|2132| <<nvme_sec_submit>> ADMIN_TIMEOUT, NVME_QID_ANY, 1, 0, false);
+ *   - drivers/nvme/host/fc.c|3140| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/lightnvm.c|777| <<nvme_nvm_submit_user_cmd>> rq->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|1560| <<nvme_timeout>> abort_req->timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|1945| <<nvme_alloc_admin_tags>> dev->admin_tagset.timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|2747| <<nvme_delete_queue>> req->timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/pci.c|2763| <<__nvme_disable_io_queues>> timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/rdma.c|737| <<nvme_rdma_alloc_tagset>> set->timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/host/tcp.c|1477| <<nvme_tcp_alloc_tagset>> set->timeout = ADMIN_TIMEOUT;
+ *   - drivers/nvme/target/loop.c|348| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.timeout = ADMIN_TIMEOUT;
+ */
 #define ADMIN_TIMEOUT	(admin_timeout * HZ)
 
 #define NVME_DEFAULT_KATO	5
@@ -144,6 +168,13 @@ struct nvme_request {
 /*
  * Mark a bio as coming in through the mpath node.
  */
+/*
+ * 在以下使用REQ_NVME_MPATH:
+ *   - drivers/nvme/host/core.c|435| <<nvme_complete_rq>> if ((req->cmd_flags & REQ_NVME_MPATH) &&
+ *   - drivers/nvme/host/fabrics.c|552| <<nvmf_fail_nonready_command>> !blk_noretry_request(rq) && !(rq->cmd_flags & REQ_NVME_MPATH))
+ *   - drivers/nvme/host/multipath.c|335| <<nvme_ns_head_make_request>> bio->bi_opf |= REQ_NVME_MPATH;
+ *   - drivers/nvme/host/nvme.h|715| <<nvme_trace_bio_complete>> if (req->cmd_flags & REQ_NVME_MPATH)
+ */
 #define REQ_NVME_MPATH		REQ_DRV
 
 enum {
@@ -202,14 +233,50 @@ struct nvme_ctrl {
 	int instance;
 	int numa_node;
 	struct blk_mq_tag_set *tagset;
+	/*
+	 * 在以下设置admin_tagset:
+	 *   - drivers/nvme/host/fc.c|3146| <<nvme_fc_init_ctrl>> ctrl->ctrl.admin_tagset = &ctrl->admin_tag_set;
+	 *   - drivers/nvme/host/pci.c|1953| <<nvme_alloc_admin_tags>> dev->ctrl.admin_tagset = &dev->admin_tagset;
+	 *   - drivers/nvme/host/rdma.c|803| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+	 *   - drivers/nvme/host/tcp.c|1723| <<nvme_tcp_configure_admin_queue>> ctrl->admin_tagset = nvme_tcp_alloc_tagset(ctrl, true);
+	 *   - drivers/nvme/target/loop.c|360| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.admin_tagset = &ctrl->admin_tag_set;
+	 */
 	struct blk_mq_tag_set *admin_tagset;
 	struct list_head namespaces;
 	struct rw_semaphore namespaces_rwsem;
 	struct device ctrl_device;
 	struct device *device;	/* char device */
 	struct cdev cdev;
+	/*
+	 * 在以下使用reset_work:
+	 *   - drivers/nvme/host/core.c|133| <<nvme_try_sched_reset>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|143| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|155| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|169| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|2944| <<nvme_fc_reset_ctrl_work>> container_of(work, struct nvme_fc_ctrl, ctrl.reset_work);
+	 *   - drivers/nvme/host/fc.c|3107| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/fc.c|3204| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2572| <<nvme_reset_work>> container_of(work, struct nvme_dev, ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2798| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2830| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/pci.c|2902| <<nvme_reset_done>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2929| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|3094| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/rdma.c|1906| <<nvme_rdma_reset_ctrl_work>> container_of(work, struct nvme_rdma_ctrl, ctrl.reset_work);
+	 *   - drivers/nvme/host/rdma.c|2018| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|1945| <<nvme_reset_ctrl_work>> container_of(work, struct nvme_ctrl, reset_work);
+	 *   - drivers/nvme/host/tcp.c|2298| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|446| <<nvme_loop_reset_ctrl_work>> container_of(work, struct nvme_loop_ctrl, ctrl.reset_work);
+	 *   - drivers/nvme/target/loop.c|580| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	struct work_struct reset_work;
 	struct work_struct delete_work;
+	/*
+	 * 在以下使用state_wq:
+	 *   - drivers/nvme/host/core.c|535| <<nvme_change_ctrl_state>> wake_up_all(&ctrl->state_wq);
+	 *   - drivers/nvme/host/core.c|571| <<nvme_wait_reset>> wait_event(ctrl->state_wq,
+	 *   - drivers/nvme/host/core.c|4515| <<nvme_init_ctrl>> init_waitqueue_head(&ctrl->state_wq);
+	 */
 	wait_queue_head_t state_wq;
 
 	struct nvme_subsystem *subsys;
@@ -220,6 +287,26 @@ struct nvme_ctrl {
 	char name[12];
 	u16 cntlid;
 
+	/*
+	 * 使用ctrl_config的地方:
+	 *   - drivers/nvme/host/core.c|2192| <<nvme_disable_ctrl>> ctrl->ctrl_config &= ~NVME_CC_SHN_MASK;
+	 *   - drivers/nvme/host/core.c|2193| <<nvme_disable_ctrl>> ctrl->ctrl_config &= ~NVME_CC_ENABLE;
+	 *   - drivers/nvme/host/core.c|2195| <<nvme_disable_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|2232| <<nvme_enable_ctrl>> ctrl->ctrl_config = NVME_CC_CSS_NVM;
+	 *   - drivers/nvme/host/core.c|2233| <<nvme_enable_ctrl>> ctrl->ctrl_config |= (page_shift - 12) << NVME_CC_MPS_SHIFT;
+	 *   - drivers/nvme/host/core.c|2234| <<nvme_enable_ctrl>> ctrl->ctrl_config |= NVME_CC_AMS_RR | NVME_CC_SHN_NONE;
+	 *   - drivers/nvme/host/core.c|2235| <<nvme_enable_ctrl>> ctrl->ctrl_config |= NVME_CC_IOSQES | NVME_CC_IOCQES;
+	 *   - drivers/nvme/host/core.c|2236| <<nvme_enable_ctrl>> ctrl->ctrl_config |= NVME_CC_ENABLE;
+	 *   - drivers/nvme/host/core.c|2238| <<nvme_enable_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|2251| <<nvme_shutdown_ctrl>> ctrl->ctrl_config &= ~NVME_CC_SHN_MASK;
+	 *   - drivers/nvme/host/core.c|2252| <<nvme_shutdown_ctrl>> ctrl->ctrl_config |= NVME_CC_SHN_NORMAL;
+	 *   - drivers/nvme/host/core.c|2254| <<nvme_shutdown_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|3959| <<nvme_ctrl_pp_status>> return ((ctrl->ctrl_config & NVME_CC_ENABLE) && (csts & NVME_CSTS_PP));
+	 *   - drivers/nvme/host/pci.c|2703| <<nvme_reset_work>> bool was_suspend = !!(dev->ctrl.ctrl_config & NVME_CC_SHN_NORMAL);
+	 *   - drivers/nvme/host/pci.c|2715| <<nvme_reset_work>> if (dev->ctrl.ctrl_config & NVME_CC_ENABLE)
+	 *
+	 * 最终会被写入bar0或bar1的NVME_REG_CC = 0x0014, Controller Configuration
+	 */
 	u32 ctrl_config;
 	u16 mtfa;
 	u32 queue_count;
@@ -235,6 +322,14 @@ struct nvme_ctrl {
 	u16 nr_streams;
 	u16 sqsize;
 	u32 max_namespaces;
+	/*
+	 * 在以下使用abort_limit:
+	 *   - drivers/nvme/host/core.c|3345| <<nvme_init_identify>> atomic_set(&ctrl->abort_limit, id->acl + 1);
+	 *   - drivers/nvme/host/pci.c|1715| <<abort_endio>> atomic_inc(&nvmeq->dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1862| <<nvme_timeout>> if (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {
+	 *   - drivers/nvme/host/pci.c|1863| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1887| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 */
 	atomic_t abort_limit;
 	u8 vwc;
 	u32 vs;
@@ -254,6 +349,14 @@ struct nvme_ctrl {
 	struct nvme_id_power_state psd[32];
 	struct nvme_effects_log *effects;
 	struct work_struct scan_work;
+	/*
+	 * 在以下使用async_event_work:
+	 *   - drivers/nvme/host/core.c|1580| <<nvme_enable_aen>> queue_work(nvme_wq, &ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4345| <<nvme_async_event_work>> container_of(work, struct nvme_ctrl, async_event_work);
+	 *   - drivers/nvme/host/core.c|4483| <<nvme_complete_async_event>> queue_work(nvme_wq, &ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4500| <<nvme_stop_ctrl>> flush_work(&ctrl->async_event_work);
+	 *   - drivers/nvme/host/core.c|4589| <<nvme_init_ctrl>> INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
+	 */
 	struct work_struct async_event_work;
 	struct delayed_work ka_work;
 	struct nvme_command ka_cmd;
@@ -313,6 +416,16 @@ struct nvme_subsystem {
 	struct list_head	entry;
 	struct mutex		lock;
 	struct list_head	ctrls;
+	/*
+	 * 使用nsheads的地方:
+	 *   - drivers/nvme/host/core.c|3615| <<nvme_alloc_ns_head>> list_add_tail(&head->entry, &ctrl->subsys->nsheads);
+	 *   - drivers/nvme/host/core.c|2857| <<nvme_init_subsystem>> INIT_LIST_HEAD(&subsys->nsheads);
+	 *   - drivers/nvme/host/core.c|3549| <<__nvme_find_ns_head>> list_for_each_entry(h, &subsys->nsheads, entry) {
+	 *   - drivers/nvme/host/core.c|3564| <<__nvme_check_ids>> list_for_each_entry(h, &subsys->nsheads, entry) {
+	 *   - drivers/nvme/host/multipath.c|20| <<nvme_mpath_unfreeze>> list_for_each_entry(h, &subsys->nsheads, entry)
+	 *   - drivers/nvme/host/multipath.c|30| <<nvme_mpath_wait_freeze>> list_for_each_entry(h, &subsys->nsheads, entry)
+	 *   - drivers/nvme/host/multipath.c|40| <<nvme_mpath_start_freeze>> list_for_each_entry(h, &subsys->nsheads, entry)
+	 */
 	struct list_head	nsheads;
 	char			subnqn[NVMF_NQN_SIZE];
 	char			serial[20];
@@ -358,6 +471,14 @@ struct nvme_ns_head {
 	spinlock_t		requeue_lock;
 	struct work_struct	requeue_work;
 	struct mutex		lock;
+	/*
+	 * 在以下使用current_path:
+	 *   - drivers/nvme/host/multipath.c|156| <<nvme_mpath_clear_current_path>> if (ns == rcu_access_pointer(head->current_path[node])) {
+	 *   - drivers/nvme/host/multipath.c|157| <<nvme_mpath_clear_current_path>> rcu_assign_pointer(head->current_path[node], NULL);
+	 *   - drivers/nvme/host/multipath.c|220| <<__nvme_find_path>> rcu_assign_pointer(head->current_path[node], found);
+	 *   - drivers/nvme/host/multipath.c|263| <<nvme_round_robin_path>> rcu_assign_pointer(head->current_path[node], found);
+	 *   - drivers/nvme/host/multipath.c|278| <<nvme_find_path>> ns = srcu_dereference(head->current_path[node], &head->srcu);
+	 */
 	struct nvme_ns __rcu	*current_path[];
 #endif
 };
@@ -448,6 +569,17 @@ static inline sector_t nvme_lba_to_sect(struct nvme_ns *ns, u64 lba)
 	return lba << (ns->lba_shift - SECTOR_SHIFT);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|1702| <<nvme_fc_fcpio_done>> nvme_end_request(rq, status, result);
+ *   - drivers/nvme/host/pci.c|1167| <<nvme_handle_cqe>> nvme_end_request(req, cqe->status, cqe->result);
+ *   - drivers/nvme/host/rdma.c|1126| <<nvme_rdma_inv_rkey_done>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/rdma.c|1335| <<nvme_rdma_send_done>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/rdma.c|1478| <<nvme_rdma_process_nvme_rsp>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/tcp.c|442| <<nvme_tcp_process_nvme_cqe>> nvme_end_request(rq, cqe->status, cqe->result);
+ *   - drivers/nvme/host/tcp.c|632| <<nvme_tcp_end_request>> nvme_end_request(rq, cpu_to_le16(status << 1), res);
+ *   - drivers/nvme/target/loop.c|119| <<nvme_loop_queue_response>> nvme_end_request(rq, cqe->status, cqe->result);
+ */
 static inline void nvme_end_request(struct request *req, __le16 status,
 		union nvme_result result)
 {
@@ -460,6 +592,18 @@ static inline void nvme_end_request(struct request *req, __le16 status,
 	blk_mq_complete_request(req);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|203| <<nvme_delete_ctrl_sync>> nvme_get_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|1621| <<nvme_handle_ctrl_ioctl>> nvme_get_ctrl(ns->ctrl);
+ *   - drivers/nvme/host/core.c|3686| <<nvme_alloc_ns>> nvme_get_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|3184| <<nvme_fc_init_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|3131| <<nvme_remove_dead_ctrl>> nvme_get_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|3489| <<nvme_probe>> nvme_get_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|2046| <<nvme_rdma_create_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|2357| <<nvme_tcp_create_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|624| <<nvme_loop_create_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ */
 static inline void nvme_get_ctrl(struct nvme_ctrl *ctrl)
 {
 	get_device(ctrl->device);
@@ -470,6 +614,11 @@ static inline void nvme_put_ctrl(struct nvme_ctrl *ctrl)
 	put_device(ctrl->device);
 }
 
+/*
+ * 关于admin ring buffer和async event, nvme的admin queue的tagset
+ * 有30个元素, 但是ring buffer有32个,除了一个用来表示full, 另外
+ * 一个用在async event
+ */
 static inline bool nvme_is_aen_req(u16 qid, __u16 command_id)
 {
 	return !qid && command_id >= NVME_AQ_BLK_MQ_DEPTH;
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index 365a2ddbeaa7..39e75029ad90 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -28,6 +28,15 @@
 #include "trace.h"
 #include "nvme.h"
 
+/*
+ * /sys/block/nvme0n1/device/reset_controller
+ *
+ * /sys/block/nvme0n1/device/rescan_controller
+ */
+
+/*
+ * sqes用来表示sq的每个entry的大小
+ */
 #define SQ_SIZE(q)	((q)->q_depth << (q)->sqes)
 #define CQ_SIZE(q)	((q)->q_depth * sizeof(struct nvme_completion))
 
@@ -37,21 +46,68 @@
  * These can be higher, but we need to ensure that any command doesn't
  * require an sg allocation that needs more than a page of data.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2708| <<nvme_reset_work>> NVME_MAX_KB_SZ << 1, dma_max_mapping_size(dev->dev) >> 9);
+ *   - drivers/nvme/host/pci.c|2948| <<nvme_probe>> alloc_size = nvme_pci_iod_alloc_size(dev, NVME_MAX_KB_SZ,
+ */
 #define NVME_MAX_KB_SZ	4096
 #define NVME_MAX_SEGS	127
 
+/*
+ * 在以下使用use_threaded_interrupts:
+ *   - drivers/nvme/host/pci.c|44| <<global>> module_param(use_threaded_interrupts, int , 0);
+ *   - drivers/nvme/host/pci.c|1614| <<queue_request_irq>> if (use_threaded_interrupts) {
+ */
 static int use_threaded_interrupts;
 module_param(use_threaded_interrupts, int, 0);
 
+/*
+ * 在以下使用use_cmb_sqes:
+ *   - drivers/nvme/host/pci.c|47| <<global>> module_param(use_cmb_sqes, bool, 0444);
+ *   - drivers/nvme/host/pci.c|48| <<global>> MODULE_PARM_DESC(use_cmb_sqes, "use controller's memory buffer for I/O SQes");
+ *   - drivers/nvme/host/pci.c|1925| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+ */
 static bool use_cmb_sqes = true;
 module_param(use_cmb_sqes, bool, 0444);
 MODULE_PARM_DESC(use_cmb_sqes, "use controller's memory buffer for I/O SQes");
 
+/*
+ * Most modern SSDs include onboard DRAM, typically in a ratio of 1GB RAM per
+ * 1TB of NAND flash memory. This RAM is usually dedicated to tracking where
+ * each logical block address is physically stored on the NAND
+ * flash—information that changes with every write operation due to the wear
+ * leveling that flash memory requires. This information must also be consulted
+ * in order to complete any read operation. The standard DRAM to NAND ratio
+ * provides enough RAM for the SSD controller to use a simple and fast lookup
+ * table instead of more complicated data structures. This greatly reduces the
+ * work the SSD controller needs to do to handle IO operations, and is key to
+ * offering consistent performance.
+ *
+ * SSDs that omit this DRAM can be cheaper and smaller, but because they can
+ * only store their mapping tables in the flash memory instead of much faster
+ * DRAM, there's a substantial performance penalty. In the worst case, read
+ * latency is doubled as potentially every read request from the host first
+ * requires a NAND flash read to look up the logical to physical address
+ * mapping, then a second read to actually fetch the requested data.
+ *
+ * 在以下使用max_host_mem_size_mb:
+ *   - drivers/nvme/host/pci.c|51| <<global>> module_param(max_host_mem_size_mb, uint, 0444);
+ *   - drivers/nvme/host/pci.c|52| <<global>> MODULE_PARM_DESC(max_host_mem_size_mb,
+ *   - drivers/nvme/host/pci.c|2081| <<nvme_setup_host_mem>> u64 max = (u64)max_host_mem_size_mb * SZ_1M;
+ *   - drivers/nvme/host/pci.c|2091| <<nvme_setup_host_mem>> min >> ilog2(SZ_1M), max_host_mem_size_mb);
+ */
 static unsigned int max_host_mem_size_mb = 128;
 module_param(max_host_mem_size_mb, uint, 0444);
 MODULE_PARM_DESC(max_host_mem_size_mb,
 	"Maximum Host Memory Buffer (HMB) size per controller (in MiB)");
 
+/*
+ * 在以下使用sgl_threshold:
+ *   - drivers/nvme/host/pci.c|56| <<global>> module_param(sgl_threshold, uint, 0644);
+ *   - drivers/nvme/host/pci.c|57| <<global>> MODULE_PARM_DESC(sgl_threshold,
+ *   - drivers/nvme/host/pci.c|542| <<nvme_pci_use_sgls>> if (!sgl_threshold || avg_seg_size < sgl_threshold)
+ */
 static unsigned int sgl_threshold = SZ_32K;
 module_param(sgl_threshold, uint, 0644);
 MODULE_PARM_DESC(sgl_threshold,
@@ -68,12 +124,26 @@ static int io_queue_depth = 1024;
 module_param_cb(io_queue_depth, &io_queue_depth_ops, &io_queue_depth, 0644);
 MODULE_PARM_DESC(io_queue_depth, "set io queue depth, should >= 2");
 
+/*
+ * 在以下使用write_queues:
+ *   - drivers/nvme/host/pci.c|346| <<max_io_queues>> return num_possible_cpus() + write_queues + poll_queues;
+ *   - drivers/nvme/host/pci.c|2524| <<nvme_calc_irq_sets>> } else if (nrirqs == 1 || !write_queues) {
+ *   - drivers/nvme/host/pci.c|2526| <<nvme_calc_irq_sets>> } else if (write_queues >= nrirqs) {
+ *   - drivers/nvme/host/pci.c|2529| <<nvme_calc_irq_sets>> nr_read_queues = nrirqs - write_queues;
+ *   - drivers/nvme/host/pci.c|3860| <<nvme_init>> write_queues = min(write_queues, num_possible_cpus());
+ */
 static unsigned int write_queues;
 module_param(write_queues, uint, 0644);
 MODULE_PARM_DESC(write_queues,
 	"Number of queues to use for writes. If not set, reads and writes "
 	"will share a queue set.");
 
+/*
+ * 在以下使用poll_queues:
+ *   - drivers/nvme/host/pci.c|346| <<max_io_queues>> return num_possible_cpus() + write_queues + poll_queues;
+ *   - drivers/nvme/host/pci.c|2574| <<nvme_setup_irqs>> this_p_queues = poll_queues;
+ *   - drivers/nvme/host/pci.c|3861| <<nvme_init>> poll_queues = min(poll_queues, num_possible_cpus());
+ */
 static unsigned int poll_queues;
 module_param(poll_queues, uint, 0644);
 MODULE_PARM_DESC(poll_queues, "Number of queues to use for polled IO.");
@@ -97,34 +167,161 @@ struct nvme_dev {
 	struct dma_pool *prp_small_pool;
 	unsigned online_queues;
 	unsigned max_qid;
+	/*
+	 * 在以下使用io_queues:
+	 *   - drivers/nvme/host/pci.c|752| <<nvme_pci_map_queues>> map->nr_queues = dev->io_queues[i];
+	 *   - drivers/nvme/host/pci.c|2518| <<nvme_create_io_queues>> if (max != 1 && dev->io_queues[HCTX_TYPE_POLL]) {
+	 *   - drivers/nvme/host/pci.c|2519| <<nvme_create_io_queues>> rw_queues = dev->io_queues[HCTX_TYPE_DEFAULT] +
+	 *   - drivers/nvme/host/pci.c|2520| <<nvme_create_io_queues>> dev->io_queues[HCTX_TYPE_READ];
+	 *   - drivers/nvme/host/pci.c|2948| <<nvme_calc_irq_sets>> dev->io_queues[HCTX_TYPE_DEFAULT] = nrirqs - nr_read_queues;
+	 *   - drivers/nvme/host/pci.c|2950| <<nvme_calc_irq_sets>> dev->io_queues[HCTX_TYPE_READ] = nr_read_queues;
+	 *   - drivers/nvme/host/pci.c|2997| <<nvme_setup_irqs>> dev->io_queues[HCTX_TYPE_POLL] = this_p_queues;
+	 *   - drivers/nvme/host/pci.c|3000| <<nvme_setup_irqs>> dev->io_queues[HCTX_TYPE_DEFAULT] = 1;
+	 *   - drivers/nvme/host/pci.c|3001| <<nvme_setup_irqs>> dev->io_queues[HCTX_TYPE_READ] = 0;
+	 *   - drivers/nvme/host/pci.c|3113| <<nvme_setup_io_queues>> dev->max_qid = result + dev->io_queues[HCTX_TYPE_POLL];
+	 *   - drivers/nvme/host/pci.c|3146| <<nvme_setup_io_queues>> dev->io_queues[HCTX_TYPE_DEFAULT],
+	 *   - drivers/nvme/host/pci.c|3147| <<nvme_setup_io_queues>> dev->io_queues[HCTX_TYPE_READ],
+	 *   - drivers/nvme/host/pci.c|3148| <<nvme_setup_io_queues>> dev->io_queues[HCTX_TYPE_POLL]);
+	 *   - drivers/nvme/host/pci.c|3278| <<nvme_dev_add>> if (dev->io_queues[HCTX_TYPE_POLL])
+	 */
 	unsigned io_queues[HCTX_MAX_TYPES];
 	unsigned int num_vecs;
 	int q_depth;
+	/*
+	 * 在以下使用io_sqes:
+	 *   - drivers/nvme/host/pci.c|1592| <<nvme_alloc_queue>> nvmeq->sqes = qid ? dev->io_sqes : NVME_ADM_SQES;
+	 *   - drivers/nvme/host/pci.c|2559| <<nvme_pci_enable>> dev->io_sqes = 7;
+	 *   - drivers/nvme/host/pci.c|2561| <<nvme_pci_enable>> dev->io_sqes = NVME_NVM_IOSQES;
+	 */
 	int io_sqes;
 	u32 db_stride;
 	void __iomem *bar;
 	unsigned long bar_mapped_size;
+	/*
+	 * 在以下使用remove_work:
+	 *   - drivers/nvme/host/pci.c|3181| <<nvme_remove_dead_ctrl>> if (!queue_work(nvme_wq, &dev->remove_work))
+	 *   - drivers/nvme/host/pci.c|3369| <<nvme_remove_dead_ctrl_work>> struct nvme_dev *dev = container_of(work, struct nvme_dev, remove_work);
+	 *   - drivers/nvme/host/pci.c|3511| <<nvme_probe>> INIT_WORK(&dev->remove_work, nvme_remove_dead_ctrl_work);
+	 */
 	struct work_struct remove_work;
+	/*
+	 * 在以下使用shutdown_lock:
+	 *   - drivers/nvme/host/pci.c|2570| <<nvme_dev_disable>> mutex_lock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2615| <<nvme_dev_disable>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2719| <<nvme_reset_work>> mutex_lock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2745| <<nvme_reset_work>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2821| <<nvme_reset_work>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2965| <<nvme_probe>> mutex_init(&dev->shutdown_lock);
+	 */
 	struct mutex shutdown_lock;
 	bool subsystem;
+	/*
+	 * 在以下使用cmb_size:
+	 *   - drivers/nvme/host/pci.c|1548| <<nvme_cmb_qdepth>> if (q_size_aligned * nr_io_queues > dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1549| <<nvme_cmb_qdepth>> u64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);
+	 *   - drivers/nvme/host/pci.c|1946| <<nvme_map_cmb>> if (dev->cmb_size)
+	 *   - drivers/nvme/host/pci.c|1976| <<nvme_map_cmb>> dev->cmb_size = size;
+	 *   - drivers/nvme/host/pci.c|1991| <<nvme_release_cmb>> if (dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1994| <<nvme_release_cmb>> dev->cmb_size = 0;
+	 */
 	u64 cmb_size;
+	/*
+	 * 在以下使用cmb_use_sqes:
+	 *   - drivers/nvme/host/pci.c|1570| <<nvme_alloc_sq_cmds>> if (qid && dev->cmb_use_sqes && (dev->cmbsz & NVME_CMBSZ_SQS)) {
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+	 *   - drivers/nvme/host/pci.c|2306| <<nvme_setup_io_queues>> if (dev->cmb_use_sqes) {
+	 *   - drivers/nvme/host/pci.c|2312| <<nvme_setup_io_queues>> dev->cmb_use_sqes = false;
+	 */
 	bool cmb_use_sqes;
+	/*
+	 * 在以下使用cmbsz:
+	 *   - drivers/nvme/host/pci.c|1570| <<nvme_alloc_sq_cmds>> if (qid && dev->cmb_use_sqes && (dev->cmbsz & NVME_CMBSZ_SQS)) {
+	 *   - drivers/nvme/host/pci.c|1915| <<nvme_cmb_show>> ndev->cmbloc, ndev->cmbsz);
+	 *   - drivers/nvme/host/pci.c|1921| <<nvme_cmb_size_unit>> u8 szu = (dev->cmbsz >> NVME_CMBSZ_SZU_SHIFT) & NVME_CMBSZ_SZU_MASK;
+	 *   - drivers/nvme/host/pci.c|1928| <<nvme_cmb_size>> return (dev->cmbsz >> NVME_CMBSZ_SZ_SHIFT) & NVME_CMBSZ_SZ_MASK;
+	 *   - drivers/nvme/host/pci.c|1949| <<nvme_map_cmb>> dev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ);
+	 *   - drivers/nvme/host/pci.c|1950| <<nvme_map_cmb>> if (!dev->cmbsz)
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+	 *   - drivers/nvme/host/pci.c|1979| <<nvme_map_cmb>> if ((dev->cmbsz & (NVME_CMBSZ_WDS | NVME_CMBSZ_RDS)) ==
+	 */
 	u32 cmbsz;
+	/*
+	 * 在以下使用cmbloc:
+	 *   - drivers/nvme/host/pci.c|1915| <<nvme_cmb_show>> ndev->cmbloc, ndev->cmbsz);
+	 *   - drivers/nvme/host/pci.c|1952| <<nvme_map_cmb>> dev->cmbloc = readl(dev->bar + NVME_REG_CMBLOC);
+	 *   - drivers/nvme/host/pci.c|1955| <<nvme_map_cmb>> offset = nvme_cmb_size_unit(dev) * NVME_CMB_OFST(dev->cmbloc);
+	 *   - drivers/nvme/host/pci.c|1956| <<nvme_map_cmb>> bar = NVME_CMB_BIR(dev->cmbloc);
+	 *   - include/linux/nvme.h|160| <<NVME_CMB_BIR>> #define NVME_CMB_BIR(cmbloc) ((cmbloc) & 0x7)
+	 *   - include/linux/nvme.h|161| <<NVME_CMB_OFST>> #define NVME_CMB_OFST(cmbloc) (((cmbloc) >> 12) & 0xfffff)
+	 */
 	u32 cmbloc;
 	struct nvme_ctrl ctrl;
 	u32 last_ps;
 
+	/*
+	 * 在以下使用iod_mempool:
+	 *   - drivers/nvme/host/pci.c|976| <<nvme_unmap_data>> mempool_free(iod->sg, dev->iod_mempool);
+	 *   - drivers/nvme/host/pci.c|1226| <<nvme_map_data>> iod->sg = mempool_alloc(dev->iod_mempool, GFP_ATOMIC);
+	 *   - drivers/nvme/host/pci.c|3629| <<nvme_pci_free_ctrl>> mempool_destroy(dev->iod_mempool);
+	 *   - drivers/nvme/host/pci.c|4036| <<nvme_probe>> dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
+	 *   - drivers/nvme/host/pci.c|4040| <<nvme_probe>> if (!dev->iod_mempool) {
+	 *   - drivers/nvme/host/pci.c|4063| <<nvme_probe>> mempool_destroy(dev->iod_mempool);
+	 */
 	mempool_t *iod_mempool;
 
 	/* shadow doorbell buffer support: */
+	/*
+	 * 5.7 Doorbell Buffer Config command
+	 * The Doorbell Buffer Config command is used to provide two separate memory buffers that mirror the
+	 * controller's doorbell registers defined in section 3. This command is intended for emulated controllers and
+	 * is not typically supported by a physical NVMe controller. The two buffers are known as "Shadow Doorbell"
+	 * and "EventIdx", respectively. Refer to section 7.13 for an example of how these buffers may be used.
+	 *
+	 *
+	 * nvme: improve performance for virtual NVMe devices
+	 *
+	 * This change provides a mechanism to reduce the number of MMIO doorbell
+	 * writes for the NVMe driver. When running in a virtualized environment
+	 * like QEMU, the cost of an MMIO is quite hefy here. The main idea for
+	 * the patch is provide the device two memory location locations:
+	 * 1) to store the doorbell values so they can be lookup without the doorbell
+	 * MMIO write
+	 * 2) to store an event index.
+	 * I believe the doorbell value is obvious, the event index not so much.
+	 * Similar to the virtio specification, the virtual device can tell the
+	 * driver (guest OS) not to write MMIO unless you are writing past this
+	 * value.
+	 *
+	 * FYI: doorbell values are written by the nvme driver (guest OS) and the
+	 * event index is written by the virtual device (host OS).
+	 *
+	 * The patch implements a new admin command that will communicate where
+	 * these two memory locations reside. If the command fails, the nvme
+	 * driver will work as before without any optimizations.
+	 */
 	u32 *dbbuf_dbs;
 	dma_addr_t dbbuf_dbs_dma_addr;
 	u32 *dbbuf_eis;
 	dma_addr_t dbbuf_eis_dma_addr;
 
 	/* host memory buffer support: */
+	/*
+	 * The Host Memory Buffer feature provides a mechanism for the host to allocate a portion of host memory
+	 * for the exclusive use of the controller. After a successful completion of a Set Features command enabling
+	 * the host memory buffer, the host shall not write to:
+	 * a)The Host Memory Descriptor List (refer to Figure 296); and
+	 * b)the associated host memory region (i.e., the memory regions described by the Host Memory
+	 * Descriptor List),
+	 * until the host memory buffer has been disabled.
+	 */
 	u64 host_mem_size;
 	u32 nr_host_mem_descs;
+	/*
+	 * 在以下使用host_mem_descs_dma:
+	 *   - drivers/nvme/host/pci.c|2599| <<nvme_set_host_mem>> u64 dma_addr = dev->host_mem_descs_dma;
+	 *   - drivers/nvme/host/pci.c|2640| <<nvme_free_host_mem>> dev->host_mem_descs, dev->host_mem_descs_dma);
+	 *   - drivers/nvme/host/pci.c|2691| <<__nvme_alloc_host_mem>> dev->host_mem_descs_dma = descs_dma;
+	 */
 	dma_addr_t host_mem_descs_dma;
 	struct nvme_host_mem_buf_desc *host_mem_descs;
 	void **host_mem_desc_bufs;
@@ -141,11 +338,17 @@ static int io_queue_depth_set(const char *val, const struct kernel_param *kp)
 	return param_set_int(val, kp);
 }
 
+/*
+ * 返回submission queue tail doorbell
+ */
 static inline unsigned int sq_idx(unsigned int qid, u32 stride)
 {
 	return qid * 2 * stride;
 }
 
+/*
+ * 返回completion queue head doorbell
+ */
 static inline unsigned int cq_idx(unsigned int qid, u32 stride)
 {
 	return (qid * 2 + 1) * stride;
@@ -165,29 +368,101 @@ struct nvme_queue {
 	spinlock_t sq_lock;
 	void *sq_cmds;
 	 /* only used for poll queues: */
+	/*
+	 * 在以下使用cq_poll_lock:
+	 *   - drivers/nvme/host/pci.c|1063| <<nvme_poll_irqdisable>> spin_lock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1065| <<nvme_poll_irqdisable>> spin_unlock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1106| <<nvme_poll>> spin_lock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1108| <<nvme_poll>> spin_unlock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1500| <<nvme_alloc_queue>> spin_lock_init(&nvmeq->cq_poll_lock);
+	 */
 	spinlock_t cq_poll_lock ____cacheline_aligned_in_smp;
 	volatile struct nvme_completion *cqes;
 	struct blk_mq_tags **tags;
+	/*
+	 * 在以下使用sq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1324| <<adapter_alloc_sq>> c.create_sq.prp1 = cpu_to_le64(nvmeq->sq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1523| <<nvme_free_queue>> nvmeq->sq_cmds, nvmeq->sq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1609| <<nvme_alloc_sq_cmds>> nvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev,
+	 *   - drivers/nvme/host/pci.c|1611| <<nvme_alloc_sq_cmds>> if (nvmeq->sq_dma_addr) {
+	 *   - drivers/nvme/host/pci.c|1621| <<nvme_alloc_sq_cmds>> &nvmeq->sq_dma_addr, GFP_KERNEL);
+	 *   - drivers/nvme/host/pci.c|1888| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
+	 */
 	dma_addr_t sq_dma_addr;
+	/*
+	 * 在以下使用cq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1294| <<adapter_alloc_cq>> c.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1514| <<nvme_free_queue>> (void *)nvmeq->cqes, nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1637| <<nvme_alloc_queue>> &nvmeq->cq_dma_addr, GFP_KERNEL);
+	 *   - drivers/nvme/host/pci.c|1657| <<nvme_alloc_queue>> nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1889| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
+	 */
 	dma_addr_t cq_dma_addr;
+	/*
+	 * 在以下使用q_db:
+	 *   - drivers/nvme/host/pci.c|729| <<nvme_write_sq_db>> writel(nvmeq->sq_tail, nvmeq->q_db);
+	 *   - drivers/nvme/host/pci.c|1218| <<nvme_ring_cq_doorbell>> writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
+	 *   - drivers/nvme/host/pci.c|1902| <<nvme_alloc_queue>> nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
+	 *   - drivers/nvme/host/pci.c|1969| <<nvme_init_queue>> nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
+	 *   - drivers/nvme/host/pci.c|2838| <<nvme_setup_io_queues>> adminq->q_db = dev->dbs;
+	 */
 	u32 __iomem *q_db;
 	u16 q_depth;
+	/*
+	 * 设置cq_vector的地方:
+	 *   - drivers/nvme/host/pci.c|1796| <<nvme_create_queue>> nvmeq->cq_vector = vector;
+	 *   - drivers/nvme/host/pci.c|1969| <<nvme_pci_configure_admin_queue>> nvmeq->cq_vector = 0;
+	 */
 	u16 cq_vector;
 	u16 sq_tail;
+	/*
+	 * 在以下使用last_sq_tail:
+	 *   - drivers/nvme/host/pci.c|734| <<nvme_write_sq_db>> if (next_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|741| <<nvme_write_sq_db>> nvmeq->last_sq_tail = nvmeq->sq_tail;
+	 *   - drivers/nvme/host/pci.c|779| <<nvme_commit_rqs>> if (nvmeq->sq_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|2021| <<nvme_init_queue>> nvmeq->last_sq_tail = 0;
+	 */
 	u16 last_sq_tail;
 	u16 cq_head;
 	u16 qid;
 	u8 cq_phase;
 	u8 sqes;
 	unsigned long flags;
+/*
+ * 在以下使用NVMEQ_ENABLED:
+ *   - drivers/nvme/host/pci.c|1001| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+ *   - drivers/nvme/host/pci.c|1560| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+ *   - drivers/nvme/host/pci.c|1805| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+ *   - drivers/nvme/host/pci.c|1977| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+ *   - drivers/nvme/host/pci.c|2468| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+ *   - drivers/nvme/host/pci.c|2516| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+ */
 #define NVMEQ_ENABLED		0
 #define NVMEQ_SQ_CMB		1
 #define NVMEQ_DELETE_ERROR	2
+/*
+ * 在以下使用NVMEQ_POLLED:
+ *   - drivers/nvme/host/pci.c|1228| <<nvme_poll_irqdisable>> if (test_bit(NVMEQ_POLLED, &nvmeq->flags)) {
+ *   - drivers/nvme/host/pci.c|1325| <<adapter_alloc_cq>> if (!test_bit(NVMEQ_POLLED, &nvmeq->flags))
+ *   - drivers/nvme/host/pci.c|1592| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_POLLED, &nvmeq->flags))
+ *   - drivers/nvme/host/pci.c|1816| <<nvme_create_queue>> set_bit(NVMEQ_POLLED, &nvmeq->flags);
+ */
 #define NVMEQ_POLLED		3
 	u32 *dbbuf_sq_db;
 	u32 *dbbuf_cq_db;
 	u32 *dbbuf_sq_ei;
+	/*
+	 * 在以下使用dbbuf_cq_ei:
+	 *   - drivers/nvme/host/pci.c|434| <<nvme_dbbuf_init>> nvmeq->dbbuf_cq_ei = &dev->dbbuf_eis[cq_idx(qid, dev->db_stride)];
+	 *   - drivers/nvme/host/pci.c|1133| <<nvme_ring_cq_doorbell>> nvmeq->dbbuf_cq_ei))
+	 */
 	u32 *dbbuf_cq_ei;
+	/*
+	 * 在以下使用delete_done:
+	 *   - drivers/nvme/host/pci.c|3095| <<nvme_del_queue_end>> complete(&nvmeq->delete_done);
+	 *   - drivers/nvme/host/pci.c|3129| <<nvme_delete_queue>> init_completion(&nvmeq->delete_done);
+	 *   - drivers/nvme/host/pci.c|3157| <<__nvme_disable_io_queues>> timeout = wait_for_completion_io_timeout(&nvmeq->delete_done,
+	 */
 	struct completion delete_done;
 };
 
@@ -210,6 +485,11 @@ struct nvme_iod {
 	struct scatterlist *sg;
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|340| <<max_queue_count>> return 1 + max_io_queues();
+ *   - drivers/nvme/host/pci.c|2529| <<nvme_setup_io_queues>> nr_io_queues = max_io_queues();
+ */
 static unsigned int max_io_queues(void)
 {
 	return num_possible_cpus() + write_queues + poll_queues;
@@ -226,6 +506,10 @@ static inline unsigned int nvme_dbbuf_size(u32 stride)
 	return (max_queue_count() * 8 * stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3389| <<nvme_reset_work>> result = nvme_dbbuf_dma_alloc(dev);
+ */
 static int nvme_dbbuf_dma_alloc(struct nvme_dev *dev)
 {
 	unsigned int mem_size = nvme_dbbuf_size(dev->db_stride);
@@ -267,9 +551,21 @@ static void nvme_dbbuf_dma_free(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1778| <<nvme_init_queue>> nvme_dbbuf_init(dev, nvmeq, qid);
+ */
 static void nvme_dbbuf_init(struct nvme_dev *dev,
 			    struct nvme_queue *nvmeq, int qid)
 {
+	/*
+	 * 在qemu上测试:
+	 * [    0.832968] orabug: nvme_dbbuf_init() qid=0, 0x0000000000000000
+	 * [    0.835749] orabug: nvme_dbbuf_init() qid=1, 0x0000000000000000
+	 * [    0.836705] orabug: nvme_dbbuf_init() qid=2, 0x0000000000000000
+	 * [    0.837599] orabug: nvme_dbbuf_init() qid=3, 0x0000000000000000
+	 * [    0.838496] orabug: nvme_dbbuf_init() qid=4, 0x0000000000000000
+	 */
 	if (!dev->dbbuf_dbs || !qid)
 		return;
 
@@ -298,15 +594,39 @@ static void nvme_dbbuf_set(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|485| <<nvme_dbbuf_update_and_check_event>> if (!nvme_dbbuf_need_event(*dbbuf_ei, value, old_value))
+ */
 static inline int nvme_dbbuf_need_event(u16 event_idx, u16 new_idx, u16 old)
 {
 	return (u16)(new_idx - event_idx - 1) < (u16)(new_idx - old);
 }
 
 /* Update dbbuf and return true if an MMIO is required */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|651| <<nvme_write_sq_db>> if (nvme_dbbuf_update_and_check_event(nvmeq->sq_tail,
+ *   - drivers/nvme/host/pci.c|1132| <<nvme_ring_cq_doorbell>> if (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,
+ */
 static bool nvme_dbbuf_update_and_check_event(u16 value, u32 *dbbuf_db,
 					      volatile u32 *dbbuf_ei)
 {
+	/*
+	 * Controllers that support the Doorbell Buffer Config command are typically emulated controllers where this
+	 * feature is used to enhance the performance of host software running in Virtual Machines. If supported by
+	 * he controller, host software may enable Shadow Doorbell buffers by submitting the Doorbell Buffer Config
+	 * command (refer to section 5.7).
+	 * After the completion of the Doorbell Buffer Config command, host software shall submit commands by
+	 * updating the appropriate entry in the Shadow Doorbell buffer instead of updating the controller's
+	 * corresponding doorbell register. If updating an entry in the Shadow Doorbell buffer changes the value from
+	 * being less than or equal to the value of the corresponding EventIdx buffer entry to being greater than that
+	 * value, then the host shall also update the controller's corresponding doorbell register to match the value of
+	 * that entry in the Shadow Doorbell buffer. Queue wrap conditions shall be taken into account in all
+	 * comparisons in this paragraph.
+	 *
+	 * 在qemu测试(猜测bm也是如此), dbbuf_db总是NULL
+	 */
 	if (dbbuf_db) {
 		u16 old_value;
 
@@ -339,6 +659,10 @@ static bool nvme_dbbuf_update_and_check_event(u16 value, u32 *dbbuf_db,
  * as it only leads to a small amount of wasted memory for the lifetime of
  * the I/O.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|658| <<nvme_pci_iod_alloc_size>> alloc_size = sizeof(__le64 *) * nvme_npages(size, dev);
+ */
 static int nvme_npages(unsigned size, struct nvme_dev *dev)
 {
 	unsigned nprps = DIV_ROUND_UP(size + dev->ctrl.page_size,
@@ -350,11 +674,19 @@ static int nvme_npages(unsigned size, struct nvme_dev *dev)
  * Calculates the number of pages needed for the SGL segments. For example a 4k
  * page can accommodate 256 SGL descriptors.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|677| <<nvme_pci_iod_alloc_size>> alloc_size = sizeof(__le64 *) * nvme_pci_npages_sgl(nseg);
+ */
 static int nvme_pci_npages_sgl(unsigned int num_seg)
 {
 	return DIV_ROUND_UP(num_seg * sizeof(struct nvme_sgl_desc), PAGE_SIZE);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3823| <<nvme_probe>> alloc_size = nvme_pci_iod_alloc_size(dev, NVME_MAX_KB_SZ,
+ */
 static unsigned int nvme_pci_iod_alloc_size(struct nvme_dev *dev,
 		unsigned int size, unsigned int nseg, bool use_sgl)
 {
@@ -368,6 +700,9 @@ static unsigned int nvme_pci_iod_alloc_size(struct nvme_dev *dev,
 	return alloc_size + sizeof(struct scatterlist) * nseg;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.init_hctx = nvme_admin_init_hctx()
+ */
 static int nvme_admin_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 				unsigned int hctx_idx)
 {
@@ -379,10 +714,18 @@ static int nvme_admin_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	WARN_ON(nvmeq->tags);
 
 	hctx->driver_data = nvmeq;
+	/*
+	 * struct nvme_dev:
+	 *  - struct blk_mq_tag_set admin_tagset;
+	 *     - struct blk_mq_tags **tags;
+	 */
 	nvmeq->tags = &dev->admin_tagset.tags[0];
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.exit_hctx = nvme_admin_exit_hctx()
+ */
 static void nvme_admin_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
@@ -390,6 +733,9 @@ static void nvme_admin_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_i
 	nvmeq->tags = NULL;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_ops.init_hctx = nvme_init_hctx()
+ */
 static int nvme_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 			  unsigned int hctx_idx)
 {
@@ -404,6 +750,10 @@ static int nvme_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.init_request = nvme_init_request()
+ * struct blk_mq_ops nvme_mq_ops.init_request = nvme_init_request()
+ */
 static int nvme_init_request(struct blk_mq_tag_set *set, struct request *req,
 		unsigned int hctx_idx, unsigned int numa_node)
 {
@@ -419,6 +769,10 @@ static int nvme_init_request(struct blk_mq_tag_set *set, struct request *req,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|574| <<nvme_pci_map_queues>> offset = queue_irq_offset(dev);
+ */
 static int queue_irq_offset(struct nvme_dev *dev)
 {
 	/* if we have more than 1 vec, admin queue offsets us by 1 */
@@ -428,12 +782,18 @@ static int queue_irq_offset(struct nvme_dev *dev)
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_ops.map_queues = nvme_pci_map_queues()
+ */
 static int nvme_pci_map_queues(struct blk_mq_tag_set *set)
 {
 	struct nvme_dev *dev = set->driver_data;
 	int i, qoff, offset;
 
 	offset = queue_irq_offset(dev);
+	/*
+	 * i是read, write和poll的种类
+	 */
 	for (i = 0, qoff = 0; i < set->nr_maps; i++) {
 		struct blk_mq_queue_map *map = &set->map[i];
 
@@ -462,9 +822,18 @@ static int nvme_pci_map_queues(struct blk_mq_tag_set *set)
 /*
  * Write sq tail if we are asked to, or if the next command would wrap.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|767| <<nvme_submit_cmd>> nvme_write_sq_db(nvmeq, write_sq);
+ *   - drivers/nvme/host/pci.c|780| <<nvme_commit_rqs>> nvme_write_sq_db(nvmeq, true);
+ */
 static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
 {
 	if (!write_sq) {
+		/*
+		 * 如果不commit的话, sq_tail在增长, 其实next_tail一直比last_sq_tail大
+		 * 所以就会return
+		 */
 		u16 next_tail = nvmeq->sq_tail + 1;
 
 		if (next_tail == nvmeq->q_depth)
@@ -476,6 +845,13 @@ static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
 	if (nvme_dbbuf_update_and_check_event(nvmeq->sq_tail,
 			nvmeq->dbbuf_sq_db, nvmeq->dbbuf_sq_ei))
 		writel(nvmeq->sq_tail, nvmeq->q_db);
+	/*
+	 * 在以下使用last_sq_tail:
+	 *   - drivers/nvme/host/pci.c|734| <<nvme_write_sq_db>> if (next_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|741| <<nvme_write_sq_db>> nvmeq->last_sq_tail = nvmeq->sq_tail;
+	 *   - drivers/nvme/host/pci.c|779| <<nvme_commit_rqs>> if (nvmeq->sq_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|2021| <<nvme_init_queue>> nvmeq->last_sq_tail = 0;
+	 */
 	nvmeq->last_sq_tail = nvmeq->sq_tail;
 }
 
@@ -485,10 +861,21 @@ static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
  * @cmd: The command to send
  * @write_sq: whether to write to the SQ doorbell
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1185| <<nvme_queue_rq>> nvme_submit_cmd(nvmeq, &cmnd, bd->last);
+ *   - drivers/nvme/host/pci.c|1496| <<nvme_pci_submit_async_event>> nvme_submit_cmd(nvmeq, &c, true);
+ */
 static void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
 			    bool write_sq)
 {
 	spin_lock(&nvmeq->sq_lock);
+	/*
+	 * struct nvme_queue:
+	 *  - void *sq_cmds;
+	 *
+	 * nvmeq->sqes是每个sq entry的大小 (其实是最大)
+	 */
 	memcpy(nvmeq->sq_cmds + (nvmeq->sq_tail << nvmeq->sqes),
 	       cmd, sizeof(*cmd));
 	if (++nvmeq->sq_tail == nvmeq->q_depth)
@@ -497,22 +884,64 @@ static void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
 	spin_unlock(&nvmeq->sq_lock);
 }
 
+/*
+ * commit 04f3eafda6e05adc56afed4d3ae6e24aaa429058
+ * Author: Jens Axboe <axboe@kernel.dk>
+ * Date:   Thu Nov 29 10:02:29 2018 -0700
+ *
+ * nvme: implement mq_ops->commit_rqs() hook
+ *
+ * Split the command submission and the SQ doorbell ring, and add the
+ * doorbell ring as our ->commit_rqs() hook. This allows a list of
+ * requests to be issued, with nvme only writing the SQ update when
+ * it's necessary. This is more efficient if we have lists of requests
+ * to issue, particularly on virtualized hardware, where writing the
+ * SQ doorbell is more expensive than on real hardware. For those cases,
+ * performance increases of 2-3x have been observed.
+ *
+ * The use case for this is plugged IO, where blk-mq flushes a batch of
+ * requests at the time.
+ *
+ * Reviewed-by: Christoph Hellwig <hch@lst.de>
+ * Signed-off-by: Jens Axboe <axboe@kernel.dk>
+ *
+ * struct blk_mq_ops nvme_mq_ops.commit_rqs = nvme_commit_rqs()
+ */
 static void nvme_commit_rqs(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
 
 	spin_lock(&nvmeq->sq_lock);
+	/*
+	 * 在以下使用last_sq_tail:
+	 *   - drivers/nvme/host/pci.c|734| <<nvme_write_sq_db>> if (next_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|741| <<nvme_write_sq_db>> nvmeq->last_sq_tail = nvmeq->sq_tail;
+	 *   - drivers/nvme/host/pci.c|779| <<nvme_commit_rqs>> if (nvmeq->sq_tail != nvmeq->last_sq_tail)
+	 *   - drivers/nvme/host/pci.c|2021| <<nvme_init_queue>> nvmeq->last_sq_tail = 0;
+	 */
 	if (nvmeq->sq_tail != nvmeq->last_sq_tail)
 		nvme_write_sq_db(nvmeq, true);
 	spin_unlock(&nvmeq->sq_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|955| <<nvme_unmap_data>> dma_pool_free(dev->prp_small_pool, nvme_pci_iod_list(req)[0],
+ *   - drivers/nvme/host/pci.c|959| <<nvme_unmap_data>> void *addr = nvme_pci_iod_list(req)[i];
+ *   - drivers/nvme/host/pci.c|1005| <<nvme_pci_setup_prps>> void **list = nvme_pci_iod_list(req);
+ *   - drivers/nvme/host/pci.c|1139| <<nvme_pci_setup_sgls>> nvme_pci_iod_list(req)[0] = sg_list;
+ *   - drivers/nvme/host/pci.c|1154| <<nvme_pci_setup_sgls>> nvme_pci_iod_list(req)[iod->npages++] = sg_list;
+ */
 static void **nvme_pci_iod_list(struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
 	return (void **)(iod->sg + blk_rq_nr_phys_segments(req));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1243| <<nvme_map_data>> iod->use_sgl = nvme_pci_use_sgls(dev, req);
+ */
 static inline bool nvme_pci_use_sgls(struct nvme_dev *dev, struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -533,6 +962,12 @@ static inline bool nvme_pci_use_sgls(struct nvme_dev *dev, struct request *req)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1116| <<nvme_map_data>> nvme_unmap_data(dev, req);
+ *   - drivers/nvme/host/pci.c|1182| <<nvme_queue_rq>> nvme_unmap_data(dev, req);
+ *   - drivers/nvme/host/pci.c|1201| <<nvme_pci_complete_rq>> nvme_unmap_data(dev, req);
+ */
 static void nvme_unmap_data(struct nvme_dev *dev, struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -767,6 +1202,10 @@ static blk_status_t nvme_pci_setup_sgls(struct nvme_dev *dev,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1248| <<nvme_map_data>> return nvme_setup_prp_simple(dev, req,
+ */
 static blk_status_t nvme_setup_prp_simple(struct nvme_dev *dev,
 		struct request *req, struct nvme_rw_command *cmnd,
 		struct bio_vec *bv)
@@ -786,6 +1225,10 @@ static blk_status_t nvme_setup_prp_simple(struct nvme_dev *dev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1253| <<nvme_map_data>> return nvme_setup_sgl_simple(dev, req,
+ */
 static blk_status_t nvme_setup_sgl_simple(struct nvme_dev *dev,
 		struct request *req, struct nvme_rw_command *cmnd,
 		struct bio_vec *bv)
@@ -804,6 +1247,10 @@ static blk_status_t nvme_setup_sgl_simple(struct nvme_dev *dev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1339| <<nvme_queue_rq>> ret = nvme_map_data(dev, req, &cmnd);
+ */
 static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 		struct nvme_command *cmnd)
 {
@@ -855,6 +1302,10 @@ static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1345| <<nvme_queue_rq>> ret = nvme_map_metadata(dev, req, &cmnd);
+ */
 static blk_status_t nvme_map_metadata(struct nvme_dev *dev, struct request *req,
 		struct nvme_command *cmnd)
 {
@@ -871,6 +1322,10 @@ static blk_status_t nvme_map_metadata(struct nvme_dev *dev, struct request *req,
 /*
  * NOTE: ns is NULL when called on the admin queue.
  */
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.queue_rq = nvme_queue_rq()
+ * struct blk_mq_ops nvme_mq_ops.queue_rq = nvme_queue_rq()
+ */
 static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 			 const struct blk_mq_queue_data *bd)
 {
@@ -882,6 +1337,11 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	struct nvme_command cmnd;
 	blk_status_t ret;
 
+	/*
+	 * 修改aborted的地方:
+	 *   - drivers/nvme/host/pci.c|1264| <<nvme_queue_rq>> iod->aborted = 0;
+	 *   - drivers/nvme/host/pci.c|1841| <<nvme_timeout>> iod->aborted = 1;
+	 */
 	iod->aborted = 0;
 	iod->npages = -1;
 	iod->nents = 0;
@@ -919,6 +1379,10 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.complete = nvme_pci_complete_rq()
+ * struct blk_mq_ops nvme_mq_ops.complete = nvme_pci_complete_rq()
+ */
 static void nvme_pci_complete_rq(struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -933,14 +1397,42 @@ static void nvme_pci_complete_rq(struct request *req)
 }
 
 /* We read the CQE phase first to check if the rest of the entry is valid */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1310| <<nvme_process_cq>> while (nvme_cqe_pending(nvmeq)) {
+ *   - drivers/nvme/host/pci.c|1353| <<nvme_irq_check>> if (nvme_cqe_pending(nvmeq))
+ *   - drivers/nvme/host/pci.c|1430| <<nvme_poll>> if (!nvme_cqe_pending(nvmeq))
+ */
 static inline bool nvme_cqe_pending(struct nvme_queue *nvmeq)
 {
+	/*
+	 * struct nvme_queue:
+	 *   - volatile struct nvme_completion *cqes;
+	 *
+	 * Phase Tag (P): Identifies whether a Completion Queue entry is new. The Phase Tag values for
+	 * all Completion Queue entries shall be initialized to '0' by host software prior to setting CC.EN to
+	 * '1'. When the controller places an entry in the Completion Queue, the controller shall invert the
+	 * Phase Tag to enable host software to discriminate a new entry. Specifically, for the first set of
+	 * completion queue entries after CC.EN is set to '1' all Phase Tags are set to '1' when they are
+	 * posted. For the second set of completion queue entries, when the controller has wrapped around
+	 * to the top of the Completion Queue, all Phase Tags are cleared to '0' when they are posted. The
+	 * value of the Phase Tag is inverted each pass through the Completion Queue.
+	 * This is a reserved bit in NVMe over Fabrics implementations.
+	 */
 	return (le16_to_cpu(nvmeq->cqes[nvmeq->cq_head].status) & 1) ==
 			nvmeq->cq_phase;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1223| <<nvme_process_cq>> nvme_ring_cq_doorbell(nvmeq);
+ */
 static inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)
 {
+	/*
+	 * sq: driver更新tail, controller更新head
+	 * cq: driver更新head, controller更新tail
+	 */
 	u16 head = nvmeq->cq_head;
 
 	if (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,
@@ -948,8 +1440,16 @@ static inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)
 		writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|983| <<nvme_complete_cqes>> nvme_handle_cqe(nvmeq, start);
+ */
 static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 {
+	/*
+	 * struct nvme_queue:
+	 *   - volatile struct nvme_completion *cqes;
+	 */
 	volatile struct nvme_completion *cqe = &nvmeq->cqes[idx];
 	struct request *req;
 
@@ -966,6 +1466,14 @@ static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 	 * aborts.  We don't even bother to allocate a struct request
 	 * for them but rather special case them here.
 	 */
+	/*
+	 * 使用admin queue来接受async event notification
+	 * 比如IO的sq的head写错了, 就会触发admin queue接收一个cqe用来aen
+	 *
+	 * 关于admin ring buffer和async event, nvme的admin queue的tagset
+	 * 有30个元素, 但是ring buffer有32个,除了一个用来表示full, 另外
+	 * 一个用在async event
+	 */
 	if (unlikely(nvme_is_aen_req(nvmeq->qid, cqe->command_id))) {
 		nvme_complete_async_event(&nvmeq->dev->ctrl,
 				cqe->status, &cqe->result);
@@ -974,9 +1482,22 @@ static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 
 	req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
 	trace_nvme_sq(req, cqe->sq_head, nvmeq->sq_tail);
+	/*
+	 * 一开始volatile struct nvme_completion *cqe = &nvmeq->cqes[idx];
+	 *
+	 * status是__le16  status; --> 16位
+	 */
 	nvme_end_request(req, cqe->status, cqe->result);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1032| <<nvme_irq>> nvme_complete_cqes(nvmeq, start, end);
+ *   - drivers/nvme/host/pci.c|1072| <<nvme_poll_irqdisable>> nvme_complete_cqes(nvmeq, start, end);
+ *   - drivers/nvme/host/pci.c|1110| <<nvme_poll>> nvme_complete_cqes(nvmeq, start, end);
+ *
+ * 函数实现中的条件是(start != end), 说明end不会被处理
+ */
 static void nvme_complete_cqes(struct nvme_queue *nvmeq, u16 start, u16 end)
 {
 	while (start != end) {
@@ -986,6 +1507,12 @@ static void nvme_complete_cqes(struct nvme_queue *nvmeq, u16 start, u16 end)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1033| <<nvme_process_cq>> nvme_update_cq_head(nvmeq);
+ *
+ * 核心思想是增加nvmeq->cq_head
+ */
 static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
 {
 	if (nvmeq->cq_head == nvmeq->q_depth - 1) {
@@ -996,6 +1523,13 @@ static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1046| <<nvme_irq>> nvme_process_cq(nvmeq, &start, &end, -1);
+ *   - drivers/nvme/host/pci.c|1082| <<nvme_poll_irqdisable>> found = nvme_process_cq(nvmeq, &start, &end, tag);
+ *   - drivers/nvme/host/pci.c|1086| <<nvme_poll_irqdisable>> found = nvme_process_cq(nvmeq, &start, &end, tag);
+ *   - drivers/nvme/host/pci.c|1125| <<nvme_poll>> found = nvme_process_cq(nvmeq, &start, &end, -1);
+ */
 static inline int nvme_process_cq(struct nvme_queue *nvmeq, u16 *start,
 				  u16 *end, unsigned int tag)
 {
@@ -1003,8 +1537,12 @@ static inline int nvme_process_cq(struct nvme_queue *nvmeq, u16 *start,
 
 	*start = nvmeq->cq_head;
 	while (nvme_cqe_pending(nvmeq)) {
+		/*
+		 * 什么时候tag是-1呢?
+		 */
 		if (tag == -1U || nvmeq->cqes[nvmeq->cq_head].command_id == tag)
 			found++;
+		/* 核心思想是增加nvmeq->cq_head */
 		nvme_update_cq_head(nvmeq);
 	}
 	*end = nvmeq->cq_head;
@@ -1014,6 +1552,11 @@ static inline int nvme_process_cq(struct nvme_queue *nvmeq, u16 *start,
 	return found;
 }
 
+/*
+ * 在以下使用nvme_irq():
+ *   - drivers/nvme/host/pci.c|1583| <<queue_request_irq>> nvme_irq, nvmeq, "nvme%dq%d", nr, nvmeq->qid);
+ *   - drivers/nvme/host/pci.c|1585| <<queue_request_irq>> return pci_request_irq(pdev, nvmeq->cq_vector, nvme_irq,
+ */
 static irqreturn_t nvme_irq(int irq, void *data)
 {
 	struct nvme_queue *nvmeq = data;
@@ -1048,6 +1591,12 @@ static irqreturn_t nvme_irq_check(int irq, void *data)
  * Poll for completions any queue, including those not dedicated to polling.
  * Can be called from any context.
  */
+/*
+ * calld by:
+ *   - drivers/nvme/host/pci.c|1318| <<nvme_timeout>> if (nvme_poll_irqdisable(nvmeq, req->tag)) {
+ *   - drivers/nvme/host/pci.c|1461| <<nvme_disable_admin_queue>> nvme_poll_irqdisable(nvmeq, -1);
+ *   - drivers/nvme/host/pci.c|2347| <<__nvme_disable_io_queues>> nvme_poll_irqdisable(nvmeq, -1);
+ */
 static int nvme_poll_irqdisable(struct nvme_queue *nvmeq, unsigned int tag)
 {
 	struct pci_dev *pdev = to_pci_dev(nvmeq->dev->dev);
@@ -1060,6 +1609,14 @@ static int nvme_poll_irqdisable(struct nvme_queue *nvmeq, unsigned int tag)
 	 * to disable the interrupt to avoid racing with it.
 	 */
 	if (test_bit(NVMEQ_POLLED, &nvmeq->flags)) {
+		/*
+		 * 在以下使用cq_poll_lock:
+		 *   - drivers/nvme/host/pci.c|1063| <<nvme_poll_irqdisable>> spin_lock(&nvmeq->cq_poll_lock);
+		 *   - drivers/nvme/host/pci.c|1065| <<nvme_poll_irqdisable>> spin_unlock(&nvmeq->cq_poll_lock);
+		 *   - drivers/nvme/host/pci.c|1106| <<nvme_poll>> spin_lock(&nvmeq->cq_poll_lock);
+		 *   - drivers/nvme/host/pci.c|1108| <<nvme_poll>> spin_unlock(&nvmeq->cq_poll_lock);
+		 *   - drivers/nvme/host/pci.c|1500| <<nvme_alloc_queue>> spin_lock_init(&nvmeq->cq_poll_lock);
+		 */
 		spin_lock(&nvmeq->cq_poll_lock);
 		found = nvme_process_cq(nvmeq, &start, &end, tag);
 		spin_unlock(&nvmeq->cq_poll_lock);
@@ -1073,6 +1630,29 @@ static int nvme_poll_irqdisable(struct nvme_queue *nvmeq, unsigned int tag)
 	return found;
 }
 
+/*
+ * 激活poll的方法:
+ *
+ * 激活io_poll:
+ * # echo 1 > /sys/block/nvme0n1/queue/io_poll
+ *
+ * 在fio中使用pvsync2加上hipri:
+ * # fio -name iops -rw=read -bs=4k -runtime=60 -iodepth 32 -filename /dev/nvme0n1 -ioengine pvsync2 -direct=1 -hipri=1
+ *
+ * [0] nvme_poll
+ * [0] blk_poll
+ * [0] __blkdev_direct_IO_simple
+ * [0] blkdev_direct_IO
+ * [0] generic_file_read_iter
+ * [0] do_iter_readv_writev
+ * [0] do_iter_read
+ * [0] vfs_readv
+ * [0] do_preadv
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * struct blk_mq_ops nvme_mq_ops.poll = nvme_poll()
+ */
 static int nvme_poll(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
@@ -1082,6 +1662,14 @@ static int nvme_poll(struct blk_mq_hw_ctx *hctx)
 	if (!nvme_cqe_pending(nvmeq))
 		return 0;
 
+	/*
+	 * 在以下使用cq_poll_lock:
+	 *   - drivers/nvme/host/pci.c|1063| <<nvme_poll_irqdisable>> spin_lock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1065| <<nvme_poll_irqdisable>> spin_unlock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1106| <<nvme_poll>> spin_lock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1108| <<nvme_poll>> spin_unlock(&nvmeq->cq_poll_lock);
+	 *   - drivers/nvme/host/pci.c|1500| <<nvme_alloc_queue>> spin_lock_init(&nvmeq->cq_poll_lock);
+	 */
 	spin_lock(&nvmeq->cq_poll_lock);
 	found = nvme_process_cq(nvmeq, &start, &end, -1);
 	spin_unlock(&nvmeq->cq_poll_lock);
@@ -1090,6 +1678,12 @@ static int nvme_poll(struct blk_mq_hw_ctx *hctx)
 	return found;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|4584| <<nvme_async_event_work>> ctrl->ops->submit_async_event(ctrl);
+ *
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.submit_async_event()
+ */
 static void nvme_pci_submit_async_event(struct nvme_ctrl *ctrl)
 {
 	struct nvme_dev *dev = to_nvme_dev(ctrl);
@@ -1098,10 +1692,22 @@ static void nvme_pci_submit_async_event(struct nvme_ctrl *ctrl)
 
 	memset(&c, 0, sizeof(c));
 	c.common.opcode = nvme_admin_async_event;
+	/*
+	 * 这里command_id是31
+	 *
+	 * 关于admin ring buffer和async event, nvme的admin queue的
+	 * tagset有30个元素, 但是ring buffer有32个,除了一个用来表示
+	 * full, 另外一个用在async event
+	 */
 	c.common.command_id = NVME_AQ_BLK_MQ_DEPTH;
 	nvme_submit_cmd(nvmeq, &c, true);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1755| <<adapter_delete_cq>> return adapter_delete_queue(dev, nvme_admin_delete_cq, cqid);
+ *   - drivers/nvme/host/pci.c|1764| <<adapter_delete_sq>> return adapter_delete_queue(dev, nvme_admin_delete_sq, sqid);
+ */
 static int adapter_delete_queue(struct nvme_dev *dev, u8 opcode, u16 id)
 {
 	struct nvme_command c;
@@ -1113,6 +1719,13 @@ static int adapter_delete_queue(struct nvme_dev *dev, u8 opcode, u16 id)
 	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1825| <<nvme_create_queue>> result = adapter_alloc_cq(dev, qid, nvmeq, vector);
+ *
+ * admin的sq和cq的dma地址是写入bar的
+ * 而io queue的dma通过admin的cmd下发设置
+ */
 static int adapter_alloc_cq(struct nvme_dev *dev, u16 qid,
 		struct nvme_queue *nvmeq, s16 vector)
 {
@@ -1137,6 +1750,13 @@ static int adapter_alloc_cq(struct nvme_dev *dev, u16 qid,
 	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1829| <<nvme_create_queue>> result = adapter_alloc_sq(dev, qid, nvmeq);
+ *
+ * admin的sq和cq的dma地址是写入bar的
+ * 而io queue的dma通过admin的cmd下发设置
+ */
 static int adapter_alloc_sq(struct nvme_dev *dev, u16 qid,
 						struct nvme_queue *nvmeq)
 {
@@ -1167,16 +1787,30 @@ static int adapter_alloc_sq(struct nvme_dev *dev, u16 qid,
 	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1916| <<nvme_create_queue>> adapter_delete_cq(dev, qid);
+ */
 static int adapter_delete_cq(struct nvme_dev *dev, u16 cqid)
 {
 	return adapter_delete_queue(dev, nvme_admin_delete_cq, cqid);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1914| <<nvme_create_queue>> adapter_delete_sq(dev, qid);
+ */
 static int adapter_delete_sq(struct nvme_dev *dev, u16 sqid)
 {
 	return adapter_delete_queue(dev, nvme_admin_delete_sq, sqid);
 }
 
+/*
+ * 在以下使用abort_endio():
+ *   - drivers/nvme/host/pci.c|2006| <<nvme_timeout>> blk_execute_rq_nowait(abort_req->q, NULL, abort_req, 0, abort_endio);
+ *
+ * admin的abort request的callback
+ */
 static void abort_endio(struct request *req, blk_status_t error)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -1184,10 +1818,25 @@ static void abort_endio(struct request *req, blk_status_t error)
 
 	dev_warn(nvmeq->dev->ctrl.device,
 		 "Abort status: 0x%x", nvme_req(req)->status);
+	/*
+	 * 在以下使用abort_limit:
+	 *   - drivers/nvme/host/core.c|3345| <<nvme_init_identify>> atomic_set(&ctrl->abort_limit, id->acl + 1);
+	 *   - drivers/nvme/host/pci.c|1715| <<abort_endio>> atomic_inc(&nvmeq->dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1862| <<nvme_timeout>> if (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {
+	 *   - drivers/nvme/host/pci.c|1863| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1887| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 */
 	atomic_inc(&nvmeq->dev->ctrl.abort_limit);
 	blk_mq_free_request(req);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1900| <<nvme_timeout>> if (nvme_should_reset(dev, csts)) {
+ *
+ * csts相当于:
+ * u32 csts = readl(dev->bar + NVME_REG_CSTS);
+ */
 static bool nvme_should_reset(struct nvme_dev *dev, u32 csts)
 {
 
@@ -1214,6 +1863,10 @@ static bool nvme_should_reset(struct nvme_dev *dev, u32 csts)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1901| <<nvme_timeout>> nvme_warn_reset(dev, csts);
+ */
 static void nvme_warn_reset(struct nvme_dev *dev, u32 csts)
 {
 	/* Read a config register to help see what died. */
@@ -1232,6 +1885,30 @@ static void nvme_warn_reset(struct nvme_dev *dev, u32 csts)
 			 csts, result);
 }
 
+/*
+ * Upon completion of the Abort command, the controller posts a completion queue entry to the Admin
+ * Completion Queue indicating the status for the Abort command and indicating whether the command to
+ * abort was aborted. Dword 0 of the completion queue entry indicates whether the command to abort was
+ * aborted.
+ * If the command to abort was successfully aborted, then a completion queue entry for the aborted command
+ * shall be posted to the appropriate Admin or I/O Completion Queue with a status of Command Abort
+ * Requested before the completion queue entry for the Abort command is posted to the Admin Completion
+ * Queue, and bit 0 of Dword 0 shall be cleared to '0' in the completion queue entry for the Abort command.
+ * If the command to abort was not aborted for any reason, then bit 0 of Dword 0 shall be set to ‘1’ in the
+ * completion queue entry for the Abort command.
+ *
+ * called by:
+ *   - block/blk-mq.c|1039| <<blk_mq_rq_timed_out>> ret = req->q->mq_ops->timeout(req, reserved);
+ *
+ * struct blk_mq_ops nvme_mq_admin_ops.timeout = nvme_timeout()
+ * struct blk_mq_ops nvme_mq_ops.timeout = nvme_timeout()
+ *
+ * blk_rq_timed_out_timer()
+ * -> 调用q->timeout_work = blk_mq_timeout_work()
+ *     -> 为每一个inflight request调用blk_mq_check_expired()
+ *        -> expire了就调用blk_mq_rq_timed_out()
+ *           -> req->q->mq_ops->timeout = nvme_timeout()
+ */
 static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -1239,6 +1916,7 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 	struct nvme_dev *dev = nvmeq->dev;
 	struct request *abort_req;
 	struct nvme_command cmd;
+	/* Controller Status */
 	u32 csts = readl(dev->bar + NVME_REG_CSTS);
 
 	/* If PCI error recovery process is happening, we cannot reset or
@@ -1261,6 +1939,9 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 	/*
 	 * Did we miss an interrupt?
 	 */
+	/*
+	 * nvme_poll_irqdisable()返回的是cq上处理了的数目
+	 */
 	if (nvme_poll_irqdisable(nvmeq, req->tag)) {
 		dev_warn(dev->ctrl.device,
 			 "I/O %d QID %d timeout, completion polled\n",
@@ -1296,6 +1977,11 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
  	 * command was already aborted once before and still hasn't been
  	 * returned to the driver, or if this is the admin queue.
 	 */
+	/*
+	 * 修改aborted的地方:
+	 *   - drivers/nvme/host/pci.c|1264| <<nvme_queue_rq>> iod->aborted = 0;
+	 *   - drivers/nvme/host/pci.c|1841| <<nvme_timeout>> iod->aborted = 1;
+	 */
 	if (!nvmeq->qid || iod->aborted) {
 		dev_warn(dev->ctrl.device,
 			 "I/O %d QID %d timeout, reset controller\n",
@@ -1307,10 +1993,32 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 		return BLK_EH_DONE;
 	}
 
+	/*
+	 * 在以下使用abort_limit:
+	 *   - drivers/nvme/host/core.c|3345| <<nvme_init_identify>> atomic_set(&ctrl->abort_limit, id->acl + 1);
+	 *   - drivers/nvme/host/pci.c|1715| <<abort_endio>> atomic_inc(&nvmeq->dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1862| <<nvme_timeout>> if (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {
+	 *   - drivers/nvme/host/pci.c|1863| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 *   - drivers/nvme/host/pci.c|1887| <<nvme_timeout>> atomic_inc(&dev->ctrl.abort_limit);
+	 *
+	 * To abort a large number of commands (e.g., a larger number of commands than the limit listed in the ACL
+	 * field), the host should follow the procedures described in section 7.3.3 to delete the I/O Submission Queue
+	 * and recreate the I/O Submission Queue.
+	 *
+	 * nvme controller只允许abort有限的(id->acl+1)的req.
+	 * 这里判断如果太多了, 就不能abort了. 重新reset timer,等待下一次.
+	 */
 	if (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {
 		atomic_inc(&dev->ctrl.abort_limit);
 		return BLK_EH_RESET_TIMER;
 	}
+	/*
+	 * 修改aborted的地方:
+	 *   - drivers/nvme/host/pci.c|1264| <<nvme_queue_rq>> iod->aborted = 0;
+	 *   - drivers/nvme/host/pci.c|1841| <<nvme_timeout>> iod->aborted = 1;
+	 *
+	 * 下面的部分是下发一个nvme_admin_abort_cmd来abort某个request->tag
+	 */
 	iod->aborted = 1;
 
 	memset(&cmd, 0, sizeof(cmd));
@@ -1331,6 +2039,9 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 
 	abort_req->timeout = ADMIN_TIMEOUT;
 	abort_req->end_io_data = NULL;
+	/*
+	 * abort的callback是abort_endio()
+	 */
 	blk_execute_rq_nowait(abort_req->q, NULL, abort_req, 0, abort_endio);
 
 	/*
@@ -1341,8 +2052,22 @@ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 	return BLK_EH_RESET_TIMER;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2038| <<nvme_free_queues>> nvme_free_queue(&dev->queues[i]);
+ *
+ * 核心思想是释放sq和cq的dma ring buffer
+ */
 static void nvme_free_queue(struct nvme_queue *nvmeq)
 {
+	/*
+	 * 在以下使用cq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1294| <<adapter_alloc_cq>> c.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1514| <<nvme_free_queue>> (void *)nvmeq->cqes, nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1637| <<nvme_alloc_queue>> &nvmeq->cq_dma_addr, GFP_KERNEL);
+	 *   - drivers/nvme/host/pci.c|1657| <<nvme_alloc_queue>> nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1889| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
+	 */
 	dma_free_coherent(nvmeq->dev->dev, CQ_SIZE(nvmeq),
 				(void *)nvmeq->cqes, nvmeq->cq_dma_addr);
 	if (!nvmeq->sq_cmds)
@@ -1352,17 +2077,35 @@ static void nvme_free_queue(struct nvme_queue *nvmeq)
 		pci_free_p2pmem(to_pci_dev(nvmeq->dev->dev),
 				nvmeq->sq_cmds, SQ_SIZE(nvmeq));
 	} else {
+		/*
+		 *   - drivers/nvme/host/pci.c|1324| <<adapter_alloc_sq>> c.create_sq.prp1 = cpu_to_le64(nvmeq->sq_dma_addr);
+		 *   - drivers/nvme/host/pci.c|1523| <<nvme_free_queue>> nvmeq->sq_cmds, nvmeq->sq_dma_addr);
+		 *   - drivers/nvme/host/pci.c|1609| <<nvme_alloc_sq_cmds>> nvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev,
+		 *   - drivers/nvme/host/pci.c|1611| <<nvme_alloc_sq_cmds>> if (nvmeq->sq_dma_addr) {
+		 *   - drivers/nvme/host/pci.c|1621| <<nvme_alloc_sq_cmds>> &nvmeq->sq_dma_addr, GFP_KERNEL);
+		 *   - drivers/nvme/host/pci.c|1888| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
+		 */
 		dma_free_coherent(nvmeq->dev->dev, SQ_SIZE(nvmeq),
 				nvmeq->sq_cmds, nvmeq->sq_dma_addr);
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3403| <<nvme_dev_add>> nvme_free_queues(dev, dev->online_queues);
+ *   - drivers/nvme/host/pci.c|4226| <<nvme_remove>> nvme_free_queues(dev, 0);
+ *
+ * 核心思想是释放sq和cq的dma ring buffer
+ * queue是从dev->ctrl.queue_count - 1到参数的lowest
+ * 当被nvme_free_queues()调用的时候就是全部queue (dev->ctrl.queue_count-1到0)
+ */
 static void nvme_free_queues(struct nvme_dev *dev, int lowest)
 {
 	int i;
 
 	for (i = dev->ctrl.queue_count - 1; i >= lowest; i--) {
 		dev->ctrl.queue_count--;
+		/* 核心思想是释放sq和cq的dma ring buffer */
 		nvme_free_queue(&dev->queues[i]);
 	}
 }
@@ -1371,8 +2114,24 @@ static void nvme_free_queues(struct nvme_dev *dev, int lowest)
  * nvme_suspend_queue - put queue into suspended state
  * @nvmeq: queue to suspend
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2098| <<nvme_suspend_io_queues>> nvme_suspend_queue(&dev->queues[i]);
+ *   - drivers/nvme/host/pci.c|3652| <<nvme_dev_disable>> nvme_suspend_queue(&dev->queues[0]);
+ */
 static int nvme_suspend_queue(struct nvme_queue *nvmeq)
 {
+	/*
+	 * 在以下使用NVMEQ_ENABLED:
+	 *   - drivers/nvme/host/pci.c|1001| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+	 *   - drivers/nvme/host/pci.c|1560| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1805| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|2468| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *   - drivers/nvme/host/pci.c|2516| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *
+	 * 这样nvme_queue_rq()就会退出了
+	 */
 	if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
 		return 1;
 
@@ -1380,6 +2139,12 @@ static int nvme_suspend_queue(struct nvme_queue *nvmeq)
 	mb();
 
 	nvmeq->dev->online_queues--;
+	/*
+	 * blk_mq_quiesce_queue():
+	 *     wait until all ongoing dispatches have finished
+	 *
+	 * 这里只针对admin queue
+	 */
 	if (!nvmeq->qid && nvmeq->dev->ctrl.admin_q)
 		blk_mq_quiesce_queue(nvmeq->dev->ctrl.admin_q);
 	if (!test_and_clear_bit(NVMEQ_POLLED, &nvmeq->flags))
@@ -1387,6 +2152,11 @@ static int nvme_suspend_queue(struct nvme_queue *nvmeq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2767| <<nvme_setup_io_queues>> nvme_suspend_io_queues(dev);
+ *   - drivers/nvme/host/pci.c|3124| <<nvme_dev_disable>> nvme_suspend_io_queues(dev);
+ */
 static void nvme_suspend_io_queues(struct nvme_dev *dev)
 {
 	int i;
@@ -1395,6 +2165,10 @@ static void nvme_suspend_io_queues(struct nvme_dev *dev)
 		nvme_suspend_queue(&dev->queues[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3122| <<nvme_dev_disable>> nvme_disable_admin_queue(dev, shutdown);
+ */
 static void nvme_disable_admin_queue(struct nvme_dev *dev, bool shutdown)
 {
 	struct nvme_queue *nvmeq = &dev->queues[0];
@@ -1407,6 +2181,10 @@ static void nvme_disable_admin_queue(struct nvme_dev *dev, bool shutdown)
 	nvme_poll_irqdisable(nvmeq, -1);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3190| <<nvme_setup_io_queues>> result = nvme_cmb_qdepth(dev, nr_io_queues,
+ */
 static int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,
 				int entry_size)
 {
@@ -1431,6 +2209,12 @@ static int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,
 	return q_depth;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1641| <<nvme_alloc_queue>> if (nvme_alloc_sq_cmds(dev, nvmeq, qid))
+ *
+ * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+ */
 static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
 				int qid)
 {
@@ -1457,20 +2241,54 @@ static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1879| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/pci.c|1913| <<nvme_create_io_queues>> if (nvme_alloc_queue(dev, i, dev->q_depth)) {
+ *
+ * 分配nvme_queue, 会分配ring buffer
+ * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+ * 分配cq的ring buffer的时候, 只用dma
+ */
 static int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)
 {
+	/*
+	 * struct nvme_dev:
+	 *   - struct nvme_queue *queues;
+	 * 是指针
+	 */
 	struct nvme_queue *nvmeq = &dev->queues[qid];
 
 	if (dev->ctrl.queue_count > qid)
 		return 0;
 
 	nvmeq->sqes = qid ? dev->io_sqes : NVME_ADM_SQES;
+	/*
+	 * SQ_SIZE(q)和CQ_SIZE(q)都是用q_depth
+	 *
+	 * nvme的admin queue的tagset有30个元素, 但是ring buffer有32个
+	 * 除了一个用来表示full, 另外一个用在async event
+	 */
 	nvmeq->q_depth = depth;
+	/*
+	 * 在以下使用cq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1294| <<adapter_alloc_cq>> c.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1514| <<nvme_free_queue>> (void *)nvmeq->cqes, nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1637| <<nvme_alloc_queue>> &nvmeq->cq_dma_addr, GFP_KERNEL);
+	 *   - drivers/nvme/host/pci.c|1657| <<nvme_alloc_queue>> nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1889| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
+	 *
+	 * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+	 * 但是下面分配cq的ring buffer的时候, 只分配dma
+	 */
 	nvmeq->cqes = dma_alloc_coherent(dev->dev, CQ_SIZE(nvmeq),
 					 &nvmeq->cq_dma_addr, GFP_KERNEL);
 	if (!nvmeq->cqes)
 		goto free_nvmeq;
 
+	/*
+	 * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+	 */
 	if (nvme_alloc_sq_cmds(dev, nvmeq, qid))
 		goto free_cqdma;
 
@@ -1492,6 +2310,33 @@ static int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)
 	return -ENOMEM;
 }
 
+/*
+ * qemu下nvme的一个例子:
+ * [0] msi_domain_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [    0.682898] orabug: msi_domain_set_affinity() mask=f
+ * [    0.695652] orabug: msi_domain_set_affinity() mask=f
+ * [    0.706281] orabug: msi_domain_set_affinity() mask=1
+ * [    0.715123] orabug: msi_domain_set_affinity() mask=2
+ * [    0.724427] orabug: msi_domain_set_affinity() mask=4
+ * [    0.733339] orabug: msi_domain_set_affinity() mask=8
+ *
+ * called by:
+ *   - drivers/nvme/host/pci.c|1555| <<nvme_create_queue>> result = queue_request_irq(nvmeq);
+ *   - drivers/nvme/host/pci.c|1703| <<nvme_pci_configure_admin_queue>> result = queue_request_irq(nvmeq);
+ *   - drivers/nvme/host/pci.c|2160| <<nvme_setup_io_queues>> result = queue_request_irq(adminq);
+ */
 static int queue_request_irq(struct nvme_queue *nvmeq)
 {
 	struct pci_dev *pdev = to_pci_dev(nvmeq->dev->dev);
@@ -1506,6 +2351,11 @@ static int queue_request_irq(struct nvme_queue *nvmeq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1811| <<nvme_create_queue>> nvme_init_queue(nvmeq, qid);
+ *   - drivers/nvme/host/pci.c|1989| <<nvme_pci_configure_admin_queue>> nvme_init_queue(nvmeq, 0);
+ */
 static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
 {
 	struct nvme_dev *dev = nvmeq->dev;
@@ -1521,6 +2371,10 @@ static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
 	wmb(); /* ensure the first interrupt sees the initialization */
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2038| <<nvme_create_io_queues>> ret = nvme_create_queue(&dev->queues[i], i, polled);
+ */
 static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 {
 	struct nvme_dev *dev = nvmeq->dev;
@@ -1538,16 +2392,29 @@ static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 	else
 		set_bit(NVMEQ_POLLED, &nvmeq->flags);
 
+	/*
+	 * admin的sq和cq的dma地址是写入bar的
+	 * 而io queue的dma通过admin的cmd下发设置
+	 */
 	result = adapter_alloc_cq(dev, qid, nvmeq, vector);
 	if (result)
 		return result;
 
+	/*
+	 * admin的sq和cq的dma地址是写入bar的
+	 * 而io queue的dma通过admin的cmd下发设置
+	 */
 	result = adapter_alloc_sq(dev, qid, nvmeq);
 	if (result < 0)
 		return result;
 	if (result)
 		goto release_cq;
 
+	/*
+	 * 设置cq_vector的地方:
+	 *   - drivers/nvme/host/pci.c|1796| <<nvme_create_queue>> nvmeq->cq_vector = vector;
+	 *   - drivers/nvme/host/pci.c|1969| <<nvme_pci_configure_admin_queue>> nvmeq->cq_vector = 0;
+	 */
 	nvmeq->cq_vector = vector;
 	nvme_init_queue(nvmeq, qid);
 
@@ -1557,6 +2424,15 @@ static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 			goto release_sq;
 	}
 
+	/*
+	 * 在以下使用NVMEQ_ENABLED:
+	 *   - drivers/nvme/host/pci.c|1001| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+	 *   - drivers/nvme/host/pci.c|1560| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1805| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|2468| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *   - drivers/nvme/host/pci.c|2516| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+	 */
 	set_bit(NVMEQ_ENABLED, &nvmeq->flags);
 	return result;
 
@@ -1568,6 +2444,10 @@ static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 	return result;
 }
 
+/*
+ * 在以下使用nvme_mq_admin_ops:
+ *   - drivers/nvme/host/pci.c|1968| <<nvme_alloc_admin_tags>> dev->admin_tagset.ops = &nvme_mq_admin_ops;
+ */
 static const struct blk_mq_ops nvme_mq_admin_ops = {
 	.queue_rq	= nvme_queue_rq,
 	.complete	= nvme_pci_complete_rq,
@@ -1577,6 +2457,10 @@ static const struct blk_mq_ops nvme_mq_admin_ops = {
 	.timeout	= nvme_timeout,
 };
 
+/*
+ * 在以下使用nvme_mq_ops:
+ *   - drivers/nvme/host/pci.c|2851| <<nvme_dev_add>> dev->tagset.ops = &nvme_mq_ops;
+ */
 static const struct blk_mq_ops nvme_mq_ops = {
 	.queue_rq	= nvme_queue_rq,
 	.complete	= nvme_pci_complete_rq,
@@ -1602,12 +2486,26 @@ static void nvme_dev_remove_admin(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3073| <<nvme_reset_work>> result = nvme_alloc_admin_tags(dev);
+ *
+ * 初始化nvme_dev->admin_tagset.
+ * struct nvme_dev:
+ *   - struct nvme_queue *queues;
+ *   - struct blk_mq_tag_set tagset;
+ *   - struct blk_mq_tag_set admin_tagset;
+ *
+ * 这是初始化admin专用的tagset, 就一个queue (nr_hw_queues = 1)
+ * 会调用blk_mq_alloc_tag_set()和blk_mq_init_queue()
+ */
 static int nvme_alloc_admin_tags(struct nvme_dev *dev)
 {
 	if (!dev->ctrl.admin_q) {
 		dev->admin_tagset.ops = &nvme_mq_admin_ops;
 		dev->admin_tagset.nr_hw_queues = 1;
 
+		/* 30 */
 		dev->admin_tagset.queue_depth = NVME_AQ_MQ_TAG_DEPTH;
 		dev->admin_tagset.timeout = ADMIN_TIMEOUT;
 		dev->admin_tagset.numa_node = dev_to_node(dev->dev);
@@ -1619,11 +2517,17 @@ static int nvme_alloc_admin_tags(struct nvme_dev *dev)
 			return -ENOMEM;
 		dev->ctrl.admin_tagset = &dev->admin_tagset;
 
+		/*
+		 * admin_q是struct request_queue
+		 */
 		dev->ctrl.admin_q = blk_mq_init_queue(&dev->admin_tagset);
 		if (IS_ERR(dev->ctrl.admin_q)) {
 			blk_mq_free_tag_set(&dev->admin_tagset);
 			return -ENOMEM;
 		}
+		/*
+		 * admin_q是struct request_queue
+		 */
 		if (!blk_get_queue(dev->ctrl.admin_q)) {
 			nvme_dev_remove_admin(dev);
 			dev->ctrl.admin_q = NULL;
@@ -1635,11 +2539,22 @@ static int nvme_alloc_admin_tags(struct nvme_dev *dev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1864| <<nvme_pci_configure_admin_queue>> result = nvme_remap_bar(dev, db_bar_size(dev, 0));
+ *   - drivers/nvme/host/pci.c|2406| <<nvme_setup_io_queues>> size = db_bar_size(dev, nr_io_queues);
+ */
 static unsigned long db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)
 {
 	return NVME_REG_DBS + ((nr_io_queues + 1) * 8 * dev->db_stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1864| <<nvme_pci_configure_admin_queue>> result = nvme_remap_bar(dev, db_bar_size(dev, 0));
+ *   - drivers/nvme/host/pci.c|2407| <<nvme_setup_io_queues>> result = nvme_remap_bar(dev, size);
+ *   - drivers/nvme/host/pci.c|3087| <<nvme_dev_map>> if (nvme_remap_bar(dev, NVME_REG_DBS + 4096))
+ */
 static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -1650,6 +2565,7 @@ static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
 		return -ENOMEM;
 	if (dev->bar)
 		iounmap(dev->bar);
+	/* void __iomem *bar; */
 	dev->bar = ioremap(pci_resource_start(pdev, 0), size);
 	if (!dev->bar) {
 		dev->bar_mapped_size = 0;
@@ -1661,6 +2577,14 @@ static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3035| <<nvme_reset_work>> result = nvme_pci_configure_admin_queue(dev);
+ *
+ * 初始化(很复杂)nvme_dev->queues[0]
+ * struct nvme_dev:
+ *   - struct nvme_queue *queues;
+ */
 static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 {
 	int result;
@@ -1671,6 +2595,7 @@ static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 	if (result < 0)
 		return result;
 
+	/* bool subsystem; */
 	dev->subsystem = readl(dev->bar + NVME_REG_VS) >= NVME_VS(1, 1, 0) ?
 				NVME_CAP_NSSRC(dev->ctrl.cap) : 0;
 
@@ -1678,10 +2603,22 @@ static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 	    (readl(dev->bar + NVME_REG_CSTS) & NVME_CSTS_NSSRO))
 		writel(NVME_CSTS_NSSRO, dev->bar + NVME_REG_CSTS);
 
+	/*
+	 * 对于pci来说, nvme_enable_ctrl()和nvme_disable_ctrl()都是在nvme_pci_configure_admin_queue()调用
+	 */
 	result = nvme_disable_ctrl(&dev->ctrl);
 	if (result < 0)
 		return result;
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|1879| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+	 *   - drivers/nvme/host/pci.c|1913| <<nvme_create_io_queues>> if (nvme_alloc_queue(dev, i, dev->q_depth)) {
+	 *
+	 * 分配nvme_queue, 会分配ring buffer
+	 * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+	 * 分配cq的ring buffer的时候, 只用dma
+	 */
 	result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
 	if (result)
 		return result;
@@ -1690,32 +2627,80 @@ static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 	aqa = nvmeq->q_depth - 1;
 	aqa |= aqa << 16;
 
+	/* Admin Queue Attributes */
 	writel(aqa, dev->bar + NVME_REG_AQA);
+	/* Admin SQ Base Address */
 	lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
+	/* Admin CQ Base Address */
 	lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
 
+	/*
+	 * 在以下调用nvme_disable_ctrl(), 不是nvme_enable_ctrl()!!
+	 *   - drivers/nvme/host/pci.c|1572| <<nvme_disable_admin_queue>> nvme_disable_ctrl(&dev->ctrl);
+	 *   - drivers/nvme/host/pci.c|1875| <<nvme_pci_configure_admin_queue>> result = nvme_disable_ctrl(&dev->ctrl);
+	 *   - drivers/nvme/host/rdma.c|1894| <<nvme_rdma_shutdown_ctrl>> nvme_disable_ctrl(&ctrl->ctrl);
+	 *   - drivers/nvme/host/tcp.c|1933| <<nvme_tcp_teardown_ctrl>> nvme_disable_ctrl(ctrl);
+	 */
 	result = nvme_enable_ctrl(&dev->ctrl);
 	if (result)
 		return result;
 
+	/*
+	 * 设置cq_vector的地方:
+	 *   - drivers/nvme/host/pci.c|1796| <<nvme_create_queue>> nvmeq->cq_vector = vector;
+	 *   - drivers/nvme/host/pci.c|1969| <<nvme_pci_configure_admin_queue>> nvmeq->cq_vector = 0;
+	 */
 	nvmeq->cq_vector = 0;
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|1811| <<nvme_create_queue>> nvme_init_queue(nvmeq, qid);
+	 *   - drivers/nvme/host/pci.c|1989| <<nvme_pci_configure_admin_queue>> nvme_init_queue(nvmeq, 0);
+	 */
 	nvme_init_queue(nvmeq, 0);
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|1555| <<nvme_create_queue>> result = queue_request_irq(nvmeq);
+	 *   - drivers/nvme/host/pci.c|1703| <<nvme_pci_configure_admin_queue>> result = queue_request_irq(nvmeq);
+	 *   - drivers/nvme/host/pci.c|2160| <<nvme_setup_io_queues>> result = queue_request_irq(adminq);
+	 */
 	result = queue_request_irq(nvmeq);
 	if (result) {
 		dev->online_queues--;
 		return result;
 	}
 
+	/*
+	 * 在以下使用NVMEQ_ENABLED:
+	 *   - drivers/nvme/host/pci.c|1001| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+	 *   - drivers/nvme/host/pci.c|1560| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1805| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|2468| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *   - drivers/nvme/host/pci.c|2516| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+	 */
 	set_bit(NVMEQ_ENABLED, &nvmeq->flags);
 	return result;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2595| <<nvme_setup_io_queues>> result = nvme_create_io_queues(dev);
+ */
 static int nvme_create_io_queues(struct nvme_dev *dev)
 {
 	unsigned i, max, rw_queues;
 	int ret = 0;
 
 	for (i = dev->ctrl.queue_count; i <= dev->max_qid; i++) {
+		/*
+		 * called by:
+		 *   - drivers/nvme/host/pci.c|1879| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+		 *   - drivers/nvme/host/pci.c|1913| <<nvme_create_io_queues>> if (nvme_alloc_queue(dev, i, dev->q_depth)) {
+		 *
+		 * 分配nvme_queue, 会分配ring buffer
+		 * 分配sq的ring buffer的时候, 要么用p2pmem的cmb, 要么分配dma
+		 * 分配cq的ring buffer的时候, 只用dma
+		 */
 		if (nvme_alloc_queue(dev, i, dev->q_depth)) {
 			ret = -ENOMEM;
 			break;
@@ -1733,6 +2718,12 @@ static int nvme_create_io_queues(struct nvme_dev *dev)
 	for (i = dev->online_queues; i <= max; i++) {
 		bool polled = i > rw_queues;
 
+		/*
+		 * struct nvme_dev:
+		 *   - struct nvme_queue *queues;
+		 *   - struct blk_mq_tag_set tagset;
+		 *   - struct blk_mq_tag_set admin_tagset;
+		 */
 		ret = nvme_create_queue(&dev->queues[i], i, polled);
 		if (ret)
 			break;
@@ -1770,6 +2761,16 @@ static u32 nvme_cmb_size(struct nvme_dev *dev)
 	return (dev->cmbsz >> NVME_CMBSZ_SZ_SHIFT) & NVME_CMBSZ_SZ_MASK;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2592| <<nvme_pci_enable>> nvme_map_cmb(dev);
+ *
+ * nvme_reset_work()
+ *  -> nvme_pci_enable()
+ *      -> nvme_map_cmd()
+ *
+ * 在qemu的参数设置cmb_size_mb=64会得到dev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ) = 262681
+ */
 static void nvme_map_cmb(struct nvme_dev *dev)
 {
 	u64 size, offset;
@@ -1777,9 +2778,26 @@ static void nvme_map_cmb(struct nvme_dev *dev)
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 	int bar;
 
+	/*
+	 * 在以下使用cmb_size:
+	 *   - drivers/nvme/host/pci.c|1548| <<nvme_cmb_qdepth>> if (q_size_aligned * nr_io_queues > dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1549| <<nvme_cmb_qdepth>> u64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);
+	 *   - drivers/nvme/host/pci.c|1946| <<nvme_map_cmb>> if (dev->cmb_size)
+	 *   - drivers/nvme/host/pci.c|1976| <<nvme_map_cmb>> dev->cmb_size = size;
+	 *   - drivers/nvme/host/pci.c|1991| <<nvme_release_cmb>> if (dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1994| <<nvme_release_cmb>> dev->cmb_size = 0;
+	 */
 	if (dev->cmb_size)
 		return;
 
+	/*
+	 * Controller Memory Buffer Size
+	 * This optional register defines the size of the Controller Memory Buffer (refer to section 4.7). If the controller
+	 * does not support the Controller Memory Buffer feature or if the controller supports the Controller Memory
+	 * Buffer (CAP.CMBS) and CMBMSC.CRE is cleared to ‘0’, then this register shall be cleared to 0h.
+	 *
+	 * 在qemu的参数设置cmb_size_mb=64会得到dev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ) = 262681
+	 */
 	dev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ);
 	if (!dev->cmbsz)
 		return;
@@ -1801,15 +2819,50 @@ static void nvme_map_cmb(struct nvme_dev *dev)
 	if (size > bar_size - offset)
 		size = bar_size - offset;
 
+	/*
+	 * pci_p2pdma_add_resource - add memory for use as p2p memory
+	 * @pdev: the device to add the memory to
+	 * @bar: PCI BAR to add
+	 * @size: size of the memory to add, may be zero to use the whole BAR
+	 * @offset: offset into the PCI BAR
+	 *
+	 * The memory will be given ZONE_DEVICE struct pages so that it may
+	 * be used with any DMA request.
+	 */
 	if (pci_p2pdma_add_resource(pdev, bar, size, offset)) {
 		dev_warn(dev->ctrl.device,
 			 "failed to register the CMB\n");
 		return;
 	}
 
+	/*
+	 * 在以下使用cmb_size:
+	 *   - drivers/nvme/host/pci.c|1548| <<nvme_cmb_qdepth>> if (q_size_aligned * nr_io_queues > dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1549| <<nvme_cmb_qdepth>> u64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);
+	 *   - drivers/nvme/host/pci.c|1946| <<nvme_map_cmb>> if (dev->cmb_size)
+	 *   - drivers/nvme/host/pci.c|1976| <<nvme_map_cmb>> dev->cmb_size = size;
+	 *   - drivers/nvme/host/pci.c|1991| <<nvme_release_cmb>> if (dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|1994| <<nvme_release_cmb>> dev->cmb_size = 0;
+	 */
 	dev->cmb_size = size;
+	/*
+	 * 在以下使用cmb_use_sqes:
+	 *   - drivers/nvme/host/pci.c|1570| <<nvme_alloc_sq_cmds>> if (qid && dev->cmb_use_sqes && (dev->cmbsz & NVME_CMBSZ_SQS)) {
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+	 *   - drivers/nvme/host/pci.c|2306| <<nvme_setup_io_queues>> if (dev->cmb_use_sqes) {
+	 *   - drivers/nvme/host/pci.c|2312| <<nvme_setup_io_queues>> dev->cmb_use_sqes = false;
+	 *
+	 * cmb_use_sqes:
+	 * "use controller's memory buffer for I/O SQes"
+	 */
 	dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
 
+	/*
+	 * Published memory can be used by other PCI device drivers for
+	 * peer-2-peer DMA operations. Non-published memory is reserved for
+	 * exclusive use of the device driver that registers the peer-to-peer
+	 * memory.
+	 */
 	if ((dev->cmbsz & (NVME_CMBSZ_WDS | NVME_CMBSZ_RDS)) ==
 			(NVME_CMBSZ_WDS | NVME_CMBSZ_RDS))
 		pci_p2pmem_publish(pdev, true);
@@ -1829,9 +2882,22 @@ static inline void nvme_release_cmb(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2774| <<nvme_setup_host_mem>> ret = nvme_set_host_mem(dev, enable_bits);
+ *
+ * The Host Memory Buffer feature provides a mechanism for the host to allocate a portion of host memory
+ * for the exclusive use of the controller. After a successful completion of a Set Features command enabling
+ * the host memory buffer, the host shall not write to:
+ * a)The Host Memory Descriptor List (refer to Figure 296); and
+ * b)the associated host memory region (i.e., the memory regions described by the Host Memory
+ * Descriptor List),
+ * until the host memory buffer has been disabled.
+ */
 static int nvme_set_host_mem(struct nvme_dev *dev, u32 bits)
 {
 	u64 dma_addr = dev->host_mem_descs_dma;
+	/* 包含struct nvme_features features;作为union */
 	struct nvme_command c;
 	int ret;
 
@@ -1876,6 +2942,10 @@ static void nvme_free_host_mem(struct nvme_dev *dev)
 	dev->nr_host_mem_descs = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2721| <<nvme_alloc_host_mem>> if (!__nvme_alloc_host_mem(dev, preferred, chunk_size)) {
+ */
 static int __nvme_alloc_host_mem(struct nvme_dev *dev, u64 preferred,
 		u32 chunk_size)
 {
@@ -1944,6 +3014,10 @@ static int __nvme_alloc_host_mem(struct nvme_dev *dev, u64 preferred,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2763| <<nvme_setup_host_mem>> if (nvme_alloc_host_mem(dev, min, preferred)) {
+ */
 static int nvme_alloc_host_mem(struct nvme_dev *dev, u64 min, u64 preferred)
 {
 	u32 chunk_size;
@@ -1962,6 +3036,28 @@ static int nvme_alloc_host_mem(struct nvme_dev *dev, u64 min, u64 preferred)
 	return -ENOMEM;
 }
 
+/*
+ * Most modern SSDs include onboard DRAM, typically in a ratio of 1GB RAM per
+ * 1TB of NAND flash memory. This RAM is usually dedicated to tracking where
+ * each logical block address is physically stored on the NAND
+ * flash—information that changes with every write operation due to the wear
+ * leveling that flash memory requires. This information must also be consulted
+ * in order to complete any read operation. The standard DRAM to NAND ratio
+ * provides enough RAM for the SSD controller to use a simple and fast lookup
+ * table instead of more complicated data structures. This greatly reduces the
+ * work the SSD controller needs to do to handle IO operations, and is key to
+ * offering consistent performance.
+ *
+ * SSDs that omit this DRAM can be cheaper and smaller, but because they can
+ * only store their mapping tables in the flash memory instead of much faster
+ * DRAM, there's a substantial performance penalty. In the worst case, read
+ * latency is doubled as potentially every read request from the host first
+ * requires a NAND flash read to look up the logical to physical address
+ * mapping, then a second read to actually fetch the requested data.
+ *
+ * called by:
+ *   - drivers/nvme/host/pci.c|2781| <<nvme_reset_work>> result = nvme_setup_host_mem(dev);
+ */
 static int nvme_setup_host_mem(struct nvme_dev *dev)
 {
 	u64 max = (u64)max_host_mem_size_mb * SZ_1M;
@@ -2011,6 +3107,10 @@ static int nvme_setup_host_mem(struct nvme_dev *dev)
  * nirqs is the number of interrupts available for write and read
  * queues. The core already reserved an interrupt for the admin queue.
  */
+/*
+ * 在以下使用nvme_calc_irq_sets():
+ *   - drivers/nvme/host/pci.c|2592| <<nvme_setup_irqs>> .calc_sets = nvme_calc_irq_sets,
+ */
 static void nvme_calc_irq_sets(struct irq_affinity *affd, unsigned int nrirqs)
 {
 	struct nvme_dev *dev = affd->priv;
@@ -2045,8 +3145,29 @@ static void nvme_calc_irq_sets(struct irq_affinity *affd, unsigned int nrirqs)
 	affd->nr_sets = nr_read_queues ? 2 : 1;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2146| <<nvme_setup_io_queues>> result = nvme_setup_irqs(dev, nr_io_queues);
+ *
+ * 会调用pci_alloc_irq_vectors_affinity()
+ * 刚进入的时候nr_io_queues是4, 到最后irq_queues是5
+ */
 static int nvme_setup_irqs(struct nvme_dev *dev, unsigned int nr_io_queues)
 {
+	/*
+	 * struct irq_affinity - Description for automatic irq affinity assignements
+	 * @pre_vectors:        Don't apply affinity to @pre_vectors at beginning of
+	 *                      the MSI(-X) vector space
+	 * @post_vectors:       Don't apply affinity to @post_vectors at end of
+	 *                      the MSI(-X) vector space
+	 * @nr_sets:            The number of interrupt sets for which affinity
+	 *                      spreading is required
+	 * @set_size:           Array holding the size of each interrupt set
+	 * @calc_sets:          Callback for calculating the number and size
+	 *                      of interrupt sets
+	 * @priv:               Private data for usage by @calc_sets, usually a
+	 *                      pointer to driver/device specific data.
+	 */
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 	struct irq_affinity affd = {
 		.pre_vectors	= 1,
@@ -2079,16 +3200,26 @@ static int nvme_setup_irqs(struct nvme_dev *dev, unsigned int nr_io_queues)
 	if (dev->ctrl.quirks & NVME_QUIRK_SINGLE_VECTOR)
 		irq_queues = 1;
 
+	/* 刚进入的时候nr_io_queues是4, 到最后irq_queues是5 */
 	return pci_alloc_irq_vectors_affinity(pdev, 1, irq_queues,
 			      PCI_IRQ_ALL_TYPES | PCI_IRQ_AFFINITY, &affd);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2747| <<nvme_setup_io_queues>> nvme_disable_io_queues(dev);
+ *   - drivers/nvme/host/pci.c|3102| <<nvme_dev_disable>> nvme_disable_io_queues(dev);
+ */
 static void nvme_disable_io_queues(struct nvme_dev *dev)
 {
 	if (__nvme_disable_io_queues(dev, nvme_admin_delete_sq))
 		__nvme_disable_io_queues(dev, nvme_admin_delete_cq);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3131| <<nvme_reset_work>> result = nvme_setup_io_queues(dev);
+ */
 static int nvme_setup_io_queues(struct nvme_dev *dev)
 {
 	struct nvme_queue *adminq = &dev->queues[0];
@@ -2105,13 +3236,25 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 	if (dev->ctrl.quirks & NVME_QUIRK_SHARED_TAGS)
 		nr_io_queues = 1;
 
+	/*
+	 * 通过admin queue设置
+	 */
 	result = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);
 	if (result < 0)
 		return result;
 
 	if (nr_io_queues == 0)
 		return 0;
-	
+
+	/*
+	 * 在以下使用NVMEQ_ENABLED:
+	 *   - drivers/nvme/host/pci.c|1001| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+	 *   - drivers/nvme/host/pci.c|1560| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1805| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|1977| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|2468| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *   - drivers/nvme/host/pci.c|2516| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+	 */
 	clear_bit(NVMEQ_ENABLED, &adminq->flags);
 
 	if (dev->cmb_use_sqes) {
@@ -2141,14 +3284,25 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 	 * If we enable msix early due to not intx, disable it again before
 	 * setting up the full range we need.
 	 */
+	/*
+	 * 把这个pci设备上的
+	 */
 	pci_free_irq_vectors(pdev);
 
+	/*
+	 * 会调用pci_alloc_irq_vectors_affinity()
+	 * 假设io queue的数量是4, 调用pci_alloc_irq_vectors_affinity()的时候申请5个
+	 * 加上admin的
+	 *
+	 * 返回分配的vector的数量
+	 */
 	result = nvme_setup_irqs(dev, nr_io_queues);
 	if (result <= 0)
 		return -EIO;
 
 	dev->num_vecs = result;
 	result = max(result - 1, 1);
+	/* poll queue是不需要申请vector的 */
 	dev->max_qid = result + dev->io_queues[HCTX_TYPE_POLL];
 
 	/*
@@ -2157,11 +3311,20 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 	 * path to scale better, even if the receive path is limited by the
 	 * number of interrupts.
 	 */
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|1555| <<nvme_create_queue>> result = queue_request_irq(nvmeq);
+	 *   - drivers/nvme/host/pci.c|1703| <<nvme_pci_configure_admin_queue>> result = queue_request_irq(nvmeq);
+	 *   - drivers/nvme/host/pci.c|2160| <<nvme_setup_io_queues>> result = queue_request_irq(adminq);
+	 */
 	result = queue_request_irq(adminq);
 	if (result)
 		return result;
 	set_bit(NVMEQ_ENABLED, &adminq->flags);
 
+	/*
+	 * 只在这里调用
+	 */
 	result = nvme_create_io_queues(dev);
 	if (result || dev->online_queues < 2)
 		return result;
@@ -2179,6 +3342,11 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3105| <<nvme_del_cq_end>> nvme_del_queue_end(req, error);
+ *   - drivers/nvme/host/pci.c|3132| <<nvme_delete_queue>> nvme_del_cq_end : nvme_del_queue_end);
+ */
 static void nvme_del_queue_end(struct request *req, blk_status_t error)
 {
 	struct nvme_queue *nvmeq = req->end_io_data;
@@ -2197,6 +3365,15 @@ static void nvme_del_cq_end(struct request *req, blk_status_t error)
 	nvme_del_queue_end(req, error);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2765| <<__nvme_disable_io_queues>> if (nvme_delete_queue(&dev->queues[nr_queues], opcode))
+ *
+ * nvme_disable_io_queues()
+ *  -> __nvme_disable_io_queues()
+ *      -> nvme_delete_queue()
+ * 所以opcode只有nvme_admin_delete_cq
+ */
 static int nvme_delete_queue(struct nvme_queue *nvmeq, u8 opcode)
 {
 	struct request_queue *q = nvmeq->dev->ctrl.admin_q;
@@ -2207,6 +3384,11 @@ static int nvme_delete_queue(struct nvme_queue *nvmeq, u8 opcode)
 	cmd.delete_queue.opcode = opcode;
 	cmd.delete_queue.qid = cpu_to_le16(nvmeq->qid);
 
+	/*
+	 * blk_mq_init_queue()用来在一个已初始化的tagset上分配一个request_queue.
+	 * 因为admin_queue只有一个request_queue, 所以在初始化的时候调用blk_mq_init_queue().
+	 * 而io_queue可能被多个namespace共享, 所以在分配namespace的时候调用blk_mq_init_queue().
+	 */
 	req = nvme_alloc_request(q, &cmd, BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
 	if (IS_ERR(req))
 		return PTR_ERR(req);
@@ -2214,6 +3396,12 @@ static int nvme_delete_queue(struct nvme_queue *nvmeq, u8 opcode)
 	req->timeout = ADMIN_TIMEOUT;
 	req->end_io_data = nvmeq;
 
+	/*
+	 * 在以下使用delete_done:
+	 *   - drivers/nvme/host/pci.c|3095| <<nvme_del_queue_end>> complete(&nvmeq->delete_done);
+	 *   - drivers/nvme/host/pci.c|3129| <<nvme_delete_queue>> init_completion(&nvmeq->delete_done);
+	 *   - drivers/nvme/host/pci.c|3157| <<__nvme_disable_io_queues>> timeout = wait_for_completion_io_timeout(&nvmeq->delete_done,
+	 */
 	init_completion(&nvmeq->delete_done);
 	blk_execute_rq_nowait(q, NULL, req, false,
 			opcode == nvme_admin_delete_cq ?
@@ -2221,6 +3409,11 @@ static int nvme_delete_queue(struct nvme_queue *nvmeq, u8 opcode)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2585| <<nvme_disable_io_queues>> if (__nvme_disable_io_queues(dev, nvme_admin_delete_sq))
+ *   - drivers/nvme/host/pci.c|2586| <<nvme_disable_io_queues>> __nvme_disable_io_queues(dev, nvme_admin_delete_cq);
+ */
 static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode)
 {
 	int nr_queues = dev->online_queues - 1, sent = 0;
@@ -2237,6 +3430,15 @@ static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode)
 	while (sent) {
 		struct nvme_queue *nvmeq = &dev->queues[nr_queues + sent];
 
+		/*
+		 * This waits for either a completion of a specific task to be signaled or for a
+		 * specified timeout to expire. The timeout is in jiffies. It is not
+		 * interruptible. The caller is accounted as waiting for IO (which traditionally
+		 * means blkio only).
+		 *
+		 * Return: 0 if timed out, and positive (at least 1, or number of jiffies left
+		 * till timeout) if completed.
+		 */
 		timeout = wait_for_completion_io_timeout(&nvmeq->delete_done,
 				timeout);
 		if (timeout == 0)
@@ -2254,6 +3456,10 @@ static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3302| <<nvme_reset_work>> nvme_dev_add(dev);
+ */
 static void nvme_dev_add(struct nvme_dev *dev)
 {
 	int ret;
@@ -2297,19 +3503,35 @@ static void nvme_dev_add(struct nvme_dev *dev)
 	nvme_dbbuf_set(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2720| <<nvme_reset_work>> result = nvme_pci_enable(dev);
+ */
 static int nvme_pci_enable(struct nvme_dev *dev)
 {
 	int result = -ENOMEM;
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 
+	/*
+	 * Initialize device before it's used by a driver. Ask low-level code
+	 * to enable Memory resources. Wake up the device if it was suspended.
+	 * Beware, this function can fail.
+	 */
 	if (pci_enable_device_mem(pdev))
 		return result;
 
+	/*
+	 * Enables bus-mastering on the device and calls pcibios_set_master()
+	 * to do the needed arch specific settings.
+	 */
 	pci_set_master(pdev);
 
 	if (dma_set_mask_and_coherent(dev->dev, DMA_BIT_MASK(64)))
 		goto disable;
 
+	/*
+	 * Controller Status
+	 */
 	if (readl(dev->bar + NVME_REG_CSTS) == -1) {
 		result = -ENODEV;
 		goto disable;
@@ -2326,10 +3548,37 @@ static int nvme_pci_enable(struct nvme_dev *dev)
 
 	dev->ctrl.cap = lo_hi_readq(dev->bar + NVME_REG_CAP);
 
+	/*
+	 * bit 0 - 15, Read only
+	 * Maximum Queue Entries Supported (MQES): This field indicates the
+	 * maximum individual queue size that the controller supports. For NVMe over PCIe
+	 * implementations, this value applies to the I/O Submission Queues and I/O
+	 * Completion Queues that the host creates. For NVMe over Fabrics
+	 * implementations, this value applies to only the I/O Submission Queues that the
+	 * host creates. This is a 0’s based value. The minimum value is 1h, indicating two
+	 * entries.
+	 */
 	dev->q_depth = min_t(int, NVME_CAP_MQES(dev->ctrl.cap) + 1,
 				io_queue_depth);
+	/* 从0开始的queue depth */
 	dev->ctrl.sqsize = dev->q_depth - 1; /* 0's based queue depth */
+	/*
+	 * Doorbell Stride (DSTRD): Each Submission Queue and Completion Queue
+	 * Doorbell register is 32-bits in size. This register indicates the stride between
+	 * doorbell registers. The stride is specified as (2 ^ (2 + DSTRD)) in bytes. A value
+	 * of 0h indicates a stride of 4 bytes, where the doorbell registers are packed without
+	 * reserved space between each register. Refer to section 8.6.
+	 */
 	dev->db_stride = 1 << NVME_CAP_STRIDE(dev->ctrl.cap);
+	/*
+	 * 从bar的0x1000 = 4096开始:
+	 * Submission Queue 0 Tail Doorbell (Admin)
+	 * Completion Queue 0 Head Doorbell (Admin)
+	 * Submission Queue 1 Tail Doorbell
+	 * Completion Queue 1 Head Doorbell
+	 * ... ...
+	 * 根据dev->db_stride中间有间隔的
+	 */
 	dev->dbs = dev->bar + 4096;
 
 	/*
@@ -2337,6 +3586,12 @@ static int nvme_pci_enable(struct nvme_dev *dev)
 	 * Interestingly they also seem to ignore the CC:IOSQES register
 	 * so we don't bother updating it here.
 	 */
+	/*
+	 * 在以下使用io_sqes:
+	 *   - drivers/nvme/host/pci.c|1592| <<nvme_alloc_queue>> nvmeq->sqes = qid ? dev->io_sqes : NVME_ADM_SQES;
+	 *   - drivers/nvme/host/pci.c|2559| <<nvme_pci_enable>> dev->io_sqes = 7;
+	 *   - drivers/nvme/host/pci.c|2561| <<nvme_pci_enable>> dev->io_sqes = NVME_NVM_IOSQES;
+	 */
 	if (dev->ctrl.quirks & NVME_QUIRK_128_BYTES_SQES)
 		dev->io_sqes = 7;
 	else
@@ -2363,6 +3618,13 @@ static int nvme_pci_enable(struct nvme_dev *dev)
 	 * Controllers with the shared tags quirk need the IO queue to be
 	 * big enough so that we get 32 tags for the admin queue
 	 */
+	/*
+	 * 这个情况设置:
+	 * 3390         { PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2005),
+	 * 3391                 .driver_data = NVME_QUIRK_SINGLE_VECTOR |
+	 * 3392                                 NVME_QUIRK_128_BYTES_SQES |
+	 * 3393                                 NVME_QUIRK_SHARED_TAGS },
+	 */
 	if ((dev->ctrl.quirks & NVME_QUIRK_SHARED_TAGS) &&
 	    (dev->q_depth < (NVME_AQ_DEPTH + 2))) {
 		dev->q_depth = NVME_AQ_DEPTH + 2;
@@ -2371,13 +3633,26 @@ static int nvme_pci_enable(struct nvme_dev *dev)
 	}
 
 
+	/*
+	 * nvme_reset_work()
+	 *  -> nvme_pci_enable()
+	 *      -> nvme_map_cmd()
+	 */
 	nvme_map_cmb(dev);
 
 	pci_enable_pcie_error_reporting(pdev);
+	/* save the PCI configuration space of a device before suspending */
 	pci_save_state(pdev);
 	return 0;
 
  disable:
+	/*
+	 * Signal to the system that the PCI device is not in use by the system
+	 * anymore.  This only involves disabling PCI bus-mastering, if active.
+	 *
+	 * Note we don't actually disable the device until all callers of
+	 * pci_enable_device() have called pci_disable_device().
+	 */
 	pci_disable_device(pdev);
 	return result;
 }
@@ -2401,6 +3676,18 @@ static void nvme_pci_disable(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1372| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|1401| <<nvme_timeout>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|1419| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2622| <<nvme_disable_prepare_reset>> nvme_dev_disable(dev, shutdown);
+ *   - drivers/nvme/host/pci.c|2679| <<nvme_remove_dead_ctrl>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2716| <<nvme_reset_work>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|3064| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|3071| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|3209| <<nvme_error_detected>> nvme_dev_disable(dev, false);
+ */
 static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 {
 	bool dead = true, freeze = false;
@@ -2485,6 +3772,11 @@ static void nvme_release_prp_pools(struct nvme_dev *dev)
 	dma_pool_destroy(dev->prp_small_pool);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3174| <<nvme_pci_free_ctrl>> nvme_free_tagset(dev);
+ *   - drivers/nvme/host/pci.c|3352| <<nvme_reset_work>> nvme_free_tagset(dev);
+ */
 static void nvme_free_tagset(struct nvme_dev *dev)
 {
 	if (dev->tagset.tags)
@@ -2492,6 +3784,9 @@ static void nvme_free_tagset(struct nvme_dev *dev)
 	dev->ctrl.tagset = NULL;
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.free_ctrl = nvme_pci_free_ctrl()
+ */
 static void nvme_pci_free_ctrl(struct nvme_ctrl *ctrl)
 {
 	struct nvme_dev *dev = to_nvme_dev(ctrl);
@@ -2507,6 +3802,10 @@ static void nvme_pci_free_ctrl(struct nvme_ctrl *ctrl)
 	kfree(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3364| <<nvme_reset_work>> nvme_remove_dead_ctrl(dev);
+ */
 static void nvme_remove_dead_ctrl(struct nvme_dev *dev)
 {
 	/*
@@ -2521,10 +3820,28 @@ static void nvme_remove_dead_ctrl(struct nvme_dev *dev)
 		nvme_put_ctrl(&dev->ctrl);
 }
 
+/*
+ * pci在以下调用:
+ *   - drivers/nvme/host/core.c|133| <<nvme_try_sched_reset>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+ *   - drivers/nvme/host/core.c|143| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+ *   - drivers/nvme/host/core.c|155| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/core.c|169| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/pci.c|2798| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|2902| <<nvme_reset_done>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|2929| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|3094| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+ *
+ * 在以下使用nvme_reset_work():
+ *   - drivers/nvme/host/pci.c|2830| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+ */
 static void nvme_reset_work(struct work_struct *work)
 {
 	struct nvme_dev *dev =
 		container_of(work, struct nvme_dev, ctrl.reset_work);
+	/*
+	 * ctrl.ctrl_config
+	 * 最终会被写入bar0或bar1的NVME_REG_CC = 0x0014, Controller Configuration
+	 */
 	bool was_suspend = !!(dev->ctrl.ctrl_config & NVME_CC_SHN_NORMAL);
 	int result;
 
@@ -2541,15 +3858,45 @@ static void nvme_reset_work(struct work_struct *work)
 		nvme_dev_disable(dev, false);
 	nvme_sync_queues(&dev->ctrl);
 
+	/*
+	 * 在以下使用shutdown_lock:
+	 *   - drivers/nvme/host/pci.c|2570| <<nvme_dev_disable>> mutex_lock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2615| <<nvme_dev_disable>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2719| <<nvme_reset_work>> mutex_lock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2745| <<nvme_reset_work>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2821| <<nvme_reset_work>> mutex_unlock(&dev->shutdown_lock);
+	 *   - drivers/nvme/host/pci.c|2965| <<nvme_probe>> mutex_init(&dev->shutdown_lock);
+	 */
 	mutex_lock(&dev->shutdown_lock);
 	result = nvme_pci_enable(dev);
 	if (result)
 		goto out_unlock;
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|3035| <<nvme_reset_work>> result = nvme_pci_configure_admin_queue(dev);
+	 *
+	 * 初始化(很复杂)nvme_dev->queues[0]
+	 * struct nvme_dev:
+	 *   - struct nvme_queue *queues;
+	 */
 	result = nvme_pci_configure_admin_queue(dev);
 	if (result)
 		goto out_unlock;
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|3073| <<nvme_reset_work>> result = nvme_alloc_admin_tags(dev);
+	 *
+	 * 初始化nvme_dev->admin_tagset.
+	 * struct nvme_dev:
+	 *   - struct nvme_queue *queues;
+	 *   - struct blk_mq_tag_set tagset;
+	 *   - struct blk_mq_tag_set admin_tagset;
+	 *
+	 * 这是初始化admin专用的tagset, 就一个queue (nr_hw_queues = 1)
+	 * 会调用blk_mq_alloc_tag_set()和blk_mq_init_queue()
+	 */
 	result = nvme_alloc_admin_tags(dev);
 	if (result)
 		goto out_unlock;
@@ -2580,6 +3927,15 @@ static void nvme_reset_work(struct work_struct *work)
 		goto out;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/core.c|1408| <<nvme_passthru_end>> nvme_init_identify(ctrl);
+	 *   - drivers/nvme/host/fc.c|2681| <<nvme_fc_create_association>> ret = nvme_init_identify(&ctrl->ctrl);
+	 *   - drivers/nvme/host/pci.c|2628| <<nvme_reset_work>> result = nvme_init_identify(&dev->ctrl);
+	 *   - drivers/nvme/host/rdma.c|835| <<nvme_rdma_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+	 *   - drivers/nvme/host/tcp.c|1752| <<nvme_tcp_configure_admin_queue>> error = nvme_init_identify(ctrl);
+	 *   - drivers/nvme/target/loop.c|389| <<nvme_loop_configure_admin_queue>> error = nvme_init_identify(&ctrl->ctrl);
+	 */
 	result = nvme_init_identify(&dev->ctrl);
 	if (result)
 		goto out;
@@ -2602,12 +3958,23 @@ static void nvme_reset_work(struct work_struct *work)
 				 "unable to allocate dma for dbbuf\n");
 	}
 
+	/*
+	 * Host Memory Buffer Preferred Size (HMPRE): This field indicates the preferred size
+	 * that the host is requested to allocate for the Host Memory Buffer feature in 4 KiB units.
+	 * This value shall be greater than or equal to the Host Memory Buffer Minimum Size. If
+	 * this field is non-zero, then the Host Memory Buffer feature is supported. If this field is
+	 * cleared to 0h, then the Host Memory Buffer feature is not supported.
+	 */
 	if (dev->ctrl.hmpre) {
 		result = nvme_setup_host_mem(dev);
 		if (result < 0)
 			goto out;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/nvme/host/pci.c|3131| <<nvme_reset_work>> result = nvme_setup_io_queues(dev);
+	 */
 	result = nvme_setup_io_queues(dev);
 	if (result)
 		goto out;
@@ -2624,6 +3991,9 @@ static void nvme_reset_work(struct work_struct *work)
 	} else {
 		nvme_start_queues(&dev->ctrl);
 		nvme_wait_freeze(&dev->ctrl);
+		/*
+		 * tagset在这里面初始化
+		 */
 		nvme_dev_add(dev);
 		nvme_unfreeze(&dev->ctrl);
 	}
@@ -2651,6 +4021,13 @@ static void nvme_reset_work(struct work_struct *work)
 	nvme_remove_dead_ctrl(dev);
 }
 
+/*
+ * 在以下使用nvme_remove_dead_ctrl_work():
+ *   - drivers/nvme/host/pci.c|3511| <<nvme_probe>> INIT_WORK(&dev->remove_work, nvme_remove_dead_ctrl_work);
+ *
+ * 在以下调用remove_work:
+ *   - drivers/nvme/host/pci.c|3181| <<nvme_remove_dead_ctrl>> if (!queue_work(nvme_wq, &dev->remove_work))
+ */
 static void nvme_remove_dead_ctrl_work(struct work_struct *work)
 {
 	struct nvme_dev *dev = container_of(work, struct nvme_dev, remove_work);
@@ -2661,24 +4038,36 @@ static void nvme_remove_dead_ctrl_work(struct work_struct *work)
 	nvme_put_ctrl(&dev->ctrl);
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.reg_read32 = nvme_pci_reg_read32()
+ */
 static int nvme_pci_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)
 {
 	*val = readl(to_nvme_dev(ctrl)->bar + off);
 	return 0;
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.reg_write32 = nvme_pci_reg_write32()
+ */
 static int nvme_pci_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val)
 {
 	writel(val, to_nvme_dev(ctrl)->bar + off);
 	return 0;
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.reg_read64 = nvme_pci_reg_read64()
+ */
 static int nvme_pci_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val)
 {
 	*val = lo_hi_readq(to_nvme_dev(ctrl)->bar + off);
 	return 0;
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.get_address()
+ */
 static int nvme_pci_get_address(struct nvme_ctrl *ctrl, char *buf, int size)
 {
 	struct pci_dev *pdev = to_pci_dev(to_nvme_dev(ctrl)->dev);
@@ -2686,6 +4075,10 @@ static int nvme_pci_get_address(struct nvme_ctrl *ctrl, char *buf, int size)
 	return snprintf(buf, size, "%s", dev_name(&pdev->dev));
 }
 
+/*
+ * 在以下使用nvme_pci_ctrl_ops:
+ *   - drivers/nvme/host/pci.c|3576| <<nvme_probe>> result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
+ */
 static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
 	.name			= "pcie",
 	.module			= THIS_MODULE,
@@ -2699,6 +4092,10 @@ static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
 	.get_address		= nvme_pci_get_address,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3502| <<nvme_probe>> result = nvme_dev_map(dev);
+ */
 static int nvme_dev_map(struct nvme_dev *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -2746,6 +4143,10 @@ static unsigned long check_vendor_combination_bug(struct pci_dev *pdev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3490| <<nvme_probe>> async_schedule(nvme_async_probe, dev);
+ */
 static void nvme_async_probe(void *data, async_cookie_t cookie)
 {
 	struct nvme_dev *dev = data;
@@ -2755,6 +4156,9 @@ static void nvme_async_probe(void *data, async_cookie_t cookie)
 	nvme_put_ctrl(&dev->ctrl);
 }
 
+/*
+ * struct pci_driver nvme_driver.probe = nvme_probe()
+ */
 static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 {
 	int node, result = -ENOMEM;
@@ -2766,6 +4170,7 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	if (node == NUMA_NO_NODE)
 		set_dev_node(&pdev->dev, first_memory_node);
 
+	/* 分配struct nvme_dev */
 	dev = kzalloc_node(sizeof(*dev), GFP_KERNEL, node);
 	if (!dev)
 		return -ENOMEM;
@@ -2809,6 +4214,10 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 		goto release_pools;
 	}
 
+	/*
+	 * struct nvme_dev:
+	 *   - struct nvme_ctrl ctrl;
+	 */
 	result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
 			quirks);
 	if (result)
@@ -2836,6 +4245,9 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	return result;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.reset_prepare = nvme_reset_prepare()
+ */
 static void nvme_reset_prepare(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2849,6 +4261,9 @@ static void nvme_reset_prepare(struct pci_dev *pdev)
 	nvme_sync_queues(&dev->ctrl);
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.reset_done = nvme_reset_done()
+ */
 static void nvme_reset_done(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2857,6 +4272,9 @@ static void nvme_reset_done(struct pci_dev *pdev)
 		flush_work(&dev->ctrl.reset_work);
 }
 
+/*
+ * struct pci_driver nvme_driver.shutdown = nvme_shutdown()
+ */
 static void nvme_shutdown(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2868,6 +4286,20 @@ static void nvme_shutdown(struct pci_dev *pdev)
  * state. This function must not have any dependencies on the device state in
  * order to proceed.
  */
+/*
+ * struct pci_driver nvme_driver.remove = nvme_remove()
+ *
+ * # echo 0000:00:04.0 >  /sys/bus/pci/devices/0000\:00\:04.0/driver/unbind
+ * [0] nvme_remove
+ * [0] pci_device_remove
+ * [0] device_release_driver_internal
+ * [0] unbind_store
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void nvme_remove(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2906,6 +4338,9 @@ static int nvme_set_power_state(struct nvme_ctrl *ctrl, u32 ps)
 	return nvme_set_features(ctrl, NVME_FEAT_POWER_MGMT, ps, NULL, 0, NULL);
 }
 
+/*
+ * struct dev_pm_ops nvme_dev_pm_ops.resume = nvme_resume()
+ */
 static int nvme_resume(struct device *dev)
 {
 	struct nvme_dev *ndev = pci_get_drvdata(to_pci_dev(dev));
@@ -2917,6 +4352,9 @@ static int nvme_resume(struct device *dev)
 	return 0;
 }
 
+/*
+ * struct dev_pm_ops nvme_dev_pm_ops.suspend = nvme_suspend()
+ */
 static int nvme_suspend(struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
@@ -2982,12 +4420,20 @@ static int nvme_suspend(struct device *dev)
 	return ret;
 }
 
+/*
+ * struct dev_pm_ops nvme_dev_pm_ops.freeze = nvme_simple_suspend()
+ * struct dev_pm_ops nvme_dev_pm_ops.poweroff = nvme_simple_suspend()
+ */
 static int nvme_simple_suspend(struct device *dev)
 {
 	struct nvme_dev *ndev = pci_get_drvdata(to_pci_dev(dev));
 	return nvme_disable_prepare_reset(ndev, true);
 }
 
+/*
+ * struct dev_pm_ops nvme_dev_pm_ops.thaw = nvme_simple_resume()
+ * struct dev_pm_ops nvme_dev_pm_ops.restore = nvme_simple_resume()
+ */
 static int nvme_simple_resume(struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
@@ -3006,6 +4452,9 @@ static const struct dev_pm_ops nvme_dev_pm_ops = {
 };
 #endif /* CONFIG_PM_SLEEP */
 
+/*
+ * struct pci_error_handlers nvme_err_handler.error_detected = nvme_error_detected()
+ */
 static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
 						pci_channel_state_t state)
 {
@@ -3032,6 +4481,9 @@ static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
 	return PCI_ERS_RESULT_NEED_RESET;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.slot_reset = nvme_slot_reset()
+ */
 static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -3042,6 +4494,9 @@ static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
 	return PCI_ERS_RESULT_RECOVERED;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.resume = nvme_error_resume()
+ */
 static void nvme_error_resume(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -3049,6 +4504,9 @@ static void nvme_error_resume(struct pci_dev *pdev)
 	flush_work(&dev->ctrl.reset_work);
 }
 
+/*
+ * struct pci_driver nvme_driver.err_handler = nvme_err_handler
+ */
 static const struct pci_error_handlers nvme_err_handler = {
 	.error_detected	= nvme_error_detected,
 	.slot_reset	= nvme_slot_reset,
@@ -3057,6 +4515,9 @@ static const struct pci_error_handlers nvme_err_handler = {
 	.reset_done	= nvme_reset_done,
 };
 
+/*
+ * struct pci_driver nvme_driver.id_table = nvme_id_table[]
+ */
 static const struct pci_device_id nvme_id_table[] = {
 	{ PCI_VDEVICE(INTEL, 0x0953),
 		.driver_data = NVME_QUIRK_STRIPE_SIZE |
diff --git a/drivers/nvme/target/loop.c b/drivers/nvme/target/loop.c
index 4df4ebde208a..78c72579e2a1 100644
--- a/drivers/nvme/target/loop.c
+++ b/drivers/nvme/target/loop.c
@@ -564,6 +564,9 @@ static struct nvmet_port *nvme_loop_find_port(struct nvme_ctrl *ctrl)
 	return found;
 }
 
+/*
+ * struct nvmf_transport_ops nvme_loop_transport.create_ctrl = nvme_loop_create_ctrl()
+ */
 static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 		struct nvmf_ctrl_options *opts)
 {
diff --git a/drivers/pci/msi.c b/drivers/pci/msi.c
index c7709e49f0e4..750f7bccd386 100644
--- a/drivers/pci/msi.c
+++ b/drivers/pci/msi.c
@@ -691,6 +691,39 @@ static void __iomem *msix_map_region(struct pci_dev *dev, unsigned nr_entries)
 	return ioremap_nocache(phys_addr, nr_entries * PCI_MSIX_ENTRY_SIZE);
 }
 
+/*
+ * [0] __pci_enable_msix_range
+ * [0] pci_alloc_irq_vectors_affinity
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] __pci_enable_msix_range
+ * [0] pci_alloc_irq_vectors_affinity
+ * [0] vp_find_vqs_msix
+ * [0] vp_find_vqs
+ * [0] vp_modern_find_vqs
+ * [0] init_vq
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/pci/msi.c|794| <<msix_capability_init>> ret = msix_setup_entries(dev, base, entries, nvec, affd);
+ */
 static int msix_setup_entries(struct pci_dev *dev, void __iomem *base,
 			      struct msix_entry *entries, int nvec,
 			      struct irq_affinity *affd)
@@ -698,11 +731,41 @@ static int msix_setup_entries(struct pci_dev *dev, void __iomem *base,
 	struct irq_affinity_desc *curmsk, *masks = NULL;
 	struct msi_desc *entry;
 	int ret, i;
+	/* return the number of device's MSI-X table entries */
 	int vec_count = pci_msix_vec_count(dev);
 
+	/*
+	 * irq_create_affinity_masks - Create affinity masks for multiqueue spreading
+	 * @nvecs:      The total number of vectors
+	 * @affd:       Description of the affinity requirements
+	 *
+	 * Returns the irq_affinity_desc pointer or NULL if allocation failed.
+	 */
 	if (affd)
 		masks = irq_create_affinity_masks(nvec, affd);
 
+	/*
+	 * 在nvme和virtio-blk测试的时候...
+	 *
+	 * nvme: 8个cpu, 9个queue, 9个vector
+	 * 0 (admin) : masks[0].mask=0-7, masks[0].is_managed=0
+	 * 1 (io)    : masks[1].mask=0, masks[0].is_managed=1
+	 * 2 (io)    : masks[2].mask=1, masks[1].is_managed=1
+	 * 3 (io)    : masks[3].mask=2, masks[2].is_managed=1
+	 * 4 (io)    : masks[4].mask=3, masks[3].is_managed=1
+	 * 5 (io)    : masks[5].mask=4, masks[4].is_managed=1
+	 * 6 (io)    : masks[6].mask=5, masks[5].is_managed=1
+	 * 7 (io)    : masks[7].mask=6, masks[6].is_managed=1
+	 * 8 (io)    : masks[8].mask=7, masks[7].is_managed=1
+	 *
+	 * virtblk: 8个cpu, 5个queue, 5个vector
+	 * 0 (admin) : masks[0].mask=0-7, masks[0].is_managed=0
+	 * 1 (io)    : masks[1].mask=0-1, masks[1].is_managed=1
+	 * 2 (io)    : masks[2].mask=2-3, masks[2].is_managed=1
+	 * 3 (io)    : masks[3].mask=4-5, masks[3].is_managed=1
+	 * 4 (io)    : masks[4].mask=6-7, masks[4].is_managed=1
+	 */
+
 	for (i = 0, curmsk = masks; i < nvec; i++) {
 		entry = alloc_msi_entry(&dev->dev, 1, curmsk);
 		if (!entry) {
@@ -771,6 +834,10 @@ static void msix_program_entries(struct pci_dev *dev,
  * single MSI-X IRQ. A return of zero indicates the successful setup of
  * requested MSI-X entries with allocated IRQs or non-zero for otherwise.
  **/
+/*
+ * called by:
+ *   - drivers/pci/msi.c|1002| <<__pci_enable_msix>> return msix_capability_init(dev, entries, nvec, affd);
+ */
 static int msix_capability_init(struct pci_dev *dev, struct msix_entry *entries,
 				int nvec, struct irq_affinity *affd)
 {
@@ -1186,10 +1253,36 @@ EXPORT_SYMBOL(pci_enable_msix_range);
  * To get the Linux IRQ number used for a vector that can be passed to
  * request_irq() use the pci_irq_vector() helper.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2519| <<nvme_setup_irqs>> return pci_alloc_irq_vectors_affinity(pdev, 1, irq_queues,
+ *   - drivers/scsi/be2iscsi/be_main.c|3572| <<be2iscsi_enable_msix>> if (pci_alloc_irq_vectors_affinity(phba->pcidev, 2, nvec,
+ *   - drivers/scsi/csiostor/csio_isr.c|520| <<csio_enable_msix>> cnt = pci_alloc_irq_vectors_affinity(hw->pdev, min, cnt,
+ *   - drivers/scsi/hisi_sas/hisi_sas_v3_hw.c|2399| <<interrupt_init_v3_hw>> vectors = pci_alloc_irq_vectors_affinity(hisi_hba->pci_dev,
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|5795| <<__megasas_alloc_irq_vectors>> i = pci_alloc_irq_vectors_affinity(instance->pdev,
+ *   - drivers/scsi/mpt3sas/mpt3sas_base.c|3049| <<_base_alloc_irq_vectors>> i = pci_alloc_irq_vectors_affinity(ioc->pdev,
+ *   - drivers/scsi/qla2xxx/qla_isr.c|3478| <<qla24xx_enable_msix>> ret = pci_alloc_irq_vectors_affinity(ha->pdev, min_vecs,
+ *   - drivers/virtio/virtio_pci_common.c|133| <<vp_request_msix_vectors>> err = pci_alloc_irq_vectors_affinity(vp_dev->pci_dev, nvectors,
+ *   - include/linux/pci.h|1777| <<pci_alloc_irq_vectors>> return pci_alloc_irq_vectors_affinity(dev, min_vecs, max_vecs, flags,
+ */
 int pci_alloc_irq_vectors_affinity(struct pci_dev *dev, unsigned int min_vecs,
 				   unsigned int max_vecs, unsigned int flags,
 				   struct irq_affinity *affd)
 {
+	/*
+	 * struct irq_affinity - Description for automatic irq affinity assignements
+	 * @pre_vectors:        Don't apply affinity to @pre_vectors at beginning of
+	 *                      the MSI(-X) vector space
+	 * @post_vectors:       Don't apply affinity to @post_vectors at end of
+	 *                      the MSI(-X) vector space
+	 * @nr_sets:            The number of interrupt sets for which affinity
+	 *                      spreading is required
+	 * @set_size:           Array holding the size of each interrupt set
+	 * @calc_sets:          Callback for calculating the number and size
+	 *                      of interrupt sets
+	 * @priv:               Private data for usage by @calc_sets, usually a
+	 *                      pointer to driver/device specific data.
+	 */
 	struct irq_affinity msi_default_affd = {0};
 	int msix_vecs = -ENOSPC;
 	int msi_vecs = -ENOSPC;
@@ -1289,12 +1382,24 @@ EXPORT_SYMBOL(pci_irq_vector);
  * @dev:	PCI device to operate on
  * @nr:		device-relative interrupt vector index (0-based).
  */
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|65| <<blk_mq_pci_map_queues>> mask = pci_irq_get_affinity(pdev, queue + offset);
+ *   - drivers/scsi/hisi_sas/hisi_sas_v3_hw.c|2364| <<setup_reply_map_v3_hw>> mask = pci_irq_get_affinity(hisi_hba->pci_dev, queue +
+ *   - drivers/scsi/hpsa.c|7444| <<hpsa_setup_reply_map>> mask = pci_irq_get_affinity(h->pdev, queue);
+ *   - drivers/scsi/lpfc/lpfc_init.c|11124| <<lpfc_cpuhp_get_eq>> maskp = pci_irq_get_affinity(phba->pcidev, idx);
+ *   - drivers/scsi/lpfc/lpfc_init.c|11465| <<lpfc_sli4_enable_msix>> maskp = pci_irq_get_affinity(phba->pcidev, index);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|5706| <<megasas_setup_reply_map>> mask = pci_irq_get_affinity(instance->pdev, queue);
+ *   - drivers/scsi/mpt3sas/mpt3sas_base.c|2927| <<_base_assign_reply_queues>> mask = pci_irq_get_affinity(ioc->pdev,
+ *   - drivers/virtio/virtio_pci_common.c|455| <<vp_get_vq_affinity>> return pci_irq_get_affinity(vp_dev->pci_dev,
+ */
 const struct cpumask *pci_irq_get_affinity(struct pci_dev *dev, int nr)
 {
 	if (dev->msix_enabled) {
 		struct msi_desc *entry;
 		int i = 0;
 
+		/* entry的类型是struct msi_desc */
 		for_each_pci_msi_entry(entry, dev) {
 			if (i == nr)
 				return &entry->affinity->mask;
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 867c7ebd3f10..3d74c50dff79 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -91,6 +91,22 @@ struct vring_virtqueue {
 	bool packed_ring;
 
 	/* Is DMA API used? */
+	/*
+	 * 在以下设置use_dma_api:
+	 *   - drivers/virtio/virtio_ring.c|1610| <<vring_create_virtqueue_packed>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 *   - drivers/virtio/virtio_ring.c|2084| <<__vring_new_virtqueue>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 * 在以下使用use_dma_api:
+	 *   - drivers/virtio/virtio_ring.c|329| <<vring_map_one_sg>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|346| <<vring_map_single>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|356| <<vring_mapping_error>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|372| <<vring_unmap_one_split>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|919| <<vring_unmap_state_packed>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|942| <<vring_unmap_desc_packed>> if (!vq->use_dma_api)
+	 *   - drivers/virtio/virtio_ring.c|1034| <<virtqueue_add_indirect_packed>> if (vq->use_dma_api) {
+	 *   - drivers/virtio/virtio_ring.c|1165| <<virtqueue_add_packed>> if (unlikely(vq->use_dma_api)) {
+	 *   - drivers/virtio/virtio_ring.c|1297| <<detach_buf_packed>> if (unlikely(vq->use_dma_api)) {
+	 *   - drivers/virtio/virtio_ring.c|1314| <<detach_buf_packed>> if (vq->use_dma_api) {
+	 */
 	bool use_dma_api;
 
 	/* Can we use weak barriers? */
@@ -238,6 +254,14 @@ static inline bool virtqueue_use_indirect(struct virtqueue *_vq,
  * unconditionally on data path.
  */
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|281| <<virtio_max_dma_size>> if (vring_use_dma_api(vdev))
+ *   - drivers/virtio/virtio_ring.c|291| <<vring_alloc_queue>> if (vring_use_dma_api(vdev)) {
+ *   - drivers/virtio/virtio_ring.c|324| <<vring_free_queue>> if (vring_use_dma_api(vdev))
+ *   - drivers/virtio/virtio_ring.c|1626| <<bool>> vq->use_dma_api = vring_use_dma_api(vdev);
+ *   - drivers/virtio/virtio_ring.c|2100| <<bool>> vq->use_dma_api = vring_use_dma_api(vdev);
+ */
 static bool vring_use_dma_api(struct virtio_device *vdev)
 {
 	if (!virtio_has_iommu_quirk(vdev))
diff --git a/fs/block_dev.c b/fs/block_dev.c
index 69bf2fb6f7cd..10e87c18fdbe 100644
--- a/fs/block_dev.c
+++ b/fs/block_dev.c
@@ -1508,6 +1508,15 @@ EXPORT_SYMBOL(bd_set_size);
 
 static void __blkdev_put(struct block_device *bdev, fmode_t mode, int for_part);
 
+/*
+ * called by:
+ *   - block/ioctl.c|168| <<blkdev_reread_part>> ret = bdev_disk_changed(bdev, false);
+ *   - drivers/block/loop.c|644| <<loop_reread_partitions>> rc = bdev_disk_changed(bdev, false);
+ *   - drivers/block/loop.c|1171| <<__loop_clr_fd>> err = bdev_disk_changed(bdev, false);
+ *   - drivers/s390/block/dasd_genhd.c|120| <<dasd_scan_partitions>> rc = bdev_disk_changed(bdev, false);
+ *   - fs/block_dev.c|1634| <<__blkdev_get>> bdev_disk_changed(bdev, ret == -ENOMEDIUM);
+ *   - fs/block_dev.c|1669| <<__blkdev_get>> bdev_disk_changed(bdev, ret == -ENOMEDIUM);
+ */
 int bdev_disk_changed(struct block_device *bdev, bool invalidate)
 {
 	struct gendisk *disk = bdev->bd_disk;
diff --git a/fs/direct-io.c b/fs/direct-io.c
index 00b4d15bb811..356584570bc3 100644
--- a/fs/direct-io.c
+++ b/fs/direct-io.c
@@ -940,6 +940,10 @@ static inline void dio_zero_block(struct dio *dio, struct dio_submit *sdio,
  * it should set b_size to PAGE_SIZE or more inside get_block().  This gives
  * fine alignment but still allows this function to work in PAGE_SIZE units.
  */
+/*
+ * called by:
+ *   - fs/direct-io.c|1311| <<do_blockdev_direct_IO>> retval = do_direct_IO(dio, &sdio, &map_bh);
+ */
 static int do_direct_IO(struct dio *dio, struct dio_submit *sdio,
 			struct buffer_head *map_bh)
 {
@@ -1141,6 +1145,10 @@ static inline int drop_refcount(struct dio *dio)
  * individual fields and will generate much worse code. This is important
  * for the whole file.
  */
+/*
+ * called by:
+ *   - fs/direct-io.c|1393| <<__blockdev_direct_IO>> return do_blockdev_direct_IO(iocb, inode, bdev, iter, get_block,
+ */
 static inline ssize_t
 do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 		      struct block_device *bdev, struct iov_iter *iter,
@@ -1372,6 +1380,13 @@ do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 	return retval;
 }
 
+/*
+ * called by:
+ *   - fs/btrfs/inode.c|8741| <<btrfs_direct_IO>> ret = __blockdev_direct_IO(iocb, inode,
+ *   - fs/f2fs/data.c|2972| <<f2fs_direct_IO>> err = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - fs/ocfs2/aops.c|2470| <<ocfs2_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - include/linux/fs.h|3171| <<blockdev_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev, iter,
+ */
 ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     struct block_device *bdev, struct iov_iter *iter,
 			     get_block_t get_block,
diff --git a/fs/ext4/file.c b/fs/ext4/file.c
index 6a7293a5cda2..a3a0432c6db4 100644
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@ -36,6 +36,11 @@
 #include "acl.h"
 #include "truncate.h"
 
+/*
+ * called by:
+ *   - fs/ext4/file.c|64| <<ext4_dio_read_iter>> if (!ext4_dio_supported(inode)) {
+ *   - fs/ext4/file.c|382| <<ext4_dio_write_iter>> if (!ext4_dio_supported(inode)) {
+ */
 static bool ext4_dio_supported(struct inode *inode)
 {
 	if (IS_ENABLED(CONFIG_FS_ENCRYPTION) && IS_ENCRYPTED(inode))
diff --git a/fs/iomap/direct-io.c b/fs/iomap/direct-io.c
index 23837926c0c5..d5663a649119 100644
--- a/fs/iomap/direct-io.c
+++ b/fs/iomap/direct-io.c
@@ -58,6 +58,11 @@ int iomap_dio_iopoll(struct kiocb *kiocb, bool spin)
 }
 EXPORT_SYMBOL_GPL(iomap_dio_iopoll);
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|194| <<iomap_dio_zero>> iomap_dio_submit_bio(dio, iomap, bio);
+ *   - fs/iomap/direct-io.c|306| <<iomap_dio_bio_actor>> iomap_dio_submit_bio(dio, iomap, bio);
+ */
 static void iomap_dio_submit_bio(struct iomap_dio *dio, struct iomap *iomap,
 		struct bio *bio)
 {
@@ -174,6 +179,11 @@ static void iomap_dio_bio_end_io(struct bio *bio)
 	}
 }
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|256| <<iomap_dio_bio_actor>> iomap_dio_zero(dio, iomap, pos - pad, pad);
+ *   - fs/iomap/direct-io.c|321| <<iomap_dio_bio_actor>> iomap_dio_zero(dio, iomap, pos, fs_block_size - pad);
+ */
 static void
 iomap_dio_zero(struct iomap_dio *dio, struct iomap *iomap, loff_t pos,
 		unsigned len)
@@ -194,6 +204,11 @@ iomap_dio_zero(struct iomap_dio *dio, struct iomap *iomap, loff_t pos,
 	iomap_dio_submit_bio(dio, iomap, bio);
 }
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|380| <<iomap_dio_actor>> return iomap_dio_bio_actor(inode, pos, length, dio, iomap);
+ *   - fs/iomap/direct-io.c|382| <<iomap_dio_actor>> return iomap_dio_bio_actor(inode, pos, length, dio, iomap);
+ */
 static loff_t
 iomap_dio_bio_actor(struct inode *inode, loff_t pos, loff_t length,
 		struct iomap_dio *dio, struct iomap *iomap)
@@ -363,6 +378,10 @@ iomap_dio_inline_actor(struct inode *inode, loff_t pos, loff_t length,
 	return copied;
 }
 
+/*
+ * called by:
+ *   - fs/iomap/direct-io.c|523| <<iomap_dio_rw>> iomap_dio_actor);
+ */
 static loff_t
 iomap_dio_actor(struct inode *inode, loff_t pos, loff_t length,
 		void *data, struct iomap *iomap, struct iomap *srcmap)
@@ -397,6 +416,15 @@ iomap_dio_actor(struct inode *inode, loff_t pos, loff_t length,
  * may be pure data writes. In that case, we still need to do a full data sync
  * completion.
  */
+/*
+ * called by:
+ *   - fs/ext4/file.c|77| <<ext4_dio_read_iter>> ret = iomap_dio_rw(iocb, to, &ext4_iomap_ops, NULL,
+ *   - fs/ext4/file.c|438| <<ext4_dio_write_iter>> ret = iomap_dio_rw(iocb, from, &ext4_iomap_ops, &ext4_dio_write_ops,
+ *   - fs/gfs2/file.c|774| <<gfs2_file_direct_read>> ret = iomap_dio_rw(iocb, to, &gfs2_iomap_ops, NULL,
+ *   - fs/gfs2/file.c|810| <<gfs2_file_direct_write>> ret = iomap_dio_rw(iocb, from, &gfs2_iomap_ops, NULL,
+ *   - fs/xfs/xfs_file.c|191| <<xfs_file_dio_aio_read>> ret = iomap_dio_rw(iocb, to, &xfs_read_iomap_ops, NULL,
+ *   - fs/xfs/xfs_file.c|554| <<xfs_file_dio_aio_write>> ret = iomap_dio_rw(iocb, from, &xfs_direct_write_iomap_ops,
+ */
 ssize_t
 iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
diff --git a/include/linux/badblocks.h b/include/linux/badblocks.h
index 2426276b9bd3..1d7eeb551403 100644
--- a/include/linux/badblocks.h
+++ b/include/linux/badblocks.h
@@ -8,13 +8,40 @@
 #include <linux/stddef.h>
 #include <linux/types.h>
 
+/*
+ * 在以下使用BB_LEN_MASK:
+ *   - include/linux/badblocks.h|16| <<BB_LEN>> #define BB_LEN(x) (((x) & BB_LEN_MASK) + 1)
+ *
+ * 最后9个bit都是1 (表示1个range的长度, 最多512个sector)
+ */
 #define BB_LEN_MASK	(0x00000000000001FFULL)
+/*
+ * 在以下使用BB_OFFSET_MASK:
+ *   - include/linux/badblocks.h|15| <<BB_OFFSET>> #define BB_OFFSET(x) (((x) & BB_OFFSET_MASK) >> 9)
+ */
 #define BB_OFFSET_MASK	(0x7FFFFFFFFFFFFE00ULL)
+/*
+ * 在以下使用BB_ACK_MASK:
+ *   - include/linux/badblocks.h|17| <<BB_ACK>> #define BB_ACK(x) (!!((x) & BB_ACK_MASK))
+ */
 #define BB_ACK_MASK	(0x8000000000000000ULL)
 #define BB_MAX_LEN	512
 #define BB_OFFSET(x)	(((x) & BB_OFFSET_MASK) >> 9)
 #define BB_LEN(x)	(((x) & BB_LEN_MASK) + 1)
 #define BB_ACK(x)	(!!((x) & BB_ACK_MASK))
+/*
+ * 在以下调用BB_MAKE():
+ *   - block/badblocks.c|244| <<badblocks_set>> p[lo] = BB_MAKE(a, e-a, ack);
+ *   - block/badblocks.c|251| <<badblocks_set>> p[lo] = BB_MAKE(a, BB_MAX_LEN, ack);
+ *   - block/badblocks.c|276| <<badblocks_set>> p[hi] = BB_MAKE(a, e-a, ack);
+ *   - block/badblocks.c|279| <<badblocks_set>> p[hi] = BB_MAKE(a, BB_MAX_LEN, ack);
+ *   - block/badblocks.c|299| <<badblocks_set>> p[lo] = BB_MAKE(BB_OFFSET(p[lo]), newlen, ack);
+ *   - block/badblocks.c|322| <<badblocks_set>> p[hi] = BB_MAKE(s, this_sectors, acknowledged);
+ *   - block/badblocks.c|408| <<badblocks_clear>> p[lo] = BB_MAKE(a, s-a, ack);
+ *   - block/badblocks.c|411| <<badblocks_clear>> p[lo] = BB_MAKE(target, end - target, ack);
+ *   - block/badblocks.c|425| <<badblocks_clear>> p[lo] = BB_MAKE(start, s - start, ack);
+ *   - block/badblocks.c|471| <<ack_all_badblocks>> p[i] = BB_MAKE(start, len, 1);
+ */
 #define BB_MAKE(a, l, ack) (((a)<<9) | ((l)-1) | ((u64)(!!(ack)) << 63))
 
 /* Bad block numbers are stored sorted in a single page.
@@ -22,18 +49,52 @@
  * 54 bits are sector number, 9 bits are extent size,
  * 1 bit is an 'acknowledged' flag.
  */
+/*
+ * 在以下使用MAX_BADBLOCKS:
+ *   - block/badblocks.c|309| <<badblocks_set>> if (bb->count >= MAX_BADBLOCKS) {
+ *   - block/badblocks.c|402| <<badblocks_clear>> if (bb->count >= MAX_BADBLOCKS) {
+ */
 #define MAX_BADBLOCKS	(PAGE_SIZE/8)
 
 struct badblocks {
 	struct device *dev;	/* set by devm_init_badblocks */
+	/*
+	 * 在以下修改count:
+	 *   - block/badblocks.c|294| <<badblocks_set>> bb->count--;
+	 *   - block/badblocks.c|310| <<badblocks_set>> bb->count++;
+	 *   - block/badblocks.c|399| <<badblocks_clear>> bb->count++;
+	 *   - block/badblocks.c|428| <<badblocks_clear>> bb->count -= (hi - lo - 1);
+	 *   - block/badblocks.c|581| <<__badblocks_init>> bb->count = 0;
+	 */
 	int count;		/* count of bad blocks */
+	/*
+	 * 在以下使用unacked_exist:
+	 *   - block/badblocks.c|148| <<badblocks_update_acked>> if (!bb->unacked_exist)
+	 *   - block/badblocks.c|159| <<badblocks_update_acked>> bb->unacked_exist = 0;
+	 *   - block/badblocks.c|330| <<badblocks_set>> bb->unacked_exist = 1;
+	 *   - block/badblocks.c|462| <<ack_all_badblocks>> if (bb->changed == 0 && bb->unacked_exist) {
+	 *   - block/badblocks.c|474| <<ack_all_badblocks>> bb->unacked_exist = 0;
+	 *   - block/badblocks.c|537| <<badblocks_show>> bb->unacked_exist = 0;
+	 *   - drivers/md/md.c|2810| <<state_show>> rdev->badblocks.unacked_exist))
+	 *   - drivers/md/md.c|2819| <<state_show>> (rdev->badblocks.unacked_exist
+	 *   - drivers/md/md.c|2903| <<state_store>> rdev->badblocks.unacked_exist) {
+	 */
 	int unacked_exist;	/* there probably are unacknowledged
 				 * bad blocks.  This is only cleared
 				 * when a read discovers none
 				 */
+	/*
+	 * 设置shift的地方:
+	 *   - block/badblocks.c|600| <<__badblocks_init>> bb->shift = 0;
+	 *   - block/badblocks.c|602| <<__badblocks_init>> bb->shift = -1;
+	 *   - block/badblocks.c|614| <<__badblocks_init>> bb->shift = -1;
+	 */
 	int shift;		/* shift from sectors to block size
 				 * a -ve shift means badblocks are
 				 * disabled.*/
+	/*
+	 * 在__badblocks_init()中分配PAGE_SIZE
+	 */
 	u64 *page;		/* badblock list */
 	int changed;
 	seqlock_t lock;
diff --git a/include/linux/bio.h b/include/linux/bio.h
index 853d92ceee64..c1c44f9e943f 100644
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@ -827,6 +827,12 @@ static inline int bio_integrity_add_page(struct bio *bio, struct page *page,
  * must be found by the caller. This is different than IRQ driven IO, where
  * it's safe to wait for IO to complete.
  */
+/*
+ * called by:
+ *   - fs/block_dev.c|249| <<__blkdev_direct_IO_simple>> bio_set_polled(&bio, iocb);
+ *   - fs/block_dev.c|410| <<__blkdev_direct_IO>> bio_set_polled(bio, iocb);
+ *   - fs/iomap/direct-io.c|67| <<iomap_dio_submit_bio>> bio_set_polled(bio, dio->iocb);
+ */
 static inline void bio_set_polled(struct bio *bio, struct kiocb *kiocb)
 {
 	bio->bi_opf |= REQ_HIPRI;
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 11cfd6470b1a..1e5b3b0ec429 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -23,6 +23,22 @@ struct blk_mq_hw_ctx {
 		 * resources) could not be sent to the hardware. As soon as the
 		 * driver can send new requests, requests at this list will
 		 * be sent first for a fairer dispatch.
+		 */
+		/*
+		 * 在以下添加rq到hctx->dispatch:
+		 *   - block/blk-mq-sched.c|376| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1414| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1794| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2373| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 * 在以下使用hctx->dispatch:
+		 *   - block/blk-mq.c|2491| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+		 *   - block/blk-mq-sched.c|199| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *   - block/blk-mq-sched.c|196| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+		 *   - block/blk-mq-sched.c|198| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+		 *   - block/blk-mq.c|82| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+		 *   - block/blk-mq-debugfs.c|363| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+		 *   - block/blk-mq-debugfs.c|370| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+
 		 */
 		struct list_head	dispatch;
 		 /**
@@ -35,6 +51,14 @@ struct blk_mq_hw_ctx {
 	/**
 	 * @run_work: Used for scheduling a hardware queue run at a later time.
 	 */
+	/*
+	 * 在以下使用run_work:
+	 *   - block/blk-mq.c|2489| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 *   - block/blk-mq.c|1587| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|1671| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1738| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 */
 	struct delayed_work	run_work;
 	/** @cpumask: Map of available CPUs where this hctx can run. */
 	cpumask_var_t		cpumask;
@@ -86,6 +110,15 @@ struct blk_mq_hw_ctx {
 	 * decide if the hw_queue is busy using Exponential Weighted Moving
 	 * Average algorithm.
 	 */
+	/*
+	 * 在以下使用dispatch_busy:
+	 *   - block/blk-mq-debugfs.c|621| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+	 *   - block/blk-mq-sched.c|217| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+	 *   - block/blk-mq-sched.c|436| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+	 *   - block/blk-mq.c|1215| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+	 *   - block/blk-mq.c|1225| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 *   - block/blk-mq.c|2071| <<blk_mq_make_request>> !data.hctx->dispatch_busy) {
+	 */
 	unsigned int		dispatch_busy;
 
 	/** @type: HCTX_TYPE_* flags. Type of hardware queue. */
@@ -107,6 +140,10 @@ struct blk_mq_hw_ctx {
 	 * @wait_index: Index of next available dispatch_wait queue to insert
 	 * requests.
 	 */
+	/*
+	 * 使用的地方:
+	 *   - block/blk-mq-tag.h|43| <<bt_wait_ptr>> return sbq_wait_ptr(bt, &hctx->wait_index);
+	 */
 	atomic_t		wait_index;
 
 	/**
@@ -138,6 +175,16 @@ struct blk_mq_hw_ctx {
 	 * @nr_active: Number of active requests. Only used when a tag set is
 	 * shared across request queues.
 	 */
+	/*
+	 * 修改nr_active的地方:
+	 *   - block/blk-mq.c|298| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|1072| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|518| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.h|207| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|2348| <<blk_mq_alloc_hctx>> atomic_set(&hctx->nr_active, 0);
+	 * 读取nr_active的地方:
+	 *   - block/blk-mq-tag.c|87| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 */
 	atomic_t		nr_active;
 
 	/** @cpuhp_dead: List to store request if some CPU die. */
@@ -163,6 +210,16 @@ struct blk_mq_hw_ctx {
 #endif
 
 	/** @hctx_list:	List of all hardware queues. */
+	/*
+	 * 实际是if this hctx is not in use, this is an entry in q->unused_hctx_list.
+	 *   - block/blk-mq.c|2476| <<blk_mq_exit_hctx>> list_add(&hctx->hctx_list, &q->unused_hctx_list);
+	 *   - block/blk-mq.c|2576| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->hctx_list);
+	 *   - block/blk-mq.c|2944| <<blk_mq_release>> WARN_ON_ONCE(hctx && list_empty(&hctx->hctx_list));
+	 *   - block/blk-mq.c|2947| <<blk_mq_release>> list_for_each_entry_safe(hctx, next, &q->unused_hctx_list, hctx_list) {
+	 *   - block/blk-mq.c|2948| <<blk_mq_release>> list_del_init(&hctx->hctx_list);
+	 *   - block/blk-mq.c|3054| <<blk_mq_alloc_and_init_hctx>> list_for_each_entry(tmp, &q->unused_hctx_list, hctx_list) {
+	 *   - block/blk-mq.c|3061| <<blk_mq_alloc_and_init_hctx>> list_del_init(&hctx->hctx_list);
+	 */
 	struct list_head	hctx_list;
 
 	/**
@@ -185,7 +242,31 @@ struct blk_mq_hw_ctx {
  */
 struct blk_mq_queue_map {
 	unsigned int *mq_map;
+	/*
+	 * 修改map[]->nr_queues的地方:
+	 *   - block/blk-mq.c|3457| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 *   - drivers/nvme/host/pci.c|440| <<nvme_pci_map_queues>> map->nr_queues = dev->io_queues[i];
+	 *   - drivers/nvme/host/rdma.c|1824| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1827| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1833| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1836| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|1847| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2179| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2182| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2188| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2191| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2200| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 */
 	unsigned int nr_queues;
+	/*
+	 * 对于nvme来说
+	 * The poll queue(s) doesn't have an IRQ (and hence IRQ
+	 * affinity), so use the regular blk-mq cpu mapping
+	 *
+	 * First hardware queue to map onto. Used by the PCIe NVMe
+	 * driver to map each hardware queue type (enum hctx_type) onto a distinct
+	 * set of hardware queues.
+	 */
 	unsigned int queue_offset;
 };
 
@@ -232,10 +313,73 @@ enum hctx_type {
  * @tag_list:	   List of the request queues that use this tag set. See also
  *		   request_queue.tag_set_list.
  */
+/*
+ * 认真来说,set中没有hwqueue的概念.set中主要是tags.
+ * tags (struct blk_mq_tags) 在数组中管理,每个数组元素相当于是一个hwqueue了.
+ *
+ * 'struct request_queue'中的'struct blk_mq_hw_ctx'可以告诉我们去找哪一个'struct blk_mq_tags'.
+ *
+ * 因为'struct blk_mq_tags'没法管理下发的request,所以在'struct blk_mq_hw_ctx'中管理.
+ *
+ * 在set中记录cpu到hw queue的mapping
+ *
+ * struct blk_mq_tag_set
+ *  -> struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+ *      -> unsigned int *mq_map; --> 记录set中cpu到hw queue的mapping
+ *      -> unsigned int nr_queues;
+ *      -> unsigned int queue_offset;
+ *
+ * 在blk_mq_update_queue_map()设置的set中的mapping
+ *
+ *
+ * struct request_queue:
+ *  -> struct blk_mq_ctx __percpu *queue_ctx;
+ *  -> struct blk_mq_hw_ctx **queue_hw_ctx;
+ *
+ * 在blk_mq_alloc_ctxs()分配percpu的sw queue
+ */ 
 struct blk_mq_tag_set {
 	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
+	/*
+	 * 修改set->nr_maps的地方:
+	 *   - block/blk-mq.c|3013| <<blk_mq_init_sq_queue>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3409| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3420| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - drivers/block/paride/pd.c|908| <<pd_probe_drive>> disk->tag_set.nr_maps = 1;
+	 *   - drivers/block/sx8.c|1463| <<carm_init_one>> host->tag_set.nr_maps = 1;
+	 *   - drivers/nvme/host/pci.c|2309| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+	 *   - drivers/nvme/host/pci.c|2311| <<nvme_dev_add>> dev->tagset.nr_maps++;
+	 *   - drivers/nvme/host/rdma.c|752| <<nvme_rdma_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+	 *   - drivers/nvme/host/tcp.c|1490| <<nvme_tcp_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+	 */
 	unsigned int		nr_maps;
 	const struct blk_mq_ops	*ops;
+	/*
+	 * 部分修改set->nr_hw_queues的地方:
+	 *   - block/blk-mq.c|3022| <<blk_mq_init_sq_queue>> set->nr_hw_queues = 1;
+	 *   - block/blk-mq.c|3361| <<blk_mq_realloc_tag_set_tags>> set->nr_hw_queues = new_nr_hw_queues;
+	 *   - block/blk-mq.c|3429| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = 1;
+	 *   - block/blk-mq.c|3438| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = nr_cpu_ids;
+	 *   - block/blk-mq.c|3655| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = nr_hw_queues;
+	 *   - block/blk-mq.c|3663| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = prev_nr_hw_queues;
+	 *   - drivers/block/loop.c|2107| <<loop_add>> lo->tag_set.nr_hw_queues = 1;
+	 *   - drivers/block/nbd.c|1679| <<nbd_dev_add>> nbd->tag_set.nr_hw_queues = 1;
+	 *   - drivers/block/null_blk_main.c|1968| <<null_init_tag_set>> set->nr_hw_queues = nullb ? nullb->dev->submit_queues :
+	 *   - drivers/block/virtio_blk.c|817| <<virtblk_probe>> vblk->tag_set.nr_hw_queues = vblk->num_vqs;
+	 *   - drivers/block/xen-blkfront.c|968| <<xlvbd_init_blk_queue>> info->tag_set.nr_hw_queues = info->nr_rings;
+	 *   - drivers/md/dm-rq.c|551| <<dm_mq_init_request_queue>> md->tag_set->nr_hw_queues = dm_get_blk_mq_nr_hw_queues();
+	 *   - drivers/nvme/host/fc.c|2472| <<nvme_fc_create_io_queues>> ctrl->tag_set.nr_hw_queues = ctrl->ctrl.queue_count - 1;
+	 *   - drivers/nvme/host/fc.c|3139| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.nr_hw_queues = 1;
+	 *   - drivers/nvme/host/pci.c|1636| <<nvme_alloc_admin_tags>> dev->admin_tagset.nr_hw_queues = 1;
+	 *   - drivers/nvme/host/pci.c|2308| <<nvme_dev_add>> dev->tagset.nr_hw_queues = dev->online_queues - 1;
+	 *   - drivers/nvme/host/rdma.c|736| <<nvme_rdma_alloc_tagset>> set->nr_hw_queues = 1;
+	 *   - drivers/nvme/host/rdma.c|750| <<nvme_rdma_alloc_tagset>> set->nr_hw_queues = nctrl->queue_count - 1;
+	 *   - drivers/nvme/host/tcp.c|1476| <<nvme_tcp_alloc_tagset>> set->nr_hw_queues = 1;
+	 *   - drivers/nvme/host/tcp.c|1488| <<nvme_tcp_alloc_tagset>> set->nr_hw_queues = nctrl->queue_count - 1;
+	 *   - drivers/nvme/target/loop.c|347| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.nr_hw_queues = 1;
+	 *   - drivers/nvme/target/loop.c|521| <<nvme_loop_create_io_queues>> ctrl->tag_set.nr_hw_queues = ctrl->ctrl.queue_count - 1;
+	 *   - drivers/scsi/scsi_lib.c|1897| <<scsi_mq_setup_tags>> shost->tag_set.nr_hw_queues = shost->nr_hw_queues ? : 1;
+	 */
 	unsigned int		nr_hw_queues;
 	unsigned int		queue_depth;
 	unsigned int		reserved_tags;
@@ -248,6 +392,10 @@ struct blk_mq_tag_set {
 	struct blk_mq_tags	**tags;
 
 	struct mutex		tag_list_lock;
+	/*
+	 * 添加的地方:
+	 *   - block/blk-mq.c|2626| <<blk_mq_add_queue_tag_set>> list_add_tail_rcu(&q->tag_set_list, &set->tag_list);
+	 */
 	struct list_head	tag_list;
 };
 
@@ -394,7 +542,22 @@ enum {
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
 	BLK_MQ_S_STOPPED	= 0,
+	/*
+	 * 使用BLK_MQ_S_TAG_ACTIVE的地方:
+	 *   - block/blk-mq-tag.c|26| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|27| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|51| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|70| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 */
 	BLK_MQ_S_TAG_ACTIVE	= 1,
+	/*
+	 * 使用BLK_MQ_S_SCHED_RESTART的地方:
+	 *   - block/blk-mq-sched.c|67| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|70| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|76| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|85| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
@@ -476,6 +639,11 @@ static inline int blk_mq_request_started(struct request *rq)
 	return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|384| <<blk_mq_tagset_count_completed_rqs>> if (blk_mq_request_completed(rq))
+ *   - drivers/nvme/host/core.c|317| <<nvme_cancel_request>> if (blk_mq_request_completed(req))
+ */
 static inline int blk_mq_request_completed(struct request *rq)
 {
 	return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
@@ -548,10 +716,16 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	return rq + 1;
 }
 
+/*
+ * 对于q->nr_hw_queues范围内的每一个q->queue_hw_ctx[i]
+ */
 #define queue_for_each_hw_ctx(q, hctx, i)				\
 	for ((i) = 0; (i) < (q)->nr_hw_queues &&			\
 	     ({ hctx = (q)->queue_hw_ctx[i]; 1; }); (i)++)
 
+/*
+ * 对于hctx->nr_ctx范围内的每一个hctx->ctxs[i]
+ */
 #define hctx_for_each_ctx(hctx, ctx, i)					\
 	for ((i) = 0; (i) < (hctx)->nr_ctx &&				\
 	     ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index 70254ae11769..e95cb9461557 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -301,7 +301,19 @@ enum req_opf {
 	REQ_OP_SCSI_IN		= 32,
 	REQ_OP_SCSI_OUT		= 33,
 	/* Driver private requests */
+	/*
+	 * 部分使用REQ_OP_DRV_IN的例子:
+	 *   - drivers/block/virtio_blk.c|306| <<virtio_queue_rq>> case REQ_OP_DRV_IN:
+	 *   - drivers/block/virtio_blk.c|369| <<virtblk_get_id>> req = blk_get_request(q, REQ_OP_DRV_IN, 0);
+	 *   - drivers/nvme/host/core.c|485| <<nvme_alloc_request>> unsigned op = nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;
+	 *   - drivers/nvme/host/core.c|764| <<nvme_setup_cmd>> case REQ_OP_DRV_IN:
+	 */
 	REQ_OP_DRV_IN		= 34,
+	/*
+	 * 部分使用REQ_OP_DRV_OUT的例子:
+	 *   - drivers/nvme/host/core.c|485| <<nvme_alloc_request>> unsigned op = nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;
+	 *   - drivers/nvme/host/core.c|765| <<nvme_setup_cmd>> case REQ_OP_DRV_OUT:
+	 */
 	REQ_OP_DRV_OUT		= 35,
 
 	REQ_OP_LAST,
@@ -362,6 +374,16 @@ enum req_flag_bits {
 #define REQ_CGROUP_PUNT		(1ULL << __REQ_CGROUP_PUNT)
 
 #define REQ_NOUNMAP		(1ULL << __REQ_NOUNMAP)
+/*
+ * 使用REQ_HIPRI的地方:
+ *   - block/blk-core.c|937| <<generic_make_request_checks>> bio->bi_opf &= ~REQ_HIPRI;
+ *   - block/blk-mq.c|608| <<__blk_mq_complete_request>> if ((rq->cmd_flags & REQ_HIPRI) ||
+ *   - block/blk-mq.h|191| <<blk_mq_map_queue>> if (flags & REQ_HIPRI)
+ *   - drivers/nvme/host/core.c|807| <<nvme_execute_rq_polled>> rq->cmd_flags |= REQ_HIPRI;
+ *   - fs/direct-io.c|1243| <<do_blockdev_direct_IO>> dio->op_flags |= REQ_HIPRI;
+ *   - include/linux/bio.h|832| <<bio_set_polled>> bio->bi_opf |= REQ_HIPRI;
+ *   - mm/page_io.c|412| <<swap_readpage>> bio->bi_opf |= REQ_HIPRI;
+ */
 #define REQ_HIPRI		(1ULL << __REQ_HIPRI)
 
 #define REQ_DRV			(1ULL << __REQ_DRV)
@@ -461,16 +483,28 @@ static inline bool blk_qc_t_valid(blk_qc_t cookie)
 	return cookie != BLK_QC_T_NONE && cookie != BLK_QC_T_EAGAIN;
 }
 
+/*
+ * 把cookie的第31位(最高位)清空,然后向右移动16位,获得queue num
+ */
 static inline unsigned int blk_qc_t_to_queue_num(blk_qc_t cookie)
 {
+	/*
+	 * ~BLK_QC_T_INTERNAL的第31位是0, 剩下都是1
+	 */
 	return (cookie & ~BLK_QC_T_INTERNAL) >> BLK_QC_T_SHIFT;
 }
 
+/*
+ * 获取cookie的低16位, 表示tag id
+ */
 static inline unsigned int blk_qc_t_to_tag(blk_qc_t cookie)
 {
 	return cookie & ((1u << BLK_QC_T_SHIFT) - 1);
 }
 
+/*
+ * 如果cookie最高位31位设置了,就是INTERNAL
+ */
 static inline bool blk_qc_t_is_internal(blk_qc_t cookie)
 {
 	return (cookie & BLK_QC_T_INTERNAL) != 0;
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4c636c42ad68..55c79257497b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -72,6 +72,17 @@ typedef __u32 __bitwise req_flags_t;
 /* may not be passed by ioscheduler */
 #define RQF_SOFTBARRIER		((__force req_flags_t)(1 << 3))
 /* request for flush sequence */
+/*
+ * 在以下使用RQF_FLUSH_SEQ:
+ *   - block/blk-core.c|244| <<req_bio_endio>> if (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))
+ *   - block/blk-core.c|1347| <<blk_account_io_done>> !(req->rq_flags & RQF_FLUSH_SEQ)) {
+ *   - block/blk-flush.c|205| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|426| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|528| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+ *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> if (rq->rq_flags & RQF_FLUSH_SEQ) {
+ *   - block/blk-mq-sched.c|387| <<blk_mq_sched_insert_request>> if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
+ *   - include/linux/blkdev.h|120| <<RQF_NOMERGE_FLAGS>> (RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
+ */
 #define RQF_FLUSH_SEQ		((__force req_flags_t)(1 << 4))
 /* merge of different types, fail separately */
 #define RQF_MIXED_MERGE		((__force req_flags_t)(1 << 5))
@@ -106,6 +117,11 @@ typedef __u32 __bitwise req_flags_t;
 /* The per-zone write lock is held for this request */
 #define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
 /* already slept for hybrid poll */
+/*
+ * 在以下使用RQF_MQ_POLL_SLEPT:
+ *   - block/blk-mq.c|3444| <<blk_mq_poll_hybrid_sleep>> if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
+ *   - block/blk-mq.c|3461| <<blk_mq_poll_hybrid_sleep>> rq->rq_flags |= RQF_MQ_POLL_SLEPT;
+ */
 #define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
 /* ->timeout has been called, don't expire again */
 #define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))
@@ -120,6 +136,13 @@ typedef __u32 __bitwise req_flags_t;
 enum mq_rq_state {
 	MQ_RQ_IDLE		= 0,
 	MQ_RQ_IN_FLIGHT		= 1,
+	/*
+	 * 使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+	 *   - block/blk-mq.c|580| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|3474| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 *   - include/linux/blk-mq.h|481| <<blk_mq_request_completed>> return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
+	 */
 	MQ_RQ_COMPLETE		= 2,
 };
 
@@ -188,6 +211,11 @@ struct request {
 		struct {
 			unsigned int		seq;
 			struct list_head	list;
+			/*
+			 * 使用saved_end_io的地方:
+			 *   - block/blk-flush.c|221| <<blk_flush_restore_request>> rq->end_io = rq->flush.saved_end_io;
+			 *   - block/blk-flush.c|615| <<blk_insert_flush>> rq->flush.saved_end_io = rq->end_io;
+			 */
 			rq_end_io_fn		*saved_end_io;
 		} flush;
 	};
@@ -211,6 +239,12 @@ struct request {
 	 * with blk_rq_sectors(rq), except that it never be zeroed
 	 * by completion.
 	 */
+	/*
+	 * 在以下使用stats_sectors:
+	 *   - block/blk-mq.c|328| <<blk_mq_rq_ctx_init>> rq->stats_sectors = 0;
+	 *   - block/blk-mq.c|681| <<blk_mq_start_request>> rq->stats_sectors = blk_rq_sectors(rq);
+	 *   - include/linux/blkdev.h|970| <<blk_rq_stats_sectors>> return rq->stats_sectors;
+	 */
 	unsigned short stats_sectors;
 
 	/*
@@ -409,7 +443,17 @@ struct request_queue {
 	unsigned int		queue_depth;
 
 	/* hw dispatch queues */
+	/*
+	 * 设置queue_hw_ctx的地方:
+	 *   - block/blk-mq.c|2793| <<blk_mq_realloc_hw_ctxs>> q->queue_hw_ctx = new_hctxs;
+	 */
 	struct blk_mq_hw_ctx	**queue_hw_ctx;
+	/*
+	 * 设置nr_hw_queues的地方:
+	 *   - block/blk-mq.c|3103| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3147| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3233| <<blk_mq_init_allocated_queue>> q->nr_hw_queues = 0;
+	 */
 	unsigned int		nr_hw_queues;
 
 	struct backing_dev_info	*backing_dev_info;
@@ -475,9 +519,41 @@ struct request_queue {
 	unsigned int		dma_alignment;
 
 	unsigned int		rq_timeout;
+	/*
+	 * 设置poll_nsec的地方:
+	 *   - block/blk-mq.c|2906| <<blk_mq_init_allocated_queue>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|384| <<queue_poll_delay_store>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|386| <<queue_poll_delay_store>> q->poll_nsec = val * 1000;
+	 * 使用poll_nsec的地方:
+	 *   - block/blk-mq.c|3460| <<blk_mq_poll_hybrid_sleep>> if (q->poll_nsec > 0)
+	 *   - block/blk-mq.c|3461| <<blk_mq_poll_hybrid_sleep>> nsecs = q->poll_nsec;
+	 *   - block/blk-mq.c|3531| <<blk_mq_poll_hybrid>> if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
+	 *   - block/blk-sysfs.c|363| <<queue_poll_delay_show>> if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
+	 *   - block/blk-sysfs.c|366| <<queue_poll_delay_show>> val = q->poll_nsec / 1000;
+	 */
 	int			poll_nsec;
 
+	/*
+	 * 使用poll_cb的地方:
+	 *   - block/blk-mq.c|2861| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+	 *   - block/blk-mq.c|2864| <<blk_mq_init_allocated_queue>> if (!q->poll_cb)
+	 *   - block/blk-mq.c|2922| <<blk_mq_init_allocated_queue>> blk_stat_free_callback(q->poll_cb);
+	 *   - block/blk-mq.c|2923| <<blk_mq_init_allocated_queue>> q->poll_cb = NULL;
+	 *   - block/blk-mq.c|3354| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+	 *   - block/blk-mq.c|3369| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+	 *   - block/blk-mq.c|3372| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+	 *   - block/blk-sysfs.c|881| <<__blk_release_queue>> blk_stat_remove_callback(q, q->poll_cb);
+	 *   - block/blk-sysfs.c|882| <<__blk_release_queue>> blk_stat_free_callback(q->poll_cb);
+	 */
 	struct blk_stat_callback	*poll_cb;
+	/*
+	 * 使用poll_stat[]的地方:
+	 *   - block/blk-mq-debugfs.c|34| <<queue_poll_stat_show>> print_stat(m, &q->poll_stat[2 * bucket]);
+	 *   - block/blk-mq-debugfs.c|38| <<queue_poll_stat_show>> print_stat(m, &q->poll_stat[2 * bucket + 1]);
+	 *   - block/blk-mq.c|3388| <<blk_mq_poll_stats_fn>> q->poll_stat[bucket] = cb->stat[bucket];
+	 *   - block/blk-mq.c|3425| <<blk_mq_poll_nsecs>> if (q->poll_stat[bucket].nr_samples)
+	 *   - block/blk-mq.c|3426| <<blk_mq_poll_nsecs>> ret = (q->poll_stat[bucket].mean + 1) / 2;
+	 */
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
 	struct timer_list	timeout;
@@ -600,14 +676,62 @@ struct request_queue {
 #define QUEUE_FLAG_SAME_FORCE	12	/* force complete on same CPU */
 #define QUEUE_FLAG_DEAD		13	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE	14	/* queue is initialized */
+/*
+ * 使用QUEUE_FLAG_POLL的地方:
+ *   - block/blk-mq.c|3021| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+ *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+ *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+ *   - block/blk-core.c|936| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ *   - block/blk-mq.c|3774| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ *   - drivers/nvme/host/core.c|805| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+ *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+ */
 #define QUEUE_FLAG_POLL		16	/* IO polling enabled if set */
+/*
+ * 在以下设置QUEUE_FLAG_WC的几个例子:
+ *   - block/blk-settings.c|824| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+ *   - block/blk-settings.c|826| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_WC, q)
+ *   - block/blk-sysfs.c|530| <<queue_wc_store>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+ *   - drivers/md/dm-table.c|1905| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_WC))) {
+ */
 #define QUEUE_FLAG_WC		17	/* Write back caching */
+/*
+ * 使用QUEUE_FLAG_FUA的地方:
+ *   - block/blk-flush.c|158| <<blk_flush_policy>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&
+ *   - block/blk-flush.c|471| <<blk_insert_flush>> if (!(fflags & (1UL << QUEUE_FLAG_FUA)))
+ *   - block/blk-settings.c|828| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_FUA, q);
+ *   - block/blk-settings.c|830| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_FUA, q);
+ *   - block/blk-sysfs.c|539| <<queue_fua_show>> return sprintf(page, "%u\n", test_bit(QUEUE_FLAG_FUA, &q->queue_flags));
+ *   - drivers/md/dm-table.c|1907| <<dm_table_set_restrictions>> if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_FUA)))
+ *   - drivers/target/target_core_iblock.c|703| <<iblock_execute_rw>> if (test_bit(QUEUE_FLAG_FUA, &q->queue_flags)) {
+ *   - include/linux/blkdev.h|733| <<blk_queue_fua>> #define blk_queue_fua(q) test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 在以下使用QUEUE_FLAG_STATS:
+ *   - block/blk-mq.c|670| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ *   - block/blk-stat.c|320| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|335| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|376| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
+/*
+ * 在以下使用QUEUE_FLAG_POLL_STATS:
+ *   - block/blk-mq.c|3331| <<blk_poll_stats_enable>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+ *   - block/blk-mq.c|3332| <<blk_poll_stats_enable>> blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
+ *   - block/blk-mq.c|3344| <<blk_mq_poll_stats_start>> if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+ *   - block/blk-sysfs.c|880| <<__blk_release_queue>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
+ */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
+/*
+ * 在以下使用QUEUE_FLAG_QUIESCED:
+ *   - block/blk-mq.c|218| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+ *   - block/blk-mq.c|259| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+ *   - include/linux/blkdev.h|771| <<blk_queue_quiesced>> #define blk_queue_quiesced(q) test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 #define QUEUE_FLAG_ZONE_RESETALL 26	/* supports Zone Reset All */
@@ -650,6 +774,13 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
 			     REQ_FAILFAST_DRIVER))
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|195| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|1790| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+ *   - block/blk-mq.c|2156| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+ *   - drivers/mmc/core/queue.c|506| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+ */
 #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
 #define blk_queue_pm_only(q)	atomic_read(&(q)->pm_only)
 #define blk_queue_fua(q)	test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
@@ -948,6 +1079,16 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 
 static inline unsigned int blk_rq_stats_sectors(const struct request *rq)
 {
+	/*
+	 * rq sectors used for blk stats. It has the same value
+	 * with blk_rq_sectors(rq), except that it never be zeroed
+	 * by completion.
+	 *
+	 * 在以下使用stats_sectors:
+	 *   - block/blk-mq.c|328| <<blk_mq_rq_ctx_init>> rq->stats_sectors = 0;
+	 *   - block/blk-mq.c|681| <<blk_mq_start_request>> rq->stats_sectors = blk_rq_sectors(rq);
+	 *   - include/linux/blkdev.h|970| <<blk_rq_stats_sectors>> return rq->stats_sectors;
+	 */
 	return rq->stats_sectors;
 }
 
@@ -987,6 +1128,14 @@ static inline struct bio_vec req_bvec(struct request *rq)
 	return mp_bvec_iter_bvec(rq->bio->bi_io_vec, rq->bio->bi_iter);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1236| <<blk_cloned_rq_check_limits>> if (blk_rq_sectors(rq) > blk_queue_get_max_sectors(q, req_op(rq))) {
+ *   - block/blk-core.c|1239| <<blk_cloned_rq_check_limits>> blk_queue_get_max_sectors(q, req_op(rq)));
+ *   - drivers/scsi/cxlflash/vlun.c|433| <<write_same16>> const u32 ws_limit = blk_queue_get_max_sectors(sdev->request_queue,
+ *   - include/linux/blkdev.h|1157| <<blk_rq_get_max_sectors>> return blk_queue_get_max_sectors(q, req_op(rq));
+ *   - include/linux/blkdev.h|1160| <<blk_rq_get_max_sectors>> blk_queue_get_max_sectors(q, req_op(rq)));
+ */
 static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 						     int op)
 {
@@ -1000,6 +1149,7 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 	if (unlikely(op == REQ_OP_WRITE_ZEROES))
 		return q->limits.max_write_zeroes_sectors;
 
+	/* max sectors for a request for this queue */
 	return q->limits.max_sectors;
 }
 
@@ -1010,6 +1160,11 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 static inline unsigned int blk_max_size_offset(struct request_queue *q,
 					       sector_t offset)
 {
+	/*
+	 * 似乎virtio和xen没设置chunk_sectors.
+	 * xen设置了q->limits.max_sectors
+	 * virtio-blk就是-1U
+	 */
 	if (!q->limits.chunk_sectors)
 		return q->limits.max_sectors;
 
@@ -1017,6 +1172,14 @@ static inline unsigned int blk_max_size_offset(struct request_queue *q,
 			(offset & (q->limits.chunk_sectors - 1))));
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|668| <<bio_attempt_discard_merge>> blk_rq_get_max_sectors(req, blk_rq_pos(req)))
+ *   - block/blk-merge.c|600| <<ll_back_merge_fn>> blk_rq_get_max_sectors(req, blk_rq_pos(req))) {
+ *   - block/blk-merge.c|616| <<ll_front_merge_fn>> blk_rq_get_max_sectors(req, bio->bi_iter.bi_sector)) {
+ *   - block/blk-merge.c|632| <<req_attempt_discard_merge>> blk_rq_get_max_sectors(req, blk_rq_pos(req)))
+ *   - block/blk-merge.c|654| <<ll_merge_requests_fn>> blk_rq_get_max_sectors(req, blk_rq_pos(req)))
+ */
 static inline unsigned int blk_rq_get_max_sectors(struct request *rq,
 						  sector_t offset)
 {
@@ -1311,6 +1474,14 @@ static inline unsigned int queue_physical_block_size(const struct request_queue
 	return q->limits.physical_block_size;
 }
 
+/*
+ * called by:
+ *   - block/compat_ioctl.c|339| <<compat_blkdev_ioctl>> return compat_put_uint(arg, bdev_physical_block_size(bdev));
+ *   - block/ioctl.c|530| <<blkdev_ioctl>> return put_uint(arg, bdev_physical_block_size(bdev));
+ *   - drivers/block/xen-blkback/xenbus.c|925| <<connect>> bdev_physical_block_size(be->blkif->vbd.bdev));
+ *   - drivers/md/dm-log-writes.c|898| <<log_writes_io_hints>> limits->physical_block_size = bdev_physical_block_size(lc->dev->bdev);
+ *   - drivers/target/target_core_iblock.c|819| <<iblock_get_lbppbe>> int logs_per_phys = bdev_physical_block_size(bd) / bdev_logical_block_size(bd);
+ */
 static inline unsigned int bdev_physical_block_size(struct block_device *bdev)
 {
 	return queue_physical_block_size(bdev_get_queue(bdev));
@@ -1417,10 +1588,25 @@ static inline unsigned int bdev_write_same(struct block_device *bdev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-lib.c|227| <<__blkdev_issue_write_zeroes>> max_write_zeroes_sectors = bdev_write_zeroes_sectors(bdev);
+ *   - block/blk-lib.c|365| <<blkdev_issue_zeroout>> bool try_write_zeroes = !!bdev_write_zeroes_sectors(bdev);
+ *   - block/blk-lib.c|394| <<blkdev_issue_zeroout>> if (!bdev_write_zeroes_sectors(bdev)) {
+ *   - drivers/md/dm-kcopyd.c|834| <<dm_kcopyd_copy>> if (!bdev_write_zeroes_sectors(job->dests[i].bdev)) {
+ *   - drivers/target/target_core_iblock.c|120| <<iblock_configure_device>> max_write_zeroes_sectors = bdev_write_zeroes_sectors(bd);
+ *   - drivers/target/target_core_iblock.c|471| <<iblock_execute_write_same>> if (bdev_write_zeroes_sectors(bdev)) {
+ *
+ * 返回The maximum number of write zeroes sectors (in 512-byte sectors) in
+ * one segment.
+ */
 static inline unsigned int bdev_write_zeroes_sectors(struct block_device *bdev)
 {
 	struct request_queue *q = bdev_get_queue(bdev);
 
+	/*
+	 * 设置的一处: blk_queue_max_write_zeroes_sectors()
+	 */
 	if (q)
 		return q->limits.max_write_zeroes_sectors;
 
diff --git a/include/linux/fs.h b/include/linux/fs.h
index 98e0349adb52..887db5860145 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -309,6 +309,25 @@ enum rw_hint {
 #define IOCB_EVENTFD		(1 << 0)
 #define IOCB_APPEND		(1 << 1)
 #define IOCB_DIRECT		(1 << 2)
+/*
+ * 设置和取消IOCB_HIPRI的地方:
+ *   - include/linux/fs.h|3427| <<kiocb_set_rw_flags>> ki->ki_flags |= IOCB_HIPRI;
+ *   - fs/io_uring.c|1536| <<io_prep_rw>> kiocb->ki_flags |= IOCB_HIPRI;
+ *   - fs/aio.c|1476| <<aio_prep_rw>> req->ki_flags &= ~IOCB_HIPRI;
+ * 其他使用IOCB_HIPRI的地方:
+ *   - fs/block_dev.c|248| <<__blkdev_direct_IO_simple>> if (iocb->ki_flags & IOCB_HIPRI)
+ *   - fs/block_dev.c|256| <<__blkdev_direct_IO_simple>> if (!(iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/block_dev.c|346| <<__blkdev_direct_IO>> bool is_poll = (iocb->ki_flags & IOCB_HIPRI) != 0;
+ *   - fs/block_dev.c|409| <<__blkdev_direct_IO>> if (iocb->ki_flags & IOCB_HIPRI) {
+ *   - fs/block_dev.c|450| <<__blkdev_direct_IO>> if (!(iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/direct-io.c|501| <<dio_await_one>> if (!(dio->iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/direct-io.c|1250| <<do_blockdev_direct_IO>> if (iocb->ki_flags & IOCB_HIPRI)
+ *   - fs/io_uring.c|1540| <<io_prep_rw>> if (kiocb->ki_flags & IOCB_HIPRI)
+ *   - fs/io_uring.c|1710| <<loop_rw_iter>> if (kiocb->ki_flags & IOCB_HIPRI)
+ *   - fs/iomap/direct-io.c|71| <<iomap_dio_submit_bio>> if (dio->iocb->ki_flags & IOCB_HIPRI)
+ *   - fs/iomap/direct-io.c|584| <<iomap_dio_rw>> if (!(iocb->ki_flags & IOCB_HIPRI) ||
+ *   - fs/overlayfs/file.c|218| <<ovl_iocb_to_rwf>> if (ifl & IOCB_HIPRI)
+ */
 #define IOCB_HIPRI		(1 << 3)
 #define IOCB_DSYNC		(1 << 4)
 #define IOCB_SYNC		(1 << 5)
@@ -3163,6 +3182,19 @@ ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     dio_iodone_t end_io, dio_submit_t submit_io,
 			     int flags);
 
+/*
+ * called by:
+ *   - drivers/staging/exfat/exfat_super.c|3089| <<exfat_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, exfat_get_block);
+ *   - fs/affs/file.c|409| <<affs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, affs_get_block);
+ *   - fs/ext2/inode.c|948| <<ext2_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, ext2_get_block);
+ *   - fs/fat/inode.c|288| <<fat_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, fat_get_block);
+ *   - fs/hfs/inode.c|137| <<hfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfs_get_block);
+ *   - fs/hfsplus/inode.c|134| <<hfsplus_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfsplus_get_block);
+ *   - fs/jfs/inode.c|342| <<jfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, jfs_get_block);
+ *   - fs/nilfs2/inode.c|303| <<nilfs_direct_IO>> return blockdev_direct_IO(iocb, inode, iter, nilfs_get_block);
+ *   - fs/reiserfs/inode.c|3270| <<reiserfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter,
+ *   - fs/udf/inode.c|224| <<udf_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, udf_get_block);
+ */
 static inline ssize_t blockdev_direct_IO(struct kiocb *iocb,
 					 struct inode *inode,
 					 struct iov_iter *iter,
diff --git a/include/linux/genhd.h b/include/linux/genhd.h
index ea4c133b4139..94179c59aed7 100644
--- a/include/linux/genhd.h
+++ b/include/linux/genhd.h
@@ -24,6 +24,11 @@
 #define dev_to_disk(device)	container_of((device), struct gendisk, part0.__dev)
 #define dev_to_part(device)	container_of((device), struct hd_struct, __dev)
 #define disk_to_dev(disk)	(&(disk)->part0.__dev)
+/*
+ * struct hd_struct:
+ *  -> struct device __dev;
+ * 返回hd_struct->__dev
+ */
 #define part_to_dev(part)	(&((part)->__dev))
 
 extern struct device_type part_type;
@@ -142,6 +147,15 @@ struct hd_struct {
 #define GENHD_FL_EXT_DEVT			64 /* allow extended devt */
 #define GENHD_FL_NATIVE_CAPACITY		128
 #define GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE	256
+/*
+ * 在以下使用GENHD_FL_NO_PART_SCAN:
+ *   - block/genhd.c|799| <<__device_add_disk>> disk->flags |= GENHD_FL_NO_PART_SCAN;
+ *   - drivers/block/loop.c|1193| <<__loop_clr_fd>> lo->lo_disk->flags |= GENHD_FL_NO_PART_SCAN;
+ *   - drivers/block/loop.c|1349| <<loop_set_status>> lo->lo_disk->flags &= ~GENHD_FL_NO_PART_SCAN;
+ *   - drivers/block/loop.c|2063| <<loop_add>> disk->flags |= GENHD_FL_NO_PART_SCAN;
+ *   - drivers/mmc/core/block.c|2322| <<mmc_blk_alloc_req>> md->disk->flags |= GENHD_FL_NO_PART_SCAN
+ *   - include/linux/genhd.h|250| <<disk_part_scan_enabled>> !(disk->flags & GENHD_FL_NO_PART_SCAN);
+ */
 #define GENHD_FL_NO_PART_SCAN			512
 #define GENHD_FL_HIDDEN				1024
 
@@ -245,6 +259,20 @@ static inline bool disk_part_scan_enabled(struct gendisk *disk)
 		!(disk->flags & GENHD_FL_NO_PART_SCAN);
 }
 
+/*
+ * 这里似乎有bug, 根据最新的一个fix:
+ * Commit b72053072c0b ("block: allow partitions on host aware zone
+ * devices") introduced the helper function disk_has_partitions() to check
+ * if a given disk has valid partitions. However, since this function result
+ * directly depends on the disk partition table length rather than the
+ * actual existence of valid partitions in the table, it returns true even
+ * after all partitions are removed from the disk. For host aware zoned
+ * block devices, this results in zone management support to be kept
+ * disabled even after removing all partitions.
+ *
+ * called by:
+ *   - drivers/scsi/sd.c|2961| <<sd_read_block_characteristics>> if (sdkp->zoned == 1 && !disk_has_partitions(sdkp->disk)) {
+ */
 static inline bool disk_has_partitions(struct gendisk *disk)
 {
 	bool ret = false;
@@ -384,6 +412,12 @@ static inline void free_part_stats(struct hd_struct *part)
 	 part_stat_read(part, field[STAT_WRITE]) +			\
 	 part_stat_read(part, field[STAT_DISCARD]))
 
+/*
+ * called by:
+ *   - block/bio.c|1762| <<update_io_ticks>> __part_stat_add(part, io_ticks, 1);
+ *   - include/linux/genhd.h|419| <<part_stat_add>> __part_stat_add((part), field, addnd); \
+ *   - include/linux/genhd.h|421| <<part_stat_add>> __part_stat_add(&part_to_disk((part))->part0, \
+ */
 #define __part_stat_add(part, field, addnd)				\
 	(part_stat_get(part, field) += (addnd))
 
@@ -671,6 +705,16 @@ extern ssize_t part_fail_store(struct device *dev,
 			       const char *buf, size_t count);
 #endif /* CONFIG_FAIL_MAKE_REQUEST */
 
+/*
+ * called by:
+ *   - drivers/block/mtip32xx/mtip32xx.c|3591| <<mtip_block_initialize>> dd->disk = alloc_disk_node(MTIP_MAX_MINORS, dd->numa_node);
+ *   - drivers/block/null_blk_main.c|1565| <<null_gendisk_register>> disk = nullb->disk = alloc_disk_node(1, nullb->dev->home_node);
+ *   - drivers/ide/ide-gd.c|389| <<ide_gd_probe>> g = alloc_disk_node(IDE_DISK_MINORS, hwif_to_node(drive->hwif));
+ *   - drivers/md/dm.c|1954| <<alloc_dev>> md->disk = alloc_disk_node(1, md->numa_node_id);
+ *   - drivers/nvdimm/pmem.c|451| <<pmem_attach_disk>> disk = alloc_disk_node(0, nid);
+ *   - drivers/nvme/host/core.c|3533| <<nvme_alloc_ns>> disk = alloc_disk_node(0, node);
+ *   - include/linux/genhd.h|690| <<alloc_disk>> #define alloc_disk(minors) alloc_disk_node(minors, NUMA_NO_NODE)
+ */
 #define alloc_disk_node(minors, node_id)				\
 ({									\
 	static struct lock_class_key __key;				\
@@ -712,6 +756,10 @@ static inline void hd_struct_put(struct hd_struct *part)
 	percpu_ref_put(&part->ref);
 }
 
+/*
+ * called by:
+ *   - block/partition-generic.c|298| <<delete_partition>> hd_struct_kill(part);
+ */
 static inline void hd_struct_kill(struct hd_struct *part)
 {
 	percpu_ref_kill(&part->ref);
diff --git a/include/linux/irq.h b/include/linux/irq.h
index 7853eb9301f2..345b5be6a904 100644
--- a/include/linux/irq.h
+++ b/include/linux/irq.h
@@ -213,6 +213,13 @@ struct irq_data {
 enum {
 	IRQD_TRIGGER_MASK		= 0xf,
 	IRQD_SETAFFINITY_PENDING	= (1 <<  8),
+	/*
+	 * 在以下使用IRQD_ACTIVATED:
+	 *   - kernel/irq/debugfs.c|101| <<global>> BIT_MASK_DESCR(IRQD_ACTIVATED),
+	 *   - include/linux/irq.h|374| <<irqd_is_activated>> return __irqd_to_state(d) & IRQD_ACTIVATED;
+	 *   - include/linux/irq.h|379| <<irqd_set_activated>> __irqd_to_state(d) |= IRQD_ACTIVATED;
+	 *   - include/linux/irq.h|384| <<irqd_clr_activated>> __irqd_to_state(d) &= ~IRQD_ACTIVATED;
+	 */
 	IRQD_ACTIVATED			= (1 <<  9),
 	IRQD_NO_BALANCING		= (1 << 10),
 	IRQD_PER_CPU			= (1 << 11),
@@ -225,6 +232,12 @@ enum {
 	IRQD_IRQ_INPROGRESS		= (1 << 18),
 	IRQD_WAKEUP_ARMED		= (1 << 19),
 	IRQD_FORWARDED_TO_VCPU		= (1 << 20),
+	/*
+	 * 在以下使用IRQD_AFFINITY_MANAGED:
+	 *   - kernel/irq/debugfs.c|114| <<global>> BIT_MASK_DESCR(IRQD_AFFINITY_MANAGED),
+	 *   - include/linux/irq.h|350| <<irqd_affinity_is_managed>> return __irqd_to_state(d) & IRQD_AFFINITY_MANAGED;
+	 *   - kernel/irq/irqdesc.c|487| <<alloc_descs>> flags = IRQD_AFFINITY_MANAGED |
+	 */
 	IRQD_AFFINITY_MANAGED		= (1 << 21),
 	IRQD_IRQ_STARTED		= (1 << 22),
 	IRQD_MANAGED_SHUTDOWN		= (1 << 23),
@@ -345,6 +358,19 @@ static inline void irqd_clr_forwarded_to_vcpu(struct irq_data *d)
 	__irqd_to_state(d) &= ~IRQD_FORWARDED_TO_VCPU;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|136| <<apic_update_vector>> bool managed = irqd_affinity_is_managed(irqd);
+ *   - arch/x86/kernel/apic/vector.c|296| <<assign_irq_vector_policy>> if (irqd_affinity_is_managed(irqd))
+ *   - arch/x86/kernel/apic/vector.c|332| <<clear_irq_vector>> bool managed = irqd_affinity_is_managed(irqd);
+ *   - arch/x86/kernel/apic/vector.c|789| <<apic_set_affinity>> if (irqd_affinity_is_managed(irqd))
+ *   - kernel/irq/chip.c|198| <<__irq_startup_managed>> if (!irqd_affinity_is_managed(d))
+ *   - kernel/irq/chip.c|290| <<irq_activate>> if (!irqd_affinity_is_managed(d))
+ *   - kernel/irq/cpuhotplug.c|117| <<migrate_one_irq>> if (irqd_affinity_is_managed(d)) {
+ *   - kernel/irq/cpuhotplug.c|179| <<irq_restore_affinity_of_irq>> if (!irqd_affinity_is_managed(data) || !desc->action ||
+ *   - kernel/irq/manage.c|176| <<irq_can_set_affinity_usr>> !irqd_affinity_is_managed(&desc->irq_data);
+ *   - kernel/irq/manage.c|415| <<irq_setup_affinity>> if (irqd_affinity_is_managed(&desc->irq_data) ||
+ */
 static inline bool irqd_affinity_is_managed(struct irq_data *d)
 {
 	return __irqd_to_state(d) & IRQD_AFFINITY_MANAGED;
@@ -352,9 +378,21 @@ static inline bool irqd_affinity_is_managed(struct irq_data *d)
 
 static inline bool irqd_is_activated(struct irq_data *d)
 {
+	/*
+	 * 在以下使用IRQD_ACTIVATED:
+	 *   - kernel/irq/debugfs.c|101| <<global>> BIT_MASK_DESCR(IRQD_ACTIVATED),
+	 *   - include/linux/irq.h|374| <<irqd_is_activated>> return __irqd_to_state(d) & IRQD_ACTIVATED;
+	 *   - include/linux/irq.h|379| <<irqd_set_activated>> __irqd_to_state(d) |= IRQD_ACTIVATED;
+	 *   - include/linux/irq.h|384| <<irqd_clr_activated>> __irqd_to_state(d) &= ~IRQD_ACTIVATED;
+	 */
 	return __irqd_to_state(d) & IRQD_ACTIVATED;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/internals.h|461| <<irq_domain_activate_irq>> irqd_set_activated(data);
+ *   - kernel/irq/irqdomain.c|1655| <<irq_domain_activate_irq>> irqd_set_activated(irq_data);
+ */
 static inline void irqd_set_activated(struct irq_data *d)
 {
 	__irqd_to_state(d) |= IRQD_ACTIVATED;
@@ -825,6 +863,13 @@ static inline struct cpumask *irq_get_affinity_mask(int irq)
 
 static inline struct cpumask *irq_data_get_affinity_mask(struct irq_data *d)
 {
+	/*
+	 * struct irq_common_data  irq_common_data;
+	 *   - cpumask_var_t           affinity;
+	 *   - cpumask_var_t           effective_affinity;
+	 * struct irq_data         irq_data;
+	 *   - struct irq_common_data  *common;
+	 */
 	return d->common->affinity;
 }
 
@@ -834,6 +879,29 @@ struct cpumask *irq_data_get_effective_affinity_mask(struct irq_data *d)
 {
 	return d->common->effective_affinity;
 }
+/*
+ * [0] irq_data_update_effective_affinity()
+ * [0] apic_update_irq_cfg
+ * [0] assign_managed_vector.constprop.27
+ * [0] x86_vector_activate
+ * [0] __irq_domain_activate_irq
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * x86调用irq_data_update_effective_affinity()的例子:
+ *   - arch/x86/kernel/apic/vector.c|126| <<apic_update_irq_cfg>> irq_data_update_effective_affinity(irqd, cpumask_of(cpu));
+ *   - drivers/xen/events/events_base.c|1329| <<set_affinity_irq>> irq_data_update_effective_affinity(data, cpumask_of(tcpu));
+ */
 static inline void irq_data_update_effective_affinity(struct irq_data *d,
 						      const struct cpumask *m)
 {
diff --git a/include/linux/irqdesc.h b/include/linux/irqdesc.h
index d6e2ab538ef2..6364918a9402 100644
--- a/include/linux/irqdesc.h
+++ b/include/linux/irqdesc.h
@@ -129,6 +129,13 @@ static inline unsigned int irq_desc_get_irq(struct irq_desc *desc)
 
 static inline struct irq_data *irq_desc_get_irq_data(struct irq_desc *desc)
 {
+	/*
+	 * struct irq_common_data  irq_common_data;
+	 *   - cpumask_var_t           affinity;
+	 *   - cpumask_var_t           effective_affinity;
+	 * struct irq_data         irq_data;
+	 *   - struct irq_common_data  *common;
+	 */
 	return &desc->irq_data;
 }
 
diff --git a/include/linux/nvme.h b/include/linux/nvme.h
index 3d5189f46cb1..cc09ee6c7e56 100644
--- a/include/linux/nvme.h
+++ b/include/linux/nvme.h
@@ -86,14 +86,61 @@ enum {
 	NVMF_RDMA_CMS_RDMA_CM	= 1, /* Sockets based endpoint addressing */
 };
 
+/*
+ * 在以下使用NVME_AQ_DEPTH:
+ *   - drivers/nvme/host/fabrics.c|378| <<nvmf_connect_admin_queue>> cmd.connect.sqsize = cpu_to_le16(NVME_AQ_DEPTH - 1);
+ *   - drivers/nvme/host/fc.c|2650| <<nvme_fc_create_association>> NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/fc.c|2655| <<nvme_fc_create_association>> NVME_AQ_DEPTH, (NVME_AQ_DEPTH / 4));
+ *   - drivers/nvme/host/pci.c|2507| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/pci.c|3372| <<nvme_dev_add>> dev->tagset.reserved_tags = NVME_AQ_DEPTH;
+ *   - drivers/nvme/host/pci.c|3514| <<nvme_pci_enable>> (dev->q_depth < (NVME_AQ_DEPTH + 2))) {
+ *   - drivers/nvme/host/pci.c|3515| <<nvme_pci_enable>> dev->q_depth = NVME_AQ_DEPTH + 2;
+ *   - drivers/nvme/host/rdma.c|783| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/rdma.c|1605| <<nvme_rdma_route_resolved>> priv.hrqsize = cpu_to_le16(NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/rdma.c|1606| <<nvme_rdma_route_resolved>> priv.hsqsize = cpu_to_le16(NVME_AQ_DEPTH - 1);
+ *   - drivers/nvme/host/tcp.c|1548| <<nvme_tcp_alloc_admin_queue>> ret = nvme_tcp_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/target/discovery.c|116| <<nvmet_format_discovery_entry>> e->asqsz = cpu_to_le16(NVME_AQ_DEPTH);
+ *   - drivers/nvme/target/rdma.c|1114| <<nvmet_rdma_parse_cm_connect_req>> if (!queue->host_qid && queue->recv_queue_size > NVME_AQ_DEPTH)
+ *   - include/linux/nvme.h|92| <<NVME_AQ_BLK_MQ_DEPTH>> #define NVME_AQ_BLK_MQ_DEPTH (NVME_AQ_DEPTH - NVME_NR_AEN_COMMANDS)
+ */
 #define NVME_AQ_DEPTH		32
+/*
+ * 在以下使用NVME_NR_AEN_COMMANDS:
+ *   - drivers/nvme/host/fc.c|1529| <<nvme_fc_abort_aen_ops>> for (i = 0; i < NVME_NR_AEN_COMMANDS; i++, aen_op++)
+ *   - drivers/nvme/host/fc.c|1791| <<nvme_fc_init_aen_ops>> for (i = 0; i < NVME_NR_AEN_COMMANDS; i++, aen_op++) {
+ *   - drivers/nvme/host/fc.c|1825| <<nvme_fc_term_aen_ops>> for (i = 0; i < NVME_NR_AEN_COMMANDS; i++, aen_op++) {
+ *   - include/linux/nvme.h|92| <<NVME_AQ_BLK_MQ_DEPTH>> #define NVME_AQ_BLK_MQ_DEPTH (NVME_AQ_DEPTH - NVME_NR_AEN_COMMANDS)
+ */
 #define NVME_NR_AEN_COMMANDS	1
+/*
+ * 在以下使用NVME_AQ_BLK_MQ_DEPTH:
+ *   - drivers/nvme/host/fc.c|1801| <<nvme_fc_init_aen_ops>> (NVME_AQ_BLK_MQ_DEPTH + i));
+ *   - drivers/nvme/host/fc.c|1813| <<nvme_fc_init_aen_ops>> sqe->common.command_id = NVME_AQ_BLK_MQ_DEPTH + i;
+ *   - drivers/nvme/host/nvme.h|619| <<nvme_is_aen_req>> return !qid && command_id >= NVME_AQ_BLK_MQ_DEPTH;
+ *   - drivers/nvme/host/pci.c|1685| <<nvme_pci_submit_async_event>> c.common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+ *   - drivers/nvme/host/rdma.c|1424| <<nvme_rdma_submit_async_event>> cmd->common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+ *   - drivers/nvme/host/tcp.c|2030| <<nvme_tcp_submit_async_event>> cmd->common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+ *   - drivers/nvme/target/loop.c|181| <<nvme_loop_submit_async_event>> iod->cmd.common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+ *   - include/linux/nvme.h|108| <<NVME_AQ_MQ_TAG_DEPTH>> #define NVME_AQ_MQ_TAG_DEPTH (NVME_AQ_BLK_MQ_DEPTH - 1)
+ *
+ * 32 - 1 = 31
+ */
 #define NVME_AQ_BLK_MQ_DEPTH	(NVME_AQ_DEPTH - NVME_NR_AEN_COMMANDS)
 
 /*
  * Subtract one to leave an empty queue entry for 'Full Queue' condition. See
  * NVM-Express 1.2 specification, section 4.1.2.
  */
+/*
+ * 在以下使用NVME_AQ_MQ_TAG_DEPTH:
+ *   - drivers/nvme/host/fc.c|3132| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+ *   - drivers/nvme/host/pci.c|2394| <<nvme_alloc_admin_tags>> dev->admin_tagset.queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+ *   - drivers/nvme/host/rdma.c|730| <<nvme_rdma_alloc_tagset>> set->queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+ *   - drivers/nvme/host/tcp.c|1471| <<nvme_tcp_alloc_tagset>> set->queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+ *   - drivers/nvme/target/loop.c|341| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+ *
+ * 31 - 1 = 30
+ */
 #define NVME_AQ_MQ_TAG_DEPTH	(NVME_AQ_BLK_MQ_DEPTH - 1)
 
 enum {
@@ -101,7 +148,31 @@ enum {
 	NVME_REG_VS	= 0x0008,	/* Version */
 	NVME_REG_INTMS	= 0x000c,	/* Interrupt Mask Set */
 	NVME_REG_INTMC	= 0x0010,	/* Interrupt Mask Clear */
+	/*
+	 * 在以下使用NVME_REG_CC:
+	 *   - drivers/nvme/host/core.c|2195| <<nvme_disable_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|2238| <<nvme_enable_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|2254| <<nvme_shutdown_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/target/fabrics-cmd.c|26| <<nvmet_execute_prop_set>> case NVME_REG_CC:
+	 *   - drivers/nvme/target/fabrics-cmd.c|61| <<nvmet_execute_prop_get>> case NVME_REG_CC:
+	 *   - drivers/pci/quirks.c|3822| <<nvme_disable_and_flr>> bar = pci_iomap(dev, 0, NVME_REG_CC + sizeof(cfg));
+	 *   - drivers/pci/quirks.c|3829| <<nvme_disable_and_flr>> cfg = readl(bar + NVME_REG_CC);
+	 *   - drivers/pci/quirks.c|3843| <<nvme_disable_and_flr>> writel(cfg, bar + NVME_REG_CC);
+	 */
 	NVME_REG_CC	= 0x0014,	/* Controller Configuration */
+	/*
+	 * 在以下使用NVME_REG_CSTS:
+	 *   - drivers/nvme/host/core.c|2102| <<nvme_wait_ready>> while ((ret = ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts)) == 0) {
+	 *   - drivers/nvme/host/core.c|2198| <<nvme_shutdown_ctrl>> while ((ret = ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts)) == 0) {
+	 *   - drivers/nvme/host/core.c|3884| <<nvme_ctrl_pp_status>> if (ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts))
+	 *   - drivers/nvme/host/pci.c|1263| <<nvme_timeout>> u32 csts = readl(dev->bar + NVME_REG_CSTS);
+	 *   - drivers/nvme/host/pci.c|1705| <<nvme_pci_configure_admin_queue>> (readl(dev->bar + NVME_REG_CSTS) & NVME_CSTS_NSSRO))
+	 *   - drivers/nvme/host/pci.c|1706| <<nvme_pci_configure_admin_queue>> writel(NVME_CSTS_NSSRO, dev->bar + NVME_REG_CSTS);
+	 *   - drivers/nvme/host/pci.c|2358| <<nvme_pci_enable>> if (readl(dev->bar + NVME_REG_CSTS) == -1) {
+	 *   - drivers/nvme/host/pci.c|2456| <<nvme_dev_disable>> u32 csts = readl(dev->bar + NVME_REG_CSTS);
+	 *   - drivers/nvme/target/fabrics-cmd.c|64| <<nvmet_execute_prop_get>> case NVME_REG_CSTS:
+	 *   - drivers/pci/quirks.c|3855| <<nvme_disable_and_flr>> u32 status = readl(bar + NVME_REG_CSTS);
+	 */
 	NVME_REG_CSTS	= 0x001c,	/* Controller Status */
 	NVME_REG_NSSR	= 0x0020,	/* NVM Subsystem Reset */
 	NVME_REG_AQA	= 0x0024,	/* Admin Queue Attributes */
@@ -840,6 +911,19 @@ enum nvme_admin_opcode {
 	nvme_admin_abort_cmd		= 0x08,
 	nvme_admin_set_features		= 0x09,
 	nvme_admin_get_features		= 0x0a,
+	/*
+	 * 在以下使用nvme_admin_async_event:
+	 *   - drivers/nvme/host/fc.c|1811| <<nvme_fc_init_aen_ops>> sqe->common.opcode = nvme_admin_async_event;
+	 *   - drivers/nvme/host/pci.c|1665| <<nvme_pci_submit_async_event>> c.common.opcode = nvme_admin_async_event;
+	 *   - drivers/nvme/host/rdma.c|1423| <<nvme_rdma_submit_async_event>> cmd->common.opcode = nvme_admin_async_event;
+	 *   - drivers/nvme/host/tcp.c|2029| <<nvme_tcp_submit_async_event>> cmd->common.opcode = nvme_admin_async_event;
+	 *   - drivers/nvme/target/admin-cmd.c|187| <<nvmet_execute_get_log_cmd_effects_ns>> log->acs[nvme_admin_async_event] = cpu_to_le32(1 << 0);
+	 *   - drivers/nvme/target/admin-cmd.c|917| <<nvmet_parse_admin_cmd>> case nvme_admin_async_event:
+	 *   - drivers/nvme/target/discovery.c|366| <<nvmet_parse_discovery_cmd>> case nvme_admin_async_event:
+	 *   - drivers/nvme/target/loop.c|180| <<nvme_loop_submit_async_event>> iod->cmd.common.opcode = nvme_admin_async_event;
+	 *   - drivers/scsi/qla2xxx/qla_nvme.c|403| <<qla2x00_start_nvme_mq>> if (cmd->sqe.common.opcode == nvme_admin_async_event) {
+	 *   - include/linux/nvme.h|899| <<show_admin_opcode_name>> nvme_admin_opcode_name(nvme_admin_async_event), \
+	 */
 	nvme_admin_async_event		= 0x0c,
 	nvme_admin_ns_mgmt		= 0x0d,
 	nvme_admin_activate_fw		= 0x10,
diff --git a/include/linux/pci.h b/include/linux/pci.h
index c393dff2d66f..3c41cd0912d2 100644
--- a/include/linux/pci.h
+++ b/include/linux/pci.h
@@ -922,6 +922,12 @@ enum {
 #define PCI_IRQ_LEGACY		(1 << 0) /* Allow legacy interrupts */
 #define PCI_IRQ_MSI		(1 << 1) /* Allow MSI interrupts */
 #define PCI_IRQ_MSIX		(1 << 2) /* Allow MSI-X interrupts */
+/*
+ * 部分使用PCI_IRQ_AFFINITY的例子:
+ *   - drivers/nvme/host/pci.c|2212| <<nvme_setup_irqs>> PCI_IRQ_ALL_TYPES | PCI_IRQ_AFFINITY, &affd);
+ *   - drivers/pci/msi.c|1211| <<pci_alloc_irq_vectors_affinity>> if (flags & PCI_IRQ_AFFINITY) {
+ *   - drivers/virtio/virtio_pci_common.c|129| <<vp_request_msix_vectors>> flags |= PCI_IRQ_AFFINITY;
+ */
 #define PCI_IRQ_AFFINITY	(1 << 3) /* Auto-assign affinity */
 
 /* These external functions are only available when PCI support is enabled */
diff --git a/include/linux/virtio_config.h b/include/linux/virtio_config.h
index bb4cc4910750..297ba40099f6 100644
--- a/include/linux/virtio_config.h
+++ b/include/linux/virtio_config.h
@@ -165,6 +165,13 @@ static inline bool virtio_has_feature(const struct virtio_device *vdev,
  * virtio_has_iommu_quirk - determine whether this device has the iommu quirk
  * @vdev: the device
  */
+/*
+ * called by:
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|975| <<virtio_gpu_cmd_transfer_to_host_3d>> bool use_dma_api = !virtio_has_iommu_quirk(vgdev->vdev);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|1050| <<virtio_gpu_object_attach>> bool use_dma_api = !virtio_has_iommu_quirk(vgdev->vdev);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|1104| <<virtio_gpu_object_detach>> bool use_dma_api = !virtio_has_iommu_quirk(vgdev->vdev);
+ *   - drivers/virtio/virtio_ring.c|259| <<vring_use_dma_api>> if (!virtio_has_iommu_quirk(vdev))
+ */
 static inline bool virtio_has_iommu_quirk(const struct virtio_device *vdev)
 {
 	/*
diff --git a/include/scsi/scsi_device.h b/include/scsi/scsi_device.h
index 3ed836db5306..acb7bbd956f8 100644
--- a/include/scsi/scsi_device.h
+++ b/include/scsi/scsi_device.h
@@ -106,6 +106,24 @@ struct scsi_device {
 	struct list_head    siblings;   /* list of all devices on this host */
 	struct list_head    same_target_siblings; /* just the devices sharing same target id */
 
+	/*
+	 * 使用device_busy的地方:
+	 *   - drivers/scsi/scsi_sysfs.c|664| <<global>> static DEVICE_ATTR(device_busy, S_IRUGO, sdev_show_device_busy, NULL);
+	 *   - drivers/message/fusion/mptsas.c|3759| <<mptsas_send_link_status_event>> atomic_read(&sdev->device_busy)));
+	 *   - drivers/scsi/megaraid/megaraid_sas_fusion.c|2829| <<megasas_build_ldio_fusion>> atomic_read(&scp->device->device_busy) >
+	 *   - drivers/scsi/megaraid/megaraid_sas_fusion.c|3162| <<megasas_build_syspd_fusion>> atomic_read(&scmd->device->device_busy) > MR_DEVICE_HIGH_IOPS_DEPTH)
+	 *   - drivers/scsi/mpt3sas/mpt3sas_base.c|3488| <<_base_get_high_iops_msix_index>> if (atomic_read(&scmd->device->device_busy) >
+	 *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|2996| <<scsih_dev_reset>> if (r == SUCCESS && atomic_read(&scmd->device->device_busy))
+	 *   - drivers/scsi/scsi_lib.c|357| <<scsi_device_unbusy>> atomic_dec(&sdev->device_busy);
+	 *   - drivers/scsi/scsi_lib.c|413| <<scsi_device_is_busy>> if (atomic_read(&sdev->device_busy) >= sdev->queue_depth)
+	 *   - drivers/scsi/scsi_lib.c|1287| <<scsi_dev_queue_ready>> busy = atomic_inc_return(&sdev->device_busy) - 1;
+	 *   - drivers/scsi/scsi_lib.c|1306| <<scsi_dev_queue_ready>> atomic_dec(&sdev->device_busy);
+	 *   - drivers/scsi/scsi_lib.c|1627| <<scsi_mq_put_budget>> atomic_dec(&sdev->device_busy);
+	 *   - drivers/scsi/scsi_lib.c|1638| <<scsi_mq_get_budget>> if (atomic_read(&sdev->device_busy) == 0 && !scsi_device_blocked(sdev))
+	 *   - drivers/scsi/scsi_lib.c|1709| <<scsi_queue_rq>> if (atomic_read(&sdev->device_busy) ||
+	 *   - drivers/scsi/scsi_sysfs.c|662| <<sdev_show_device_busy>> return snprintf(buf, 20, "%d\n", atomic_read(&sdev->device_busy));
+	 *   - drivers/scsi/sg.c|2510| <<sg_proc_seq_show_dev>> (int ) atomic_read(&scsidp->device_busy),
+	 */
 	atomic_t device_busy;		/* commands actually active on LLDD */
 	atomic_t device_blocked;	/* Device returned QUEUE_FULL. */
 
diff --git a/include/scsi/scsi_host.h b/include/scsi/scsi_host.h
index f577647bf5f2..d99940273777 100644
--- a/include/scsi/scsi_host.h
+++ b/include/scsi/scsi_host.h
@@ -598,6 +598,12 @@ struct Scsi_Host {
 	 * can_queue. In other words, the total queue depth per host
 	 * is nr_hw_queues * can_queue.
 	 */
+	/*
+	 * 修改request_queue->nr_hw_queues的地方:
+	 *   - block/blk-mq.c|3103| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3147| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *   - block/blk-mq.c|3233| <<blk_mq_init_allocated_queue>> q->nr_hw_queues = 0;
+	 */
 	unsigned nr_hw_queues;
 	unsigned active_mode:2;
 	unsigned unchecked_isa_dma:1;
diff --git a/include/uapi/linux/fs.h b/include/uapi/linux/fs.h
index 379a612f8f1d..f1fd79764d02 100644
--- a/include/uapi/linux/fs.h
+++ b/include/uapi/linux/fs.h
@@ -285,6 +285,13 @@ struct fsxattr {
 typedef int __bitwise __kernel_rwf_t;
 
 /* high priority request, poll if possible */
+/*
+ * 在以下使用RWF_HIPRI:
+ *   - fs/overlayfs/file.c|219| <<ovl_iocb_to_rwf>> flags |= RWF_HIPRI;
+ *   - fs/read_write.c|706| <<do_loop_readv_writev>> if (flags & ~RWF_HIPRI)
+ *   - include/linux/fs.h|3426| <<kiocb_set_rw_flags>> if (flags & RWF_HIPRI)
+ *   - include/uapi/linux/fs.h|303| <<RWF_SUPPORTED>> #define RWF_SUPPORTED (RWF_HIPRI | RWF_DSYNC | RWF_SYNC | RWF_NOWAIT |\
+ */
 #define RWF_HIPRI	((__force __kernel_rwf_t)0x00000001)
 
 /* per-IO O_DSYNC */
@@ -300,6 +307,10 @@ typedef int __bitwise __kernel_rwf_t;
 #define RWF_APPEND	((__force __kernel_rwf_t)0x00000010)
 
 /* mask of flags supported by the kernel */
+/*
+ * 在以下使用RWF_SUPPORTED:
+ *   - include/linux/fs.h|3418| <<kiocb_set_rw_flags>> if (unlikely(flags & ~RWF_SUPPORTED))
+ */
 #define RWF_SUPPORTED	(RWF_HIPRI | RWF_DSYNC | RWF_SYNC | RWF_NOWAIT |\
 			 RWF_APPEND)
 
diff --git a/kernel/irq/affinity.c b/kernel/irq/affinity.c
index 4d89ad4fae3b..c1c6a1cf0691 100644
--- a/kernel/irq/affinity.c
+++ b/kernel/irq/affinity.c
@@ -412,6 +412,12 @@ static void default_calc_sets(struct irq_affinity *affd, unsigned int affvecs)
  *
  * Returns the irq_affinity_desc pointer or NULL if allocation failed.
  */
+/*
+ * called by:
+ *   - drivers/pci/msi.c|565| <<msi_setup_entry>> masks = irq_create_affinity_masks(nvec, affd);
+ *   - drivers/pci/msi.c|708| <<msix_setup_entries>> masks = irq_create_affinity_masks(nvec, affd);
+ *   - drivers/pci/msi.c|1250| <<pci_alloc_irq_vectors_affinity>> irq_create_affinity_masks(1, affd);
+ */
 struct irq_affinity_desc *
 irq_create_affinity_masks(unsigned int nvecs, struct irq_affinity *affd)
 {
@@ -437,6 +443,9 @@ irq_create_affinity_masks(unsigned int nvecs, struct irq_affinity *affd)
 		affd->calc_sets = default_calc_sets;
 
 	/* Recalculate the sets */
+	/*
+	 * nvme: nvme_calc_irq_sets()
+	 */
 	affd->calc_sets(affd, affvecs);
 
 	if (WARN_ON_ONCE(affd->nr_sets > IRQ_AFFINITY_MAX_SETS))
@@ -462,6 +471,11 @@ irq_create_affinity_masks(unsigned int nvecs, struct irq_affinity *affd)
 		unsigned int this_vecs = affd->set_size[i];
 		int ret;
 
+		/*
+		 * build affinity in two stages:
+		 *      1) spread present CPU on these vectors
+		 *      2) spread other possible CPUs on these vectors
+		 */
 		ret = irq_build_affinity_masks(curvec, this_vecs,
 					       curvec, masks);
 		if (ret) {
diff --git a/kernel/irq/chip.c b/kernel/irq/chip.c
index b3fa2d87d2f3..e46056543a5f 100644
--- a/kernel/irq/chip.c
+++ b/kernel/irq/chip.c
@@ -190,6 +190,10 @@ enum {
 };
 
 #ifdef CONFIG_SMP
+/*
+ * called by:
+ *   - kernel/irq/chip.c|266| <<irq_startup>> switch (__irq_startup_managed(desc, aff, force)) {
+ */
 static int
 __irq_startup_managed(struct irq_desc *desc, struct cpumask *aff, bool force)
 {
@@ -252,6 +256,14 @@ static int __irq_startup(struct irq_desc *desc)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/chip.c|245| <<__irq_startup>> ret = d->chip->irq_startup(d);
+ *   - kernel/irq/chip.c|299| <<irq_activate_and_startup>> return irq_startup(desc, resend, IRQ_START_FORCE);
+ *   - kernel/irq/cpuhotplug.c|184| <<irq_restore_affinity_of_irq>> irq_startup(desc, IRQ_RESEND, IRQ_START_COND);
+ *   - kernel/irq/manage.c|636| <<__enable_irq>> irq_startup(desc, IRQ_RESEND, IRQ_START_FORCE);
+ *   - kernel/irq/manage.c|1571| <<__setup_irq>> irq_startup(desc, IRQ_RESEND, IRQ_START_COND);
+ */
 int irq_startup(struct irq_desc *desc, bool resend, bool force)
 {
 	struct irq_data *d = irq_desc_get_irq_data(desc);
diff --git a/kernel/irq/irqdesc.c b/kernel/irq/irqdesc.c
index 5b8fdd659e54..4f981e920991 100644
--- a/kernel/irq/irqdesc.c
+++ b/kernel/irq/irqdesc.c
@@ -762,6 +762,17 @@ EXPORT_SYMBOL_GPL(irq_free_descs);
  *
  * Returns the first irq number or error code
  */
+/*
+ * called by:
+ *   - arch/s390/pci/pci_irq.c|279| <<arch_setup_msi_irqs>> irq = __irq_alloc_descs(-1, 0, 1, 0, THIS_MODULE, msi->affinity);
+ *   - arch/sparc/kernel/irq_64.c|245| <<irq_alloc>> irq = __irq_alloc_descs(-1, 1, 1, numa_node_id(), NULL, NULL);
+ *   - include/linux/irq.h|908| <<irq_alloc_descs>> __irq_alloc_descs(irq, from, cnt, node, THIS_MODULE, NULL)
+ *   - kernel/irq/devres.c|189| <<__devm_irq_alloc_descs>> base = __irq_alloc_descs(irq, from, cnt, node, owner, affinity);
+ *   - kernel/irq/irqdesc.c|817| <<irq_alloc_hwirqs>> int i, irq = __irq_alloc_descs(-1, 0, cnt, node, NULL, NULL);
+ *   - kernel/irq/irqdomain.c|1016| <<irq_domain_alloc_descs>> virq = __irq_alloc_descs(virq, virq, cnt, node, THIS_MODULE,
+ *   - kernel/irq/irqdomain.c|1022| <<irq_domain_alloc_descs>> virq = __irq_alloc_descs(-1, hint, cnt, node, THIS_MODULE,
+ *   - kernel/irq/irqdomain.c|1025| <<irq_domain_alloc_descs>> virq = __irq_alloc_descs(-1, 1, cnt, node, THIS_MODULE,
+ */
 int __ref
 __irq_alloc_descs(int irq, unsigned int from, unsigned int cnt, int node,
 		  struct module *owner, const struct irq_affinity_desc *affinity)
diff --git a/kernel/irq/irqdomain.c b/kernel/irq/irqdomain.c
index dd822fd8a7d5..dae331432037 100644
--- a/kernel/irq/irqdomain.c
+++ b/kernel/irq/irqdomain.c
@@ -1645,6 +1645,14 @@ static int __irq_domain_activate_irq(struct irq_data *irqd, bool reserve)
  * This is the second step to call domain_ops->activate to program interrupt
  * controllers, so the interrupt could actually get delivered.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/io_apic.c|2200| <<check_timer>> irq_domain_activate_irq(irq_data, false);
+ *   - arch/x86/kernel/apic/io_apic.c|2222| <<check_timer>> irq_domain_activate_irq(irq_data, false);
+ *   - kernel/irq/chip.c|224| <<__irq_startup_managed>> if (WARN_ON(irq_domain_activate_irq(d, false)))
+ *   - kernel/irq/chip.c|291| <<irq_activate>> return irq_domain_activate_irq(d, false);
+ *   - kernel/irq/msi.c|519| <<msi_domain_alloc_irqs>> ret = irq_domain_activate_irq(irq_data, can_reserve);
+ */
 int irq_domain_activate_irq(struct irq_data *irq_data, bool reserve)
 {
 	int ret = 0;
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index 1753486b440c..d050d3be6278 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -207,6 +207,14 @@ static void irq_validate_effective_affinity(struct irq_data *data)
 #endif
 }
 
+/*
+ * called by:
+ *   - kernel/irq/chip.c|272| <<irq_startup>> irq_do_set_affinity(d, aff, false);
+ *   - kernel/irq/cpuhotplug.c|131| <<migrate_one_irq>> err = irq_do_set_affinity(d, affinity, false);
+ *   - kernel/irq/manage.c|256| <<irq_try_set_affinity>> int ret = irq_do_set_affinity(data, dest, force);
+ *   - kernel/irq/manage.c|435| <<irq_setup_affinity>> ret = irq_do_set_affinity(&desc->irq_data, &mask, false);
+ *   - kernel/irq/migration.c|80| <<irq_move_masked_irq>> ret = irq_do_set_affinity(data, desc->pending_mask, false);
+ */
 int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,
 			bool force)
 {
@@ -217,6 +225,9 @@ int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,
 	if (!chip || !chip->irq_set_affinity)
 		return -EINVAL;
 
+	/*
+	 * msi_domain_set_affinity
+	 */
 	ret = chip->irq_set_affinity(data, mask, force);
 	switch (ret) {
 	case IRQ_SET_MASK_OK:
@@ -396,6 +407,11 @@ EXPORT_SYMBOL_GPL(irq_set_affinity_notifier);
 /*
  * Generic version of the affinity autoselector.
  */
+/*
+ * called by:
+ *   - kernel/irq/chip.c|269| <<irq_startup>> irq_setup_affinity(desc);
+ *   - kernel/irq/manage.c|457| <<irq_select_affinity_usr>> ret = irq_setup_affinity(desc);
+ */
 int irq_setup_affinity(struct irq_desc *desc)
 {
 	struct cpumask *set = irq_default_affinity;
diff --git a/kernel/irq/matrix.c b/kernel/irq/matrix.c
index 30cc217b8631..70b1ae641845 100644
--- a/kernel/irq/matrix.c
+++ b/kernel/irq/matrix.c
@@ -282,6 +282,10 @@ void irq_matrix_remove_managed(struct irq_matrix *m, const struct cpumask *msk)
  * @m:		Matrix pointer
  * @cpu:	On which CPU the interrupt should be allocated
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|319| <<assign_managed_vector>> vector = irq_matrix_alloc_managed(vector_matrix, vector_searchmask,
+ */
 int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,
 			     unsigned int *mapped_cpu)
 {
diff --git a/kernel/irq/msi.c b/kernel/irq/msi.c
index ad26fbcfbfc8..698cc5145d24 100644
--- a/kernel/irq/msi.c
+++ b/kernel/irq/msi.c
@@ -26,6 +26,14 @@
  * If @affinity is not NULL then an affinity array[@nvec] is allocated
  * and the affinity masks and flags from @affinity are copied.
  */
+/*
+ * called by:
+ *   - drivers/base/platform-msi.c|137| <<platform_msi_alloc_descs_with_irq>> desc = alloc_msi_entry(dev, 1, NULL);
+ *   - drivers/bus/fsl-mc/fsl-mc-msi.c|217| <<fsl_mc_msi_alloc_descs>> msi_desc = alloc_msi_entry(dev, 1, NULL);
+ *   - drivers/pci/msi.c|568| <<msi_setup_entry>> entry = alloc_msi_entry(&dev->dev, nvec, masks);
+ *   - drivers/pci/msi.c|707| <<msix_setup_entries>> entry = alloc_msi_entry(&dev->dev, 1, curmsk);
+ *   - drivers/soc/ti/ti_sci_inta_msi.c|81| <<ti_sci_inta_msi_alloc_descs>> msi_desc = alloc_msi_entry(dev, 1, NULL);
+ */
 struct msi_desc *alloc_msi_entry(struct device *dev, int nvec,
 				 const struct irq_affinity_desc *affinity)
 {
@@ -39,6 +47,7 @@ struct msi_desc *alloc_msi_entry(struct device *dev, int nvec,
 	desc->dev = dev;
 	desc->nvec_used = nvec;
 	if (affinity) {
+		/* duplicate region of memory */
 		desc->affinity = kmemdup(affinity,
 			nvec * sizeof(*desc->affinity), GFP_KERNEL);
 		if (!desc->affinity) {
@@ -98,6 +107,28 @@ static void msi_check_level(struct irq_domain *domain, struct msi_msg *msg)
  * Intended to be used by MSI interrupt controllers which are
  * implemented with hierarchical domains.
  */
+/*
+ * qemu下nvme的一个例子:
+ * [0] msi_domain_set_affinity
+ * [0] irq_do_set_affinity
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [    0.682898] orabug: msi_domain_set_affinity() mask=f
+ * [    0.695652] orabug: msi_domain_set_affinity() mask=f
+ * [    0.706281] orabug: msi_domain_set_affinity() mask=1
+ * [    0.715123] orabug: msi_domain_set_affinity() mask=2
+ * [    0.724427] orabug: msi_domain_set_affinity() mask=4
+ * [    0.733339] orabug: msi_domain_set_affinity() mask=8
+ */
 int msi_domain_set_affinity(struct irq_data *irq_data,
 			    const struct cpumask *mask, bool force)
 {
@@ -105,6 +136,18 @@ int msi_domain_set_affinity(struct irq_data *irq_data,
 	struct msi_msg msg[2] = { [1] = { }, };
 	int ret;
 
+	/*
+	 * 对于virtblk和nvme, 到这里的时候:
+	 * parent->chip->name = APIC
+	 * parent->domain->name = VECTOR
+	 *
+	 * static struct irq_chip lapic_controller = {
+	 *	.name                   = "APIC",
+	 *	.irq_ack                = apic_ack_edge,
+	 *	.irq_set_affinity       = apic_set_affinity,
+	 *	.irq_retrigger          = apic_retrigger_irq,
+	 * };
+	 */
 	ret = parent->chip->irq_set_affinity(parent, mask, force);
 	if (ret >= 0 && ret != IRQ_SET_MASK_OK_DONE) {
 		BUG_ON(irq_chip_compose_msi_msg(irq_data, msg));
@@ -396,6 +439,24 @@ static bool msi_check_reservation_mode(struct irq_domain *domain,
  *
  * Returns 0 on success or an error code.
  */
+/*
+ * [0] msi_domain_alloc_irqs
+ * [0] native_setup_msi_irqs
+ * [0] __pci_enable_msix_range
+ * [0] pci_alloc_irq_vectors_affinity
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - arch/x86/kernel/apic/msi.c|116| <<native_setup_msi_irqs>> return msi_domain_alloc_irqs(domain, &dev->dev, nvec);
+ *   - drivers/base/platform-msi.c|265| <<platform_msi_domain_alloc_irqs>> err = msi_domain_alloc_irqs(dev->msi_domain, dev, nvec);
+ *   - drivers/bus/fsl-mc/fsl-mc-msi.c|259| <<fsl_mc_msi_domain_alloc_irqs>> error = msi_domain_alloc_irqs(msi_domain, dev, irq_count);
+ *   - drivers/pci/msi.c|41| <<pci_msi_setup_msi_irqs>> return msi_domain_alloc_irqs(domain, &dev->dev, nvec);
+ *   - drivers/soc/ti/ti_sci_inta_msi.c|115| <<ti_sci_inta_msi_domain_alloc_irqs>> ret = msi_domain_alloc_irqs(msi_domain, dev, nvec);
+ */
 int msi_domain_alloc_irqs(struct irq_domain *domain, struct device *dev,
 			  int nvec)
 {
diff --git a/kernel/irq/proc.c b/kernel/irq/proc.c
index cfc4f088a0e7..6b7c65a98c35 100644
--- a/kernel/irq/proc.c
+++ b/kernel/irq/proc.c
@@ -43,8 +43,34 @@ enum {
 	EFFECTIVE_LIST,
 };
 
+/*
+ * 更新effective_affinity的地方:
+ * [0] irq_data_update_effective_affinity()
+ * [0] apic_update_irq_cfg
+ * [0] assign_managed_vector.constprop.27
+ * [0] x86_vector_activate
+ * [0] __irq_domain_activate_irq
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static int show_irq_affinity(int type, struct seq_file *m)
 {
+	/*
+	 * struct irq_common_data  irq_common_data;
+	 *   -> cpumask_var_t           affinity;
+	 *   -> cpumask_var_t           effective_affinity;
+	 * struct irq_data         irq_data;
+	 */
 	struct irq_desc *desc = irq_to_desc((long)m->private);
 	const struct cpumask *mask;
 
diff --git a/kernel/trace/blktrace.c b/kernel/trace/blktrace.c
index 475e29498bca..2dafe471a164 100644
--- a/kernel/trace/blktrace.c
+++ b/kernel/trace/blktrace.c
@@ -209,6 +209,19 @@ static const u32 ddir_act[2] = { BLK_TC_ACT(BLK_TC_READ),
  * The worker for the various blk_add_trace*() types. Fills out a
  * blk_io_trace structure and places it in a per-cpu subbuffer.
  */
+/*
+ * called by:
+ *   - kernel/trace/blktrace.c|809| <<blk_add_trace_rq>> __blk_add_trace(bt, blk_rq_trace_sector(rq), nr_bytes, req_op(rq),
+ *   - kernel/trace/blktrace.c|861| <<blk_add_trace_bio>> __blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,
+ *   - kernel/trace/blktrace.c|911| <<blk_add_trace_getrq>> __blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_GETRQ, 0, 0,
+ *   - kernel/trace/blktrace.c|927| <<blk_add_trace_sleeprq>> __blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_SLEEPRQ,
+ *   - kernel/trace/blktrace.c|937| <<blk_add_trace_plug>> __blk_add_trace(bt, 0, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL, 0);
+ *   - kernel/trace/blktrace.c|954| <<blk_add_trace_unplug>> __blk_add_trace(bt, 0, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu, 0);
+ *   - kernel/trace/blktrace.c|967| <<blk_add_trace_split>> __blk_add_trace(bt, bio->bi_iter.bi_sector,
+ *   - kernel/trace/blktrace.c|1001| <<blk_add_trace_bio_remap>> __blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,
+ *   - kernel/trace/blktrace.c|1034| <<blk_add_trace_rq_remap>> __blk_add_trace(bt, blk_rq_pos(rq), blk_rq_bytes(rq),
+ *   - kernel/trace/blktrace.c|1059| <<blk_add_driver_data>> __blk_add_trace(bt, blk_rq_trace_sector(rq), blk_rq_bytes(rq), 0, 0,
+ */
 static void __blk_add_trace(struct blk_trace *bt, sector_t sector, int bytes,
 		     int op, int op_flags, u32 what, int error, int pdu_len,
 		     void *pdu_data, u64 cgid)
diff --git a/lib/fault-inject.c b/lib/fault-inject.c
index 8186ca84910b..0f460e99d3eb 100644
--- a/lib/fault-inject.c
+++ b/lib/fault-inject.c
@@ -15,6 +15,18 @@
  * setup_fault_attr() is a helper function for various __setup handlers, so it
  * returns 0 on error, because that is what __setup handlers do.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/iommu.c|82| <<setup_fail_iommu>> return setup_fault_attr(&fail_iommu, str);
+ *   - block/blk-core.c|773| <<setup_fail_make_request>> return setup_fault_attr(&fail_make_request, str);
+ *   - block/blk-timeout.c|19| <<setup_fail_io_timeout>> return setup_fault_attr(&fail_io_timeout, str);
+ *   - drivers/block/null_blk_main.c|2651| <<__null_setup_fault>> if (!setup_fault_attr(attr, str))
+ *   - drivers/mmc/core/debugfs.c|240| <<mmc_add_host_debugfs>> setup_fault_attr(&fail_default_attr, fail_request);
+ *   - drivers/nvme/host/fault_inject.c|26| <<nvme_fault_inject_init>> setup_fault_attr(&fail_default_attr, fail_request);
+ *   - kernel/futex.c|288| <<setup_fail_futex>> return setup_fault_attr(&fail_futex.attr, str);
+ *   - mm/failslab.c|38| <<setup_failslab>> return setup_fault_attr(&failslab.attr, str);
+ *   - mm/page_alloc.c|3327| <<setup_fail_page_alloc>> return setup_fault_attr(&fail_page_alloc.attr, str);
+ */
 int setup_fault_attr(struct fault_attr *attr, char *str)
 {
 	unsigned long probability;
-- 
2.17.1

