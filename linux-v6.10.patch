From f7904eedcdeb2983eea6ad8e20fda4bc2908ab64 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sat, 10 Aug 2024 14:55:33 -0700
Subject: [PATCH 1/1] linux-v6.10

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/kvm_host.h | 185 ++++++++
 arch/x86/include/uapi/asm/kvm.h |   6 +
 arch/x86/kernel/e820.c          |   4 +
 arch/x86/kvm/irq.c              |  11 +
 arch/x86/kvm/irq_comm.c         |  18 +
 arch/x86/kvm/lapic.c            | 676 ++++++++++++++++++++++++++++
 arch/x86/kvm/lapic.h            | 124 ++++++
 arch/x86/kvm/mmu.h              |  26 ++
 arch/x86/kvm/mmu/mmu.c          |  28 ++
 arch/x86/kvm/mmu/mmu_internal.h | 190 ++++++++
 arch/x86/kvm/mmu/spte.c         | 765 ++++++++++++++++++++++++++++++++
 arch/x86/kvm/mmu/spte.h         | 404 +++++++++++++++++
 arch/x86/kvm/mmu/tdp_mmu.c      |   9 +
 arch/x86/kvm/svm/svm.c          |   6 +
 arch/x86/kvm/vmx/posted_intr.c  | 140 ++++++
 arch/x86/kvm/vmx/vmx.c          |  26 ++
 arch/x86/kvm/vmx/vmx.h          |  23 +
 arch/x86/kvm/x86.c              | 153 +++++++
 arch/x86/kvm/x86.h              |   5 +
 drivers/acpi/acpica/acmacros.h  |  22 +
 drivers/acpi/acpica/dsfield.c   |   6 +
 drivers/acpi/acpica/tbxfload.c  |   4 +
 drivers/acpi/acpica/uterror.c   |   4 +
 virt/kvm/irqchip.c              |   3 +
 virt/kvm/kvm_main.c             |  16 +
 25 files changed, 2854 insertions(+)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f8ca74e76..270c8b270 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -108,6 +108,18 @@
 #define KVM_REQ_HV_STIMER		KVM_ARCH_REQ(22)
 #define KVM_REQ_LOAD_EOI_EXITMAP	KVM_ARCH_REQ(23)
 #define KVM_REQ_GET_NESTED_STATE_PAGES	KVM_ARCH_REQ(24)
+/*
+ * 在以下使用KVM_REQ_APICV_UPDATE:
+ *   - arch/x86/kvm/lapic.c|3138| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+ *   - arch/x86/kvm/lapic.c|3452| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+ *   - arch/x86/kvm/svm/nested.c|830| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+ *   - arch/x86/kvm/svm/nested.c|1245| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4924| <<nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10778| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+ *   - arch/x86/kvm/x86.c|11018| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu)) kvm_vcpu_update_apicv(vcpu);
+ *
+ * 处理的函数是kvm_vcpu_update_apicv(vcpu);
+ */
 #define KVM_REQ_APICV_UPDATE \
 	KVM_ARCH_REQ_FLAGS(25, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_TLB_FLUSH_CURRENT	KVM_ARCH_REQ(26)
@@ -146,10 +158,45 @@
 /* KVM Hugepage definitions for x86 */
 #define KVM_MAX_HUGEPAGE_LEVEL	PG_LEVEL_1G
 #define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)
+/*
+ * KVM_HPAGE_GFN_SHIFT(1) : (((1) - 1) * 9) =  0
+ * KVM_HPAGE_GFN_SHIFT(2) : (((2) - 1) * 9) =  9
+ * KVM_HPAGE_GFN_SHIFT(3) : (((3) - 1) * 9) = 18
+ * KVM_HPAGE_GFN_SHIFT(4) : (((4) - 1) * 9) = 27
+ * KVM_HPAGE_GFN_SHIFT(5) : (((5) - 1) * 9) = 36
+ */
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+/*
+ * KVM_HPAGE_SHIFT(1) : 12 +  0 = 12
+ * KVM_HPAGE_SHIFT(2) : 12 +  9 = 21
+ * KVM_HPAGE_SHIFT(3) : 12 + 18 = 30
+ * KVM_HPAGE_SHIFT(4) : 12 + 27 = 39
+ * KVM_HPAGE_SHIFT(5) : 12 + 36 = 48
+ */
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
+/*
+ * KVM_HPAGE_SIZE(1) : 1 << 12 = 4096 (4K)
+ * KVM_HPAGE_SIZE(2) : 1 << 21 = 2097152 (2M)
+ * KVM_HPAGE_SIZE(3) : 1 << 30 = 1073741824 (1G)
+ * KVM_HPAGE_SIZE(4) : 1 << 39 = 549755813888 (512G)
+ * KVM_HPAGE_SIZE(5) : 1 << 48 = 281474976710656 (262144G=256T)
+ */
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
+/*
+ * KVM_HPAGE_MASK(1) : 4K的mask
+ * KVM_HPAGE_MASK(2) : 2M的mask
+ * KVM_HPAGE_MASK(3) : 1G的mask
+ * KVM_HPAGE_MASK(4) : 512G的mask
+ * KVM_HPAGE_MASK(5) : 262144G(256T)的mask
+ */
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
+/*
+ * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+ * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+ * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+ * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+ * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+ */
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
 #define KVM_MEMSLOT_PAGES_TO_MMU_PAGES_RATIO 50
@@ -763,6 +810,22 @@ struct kvm_vcpu_arch {
 	u32 pkru;
 	u32 hflags;
 	u64 efer;
+	/*
+	 * 在以下修改kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|2974| <<kvm_lapic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3289| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|286| <<__kvm_update_cpuid_runtime>> vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|2916| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|2971| <<kvm_lapic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3007| <<kvm_lapic_set_base>> apic->base_address = apic->vcpu->arch.apic_base &
+	 *   - arch/x86/kvm/lapic.c|3448| <<kvm_apic_set_state>> kvm_lapic_set_base(vcpu, vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/lapic.h|225| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|276| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/vmx/nested.c|912| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base & X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|470| <<kvm_get_apic_base>> return vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12644| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
 	bool load_eoi_exitmap_pending;
@@ -1080,12 +1143,56 @@ enum kvm_apic_logical_mode {
 
 struct kvm_apic_map {
 	struct rcu_head rcu;
+	/*
+	 * 在以下设置kvm_apic_map->logical_mode:
+	 *   - arch/x86/kvm/lapic.c|393| <<kvm_recalculate_logical_map>> new->logical_mode = logical_mode;
+	 *   - arch/x86/kvm/lapic.c|395| <<kvm_recalculate_logical_map>> new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
+	 *   - arch/x86/kvm/lapic.c|413| <<kvm_recalculate_logical_map>> new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
+	 *   - arch/x86/kvm/lapic.c|422| <<kvm_recalculate_logical_map>> new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
+	 *   - arch/x86/kvm/lapic.c|520| <<kvm_recalculate_apic_map>> new->logical_mode = KVM_APIC_MODE_SW_DISABLED;
+
+	 * 在以下使用kvm_apic_map->logical_mode:
+	 *   - arch/x86/kvm/lapic.c|230| <<kvm_apic_map_get_logical_dest>> switch (map->logical_mode) {
+	 *   - arch/x86/kvm/lapic.c|368| <<kvm_recalculate_logical_map>> if (new->logical_mode == KVM_APIC_MODE_MAP_DISABLED)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_logical_map>> if (new->logical_mode == KVM_APIC_MODE_SW_DISABLED) {
+	 *   - arch/x86/kvm/lapic.c|394| <<kvm_recalculate_logical_map>> } else if (new->logical_mode != logical_mode) {
+	 *   - arch/x86/kvm/lapic.c|570| <<kvm_recalculate_apic_map>> if (!new || new->logical_mode == KVM_APIC_MODE_MAP_DISABLED)
+	 *   - arch/x86/kvm/lapic.c|1252| <<kvm_apic_is_broadcast_dest>> if ((irq->dest_id == APIC_BROADCAST && map->logical_mode != KVM_APIC_MODE_X2APIC))
+	 */
 	enum kvm_apic_logical_mode logical_mode;
+	/*
+	 * 在以下设置kvm_apic_map->max_apic_id:
+	 *   - arch/x86/kvm/lapic.c|619| <<kvm_recalculate_apic_map>> new->max_apic_id = max_id;
+	 * 在以下使用kvm_apic_map->max_apic_id:
+	 *   - arch/x86/kvm/lapic.c|333| <<kvm_apic_map_get_logical_dest>> u32 max_apic_id = map->max_apic_id;
+	 *   - arch/x86/kvm/lapic.c|338| <<kvm_apic_map_get_logical_dest>> offset = array_index_nospec(offset, map->max_apic_id + 1);
+	 *   - arch/x86/kvm/lapic.c|390| <<kvm_recalculate_phys_map>> if (WARN_ON_ONCE(xapic_id > new->max_apic_id))
+	 *   - arch/x86/kvm/lapic.c|399| <<kvm_recalculate_phys_map>> if (x2apic_id > new->max_apic_id)
+	 *   - arch/x86/kvm/lapic.c|1090| <<__pv_send_ipi>> if (min > map->max_apic_id)
+	 *   - arch/x86/kvm/lapic.c|1094| <<__pv_send_ipi>> min((u32)BITS_PER_LONG, (map->max_apic_id - min + 1))) {
+	 *   - arch/x86/kvm/lapic.c|1478| <<kvm_apic_map_get_dest_lapic>> if (irq->dest_id > map->max_apic_id) {
+	 *   - arch/x86/kvm/lapic.c|1481| <<kvm_apic_map_get_dest_lapic>> u32 dest_id = array_index_nospec(irq->dest_id, map->max_apic_id + 1);
+	 *   - arch/x86/kvm/x86.c|10082| <<kvm_sched_yield>> if (likely(map) && dest_id <= map->max_apic_id && map->phys_map[dest_id])
+	 */
 	u32 max_apic_id;
 	union {
 		struct kvm_lapic *xapic_flat_map[8];
 		struct kvm_lapic *xapic_cluster_map[16][4];
 	};
+	/*
+	 * 在以下使用kvm_apic_map->(*phys_map[]):
+	 *   - arch/x86/kvm/lapic.c|200| <<kvm_apic_map_get_logical_dest>> *cluster = &map->phys_map[offset];
+	 *   - arch/x86/kvm/lapic.c|283| <<kvm_recalculate_phys_map>> new->phys_map[x2apic_id] = apic;
+	 *   - arch/x86/kvm/lapic.c|285| <<kvm_recalculate_phys_map>> if (!apic_x2apic_mode(apic) && !new->phys_map[xapic_id])
+	 *   - arch/x86/kvm/lapic.c|286| <<kvm_recalculate_phys_map>> new->phys_map[xapic_id] = apic;
+	 *   - arch/x86/kvm/lapic.c|298| <<kvm_recalculate_phys_map>> if (new->phys_map[physical_id])
+	 *   - arch/x86/kvm/lapic.c|301| <<kvm_recalculate_phys_map>> new->phys_map[physical_id] = apic;
+	 *   - arch/x86/kvm/lapic.c|856| <<__pv_send_ipi>> if (map->phys_map[min + i]) {
+	 *   - arch/x86/kvm/lapic.c|857| <<__pv_send_ipi>> vcpu = map->phys_map[min + i]->vcpu;
+	 *   - arch/x86/kvm/lapic.c|1177| <<kvm_apic_map_get_dest_lapic>> *dst = &map->phys_map[dest_id];
+	 *   - arch/x86/kvm/x86.c|10043| <<kvm_sched_yield>> if (likely(map) && dest_id <= map->max_apic_id && map->phys_map[dest_id])
+	 *   - arch/x86/kvm/x86.c|10044| <<kvm_sched_yield>> target = map->phys_map[dest_id]->vcpu;
+	 */
 	struct kvm_lapic *phys_map[];
 };
 
@@ -1314,6 +1421,16 @@ struct kvm_arch {
 	 */
 	spinlock_t mmu_unsync_pages_lock;
 
+	/*
+	 * 在以下使用全局的shadow_mmio_value:
+	 *   - arch/x86/kvm/mmu/mmu.c|6360| <<kvm_mmu_init_vm>> kvm->arch.shadow_mmio_value = shadow_mmio_value;
+	 *   - arch/x86/kvm/mmu/spte.c|547| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_value = mmio_value;
+	 * 在以下使用kvm_arch->shadow_mmio_value:
+	 *   - arch/x86/kvm/mmu/mmu.c|6360| <<kvm_mmu_init_vm>> kvm->arch.shadow_mmio_value = shadow_mmio_value;
+	 *   - arch/x86/kvm/mmu/spte.c|187| <<make_mmio_spte>> WARN_ON_ONCE(!vcpu->kvm->arch.shadow_mmio_value);
+	 *   - arch/x86/kvm/mmu/spte.c|190| <<make_mmio_spte>> spte |= vcpu->kvm->arch.shadow_mmio_value | access;
+	 *   - arch/x86/kvm/mmu/spte.h|286| <<is_mmio_spte>> return (spte & shadow_mmio_mask) == kvm->arch.shadow_mmio_value && likely(enable_mmio_caching);
+	 */
 	u64 shadow_mmio_value;
 
 	struct iommu_domain *iommu_domain;
@@ -1327,7 +1444,31 @@ struct kvm_arch {
 	struct kvm_pit *vpit;
 	atomic_t vapics_in_nmi_mode;
 	struct mutex apic_map_lock;
+	/*
+	 * 在以下使用kvm_arch->apic_map(x86):
+	 *   - arch/x86/kvm/lapic.c|490| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map, lockdep_is_held(&kvm->arch.apic_map_lock));
+	 *   - arch/x86/kvm/lapic.c|492| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|891| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1256| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1298| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1433| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10041| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|12779| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	struct kvm_apic_map __rcu *apic_map;
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty(x86):
+	 *   - arch/x86/kvm/lapic.c|397| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|413| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty, DIRTY, UPDATE_IN_PROGRESS) == CLEAN) {
+	 *   - arch/x86/kvm/lapic.c|489| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty, UPDATE_IN_PROGRESS, CLEAN);
+	 *   - arch/x86/kvm/lapic.c|512| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|525| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|531| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|537| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|548| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|2565| <<kvm_lapic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3033| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_t apic_map_dirty;
 
 	bool apic_access_memslot_enabled;
@@ -1335,6 +1476,15 @@ struct kvm_arch {
 
 	/* Protects apicv_inhibit_reasons */
 	struct rw_semaphore apicv_update_lock;
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons(x86):
+	 *   - arch/x86/kvm/x86.c|9996| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10002| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10025| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10679| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10697| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10706| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 */
 	unsigned long apicv_inhibit_reasons;
 
 	gpa_t wall_clock;
@@ -1396,7 +1546,22 @@ struct kvm_arch {
 
 	bool disabled_lapic_found;
 
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq_comm.c|111| <<kvm_set_msi_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq_comm.c|114| <<kvm_set_msi_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq_comm.c|128| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|328| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|3095| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6565| <<kvm_vm_ioctl_enable_cap>> if (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS) kvm->arch.x2apic_format = true;
+	 */
 	bool x2apic_format;
+	/*
+	 * 在以下使用kvm_arch->x2apic_broadcast_quirk_disabled:
+	 *   - arch/x86/kvm/lapic.c|1341| <<kvm_apic_mda>> if (!vcpu->kvm->arch.x2apic_broadcast_quirk_disabled &&
+	 *   - arch/x86/kvm/lapic.c|1421| <<kvm_apic_is_broadcast_dest>> if (kvm->arch.x2apic_broadcast_quirk_disabled) {
+	 *   - arch/x86/kvm/x86.c|6567| <<kvm_vm_ioctl_enable_cap(KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK)>> kvm->arch.x2apic_broadcast_quirk_disabled = true;
+	 */
 	bool x2apic_broadcast_quirk_disabled;
 
 	bool guest_can_read_msr_platform_info;
@@ -2134,12 +2299,32 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 void kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				    enum kvm_apicv_inhibit reason, bool set);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/lapic.c|476| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|481| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|486| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+ *   - arch/x86/kvm/lapic.c|2600| <<kvm_lapic_set_base>> kvm_set_apicv_inhibit(apic->vcpu->kvm, APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+ *   - arch/x86/kvm/svm/sev.c|304| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+ *   - arch/x86/kvm/svm/svm.c|3862| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+ */
 static inline void kvm_set_apicv_inhibit(struct kvm *kvm,
 					 enum kvm_apicv_inhibit reason)
 {
 	kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|314| <<kvm_pit_set_reinject>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/lapic.c|478| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|483| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|488| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+ *   - arch/x86/kvm/svm/svm.c|3217| <<interrupt_window_interception>> kvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+ *   - arch/x86/kvm/x86.c|6553| <<kvm_vm_ioctl_enable_cap>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+ *   - arch/x86/kvm/x86.c|7075| <<kvm_arch_vm_ioctl>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+ */
 static inline void kvm_clear_apicv_inhibit(struct kvm *kvm,
 					   enum kvm_apicv_inhibit reason)
 {
diff --git a/arch/x86/include/uapi/asm/kvm.h b/arch/x86/include/uapi/asm/kvm.h
index 9fae1b73b..d8acaa7c3 100644
--- a/arch/x86/include/uapi/asm/kvm.h
+++ b/arch/x86/include/uapi/asm/kvm.h
@@ -824,6 +824,12 @@ struct kvm_sev_receive_update_data {
 	__u32 pad2;
 };
 
+/*
+ * 在以下使用KVM_X2APIC_API_USE_32BIT_IDS:
+ *   - arch/x86/kvm/x86.c|125| <<KVM_X2APIC_API_VALID_FLAGS>> #define KVM_X2APIC_API_VALID_FLAGS (KVM_X2APIC_API_USE_32BIT_IDS | \
+ *   - arch/x86/kvm/x86.c|6564| <<kvm_vm_ioctl_enable_cap>> if (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS)
+ *   - tools/testing/selftests/kvm/x86_64/xapic_state_test.c|169| <<test_apic_id>> vm_enable_cap(vm, KVM_CAP_X2APIC_API, KVM_X2APIC_API_USE_32BIT_IDS)
+ */
 #define KVM_X2APIC_API_USE_32BIT_IDS            (1ULL << 0)
 #define KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK  (1ULL << 1)
 
diff --git a/arch/x86/kernel/e820.c b/arch/x86/kernel/e820.c
index 68b09f718..7abb95bb8 100644
--- a/arch/x86/kernel/e820.c
+++ b/arch/x86/kernel/e820.c
@@ -91,6 +91,10 @@ static bool _e820__mapped_any(struct e820_table *table,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|671| <<kvm_is_mmio_pfn>> return !e820__mapped_raw_any(pfn_to_hpa(pfn)
+ */
 bool e820__mapped_raw_any(u64 start, u64 end, enum e820_type type)
 {
 	return _e820__mapped_any(e820_table_firmware, start, end, type);
diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index ad9ca8a60..2870f1964 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -79,6 +79,12 @@ int kvm_cpu_has_extint(struct kvm_vcpu *v)
  * interrupt from apic will handled by hardware,
  * we don't need to check it here.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2431| <<svm_set_gif>> if (svm->vcpu.arch.smi_pending || svm->vcpu.arch.nmi_pending || kvm_cpu_has_injectable_intr(&svm->vcpu) || kvm_apic_has_pending_init_or_sipi(&svm->vcpu))
+ *   - arch/x86/kvm/x86.c|10543| <<kvm_check_and_inject_events>> if (kvm_cpu_has_injectable_intr(vcpu)) {
+ *   - arch/x86/kvm/x86.c|10561| <<kvm_check_and_inject_events>> if (kvm_cpu_has_injectable_intr(vcpu))
+ */
 int kvm_cpu_has_injectable_intr(struct kvm_vcpu *v)
 {
 	if (kvm_cpu_has_extint(v))
@@ -135,6 +141,11 @@ static int kvm_cpu_get_extint(struct kvm_vcpu *v)
 /*
  * Read pending interrupt vector and intack.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4937| <<nested_vmx_vmexit>> int irq = kvm_cpu_get_interrupt(vcpu);
+ *   - arch/x86/kvm/x86.c|10544| <<kvm_check_and_inject_events>> int irq = kvm_cpu_get_interrupt(vcpu);
+ */
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v)
 {
 	int vector = kvm_cpu_get_extint(v);
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 68f3f6c26..a67cf688f 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -108,6 +108,15 @@ void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 			       .address_hi = e->msi.address_hi,
 			       .data = e->msi.data };
 
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq_comm.c|111| <<kvm_set_msi_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq_comm.c|114| <<kvm_set_msi_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq_comm.c|128| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|328| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|3095| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6565| <<kvm_vm_ioctl_enable_cap>> if (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS) kvm->arch.x2apic_format = true;
+	 */
 	trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
 			      (u64)msg.address_hi << 32 : 0), msg.data);
 
@@ -125,6 +134,15 @@ EXPORT_SYMBOL_GPL(kvm_set_msi_irq);
 static inline bool kvm_msi_route_invalid(struct kvm *kvm,
 		struct kvm_kernel_irq_routing_entry *e)
 {
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq_comm.c|111| <<kvm_set_msi_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq_comm.c|114| <<kvm_set_msi_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq_comm.c|128| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|328| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|3095| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6565| <<kvm_vm_ioctl_enable_cap>> if (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS) kvm->arch.x2apic_format = true;
+	 */
 	return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
 }
 
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index acd7d4810..4703e844e 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -46,17 +46,95 @@
 #include "hyperv.h"
 #include "smm.h"
 
+/*
+ * enable_apicv=N, vfio通过vfio_msihandler()唤醒irqfd_wakeup()插入中断.
+ */
+
+/*
+ * The x2APIC architecture extends the xAPIC architecture in a backward
+ * compatible manner and provides forward extendability for future Intel
+ * platform innovations. Specifically, the x2APIC architecture does the
+ * following:
+ *
+ * - Retains all key elements of compatibility to the xAPIC architecture:
+ *   1. delivery modes,
+ *   2. interrupt and processor priorities,
+ *   3. interrupt sources,
+ *   4. interrupt destination types;
+ * - Provides extensions to scal processor addressability for both the logical
+ *   and physical destination modes;
+ * - Adds new features to enhance performance of interrupt delivery;
+ * - Reduces complexity of logical destination mode interrupt delivery on link
+ *   based platform architectures/
+ * - Use MSR programming interface to access APIC registers when operating in
+ *   XAPIC mode.
+ *
+ * TPR (Task Priority Register)
+ * 任务优先级寄存器,确定当前CPU能够处理什么优先级别的中断,CPU只处理比TPR中级别更高的中断.
+ * 比它低的中断暂时屏蔽掉,也就是在IRR中继续等到.
+ *
+ * 另外 优先级别=vector/16, vector为每个中断对应的中断向量号.
+ *
+ * PPR (Processor Priority Register)
+ * 处理器优先级寄存器，表示当前正处理的中断的优先级,以此来决定处于IRR中的中断是否发送给CPU.
+ * 处于IRR中的中断只有优先级高于处理器优先级才会被发送给处理器. PPR的值为ISR中正服务的最高
+ * 优先级中断和TPR两者之间选取优先级较大的,所以TPR就是靠间接控制PPR来实现暂时屏蔽比TPR优先
+ * 级小的中断的.
+ */
+/*
+ * pv-eoi (减少vmexit的数量)
+ *
+ * x86 PC体系架构中的中断控制器,早先是8259A,现在更普遍使用的是APIC,他们处理中断的流程遵循如下流程:
+ *
+ * 1. 外部设备产生一个中断,如果该中断没有被屏蔽掉,中断控制器将IRR寄存器中相应的位置1,表示收到中断,但是还未提交给CPU处理.
+ * 2. 中断控制器将该中断提交给CPU,CPU收到中断请求后,会应答中断控制器.
+ * 3. 中断控制器收到CPU的中断应答后,将IRR寄存器中相应的位清0,并将ISR寄存器相应的位置1,表示CPU正在处理该中断.
+ * 4. 当该中断的处理程序结束以前,需要将中断控制器的EOI寄存器对应的位置1,表示CPU完成了对该中断的处理.
+ * 5. 中断控制器收到EOI后,ISR寄存器中相应的位清0，允许下次中断.
+ * 6. 在虚拟化场景中,该流程至少会导致两次VM Exit: 第一次是VMM截获到设备中断的时候,通知客户机退出,将这个中断注入到客户机中;
+ *    另外一次是当客户机操作系统处理完该中断后,写中断控制器的EOI寄存器,这是个MMIO操作,也会导致客户机退出.
+ *    在一个外部IO比较频繁的场景中,外部中断会导致大量的VM Exit,影响客户机的整体性能.
+ *
+ * PV-EOI其实就是通过半虚拟化的办法来优化上述的VM Exit影响,virtio也是使用这个思想来优化网络和磁盘;就EOI的优化来说,其思想本质上很简单:
+ *
+ * 1. 客户机和VMM协商,首先确定双方是否都能支持PV-EOI特性,如果成功,则进一步协商一块2 bytes的内存区间作为双方处理EOI的共享缓存;
+ * 2. 在VMM向客户机注入中断之前,会把缓存的最低位置1,表示客户机不需要通过写EOI寄存器;
+ * 3. 客户机在写EOI之前,如果发现该位被设置,则将该位清0;VMM轮询这个标志位,当检查到清0后,会更新模拟中断控制器中的EOI寄存器;
+ *    如果客户机发现该位未被设置,则继续使用MMIO或者MSR写EOI寄存器;
+ *
+ * 需要注意的是,为了保证客户机和VMM同时处理共享内存的性能和可靠性,目前KVM的PV-EOF方案采用了如下的优化措施:
+ *
+ * 1. VMM保障仅会在客户机VCPU的上下文中更改共享内存中的最低位,从而避免了客户机采用任何锁机制来与VMM进行同步;
+ * 2. 客户机必须使用原子的test_and_clear操作来更改共享内存中的最低位,这是因为VMM在任何时候都有可能设置或者清除该位;
+ */
+
+/*
+ * 在以下使用mod_64():
+ *   - arch/x86/kvm/lapic.c|1600| <<apic_get_tmcct>> ns = mod_64(ktime_to_ns(remaining), apic->lapic_timer.period);
+ */
 #ifndef CONFIG_X86_64
 #define mod_64(x, y) ((x) - (y) * div64_u64(x, y))
 #else
 #define mod_64(x, y) ((x) % (y))
 #endif
 
+/*
+ * 在以下使用APIC_VERSION:
+ *   - arch/x86/kvm/lapic.c|629| <<kvm_apic_set_version>> v = APIC_VERSION | ((apic->nr_lvt_entries - 1) << 16);
+ */
 /* 14 is the version for Xeon and Pentium 8.4.8*/
 #define APIC_VERSION			0x14UL
+/*
+ * 在以下使用LAPIC_MMIO_LENGTH:
+ *   - arch/x86/kvm/lapic.c|1624| <<__apic_read>> if (offset >= LAPIC_MMIO_LENGTH) return 0;
+ *   - arch/x86/kvm/lapic.c|1739| <<apic_mmio_in_range>> return addr >= apic->base_address && addr < apic->base_address + LAPIC_MMIO_LENGTH;
+ */
 #define LAPIC_MMIO_LENGTH		(1 << 12)
 /* followed define is not in apicdef.h */
 #define MAX_APIC_VECTOR			256
+/*
+ * 256 / 32 = 8
+ */
 #define APIC_VECTORS_PER_REG		32
 
 /*
@@ -67,6 +145,10 @@
  * the guest, i.e. so that the interrupt arrives in the guest with minimal
  * latency relative to the deadline programmed by the guest.
  */
+/*
+ * 在以下使用lapic_timer_advance:
+ *   - arch/x86/kvm/lapic.c|2897| <<kvm_create_lapic>> if (lapic_timer_advance)
+ */
 static bool lapic_timer_advance __read_mostly = true;
 module_param(lapic_timer_advance, bool, 0444);
 
@@ -79,6 +161,12 @@ module_param(lapic_timer_advance, bool, 0444);
 static int kvm_lapic_msr_read(struct kvm_lapic *apic, u32 reg, u64 *data);
 static int kvm_lapic_msr_write(struct kvm_lapic *apic, u32 reg, u64 data);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|89| <<kvm_lapic_set_reg>> __kvm_lapic_set_reg(apic->regs, reg_off, val);
+  3 arch/x86/kvm/lapic.c|3035| <<kvm_apic_state_fixup>> __kvm_lapic_set_reg(s->regs, APIC_ICR2, icr >> 32);
+  4 arch/x86/kvm/lapic.c|3050| <<kvm_apic_get_state>> __kvm_lapic_set_reg(s->regs, APIC_TMCCT,
+ */
 static inline void __kvm_lapic_set_reg(char *regs, int reg_off, u32 val)
 {
 	*((u32 *) (regs + reg_off)) = val;
@@ -117,6 +205,13 @@ static inline int apic_test_vector(int vec, void *bitmap)
 	return test_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+ *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+ *   - arch/x86/kvm/ioapic.c|301| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+ *   - arch/x86/kvm/irq_comm.c|437| <<kvm_scan_ioapic_routes>> kvm_apic_pending_eoi(vcpu, irq.vector)))
+ */
 bool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -138,7 +233,48 @@ static inline int __apic_test_and_clear_vector(int vec, void *bitmap)
 __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
 EXPORT_SYMBOL_GPL(kvm_has_noapic_vcpu);
 
+/*
+ * apic_hw_disabled代表HW enabled APIC in APIC_BASE MSR
+ * 比如过去是return (apic)->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+ *
+ * commit c5cc421ba3219b90f11d151bc55f1608c12830fa
+ * Author: Gleb Natapov <gleb@redhat.com>
+ * Date:   Sun Aug 5 15:58:30 2012 +0300
+ *
+ * KVM: use jump label to optimize checking for HW enabled APIC in APIC_BASE MSR
+ *
+ * Usually all APICs are HW enabled so the check can be optimized out.
+ *
+ * Signed-off-by: Gleb Natapov <gleb@redhat.com>
+ * Signed-off-by: Avi Kivity <avi@redhat.com>
+ */
 __read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_hw_disabled, HZ);
+/*
+ * apic_sw_disabled代表SW enabled apic in spurious interrupt register,
+ * 比如过去是return apic_get_reg(apic, APIC_SPIV) & APIC_SPIV_APIC_ENABLED;
+ *
+ * commit f8c1ea103947038b7197bdd4c8451886a58af0c0
+ * Author: Gleb Natapov <gleb@redhat.com>
+ * Date:   Sun Aug 5 15:58:31 2012 +0300
+ *
+ * KVM: use jump label to optimize checking for SW enabled apic in spurious interrupt register
+ *
+ * Usually all APICs are SW enabled so the check can be optimized out.
+ *
+ * Signed-off-by: Gleb Natapov <gleb@redhat.com>
+ * Signed-off-by: Avi Kivity <avi@redhat.com>
+ *
+ * 在以下使用apic_sw_disabled:
+ *   - arch/x86/kvm/lapic.c|263| <<global>> __read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_sw_disabled, HZ);
+ *   - arch/x86/kvm/lapic.h|229| <<global>> extern struct static_key_false_deferred apic_sw_disabled;
+ *   - arch/x86/kvm/lapic.c|837| <<apic_set_spiv>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|839| <<apic_set_spiv>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|2920| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|3290| <<kvm_create_lapic>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|3779| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|3780| <<kvm_lapic_exit>> WARN_ON(static_branch_unlikely(&apic_sw_disabled.key));
+ *   - arch/x86/kvm/lapic.h|255| <<kvm_apic_sw_enabled>> if (static_branch_unlikely(&apic_sw_disabled.key))
+ */
 __read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_sw_disabled, HZ);
 
 static inline int apic_enabled(struct kvm_lapic *apic)
@@ -176,13 +312,39 @@ static bool kvm_use_posted_timer_interrupt(struct kvm_vcpu *vcpu)
 	return kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|389| <<kvm_recalculate_logical_map>> WARN_ON_ONCE(ldr != kvm_apic_calc_x2apic_ldr(kvm_x2apic_id(apic)));
+ *   - arch/x86/kvm/lapic.c|642| <<kvm_apic_set_x2apic_id>> u32 ldr = kvm_apic_calc_x2apic_ldr(id);
+ *   - arch/x86/kvm/lapic.c|3093| <<kvm_apic_state_fixup>> *ldr = kvm_apic_calc_x2apic_ldr(*id);
+ */
 static inline u32 kvm_apic_calc_x2apic_ldr(u32 id)
 {
 	return ((id >> 4) << 16) | (1 << (id & 0xf));
 }
 
+/*
+ * 为了加速对于目标VCPU的查找,在kvm.arch.apic_map中保存了kvm_apic_map结构体.
+ * phys_map成员和logical_map成员记录了RTE的destination filed同VLAPIC结构体的
+ * 对应的关系,分别对应physical mode和logic mode.在发送中断的时候,如果有该map表,
+ * 且中断不是lowest priority和广播,则通过RTE的destination filed就可以直接找到
+ * 目标VCPU，进行快速的分发.否则需要遍历所有的VCPU,逐一的和RTE的destination
+ * field进行匹配.
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|393| <<kvm_recalculate_logical_map>> if (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr, &cluster, &mask))) {
+ *   - arch/x86/kvm/lapic.c|1290| <<kvm_apic_map_get_dest_lapic>> if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst, (u16 *)bitmap))
+ */
 static inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,
 		u32 dest_id, struct kvm_lapic ***cluster, u16 *mask) {
+	/*
+	 * 在以下设置kvm_apic_map->logical_mode:
+	 *   - arch/x86/kvm/lapic.c|393| <<kvm_recalculate_logical_map>> new->logical_mode = logical_mode;
+	 *   - arch/x86/kvm/lapic.c|395| <<kvm_recalculate_logical_map>> new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
+	 *   - arch/x86/kvm/lapic.c|413| <<kvm_recalculate_logical_map>> new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
+	 *   - arch/x86/kvm/lapic.c|422| <<kvm_recalculate_logical_map>> new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
+	 *   - arch/x86/kvm/lapic.c|520| <<kvm_recalculate_apic_map>> new->logical_mode = KVM_APIC_MODE_SW_DISABLED;
+	 */
 	switch (map->logical_mode) {
 	case KVM_APIC_MODE_SW_DISABLED:
 		/* Arbitrarily use the flat map so that @cluster isn't NULL. */
@@ -228,15 +390,41 @@ static void kvm_apic_map_free(struct rcu_head *rcu)
 	kvfree(map);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|474| <<kvm_recalculate_apic_map>> r = kvm_recalculate_phys_map(new, vcpu, &xapic_id_mismatch);
+ *
+ * 初始化kvm_apic_map->phys_map[]
+ */
 static int kvm_recalculate_phys_map(struct kvm_apic_map *new,
 				    struct kvm_vcpu *vcpu,
 				    bool *xapic_id_mismatch)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
+	/*
+	 * 代码: return apic->vcpu->vcpu_id;
+	 */
 	u32 x2apic_id = kvm_x2apic_id(apic);
+	/*
+	 * 代码: return kvm_lapic_get_reg(apic, APIC_ID) >> 24;
+	 */
 	u32 xapic_id = kvm_xapic_id(apic);
 	u32 physical_id;
 
+	/*
+	 * 在以下设置kvm_apic_map->max_apic_id:
+	 *   - arch/x86/kvm/lapic.c|619| <<kvm_recalculate_apic_map>> new->max_apic_id = max_id;
+	 * 在以下使用kvm_apic_map->max_apic_id:
+	 *   - arch/x86/kvm/lapic.c|333| <<kvm_apic_map_get_logical_dest>> u32 max_apic_id = map->max_apic_id;
+	 *   - arch/x86/kvm/lapic.c|338| <<kvm_apic_map_get_logical_dest>> offset = array_index_nospec(offset, map->max_apic_id + 1);
+	 *   - arch/x86/kvm/lapic.c|390| <<kvm_recalculate_phys_map>> if (WARN_ON_ONCE(xapic_id > new->max_apic_id))
+	 *   - arch/x86/kvm/lapic.c|399| <<kvm_recalculate_phys_map>> if (x2apic_id > new->max_apic_id)
+	 *   - arch/x86/kvm/lapic.c|1090| <<__pv_send_ipi>> if (min > map->max_apic_id)
+	 *   - arch/x86/kvm/lapic.c|1094| <<__pv_send_ipi>> min((u32)BITS_PER_LONG, (map->max_apic_id - min + 1))) {
+	 *   - arch/x86/kvm/lapic.c|1478| <<kvm_apic_map_get_dest_lapic>> if (irq->dest_id > map->max_apic_id) {
+	 *   - arch/x86/kvm/lapic.c|1481| <<kvm_apic_map_get_dest_lapic>> u32 dest_id = array_index_nospec(irq->dest_id, map->max_apic_id + 1);
+	 *   - arch/x86/kvm/x86.c|10082| <<kvm_sched_yield>> if (likely(map) && dest_id <= map->max_apic_id && map->phys_map[dest_id])
+	 */
 	/*
 	 * For simplicity, KVM always allocates enough space for all possible
 	 * xAPIC IDs.  Yell, but don't kill the VM, as KVM can continue on
@@ -277,6 +465,15 @@ static int kvm_recalculate_phys_map(struct kvm_apic_map *new,
 	 * manually modified its xAPIC IDs, events targeting that ID are
 	 * supposed to be recognized by all vCPUs with said ID.
 	 */
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq_comm.c|111| <<kvm_set_msi_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq_comm.c|114| <<kvm_set_msi_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq_comm.c|128| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|328| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|3095| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6565| <<kvm_vm_ioctl_enable_cap>> if (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS) kvm->arch.x2apic_format = true;
+	 */
 	if (vcpu->kvm->arch.x2apic_format) {
 		/* See also kvm_apic_match_physical_addr(). */
 		if (apic_x2apic_mode(apic) || x2apic_id > 0xff)
@@ -285,6 +482,12 @@ static int kvm_recalculate_phys_map(struct kvm_apic_map *new,
 		if (!apic_x2apic_mode(apic) && !new->phys_map[xapic_id])
 			new->phys_map[xapic_id] = apic;
 	} else {
+		/*
+		 * 代码: return apic->vcpu->vcpu_id;
+		 * u32 x2apic_id = kvm_x2apic_id(apic);
+		 * 代码: return kvm_lapic_get_reg(apic, APIC_ID) >> 24;
+		 * u32 xapic_id = kvm_xapic_id(apic);
+		 */
 		/*
 		 * Disable the optimized map if the physical APIC ID is already
 		 * mapped, i.e. is aliased to multiple vCPUs.  The optimized
@@ -304,28 +507,78 @@ static int kvm_recalculate_phys_map(struct kvm_apic_map *new,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|486| <<kvm_recalculate_apic_map>> kvm_recalculate_logical_map(new, vcpu);
+ */
 static void kvm_recalculate_logical_map(struct kvm_apic_map *new,
 					struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
+	/*
+	 * enum kvm_apic_logical_mode {
+	 *     // All local APICs are software disabled.
+	 *     KVM_APIC_MODE_SW_DISABLED,
+	 *     // All software enabled local APICs in xAPIC cluster addressing mode.
+	 *     KVM_APIC_MODE_XAPIC_CLUSTER,
+	 *     // All software enabled local APICs in xAPIC flat addressing mode.
+	 *     KVM_APIC_MODE_XAPIC_FLAT,
+	 *     // All software enabled local APICs in x2APIC mode.
+	 *     KVM_APIC_MODE_X2APIC,
+	 *     //
+	 *     // Optimized map disabled, e.g. not all local APICs in the same logical
+	 *     // mode, same logical ID assigned to multiple APICs, etc.
+	 *     //
+	 *     KVM_APIC_MODE_MAP_DISABLED,
+	 * };
+	 */
 	enum kvm_apic_logical_mode logical_mode;
 	struct kvm_lapic **cluster;
 	u16 mask;
 	u32 ldr;
 
+	/*
+	 * 在以下设置kvm_apic_map->logical_mode:
+	 *   - arch/x86/kvm/lapic.c|393| <<kvm_recalculate_logical_map>> new->logical_mode = logical_mode;
+	 *   - arch/x86/kvm/lapic.c|395| <<kvm_recalculate_logical_map>> new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
+	 *   - arch/x86/kvm/lapic.c|413| <<kvm_recalculate_logical_map>> new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
+	 *   - arch/x86/kvm/lapic.c|422| <<kvm_recalculate_logical_map>> new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
+	 *   - arch/x86/kvm/lapic.c|520| <<kvm_recalculate_apic_map>> new->logical_mode = KVM_APIC_MODE_SW_DISABLED;
+	 */
 	if (new->logical_mode == KVM_APIC_MODE_MAP_DISABLED)
 		return;
 
+	/*
+	 * apic_sw_disabled代表SW enabled apic in spurious interrupt register,
+	 * 比如过去是return apic_get_reg(apic, APIC_SPIV) & APIC_SPIV_APIC_ENABLED;
+	 */
 	if (!kvm_apic_sw_enabled(apic))
 		return;
 
+	/*
+	 * 取出LDR:
+	 *
+	 * Bits 0-23     Reserved.
+	 * Bits 24-31
+	 *     Flat model     Bitmap of target processors (each bit identifies single processor; supports a maximum of 8 local APIC units)
+	 *     Cluster model
+	 *         Bits 24-27     Local APIC address (identifies the specific processor in a group)
+	 *         Bits 28-31     Cluster address (identifies a group of processors)
+	 */
 	ldr = kvm_lapic_get_reg(apic, APIC_LDR);
 	if (!ldr)
 		return;
 
 	if (apic_x2apic_mode(apic)) {
+		/*
+		 * 注释: All software enabled local APICs in x2APIC mode.
+		 */
 		logical_mode = KVM_APIC_MODE_X2APIC;
 	} else {
+		/*
+		 * #define         GET_APIC_LOGICAL_ID(x)  (((x) >> 24) & 0xFFu)
+		 * 实际是高8位, 24-31
+		 */
 		ldr = GET_APIC_LOGICAL_ID(ldr);
 		if (kvm_lapic_get_reg(apic, APIC_DFR) == APIC_DFR_FLAT)
 			logical_mode = KVM_APIC_MODE_XAPIC_FLAT;
@@ -340,6 +593,11 @@ static void kvm_recalculate_logical_map(struct kvm_apic_map *new,
 	if (new->logical_mode == KVM_APIC_MODE_SW_DISABLED) {
 		new->logical_mode = logical_mode;
 	} else if (new->logical_mode != logical_mode) {
+		/*
+		 * 注释:
+		 * Optimized map disabled, e.g. not all local APICs in the same logical
+		 * mode, same logical ID assigned to multiple APICs, etc.
+		 */
 		new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
 		return;
 	}
@@ -351,11 +609,34 @@ static void kvm_recalculate_logical_map(struct kvm_apic_map *new,
 	 * reversing the LDR calculation to get cluster of APICs, i.e. no
 	 * additional work is required.
 	 */
+	/*
+	 * 看到一句"The Destination Format Register (DFR) is not supported in x2APIC mode"
+	 */
 	if (apic_x2apic_mode(apic)) {
+		/*
+		 * 在以下使用kvm_apic_calc_x2apic_ldr():
+		 *   - arch/x86/kvm/lapic.c|389| <<kvm_recalculate_logical_map>> WARN_ON_ONCE(ldr != kvm_apic_calc_x2apic_ldr(kvm_x2apic_id(apic)));
+		 *   - arch/x86/kvm/lapic.c|642| <<kvm_apic_set_x2apic_id>> u32 ldr = kvm_apic_calc_x2apic_ldr(id);
+		 *   - arch/x86/kvm/lapic.c|3093| <<kvm_apic_state_fixup>> *ldr = kvm_apic_calc_x2apic_ldr(*id);
+		 */
 		WARN_ON_ONCE(ldr != kvm_apic_calc_x2apic_ldr(kvm_x2apic_id(apic)));
 		return;
 	}
 
+	/*
+	 * 为了加速对于目标VCPU的查找,在kvm.arch.apic_map中保存了kvm_apic_map结构体.
+	 * phys_map成员和logical_map成员记录了RTE的destination filed同VLAPIC结构体的
+	 * 对应的关系,分别对应physical mode和logic mode.在发送中断的时候,如果有该map表,
+	 * 且中断不是lowest priority和广播,则通过RTE的destination filed就可以直接找到
+	 * 目标VCPU，进行快速的分发.否则需要遍历所有的VCPU,逐一的和RTE的destination
+	 * field进行匹配.
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/lapic.c|393| <<kvm_recalculate_logical_map>> if (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr, &cluster, &mask))) {
+	 *   - arch/x86/kvm/lapic.c|1290| <<kvm_apic_map_get_dest_lapic>> if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst, (u16 *)bitmap))
+	 *
+	 * 核心部分
+	 */
 	if (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr,
 							&cluster, &mask))) {
 		new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
@@ -384,6 +665,62 @@ enum {
 	DIRTY
 };
 
+/*
+ * 5.15的例子.
+ *
+ * [0] kvm_recalculate_apic_map
+ * [0] kvm_vcpu_reset
+ * [0] kvm_arch_vcpu_create
+ * [0] kvm_vm_ioctl_create_vcpu
+ * [0] kvm_vm_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] kvm_recalculate_apic_map
+ * [0] kvm_apic_set_state
+ * [0] kvm_arch_vcpu_ioctl
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] kvm_recalculate_apic_map
+ * [0] kvm_lapic_reg_write
+ * [0] handle_apic_write
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] vcpu_run
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] kvm_recalculate_apic_map
+ * [0] kvm_lapic_reg_write
+ * [0] apic_mmio_write
+ * [0] write_mmio
+ * [0] emulator_read_write_onepage
+ * [0] emulator_read_write
+ * [0] x86_emulate_insn
+ * [0] x86_emulate_instruction
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] vcpu_run
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2412| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|2751| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|3028| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|3034| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/x86.c|496| <<kvm_set_apic_base>> kvm_recalculate_apic_map(vcpu->kvm);
+ */
 void kvm_recalculate_apic_map(struct kvm *kvm)
 {
 	struct kvm_apic_map *new, *old = NULL;
@@ -393,6 +730,25 @@ void kvm_recalculate_apic_map(struct kvm *kvm)
 	bool xapic_id_mismatch;
 	int r;
 
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty(x86):
+	 *   - arch/x86/kvm/lapic.c|397| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|413| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty, DIRTY, UPDATE_IN_PROGRESS) == CLEAN) {
+	 *   - arch/x86/kvm/lapic.c|489| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty, UPDATE_IN_PROGRESS, CLEAN);
+	 *   - arch/x86/kvm/lapic.c|512| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|525| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|531| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|537| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|548| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|2565| <<kvm_lapic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3033| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *
+	 * enum {
+	 *     CLEAN,
+	 *     UPDATE_IN_PROGRESS,
+	 *     DIRTY
+	 * };
+	 */
 	/* Read kvm->arch.apic_map_dirty before kvm->arch.apic_map.  */
 	if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
 		return;
@@ -437,6 +793,22 @@ void kvm_recalculate_apic_map(struct kvm *kvm)
 	if (!new)
 		goto out;
 
+	/*
+	 * 在以下设置kvm_apic_map->max_apic_id:
+	 *   - arch/x86/kvm/lapic.c|619| <<kvm_recalculate_apic_map>> new->max_apic_id = max_id;
+	 * 在以下使用kvm_apic_map->max_apic_id:
+	 *   - arch/x86/kvm/lapic.c|333| <<kvm_apic_map_get_logical_dest>> u32 max_apic_id = map->max_apic_id;
+	 *   - arch/x86/kvm/lapic.c|338| <<kvm_apic_map_get_logical_dest>> offset = array_index_nospec(offset, map->max_apic_id + 1);
+	 *   - arch/x86/kvm/lapic.c|390| <<kvm_recalculate_phys_map>> if (WARN_ON_ONCE(xapic_id > new->max_apic_id))
+	 *   - arch/x86/kvm/lapic.c|399| <<kvm_recalculate_phys_map>> if (x2apic_id > new->max_apic_id)
+	 *   - arch/x86/kvm/lapic.c|1090| <<__pv_send_ipi>> if (min > map->max_apic_id)
+	 *   - arch/x86/kvm/lapic.c|1094| <<__pv_send_ipi>> min((u32)BITS_PER_LONG, (map->max_apic_id - min + 1))) {
+	 *   - arch/x86/kvm/lapic.c|1478| <<kvm_apic_map_get_dest_lapic>> if (irq->dest_id > map->max_apic_id) {
+	 *   - arch/x86/kvm/lapic.c|1481| <<kvm_apic_map_get_dest_lapic>> u32 dest_id = array_index_nospec(irq->dest_id, map->max_apic_id + 1);
+	 *   - arch/x86/kvm/x86.c|10082| <<kvm_sched_yield>> if (likely(map) && dest_id <= map->max_apic_id && map->phys_map[dest_id])
+	 *
+	 * 只在此处设置kvm_apic_map->max_apic_id:
+	 */
 	new->max_apic_id = max_id;
 	new->logical_mode = KVM_APIC_MODE_SW_DISABLED;
 
@@ -444,6 +816,12 @@ void kvm_recalculate_apic_map(struct kvm *kvm)
 		if (!kvm_apic_present(vcpu))
 			continue;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/lapic.c|474| <<kvm_recalculate_apic_map>> r = kvm_recalculate_phys_map(new, vcpu, &xapic_id_mismatch);
+		 *
+		 * 初始化kvm_apic_map->phys_map[]
+		 */
 		r = kvm_recalculate_phys_map(new, vcpu, &xapic_id_mismatch);
 		if (r) {
 			kvfree(new);
@@ -456,24 +834,53 @@ void kvm_recalculate_apic_map(struct kvm *kvm)
 			goto out;
 		}
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/lapic.c|486| <<kvm_recalculate_apic_map>> kvm_recalculate_logical_map(new, vcpu);
+		 */
 		kvm_recalculate_logical_map(new, vcpu);
 	}
 out:
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons(x86):
+	 *   - arch/x86/kvm/x86.c|9996| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10002| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10025| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10679| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10697| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10706| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 */
 	/*
 	 * The optimized map is effectively KVM's internal version of APICv,
 	 * and all unwanted aliasing that results in disabling the optimized
 	 * map also applies to APICv.
 	 */
+	/*
+	 * 注释:
+	 * APICv is disabled because not all vCPUs have a 1:1 mapping between
+	 * APIC ID and vCPU, _and_ KVM is not applying its x2APIC hotplug hack.
+	 */
 	if (!new)
 		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
 	else
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
 
+	/*
+	 * 注释:
+	 * AVIC is disabled because not all vCPUs with a valid LDR have a 1:1
+	 * mapping between logical ID and vCPU.
+	 */
 	if (!new || new->logical_mode == KVM_APIC_MODE_MAP_DISABLED)
 		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
 	else
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
 
+	/*
+	 * 注释:
+	 * For simplicity, the APIC acceleration is inhibited
+	 * first time either APIC ID or APIC base are changed by the guest
+	 * from their reset values.
+	 */
 	if (xapic_id_mismatch)
 		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
 	else
@@ -493,9 +900,23 @@ void kvm_recalculate_apic_map(struct kvm *kvm)
 	if (old)
 		call_rcu(&old->rcu, kvm_apic_map_free);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/ioapic.c|313| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/ioapic.c|436| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+	 *   - arch/x86/kvm/ioapic.c|773| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/irq_comm.c|409| <<kvm_arch_post_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/lapic.c|504| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+	 */
 	kvm_make_scan_ioapic_request(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2349| <<kvm_lapic_reg_write(APIC_SPIV)>> apic_set_spiv(apic, val & mask);
+ *   - arch/x86/kvm/lapic.c|2759| <<kvm_lapic_reset>> apic_set_spiv(apic, 0xff);
+ *   - arch/x86/kvm/lapic.c|3065| <<kvm_apic_set_state>> apic_set_spiv(apic, *((u32 *)(s->regs + APIC_SPIV)));
+ */
 static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 {
 	bool enabled = val & APIC_SPIV_APIC_ENABLED;
@@ -625,6 +1046,27 @@ void kvm_apic_after_set_mcg_cap(struct kvm_vcpu *vcpu)
 	kvm_apic_set_version(vcpu);
 }
 
+/*
+ * 320h        LVT Timer Register      Read/Write
+ * 330h        LVT Thermal Sensor Register     Read/Write
+ * 340h        LVT Performance Monitoring Counters Register    Read/Write
+ * 350h        LVT LINT0 Register      Read/Write
+ * 360h        LVT LINT1 Register      Read/Write
+ * 370h        LVT Error Register      Read/Write
+ *
+ *
+ * 32 enum lapic_lvt_entry {
+ * 33         LVT_TIMER,
+ * 34         LVT_THERMAL_MONITOR,
+ * 35         LVT_PERFORMANCE_COUNTER,
+ * 36         LVT_LINT0,
+ * 37         LVT_LINT1,
+ * 38         LVT_ERROR,
+ * 39         LVT_CMCI,
+ * 40
+ * 41         KVM_APIC_MAX_NR_LVT_ENTRIES,
+ * 42 };
+ */
 static const unsigned int apic_lvt_mask[KVM_APIC_MAX_NR_LVT_ENTRIES] = {
 	[LVT_TIMER] = LVT_MASK,      /* timer mode mask added at runtime */
 	[LVT_THERMAL_MONITOR] = LVT_MASK | APIC_MODE_MASK,
@@ -664,6 +1106,11 @@ static u8 count_vectors(void *bitmap)
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1071| <<kvm_apic_update_irr>> bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+ *   - arch/x86/kvm/vmx/nested.c|3908| <<vmx_complete_nested_posted_interrupt>> __kvm_apic_update_irr(vmx->nested.pi_desc->pir, vapic_page, &max_irr);
+ */
 bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 {
 	u32 i, vec;
@@ -700,11 +1147,31 @@ bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 }
 EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6937| <<vmx_sync_pir_to_irr>> kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
+ */
 bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 	bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
 
+	/*
+	 * 在以下设置kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|1079| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|1119| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|1122| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.h|183| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|342| <<avic_kick_vcpu>> vcpu->arch.apic->irr_pending = true;
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|1097| <<int apic_find_highest_irr>> if (!apic->irr_pending) return -1;
+	 *   - arch/x86/kvm/lapic.c|3577| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *
+	 * 在以下设置kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|2919| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/x86.c|10671| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (unlikely(!apic->apicv_active && irr_updated))
 		apic->irr_pending = true;
 	return irr_updated;
@@ -716,6 +1183,14 @@ static inline int apic_search_irr(struct kvm_lapic *apic)
 	return find_highest_vector(apic->regs + APIC_IRR);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1117| <<apic_clear_irr>> apic_find_highest_irr(apic));
+ *   - arch/x86/kvm/lapic.c|1203| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|1326| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+ *   - arch/x86/kvm/lapic.c|3494| <<kvm_apic_set_state>> static_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));
+ *   - arch/x86/kvm/lapic.c|3604| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+ */
 static inline int apic_find_highest_irr(struct kvm_lapic *apic)
 {
 	int result;
@@ -724,6 +1199,18 @@ static inline int apic_find_highest_irr(struct kvm_lapic *apic)
 	 * Note that irr_pending is just a hint. It will be always
 	 * true with virtual interrupt delivery enabled.
 	 */
+	/*
+	 * 在以下设置kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|1079| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|1119| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|1122| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.h|183| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|342| <<avic_kick_vcpu>> vcpu->arch.apic->irr_pending = true;
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|1097| <<int apic_find_highest_irr>> if (!apic->irr_pending) return -1;
+	 *   - arch/x86/kvm/lapic.c|3577| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 */
 	if (!apic->irr_pending)
 		return -1;
 
@@ -735,12 +1222,29 @@ static inline int apic_find_highest_irr(struct kvm_lapic *apic)
 
 static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下设置kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|2919| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/x86.c|10671| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (unlikely(apic->apicv_active)) {
 		/* need to update RVI */
 		kvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);
 		static_call_cond(kvm_x86_hwapic_irr_update)(apic->vcpu,
 							    apic_find_highest_irr(apic));
 	} else {
+		/*
+		 * 在以下设置kvm_lapic->irr_pending:
+		 *   - arch/x86/kvm/lapic.c|1079| <<kvm_apic_update_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.c|1119| <<apic_clear_irr>> apic->irr_pending = false;
+		 *   - arch/x86/kvm/lapic.c|1122| <<apic_clear_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.h|183| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/svm/avic.c|342| <<avic_kick_vcpu>> vcpu->arch.apic->irr_pending = true;
+		 * 在以下使用kvm_lapic->irr_pending:
+		 *   - arch/x86/kvm/lapic.c|1097| <<int apic_find_highest_irr>> if (!apic->irr_pending) return -1;
+		 *   - arch/x86/kvm/lapic.c|3577| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+		 */
 		apic->irr_pending = false;
 		kvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);
 		if (apic_search_irr(apic) != -1)
@@ -818,6 +1322,11 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6939| <<vmx_sync_pir_to_irr>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ *   - arch/x86/kvm/x86.c|10299| <<update_cr8_intercept>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ */
 int kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)
 {
 	/* This may race with setting of irr in __apic_accept_irq() and
@@ -942,9 +1451,17 @@ static bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)
 	return val;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1359| <<apic_update_ppr>> apic_has_interrupt_for_ppr(apic, ppr) != -1)
+ *   - arch/x86/kvm/lapic.c|3340| <<kvm_apic_has_interrupt>> return apic_has_interrupt_for_ppr(apic, ppr);
+ */
 static int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)
 {
 	int highest_irr;
+	/*
+	 * vmx_sync_pir_to_irr()
+	 */
 	if (kvm_x86_ops.sync_pir_to_irr)
 		highest_irr = static_call(kvm_x86_sync_pir_to_irr)(apic->vcpu);
 	else
@@ -976,6 +1493,16 @@ static bool __apic_update_ppr(struct kvm_lapic *apic, u32 *new_ppr)
 	return ppr < old_ppr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1475| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|1482| <<apic_set_tpr>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|2043| <<apic_set_eoi>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|2148| <<__apic_read>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|3304| <<kvm_lapic_reset>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|3515| <<kvm_get_apic_interrupt>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|3604| <<kvm_apic_set_state>> apic_update_ppr(apic);
+ */
 static void apic_update_ppr(struct kvm_lapic *apic)
 {
 	u32 ppr;
@@ -1023,6 +1550,10 @@ static bool kvm_apic_match_physical_addr(struct kvm_lapic *apic, u32 mda)
 	return mda == kvm_xapic_id(apic);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1331| <<kvm_apic_match_dest>> return kvm_apic_match_logical_addr(target, mda);
+ */
 static bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)
 {
 	u32 logical_id;
@@ -1030,14 +1561,39 @@ static bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)
 	if (kvm_apic_broadcast(apic, mda))
 		return true;
 
+	/*
+	 * 取出LDR:
+	 *
+	 * Bits 0-23     Reserved.
+	 * Bits 24-31
+	 *     Flat model     Bitmap of target processors (each bit identifies single processor; supports a maximum of 8 local APIC units)
+	 *     Cluster model
+	 *         Bits 24-27     Local APIC address (identifies the specific processor in a group)
+	 *         Bits 28-31     Cluster address (identifies a group of processors)
+	 */
 	logical_id = kvm_lapic_get_reg(apic, APIC_LDR);
 
+	/*
+	 * 对于x2apic, 这里的logical_id是LDR
+	 *
+	 * 根据x2apic的manual, 似乎高16-bit是cluster, 低16-bit是id
+	 */
 	if (apic_x2apic_mode(apic))
 		return ((logical_id >> 16) == (mda >> 16))
 		       && (logical_id & mda & 0xffff) != 0;
 
 	logical_id = GET_APIC_LOGICAL_ID(logical_id);
 
+	/*
+	 * 注释:
+	 * The DFR (destination format register) specifies Flat or Cluster
+	 * model and is structured as follows
+	 * Bits 0-27	Reserved.
+	 * Bits 28-31	Model (1111b = Flat model, 0000b = Cluster model)
+	 *
+	 * #define         APIC_DFR_CLUSTER                0x0FFFFFFFul
+	 * #define         APIC_DFR_FLAT                   0xFFFFFFFFul
+	 */
 	switch (kvm_lapic_get_reg(apic, APIC_DFR)) {
 	case APIC_DFR_FLAT:
 		return (logical_id & mda) != 0;
@@ -1065,11 +1621,21 @@ static bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)
  * important when userspace wants to use x2APIC-format MSIs, because
  * APIC_BROADCAST (0xff) is a legal route for "cluster 0, CPUs 0-7".
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1373| <<kvm_apic_match_dest>> u32 mda = kvm_apic_mda(vcpu, dest, source, target);
+ */
 static u32 kvm_apic_mda(struct kvm_vcpu *vcpu, unsigned int dest_id,
 		struct kvm_lapic *source, struct kvm_lapic *target)
 {
 	bool ipi = source != NULL;
 
+	/*
+	 * 在以下使用kvm_arch->x2apic_broadcast_quirk_disabled:
+	 *   - arch/x86/kvm/lapic.c|1341| <<kvm_apic_mda>> if (!vcpu->kvm->arch.x2apic_broadcast_quirk_disabled &&
+	 *   - arch/x86/kvm/lapic.c|1421| <<kvm_apic_is_broadcast_dest>> if (kvm->arch.x2apic_broadcast_quirk_disabled) {
+	 *   - arch/x86/kvm/x86.c|6567| <<kvm_vm_ioctl_enable_cap(KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK)>> kvm->arch.x2apic_broadcast_quirk_disabled = true;
+	 */
 	if (!vcpu->kvm->arch.x2apic_broadcast_quirk_disabled &&
 	    !ipi && dest_id == APIC_BROADCAST && apic_x2apic_mode(target))
 		return X2APIC_BROADCAST;
@@ -1077,6 +1643,27 @@ static u32 kvm_apic_mda(struct kvm_vcpu *vcpu, unsigned int dest_id,
 	return dest_id;
 }
 
+/*
+ * shorthand可以是:
+ * - APIC_DEST_NOSHORT
+ * - APIC_DEST_SELF
+ * - APIC_DEST_ALLINC
+ * - APIC_DEST_ALLBUT
+ *
+ * dest_mode可以是:
+ * - APIC_DEST_PHYSICAL
+ * - APIC_DEST_LOGICAL
+ *
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+ *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT, entry->fields.dest_id, entry->fields.dest_mode) ||
+ *   - arch/x86/kvm/ioapic.c|299| <<kvm_ioapic_scan_entry>> if (kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT, e->fields.dest_id, dm)
+ *   - arch/x86/kvm/irq_comm.c|70| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu, src, irq->shorthand, irq->dest_id, irq->dest_mode))
+ *   - arch/x86/kvm/irq_comm.c|352| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu, NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+ *   - arch/x86/kvm/irq_comm.c|435| <<kvm_scan_ioapic_routes>> (kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT, irq.dest_id, irq.dest_mode) ||
+ *   - arch/x86/kvm/lapic.c|1679| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu, NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+ *   - arch/x86/kvm/svm/avic.c|484| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu, source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+ */
 bool kvm_apic_match_dest(struct kvm_vcpu *vcpu, struct kvm_lapic *source,
 			   int shorthand, unsigned int dest, int dest_mode)
 {
@@ -1152,6 +1739,12 @@ static bool kvm_apic_is_broadcast_dest(struct kvm *kvm, struct kvm_lapic **src,
  * means that the interrupt should be dropped.  In this case, *bitmap would be
  * zero and *dst undefined.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1244| <<kvm_irq_delivery_to_apic_fast>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
+ *   - arch/x86/kvm/lapic.c|1286| <<kvm_intr_is_single_vcpu_fast>> if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
+ *   - arch/x86/kvm/lapic.c|1421| <<kvm_bitmap_or_dest_vcpus>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
+ */
 static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 		struct kvm_lapic **src, struct kvm_lapic_irq *irq,
 		struct kvm_apic_map *map, struct kvm_lapic ***dst,
@@ -2540,6 +3133,12 @@ u64 kvm_lapic_get_cr8(struct kvm_vcpu *vcpu)
 	return (tpr & 0xf0) >> 4;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|3267| <<kvm_lapic_reset>> kvm_lapic_set_base(vcpu, msr_val);
+ *   - arch/x86/kvm/lapic.c|3599| <<kvm_apic_set_state>> kvm_lapic_set_base(vcpu, vcpu->arch.apic_base);
+ *   - arch/x86/kvm/x86.c|495| <<kvm_set_apic_base>> kvm_lapic_set_base(vcpu, msr_info->data);
+ */
 void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 {
 	u64 old_value = vcpu->arch.apic_base;
@@ -2574,6 +3173,18 @@ void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 	}
 
 	if ((old_value ^ value) & (MSR_IA32_APICBASE_ENABLE | X2APIC_ENABLE)) {
+		/*
+		 * 在以下使用KVM_REQ_APICV_UPDATE:
+		 *   - arch/x86/kvm/lapic.c|3138| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/lapic.c|3452| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|830| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1245| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|4924| <<nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10778| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+		 *   - arch/x86/kvm/x86.c|11018| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu)) kvm_vcpu_update_apicv(vcpu);
+		 *
+		 * 处理的函数是kvm_vcpu_update_apicv(vcpu);
+		 */
 		kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
 		static_call_cond(kvm_x86_set_virtual_apic_mode)(vcpu);
 	}
@@ -2593,6 +3204,18 @@ void kvm_apic_update_apicv(struct kvm_vcpu *vcpu)
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	if (apic->apicv_active) {
+		/*
+		 * 在以下设置kvm_lapic->irr_pending:
+		 *   - arch/x86/kvm/lapic.c|1079| <<kvm_apic_update_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.c|1119| <<apic_clear_irr>> apic->irr_pending = false;
+		 *   - arch/x86/kvm/lapic.c|1122| <<apic_clear_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.h|183| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/svm/avic.c|342| <<avic_kick_vcpu>> vcpu->arch.apic->irr_pending = true;
+		 * 在以下使用kvm_lapic->irr_pending:
+		 *   - arch/x86/kvm/lapic.c|1097| <<int apic_find_highest_irr>> if (!apic->irr_pending) return -1;
+		 *   - arch/x86/kvm/lapic.c|3577| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+		 */
 		/* irr_pending is always true when apicv is activated. */
 		apic->irr_pending = true;
 		apic->isr_count = 1;
@@ -2876,6 +3499,18 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 	 */
 	if (enable_apicv) {
 		apic->apicv_active = true;
+		/*
+		 * 在以下使用KVM_REQ_APICV_UPDATE:
+		 *   - arch/x86/kvm/lapic.c|3138| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/lapic.c|3452| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|830| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1245| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|4924| <<nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10778| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+		 *   - arch/x86/kvm/x86.c|11018| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu)) kvm_vcpu_update_apicv(vcpu);
+		 *
+		 * 处理的函数是kvm_vcpu_update_apicv(vcpu);
+		 */
 		kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
 	}
 
@@ -2887,6 +3522,13 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|96| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+ *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+ *   - arch/x86/kvm/lapic.c|3534| <<kvm_get_apic_interrupt>> int vector = kvm_apic_has_interrupt(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3703| <<nested_vmx_run>> kvm_apic_has_interrupt(vcpu) == vmx->nested.posted_intr_nv) {
+ */
 int kvm_apic_has_interrupt(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2922,6 +3564,10 @@ void kvm_inject_apic_timer_irqs(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|149| <<kvm_cpu_get_interrupt>> return kvm_get_apic_interrupt(v);
+ */
 int kvm_get_apic_interrupt(struct kvm_vcpu *vcpu)
 {
 	int vector = kvm_apic_has_interrupt(vcpu);
@@ -2968,6 +3614,15 @@ static int kvm_apic_state_fixup(struct kvm_vcpu *vcpu,
 		u32 *ldr = (u32 *)(s->regs + APIC_LDR);
 		u64 icr;
 
+		/*
+		 * 在以下使用kvm_arch->x2apic_format:
+		 *   - arch/x86/kvm/irq_comm.c|111| <<kvm_set_msi_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+		 *   - arch/x86/kvm/irq_comm.c|114| <<kvm_set_msi_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+		 *   - arch/x86/kvm/irq_comm.c|128| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+		 *   - arch/x86/kvm/lapic.c|328| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/lapic.c|3095| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/x86.c|6565| <<kvm_vm_ioctl_enable_cap>> if (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS) kvm->arch.x2apic_format = true;
+		 */
 		if (vcpu->kvm->arch.x2apic_format) {
 			if (*id != vcpu->vcpu_id)
 				return -EINVAL;
@@ -3043,6 +3698,11 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	__start_apic_timer(apic, APIC_TMCCT);
 	kvm_lapic_set_reg(apic, APIC_TMCCT, 0);
 	kvm_apic_update_apicv(vcpu);
+	/*
+	 * 在以下设置kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|2919| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/x86.c|10671| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (apic->apicv_active) {
 		static_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);
 		static_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));
@@ -3057,6 +3717,10 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|169| <<__kvm_migrate_timers>> __kvm_migrate_apic_timer(vcpu);
+ */
 void __kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)
 {
 	struct hrtimer *timer;
@@ -3126,6 +3790,18 @@ void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)
 static void apic_sync_pv_eoi_to_guest(struct kvm_vcpu *vcpu,
 					struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下设置kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|1079| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|1119| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|1122| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.h|183| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|342| <<avic_kick_vcpu>> vcpu->arch.apic->irr_pending = true;
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|1097| <<int apic_find_highest_irr>> if (!apic->irr_pending) return -1;
+	 *   - arch/x86/kvm/lapic.c|3577| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 */
 	if (!pv_eoi_enabled(vcpu) ||
 	    /* IRR set or many bits in ISR: could be nested. */
 	    apic->irr_pending ||
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index a69e706b9..95c702583 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -62,8 +62,32 @@ struct kvm_lapic {
 	struct kvm_timer lapic_timer;
 	u32 divide_count;
 	struct kvm_vcpu *vcpu;
+	/*
+	 * 在以下设置kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|2919| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/x86.c|10671| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	bool apicv_active;
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|540| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|541| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2534| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|207| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	bool sw_enabled;
+	/*
+	 * 在以下设置kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|1079| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|1119| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|1122| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.h|183| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|342| <<avic_kick_vcpu>> vcpu->arch.apic->irr_pending = true;
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|1097| <<int apic_find_highest_irr>> if (!apic->irr_pending) return -1;
+	 *   - arch/x86/kvm/lapic.c|3577| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 */
 	bool irr_pending;
 	bool lvt0_in_nmi_mode;
 	/* Number of bits set in ISR. */
@@ -161,9 +185,26 @@ static inline void kvm_lapic_set_vector(int vec, void *bitmap)
 	set_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3695| <<svm_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+ *   - arch/x86/kvm/vmx/vmx.c|4286| <<vmx_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+ */
 static inline void kvm_lapic_set_irr(int vec, struct kvm_lapic *apic)
 {
 	kvm_lapic_set_vector(vec, apic->regs + APIC_IRR);
+	/*
+	 * 在以下设置kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|1079| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|1119| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|1122| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.h|183| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|342| <<avic_kick_vcpu>> vcpu->arch.apic->irr_pending = true;
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|1097| <<int apic_find_highest_irr>> if (!apic->irr_pending) return -1;
+	 *   - arch/x86/kvm/lapic.c|3577| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 */
 	/*
 	 * irr_pending must be true if any interrupt is pending; set it after
 	 * APIC_IRR to avoid race with apic_clear_irr
@@ -192,8 +233,39 @@ static inline bool lapic_in_kernel(struct kvm_vcpu *vcpu)
 
 extern struct static_key_false_deferred apic_hw_disabled;
 
+/*
+ * apic_hw_disabled代表HW enabled APIC in APIC_BASE MSR
+ * 比如过去是return (apic)->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+ *
+ * commit c5cc421ba3219b90f11d151bc55f1608c12830fa
+ * Author: Gleb Natapov <gleb@redhat.com>
+ * Date:   Sun Aug 5 15:58:30 2012 +0300
+ *
+ * KVM: use jump label to optimize checking for HW enabled APIC in APIC_BASE MSR
+ *
+ * Usually all APICs are HW enabled so the check can be optimized out.
+ *
+ * Signed-off-by: Gleb Natapov <gleb@redhat.com>
+ * Signed-off-by: Avi Kivity <avi@redhat.com>
+ */
 static inline bool kvm_apic_hw_enabled(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下修改kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|2974| <<kvm_lapic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3289| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|286| <<__kvm_update_cpuid_runtime>> vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|2916| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|2971| <<kvm_lapic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3007| <<kvm_lapic_set_base>> apic->base_address = apic->vcpu->arch.apic_base &
+	 *   - arch/x86/kvm/lapic.c|3448| <<kvm_apic_set_state>> kvm_lapic_set_base(vcpu, vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/lapic.h|225| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|276| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/vmx/nested.c|912| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base & X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|470| <<kvm_get_apic_base>> return vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12644| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	if (static_branch_unlikely(&apic_hw_disabled.key))
 		return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
 	return true;
@@ -201,8 +273,30 @@ static inline bool kvm_apic_hw_enabled(struct kvm_lapic *apic)
 
 extern struct static_key_false_deferred apic_sw_disabled;
 
+/*
+ * apic_sw_disabled代表SW enabled apic in spurious interrupt register,
+ * 比如过去是return apic_get_reg(apic, APIC_SPIV) & APIC_SPIV_APIC_ENABLED;
+ *
+ * commit f8c1ea103947038b7197bdd4c8451886a58af0c0
+ * Author: Gleb Natapov <gleb@redhat.com>
+ * Date:   Sun Aug 5 15:58:31 2012 +0300
+ *
+ * KVM: use jump label to optimize checking for SW enabled apic in spurious interrupt register
+ *
+ * Usually all APICs are SW enabled so the check can be optimized out.
+ *
+ * Signed-off-by: Gleb Natapov <gleb@redhat.com>
+ * Signed-off-by: Avi Kivity <avi@redhat.com>
+ */
 static inline bool kvm_apic_sw_enabled(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|540| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|541| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2534| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|207| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (static_branch_unlikely(&apic_sw_disabled.key))
 		return apic->sw_enabled;
 	return true;
@@ -210,6 +304,10 @@ static inline bool kvm_apic_sw_enabled(struct kvm_lapic *apic)
 
 static inline bool kvm_apic_present(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * apic_hw_disabled代表HW enabled APIC in APIC_BASE MSR
+	 * 比如过去是return (apic)->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 */
 	return lapic_in_kernel(vcpu) && kvm_apic_hw_enabled(vcpu->arch.apic);
 }
 
@@ -223,6 +321,32 @@ static inline int apic_x2apic_mode(struct kvm_lapic *apic)
 	return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|87| <<kvm_cpu_has_injectable_intr>> if (!is_guest_mode(v) && kvm_vcpu_apicv_active(v))
+ *   - arch/x86/kvm/lapic.c|299| <<kvm_can_post_timer_interrupt>> return pi_inject_timer && kvm_vcpu_apicv_active(vcpu) &&
+ *   - arch/x86/kvm/svm/avic.c|933| <<avic_pi_update_irte>> kvm_vcpu_apicv_active(&svm->vcpu)) {
+ *   - arch/x86/kvm/svm/avic.c|1120| <<avic_refresh_virtual_apic_mode>> if (kvm_vcpu_apicv_active(vcpu)) {
+ *   - arch/x86/kvm/svm/avic.c|1138| <<avic_refresh_apicv_exec_ctrl>> bool activated = kvm_vcpu_apicv_active(vcpu);
+ *   - arch/x86/kvm/svm/avic.c|1155| <<avic_vcpu_blocking>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/avic.c|1176| <<avic_vcpu_unblocking>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/nested.c|829| <<enter_svm_guest_mode>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1246| <<init_vmcb>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1360| <<init_vmcb>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1560| <<svm_vcpu_load>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1566| <<svm_vcpu_put>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|4004| <<sync_lapic_to_cr8>> kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|3479| <<nested_vmx_enter_non_root_mode>> if (likely(!evaluate_pending_interrupts) && kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|4055| <<vmx_update_msr_bitmap_x2apic>> if (enable_apicv && kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|4388| <<vmx_pin_based_exec_ctrl>> if (!kvm_vcpu_apicv_active(&vmx->vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|4454| <<vmx_refresh_apicv_exec_ctrl>> if (kvm_vcpu_apicv_active(vcpu)) {
+ *   - arch/x86/kvm/vmx/vmx.c|4523| <<vmx_tertiary_exec_control>> if (!enable_ipiv || !kvm_vcpu_apicv_active(&vmx->vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|4616| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|6972| <<vmx_sync_pir_to_irr>> if (!is_guest_mode(vcpu) && kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|6982| <<vmx_load_eoi_exitmap>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/x86.c|11164| <<vcpu_enter_guest>> WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
+ *   - arch/x86/kvm/x86.c|13284| <<kvm_arch_dy_has_pending_interrupt>> return kvm_vcpu_apicv_active(vcpu) &&
+ */
 static inline bool kvm_vcpu_apicv_active(struct kvm_vcpu *vcpu)
 {
 	return lapic_in_kernel(vcpu) && vcpu->arch.apic->apicv_active;
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 2e454316f..3b91c7293 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -81,8 +81,27 @@ static inline gfn_t kvm_mmu_max_gfn(void)
 	return (1ULL << (max_gpa_bits - PAGE_SHIFT)) - 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|1039| <<kvm_mmu_reset_all_pte_masks>> shadow_phys_bits = kvm_get_shadow_phys_bits();
+ *   - arch/x86/kvm/vmx/vmx.c|8434| <<vmx_setup_me_spte_mask>> if (boot_cpu_data.x86_phys_bits != kvm_get_shadow_phys_bits())
+ *   - arch/x86/kvm/vmx/vmx.c|8436| <<vmx_setup_me_spte_mask>> me_mask = rsvd_bits(boot_cpu_data.x86_phys_bits, kvm_get_shadow_phys_bits() - 1);
+ *   - arch/x86/kvm/vmx/vmx.h|753| <<vmx_need_pf_intercept>> return allow_smaller_maxphyaddr && cpuid_maxphyaddr(vcpu) < kvm_get_shadow_phys_bits()
+ */
 static inline u8 kvm_get_shadow_phys_bits(void)
 {
+	/*
+	 * # cpuid -l 0x80000008 -1
+         * CPU:
+         *    Physical Address and Linear Address Size (0x80000008/eax):
+         *       maximum physical address bits         = 0x2e (46)
+         *       maximum linear (virtual) address bits = 0x30 (48)
+         *       maximum guest physical address bits   = 0x0 (0)
+	 *
+	 * # cpuid -l 0x80000008 -1 -r
+         * CPU:
+         *    0x80000008 0x00: eax=0x0000302e ebx=0x00000000 ecx=0x00000000 edx=0x00000000
+	 */
 	/*
 	 * boot_cpu_data.x86_phys_bits is reduced when MKTME or SME are detected
 	 * in CPU detection code, but the processor treats those reduced bits as
@@ -311,6 +330,13 @@ static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u64 access,
 			   struct x86_exception *exception);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4082| <<nonpaging_gva_to_gpa>> return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|379| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn), nested_access, &walker->fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|448| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(gfn), access, &walker->fault);
+ *   - arch/x86/kvm/x86.c|873| <<load_pdptrs>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn), PFERR_USER_MASK | PFERR_WRITE_MASK, NULL);
+ */
 static inline gpa_t kvm_translate_gpa(struct kvm_vcpu *vcpu,
 				      struct kvm_mmu *mmu,
 				      gpa_t gpa, u64 access,
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 8d74bdef6..d297bbd06 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -289,6 +289,11 @@ static void kvm_flush_remote_tlbs_sptep(struct kvm *kvm, u64 *sptep)
 	kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2885| <<mmu_set_spte>> mark_mmio_spte(vcpu, sptep, gfn, pte_access);
+ *   - arch/x86/kvm/mmu/mmu.c|4823| <<sync_mmio_spte>> mark_mmio_spte(vcpu, sptep, gfn, access);
+ */
 static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 			   unsigned int access)
 {
@@ -2863,6 +2868,13 @@ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2962| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, slot, start, access, gfn, page_to_pfn(pages[i]), NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|3237| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL, base_gfn, fault->pfn, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|556| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, slot, spte, pte_access, gfn, pfn, NULL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|751| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access, base_gfn, fault->pfn, fault);
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			u64 *sptep, unsigned int pte_access, gfn_t gfn,
 			kvm_pfn_t pfn, struct kvm_page_fault *fault)
@@ -3268,6 +3280,12 @@ static int kvm_handle_error_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fa
 	return -EFAULT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4384| <<kvm_faultin_pfn>> return kvm_handle_noslot_fault(vcpu, fault, access);
+ *   - arch/x86/kvm/mmu/mmu.c|4407| <<kvm_faultin_pfn>> return kvm_handle_noslot_fault(vcpu, fault, access);
+ *   - arch/x86/kvm/mmu/mmu.c|4451| <<kvm_faultin_pfn>> return kvm_handle_noslot_fault(vcpu, fault, access);
+ */
 static int kvm_handle_noslot_fault(struct kvm_vcpu *vcpu,
 				   struct kvm_page_fault *fault,
 				   unsigned int access)
@@ -4351,6 +4369,12 @@ static int __kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	return RET_PF_CONTINUE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4520| <<direct_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|4610| <<kvm_tdp_mmu_page_fault>> r = kvm_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|815| <<FNAME(page_fault)>> r = kvm_faultin_pfn(vcpu, fault, walker.pte_access);
+ */
 static int kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 			   unsigned int access)
 {
@@ -7073,6 +7097,10 @@ void __init kvm_mmu_x86_module_init(void)
  * loaded as many of the masks/values may be modified by VMX or SVM, i.e. need
  * to be reset when a potentially different vendor module is loaded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9780| <<kvm_x86_vendor_init>> r = kvm_mmu_vendor_module_init();
+ */
 int kvm_mmu_vendor_module_init(void)
 {
 	int ret = -ENOMEM;
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index ce2fcd19b..8995688b3 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -37,11 +37,23 @@
 #define INVALID_PAE_ROOT	0
 #define IS_VALID_PAE_ROOT(x)	(!!(x))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3809| <<mmu_alloc_shadow_roots>> mmu->root.hpa = kvm_mmu_get_dummy_root();
+ */
 static inline hpa_t kvm_mmu_get_dummy_root(void)
 {
 	return my_zero_pfn(0) << PAGE_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3603| <<kvm_mmu_free_roots>> if (kvm_mmu_is_dummy_root(mmu->root.hpa)) {
+ *   - arch/x86/kvm/mmu/mmu.c|3978| <<is_unsync_root>> if (!VALID_PAGE(root) || kvm_mmu_is_dummy_root(root))
+ *   - arch/x86/kvm/mmu/mmu.c|4506| <<direct_page_fault>> if (WARN_ON_ONCE(kvm_mmu_is_dummy_root(vcpu->arch.mmu->root.hpa)))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|660| <<FNAME(fetch)>> if (unlikely(kvm_mmu_is_dummy_root(vcpu->arch.mmu->root.hpa))) {
+ *   - arch/x86/kvm/mmu/spte.h|365| <<root_to_sp>> if (kvm_mmu_is_dummy_root(root))
+ */
 static inline bool kvm_mmu_is_dummy_root(hpa_t shadow_page)
 {
 	return is_zero_pfn(shadow_page >> PAGE_SHIFT);
@@ -49,6 +61,122 @@ static inline bool kvm_mmu_is_dummy_root(hpa_t shadow_page)
 
 typedef u64 __rcu *tdp_ptep_t;
 
+/*
+ * 特别旧的数据
+ *
+ * The root_hpa for vcpu 1 is at 132134662144 = 0x1ec3d6f000. Therefore, the
+ * 'struct page' of root_hpa is fffffc1a7b0f5bc0.
+ *
+ * 30 crash> kmem 1ec3d6f000
+ * 31       PAGE         PHYSICAL      MAPPING       INDEX CNT FLAGS
+ * 32 fffffc1a7b0f5bc0 1ec3d6f000                0        0  1 17ffffc0000000
+ *
+ * The root kvm_mmu_page is at
+ *
+ * 37 crash> page.private fffffc1a7b0f5bc0
+ * 38     private = 18446616489625135080 --> ffff8bf683f827e8
+ *
+ * 50 crash> kvm_mmu_page ffff8bf683f827e8
+ * 51 struct kvm_mmu_page {
+ * 52   link = {
+ * 53     next = 0xffffa62460049a70,
+ * 54     prev = 0xffff8bf683f82730
+ * 55   },
+ * 56   hash_link = {
+ * 57     next = 0x0,
+ * 58     pprev = 0xffff8bf683f82740
+ * 59   },
+ * 60   lpage_disallowed_link = {
+ * 61     next = 0x0,
+ * 62     prev = 0x0
+ * 63   },
+ * 64   gfn = 0,
+ * 65   role = {
+ * 66     word = 1924,
+ * 67     {
+ * 68       level = 4,
+ * 69       cr4_pae = 0,
+ * 70       quadrant = 0,
+ * 71       direct = 1,
+ * 72       access = 7,
+ * 73       invalid = 0,
+ * 74       nxe = 0,
+ * 75       cr0_wp = 0,
+ * 76       smep_andnot_wp = 0,
+ * 77       smap_andnot_wp = 0,
+ * 78       ad_disabled = 0,
+ * 79       guest_mode = 0,
+ * 80       smm = 0
+ * 81     }
+ * 82   },
+ * 83   spt = 0xffff8bf683d6f000,
+ * 84   gfns = 0x0,
+ * 85   lpage_disallowed = false,
+ * 86   unsync = false,
+ * 87   root_count = 8,
+ * 88   unsync_children = 0,
+ * 89   parent_ptes = {
+ * 90     val = 0
+ * 91   },
+ * 92   mmu_valid_gen = 47,
+ * 93   unsync_child_bitmap = {0, 0, 0, 0, 0, 0, 0, 0},
+ * 94   write_flooding_count = {
+ * 95     counter = 0
+ * 96   }
+ * 97 }
+ *
+ * 另外一个level 1的page
+ *
+ * 105 crash> kvm_mmu_page ffff8bf683f72c38
+ * 106 struct kvm_mmu_page {
+ * 107   link = {
+ * 108     next = 0xffff8bf683f72cf0,
+ * 109     prev = 0xffff8bf683f72b80
+ * 110   },
+ * 111   hash_link = {
+ * 112     next = 0x0,
+ * 113     pprev = 0xffffa62460048270
+ * 114   },
+ * 115   lpage_disallowed_link = {
+ * 116     next = 0xffff8bf683f72d10,
+ * 117     prev = 0xffff8bf683f72ba0
+ * 118   },
+ * 119   gfn = 4365312,
+ * 120   role = {
+ * 121     word = 1921,
+ * 122     {
+ * 123       level = 1,
+ * 124       cr4_pae = 0,
+ * 125       quadrant = 0,
+ * 126       direct = 1,
+ * 127       access = 7,
+ * 128       invalid = 0,
+ * 129       nxe = 0,
+ * 130       cr0_wp = 0,
+ * 131       smep_andnot_wp = 0,
+ * 132       smap_andnot_wp = 0,
+ * 133       ad_disabled = 0,
+ * 134       guest_mode = 0,
+ * 135       smm = 0
+ * 136     }
+ * 137   },
+ * 138   spt = 0xffff8bf6ce3d2000, --> 1f0e3d2000
+ * 139   gfns = 0x0,
+ * 140   lpage_disallowed = true,
+ * 141   unsync = false,
+ * 142   root_count = 0,
+ * 143   unsync_children = 0,
+ * 144   parent_ptes = {
+ * 145     val = 18446616490895231600 --> ffff8bf6cfac4a70
+ * 146   },
+ * 147   mmu_valid_gen = 47,
+ * 148   unsync_child_bitmap = {0, 0, 0, 0, 0, 0, 0, 0},
+ * 149   write_flooding_count = {
+ * 150     counter = 0
+ * 151   }
+ * 152 }
+ */
+
 struct kvm_mmu_page {
 	/*
 	 * Note, "link" through "spt" fit in a single 64 byte cache line on
@@ -140,11 +268,24 @@ static inline int kvm_mmu_role_as_id(union kvm_mmu_page_role role)
 	return role.smm ? 1 : 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|52| <<tdp_iter_start>> iter->as_id = kvm_mmu_page_as_id(root);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|156| <<__for_each_tdp_mmu_root_yield_safe>> if (_as_id >= 0 && kvm_mmu_page_as_id(_root) != _as_id) { \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|177| <<__for_each_tdp_mmu_root>> ((_as_id >= 0 && kvm_mmu_page_as_id(_root) != _as_id) || \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|418| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|834| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte,
+ */
 static inline int kvm_mmu_page_as_id(struct kvm_mmu_page *sp)
 {
 	return kvm_mmu_role_as_id(sp->role);
 }
 
+/*
+ * called by:
+ *  - arch/x86/kvm/mmu/spte.c|748| <<make_spte>> else if (kvm_mmu_page_ad_need_write_protect(sp))
+ *  - arch/x86/kvm/mmu/tdp_mmu.c|1544| <<tdp_mmu_need_write_protect>> return kvm_mmu_page_ad_need_write_protect(sp) || !kvm_ad_enabled();
+ */
 static inline bool kvm_mmu_page_ad_need_write_protect(struct kvm_mmu_page *sp)
 {
 	/*
@@ -158,8 +299,35 @@ static inline bool kvm_mmu_page_ad_need_write_protect(struct kvm_mmu_page *sp)
 	return kvm_x86_ops.cpu_dirty_log_size && sp->role.guest_mode;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3227| <<direct_map>> base_gfn = gfn_round_for_level(fault->gfn, it.level);
+ *   - arch/x86/kvm/mmu/mmu.c|4672| <<kvm_tdp_page_fault>> gfn_t base = gfn_round_for_level(fault->gfn,
+ *   - arch/x86/kvm/mmu/mmu.c|7458| <<kvm_arch_post_set_memory_attributes>> gfn_t gfn = gfn_round_for_level(range->start, level);
+ *   - arch/x86/kvm/mmu/mmu.c|7514| <<kvm_mmu_init_memslot_memory_attributes>> gfn_t end = gfn_round_for_level(slot->base_gfn + slot->npages, level);
+ *   - arch/x86/kvm/mmu/mmu.c|7515| <<kvm_mmu_init_memslot_memory_attributes>> gfn_t start = gfn_round_for_level(slot->base_gfn, level);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|302| <<kvm_flush_remote_tlbs_gfn>> kvm_flush_remote_tlbs_range(kvm, gfn_round_for_level(gfn, level),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME>> base_gfn = gfn_round_for_level(fault->gfn, it.level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|97| <<try_step_down>> iter->gfn = gfn_round_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|139| <<try_step_up>> iter->gfn = gfn_round_for_level(iter->gfn, iter->level);
+ *
+ * 把gfn根据level进行round:
+ * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+ * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+ * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+ * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+ * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+ */
 static inline gfn_t gfn_round_for_level(gfn_t gfn, int level)
 {
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) : 4K / 4096 = 1
+	 * KVM_PAGES_PER_HPAGE(2) : 2M / 4096 = 512
+	 * KVM_PAGES_PER_HPAGE(3) : 1G / 4096 = 262144
+	 * KVM_PAGES_PER_HPAGE(4) : 512G / 4096 = 134217728
+	 * KVM_PAGES_PER_HPAGE(5) : 262144G(256T) / 4096 = 68719476736
+	 */
 	return gfn & -KVM_PAGES_PER_HPAGE(level);
 }
 
@@ -172,6 +340,15 @@ bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 				    struct kvm_memory_slot *slot, u64 gfn,
 				    int min_level);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|289| <<kvm_flush_remote_tlbs_sptep>> kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
+ *   - arch/x86/kvm/mmu/mmu.c|862| <<account_shadowed>> kvm_flush_remote_tlbs_gfn(kvm, gfn, PG_LEVEL_4K);
+ *   - arch/x86/kvm/mmu/mmu.c|1625| <<__rmap_add>> kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
+ *   - arch/x86/kvm/mmu/mmu.c|2936| <<mmu_set_spte>> kvm_flush_remote_tlbs_gfn(vcpu->kvm, gfn, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|612| <<tdp_mmu_zap_spte_atomic>> kvm_flush_remote_tlbs_gfn(kvm, iter->gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1046| <<tdp_mmu_map_handle_target_level>> kvm_flush_remote_tlbs_gfn(vcpu->kvm, iter->gfn, iter->level);
+ */
 /* Flush the given page (huge or not) of guest memory. */
 static inline void kvm_flush_remote_tlbs_gfn(struct kvm *kvm, gfn_t gfn, int level)
 {
@@ -279,6 +456,14 @@ enum {
 	RET_PF_SPURIOUS,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3296| <<kvm_handle_noslot_fault>> kvm_mmu_prepare_memory_fault_exit(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4318| <<kvm_faultin_pfn_private>> kvm_mmu_prepare_memory_fault_exit(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4325| <<kvm_faultin_pfn_private>> kvm_mmu_prepare_memory_fault_exit(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4398| <<kvm_faultin_pfn>> kvm_mmu_prepare_memory_fault_exit(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|465| <<kvm_mmu_do_page_fault>> kvm_mmu_prepare_memory_fault_exit(vcpu, &fault);
+ */
 static inline void kvm_mmu_prepare_memory_fault_exit(struct kvm_vcpu *vcpu,
 						     struct kvm_page_fault *fault)
 {
@@ -287,6 +472,11 @@ static inline void kvm_mmu_prepare_memory_fault_exit(struct kvm_vcpu *vcpu,
 				      fault->is_private);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4292| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code, true, NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|5913| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false, &emulation_type);
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u64 err, bool prefetch, int *emulation_type)
 {
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index a5e014d7b..470a6962d 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -19,32 +19,514 @@
 #include <asm/memtype.h>
 #include <asm/vmx.h>
 
+/*
+ * 在以下使用enable_mmio_caching:
+ *   - arch/x86/kvm/mmu/spte.c|22| <<global>> bool __read_mostly enable_mmio_caching = true;
+ *   - arch/x86/kvm/mmu/spte.c|24| <<global>> module_param_named(mmio_caching, enable_mmio_caching, bool, 0444);
+ *   - arch/x86/kvm/mmu/mmu.c|3295| <<kvm_handle_noslot_fault>> if (unlikely(!enable_mmio_caching))
+ *   - arch/x86/kvm/mmu/spte.c|57| <<kvm_mmu_spte_module_init>> allow_mmio_caching = enable_mmio_caching;
+ *   - arch/x86/kvm/mmu/spte.c|356| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = allow_mmio_caching;
+ *   - arch/x86/kvm/mmu/spte.c|357| <<kvm_mmu_set_mmio_spte_mask>> if (!enable_mmio_caching)
+ *   - arch/x86/kvm/mmu/spte.c|390| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = false;
+ *   - arch/x86/kvm/mmu/spte.h|273| <<is_mmio_spte>> likely(enable_mmio_caching);
+ *   - arch/x86/kvm/svm/sev.c|2410| <<sev_hardware_setup>> if (!enable_mmio_caching)
+ *
+ * 核心的处理条件在kvm_handle_noslot_fault()
+ */
 bool __read_mostly enable_mmio_caching = true;
+/*
+ * 在以下使用allow_mmio_caching:
+ *   - arch/x86/kvm/mmu/spte.c|242| <<kvm_mmu_spte_module_init>> allow_mmio_caching = enable_mmio_caching;
+ *   - arch/x86/kvm/mmu/spte.c|595| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = allow_mmio_caching;
+ */
 static bool __ro_after_init allow_mmio_caching;
 module_param_named(mmio_caching, enable_mmio_caching, bool, 0444);
 EXPORT_SYMBOL_GPL(enable_mmio_caching);
 
+/*
+ * 有三种writable.
+ * 1. MMU-writable: shadow_mmu_writable_mask
+ * 2. Host-eritable: shadow_host_writable_mask
+ * 3. Writable SPTE: is_writable_pte(spte)
+ *
+ * 有三个mask:
+ * - shadow_host_writable_mask:
+ *   backend的page在host上是否writable(EPT用EPT_SPTE_HOST_WRITABLE, 普通用DEFAULT_SPTE_HOST_WRITABLE)
+ *   选取的是ept和普通分别不用的bit
+ *   更多是GUP从host获得指示
+ * - shadow_mmu_writable_mask
+ *   指示这个pte的page是不是writable的, 不能简单用bit=1, 没法分清是真的不能还是mmio或是别的
+ *   设置这个mask是告诉user这个pte一定是writable
+ *   EPT用EPT_SPTE_MMU_WRITABLE, 普通用DEFAULT_SPTE_MMU_WRITABLE
+ * - pte & PT_WRITABLE_MASK (看看bit设置了吗)
+ *   这个单纯是看pte的bit=1
+ *
+ *  shadow_mmu_writable_mask, aka MMU-writable -
+ *    Cleared on SPTEs that KVM is currently write-protecting for shadow paging
+ *    purposes (case 2 above). --> 保护影子页表
+ *
+ *  shadow_host_writable_mask, aka Host-writable -
+ *    Cleared on SPTEs that are not host-writable (case 3 above) --> backend memslot不可写
+ *
+ * 查看is_writable_pte()的注释
+ *
+ * 在以下设置shadow_host_writable_mask:
+ *   - arch/x86/kvm/mmu/spte.c|613| <<kvm_mmu_set_ept_masks>> shadow_host_writable_mask = EPT_SPTE_HOST_WRITABLE;
+ *   - arch/x86/kvm/mmu/spte.c|683| <<kvm_mmu_reset_all_pte_masks>> shadow_host_writable_mask = DEFAULT_SPTE_HOST_WRITABLE;
+ * 在以下使用shadow_host_writable_mask:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|961| <<FNAME(sync_spte)>> host_writable = spte & shadow_host_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|346| <<make_spte>> spte |= shadow_host_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.h|489| <<check_spte_writable_invariants>> WARN_ONCE(!(spte & shadow_host_writable_mask),
+ *                                             KBUILD_MODNAME ": MMU-writable SPTE is not Host-writable: %llx", spte);
+ */
 u64 __read_mostly shadow_host_writable_mask;
+/*
+ * 在以下设置shadow_mmu_writable_mask:
+ *   - arch/x86/kvm/mmu/spte.c|614| <<kvm_mmu_set_ept_masks>> shadow_mmu_writable_mask = EPT_SPTE_MMU_WRITABLE;
+ *   - arch/x86/kvm/mmu/spte.c|684| <<kvm_mmu_reset_all_pte_masks>> shadow_mmu_writable_mask = DEFAULT_SPTE_MMU_WRITABLE;
+ * 在以下使用shadow_mmu_writable_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|1255| <<spte_write_protect>> spte &= ~shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|356| <<make_spte>> spte |= PT_WRITABLE_MASK | shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|376| <<make_spte>> spte &= ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ *   - arch/x86/kvm/mmu/spte.h|488| <<check_spte_writable_invariants>> if (spte & shadow_mmu_writable_mask)
+ *   - arch/x86/kvm/mmu/spte.h|499| <<is_mmu_writable_spte>> return spte & shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1736| <<write_protect_gfn>> new_spte = iter.old_spte & ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ *
+ * 用来标记虚拟化的页表里的entry是否指示writable
+ */
 u64 __read_mostly shadow_mmu_writable_mask;
+/*
+ * 在以下设置shadow_nx_mask:
+ *   - arch/x86/kvm/mmu/spte.c|600| <<kvm_mmu_set_ept_masks>> shadow_nx_mask = 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|669| <<kvm_mmu_reset_all_pte_masks>> shadow_nx_mask = PT64_NX_MASK;
+ * 在以下使用shadow_nx_mask:
+ *   - arch/x86/kvm/mmu/spte.c|334| <<make_spte>> spte |= shadow_nx_mask;
+ *   - arch/x86/kvm/mmu/spte.c|412| <<make_spte_executable>> spte &= ~shadow_nx_mask;
+ *   - arch/x86/kvm/mmu/spte.h|46| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *   - arch/x86/kvm/mmu/spte.h|364| <<is_executable_pte>> return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
+ *
+ * 普通page table entry的bit=63 (XD):
+ * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+ * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+ * otherwise, reserved (must be 0)
+ *
+ * 普通pte支持nx bit, ept不支持nx bit
+ * 普通pte不支持x bit, ept支持x bit
+ */
 u64 __read_mostly shadow_nx_mask;
+/*
+ * 在以下设置shadow_x_mask:
+ *   - arch/x86/kvm/mmu/spte.c|601| <<kvm_mmu_set_ept_masks>> shadow_x_mask = VMX_EPT_EXECUTABLE_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|670| <<kvm_mmu_reset_all_pte_masks>> shadow_x_mask = 0;
+ * 在以下使用shadow_x_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|5044| <<boot_cpu_is_amd>> return shadow_x_mask == 0;
+ *   - arch/x86/kvm/mmu/spte.c|332| <<make_spte>> spte |= shadow_x_mask;
+ *   - arch/x86/kvm/mmu/spte.c|413| <<make_spte_executable>> spte |= shadow_x_mask;
+ *   - arch/x86/kvm/mmu/spte.c|479| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK | shadow_user_mask | shadow_x_mask | shadow_me_value;
+ *   - arch/x86/kvm/mmu/spte.h|46| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *   - arch/x86/kvm/mmu/spte.h|364| <<is_executable_pte>> return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
+ *
+ * 普通page table entry的bit=63 (XD):
+ * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+ * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+ * otherwise, reserved (must be 0)
+ *
+ * 普通pte支持nx bit, ept不支持nx bit
+ * 普通pte不支持x bit, ept支持x bit
+ */
 u64 __read_mostly shadow_x_mask; /* mutual exclusive with nx_mask */
+/*
+ * 在以下设置shadow_user_mask:
+ *   - arch/x86/kvm/mmu/spte.c|597| <<kvm_mmu_set_ept_masks>> shadow_user_mask = VMX_EPT_READABLE_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|666| <<kvm_mmu_reset_all_pte_masks>> shadow_user_mask = PT_USER_MASK;
+ * 在以下使用shadow_user_mask:
+ *   - arch/x86/kvm/mmu/mmutrace.h|356| <<__field>> __entry->u = shadow_user_mask ? !!(__entry->spte & shadow_user_mask) : -1;
+ *   - arch/x86/kvm/mmu/spte.c|337| <<make_spte>> if (pte_access & ACC_USER_MASK) spte |= shadow_user_mask;
+ *   - arch/x86/kvm/mmu/spte.c|479| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK | shadow_user_mask | shadow_x_mask | shadow_me_value;
+ *   - arch/x86/kvm/mmu/spte.h|45| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *
+ * 关于PT_USER_MASK, bit=2(U/S):
+ * User/supervisor; if 0, user-mode accesses are not allowed to the 4-KByte
+ * page referenced by this entry (see Section 4.6)
+ *
+ * 普通page fault来的时候error code会设置PT_USER_MASK
+ *
+ * 注释: 因为ept支持exec bit, 所以用shadow_user_mask表示readable.
+ * For the EPT case, shadow_present_mask is 0 if hardware
+ * supports exec-only page table entries.  In that case,
+ * ACC_USER_MASK and shadow_user_mask are used to represent
+ * read access.  See FNAME(gpte_access) in paging_tmpl.h.
+ */
 u64 __read_mostly shadow_user_mask;
+/*
+ * 在以下使用shadow_accessed_mask:
+ *   - arch/x86/kvm/mmu/spte.c|598| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|667| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+ * 在以下设置shadow_accessed_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|631| <<mmu_spte_age>> clear_bit((ffs(shadow_accessed_mask) - 1), (unsigned long *)sptep);
+ *   - arch/x86/kvm/mmu/spte.c|272| <<spte_has_volatile_bits>> if (!(spte & shadow_accessed_mask) || (is_writable_pte(spte) && !(spte & shadow_dirty_mask)))
+ *   - arch/x86/kvm/mmu/spte.c|484| <<make_nonleaf_spte>> spte |= shadow_accessed_mask;
+ *   - arch/x86/kvm/mmu/spte.c|499| <<mark_spte_for_access_track>> return spte & ~shadow_accessed_mask;
+ *   - arch/x86/kvm/mmu/spte.h|310| <<kvm_ad_enabled>> return !!shadow_accessed_mask;
+ *   - arch/x86/kvm/mmu/spte.h|338| <<spte_shadow_accessed_mask>> return spte_ad_enabled(spte) ? shadow_accessed_mask : 0;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1246| <<age_gfn_range>> iter->old_spte = tdp_mmu_clear_spte_bits(iter->sptep, iter->old_spte, shadow_accessed_mask, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1248| <<age_gfn_range>> new_spte = iter->old_spte & ~shadow_accessed_mask;
+ *
+ * 对于普通的页表:
+ * bit-5: Accessed: indicates whether software has accessed the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ * bit-6: Dirty: indicates whether software has written to the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ *
+ * 对于EPT:
+ * - bit-8: If bit 6 (ignore guest PAT) of EPTP is 1, accessed flag for EPT;
+ *   indicates whether software has accessed the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ * - bit-9: If bit 6 (ignore guest PAT) of EPTP is 1, dirty flag for EPT;
+ *   indicates whether software has written to the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ *
+ * 哪些bit用来记录pte被访问
+ */
 u64 __read_mostly shadow_accessed_mask;
+/*
+ * 在以下设置shadow_dirty_mask:
+ *   - arch/x86/kvm/mmu/spte.c|577| <<kvm_mmu_set_ept_masks>> shadow_dirty_mask = has_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|646| <<kvm_mmu_reset_all_pte_masks>> shadow_dirty_mask = PT_DIRTY_MASK;
+ * 在以下使用shadow_dirty_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|1279| <<spte_clear_dirty>> spte &= ~shadow_dirty_mask;
+ *   - arch/x86/kvm/mmu/spte.c|251| <<spte_has_volatile_bits>> if (!(spte & shadow_accessed_mask) || (is_writable_pte(spte) && !(spte & shadow_dirty_mask)))
+ *   - arch/x86/kvm/mmu/spte.h|344| <<spte_shadow_dirty_mask>> return spte_ad_enabled(spte) ? shadow_dirty_mask : 0;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1542| <<clear_dirty_gfn_range>> const u64 dbit = tdp_mmu_need_write_protect(root) ? PT_WRITABLE_MASK : shadow_dirty_mask;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1557| <<clear_dirty_gfn_range>> KVM_MMU_WARN_ON(dbit == shadow_dirty_mask && spte_ad_need_write_protect(iter.old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1596| <<clear_dirty_pt_masked>> const u64 dbit = (wrprot || tdp_mmu_need_write_protect(root)) ? PT_WRITABLE_MASK : shadow_dirty_mask;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1608| <<clear_dirty_pt_masked>> KVM_MMU_WARN_ON(dbit == shadow_dirty_mask && spte_ad_need_write_protect(iter.old_spte));
+ *
+ * 对于普通的页表:
+ * bit-5: Accessed: indicates whether software has accessed the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ * bit-6: Dirty: indicates whether software has written to the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ *
+ * 对于EPT:
+ * - bit-8: If bit 6 (ignore guest PAT) of EPTP is 1, accessed flag for EPT;
+ *   indicates whether software has accessed the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ * - bit-9: If bit 6 (ignore guest PAT) of EPTP is 1, dirty flag for EPT;
+ *   indicates whether software has written to the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ *
+ * 哪些bit用来记录被dirty过
+ */
 u64 __read_mostly shadow_dirty_mask;
+/*
+ * 在以下使用全局的shadow_mmio_value:
+ *   - arch/x86/kvm/mmu/mmu.c|6360| <<kvm_mmu_init_vm>> kvm->arch.shadow_mmio_value = shadow_mmio_value;
+ *   - arch/x86/kvm/mmu/spte.c|547| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_value = mmio_value;
+ * 在以下使用kvm_arch->shadow_mmio_value:
+ *   - arch/x86/kvm/mmu/mmu.c|6360| <<kvm_mmu_init_vm>> kvm->arch.shadow_mmio_value = shadow_mmio_value;
+ *   - arch/x86/kvm/mmu/spte.c|187| <<make_mmio_spte>> WARN_ON_ONCE(!vcpu->kvm->arch.shadow_mmio_value);
+ *   - arch/x86/kvm/mmu/spte.c|190| <<make_mmio_spte>> spte |= vcpu->kvm->arch.shadow_mmio_value | access;
+ *   - arch/x86/kvm/mmu/spte.h|286| <<is_mmio_spte>> return (spte & shadow_mmio_mask) == kvm->arch.shadow_mmio_value && likely(enable_mmio_caching);
+ *
+ * 被shadow_mmio_mask后什么样子的value代表这是mmio entry
+ *
+ * 应该是只有mmio cache被enable的时候才用
+ * 1. 如果disable了, 每次都page fault, 然后查看memslot有没有
+ * 没有就是mmio, 然后emulate. 这种混在一起每次查memslot会影响性能.
+ * 2. 如果enable了, 会尽可能优化来避免每次mmio查看memslot
+ * EPT的mmio value就是会产生ept misconfig的value, 在mis config处理mmio, 不需要查找memslot
+ * regular pte的话支持太多bit就不行了,
+ * 否则可以用一个high reserved bit产生因访问reserved bit而造成的page fault:
+ * Set a reserved PA bit in MMIO SPTEs to generate page faults with
+ * PFEC.RSVD=1 on MMIO accesses.  64-bit PTEs (PAE, x86-64, and EPT
+ * paging) support a maximum of 52 bits of PA, i.e. if the CPU supports
+ * 52-bit physical addresses then there are no reserved PA bits in the
+ * PTEs and so the reserved PA approach must be disabled.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|476| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK | VMX_EPT_SUPPRESS_VE_BIT, 0);
+ *   - arch/x86/kvm/mmu/spte.c|544| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+ *   - arch/x86/kvm/svm/svm.c|5138| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK)
+ */
 u64 __read_mostly shadow_mmio_value;
+/*
+ * 在以下设置shadow_mmio_mask:
+ *   - arch/x86/kvm/mmu/spte.c|393| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_mask = mmio_mask;
+ * 在以下使用shadow_mmio_mask:
+ *   - arch/x86/kvm/mmu/spte.h|272| <<is_mmio_spte>> return (spte & shadow_mmio_mask) == kvm->arch.shadow_mmio_value && likely(enable_mmio_caching);
+ *
+ * shadow_mmio_mask表示哪些bit可能用作mmio
+ */
 u64 __read_mostly shadow_mmio_mask;
+/*
+ * 关于shadow_mmio_access_mask.
+ *
+ * EPT是0
+ * shadow是ACC_WRITE_MASK | ACC_USER_MASK
+ *
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> kvm_mmu_vendor_module_init()
+ *       -> kvm_mmu_reset_all_pte_masks()
+ *          -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *          -> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK); --> 根据情况, 把shadow的设置成ACC_WRITE_MASK | ACC_USER_MASK
+ *    -> ops->hardware_setup = vmx_hardware_setup()
+ *       -> kvm_mmu_set_ept_masks(enable_ept_ad_bits)
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+ *          -> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK | VMX_EPT_SUPPRESS_VE_BIT, 0); --> ept的话会覆盖成0
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ *
+ * 是被下面的patch引入的.
+ *
+ * 4af7715110a2617fc40ac2c1232f664019269f3a
+ * KVM: x86/mmu: Add explicit access mask for MMIO SPTEs
+ *
+ * commit的一些注释:
+ * When shadow paging is enabled, KVM tracks the allowed access type for
+ * MMIO SPTEs so that it can do a permission check on a MMIO GVA cache hit
+ * without having to walk the guest's page tables.  The tracking is done
+ * by retaining the WRITE and USER bits of the access when inserting the
+ * MMIO SPTE (read access is implicitly allowed), which allows the MMIO
+ * page fault handler to retrieve and cache the WRITE/USER bits from the
+ * SPTE. --> "retaining the WRITE and USER bits"是指在host的shadow table保留吗.
+ *
+ * 针对shadow page table来说, 所以shadow_mmio_access_mask:
+ *
+ * 1. 从一个pte中取出R/W和U/S权限, 在fastpath给vcpu_cache_mmio_info()缓存的时候用.
+ *
+ * 2. 从FNAME(gpte_access)改装过的access中取出R/W和U/S权限, 在制作mmio pte的时候再放进去.
+ *
+ * 3. 从FNAME(gpte_access)改装过的access中取出R/W和U/S权限, 在slowpath给vcpu_cache_mmio_info()缓存的时候用.
+ *
+ * 表示在cache mmio_access的时候都cache哪些bit
+ * struct kvm_vcpu *vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> u64 mmio_gva;
+ *    -> unsigned mmio_access;
+ *    -> gfn_t mmio_gfn;
+ *    -> u64 mmio_gen;
+ *
+ * error code:
+ * - P  1 bit   Present When set, the page fault was caused by a
+ *   page-protection violation. When not set, it was caused by a
+ *   non-present page.
+ * - W  1 bit   Write   When set, the page fault was caused by a write
+ *   access. When not set, it was caused by a read access.
+ * - U  1 bit   User    When set, the page fault was caused while CPL =
+ *   3. This does not necessarily mean that the page fault was a
+ *   privilege violation.
+ * - R  1 bit   Reserved write  When set, one or more page directory
+ *   entries contain reserved bits which are set to 1. This only
+ *   applies when the PSE or PAE flags in CR4 are set to 1.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|476| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK | VMX_EPT_SUPPRESS_VE_BIT, 0);
+ *   - arch/x86/kvm/mmu/spte.c|544| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+ *   - arch/x86/kvm/svm/svm.c|5138| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+ *
+ * 在以下设置shadow_mmio_access_mask:
+ *   - arch/x86/kvm/mmu/spte.c|394| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_access_mask = access_mask;
+ * 在以下使用shadow_mmio_access_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|313| <<get_mmio_spte_access>> return spte & shadow_mmio_access_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|3283| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+ *   - arch/x86/kvm/mmu/spte.c|79| <<make_mmio_spte>> access &= shadow_mmio_access_mask;
+ */
 u64 __read_mostly shadow_mmio_access_mask;
+/*
+ * 在以下设置shadow_present_mask:
+ *   - arch/x86/kvm/mmu/spte.c|563| <<kvm_mmu_set_ept_masks>> shadow_present_mask = (has_exec_only ? 0ull : VMX_EPT_READABLE_MASK) | VMX_EPT_SUPPRESS_VE_BIT;
+ *   - arch/x86/kvm/mmu/spte.c|631| <<kvm_mmu_reset_all_pte_masks>> shadow_present_mask = PT_PRESENT_MASK;
+ * 在以下使用shadow_present_mask:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|942| <<FNAME(sync_spte)>> if ((pte_access | shadow_present_mask) == SHADOW_NONPRESENT_VALUE || gfn != kvm_mmu_page_get_gfn(sp, i)) {
+ *   - arch/x86/kvm/mmu/spte.c|263| <<make_spte>> WARN_ON_ONCE((pte_access | shadow_present_mask) == SHADOW_NONPRESENT_VALUE);
+ *   - arch/x86/kvm/mmu/spte.c|270| <<make_spte>> spte |= shadow_present_mask;
+ *   - arch/x86/kvm/mmu/spte.c|438| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK | shadow_user_mask | shadow_x_mask | shadow_me_value;
+ *   - arch/x86/kvm/mmu/spte.h|297| <<is_ept_ve_possible>> return (shadow_present_mask & VMX_EPT_SUPPRESS_VE_BIT) && !(spte & VMX_EPT_SUPPRESS_VE_BIT) &&
+ *                                                             (spte & VMX_EPT_RWX_MASK) != VMX_EPT_MISCONFIG_WX_VALUE;
+ *
+ * 注释: 因为ept支持exec bit, 所以用shadow_user_mask表示readable.
+ * For the EPT case, shadow_present_mask is 0 if hardware
+ * supports exec-only page table entries.  In that case,
+ * ACC_USER_MASK and shadow_user_mask are used to represent
+ * read access.  See FNAME(gpte_access) in paging_tmpl.h.
+ */
 u64 __read_mostly shadow_present_mask;
+/*
+ * 在以下设置shadow_memtype_mask:
+ *   - arch/x86/kvm/mmu/spte.c|571| <<kvm_mmu_set_ept_masks>> shadow_memtype_mask = VMX_EPT_MT_MASK | VMX_EPT_IPAT_BIT;
+ *   - arch/x86/kvm/mmu/spte.c|638| <<kvm_mmu_reset_all_pte_masks>> shadow_memtype_mask = 0;
+ * 在以下使用shadow_memtype_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|4640| <<__kvm_mmu_honors_guest_mtrrs>> return vm_has_noncoherent_dma && shadow_memtype_mask;
+ *   - arch/x86/kvm/mmu/spte.c|302| <<make_spte>> if (shadow_memtype_mask)
+ *
+ * 对于EPT,
+ * bit-3/4/5(VMX_EPT_MT_MASK): EPT memory type for this 4-KByte page (see Section 29.3.7).
+ * bit-6(VMX_EPT_IPAT_BIT): Ignore PAT memory type for this 4-KByte page (see Section 29.3.7).
+ */
 u64 __read_mostly shadow_memtype_mask;
+/*
+ * 在以下设置shadow_me_value:
+ *   - arch/x86/kvm/mmu/spte.c|462| <<kvm_mmu_set_me_spte_mask>> shadow_me_value = me_value;
+ *   - arch/x86/kvm/mmu/spte.c|557| <<kvm_mmu_reset_all_pte_masks>> shadow_me_value = 0;
+ * 在以下使用shadow_me_value:
+ *   - arch/x86/kvm/mmu/mmu.c|3707| <<mmu_alloc_direct_roots>> mmu->pae_root[i] = root | PT_PRESENT_MASK | shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|3845| <<mmu_alloc_shadow_roots>> pm_mask = PT_PRESENT_MASK | shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|5035| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[0][i] &= ~shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|5036| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[1][i] &= ~shadow_me_value;
+ *   - arch/x86/kvm/mmu/mmu.c|6196| <<__kvm_mmu_create>> WARN_ON_ONCE(shadow_me_value);
+ *   - arch/x86/kvm/mmu/spte.c|247| <<make_spte>> if (shadow_me_value && !kvm_is_mmio_pfn(pfn))
+ *   - arch/x86/kvm/mmu/spte.c|248| <<make_spte>> spte |= shadow_me_value;
+ *   - arch/x86/kvm/mmu/spte.c|362| <<make_nonleaf_spte>> spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK | shadow_user_mask | shadow_x_mask | shadow_me_value;
+ *
+ * 用在memory encryption
+ * 对于vmx: me_value = 0
+ */
 u64 __read_mostly shadow_me_value;
+/*
+ * 在以下设置shadow_me_mask:
+ *   - arch/x86/kvm/mmu/spte.c|463| <<kvm_mmu_set_me_spte_mask>> shadow_me_mask = me_mask;
+ *   - arch/x86/kvm/mmu/spte.c|556| <<kvm_mmu_reset_all_pte_masks>> shadow_me_mask = 0;
+ * 在以下使用shadow_me_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|5023| <<reset_shadow_zero_bits_mask>> if (!shadow_me_mask)
+ *   - arch/x86/kvm/mmu/mmu.c|5033| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[0][i] |= shadow_me_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|5034| <<reset_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[1][i] |= shadow_me_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|5068| <<reset_tdp_shadow_zero_bits_mask>> if (!shadow_me_mask)
+ *   - arch/x86/kvm/mmu/mmu.c|5072| <<reset_tdp_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[0][i] &= ~shadow_me_mask;
+ *   - arch/x86/kvm/mmu/mmu.c|5073| <<reset_tdp_shadow_zero_bits_mask>> shadow_zero_check->rsvd_bits_mask[1][i] &= ~shadow_me_mask;
+ *   - arch/x86/kvm/mmu/spte.h|46| <<SPTE_PERM_MASK>> #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \ | shadow_x_mask | shadow_nx_mask | shadow_me_mask)
+ *
+ * 用在memory encryption
+ * 对于vmx: me_mask = rsvd_bits(boot_cpu_data.x86_phys_bits, kvm_get_shadow_phys_bits() - 1);
+ */
 u64 __read_mostly shadow_me_mask;
+/*
+ * 在以下设置shadow_acc_track_mask:
+ *   - arch/x86/kvm/mmu/spte.c|488| <<kvm_mmu_set_ept_masks>> shadow_acc_track_mask = VMX_EPT_RWX_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|555| <<kvm_mmu_reset_all_pte_masks>> shadow_acc_track_mask = 0;
+ * 在以下使用shadow_acc_track_mask:
+ *   - arch/x86/kvm/mmu/spte.c|388| <<mark_spte_for_access_track>> spte &= ~shadow_acc_track_mask;
+ *   - arch/x86/kvm/mmu/spte.h|349| <<is_access_track_spte>> return !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;
+ *   - arch/x86/kvm/mmu/spte.h|530| <<restore_acc_track_spte>> spte &= ~shadow_acc_track_mask;
+ *
+ * 注释:
+ * SPTEs in MMUs without A/D bits are marked with SPTE_TDP_AD_DISABLED;
+ * shadow_acc_track_mask is the set of bits to be cleared in non-accessed
+ * pages.
+ *
+ * 用来trap对page的access
+ * R, W, X
+ *
+ * 不支持A/D bits的时候用的
+ */
 u64 __read_mostly shadow_acc_track_mask;
 
+/*
+ * commit 28a1f3ac1d0c8558ee4453d9634dad891a6e922e
+ * Author: Junaid Shahid <junaids@google.com>
+ * Date:   Tue Aug 14 10:15:34 2018 -0700
+ *
+ * kvm: x86: Set highest physical address bits in non-present/reserved SPTEs
+ *
+ * Always set the 5 upper-most supported physical address bits to 1 for SPTEs
+ * that are marked as non-present or reserved, to make them unusable for
+ * L1TF attacks from the guest. Currently, this just applies to MMIO SPTEs.
+ * (We do not need to mark PTEs that are completely 0 as physical page 0
+ * is already reserved.)
+ *
+ * This allows mitigation of L1TF without disabling hyper-threading by using
+ * shadow paging mode instead of EPT.
+ *
+ * Signed-off-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ *
+ * 在以下设置shadow_nonpresent_or_rsvd_mask:
+ *   - arch/x86/kvm/mmu/spte.c|456| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_mask = 0;
+ *   - arch/x86/kvm/mmu/spte.c|463| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_mask = rsvd_bits(low_phys_bits, boot_cpu_data.x86_cache_bits - 1);
+ * 在以下使用shadow_nonpresent_or_rsvd_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|306| <<get_mmio_spte_gfn>> gpa |= (spte >> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN) & shadow_nonpresent_or_rsvd_mask;
+ *   - arch/x86/kvm/mmu/spte.c|81| <<make_mmio_spte>> spte |= gpa | shadow_nonpresent_or_rsvd_mask;
+ *   - arch/x86/kvm/mmu/spte.c|82| <<make_mmio_spte>> spte |= (gpa & shadow_nonpresent_or_rsvd_mask)
+ *   - arch/x86/kvm/mmu/spte.c|375| <<kvm_mmu_set_mmio_spte_mask>> if (WARN_ON(mmio_value & (shadow_nonpresent_or_rsvd_mask <<
+ *
+ * 测试的例子 (不支持ME):
+ * boot_cpu_data.x86_phys_bits = 46
+ * boot_cpu_data.x86_cache_bits = 46
+ * shadow_phys_bits=46
+ * bit: 41 - 45 (5个bit)
+ * shadow_nonpresent_or_rsvd_mask=0x00003e0000000000
+ * bit: 12 - 40
+ * shadow_nonpresent_or_rsvd_lower_gfn_mask=0x000001fffffff000
+ *
+ * 注释:
+ * This mask must be set on all non-zero Non-Present or Reserved SPTEs in order
+ * to guard against L1TF attacks.
+ */
 u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
+/*
+ * 在以下设置shadow_nonpresent_or_rsvd_lower_gfn_mask:
+ *   - arch/x86/kvm/mmu/spte.c|539| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_lower_gfn_mask = GENMASK_ULL(low_phys_bits - 1, PAGE_SHIFT);
+ * 在以下使用shadow_nonpresent_or_rsvd_lower_gfn_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|308| <<get_mmio_spte_gfn>> u64 gpa = spte & shadow_nonpresent_or_rsvd_lower_gfn_mask;
+ *   - arch/x86/kvm/mmu/spte.c|402| <<kvm_mmu_set_mmio_spte_mask>> WARN_ON(mmio_value & shadow_nonpresent_or_rsvd_lower_gfn_mask);
+ */
 u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
+/*
+ * 在以下设置shadow_phys_bits:
+ *   - arch/x86/kvm/mmu/spte.c|516| <<kvm_mmu_reset_all_pte_masks>> shadow_phys_bits = kvm_get_shadow_phys_bits();
+ * 在以下使用shadow_phys_bits:
+ *   - arch/x86/kvm/mmu.h|79| <<kvm_mmu_max_gfn>> int max_gpa_bits = likely(tdp_enabled) ? shadow_phys_bits : 52;
+ *   - arch/x86/kvm/mmu/mmu.c|4996| <<reserved_hpa_bits>> return rsvd_bits(shadow_phys_bits, 63);
+ *   - arch/x86/kvm/mmu/spte.c|569| <<kvm_mmu_reset_all_pte_masks>> if (shadow_phys_bits < 52)
+ *
+ * The number of non-reserved physical address bits irrespective of features
+ * that repurpose legal bits, e.g. MKTME.
+ * host phys bit最多支持的bit数目
+ */
 u8 __read_mostly shadow_phys_bits;
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> kvm_mmu_vendor_module_init()
+ *       -> kvm_mmu_reset_all_pte_masks()
+ *          -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *          -> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK); --> 根据情况, 把shadow的设置成ACC_WRITE_MASK | ACC_USER_MASK
+ *    -> ops->hardware_setup = vmx_hardware_setup()
+ *       -> kvm_mmu_set_ept_masks(enable_ept_ad_bits)
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+ *          -> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK | VMX_EPT_SUPPRESS_VE_BIT, 0); --> ept的话会覆盖成0
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7073| <<kvm_mmu_x86_module_init>> kvm_mmu_spte_module_init();
+ */
 void __init kvm_mmu_spte_module_init(void)
 {
 	/*
@@ -57,6 +539,10 @@ void __init kvm_mmu_spte_module_init(void)
 	allow_mmio_caching = enable_mmio_caching;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|74| <<make_mmio_spte>> u64 spte = generation_mmio_spte_mask(gen);
+ */
 static u64 generation_mmio_spte_mask(u64 gen)
 {
 	u64 mask;
@@ -68,6 +554,51 @@ static u64 generation_mmio_spte_mask(u64 gen)
 	return mask;
 }
 
+/*
+ * commit 28a1f3ac1d0c8558ee4453d9634dad891a6e922e
+ * Author: Junaid Shahid <junaids@google.com>
+ * Date:   Tue Aug 14 10:15:34 2018 -0700
+ *
+ * kvm: x86: Set highest physical address bits in non-present/reserved SPTEs
+ *
+ * Always set the 5 upper-most supported physical address bits to 1 for SPTEs
+ * that are marked as non-present or reserved, to make them unusable for
+ * L1TF attacks from the guest. Currently, this just applies to MMIO SPTEs.
+ * (We do not need to mark PTEs that are completely 0 as physical page 0
+ * is already reserved.)
+ *
+ * This allows mitigation of L1TF without disabling hyper-threading by using
+ * shadow paging mode instead of EPT.
+ *
+ * Signed-off-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ *
+ * 在以下设置shadow_nonpresent_or_rsvd_mask:
+ *   - arch/x86/kvm/mmu/spte.c|456| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_mask = 0;
+ *   - arch/x86/kvm/mmu/spte.c|463| <<kvm_mmu_reset_all_pte_masks>> shadow_nonpresent_or_rsvd_mask = rsvd_bits(low_phys_bits, boot_cpu_data.x86_cache_bits - 1);
+ * 在以下使用shadow_nonpresent_or_rsvd_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|306| <<get_mmio_spte_gfn>> gpa |= (spte >> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN) & shadow_nonpresent_or_rsvd_mask;
+ *   - arch/x86/kvm/mmu/spte.c|81| <<make_mmio_spte>> spte |= gpa | shadow_nonpresent_or_rsvd_mask;
+ *   - arch/x86/kvm/mmu/spte.c|82| <<make_mmio_spte>> spte |= (gpa & shadow_nonpresent_or_rsvd_mask)
+ *   - arch/x86/kvm/mmu/spte.c|375| <<kvm_mmu_set_mmio_spte_mask>> if (WARN_ON(mmio_value & (shadow_nonpresent_or_rsvd_mask <<
+ *
+ * 测试的例子 (不支持ME):
+ * boot_cpu_data.x86_phys_bits = 46
+ * boot_cpu_data.x86_cache_bits = 46
+ * shadow_phys_bits=46
+ * bit: 41 - 45 (5个bit)
+ * shadow_nonpresent_or_rsvd_mask=0x00003e0000000000
+ * bit: 12 - 40
+ * shadow_nonpresent_or_rsvd_lower_gfn_mask=0x000001fffffff000
+ *
+ * 注释:
+ * This mask must be set on all non-zero Non-Present or Reserved SPTEs in order
+ * to guard against L1TF attacks.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|295| <<mark_mmio_spte>> u64 spte = make_mmio_spte(vcpu, gfn, access);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1029| <<tdp_mmu_map_handle_target_level>> new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
+ */
 u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)
 {
 	u64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;
@@ -76,8 +607,39 @@ u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)
 
 	WARN_ON_ONCE(!vcpu->kvm->arch.shadow_mmio_value);
 
+	/*
+	 * 在以下设置shadow_mmio_access_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|394| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_access_mask = access_mask;
+	 * 在以下使用shadow_mmio_access_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|313| <<get_mmio_spte_access>> return spte & shadow_mmio_access_mask;
+	 *   - arch/x86/kvm/mmu/mmu.c|3283| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+	 *   - arch/x86/kvm/mmu/spte.c|79| <<make_mmio_spte>> access &= shadow_mmio_access_mask;
+	 */
 	access &= shadow_mmio_access_mask;
+	/*
+	 * 在以下使用全局的shadow_mmio_value:
+	 *   - arch/x86/kvm/mmu/mmu.c|6360| <<kvm_mmu_init_vm>> kvm->arch.shadow_mmio_value = shadow_mmio_value;
+	 *   - arch/x86/kvm/mmu/spte.c|547| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_value = mmio_value;
+	 * 在以下使用kvm_arch->shadow_mmio_value:
+	 *   - arch/x86/kvm/mmu/mmu.c|6360| <<kvm_mmu_init_vm>> kvm->arch.shadow_mmio_value = shadow_mmio_value;
+	 *   - arch/x86/kvm/mmu/spte.c|187| <<make_mmio_spte>> WARN_ON_ONCE(!vcpu->kvm->arch.shadow_mmio_value);
+	 *   - arch/x86/kvm/mmu/spte.c|190| <<make_mmio_spte>> spte |= vcpu->kvm->arch.shadow_mmio_value | access;
+	 *   - arch/x86/kvm/mmu/spte.h|286| <<is_mmio_spte>> return (spte & shadow_mmio_mask) == kvm->arch.shadow_mmio_value && likely(enable_mmio_caching);
+	 */
 	spte |= vcpu->kvm->arch.shadow_mmio_value | access;
+	/*
+	 * commit的注释
+	 * kvm: x86: Set highest physical address bits in non-present/reserved SPTEs
+	 *
+	 * Always set the 5 upper-most supported physical address bits to 1 for SPTEs
+	 * that are marked as non-present or reserved, to make them unusable for
+	 * L1TF attacks from the guest. Currently, this just applies to MMIO SPTEs.
+	 * (We do not need to mark PTEs that are completely 0 as physical page 0
+	 * is already reserved.)
+	 *
+	 * This allows mitigation of L1TF without disabling hyper-threading by using
+	 * shadow paging mode instead of EPT.
+	 */
 	spte |= gpa | shadow_nonpresent_or_rsvd_mask;
 	spte |= (gpa & shadow_nonpresent_or_rsvd_mask)
 		<< SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;
@@ -85,6 +647,11 @@ u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|236| <<make_spte>> spte |= static_call(kvm_x86_get_mt_mask)(vcpu, gfn, kvm_is_mmio_pfn(pfn));
+ *   - arch/x86/kvm/mmu/spte.c|242| <<make_spte>> if (shadow_me_value && !kvm_is_mmio_pfn(pfn))
+ */
 static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
 {
 	if (pfn_valid(pfn))
@@ -111,6 +678,12 @@ static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
  * The caller is responsible for checking if the SPTE is shadow-present, and
  * for determining whether or not the caller cares about non-leaf SPTEs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|509| <<mmu_spte_update_no_track>> if (!spte_has_volatile_bits(old_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|577| <<mmu_spte_clear_track_bits>> !spte_has_volatile_bits(old_spte))
+ *   - arch/x86/kvm/mmu/tdp_iter.h|50| <<kvm_tdp_mmu_spte_need_atomic_write>> spte_has_volatile_bits(old_spte);
+ */
 bool spte_has_volatile_bits(u64 spte)
 {
 	/*
@@ -134,12 +707,30 @@ bool spte_has_volatile_bits(u64 spte)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2913| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch, true, host_writable, &spte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|963| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access, gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1031| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte,
+ *                                                            fault->prefetch, true, fault->map_writable, &new_spte);
+ */
 bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	       const struct kvm_memory_slot *slot,
 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
 	       u64 old_spte, bool prefetch, bool can_unsync,
 	       bool host_writable, u64 *new_spte)
 {
+	/*
+	 * 关于level的注释:
+	 * the 4 bits of level are effectively limited to the values 2/3/4/5,
+	 *     as 4k SPs are not tracked (allowed to go unsync).  In addition non-PAE
+	 *     paging has exactly one upper level, making level completely redundant
+	 *     when has_4_byte_gpte=1.
+	 *
+	 * struct kvm_mmu_page *sp:
+	 * -> union kvm_mmu_page_role role;
+	 *    -> unsigned level:4;
+	 */
 	int level = sp->role.level;
 	u64 spte = SPTE_MMU_PRESENT_MASK;
 	bool wrprot = false;
@@ -173,6 +764,16 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	 * would tie make_spte() further to vCPU/MMU state, and add complexity
 	 * just to optimize a mode that is anything but performance critical.
 	 */
+	/*
+	 * enum pg_level {
+	 *     PG_LEVEL_NONE,
+	 *     PG_LEVEL_4K,
+	 *     PG_LEVEL_2M,
+	 *     PG_LEVEL_1G,
+	 *     PG_LEVEL_512G,
+	 *     PG_LEVEL_NUM
+	 * };
+	 */
 	if (level > PG_LEVEL_4K && (pte_access & ACC_EXEC_MASK) &&
 	    is_nx_huge_page_enabled(vcpu->kvm)) {
 		pte_access &= ~ACC_EXEC_MASK;
@@ -189,6 +790,9 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	if (level > PG_LEVEL_4K)
 		spte |= PT_PAGE_SIZE_MASK;
 
+	/*
+	 * vmx_get_mt_mask()
+	 */
 	if (shadow_memtype_mask)
 		spte |= static_call(kvm_x86_get_mt_mask)(vcpu, gfn,
 							 kvm_is_mmio_pfn(pfn));
@@ -241,6 +845,22 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	if ((spte & PT_WRITABLE_MASK) && kvm_slot_dirty_track_enabled(slot)) {
 		/* Enforced by kvm_mmu_hugepage_adjust. */
 		WARN_ON_ONCE(level > PG_LEVEL_4K);
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/mmu.c|1592| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+		 *   - arch/loongarch/kvm/mmu.c|891| <<kvm_map_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+		 *   - arch/s390/kvm/gaccess.c|1025| <<access_guest_page_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+		 *   - arch/s390/kvm/gaccess.c|1247| <<cmpxchg_guest_abs_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+		 *   - arch/x86/kvm/mmu/mmu.c|3382| <<fast_pf_fix_direct_spte>> mark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);
+		 *   - arch/x86/kvm/mmu/spte.c|824| <<make_spte>> mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
+		 *   - arch/x86/kvm/x86.c|3761| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+		 *   - arch/x86/kvm/x86.c|5097| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+		 *   - include/linux/kvm_host.h|1823| <<kvm_gpc_mark_dirty_in_slot>> mark_page_dirty_in_slot(gpc->kvm, gpc->memslot, gpa_to_gfn(gpc->gpa));
+		 *   - virt/kvm/kvm_main.c|3384| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+		 *   - virt/kvm/kvm_main.c|3522| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+		 *   - virt/kvm/kvm_main.c|3622| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+		 *   - virt/kvm/kvm_main.c|3631| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+		 */
 		mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
 	}
 
@@ -248,6 +868,10 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	return wrprot;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|350| <<make_huge_page_split_spte>> child_spte = make_spte_executable(child_spte);
+ */
 static u64 make_spte_executable(u64 spte)
 {
 	bool is_access_track = is_access_track_spte(spte);
@@ -271,6 +895,11 @@ static u64 make_spte_executable(u64 spte)
  * This is used during huge page splitting to build the SPTEs that make up the
  * new page table.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6597| <<shadow_mmu_split_huge_page>> spte = make_huge_page_split_spte(kvm, huge_spte, sp->role, index);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1415| <<tdp_mmu_split_huge_page>> sp->spt[i] = make_huge_page_split_spte(kvm, huge_spte, sp->role, i);
+ */
 u64 make_huge_page_split_spte(struct kvm *kvm, u64 huge_spte, union kvm_mmu_page_role role,
 			      int index)
 {
@@ -307,6 +936,11 @@ u64 make_huge_page_split_spte(struct kvm *kvm, u64 huge_spte, union kvm_mmu_page
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2410| <<__link_shadow_page>> spte = make_nonleaf_spte(sp->spt, sp_ad_disabled(sp));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1082| <<tdp_mmu_link_sp>> u64 spte = make_nonleaf_spte(sp->spt, !kvm_ad_enabled());
+ */
 u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 {
 	u64 spte = SPTE_MMU_PRESENT_MASK;
@@ -322,6 +956,13 @@ u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|641| <<mmu_spte_age>> spte = mark_spte_for_access_track(spte);
+ *   - arch/x86/kvm/mmu/spte.c|282| <<make_spte>> spte = mark_spte_for_access_track(spte);
+ *   - arch/x86/kvm/mmu/spte.c|309| <<make_spte_executable>> spte = mark_spte_for_access_track(spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1257| <<age_gfn_range>> new_spte = mark_spte_for_access_track(iter->old_spte);
+ */
 u64 mark_spte_for_access_track(u64 spte)
 {
 	if (spte_ad_enabled(spte))
@@ -343,6 +984,12 @@ u64 mark_spte_for_access_track(u64 spte)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|476| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK | VMX_EPT_SUPPRESS_VE_BIT, 0);
+ *   - arch/x86/kvm/mmu/spte.c|544| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+ *   - arch/x86/kvm/svm/svm.c|5138| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+ */
 void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 {
 	BUG_ON((u64)(unsigned)access_mask != access_mask);
@@ -386,6 +1033,20 @@ void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 	    WARN_ON(mmio_value && (REMOVED_SPTE & mmio_mask) == mmio_value))
 		mmio_value = 0;
 
+	/*
+	 * 在以下使用enable_mmio_caching:
+	 *   - arch/x86/kvm/mmu/spte.c|22| <<global>> bool __read_mostly enable_mmio_caching = true;
+	 *   - arch/x86/kvm/mmu/spte.c|24| <<global>> module_param_named(mmio_caching, enable_mmio_caching, bool, 0444);
+	 *   - arch/x86/kvm/mmu/mmu.c|3295| <<kvm_handle_noslot_fault>> if (unlikely(!enable_mmio_caching))
+	 *   - arch/x86/kvm/mmu/spte.c|57| <<kvm_mmu_spte_module_init>> allow_mmio_caching = enable_mmio_caching;
+	 *   - arch/x86/kvm/mmu/spte.c|356| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = allow_mmio_caching;
+	 *   - arch/x86/kvm/mmu/spte.c|357| <<kvm_mmu_set_mmio_spte_mask>> if (!enable_mmio_caching)
+	 *   - arch/x86/kvm/mmu/spte.c|390| <<kvm_mmu_set_mmio_spte_mask>> enable_mmio_caching = false;
+	 *   - arch/x86/kvm/mmu/spte.h|273| <<is_mmio_spte>> likely(enable_mmio_caching);
+	 *   - arch/x86/kvm/svm/sev.c|2410| <<sev_hardware_setup>> if (!enable_mmio_caching)
+	 *
+	 * 核心的处理条件在kvm_handle_noslot_fault()
+	 */
 	if (!mmio_value)
 		enable_mmio_caching = false;
 
@@ -395,6 +1056,11 @@ void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mmio_spte_mask);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5297| <<svm_hardware_setup>> kvm_mmu_set_me_spte_mask(sme_me_mask, sme_me_mask);
+ *   - arch/x86/kvm/vmx/vmx.c|8438| <<vmx_setup_me_spte_mask>> kvm_mmu_set_me_spte_mask(0, me_mask);
+ */
 void kvm_mmu_set_me_spte_mask(u64 me_value, u64 me_mask)
 {
 	/* shadow_me_value must be a subset of shadow_me_mask */
@@ -406,6 +1072,10 @@ void kvm_mmu_set_me_spte_mask(u64 me_value, u64 me_mask)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_me_spte_mask);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8550| <<vmx_hardware_setup>> kvm_mmu_set_ept_masks(enable_ept_ad_bits, cpu_has_vmx_ept_execute_only());
+ */
 void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)
 {
 	shadow_user_mask	= VMX_EPT_READABLE_MASK;
@@ -431,16 +1101,64 @@ void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)
 	 * EPT Misconfigurations are generated if the value of bits 2:0
 	 * of an EPT paging-structure entry is 110b (write/execute).
 	 */
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/spte.c|476| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK | VMX_EPT_SUPPRESS_VE_BIT, 0);
+	 *   - arch/x86/kvm/mmu/spte.c|544| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+	 *   - arch/x86/kvm/svm/svm.c|5138| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+	 */
 	kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE,
 				   VMX_EPT_RWX_MASK | VMX_EPT_SUPPRESS_VE_BIT, 0);
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_ept_masks);
 
+/*
+ * kvm_x86_init()
+ * -> kvm_mmu_x86_module_init()
+ *    -> tdp_mmu_allowed = tdp_mmu_enabled
+ *    -> kvm_mmu_spte_module_init()
+ *       -> allow_mmio_caching = enable_mmio_caching;
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> kvm_mmu_vendor_module_init()
+ *       -> kvm_mmu_reset_all_pte_masks()
+ *          -> shadow_accessed_mask = PT_ACCESSED_MASK; (1 << 5)
+ *          -> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK); --> 根据情况, 把shadow的设置成ACC_WRITE_MASK | ACC_USER_MASK
+ *    -> ops->hardware_setup = vmx_hardware_setup()
+ *       -> kvm_mmu_set_ept_masks(enable_ept_ad_bits)
+ *          -> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+ *          -> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK | VMX_EPT_SUPPRESS_VE_BIT, 0); --> ept的话会覆盖成0
+ *
+ * kvm_create_vm()
+ * -> hardware_enable_all()
+ *    -> on_each_cpu(hardware_enable_nolock, &failed, 1);
+ *       -> __hardware_enable_nolock()
+ *          -> kvm_arch_hardware_enable()
+ *             -> static_call(kvm_x86_hardware_enable)() = hardware_enable()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7095| <<kvm_mmu_vendor_module_init>> kvm_mmu_reset_all_pte_masks();
+ */
 void kvm_mmu_reset_all_pte_masks(void)
 {
 	u8 low_phys_bits;
 	u64 mask;
 
+	/*
+	 * 比方是46位
+	 *
+	 * # cpuid -l 0x80000008 -1
+	 * CPU:
+	 *    Physical Address and Linear Address Size (0x80000008/eax):
+	 *       maximum physical address bits         = 0x2e (46)
+	 *       maximum linear (virtual) address bits = 0x30 (48)
+	 *       maximum guest physical address bits   = 0x0 (0)
+	 *
+	 * # cpuid -l 0x80000008 -1 -r
+	 * CPU:
+	 *    0x80000008 0x00: eax=0x0000302e ebx=0x00000000 ecx=0x00000000 edx=0x00000000
+	 */
 	shadow_phys_bits = kvm_get_shadow_phys_bits();
 
 	/*
@@ -455,6 +1173,18 @@ void kvm_mmu_reset_all_pte_masks(void)
 	 */
 	shadow_nonpresent_or_rsvd_mask = 0;
 	low_phys_bits = boot_cpu_data.x86_phys_bits;
+	/*
+	 * 测试的例子 (不支持ME):
+	 * boot_cpu_data.x86_phys_bits = 46
+	 * boot_cpu_data.x86_cache_bits = 46
+	 * shadow_phys_bits=46
+	 * bit: 41 - 45 (5个bit)
+	 * shadow_nonpresent_or_rsvd_mask=0x00003e0000000000
+	 * bit: 12 - 40
+	 * shadow_nonpresent_or_rsvd_lower_gfn_mask=0x000001fffffff000
+	 *
+	 * 52 - 5 = 47
+	 */
 	if (boot_cpu_has_bug(X86_BUG_L1TF) &&
 	    !WARN_ON_ONCE(boot_cpu_data.x86_cache_bits >=
 			  52 - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)) {
@@ -467,10 +1197,39 @@ void kvm_mmu_reset_all_pte_masks(void)
 	shadow_nonpresent_or_rsvd_lower_gfn_mask =
 		GENMASK_ULL(low_phys_bits - 1, PAGE_SHIFT);
 
+	/*
+	 * 1 << 2
+	 */
 	shadow_user_mask	= PT_USER_MASK;
+	/*
+	 * 1 << 5
+	 */
 	shadow_accessed_mask	= PT_ACCESSED_MASK;
+	/*
+	 * 1 << 6
+	 */
 	shadow_dirty_mask	= PT_DIRTY_MASK;
+	/*
+	 * 普通page table entry的bit=63 (XD):
+	 * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+	 * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+	 * otherwise, reserved (must be 0)
+	 *
+	 * 普通pte支持nx bit, ept不支持nx bit
+	 * 普通pte不支持x bit, ept支持x bit
+	 *
+	 * 1 << 63
+	 */
 	shadow_nx_mask		= PT64_NX_MASK;
+	/*
+	 * 普通page table entry的bit=63 (XD):
+	 * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+	 * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+	 * otherwise, reserved (must be 0)
+	 *
+	 * 普通pte支持nx bit, ept不支持nx bit
+	 * 普通pte不支持x bit, ept支持x bit
+	 */
 	shadow_x_mask		= 0;
 	shadow_present_mask	= PT_PRESENT_MASK;
 
@@ -499,5 +1258,11 @@ void kvm_mmu_reset_all_pte_masks(void)
 	else
 		mask = 0;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/spte.c|476| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK | VMX_EPT_SUPPRESS_VE_BIT, 0);
+	 *   - arch/x86/kvm/mmu/spte.c|544| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+	 *   - arch/x86/kvm/svm/svm.c|5138| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+	 */
 	kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
 }
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index 52fa004a1..9b9b0c26d 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -15,6 +15,14 @@
  * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
  * enough that the improved code generation is noticeable in KVM's footprint.
  */
+/*
+ * 在以下使用SPTE_MMU_PRESENT_MASK(非assert):
+ *   - arch/x86/kvm/mmu/spte.c|356| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|538| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|292| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+ *
+ * mmio是不设置这个bit=11
+ */
 #define SPTE_MMU_PRESENT_MASK		BIT_ULL(11)
 
 /*
@@ -29,6 +37,14 @@
  * TDP with CPU dirty logging (PML).  If NPT ever gains PML-like support, it
  * must be restricted to 64-bit KVM.
  */
+/*
+ * 在以下使用SPTE_TDP_AD_ENABLED:
+ *   - arch/x86/kvm/mmu/spte.h|37| <<global>> static_assert(SPTE_TDP_AD_ENABLED == 0);
+ *   - arch/x86/kvm/mmu/spte.h|332| <<spte_ad_need_write_protect>> return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_ENABLED;
+ *
+ * 在以下使用SPTE_TDP_AD_WRPROT_ONLY:
+ *   - arch/x86/kvm/mmu/spte.c|738| <<make_spte>> else if (kvm_mmu_page_ad_need_write_protect(sp)) spte |= SPTE_TDP_AD_WRPROT_ONLY;
+ */
 #define SPTE_TDP_AD_SHIFT		52
 #define SPTE_TDP_AD_MASK		(3ULL << SPTE_TDP_AD_SHIFT)
 #define SPTE_TDP_AD_ENABLED		(0ULL << SPTE_TDP_AD_SHIFT)
@@ -39,24 +55,66 @@ static_assert(SPTE_TDP_AD_ENABLED == 0);
 #ifdef CONFIG_DYNAMIC_PHYSICAL_MASK
 #define SPTE_BASE_ADDR_MASK (physical_mask & ~(u64)(PAGE_SIZE-1))
 #else
+/*
+ * 以前的注释
+ * ((1ULL << 52) - 1) = 0xfffffffffffff (13个f)
+ * ~(u64)(PAGE_SIZE-1) = 除了后12位都是0
+ * 在没有CONFIG_DYNAMIC_PHYSICAL_MASK的时候是0xffffffffff000 (13个hex)
+ */
 #define SPTE_BASE_ADDR_MASK (((1ULL << 52) - 1) & ~(u64)(PAGE_SIZE-1))
 #endif
 
+/*
+ * 没人使用SPTE_PERM_MASK
+ */
 #define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \
 			| shadow_x_mask | shadow_nx_mask | shadow_me_mask)
 
+/*
+ * 在以下使用ACC_EXEC_MASK:
+ *   - arch/x86/kvm/mmu/mmu.c|5103| <<update_permission_bitmask>> const u8 x = BYTE_MASK(ACC_EXEC_MASK);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|184| <<FNAME(gpte_access)>> ((gpte & VMX_EPT_EXECUTABLE_MASK) ? ACC_EXEC_MASK : 0) |
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|187| <<FNAME(gpte_access)>> BUILD_BUG_ON(ACC_EXEC_MASK != PT_PRESENT_MASK);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|188| <<FNAME(gpte_access)>> BUILD_BUG_ON(ACC_EXEC_MASK != 1);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|835| <<FNAME(page_fault)>> walker.pte_access &= ~ACC_EXEC_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|388| <<make_spte>> if (level > PG_LEVEL_4K && (pte_access & ACC_EXEC_MASK) &&
+ *   - arch/x86/kvm/mmu/spte.c|390| <<make_spte>> pte_access &= ~ACC_EXEC_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|393| <<make_spte>> if (pte_access & ACC_EXEC_MASK)
+ *   - arch/x86/kvm/mmu/spte.c|523| <<make_huge_page_split_spte>> if ((role.access & ACC_EXEC_MASK) && is_nx_huge_page_enabled(kvm))
+ *   - arch/x86/kvm/mmu/spte.h|51| <<ACC_ALL>> #define ACC_ALL (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
+ */
 #define ACC_EXEC_MASK    1
 #define ACC_WRITE_MASK   PT_WRITABLE_MASK
 #define ACC_USER_MASK    PT_USER_MASK
 #define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)
 
 /* The mask for the R/X bits in EPT PTEs */
+/*
+ * 在以下使用SPTE_EPT_READABLE_MASK和SPTE_EPT_EXECUTABLE_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|104| <<SHADOW_ACC_TRACK_SAVED_BITS_MASK>> #define SHADOW_ACC_TRACK_SAVED_BITS_MASK (SPTE_EPT_READABLE_MASK | \ SPTE_EPT_EXECUTABLE_MASK)
+ */
 #define SPTE_EPT_READABLE_MASK			0x1ull
 #define SPTE_EPT_EXECUTABLE_MASK		0x4ull
 
 #define SPTE_LEVEL_BITS			9
+/*
+ * SPTE_LEVEL_SHIFT(5): 12 + 4 * 9 = 48
+ * SPTE_LEVEL_SHIFT(4): 12 + 3 * 9 = 39
+ * SPTE_LEVEL_SHIFT(3): 12 + 2 * 9 = 30
+ * SPTE_LEVEL_SHIFT(2): 12 + 1 * 9 = 21
+ * SPTE_LEVEL_SHIFT(1): 12 + 0 * 9 = 12
+ */
 #define SPTE_LEVEL_SHIFT(level)		__PT_LEVEL_SHIFT(level, SPTE_LEVEL_BITS)
+/*
+ * 在以下使用SPTE_INDEX():
+ *   - arch/x86/kvm/mmu/mmu.c|2372| <<shadow_walk_okay>> iterator->index = SPTE_INDEX(iterator->addr, iterator->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|15| <<tdp_iter_refresh_sptep>> SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|116| <<try_step_side>> if (SPTE_INDEX(iter->gfn << PAGE_SHIFT, iter->level) ==
+ */
 #define SPTE_INDEX(address, level)	__PT_INDEX(address, level, SPTE_LEVEL_BITS)
+/*
+ * 1 << 9 = 512
+ */
 #define SPTE_ENT_PER_PAGE		__PT_ENT_PER_PAGE(SPTE_LEVEL_BITS)
 
 /*
@@ -66,9 +124,20 @@ static_assert(SPTE_TDP_AD_ENABLED == 0);
  * restored only when a write is attempted to the page.  This mask obviously
  * must not overlap the A/D type mask.
  */
+/*
+ * 在以下使用SHADOW_ACC_TRACK_SAVED_BITS_MASK:
+ *   - arch/x86/kvm/mmu/spte.c|568| <<mark_spte_for_access_track>> WARN_ONCE(spte & (SHADOW_ACC_TRACK_SAVED_BITS_MASK <<
+ *   - arch/x86/kvm/mmu/spte.c|572| <<mark_spte_for_access_track>> spte |= (spte & SHADOW_ACC_TRACK_SAVED_BITS_MASK) <<
+ *   - arch/x86/kvm/mmu/spte.h|107| <<SHADOW_ACC_TRACK_SAVED_MASK>> #define SHADOW_ACC_TRACK_SAVED_MASK (SHADOW_ACC_TRACK_SAVED_BITS_MASK << \
+ *   - arch/x86/kvm/mmu/spte.h|666| <<restore_acc_track_spte>> & SHADOW_ACC_TRACK_SAVED_BITS_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|669| <<restore_acc_track_spte>> spte &= ~(SHADOW_ACC_TRACK_SAVED_BITS_MASK <<
+ */
 #define SHADOW_ACC_TRACK_SAVED_BITS_MASK (SPTE_EPT_READABLE_MASK | \
 					  SPTE_EPT_EXECUTABLE_MASK)
 #define SHADOW_ACC_TRACK_SAVED_BITS_SHIFT 54
+/*
+ * 把bit0和bit2转移到bit54
+ */
 #define SHADOW_ACC_TRACK_SAVED_MASK	(SHADOW_ACC_TRACK_SAVED_BITS_MASK << \
 					 SHADOW_ACC_TRACK_SAVED_BITS_SHIFT)
 static_assert(!(SPTE_TDP_AD_MASK & SHADOW_ACC_TRACK_SAVED_MASK));
@@ -78,6 +147,13 @@ static_assert(!(SPTE_TDP_AD_MASK & SHADOW_ACC_TRACK_SAVED_MASK));
  * SPTE is write-protected. See is_writable_pte() for details.
  */
 
+/*
+ * 在以下使用DEFAULT_SPTE_HOST_WRITABLE:
+ *   - arch/x86/kvm/mmu/spte.c|745| <<kvm_mmu_reset_all_pte_masks>> shadow_host_writable_mask = DEFAULT_SPTE_HOST_WRITABLE;
+ *
+ * 在以下使用DEFAULT_SPTE_MMU_WRITABLE:
+ *   - arch/x86/kvm/mmu/spte.c|746| <<kvm_mmu_reset_all_pte_masks>> shadow_mmu_writable_mask = DEFAULT_SPTE_MMU_WRITABLE;
+ */
 /* Bits 9 and 10 are ignored by all non-EPT PTEs. */
 #define DEFAULT_SPTE_HOST_WRITABLE	BIT_ULL(9)
 #define DEFAULT_SPTE_MMU_WRITABLE	BIT_ULL(10)
@@ -195,6 +271,14 @@ extern u64 __read_mostly shadow_acc_track_mask;
  */
 extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
 
+/*
+ * 在以下使用SHADOW_NONPRESENT_OR_RSVD_MASK_LEN:
+ *   - arch/x86/kvm/mmu/mmu.c|310| <<get_mmio_spte_gfn>> gpa |= (spte >> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN) & shadow_nonpresent_or_rsvd_mask;
+ *   - arch/x86/kvm/mmu/spte.c|645| <<make_mmio_spte>> spte |= (gpa & shadow_nonpresent_or_rsvd_mask) << SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;
+ *   - arch/x86/kvm/mmu/spte.c|983| <<kvm_mmu_set_mmio_spte_mask>> if (WARN_ON(mmio_value & (shadow_nonpresent_or_rsvd_mask << SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)))
+ *   - arch/x86/kvm/mmu/spte.c|1150| <<kvm_mmu_reset_all_pte_masks>> if (boot_cpu_has_bug(X86_BUG_L1TF) && !WARN_ON_ONCE(boot_cpu_data.x86_cache_bits >= 52 - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)) {
+ *   - arch/x86/kvm/mmu/spte.c|1152| <<kvm_mmu_reset_all_pte_masks>> low_phys_bits = boot_cpu_data.x86_cache_bits - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;
+ */
 /*
  * The number of high-order 1 bits to use in the mask above.
  */
@@ -212,6 +296,17 @@ extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
  *
  * Only used by the TDP MMU.
  */
+/*
+ * 在以下使用REMOVED_SPTE:
+ *   - arch/x86/kvm/mmu/spte.h|302| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/mmu.c|3452| <<fast_page_fault>> spte = REMOVED_SPTE;
+ *   - arch/x86/kvm/mmu/spte.c|993| <<kvm_mmu_set_mmio_spte_mask>> WARN_ON(mmio_value && (REMOVED_SPTE & mmio_mask) == mmio_value))
+ *   - arch/x86/kvm/mmu/spte.h|306| <<is_removed_spte>> return spte == REMOVED_SPTE;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|368| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|416| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, REMOVED_SPTE, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|419| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, REMOVED_SPTE, level, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|608| <<tdp_mmu_zap_spte_atomic>> ret = __tdp_mmu_set_spte_atomic(iter, REMOVED_SPTE);
+ */
 #define REMOVED_SPTE	(SHADOW_NONPRESENT_VALUE | 0x5a0ULL)
 
 /* Removed SPTEs must not be misconstrued as shadow present PTEs. */
@@ -222,9 +317,33 @@ static inline bool is_removed_spte(u64 spte)
 	return spte == REMOVED_SPTE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|287| <<kvm_flush_remote_tlbs_sptep>> gfn_t gfn = kvm_mmu_page_get_gfn(sp, spte_index(sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|1121| <<rmap_remove>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|1615| <<__rmap_add>> kvm_mmu_page_set_translation(sp, spte_index(spte), gfn, access);
+ *   - arch/x86/kvm/mmu/mmu.c|1756| <<mark_unsync>> if (__test_and_set_bit(spte_index(spte), sp->unsync_child_bitmap))
+ *   - arch/x86/kvm/mmu/mmu.c|2312| <<kvm_mmu_child_role>> role.quadrant = spte_index(sptep) & 1;
+ *   - arch/x86/kvm/mmu/mmu.c|2936| <<mmu_set_spte>> kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
+ *   - arch/x86/kvm/mmu/mmu.c|2952| <<direct_pte_prefetch_many>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(start));
+ *   - arch/x86/kvm/mmu/mmu.c|2978| <<__direct_pte_prefetch>> i = spte_index(sptep) & ~(PTE_PREFETCH_NUM - 1);
+ *   - arch/x86/kvm/mmu/mmu.c|6540| <<shadow_mmu_get_sp_for_split>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|6541| <<shadow_mmu_get_sp_for_split>> access = kvm_mmu_page_get_access(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|6615| <<shadow_mmu_try_split_huge_page>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|607| <<FNAME>> i = spte_index(sptep) & ~(PTE_PREFETCH_NUM - 1);
+ *
+ * sptep是指向的一个page table中的一个entry的指针
+ * entry的地址都是sizeof(*sptep) aligned的
+ * 所以是计算sptep所在的table的index
+ */
 /* Get an SPTE's index into its parent's page table (and the spt array). */
 static inline int spte_index(u64 *sptep)
 {
+	/*
+	 * sptep是指向的一个page table中的一个entry
+	 * entry的地址都是sizeof(*sptep) aligned的
+	 * 所以是计算sptep所在的table的index
+	 */
 	return ((unsigned long)sptep / sizeof(*sptep)) & (SPTE_ENT_PER_PAGE - 1);
 }
 
@@ -238,6 +357,11 @@ static inline int spte_index(u64 *sptep)
  */
 extern u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.h|285| <<spte_to_child_sp>> return to_shadow_page(spte & SPTE_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/spte.h|290| <<sptep_to_sp>> return to_shadow_page(__pa(sptep));
+ */
 static inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)
 {
 	struct page *page = pfn_to_page((shadow_page) >> PAGE_SHIFT);
@@ -245,16 +369,41 @@ static inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)
 	return (struct kvm_mmu_page *)page_private(page);
 }
 
+/*
+ * 根据一个spte中的内容找到下一级页表页的kvm_mmu_page
+ */
 static inline struct kvm_mmu_page *spte_to_child_sp(u64 spte)
 {
 	return to_shadow_page(spte & SPTE_BASE_ADDR_MASK);
 }
 
+/*
+ * 给一个指向一个entry的指针
+ * 获得这个entry所在的页面的kvm_mmu_page
+ */
 static inline struct kvm_mmu_page *sptep_to_sp(u64 *sptep)
 {
 	return to_shadow_page(__pa(sptep));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3549| <<mmu_free_root_page>> sp = root_to_sp(*root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|3605| <<kvm_mmu_free_roots>> } else if (root_to_sp(mmu->root.hpa)) {
+ *   - arch/x86/kvm/mmu/mmu.c|3649| <<kvm_mmu_free_guest_mode_roots>> sp = root_to_sp(root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|3994| <<is_unsync_root>> sp = root_to_sp(root);
+ *   - arch/x86/kvm/mmu/mmu.c|4028| <<kvm_mmu_sync_roots>> sp = root_to_sp(root);
+ *   - arch/x86/kvm/mmu/mmu.c|4475| <<is_page_fault_stale>> struct kvm_mmu_page *sp = root_to_sp(vcpu->arch.mmu->root.hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4687| <<is_root_usable>> sp = root_to_sp(root->hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4764| <<fast_pgd_switch>> if (VALID_PAGE(mmu->root.hpa) && !root_to_sp(mmu->root.hpa))
+ *   - arch/x86/kvm/mmu/mmu.c|4811| <<kvm_mmu_new_pgd>> struct kvm_mmu_page *sp = root_to_sp(vcpu->arch.mmu->root.hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|5680| <<is_obsolete_root>> sp = root_to_sp(root_hpa);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|688| <<tdp_mmu_for_each_pte>> for_each_tdp_pte(_iter, root_to_sp(_mmu->root.hpa), _start, _end)
+ *
+ * 输入的是一个hpa, 地址是一个页面的
+ * 这个页面我们不需要低12位
+ * 通过这个页面的物理地址,获得kvm_mmu_page
+ */
 static inline struct kvm_mmu_page *root_to_sp(hpa_t root)
 {
 	if (kvm_mmu_is_dummy_root(root))
@@ -267,14 +416,41 @@ static inline struct kvm_mmu_page *root_to_sp(hpa_t root)
 	return spte_to_child_sp(root);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2477| <<mmu_page_zap_pte>> } else if (is_mmio_spte(kvm, pte)) {
+ *   - arch/x86/kvm/mmu/mmu.c|4177| <<handle_mmio_page_fault>> if (is_mmio_spte(vcpu->kvm, spte)) {
+ *   - arch/x86/kvm/mmu/mmu.c|4817| <<sync_mmio_spte>> if (unlikely(is_mmio_spte(vcpu->kvm, *sptep))) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|498| <<handle_changed_spte>> if (WARN_ON_ONCE(!is_mmio_spte(kvm, old_spte) && !is_mmio_spte(kvm, new_spte) && !is_removed_spte(new_spte)))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|499| <<handle_changed_spte>> if (WARN_ON_ONCE(!is_mmio_spte(kvm, old_spte) && !is_mmio_spte(kvm, new_spte) && !is_removed_spte(new_spte)))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1054| <<tdp_mmu_map_handle_target_level>> if (unlikely(is_mmio_spte(vcpu->kvm, new_spte))) {
+ */
 static inline bool is_mmio_spte(struct kvm *kvm, u64 spte)
 {
+	/*
+	 * 在以下使用shadow_mmio_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|393| <<kvm_mmu_set_mmio_spte_mask>> shadow_mmio_mask = mmio_mask;
+	 *   - arch/x86/kvm/mmu/spte.h|272| <<is_mmio_spte>> return (spte & shadow_mmio_mask) == kvm->arch.shadow_mmio_value && likely(enable_mmio_caching);
+	 */
 	return (spte & shadow_mmio_mask) == kvm->arch.shadow_mmio_value &&
 	       likely(enable_mmio_caching);
 }
 
+/*
+ * 特别多的调用
+ */
 static inline bool is_shadow_present_pte(u64 pte)
 {
+	/*
+	 * 注释:
+	 * A MMU present SPTE is backed by actual memory and may or may not be present
+	 * in hardware.  E.g. MMIO SPTEs are not considered present.  Use bit 11, as it
+	 * is ignored by all flavors of SPTEs and checking a low bit often generates
+	 * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
+	 * enough that the improved code generation is noticeable in KVM's footprint.
+	 *
+	 * mmio是不设置这个bit=11
+	 */
 	return !!(pte & SPTE_MMU_PRESENT_MASK);
 }
 
@@ -307,6 +483,12 @@ static inline bool spte_ad_enabled(u64 spte)
 	return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_DISABLED;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1307| <<__rmap_clear_dirty>> if (spte_ad_need_write_protect(*sptep))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1567| <<clear_dirty_gfn_range>> spte_ad_need_write_protect(iter.old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1618| <<clear_dirty_pt_masked>> spte_ad_need_write_protect(iter.old_spte));
+ */
 static inline bool spte_ad_need_write_protect(u64 spte)
 {
 	KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
@@ -318,18 +500,53 @@ static inline bool spte_ad_need_write_protect(u64 spte)
 	return (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_ENABLED;
 }
 
+/*
+ * 对于普通的页表:
+ * bit-5: Accessed: indicates whether software has accessed the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ * bit-6: Dirty: indicates whether software has written to the 4-KByte page
+ *        referenced by this entry (see Section 4.8)
+ *
+ * 对于EPT:
+ * - bit-8: If bit 6 (ignore guest PAT) of EPTP is 1, accessed flag for EPT;
+ *   indicates whether software has accessed the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ * - bit-9: If bit 6 (ignore guest PAT) of EPTP is 1, dirty flag for EPT;
+ *   indicates whether software has written to the 4-KByte page referenced by
+ *   this entry (see Section 29.3.5).  Ignored if bit 6 of EPTP is 0.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|374| <<make_spte>> spte |= spte_shadow_accessed_mask(spte);
+ *   - arch/x86/kvm/mmu/spte.h|515| <<is_accessed_spte>> u64 accessed_mask = spte_shadow_accessed_mask(spte);
+ */
 static inline u64 spte_shadow_accessed_mask(u64 spte)
 {
 	KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
 	return spte_ad_enabled(spte) ? shadow_accessed_mask : 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|443| <<make_spte>> spte |= spte_shadow_dirty_mask(spte);
+ *   - arch/x86/kvm/mmu/spte.h|530| <<is_dirty_spte>> u64 dirty_mask = spte_shadow_dirty_mask(spte);
+ *
+ * 支持ad, 返回shadow_dirty_mask
+ * 不支持ad, 返回0
+ */
 static inline u64 spte_shadow_dirty_mask(u64 spte)
 {
 	KVM_MMU_WARN_ON(!is_shadow_present_pte(spte));
 	return spte_ad_enabled(spte) ? shadow_dirty_mask : 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3478| <<fast_page_fault>> if (unlikely(!kvm_ad_enabled()) && is_access_track_spte(spte))
+ *   - arch/x86/kvm/mmu/spte.c|330| <<spte_has_volatile_bits>> if (is_access_track_spte(spte))
+ *   - arch/x86/kvm/mmu/spte.c|469| <<make_spte_executable>> bool is_access_track = is_access_track_spte(spte);
+ *   - arch/x86/kvm/mmu/spte.c|563| <<mark_spte_for_access_track>> if (is_access_track_spte(spte))
+ *   - arch/x86/kvm/mmu/spte.h|546| <<is_accessed_spte>> return accessed_mask ? spte & accessed_mask : !is_access_track_spte(spte);
+ */
 static inline bool is_access_track_spte(u64 spte)
 {
 	return !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;
@@ -345,16 +562,58 @@ static inline bool is_last_spte(u64 pte, int level)
 	return (level == PG_LEVEL_4K) || is_large_pte(pte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3384| <<is_access_allowed>> return is_executable_pte(spte);
+ *   - arch/x86/kvm/mmu/mmutrace.h|355| <<__field>> __entry->x = is_executable_pte(__entry->spte);
+ */
 static inline bool is_executable_pte(u64 spte)
 {
+	/*
+	 * 普通page table entry的bit=63 (XD):
+	 * If IA32_EFER.NXE = 1, execute-disable (if 1, instruction fetches are not
+	 * allowed from the 4-KByte page controlled by this entry; see Section 4.6);
+	 * otherwise, reserved (must be 0)
+	 *
+	 * 普通pte支持nx bit, ept不支持nx bit
+	 * 普通pte不支持x bit, ept支持x bit
+	 */
 	return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|514| <<mmu_spte_update_no_track>> WARN_ON_ONCE(spte_to_pfn(old_spte) != spte_to_pfn(new_spte));
+ *   - arch/x86/kvm/mmu/mmu.c|552| <<mmu_spte_update>> kvm_set_pfn_accessed(spte_to_pfn(old_spte));
+ *   - arch/x86/kvm/mmu/mmu.c|557| <<mmu_spte_update>> kvm_set_pfn_dirty(spte_to_pfn(old_spte));
+ *   - arch/x86/kvm/mmu/mmu.c|587| <<mmu_spte_clear_track_bits>> pfn = spte_to_pfn(old_spte);
+ *   - arch/x86/kvm/mmu/mmu.c|639| <<mmu_spte_age>> kvm_set_pfn_dirty(spte_to_pfn(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|1288| <<spte_wrprot_for_clear_dirty>> kvm_set_pfn_dirty(spte_to_pfn(*sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|2906| <<mmu_set_spte>> } else if (pfn != spte_to_pfn(*sptep)) {
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|964| <<FNAME(sync_spte)>> spte_to_pfn(spte), spte, true, false,
+ *   - arch/x86/kvm/mmu/tdp_iter.c|71| <<spte_to_child_pt>> return (tdp_ptep_t)__va(spte_to_pfn(spte) << PAGE_SHIFT);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|449| <<handle_changed_spte>> bool pfn_changed = spte_to_pfn(old_spte) != spte_to_pfn(new_spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|516| <<handle_changed_spte>> kvm_set_pfn_dirty(spte_to_pfn(old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|530| <<handle_changed_spte>> kvm_set_pfn_accessed(spte_to_pfn(old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1255| <<age_gfn_range>> kvm_set_pfn_dirty(spte_to_pfn(iter->old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1627| <<clear_dirty_pt_masked>> kvm_set_pfn_dirty(spte_to_pfn(iter.old_spte));
+ */
 static inline kvm_pfn_t spte_to_pfn(u64 pte)
 {
 	return (pte & SPTE_BASE_ADDR_MASK) >> PAGE_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|550| <<mmu_spte_update>> if (is_accessed_spte(old_spte) && !is_accessed_spte(new_spte)) {
+ *   - arch/x86/kvm/mmu/mmu.c|598| <<mmu_spte_clear_track_bits>> if (is_accessed_spte(old_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|627| <<mmu_spte_age>> if (!is_accessed_spte(spte))
+ *   - arch/x86/kvm/mmu/mmu.c|1598| <<kvm_test_age_rmap>> if (is_accessed_spte(*sptep))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|528| <<handle_changed_spte>> if (was_leaf && is_accessed_spte(old_spte) &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|529| <<handle_changed_spte>> (!is_present || !is_accessed_spte(new_spte) || pfn_changed))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1240| <<age_gfn_range>> if (!is_accessed_spte(iter->old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1276| <<test_age_gfn>> return is_accessed_spte(iter->old_spte);
+ */
 static inline bool is_accessed_spte(u64 spte)
 {
 	u64 accessed_mask = spte_shadow_accessed_mask(spte);
@@ -363,13 +622,31 @@ static inline bool is_accessed_spte(u64 spte)
 			     : !is_access_track_spte(spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|555| <<mmu_spte_update>> if (is_dirty_spte(old_spte) && !is_dirty_spte(new_spte)) {
+ *   - arch/x86/kvm/mmu/mmu.c|601| <<mmu_spte_clear_track_bits>> if (is_dirty_spte(old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|514| <<handle_changed_spte>> if (was_leaf && is_dirty_spte(old_spte) &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|515| <<handle_changed_spte>> (!is_present || !is_dirty_spte(new_spte) || pfn_changed))
+ */
 static inline bool is_dirty_spte(u64 spte)
 {
+	/*
+	 * 支持ad, 返回shadow_dirty_mask
+	 * 不支持ad, 返回0
+	 */
 	u64 dirty_mask = spte_shadow_dirty_mask(spte);
 
 	return dirty_mask ? spte & dirty_mask : spte & PT_WRITABLE_MASK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4164| <<get_mmio_spte>> pr_err("------ spte = 0x%llx level = %d, rsvd bits = 0x%llx", sptes[level], level, get_rsvd_bits(rsvd_check, sptes[level], level));
+ *   - arch/x86/kvm/mmu/spte.c|451| <<make_spte>> WARN_ONCE(is_rsvd_spte(&vcpu->arch.mmu->shadow_zero_check, spte, level),
+ *                            "spte = 0x%llx, level = %d, rsvd bits = 0x%llx", spte, level, get_rsvd_bits(&vcpu->arch.mmu->shadow_zero_check, spte, level));
+ *   - arch/x86/kvm/mmu/spte.h|398| <<__is_rsvd_bits_set>> return pte & get_rsvd_bits(rsvd_check, pte, level);
+ */
 static inline u64 get_rsvd_bits(struct rsvd_bits_validate *rsvd_check, u64 pte,
 				int level)
 {
@@ -378,18 +655,40 @@ static inline u64 get_rsvd_bits(struct rsvd_bits_validate *rsvd_check, u64 pte,
 	return rsvd_check->rsvd_bits_mask[bit7][level-1];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|147| <<FNAME(is_rsvd_bits_set)>> return __is_rsvd_bits_set(&mmu->guest_rsvd_check, gpte, level) || FNAME(is_bad_mt_xwr)(&mmu->guest_rsvd_check, gpte);
+ *   - arch/x86/kvm/mmu/spte.h|423| <<is_rsvd_spte>> return __is_bad_mt_xwr(rsvd_check, spte) || __is_rsvd_bits_set(rsvd_check, spte, level);
+ */
 static inline bool __is_rsvd_bits_set(struct rsvd_bits_validate *rsvd_check,
 				      u64 pte, int level)
 {
 	return pte & get_rsvd_bits(rsvd_check, pte, level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|141| <<FNAME>> return __is_bad_mt_xwr(rsvd_check, gpte);
+ *   - arch/x86/kvm/mmu/spte.h|410| <<is_rsvd_spte>> return __is_bad_mt_xwr(rsvd_check, spte) || __is_rsvd_bits_set(rsvd_check, spte, level);
+ */
 static inline bool __is_bad_mt_xwr(struct rsvd_bits_validate *rsvd_check,
 				   u64 pte)
 {
 	return rsvd_check->bad_mt_xwr & BIT_ULL(pte & 0x3f);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4156| <<get_mmio_spte>> reserved |= is_rsvd_spte(rsvd_check, sptes[level], level);
+ *   - arch/x86/kvm/mmu/spte.c|449| <<make_spte>> WARN_ONCE(is_rsvd_spte(&vcpu->arch.mmu->shadow_zero_check, spte, level),
+ *                                                          "spte = 0x%llx, level = %d, rsvd bits = 0x%llx", spte, level,
+ *                                                          get_rsvd_bits(&vcpu->arch.mmu->shadow_zero_check, spte, level));
+ *
+ * struct rsvd_bits_validate {
+ *     u64 rsvd_bits_mask[2][PT64_ROOT_MAX_LEVEL];
+ *     u64 bad_mt_xwr;
+ * };
+ */
 static __always_inline bool is_rsvd_spte(struct rsvd_bits_validate *rsvd_check,
 					 u64 spte, int level)
 {
@@ -463,14 +762,76 @@ static __always_inline bool is_rsvd_spte(struct rsvd_bits_validate *rsvd_check,
  * cleared when an SPTE is first faulted in from non-present and then remains
  * immutable.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|542| <<mmu_spte_update>> if (is_mmu_writable_spte(old_spte) && !is_writable_pte(new_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|638| <<mmu_spte_age>> if (is_writable_pte(spte))
+ *   - arch/x86/kvm/mmu/mmu.c|1250| <<spte_write_protect>> if (!is_writable_pte(spte) && !(pt_protect && is_mmu_writable_spte(spte)))
+ *   - arch/x86/kvm/mmu/mmu.c|3375| <<fast_pf_fix_direct_spte>> if (is_writable_pte(new_spte) && !is_writable_pte(old_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|3387| <<is_access_allowed>> return is_writable_pte(spte);
+ *   - arch/x86/kvm/mmu/spte.c|327| <<spte_has_volatile_bits>> if (!is_writable_pte(spte) && is_mmu_writable_spte(spte))
+ *   - arch/x86/kvm/mmu/spte.c|335| <<spte_has_volatile_bits>> if (!(spte & shadow_accessed_mask) || (is_writable_pte(spte) && !(spte & shadow_dirty_mask)))
+ *   - arch/x86/kvm/mmu/spte.c|426| <<make_spte>> if (is_writable_pte(old_spte))
+ *   - arch/x86/kvm/mmu/spte.h|493| <<check_spte_writable_invariants>> WARN_ONCE(is_writable_pte(spte), KBUILD_MODNAME ": Writable SPTE is not MMU-writable: %llx", spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1254| <<age_gfn_range>> if (is_writable_pte(iter->old_spte))
+ *
+ * 有三种writable.
+ * 1. MMU-writable: shadow_mmu_writable_mask
+ * 2. Host-eritable: shadow_host_writable_mask
+ * 3. Writable SPTE: is_writable_pte(spte)
+ *
+ * 有三个mask:
+ * - shadow_host_writable_mask:
+ *   backend的page在host上是否writable(EPT用EPT_SPTE_HOST_WRITABLE, 普通用DEFAULT_SPTE_HOST_WRITABLE)
+ *   选取的是ept和普通分别不用的bit
+ *   更多是GUP从host获得指示
+ * - shadow_mmu_writable_mask
+ *   指示这个pte的page是不是writable的, 不能简单用bit=1, 没法分清是真的不能还是mmio或是别的
+ *   设置这个mask是告诉user这个pte一定是writable
+ *   EPT用EPT_SPTE_MMU_WRITABLE, 普通用DEFAULT_SPTE_MMU_WRITABLE
+ * - pte & PT_WRITABLE_MASK (看看bit设置了吗)
+ *   这个单纯是看pte的bit=1
+ *
+ *  shadow_mmu_writable_mask, aka MMU-writable -
+ *    Cleared on SPTEs that KVM is currently write-protecting for shadow paging
+ *    purposes (case 2 above). --> 保护影子页表
+ *
+ *  shadow_host_writable_mask, aka Host-writable -
+ *    Cleared on SPTEs that are not host-writable (case 3 above) --> backend memslot不可写
+ *
+ * 查看is_writable_pte()的注释
+ */
 static inline bool is_writable_pte(unsigned long pte)
 {
 	return pte & PT_WRITABLE_MASK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|502| <<mmu_spte_update_no_track>> check_spte_writable_invariants(new_spte);
+ *   - arch/x86/kvm/mmu/spte.c|566| <<mark_spte_for_access_track>> check_spte_writable_invariants(spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|484| <<handle_changed_spte>> check_spte_writable_invariants(new_spte);
+ */
 /* Note: spte must be a shadow-present leaf SPTE. */
 static inline void check_spte_writable_invariants(u64 spte)
 {
+	/*
+	 * 在以下设置shadow_mmu_writable_mask:
+	 *   - arch/x86/kvm/mmu/spte.c|614| <<kvm_mmu_set_ept_masks>> shadow_mmu_writable_mask = EPT_SPTE_MMU_WRITABLE;
+	 *   - arch/x86/kvm/mmu/spte.c|684| <<kvm_mmu_reset_all_pte_masks>> shadow_mmu_writable_mask = DEFAULT_SPTE_MMU_WRITABLE;
+	 * 在以下使用shadow_mmu_writable_mask:
+	 *   - arch/x86/kvm/mmu/mmu.c|1255| <<spte_write_protect>> spte &= ~shadow_mmu_writable_mask;
+	 *   - arch/x86/kvm/mmu/spte.c|356| <<make_spte>> spte |= PT_WRITABLE_MASK | shadow_mmu_writable_mask;
+	 *   - arch/x86/kvm/mmu/spte.c|376| <<make_spte>> spte &= ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+	 *   - arch/x86/kvm/mmu/spte.h|488| <<check_spte_writable_invariants>> if (spte & shadow_mmu_writable_mask)
+	 *   - arch/x86/kvm/mmu/spte.h|499| <<is_mmu_writable_spte>> return spte & shadow_mmu_writable_mask;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1736| <<write_protect_gfn>> new_spte = iter.old_spte & ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+	 *
+	 * 有三种writable.
+	 * 1. MMU-writable: shadow_mmu_writable_mask
+	 * 2. Host-eritable: shadow_host_writable_mask
+	 * 3. Writable SPTE: is_writable_pte(spte)
+	 */
 	if (spte & shadow_mmu_writable_mask)
 		WARN_ONCE(!(spte & shadow_host_writable_mask),
 			  KBUILD_MODNAME ": MMU-writable SPTE is not Host-writable: %llx",
@@ -480,11 +841,49 @@ static inline void check_spte_writable_invariants(u64 spte)
 			  KBUILD_MODNAME ": Writable SPTE is not MMU-writable: %llx", spte);
 }
 
+/*
+ * 有三种writable.
+ * 1. MMU-writable: shadow_mmu_writable_mask
+ * 2. Host-eritable: shadow_host_writable_mask
+ * 3. Writable SPTE: is_writable_pte(spte)
+ *
+ * 有三个mask:
+ * - shadow_host_writable_mask:
+ *   backend的page在host上是否writable(EPT用EPT_SPTE_HOST_WRITABLE, 普通用DEFAULT_SPTE_HOST_WRITABLE)
+ *   选取的是ept和普通分别不用的bit
+ *   更多是GUP从host获得指示
+ * - shadow_mmu_writable_mask
+ *   指示这个pte的page是不是writable的, 不能简单用bit=1, 没法分清是真的不能还是mmio或是别的
+ *   设置这个mask是告诉user这个pte一定是writable
+ *   EPT用EPT_SPTE_MMU_WRITABLE, 普通用DEFAULT_SPTE_MMU_WRITABLE
+ * - pte & PT_WRITABLE_MASK (看看bit设置了吗)
+ *   这个单纯是看pte的bit=1
+ *
+ *  shadow_mmu_writable_mask, aka MMU-writable -
+ *    Cleared on SPTEs that KVM is currently write-protecting for shadow paging
+ *    purposes (case 2 above). --> 保护影子页表
+ *
+ *  shadow_host_writable_mask, aka Host-writable -
+ *    Cleared on SPTEs that are not host-writable (case 3 above) --> backend memslot不可写
+ *
+ * 查看is_writable_pte()的注释
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|541| <<mmu_spte_update>> if (is_mmu_writable_spte(old_spte) && !is_writable_pte(new_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|1251| <<spte_write_protect>> if (!is_writable_pte(spte) && !(pt_protect && is_mmu_writable_spte(spte)))
+ *   - arch/x86/kvm/mmu/mmu.c|3492| <<fast_page_fault>> if (fault->write && is_mmu_writable_spte(spte)) {
+ *   - arch/x86/kvm/mmu/spte.c|327| <<spte_has_volatile_bits>> if (!is_writable_pte(spte) && is_mmu_writable_spte(spte))
+ */
 static inline bool is_mmu_writable_spte(u64 spte)
 {
 	return spte & shadow_mmu_writable_mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|330| <<check_mmio_spte>> spte_gen = get_mmio_spte_generation(spte);
+ *   - arch/x86/kvm/mmu/mmutrace.h|226| <<__field>> __entry->gen = get_mmio_spte_generation(spte);
+ */
 static inline u64 get_mmio_spte_generation(u64 spte)
 {
 	u64 gen;
@@ -507,6 +906,11 @@ u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled);
 u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access);
 u64 mark_spte_for_access_track(u64 spte);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3479| <<fast_page_fault>> new_spte = restore_acc_track_spte(new_spte);
+ *   - arch/x86/kvm/mmu/spte.c|472| <<make_spte_executable>> spte = restore_acc_track_spte(spte);
+ */
 /* Restore an acc-track PTE back to a regular PTE */
 static inline u64 restore_acc_track_spte(u64 spte)
 {
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index 36539c1b3..60adc90d7 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -1025,6 +1025,11 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 	if (WARN_ON_ONCE(sp->role.level != fault->goal_level))
 		return RET_PF_RETRY;
 
+	/*
+	 * 在以下调用make_mmio_spte():
+	 *   - arch/x86/kvm/mmu/mmu.c|295| <<mark_mmio_spte>> u64 spte = make_mmio_spte(vcpu, gfn, access);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1029| <<tdp_mmu_map_handle_target_level>> new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
+	 */
 	if (unlikely(!fault->slot))
 		new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
 	else
@@ -1102,6 +1107,10 @@ static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
  * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
  * page tables and SPTEs to translate the faulting guest physical address.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4632| <<kvm_tdp_mmu_page_fault>> r = kvm_tdp_mmu_map(vcpu, fault);
+ */
 int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index c95d3900f..e8351c013 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -5135,6 +5135,12 @@ static __init void svm_adjust_mmio_mask(void)
 	 */
 	mask = (mask_bit < 52) ? rsvd_bits(mask_bit, 51) | PT_PRESENT_MASK : 0;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/spte.c|476| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE, VMX_EPT_RWX_MASK | VMX_EPT_SUPPRESS_VE_BIT, 0);
+	 *   - arch/x86/kvm/mmu/spte.c|544| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+	 *   - arch/x86/kvm/svm/svm.c|5138| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+	 */
 	kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
 }
 
diff --git a/arch/x86/kvm/vmx/posted_intr.c b/arch/x86/kvm/vmx/posted_intr.c
index ec08fa3ca..fab14817a 100644
--- a/arch/x86/kvm/vmx/posted_intr.c
+++ b/arch/x86/kvm/vmx/posted_intr.c
@@ -12,6 +12,56 @@
 #include "trace.h"
 #include "vmx.h"
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8314| <<vmx_hardware_unsetup>> kvm_set_posted_intr_wakeup_handler(NULL);
+ *   - arch/x86/kvm/vmx/vmx.c|8624| <<vmx_hardware_setup>> kvm_set_posted_intr_wakeup_handler(pi_wakeup_handler);
+ *
+ * 311 #if IS_ENABLED(CONFIG_KVM)
+ * 312 static void dummy_handler(void) {}
+ * 313 static void (*kvm_posted_intr_wakeup_handler)(void) = dummy_handler;
+ * 314
+ * 315 void kvm_set_posted_intr_wakeup_handler(void (*handler)(void))
+ * 316 {
+ * 317         if (handler)
+ * 318                 kvm_posted_intr_wakeup_handler = handler;
+ * 319         else {
+ * 320                 kvm_posted_intr_wakeup_handler = dummy_handler;
+ * 321                 synchronize_rcu();
+ * 322         }
+ * 323 }
+ * 324 EXPORT_SYMBOL_GPL(kvm_set_posted_intr_wakeup_handler);
+ * 325
+ * 326 //
+ * 327 // Handler for POSTED_INTERRUPT_VECTOR.
+ * 328 //
+ * 329 DEFINE_IDTENTRY_SYSVEC_SIMPLE(sysvec_kvm_posted_intr_ipi)
+ * 330 {
+ * 331         apic_eoi();
+ * 332         inc_irq_stat(kvm_posted_intr_ipis);
+ * 333 }
+ * 334
+ * 335 //
+ * 336 // Handler for POSTED_INTERRUPT_WAKEUP_VECTOR.
+ * 337 //
+ * 338 DEFINE_IDTENTRY_SYSVEC(sysvec_kvm_posted_intr_wakeup_ipi)
+ * 339 {
+ * 340         apic_eoi();
+ * 341         inc_irq_stat(kvm_posted_intr_wakeup_ipis);
+ * 342         kvm_posted_intr_wakeup_handler();
+ * 343 }
+ * 344
+ * 345 //
+ * 346 // Handler for POSTED_INTERRUPT_NESTED_VECTOR.
+ * 347 //
+ * 348 DEFINE_IDTENTRY_SYSVEC_SIMPLE(sysvec_kvm_posted_intr_nested_ipi)
+ * 349 {
+ * 350         apic_eoi();
+ * 351         inc_irq_stat(kvm_posted_intr_nested_ipis);
+ * 352 }
+ * 353 #endif
+ */
+
 /*
  * Maintain a per-CPU list of vCPUs that need to be awakened by wakeup_handler()
  * when a WAKEUP_VECTOR interrupted is posted.  vCPUs are added to the list when
@@ -21,6 +71,13 @@
  * wake the target vCPUs.  vCPUs are removed from the list and the notification
  * vector is reset when the vCPU is scheduled in.
  */
+/*
+ * 在以下使用wakeup_vcpus_on_cpu:
+ *   - arch/x86/kvm/vmx/posted_intr.c|24| <<global>> static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|157| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|221| <<pi_wakeup_handler>> struct list_head *wakeup_list = &per_cpu(wakeup_vcpus_on_cpu, cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|236| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
+ */
 static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
 /*
  * Protect the per-CPU list with a per-CPU spinlock to handle task migration.
@@ -29,8 +86,26 @@ static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
  * CPU.  IRQs must be disabled when taking this lock, otherwise deadlock will
  * occur if a wakeup IRQ arrives and attempts to acquire the lock.
  */
+/*
+ * 在以下使用wakeup_vcpus_on_cpu_lock:
+ *   - arch/x86/kvm/vmx/posted_intr.c|32| <<global>> static DEFINE_PER_CPU(raw_spinlock_t, wakeup_vcpus_on_cpu_lock);
+ *   - arch/x86/kvm/vmx/posted_intr.c|92| <<vmx_vcpu_pi_load>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|94| <<vmx_vcpu_pi_load>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_enable_wakeup_handler>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|158| <<pi_enable_wakeup_handler>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|222| <<pi_wakeup_handler>> raw_spinlock_t *spinlock = &per_cpu(wakeup_vcpus_on_cpu_lock, cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|237| <<pi_init_cpu>> raw_spin_lock_init(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+ */
 static DEFINE_PER_CPU(raw_spinlock_t, wakeup_vcpus_on_cpu_lock);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|55| <<vmx_vcpu_pi_load>> struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|148| <<pi_enable_wakeup_handler>> struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|198| <<vmx_vcpu_pi_put>> struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|242| <<pi_has_pending_interrupt>> struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|337| <<vmx_pi_update_irte>> vcpu_info.pi_desc_addr = __pa(vcpu_to_pi_desc(vcpu));
+ */
 static inline struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)
 {
 	return &(to_vmx(vcpu)->pi_desc);
@@ -50,6 +125,10 @@ static int pi_try_set_control(struct pi_desc *pi_desc, u64 *pold, u64 new)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1491| <<vmx_vcpu_load>> vmx_vcpu_pi_load(vcpu, cpu);
+ */
 void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -100,6 +179,26 @@ void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 
 	old.control = READ_ONCE(pi_desc->control);
 	do {
+		/*
+		 * 12 struct pi_desc {
+		 * 13         union {
+		 * 14                 u32 pir[8];     // Posted interrupt requested
+		 * 15                 u64 pir64[4];
+		 * 16         };      
+		 * 17         union {
+		 * 18                 struct {
+		 * 19                         u16     notifications; // Suppress and outstanding bits
+		 * 20                         u8      nv;
+		 * 21                         u8      rsvd_2;
+		 * 22                         u32     ndst;
+		 * 23                 };
+		 * 24                 u64 control;
+		 * 25         };      
+		 * 26         u32 rsvd[6];
+		 * 27 } __aligned(64);
+		 *
+		 * struct pi_desc old, new;
+		 */
 		new.control = old.control;
 
 		/*
@@ -143,6 +242,10 @@ static bool vmx_can_use_vtd_pi(struct kvm *kvm)
  * Put the vCPU on this pCPU's list of vCPUs that needs to be awakened and set
  * WAKEUP as the notification vector in the PI descriptor.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|204| <<vmx_vcpu_pi_put>> pi_enable_wakeup_handler(vcpu);
+ */
 static void pi_enable_wakeup_handler(struct kvm_vcpu *vcpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -153,6 +256,13 @@ static void pi_enable_wakeup_handler(struct kvm_vcpu *vcpu)
 	local_irq_save(flags);
 
 	raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+	/*
+	 * 在以下使用wakeup_vcpus_on_cpu:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|24| <<global>> static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|157| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+	 *   - arch/x86/kvm/vmx/posted_intr.c|221| <<pi_wakeup_handler>> struct list_head *wakeup_list = &per_cpu(wakeup_vcpus_on_cpu, cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|236| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
+	 */
 	list_add_tail(&vmx->pi_wakeup_list,
 		      &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
 	raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
@@ -218,10 +328,27 @@ void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)
 void pi_wakeup_handler(void)
 {
 	int cpu = smp_processor_id();
+	/*
+	 * 在以下使用wakeup_vcpus_on_cpu:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|24| <<global>> static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|157| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+	 *   - arch/x86/kvm/vmx/posted_intr.c|221| <<pi_wakeup_handler>> struct list_head *wakeup_list = &per_cpu(wakeup_vcpus_on_cpu, cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|236| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
+	 */
 	struct list_head *wakeup_list = &per_cpu(wakeup_vcpus_on_cpu, cpu);
 	raw_spinlock_t *spinlock = &per_cpu(wakeup_vcpus_on_cpu_lock, cpu);
 	struct vcpu_vmx *vmx;
 
+	/*
+	 * 注释
+	 * Maintain a per-CPU list of vCPUs that need to be awakened by wakeup_handler()
+	 * when a WAKEUP_VECTOR interrupted is posted.  vCPUs are added to the list when
+	 * the vCPU is scheduled out and is blocking (e.g. in HLT) with IRQs enabled.
+	 * The vCPUs posted interrupt descriptor is updated at the same time to set its
+	 * notification vector to WAKEUP_VECTOR, so that posted interrupt from devices
+	 * wake the target vCPUs.  vCPUs are removed from the list and the notification
+	 * vector is reset when the vCPU is scheduled in.
+	 */
 	raw_spin_lock(spinlock);
 	list_for_each_entry(vmx, wakeup_list, pi_wakeup_list) {
 
@@ -233,6 +360,13 @@ void pi_wakeup_handler(void)
 
 void __init pi_init_cpu(int cpu)
 {
+	/*
+	 * 在以下使用wakeup_vcpus_on_cpu:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|24| <<global>> static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|157| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list, &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+	 *   - arch/x86/kvm/vmx/posted_intr.c|221| <<pi_wakeup_handler>> struct list_head *wakeup_list = &per_cpu(wakeup_vcpus_on_cpu, cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|236| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
+	 */
 	INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
 	raw_spin_lock_init(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
 }
@@ -269,6 +403,12 @@ void vmx_pi_start_assignment(struct kvm *kvm)
  * @set: set or unset PI
  * returns 0 on success, < 0 on failure
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13619| <<kvm_arch_irq_bypass_add_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm,
+ *   - arch/x86/kvm/x86.c|13644| <<kvm_arch_irq_bypass_del_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);
+ *   - arch/x86/kvm/x86.c|13655| <<kvm_arch_update_irqfd_routing>> return static_call(kvm_x86_pi_update_irte)(kvm, host_irq, guest_irq, set);
+ */
 int vmx_pi_update_irte(struct kvm *kvm, unsigned int host_irq,
 		       uint32_t guest_irq, bool set)
 {
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index b3c83c06f..78a2f13d0 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -4251,6 +4251,11 @@ static int vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
 	if (!r)
 		return 0;
 
+	/*
+	 * 在以下设置kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|2919| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/x86.c|10671| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	/* Note, this is called iff the local APIC is in-kernel. */
 	if (!vcpu->arch.apic->apicv_active)
 		return -1;
@@ -4272,6 +4277,9 @@ static int vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
 	return 0;
 }
 
+/*
+ * arch/x86/kvm/lapic.c|1934| <<__apic_accept_irq>> static_call(kvm_x86_deliver_interrupt)(apic, delivery_mode, trig_mode, vector);
+ */
 void vmx_deliver_interrupt(struct kvm_lapic *apic, int delivery_mode,
 			   int trig_mode, int vector)
 {
@@ -6912,6 +6920,14 @@ void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 		vmx_set_rvi(max_irr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1324| <<apic_has_interrupt_for_ppr>> highest_irr = static_call(kvm_x86_sync_pir_to_irr)(apic->vcpu);
+ *   - arch/x86/kvm/x86.c|5122| <<kvm_vcpu_ioctl_get_lapic>> static_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|10807| <<vcpu_scan_ioapic>> static_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|11090| <<vcpu_enter_guest>> if (kvm_lapic_enabled(vcpu)) static_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|11139| <<vcpu_enter_guest>> if (kvm_lapic_enabled(vcpu)) static_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);
+ */
 int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6928,6 +6944,12 @@ int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
 		 * But on x86 this is just a compiler barrier anyway.
 		 */
 		smp_mb__after_atomic();
+		/*
+		 * struct vcpu_vmx *vmx:
+		 * -> struct pi_desc pi_desc;
+		 *    -> u32 pir[8];     // Posted interrupt requested
+		 *    -> u64 pir64[4];
+		 */
 		got_posted_interrupt =
 			kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
 	} else {
@@ -8391,6 +8413,10 @@ static __init void vmx_setup_user_return_msrs(void)
 		kvm_add_user_return_msr(vmx_uret_msrs_list[i]);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8557| <<vmx_hardware_setup>> vmx_setup_me_spte_mask();
+ */
 static void __init vmx_setup_me_spte_mask(void)
 {
 	u64 me_mask = 0;
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 7b64e271a..432cf17ad 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -322,9 +322,32 @@ struct vcpu_vmx {
 
 	union vmx_exit_reason exit_reason;
 
+	/*
+	 * 在以下使用vcpu_vmx->pi_desc:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|111| <<vcpu_to_pi_desc>> return &(to_vmx(vcpu)->pi_desc);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|340| <<pi_wakeup_handler>> if (pi_test_on(&vmx->pi_desc))
+	 *   - arch/x86/kvm/vmx/vmx.c|4263| <<vmx_deliver_posted_interrupt>> if (pi_test_and_set_pir(vector, &vmx->pi_desc))
+	 *   - arch/x86/kvm/vmx/vmx.c|4267| <<vmx_deliver_posted_interrupt>> if (pi_test_and_set_on(&vmx->pi_desc))
+	 *   - arch/x86/kvm/vmx/vmx.c|4752| <<init_vmcs>> vmcs_write64(POSTED_INTR_DESC_ADDR, __pa((&vmx->pi_desc)));
+	 *   - arch/x86/kvm/vmx/vmx.c|4863| <<__vmx_vcpu_reset>> vmx->pi_desc.nv = POSTED_INTR_VECTOR;
+	 *   - arch/x86/kvm/vmx/vmx.c|4864| <<__vmx_vcpu_reset>> __pi_set_sn(&vmx->pi_desc);
+	 *   - arch/x86/kvm/vmx/vmx.c|6937| <<vmx_sync_pir_to_irr>> if (pi_test_on(&vmx->pi_desc)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6938| <<vmx_sync_pir_to_irr>> pi_clear_on(&vmx->pi_desc);
+	 *   - arch/x86/kvm/vmx/vmx.c|6951| <<vmx_sync_pir_to_irr>> kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
+	 *   - arch/x86/kvm/vmx/vmx.c|6995| <<vmx_apicv_pre_state_restore>> pi_clear_on(&vmx->pi_desc);
+	 *   - arch/x86/kvm/vmx/vmx.c|6996| <<vmx_apicv_pre_state_restore>> memset(vmx->pi_desc.pir, 0, sizeof(vmx->pi_desc.pir));
+	 *   - arch/x86/kvm/vmx/vmx.c|7638| <<vmx_vcpu_create>>WRITE_ONCE(to_kvm_vmx(vcpu->kvm)->pid_table[vcpu->vcpu_id], __pa(&vmx->pi_desc) | PID_TABLE_ENTRY_VALID);
+	 */
 	/* Posted interrupt descriptor */
 	struct pi_desc pi_desc;
 
+	/*
+	 * 在以下使用vcpu_vmx->pi_wakeup_list:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|93| <<vmx_vcpu_pi_load>> list_del(&vmx->pi_wakeup_list);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|156| <<pi_enable_wakeup_handler>> list_add_tail(&vmx->pi_wakeup_list,
+	 *   - arch/x86/kvm/vmx/posted_intr.c|226| <<pi_wakeup_handler>> list_for_each_entry(vmx, wakeup_list, pi_wakeup_list) {
+	 *   - arch/x86/kvm/vmx/vmx.c|7526| <<vmx_vcpu_create>> INIT_LIST_HEAD(&vmx->pi_wakeup_list);
+	 */
 	/* Used if this vCPU is waiting for PI notification wakeup. */
 	struct list_head pi_wakeup_list;
 
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 0763a0f72..61d3372f2 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -5046,6 +5046,10 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	kvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5111| <<kvm_arch_vcpu_put>> kvm_steal_time_set_preempted(vcpu);
+ */
 static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)
 {
 	struct gfn_to_hva_cache *ghc = &vcpu->arch.st.cache;
@@ -6561,6 +6565,15 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 		if (cap->args[0] & ~KVM_X2APIC_API_VALID_FLAGS)
 			break;
 
+		/*
+		 * 在以下使用kvm_arch->x2apic_format:
+		 *   - arch/x86/kvm/irq_comm.c|111| <<kvm_set_msi_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+		 *   - arch/x86/kvm/irq_comm.c|114| <<kvm_set_msi_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+		 *   - arch/x86/kvm/irq_comm.c|128| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+		 *   - arch/x86/kvm/lapic.c|328| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/lapic.c|3095| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/x86.c|6565| <<kvm_vm_ioctl_enable_cap>> if (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS) kvm->arch.x2apic_format = true;
+		 */
 		if (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS)
 			kvm->arch.x2apic_format = true;
 		if (cap->args[0] & KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK)
@@ -9708,6 +9721,11 @@ static void kvm_x86_check_cpu_compat(void *ret)
 	*(int *)ret = kvm_x86_check_processor_compatibility();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5413| <<svm_init>> r = kvm_x86_vendor_init(&svm_init_ops);
+ *   - arch/x86/kvm/vmx/vmx.c|8673| <<vmx_init>> r = kvm_x86_vendor_init(&vt_init_ops);
+ */
 int kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 {
 	u64 host_pat;
@@ -9991,14 +10009,48 @@ static void kvm_pv_kick_cpu_op(struct kvm *kvm, int apicid)
 	kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9998| <<global>> EXPORT_SYMBOL_GPL(kvm_apicv_activated);
+ *   - arch/x86/kvm/ioapic.c|229| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|4410| <<kvm_faultin_pfn>> if (!kvm_apicv_activated(vcpu->kvm))
+ *   - arch/x86/kvm/svm/avic.c|256| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+ *   - arch/x86/kvm/svm/avic.c|290| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/nested.c|1160| <<nested_svm_vmexit>> if (kvm_apicv_activated(vcpu->kvm))
+ *   - arch/x86/kvm/svm/nested.c|1244| <<svm_leave_nested>> if (kvm_apicv_activated(vcpu->kvm))
+ */
 bool kvm_apicv_activated(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons(x86):
+	 *   - arch/x86/kvm/x86.c|9996| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10002| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10025| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10679| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10697| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10706| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 */
 	return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
 }
 EXPORT_SYMBOL_GPL(kvm_apicv_activated);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1636| <<svm_set_vintr>> WARN_ON(kvm_vcpu_apicv_activated(&svm->vcpu));
+ *   - arch/x86/kvm/x86.c|10621| <<__kvm_vcpu_update_apicv>> activate = kvm_vcpu_apicv_activated(vcpu) && (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED);
+ *   - arch/x86/kvm/x86.c|11053| <<vcpu_enter_guest>> WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) && (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));
+ */
 bool kvm_vcpu_apicv_activated(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons(x86):
+	 *   - arch/x86/kvm/x86.c|9996| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10002| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10025| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10679| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10697| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10706| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 */
 	ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
 	ulong vcpu_reasons = static_call(kvm_x86_vcpu_get_apicv_inhibit_reasons)(vcpu);
 
@@ -10006,6 +10058,11 @@ bool kvm_vcpu_apicv_activated(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_apicv_activated);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10025| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+ *   - arch/x86/kvm/x86.c|10681| <<__kvm_set_or_clear_apicv_inhibit>> set_or_clear_apicv_inhibit(&new, reason, set);
+ */
 static void set_or_clear_apicv_inhibit(unsigned long *inhibits,
 				       enum kvm_apicv_inhibit reason, bool set)
 {
@@ -10338,6 +10395,10 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
  * ordering between that side effect, the instruction completing, _and_ the
  * delivery of the asynchronous event.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11033| <<vcpu_enter_guest>> r = kvm_check_and_inject_events(vcpu, &req_immediate_exit);
+ */
 static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 				       bool *req_immediate_exit)
 {
@@ -10502,6 +10563,11 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 		if (r < 0)
 			goto out;
 		if (r) {
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/vmx/nested.c|4937| <<nested_vmx_vmexit>> int irq = kvm_cpu_get_interrupt(vcpu);
+			 *   - arch/x86/kvm/x86.c|10544| <<kvm_check_and_inject_events>> int irq = kvm_cpu_get_interrupt(vcpu);
+			 */
 			int irq = kvm_cpu_get_interrupt(vcpu);
 
 			if (!WARN_ON_ONCE(irq == -1)) {
@@ -10593,11 +10659,24 @@ void kvm_make_scan_ioapic_request_mask(struct kvm *kvm,
 	kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC, vcpu_bitmap);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|313| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/ioapic.c|436| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+ *   - arch/x86/kvm/ioapic.c|773| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/irq_comm.c|409| <<kvm_arch_post_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/lapic.c|504| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+ */
 void kvm_make_scan_ioapic_request(struct kvm *kvm)
 {
 	kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|1161| <<nested_svm_vmexit>> __kvm_vcpu_update_apicv(vcpu);
+ *   - arch/x86/kvm/x86.c|10687| <<kvm_vcpu_update_apicv>> __kvm_vcpu_update_apicv(vcpu);
+ */
 void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -10613,6 +10692,11 @@ void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	activate = kvm_vcpu_apicv_activated(vcpu) &&
 		   (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED);
 
+	/*
+	 * 在以下设置kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|2919| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/x86.c|10671| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (apic->apicv_active == activate)
 		goto out;
 
@@ -10635,6 +10719,10 @@ void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(__kvm_vcpu_update_apicv);
 
+/*
+ * 处理KVM_REQ_APICV_UPDATE:
+ *   - arch/x86/kvm/x86.c|10978| <<vcpu_enter_guest(KVM_REQ_APICV_UPDATE)>> kvm_vcpu_update_apicv(vcpu);
+ */
 static void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 {
 	if (!lapic_in_kernel(vcpu))
@@ -10658,6 +10746,12 @@ static void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	__kvm_vcpu_update_apicv(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|148| <<synic_update_vector>> __kvm_set_or_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_HYPERV, !!hv->synic_auto_eoi_used);
+ *   - arch/x86/kvm/x86.c|10717| <<kvm_set_or_clear_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
+ *   - arch/x86/kvm/x86.c|11956| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_BLOCKIRQ, set);
+ */
 void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				      enum kvm_apicv_inhibit reason, bool set)
 {
@@ -10668,8 +10762,22 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 	if (!(kvm_x86_ops.required_apicv_inhibits & BIT(reason)))
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons(x86):
+	 *   - arch/x86/kvm/x86.c|9996| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10002| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10025| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10679| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10697| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10706| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 */
 	old = new = kvm->arch.apicv_inhibit_reasons;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|10025| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10681| <<__kvm_set_or_clear_apicv_inhibit>> set_or_clear_apicv_inhibit(&new, reason, set);
+	 */
 	set_or_clear_apicv_inhibit(&new, reason, set);
 
 	if (!!old != !!new) {
@@ -10685,6 +10793,18 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 		 * side (handling the request) also prevents other vCPUs from
 		 * servicing the request with a stale apicv_inhibit_reasons.
 		 */
+		/*
+		 * 在以下使用KVM_REQ_APICV_UPDATE:
+		 *   - arch/x86/kvm/lapic.c|3138| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/lapic.c|3452| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|830| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1245| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|4924| <<nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10778| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+		 *   - arch/x86/kvm/x86.c|11018| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu)) kvm_vcpu_update_apicv(vcpu);
+		 *
+		 * 处理的函数是kvm_vcpu_update_apicv(vcpu);
+		 */
 		kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
 		kvm->arch.apicv_inhibit_reasons = new;
 		if (new) {
@@ -10699,6 +10819,11 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/kvm_host.h|2188| <<kvm_set_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
+ *   - arch/x86/include/asm/kvm_host.h|2204| <<kvm_clear_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
+ */
 void kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				    enum kvm_apicv_inhibit reason, bool set)
 {
@@ -10770,6 +10895,10 @@ static void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu)
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11370| <<vcpu_run>> r = vcpu_enter_guest(vcpu);
+ */
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -10920,6 +11049,18 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		if (kvm_check_request(KVM_REQ_HV_STIMER, vcpu))
 			kvm_hv_process_stimers(vcpu);
 #endif
+		/*
+		 * 在以下使用KVM_REQ_APICV_UPDATE:
+		 *   - arch/x86/kvm/lapic.c|3138| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/lapic.c|3452| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|830| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1245| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|4924| <<nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10778| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+		 *   - arch/x86/kvm/x86.c|11018| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu)) kvm_vcpu_update_apicv(vcpu);
+		 *
+		 * 处理的函数是kvm_vcpu_update_apicv(vcpu);
+		 */
 		if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
 			kvm_vcpu_update_apicv(vcpu);
 		if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
@@ -11045,10 +11186,16 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
 			     (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));
 
+		/*
+		 * vmx_vcpu_run()
+		 */
 		exit_fastpath = static_call(kvm_x86_vcpu_run)(vcpu, req_immediate_exit);
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
 
+		/*
+		 * vmx_sync_pir_to_irr()
+		 */
 		if (kvm_lapic_enabled(vcpu))
 			static_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);
 
@@ -11098,6 +11245,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.xfd_no_write_intercept)
 		fpu_sync_guest_vmexit_xfd_state();
 
+	/*
+	 * vmx_handle_exit_irqoff()
+	 */
 	static_call(kvm_x86_handle_exit_irqoff)(vcpu);
 
 	if (vcpu->arch.guest_fpu.xfd_err)
@@ -12462,6 +12612,9 @@ int kvm_arch_hardware_enable(void)
 	if (ret)
 		return ret;
 
+	/*
+	 * vmx_hardware_enable()
+	 */
 	ret = static_call(kvm_x86_hardware_enable)();
 	if (ret != 0)
 		return ret;
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index d80a4c6b5..5ea1bf89c 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -217,6 +217,11 @@ static inline bool is_noncanonical_address(u64 la, struct kvm_vcpu *vcpu)
 	return !__is_canonical_address(la, vcpu_virt_addr_bits(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3287| <<kvm_handle_noslot_fault>> vcpu_cache_mmio_info(vcpu, gva, fault->gfn, access & shadow_mmio_access_mask);
+ *   - arch/x86/kvm/mmu/mmu.c|4193| <<handle_mmio_page_fault>> vcpu_cache_mmio_info(vcpu, addr, gfn, access);
+ */
 static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
 					gva_t gva, gfn_t gfn, unsigned access)
 {
diff --git a/drivers/acpi/acpica/acmacros.h b/drivers/acpi/acpica/acmacros.h
index de83dd222..beb92fe43 100644
--- a/drivers/acpi/acpica/acmacros.h
+++ b/drivers/acpi/acpica/acmacros.h
@@ -421,6 +421,28 @@
  * the plist contains a set of parens to allow variable-length lists.
  * These macros are used for both the debug and non-debug versions of the code.
  */
+/*
+ * 在以下使用ACPI_ERROR_NAMESPACE():
+ *   - drivers/acpi/acpica/dsfield.c|184| <<acpi_ds_create_buffer_field>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dsfield.c|365| <<acpi_ds_get_field_names>> ACPI_ERROR_NAMESPACE(walk_state->
+ *   - drivers/acpi/acpica/dsfield.c|386| <<acpi_ds_get_field_names>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dsfield.c|500| <<acpi_ds_create_field>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dsfield.c|637| <<acpi_ds_init_field_objects>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dsfield.c|699| <<acpi_ds_create_bank_field>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dsfield.c|714| <<acpi_ds_create_bank_field>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dsfield.c|785| <<acpi_ds_create_index_field>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dsfield.c|799| <<acpi_ds_create_index_field>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dsobject.c|80| <<acpi_ds_build_internal_object>> ACPI_ERROR_NAMESPACE(walk_state->
+ *   - drivers/acpi/acpica/dsutils.c|548| <<acpi_ds_create_operand>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dswload.c|173| <<acpi_ds_load1_begin_op>> ACPI_ERROR_NAMESPACE(walk_state->scope_info, path,
+ *   - drivers/acpi/acpica/dswload.c|342| <<acpi_ds_load1_begin_op>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dswload2.c|156| <<acpi_ds_load2_begin_op>> ACPI_ERROR_NAMESPACE(walk_state->
+ *   - drivers/acpi/acpica/dswload2.c|162| <<acpi_ds_load2_begin_op>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dswload2.c|326| <<acpi_ds_load2_begin_op>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/dswload2.c|728| <<acpi_ds_load2_end_op>> ACPI_ERROR_NAMESPACE(walk_state->scope_info,
+ *   - drivers/acpi/acpica/nsconvert.c|464| <<acpi_ns_convert_to_reference>> ACPI_ERROR_NAMESPACE(&scope_info,
+ *   - drivers/acpi/acpica/psargs.c|330| <<acpi_ps_get_next_namepath>> ACPI_ERROR_NAMESPACE(walk_state->scope_info, path, status);
+ */
 #define ACPI_ERROR_NAMESPACE(s, p, e)       acpi_ut_prefixed_namespace_error (AE_INFO, s, p, e);
 #define ACPI_ERROR_METHOD(s, n, p, e)       acpi_ut_method_error (AE_INFO, s, n, p, e);
 #define ACPI_WARN_PREDEFINED(plist)         acpi_ut_predefined_warning plist
diff --git a/drivers/acpi/acpica/dsfield.c b/drivers/acpi/acpica/dsfield.c
index 532401ecd..d7b45f03a 100644
--- a/drivers/acpi/acpica/dsfield.c
+++ b/drivers/acpi/acpica/dsfield.c
@@ -256,6 +256,12 @@ acpi_ds_create_buffer_field(union acpi_parse_object *op,
  *
  ******************************************************************************/
 
+/*
+ * called by:
+ *   - drivers/acpi/acpica/dsfield.c|519| <<acpi_ds_create_field>> status = acpi_ds_get_field_names(&info, walk_state, arg->common.next);
+ *   - drivers/acpi/acpica/dsfield.c|747| <<acpi_ds_create_bank_field>> status = acpi_ds_get_field_names(&info, walk_state, arg->common.next);
+ *   - drivers/acpi/acpica/dsfield.c|814| <<acpi_ds_create_index_field>> status = acpi_ds_get_field_names(&info, walk_state, arg->common.next);
+ */
 static acpi_status
 acpi_ds_get_field_names(struct acpi_create_field_info *info,
 			struct acpi_walk_state *walk_state,
diff --git a/drivers/acpi/acpica/tbxfload.c b/drivers/acpi/acpica/tbxfload.c
index 0f2a7343d..9c8f49f70 100644
--- a/drivers/acpi/acpica/tbxfload.c
+++ b/drivers/acpi/acpica/tbxfload.c
@@ -98,6 +98,10 @@ ACPI_EXPORT_SYMBOL_INIT(acpi_load_tables)
  *              the RSDT/XSDT.
  *
  ******************************************************************************/
+/*
+ * called by:
+ *   - drivers/acpi/acpica/tbxfload.c|59| <<acpi_load_tables>> status = acpi_tb_load_namespace();
+ */
 acpi_status acpi_tb_load_namespace(void)
 {
 	acpi_status status;
diff --git a/drivers/acpi/acpica/uterror.c b/drivers/acpi/acpica/uterror.c
index 918aca7c4..d03dbbf07 100644
--- a/drivers/acpi/acpica/uterror.c
+++ b/drivers/acpi/acpica/uterror.c
@@ -164,6 +164,10 @@ acpi_ut_predefined_bios_error(const char *module_name,
  *
  ******************************************************************************/
 
+/*
+ * 在以下使用acpi_ut_prefixed_namespace_error():
+ *   - drivers/acpi/acpica/acmacros.h|424| <<ACPI_ERROR_NAMESPACE>> #define ACPI_ERROR_NAMESPACE(s, p, e) acpi_ut_prefixed_namespace_error (AE_INFO, s, p, e);
+ */
 void
 acpi_ut_prefixed_namespace_error(const char *module_name,
 				 u32 line_number,
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 1e567d1f6..3b24e782f 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -85,6 +85,9 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 
 	while (i--) {
 		int r;
+		/*
+		 * kvm_set_msi()
+		 */
 		r = irq_set[i].set(&irq_set[i], kvm, irq_source_id, level,
 				   line_status);
 		if (r < 0)
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 1192942ae..8aca6bf56 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -3589,6 +3589,22 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 }
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1592| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - arch/loongarch/kvm/mmu.c|891| <<kvm_map_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - arch/s390/kvm/gaccess.c|1025| <<access_guest_page_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/s390/kvm/gaccess.c|1247| <<cmpxchg_guest_abs_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|3382| <<fast_pf_fix_direct_spte>> mark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);
+ *   - arch/x86/kvm/mmu/spte.c|824| <<make_spte>> mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
+ *   - arch/x86/kvm/x86.c|3761| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/x86.c|5097| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - include/linux/kvm_host.h|1823| <<kvm_gpc_mark_dirty_in_slot>> mark_page_dirty_in_slot(gpc->kvm, gpc->memslot, gpa_to_gfn(gpc->gpa));
+ *   - virt/kvm/kvm_main.c|3384| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3522| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3622| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3631| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+ */
 void mark_page_dirty_in_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *memslot,
 		 	     gfn_t gfn)
-- 
2.43.5

