From 72c1e76b23c213470e1bb0f392b0bcd9ac3dc262 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 2 Feb 2020 19:25:33 -0800
Subject: [PATCH 1/1] linux block for linux-5.6-39bed42de2e7d74

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/blk-core.c          |  12 ++
 block/blk-mq.c            | 172 ++++++++++++++++++++++++
 block/blk-stat.c          | 275 ++++++++++++++++++++++++++++++++++++++
 block/blk-stat.h          |  27 ++++
 block/blk-sysfs.c         |   4 +
 drivers/nvme/host/pci.c   |  24 ++++
 drivers/pci/msi.c         |  14 ++
 include/linux/blk_types.h |  12 ++
 include/linux/blkdev.h    |  74 ++++++++++
 9 files changed, 614 insertions(+)

diff --git a/block/blk-core.c b/block/blk-core.c
index 089e890ab208..b49e4e5db35a 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -476,6 +476,18 @@ static void blk_timeout_work(struct work_struct *work)
  * @gfp_mask: memory allocation flags
  * @node_id: NUMA node to allocate memory from
  */
+/*
+ * called by:
+ *   - block/blk-core.c|394| <<blk_alloc_queue>> return blk_alloc_queue_node(gfp_mask, NUMA_NO_NODE);
+ *   - block/blk-mq.c|2743| <<blk_mq_init_queue>> uninit_q = blk_alloc_queue_node(GFP_KERNEL, set->numa_node);
+ *   - drivers/block/drbd/drbd_main.c|2804| <<drbd_create_device>> q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
+ *   - drivers/block/null_blk_main.c|1724| <<null_add_dev>> nullb->q = blk_alloc_queue_node(GFP_KERNEL, dev->home_node);
+ *   - drivers/block/umem.c|888| <<mm_pci_probe>> card->queue = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
+ *   - drivers/lightnvm/core.c|383| <<nvm_create_tgt>> tqueue = blk_alloc_queue_node(GFP_KERNEL, dev->q->node);
+ *   - drivers/md/dm.c|1949| <<alloc_dev>> md->queue = blk_alloc_queue_node(GFP_KERNEL, numa_node_id);
+ *   - drivers/nvdimm/pmem.c|404| <<pmem_attach_disk>> q = blk_alloc_queue_node(GFP_KERNEL, dev_to_node(dev));
+ *   - drivers/nvme/host/multipath.c|380| <<nvme_mpath_alloc_disk>> q = blk_alloc_queue_node(GFP_KERNEL, ctrl->numa_node);
+ */
 struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 {
 	struct request_queue *q;
diff --git a/block/blk-mq.c b/block/blk-mq.c
index a12b1763508d..7bb2f1998664 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -43,13 +43,22 @@
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
+/*
+ * called by;
+ *   - block/blk-mq.c|2904| <<blk_mq_init_allocated_queue>> blk_mq_poll_stats_bkt,
+ *   - block/blk-mq.c|3452| <<blk_mq_poll_nsecs>> bucket = blk_mq_poll_stats_bkt(rq);
+ */
 static int blk_mq_poll_stats_bkt(const struct request *rq)
 {
 	int ddir, sectors, bucket;
 
+	/* read or write */
 	ddir = rq_data_dir(rq);
 	sectors = blk_rq_stats_sectors(rq);
 
+	/*
+	 * ilog2(sectors)相当于获得sectors的order吧
+	 */
 	bucket = ddir + 2 * ilog2(sectors);
 
 	if (bucket < 0)
@@ -64,6 +73,10 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
  * Check if any of the ctx, dispatch list or elevator
  * have pending work in this hardware queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1508| <<blk_mq_run_hw_queue>> blk_mq_hctx_has_pending(hctx);
+ */
 static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
 	return !list_empty_careful(&hctx->dispatch) ||
@@ -655,7 +668,23 @@ void blk_mq_start_request(struct request *rq)
 
 	trace_block_rq_issue(q, rq);
 
+	/*
+	 * 在以下使用QUEUE_FLAG_STATS:
+	 *   - block/blk-mq.c|671| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+	 *   - block/blk-stat.c|308| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|323| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|364| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 */
 	if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+		/*
+		 * 在以下使用io_start_time_ns:
+		 *   - block/bfq-iosched.c|5911| <<bfq_finish_requeue_request>> rq->io_start_time_ns,
+		 *   - block/blk-mq.c|318| <<blk_mq_rq_ctx_init>> rq->io_start_time_ns = 0;
+		 *   - block/blk-mq.c|663| <<blk_mq_start_request>> rq->io_start_time_ns = ktime_get_ns();
+		 *   - block/blk-stat.c|137| <<blk_stat_add>> value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
+		 *   - block/blk-wbt.c|619| <<wbt_issue>> rwb->sync_issue = rq->io_start_time_ns;
+		 *   - block/kyber-iosched.c|651| <<kyber_completed_request>> now - rq->io_start_time_ns);
+		 */
 		rq->io_start_time_ns = ktime_get_ns();
 		rq->stats_sectors = blk_rq_sectors(rq);
 		rq->rq_flags |= RQF_STATS;
@@ -3364,8 +3393,19 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
 
 /* Enable polling stats and return whether they were already enabled. */
+/*
+ * called by:
+ *   - block/blk-mq.c|3411| <<blk_mq_poll_nsecs>> if (!blk_poll_stats_enable(q))
+ */
 static bool blk_poll_stats_enable(struct request_queue *q)
 {
+	/*
+	 * 在以下使用QUEUE_FLAG_POLL_STATS (collecting stats for hybrid polling):
+	 *   - block/blk-mq.c|3395| <<blk_poll_stats_enable>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+	 *   - block/blk-mq.c|3396| <<blk_poll_stats_enable>> blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
+	 *   - block/blk-mq.c|3412| <<blk_mq_poll_stats_start>> if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+	 *   - block/blk-sysfs.c|884| <<__blk_release_queue>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
+	 */
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
 	    blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
 		return true;
@@ -3373,6 +3413,10 @@ static bool blk_poll_stats_enable(struct request_queue *q)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|526| <<__blk_mq_end_request>> blk_mq_poll_stats_start(rq->q);
+ */
 static void blk_mq_poll_stats_start(struct request_queue *q)
 {
 	/*
@@ -3386,6 +3430,12 @@ static void blk_mq_poll_stats_start(struct request_queue *q)
 	blk_stat_activate_msecs(q->poll_cb, 100);
 }
 
+/*
+ * 在以下使用blk_mq_poll_stats_fn():
+ *   - block/blk-mq.c|2890| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *
+ * 核心思想是把BLK_MQ_POLL_STATS_BKTS个bucket设置q->poll_stat[bucket] = cb->stat[bucket]
+ */
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 {
 	struct request_queue *q = cb->data;
@@ -3397,6 +3447,12 @@ static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3458| <<blk_mq_poll_hybrid_sleep>> nsecs = blk_mq_poll_nsecs(q, hctx, rq);
+ *
+ * 这个函数应该就是用来估算一个要sleep的时间
+ */
 static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 				       struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
@@ -3430,6 +3486,10 @@ static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3534| <<blk_mq_poll_hybrid>> return blk_mq_poll_hybrid_sleep(q, hctx, rq);
+ */
 static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 				     struct blk_mq_hw_ctx *hctx,
 				     struct request *rq)
@@ -3439,6 +3499,13 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	unsigned int nsecs;
 	ktime_t kt;
 
+	/*
+	 * 只在以下使用RQF_MQ_POLL_SLEPT:
+	 *   - block/blk-mq.c|3486| <<blk_mq_poll_hybrid_sleep>> if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
+	 *   - block/blk-mq.c|3503| <<blk_mq_poll_hybrid_sleep>> rq->rq_flags |= RQF_MQ_POLL_SLEPT;
+	 *
+	 * already slept for hybrid poll
+	 */
 	if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
 		return false;
 
@@ -3462,6 +3529,9 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	 * This will be replaced with the stats tracking code, using
 	 * 'avg_completion_time / 2' as the pre-sleep target.
 	 */
+	/*
+	 * kt是非常核心的时间!!!
+	 */
 	kt = nsecs;
 
 	mode = HRTIMER_MODE_REL;
@@ -3469,6 +3539,13 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	hrtimer_set_expires(&hs.timer, kt);
 
 	do {
+		/*
+		 * 使用MQ_RQ_COMPLETE的地方:
+		 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+		 *   - block/blk-mq.c|580| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+		 *   - block/blk-mq.c|3516| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+		 *   - include/linux/blk-mq.h|481| <<blk_mq_request_completed>> return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
+		 */
 		if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
 			break;
 		set_current_state(TASK_UNINTERRUPTIBLE);
@@ -3484,6 +3561,33 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	return true;
 }
 
+/*
+ * 在以下patch加入的函数
+ * commit 06426adf072bca62ac31ea396ff2159a34f276c2
+ * Author: Jens Axboe <axboe@fb.com>
+ * Date:   Mon Nov 14 13:01:59 2016 -0700
+ *
+ * blk-mq: implement hybrid poll mode for sync O_DIRECT
+ *
+ * This patch enables a hybrid polling mode. Instead of polling after IO
+ * submission, we can induce an artificial delay, and then poll after that.
+ * For example, if the IO is presumed to complete in 8 usecs from now, we
+ * can sleep for 4 usecs, wake up, and then do our polling. This still puts
+ * a sleep/wakeup cycle in the IO path, but instead of the wakeup happening
+ * after the IO has completed, it'll happen before. With this hybrid
+ * scheme, we can achieve big latency reductions while still using the same
+ * (or less) amount of CPU.
+ *
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ * Tested-By: Stephen Bates <sbates@raithlin.com>
+ * Reviewed-By: Stephen Bates <sbates@raithlin.com>
+ *
+ * called by:
+ *   - block/blk-mq.c|3608| <<blk_poll>> if (blk_mq_poll_hybrid(q, hctx, cookie))
+ *
+ * If the device access time exceeds the IRQ model overhead,
+ * sleeping before the I/O completion will not hurt latency
+ */
 static bool blk_mq_poll_hybrid(struct request_queue *q,
 			       struct blk_mq_hw_ctx *hctx, blk_qc_t cookie)
 {
@@ -3521,6 +3625,62 @@ static bool blk_mq_poll_hybrid(struct request_queue *q,
  *    looping until at least one completion is found, unless the task is
  *    otherwise marked running (or we need to reschedule).
  */
+/*
+ * poll是可以有专门的hctx的. 用cookie来标记用的哪个hctx甚至哪个tag.
+ *
+ * blk_qc_t_to_queue_num()把cookie转换成q->queue_hw_ctx[]的index.
+ *
+ * blk_qc_t_to_tag()把cookie转换成tag.
+ *
+ * ext4_direct_IO_write()
+ *  -> __blockdev_direct_IO()
+ *      -> do_blockdev_direct_IO
+ *          -> dio_bio_submit
+ *              -> submit_bio()
+ *          -> dio_await_completion()
+ *              -> dio_await_one()
+ *                  -> blk_poll()
+ *
+ * cookie一直通过submit_bio()返回, 然后通过blk_poll()去poll()查看IO是否完成.
+ *
+ * slides: I/O Latency Optimization with Polling
+ *
+ *
+ * 根据一位网友的测试:
+ * 从上述测试结果来看,IO-Polling对于sync模式的direct-io的延迟有较好的提升,
+ * sync模式下,无论4K随机读或者随机写IO压力下,延迟平均大约减少5μs,而这5μs
+ * 几乎就是中断模式下,处理中断时,上下文切换的时间差.
+ * 相比随机读,对随机写的延迟降低约20%,这对延迟敏感的IO请求来说是极大的性能提升.
+ *
+ *
+ * 根据blk_mq_map_queue(), 只有REQ_HIPRI的才会被map到poll的queue.
+ *
+ * 102 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
+ * 103                                                      unsigned int flags,
+ * 104                                                      struct blk_mq_ctx *ctx)
+ * 105 {
+ * 106         enum hctx_type type = HCTX_TYPE_DEFAULT;
+ * 107
+ * 108         //
+ * 109	    // The caller ensure that if REQ_HIPRI, poll must be enabled.
+ * 110	    //
+ * 111         if (flags & REQ_HIPRI)
+ * 112                 type = HCTX_TYPE_POLL;
+ * 113         else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
+ * 114                 type = HCTX_TYPE_READ;115
+ * 116         return ctx->hctxs[type];
+ * 117 }
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|812| <<nvme_execute_rq_polled>> blk_poll(q, request_to_qc_t(rq->mq_hctx, rq), true);
+ *   - fs/block_dev.c|257| <<__blkdev_direct_IO_simple>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/block_dev.c|295| <<blkdev_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), wait);
+ *   - fs/block_dev.c|451| <<__blkdev_direct_IO>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/direct-io.c|502| <<dio_await_one>> !blk_poll(dio->bio_disk->queue, dio->bio_cookie, true))
+ *   - fs/iomap/direct-io.c|57| <<iomap_dio_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), spin);
+ *   - fs/iomap/direct-io.c|562| <<iomap_dio_rw>> !blk_poll(dio->submit.last_queue,
+ *   - mm/page_io.c|424| <<swap_readpage>> if (!blk_poll(disk->queue, qc, true))
+ */
 int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -3533,6 +3693,10 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 	if (current->plug)
 		blk_flush_plug_list(current->plug, false);
 
+	/*
+	 * blk_qc_t_to_queue_num():
+	 * 把cookie的第31位(最高位)清空,然后向右移动16位,获得queue num
+	 */
 	hctx = q->queue_hw_ctx[blk_qc_t_to_queue_num(cookie)];
 
 	/*
@@ -3553,6 +3717,9 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 
 		hctx->poll_invoked++;
 
+		/*
+		 * nvme pci的例子是nvme_poll()
+		 */
 		ret = q->mq_ops->poll(hctx);
 		if (ret > 0) {
 			hctx->poll_success++;
@@ -3575,6 +3742,11 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 }
 EXPORT_SYMBOL_GPL(blk_poll);
 
+/*
+ * called by:
+ *   - drivers/scsi/bnx2i/bnx2i_hwi.c|1918| <<bnx2i_queue_scsi_cmd_resp>> p = &per_cpu(bnx2i_percpu, blk_mq_rq_cpu(sc->request));
+ *   - drivers/scsi/csiostor/csio_scsi.c|1789| <<csio_queuecommand>> sqset = &hw->sqset[ln->portid][blk_mq_rq_cpu(cmnd->request)];
+ */
 unsigned int blk_mq_rq_cpu(struct request *rq)
 {
 	return rq->mq_ctx->cpu;
diff --git a/block/blk-stat.c b/block/blk-stat.c
index 7da302ff88d0..b15a5afbd665 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -12,12 +12,115 @@
 #include "blk-mq.h"
 #include "blk.h"
 
+/*
+ * 核心的patch
+ *
+ * commit 34dbad5d26e2f4b88e60f0e9ad03f99480802812
+ * Author: Omar Sandoval <osandov@fb.com>
+ * Date:   Tue Mar 21 08:56:08 2017 -0700
+ *
+ * blk-stat: convert to callback-based statistics reporting
+ *
+ * Currently, statistics are gathered in ~0.13s windows, and users grab the
+ * statistics whenever they need them. This is not ideal for both in-tree
+ * users:
+ *
+ * 1. Writeback throttling wants its own dynamically sized window of
+ *    statistics. Since the blk-stats statistics are reset after every
+ *    window and the wbt windows don't line up with the blk-stats windows,
+ *    wbt doesn't see every I/O.
+ * 2. Polling currently grabs the statistics on every I/O. Again, depending
+ *    on how the window lines up, we may miss some I/Os. It's also
+ *    unnecessary overhead to get the statistics on every I/O; the hybrid
+ *    polling heuristic would be just as happy with the statistics from the
+ *    previous full window.
+ *
+ * This reworks the blk-stats infrastructure to be callback-based: users
+ * register a callback that they want called at a given time with all of
+ * the statistics from the window during which the callback was active.
+ * Users can dynamically bucketize the statistics. wbt and polling both
+ * currently use read vs. write, but polling can be extended to further
+ * subdivide based on request size.
+ *
+ * The callbacks are kept on an RCU list, and each callback has percpu
+ * stats buffers. There will only be a few users, so the overhead on the
+ * I/O completion side is low. The stats flushing is also simplified
+ * considerably: since the timer function is responsible for clearing the
+ * statistics, we don't have to worry about stale statistics.
+ *
+ * wbt is a trivial conversion. After the conversion, the windowing problem
+ * mentioned above is fixed.
+ *
+ * For polling, we register an extra callback that caches the previous
+ * window's statistics in the struct request_queue for the hybrid polling
+ * heuristic to use.
+ *
+ * Since we no longer have a single stats buffer for the request queue,
+ * this also removes the sysfs and debugfs stats entries. To replace those,
+ * we add a debugfs entry for the poll statistics.
+ *
+ * Signed-off-by: Omar Sandoval <osandov@fb.com>
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ *
+ *
+ * 在blk_alloc_queue_stats()分配一个struct blk_queue_stats结构,
+ * 初始化清空callback链表,设置stats->enable_accounting = false.
+ *
+ * blk_alloc_queue_node()
+ *  -> blk_alloc_queue_stats()
+ *
+ * 在poll的时候把struct blk_stat_callback链接入q->stats->callbacks链表
+ * 设置blk_queue_flag_set(QUEUE_FLAG_STATS, q)
+ *
+ * blk_poll()
+ *  -> blk_mq_poll_hybrid()
+ *      -> blk_mq_poll_hybrid_sleep()
+ *          -> blk_mq_poll_nsecs()
+ *              -> blk_poll_stats_enable()
+ *                  -> blk_stat_add_callback()
+ *
+ * 在end request的时候, 会调用blk_stat_add().
+ * 核心思想是找到request的request_queue->stats,遍历其callback链表,
+ * 对于链表上的每一个struct blk_stat_callback, 调用cb->bucket_fn()选取bucket,
+ * 然后把参数的now汇入对应bucket的struct blk_rq_stat
+ *
+ * 在end request的时候, 还会触发timer,
+ *
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ * 
+ * timer最终触发blk_stat_timer_fn()->blk_mq_poll_stats_fn().
+ * blk_mq_poll_stats_fn()的核心思想是把每个bucket的
+ * q->poll_stat[bucket] = cb->stat[bucket];
+ *
+ * 在poll的时候会根据这些stat决定要先在hybrid的时候sleep多久!
+ *
+ *
+ * slides: I/O Latency Optimization with Polling
+ */
+
 struct blk_queue_stats {
 	struct list_head callbacks;
 	spinlock_t lock;
+	/*
+	 * 在以下使用enable_accounting:
+	 *   - block/blk-stat.c|161| <<blk_stat_remove_callback>> if (list_empty(&q->stats->callbacks) && !q->stats->enable_accounting)
+	 *   - block/blk-stat.c|187| <<blk_stat_enable_accounting>> q->stats->enable_accounting = true;
+	 *   - block/blk-stat.c|203| <<blk_alloc_queue_stats>> stats->enable_accounting = false;
+	 */
 	bool enable_accounting;
 };
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|199| <<latency_stat_init>> blk_rq_stat_init(&stat->rqs);
+ *   - block/blk-stat.c|87| <<blk_stat_timer_fn>> blk_rq_stat_init(&cb->stat[bucket]);
+ *   - block/blk-stat.c|95| <<blk_stat_timer_fn>> blk_rq_stat_init(&cpu_stat[bucket]);
+ *   - block/blk-stat.c|147| <<blk_stat_add_callback>> blk_rq_stat_init(&cpu_stat[bucket]);
+ */
 void blk_rq_stat_init(struct blk_rq_stat *stat)
 {
 	stat->min = -1ULL;
@@ -26,6 +129,11 @@ void blk_rq_stat_init(struct blk_rq_stat *stat)
 }
 
 /* src is a per-cpu stat, mean isn't initialized */
+/*
+ * called by:
+ *   - block/blk-iolatency.c|210| <<latency_stat_sum>> blk_rq_stat_sum(&sum->rqs, &stat->rqs);
+ *   - block/blk-stat.c|94| <<blk_stat_timer_fn>> blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
+ */
 void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 {
 	if (!src->nr_samples)
@@ -40,6 +148,18 @@ void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 	dst->nr_samples += src->nr_samples;
 }
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|222| <<latency_stat_record_time>> blk_rq_stat_add(&stat->rqs, req_time);
+ *   - block/blk-stat.c|74| <<blk_stat_add>> blk_rq_stat_add(stat, value);
+ *
+ * 调用的一个例子:
+ * __blk_mq_end_request()
+ *  -> blk_stat_add()
+ *      -> blk_rq_stat_add()
+ *
+ * 把参数的value汇入struct blk_rq_stat
+ */
 void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 {
 	stat->min = min(stat->min, value);
@@ -48,6 +168,14 @@ void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|527| <<__blk_mq_end_request>> blk_stat_add(rq, now);
+ *
+ * 核心思想是找到request的request_queue->stats,遍历其callback链表,
+ * 对于链表上的每一个struct blk_stat_callback, 调用cb->bucket_fn()选取bucket,
+ * 然后把参数的now汇入对应bucket的struct blk_rq_stat
+ */
 void blk_stat_add(struct request *rq, u64 now)
 {
 	struct request_queue *q = rq->q;
@@ -56,6 +184,15 @@ void blk_stat_add(struct request *rq, u64 now)
 	int bucket, cpu;
 	u64 value;
 
+	/*
+	 * 在以下使用io_start_time_ns:
+	 *   - block/bfq-iosched.c|5911| <<bfq_finish_requeue_request>> rq->io_start_time_ns,
+	 *   - block/blk-mq.c|318| <<blk_mq_rq_ctx_init>> rq->io_start_time_ns = 0;
+	 *   - block/blk-mq.c|663| <<blk_mq_start_request>> rq->io_start_time_ns = ktime_get_ns();
+	 *   - block/blk-stat.c|137| <<blk_stat_add>> value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
+	 *   - block/blk-wbt.c|619| <<wbt_issue>> rwb->sync_issue = rq->io_start_time_ns;
+	 *   - block/kyber-iosched.c|651| <<kyber_completed_request>> now - rq->io_start_time_ns);
+	 */
 	value = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;
 
 	blk_throtl_stat_add(rq, value);
@@ -70,13 +207,34 @@ void blk_stat_add(struct request *rq, u64 now)
 		if (bucket < 0)
 			continue;
 
+		/*
+		 * struct blk_stat_callback:
+		 *  -> struct blk_rq_stat __percpu *cpu_stat;
+		 *  -> struct blk_rq_stat stat;
+		 */
 		stat = &per_cpu_ptr(cb->cpu_stat, cpu)[bucket];
+		/*
+		 * 把参数的value汇入struct blk_rq_stat
+		 */
 		blk_rq_stat_add(stat, value);
 	}
 	put_cpu();
 	rcu_read_unlock();
 }
 
+/*
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ * 说明end request的时候mod timer
+ * The timer callback will be called when the window expires
+ * to gather block statistics during a time window in milliseconds.
+ *
+ * 在以下使用blk_stat_timer_fn():
+ *   - block/blk-stat.c|131| <<blk_stat_alloc_callback>> timer_setup(&cb->timer, blk_stat_timer_fn, 0);
+ */
 static void blk_stat_timer_fn(struct timer_list *t)
 {
 	struct blk_stat_callback *cb = from_timer(cb, t, timer);
@@ -89,16 +247,44 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	for_each_online_cpu(cpu) {
 		struct blk_rq_stat *cpu_stat;
 
+		/*
+		 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+		 */
 		cpu_stat = per_cpu_ptr(cb->cpu_stat, cpu);
 		for (bucket = 0; bucket < cb->buckets; bucket++) {
+			/*
+			 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+			 */
 			blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
 			blk_rq_stat_init(&cpu_stat[bucket]);
 		}
 	}
 
+	/*
+	 * __blk_mq_end_request()
+	 *  -> blk_mq_poll_stats_start()
+	 *      -> blk_stat_activate_msecs()
+	 *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+	 *
+	 * 说明end request的时候mod timer
+	 * The timer callback will be called when the window expires
+	 * to gather block statistics during a time window in milliseconds.
+	 *
+	 * -----------------------------------------
+	 *
+	 * 从blk_mq_init_allocated_queue()进来blk_stat_alloc_callback()
+	 * 的话timer_fn是blk_mq_poll_stats_fn()
+	 */
 	cb->timer_fn(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2890| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *   - block/blk-wbt.c|829| <<wbt_init>> rwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);
+ *
+ * 分配一个struct blk_stat_callback
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
@@ -110,12 +296,18 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 	if (!cb)
 		return NULL;
 
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话buckets是BLK_MQ_POLL_STATS_BKTS
+	 */
 	cb->stat = kmalloc_array(buckets, sizeof(struct blk_rq_stat),
 				 GFP_KERNEL);
 	if (!cb->stat) {
 		kfree(cb);
 		return NULL;
 	}
+	/*
+	 * 注意!!! 每个cpu都有一个buckets * sizeof(struct blk_rq_stat)!!
+	 */
 	cb->cpu_stat = __alloc_percpu(buckets * sizeof(struct blk_rq_stat),
 				      __alignof__(struct blk_rq_stat));
 	if (!cb->cpu_stat) {
@@ -124,15 +316,59 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 		return NULL;
 	}
 
+	/*
+	 * 调用timer_fn的地方:
+	 *   - block/blk-stat.c|190| <<blk_stat_timer_fn>> cb->timer_fn(cb);
+	 *
+	 * 从blk_mq_init_allocated_queue()进来的话timer_fn是blk_mq_poll_stats_fn()
+	 */
 	cb->timer_fn = timer_fn;
+	/*
+	 * 调用bucket_fn的地方: __blk_mq_end_request()-->blk_stat_add()
+	 *   - block/blk-stat.c|156| <<blk_stat_add>> bucket = cb->bucket_fn(rq);
+	 *
+	 * 从blk_mq_init_allocated_queue()进来的话bucket_fn是blk_mq_poll_stats_bkt()
+	 */
 	cb->bucket_fn = bucket_fn;
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话data是'struct request_queue'
+	 */
 	cb->data = data;
+	/*
+	 * 从blk_mq_init_allocated_queue()进来的话buckets是BLK_MQ_POLL_STATS_BKTS
+	 */
 	cb->buckets = buckets;
+	/*
+	 * __blk_mq_end_request()
+	 *  -> blk_mq_poll_stats_start()
+	 *      -> blk_stat_activate_msecs()
+	 *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+	 *
+	 * 说明end request的时候mod timer
+	 * The timer callback will be called when the window expires
+	 * to gather block statistics during a time window in milliseconds.
+	 */
 	timer_setup(&cb->timer, blk_stat_timer_fn, 0);
 
 	return cb;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3376| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|852| <<wbt_init>> blk_stat_add_callback(q, rwb->cb);
+ *
+ * poll调用的例子
+ * blk_poll()
+ *  -> blk_mq_poll_hybrid()
+ *      -> blk_mq_poll_hybrid_sleep()
+ *          -> blk_mq_poll_nsecs()
+ *              -> blk_poll_stats_enable()
+ *                  -> blk_stat_add_callback()
+ *
+ * 核心思想是把struct blk_stat_callback链接入q->stats->callbacks链表
+ * 设置blk_queue_flag_set(QUEUE_FLAG_STATS, q)
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -149,10 +385,22 @@ void blk_stat_add_callback(struct request_queue *q,
 
 	spin_lock(&q->stats->lock);
 	list_add_tail_rcu(&cb->list, &q->stats->callbacks);
+	/*
+	 * 在以下使用QUEUE_FLAG_STATS:
+	 *   - block/blk-mq.c|671| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+	 *   - block/blk-stat.c|308| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|323| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|364| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 */
 	blk_queue_flag_set(QUEUE_FLAG_STATS, q);
 	spin_unlock(&q->stats->lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|881| <<__blk_release_queue>> blk_stat_remove_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|696| <<wbt_exit>> blk_stat_remove_callback(q, rwb->cb);
+ */
 void blk_stat_remove_callback(struct request_queue *q,
 			      struct blk_stat_callback *cb)
 {
@@ -165,6 +413,10 @@ void blk_stat_remove_callback(struct request_queue *q,
 	del_timer_sync(&cb->timer);
 }
 
+/*
+ * 在以下使用blk_stat_free_callback_rcu():
+ *   - block/blk-stat.c|303| <<blk_stat_free_callback>> call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
+ */
 static void blk_stat_free_callback_rcu(struct rcu_head *head)
 {
 	struct blk_stat_callback *cb;
@@ -175,12 +427,23 @@ static void blk_stat_free_callback_rcu(struct rcu_head *head)
 	kfree(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2973| <<blk_mq_init_allocated_queue>> blk_stat_free_callback(q->poll_cb);
+ *   - block/blk-sysfs.c|882| <<__blk_release_queue>> blk_stat_free_callback(q->poll_cb);
+ *   - block/blk-wbt.c|697| <<wbt_exit>> blk_stat_free_callback(rwb->cb);
+ */
 void blk_stat_free_callback(struct blk_stat_callback *cb)
 {
 	if (cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
 
+/*
+ * called by:
+ *   - block/blk-throttle.c|2503| <<blk_throtl_register_queue>> blk_stat_enable_accounting(q);
+ *   - block/kyber-iosched.c|431| <<kyber_init_sched>> blk_stat_enable_accounting(q);
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	spin_lock(&q->stats->lock);
@@ -190,6 +453,13 @@ void blk_stat_enable_accounting(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_stat_enable_accounting);
 
+/*
+ * called by:
+ *   - block/blk-core.c|503| <<blk_alloc_queue_node>> q->stats = blk_alloc_queue_stats();
+ *
+ * 分配一个struct blk_queue_stats结构, 初始化清空callback链表
+ * 设置stats->enable_accounting = false;
+ */
 struct blk_queue_stats *blk_alloc_queue_stats(void)
 {
 	struct blk_queue_stats *stats;
@@ -205,6 +475,11 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 	return stats;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|550| <<blk_alloc_queue_node>> blk_free_queue_stats(q->stats);
+ *   - block/blk-sysfs.c|884| <<__blk_release_queue>> blk_free_queue_stats(q->stats);
+ */
 void blk_free_queue_stats(struct blk_queue_stats *stats)
 {
 	if (!stats)
diff --git a/block/blk-stat.h b/block/blk-stat.h
index 17b47a86eefb..447c50693721 100644
--- a/block/blk-stat.h
+++ b/block/blk-stat.h
@@ -126,6 +126,12 @@ void blk_stat_free_callback(struct blk_stat_callback *cb);
  * gathering statistics.
  * @cb: The callback.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3420| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+ *   - block/blk-stat.c|153| <<blk_stat_add>> if (!blk_stat_is_active(cb))
+ *   - block/blk-wbt.c|593| <<wbt_wait>> if (!blk_stat_is_active(rwb->cb))
+ */
 static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
 {
 	return timer_pending(&cb->timer);
@@ -139,12 +145,20 @@ static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-wbt.c|349| <<rwb_arm_timer>> blk_stat_activate_nsecs(rwb->cb, rwb->cur_win_nsec);
+ */
 static inline void blk_stat_activate_nsecs(struct blk_stat_callback *cb,
 					   u64 nsecs)
 {
 	mod_timer(&cb->timer, jiffies + nsecs_to_jiffies(nsecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-wbt.c|712| <<wbt_disable_default>> blk_stat_deactivate(rwb->cb);
+ */
 static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
 {
 	del_timer_sync(&cb->timer);
@@ -158,6 +172,19 @@ static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3394| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+ *
+ * __blk_mq_end_request()
+ *  -> blk_mq_poll_stats_start()
+ *      -> blk_stat_activate_msecs()
+ *          -> mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
+ *
+ * 说明end request的时候mod timer
+ * The timer callback will be called when the window expires
+ * to gather block statistics during a time window in milliseconds.
+ */
 static inline void blk_stat_activate_msecs(struct blk_stat_callback *cb,
 					   unsigned int msecs)
 {
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index fca9b158f4a0..1ff764db605c 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -873,6 +873,10 @@ static void blk_exit_queue(struct request_queue *q)
  *     of the request queue reaches zero, blk_release_queue is called to release
  *     all allocated resources of the request queue.
  */
+/*
+ * 在以下使用__blk_release_queue():
+ *   - block/blk-sysfs.c|912| <<blk_release_queue>> INIT_WORK(&q->release_work, __blk_release_queue);
+ */
 static void __blk_release_queue(struct work_struct *work)
 {
 	struct request_queue *q = container_of(work, typeof(*q), release_work);
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index 365a2ddbeaa7..830bd0b1bd3a 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -1492,6 +1492,12 @@ static int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1555| <<nvme_create_queue>> result = queue_request_irq(nvmeq);
+ *   - drivers/nvme/host/pci.c|1703| <<nvme_pci_configure_admin_queue>> result = queue_request_irq(nvmeq);
+ *   - drivers/nvme/host/pci.c|2160| <<nvme_setup_io_queues>> result = queue_request_irq(adminq);
+ */
 static int queue_request_irq(struct nvme_queue *nvmeq)
 {
 	struct pci_dev *pdev = to_pci_dev(nvmeq->dev->dev);
@@ -2045,8 +2051,26 @@ static void nvme_calc_irq_sets(struct irq_affinity *affd, unsigned int nrirqs)
 	affd->nr_sets = nr_read_queues ? 2 : 1;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2146| <<nvme_setup_io_queues>> result = nvme_setup_irqs(dev, nr_io_queues);
+ */
 static int nvme_setup_irqs(struct nvme_dev *dev, unsigned int nr_io_queues)
 {
+	/*
+	 * struct irq_affinity - Description for automatic irq affinity assignements
+	 * @pre_vectors:        Don't apply affinity to @pre_vectors at beginning of
+	 *                      the MSI(-X) vector space
+	 * @post_vectors:       Don't apply affinity to @post_vectors at end of
+	 *                      the MSI(-X) vector space
+	 * @nr_sets:            The number of interrupt sets for which affinity
+	 *                      spreading is required
+	 * @set_size:           Array holding the size of each interrupt set
+	 * @calc_sets:          Callback for calculating the number and size
+	 *                      of interrupt sets
+	 * @priv:               Private data for usage by @calc_sets, usually a
+	 *                      pointer to driver/device specific data.
+	 */
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 	struct irq_affinity affd = {
 		.pre_vectors	= 1,
diff --git a/drivers/pci/msi.c b/drivers/pci/msi.c
index 6b43a5455c7a..e6817052992a 100644
--- a/drivers/pci/msi.c
+++ b/drivers/pci/msi.c
@@ -1190,6 +1190,20 @@ int pci_alloc_irq_vectors_affinity(struct pci_dev *dev, unsigned int min_vecs,
 				   unsigned int max_vecs, unsigned int flags,
 				   struct irq_affinity *affd)
 {
+	/*
+	 * struct irq_affinity - Description for automatic irq affinity assignements
+280  * @pre_vectors:        Don't apply affinity to @pre_vectors at beginning of
+281  *                      the MSI(-X) vector space
+282  * @post_vectors:       Don't apply affinity to @post_vectors at end of
+283  *                      the MSI(-X) vector space
+284  * @nr_sets:            The number of interrupt sets for which affinity
+285  *                      spreading is required
+286  * @set_size:           Array holding the size of each interrupt set
+287  * @calc_sets:          Callback for calculating the number and size
+288  *                      of interrupt sets
+289  * @priv:               Private data for usage by @calc_sets, usually a
+290  *                      pointer to driver/device specific data.
+	 */
 	struct irq_affinity msi_default_affd = {0};
 	int msix_vecs = -ENOSPC;
 	int msi_vecs = -ENOSPC;
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index 70254ae11769..00398d151629 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -461,16 +461,28 @@ static inline bool blk_qc_t_valid(blk_qc_t cookie)
 	return cookie != BLK_QC_T_NONE && cookie != BLK_QC_T_EAGAIN;
 }
 
+/*
+ * 把cookie的第31位(最高位)清空,然后向右移动16位,获得queue num
+ */
 static inline unsigned int blk_qc_t_to_queue_num(blk_qc_t cookie)
 {
+	/*
+	 * ~BLK_QC_T_INTERNAL的第31位是0, 剩下都是1
+	 */
 	return (cookie & ~BLK_QC_T_INTERNAL) >> BLK_QC_T_SHIFT;
 }
 
+/*
+ * 获取cookie的低16位, 表示tag id
+ */
 static inline unsigned int blk_qc_t_to_tag(blk_qc_t cookie)
 {
 	return cookie & ((1u << BLK_QC_T_SHIFT) - 1);
 }
 
+/*
+ * 如果cookie最高位31位设置了,就是INTERNAL
+ */
 static inline bool blk_qc_t_is_internal(blk_qc_t cookie)
 {
 	return (cookie & BLK_QC_T_INTERNAL) != 0;
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 053ea4b51988..d0490c8c485d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -106,6 +106,11 @@ typedef __u32 __bitwise req_flags_t;
 /* The per-zone write lock is held for this request */
 #define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
 /* already slept for hybrid poll */
+/*
+ * 只在以下使用RQF_MQ_POLL_SLEPT:
+ *   - block/blk-mq.c|3486| <<blk_mq_poll_hybrid_sleep>> if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
+ *   - block/blk-mq.c|3503| <<blk_mq_poll_hybrid_sleep>> rq->rq_flags |= RQF_MQ_POLL_SLEPT;
+ */
 #define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
 /* ->timeout has been called, don't expire again */
 #define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))
@@ -120,6 +125,13 @@ typedef __u32 __bitwise req_flags_t;
 enum mq_rq_state {
 	MQ_RQ_IDLE		= 0,
 	MQ_RQ_IN_FLIGHT		= 1,
+	/*
+	 * 使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+	 *   - block/blk-mq.c|580| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|3516| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 *   - include/linux/blk-mq.h|481| <<blk_mq_request_completed>> return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
+	 */
 	MQ_RQ_COMPLETE		= 2,
 };
 
@@ -211,6 +223,12 @@ struct request {
 	 * with blk_rq_sectors(rq), except that it never be zeroed
 	 * by completion.
 	 */
+	/*
+	 * 在以下使用stats_sectors:
+	 *   - block/blk-mq.c|319| <<blk_mq_rq_ctx_init>> rq->stats_sectors = 0;
+	 *   - block/blk-mq.c|673| <<blk_mq_start_request>> rq->stats_sectors = blk_rq_sectors(rq);
+	 *   - include/linux/blkdev.h|951| <<blk_rq_stats_sectors>> return rq->stats_sectors;
+	 */
 	unsigned short stats_sectors;
 
 	/*
@@ -475,9 +493,41 @@ struct request_queue {
 	unsigned int		dma_alignment;
 
 	unsigned int		rq_timeout;
+	/*
+	 * 设置poll_nsec的地方:
+	 *   - block/blk-mq.c|2957| <<blk_mq_init_allocated_queue>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|384| <<queue_poll_dely_store>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|386| <<queue_poll_delay_store>> q->poll_nsec = val * 1000;
+	 * 使用poll_nsec的地方:
+	 *   - block/blk-mq.c|3495| <<blk_mq_poll_hybrid_sleep>> if (q->poll_nsec > 0)
+	 *   - block/blk-mq.c|3496| <<blk_mq_poll_hybrid_sleep>> nsecs = q->poll_nsec;
+	 *   - block/blk-mq.c|3560| <<blk_mq_poll_hybrid>> if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
+	 *   - block/blk-sysfs.c|363| <<queue_poll_delay_show>> if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
+	 *   - block/blk-sysfs.c|366| <<queue_poll_delay_show>> val = q->poll_nsec / 1000;
+	 */
 	int			poll_nsec;
 
+	/*
+	 * 使用poll_cb的地方:
+	 *   - block/blk-mq.c|2912| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+	 *   - block/blk-mq.c|2915| <<blk_mq_init_allocated_queue>> if (!q->poll_cb)
+	 *   - block/blk-mq.c|2973| <<blk_mq_init_allocated_queue>> blk_stat_free_callback(q->poll_cb);
+	 *   - block/blk-mq.c|2974| <<blk_mq_init_allocated_queue>> q->poll_cb = NULL;
+	 *   - block/blk-mq.c|3398| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+	 *   - block/blk-mq.c|3413| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+	 *   - block/blk-mq.c|3416| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+	 *   - block/blk-sysfs.c|881| <<__blk_release_queue>> blk_stat_remove_callback(q, q->poll_cb);
+	 *   - block/blk-sysfs.c|882| <<__blk_release_queue>> blk_stat_free_callback(q->poll_cb);
+	 */
 	struct blk_stat_callback	*poll_cb;
+	/*
+	 * 使用poll_stat[]的地方:
+	 *   - block/blk-mq-debugfs.c|34| <<queue_poll_stat_show>> print_stat(m, &q->poll_stat[2 * bucket]);
+	 *   - block/blk-mq-debugfs.c|38| <<queue_poll_stat_show>> print_stat(m, &q->poll_stat[2 * bucket + 1]);
+	 *   - block/blk-mq.c|3432| <<blk_mq_poll_stats_fn>> q->poll_stat[bucket] = cb->stat[bucket];
+	 *   - block/blk-mq.c|3467| <<blk_mq_poll_nsecs>> if (q->poll_stat[bucket].nr_samples)
+	 *   - block/blk-mq.c|3468| <<blk_mq_poll_nsecs>> ret = (q->poll_stat[bucket].mean + 1) / 2;
+	 */
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
 	struct timer_list	timeout;
@@ -604,7 +654,21 @@ struct request_queue {
 #define QUEUE_FLAG_WC		17	/* Write back caching */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 在以下使用QUEUE_FLAG_STATS:
+ *   - block/blk-mq.c|671| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ *   - block/blk-stat.c|308| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|323| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|364| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
+/*
+ * 在以下使用QUEUE_FLAG_POLL_STATS:
+ *   - block/blk-mq.c|3395| <<blk_poll_stats_enable>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+ *   - block/blk-mq.c|3396| <<blk_poll_stats_enable>> blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
+ *   - block/blk-mq.c|3412| <<blk_mq_poll_stats_start>> if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+ *   - block/blk-sysfs.c|884| <<__blk_release_queue>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
+ */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
@@ -948,6 +1012,16 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 
 static inline unsigned int blk_rq_stats_sectors(const struct request *rq)
 {
+	/*
+	 * rq sectors used for blk stats. It has the same value
+	 * with blk_rq_sectors(rq), except that it never be zeroed
+	 * by completion.
+	 *
+	 * 在以下使用stats_sectors:
+	 *   - block/blk-mq.c|319| <<blk_mq_rq_ctx_init>> rq->stats_sectors = 0;
+	 *   - block/blk-mq.c|673| <<blk_mq_start_request>> rq->stats_sectors = blk_rq_sectors(rq);
+	 *   - include/linux/blkdev.h|951| <<blk_rq_stats_sectors>> return rq->stats_sectors;
+	 */
 	return rq->stats_sectors;
 }
 
-- 
2.17.1

