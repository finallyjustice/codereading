From 922655ac926a747d4717b7d30aa3986f275b6b61 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 17 May 2022 17:09:24 -0700
Subject: [PATCH 1/1] linux v5.17

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/kvm/vgic/vgic-debug.c              |   4 +
 arch/arm64/kvm/vgic/vgic-init.c               |   7 +
 arch/x86/include/asm/kvm_host.h               | 273 +++++++
 arch/x86/include/asm/msr.h                    |   8 +
 arch/x86/include/asm/pvclock-abi.h            |  15 +
 arch/x86/include/asm/pvclock.h                |   4 +
 arch/x86/kernel/kvm.c                         |  35 +
 arch/x86/kernel/kvmclock.c                    |  21 +
 arch/x86/kernel/paravirt.c                    |   7 +
 arch/x86/kernel/smpboot.c                     |  18 +
 arch/x86/kernel/tsc.c                         |  10 +
 arch/x86/kernel/tsc_sync.c                    |  30 +
 arch/x86/kvm/cpuid.c                          |  23 +
 arch/x86/kvm/cpuid.h                          |   5 +
 arch/x86/kvm/lapic.c                          | 107 +++
 arch/x86/kvm/lapic.h                          |  34 +
 arch/x86/kvm/mmu.h                            | 106 +++
 arch/x86/kvm/mmu/mmu.c                        | 726 ++++++++++++++++++
 arch/x86/kvm/mmu/mmu_audit.c                  |   8 +
 arch/x86/kvm/mmu/mmu_internal.h               | 100 +++
 arch/x86/kvm/mmu/paging_tmpl.h                |   6 +
 arch/x86/kvm/mmu/spte.c                       |  60 ++
 arch/x86/kvm/mmu/spte.h                       |  61 ++
 arch/x86/kvm/mmu/tdp_iter.c                   | 109 +++
 arch/x86/kvm/mmu/tdp_iter.h                   |  39 +
 arch/x86/kvm/mmu/tdp_mmu.c                    | 484 ++++++++++++
 arch/x86/kvm/mmu/tdp_mmu.h                    |  54 ++
 arch/x86/kvm/pmu.c                            |  26 +
 arch/x86/kvm/svm/sev.c                        |   4 +
 arch/x86/kvm/vmx/nested.h                     |   6 +
 arch/x86/kvm/vmx/posted_intr.c                |  11 +
 arch/x86/kvm/vmx/vmx.c                        |  44 ++
 arch/x86/kvm/x86.c                            | 444 +++++++++++
 arch/x86/kvm/x86.h                            |   6 +
 block/blk-map.c                               |  47 ++
 block/blk-merge.c                             |  10 +
 block/blk-settings.c                          |  40 +
 drivers/acpi/acpi_processor.c                 |  11 +
 drivers/acpi/acpica/evmisc.c                  |   4 +
 drivers/acpi/bus.c                            |   4 +
 drivers/acpi/osl.c                            |  17 +
 drivers/acpi/scan.c                           |  45 ++
 drivers/cpuidle/cpuidle-haltpoll.c            |  44 ++
 drivers/cpuidle/cpuidle.c                     |  42 +
 drivers/cpuidle/governors/haltpoll.c          |  56 ++
 drivers/cpuidle/poll_state.c                  |  11 +
 drivers/scsi/scsi_ioctl.c                     |  10 +
 drivers/vfio/vfio_iommu_type1.c               |  10 +
 drivers/xen/swiotlb-xen.c                     |   5 +
 include/linux/cpuidle.h                       |  28 +
 include/linux/kvm_dirty_ring.h                |  10 +
 include/linux/kvm_host.h                      |  53 ++
 include/linux/virtio_net.h                    |  11 +
 include/linux/vmstat.h                        |  10 +
 kernel/cpu.c                                  |   5 +
 kernel/dma/swiotlb.c                          |  81 ++
 kernel/power/suspend.c                        |   3 +
 kernel/sched/core_sched.c                     |   6 +
 kernel/sched/idle.c                           |   4 +
 kernel/time/clocksource.c                     |  29 +
 kernel/time/tick-sched.c                      |   6 +
 kernel/time/timekeeping.c                     |  24 +
 lib/iov_iter.c                                |  10 +
 net/core/dev.c                                |   3 +
 net/packet/af_packet.c                        |   5 +
 sound/core/timer.c                            |   4 +
 .../selftests/kvm/kvm_page_table_test.c       |  20 +
 tools/testing/selftests/kvm/lib/guest_modes.c |  10 +
 tools/testing/selftests/kvm/lib/kvm_util.c    |   6 +
 virt/kvm/async_pf.c                           |   6 +
 virt/kvm/dirty_ring.c                         |  24 +
 virt/kvm/kvm_main.c                           | 303 ++++++++
 virt/kvm/pfncache.c                           |  35 +
 73 files changed, 3947 insertions(+)

diff --git a/arch/arm64/kvm/vgic/vgic-debug.c b/arch/arm64/kvm/vgic/vgic-debug.c
index f38c40a76251..0ad6b9fa9591 100644
--- a/arch/arm64/kvm/vgic/vgic-debug.c
+++ b/arch/arm64/kvm/vgic/vgic-debug.c
@@ -269,6 +269,10 @@ static const struct seq_operations vgic_debug_sops = {
 
 DEFINE_SEQ_ATTRIBUTE(vgic_debug);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|320| <<vgic_init>> vgic_debug_init(kvm);
+ */
 void vgic_debug_init(struct kvm *kvm)
 {
 	debugfs_create_file("vgic-state", 0444, kvm->debugfs_dentry, kvm,
diff --git a/arch/arm64/kvm/vgic/vgic-init.c b/arch/arm64/kvm/vgic/vgic-init.c
index fc00304fe7d8..a73042109880 100644
--- a/arch/arm64/kvm/vgic/vgic-init.c
+++ b/arch/arm64/kvm/vgic/vgic-init.c
@@ -252,6 +252,13 @@ static void kvm_vgic_vcpu_enable(struct kvm_vcpu *vcpu)
  * vgic_initialized() returns true when this function has succeeded.
  * Must be called with kvm->lock held!
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|413| <<vgic_lazy_init>> ret = vgic_init(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|214| <<vgic_set_common_attr>> r = vgic_init(dev->kvm);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|372| <<vgic_v2_attr_regs_access>> ret = vgic_init(dev->kvm);
+ *   - arch/arm64/kvm/vgic/vgic-v2.c|309| <<vgic_v2_map_resources>> ret = vgic_init(kvm);
+ */
 int vgic_init(struct kvm *kvm)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ec9830d2aabf..7deec6c932e9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -129,10 +129,36 @@
 /* KVM Hugepage definitions for x86 */
 #define KVM_MAX_HUGEPAGE_LEVEL	PG_LEVEL_1G
 #define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)
+/*
+ * KVM_HPAGE_GFN_SHIFT(1) = (1 - 1) * 9 =  0
+ * KVM_HPAGE_GFN_SHIFT(2) = (2 - 1) * 9 =  9
+ * KVM_HPAGE_GFN_SHIFT(3) = (3 - 1) * 9 = 18
+ * KVM_HPAGE_GFN_SHIFT(4) = (4 - 1) * 9 = 27
+ * KVM_HPAGE_GFN_SHIFT(5) = (5 - 1) * 9 = 36
+ */
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+/*
+ * KVM_HPAGE_SHIFT(1) = 12 +  0 = 12
+ * KVM_HPAGE_SHIFT(2) = 12 +  9 = 21
+ * KVM_HPAGE_SHIFT(3) = 12 + 18 = 30
+ * KVM_HPAGE_SHIFT(4) = 12 + 27 = 38
+ * KVM_HPAGE_SHIFT(5) = 12 + 36 = 48
+ */
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
+/*
+ * KVM_HPAGE_SIZE(1) : 1 << 12 = 4K
+ * KVM_HPAGE_SIZE(2) : 1 << 21 = 2M
+ * KVM_HPAGE_SIZE(3) : 1 << 30 = 1G
+ * KVM_HPAGE_SIZE(4) : 1 << 38 = 256G
+ * KVM_HPAGE_SIZE(5) : 1 << 48 = 262144G
+ */
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
+/*
+ * KVM_PAGES_PER_HPAGE(1) = KVM_HPAGE_SIZE(1) / 4096 = 4K / 4K = 1
+ * KVM_PAGES_PER_HPAGE(2) = KVM_HPAGE_SIZE(2) / 4096 = 2M / 4K = 512
+ * KVM_PAGES_PER_HPAGE(3) = KVM_HPAGE_SIZE(3) / 4096 = 1G / 4K = 262144
+ */
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
 #define KVM_MEMSLOT_PAGES_TO_MMU_PAGES_RATIO 50
@@ -438,6 +464,14 @@ struct kvm_mmu {
 	u8 root_level;
 	u8 shadow_root_level;
 	u8 ept_ad;
+	/*
+	 * 在以下设置kvm_mmu->direct_map:
+	 *   - arch/x86/kvm/mmu/mmu.c|4666| <<nonpaging_init_context>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|5233| <<paging64_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu/mmu.c|5242| <<paging32_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu/mmu.c|5332| <<init_kvm_tdp_mmu>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|5495| <<kvm_init_shadow_ept_mmu>> context->direct_map = false;
+	 */
 	bool direct_map;
 	struct kvm_mmu_root_info prev_roots[KVM_MMU_NUM_PREV_ROOTS];
 
@@ -457,6 +491,17 @@ struct kvm_mmu {
 	u32 pkru_mask;
 
 	u64 *pae_root;
+	/*
+	 * 在以下使用kvm_mmu->pml4_root:
+	 *   - arch/x86/kvm/mmu/mmu.c|3989| <<mmu_alloc_shadow_roots>> if (WARN_ON_ONCE(!mmu->pml4_root)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|3993| <<mmu_alloc_shadow_roots>> mmu->pml4_root[0] = __pa(mmu->pae_root) | pm_mask;
+	 *   - arch/x86/kvm/mmu/mmu.c|4000| <<mmu_alloc_shadow_roots>> mmu->pml5_root[0] = __pa(mmu->pml4_root) | pm_mask;
+	 *   - arch/x86/kvm/mmu/mmu.c|4023| <<mmu_alloc_shadow_roots>> mmu->root_hpa = __pa(mmu->pml4_root);
+	 *   - arch/x86/kvm/mmu/mmu.c|4060| <<mmu_alloc_special_roots>> if (mmu->pae_root && mmu->pml4_root && (!need_pml5 || mmu->pml5_root))
+	 *   - arch/x86/kvm/mmu/mmu.c|4067| <<mmu_alloc_special_roots>> if (WARN_ON_ONCE(!tdp_enabled || mmu->pae_root || mmu->pml4_root ||
+	 *   - arch/x86/kvm/mmu/mmu.c|4092| <<mmu_alloc_special_roots>> mmu->pml4_root = pml4_root;
+	 *   - arch/x86/kvm/mmu/mmu.c|6066| <<free_mmu_pages>> free_page((unsigned long )mmu->pml4_root);
+	 */
 	u64 *pml4_root;
 	u64 *pml5_root;
 
@@ -759,19 +804,63 @@ struct kvm_vcpu_arch {
 		struct gfn_to_hva_cache cache;
 	} st;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2485| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = l1_offset;
+	 */
 	u64 l1_tsc_offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|574| <<nested_vmcb02_prepare_control>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+	 *   - arch/x86/kvm/svm/nested.c|864| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2542| <<prepare_vmcs02>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+	 *   - arch/x86/kvm/vmx/nested.c|3500| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|4551| <<nested_vmx_vmexit>> vcpu->arch.tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2501| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+	 *   - arch/x86/kvm/x86.c|2506| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = l1_offset;
+	 * 在以下使用kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/debugfs.c|34| <<vcpu_get_tsc_offset>> *val = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|579| <<nested_vmcb02_prepare_control>> svm->vmcb->control.tsc_offset = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|865| <<nested_svm_vmexit>> if (svm->vmcb->control.tsc_offset != svm->vcpu.arch.tsc_offset) {
+	 *   - arch/x86/kvm/svm/nested.c|866| <<nested_svm_vmexit>> svm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2551| <<prepare_vmcs02>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/nested.c|4590| <<nested_vmx_vmexit>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/x86.c|2508| <<kvm_vcpu_write_tsc_offset>> static_call(kvm_x86_write_tsc_offset)(vcpu, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/x86.c|3980| <<kvm_get_msr_common>> offset = vcpu->arch.tsc_offset;
+	 */
 	u64 tsc_offset; /* current tsc offset */
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2554| <<__kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = tsc;
+	 *   - arch/x86/kvm/x86.c|3128| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|10291| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|4633| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 */
 	u64 last_guest_tsc;
 	u64 last_host_tsc;
 	u64 tsc_offset_adjustment;
 	u64 this_tsc_nsec;
 	u64 this_tsc_write;
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2306| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3099| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4635| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;
 	u32 virtual_tsc_mult;
 	u32 virtual_tsc_khz;
+	/*
+	 * 在以下使用kvm_vcpu_arch->ia32_tsc_adjust_msr:
+	 *   - arch/x86/kvm/x86.c|3532| <<kvm_set_msr_common>> s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
+	 *   - arch/x86/kvm/x86.c|3539| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr = data;
+	 *   - arch/x86/kvm/x86.c|3567| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr += adj;
+	 *   - arch/x86/kvm/x86.c|3932| <<kvm_get_msr_common>> msr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;
+	 */
 	s64 ia32_tsc_adjust_msr;
 	u64 msr_ia32_power_ctl;
 	u64 l1_tsc_scaling_ratio;
@@ -781,6 +870,13 @@ struct kvm_vcpu_arch {
 	unsigned nmi_pending; /* NMI queued after currently running handler */
 	bool nmi_injected;    /* Trying to inject an NMI this entry */
 	bool smi_pending;    /* SMI queued after currently running handler */
+	/*
+	 * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+	 *   - arch/x86/include/asm/kvm_host.h|1796| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+	 *   - arch/x86/kvm/x86.h|359| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+	 *   - arch/x86/kvm/x86.h|364| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+	 *   - arch/x86/kvm/x86.h|369| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+	 */
 	u8 handling_intr_from_guest;
 
 	struct kvm_mtrr mtrr_state;
@@ -847,6 +943,13 @@ struct kvm_vcpu_arch {
 		struct gfn_to_hva_cache data;
 	} pv_eoi;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->msr_kvm_poll_control:
+	 *   - arch/x86/kvm/x86.c|3647| <<kvm_set_msr_common>> vcpu->arch.msr_kvm_poll_control = data;
+	 *   - arch/x86/kvm/x86.c|3988| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.msr_kvm_poll_control;
+	 *   - arch/x86/kvm/x86.c|11275| <<kvm_arch_vcpu_postcreate>> vcpu->arch.msr_kvm_poll_control = 1;
+	 *   - arch/x86/kvm/x86.c|12602| <<kvm_arch_no_poll>> return (vcpu->arch.msr_kvm_poll_control & 1) == 0;
+	 */
 	u64 msr_kvm_poll_control;
 
 	/*
@@ -1047,14 +1150,49 @@ struct kvm_x86_msr_filter {
 #define APICV_INHIBIT_REASON_ABSENT	7
 
 struct kvm_arch {
+	/*
+	 * 在以下使用kvm_arch->n_used_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1720| <<kvm_mod_used_mmu_pages>> kvm->arch.n_used_mmu_pages += nr;
+	 *   - arch/x86/kvm/mmu/mmu.c|2589| <<kvm_mmu_available_pages>> if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
+	 *   - arch/x86/kvm/mmu/mmu.c|2591| <<kvm_mmu_available_pages>> kvm->arch.n_used_mmu_pages;
+	 *   - arch/x86/kvm/mmu/mmu.c|2627| <<kvm_mmu_change_mmu_pages>> if (kvm->arch.n_used_mmu_pages > goal_nr_mmu_pages) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2628| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
+	 *   - arch/x86/kvm/mmu/mmu.c|2631| <<kvm_mmu_change_mmu_pages>> goal_nr_mmu_pages = kvm->arch.n_used_mmu_pages;
+	 *   - arch/x86/kvm/mmu/mmu.c|6232| <<mmu_shrink_scan>> if (!kvm->arch.n_used_mmu_pages &&
+	 */
 	unsigned long n_used_mmu_pages;
 	unsigned long n_requested_mmu_pages;
+	/*
+	 * 在以下使用kvm_arch->n_max_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|2727| <<kvm_mmu_available_pages>> if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
+	 *   - arch/x86/kvm/mmu/mmu.c|2728| <<kvm_mmu_available_pages>> return kvm->arch.n_max_mmu_pages -
+	 *   - arch/x86/kvm/mmu/mmu.c|2779| <<kvm_mmu_change_mmu_pages>> kvm->arch.n_max_mmu_pages = goal_nr_mmu_pages;
+	 *   - arch/x86/kvm/x86.c|5728| <<kvm_vm_ioctl_get_nr_mmu_pages>> return kvm->arch.n_max_mmu_pages;
+	 */
 	unsigned long n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
+	/*
+	 * 在以下使用kvm_arch->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmu.c|6279| <<kvm_mmu_zap_all_fast>> kvm->arch.mmu_valid_gen = kvm->arch.mmu_valid_gen ? 0 : 1;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|299| <<__field>> __entry->mmu_valid_gen = kvm->arch.mmu_valid_gen;
+	 * 在以下使用kvm_mmu_page->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmutrace.h|19| <<KVM_MMU_PAGE_ASSIGN>> __entry->mmu_valid_gen = sp->mmu_valid_gen; \
+	 */
 	u8 mmu_valid_gen;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	struct list_head active_mmu_pages;
 	struct list_head zapped_obsolete_pages;
+	/*
+	 * 在以下使用kvm_arch->lpage_disallowed_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|947| <<account_huge_nx_page>> list_add_tail(&sp->lpage_disallowed_link, &kvm->arch.lpage_disallowed_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6641| <<kvm_recover_nx_lpages>> if (list_empty(&kvm->arch.lpage_disallowed_mmu_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|6649| <<kvm_recover_nx_lpages>> sp = list_first_entry(&kvm->arch.lpage_disallowed_mmu_pages,
+	 *   - arch/x86/kvm/x86.c|11639| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.lpage_disallowed_mmu_pages);
+	 */
 	struct list_head lpage_disallowed_mmu_pages;
 	struct kvm_page_track_notifier_node mmu_sp_tracker;
 	struct kvm_page_track_notifier_head track_notifier_head;
@@ -1085,6 +1223,15 @@ struct kvm_arch {
 	struct rw_semaphore apicv_update_lock;
 
 	bool apic_access_memslot_enabled;
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9032| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9041| <<kvm_apicv_init>> &kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9044| <<kvm_apicv_init>> &kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9733| <<__kvm_request_apicv_update>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|9755| <<__kvm_request_apicv_update>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|9761| <<__kvm_request_apicv_update>> kvm->arch.apicv_inhibit_reasons = new;
+	 */
 	unsigned long apicv_inhibit_reasons;
 
 	gpa_t wall_clock;
@@ -1095,27 +1242,96 @@ struct kvm_arch {
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * 在以下设置kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/x86.c|6312| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|11826| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|361| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|2941| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|2947| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3127| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 */
 	s64 kvmclock_offset;
 
 	/*
 	 * This also protects nr_vcpus_matched_tsc which is read from a
 	 * preemption-disabled region, so it must be a raw spinlock.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2601| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2656| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2710| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2912| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2944| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2963| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|5340| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5350| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11939| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|11941| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|11944| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11946| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 */
 	raw_spinlock_t tsc_write_lock;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2611| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2666| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|11802| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	u64 last_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2612| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+	 *   - arch/x86/kvm/x86.c|2677| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+	 *   - arch/x86/kvm/x86.c|11803| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_write = 0;
+	 */
 	u64 last_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2609| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2693| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|5343| <<kvm_arch_tsc_set_attr>> kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 */
 	u32 last_tsc_khz;
 	u64 last_tsc_offset;
 	u64 cur_tsc_nsec;
 	u64 cur_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2596| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|2662| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+	 */
 	u64 cur_tsc_offset;
 	u64 cur_tsc_generation;
 	int nr_vcpus_matched_tsc;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_sc:
+	 *   - arch/x86/kvm/x86.c|2917| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|2934| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3006| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3008| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3101| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3114| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|11895| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 */
 	seqcount_raw_spinlock_t pvclock_sc;
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2828| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 */
 	bool use_master_clock;
 	u64 master_kernel_ns;
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3185| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|3201| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11839| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11885| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
 	struct delayed_work kvmclock_sync_work;
 
@@ -1177,6 +1393,12 @@ struct kvm_arch {
 	 * true, TDP MMU handler functions will run for various MMU
 	 * operations.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_enabled:
+	 *   - arch/x86/kvm/mmu.h|328| <<is_tdp_mmu_enabled>> static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return kvm->arch.tdp_mmu_enabled; }
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|23| <<kvm_mmu_init_tdp_mmu>> kvm->arch.tdp_mmu_enabled = true;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|56| <<kvm_mmu_uninit_tdp_mmu>> if (!kvm->arch.tdp_mmu_enabled)
+	 */
 	bool tdp_mmu_enabled;
 
 	/*
@@ -1218,6 +1440,19 @@ struct kvm_arch {
 	 * It is acceptable, but not necessary, to acquire this lock when
 	 * the thread holds the MMU lock in write mode.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_pages_lock:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|32| <<kvm_mmu_init_tdp_mmu>> spin_lock_init(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|134| <<kvm_tdp_mmu_put_root>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|136| <<kvm_tdp_mmu_put_root>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|247| <<for_each_tdp_mmu_root>> lockdep_is_held(&kvm->arch.tdp_mmu_pages_lock)) \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<kvm_tdp_mmu_get_vcpu_root_hpa>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|318| <<kvm_tdp_mmu_get_vcpu_root_hpa>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|378| <<tdp_mmu_link_page>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|382| <<tdp_mmu_link_page>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|402| <<tdp_mmu_unlink_page>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|411| <<tdp_mmu_unlink_page>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 */
 	spinlock_t tdp_mmu_pages_lock;
 #endif /* CONFIG_X86_64 */
 
@@ -1226,6 +1461,11 @@ struct kvm_arch {
 	 * is used as one input when determining whether certain memslot
 	 * related allocations are necessary.
 	 */
+	/*
+	 * 在以下使用kvm_arch->shadow_root_allocated:
+	 *   - arch/x86/kvm/mmu.h|345| <<kvm_shadow_root_allocated>> return smp_load_acquire(&kvm->arch.shadow_root_allocated);
+	 *   - arch/x86/kvm/mmu/mmu.c|3721| <<mmu_first_shadow_root_alloc>> smp_store_release(&kvm->arch.shadow_root_allocated, true);
+	 */
 	bool shadow_root_allocated;
 
 #if IS_ENABLED(CONFIG_HYPERV)
@@ -1570,6 +1810,13 @@ static inline int kvm_arch_flush_remote_tlb(struct kvm *kvm)
 		return -ENOTSUPP;
 }
 
+/*
+ * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+ *   - arch/x86/include/asm/kvm_host.h|1796| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+ *   - arch/x86/kvm/x86.h|359| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+ *   - arch/x86/kvm/x86.h|364| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+ *   - arch/x86/kvm/x86.h|369| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+ */
 #define kvm_arch_pmi_in_guest(vcpu) \
 	((vcpu) && (vcpu)->arch.handling_intr_from_guest)
 
@@ -1823,6 +2070,32 @@ static inline unsigned long read_msr(unsigned long msr)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|674| <<nested_svm_vmrun>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/svm/nested.c|686| <<nested_svm_vmrun>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/svm/nested.c|799| <<nested_svm_vmexit>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/svm/nested.c|1119| <<nested_svm_check_permissions>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|1996| <<vmload_vmsave_interception>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|4463| <<svm_can_emulate_instruction>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/nested.c|3333| <<nested_vmx_check_permission>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4953| <<handle_vmon>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4962| <<handle_vmon>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|49| <<sgx_get_encls_gva>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|134| <<sgx_inject_fault>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|168| <<__handle_encls_ecreate>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|178| <<__handle_encls_ecreate>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|186| <<__handle_encls_ecreate>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|376| <<handle_encls>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|746| <<kvm_complete_insn_gp>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|757| <<complete_emulated_insn_gp>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|1081| <<kvm_emulate_xsetbv>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|1369| <<kvm_emulate_rdpmc>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|13030| <<kvm_handle_invpcid>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|13040| <<kvm_handle_invpcid>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|13048| <<kvm_handle_invpcid>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|13069| <<kvm_handle_invpcid>> kvm_inject_gp(vcpu, 0);
+ */
 static inline void kvm_inject_gp(struct kvm_vcpu *vcpu, u32 error_code)
 {
 	kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
diff --git a/arch/x86/include/asm/msr.h b/arch/x86/include/asm/msr.h
index d42e6c6b47b1..98ab3c0028b2 100644
--- a/arch/x86/include/asm/msr.h
+++ b/arch/x86/include/asm/msr.h
@@ -120,6 +120,14 @@ do {							\
 	__wrmsr((msr), (u32)((u64)(val)),		\
 		       (u32)((u64)(val) >> 32))
 
+/*
+ * 在以下使用native_read_msr():
+ *   - arch/x86/kernel/paravirt.c|301| <<global>> .cpu.read_msr = native_read_msr,
+ *   - arch/x86/include/asm/msr.h|256| <<rdmsr>> u64 __val = native_read_msr((msr)); \
+ *   - arch/x86/include/asm/msr.h|267| <<rdmsrl>> ((val) = native_read_msr((msr)))
+ *   - arch/x86/kvm/svm/svm.c|3799| <<svm_vcpu_run>> svm->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|6928| <<vmx_vcpu_run>> vmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
+ */
 static inline unsigned long long native_read_msr(unsigned int msr)
 {
 	unsigned long long val;
diff --git a/arch/x86/include/asm/pvclock-abi.h b/arch/x86/include/asm/pvclock-abi.h
index 1436226efe3e..6062f1a39619 100644
--- a/arch/x86/include/asm/pvclock-abi.h
+++ b/arch/x86/include/asm/pvclock-abi.h
@@ -24,9 +24,24 @@
  */
 
 struct pvclock_vcpu_time_info {
+	/*
+	 * 奇数表示Host正在修改,偶数表示已修改完毕
+	 */
 	u32   version;
 	u32   pad0;
+	/*
+	 * 在以下设置pvclock_vcpu_time_info->tsc_timestamp:
+	 *   - arch/x86/kvm/x86.c|3072| <<kvm_guest_time_update>> vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
+	 *
+	 * 更新pvti时, vCPU的vTSC
+	 */
 	u64   tsc_timestamp;
+	/*
+	 * 在以下设置pvclock_vcpu_time_info->system_time:
+	 *   - arch/x86/kvm/x86.c|3073| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *
+	 * 更新pvti时,Guest的虚拟时间,单位为纳秒
+	 */
 	u64   system_time;
 	u32   tsc_to_system_mul;
 	s8    tsc_shift;
diff --git a/arch/x86/include/asm/pvclock.h b/arch/x86/include/asm/pvclock.h
index 19b695ff2c68..67c34080218c 100644
--- a/arch/x86/include/asm/pvclock.h
+++ b/arch/x86/include/asm/pvclock.h
@@ -81,6 +81,10 @@ static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 static __always_inline
 u64 __pvclock_read_cycles(const struct pvclock_vcpu_time_info *src, u64 tsc)
 {
+	/*
+	 * 在以下设置pvclock_vcpu_time_info->tsc_timestamp:
+	 *   - arch/x86/kvm/x86.c|3072| <<kvm_guest_time_update>> vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
+	 */
 	u64 delta = tsc - src->tsc_timestamp;
 	u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
 					     src->tsc_shift);
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index d77481ecb0d5..69e4122189e3 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -704,6 +704,19 @@ static int kvm_cpu_down_prepare(unsigned int cpu)
 
 #endif
 
+/*
+ * [0] kvm_suspend
+ * [0] syscore_suspend
+ * [0] suspend_devices_and_enter
+ * [0] pm_suspend
+ * [0] state_store
+ * [0] kernfs_fop_write_iter
+ * [0] new_sync_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int kvm_suspend(void)
 {
 	kvm_guest_cpu_offline(false);
@@ -711,6 +724,19 @@ static int kvm_suspend(void)
 	return 0;
 }
 
+/*
+ * [0] kvm_resume
+ * [0] syscore_resume
+ * [0] suspend_devices_and_enter
+ * [0] pm_suspend
+ * [0] state_store
+ * [0] kernfs_fop_write_iter
+ * [0] new_sync_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void kvm_resume(void)
 {
 	kvm_cpu_online(raw_smp_processor_id());
@@ -1108,6 +1134,15 @@ static void kvm_enable_host_haltpoll(void *i)
 	wrmsrl(MSR_KVM_POLL_CONTROL, 1);
 }
 
+/*
+ * [0] arch_haltpoll_enable
+ * [0] haltpoll_cpu_online
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 void arch_haltpoll_enable(unsigned int cpu)
 {
 	if (!kvm_para_has_feature(KVM_FEATURE_POLL_CONTROL)) {
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index c5caa7311bd8..39f3d6279aec 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -26,6 +26,13 @@ static int kvmclock __initdata = 1;
 static int kvmclock_vsyscall __initdata = 1;
 static int msr_kvm_system_time __ro_after_init = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock __ro_after_init = MSR_KVM_WALL_CLOCK;
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|91| <<kvm_sched_clock_read>> return kvm_clock_read() - kvm_sched_clock_offset;
+ *   - arch/x86/kernel/kvmclock.c|98| <<kvm_sched_clock_init>> kvm_sched_clock_offset = kvm_clock_read();
+ *   - arch/x86/kernel/kvmclock.c|102| <<kvm_sched_clock_init>> kvm_sched_clock_offset);
+ *   - arch/x86/kernel/kvmclock.c|104| <<kvm_sched_clock_init>> BUILD_BUG_ON(sizeof(kvm_sched_clock_offset) >
+ */
 static u64 kvm_sched_clock_offset __ro_after_init;
 
 static int __init parse_no_kvmclock(char *arg)
@@ -91,6 +98,10 @@ static u64 kvm_sched_clock_read(void)
 	return kvm_clock_read() - kvm_sched_clock_offset;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|334| <<kvmclock_init>> kvm_sched_clock_init(flags & PVCLOCK_TSC_STABLE_BIT);
+ */
 static inline void kvm_sched_clock_init(bool stable)
 {
 	if (!stable)
@@ -164,6 +175,12 @@ struct clocksource kvm_clock = {
 };
 EXPORT_SYMBOL_GPL(kvm_clock);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|186| <<kvm_restore_sched_clock_state>> kvm_register_clock("primary cpu clock, resume");
+ *   - arch/x86/kernel/kvmclock.c|192| <<kvm_setup_secondary_clock>> kvm_register_clock("secondary cpu clock");
+ *   - arch/x86/kernel/kvmclock.c|310| <<kvmclock_init>> kvm_register_clock("primary cpu clock");
+ */
 static void kvm_register_clock(char *txt)
 {
 	struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
@@ -187,6 +204,10 @@ static void kvm_restore_sched_clock_state(void)
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC
+/*
+ * 在以下使用kvm_setup_secondary_clock():
+ *   - arch/x86/kernel/kvmclock.c|324| <<kvmclock_init>> x86_cpuinit.early_percpu_clock_init = kvm_setup_secondary_clock;
+ */
 static void kvm_setup_secondary_clock(void)
 {
 	kvm_register_clock("secondary cpu clock");
diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c
index 4420499f7bb4..2060736fdd9b 100644
--- a/arch/x86/kernel/paravirt.c
+++ b/arch/x86/kernel/paravirt.c
@@ -143,6 +143,13 @@ static u64 native_steal_clock(int cpu)
 DEFINE_STATIC_CALL(pv_steal_clock, native_steal_clock);
 DEFINE_STATIC_CALL(pv_sched_clock, native_sched_clock);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/vmware.c|340| <<vmware_paravirt_ops_setup>> paravirt_set_sched_clock(vmware_sched_clock);
+ *   - arch/x86/kernel/kvmclock.c|106| <<kvm_sched_clock_init>> paravirt_set_sched_clock(kvm_sched_clock_read);
+ *   - arch/x86/xen/time.c|527| <<xen_init_time_common>> paravirt_set_sched_clock(xen_sched_clock);
+ *   - drivers/clocksource/hyperv_timer.c|490| <<hv_setup_sched_clock>> paravirt_set_sched_clock(sched_clock);
+ */
 void paravirt_set_sched_clock(u64 (*func)(void))
 {
 	static_call_update(pv_sched_clock, func);
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 617012f4619f..bbf523776e58 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -236,6 +236,11 @@ static void notrace start_secondary(void *unused)
 #endif
 	cpu_init_secondary();
 	rcu_cpu_starting(raw_smp_processor_id());
+	/*
+	 * 两处设置的地方:
+	 *   - arch/x86/kernel/x86_init.c|126| <<global>> .early_percpu_clock_init = x86_init_noop,
+	 *   - arch/x86/kernel/kvmclock.c|324| <<kvmclock_init>> x86_cpuinit.early_percpu_clock_init = kvm_setup_secondary_clock;
+	 */
 	x86_cpuinit.early_percpu_clock_init();
 	smp_callin();
 
@@ -952,6 +957,10 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 }
 
 /* reduce the number of lines printed when booting a large cpu count system */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1100| <<do_boot_cpu>> announce_cpu(cpu, apicid);
+ */
 static void announce_cpu(int cpu, int apicid)
 {
 	static int current_node = NUMA_NO_NODE;
@@ -1079,6 +1088,10 @@ int common_cpu_up(unsigned int cpu, struct task_struct *idle)
  * Returns zero if CPU booted OK, else error code from
  * ->wakeup_secondary_cpu.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1239| <<native_cpu_up>> err = do_boot_cpu(apicid, cpu, tidle, &cpu0_nmi_registered);
+ */
 static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 		       int *cpu0_nmi_registered)
 {
@@ -1646,6 +1659,11 @@ static void remove_cpu_from_maps(int cpu)
 	numa_remove_cpu(cpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1684| <<native_cpu_disable>> cpu_disable_common();
+ *   - arch/x86/xen/smp_pv.c|359| <<xen_pv_cpu_disable>> cpu_disable_common();
+ */
 void cpu_disable_common(void)
 {
 	int cpu = smp_processor_id();
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a698196377be..b32c2c904116 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1313,6 +1313,12 @@ EXPORT_SYMBOL(convert_art_ns_to_tsc);
 
 
 static void tsc_refine_calibration_work(struct work_struct *work);
+/*
+ * 在以下使用tsc_irqwork:
+ *   - arch/x86/kernel/tsc.c|1316| <<global>> static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
+ *   - arch/x86/kernel/tsc.c|1356| <<tsc_refine_calibration_work>> schedule_delayed_work(&tsc_irqwork, HZ);
+ *   - arch/x86/kernel/tsc.c|1429| <<init_tsc_clocksource>> schedule_delayed_work(&tsc_irqwork, 0);
+ */
 static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
 /**
  * tsc_refine_calibration_work - Further refine tsc freq calibration
@@ -1328,6 +1334,10 @@ static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
  * calibration, we throw out the new calibration and use the
  * early calibration.
  */
+/*
+ * 在以下使用tsc_refine_calibration_work():
+ *   - arch/x86/kernel/tsc.c|1316| <<global>> static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
+ */
 static void tsc_refine_calibration_work(struct work_struct *work)
 {
 	static u64 tsc_start = ULLONG_MAX, ref_start;
diff --git a/arch/x86/kernel/tsc_sync.c b/arch/x86/kernel/tsc_sync.c
index 9452dc9664b5..e0e24d137cf5 100644
--- a/arch/x86/kernel/tsc_sync.c
+++ b/arch/x86/kernel/tsc_sync.c
@@ -29,6 +29,15 @@ struct tsc_adjust {
 	bool		warned;
 };
 
+/*
+ * 在以下使用percpu的tsc_adjust:
+ *   - arch/x86/kernel/tsc_sync.c|32| <<global>> static DEFINE_PER_CPU(struct tsc_adjust, tsc_adjust);
+ *   - arch/x86/kernel/tsc_sync.c|58| <<tsc_verify_tsc_adjust>> struct tsc_adjust *adj = this_cpu_ptr(&tsc_adjust);
+ *   - arch/x86/kernel/tsc_sync.c|169| <<tsc_store_and_check_tsc_adjust>> struct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);
+ *   - arch/x86/kernel/tsc_sync.c|198| <<tsc_store_and_check_tsc_adjust>> struct tsc_adjust *ref, *cur = this_cpu_ptr(&tsc_adjust);
+ *   - arch/x86/kernel/tsc_sync.c|234| <<tsc_store_and_check_tsc_adjust>> ref = per_cpu_ptr(&tsc_adjust, refcpu);
+ *   - arch/x86/kernel/tsc_sync.c|463| <<check_tsc_sync_target>> struct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);
+ */
 static DEFINE_PER_CPU(struct tsc_adjust, tsc_adjust);
 static struct timer_list tsc_sync_check_timer;
 
@@ -46,6 +55,13 @@ void mark_tsc_async_resets(char *reason)
 	pr_info("tsc: Marking TSC async resets true due to %s\n", reason);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/process.c|711| <<arch_cpu_idle_enter>> tsc_verify_tsc_adjust(false);
+ *   - arch/x86/kernel/tsc.c|1074| <<tsc_resume>> tsc_verify_tsc_adjust(true);
+ *   - arch/x86/kernel/tsc_sync.c|97| <<tsc_sync_check_timer_fn>> tsc_verify_tsc_adjust(false);
+ *   - arch/x86/power/cpu.c|260| <<__restore_processor_state>> tsc_verify_tsc_adjust(true);
+ */
 void tsc_verify_tsc_adjust(bool resume)
 {
 	struct tsc_adjust *adj = this_cpu_ptr(&tsc_adjust);
@@ -152,6 +168,11 @@ static void tsc_sanitize_first_cpu(struct tsc_adjust *cur, s64 bootval,
 }
 
 #ifndef CONFIG_SMP
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|1491| <<tsc_enable_sched_clock>> tsc_store_and_check_tsc_adjust(true);
+ *   - arch/x86/kernel/tsc_sync.c|460| <<check_tsc_sync_target>> if (tsc_store_and_check_tsc_adjust(false) || tsc_clocksource_reliable) {
+ */
 bool __init tsc_store_and_check_tsc_adjust(bool bootcpu)
 {
 	struct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);
@@ -176,6 +197,11 @@ bool __init tsc_store_and_check_tsc_adjust(bool bootcpu)
 /*
  * Store and check the TSC ADJUST MSR if available
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|1491| <<tsc_enable_sched_clock>> tsc_store_and_check_tsc_adjust(true);
+ *   - arch/x86/kernel/tsc_sync.c|460| <<check_tsc_sync_target>> if (tsc_store_and_check_tsc_adjust(false) || tsc_clocksource_reliable) {
+ */
 bool tsc_store_and_check_tsc_adjust(bool bootcpu)
 {
 	struct tsc_adjust *ref, *cur = this_cpu_ptr(&tsc_adjust);
@@ -437,6 +463,10 @@ void check_tsc_sync_source(int cpu)
 /*
  * Freshly booted CPUs call into this:
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|254| <<start_secondary>> check_tsc_sync_target();
+ */
 void check_tsc_sync_target(void)
 {
 	struct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index b8f8d268d058..c31ff5e99e69 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -29,6 +29,21 @@
  * Unlike "struct cpuinfo_x86.x86_capability", kvm_cpu_caps doesn't need to be
  * aligned to sizeof(unsigned long) because it's not accessed via bitops.
  */
+/*
+ * 在以下使用kvm_cpu_caps[NR_KVM_CPU_CAPS]:
+ *   -  arch/x86/kvm/cpuid.c|488| <<__kvm_cpu_cap_mask>> kvm_cpu_caps[leaf] &= *__cpuid_entry_get_reg(&entry, cpuid.reg);
+ *   - arch/x86/kvm/cpuid.c|497| <<kvm_cpu_cap_init_scattered>> kvm_cpu_caps[leaf] = mask;
+ *   - arch/x86/kvm/cpuid.c|507| <<kvm_cpu_cap_mask>> kvm_cpu_caps[leaf] &= mask;
+ *   - arch/x86/kvm/cpuid.c|523| <<kvm_set_cpu_caps>> memset(kvm_cpu_caps, 0, sizeof(kvm_cpu_caps));
+ *   - arch/x86/kvm/cpuid.c|525| <<kvm_set_cpu_caps>> BUILD_BUG_ON(sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)) >
+ *   - arch/x86/kvm/cpuid.c|528| <<kvm_set_cpu_caps>> memcpy(&kvm_cpu_caps, &boot_cpu_data.x86_capability,
+ *   - arch/x86/kvm/cpuid.c|529| <<kvm_set_cpu_caps>> sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)));
+ *   - arch/x86/kvm/cpuid.h|69| <<cpuid_entry_override>> BUILD_BUG_ON(leaf >= ARRAY_SIZE(kvm_cpu_caps));
+ *   - arch/x86/kvm/cpuid.h|70| <<cpuid_entry_override>> *reg = kvm_cpu_caps[leaf];
+ *   - arch/x86/kvm/cpuid.h|189| <<kvm_cpu_cap_clear>> kvm_cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
+ *   - arch/x86/kvm/cpuid.h|197| <<kvm_cpu_cap_set>> kvm_cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
+ *   - arch/x86/kvm/cpuid.h|205| <<kvm_cpu_cap_get>> return kvm_cpu_caps[x86_leaf] & __feature_bit(x86_feature);
+ */
 u32 kvm_cpu_caps[NR_KVM_CPU_CAPS] __read_mostly;
 EXPORT_SYMBOL_GPL(kvm_cpu_caps);
 
@@ -1157,6 +1172,10 @@ static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1251| <<kvm_dev_ioctl_get_cpuid>> r = get_cpuid_func(&array, funcs[i], type);
+ */
 static int do_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 			 unsigned int type)
 {
@@ -1219,6 +1238,10 @@ static bool sanity_check_entries(struct kvm_cpuid_entry2 __user *entries,
 	return false;
 }
 
+/*
+ * 处理KVM_GET_SUPPORTED_CPUID和KVM_GET_EMULATED_CPUID:
+ *   - arch/x86/kvm/x86.c|4442| <<kvm_arch_dev_ioctl>> r = kvm_dev_ioctl_get_cpuid(&cpuid, cpuid_arg->entries,
+ */
 int kvm_dev_ioctl_get_cpuid(struct kvm_cpuid2 *cpuid,
 			    struct kvm_cpuid_entry2 __user *entries,
 			    unsigned int type)
diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h
index 8a770b481d9d..8b2cebb0054c 100644
--- a/arch/x86/kvm/cpuid.h
+++ b/arch/x86/kvm/cpuid.h
@@ -197,6 +197,11 @@ static __always_inline void kvm_cpu_cap_set(unsigned int x86_feature)
 	kvm_cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.h|210| <<kvm_cpu_cap_has>> return !!kvm_cpu_cap_get(x86_feature);
+ *   - arch/x86/kvm/vmx/evmcs.c|336| <<nested_get_evmcs_version>> if (kvm_cpu_cap_get(X86_FEATURE_VMX) &&
+ */
 static __always_inline u32 kvm_cpu_cap_get(unsigned int x86_feature)
 {
 	unsigned int x86_leaf = __feature_leaf(x86_feature);
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 9322e6340a74..e6730cf24981 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -1272,6 +1272,11 @@ void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_set_eoi_accelerated);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2057| <<kvm_lapic_reg_write>> kvm_apic_send_ipi(apic, val, kvm_lapic_get_reg(apic, APIC_ICR2));
+ *   - arch/x86/kvm/x86.c|2034| <<handle_fastpath_set_x2apic_icr_irqoff>> kvm_apic_send_ipi(vcpu->arch.apic, (u32)data, (u32)(data >> 32));
+ */
 void kvm_apic_send_ipi(struct kvm_lapic *apic, u32 icr_low, u32 icr_high)
 {
 	struct kvm_lapic_irq irq;
@@ -1496,6 +1501,12 @@ static void limit_periodic_timer_frequency(struct kvm_lapic *apic)
 
 static void cancel_hv_timer(struct kvm_lapic *apic);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1528| <<apic_update_lvtt>> cancel_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2197| <<kvm_lapic_reg_write(APIC_TMICT)>> cancel_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2732| <<kvm_apic_set_state>> cancel_apic_timer(apic);
+ */
 static void cancel_apic_timer(struct kvm_lapic *apic)
 {
 	hrtimer_cancel(&apic->lapic_timer.timer);
@@ -1505,6 +1516,13 @@ static void cancel_apic_timer(struct kvm_lapic *apic)
 	preempt_enable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2134| <<kvm_lapic_reg_write(APIC_SPIV)>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|2179| <<kvm_lapic_reg_write(APIC_LVTT)>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|2436| <<kvm_lapic_reset>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|2723| <<kvm_apic_set_state>> apic_update_lvtt(apic);
+ */
 static void apic_update_lvtt(struct kvm_lapic *apic)
 {
 	u32 timer_mode = kvm_lapic_get_reg(apic, APIC_LVTT) &
@@ -1517,6 +1535,10 @@ static void apic_update_lvtt(struct kvm_lapic *apic)
 			kvm_lapic_set_reg(apic, APIC_TMICT, 0);
 			apic->lapic_timer.period = 0;
 			apic->lapic_timer.tscdeadline = 0;
+			/*
+			 * 这里缺少了
+			 * atomic_set(&apic->lapic_timer.pending, 0);
+			 */
 		}
 		apic->lapic_timer.timer_mode = timer_mode;
 		limit_periodic_timer_frequency(apic);
@@ -1546,8 +1568,18 @@ static bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1625| <<__kvm_wait_lapic_expire>> __wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);
+ */
 static inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)
 {
+	/*
+	 * 在以下修改kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/lapic.c|1604| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2545| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2548| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 */
 	u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
 
 	/*
@@ -1566,6 +1598,10 @@ static inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1631| <<__kvm_wait_lapic_expire>> adjust_lapic_timer_advance(vcpu, apic->lapic_timer.advance_expire_delta);
+ */
 static inline void adjust_lapic_timer_advance(struct kvm_vcpu *vcpu,
 					      s64 advance_expire_delta)
 {
@@ -1595,14 +1631,37 @@ static inline void adjust_lapic_timer_advance(struct kvm_vcpu *vcpu,
 	apic->lapic_timer.timer_advance_ns = timer_advance_ns;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1638| <<kvm_wait_lapic_expire>> __kvm_wait_lapic_expire(vcpu);
+ *   - arch/x86/kvm/lapic.c|1690| <<apic_timer_expired>> __kvm_wait_lapic_expire(vcpu);
+ */
 static void __kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 	u64 guest_tsc, tsc_deadline;
 
+	/*
+	 * 在以下设置kvm_timer->expired_tscdeadline:
+	 *   - arch/x86/kvm/lapic.c|1628| <<__kvm_wait_lapic_expire>> apic->lapic_timer.expired_tscdeadline = 0;
+	 *   - arch/x86/kvm/lapic.c|1727| <<apic_timer_expired>> ktimer->expired_tscdeadline = ktimer->tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|2712| <<kvm_apic_set_state>> apic->lapic_timer.expired_tscdeadline = 0;
+	 * 在以下使用kvm_timer->expired_tscdeadline:
+	 *   - arch/x86/kvm/lapic.c|1627| <<__kvm_wait_lapic_expire>> tsc_deadline = apic->lapic_timer.expired_tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|1690| <<kvm_wait_lapic_expire>> vcpu->arch.apic->lapic_timer.expired_tscdeadline &&
+	 *   - arch/x86/kvm/lapic.c|1743| <<apic_timer_expired>> if (vcpu->arch.apic->lapic_timer.expired_tscdeadline &&
+	 */
 	tsc_deadline = apic->lapic_timer.expired_tscdeadline;
 	apic->lapic_timer.expired_tscdeadline = 0;
 	guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	/*
+	 * 在以下修改kvm_timer->advance_expire_delta:
+	 *   - arch/x86/kvm/lapic.c|1615| <<__kvm_wait_lapic_expire>> apic->lapic_timer.advance_expire_delta = guest_tsc - tsc_deadline;
+	 *   - arch/x86/kvm/x86.c|10243| <<vcpu_enter_guest>> vcpu->arch.apic->lapic_timer.advance_expire_delta = S64_MIN;
+	 * 在以下使用kvm_timer->advance_expire_delta:
+	 *   - arch/x86/kvm/lapic.c|1618| <<__kvm_wait_lapic_expire>> adjust_lapic_timer_advance(vcpu, apic->lapic_timer.advance_expire_delta);
+	 *   - arch/x86/kvm/x86.c|10240| <<vcpu_enter_guest>> s64 delta = vcpu->arch.apic->lapic_timer.advance_expire_delta;
+	 */
 	apic->lapic_timer.advance_expire_delta = guest_tsc - tsc_deadline;
 
 	if (lapic_timer_advance_dynamic) {
@@ -1620,6 +1679,38 @@ static void __kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 		__wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);
 }
 
+/*
+ * commit d0659d946be05e098883b6955d2764595997f6a4
+ * Author: Marcelo Tosatti <mtosatti@redhat.com>
+ * Date:   Tue Dec 16 09:08:15 2014 -0500
+ *
+ * KVM: x86: add option to advance tscdeadline hrtimer expiration
+ *
+ * For the hrtimer which emulates the tscdeadline timer in the guest,
+ * add an option to advance expiration, and busy spin on VM-entry waiting
+ * for the actual expiration time to elapse.
+ *
+ * This allows achieving low latencies in cyclictest (or any scenario
+ * which requires strict timing regarding timer expiration).
+ *
+ * Reduces average cyclictest latency from 12us to 8us
+ * on Core i5 desktop.
+ *
+ * Note: this option requires tuning to find the appropriate value
+ * for a particular hardware/guest combination. One method is to measure the
+ * average delay between apic_timer_fn and VM-entry.
+ * Another method is to start with 1000ns, and increase the value
+ * in say 500ns increments until avg cyclictest numbers stop decreasing.
+ *
+ * Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ */
+
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3769| <<svm_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6899| <<vmx_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+ */
 void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu) &&
@@ -1643,6 +1734,14 @@ static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1721| <<start_sw_tscdeadline>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1829| <<start_sw_period>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1891| <<start_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1940| <<kvm_lapic_expired_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2454| <<apic_timer_fn>> apic_timer_expired(apic, true);
+ */
 static void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)
 {
 	struct kvm_vcpu *vcpu = apic->vcpu;
@@ -1919,6 +2018,10 @@ static void restart_apic_timer(struct kvm_lapic *apic)
 	preempt_enable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5665| <<handle_fastpath_preemption_timer>> kvm_lapic_expired_hv_timer(vcpu);
+ */
 void kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2178,6 +2281,10 @@ void kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_lapic_set_eoi);
 
 /* emulate APIC access in a trap manner */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5316| <<handle_apic_write>> kvm_apic_write_nodecode(vcpu, offset);
+ */
 void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)
 {
 	u32 val = 0;
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 2b44e533fc8d..0138be974b68 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -36,8 +36,42 @@ struct kvm_timer {
 	u32 timer_mode;
 	u32 timer_mode_mask;
 	u64 tscdeadline;
+	/*
+	 * 在以下设置kvm_timer->expired_tscdeadline:
+	 *   - arch/x86/kvm/lapic.c|1628| <<__kvm_wait_lapic_expire>> apic->lapic_timer.expired_tscdeadline = 0;
+	 *   - arch/x86/kvm/lapic.c|1727| <<apic_timer_expired>> ktimer->expired_tscdeadline = ktimer->tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|2712| <<kvm_apic_set_state>> apic->lapic_timer.expired_tscdeadline = 0;
+	 * 在以下使用kvm_timer->expired_tscdeadline:
+	 *   - arch/x86/kvm/lapic.c|1627| <<__kvm_wait_lapic_expire>> tsc_deadline = apic->lapic_timer.expired_tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|1690| <<kvm_wait_lapic_expire>> vcpu->arch.apic->lapic_timer.expired_tscdeadline &&
+	 *   - arch/x86/kvm/lapic.c|1743| <<apic_timer_expired>> if (vcpu->arch.apic->lapic_timer.expired_tscdeadline &&
+	 */
 	u64 expired_tscdeadline;
+	/*
+	 * 在以下修改kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/lapic.c|1604| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2545| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2548| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 * 在以下使用kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/debugfs.c|16| <<vcpu_get_timer_advance_ns>> *val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|1560| <<__wait_lapic_expire>> u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|1582| <<adjust_lapic_timer_advance>> u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|1681| <<kvm_wait_lapic_expire>> vcpu->arch.apic->lapic_timer.timer_advance_ns &&
+	 *   - arch/x86/kvm/lapic.c|1734| <<apic_timer_expired>> vcpu->arch.apic->lapic_timer.timer_advance_ns)
+	 *   - arch/x86/kvm/lapic.c|1769| <<start_sw_tscdeadline>> likely(ns > apic->lapic_timer.timer_advance_ns)) {
+	 *   - arch/x86/kvm/lapic.c|1771| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/lapic.c|2544| <<kvm_create_lapic>> if (timer_advance_ns == -1) {
+	 *   - arch/x86/kvm/vmx/vmx.c|7592| <<vmx_set_hv_timer>> ktimer->timer_advance_ns);
+	 */
 	u32 timer_advance_ns;
+	/*
+	 * 在以下修改kvm_timer->advance_expire_delta:
+	 *   - arch/x86/kvm/lapic.c|1615| <<__kvm_wait_lapic_expire>> apic->lapic_timer.advance_expire_delta = guest_tsc - tsc_deadline;
+	 *   - arch/x86/kvm/x86.c|10243| <<vcpu_enter_guest>> vcpu->arch.apic->lapic_timer.advance_expire_delta = S64_MIN;
+	 * 在以下使用kvm_timer->advance_expire_delta:
+	 *   - arch/x86/kvm/lapic.c|1618| <<__kvm_wait_lapic_expire>> adjust_lapic_timer_advance(vcpu, apic->lapic_timer.advance_expire_delta);
+	 *   - arch/x86/kvm/x86.c|10240| <<vcpu_enter_guest>> s64 delta = vcpu->arch.apic->lapic_timer.advance_expire_delta;
+	 */
 	s64 advance_expire_delta;
 	atomic_t pending;			/* accumulated triggered timers */
 	bool hv_timer_in_use;
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index e9fbb2c8bbe2..9486cebbf6d1 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -82,6 +82,11 @@ void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_prev_roots(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10048| <<vcpu_enter_guest>> r = kvm_mmu_reload(vcpu);
+ *   - arch/x86/kvm/x86.c|12271| <<kvm_arch_async_page_ready>> r = kvm_mmu_reload(vcpu);
+ */
 static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
 {
 	if (likely(vcpu->arch.mmu->root_hpa != INVALID_PAGE))
@@ -130,12 +135,28 @@ struct kvm_page_fault {
 
 	/* Derived from mmu and global state.  */
 	const bool is_tdp;
+	/*
+	 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+	 *   - arch/x86/kvm/mmu.h|205| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+	 *   - arch/x86/kvm/mmu/mmu.c|3100| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3179| <<__direct_map>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+	 */
 	const bool nx_huge_page_workaround_enabled;
 
 	/*
 	 * Whether a >4KB mapping can be created or is forbidden due to NX
 	 * hugepages.
 	 */
+	/*
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2895| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|2913| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<__direct_map>> if (fault->is_tdp && fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|746| <<FNAME(fetch)>> if (fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1072| <<kvm_tdp_mmu_map>> fault->huge_page_disallowed &&
+	 */
 	bool huge_page_disallowed;
 
 	/*
@@ -171,14 +192,32 @@ struct kvm_page_fault {
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);
 
 extern int nx_huge_pages;
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|221| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ *   - arch/x86/kvm/mmu/spte.c|119| <<make_spte>> is_nx_huge_page_enabled()) {
+ */
 static inline bool is_nx_huge_page_enabled(void)
 {
 	return READ_ONCE(nx_huge_pages);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5511| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa,
+ *   - arch/x86/kvm/x86.c|12199| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true);
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u32 err, bool prefetch)
 {
+	/*
+	 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+	 *   - arch/x86/kvm/mmu.h|205| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+	 *   - arch/x86/kvm/mmu/mmu.c|3100| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3179| <<__direct_map>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+	 */
 	struct kvm_page_fault fault = {
 		.addr = cr2_or_gpa,
 		.error_code = err,
@@ -235,6 +274,22 @@ static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
  *
  * TODO: introduce APIs to split these two cases.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|586| <<spte_has_volatile_bits>> (is_writable_pte(spte) && (spte & shadow_dirty_mask) == 0))
+ *   - arch/x86/kvm/mmu/mmu.c|655| <<mmu_spte_update>> !is_writable_pte(new_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|765| <<mmu_spte_age>> if (is_writable_pte(spte))
+ *   - arch/x86/kvm/mmu/mmu.c|1356| <<spte_write_protect>> if (!is_writable_pte(spte) &&
+ *   - arch/x86/kvm/mmu/mmu.c|3492| <<fast_pf_fix_direct_spte>> if (is_writable_pte(new_spte) && !is_writable_pte(old_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|3504| <<is_access_allowed>> return is_writable_pte(spte);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|206| <<audit_write_protection>> if (is_writable_pte(*sptep))
+ *   - arch/x86/kvm/mmu/spte.c|184| <<make_spte>> if (is_writable_pte(old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|366| <<handle_changed_spte_dirty_log>> if ((!is_writable_pte(old_spte) || pfn_changed) &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|367| <<handle_changed_spte_dirty_log>> is_writable_pte(new_spte)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1523| <<age_gfn_range>> if (is_writable_pte(new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1691| <<clear_dirty_gfn_range>> if (is_writable_pte(iter.old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1766| <<clear_dirty_pt_masked>> if (is_writable_pte(iter.old_spte))
+ */
 static inline bool is_writable_pte(unsigned long pte)
 {
 	return pte & PT_WRITABLE_MASK;
@@ -248,6 +303,11 @@ static inline bool is_writable_pte(unsigned long pte)
  * Return zero if the access does not fault; return the page fault error code
  * if the access faults.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|459| <<FNAME(walk_addr_generic)>> errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
+ *   - arch/x86/kvm/x86.c|6952| <<vcpu_mmio_gva_to_gpa>> !permission_fault(vcpu, vcpu->arch.walk_mmu,
+ */
 static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 				  unsigned pte_access, unsigned pte_pkey,
 				  unsigned pfec)
@@ -322,6 +382,23 @@ static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return kvm->arch.tdp_mm
 static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return false; }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|98| <<kvm_mmu_rmaps_stat_show>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1409| <<kvm_mmu_write_protect_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1442| <<kvm_mmu_clear_dirty_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1511| <<kvm_mmu_slot_gfn_write_protect>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|1676| <<kvm_unmap_gfn_range>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1689| <<kvm_set_spte_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1750| <<kvm_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1763| <<kvm_test_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|3690| <<mmu_first_shadow_root_alloc>> if (kvm_memslots_have_rmaps(kvm) &&
+ *   - arch/x86/kvm/mmu/mmu.c|6062| <<__kvm_zap_rmaps>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|6137| <<kvm_mmu_slot_remove_write_access>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|6221| <<kvm_mmu_zap_collapsible_sptes>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|6260| <<kvm_mmu_slot_leaf_clear_dirty>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/x86.c|11917| <<kvm_alloc_memslot_metadata>> if (kvm_memslots_have_rmaps(kvm)) {
+ */
 static inline bool kvm_memslots_have_rmaps(struct kvm *kvm)
 {
 	return !is_tdp_mmu_enabled(kvm) || kvm_shadow_root_allocated(kvm);
@@ -334,6 +411,10 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|119| <<kvm_mmu_rmaps_stat_show>> lpage_size = kvm_mmu_slot_lpages(slot, k + 1);
+ */
 static inline unsigned long
 __kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, unsigned long npages,
 		      int level)
@@ -348,14 +429,39 @@ kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, int level)
 	return __kvm_mmu_slot_lpages(slot, slot->npages, level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|689| <<mmu_spte_clear_track_bits>> kvm_update_page_stats(kvm, level, -1);
+ *   - arch/x86/kvm/mmu/mmu.c|2958| <<mmu_set_spte>> kvm_update_page_stats(vcpu->kvm, level, 1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|594| <<__handle_changed_spte>> kvm_update_page_stats(kvm, level, is_leaf ? 1 : -1);
+ */
 static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 {
+	/*
+	 * struct kvm:
+	 * -> struct kvm_vm_stat stat;
+	 *    -> union {
+	 *           struct {
+	 *               atomic64_t pages_4k;
+	 *               atomic64_t pages_2m;
+	 *               atomic64_t pages_1g;
+	 *           };
+	 *           atomic64_t pages[KVM_NR_PAGE_SIZES];
+	 *       };
+	 */
 	atomic64_add(count, &kvm->stat.pages[level - 1]);
 }
 
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 			   struct x86_exception *exception);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4002| <<nonpaging_gva_to_gpa>> return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|406| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|469| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(gfn), access, &walker->fault);
+ *   - arch/x86/kvm/x86.c|833| <<load_pdptrs>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn),
+ */
 static inline gpa_t kvm_translate_gpa(struct kvm_vcpu *vcpu,
 				      struct kvm_mmu *mmu,
 				      gpa_t gpa, u32 access,
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 5628d0ba637e..b951d1e9129e 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -57,6 +57,42 @@
 
 extern bool itlb_multihit_kvm_mitigation;
 
+/*
+ * commit 1aa9b9572b10529c2e64e2b8f44025d86e124308
+ * Author: Junaid Shahid <junaids@google.com>
+ * Date:   Mon Nov 4 20:26:00 2019 +0100
+ *
+ * kvm: x86: mmu: Recovery of shattered NX large pages
+ *
+ * The page table pages corresponding to broken down large pages are zapped in
+ * FIFO order, so that the large page can potentially be recovered, if it is
+ * not longer being used for execution.  This removes the performance penalty
+ * for walking deeper EPT page tables.
+ *
+ * By default, one large page will last about one hour once the guest
+ * reaches a steady state.
+ *
+ * Signed-off-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ * Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
+ *
+ *
+ * kvm.nx_huge_pages_recovery_ratio=
+ *     [KVM] Controls how many 4KiB pages are periodically zapped
+ *     back to huge pages.  0 disables the recovery, otherwise if
+ *     the value is N KVM will zap 1/Nth of the 4KiB pages every
+ *     minute.  The default is 60.
+ *
+ * 在以下使用nx_huge_pages:
+ *   - arch/x86/kvm/mmu/mmu.c|60| <<global>> int __read_mostly nx_huge_pages = -1;
+ *   - arch/x86/kvm/mmu/mmu.c|82| <<global>> module_param_cb(nx_huge_pages, &nx_huge_pages_ops, &nx_huge_pages, 0644);
+ *   - arch/x86/kvm/mmu/mmu.c|83| <<global>> __MODULE_PARM_TYPE(nx_huge_pages, "bool");
+ *   - arch/x86/kvm/mmu.h|184| <<is_nx_huge_page_enabled>> return READ_ONCE(nx_huge_pages);
+ *   - arch/x86/kvm/mmu/mmu.c|6328| <<__set_nx_huge_pages>> nx_huge_pages = itlb_multihit_kvm_mitigation = val;
+ *   - arch/x86/kvm/mmu/mmu.c|6333| <<set_nx_huge_pages>> bool old_val = nx_huge_pages;
+ *   - arch/x86/kvm/mmu/mmu.c|6374| <<kvm_mmu_module_init>> if (nx_huge_pages == -1)
+ *   - arch/x86/kvm/mmu/mmu.c|6441| <<calc_nx_huge_pages_recovery_period>> bool enabled = READ_ONCE(nx_huge_pages);
+ */
 int __read_mostly nx_huge_pages = -1;
 static uint __read_mostly nx_huge_pages_recovery_period_ms;
 #ifdef CONFIG_PREEMPT_RT
@@ -88,6 +124,26 @@ module_param_cb(nx_huge_pages_recovery_period_ms, &nx_huge_pages_recovery_param_
 		&nx_huge_pages_recovery_period_ms, 0644);
 __MODULE_PARM_TYPE(nx_huge_pages_recovery_period_ms, "uint");
 
+/*
+ * commit 71fe70130d88729abc0e658d3202618c340d2e71
+ * Author: Sean Christopherson <sean.j.christopherson@intel.com>
+ * Date:   Fri Mar 20 14:28:28 2020 -0700
+ *
+ * KVM: x86/mmu: Add module param to force TLB flush on root reuse
+ *
+ * Add a module param, flush_on_reuse, to override skip_tlb_flush and
+ * skip_mmu_sync when performing a so called "fast cr3 switch", i.e. when
+ * reusing a cached root.  The primary motiviation for the control is to
+ * provide a fallback mechanism in the event that TLB flushing and/or MMU
+ * sync bugs are exposed/introduced by upcoming changes to stop
+ * unconditionally flushing on nested VMX transitions.
+ *
+ * Suggested-by: Jim Mattson <jmattson@google.com>
+ * Suggested-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
+ * Message-Id: <20200320212833.3507-33-sean.j.christopherson@intel.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ */
 static bool __read_mostly force_flush_and_sync_on_reuse;
 module_param_named(flush_on_reuse, force_flush_and_sync_on_reuse, bool, 0644);
 
@@ -102,6 +158,12 @@ bool tdp_enabled = false;
 
 static int max_huge_page_level __read_mostly;
 static int tdp_root_level __read_mostly;
+/*
+ * 在以下使用max_tdp_level:
+ *   - arch/x86/kvm/mmu/mmu.c|4809| <<kvm_mmu_get_tdp_level>> if (max_tdp_level == 5 && cpuid_maxphyaddr(vcpu) <= 48)
+ *   - arch/x86/kvm/mmu/mmu.c|4812| <<kvm_mmu_get_tdp_level>> return max_tdp_level;
+ *   - arch/x86/kvm/mmu/mmu.c|5496| <<kvm_configure_mmu>> max_tdp_level = tdp_max_root_level;
+ */
 static int max_tdp_level __read_mostly;
 
 enum {
@@ -168,17 +230,30 @@ struct kvm_shadow_walk_iterator {
 	unsigned index;
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|954| <<FNAME(invlpg)>> for_each_shadow_entry_using_root(vcpu, root_hpa, gva, iterator) {
+ */
 #define for_each_shadow_entry_using_root(_vcpu, _root, _addr, _walker)     \
 	for (shadow_walk_init_using_root(&(_walker), (_vcpu),              \
 					 (_root), (_addr));                \
 	     shadow_walk_okay(&(_walker));			           \
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3058| <<__direct_map>> for_each_shadow_entry(vcpu, fault->addr, it) {
+ */
 #define for_each_shadow_entry(_vcpu, _addr, _walker)            \
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);	\
 	     shadow_walk_okay(&(_walker));			\
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3233| <<fast_pf_get_last_sptep>> for_each_shadow_entry_lockless(vcpu, gpa, iterator, old_spte) {
+ *   - arch/x86/kvm/mmu/mmu.c|4009| <<shadow_page_table_clear_flood>> for_each_shadow_entry_lockless(vcpu, addr, iterator, spte)
+ */
 #define for_each_shadow_entry_lockless(_vcpu, _addr, _walker, spte)	\
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);		\
 	     shadow_walk_okay(&(_walker)) &&				\
@@ -187,6 +262,13 @@ struct kvm_shadow_walk_iterator {
 
 static struct kmem_cache *pte_list_desc_cache;
 struct kmem_cache *mmu_page_header_cache;
+/*
+ * 在以下使用kvm_total_used_mmu_pages:
+ *   - arch/x86/kvm/mmu/mmu.c|1831| <<kvm_mod_used_mmu_pages>> percpu_counter_add(&kvm_total_used_mmu_pages, nr);
+ *   - arch/x86/kvm/mmu/mmu.c|6557| <<mmu_shrink_count>> return percpu_counter_read_positive(&kvm_total_used_mmu_pages);
+ *   - arch/x86/kvm/mmu/mmu.c|6700| <<kvm_mmu_module_init>> if (percpu_counter_init(&kvm_total_used_mmu_pages, 0, GFP_KERNEL))
+ *   - arch/x86/kvm/mmu/mmu.c|6725| <<kvm_mmu_module_exit>> percpu_counter_destroy(&kvm_total_used_mmu_pages);
+ */
 static struct percpu_counter kvm_total_used_mmu_pages;
 
 static void mmu_spte_set(u64 *sptep, u64 spte);
@@ -305,6 +387,11 @@ static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 	mmu_spte_set(sptep, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4309| <<handle_mmio_page_fault>> gfn_t gfn = get_mmio_spte_gfn(spte);
+ *   - arch/x86/kvm/mmu/mmu.c|4785| <<sync_mmio_spte>> if (gfn != get_mmio_spte_gfn(*sptep)) {
+ */
 static gfn_t get_mmio_spte_gfn(u64 spte)
 {
 	u64 gpa = spte & shadow_nonpresent_or_rsvd_lower_gfn_mask;
@@ -315,11 +402,19 @@ static gfn_t get_mmio_spte_gfn(u64 spte)
 	return gpa >> PAGE_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4310| <<handle_mmio_page_fault>> unsigned int access = get_mmio_spte_access(spte);
+ */
 static unsigned get_mmio_spte_access(u64 spte)
 {
 	return spte & shadow_mmio_access_mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4312| <<handle_mmio_page_fault>> if (!check_mmio_spte(vcpu, spte))
+ */
 static bool check_mmio_spte(struct kvm_vcpu *vcpu, u64 spte)
 {
 	u64 kvm_gen, spte_gen, gen;
@@ -484,6 +579,11 @@ static u64 __get_spte_lockless(u64 *sptep)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|620| <<mmu_spte_update_no_track>> if (!spte_has_volatile_bits(old_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|688| <<mmu_spte_clear_track_bits>> if (!spte_has_volatile_bits(old_spte))
+ */
 static bool spte_has_volatile_bits(u64 spte)
 {
 	if (!is_shadow_present_pte(spte))
@@ -647,6 +747,10 @@ static u64 mmu_spte_get_lockless(u64 *sptep)
 }
 
 /* Restore an acc-track PTE back to a regular PTE */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3618| <<fast_page_fault>> new_spte = restore_acc_track_spte(new_spte);
+ */
 static u64 restore_acc_track_spte(u64 spte)
 {
 	u64 new_spte = spte;
@@ -813,16 +917,30 @@ static void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|917| <<account_shadowed>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|131| <<kvm_slot_page_track_add_page>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|965| <<unaccount_shadowed>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|169| <<kvm_slot_page_track_remove_page>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, -1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2270| <<kvm_mmu_get_page>> account_shadowed(vcpu->kvm, sp);
+ */
 static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -842,17 +960,41 @@ static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	kvm_mmu_gfn_disallow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2986| <<__direct_map>> account_huge_nx_page(vcpu->kvm, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|748| <<FNAME(fetch)>> account_huge_nx_page(vcpu->kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|345| <<tdp_mmu_link_page>> account_huge_nx_page(kvm, sp);
+ */
 void account_huge_nx_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
+	/*
+	 * 注释:
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	if (sp->lpage_disallowed)
 		return;
 
 	++kvm->stat.nx_lpage_splits;
+	/*
+	 * 注释:
+	 * Use to track shadow pages that, if zapped, would allow KVM to create
+	 * an NX huge page.  A shadow page will have nx_huge_page_disallowed
+	 * set but not be on the list if a huge page is disallowed for other
+	 * reasons, e.g. because KVM is shadowing a PTE at the same gfn, the
+	 * memslot isn't properly aligned, etc...
+	 */
 	list_add_tail(&sp->lpage_disallowed_link,
 		      &kvm->arch.lpage_disallowed_mmu_pages);
 	sp->lpage_disallowed = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2550| <<__kvm_mmu_prepare_zap_page>> unaccount_shadowed(kvm, sp);
+ */
 static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -870,9 +1012,20 @@ static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	kvm_mmu_gfn_allow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2585| <<__kvm_mmu_prepare_zap_page>> unaccount_huge_nx_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|418| <<tdp_mmu_unlink_page>> unaccount_huge_nx_page(kvm, sp);
+ */
 void unaccount_huge_nx_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	--kvm->stat.nx_lpage_splits;
+	/*
+	 * 注释:
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	sp->lpage_disallowed = false;
 	list_del(&sp->lpage_disallowed_link);
 }
@@ -1189,6 +1342,12 @@ static bool __drop_large_spte(struct kvm *kvm, u64 *sptep)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3249| <<__direct_map>> drop_large_spte(vcpu, it.sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|680| <<FNAME(fetch)>> drop_large_spte(vcpu, it.sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|740| <<FNAME(fetch)>> drop_large_spte(vcpu, it.sptep);
+ */
 static void drop_large_spte(struct kvm_vcpu *vcpu, u64 *sptep)
 {
 	if (__drop_large_spte(vcpu->kvm, sptep)) {
@@ -1690,6 +1849,11 @@ static int is_empty_shadow_page(u64 *spt)
  * aggregate version in order to make the slab shrinker
  * faster
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1779| <<kvm_mmu_alloc_page>> kvm_mod_used_mmu_pages(vcpu->kvm, +1);
+ *   - arch/x86/kvm/mmu/mmu.c|2480| <<__kvm_mmu_prepare_zap_page>> kvm_mod_used_mmu_pages(kvm, -1);
+ */
 static inline void kvm_mod_used_mmu_pages(struct kvm *kvm, long nr)
 {
 	kvm->arch.n_used_mmu_pages += nr;
@@ -1864,6 +2028,20 @@ static int mmu_unsync_walk(struct kvm_mmu_page *sp,
 			   struct kvm_mmu_pages *pvec)
 {
 	pvec->nr = 0;
+	/*
+	 * 在以下修改kvm_mmu_page->unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|1942| <<mark_unsync>> if (sp->unsync_children++)
+	 *   - arch/x86/kvm/mmu/mmu.c|1981| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 * 在以下使用kvm_mmu_page->unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|1982| <<clear_unsync_child_bit>> WARN_ON((int )sp->unsync_children < 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|2002| <<__mmu_unsync_walk>> if (child->unsync_children) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2031| <<mmu_unsync_walk>> if (!sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/mmu.c|2185| <<mmu_pages_clear_parents>> } while (!sp->unsync_children);
+	 *   - arch/x86/kvm/mmu/mmu.c|2500| <<link_shadow_page>> if (sp->unsync_children || sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|4166| <<is_unsync_root>> if (sp->unsync || sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|704| <<FNAME(fetch)>> if (sp->unsync_children &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|991| <<FNAME(nvlpg)>> if (!sp->unsync_children)
+	 */
 	if (!sp->unsync_children)
 		return 0;
 
@@ -1933,6 +2111,17 @@ static bool is_obsolete_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 	if (sp->role.invalid)
 		return true;
 
+	/*
+	 * 在以下使用kvm_arch->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmu.c|6279| <<kvm_mmu_zap_all_fast>> kvm->arch.mmu_valid_gen = kvm->arch.mmu_valid_gen ? 0 : 1;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|299| <<__field>> __entry->mmu_valid_gen = kvm->arch.mmu_valid_gen;
+	 * 在以下使用kvm_mmu_page->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmutrace.h|19| <<KVM_MMU_PAGE_ASSIGN>> __entry->mmu_valid_gen = sp->mmu_valid_gen; \
+	 */
 	/* TDP MMU pages due not use the MMU generation. */
 	return !sp->tdp_mmu_page &&
 	       unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
@@ -2062,6 +2251,13 @@ static void clear_sp_write_flooding_count(u64 *spte)
 	__clear_sp_write_flooding_count(sptep_to_sp(spte));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3096| <<__direct_map>> sp = kvm_mmu_get_page(vcpu, base_gfn, it.addr,
+ *   - arch/x86/kvm/mmu/mmu.c|3487| <<mmu_alloc_root>> sp = kvm_mmu_get_page(vcpu, gfn, gva, level, direct, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|686| <<FNAME(fetch)>> sp = kvm_mmu_get_page(vcpu, table_gfn, fault->addr,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> sp = kvm_mmu_get_page(vcpu, base_gfn, fault->addr,
+ */
 static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 					     gfn_t gfn,
 					     gva_t gaddr,
@@ -2161,10 +2357,29 @@ static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|172| <<for_each_shadow_entry_using_root>> for (shadow_walk_init_using_root(&(_walker), (_vcpu), \
+ *   - arch/x86/kvm/mmu/mmu.c|2202| <<shadow_walk_init>> shadow_walk_init_using_root(iterator, vcpu, vcpu->arch.mmu->root_hpa,
+ *
+ * 核心思想.
+ * iterator->addr = addr;
+ * iterator->shadow_addr = root;
+ * iterator->level = vcpu->arch.mmu->shadow_root_level;
+ */
 static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterator,
 					struct kvm_vcpu *vcpu, hpa_t root,
 					u64 addr)
 {
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址
+	 *     int level;         // shadow_addr所在的level
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->addr = addr;
 	iterator->shadow_addr = root;
 	iterator->level = vcpu->arch.mmu->shadow_root_level;
@@ -2190,23 +2405,60 @@ static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterato
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|178| <<for_each_shadow_entry>> for (shadow_walk_init(&(_walker), _vcpu, _addr); \
+ *   - arch/x86/kvm/mmu/mmu.c|183| <<for_each_shadow_entry_lockless>> for (shadow_walk_init(&(_walker), _vcpu, _addr); \
+ *   - arch/x86/kvm/mmu/mmu.c|3815| <<get_walk>> for (shadow_walk_init(&iterator, vcpu, addr),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|674| <<FNAME(fetch)>> for (shadow_walk_init(&it, vcpu, fault->addr);
+ */
 static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,
 			     struct kvm_vcpu *vcpu, u64 addr)
 {
+	/*
+	 * 核心思想.
+	 * iterator->addr = addr;
+	 * iterator->shadow_addr = vcpu->arch.mmu->root_hpa;
+	 * iterator->level = vcpu->arch.mmu->shadow_root_level;
+	 */
 	shadow_walk_init_using_root(iterator, vcpu, vcpu->arch.mmu->root_hpa,
 				    addr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|174| <<for_each_shadow_entry_using_root>> shadow_walk_okay(&(_walker)); \
+ *   - arch/x86/kvm/mmu/mmu.c|179| <<for_each_shadow_entry>> shadow_walk_okay(&(_walker)); \
+ *   - arch/x86/kvm/mmu/mmu.c|184| <<for_each_shadow_entry_lockless>> shadow_walk_okay(&(_walker)) && \
+ *   - arch/x86/kvm/mmu/mmu.c|3817| <<get_walk>> shadow_walk_okay(&iterator);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|675| <<FNAME(fetch)>> shadow_walk_okay(&it) && it.level > gw->level;
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|724| <<FNAME(fetch)>> for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+ */
 static bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator)
 {
 	if (iterator->level < PG_LEVEL_4K)
 		return false;
 
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址 
+	 *     int level;         // shadow_addr所在的level
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->index = SHADOW_PT_INDEX(iterator->addr, iterator->level);
 	iterator->sptep	= ((u64 *)__va(iterator->shadow_addr)) + iterator->index;
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|186| <<for_each_shadow_entry_lockless>> __shadow_walk_next(&(_walker), spte))
+ *   - arch/x86/kvm/mmu/mmu.c|2230| <<shadow_walk_next>> __shadow_walk_next(iterator, *iterator->sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|3818| <<get_walk>> __shadow_walk_next(&iterator, spte)) {
+ */
 static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,
 			       u64 spte)
 {
@@ -2215,15 +2467,37 @@ static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,
 		return;
 	}
 
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址
+	 *     int level;         // shadow_addr所在的level (比方shadow_addr可以是hpa)
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->shadow_addr = spte & PT64_BASE_ADDR_MASK;
 	--iterator->level;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|175| <<for_each_shadow_entry_using_root>> shadow_walk_next(&(_walker)))
+ *   - arch/x86/kvm/mmu/mmu.c|180| <<for_each_shadow_entry>> shadow_walk_next(&(_walker)))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|676| <<FNAME(fetch)>> shadow_walk_next(&it)) {
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|724| <<FNAME(fetch)>> for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+ */
 static void shadow_walk_next(struct kvm_shadow_walk_iterator *iterator)
 {
 	__shadow_walk_next(iterator, *iterator->sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3005| <<__direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|717| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|745| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ */
 static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 			     struct kvm_mmu_page *sp)
 {
@@ -2241,6 +2515,10 @@ static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 		mark_unsync(sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|738| <<FNAME(fetch)>> validate_direct_spte(vcpu, it.sptep, direct_access);
+ */
 static void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 				   unsigned direct_access)
 {
@@ -2340,6 +2618,13 @@ static int mmu_zap_unsync_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2671| <<kvm_mmu_prepare_zap_page>> __kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|2727| <<kvm_mmu_zap_oldest_mmu_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list,
+ *   - arch/x86/kvm/mmu/mmu.c|6235| <<kvm_zap_obsolete_pages>> if (__kvm_mmu_prepare_zap_page(kvm, sp,
+ *   - arch/x86/kvm/mmu/mmu.c|6597| <<kvm_mmu_zap_all>> if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))
+ */
 static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 				       struct kvm_mmu_page *sp,
 				       struct list_head *invalid_list,
@@ -2398,6 +2683,18 @@ static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 	return list_unstable;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1909| <<kvm_sync_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2114| <<kvm_mmu_get_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp,
+ *   - arch/x86/kvm/mmu/mmu.c|2294| <<mmu_page_zap_pte>> return kvm_mmu_prepare_zap_page(kvm, child,
+ *   - arch/x86/kvm/mmu/mmu.c|2340| <<mmu_zap_unsync_children>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2543| <<kvm_mmu_unprotect_page>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3276| <<mmu_free_root_page>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|5313| <<kvm_mmu_pte_write>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|6327| <<kvm_recover_nx_lpages>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmutrace.h|203| <<KVM_MMU_PAGE_ASSIGN>> DEFINE_EVENT(kvm_mmu_page_class, kvm_mmu_prepare_zap_page,
+ */
 static bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				     struct list_head *invalid_list)
 {
@@ -2432,6 +2729,12 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2619| <<make_mmu_pages_available>> kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm, KVM_REFILL_PAGES - avail);
+ *   - arch/x86/kvm/mmu/mmu.c|2644| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
+ *   - arch/x86/kvm/mmu/mmu.c|6261| <<mmu_shrink_scan>> freed = kvm_mmu_zap_oldest_mmu_pages(kvm, sc->nr_to_scan);
+ */
 static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 						  unsigned long nr_to_zap)
 {
@@ -2469,6 +2772,10 @@ static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 	return total_zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2614| <<make_mmu_pages_available>> if (!kvm_mmu_available_pages(vcpu->kvm))
+ */
 static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 {
 	if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
@@ -2478,6 +2785,13 @@ static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3522| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|3656| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|4211| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|908| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+ */
 static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 {
 	unsigned long avail = kvm_mmu_available_pages(vcpu->kvm);
@@ -2505,6 +2819,11 @@ static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
  * Changing the number of mmu pages allocated to the vm
  * Note: if goal_nr_mmu_pages is too small, you will get dead lock
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5719| <<kvm_vm_ioctl_set_nr_mmu_pages>> kvm_mmu_change_mmu_pages(kvm, kvm_nr_mmu_pages);
+ *   - arch/x86/kvm/x86.c|12101| <<kvm_arch_commit_memory_region>> kvm_mmu_change_mmu_pages(kvm, nr_mmu_pages);
+ */
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned long goal_nr_mmu_pages)
 {
 	write_lock(&kvm->mmu_lock);
@@ -2673,6 +2992,13 @@ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2985| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, slot, start, access, gfn,
+ *   - arch/x86/kvm/mmu/mmu.c|3265| <<__direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|576| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, slot, spte, pte_access, gfn, pfn, NULL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|755| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access,
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			u64 *sptep, unsigned int pte_access, gfn_t gfn,
 			kvm_pfn_t pfn, struct kvm_page_fault *fault)
@@ -2777,6 +3103,11 @@ static int direct_pte_prefetch_many(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3042| <<direct_pte_prefetch>> __direct_pte_prefetch(vcpu, sp, sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|625| <<FNAME(pte_prefetch)>> return __direct_pte_prefetch(vcpu, sp, sptep);
+ */
 static void __direct_pte_prefetch(struct kvm_vcpu *vcpu,
 				  struct kvm_mmu_page *sp, u64 *sptep)
 {
@@ -2802,6 +3133,10 @@ static void __direct_pte_prefetch(struct kvm_vcpu *vcpu,
 		direct_pte_prefetch_many(vcpu, sp, start, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3270| <<__direct_map>> direct_pte_prefetch(vcpu, it.sptep);
+ */
 static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 {
 	struct kvm_mmu_page *sp;
@@ -2829,6 +3164,10 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 	__direct_pte_prefetch(vcpu, sp, sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2876| <<kvm_mmu_max_mapping_level>> host_level = host_pfn_mapping_level(kvm, gfn, pfn, slot);
+ */
 static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 				  const struct kvm_memory_slot *slot)
 {
@@ -2849,6 +3188,10 @@ static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 	 */
 	hva = __gfn_to_hva_memslot(slot, gfn);
 
+	/*
+	 * Lookup the page table entry for a virtual address in a given mm. Return a
+	 * pointer to the entry and the level of the mapping.
+	 */
 	pte = lookup_address_in_mm(kvm->mm, hva, &level);
 	if (unlikely(!pte))
 		return PG_LEVEL_4K;
@@ -2856,6 +3199,12 @@ static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 	return level;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2900| <<kvm_mmu_hugepage_adjust>> fault->req_level = kvm_mmu_max_mapping_level(vcpu->kvm, slot,
+ *   - arch/x86/kvm/mmu/mmu.c|5889| <<kvm_mmu_zap_collapsible_spte>> sp->role.level < kvm_mmu_max_mapping_level(kvm, slot, sp->gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1390| <<zap_collapsible_spte_range>> iter.level >= kvm_mmu_max_mapping_level(kvm, slot, iter.gfn,
+ */
 int kvm_mmu_max_mapping_level(struct kvm *kvm,
 			      const struct kvm_memory_slot *slot, gfn_t gfn,
 			      kvm_pfn_t pfn, int max_level)
@@ -2877,11 +3226,25 @@ int kvm_mmu_max_mapping_level(struct kvm *kvm,
 	return min(host_level, max_level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3200| <<__direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|720| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1282| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ */
 void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_memory_slot *slot = fault->slot;
 	kvm_pfn_t mask;
 
+	/*
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2895| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|2913| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<__direct_map>> if (fault->is_tdp && fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|746| <<FNAME(fetch)>> if (fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1072| <<kvm_tdp_mmu_map>> fault->huge_page_disallowed &&
+	 */
 	fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
 
 	if (unlikely(fault->max_level == PG_LEVEL_4K))
@@ -2913,6 +3276,15 @@ void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	fault->pfn &= ~mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3180| <<__direct_map>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|732| <<FNAME(fetch)>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1253| <<kvm_tdp_mmu_map>> disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
+ *
+ * 核心思想: 本来是想用huge page的,但是已经存在一个present的pte但是不是large pte,
+ * 所以就没法huge page了, 只能让goal_level再下降一位
+ */
 void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_level)
 {
 	if (cur_level > PG_LEVEL_4K &&
@@ -2933,21 +3305,90 @@ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_
 	}
 }
 
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4103| <<direct_page_fault>> r = __direct_map(vcpu, fault);
+ *
+ * tdp mmu的版本是kvm_tdp_mmu_map()
+ */
 static int __direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;
+	 *     hpa_t shadow_addr;
+	 *     u64 *sptep;
+	 *     int level;
+	 *     unsigned index;
+	 * };
+	 */
 	struct kvm_shadow_walk_iterator it;
 	struct kvm_mmu_page *sp;
 	int ret;
 	gfn_t base_gfn = fault->gfn;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3200| <<__direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|720| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1282| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3266| <<__direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|722| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1335| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址
+	 *     int level;         // shadow_addr所在的level (比方shadow_addr可以是hpa)
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	for_each_shadow_entry(vcpu, fault->addr, it) {
 		/*
 		 * We cannot overwrite existing page tables with an NX
 		 * large page, as the leaf could be executable.
 		 */
+		/*
+		 * disallowed_hugepage_adjust():
+		 * 核心思想: 本来是想用huge page的,但是已经存在一个present的pte但是不是large pte,
+		 * 所以就没法huge page了, 只能让goal_level再下降一位
+		 */
 		if (fault->nx_huge_page_workaround_enabled)
 			disallowed_hugepage_adjust(fault, *it.sptep, it.level);
 
@@ -2963,11 +3404,20 @@ static int __direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 				      it.level - 1, true, ACC_ALL);
 
 		link_shadow_page(vcpu, it.sptep, sp);
+		/*
+		 * 在以下使用account_huge_nx_page():
+		 *   - arch/x86/kvm/mmu/mmu.c|2986| <<__direct_map>> account_huge_nx_page(vcpu->kvm, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|748| <<FNAME(fetch)>> account_huge_nx_page(vcpu->kvm, sp);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|345| <<tdp_mmu_link_page>> account_huge_nx_page(kvm, sp);
+		 */
 		if (fault->is_tdp && fault->huge_page_disallowed &&
 		    fault->req_level >= it.level)
 			account_huge_nx_page(vcpu->kvm, sp);
 	}
 
+	/*
+	 * WARN_ON_ONCE说明这里不会发生
+	 */
 	if (WARN_ON_ONCE(it.level != fault->goal_level))
 		return -EFAULT;
 
@@ -3004,6 +3454,11 @@ static int kvm_handle_bad_page(struct kvm_vcpu *vcpu, gfn_t gfn, kvm_pfn_t pfn)
 	return -EFAULT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4316| <<direct_page_fault>> if (handle_abnormal_pfn(vcpu, fault, ACC_ALL, &r))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|879| <<FNAME(page_fault)>> if (handle_abnormal_pfn(vcpu, fault, walker.pte_access, &r))
+ */
 static bool handle_abnormal_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 				unsigned int access, int *ret_val)
 {
@@ -3032,6 +3487,10 @@ static bool handle_abnormal_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fa
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3589| <<fast_page_fault>> if (!page_fault_can_be_fast(fault))
+ */
 static bool page_fault_can_be_fast(struct kvm_page_fault *fault)
 {
 	/*
@@ -3041,6 +3500,9 @@ static bool page_fault_can_be_fast(struct kvm_page_fault *fault)
 	if (fault->rsvd)
 		return false;
 
+	/*
+	 * 如果non-present, 下面一定不返回, 会继续 ... present就是read
+	 */
 	/* See if the page fault is due to an NX violation */
 	if (unlikely(fault->exec && fault->present))
 		return false;
@@ -3112,6 +3574,10 @@ static bool is_access_allowed(struct kvm_page_fault *fault, u64 spte)
  *  - Must be called between walk_shadow_page_lockless_{begin,end}.
  *  - The returned sptep must not be used after walk_shadow_page_lockless_end.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3538| <<fast_page_fault>> sptep = fast_pf_get_last_sptep(vcpu, fault->addr, &spte);
+ */
 static u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)
 {
 	struct kvm_shadow_walk_iterator iterator;
@@ -3129,6 +3595,38 @@ static u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)
 /*
  * Returns one of RET_PF_INVALID, RET_PF_FIXED or RET_PF_SPURIOUS.
  */
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4426| <<direct_page_fault>> r = fast_page_fault(vcpu, fault);
+ */
 static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu_page *sp;
@@ -3174,6 +3672,11 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 
 		new_spte = spte;
 
+		/*
+		 * 这里应该是如果hardware的a/d bit不被支持的话,
+		 * 需要通过page fault来标记
+		 * 如果硬件支持就可以不用page fault
+		 */
 		if (is_access_track_spte(spte))
 			new_spte = restore_acc_track_spte(new_spte);
 
@@ -3230,6 +3733,12 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3287| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->prev_roots[i].hpa,
+ *   - arch/x86/kvm/mmu/mmu.c|3293| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->root_hpa, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3299| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->pae_root[i],
+ */
 static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 			       struct list_head *invalid_list)
 {
@@ -3350,6 +3859,10 @@ static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, gva_t gva,
 	return __pa(sp->spt);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5129| <<kvm_mmu_load>> r = mmu_alloc_direct_roots(vcpu);
+ */
 static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -3568,6 +4081,10 @@ static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5639| <<kvm_mmu_load>> r = mmu_alloc_special_roots(vcpu);
+ */
 static int mmu_alloc_special_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -3825,6 +4342,10 @@ static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 	return reserved;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5863| <<kvm_mmu_page_fault>> r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
+ */
 static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 {
 	u64 spte;
@@ -3914,6 +4435,13 @@ static bool kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 				  kvm_vcpu_gfn_to_hva(vcpu, gfn), &arch);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4302| <<direct_page_fault>> if (kvm_faultin_pfn(vcpu, fault, &r))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|876| <<FNAME(page_fault)>> if (kvm_faultin_pfn(vcpu, fault, &r))
+ *
+ * 核心思想是填充fault->pfn
+ */
 static bool kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault, int *r)
 {
 	struct kvm_memory_slot *slot = fault->slot;
@@ -4003,6 +4531,11 @@ static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 	       mmu_notifier_retry_hva(vcpu->kvm, mmu_seq, fault->hva);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4121| <<nonpaging_page_fault>> return direct_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4169| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, fault);
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	bool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);
@@ -4024,17 +4557,39 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (r)
 		return r;
 
+	/*
+	 * arm和x86在以下使用kvm->mmu_notifier_seq:
+	 *   - arch/arm64/kvm/mmu.c|1167| <<user_mem_abort>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - arch/x86/kvm/mmu/mmu.c|4528| <<direct_page_fault>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|879| <<FNAME(page_fault)>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - include/linux/kvm_host.h|1936| <<mmu_notifier_retry>> if (kvm->mmu_notifier_seq != mmu_seq)
+	 *   - include/linux/kvm_host.h|1956| <<mmu_notifier_retry_hva>> if (kvm->mmu_notifier_seq != mmu_seq)
+	 *   - virt/kvm/kvm_main.c|745| <<kvm_dec_notifier_count>> kvm->mmu_notifier_seq++;
+	 *   - virt/kvm/pfncache.c|151| <<hva_to_pfn_retry>> mmu_seq = kvm->mmu_notifier_seq;
+	 */
 	mmu_seq = vcpu->kvm->mmu_notifier_seq;
 	smp_rmb();
 
+	/*
+	 * 核心思想是填充fault->pfn
+	 */
 	if (kvm_faultin_pfn(vcpu, fault, &r))
 		return r;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4316| <<direct_page_fault>> if (handle_abnormal_pfn(vcpu, fault, ACC_ALL, &r))
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|879| <<FNAME(page_fault)>> if (handle_abnormal_pfn(vcpu, fault, walker.pte_access, &r))
+	 */
 	if (handle_abnormal_pfn(vcpu, fault, ACC_ALL, &r))
 		return r;
 
 	r = RET_PF_RETRY;
 
+	/*
+	 * 如果走上面的fast_page_fault()就不用任何lock了
+	 * 所以是fast path
+	 */
 	if (is_tdp_mmu_fault)
 		read_lock(&vcpu->kvm->mmu_lock);
 	else
@@ -4061,6 +4616,10 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	return r;
 }
 
+/*
+ * 在以下使用nonpaging_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|4174| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+ */
 static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
 				struct kvm_page_fault *fault)
 {
@@ -4071,6 +4630,11 @@ static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
 	return direct_page_fault(vcpu, fault);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1775| <<pf_interception>> return kvm_handle_page_fault(vcpu, error_code, fault_address,
+ *   - arch/x86/kvm/vmx/vmx.c|4903| <<handle_exception_nmi>> return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
+ */
 int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 				u64 fault_address, char *insn, int insn_len)
 {
@@ -4104,9 +4668,46 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 }
 EXPORT_SYMBOL_GPL(kvm_handle_page_fault);
 
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * 在以下使用kvm_tdp_page_fault():
+ *   - arch/x86/kvm/mmu.h|199| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *   - arch/x86/kvm/mmu.h|208| <<kvm_mmu_do_page_fault>> return kvm_tdp_page_fault(vcpu, &fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4840| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	while (fault->max_level > PG_LEVEL_4K) {
+		/*
+		 * kvm最大1G, 2M, 4K ...
+		 */
 		int page_num = KVM_PAGES_PER_HPAGE(fault->max_level);
 		gfn_t base = (fault->addr >> PAGE_SHIFT) & ~(page_num - 1);
 
@@ -4115,6 +4716,10 @@ int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 
 		--fault->max_level;
 	}
+	/*
+	 * 大部分情况可以假设, "--fault->max_level"不现实
+	 * 结果还是.max_level = KVM_MAX_HUGEPAGE_LEVEL
+	 */
 
 	return direct_page_fault(vcpu, fault);
 }
@@ -4236,6 +4841,10 @@ static unsigned long get_cr3(struct kvm_vcpu *vcpu)
 	return kvm_read_cr3(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1094| <<FNAME(sync_page)>> if (sync_mmio_spte(vcpu, &sp->spt[i], gfn, pte_access))
+ */
 static bool sync_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,
 			   unsigned int access)
 {
@@ -5300,6 +5909,13 @@ static void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 	write_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4277| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+ *   - arch/x86/kvm/svm/svm.c|1789| <<npf_interception>> return kvm_mmu_page_fault(vcpu, fault_address, error_code,
+ *   - arch/x86/kvm/vmx/vmx.c|5430| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5451| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -5575,6 +6191,10 @@ static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11180| <<kvm_arch_vcpu_create>> r = kvm_mmu_create(vcpu);
+ */
 int kvm_mmu_create(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -5605,6 +6225,10 @@ int kvm_mmu_create(struct kvm_vcpu *vcpu)
 }
 
 #define BATCH_ZAP_PAGES	10
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5899| <<kvm_mmu_zap_all_fast>> kvm_zap_obsolete_pages(kvm);
+ */
 static void kvm_zap_obsolete_pages(struct kvm *kvm)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -5664,6 +6288,12 @@ static void kvm_zap_obsolete_pages(struct kvm *kvm)
  * not use any resource of the being-deleted slot or all slots
  * after calling the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5772| <<kvm_mmu_invalidate_zap_pages_in_memslot>> kvm_mmu_zap_all_fast(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|6070| <<kvm_mmu_invalidate_mmio_sptes>> kvm_mmu_zap_all_fast(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|6186| <<set_nx_huge_pages>> kvm_mmu_zap_all_fast(kvm);
+ */
 static void kvm_mmu_zap_all_fast(struct kvm *kvm)
 {
 	lockdep_assert_held(&kvm->slots_lock);
@@ -5715,6 +6345,10 @@ static bool kvm_has_zapped_obsolete_pages(struct kvm *kvm)
 	return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));
 }
 
+/*
+ * 在以下使用kvm_mmu_invalidate_zap_pages_in_memslot():
+ *   - arch/x86/kvm/mmu/mmu.c|6199| <<kvm_mmu_init_vm>> node->track_flush_slot = kvm_mmu_invalidate_zap_pages_in_memslot;
+ */
 static void kvm_mmu_invalidate_zap_pages_in_memslot(struct kvm *kvm,
 			struct kvm_memory_slot *slot,
 			struct kvm_page_track_notifier_node *node)
@@ -5780,6 +6414,12 @@ static bool __kvm_zap_rmaps(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
  * Invalidate (zap) SPTEs that cover GFNs from gfn_start and up to gfn_end
  * (not including it)
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|332| <<update_mtrr>> kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));
+ *   - arch/x86/kvm/x86.c|880| <<kvm_post_set_cr0>> kvm_zap_gfn_range(vcpu->kvm, 0, ~0ULL);
+ *   - arch/x86/kvm/x86.c|9758| <<__kvm_request_apicv_update>> kvm_zap_gfn_range(kvm, gfn, gfn+1);
+ */
 void kvm_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
 {
 	bool flush;
@@ -5971,6 +6611,10 @@ void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 		kvm_arch_flush_remote_tlbs_memslot(kvm, memslot);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12113| <<kvm_arch_flush_shadow_all>> kvm_mmu_zap_all(kvm);
+ */
 void kvm_mmu_zap_all(struct kvm *kvm)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -6083,6 +6727,13 @@ mmu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)
 static unsigned long
 mmu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)
 {
+	/*
+	 * 在以下使用kvm_total_used_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1831| <<kvm_mod_used_mmu_pages>> percpu_counter_add(&kvm_total_used_mmu_pages, nr);
+	 *   - arch/x86/kvm/mmu/mmu.c|6557| <<mmu_shrink_count>> return percpu_counter_read_positive(&kvm_total_used_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6700| <<kvm_mmu_module_init>> if (percpu_counter_init(&kvm_total_used_mmu_pages, 0, GFP_KERNEL))
+	 *   - arch/x86/kvm/mmu/mmu.c|6725| <<kvm_mmu_module_exit>> percpu_counter_destroy(&kvm_total_used_mmu_pages);
+	 */
 	return percpu_counter_read_positive(&kvm_total_used_mmu_pages);
 }
 
@@ -6092,6 +6743,11 @@ static struct shrinker mmu_shrinker = {
 	.seeks = DEFAULT_SEEKS * 10,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6369| <<kvm_mmu_module_init>> mmu_destroy_caches();
+ *   - arch/x86/kvm/mmu/mmu.c|6383| <<kvm_mmu_module_exit>> mmu_destroy_caches();
+ */
 static void mmu_destroy_caches(void)
 {
 	kmem_cache_destroy(pte_list_desc_cache);
@@ -6104,11 +6760,48 @@ static bool get_nx_auto_mode(void)
 	return boot_cpu_has_bug(X86_BUG_ITLB_MULTIHIT) && !cpu_mitigations_off();
 }
 
+/*
+ * commit b8e8c8303ff28c61046a4d0f6ea99aea609a7dc0
+ * Author: Paolo Bonzini <pbonzini@redhat.com>
+ * Date:   Mon Nov 4 12:22:02 2019 +0100
+ *
+ * kvm: mmu: ITLB_MULTIHIT mitigation
+ *
+ * With some Intel processors, putting the same virtual address in the TLB
+ * as both a 4 KiB and 2 MiB page can confuse the instruction fetch unit
+ * and cause the processor to issue a machine check resulting in a CPU lockup.
+ *
+ * Unfortunately when EPT page tables use huge pages, it is possible for a
+ * malicious guest to cause this situation.
+ *
+ * Add a knob to mark huge pages as non-executable. When the nx_huge_pages
+ * parameter is enabled (and we are using EPT), all huge pages are marked as
+ * NX. If the guest attempts to execute in one of those pages, the page is
+ * broken down into 4K pages, which are then marked executable.
+ *
+ * This is not an issue for shadow paging (except nested EPT), because then
+ * the host is in control of TLB flushes and the problematic situation cannot
+ * happen.  With nested EPT, again the nested guest can cause problems shadow
+ * and direct EPT is treated in the same way.
+ *
+ * [ tglx: Fixup default to auto and massage wording a bit ]
+ *
+ * Originally-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ * Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6346| <<set_nx_huge_pages>> __set_nx_huge_pages(new_val);
+ *   - arch/x86/kvm/mmu/mmu.c|6375| <<kvm_mmu_module_init>> __set_nx_huge_pages(get_nx_auto_mode());
+ */
 static void __set_nx_huge_pages(bool val)
 {
 	nx_huge_pages = itlb_multihit_kvm_mitigation = val;
 }
 
+/*
+ * struct kernel_param_ops nx_huge_pages_ops.set = set_nx_huge_pages()
+ */
 static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
 {
 	bool old_val = nx_huge_pages;
@@ -6144,6 +6837,15 @@ static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8844| <<kvm_arch_init>> r = kvm_mmu_module_init();
+ *
+ * vmx_init()
+ * -> kvm_init()
+ *    -> kvm_arch_init()
+ *       -> kvm_mmu_module_init()
+ */
 int kvm_mmu_module_init(void)
 {
 	int ret = -ENOMEM;
@@ -6189,6 +6891,11 @@ int kvm_mmu_module_init(void)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11260| <<kvm_arch_vcpu_create>> kvm_mmu_destroy(vcpu);
+ *   - arch/x86/kvm/x86.c|11301| <<kvm_arch_vcpu_destroy>> kvm_mmu_destroy(vcpu);
+ */
 void kvm_mmu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_unload(vcpu);
@@ -6197,6 +6904,10 @@ void kvm_mmu_destroy(struct kvm_vcpu *vcpu)
 	mmu_free_memory_caches(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8920| <<kvm_arch_exit>> kvm_mmu_module_exit();
+ */
 void kvm_mmu_module_exit(void)
 {
 	mmu_destroy_caches();
@@ -6275,6 +6986,13 @@ static void kvm_recover_nx_lpages(struct kvm *kvm)
 	ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
 	to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
 	for ( ; to_zap; --to_zap) {
+		/*
+		 * 在以下使用kvm_arch->lpage_disallowed_mmu_pages:
+		 *   - arch/x86/kvm/mmu/mmu.c|947| <<account_huge_nx_page>> list_add_tail(&sp->lpage_disallowed_link, &kvm->arch.lpage_disallowed_mmu_pages);
+		 *   - arch/x86/kvm/mmu/mmu.c|6641| <<kvm_recover_nx_lpages>> if (list_empty(&kvm->arch.lpage_disallowed_mmu_pages))
+		 *   - arch/x86/kvm/mmu/mmu.c|6649| <<kvm_recover_nx_lpages>> sp = list_first_entry(&kvm->arch.lpage_disallowed_mmu_pages,
+		 *   - arch/x86/kvm/x86.c|11639| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.lpage_disallowed_mmu_pages);
+		 */
 		if (list_empty(&kvm->arch.lpage_disallowed_mmu_pages))
 			break;
 
@@ -6342,6 +7060,10 @@ static int kvm_nx_lpage_recovery_worker(struct kvm *kvm, uintptr_t data)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11707| <<kvm_arch_post_init_vm>> return kvm_mmu_post_init_vm(kvm);
+ */
 int kvm_mmu_post_init_vm(struct kvm *kvm)
 {
 	int err;
@@ -6355,6 +7077,10 @@ int kvm_mmu_post_init_vm(struct kvm *kvm)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11821| <<kvm_arch_pre_destroy_vm>> kvm_mmu_pre_destroy_vm(kvm);
+ */
 void kvm_mmu_pre_destroy_vm(struct kvm *kvm)
 {
 	if (kvm->arch.nx_lpage_recovery_thread)
diff --git a/arch/x86/kvm/mmu/mmu_audit.c b/arch/x86/kvm/mmu/mmu_audit.c
index 9e7dcf999f08..b62833c457f0 100644
--- a/arch/x86/kvm/mmu/mmu_audit.c
+++ b/arch/x86/kvm/mmu/mmu_audit.c
@@ -233,6 +233,14 @@ static void audit_vcpu_spte(struct kvm_vcpu *vcpu)
 	mmu_spte_walk(vcpu, audit_spte);
 }
 
+/*
+ * 在以下使用mmu_audit:
+ *   - arch/x86/kvm/mmu/mmu_audit.c|303| <<global>> arch_param_cb(mmu_audit, &audit_param_ops, &mmu_audit, 0644);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|259| <<mmu_audit_enable>> if (mmu_audit)
+ *   - arch/x86/kvm/mmu/mmu_audit.c|263| <<mmu_audit_enable>> mmu_audit = true;
+ *   - arch/x86/kvm/mmu/mmu_audit.c|268| <<mmu_audit_disable>> if (!mmu_audit)
+ *   - arch/x86/kvm/mmu/mmu_audit.c|272| <<mmu_audit_disable>> mmu_audit = false;
+ */
 static bool mmu_audit;
 static DEFINE_STATIC_KEY_FALSE(mmu_audit_key);
 
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index da6166b5c377..407205ee242c 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -40,7 +40,35 @@ struct kvm_mmu_page {
 
 	bool tdp_mmu_page;
 	bool unsync;
+	/*
+	 * 在以下使用kvm_arch->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmu.c|6279| <<kvm_mmu_zap_all_fast>> kvm->arch.mmu_valid_gen = kvm->arch.mmu_valid_gen ? 0 : 1;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|299| <<__field>> __entry->mmu_valid_gen = kvm->arch.mmu_valid_gen;
+	 * 在以下使用kvm_mmu_page->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmutrace.h|19| <<KVM_MMU_PAGE_ASSIGN>> __entry->mmu_valid_gen = sp->mmu_valid_gen; \
+	 */
 	u8 mmu_valid_gen;
+	/*
+	 * 注释:
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
+	/*
+	 * 在以下设置kvm_mmu_page->lpage_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|934| <<account_huge_nx_page>> sp->lpage_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|957| <<unaccount_huge_nx_page>> sp->lpage_disallowed = false;
+	 * 在以下使用kvm_mmu_page->lpage_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|928| <<account_huge_nx_page>> if (sp->lpage_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2558| <<__kvm_mmu_prepare_zap_page>> if (sp->lpage_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|6620| <<kvm_recover_nx_lpages>> WARN_ON_ONCE(!sp->lpage_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|6625| <<kvm_recover_nx_lpages>> WARN_ON_ONCE(sp->lpage_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|407| <<tdp_mmu_unlink_page>> if (sp->lpage_disallowed)
+	 */
 	bool lpage_disallowed; /* Can't be replaced by an equiv large page */
 
 	/*
@@ -55,13 +83,60 @@ struct kvm_mmu_page {
 	gfn_t *gfns;
 	/* Currently serving as active root */
 	union {
+		/*
+		 * 在以下设置kvm_mmu_page->root_count:
+		 *   - arch/x86/kvm/mmu/mmu.c|3358| <<mmu_alloc_root>> ++sp->root_count;
+		 *   - arch/x86/kvm/mmu/mmu.c|3255| <<mmu_free_root_page>> else if (!--sp->root_count && sp->role.invalid)
+		 * 在以下使用kvm_mmu_page->root_count:
+		 *   - arch/x86/kvm/mmu/mmu.c|2364| <<__kvm_mmu_prepare_zap_page>> if (!sp->root_count) {
+		 *   - arch/x86/kvm/mmu/mmu.c|2430| <<kvm_mmu_commit_zap_page>> WARN_ON(!sp->role.invalid || sp->root_count);
+		 *   - arch/x86/kvm/mmu/mmu.c|2453| <<kvm_mmu_zap_oldest_mmu_pages>> if (sp->root_count)
+		 *   - arch/x86/kvm/mmu/mmutrace.h|22| <<KVM_MMU_PAGE_ASSIGN>> __entry->root_count = sp->root_count; \
+		 *   - arch/x86/kvm/mmu/tdp_mmu.h|108| <<is_tdp_mmu>> return sp && is_tdp_mmu_page(sp) && sp->root_count;
+		 */
 		int root_count;
+		/*
+		 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+		 */
 		refcount_t tdp_mmu_root_count;
 	};
+	/*
+	 * 在以下修改kvm_mmu_page->unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|1942| <<mark_unsync>> if (sp->unsync_children++)
+	 *   - arch/x86/kvm/mmu/mmu.c|1981| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 * 在以下使用kvm_mmu_page->unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|1982| <<clear_unsync_child_bit>> WARN_ON((int )sp->unsync_children < 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|2002| <<__mmu_unsync_walk>> if (child->unsync_children) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2031| <<mmu_unsync_walk>> if (!sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/mmu.c|2185| <<mmu_pages_clear_parents>> } while (!sp->unsync_children);
+	 *   - arch/x86/kvm/mmu/mmu.c|2500| <<link_shadow_page>> if (sp->unsync_children || sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|4166| <<is_unsync_root>> if (sp->unsync || sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|704| <<FNAME(fetch)>> if (sp->unsync_children &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|991| <<FNAME(nvlpg)>> if (!sp->unsync_children)
+	 */
 	unsigned int unsync_children;
 	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
+	/*
+	 * 注释:
+	 * Use to track shadow pages that, if zapped, would allow KVM to create
+	 * an NX huge page.  A shadow page will have nx_huge_page_disallowed
+	 * set but not be on the list if a huge page is disallowed for other
+	 * reasons, e.g. because KVM is shadowing a PTE at the same gfn, the
+	 * memslot isn't properly aligned, etc...
+	 */
+	/*
+	 * 在kvm_mmu_page->lpage_disallowed_link:
+	 *   - arch/x86/kvm/mmu/mmu.c|932| <<account_huge_nx_page>> list_add_tail(&sp->lpage_disallowed_link,
+	 *   - arch/x86/kvm/mmu/mmu.c|958| <<unaccount_huge_nx_page>> list_del(&sp->lpage_disallowed_link);
+	 *   - arch/x86/kvm/mmu/mmu.c|6619| <<kvm_recover_nx_lpages>> sp = list_first_entry(&kvm->arch.lpage_disallowed_mmu_pages, struct kvm_mmu_page, lpage_disallowed_link);
+	 */
 	struct list_head lpage_disallowed_link;
 #ifdef CONFIG_X86_32
 	/*
@@ -82,6 +157,31 @@ struct kvm_mmu_page {
 
 extern struct kmem_cache *mmu_page_header_cache;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1836| <<__mmu_unsync_walk>> child = to_shadow_page(ent & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2257| <<validate_direct_spte>> child = to_shadow_page(*sptep & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2278| <<mmu_page_zap_pte>> child = to_shadow_page(pte & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2710| <<mmu_set_spte>> child = to_shadow_page(pte & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|3251| <<mmu_free_root_page>> sp = to_shadow_page(*root_hpa & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|3330| <<kvm_mmu_free_guest_mode_roots>> if (!to_shadow_page(root_hpa) ||
+ *   - arch/x86/kvm/mmu/mmu.c|3331| <<kvm_mmu_free_guest_mode_roots>> to_shadow_page(root_hpa)->role.guest_mode)
+ *   - arch/x86/kvm/mmu/mmu.c|3672| <<is_unsync_root>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3694| <<kvm_mmu_sync_roots>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3717| <<kvm_mmu_sync_roots>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3995| <<is_page_fault_stale>> struct kvm_mmu_page *sp = to_shadow_page(vcpu->arch.mmu->root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4145| <<is_root_usable>> VALID_PAGE(root->hpa) && to_shadow_page(root->hpa) &&
+ *   - arch/x86/kvm/mmu/mmu.c|4146| <<is_root_usable>> role.word == to_shadow_page(root->hpa)->role.word;
+ *   - arch/x86/kvm/mmu/mmu.c|4235| <<__kvm_mmu_new_pgd>> to_shadow_page(vcpu->arch.mmu->root_hpa));
+ *   - arch/x86/kvm/mmu/mmu_audit.c|48| <<__mmu_spte_walk>> child = to_shadow_page(ent[i] & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|65| <<mmu_spte_walk>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|75| <<mmu_spte_walk>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|94| <<sptep_to_sp>> return to_shadow_page(__pa(sptep));
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|107| <<is_tdp_mmu>> sp = to_shadow_page(hpa);
+ *
+ * In some case, special roots are used in mmu.  It is often using
+ * to_shadow_page(mmu->root.hpa) to check if special roots are used.
+ */
 static inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)
 {
 	struct page *page = pfn_to_page(shadow_page >> PAGE_SHIFT);
diff --git a/arch/x86/kvm/mmu/paging_tmpl.h b/arch/x86/kvm/mmu/paging_tmpl.h
index 5b5bdac97c7b..a683aed3f11b 100644
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@ -719,6 +719,12 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3266| <<__direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|722| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1335| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 73cfe62fdad1..5bac27455b44 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -23,10 +23,27 @@ static bool __read_mostly enable_mmio_caching = true;
 module_param_named(mmio_caching, enable_mmio_caching, bool, 0444);
 
 u64 __read_mostly shadow_host_writable_mask;
+/*
+ * 在以下设置shadow_mmu_writable_mask:
+ *   - arch/x86/kvm/mmu/spte.c|353| <<kvm_mmu_set_ept_masks>> shadow_mmu_writable_mask = EPT_SPTE_MMU_WRITABLE;
+ *   - arch/x86/kvm/mmu/spte.c|405| <<kvm_mmu_reset_all_pte_masks>> shadow_mmu_writable_mask = DEFAULT_SPTE_MMU_WRITEABLE;
+ * 在以下使用shadow_mmu_writable_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|1363| <<spte_write_protect>> spte &= ~shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|164| <<make_spte>> spte |= PT_WRITABLE_MASK | shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|186| <<make_spte>> spte &= ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ *   - arch/x86/kvm/mmu/spte.c|248| <<kvm_mmu_changed_pte_notifier_make_spte>> new_spte &= ~shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.h|391| <<spte_can_locklessly_be_made_writable>> if (spte & shadow_mmu_writable_mask) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1904| <<write_protect_gfn>> ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ */
 u64 __read_mostly shadow_mmu_writable_mask;
 u64 __read_mostly shadow_nx_mask;
 u64 __read_mostly shadow_x_mask; /* mutual exclusive with nx_mask */
 u64 __read_mostly shadow_user_mask;
+/*
+ * 在以下设置shadow_accessed_mask:
+ *   - arch/x86/kvm/mmu/spte.c|330| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|382| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+ */
 u64 __read_mostly shadow_accessed_mask;
 u64 __read_mostly shadow_dirty_mask;
 u64 __read_mostly shadow_mmio_value;
@@ -34,6 +51,11 @@ u64 __read_mostly shadow_mmio_mask;
 u64 __read_mostly shadow_mmio_access_mask;
 u64 __read_mostly shadow_present_mask;
 u64 __read_mostly shadow_me_mask;
+/*
+ * 在以下设置shadow_acc_track_mask:
+ *   - arch/x86/kvm/mmu/spte.c|361| <<kvm_mmu_set_ept_masks>> shadow_acc_track_mask = VMX_EPT_RWX_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|413| <<kvm_mmu_reset_all_pte_masks>> shadow_acc_track_mask = 0;
+ */
 u64 __read_mostly shadow_acc_track_mask;
 
 u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
@@ -52,6 +74,11 @@ static u64 generation_mmio_spte_mask(u64 gen)
 	return mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|377| <<mark_mmio_spte>> u64 spte = make_mmio_spte(vcpu, gfn, access);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1240| <<tdp_mmu_map_handle_target_level>> new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
+ */
 u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)
 {
 	u64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;
@@ -90,6 +117,12 @@ static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
 				     E820_TYPE_RAM);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2948| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1101| <<FNAME(sync_page)>> make_spte(vcpu, sp, slot, pte_access, gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1242| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn,
+ */
 bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	       const struct kvm_memory_slot *slot,
 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
@@ -192,8 +225,21 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	return wrprot;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2420| <<link_shadow_page>> spte = make_nonleaf_spte(sp->spt, sp_ad_disabled(sp));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1417| <<kvm_tdp_mmu_map>> new_spte = make_nonleaf_spte(child_pt,
+ */
 u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 {
+	/*
+	 * 在以下使用SPTE_MMU_PRESENT_MASK:
+	 *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+	 *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+	 */
 	u64 spte = SPTE_MMU_PRESENT_MASK;
 
 	spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK |
@@ -270,6 +316,12 @@ u64 mark_spte_for_access_track(u64 spte)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|376| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE,
+ *   - arch/x86/kvm/mmu/spte.c|436| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+ *   - arch/x86/kvm/svm/svm.c|4701| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+ */
 void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 {
 	BUG_ON((u64)(unsigned)access_mask != access_mask);
@@ -305,6 +357,10 @@ void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mmio_spte_mask);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7997| <<hardware_setup>> kvm_mmu_set_ept_masks(enable_ept_ad_bits,
+ */
 void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)
 {
 	shadow_user_mask	= VMX_EPT_READABLE_MASK;
@@ -328,6 +384,10 @@ void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_ept_masks);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6799| <<kvm_mmu_module_init>> kvm_mmu_reset_all_pte_masks();
+ */
 void kvm_mmu_reset_all_pte_masks(void)
 {
 	u8 low_phys_bits;
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index be6a007a4af3..4ecd2e6076eb 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -12,6 +12,14 @@
  * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
  * enough that the improved code generation is noticeable in KVM's footprint.
  */
+/*
+ * 在以下使用SPTE_MMU_PRESENT_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+ *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+ */
 #define SPTE_MMU_PRESENT_MASK		BIT_ULL(11)
 
 /*
@@ -53,11 +61,24 @@ static_assert(SPTE_TDP_AD_ENABLED_MASK == 0);
 
 #define PT64_LEVEL_BITS 9
 
+/*
+ * PT64_LEVEL_SHIFT(1) = 12 + 0 * 9 = 12
+ * PT64_LEVEL_SHIFT(2) = 12 + 1 * 9 = 21
+ * PT64_LEVEL_SHIFT(3) = 12 + 2 * 9 = 30
+ * PT64_LEVEL_SHIFT(4) = 12 + 3 * 9 = 39
+ * PT64_LEVEL_SHIFT(5) = 12 + 4 * 9 = 48
+ */
 #define PT64_LEVEL_SHIFT(level) \
 		(PAGE_SHIFT + (level - 1) * PT64_LEVEL_BITS)
 
 #define PT64_INDEX(address, level)\
 	(((address) >> PT64_LEVEL_SHIFT(level)) & ((1 << PT64_LEVEL_BITS) - 1))
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2211| <<shadow_walk_okay>> iterator->index = SHADOW_PT_INDEX(iterator->addr, iterator->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|26| <<tdp_iter_refresh_sptep>> SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|181| <<try_step_side>> if (SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level) ==
+ */
 #define SHADOW_PT_INDEX(addr, level) PT64_INDEX(addr, level)
 
 /*
@@ -201,11 +222,29 @@ extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
  *
  * Only used by the TDP MMU.
  */
+/*
+ * 在以下使用REMOVED_SPTE:
+ *   - arch/x86/kvm/mmu/spte.h|215| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|299| <<kvm_mmu_set_mmio_spte_mask>> WARN_ON(mmio_value && (REMOVED_SPTE & mmio_mask) == mmio_value))
+ *   - arch/x86/kvm/mmu/spte.h|219| <<is_removed_spte>> return spte == REMOVED_SPTE;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|453| <<handle_removed_tdp_mmu_page>> old_child_spte = xchg(sptep, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|480| <<handle_removed_tdp_mmu_page>> WRITE_ONCE(*sptep, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|483| <<handle_removed_tdp_mmu_page>> old_child_spte, REMOVED_SPTE, level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|668| <<tdp_mmu_zap_spte_atomic>> if (!tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE))
+ */
 #define REMOVED_SPTE	0x5a0ULL
 
 /* Removed SPTEs must not be misconstrued as shadow present PTEs. */
 static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|484| <<handle_removed_tdp_mmu_page>> if (!is_removed_spte(old_child_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|607| <<__handle_changed_spte>> !is_removed_spte(new_spte)))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|686| <<tdp_mmu_set_spte_atomic>> if (is_removed_spte(iter->old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|798| <<__tdp_mmu_set_spte>> WARN_ON(is_removed_spte(iter->old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1417| <<kvm_tdp_mmu_map>> if (is_removed_spte(iter.old_spte))
+ */
 static inline bool is_removed_spte(u64 spte)
 {
 	return spte == REMOVED_SPTE;
@@ -235,6 +274,20 @@ static inline bool is_mmio_spte(u64 spte)
 
 static inline bool is_shadow_present_pte(u64 pte)
 {
+	/*
+	 * A MMU present SPTE is backed by actual memory and may or may not be present
+	 * in hardware.  E.g. MMIO SPTEs are not considered present.  Use bit 11, as it
+	 * is ignored by all flavors of SPTEs and checking a low bit often generates
+	 * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
+	 * enough that the improved code generation is noticeable in KVM's footprint.
+	 *
+	 * 在以下使用SPTE_MMU_PRESENT_MASK:
+	 *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+	 *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+	 */
 	return !!(pte & SPTE_MMU_PRESENT_MASK);
 }
 
@@ -272,6 +325,14 @@ static inline u64 spte_shadow_dirty_mask(u64 spte)
 	return spte_ad_enabled(spte) ? shadow_dirty_mask : 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|581| <<spte_has_volatile_bits>> is_access_track_spte(spte))
+ *   - arch/x86/kvm/mmu/mmu.c|739| <<restore_acc_track_spte>> WARN_ON_ONCE(!is_access_track_spte(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|3617| <<fast_page_fault>> if (is_access_track_spte(spte))
+ *   - arch/x86/kvm/mmu/spte.c|291| <<mark_spte_for_access_track>> if (is_access_track_spte(spte))
+ *   - arch/x86/kvm/mmu/spte.h|358| <<is_accessed_spte>> : !is_access_track_spte(spte);
+ */
 static inline bool is_access_track_spte(u64 spte)
 {
 	return !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;
diff --git a/arch/x86/kvm/mmu/tdp_iter.c b/arch/x86/kvm/mmu/tdp_iter.c
index caa96c270b95..a588b75c200b 100644
--- a/arch/x86/kvm/mmu/tdp_iter.c
+++ b/arch/x86/kvm/mmu/tdp_iter.c
@@ -8,15 +8,38 @@
  * Recalculates the pointer to the SPTE for the current GFN and level and
  * reread the SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|34| <<tdp_iter_restart>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|99| <<try_step_down>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|141| <<try_step_up>> tdp_iter_refresh_sptep(iter);
+ */
 static void tdp_iter_refresh_sptep(struct tdp_iter *iter)
 {
+	/*
+	 * tdp_iter->sptep:
+	 *    A pointer to the current SPTE
+	 * tdp_iter->pt_path[PT64_ROOT_MAX_LEVEL]:
+	 *    Pointers to the page tables traversed to reach the current SPTE
+	 */
 	iter->sptep = iter->pt_path[iter->level - 1] +
 		SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|33| <<tdp_iter_restart>> iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|98| <<try_step_down>> iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|140| <<try_step_up>> iter->gfn = round_gfn_for_level(iter->gfn, iter->level);
+ */
 static gfn_t round_gfn_for_level(gfn_t gfn, int level)
 {
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) = KVM_HPAGE_SIZE(1) / 4096 = 4K / 4K = 1
+	 * KVM_PAGES_PER_HPAGE(2) = KVM_HPAGE_SIZE(2) / 4096 = 2M / 4K = 512
+	 * KVM_PAGES_PER_HPAGE(3) = KVM_HPAGE_SIZE(3) / 4096 = 1G / 4K = 262144
+	 */
 	return gfn & -KVM_PAGES_PER_HPAGE(level);
 }
 
@@ -24,12 +47,30 @@ static gfn_t round_gfn_for_level(gfn_t gfn, int level)
  * Return the TDP iterator to the root PT and allow it to continue its
  * traversal over the paging structure from there.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|55| <<tdp_iter_start>> tdp_iter_restart(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|165| <<tdp_iter_next>> tdp_iter_restart(iter);
+ */
 void tdp_iter_restart(struct tdp_iter *iter)
 {
 	iter->yielded = false;
+	/*
+	 * 在以下使用tdp_iter->yielded_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|661| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 */
 	iter->yielded_gfn = iter->next_last_level_gfn;
 	iter->level = iter->root_level;
 
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 *
+	 * tdp_iter->gfn:
+	 *    The lowest GFN mapped by the current SPTE
+	 */
 	iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
 	tdp_iter_refresh_sptep(iter);
 
@@ -40,6 +81,10 @@ void tdp_iter_restart(struct tdp_iter *iter)
  * Sets a TDP iterator to walk a pre-order traversal of the paging structure
  * rooted at root_pt, starting with the walk to translate next_last_level_gfn.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|61| <<for_each_tdp_pte_min_level>> for (tdp_iter_start(&iter, root, root_level, min_level, start); \
+ */
 void tdp_iter_start(struct tdp_iter *iter, u64 *root_pt, int root_level,
 		    int min_level, gfn_t next_last_level_gfn)
 {
@@ -47,8 +92,17 @@ void tdp_iter_start(struct tdp_iter *iter, u64 *root_pt, int root_level,
 	WARN_ON(root_level > PT64_ROOT_MAX_LEVEL);
 
 	iter->next_last_level_gfn = next_last_level_gfn;
+	/*
+	 * The level of the root page given to the iterator
+	 */
 	iter->root_level = root_level;
+	/*
+	 * The lowest level the iterator should traverse to
+	 */
 	iter->min_level = min_level;
+	/*
+	 * root_pt是虚拟地址(va)
+	 */
 	iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root_pt;
 	iter->as_id = kvm_mmu_page_as_id(sptep_to_sp(root_pt));
 
@@ -76,6 +130,10 @@ tdp_ptep_t spte_to_child_pt(u64 spte, int level)
  * Steps down one level in the paging structure towards the goal GFN. Returns
  * true if the iterator was able to step down a level, false otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|220| <<tdp_iter_next>> if (try_step_down(iter))
+ */
 static bool try_step_down(struct tdp_iter *iter)
 {
 	tdp_ptep_t child_pt;
@@ -87,14 +145,28 @@ static bool try_step_down(struct tdp_iter *iter)
 	 * Reread the SPTE before stepping down to avoid traversing into page
 	 * tables that are no longer linked from this entry.
 	 */
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 
+	/*
+	 * 这里得到的是当前pte所指向的下一个page table的page
+	 */
 	child_pt = spte_to_child_pt(iter->old_spte, iter->level);
 	if (!child_pt)
 		return false;
 
 	iter->level--;
 	iter->pt_path[iter->level - 1] = child_pt;
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
 	tdp_iter_refresh_sptep(iter);
 
@@ -118,9 +190,27 @@ static bool try_step_side(struct tdp_iter *iter)
             (PT64_ENT_PER_PAGE - 1))
 		return false;
 
+	/*
+	 * KVM_HPAGE_SIZE(1) : 1 << 12 = 4K
+	 * KVM_HPAGE_SIZE(2) : 1 << 21 = 2M
+	 * KVM_HPAGE_SIZE(3) : 1 << 30 = 1G
+	 * KVM_HPAGE_SIZE(4) : 1 << 38 = 256G
+	 * KVM_HPAGE_SIZE(5) : 1 << 48 = 262144G
+	 */
 	iter->gfn += KVM_PAGES_PER_HPAGE(iter->level);
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	iter->next_last_level_gfn = iter->gfn;
 	iter->sptep++;
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 
 	return true;
@@ -131,6 +221,10 @@ static bool try_step_side(struct tdp_iter *iter)
  * can continue from the next entry in the parent page table. Returns true on a
  * successful step up, false if already in the root page.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|272| <<tdp_iter_next>> } while (try_step_up(iter));
+ */
 static bool try_step_up(struct tdp_iter *iter)
 {
 	if (iter->level == iter->root_level)
@@ -159,8 +253,23 @@ static bool try_step_up(struct tdp_iter *iter)
  *    SPTE will have already been visited, and so the iterator must also step
  *    to the side again.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|63| <<for_each_tdp_pte_min_level>> tdp_iter_next(&iter))
+ */
 void tdp_iter_next(struct tdp_iter *iter)
 {
+	/*
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 * 在以下使用tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+	 */
 	if (iter->yielded) {
 		tdp_iter_restart(iter);
 		return;
diff --git a/arch/x86/kvm/mmu/tdp_iter.h b/arch/x86/kvm/mmu/tdp_iter.h
index e19cabbcb65c..4aa6800c2650 100644
--- a/arch/x86/kvm/mmu/tdp_iter.h
+++ b/arch/x86/kvm/mmu/tdp_iter.h
@@ -17,12 +17,22 @@ struct tdp_iter {
 	 * The iterator will traverse the paging structure towards the mapping
 	 * for this GFN.
 	 */
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	gfn_t next_last_level_gfn;
 	/*
 	 * The next_last_level_gfn at the time when the thread last
 	 * yielded. Only yielding when the next_last_level_gfn !=
 	 * yielded_gfn helps ensure forward progress.
 	 */
+	/*
+	 * 在以下使用tdp_iter->yielded_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|661| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 */
 	gfn_t yielded_gfn;
 	/* Pointers to the page tables traversed to reach the current SPTE */
 	tdp_ptep_t pt_path[PT64_ROOT_MAX_LEVEL];
@@ -39,6 +49,12 @@ struct tdp_iter {
 	/* The address space ID, i.e. SMM vs. regular. */
 	int as_id;
 	/* A snapshot of the value at sptep */
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	u64 old_spte;
 	/*
 	 * Whether the iterator has a valid state. This will be false if the
@@ -50,6 +66,17 @@ struct tdp_iter {
 	 * which case tdp_iter_next() needs to restart the walk at the root
 	 * level instead of advancing to the next entry.
 	 */
+	/*
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 * 在以下使用tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+	 */
 	bool yielded;
 };
 
@@ -57,11 +84,23 @@ struct tdp_iter {
  * Iterates over every SPTE mapping the GFN range [start, end) in a
  * preorder traversal.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|66| <<for_each_tdp_pte>> for_each_tdp_pte_min_level(iter, root, root_level, PG_LEVEL_4K, start, end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|727| <<zap_gfn_range>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1183| <<wrprot_gfn_range>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1439| <<write_protect_gfn>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ */
 #define for_each_tdp_pte_min_level(iter, root, root_level, min_level, start, end) \
 	for (tdp_iter_start(&iter, root, root_level, min_level, start); \
 	     iter.valid && iter.gfn < end;		     \
 	     tdp_iter_next(&iter))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|627| <<tdp_root_for_each_pte>> for_each_tdp_pte(_iter, _root->spt, _root->role.level, _start, _end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|637| <<tdp_mmu_for_each_pte>> for_each_tdp_pte(_iter, __va(_mmu->root_hpa), \
+ */
 #define for_each_tdp_pte(iter, root, root_level, start, end) \
 	for_each_tdp_pte_min_level(iter, root, root_level, PG_LEVEL_4K, start, end)
 
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index bc9e3553fba2..38c41deb651d 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -10,6 +10,12 @@
 #include <asm/cmpxchg.h>
 #include <trace/events/kvm.h>
 
+/*
+ * 在以下使用tdp_mmu_enabled:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|13| <<global>> static bool __read_mostly tdp_mmu_enabled = true;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|14| <<global>> module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0644);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|19| <<kvm_mmu_init_tdp_mmu>> if (!tdp_enabled || !READ_ONCE(tdp_mmu_enabled))
+ */
 static bool __read_mostly tdp_mmu_enabled = true;
 module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0644);
 
@@ -29,15 +35,32 @@ bool kvm_mmu_init_tdp_mmu(struct kvm *kvm)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|85| <<kvm_tdp_mmu_put_root>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|723| <<zap_gfn_range>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ */
 static __always_inline void kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
 							     bool shared)
 {
+	/*
+	 * struct kvm *kvm:
+	 *     #ifdef KVM_HAVE_MMU_RWLOCK
+	 *     rwlock_t mmu_lock;
+	 *     #else
+	 *     spinlock_t mmu_lock;
+	 *     #endif
+	 */
 	if (shared)
 		lockdep_assert_held_read(&kvm->mmu_lock);
 	else
 		lockdep_assert_held_write(&kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5794| <<kvm_mmu_uninit_vm>> kvm_mmu_uninit_tdp_mmu(kvm);
+ */
 void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 {
 	if (!kvm->arch.tdp_mmu_enabled)
@@ -50,6 +73,9 @@ void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 	 * Ensure that all the outstanding RCU callbacks to free shadow pages
 	 * can run before the VM is torn down.
 	 */
+	/*
+	 * 注释: Wait until all in-flight call_rcu() callbacks complete.
+	 */
 	rcu_barrier();
 }
 
@@ -57,6 +83,11 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			  gfn_t start, gfn_t end, bool can_yield, bool flush,
 			  bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|84| <<tdp_mmu_free_sp_rcu_callback>> tdp_mmu_free_sp(sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1080| <<kvm_tdp_mmu_map>> tdp_mmu_free_sp(sp);
+ */
 static void tdp_mmu_free_sp(struct kvm_mmu_page *sp)
 {
 	free_page((unsigned long)sp->spt);
@@ -71,6 +102,11 @@ static void tdp_mmu_free_sp(struct kvm_mmu_page *sp)
  * section, and freeing it after a grace period, lockless access to that
  * memory won't use it after it is freed.
  */
+/*
+ * 在以下使用tdp_mmu_free_sp_rcu_callback():
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|109| <<kvm_tdp_mmu_put_root>> call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|417| <<handle_removed_tdp_mmu_page>> call_rcu(&sp->rcu_head, tdp_mmu_free_sp_rcu_callback);
+ */
 static void tdp_mmu_free_sp_rcu_callback(struct rcu_head *head)
 {
 	struct kvm_mmu_page *sp = container_of(head, struct kvm_mmu_page,
@@ -79,6 +115,12 @@ static void tdp_mmu_free_sp_rcu_callback(struct rcu_head *head)
 	tdp_mmu_free_sp(sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3254| <<mmu_free_root_page>> kvm_tdp_mmu_put_root(kvm, sp, false);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|136| <<tdp_mmu_next_root>> kvm_tdp_mmu_put_root(kvm, prev_root, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|860| <<kvm_tdp_mmu_zap_invalidated_roots>> kvm_tdp_mmu_put_root(kvm, root, true);
+ */
 void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 			  bool shared)
 {
@@ -93,6 +135,23 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 	list_del_rcu(&root->link);
 	spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
 
+	/*
+	 * Tears down the mappings for the range of gfns, [start, end), and frees the
+	 * non-root pages mapping GFNs strictly within that range. Returns true if
+	 * SPTEs have been cleared and a TLB flush is needed before releasing the
+	 * MMU lock.
+	 *
+	 * If can_yield is true, will release the MMU lock and reschedule if the
+	 * scheduler needs the CPU or there is contention on the MMU lock. If this
+	 * function cannot yield, it will not release the MMU lock or reschedule and
+	 * the caller must ensure it does not supply too large a GFN range, or the
+	 * operation can cause a soft lockup.
+	 *
+	 * If shared is true, this thread holds the MMU lock in read mode and must
+	 * account for the possibility that other threads are modifying the paging
+	 * structures concurrently. If shared is false, this thread should hold the
+	 * MMU lock in write mode.
+	 */
 	zap_gfn_range(kvm, root, 0, -1ull, false, false, shared);
 
 	call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
@@ -105,6 +164,11 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
  * that reference will be dropped. If no valid root is found, this
  * function will return NULL.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|152| <<for_each_tdp_mmu_root_yield_safe>> for (_root = tdp_mmu_next_root(_kvm, NULL, _shared); \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|154| <<for_each_tdp_mmu_root_yield_safe>> _root = tdp_mmu_next_root(_kvm, _root, _shared)) \
+ */
 static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 					      struct kvm_mmu_page *prev_root,
 					      bool shared)
@@ -113,6 +177,10 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 
 	rcu_read_lock();
 
+	/*
+	 * 关于list_next_or_null_rcu()注意:
+	 * Note that if the ptr is at the end of the list, NULL is returned.
+	 */
 	if (prev_root)
 		next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
 						  &prev_root->link,
@@ -143,6 +211,18 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
  * mode. In the unlikely event that this thread must free a root, the lock
  * will be temporarily dropped and reacquired in write mode.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|788| <<__kvm_tdp_mmu_zap_gfn_range>> for_each_tdp_mmu_root_yield_safe(kvm, root, as_id, false)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1047| <<kvm_tdp_mmu_unmap_gfn_range>> for_each_tdp_mmu_root_yield_safe(kvm, root, range->slot->as_id, false)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1234| <<kvm_tdp_mmu_wrprot_slot>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1304| <<kvm_tdp_mmu_clear_dirty_slot>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1429| <<kvm_tdp_mmu_zap_collapsible_sptes>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *
+ * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+ * 前者不需要锁, 后者需要锁
+ * 主要是用第二个参数_root "struct kvm_mmu_page *"
+ */
 #define for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared)	\
 	for (_root = tdp_mmu_next_root(_kvm, NULL, _shared);		\
 	     _root;							\
@@ -150,6 +230,17 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 		if (kvm_mmu_page_as_id(_root) != _as_id) {		\
 		} else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|209| <<kvm_tdp_mmu_get_vcpu_root_hpa>> for_each_tdp_mmu_root(kvm, root, kvm_mmu_role_as_id(role)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1071| <<kvm_tdp_mmu_handle_gfn>> for_each_tdp_mmu_root(kvm, root, range->slot->as_id) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1370| <<kvm_tdp_mmu_clear_dirty_pt_masked>> for_each_tdp_mmu_root(kvm, root, slot->as_id)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1483| <<kvm_tdp_mmu_write_protect_gfn>> for_each_tdp_mmu_root(kvm, root, slot->as_id)
+ *
+ * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+ * 前者不需要锁, 后者需要锁
+ * 主要是用第二个参数_root "struct kvm_mmu_page *"
+ */
 #define for_each_tdp_mmu_root(_kvm, _root, _as_id)				\
 	list_for_each_entry_rcu(_root, &_kvm->arch.tdp_mmu_roots, link,		\
 				lockdep_is_held_type(&kvm->mmu_lock, 0) ||	\
@@ -157,6 +248,11 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 		if (kvm_mmu_page_as_id(_root) != _as_id) {		\
 		} else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|275| <<alloc_tdp_mmu_page>> sp->role.word = page_role_for_level(vcpu, level).word;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|300| <<kvm_tdp_mmu_get_vcpu_root_hpa>> role = page_role_for_level(vcpu, vcpu->arch.mmu->shadow_root_level);
+ */
 static union kvm_mmu_page_role page_role_for_level(struct kvm_vcpu *vcpu,
 						   int level)
 {
@@ -172,6 +268,11 @@ static union kvm_mmu_page_role page_role_for_level(struct kvm_vcpu *vcpu,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|313| <<kvm_tdp_mmu_get_vcpu_root_hpa>> root = alloc_tdp_mmu_page(vcpu, 0, vcpu->arch.mmu->shadow_root_level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1299| <<kvm_tdp_mmu_map>> sp = alloc_tdp_mmu_page(vcpu, iter.gfn, iter.level - 1);
+ */
 static struct kvm_mmu_page *alloc_tdp_mmu_page(struct kvm_vcpu *vcpu, gfn_t gfn,
 					       int level)
 {
@@ -190,6 +291,14 @@ static struct kvm_mmu_page *alloc_tdp_mmu_page(struct kvm_vcpu *vcpu, gfn_t gfn,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3377| <<mmu_alloc_direct_roots>> root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);
+ *
+ * kvm_mmu_load()
+ * -> mmu_alloc_direct_roots()
+ *    -> kvm_tdp_mmu_get_vcpu_root_hpa()
+ */
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 {
 	union kvm_mmu_page_role role;
@@ -201,6 +310,10 @@ hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 	role = page_role_for_level(vcpu, vcpu->arch.mmu->shadow_root_level);
 
 	/* Check for an existing root before allocating a new one. */
+	/*
+	 * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+	 * 前者不需要锁, 后者需要锁
+	 */
 	for_each_tdp_mmu_root(kvm, root, kvm_mmu_role_as_id(role)) {
 		if (root->role.word == role.word &&
 		    kvm_tdp_mmu_get_root(kvm, root))
@@ -222,6 +335,12 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|562| <<handle_changed_spte>> handle_changed_spte_acc_track(old_spte, new_spte, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|603| <<tdp_mmu_set_spte_atomic>> handle_changed_spte_acc_track(iter->old_spte, new_spte, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|681| <<__tdp_mmu_set_spte>> handle_changed_spte_acc_track(iter->old_spte, new_spte,
+ */
 static void handle_changed_spte_acc_track(u64 old_spte, u64 new_spte, int level)
 {
 	if (!is_shadow_present_pte(old_spte) || !is_last_spte(old_spte, level))
@@ -259,6 +378,10 @@ static void handle_changed_spte_dirty_log(struct kvm *kvm, int as_id, gfn_t gfn,
  * @account_nx: This page replaces a NX large page and should be marked for
  *		eventual reclaim.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<kvm_tdp_mmu_map>> tdp_mmu_link_page(vcpu->kvm, sp,
+ */
 static void tdp_mmu_link_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 			      bool account_nx)
 {
@@ -278,6 +401,10 @@ static void tdp_mmu_link_page(struct kvm *kvm, struct kvm_mmu_page *sp,
  *	    the MMU lock and the operation must synchronize with other
  *	    threads that might be adding or removing pages.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|401| <<handle_removed_tdp_mmu_page>> tdp_mmu_unlink_page(kvm, sp, shared);
+ */
 static void tdp_mmu_unlink_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				bool shared)
 {
@@ -311,6 +438,14 @@ static void tdp_mmu_unlink_page(struct kvm *kvm, struct kvm_mmu_page *sp,
  * this thread will be responsible for ensuring the page is freed. Hence the
  * early rcu_dereferences in the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|552| <<__handle_changed_spte>> handle_removed_tdp_mmu_page(kvm, spte_to_child_pt(old_spte, level), shared);
+ *
+ * 核心思想是把一个sp->pt上的每一个entry标记为REMOVED_SPTE, 然后把sp给free
+ * Given a page table that has been removed from the TDP paging structure,
+ * iterates through the page table to clear SPTEs and free child page tables.
+ */
 static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
 					bool shared)
 {
@@ -321,8 +456,15 @@ static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
 
 	trace_kvm_mmu_prepare_zap_page(sp);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|401| <<handle_removed_tdp_mmu_page>> tdp_mmu_unlink_page(kvm, sp, shared);
+	 */
 	tdp_mmu_unlink_page(kvm, sp, shared);
 
+	/*
+	 * 1 << 9 = 512
+	 */
 	for (i = 0; i < PT64_ENT_PER_PAGE; i++) {
 		u64 *sptep = rcu_dereference(pt) + i;
 		gfn_t gfn = base_gfn + i * KVM_PAGES_PER_HPAGE(level);
@@ -393,6 +535,19 @@ static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
  * Handle bookkeeping that might result from the modification of a SPTE.
  * This function must be called for all TDP SPTE modifications.
  */
+/*
+ * 如果想要触发handle_removed_tdp_mmu_page()要满足这些条件:
+ * was_present && !was_leaf && (pfn_changed || !is_present)
+ *
+ * - was_present
+ * - !was_leaf
+ * - (pfn_changed || !is_present)
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<handle_changed_spte>> __handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|601| <<tdp_mmu_set_spte_atomic>> __handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|678| <<__tdp_mmu_set_spte>> __handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+ */
 static void __handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				  u64 old_spte, u64 new_spte, int level,
 				  bool shared)
@@ -471,11 +626,21 @@ static void __handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 	 * Recursively handle child PTs if the change removed a subtree from
 	 * the paging structure.
 	 */
+	/*
+	 * handle_removed_tdp_mmu_page()
+	 * 核心思想是把一个sp->pt上的每一个entry标记为REMOVED_SPTE, 然后把sp给free
+	 * Given a page table that has been removed from the TDP paging structure,
+	 * iterates through the page table to clear SPTEs and free child page tables.
+	 */
 	if (was_present && !was_leaf && (pfn_changed || !is_present))
 		handle_removed_tdp_mmu_page(kvm,
 				spte_to_child_pt(old_spte, level), shared);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|447| <<handle_removed_tdp_mmu_page>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_child_spte, REMOVED_SPTE, level, shared);
+ */
 static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared)
@@ -498,6 +663,14 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
  * Returns: true if the SPTE was set, false if it was not. If false is returned,
  *	    this function will have no side-effects.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|668| <<tdp_mmu_zap_spte_atomic>> if (!tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1156| <<tdp_mmu_map_handle_target_level>> else if (!tdp_mmu_set_spte_atomic(vcpu->kvm, iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1256| <<kvm_tdp_mmu_map>> if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter, new_spte)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1444| <<wrprot_gfn_range>> if (!tdp_mmu_set_spte_atomic(kvm, &iter, new_spte)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1512| <<clear_dirty_gfn_range>> if (!tdp_mmu_set_spte_atomic(kvm, &iter, new_spte)) {
+ */
 static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 					   struct tdp_iter *iter,
 					   u64 new_spte)
@@ -517,10 +690,20 @@ static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	 * Note, fast_pf_fix_direct_spte() can also modify TDP MMU SPTEs and
 	 * does not hold the mmu_lock.
 	 */
+	/*
+	 * cmpxchg(void* ptr, int old, int new), 如果ptr和old的值一样,则把new写到ptr内存,
+	 * 否则返回ptr的值. 整个操作是原子的.
+	 * 在Intel平台下,会用lock cmpxchg来实现,这里的lock个人理解是锁住内存总线,
+	 * 这样如果有另一个线程想访问ptr的内存,就会被block住.
+	 */
 	if (cmpxchg64(rcu_dereference(iter->sptep), iter->old_spte,
 		      new_spte) != iter->old_spte)
 		return false;
 
+	/*
+	 * Handle bookkeeping that might result from the modification of a SPTE.
+	 * This function must be called for all TDP SPTE modifications.
+	 */
 	__handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			      new_spte, iter->level, true);
 	handle_changed_spte_acc_track(iter->old_spte, new_spte, iter->level);
@@ -528,6 +711,12 @@ static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|810| <<zap_gfn_range>> } else if (!tdp_mmu_zap_spte_atomic(kvm, &iter)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1044| <<kvm_tdp_mmu_map>> if (!tdp_mmu_zap_spte_atomic(vcpu->kvm, &iter))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1460| <<zap_collapsible_spte_range>> if (!tdp_mmu_zap_spte_atomic(kvm, &iter)) {
+ */
 static inline bool tdp_mmu_zap_spte_atomic(struct kvm *kvm,
 					   struct tdp_iter *iter)
 {
@@ -573,10 +762,25 @@ static inline bool tdp_mmu_zap_spte_atomic(struct kvm *kvm,
  *		      Leaving record_dirty_log unset in that case prevents page
  *		      writes from being double counted.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|822| <<tdp_mmu_set_spte>> __tdp_mmu_set_spte(kvm, iter, new_spte, true, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|833| <<tdp_mmu_set_spte_no_acc_track>> __tdp_mmu_set_spte(kvm, iter, new_spte, false, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|844| <<tdp_mmu_set_spte_no_dirty_log>> __tdp_mmu_set_spte(kvm, iter, new_spte, true, false);
+ */
 static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 				      u64 new_spte, bool record_acc_track,
 				      bool record_dirty_log)
 {
+	/*
+	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+	 * which case tdp_iter_next() needs to restart the walk at the root
+	 * level instead of advancing to the next entry.
+	 *
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 */
 	WARN_ON_ONCE(iter->yielded);
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
@@ -588,10 +792,18 @@ static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 	 * should be used. If operating under the MMU lock in write mode, the
 	 * use of the removed SPTE should not be necessary.
 	 */
+	/*
+	 * return spte == REMOVED_SPTE
+	 */
 	WARN_ON(is_removed_spte(iter->old_spte));
 
 	WRITE_ONCE(*rcu_dereference(iter->sptep), new_spte);
 
+	/*
+	 * 应该就是记录stat吧
+	 * Handle bookkeeping that might result from the modification of a SPTE.
+	 * This function must be called for all TDP SPTE modifications.
+	 */
 	__handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			      new_spte, iter->level, false);
 	if (record_acc_track)
@@ -603,12 +815,23 @@ static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 					      iter->level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|852| <<zap_gfn_range>> tdp_mmu_set_spte(kvm, &iter, 0);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<set_spte_gfn>> tdp_mmu_set_spte(kvm, iter, 0);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1258| <<set_spte_gfn>> tdp_mmu_set_spte(kvm, iter, new_spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1565| <<write_protect_gfn>> tdp_mmu_set_spte(kvm, &iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 				    u64 new_spte)
 {
 	__tdp_mmu_set_spte(kvm, iter, new_spte, true, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1282| <<age_gfn_range>> tdp_mmu_set_spte_no_acc_track(kvm, iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte_no_acc_track(struct kvm *kvm,
 						 struct tdp_iter *iter,
 						 u64 new_spte)
@@ -616,6 +839,10 @@ static inline void tdp_mmu_set_spte_no_acc_track(struct kvm *kvm,
 	__tdp_mmu_set_spte(kvm, iter, new_spte, false, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1522| <<clear_dirty_pt_masked>> tdp_mmu_set_spte_no_dirty_log(kvm, &iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 						 struct tdp_iter *iter,
 						 u64 new_spte)
@@ -623,6 +850,10 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 	__tdp_mmu_set_spte(kvm, iter, new_spte, true, false);
 }
 
+/*
+ * Iterates over every SPTE mapping the GFN range [start, end) in a
+ * preorder traversal.
+ */
 #define tdp_root_for_each_pte(_iter, _root, _start, _end) \
 	for_each_tdp_pte(_iter, _root->spt, _root->role.level, _start, _end)
 
@@ -633,6 +864,12 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 			continue;					\
 		else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1213| <<kvm_tdp_mmu_map>> tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1777| <<kvm_tdp_mmu_get_walk>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1812| <<kvm_tdp_mmu_fast_pf_get_last_sptep>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ */
 #define tdp_mmu_for_each_pte(_iter, _mmu, _start, _end)		\
 	for_each_tdp_pte(_iter, __va(_mmu->root_hpa),		\
 			 _mmu->shadow_root_level, _start, _end)
@@ -651,10 +888,26 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
  *
  * Returns true if this function yielded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|736| <<zap_gfn_range>> tdp_mmu_iter_cond_resched(kvm, &iter, flush, shared)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1196| <<wrprot_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1259| <<clear_dirty_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1391| <<zap_collapsible_spte_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ */
 static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 							  struct tdp_iter *iter,
 							  bool flush, bool shared)
 {
+	/*
+	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+	 * which case tdp_iter_next() needs to restart the walk at the root
+	 * level instead of advancing to the next entry.
+	 *
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 */
 	WARN_ON(iter->yielded);
 
 	/* Ensure forward progress has been made before yielding. */
@@ -676,6 +929,17 @@ static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 
 		WARN_ON(iter->gfn > iter->next_last_level_gfn);
 
+		/*
+		 * 在以下设置tdp_iter->yielded:
+		 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+		 * 在以下使用tdp_iter->yielded:
+		 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+		 */
 		iter->yielded = true;
 	}
 
@@ -699,6 +963,13 @@ static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
  * structures concurrently. If shared is false, this thread should hold the
  * MMU lock in write mode.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|145| <<kvm_tdp_mmu_put_root>> zap_gfn_range(kvm, root, 0, -1ull, false, false, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|954| <<__kvm_tdp_mmu_zap_gfn_range>> flush = zap_gfn_range(kvm, root, start, end, can_yield, flush,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1019| <<kvm_tdp_mmu_zap_invalidated_roots>> flush = zap_gfn_range(kvm, root, 0, -1ull, true, flush, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1221| <<kvm_tdp_mmu_unmap_gfn_range>> flush = zap_gfn_range(kvm, root, range->start, range->end,
+ */
 static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			  gfn_t start, gfn_t end, bool can_yield, bool flush,
 			  bool shared)
@@ -720,10 +991,17 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 	 */
 	end = min(end, max_gfn_host);
 
+	/*
+	 * kvm->mmu_lock
+	 */
 	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
 
 	rcu_read_lock();
 
+	/*
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 */
 	for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
 				   min_level, start, end) {
 retry:
@@ -770,11 +1048,34 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
  * SPTEs have been cleared and a TLB flush is needed before releasing the
  * MMU lock.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|27| <<kvm_tdp_mmu_zap_gfn_range>> return __kvm_tdp_mmu_zap_gfn_range(kvm, as_id, start, end, true, flush);
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|43| <<kvm_tdp_mmu_zap_sp>> return __kvm_tdp_mmu_zap_gfn_range(kvm, kvm_mmu_page_as_id(sp),
+ */
 bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 				 gfn_t end, bool can_yield, bool flush)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * zap_gfn_range()注释:
+	 * Tears down the mappings for the range of gfns, [start, end), and frees the
+	 * non-root pages mapping GFNs strictly within that range. Returns true if
+	 * SPTEs have been cleared and a TLB flush is needed before releasing the
+	 * MMU lock.
+	 *
+	 * If can_yield is true, will release the MMU lock and reschedule if the
+	 * scheduler needs the CPU or there is contention on the MMU lock. If this
+	 * function cannot yield, it will not release the MMU lock or reschedule and
+	 * the caller must ensure it does not supply too large a GFN range, or the
+	 * operation can cause a soft lockup.
+	 *
+	 * If shared is true, this thread holds the MMU lock in read mode and must
+	 * account for the possibility that other threads are modifying the paging
+	 * structures concurrently. If shared is false, this thread should hold the
+	 * MMU lock in write mode.
+	 */
 	for_each_tdp_mmu_root_yield_safe(kvm, root, as_id, false)
 		flush = zap_gfn_range(kvm, root, start, end, can_yield, flush,
 				      false);
@@ -782,6 +1083,10 @@ bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 	return flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6028| <<kvm_mmu_zap_all>> kvm_tdp_mmu_zap_all(kvm);
+ */
 void kvm_tdp_mmu_zap_all(struct kvm *kvm)
 {
 	bool flush = false;
@@ -794,6 +1099,11 @@ void kvm_tdp_mmu_zap_all(struct kvm *kvm)
 		kvm_flush_remote_tlbs(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1047| <<kvm_tdp_mmu_zap_invalidated_roots>> root = next_invalidated_root(kvm, NULL);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1050| <<kvm_tdp_mmu_zap_invalidated_roots>> next_root = next_invalidated_root(kvm, root);
+ */
 static struct kvm_mmu_page *next_invalidated_root(struct kvm *kvm,
 						  struct kvm_mmu_page *prev_root)
 {
@@ -824,6 +1134,10 @@ static struct kvm_mmu_page *next_invalidated_root(struct kvm *kvm,
  * only has to do a trivial amount of work. Since the roots are invalid,
  * no new SPTEs should be created under them.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5742| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_zap_invalidated_roots(kvm);
+ */
 void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
 {
 	struct kvm_mmu_page *next_root;
@@ -880,10 +1194,27 @@ void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
  * This has essentially the same effect for the TDP MMU
  * as updating mmu_valid_gen does for the shadow MMU.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5724| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_invalidate_all_roots(kvm);
+ */
 void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * List of struct kvm_mmu_pages being used as roots.
+	 * All struct kvm_mmu_pages in the list should have
+	 * tdp_mmu_page set.
+	 *
+	 * For reads, this list is protected by:
+	 *      the MMU lock in read mode + RCU or
+	 *      the MMU lock in write mode
+	 *
+	 * For writes, this list is protected by:
+	 *      the MMU lock in read mode + the tdp_mmu_pages_lock or
+	 *      the MMU lock in write mode
+	 */
 	lockdep_assert_held_write(&kvm->mmu_lock);
 	list_for_each_entry(root, &kvm->arch.tdp_mmu_roots, link)
 		if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
@@ -894,6 +1225,20 @@ void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
  * Installs a last-level SPTE to handle a TDP page fault.
  * (NPT/EPT violation/misconfiguration)
  */
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()
+ *                -> tdp_mmu_map_handle_target_level()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1274| <<kvm_tdp_mmu_map>> ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
+ */
 static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 					  struct kvm_page_fault *fault,
 					  struct tdp_iter *iter)
@@ -950,6 +1295,38 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
  * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
  * page tables and SPTEs to translate the faulting guest physical address.
  */
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4249| <<direct_page_fault>> r = kvm_tdp_mmu_map(vcpu, fault);
+ */
 int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -961,14 +1338,39 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3266| <<__direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|722| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1335| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	rcu_read_lock();
 
+	/*
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 * end是')'
+	 */
 	tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+		/*
+		 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+		 *   - arch/x86/kvm/mmu.h|205| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+		 *   - arch/x86/kvm/mmu/mmu.c|3100| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 *   - arch/x86/kvm/mmu/mmu.c|3179| <<__direct_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *
+		 * 核心思想: 本来是想用huge page的,但是已经存在一个present的pte但是不是large pte,
+		 * 所以就没法huge page了, 只能让goal_level再下降一位
+		 */
 		if (fault->nx_huge_page_workaround_enabled)
 			disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
 
+		/*
+		 * 如果到了想要的level, 必然要退出
+		 */
 		if (iter.level == fault->goal_level)
 			break;
 
@@ -990,21 +1392,45 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 			iter.old_spte = READ_ONCE(*rcu_dereference(iter.sptep));
 		}
 
+		/*
+		 * iter.old_spte: snapshot of the value at sptep
+		 */
 		if (!is_shadow_present_pte(iter.old_spte)) {
 			/*
 			 * If SPTE has been frozen by another thread, just
 			 * give up and retry, avoiding unnecessary page table
 			 * allocation and free.
 			 */
+			/*
+			 * 关于REMOVED_SPTE:
+			 * If a thread running without exclusive control of the MMU lock must perform a
+			 * multi-part operation on an SPTE, it can set the SPTE to REMOVED_SPTE as a
+			 * non-present intermediate value. Other threads which encounter this value
+			 * should not modify the SPTE.
+			 *
+			 * Use a semi-arbitrary value that doesn't set RWX bits, i.e. is not-present on
+			 * bot AMD and Intel CPUs, and doesn't set PFN bits, i.e. doesn't create a L1TF
+			 * vulnerability.  Use only low bits to avoid 64-bit immediates.
+			 *
+			 * Only used by the TDP MMU.
+			 */
 			if (is_removed_spte(iter.old_spte))
 				break;
 
 			sp = alloc_tdp_mmu_page(vcpu, iter.gfn, iter.level - 1);
 			child_pt = sp->spt;
 
+			/*
+			 * 在以下使用shadow_accessed_mask:
+			 *   - arch/x86/kvm/mmu/spte.c|330| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+			 *   - arch/x86/kvm/mmu/spte.c|382| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+			 */
 			new_spte = make_nonleaf_spte(child_pt,
 						     !shadow_accessed_mask);
 
+			/*
+			 * 走到这里说明iter.old_spte不present
+			 */
 			if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter, new_spte)) {
 				tdp_mmu_link_page(vcpu->kvm, sp,
 						  fault->huge_page_disallowed &&
@@ -1023,6 +1449,10 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		return RET_PF_RETRY;
 	}
 
+	/*
+	 * Installs a last-level SPTE to handle a TDP page fault.
+	 * (NPT/EPT violation/misconfiguration)
+	 */
 	ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
 	rcu_read_unlock();
 
@@ -1169,6 +1599,10 @@ bool kvm_tdp_mmu_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
  * [start, end). Returns true if an SPTE has been changed and the TLBs need to
  * be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1648| <<kvm_tdp_mmu_wrprot_slot>> spte_set |= wrprot_gfn_range(kvm, root, slot->base_gfn,
+ */
 static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			     gfn_t start, gfn_t end, int min_level)
 {
@@ -1213,6 +1647,10 @@ static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
  * only affect leaf SPTEs down to min_level.
  * Returns true if an SPTE has been changed and the TLBs need to be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6147| <<kvm_mmu_slot_remove_write_access>> flush |= kvm_tdp_mmu_wrprot_slot(kvm, memslot, start_level);
+ */
 bool kvm_tdp_mmu_wrprot_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *slot, int min_level)
 {
@@ -1349,6 +1787,11 @@ static void clear_dirty_pt_masked(struct kvm *kvm, struct kvm_mmu_page *root,
  * clearing the dirty status will involve clearing the dirty bit on each SPTE
  * or, if AD bits are not enabled, clearing the writable bit on each SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1311| <<kvm_mmu_write_protect_pt_masked>> kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
+ *   - arch/x86/kvm/mmu/mmu.c|1344| <<kvm_mmu_clear_dirty_pt_masked>> kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
+ */
 void kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 				       struct kvm_memory_slot *slot,
 				       gfn_t gfn, unsigned long mask,
@@ -1365,6 +1808,10 @@ void kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,
  * Clear leaf entries which could be replaced by large mappings, for
  * GFNs within the slot.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1641| <<kvm_tdp_mmu_zap_collapsible_sptes>> zap_collapsible_spte_range(kvm, root, slot);
+ */
 static void zap_collapsible_spte_range(struct kvm *kvm,
 				       struct kvm_mmu_page *root,
 				       const struct kvm_memory_slot *slot)
@@ -1409,11 +1856,18 @@ static void zap_collapsible_spte_range(struct kvm *kvm,
  * Clear non-leaf entries (and free associated page tables) which could
  * be replaced by large mappings, for GFNs within the slot.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5957| <<kvm_mmu_zap_collapsible_sptes>> kvm_tdp_mmu_zap_collapsible_sptes(kvm, slot);
+ */
 void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
 				       const struct kvm_memory_slot *slot)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * read lock !!!!
+	 */
 	lockdep_assert_held_read(&kvm->mmu_lock);
 
 	for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
@@ -1425,6 +1879,10 @@ void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1657| <<kvm_tdp_mmu_write_protect_gfn>> spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
+ */
 static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
 			      gfn_t gfn, int min_level)
 {
@@ -1462,6 +1920,10 @@ static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1425| <<kvm_mmu_slot_gfn_write_protect>> kvm_tdp_mmu_write_protect_gfn(kvm, slot, gfn, min_level);
+ */
 bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level)
@@ -1470,6 +1932,12 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 	bool spte_set = false;
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
+	/*
+	 * write_protect_gfn():
+	 *   Removes write access on the last level SPTE mapping this GFN and unsets the
+	 *   MMU-writable bit to ensure future writes continue to be intercepted.
+	 *   Returns true if an SPTE was set and a TLB flush is needed.
+	 */
 	for_each_tdp_mmu_root(kvm, root, slot->as_id)
 		spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
 
@@ -1482,6 +1950,10 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
  *
  * Must be called between kvm_tdp_mmu_walk_lockless_{begin,end}.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3823| <<get_mmio_spte>> leaf = kvm_tdp_mmu_get_walk(vcpu, addr, sptes, &root);
+ */
 int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 			 int *root_level)
 {
@@ -1494,6 +1966,10 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 
 	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
 		leaf = iter.level;
+		/*
+		 * 从get_mmio_spte()来的时候:
+		 * u64 sptes[PT64_ROOT_MAX_LEVEL + 1];
+		 */
 		sptes[leaf] = iter.old_spte;
 	}
 
@@ -1511,6 +1987,10 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
  *
  * WARNING: This function is only intended to be called during fast_page_fault.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3173| <<fast_page_fault>> sptep = kvm_tdp_mmu_fast_pf_get_last_sptep(vcpu, fault->addr, &spte);
+ */
 u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 					u64 *spte)
 {
@@ -1520,6 +2000,10 @@ u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 	tdp_ptep_t sptep = NULL;
 
 	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+		/*
+		 * iter.old_spte : A snapshot of the value at sptep
+		 * iter.sptep    : A pointer to the current SPTE
+		 */
 		*spte = iter.old_spte;
 		sptep = iter.sptep;
 	}
diff --git a/arch/x86/kvm/mmu/tdp_mmu.h b/arch/x86/kvm/mmu/tdp_mmu.h
index 3899004a5d91..8dd0d8311add 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@ -7,12 +7,29 @@
 
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|182| <<tdp_mmu_next_root>> while (next_root && !kvm_tdp_mmu_get_root(kvm, next_root))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|297| <<kvm_tdp_mmu_get_vcpu_root_hpa>> kvm_tdp_mmu_get_root(kvm, root))
+ */
 __must_check static inline bool kvm_tdp_mmu_get_root(struct kvm *kvm,
 						     struct kvm_mmu_page *root)
 {
 	if (root->role.invalid)
 		return false;
 
+	/*
+	 * increment a refcount unless it is 0
+	 * Return: true if the increment was successful, false otherwise
+	 */
+	/*
+	 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+	 */
 	return refcount_inc_not_zero(&root->tdp_mmu_root_count);
 }
 
@@ -21,11 +38,20 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 
 bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 				 gfn_t end, bool can_yield, bool flush);
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5809| <<kvm_zap_gfn_range>> flush = kvm_tdp_mmu_zap_gfn_range(kvm, i, gfn_start,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|801| <<kvm_tdp_mmu_zap_all>> flush = kvm_tdp_mmu_zap_gfn_range(kvm, i, 0, -1ull, flush);
+ */
 static inline bool kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id,
 					     gfn_t start, gfn_t end, bool flush)
 {
 	return __kvm_tdp_mmu_zap_gfn_range(kvm, as_id, start, end, true, flush);
 }
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6301| <<kvm_recover_nx_lpages>> flush |= kvm_tdp_mmu_zap_sp(kvm, sp);
+ */
 static inline bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	gfn_t end = sp->gfn + KVM_PAGES_PER_HPAGE(sp->role.level + 1);
@@ -40,6 +66,12 @@ static inline bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 	 * of the shadow page's gfn range and stop iterating before yielding.
 	 */
 	lockdep_assert_held_write(&kvm->mmu_lock);
+	/*
+	 * Tears down the mappings for the range of gfns, [start, end), and frees the
+	 * non-root pages mapping GFNs strictly within that range. Returns true if
+	 * SPTEs have been cleared and a TLB flush is needed before releasing the
+	 * MMU lock.
+	 */
 	return __kvm_tdp_mmu_zap_gfn_range(kvm, kvm_mmu_page_as_id(sp),
 					   sp->gfn, end, false, false);
 }
@@ -71,6 +103,10 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|696| <<walk_shadow_page_lockless_begin>> kvm_tdp_mmu_walk_lockless_begin();
+ */
 static inline void kvm_tdp_mmu_walk_lockless_begin(void)
 {
 	rcu_read_lock();
@@ -105,6 +141,24 @@ static inline bool is_tdp_mmu(struct kvm_mmu *mmu)
 	 * pae_root page, not a shadow page.
 	 */
 	sp = to_shadow_page(hpa);
+	/*
+	 * 在以下设置kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|3358| <<mmu_alloc_root>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu/mmu.c|3255| <<mmu_free_root_page>> else if (!--sp->root_count && sp->role.invalid)
+	 * 在以下使用kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|2364| <<__kvm_mmu_prepare_zap_page>> if (!sp->root_count) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2430| <<kvm_mmu_commit_zap_page>> WARN_ON(!sp->role.invalid || sp->root_count);
+	 *   - arch/x86/kvm/mmu/mmu.c|2453| <<kvm_mmu_zap_oldest_mmu_pages>> if (sp->root_count)
+	 *   - arch/x86/kvm/mmu/mmutrace.h|22| <<KVM_MMU_PAGE_ASSIGN>> __entry->root_count = sp->root_count; \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|108| <<is_tdp_mmu>> return sp && is_tdp_mmu_page(sp) && sp->root_count;
+	 *
+	 * 在以下使用tdp_mmu_root_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+	 */
 	return sp && is_tdp_mmu_page(sp) && sp->root_count;
 }
 #else
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index b1a02993782b..104b07a5201a 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -238,6 +238,11 @@ void reprogram_gp_counter(struct kvm_pmc *pmc, u64 eventsel)
 }
 EXPORT_SYMBOL_GPL(reprogram_gp_counter);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|290| <<reprogram_counter>> reprogram_fixed_counter(pmc, ctrl, idx);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|53| <<reprogram_fixed_counters>> reprogram_fixed_counter(pmc, new_ctrl, i);
+ */
 void reprogram_fixed_counter(struct kvm_pmc *pmc, u8 ctrl, int idx)
 {
 	unsigned en_field = ctrl & 0x3;
@@ -274,6 +279,12 @@ void reprogram_fixed_counter(struct kvm_pmc *pmc, u8 ctrl, int idx)
 }
 EXPORT_SYMBOL_GPL(reprogram_fixed_counter);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|308| <<kvm_pmu_handle_event>> reprogram_counter(pmu, bit);
+ *   - arch/x86/kvm/pmu.c|500| <<kvm_pmu_incr_counter>> reprogram_counter(pmu, pmc->idx);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|68| <<global_ctrl_changed>> reprogram_counter(pmu, bit);
+ */
 void reprogram_counter(struct kvm_pmu *pmu, int pmc_idx)
 {
 	struct kvm_pmc *pmc = kvm_x86_ops.pmu_ops->pmc_idx_to_pmc(pmu, pmc_idx);
@@ -422,8 +433,16 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  * settings are changed (such as changes of PMU CPUID by guest VMs), which
  * should rarely happen.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|322| <<kvm_vcpu_after_set_cpuid>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/pmu.c|458| <<kvm_pmu_init>> kvm_pmu_refresh(vcpu);
+ */
 void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * intel_pmu_refresh()
+	 */
 	kvm_x86_ops.pmu_ops->refresh(vcpu);
 }
 
@@ -530,6 +549,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3537| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8229| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8499| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8501| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/svm/sev.c b/arch/x86/kvm/svm/sev.c
index 17b53457d866..e86056157d89 100644
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@ -234,6 +234,10 @@ static void sev_unbind_asid(struct kvm *kvm, unsigned int handle)
 	sev_decommission(handle);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/sev.c|1795| <<svm_mem_enc_op>> r = sev_guest_init(kvm, &sev_cmd);
+ */
 static int sev_guest_init(struct kvm *kvm, struct kvm_sev_cmd *argp)
 {
 	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
diff --git a/arch/x86/kvm/vmx/nested.h b/arch/x86/kvm/vmx/nested.h
index b69a80f43b37..f56f9b239732 100644
--- a/arch/x86/kvm/vmx/nested.h
+++ b/arch/x86/kvm/vmx/nested.h
@@ -39,6 +39,12 @@ bool nested_vmx_check_io_bitmaps(struct kvm_vcpu *vcpu, unsigned int port,
 
 static inline struct vmcs12 *get_vmcs12(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;
+	 */
 	return to_vmx(vcpu)->nested.cached_vmcs12;
 }
 
diff --git a/arch/x86/kvm/vmx/posted_intr.c b/arch/x86/kvm/vmx/posted_intr.c
index aa1fe9085d77..2199288bf023 100644
--- a/arch/x86/kvm/vmx/posted_intr.c
+++ b/arch/x86/kvm/vmx/posted_intr.c
@@ -27,6 +27,17 @@ static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
  * CPU.  IRQs must be disabled when taking this lock, otherwise deadlock will
  * occur if a wakeup IRQ arrives and attempts to acquire the lock.
  */
+/*
+ * 在以下使用wakeup_vcpus_on_cpu_lock:
+ *   - arch/x86/kvm/vmx/posted_intr.c|30| <<global>> static DEFINE_PER_CPU(raw_spinlock_t, wakeup_vcpus_on_cpu_lock);
+ *   - arch/x86/kvm/vmx/posted_intr.c|90| <<vmx_vcpu_pi_load>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|92| <<vmx_vcpu_pi_load>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|152| <<pi_enable_wakeup_handler>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_enable_wakeup_handler>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|207| <<pi_wakeup_handler>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|214| <<pi_wakeup_handler>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|220| <<pi_init_cpu>> raw_spin_lock_init(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+ */
 static DEFINE_PER_CPU(raw_spinlock_t, wakeup_vcpus_on_cpu_lock);
 
 static inline struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index b730d799c26e..48a044dd681d 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -1240,6 +1240,13 @@ static void vmx_write_guest_kernel_gs_base(struct vcpu_vmx *vmx, u64 data)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|269| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu, prev);
+ *   - arch/x86/kvm/vmx/nested.c|4100| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|4105| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu, &vmx->nested.vmcs02);
+ *   - arch/x86/kvm/vmx/vmx.c|1316| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu, NULL);
+ */
 void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
 			struct loaded_vmcs *buddy)
 {
@@ -1274,6 +1281,9 @@ void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
 		 * the active VMCS within a guest, e.g. on nested VM-Enter.
 		 * The L1 VMM can protect itself with retpolines, IBPB or IBRS.
 		 */
+		/*
+		 * 只有nested的时候buddy才不为NULL
+		 */
 		if (!buddy || WARN_ON_ONCE(buddy->vmcs != prev))
 			indirect_branch_prediction_barrier();
 	}
@@ -5307,6 +5317,9 @@ static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_APIC_WRITE]
+ */
 static int handle_apic_write(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);
@@ -5463,10 +5476,29 @@ static int handle_nmi_window(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5496| <<handle_invalid_guest_state>> if (vmx_emulation_required_with_pending_exception(vcpu)) {
+ *   - arch/x86/kvm/vmx/vmx.c|5520| <<vmx_vcpu_pre_run>> if (vmx_emulation_required_with_pending_exception(vcpu)) {
+ */
 static bool vmx_emulation_required_with_pending_exception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_queued_exception {
+	 *           bool pending;
+	 *           bool injected;
+	 *           bool has_error_code;
+	 *           u8 nr;
+	 *           u32 error_code;
+	 *           unsigned long payload;
+	 *           bool has_payload;
+	 *           u8 nested_apf;
+	 *       } exception;
+	 */
 	return vmx->emulation_required && !vmx->rmode.vm86_active &&
 	       vcpu->arch.exception.pending;
 }
@@ -5634,6 +5666,11 @@ static int handle_pml_full(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5674| <<handle_preemption_timer>> handle_fastpath_preemption_timer(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6778| <<vmx_exit_handlers_fastpath>> return handle_fastpath_preemption_timer(vcpu);
+ */
 static fastpath_t handle_fastpath_preemption_timer(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7398,6 +7435,10 @@ static void vmx_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 	vmx_update_exception_bitmap(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8065| <<hardware_setup>> vmx_set_cpu_caps();
+ */
 static __init void vmx_set_cpu_caps(void)
 {
 	kvm_set_cpu_caps();
@@ -7890,6 +7931,9 @@ static __init void vmx_setup_user_return_msrs(void)
 
 static struct kvm_x86_init_ops vmx_init_ops __initdata;
 
+/*
+ * struct kvm_x86_init_ops vmx_init_ops.hardware_setup = hardware_setup()
+ */
 static __init int hardware_setup(void)
 {
 	unsigned long host_bndcfgs;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index eb4029660bd9..a5abd4f5be24 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -154,10 +154,32 @@ bool __read_mostly kvm_has_tsc_control;
 EXPORT_SYMBOL_GPL(kvm_has_tsc_control);
 u32  __read_mostly kvm_max_guest_tsc_khz;
 EXPORT_SYMBOL_GPL(kvm_max_guest_tsc_khz);
+/*
+ * 在以下设置kvm_tsc_scaling_ratio_frac_bits:
+ *   - arch/x86/kvm/svm/svm.c|4782| <<svm_hardware_setup>> kvm_tsc_scaling_ratio_frac_bits = 32;
+ *   - arch/x86/kvm/vmx/vmx.c|8017| <<hardware_setup>> kvm_tsc_scaling_ratio_frac_bits = 48;
+ */
 u8   __read_mostly kvm_tsc_scaling_ratio_frac_bits;
 EXPORT_SYMBOL_GPL(kvm_tsc_scaling_ratio_frac_bits);
 u64  __read_mostly kvm_max_tsc_scaling_ratio;
 EXPORT_SYMBOL_GPL(kvm_max_tsc_scaling_ratio);
+/*
+ * 在以下设置kvm_default_tsc_scaling_ratio:
+ *   - arch/x86/kvm/x86.c|11825| <<kvm_arch_hardware_setup>> kvm_default_tsc_scaling_ratio = 1ULL << kvm_tsc_scaling_ratio_frac_bits;
+ * 在以下使用kvm_default_tsc_scaling_ratio:
+ *   - arch/x86/kvm/lapic.c|1591| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+ *   - arch/x86/kvm/svm/nested.c|581| <<nested_vmcb02_prepare_control>> if (svm->tsc_ratio_msr != kvm_default_tsc_scaling_ratio) {
+ *   - arch/x86/kvm/svm/nested.c|870| <<nested_svm_vmexit>> if (svm->tsc_ratio_msr != kvm_default_tsc_scaling_ratio) {
+ *   - arch/x86/kvm/svm/svm.c|1153| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_default_tsc_scaling_ratio;
+ *   - arch/x86/kvm/vmx/vmx.c|1720| <<vmx_get_l2_tsc_multiplier>> return kvm_default_tsc_scaling_ratio;
+ *   - arch/x86/kvm/vmx/vmx.c|7600| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+ *   - arch/x86/kvm/x86.c|2299| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_default_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2342| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_default_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2439| <<kvm_scale_tsc>> if (ratio != kvm_default_tsc_scaling_ratio)
+ *   - arch/x86/kvm/x86.c|2477| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_default_tsc_scaling_ratio)
+ *   - arch/x86/kvm/x86.c|2490| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_default_tsc_scaling_ratio)
+ *   - arch/x86/kvm/x86.c|2701| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+ */
 u64 __read_mostly kvm_default_tsc_scaling_ratio;
 EXPORT_SYMBOL_GPL(kvm_default_tsc_scaling_ratio);
 bool __read_mostly kvm_has_bus_lock_exit;
@@ -190,6 +212,17 @@ int __read_mostly pi_inject_timer = -1;
 module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
 
 /* Enable/disable PMU virtualization */
+/*
+ * 在以下使用enable_pmu:
+ *   - arch/x86/kvm/x86.c|215| <<global>> bool __read_mostly enable_pmu = true;
+ *   - arch/x86/kvm/x86.c|217| <<global>> module_param(enable_pmu, bool, 0444);
+ *   - arch/x86/kvm/cpuid.c|889| <<__do_cpuid_func>> if (!cap.version || !enable_pmu)
+ *   - arch/x86/kvm/svm/pmu.c|104| <<get_gp_pmc_amd>> if (!enable_pmu)
+ *   - arch/x86/kvm/svm/svm.c|4734| <<svm_set_cpu_caps>> if (enable_pmu && boot_cpu_has(X86_FEATURE_PERFCTR_CORE))
+ *   - arch/x86/kvm/svm/svm.c|4873| <<svm_hardware_setup>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/capabilities.h|392| <<vmx_get_perf_capabilities>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/pmu_intel.c|490| <<intel_pmu_refresh>> if (!entry || !enable_pmu)
+ */
 bool __read_mostly enable_pmu = true;
 EXPORT_SYMBOL_GPL(enable_pmu);
 module_param(enable_pmu, bool, 0444);
@@ -707,6 +740,20 @@ static void kvm_queue_exception_e_p(struct kvm_vcpu *vcpu, unsigned nr,
 			       true, payload, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7884| <<global>> .complete_emulated_msr = kvm_complete_insn_gp,
+ *   - arch/x86/kvm/svm/svm.c|2411| <<cr_interception>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/svm/svm.c|2446| <<cr_trap>> return kvm_complete_insn_gp(vcpu, ret);
+ *   - arch/x86/kvm/svm/svm.c|2481| <<dr_interception>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/svm/svm.c|2515| <<efer_trap>> return kvm_complete_insn_gp(vcpu, ret);
+ *   - arch/x86/kvm/svm/svm.c|2652| <<svm_complete_emulated_msr>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/vmx/vmx.c|5112| <<handle_cr>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/vmx/vmx.c|5117| <<handle_cr>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/vmx/vmx.c|5120| <<handle_cr>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/vmx/vmx.c|5125| <<handle_cr>> ret = kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/vmx/vmx.c|5234| <<handle_dr>> return kvm_complete_insn_gp(vcpu, err);
+ */
 int kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err)
 {
 	if (err)
@@ -2207,6 +2254,11 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3640| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME_NEW)>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|3646| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME)>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
@@ -2273,6 +2325,16 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
+/*
+ * 在以下使用percpu的cpu_tsc_khz:
+ *   - arch/x86/kvm/x86.c|2303| <<global>> static DEFINE_PER_CPU(unsigned long , cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|3022| <<__get_kvmclock>> if (ka->use_master_clock && __this_cpu_read(cpu_tsc_khz)) {
+ *   - arch/x86/kvm/x86.c|3036| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+ *   - arch/x86/kvm/x86.c|3170| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|8802| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+ *   - arch/x86/kvm/x86.c|8833| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+ *   - arch/x86/kvm/x86.c|8851| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+ */
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
 static unsigned long max_tsc_khz;
 
@@ -2321,6 +2383,11 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5686| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|11438| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, max_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2370,6 +2437,10 @@ static inline int gtod_is_based_on_tsc(int mode)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2586| <<__kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_X86_64
@@ -2377,6 +2448,11 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 
+	/*
+	 * x86在以下设置kvm->online_vcpus:
+	 *   - virt/kvm/kvm_main.c|477| <<kvm_destroy_vcpus>> atomic_set(&kvm->online_vcpus, 0);
+	 *   - virt/kvm/kvm_main.c|3914| <<kvm_vm_ioctl_create_vcpu>> atomic_inc(&kvm->online_vcpus);
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			 atomic_read(&vcpu->kvm->online_vcpus));
 
@@ -2388,6 +2464,10 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	 * and the vcpus need to have matched TSCs.  When that happens,
 	 * perform request to enable masterclock.
 	 */
+	/*
+	 * KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   kvm_update_masterclock(vcpu->kvm);
+	 */
 	if (ka->use_master_clock ||
 	    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
 		kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -2424,6 +2504,13 @@ u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc, u64 ratio)
 }
 EXPORT_SYMBOL_GPL(kvm_scale_tsc);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2615| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|2654| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3649| <<kvm_set_msr_common>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+ *   - arch/x86/kvm/x86.c|4673| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu,
+ */
 static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
@@ -2435,6 +2522,10 @@ static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2485| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = l1_offset;
+	 */
 	return vcpu->arch.l1_tsc_offset +
 		kvm_scale_tsc(vcpu, host_tsc, vcpu->arch.l1_tsc_scaling_ratio);
 }
@@ -2465,6 +2556,12 @@ u64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)
 }
 EXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_multiplier);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2545| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2635| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|4578| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 {
 	trace_kvm_write_tsc_offset(vcpu->vcpu_id,
@@ -2489,6 +2586,12 @@ static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 	static_call(kvm_x86_write_tsc_offset)(vcpu, vcpu->arch.tsc_offset);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2299| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_default_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2325| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, ratio);
+ *   - arch/x86/kvm/x86.c|2342| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_default_tsc_scaling_ratio);
+ */
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier)
 {
 	vcpu->arch.l1_tsc_scaling_ratio = l1_multiplier;
@@ -2524,6 +2627,11 @@ static inline bool kvm_check_tsc_unstable(void)
  * offset for the vcpu and tracks the TSC matching generation that the vcpu
  * participates in.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2647| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched);
+ *   - arch/x86/kvm/x86.c|5269| <<kvm_arch_tsc_set_attr>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched);
+ */
 static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 				  u64 ns, bool matched)
 {
@@ -2571,6 +2679,11 @@ static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 	kvm_track_tsc_matching(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3601| <<kvm_set_msr_common(MSR_IA32_TSC)>> kvm_synchronize_tsc(vcpu, data); --> 要求msr_info->host_initiated=1
+ *   - arch/x86/kvm/x86.c|11403| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, 0);
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -2580,8 +2693,17 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	bool synchronizing = false;
 
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	/*
+	 * 0 - rdtsc()
+	 */
 	offset = kvm_compute_l1_tsc_offset(vcpu, data);
 	ns = get_kvmclock_base_ns();
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2611| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2666| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|11802| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	elapsed = ns - kvm->arch.last_tsc_nsec;
 
 	if (vcpu->arch.virtual_tsc_khz) {
@@ -2593,6 +2715,12 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			 */
 			synchronizing = true;
 		} else {
+			/*
+			 * 在以下使用kvm_arch->last_tsc_write:
+			 *   - arch/x86/kvm/x86.c|2612| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+			 *   - arch/x86/kvm/x86.c|2677| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+			 *   - arch/x86/kvm/x86.c|11803| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_write = 0;
+			 */
 			u64 tsc_exp = kvm->arch.last_tsc_write +
 						nsec_to_cycles(vcpu, elapsed);
 			u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
@@ -2615,6 +2743,11 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	if (synchronizing &&
 	    vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
 		if (!kvm_check_tsc_unstable()) {
+			/*
+			 * 在以下使用kvm_arch->cur_tsc_offset:
+			 *   - arch/x86/kvm/x86.c|2596| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+			 *   - arch/x86/kvm/x86.c|2662| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+			 */
 			offset = kvm->arch.cur_tsc_offset;
 		} else {
 			u64 delta = nsec_to_cycles(vcpu, elapsed);
@@ -2628,6 +2761,13 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2644| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+ *   - arch/x86/kvm/x86.c|3072| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+ *   - arch/x86/kvm/x86.c|3533| <<kvm_set_msr_common>> adjust_tsc_offset_guest(vcpu, adj);
+ *   - arch/x86/kvm/x86.c|3566| <<kvm_set_msr_common>> adjust_tsc_offset_guest(vcpu, adj);
+ */
 static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 					   s64 adjustment)
 {
@@ -2806,6 +2946,13 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2893| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|6250| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|8692| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|11786| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2873,6 +3020,18 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 		kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
 }
 
+/*
+ * 在以下触发KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1368| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2222| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2412| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8946| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11743| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|98| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|10054| <<vcpu_enter_guest(KVM_REQ_MASTERCLOCK_UPDATE)>> kvm_update_masterclock(vcpu->kvm);
+ */
 static void kvm_update_masterclock(struct kvm *kvm)
 {
 	kvm_hv_invalidate_tsc_page(kvm);
@@ -2882,6 +3041,19 @@ static void kvm_update_masterclock(struct kvm *kvm)
 }
 
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
+/*
+ * struct kvm_clock_data {
+ *     __u64 clock;
+ *     __u32 flags;
+ *     __u32 pad0;
+ *     __u64 realtime;
+ *     __u64 host_tsc;
+ *     __u32 pad[4];
+ * };
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|2926| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2916,6 +3088,11 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	put_cpu();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2934| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+ *   - arch/x86/kvm/x86.c|6175| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+ */
 static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2994,6 +3171,10 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 				     sizeof(vcpu->hv_clock.version));
 }
 
+/*
+ * 处理KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|9944| <<vcpu_enter_guest>> r = kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -3014,8 +3195,15 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 */
 	do {
 		seq = read_seqcount_begin(&ka->pvclock_sc);
+		/*
+		 * 在以下设置kvm_arch->use_master_clock:
+		 *   - arch/x86/kvm/x86.c|2828| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+		 */
 		use_master_clock = ka->use_master_clock;
 		if (use_master_clock) {
+			/*
+			 * 最早的时候live crash了好多次, 这两个都不变
+			 */
 			host_tsc = ka->master_cycle_now;
 			kernel_ns = ka->master_kernel_ns;
 		}
@@ -3031,6 +3219,9 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	}
 	if (!use_master_clock) {
 		host_tsc = rdtsc();
+		/*
+		 * Returns monotonic time since boot in ns
+		 */
 		kernel_ns = get_kvmclock_base_ns();
 	}
 
@@ -3046,6 +3237,12 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 *      time to disappear, and the guest to stand still or run
 	 *	very slowly.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2306| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3099| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4635| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	if (vcpu->tsc_catchup) {
 		u64 tsc = compute_guest_tsc(v, kernel_ns);
 		if (tsc > tsc_timestamp) {
@@ -3058,6 +3255,9 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	/* With all the info we got, fill in the values */
 
+	/*
+	 * TSC Scaling
+	 */
 	if (kvm_has_tsc_control)
 		tgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz,
 					    v->arch.l1_tsc_scaling_ratio);
@@ -3069,8 +3269,20 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		vcpu->hw_tsc_khz = tgt_tsc_khz;
 	}
 
+	/*
+	 * 在以下设置pvclock_vcpu_time_info->tsc_timestamp:
+	 *   - arch/x86/kvm/x86.c|3072| <<kvm_guest_time_update>> vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
+	 */
 	vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
 	vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2554| <<__kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = tsc;
+	 *   - arch/x86/kvm/x86.c|3128| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|10291| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|4633| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 */
 	vcpu->last_guest_tsc = tsc_timestamp;
 
 	/* If the host uses TSC clocksource, then it is stable */
@@ -3128,6 +3340,13 @@ static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 	struct kvm *kvm = v->kvm;
 
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3185| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|3201| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11839| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11885| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
 }
@@ -3161,6 +3380,10 @@ static bool can_set_mci_status(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * 处理MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:
+ *   - arch/x86/kvm/x86.c|3871| <<kvm_set_msr_common>> return set_msr_mce(vcpu, msr_info);
+ */
 static int set_msr_mce(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	u64 mcg_cap = vcpu->arch.mcg_cap;
@@ -3326,6 +3549,10 @@ void kvm_service_local_tlb_flush_requests(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_service_local_tlb_flush_requests);
 
+/*
+ * 处理KVM_REQ_STEAL_UPDATE:
+ *   - arch/x86/kvm/x86.c|9907| <<vcpu_enter_guest>> record_steal_time(vcpu);
+ */
 static void record_steal_time(struct kvm_vcpu *vcpu)
 {
 	struct gfn_to_hva_cache *ghc = &vcpu->arch.st.cache;
@@ -3499,8 +3726,21 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		kvm_set_lapic_tscdeadline_msr(vcpu, data);
 		break;
 	case MSR_IA32_TSC_ADJUST:
+		/*
+		 * 关于msr_tsc_adjust
+		 * 如果想要修改tsc的counter, 可以直接写入msr_tsc.
+		 * 然而这样很难保证所有的CPU是同步的 (因为每个CPU都要往msr_tsc写一个absolute value)
+		 * 使用msr_tsc_adjust就可以解决这个问题, 因为为每个CPU写入的是offset, 不是absolute value
+		 */
 		if (guest_cpuid_has(vcpu, X86_FEATURE_TSC_ADJUST)) {
 			if (!msr_info->host_initiated) {
+				/*
+				 * 在以下使用kvm_vcpu_arch->ia32_tsc_adjust_msr:
+				 *   - arch/x86/kvm/x86.c|3532| <<kvm_set_msr_common>> s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
+				 *   - arch/x86/kvm/x86.c|3539| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr = data;
+				 *   - arch/x86/kvm/x86.c|3567| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr += adj;
+				 *   - arch/x86/kvm/x86.c|3932| <<kvm_get_msr_common>> msr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;
+				 */
 				s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
 				adjust_tsc_offset_guest(vcpu, adj);
 				/* Before back to guest, tsc_timestamp must be adjusted
@@ -4515,6 +4755,19 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * [0] kvm_arch_vcpu_load
+ * [0] vcpu_load
+ * [0] kvm_arch_vcpu_create
+ * [0] kvm_vm_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|214| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5792| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Address WBINVD may be executed by guest */
@@ -4558,10 +4811,19 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 		 * On a host with synchronized TSC, there is no need to update
 		 * kvmclock on vcpu->cpu migration
 		 */
+		/*
+		 * 在以下设置kvm_arch->use_master_clock:
+		 *   - arch/x86/kvm/x86.c|2828| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+		 *
+		 * 触发kvm_gen_kvmclock_update()
+		 */
 		if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
 			kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 		if (vcpu->cpu != cpu)
 			kvm_make_request(KVM_REQ_MIGRATE_TIMER, vcpu);
+		/*
+		 * 这里设置了vcpu->cpu !!!
+		 */
 		vcpu->cpu = cpu;
 	}
 
@@ -6175,6 +6437,10 @@ static int kvm_vm_ioctl_get_clock(struct kvm *kvm, void __user *argp)
 	return 0;
 }
 
+/*
+ * 处理KVM_SET_CLOCK:
+ *   - arch/x86/kvm/x86.c|6625| <<kvm_arch_vm_ioctl(KVM_SET_CLOCK)>> r = kvm_vm_ioctl_set_clock(kvm, argp);
+ */
 static int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -7096,6 +7362,22 @@ static int emulator_read_write_onepage(unsigned long addr, void *val,
 	return X86EMUL_CONTINUE;
 }
 
+/*
+ * [0] emulator_read_write
+ * [0] segmented_read
+ * [0] x86_emulate_insn
+ * [0] x86_emulate_instruction
+ * [0] vmx_handle_exit
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|7163| <<emulator_read_emulated>> return emulator_read_write(ctxt, addr, val, bytes,
+ *   - arch/x86/kvm/x86.c|7173| <<emulator_write_emulated>> return emulator_read_write(ctxt, addr, (void *)val, bytes,
+ */
 static int emulator_read_write(struct x86_emulate_ctxt *ctxt,
 			unsigned long addr,
 			void *val, unsigned int bytes,
@@ -8269,6 +8551,12 @@ int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 }
 EXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5932| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+ *   - arch/x86/kvm/x86.c|8454| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|8461| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len)
 {
@@ -8563,6 +8851,22 @@ static int kvmclock_cpu_down_prep(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * "constant tsc" (或者"synchronized tsc")是指"The TSC is synchronized
+ * across all sockets/cores".
+ *
+ * 下面是"/proc/cpuinfo"中部分关于tsc的解释
+ *
+ * tsc          : The system has a TSC clock
+ * rdtscp       : The RDTSCP instruction is available
+ * constant_tsc : The TSC is synchronized across all sockets/cores
+ * nonstop_tsc  : The TSC is not affected by power management code
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|8658| <<__kvmclock_cpufreq_notifier>> smp_call_function_single(cpu, tsc_khz_changed, freq, 1);
+ *   - arch/x86/kvm/x86.c|8685| <<__kvmclock_cpufreq_notifier>> smp_call_function_single(cpu, tsc_khz_changed, freq, 1);
+ *   - arch/x86/kvm/x86.c|8712| <<kvmclock_cpu_online>> tsc_khz_changed(NULL);
+ */
 static void tsc_khz_changed(void *data)
 {
 	struct cpufreq_freqs *freq = data;
@@ -8574,6 +8878,16 @@ static void tsc_khz_changed(void *data)
 		khz = cpufreq_quick_get(raw_smp_processor_id());
 	if (!khz)
 		khz = tsc_khz;
+	/*
+	 * 在以下使用percpu的cpu_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2303| <<global>> static DEFINE_PER_CPU(unsigned long , cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|3022| <<__get_kvmclock>> if (ka->use_master_clock && __this_cpu_read(cpu_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3036| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+	 *   - arch/x86/kvm/x86.c|3170| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|8802| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+	 *   - arch/x86/kvm/x86.c|8833| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+	 *   - arch/x86/kvm/x86.c|8851| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+	 */
 	__this_cpu_write(cpu_tsc_khz, khz);
 }
 
@@ -8709,6 +9023,10 @@ static int kvmclock_cpu_online(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9150| <<kvm_arch_init>> kvm_timer_init();
+ */
 static void kvm_timer_init(void)
 {
 	max_tsc_khz = tsc_khz;
@@ -8736,6 +9054,10 @@ static void kvm_timer_init(void)
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用pvclock_gtod_update_fn():
+ *   - arch/x86/kvm/x86.c|8909| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ */
 static void pvclock_gtod_update_fn(struct work_struct *work)
 {
 	struct kvm *kvm;
@@ -8767,6 +9089,9 @@ static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
 /*
  * Notification about pvclock gtod data update.
  */
+/*
+ * 调用的很频繁
+ */
 static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 			       void *priv)
 {
@@ -8791,6 +9116,14 @@ static struct notifier_block pvclock_gtod_notifier = {
 };
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5664| <<kvm_init>> r = kvm_arch_init(opaque);
+ *
+ * vmx_init()
+ * -> kvm_init()
+ *    -> kvm_arch_init()
+ */
 int kvm_arch_init(void *opaque)
 {
 	struct kvm_x86_init_ops *ops = opaque;
@@ -8999,6 +9332,18 @@ static void kvm_pv_kick_cpu_op(struct kvm *kvm, unsigned long flags, int apicid)
 	kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|230| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|4212| <<kvm_faultin_pfn>> !kvm_apicv_activated(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/avic.c|182| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+ *   - arch/x86/kvm/svm/avic.c|247| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/nested.c|558| <<nested_vmcb02_prepare_control>> WARN_ON(kvm_apicv_activated(svm->vcpu.kvm));
+ *   - arch/x86/kvm/svm/svm.c|1393| <<svm_set_vintr>> WARN_ON(kvm_apicv_activated(svm->vcpu.kvm));
+ *   - arch/x86/kvm/x86.c|9701| <<kvm_vcpu_update_apicv>> activate = kvm_apicv_activated(vcpu->kvm);
+ *   - arch/x86/kvm/x86.c|10105| <<vcpu_enter_guest>> WARN_ON_ONCE(kvm_apicv_activated(vcpu->kvm) != kvm_vcpu_apicv_active(vcpu));
+ *   - arch/x86/kvm/x86.c|11159| <<kvm_arch_vcpu_create>> if (kvm_apicv_activated(vcpu->kvm))
+ */
 bool kvm_apicv_activated(struct kvm *kvm)
 {
 	return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
@@ -9234,6 +9579,11 @@ int kvm_check_nested_events(struct kvm_vcpu *vcpu)
 	return kvm_x86_ops.nested_ops->check_events(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9260| <<inject_pending_event>> kvm_inject_exception(vcpu);
+ *   - arch/x86/kvm/x86.c|9323| <<inject_pending_event>> kvm_inject_exception(vcpu);
+ */
 static void kvm_inject_exception(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->arch.exception.error_code && !is_protmode(vcpu))
@@ -9656,6 +10006,10 @@ void kvm_make_scan_ioapic_request(struct kvm *kvm)
 	kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
 }
 
+/*
+ * 处理KVM_REQ_APICV_UPDATE:
+ *   - arch/x86/kvm/x86.c|10033| <<vcpu_enter_guest>> kvm_vcpu_update_apicv(vcpu);
+ */
 void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 {
 	bool activate;
@@ -9687,6 +10041,12 @@ void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_update_apicv);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|122| <<synic_update_vector>> __kvm_request_apicv_update(vcpu->kvm,
+ *   - arch/x86/kvm/x86.c|9768| <<kvm_request_apicv_update>> __kvm_request_apicv_update(kvm, activate, bit);
+ *   - arch/x86/kvm/x86.c|10952| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_request_apicv_update(kvm, !inhibit, APICV_INHIBIT_REASON_BLOCKIRQ);
+ */
 void __kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
 {
 	unsigned long old, new;
@@ -9729,6 +10089,17 @@ void __kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
 }
 EXPORT_SYMBOL_GPL(__kvm_request_apicv_update);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_request_apicv_update(kvm, false, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/i8254.c|315| <<kvm_pit_set_reinject>> kvm_request_apicv_update(kvm, true, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/svm/svm.c|2909| <<interrupt_window_interception>> kvm_request_apicv_update(vcpu->kvm, true, APICV_INHIBIT_REASON_IRQWIN);
+ *   - arch/x86/kvm/svm/svm.c|3507| <<svm_enable_irq_window>> kvm_request_apicv_update(vcpu->kvm, false, APICV_INHIBIT_REASON_IRQWIN);
+ *   - arch/x86/kvm/svm/svm.c|3980| <<svm_vcpu_after_set_cpuid>> kvm_request_apicv_update(vcpu->kvm, false, APICV_INHIBIT_REASON_X2APIC);
+ *   - arch/x86/kvm/svm/svm.c|3988| <<svm_vcpu_after_set_cpuid>> kvm_request_apicv_update(vcpu->kvm, false, APICV_INHIBIT_REASON_NESTED);
+ *   - arch/x86/kvm/x86.c|5918| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm_request_apicv_update(kvm, true, APICV_INHIBIT_REASON_ABSENT);
+ *   - arch/x86/kvm/x86.c|6300| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm_request_apicv_update(kvm, true, APICV_INHIBIT_REASON_ABSENT);
+ */
 void kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
 {
 	down_write(&kvm->arch.apicv_update_lock);
@@ -10071,6 +10442,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		 */
 		WARN_ON_ONCE(kvm_apicv_activated(vcpu->kvm) != kvm_vcpu_apicv_active(vcpu));
 
+		/*
+		 * vmx_vcpu_run()
+		 */
 		exit_fastpath = static_call(kvm_x86_run)(vcpu);
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
@@ -10175,6 +10549,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.apic_attention)
 		kvm_lapic_sync_from_vapic(vcpu);
 
+	/*
+	 * vmx_handle_exit()
+	 */
 	r = static_call(kvm_x86_handle_exit)(vcpu, exit_fastpath);
 	return r;
 
@@ -10290,6 +10667,11 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10340| <<complete_emulated_pio>> return complete_emulated_io(vcpu);
+ *   - arch/x86/kvm/x86.c|10393| <<complete_emulated_mmio>> return complete_emulated_io(vcpu);
+ */
 static inline int complete_emulated_io(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -10325,6 +10707,11 @@ static int complete_emulated_pio(struct kvm_vcpu *vcpu)
  *       copy data
  *       exit
  */
+/*
+ * 在以下使用complete_emulated_mmio():
+ *   - arch/x86/kvm/x86.c|8417| <<x86_emulate_instruction>> vcpu->arch.complete_userspace_io = complete_emulated_mmio;
+ *   - arch/x86/kvm/x86.c|10445| <<complete_emulated_mmio>> vcpu->arch.complete_userspace_io = complete_emulated_mmio;
+ */
 static int complete_emulated_mmio(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *run = vcpu->run;
@@ -10389,6 +10776,10 @@ static void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 	trace_kvm_fpu(0);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4032| <<kvm_vcpu_ioctl>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *kvm_run = vcpu->run;
@@ -10450,6 +10841,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		}
 	}
 
+	/*
+	 * 比如complete_emulated_mmio()或complete_emulated_pio()或者更多
+	 */
 	if (unlikely(vcpu->arch.complete_userspace_io)) {
 		int (*cui)(struct kvm_vcpu *) = vcpu->arch.complete_userspace_io;
 		vcpu->arch.complete_userspace_io = NULL;
@@ -11062,6 +11456,10 @@ static void store_regs(struct kvm_vcpu *vcpu)
 				vcpu, &vcpu->run->s.regs.events);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10478| <<kvm_arch_vcpu_ioctl_run>> r = sync_regs(vcpu);
+ */
 static int sync_regs(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_REGS) {
@@ -11092,6 +11490,10 @@ int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3873| <<kvm_vm_ioctl_create_vcpu>> r = kvm_arch_vcpu_create(vcpu);
+ */
 int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 {
 	struct page *page;
@@ -11384,6 +11786,10 @@ void kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_deliver_sipi_vector);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5078| <<hardware_enable_nolock>> r = kvm_arch_hardware_enable();
+ */
 int kvm_arch_hardware_enable(void)
 {
 	struct kvm *kvm;
@@ -11481,6 +11887,10 @@ void kvm_arch_hardware_disable(void)
 	drop_user_return_notifiers();
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5855| <<kvm_init>> r = kvm_arch_hardware_setup(opaque);
+ */
 int kvm_arch_hardware_setup(void *opaque)
 {
 	struct kvm_x86_init_ops *ops = opaque;
@@ -11643,6 +12053,10 @@ static void kvm_unload_vcpu_mmu(struct kvm_vcpu *vcpu)
 	vcpu_put(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11782| <<kvm_arch_destroy_vm>> kvm_free_vcpus(kvm);
+ */
 static void kvm_free_vcpus(struct kvm *kvm)
 {
 	unsigned long i;
@@ -11688,6 +12102,16 @@ void kvm_arch_sync_events(struct kvm *kvm)
  * address, i.e. its accessibility is not guaranteed, and must be
  * accessed via __copy_{to,from}_user().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|220| <<avic_alloc_access_page>> ret = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|3626| <<init_rmode_identity_map>> uaddr = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|3675| <<alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/vmx/vmx.c|4728| <<vmx_set_tss_addr>> ret = __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,
+ *   - arch/x86/kvm/x86.c|11838| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|11840| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|11842| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);
+ */
 void __user * __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa,
 				      u32 size)
 {
@@ -11746,6 +12170,11 @@ void kvm_arch_pre_destroy_vm(struct kvm *kvm)
 	kvm_mmu_pre_destroy_vm(kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1150| <<kvm_create_vm>> kvm_arch_destroy_vm(kvm);
+ *   - virt/kvm/kvm_main.c|1217| <<kvm_destroy_vm>> kvm_arch_destroy_vm(kvm);
+ */
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	if (current->mm == kvm->mm) {
@@ -12180,6 +12609,10 @@ void kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 }
 EXPORT_SYMBOL_GPL(kvm_set_rflags);
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|149| <<kvm_check_async_pf_completion>> kvm_arch_async_page_ready(vcpu, work);
+ */
 void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 {
 	int r;
@@ -12499,8 +12932,19 @@ bool kvm_vector_hashing_enabled(void)
 	return vector_hashing;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3383| <<kvm_vcpu_halt>> bool halt_poll_allowed = !kvm_arch_no_poll(vcpu);
+ */
 bool kvm_arch_no_poll(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->msr_kvm_poll_control:
+	 *   - arch/x86/kvm/x86.c|3647| <<kvm_set_msr_common>> vcpu->arch.msr_kvm_poll_control = data;
+	 *   - arch/x86/kvm/x86.c|3988| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.msr_kvm_poll_control;
+	 *   - arch/x86/kvm/x86.c|11275| <<kvm_arch_vcpu_postcreate>> vcpu->arch.msr_kvm_poll_control = 1;
+	 *   - arch/x86/kvm/x86.c|12602| <<kvm_arch_no_poll>> return (vcpu->arch.msr_kvm_poll_control & 1) == 0;
+	 */
 	return (vcpu->arch.msr_kvm_poll_control & 1) == 0;
 }
 EXPORT_SYMBOL_GPL(kvm_arch_no_poll);
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 767ec7f99516..2405d30b143b 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -353,6 +353,12 @@ enum kvm_intr_type {
 	KVM_HANDLING_NMI,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3816| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|6520| <<handle_interrupt_nmi_irqoff>> kvm_before_interrupt(vcpu, is_nmi ? KVM_HANDLING_NMI : KVM_HANDLING_IRQ);
+ *   - arch/x86/kvm/x86.c|10448| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+ */
 static inline void kvm_before_interrupt(struct kvm_vcpu *vcpu,
 					enum kvm_intr_type intr)
 {
diff --git a/block/blk-map.c b/block/blk-map.c
index c7f71d83eff1..777f7cc6b90a 100644
--- a/block/blk-map.c
+++ b/block/blk-map.c
@@ -230,6 +230,10 @@ static int bio_copy_user_iov(struct request *rq, struct rq_map_data *map_data,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|564| <<blk_rq_map_user_iov>> ret = bio_map_user_iov(rq, &i, gfp_mask);
+ */
 static int bio_map_user_iov(struct request *rq, struct iov_iter *iter,
 		gfp_t gfp_mask)
 {
@@ -296,6 +300,11 @@ static int bio_map_user_iov(struct request *rq, struct iov_iter *iter,
 			break;
 	}
 
+	/*
+	 * 注释
+	 * Append a bio to a passthrough request.  Only works if the bio can be merged
+	 * into the request based on the driver constraints.
+	 */
 	ret = blk_rq_append_bio(rq, bio);
 	if (ret)
 		goto out_unmap;
@@ -479,6 +488,16 @@ static struct bio *bio_copy_kern(struct request_queue *q, void *data,
  * Append a bio to a passthrough request.  Only works if the bio can be merged
  * into the request based on the driver constraints.
  */
+/*
+ * called by:
+ *   - block/blk-map.c|504| <<global>> EXPORT_SYMBOL(blk_rq_append_bio);
+ *   - block/blk-map.c|220| <<bio_copy_user_iov>> ret = blk_rq_append_bio(rq, bio);
+ *   - block/blk-map.c|299| <<bio_map_user_iov>> ret = blk_rq_append_bio(rq, bio);
+ *   - block/blk-map.c|650| <<blk_rq_map_kern>> ret = blk_rq_append_bio(rq, bio);
+ *   - drivers/scsi/ufs/ufshpb.c|708| <<ufshpb_execute_map_req>> blk_rq_append_bio(req, map_req->bio);
+ *   - drivers/target/target_core_pscsi.c|912| <<pscsi_map_sg>> rc = blk_rq_append_bio(req, bio);
+ *   - drivers/target/target_core_pscsi.c|931| <<pscsi_map_sg>> rc = blk_rq_append_bio(req, bio);
+ */
 int blk_rq_append_bio(struct request *rq, struct bio *bio)
 {
 	struct bvec_iter iter;
@@ -518,6 +537,12 @@ EXPORT_SYMBOL(blk_rq_append_bio);
  *    A matching blk_rq_unmap_user() must be issued at the end of I/O, while
  *    still in process context.
  */
+/*
+ * called by:
+ *   - block/blk-map.c|576| <<blk_rq_map_user>> return blk_rq_map_user_iov(q, rq, map_data, &i, gfp_mask);
+ *   - drivers/scsi/scsi_ioctl.c|476| <<sg_io>> ret = blk_rq_map_user_iov(rq->q, rq, NULL, &i, GFP_KERNEL);
+ *   - drivers/scsi/sg.c|1833| <<sg_start_req>> res = blk_rq_map_user_iov(q, rq, md, &i, GFP_ATOMIC);
+ */
 int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 			struct rq_map_data *map_data,
 			const struct iov_iter *iter, gfp_t gfp_mask)
@@ -541,6 +566,9 @@ int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 		copy = queue_virt_boundary(q) & iov_iter_gap_alignment(iter);
 
 	i = *iter;
+	/*
+	 * 这里是个循环啊!!!
+	 */
 	do {
 		if (copy)
 			ret = bio_copy_user_iov(rq, map_data, &i, gfp_mask);
@@ -562,6 +590,19 @@ int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 }
 EXPORT_SYMBOL(blk_rq_map_user_iov);
 
+/*
+ * called by:
+ *   - block/bsg-lib.c|70| <<bsg_transport_sg_io_fn>> ret = blk_rq_map_user(rq->q, job->bidi_rq, NULL,
+ *   - block/bsg-lib.c|84| <<bsg_transport_sg_io_fn>> ret = blk_rq_map_user(rq->q, rq, NULL, uptr64(hdr->dout_xferp),
+ *   - block/bsg-lib.c|87| <<bsg_transport_sg_io_fn>> ret = blk_rq_map_user(rq->q, rq, NULL, uptr64(hdr->din_xferp),
+ *   - drivers/nvme/host/ioctl.c|78| <<nvme_submit_user_cmd>> ret = blk_rq_map_user(q, req, NULL, ubuffer, bufflen,
+ *   - drivers/scsi/scsi_bsg.c|52| <<scsi_bsg_sg_io_fn>> ret = blk_rq_map_user(rq->q, rq, NULL, uptr64(hdr->dout_xferp),
+ *   - drivers/scsi/scsi_bsg.c|55| <<scsi_bsg_sg_io_fn>> ret = blk_rq_map_user(rq->q, rq, NULL, uptr64(hdr->din_xferp),
+ *   - drivers/scsi/scsi_ioctl.c|479| <<sg_io>> ret = blk_rq_map_user(rq->q, rq, NULL, hdr->dxferp,
+ *   - drivers/scsi/sg.c|1836| <<sg_start_req>> res = blk_rq_map_user(q, rq, md, hp->dxferp,
+ *   - drivers/scsi/sr.c|978| <<sr_read_cdda_bpc>> ret = blk_rq_map_user(disk->queue, rq, NULL, ubuf, len, GFP_KERNEL);
+ *   - drivers/scsi/st.c|557| <<st_scsi_execute>> err = blk_rq_map_user(req->q, req, mdata, NULL, bufflen,
+ */
 int blk_rq_map_user(struct request_queue *q, struct request *rq,
 		    struct rq_map_data *map_data, void __user *ubuf,
 		    unsigned long len, gfp_t gfp_mask)
@@ -573,6 +614,12 @@ int blk_rq_map_user(struct request_queue *q, struct request *rq,
 	if (unlikely(ret < 0))
 		return ret;
 
+	/*
+	 * called by:
+	 *   - block/blk-map.c|576| <<blk_rq_map_user>> return blk_rq_map_user_iov(q, rq, map_data, &i, gfp_mask);
+	 *   - drivers/scsi/scsi_ioctl.c|476| <<sg_io>> ret = blk_rq_map_user_iov(rq->q, rq, NULL, &i, GFP_KERNEL);
+	 *   - drivers/scsi/sg.c|1833| <<sg_start_req>> res = blk_rq_map_user_iov(q, rq, md, &i, GFP_ATOMIC);
+	 */
 	return blk_rq_map_user_iov(q, rq, map_data, &i, gfp_mask);
 }
 EXPORT_SYMBOL(blk_rq_map_user);
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 4de34a332c9f..47691ac97d04 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -597,6 +597,11 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq,
 			blk_queue_get_max_sectors(q, req_op(rq)));
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|640| <<ll_back_merge_fn>> return ll_new_hw_segment(req, bio, nr_segs);
+ *   - block/blk-merge.c|659| <<ll_front_merge_fn>> return ll_new_hw_segment(req, bio, nr_segs);
+ */
 static inline int ll_new_hw_segment(struct request *req, struct bio *bio,
 		unsigned int nr_phys_segs)
 {
@@ -622,6 +627,11 @@ static inline int ll_new_hw_segment(struct request *req, struct bio *bio,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|494| <<blk_rq_append_bio>> if (!ll_back_merge_fn(rq, bio, nr_segs))
+ *   - block/blk-merge.c|965| <<bio_attempt_back_merge>> if (!ll_back_merge_fn(req, bio, nr_segs))
+ */
 int ll_back_merge_fn(struct request *req, struct bio *bio, unsigned int nr_segs)
 {
 	if (req_gap_back_merge(req, bio))
diff --git a/block/blk-settings.c b/block/blk-settings.c
index b880c70e22e4..a7e91f0669fd 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -239,6 +239,46 @@ EXPORT_SYMBOL_GPL(blk_queue_max_zone_append_sectors);
  *    Enables a low level driver to set an upper limit on the number of
  *    hw data segments in a request.
  **/
+/*
+ * [0] blk_queue_max_segments
+ * [0] __scsi_init_queue
+ * [0] scsi_alloc_sdev
+ * [0] scsi_probe_and_add_lun
+ * [0] __scsi_add_device
+ * [0] scsi_add_device
+ * [0] tcm_loop_port_link
+ * [0] target_fabric_port_link
+ * [0] configfs_symlink
+ * [0] vfs_symlink
+ * [0] do_symlinkat
+ * [0] __x64_sys_symlink
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * blk_queue_max_segments
+ * [0] __scsi_init_queue
+ * [0] scsi_alloc_sdev
+ * [0] scsi_probe_and_add_lun
+ * [0] __scsi_scan_target
+ * [0] scsi_scan_channel
+ * [0] scsi_scan_host_selected
+ * [0] scsi_scan_host
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] do_init_module
+ * [0] __do_sys_finit_module
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 void blk_queue_max_segments(struct request_queue *q, unsigned short max_segments)
 {
 	if (!max_segments) {
diff --git a/drivers/acpi/acpi_processor.c b/drivers/acpi/acpi_processor.c
index 6737b1cbf6d6..bccb679b036a 100644
--- a/drivers/acpi/acpi_processor.c
+++ b/drivers/acpi/acpi_processor.c
@@ -168,6 +168,10 @@ int __weak arch_register_cpu(int cpu)
 
 void __weak arch_unregister_cpu(int cpu) {}
 
+/*
+ * called by:
+ *   - drivers/acpi/acpi_processor.c|302| <<acpi_processor_get_info>> int ret = acpi_processor_hotadd_init(pr);
+ */
 static int acpi_processor_hotadd_init(struct acpi_processor *pr)
 {
 	unsigned long long sta;
@@ -214,6 +218,10 @@ static inline int acpi_processor_hotadd_init(struct acpi_processor *pr)
 }
 #endif /* CONFIG_ACPI_HOTPLUG_CPU */
 
+/*
+ * called by:
+ *   - drivers/acpi/acpi_processor.c|374| <<acpi_processor_add>> result = acpi_processor_get_info(device);
+ */
 static int acpi_processor_get_info(struct acpi_device *device)
 {
 	union acpi_object object = { 0 };
@@ -350,6 +358,9 @@ static int acpi_processor_get_info(struct acpi_device *device)
  */
 static DEFINE_PER_CPU(void *, processor_device_array);
 
+/*
+ * struct acpi_scan_handler processor_handler.attach = acpi_processor_add()
+ */
 static int acpi_processor_add(struct acpi_device *device,
 					const struct acpi_device_id *id)
 {
diff --git a/drivers/acpi/acpica/evmisc.c b/drivers/acpi/acpica/evmisc.c
index f14ebcd610ab..2b6c2537071e 100644
--- a/drivers/acpi/acpica/evmisc.c
+++ b/drivers/acpi/acpica/evmisc.c
@@ -158,6 +158,10 @@ acpi_ev_queue_notify_request(struct acpi_namespace_node *node, u32 notify_value)
  *
  ******************************************************************************/
 
+/*
+ * called by:
+ *   - drivers/acpi/acpica/evmisc.c|140| <<acpi_ev_queue_notify_request>> acpi_ev_notify_dispatch, info);
+ */
 static void ACPI_SYSTEM_XFACE acpi_ev_notify_dispatch(void *context)
 {
 	union acpi_generic_state *info = (union acpi_generic_state *)context;
diff --git a/drivers/acpi/bus.c b/drivers/acpi/bus.c
index 07f604832fd6..5ff640a5e333 100644
--- a/drivers/acpi/bus.c
+++ b/drivers/acpi/bus.c
@@ -424,6 +424,10 @@ static void acpi_bus_osc_negotiate_usb_control(void)
  * ---------------
  * Callback for all 'system-level' device notifications (values 0x00-0x7F).
  */
+/*
+ * 在以下使用acpi_bus_notify():
+ *   - drivers/acpi/bus.c|1286| <<acpi_bus_init>> acpi_install_notify_handler(ACPI_ROOT_OBJECT, ACPI_SYSTEM_NOTIFY, &acpi_bus_notify, NULL);
+ */
 static void acpi_bus_notify(acpi_handle handle, u32 type, void *data)
 {
 	struct acpi_device *adev;
diff --git a/drivers/acpi/osl.c b/drivers/acpi/osl.c
index 45c5c0e45e33..881ca410e00f 100644
--- a/drivers/acpi/osl.c
+++ b/drivers/acpi/osl.c
@@ -1058,6 +1058,18 @@ int __init acpi_debugger_init(void)
  *
  ******************************************************************************/
 
+/*
+ * called by:
+ *   - drivers/acpi/acpica/dbexec.c|695| <<acpi_db_create_execution_thread>> status = acpi_os_execute(OSL_DEBUGGER_EXEC_THREAD,
+ *   - drivers/acpi/acpica/dbexec.c|849| <<acpi_db_create_execution_threads>> acpi_os_execute(OSL_DEBUGGER_EXEC_THREAD,
+ *   - drivers/acpi/acpica/dbxface.c|448| <<acpi_initialize_debugger>> status = acpi_os_execute(OSL_DEBUGGER_MAIN_THREAD,
+ *   - drivers/acpi/acpica/evgpe.c|526| <<acpi_ev_asynch_execute_gpe_method>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/acpi/acpica/evgpe.c|823| <<acpi_ev_gpe_dispatch>> status = acpi_os_execute(OSL_GPE_HANDLER,
+ *   - drivers/acpi/acpica/evmisc.c|139| <<acpi_ev_queue_notify_request>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/acpi/bus.c|522| <<acpi_device_fixed_event>> acpi_os_execute(OSL_NOTIFY_HANDLER, acpi_notify_device_fixed, data);
+ *   - drivers/acpi/sbshc.c|232| <<smbus_alarm>> acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/platform/x86/dell/dell-rbtn.c|278| <<rbtn_resume>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ */
 acpi_status acpi_os_execute(acpi_execute_type type,
 			    acpi_osd_exec_callback function, void *context)
 {
@@ -1161,6 +1173,11 @@ static void acpi_hotplug_work_fn(struct work_struct *work)
 	kfree(hpw);
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/bus.c|492| <<acpi_bus_notify>> if (ACPI_SUCCESS(acpi_hotplug_schedule(adev, type)))
+ *   - drivers/acpi/device_sysfs.c|387| <<eject_store>> status = acpi_hotplug_schedule(acpi_device, ACPI_OST_EC_OSPM_EJECT);
+ */
 acpi_status acpi_hotplug_schedule(struct acpi_device *adev, u32 src)
 {
 	struct acpi_hp_work *hpw;
diff --git a/drivers/acpi/scan.c b/drivers/acpi/scan.c
index 8b2e5ef15559..d473a8c263a2 100644
--- a/drivers/acpi/scan.c
+++ b/drivers/acpi/scan.c
@@ -133,6 +133,13 @@ bool acpi_scan_is_offline(struct acpi_device *adev, bool uevent)
 	return offline;
 }
 
+/*
+ * 在以下使用acpi_bus_offline():
+ *   - drivers/acpi/scan.c|219| <<acpi_scan_try_to_offline>> NULL, acpi_bus_offline, (void *)false,
+ *   - drivers/acpi/scan.c|227| <<acpi_scan_try_to_offline>> acpi_bus_offline(handle, 0, (void *)false, (void **)&errdev);
+ *   - drivers/acpi/scan.c|231| <<acpi_scan_try_to_offline>> NULL, acpi_bus_offline, (void *)true,
+ *   - drivers/acpi/scan.c|234| <<acpi_scan_try_to_offline>> acpi_bus_offline(handle, 0, (void *)true,
+ */
 static acpi_status acpi_bus_offline(acpi_handle handle, u32 lvl, void *data,
 				    void **ret_p)
 {
@@ -200,6 +207,10 @@ static acpi_status acpi_bus_online(acpi_handle handle, u32 lvl, void *data,
 	return AE_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|259| <<acpi_scan_hot_remove>> int error = acpi_scan_try_to_offline(device);
+ */
 static int acpi_scan_try_to_offline(struct acpi_device *device)
 {
 	acpi_handle handle = device->handle;
@@ -246,6 +257,10 @@ static int acpi_scan_try_to_offline(struct acpi_device *device)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|379| <<acpi_generic_hotplug_event>> return acpi_scan_hot_remove(adev);
+ */
 static int acpi_scan_hot_remove(struct acpi_device *device)
 {
 	acpi_handle handle = device->handle;
@@ -361,6 +376,10 @@ static int acpi_scan_bus_check(struct acpi_device *adev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|407| <<acpi_device_hotplug>> error = acpi_generic_hotplug_event(adev, src);
+ */
 static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 {
 	switch (type) {
@@ -381,6 +400,10 @@ static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/osl.c|1160| <<acpi_hotplug_work_fn>> acpi_device_hotplug(hpw->adev, hpw->src);
+ */
 void acpi_device_hotplug(struct acpi_device *adev, u32 src)
 {
 	u32 ost_code = ACPI_OST_SC_NON_SPECIFIC_FAILURE;
@@ -2161,6 +2184,10 @@ static struct acpi_scan_handler generic_device_handler = {
 	.attach = acpi_generic_device_attach,
 };
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|2228| <<acpi_bus_attach>> ret = acpi_scan_attach_handler(device);
+ */
 static int acpi_scan_attach_handler(struct acpi_device *device)
 {
 	struct acpi_hardware_id *hwid;
@@ -2190,6 +2217,13 @@ static int acpi_scan_attach_handler(struct acpi_device *device)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|2249| <<acpi_bus_attach>> acpi_bus_attach(child, first_pass);
+ *   - drivers/acpi/scan.c|2280| <<acpi_scan_clear_dep_fn>> acpi_bus_attach(cdw->adev, true);
+ *   - drivers/acpi/scan.c|2438| <<acpi_bus_scan>> acpi_bus_attach(device, true);
+ *   - drivers/acpi/scan.c|2452| <<acpi_bus_scan>> acpi_bus_attach(device, false);
+ */
 static void acpi_bus_attach(struct acpi_device *device, bool first_pass)
 {
 	struct acpi_device *child;
@@ -2419,6 +2453,17 @@ EXPORT_SYMBOL_GPL(acpi_dev_get_first_consumer_dev);
  *
  * Must be called under acpi_scan_lock.
  */
+/*
+ * called by:
+ *   - drivers/acpi/dock.c|273| <<hotplug_dock_devices>> int ret = acpi_bus_scan(adev->handle);
+ *   - drivers/acpi/scan.c|322| <<acpi_scan_device_check>> error = acpi_bus_scan(adev->handle);
+ *   - drivers/acpi/scan.c|351| <<acpi_scan_bus_check>> error = acpi_bus_scan(adev->handle);
+ *   - drivers/acpi/scan.c|2603| <<acpi_scan_init>> if (acpi_bus_scan(ACPI_ROOT_OBJECT))
+ *   - drivers/acpi/scan.c|2664| <<acpi_table_events_fn>> acpi_bus_scan(ACPI_ROOT_OBJECT);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|428| <<acpiphp_rescan_slot>> acpi_bus_scan(adev->handle);
+ *   - drivers/platform/surface/surface3-wmi.c|232| <<s3_wmi_probe>> acpi_bus_scan(s3_wmi.pnp0c0d_adev->handle);
+ *   - drivers/platform/surface/surface3-wmi.c|242| <<s3_wmi_remove>> acpi_bus_scan(s3_wmi.pnp0c0d_adev->handle);
+ */
 int acpi_bus_scan(acpi_handle handle)
 {
 	struct acpi_device *device = NULL;
diff --git a/drivers/cpuidle/cpuidle-haltpoll.c b/drivers/cpuidle/cpuidle-haltpoll.c
index fcc53215bac8..83b36331c94a 100644
--- a/drivers/cpuidle/cpuidle-haltpoll.c
+++ b/drivers/cpuidle/cpuidle-haltpoll.c
@@ -22,6 +22,16 @@ static bool force __read_mostly;
 module_param(force, bool, 0444);
 MODULE_PARM_DESC(force, "Load unconditionally");
 
+/*
+ * 在以下使用percpu的haltpoll_cpuidle_devices:
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|25| <<global>> static struct cpuidle_device __percpu *haltpoll_cpuidle_devices;
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|61| <<haltpoll_cpu_online>> dev = per_cpu_ptr(haltpoll_cpuidle_devices, cpu);
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|78| <<haltpoll_cpu_offline>> dev = per_cpu_ptr(haltpoll_cpuidle_devices, cpu);
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|93| <<haltpoll_uninit>> free_percpu(haltpoll_cpuidle_devices);
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|94| <<haltpoll_uninit>> haltpoll_cpuidle_devices = NULL;
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|149| <<haltpoll_init>> haltpoll_cpuidle_devices = alloc_percpu(struct cpuidle_device);
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|150| <<haltpoll_init>> if (haltpoll_cpuidle_devices == NULL) {
+ */
 static struct cpuidle_device __percpu *haltpoll_cpuidle_devices;
 static enum cpuhp_state haltpoll_hp_state;
 
@@ -36,6 +46,11 @@ static int default_enter_idle(struct cpuidle_device *dev,
 	return index;
 }
 
+/*
+ * struct cpuidle_driver haltpoll_driver:
+ * -> struct cpuidle_state    states[CPUIDLE_STATE_MAX];
+ * -> int                     state_count;
+ */
 static struct cpuidle_driver haltpoll_driver = {
 	.name = "haltpoll",
 	.governor = "haltpoll",
@@ -99,12 +114,41 @@ static bool haltpoll_want(void)
 	return kvm_para_has_hint(KVM_HINTS_REALTIME) || force;
 }
 
+/*
+ * The x86 architecture support code recognizes three kernel command line
+ * options related to CPU idle time management: idle=poll, idle=halt, and
+ * idle=nomwait. The first two of them disable the acpi_idle and intel_idle
+ * drivers altogether, which effectively causes the entire CPUIdle subsystem to
+ * be disabled and makes the idle loop invoke the architecture support code to
+ * deal with idle CPUs. How it does that depends on which of the two parameters
+ * is added to the kernel command line. In the idle=halt case, the architecture
+ * support code will use the HLT instruction of the CPUs (which, as a rule,
+ * suspends the execution of the program and causes the hardware to attempt to
+ * enter the shallowest available idle state) for this purpose, and if
+ * idle=poll is used, idle CPUs will execute a more or less lightweight''
+ * sequence of instructions in a tight loop.  [Note that using ``idle=poll is
+ * somewhat drastic in many cases, as preventing idle CPUs from saving almost
+ * any energy at all may not be the only effect of it. For example, on Intel
+ * hardware it effectively prevents CPUs from using P-states (see CPU
+ * Performance Scaling) that require any number of CPUs in a package to be
+ * idle, so it very well may hurt single-thread computations performance as
+ * well as energy-efficiency. Thus using it for performance reasons may not be
+ * a good idea at all.
+ *
+ * The idle=nomwait option disables the intel_idle driver and causes acpi_idle
+ * to be used (as long as all of the information needed by it is there in the
+ * system’s ACPI tables), but it is not allowed to use the MWAIT instruction of
+ * the CPUs to ask the hardware to enter idle states.
+ */
 static int __init haltpoll_init(void)
 {
 	int ret;
 	struct cpuidle_driver *drv = &haltpoll_driver;
 
 	/* Do not load haltpoll if idle= is passed */
+	/*
+	 * IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT, IDLE_POLL
+	 */
 	if (boot_option_idle_override != IDLE_NO_OVERRIDE)
 		return -ENODEV;
 
diff --git a/drivers/cpuidle/cpuidle.c b/drivers/cpuidle/cpuidle.c
index ef2ea1b12cd8..fd70b8a8e331 100644
--- a/drivers/cpuidle/cpuidle.c
+++ b/drivers/cpuidle/cpuidle.c
@@ -197,6 +197,13 @@ int cpuidle_enter_s2idle(struct cpuidle_driver *drv, struct cpuidle_device *dev)
  * @drv: cpuidle driver for this cpu
  * @index: index into the states table in @drv of the state to enter
  */
+/*
+ * called by:
+ *   - drivers/cpuidle/coupled.c|486| <<cpuidle_enter_state_coupled>> entered_state = cpuidle_enter_state(dev, drv,
+ *   - drivers/cpuidle/coupled.c|534| <<cpuidle_enter_state_coupled>> entered_state = cpuidle_enter_state(dev, drv,
+ *   - drivers/cpuidle/coupled.c|595| <<cpuidle_enter_state_coupled>> entered_state = cpuidle_enter_state(dev, drv, next_state);
+ *   - drivers/cpuidle/cpuidle.c|351| <<cpuidle_enter>> ret = cpuidle_enter_state(dev, drv, index);
+ */
 int cpuidle_enter_state(struct cpuidle_device *dev, struct cpuidle_driver *drv,
 			int index)
 {
@@ -234,6 +241,27 @@ int cpuidle_enter_state(struct cpuidle_device *dev, struct cpuidle_driver *drv,
 	stop_critical_timings();
 	if (!(target_state->flags & CPUIDLE_FLAG_RCU_IDLE))
 		rcu_idle_enter();
+	/*
+	 * haltpoll_driver
+	 *
+	 *  44 static struct cpuidle_driver haltpoll_driver = {
+	 *  45         .name = "haltpoll",
+	 *  46         .governor = "haltpoll",
+	 *  47         .states = {
+	 *  48                 { // entry 0 is for polling  },
+	 *  49                 {
+	 *  50                         .enter                  = default_enter_idle,
+	 *  51                         .exit_latency           = 1,
+	 *  52                         .target_residency       = 1,
+	 *  53                         .power_usage            = -1,
+	 *  54                         .name                   = "haltpoll idle",
+	 *  55                         .desc                   = "default architecture idle",
+	 *  56                 },
+	 *  57         },
+	 *  58         .safe_state_index = 0,
+	 *  59         .state_count = 2,
+	 *  60 };
+	 */
 	entered_state = target_state->enter(dev, drv, index);
 	if (!(target_state->flags & CPUIDLE_FLAG_RCU_IDLE))
 		rcu_idle_exit();
@@ -257,6 +285,9 @@ int cpuidle_enter_state(struct cpuidle_device *dev, struct cpuidle_driver *drv,
 		local_irq_enable();
 
 	if (entered_state >= 0) {
+		/*
+		 * haltpoll_driver
+		 */
 		s64 diff, delay = drv->states[entered_state].exit_latency_ns;
 		int i;
 
@@ -316,9 +347,16 @@ int cpuidle_enter_state(struct cpuidle_device *dev, struct cpuidle_driver *drv,
  * 'false' boolean value if the scheduler tick should not be stopped before
  * entering the returned state.
  */
+/*
+ * called by:
+ *   - kernel/sched/idle.c|232| <<cpuidle_idle_call>> next_state = cpuidle_select(drv, dev, &stop_tick);
+ */
 int cpuidle_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		   bool *stop_tick)
 {
+	/*
+	 * haltpoll_governor
+	 */
 	return cpuidle_curr_governor->select(drv, dev, stop_tick);
 }
 
@@ -389,6 +427,10 @@ void cpuidle_reflect(struct cpuidle_device *dev, int index)
  * @dev:   the cpuidle device
  *
  */
+/*
+ * called by:
+ *   - drivers/cpuidle/poll_state.c|25| <<poll_idle>> limit = cpuidle_poll_time(drv, dev);
+ */
 u64 cpuidle_poll_time(struct cpuidle_driver *drv,
 		      struct cpuidle_device *dev)
 {
diff --git a/drivers/cpuidle/governors/haltpoll.c b/drivers/cpuidle/governors/haltpoll.c
index cb2a96eafc02..9b0977b955d5 100644
--- a/drivers/cpuidle/governors/haltpoll.c
+++ b/drivers/cpuidle/governors/haltpoll.c
@@ -20,22 +20,56 @@
 #include <linux/module.h>
 #include <linux/kvm_para.h>
 
+/*
+ * 在以下使用guest_halt_poll_ns:
+ *   - drivers/cpuidle/governors/haltpoll.c|23| <<global>> static unsigned int guest_halt_poll_ns __read_mostly = 200000;
+ *   - drivers/cpuidle/governors/haltpoll.c|24| <<global>> module_param(guest_halt_poll_ns, uint, 0644);
+ *   - drivers/cpuidle/governors/haltpoll.c|85| <<adjust_poll_limit>> if (block_ns > dev->poll_limit_ns && block_ns <= guest_halt_poll_ns) {
+ *   - drivers/cpuidle/governors/haltpoll.c|90| <<adjust_poll_limit>> if (val > guest_halt_poll_ns)
+ *   - drivers/cpuidle/governors/haltpoll.c|91| <<adjust_poll_limit>> val = guest_halt_poll_ns;
+ *   - drivers/cpuidle/governors/haltpoll.c|94| <<adjust_poll_limit>> } else if (block_ns > guest_halt_poll_ns &&
+ */
 static unsigned int guest_halt_poll_ns __read_mostly = 200000;
 module_param(guest_halt_poll_ns, uint, 0644);
 
 /* division factor to shrink halt_poll_ns */
+/*
+ * 在以下使用guest_halt_poll_shrink:
+ *   - drivers/cpuidle/governors/haltpoll.c|27| <<global>> static unsigned int guest_halt_poll_shrink __read_mostly = 2;
+ *   - drivers/cpuidle/governors/haltpoll.c|28| <<global>> module_param(guest_halt_poll_shrink, uint, 0644);
+ *   - drivers/cpuidle/governors/haltpoll.c|96| <<adjust_poll_limit>> unsigned int shrink = guest_halt_poll_shrink;
+ */
 static unsigned int guest_halt_poll_shrink __read_mostly = 2;
 module_param(guest_halt_poll_shrink, uint, 0644);
 
 /* multiplication factor to grow per-cpu poll_limit_ns */
+/*
+ * 在以下使用guest_halt_poll_grow:
+ *   - drivers/cpuidle/governors/haltpoll.c|31| <<global>> static unsigned int guest_halt_poll_grow __read_mostly = 2;
+ *   - drivers/cpuidle/governors/haltpoll.c|32| <<global>> module_param(guest_halt_poll_grow, uint, 0644);
+ *   - drivers/cpuidle/governors/haltpoll.c|86| <<adjust_poll_limit>> val = dev->poll_limit_ns * guest_halt_poll_grow;
+ */
 static unsigned int guest_halt_poll_grow __read_mostly = 2;
 module_param(guest_halt_poll_grow, uint, 0644);
 
 /* value in us to start growing per-cpu halt_poll_ns */
+/*
+ * 在以下使用guest_halt_poll_grow_start:
+ *   - drivers/cpuidle/governors/haltpoll.c|35| <<global>> static unsigned int guest_halt_poll_grow_start __read_mostly = 50000;
+ *   - drivers/cpuidle/governors/haltpoll.c|36| <<global>> module_param(guest_halt_poll_grow_start, uint, 0644);
+ *   - drivers/cpuidle/governors/haltpoll.c|88| <<adjust_poll_limit>> if (val < guest_halt_poll_grow_start)
+ *   - drivers/cpuidle/governors/haltpoll.c|89| <<adjust_poll_limit>> val = guest_halt_poll_grow_start;
+ */
 static unsigned int guest_halt_poll_grow_start __read_mostly = 50000;
 module_param(guest_halt_poll_grow_start, uint, 0644);
 
 /* allow shrinking guest halt poll */
+/*
+ * 在以下使用guest_halt_poll_allow_shrink:
+ *   - drivers/cpuidle/governors/haltpoll.c|39| <<global>> static bool guest_halt_poll_allow_shrink __read_mostly = true;
+ *   - drivers/cpuidle/governors/haltpoll.c|40| <<global>> module_param(guest_halt_poll_allow_shrink, bool, 0644);
+ *   - drivers/cpuidle/governors/haltpoll.c|95| <<adjust_poll_limit>> guest_halt_poll_allow_shrink) {
+ */
 static bool guest_halt_poll_allow_shrink __read_mostly = true;
 module_param(guest_halt_poll_allow_shrink, bool, 0644);
 
@@ -75,6 +109,16 @@ static int haltpoll_select(struct cpuidle_driver *drv,
 	return 0;
 }
 
+/*
+ * [0] adjust_poll_limit
+ * [0] haltpoll_reflect
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] secondary_startup_64_no_verify
+ *
+ * called by:
+ *   - drivers/cpuidle/governors/haltpoll.c|117| <<haltpoll_reflect>> adjust_poll_limit(dev, dev->last_residency_ns);
+ */
 static void adjust_poll_limit(struct cpuidle_device *dev, u64 block_ns)
 {
 	unsigned int val;
@@ -113,6 +157,18 @@ static void haltpoll_reflect(struct cpuidle_device *dev, int index)
 {
 	dev->last_state_idx = index;
 
+	/*
+	 * 在以下设置修改cpuidle_device->last_residency_ns:
+	 *   - drivers/cpuidle/cpuidle.c|277| <<cpuidle_enter_state>> dev->last_residency_ns = diff;
+	 *   - drivers/cpuidle/cpuidle.c|306| <<cpuidle_enter_state>> dev->last_residency_ns = 0;
+	 *   - kernel/sched/idle.c|148| <<call_cpuidle>> dev->last_residency_ns = 0;
+	 *   - drivers/cpuidle/cpuidle.c|597| <<__cpuidle_device_init>> dev->last_residency_ns = 0;
+	 * 在以下使用cpuidle_device->last_residency_ns:
+	 *   - drivers/cpuidle/governors/haltpoll.c|155| <<haltpoll_reflect>> adjust_poll_limit(dev, dev->last_residency_ns);
+	 *   - drivers/cpuidle/governors/ladder.c|84| <<ladder_select_state>> last_residency = dev->last_residency_ns - drv->states[last_idx].exit_latency_ns;
+	 *   - drivers/cpuidle/governors/menu.c|496| <<menu_update>> measured_ns = dev->last_residency_ns;
+	 *   - drivers/cpuidle/governors/teo.c|183| <<teo_update>> measured_ns = dev->last_residency_ns;
+	 */
 	if (index != 0)
 		adjust_poll_limit(dev, dev->last_residency_ns);
 }
diff --git a/drivers/cpuidle/poll_state.c b/drivers/cpuidle/poll_state.c
index f7e83613ae94..5e5b9f04df7b 100644
--- a/drivers/cpuidle/poll_state.c
+++ b/drivers/cpuidle/poll_state.c
@@ -10,6 +10,17 @@
 
 #define POLL_IDLE_RELAX_COUNT	200
 
+/*
+ * [0] poll_idle
+ * [0] cpuidle_enter_state
+ * [0] cpuidle_enter
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] secondary_startup_64_no_verify
+ *
+ * 在以下使用poll_idle():
+ *   - drivers/cpuidle/poll_state.c|55| <<cpuidle_poll_state_init>> state->enter = poll_idle;
+ */
 static int __cpuidle poll_idle(struct cpuidle_device *dev,
 			       struct cpuidle_driver *drv, int index)
 {
diff --git a/drivers/scsi/scsi_ioctl.c b/drivers/scsi/scsi_ioctl.c
index e13fd380deb6..d11a840334a6 100644
--- a/drivers/scsi/scsi_ioctl.c
+++ b/drivers/scsi/scsi_ioctl.c
@@ -408,6 +408,11 @@ static int scsi_complete_sghdr_rq(struct request *rq, struct sg_io_hdr *hdr,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_ioctl.c|847| <<scsi_cdrom_send_packet>> err = sg_io(sdev, &hdr, mode);
+ *   - drivers/scsi/scsi_ioctl.c|871| <<scsi_ioctl_sg_io>> error = sg_io(sdev, &hdr, mode);
+ */
 static int sg_io(struct scsi_device *sdev, struct sg_io_hdr *hdr, fmode_t mode)
 {
 	unsigned long start_time;
@@ -445,6 +450,11 @@ static int sg_io(struct scsi_device *sdev, struct sg_io_hdr *hdr, fmode_t mode)
 		return PTR_ERR(rq);
 	req = scsi_req(rq);
 
+	/*
+	 * struct sg_io_hdr *hdr:
+	 * -> unsigned char cmd_len; // [i] SCSI command length
+	 * -> unsigned short iovec_count; // [i] 0 implies no scatter gather
+	 */
 	if (hdr->cmd_len > BLK_MAX_CDB) {
 		req->cmd = kzalloc(hdr->cmd_len, GFP_KERNEL);
 		if (!req->cmd)
diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c
index 9394aa9444c1..9a2b3a09282a 100644
--- a/drivers/vfio/vfio_iommu_type1.c
+++ b/drivers/vfio/vfio_iommu_type1.c
@@ -412,6 +412,16 @@ static int vfio_iova_put_vfio_pfn(struct vfio_dma *dma, struct vfio_pfn *vpfn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|735| <<vfio_pin_pages_remote>> ret = vfio_lock_acct(dma, lock_acct, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|773| <<vfio_unpin_pages_remote>> vfio_lock_acct(dma, locked - unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|796| <<vfio_pin_page_external>> ret = vfio_lock_acct(dma, 1, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|824| <<vfio_unpin_page_external>> vfio_lock_acct(dma, -unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|911| <<vfio_iommu_type1_pin_pages>> vfio_lock_acct(dma, -1, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|1154| <<vfio_unmap_unpin>> vfio_lock_acct(dma, -unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|2371| <<vfio_iommu_unmap_unpin_reaccount>> vfio_lock_acct(dma, locked - unlocked, true);
+ */
 static int vfio_lock_acct(struct vfio_dma *dma, long npage, bool async)
 {
 	struct mm_struct *mm;
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index 47aebd98f52f..aabc084ca0dc 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -152,6 +152,11 @@ static const char *xen_swiotlb_error(enum xen_swiotlb_err err)
 	return "";
 }
 
+/*
+ * called by:
+ *   - arch/arm/xen/mm.c|146| <<xen_mm_init>> rc = xen_swiotlb_init();
+ *   - arch/x86/xen/pci-swiotlb-xen.c|79| <<pci_xen_swiotlb_init_late>> rc = xen_swiotlb_init();
+ */
 int xen_swiotlb_init(void)
 {
 	enum xen_swiotlb_err m_ret = XEN_SWIOTLB_UNKNOWN;
diff --git a/include/linux/cpuidle.h b/include/linux/cpuidle.h
index fce476275e16..4a5c84865357 100644
--- a/include/linux/cpuidle.h
+++ b/include/linux/cpuidle.h
@@ -97,7 +97,35 @@ struct cpuidle_device {
 	ktime_t			next_hrtimer;
 
 	int			last_state_idx;
+	/*
+	 * 在以下设置修改cpuidle_device->last_residency_ns:
+	 *   - drivers/cpuidle/cpuidle.c|277| <<cpuidle_enter_state>> dev->last_residency_ns = diff;
+	 *   - drivers/cpuidle/cpuidle.c|306| <<cpuidle_enter_state>> dev->last_residency_ns = 0;
+	 *   - kernel/sched/idle.c|148| <<call_cpuidle>> dev->last_residency_ns = 0;
+	 *   - drivers/cpuidle/cpuidle.c|597| <<__cpuidle_device_init>> dev->last_residency_ns = 0;
+	 * 在以下使用cpuidle_device->last_residency_ns:
+	 *   - drivers/cpuidle/governors/haltpoll.c|155| <<haltpoll_reflect>> adjust_poll_limit(dev, dev->last_residency_ns);
+	 *   - drivers/cpuidle/governors/ladder.c|84| <<ladder_select_state>> last_residency = dev->last_residency_ns - drv->states[last_idx].exit_latency_ns;
+	 *   - drivers/cpuidle/governors/menu.c|496| <<menu_update>> measured_ns = dev->last_residency_ns;
+	 *   - drivers/cpuidle/governors/teo.c|183| <<teo_update>> measured_ns = dev->last_residency_ns;
+	 */
 	u64			last_residency_ns;
+	/*
+	 * 在以下设置修改cpuidle_device->poll_limit_ns:
+	 *   - drivers/cpuidle/cpuidle.c|418| <<cpuidle_poll_time>> dev->poll_limit_ns = limit_ns;
+	 *   - drivers/cpuidle/governors/haltpoll.c|93| <<adjust_poll_limit>> dev->poll_limit_ns = val;
+	 *   - drivers/cpuidle/governors/haltpoll.c|103| <<adjust_poll_limit>> dev->poll_limit_ns = val;
+	 *   - drivers/cpuidle/governors/haltpoll.c|128| <<haltpoll_enable_device>> dev->poll_limit_ns = 0;
+	 *   - drivers/cpuidle/sysfs.c|433| <<cpuidle_state_store>> dev->poll_limit_ns = 0;
+	 * 在以下使用cpuidle_device->poll_limit_ns:
+	 *   - drivers/cpuidle/cpuidle.c|400| <<cpuidle_poll_time>> if (dev->poll_limit_ns)
+	 *   - drivers/cpuidle/cpuidle.c|401| <<cpuidle_poll_time>> return dev->poll_limit_ns;
+	 *   - drivers/cpuidle/cpuidle.c|420| <<cpuidle_poll_time>> return dev->poll_limit_ns;
+	 *   - drivers/cpuidle/governors/haltpoll.c|59| <<haltpoll_select>> if (dev->poll_limit_ns == 0)
+	 *   - drivers/cpuidle/governors/haltpoll.c|85| <<adjust_poll_limit>> if (block_ns > dev->poll_limit_ns && block_ns <= guest_halt_poll_ns) {
+	 *   - drivers/cpuidle/governors/haltpoll.c|86| <<adjust_poll_limit>> val = dev->poll_limit_ns * guest_halt_poll_grow;
+	 *   - drivers/cpuidle/governors/haltpoll.c|98| <<adjust_poll_limit>> val = dev->poll_limit_ns;
+	 */
 	u64			poll_limit_ns;
 	u64			forced_idle_latency_limit_ns;
 	struct cpuidle_state_usage	states_usage[CPUIDLE_STATE_MAX];
diff --git a/include/linux/kvm_dirty_ring.h b/include/linux/kvm_dirty_ring.h
index 906f899813dc..b80548c292b5 100644
--- a/include/linux/kvm_dirty_ring.h
+++ b/include/linux/kvm_dirty_ring.h
@@ -23,6 +23,16 @@ struct kvm_dirty_ring {
 	u32 reset_index;
 	u32 size;
 	u32 soft_limit;
+	/*
+	 * 在以下使用dirty_ring->dirty_gfns:
+	 *   - virt/kvm/dirty_ring.c|62| <<kvm_dirty_ring_alloc>> ring->dirty_gfns = vzalloc(size);
+	 *   - virt/kvm/dirty_ring.c|63| <<kvm_dirty_ring_alloc>> if (!ring->dirty_gfns)
+	 *   - virt/kvm/dirty_ring.c|103| <<kvm_dirty_ring_reset>> entry = &ring->dirty_gfns[ring->reset_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|157| <<kvm_dirty_ring_push>> entry = &ring->dirty_gfns[ring->dirty_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|173| <<kvm_dirty_ring_get_page>> return vmalloc_to_page((void *)ring->dirty_gfns + offset * PAGE_SIZE);
+	 *   - virt/kvm/dirty_ring.c|178| <<kvm_dirty_ring_free>> vfree(ring->dirty_gfns);
+	 *   - virt/kvm/dirty_ring.c|179| <<kvm_dirty_ring_free>> ring->dirty_gfns = NULL;
+	 */
 	struct kvm_dirty_gfn *dirty_gfns;
 	int index;
 };
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f11039944c08..db11c6e451f9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -324,6 +324,20 @@ struct kvm_vcpu {
 	bool valid_wakeup;
 
 #ifdef CONFIG_HAS_IOMEM
+	/*
+	 * x86在以下使用kvm_vcpu->mmio_needed:
+	 *   - arch/x86/kvm/vmx/vmx.c|4994| <<handle_triple_fault>> vcpu->mmio_needed = 0;
+	 *   - arch/x86/kvm/x86.c|7146| <<emulator_read_write>> vcpu->mmio_needed = 1; 
+	 *   - arch/x86/kvm/x86.c|8411| <<x86_emulate_instruction>> } else if (vcpu->mmio_needed) {
+	 *   - arch/x86/kvm/x86.c|9950| <<vcpu_enter_guest>> vcpu->mmio_needed = 0;
+	 *   - arch/x86/kvm/x86.c|10407| <<complete_emulated_mmio>> BUG_ON(!vcpu->mmio_needed);
+	 *   - arch/x86/kvm/x86.c|10427| <<complete_emulated_mmio>> vcpu->mmio_needed = 0;
+	 *   - arch/x86/kvm/x86.c|10533| <<kvm_arch_vcpu_ioctl_run>> WARN_ON(vcpu->arch.pio.count || vcpu->mmio_needed);
+	 *   - arch/x86/kvm/x86.c|12766| <<complete_sev_es_emulated_mmio>> BUG_ON(!vcpu->mmio_needed);
+	 *   - arch/x86/kvm/x86.c|12786| <<complete_sev_es_emulated_mmio>> vcpu->mmio_needed = 0;
+	 *   - arch/x86/kvm/x86.c|12830| <<kvm_sev_es_mmio_write>> vcpu->mmio_needed = 1;
+	 *   - arch/x86/kvm/x86.c|12869| <<kvm_sev_es_mmio_read>> vcpu->mmio_needed = 1;
+	 */
 	int mmio_needed;
 	int mmio_read_completed;
 	int mmio_is_write;
@@ -714,6 +728,20 @@ struct kvm {
 	 * incremented after storing the kvm_vcpu pointer in vcpus,
 	 * and is accessed atomically.
 	 */
+	/*
+	 * x86在以下设置kvm->online_vcpus:
+	 *   - virt/kvm/kvm_main.c|477| <<kvm_destroy_vcpus>> atomic_set(&kvm->online_vcpus, 0);
+	 *   - virt/kvm/kvm_main.c|3914| <<kvm_vm_ioctl_create_vcpu>> atomic_inc(&kvm->online_vcpus);
+	 * x86在以下使用kvm->online_vcpus:
+	 *   - arch/x86/kvm/svm/sev.c|1654| <<sev_es_migrate_from>> if (atomic_read(&src->online_vcpus) != atomic_read(&dst->online_vcpus))
+	 *   - arch/x86/kvm/x86.c|2381| <<kvm_track_tsc_matching>> atomic_read(&vcpu->kvm->online_vcpus));
+	 *   - arch/x86/kvm/x86.c|2396| <<kvm_track_tsc_matching>> atomic_read(&vcpu->kvm->online_vcpus),
+	 *   - arch/x86/kvm/x86.c|2818| <<pvclock_update_vm_gtod_copy>> atomic_read(&kvm->online_vcpus));
+	 *   - arch/x86/kvm/x86.c|11196| <<kvm_arch_vcpu_precreate>> if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
+	 *   - include/linux/kvm_host.h|873| <<kvm_get_vcpu>> int num_vcpus = atomic_read(&kvm->online_vcpus);
+	 *   - include/linux/kvm_host.h|883| <<kvm_for_each_vcpu>> (atomic_read(&kvm->online_vcpus) - 1))
+	 *   - virt/kvm/kvm_main.c|3890| <<kvm_vm_ioctl_create_vcpu>> vcpu->vcpu_idx = atomic_read(&kvm->online_vcpus);
+	 */
 	atomic_t online_vcpus;
 	int created_vcpus;
 	int last_boosted_vcpu;
@@ -731,6 +759,17 @@ struct kvm {
 #endif
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|843| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|927| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1107| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|1173| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1272| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1287| <<kvm_get_kvm_safe>> return refcount_inc_not_zero(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1313| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|1327| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 */
 	refcount_t users_count;
 #ifdef CONFIG_KVM_MMIO
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
@@ -751,6 +790,16 @@ struct kvm {
 
 #if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)
 	struct mmu_notifier mmu_notifier;
+	/*
+	 * arm和x86在以下使用kvm->mmu_notifier_seq:
+	 *   - arch/arm64/kvm/mmu.c|1167| <<user_mem_abort>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - arch/x86/kvm/mmu/mmu.c|4528| <<direct_page_fault>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|879| <<FNAME>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - include/linux/kvm_host.h|1936| <<mmu_notifier_retry>> if (kvm->mmu_notifier_seq != mmu_seq)
+	 *   - include/linux/kvm_host.h|1956| <<mmu_notifier_retry_hva>> if (kvm->mmu_notifier_seq != mmu_seq)
+	 *   - virt/kvm/kvm_main.c|745| <<kvm_dec_notifier_count>> kvm->mmu_notifier_seq++;
+	 *   - virt/kvm/pfncache.c|151| <<hva_to_pfn_retry>> mmu_seq = kvm->mmu_notifier_seq;
+	 */
 	unsigned long mmu_notifier_seq;
 	long mmu_notifier_count;
 	unsigned long mmu_notifier_range_start;
@@ -2003,6 +2052,10 @@ static inline bool kvm_request_pending(struct kvm_vcpu *vcpu)
 
 static inline bool kvm_test_request(int req, struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> u64 requests;
+	 */
 	return test_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);
 }
 
diff --git a/include/linux/virtio_net.h b/include/linux/virtio_net.h
index a960de68ac69..d3a01ea878e5 100644
--- a/include/linux/virtio_net.h
+++ b/include/linux/virtio_net.h
@@ -43,6 +43,17 @@ static inline int virtio_net_hdr_set_proto(struct sk_buff *skb,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/um/drivers/vector_transports.c|212| <<raw_verify_header>> virtio_net_hdr_to_skb(skb, vheader, virtio_legacy_is_little_endian());
+ *   - drivers/net/tap.c|707| <<tap_get_user>> err = virtio_net_hdr_to_skb(skb, &vnet_hdr,
+ *   - drivers/net/tap.c|1159| <<tap_get_user_xdp>> err = virtio_net_hdr_to_skb(skb, gso, tap_is_little_endian(q));
+ *   - drivers/net/tun.c|1835| <<tun_get_user>> if (virtio_net_hdr_to_skb(skb, &gso, tun_is_little_endian(tun))) {
+ *   - drivers/net/tun.c|2445| <<tun_xdp_one>> if (virtio_net_hdr_to_skb(skb, gso, tun_is_little_endian(tun))) {
+ *   - drivers/net/virtio_net.c|1164| <<receive_buf>> if (virtio_net_hdr_to_skb(skb, &hdr->hdr,
+ *   - net/packet/af_packet.c|2846| <<tpacket_snd>> if (virtio_net_hdr_to_skb(skb, vnet_hdr, vio_le())) {
+ *   - net/packet/af_packet.c|3048| <<packet_snd>> err = virtio_net_hdr_to_skb(skb, &vnet_hdr, vio_le());
+ */
 static inline int virtio_net_hdr_to_skb(struct sk_buff *skb,
 					const struct virtio_net_hdr *hdr,
 					bool little_endian)
diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index bfe38869498d..f72e91dfb35c 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -559,6 +559,16 @@ static inline void mod_lruvec_state(struct lruvec *lruvec,
 void __mod_lruvec_page_state(struct page *page,
 			     enum node_stat_item idx, int val);
 
+/*
+ * called by:
+ *   - include/linux/vmstat.h|633| <<inc_lruvec_page_state>> mod_lruvec_page_state(page, idx, 1);
+ *   - include/linux/vmstat.h|639| <<dec_lruvec_page_state>> mod_lruvec_page_state(page, idx, -1);
+ *   - include/linux/vmstat.h|645| <<lruvec_stat_mod_folio>> mod_lruvec_page_state(&folio->page, idx, val);
+ *   - kernel/fork.c|389| <<account_kernel_stack>> mod_lruvec_page_state(vm->pages[i], NR_KERNEL_STACK_KB,
+ *   - mm/slab_common.c|947| <<kmalloc_order>> mod_lruvec_page_state(page, NR_SLAB_UNRECLAIMABLE_B,
+ *   - mm/slub.c|3546| <<free_large_kmalloc>> mod_lruvec_page_state(folio_page(folio, 0), NR_SLAB_UNRECLAIMABLE_B,
+ *   - mm/slub.c|4441| <<kmalloc_large_node>> mod_lruvec_page_state(page, NR_SLAB_UNRECLAIMABLE_B,
+ */
 static inline void mod_lruvec_page_state(struct page *page,
 					 enum node_stat_item idx, int val)
 {
diff --git a/kernel/cpu.c b/kernel/cpu.c
index 407a2568f35e..aeee5a24bb98 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -1190,6 +1190,11 @@ static int cpu_down_maps_locked(unsigned int cpu, enum cpuhp_state target)
 	return _cpu_down(cpu, 0, target);
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|1215| <<cpu_device_down>> return cpu_down(dev->id, CPUHP_OFFLINE);
+ *   - kernel/cpu.c|2308| <<target_store>> ret = cpu_down(dev->id, target);
+ */
 static int cpu_down(unsigned int cpu, enum cpuhp_state target)
 {
 	int err;
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index 6db1c475ec82..fd6c7c019c26 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -71,6 +71,20 @@
 
 enum swiotlb_force swiotlb_force;
 
+/*
+ * 在以下使用io_tlb_default_mem:
+ *   - drivers/base/core.c|2885| <<device_initialize>> dev->dma_io_tlb_mem = &io_tlb_default_mem;
+ *   - drivers/xen/swiotlb-xen.c|164| <<xen_swiotlb_init>> if (io_tlb_default_mem.nslabs) {
+ *   - drivers/xen/swiotlb-xen.c|548| <<xen_swiotlb_dma_supported>> return xen_phys_to_dma(hwdev, io_tlb_default_mem.end - 1) <= mask;
+ *   - kernel/dma/swiotlb.c|107| <<swiotlb_max_segment>> return io_tlb_default_mem.nslabs ? max_segment : 0;
+ *   - kernel/dma/swiotlb.c|140| <<swiotlb_print_info>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|197| <<swiotlb_update_mem_attributes>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|250| <<swiotlb_init_with_tbl>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|352| <<swiotlb_late_init_with_tbl>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|377| <<swiotlb_exit>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|780| <<swiotlb_create_default_debugfs>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|882| <<rmem_swiotlb_device_release>> dev->dma_io_tlb_mem = &io_tlb_default_mem;
+ */
 struct io_tlb_mem io_tlb_default_mem;
 
 phys_addr_t swiotlb_unencrypted_base;
@@ -81,6 +95,15 @@ phys_addr_t swiotlb_unencrypted_base;
  */
 static unsigned int max_segment;
 
+/*
+ * 在以下使用default_nslabs:
+ *   - kernel/dma/swiotlb.c|105| <<setup_io_tlb_npages>> default_nslabs =
+ *   - kernel/dma/swiotlb.c|135| <<swiotlb_size_or_default>> return default_nslabs << IO_TLB_SHIFT;
+ *   - kernel/dma/swiotlb.c|145| <<swiotlb_adjust_size>> if (default_nslabs != IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT)
+ *   - kernel/dma/swiotlb.c|148| <<swiotlb_adjust_size>> default_nslabs = ALIGN(size >> IO_TLB_SHIFT, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|333| <<swiotlb_init>> size_t bytes = PAGE_ALIGN(default_nslabs << IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|343| <<swiotlb_init>> if (swiotlb_init_with_tbl(tlb, default_nslabs, verbose))
+ */
 static unsigned long default_nslabs = IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT;
 
 static int __init
@@ -121,6 +144,10 @@ unsigned long swiotlb_size_or_default(void)
 	return default_nslabs << IO_TLB_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/mem_encrypt_amd.c|227| <<sev_setup_arch>> swiotlb_adjust_size(size);
+ */
 void __init swiotlb_adjust_size(unsigned long size)
 {
 	/*
@@ -135,6 +162,13 @@ void __init swiotlb_adjust_size(unsigned long size)
 	pr_info("SWIOTLB bounce buffer size adjusted to %luMB", size >> 20);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kernel/dma-swiotlb.c|23| <<check_swiotlb_enabled>> swiotlb_print_info();
+ *   - arch/x86/kernel/pci-swiotlb.c|75| <<pci_swiotlb_late_init>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|269| <<swiotlb_init_with_tbl>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|370| <<swiotlb_late_init_with_tbl>> swiotlb_print_info();
+ */
 void swiotlb_print_info(void)
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
@@ -211,6 +245,12 @@ void __init swiotlb_update_mem_attributes(void)
 	memset(mem->vaddr, 0, bytes);
 }
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|266| <<swiotlb_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
+ *   - kernel/dma/swiotlb.c|368| <<swiotlb_late_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, virt_to_phys(tlb), nslabs, true);
+ *   - kernel/dma/swiotlb.c|865| <<rmem_swiotlb_device_init>> swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+ */
 static void swiotlb_init_io_tlb_mem(struct io_tlb_mem *mem, phys_addr_t start,
 				    unsigned long nslabs, bool late_alloc)
 {
@@ -245,6 +285,13 @@ static void swiotlb_init_io_tlb_mem(struct io_tlb_mem *mem, phys_addr_t start,
 	return;
 }
 
+/*
+ * called by:
+ *   - arch/mips/cavium-octeon/dma-octeon.c|248| <<plat_swiotlb_setup>> if (swiotlb_init_with_tbl(octeon_swiotlb, swiotlb_nslabs, 1) == -ENOMEM)
+ *   - arch/powerpc/platforms/pseries/svm.c|56| <<svm_swiotlb_init>> if (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, false))
+ *   - drivers/xen/swiotlb-xen.c|255| <<xen_swiotlb_init_early>> if (swiotlb_init_with_tbl(start, nslabs, true))
+ *   - kernel/dma/swiotlb.c|291| <<swiotlb_init>> if (swiotlb_init_with_tbl(tlb, default_nslabs, verbose))
+ */
 int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
@@ -263,6 +310,12 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
 		      __func__, alloc_size, PAGE_SIZE);
 
+	/*
+	 * called by:
+	 *   - kernel/dma/swiotlb.c|266| <<swiotlb_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
+	 *   - kernel/dma/swiotlb.c|368| <<swiotlb_late_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, virt_to_phys(tlb), nslabs, true);
+	 *   - kernel/dma/swiotlb.c|865| <<rmem_swiotlb_device_init>> swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+	 */
 	swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
 
 	if (verbose)
@@ -275,6 +328,18 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
  * Statically reserve bounce buffer space and initialize bounce buffer data
  * structures for the software IO TLB used to implement the DMA API.
  */
+/*
+ * called by:
+ *   - arch/arm/mm/init.c|317| <<mem_init>> swiotlb_init(1);
+ *   - arch/arm64/mm/init.c|378| <<mem_init>> swiotlb_init(1); 
+ *   - arch/ia64/mm/init.c|441| <<mem_init>> swiotlb_init(1);
+ *   - arch/mips/loongson64/dma.c|27| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/mips/sibyte/common/dma.c|13| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/powerpc/mm/mem.c|254| <<mem_init>> swiotlb_init(0);
+ *   - arch/riscv/mm/init.c|124| <<mem_init>> swiotlb_init(1);
+ *   - arch/s390/mm/init.c|189| <<pv_init>> swiotlb_init(1);
+ *   - arch/x86/kernel/pci-swiotlb.c|64| <<pci_swiotlb_init>> swiotlb_init(0);
+ */
 void  __init
 swiotlb_init(int verbose)
 {
@@ -303,6 +368,10 @@ swiotlb_init(int verbose)
  * initialize the swiotlb later using the slab allocator if needed.
  * This should be just like above, but with some error catching.
  */
+/*
+ * called by:
+ *   - arch/x86/pci/sta2x11-fixup.c|60| <<sta2x11_new_instance>> if (swiotlb_late_init_with_default_size(size))
+ */
 int
 swiotlb_late_init_with_default_size(size_t default_size)
 {
@@ -758,6 +827,13 @@ size_t swiotlb_max_mapping_size(struct device *dev)
 	return ((size_t)IO_TLB_SIZE) * IO_TLB_SEGSIZE;
 }
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/i915/gem/i915_gem_internal.c|45| <<i915_gem_object_get_pages_internal>> if (is_swiotlb_active(obj->base.dev->dev)) {
+ *   - drivers/gpu/drm/nouveau/nouveau_ttm.c|279| <<nouveau_ttm_init>> need_swiotlb = is_swiotlb_active(dev->dev);
+ *   - drivers/pci/xen-pcifront.c|684| <<pcifront_connect_and_init_dma>> if (!err && !is_swiotlb_active(&pdev->xdev->dev)) {
+ *   - kernel/dma/direct.c|580| <<dma_direct_max_mapping_size>> if (is_swiotlb_active(dev) &&
+ */
 bool is_swiotlb_active(struct device *dev)
 {
 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
@@ -769,6 +845,11 @@ EXPORT_SYMBOL_GPL(is_swiotlb_active);
 #ifdef CONFIG_DEBUG_FS
 static struct dentry *debugfs_dir;
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|785| <<swiotlb_create_default_debugfs>> swiotlb_create_debugfs_files(mem);
+ *   - kernel/dma/swiotlb.c|802| <<rmem_swiotlb_debugfs_init>> swiotlb_create_debugfs_files(mem);
+ */
 static void swiotlb_create_debugfs_files(struct io_tlb_mem *mem)
 {
 	debugfs_create_ulong("io_tlb_nslabs", 0400, mem->debugfs, &mem->nslabs);
diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index 6fcdee7e87a5..f67507ca7a39 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -415,6 +415,9 @@ static int suspend_enter(suspend_state_t state, bool *wakeup)
 	if (suspend_test(TEST_PLATFORM))
 		goto Platform_wake;
 
+	/*
+	 * echo "freeze" > /sys/power/state到这里就结束了
+	 */
 	if (state == PM_SUSPEND_TO_IDLE) {
 		s2idle_loop();
 		goto Platform_wake;
diff --git a/kernel/sched/core_sched.c b/kernel/sched/core_sched.c
index c8746a9a7ada..90401e6ca690 100644
--- a/kernel/sched/core_sched.c
+++ b/kernel/sched/core_sched.c
@@ -120,6 +120,12 @@ void sched_core_free(struct task_struct *p)
 	sched_core_put_cookie(p->core_cookie);
 }
 
+/*
+ * called by:
+ *   - kernel/sched/core_sched.c|204| <<sched_core_share_pid>> __sched_core_set(current, cookie);
+ *   - kernel/sched/core_sched.c|213| <<sched_core_share_pid>> __sched_core_set(task, cookie);
+ *   - kernel/sched/core_sched.c|228| <<sched_core_share_pid>> __sched_core_set(p, cookie);
+ */
 static void __sched_core_set(struct task_struct *p, unsigned long cookie)
 {
 	cookie = sched_core_get_cookie(cookie);
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index d17b0a5ce6ac..47f825864cb7 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -167,6 +167,10 @@ static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
  * set, and it returns with polling set.  If it ever stops polling, it
  * must clear the polling bit.
  */
+/*
+ * called by:
+ *   - kernel/sched/idle.c|306| <<do_idle>> cpuidle_idle_call();
+ */
 static void cpuidle_idle_call(void)
 {
 	struct cpuidle_device *dev = cpuidle_get_device();
diff --git a/kernel/time/clocksource.c b/kernel/time/clocksource.c
index 1cf73807b450..18a713ad29ad 100644
--- a/kernel/time/clocksource.c
+++ b/kernel/time/clocksource.c
@@ -113,6 +113,17 @@ static u64 suspend_start;
 static void clocksource_watchdog_work(struct work_struct *work);
 static void clocksource_select(void);
 
+/*
+ * 在以下使用watchdog_list:
+ *   - kernel/time/clocksource.c|116| <<global>> static LIST_HEAD(watchdog_list);
+ *   - kernel/time/clocksource.c|196| <<clocksource_mark_unstable>> list_add(&cs->wd_list, &watchdog_list);
+ *   - kernel/time/clocksource.c|400| <<clocksource_watchdog>> list_for_each_entry(cs, &watchdog_list, wd_list) {
+ *   - kernel/time/clocksource.c|522| <<clocksource_start_watchdog>> if (watchdog_running || !watchdog || list_empty(&watchdog_list))
+ *   - kernel/time/clocksource.c|532| <<clocksource_stop_watchdog>> if (!watchdog_running || (watchdog && !list_empty(&watchdog_list)))
+ *   - kernel/time/clocksource.c|542| <<clocksource_reset_watchdog>> list_for_each_entry(cs, &watchdog_list, wd_list)
+ *   - kernel/time/clocksource.c|557| <<clocksource_enqueue_watchdog>> list_add(&cs->wd_list, &watchdog_list);
+ *   - kernel/time/clocksource.c|628| <<__clocksource_watchdog_kthread>> list_for_each_entry_safe(cs, tmp, &watchdog_list, wd_list) {
+ */
 static LIST_HEAD(watchdog_list);
 static struct clocksource *watchdog;
 static struct timer_list watchdog_timer;
@@ -378,6 +389,10 @@ void clocksource_verify_percpu(struct clocksource *cs)
 }
 EXPORT_SYMBOL_GPL(clocksource_verify_percpu);
 
+/*
+ * called by:
+ *   - kernel/time/clocksource.c|520| <<clocksource_start_watchdog>> timer_setup(&watchdog_timer, clocksource_watchdog, 0);
+ */
 static void clocksource_watchdog(struct timer_list *unused)
 {
 	u64 csnow, wdnow, cslast, wdlast, delta;
@@ -393,8 +408,22 @@ static void clocksource_watchdog(struct timer_list *unused)
 
 	reset_pending = atomic_read(&watchdog_reset_pending);
 
+	/*
+	 * 在以下使用watchdog_list:
+	 *   - kernel/time/clocksource.c|116| <<global>> static LIST_HEAD(watchdog_list);
+	 *   - kernel/time/clocksource.c|196| <<clocksource_mark_unstable>> list_add(&cs->wd_list, &watchdog_list);
+	 *   - kernel/time/clocksource.c|400| <<clocksource_watchdog>> list_for_each_entry(cs, &watchdog_list, wd_list) {
+	 *   - kernel/time/clocksource.c|522| <<clocksource_start_watchdog>> if (watchdog_running || !watchdog || list_empty(&watchdog_list))
+	 *   - kernel/time/clocksource.c|532| <<clocksource_stop_watchdog>> if (!watchdog_running || (watchdog && !list_empty(&watchdog_list)))
+	 *   - kernel/time/clocksource.c|542| <<clocksource_reset_watchdog>> list_for_each_entry(cs, &watchdog_list, wd_list)
+	 *   - kernel/time/clocksource.c|557| <<clocksource_enqueue_watchdog>> list_add(&cs->wd_list, &watchdog_list);
+	 *   - kernel/time/clocksource.c|628| <<__clocksource_watchdog_kthread>> list_for_each_entry_safe(cs, tmp, &watchdog_list, wd_list) {
+	 */
 	list_for_each_entry(cs, &watchdog_list, wd_list) {
 
+		/*
+		 * struct clocksource *cs;
+		 */
 		/* Clocksource already marked unstable? */
 		if (cs->flags & CLOCK_SOURCE_UNSTABLE) {
 			if (finished_booting)
diff --git a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c
index 17a283ce2b20..6613e8834bd4 100644
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@ -54,6 +54,12 @@ static ktime_t last_jiffies_update;
 /*
  * Must be called with interrupts disabled !
  */
+/*
+ * called by:
+ *   - kernel/time/tick-sched.c|197| <<tick_sched_do_timer>> tick_do_update_jiffies64(now);
+ *   - kernel/time/tick-sched.c|618| <<tick_nohz_update_jiffies>> tick_do_update_jiffies64(now);
+ *   - kernel/time/tick-sched.c|946| <<tick_nohz_restart_sched_tick>> tick_do_update_jiffies64(now);
+ */
 static void tick_do_update_jiffies64(ktime_t now)
 {
 	unsigned long ticks = 1;
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index dcdcb85121e4..421d5138ee2e 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -186,6 +186,20 @@ static inline void tk_update_sleep_time(struct timekeeper *tk, ktime_t delta)
  * a read of the fast-timekeeper tkrs (which is protected by its own locking
  * and update logic).
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|253| <<timekeeping_get_delta>> now = tk_clock_read(tkr);
+ *   - kernel/time/timekeeping.c|287| <<timekeeping_get_delta>> cycle_now = tk_clock_read(tkr);
+ *   - kernel/time/timekeeping.c|317| <<tk_setup_internals>> tk->tkr_mono.cycle_last = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|445| <<__ktime_get_fast_ns>> tk_clock_read(tkr),
+ *   - kernel/time/timekeeping.c|548| <<__ktime_get_real_fast>> clocksource_delta(tk_clock_read(tkr),
+ *   - kernel/time/timekeeping.c|638| <<halt_fast_timekeeper>> cycles_at_suspend = tk_clock_read(tkr);
+ *   - kernel/time/timekeeping.c|779| <<timekeeping_forward_now>> cycle_now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|1050| <<ktime_get_snapshot>> now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|1228| <<ktime_get_snapshot>> now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|1785| <<timekeeping_resume>> cycle_now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|2144| <<timekeeping_advance>> offset = clocksource_delta(tk_clock_read(&tk->tkr_mono),
+ */
 static inline u64 tk_clock_read(const struct tk_read_base *tkr)
 {
 	struct clocksource *clock = READ_ONCE(tkr->clock);
@@ -1608,6 +1622,10 @@ static bool persistent_clock_exists;
 /*
  * timekeeping_init - Initializes the clocksource and common timekeeping values
  */
+/*
+ * called by:
+ *   - init/main.c|1035| <<start_kernel>> timekeeping_init();
+ */
 void __init timekeeping_init(void)
 {
 	struct timespec64 wall_time, boot_offset, wall_to_mono;
@@ -2205,6 +2223,12 @@ static bool timekeeping_advance(enum timekeeping_adv_mode mode)
  * update_wall_time - Uses the current clocksource to increment the wall time
  *
  */
+/*
+ * called by:
+ *   - kernel/time/tick-common.c|97| <<tick_periodic>> update_wall_time();
+ *   - kernel/time/tick-legacy.c|33| <<legacy_timer_tick>> update_wall_time();
+ *   - kernel/time/tick-sched.c|151| <<tick_do_update_jiffies64>> update_wall_time();
+ */
 void update_wall_time(void)
 {
 	if (timekeeping_advance(TK_ADV_TICK))
diff --git a/lib/iov_iter.c b/lib/iov_iter.c
index 6dd5330f7a99..080d0eb48e73 100644
--- a/lib/iov_iter.c
+++ b/lib/iov_iter.c
@@ -1640,6 +1640,16 @@ static ssize_t iter_xarray_get_pages_alloc(struct iov_iter *i,
 	return actual;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|259| <<bio_map_user_iov>> bytes = iov_iter_get_pages_alloc(iter, &pages, LONG_MAX, &offs);
+ *   - fs/ceph/addr.c|274| <<ceph_netfs_issue_op>> err = iov_iter_get_pages_alloc(&iter, &pages, len, &page_off);
+ *   - fs/cifs/file.c|3022| <<cifs_write_from_iter>> result = iov_iter_get_pages_alloc(
+ *   - fs/cifs/file.c|3753| <<cifs_send_async_read>> result = iov_iter_get_pages_alloc(
+ *   - fs/nfs/direct.c|370| <<nfs_direct_read_schedule_iovec>> result = iov_iter_get_pages_alloc(iter, &pagevec,
+ *   - fs/nfs/direct.c|814| <<nfs_direct_write_schedule_iovec>> result = iov_iter_get_pages_alloc(iter, &pagevec,
+ *   - net/9p/trans_virtio.c|334| <<p9_get_mapped_pages>> n = iov_iter_get_pages_alloc(data, pages, count, offs);
+ */
 ssize_t iov_iter_get_pages_alloc(struct iov_iter *i,
 		   struct page ***pages, size_t maxsize,
 		   size_t *start)
diff --git a/net/core/dev.c b/net/core/dev.c
index 1baab07820f6..cc30d7084804 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -3464,6 +3464,9 @@ static int xmit_one(struct sk_buff *skb, struct net_device *dev,
 	unsigned int len;
 	int rc;
 
+	/*
+	 * dev_nit_active - return true if any network interface taps are in use
+	 */
 	if (dev_nit_active(dev))
 		dev_queue_xmit_nit(skb, dev);
 
diff --git a/net/packet/af_packet.c b/net/packet/af_packet.c
index a7273af2d900..b8905e804ee6 100644
--- a/net/packet/af_packet.c
+++ b/net/packet/af_packet.c
@@ -2222,6 +2222,11 @@ static int packet_rcv(struct sk_buff *skb, struct net_device *dev,
 	return 0;
 }
 
+/*
+ * 在以下使用tpacket_rcv():
+ *   - net/packet/af_packet.c|1279| <<__packet_rcv_has_room>> if (po->prot_hook.func != tpacket_rcv) {
+ *   - net/packet/af_packet.c|4483| <<packet_set_ring>> tpacket_rcv : packet_rcv;
+ */
 static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,
 		       struct packet_type *pt, struct net_device *orig_dev)
 {
diff --git a/sound/core/timer.c b/sound/core/timer.c
index b3214baa8919..1ef23157d408 100644
--- a/sound/core/timer.c
+++ b/sound/core/timer.c
@@ -922,6 +922,10 @@ void snd_timer_interrupt(struct snd_timer * timer, unsigned long ticks_left)
 	use_work = !list_empty(&timer->sack_list_head);
 	spin_unlock_irqrestore(&timer->lock, flags);
 
+	/*
+	 * struct snd_timer * timer:
+	 * -> struct work_struct task_work;
+	 */
 	if (use_work)
 		queue_work(system_highpri_wq, &timer->task_work);
 }
diff --git a/tools/testing/selftests/kvm/kvm_page_table_test.c b/tools/testing/selftests/kvm/kvm_page_table_test.c
index ba1fdc3dcf4a..cdb88794e4fb 100644
--- a/tools/testing/selftests/kvm/kvm_page_table_test.c
+++ b/tools/testing/selftests/kvm/kvm_page_table_test.c
@@ -67,6 +67,11 @@ struct test_args {
  * Guest variables. Use addr_gva2hva() if these variables need
  * to be changed in host.
  */
+/*
+ * 在以下使用guest_test_stage:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|99| <<guest_code>> enum test_stage *current_stage = &guest_test_stage;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|321| <<pre_init_before_test>> current_stage = addr_gva2hva(vm, (vm_vaddr_t)(&guest_test_stage));
+ */
 static enum test_stage guest_test_stage;
 
 /* Host variables */
@@ -90,12 +95,20 @@ static uint64_t guest_test_phys_mem;
  * Guest virtual memory offset of the testing memory slot.
  * Must not conflict with identity mapped test code.
  */
+/*
+ * 0xc0000000
+ */
 static uint64_t guest_test_virt_mem = DEFAULT_GUEST_TEST_MEM;
 
 static void guest_code(int vcpu_id)
 {
 	struct test_args *p = &test_args;
 	struct vcpu_args *vcpu_args = &p->vcpu_args[vcpu_id];
+	/*
+	 * 在以下使用guest_test_stage:
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|99| <<guest_code>> enum test_stage *current_stage = &guest_test_stage;
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|321| <<pre_init_before_test>> current_stage = addr_gva2hva(vm, (vm_vaddr_t)(&guest_test_stage));
+	 */
 	enum test_stage *current_stage = &guest_test_stage;
 	uint64_t addr;
 	int i, j;
@@ -336,6 +349,13 @@ static struct kvm_vm *pre_init_before_test(enum vm_guest_mode mode, void *arg)
 	return vm;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|400| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|407| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|420| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|432| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ */
 static void vcpus_complete_new_stage(enum test_stage stage)
 {
 	int ret;
diff --git a/tools/testing/selftests/kvm/lib/guest_modes.c b/tools/testing/selftests/kvm/lib/guest_modes.c
index 8784013b747c..981fc1a5a829 100644
--- a/tools/testing/selftests/kvm/lib/guest_modes.c
+++ b/tools/testing/selftests/kvm/lib/guest_modes.c
@@ -11,6 +11,16 @@ enum vm_guest_mode vm_mode_default;
 
 struct guest_mode guest_modes[NUM_VM_MODES];
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|355| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|408| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|344| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|864| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|485| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|501| <<init_guest_modes>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|154| <<main>> guest_modes_append_default();
+ */
 void guest_modes_append_default(void)
 {
 #ifndef __aarch64__
diff --git a/tools/testing/selftests/kvm/lib/kvm_util.c b/tools/testing/selftests/kvm/lib/kvm_util.c
index d8cf851ab119..1c71854e485b 100644
--- a/tools/testing/selftests/kvm/lib/kvm_util.c
+++ b/tools/testing/selftests/kvm/lib/kvm_util.c
@@ -1064,6 +1064,12 @@ memslot2region(struct kvm_vm *vm, uint32_t memslot)
  * Sets the flags of the memory region specified by the value of slot,
  * to the values given by flags.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|104| <<toggle_dirty_logging>> vm_mem_region_set_flags(vm, slot, flags);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|421| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX,
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|434| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX, 0);
+ */
 void vm_mem_region_set_flags(struct kvm_vm *vm, uint32_t slot, uint32_t flags)
 {
 	int ret;
diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c
index 9bfe1d6f6529..475c21c3de57 100644
--- a/virt/kvm/async_pf.c
+++ b/virt/kvm/async_pf.c
@@ -134,6 +134,12 @@ void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 	vcpu->async_pf.queued = 0;
 }
 
+/*
+ * called by:
+ *   - arch/s390/kvm/kvm-s390.c|4003| <<vcpu_pre_run>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|3610| <<kvm_set_msr_common(MSR_KVM_ASYNC_PF_ACK)>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|10013| <<vcpu_enter_guest(处理KVM_REQ_APF_READY)>> kvm_check_async_pf_completion(vcpu);
+ */
 void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
 {
 	struct kvm_async_pf *work;
diff --git a/virt/kvm/dirty_ring.c b/virt/kvm/dirty_ring.c
index 222ecc81d7df..50be47c94c8f 100644
--- a/virt/kvm/dirty_ring.c
+++ b/virt/kvm/dirty_ring.c
@@ -147,6 +147,20 @@ int kvm_dirty_ring_reset(struct kvm *kvm, struct kvm_dirty_ring *ring)
 	return count;
 }
 
+/*
+ * 一个callstack的例子
+ * [0] kvm_dirty_ring_push
+ * [0] mark_page_dirty_in_slot
+ * [0] kvm_steal_time_set_preempted
+ * [0] kvm_arch_vcpu_put
+ * [0] vcpu_put
+ * [0] vmx_free_vcpu
+ * [0] kvm_arch_vcpu_destroy
+ * [0] kvm_vcpu_destroy
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|3171| <<mark_page_dirty_in_slot>> kvm_dirty_ring_push(&vcpu->dirty_ring,
+ */
 void kvm_dirty_ring_push(struct kvm_dirty_ring *ring, u32 slot, u64 offset)
 {
 	struct kvm_dirty_gfn *entry;
@@ -175,6 +189,16 @@ struct page *kvm_dirty_ring_get_page(struct kvm_dirty_ring *ring, u32 offset)
 
 void kvm_dirty_ring_free(struct kvm_dirty_ring *ring)
 {
+	/*
+	 * 在以下使用dirty_ring->dirty_gfns:
+	 *   - virt/kvm/dirty_ring.c|62| <<kvm_dirty_ring_alloc>> ring->dirty_gfns = vzalloc(size);
+	 *   - virt/kvm/dirty_ring.c|63| <<kvm_dirty_ring_alloc>> if (!ring->dirty_gfns)
+	 *   - virt/kvm/dirty_ring.c|103| <<kvm_dirty_ring_reset>> entry = &ring->dirty_gfns[ring->reset_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|157| <<kvm_dirty_ring_push>> entry = &ring->dirty_gfns[ring->dirty_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|173| <<kvm_dirty_ring_get_page>> return vmalloc_to_page((void *)ring->dirty_gfns + offset * PAGE_SIZE);
+	 *   - virt/kvm/dirty_ring.c|178| <<kvm_dirty_ring_free>> vfree(ring->dirty_gfns);
+	 *   - virt/kvm/dirty_ring.c|179| <<kvm_dirty_ring_free>> ring->dirty_gfns = NULL;
+	 */
 	vfree(ring->dirty_gfns);
 	ring->dirty_gfns = NULL;
 }
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 0afc016cc54d..6d4a26db2c48 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -109,7 +109,24 @@ static atomic_t hardware_enable_failed;
 
 static struct kmem_cache *kvm_vcpu_cache;
 
+/*
+ * 在以下使用kvm_preempt_ops:
+ *   - virt/kvm/kvm_main.c|468| <<kvm_vcpu_init>> preempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);
+ *   - virt/kvm/kvm_main.c|6034| <<kvm_init>> kvm_preempt_ops.sched_in = kvm_sched_in;
+ *   - virt/kvm/kvm_main.c|6035| <<kvm_init>> kvm_preempt_ops.sched_out = kvm_sched_out;
+ */
 static __read_mostly struct preempt_ops kvm_preempt_ops;
+/*
+ * 在以下使用percpu kvm_running_vcpu:
+ *   - virt/kvm/kvm_main.c|113| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+ *   - virt/kvm/kvm_main.c|201| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|213| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|3606| <<kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+ *   - virt/kvm/kvm_main.c|5779| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|5794| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|5811| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+ *   - virt/kvm/kvm_main.c|5823| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+ */
 static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
 
 struct dentry *kvm_debugfs_dir;
@@ -194,6 +211,25 @@ bool kvm_is_reserved_pfn(kvm_pfn_t pfn)
 /*
  * Switches to specified vcpu, until a matching vcpu_put()
  */
+/*
+ * x86在以下调用:
+ *   - arch/x86/kvm/vmx/nested.c|336| <<nested_vmx_free_vcpu>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|5312| <<kvm_arch_vcpu_ioctl>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10556| <<kvm_arch_vcpu_ioctl_run>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10684| <<kvm_arch_vcpu_ioctl_get_regs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10724| <<kvm_arch_vcpu_ioctl_set_regs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10806| <<kvm_arch_vcpu_ioctl_get_sregs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10817| <<kvm_arch_vcpu_ioctl_get_mpstate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10845| <<kvm_arch_vcpu_ioctl_set_mpstate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11055| <<kvm_arch_vcpu_ioctl_set_sregs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11088| <<kvm_arch_vcpu_ioctl_set_guest_debug>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11150| <<kvm_arch_vcpu_ioctl_translate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11171| <<kvm_arch_vcpu_ioctl_get_fpu>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11194| <<kvm_arch_vcpu_ioctl_set_fpu>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11338| <<kvm_arch_vcpu_create>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11368| <<kvm_arch_vcpu_postcreate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11814| <<kvm_unload_vcpu_mmu>> vcpu_load(vcpu);
+ */
 void vcpu_load(struct kvm_vcpu *vcpu)
 {
 	int cpu = get_cpu();
@@ -415,6 +451,10 @@ void *kvm_mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)
 }
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3949| <<kvm_vm_ioctl_create_vcpu>> kvm_vcpu_init(vcpu, kvm, id);
+ */
 static void kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)
 {
 	mutex_init(&vcpu->mutex);
@@ -435,6 +475,10 @@ static void kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)
 	vcpu->last_used_slot = NULL;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|460| <<kvm_destroy_vcpus>> kvm_vcpu_destroy(vcpu);
+ */
 static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_dirty_ring_free(&vcpu->dirty_ring);
@@ -451,6 +495,15 @@ static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|182| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/mips/kvm/mips.c|183| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/powerpc/kvm/powerpc.c|476| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/riscv/kvm/vm.c|49| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/s390/kvm/kvm-s390.c|2798| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/x86/kvm/x86.c|11672| <<kvm_free_vcpus>> kvm_destroy_vcpus(kvm);
+ */
 void kvm_destroy_vcpus(struct kvm *kvm)
 {
 	unsigned long i;
@@ -716,6 +769,11 @@ static int kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6405| <<kvm_zap_gfn_range>> kvm_dec_notifier_count(kvm, gfn_start, gfn_end);
+ *   - virt/kvm/kvm_main.c|759| <<kvm_mmu_notifier_invalidate_range_end>> .on_lock = kvm_dec_notifier_count,
+ */
 void kvm_dec_notifier_count(struct kvm *kvm, unsigned long start,
 				   unsigned long end)
 {
@@ -820,6 +878,10 @@ static void kvm_mmu_notifier_release(struct mmu_notifier *mn,
 	srcu_read_unlock(&kvm->srcu, idx);
 }
 
+/*
+ * 在以下使用kvm_mmu_notifier_ops:
+ *   - virt/kvm/kvm_main.c|849| <<kvm_init_mmu_notifier>> kvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;
+ */
 static const struct mmu_notifier_ops kvm_mmu_notifier_ops = {
 	.invalidate_range	= kvm_mmu_notifier_invalidate_range,
 	.invalidate_range_start	= kvm_mmu_notifier_invalidate_range_start,
@@ -947,6 +1009,10 @@ static void kvm_destroy_vm_debugfs(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4847| <<kvm_dev_ioctl_create_vm>> if (kvm_create_vm_debugfs(kvm, r) < 0) {
+ */
 static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 {
 	static DEFINE_MUTEX(kvm_debugfs_lock);
@@ -1049,6 +1115,10 @@ int __weak kvm_arch_create_vm_debugfs(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4831| <<kvm_dev_ioctl_create_vm>> kvm = kvm_create_vm(type);
+ */
 static struct kvm *kvm_create_vm(unsigned long type)
 {
 	struct kvm *kvm = kvm_arch_alloc_vm();
@@ -1083,6 +1153,17 @@ static struct kvm *kvm_create_vm(unsigned long type)
 	if (init_srcu_struct(&kvm->irq_srcu))
 		goto out_err_no_irq_srcu;
 
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|843| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|927| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1107| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|1173| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1272| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1287| <<kvm_get_kvm_safe>> return refcount_inc_not_zero(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1313| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|1327| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 */
 	refcount_set(&kvm->users_count, 1);
 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
 		for (j = 0; j < 2; j++) {
@@ -1176,6 +1257,10 @@ static void kvm_destroy_devices(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1250| <<kvm_put_kvm>> kvm_destroy_vm(kvm);
+ */
 static void kvm_destroy_vm(struct kvm *kvm)
 {
 	int i;
@@ -1228,6 +1313,20 @@ static void kvm_destroy_vm(struct kvm *kvm)
 	mmdrop(mm);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1956| <<kvm_vm_ioctl_get_htab_fd>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1999| <<debugfs_htab_open>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|1271| <<debugfs_radix_open>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|321| <<kvm_vm_ioctl_create_spapr_tce>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_hv.c|2673| <<debugfs_timings_open>> kvm_get_kvm(vcpu->kvm);
+ *   - arch/x86/kvm/svm/sev.c|2014| <<svm_vm_copy_asid_from>> kvm_get_kvm(source_kvm);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1941| <<kvmgt_guest_init>> kvm_get_kvm(info->kvm);
+ *   - drivers/s390/crypto/vfio_ap_ops.c|1096| <<vfio_ap_mdev_set_kvm>> kvm_get_kvm(kvm);
+ *   - virt/kvm/async_pf.c|196| <<kvm_setup_async_pf>> kvm_get_kvm(work->vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|3862| <<kvm_vm_ioctl_create_vcpu>> kvm_get_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|4346| <<kvm_ioctl_create_device>> kvm_get_kvm(kvm);
+ */
 void kvm_get_kvm(struct kvm *kvm)
 {
 	refcount_inc(&kvm->users_count);
@@ -1238,12 +1337,37 @@ EXPORT_SYMBOL_GPL(kvm_get_kvm);
  * Make sure the vm is not during destruction, which is a safe version of
  * kvm_get_kvm().  Return true if kvm referenced successfully, false otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|162| <<kvm_mmu_rmaps_stat_open>> if (!kvm_get_kvm_safe(kvm))
+ *   - virt/kvm/kvm_main.c|5336| <<kvm_debugfs_open>> if (!kvm_get_kvm_safe(stat_data->kvm))
+ */
 bool kvm_get_kvm_safe(struct kvm *kvm)
 {
 	return refcount_inc_not_zero(&kvm->users_count);
 }
 EXPORT_SYMBOL_GPL(kvm_get_kvm_safe);
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1932| <<kvm_htab_release>> kvm_put_kvm(ctx->kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|2011| <<debugfs_htab_release>> kvm_put_kvm(p->kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|1283| <<debugfs_radix_release>> kvm_put_kvm(p->kvm);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|267| <<kvm_spapr_tce_release>> kvm_put_kvm(stt->kvm);
+ *   - arch/powerpc/kvm/book3s_hv.c|2684| <<debugfs_timings_release>> kvm_put_kvm(p->vcpu->kvm);
+ *   - arch/x86/kvm/debugfs.c|172| <<kvm_mmu_rmaps_stat_release>> kvm_put_kvm(kvm);
+ *   - arch/x86/kvm/svm/sev.c|2058| <<sev_vm_destroy>> kvm_put_kvm(owner_kvm);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1961| <<kvmgt_guest_exit>> kvm_put_kvm(info->kvm);
+ *   - drivers/s390/crypto/vfio_ap_ops.c|1167| <<vfio_ap_mdev_unset_kvm>> kvm_put_kvm(kvm);
+ *   - virt/kvm/async_pf.c|91| <<async_pf_execute>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/async_pf.c|118| <<kvm_clear_async_pf_completion_queue>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|1273| <<kvm_vm_release>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|3668| <<kvm_vcpu_release>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|4177| <<kvm_device_release>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|4781| <<kvm_dev_ioctl_create_vm>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|5254| <<kvm_debugfs_open>> kvm_put_kvm(stat_data->kvm);
+ *   - virt/kvm/kvm_main.c|5267| <<kvm_debugfs_release>> kvm_put_kvm(stat_data->kvm);
+ */
 void kvm_put_kvm(struct kvm *kvm)
 {
 	if (refcount_dec_and_test(&kvm->users_count))
@@ -2698,6 +2822,11 @@ struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(gfn_to_page);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2839| <<kvm_vcpu_unmap>> kvm_release_pfn(map->pfn, dirty);
+ *   - virt/kvm/pfncache.c|134| <<__release_gpc>> kvm_release_pfn(pfn, dirty);
+ */
 void kvm_release_pfn(kvm_pfn_t pfn, bool dirty)
 {
 	if (pfn == 0)
@@ -3044,14 +3173,49 @@ static int __kvm_gfn_to_hva_cache_init(struct kvm_memslots *slots,
 	else
 		ghc->memslot = NULL;
 
+	/*
+	 * struct gfn_to_hva_cache {
+	 *     u64 generation;
+	 *     gpa_t gpa;
+	 *     unsigned long hva;
+	 *     unsigned long len;
+	 *     struct kvm_memory_slot *memslot;
+	 * };
+	 */
 	ghc->gpa = gpa;
 	ghc->len = len;
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2839| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/lapic.c|2937| <<kvm_lapic_set_pv_eoi>> ret = kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, addr, new_len);
+ *   - arch/x86/kvm/vmx/nested.c|706| <<nested_cache_shadow_vmcs12>> kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc,
+ *   - arch/x86/kvm/vmx/nested.c|725| <<nested_flush_cached_shadow_vmcs12>> kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc,
+ *   - arch/x86/kvm/vmx/nested.c|2966| <<nested_vmx_check_vmcs_link_ptr>> CC(kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc,
+ *   - arch/x86/kvm/vmx/nested.c|5346| <<handle_vmptrld>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, vmptr, VMCS12_SIZE)) {
+ *   - arch/x86/kvm/x86.c|2230| <<kvm_write_system_time>> if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/x86.c|3248| <<kvm_pv_enable_async_pf>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apf.data, gpa,
+ *   - arch/x86/kvm/x86.c|3361| <<record_steal_time>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, gfn, sizeof(*st)) ||
+ *   - arch/x86/kvm/xen.c|149| <<kvm_xen_update_runstate_guest>> kvm_gfn_to_hva_cache_init(v->kvm, ghc, ghc->gpa, ghc->len))
+ *   - arch/x86/kvm/xen.c|304| <<__kvm_xen_has_interrupt>> err = kvm_gfn_to_hva_cache_init(v->kvm, ghc, ghc->gpa, ghc->len);
+ *   - arch/x86/kvm/xen.c|471| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/xen.c|494| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/xen.c|521| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ */
 int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			      gpa_t gpa, unsigned long len)
 {
+	/*
+	 * struct gfn_to_hva_cache {
+	 *     u64 generation;
+	 *     gpa_t gpa;
+	 *     unsigned long hva;
+	 *     unsigned long len;
+	 *     struct kvm_memory_slot *memslot;
+	 * };
+	 */
 	struct kvm_memslots *slots = kvm_memslots(kvm);
 	return __kvm_gfn_to_hva_cache_init(slots, ghc, gpa, len);
 }
@@ -3152,6 +3316,21 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 }
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1270| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|3272| <<fast_pf_fix_direct_spte>> mark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);
+ *   - arch/x86/kvm/mmu/spte.c|188| <<make_spte>> mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|359| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/x86/kvm/x86.c|3428| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/x86.c|4604| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/xen.c|244| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/xen.c|366| <<__kvm_xen_has_interrupt>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|2984| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3122| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3221| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3230| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+ */
 void mark_page_dirty_in_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *memslot,
 		 	     gfn_t gfn)
@@ -3176,6 +3355,22 @@ void mark_page_dirty_in_slot(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(mark_page_dirty_in_slot);
 
+/*
+ * called by:
+ *   - arch/mips/kvm/mmu.c|547| <<_kvm_mips_map_page_fast>> mark_page_dirty(kvm, gfn);
+ *   - arch/mips/kvm/mmu.c|661| <<kvm_mips_map_page>> mark_page_dirty(kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_32_mmu_host.c|200| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, orig_pte->raddr >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_64_mmu_host.c|128| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_hv.c|884| <<kvmppc_copy_guest>> mark_page_dirty(kvm, to >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_xive_native.c|909| <<kvmppc_xive_native_vcpu_eq_sync>> mark_page_dirty(vcpu->kvm, gpa_to_gfn(q->guest_qaddr));
+ *   - arch/riscv/kvm/mmu.c|682| <<kvm_riscv_stage2_map>> mark_page_dirty(kvm, gfn);
+ *   - arch/s390/kvm/interrupt.c|2808| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->ind_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/interrupt.c|2814| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->summary_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/kvm-s390.c|636| <<kvm_arch_sync_dirty_log>> mark_page_dirty(kvm, cur_gfn + i);
+ *   - arch/s390/kvm/vsie.c|658| <<unpin_guest_page>> mark_page_dirty(kvm, gpa_to_gfn(gpa));
+ *   - include/linux/kvm_host.h|1175| <<__kvm_put_guest>> mark_page_dirty(kvm, gfn); \
+ *   - virt/kvm/pfncache.c|123| <<__release_gpc>> mark_page_dirty(kvm, gpa);
+ */
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn)
 {
 	struct kvm_memory_slot *memslot;
@@ -3185,6 +3380,15 @@ void mark_page_dirty(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(mark_page_dirty);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|1492| <<kvm_hv_set_msr>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|291| <<FNAME>> kvm_vcpu_mark_page_dirty(vcpu, table_gfn);
+ *   - arch/x86/kvm/vmx/nested.c|3752| <<nested_mark_vmcs12_pages_dirty>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/vmx/nested.c|3757| <<nested_mark_vmcs12_pages_dirty>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/vmx/vmx.c|5809| <<vmx_flush_pml_buffer>> kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|2799| <<kvm_vcpu_unmap>> kvm_vcpu_mark_page_dirty(vcpu, map->gfn);
+ */
 void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	struct kvm_memory_slot *memslot;
@@ -3688,6 +3892,10 @@ static int create_vcpu_fd(struct kvm_vcpu *vcpu)
 	return anon_inode_getfd(name, &kvm_vcpu_fops, vcpu, O_RDWR | O_CLOEXEC);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3874| <<kvm_vm_ioctl_create_vcpu>> kvm_create_vcpu_debugfs(vcpu);
+ */
 static void kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
 {
 #ifdef __KVM_HAVE_ARCH_VCPU_DEBUGFS
@@ -3708,6 +3916,10 @@ static void kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
 /*
  * Creates some virtual cpus.  Good luck creating more than one.
  */
+/*
+ * called by(KVM_CREATE_VCPU):
+ *   - virt/kvm/kvm_main.c|4605| <<kvm_vm_ioctl(KVM_CREATE_VCPU)>> r = kvm_vm_ioctl_create_vcpu(kvm, arg);
+ */
 static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)
 {
 	int r;
@@ -3744,6 +3956,9 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)
 	}
 	vcpu->run = page_address(page);
 
+	/*
+	 * 设置vcpu->cpu = -1;
+	 */
 	kvm_vcpu_init(vcpu, kvm, id);
 
 	r = kvm_arch_vcpu_create(vcpu);
@@ -4001,6 +4216,19 @@ static long kvm_vcpu_ioctl(struct file *filp,
 		break;
 	}
 	case KVM_SET_GUEST_DEBUG: {
+		/*
+		 * KVM_SET_GUEST_DEBUG的例子:
+		 *
+		 * struct kvm_guest_debug debug;
+		 *
+		 * memset(&debug, 0, sizeof(debug));
+		 * debug.control = KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP;
+		 * ioctl(vcpu->fd, KVM_SET_GUEST_DEBUG, &debug);
+		 *
+		 * That will intercept #BP and exit to userspace with KVM_EXIT_DEBUG.  Note, it's
+		 * userspace's responsibility to re-inject the #BP if userspace wants to forward the
+		 * #BP to the guest.
+		 */
 		struct kvm_guest_debug dbg;
 
 		r = -EFAULT;
@@ -4273,6 +4501,11 @@ static int kvm_ioctl_create_device(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4724| <<kvm_vm_ioctl(KVM_CHECK_EXTENSION)>> r = kvm_vm_ioctl_check_extension_generic(kvm, arg);
+ *   - virt/kvm/kvm_main.c|4889| <<kvm_dev_ioctl(KVM_CHECK_EXTENSION)>> r = kvm_vm_ioctl_check_extension_generic(NULL, arg);
+ */
 static long kvm_vm_ioctl_check_extension_generic(struct kvm *kvm, long arg)
 {
 	switch (arg) {
@@ -4459,6 +4692,9 @@ static int kvm_vm_ioctl_get_stats_fd(struct kvm *kvm)
 	return fd;
 }
 
+/*
+ * struct file_operations kvm_vm_fops.unlocked_ioctl = kvm_vm_ioctl()
+ */
 static long kvm_vm_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -4482,6 +4718,16 @@ static long kvm_vm_ioctl(struct file *filp,
 		break;
 	}
 	case KVM_SET_USER_MEMORY_REGION: {
+		/*
+		 * // for KVM_SET_USER_MEMORY_REGION
+		 * struct kvm_userspace_memory_region {
+		 *     __u32 slot;
+		 *     __u32 flags;        // KVM_MEM_LOG_DIRTY_PAGES or KVM_MEM_READONLY
+		 *     __u64 guest_phys_addr;
+		 *     __u64 memory_size; // bytes
+		 *     __u64 userspace_addr; // start of the userspace allocated memory
+		 * };
+		 */
 		struct kvm_userspace_memory_region kvm_userspace_mem;
 
 		r = -EFAULT;
@@ -4720,6 +4966,12 @@ static long kvm_vm_compat_ioctl(struct file *filp,
 }
 #endif
 
+/*
+ * 在以下使用kvm_vm_fops:
+ *   - virt/kvm/kvm_main.c|4817| <<file_is_kvm>> return file && file->f_op == &kvm_vm_fops;
+ *   - virt/kvm/kvm_main.c|4846| <<kvm_dev_ioctl_create_vm>> file = anon_inode_getfile("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);
+ *   - virt/kvm/kvm_main.c|5836| <<kvm_init>> kvm_vm_fops.owner = module;
+ */
 static struct file_operations kvm_vm_fops = {
 	.release        = kvm_vm_release,
 	.unlocked_ioctl = kvm_vm_ioctl,
@@ -4733,6 +4985,10 @@ bool file_is_kvm(struct file *file)
 }
 EXPORT_SYMBOL_GPL(file_is_kvm);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4874| <<kvm_dev_ioctl(KVM_CREATE_VM)>> r = kvm_dev_ioctl_create_vm(arg);
+ */
 static int kvm_dev_ioctl_create_vm(unsigned long type)
 {
 	int r;
@@ -4828,12 +5084,25 @@ static struct file_operations kvm_chardev_ops = {
 	KVM_COMPAT(kvm_dev_ioctl),
 };
 
+/*
+ * 在以下使用kvm_dev:
+ *   - virt/kvm/kvm_main.c|5539| <<kvm_uevent_notify_change>> if (!kvm_dev.this_device || !kvm)
+ *   - virt/kvm/kvm_main.c|5580| <<kvm_uevent_notify_change>> kobject_uevent_env(&kvm_dev.this_device->kobj, KOBJ_CHANGE, env->envp);
+ *   - virt/kvm/kvm_main.c|5839| <<kvm_init>> r = misc_register(&kvm_dev);
+ *   - virt/kvm/kvm_main.c|5885| <<kvm_exit>> misc_deregister(&kvm_dev);
+ */
 static struct miscdevice kvm_dev = {
 	KVM_MINOR,
 	"kvm",
 	&kvm_chardev_ops,
 };
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5091| <<kvm_starting_cpu>> hardware_enable_nolock(NULL);
+ *   - virt/kvm/kvm_main.c|5140| <<hardware_enable_all>> on_each_cpu(hardware_enable_nolock, NULL, 1);
+ *   - virt/kvm/kvm_main.c|5768| <<kvm_resume>> hardware_enable_nolock(NULL);
+ */
 static void hardware_enable_nolock(void *junk)
 {
 	int cpu = raw_smp_processor_id();
@@ -5437,6 +5706,11 @@ DEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_fops, vcpu_stat_get, vcpu_stat_clear,
 			"%llu\n");
 DEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_readonly_fops, vcpu_stat_get, NULL, "%llu\n");
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1185| <<kvm_destroy_vm>> kvm_uevent_notify_change(KVM_EVENT_DESTROY_VM, kvm);
+ *   - virt/kvm/kvm_main.c|4775| <<kvm_dev_ioctl_create_vm>> kvm_uevent_notify_change(KVM_EVENT_CREATE_VM, kvm);
+ */
 static void kvm_uevent_notify_change(unsigned int type, struct kvm *kvm)
 {
 	struct kobj_uevent_env *env;
@@ -5583,6 +5857,17 @@ struct kvm_vcpu *kvm_get_running_vcpu(void)
 	struct kvm_vcpu *vcpu;
 
 	preempt_disable();
+	/*
+	 * 在以下使用percpu kvm_running_vcpu:
+	 *   - virt/kvm/kvm_main.c|113| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|201| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|213| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|3606| <<kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+	 *   - virt/kvm/kvm_main.c|5779| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|5794| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|5811| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|5823| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+	 */
 	vcpu = __this_cpu_read(kvm_running_vcpu);
 	preempt_enable();
 
@@ -5654,6 +5939,20 @@ static void check_processor_compat(void *data)
 	*c->ret = kvm_arch_check_processor_compat(c->opaque);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2240| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/mips/kvm/mips.c|1649| <<kvm_mips_init>> ret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/book3s.c|1065| <<kvmppc_book3s_init>> r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500.c|533| <<kvmppc_e500_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500mc.c|403| <<kvmppc_e500mc_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/riscv/kvm/main.c|124| <<riscv_kvm_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/s390/kvm/kvm-s390.c|5075| <<kvm_s390_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/x86/kvm/svm/svm.c|4914| <<svm_init>> return kvm_init(&svm_init_ops, sizeof(struct vcpu_svm),
+ *   - arch/x86/kvm/vmx/vmx.c|8166| <<vmx_init>> r = kvm_init(&vmx_init_ops, sizeof(struct vcpu_vmx),
+ *
+ * 在intel下被vmx_init()调用
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
@@ -5846,6 +6145,10 @@ static int kvm_vm_worker_thread(void *context)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7039| <<kvm_mmu_post_init_vm>> err = kvm_vm_create_worker_thread(kvm, kvm_nx_lpage_recovery_worker, 0, "kvm-nx-lpage-recovery", &kvm->arch.nx_lpage_recovery_thread);
+ */
 int kvm_vm_create_worker_thread(struct kvm *kvm, kvm_vm_thread_fn_t thread_fn,
 				uintptr_t data, const char *name,
 				struct task_struct **thread_ptr)
diff --git a/virt/kvm/pfncache.c b/virt/kvm/pfncache.c
index ce878f4be4da..c0a6f004644d 100644
--- a/virt/kvm/pfncache.c
+++ b/virt/kvm/pfncache.c
@@ -22,6 +22,10 @@
 /*
  * MMU notifier 'invalidate_range_start' hook.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|724| <<kvm_mmu_notifier_invalidate_range_start>> gfn_to_pfn_cache_invalidate_start(kvm, range->start, range->end,
+ */
 void gfn_to_pfn_cache_invalidate_start(struct kvm *kvm, unsigned long start,
 				       unsigned long end, bool may_block)
 {
@@ -85,6 +89,10 @@ void gfn_to_pfn_cache_invalidate_start(struct kvm *kvm, unsigned long start,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|925| <<kvm_xen_set_evtchn_fast>> if (!kvm_gfn_to_pfn_cache_check(kvm, gpc, gpc->gpa, PAGE_SIZE))
+ */
 bool kvm_gfn_to_pfn_cache_check(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 				gpa_t gpa, unsigned long len)
 {
@@ -104,6 +112,11 @@ bool kvm_gfn_to_pfn_cache_check(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 }
 EXPORT_SYMBOL_GPL(kvm_gfn_to_pfn_cache_check);
 
+/*
+ * called by:
+ *   - virt/kvm/pfncache.c|265| <<kvm_gfn_to_pfn_cache_refresh>> __release_gpc(kvm, old_pfn, old_khva, old_gpa, old_dirty);
+ *   - virt/kvm/pfncache.c|296| <<kvm_gfn_to_pfn_cache_unmap>> __release_gpc(kvm, old_pfn, old_khva, old_gpa, old_dirty);
+ */
 static void __release_gpc(struct kvm *kvm, kvm_pfn_t pfn, void *khva,
 			  gpa_t gpa, bool dirty)
 {
@@ -124,6 +137,10 @@ static void __release_gpc(struct kvm *kvm, kvm_pfn_t pfn, void *khva,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/pfncache.c|216| <<kvm_gfn_to_pfn_cache_refresh>> new_pfn = hva_to_pfn_retry(kvm, uhva);
+ */
 static kvm_pfn_t hva_to_pfn_retry(struct kvm *kvm, unsigned long uhva)
 {
 	unsigned long mmu_seq;
@@ -151,6 +168,11 @@ static kvm_pfn_t hva_to_pfn_retry(struct kvm *kvm, unsigned long uhva)
 	return new_pfn;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|1028| <<evtchn_set_fn>> rc = kvm_gfn_to_pfn_cache_refresh(kvm, gpc, gpc->gpa,
+ *   - virt/kvm/pfncache.c|322| <<kvm_gfn_to_pfn_cache_init>> return kvm_gfn_to_pfn_cache_refresh(kvm, gpc, gpa, len, dirty);
+ */
 int kvm_gfn_to_pfn_cache_refresh(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 				 gpa_t gpa, unsigned long len, bool dirty)
 {
@@ -268,6 +290,10 @@ int kvm_gfn_to_pfn_cache_refresh(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 }
 EXPORT_SYMBOL_GPL(kvm_gfn_to_pfn_cache_refresh);
 
+/*
+ * called by:
+ *   - virt/kvm/pfncache.c|333| <<kvm_gfn_to_pfn_cache_destroy>> kvm_gfn_to_pfn_cache_unmap(kvm, gpc);
+ */
 void kvm_gfn_to_pfn_cache_unmap(struct kvm *kvm, struct gfn_to_pfn_cache *gpc)
 {
 	void *old_khva;
@@ -298,6 +324,10 @@ void kvm_gfn_to_pfn_cache_unmap(struct kvm *kvm, struct gfn_to_pfn_cache *gpc)
 EXPORT_SYMBOL_GPL(kvm_gfn_to_pfn_cache_unmap);
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|42| <<kvm_xen_shared_info_init>> ret = kvm_gfn_to_pfn_cache_init(kvm, gpc, NULL, false, true,
+ */
 int kvm_gfn_to_pfn_cache_init(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 			      struct kvm_vcpu *vcpu, bool guest_uses_pa,
 			      bool kernel_map, gpa_t gpa, unsigned long len,
@@ -323,6 +353,11 @@ int kvm_gfn_to_pfn_cache_init(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 }
 EXPORT_SYMBOL_GPL(kvm_gfn_to_pfn_cache_init);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|37| <<kvm_xen_shared_info_init>> kvm_gfn_to_pfn_cache_destroy(kvm, gpc);
+ *   - arch/x86/kvm/xen.c|811| <<kvm_xen_destroy_vm>> kvm_gfn_to_pfn_cache_destroy(kvm, &kvm->arch.xen.shinfo_cache);
+ */
 void kvm_gfn_to_pfn_cache_destroy(struct kvm *kvm, struct gfn_to_pfn_cache *gpc)
 {
 	if (gpc->active) {
-- 
2.17.1

