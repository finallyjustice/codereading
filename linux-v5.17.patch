From ec9e4be6c4583a0c4700e36947a07dfdcbeda822 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 12 Apr 2022 23:33:38 -0700
Subject: [PATCH 1/1] linux v5.17

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/kvm/vgic/vgic-debug.c              |   4 +
 arch/arm64/kvm/vgic/vgic-init.c               |   7 +
 arch/x86/include/asm/kvm_host.h               |  37 ++
 arch/x86/kernel/kvm.c                         |   9 +
 arch/x86/kvm/mmu.h                            |  13 +
 arch/x86/kvm/mmu/mmu.c                        | 276 +++++++++++++++
 arch/x86/kvm/mmu/mmu_audit.c                  |   8 +
 arch/x86/kvm/mmu/mmu_internal.h               |  41 +++
 arch/x86/kvm/mmu/spte.c                       |   8 +
 arch/x86/kvm/mmu/spte.h                       |  39 ++
 arch/x86/kvm/mmu/tdp_iter.c                   |  97 +++++
 arch/x86/kvm/mmu/tdp_iter.h                   |  39 ++
 arch/x86/kvm/mmu/tdp_mmu.c                    | 332 ++++++++++++++++++
 arch/x86/kvm/mmu/tdp_mmu.h                    |  54 +++
 arch/x86/kvm/x86.c                            |  30 ++
 drivers/cpuidle/cpuidle-haltpoll.c            |  29 ++
 drivers/xen/swiotlb-xen.c                     |   5 +
 include/linux/kvm_dirty_ring.h                |  10 +
 kernel/dma/swiotlb.c                          |  81 +++++
 kernel/time/clocksource.c                     |   4 +
 .../selftests/kvm/kvm_page_table_test.c       |  20 ++
 tools/testing/selftests/kvm/lib/guest_modes.c |  10 +
 virt/kvm/dirty_ring.c                         |  24 ++
 virt/kvm/kvm_main.c                           |  54 +++
 24 files changed, 1231 insertions(+)

diff --git a/arch/arm64/kvm/vgic/vgic-debug.c b/arch/arm64/kvm/vgic/vgic-debug.c
index f38c40a76251..0ad6b9fa9591 100644
--- a/arch/arm64/kvm/vgic/vgic-debug.c
+++ b/arch/arm64/kvm/vgic/vgic-debug.c
@@ -269,6 +269,10 @@ static const struct seq_operations vgic_debug_sops = {
 
 DEFINE_SEQ_ATTRIBUTE(vgic_debug);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|320| <<vgic_init>> vgic_debug_init(kvm);
+ */
 void vgic_debug_init(struct kvm *kvm)
 {
 	debugfs_create_file("vgic-state", 0444, kvm->debugfs_dentry, kvm,
diff --git a/arch/arm64/kvm/vgic/vgic-init.c b/arch/arm64/kvm/vgic/vgic-init.c
index fc00304fe7d8..a73042109880 100644
--- a/arch/arm64/kvm/vgic/vgic-init.c
+++ b/arch/arm64/kvm/vgic/vgic-init.c
@@ -252,6 +252,13 @@ static void kvm_vgic_vcpu_enable(struct kvm_vcpu *vcpu)
  * vgic_initialized() returns true when this function has succeeded.
  * Must be called with kvm->lock held!
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|413| <<vgic_lazy_init>> ret = vgic_init(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|214| <<vgic_set_common_attr>> r = vgic_init(dev->kvm);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|372| <<vgic_v2_attr_regs_access>> ret = vgic_init(dev->kvm);
+ *   - arch/arm64/kvm/vgic/vgic-v2.c|309| <<vgic_v2_map_resources>> ret = vgic_init(kvm);
+ */
 int vgic_init(struct kvm *kvm)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ec9830d2aabf..35948c464076 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -129,8 +129,29 @@
 /* KVM Hugepage definitions for x86 */
 #define KVM_MAX_HUGEPAGE_LEVEL	PG_LEVEL_1G
 #define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)
+/*
+ * KVM_HPAGE_GFN_SHIFT(1) = (1 - 1) * 9 =  0
+ * KVM_HPAGE_GFN_SHIFT(2) = (2 - 1) * 9 =  9
+ * KVM_HPAGE_GFN_SHIFT(3) = (3 - 1) * 9 = 18
+ * KVM_HPAGE_GFN_SHIFT(4) = (4 - 1) * 9 = 27
+ * KVM_HPAGE_GFN_SHIFT(5) = (5 - 1) * 9 = 36
+ */
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+/*
+ * KVM_HPAGE_SHIFT(1) = 12 +  0 = 12
+ * KVM_HPAGE_SHIFT(2) = 12 +  9 = 21
+ * KVM_HPAGE_SHIFT(3) = 12 + 18 = 30
+ * KVM_HPAGE_SHIFT(4) = 12 + 27 = 38
+ * KVM_HPAGE_SHIFT(5) = 12 + 36 = 48
+ */
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
+/*
+ * KVM_HPAGE_SIZE(1) : 1 << 12 = 4K
+ * KVM_HPAGE_SIZE(2) : 1 << 21 = 2M
+ * KVM_HPAGE_SIZE(3) : 1 << 30 = 1G
+ * KVM_HPAGE_SIZE(4) : 1 << 38 = 256G
+ * KVM_HPAGE_SIZE(5) : 1 << 48 = 262144G
+ */
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
@@ -1047,6 +1068,16 @@ struct kvm_x86_msr_filter {
 #define APICV_INHIBIT_REASON_ABSENT	7
 
 struct kvm_arch {
+	/*
+	 * 在以下使用kvm_arch->n_used_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1720| <<kvm_mod_used_mmu_pages>> kvm->arch.n_used_mmu_pages += nr;
+	 *   - arch/x86/kvm/mmu/mmu.c|2589| <<kvm_mmu_available_pages>> if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
+	 *   - arch/x86/kvm/mmu/mmu.c|2591| <<kvm_mmu_available_pages>> kvm->arch.n_used_mmu_pages;
+	 *   - arch/x86/kvm/mmu/mmu.c|2627| <<kvm_mmu_change_mmu_pages>> if (kvm->arch.n_used_mmu_pages > goal_nr_mmu_pages) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2628| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
+	 *   - arch/x86/kvm/mmu/mmu.c|2631| <<kvm_mmu_change_mmu_pages>> goal_nr_mmu_pages = kvm->arch.n_used_mmu_pages;
+	 *   - arch/x86/kvm/mmu/mmu.c|6232| <<mmu_shrink_scan>> if (!kvm->arch.n_used_mmu_pages &&
+	 */
 	unsigned long n_used_mmu_pages;
 	unsigned long n_requested_mmu_pages;
 	unsigned long n_max_mmu_pages;
@@ -1177,6 +1208,12 @@ struct kvm_arch {
 	 * true, TDP MMU handler functions will run for various MMU
 	 * operations.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_enabled:
+	 *   - arch/x86/kvm/mmu.h|328| <<is_tdp_mmu_enabled>> static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return kvm->arch.tdp_mmu_enabled; }
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|23| <<kvm_mmu_init_tdp_mmu>> kvm->arch.tdp_mmu_enabled = true;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|56| <<kvm_mmu_uninit_tdp_mmu>> if (!kvm->arch.tdp_mmu_enabled)
+	 */
 	bool tdp_mmu_enabled;
 
 	/*
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index d77481ecb0d5..f1b5a94835b6 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -1108,6 +1108,15 @@ static void kvm_enable_host_haltpoll(void *i)
 	wrmsrl(MSR_KVM_POLL_CONTROL, 1);
 }
 
+/*
+ * [0] arch_haltpoll_enable
+ * [0] haltpoll_cpu_online
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 void arch_haltpoll_enable(unsigned int cpu)
 {
 	if (!kvm_para_has_feature(KVM_FEATURE_POLL_CONTROL)) {
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index e9fbb2c8bbe2..3c1f3cd4a7f7 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -136,6 +136,14 @@ struct kvm_page_fault {
 	 * Whether a >4KB mapping can be created or is forbidden due to NX
 	 * hugepages.
 	 */
+	/*
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2895| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|2913| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<__direct_map>> if (fault->is_tdp && fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|746| <<FNAME(fetch)>> if (fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1072| <<kvm_tdp_mmu_map>> fault->huge_page_disallowed &&
+	 */
 	bool huge_page_disallowed;
 
 	/*
@@ -176,6 +184,11 @@ static inline bool is_nx_huge_page_enabled(void)
 	return READ_ONCE(nx_huge_pages);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5511| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa,
+ *   - arch/x86/kvm/x86.c|12199| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true);
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u32 err, bool prefetch)
 {
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 5628d0ba637e..fc4981e664e0 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -57,6 +57,17 @@
 
 extern bool itlb_multihit_kvm_mitigation;
 
+/*
+ * 在以下使用nx_huge_pages:
+ *   - arch/x86/kvm/mmu/mmu.c|60| <<global>> int __read_mostly nx_huge_pages = -1;
+ *   - arch/x86/kvm/mmu/mmu.c|82| <<global>> module_param_cb(nx_huge_pages, &nx_huge_pages_ops, &nx_huge_pages, 0644);
+ *   - arch/x86/kvm/mmu/mmu.c|83| <<global>> __MODULE_PARM_TYPE(nx_huge_pages, "bool");
+ *   - arch/x86/kvm/mmu.h|184| <<is_nx_huge_page_enabled>> return READ_ONCE(nx_huge_pages);
+ *   - arch/x86/kvm/mmu/mmu.c|6328| <<__set_nx_huge_pages>> nx_huge_pages = itlb_multihit_kvm_mitigation = val;
+ *   - arch/x86/kvm/mmu/mmu.c|6333| <<set_nx_huge_pages>> bool old_val = nx_huge_pages;
+ *   - arch/x86/kvm/mmu/mmu.c|6374| <<kvm_mmu_module_init>> if (nx_huge_pages == -1)
+ *   - arch/x86/kvm/mmu/mmu.c|6441| <<calc_nx_huge_pages_recovery_period>> bool enabled = READ_ONCE(nx_huge_pages);
+ */
 int __read_mostly nx_huge_pages = -1;
 static uint __read_mostly nx_huge_pages_recovery_period_ms;
 #ifdef CONFIG_PREEMPT_RT
@@ -102,6 +113,12 @@ bool tdp_enabled = false;
 
 static int max_huge_page_level __read_mostly;
 static int tdp_root_level __read_mostly;
+/*
+ * 在以下使用max_tdp_level:
+ *   - arch/x86/kvm/mmu/mmu.c|4809| <<kvm_mmu_get_tdp_level>> if (max_tdp_level == 5 && cpuid_maxphyaddr(vcpu) <= 48)
+ *   - arch/x86/kvm/mmu/mmu.c|4812| <<kvm_mmu_get_tdp_level>> return max_tdp_level;
+ *   - arch/x86/kvm/mmu/mmu.c|5496| <<kvm_configure_mmu>> max_tdp_level = tdp_max_root_level;
+ */
 static int max_tdp_level __read_mostly;
 
 enum {
@@ -168,17 +185,30 @@ struct kvm_shadow_walk_iterator {
 	unsigned index;
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|954| <<FNAME(invlpg)>> for_each_shadow_entry_using_root(vcpu, root_hpa, gva, iterator) {
+ */
 #define for_each_shadow_entry_using_root(_vcpu, _root, _addr, _walker)     \
 	for (shadow_walk_init_using_root(&(_walker), (_vcpu),              \
 					 (_root), (_addr));                \
 	     shadow_walk_okay(&(_walker));			           \
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3058| <<__direct_map>> for_each_shadow_entry(vcpu, fault->addr, it) {
+ */
 #define for_each_shadow_entry(_vcpu, _addr, _walker)            \
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);	\
 	     shadow_walk_okay(&(_walker));			\
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3233| <<fast_pf_get_last_sptep>> for_each_shadow_entry_lockless(vcpu, gpa, iterator, old_spte) {
+ *   - arch/x86/kvm/mmu/mmu.c|4009| <<shadow_page_table_clear_flood>> for_each_shadow_entry_lockless(vcpu, addr, iterator, spte)
+ */
 #define for_each_shadow_entry_lockless(_vcpu, _addr, _walker, spte)	\
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);		\
 	     shadow_walk_okay(&(_walker)) &&				\
@@ -842,6 +872,12 @@ static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	kvm_mmu_gfn_disallow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2986| <<__direct_map>> account_huge_nx_page(vcpu->kvm, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|748| <<FNAME(fetch)>> account_huge_nx_page(vcpu->kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|345| <<tdp_mmu_link_page>> account_huge_nx_page(kvm, sp);
+ */
 void account_huge_nx_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	if (sp->lpage_disallowed)
@@ -1690,6 +1726,11 @@ static int is_empty_shadow_page(u64 *spt)
  * aggregate version in order to make the slab shrinker
  * faster
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1779| <<kvm_mmu_alloc_page>> kvm_mod_used_mmu_pages(vcpu->kvm, +1);
+ *   - arch/x86/kvm/mmu/mmu.c|2480| <<__kvm_mmu_prepare_zap_page>> kvm_mod_used_mmu_pages(kvm, -1);
+ */
 static inline void kvm_mod_used_mmu_pages(struct kvm *kvm, long nr)
 {
 	kvm->arch.n_used_mmu_pages += nr;
@@ -2062,6 +2103,13 @@ static void clear_sp_write_flooding_count(u64 *spte)
 	__clear_sp_write_flooding_count(sptep_to_sp(spte));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3096| <<__direct_map>> sp = kvm_mmu_get_page(vcpu, base_gfn, it.addr,
+ *   - arch/x86/kvm/mmu/mmu.c|3487| <<mmu_alloc_root>> sp = kvm_mmu_get_page(vcpu, gfn, gva, level, direct, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|686| <<FNAME(fetch)>> sp = kvm_mmu_get_page(vcpu, table_gfn, fault->addr,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> sp = kvm_mmu_get_page(vcpu, base_gfn, fault->addr,
+ */
 static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 					     gfn_t gfn,
 					     gva_t gaddr,
@@ -2161,10 +2209,24 @@ static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|172| <<for_each_shadow_entry_using_root>> for (shadow_walk_init_using_root(&(_walker), (_vcpu), \
+ *   - arch/x86/kvm/mmu/mmu.c|2202| <<shadow_walk_init>> shadow_walk_init_using_root(iterator, vcpu, vcpu->arch.mmu->root_hpa,
+ */
 static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterator,
 					struct kvm_vcpu *vcpu, hpa_t root,
 					u64 addr)
 {
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址
+	 *     int level;         // shadow_addr所在的level
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->addr = addr;
 	iterator->shadow_addr = root;
 	iterator->level = vcpu->arch.mmu->shadow_root_level;
@@ -2190,6 +2252,13 @@ static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterato
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|178| <<for_each_shadow_entry>> for (shadow_walk_init(&(_walker), _vcpu, _addr); \
+ *   - arch/x86/kvm/mmu/mmu.c|183| <<for_each_shadow_entry_lockless>> for (shadow_walk_init(&(_walker), _vcpu, _addr); \
+ *   - arch/x86/kvm/mmu/mmu.c|3815| <<get_walk>> for (shadow_walk_init(&iterator, vcpu, addr),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|674| <<FNAME(fetch)>> for (shadow_walk_init(&it, vcpu, fault->addr);
+ */
 static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,
 			     struct kvm_vcpu *vcpu, u64 addr)
 {
@@ -2197,16 +2266,40 @@ static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,
 				    addr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|174| <<for_each_shadow_entry_using_root>> shadow_walk_okay(&(_walker)); \
+ *   - arch/x86/kvm/mmu/mmu.c|179| <<for_each_shadow_entry>> shadow_walk_okay(&(_walker)); \
+ *   - arch/x86/kvm/mmu/mmu.c|184| <<for_each_shadow_entry_lockless>> shadow_walk_okay(&(_walker)) && \
+ *   - arch/x86/kvm/mmu/mmu.c|3817| <<get_walk>> shadow_walk_okay(&iterator);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|675| <<FNAME(fetch)>> shadow_walk_okay(&it) && it.level > gw->level;
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|724| <<FNAME(fetch)>> for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+ */
 static bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator)
 {
 	if (iterator->level < PG_LEVEL_4K)
 		return false;
 
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址 
+	 *     int level;         // shadow_addr所在的level
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->index = SHADOW_PT_INDEX(iterator->addr, iterator->level);
 	iterator->sptep	= ((u64 *)__va(iterator->shadow_addr)) + iterator->index;
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|186| <<for_each_shadow_entry_lockless>> __shadow_walk_next(&(_walker), spte))
+ *   - arch/x86/kvm/mmu/mmu.c|2230| <<shadow_walk_next>> __shadow_walk_next(iterator, *iterator->sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|3818| <<get_walk>> __shadow_walk_next(&iterator, spte)) {
+ */
 static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,
 			       u64 spte)
 {
@@ -2215,15 +2308,37 @@ static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,
 		return;
 	}
 
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址
+	 *     int level;         // shadow_addr所在的level
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->shadow_addr = spte & PT64_BASE_ADDR_MASK;
 	--iterator->level;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|175| <<for_each_shadow_entry_using_root>> shadow_walk_next(&(_walker)))
+ *   - arch/x86/kvm/mmu/mmu.c|180| <<for_each_shadow_entry>> shadow_walk_next(&(_walker)))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|676| <<FNAME(fetch)>> shadow_walk_next(&it)) {
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|724| <<FNAME(fetch)>> for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+ */
 static void shadow_walk_next(struct kvm_shadow_walk_iterator *iterator)
 {
 	__shadow_walk_next(iterator, *iterator->sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3005| <<__direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|717| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|745| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ */
 static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 			     struct kvm_mmu_page *sp)
 {
@@ -2241,6 +2356,10 @@ static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 		mark_unsync(sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|738| <<FNAME(fetch)>> validate_direct_spte(vcpu, it.sptep, direct_access);
+ */
 static void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 				   unsigned direct_access)
 {
@@ -2398,6 +2517,18 @@ static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 	return list_unstable;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1909| <<kvm_sync_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2114| <<kvm_mmu_get_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp,
+ *   - arch/x86/kvm/mmu/mmu.c|2294| <<mmu_page_zap_pte>> return kvm_mmu_prepare_zap_page(kvm, child,
+ *   - arch/x86/kvm/mmu/mmu.c|2340| <<mmu_zap_unsync_children>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2543| <<kvm_mmu_unprotect_page>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3276| <<mmu_free_root_page>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|5313| <<kvm_mmu_pte_write>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|6327| <<kvm_recover_nx_lpages>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmutrace.h|203| <<KVM_MMU_PAGE_ASSIGN>> DEFINE_EVENT(kvm_mmu_page_class, kvm_mmu_prepare_zap_page,
+ */
 static bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				     struct list_head *invalid_list)
 {
@@ -2432,6 +2563,12 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2619| <<make_mmu_pages_available>> kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm, KVM_REFILL_PAGES - avail);
+ *   - arch/x86/kvm/mmu/mmu.c|2644| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
+ *   - arch/x86/kvm/mmu/mmu.c|6261| <<mmu_shrink_scan>> freed = kvm_mmu_zap_oldest_mmu_pages(kvm, sc->nr_to_scan);
+ */
 static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 						  unsigned long nr_to_zap)
 {
@@ -2469,6 +2606,10 @@ static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 	return total_zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2614| <<make_mmu_pages_available>> if (!kvm_mmu_available_pages(vcpu->kvm))
+ */
 static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 {
 	if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
@@ -2478,6 +2619,13 @@ static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3522| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|3656| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|4211| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|908| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+ */
 static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 {
 	unsigned long avail = kvm_mmu_available_pages(vcpu->kvm);
@@ -2829,6 +2977,10 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 	__direct_pte_prefetch(vcpu, sp, sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2876| <<kvm_mmu_max_mapping_level>> host_level = host_pfn_mapping_level(kvm, gfn, pfn, slot);
+ */
 static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 				  const struct kvm_memory_slot *slot)
 {
@@ -2849,6 +3001,10 @@ static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 	 */
 	hva = __gfn_to_hva_memslot(slot, gfn);
 
+	/*
+	 * Lookup the page table entry for a virtual address in a given mm. Return a
+	 * pointer to the entry and the level of the mapping.
+	 */
 	pte = lookup_address_in_mm(kvm->mm, hva, &level);
 	if (unlikely(!pte))
 		return PG_LEVEL_4K;
@@ -2856,6 +3012,12 @@ static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 	return level;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2900| <<kvm_mmu_hugepage_adjust>> fault->req_level = kvm_mmu_max_mapping_level(vcpu->kvm, slot,
+ *   - arch/x86/kvm/mmu/mmu.c|5889| <<kvm_mmu_zap_collapsible_spte>> sp->role.level < kvm_mmu_max_mapping_level(kvm, slot, sp->gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1390| <<zap_collapsible_spte_range>> iter.level >= kvm_mmu_max_mapping_level(kvm, slot, iter.gfn,
+ */
 int kvm_mmu_max_mapping_level(struct kvm *kvm,
 			      const struct kvm_memory_slot *slot, gfn_t gfn,
 			      kvm_pfn_t pfn, int max_level)
@@ -2882,6 +3044,14 @@ void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	struct kvm_memory_slot *slot = fault->slot;
 	kvm_pfn_t mask;
 
+	/*
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2895| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|2913| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<__direct_map>> if (fault->is_tdp && fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|746| <<FNAME(fetch)>> if (fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1072| <<kvm_tdp_mmu_map>> fault->huge_page_disallowed &&
+	 */
 	fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
 
 	if (unlikely(fault->max_level == PG_LEVEL_4K))
@@ -2933,8 +3103,21 @@ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4103| <<direct_page_fault>> r = __direct_map(vcpu, fault);
+ */
 static int __direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;
+	 *     hpa_t shadow_addr;
+	 *     u64 *sptep;
+	 *     int level;
+	 *     unsigned index;
+	 * };
+	 */
 	struct kvm_shadow_walk_iterator it;
 	struct kvm_mmu_page *sp;
 	int ret;
@@ -3230,6 +3413,12 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3287| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->prev_roots[i].hpa,
+ *   - arch/x86/kvm/mmu/mmu.c|3293| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->root_hpa, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3299| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->pae_root[i],
+ */
 static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 			       struct list_head *invalid_list)
 {
@@ -3350,6 +3539,10 @@ static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, gva_t gva,
 	return __pa(sp->spt);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5129| <<kvm_mmu_load>> r = mmu_alloc_direct_roots(vcpu);
+ */
 static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -4003,6 +4196,11 @@ static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 	       mmu_notifier_retry_hva(vcpu->kvm, mmu_seq, fault->hva);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4121| <<nonpaging_page_fault>> return direct_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4169| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, fault);
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	bool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);
@@ -4061,6 +4259,10 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	return r;
 }
 
+/*
+ * 在以下使用nonpaging_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|4174| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+ */
 static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
 				struct kvm_page_fault *fault)
 {
@@ -4104,6 +4306,12 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 }
 EXPORT_SYMBOL_GPL(kvm_handle_page_fault);
 
+/*
+ * 在以下使用kvm_tdp_page_fault():
+ *   - arch/x86/kvm/mmu.h|199| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *   - arch/x86/kvm/mmu.h|208| <<kvm_mmu_do_page_fault>> return kvm_tdp_page_fault(vcpu, &fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4840| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	while (fault->max_level > PG_LEVEL_4K) {
@@ -5300,6 +5508,13 @@ static void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 	write_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4277| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+ *   - arch/x86/kvm/svm/svm.c|1789| <<npf_interception>> return kvm_mmu_page_fault(vcpu, fault_address, error_code,
+ *   - arch/x86/kvm/vmx/vmx.c|5430| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5451| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -5605,6 +5820,10 @@ int kvm_mmu_create(struct kvm_vcpu *vcpu)
 }
 
 #define BATCH_ZAP_PAGES	10
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5899| <<kvm_mmu_zap_all_fast>> kvm_zap_obsolete_pages(kvm);
+ */
 static void kvm_zap_obsolete_pages(struct kvm *kvm)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -5664,6 +5883,12 @@ static void kvm_zap_obsolete_pages(struct kvm *kvm)
  * not use any resource of the being-deleted slot or all slots
  * after calling the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5772| <<kvm_mmu_invalidate_zap_pages_in_memslot>> kvm_mmu_zap_all_fast(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|6070| <<kvm_mmu_invalidate_mmio_sptes>> kvm_mmu_zap_all_fast(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|6186| <<set_nx_huge_pages>> kvm_mmu_zap_all_fast(kvm);
+ */
 static void kvm_mmu_zap_all_fast(struct kvm *kvm)
 {
 	lockdep_assert_held(&kvm->slots_lock);
@@ -6092,6 +6317,11 @@ static struct shrinker mmu_shrinker = {
 	.seeks = DEFAULT_SEEKS * 10,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6369| <<kvm_mmu_module_init>> mmu_destroy_caches();
+ *   - arch/x86/kvm/mmu/mmu.c|6383| <<kvm_mmu_module_exit>> mmu_destroy_caches();
+ */
 static void mmu_destroy_caches(void)
 {
 	kmem_cache_destroy(pte_list_desc_cache);
@@ -6104,11 +6334,48 @@ static bool get_nx_auto_mode(void)
 	return boot_cpu_has_bug(X86_BUG_ITLB_MULTIHIT) && !cpu_mitigations_off();
 }
 
+/*
+ * commit b8e8c8303ff28c61046a4d0f6ea99aea609a7dc0
+ * Author: Paolo Bonzini <pbonzini@redhat.com>
+ * Date:   Mon Nov 4 12:22:02 2019 +0100
+ *
+ * kvm: mmu: ITLB_MULTIHIT mitigation
+ *
+ * With some Intel processors, putting the same virtual address in the TLB
+ * as both a 4 KiB and 2 MiB page can confuse the instruction fetch unit
+ * and cause the processor to issue a machine check resulting in a CPU lockup.
+ *
+ * Unfortunately when EPT page tables use huge pages, it is possible for a
+ * malicious guest to cause this situation.
+ *
+ * Add a knob to mark huge pages as non-executable. When the nx_huge_pages
+ * parameter is enabled (and we are using EPT), all huge pages are marked as
+ * NX. If the guest attempts to execute in one of those pages, the page is
+ * broken down into 4K pages, which are then marked executable.
+ *
+ * This is not an issue for shadow paging (except nested EPT), because then
+ * the host is in control of TLB flushes and the problematic situation cannot
+ * happen.  With nested EPT, again the nested guest can cause problems shadow
+ * and direct EPT is treated in the same way.
+ *
+ * [ tglx: Fixup default to auto and massage wording a bit ]
+ *
+ * Originally-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ * Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6346| <<set_nx_huge_pages>> __set_nx_huge_pages(new_val);
+ *   - arch/x86/kvm/mmu/mmu.c|6375| <<kvm_mmu_module_init>> __set_nx_huge_pages(get_nx_auto_mode());
+ */
 static void __set_nx_huge_pages(bool val)
 {
 	nx_huge_pages = itlb_multihit_kvm_mitigation = val;
 }
 
+/*
+ * struct kernel_param_ops nx_huge_pages_ops.set = set_nx_huge_pages()
+ */
 static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
 {
 	bool old_val = nx_huge_pages;
@@ -6144,6 +6411,15 @@ static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8844| <<kvm_arch_init>> r = kvm_mmu_module_init();
+ *
+ * vmx_init()
+ * -> kvm_init()
+ *    -> kvm_arch_init()
+ *       -> kvm_mmu_module_init()
+ */
 int kvm_mmu_module_init(void)
 {
 	int ret = -ENOMEM;
diff --git a/arch/x86/kvm/mmu/mmu_audit.c b/arch/x86/kvm/mmu/mmu_audit.c
index 9e7dcf999f08..b62833c457f0 100644
--- a/arch/x86/kvm/mmu/mmu_audit.c
+++ b/arch/x86/kvm/mmu/mmu_audit.c
@@ -233,6 +233,14 @@ static void audit_vcpu_spte(struct kvm_vcpu *vcpu)
 	mmu_spte_walk(vcpu, audit_spte);
 }
 
+/*
+ * 在以下使用mmu_audit:
+ *   - arch/x86/kvm/mmu/mmu_audit.c|303| <<global>> arch_param_cb(mmu_audit, &audit_param_ops, &mmu_audit, 0644);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|259| <<mmu_audit_enable>> if (mmu_audit)
+ *   - arch/x86/kvm/mmu/mmu_audit.c|263| <<mmu_audit_enable>> mmu_audit = true;
+ *   - arch/x86/kvm/mmu/mmu_audit.c|268| <<mmu_audit_disable>> if (!mmu_audit)
+ *   - arch/x86/kvm/mmu/mmu_audit.c|272| <<mmu_audit_disable>> mmu_audit = false;
+ */
 static bool mmu_audit;
 static DEFINE_STATIC_KEY_FALSE(mmu_audit_key);
 
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index da6166b5c377..293ef4ed05c8 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -55,7 +55,26 @@ struct kvm_mmu_page {
 	gfn_t *gfns;
 	/* Currently serving as active root */
 	union {
+		/*
+		 * 在以下设置kvm_mmu_page->root_count:
+		 *   - arch/x86/kvm/mmu/mmu.c|3358| <<mmu_alloc_root>> ++sp->root_count;
+		 *   - arch/x86/kvm/mmu/mmu.c|3255| <<mmu_free_root_page>> else if (!--sp->root_count && sp->role.invalid)
+		 * 在以下使用kvm_mmu_page->root_count:
+		 *   - arch/x86/kvm/mmu/mmu.c|2364| <<__kvm_mmu_prepare_zap_page>> if (!sp->root_count) {
+		 *   - arch/x86/kvm/mmu/mmu.c|2430| <<kvm_mmu_commit_zap_page>> WARN_ON(!sp->role.invalid || sp->root_count);
+		 *   - arch/x86/kvm/mmu/mmu.c|2453| <<kvm_mmu_zap_oldest_mmu_pages>> if (sp->root_count)
+		 *   - arch/x86/kvm/mmu/mmutrace.h|22| <<KVM_MMU_PAGE_ASSIGN>> __entry->root_count = sp->root_count; \
+		 *   - arch/x86/kvm/mmu/tdp_mmu.h|108| <<is_tdp_mmu>> return sp && is_tdp_mmu_page(sp) && sp->root_count;
+		 */
 		int root_count;
+		/*
+		 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+		 */
 		refcount_t tdp_mmu_root_count;
 	};
 	unsigned int unsync_children;
@@ -82,6 +101,28 @@ struct kvm_mmu_page {
 
 extern struct kmem_cache *mmu_page_header_cache;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1836| <<__mmu_unsync_walk>> child = to_shadow_page(ent & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2257| <<validate_direct_spte>> child = to_shadow_page(*sptep & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2278| <<mmu_page_zap_pte>> child = to_shadow_page(pte & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2710| <<mmu_set_spte>> child = to_shadow_page(pte & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|3251| <<mmu_free_root_page>> sp = to_shadow_page(*root_hpa & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|3330| <<kvm_mmu_free_guest_mode_roots>> if (!to_shadow_page(root_hpa) ||
+ *   - arch/x86/kvm/mmu/mmu.c|3331| <<kvm_mmu_free_guest_mode_roots>> to_shadow_page(root_hpa)->role.guest_mode)
+ *   - arch/x86/kvm/mmu/mmu.c|3672| <<is_unsync_root>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3694| <<kvm_mmu_sync_roots>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3717| <<kvm_mmu_sync_roots>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3995| <<is_page_fault_stale>> struct kvm_mmu_page *sp = to_shadow_page(vcpu->arch.mmu->root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4145| <<is_root_usable>> VALID_PAGE(root->hpa) && to_shadow_page(root->hpa) &&
+ *   - arch/x86/kvm/mmu/mmu.c|4146| <<is_root_usable>> role.word == to_shadow_page(root->hpa)->role.word;
+ *   - arch/x86/kvm/mmu/mmu.c|4235| <<__kvm_mmu_new_pgd>> to_shadow_page(vcpu->arch.mmu->root_hpa));
+ *   - arch/x86/kvm/mmu/mmu_audit.c|48| <<__mmu_spte_walk>> child = to_shadow_page(ent[i] & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|65| <<mmu_spte_walk>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|75| <<mmu_spte_walk>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|94| <<sptep_to_sp>> return to_shadow_page(__pa(sptep));
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|107| <<is_tdp_mmu>> sp = to_shadow_page(hpa);
+ */
 static inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)
 {
 	struct page *page = pfn_to_page(shadow_page >> PAGE_SHIFT);
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 73cfe62fdad1..b9a9a6ae70cf 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -194,6 +194,14 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 
 u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 {
+	/*
+	 * 在以下使用SPTE_MMU_PRESENT_MASK:
+	 *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+	 *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+	 */
 	u64 spte = SPTE_MMU_PRESENT_MASK;
 
 	spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK |
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index be6a007a4af3..7b6fd2fd97d8 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -12,6 +12,14 @@
  * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
  * enough that the improved code generation is noticeable in KVM's footprint.
  */
+/*
+ * 在以下使用SPTE_MMU_PRESENT_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+ *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+ */
 #define SPTE_MMU_PRESENT_MASK		BIT_ULL(11)
 
 /*
@@ -53,11 +61,24 @@ static_assert(SPTE_TDP_AD_ENABLED_MASK == 0);
 
 #define PT64_LEVEL_BITS 9
 
+/*
+ * PT64_LEVEL_SHIFT(1) = 12 + 0 * 9 = 12
+ * PT64_LEVEL_SHIFT(2) = 12 + 1 * 9 = 21
+ * PT64_LEVEL_SHIFT(3) = 12 + 2 * 9 = 30
+ * PT64_LEVEL_SHIFT(4) = 12 + 3 * 9 = 39
+ * PT64_LEVEL_SHIFT(5) = 12 + 4 * 9 = 48
+ */
 #define PT64_LEVEL_SHIFT(level) \
 		(PAGE_SHIFT + (level - 1) * PT64_LEVEL_BITS)
 
 #define PT64_INDEX(address, level)\
 	(((address) >> PT64_LEVEL_SHIFT(level)) & ((1 << PT64_LEVEL_BITS) - 1))
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2211| <<shadow_walk_okay>> iterator->index = SHADOW_PT_INDEX(iterator->addr, iterator->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|26| <<tdp_iter_refresh_sptep>> SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|181| <<try_step_side>> if (SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level) ==
+ */
 #define SHADOW_PT_INDEX(addr, level) PT64_INDEX(addr, level)
 
 /*
@@ -201,6 +222,16 @@ extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
  *
  * Only used by the TDP MMU.
  */
+/*
+ * 在以下使用REMOVED_SPTE:
+ *   - arch/x86/kvm/mmu/spte.h|215| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|299| <<kvm_mmu_set_mmio_spte_mask>> WARN_ON(mmio_value && (REMOVED_SPTE & mmio_mask) == mmio_value))
+ *   - arch/x86/kvm/mmu/spte.h|219| <<is_removed_spte>> return spte == REMOVED_SPTE;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|453| <<handle_removed_tdp_mmu_page>> old_child_spte = xchg(sptep, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|480| <<handle_removed_tdp_mmu_page>> WRITE_ONCE(*sptep, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|483| <<handle_removed_tdp_mmu_page>> old_child_spte, REMOVED_SPTE, level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|668| <<tdp_mmu_zap_spte_atomic>> if (!tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE))
+ */
 #define REMOVED_SPTE	0x5a0ULL
 
 /* Removed SPTEs must not be misconstrued as shadow present PTEs. */
@@ -235,6 +266,14 @@ static inline bool is_mmio_spte(u64 spte)
 
 static inline bool is_shadow_present_pte(u64 pte)
 {
+	/*
+	 * 在以下使用SPTE_MMU_PRESENT_MASK:
+	 *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+	 *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+	 */
 	return !!(pte & SPTE_MMU_PRESENT_MASK);
 }
 
diff --git a/arch/x86/kvm/mmu/tdp_iter.c b/arch/x86/kvm/mmu/tdp_iter.c
index caa96c270b95..5e83b9ddf130 100644
--- a/arch/x86/kvm/mmu/tdp_iter.c
+++ b/arch/x86/kvm/mmu/tdp_iter.c
@@ -8,13 +8,31 @@
  * Recalculates the pointer to the SPTE for the current GFN and level and
  * reread the SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|34| <<tdp_iter_restart>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|99| <<try_step_down>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|141| <<try_step_up>> tdp_iter_refresh_sptep(iter);
+ */
 static void tdp_iter_refresh_sptep(struct tdp_iter *iter)
 {
+	/*
+	 * tdp_iter->sptep:
+	 *    A pointer to the current SPTE
+	 * tdp_iter->pt_path[PT64_ROOT_MAX_LEVEL]:
+	 *    Pointers to the page tables traversed to reach the current SPTE
+	 */
 	iter->sptep = iter->pt_path[iter->level - 1] +
 		SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|33| <<tdp_iter_restart>> iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|98| <<try_step_down>> iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|140| <<try_step_up>> iter->gfn = round_gfn_for_level(iter->gfn, iter->level);
+ */
 static gfn_t round_gfn_for_level(gfn_t gfn, int level)
 {
 	return gfn & -KVM_PAGES_PER_HPAGE(level);
@@ -24,12 +42,30 @@ static gfn_t round_gfn_for_level(gfn_t gfn, int level)
  * Return the TDP iterator to the root PT and allow it to continue its
  * traversal over the paging structure from there.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|55| <<tdp_iter_start>> tdp_iter_restart(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|165| <<tdp_iter_next>> tdp_iter_restart(iter);
+ */
 void tdp_iter_restart(struct tdp_iter *iter)
 {
 	iter->yielded = false;
+	/*
+	 * 在以下使用tdp_iter->yielded_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|661| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 */
 	iter->yielded_gfn = iter->next_last_level_gfn;
 	iter->level = iter->root_level;
 
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 *
+	 * tdp_iter->gfn:
+	 *    The lowest GFN mapped by the current SPTE
+	 */
 	iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
 	tdp_iter_refresh_sptep(iter);
 
@@ -40,6 +76,10 @@ void tdp_iter_restart(struct tdp_iter *iter)
  * Sets a TDP iterator to walk a pre-order traversal of the paging structure
  * rooted at root_pt, starting with the walk to translate next_last_level_gfn.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|61| <<for_each_tdp_pte_min_level>> for (tdp_iter_start(&iter, root, root_level, min_level, start); \
+ */
 void tdp_iter_start(struct tdp_iter *iter, u64 *root_pt, int root_level,
 		    int min_level, gfn_t next_last_level_gfn)
 {
@@ -47,7 +87,13 @@ void tdp_iter_start(struct tdp_iter *iter, u64 *root_pt, int root_level,
 	WARN_ON(root_level > PT64_ROOT_MAX_LEVEL);
 
 	iter->next_last_level_gfn = next_last_level_gfn;
+	/*
+	 * The level of the root page given to the iterator
+	 */
 	iter->root_level = root_level;
+	/*
+	 * The lowest level the iterator should traverse to
+	 */
 	iter->min_level = min_level;
 	iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root_pt;
 	iter->as_id = kvm_mmu_page_as_id(sptep_to_sp(root_pt));
@@ -76,6 +122,10 @@ tdp_ptep_t spte_to_child_pt(u64 spte, int level)
  * Steps down one level in the paging structure towards the goal GFN. Returns
  * true if the iterator was able to step down a level, false otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|220| <<tdp_iter_next>> if (try_step_down(iter))
+ */
 static bool try_step_down(struct tdp_iter *iter)
 {
 	tdp_ptep_t child_pt;
@@ -87,14 +137,28 @@ static bool try_step_down(struct tdp_iter *iter)
 	 * Reread the SPTE before stepping down to avoid traversing into page
 	 * tables that are no longer linked from this entry.
 	 */
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 
+	/*
+	 * 这里得到的是当前pte所指向的下一个page table的page
+	 */
 	child_pt = spte_to_child_pt(iter->old_spte, iter->level);
 	if (!child_pt)
 		return false;
 
 	iter->level--;
 	iter->pt_path[iter->level - 1] = child_pt;
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
 	tdp_iter_refresh_sptep(iter);
 
@@ -118,9 +182,27 @@ static bool try_step_side(struct tdp_iter *iter)
             (PT64_ENT_PER_PAGE - 1))
 		return false;
 
+	/*
+	 * KVM_HPAGE_SIZE(1) : 1 << 12 = 4K
+	 * KVM_HPAGE_SIZE(2) : 1 << 21 = 2M
+	 * KVM_HPAGE_SIZE(3) : 1 << 30 = 1G
+	 * KVM_HPAGE_SIZE(4) : 1 << 38 = 256G
+	 * KVM_HPAGE_SIZE(5) : 1 << 48 = 262144G
+	 */
 	iter->gfn += KVM_PAGES_PER_HPAGE(iter->level);
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	iter->next_last_level_gfn = iter->gfn;
 	iter->sptep++;
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 
 	return true;
@@ -159,8 +241,23 @@ static bool try_step_up(struct tdp_iter *iter)
  *    SPTE will have already been visited, and so the iterator must also step
  *    to the side again.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|63| <<for_each_tdp_pte_min_level>> tdp_iter_next(&iter))
+ */
 void tdp_iter_next(struct tdp_iter *iter)
 {
+	/*
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 * 在以下使用tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+	 */
 	if (iter->yielded) {
 		tdp_iter_restart(iter);
 		return;
diff --git a/arch/x86/kvm/mmu/tdp_iter.h b/arch/x86/kvm/mmu/tdp_iter.h
index e19cabbcb65c..4aa6800c2650 100644
--- a/arch/x86/kvm/mmu/tdp_iter.h
+++ b/arch/x86/kvm/mmu/tdp_iter.h
@@ -17,12 +17,22 @@ struct tdp_iter {
 	 * The iterator will traverse the paging structure towards the mapping
 	 * for this GFN.
 	 */
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	gfn_t next_last_level_gfn;
 	/*
 	 * The next_last_level_gfn at the time when the thread last
 	 * yielded. Only yielding when the next_last_level_gfn !=
 	 * yielded_gfn helps ensure forward progress.
 	 */
+	/*
+	 * 在以下使用tdp_iter->yielded_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|661| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 */
 	gfn_t yielded_gfn;
 	/* Pointers to the page tables traversed to reach the current SPTE */
 	tdp_ptep_t pt_path[PT64_ROOT_MAX_LEVEL];
@@ -39,6 +49,12 @@ struct tdp_iter {
 	/* The address space ID, i.e. SMM vs. regular. */
 	int as_id;
 	/* A snapshot of the value at sptep */
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	u64 old_spte;
 	/*
 	 * Whether the iterator has a valid state. This will be false if the
@@ -50,6 +66,17 @@ struct tdp_iter {
 	 * which case tdp_iter_next() needs to restart the walk at the root
 	 * level instead of advancing to the next entry.
 	 */
+	/*
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 * 在以下使用tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+	 */
 	bool yielded;
 };
 
@@ -57,11 +84,23 @@ struct tdp_iter {
  * Iterates over every SPTE mapping the GFN range [start, end) in a
  * preorder traversal.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|66| <<for_each_tdp_pte>> for_each_tdp_pte_min_level(iter, root, root_level, PG_LEVEL_4K, start, end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|727| <<zap_gfn_range>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1183| <<wrprot_gfn_range>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1439| <<write_protect_gfn>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ */
 #define for_each_tdp_pte_min_level(iter, root, root_level, min_level, start, end) \
 	for (tdp_iter_start(&iter, root, root_level, min_level, start); \
 	     iter.valid && iter.gfn < end;		     \
 	     tdp_iter_next(&iter))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|627| <<tdp_root_for_each_pte>> for_each_tdp_pte(_iter, _root->spt, _root->role.level, _start, _end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|637| <<tdp_mmu_for_each_pte>> for_each_tdp_pte(_iter, __va(_mmu->root_hpa), \
+ */
 #define for_each_tdp_pte(iter, root, root_level, start, end) \
 	for_each_tdp_pte_min_level(iter, root, root_level, PG_LEVEL_4K, start, end)
 
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index bc9e3553fba2..a95ad8b5d827 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -10,6 +10,12 @@
 #include <asm/cmpxchg.h>
 #include <trace/events/kvm.h>
 
+/*
+ * 在以下使用tdp_mmu_enabled:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|13| <<global>> static bool __read_mostly tdp_mmu_enabled = true;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|14| <<global>> module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0644);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|19| <<kvm_mmu_init_tdp_mmu>> if (!tdp_enabled || !READ_ONCE(tdp_mmu_enabled))
+ */
 static bool __read_mostly tdp_mmu_enabled = true;
 module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0644);
 
@@ -29,15 +35,32 @@ bool kvm_mmu_init_tdp_mmu(struct kvm *kvm)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|85| <<kvm_tdp_mmu_put_root>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|723| <<zap_gfn_range>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ */
 static __always_inline void kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
 							     bool shared)
 {
+	/*
+	 * struct kvm *kvm:
+	 *     #ifdef KVM_HAVE_MMU_RWLOCK
+	 *     rwlock_t mmu_lock;
+	 *     #else
+	 *     spinlock_t mmu_lock;
+	 *     #endif
+	 */
 	if (shared)
 		lockdep_assert_held_read(&kvm->mmu_lock);
 	else
 		lockdep_assert_held_write(&kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5794| <<kvm_mmu_uninit_vm>> kvm_mmu_uninit_tdp_mmu(kvm);
+ */
 void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 {
 	if (!kvm->arch.tdp_mmu_enabled)
@@ -50,6 +73,9 @@ void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 	 * Ensure that all the outstanding RCU callbacks to free shadow pages
 	 * can run before the VM is torn down.
 	 */
+	/*
+	 * 注释: Wait until all in-flight call_rcu() callbacks complete.
+	 */
 	rcu_barrier();
 }
 
@@ -57,6 +83,11 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			  gfn_t start, gfn_t end, bool can_yield, bool flush,
 			  bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|84| <<tdp_mmu_free_sp_rcu_callback>> tdp_mmu_free_sp(sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1080| <<kvm_tdp_mmu_map>> tdp_mmu_free_sp(sp);
+ */
 static void tdp_mmu_free_sp(struct kvm_mmu_page *sp)
 {
 	free_page((unsigned long)sp->spt);
@@ -71,6 +102,11 @@ static void tdp_mmu_free_sp(struct kvm_mmu_page *sp)
  * section, and freeing it after a grace period, lockless access to that
  * memory won't use it after it is freed.
  */
+/*
+ * 在以下使用tdp_mmu_free_sp_rcu_callback():
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|109| <<kvm_tdp_mmu_put_root>> call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|417| <<handle_removed_tdp_mmu_page>> call_rcu(&sp->rcu_head, tdp_mmu_free_sp_rcu_callback);
+ */
 static void tdp_mmu_free_sp_rcu_callback(struct rcu_head *head)
 {
 	struct kvm_mmu_page *sp = container_of(head, struct kvm_mmu_page,
@@ -79,6 +115,12 @@ static void tdp_mmu_free_sp_rcu_callback(struct rcu_head *head)
 	tdp_mmu_free_sp(sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3254| <<mmu_free_root_page>> kvm_tdp_mmu_put_root(kvm, sp, false);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|136| <<tdp_mmu_next_root>> kvm_tdp_mmu_put_root(kvm, prev_root, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|860| <<kvm_tdp_mmu_zap_invalidated_roots>> kvm_tdp_mmu_put_root(kvm, root, true);
+ */
 void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 			  bool shared)
 {
@@ -93,6 +135,23 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 	list_del_rcu(&root->link);
 	spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
 
+	/*
+	 * Tears down the mappings for the range of gfns, [start, end), and frees the
+	 * non-root pages mapping GFNs strictly within that range. Returns true if
+	 * SPTEs have been cleared and a TLB flush is needed before releasing the
+	 * MMU lock.
+	 *
+	 * If can_yield is true, will release the MMU lock and reschedule if the
+	 * scheduler needs the CPU or there is contention on the MMU lock. If this
+	 * function cannot yield, it will not release the MMU lock or reschedule and
+	 * the caller must ensure it does not supply too large a GFN range, or the
+	 * operation can cause a soft lockup.
+	 *
+	 * If shared is true, this thread holds the MMU lock in read mode and must
+	 * account for the possibility that other threads are modifying the paging
+	 * structures concurrently. If shared is false, this thread should hold the
+	 * MMU lock in write mode.
+	 */
 	zap_gfn_range(kvm, root, 0, -1ull, false, false, shared);
 
 	call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
@@ -105,6 +164,11 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
  * that reference will be dropped. If no valid root is found, this
  * function will return NULL.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|152| <<for_each_tdp_mmu_root_yield_safe>> for (_root = tdp_mmu_next_root(_kvm, NULL, _shared); \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|154| <<for_each_tdp_mmu_root_yield_safe>> _root = tdp_mmu_next_root(_kvm, _root, _shared)) \
+ */
 static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 					      struct kvm_mmu_page *prev_root,
 					      bool shared)
@@ -113,6 +177,10 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 
 	rcu_read_lock();
 
+	/*
+	 * 关于list_next_or_null_rcu()注意:
+	 * Note that if the ptr is at the end of the list, NULL is returned.
+	 */
 	if (prev_root)
 		next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
 						  &prev_root->link,
@@ -143,6 +211,18 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
  * mode. In the unlikely event that this thread must free a root, the lock
  * will be temporarily dropped and reacquired in write mode.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|788| <<__kvm_tdp_mmu_zap_gfn_range>> for_each_tdp_mmu_root_yield_safe(kvm, root, as_id, false)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1047| <<kvm_tdp_mmu_unmap_gfn_range>> for_each_tdp_mmu_root_yield_safe(kvm, root, range->slot->as_id, false)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1234| <<kvm_tdp_mmu_wrprot_slot>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1304| <<kvm_tdp_mmu_clear_dirty_slot>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1429| <<kvm_tdp_mmu_zap_collapsible_sptes>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *
+ * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+ * 前者不需要锁, 后者需要锁
+ * 主要是用第二个参数_root "struct kvm_mmu_page *"
+ */
 #define for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared)	\
 	for (_root = tdp_mmu_next_root(_kvm, NULL, _shared);		\
 	     _root;							\
@@ -150,6 +230,17 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 		if (kvm_mmu_page_as_id(_root) != _as_id) {		\
 		} else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|209| <<kvm_tdp_mmu_get_vcpu_root_hpa>> for_each_tdp_mmu_root(kvm, root, kvm_mmu_role_as_id(role)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1071| <<kvm_tdp_mmu_handle_gfn>> for_each_tdp_mmu_root(kvm, root, range->slot->as_id) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1370| <<kvm_tdp_mmu_clear_dirty_pt_masked>> for_each_tdp_mmu_root(kvm, root, slot->as_id)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1483| <<kvm_tdp_mmu_write_protect_gfn>> for_each_tdp_mmu_root(kvm, root, slot->as_id)
+ *
+ * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+ * 前者不需要锁, 后者需要锁
+ * 主要是用第二个参数_root "struct kvm_mmu_page *"
+ */
 #define for_each_tdp_mmu_root(_kvm, _root, _as_id)				\
 	list_for_each_entry_rcu(_root, &_kvm->arch.tdp_mmu_roots, link,		\
 				lockdep_is_held_type(&kvm->mmu_lock, 0) ||	\
@@ -190,6 +281,14 @@ static struct kvm_mmu_page *alloc_tdp_mmu_page(struct kvm_vcpu *vcpu, gfn_t gfn,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3377| <<mmu_alloc_direct_roots>> root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);
+ *
+ * kvm_mmu_load()
+ * -> mmu_alloc_direct_roots()
+ *    -> kvm_tdp_mmu_get_vcpu_root_hpa()
+ */
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 {
 	union kvm_mmu_page_role role;
@@ -201,6 +300,10 @@ hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 	role = page_role_for_level(vcpu, vcpu->arch.mmu->shadow_root_level);
 
 	/* Check for an existing root before allocating a new one. */
+	/*
+	 * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+	 * 前者不需要锁, 后者需要锁
+	 */
 	for_each_tdp_mmu_root(kvm, root, kvm_mmu_role_as_id(role)) {
 		if (root->role.word == role.word &&
 		    kvm_tdp_mmu_get_root(kvm, root))
@@ -222,6 +325,12 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|562| <<handle_changed_spte>> handle_changed_spte_acc_track(old_spte, new_spte, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|603| <<tdp_mmu_set_spte_atomic>> handle_changed_spte_acc_track(iter->old_spte, new_spte, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|681| <<__tdp_mmu_set_spte>> handle_changed_spte_acc_track(iter->old_spte, new_spte,
+ */
 static void handle_changed_spte_acc_track(u64 old_spte, u64 new_spte, int level)
 {
 	if (!is_shadow_present_pte(old_spte) || !is_last_spte(old_spte, level))
@@ -259,6 +368,10 @@ static void handle_changed_spte_dirty_log(struct kvm *kvm, int as_id, gfn_t gfn,
  * @account_nx: This page replaces a NX large page and should be marked for
  *		eventual reclaim.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<kvm_tdp_mmu_map>> tdp_mmu_link_page(vcpu->kvm, sp,
+ */
 static void tdp_mmu_link_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 			      bool account_nx)
 {
@@ -278,6 +391,10 @@ static void tdp_mmu_link_page(struct kvm *kvm, struct kvm_mmu_page *sp,
  *	    the MMU lock and the operation must synchronize with other
  *	    threads that might be adding or removing pages.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|401| <<handle_removed_tdp_mmu_page>> tdp_mmu_unlink_page(kvm, sp, shared);
+ */
 static void tdp_mmu_unlink_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				bool shared)
 {
@@ -311,6 +428,10 @@ static void tdp_mmu_unlink_page(struct kvm *kvm, struct kvm_mmu_page *sp,
  * this thread will be responsible for ensuring the page is freed. Hence the
  * early rcu_dereferences in the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|552| <<__handle_changed_spte>> handle_removed_tdp_mmu_page(kvm, spte_to_child_pt(old_spte, level), shared);
+ */
 static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
 					bool shared)
 {
@@ -323,6 +444,9 @@ static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
 
 	tdp_mmu_unlink_page(kvm, sp, shared);
 
+	/*
+	 * 1 << 9 = 512
+	 */
 	for (i = 0; i < PT64_ENT_PER_PAGE; i++) {
 		u64 *sptep = rcu_dereference(pt) + i;
 		gfn_t gfn = base_gfn + i * KVM_PAGES_PER_HPAGE(level);
@@ -393,6 +517,12 @@ static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
  * Handle bookkeeping that might result from the modification of a SPTE.
  * This function must be called for all TDP SPTE modifications.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<handle_changed_spte>> __handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|601| <<tdp_mmu_set_spte_atomic>> __handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|678| <<__tdp_mmu_set_spte>> __handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+ */
 static void __handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				  u64 old_spte, u64 new_spte, int level,
 				  bool shared)
@@ -476,6 +606,10 @@ static void __handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				spte_to_child_pt(old_spte, level), shared);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|447| <<handle_removed_tdp_mmu_page>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_child_spte, REMOVED_SPTE, level, shared);
+ */
 static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared)
@@ -498,6 +632,14 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
  * Returns: true if the SPTE was set, false if it was not. If false is returned,
  *	    this function will have no side-effects.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|668| <<tdp_mmu_zap_spte_atomic>> if (!tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1156| <<tdp_mmu_map_handle_target_level>> else if (!tdp_mmu_set_spte_atomic(vcpu->kvm, iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1256| <<kvm_tdp_mmu_map>> if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter, new_spte)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1444| <<wrprot_gfn_range>> if (!tdp_mmu_set_spte_atomic(kvm, &iter, new_spte)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1512| <<clear_dirty_gfn_range>> if (!tdp_mmu_set_spte_atomic(kvm, &iter, new_spte)) {
+ */
 static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 					   struct tdp_iter *iter,
 					   u64 new_spte)
@@ -521,6 +663,10 @@ static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 		      new_spte) != iter->old_spte)
 		return false;
 
+	/*
+	 * Handle bookkeeping that might result from the modification of a SPTE.
+	 * This function must be called for all TDP SPTE modifications.
+	 */
 	__handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			      new_spte, iter->level, true);
 	handle_changed_spte_acc_track(iter->old_spte, new_spte, iter->level);
@@ -528,6 +674,12 @@ static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|810| <<zap_gfn_range>> } else if (!tdp_mmu_zap_spte_atomic(kvm, &iter)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1044| <<kvm_tdp_mmu_map>> if (!tdp_mmu_zap_spte_atomic(vcpu->kvm, &iter))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1460| <<zap_collapsible_spte_range>> if (!tdp_mmu_zap_spte_atomic(kvm, &iter)) {
+ */
 static inline bool tdp_mmu_zap_spte_atomic(struct kvm *kvm,
 					   struct tdp_iter *iter)
 {
@@ -577,6 +729,15 @@ static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 				      u64 new_spte, bool record_acc_track,
 				      bool record_dirty_log)
 {
+	/*
+	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+	 * which case tdp_iter_next() needs to restart the walk at the root
+	 * level instead of advancing to the next entry.
+	 *
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 */
 	WARN_ON_ONCE(iter->yielded);
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
@@ -588,10 +749,18 @@ static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 	 * should be used. If operating under the MMU lock in write mode, the
 	 * use of the removed SPTE should not be necessary.
 	 */
+	/*
+	 * return spte == REMOVED_SPTE
+	 */
 	WARN_ON(is_removed_spte(iter->old_spte));
 
 	WRITE_ONCE(*rcu_dereference(iter->sptep), new_spte);
 
+	/*
+	 * 应该就是记录stat吧
+	 * Handle bookkeeping that might result from the modification of a SPTE.
+	 * This function must be called for all TDP SPTE modifications.
+	 */
 	__handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			      new_spte, iter->level, false);
 	if (record_acc_track)
@@ -603,12 +772,23 @@ static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 					      iter->level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|852| <<zap_gfn_range>> tdp_mmu_set_spte(kvm, &iter, 0);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<set_spte_gfn>> tdp_mmu_set_spte(kvm, iter, 0);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1258| <<set_spte_gfn>> tdp_mmu_set_spte(kvm, iter, new_spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1565| <<write_protect_gfn>> tdp_mmu_set_spte(kvm, &iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 				    u64 new_spte)
 {
 	__tdp_mmu_set_spte(kvm, iter, new_spte, true, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1282| <<age_gfn_range>> tdp_mmu_set_spte_no_acc_track(kvm, iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte_no_acc_track(struct kvm *kvm,
 						 struct tdp_iter *iter,
 						 u64 new_spte)
@@ -616,6 +796,10 @@ static inline void tdp_mmu_set_spte_no_acc_track(struct kvm *kvm,
 	__tdp_mmu_set_spte(kvm, iter, new_spte, false, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1522| <<clear_dirty_pt_masked>> tdp_mmu_set_spte_no_dirty_log(kvm, &iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 						 struct tdp_iter *iter,
 						 u64 new_spte)
@@ -623,6 +807,10 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 	__tdp_mmu_set_spte(kvm, iter, new_spte, true, false);
 }
 
+/*
+ * Iterates over every SPTE mapping the GFN range [start, end) in a
+ * preorder traversal.
+ */
 #define tdp_root_for_each_pte(_iter, _root, _start, _end) \
 	for_each_tdp_pte(_iter, _root->spt, _root->role.level, _start, _end)
 
@@ -633,6 +821,12 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 			continue;					\
 		else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1213| <<kvm_tdp_mmu_map>> tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1777| <<kvm_tdp_mmu_get_walk>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1812| <<kvm_tdp_mmu_fast_pf_get_last_sptep>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ */
 #define tdp_mmu_for_each_pte(_iter, _mmu, _start, _end)		\
 	for_each_tdp_pte(_iter, __va(_mmu->root_hpa),		\
 			 _mmu->shadow_root_level, _start, _end)
@@ -651,10 +845,26 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
  *
  * Returns true if this function yielded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|736| <<zap_gfn_range>> tdp_mmu_iter_cond_resched(kvm, &iter, flush, shared)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1196| <<wrprot_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1259| <<clear_dirty_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1391| <<zap_collapsible_spte_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ */
 static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 							  struct tdp_iter *iter,
 							  bool flush, bool shared)
 {
+	/*
+	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+	 * which case tdp_iter_next() needs to restart the walk at the root
+	 * level instead of advancing to the next entry.
+	 *
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 */
 	WARN_ON(iter->yielded);
 
 	/* Ensure forward progress has been made before yielding. */
@@ -676,6 +886,17 @@ static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 
 		WARN_ON(iter->gfn > iter->next_last_level_gfn);
 
+		/*
+		 * 在以下设置tdp_iter->yielded:
+		 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+		 * 在以下使用tdp_iter->yielded:
+		 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+		 */
 		iter->yielded = true;
 	}
 
@@ -699,6 +920,13 @@ static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
  * structures concurrently. If shared is false, this thread should hold the
  * MMU lock in write mode.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|145| <<kvm_tdp_mmu_put_root>> zap_gfn_range(kvm, root, 0, -1ull, false, false, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|954| <<__kvm_tdp_mmu_zap_gfn_range>> flush = zap_gfn_range(kvm, root, start, end, can_yield, flush,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1019| <<kvm_tdp_mmu_zap_invalidated_roots>> flush = zap_gfn_range(kvm, root, 0, -1ull, true, flush, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1221| <<kvm_tdp_mmu_unmap_gfn_range>> flush = zap_gfn_range(kvm, root, range->start, range->end,
+ */
 static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			  gfn_t start, gfn_t end, bool can_yield, bool flush,
 			  bool shared)
@@ -720,10 +948,17 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 	 */
 	end = min(end, max_gfn_host);
 
+	/*
+	 * kvm->mmu_lock
+	 */
 	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
 
 	rcu_read_lock();
 
+	/*
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 */
 	for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
 				   min_level, start, end) {
 retry:
@@ -770,6 +1005,11 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
  * SPTEs have been cleared and a TLB flush is needed before releasing the
  * MMU lock.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|27| <<kvm_tdp_mmu_zap_gfn_range>> return __kvm_tdp_mmu_zap_gfn_range(kvm, as_id, start, end, true, flush);
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|43| <<kvm_tdp_mmu_zap_sp>> return __kvm_tdp_mmu_zap_gfn_range(kvm, kvm_mmu_page_as_id(sp),
+ */
 bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 				 gfn_t end, bool can_yield, bool flush)
 {
@@ -782,6 +1022,10 @@ bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 	return flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6028| <<kvm_mmu_zap_all>> kvm_tdp_mmu_zap_all(kvm);
+ */
 void kvm_tdp_mmu_zap_all(struct kvm *kvm)
 {
 	bool flush = false;
@@ -794,6 +1038,11 @@ void kvm_tdp_mmu_zap_all(struct kvm *kvm)
 		kvm_flush_remote_tlbs(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1047| <<kvm_tdp_mmu_zap_invalidated_roots>> root = next_invalidated_root(kvm, NULL);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1050| <<kvm_tdp_mmu_zap_invalidated_roots>> next_root = next_invalidated_root(kvm, root);
+ */
 static struct kvm_mmu_page *next_invalidated_root(struct kvm *kvm,
 						  struct kvm_mmu_page *prev_root)
 {
@@ -824,6 +1073,10 @@ static struct kvm_mmu_page *next_invalidated_root(struct kvm *kvm,
  * only has to do a trivial amount of work. Since the roots are invalid,
  * no new SPTEs should be created under them.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5742| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_zap_invalidated_roots(kvm);
+ */
 void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
 {
 	struct kvm_mmu_page *next_root;
@@ -880,10 +1133,27 @@ void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
  * This has essentially the same effect for the TDP MMU
  * as updating mmu_valid_gen does for the shadow MMU.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5724| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_invalidate_all_roots(kvm);
+ */
 void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * List of struct kvm_mmu_pages being used as roots.
+	 * All struct kvm_mmu_pages in the list should have
+	 * tdp_mmu_page set.
+	 *
+	 * For reads, this list is protected by:
+	 *      the MMU lock in read mode + RCU or
+	 *      the MMU lock in write mode
+	 *
+	 * For writes, this list is protected by:
+	 *      the MMU lock in read mode + the tdp_mmu_pages_lock or
+	 *      the MMU lock in write mode
+	 */
 	lockdep_assert_held_write(&kvm->mmu_lock);
 	list_for_each_entry(root, &kvm->arch.tdp_mmu_roots, link)
 		if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
@@ -894,6 +1164,10 @@ void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
  * Installs a last-level SPTE to handle a TDP page fault.
  * (NPT/EPT violation/misconfiguration)
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1274| <<kvm_tdp_mmu_map>> ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
+ */
 static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 					  struct kvm_page_fault *fault,
 					  struct tdp_iter *iter)
@@ -965,6 +1239,11 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 
 	rcu_read_lock();
 
+	/*
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 * end是')'
+	 */
 	tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
 		if (fault->nx_huge_page_workaround_enabled)
 			disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
@@ -990,6 +1269,9 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 			iter.old_spte = READ_ONCE(*rcu_dereference(iter.sptep));
 		}
 
+		/*
+		 * iter.old_spte: snapshot of the value at sptep
+		 */
 		if (!is_shadow_present_pte(iter.old_spte)) {
 			/*
 			 * If SPTE has been frozen by another thread, just
@@ -1023,6 +1305,10 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		return RET_PF_RETRY;
 	}
 
+	/*
+	 * Installs a last-level SPTE to handle a TDP page fault.
+	 * (NPT/EPT violation/misconfiguration)
+	 */
 	ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
 	rcu_read_unlock();
 
@@ -1349,6 +1635,11 @@ static void clear_dirty_pt_masked(struct kvm *kvm, struct kvm_mmu_page *root,
  * clearing the dirty status will involve clearing the dirty bit on each SPTE
  * or, if AD bits are not enabled, clearing the writable bit on each SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1311| <<kvm_mmu_write_protect_pt_masked>> kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
+ *   - arch/x86/kvm/mmu/mmu.c|1344| <<kvm_mmu_clear_dirty_pt_masked>> kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
+ */
 void kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 				       struct kvm_memory_slot *slot,
 				       gfn_t gfn, unsigned long mask,
@@ -1365,6 +1656,10 @@ void kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,
  * Clear leaf entries which could be replaced by large mappings, for
  * GFNs within the slot.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1641| <<kvm_tdp_mmu_zap_collapsible_sptes>> zap_collapsible_spte_range(kvm, root, slot);
+ */
 static void zap_collapsible_spte_range(struct kvm *kvm,
 				       struct kvm_mmu_page *root,
 				       const struct kvm_memory_slot *slot)
@@ -1409,11 +1704,18 @@ static void zap_collapsible_spte_range(struct kvm *kvm,
  * Clear non-leaf entries (and free associated page tables) which could
  * be replaced by large mappings, for GFNs within the slot.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5957| <<kvm_mmu_zap_collapsible_sptes>> kvm_tdp_mmu_zap_collapsible_sptes(kvm, slot);
+ */
 void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
 				       const struct kvm_memory_slot *slot)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * read lock !!!!
+	 */
 	lockdep_assert_held_read(&kvm->mmu_lock);
 
 	for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
@@ -1425,6 +1727,10 @@ void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1657| <<kvm_tdp_mmu_write_protect_gfn>> spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
+ */
 static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
 			      gfn_t gfn, int min_level)
 {
@@ -1462,6 +1768,10 @@ static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1425| <<kvm_mmu_slot_gfn_write_protect>> kvm_tdp_mmu_write_protect_gfn(kvm, slot, gfn, min_level);
+ */
 bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level)
@@ -1470,6 +1780,12 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 	bool spte_set = false;
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
+	/*
+	 * write_protect_gfn():
+	 *   Removes write access on the last level SPTE mapping this GFN and unsets the
+	 *   MMU-writable bit to ensure future writes continue to be intercepted.
+	 *   Returns true if an SPTE was set and a TLB flush is needed.
+	 */
 	for_each_tdp_mmu_root(kvm, root, slot->as_id)
 		spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
 
@@ -1482,6 +1798,10 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
  *
  * Must be called between kvm_tdp_mmu_walk_lockless_{begin,end}.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3823| <<get_mmio_spte>> leaf = kvm_tdp_mmu_get_walk(vcpu, addr, sptes, &root);
+ */
 int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 			 int *root_level)
 {
@@ -1494,6 +1814,10 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 
 	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
 		leaf = iter.level;
+		/*
+		 * 从get_mmio_spte()来的时候:
+		 * u64 sptes[PT64_ROOT_MAX_LEVEL + 1];
+		 */
 		sptes[leaf] = iter.old_spte;
 	}
 
@@ -1511,6 +1835,10 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
  *
  * WARNING: This function is only intended to be called during fast_page_fault.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3173| <<fast_page_fault>> sptep = kvm_tdp_mmu_fast_pf_get_last_sptep(vcpu, fault->addr, &spte);
+ */
 u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 					u64 *spte)
 {
@@ -1520,6 +1848,10 @@ u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 	tdp_ptep_t sptep = NULL;
 
 	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+		/*
+		 * iter.old_spte : A snapshot of the value at sptep
+		 * iter.sptep    : A pointer to the current SPTE
+		 */
 		*spte = iter.old_spte;
 		sptep = iter.sptep;
 	}
diff --git a/arch/x86/kvm/mmu/tdp_mmu.h b/arch/x86/kvm/mmu/tdp_mmu.h
index 3899004a5d91..8dd0d8311add 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@ -7,12 +7,29 @@
 
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|182| <<tdp_mmu_next_root>> while (next_root && !kvm_tdp_mmu_get_root(kvm, next_root))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|297| <<kvm_tdp_mmu_get_vcpu_root_hpa>> kvm_tdp_mmu_get_root(kvm, root))
+ */
 __must_check static inline bool kvm_tdp_mmu_get_root(struct kvm *kvm,
 						     struct kvm_mmu_page *root)
 {
 	if (root->role.invalid)
 		return false;
 
+	/*
+	 * increment a refcount unless it is 0
+	 * Return: true if the increment was successful, false otherwise
+	 */
+	/*
+	 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+	 */
 	return refcount_inc_not_zero(&root->tdp_mmu_root_count);
 }
 
@@ -21,11 +38,20 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 
 bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 				 gfn_t end, bool can_yield, bool flush);
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5809| <<kvm_zap_gfn_range>> flush = kvm_tdp_mmu_zap_gfn_range(kvm, i, gfn_start,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|801| <<kvm_tdp_mmu_zap_all>> flush = kvm_tdp_mmu_zap_gfn_range(kvm, i, 0, -1ull, flush);
+ */
 static inline bool kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id,
 					     gfn_t start, gfn_t end, bool flush)
 {
 	return __kvm_tdp_mmu_zap_gfn_range(kvm, as_id, start, end, true, flush);
 }
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6301| <<kvm_recover_nx_lpages>> flush |= kvm_tdp_mmu_zap_sp(kvm, sp);
+ */
 static inline bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	gfn_t end = sp->gfn + KVM_PAGES_PER_HPAGE(sp->role.level + 1);
@@ -40,6 +66,12 @@ static inline bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 	 * of the shadow page's gfn range and stop iterating before yielding.
 	 */
 	lockdep_assert_held_write(&kvm->mmu_lock);
+	/*
+	 * Tears down the mappings for the range of gfns, [start, end), and frees the
+	 * non-root pages mapping GFNs strictly within that range. Returns true if
+	 * SPTEs have been cleared and a TLB flush is needed before releasing the
+	 * MMU lock.
+	 */
 	return __kvm_tdp_mmu_zap_gfn_range(kvm, kvm_mmu_page_as_id(sp),
 					   sp->gfn, end, false, false);
 }
@@ -71,6 +103,10 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|696| <<walk_shadow_page_lockless_begin>> kvm_tdp_mmu_walk_lockless_begin();
+ */
 static inline void kvm_tdp_mmu_walk_lockless_begin(void)
 {
 	rcu_read_lock();
@@ -105,6 +141,24 @@ static inline bool is_tdp_mmu(struct kvm_mmu *mmu)
 	 * pae_root page, not a shadow page.
 	 */
 	sp = to_shadow_page(hpa);
+	/*
+	 * 在以下设置kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|3358| <<mmu_alloc_root>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu/mmu.c|3255| <<mmu_free_root_page>> else if (!--sp->root_count && sp->role.invalid)
+	 * 在以下使用kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|2364| <<__kvm_mmu_prepare_zap_page>> if (!sp->root_count) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2430| <<kvm_mmu_commit_zap_page>> WARN_ON(!sp->role.invalid || sp->root_count);
+	 *   - arch/x86/kvm/mmu/mmu.c|2453| <<kvm_mmu_zap_oldest_mmu_pages>> if (sp->root_count)
+	 *   - arch/x86/kvm/mmu/mmutrace.h|22| <<KVM_MMU_PAGE_ASSIGN>> __entry->root_count = sp->root_count; \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|108| <<is_tdp_mmu>> return sp && is_tdp_mmu_page(sp) && sp->root_count;
+	 *
+	 * 在以下使用tdp_mmu_root_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+	 */
 	return sp && is_tdp_mmu_page(sp) && sp->root_count;
 }
 #else
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index eb4029660bd9..3d93c3984825 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -3326,6 +3326,10 @@ void kvm_service_local_tlb_flush_requests(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_service_local_tlb_flush_requests);
 
+/*
+ * 处理KVM_REQ_STEAL_UPDATE:
+ *   - arch/x86/kvm/x86.c|9907| <<vcpu_enter_guest>> record_steal_time(vcpu);
+ */
 static void record_steal_time(struct kvm_vcpu *vcpu)
 {
 	struct gfn_to_hva_cache *ghc = &vcpu->arch.st.cache;
@@ -8791,6 +8795,14 @@ static struct notifier_block pvclock_gtod_notifier = {
 };
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5664| <<kvm_init>> r = kvm_arch_init(opaque);
+ *
+ * vmx_init()
+ * -> kvm_init()
+ *    -> kvm_arch_init()
+ */
 int kvm_arch_init(void *opaque)
 {
 	struct kvm_x86_init_ops *ops = opaque;
@@ -9234,6 +9246,11 @@ int kvm_check_nested_events(struct kvm_vcpu *vcpu)
 	return kvm_x86_ops.nested_ops->check_events(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9260| <<inject_pending_event>> kvm_inject_exception(vcpu);
+ *   - arch/x86/kvm/x86.c|9323| <<inject_pending_event>> kvm_inject_exception(vcpu);
+ */
 static void kvm_inject_exception(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->arch.exception.error_code && !is_protmode(vcpu))
@@ -11643,6 +11660,10 @@ static void kvm_unload_vcpu_mmu(struct kvm_vcpu *vcpu)
 	vcpu_put(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11782| <<kvm_arch_destroy_vm>> kvm_free_vcpus(kvm);
+ */
 static void kvm_free_vcpus(struct kvm *kvm)
 {
 	unsigned long i;
@@ -11746,6 +11767,11 @@ void kvm_arch_pre_destroy_vm(struct kvm *kvm)
 	kvm_mmu_pre_destroy_vm(kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1150| <<kvm_create_vm>> kvm_arch_destroy_vm(kvm);
+ *   - virt/kvm/kvm_main.c|1217| <<kvm_destroy_vm>> kvm_arch_destroy_vm(kvm);
+ */
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	if (current->mm == kvm->mm) {
@@ -12499,6 +12525,10 @@ bool kvm_vector_hashing_enabled(void)
 	return vector_hashing;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3383| <<kvm_vcpu_halt>> bool halt_poll_allowed = !kvm_arch_no_poll(vcpu);
+ */
 bool kvm_arch_no_poll(struct kvm_vcpu *vcpu)
 {
 	return (vcpu->arch.msr_kvm_poll_control & 1) == 0;
diff --git a/drivers/cpuidle/cpuidle-haltpoll.c b/drivers/cpuidle/cpuidle-haltpoll.c
index fcc53215bac8..8bd37881913e 100644
--- a/drivers/cpuidle/cpuidle-haltpoll.c
+++ b/drivers/cpuidle/cpuidle-haltpoll.c
@@ -99,12 +99,41 @@ static bool haltpoll_want(void)
 	return kvm_para_has_hint(KVM_HINTS_REALTIME) || force;
 }
 
+/*
+ * The x86 architecture support code recognizes three kernel command line
+ * options related to CPU idle time management: idle=poll, idle=halt, and
+ * idle=nomwait. The first two of them disable the acpi_idle and intel_idle
+ * drivers altogether, which effectively causes the entire CPUIdle subsystem to
+ * be disabled and makes the idle loop invoke the architecture support code to
+ * deal with idle CPUs. How it does that depends on which of the two parameters
+ * is added to the kernel command line. In the idle=halt case, the architecture
+ * support code will use the HLT instruction of the CPUs (which, as a rule,
+ * suspends the execution of the program and causes the hardware to attempt to
+ * enter the shallowest available idle state) for this purpose, and if
+ * idle=poll is used, idle CPUs will execute a more or less lightweight''
+ * sequence of instructions in a tight loop.  [Note that using ``idle=poll is
+ * somewhat drastic in many cases, as preventing idle CPUs from saving almost
+ * any energy at all may not be the only effect of it. For example, on Intel
+ * hardware it effectively prevents CPUs from using P-states (see CPU
+ * Performance Scaling) that require any number of CPUs in a package to be
+ * idle, so it very well may hurt single-thread computations performance as
+ * well as energy-efficiency. Thus using it for performance reasons may not be
+ * a good idea at all.
+ *
+ * The idle=nomwait option disables the intel_idle driver and causes acpi_idle
+ * to be used (as long as all of the information needed by it is there in the
+ * system’s ACPI tables), but it is not allowed to use the MWAIT instruction of
+ * the CPUs to ask the hardware to enter idle states.
+ */
 static int __init haltpoll_init(void)
 {
 	int ret;
 	struct cpuidle_driver *drv = &haltpoll_driver;
 
 	/* Do not load haltpoll if idle= is passed */
+	/*
+	 * IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT, IDLE_POLL
+	 */
 	if (boot_option_idle_override != IDLE_NO_OVERRIDE)
 		return -ENODEV;
 
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index 47aebd98f52f..aabc084ca0dc 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -152,6 +152,11 @@ static const char *xen_swiotlb_error(enum xen_swiotlb_err err)
 	return "";
 }
 
+/*
+ * called by:
+ *   - arch/arm/xen/mm.c|146| <<xen_mm_init>> rc = xen_swiotlb_init();
+ *   - arch/x86/xen/pci-swiotlb-xen.c|79| <<pci_xen_swiotlb_init_late>> rc = xen_swiotlb_init();
+ */
 int xen_swiotlb_init(void)
 {
 	enum xen_swiotlb_err m_ret = XEN_SWIOTLB_UNKNOWN;
diff --git a/include/linux/kvm_dirty_ring.h b/include/linux/kvm_dirty_ring.h
index 906f899813dc..b80548c292b5 100644
--- a/include/linux/kvm_dirty_ring.h
+++ b/include/linux/kvm_dirty_ring.h
@@ -23,6 +23,16 @@ struct kvm_dirty_ring {
 	u32 reset_index;
 	u32 size;
 	u32 soft_limit;
+	/*
+	 * 在以下使用dirty_ring->dirty_gfns:
+	 *   - virt/kvm/dirty_ring.c|62| <<kvm_dirty_ring_alloc>> ring->dirty_gfns = vzalloc(size);
+	 *   - virt/kvm/dirty_ring.c|63| <<kvm_dirty_ring_alloc>> if (!ring->dirty_gfns)
+	 *   - virt/kvm/dirty_ring.c|103| <<kvm_dirty_ring_reset>> entry = &ring->dirty_gfns[ring->reset_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|157| <<kvm_dirty_ring_push>> entry = &ring->dirty_gfns[ring->dirty_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|173| <<kvm_dirty_ring_get_page>> return vmalloc_to_page((void *)ring->dirty_gfns + offset * PAGE_SIZE);
+	 *   - virt/kvm/dirty_ring.c|178| <<kvm_dirty_ring_free>> vfree(ring->dirty_gfns);
+	 *   - virt/kvm/dirty_ring.c|179| <<kvm_dirty_ring_free>> ring->dirty_gfns = NULL;
+	 */
 	struct kvm_dirty_gfn *dirty_gfns;
 	int index;
 };
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index 6db1c475ec82..fd6c7c019c26 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -71,6 +71,20 @@
 
 enum swiotlb_force swiotlb_force;
 
+/*
+ * 在以下使用io_tlb_default_mem:
+ *   - drivers/base/core.c|2885| <<device_initialize>> dev->dma_io_tlb_mem = &io_tlb_default_mem;
+ *   - drivers/xen/swiotlb-xen.c|164| <<xen_swiotlb_init>> if (io_tlb_default_mem.nslabs) {
+ *   - drivers/xen/swiotlb-xen.c|548| <<xen_swiotlb_dma_supported>> return xen_phys_to_dma(hwdev, io_tlb_default_mem.end - 1) <= mask;
+ *   - kernel/dma/swiotlb.c|107| <<swiotlb_max_segment>> return io_tlb_default_mem.nslabs ? max_segment : 0;
+ *   - kernel/dma/swiotlb.c|140| <<swiotlb_print_info>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|197| <<swiotlb_update_mem_attributes>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|250| <<swiotlb_init_with_tbl>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|352| <<swiotlb_late_init_with_tbl>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|377| <<swiotlb_exit>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|780| <<swiotlb_create_default_debugfs>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|882| <<rmem_swiotlb_device_release>> dev->dma_io_tlb_mem = &io_tlb_default_mem;
+ */
 struct io_tlb_mem io_tlb_default_mem;
 
 phys_addr_t swiotlb_unencrypted_base;
@@ -81,6 +95,15 @@ phys_addr_t swiotlb_unencrypted_base;
  */
 static unsigned int max_segment;
 
+/*
+ * 在以下使用default_nslabs:
+ *   - kernel/dma/swiotlb.c|105| <<setup_io_tlb_npages>> default_nslabs =
+ *   - kernel/dma/swiotlb.c|135| <<swiotlb_size_or_default>> return default_nslabs << IO_TLB_SHIFT;
+ *   - kernel/dma/swiotlb.c|145| <<swiotlb_adjust_size>> if (default_nslabs != IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT)
+ *   - kernel/dma/swiotlb.c|148| <<swiotlb_adjust_size>> default_nslabs = ALIGN(size >> IO_TLB_SHIFT, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|333| <<swiotlb_init>> size_t bytes = PAGE_ALIGN(default_nslabs << IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|343| <<swiotlb_init>> if (swiotlb_init_with_tbl(tlb, default_nslabs, verbose))
+ */
 static unsigned long default_nslabs = IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT;
 
 static int __init
@@ -121,6 +144,10 @@ unsigned long swiotlb_size_or_default(void)
 	return default_nslabs << IO_TLB_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/mem_encrypt_amd.c|227| <<sev_setup_arch>> swiotlb_adjust_size(size);
+ */
 void __init swiotlb_adjust_size(unsigned long size)
 {
 	/*
@@ -135,6 +162,13 @@ void __init swiotlb_adjust_size(unsigned long size)
 	pr_info("SWIOTLB bounce buffer size adjusted to %luMB", size >> 20);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kernel/dma-swiotlb.c|23| <<check_swiotlb_enabled>> swiotlb_print_info();
+ *   - arch/x86/kernel/pci-swiotlb.c|75| <<pci_swiotlb_late_init>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|269| <<swiotlb_init_with_tbl>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|370| <<swiotlb_late_init_with_tbl>> swiotlb_print_info();
+ */
 void swiotlb_print_info(void)
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
@@ -211,6 +245,12 @@ void __init swiotlb_update_mem_attributes(void)
 	memset(mem->vaddr, 0, bytes);
 }
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|266| <<swiotlb_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
+ *   - kernel/dma/swiotlb.c|368| <<swiotlb_late_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, virt_to_phys(tlb), nslabs, true);
+ *   - kernel/dma/swiotlb.c|865| <<rmem_swiotlb_device_init>> swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+ */
 static void swiotlb_init_io_tlb_mem(struct io_tlb_mem *mem, phys_addr_t start,
 				    unsigned long nslabs, bool late_alloc)
 {
@@ -245,6 +285,13 @@ static void swiotlb_init_io_tlb_mem(struct io_tlb_mem *mem, phys_addr_t start,
 	return;
 }
 
+/*
+ * called by:
+ *   - arch/mips/cavium-octeon/dma-octeon.c|248| <<plat_swiotlb_setup>> if (swiotlb_init_with_tbl(octeon_swiotlb, swiotlb_nslabs, 1) == -ENOMEM)
+ *   - arch/powerpc/platforms/pseries/svm.c|56| <<svm_swiotlb_init>> if (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, false))
+ *   - drivers/xen/swiotlb-xen.c|255| <<xen_swiotlb_init_early>> if (swiotlb_init_with_tbl(start, nslabs, true))
+ *   - kernel/dma/swiotlb.c|291| <<swiotlb_init>> if (swiotlb_init_with_tbl(tlb, default_nslabs, verbose))
+ */
 int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
@@ -263,6 +310,12 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
 		      __func__, alloc_size, PAGE_SIZE);
 
+	/*
+	 * called by:
+	 *   - kernel/dma/swiotlb.c|266| <<swiotlb_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
+	 *   - kernel/dma/swiotlb.c|368| <<swiotlb_late_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, virt_to_phys(tlb), nslabs, true);
+	 *   - kernel/dma/swiotlb.c|865| <<rmem_swiotlb_device_init>> swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+	 */
 	swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
 
 	if (verbose)
@@ -275,6 +328,18 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
  * Statically reserve bounce buffer space and initialize bounce buffer data
  * structures for the software IO TLB used to implement the DMA API.
  */
+/*
+ * called by:
+ *   - arch/arm/mm/init.c|317| <<mem_init>> swiotlb_init(1);
+ *   - arch/arm64/mm/init.c|378| <<mem_init>> swiotlb_init(1); 
+ *   - arch/ia64/mm/init.c|441| <<mem_init>> swiotlb_init(1);
+ *   - arch/mips/loongson64/dma.c|27| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/mips/sibyte/common/dma.c|13| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/powerpc/mm/mem.c|254| <<mem_init>> swiotlb_init(0);
+ *   - arch/riscv/mm/init.c|124| <<mem_init>> swiotlb_init(1);
+ *   - arch/s390/mm/init.c|189| <<pv_init>> swiotlb_init(1);
+ *   - arch/x86/kernel/pci-swiotlb.c|64| <<pci_swiotlb_init>> swiotlb_init(0);
+ */
 void  __init
 swiotlb_init(int verbose)
 {
@@ -303,6 +368,10 @@ swiotlb_init(int verbose)
  * initialize the swiotlb later using the slab allocator if needed.
  * This should be just like above, but with some error catching.
  */
+/*
+ * called by:
+ *   - arch/x86/pci/sta2x11-fixup.c|60| <<sta2x11_new_instance>> if (swiotlb_late_init_with_default_size(size))
+ */
 int
 swiotlb_late_init_with_default_size(size_t default_size)
 {
@@ -758,6 +827,13 @@ size_t swiotlb_max_mapping_size(struct device *dev)
 	return ((size_t)IO_TLB_SIZE) * IO_TLB_SEGSIZE;
 }
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/i915/gem/i915_gem_internal.c|45| <<i915_gem_object_get_pages_internal>> if (is_swiotlb_active(obj->base.dev->dev)) {
+ *   - drivers/gpu/drm/nouveau/nouveau_ttm.c|279| <<nouveau_ttm_init>> need_swiotlb = is_swiotlb_active(dev->dev);
+ *   - drivers/pci/xen-pcifront.c|684| <<pcifront_connect_and_init_dma>> if (!err && !is_swiotlb_active(&pdev->xdev->dev)) {
+ *   - kernel/dma/direct.c|580| <<dma_direct_max_mapping_size>> if (is_swiotlb_active(dev) &&
+ */
 bool is_swiotlb_active(struct device *dev)
 {
 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
@@ -769,6 +845,11 @@ EXPORT_SYMBOL_GPL(is_swiotlb_active);
 #ifdef CONFIG_DEBUG_FS
 static struct dentry *debugfs_dir;
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|785| <<swiotlb_create_default_debugfs>> swiotlb_create_debugfs_files(mem);
+ *   - kernel/dma/swiotlb.c|802| <<rmem_swiotlb_debugfs_init>> swiotlb_create_debugfs_files(mem);
+ */
 static void swiotlb_create_debugfs_files(struct io_tlb_mem *mem)
 {
 	debugfs_create_ulong("io_tlb_nslabs", 0400, mem->debugfs, &mem->nslabs);
diff --git a/kernel/time/clocksource.c b/kernel/time/clocksource.c
index 1cf73807b450..0b7c96f7c713 100644
--- a/kernel/time/clocksource.c
+++ b/kernel/time/clocksource.c
@@ -378,6 +378,10 @@ void clocksource_verify_percpu(struct clocksource *cs)
 }
 EXPORT_SYMBOL_GPL(clocksource_verify_percpu);
 
+/*
+ * called by:
+ *   - kernel/time/clocksource.c|520| <<clocksource_start_watchdog>> timer_setup(&watchdog_timer, clocksource_watchdog, 0);
+ */
 static void clocksource_watchdog(struct timer_list *unused)
 {
 	u64 csnow, wdnow, cslast, wdlast, delta;
diff --git a/tools/testing/selftests/kvm/kvm_page_table_test.c b/tools/testing/selftests/kvm/kvm_page_table_test.c
index ba1fdc3dcf4a..cdb88794e4fb 100644
--- a/tools/testing/selftests/kvm/kvm_page_table_test.c
+++ b/tools/testing/selftests/kvm/kvm_page_table_test.c
@@ -67,6 +67,11 @@ struct test_args {
  * Guest variables. Use addr_gva2hva() if these variables need
  * to be changed in host.
  */
+/*
+ * 在以下使用guest_test_stage:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|99| <<guest_code>> enum test_stage *current_stage = &guest_test_stage;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|321| <<pre_init_before_test>> current_stage = addr_gva2hva(vm, (vm_vaddr_t)(&guest_test_stage));
+ */
 static enum test_stage guest_test_stage;
 
 /* Host variables */
@@ -90,12 +95,20 @@ static uint64_t guest_test_phys_mem;
  * Guest virtual memory offset of the testing memory slot.
  * Must not conflict with identity mapped test code.
  */
+/*
+ * 0xc0000000
+ */
 static uint64_t guest_test_virt_mem = DEFAULT_GUEST_TEST_MEM;
 
 static void guest_code(int vcpu_id)
 {
 	struct test_args *p = &test_args;
 	struct vcpu_args *vcpu_args = &p->vcpu_args[vcpu_id];
+	/*
+	 * 在以下使用guest_test_stage:
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|99| <<guest_code>> enum test_stage *current_stage = &guest_test_stage;
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|321| <<pre_init_before_test>> current_stage = addr_gva2hva(vm, (vm_vaddr_t)(&guest_test_stage));
+	 */
 	enum test_stage *current_stage = &guest_test_stage;
 	uint64_t addr;
 	int i, j;
@@ -336,6 +349,13 @@ static struct kvm_vm *pre_init_before_test(enum vm_guest_mode mode, void *arg)
 	return vm;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|400| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|407| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|420| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|432| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ */
 static void vcpus_complete_new_stage(enum test_stage stage)
 {
 	int ret;
diff --git a/tools/testing/selftests/kvm/lib/guest_modes.c b/tools/testing/selftests/kvm/lib/guest_modes.c
index 8784013b747c..981fc1a5a829 100644
--- a/tools/testing/selftests/kvm/lib/guest_modes.c
+++ b/tools/testing/selftests/kvm/lib/guest_modes.c
@@ -11,6 +11,16 @@ enum vm_guest_mode vm_mode_default;
 
 struct guest_mode guest_modes[NUM_VM_MODES];
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|355| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|408| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|344| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|864| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|485| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|501| <<init_guest_modes>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|154| <<main>> guest_modes_append_default();
+ */
 void guest_modes_append_default(void)
 {
 #ifndef __aarch64__
diff --git a/virt/kvm/dirty_ring.c b/virt/kvm/dirty_ring.c
index 222ecc81d7df..50be47c94c8f 100644
--- a/virt/kvm/dirty_ring.c
+++ b/virt/kvm/dirty_ring.c
@@ -147,6 +147,20 @@ int kvm_dirty_ring_reset(struct kvm *kvm, struct kvm_dirty_ring *ring)
 	return count;
 }
 
+/*
+ * 一个callstack的例子
+ * [0] kvm_dirty_ring_push
+ * [0] mark_page_dirty_in_slot
+ * [0] kvm_steal_time_set_preempted
+ * [0] kvm_arch_vcpu_put
+ * [0] vcpu_put
+ * [0] vmx_free_vcpu
+ * [0] kvm_arch_vcpu_destroy
+ * [0] kvm_vcpu_destroy
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|3171| <<mark_page_dirty_in_slot>> kvm_dirty_ring_push(&vcpu->dirty_ring,
+ */
 void kvm_dirty_ring_push(struct kvm_dirty_ring *ring, u32 slot, u64 offset)
 {
 	struct kvm_dirty_gfn *entry;
@@ -175,6 +189,16 @@ struct page *kvm_dirty_ring_get_page(struct kvm_dirty_ring *ring, u32 offset)
 
 void kvm_dirty_ring_free(struct kvm_dirty_ring *ring)
 {
+	/*
+	 * 在以下使用dirty_ring->dirty_gfns:
+	 *   - virt/kvm/dirty_ring.c|62| <<kvm_dirty_ring_alloc>> ring->dirty_gfns = vzalloc(size);
+	 *   - virt/kvm/dirty_ring.c|63| <<kvm_dirty_ring_alloc>> if (!ring->dirty_gfns)
+	 *   - virt/kvm/dirty_ring.c|103| <<kvm_dirty_ring_reset>> entry = &ring->dirty_gfns[ring->reset_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|157| <<kvm_dirty_ring_push>> entry = &ring->dirty_gfns[ring->dirty_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|173| <<kvm_dirty_ring_get_page>> return vmalloc_to_page((void *)ring->dirty_gfns + offset * PAGE_SIZE);
+	 *   - virt/kvm/dirty_ring.c|178| <<kvm_dirty_ring_free>> vfree(ring->dirty_gfns);
+	 *   - virt/kvm/dirty_ring.c|179| <<kvm_dirty_ring_free>> ring->dirty_gfns = NULL;
+	 */
 	vfree(ring->dirty_gfns);
 	ring->dirty_gfns = NULL;
 }
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 0afc016cc54d..10d6f60b3d9b 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -435,6 +435,10 @@ static void kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)
 	vcpu->last_used_slot = NULL;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|460| <<kvm_destroy_vcpus>> kvm_vcpu_destroy(vcpu);
+ */
 static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_dirty_ring_free(&vcpu->dirty_ring);
@@ -451,6 +455,15 @@ static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|182| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/mips/kvm/mips.c|183| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/powerpc/kvm/powerpc.c|476| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/riscv/kvm/vm.c|49| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/s390/kvm/kvm-s390.c|2798| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/x86/kvm/x86.c|11672| <<kvm_free_vcpus>> kvm_destroy_vcpus(kvm);
+ */
 void kvm_destroy_vcpus(struct kvm *kvm)
 {
 	unsigned long i;
@@ -1176,6 +1189,10 @@ static void kvm_destroy_devices(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1250| <<kvm_put_kvm>> kvm_destroy_vm(kvm);
+ */
 static void kvm_destroy_vm(struct kvm *kvm)
 {
 	int i;
@@ -1244,6 +1261,26 @@ bool kvm_get_kvm_safe(struct kvm *kvm)
 }
 EXPORT_SYMBOL_GPL(kvm_get_kvm_safe);
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1932| <<kvm_htab_release>> kvm_put_kvm(ctx->kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|2011| <<debugfs_htab_release>> kvm_put_kvm(p->kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|1283| <<debugfs_radix_release>> kvm_put_kvm(p->kvm);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|267| <<kvm_spapr_tce_release>> kvm_put_kvm(stt->kvm);
+ *   - arch/powerpc/kvm/book3s_hv.c|2684| <<debugfs_timings_release>> kvm_put_kvm(p->vcpu->kvm);
+ *   - arch/x86/kvm/debugfs.c|172| <<kvm_mmu_rmaps_stat_release>> kvm_put_kvm(kvm);
+ *   - arch/x86/kvm/svm/sev.c|2058| <<sev_vm_destroy>> kvm_put_kvm(owner_kvm);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1961| <<kvmgt_guest_exit>> kvm_put_kvm(info->kvm);
+ *   - drivers/s390/crypto/vfio_ap_ops.c|1167| <<vfio_ap_mdev_unset_kvm>> kvm_put_kvm(kvm);
+ *   - virt/kvm/async_pf.c|91| <<async_pf_execute>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/async_pf.c|118| <<kvm_clear_async_pf_completion_queue>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|1273| <<kvm_vm_release>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|3668| <<kvm_vcpu_release>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|4177| <<kvm_device_release>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|4781| <<kvm_dev_ioctl_create_vm>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|5254| <<kvm_debugfs_open>> kvm_put_kvm(stat_data->kvm);
+ *   - virt/kvm/kvm_main.c|5267| <<kvm_debugfs_release>> kvm_put_kvm(stat_data->kvm);
+ */
 void kvm_put_kvm(struct kvm *kvm)
 {
 	if (refcount_dec_and_test(&kvm->users_count))
@@ -5437,6 +5474,11 @@ DEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_fops, vcpu_stat_get, vcpu_stat_clear,
 			"%llu\n");
 DEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_readonly_fops, vcpu_stat_get, NULL, "%llu\n");
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1185| <<kvm_destroy_vm>> kvm_uevent_notify_change(KVM_EVENT_DESTROY_VM, kvm);
+ *   - virt/kvm/kvm_main.c|4775| <<kvm_dev_ioctl_create_vm>> kvm_uevent_notify_change(KVM_EVENT_CREATE_VM, kvm);
+ */
 static void kvm_uevent_notify_change(unsigned int type, struct kvm *kvm)
 {
 	struct kobj_uevent_env *env;
@@ -5654,6 +5696,18 @@ static void check_processor_compat(void *data)
 	*c->ret = kvm_arch_check_processor_compat(c->opaque);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2240| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/mips/kvm/mips.c|1649| <<kvm_mips_init>> ret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/book3s.c|1065| <<kvmppc_book3s_init>> r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500.c|533| <<kvmppc_e500_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500mc.c|403| <<kvmppc_e500mc_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/riscv/kvm/main.c|124| <<riscv_kvm_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/s390/kvm/kvm-s390.c|5075| <<kvm_s390_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/x86/kvm/svm/svm.c|4914| <<svm_init>> return kvm_init(&svm_init_ops, sizeof(struct vcpu_svm),
+ *   - arch/x86/kvm/vmx/vmx.c|8166| <<vmx_init>> r = kvm_init(&vmx_init_ops, sizeof(struct vcpu_vmx),
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
-- 
2.17.1

